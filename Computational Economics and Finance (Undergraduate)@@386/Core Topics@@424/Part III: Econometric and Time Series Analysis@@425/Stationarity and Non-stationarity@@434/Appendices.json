{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise in time series decomposition. We will investigate a process whose non-stationarity arises from a time-varying intercept that follows a random walk, a structure known as a stochastic trend. By determining the minimum number of differences required to achieve stationarity, you will develop the crucial skill of identifying and handling unit root non-stationarity, a common feature in economic and financial data [@problem_id:2433747].", "id": "2433747", "problem": "Consider a discrete-time stochastic process $\\{y_t\\}_{t \\in \\mathbb{Z}}$ defined by\n$$\ny_t \\;=\\; \\alpha_t \\;+\\; 0.5\\, y_{t-1} \\;+\\; \\epsilon_t,\n$$\nwhere the time-varying intercept $\\{\\alpha_t\\}$ follows a random walk\n$$\n\\alpha_t \\;=\\; \\alpha_{t-1} \\;+\\; u_t.\n$$\nAssume $\\{\\epsilon_t\\}$ and $\\{u_t\\}$ are mutually independent sequences of independent and identically distributed (i.i.d.) random variables with $\\mathbb{E}[\\epsilon_t]=0$, $\\mathbb{E}[u_t]=0$, $\\operatorname{Var}(\\epsilon_t)=\\sigma_{\\epsilon}^{2}\\in (0,\\infty)$, and $\\operatorname{Var}(u_t)=\\sigma_{u}^{2}\\in (0,\\infty)$. Assume $\\alpha_0$ and $y_{-1}$ are finite-variance constants independent of $\\{\\epsilon_t\\}$ and $\\{u_t\\}$. Let the first-difference operator be defined by $\\Delta y_t \\equiv y_t - y_{t-1}$ and for an integer $d \\ge 0$, define $\\Delta^{d}$ recursively by $\\Delta^{0} y_t \\equiv y_t$ and $\\Delta^{d} y_t \\equiv \\Delta(\\Delta^{d-1} y_t)$ for $d \\ge 1$.\n\nDetermine the smallest integer $d \\ge 0$ such that $\\{\\Delta^{d} y_t\\}$ is weakly stationary (covariance-stationary). Provide your answer as the value of $d$. No units are required. Your answer must be a single integer with no rounding.", "solution": "The problem is to determine the smallest non-negative integer $d$ such that the process $\\{\\Delta^d y_t\\}$ is weakly stationary. A process $\\{z_t\\}$ is weakly stationary if its mean $\\mathbb{E}[z_t]$ and variance $\\operatorname{Var}(z_t)$ are constant over time, and the autocovariance $\\operatorname{Cov}(z_t, z_{t-h})$ depends only on the lag $h$.\n\nThe process $\\{y_t\\}$ is defined by the system of equations:\n$$\n(1) \\quad y_t = \\alpha_t + 0.5 y_{t-1} + \\epsilon_t\n$$\n$$\n(2) \\quad \\alpha_t = \\alpha_{t-1} + u_t\n$$\nwhere $\\{\\epsilon_t\\}$ and $\\{u_t\\}$ are mutually independent i.i.d. sequences with zero mean and finite positive variances $\\sigma_{\\epsilon}^{2}$ and $\\sigma_{u}^{2}$, respectively.\n\nLet us first analyze the stationarity of $\\{y_t\\}$ itself, which corresponds to the case $d=0$.\nWe can express the system in terms of $\\{y_t\\}$ alone. From equation (1), we have:\n$$\n\\alpha_t = y_t - 0.5 y_{t-1} - \\epsilon_t\n$$\nLagging this equation by one period gives:\n$$\n\\alpha_{t-1} = y_{t-1} - 0.5 y_{t-2} - \\epsilon_{t-1}\n$$\nSubstituting these expressions for $\\alpha_t$ and $\\alpha_{t-1}$ into equation (2) yields:\n$$\n(y_t - 0.5 y_{t-1} - \\epsilon_t) = (y_{t-1} - 0.5 y_{t-2} - \\epsilon_{t-1}) + u_t\n$$\nRearranging the terms, we obtain a single equation for $\\{y_t\\}$:\n$$\ny_t - 1.5 y_{t-1} + 0.5 y_{t-2} = u_t + \\epsilon_t - \\epsilon_{t-1}\n$$\nThis is an autoregressive moving average (ARMA) model for $\\{y_t\\}$. We can write it using the lag operator $L$, where $L x_t = x_{t-1}$:\n$$\n(1 - 1.5 L + 0.5 L^2) y_t = u_t + (1 - L) \\epsilon_t\n$$\nFor an autoregressive process to be stationary, the roots of its characteristic polynomial must all lie outside the unit circle. The autoregressive characteristic polynomial is $\\phi(z) = 1 - 1.5 z + 0.5 z^2$. We find its roots by solving $\\phi(z)=0$:\n$$\n0.5 z^2 - 1.5 z + 1 = 0\n$$\n$$\nz^2 - 3 z + 2 = 0\n$$\n$$\n(z-1)(z-2) = 0\n$$\nThe roots are $z_1=1$ and $z_2=2$. One of the roots, $z_1=1$, lies on the unit circle. This is a \"unit root,\" which is a definitive indicator of non-stationarity. The presence of a unit root implies that shocks to the system have a permanent effect, and the variance of the process is not constant over time. Specifically, the variance of the random walk component $\\{\\alpha_t\\}$ is $\\operatorname{Var}(\\alpha_t) = t \\sigma_u^2$ (assuming $\\alpha_0$ is a constant), which grows with time. This non-stationarity is inherited by $\\{y_t\\}$. Therefore, for $d=0$, the process $\\{\\Delta^0 y_t\\} = \\{y_t\\}$ is non-stationary.\n\nNext, let us investigate the case $d=1$. We analyze the first-differenced process, $w_t = \\Delta y_t = y_t - y_{t-1} = (1-L)y_t$.\nThe AR polynomial for $y_t$ can be factored as $\\phi(L) = (1-L)(1-0.5L)$. Substituting this into the equation for $y_t$:\n$$\n(1-L)(1-0.5L) y_t = u_t + \\epsilon_t - \\epsilon_{t-1}\n$$\nSubstituting $w_t = (1-L)y_t$, we get an equation for $\\{w_t\\}$:\n$$\n(1-0.5L) w_t = u_t + \\epsilon_t - \\epsilon_{t-1}\n$$\nThis can be written as:\n$$\nw_t = 0.5 w_{t-1} + v_t\n$$\nwhere $v_t = u_t + \\epsilon_t - \\epsilon_{t-1}$. This shows that $\\{w_t\\}$ is an AR(1) process driven by the error term $\\{v_t\\}$. For $\\{w_t\\}$ to be weakly stationary, two conditions must be met:\n1. The autoregressive coefficient must be less than $1$ in absolute value. Here, the coefficient is $0.5$, and $|0.5| < 1$, so this condition is satisfied.\n2. The driving noise process $\\{v_t\\}$ must be weakly stationary.\n\nLet's check the properties of $\\{v_t\\}$:\n- Mean: $\\mathbb{E}[v_t] = \\mathbb{E}[u_t] + \\mathbb{E}[\\epsilon_t] - \\mathbb{E}[\\epsilon_{t-1}] = 0 + 0 - 0 = 0$. The mean is constant.\n- Variance: Since $\\{u_t\\}$ and $\\{\\epsilon_t\\}$ are mutually independent i.i.d. sequences, $u_t$, $\\epsilon_t$, and $\\epsilon_{t-1}$ are mutually uncorrelated.\n$$\n\\operatorname{Var}(v_t) = \\operatorname{Var}(u_t + \\epsilon_t - \\epsilon_{t-1}) = \\operatorname{Var}(u_t) + \\operatorname{Var}(\\epsilon_t) + \\operatorname{Var}(-\\epsilon_{t-1}) = \\sigma_u^2 + \\sigma_\\epsilon^2 + (-1)^2 \\sigma_\\epsilon^2 = \\sigma_u^2 + 2\\sigma_\\epsilon^2\n$$\nThe variance is constant and finite, as $\\sigma_u^2, \\sigma_\\epsilon^2 \\in (0, \\infty)$.\n- Autocovariance: For lag $h=1$:\n$$\n\\operatorname{Cov}(v_t, v_{t-1}) = \\mathbb{E}[v_t v_{t-1}] = \\mathbb{E}[(u_t + \\epsilon_t - \\epsilon_{t-1})(u_{t-1} + \\epsilon_{t-1} - \\epsilon_{t-2})]\n$$\nExpanding this and using the independence and i.i.d. properties, the only non-zero expectation is $\\mathbb{E}[(-\\epsilon_{t-1})(\\epsilon_{t-1})] = -\\mathbb{E}[\\epsilon_{t-1}^2] = -\\sigma_\\epsilon^2$. So, $\\operatorname{Cov}(v_t, v_{t-1}) = -\\sigma_\\epsilon^2$.\nFor lag $h \\ge 2$:\n$$\n\\operatorname{Cov}(v_t, v_{t-h}) = \\mathbb{E}[(u_t + \\epsilon_t - \\epsilon_{t-1})(u_{t-h} + \\epsilon_{t-h} - \\epsilon_{t-h-1})] = 0\n$$\nas all cross terms involve variables from different time periods.\nSince the mean, variance, and autocovariances of $\\{v_t\\}$ do not depend on time $t$, $\\{v_t\\}$ is a weakly stationary process (specifically, a MA(1) process).\n\nBecause $\\{w_t\\} = \\{\\Delta y_t\\}$ is a stable AR(1) process driven by a stationary noise process $\\{v_t\\}$, $\\{w_t\\}$ itself is weakly stationary. Its unconditional mean is $\\mathbb{E}[w_t] = 0$, and its variance is constant and finite. The autocovariance function $\\operatorname{Cov}(w_t, w_{t-h})$ can be shown to depend only on the lag $h$.\nThus, for $d=1$, the process $\\{\\Delta^1 y_t\\}$ is weakly stationary.\n\nSince $\\{y_t\\}$ (for $d=0$) is non-stationary and $\\{\\Delta y_t\\}$ (for $d=1$) is stationary, the smallest non-negative integer $d$ for which $\\{\\Delta^d y_t\\}$ is weakly stationary is $1$. Any further differencing (e.g., $d=2$) would also result in a stationary process, as differencing a stationary process yields another stationary process, but $d=1$ is the minimum required order of differencing.", "answer": "$$\\boxed{1}$$"}, {"introduction": "Building upon the concept of unit roots, this exercise explores a process with a higher order of integration, which is essential for modeling variables like nominal price levels whose growth rates (inflation) may also be non-stationary. You will derive the forecasting properties of an integrated of order two, or $I(2)$, process and connect its mathematical behavior to the important economic concept of shock persistence [@problem_id:2433752]. This practice demonstrates how abstract statistical properties translate into meaningful statements about economic uncertainty and dynamics.", "id": "2433752", "problem": "In computational economics and finance, it is often assumed that the logarithm of the nominal price level, denoted by $p_t$, is an integrated process. Consider the discrete-time process for the log price level that satisfies\n$$\n\\Delta^{2} p_t \\equiv (1 - L)^{2} p_t = \\varepsilon_t,\n$$\nwhere $L$ is the lag operator, $\\Delta$ is the first-difference operator, and $\\{\\varepsilon_t\\}$ is independent and identically distributed (i.i.d.) with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^{2})$. Let $\\mathcal{F}_t$ denote the information set generated by $\\{p_s: s \\le t\\}$.\n\na) Using only the definitions of integration order and covariance stationarity, establish the integration order of $p_t$.\n\nb) Derive the $h$-step-ahead minimum mean squared error forecast $\\mathbb{E}[p_{t+h}\\mid \\mathcal{F}_t]$ and express the forecast error $p_{t+h} - \\mathbb{E}[p_{t+h}\\mid \\mathcal{F}_t]$ as a weighted sum of the future innovations $\\{\\varepsilon_{t+1},\\dots,\\varepsilon_{t+h}\\}$.\n\nc) Compute the $h$-step-ahead forecast error variance $\\operatorname{Var}\\!\\left(p_{t+h} - \\mathbb{E}[p_{t+h}\\mid \\mathcal{F}_t]\\right)$ in closed form as a function of $h$ and $\\sigma^{2}$.\n\nd) For $\\sigma^{2} = 2.5 \\times 10^{-5}$ and $h = 12$, compute the numerical value of the forecast error standard deviation of $p_{t+h}$ implied by this model. Round your answer to four significant figures. Express the final answer as a pure number in natural-log units (no unit symbols).\n\ne) Based on your derivations and without adding any new assumptions, state one economic reason why a nominal price level $p_t$ or a nominal stock index might be better modeled as integrated of order two rather than integrated of order one, focusing on the persistence properties of $\\Delta p_t$.\n\nYour final answer must be the single numerical value requested in part d).", "solution": "The problem statement will first be validated for scientific soundness, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n- The log price level, $p_t$, is a discrete-time process.\n- The process satisfies the equation $\\Delta^{2} p_t \\equiv (1 - L)^{2} p_t = \\varepsilon_t$.\n- $L$ is the lag operator.\n- $\\Delta$ is the first-difference operator.\n- $\\{\\varepsilon_t\\}$ is an independent and identically distributed (i.i.d.) sequence of random variables.\n- The distribution of the error term is normal: $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^{2})$.\n- $\\mathcal{F}_t$ is the information set generated by $\\{p_s: s \\le t\\}$.\n- For part d), specific values are given: $\\sigma^{2} = 2.5 \\times 10^{-5}$ and $h = 12$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard definitions and concepts from time series econometrics, specifically integrated processes (I(d)), autoregressive integrated moving average (ARIMA) models, minimum mean squared error (MMSE) forecasting, and forecast error variance. The model specified, $\\Delta^2 p_t = \\varepsilon_t$, is a canonical example of a process integrated of order two, often denoted as an ARIMA(0,2,0) process. All concepts are firmly based in established statistical and economic theory.\n- **Well-Posed:** The problem is well-posed. Each part asks for a specific derivation or calculation that has a unique and meaningful solution based on the provided model.\n- **Objective:** The problem is stated in precise, objective, and formal mathematical language. It is free of ambiguity and subjective claims.\n- **Other Flaws:** The problem is self-contained, consistent, and scientifically verifiable. It does not exhibit any of the flaws listed in the validation criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete, reasoned solution will be provided for all parts.\n\n**a) Integration Order of $p_t$**\nA time series process is defined as integrated of order $d$, denoted $I(d)$, if it must be differenced $d$ times to become a covariance-stationary process.\n\nThe given process is $\\Delta^{2} p_t = \\varepsilon_t$.\nThe process $\\{\\varepsilon_t\\}$ is i.i.d. with mean $\\mathbb{E}[\\varepsilon_t] = 0$ and variance $\\operatorname{Var}(\\varepsilon_t) = \\sigma^2$. The autocovariance $\\operatorname{Cov}(\\varepsilon_t, \\varepsilon_{t-k}) = 0$ for $k \\ne 0$. Since the mean and autocovariances are independent of time $t$, $\\{\\varepsilon_t\\}$ is a covariance-stationary process. As differencing the process $p_t$ twice yields a stationary process, the order of integration is at most $2$.\n\nTo establish that the order is exactly $2$, we must show that differencing once is not sufficient. Let $y_t = \\Delta p_t$. The model can be written as $\\Delta y_t = \\varepsilon_t$, which implies $y_t - y_{t-1} = \\varepsilon_t$, or $y_t = y_{t-1} + \\varepsilon_t$.\nThis is the equation for a random walk process. To check for covariance stationarity of $y_t$, we examine its first two moments. Assume the process starts at time $s=1$ from a fixed initial value $y_0$. By recursive substitution, we have:\n$$\ny_t = y_0 + \\sum_{s=1}^t \\varepsilon_s\n$$\nThe expected value of $y_t$ is $\\mathbb{E}[y_t] = \\mathbb{E}[y_0] + \\sum_{s=1}^t \\mathbb{E}[\\varepsilon_s] = y_0$, assuming $y_0$ is non-random. The mean is constant.\nThe variance of $y_t$ is:\n$$\n\\operatorname{Var}(y_t) = \\operatorname{Var}\\left(y_0 + \\sum_{s=1}^t \\varepsilon_s\\right) = \\operatorname{Var}\\left(\\sum_{s=1}^t \\varepsilon_s\\right) = \\sum_{s=1}^t \\operatorname{Var}(\\varepsilon_s) = \\sum_{s=1}^t \\sigma^2 = t\\sigma^2\n$$\nSince the variance $\\operatorname{Var}(y_t)$ depends on time $t$, the process $y_t = \\Delta p_t$ is not covariance-stationary.\nBecause $\\Delta p_t$ is non-stationary but $\\Delta^2 p_t$ is stationary, the process $p_t$ is integrated of order $2$, or $p_t \\sim I(2)$.\n\n**b) MMSE Forecast and Forecast Error**\nThe model $(1 - L)^{2} p_t = \\varepsilon_t$ can be expanded as $(1 - 2L + L^2)p_t = \\varepsilon_t$, which gives the autoregressive form:\n$$\np_t - 2p_{t-1} + p_{t-2} = \\varepsilon_t \\implies p_t = 2p_{t-1} - p_{t-2} + \\varepsilon_t\n$$\nThe $h$-step-ahead minimum mean squared error (MMSE) forecast is the conditional expectation $\\hat{p}_{t+h|t} \\equiv \\mathbb{E}[p_{t+h}\\mid \\mathcal{F}_t]$. For any $j > 0$, $\\mathbb{E}[\\varepsilon_{t+j}\\mid \\mathcal{F}_t] = 0$.\nFor $h=1$:\n$$\n\\hat{p}_{t+1|t} = \\mathbb{E}[2p_t - p_{t-1} + \\varepsilon_{t+1} \\mid \\mathcal{F}_t] = 2p_t - p_{t-1}\n$$\nFor $h > 1$, we use the law of iterated expectations:\n$$\n\\hat{p}_{t+h|t} = \\mathbb{E}[2p_{t+h-1} - p_{t+h-2} + \\varepsilon_{t+h} \\mid \\mathcal{F}_t] = 2\\mathbb{E}[p_{t+h-1} \\mid \\mathcal{F}_t] - \\mathbb{E}[p_{t+h-2} \\mid \\mathcal{F}_t] = 2\\hat{p}_{t+h-1|t} - \\hat{p}_{t+h-2|t}\n$$\nThis is a homogeneous second-order linear difference equation for the forecast $\\hat{p}_{t+h|t}$ as a function of $h$. The characteristic equation is $r^2 - 2r + 1 = 0$, or $(r-1)^2 = 0$, which has a repeated root $r=1$. The general solution is of the form $\\hat{p}_{t+h|t} = c_1 (1)^h + c_2 h (1)^h = c_1 + c_2 h$.\nThe constants $c_1$ and $c_2$ are determined by the initial conditions at the forecast origin $t$.\nFor $h=0$: $\\hat{p}_{t|t} = \\mathbb{E}[p_t \\mid \\mathcal{F}_t] = p_t$. This gives $c_1 = p_t$.\nFor $h=1$: $\\hat{p}_{t+1|t} = 2p_t - p_{t-1}$. This gives $c_1 + c_2 = p_t + c_2 = 2p_t - p_{t-1}$, so $c_2 = p_t - p_{t-1} = \\Delta p_t$.\nTherefore, the $h$-step-ahead forecast is:\n$$\n\\mathbb{E}[p_{t+h}\\mid \\mathcal{F}_t] = p_t + h(p_t - p_{t-1}) = p_t + h\\Delta p_t\n$$\nThe forecast error is $e_{t+h|t} = p_{t+h} - \\hat{p}_{t+h|t}$.\nWe can find a recursive expression for the error:\n$e_{t+h|t} = (2p_{t+h-1} - p_{t+h-2} + \\varepsilon_{t+h}) - (2\\hat{p}_{t+h-1|t} - \\hat{p}_{t+h-2|t}) = 2e_{t+h-1|t} - e_{t+h-2|t} + \\varepsilon_{t+h}$.\nThe initial conditions are $e_{t|t} = 0$ and $e_{t-1|t} = 0$.\nFor $h=1$: $e_{t+1|t} = 2e_{t|t} - e_{t-1|t} + \\varepsilon_{t+1} = \\varepsilon_{t+1}$.\nFor $h=2$: $e_{t+2|t} = 2e_{t+1|t} - e_{t|t} + \\varepsilon_{t+2} = 2\\varepsilon_{t+1} + \\varepsilon_{t+2}$.\nFor $h=3$: $e_{t+3|t} = 2e_{t+2|t} - e_{t+1|t} + \\varepsilon_{t+3} = 2(2\\varepsilon_{t+1} + \\varepsilon_{t+2}) - \\varepsilon_{t+1} + \\varepsilon_{t+3} = 3\\varepsilon_{t+1} + 2\\varepsilon_{t+2} + \\varepsilon_{t+3}$.\nBy induction, the general form of the forecast error is a weighted sum of future innovations:\n$$\np_{t+h} - \\mathbb{E}[p_{t+h}\\mid \\mathcal{F}_t] = \\sum_{j=1}^{h} (h-j+1) \\varepsilon_{t+j}\n$$\n\n**c) Forecast Error Variance**\nThe forecast error variance is $\\operatorname{Var}(e_{t+h|t})$. The innovations $\\{\\varepsilon_t\\}$ are i.i.d., so $\\operatorname{Cov}(\\varepsilon_i, \\varepsilon_j) = 0$ for $i \\ne j$ and $\\operatorname{Var}(\\varepsilon_j) = \\sigma^2$. The variance of a sum of independent variables is the sum of their variances.\n$$\n\\operatorname{Var}(e_{t+h|t}) = \\operatorname{Var}\\left(\\sum_{j=1}^{h} (h-j+1) \\varepsilon_{t+j}\\right) = \\sum_{j=1}^{h} \\operatorname{Var}\\left((h-j+1) \\varepsilon_{t+j}\\right)\n$$\n$$\n= \\sum_{j=1}^{h} (h-j+1)^2 \\operatorname{Var}(\\varepsilon_{t+j}) = \\sigma^2 \\sum_{j=1}^{h} (h-j+1)^2\n$$\nLet $k = h-j+1$. As $j$ ranges from $1$ to $h$, $k$ ranges from $h$ down to $1$. The sum is therefore the sum of the first $h$ squared integers:\n$$\n\\sum_{k=1}^{h} k^2 = \\frac{h(h+1)(2h+1)}{6}\n$$\nThus, the $h$-step-ahead forecast error variance is:\n$$\n\\operatorname{Var}\\left(p_{t+h} - \\mathbb{E}[p_{t+h}\\mid \\mathcal{F}_t]\\right) = \\frac{h(h+1)(2h+1)}{6} \\sigma^2\n$$\n\n**d) Numerical Calculation**\nWe are given $\\sigma^2 = 2.5 \\times 10^{-5}$ and $h=12$. We must compute the forecast error standard deviation, which is the square root of the variance avaluated at these parameters.\nFirst, compute the variance:\n$$\n\\operatorname{Var}(e_{t+12|t}) = \\frac{12(12+1)(2 \\cdot 12 + 1)}{6} \\times (2.5 \\times 10^{-5})\n$$\n$$\n= \\frac{12 \\cdot 13 \\cdot 25}{6} \\times (2.5 \\times 10^{-5})\n$$\n$$\n= (2 \\cdot 13 \\cdot 25) \\times (2.5 \\times 10^{-5})\n$$\n$$\n= 650 \\times (2.5 \\times 10^{-5}) = 1625 \\times 10^{-5} = 0.01625\n$$\nThe forecast error standard deviation is the square root of this value:\n$$\n\\text{Std. Dev.} = \\sqrt{0.01625} \\approx 0.12747549\n$$\nRounding to four significant figures, we get $0.1275$.\n\n**e) Economic Interpretation**\nFor a standard $I(1)$ process, such as a random walk with drift $p_t = \\mu + p_{t-1} + u_t$, the first difference $\\Delta p_t = \\mu + u_t$ is stationary. If $p_t$ is the log price level, $\\Delta p_t$ is the rate of inflation. Stationarity implies that shocks to inflation ($u_t$) are temporary, and the inflation rate reverts to its long-run mean $\\mu$.\nIn contrast, for the $I(2)$ process given, $p_t \\sim I(2)$, we established that $\\Delta p_t$ is itself a non-stationary $I(1)$ process (a random walk). This means that a shock to the *change* in inflation, $\\Delta(\\Delta p_t) = \\varepsilon_t$, has a permanent effect on the *level* of inflation, $\\Delta p_t$. The inflation rate does not revert to a constant mean; instead, it follows a path where its future value is its current value plus a random shock.\nThis high degree of persistence captured by an $I(2)$ model can be more realistic for nominal price levels or stock indices under certain economic conditions. For example, in an economy with unanchored inflation expectations or during periods of hyperinflation, shocks to inflation (e.g., from policy mistakes, supply shocks) are not quickly corrected and can permanently alter the future trajectory of inflation. The inflation rate itself wanders without a stable anchor. Similarly, for a stock index, an $I(2)$ model suggests that the growth rate of the index is itself a random walk, implying that shocks (e.g., major technological innovations, regulatory changes) can permanently shift market expectations about future growth, rather than causing temporary deviations from a stable growth trend. The $I(2)$ property thus models a form of \"hysteresis\" or extreme persistence in the rate of change of the series.", "answer": "$$\\boxed{0.1275}$$"}, {"introduction": "Real-world financial series often exhibit behavior that changes over time, challenging the assumption of a single, fixed data-generating process. This final hands-on practice moves beyond linear models to simulate a Threshold Autoregressive (TAR) model, where the series switches between a stationary and a non-stationary unit root regime based on market volatility [@problem_id:2433671]. Implementing this model provides direct experience with the powerful idea of non-linear, state-dependent dynamics, a cornerstone of modern financial econometrics.", "id": "2433671", "problem": "Consider a discrete-time financial series defined by a Threshold Autoregressive (TAR) model with two regimes that depend on a volatility threshold. Let $\\{y_t\\}_{t=1}^T$ denote the series, with pre-sample values $y_{-w+1}=\\cdots=y_{-1}=y_0=0$. For $t \\geq 1$, define the rolling volatility estimator based on the last $w$ values as\n$$\n\\bar{y}_{t-1,w}=\\frac{1}{w}\\sum_{i=1}^{w} y_{t-i},\\qquad\nv_{t-1}=\\sqrt{\\frac{1}{w}\\sum_{i=1}^{w}\\left(y_{t-i}-\\bar{y}_{t-1,w}\\right)^2}.\n$$\nLet the threshold be $\\tau \\ge 0$. The process switches regimes as follows:\n- If $v_{t-1} \\le \\tau$, then $y_t = c_s + \\phi_s y_{t-1} + \\varepsilon_t$ (stationary regime).\n- If $v_{t-1} > \\tau$, then $y_t = c_u + 1 \\cdot y_{t-1} + \\varepsilon_t$ (unit root regime).\n\nAssume the innovations $\\{\\varepsilon_t\\}$ are independent and identically distributed (i.i.d.) normal with mean $0$ and variance $\\sigma^2$. For reproducibility, the sequence of innovations is generated under a fixed seed $s$ for each parameter set.\n\nYour task is to implement the above model exactly as stated and, for each parameter set in the test suite below, compute the following three quantities:\n- The proportion of times the unit root regime is active:\n$$\n\\hat{p}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{1}\\{v_{t-1}>\\tau\\},\n$$\nexpressed as a decimal rounded to six digits after the decimal point.\n- The finite-sample variance of $\\{y_t\\}_{t=1}^T$:\n$$\n\\hat{V}=\\frac{1}{T}\\sum_{t=1}^{T}\\left(y_t - \\bar{y}\\right)^2,\\quad \\text{where}\\quad \\bar{y}=\\frac{1}{T}\\sum_{t=1}^{T}y_t,\n$$\nrounded to six digits after the decimal point.\n- The final value $y_T$, rounded to six digits after the decimal point.\n\nAll quantities are dimensionless. Angles do not appear. All proportions must be expressed as decimals.\n\nTest Suite (each tuple lists $(T,w,\\tau,\\phi_s,c_s,c_u,\\sigma,s)$):\n- Case $1$: $(200,\\,10,\\,0.8,\\,0.6,\\,0.0,\\,0.0,\\,1.0,\\,20240229)$.\n- Case $2$ (boundary emphasis at start with $\\tau=0$): $(200,\\,10,\\,0.0,\\,0.6,\\,0.0,\\,0.0,\\,1.0,\\,7)$.\n- Case $3$ (effectively always stationary due to high threshold): $(200,\\,10,\\,5.0,\\,0.6,\\,0.0,\\,0.0,\\,1.0,\\,12345)$.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of three sublists in the order given above. Each sublist must contain the three rounded values $[\\hat{p},\\hat{V},y_T]$ for the corresponding case. The exact format is:\n[[p1,V1,yT1],[p2,V2,yT2],[p3,V3,yT3]]", "solution": "The model is a two-regime Threshold Autoregressive (TAR) process. The regimes are triggered by a threshold $\\tau$ applied to a volatility estimator. We formalize the problem by specifying the recursion, the volatility measure, and the summary statistics to be computed.\n\nFirst, we define the rolling volatility estimator using the last $w$ past values. For each $t \\ge 1$, we compute\n$$\n\\bar{y}_{t-1,w}=\\frac{1}{w}\\sum_{i=1}^{w} y_{t-i},\\qquad\nv_{t-1}=\\sqrt{\\frac{1}{w}\\sum_{i=1}^{w}\\left(y_{t-i}-\\bar{y}_{t-1,w}\\right)^2}.\n$$\nBecause we set pre-sample values $y_{-w+1}=\\cdots=y_0=0$, the volatility estimator starts at $v_0=0$ and evolves as the simulated path departs from zero.\n\nSecond, we define the regime-dependent recursion:\n- Stationary regime (low volatility): if $v_{t-1}\\le \\tau$, then\n$$\ny_t = c_s + \\phi_s y_{t-1} + \\varepsilon_t,\n$$\nwhere $|\\phi_s|<1$ ensures covariance stationarity of the $AR(1)$ component in this regime. In our test suite, $\\phi_s=0.6$ satisfies $|\\phi_s|<1$.\n- Unit root regime (high volatility): if $v_{t-1} > \\tau$, then\n$$\ny_t = c_u + 1\\cdot y_{t-1} + \\varepsilon_t,\n$$\nwhich is a random walk with drift $c_u$; this regime is non-stationary due to the unit root.\n\nThe innovations satisfy $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, independently across $t$. To make the output deterministic and testable, for each parameter set we use a fixed seed $s$ to initialize the pseudo-random generator, thereby fixing the realization $\\{\\varepsilon_t\\}_{t=1}^T$.\n\nGiven $(T,w,\\tau,\\phi_s,c_s,c_u,\\sigma,s)$, we proceed for $t=1,\\dots,T$:\n- Compute $v_{t-1}$ from the last $w$ values $y_{t-1},y_{t-2},\\dots,y_{t-w}$.\n- Select the regime based on whether $v_{t-1}\\le \\tau$ or $v_{t-1}>\\tau$ and update $y_t$ accordingly.\n\nAfter simulating $\\{y_t\\}_{t=1}^T$, we compute the required summary statistics:\n- The proportion of times the unit root regime is active:\n$$\n\\hat{p}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{1}\\{v_{t-1}>\\tau\\}.\n$$\n- The finite-sample variance (using divisor $T$):\n$$\n\\bar{y}=\\frac{1}{T}\\sum_{t=1}^{T}y_t,\\qquad\n\\hat{V}=\\frac{1}{T}\\sum_{t=1}^{T}\\left(y_t - \\bar{y}\\right)^2.\n$$\n- The terminal value $y_T$.\n\nFinally, we round each of $\\hat{p}$, $\\hat{V}$, and $y_T$ to six digits after the decimal point. We repeat this computation for each of the three test cases and aggregate the three triplets $[\\hat{p},\\hat{V},y_T]$ into a single line of output of the form:\n$$\n[[p_1,V_1,yT_1],[p_2,V_2,yT_2],[p_3,V_3,yT_3]].\n$$\n\nFrom a stationarity perspective, within the low-volatility regime, the process behaves like a stable $AR(1)$ as $|\\phi_s|<1$, implying a well-defined unconditional variance approximately $\\sigma^2/(1-\\phi_s^2)$ if the regime were permanent. In the high-volatility regime, the unit root implies that shocks accumulate without mean reversion, making the path non-stationary during those periods. The TAR mechanism is non-linear and path-dependent: the realized volatility determines regime selection, and regime dynamics determine future volatility. The requested summary statistics capture the empirical frequency of non-stationary behavior via $\\hat{p}$, the dispersion of the realized path via $\\hat{V}$, and the terminal level $y_T$ reflecting cumulative dynamics. The boundary case with $\\tau=0$ ensures that at $t=1$, the process starts in the stationary regime because $v_0=0\\le 0$, but any subsequent positive volatility will immediately allow switching to the unit root regime whenever $v_{t-1}>0$.", "answer": "```python\nimport numpy as np\n\ndef simulate_tar(T, w, tau, phi_s, c_s, c_u, sigma, seed):\n    \"\"\"\n    Simulate the TAR process:\n      y_t = c_s + phi_s * y_{t-1} + eps_t, if v_{t-1} <= tau\n      y_t = c_u + 1.0   * y_{t-1} + eps_t, if v_{t-1} >  tau\n    where v_{t-1} is the population standard deviation (dividing by w) of the last w values.\n    Pre-sample values y_{-w+1} = ... = y_0 = 0.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    eps = rng.normal(loc=0.0, scale=sigma, size=T)\n\n    # Maintain a window of last w observations; initialize with zeros\n    window = [0.0] * w\n    y_values = []  # store y_1 ... y_T\n    unit_root_count = 0\n\n    for t in range(T):\n        # Compute rolling volatility v_{t-1}: std with divisor w\n        mean_window = sum(window) / w\n        var_window = sum((val - mean_window) ** 2 for val in window) / w\n        v_prev = var_window ** 0.5\n\n        y_prev = window[-1]\n        if v_prev <= tau:\n            y_t = c_s + phi_s * y_prev + eps[t]\n        else:\n            y_t = c_u + 1.0 * y_prev + eps[t]\n            unit_root_count += 1\n\n        # Update structures\n        y_values.append(y_t)\n        window.pop(0)\n        window.append(y_t)\n\n    # Compute summary statistics\n    y_arr = np.array(y_values, dtype=float)\n    mean_y = float(np.sum(y_arr) / T)\n    var_y = float(np.sum((y_arr - mean_y) ** 2) / T)\n    frac_unit_root = unit_root_count / T\n    y_T = float(y_arr[-1])\n\n    return frac_unit_root, var_y, y_T\n\ndef fmt_float(x):\n    y = round(float(x), 6)\n    # Normalize negative zero to zero for cleaner output\n    if y == 0.0:\n        y = 0.0\n    return f\"{y:.6f}\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (T, w, tau, phi_s, c_s, c_u, sigma, seed)\n    test_cases = [\n        (200, 10, 0.8, 0.6, 0.0, 0.0, 1.0, 20240229),\n        (200, 10, 0.0, 0.6, 0.0, 0.0, 1.0, 7),\n        (200, 10, 5.0, 0.6, 0.0, 0.0, 1.0, 12345),\n    ]\n\n    results = []\n    for case in test_cases:\n        T, w, tau, phi_s, c_s, c_u, sigma, seed = case\n        frac_unit_root, var_y, y_T = simulate_tar(T, w, tau, phi_s, c_s, c_u, sigma, seed)\n        results.append([frac_unit_root, var_y, y_T])\n\n    # Format output as a single line: [[p1,V1,yT1],[p2,V2,yT2],[p3,V3,yT3]]\n    formatted_groups = []\n    for triplet in results:\n        formatted = \"[\" + \",\".join(fmt_float(x) for x in triplet) + \"]\"\n        formatted_groups.append(formatted)\n    output_line = \"[\" + \",\".join(formatted_groups) + \"]\"\n    print(output_line)\n\nif __name__ == \"__main__\":\n    solve()\n```"}]}