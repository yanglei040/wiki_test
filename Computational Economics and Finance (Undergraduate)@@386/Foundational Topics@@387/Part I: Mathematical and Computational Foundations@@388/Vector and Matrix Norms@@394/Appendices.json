{"hands_on_practices": [{"introduction": "The choice of a \"distance\" measure is fundamental in algorithms across machine learning and economic modeling. This practice explores how different vector norms—specifically the $L_1$, $L_2$, and $L_\\infty$ norms—create distinct notions of distance, leading to different outcomes in a clustering task [@problem_id:2447279]. By applying these norms to a hypothetical dataset of companies, you will gain a hands-on understanding of their geometric properties and practical implications for data analysis.", "id": "2447279", "problem": "A set of companies is represented by vectors of three financial ratios per company, with each vector in $\\mathbb{R}^3$. Consider three vector norms on $\\mathbb{R}^3$: the $L_1$ norm defined by $\\lVert x \\rVert_1 = \\sum_{j=1}^3 \\lvert x_j \\rvert$, the $L_2$ norm defined by $\\lVert x \\rVert_2 = \\left(\\sum_{j=1}^3 x_j^2\\right)^{1/2}$, and the $L_\\infty$ norm defined by $\\lVert x \\rVert_\\infty = \\max\\{\\lvert x_1 \\rvert, \\lvert x_2 \\rvert, \\lvert x_3 \\rvert\\}$. For any two vectors $x,y \\in \\mathbb{R}^3$, define the distance under norm $p \\in \\{1,2,\\infty\\}$ by $d_p(x,y) = \\lVert x-y \\rVert_p$.\n\nYou must implement the following clustering procedure, parameterized by the choice of norm $p \\in \\{1,2,\\infty\\}$:\n- The number of clusters is $K=2$.\n- Initialize cluster centroids $\\mu_1^{(0)}, \\mu_2^{(0)} \\in \\mathbb{R}^3$ as specified for each dataset below.\n- At iteration $t \\in \\{0,1,2,\\dots\\}$, perform the assignment step: for each data vector $x_i$, assign it to the cluster index $k^\\star \\in \\{1,2\\}$ minimizing $d_p(x_i,\\mu_{k}^{(t)})$. If there is a tie, select the smallest cluster index $k^\\star$ achieving the minimum.\n- Perform the update step: for each cluster $k \\in \\{1,2\\}$, if the set of assigned vectors to cluster $k$ at iteration $t$ is nonempty, set $\\mu_k^{(t+1)}$ to the arithmetic mean of those assigned vectors; if it is empty, set $\\mu_k^{(t+1)} = \\mu_k^{(t)}$.\n- Stop when an iteration produces no change in any assignment compared to the previous iteration, or after $T=10$ iterations, whichever occurs first.\n\nAll quantities are unitless ratios; no physical units apply. Angles are not involved. Percentages do not appear.\n\nYour task is to run the above procedure on the datasets and norms listed in the test suite below and report, for each test case, the final cluster assignment vector as a list of integers in $\\{1,2\\}$, in the same order as the companies are listed. Cluster labels must be $1$-indexed as specified. The arithmetic mean is the componentwise mean in $\\mathbb{R}^3$.\n\nTest suite:\n- Dataset A (happy path; well-separated clusters):\n  - Companies (in order): $x_1=(0.9,1.1,1.0)$, $x_2=(1.2,0.9,1.1)$, $x_3=(1.0,1.2,0.8)$, $x_4=(8.1,7.9,8.0)$, $x_5=(7.8,8.2,8.1)$, $x_6=(8.0,8.1,7.9)$.\n  - Initial centroids: $\\mu_1^{(0)}=(1.0,1.0,1.0)$, $\\mu_2^{(0)}=(8.0,8.0,8.0)$.\n  - Norms to test: $p \\in \\{2,1,\\infty\\}$, producing three distinct test cases.\n- Dataset B (boundary condition; equidistant point requiring tie-breaking):\n  - Companies (in order): $x_1=(0.0,0.0,0.0)$, $x_2=(2.0,0.0,0.0)$, $x_3=(1.0,0.0,0.0)$.\n  - Initial centroids: $\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(2.0,0.0,0.0)$.\n  - Norms to test: $p \\in \\{2\\}$, producing one test case.\n- Dataset C (edge case; anisotropy and an outlier affecting norms differently):\n  - Companies (in order): $x_1=(0.1,0.1,0.1)$, $x_2=(0.0,0.0,0.0)$, $x_3=(0.2,0.1,0.0)$, $x_4=(4.2,0.1,0.0)$, $x_5=(3.8,-0.1,0.0)$, $x_6=(3.0,0.0,5.0)$.\n  - Initial centroids: $\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(4.0,0.0,0.0)$.\n  - Norms to test: $p \\in \\{2,\\infty,1\\}$, producing three distinct test cases.\n\nTherefore, there are $7$ test cases total, in this order:\n$($A with $p=2)$, $($A with $p=1)$, $($A with $p=\\infty)$, $($B with $p=2)$, $($C with $p=2)$, $($C with $p=\\infty)$, $($C with $p=1)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of integers indicating the final cluster labels for the corresponding test case, in the order specified above. For example, an output with two test cases might look like $[[1,1,2],[2,1]]$.", "solution": "The problem presented is a well-posed and computationally tractable task. It requires the implementation of a variant of the K-means clustering algorithm, a fundamental procedure in unsupervised machine learning and data analysis. The problem is scientifically grounded, relying on standard definitions of vector norms ($L_1$, $L_2$, and $L_\\infty$) and the arithmetic mean. All parameters, initial conditions, data, and procedural rules—including termination and tie-breaking—are specified with sufficient precision to guarantee a unique, deterministic outcome. The context of applying these methods to vectors of financial ratios is a standard application within computational economics and finance, where different norms can represent distinct concepts of 'distance' or 'dissimilarity' between economic entities. For example, the $L_2$ norm corresponds to the familiar Euclidean distance, the $L_1$ norm (or 'Manhattan distance') sums the absolute differences across ratios, and the $L_\\infty$ norm (or 'Chebyshev distance') is determined by the single largest difference in any ratio, which can be relevant for worst-case analysis. The choice of norm fundamentally alters the geometry of the space, thereby influencing the shape of cluster boundaries and, consequently, the final assignments. This problem is therefore a valid and instructive exercise in understanding the practical implications of theoretical norms.\n\nThe algorithm to be implemented is an iterative procedure defined as follows. Given a set of data vectors $\\{x_i\\}_{i=1}^N$ in $\\mathbb{R}^3$, an initial set of $K=2$ centroids $\\{\\mu_1^{(0)}, \\mu_2^{(0)}\\}$, and a chosen norm $p \\in \\{1, 2, \\infty\\}$, we proceed through iterations indexed by $t=0, 1, 2, \\dots$. Each iteration consists of two steps.\n\nFirst, the **Assignment Step**: Each data vector $x_i$ is assigned to a cluster $k^\\star \\in \\{1, 2\\}$. The assignment is made by finding the cluster centroid $\\mu_k^{(t)}$ that is closest to $x_i$ as measured by the distance $d_p(x_i, \\mu_k^{(t)}) = \\lVert x_i - \\mu_k^{(t)} \\rVert_p$. The cluster index $k^\\star$ is thus given by:\n$$\nk^\\star = \\underset{k \\in \\{1,2\\}}{\\arg\\min} \\lVert x_i - \\mu_k^{(t)} \\rVert_p\n$$\nThe problem specifies a tie-breaking rule: if distances to both centroids are equal, the point is assigned to cluster $1$. Let $C_k^{(t)}$ be the set of indices of data vectors assigned to cluster $k$ at iteration $t$.\n\nSecond, the **Update Step**: The centroids for the next iteration, $\\mu_k^{(t+1)}$, are recalculated. For each cluster $k$, if the set of assigned vectors is non-empty (i.e., $C_k^{(t)} \\neq \\emptyset$), the new centroid is the component-wise arithmetic mean of all vectors assigned to that cluster:\n$$\n\\mu_k^{(t+1)} = \\frac{1}{|C_k^{(t)}|} \\sum_{i \\in C_k^{(t)}} x_i\n$$\nIf a cluster becomes empty ($C_k^{(t)} = \\emptyset$), its centroid is not updated: $\\mu_k^{(t+1)} = \\mu_k^{(t)}$.\n\nThe process **Terminates** under one of two conditions: either the cluster assignments at iteration $t$ are identical to the assignments at iteration $t-1$, indicating convergence, or a maximum of $T=10$ iterations have been completed.\n\nThe solution will be produced by a program that systematically executes this algorithm for each of the seven specified test cases, which combine three datasets (A, B, C) with different norms ($p=2, 1, \\infty$). The program will compute the final cluster assignments for each case and format the output according to the specified requirements. The use of NumPy is appropriate for efficient and accurate vector and matrix operations, including the calculation of norms and means.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the clustering problem for all test cases as specified.\n    \"\"\"\n\n    def get_norm_func(p):\n        \"\"\"Returns a function to compute the distance based on the specified norm.\"\"\"\n        if p == 1:\n            return lambda a, b: np.linalg.norm(a - b, ord=1)\n        elif p == 2:\n            return lambda a, b: np.linalg.norm(a - b, ord=2)\n        elif p == 'inf':\n            return lambda a, b: np.linalg.norm(a - b, ord=np.inf)\n        else:\n            raise ValueError(\"Unsupported norm type.\")\n\n    def run_clustering(data, initial_centroids, p, max_iter=10):\n        \"\"\"\n        Runs the specified k-means variant clustering algorithm.\n\n        Args:\n            data (np.ndarray): The dataset of points (N, D).\n            initial_centroids (np.ndarray): The initial cluster centroids (K, D).\n            p (int or 'inf'): The norm to use for distance calculation (1, 2, or 'inf').\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            list: A list of final cluster assignments (1-indexed).\n        \"\"\"\n        num_points = data.shape[0]\n        num_clusters = initial_centroids.shape[0]\n        \n        centroids = np.copy(initial_centroids)\n        assignments = np.zeros(num_points, dtype=int)\n        distance_func = get_norm_func(p)\n\n        for _ in range(max_iter):\n            old_assignments = np.copy(assignments)\n\n            # Assignment step\n            for i in range(num_points):\n                point = data[i]\n                \n                # Use a small epsilon for floating point comparisons to be robust,\n                # but the problem rule is a direct tie-break.\n                # d1 <= d2 is used to handle ties, assigning to cluster 1.\n                dist_to_c1 = distance_func(point, centroids[0])\n                dist_to_c2 = distance_func(point, centroids[1])\n                \n                if dist_to_c1 <= dist_to_c2:\n                    assignments[i] = 1\n                else:\n                    assignments[i] = 2\n\n            # Termination check\n            if np.array_equal(assignments, old_assignments):\n                break\n\n            # Update step\n            new_centroids = np.copy(centroids)\n            for k in range(1, num_clusters + 1):\n                # Get all points assigned to cluster k\n                cluster_points = data[assignments == k]\n                \n                if cluster_points.shape[0] > 0:\n                    # Update centroid to the mean of the assigned points\n                    new_centroids[k-1] = np.mean(cluster_points, axis=0)\n                # Else: centroid remains unchanged, as handled by copying `centroids` beforehand.\n            \n            centroids = new_centroids\n\n        return assignments.tolist()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 1,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [1.0, 0.0, 0.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 1,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_clustering(case[\"data\"], case[\"centroids\"], case[\"norm\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified, with no spaces.\n    results_str_list = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output_str = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"}, {"introduction": "While vector norms measure the magnitude of vectors, matrix norms quantify the \"size\" of a linear transformation, particularly how much it can amplify a vector's length. This exercise challenges you to implement the power iteration method to estimate the induced 2-norm, $\\lVert A \\rVert_2$, from first principles [@problem_id:2449590]. This practice illuminates the deep connection between a matrix $A$, its transpose $A^\\top$, and its largest singular value, providing a cornerstone skill for numerical analysis.", "id": "2449590", "problem": "You are asked to design and implement a deterministic program that estimates the induced matrix $2$-norm $\\lVert A \\rVert_2$ for real matrices $A \\in \\mathbb{R}^{m \\times n}$ using a power-iteration-based algorithm, starting only from fundamental definitions of vector and matrix norms and basic properties of eigenvalues of symmetric matrices. Your goal is to derive, justify, and code an algorithm that does not rely on explicitly forming any matrix product beyond standard matrix-vector multiplication, and that works for both square and rectangular matrices.\n\nTasks to complete:\n1. Starting from the core definition of the induced matrix $2$-norm $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$ and the fact that for any real matrix $A$ the matrix $A^\\top A$ is symmetric positive semidefinite, derive an iterative scheme that estimates $\\lVert A \\rVert_2$ by repeatedly applying matrix-vector multiplications of the form $A \\mathbf{x}$ and $A^\\top \\mathbf{y}$ without explicitly forming $A^\\top A$. Your derivation must be grounded in these definitions and properties and should include a clear stopping criterion based on the change of the estimate.\n2. Implement the derived algorithm as a complete, runnable program. The algorithm must:\n   - Initialize with a deterministic nonzero vector in $\\mathbb{R}^n$, normalize it in the $\\ell_2$ sense, and at each iteration use only the operations $A \\mathbf{x}$ and $A^\\top \\mathbf{y}$.\n   - Terminate when the relative change in the estimated norm between successive iterations is below a tolerance $\\varepsilon = 10^{-10}$, or when a maximum of $10^4$ iterations is reached, whichever occurs first.\n   - Handle the edge case $A = 0$ robustly, yielding the estimate $\\lVert A \\rVert_2 = 0$.\n   - Return a nonnegative estimate of $\\lVert A \\rVert_2$.\n3. Your program must evaluate the following test suite of matrices and report the estimated norms in the specified format:\n   - Case $1$ (square, symmetric positive definite):\n     $$A_1 = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}.$$\n   - Case $2$ (square, highly non-normal):\n     $$A_2 = \\begin{bmatrix} 1 & 10 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0.1 \\end{bmatrix}.$$\n   - Case $3$ (tall rectangular):\n     $$A_3 = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\\\ 2 & 0 \\\\ 0 & 0 \\end{bmatrix}.$$\n   - Case $4$ (wide rectangular):\n     $$A_4 = \\begin{bmatrix} 1 & 0 & 2 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}.$$\n   - Case $5$ (zero matrix):\n     $$A_5 = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}.$$\n   - Case $6$ (square, nearly multiple dominant singular values):\n     $$A_6 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0.999 \\end{bmatrix}.$$\n4. Final output format: Your program should produce a single line of output containing the results as a comma-separated list of the six estimated norms, in the order $A_1$ through $A_6$, enclosed in square brackets. Each value must be rounded to $8$ decimal places. For example, a valid output format is\n   $$[\\text{v}_1,\\text{v}_2,\\text{v}_3,\\text{v}_4,\\text{v}_5,\\text{v}_6],$$\n   where each $\\text{v}_i$ is a decimal rounded to $8$ places. No additional text or lines should be printed.\n\nImplementation constraints:\n- The program must be fully self-contained, require no user input, and use only the Python standard library and permitted libraries.\n- Angles are not used in this problem.\n- No physical units are involved.", "solution": "The problem requires the derivation and implementation of an iterative algorithm to estimate the induced matrix $2$-norm, $\\lVert A \\rVert_2$, for a real matrix $A \\in \\mathbb{R}^{m \\times n}$. The derivation must be based on first principles and avoid the explicit formation of matrix products such as $A^\\top A$.\n\nThe validation of the problem statement is performed first.\n\n**Step 1: Extract Givens**\n- **Definition**: The induced matrix $2$-norm is defined as $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$.\n- **Property**: For any real matrix $A$, the matrix $A^\\top A$ is symmetric positive semidefinite.\n- **Algorithm Goal**: Derive an iterative scheme using only matrix-vector multiplications of the form $A \\mathbf{x}$ and $A^\\top \\mathbf{y}$ to estimate $\\lVert A \\rVert_2$.\n- **Implementation Constraints**:\n  - **Initialization**: Use a deterministic, normalized, nonzero vector in $\\mathbb{R}^n$.\n  - **Stopping Criteria**: Terminate when the relative change in the norm estimate is less than a tolerance $\\varepsilon = 10^{-10}$, or after a maximum of $10^4$ iterations.\n  - **Edge Case**: Handle the zero matrix $A = 0$ correctly, yielding an estimate of $0$.\n  - **Return Value**: The function must return a nonnegative estimate of $\\lVert A \\rVert_2$.\n- **Test Suite**: Six matrices ($A_1$ to $A_6$) are provided for evaluation.\n- **Output Format**: A single line with a comma-separated list of the six estimated norms, rounded to $8$ decimal places, enclosed in square brackets.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientifically Grounded**: The problem is based on the standard definition of the induced $2$-norm and its fundamental relationship with the largest eigenvalue of $A^\\top A$. The proposed method, power iteration, is a classical and scientifically sound algorithm in numerical linear algebra for finding the dominant eigenvalue. The problem is firmly grounded in established mathematical principles.\n2.  **Well-Posed**: The problem is well-posed. It asks for an estimate of a uniquely defined mathematical quantity ($\\lVert A \\rVert_2$). The algorithm's termination is guaranteed by the maximum iteration limit and the convergence criterion.\n3.  **Objective**: The problem is stated in precise, objective mathematical language, free of ambiguity or subjective claims.\n4.  **Flaw Analysis**:\n    - **Scientific or Factual Unsoundness**: None. The premises are correct.\n    - **Non-Formalizable or Irrelevant**: None. The problem is a standard task in computational engineering and numerical analysis, directly concerning vector and matrix norms.\n    - **Incomplete or Contradictory Setup**: None. All necessary components are specified: the goal, the constraints on the method, termination criteria, and test cases.\n    - **Unrealistic or Infeasible**: None. The algorithm is practical, and the test matrices are standard examples.\n    - **Ill-Posed or Poorly Structured**: None. The structure is clear, guiding from theoretical derivation to implementation.\n    - **Outside Scientific Verifiability**: None. The correctness of the algorithm and the accuracy of its results can be verified against known analytical solutions or standard library functions (e.g., singular value decomposition).\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A complete, reasoned solution will be provided.\n\n**Derivation and Algorithmic Design**\n\nThe starting point is the definition of the induced matrix $2$-norm for a matrix $A \\in \\mathbb{R}^{m \\times n}$:\n$$\n\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}\n$$\nSince the norm is always non-negative, we can consider its square:\n$$\n\\lVert A \\rVert_2^2 = \\left( \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2} \\right)^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2^2}{\\lVert \\mathbf{x} \\rVert_2^2}\n$$\nUsing the definition of the Euclidean norm, $\\lVert \\mathbf{v} \\rVert_2^2 = \\mathbf{v}^\\top \\mathbf{v}$, we can rewrite the expression as:\n$$\n\\lVert A \\rVert_2^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{(A \\mathbf{x})^\\top (A \\mathbf{x})}{\\mathbf{x}^\\top \\mathbf{x}} = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\mathbf{x}^\\top A^\\top A \\mathbf{x}}{\\mathbf{x}^\\top \\mathbf{x}}\n$$\nThis expression is the Rayleigh quotient for the matrix $B = A^\\top A$. A fundamental theorem in linear algebra states that the supremum of the Rayleigh quotient for a symmetric matrix is its largest eigenvalue, $\\lambda_{\\text{max}}$. The matrix $B = A^\\top A$ is indeed symmetric (since $(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A$) and positive semidefinite. Therefore, we have the crucial relationship:\n$$\n\\lVert A \\rVert_2^2 = \\lambda_{\\text{max}}(A^\\top A)\n$$\nThis implies that the induced matrix $2$-norm is the square root of the largest eigenvalue of $A^\\top A$:\n$$\n\\lVert A \\rVert_2 = \\sqrt{\\lambda_{\\text{max}}(A^\\top A)}\n$$\nThe value $\\sqrt{\\lambda_{\\text{max}}(A^\\top A)}$ is also, by definition, the largest singular value of $A$, denoted $\\sigma_1(A)$.\n\nThe problem now reduces to finding $\\lambda_{\\text{max}}(A^\\top A)$ without explicitly computing the matrix $A^\\top A$. This can be achieved using the **power iteration** method. The power method is an iterative algorithm to find the eigenvalue with the largest magnitude (the dominant eigenvalue) and its corresponding eigenvector. For a symmetric positive semidefinite matrix like $A^\\top A$, all eigenvalues are real and non-negative, so the largest magnitude eigenvalue is simply $\\lambda_{\\text{max}}$.\n\nThe standard power iteration for a matrix $B$ is:\n1.  Start with a non-zero vector $\\mathbf{v}_0$.\n2.  Iterate for $k = 1, 2, \\dots$: $\\mathbf{v}_k = \\frac{B \\mathbf{v}_{k-1}}{\\lVert B \\mathbf{v}_{k-1} \\rVert_2}$.\nThe sequence of vectors $\\{\\mathbf{v}_k\\}$ converges to the eigenvector corresponding to $\\lambda_{\\text{max}}(B)$, provided the initial vector $\\mathbf{v}_0$ has a non-zero component in the direction of this eigenvector.\n\nIn our case, $B = A^\\top A$. The iterative step is $\\mathbf{v}_k \\propto (A^\\top A) \\mathbf{v}_{k-1}$. As required, we avoid forming $A^\\top A$ by performing the multiplication as two sequential matrix-vector products:\n1.  First, compute $\\mathbf{y}_{k-1} = A \\mathbf{v}_{k-1}$.\n2.  Then, compute $\\mathbf{x}_k = A^\\top \\mathbf{y}_{k-1}$.\nSo, the core update is $\\mathbf{x}_k = A^\\top (A \\mathbf{v}_{k-1})$. The next normalized vector is $\\mathbf{v}_k = \\mathbf{x}_k / \\lVert \\mathbf{x}_k \\rVert_2$.\n\nWe also need an estimate for $\\lambda_{\\text{max}}(A^\\top A)$ at each iteration. This can be obtained from the Rayleigh quotient using the current eigenvector estimate $\\mathbf{v}_{k-1}$:\n$$\n\\lambda_k \\approx \\frac{\\mathbf{v}_{k-1}^\\top (A^\\top A) \\mathbf{v}_{k-1}}{\\mathbf{v}_{k-1}^\\top \\mathbf{v}_{k-1}}\n$$\nSince $\\mathbf{v}_{k-1}$ is a unit vector ($\\lVert \\mathbf{v}_{k-1} \\rVert_2 = 1$), its denominator is $1$. The numerator becomes:\n$$\n\\mathbf{v}_{k-1}^\\top A^\\top A \\mathbf{v}_{k-1} = (A \\mathbf{v}_{k-1})^\\top (A \\mathbf{v}_{k-1}) = \\mathbf{y}_{k-1}^\\top \\mathbf{y}_{k-1} = \\lVert \\mathbf{y}_{k-1} \\rVert_2^2\n$$\nSo, the estimate for the largest eigenvalue at iteration $k$ is $\\lambda_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2^2$.\nConsequently, the estimate for the matrix $2$-norm, $\\sigma_k = \\sqrt{\\lambda_k}$, is simply:\n$$\n\\sigma_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2 = \\lVert \\mathbf{y}_{k-1} \\rVert_2\n$$\nThis provides a simple and efficient way to update the norm estimate at each iteration.\n\n**Final Algorithm:**\nLet $A$ be an $m \\times n$ matrix. Let $\\varepsilon = 10^{-10}$ be the tolerance and $K_{\\text{max}} = 10^4$ be the maximum number of iterations.\n\n1.  **Handle trivial cases**: If $n=0$, the domain is empty, so $\\lVert A \\rVert_2 = 0$.\n2.  **Initialization**:\n    - Choose a deterministic non-zero starting vector $\\mathbf{v}_0 \\in \\mathbb{R}^n$. A standard choice is a vector of ones.\n    - Normalize it: $\\mathbf{v} \\leftarrow \\frac{\\mathbf{v}_0}{\\lVert \\mathbf{v}_0 \\rVert_2}$.\n    - Initialize the norm estimate, e.g., $\\sigma_{\\text{new}} \\leftarrow 0$.\n3.  **Iteration**: For $k=1, \\dots, K_{\\text{max}}$:\n    a.  Store the previous estimate: $\\sigma_{\\text{old}} \\leftarrow \\sigma_{\\text{new}}$.\n    b.  Apply the first matrix-vector product: $\\mathbf{y} \\leftarrow A \\mathbf{v}$.\n    c.  Update the norm estimate: $\\sigma_{\\text{new}} \\leftarrow \\lVert \\mathbf{y} \\rVert_2$.\n    d.  **Check for convergence**: If $k>1$ and $|\\sigma_{\\text{new}} - \\sigma_{\\text{old}}| < \\varepsilon \\cdot \\sigma_{\\text{new}}$, break the loop.\n    e.  **Handle zero matrix case**: If $\\sigma_{\\text{new}} = 0$, it implies $\\mathbf{y}=\\mathbf{0}$. This means $A\\mathbf{v}=\\mathbf{0}$. The matrix $A$ is singular, or possibly the zero matrix. The algorithm correctly yields $\\sigma_{\\text{new}} = 0$ and should terminate. We can break.\n    f.  Apply the second matrix-vector product: $\\mathbf{x} \\leftarrow A^\\top \\mathbf{y}$.\n    g.  Normalize the resulting vector for the next iteration: $\\mathbf{v} \\leftarrow \\frac{\\mathbf{x}}{\\lVert \\mathbf{x} \\rVert_2}$. If $\\lVert \\mathbf{x} \\rVert_2 = 0$, break. This happens if $\\mathbf{y}$ is in the null space of $A^\\top$, which for $\\mathbf{y}=A\\mathbf{v}$ implies $A\\mathbf{v}=\\mathbf{0}$, correctly yielding a norm of $0$.\n4.  **Return**: The final estimate $\\sigma_{\\text{new}}$.\n\nThis algorithm only uses matrix-vector products, adheres to all constraints, and correctly estimates $\\lVert A \\rVert_2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_induced_2_norm(A, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Estimates the induced matrix 2-norm (largest singular value) of a real matrix A\n    using the power iteration method.\n\n    The algorithm iteratively computes v_k = A^T * A * v_{k-1} without explicitly\n    forming the matrix A^T*A. The norm is estimated as ||A*v_k||_2.\n\n    Args:\n        A (np.ndarray): The input matrix, m x n.\n        tol (float): The relative tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The estimated induced 2-norm of A.\n    \"\"\"\n    # Get matrix dimensions\n    m, n = A.shape\n\n    # Handle the edge case of a matrix with zero columns.\n    if n == 0:\n        return 0.0\n\n    # Initialize with a deterministic non-zero vector in R^n.\n    # A vector of ones is a standard deterministic choice.\n    # The power method might fail if this initial vector is orthogonal\n    # to the dominant eigenvector of A^T*A. In practice, for general\n    # matrices and with finite-precision arithmetic, this is rare.\n    v = np.ones(n)\n    v /= np.linalg.norm(v)\n\n    norm_est = 0.0\n\n    for _ in range(max_iter):\n        norm_est_prev = norm_est\n\n        # First matrix-vector product: y = A*v\n        y = A @ v\n\n        # Update the norm estimate: ||A||_2 approx ||y||_2\n        norm_est = np.linalg.norm(y)\n\n        # Check for convergence using relative change.\n        # This check is safe because for a non-zero matrix, norm_est converges\n        # to a positive value.\n        if norm_est > 0 and abs(norm_est - norm_est_prev) < tol * norm_est:\n            break\n        \n        # Handle the case where A is the zero matrix or v is in the null space of A.\n        if norm_est == 0:\n            return 0.0\n\n        # Second matrix-vector product: x = A^T*y\n        x = A.T @ y\n        \n        # Normalize the vector for the next iteration.\n        norm_x = np.linalg.norm(x)\n\n        # If norm_x is zero, the iteration has converged to the null space.\n        if norm_x == 0:\n            break\n            \n        v = x / norm_x\n\n    return norm_est\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the norm estimation, and prints the results.\n    \"\"\"\n    \n    A1 = np.array([[3.0, 1.0], \n                   [1.0, 3.0]])\n\n    A2 = np.array([[1.0, 10.0, 0.0],\n                   [0.0, 1.0, 0.0],\n                   [0.0, 0.0, 0.1]])\n\n    A3 = np.array([[1.0, 2.0],\n                   [0.0, 1.0],\n                   [2.0, 0.0],\n                   [0.0, 0.0]])\n\n    A4 = np.array([[1.0, 0.0, 2.0, 0.0],\n                   [0.0, 1.0, 0.0, 1.0]])\n\n    A5 = np.array([[0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0]])\n\n    A6 = np.array([[1.0, 0.0],\n                   [0.0, 0.999]])\n                   \n    test_cases = [A1, A2, A3, A4, A5, A6]\n\n    results = []\n    for A in test_cases:\n        norm_estimate = estimate_induced_2_norm(A, tol=1e-10, max_iter=10000)\n        results.append(f\"{norm_estimate:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "In computational finance, we often rely on solutions to large systems of linear equations of the form $Ax=b$; but how trustworthy are these solutions in the face of inevitable small measurement or rounding errors? This practice demonstrates how matrix norms are crucial for assessing the reliability of such computations through the concept of the condition number [@problem_id:2449583]. You will design a numerical experiment to see firsthand how ill-conditioned matrices can dramatically amplify small input perturbations, a critical insight for building robust models.", "id": "2449583", "problem": "Design and implement a complete, runnable program that performs a numerical experiment demonstrating that for an ill-conditioned matrix $A$, a small relative error in $b$ can lead to a large relative error in the solution $x$ of the linear system $A x = b$. The experiment must be based strictly on first principles: norms, relative errors, and the definition of the linear solve. All quantities must use the vector and matrix $2$-norm. The program must compute, for each specified test case, the amplification factor defined as\n$$\nr \\;=\\; \\frac{\\|x_{\\epsilon} - x^\\star\\|_{2} / \\|x^\\star\\|_{2}}{\\|\\delta b\\|_{2} / \\|b\\|_{2}},\n$$\nwhere $x^\\star$ is the exact solution corresponding to the unperturbed right-hand side $b$, $\\delta b$ is a perturbation of $b$, and $x_{\\epsilon}$ is the solution of $A x = b + \\delta b$. The program must use the following experiment setup for every test case:\n- Let $x^\\star$ be the vector in $\\mathbb{R}^n$ with all components equal to $1$.\n- Let $b = A x^\\star$.\n- Let the perturbation direction $v \\in \\mathbb{R}^n$ be defined componentwise by $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$ for $i = 1, \\dots, n$ so that $\\|v\\|_2 = 1$.\n- Let $\\delta b = \\epsilon \\, \\|b\\|_2 \\, v$, where $\\epsilon$ is the prescribed relative perturbation magnitude for the test.\n- Let $x_{\\epsilon}$ be the solution of $A x = b + \\delta b$.\n\nYour program must compute, for each test case, the amplification factor $r$ as a floating-point number. The set of test cases is as follows, each identified by its case number and executed in the listed order:\n- Case $1$: $A$ is the Hilbert matrix $H \\in \\mathbb{R}^{5 \\times 5}$ with entries $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1,\\dots,5\\}$, and $\\epsilon = 10^{-8}$.\n- Case $2$: $A$ is the Hilbert matrix $H \\in \\mathbb{R}^{10 \\times 10}$ with entries $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1,\\dots,10\\}$, and $\\epsilon = 10^{-8}$.\n- Case $3$: $A$ is the identity matrix $I \\in \\mathbb{R}^{8 \\times 8}$, and $\\epsilon = 10^{-8}$.\n- Case $4$: $A \\in \\mathbb{R}^{2 \\times 2}$ is given by\n$$\nA = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 1 + 10^{-10}\n\\end{bmatrix},\n$$\nand $\\epsilon = 10^{-12}$.\n\nAll norms must be the $2$-norm. There are no physical units involved. Angles are not used. Percentages must not be used; all ratios and magnitudes must be expressed as decimal floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[r1,r2,r3,r4]\"). Each entry must be the amplification factor $r$ for the corresponding case, rounded to $6$ significant digits, in the order of cases $1$ through $4$.", "solution": "The experiment is designed from first principles of linear algebra and norm-based error analysis. We consider a linear system $A x = b$ with exact data and its perturbed counterpart $A x = b + \\delta b$. For a given matrix $A \\in \\mathbb{R}^{n \\times n}$ and an exact solution vector $x^\\star \\in \\mathbb{R}^n$, we define $b = A x^\\star$. We then construct a perturbation $\\delta b$ with prescribed relative magnitude $\\epsilon$ as follows. Let $v \\in \\mathbb{R}^n$ with entries $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$, for $i = 1, \\dots, n$. This $v$ satisfies $\\|v\\|_2 = 1$ by construction:\n$$\n\\|v\\|_2^2 = \\sum_{i=1}^n \\left(\\frac{1}{\\sqrt{n}}\\right)^2 = \\frac{n}{n} = 1.\n$$\nWe set $\\delta b = \\epsilon \\, \\|b\\|_2 \\, v$. Then the relative perturbation in $b$ is exactly $\\frac{\\|\\delta b\\|_2}{\\|b\\|_2} = \\epsilon$ because\n$$\n\\|\\delta b\\|_2 = \\epsilon \\, \\|b\\|_2 \\, \\|v\\|_2 = \\epsilon \\, \\|b\\|_2.\n$$\nLet $x_\\epsilon$ denote the solution to the perturbed system $A x = b + \\delta b$. The error in the solution equals\n$$\nx_\\epsilon - x^\\star = A^{-1}\\,(b+\\delta b) - A^{-1} b = A^{-1}\\,\\delta b.\n$$\nTherefore the relative error in $x$ is\n$$\n\\frac{\\|x_\\epsilon - x^\\star\\|_2}{\\|x^\\star\\|_2} = \\frac{\\|A^{-1}\\,\\delta b\\|_2}{\\|x^\\star\\|_2}.\n$$\nThe amplification factor $r$ that the program reports for each case is\n$$\nr = \\frac{\\|x_{\\epsilon} - x^\\star\\|_{2} / \\|x^\\star\\|_{2}}{\\|\\delta b\\|_{2} / \\|b\\|_{2}}.\n$$\nFrom norm properties and the definition of the matrix $2$-norm, we can relate this amplification to the condition number in the matrix $2$-norm, $\\kappa_2(A) = \\|A\\|_2 \\, \\|A^{-1}\\|_2$. Specifically, using $\\|A^{-1} \\delta b\\|_2 \\le \\|A^{-1}\\|_2 \\, \\|\\delta b\\|_2$ and $\\|b\\|_2 = \\|A x^\\star\\|_2 \\le \\|A\\|_2 \\, \\|x^\\star\\|_2$, we obtain\n$$\n\\frac{\\|x_\\epsilon - x^\\star\\|_2}{\\|x^\\star\\|_2} \\le \\|A^{-1}\\|_2 \\, \\|\\delta b\\|_2 \\,\\frac{1}{\\|x^\\star\\|_2}\n\\le \\|A^{-1}\\|_2 \\, \\|A\\|_2 \\, \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} = \\kappa_2(A) \\, \\frac{\\|\\delta b\\|_2}{\\|b\\|_2}.\n$$\nHence\n$$\nr \\le \\kappa_2(A).\n$$\nThis inequality shows that the relative error in the solution can be amplified by up to approximately the condition number. For well-conditioned matrices such as the identity matrix $I$, we have $\\kappa_2(I) = 1$, and thus $r$ should be close to $1$. For ill-conditioned matrices such as Hilbert matrices, $\\kappa_2(A)$ is very large, and even a very small $\\epsilon$ can result in a large relative error in $x$, producing a large $r$.\n\nThe test suite covers several regimes:\n- Case $1$ uses a Hilbert matrix of size $5$, which is ill-conditioned but moderate in size.\n- Case $2$ uses a Hilbert matrix of size $10$, which is more ill-conditioned, typically yielding a much larger amplification $r$.\n- Case $3$ uses the identity matrix of size $8$, which is perfectly conditioned, so $r$ should be approximately $1$.\n- Case $4$ uses a nearly singular $2 \\times 2$ matrix with entries differing by $10^{-10}$ on one element, producing a very large amplification.\n\nFor each case, the program constructs $x^\\star$, $b$, the unit-norm perturbation direction $v$, the perturbation $\\delta b$ with the specified $\\epsilon$, solves for $x_\\epsilon$, computes the relative errors, and reports $r$ rounded to $6$ significant digits. The final output is a single line containing the list $[r_1, r_2, r_3, r_4]$ in that order. This procedure directly and transparently exhibits how ill-conditioning in $A$ magnifies small relative perturbations in $b$ into large relative errors in the solution $x$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hilbert(n: int) -> np.ndarray:\n    # H[i,j] = 1 / (i + j + 1) with zero-based i,j; but use one-based formula directly\n    i = np.arange(1, n + 1).reshape(-1, 1)\n    j = np.arange(1, n + 1).reshape(1, -1)\n    return 1.0 / (i + j - 1.0)\n\ndef alternating_unit_vector(n: int) -> np.ndarray:\n    # v_i = (-1)^(i-1) / sqrt(n), i = 1..n\n    signs = (-1.0) ** np.arange(n)\n    v = signs / np.sqrt(n)\n    # Ensure unit norm numerically\n    return v / np.linalg.norm(v, 2)\n\ndef amplification_factor(A: np.ndarray, eps: float) -> float:\n    n = A.shape[0]\n    x_star = np.ones(n, dtype=float)\n    b = A @ x_star\n    nb = np.linalg.norm(b, 2)\n    if nb == 0.0:\n        # Degenerate, but not expected with provided tests; return NaN-like large value\n        return float('nan')\n    v = alternating_unit_vector(n)\n    delta_b = eps * nb * v\n    b_tilde = b + delta_b\n    # Solve for perturbed solution\n    x_tilde = np.linalg.solve(A, b_tilde)\n    # Relative errors\n    rel_b = np.linalg.norm(delta_b, 2) / nb\n    rel_x = np.linalg.norm(x_tilde - x_star, 2) / np.linalg.norm(x_star, 2)\n    # Amplification factor\n    return rel_x / rel_b\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (A_matrix, epsilon)\n    A1 = hilbert(5)\n    eps1 = 1e-8\n\n    A2 = hilbert(10)\n    eps2 = 1e-8\n\n    A3 = np.eye(8, dtype=float)\n    eps3 = 1e-8\n\n    A4 = np.array([[1.0, 1.0],\n                   [1.0, 1.0 + 1e-10]], dtype=float)\n    eps4 = 1e-12\n\n    test_cases = [\n        (A1, eps1),\n        (A2, eps2),\n        (A3, eps3),\n        (A4, eps4),\n    ]\n\n    results = []\n    for A, eps in test_cases:\n        r = amplification_factor(A, eps)\n        # Round to 6 significant digits\n        if np.isnan(r) or np.isinf(r):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{r:.6g}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}]}