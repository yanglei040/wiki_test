{"hands_on_practices": [{"introduction": "Before leveraging a powerful tool, it is essential to understand its inner workings. This first exercise guides you through the fundamental mechanics of Cholesky decomposition by having you compute it for a small, concrete matrix. By directly applying the defining equation $A = LL^T$ and solving for the elements of the lower-triangular matrix $L$ sequentially, you will gain a firm grasp of the algorithm that underpins this important factorization technique [@problem_id:950176].", "id": "950176", "problem": "Consider the symmetric positive definite matrix  \n\n$$\nA = \\begin{bmatrix}\n4 & 1 & 2 \\\\\n1 & 5 & 0 \\\\\n2 & 0 & 6\n\\end{bmatrix}.\n$$\n  \nThe Cholesky decomposition of $A$ is $A = LL^T$, where $L$ is a lower triangular matrix with positive diagonal entries. Compute the $(3,3)$-entry of $L$.\n\n", "solution": "Let \n$$L=\\begin{pmatrix}\n\\ell_{11}&0&0\\\\\n\\ell_{21}&\\ell_{22}&0\\\\\n\\ell_{31}&\\ell_{32}&\\ell_{33}\n\\end{pmatrix},$$\nso that $A=LL^T$.  Equating entries gives:\n\n1. From $(1,1)$‐entry: \n$$4=\\ell_{11}^2\\quad\\Longrightarrow\\quad \\ell_{11}=\\sqrt{4}=2.$$\n\n2. From $(2,1)$‐entry:\n$$1=\\ell_{21}\\,\\ell_{11}\\quad\\Longrightarrow\\quad \\ell_{21}=\\frac1{\\ell_{11}}=\\frac12.$$\n\n3. From $(3,1)$‐entry:\n$$2=\\ell_{31}\\,\\ell_{11}\\quad\\Longrightarrow\\quad \\ell_{31}=\\frac2{\\ell_{11}}=1.$$\n\n4. From $(2,2)$‐entry:\n$$5=\\ell_{21}^2+\\ell_{22}^2\n=\\Bigl(\\frac12\\Bigr)^2+\\ell_{22}^2\n\\quad\\Longrightarrow\\quad\n\\ell_{22}=\\sqrt{5-\\tfrac14}\n=\\sqrt{\\frac{19}{4}}\n=\\frac{\\sqrt{19}}2.$$\n\n5. From $(3,2)$‐entry:\n$$0=\\ell_{31}\\,\\ell_{21}+\\ell_{32}\\,\\ell_{22}\n=\\frac12+\\ell_{32}\\,\\frac{\\sqrt{19}}2\n\\quad\\Longrightarrow\\quad\n\\ell_{32}=-\\frac{\\tfrac12}{\\tfrac{\\sqrt{19}}2}\n=-\\frac1{\\sqrt{19}}.$$\n\n6. From $(3,3)$‐entry:\n$$6=\\ell_{31}^2+\\ell_{32}^2+\\ell_{33}^2\n=1^2+\\Bigl(-\\tfrac1{\\sqrt{19}}\\Bigr)^2+\\ell_{33}^2\n=1+\\frac1{19}+\\ell_{33}^2\n=\\frac{20}{19}+\\ell_{33}^2,$$\nhence\n$$\\ell_{33}^2\n=6-\\frac{20}{19}\n=\\frac{114-20}{19}\n=\\frac{94}{19}\n\\quad\\Longrightarrow\\quad\n\\ell_{33}=\\sqrt{\\frac{94}{19}}.$$", "answer": "\n$$\\boxed{\\sqrt{\\frac{94}{19}}}$$\n"}, {"introduction": "One of the primary motivations for developing matrix factorizations like the Cholesky decomposition is to efficiently solve systems of linear equations. This practice demonstrates this key application by transforming a single difficult problem, $A\\mathbf{x} = \\mathbf{b}$, into two much simpler ones. By using the factorization $A = LL^T$, you will solve for an intermediate vector $\\mathbf{y}$ using forward substitution in $L\\mathbf{y} = \\mathbf{b}$, and then solve for the final solution $\\mathbf{x}$ using back substitution in $L^T\\mathbf{x} = \\mathbf{y}$ [@problem_id:2481].", "id": "2481", "problem": "Consider a system of linear equations given by $A\\mathbf{x} = \\mathbf{b}$, where $A$ is a $3 \\times 3$ symmetric positive-definite matrix, $\\mathbf{x}$ is the vector of unknowns, and $\\mathbf{b}$ is a known vector.\n\nThe matrix $A$ has a Cholesky decomposition $A = LL^T$, where $L$ is a lower triangular matrix. The matrices $L$ and $\\mathbf{b}$ are given in symbolic form:\n\n$$\nL = \\begin{pmatrix} l_{11} & 0 & 0 \\\\ l_{21} & l_{22} & 0 \\\\ l_{31} & l_{32} & l_{33} \\end{pmatrix}\n$$\n\n$$\n\\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{pmatrix}\n$$\n\nLet the vector of unknowns be $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$, where it is assumed that $l_{ii} \\neq 0$ for all $i$.\n\nUsing the Cholesky decomposition, solve the system of equations for $\\mathbf{x}$. Your task is to derive a single, closed-form expression for the third component of the solution vector, $x_3$, in terms of the components of $L$ and $\\mathbf{b}$.\n\n", "solution": "We solve $A\\mathbf{x}=\\mathbf{b}$ by first writing $A=LL^T$ and setting $L\\mathbf{y}=\\mathbf{b}$, $L^T\\mathbf{x}=\\mathbf{y}$.  \n1. Forward substitution for $\\mathbf{y}$:\n$$y_1=\\frac{b_1}{l_{11}},$$\n$$y_2=\\frac{b_2-l_{21}y_1}{l_{22}}\n=\\frac{b_2-\\displaystyle l_{21}\\frac{b_1}{l_{11}}}{l_{22}},$$\n$$y_3=\\frac{b_3-l_{31}y_1-l_{32}y_2}{l_{33}}\n=\\frac{b_3-\\displaystyle l_{31}\\frac{b_1}{l_{11}}-l_{32}\\frac{b_2-\\displaystyle l_{21}\\frac{b_1}{l_{11}}}{l_{22}}}{l_{33}}.$$\n2. Back substitution for $x_3$ from $L^T\\mathbf{x}=\\mathbf{y}$ gives\n$$l_{33}x_3=y_3\n\\quad\\Longrightarrow\\quad\nx_3=\\frac{y_3}{l_{33}}.$$\nSubstitute $y_3$:\n$$x_3\n=\\frac{1}{l_{33}}\\,\n\\frac{b_3-\\frac{l_{31}b_1}{l_{11}}-\\frac{l_{32}(b_2-\\frac{l_{21}b_1}{l_{11}})}{l_{22}}}{l_{33}}\n=\\frac{b_3-\\frac{l_{31}b_1}{l_{11}}-\\frac{l_{32}b_2}{l_{22}}+\\frac{l_{32}l_{21}b_1}{l_{11}l_{22}}}{l_{33}^2}.$$\nCombine over common denominator $l_{11}l_{22}$:\n$$x_3\n=\\frac{b_3\\,l_{11}l_{22}-b_2\\,l_{11}l_{32}-b_1\\,l_{22}l_{31}+b_1\\,l_{21}l_{32}}\n{l_{11}l_{22}\\,l_{33}^2}.$$", "answer": "$$\\boxed{\\frac{b_3\\,l_{11}l_{22}-b_2\\,l_{11}l_{32}-b_1\\,l_{22}l_{31}+b_1\\,l_{21}l_{32}}{l_{11}l_{22}\\,l_{33}^2}}$$"}, {"introduction": "In the world of computational finance, Cholesky decomposition is the cornerstone for Monte Carlo simulations involving multiple correlated assets. This advanced exercise places you in a realistic scenario: pricing a basket option, where the underlying asset returns are correlated. The practice not only involves applying the decomposition to generate correlated random variables but also challenges you to diagnose a common and critical implementation bug, contrasting the correct transformation $\\mathbf{x} = L\\mathbf{z}$ with an incorrect one. Successfully completing this task [@problem_id:2379747] will solidify your understanding of how to correctly model correlated systems, a vital skill in quantitative finance.", "id": "2379747", "problem": "You are tasked with diagnosing the effect of an incorrect use of the Cholesky decomposition on a Monte Carlo (MC) simulation for pricing a European call option on a weighted basket of assets under a multi-asset Geometric Brownian Motion (GBM) model. Let the instantaneous correlation matrix be denoted by $\\mathbf{R}$, the vector of volatilities by $\\boldsymbol{\\sigma}$, the time to maturity by $T$, and the continuously compounded risk-free rate by $r$. Define the target covariance matrix of the Gaussian log-return innovations by\n$$\n\\boldsymbol{\\Sigma} = \\left(\\operatorname{diag}(\\boldsymbol{\\sigma}) \\, \\mathbf{R} \\, \\operatorname{diag}(\\boldsymbol{\\sigma})\\right) \\, T.\n$$\nLet $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^{\\top}$ be the Cholesky decomposition with $\\mathbf{L}$ lower triangular and with strictly positive diagonal entries. Consider the two linear mappings on independent standard normal vectors $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$:\n- $\\mathbf{f}(\\mathbf{z}) = \\mathbf{L}\\mathbf{z}$,\n- $\\mathbf{g}(\\mathbf{z}) = \\mathbf{L}^{\\top}\\mathbf{z}$.\nFor each test case below, draw $N$ independent identically distributed samples $\\{\\mathbf{z}^{(n)}\\}_{n=1}^{N}$ with each $\\mathbf{z}^{(n)} \\in \\mathbb{R}^d$ having independent standard normal entries, and form the simulated innovation matrices $\\mathbf{X}_{\\text{right}}$ and $\\mathbf{X}_{\\text{wrong}}$ by applying $\\mathbf{f}$ and $\\mathbf{g}$ row-wise to the sample matrix $\\mathbf{Z}$ whose rows are the $\\mathbf{z}^{(n)}$. Using these draws, perform both of the following for each test case:\n1. Compute the empirical covariance matrices\n$$\n\\widehat{\\boldsymbol{\\Sigma}}_{\\text{right}} = \\frac{1}{N}\\sum_{n=1}^{N}\\left(\\mathbf{f}(\\mathbf{z}^{(n)}) - \\bar{\\mathbf{x}}_{\\text{right}}\\right)\\left(\\mathbf{f}(\\mathbf{z}^{(n)}) - \\bar{\\mathbf{x}}_{\\text{right}}\\right)^{\\top},\n\\quad\n\\bar{\\mathbf{x}}_{\\text{right}} = \\frac{1}{N}\\sum_{n=1}^{N} \\mathbf{f}(\\mathbf{z}^{(n)}),\n$$\nand\n$$\n\\widehat{\\boldsymbol{\\Sigma}}_{\\text{wrong}} = \\frac{1}{N}\\sum_{n=1}^{N}\\left(\\mathbf{g}(\\mathbf{z}^{(n)}) - \\bar{\\mathbf{x}}_{\\text{wrong}}\\right)\\left(\\mathbf{g}(\\mathbf{z}^{(n)}) - \\bar{\\mathbf{x}}_{\\text{wrong}}\\right)^{\\top},\n\\quad\n\\bar{\\mathbf{x}}_{\\text{wrong}} = \\frac{1}{N}\\sum_{n=1}^{N} \\mathbf{g}(\\mathbf{z}^{(n)}).\n$$\nMeasure the mismatch to the target covariance $\\boldsymbol{\\Sigma}$ using the Frobenius norm errors\n$$\ne_{\\text{right}} = \\left\\| \\widehat{\\boldsymbol{\\Sigma}}_{\\text{right}} - \\boldsymbol{\\Sigma} \\right\\|_{F},\n\\quad\ne_{\\text{wrong}} = \\left\\| \\widehat{\\boldsymbol{\\Sigma}}_{\\text{wrong}} - \\boldsymbol{\\Sigma} \\right\\|_{F}.\n$$\n2. Price a European call on the weighted basket with weights $\\mathbf{w}$, strike $K$, and initial prices $\\mathbf{S}_0$ by simulating terminal prices\n$$\n\\mathbf{S}_T^{\\text{right},(n)} = \\mathbf{S}_0 \\odot \\exp\\!\\left( \\left(r\\mathbf{1} - \\tfrac{1}{2}\\boldsymbol{\\sigma}\\odot\\boldsymbol{\\sigma}\\right) T + \\mathbf{f}(\\mathbf{z}^{(n)}) \\right),\n\\quad\n\\mathbf{S}_T^{\\text{wrong},(n)} = \\mathbf{S}_0 \\odot \\exp\\!\\left( \\left(r\\mathbf{1} - \\tfrac{1}{2}\\boldsymbol{\\sigma}\\odot\\boldsymbol{\\sigma}\\right) T + \\mathbf{g}(\\mathbf{z}^{(n)}) \\right),\n$$\nwhere $\\odot$ denotes elementwise multiplication and the exponential is applied elementwise. For each mapping, compute the discounted MC estimator of the call price on the basket $\\mathbf{w}^{\\top}\\mathbf{S}_T$ with payoff $\\max(\\mathbf{w}^{\\top}\\mathbf{S}_T - K, 0)$:\n$$\nP_{\\text{right}} = e^{-rT} \\cdot \\frac{1}{N} \\sum_{n=1}^{N} \\max\\!\\left(\\mathbf{w}^{\\top}\\mathbf{S}_T^{\\text{right},(n)} - K, 0\\right),\n\\quad\nP_{\\text{wrong}} = e^{-rT} \\cdot \\frac{1}{N} \\sum_{n=1}^{N} \\max\\!\\left(\\mathbf{w}^{\\top}\\mathbf{S}_T^{\\text{wrong},(n)} - K, 0\\right).\n$$\nUse the same set of $\\{\\mathbf{z}^{(n)}\\}$ for both mappings in each test case. Use the provided random seeds exactly to ensure reproducibility. Angles do not appear; no unit conversion is required.\n\nTest suite. For each test case, all vectors are ordered consistently by asset index.\n- Test case 1 (dimension $d=2$):\n  - $\\mathbf{S}_0 = [\\,100.0,\\,100.0\\,]$, $\\mathbf{w} = [\\,0.5,\\,0.5\\,]$, $K = 100.0$,\n  - $\\boldsymbol{\\sigma} = [\\,0.2,\\,0.3\\,]$, \n  - $\\mathbf{R} = \\begin{bmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix}$,\n  - $r = 0.02$, $T = 1.0$, $N = 200000$, random seed $= 20231105$.\n- Test case 2 (dimension $d=3$):\n  - $\\mathbf{S}_0 = [\\,95.0,\\,105.0,\\,100.0\\,]$, $\\mathbf{w} = [\\,0.2,\\,0.5,\\,0.3\\,]$, $K = 100.0$,\n  - $\\boldsymbol{\\sigma} = [\\,0.25,\\,0.4,\\,0.3\\,]$, \n  - $\\mathbf{R} = \\begin{bmatrix} 1 & -0.5 & 0.1 \\\\ -0.5 & 1 & 0.2 \\\\ 0.1 & 0.2 & 1 \\end{bmatrix}$,\n  - $r = 0.01$, $T = 2.0$, $N = 200000$, random seed $= 20231106$.\n- Test case 3 (dimension $d=4$):\n  - $\\mathbf{S}_0 = [\\,120.0,\\,80.0,\\,100.0,\\,110.0\\,]$, $\\mathbf{w} = [\\,0.1,\\,0.3,\\,0.4,\\,0.2\\,]$, $K = 105.0$,\n  - $\\boldsymbol{\\sigma} = [\\,0.15,\\,0.25,\\,0.2,\\,0.3\\,]$, \n  - $\\mathbf{R} = \\begin{bmatrix} 1 & 0.6 & 0.3 & 0.0 \\\\ 0.6 & 1 & -0.2 & 0.1 \\\\ 0.3 & -0.2 & 1 & 0.4 \\\\ 0.0 & 0.1 & 0.4 & 1 \\end{bmatrix}$,\n  - $r = 0.03$, $T = 0.5$, $N = 200000$, random seed $= 20231107$.\n\nRequired final output. Your program must produce a single line containing a comma-separated list enclosed in square brackets. The list must contain, in order, for test case $1$, then $2$, then $3$, the four floats\n$$\n\\left[e_{\\text{wrong}},\\, e_{\\text{right}},\\, P_{\\text{wrong}},\\, P_{\\text{right}}\\right],\n$$\nflattened across the three test cases. Concretely, the output must be\n$$\n\\big[ e_{\\text{wrong},1},\\, e_{\\text{right},1},\\, P_{\\text{wrong},1},\\, P_{\\text{right},1},\\, e_{\\text{wrong},2},\\, e_{\\text{right},2},\\, P_{\\text{wrong},2},\\, P_{\\text{right},2},\\, e_{\\text{wrong},3},\\, e_{\\text{right},3},\\, P_{\\text{wrong},3},\\, P_{\\text{right},3} \\big].\n$$", "solution": "The problem posed is a valid exercise in computational finance, designed to diagnose a common but critical error in the implementation of Monte Carlo simulations for multi-asset models. I will first establish the mathematical principle and then outline the computational procedure.\n\nThe core principle is the generation of correlated random vectors through linear transformation. Let $\\mathbf{z}$ be a $d$-dimensional random vector whose components are independent standard normal random variables. This implies its expectation is the zero vector, $\\mathbb{E}[\\mathbf{z}] = \\mathbf{0}$, and its covariance matrix is the identity matrix, $\\operatorname{Cov}(\\mathbf{z}) = \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top] = \\mathbf{I}$. Our objective is to generate a new random vector $\\mathbf{x}$ that has a specified target covariance matrix $\\boldsymbol{\\Sigma}$. We achieve this by applying a linear transformation $\\mathbf{x} = \\mathbf{A}\\mathbf{z}$, where $\\mathbf{A}$ is a $d \\times d$ matrix. The covariance of the resulting vector $\\mathbf{x}$ is\n$$\n\\operatorname{Cov}(\\mathbf{x}) = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top] = \\mathbb{E}[(\\mathbf{A}\\mathbf{z})(\\mathbf{A}\\mathbf{z})^\\top] = \\mathbb{E}[\\mathbf{A}\\mathbf{z}\\mathbf{z}^\\top\\mathbf{A}^\\top] = \\mathbf{A}\\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top]\\mathbf{A}^\\top = \\mathbf{A}\\mathbf{I}\\mathbf{A}^\\top = \\mathbf{A}\\mathbf{A}^\\top.\n$$\nTo ensure that $\\operatorname{Cov}(\\mathbf{x}) = \\boldsymbol{\\Sigma}$, we must find a matrix $\\mathbf{A}$ such that $\\mathbf{A}\\mathbf{A}^\\top = \\boldsymbol{\\Sigma}$. The Cholesky decomposition provides such a matrix. Given that the correlation matrix $\\mathbf{R}$ is symmetric and positive definite, and volatilities are positive, the target covariance matrix $\\boldsymbol{\\Sigma} = (\\operatorname{diag}(\\boldsymbol{\\sigma}) \\, \\mathbf{R} \\, \\operatorname{diag}(\\boldsymbol{\\sigma})) \\, T$ is also symmetric and positive definite. Consequently, there exists a unique lower triangular matrix $\\mathbf{L}$ with strictly positive diagonal entries such that $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top$. The correct transformation is therefore $\\mathbf{f}(\\mathbf{z}) = \\mathbf{L}\\mathbf{z}$.\n\nThe problem specifies an incorrect alternative transformation, $\\mathbf{g}(\\mathbf{z}) = \\mathbf{L}^\\top\\mathbf{z}$. Let us analyze the covariance structure produced by this mapping. Let $\\mathbf{y} = \\mathbf{g}(\\mathbf{z})$. Its covariance matrix is\n$$\n\\operatorname{Cov}(\\mathbf{y}) = \\mathbb{E}[(\\mathbf{L}^\\top\\mathbf{z})(\\mathbf{L}^\\top\\mathbf{z})^\\top] = \\mathbf{L}^\\top \\mathbb{E}[\\mathbf{z}\\mathbf{z}^\\top] (\\mathbf{L}^\\top)^\\top = \\mathbf{L}^\\top\\mathbf{I}\\mathbf{L} = \\mathbf{L}^\\top\\mathbf{L}.\n$$\nMatrix multiplication is not commutative, so in general, $\\mathbf{L}\\mathbf{L}^\\top \\neq \\mathbf{L}^\\top\\mathbf{L}$. The incorrect transformation generates random vectors with covariance matrix $\\mathbf{L}^\\top\\mathbf{L}$, which is not the desired target $\\boldsymbol{\\Sigma}$.\n\nThe computational procedure follows from this analysis.\n1.  For each test case, we first construct the numerical target covariance matrix $\\boldsymbol{\\Sigma}$ using the provided parameters $\\boldsymbol{\\sigma}$, $\\mathbf{R}$, and $T$. We then compute its lower triangular Cholesky factor $\\mathbf{L}$.\n\n2.  Using the specified random seed, we generate a matrix $\\mathbf{Z}$ of size $N \\times d$, where each row is an independent sample from $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$.\n\n3.  We generate two sets of correlated random increments. For the correct case, we compute $\\mathbf{X}_{\\text{right}} = \\mathbf{Z}\\mathbf{L}^\\top$, where each row has the target covariance $\\boldsymbol{\\Sigma}$. For the incorrect case, we compute $\\mathbf{X}_{\\text{wrong}} = \\mathbf{Z}\\mathbf{L}$, where each row has the incorrect covariance $\\mathbf{L}^\\top\\mathbf{L}$. Note that if our sample vectors $\\mathbf{z}^{(n)}$ are column vectors, the operations are $\\mathbf{L}\\mathbf{z}^{(n)}$ and $\\mathbf{L}^{\\top}\\mathbf{z}^{(n)}$ respectively. When working with a matrix of row samples $\\mathbf{Z}$, the equivalent operations become $\\mathbf{Z}\\mathbf{L}^\\top$ and $\\mathbf{Z}\\mathbf{L}$.\n\n4.  We compute the empirical covariance matrices $\\widehat{\\boldsymbol{\\Sigma}}_{\\text{right}}$ and $\\widehat{\\boldsymbol{\\Sigma}}_{\\text{wrong}}$ from the sample matrices $\\mathbf{X}_{\\text{right}}$ and $\\mathbf{X}_{\\text{wrong}}$. We then measure their deviation from the target matrix $\\boldsymbol{\\Sigma}$ using the Frobenius norm, obtaining the errors $e_{\\text{right}}$ and $e_{\\text{wrong}}$. We expect $e_{\\text{right}}$ to be small, reflecting only the sampling error, whereas $e_{\\text{wrong}}$ will be substantial, reflecting the fundamental model misspecification.\n\n5.  Finally, we simulate the terminal asset prices for a multi-asset Geometric Brownian Motion. The solution for the log-prices is\n    $$\n    \\log(\\mathbf{S}_T) = \\log(\\mathbf{S}_0) + \\left(r\\mathbf{1} - \\frac{1}{2}\\boldsymbol{\\sigma} \\odot \\boldsymbol{\\sigma}\\right)T + \\text{innovations}.\n    $$\n    We use the vectors from $\\mathbf{X}_{\\text{right}}$ and $\\mathbf{X}_{\\text{wrong}}$ as the `innovations`. This gives two sets of terminal prices, $\\mathbf{S}_T^{\\text{right}}$ and $\\mathbf{S}_T^{\\text{wrong}}$. From these, we compute the basket values $\\mathbf{w}^{\\top}\\mathbf{S}_T$, the option payoffs $\\max(\\mathbf{w}^{\\top}\\mathbf{S}_T - K, 0)$, and the final discounted Monte Carlo price estimates, $P_{\\text{right}}$ and $P_{\\text{wrong}}$. The incorrect covariance structure in the second case will lead to a mispricing of the option, which our calculations will quantify.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n\n    def run_simulation(S0, w, K, sigma, R, r, T, N, seed):\n        \"\"\"\n        Runs the Monte Carlo simulation for a single test case.\n        \n        Args:\n            S0 (np.ndarray): Initial asset prices.\n            w (np.ndarray): Asset weights in the basket.\n            K (float): Strike price of the call option.\n            sigma (np.ndarray): Vector of asset volatilities.\n            R (np.ndarray): Correlation matrix of assets.\n            r (float): Risk-free interest rate.\n            T (float): Time to maturity.\n            N (int): Number of Monte Carlo paths.\n            seed (int): Random number generator seed.\n\n        Returns:\n            tuple: A tuple containing (e_wrong, e_right, P_wrong, P_right).\n        \"\"\"\n        # Initialize a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n        d = len(S0)\n\n        # 1. Construct the target covariance matrix Sigma for log-return innovations.\n        # Sigma = (diag(sigma) @ R @ diag(sigma)) * T\n        diag_sigma = np.diag(sigma)\n        Sigma = (diag_sigma @ R @ diag_sigma) * T\n\n        # 2. Perform Cholesky decomposition: Sigma = L @ L.T\n        # We use scipy.linalg.cholesky for robustness, specifying lower=True.\n        L = scipy.linalg.cholesky(Sigma, lower=True)\n\n        # 3. Generate N samples of d-dimensional independent standard normal variates.\n        Z = rng.standard_normal(size=(N, d))\n\n        # 4. Generate correlated random shocks using right and wrong transformations.\n        # Right method: rows of X_right are samples with covariance L @ L.T = Sigma\n        X_right = Z @ L.T\n        # Wrong method: rows of X_wrong are samples with covariance L.T @ L\n        X_wrong = Z @ L\n\n        # 5. Compute empirical covariance matrices and their errors.\n        # The problem specifies normalization by N, so we set ddof=0.\n        Sigma_hat_right = np.cov(X_right, rowvar=False, ddof=0)\n        Sigma_hat_wrong = np.cov(X_wrong, rowvar=False, ddof=0)\n        \n        # Calculate Frobenius norm of errors relative to the target Sigma.\n        e_right = np.linalg.norm(Sigma_hat_right - Sigma, 'fro')\n        e_wrong = np.linalg.norm(Sigma_hat_wrong - Sigma, 'fro')\n\n        # 6. Price the European call option on the basket.\n        # Calculate the drift term for the GBM price simulation.\n        drift = (r - 0.5 * sigma**2) * T\n\n        # Simulate terminal asset prices for both sets of shocks.\n        S_T_right = S0 * np.exp(drift + X_right)\n        S_T_wrong = S0 * np.exp(drift + X_wrong)\n\n        # Calculate the value of the basket for each simulation path.\n        basket_val_right = S_T_right @ w\n        basket_val_wrong = S_T_wrong @ w\n\n        # Calculate the payoff of the call option for each path.\n        payoff_right = np.maximum(basket_val_right - K, 0)\n        payoff_wrong = np.maximum(basket_val_wrong - K, 0)\n\n        # Calculate the discounted average payoff (Monte Carlo price).\n        P_right = np.exp(-r * T) * np.mean(payoff_right)\n        P_wrong = np.exp(-r * T) * np.mean(payoff_wrong)\n\n        return e_wrong, e_right, P_wrong, P_right\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"S0\": np.array([100.0, 100.0]), \"w\": np.array([0.5, 0.5]), \"K\": 100.0,\n            \"sigma\": np.array([0.2, 0.3]), \"R\": np.array([[1, 0.7], [0.7, 1]]),\n            \"r\": 0.02, \"T\": 1.0, \"N\": 200000, \"seed\": 20231105\n        },\n        # Test case 2\n        {\n            \"S0\": np.array([95.0, 105.0, 100.0]), \"w\": np.array([0.2, 0.5, 0.3]), \"K\": 100.0,\n            \"sigma\": np.array([0.25, 0.4, 0.3]),\n            \"R\": np.array([[1, -0.5, 0.1], [-0.5, 1, 0.2], [0.1, 0.2, 1]]),\n            \"r\": 0.01, \"T\": 2.0, \"N\": 200000, \"seed\": 20231106\n        },\n        # Test case 3\n        {\n            \"S0\": np.array([120.0, 80.0, 100.0, 110.0]), \"w\": np.array([0.1, 0.3, 0.4, 0.2]), \"K\": 105.0,\n            \"sigma\": np.array([0.15, 0.25, 0.2, 0.3]),\n            \"R\": np.array([[1, 0.6, 0.3, 0.0], [0.6, 1, -0.2, 0.1], [0.3, -0.2, 1, 0.4], [0.0, 0.1, 0.4, 1]]),\n            \"r\": 0.03, \"T\": 0.5, \"N\": 200000, \"seed\": 20231107\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Run the simulation for the current case and collect results.\n        results_tuple = run_simulation(**case)\n        all_results.extend(results_tuple)\n\n    # Print the final flattened list of results in the required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}]}