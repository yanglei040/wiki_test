{"hands_on_practices": [{"introduction": "To truly understand a numerical method, it is essential to trace its steps manually. This first exercise invites you to do just that by performing two iterations of both the secant method and the bisection method. By comparing the approximations they produce, you will gain a practical feel for their distinct approaches and see firsthand why the secant method often converges more quickly [@problem_id:2199000].", "id": "2199000", "problem": "Two common numerical methods for finding roots of a function are the bisection method and the secant method. Consider the polynomial function $p(x) = x^3 - 4x + 1$. We are interested in finding the largest real root of this polynomial, which is known to lie in the interval $[1, 2]$.\n\nYou are tasked to compare the results of the first few iterations of both methods.\n\nFirst, using the bisection method, start with the interval $[a_0, b_0] = [1, 2]$ and perform two iterations to find the approximation of the root. Let this approximation be denoted by $x_B$.\n\nSecond, using the secant method, start with the initial guesses $x_0 = 1$ and $x_1 = 2$ and perform two iterations to find the approximation of the root. Let this approximation be denoted by $x_S$.\n\nCalculate the absolute difference between these two approximations, $|x_B - x_S|$. Report your final answer rounded to four significant figures.\n\n", "solution": "We are given the polynomial $p(x)=x^{3}-4x+1$ and that its largest real root lies in $[1,2]$. First, we apply two iterations of the bisection method starting from $[a_{0},b_{0}]=[1,2]$, then two iterations of the secant method starting from $x_{0}=1$, $x_{1}=2$. Finally, we compute the absolute difference between the two approximations.\n\nBisection method:\n- Evaluate the endpoints: $p(1)=1-4+1=-2<0$ and $p(2)=8-8+1=1>0$, so a root lies in $[1,2]$.\n- Iteration 1: midpoint $m_{1}=\\frac{1+2}{2}=\\frac{3}{2}$. Then\n$$\np\\!\\left(\\frac{3}{2}\\right)=\\left(\\frac{3}{2}\\right)^{3}-4\\cdot\\frac{3}{2}+1=\\frac{27}{8}-6+1=\\frac{27}{8}-\\frac{40}{8}=-\\frac{13}{8}<0.\n$$\nSince $p(m_{1})<0$ and $p(2)>0$, the new interval is $[a_{1},b_{1}]=\\left[\\frac{3}{2},2\\right]$.\n- Iteration 2: midpoint $m_{2}=\\frac{\\frac{3}{2}+2}{2}=\\frac{7}{4}$. Then\n$$\np\\!\\left(\\frac{7}{4}\\right)=\\left(\\frac{7}{4}\\right)^{3}-4\\cdot\\frac{7}{4}+1=\\frac{343}{64}-7+1=\\frac{343}{64}-6=\\frac{343-384}{64}=-\\frac{41}{64}<0.\n$$\nAgain $p(m_{2})<0$ and $p(2)>0$, so after two iterations the bisection approximation is\n$$\nx_{B}=m_{2}=\\frac{7}{4}.\n$$\n\nSecant method:\nUse the update $x_{k+1}=x_{k}-p(x_{k})\\frac{x_{k}-x_{k-1}}{p(x_{k})-p(x_{k-1})}$.\n- With $x_{0}=1$, $x_{1}=2$, we have $p(1)=-2$, $p(2)=1$. The first update gives\n$$\nx_{2}=2-1\\cdot\\frac{2-1}{1-(-2)}=2-\\frac{1}{3}=\\frac{5}{3}.\n$$\nCompute\n$$\np\\!\\left(\\frac{5}{3}\\right)=\\left(\\frac{5}{3}\\right)^{3}-4\\cdot\\frac{5}{3}+1=\\frac{125}{27}-\\frac{20}{3}+1=\\frac{125-180+27}{27}=-\\frac{28}{27}.\n$$\n- The second update uses $x_{1}=2$, $x_{2}=\\frac{5}{3}$:\n$$\nx_{3}=\\frac{5}{3}-\\left(-\\frac{28}{27}\\right)\\frac{\\frac{5}{3}-2}{-\\frac{28}{27}-1}\n=\\frac{5}{3}-\\left(-\\frac{28}{27}\\right)\\frac{-\\frac{1}{3}}{-\\frac{55}{27}}\n=\\frac{5}{3}-\\frac{28}{81}\\cdot\\frac{27}{-55}\n=\\frac{5}{3}-\\left(-\\frac{28}{165}\\right)\n=\\frac{5}{3}+\\frac{28}{165}\n=\\frac{275}{165}+\\frac{28}{165}\n=\\frac{303}{165}\n=\\frac{101}{55}.\n$$\nThus after two iterations the secant approximation is\n$$\nx_{S}=\\frac{101}{55}.\n$$\n\nAbsolute difference and rounding:\nCompute\n$$\n|x_{B}-x_{S}|=\\left|\\frac{7}{4}-\\frac{101}{55}\\right|=\\left|\\frac{385-404}{220}\\right|=\\frac{19}{220}.\n$$\nAs a decimal, $\\frac{19}{220}=0.0863636\\ldots$, which rounded to four significant figures is $0.08636$.", "answer": "$$\\boxed{0.08636}$$"}, {"introduction": "Beyond pure calculation, a deep understanding of a numerical method comes from exploring its behavior in special cases. This problem presents a thought experiment involving an odd function and symmetric initial guesses, a scenario where the secant method finds the root with surprising efficiency. Solving this puzzle reveals the crucial link between the algorithm's formula and the function's underlying geometry, highlighting how problem-specific knowledge can be used to accelerate convergence [@problem_id:2163434].", "id": "2163434", "problem": "The secant method is a numerical algorithm for finding the root of a function $f(x)$. Given two initial guesses $x_{n-1}$ and $x_n$, the next guess $x_{n+1}$ is calculated using the iterative formula:\n$$x_{n+1} = x_n - f(x_n) \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}$$\nConsider a continuous function $f(x)$ that is an odd function, which satisfies the property $f(-x) = -f(x)$ for all $x$ in its domain. To find a root of this function, the secant method is initiated with two distinct, non-zero starting guesses that are symmetric with respect to the origin: $x_1 = -x_0$. We are given that $x_0 \\neq 0$ and $f(x_0) \\neq 0$.\n\nDetermine the exact number of iterations required for the secant method to find a root of the function $f(x)$ under these specific initial conditions. An \"iteration\" refers to a single application of the formula to compute the next guess.\n\n", "solution": "We are given that $f$ is odd, so $f(-x)=-f(x)$ for all $x$. In particular, setting $x=0$ gives $f(0)=-f(0)$, hence $f(0)=0$. Therefore $x=0$ is a root of $f$.\n\nThe secant iteration is\n$$\nx_{n+1}=x_{n}-f(x_{n})\\,\\frac{x_{n}-x_{n-1}}{f(x_{n})-f(x_{n-1})}.\n$$\nWe start with symmetric, nonzero initial guesses $x_{0}$ and $x_{1}=-x_{0}$, with $x_{0}\\neq 0$ and $f(x_{0})\\neq 0$. Compute $x_{2}$ from $x_{0}$ and $x_{1}$. Using oddness, we have\n$$\nf(x_{1})=f(-x_{0})=-f(x_{0}).\n$$\nThen\n$$\nx_{1}-x_{0}=-x_{0}-x_{0}=-2x_{0},\\quad f(x_{1})-f(x_{0})=-f(x_{0})-f(x_{0})=-2f(x_{0}).\n$$\nSince $f(x_{0})\\neq 0$, the denominator $-2f(x_{0})\\neq 0$, so the secant update is well-defined. The fraction simplifies to\n$$\n\\frac{f(x_{1})(x_{1}-x_{0})}{f(x_{1})-f(x_{0})}\n=\\frac{(-f(x_{0}))(-2x_{0})}{-2f(x_{0})}\n=\\frac{2x_{0}f(x_{0})}{-2f(x_{0})}\n=-x_{0}.\n$$\nTherefore,\n$$\nx_{2}=x_{1}-\\left(-x_{0}\\right)=x_{1}+x_{0}=-x_{0}+x_{0}=0.\n$$\nAs shown above, $x=0$ is a root of $f$, so the secant method finds the root exactly after a single application of the iteration formula starting from the given symmetric initial guesses. Hence, the exact number of iterations required is $1$.", "answer": "$$\\boxed{1}$$"}, {"introduction": "The standard secant method is fast, but it lacks the convergence guarantee of bracketing methods and can fail if not handled carefully. This final practice moves from theory to robust implementation, challenging you to design a hybrid algorithm that combines the speed of the secant method with the safety of bisection. By building a solver for practical problems in finance and economics, you will learn how to create numerical tools that are both efficient and reliable, a core skill in computational finance [@problem_id:2443706].", "id": "2443706", "problem": "You are given continuous real-valued functions that arise in computational economics and finance, each defined on a closed interval with opposite signs at the endpoints. For each function, compute a real root $x^\\star$ in the given interval using only function evaluations (no derivatives). The algorithm must satisfy all of the following properties for every iteration until termination: (i) all function evaluations must occur inside the current bracketing interval $\\left[a,b\\right]$, (ii) the interval must continue to bracket a root, meaning $f(a)\\cdot f(b)\\le 0$ at all times, and (iii) the sequence of trial points must be chosen by a rule that can exploit information from up to three previously evaluated points if doing so preserves the bracketing property. The routine must terminate when either the width of the current bracket satisfies $\\lvert b-a\\rvert \\le \\varepsilon_x$ or the function value at the current approximation satisfies $\\lvert f(x)\\rvert \\le \\varepsilon_f$, where $\\varepsilon_x=\\varepsilon_f=10^{-10}$. Use a maximum of $100$ iterations per test instance. If the termination conditions are met earlier, return immediately. The returned approximation must lie in the final bracketing interval.\n\nDefine the following test suite of root-finding instances. Each instance specifies a function $f(x)$, an interval $\\left[a,b\\right]$ with $f(a)\\cdot f(b)\\le 0$, and any needed parameters.\n\nTest case A (market-clearing price with constant elasticity demand and linear supply):\n- Variable: price $p$.\n- Demand: $D(p)=A\\,p^{-\\eta}$ with $A=120$ and $\\eta=1.5$.\n- Supply: $S(p)=c_0+c_1 p$ with $c_0=10$ and $c_1=2$.\n- Excess demand: $f(p)=D(p)-S(p)=A\\,p^{-\\eta}-(c_0+c_1 p)$.\n- Interval: $[a,b]=[1,20]$.\n\nTest case B (yield to maturity from price of a level-coupon bond):\n- Variable: yield $y$.\n- Coupon each period: $C=5$, face value: $F=100$, maturity in periods: $T=10$, observed price: $P=95$.\n- Present value function: $\\mathrm{PV}(y)=\\sum_{t=1}^{T}\\dfrac{C}{(1+y)^t}+\\dfrac{F}{(1+y)^T}$.\n- Root function: $f(y)=\\mathrm{PV}(y)-P$.\n- Interval: $[a,b]=[0,0.2]$.\n\nTest case C (implied volatility from Black–Scholes–Merton call price):\n- Variable: volatility $\\sigma$.\n- Underlying price: $S=100$, strike: $K=100$, continuously compounded risk-free rate: $r=0.02$, time to maturity in years: $\\tau=1$, observed call price: $C_{\\text{mkt}}=10$.\n- Black–Scholes–Merton call price: $C(\\sigma)=S\\,N(d_1)-K e^{-r\\tau} N(d_2)$ where $d_1=\\dfrac{\\ln(S/K)+(r+\\tfrac{1}{2}\\sigma^2)\\tau}{\\sigma\\sqrt{\\tau}}$, $d_2=d_1-\\sigma\\sqrt{\\tau}$, and $N(\\cdot)$ is the standard normal cumulative distribution function (CDF).\n- Root function: $f(\\sigma)=C(\\sigma)-C_{\\text{mkt}}$.\n- Interval: $[a,b]=[0.01,1.0]$.\n\nTest case D (boundary root in a zero-coupon yield equation):\n- Variable: yield $y$.\n- Face value: $F=100$, maturity in periods: $T=5$, reference yield: $y_0=0.04$, observed price: $P=F/(1+y_0)^T$.\n- Present value function: $\\mathrm{PV}(y)=\\dfrac{F}{(1+y)^T}$.\n- Root function: $f(y)=\\mathrm{PV}(y)-P$.\n- Interval: $[a,b]=[0.04,0.20]$.\n\nYour program must evaluate all four test cases in the order A, B, C, D using the same generic solver and produce a single line of output containing the four numerical approximations to the roots as a comma-separated list enclosed in square brackets, for example $[x_A,x_B,x_C,x_D]$. Each $x$ should be printed as a floating-point number. No angles are involved. There are no physical units in the answers. The output must appear on a single line exactly in the described format.", "solution": "The problem is subjected to validation and is determined to be **valid**. It is scientifically grounded, well-posed, objective, and provides a complete, contradiction-free specification for a numerical task. The problem asks for the implementation of a specific class of root-finding algorithm to solve several well-defined problems from computational economics and finance.\n\nThe requirements for the algorithm are:\n1.  It must be a bracketing method, ensuring the root remains between two points, $a$ and $b$, such that $f(a) \\cdot f(b) \\le 0$.\n2.  All function evaluations must occur within the current bracketing interval.\n3.  It must use interpolation, based on up to three prior points, to accelerate convergence, but fall back to a safe method if the interpolated point is not satisfactory.\n4.  It must use function evaluations only, with no derivatives.\n5.  Termination must occur when either a tolerance on the interval width, $\\varepsilon_x = 10^{-10}$, or a tolerance on the function value, $\\varepsilon_f = 10^{-10}$, is met.\n\nThese constraints describe a robust, hybrid root-finding algorithm. The standard secant method does not guarantee that iterates remain inside a bracket. Therefore, a more sophisticated method is required. The specified properties are archetypal of the methods developed by Dekker and Brent, which combine a fast, open method (like the secant method or inverse quadratic interpolation) with a safe, closed method (bisection).\n\nThe algorithm implemented here is a variant of Dekker's method, a direct precursor to Brent's method. It satisfies all problem constraints.\n\n**Algorithm Design: Dekker-Brent Method**\n\nThe core of the method is to maintain a bracket $[a, b]$ and a convention that $b$ represents the best current approximation to the root $x^{\\star}$. Thus, at every step, we ensure $|f(b)| \\le |f(a)|$.\n\n1.  **Initialization**: Given an interval $[a, b]$ where $f(a)f(b) \\le 0$, we evaluate $f(a)$ and $f(b)$. If either endpoint is a root within tolerance $\\varepsilon_f$, we terminate. Otherwise, we establish the invariant that $|f(b)| \\le |f(a)|$ by swapping $a$ and $b$ if necessary. The point $c=a$ is stored as the *previous* best guess.\n\n2.  **Iteration**: The main loop consists of generating and evaluating a new trial point, $x_{\\text{next}}$, to shrink the bracket $[a, b]$.\n\n3.  **Trial Point Generation**:\n    *   **Interpolation Step (Secant Method)**: A trial point $s$ is computed using the secant method, which forms a line through the two most recent best guesses: the current best guess $(b, f(b))$ and the previous best guess $(c, f(c))$. The formula is:\n        $$ s = b - f(b) \\frac{b - c}{f(b) - f(c)} $$\n        This step uses two previous points ($b$ and $c$) to achieve super-linear convergence.\n    *   **Bisection Step**: A guaranteed, but slower, fallback is the bisection midpoint $m = (a+b)/2$.\n\n4.  **Hybrid Strategy**: A crucial decision is made at each step. To ensure convergence and satisfy the problem constraint that evaluations occur inside the bracket, the fast secant step $s$ is accepted only if it is \"reasonable\". A simple and effective condition, central to Dekker's method, is to accept $s$ only if it lies between the current best guess $b$ and the bisection midpoint $m$. If this condition fails, we distrust the interpolation and default to the safe bisection point $m$. This ensures $x_{\\text{next}}$ is always within the current bracket $[a, b]$.\n\n5.  **Bracket Update**: After evaluating $f(x_{\\text{next}})$, the bracket is updated. If $f(a)$ and $f(x_{\\text{next}})$ have opposite signs, the new bracket becomes $[a, x_{\\text{next}}]$; otherwise, it becomes $[x_{\\text{next}}, b]$. The new points are then relabeled to maintain the invariant that $b$ is the best guess, and the process repeats.\n\n6.  **Termination**: The loop terminates if the bracket width $|b-a|$ is less than or equal to $\\varepsilon_x$ OR the function value at the best guess $|f(b)|$ is less than or equal to $\\varepsilon_f$.\n\n**Test Case Implementation**\n\nThe four test cases are implemented as Python functions.\n- For test case B (yield to maturity), the present value is calculated by direct summation to maintain numerical stability and correctness for yield values near zero.\n- For test case C (implied volatility), the standard normal cumulative distribution function $N(x)$ is required. It is implemented using the error function, $\\mathrm{erf}(x)$, available in Python's standard `math` library, via the relation $N(x) = \\frac{1}{2}(1 + \\mathrm{erf}(x/\\sqrt{2}))$.\n- For test case D, the root is exactly at the boundary of the initial interval. The solver correctly identifies this on the first check and terminates immediately with the correct answer.\n\nThis robust, principle-based design ensures correct and efficient computation of the roots for all specified test cases.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver.\n    The output is a single line with comma-separated roots in a list format.\n    \"\"\"\n    # Define global constants for the solver.\n    EPS_X = 1e-10\n    EPS_F = 1e-10\n    MAX_ITER = 100\n\n    def dekker_brent_solver(f, a, b, eps_x, eps_f, max_iter):\n        \"\"\"\n        A robust root-finding algorithm based on Dekker's and Brent's methods.\n        It combines a fast interpolation step (secant method) with a safe\n        fallback (bisection method) to guarantee convergence while ensuring the\n        root remains bracketed.\n        \"\"\"\n        fa = f(a)\n        fb = f(b)\n\n        if fa * fb > 0:\n            raise ValueError(\"Root not bracketed in initial interval [a, b].\")\n\n        # Check if endpoints are already the root within tolerance.\n        if abs(fa) <= eps_f:\n            return a\n        if abs(fb) <= eps_f:\n            return b\n\n        # Convention: 'b' is always the current best guess for the root.\n        # 'a' is the contrapoint, ensuring f(a) and f(b) have opposite signs.\n        if abs(fa) < abs(fb):\n            a, b = b, a\n            fa, fb = fb, fa\n        \n        # 'c' is the previous best guess, used for the secant step.\n        c = a\n        fc = fa\n        \n        for _ in range(max_iter):\n            # Check for termination on either interval width or function value.\n            if abs(b - a) <= eps_x or abs(fb) <= eps_f:\n                return b\n\n            s = None\n            # Propose a new point 's' using the secant method (linear interpolation).\n            # This step uses the current best guess (b) and previous best guess (c).\n            if abs(fb - fc) > 1e-15:  # Avoid division by zero or large floating-point errors.\n                s = b - fb * (b - c) / (fb - fc)\n\n            # The bisection midpoint serves as a safe fallback.\n            m = (a + b) / 2\n            \n            # Hybrid strategy: Accept the secant step 's' only if it's reasonable.\n            # \"Reasonable\" means it falls strictly between 'b' and the bisection point 'm'.\n            # This ensures the new point is inside the bracket and promotes convergence.\n            is_secant_step_acceptable = False\n            if s is not None:\n                # The order of b and m is not known, so check both cases.\n                if (b < m and s > b and s < m) or (b > m and s < b and s > m):\n                    is_secant_step_acceptable = True\n\n            if is_secant_step_acceptable:\n                x_next = s\n            else:\n                # If interpolation is untrustworthy, fall back to bisection.\n                x_next = m\n            \n            f_next = f(x_next)\n\n            # Update state for the next iteration: the old 'b' becomes the new 'c'.\n            c, fc = b, fb\n\n            # Update the bracketing interval based on the sign of f_next.\n            if fa * f_next < 0:\n                b, fb = x_next, f_next\n            else:\n                a, fa = x_next, f_next\n\n            # Maintain the invariant that 'b' is the best guess so far (|f(b)| is minimal).\n            if abs(fa) < abs(fb):\n                a, b = b, a\n                fa, fb = fb, fa\n        \n        # If max iterations are reached, return the best approximation found.\n        return b\n\n    # --- Test Case Definitions ---\n\n    # Test Case A: Market-clearing price\n    def f_A(p):\n        A, eta, c0, c1 = 120.0, 1.5, 10.0, 2.0\n        if p <= 0: return float('inf')\n        return A * p**(-eta) - (c0 + c1 * p)\n    \n    # Test Case B: Yield to maturity\n    def f_B(y):\n        C, F, T, P = 5.0, 100.0, 10, 95.0\n        if y <= -1: return float('inf')\n        one_plus_y = 1.0 + y\n        # Direct summation is robust against numerical issues near y=0.\n        terms = [C / (one_plus_y**t) for t in range(1, T + 1)]\n        pv = np.sum(terms) + F / (one_plus_y**T)\n        return pv - P\n    \n    # Test Case C: Implied volatility\n    def f_C(sigma):\n        S, K, r, tau, C_mkt = 100.0, 100.0, 0.02, 1.0, 10.0\n        if sigma <= 0: return -C_mkt # C(0) = 0, so f(0) = -C_mkt\n\n        # Standard Normal CDF N(x) using math.erf from the standard library\n        def N(x):\n            return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))\n        \n        d1 = (math.log(S / K) + (r + 0.5 * sigma**2) * tau) / (sigma * math.sqrt(tau))\n        d2 = d1 - sigma * math.sqrt(tau)\n        call_price = S * N(d1) - K * math.exp(-r * tau) * N(d2)\n        return call_price - C_mkt\n\n    # Test Case D: Boundary root\n    F_D, T_D, y0_D = 100.0, 5, 0.04\n    P_D = F_D / (1.0 + y0_D)**T_D\n    def f_D(y):\n        if y <= -1: return float('inf')\n        return F_D / (1.0 + y)**T_D - P_D\n\n    test_cases = [\n        {'func': f_A, 'a': 1.0, 'b': 20.0},\n        {'func': f_B, 'a': 0.0, 'b': 0.2},\n        {'func': f_C, 'a': 0.01, 'b': 1.0},\n        {'func': f_D, 'a': 0.04, 'b': 0.20},\n    ]\n\n    results = []\n    for case in test_cases:\n        root = dekker_brent_solver(case['func'], case['a'], case['b'], EPS_X, EPS_F, MAX_ITER)\n        results.append(root)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}