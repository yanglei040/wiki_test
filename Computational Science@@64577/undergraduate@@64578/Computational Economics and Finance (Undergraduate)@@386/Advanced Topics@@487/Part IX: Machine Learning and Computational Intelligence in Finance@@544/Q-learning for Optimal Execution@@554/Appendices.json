{"hands_on_practices": [{"introduction": "The first step in any reinforcement learning application is to build the environment in which the agent will operate. This exercise provides direct, hands-on practice in implementing the core dynamics of a simulated financial marketâ€”from how inventory evolves to how trades create price impact. By translating a formal model into a working simulation, you will build the foundational 'world' where an agent can learn to trade optimally [@problem_id:2423600].", "id": "2423600", "problem": "You are given a discrete-time execution environment for selling a fixed inventory over a finite horizon. At each time step, a trader chooses a nonnegative integer action that specifies a quantity to sell. The trade generates immediate cashflow at an execution price that includes temporary price impact, and it permanently moves the mid-price used in subsequent steps. A terminal penalty is applied at the deadline if the full inventory has not been liquidated. The objective is to compute, for each specified parameter set, the total episodic reward obtained from a given action sequence.\n\nFormally, let the time indices be $t \\in \\{0,1,\\dots,T-1\\}$, with deadline at $T$. The state contains the current mid-price $S_t \\in \\mathbb{R}$ and remaining inventory $x_t \\in \\mathbb{N}_0$. The trader selects an intended action $a_t \\in \\mathbb{N}_0$ representing units to sell. The executed quantity is\n$$\na'_t \\equiv \\min\\{a_t, x_t\\}.\n$$\nThe execution price at time $t$ is\n$$\n\\tilde{S}_t \\equiv S_t - \\eta\\, a'_t,\n$$\nwhere $\\eta \\ge 0$ is the temporary impact coefficient. The immediate reward (cashflow) at time $t$ is\n$$\nr_t \\equiv a'_t \\, \\tilde{S}_t = a'_t \\left(S_t - \\eta\\, a'_t\\right).\n$$\nThe inventory and price evolve as\n$$\nx_{t+1} = x_t - a'_t,\\qquad\nS_{t+1} = S_t - \\kappa\\, a'_t + \\epsilon_t,\n$$\nwhere $\\kappa \\ge 0$ is the permanent impact coefficient and $\\{\\epsilon_t\\}_{t=0}^{T-1}$ is a specified deterministic sequence of exogenous price shocks. At the terminal time $T$, a penalty is applied for any remaining inventory:\n$$\nR_T \\equiv -\\lambda\\, x_T^2,\n$$\nwith $\\lambda \\ge 0$. The total episodic reward is\n$$\nG \\equiv \\sum_{t=0}^{T-1} r_t + R_T.\n$$\nAll prices, costs, and rewards are in an arbitrary monetary unit; no physical units are involved.\n\nImplement a program that, for each parameter set in the test suite below, computes $G$ for the provided action sequence. For each case, use the dynamics above with the given parameters. You must treat $a_t$ as intended actions and execute $a'_t = \\min\\{a_t, x_t\\}$ as specified.\n\nTest suite (each case specifies $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$):\n- Case $1$: $S_0 = 100.0$, $x_0 = 5$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, actions $[2,2,1]$, shocks $[0.0,0.0,0.0]$.\n- Case $2$: $S_0 = 100.0$, $x_0 = 5$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, actions $[1,1,1]$, shocks $[0.0,0.0,0.0]$.\n- Case $3$: $S_0 = 100.0$, $x_0 = 0$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, actions $[2,2,2]$, shocks $[0.0,0.0,0.0]$.\n- Case $4$: $S_0 = 50.0$, $x_0 = 3$, $T = 2$, $\\kappa = 0.3$, $\\eta = 1.0$, $\\lambda = 5.0$, actions $[2,2]$, shocks $[1.5,0.0]$.\n- Case $5$: $S_0 = 20.0$, $x_0 = 10$, $T = 2$, $\\kappa = 0.0$, $\\eta = 0.0$, $\\lambda = 100.0$, actions $[0,0]$, shocks $[0.0,0.0]$.\n- Case $6$: $S_0 = 100.0$, $x_0 = 4$, $T = 2$, $\\kappa = 0.5$, $\\eta = 0.25$, $\\lambda = 1.0$, actions $[3,1]$, shocks $[-10.0,0.0]$.\n\nYour program should produce a single line of output containing the results for the six cases as a comma-separated list enclosed in square brackets. Each result must be rounded to six decimal places. The output must be in the exact format:\n- One line only.\n- Square brackets enclosing the list.\n- Values separated by commas.\n- Each value displayed with exactly six digits after the decimal point.", "solution": "The problem statement is valid. It presents a well-posed, scientifically grounded task within the domain of computational finance, specifically concerning optimal execution models. The problem requires the simulation of a trading process over a discrete, finite time horizon, for which all governing equations, parameters, and initial conditions are explicitly and unambiguously provided. The objective is to compute the total episodic reward for a given sequence of actions, which is a deterministic calculation based on the specified model dynamics.\n\nThe model describes the state of the system at time $t$ by the mid-price $S_t$ and the remaining inventory $x_t$. The process is simulated over the time steps $t \\in \\{0, 1, \\dots, T-1\\}$.\n\nThe procedure to calculate the total episodic reward $G$ for a given parameter set $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$ is as follows.\n\nFirst, initialize the total reward accumulator to zero, let us call it $G_{acc} = 0$. The initial state is given by $(S_0, x_0)$.\n\nNext, we iterate through each time step $t$ from $0$ to $T-1$. At each step:\n$1$. Determine the executed quantity $a'_t$. Since the trader cannot sell more inventory than is available, the executed quantity is the minimum of the intended action $a_t$ and the current inventory $x_t$.\n$$\na'_t = \\min\\{a_t, x_t\\}\n$$\n$2$. Calculate the immediate reward $r_t$ obtained at time $t$. This is the cashflow from the sale, which is the product of the executed quantity $a'_t$ and the execution price $\\tilde{S}_t$. The execution price is the mid-price $S_t$ adjusted for a temporary price impact proportional to the size of the trade, with coefficient $\\eta$.\n$$\nr_t = a'_t \\cdot \\tilde{S}_t = a'_t \\cdot (S_t - \\eta \\cdot a'_t)\n$$\n$3$. Add this immediate reward to the accumulated total reward:\n$$\nG_{acc} \\leftarrow G_{acc} + r_t\n$$\n$4$. Update the state variables for the next time step, $t+1$. The inventory is depleted by the executed quantity $a'_t$. The mid-price is updated by accounting for a permanent price impact, proportional to $a'_t$ with coefficient $\\kappa$, and an exogenous price shock $\\epsilon_t$.\n$$\nx_{t+1} = x_t - a'_t\n$$\n$$\nS_{t+1} = S_t - \\kappa \\cdot a'_t + \\epsilon_t\n$$\nThis loop continues until $t=T-1$. After the final step of the loop, the state is $(S_T, x_T)$.\n\nFinally, we apply a terminal penalty, $R_T$, for any inventory $x_T$ that remains unsold at the deadline $T$. The penalty is a quadratic function of the remaining inventory, with a penalty coefficient $\\lambda$.\n$$\nR_T = -\\lambda \\cdot x_T^2\n$$\nThe total episodic reward $G$ is the sum of all immediate rewards accumulated over the trading horizon and the terminal penalty.\n$$\nG = G_{acc} + R_T = \\left(\\sum_{t=0}^{T-1} r_t\\right) + R_T\n$$\nThis procedure yields a unique, deterministic value for $G$ for each provided test case. The computation is a direct application of the given formulae.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total episodic reward for a series of optimal execution problems.\n    \"\"\"\n    # Test suite (S0, x0, T, kappa, eta, lambda, actions, shocks)\n    test_cases = [\n        # Case 1\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [2, 2, 1], [0.0, 0.0, 0.0]),\n        # Case 2\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [1, 1, 1], [0.0, 0.0, 0.0]),\n        # Case 3\n        (100.0, 0, 3, 0.2, 0.5, 10.0, [2, 2, 2], [0.0, 0.0, 0.0]),\n        # Case 4\n        (50.0, 3, 2, 0.3, 1.0, 5.0, [2, 2], [1.5, 0.0]),\n        # Case 5\n        (20.0, 10, 2, 0.0, 0.0, 100.0, [0, 0], [0.0, 0.0]),\n        # Case 6\n        (100.0, 4, 2, 0.5, 0.25, 1.0, [3, 1], [-10.0, 0.0]),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        s0, x0, T, kappa, eta, lambd, actions, shocks = case\n        \n        # Initialize state variables\n        s_t = s0\n        x_t = x0\n        total_reward = 0.0\n\n        # Simulate the execution process over the time horizon\n        for t in range(T):\n            action_t = actions[t]\n            shock_t = shocks[t]\n\n            # 1. Determine executed quantity\n            executed_quantity = min(action_t, x_t)\n\n            # 2. Calculate immediate reward\n            if executed_quantity > 0:\n                execution_price = s_t - eta * executed_quantity\n                immediate_reward = executed_quantity * execution_price\n                total_reward += immediate_reward\n            \n            # 3. Update state for the next time step\n            x_t_plus_1 = x_t - executed_quantity\n            s_t_plus_1 = s_t - kappa * executed_quantity + shock_t\n            \n            # Move to the next state\n            x_t = x_t_plus_1\n            s_t = s_t_plus_1\n\n        # 4. Calculate terminal penalty\n        terminal_penalty = -lambd * (x_t ** 2)\n        total_reward += terminal_penalty\n        \n        results.append(total_reward)\n\n    # Format the results as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "An artificial agent learns by seeking to maximize a reward signal, making the design of the reward function one of the most critical aspects of applied reinforcement learning. This practice focuses on 'reward engineering,' where you will implement a function that balances the competing objectives of achieving a favorable execution price against the costs associated with aggressive actions and incomplete orders. This will give you insight into how abstract financial goals are translated into a concrete mathematical objective for the agent [@problem_id:2423592].", "id": "2423592", "problem": "Consider a single-step reward design for an action-value function (Q-function) in Reinforcement Learning (RL), applied to optimal execution of a single asset in a limit order market. Let the state at a decision time include the prevailing mid-price $m$, the instantaneous bid-ask spread $s&gt;0$, and the agentâ€™s objective side $\\sigma \\in \\{+1,-1\\}$, where $\\sigma=+1$ denotes a buy objective (acquire shares) and $\\sigma=-1$ denotes a sell objective (liquidate shares). During the step, the agent executes $x \\ge 0$ shares at an average execution price $p$, incurring an indicator $I_{\\text{cross}} \\in \\{0,1\\}$ that equals $1$ if the action crosses the spread (a marketable action) and $0$ if the action is passive (non-marketable). After the step, the remaining inventory is $R \\ge 0$. The parameters $\\lambda_{\\text{cross}} \\ge 0$ and $\\lambda_{\\text{rem}} \\ge 0$ are given nonnegative weights.\n\nDefine the per-step reward $r$ as the unique affine function of per-share price improvement and the crossing indicator that simultaneously satisfies the following principles:\n- Price improvement principle: per-share contribution is $\\sigma (m - p)$, rewarding executions that achieve better prices relative to the mid-price consistent with the objective side.\n- Spread-crossing disincentive: a crossing action incurs an additional per-share penalty proportional to the spread $s$, weighted by $\\lambda_{\\text{cross}}$ and activated by $I_{\\text{cross}}$.\n- Unexecuted-inventory penalty: a remaining-inventory penalty proportional to $R$, weighted by $\\lambda_{\\text{rem}}$.\n\nThese principles imply the reward\n$$\nr \\;=\\; x \\,\\big[\\, \\sigma (m - p) \\;-\\; \\lambda_{\\text{cross}} \\, I_{\\text{cross}} \\, s \\,\\big] \\;-\\; \\lambda_{\\text{rem}} \\, R.\n$$\n\nImplement a program that, for each test case below, computes $r$ using the input values for $(m, s, \\sigma, p, x, I_{\\text{cross}}, R, \\lambda_{\\text{cross}}, \\lambda_{\\text{rem}})$, with all arithmetic performed in real numbers. The outputs must be rounded to six decimal places.\n\nUse the following test suite:\n- Test case 1 (buy, passive fill at bid): $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=99.99$, $x=100$, $I_{\\text{cross}}=0$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$.\n- Test case 2 (buy, market fill at ask): $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=100.01$, $x=100$, $I_{\\text{cross}}=1$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$.\n- Test case 3 (sell, passive fill at ask): $m=250.50$, $s=0.10$, $\\sigma=-1$, $p=250.55$, $x=50$, $I_{\\text{cross}}=0$, $R=0$, $\\lambda_{\\text{cross}}=0.25$, $\\lambda_{\\text{rem}}=0.0$.\n- Test case 4 (zero spread boundary): $m=10.00$, $s=0.00$, $\\sigma=+1$, $p=10.00$, $x=10$, $I_{\\text{cross}}=1$, $R=0$, $\\lambda_{\\text{cross}}=999.0$, $\\lambda_{\\text{rem}}=0.0$.\n- Test case 5 (no execution, remaining inventory penalty only): $m=100.00$, $s=0.20$, $\\sigma=+1$, $p=100.00$, $x=0$, $I_{\\text{cross}}=0$, $R=1000$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.01$.\n- Test case 6 (sell, market fill at bid with partial completion): $m=50.00$, $s=0.04$, $\\sigma=-1$, $p=49.98$, $x=200$, $I_{\\text{cross}}=1$, $R=300$, $\\lambda_{\\text{cross}}=0.1$, $\\lambda_{\\text{rem}}=0.05$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$), with each $r_i$ rounded to six decimal places and no additional text.", "solution": "The problem presented requires the computation of a single-step reward, denoted $r$, for a Q-learning agent engaged in optimal execution. The problem statement is subjected to validation before a solution is attempted.\n\nFirst, we extract the given information.\nThe state and action space variables are defined:\n- Mid-price: $m$\n- Bid-ask spread: $s > 0$\n- Agent's objective side: $\\sigma \\in \\{+1, -1\\}$, where $+1$ is for buying and $-1$ is for selling.\n- Executed shares: $x \\ge 0$\n- Average execution price: $p$\n- Spread-crossing indicator: $I_{\\text{cross}} \\in \\{0, 1\\}$\n- Remaining inventory: $R \\ge 0$\n- Nonnegative weights: $\\lambda_{\\text{cross}} \\ge 0$ and $\\lambda_{\\text{rem}} \\ge 0$.\n\nThe single-step reward function $r$ is explicitly defined as:\n$$\nr \\;=\\; x \\,\\big[\\, \\sigma (m - p) \\;-\\; \\lambda_{\\text{cross}} \\, I_{\\text{cross}} \\, s \\,\\big] \\;-\\; \\lambda_{\\text{rem}} \\, R.\n$$\nThe problem is to compute $r$ for a given test suite of parameters.\n\nNext, we validate the problem.\n1.  **Scientific Grounding**: The formulation is standard in computational finance and algorithmic trading. The reward function is a linear combination of three key performance indicators: implementation shortfall (price improvement), spread-crossing costs, and inventory penalty. These are fundamental, well-established concepts in market microstructure. The model is a valid, if simplified, representation of the trade-offs in optimal execution.\n2.  **Well-Posedness**: The problem requires a direct evaluation of a given algebraic expression. For each set of inputs provided in the test cases, the formula yields a single, unique, and well-defined real number. The setup is not under- or over-constrained.\n3.  **Objectivity**: The problem is stated in precise, mathematical language, free of ambiguity, subjectivity, or opinion. One test case includes $s=0$, which technically violates the premise $s>0$. However, this is a boundary case and does not invalidate the formula itself, which is well-defined for $s=0$. The problem is therefore considered valid.\n\nThe problem is valid. We proceed with the solution. The calculation is a direct application of the provided formula for each test case.\n\nThe reward function can be decomposed into three components as prescribed by the guiding principles:\n1.  **Price Improvement Term**: $x \\cdot \\sigma (m - p)$. This term measures the performance against the mid-price $m$ at the time of the decision. If $\\sigma = +1$ (buy), a positive reward is achieved if the execution price $p$ is less than $m$. If $\\sigma = -1$ (sell), a positive reward is achieved if $p$ is greater than $m$. This aligns with the objective of buying low and selling high. The total contribution is scaled by the number of shares executed, $x$.\n2.  **Spread-Crossing Penalty**: $-x \\cdot \\lambda_{\\text{cross}} \\cdot I_{\\text{cross}} \\cdot s$. This term penalizes aggressive actions that \"cross the spread\" to gain immediate execution. The indicator $I_{\\text{cross}}=1$ activates the penalty. The magnitude is proportional to the number of shares $x$, the prevailing spread $s$, and the parameter $\\lambda_{\\text{cross}}$. If the action is passive ($I_{\\text{cross}}=0$), this term is zero.\n3.  **Remaining Inventory Penalty**: $-\\lambda_{\\text{rem}} \\cdot R$. This term penalizes the agent for failing to complete its execution objective, as represented by the remaining inventory $R$. Its magnitude is controlled by the parameter $\\lambda_{\\text{rem}}$.\n\nWe now compute the reward $r$ for each test case.\n\n**Test Case 1**:\n- Inputs: $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=99.99$, $x=100$, $I_{\\text{cross}}=0$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$.\n- Calculation:\n$$\nr_1 = 100 \\cdot [+1 \\cdot (100.00 - 99.99) - 0.5 \\cdot 0 \\cdot 0.02] - 0.001 \\cdot 900\n$$\n$$\nr_1 = 100 \\cdot (0.01 - 0) - 0.9 = 1.0 - 0.9 = 0.1\n$$\n\n**Test Case 2**:\n- Inputs: $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=100.01$, $x=100$, $I_{\\text{cross}}=1$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$.\n- Calculation:\n$$\nr_2 = 100 \\cdot [+1 \\cdot (100.00 - 100.01) - 0.5 \\cdot 1 \\cdot 0.02] - 0.001 \\cdot 900\n$$\n$$\nr_2 = 100 \\cdot (-0.01 - 0.01) - 0.9 = 100 \\cdot (-0.02) - 0.9 = -2.0 - 0.9 = -2.9\n$$\n\n**Test Case 3**:\n- Inputs: $m=250.50$, $s=0.10$, $\\sigma=-1$, $p=250.55$, $x=50$, $I_{\\text{cross}}=0$, $R=0$, $\\lambda_{\\text{cross}}=0.25$, $\\lambda_{\\text{rem}}=0.0$.\n- Calculation:\n$$\nr_3 = 50 \\cdot [-1 \\cdot (250.50 - 250.55) - 0.25 \\cdot 0 \\cdot 0.10] - 0.0 \\cdot 0\n$$\n$$\nr_3 = 50 \\cdot [-1 \\cdot (-0.05) - 0] - 0 = 50 \\cdot (0.05) = 2.5\n$$\n\n**Test Case 4**:\n- Inputs: $m=10.00$, $s=0.00$, $\\sigma=+1$, $p=10.00$, $x=10$, $I_{\\text{cross}}=1$, $R=0$, $\\lambda_{\\text{cross}}=999.0$, $\\lambda_{\\text{rem}}=0.0$.\n- Calculation:\n$$\nr_4 = 10 \\cdot [+1 \\cdot (10.00 - 10.00) - 999.0 \\cdot 1 \\cdot 0.00] - 0.0 \\cdot 0\n$$\n$$\nr_4 = 10 \\cdot (0 - 0) - 0 = 0.0\n$$\n\n**Test Case 5**:\n- Inputs: $m=100.00$, $s=0.20$, $\\sigma=+1$, $p=100.00$, $x=0$, $I_{\\text{cross}}=0$, $R=1000$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.01$.\n- Calculation: Since $x=0$, the entire first term is $0$.\n$$\nr_5 = 0 \\cdot [\\dots] - 0.01 \\cdot 1000 = 0 - 10.0 = -10.0\n$$\n\n**Test Case 6**:\n- Inputs: $m=50.00$, $s=0.04$, $\\sigma=-1$, $p=49.98$, $x=200$, $I_{\\text{cross}}=1$, $R=300$, $\\lambda_{\\text{cross}}=0.1$, $\\lambda_{\\text{rem}}=0.05$.\n- Calculation:\n$$\nr_6 = 200 \\cdot [-1 \\cdot (50.00 - 49.98) - 0.1 \\cdot 1 \\cdot 0.04] - 0.05 \\cdot 300\n$$\n$$\nr_6 = 200 \\cdot [-1 \\cdot (0.02) - 0.004] - 15.0 = 200 \\cdot (-0.02 - 0.004) - 15.0\n$$\n$$\nr_6 = 200 \\cdot (-0.024) - 15.0 = -4.8 - 15.0 = -19.8\n$$\n\nThe final results, rounded to six decimal places, are: $r_1=0.100000$, $r_2=-2.900000$, $r_3=2.500000$, $r_4=0.000000$, $r_5=-10.000000$, and $r_6=-19.800000$. These will be implemented in the program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the single-step reward 'r' for a series of test cases in an\n    optimal execution problem, based on a given formula.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple represents a test case with parameters:\n    # (m, s, sigma, p, x, I_cross, R, lambda_cross, lambda_rem)\n    test_cases = [\n        (100.00, 0.02, +1, 99.99, 100, 0, 900, 0.5, 0.001),  # Test case 1\n        (100.00, 0.02, +1, 100.01, 100, 1, 900, 0.5, 0.001),  # Test case 2\n        (250.50, 0.10, -1, 250.55, 50, 0, 0, 0.25, 0.0),      # Test case 3\n        (10.00, 0.00, +1, 10.00, 10, 1, 0, 999.0, 0.0),       # Test case 4\n        (100.00, 0.20, +1, 100.00, 0, 0, 1000, 0.5, 0.01),     # Test case 5\n        (50.00, 0.04, -1, 49.98, 200, 1, 300, 0.1, 0.05),     # Test case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack the parameters for clarity\n        m, s, sigma, p, x, I_cross, R, lambda_cross, lambda_rem = case\n        \n        # Calculate the reward 'r' using the provided formula:\n        # r = x * [sigma * (m - p) - lambda_cross * I_cross * s] - lambda_rem * R\n        \n        price_improvement_term = sigma * (m - p)\n        spread_crossing_penalty = lambda_cross * I_cross * s\n        execution_component = x * (price_improvement_term - spread_crossing_penalty)\n        \n        inventory_penalty = lambda_rem * R\n        \n        reward = execution_component - inventory_penalty\n        \n        # Round the result to six decimal places as required.\n        rounded_reward = round(reward, 6)\n        results.append(f\"{rounded_reward:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "With the environment and reward function in place, the final component is the agent's learning algorithm. This capstone exercise guides you through implementing the complete tabular Q-learning algorithm, allowing your agent to autonomously discover an optimal trading strategy. Furthermore, by experimenting with the discount factor $\\gamma$, you will gain a deep, practical understanding of how RL agents balance the trade-off between myopic, short-term gains and far-sighted, long-term value [@problem_id:2423640].", "id": "2423640", "problem": "You are given a stylized optimal execution problem formulated as a Markov Decision Process (MDP) in discrete time with a Reinforcement Learning (RL) agent that learns an action-value function via the standard Q-learning recursion. The goal is to examine how the discount factor $\\,\\gamma\\,$ affects the agentâ€™s effective trading horizon, defined as the number of time steps it takes to fully liquidate a fixed inventory under the greedy policy induced by the learned action-value function.\n\nThe MDP is specified as follows.\n\n- Time: $\\,t \\in \\{0,1,\\dots,T\\}\\,$ with a fixed horizon $\\,T\\,$. The episode terminates when $\\,t=T\\,$ or when inventory hits zero, whichever occurs first.\n- State: $\\,s_t=(t,x_t)\\,$ where inventory $\\,x_t \\in \\{0,1,\\dots,X_0\\}\\,$ is the number of discrete units remaining at time $\\,t\\,$.\n- Actions: At state $\\,s_t=(t,x_t)\\,$ the agent chooses an integer trade size $\\,a_t \\in \\{0,1,\\dots,\\min(s_{\\max},x_t)\\}\\,$ representing the number of units to sell during period $\\,t\\,$.\n- Transition: Inventory evolves deterministically as $\\,x_{t+1}=x_t - a_t\\,$ and time increments $\\,t \\mapsto t+1\\,$.\n- Immediate reward: For taking action $\\,a_t\\,$ in state $\\,s_t=(t,x_t)\\,$, the reward is\n$$\nr_t \\;=\\; -\\big(k\\,a_t^2 \\;+\\; \\lambda\\,x_t^2\\big),\n$$\nwhere $\\,k>0\\,$ is the transaction cost curvature and $\\,\\lambda>0\\,$ is the inventory holding-risk penalty. The terminal state carries zero continuation value.\n- Objective: For a fixed discount factor $\\,\\gamma \\in [0,1]\\,$, the agent seeks to maximize the expected discounted return $\\,\\sum_{t=0}^{T-1} \\gamma^t\\,r_t\\,$.\n\nThe Q-learning agent updates action-value estimates according to the recursion\n$$\nQ(s_t,a_t) \\;\\leftarrow\\; (1-\\alpha)\\,Q(s_t,a_t) \\;+\\; \\alpha \\left[r_t \\;+\\; \\gamma \\,\\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1},a')\\right],\n$$\nwith the convention that if $\\,s_{t+1}\\,$ is terminal (i.e., $\\,t+1=T\\,$ or $\\,x_{t+1}=0\\,$), then the target reduces to $\\,r_t\\,$ because there is no continuation. Here $\\,\\alpha \\in (0,1]\\,$ is the learning rate and $\\,\\mathcal{A}(s)\\,$ denotes the feasible actions at state $\\,s\\,$. During learning, the agent follows an $\\varepsilon$-greedy behavior policy: with probability $\\,\\varepsilon\\,$ it selects a feasible action uniformly at random, and with probability $\\,1-\\varepsilon\\,$ it selects a greedy action that maximizes the current $\\,Q\\,$-estimate. Ties in $\\arg\\max$ are always broken in favor of the largest action (i.e., the largest $\\,a\\,$ among the maximizers).\n\nAll randomness used for action selection during training must be generated from a pseudo-random number generator initialized with the seed $\\,0\\,$ to ensure reproducibility.\n\nParameter values to use are:\n- Horizon $\\,T=10\\,$.\n- Initial inventory $\\,X_0=10\\,$.\n- Maximum per-period sell size $\\,s_{\\max}=3\\,$.\n- Transaction cost curvature $\\,k=0.05\\,$.\n- Inventory holding-risk penalty $\\,\\lambda=0.10\\,$.\n- Learning rate $\\,\\alpha=0.10\\,$.\n- Exploration probability $\\,\\varepsilon=0.20\\,$.\n- Number of training episodes $\\,N_{\\text{episodes}}=20000\\,$.\n- Start each training episode from the fixed initial state $\\,s_0=(0,X_0)\\,$.\n\nFor each discount factor $\\,\\gamma\\,$ in the test suite below, you must:\n- Train the Q-learning agent under the above specifications.\n- After training, derive the greedy policy induced by the learned action-value function.\n- From the fixed initial state $\\,s_0=(0,X_0)\\,$, simulate the greedy policy without exploration to obtain the liquidation time $\\,L(\\gamma)\\,$ defined as the smallest integer $\\,\\ell \\in \\{0,1,\\dots,T\\}\\,$ such that $\\,x_{\\ell}=0\\,$. If inventory is not fully liquidated by $\\,T\\,$, then define $\\,L(\\gamma)=T\\,$.\n\nTest suite of discount factors:\n- $\\,\\gamma \\in \\{0.0,\\,0.3,\\,0.6,\\,0.9,\\,0.99,\\,1.0\\}\\,$.\n\nYour program must output the six liquidation times as a single line containing a comma-separated list enclosed in square brackets, in the order of the test suite. The required final output format is:\n- A single line: for example, $\\,[1,2,3,4,5,6]\\,$, where each entry is an integer liquidation time corresponding to the same-position discount factor in the test suite. No spaces are permitted in the output line.", "solution": "The problem presented is a valid and well-posed optimal control problem within the domain of computational finance, specifically, optimal trade execution. It requires the implementation of the Q-learning algorithm to solve a discrete-time Markov Decision Process (MDP). I will first elucidate the theoretical underpinnings and then describe the algorithmic procedure for obtaining the solution.\n\nThe problem is formulated as an MDP defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$.\n- The state space $\\mathcal{S}$ is composed of pairs $s_t=(t,x_t)$, where $t \\in \\{0, 1, \\dots, T\\}$ is the time step and $x_t \\in \\{0, 1, \\dots, X_0\\}$ is the remaining inventory. With $T=10$ and $X_0=10$, the number of states is $(T+1) \\times (X_0+1) = 11 \\times 11 = 121$.\n- The action space $\\mathcal{A}(s_t)$ at state $s_t=(t,x_t)$ consists of the set of allowed trade sizes $a_t \\in \\{0, 1, \\dots, \\min(s_{\\max}, x_t)\\}$, where $s_{\\max}=3$.\n- The transition function is deterministic. Given state $s_t=(t,x_t)$ and action $a_t$, the next state is $s_{t+1}=(t+1, x_t - a_t)$.\n- The immediate reward function $r(s_t, a_t)$ is given by $r_t = -\\big(k\\,a_t^2 + \\lambda\\,x_t^2\\big)$. This function captures the fundamental trade-off in optimal execution: selling a large quantity $a_t$ incurs a high quadratic transaction cost ($k\\,a_t^2$), while holding a large inventory $x_t$ incurs a high quadratic holding risk cost ($\\lambda\\,x_t^2$). The agent's goal is to minimize the total accumulated cost, which is equivalent to maximizing the total accumulated negative reward.\n- The discount factor $\\gamma \\in [0,1]$ determines the present value of future rewards.\n\nThe objective is to find an optimal policy $\\pi^*: \\mathcal{S} \\to \\mathcal{A}$ that maximizes the expected discounted sum of rewards, known as the value function, for any starting state. We use Q-learning, a model-free reinforcement learning algorithm, to find the optimal action-value function $Q^*(s,a)$, which represents the maximum expected return starting from state $s$, taking action $a$, and thereafter following the optimal policy. The optimal action-value function satisfies the Bellman optimality equation:\n$$\nQ^*(s,a) = \\mathbb{E}\\left[r + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^*(s', a') \\mid s, a\\right]\n$$\nwhere $s'$ is the state following $s$ after taking action $a$. Given the deterministic transitions, this simplifies to:\n$$\nQ^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q^*(s_{t+1}, a')\n$$\nQ-learning iteratively approximates $Q^*$ by updating an estimate, $Q(s,a)$, using samples from interaction with the environment. The update rule is:\n$$\nQ(s_t, a_t) \\leftarrow (1-\\alpha)Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1}, a')\\right]\n$$\nHere, $\\alpha=0.10$ is the learning rate. An $\\varepsilon$-greedy policy with $\\varepsilon=0.20$ is used during training to ensure exploration of the state-action space. The random seed is fixed to $0$ for reproducibility.\n\nThe role of the discount factor $\\gamma$ is central to this problem. It dictates the agent's time preference.\n- For $\\gamma \\approx 0$, the agent is myopic and heavily discounts future rewards (costs). It primarily seeks to maximize the immediate reward $r_t = -k\\,a_t^2 - \\lambda\\,x_t^2$. Since $x_t$ is fixed at state $s_t$, this simplifies to maximizing $-k\\,a_t^2$, which is achieved by minimizing $a_t$. The agent learns to trade as little as possible, leading to a very long or incomplete liquidation. The liquidation time $L(\\gamma)$ should be large, likely $T=10$.\n- For $\\gamma \\approx 1$, the agent is far-sighted. The term $\\gamma \\max_{a'} Q(s_{t+1}, a')$ becomes significant. This term carries information about all future costs. The agent recognizes that holding inventory ($x > 0$) will incur a stream of future holding costs. To minimize the total sum of costs, it is incentivized to liquidate inventory more quickly, even if it means incurring higher immediate transaction costs. This will result in a shorter liquidation time.\n\nTherefore, we expect the liquidation time $L(\\gamma)$ to be a non-increasing function of the discount factor $\\gamma$.\n\nThe solution is implemented as follows:\n1. For each given value of $\\gamma$, a Q-table of dimensions $(T+1, X_0+1, s_{\\max}+1)$ is initialized to zeros.\n2. The agent is trained for $N_{\\text{episodes}} = 20000$ episodes. In each episode, starting from $s_0=(0, 10)$, the agent interacts with the environment until a terminal state ($t=T$ or $x=0$) is reached.\n3. At each step, an action is chosen via the $\\varepsilon$-greedy policy. The tie-breaking rule, which favors the largest trade size among actions with identical Q-values, is strictly enforced.\n4. The Q-table is updated according to the specified recursion.\n5. After training, the greedy policy $\\pi_G(s) = \\arg\\max_a Q(s,a)$ is extracted. The same tie-breaking rule applies.\n6. A single, deterministic simulation is run starting from $s_0=(0, 10)$ and following $\\pi_G$. The number of time steps required to bring the inventory $x_t$ to $0$ is recorded as the liquidation time $L(\\gamma)$. If the inventory is not zero by $t=T$, $L(\\gamma)$ is set to $T$.\nThis process is repeated for all specified $\\gamma$ values, and the resulting liquidation times are reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal execution problem using Q-learning for a suite of discount factors.\n    \"\"\"\n    \n    # Problem parameters\n    T = 10\n    X0 = 10\n    S_MAX = 3\n    K = 0.05\n    LAMBDA = 0.10\n\n    # Q-learning parameters\n    ALPHA = 0.10\n    EPSILON = 0.20\n    N_EPISODES = 20000\n    SEED = 0\n    \n    # Test suite for the discount factor\n    gammas = [0.0, 0.3, 0.6, 0.9, 0.99, 1.0]\n    \n    liquidation_times = []\n\n    for gamma in gammas:\n        # Initialize Q-table: Q(t, x, a)\n        # Dimensions: (time_steps, inventory_levels, action_choices)\n        q_table = np.zeros((T + 1, X0 + 1, S_MAX + 1))\n        \n        # Initialize pseudo-random number generator for reproducibility\n        rng = np.random.default_rng(SEED)\n\n        # Main training loop\n        for _ in range(N_EPISODES):\n            t = 0\n            x = X0\n            \n            # An episode runs until a terminal state is reached\n            while t < T and x > 0:\n                current_state_t = t\n                current_state_x = x\n\n                # Determine the set of feasible actions\n                feasible_actions = list(range(min(S_MAX, current_state_x) + 1))\n\n                # Epsilon-greedy action selection\n                if rng.random() < EPSILON:\n                    # Exploration: choose a random feasible action\n                    action = rng.choice(feasible_actions)\n                else:\n                    # Exploitation: choose the best action based on Q-values\n                    q_values_for_state = q_table[current_state_t, current_state_x, feasible_actions]\n                    max_q = np.max(q_values_for_state)\n                    \n                    # Tie-breaking: choose the largest action among those with max Q-value\n                    best_actions = [a for i, a in enumerate(feasible_actions) if q_values_for_state[i] == max_q]\n                    action = max(best_actions)\n\n                # Execute action: calculate reward and find next state\n                reward = -(K * action**2 + LAMBDA * current_state_x**2)\n                next_t = t + 1\n                next_x = x - action\n\n                # Q-table update\n                is_next_state_terminal = (next_t == T) or (next_x == 0)\n                \n                if is_next_state_terminal:\n                    q_max_next = 0.0\n                else:\n                    next_feasible_actions = list(range(min(S_MAX, next_x) + 1))\n                    q_max_next = np.max(q_table[next_t, next_x, next_feasible_actions])\n                \n                target = reward + gamma * q_max_next\n                \n                old_q_value = q_table[current_state_t, current_state_x, action]\n                q_table[current_state_t, current_state_x, action] = \\\n                    (1 - ALPHA) * old_q_value + ALPHA * target\n\n                # Transition to the next state\n                t = next_t\n                x = next_x\n        \n        # After training, simulate the greedy policy to find liquidation time\n        t_sim = 0\n        x_sim = X0\n        liquidation_time = T # Default if not liquidated by T\n\n        while t_sim < T and x_sim > 0:\n            current_state_t_sim = t_sim\n            current_state_x_sim = x_sim\n\n            # Select greedy action with the apecified tie-breaking rule\n            sim_feasible_actions = list(range(min(S_MAX, current_state_x_sim) + 1))\n            q_values_sim = q_table[current_state_t_sim, current_state_x_sim, sim_feasible_actions]\n            max_q_sim = np.max(q_values_sim)\n            best_actions_sim = [a for i, a in enumerate(sim_feasible_actions) if q_values_sim[i] == max_q_sim]\n            action_sim = max(best_actions_sim)\n\n            x_sim -= action_sim\n            t_sim += 1\n            \n            if x_sim == 0:\n                liquidation_time = t_sim\n                break\n        \n        liquidation_times.append(liquidation_time)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, liquidation_times))}]\")\n\nsolve()\n```"}]}