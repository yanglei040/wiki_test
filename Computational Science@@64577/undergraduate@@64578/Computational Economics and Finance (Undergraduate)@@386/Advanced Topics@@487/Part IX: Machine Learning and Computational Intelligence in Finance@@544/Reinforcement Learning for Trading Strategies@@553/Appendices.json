{"hands_on_practices": [{"introduction": "Before an agent can learn to trade, it needs a virtual world in which to practice. This first exercise guides you through the foundational task of building a trading environment simulation. You will implement the core mechanics of wealth accumulation, transaction costs, and a crucial risk management rule known as a 'circuit breaker' [@problem_id:2426620], which halts trading if the agent's drawdown exceeds a predefined threshold. Mastering this simulation is the first step toward creating a robust testbed for any reinforcement learning strategy.", "id": "2426620", "problem": "You must write a complete, runnable program that simulates a discrete-time trading environment with a mathematically specified \"circuit breaker\" that halts trading when the realized drawdown exceeds a fixed threshold. The environment evolves over time indices $t \\in \\{0,1,\\dots,T\\}$ with a given strictly positive price sequence $\\{p_t\\}_{t=0}^{T}$. The agent selects an action sequence $\\{a_t\\}_{t=0}^{T-1}$ where each $a_t \\in \\{-1,0,1\\}$. The initial wealth is $W_0&gt;0$, the trading cost rate is $\\kappa \\in [0,1)$, and the drawdown threshold is $\\bar{d} \\in [0,1)$. Let the one-step gross return on the risky asset be defined by\n$$\nr_{t+1} \\equiv \\frac{p_{t+1}-p_t}{p_t}, \\quad t=0,1,\\dots,T-1.\n$$\nThe wealth update from time $t$ to $t+1$ is given by\n$$\nW_{t+1} \\equiv W_t \\left(1 + a_t\\, r_{t+1}\\right) - \\kappa\\, W_t \\, \\mathbf{1}\\{a_t \\neq a_{t-1}\\},\n$$\nwith the convention $a_{-1} \\equiv 0$. Define the running high-water mark $H_t$ and the drawdown $D_t$ by\n$$\nH_t \\equiv \\max_{0 \\le s \\le t} W_s, \\qquad D_t \\equiv 1 - \\frac{W_t}{H_t},\n$$\nfor $t \\ge 0$, with $H_0 \\equiv W_0$ and $D_0 \\equiv 0$. The circuit breaker halts the process at the first time\n$$\n\\tau \\equiv \\min\\{t \\in \\{1,\\dots,T\\} : D_t &gt; \\bar{d}\\},\n$$\nif such a time exists; otherwise, set $\\tau \\equiv T$. The simulation must stop at time $\\tau$ and not apply any subsequent updates. For each test case, your program must compute:\n- a halting indicator $h$, where $h=1$ if the circuit breaker triggers (i.e., $\\tau &lt; T$) and $h=0$ otherwise,\n- the final wealth $W_{\\tau}$,\n- the final drawdown $D_{\\tau}$,\n- the final time index $\\tau$.\n\nAll real-valued outputs must be rounded to $6$ decimal places. Percentages such as the drawdown threshold and drawdown must be expressed as decimals (for example, $0.05$ for five percent), not using a percentage sign.\n\nUse the following test suite of parameter values. Each test case is a tuple $\\left(\\{p_t\\}_{t=0}^{T}, \\{a_t\\}_{t=0}^{T-1}, W_0, \\kappa, \\bar{d}\\right)$, where the price sequence and action sequence are provided explicitly:\n\n- Test Case $1$ (no halt under moderate fluctuations):\n  - Prices: $[100,\\,101,\\,99,\\,100,\\,103]$\n  - Actions: $[1,\\,1,\\,1,\\,1]$\n  - $W_0 = 1.0$, $\\kappa = 0$, $\\bar{d} = 0.2$.\n\n- Test Case $2$ (boundary case where drawdown equals threshold exactly, which must not halt):\n  - Prices: $[100,\\,90,\\,100]$\n  - Actions: $[1,\\,1]$\n  - $W_0 = 1.0$, $\\kappa = 0$, $\\bar{d} = 0.1$.\n\n- Test Case $3$ (immediate halt due to a small threshold and an initial loss):\n  - Prices: $[100,\\,98,\\,200]$\n  - Actions: $[1,\\,1]$\n  - $W_0 = 1.0$, $\\kappa = 0$, $\\bar{d} = 0.01$.\n\n- Test Case $4$ (halt caused by trading costs despite zero returns):\n  - Prices: $[100,\\,100,\\,100,\\,100]$\n  - Actions: $[1,\\,-1,\\,1]$\n  - $W_0 = 1.0$, $\\kappa = 0.03$, $\\bar{d} = 0.05$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case reported as a list in the order $[h, W_{\\tau}, D_{\\tau}, \\tau]$. Specifically, the required output format is\n$$\n\\big[ [h_1, W_{\\tau,1}, D_{\\tau,1}, \\tau_1], [h_2, W_{\\tau,2}, D_{\\tau,2}, \\tau_2], [h_3, W_{\\tau,3}, D_{\\tau,3}, \\tau_3], [h_4, W_{\\tau,4}, D_{\\tau,4}, \\tau_4] \\big],\n$$\nwith no additional text. All floats must be rounded to $6$ decimal places as specified.", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n\nThe provided mathematical model is defined by the following components:\n- Time indices: $t \\in \\{0, 1, \\dots, T\\}$.\n- A strictly positive price sequence: $\\{p_t\\}_{t=0}^{T}$.\n- An agent's action sequence: $\\{a_t\\}_{t=0}^{T-1}$, where $a_t \\in \\{-1, 0, 1\\}$.\n- Initial wealth: $W_0 > 0$.\n- Trading cost rate: $\\kappa \\in [0, 1)$.\n- Drawdown threshold: $\\bar{d} \\in [0, 1)$.\n\nThe dynamics of the system are governed by these equations:\n- One-step gross return: $r_{t+1} \\equiv \\frac{p_{t+1}-p_t}{p_t}$, for $t=0, 1, \\dots, T-1$.\n- Wealth update: $W_{t+1} \\equiv W_t \\left(1 + a_t\\, r_{t+1}\\right) - \\kappa\\, W_t \\, \\mathbf{1}\\{a_t \\neq a_{t-1}\\}$, with the convention $a_{-1} \\equiv 0$.\n- Running high-water mark: $H_t \\equiv \\max_{0 \\le s \\le t} W_s$, with $H_0 \\equiv W_0$.\n- Drawdown: $D_t \\equiv 1 - \\frac{W_t}{H_t}$, with $D_0 \\equiv 0$.\n- Halting time (circuit breaker): $\\tau \\equiv \\min\\{t \\in \\{1,\\dots,T\\} : D_t > \\bar{d}\\}$, with $\\tau \\equiv T$ if the set is empty.\n\nThe required outputs for each test case are:\n- $h$: A halting indicator ($1$ if $\\tau < T$, $0$ otherwise).\n- $W_{\\tau}$: The final wealth at time $\\tau$.\n- $D_{\\tau}$: The final drawdown at time $\\tau$.\n- $\\tau$: The final time index.\n\nThe test suite consists of four specific cases with all necessary parameters provided.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem describes a discrete-time simulation of a trading strategy. The model incorporates standard elements from quantitative finance, including returns, transaction costs, and a drawdown-based risk management rule. The mathematical formulation is logically consistent and based on established principles in financial modeling. The problem is scientifically sound.\n- **Well-Posed**: The problem is formulated as a deterministic initial value problem. Given the initial state ($W_0$, $a_{-1}=0$) and the input sequences of prices and actions, the state of the system ($W_t, H_t, D_t$) at any future time $t$ is uniquely determined. The halting condition is unambiguous (using a strict inequality, $D_t > \\bar{d}$), ensuring that the stopping time $\\tau$ is well-defined. Thus, a unique solution exists for each test case.\n- **Objective**: The problem is stated in precise, formal mathematical language. All terms are defined explicitly, leaving no room for subjective interpretation.\n- **Completeness and Consistency**: All parameters, including price and action sequences, initial wealth, cost rate, and drawdown threshold, are provided for each test case. The length of the price sequence determines the total duration $T$, and the action sequence has the correct corresponding length of $T$. The constraints on parameters ($p_t > 0$, $W_0>0$, etc.) are consistent and prevent issues such as division by zero.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is scientifically grounded, well-posed, objective, and self-contained. A solution will be provided.\n\n### Solution Design\n\nThe problem requires a discrete-time simulation of an agent's wealth under a specific trading strategy and risk constraint. The simulation evolves step-by-step, updating the system's state variables at each time index.\n\nLet the state of the system at time $t$ be characterized by the wealth $W_t$, the high-water mark $H_t$, and the drawdown $D_t$. The simulation proceeds as follows:\n\n1.  **Initialization (at $t=0$)**:\n    - The initial wealth $W_0$ is given.\n    - The initial high-water mark is $H_0 = W_0$.\n    - The initial drawdown is $D_0 = 1 - W_0/H_0 = 0$.\n    - The action at the previous step is set by convention to $a_{-1} = 0$.\n    - The total simulation horizon $T$ is determined by the length of the given price sequence, specifically $T = \\text{length}(\\{p_t\\}) - 1$.\n\n2.  **Time-Stepping Loop (for $t = 0, 1, \\dots, T-1$)**:\n    At each step $t$, the state for the next time index, $t+1$, is computed.\n    - **a. Calculate Return**: The one-step gross return on the risky asset is computed from the price data:\n      $$r_{t+1} = \\frac{p_{t+1} - p_t}{p_t}$$\n    - **b. Calculate Trading Cost**: A trading cost is incurred if the action changes from the previous step. The cost is:\n      $$C_t = \\kappa\\, W_t \\, \\mathbf{1}\\{a_t \\neq a_{t-1}\\}$$\n      where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, which is $1$ if the condition is true and $0$ otherwise.\n    - **c. Update Wealth**: The wealth is updated based on the investment return and the trading cost:\n      $$W_{t+1} = W_t(1 + a_t r_{t+1}) - C_t$$\n    - **d. Update High-Water Mark**: The running maximum of wealth is updated:\n      $$H_{t+1} = \\max(H_t, W_{t+1})$$\n    - **e. Calculate Drawdown**: The new drawdown is calculated. Since $W_0>0$ and $p_t>0$, $H_t$ will remain positive for all $t < \\tau$.\n      $$D_{t+1} = 1 - \\frac{W_{t+1}}{H_{t+1}}$$\n    - **f. Check Circuit Breaker**: The core risk management rule is checked. If the newly computed drawdown exceeds the threshold, the simulation is halted.\n      If $D_{t+1} > \\bar{d}$:\n        - Set the halting time $\\tau = t+1$.\n        - Set the halting indicator $h=1$.\n        - The process terminates. The final state is $\\{W_\\tau, D_\\tau, \\tau\\}$.\n    - **g. Update Previous Action**: For the next iteration, the current action becomes the previous action: set $a_{t-1} \\leftarrow a_t$.\n\n3.  **Termination**:\n    - If the loop is terminated early by the circuit breaker at time $\\tau$, the final results are $\\{h=1, W_\\tau, D_\\tau, \\tau\\}$.\n    - If the loop completes without the drawdown ever exceeding the threshold, the simulation runs to its full course. The termination time is $\\tau=T$. The final results are $\\{h=0, W_T, D_T, \\tau=T\\}$.\n\nAll floating-point output values ($W_\\tau$, $D_\\tau$) must be rounded to $6$ decimal places. This procedure will be implemented for each test case provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trading simulation problem for a given set of test cases.\n    \"\"\"\n\n    def simulate_trading(prices, actions, W0, kappa, d_bar):\n        \"\"\"\n        Simulates the trading environment with a circuit breaker.\n\n        Args:\n            prices (list or np.array): The sequence of asset prices {p_t}.\n            actions (list or np.array): The sequence of agent actions {a_t}.\n            W0 (float): Initial wealth.\n            kappa (float): Trading cost rate.\n            d_bar (float): Drawdown threshold.\n\n        Returns:\n            list: A list containing [h, W_tau, D_tau, tau].\n        \"\"\"\n        prices = np.array(prices, dtype=np.float64)\n        actions = np.array(actions, dtype=np.int8)\n        \n        T = len(prices) - 1\n        \n        # State variables\n        W = np.zeros(T + 1, dtype=np.float64)\n        H = np.zeros(T + 1, dtype=np.float64)\n        D = np.zeros(T + 1, dtype=np.float64)\n        \n        # Initialization at t=0\n        W[0] = float(W0)\n        H[0] = float(W0)\n        D[0] = 0.0\n        \n        # Convention for action at t=-1\n        a_prev = 0\n        \n        # Time-stepping simulation\n        for t in range(T):\n            # (a) Calculate return for step t -> t+1\n            r_tplus1 = (prices[t+1] - prices[t]) / prices[t]\n            \n            a_t = actions[t]\n            \n            # (b) Calculate trading cost\n            cost_indicator = 1 if a_t != a_prev else 0\n            cost = kappa * W[t] * cost_indicator\n            \n            # (c) Update wealth\n            W[t+1] = W[t] * (1 + a_t * r_tplus1) - cost\n            \n            # (d) Update high-water mark\n            H[t+1] = max(H[t], W[t+1])\n            \n            # (e) Calculate drawdown\n            if H[t+1] > 0:\n                D[t+1] = 1.0 - (W[t+1] / H[t+1])\n            else:\n                # This case implies ruin and should result in max drawdown.\n                # Given problem constraints (W0>0, p_t>0), H_t should not be <= 0 unless W_t becomes <=0.\n                D[t+1] = 1.0 \n\n            # (f) Check circuit breaker\n            if D[t+1] > d_bar:\n                tau = t + 1\n                h = 1\n                # Return results at halt time\n                return [h, W[tau], D[tau], tau]\n            \n            # (g) Update previous action for next iteration\n            a_prev = a_t\n            \n        # If loop completes without halting\n        tau = T\n        h = 0\n        return [h, W[T], D[T], T]\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Test Case 1\n        {\n            \"prices\": [100.0, 101.0, 99.0, 100.0, 103.0],\n            \"actions\": [1, 1, 1, 1],\n            \"W0\": 1.0, \"kappa\": 0.0, \"d_bar\": 0.2\n        },\n        # Test Case 2\n        {\n            \"prices\": [100.0, 90.0, 100.0],\n            \"actions\": [1, 1],\n            \"W0\": 1.0, \"kappa\": 0.0, \"d_bar\": 0.1\n        },\n        # Test Case 3\n        {\n            \"prices\": [100.0, 98.0, 200.0],\n            \"actions\": [1, 1],\n            \"W0\": 1.0, \"kappa\": 0.0, \"d_bar\": 0.01\n        },\n        # Test Case 4\n        {\n            \"prices\": [100.0, 100.0, 100.0, 100.0],\n            \"actions\": [1, -1, 1],\n            \"W0\": 1.0, \"kappa\": 0.03, \"d_bar\": 0.05\n        }\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        result = simulate_trading(\n            case[\"prices\"], case[\"actions\"], case[\"W0\"], case[\"kappa\"], case[\"d_bar\"]\n        )\n        # Format the result list into the required string format\n        h, W_tau, D_tau, tau = result\n        result_str = f\"[{h}, {W_tau:.6f}, {D_tau:.6f}, {tau}]\"\n        results_str_list.append(result_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"}, {"introduction": "With a trading environment defined, the next question is how to determine an optimal course of action. This practice introduces you to dynamic programming, a powerful technique for solving sequential decision problems where the environment's dynamics are known [@problem_id:2426659]. By modeling overnight risk as a terminal penalty, you will compute the optimal trading policy that balances the desire for intraday profit against the need to flatten the inventory by market close. This exercise provides the conceptual foundation for value-based reinforcement learning, demonstrating how optimal actions are derived by looking ahead to future rewards.", "id": "2426659", "problem": "You are asked to formalize overnight risk as a terminal inventory penalty and compute the optimal expected reward for a discrete-time, single-asset trading day where an agent learns to flatten its position before the market closes. The trading day has a finite number of intraday decision times. At each decision time, the agent may adjust its inventory by at most one unit per period, subject to hard inventory bounds.\n\nThe environment is defined as follows. There is a finite time horizon with intraday decision times indexed by $t \\in \\{0,1,\\dots,T-1\\}$ and a market close at $t = T$. The agent’s inventory at time $t$ is $p_t \\in \\{-P_{\\max},-P_{\\max}+1,\\dots,P_{\\max}-1,P_{\\max}\\}$ with $p_0 = 0$. At each $t \\in \\{0,1,\\dots,T-1\\}$, the agent chooses an action $a_t \\in \\{-1,0,1\\}$. Inventory transitions obey\n$$\np_{t+1} = \\mathrm{clip}(p_t + a_t, -P_{\\max}, P_{\\max}),\n$$\nwhere $\\mathrm{clip}(x,\\ell,u) = \\min\\{u,\\max\\{\\ell,x\\}\\}$.\n\nThe expected price change over period $[t,t+1]$ is given and denoted by $\\mu_t$. The per-trade cost is $c \\cdot |a_t|$ for any action at time $t$. The running risk penalty per period is $k \\cdot p_{t+1}^2$ at time $t$. At the market close, an overnight risk penalty is imposed: $-\\beta \\cdot p_T^2$.\n\nThe per-period expected reward for the decision at time $t$ is defined as the expected trading profit from holding $p_{t+1}$ over $[t,t+1]$ minus the transaction cost and the running inventory penalty:\n$$\nr_t(p_t,a_t) = p_{t+1} \\cdot \\mu_t - c\\cdot|a_t| - k\\cdot p_{t+1}^2.\n$$\nThe terminal contribution is\n$$\nr_T(p_T) = -\\beta \\cdot p_T^2.\n$$\nThe objective is to compute the optimal expected total reward starting from $p_0=0$,\n$$\nR^\\star = \\max_{\\{a_t\\}_{t=0}^{T-1}} \\sum_{t=0}^{T-1} r_t(p_t,a_t) + r_T(p_T),\n$$\nsubject to the transition dynamics for $p_{t+1}$ and the action constraints.\n\nA deterministic tie-breaking rule must be used to ensure a unique optimal action when multiple actions yield the same maximal expected value at any decision time $t$. The tie-breaking rule is:\n- Among actions that maximize the expected value, prefer those resulting in the smallest $|p_{t+1}|$.\n- If still tied, among those, prefer the action with the smallest $|a_t|$.\n- If still tied, choose the smallest $a_t$ in the usual order $-1 < 0 < 1$.\n\nFor each test case, starting from $p_0=0$, compute:\n- The optimal expected total reward $R^\\star$.\n- The resulting terminal inventory $p_T$ induced by following the optimal actions under the specified tie-breaking rule.\n\nYour program should return, for each test case, the pair $\\left(R^\\star, p_T\\right)$, where $R^\\star$ is rounded to $6$ decimal places using standard rounding, and $p_T$ is an integer. Aggregate all test case results into a single list in the order given below.\n\nTest suite:\n1. Case A (general multi-period with mixed drifts): $T=4$, $\\mu = [0.02, 0.01, 0.00, -0.01]$, $c=0.003$, $k=0.0005$, $\\beta=0.05$, $P_{\\max}=3$.\n2. Case B (zero drift baseline): $T=3$, $\\mu = [0.00, 0.00, 0.00]$, $c=0.001$, $k=0.00$, $\\beta=0.10$, $P_{\\max}=2$.\n3. Case C (immediate close boundary): $T=1$, $\\mu = [0.05]$, $c=0.002$, $k=0.00$, $\\beta=0.03$, $P_{\\max}=1$.\n4. Case D (flattening incentive near close): $T=2$, $\\mu = [0.03, 0.03]$, $c=0.002$, $k=0.00$, $\\beta=0.20$, $P_{\\max}=2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{Case A } R^\\star,\\text{Case A } p_T,\\text{Case B } R^\\star,\\text{Case B } p_T,\\text{Case C } R^\\star,\\text{Case C } p_T,\\text{Case D } R^\\star,\\text{Case D } p_T]$.\n- The $R^\\star$ entries must be printed as decimal numbers rounded to $6$ decimal places, and the $p_T$ entries must be printed as integers.", "solution": "The problem presented is a discrete-time, finite-horizon optimal control problem, which is a standard formulation in computational finance for modeling optimal execution strategies. The problem is well-posed and scientifically sound, allowing for a solution via dynamic programming. The objective is to find a sequence of actions $\\{a_t\\}_{t=0}^{T-1}$ that maximizes the total expected reward, starting from an initial inventory $p_0=0$.\n\nThe solution proceeds by defining a value function, $V_t(p)$, which represents the maximum possible sum of rewards from time $t$ until the market close at time $T$, given that the inventory at time $t$ is $p_t = p$. The problem is solved by backward induction, starting from the terminal time $T$ and working back to the initial time $t=0$.\n\nThe state of the system at any time $t \\in \\{0, 1, \\dots, T\\}$ is the agent's inventory, $p_t$. The state space is discrete and finite, given by $p_t \\in \\{-P_{\\max}, \\dots, P_{\\max}\\}$. The action space at any decision time $t \\in \\{0, 1, \\dots, T-1\\}$ is also discrete and finite: $a_t \\in \\{-1, 0, 1\\}$.\n\nThe Bellman principle of optimality provides the recursive relationship for the value function. At the terminal time $t=T$, the value function is determined solely by the overnight risk penalty:\n$$\nV_T(p_T) = r_T(p_T) = -\\beta \\cdot p_T^2\n$$\nFor any preceding time $t \\in \\{T-1, T-2, \\dots, 0\\}$, the value function $V_t(p_t)$ is the maximum value achievable over all possible actions $a_t$. This is the sum of the immediate reward $r_t(p_t, a_t)$ and the value of the subsequent state $V_{t+1}(p_{t+1})$:\n$$\nV_t(p_t) = \\max_{a_t \\in \\{-1, 0, 1\\}} \\left\\{ r_t(p_t, a_t) + V_{t+1}(p_{t+1}) \\right\\}\n$$\nwhere the state transition is given by $p_{t+1} = \\mathrm{clip}(p_t + a_t, -P_{\\max}, P_{\\max})$ and the immediate reward is $r_t(p_t, a_t) = p_{t+1} \\cdot \\mu_t - c \\cdot |a_t| - k \\cdot p_{t+1}^2$.\n\nLet us define the quality function, or Q-value, $Q_t(p_t, a_t)$, as the value of taking action $a_t$ in state $p_t$ at time $t$:\n$$\nQ_t(p_t, a_t) = p_{t+1} \\cdot \\mu_t - c \\cdot |a_t| - k \\cdot p_{t+1}^2 + V_{t+1}(p_{t+1})\n$$\nThe Bellman equation can then be written as $V_t(p_t) = \\max_{a_t} Q_t(p_t, a_t)$.\n\nThe computational procedure is as follows:\n1.  A table is initialized to store the values $V_t(p)$ for all $t \\in \\{0, \\dots, T\\}$ and $p \\in \\{-P_{\\max}, \\dots, P_{\\max}\\}$.\n2.  At $t=T$, the table is populated with the terminal values $V_T(p_T) = -\\beta \\cdot p_T^2$ for all possible terminal inventories $p_T$.\n3.  The algorithm iterates backward in time from $t=T-1$ down to $t=0$. In each step $t$, for each possible inventory state $p_t$, the values $Q_t(p_t, a_t)$ for all three actions $a_t \\in \\{-1, 0, 1\\}$ are computed using the already known values of $V_{t+1}$.\n4.  To ensure a unique optimal action $\\pi_t(p_t)$, the specified tie-breaking rule must be strictly enforced. When multiple actions yield the same maximal $Q$-value, the selection is refined by a lexicographical comparison. The chosen action $a^\\star$ must be the one that minimizes the following criteria in sequence:\n    a. Maximize the $Q$-value, $Q_t(p_t, a_t)$.\n    b. Minimize the absolute value of the next inventory, $|p_{t+1}|$.\n    c. Minimize the absolute value of the action, $|a_t|$.\n    d. Minimize the action value itself, with the natural ordering $-1 < 0 < 1$.\n5.  The optimal value $V_t(p_t) = Q_t(p_t, \\pi_t(p_t))$ is stored.\n6.  The optimal total expected reward for the entire trading day, starting from $p_0=0$, is $R^\\star = V_0(0)$.\n7.  To find the corresponding terminal inventory $p_T$, one can simulate the trajectory forward from the initial state $(t,p) = (0,0)$ using the computed optimal policy $\\pi_t(p_t)$. A more direct method, employed in the algorithm, is to maintain another table during the backward pass that maps each state-time pair $(t, p_t)$ to the resulting terminal inventory $p_T$ if the optimal policy is followed from that point. Let this be $P_T(t, p_t)$. Then $P_T(T, p_T) = p_T$, and for $t < T$, $P_T(t, p_t) = P_T(t+1, \\mathrm{clip}(p_t + \\pi_t(p_t), -P_{\\max}, P_{\\max}))$. The final answer for the terminal inventory is then $p_T = P_T(0, 0)$.\n\nThis dynamic programming approach guarantees the discovery of the globally optimal solution under the model's assumptions. The implementation calculates these values for each test case to determine the specified outputs.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the given test cases for the optimal trading problem.\n    \"\"\"\n\n    def solve_case(T, mu, c, k, beta, Pmax):\n        \"\"\"\n        Solves a single instance of the optimal trading problem using dynamic programming.\n\n        Args:\n            T (int): Number of time steps.\n            mu (list): List of expected price changes.\n            c (float): Transaction cost per share.\n            k (float): Running inventory risk penalty coefficient.\n            beta (float): Terminal inventory risk penalty coefficient.\n            Pmax (int): Maximum absolute inventory.\n\n        Returns:\n            tuple: A tuple containing the optimal total reward (R_star) and the\n                   resulting terminal inventory (p_T).\n        \"\"\"\n        # State space for inventory p\n        num_p_states = 2 * Pmax + 1\n        p_states = np.arange(-Pmax, Pmax + 1)\n        \n        # DP tables\n        # V_table stores the value function V_t(p)\n        V_table = np.zeros((T + 1, num_p_states))\n        # pT_table stores the resulting terminal inventory p_T if starting from state p at time t\n        pT_table = np.zeros((T + 1, num_p_states), dtype=np.int32)\n\n        # Helper function to map inventory p to an array index\n        def p_to_idx(p):\n            return int(p + Pmax)\n\n        # --- Backward Pass ---\n\n        # Time t = T: Terminal condition\n        for p_idx, p_T in enumerate(p_states):\n            V_table[T, p_idx] = -beta * p_T**2\n            pT_table[T, p_idx] = p_T\n\n        # Recursion from t = T-1 down to 0\n        for t in range(T - 1, -1, -1):\n            for p_idx, p_t in enumerate(p_states):\n                \n                candidates = []\n                \n                # Evaluate all possible actions a_t in {-1, 0, 1}\n                for a_t in [-1, 0, 1]:\n                    # State transition\n                    p_t_plus_1 = int(np.clip(p_t + a_t, -Pmax, Pmax))\n                    p_t_plus_1_idx = p_to_idx(p_t_plus_1)\n                    \n                    # Immediate reward\n                    reward = p_t_plus_1 * mu[t] - c * abs(a_t) - k * p_t_plus_1**2\n                    \n                    # Q-value (sum of immediate reward and future value)\n                    q_value = reward + V_table[t + 1, p_t_plus_1_idx]\n                    \n                    # Store candidate with its tie-breaking criteria as a sortable tuple:\n                    # 1. Maximize Q-value (equivalent to minimizing -q_value)\n                    # 2. Minimize |p_{t+1}|\n                    # 3. Minimize |a_t|\n                    # 4. Minimize a_t\n                    sort_key = (-q_value, abs(p_t_plus_1), abs(a_t), a_t)\n                    candidates.append((sort_key, p_t_plus_1, q_value))\n                \n                # Sort to find the best action according to the tie-breaking rules\n                candidates.sort(key=lambda x: x[0])\n                \n                # The best choice is the first element after sorting\n                best_choice = candidates[0]\n                best_p_next = best_choice[1]\n                best_q = best_choice[2]\n                \n                # Store results in the DP tables\n                V_table[t, p_idx] = best_q\n                \n                # Propagate the terminal position for this state-time pair\n                best_p_next_idx = p_to_idx(best_p_next)\n                pT_table[t, p_idx] = pT_table[t + 1, best_p_next_idx]\n\n        # --- Extract Final Answer ---\n        \n        # The solution corresponds to the initial state p_0 = 0 at t = 0\n        p0_idx = p_to_idx(0)\n        \n        # Optimal total expected reward\n        R_star = V_table[0, p0_idx]\n        \n        # Resulting terminal inventory\n        p_T_final = pT_table[0, p0_idx]\n        \n        return R_star, p_T_final\n\n    test_cases = [\n        # Case A: general multi-period with mixed drifts\n        {'T': 4, 'mu': [0.02, 0.01, 0.00, -0.01], 'c': 0.003, 'k': 0.0005, 'beta': 0.05, 'Pmax': 3},\n        # Case B: zero drift baseline\n        {'T': 3, 'mu': [0.00, 0.00, 0.00], 'c': 0.001, 'k': 0.00, 'beta': 0.10, 'Pmax': 2},\n        # Case C: immediate close boundary\n        {'T': 1, 'mu': [0.05], 'c': 0.002, 'k': 0.00, 'beta': 0.03, 'Pmax': 1},\n        # Case D: flattening incentive near close\n        {'T': 2, 'mu': [0.03, 0.03], 'c': 0.002, 'k': 0.00, 'beta': 0.20, 'Pmax': 2},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        R_star, p_T = solve_case(\n            case['T'], case['mu'], case['c'], case['k'], case['beta'], case['Pmax']\n        )\n        all_results.append(format(R_star, '.6f'))\n        all_results.append(p_T)\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}, {"introduction": "While dynamic programming is powerful, it requires perfect knowledge of the environment, a luxury we rarely have. This capstone exercise challenges you to build a true learning agent that adapts through experience, using Q-learning with linear function approximation [@problem_id:2426648]. You will train this agent in a market with shifting dynamics and investigate a crucial aspect of agent design: the state representation. By systematically varying the length of the historical data window the agent observes, you will conduct an empirical study to see how what an agent 'sees' impacts its performance, mirroring the experimental process of a quantitative researcher.", "id": "2426648", "problem": "Construct a complete, runnable program that studies how the observation window length in the state representation of a trading agent affects out-of-sample performance. The trading environment is a discrete-time, frictional market with a synthetic return process and a finite action set. The agent must be trained separately under each specified window length and then evaluated on an independent test trajectory. The final output must aggregate the results for all cases into a single line as specified below.\n\nDefine the market as follows. Time is indexed by integers $t \\in \\{0,1,\\dots\\}$. Let $r_t$ denote the per-step log-return (expressed as a decimal, not a percentage sign) of a synthetic asset. The return process switches deterministically between a positively autocorrelated (trending) regime and a negatively autocorrelated (mean-reverting) regime every $L$ steps. Specifically, for a given $t$, define the regime index $z_t = \\left\\lfloor \\frac{t}{L} \\right\\rfloor \\bmod 2$. Let $(\\mu_0,\\phi_0)$ be the parameters for the trending regime and $(\\mu_1,\\phi_1)$ the parameters for the mean-reverting regime. The return process is\n$$\nr_t \\;=\\; \\mu_{z_t} \\;+\\; \\phi_{z_t}\\, r_{t-1} \\;+\\; \\sigma\\, \\varepsilon_t,\n$$\nwith $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ independent and identically distributed. The initial condition is $r_{-1} = 0$. Use the constants $L=200$, $\\mu_0=0.0005$, $\\phi_0=0.9$, $\\mu_1=0.0$, $\\phi_1=-0.9$, and $\\sigma=0.002$. The agent’s actions at each $t$ are $a_t \\in \\{-1,0,+1\\}$, representing fully short, flat, or fully long positions, respectively. The immediately realized reward per step is\n$$\nR_t \\;=\\; a_t \\, r_t \\;-\\; c \\, |a_t - a_{t-1}| \\;-\\; \\lambda \\, \\hat{v}_t \\, a_t^2,\n$$\nwhere $a_{-1}=0$, $c=0.0005$ is the transaction cost per unit change in position, and $\\lambda=10.0$ is a risk-aversion coefficient applied to a rolling variance estimate $\\hat{v}_t$ of returns computed over the most recent $W$ observed returns available prior to choosing $a_t$ (if fewer than $W$ past returns are available, use all available). The initial variance estimate may be taken as $0$ when insufficient data exist. The return process is exogenous and independent of the agent’s actions.\n\nFor state representation, the agent must form at time $t$ a feature vector using only information available prior to choosing $a_t$. Let $W$ denote the observation window length. The state summary at decision time $t$ is a vector comprising:\n- a constant bias term equal to $1$,\n- the mean of the last up to $W$ realized returns $\\{r_{t-1}, r_{t-2}, \\dots\\}$,\n- the most recent realized return $r_{t-1}$ (taken as $0$ if none),\n- the standard deviation of the last up to $W$ returns,\n- the previous position $a_{t-1}$.\n\nTraining objective: for each specified window length $W$, from first principles, construct a stationary policy $\\pi_W$ that approximately maximizes the expected discounted sum of rewards,\n$$\n\\mathbb{E}\\!\\left[ \\sum_{t=0}^{T_{\\text{train}}-1} \\gamma^t R_t \\right],\n$$\nwith discount factor $\\gamma = 0.95$, by interacting with the environment on training trajectories. The training horizon per episode is $T_{\\text{train}} = 1000$ steps, and the number of training episodes is $E_{\\text{train}} = 8$. The randomness in training must be generated with independent and identically distributed standard normal shocks and be reproducible across window lengths: use the training seeds $s_{\\text{train}} + e$ for episode index $e \\in \\{0,1,\\dots,E_{\\text{train}}-1\\}$ with $s_{\\text{train}} = 12345$. After training $\\pi_W$, evaluate out-of-sample performance by applying the resulting stationary policy greedily without exploration on an independent test trajectory of length $T_{\\text{test}} = 3000$, generated with a distinct seed $s_{\\text{test}} = 54321$ and the same data-generating process. The evaluation metric for each $W$ is the average realized reward per step on the test trajectory,\n$$\n\\bar{R}(W) \\;=\\; \\frac{1}{T_{\\text{test}}} \\sum_{t=0}^{T_{\\text{test}}-1} R_t,\n$$\nexpressed as a decimal.\n\nTest suite. Train and evaluate the agent independently for each observation window length in the list\n$$\n\\mathcal{W} \\;=\\; [\\,1,\\,5,\\,30,\\,240,\\,600\\,].\n$$\n\nYour program must:\n- implement the environment and learning agent strictly according to the definitions above,\n- produce deterministic results that depend only on the specified seeds and parameters,\n- for each $W \\in \\mathcal{W}$, output the corresponding $\\bar{R}(W)$ as a floating-point number rounded to six decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as $\\mathcal{W}$. For example, an output over five test cases must look like\n$$\n[\\bar{R}(1),\\bar{R}(5),\\bar{R}(30),\\bar{R}(240),\\bar{R}(600)],\n$$\nwith each entry rounded to six decimal places. No other text should be printed.\n\nNotes:\n- There are no physical units involved; all returns and rewards must be expressed as decimals.\n- Angles do not appear in this problem.\n- Percentages, if mentioned, must be expressed as decimals (for example, $0.01$ instead of $1\\%$).", "solution": "The problem requires the construction and evaluation of a reinforcement learning agent for a synthetic financial trading task. The primary objective is to study the effect of the observation window length, $W$, on the agent's out-of-sample performance. Before proceeding, we must validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Synthetic Return Process**: $r_t = \\mu_{z_t} + \\phi_{z_t} r_{t-1} + \\sigma \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,1)$, $r_{-1} = 0$.\n- **Regime Switching**: Index $z_t = \\lfloor t/L \\rfloor \\bmod 2$, with period $L=200$.\n- **Regime Parameters**: Trending ($z_t=0$): $\\mu_0=0.0005, \\phi_0=0.9$. Mean-reverting ($z_t=1$): $\\mu_1=0.0, \\phi_1=-0.9$.\n- **Volatility**: $\\sigma=0.002$.\n- **Agent Actions**: $a_t \\in \\{-1, 0, +1\\}$, with initial position $a_{-1}=0$.\n- **Reward Function**: $R_t = a_t r_t - c |a_t - a_{t-1}| - \\lambda \\hat{v}_t a_t^2$.\n- **Reward Parameters**: Transaction cost $c=0.0005$, risk aversion $\\lambda=10.0$.\n- **Rolling Variance**: $\\hat{v}_t$ is the variance of the last up to $W$ returns available before time $t$.\n- **State Vector ($s_t$ at decision time $t$)**: A vector of 5 features: [$1$ (bias), mean of recent $\\le W$ returns, $r_{t-1}$, standard deviation of recent $\\le W$ returns, $a_{t-1}$].\n- **Training Objective**: Maximize $\\mathbb{E}[\\sum_{t=0}^{T_{\\text{train}}-1} \\gamma^t R_t]$.\n- **Training Parameters**: Discount factor $\\gamma = 0.95$, training horizon $T_{\\text{train}} = 1000$, number of episodes $E_{\\text{train}} = 8$. Training seeds are $s_{\\text{train}} + e$ for episode $e$, where $s_{\\text{train}} = 12345$.\n- **Evaluation**: Greedy policy (no exploration) on a test trajectory of length $T_{\\text{test}} = 3000$ using seed $s_{\\text{test}} = 54321$.\n- **Evaluation Metric**: Average per-step reward $\\bar{R}(W) = \\frac{1}{T_{\\text{test}}} \\sum_{t=0}^{T_{\\text{test}}-1} R_t$.\n- **Test Suite**: $W \\in \\mathcal{W} = [1, 5, 30, 240, 600]$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded within the field of computational finance, employing standard models like autoregressive processes for returns and a common reward structure. It is well-posed, with all necessary parameters, initial conditions, and evaluation metrics clearly defined. The language is objective and unambiguous. While the specific reinforcement learning algorithm is not prescribed, the directive to \"from first principles, construct a stationary policy\" in this context points toward a fundamental, model-free algorithm. Given the discrete action space and continuous (vector) state space, Q-learning with linear function approximation is the most direct and standard choice. This interpretation is consistent with the level of detail provided elsewhere. The problem is complete, consistent, and computationally feasible.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be constructed.\n\n**Methodology**\nThe task is to train a trading agent using reinforcement learning. We will implement Q-learning, a model-free temporal difference (TD) learning algorithm, with a linear function approximator for the action-value function, $Q(s, a)$.\n\n**1. State and Action-Value Function**\nThe state $s_t$ at time $t$ is a $5$-dimensional feature vector as defined in the problem. The action space is discrete: $\\mathcal{A} = \\{-1, 0, 1\\}$. We approximate the Q-function for each action $a \\in \\mathcal{A}$ as a linear combination of the state features:\n$$\nQ(s_t, a; \\theta_a) = s_t^T \\theta_a\n$$\nwhere $\\theta_a$ is a $5$-dimensional weight vector for action $a$. The agent's goal is to learn the optimal weight vectors $\\theta_a^*$ for all actions.\n\n**2. Learning Algorithm: Q-Learning**\nDuring training, the agent interacts with the environment. At each step $t$, it is in state $s_t$, takes an action $a_t$, receives a reward $R_t$, and observes the next state $s_{t+1}$. The weight vector for the chosen action $a_t$ is updated using the TD error:\n$$\n\\text{TD Error: } \\delta_t = R_t + \\gamma \\max_{a' \\in \\mathcal{A}} Q(s_{t+1}, a'; \\theta_{a'}) - Q(s_t, a_t; \\theta_{a_t})\n$$\n$$\n\\text{Update Rule: } \\theta_{a_t} \\leftarrow \\theta_{a_t} + \\alpha \\cdot \\delta_t \\cdot \\nabla_{\\theta_{a_t}} Q(s_t, a_t; \\theta_{a_t})\n$$\nSince $Q(s_t, a_t; \\theta_{a_t}) = s_t^T \\theta_{a_t}$, the gradient is simply the state vector $s_t$. The update becomes:\n$$\n\\theta_{a_t} \\leftarrow \\theta_{a_t} + \\alpha \\cdot \\delta_t \\cdot s_t\n$$\nHere, $\\alpha$ is the learning rate. We select a standard constant value $\\alpha = 0.01$.\n\n**3. Exploration**\nTo ensure sufficient exploration of the state-action space during training, an $\\epsilon$-greedy policy is used. With probability $\\epsilon$, the agent chooses a random action; otherwise, it chooses the action with the highest estimated Q-value (greedily):\n$$\na_t = \\arg\\max_{a' \\in \\mathcal{A}} Q(s_t, a'; \\theta_{a'})\n$$\nWe use a standard constant exploration rate of $\\epsilon = 0.1$.\n\n**4. Implementation Details**\n- **Reproducibility**: To isolate the effect of the window length $W$, the sequence of random shocks $(\\varepsilon_t)$ for the return process and the random draws for the $\\epsilon$-greedy exploration are pre-generated for each training episode and for the test run. This ensures that for a given episode index, the market dynamics and exploration choices are identical across all experiments for different values of $W$.\n- **State Calculation**: The state vector $s_t$ is constructed using returns realized up to time $t-1$. For $t=0$, where no prior returns exist, the statistical features (mean, standard deviation, last return) are set to $0$. When the number of available returns is less than $W$, all available returns are used for calculations. The standard deviation, and thus the variance term $\\hat{v}_t$ in the reward, is $0$ if fewer than two data points are available.\n- **Training and Evaluation**: For each $W \\in \\mathcal{W}$, the agent's weights are initialized to zero and then trained over $E_{\\text{train}}=8$ episodes, each of length $T_{\\text{train}}=1000$. After training, the learned policy is evaluated on a separate, longer test trajectory of $T_{\\text{test}}=3000$ steps. During evaluation, the policy is purely greedy ($\\epsilon=0$) to measure its out-of-sample performance. The final metric is the average reward per step over this test trajectory.\n\nThis systematic approach allows for a direct comparison of performance attributable solely to the change in the agent's observation window length $W$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates a reinforcement learning trading agent for various\n    observation window lengths, as specified in the problem statement.\n    \"\"\"\n    # Problem Parameters\n    L = 200\n    MU0, PHI0 = 0.0005, 0.9\n    MU1, PHI1 = 0.0, -0.9\n    SIGMA = 0.002\n    C = 0.0005\n    LAMBDA = 10.0\n    GAMMA = 0.95\n    T_TRAIN = 1000\n    E_TRAIN = 8\n    S_TRAIN = 12345\n    T_TEST = 3000\n    S_TEST = 54321\n    WINDOW_LENGTHS = [1, 5, 30, 240, 600]\n    ACTIONS = [-1, 0, 1]\n\n    # RL Hyperparameters\n    ALPHA = 0.01\n    EPSILON = 0.1\n\n    def get_state(t, W, rt_history, at_history):\n        \"\"\"\n        Computes the state vector at time t.\n        State vector: [bias, mean(r), r_{t-1}, std(r), a_{t-1}]\n        \"\"\"\n        if t == 0:\n            return np.array([1.0, 0.0, 0.0, 0.0, 0.0])\n\n        realized_returns = rt_history[1:]  # Exclude r_{-1}\n        \n        window = realized_returns[max(0, len(realized_returns) - W):]\n        \n        r_mean = np.mean(window)\n        r_std = np.std(window) if len(window) > 1 else 0.0\n        r_tm1_feat = realized_returns[-1]\n        a_tm1 = at_history[-1]\n        \n        return np.array([1.0, r_mean, r_tm1_feat, r_std, a_tm1])\n\n    # Pre-generate random numbers for reproducibility across different W\n    train_market_shocks = []\n    train_expl_draws = []\n    train_expl_actions = []\n    for e in range(E_TRAIN):\n        rng_episode = np.random.default_rng(S_TRAIN + e)\n        train_market_shocks.append(rng_episode.standard_normal(size=T_TRAIN))\n        train_expl_draws.append(rng_episode.random(size=T_TRAIN))\n        train_expl_actions.append(rng_episode.choice(ACTIONS, size=T_TRAIN))\n\n    rng_test = np.random.default_rng(S_TEST)\n    test_market_shocks = rng_test.standard_normal(size=T_TEST)\n\n    all_results = []\n    \n    for W in WINDOW_LENGTHS:\n        # --- Training Phase ---\n        weights = {a: np.zeros(5) for a in ACTIONS}\n        \n        for e in range(E_TRAIN):\n            at_history = [0]\n            rt_history = [0.0]\n            for t in range(T_TRAIN):\n                s_t = get_state(t, W, rt_history, at_history)\n                \n                # 1. Choose action a_t using epsilon-greedy policy\n                q_values = {a: np.dot(s_t, weights[a]) for a in ACTIONS}\n                if train_expl_draws[e][t] < EPSILON:\n                    a_t = train_expl_actions[e][t]\n                else:\n                    q_vals_list = [q_values[a] for a in ACTIONS]\n                    best_action_idx = np.argmax(q_vals_list)\n                    a_t = ACTIONS[best_action_idx]\n\n                # 2. Environment step: generate r_t and calculate R_t\n                z_t = (t // L) % 2\n                mu, phi = (MU0, PHI0) if z_t == 0 else (MU1, PHI1)\n                r_tm1 = rt_history[-1]\n                r_t = mu + phi * r_tm1 + SIGMA * train_market_shocks[e][t]\n                \n                a_tm1 = at_history[-1]\n                v_hat_t = s_t[3]**2\n                R_t = a_t * r_t - C * abs(a_t - a_tm1) - LAMBDA * v_hat_t * (a_t**2)\n\n                rt_history.append(r_t)\n                at_history.append(a_t)\n                \n                # 3. Perform Q-learning update\n                s_tp1 = get_state(t + 1, W, rt_history, at_history)\n                q_values_tp1 = {a: np.dot(s_tp1, weights[a]) for a in ACTIONS}\n                max_q_tp1 = max(q_values_tp1.values())\n                td_target = R_t + GAMMA * max_q_tp1\n                td_error = td_target - q_values[a_t]\n                weights[a_t] += ALPHA * td_error * s_t\n\n        # --- Evaluation Phase ---\n        total_reward = 0.0\n        at_history = [0]\n        rt_history = [0.0]\n        for t in range(T_TEST):\n            s_t = get_state(t, W, rt_history, at_history)\n\n            # 1. Choose action a_t greedily\n            q_values = {a: np.dot(s_t, weights[a]) for a in ACTIONS}\n            q_vals_list = [q_values[a] for a in ACTIONS]\n            best_action_idx = np.argmax(q_vals_list)\n            a_t = ACTIONS[best_action_idx]\n            \n            # 2. Environment step\n            z_t = (t // L) % 2\n            mu, phi = (MU0, PHI0) if z_t == 0 else (MU1, PHI1)\n            r_tm1 = rt_history[-1]\n            r_t = mu + phi * r_tm1 + SIGMA * test_market_shocks[t]\n\n            a_tm1 = at_history[-1]\n            v_hat_t = s_t[3]**2\n            R_t = a_t * r_t - C * abs(a_t - a_tm1) - LAMBDA * v_hat_t * (a_t**2)\n            total_reward += R_t\n            \n            rt_history.append(r_t)\n            at_history.append(a_t)\n\n        avg_reward = total_reward / T_TEST\n        all_results.append(round(avg_reward, 6))\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}]}