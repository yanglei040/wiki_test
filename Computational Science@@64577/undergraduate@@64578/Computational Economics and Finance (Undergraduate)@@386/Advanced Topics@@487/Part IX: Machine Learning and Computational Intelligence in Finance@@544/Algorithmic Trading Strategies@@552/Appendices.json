{"hands_on_practices": [{"introduction": "We begin our hands-on journey by constructing a classic quantitative strategy from the ground up. This exercise [@problem_id:2371402] demystifies the process of translating a market hypothesis—in this case, a contrarian view on option-market sentiment—into a concrete, testable trading algorithm. By implementing this strategy, you will gain practical experience in the essential workflow of signal generation, position management, and performance evaluation, including the crucial impact of transaction costs.", "id": "2371402", "problem": "You are given a discrete-time financial market with an underlying asset and daily option market activity summarized by call and put trading volumes. For each trading day indexed by integer time $t \\in \\{0,1,\\dots,T-1\\}$, let $C_t \\ge 0$ denote the call option volume, $P_t \\ge 0$ denote the put option volume, and let $r_t$ denote the asset’s simple return for day $t$ expressed as a decimal (for example, $r_t = 0.01$ denotes a $1$ percent increase, which must be treated as the decimal $0.01$ rather than a percentage). A contrarian sentiment trading rule is defined as follows. First, define the call-put volume ratio $q_t$ by\n$$\nq_t =\n\\begin{cases}\n\\dfrac{C_t}{P_t}, & \\text{if } P_t > 0, \\\\\n+\\infty, & \\text{if } P_t = 0 \\text{ and } C_t > 0, \\\\\n0, & \\text{if } P_t = 0 \\text{ and } C_t = 0.\n\\end{cases}\n$$\nGiven two thresholds $t_\\ell$ and $t_h$ with $0 < t_\\ell < t_h$, define the trading signal $s_t$ by\n$$\ns_t =\n\\begin{cases}\n+1, & \\text{if } q_t < t_\\ell, \\\\\n-1, & \\text{if } q_t > t_h, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nInterpret $s_t \\in \\{-1,0,+1\\}$ as the position decided at the close of day $t$ to be applied to the next day: $+1$ denotes a fully invested long position in the underlying, $-1$ denotes a fully invested short position in the underlying, and $0$ denotes no position. Let $c \\ge 0$ be a proportional transaction cost (as a decimal) paid once whenever the position changes at the close of day $t$. Assume the initial position is $s_{-1} = 0$. The one-day profit for day $t+1$ is\n$$\np_{t+1} = s_t \\, r_{t+1} - c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\},\n$$\nfor $t = 0,1,\\dots,T-2$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The cumulative simple return over the horizon from day $1$ to day $T-1$ is\n$$\nR = \\prod_{u=1}^{T-1} (1 + p_u) - 1.\n$$\nYour program must compute $R$ for each test case provided below and output each $R$ rounded to $6$ decimal places.\n\nTest Suite (each case provides $C_t$, $P_t$, $r_t$, $t_\\ell$, $t_h$, and $c$):\n- Case $1$ (happy path, varying signals and costs): $T = 6$ with $C_t$ equal to $100, 150, 80, 50, 200, 90$; $P_t$ equal to $200, 100, 120, 100, 50, 110$; $r_t$ equal to $0.01, -0.005, 0.008, -0.02, 0.015, 0.005$; $t_\\ell = 0.8$; $t_h = 1.2$; $c = 0.001$.\n- Case $2$ (boundary conditions where $q_t$ equals thresholds yields no trade): $T = 5$ with $C_t$ equal to $50, 100, 200, 60, 40$; $P_t$ equal to $100, 50, 100, 120, 80$; $r_t$ equal to $0.01, 0.02, -0.01, 0.005, -0.005$; $t_\\ell = 0.5$; $t_h = 2.0$; $c = 0.002$.\n- Case $3$ (zero put volume and both-zero volumes, strict contrarian behavior): $T = 6$ with $C_t$ equal to $0, 10, 20, 0, 5, 0$; $P_t$ equal to $0, 0, 5, 0, 10, 0$; $r_t$ equal to $0.01, -0.02, 0.03, -0.01, 0.005, 0.002$; $t_\\ell = 0.8$; $t_h = 3.0$; $c = 0.0015$.\n- Case $4$ (persistent extreme sentiment, mostly one-sided position with minimal turnover): $T = 5$ with $C_t$ equal to $300, 320, 310, 305, 315$; $P_t$ equal to $100, 90, 95, 100, 85$; $r_t$ equal to $0.01, -0.005, 0.002, 0.004, -0.003$; $t_\\ell = 0.7$; $t_h = 2.5$; $c = 0.0005$.\n\nRequired final output format: Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, for example, $\\texttt{[0.123456,-0.010000,0.000000,0.250000]}$.", "solution": "The problem requires the computation of the cumulative return for a quantitative trading strategy based on a contrarian interpretation of option market sentiment. The strategy is implemented over a discrete-time horizon $t \\in \\{0, 1, \\dots, T-1\\}$. The validation of the problem statement confirms its scientific and mathematical soundness, completeness, and objectivity. It constitutes a well-posed problem in computational finance. We proceed with the solution.\n\nThe solution is derived by algorithmically simulating the strategy day by day. The process consists of three main stages: signal generation, daily profit calculation, and cumulative return aggregation.\n\n**1. Signal Generation**\n\nFor each trading day $t$, from $t=0$ to $t=T-1$, a trading signal $s_t \\in \\{-1, 0, +1\\}$ is determined. This signal dictates the position to be held for the following day, $t+1$. The signal is derived from the call-put volume ratio, $q_t$.\n\nFirst, the ratio $q_t$ is calculated based on the daily call volume $C_t$ and put volume $P_t$:\n$$\nq_t =\n\\begin{cases}\n\\dfrac{C_t}{P_t}, & \\text{if } P_t > 0, \\\\\n+\\infty, & \\text{if } P_t = 0 \\text{ and } C_t > 0, \\\\\n0, & \\text{if } P_t = 0 \\text{ and } C_t = 0.\n\\end{cases}\n$$\nThe value of $q_t=+\\infty$ is handled computationally using floating-point infinity.\n\nNext, the signal $s_t$ is determined by comparing $q_t$ against two predefined thresholds, a lower threshold $t_\\ell$ and an upper threshold $t_h$, where $0 < t_\\ell < t_h$:\n$$\ns_t =\n\\begin{cases}\n+1, & \\text{if } q_t < t_\\ell, \\\\\n-1, & \\text{if } q_t > t_h, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nA signal $s_t=+1$ represents a long position, $s_t=-1$ a short position, and $s_t=0$ a neutral (flat) position. This process is repeated for each day $t$ to generate a time series of signals, $s_0, s_1, \\dots, s_{T-1}$.\n\n**2. Daily Profit and Loss (P&L) Calculation**\n\nThe profit for day $u$, denoted $p_u$, is calculated for $u \\in \\{1, \\dots, T-1\\}$. The profit $p_{t+1}$ depends on the signal $s_t$ (determined at the close of day $t$), the asset's simple return $r_{t+1}$ on day $t+1$, and a proportional transaction cost $c$. The formula for the profit on day $t+1$ is given by:\n$$\np_{t+1} = s_t \\cdot r_{t+1} - c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\}\n$$\nfor $t = 0, 1, \\dots, T-2$. The initial position is given as $s_{-1} = 0$.\n\nThe term $s_t \\cdot r_{t+1}$ represents the investment return. A long position ($s_t=+1$) profits from a positive return ($r_{t+1}>0$) and loses from a negative return. A short position ($s_t=-1$) profits from a negative return ($r_{t+1}<0$) and loses from a positive return.\n\nThe term $c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\}$ represents the transaction cost. The indicator function $\\mathbf{1}\\{\\cdot\\}$ is $1$ if its argument is true and $0$ otherwise. A cost of $c$ is incurred only when the position at the end of day $t$ ($s_t$) is different from the position held during day $t$ (which was determined by $s_{t-1}$). The sequence of daily profits $p_1, p_2, \\dots, p_{T-1}$ is calculated by iterating from $t=0$ to $t=T-2$.\n\n**3. Cumulative Return Calculation**\n\nThe total performance of the strategy over the horizon from day $1$ to day $T-1$ is measured by the cumulative simple return, $R$. This is calculated by geometrically compounding the daily returns $(1+p_u)$:\n$$\nR = \\prod_{u=1}^{T-1} (1 + p_u) - 1.\n$$\nThis final value $R$ represents the total growth of the initial capital, net of all transaction costs.\n\nThe implementation will process each test case by sequentially applying these steps. The time series for $C_t$, $P_t$, and $r_t$ are stored in arrays. The signals $s_t$ are first computed for all $t$ and stored. Then, a second loop computes the daily profits $p_{t+1}$ using the signals and returns. Finally, the cumulative product of $(1+p_u)$ yields the total return. The result for each case is rounded to $6$ decimal places as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_cumulative_return(T, C, P, r, t_ell, t_h, c):\n    \"\"\"\n    Calculates the cumulative return of the contrarian trading strategy.\n\n    Args:\n        T (int): The total number of days in the data series.\n        C (np.ndarray): Array of call option volumes.\n        P (np.ndarray): Array of put option volumes.\n        r (np.ndarray): Array of asset's simple returns.\n        t_ell (float): The lower threshold for the call-put ratio.\n        t_h (float): The upper threshold for the call-put ratio.\n        c (float): The proportional transaction cost.\n\n    Returns:\n        float: The cumulative simple return R.\n    \"\"\"\n    \n    # Step 1: Generate trading signals s_t for t = 0, ..., T-1.\n    s = np.zeros(T)\n    for t in range(T):\n        C_t, P_t = C[t], P[t]\n        \n        q_t = 0.0\n        if P_t > 0:\n            q_t = C_t / P_t\n        elif P_t == 0 and C_t > 0:\n            q_t = np.inf\n        # If P_t == 0 and C_t == 0, q_t remains 0.0 as initialized.\n        \n        if q_t < t_ell:\n            s[t] = 1.0\n        elif q_t > t_h:\n            s[t] = -1.0\n        # Otherwise, s[t] remains 0.0 as initialized.\n\n    # Step 2: Calculate daily profits p_{t+1} for t = 0, ..., T-2.\n    # This yields profits p_1, ..., p_{T-1}.\n    profits = []\n    s_prev = 0.0 # Initial position s_{-1} = 0.\n    \n    # The loop runs for t from 0 to T-2.\n    for t in range(T - 1):\n        s_t = s[t]\n        r_t_plus_1 = r[t + 1]\n        \n        # Calculate transaction cost\n        transaction_cost = c if s_t != s_prev else 0.0\n        \n        # Calculate profit for day t+1\n        profit_t_plus_1 = s_t * r_t_plus_1 - transaction_cost\n        profits.append(profit_t_plus_1)\n        \n        # Update s_prev for the next iteration\n        s_prev = s_t\n\n    # Step 3: Calculate the cumulative simple return R.\n    # R = product(1 + p_u) - 1, for u = 1, ..., T-1.\n    if not profits:\n        return 0.0\n\n    compounded_factors = [1 + p for p in profits]\n    cumulative_return = np.prod(compounded_factors) - 1.0\n    \n    return cumulative_return\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": 6,\n            \"C\": np.array([100, 150, 80, 50, 200, 90]),\n            \"P\": np.array([200, 100, 120, 100, 50, 110]),\n            \"r\": np.array([0.01, -0.005, 0.008, -0.02, 0.015, 0.005]),\n            \"t_ell\": 0.8, \"t_h\": 1.2, \"c\": 0.001\n        },\n        {\n            \"T\": 5,\n            \"C\": np.array([50, 100, 200, 60, 40]),\n            \"P\": np.array([100, 50, 100, 120, 80]),\n            \"r\": np.array([0.01, 0.02, -0.01, 0.005, -0.005]),\n            \"t_ell\": 0.5, \"t_h\": 2.0, \"c\": 0.002\n        },\n        {\n            \"T\": 6,\n            \"C\": np.array([0, 10, 20, 0, 5, 0]),\n            \"P\": np.array([0, 0, 5, 0, 10, 0]),\n            \"r\": np.array([0.01, -0.02, 0.03, -0.01, 0.005, 0.002]),\n            \"t_ell\": 0.8, \"t_h\": 3.0, \"c\": 0.0015\n        },\n        {\n            \"T\": 5,\n            \"C\": np.array([300, 320, 310, 305, 315]),\n            \"P\": np.array([100, 90, 95, 100, 85]),\n            \"r\": np.array([0.01, -0.005, 0.002, 0.004, -0.003]),\n            \"t_ell\": 0.7, \"t_h\": 2.5, \"c\": 0.0005\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        R = calculate_cumulative_return(\n            case[\"T\"], case[\"C\"], case[\"P\"], case[\"r\"],\n            case[\"t_ell\"], case[\"t_h\"], case[\"c\"]\n        )\n        # Format to 6 decimal places.\n        results.append(f\"{R:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "Moving beyond simple rules on raw data, our next practice explores how advanced signal processing can enhance traditional trading indicators. This problem [@problem_id:2371373] challenges you to implement a Haar wavelet transform to denoise a simulated price series before calculating the common Moving Average Convergence Divergence (MACD) indicator. This exercise is invaluable as it demonstrates a powerful technique for improving signal quality and builds foundational skills in applying sophisticated mathematical tools to financial data.", "id": "2371373", "problem": "You are given a specification to construct, from first principles, a program that evaluates a simple algorithmic trading signal derived from a denoised price series. The price series is a simulated asset path generated by a Geometric Brownian Motion (GBM). The denoising must be performed using a discrete wavelet transform in the Haar basis with hard-thresholding, after which a Moving Average Convergence Divergence (MACD) indicator is computed on the denoised series. The final outputs are the last values of the MACD histogram for a provided suite of test cases.\n\nMathematical setup:\n\n1. Price generation via Geometric Brownian Motion. For each test case, generate a price sequence $\\{P_t\\}_{t=0}^{T-1}$ in units of an arbitrary currency (no physical units required) with initial price $P_0 = S_0$ and time step $\\Delta t = dt$ using\n$$\nP_{t+1} = P_t \\exp\\left(\\left(\\mu - \\tfrac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t}\\, Z_t\\right),\n$$\nwhere $\\mu$ is the drift, $\\sigma$ is the volatility, and $Z_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed standard normal random variables. Randomness must be controlled by a fixed integer seed in each test case to ensure reproducibility.\n\n2. Denoising by discrete Haar wavelet transform with hard thresholding.\n   - Let $\\{x_n\\}_{n=0}^{N-1}$ denote the observed sequence to be denoised, here $x_n = P_n$. Let $M$ be the smallest power of two such that $M \\ge N$. Construct a symmetrically reflected right-padding of $\\{x_n\\}$ to length $M$ to obtain $\\{\\tilde{x}_n\\}_{n=0}^{M-1}$. Reflection is defined by extending the sequence by mirroring around the right endpoint without duplicating the endpoint itself.\n   - Define the one-level orthonormal Haar transform mapping an even-length sequence $\\{a_n\\}_{n=0}^{L-1}$ into approximation coefficients $\\{s_k\\}_{k=0}^{L/2-1}$ and detail coefficients $\\{d_k\\}_{k=0}^{L/2-1}$ by\n     $$\n     s_k = \\frac{a_{2k} + a_{2k+1}}{\\sqrt{2}}, \\quad d_k = \\frac{a_{2k} - a_{2k+1}}{\\sqrt{2}}.\n     $$\n     A multilevel transform of depth $J$ is formed by iterating this mapping $J$ times on the approximation coefficients only, producing a top-level approximation $\\{s^{(J)}\\}$ and detail sets $\\{d^{(j)}\\}$ for $j=1,\\dots,J$.\n   - Estimate the noise level using the median absolute deviation of the first-level details:\n     $$\n     \\hat{\\sigma} = \\frac{\\operatorname{median}\\left(\\left|d^{(1)}_k - \\operatorname{median}(d^{(1)})\\right|\\right)}{0.67448975}.\n     $$\n     Define the universal hard-threshold\n     $$\n     \\lambda = k \\, \\hat{\\sigma} \\sqrt{2 \\log M},\n     $$\n     where $k$ is a given scalar. Apply hard-thresholding to every detail coefficient at every level using the same $\\lambda$:\n     $$\n     \\tilde{d}^{(j)}_k = \\begin{cases}\n     d^{(j)}_k & \\text{if } |d^{(j)}_k| \\ge \\lambda, \\\\\n     0 & \\text{otherwise}.\n     \\end{cases}\n     $$\n     Keep the top-level approximation unchanged, $\\tilde{s}^{(J)} = s^{(J)}$.\n   - Reconstruct the denoised sequence $\\{\\hat{x}_n\\}_{n=0}^{M-1}$ by inverse transforms level by level using\n     $$\n     a_{2k} = \\frac{s_k + d_k}{\\sqrt{2}}, \\quad a_{2k+1} = \\frac{s_k - d_k}{\\sqrt{2}}.\n     $$\n     After $J$ inverse levels, truncate the right-reflected padding to return to the original length $N$, yielding $\\{\\hat{x}_n\\}_{n=0}^{N-1}$.\n\n3. Moving Average Convergence Divergence (MACD) on the denoised price. Let $\\{\\hat{P}_t\\}_{t=0}^{T-1}$ be the denoised prices. For integers $n_{\\text{fast}}$, $n_{\\text{slow}}$, and $n_{\\text{sig}}$, define exponential moving averages (EMA) using the recursion\n   $$\n   \\operatorname{EMA}^{(n)}_t = \\alpha_n \\hat{P}_t + (1 - \\alpha_n)\\operatorname{EMA}^{(n)}_{t-1}, \\quad \\alpha_n = \\frac{2}{n+1}, \\quad \\operatorname{EMA}^{(n)}_0 = \\hat{P}_0.\n   $$\n   The MACD line is\n   $$\n   \\operatorname{MACD}_t = \\operatorname{EMA}^{(n_{\\text{fast}})}_t - \\operatorname{EMA}^{(n_{\\text{slow}})}_t.\n   $$\n   The signal line is the exponential moving average of the MACD line,\n   $$\n   \\operatorname{SIG}_t = \\alpha_{n_{\\text{sig}}} \\operatorname{MACD}_t + (1 - \\alpha_{n_{\\text{sig}}}) \\operatorname{SIG}_{t-1}, \\quad \\operatorname{SIG}_0 = \\operatorname{MACD}_0.\n   $$\n   The histogram is\n   $$\n   H_t = \\operatorname{MACD}_t - \\operatorname{SIG}_t.\n   $$\n   The required per-test-case output is the final histogram value $H_{T-1}$.\n\nTest suite:\n\nImplement the above for the following four test cases. Each test case specifies the GBM parameters $(T, S_0, \\mu, \\sigma, dt, \\text{seed})$, the wavelet denoising parameters $(J, k)$, and the MACD parameters $(n_{\\text{fast}}, n_{\\text{slow}}, n_{\\text{sig}})$. All integers and reals are given explicitly.\n\n- Case A (general case):\n  - $T = 256$, $S_0 = 100$, $\\mu = 0.06$, $\\sigma = 0.2$, $dt = \\frac{1}{252}$, $\\text{seed} = 12345$.\n  - $J = 3$, $k = 1.0$.\n  - $n_{\\text{fast}} = 12$, $n_{\\text{slow}} = 26$, $n_{\\text{sig}} = 9$.\n\n- Case B (low noise boundary):\n  - $T = 256$, $S_0 = 100$, $\\mu = 0.0$, $\\sigma = 0.0001$, $dt = \\frac{1}{252}$, $\\text{seed} = 2024$.\n  - $J = 3$, $k = 1.0$.\n  - $n_{\\text{fast}} = 12$, $n_{\\text{slow}} = 26$, $n_{\\text{sig}} = 9$.\n\n- Case C (non-power-of-two length and wider slow window):\n  - $T = 90$, $S_0 = 50$, $\\mu = 0.03$, $\\sigma = 0.3$, $dt = \\frac{1}{252}$, $\\text{seed} = 99$.\n  - $J = 4$, $k = 0.5$.\n  - $n_{\\text{fast}} = 5$, $n_{\\text{slow}} = 35$, $n_{\\text{sig}} = 5$.\n\n- Case D (high threshold stress):\n  - $T = 128$, $S_0 = 200$, $\\mu = -0.02$, $\\sigma = 0.25$, $dt = \\frac{1}{252}$, $\\text{seed} = 7$.\n  - $J = 3$, $k = 3.0$.\n  - $n_{\\text{fast}} = 12$, $n_{\\text{slow}} = 26$, $n_{\\text{sig}} = 9$.\n\nAnswer and output format:\n\n- For each case, compute the final histogram value $H_{T-1}$ as a real number.\n- Express each result as a decimal rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_i$ is the rounded value for case $i$ in the order A, B, C, D.", "solution": "The problem presented is a complete and rigorously defined exercise in computational signal processing applied to financial time series analysis. It is scientifically grounded, well-posed, and objective. We shall proceed with its solution.\n\nThe task is to construct a computational procedure that involves three primary stages:\n1.  Generation of a simulated asset price series using the Geometric Brownian Motion (GBM) model.\n2.  Denoising of this price series using a discrete Haar wavelet transform with a specific hard-thresholding strategy.\n3.  Calculation of the Moving Average Convergence Divergence (MACD) technical indicator on the denoised series.\n\nThe final output is the last value of the MACD histogram for several specified parameter sets. We will construct the solution methodologically, addressing each stage from first principles as stipulated.\n\n**1. Price Series Generation via Geometric Brownian Motion**\n\nThe price of an asset, $P_t$, at time $t$ is modeled by the stochastic differential equation for GBM. The discrete-time solution for a time step $\\Delta t$ is provided:\n$$\nP_{t+1} = P_t \\exp\\left(\\left(\\mu - \\tfrac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t}\\, Z_t\\right)\n$$\nHere, $\\mu$ is the drift rate, $\\sigma$ is the volatility, and $Z_t$ are independent and identically distributed random variables from the standard normal distribution, $Z_t \\sim \\mathcal{N}(0,1)$. The problem requires simulations for $T$ time steps, starting from an initial price $P_0 = S_0$. To ensure reproducibility, a fixed seed must be used for the pseudo-random number generator for each test case.\n\nFor each case, we will generate a sequence $\\{P_t\\}_{t=0}^{T-1}$ of length $T$ by iterating the above equation for $t$ from $0$ to $T-2$.\n\n**2. Wavelet Denoising**\n\nThis stage involves a multi-level discrete Haar wavelet transform (DWT), thresholding of detail coefficients, and an inverse transform.\n\n**2.1. Padding**\nThe input sequence $\\{P_n\\}_{n=0}^{N-1}$ (where $N=T$) must have a length $M$ that is a power of two for the specified DWT algorithm. If $N$ is not a power of two, we determine the smallest integer $M$ such that $M=2^k$ and $M \\ge N$. The sequence is then extended to length $M$ via symmetric reflection on the right boundary. The description \"mirroring around the right endpoint without duplicating the endpoint itself\" implies a 'reflect' padding mode. For a sequence $\\{x_n\\}_{n=0}^{N-1}$, the padded sequence $\\{\\tilde{x}_n\\}_{n=0}^{M-1}$ is constructed by appending the elements $\\{x_{N-2}, x_{N-3}, \\dots \\}$. Let $K=M-N$ be the number of elements to append. The appended part will be $\\{x_{N-2}, x_{N-3}, ..., x_{N-1-K}\\}$. This procedure is only necessary if $N$ is not a power of two, which occurs only in Case C. For all other cases, $N$ is a power of two, so $M=N$ and no padding is performed.\n\n**2.2. Forward Haar DWT**\nThe orthonormal Haar DWT maps a sequence of length $L$ (where $L$ is even) to a sequence of approximation coefficients $\\{s_k\\}$ and detail coefficients $\\{d_k\\}$, each of length $L/2$. The transformations are:\n$$\ns_k = \\frac{a_{2k} + a_{2k+1}}{\\sqrt{2}}, \\quad d_k = \\frac{a_{2k} - a_{2k+1}}{\\sqrt{2}}\n$$\nA $J$-level transform is performed by recursively applying this decomposition to the approximation coefficients. Starting with $\\{\\tilde{x}_n\\}_{n=0}^{M-1}$, we compute the first-level details $\\{d^{(1)}_k\\}$ and approximations $\\{s^{(1)}_k\\}$. Then, we decompose $\\{s^{(1)}_k\\}$ to get $\\{d^{(2)}_k\\}$ and $\\{s^{(2)}_k\\}$, and so on, for $J$ levels. This yields a set of coefficient arrays: the final approximations $\\{s^{(J)}\\}$ and the detail coefficients at each level $\\{d^{(j)}\\}$ for $j=1, \\dots, J$.\n\n**2.3. Thresholding**\nDenoising is achieved by thresholding the detail coefficients. First, the noise level $\\hat{\\sigma}$ is estimated from the first-level detail coefficients $d^{(1)}$, which capture the finest-scale variations. The robust Median Absolute Deviation (MAD) estimator is used:\n$$\n\\hat{\\sigma} = \\frac{\\operatorname{median}\\left(\\left|d^{(1)}_k - \\operatorname{median}(d^{(1)})\\right|\\right)}{C}\n$$\nwhere $C = 0.67448975 \\approx 1/\\Phi^{-1}(3/4)$ is the normalization constant for asymptotically normal data.\n\nA universal hard threshold $\\lambda$ is then calculated:\n$$\n\\lambda = k \\, \\hat{\\sigma} \\sqrt{2 \\log M}\n$$\nwhere $k$ is a specified scaling factor. This threshold is applied to all detail coefficients at all levels $j=1, \\dots, J$:\n$$\n\\tilde{d}^{(j)}_k = d^{(j)}_k \\cdot \\mathbf{1}_{|d^{(j)}_k| \\ge \\lambda} = \\begin{cases}\nd^{(j)}_k & \\text{if } |d^{(j)}_k| \\ge \\lambda, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nThe top-level approximation coefficients $\\{s^{(J)}\\}$ are not modified, i.e., $\\tilde{s}^{(J)} = s^{(J)}$. This preserves the coarse structure of the signal.\n\n**2.4. Inverse Haar DWT and Truncation**\nThe denoised signal is reconstructed from the modified coefficients $\\{\\tilde{s}^{(J)}\\}$ and $\\{\\tilde{d}^{(j)}\\}$. The one-level inverse Haar transform is:\n$$\na_{2k} = \\frac{s_k + d_k}{\\sqrt{2}}, \\quad a_{2k+1} = \\frac{s_k - d_k}{\\sqrt{2}}\n$$\nStarting with $\\{\\tilde{s}^{(J)}\\}$ and $\\{\\tilde{d}^{(J)}\\}$, we reconstruct $\\{\\tilde{s}^{(J-1)}\\}$. This process is iterated $J$ times, using the detail coefficients from each corresponding level, until the full-length signal $\\{\\hat{x}_n\\}_{n=0}^{M-1}$ is recovered.\n\nFinally, if the signal was padded, we truncate the reconstructed signal to its original length $N$ by taking the first $N$ elements. This yields the denoised price series $\\{\\hat{P}_t\\}_{t=0}^{T-1}$.\n\n**3. MACD Calculation**\n\nThe MACD indicator is calculated on the denoised price series $\\{\\hat{P}_t\\}$. The process requires computing several Exponential Moving Averages (EMAs). An EMA of period $n$ is defined by the recurrence relation:\n$$\n\\operatorname{EMA}^{(n)}_t = \\alpha_n \\hat{P}_t + (1 - \\alpha_n)\\operatorname{EMA}^{(n)}_{t-1}\n$$\nwith smoothing factor $\\alpha_n = 2/(n+1)$ and initial condition $\\operatorname{EMA}^{(n)}_0 = \\hat{P}_0$.\n\nThe components of the MACD indicator are:\n1.  **MACD Line**: The difference between a \"fast\" EMA (short period $n_{\\text{fast}}$) and a \"slow\" EMA (long period $n_{\\text{slow}}$).\n    $$\n    \\operatorname{MACD}_t = \\operatorname{EMA}^{(n_{\\text{fast}})}_t - \\operatorname{EMA}^{(n_{\\text{slow}})}_t\n    $$\n2.  **Signal Line**: An EMA of the MACD line itself, with period $n_{\\text{sig}}$.\n    $$\n    \\operatorname{SIG}_t = \\alpha_{n_{\\text{sig}}} \\operatorname{MACD}_t + (1 - \\alpha_{n_{\\text{sig}}}) \\operatorname{SIG}_{t-1}\n    $$\n    The initial condition is specified as $\\operatorname{SIG}_0 = \\operatorname{MACD}_0$.\n3.  **Histogram**: The difference between the MACD line and the Signal line.\n    $$\n    H_t = \\operatorname{MACD}_t - \\operatorname{SIG}_t\n    $$\nThe final value to be computed for each test case is $H_{T-1}$.\n\nThe entire procedure is deterministic given the parameters and random seed for each case. The implementation must follow these steps precisely to arrive at the correct numerical answers.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the GBM simulation, wavelet denoising, and MACD calculation.\n    \"\"\"\n\n    test_cases = [\n        # Case A\n        {'T': 256, 'S0': 100.0, 'mu': 0.06, 'sigma': 0.2, 'dt': 1/252, 'seed': 12345,\n         'J': 3, 'k': 1.0, 'n_fast': 12, 'n_slow': 26, 'n_sig': 9},\n        # Case B\n        {'T': 256, 'S0': 100.0, 'mu': 0.0, 'sigma': 0.0001, 'dt': 1/252, 'seed': 2024,\n         'J': 3, 'k': 1.0, 'n_fast': 12, 'n_slow': 26, 'n_sig': 9},\n        # Case C\n        {'T': 90, 'S0': 50.0, 'mu': 0.03, 'sigma': 0.3, 'dt': 1/252, 'seed': 99,\n         'J': 4, 'k': 0.5, 'n_fast': 5, 'n_slow': 35, 'n_sig': 5},\n        # Case D\n        {'T': 128, 'S0': 200.0, 'mu': -0.02, 'sigma': 0.25, 'dt': 1/252, 'seed': 7,\n         'J': 3, 'k': 3.0, 'n_fast': 12, 'n_slow': 26, 'n_sig': 9},\n    ]\n\n    results = []\n    \n    # Pre-calculate sqrt(2) as it is used repeatedly in wavelet transforms.\n    SQRT2 = np.sqrt(2)\n    MAD_NORMALIZATION_CONSTANT = 0.67448975\n\n    def generate_gbm(T, S0, mu, sigma, dt, seed):\n        \"\"\"Generates a price series using Geometric Brownian Motion.\"\"\"\n        rng = np.random.default_rng(seed)\n        prices = np.zeros(T)\n        prices[0] = S0\n        drift = (mu - 0.5 * sigma**2) * dt\n        diffusion = sigma * np.sqrt(dt)\n        \n        for t in range(T - 1):\n            Z = rng.standard_normal()\n            prices[t+1] = prices[t] * np.exp(drift + diffusion * Z)\n        return prices\n\n    def fwt_haar(data, level):\n        \"\"\"Performs a J-level forward discrete Haar wavelet transform.\"\"\"\n        coeffs = []\n        a = data.copy()\n        for _ in range(level):\n            s = (a[0::2] + a[1::2]) / SQRT2\n            d = (a[0::2] - a[1::2]) / SQRT2\n            coeffs.insert(0, d)\n            a = s\n        coeffs.insert(0, a)\n        return coeffs\n\n    def iwt_haar(coeffs):\n        \"\"\"Performs a J-level inverse discrete Haar wavelet transform.\"\"\"\n        a = coeffs[0]\n        for d in coeffs[1:]:\n            s_plus_d = a + d\n            s_minus_d = a - d\n            a_new = np.zeros(2 * len(a))\n            a_new[0::2] = s_plus_d / SQRT2\n            a_new[1::2] = s_minus_d / SQRT2\n            a = a_new\n        return a\n\n    def denoise_wavelet(prices, J, k):\n        \"\"\"Denoises a time series using Haar wavelet transform and hard thresholding.\"\"\"\n        N = len(prices)\n        \n        # Calculate M = smallest power of 2 >= N\n        if (N & (N - 1) == 0) and N > 0: # N is a power of two\n            M = N\n        else:\n            M = 1 << N.bit_length()\n\n        # Pad the signal if necessary\n        if N < M:\n            padded_prices = np.zeros(M)\n            padded_prices[:N] = prices\n            pad_len = M - N\n            # 'reflect' padding: mirror around the endpoint without duplication\n            pad_values = np.flip(prices[N-pad_len-1:N-1]) # numpy flip is more robust than [::-1]\n            padded_prices[N:] = pad_values\n        else:\n            padded_prices = prices\n\n        # Forward Wavelet Transform\n        coeffs = fwt_haar(padded_prices, J)\n        \n        # Noise estimation and thresholding\n        d1 = coeffs[-1]\n        med_d1 = np.median(d1)\n        mad = np.median(np.abs(d1 - med_d1))\n        sigma_hat = mad / MAD_NORMALIZATION_CONSTANT\n        \n        if sigma_hat == 0: # Prevent division by zero or invalid log\n            lambda_thresh = 0\n        else:\n            lambda_thresh = k * sigma_hat * np.sqrt(2 * np.log(M))\n\n        # Apply hard thresholding to detail coefficients\n        thresholded_coeffs = [coeffs[0]] # Keep approximation coeffs\n        for d in coeffs[1:]:\n            d_thresh = np.where(np.abs(d) >= lambda_thresh, d, 0)\n            thresholded_coeffs.append(d_thresh)\n            \n        # Inverse Wavelet Transform\n        reconstructed_signal = iwt_haar(thresholded_coeffs)\n        \n        # Truncate to original length\n        return reconstructed_signal[:N]\n\n    def calculate_ema(data, n):\n        \"\"\"Calculates Exponential Moving Average.\"\"\"\n        T = len(data)\n        ema = np.zeros(T)\n        alpha = 2 / (n + 1)\n        ema[0] = data[0]\n        for t in range(1, T):\n            ema[t] = alpha * data[t] + (1 - alpha) * ema[t-1]\n        return ema\n        \n    for case in test_cases:\n        # 1. Generate price series\n        prices = generate_gbm(case['T'], case['S0'], case['mu'], case['sigma'], case['dt'], case['seed'])\n        \n        # 2. Denoise the price series\n        denoised_prices = denoise_wavelet(prices, case['J'], case['k'])\n        \n        # 3. Calculate MACD\n        ema_fast = calculate_ema(denoised_prices, case['n_fast'])\n        ema_slow = calculate_ema(denoised_prices, case['n_slow'])\n        \n        macd_line = ema_fast - ema_slow\n        \n        # Calculate Signal line\n        sig_line = np.zeros_like(macd_line)\n        alpha_sig = 2 / (case['n_sig'] + 1)\n        sig_line[0] = macd_line[0]\n        for t in range(1, len(macd_line)):\n            sig_line[t] = alpha_sig * macd_line[t] + (1 - alpha_sig) * sig_line[t-1]\n        \n        histogram = macd_line - sig_line\n        \n        final_hist_val = histogram[-1]\n        results.append(round(final_hist_val, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"}, {"introduction": "Our final practice takes a leap into the frontier of algorithmic trading by employing machine learning to create an adaptive strategy. In this exercise [@problem_id:2371418], you will implement a Q-learning agent that learns to dynamically switch between different trading strategies based on its assessment of the prevailing market regime. This hands-on implementation of a reinforcement learning model introduces a powerful paradigm for building intelligent systems that can adapt their behavior in response to evolving market conditions.", "id": "2371418", "problem": "You are asked to implement a complete, runnable program that trains an off-policy control agent using Q-learning in a finite-state Markov Decision Process (MDP) representation of market regimes. The agent’s states are market regimes and its actions are algorithmic trading strategies. The environment is fully specified and stationary. Your task is to compute, for a small test suite, the greedy policy induced by the learned state-action value function after training concludes. Your program must produce exactly one line of output in the format specified at the end.\n\nThe fundamental base for this problem is the definition of a finite-state Markov Decision Process (MDP) with stationary transition probabilities and stationary reward function, the definition of discounted return with a discount factor, and the principle that optimal control in an MDP is characterized by the Bellman optimality condition. You must implement off-policy Temporal-Difference (TD) control using Q-learning based on these foundations, without relying on any external libraries beyond those permitted.\n\nState space and action space:\n- There are $3$ states (market regimes) indexed in the fixed order $[0,1,2]$ corresponding to $[\\,\\text{bull},\\,\\text{bear},\\,\\text{volatile}\\,]$.\n- There are $3$ actions (trading strategies) indexed in the fixed order $[0,1,2]$ corresponding to $[\\,\\text{momentum},\\,\\text{mean\\text{-}reversion},\\,\\text{cash/flat}\\,]$.\n\nEpisodic training and schedules:\n- Each training run consists of $E$ episodes, each episode has a fixed horizon length $H$ steps.\n- The initial state of each episode is drawn independently and uniformly from $\\{0,1,2\\}$.\n- Exploration uses an $\\varepsilon$-greedy policy with a linearly decaying exploration rate over the total training steps $T = E \\times H$. Specifically, if $t \\in \\{0,1,\\dots,T-1\\}$ indexes the global step count over the entire training run, then\n$$\n\\varepsilon(t) \\;=\\; \\max\\!\\left\\{\\varepsilon_{\\min},\\; \\varepsilon_{\\text{start}} \\;+\\; \\frac{t}{T-1}\\,\\big(\\varepsilon_{\\min} - \\varepsilon_{\\text{start}}\\big)\\right\\}.\n$$\n- The learning rate for each state-action pair is a diminishing sequence based on visit counts $N(s,a)$, given by\n$$\n\\alpha(s,a) \\;=\\; \\frac{\\alpha_0}{1 + N(s,a)}.\n$$\n- The discount factor is $\\gamma \\in [0,1)$.\n- All random draws (initial states, action sampling under $\\varepsilon$-greedy, and next-state transitions) must use a fixed pseudorandom number generator seed equal to $7$ for reproducibility.\n- Rewards are deterministic and given by the reward matrix; there is no exogenous reward noise.\n- Tie-breaking rule: whenever a maximum over actions is required (both for action selection at exploitation time and for computing the greedy policy after training), if multiple actions attain the same maximal value, select the action with the smallest index.\n\nEnvironment specification:\n- The reward function is given by a matrix $R \\in \\mathbb{R}^{3 \\times 3}$ where $R[s,a]$ is the deterministic immediate reward in state $s$ when taking action $a$.\n- The transition dynamics are given either by a single action-independent transition matrix $P \\in [0,1]^{3 \\times 3}$ with rows summing to $1$ (identical for all actions), or by an action-dependent kernel $\\{P^{(a)}\\}_{a=0}^2$ with $P^{(a)} \\in [0,1]^{3 \\times 3}$ and each row summing to $1$.\n\nYou must implement Q-learning (off-policy TD control) consistent with the above, using $\\varepsilon$-greedy behavior during training and greedy action selection only for the final reported policy.\n\nTest suite:\nImplement your solution to handle the following four test cases. For each case, train a fresh agent from scratch, then report the final greedy policy as a list of three integers $[\\,\\pi(0),\\,\\pi(1),\\,\\pi(2)\\,]$ in the prescribed state order.\n\n- Case A (happy path; action-independent dynamics; nonzero discount):\n    - Discount: $\\gamma = 0.95$.\n    - Episodes and horizon: $E = 10000$, $H = 40$.\n    - Exploration: $\\varepsilon_{\\text{start}} = 0.8$, $\\varepsilon_{\\min} = 0.05$.\n    - Learning rate base: $\\alpha_0 = 0.5$.\n    - Rewards:\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.8 & -0.3 & 0.1 \\\\\n    -0.6 & 0.5 & 0.05 \\\\\n    0.2 & 0.3 & 0.02\n    \\end{bmatrix}.\n    $$\n    - Transitions (action-independent):\n    $$\n    P \\;=\\; \\begin{bmatrix}\n    0.85 & 0.05 & 0.10 \\\\\n    0.05 & 0.80 & 0.15 \\\\\n    0.20 & 0.20 & 0.60\n    \\end{bmatrix}.\n    $$\n\n- Case B (boundary condition; myopic control with zero discount):\n    - Discount: $\\gamma = 0$.\n    - Episodes and horizon: $E = 2000$, $H = 30$.\n    - Exploration: $\\varepsilon_{\\text{start}} = 0.8$, $\\varepsilon_{\\min} = 0.1$.\n    - Learning rate base: $\\alpha_0 = 0.5$.\n    - Rewards: same $R$ as in Case A.\n    - Transitions: same $P$ as in Case A (action-independent).\n\n- Case C (edge case; tie in one state’s immediate rewards; action-independent dynamics):\n    - Discount: $\\gamma = 0.95$.\n    - Episodes and horizon: $E = 10000$, $H = 40$.\n    - Exploration: $\\varepsilon_{\\text{start}} = 0.8$, $\\varepsilon_{\\min} = 0.05$.\n    - Learning rate base: $\\alpha_0 = 0.5$.\n    - Rewards:\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.8 & -0.3 & 0.1 \\\\\n    -0.6 & 0.5 & 0.05 \\\\\n    0.3 & 0.3 & 0.02\n    \\end{bmatrix}.\n    $$\n    - Transitions (action-independent): same $P$ as in Case A.\n    - Note: In state $2$ (volatile), actions $0$ and $1$ have equal immediate rewards. Your tie-breaking rule must select the smallest index among maximizers.\n\n- Case D (action-dependent dynamics; long-run effect matters):\n    - Discount: $\\gamma = 0.99$.\n    - Episodes and horizon: $E = 20000$, $H = 50$.\n    - Exploration: $\\varepsilon_{\\text{start}} = 0.9$, $\\varepsilon_{\\min} = 0.05$.\n    - Learning rate base: $\\alpha_0 = 0.5$.\n    - Rewards:\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.7 & 0.2 & 0.1 \\\\\n    -0.1 & 0.05 & 0.02 \\\\\n    0.25 & 0.35 & 0.02\n    \\end{bmatrix}.\n    $$\n    - Action-dependent transitions:\n    $$\n    P^{(0)} \\;=\\; \\begin{bmatrix}\n    0.92 & 0.03 & 0.05 \\\\\n    0.99 & 0.005 & 0.005 \\\\\n    0.50 & 0.10 & 0.40\n    \\end{bmatrix},\\quad\n    P^{(1)} \\;=\\; \\begin{bmatrix}\n    0.80 & 0.05 & 0.15 \\\\\n    0.05 & 0.94 & 0.01 \\\\\n    0.40 & 0.20 & 0.40\n    \\end{bmatrix},\\quad\n    P^{(2)} \\;=\\; \\begin{bmatrix}\n    0.70 & 0.10 & 0.20 \\\\\n    0.01 & 0.98 & 0.01 \\\\\n    0.10 & 0.30 & 0.60\n    \\end{bmatrix}.\n    $$\n\nOutput specification:\n- After training completes for each case, compute the greedy policy $\\pi(s) = \\arg\\max_{a \\in \\{0,1,2\\}} Q(s,a)$ using the tie-breaking rule described above.\n- Your program should produce a single line of output containing all four case results as a comma-separated list of lists, with no spaces, enclosed in square brackets. For example, the printed line must look like\n\"[[x0,x1,x2],[y0,y1,y2],[z0,z1,z2],[w0,w1,w2]]\"\nwhere each symbol is an integer. Replace these placeholders with the actual learned policies for Cases A, B, C, and D, respectively, in that order.", "solution": "The problem presented is a well-posed and computationally verifiable exercise in reinforcement learning, specifically applying the Q-learning algorithm to a finite-state Markov Decision Process (MDP). The task is to determine the optimal control policy for a simplified model of market regimes. The problem statement is scientifically sound and provides all necessary parameters and specifications for a unique, reproducible solution. We shall proceed with the derivation and implementation.\n\nThe core of the problem lies in the framework of a Markov Decision Process, which is defined by a tuple $(S, A, P, R, \\gamma)$.\nThe state space $S$ consists of $3$ market regimes: $s \\in \\{0, 1, 2\\}$, corresponding to $[\\text{bull}, \\text{bear}, \\text{volatile}]$.\nThe action space $A$ consists of $3$ trading strategies: $a \\in \\{0, 1, 2\\}$, corresponding to $[\\text{momentum}, \\text{mean-reversion}, \\text{cash/flat}]$.\nThe reward function $R(s, a)$ provides the immediate, deterministic reward for taking action $a$ in state $s$.\nThe transition probability function $P(s'|s, a)$ gives the probability of transitioning to state $s'$ from state $s$ after taking action $a$.\nThe discount factor $\\gamma \\in [0, 1)$ determines the present value of future rewards.\n\nThe goal of a reinforcement learning agent is to find an optimal policy $\\pi^*(s)$ which maps states to actions in a way that maximizes the expected discounted return. The value of a state-action pair $(s, a)$ under a policy $\\pi$ is given by the state-action value function $Q^{\\pi}(s, a)$, representing the expected return starting from state $s$, taking action $a$, and thereafter following policy $\\pi$.\n\nThe optimal state-action value function, $Q^*(s, a) = \\max_{\\pi} Q^{\\pi}(s, a)$, must satisfy the Bellman optimality equation:\n$$\nQ^*(s, a) = \\mathbb{E}\\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]\n$$\nFor a deterministic reward function $R(s,a)$, this simplifies to:\n$$\nQ^*(s, a) = R(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) \\max_{a'} Q^*(s', a')\n$$\n\nSince the transition model $P$ and reward function $R$ are given, this system could be solved using dynamic programming methods like value iteration. However, the problem specifies the use of Q-learning, a model-free temporal difference (TD) learning algorithm. Q-learning directly estimates the optimal $Q^*$-values without requiring explicit knowledge of $P$ and $R$ (though in our case they are known and used to simulate the environment).\n\nQ-learning works by iteratively updating an estimate of the $Q$-value, denoted $Q(s, a)$, based on experience tuples $(S_t, A_t, R_{t+1}, S_{t+1})$. At each global time step $t$ of the training process, the agent is in state $S_t$ and takes an action $A_t$. It observes a reward $R_{t+1}$ and the next state $S_{t+1}$. The update rule is:\n$$\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha_t(S_t, A_t) \\left[ \\underbrace{R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a)}_{\\text{TD Target}} - Q(S_t, A_t) \\right]\n$$\nHere, $\\alpha_t(S_t, A_t)$ is the learning rate at step $t$ for the pair $(S_t, A_t)$. For the algorithm to converge, the learning rate must satisfy the Robbins-Monro conditions, which is approximated in this problem by a diminishing schedule based on visit counts $N(s, a)$:\n$$\n\\alpha(s, a) = \\frac{\\alpha_0}{1 + N(s, a)}\n$$\nwhere a base learning rate $\\alpha_0$ is given.\n\nQ-learning is an off-policy algorithm. This means it can learn the optimal policy while behaving according to a different, exploratory policy. The problem specifies an $\\varepsilon$-greedy behavioral policy. At each step $t$, the agent selects a random action with probability $\\varepsilon(t)$ and the greedy action $a = \\arg\\max_{a'} Q(S_t, a')$ with probability $1 - \\varepsilon(t)$. The exploration rate $\\varepsilon(t)$ decays linearly over the total training steps $T = E \\times H$, from a starting value $\\varepsilon_{\\text{start}}$ to a minimum value $\\varepsilon_{\\min}$:\n$$\n\\varepsilon(t) = \\max\\left\\{\\varepsilon_{\\min}, \\varepsilon_{\\text{start}} + \\frac{t}{T-1}\\left(\\varepsilon_{\\min} - \\varepsilon_{\\text{start}}\\right)\\right\\}\n$$\nThis ensures sufficient exploration in the beginning of training and a transition towards exploitation as the $Q$-value estimates become more reliable.\n\nThe implementation will proceed by initializing a $Q$-table, $Q(s, a) \\in \\mathbb{R}^{3 \\times 3}$, to all zeros. A visit count matrix, $N(s, a) \\in \\mathbb{Z}^{3 \\times 3}$, is also initialized to zeros. The training will run for $E$ episodes, each lasting $H$ steps. A pseudorandom number generator with a fixed seed of $7$ will be used for all stochastic elements: initial state sampling, $\\varepsilon$-greedy action selection, and next-state sampling from the transition probability distribution $P(\\cdot|s, a)$.\n\nAfter training is complete for each test case, the final learned $Q$-table is used to derive the purely greedy policy $\\pi(s)$:\n$$\n\\pi(s) = \\arg\\max_{a \\in A} Q(s, a)\n$$\nThe problem specifies a tie-breaking rule: if multiple actions yield the same maximal $Q$-value for a given state, the action with the smallest index is chosen. This rule will be strictly followed during both the exploitation phase of the $\\varepsilon$-greedy policy and the derivation of the final policy. The algorithm is implemented for each of the four test cases specified, and the resulting policies are reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Q-learning problem for four test cases and prints the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"name\": \"A\",\n            \"gamma\": 0.95,\n            \"E\": 10000,\n            \"H\": 40,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.2, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case B\n        {\n            \"name\": \"B\",\n            \"gamma\": 0.0,\n            \"E\": 2000,\n            \"H\": 30,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.1,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.2, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case C\n        {\n            \"name\": \"C\",\n            \"gamma\": 0.95,\n            \"E\": 10000,\n            \"H\": 40,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.3, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case D\n        {\n            \"name\": \"D\",\n            \"gamma\": 0.99,\n            \"E\": 20000,\n            \"H\": 50,\n            \"eps_start\": 0.9,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.7, 0.2, 0.1],\n                [-0.1, 0.05, 0.02],\n                [0.25, 0.35, 0.02]\n            ]),\n            \"P\": np.array([\n                # P for action 0\n                [[0.92, 0.03, 0.05], [0.99, 0.005, 0.005], [0.50, 0.10, 0.40]],\n                # P for action 1\n                [[0.80, 0.05, 0.15], [0.05, 0.94, 0.01], [0.40, 0.20, 0.40]],\n                # P for action 2\n                [[0.70, 0.10, 0.20], [0.01, 0.98, 0.01], [0.10, 0.30, 0.60]],\n            ]),\n            \"action_dependent_P\": True,\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Initialize RNG with fixed seed for reproducibility for each case\n        rng = np.random.default_rng(seed=7)\n        \n        # Extract parameters\n        gamma = case[\"gamma\"]\n        E = case[\"E\"]\n        H = case[\"H\"]\n        eps_start = case[\"eps_start\"]\n        eps_min = case[\"eps_min\"]\n        alpha_0 = case[\"alpha_0\"]\n        R = case[\"R\"]\n        P = case[\"P\"]\n        action_dependent_P = case[\"action_dependent_P\"]\n\n        num_states = 3\n        num_actions = 3\n        \n        # Initialize Q-table and visit counts\n        q_table = np.zeros((num_states, num_actions))\n        visit_counts = np.zeros((num_states, num_actions))\n        \n        total_steps = E * H\n        global_step_count = 0\n\n        # Training loop\n        for episode in range(E):\n            current_state = rng.choice(num_states)\n            \n            for step in range(H):\n                # Epsilon calculation\n                if total_steps > 1:\n                    epsilon = eps_start + (global_step_count / (total_steps - 1)) * (eps_min - eps_start)\n                    epsilon = max(eps_min, epsilon)\n                else:\n                    epsilon = eps_start\n\n                # Action selection (epsilon-greedy)\n                if rng.random() < epsilon:\n                    action = rng.choice(num_actions)\n                else:\n                    # Exploit: choose best action, np.argmax breaks ties by taking the first one\n                    action = np.argmax(q_table[current_state, :])\n                \n                # Get reward\n                reward = R[current_state, action]\n                \n                # Get next state from transition dynamics\n                if action_dependent_P:\n                    transition_probs = P[action, current_state, :]\n                else:\n                    transition_probs = P[current_state, :]\n                next_state = rng.choice(num_states, p=transition_probs)\n                \n                # Q-learning update\n                visit_counts[current_state, action] += 1\n                alpha = alpha_0 / (1 + visit_counts[current_state, action])\n                \n                max_next_q = np.max(q_table[next_state, :])\n                td_target = reward + gamma * max_next_q\n                td_error = td_target - q_table[current_state, action]\n                \n                q_table[current_state, action] += alpha * td_error\n                \n                # Move to next state\n                current_state = next_state\n                global_step_count += 1\n\n        # Derive final greedy policy\n        final_policy = []\n        for s in range(num_states):\n            # np.argmax handles the tie-breaking rule (select smallest index)\n            best_action = np.argmax(q_table[s, :])\n            final_policy.append(int(best_action))\n            \n        all_results.append(final_policy)\n\n    # Final print statement in the exact required format.\n    # The str representation of a list of lists is already in the right format.\n    # We just need to remove spaces.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"}]}