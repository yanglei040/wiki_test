{"hands_on_practices": [{"introduction": "Why can't we just use the familiar tool of linear regression for a yes/no prediction? This foundational exercise [@problem_id:2407549] tackles that very question, revealing a critical limitation of applying linear models to classification tasks in finance. You will calculate by hand how a simple linear model can predict a firm's default \"probability\" to be over $100\\%$, an impossible outcome highlighting the need for a more principled approach like logistic regression.", "id": "2407549", "problem": "In a credit risk classification task in computational finance, consider a binary response variable $y \\in \\{0,1\\}$ indicating whether a firm defaults on its obligations within one year, and a single predictor $x$ that is a standardized leverage index (unitless). You observe the sample of three firms with $(x,y)$ pairs: $(0,0)$, $(1,0)$, and $(2,1)$. \n\nUsing the linear probability model estimated by Ordinary Least Squares (OLS), fit the model $y = \\beta_{0} + \\beta_{1} x$ to this sample, and compute the OLS predicted value at $x = 3$. Separately, recall that the logistic regression model specifies the default probability at predictor value $x$ as $p(x) = \\frac{1}{1 + \\exp\\!\\big(-(\\gamma_{0} + \\gamma_{1} x)\\big)}$, which takes values strictly in the open interval $(0,1)$ for any real parameters $\\gamma_{0}$ and $\\gamma_{1}$. Evaluate this logistic probability at $x = 3$ for the given parameters $\\gamma_{0} = -\\frac{1}{6}$ and $\\gamma_{1} = \\frac{1}{2}$, and use this to contrast the two modeling approaches.\n\nProvide as your final answer only the OLS predicted value at $x = 3$, rounded to four significant figures.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It presents a standard comparison between two statistical models used for binary classification in econometrics and finance: the Linear Probability Model (LPM) and the logistic regression model. The data and parameters provided are complete and consistent, allowing for a unique and meaningful solution. The problem is therefore valid.\n\nWe first address the task of fitting the Linear Probability Model, $y = \\beta_{0} + \\beta_{1} x$, using Ordinary Least Squares (OLS) on the provided sample of three observations: $(x_{1}, y_{1}) = (0, 0)$, $(x_{2}, y_{2}) = (1, 0)$, and $(x_{3}, y_{3}) = (2, 1)$.\n\nThe OLS estimators for the simple linear regression coefficients, $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$, are calculated to minimize the sum of squared residuals. The formulas for these estimators are:\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2}\n$$\n$$\n\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1} \\bar{x}\n$$\nwhere $n$ is the sample size, $\\bar{x}$ is the sample mean of the predictor variable, and $\\bar{y}$ is the sample mean of the response variable.\n\nFor the given data with $n=3$:\nThe sample mean of $x$ is:\n$$\n\\bar{x} = \\frac{0 + 1 + 2}{3} = \\frac{3}{3} = 1\n$$\nThe sample mean of $y$ is:\n$$\n\\bar{y} = \\frac{0 + 0 + 1}{3} = \\frac{1}{3}\n$$\n\nNext, we compute the necessary sums.\nThe sum of squared deviations for $x$ is:\n$$\n\\sum_{i=1}^{3} (x_{i} - \\bar{x})^2 = (0 - 1)^2 + (1 - 1)^2 + (2 - 1)^2 = (-1)^2 + 0^2 + 1^2 = 1 + 0 + 1 = 2\n$$\nThe sum of the products of deviations is:\n$$\n\\sum_{i=1}^{3} (x_{i} - \\bar{x})(y_{i} - \\bar{y}) = (0 - 1)(0 - \\frac{1}{3}) + (1 - 1)(0 - \\frac{1}{3}) + (2 - 1)(1 - \\frac{1}{3})\n$$\n$$\n= (-1)(-\\frac{1}{3}) + (0)(-\\frac{1}{3}) + (1)(\\frac{2}{3}) = \\frac{1}{3} + 0 + \\frac{2}{3} = 1\n$$\n\nNow we can calculate the OLS estimators:\n$$\n\\hat{\\beta}_{1} = \\frac{1}{2}\n$$\n$$\n\\hat{\\beta}_{0} = \\frac{1}{3} - \\hat{\\beta}_{1} \\bar{x} = \\frac{1}{3} - (\\frac{1}{2})(1) = \\frac{2 - 3}{6} = -\\frac{1}{6}\n$$\n\nThus, the fitted Linear Probability Model is:\n$$\n\\hat{y} = -\\frac{1}{6} + \\frac{1}{2} x\n$$\nWe are asked to compute the predicted value at $x = 3$. Substituting $x=3$ into the fitted model yields:\n$$\n\\hat{y}|_{x=3} = -\\frac{1}{6} + \\frac{1}{2}(3) = -\\frac{1}{6} + \\frac{3}{2} = -\\frac{1}{6} + \\frac{9}{6} = \\frac{8}{6} = \\frac{4}{3}\n$$\n\nFor the purpose of contrast as requested, we now evaluate the logistic probability at $x=3$. The logistic model gives the probability $p(x)$ as:\n$$\np(x) = \\frac{1}{1 + \\exp(-(\\gamma_{0} + \\gamma_{1} x))}\n$$\nUsing the provided parameters $\\gamma_{0} = -\\frac{1}{6}$ and $\\gamma_{1} = \\frac{1}{2}$, the probability at $x=3$ is:\n$$\np(3) = \\frac{1}{1 + \\exp(-(-\\frac{1}{6} + \\frac{1}{2}(3)))} = \\frac{1}{1 + \\exp(-(-\\frac{1}{6} + \\frac{3}{2}))} = \\frac{1}{1 + \\exp(-(\\frac{8}{6}))} = \\frac{1}{1 + \\exp(-\\frac{4}{3})}\n$$\nNumerically, $p(3) \\approx 0.7914$.\n\nThe contrast between the two models is stark. The OLS predicted value from the LPM is $\\hat{y}|_{x=3} = \\frac{4}{3} \\approx 1.3333$, which is greater than $1$. Interpreting this value as a probability of default is nonsensical, as probabilities must lie in the interval $[0, 1]$. This is a well-known critical deficiency of the Linear Probability Model. In contrast, the logistic regression model, by its mathematical definition, produces probability estimates that are strictly bounded between $0$ and $1$. The value $p(3) \\approx 0.7914$ is a valid probability. This illustrates the theoretical superiority of logistic regression for modeling binary outcomes.\n\nThe problem asks for the OLS predicted value at $x=3$, rounded to four significant figures.\n$$\n\\frac{4}{3} = 1.3333...\n$$\nRounding to four significant figures gives $1.333$.", "answer": "$$\n\\boxed{1.333}\n$$"}, {"introduction": "With the theoretical motivation established, we now roll up our sleeves and build a logistic regression model from its mathematical foundations. This practice [@problem_id:2407577] guides you through implementing the powerful Newton-Raphson method to estimate the model parameters for predicting bank failures. By deriving and coding the gradient and Hessian updates yourself, you will gain a deep appreciation for the optimization engine at the heart of many statistical learning algorithms.", "id": "2407577", "problem": "You are asked to implement from first principles a binary classification model appropriate for default prediction in computational finance. Model the bank failure indicator as a Bernoulli random variable whose success probability is linked to a linear index of explanatory variables through the canonical logistic link. You must derive, implement, and solve the maximum likelihood estimation problem with an added squared Euclidean penalty on the slope coefficients only, excluding the intercept. Your implementation must use only basic numerical linear algebra and optimization building blocks available in standard numerical libraries.\n\nThe economic context is to predict whether a bank fails based on two capital adequacy measures: the capital adequacy ratio and the nonperforming loans ratio. You are given a small, fixed training dataset of banks with two features each: capital adequacy ratio and nonperforming loans ratio, both as decimals (not percentages). Let the two features be denoted by $x_1$ and $x_2$, respectively, and include an intercept term $x_0$ equal to $1$ in the linear index. The training dataset consists of the following $16$ observations:\n- Sample $1$: $(x_1, x_2) = (0.12, 0.03)$, label $y = 0$.\n- Sample $2$: $(x_1, x_2) = (0.10, 0.05)$, label $y = 0$.\n- Sample $3$: $(x_1, x_2) = (0.08, 0.07)$, label $y = 0$.\n- Sample $4$: $(x_1, x_2) = (0.06, 0.12)$, label $y = 1$.\n- Sample $5$: $(x_1, x_2) = (0.09, 0.10)$, label $y = 0$.\n- Sample $6$: $(x_1, x_2) = (0.07, 0.11)$, label $y = 1$.\n- Sample $7$: $(x_1, x_2) = (0.05, 0.14)$, label $y = 1$.\n- Sample $8$: $(x_1, x_2) = (0.11, 0.04)$, label $y = 0$.\n- Sample $9$: $(x_1, x_2) = (0.13, 0.06)$, label $y = 0$.\n- Sample $10$: $(x_1, x_2) = (0.04, 0.15)$, label $y = 1$.\n- Sample $11$: $(x_1, x_2) = (0.09, 0.02)$, label $y = 0$.\n- Sample $12$: $(x_1, x_2) = (0.06, 0.08)$, label $y = 0$.\n- Sample $13$: $(x_1, x_2) = (0.07, 0.05)$, label $y = 0$.\n- Sample $14$: $(x_1, x_2) = (0.05, 0.09)$, label $y = 1$.\n- Sample $15$: $(x_1, x_2) = (0.12, 0.10)$, label $y = 0$.\n- Sample $16$: $(x_1, x_2) = (0.03, 0.12)$, label $y = 1$.\n\nYour task is to:\n- Start from the Bernoulli likelihood for independent observations and the definition of the logistic link from generalized linear models.\n- Derive the objective function to be minimized, which is the negative log-likelihood augmented with an $L_2$ penalty on the slope coefficients. Do not penalize the intercept.\n- Derive the gradient and a suitable second-order approximation suitable for a Newton-type method that ensures convergence for convex objectives.\n- Implement a solver that uses only basic numerical operations, vectorization, and linear algebra. Ensure numerical stability for large positive or negative linear indices.\n- Train the model multiple times with different regularization strengths and then compute out-of-sample predicted failure probabilities for specified feature vectors.\n\nTest suite and required outputs:\n- You must fit the model three times, each with the same training data but a different regularization strength $\\lambda$ and a different single out-of-sample bank $(x_1, x_2)$ for which you must produce the predicted probability of failure as a decimal in $[0,1]$:\n    1. Case A (general case): $\\lambda = 1.0$, evaluate at $(x_1, x_2) = (0.055, 0.13)$.\n    2. Case B (near-decision boundary): $\\lambda = 0.01$, evaluate at $(x_1, x_2) = (0.07, 0.10)$.\n    3. Case C (heavy regularization edge case): $\\lambda = 100.0$, evaluate at $(x_1, x_2) = (0.03, 0.15)$.\n- For each case, output a single floating-point number equal to the predicted probability of failure for the specified bank, rounded to exactly $6$ decimal places. No physical units are involved. All ratios are decimals, not percentages.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, for example $[p_A,p_B,p_C]$ where each entry is rounded to exactly $6$ decimal places.\n- The program must be fully self-contained, must not read any input or write any files, and must use only the specified numerical libraries.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique and meaningful solution. The task is a standard application of penalized maximum likelihood estimation for a generalized linear model, a cornerstone of statistical machine learning and computational economics. We shall now proceed with the formal derivation and solution.\n\nThe problem requires the construction of a binary classification model for bank failure prediction. The outcome for bank $i$, denoted by $y_i$, is a Bernoulli random variable, where $y_i=1$ indicates failure and $y_i=0$ indicates non-failure. The model links the probability of failure, $p_i = P(y_i=1)$, to a set of explanatory variables through a logistic function.\n\nLet the feature vector for the $i$-th bank be $\\mathbf{x}_i = [x_{i0}, x_{i1}, x_{i2}]^T$, where $x_{i0} = 1$ is the intercept term, $x_{i1}$ is the capital adequacy ratio, and $x_{i2}$ is the nonperforming loans ratio. Let the corresponding coefficient vector be $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^T$. The linear index, $\\eta_i$, is given by the inner product:\n$$\n\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}\n$$\nThe probability of failure $p_i$ is modeled using the canonical logistic link function, also known as the sigmoid function $\\sigma(\\cdot)$:\n$$\np_i(\\boldsymbol{\\beta}) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n$$\nThe likelihood of observing the outcome $y_i$ for a single bank, given its features $\\mathbf{x}_i$ and parameters $\\boldsymbol{\\beta}$, is described by the Bernoulli probability mass function:\n$$\nL_i(\\boldsymbol{\\beta} | y_i, \\mathbf{x}_i) = p_i^{y_i} (1 - p_i)^{1 - y_i}\n$$\nAssuming the observations are independent, the total likelihood for a dataset of $N$ banks is the product of the individual likelihoods:\n$$\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\prod_{i=1}^{N} L_i(\\boldsymbol{\\beta}) = \\prod_{i=1}^{N} p_i^{y_i} (1 - p_i)^{1 - y_i}\n$$\nFor analytical and numerical convenience, we work with the log-likelihood function, $\\ell(\\boldsymbol{\\beta})$:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\log \\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n$$\nSubstituting the expressions for $p_i$ and $1-p_i = 1/(1+e^{\\eta_i})$, we can simplify the log-likelihood for a single observation:\n$$\n\\ell_i(\\boldsymbol{\\beta}) = y_i \\log\\left(\\frac{p_i}{1-p_i}\\right) + \\log(1-p_i) = y_i \\eta_i - \\log(1+e^{\\eta_i})\n$$\nThe total log-likelihood is then:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left( y_i \\eta_i - \\log(1+e^{\\eta_i}) \\right)\n$$\nThe problem requires minimizing the negative log-likelihood, augmented with a squared Euclidean penalty ($L_2$ regularization) on the slope coefficients $\\beta_1$ and $\\beta_2$, but not on the intercept $\\beta_0$. The regularization parameter is $\\lambda$. The objective function to be minimized, $J(\\boldsymbol{\\beta})$, is:\n$$\nJ(\\boldsymbol{\\beta}) = - \\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\left( \\beta_1^2 + \\beta_2^2 \\right) = \\sum_{i=1}^{N} \\left[ \\log(1+e^{\\eta_i}) - y_i \\eta_i \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^{2} \\beta_j^2\n$$\nThe factor of $1/2$ is a standard convention to simplify the gradient.\n\nTo solve this minimization problem, we employ a Newton-Raphson optimization algorithm, a second-order method that requires the gradient and Hessian of the objective function.\n\nThe gradient of $J(\\boldsymbol{\\beta})$, denoted $\\nabla J(\\boldsymbol{\\beta})$, is a vector of partial derivatives with respect to each component of $\\boldsymbol{\\beta}$.\nFirst, we find the partial derivative of $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$ with respect to $\\boldsymbol{\\beta}$, which is simply $\\mathbf{x}_i$.\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\log(1+e^{\\eta_i}) - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} (y_i \\eta_i) \\right] + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\left( \\frac{\\lambda}{2} \\sum_{j=1}^{2} \\beta_j^2 \\right)\n$$\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} \\left[ \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} \\mathbf{x}_i - y_i \\mathbf{x}_i \\right] + \\lambda [0, \\beta_1, \\beta_2]^T\n$$\nRecognizing that $e^{\\eta_i}/(1+e^{\\eta_i}) = p_i$, we obtain:\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} (p_i - y_i) \\mathbf{x}_i + \\lambda \\mathbf{P} \\boldsymbol{\\beta}\n$$\nwhere $\\mathbf{P}$ is a projection matrix that zeros out the intercept term: $\\mathbf{P} = \\mathrm{diag}(0, 1, 1)$. In matrix notation, let $\\mathbf{X}$ be the $N \\times 3$ design matrix, $\\mathbf{y}$ be the $N \\times 1$ vector of labels, and $\\mathbf{p}$ be the $N \\times 1$ vector of predicted probabilities. The gradient is:\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{P} \\boldsymbol{\\beta}\n$$\nThe Hessian matrix, $\\mathbf{H}(\\boldsymbol{\\beta})$, is the matrix of second partial derivatives, $\\nabla^2 J(\\boldsymbol{\\beta})$. Differentiating the gradient with respect to $\\boldsymbol{\\beta}^T$:\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}^T} \\left( \\sum_{i=1}^N (p_i - y_i) \\mathbf{x}_i \\right) + \\lambda \\mathbf{P} = \\sum_{i=1}^N \\frac{\\partial p_i}{\\partial \\boldsymbol{\\beta}^T} \\mathbf{x}_i + \\lambda \\mathbf{P}\n$$\nThe derivative of the sigmoid function is $\\sigma'(\\eta_i) = \\sigma(\\eta_i)(1-\\sigma(\\eta_i)) = p_i(1-p_i)$.\n$$\n\\frac{\\partial p_i}{\\partial \\boldsymbol{\\beta}^T} = \\frac{\\partial \\sigma(\\eta_i)}{\\partial \\boldsymbol{\\beta}^T} = \\sigma'(\\eta_i) \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}^T} = p_i(1-p_i) \\mathbf{x}_i^T\n$$\nSubstituting this into the Hessian expression:\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^T + \\lambda \\mathbf{P}\n$$\nIn matrix notation, let $\\mathbf{W}$ be a $N \\times N$ diagonal matrix with diagonal elements $W_{ii} = p_i(1-p_i)$. The Hessian is:\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^T \\mathbf{W} \\mathbf{X} + \\lambda \\mathbf{P}\n$$\nFor $\\lambda > 0$ and non-degenerate data, this Hessian is positive-definite, which guarantees that the objective function $J(\\boldsymbol{\\beta})$ is strictly convex and has a unique minimum. The Newton-Raphson update step to find this minimum is:\n$$\n\\boldsymbol{\\beta}_{k+1} = \\boldsymbol{\\beta}_k - \\mathbf{H}(\\boldsymbol{\\beta}_k)^{-1} \\nabla J(\\boldsymbol{\\beta}_k)\n$$\nIn practice, we do not compute the inverse of the Hessian. Instead, we solve the linear system for the update step $\\Delta \\boldsymbol{\\beta}_k = - \\mathbf{H}(\\boldsymbol{\\beta}_k)^{-1} \\nabla J(\\boldsymbol{\\beta}_k)$:\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}_k) \\Delta \\boldsymbol{\\beta}_k = - \\nabla J(\\boldsymbol{\\beta}_k)\n$$\nThen, we update the parameters: $\\boldsymbol{\\beta}_{k+1} = \\boldsymbol{\\beta}_k + \\Delta \\boldsymbol{\\beta}_k$. The algorithm starts with an initial guess, e.g., $\\boldsymbol{\\beta}_0 = \\mathbf{0}$, and iterates until the norm of the gradient is below a specified tolerance or a maximum number of iterations is reached.\n\nFor numerical stability, the sigmoid function $\\sigma(z) = 1/(1+e^{-z})$ must be implemented carefully to avoid overflow when $z$ is a large negative number. A robust implementation is:\n$$\n\\sigma(z) = \\begin{cases} 1 / (1 + e^{-z}) & \\text{if } z \\ge 0 \\\\ e^z / (1 + e^z) & \\text{if } z < 0 \\end{cases}\n$$\nThis ensures that the exponent is always non-positive, preventing overflow.\n\nThe implementation will consist of a function that takes the training data $(\\mathbf{X}, \\mathbf{y})$ and the regularization parameter $\\lambda$, performs the Newton-Raphson optimization to find the optimal $\\boldsymbol{\\beta}$, and then uses this estimated $\\boldsymbol{\\beta}$ to predict the failure probability for new out-of-sample data.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per constraints to implement from first principles.\n\ndef solve():\n    \"\"\"\n    Implements and solves a regularized logistic regression problem\n    for bank failure prediction from first principles.\n    \"\"\"\n\n    # Training data provided in the problem statement.\n    # N = 16 observations\n    # Features: x_1 = capital adequacy ratio, x_2 = nonperforming loans ratio\n    # Label: y = 1 for failure, 0 for non-failure\n    training_data = np.array([\n        [0.12, 0.03, 0.], [0.10, 0.05, 0.], [0.08, 0.07, 0.], [0.06, 0.12, 1.],\n        [0.09, 0.10, 0.], [0.07, 0.11, 1.], [0.05, 0.14, 1.], [0.11, 0.04, 0.],\n        [0.13, 0.06, 0.], [0.04, 0.15, 1.], [0.09, 0.02, 0.], [0.06, 0.08, 0.],\n        [0.07, 0.05, 0.], [0.05, 0.09, 1.], [0.12, 0.10, 0.], [0.03, 0.12, 1.]\n    ])\n\n    # Feature matrix X (with intercept) and target vector y\n    N = training_data.shape[0]\n    X_features = training_data[:, :2]\n    X = np.c_[np.ones(N), X_features]  # Design matrix, shape (16, 3)\n    y = training_data[:, 2]            # Target vector, shape (16,)\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (lambda, (x1, x2))\n        (1.0, (0.055, 0.13)),\n        (0.01, (0.07, 0.10)),\n        (100.0, (0.03, 0.15)),\n    ]\n\n    def _sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        # Use np.where to handle arrays efficiently\n        return np.where(z >= 0, \n                        1 / (1 + np.exp(-z)), \n                        np.exp(z) / (1 + np.exp(z)))\n\n    def _fit_logistic_ridge(X, y, lambda_val, max_iter=100, tol=1e-9):\n        \"\"\"\n        Fits a logistic regression model with L2 penalty on slope coefficients\n        using the Newton-Raphson method.\n\n        Args:\n            X (np.ndarray): Design matrix with shape (N, D), including intercept.\n            y (np.ndarray): Target vector with shape (N,).\n            lambda_val (float): Regularization strength.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Convergence tolerance for the gradient norm.\n\n        Returns:\n            np.ndarray: Estimated coefficient vector beta with shape (D,).\n        \"\"\"\n        n_samples, n_features = X.shape\n        beta = np.zeros(n_features)\n\n        # Regularization matrix P, does not penalize the intercept (beta_0)\n        P = np.diag([0.] + [1.] * (n_features - 1))\n\n        for i in range(max_iter):\n            # Calculate linear predictor, eta = X @ beta\n            eta = X.dot(beta)\n\n            # Calculate probabilities, p = sigma(eta)\n            p = _sigmoid(eta)\n\n            # Calculate gradient of the objective function\n            # g = X.T @ (p - y) + lambda * P @ beta\n            g = X.T.dot(p - y) + lambda_val * P.dot(beta)\n            \n            # Check for convergence\n            if np.linalg.norm(g) < tol:\n                break\n            \n            # Calculate Hessian of the objective function\n            # H = X.T @ W @ X + lambda * P\n            # W is a diagonal matrix with W_ii = p_i * (1 - p_i)\n            # For efficiency, we can compute X.T @ W @ X without explicitly forming W\n            # H = (X.T * (p * (1 - p))) @ X + lambda_val * P\n            w_diag = p * (1 - p)\n            H = X.T.dot(np.diag(w_diag)).dot(X) + lambda_val * P\n            \n            # Solve the linear system H * delta_beta = -g for the update step\n            # This is numerically more stable than inverting H\n            delta_beta = np.linalg.solve(H, -g)\n\n            # Update beta\n            beta += delta_beta\n        \n        return beta\n\n    def _predict_proba(x_new_features, beta):\n        \"\"\"\n        Predicts the probability for a new observation.\n\n        Args:\n            x_new_features (tuple): A tuple (x1, x2) for the new observation.\n            beta (np.ndarray): The trained coefficient vector.\n\n        Returns:\n            float: The predicted probability P(y=1).\n        \"\"\"\n        x_new = np.array([1.] + list(x_new_features))\n        eta_new = x_new.dot(beta)\n        return _sigmoid(eta_new)\n\n    results = []\n    for lambda_val, x_new_features in test_cases:\n        # Fit the model for the given lambda\n        beta_hat = _fit_logistic_ridge(X, y, lambda_val)\n        \n        # Predict the probability for the new observation\n        p_hat = _predict_proba(x_new_features, beta_hat)\n        \n        # Format the result to 6 decimal places and append\n        results.append(f\"{p_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "Standard model training often assumes a balanced dataset, but in finance and economics, we frequently hunt for rare events like market crashes or championship-winning strategies. This advanced exercise [@problem_id:2407556] immerses you in such a scenario, where you must predict a rare outcome and simple accuracy is a deceptive metric. You will learn to use class weights to handle the data imbalance and master a suite of sophisticated evaluation tools—like the $F_1$ score, balanced accuracy, and the Area Under the Precision-Recall curve (AUC-PR)—that provide a much truer picture of your model's predictive power.", "id": "2407556", "problem": "You are asked to write a complete, runnable program that trains and evaluates a logistic regression classifier for a rare-event prediction problem framed as follows. A team-season is represented by a vector of standardized season statistics: win fraction, average point differential per game, payroll ratio to league median, and a playoff experience index. The target is whether the team wins the championship in that season. Winning the championship is a rare event. The model is logistic regression with an intercept and an optional class-weight for the positive class to address imbalance. The evaluation emphasizes imbalanced-class metrics and thresholding effects.\n\nStart from the following fundamental base:\n- The outcome for each observation is a Bernoulli random variable with success probability parameterized by a logistic link. Let $y_i \\in \\{0,1\\}$ be the binary label and $x_i \\in \\mathbb{R}^d$ be the feature vector.\n- The logistic link sets $p_i = \\Pr(y_i = 1 \\mid x_i) = \\sigma(z_i)$ with $z_i = \\beta_0 + x_i^\\top \\beta$, where $\\sigma(u) = \\dfrac{1}{1 + e^{-u}}$.\n- The (optionally) weighted negative log-likelihood with $\\ell_2$ regularization (ridge) is to be minimized. If $w_i > 0$ is the observation weight and $\\lambda \\ge 0$ is the regularization strength, then the objective is\n$$\n\\mathcal{L}(\\beta_0, \\beta) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\lVert \\beta \\rVert_2^2,\n$$\nwhere the intercept $\\beta_0$ is not penalized.\n\nYour program must:\n- Implement logistic regression training by minimizing the above objective using the Newton–Raphson method on the full data (no mini-batches). Use the exact gradient and Hessian, a backtracking line search for stability, and do not penalize the intercept. Allow a scalar positive-class weight $w_+ > 1$ by setting $w_i = w_+$ for $y_i = 1$ and $w_i = 1$ for $y_i = 0$; unweighted training uses $w_+ = 1$.\n- After training, compute predicted probabilities $\\hat{p}_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$ on the test set and convert them to class predictions using a fixed threshold $\\tau \\in (0,1)$: predict $\\hat{y}_i = 1$ if $\\hat{p}_i \\ge \\tau$ and $\\hat{y}_i = 0$ otherwise.\n- Compute the following evaluation metrics on the test set, where $\\mathrm{TP}$, $\\mathrm{FP}$, $\\mathrm{TN}$, $\\mathrm{FN}$ are true positives, false positives, true negatives, and false negatives:\n    - Precision: $\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$ if the denominator is nonzero, otherwise define it to be $0$.\n    - Recall (true positive rate): $\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}$ if the denominator is nonzero, otherwise define it to be $0$.\n    - $F_1$ score: $F_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$ if the denominator is nonzero, otherwise define it to be $0$.\n    - Balanced accuracy: $\\mathrm{BAcc} = \\dfrac{1}{2}\\left( \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} + \\dfrac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FP}} \\right)$, with each fraction defined as $0$ if its denominator is zero.\n    - Matthews Correlation Coefficient (MCC): $\\mathrm{MCC} = \\dfrac{\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}}{\\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}}$, defined as $0$ if the denominator is zero.\n    - Area under the Precision–Recall curve (AUC-PR): construct the precision–recall curve by sorting test instances by $\\hat{p}_i$ in descending order and evaluating precision and recall at cumulative thresholds; compute the area using the trapezoidal rule in recall.\n\nTraining and test data are provided below as explicit numeric arrays. Each sample is a vector of four features $\\left[x_1, x_2, x_3, x_4\\right]$ corresponding to win fraction, point differential per game, payroll ratio to league median, and playoff experience index. Labels $y$ indicate championship outcome.\n\nUse these arrays exactly as given for features and labels.\n\n- Training set $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$ with $n = 16$:\n    - $[0.55, -1.0, 0.90, 0.20] \\rightarrow 0$\n    - $[0.60, 1.50, 1.10, 0.30] \\rightarrow 0$\n    - $[0.48, -3.50, 0.85, 0.10] \\rightarrow 0$\n    - $[0.70, 5.00, 1.20, 0.50] \\rightarrow 0$\n    - $[0.75, 6.00, 1.40, 0.60] \\rightarrow 1$\n    - $[0.65, 3.00, 1.00, 0.40] \\rightarrow 0$\n    - $[0.80, 8.50, 1.50, 0.70] \\rightarrow 1$\n    - $[0.42, -6.00, 0.75, 0.10] \\rightarrow 0$\n    - $[0.58, 0.50, 0.95, 0.20] \\rightarrow 0$\n    - $[0.62, 2.00, 1.05, 0.35] \\rightarrow 0$\n    - $[0.68, 4.00, 1.10, 0.45] \\rightarrow 0$\n    - $[0.85, 10.00, 1.60, 0.80] \\rightarrow 1$\n    - $[0.50, -2.00, 0.90, 0.15] \\rightarrow 0$\n    - $[0.73, 5.50, 1.30, 0.55] \\rightarrow 0$\n    - $[0.66, 3.50, 1.15, 0.45] \\rightarrow 0$\n    - $[0.77, 7.00, 1.35, 0.60] \\rightarrow 0$\n\n- Test set A $\\left(X_{\\text{A}}, y_{\\text{A}}\\right)$ with $m = 6$:\n    - $[0.74, 6.00, 1.25, 0.50] \\rightarrow 1$\n    - $[0.57, 0.00, 1.00, 0.20] \\rightarrow 0$\n    - $[0.82, 9.00, 1.55, 0.75] \\rightarrow 1$\n    - $[0.45, -4.50, 0.80, 0.05] \\rightarrow 0$\n    - $[0.63, 2.50, 1.10, 0.40] \\rightarrow 0$\n    - $[0.79, 7.50, 1.45, 0.65] \\rightarrow 1$\n\n- Test set B $\\left(X_{\\text{B}}, y_{\\text{B}}\\right)$ with $m = 8$:\n    - $[0.61, 1.50, 1.05, 0.30] \\rightarrow 0$\n    - $[0.52, -1.50, 0.90, 0.12] \\rightarrow 0$\n    - $[0.76, 6.50, 1.40, 0.60] \\rightarrow 1$\n    - $[0.59, 0.50, 0.95, 0.25] \\rightarrow 0$\n    - $[0.47, -4.00, 0.82, 0.08] \\rightarrow 0$\n    - $[0.66, 3.00, 1.12, 0.40] \\rightarrow 0$\n    - $[0.64, 2.00, 1.08, 0.38] \\rightarrow 0$\n    - $[0.54, -0.50, 0.97, 0.20] \\rightarrow 0$\n\nTest suite. Train on $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$ in all cases, but vary threshold and positive-class weight to test different facets:\n- Case 1 (happy path): unweighted training with $\\lambda = 1.0$, $\\tau = 0.5$, maximum iterations $= 100$, tolerance $= 10^{-9}$; evaluate on test set A.\n- Case 2 (threshold boundary): unweighted training with $\\lambda = 1.0$, $\\tau = 0.9$, maximum iterations $= 100$, tolerance $= 10^{-9}$; evaluate on test set A.\n- Case 3 (class-imbalance mitigation): weighted training with positive-class weight $w_+ = 3.0$, $\\lambda = 1.0$, $\\tau = 0.5$, maximum iterations $= 100$, tolerance $= 10^{-9}$; evaluate on test set B.\n\nFinal output requirements:\n- For each case, report the metrics as a list in this exact order: $[\\mathrm{Prec}, \\mathrm{Rec}, F_1, \\mathrm{BAcc}, \\mathrm{MCC}, \\mathrm{AUC\\mbox{-}PR}]$, with each value rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of the three case-level lists, enclosed in square brackets, for example: $[[a_1,\\dots,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6]]$ where each $a_j$, $b_j$, $c_j$ is a float rounded to $6$ decimal places.\n\nNo user input or external files are allowed; the program must be fully self-contained and deterministic. Angles are not used in this problem. No physical units are involved. All answers are floats and must be rounded as specified.", "solution": "The problem presented is a well-defined task in computational statistics: to implement, train, and evaluate a logistic regression classifier for a binary classification problem, specifically for a rare-event scenario. The problem is scientifically grounded, mathematically complete, and algorithmically specified. It is therefore valid, and we proceed with a rigorous solution constructed from first principles.\n\nThe core of the problem is to find the parameters of a logistic regression model that minimize a specified objective function. The model predicts the probability of a positive outcome ($y_i=1$) for an observation with feature vector $x_i \\in \\mathbb{R}^d$ via the logistic function, $p_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$, where $\\sigma(u) = (1 + e^{-u})^{-1}$. For notational convenience, we augment the feature vector to $\\tilde{x}_i = [1, x_i^\\top]^\\top$ and the parameter vector to $\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$, so that the linear component is $z_i = \\tilde{x}_i^\\top \\tilde{\\beta}$.\n\nThe objective function to be minimized is the weighted negative log-likelihood with an $\\ell_2$ penalty on the feature coefficients $\\beta$ (but not the intercept $\\beta_0$):\n$$\n\\mathcal{L}(\\tilde{\\beta}) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\beta^\\top \\beta\n$$\nHere, $n$ is the number of training samples, $w_i$ are the sample weights, and $\\lambda$ is the regularization strength. The weight $w_i$ is set to a specified positive-class weight $w_+$ if $y_i=1$, and to $1$ if $y_i=0$.\n\nOptimization is performed using the Newton-Raphson method, a second-order iterative algorithm. Each iteration updates the parameter vector $\\tilde{\\beta}$ according to the rule:\n$$\n\\tilde{\\beta}^{(k+1)} = \\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}\n$$\nwhere $\\alpha$ is a step size and $\\Delta \\tilde{\\beta}^{(k)}$ is the Newton step, found by solving the linear system:\n$$\n\\mathbf{H}(\\tilde{\\beta}^{(k)}) \\Delta \\tilde{\\beta}^{(k)} = -\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)})\n$$\nIn this equation, $\\nabla \\mathcal{L}$ is the gradient (vector of first partial derivatives) and $\\mathbf{H}$ is the Hessian matrix (matrix of second partial derivatives) of the objective function $\\mathcal{L}$.\n\nThe gradient vector $\\nabla \\mathcal{L}(\\tilde{\\beta})$ is derived as:\n$$\n\\nabla \\mathcal{L}(\\tilde{\\beta}) = \\tilde{X}^\\top W (p - y) + \\lambda \\Lambda \\tilde{\\beta}\n$$\nHere, $\\tilde{X}$ is the $n \\times (d+1)$ augmented design matrix, $W$ is an $n \\times n$ diagonal matrix of sample weights $w_i$, $p$ is the vector of predicted probabilities, $y$ is the vector of true labels, and $\\Lambda$ is a $(d+1) \\times (d+1)$ diagonal matrix with $\\Lambda_{00}=0$ and $\\Lambda_{jj}=1$ for $j>0$, which ensures the intercept is not penalized.\n\nThe Hessian matrix $\\mathbf{H}(\\tilde{\\beta})$ is derived as:\n$$\n\\mathbf{H}(\\tilde{\\beta}) = \\tilde{X}^\\top S \\tilde{X} + \\lambda \\Lambda\n$$\nwhere $S$ is an $n \\times n$ diagonal matrix with elements $S_{ii} = w_i p_i (1 - p_i)$. Since this Hessian is symmetric and, for $\\lambda > 0$, positive definite, the linear system for the Newton step has a unique solution.\n\nTo ensure robust convergence, the step size $\\alpha$ is determined using a backtracking line search. Starting with $\\alpha=1$, the step size is iteratively reduced by a factor $\\rho$ (e.g., $0.5$) until the Armijo-Goldstein condition is satisfied:\n$$\n\\mathcal{L}(\\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}) \\le \\mathcal{L}(\\tilde{\\beta}^{(k)}) + c_1 \\alpha (\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)}))^\\top \\Delta \\tilde{\\beta}^{(k)}\n$$\nwith a small constant $c_1$ (e.g., $10^{-4}$).\n\nAfter training, the model's performance is evaluated on a test set. Predicted probabilities $\\hat{p}_i$ are calculated and converted to class labels $\\hat{y}_i \\in \\{0,1\\}$ using a specified threshold $\\tau$. The evaluation relies on several metrics, defined in terms of true positives ($\\mathrm{TP}$), false positives ($\\mathrm{FP}$), true negatives ($\\mathrm{TN}$), and false negatives ($\\mathrm{FN}$):\n- **Precision**: $\\mathrm{Prec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FP})$\n- **Recall**: $\\mathrm{Rec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FN})$\n- **$F_1$ Score**: $F_1 = 2 \\cdot (\\mathrm{Prec} \\cdot \\mathrm{Rec}) / (\\mathrm{Prec} + \\mathrm{Rec})$\n- **Balanced Accuracy**: $\\mathrm{BAcc} = 0.5 \\cdot (\\mathrm{Rec} + \\mathrm{TN} / (\\mathrm{TN} + \\mathrm{FP}))$\n- **Matthews Correlation Coefficient (MCC)**: $\\mathrm{MCC} = (\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}) / \\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}$\nEach metric is defined to be $0$ if its denominator is zero.\n\nThe **Area Under the Precision-Recall curve (AUC-PR)** is computed via numerical integration. The test instances are sorted by predicted probability in descending order. A sequence of precision and recall values is generated by considering successively larger subsets of the sorted instances. The area under the resulting curve is then calculated using the trapezoidal rule. The recall values, which are non-decreasing, serve as the points of integration. The curve is anchored at a starting point of $(\\text{recall}=0, \\text{precision}=1)$ to properly account for the area at the beginning of the recall range.\n\nThe provided implementation encapsulates this entire procedure. It defines the necessary data structures, implements the Newton-Raphson trainer with backtracking, and computes the full suite of evaluation metrics for each of the three test cases specified in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve as solve_linear_system\n\ndef solve():\n    \"\"\"\n    Main function to run the logistic regression experiments as specified.\n    \"\"\"\n    \n    # --- Data Definition ---\n    X_train = np.array([\n        [0.55, -1.0, 0.90, 0.20], [0.60, 1.50, 1.10, 0.30], [0.48, -3.50, 0.85, 0.10],\n        [0.70, 5.00, 1.20, 0.50], [0.75, 6.00, 1.40, 0.60], [0.65, 3.00, 1.00, 0.40],\n        [0.80, 8.50, 1.50, 0.70], [0.42, -6.00, 0.75, 0.10], [0.58, 0.50, 0.95, 0.20],\n        [0.62, 2.00, 1.05, 0.35], [0.68, 4.00, 1.10, 0.45], [0.85, 10.00, 1.60, 0.80],\n        [0.50, -2.00, 0.90, 0.15], [0.73, 5.50, 1.30, 0.55], [0.66, 3.50, 1.15, 0.45],\n        [0.77, 7.00, 1.35, 0.60]\n    ])\n    y_train = np.array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n    X_A = np.array([\n        [0.74, 6.00, 1.25, 0.50], [0.57, 0.00, 1.00, 0.20], [0.82, 9.00, 1.55, 0.75],\n        [0.45, -4.50, 0.80, 0.05], [0.63, 2.50, 1.10, 0.40], [0.79, 7.50, 1.45, 0.65]\n    ])\n    y_A = np.array([1, 0, 1, 0, 0, 1])\n\n    X_B = np.array([\n        [0.61, 1.50, 1.05, 0.30], [0.52, -1.50, 0.90, 0.12], [0.76, 6.50, 1.40, 0.60],\n        [0.59, 0.50, 0.95, 0.25], [0.47, -4.00, 0.82, 0.08], [0.66, 3.00, 1.12, 0.40],\n        [0.64, 2.00, 1.08, 0.38], [0.54, -0.50, 0.97, 0.20]\n    ])\n    y_B = np.array([0, 0, 1, 0, 0, 0, 0, 0])\n    \n    test_cases = [\n        # (lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol)\n        (1.0, 1.0, 0.5, X_A, y_A, 100, 1e-9),\n        (1.0, 1.0, 0.9, X_A, y_A, 100, 1e-9),\n        (1.0, 3.0, 0.5, X_B, y_B, 100, 1e-9)\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol = case\n        \n        # Train model\n        beta = _logistic_regression_train(X_train, y_train, lambda_reg, w_plus, max_iter, tol)\n        \n        # Get predictions\n        y_proba = _predict_proba(X_test, beta)\n        y_pred = (y_proba >= tau).astype(int)\n        \n        # Evaluate metrics\n        metrics = _evaluate_metrics(y_test, y_pred, y_proba)\n        results.append(metrics)\n        \n    # Format and print the final output\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef _sigmoid(z):\n    # Clip to avoid overflow/underflow in exp\n    z_clipped = np.clip(z, -500, 500)\n    p = 1 / (1 + np.exp(-z_clipped))\n    # Clip probabilities to avoid log(0)\n    return np.clip(p, 1e-15, 1 - 1e-15)\n\ndef _calculate_cost(X_aug, y, beta, weights, lambda_reg):\n    p = _sigmoid(X_aug @ beta)\n    log_likelihood = -np.sum(weights * (y * np.log(p) + (1 - y) * np.log(1 - p)))\n    reg_term = (lambda_reg / 2) * np.dot(beta[1:], beta[1:])\n    return log_likelihood + reg_term\n\ndef _logistic_regression_train(X, y, lambda_reg, w_plus, max_iter, tol):\n    n_samples, n_features = X.shape\n    X_aug = np.c_[np.ones(n_samples), X]\n    beta = np.zeros(n_features + 1)\n    \n    weights = np.ones(n_samples)\n    weights[y == 1] = w_plus\n\n    lambda_vec = np.full(n_features + 1, lambda_reg)\n    lambda_vec[0] = 0.0\n\n    # Newton-Raphson iterations\n    for _ in range(max_iter):\n        z = X_aug @ beta\n        p = _sigmoid(z)\n        \n        error = p - y\n        grad = X_aug.T @ (weights * error) + lambda_vec * beta\n        \n        S_diag = weights * p * (1 - p)\n        H = (X_aug.T * S_diag) @ X_aug + np.diag(lambda_vec)\n        \n        # Solve H * delta_beta = -grad\n        # Use scipy.linalg.solve for stability and performance\n        search_dir = solve_linear_system(H, -grad, assume_a='sym')\n        \n        # Backtracking line search\n        alpha = 1.0\n        c1 = 1e-4\n        rho = 0.5\n        cost_current = _calculate_cost(X_aug, y, beta, weights, lambda_reg)\n        grad_dot_dir = np.dot(grad, search_dir)\n        \n        while True:\n            beta_new = beta + alpha * search_dir\n            cost_new = _calculate_cost(X_aug, y, beta_new, weights, lambda_reg)\n            if cost_new <= cost_current + c1 * alpha * grad_dot_dir or alpha < 1e-9:\n                break\n            alpha *= rho\n        \n        step = alpha * search_dir\n        beta += step\n        \n        if np.linalg.norm(step) < tol:\n            break\n            \n    return beta\n\ndef _predict_proba(X, beta):\n    X_aug = np.c_[np.ones(X.shape[0]), X]\n    return _sigmoid(X_aug @ beta)\n\ndef _evaluate_metrics(y_true, y_pred, y_proba):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    \n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    tpr = recall\n    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    balanced_accuracy = 0.5 * (tpr + tnr)\n\n    mcc_denom_sq = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = (tp * tn - fp * fn) / np.sqrt(mcc_denom_sq) if mcc_denom_sq > 0 else 0.0\n        \n    # AUC-PR calculation\n    sort_indices = np.argsort(y_proba)[::-1]\n    y_true_sorted = y_true[sort_indices]\n    \n    tps = np.cumsum(y_true_sorted)\n    fps = np.cumsum(1 - y_true_sorted)\n    \n    total_positives = np.sum(y_true)\n    if total_positives == 0:\n        auc_pr = 0.0\n    else:\n        recalls = tps / total_positives\n        precisions = tps / (tps + fps)\n        \n        # Keep only points where recall changes (at positive instances)\n        is_positive = (y_true_sorted == 1)\n        recall_points = recalls[is_positive]\n        precision_points = precisions[is_positive]\n\n        # Prepend starting point for trapezoidal rule\n        recall_aug = np.concatenate([[0.], recall_points])\n        precision_aug = np.concatenate([[1.], precision_points])\n        \n        auc_pr = np.trapz(precision_aug, recall_aug)\n\n    metrics = [precision, recall, f1_score, balanced_accuracy, mcc, auc_pr]\n    return [round(m, 6) for m in metrics]\n\nif __name__ == '__main__':\n    solve()\n```"}]}