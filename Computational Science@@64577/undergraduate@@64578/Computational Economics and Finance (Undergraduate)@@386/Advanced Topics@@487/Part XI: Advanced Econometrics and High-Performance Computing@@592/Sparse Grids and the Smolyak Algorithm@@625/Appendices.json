{"hands_on_practices": [{"introduction": "High-dimensional integration is a common challenge in finance, exemplified by the pricing of options on a basket of assets. This exercise directly confronts the infamous \"curse of dimensionality\" by having you contrast the exponential cost growth of a full tensor-product grid with the far more manageable complexity of a Smolyak sparse grid. By implementing and comparing both methods from first principles, you will gain a concrete, practical understanding of why sparse grids are an indispensable tool for tackling moderately high-dimensional problems. [@problem_id:2396782]", "id": "2396782", "problem": "Consider a European call option written on a basket of $d$ risky assets under the risk-neutral Black–Scholes framework. Let $S_i(0)$ denote the initial price of asset $i$, $r$ the constant risk-free rate, $\\sigma_i$ the volatility of asset $i$, and $T$ the maturity time. Under the risk-neutral measure, the terminal price of asset $i$ is given by\n$$\nS_i(T) = S_i(0)\\,\\exp\\!\\Big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\Big),\n$$\nwhere $Z_i$ are independent standard normal random variables. The basket call option has payoff\n$$\n\\Pi = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(T) - K,\\, 0\\Big),\n$$\nwith strike $K$. The arbitrage-free price at time $0$ is\n$$\nV_0 = e^{-rT}\\,\\mathbb{E}\\big[\\Pi\\big].\n$$\n\nYour task is to approximate $V_0$ for the case $d=5$ using Gaussian quadrature and to numerically demonstrate the curse of dimensionality by comparing a full tensor-product Gaussian quadrature with a Smolyak sparse grid constructed from the same one-dimensional rule.\n\nYou must proceed from first principles by turning the $d$-dimensional expectation under the standard normal distribution into a Gaussian–Hermite quadrature. Recall the Gaussian–Hermite quadrature rule of order $n$ for one dimension,\n$$\n\\int_{-\\infty}^{\\infty} e^{-x^2} f(x)\\,dx \\approx \\sum_{j=1}^{n} w_j f(x_j),\n$$\nwhere $x_j$ are the nodes and $w_j$ are the weights. Show that for a standard normal random variable $Z$,\n$$\n\\mathbb{E}[g(Z)] = \\frac{1}{\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty} e^{-x^2}\\, g\\!\\big(\\sqrt{2}\\,x\\big)\\,dx,\n$$\nand generalize this to $d$ independent standard normals to justify a $d$-dimensional tensor product of one-dimensional Gaussian–Hermite rules. Use this to produce:\n\n- A $d$-dimensional tensor-product Gaussian–Hermite quadrature with $n$ nodes per dimension (total nodes $n^d$).\n- A Smolyak sparse grid of isotropic level $L$ built from the same one-dimensional Gaussian–Hermite family $\\{Q_\\ell\\}_{\\ell \\ge 1}$, where the one-dimensional order is $m(\\ell) = 2\\ell - 1$. Use the Smolyak combination formula with index set\n$$\n\\mathcal{I}(L,d) = \\big\\{\\boldsymbol{\\ell}\\in \\mathbb{N}^d : 1 \\le \\ell_k, \\ \\ |\\boldsymbol{\\ell}|_1 \\le L + d - 1 \\big\\},\n$$\nand coefficients\n$$\nc(\\boldsymbol{\\ell}) = (-1)^{L + d - 1 - |\\boldsymbol{\\ell}|_1}\\binom{d-1}{L + d - 1 - |\\boldsymbol{\\ell}|_1}.\n$$\nHere $Q_{\\ell}$ denotes the one-dimensional Gaussian–Hermite rule of order $m(\\ell)$, and the $d$-dimensional operator is the tensor product $\\bigotimes_{k=1}^d Q_{\\ell_k}$.\n\nFrom the above foundations, design an algorithm that:\n- Maps the expectation to the Gaussian–Hermite weighted integral via the change of variables $z = \\sqrt{2}\\,x$ in each dimension.\n- Correctly applies the normalization factor $\\pi^{-d/2}$ for $d$ dimensions.\n- Constructs the tensor grid and the Smolyak sparse grid and evaluates the discounted payoff at all required nodes.\n- Counts the number of function evaluations for each method as a proxy for computational cost.\n\nTest Suite. Implement the following two computational test cases and one comparison task:\n\n- Case A (happy path, $d=5$):\n  - Parameters: $S_0 = (100,\\, 90,\\, 110,\\, 95,\\, 105)$, $\\sigma = (0.2,\\, 0.25,\\, 0.15,\\, 0.3,\\, 0.18)$, $r = 0.02$, $T = 1.0$, $K = 100$.\n  - Tensor-product Gaussian–Hermite quadrature with $n=3$ nodes per dimension.\n  - Smolyak sparse grid with isotropic level $L=3$ and one-dimensional order $m(\\ell) = 2\\ell - 1$.\n  - A high-accuracy reference computed by tensor-product Gaussian–Hermite quadrature with $n_{\\text{ref}}=9$ nodes per dimension.\n  - Outputs to compute:\n    - The absolute error of the tensor-product approximation relative to the reference as a float.\n    - The absolute error of the sparse-grid approximation relative to the reference as a float.\n    - The tensor-product node count as an integer.\n    - The sparse-grid node count as an integer, taken as the sum of tensor-product sizes of all constituent terms in the Smolyak combination.\n\n- Case B (boundary condition with zero volatility, $d=5$):\n  - Parameters: same $S_0$, $r$, $T$, $K$ as in Case A, but $\\sigma = (0,\\,0,\\,0,\\,0,\\,0)$.\n  - The exact price is given by the deterministic value\n    $$\n    V_0^{\\text{det}} = e^{-rT}\\,\\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,e^{rT} - K,\\, 0\\Big) = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0) - K e^{-rT},\\, 0\\Big).\n    $$\n  - Verify that both the tensor-product quadrature with $n=3$ and the sparse grid with $L=3$ match $V_0^{\\text{det}}$ within an absolute tolerance of $10^{-10}$. Output a boolean indicating whether both pass.\n\n- Efficiency comparison:\n  - For Case A, output a boolean indicating whether the sparse grid achieves absolute error less than or equal to the tensor-product error while using strictly fewer nodes.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list entries must be, in order:\n- The tensor-product absolute error for Case A (float).\n- The sparse-grid absolute error for Case A (float).\n- The tensor-product node count for Case A (integer).\n- The sparse-grid node count for Case A (integer).\n- The boundary condition check for Case B (boolean).\n- The efficiency comparison check for Case A (boolean).\n\nFor example, the output format must be\n$$\n[\\text{err\\_tensor},\\text{err\\_sparse},\\text{nodes\\_tensor},\\text{nodes\\_sparse},\\text{boundary\\_ok},\\text{efficiency\\_ok}],\n$$\nwith the two errors as decimal numbers, the two node counts as integers, and the last two entries as booleans. No additional text should be printed.", "solution": "We begin from the risk-neutral valuation principle for a European derivative: the time-$0$ price $V_0$ of a payoff $\\Pi$ at time $T$ is given by $V_0 = e^{-rT}\\,\\mathbb{E}[\\Pi]$, where the expectation is under the risk-neutral measure. In the Black–Scholes framework with independent standard normals $Z_i \\sim \\mathcal{N}(0,1)$, the terminal asset values are\n$$\nS_i(T) = S_i(0)\\,\\exp\\!\\Big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\Big),\n$$\nand the basket call payoff is\n$$\n\\Pi = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(T) - K,\\, 0\\Big).\n$$\nThus,\n$$\nV_0 = e^{-rT}\\,\\mathbb{E}\\left[\\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,\\exp\\!\\big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\big) - K,\\, 0\\Big)\\right].\n$$\n\nTo connect to Gaussian–Hermite quadrature, we transform expectations under the standard normal law to integrals with the Gaussian–Hermite weight. For a single standard normal random variable $Z \\sim \\mathcal{N}(0,1)$ and a suitable test function $g$, we have\n$$\n\\mathbb{E}[g(Z)] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} g(z)\\,dz.\n$$\nWith the change of variables $z = \\sqrt{2}\\,x$ (so $dz = \\sqrt{2}\\,dx$), one obtains\n$$\n\\mathbb{E}[g(Z)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-(\\sqrt{2}x)^2/2} g(\\sqrt{2}x)\\,\\sqrt{2}\\,dx = \\frac{1}{\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty} e^{-x^2} g(\\sqrt{2}\\,x)\\,dx.\n$$\nIn $d$ dimensions with independent standard normals $\\boldsymbol{Z} = (Z_1,\\dots,Z_d)$, the joint density factorizes, and by applying the change of variables componentwise, we obtain\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] = \\frac{1}{\\pi^{d/2}} \\int_{\\mathbb{R}^d} e^{-\\|\\boldsymbol{x}\\|_2^2}\\, g(\\sqrt{2}\\,\\boldsymbol{x})\\, d\\boldsymbol{x}.\n$$\nHence a $d$-dimensional Gaussian–Hermite tensor-product quadrature yields\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] \\approx \\frac{1}{\\pi^{d/2}} \\sum_{j_1=1}^{n}\\cdots \\sum_{j_d=1}^{n} \\Big(\\prod_{k=1}^{d} w_{j_k}\\Big)\\, g\\!\\Big(\\sqrt{2}\\,x_{j_1},\\dots,\\sqrt{2}\\,x_{j_d}\\Big),\n$$\nwhere $(x_{j},w_{j})$ are the one-dimensional Gaussian–Hermite nodes and weights at order $n$, associated with the weight $e^{-x^2}$.\n\nApplying this to the basket payoff, we define\n$$\ng(\\boldsymbol{z}) = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,\\exp\\!\\big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, z_i \\big) - K,\\, 0\\Big),\n$$\nand the price approximation is\n$$\nV_0 \\approx e^{-rT}\\,\\frac{1}{\\pi^{d/2}} \\sum_{j_1=1}^{n}\\cdots \\sum_{j_d=1}^{n} \\Big(\\prod_{k=1}^{d} w_{j_k}\\Big)\\, g\\!\\Big(\\sqrt{2}\\,x_{j_1},\\dots,\\sqrt{2}\\,x_{j_d}\\Big).\n$$\n\nThis tensor-product rule uses $n^d$ function evaluations. For $d=5$, the node count grows as $n^5$, which is a manifestation of the curse of dimensionality: even modest increases in $n$ cause exponential growth in cost.\n\nTo mitigate this, we consider a Smolyak sparse grid constructed from the same one-dimensional Gaussian–Hermite family. Let $Q_{\\ell}$ denote the one-dimensional Gaussian–Hermite quadrature of order $m(\\ell)=2\\ell-1$ for level $\\ell \\in \\mathbb{N}$. For isotropic level $L \\in \\mathbb{N}$ in $d$ dimensions, the Smolyak operator is\n$$\nA(L,d) = \\sum_{\\boldsymbol{\\ell}\\in \\mathcal{I}(L,d)} c(\\boldsymbol{\\ell}) \\bigotimes_{k=1}^{d} Q_{\\ell_k},\n$$\nwith index set\n$$\n\\mathcal{I}(L,d) = \\big\\{\\boldsymbol{\\ell}\\in \\mathbb{N}^d : 1 \\le \\ell_k,\\ \\ |\\boldsymbol{\\ell}|_1 \\le L + d - 1\\big\\},\n$$\nand combination coefficients\n$$\nc(\\boldsymbol{\\ell}) = (-1)^{L + d - 1 - |\\boldsymbol{\\ell}|_1}\\binom{d-1}{L + d - 1 - |\\boldsymbol{\\ell}|_1}.\n$$\nThe resulting sparse grid approximation of the expectation is\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] \\approx \\frac{1}{\\pi^{d/2}} \\sum_{\\boldsymbol{\\ell}\\in \\mathcal{I}(L,d)} c(\\boldsymbol{\\ell}) \\sum_{j_1=1}^{m(\\ell_1)} \\cdots \\sum_{j_d=1}^{m(\\ell_d)} \\left(\\prod_{k=1}^{d} w_{j_k}^{(\\ell_k)}\\right) g\\!\\Big(\\sqrt{2}\\,x_{j_1}^{(\\ell_1)},\\dots,\\sqrt{2}\\,x_{j_d}^{(\\ell_d)}\\Big),\n$$\nwhere $(x_j^{(\\ell)}, w_j^{(\\ell)})$ are the one-dimensional Gaussian–Hermite nodes and weights at order $m(\\ell)$. Note that this combination utilizes the same one-dimensional rules as the tensor product but mixes tensor products of varying orders through the Smolyak coefficients. The computational cost is the sum over tensor-product sizes $\\prod_{k=1}^d m(\\ell_k)$ for all $\\boldsymbol{\\ell} \\in \\mathcal{I}(L,d)$; this grows more gently in $d$ than $n^d$ for comparable accuracy.\n\nAlgorithmic design:\n\n- Precompute the one-dimensional Gaussian–Hermite nodes and weights for all required orders via a stable generator for orthogonal polynomials. The nodes and weights integrate $e^{-x^2}$ exactly up to degree $2n-1$, and the sum of weights equals $\\sqrt{\\pi}$, ensuring exactness for constant integrands.\n- Implement the tensor-product quadrature by forming a Cartesian product of one-dimensional nodes and weights, applying the $\\sqrt{2}$ scaling to map to standard normals, and multiplying the product weights. Multiply the accumulated sum by $\\pi^{-d/2}$ and $e^{-rT}$.\n- Implement the Smolyak sparse grid by iterating over all multi-indices $\\boldsymbol{\\ell}$ in $\\mathcal{I}(L,d)$, computing the combination coefficient $c(\\boldsymbol{\\ell})$, forming the tensor grid for each $\\boldsymbol{\\ell}$ using $m(\\ell_k)$ points in dimension $k$, evaluating and accumulating the weighted contributions with the same normalization factors. Count the computational work as the sum of tensor sizes across all $\\boldsymbol{\\ell}$.\n- For the boundary case with $\\sigma_i = 0$, the terminal prices are deterministic, $S_i(T) = S_i(0)e^{rT}$, and therefore the exact price is $V_0^{\\text{det}} = \\max\\!\\big(\\frac{1}{d}\\sum_i S_i(0) - K e^{-rT}, 0\\big)$. Because Gaussian–Hermite rules integrate constants exactly, both the tensor and sparse-grid approximations should match this value up to rounding errors.\n\nTest Suite implementation:\n\n- Case A: $d=5$, $S_0=(100,90,110,95,105)$, $\\sigma=(0.2,0.25,0.15,0.3,0.18)$, $r=0.02$, $T=1.0$, $K=100$. Compute:\n  - Reference with tensor $n_{\\text{ref}}=9$.\n  - Tensor with $n=3$; record absolute error and node count $3^5$.\n  - Smolyak with $L=3$ and $m(\\ell)=2\\ell-1$; record absolute error against reference and the sparse-grid node count as the sum of tensor sizes over $\\mathcal{I}(L,d)$.\n- Case B: same as Case A but $\\sigma=\\boldsymbol{0}$. Verify both tensor $n=3$ and sparse $L=3$ are within $10^{-10}$ of $V_0^{\\text{det}}$.\n- Efficiency comparison: For Case A, check whether the sparse grid achieves error less than or equal to the tensor-product error while using strictly fewer nodes.\n\nThe program will produce a single output line:\n$[\\text{err\\_tensor},\\text{err\\_sparse},\\text{nodes\\_tensor},\\text{nodes\\_sparse},\\text{boundary\\_ok},\\text{efficiency\\_ok}]$,\nwith the entries defined as above. This design demonstrates the curse of dimensionality via the $n^5$ scaling for the tensor grid and contrasts it with the Smolyak sparse grid, which, for the same one-dimensional Gaussian–Hermite family, achieves competitive accuracy with substantially fewer function evaluations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.polynomial.hermite import hermgauss\nimport math\nfrom itertools import product\n\ndef gh_rule(n):\n    # One-dimensional Gauss-Hermite nodes and weights for weight e^{-x^2}\n    x, w = hermgauss(n)\n    return x, w\n\ndef basket_payoff(Z, S0, sigmas, r, T, K):\n    # Z: shape (N, d)\n    d = Z.shape[1]\n    S0 = np.asarray(S0, dtype=float)\n    sigmas = np.asarray(sigmas, dtype=float)\n    mu = (r - 0.5 * sigmas**2) * T\n    sgsqrtT = sigmas * math.sqrt(T)\n    # exponent per point and dimension\n    exponents = mu + sgsqrtT * Z  # shape (N, d)\n    ST = S0 * np.exp(exponents)   # broadcasting over (N, d)\n    avg = np.mean(ST, axis=1)\n    payoff = np.maximum(avg - K, 0.0)\n    return payoff\n\ndef tensor_gauss_hermite_price(S0, sigmas, r, T, K, n):\n    d = len(S0)\n    x, w = gh_rule(n)\n    # Build tensor grid via meshgrid\n    grids_x = np.meshgrid(*([x]*d), indexing='ij')\n    grids_w = np.meshgrid(*([w]*d), indexing='ij')\n    # Flatten\n    X_cols = [g.ravel() for g in grids_x]  # list length d, each shape (n**d,)\n    W_arrays = [gw.ravel() for gw in grids_w]\n    # Product weights\n    W_prod = np.ones_like(W_arrays[0])\n    for Wa in W_arrays:\n        W_prod *= Wa\n    # Map to standard normals: Z = sqrt(2) * x\n    Z = np.sqrt(2.0) * np.column_stack(X_cols)  # shape (N, d)\n    payoff = basket_payoff(Z, S0, sigmas, r, T, K)\n    # Normalization factor for d-dim expectation\n    factor = math.pi ** (-d / 2.0)\n    expectation = factor * np.sum(W_prod * payoff)\n    price = math.exp(-r * T) * expectation\n    nodes = n ** d\n    return price, nodes\n\ndef generate_multi_indices_sum_d(s, d, min_level=1):\n    # Generate all tuples of length d of positive integers >= min_level with sum s\n    # Use recursive backtracking\n    result = []\n    def backtrack(prefix, remaining, k):\n        if k == d - 1:\n            last = remaining\n            if last >= min_level:\n                result.append(tuple(prefix + [last]))\n            return\n        # Each component at least min_level\n        min_val = min_level\n        # Remaining must allow at least min_level for remaining dims\n        max_val = remaining - (d - k - 1) * min_level\n        for val in range(min_val, max_val + 1):\n            backtrack(prefix + [val], remaining - val, k + 1)\n    backtrack([], s, 0)\n    return result\n\ndef smolyak_sparse_price(S0, sigmas, r, T, K, L):\n    d = len(S0)\n    # Precompute 1D rules for levels 1..(L + d - 1) because indices can reach that in sum, but each component level is bounded by sum\n    # However, for efficiency we build on demand and cache by level\n    rule_cache = {}\n    def one_d_rule_for_level(level):\n        if level not in rule_cache:\n            n = 2 * level - 1\n            rule_cache[level] = gh_rule(n)\n        return rule_cache[level]\n\n    total_nodes_cost = 0\n    factor = math.pi ** (-d / 2.0)\n    acc = 0.0\n    # Iterate sums s from d to L + d - 1\n    for s in range(d, L + d):\n        # Binomial coefficient for all multi-indices with |ell|_1 = s\n        comb_coeff = math.comb(d - 1, L + d - 1 - s)\n        sign = -1 if ((L + d - 1 - s) % 2 == 1) else 1\n        c_s = sign * comb_coeff\n        if c_s == 0:\n            continue\n        # All multi-indices with sum s\n        for ell in generate_multi_indices_sum_d(s, d, min_level=1):\n            # Build per-dimension nodes and weights at levels in ell\n            x_list = []\n            w_list = []\n            n_points_list = []\n            for lev in ell:\n                xk, wk = one_d_rule_for_level(lev)\n                x_list.append(xk)\n                w_list.append(wk)\n                n_points_list.append(len(xk))\n            # Tensor product for this multi-index\n            grids_x = np.meshgrid(*x_list, indexing='ij')\n            grids_w = np.meshgrid(*w_list, indexing='ij')\n            # Flatten\n            X_cols = [g.ravel() for g in grids_x]\n            W_arrays = [gw.ravel() for gw in grids_w]\n            # Product weights\n            W_prod = np.ones_like(W_arrays[0])\n            for Wa in W_arrays:\n                W_prod *= Wa\n            # Map to Z\n            Z = np.sqrt(2.0) * np.column_stack(X_cols)\n            payoff = basket_payoff(Z, S0, sigmas, r, T, K)\n            contrib = factor * np.sum(W_prod * payoff)\n            acc += c_s * contrib\n            # Cost accumulation: number of nodes in this tensor product\n            nodes_this = 1\n            for npt in n_points_list:\n                nodes_this *= npt\n            total_nodes_cost += nodes_this\n    price = math.exp(-r * T) * acc\n    return price, total_nodes_cost\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A parameters\n    S0 = [100.0, 90.0, 110.0, 95.0, 105.0]\n    sigmas_A = [0.2, 0.25, 0.15, 0.3, 0.18]\n    r = 0.02\n    T = 1.0\n    K = 100.0\n    d = len(S0)\n\n    # Reference with tensor n_ref=9\n    price_ref, nodes_ref = tensor_gauss_hermite_price(S0, sigmas_A, r, T, K, n=9)\n\n    # Tensor with n=3\n    price_tensor, nodes_tensor = tensor_gauss_hermite_price(S0, sigmas_A, r, T, K, n=3)\n    err_tensor = abs(price_tensor - price_ref)\n\n    # Sparse grid with L=3\n    price_sparse, nodes_sparse = smolyak_sparse_price(S0, sigmas_A, r, T, K, L=3)\n    err_sparse = abs(price_sparse - price_ref)\n\n    # Case B: zero volatility boundary\n    sigmas_B = [0.0, 0.0, 0.0, 0.0, 0.0]\n    # Deterministic price\n    mean_S0 = sum(S0) / d\n    price_det = max(mean_S0 - K * math.exp(-r * T), 0.0)\n\n    price_tensor_B, _ = tensor_gauss_hermite_price(S0, sigmas_B, r, T, K, n=3)\n    price_sparse_B, _ = smolyak_sparse_price(S0, sigmas_B, r, T, K, L=3)\n\n    tol = 1e-10\n    boundary_ok = (abs(price_tensor_B - price_det) <= tol) and (abs(price_sparse_B - price_det) <= tol)\n\n    # Efficiency comparison for Case A\n    efficiency_ok = (err_sparse <= err_tensor) and (nodes_sparse < nodes_tensor)\n\n    results = [\n        float(err_tensor),\n        float(err_sparse),\n        int(nodes_tensor),\n        int(nodes_sparse),\n        bool(boundary_ok),\n        bool(efficiency_ok),\n    ]\n\n    # Final print statement in the exact required format.\n    # Ensure default Python representation for floats and booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "After establishing the superiority of sparse grids over tensor products, a natural question arises: how do they stack up against other workhorses of high-dimensional integration, like the Monte Carlo method? This practice challenges you to compare the deterministic convergence of a sparse grid with the probabilistic convergence of a Monte Carlo simulation. You will analytically derive the necessary benchmarks and then numerically find the \"break-even\" point where the sparse grid's faster convergence rate overcomes its higher initial setup cost, solidifying your understanding of efficiency trade-offs. [@problem_id:2432650]", "id": "2432650", "problem": "You are asked to design and implement a program that, for a smooth, non-separable integrand, determines the minimum number of function evaluations at sparse grid nodes (Smolyak quadrature points) required to outperform a Monte Carlo estimator using the same number of samples. The setting is integration over a hypercube and uses a canonical nested one-dimensional quadrature rule as the building block for the Smolyak construction. Angles are to be interpreted in radians.\n\nLet the integration domain be the $d$-dimensional hypercube $[-1,1]^d$, and define the integrand by\n$$\nf(\\mathbf{x}) \\equiv \\cos\\!\\Big(\\sum_{i=1}^{d} x_i\\Big), \\quad \\mathbf{x} = (x_1,\\dots,x_d) \\in [-1,1]^d.\n$$\nWe are interested in the volume integral\n$$\nI_d \\equiv \\int_{[-1,1]^d} f(\\mathbf{x})\\, d\\mathbf{x}.\n$$\nYour program must construct isotropic Smolyak quadrature rules based on nested one-dimensional Clenshaw–Curtis rules, and compare their absolute integration error against the root-mean-square error of a Monte Carlo estimator with the same number of function evaluations $N$.\n\nFundamental base and definitions to use:\n- One-dimensional Clenshaw–Curtis nodes of order $n$ are given by\n$$\nx_j = \\cos\\!\\Big(\\frac{\\pi j}{n-1}\\Big), \\quad j=0,1,\\dots,n-1,\n$$\nwith the convention that for $n=1$ the single node is $x_0=1$. The Clenshaw–Curtis weights $w_j$ are uniquely determined by exactness on the first $n$ Chebyshev polynomials of the first kind, that is, for $k=0,1,\\dots,n-1$,\n$$\n\\sum_{j=0}^{n-1} w_j\\, T_k(x_j) = \\int_{-1}^{1} T_k(x)\\,dx,\n$$\nwhere $T_k(x)$ are the Chebyshev polynomials defined by $T_k(\\cos\\theta)=\\cos(k\\theta)$. The right-hand side has the known values\n$$\n\\int_{-1}^{1} T_0(x)\\,dx = 2,\\quad \\int_{-1}^{1} T_k(x)\\,dx = \\begin{cases} \\dfrac{2}{1-k^2}, & k \\text{ even},\\, k\\ge 2,\\\\ 0, & k \\text{ odd}. \\end{cases}\n$$\nUse these equalities to determine the weights $w_j$ by solving a linear system for each one-dimensional rule.\n- Use a nested growth rule for the one-dimensional levels $\\ell \\in \\mathbb{N}$: the quadrature $K_\\ell$ has $m(\\ell)$ points with\n$$\nm(1)=1,\\quad m(\\ell)=2^{\\ell-1}+1 \\text{ for } \\ell\\ge 2,\n$$\nwhich yields nested nodes for Clenshaw–Curtis quadrature.\n- For dimension $d$ and nonnegative integer Smolyak level $s$, define the Smolyak quadrature $A(s,d)$ by the standard combination (Smolyak) formula\n$$\nA(s,d) f = \\sum_{\\mathbf{i}\\in\\mathbb{N}^d:\\, \\|\\mathbf{i}\\|_1 \\le s+d} \\left[ (-1)^{\\,s+d-\\|\\mathbf{i}\\|_1}\\binom{d-1}{\\,s+d-\\|\\mathbf{i}\\|_1} \\prod_{k=1}^{d} K_{i_k} \\right] f,\n$$\nwhere $K_{i_k}$ is the one-dimensional quadrature at level $i_k$, $\\|\\mathbf{i}\\|_1 = \\sum_{k=1}^{d} i_k$, and the binomial coefficient is taken to be zero if its lower argument is outside $\\{0,1,2,\\dots\\}$. Implement this formula literally by forming tensor products of one-dimensional rules, summing weights for identical node locations across terms, and then summing the weighted function values to produce the Smolyak approximation. The number of distinct nodes in $A(s,d)$ is the count of unique points after merging identical locations with their combined weights; denote this count by $N_{\\text{SG}}(s,d)$.\n- The Monte Carlo estimator for $I_d$ using $N$ samples is $2^d$ times the sample mean of $f(\\mathbf{X})$ with $\\mathbf{X}$ uniformly distributed on $[-1,1]^d$. Its root-mean-square error equals $2^d \\sqrt{\\operatorname{Var}(f(\\mathbf{X}))/N}$. For $f(\\mathbf{x})=\\cos(\\sum_{i=1}^d x_i)$ and $\\mathbf{X}$ uniform on $[-1,1]^d$, you must compute $\\operatorname{Var}(f(\\mathbf{X}))$ exactly from first principles using independence and characteristic functions.\n\nYour program must:\n- Derive and use a closed-form expression for $I_d$.\n- Derive and use a closed-form expression for $\\operatorname{Var}(f(\\mathbf{X}))$ under the uniform distribution on $[-1,1]^d$.\n- For each test case dimension $d$, search over Smolyak levels $s\\in\\{0,1,2,\\dots,s_{\\max}\\}$ with $s_{\\max}=8$ and determine the smallest $s$ such that the absolute quadrature error\n$$\nE_{\\text{SG}}(s,d) \\equiv \\left|A(s,d)f - I_d\\right|\n$$\nis less than or equal to the Monte Carlo root-mean-square error at the same number of function evaluations $N_{\\text{SG}}(s,d)$:\n$$\nE_{\\text{SG}}(s,d) \\le 2^{d} \\sqrt{\\frac{\\operatorname{Var}(f(\\mathbf{X}))}{N_{\\text{SG}}(s,d)}}.\n$$\nIf no such $s$ exists up to $s_{\\max}$, return $-1$ for that test case.\n- Define “function evaluations” as the number of distinct nodes in the sparse grid after merging coincident nodes across tensor products.\n\nTest suite:\n- Case A: $d=1$ (boundary check in one dimension).\n- Case B: $d=2$ (typical case).\n- Case C: $d=5$ (higher-dimensional case).\n\nFinal output format:\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (e.g., “[3,7,129]”). Each entry must be the minimum number of distinct sparse-grid nodes $N_{\\text{SG}}(s,d)$ that achieves the criterion above for the corresponding $d$ in the test suite, or $-1$ if none is found up to $s_{\\max}$.\n\nAll angles must be in radians. No physical units are involved. All numerical answers must be expressed as integers. The output must match the exact format specification.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It outlines a standard task in numerical analysis: comparing the efficiency of sparse grid quadrature against a Monte Carlo method for a specific multidimensional integral. All definitions and formulas, while including a peculiar convention for the base case of the quadrature rule, are explicit and mathematically consistent. Therefore, I will proceed with a full solution.\n\nThe problem requires us to find the minimum number of nodes for an isotropic Smolyak sparse grid, $N_{\\text{SG}}(s,d)$, at which the absolute integration error is smaller than the root-mean-square (RMS) error of a Monte Carlo estimate using the same number of function evaluations. This must be determined for dimensions $d=1, 2, 5$ and for a Smolyak level $s$ searched from $0$ to $8$.\n\nFirst, we must derive the necessary analytical expressions for the true value of the integral and the variance of the integrand, which serve as benchmarks for our numerical methods.\n\n**1. Analytical Value of the Integral**\n\nThe integral to be computed is\n$$\nI_d = \\int_{[-1,1]^d} \\cos\\Big(\\sum_{i=1}^{d} x_i\\Big)\\, d\\mathbf{x}.\n$$\nUsing Euler's formula, $\\cos(\\theta) = \\operatorname{Re}(e^{i\\theta})$, the integral can be rewritten as\n$$\nI_d = \\operatorname{Re}\\left( \\int_{[-1,1]^d} \\exp\\left(i \\sum_{k=1}^{d} x_k\\right) d\\mathbf{x} \\right).\n$$\nThe exponential of a sum is a product of exponentials. By Fubini's theorem, the multidimensional integral becomes a product of one-dimensional integrals:\n$$\nI_d = \\operatorname{Re}\\left( \\prod_{k=1}^{d} \\int_{-1}^{1} e^{ix_k} dx_k \\right).\n$$\nThe one-dimensional integral is elementary:\n$$\n\\int_{-1}^{1} e^{ix} dx = \\left[\\frac{e^{ix}}{i}\\right]_{-1}^{1} = \\frac{e^i - e^{-i}}{i} = 2 \\frac{e^i - e^{-i}}{2i} = 2\\sin(1).\n$$\nSince $2\\sin(1)$ is a real-valued constant, the integral $I_d$ is simply this value raised to the power of the dimension $d$:\n$$\nI_d = (2\\sin(1))^d.\n$$\n\n**2. Analytical Variance of the Integrand**\n\nThe variance of the integrand $f(\\mathbf{X}) = \\cos(\\sum_{k=1}^d X_k)$, where $X_k$ are independent and identically distributed random variables from a Uniform$(-1, 1)$ distribution, is given by $\\operatorname{Var}(f(\\mathbf{X})) = E[f(\\mathbf{X})^2] - (E[f(\\mathbf{X})])^2$.\n\nThe expectation of $f(\\mathbf{X})$ is\n$$\nE[f(\\mathbf{X})] = \\frac{1}{\\text{Vol}([-1,1]^d)} \\int_{[-1,1]^d} f(\\mathbf{x}) d\\mathbf{x} = \\frac{I_d}{2^d} = \\left(\\frac{2\\sin(1)}{2}\\right)^d = (\\sin(1))^d.\n$$\nThe expectation of the squared function, $E[f(\\mathbf{X})^2]$, is calculated by integrating $f(\\mathbf{x})^2$ over the hypercube:\n$$\nE[f(\\mathbf{X})^2] = \\frac{1}{2^d} \\int_{[-1,1]^d} \\cos^2\\Big(\\sum_{k=1}^{d} x_k\\Big) d\\mathbf{x}.\n$$\nUsing the identity $\\cos^2(\\theta) = \\frac{1+\\cos(2\\theta)}{2}$, we get\n$$\nE[f(\\mathbf{X})^2] = \\frac{1}{2^d} \\int_{[-1,1]^d} \\frac{1}{2}\\left(1 + \\cos\\Big(2\\sum_{k=1}^{d} x_k\\Big)\\right) d\\mathbf{x} = \\frac{1}{2^{d+1}} \\left( \\int_{[-1,1]^d} 1\\,d\\mathbf{x} + \\int_{[-1,1]^d} \\cos\\Big(\\sum_{k=1}^{d} 2x_k\\Big) d\\mathbf{x} \\right).\n$$\nThe first integral is the volume $2^d$. The second integral is analogous to the calculation for $I_d$:\n$$\n\\int_{[-1,1]^d} \\cos\\Big(\\sum_{k=1}^{d} 2x_k\\Big) d\\mathbf{x} = \\operatorname{Re}\\left( \\prod_{k=1}^{d} \\int_{-1}^{1} e^{i2x_k} dx_k \\right).\n$$\nThe one-dimensional integral evaluates to $\\int_{-1}^1 e^{i2x} dx = [\\frac{e^{i2x}}{2i}]_{-1}^1 = \\frac{e^{i2}-e^{-i2}}{2i} = \\sin(2)$. Thus, the multidimensional integral is $(\\sin(2))^d$.\nSubstituting back, we find the second moment:\n$$\nE[f(\\mathbf{X})^2] = \\frac{1}{2^{d+1}} \\left( 2^d + (\\sin(2))^d \\right) = \\frac{1}{2} + \\frac{1}{2}\\left(\\frac{\\sin(2)}{2}\\right)^d.\n$$\nFinally, the variance is:\n$$\n\\operatorname{Var}(f(\\mathbf{X})) = \\left[\\frac{1}{2} + \\frac{1}{2}\\left(\\frac{\\sin(2)}{2}\\right)^d\\right] - \\left((\\sin(1))^d\\right)^2 = \\frac{1}{2} + \\frac{1}{2}\\left(\\frac{\\sin(2)}{2}\\right)^d - (\\sin(1))^{2d}.\n$$\n\n**3. Numerical Quadrature Scheme**\n\nThe core of the task is to construct the Smolyak sparse grid and evaluate its performance.\n\n**3.1. One-Dimensional Clenshaw-Curtis Rules**\nFor each level $\\ell \\in \\mathbb{N}$, a one-dimensional quadrature rule $K_\\ell$ with $n=m(\\ell)$ points is constructed. The number of points follows the nested growth rule $m(1)=1$ and $m(\\ell)=2^{\\ell-1}+1$ for $\\ell \\ge 2$. The nodes are $x_j = \\cos(\\frac{\\pi j}{n-1})$ for $j=0, \\dots, n-1$, with the explicit, albeit unusual, convention that for $n=1$, the single node is $x_0=1$.\nThe weights $w_j$ for each rule are found by enforcing exactness for the first $n$ Chebyshev polynomials $T_k(x)$, which requires solving the $n \\times n$ linear system:\n$$\n\\sum_{j=0}^{n-1} w_j T_k(x_j) = \\int_{-1}^{1} T_k(x) dx \\quad \\text{for } k=0, \\dots, n-1.\n$$\nThe matrix elements are $A_{kj} = T_k(x_j) = \\cos(k \\arccos(x_j)) = \\cos(k \\frac{\\pi j}{n-1})$. The right-hand side vector comprises the known integrals of Chebyshev polynomials. These rules are pre-computed and cached for efficiency.\n\n**3.2. Smolyak Sparse Grid Construction**\nThe Smolyak quadrature rule $A(s,d)$ for dimension $d$ and level $s$ is built using the combination formula:\n$$\nA(s,d) f = \\sum_{\\mathbf{i} \\in \\mathbb{N}^d,\\, s+1 \\le \\|\\mathbf{i}\\|_1 \\le s+d} c(\\mathbf{i}) \\left( \\bigotimes_{k=1}^{d} K_{i_k} \\right) f,\n$$\nwhere $\\|\\mathbf{i}\\|_1 = \\sum_{k=1}^d i_k$, the coefficients are $c(\\mathbf{i}) = (-1)^{s+d-\\|\\mathbf{i}\\|_1}\\binom{d-1}{s+d-\\|\\mathbf{i}\\|_1}$, and $\\bigotimes_{k=1}^d K_{i_k}$ is the tensor product of the one-dimensional rules. The sum is restricted to multi-indices $\\mathbf{i}$ where the binomial coefficient is non-zero, which corresponds to $s+1 \\le \\|\\mathbf{i}\\|_1 \\le s+d$.\n\nThe algorithm proceeds as follows:\n1. Iterate through all valid multi-indices $\\mathbf{i}$.\n2. For each $\\mathbf{i}$, calculate the coefficient $c(\\mathbf{i})$.\n3. Construct the tensor product grid. A point in this grid is a $d$-tuple of nodes $(\\xi_1, \\dots, \\xi_d)$, where $\\xi_k$ is a node from $K_{i_k}$. Its corresponding weight is the product of the individual 1D weights.\n4. Scale the weight of each tensor product point by $c(\\mathbf{i})$.\n5. Accumulate these points and their scaled weights into a global dictionary, where keys are node coordinates (as tuples) and values are the total summed weights. The use of a dictionary naturally merges points that appear in multiple tensor-product terms.\n\nAfter iterating through all $\\mathbf{i}$, this dictionary represents the complete sparse grid. The number of distinct nodes, $N_{\\text{SG}}(s,d)$, is the number of entries in this dictionary. The integral approximation is $\\sum_{\\mathbf{x}, W} W \\cdot f(\\mathbf{x})$.\n\n**4. The Comparison Criterion**\n\nFor each dimension $d$ in the test suite, we iterate through the Smolyak level $s$ from $0$ to $s_{\\max}=8$. For each $(s,d)$, we compute the sparse grid approximation $A(s,d)f$ and its absolute error $E_{\\text{SG}}(s,d) = |A(s,d)f - I_d|$. We also compute the number of nodes $N_{\\text{SG}}(s,d)$. This is compared against the Monte Carlo RMS error for the same number of evaluations:\n$$\nE_{\\text{MC}}(N_{\\text{SG}}) = 2^d \\sqrt{\\frac{\\operatorname{Var}(f(\\mathbf{X}))}{N_{\\text{SG}}(s,d)}}.\n$$\nThe first level $s$ for which $E_{\\text{SG}}(s,d) \\le E_{\\text{MC}}(N_{\\text{SG}})$ is found, and the corresponding number of nodes $N_{\\text{SG}}(s,d)$ is recorded. If no such level is found up to $s_{\\max}$, the result is $-1$. This procedure is repeated for all test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nimport math\nfrom itertools import product\nimport sys\n\n# Set a higher recursion limit for multi-index generation, which can be deep for d=5.\nsys.setrecursionlimit(2500)\n\n# Global caches for memoization to speed up repeated computations.\n_1d_rules_cache = {}\n_compositions_cache = {}\n\ndef get_1d_rule(level: int):\n    \"\"\"\n    Computes or retrieves from cache the Clenshaw-Curtis nodes and weights for a given level.\n    \"\"\"\n    if level in _1d_rules_cache:\n        return _1d_rules_cache[level]\n\n    if level == 1:\n        # Per problem statement: n=1, node is 1.\n        # Weight w_0 from w_0 * T_0(1) = integral(T_0) = 2. Since T_0(x)=1, w_0=2.\n        nodes = np.array([1.0])\n        weights = np.array([2.0])\n        _1d_rules_cache[level] = (nodes, weights)\n        return nodes, weights\n\n    n = 2**(level - 1) + 1\n    \n    # Nodes are the extrema of Chebyshev polynomials.\n    j = np.arange(n)\n    nodes = np.cos(math.pi * j / (n - 1))\n\n    # Right-hand side of the linear system for weights.\n    b = np.zeros(n)\n    b[0] = 2.0\n    for k in range(2, n, 2):\n        b[k] = 2.0 / (1.0 - k**2)\n\n    # Matrix A where A_kj = T_k(x_j) = cos(k*j*pi/(n-1)).\n    k_vals = np.arange(n)\n    A = np.cos(np.outer(k_vals, j * math.pi / (n - 1)))\n    \n    # Solve for weights.\n    weights = np.linalg.solve(A, b)\n    \n    _1d_rules_cache[level] = (nodes, weights)\n    return nodes, weights\n\ndef generate_compositions(n: int, k: int):\n    \"\"\"\n    Generates all compositions of integer k into n parts (each part >= 1), with memoization.\n    \"\"\"\n    if (n, k) in _compositions_cache:\n        return _compositions_cache[(n, k)]\n\n    if n == 1:\n        return [(k,)] if k >= 1 else []\n    if k < n:\n        return []\n\n    res = []\n    for i in range(1, k - (n - 1) + 1):\n        for rest in generate_compositions(n - 1, k - i):\n            res.append((i,) + rest)\n    \n    _compositions_cache[(n, k)] = res\n    return res\n\ndef get_smolyak_grid(s: int, d: int):\n    \"\"\"\n    Constructs the Smolyak sparse grid for a given level s and dimension d.\n    Returns a dictionary mapping node coordinates to their combined weights.\n    \"\"\"\n    grid_points = {}  # {node_tuple: weight}\n\n    # The sum is effectively over multi-indices i where s+1 <= sum(i) <= s+d\n    # because the combinatorial coefficient is zero otherwise.\n    for norm_i in range(s + 1, s + d + 1):\n        c = (-1)**(s + d - norm_i) * comb(d - 1, s + d - norm_i, exact=True)\n        if c == 0:\n            continue\n            \n        compositions = generate_compositions(d, norm_i)\n        \n        for i_tuple in compositions:\n            rules_for_i = [get_1d_rule(level) for level in i_tuple]\n            \n            # Use itertools.product to form tensor product points and weights\n            node_indices_ranges = [range(len(r[0])) for r in rules_for_i]\n            \n            for index_tuple in product(*node_indices_ranges):\n                node_coords = tuple(rules_for_i[k][0][index_tuple[k]] for k in range(d))\n                \n                weight_prod = 1.0\n                for k in range(d):\n                    weight_prod *= rules_for_i[k][1][index_tuple[k]]\n                \n                total_weight = c * weight_prod\n                grid_points[node_coords] = grid_points.get(node_coords, 0.0) + total_weight\n                \n    return grid_points\n\ndef f(x: tuple):\n    \"\"\"The integrand function.\"\"\"\n    return np.cos(np.sum(x))\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the entire process.\n    \"\"\"\n    test_cases = [1, 2, 5]\n    s_max = 8\n    results = []\n\n    for d in test_cases:\n        # Pre-calculated analytical values for the integral and variance\n        I_d = (2 * math.sin(1))**d\n        var_f = 0.5 + 0.5 * (math.sin(2) / 2)**d - (math.sin(1))**(2 * d)\n        \n        found_n = -1\n        \n        for s in range(s_max + 1):\n            smolyak_grid = get_smolyak_grid(s, d)\n            \n            if not smolyak_grid:\n                continue\n\n            n_sg = len(smolyak_grid)\n            \n            integral_approx = 0.0\n            for node, weight in smolyak_grid.items():\n                integral_approx += weight * f(node)\n            \n            e_sg = abs(integral_approx - I_d)\n            e_mc_rms = (2**d) * math.sqrt(var_f / n_sg)\n            \n            if e_sg <= e_mc_rms:\n                found_n = n_sg\n                break\n        \n        results.append(found_n)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"}, {"introduction": "Standard sparse grids distribute points based on a-priori assumptions about a function's smoothness, but many real-world problems feature localized behavior like sharp peaks or kinks. This advanced exercise guides you through building a powerful adaptive sparse grid that intelligently adds points only where they are needed most, driven by the magnitude of hierarchical surplus coefficients which act as local error indicators. Implementing this algorithm will equip you with a sophisticated approach to computational efficiency, enabling you to accurately approximate complex functions with a minimal number of evaluations. [@problem_id:2432623]", "id": "2432623", "problem": "You are to design and implement an adaptive sparse grid interpolant based on the Smolyak construction with a hierarchical, piecewise-linear (hat) basis on the hypercube domain $[0,1]^d$. The algorithm must refine the grid adaptively using the magnitude of the hierarchical surplus coefficients as the refinement indicator. Your implementation must be a complete, runnable program that computes specified quantitative outputs for a defined test suite.\n\nThe required algorithmic elements must be derived from the following foundational base:\n- Core definitions of hierarchical bases in one dimension: For level $l \\in \\mathbb{N}$ and odd index $i \\in \\{1,3,\\dots,2^l - 1\\}$, define the 1-dimensional node $x_{l,i} = i / 2^l$ and the hat basis function\n$$\n\\varphi_{l,i}(x) = \\max\\left(1 - 2^l \\left| x - \\frac{i}{2^l} \\right|, 0\\right).\n$$\n- Multidimensional tensor basis: For $d \\in \\mathbb{N}$, multi-level $\\boldsymbol{l} = (l_1,\\dots,l_d)$, and multi-index $\\boldsymbol{i} = (i_1,\\dots,i_d)$ with each $i_j \\in \\{1,3,\\dots,2^{l_j}-1\\}$, the node is $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}} = (i_1 2^{-l_1},\\dots,i_d 2^{-l_d})$ and the basis function factorizes as\n$$\n\\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}) = \\prod_{j=1}^d \\varphi_{l_j,i_j}(x_j).\n$$\n- Hierarchical interpolation: Given a set $\\mathcal{A}$ of multi-indices $(\\boldsymbol{l},\\boldsymbol{i})$, the interpolant is\n$$\n\\mathcal{I} f(\\boldsymbol{x}) = \\sum_{(\\boldsymbol{l},\\boldsymbol{i}) \\in \\mathcal{A}} \\alpha_{\\boldsymbol{l},\\boldsymbol{i}} \\, \\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}),\n$$\nwhere the hierarchical surplus coefficients are defined recursively by\n$$\n\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f\\!\\left(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}\\right) - \\mathcal{I}_{\\text{prev}} f\\!\\left(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}\\right),\n$$\nwith $\\mathcal{I}_{\\text{prev}}$ denoting the interpolant formed by all previously added basis functions corresponding to strictly coarser nodes in a downward-closed sense.\n\nYour adaptive algorithm must:\n1. Initialize with the single interior node at level $\\boldsymbol{l} = (1,\\dots,1)$ and index $\\boldsymbol{i} = (1,\\dots,1)$, compute its hierarchical surplus, and insert it into an adaptive queue keyed by the absolute surplus magnitude.\n2. Iteratively select the node in the queue with the largest absolute surplus and refine it by adding its admissible children. A child along coordinate $j$ increases $l_j$ by $1$ and replaces $i_j$ by one of $\\{2 i_j - 1, 2 i_j + 1\\}$ while keeping all other coordinates unchanged. For each newly added child, compute its hierarchical surplus using the current interpolant, and insert the child into the queue.\n3. Terminate when either the maximum absolute surplus over all currently available nodes is below a given tolerance $ \\tau > 0$, or a specified maximum number of grid points $N_{\\max}$ has been reached.\n\nUse this algorithm to construct interpolants for the following test suite and compute the maximum absolute interpolation error on a specified validation grid for each case. All angles, where applicable, must be interpreted in radians.\n\nFor each test, define:\n- The dimension $d$.\n- The target function $f : [0,1]^d \\to \\mathbb{R}$.\n- The tolerance $\\tau$ and cap $N_{\\max}$.\n- A validation grid formed as the Cartesian product of $m$ equidistant points per dimension in the open interval $(0,1)$, specifically at $x_k = \\frac{k}{m+1}$ for $k = 1,2,\\dots,m$.\n- The final outputs to be computed: the number of grid points actually used (an integer) and the maximum absolute error on the validation grid (a float), i.e.,\n$$\nE_{\\max} = \\max_{\\boldsymbol{x} \\in \\mathcal{G}_m} \\left| f(\\boldsymbol{x}) - \\mathcal{I} f(\\boldsymbol{x}) \\right|.\n$$\n\nTest suite:\n- Case A (happy path, smooth separable in three dimensions):\n  - $d = 3$.\n  - $f(\\boldsymbol{x}) = \\exp\\!\\left(0.5\\,x_1 - 0.3\\,x_2 + 0.2\\,x_3\\right)$.\n  - $\\tau = 10^{-3}$, $N_{\\max} = 500$.\n  - Validation grid parameter $m = 9$.\n- Case B (anisotropic localized Gaussian bump in two dimensions):\n  - $d = 2$.\n  - $f(\\boldsymbol{x}) = \\exp\\!\\left(-40 \\sum_{j=1}^2 (x_j - c_j)^2\\right)$ with $\\boldsymbol{c} = (0.2, 0.8)$.\n  - $\\tau = 5 \\times 10^{-4}$, $N_{\\max} = 600$.\n  - Validation grid parameter $m = 25$.\n- Case C (one-dimensional non-smooth absolute value kink):\n  - $d = 1$.\n  - $f(x) = |x - 0.3|$.\n  - $\\tau = 2 \\times 10^{-4}$, $N_{\\max} = 300$.\n  - Validation grid parameter $m = 200$.\n- Case D (moderate four-dimensional smooth function with mild oscillation):\n  - $d = 4$.\n  - $f(\\boldsymbol{x}) = \\prod_{j=1}^4 \\left(1 + 0.1\\,x_j\\right) + 0.01 \\sum_{j=1}^4 \\sin(2\\pi x_j)$, with angles in radians.\n  - $\\tau = 2 \\times 10^{-3}$, $N_{\\max} = 400$.\n  - Validation grid parameter $m = 7$.\n\nImplementation constraints and final output format:\n- Your program must be self-contained and must not read any input. It must implement the adaptive sparse grid interpolation algorithm described above and compute, for each test case, the pair $(N, E_{\\max})$, where $N$ is the number of grid points actually used at termination.\n- Your program should produce a single line of output containing all results in a flat, comma-separated Python list as\n  [N_A,E_A,N_B,E_B,N_C,E_C,N_D,E_D]\nwhere $N_\\cdot$ are integers and $E_\\cdot$ are floats. The error values must be reported as decimal numbers (not fractions), and angles, where present, must be interpreted in radians.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded, well-posed, and objective. It presents a clear task: to design and implement an adaptive sparse grid interpolation algorithm based on the Smolyak construction with a piecewise-linear hierarchical basis. The definitions, algorithmic steps, and test cases are specified with sufficient rigor to permit a unique and verifiable solution.\n\nThe objective is to construct an adaptive sparse grid interpolant for a function $f: [0,1]^d \\to \\mathbb{R}$. The domain is the $d$-dimensional hypercube $[0,1]^d$. The interpolation scheme is built upon a hierarchical basis of piecewise-linear \"hat\" functions.\n\nIn one dimension, for a given level $l \\in \\mathbb{N} = \\{1, 2, 3, \\dots\\}$, a set of nodal points is defined at $x_{l,i} = i / 2^l$ for odd indices $i \\in \\{1, 3, \\dots, 2^l - 1\\}$. Associated with each such node is a hat basis function:\n$$\n\\varphi_{l,i}(x) = \\max\\left(1 - 2^l \\left| x - x_{l,i} \\right|, 0\\right)\n$$\nThis basis is defined only on the interior of the interval $(0,1)$, which implies the resulting interpolant will be zero on the boundary.\n\nFor a $d$-dimensional problem, the basis functions are formed by a tensor product construction. A point in the hierarchical grid is identified by a multi-level $\\boldsymbol{l} = (l_1, \\dots, l_d) \\in \\mathbb{N}^d$ and a multi-index $\\boldsymbol{i} = (i_1, \\dots, i_d)$, where each component $i_j$ is an odd integer satisfying $1 \\le i_j \\le 2^{l_j}-1$. The corresponding grid node is $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}} = (x_{l_1,i_1}, \\dots, x_{l_d,i_d})$, and the multidimensional basis function is:\n$$\n\\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}) = \\prod_{j=1}^d \\varphi_{l_j,i_j}(x_j)\n$$\n\nThe sparse grid interpolant $\\mathcal{I}f(\\boldsymbol{x})$ is a linear combination of these basis functions for a selected set $\\mathcal{A}$ of multi-indices $(\\boldsymbol{l}, \\boldsymbol{i})$:\n$$\n\\mathcal{I} f(\\boldsymbol{x}) = \\sum_{(\\boldsymbol{l},\\boldsymbol{i}) \\in \\mathcal{A}} \\alpha_{\\boldsymbol{l},\\boldsymbol{i}} \\, \\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x})\n$$\nThe coefficients $\\alpha_{\\boldsymbol{l},\\boldsymbol{i}}$ are the hierarchical surpluses, defined recursively. For a new point $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}$ being added to the grid, its surplus is the difference between the true function value and the value of the interpolant constructed from all previously included, coarser points:\n$$\n\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}) - \\sum_{(\\boldsymbol{l}',\\boldsymbol{i}') \\in \\mathcal{A}_{\\text{prev}}} \\alpha_{\\boldsymbol{l}',\\boldsymbol{i}'} \\, \\Phi_{\\boldsymbol{l}',\\boldsymbol{i}'}(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}})\n$$\n\nThe core of the specified task is the adaptive nature of the algorithm. Adaptivity is driven by the magnitude of the hierarchical surpluses, which serve as an error indicator. The algorithm proceeds as follows:\n\n$1$. Initialization: The process begins with a single interior node at the coarsest level, specified by the multi-level $\\boldsymbol{l} = (1, \\dots, 1)$ and multi-index $\\boldsymbol{i} = (1, \\dots, 1)$. This corresponds to the point $\\boldsymbol{x} = (0.5, \\dots, 0.5)$. Its surplus is simply $\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}})$, as the initial interpolant is zero. This point, along with its surplus, is added to the grid and to a priority queue, which is ordered by the absolute magnitude of the surpluses.\n\n$2$. Iterative Refinement: The algorithm enters a loop that continues until a termination condition is met. In each iteration:\n   a. Selection: The grid point $(\\boldsymbol{l}_{\\text{p}}, \\boldsymbol{i}_{\\text{p}})$ with the largest absolute surplus $|\\alpha|$ is selected and removed from the priority queue. This is the parent point for refinement.\n   b. Refinement: The grid is refined by generating the admissible children of the parent point. For each dimension $j \\in \\{1, \\dots, d\\}$, two children are generated. A child's multi-level is derived from the parent's by incrementing the level in dimension $j$, i.e., $l'_j = l_j+1$ and $l'_k = l_k$ for $k \\neq j$. The corresponding index $i'_j$ for the new level $l'_j$ is given by $\\{2i_j-1, 2i_j+1\\}$, while $i'_k=i_k$ for $k \\neq j$.\n   c. Update: For each newly generated child point, its hierarchical surplus is calculated using the formula above, where the sum is over all points currently in the grid. The new point and its surplus are added to the grid data structure and inserted into the priority queue.\n\n$3$. Termination: The iterative process halts if either of two conditions is met:\n   a. The maximum absolute surplus in the priority queue falls below a specified tolerance $\\tau$.\n   b. The total number of points in the grid reaches a defined maximum, $N_{\\max}$.\n\nUpon termination, the algorithm has constructed a sparse grid and the corresponding interpolant $\\mathcal{I}f(\\boldsymbol{x})$. The final step is to assess its accuracy. This is done by calculating the maximum absolute error $E_{\\max}$ on a pre-defined validation grid $\\mathcal{G}_m$:\n$$\nE_{\\max} = \\max_{\\boldsymbol{x} \\in \\mathcal{G}_m} \\left| f(\\boldsymbol{x}) - \\mathcal{I} f(\\boldsymbol{x}) \\right|\n$$\nThe validation grid $\\mathcal{G}_m$ is the Cartesian product of $m$ equidistant points in $(0,1)$ for each dimension.\n\nThe implementation will consist of a main driver function that iterates through the specified test cases. For each case, a dedicated solver function will execute the adaptive algorithm. Key data structures will include a dictionary to store the grid points and their associated data (surpluses, coordinates), and a min-heap from Python's `heapq` module to serve as the priority queue. Helper functions will be implemented to evaluate the 1D and multidimensional basis functions, and the overall interpolant. The final output will be the number of points $N$ used in the grid and the computed maximum error $E_{\\max}$ for each test case.", "answer": "```python\nimport numpy as np\nimport heapq\nimport itertools\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the adaptive sparse grid solver for each.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    # Case A: Smooth separable function\n    def f_A(x):\n        return np.exp(0.5 * x[0] - 0.3 * x[1] + 0.2 * x[2])\n\n    # Case B: Anisotropic localized Gaussian bump\n    def f_B(x):\n        c = np.array([0.2, 0.8])\n        return np.exp(-40.0 * np.sum((x - c)**2))\n\n    # Case C: Non-smooth absolute value kink\n    def f_C(x):\n        return np.abs(x[0] - 0.3)\n\n    # Case D: Smooth function with mild oscillation\n    def f_D(x):\n        prod_term = np.prod(1.0 + 0.1 * x)\n        sin_term = 0.01 * np.sum(np.sin(2.0 * np.pi * x))\n        return prod_term + sin_term\n        \n    test_cases = [\n        {'d': 3, 'f': f_A, 'tau': 1e-3, 'n_max': 500, 'm': 9},\n        {'d': 2, 'f': f_B, 'tau': 5e-4, 'n_max': 600, 'm': 25},\n        {'d': 1, 'f': f_C, 'tau': 2e-4, 'n_max': 300, 'm': 200},\n        {'d': 4, 'f': f_D, 'tau': 2e-3, 'n_max': 400, 'm': 7}\n    ]\n\n    results = []\n    for case in test_cases:\n        N, E_max = solve_case(case['d'], case['f'], case['tau'], case['n_max'], case['m'])\n        results.extend([N, E_max])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_case(d, f, tau, n_max, m):\n    \"\"\"\n    Solves a single adaptive sparse grid interpolation problem.\n    \"\"\"\n    \n    # Memoization caches for basis function calculations\n    phi_1d_cache = {}\n    phi_multi_d_cache = {}\n\n    def phi_1d(x, l, i):\n        \"\"\"Evaluates the 1D hierarchical hat basis function.\"\"\"\n        cache_key = (x, l, i)\n        if cache_key in phi_1d_cache:\n            return phi_1d_cache[cache_key]\n\n        # Using 1 << l which is equivalent to 2**l for integers\n        val = max(0.0, 1.0 - abs((1 << l) * x - i))\n        phi_1d_cache[cache_key] = val\n        return val\n\n    def phi_multi_d(x_vec, l_vec, i_vec):\n        \"\"\"Evaluates the multidimensional tensor-product basis function.\"\"\"\n        cache_key = (tuple(x_vec), l_vec, i_vec)\n        if cache_key in phi_multi_d_cache:\n            return phi_multi_d_cache[cache_key]\n        \n        prod = 1.0\n        for j in range(d):\n            prod *= phi_1d(x_vec[j], l_vec[j], i_vec[j])\n        phi_multi_d_cache[cache_key] = prod\n        return prod\n\n    def evaluate_interpolant(x_vec, grid_points):\n        \"\"\"Evaluates the sparse grid interpolant at a point x_vec.\"\"\"\n        total = 0.0\n        for (l_vec, i_vec), data in grid_points.items():\n            alpha = data['surplus']\n            basis_val = phi_multi_d(x_vec, l_vec, i_vec)\n            total += alpha * basis_val\n        return total\n\n    grid_points = {}\n    priority_queue = []\n\n    # 1. Initialize with the first point\n    l0 = tuple([1] * d)\n    i0 = tuple([1] * d)\n    \n    x0 = tuple(i / (1 << l) for l, i in zip(l0, i0))\n    f_val = f(np.array(x0))\n    alpha0 = f_val  # I_prev is 0\n    \n    grid_points[(l0, i0)] = {'surplus': alpha0, 'coords': x0}\n    heapq.heappush(priority_queue, (-abs(alpha0), l0, i0))\n\n    # 2. Main adaptive loop\n    while priority_queue:\n        if len(grid_points) >= n_max:\n            break\n        \n        neg_abs_alpha_max, _, _ = priority_queue[0]\n        if -neg_abs_alpha_max < tau:\n            break\n\n        # Pop parent point with largest surplus\n        _, l_parent, i_parent = heapq.heappop(priority_queue)\n\n        # Generate and add children\n        for j in range(d):  # Dimension to refine\n            l_child_list = list(l_parent)\n            l_child_list[j] += 1\n            l_child = tuple(l_child_list)\n            \n            i_child_val_1 = 2 * i_parent[j] - 1\n            i_child_val_2 = 2 * i_parent[j] + 1\n            \n            for i_child_val in [i_child_val_1, i_child_val_2]:\n                if len(grid_points) >= n_max:\n                    break\n\n                i_child_list = list(i_parent)\n                i_child_list[j] = i_child_val\n                i_child = tuple(i_child_list)\n\n                if (l_child, i_child) in grid_points:\n                    continue\n                \n                # Calculate surplus for the new child point\n                x_child = tuple(i / (1 << l) for l, i in zip(l_child, i_child))\n                f_val_child = f(np.array(x_child))\n                \n                # Clear evaluation caches for new point evaluation\n                phi_1d_cache.clear()\n                phi_multi_d_cache.clear()\n                \n                I_prev_at_child = evaluate_interpolant(x_child, grid_points)\n                alpha_child = f_val_child - I_prev_at_child\n\n                # Add child to grid and priority queue\n                grid_points[(l_child, i_child)] = {'surplus': alpha_child, 'coords': x_child}\n                heapq.heappush(priority_queue, (-abs(alpha_child), l_child, i_child))\n            \n            if len(grid_points) >= n_max:\n                break\n    \n    # Final number of grid points used\n    N = len(grid_points)\n\n    # 3. Error calculation on validation grid\n    axis_pts = (np.arange(1, m + 1, dtype=float)) / (m + 1.0)\n    validation_grid = itertools.product(*([axis_pts] * d))\n    \n    max_error = 0.0\n    for x_val_tuple in validation_grid:\n        x_val = np.array(x_val_tuple)\n        f_true = f(x_val)\n        \n        phi_1d_cache.clear()\n        phi_multi_d_cache.clear()\n        \n        f_interp = evaluate_interpolant(x_val, grid_points)\n        max_error = max(max_error, abs(f_true - f_interp))\n            \n    return N, max_error\n\nif __name__ == \"__main__\":\n    solve()\n\n```"}]}