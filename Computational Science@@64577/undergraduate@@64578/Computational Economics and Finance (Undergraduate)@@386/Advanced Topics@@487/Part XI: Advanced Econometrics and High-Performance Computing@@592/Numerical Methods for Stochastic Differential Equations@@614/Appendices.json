{"hands_on_practices": [{"introduction": "The Euler-Maruyama scheme is the foundational method for numerically integrating Stochastic Differential Equations (SDEs). This exercise provides a valuable opportunity to directly quantify its performance by comparing it against an exact analytical solution, a luxury not always available in practice. By simulating the Ornstein-Uhlenbeck process with both methods, you will build concrete intuition about discretization error and learn to measure numerical efficiency by comparing the Mean-Squared Error of each method under a fixed computational budget [@problem_id:2415924].", "id": "2415924", "problem": "Consider the mean-reverting short-rate model widely used in computational economics and finance, the Ornstein–Uhlenbeck process (also called the Vasicek model), defined by the stochastic differential equation (SDE)\n$$\ndX_t = \\kappa(\\theta - X_t)\\,dt + \\sigma\\,dW_t,\\quad X_0 = x_0,\n$$\nwhere $X_t$ is the state variable (for instance, a short interest rate), $\\kappa &gt; 0$ is the mean-reversion speed, $\\theta$ is the long-run mean, $\\sigma &gt; 0$ is the diffusion coefficient, and $W_t$ is a standard Brownian motion. Your task is to demonstrate numerically, from first principles, that when estimating the expectation $E[X_T]$ at a fixed time $T$, sampling the exact solution at discrete times is more efficient than using the Euler–Maruyama scheme, when efficiency is measured by mean-squared error per unit of simulation cost under a fixed computational budget.\n\nWork from the following fundamental base:\n- The definition of an Itô stochastic differential equation and Itô integral for $dX_t = a(X_t,t)\\,dt + b(X_t,t)\\,dW_t$.\n- The Ornstein–Uhlenbeck process is a linear SDE; linear first-order SDEs can be solved using integrating factors.\n- The Euler–Maruyama scheme is obtained by time-discretizing the SDE using the definition of the Itô integral over a small interval $[t_n,t_{n+1}]$ with step size $\\Delta$ and approximating the drift and diffusion at the left endpoint.\n\nTasks:\n1) Derive, using an integrating factor, the exact discrete-time transition of the Ornstein–Uhlenbeck process over a single step of size $\\Delta$, that is, obtain an expression for $X_{t+\\Delta}$ given $X_t$ as an affine function of $X_t$ plus a Gaussian innovation. Express its mean and variance as functions of $\\kappa$, $\\theta$, $\\sigma$, and $\\Delta$.\n\n2) Derive the Euler–Maruyama update for one step of size $\\Delta$ for this SDE by applying the definition of the Itô integral over $[t_n,t_{n+1}]$ and approximating the drift and diffusion at $t_n$.\n\n3) For a fixed terminal time $T$, define a computational budget measured by the total number of Gaussian random variables generated:\n$$\nC = M \\times N,\n$$\nwhere $N = T/\\Delta$ is the number of time steps per path (assume $T/\\Delta$ is an integer) and $M$ is the number of simulated paths. For a fair comparison at fixed cost, you must hold $C$ constant while varying $\\Delta$, setting $M = \\lfloor C/N \\rfloor$.\n\n4) For each parameter configuration below, and for each time step $\\Delta$ given for that configuration, simulate $M$ independent paths up to time $T$ using:\n- The exact discrete-time transition you derived in Task $1$ (exact sampling).\n- The Euler–Maruyama scheme you derived in Task $2$ (Euler scheme).\n\nUse the same Gaussian innovations for both methods at each step and for each path to ensure a fair, variance-reduced comparison. For each configuration and $\\Delta$, estimate the mean-squared error (MSE) of the Monte Carlo estimator of $E[X_T]$ by\n$$\n\\widehat{\\mathrm{MSE}} = \\left(\\overline{X}_T - \\mu_T\\right)^2 + \\frac{S_T^2}{M},\n$$\nwhere $\\overline{X}_T$ is the sample mean of $X_T$ across $M$ paths, $S_T^2$ is the sample variance of $X_T$ across $M$ paths, and $\\mu_T = E[X_T]$ is the exact mean implied by the exact solution of the SDE at time $T$. Report, for each configuration and $\\Delta$, the ratio\n$$\nR = \\frac{\\widehat{\\mathrm{MSE}}_{\\mathrm{Euler}}}{\\widehat{\\mathrm{MSE}}_{\\mathrm{Exact}}},\n$$\nat the same computational budget $C$.\n\n5) Randomness control and fairness:\n- For each configuration index $i \\in \\{0,1,2\\}$, use a base integer seed $s_i$. For the $j$-th step size in that configuration, use seed $s_i + j$ to generate a matrix of independent standard normal random variables $Z \\in \\mathbb{R}^{M \\times N}$. Reuse the same $Z$ for both the exact and Euler methods within that configuration and step size.\n\nImplementation and numerical specifications:\n- No physical units are involved in this problem.\n- Angles are not involved in this problem.\n- Percentages are not involved; any fractional quantities must be expressed as decimals or fractions.\n- The final program must be a single, complete, runnable program that performs all computations and produces the specified output with no user input.\n\nTest suite of parameter configurations:\n- Configuration $0$: $\\kappa = 1.2$, $\\theta = 0.04$, $\\sigma = 0.25$, $x_0 = 0.03$, $T = 1.0$, step sizes $\\Delta \\in \\{0.2, 0.05, 0.01\\}$, base seed $s_0 = 13579$.\n- Configuration $1$: $\\kappa = 3.0$, $\\theta = 0.02$, $\\sigma = 0.15$, $x_0 = 0.08$, $T = 0.5$, step sizes $\\Delta \\in \\{0.1, 0.05, 0.01\\}$, base seed $s_1 = 24680$.\n- Configuration $2$: $\\kappa = 0.3$, $\\theta = 0.01$, $\\sigma = 0.05$, $x_0 = 0.00$, $T = 2.0$, step sizes $\\Delta \\in \\{0.4, 0.1, 0.02\\}$, base seed $s_2 = 11223$.\n\nIn all configurations, set the computational budget to the same value $C = 2{,}000{,}000$, and set $M = \\lfloor C/N \\rfloor$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the $9$ ratios $R$ (three per configuration, in the order of configurations and step sizes as listed above) as a comma-separated list enclosed in square brackets, with each ratio rounded to $6$ decimal places. For example, an output with three ratios would look like $[1.234000,1.050000,1.002345]$.", "solution": "The problem as stated is valid. It is scientifically grounded in the established theory of stochastic differential equations and numerical analysis, it is well-posed with all necessary parameters and conditions provided, and it is objective, relying on precise mathematical definitions. I will proceed with the solution.\n\nThe problem requires a numerical comparison between two methods for simulating the Ornstein-Uhlenbeck process, which is described by the stochastic differential equation (SDE):\n$$ dX_t = \\kappa(\\theta - X_t)\\,dt + \\sigma\\,dW_t, \\quad X_0 = x_0 $$\nwhere $\\kappa > 0$, $\\theta$, and $\\sigma > 0$ are constants. This is a linear SDE.\n\nFirst, I will derive the required simulation schemes.\n\n**1. Derivation of the Exact Discrete-Time Transition**\n\nThe SDE can be rewritten as:\n$$ dX_t + \\kappa X_t dt = \\kappa \\theta dt + \\sigma dW_t $$\nThis is a first-order linear differential equation. We employ an integrating factor, $I(t) = e^{\\int \\kappa dt} = e^{\\kappa t}$. Multiplying the SDE by $e^{\\kappa t}$ gives:\n$$ e^{\\kappa t} dX_t + \\kappa e^{\\kappa t} X_t dt = \\kappa \\theta e^{\\kappa t} dt + \\sigma e^{\\kappa t} dW_t $$\nBy the Itô product rule, the left-hand side is the differential of $e^{\\kappa t} X_t$:\n$$ d(e^{\\kappa t} X_t) = \\kappa \\theta e^{\\kappa t} dt + \\sigma e^{\\kappa t} dW_t $$\nWe integrate this expression from an arbitrary time $t$ to $t+\\Delta$:\n$$ \\int_t^{t+\\Delta} d(e^{\\kappa s} X_s) = \\int_t^{t+\\Delta} \\kappa \\theta e^{\\kappa s} ds + \\int_t^{t+\\Delta} \\sigma e^{\\kappa s} dW_s $$\n$$ e^{\\kappa (t+\\Delta)} X_{t+\\Delta} - e^{\\kappa t} X_t = \\kappa \\theta \\left[ \\frac{e^{\\kappa s}}{\\kappa} \\right]_t^{t+\\Delta} + \\sigma \\int_t^{t+\\Delta} e^{\\kappa s} dW_s $$\n$$ e^{\\kappa (t+\\Delta)} X_{t+\\Delta} = e^{\\kappa t} X_t + \\theta (e^{\\kappa (t+\\Delta)} - e^{\\kappa t}) + \\sigma \\int_t^{t+\\Delta} e^{\\kappa s} dW_s $$\nMultiplying by $e^{-\\kappa(t+\\Delta)}$, we isolate $X_{t+\\Delta}$:\n$$ X_{t+\\Delta} = X_t e^{-\\kappa\\Delta} + \\theta (1 - e^{-\\kappa\\Delta}) + \\sigma \\int_t^{t+\\Delta} e^{-\\kappa(t+\\Delta - s)} dW_s $$\nThe stochastic integral term is an Itô integral of a deterministic function, which results in a normally distributed random variable with mean zero. Its variance is calculated using the Itô isometry:\n$$ \\mathrm{Var}\\left( \\int_t^{t+\\Delta} e^{-\\kappa(t+\\Delta - s)} dW_s \\right) = \\int_t^{t+\\Delta} \\left( e^{-\\kappa(t+\\Delta - s)} \\right)^2 ds = \\int_t^{t+\\Delta} e^{-2\\kappa(t+\\Delta - s)} ds $$\nWith a change of variable $u = s - t$, this becomes:\n$$ \\int_0^{\\Delta} e^{-2\\kappa(\\Delta - u)} du = e^{-2\\kappa\\Delta} \\int_0^{\\Delta} e^{2\\kappa u} du = e^{-2\\kappa\\Delta} \\left[ \\frac{e^{2\\kappa u}}{2\\kappa} \\right]_0^{\\Delta} = \\frac{1 - e^{-2\\kappa\\Delta}}{2\\kappa} $$\nTherefore, the stochastic integral can be represented as $\\sqrt{\\frac{1 - e^{-2\\kappa\\Delta}}{2\\kappa}} Z_{t+\\Delta}$, where $Z_{t+\\Delta} \\sim \\mathcal{N}(0, 1)$.\nThe exact discrete-time transition rule is:\n$$ X_{t+\\Delta} = X_t e^{-\\kappa\\Delta} + \\theta(1 - e^{-\\kappa\\Delta}) + \\sigma \\sqrt{\\frac{1 - e^{-2\\kappa\\Delta}}{2\\kappa}} Z_{t+\\Delta} $$\nGiven $X_t$, the conditional distribution of $X_{t+\\Delta}$ is Gaussian with:\n- Mean: $E[X_{t+\\Delta} | X_t] = X_t e^{-\\kappa\\Delta} + \\theta(1 - e^{-\\kappa\\Delta})$\n- Variance: $\\mathrm{Var}(X_{t+\\Delta} | X_t) = \\sigma^2 \\frac{1 - e^{-2\\kappa\\Delta}}{2\\kappa}$\n\n**2. Derivation of the Euler–Maruyama Update**\n\nThe Euler–Maruyama scheme approximates the SDE by discretizing its integral form over a small time step $\\Delta = t_{n+1} - t_n$.\n$$ X_{t_{n+1}} - X_{t_n} = \\int_{t_n}^{t_{n+1}} \\kappa(\\theta - X_s) ds + \\int_{t_n}^{t_{n+1}} \\sigma dW_s $$\nThe scheme approximates the integrands as constant over the interval $[t_n, t_{n+1}]$, evaluated at the left endpoint $t_n$:\n$$ \\int_{t_n}^{t_{n+1}} \\kappa(\\theta - X_s) ds \\approx \\kappa(\\theta - X_{t_n})\\Delta $$\n$$ \\int_{t_n}^{t_{n+1}} \\sigma dW_s = \\sigma (W_{t_{n+1}} - W_{t_n}) = \\sigma \\sqrt{\\Delta} Z_{n+1} $$\nwhere $Z_{n+1} \\sim \\mathcal{N}(0, 1)$. Combining these gives the update rule, where we denote $X_n \\equiv X_{t_n}$:\n$$ X_{n+1} = X_n + \\kappa(\\theta - X_n)\\Delta + \\sigma\\sqrt{\\Delta}Z_{n+1} $$\nThis is the required Euler-Maruyama discretization for the Ornstein-Uhlenbeck process.\n\n**3. Numerical Experiment Design and Evaluation**\n\nThe objective is to compare the efficiency of the exact sampling method and the Euler-Maruyama scheme. Efficiency is measured by the mean-squared error (MSE) of the Monte Carlo estimator for $E[X_T]$ under a fixed computational budget $C$.\n\nThe computational budget is defined as $C = M \\times N$, where $M$ is the number of simulated paths and $N$ is the number of time steps per path. We are given $T$ and a step size $\\Delta$, so $N = T/\\Delta$. The number of paths is then $M = \\lfloor C/N \\rfloor$. This setup ensures that for any choice of $\\Delta$, the total number of generated Gaussian random numbers is approximately constant.\n\nFor each simulation run (defined by a set of parameters and a step size $\\Delta$), we perform the following:\n1.  Generate an $M \\times N$ matrix of independent standard normal deviates, $Z$. This matrix is used for both the exact and Euler simulations to ensure a fair comparison with reduced variance.\n2.  Simulate $M$ paths from $t=0$ to $t=T$ using both the exact transition and the Euler-Maruyama scheme, starting from $X_0 = x_0$.\n3.  For each method, this produces a vector of $M$ final states at time $T$. We compute the sample mean $\\overline{X}_T$ and sample variance $S_T^2$.\n4.  The true analytical mean of the process at time $T$ is obtained by solving the ODE for the expectation, $d\\mu_t/dt = \\kappa(\\theta - \\mu_t)$, which yields $\\mu_T = E[X_T] = \\theta + (x_0 - \\theta)e^{-\\kappa T}$.\n5.  The estimated MSE for each method is calculated using the prescribed formula:\n    $$ \\widehat{\\mathrm{MSE}} = \\left(\\overline{X}_T - \\mu_T\\right)^2 + \\frac{S_T^2}{M} $$\n    The term $(\\overline{X}_T - \\mu_T)^2$ captures the squared error of the sample mean, which is composed of both bias and statistical fluctuation. The term $S_T^2/M$ is an estimate of the variance of the sample mean.\n6.  Finally, we compute the ratio of the estimated MSEs:\n    $$ R = \\frac{\\widehat{\\mathrm{MSE}}_{\\mathrm{Euler}}}{\\widehat{\\mathrm{MSE}}_{\\mathrm{Exact}}} $$\nA ratio $R > 1$ indicates that the Euler-Maruyama scheme is less efficient (produces a higher MSE for the same computational cost) than the exact sampling method. The weak convergence error (bias) of the Euler method, which grows with $\\Delta$, is expected to be the primary driver of this difference in efficiency. The exact sampler is, by definition, free of this bias.\n\nThe following program implements this entire procedure for the specified test configurations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating the Ornstein-Uhlenbeck process using\n    the exact method and the Euler-Maruyama scheme, and computes the ratio\n    of their Mean-Squared Errors.\n    \"\"\"\n\n    # Test suite of parameter configurations.\n    test_cases = [\n        # Configuration 0\n        {\n            \"params\": {\"kappa\": 1.2, \"theta\": 0.04, \"sigma\": 0.25, \"x0\": 0.03, \"T\": 1.0},\n            \"deltas\": [0.2, 0.05, 0.01],\n            \"base_seed\": 13579\n        },\n        # Configuration 1\n        {\n            \"params\": {\"kappa\": 3.0, \"theta\": 0.02, \"sigma\": 0.15, \"x0\": 0.08, \"T\": 0.5},\n            \"deltas\": [0.1, 0.05, 0.01],\n            \"base_seed\": 24680\n        },\n        # Configuration 2\n        {\n            \"params\": {\"kappa\": 0.3, \"theta\": 0.01, \"sigma\": 0.05, \"x0\": 0.00, \"T\": 2.0},\n            \"deltas\": [0.4, 0.1, 0.02],\n            \"base_seed\": 11223\n        }\n    ]\n\n    C = 2_000_000\n    results = []\n\n    for case_config in test_cases:\n        p = case_config[\"params\"]\n        kappa, theta, sigma, x0, T = p[\"kappa\"], p[\"theta\"], p[\"sigma\"], p[\"x0\"], p[\"T\"]\n        \n        # Calculate the true mean at time T for MSE calculation.\n        mu_T = theta + (x0 - theta) * np.exp(-kappa * T)\n\n        for j, delta in enumerate(case_config[\"deltas\"]):\n            # Set up simulation parameters\n            N = int(T / delta)\n            M = C // N\n            seed = case_config[\"base_seed\"] + j\n            \n            # Generate all standard normal random variables at once\n            rng = np.random.default_rng(seed)\n            Z = rng.standard_normal(size=(M, N))\n\n            # --- Exact Simulation ---\n            X_exact = np.full(M, x0, dtype=np.float64)\n            # Precompute constants for the exact update rule\n            e_neg_kd = np.exp(-kappa * delta)\n            var_term = sigma * np.sqrt((1 - np.exp(-2 * kappa * delta)) / (2 * kappa))\n            for i in range(N):\n                X_exact = X_exact * e_neg_kd + theta * (1 - e_neg_kd) + var_term * Z[:, i]\n\n            # --- Euler-Maruyama Simulation ---\n            X_euler = np.full(M, x0, dtype=np.float64)\n            # Precompute constant for the Euler update rule\n            sqrt_delta = np.sqrt(delta)\n            for i in range(N):\n                X_euler = X_euler + kappa * (theta - X_euler) * delta + sigma * sqrt_delta * Z[:, i]\n            \n            # --- Calculate MSE for both methods ---\n            # MSE for Exact method\n            mean_exact = np.mean(X_exact)\n            var_exact = np.var(X_exact, ddof=1) if M > 1 else 0.0\n            mse_exact = (mean_exact - mu_T)**2 + var_exact / M\n\n            # MSE for Euler method\n            mean_euler = np.mean(X_euler)\n            var_euler = np.var(X_euler, ddof=1) if M > 1 else 0.0\n            mse_euler = (mean_euler - mu_T)**2 + var_euler / M\n\n            # Avoid division by zero, although MSE_exact should be positive\n            if mse_exact > 0:\n                ratio = mse_euler / mse_exact\n            else:\n                # If exact MSE is zero, Euler MSE must also be zero for a ratio of 1\n                ratio = 1.0 if mse_euler == 0 else np.inf\n            \n            results.append(f\"{ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the simulation and print the results.\nsolve()\n```"}, {"introduction": "In most real-world applications, we must choose a numerical scheme's parameters without an exact solution for comparison. This practice addresses a core challenge in computational finance: selecting the optimal time step, $\\Delta t$, for a Monte Carlo simulation under a fixed computational budget. You will explore the fundamental trade-off between discretization bias (which decreases with smaller $\\Delta t$) and statistical variance (which increases as smaller $\\Delta t$ reduces the number of simulation paths) [@problem_id:2415926]. This exercise in minimizing the Mean-Squared Error will equip you with a principled approach to optimizing numerical methods for efficiency and accuracy.", "id": "2415926", "problem": "Design and implement a complete program that performs a principled, simulation-based selection of the optimal time step $\\Delta t$ for a Monte Carlo estimator of a European call option price under a Geometric Brownian Motion (GBM) model. The method must quantify and balance the trade-off that decreasing $\\Delta t$ reduces discretization bias but increases computational cost for a fixed budget.\n\nYou must work within the following rigorous specification.\n\nModel and estimator definition:\n- Asset dynamics under the risk-neutral measure are given by the stochastic differential equation (SDE) $dS_t = r S_t \\, dt + \\sigma S_t \\, dW_t$ with initial condition $S_0 &gt; 0$, constant risk-free rate $r \\ge 0$, and constant volatility $\\sigma \\ge 0$ over horizon $[0,T]$. The process $W_t$ is a standard Wiener process.\n- The target quantity is the arbitrage-free price of a European call option with strike $K$, maturity $T$, and payoff $f(S_T) = \\max(S_T - K, 0)$, discounted at the risk-free rate. The exact reference price is the classical Black–Scholes closed-form value, which you must compute to serve as ground truth.\n- The numerical estimator must use the Euler–Maruyama scheme to discretize the SDE with a uniform grid of $N$ steps on $[0,T]$ where $N = T / \\Delta t$ is an integer, and time step $\\Delta t = T/N$. One Euler–Maruyama step from $S_k$ to $S_{k+1}$ is $S_{k+1} = S_k + r S_k \\Delta t + \\sigma S_k \\sqrt{\\Delta t} Z_k$ with $Z_k \\sim \\mathcal{N}(0,1)$ independent and identically distributed. For each simulation path, the discounted payoff is $\\exp(-rT)\\max(S_N - K, 0)$, and the Monte Carlo estimator is the sample mean across $M$ independent paths.\n\nCost model and budget constraint:\n- The computational cost is defined as the total number of Euler–Maruyama time steps simulated across all paths, which equals $M \\times N$ for a given choice of $N$. For each test case, you are given a hard budget $C_{\\max}$ (in Euler steps). You must enforce the constraint $M \\times N \\le C_{\\max}$ by choosing $M = \\max\\{1, \\lfloor C_{\\max}/N \\rfloor\\}$ for any candidate $N$.\n\nError metric and empirical risk minimization:\n- For a fixed $\\Delta t$ (equivalently $N$) and its implied $M$, define a single-run estimator $\\hat{V}$ as the Monte Carlo sample mean of the discounted payoff using $M$ paths.\n- Use the exact Black–Scholes value $V^\\star$ as the ground truth. For a single run, define the squared error as $(\\hat{V} - V^\\star)^2$.\n- To reduce randomness when comparing different $\\Delta t$, estimate the mean squared error (MSE) for a given $\\Delta t$ by averaging the squared error over $R$ independent replications, each with fresh randomness but the same $M$ and $N$. That is, for each candidate $\\Delta t$, compute $\\widehat{\\mathrm{MSE}}(\\Delta t) = \\frac{1}{R}\\sum_{i=1}^R (\\hat{V}^{(i)} - V^\\star)^2$.\n- The optimal time step is the $\\Delta t$ that minimizes $\\widehat{\\mathrm{MSE}}(\\Delta t)$ over a provided finite candidate set. In case of a numerical tie (equal values within absolute tolerance $\\varepsilon = 10^{-12}$), select the largest $\\Delta t$ among the minimizers.\n\nImplementation constraints:\n- Use only the Euler–Maruyama scheme; do not use exact discretization of GBM for simulation.\n- Use independent standard normal draws across time and paths within each replication; use independent replications when averaging.\n- For each test case, evaluate a prescribed finite set of candidate steps $\\{\\Delta t_j\\}$, each constructed as $\\Delta t_j = T/N_j$ with $N_j \\in \\mathbb{N}$ provided below.\n\nTest suite:\n- For each test case, return the chosen $\\Delta t$ that minimizes the estimated MSE under the budget, as a floating-point number in years.\n- Use exactly $R = 12$ replications per candidate.\n- For numerical stability and reproducibility, you must fix all random number generator seeds deterministically in a way that does not depend on library or platform specifics beyond the language and libraries specified in the execution environment.\n\nParameters and candidate sets:\n- Case A (happy path):\n  - $S_0 = 100$, $K = 100$, $r = 0.05$, $\\sigma = 0.20$, $T = 1.0$, $C_{\\max} = 100{,}000$.\n  - Candidate numbers of steps $N \\in \\{8, 16, 32, 64, 128\\}$, hence $\\Delta t \\in \\{T/8, T/16, T/32, T/64, T/128\\}$.\n- Case B (low volatility and shorter maturity):\n  - $S_0 = 100$, $K = 100$, $r = 0.02$, $\\sigma = 0.05$, $T = 0.5$, $C_{\\max} = 50{,}000$.\n  - Candidate numbers of steps $N \\in \\{4, 8, 16, 32, 64\\}$, hence $\\Delta t \\in \\{T/4, T/8, T/16, T/32, T/64\\}$.\n- Case C (high volatility and longer maturity):\n  - $S_0 = 100$, $K = 110$, $r = 0.03$, $\\sigma = 0.60$, $T = 2.0$, $C_{\\max} = 150{,}000$.\n  - Candidate numbers of steps $N \\in \\{8, 16, 32, 64, 128\\}$, hence $\\Delta t \\in \\{T/8, T/16, T/32, T/64, T/128\\}$.\n\nFinal output format:\n- Your program must output a single line containing a Python-style list with exactly three floating-point numbers corresponding to the chosen $\\Delta t$ (in years) for Case A, Case B, and Case C, in that order.\n- Each $\\Delta t$ must be rounded to exactly $6$ decimal places.\n- The output must have no additional text. For example, the syntactic form must be like $\\texttt{[a,b,c]}$.\n\nYour submission must be a complete, runnable program that performs all computations as specified and prints the required single-line output.", "solution": "The problem requires designing a procedure to select an optimal time step, $\\Delta t$, for the numerical simulation of a European call option price. The selection must be based on minimizing the mean squared error (MSE) of a Monte Carlo estimator under a fixed computational budget. This task embodies a fundamental trade-off in computational science: the balance between discretization error and statistical error.\n\nThe underlying financial model is the Geometric Brownian Motion (GBM) for a non-dividend-paying stock, described by the stochastic differential equation (SDE) under the risk-neutral measure $\\mathbb{Q}$:\n$$\ndS_t = r S_t \\, dt + \\sigma S_t \\, dW_t\n$$\nHere, $S_t$ is the asset price at time $t$, $r$ is the constant risk-free interest rate, $\\sigma$ is the constant volatility, and $W_t$ is a standard Wiener process under $\\mathbb{Q}$.\n\nThe target of our estimation is the price of a European call option at time $t=0$, given by the discounted expected payoff at maturity $T$:\n$$\nV(S_0, 0) = \\mathbb{E}^{\\mathbb{Q}}[\\exp(-rT) \\max(S_T - K, 0) | S_0]\n$$\nwhere $K$ is the strike price. This theoretical price, denoted $V^\\star$, serves as our ground truth and is given by the Black-Scholes formula:\n$$\nV^\\star = S_0 \\Phi(d_1) - K \\exp(-rT) \\Phi(d_2)\n$$\nwith\n$$\nd_1 = \\frac{\\ln(S_0/K) + (r + \\frac{1}{2}\\sigma^2)T}{\\sigma\\sqrt{T}} \\quad \\text{and} \\quad d_2 = d_1 - \\sigma\\sqrt{T}\n$$\nHere, $\\Phi(\\cdot)$ represents the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\n\nThe numerical estimator is constructed using the Euler-Maruyama scheme to discretize the SDE over a uniform time grid $0 = t_0 < t_1 < \\dots < t_N = T$, where $t_k = k \\Delta t$ and the time step is $\\Delta t = T/N$. The discrete-time evolution of the asset price is:\n$$\nS_{k+1} = S_k(1 + r \\Delta t + \\sigma \\sqrt{\\Delta t} Z_k)\n$$\nfor $k = 0, 1, \\dots, N-1$, where $Z_k$ are independent and identically distributed random variables drawn from $\\mathcal{N}(0,1)$.\n\nA single Monte Carlo simulation path generates a final asset price $S_N$. The price of the option is estimated by averaging the discounted payoffs over $M$ independent paths:\n$$\n\\hat{V} = \\frac{1}{M} \\sum_{j=1}^{M} \\exp(-rT) \\max(S_N^{(j)} - K, 0)\n$$\nThe total computational cost is defined as $C = M \\times N$. A fixed budget $C_{\\max}$ imposes the constraint $M \\times N \\le C_{\\max}$. For a chosen number of time steps $N$, the number of simulation paths $M$ is maximized under this budget: $M = \\max\\{1, \\lfloor C_{\\max}/N \\rfloor\\}$.\n\nThe core of the problem lies in the analysis of the Mean Squared Error (MSE) of the estimator $\\hat{V}$ with respect to the true value $V^\\star$:\n$$\n\\text{MSE}(\\Delta t) = \\mathbb{E}[(\\hat{V} - V^\\star)^2] = (\\mathbb{E}[\\hat{V}] - V^\\star)^2 + \\text{Var}(\\hat{V})\n$$\nThe MSE decomposes into two components:\n$1$. Squared Bias, $(\\mathbb{E}[\\hat{V}] - V^\\star)^2$: This term captures the systematic error introduced by the Euler-Maruyama discretization. The weak error of this scheme is of order $O(\\Delta t)$, so the squared bias is approximately proportional to $(\\Delta t)^2$. Increasing $N$ (decreasing $\\Delta t$) reduces this bias.\n$2$. Variance, $\\text{Var}(\\hat{V})$: This term captures the statistical error from the Monte Carlo sampling. It is given by $\\frac{1}{M} \\text{Var}(\\exp(-rT)\\max(S_N - K, 0))$. Since $M \\approx C_{\\max}/N = C_{\\max} \\Delta t / T$, the variance is approximately proportional to $1/M$, and thus proportional to $N$ or $1/\\Delta t$. Increasing $N$ (decreasing $\\Delta t$) requires a corresponding decrease in $M$ to stay within budget, which increases the variance.\n\nThis establishes the fundamental trade-off: decreasing $\\Delta t$ reduces discretization bias but increases statistical variance. The optimal $\\Delta t$ is the one that minimizes the total MSE.\n\nThe specified algorithm is an empirical risk minimization procedure. Since the true MSE is unknown, we estimate it. For each candidate time step $\\Delta t_j$ (corresponding to $N_j=T/\\Delta t_j$), we compute an empirical MSE, $\\widehat{\\mathrm{MSE}}(\\Delta t_j)$. This is done by performing $R$ independent replications of the entire Monte Carlo estimation process. For each replication $i \\in \\{1, \\dots, R\\}$, a new set of $M$ paths is simulated to produce an estimate $\\hat{V}^{(i)}$. The empirical MSE is then:\n$$\n\\widehat{\\mathrm{MSE}}(\\Delta t_j) = \\frac{1}{R} \\sum_{i=1}^{R} (\\hat{V}^{(i)} - V^\\star)^2\n$$\nThe value of $R$ is fixed at $12$. This averaging over $R$ replications provides a more stable estimate of the MSE for a given $\\Delta t_j$ than a single squared error measurement.\n\nThe final step is to select the optimal $\\Delta t$ from a given finite set of candidates $\\{\\Delta t_j\\}$. The algorithm iterates through each candidate, calculates its $\\widehat{\\mathrm{MSE}}$, and identifies the minimum $\\widehat{\\mathrm{MSE}}$ value achieved. The corresponding $\\Delta t_j$ is selected. A tie-breaking rule is specified: if multiple candidates yield an MSE value within a numerical tolerance of $\\varepsilon = 10^{-12}$ of the minimum, the one with the largest $\\Delta t$ (smallest $N$) is chosen. This is a sensible heuristic, as it favors the computationally cheaper option among equivalent performers.\n\nFor implementation, path simulations for all $M$ paths are performed simultaneously using vectorized operations in NumPy for superior performance. To ensure reproducibility, the random number generator is seeded deterministically. For each test case, a base seed is used. From this, unique and deterministic seeds are generated for each candidate $N$ and for each of the $R$ replications, ensuring that the randomness is controlled and independent across all distinct estimation tasks.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n#\n# MIT License\n#\n# Copyright (c) 2024, The Strict Russian Professor\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n\ndef black_scholes_call(S0, K, T, r, sigma):\n    \"\"\"\n    Computes the Black-Scholes price for a European call option.\n    \"\"\"\n    if sigma == 0:\n        return np.maximum(0.0, S0 * np.exp(r * T) - K) * np.exp(-r * T)\n    d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    price = S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n    return price\n\ndef vectorized_monte_carlo(S0, K, T, r, sigma, N, M, rng):\n    \"\"\"\n    Estimates European call option price using a vectorized Euler-Maruyama simulation.\n    \"\"\"\n    dt = T / N\n    S = np.full(M, S0, dtype=np.float64)\n\n    for _ in range(N):\n        Z = rng.standard_normal(size=M)\n        S = S * (1 + r * dt + sigma * np.sqrt(dt) * Z)\n\n    payoffs = np.maximum(S - K, 0)\n    discounted_mean_payoff = np.exp(-r * T) * np.mean(payoffs)\n    return discounted_mean_payoff\n\ndef estimate_mse(S0, K, T, r, sigma, C_max, N, R, V_star, base_seed):\n    \"\"\"\n    Estimates the Mean Squared Error for a given N by averaging over R replications.\n    \"\"\"\n    M = max(1, C_max // N)\n    squared_errors = []\n\n    for i in range(R):\n        seed = base_seed + i\n        rng = np.random.default_rng(seed)\n        V_hat = vectorized_monte_carlo(S0, K, T, r, sigma, N, M, rng)\n        sq_err = (V_hat - V_star)**2\n        squared_errors.append(sq_err)\n\n    return np.mean(squared_errors)\n\ndef find_optimal_dt(S0, K, T, r, sigma, C_max, N_candidates, case_base_seed):\n    \"\"\"\n    Finds the optimal time step dt by minimizing empirical MSE over a set of candidates.\n    \"\"\"\n    R = 12\n    V_star = black_scholes_call(S0, K, T, r, sigma)\n    \n    mse_results = []\n    \n    for i, N in enumerate(N_candidates):\n        # Assign a unique, deterministic block of seeds for each candidate N\n        N_base_seed = case_base_seed + i * R\n        \n        mse = estimate_mse(S0, K, T, r, sigma, C_max, N, R, V_star, N_base_seed)\n        dt = T / N\n        mse_results.append({'mse': mse, 'dt': dt})\n\n    min_mse = min(res['mse'] for res in mse_results)\n    \n    tolerance = 1e-12\n    \n    # Filter for candidates that are within the tolerance of the minimum MSE\n    minimizers = [res for res in mse_results if abs(res['mse'] - min_mse) < tolerance]\n    \n    # Tie-breaking: select the largest dt among the minimizers\n    optimal_dt = max(res['dt'] for res in minimizers)\n    \n    return optimal_dt\n\ndef solve():\n    \"\"\"\n    Main solver function to process test cases and print results.\n    \"\"\"\n    test_cases = [\n        {   # Case A\n            'params': {'S0': 100.0, 'K': 100.0, 'T': 1.0, 'r': 0.05, 'sigma': 0.20},\n            'C_max': 100000,\n            'N_candidates': [8, 16, 32, 64, 128],\n            'seed': 1234\n        },\n        {   # Case B\n            'params': {'S0': 100.0, 'K': 100.0, 'T': 0.5, 'r': 0.02, 'sigma': 0.05},\n            'C_max': 50000,\n            'N_candidates': [4, 8, 16, 32, 64],\n            'seed': 5678\n        },\n        {   # Case C\n            'params': {'S0': 100.0, 'K': 110.0, 'T': 2.0, 'r': 0.03, 'sigma': 0.60},\n            'C_max': 150000,\n            'N_candidates': [8, 16, 32, 64, 128],\n            'seed': 9101\n        }\n    ]\n\n    final_results = []\n    for case in test_cases:\n        optimal_dt = find_optimal_dt(\n            S0=case['params']['S0'],\n            K=case['params']['K'],\n            T=case['params']['T'],\n            r=case['params']['r'],\n            sigma=case['params']['sigma'],\n            C_max=case['C_max'],\n            N_candidates=case['N_candidates'],\n            case_base_seed=case['seed']\n        )\n        final_results.append(optimal_dt)\n        \n    print(f\"[{','.join(f'{res:.6f}' for res in final_results)}]\")\n\nsolve()\n```"}, {"introduction": "While our previous exercises used a fixed time step, sophisticated solvers often adapt their step size dynamically to improve efficiency. This advanced practice introduces the Milstein method, a higher-order scheme, and demonstrates how it can be paired with the Euler-Maruyama method to create an embedded error estimator. Based on this local error estimate, you will implement the logic to accept or reject a trial step and propose a new, more suitable step size for the next integration [@problem_id:3002532]. This exercise serves as a blueprint for building modern, adaptive SDE solvers that are both robust and computationally efficient.", "id": "3002532", "problem": "Consider a scalar Itō Stochastic Differential Equation (SDE) of the form $\\,\\mathrm{d}X_t = a(X_t)\\,\\mathrm{d}t + b(X_t)\\,\\mathrm{d}W_t\\,$ where $\\,W_t\\,$ is a standard Brownian motion. Starting from the definition of an Itō SDE, the Itō–Taylor expansion, and the properties of Brownian motion increments $\\,\\Delta W \\sim \\mathcal{N}(0,h)\\,$ over a time step $\\,h>0\\,$, design a blueprint for an embedded error estimator that compares one-step Euler–Maruyama (EM) and Milstein updates to produce an acceptance decision and a proposed next time step.\n\nYour program must implement the following conceptual pipeline for a single trial step:\n- Given functions $\\,a(x)\\,$, $\\,b(x)\\,$, and the derivative $\\,b'(x)\\,$ with respect to $\\,x\\,$, construct the one-step Euler–Maruyama update and the one-step Milstein update from the Itō–Taylor expansion principles appropriate to each method.\n- Define an error estimator as the absolute difference between the two one-step updates at the same state $\\,x\\,$ and the same increment $\\,\\Delta W\\,$.\n- Scale the error by a mixed absolute–relative tolerance to form a dimensionless error metric $\\,\\varepsilon\\,$:\n$$\n\\varepsilon \\;=\\; \\frac{\\lvert y_{\\text{Milstein}} - y_{\\text{EM}} \\rvert}{a_{\\text{tol}} + r_{\\text{tol}}\\,\\lvert y_{\\text{Milstein}} \\rvert},\n$$\nwhere $\\,a_{\\text{tol}}>0\\,$ is an absolute tolerance and $\\,r_{\\text{tol}}\\ge 0\\,$ is a relative tolerance.\n- Acceptance criterion: declare the trial step accepted if $\\,\\varepsilon \\le 1\\,$ and rejected otherwise.\n- Step-size adaptation: let $\\,p\\,$ be the strong convergence order of the accepted method used to advance the solution (for the scalar commutative-noise Milstein method, take $\\,p=1\\,$). Propose a new step size\n$$\nh_{\\text{new}} \\;=\\; h \\,\\cdot\\, \\operatorname{clip}\\!\\left(\\gamma \\,\\varepsilon^{-1/p},\\, \\mathrm{fac}_{\\min},\\, \\mathrm{fac}_{\\max}\\right),\n$$\nwhere $\\,\\gamma\\in(0,1)\\,$ is a safety factor and $\\,\\operatorname{clip}(u, L, U) = \\min\\{\\max\\{u,L\\},U\\}\\,$ bounds multiplicative changes by $\\,\\mathrm{fac}_{\\min}\\,$ and $\\,\\mathrm{fac}_{\\max}\\,$.\n\nImplement the above for the specific family $\\,a(x) = \\alpha x\\,$ and $\\,b(x) = \\beta x\\,$ with constants $\\,\\alpha\\,$ and $\\,\\beta\\,$. Use the following test suite, where each case specifies $\\,(\\alpha, \\beta, x, h, a_{\\text{tol}}, r_{\\text{tol}}, \\gamma, \\mathrm{fac}_{\\min}, \\mathrm{fac}_{\\max}, z)\\,$ and the Brownian increment is prescribed deterministically by $\\,\\Delta W = \\sqrt{h}\\,z\\,$:\n\n- Case 1 (happy path, moderate noise): $\\,(0.5,\\,0.2,\\,1.0,\\,0.05,\\,10^{-3},\\,10^{-2},\\,0.9,\\,0.2,\\,5.0,\\,0.1)\\,$.\n- Case 2 (edge case, large diffusion and increment): $\\,(0.1,\\,2.5,\\,1.0,\\,0.05,\\,10^{-3},\\,10^{-2},\\,0.9,\\,0.2,\\,5.0,\\,3.0)\\,$.\n- Case 3 (boundary case, very small state and step): $\\,(0.1,\\,0.8,\\,10^{-6},\\,10^{-4},\\,10^{-8},\\,10^{-1},\\,0.9,\\,0.2,\\,5.0,\\,0.3)\\,$.\n- Case 4 (stress case, large step with negative increment): $\\,(0.1,\\,1.0,\\,1.0,\\,0.5,\\,10^{-4},\\,10^{-3},\\,0.8,\\,0.2,\\,5.0,\\,-1.0)\\,$.\n\nYour program should, for each case, compute:\n- the acceptance decision encoded as $\\,1\\,$ if accepted and $\\,0\\,$ if rejected,\n- the proposed next step size $\\,h_{\\text{new}}\\,$,\n- the dimensionless error metric $\\,\\varepsilon\\,$.\n\nFinal output format: Your program should produce a single line of output containing a comma-separated list of four lists, one per test case, each inner list in the format $[ \\text{accept}, h_{\\text{new}}, \\varepsilon ]$. For example, a line of the form $[[1,0.0623,0.54],[0,0.01,2.3],[\\dots],[\\dots]]$.", "solution": "The problem presented is valid. It is scientifically grounded in the established theory of numerical methods for stochastic differential equations (SDEs), specifically the Itō-Taylor expansion and the resultant Euler-Maruyama and Milstein schemes. The problem is well-posed, providing all necessary parameters, functions, and a deterministic setup for the Wiener increment, ensuring a unique and verifiable solution for each test case. The language is objective and precise.\n\nWe are tasked with designing and implementing an adaptive step-size control mechanism for a scalar Itō SDE of the form:\n$$\n\\mathrm{d}X_t = a(X_t)\\,\\mathrm{d}t + b(X_t)\\,\\mathrm{d}W_t\n$$\nwhere $W_t$ is a standard Wiener process (Brownian motion). The core of the adaptive strategy lies in comparing a lower-order numerical approximation with a higher-order one to estimate the local error of a single step.\n\nThe foundation for both the Euler-Maruyama and Milstein methods is the Itō-Taylor expansion of the process $X_t$ around time $t_n$, which gives the value at $t_{n+1} = t_n + h$:\n$$\nX_{n+1} = X_n + a(X_n)h + b(X_n)\\Delta W_n + \\frac{1}{2} b(X_n)b'(X_n) \\left( (\\Delta W_n)^2 - h \\right) + \\mathcal{O}(h^{3/2})\n$$\nHere, $X_n \\equiv X_{t_n}$, $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is the Wiener increment over the interval $[t_n, t_{n+1}]$, which is a random variable distributed as $\\mathcal{N}(0, h)$. The notation $b'(x)$ denotes the derivative of $b(x)$ with respect to its argument $x$.\n\nFrom this expansion, we derive two numerical schemes of different strong convergence orders:\n\n1.  **Euler-Maruyama (EM) Scheme**: This method is obtained by truncating the Itō-Taylor expansion after the stochastic integral term. It has a strong convergence order of $0.5$. The one-step update, which we denote $y_{\\text{EM}}$, from a state $x$ with step size $h$ and increment $\\Delta W$ is:\n    $$\n    y_{\\text{EM}}(x, h, \\Delta W) = x + a(x)h + b(x)\\Delta W\n    $$\n\n2.  **Milstein Scheme**: By including the next term in the expansion, we obtain the Milstein method. For scalar SDEs (where noise is always commutative), this method has a strong convergence order of $1.0$. The one-step update, $y_{\\text{Milstein}}$, is:\n    $$\n    y_{\\text{Milstein}}(x, h, \\Delta W) = x + a(x)h + b(x)\\Delta W + \\frac{1}{2}b(x)b'(x)\\left( (\\Delta W)^2 - h \\right)\n    $$\n    We can express this succinctly as $y_{\\text{Milstein}} = y_{\\text{EM}} + \\frac{1}{2}b(x)b'(x)\\left( (\\Delta W)^2 - h \\right)$.\n\nThe problem specifies the SDE for a Geometric Brownian Motion:\n$$\n\\mathrm{d}X_t = \\alpha X_t\\,\\mathrm{d}t + \\beta X_t\\,\\mathrm{d}W_t\n$$\nFrom this, we identify the drift and diffusion coefficients and the derivative of the diffusion coefficient:\n$$\na(x) = \\alpha x, \\quad b(x) = \\beta x, \\quad b'(x) = \\beta\n$$\n\nThe adaptive control mechanism employs an embedded method, where the difference between the higher-order (Milstein) and lower-order (EM) updates serves as an estimate of the local error of the lower-order method. This error estimator, $E$, is:\n$$\nE = \\lvert y_{\\text{Milstein}} - y_{\\text{EM}} \\rvert = \\left\\lvert \\frac{1}{2}b(x)b'(x)\\left( (\\Delta W)^2 - h \\right) \\right\\rvert\n$$\nFor the specific SDE, this becomes:\n$$\nE = \\left\\lvert \\frac{1}{2}(\\beta x)(\\beta)\\left( (\\Delta W)^2 - h \\right) \\right\\rvert = \\left\\lvert \\frac{1}{2}\\beta^2 x \\left( (\\Delta W)^2 - h \\right) \\right\\rvert\n$$\nThe problem deterministically prescribes the Wiener increment as $\\Delta W = \\sqrt{h}\\,z$, where $z$ is a given value. Substituting this into the error expression yields a more direct formula:\n$$\nE = \\left\\lvert \\frac{1}{2}\\beta^2 x \\left( (\\sqrt{h}z)^2 - h \\right) \\right\\rvert = \\left\\lvert \\frac{1}{2}\\beta^2 x \\left( h z^2 - h \\right) \\right\\rvert = \\left\\lvert \\frac{1}{2}\\beta^2 x h (z^2-1) \\right\\rvert\n$$\nTo make an acceptance decision, this absolute error $E$ is scaled by a mixed absolute-relative tolerance criterion, forming a dimensionless error metric $\\varepsilon$:\n$$\n\\varepsilon = \\frac{E}{a_{\\text{tol}} + r_{\\text{tol}}\\,\\lvert y_{\\text{Milstein}} \\rvert}\n$$\nwhere $a_{\\text{tol}}$ is the absolute tolerance and $r_{\\text{tol}}$ is the relative tolerance. The solution is advanced using the more accurate Milstein update, $y_{\\text{Milstein}}$. A trial step is accepted if $\\varepsilon \\le 1$ and rejected otherwise.\n\nFinally, a new step size $h_{\\text{new}}$ is proposed based on the computed error $\\varepsilon$. The formula is derived from the principle that for a method of order $p$, the error behaves as $E \\propto h^p$. To achieve a target error, the step size should be adjusted by a factor proportional to $\\varepsilon^{-1/p}$. The specified formula is:\n$$\nh_{\\text{new}} = h \\cdot \\operatorname{clip}\\!\\left(\\gamma \\,\\varepsilon^{-1/p},\\, \\mathrm{fac}_{\\min},\\, \\mathrm{fac}_{\\max}\\right)\n$$\nHere, $p$ is the strong order of the accepted method (Milstein), so $p=1$. The safety factor $\\gamma$ prevents overly optimistic step-size increases, and $\\mathrm{fac}_{\\min}, \\mathrm{fac}_{\\max}$ bound the change to maintain stability. The $\\operatorname{clip}(u, L, U)$ function constrains the value $u$ to the interval $[L, U]$. In the special case where $\\varepsilon=0$, the term $\\varepsilon^{-1/p}$ is formally infinite; the standard interpretation in this context is to increase the step size by the maximum allowed factor, $\\mathrm{fac}_{\\max}$.\n\nThe algorithm for a single trial step is as follows:\n1.  Given parameters $(\\alpha, \\beta, x, h, a_{\\text{tol}}, r_{\\text{tol}}, \\gamma, \\mathrm{fac}_{\\min}, \\mathrm{fac}_{\\max}, z)$.\n2.  Calculate the Wiener increment: $\\Delta W = \\sqrt{h}\\,z$.\n3.  Calculate the error estimate directly: $E = |\\frac{1}{2}\\beta^2 x h (z^2-1)|$.\n4.  Calculate the higher-order update: $y_{\\text{Milstein}} = x + \\alpha x h + \\beta x \\Delta W + \\frac{1}{2}\\beta^2 x h (z^2-1)$.\n5.  Calculate the dimensionless error: $\\varepsilon = E / (a_{\\text{tol}} + r_{\\text{tol}}\\,|y_{\\text{Milstein}}|)$.\n6.  Determine acceptance: `accept` is $1$ if $\\varepsilon \\le 1$, and $0$ otherwise.\n7.  Calculate the new step size $h_{\\text{new}}$ using the adaptation formula with $p=1$. If $\\varepsilon=0$, the multiplicative factor is $\\mathrm{fac}_{\\max}$.\n\nThis procedure is systematically applied to each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements an adaptive step-size controller for a scalar SDE\n    based on an embedded Euler-Maruyama and Milstein method pair.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is (alpha, beta, x, h, a_tol, r_tol, gamma, fac_min, fac_max, z)\n    test_cases = [\n        (0.5, 0.2, 1.0, 0.05, 1e-3, 1e-2, 0.9, 0.2, 5.0, 0.1),\n        (0.1, 2.5, 1.0, 0.05, 1e-3, 1e-2, 0.9, 0.2, 5.0, 3.0),\n        (0.1, 0.8, 1e-6, 1e-4, 1e-8, 1e-1, 0.9, 0.2, 5.0, 0.3),\n        (0.1, 1.0, 1.0, 0.5, 1e-4, 1e-3, 0.8, 0.2, 5.0, -1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta, x, h, a_tol, r_tol, gamma, fac_min, fac_max, z = case\n\n        # The strong convergence order of the Milstein method (scalar commutative case)\n        p = 1.0\n\n        # Calculate the deterministic Wiener increment\n        delta_W = np.sqrt(h) * z\n\n        # The term that differentiates Milstein from Euler-Maruyama\n        # This is also the local error estimate before taking the absolute value.\n        # E = |milstein_term|\n        milstein_correction = 0.5 * beta**2 * x * (delta_W**2 - h)\n        # Note: delta_W**2 - h = (sqrt(h)*z)**2 - h = h*z**2 - h = h * (z**2 - 1)\n        # So, milstein_correction = 0.5 * beta**2 * x * h * (z**2 - 1)\n        \n        # Absolute error estimator\n        error_est = np.abs(milstein_correction)\n\n        # Calculate the one-step Euler-Maruyama update\n        y_em = x + alpha * x * h + beta * x * delta_W\n        \n        # Calculate the one-step Milstein update by adding the correction term\n        y_milstein = y_em + milstein_correction\n        \n        # Calculate the dimensionless error metric epsilon\n        # Handle the case where the denominator could be zero, though unlikely with a_tol > 0\n        tolerance_scale = a_tol + r_tol * np.abs(y_milstein)\n        if tolerance_scale == 0:\n            # If both error and tolerance are zero, error is controlled.\n            # If error is non-zero and tolerance is zero, error is infinite.\n            epsilon = 0.0 if error_est == 0.0 else np.inf\n        else:\n            epsilon = error_est / tolerance_scale\n\n        # Acceptance criterion\n        accept = 1 if epsilon <= 1.0 else 0\n\n        # Step-size adaptation\n        if epsilon == 0.0:\n            # If error is zero, increase step size by the maximum factor\n            scaling_factor = fac_max\n        else:\n            # Standard PI controller-based step size adaptation\n            raw_scaling_factor = gamma * (epsilon**(-1.0 / p))\n            scaling_factor = np.clip(raw_scaling_factor, fac_min, fac_max)\n        \n        h_new = h * scaling_factor\n\n        results.append([accept, h_new, epsilon])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation and join with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"}]}