{"hands_on_practices": [{"introduction": "The cornerstone of the instrumental variables method is the \"exclusion restriction,\" the assumption that an instrument affects the outcome only through its effect on the endogenous variable. This practice explores what happens when this critical assumption is violated, demonstrating that even a small correlation between the instrument $Z$ and the error term $u$ can lead to significant bias. By analytically deriving and then simulating the asymptotic bias of the IV estimator, you will gain a deep, practical understanding of why instrument exogeneity is the central pillar supporting causal inference with IV [@problem_id:2402354].", "id": "2402354", "problem": "Consider the scalar linear structural model in which an economic outcome is generated by a potentially endogenous regressor and an unobserved disturbance. The data generating process obeys the following system for each observation indexed by $i \\in \\{1,\\dots,N\\}$:\n1. Structural equation: $y_i = \\beta x_i + u_i$.\n2. First-stage equation for the endogenous regressor: $x_i = \\pi w_i + \\gamma u_i + \\varepsilon_i$.\n3. Candidate instrument constructed as a proxy that violates the exclusion restriction: $z_i = w_i + \\rho u_i + \\nu_i$.\n\nAll random variables are real-valued and have mean zero. The disturbances $u_i$, $\\varepsilon_i$, and $\\nu_i$ are mutually independent and independent of $w_i$. The instrument construction parameter $\\rho$ regulates the degree of correlation between $z_i$ and $u_i$, so that the moment condition $\\mathbb{E}[z_i u_i] = 0$ is violated whenever $|\\rho| > 0$. Assume that $(w_i,u_i,\\varepsilon_i,\\nu_i)$ are independent and identically distributed across $i$, with variances $\\operatorname{Var}(w_i) = \\sigma_w^2$, $\\operatorname{Var}(u_i) = \\sigma_u^2$, $\\operatorname{Var}(\\varepsilon_i) = \\sigma_\\varepsilon^2$, and $\\operatorname{Var}(\\nu_i) = \\sigma_\\nu^2$.\n\nYour task is to study the asymptotic bias that results when the parameter $\\beta$ is estimated using Two-Stage Least Squares (Two-Stage Least Squares (2SLS)) with the single faulty instrument $z_i$. Focus on the scalar-instrument, scalar-regressor case with no intercept term.\n\nFundamental base you may assume without proof:\n- If $\\{a_i\\}_{i=1}^N$ are independent and identically distributed with $\\mathbb{E}[a_i] = \\mu$, then the Law of Large Numbers implies $\\frac{1}{N}\\sum_{i=1}^N a_i \\to \\mu$ in probability as $N \\to \\infty$.\n- If $(a_i,b_i)$ are independent and identically distributed and square-integrable, then $\\frac{1}{N}\\sum_{i=1}^N a_i b_i \\to \\mathbb{E}[a_i b_i]$ in probability as $N \\to \\infty$.\n- The probability limit of a ratio of sample moments equals the ratio of the corresponding probability limits, provided the denominator converges to a nonzero limit.\n\nYour program must carry out the following steps:\n1. Using the above base, derive the probability limit of the single-instrument 2SLS estimator $\\hat{\\beta}_{\\text{IV}}$ as a function of population covariances, and from that, derive a closed-form expression for the asymptotic bias $B_\\infty = \\operatorname{plim}(\\hat{\\beta}_{\\text{IV}}) - \\beta$ in terms of the parameters $(\\rho,\\pi,\\gamma,\\sigma_u^2,\\sigma_w^2)$.\n2. Implement a stochastic simulation that, for a large sample size $N$, draws $w_i \\sim \\mathcal{N}(0,\\sigma_w^2)$, $u_i \\sim \\mathcal{N}(0,\\sigma_u^2)$, $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$, and $\\nu_i \\sim \\mathcal{N}(0,\\sigma_\\nu^2)$, all independent, constructs $x_i$, $y_i$, and $z_i$ according to the system above, and computes the single-instrument 2SLS estimator $\\hat{\\beta}_{\\text{IV}} = \\frac{\\sum_{i=1}^N z_i y_i}{\\sum_{i=1}^N z_i x_i}$. Use the same fixed random seed and the same $N$ across all test cases to approximate the magnitude of the asymptotic bias via the simulated absolute difference $|\\hat{\\beta}_{\\text{IV}} - \\beta|$. This simulation is required to validate numerically the direction and order of magnitude implied by your derivation.\n3. For each test case, also compute the analytical magnitude of the asymptotic bias implied by your derivation in Step $1$, i.e., $\\left|B_\\infty\\right|$ obtained from population covariances, using the given parameter values.\n\nTest suite. For all cases, use $\\sigma_u^2 = 1$, $\\sigma_w^2 = 1$, $\\sigma_\\varepsilon^2 = 1$, $\\sigma_\\nu^2 = 1$, $\\beta = 1$, sample size $N = 200000$, and a fixed random seed equal to $2025$. The parameter tuples $(\\pi,\\gamma,\\rho)$ vary by case as follows:\n- Case $1$: $(\\pi,\\gamma,\\rho) = (1.0, 0.5, 0.0)$.\n- Case $2$: $(\\pi,\\gamma,\\rho) = (1.0, 0.5, 0.2)$.\n- Case $3$: $(\\pi,\\gamma,\\rho) = (0.8, 0.4, -0.3)$.\n- Case $4$: $(\\pi,\\gamma,\\rho) = (0.1, 0.5, -0.05)$.\n- Case $5$: $(\\pi,\\gamma,\\rho) = (1.0, 0.3, 0.8)$.\n\nRequired final output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the analytical magnitude of the asymptotic bias $\\left|B_\\infty\\right|$ for the corresponding case in the order listed above. Express each number rounded to $6$ decimal places. For example, an output line of the form $[a_1,a_2,a_3,a_4,a_5]$ where each $a_j$ is a float rounded to $6$ decimal places.\n\nNo physical units are involved in this problem.", "solution": "The problem statement poses a standard question in econometric theory regarding the asymptotic properties of the Two-Stage Least Squares (2SLS) estimator when the exclusion restriction for the instrumental variable is violated. The problem is well-defined, scientifically sound, and internally consistent. It is a formal exercise in deriving and calculating the probability limit of an estimator. I will proceed with the derivation and subsequent calculation.\n\nThe model is defined by the following system of equations for each observation $i$:\n1. Structural equation: $y_i = \\beta x_i + u_i$\n2. First-stage equation: $x_i = \\pi w_i + \\gamma u_i + \\varepsilon_i$\n3. Instrument definition: $z_i = w_i + \\rho u_i + \\nu_i$\n\nThe random variables $w_i, u_i, \\varepsilon_i, \\nu_i$ are independent and identically distributed (i.i.d.), mutually independent, have mean zero, and respective variances $\\sigma_w^2, \\sigma_u^2, \\sigma_\\varepsilon^2, \\sigma_\\nu^2$. The parameter $\\beta$ is the true causal effect of $x_i$ on $y_i$. The regressor $x_i$ is endogenous because it is correlated with the structural error $u_i$ whenever $\\gamma \\neq 0$, since $\\mathbb{E}[x_i u_i] = \\mathbb{E}[(\\pi w_i + \\gamma u_i + \\varepsilon_i)u_i] = \\gamma \\sigma_u^2$. The instrument $z_i$ is invalid (not exogenous) whenever $\\rho \\neq 0$, as its correlation with the structural error is non-zero: $\\mathbb{E}[z_i u_i] = \\mathbb{E}[(w_i + \\rho u_i + \\nu_i)u_i] = \\rho \\sigma_u^2$.\n\nThe single-instrument, single-regressor 2SLS estimator for $\\beta$ without an intercept is given by:\n$$\n\\hat{\\beta}_{\\text{IV}} = \\frac{\\sum_{i=1}^N z_i y_i}{\\sum_{i=1}^N z_i x_i}\n$$\nTo find the asymptotic bias, we first derive the probability limit ($\\operatorname{plim}$) of $\\hat{\\beta}_{\\text{IV}}$ as the sample size $N \\to \\infty$. By the Law of Large Numbers and the continuous mapping theorem (specifically, for ratios), the probability limit of the estimator is the ratio of the probability limits of the sample moments in the numerator and denominator:\n$$\n\\operatorname{plim}_{N \\to \\infty} \\hat{\\beta}_{\\text{IV}} = \\frac{\\operatorname{plim}_{N \\to \\infty} \\frac{1}{N}\\sum_{i=1}^N z_i y_i}{\\operatorname{plim}_{N \\to \\infty} \\frac{1}{N}\\sum_{i=1}^N z_i x_i} = \\frac{\\mathbb{E}[z_i y_i]}{\\mathbb{E}[z_i x_i]}\n$$\nThe expectations $\\mathbb{E}[z_i y_i]$ and $\\mathbb{E}[z_i x_i]$ are equivalent to the covariances $\\operatorname{Cov}(z_i, y_i)$ and $\\operatorname{Cov}(z_i, x_i)$ respectively, since all primitive random variables have mean zero, which implies $\\mathbb{E}[x_i] = \\mathbb{E}[y_i] = \\mathbb{E}[z_i] = 0$.\n\nFirst, we compute the numerator, $\\mathbb{E}[z_i y_i]$. Substitute the structural equation $y_i = \\beta x_i + u_i$:\n$$\n\\mathbb{E}[z_i y_i] = \\mathbb{E}[z_i (\\beta x_i + u_i)] = \\beta \\mathbb{E}[z_i x_i] + \\mathbb{E}[z_i u_i]\n$$\nThis expression depends on two other moments. Let's derive them.\n\nThe first moment is the denominator term, $\\mathbb{E}[z_i x_i]$. Substitute the definitions of $z_i$ and $x_i$:\n$$\n\\mathbb{E}[z_i x_i] = \\mathbb{E}[(w_i + \\rho u_i + \\nu_i)(\\pi w_i + \\gamma u_i + \\varepsilon_i)]\n$$\nExpanding the product and applying the expectation operator, we use the mutual independence and mean-zero properties of $w_i, u_i, \\varepsilon_i, \\nu_i$. All cross-product terms like $\\mathbb{E}[w_i u_i]$, $\\mathbb{E}[w_i \\varepsilon_i]$, etc., are zero. The only non-zero terms are from the expectations of squared variables:\n$$\n\\mathbb{E}[z_i x_i] = \\pi \\mathbb{E}[w_i^2] + \\rho \\gamma \\mathbb{E}[u_i^2] = \\pi \\sigma_w^2 + \\rho \\gamma \\sigma_u^2\n$$\nThis is the covariance between the instrument and the endogenous regressor. For the instrument to be relevant, this covariance must be non-zero.\n\nThe second moment is $\\mathbb{E}[z_i u_i]$, which quantifies the endogeneity of the instrument:\n$$\n\\mathbb{E}[z_i u_i] = \\mathbb{E}[(w_i + \\rho u_i + \\nu_i)u_i]\n$$\nAgain, due to independence, $\\mathbb{E}[w_i u_i] = 0$ and $\\mathbb{E}[\\nu_i u_i] = 0$. The expression simplifies to:\n$$\n\\mathbb{E}[z_i u_i] = \\rho \\mathbb{E}[u_i^2] = \\rho \\sigma_u^2\n$$\nNow, substitute these moments back into the expression for $\\operatorname{plim} \\hat{\\beta}_{\\text{IV}}$:\n$$\n\\operatorname{plim} \\hat{\\beta}_{\\text{IV}} = \\frac{\\beta(\\pi \\sigma_w^2 + \\rho \\gamma \\sigma_u^2) + \\rho \\sigma_u^2}{\\pi \\sigma_w^2 + \\rho \\gamma \\sigma_u^2} = \\beta + \\frac{\\rho \\sigma_u^2}{\\pi \\sigma_w^2 + \\rho \\gamma \\sigma_u^2}\n$$\nThe asymptotic bias, $B_\\infty$, is defined as the difference between the probability limit of the estimator and the true parameter value $\\beta$:\n$$\nB_\\infty = \\operatorname{plim} \\hat{\\beta}_{\\text{IV}} - \\beta = \\frac{\\rho \\sigma_u^2}{\\pi \\sigma_w^2 + \\rho \\gamma \\sigma_u^2}\n$$\nThis is the closed-form expression for the asymptotic bias. Note that the bias is zero if $\\rho=0$, which corresponds to the case of a valid instrument that satisfies the exclusion restriction $\\mathbb{E}[z_i u_i] = 0$.\n\nThe provided problem specifies parameter values $\\sigma_u^2 = 1$ and $\\sigma_w^2 = 1$. The formula for the asymptotic bias simplifies to:\n$$\nB_\\infty = \\frac{\\rho}{\\pi + \\rho \\gamma}\n$$\nThe program will compute the absolute value of this quantity, $\\left|B_\\infty\\right|$, for each test case. As required, the implementation also contains a stochastic simulation which generates data according to the specified process and computes the estimator $\\hat{\\beta}_{\\text{IV}}$. This serves as a numerical verification of the derived analytical formula, as for a large sample size $N$, the simulated bias $\\hat{\\beta}_{\\text{IV}} - \\beta$ should be a close approximation to the asymptotic bias $B_\\infty$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and simulates the asymptotic bias of a 2SLS estimator with a faulty instrument.\n    \"\"\"\n    # Fixed parameters for all test cases\n    beta = 1.0\n    sigma_u2 = 1.0\n    sigma_w2 = 1.0\n    sigma_eps2 = 1.0\n    sigma_nu2 = 1.0\n    N = 200000\n    seed = 2025\n\n    # Parameter tuples (pi, gamma, rho) for each test case\n    test_cases = [\n        (1.0, 0.5, 0.0),\n        (1.0, 0.5, 0.2),\n        (0.8, 0.4, -0.3),\n        (0.1, 0.5, -0.05),\n        (1.0, 0.3, 0.8),\n    ]\n\n    # Generate base random shocks once using the fixed seed\n    # This ensures that the variation between test cases is due only to the parameters (pi, gamma, rho)\n    rng = np.random.default_rng(seed)\n    w = rng.normal(0, np.sqrt(sigma_w2), N)\n    u = rng.normal(0, np.sqrt(sigma_u2), N)\n    eps = rng.normal(0, np.sqrt(sigma_eps2), N)\n    nu = rng.normal(0, np.sqrt(sigma_nu2), N)\n    \n    analytical_results = []\n\n    for case in test_cases:\n        pi, gamma, rho = case\n\n        # Step 1: Analytical calculation of asymptotic bias\n        # B_inf = (rho * sigma_u^2) / (pi * sigma_w^2 + rho * gamma * sigma_u^2)\n        numerator = rho * sigma_u2\n        denominator = pi * sigma_w2 + rho * gamma * sigma_u2\n        \n        # The problem statement ensures the denominator is non-zero for all test cases.\n        if denominator == 0:\n            # This case represents an irrelevant instrument, where plim is undefined.\n            # Based on problem parameters, this should not be reached.\n            asymptotic_bias = float('nan') \n        else:\n            asymptotic_bias = numerator / denominator\n        \n        analytical_results.append(abs(asymptotic_bias))\n\n        # Step 2: Stochastic simulation for numerical validation (results not printed)\n        # This part of the code is required by the problem description to be implemented,\n        # but its output is not part of the final answer. It serves to verify the\n        # analytical derivation.\n        \n        # Construct model variables based on the current case's parameters\n        x = pi * w + gamma * u + eps\n        y = beta * x + u\n        z = w + rho * u + nu\n\n        # Compute the 2SLS estimator\n        sum_zy = np.sum(z * y)\n        sum_zx = np.sum(z * x)\n        \n        if sum_zx != 0:\n            beta_hat_iv = sum_zy / sum_zx\n            # The simulated bias for a large N should be close to the asymptotic bias\n            _simulated_bias = beta_hat_iv - beta # underscore to indicate it's not used in final output\n\n    # Format the final output as a comma-separated list of strings, rounded to 6 decimal places.\n    print(f\"[{','.join([f'{x:.6f}' for x in analytical_results])}]\")\n\nsolve()\n\n```"}, {"introduction": "After ensuring an instrument is plausibly exogenous, we must also verify it is \"relevant\"—strongly correlated with the endogenous regressor. An instrument that is only weakly related to the variable of interest, while technically valid, can produce highly unreliable estimates and tests with low statistical power. This Monte Carlo simulation investigates the consequences of using weak instruments, showing how the ability to detect a true effect (i.e., reject a false null hypothesis like $H_0: \\beta=0$) diminishes as instrument strength wanes [@problem_id:2402339]. This exercise highlights the practical importance of the first-stage $F$-statistic as a diagnostic tool and solidifies your intuition for why relevance is the second critical pillar of sound IV estimation.", "id": "2402339", "problem": "Construct a program that uses Monte Carlo simulation to quantify how weak instruments affect the empirical power of a test of the null hypothesis $H_0:\\ \\beta=0$ in a linear instrumental variables setting. Consider the following data-generating process for a single endogenous regressor and a single instrument, with no intercepts:\n- For each repetition and for each observation index $i \\in \\{1,\\dots,n\\}$, draw $z_i \\sim \\mathcal{N}(0,1)$.\n- Draw independent standard normal shocks $e_{1i} \\sim \\mathcal{N}(0,1)$ and $e_{2i} \\sim \\mathcal{N}(0,1)$, and construct the pair $(v_i,u_i)$ with correlation $\\rho$ by setting $v_i = e_{1i}$ and $u_i = \\rho\\,e_{1i} + \\sqrt{1-\\rho^2}\\,e_{2i}$.\n- Set $x_i = \\pi\\,z_i + v_i$ and $y_i = \\beta\\,x_i + u_i$.\n\nFor each simulated dataset of size $n$, compute:\n1. The just-identified instrumental variables estimator for $\\beta$ defined by the sample moment condition $\\frac{1}{n}\\sum_{i=1}^n z_i(y_i-\\beta x_i)=0$, namely\n$$\n\\hat{\\beta} = \\frac{\\sum_{i=1}^n z_i y_i}{\\sum_{i=1}^n z_i x_i}.\n$$\n2. The associated conventional large-sample standard error based on the scalar moment condition,\n$$\n\\widehat{V}(\\hat{\\beta}) \\;=\\; \\frac{\\widehat{S}}{n\\,\\widehat{Q}^2},\\quad \\widehat{S}=\\frac{1}{n}\\sum_{i=1}^n (z_i\\hat{u}_i)^2,\\quad \\widehat{Q}=\\frac{1}{n}\\sum_{i=1}^n z_i x_i,\\quad \\hat{u}_i = y_i - \\hat{\\beta} x_i,\n$$\nand the corresponding test statistic for $H_0:\\ \\beta=0$,\n$$\nT \\;=\\; \\frac{\\hat{\\beta}-0}{\\sqrt{\\widehat{V}(\\hat{\\beta})}}.\n$$\nUse a two-sided rejection rule based on the standard normal quantile $c_\\alpha$, rejecting $H_0$ if $|T|>c_\\alpha$, where $c_\\alpha$ is such that $\\Pr(|Z|>c_\\alpha)=\\alpha$ for $Z\\sim \\mathcal{N}(0,1)$.\n\n3. The first-stage $F$-statistic for the relevance of the instrument in the regression of $x_i$ on $z_i$ (no intercept), computed from the ordinary least squares slope $\\hat{a} = \\frac{\\sum_{i=1}^n z_i x_i}{\\sum_{i=1}^n z_i^2}$, with residuals $r_i = x_i - \\hat{a} z_i$, residual variance $\\hat{\\sigma}^2 = \\frac{1}{n-1}\\sum_{i=1}^n r_i^2$, standard error $\\operatorname{se}(\\hat{a})=\\sqrt{\\hat{\\sigma}^2 / \\sum_{i=1}^n z_i^2}$, the corresponding $t$-statistic $t_1=\\hat{a}/\\operatorname{se}(\\hat{a})$, and $F = t_1^2$.\n\nFor each parameter set below, repeat the above steps for $R$ independent repetitions and report:\n- The empirical power defined as the fraction of repetitions in which the null is rejected at significance level $\\alpha$ when the true $\\beta$ is not equal to $0$.\n- The average first-stage $F$-statistic across the $R$ repetitions.\n\nUse the following test suite of parameter values, each specified as a tuple $(n,\\pi,\\rho,\\beta,\\alpha,R,\\text{seed})$:\n- Case A (strong instrument, larger sample): $(500,\\,0.5,\\,0.6,\\,1.0,\\,0.05,\\,1000,\\,123)$.\n- Case B (weak instrument, larger sample): $(500,\\,0.05,\\,0.6,\\,1.0,\\,0.05,\\,1000,\\,456)$.\n- Case C (moderate instrument, smaller sample): $(100,\\,0.35,\\,0.6,\\,1.0,\\,0.05,\\,1000,\\,789)$.\n\nYour program must:\n- Implement the simulation exactly as described.\n- For each case, compute the empirical power and the mean first-stage $F$-statistic across the $R$ repetitions.\n- Round each reported value to three decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a two-element list in the order $[\\text{power}, \\text{average F}]$. For example, a valid output with three cases would look like $[[0.842,12.531],[0.121,1.472],[0.563,6.214]]$.\n\nThere are no physical units involved. All numeric answers must be pure numbers. Angles are not involved. Percentages must be expressed as decimals, not with a percentage sign.", "solution": "The problem statement is validated as scientifically sound, well-posed, and objective. It presents a standard exercise in computational econometrics to analyze the consequences of weak instruments on the statistical power of hypothesis tests. The solution is constructed via a Monte Carlo simulation as specified.\n\nThe core of the simulation is a data-generating process (DGP) for a linear model with one endogenous regressor $x_i$ and one instrumental variable $z_i$. The model is defined by two equations:\nThe first-stage equation, $x_i = \\pi z_i + v_i$, links the endogenous regressor $x_i$ to the instrument $z_i$. The parameter $\\pi$ governs the strength of the instrument. A value of $\\pi$ close to zero signifies a \"weak instrument,\" meaning $z_i$ has little explanatory power for $x_i$.\nThe structural equation, $y_i = \\beta x_i + u_i$, defines the outcome variable $y_i$. Endogeneity arises because the error terms $v_i$ and $u_i$ are correlated, with $\\text{Corr}(v_i, u_i) = \\rho$. This correlation is explicitly constructed from independent standard normal shocks $e_{1i} \\sim \\mathcal{N}(0,1)$ and $e_{2i} \\sim \\mathcal{N}(0,1)$ by setting $v_i = e_{1i}$ and $u_i = \\rho e_{1i} + \\sqrt{1-\\rho^2} e_{2i}$. This correlation violates the ordinary least squares assumption that the regressor is uncorrelated with the error term, necessitating the use of an instrumental variable estimator.\n\nFor each of the $R$ repetitions in a simulation run, a dataset of size $n$ is generated according to this DGP. Then, the following quantities are computed:\n\n1.  **Instrumental Variables Estimator**: The just-identified IV estimator for $\\beta$ is computed from the sample analogue of the moment condition $E[z_i(y_i - \\beta x_i)]=0$. This yields the expression $\\hat{\\beta} = \\left(\\sum_{i=1}^n z_i y_i\\right) / \\left(\\sum_{i=1}^n z_i x_i\\right)$. This estimator is consistent for $\\beta$ provided the instrument is valid, i.e., relevant ($\\pi \\neq 0$) and exogenous ($E[z_i u_i] = 0$, which holds by construction).\n\n2.  **Hypothesis Test**: A $t$-test for the null hypothesis $H_0: \\beta=0$ is performed. The test statistic is $T = (\\hat{\\beta} - 0) / \\sqrt{\\widehat{V}(\\hat{\\beta})}$, where $\\widehat{V}(\\hat{\\beta})$ is the estimated variance of $\\hat{\\beta}$. The variance formula provided, $\\widehat{V}(\\hat{\\beta}) = \\widehat{S} / (n \\widehat{Q}^2)$ with $\\widehat{S}=\\frac{1}{n}\\sum_{i=1}^n (z_i\\hat{u}_i)^2$ and $\\widehat{Q}=\\frac{1}{n}\\sum_{i=1}^n z_i x_i$, is a heteroskedasticity-robust form. The null hypothesis is rejected at significance level $\\alpha$ if the absolute value of the test statistic, $|T|$, exceeds the critical value $c_\\alpha$ from the standard normal distribution, where $c_\\alpha$ is defined by $\\Pr(|Z| > c_\\alpha) = \\alpha$ for $Z \\sim \\mathcal{N}(0,1)$. The empirical power of the test is the fraction of the $R$ repetitions in which $H_0$ is correctly rejected, given that the true value in the simulation is $\\beta \\neq 0$.\n\n3.  **First-Stage F-Statistic**: As a diagnostic for instrument strength, the $F$-statistic from the first-stage regression of $x_i$ on $z_i$ is calculated. Because there is only one instrument, this is equivalent to the square of the $t$-statistic for the coefficient on $z_i$, $\\hat{a}$. This $t$-statistic is $t_1 = \\hat{a} / \\operatorname{se}(\\hat{a})$. A low average $F$-statistic (commonly, a value below $10$ is used as a rule of thumb) is a widely used indicator of weak instruments. Such instruments are known to cause a number of problems, including biased IV estimators in finite samples and distorted test sizes.\n\nThe program implements this simulation for three distinct parameter sets. These cases are designed to highlight different scenarios: a strong instrument (Case A: $\\pi=0.5$), a weak instrument (Case B: $\\pi=0.05$), and a moderately strong instrument with a smaller sample size (Case C: $n=100$, $\\pi=0.35$). The final output reports the calculated empirical power and the average first-stage $F$-statistic for each case, providing a quantitative illustration of how instrument strength and sample size affect the reliability of statistical inference in IV models.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_simulation(n, pi, rho, beta, alpha, R, seed):\n    \"\"\"\n    Runs a Monte Carlo simulation for the instrumental variables model.\n\n    Args:\n        n (int): Sample size.\n        pi (float): Instrument strength parameter.\n        rho (float): Correlation between structural and first-stage errors.\n        beta (float): True coefficient of the endogenous regressor.\n        alpha (float): Significance level for the hypothesis test.\n        R (int): Number of repetitions.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing the empirical power and the average F-statistic.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    rejection_count = 0\n    total_f_stat = 0.0\n\n    # Calculate the two-sided critical value from the standard normal distribution.\n    c_alpha = norm.ppf(1 - alpha / 2)\n\n    for _ in range(R):\n        # Step 1: Generate data according to the DGP\n        z = rng.normal(loc=0, scale=1, size=n)\n        e1 = rng.normal(loc=0, scale=1, size=n)\n        e2 = rng.normal(loc=0, scale=1, size=n)\n\n        v = e1\n        u = rho * e1 + np.sqrt(1 - rho**2) * e2\n        x = pi * z + v\n        y = beta * x + u\n\n        # Step 2: Compute the IV estimator for beta\n        sum_zy = np.dot(z, y)\n        sum_zx = np.dot(z, x)\n        \n        # Avoid division by zero, though highly unlikely with continuous variables\n        if sum_zx == 0:\n            continue\n            \n        beta_hat = sum_zy / sum_zx\n\n        # Step 3: Compute the t-statistic for H_0: beta = 0\n        u_hat = y - beta_hat * x\n        Q_hat = sum_zx / n\n        S_hat = np.mean((z * u_hat)**2)\n        \n        # Denominator of variance estimator\n        var_denom = n * Q_hat**2\n        if var_denom == 0:\n            continue\n\n        V_hat_beta_hat = S_hat / var_denom\n        \n        # Ensure variance is non-negative before taking square root\n        if V_hat_beta_hat < 0:\n            continue\n        \n        se_beta_hat = np.sqrt(V_hat_beta_hat)\n        \n        if se_beta_hat == 0:\n            continue\n            \n        t_stat = beta_hat / se_beta_hat\n\n        # Step 4: Perform the hypothesis test\n        if np.abs(t_stat) > c_alpha:\n            rejection_count += 1\n\n        # Step 5: Compute the first-stage F-statistic\n        sum_zz = np.dot(z, z)\n        \n        if sum_zz == 0:\n            continue\n\n        a_hat = sum_zx / sum_zz\n        r = x - a_hat * z\n        sigma2_hat = np.sum(r**2) / (n - 1)\n        se_a_hat = np.sqrt(sigma2_hat / sum_zz)\n        \n        if se_a_hat == 0:\n            continue\n\n        t1_stat = a_hat / se_a_hat\n        f_stat = t1_stat**2\n        total_f_stat += f_stat\n\n    empirical_power = rejection_count / R\n    average_f_stat = total_f_stat / R\n\n    return empirical_power, average_f_stat\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, pi, rho, beta, alpha, R, seed)\n        (500, 0.5, 0.6, 1.0, 0.05, 1000, 123),  # Case A\n        (500, 0.05, 0.6, 1.0, 0.05, 1000, 456), # Case B\n        (100, 0.35, 0.6, 1.0, 0.05, 1000, 789), # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        power, avg_f = run_simulation(*case)\n        # Format each result pair to 3 decimal places\n        power_str = f\"{power:.3f}\"\n        avg_f_str = f\"{avg_f:.3f}\"\n        results.append(f\"[{power_str},{avg_f_str}]\")\n\n    # Format the final output string as a list of lists\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "While Two-Stage Least Squares (2SLS) provides an intuitive approach to IV estimation, it is a special case of a more powerful and general framework: the Generalized Method of Moments (GMM). This exercise asks you to implement both the 2SLS and GMM estimators from their foundational principles and numerically verify their equivalence in the overidentified, homoskedastic case. By coding these estimators from scratch, you will solidify your understanding of both methods and appreciate how GMM's moment-based approach provides a unified and extensible foundation for estimation [@problem_id:2402325].", "id": "2402325", "problem": "Consider the linear Instrumental Variables (IV) model with potentially endogenous regressors. Let $y \\in \\mathbb{R}^{n}$ denote the outcome, $X \\in \\mathbb{R}^{n \\times k}$ the matrix of regressors, and $Z \\in \\mathbb{R}^{n \\times \\ell}$ the matrix of instruments, where $\\ell \\ge k$. The moment conditions are $E[Z^{\\top}(y - X\\beta)] = 0$, with the parameter vector $\\beta \\in \\mathbb{R}^{k}$. The Generalized Method of Moments (GMM) estimator minimizes a quadratic form in the sample moments built from $Z$ and the residuals. Under homoskedasticity, the efficient GMM estimator is known to be numerically equivalent to Two-Stage Least Squares (2SLS), but this must be shown by deriving the estimator from first principles and then verified computationally. Your task is to write a complete, runnable program that:\n\n- Implements a data-generating process (DGP) satisfying $E[Z^{\\top}u]=0$ and allows $X$ to be endogenous by construction.\n- Implements the Two-Stage Least Squares (2SLS) estimator and the GMM estimator for an overidentified IV model using a homoskedastic weighting matrix based on the instruments.\n- Verifies numerically that, in the homoskedastic case, the homoskedastic-optimal GMM estimator equals 2SLS up to numerical precision by reporting the Euclidean norm of their difference for a set of test cases, including a boundary case.\n\nYou must base your derivations and algorithm on the following fundamental definitions and facts:\n- The IV moment conditions $E[Z^{\\top}(y - X\\beta)] = 0$.\n- The GMM criterion defined by the quadratic form in the sample moments with a positive definite weighting matrix.\n- Under homoskedasticity, the optimal GMM weighting matrix is proportional to the inverse of the second moment of the instruments, which can be consistently estimated by the sample second moment of $Z$.\n\nDo not use or quote any closed-form IV or GMM estimator expressions in this problem statement; you must derive them in your solution starting from the definitions above.\n\nData-generating process to implement in code for each test case:\n- Draw $Z \\in \\mathbb{R}^{n \\times \\ell}$ from a zero-mean multivariate normal distribution with covariance $\\Sigma_{Z}$, where $(\\Sigma_{Z})_{ij} = \\rho^{|i-j|}$ for a specified $0 \\le \\rho &lt; 1$.\n- Draw $u \\in \\mathbb{R}^{n}$ as homoskedastic noise $u \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ that is independent of $Z$.\n- Draw $\\eta \\in \\mathbb{R}^{n \\times k}$ with independent standard normal entries and choose a fixed $\\gamma \\in \\mathbb{R}^{k}$ to induce endogeneity.\n- Construct $X = Z\\Pi + \\eta + u \\gamma^{\\top}$ with a fixed matrix $\\Pi \\in \\mathbb{R}^{\\ell \\times k}$ to ensure instrument relevance and endogeneity of $X$ through $u$.\n- Fix a true parameter vector $\\beta \\in \\mathbb{R}^{k}$ and generate $y = X\\beta + u$.\n\nEstimator definitions to implement in code:\n- Two-Stage Least Squares (2SLS): Use the projection onto the instrument space implied by $Z$ and compute the 2SLS estimate of $\\beta$.\n- Generalized Method of Moments (GMM): Using the moment vector $g_{n}(\\beta) = \\frac{1}{n} Z^{\\top}(y - X\\beta)$, minimize the quadratic form in $g_{n}(\\beta)$ with a positive definite weighting matrix $W_{n}$ given by the inverse of the sample second moment of $Z$ up to proportionality. Use this to compute the homoskedastic-optimal GMM estimate of $\\beta$.\n\nYour program must compute and return, for each test case, the Euclidean norm of the difference between the 2SLS and homoskedastic-optimal GMM estimates, which should be near zero if the equivalence holds numerically.\n\nTest suite:\n- Case $1$ (overidentified, happy path): $n = 1000$, $k = 2$, $\\ell = 4$, $\\rho = 0.3$, $\\sigma = 1.0$, seed $= 42$, $\\gamma = (0.4, -0.2)$, $\\beta = (1.0, -0.5)$.\n- Case $2$ (overidentified, near-collinearity in instruments): $n = 200$, $k = 2$, $\\ell = 4$, $\\rho = 0.95$, $\\sigma = 1.0$, seed $= 123$, $\\gamma = (0.5, 0.5)$, $\\beta = (0.7, 1.2)$.\n- Case $3$ (boundary, just-identified): $n = 800$, $k = 2$, $\\ell = 2$, $\\rho = 0.5$, $\\sigma = 0.8$, seed $= 7$, $\\gamma = (0.3, -0.1)$, $\\beta = (-0.2, 0.9)$.\n- Case $4$ (overidentified, small sample): $n = 50$, $k = 2$, $\\ell = 5$, $\\rho = 0.7$, $\\sigma = 1.0$, seed $= 99$, $\\gamma = (0.6, -0.4)$, $\\beta = (1.5, -1.0)$.\n\nImplementation requirements:\n- Use the same random seed per case to make results reproducible.\n- Ensure numerical stability by using a Moore–Penrose pseudoinverse where needed to handle near-singular matrices.\n- All computations must be performed in double precision floating point.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with $4$ floating-point values, each equal to the Euclidean norm $\\lVert \\hat{\\beta}_{\\text{2SLS}} - \\hat{\\beta}_{\\text{GMM, homo}} \\rVert_{2}$ for the corresponding test case, in the same order as listed above. The line must be formatted exactly as a Python list, for example, $[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4}]$.", "solution": "The problem presented requires the derivation and subsequent numerical verification of the equivalence between the Two-Stage Least Squares (2SLS) estimator and the efficient Generalized Method of Moments (GMM) estimator under the condition of homoskedasticity. Before proceeding, a rigorous validation of the problem statement is mandatory.\n\nFirst, we must extract the given information verbatim.\nThe model is a linear structure: $y = X\\beta + u$, where $y \\in \\mathbb{R}^{n}$ is the outcome vector, $X \\in \\mathbb{R}^{n \\times k}$ is the matrix of regressors, $\\beta \\in \\mathbb{R}^{k}$ is the parameter vector to be estimated, and $u \\in \\mathbb{R}^{n}$ is the vector of unobserved error terms.\nA matrix of instruments $Z \\in \\mathbb{R}^{n \\times \\ell}$ is available, with $\\ell \\ge k$.\nThe fundamental moment conditions are given by $E[Z^{\\top}(y - X\\beta)] = 0$.\nThe problem specifies a data-generating process (DGP):\n- $Z$ is drawn from a multivariate normal distribution $\\mathcal{N}(0, \\Sigma_{Z})$, where $(\\Sigma_{Z})_{ij} = \\rho^{|i-j|}$.\n- $u$ is drawn from $\\mathcal{N}(0, \\sigma^{2} I_{n})$ and is independent of $Z$.\n- $\\eta \\in \\mathbb{R}^{n \\times k}$ consists of independent standard normal entries.\n- The regressors are constructed as $X = Z\\Pi + \\eta + u \\gamma^{\\top}$, for some fixed matrix $\\Pi \\in \\mathbb{R}^{\\ell \\times k}$ and vector $\\gamma \\in \\mathbb{R}^{k}$. This construction introduces endogeneity, as $Cov(X_{j}, u) \\ne 0$ if the corresponding element of $\\gamma$ is non-zero.\nThe estimators to be implemented are:\n1.  Two-Stage Least Squares (2SLS), defined by a two-step projection and regression procedure.\n2.  Generalized Method of Moments (GMM), defined as the minimizer of the quadratic form $g_{n}(\\beta)^{\\top} W_{n} g_{n}(\\beta)$, where $g_{n}(\\beta) = \\frac{1}{n} Z^{\\top}(y - X\\beta)$ and the weighting matrix $W_{n}$ for the homoskedastic case is proportional to $(E[Z_i Z_i^{\\top}])^{-1}$, estimated by $(\\frac{1}{n}Z^{\\top}Z)^{-1}$.\n\nThe task is to compute the Euclidean norm of the difference between the two estimated parameter vectors, $\\lVert \\hat{\\beta}_{\\text{2SLS}} - \\hat{\\beta}_{\\text{GMM, homo}} \\rVert_{2}$, for a specified set of test cases.\n\nNow, we evaluate the validity of the problem statement.\nThe problem is **scientifically grounded**. It is rooted in foundational econometric theory concerning instrumental variables estimation. The concepts of 2SLS, GMM, moment conditions, and endogeneity are standard textbook material. The DGP is a valid construction for simulating an endogenous regressor model.\nThe problem is **well-posed**. It provides a clear objective and all necessary information to achieve it. The existence and uniqueness of the estimators are guaranteed under standard full-rank assumptions, which are met with high probability by the specified random data generation process. The task of numerical verification is unambiguous. The problem does not specify the matrix $\\Pi$ but correctly states it should be a \"fixed matrix\" that ensures instrument relevance. The choice of such a matrix is part of a standard simulation setup; a fixed random draw is a valid and reproducible approach.\nThe problem is **objective**. It uses precise mathematical and statistical language, devoid of subjective or ambiguous terminology.\nThe problem meets all criteria for validity. There are no scientific flaws, no missing information that prevents a solution, and no contradictions. Therefore, the problem is deemed **valid**, and we may proceed with the solution.\n\nThe core of the problem is to demonstrate the algebraic equivalence of the 2SLS estimator and the efficient GMM estimator under homoskedasticity. We begin with the derivations.\n\nLet us first derive the GMM estimator. The GMM criterion is to minimize a quadratic form of the sample moments. Let the sample moment vector be $g_n(\\beta) = \\frac{1}{n} Z^{\\top}(y - X\\beta)$. The GMM objective function is $J(\\beta) = g_n(\\beta)^{\\top} W_n g_n(\\beta)$, where $W_n$ is a positive definite weighting matrix. For computational convenience, we can absorb the $1/n$ scaling factors into the weighting matrix and minimize the equivalent objective function:\n$$Q(\\beta) = (y - X\\beta)^{\\top}Z W Z^{\\top}(y - X\\beta)$$\nwhere $W$ is a weighting matrix. Under homoskedasticity and independence of $u_i$ and $Z_i$, the optimal choice for the asymptotic variance of the GMM estimator is achieved with a weighting matrix $W_n$ that is a consistent estimator of $(E[Z_i Z_i^{\\top}])^{-1}$. A natural sample analogue is $W_n = (\\frac{1}{n}Z^{\\top}Z)^{-1}$. Substituting this into the objective function (and ignoring the scalar $1/n$ which does not affect the minimizer), we set $W=(Z^{\\top}Z)^{-1}$. The objective function becomes:\n$$Q(\\beta) = (y - X\\beta)^{\\top}Z (Z^{\\top}Z)^{-1} Z^{\\top}(y - X\\beta)$$\nLet us define the matrix $P_Z = Z(Z^{\\top}Z)^{-1}Z^{\\top}$. This is the orthogonal projection matrix onto the column space of $Z$. The objective function simplifies to:\n$$Q(\\beta) = (y - X\\beta)^{\\top}P_Z(y - X\\beta)$$\nThis is a quadratic function of $\\beta$. To find the minimum, we compute the gradient with respect to $\\beta$ and set it to zero.\n$$\\frac{\\partial Q(\\beta)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( y^{\\top}P_Z y - 2\\beta^{\\top}X^{\\top}P_Z y + \\beta^{\\top}X^{\\top}P_Z X \\beta \\right) = -2X^{\\top}P_Z y + 2X^{\\top}P_Z X \\beta$$\nSetting the gradient to zero gives the first-order condition:\n$$X^{\\top}P_Z X \\hat{\\beta}_{\\text{GMM}} = X^{\\top}P_Z y$$\nAssuming the matrix $X^{\\top}P_Z X$ is invertible, the GMM estimator is:\n$$\\hat{\\beta}_{\\text{GMM}} = (X^{\\top}P_Z X)^{-1}X^{\\top}P_Z y$$\nIn implementation, inverses should be computed using a numerically stable method, such as the Moore-Penrose pseudoinverse, to handle potential near-singularity, as stipulated in the problem.\n\nNext, we derive the 2SLS estimator. This estimator is computed in two sequential stages of Ordinary Least Squares (OLS).\n**Stage 1:** Each column of the regressor matrix $X$ is regressed on the instrument matrix $Z$. The fitted values from this regression, which we denote by $\\hat{X}$, are the projection of $X$ onto the column space of $Z$. The OLS formula for the coefficients of this regression is $\\hat{\\Pi} = (Z^{\\top}Z)^{-1}Z^{\\top}X$. The matrix of fitted values is thus:\n$$\\hat{X} = Z\\hat{\\Pi} = Z(Z^{\\top}Z)^{-1}Z^{\\top}X = P_Z X$$\n**Stage 2:** The outcome variable $y$ is regressed on the fitted values $\\hat{X}$ from the first stage. The resulting OLS estimator for $\\beta$ is the 2SLS estimator.\n$$\\hat{\\beta}_{\\text{2SLS}} = (\\hat{X}^{\\top}\\hat{X})^{-1}\\hat{X}^{\\top}y$$\nNow, substitute the expression for $\\hat{X}$ into this formula.\n$$\\hat{\\beta}_{\\text{2SLS}} = ((P_Z X)^{\\top}(P_Z X))^{-1}(P_Z X)^{\\top}y$$\nThe projection matrix $P_Z$ is symmetric ($P_Z^{\\top} = P_Z$) and idempotent ($P_Z^2 = P_Z$). Using these properties, we simplify the terms in the expression.\nThe first term becomes: $(\\hat{X}^{\\top}\\hat{X}) = (P_Z X)^{\\top}(P_Z X) = X^{\\top}P_Z^{\\top}P_Z X = X^{\\top}P_Z P_Z X = X^{\\top}P_Z X$.\nThe second term becomes: $\\hat{X}^{\\top}y = (P_Z X)^{\\top}y = X^{\\top}P_Z^{\\top}y = X^{\\top}P_Z y$.\nSubstituting these simplified forms back into the expression for $\\hat{\\beta}_{\\text{2SLS}}$ yields:\n$$\\hat{\\beta}_{\\text{2SLS}} = (X^{\\top}P_Z X)^{-1}X^{\\top}P_Z y$$\nThis expression is identical to the one derived for $\\hat{\\beta}_{\\text{GMM}}$. The theoretical equivalence is thus established.\n\nThe final part of the task is to verify this equivalence numerically. The program will implement the DGP as described, then compute both estimators according to their respective definitions, and finally calculate the Euclidean distance between the resulting vectors.\nFor the 2SLS estimator, the code will first compute $\\hat{X} = P_Z X$ and then use it in the second-stage regression formula $\\hat{\\beta}_{\\text{2SLS}} = (\\hat{X}^{\\top}\\hat{X})^{-1}\\hat{X}^{\\top}y$.\nFor the GMM estimator, the code will directly compute the components of the derived formula $\\hat{\\beta}_{\\text{GMM}} = (X^{\\top}P_Z X)^{-1}X^{\\top}P_Z y$.\nAlthough algebraically identical, this separate implementation path verifies that the two conceptual procedures yield the same numerical result up to machine precision. The use of the Moore-Penrose pseudoinverse `np.linalg.pinv` is crucial for ensuring stability, especially in Case 2 where instruments are nearly collinear, and in the just-identified Case 3 where the matrices involved may be square but ill-conditioned. The fixed random seed for each test case ensures reproducibility of the simulated data and the final results. The expected outcome is a set of norm differences that are very close to zero.", "answer": "```python\nimport numpy as np\n\ndef generate_toeplitz_covariance(dim, rho):\n    \"\"\"\n    Generates a Toeplitz covariance matrix Sigma_Z where Sigma_ij = rho^|i-j|.\n    \"\"\"\n    indices = np.arange(dim)\n    row_col_diff = np.abs(indices[:, np.newaxis] - indices)\n    return rho ** row_col_diff\n\ndef run_simulation(n, k, l, rho, sigma, seed, gamma, beta_true):\n    \"\"\"\n    Runs a single simulation to compute the difference between 2SLS and GMM estimators.\n    \n    Args:\n        n (int): Number of observations.\n        k (int): Number of regressors.\n        l (int): Number of instruments.\n        rho (float): Autocorrelation parameter for instruments.\n        sigma (float): Standard deviation of the error term.\n        seed (int): Random seed for reproducibility.\n        gamma (np.ndarray): Parameter vector for inducing endogeneity.\n        beta_true (np.ndarray): True parameter vector for the outcome equation.\n    \n    Returns:\n        float: The Euclidean norm of the difference between 2SLS and GMM estimates.\n    \"\"\"\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Data-Generating Process (DGP)\n    # Generate covariance matrix for instruments Z\n    sigma_z = generate_toeplitz_covariance(l, rho)\n    # Generate instruments Z from a multivariate normal distribution\n    Z = rng.multivariate_normal(np.zeros(l), sigma_z, size=n, check_valid='warn') # Z is n x l\n\n    # Generate homoskedastic error term u\n    u = rng.normal(0, sigma, size=n) # u is (n,)\n\n    # Generate exogenous part of X\n    eta = rng.standard_normal(size=(n, k)) # eta is n x k\n\n    # Generate fixed Pi matrix for instrument relevance\n    # Pi is generated once per simulation run based on the seed\n    pi_matrix = rng.standard_normal(size=(l, k)) # Pi is l x k\n    \n    # Construct endogenous regressors X\n    # u is (n,), gamma is (k,). np.outer(u, gamma) gives an (n,k) matrix\n    endogeneity_term = np.outer(u, gamma)\n    X = Z @ pi_matrix + eta + endogeneity_term # X is n x k\n\n    # Generate outcome variable y\n    # X @ beta_true results in a vector of shape (n,)\n    y = X @ beta_true + u # y is (n,)\n\n    # 2. Estimation\n    # Both estimators use the projection matrix P_Z = Z @ inv(Z.T @ Z) @ Z.T\n    # We use pseudoinverse for numerical stability as required.\n    try:\n        ZtZ_inv = np.linalg.pinv(Z.T @ Z)\n    except np.linalg.LinAlgError:\n        # This case is unlikely with pinv but handled for robustness.\n        return np.nan\n\n    # For GMM, we can compute P_Z implicitly to save memory, though for the given n,\n    # explicit computation is fine.\n    # P_Z = Z @ ZtZ_inv @ Z.T\n\n    # 2a. Two-Stage Least Squares (2SLS) Estimator\n    # Stage 1: Project X onto the space of instruments Z to get X_hat\n    X_hat = Z @ ZtZ_inv @ (Z.T @ X)\n    \n    # Stage 2: Regress y on X_hat\n    try:\n        Xhat_t_Xhat = X_hat.T @ X_hat\n        Xhat_t_Xhat_inv = np.linalg.pinv(Xhat_t_Xhat)\n        beta_2sls = Xhat_t_Xhat_inv @ (X_hat.T @ y)\n    except np.linalg.LinAlgError:\n        beta_2sls = np.full(k, np.nan)\n\n    # 2b. Homoskedastic-Optimal GMM Estimator\n    # The derived formula is beta_gmm = (X.T @ P_Z @ X)^-1 @ (X.T @ P_Z @ y)\n    # Using P_Z = Z(Z'Z)^-1Z'\n    # And properties of P_Z (symmetric, idempotent): X.T @ P_Z @ X = X_hat.T @ X_hat\n    # And X.T @ P_Z @ y = X_hat.T @ y\n    # We will compute it via the GMM formula structure to verify the implementation path.\n    try:\n        X_t_PZ = X.T @ Z @ ZtZ_inv @ Z.T\n        X_t_PZ_X = X_t_PZ @ X\n        X_t_PZ_y = X_t_PZ @ y\n        \n        X_t_PZ_X_inv = np.linalg.pinv(X_t_PZ_X)\n        beta_gmm = X_t_PZ_X_inv @ X_t_PZ_y\n    except np.linalg.LinAlgError:\n        beta_gmm = np.full(k, np.nan)\n\n    # 3. Comparison\n    # Calculate the Euclidean norm of the difference\n    norm_diff = np.linalg.norm(beta_2sls - beta_gmm)\n    \n    return norm_diff\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=1000, k=2, l=4, rho=0.3, sigma=1.0, seed=42, gamma=(0.4, -0.2), beta=(1.0, -0.5)\n        {\"n\": 1000, \"k\": 2, \"l\": 4, \"rho\": 0.3, \"sigma\": 1.0, \"seed\": 42, \n         \"gamma\": np.array([0.4, -0.2]), \"beta_true\": np.array([1.0, -0.5])},\n        \n        # Case 2: n=200, k=2, l=4, rho=0.95, sigma=1.0, seed=123, gamma=(0.5, 0.5), beta=(0.7, 1.2)\n        {\"n\": 200, \"k\": 2, \"l\": 4, \"rho\": 0.95, \"sigma\": 1.0, \"seed\": 123, \n         \"gamma\": np.array([0.5, 0.5]), \"beta_true\": np.array([0.7, 1.2])},\n        \n        # Case 3: n=800, k=2, l=2, rho=0.5, sigma=0.8, seed=7, gamma=(0.3, -0.1), beta=(-0.2, 0.9)\n        {\"n\": 800, \"k\": 2, \"l\": 2, \"rho\": 0.5, \"sigma\": 0.8, \"seed\": 7, \n         \"gamma\": np.array([0.3, -0.1]), \"beta_true\": np.array([-0.2, 0.9])},\n        \n        # Case 4: n=50, k=2, l=5, rho=0.7, sigma=1.0, seed=99, gamma=(0.6, -0.4), beta=(1.5, -1.0)\n        {\"n\": 50, \"k\": 2, \"l\": 5, \"rho\": 0.7, \"sigma\": 1.0, \"seed\": 99, \n         \"gamma\": np.array([0.6, -0.4]), \"beta_true\": np.array([1.5, -1.0])}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(**case)\n        results.append(result)\n\n    # Format output as specified\n    # The repr() function provides high precision for floating-point numbers.\n    print(f\"[{','.join(map(repr, results))}]\")\n\nsolve()\n```"}]}