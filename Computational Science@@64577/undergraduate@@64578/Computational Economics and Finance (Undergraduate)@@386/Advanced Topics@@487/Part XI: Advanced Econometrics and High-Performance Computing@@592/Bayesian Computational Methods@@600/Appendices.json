{"hands_on_practices": [{"introduction": "This practice introduces the foundational Beta-Binomial model, a cornerstone of Bayesian analysis for proportions. The goal is to quantify the sentiment of a text by estimating the unknown probability $p$ that a word is \"dovish\". This exercise [@problem_id:2375504] is your first step in applying the complete Bayesian workflow: you will update a prior belief about $p$ using observed data (word counts) to arrive at a posterior distribution, and then summarize your findings using posterior means and credible intervals.", "id": "2375504", "problem": "You are given a simplified, model-based setup for quantifying the sentiment of central bank communications through the relative frequency of terms categorized as \"dovish\" and \"hawkish.\" For a single document, let $D$ denote the count of \"dovish\" terms and let $H$ denote the count of \"hawkish\" terms extracted by a pre-specified lexicon. Let $N$ denote the total number of categorized terms, so $N = D + H$. Assume that each categorized term is independently generated and that the unknown probability that a categorized term is \"dovish\" is $p \\in (0,1)$. Conditional on $N$, model the count $D$ as a Binomial random variable with parameter $p$. Place a Beta prior on $p$, so $p \\sim \\text{Beta}(a,b)$ with shape parameters $a > 0$ and $b > 0$.  \n\nDefine the documentâ€™s \"dovishness index\" as the posterior mean of $p$ under the model above. Define an equal-tailed credible interval (CI) for $p$ at level $c \\in (0,1)$ as the interval whose lower and upper endpoints are the $\\alpha/2$ and $1-\\alpha/2$ posterior quantiles of $p$, respectively, where $\\alpha = 1-c$. For classification support, define the posterior probability that the document is more \"dovish than hawkish\" at threshold $\\tau \\in (0,1)$ as $\\mathbb{P}(p > \\tau \\mid D,H,a,b)$. All probability values must be expressed as decimals in $[0,1]$.  \n\nFor each test case below, compute and return the following four quantities for the specified parameters $(D,H,a,b,c,\\tau)$:\n- the posterior mean of $p$,\n- the lower endpoint of the equal-tailed CI at level $c$,\n- the upper endpoint of the equal-tailed CI at level $c$,\n- the posterior probability $\\mathbb{P}(p > \\tau \\mid D,H,a,b)$.\n\nTest suite (each line is one test case $(D,H,a,b,c,\\tau)$):\n- $(D,H,a,b,c,\\tau) = (\\,30,\\,20,\\,2,\\,2,\\,0.95,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,0,\\,0,\\,1,\\,1,\\,0.9,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,50,\\,10,\\,0.5,\\,0.5,\\,0.95,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,1,\\,9,\\,5,\\,1,\\,0.9,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,100,\\,100,\\,10,\\,10,\\,0.99,\\,0.5\\,)$\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, where each inner list corresponds to one test case and is ordered as $[\\,\\text{posterior\\_mean},\\,\\text{CI\\_lower},\\,\\text{CI\\_upper},\\,\\mathbb{P}(p>\\tau)\\,]$. Each decimal must be rounded to $6$ digits after the decimal point. For example, the final output format must look like $[[m_1,l_1,u_1,q_1],[m_2,l_2,u_2,q_2],\\dots]$ with no spaces anywhere.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n\n- Document term counts: $D$ (dovish), $H$ (hawkish).\n- Total categorized terms: $N = D + H$.\n- Unknown parameter: $p \\in (0,1)$, the probability a term is \"dovish\".\n- Likelihood model: The count $D$ is modeled as a Binomial random variable, $D \\mid p, N \\sim \\text{Binomial}(N, p)$.\n- Prior model: The parameter $p$ is assigned a Beta prior, $p \\sim \\text{Beta}(a,b)$, with hyperparameters $a > 0$ and $b > 0$.\n- \"Dovishness index\": Defined as the posterior mean of $p$, $E[p \\mid D, H, a, b]$.\n- Credible Interval (CI): An equal-tailed interval for $p$ at level $c \\in (0,1)$. The lower and upper endpoints are the $\\alpha/2$ and $1-\\alpha/2$ quantiles of the posterior distribution, where $\\alpha = 1-c$.\n- Posterior probability: $\\mathbb{P}(p > \\tau \\mid D,H,a,b)$ for a classification threshold $\\tau \\in (0,1)$.\n- Quantities to compute for each test case $(D,H,a,b,c,\\tau)$:\n    1.  The posterior mean of $p$.\n    2.  The lower endpoint of the CI.\n    3.  The upper endpoint of the CI.\n    4.  The posterior probability $\\mathbb{P}(p > \\tau)$.\n- Test suite of parameters $(D,H,a,b,c,\\tau)$:\n    - $(\\,30,\\,20,\\,2,\\,2,\\,0.95,\\,0.5\\,)$\n    - $(\\,0,\\,0,\\,1,\\,1,\\,0.9,\\,0.5\\,)$\n    - $(\\,50,\\,10,\\,0.5,\\,0.5,\\,0.95,\\,0.5\\,)$\n    - $(\\,1,\\,9,\\,5,\\,1,\\,0.9,\\,0.5\\,)$\n    - $(\\,100,\\,100,\\,10,\\,10,\\,0.99,\\,0.5\\,)$\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is based on the Beta-Binomial model, a standard and fundamental conjugate prior construction in Bayesian statistics. This framework is scientifically sound and widely applied in fields like text analysis and econometrics. The problem is firmly grounded in established probability theory.\n- **Well-Posed**: The problem is well-posed. The combination of a Beta prior and a Binomial likelihood yields a posterior that is also a Beta distribution. For a Beta distribution, the mean, quantiles, and tail probabilities are all well-defined and unique. Given the specified inputs, a unique solution exists for all required calculations.\n- **Objective**: The problem is formulated using precise, objective mathematical language. All terms are explicitly defined (e.g., \"dovishness index\" as the posterior mean). The problem is free of subjective or ambiguous statements.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. It is scientifically sound, well-posed, and objective. It contains all necessary information for a unique solution. A complete, reasoned solution will be provided.\n\n### Solution\n\nThe problem requires the application of Bayesian inference to a standard conjugate model. The core principle is that a Beta distribution is a conjugate prior for the parameter of a Binomial likelihood. This property allows for a closed-form analytical expression for the posterior distribution, which simplifies computation significantly.\n\nLet the prior distribution for the probability $p$ be a Beta distribution with shape parameters $a$ and $b$:\n$$ p \\sim \\text{Beta}(a,b) $$\nThe probability density function (PDF) is given by:\n$$ f(p; a, b) = \\frac{p^{a-1}(1-p)^{b-1}}{B(a,b)} $$\nwhere $B(a,b)$ is the Beta function.\n\nThe observed data consists of $D$ \"dovish\" terms and $H$ \"hawkish\" terms, for a total of $N = D+H$ terms. The likelihood of observing $D$ dovish terms in a sample of size $N$, given the probability $p$, is described by the Binomial distribution:\n$$ D \\mid p, N \\sim \\text{Binomial}(N, p) $$\nThe likelihood function is:\n$$ L(p \\mid D, N) = \\binom{N}{D} p^D (1-p)^{N-D} $$\n\nAccording to Bayes' theorem, the posterior distribution of $p$ is proportional to the product of the likelihood and the prior:\n$$ f(p \\mid D, N, a, b) \\propto L(p \\mid D, N) \\times f(p; a, b) $$\n$$ f(p \\mid D, N, a, b) \\propto \\left( p^D (1-p)^{N-D} \\right) \\times \\left( p^{a-1} (1-p)^{b-1} \\right) $$\nCombining the terms with $p$ and $(1-p)$:\n$$ f(p \\mid D, N, a, b) \\propto p^{D+a-1} (1-p)^{N-D+b-1} $$\nSubstituting $N = D+H$:\n$$ f(p \\mid D, N, a, b) \\propto p^{D+a-1} (1-p)^{H+b-1} $$\nThis expression is the kernel of a Beta distribution with updated parameters $a' = a+D$ and $b' = b+H$. Thus, the posterior distribution is:\n$$ p \\mid D, H, a, b \\sim \\text{Beta}(a', b') = \\text{Beta}(a+D, b+H) $$\n\nWith the posterior distribution identified, we can compute the four required quantities.\n\n1.  **Posterior Mean (Dovishness Index)**: The mean of a $\\text{Beta}(a', b')$ distribution is given by:\n    $$ E[p \\mid D, H, a, b] = \\frac{a'}{a'+b'} = \\frac{a+D}{a+D+b+H} $$\n\n2.  **Equal-Tailed Credible Interval (CI)**: For a confidence level $c$, we set $\\alpha = 1-c$. The equal-tailed CI is defined by the $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$ quantiles of the posterior distribution. Let $F_{a',b'}^{-1}(q)$ be the quantile function (inverse CDF) of the $\\text{Beta}(a', b')$ distribution for a probability $q$.\n    - Lower endpoint: $CI_{\\text{lower}} = F_{a',b'}^{-1}\\left(\\frac{\\alpha}{2}\\right) = F_{a+D, b+H}^{-1}\\left(\\frac{1-c}{2}\\right)$\n    - Upper endpoint: $CI_{\\text{upper}} = F_{a',b'}^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) = F_{a+D, b+H}^{-1}\\left(1 - \\frac{1-c}{2}\\right)$\n\n3.  **Posterior Probability $\\mathbb{P}(p > \\tau)$**: This is the probability that $p$ exceeds a certain threshold $\\tau$, calculated from the posterior distribution. This is equivalent to the survival function (SF), $S(x) = 1 - F(x)$, evaluated at $\\tau$.\n    $$ \\mathbb{P}(p > \\tau \\mid D,H,a,b) = \\int_{\\tau}^{1} f(p \\mid D,H,a,b) \\,dp = 1 - F_{a',b'}(\\tau) = S_{a+D, b+H}(\\tau) $$\n    where $F_{a',b'}$ and $S_{a',b'}$ are the CDF and SF of the $\\text{Beta}(a', b')$ distribution, respectively. Using the survival function is computationally preferred for numerical stability, especially for $\\tau$ near $1$.\n\nThese formulae will be applied to each test case using the provided parameters. The calculations for quantiles and the survival function will be performed using standard numerical libraries.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Computes Bayesian sentiment metrics for a series of test cases.\n\n    For each case, it calculates the posterior mean, credible interval,\n    and a posterior probability based on a Beta-Binomial model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is in the format (D, H, a, b, c, tau)\n    test_cases = [\n        (30, 20, 2, 2, 0.95, 0.5),\n        (0, 0, 1, 1, 0.9, 0.5),\n        (50, 10, 0.5, 0.5, 0.95, 0.5),\n        (1, 9, 5, 1, 0.9, 0.5),\n        (100, 100, 10, 10, 0.99, 0.5),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        D, H, a, b, c, tau = case\n\n        # Calculate posterior parameters for the Beta distribution\n        # a_prime = a + D and b_prime = b + H\n        a_prime = a + D\n        b_prime = b + H\n\n        # 1. Calculate the posterior mean\n        posterior_mean = a_prime / (a_prime + b_prime)\n\n        # 2. Calculate the equal-tailed credible interval\n        alpha = 1.0 - c\n        ci_lower = beta.ppf(alpha / 2.0, a_prime, b_prime)\n        ci_upper = beta.ppf(1.0 - alpha / 2.0, a_prime, b_prime)\n\n        # 3. Calculate the posterior probability P(p > tau)\n        # This is the survival function (1 - CDF) of the posterior Beta distribution\n        prob_p_gt_tau = beta.sf(tau, a_prime, b_prime)\n        \n        case_results = [\n            posterior_mean,\n            ci_lower,\n            ci_upper,\n            prob_p_gt_tau\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string according to the specification:\n    # A comma-separated list of lists with no spaces, with values rounded to 6 decimal places.\n    # Example: [[m1,l1,u1,q1],[m2,l2,u2,q2],...]\n    inner_parts = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places\n        formatted_vals = [f\"{val:.6f}\" for val in res_list]\n        # Join numbers with a comma and enclose in brackets\n        inner_parts.append(f\"[{','.join(formatted_vals)}]\")\n    \n    # Join the inner lists with a comma and enclose in brackets for the final string\n    final_output_str = f\"[{','.join(inner_parts)}]\"\n\n    print(final_output_str)\n\nsolve()\n```"}, {"introduction": "Building on parameter estimation, this problem shifts our focus from proportions to rates using the Poisson-Gamma conjugate model, a standard for modeling event counts. We will explore a hypothetical scenario of mineral discoveries to learn about an unknown discovery rate, $\\lambda$. The key skill developed in this practice [@problem_id:2375560] is deriving the posterior predictive distribution, which allows us to move beyond simply estimating $\\lambda$ to making concrete probabilistic forecasts about future discoveries.", "id": "2375560", "problem": "An energy firm in resource economics models the discovery of new mineral deposits over time as a homogeneous Poisson process with unknown intensity (discoveries per year) denoted by $\\lambda$. The number of discoveries in any interval of length $t$ is modeled as Poisson with mean $\\lambda t$. Prior beliefs about $\\lambda$ are represented by a Gamma distribution with shape parameter $\\alpha>0$ and rate parameter $\\beta>0$, with density $f(\\lambda)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda)$ for $\\lambda>0$. Over a historical period of length $T>0$ (measured in years), a total of $S\\in\\{0,1,2,\\dots\\}$ discoveries were recorded.\n\nUsing Bayesian inference under these assumptions, derive a closed-form expression for the posterior predictive probability that at least one discovery occurs during a future monitoring window of length $\\tau>0$ (measured in years). Express your final answer as a single analytic expression in terms of $\\alpha$, $\\beta$, $S$, $T$, and $\\tau$. Do not substitute numerical values. Your answer must be a single closed-form analytic expression.", "solution": "The problem requires the derivation of the posterior predictive probability of observing at least one event in a future time interval, given a Poisson process model for discoveries, a Gamma prior distribution for the unknown rate parameter, and historical data. We shall proceed with methodical Bayesian inference.\n\nFirst, we formalize the components of the Bayesian model.\n\nThe likelihood of observing $S$ discoveries in a time period of length $T$ is given by the Poisson probability mass function, where the mean is $\\lambda T$. The likelihood function for the unknown parameter $\\lambda$ is:\n$$ L(\\lambda | S, T) = P(S | \\lambda, T) = \\frac{(\\lambda T)^S \\exp(-\\lambda T)}{S!} $$\nFor the purpose of Bayesian updating, we only need the kernel of the likelihood function, which is proportional to the function of $\\lambda$:\n$$ L(\\lambda | S, T) \\propto \\lambda^S \\exp(-\\lambda T) $$\n\nThe prior belief about $\\lambda$ is specified by a Gamma distribution with shape parameter $\\alpha > 0$ and rate parameter $\\beta > 0$. The prior probability density function is:\n$$ p(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}\\exp(-\\beta \\lambda) $$\nThe kernel of the prior is:\n$$ p(\\lambda) \\propto \\lambda^{\\alpha-1}\\exp(-\\beta \\lambda) $$\n\nAccording to Bayes' theorem, the posterior distribution of $\\lambda$ given the data $(S, T)$ is proportional to the product of the likelihood and the prior:\n$$ p(\\lambda | S, T) \\propto L(\\lambda | S, T) \\times p(\\lambda) $$\nSubstituting the kernels:\n$$ p(\\lambda | S, T) \\propto \\left( \\lambda^S \\exp(-\\lambda T) \\right) \\times \\left( \\lambda^{\\alpha-1}\\exp(-\\beta \\lambda) \\right) $$\n$$ p(\\lambda | S, T) \\propto \\lambda^{S+\\alpha-1} \\exp(-(\\beta+T)\\lambda) $$\nThis expression is the kernel of a Gamma distribution. The Gamma distribution is a conjugate prior for the Poisson likelihood. The posterior distribution for $\\lambda$ is therefore a Gamma distribution with updated parameters:\nShape parameter $\\alpha' = \\alpha + S$\nRate parameter $\\beta' = \\beta + T$\nSo, the posterior distribution is $\\lambda | S, T \\sim \\text{Gamma}(\\alpha+S, \\beta+T)$. Its full probability density function is:\n$$ p(\\lambda | S, T) = \\frac{(\\beta+T)^{\\alpha+S}}{\\Gamma(\\alpha+S)}\\lambda^{\\alpha+S-1}\\exp(-(\\beta+T)\\lambda) $$\n\nNext, we must find the posterior predictive distribution for the number of discoveries, let us call it $N_\\tau$, in a future time window of length $\\tau > 0$. The model for $N_\\tau$ given $\\lambda$ is also a Poisson distribution with mean $\\lambda \\tau$:\n$$ P(N_\\tau = k | \\lambda, \\tau) = \\frac{(\\lambda\\tau)^k \\exp(-\\lambda\\tau)}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\} $$\nThe posterior predictive probability for $N_\\tau=k$ is found by marginalizing the joint distribution of $N_\\tau$ and $\\lambda$ over $\\lambda$, using the posterior distribution of $\\lambda$:\n$$ P(N_\\tau = k | S, T, \\tau) = \\int_{0}^{\\infty} P(N_\\tau = k | \\lambda, \\tau) p(\\lambda | S, T) d\\lambda $$\n\nWe are asked for the probability of at least one discovery, which is $P(N_\\tau \\ge 1 | S, T, \\tau)$. It is more direct to calculate the probability of the complementary event, zero discoveries, $P(N_\\tau = 0 | S, T, \\tau)$, and subtract it from $1$.\nLet's first calculate $P(N_\\tau = 0 | S, T, \\tau)$. We set $k=0$ in the Poisson PMF:\n$$ P(N_\\tau = 0 | \\lambda, \\tau) = \\frac{(\\lambda\\tau)^0 \\exp(-\\lambda\\tau)}{0!} = \\exp(-\\lambda\\tau) $$\nNow, we integrate this over the posterior distribution of $\\lambda$:\n$$ P(N_\\tau = 0 | S, T, \\tau) = \\int_{0}^{\\infty} \\exp(-\\lambda\\tau) p(\\lambda | S, T) d\\lambda $$\nSubstituting the posterior PDF:\n$$ P(N_\\tau = 0 | S, T, \\tau) = \\int_{0}^{\\infty} \\exp(-\\lambda\\tau) \\frac{(\\beta+T)^{\\alpha+S}}{\\Gamma(\\alpha+S)}\\lambda^{\\alpha+S-1}\\exp(-(\\beta+T)\\lambda) d\\lambda $$\nCombine the exponential terms and move constants outside the integral:\n$$ P(N_\\tau = 0 | S, T, \\tau) = \\frac{(\\beta+T)^{\\alpha+S}}{\\Gamma(\\alpha+S)} \\int_{0}^{\\infty} \\lambda^{\\alpha+S-1} \\exp(-(\\beta+T+\\tau)\\lambda) d\\lambda $$\nThe integral is the standard form for the unnormalized Gamma integral. Recall that for any $a>0$ and $b>0$:\n$$ \\int_{0}^{\\infty} x^{a-1} \\exp(-bx) dx = \\frac{\\Gamma(a)}{b^a} $$\nIn our case, the shape is $a = \\alpha+S$ and the rate is $b = \\beta+T+\\tau$. Applying this formula:\n$$ \\int_{0}^{\\infty} \\lambda^{\\alpha+S-1} \\exp(-(\\beta+T+\\tau)\\lambda) d\\lambda = \\frac{\\Gamma(\\alpha+S)}{(\\beta+T+\\tau)^{\\alpha+S}} $$\nSubstituting this result back into our expression for the probability:\n$$ P(N_\\tau = 0 | S, T, \\tau) = \\frac{(\\beta+T)^{\\alpha+S}}{\\Gamma(\\alpha+S)} \\times \\frac{\\Gamma(\\alpha+S)}{(\\beta+T+\\tau)^{\\alpha+S}} $$\nSimplifying this expression yields:\n$$ P(N_\\tau = 0 | S, T, \\tau) = \\left( \\frac{\\beta+T}{\\beta+T+\\tau} \\right)^{\\alpha+S} $$\nThis result is the probability of observing zero discoveries in the future window. The question asks for the probability of at least one discovery, $P(N_\\tau \\ge 1)$. This is the complement of zero discoveries:\n$$ P(N_\\tau \\ge 1 | S, T, \\tau) = 1 - P(N_\\tau = 0 | S, T, \\tau) $$\nTherefore, the final expression is:\n$$ P(N_\\tau \\ge 1 | S, T, \\tau) = 1 - \\left( \\frac{\\beta+T}{\\beta+T+\\tau} \\right)^{\\alpha+S} $$\nThis is the closed-form analytical expression for the posterior predictive probability, expressed solely in terms of the given parameters and data $\\alpha$, $\\beta$, $S$, $T$, and $\\tau$, as required.", "answer": "$$\\boxed{1 - \\left( \\frac{\\beta+T}{\\beta+T+\\tau} \\right)^{\\alpha+S}}$$"}, {"introduction": "This final practice tackles one of the most powerful applications of Bayesian methods: comparing competing economic theories. You will use the Bayes Factor to evaluate which of two stylized modelsâ€”one \"Keynesian\" and one \"Real Business Cycle\"â€”better explains fluctuations in GDP growth data. This exercise [@problem_id:2375563] provides hands-on experience in calculating the marginal likelihood and highlights a critical aspect of advanced modeling: the sensitivity of model comparison results to the specification of prior beliefs.", "id": "2375563", "problem": "You are given two competing linear time series models intended as stylized representations of two economic theories for fluctuations in gross domestic product (GDP) growth. The first model, $M_{K}$, represents a simplified Keynesian mechanism with an intercept capturing demand shifts. The second model, $M_{R}$, represents a simplified Real Business Cycle (RBC) mechanism with deeper persistence through two lags and no intercept. For a univariate sequence $\\{y_t\\}_{t=1}^{T}$ of demeaned real GDP growth rates, define the models over the common sample $t=3,\\dots,T$ as follows:\n- Model $M_{K}$ (Keynesian): for $t=3,\\dots,T$, $y_t = \\alpha + \\phi y_{t-1} + \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Model $M_{R}$ (Real Business Cycle (RBC)): for $t=3,\\dots,T$, $y_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nFor each model, the parameters and prior distributions are:\n- For $M_{K}$, $\\theta_K = (\\alpha,\\phi,\\sigma^2)$ with prior\n  $\\alpha,\\phi \\mid \\sigma^2 \\sim \\mathcal{N}(m_{0,K}, \\sigma^2 V_{0,K})$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_{0,K}, b_{0,K})$.\n- For $M_{R}$, $\\theta_R = (\\beta_1,\\beta_2,\\sigma^2)$ with prior\n  $\\beta_1,\\beta_2 \\mid \\sigma^2 \\sim \\mathcal{N}(m_{0,R}, \\sigma^2 V_{0,R})$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_{0,R}, b_{0,R})$.\n\nThe Inverse-Gamma prior is parameterized by the density\n$$\np(\\sigma^2) \\;=\\; \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\, (\\sigma^2)^{-(a_0+1)} \\exp\\!\\left(-\\frac{b_0}{\\sigma^2}\\right),\n$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function.\n\nYou must compute, for each dataset and each specified prior configuration, the natural logarithm of the Bayes factor in favor of $M_{K}$ over $M_{R}$, that is,\n$$\n\\log \\text{BF}_{K,R} \\;=\\; \\log p(y \\mid M_{K}) \\;-\\; \\log p(y \\mid M_{R}),\n$$\nwhere $p(y \\mid M)$ denotes the marginal likelihood under model $M$ evaluated on the common sample $t=3,\\dots,T$.\n\nDatasets (each sequence is $y_1,\\dots,y_T$), with $T$ specified implicitly by length:\n- Dataset $1$: $[0.00, 0.10, 0.18, 0.24, 0.29, 0.31, 0.33, 0.35]$.\n- Dataset $2$: $[0.00, 0.30, 0.20, 0.27, 0.18, 0.25, 0.17, 0.24]$.\n- Dataset $3$: $[0.05, 0.02, 0.03, 0.01]$.\n\nTwo distinct prior specifications are to be used:\n\n- Prior specification $\\mathcal{A}$ (diffuse, symmetric across models):\n  - For $M_{K}$: $m_{0,K}^{\\mathcal{A}} = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}$, $V_{0,K}^{\\mathcal{A}} = \\operatorname{diag}(10000.00, 10000.00)$, $a_{0,K}^{\\mathcal{A}} = 0.01$, $b_{0,K}^{\\mathcal{A}} = 0.01$.\n  - For $M_{R}$: $m_{0,R}^{\\mathcal{A}} = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}$, $V_{0,R}^{\\mathcal{A}} = \\operatorname{diag}(10000.00, 10000.00)$, $a_{0,R}^{\\mathcal{A}} = 0.01$, $b_{0,R}^{\\mathcal{A}} = 0.01$.\n\n- Prior specification $\\mathcal{B}$ (informative, model-specific):\n  - For $M_{K}$: $m_{0,K}^{\\mathcal{B}} = \\begin{bmatrix} 0.02 \\\\ 0.80 \\end{bmatrix}$, $V_{0,K}^{\\mathcal{B}} = \\operatorname{diag}(0.01, 0.01)$, $a_{0,K}^{\\mathcal{B}} = 2.00$, $b_{0,K}^{\\mathcal{B}} = 0.01$.\n  - For $M_{R}$: $m_{0,R}^{\\mathcal{B}} = \\begin{bmatrix} 0.60 \\\\ 0.20 \\end{bmatrix}$, $V_{0,R}^{\\mathcal{B}} = \\operatorname{diag}(0.01, 0.01)$, $a_{0,R}^{\\mathcal{B}} = 2.00$, $b_{0,R}^{\\mathcal{B}} = 0.01$.\n\nTest Suite and Answer Specification:\n- The common sample for likelihood evaluation is $t=3,\\dots,T$ for both models and all datasets.\n- For each dataset $i \\in \\{1,2,3\\}$ and each prior specification $P \\in \\{\\mathcal{A},\\mathcal{B}\\}$, compute the scalar $\\log \\text{BF}_{K,R}^{(i,P)}$.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$$\n[\\log \\text{BF}_{K,R}^{(1,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(1,\\mathcal{B})}, \\log \\text{BF}_{K,R}^{(2,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(2,\\mathcal{B})}, \\log \\text{BF}_{K,R}^{(3,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(3,\\mathcal{B})}]\n$$\nAll six outputs must be floating-point numbers. No additional text or formatting is permitted beyond this single line.", "solution": "The problem statement is subjected to rigorous validation.\n\n### Step 1: Extract Givens\n- **Models**:\n    - $M_{K}$ (Keynesian): $y_t = \\alpha + \\phi y_{t-1} + \\varepsilon_t$, for $t=3,\\dots,T$.\n    - $M_{R}$ (Real Business Cycle): $y_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\varepsilon_t$, for $t=3,\\dots,T$.\n    - For both models, the error term is $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- **Priors**:\n    - For $M_{K}$, parameters $\\theta_K = (\\alpha,\\phi,\\sigma^2)$ have prior $\\alpha,\\phi \\mid \\sigma^2 \\sim \\mathcal{N}(m_{0,K}, \\sigma^2 V_{0,K})$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_{0,K}, b_{0,K})$.\n    - For $M_{R}$, parameters $\\theta_R = (\\beta_1,\\beta_2,\\sigma^2)$ have prior $\\beta_1,\\beta_2 \\mid \\sigma^2 \\sim \\mathcal{N}(m_{0,R}, \\sigma^2 V_{0,R})$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_{0,R}, b_{0,R})$.\n- **Inverse-Gamma PDF**: $p(\\sigma^2) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\, (\\sigma^2)^{-(a_0+1)} \\exp(-\\frac{b_0}{\\sigma^2})$.\n- **Objective**: Compute the natural logarithm of the Bayes factor, $\\log \\text{BF}_{K,R} = \\log p(y \\mid M_{K}) - \\log p(y \\mid M_{R})$.\n- **Datasets**:\n    - Dataset $1$: $[0.00, 0.10, 0.18, 0.24, 0.29, 0.31, 0.33, 0.35]$.\n    - Dataset $2$: $[0.00, 0.30, 0.20, 0.27, 0.18, 0.25, 0.17, 0.24]$.\n    - Dataset $3$: $[0.05, 0.02, 0.03, 0.01]$.\n- **Prior Specifications**:\n    - Specification $\\mathcal{A}$:\n        - $M_{K}$: $m_{0,K}^{\\mathcal{A}} = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}$, $V_{0,K}^{\\mathcal{A}} = \\operatorname{diag}(10000.00, 10000.00)$, $a_{0,K}^{\\mathcal{A}} = 0.01$, $b_{0,K}^{\\mathcal{A}} = 0.01$.\n        - $M_{R}$: $m_{0,R}^{\\mathcal{A}} = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}$, $V_{0,R}^{\\mathcal{A}} = \\operatorname{diag}(10000.00, 10000.00)$, $a_{0,R}^{\\mathcal{A}} = 0.01$, $b_{0,R}^{\\mathcal{A}} = 0.01$.\n    - Specification $\\mathcal{B}$:\n        - $M_{K}$: $m_{0,K}^{\\mathcal{B}} = \\begin{bmatrix} 0.02 \\\\ 0.80 \\end{bmatrix}$, $V_{0,K}^{\\mathcal{B}} = \\operatorname{diag}(0.01, 0.01)$, $a_{0,K}^{\\mathcal{B}} = 2.00$, $b_{0,K}^{\\mathcal{B}} = 0.01$.\n        - $M_{R}$: $m_{0,R}^{\\mathcal{B}} = \\begin{bmatrix} 0.60 \\\\ 0.20 \\end{bmatrix}$, $V_{0,R}^{\\mathcal{B}} = \\operatorname{diag}(0.01, 0.01)$, $a_{0,R}^{\\mathcal{B}} = 2.00$, $b_{0,R}^{\\mathcal{B}} = 0.01$.\n- **Evaluation Sample**: The common sample for likelihood evaluation is $t=3,\\dots,T$.\n- **Output Order**: $[\\log \\text{BF}_{K,R}^{(1,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(1,\\mathcal{B})}, \\log \\text{BF}_{K,R}^{(2,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(2,\\mathcal{B})}, \\log \\text{BF}_{K,R}^{(3,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(3,\\mathcal{B})}]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is a standard exercise in Bayesian econometrics. It involves the comparison of two linear regression models using the Bayes factor, calculated via the marginal likelihood. The chosen prior structure, a Normal-Inverse-Gamma distribution, is the natural conjugate prior for a linear model with unknown variance, a fundamental concept in Bayesian statistics. The models themselves, an $AR(1)$ with constant and an $AR(2)$ without constant, are textbook examples. The problem is firmly grounded in established statistical and econometric principles.\n- **Well-Posed**: All necessary componentsâ€”data, model specifications, prior distributions, and hyperparametersâ€”are provided. The objective is clearly defined. For the specified conjugate prior setup, the marginal likelihood has a well-known analytical solution. Therefore, a unique, stable, and meaningful solution exists for each case.\n- **Objective**: The problem is stated in precise mathematical language. The names \"Keynesian\" and \"RBC\" serve as labels for the mathematical structures and do not introduce any subjectivity into the required calculations.\n- **Flaw Checklist**: The problem does not violate any of the specified flaw conditions. It is scientifically sound, formalizable, complete, feasible, well-posed, and verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\nThe objective is to compute the log Bayes Factor $\\log \\text{BF}_{K,R}$, which requires calculating the log marginal likelihood, $\\log p(y \\mid M)$, for each model $M \\in \\{M_K, M_R\\}$.\n\nBoth models fit the framework of a general linear model:\n$$\ny = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_N)\n$$\nwhere $y$ is a vector of $N$ observations, $X$ is an $N \\times k$ design matrix, and $\\beta$ is a $k \\times 1$ vector of regression coefficients. The likelihood function is $p(y \\mid \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-N/2} \\exp(-\\frac{1}{2\\sigma^2}(y-X\\beta)^T(y-X\\beta))$.\n\nThe specified prior is a Normal-Inverse-Gamma distribution:\n$$\np(\\beta, \\sigma^2) = p(\\beta \\mid \\sigma^2) p(\\sigma^2)\n$$\nwhere $\\beta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 V_0)$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_0, b_0)$. This is a conjugate prior for the normal linear model.\n\nThe marginal likelihood $p(y \\mid M)$ is obtained by integrating the product of the likelihood and the prior over all parameters:\n$$\np(y \\mid M) = \\int_0^\\infty \\int_{\\mathbb{R}^k} p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2) \\, d\\beta \\, d\\sigma^2\n$$\nFor the specified conjugate setup, this integral has a closed-form solution. The posterior distribution for the parameters is also Normal-Inverse-Gamma, $p(\\beta, \\sigma^2 \\mid y) \\propto p(\\beta, \\sigma^2)p(y \\mid \\beta, \\sigma^2)$, with updated parameters:\n- Posterior precision matrix for $\\beta$: $V_n^{-1} = V_0^{-1} + X^T X$\n- Posterior mean for $\\beta$: $m_n = V_n (V_0^{-1}m_0 + X^T y)$\n- Posterior shape for $\\sigma^2$: $a_n = a_0 + N/2$\n- Posterior rate for $\\sigma^2$: $b_n = b_0 + \\frac{1}{2}(y^T y + m_0^T V_0^{-1} m_0 - m_n^T V_n^{-1} m_n)$\n\nThe marginal likelihood is the normalizing constant of the posterior, given by:\n$$\np(y \\mid M) = \\frac{\\Gamma(a_n)}{\\Gamma(a_0)} \\frac{b_0^{a_0}}{b_n^{a_n}} (2\\pi)^{-N/2} \\frac{|V_n|^{1/2}}{|V_0|^{1/2}}\n$$\nTo prevent numerical underflow and simplify calculations, we work with the natural logarithm of the marginal likelihood:\n$$\n\\log p(y \\mid M) = \\log\\Gamma(a_n) - \\log\\Gamma(a_0) + a_0\\log(b_0) - a_n\\log(b_n) - \\frac{N}{2}\\log(2\\pi) + \\frac{1}{2}(\\log|V_n| - \\log|V_0|)\n$$\nThe log-Gamma function, $\\log\\Gamma(\\cdot)$, is provided by standard scientific libraries. The log-determinants $\\log|V|$ are computed robustly.\n\nFor each dataset and model, we first construct the response vector $y$ and design matrix $X$ using the common sample for $t=3, \\dots, T$. The number of observations is $N = T - 2$.\nThe response vector is $y = [y_3, y_4, \\dots, y_T]^T$.\n\nFor model $M_K (y_t = \\alpha + \\phi y_{t-1} + \\varepsilon_t)$, the parameter vector is $\\beta_K = [\\alpha, \\phi]^T$, and the design matrix is:\n$$\nX_K = \\begin{bmatrix} 1 & y_2 \\\\ 1 & y_3 \\\\ \\vdots & \\vdots \\\\ 1 & y_{T-1} \\end{bmatrix}\n$$\nThis is an $N \\times 2$ matrix.\n\nFor model $M_R (y_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\varepsilon_t)$, the parameter vector is $\\beta_R = [\\beta_1, \\beta_2]^T$, and the design matrix is:\n$$\nX_R = \\begin{bmatrix} y_2 & y_1 \\\\ y_3 & y_2 \\\\ \\vdots & \\vdots \\\\ y_{T-1} & y_{T-2} \\end{bmatrix}\n$$\nThis is also an $N \\times 2$ matrix.\n\nThe procedure for each of the six required computations is as follows:\n1. Select a dataset and a prior specification.\n2. Construct the response vector $y$ and the design matrices $X_K$ and $X_R$.\n3. For model $M_K$, use its corresponding prior hyperparameters ($m_{0,K}, V_{0,K}, a_{0,K}, b_{0,K}$) to calculate the posterior parameters ($V_{n,K}, m_{n,K}, a_{n,K}, b_{n,K}$) and then the log marginal likelihood $\\log p(y \\mid M_K)$ using the formula above.\n4. For model $M_R$, repeat step 3 with its corresponding prior hyperparameters to find $\\log p(y \\mid M_R)$.\n5. Compute the log Bayes factor: $\\log \\text{BF}_{K,R} = \\log p(y \\mid M_K) - \\log p(y \\mid M_R)$.\nThis process is repeated for each dataset-prior pair as specified in the problem statement. The algorithm will be implemented in Python using the `numpy` library for linear algebra and `scipy` for the log-Gamma function.", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv, slogdet\n\ndef calculate_log_marginal_likelihood(y_data, X, m0, V0, a0, b0):\n    \"\"\"\n    Calculates the log marginal likelihood for a linear model with a Normal-Inverse-Gamma prior.\n    \n    Args:\n        y_data (np.ndarray): The dependent variable vector (N,).\n        X (np.ndarray): The design matrix (N, k).\n        m0 (np.ndarray): The prior mean vector for beta (k,).\n        V0 (np.ndarray): The prior covariance matrix for beta (k, k).\n        a0 (float): The prior shape parameter for sigma^2.\n        b0 (float): The prior rate parameter for sigma^2.\n\n    Returns:\n        float: The log marginal likelihood.\n    \"\"\"\n    N, k = X.shape\n\n    # Calculation of posterior parameters for beta (Vn, mn)\n    V0_inv = inv(V0)\n    Vn_inv = V0_inv + X.T @ X\n    Vn = inv(Vn_inv)\n    mn = Vn @ (V0_inv @ m0 + X.T @ y_data)\n    \n    # Calculation of posterior parameters for sigma^2 (an, bn)\n    an = a0 + N / 2\n    \n    # Quadratic forms for bn\n    # bn = b0 + 0.5 * (y'y + m0'V0_inv*m0 - mn'Vn_inv*mn)\n    ss_data = y_data.T @ y_data\n    ss_prior = m0.T @ V0_inv @ m0\n    ss_posterior_mean = mn.T @ Vn_inv @ mn\n    bn = b0 + 0.5 * (ss_data + ss_prior - ss_posterior_mean)\n\n    # Log determinant term\n    _, logdet_Vn = slogdet(Vn)\n    _, logdet_V0 = slogdet(V0)\n    log_det_term = 0.5 * (logdet_Vn - logdet_V0)\n\n    # Assemble the log marginal likelihood\n    log_ml = (gammaln(an) - gammaln(a0) +\n              a0 * np.log(b0) - an * np.log(bn) -\n              (N / 2) * np.log(2 * np.pi) +\n              log_det_term)\n\n    return log_ml\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the log Bayes Factor for each dataset and prior specification.\n    \"\"\"\n    # Define datasets\n    datasets = {\n        1: np.array([0.00, 0.10, 0.18, 0.24, 0.29, 0.31, 0.33, 0.35]),\n        2: np.array([0.00, 0.30, 0.20, 0.27, 0.18, 0.25, 0.17, 0.24]),\n        3: np.array([0.05, 0.02, 0.03, 0.01])\n    }\n\n    # Define prior specifications\n    priors = {\n        'A': {  # Prior Specification A\n            'K': {'m0': np.array([0.0, 0.0]), 'V0': np.diag([10000.0, 10000.0]), 'a0': 0.01, 'b0': 0.01},\n            'R': {'m0': np.array([0.0, 0.0]), 'V0': np.diag([10000.0, 10000.0]), 'a0': 0.01, 'b0': 0.01}\n        },\n        'B': {  # Prior Specification B\n            'K': {'m0': np.array([0.02, 0.80]), 'V0': np.diag([0.01, 0.01]), 'a0': 2.0, 'b0': 0.01},\n            'R': {'m0': np.array([0.60, 0.20]), 'V0': np.diag([0.01, 0.01]), 'a0': 2.0, 'b0': 0.01}\n        }\n    }\n    \n    test_cases = [\n        (1, 'A'), (1, 'B'),\n        (2, 'A'), (2, 'B'),\n        (3, 'A'), (3, 'B')\n    ]\n\n    results = []\n    for d_idx, p_key in test_cases:\n        y_full = datasets[d_idx]\n        T = len(y_full)\n        N = T - 2\n\n        # Common dependent variable for the sample t=3,...,T\n        y_vec = y_full[2:]\n\n        # Construct Design Matrix for Model K (Keynesian)\n        # y_t = alpha + phi*y_{t-1}\n        X_K = np.vstack([np.ones(N), y_full[1:T-1]]).T\n        \n        # Construct Design Matrix for Model R (RBC)\n        # y_t = beta1*y_{t-1} + beta2*y_{t-2}\n        X_R = np.vstack([y_full[1:T-1], y_full[0:T-2]]).T\n        \n        # Get prior hyperparameters for the current case\n        prior_K = priors[p_key]['K']\n        prior_R = priors[p_key]['R']\n\n        # Calculate log marginal likelihood for Model K\n        log_ml_K = calculate_log_marginal_likelihood(y_vec, X_K, \n            prior_K['m0'], prior_K['V0'], prior_K['a0'], prior_K['b0'])\n        \n        # Calculate log marginal likelihood for Model R\n        log_ml_R = calculate_log_marginal_likelihood(y_vec, X_R, \n            prior_R['m0'], prior_R['V0'], prior_R['a0'], prior_R['b0'])\n\n        # Compute the log Bayes Factor in favor of K over R\n        log_bf_KR = log_ml_K - log_ml_R\n        results.append(log_bf_KR)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}