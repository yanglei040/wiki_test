{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will tackle a problem using stratified sampling. This technique is a powerful, intuitive approach to variance reduction that operates on a \"divide and conquer\" principle. By partitioning the integration domain into smaller sub-domains or \"strata\" and sampling from each, we ensure a more even and representative coverage of the function, which often leads to a more accurate estimate than simple random sampling for the same number of total samples. This exercise [@problem_id:1348949] will give you concrete practice in the mechanics of applying this method to estimate a definite integral.", "id": "1348949", "problem": "A computational scientist is tasked with estimating the value of the definite integral $I = \\int_0^\\pi x \\sin(x) dx$ using a Monte Carlo simulation. To reduce the variance of the estimate compared to simple Monte Carlo, a stratified sampling approach is chosen. The domain of integration, $[0, \\pi]$, is partitioned into two strata of equal width: $S_1 = [0, \\pi/2]$ and $S_2 = [\\pi/2, \\pi]$.\n\nTwo samples are to be drawn from each stratum, for a total of four samples. The transformation to generate a sample $u$ within a stratum $[a, b]$ from a standard uniform random number $r \\in [0, 1]$ is given by $u = a + (b-a)r$.\n\nSuppose the sequence of four standard uniform random numbers generated for this task is $0.20$, $0.70$, $0.40$, and $0.90$. The first two random numbers are to be used for generating samples from stratum $S_1$, and the subsequent two are for stratum $S_2$.\n\nCalculate the stratified sampling estimate of the integral $I$. All calculations involving angles must be performed in radians. Report your final answer rounded to four significant figures.\n\n", "solution": "We estimate $I=\\int_{0}^{\\pi}x\\sin(x)\\,dx$ by stratified sampling with two strata $S_{1}=[0,\\pi/2]$ and $S_{2}=[\\pi/2,\\pi]$, each of width $L_{1}=L_{2}=\\pi/2$. With $n_{1}=n_{2}=2$ samples per stratum, the stratified estimator is\n$$\n\\hat{I}=\\sum_{h=1}^{2}L_{h}\\,\\frac{1}{n_{h}}\\sum_{i=1}^{n_{h}}f(u_{hi})\n=\\frac{\\pi}{2}\\cdot\\frac{1}{2}\\left(\\sum_{i=1}^{2}f(u_{1i})+\\sum_{i=1}^{2}f(u_{2i})\\right)\n=\\frac{\\pi}{4}\\sum_{k=1}^{4}x_{k}\\sin(x_{k}),\n$$\nwhere $f(x)=x\\sin(x)$ and $x_{k}$ are the sampled points.\n\nThe sampling transformation within $[a,b]$ is $u=a+(b-a)r$. Using the given $r$ values, we obtain:\n$$\n\\begin{aligned}\n&\\text{In }S_{1}=[0,\\tfrac{\\pi}{2}]: &&x_{1}=0+\\tfrac{\\pi}{2}\\cdot 0.20=\\tfrac{\\pi}{10},\\quad x_{2}=0+\\tfrac{\\pi}{2}\\cdot 0.70=\\tfrac{7\\pi}{20},\\\\\n&\\text{In }S_{2}=[\\tfrac{\\pi}{2},\\pi]: &&x_{3}=\\tfrac{\\pi}{2}+\\tfrac{\\pi}{2}\\cdot 0.40=\\tfrac{7\\pi}{10},\\quad x_{4}=\\tfrac{\\pi}{2}+\\tfrac{\\pi}{2}\\cdot 0.90=\\tfrac{19\\pi}{20}.\n\\end{aligned}\n$$\nThus\n$$\n\\hat{I}=\\frac{\\pi}{4}\\left[\\tfrac{\\pi}{10}\\sin\\!\\left(\\tfrac{\\pi}{10}\\right)+\\tfrac{7\\pi}{20}\\sin\\!\\left(\\tfrac{7\\pi}{20}\\right)+\\tfrac{7\\pi}{10}\\sin\\!\\left(\\tfrac{7\\pi}{10}\\right)+\\tfrac{19\\pi}{20}\\sin\\!\\left(\\tfrac{19\\pi}{20}\\right)\\right].\n$$\nUsing exact trigonometric values where available,\n$$\n\\sin\\!\\left(\\tfrac{\\pi}{10}\\right)=\\frac{\\sqrt{5}-1}{4},\\qquad \\sin\\!\\left(\\tfrac{7\\pi}{10}\\right)=\\sin\\!\\left(\\tfrac{3\\pi}{10}\\right)=\\frac{\\sqrt{5}+1}{4},\n$$\nand numerical evaluations (in radians) for the others,\n$$\n\\sin\\!\\left(\\tfrac{7\\pi}{20}\\right)\\approx 0.8910065241883679,\\qquad \\sin\\!\\left(\\tfrac{19\\pi}{20}\\right)\\approx 0.1564344650402309.\n$$\nCompute the function values:\n$$\n\\begin{aligned}\nf(x_{1})&=\\tfrac{\\pi}{10}\\cdot\\frac{\\sqrt{5}-1}{4}\\approx 0.0970805519362733,\\\\\nf(x_{2})&=\\tfrac{7\\pi}{20}\\cdot 0.8910065241883679\\approx 0.9797128427417635,\\\\\nf(x_{3})&=\\tfrac{7\\pi}{10}\\cdot\\frac{\\sqrt{5}+1}{4}\\approx 1.7791212923103408,\\\\\nf(x_{4})&=\\tfrac{19\\pi}{20}\\cdot 0.1564344650402309\\approx 0.4668824448524499.\n\\end{aligned}\n$$\nSum these to obtain\n$$\n\\sum_{k=1}^{4}f(x_{k})\\approx 3.3227971318408276,\n$$\nand thus\n$$\n\\hat{I}=\\frac{\\pi}{4}\\cdot 3.3227971318408276\\approx 2.609718764690095.\n$$\nRounding to four significant figures gives $2.610$. (For reference, the exact integral is $I=\\pi$, so the stratified estimate is reasonable.)", "answer": "$$\\boxed{2.610}$$"}, {"introduction": "Next, we explore the method of control variates, a technique that cleverly leverages information from a simpler, related problem to improve the estimate of a more complex one. The core idea is to find a \"control\" variable $C$ that is correlated with our target function $f(X)$ and has a known expected value. We then use the deviation of the sampled control, $C - \\mathbb{E}[C]$, to adjust our estimate of $\\mathbb{E}[f(X)]$. The central challenge, as you will see in this practice [@problem_id:1348947], is to determine the optimal weighting coefficient $b^*$ that minimizes the variance of the adjusted estimator, thereby maximizing the efficiency of the technique.", "id": "1348947", "problem": "In Monte Carlo methods, a common task is to estimate the value of an integral $I = \\int_a^b g(x) dx$. This is often achieved by reformulating the integral as an expected value, $E[f(X)]$, where $X$ is a random variable and $f$ is a related function. A standard 'crude' Monte Carlo estimator then uses the sample mean of $N$ independent draws of $f(X_i)$.\n\nTo improve the precision of the estimation for a given number of samples, variance reduction techniques are employed. One such technique is the method of control variates. This involves finding a random variable $C$ that is correlated with $f(X)$ and has a known expectation, $\\mu_C = E[C]$. A new, improved estimator is then formed as $Z(b) = f(X) - b(C - \\mu_C)$, where $b$ is a constant coefficient. The value of $b$ is chosen to minimize the variance of $Z(b)$. This optimal coefficient, denoted $b^*$, is given by the formula:\n$$b^* = \\frac{\\text{Cov}(f(X), C)}{\\text{Var}(C)}$$\n\nConsider the problem of estimating the integral $I = \\int_0^1 \\cos\\left(\\frac{\\pi x}{2}\\right) dx$. This is equivalent to finding the expected value of $Y = \\cos\\left(\\frac{\\pi X}{2}\\right)$ where $X$ is a random variable uniformly distributed on the interval $[0, 1]$. We will use the random variable $C = X$ as a control variate, since it is correlated with $Y$ and its properties are easily determined.\n\nYour task is to calculate the exact theoretical value of the optimal coefficient $b^*$ for this control variate. Express your answer as a single closed-form analytic expression in terms of $\\pi$.\n\n", "solution": "We have $X \\sim \\text{Unif}[0,1]$, $Y=\\cos\\left(\\frac{\\pi X}{2}\\right)$, and $C=X$. The optimal control variate coefficient is\n$$\nb^{*}=\\frac{\\text{Cov}(Y,C)}{\\text{Var}(C)}.\n$$\nFirst compute $\\text{Var}(C)$. Using $E[X]=\\int_{0}^{1}x\\,dx=\\frac{1}{2}$ and $E[X^{2}]=\\int_{0}^{1}x^{2}\\,dx=\\frac{1}{3}$, we obtain\n$$\n\\text{Var}(C)=E[X^{2}]-\\left(E[X]\\right)^{2}=\\frac{1}{3}-\\frac{1}{4}=\\frac{1}{12}.\n$$\nNext compute $E[Y]$:\n$$\nE[Y]=\\int_{0}^{1}\\cos\\left(\\frac{\\pi x}{2}\\right)\\,dx=\\left.\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi x}{2}\\right)\\right|_{0}^{1}=\\frac{2}{\\pi}.\n$$\nNow compute $E[CY]=E\\!\\left[X\\cos\\left(\\frac{\\pi X}{2}\\right)\\right]$. Let $a=\\frac{\\pi}{2}$. Then\n$$\nE[CY]=\\int_{0}^{1}x\\cos(ax)\\,dx.\n$$\nUsing integration by parts with $u=x$, $dv=\\cos(ax)\\,dx$, so $du=dx$ and $v=\\frac{1}{a}\\sin(ax)$, we get\n$$\n\\int x\\cos(ax)\\,dx=\\frac{x\\sin(ax)}{a}+\\frac{\\cos(ax)}{a^{2}}.\n$$\nEvaluating from $0$ to $1$,\n$$\n\\int_{0}^{1}x\\cos(ax)\\,dx=\\frac{\\sin(a)}{a}+\\frac{\\cos(a)-1}{a^{2}}.\n$$\nWith $a=\\frac{\\pi}{2}$, we have $\\sin(a)=1$ and $\\cos(a)=0$, hence\n$$\nE[CY]=\\frac{1}{a}-\\frac{1}{a^{2}}=\\frac{2}{\\pi}-\\frac{4}{\\pi^{2}}.\n$$\nTherefore,\n$$\n\\text{Cov}(Y,C)=E[CY]-E[Y]E[C]=\\left(\\frac{2}{\\pi}-\\frac{4}{\\pi^{2}}\\right)-\\left(\\frac{2}{\\pi}\\right)\\left(\\frac{1}{2}\\right)=\\frac{1}{\\pi}-\\frac{4}{\\pi^{2}}=\\frac{\\pi-4}{\\pi^{2}}.\n$$\nFinally,\n$$\nb^{*}=\\frac{\\text{Cov}(Y,C)}{\\text{Var}(C)}=\\frac{\\frac{\\pi-4}{\\pi^{2}}}{\\frac{1}{12}}=\\frac{12(\\pi-4)}{\\pi^{2}}.\n$$", "answer": "$$\\boxed{\\frac{12(\\pi - 4)}{\\pi^{2}}}$$"}, {"introduction": "Our final practice delves into importance sampling, one of the most powerful and versatile variance reduction techniques. Instead of sampling from the original probability distribution, we sample from a different, carefully chosen \"proposal\" distribution that concentrates samples in the \"important\" regions of the function. This change in measure is corrected by a weighting factor to ensure the estimator remains unbiased. This advanced exercise [@problem_id:2446710] goes to the heart of the method by tasking you with not just applying importance sampling, but with theoretically deriving the optimal proposal distribution that minimizes the estimator's variance, revealing the true potential of this sophisticated approach.", "id": "2446710", "problem": "Consider the task of computing the expectation of the exponential of a normally distributed random variable in a Monte Carlo setting relevant to computational economics and finance. Let $X \\sim \\mathcal{N}(0, 100)$, and define $h(x) = \\exp(x)$. You will estimate the quantity $\\mathbb{E}[\\exp(X)]$ via importance sampling using a proposal distribution $Q$ given by $Y \\sim \\mathcal{N}(\\mu, 100)$ with unknown shift parameter $\\mu \\in \\mathbb{R}$. Let $p(x)$ and $q_{\\mu}(x)$ denote the probability density functions of $\\mathcal{N}(0, 100)$ and $\\mathcal{N}(\\mu, 100)$, respectively. The importance sampling estimator based on $n$ independent and identically distributed samples $\\{Y_i\\}_{i=1}^{n}$ from $q_{\\mu}$ is\n$$\nI_n(\\mu) \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} h(Y_i)\\, w_{\\mu}(Y_i), \\quad \\text{where } w_{\\mu}(y) \\;=\\; \\frac{p(y)}{q_{\\mu}(y)}.\n$$\nWorking from first principles and without invoking any specific variance reduction formulas, do the following:\n1. Derive a closed-form expression for $\\mathbb{E}[\\exp(X)]$.\n2. Derive $\\operatorname{Var}_{q_{\\mu}}(h(Y)\\,w_{\\mu}(Y))$ as a function of $\\mu$ and identify the value $\\mu^{\\star}$ that minimizes this variance over $\\mu \\in \\mathbb{R}$.\n\nProvide your final answer as a single ordered pair consisting of the exact value of $\\mathbb{E}[\\exp(X)]$ and the optimal shift $\\mu^{\\star}$. No numerical rounding is required, and your answer should be exact with no units.", "solution": "We begin by recalling the definitions. The target distribution is $X \\sim \\mathcal{N}(0, 100)$ with density\n$$\np(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma^{2} = 100,\n$$\nand the proposal distribution is $Y \\sim \\mathcal{N}(\\mu, 100)$ with density\n$$\nq_{\\mu}(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma^{2} = 100.\n$$\nThe importance weight is $w_{\\mu}(x) = p(x)/q_{\\mu}(x)$.\n\nStep 1: Compute $\\mathbb{E}[\\exp(X)]$ in closed form. By definition,\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\int_{-\\infty}^{\\infty} \\exp(x)\\, p(x)\\, dx \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(x - \\frac{x^{2}}{2\\sigma^{2}}\\right) dx.\n$$\nComplete the square in the exponent. Write\n$$\nx - \\frac{x^{2}}{2\\sigma^{2}} \\;=\\; -\\frac{1}{2\\sigma^{2}}\\left(x^{2} - 2\\sigma^{2} x \\right) \\;=\\; -\\frac{1}{2\\sigma^{2}}\\left[(x - \\sigma^{2})^{2} - \\sigma^{4}\\right] \\;=\\; -\\frac{(x - \\sigma^{2})^{2}}{2\\sigma^{2}} + \\frac{\\sigma^{2}}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\exp\\!\\left(\\frac{\\sigma^{2}}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x - \\sigma^{2})^{2}}{2\\sigma^{2}}\\right) dx \\;=\\; \\exp\\!\\left(\\frac{\\sigma^{2}}{2}\\right),\n$$\nsince the integral equals $\\sqrt{2\\pi}\\, \\sigma$. With $\\sigma^{2} = 100$, this yields\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\exp(50).\n$$\n\nStep 2: Derive $\\operatorname{Var}_{q_{\\mu}}(h(Y)\\, w_{\\mu}(Y))$ and minimize it over $\\mu$. Note that\n$$\n\\operatorname{Var}_{q_{\\mu}}(h(Y)\\, w_{\\mu}(Y)) \\;=\\; \\mathbb{E}_{q_{\\mu}}\\!\\left[h(Y)^{2}\\, w_{\\mu}(Y)^{2}\\right] \\;-\\; \\left(\\mathbb{E}[\\exp(X)]\\right)^{2}.\n$$\nThe second term does not depend on $\\mu$, so minimizing the variance over $\\mu$ is equivalent to minimizing the second moment\n$$\nM(\\mu) \\;=\\; \\mathbb{E}_{q_{\\mu}}\\!\\left[h(Y)^{2}\\, w_{\\mu}(Y)^{2}\\right] \\;=\\; \\int_{-\\infty}^{\\infty} \\exp(2x)\\, \\frac{p(x)^{2}}{q_{\\mu}(x)}\\, dx.\n$$\nUsing the explicit forms of $p$ and $q_{\\mu}$ with common variance $\\sigma^{2}$,\n$$\n\\frac{p(x)^{2}}{q_{\\mu}(x)} \\;=\\; \\frac{\\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\right)^{2}}{\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)} \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nHence,\n$$\nM(\\mu) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(2x - \\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx.\n$$\nCombine exponents:\n$$\n2x - \\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}} \\;=\\; -\\frac{x^{2}}{2\\sigma^{2}} + \\left(2 - \\frac{\\mu}{\\sigma^{2}}\\right) x + \\frac{\\mu^{2}}{2\\sigma^{2}}.\n$$\nComplete the square by defining\n$$\nm \\;=\\; 2\\sigma^{2} - \\mu,\n$$\nso that\n$$\n-\\frac{x^{2}}{2\\sigma^{2}} + \\left(2 - \\frac{\\mu}{\\sigma^{2}}\\right) x \\;=\\; -\\frac{(x - m)^{2}}{2\\sigma^{2}} + \\frac{m^{2}}{2\\sigma^{2}}.\n$$\nTherefore,\n$$\nM(\\mu) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(\\frac{m^{2}}{2\\sigma^{2}} + \\frac{\\mu^{2}}{2\\sigma^{2}}\\right) \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x - m)^{2}}{2\\sigma^{2}}\\right) dx \\;=\\; \\exp\\!\\left(\\frac{m^{2} + \\mu^{2}}{2\\sigma^{2}}\\right).\n$$\nSubstituting $m = 2\\sigma^{2} - \\mu$,\n$$\nm^{2} + \\mu^{2} \\;=\\; (2\\sigma^{2} - \\mu)^{2} + \\mu^{2} \\;=\\; 4\\sigma^{4} - 4\\mu \\sigma^{2} + 2\\mu^{2},\n$$\nhence\n$$\nM(\\mu) \\;=\\; \\exp\\!\\left(\\frac{4\\sigma^{4} - 4\\mu \\sigma^{2} + 2\\mu^{2}}{2\\sigma^{2}}\\right) \\;=\\; \\exp\\!\\left(2\\sigma^{2} - 2\\mu + \\frac{\\mu^{2}}{\\sigma^{2}}\\right).\n$$\nAs a function of $\\mu$, minimizing $M(\\mu)$ is equivalent to minimizing the exponent\n$$\ng(\\mu) \\;=\\; \\frac{\\mu^{2}}{\\sigma^{2}} - 2\\mu + 2\\sigma^{2}.\n$$\nThis is a convex quadratic in $\\mu$ with derivative\n$$\ng'(\\mu) \\;=\\; \\frac{2\\mu}{\\sigma^{2}} - 2,\n$$\nwhich vanishes at\n$$\n\\mu^{\\star} \\;=\\; \\sigma^{2}.\n$$\nSince $g''(\\mu) = 2/\\sigma^{2} > 0$, this critical point is the unique minimizer. With $\\sigma^{2} = 100$, the optimal shift is\n$$\n\\mu^{\\star} \\;=\\; 100.\n$$\nAt $\\mu^{\\star} = \\sigma^{2}$, the proposal density $q_{\\mu^{\\star}}$ is proportional to $h(x)\\, p(x)$, which yields a zero-variance importance sampling estimator, consistent with\n$$\n\\operatorname{Var}_{q_{\\mu^{\\star}}}(h(Y)\\,w_{\\mu^{\\star}}(Y)) \\;=\\; M(\\mu^{\\star}) - \\left(\\mathbb{E}[\\exp(X)]\\right)^{2} \\;=\\; \\exp(\\sigma^{2}) - \\exp(\\sigma^{2}) \\;=\\; 0.\n$$\nCollecting results, the exact expectation is $\\exp(50)$ and the optimal shift is $\\mu^{\\star} = 100$.", "answer": "$$\\boxed{\\begin{pmatrix}\\exp(50) & 100\\end{pmatrix}}$$"}]}