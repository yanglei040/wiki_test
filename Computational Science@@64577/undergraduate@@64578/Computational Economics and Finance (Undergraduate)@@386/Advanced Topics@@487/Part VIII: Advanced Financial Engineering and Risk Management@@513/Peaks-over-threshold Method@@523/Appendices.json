{"hands_on_practices": [{"introduction": "The first and most critical step in any Peaks-over-threshold (POT) analysis is selecting an appropriate threshold $u$. The underlying theory guarantees that the Generalized Pareto Distribution (GPD) is a good model for exceedances above a *sufficiently high* threshold, which creates a practical bias-variance trade-off. This exercise [@problem_id:2418694] guides you through an essential diagnostic procedure, exploring how the estimated tail index $\\xi$ changes as the threshold is varied. By observing this sensitivity, you gain firsthand experience with a core challenge in extreme value analysis and learn the importance of threshold stability plots in practice.", "id": "2418694", "problem": "Consider a synthetic daily log-return series intended to approximate the distributional features of the Standard and Poor’s 500 (S&amp;P 500) daily returns for the purpose of tail risk analysis. Let the daily log-returns be denoted by $R_t$ for $t = 1, \\dots, n$, and define $R_t = \\mu + \\sigma \\cdot T_\\nu$, where $T_\\nu$ is a Student’s $t$ random variable with degrees of freedom $\\nu$, location $0$, and unit scale. Use $n = 6000$, $\\mu = 0$, $\\sigma = 0.01$, $\\nu = 5$, and a fixed pseudorandom number generator seed $s = 20240517$ to ensure reproducibility. Define losses by $L_t = -R_t$.\n\nFor any threshold $u$, define the exceedances by $Y_i = L_i - u$ for all indices $i$ such that $L_i > u$. Under the Peaks-Over-Threshold (POT) model, assume the exceedances $Y_i$ follow a Generalized Pareto Distribution (GPD) with tail index $\\xi$ and scale $\\beta$.\n\nYour task is to quantify the sensitivity of the estimated tail index $\\xi$ to the choice of threshold $u$ by computing, for each threshold in the test suite described below, the estimate of $\\xi$ obtained from the exceedances over $u$. For each threshold, set $u$ equal to the empirical $q$-quantile of the losses $L_t$ for the given quantile level $q$.\n\nTest suite (quantile levels for thresholds): $q \\in \\{0.80, 0.90, 0.95, 0.975, 0.99, 0.995, 0.999\\}$.\n\nFor each $q$ in the test suite, compute the corresponding estimate of $\\xi$ as a real number. The required final output is a single line containing the results for the thresholds in the specified order, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets. For example, an output with three results should look like $[x_1,x_2,x_3]$ where each $x_j$ is a decimal number rounded to $6$ decimal places.\n\nYour program must produce a single line of output containing the estimates in the exact required format and order, with no additional text. No physical units are involved. Angles are not involved. Percentages are not involved. The answers must be real-valued floats.", "solution": "The problem statement submitted for analysis is deemed valid. It presents a well-posed, scientifically grounded problem in computational finance, specifically concerning the application of the Peaks-Over-Threshold (POT) method from Extreme Value Theory (EVT). All parameters and conditions are specified, allowing for a unique and reproducible solution. The methodology rests on established statistical principles.\n\nThe core of the problem lies in the application of the Pickands–Balkema–de Haan theorem. This theorem posits that, for a wide class of distributions with heavy tails, the conditional distribution of exceedances over a sufficiently high threshold converges to a Generalized Pareto Distribution (GPD). The data model specified, $R_t = \\mu + \\sigma \\cdot T_\\nu$, where $T_\\nu$ is a random variable from a Student's $t$-distribution with $\\nu$ degrees of freedom, is a canonical example of such a heavy-tailed process. For a Student's $t$-distribution with $\\nu$ degrees of freedom, the theoretical GPD tail index is $\\xi = 1/\\nu$. In this problem, with $\\nu=5$, the expected theoretical value for the tail index is $\\xi = 1/5 = 0.2$. The exercise is to estimate this parameter from a finite sample and observe its sensitivity to the threshold choice.\n\nThe computational procedure is as follows:\n\n1.  **Data Generation**: A synthetic dataset is created to model financial losses. A sample of $n=6000$ points is generated from a Student's $t$-distribution with $\\nu=5$ degrees of freedom, location $0$, and unit scale. These points, denoted $T_t$, are transformed into log-returns $R_t = \\mu + \\sigma \\cdot T_t$ using the provided parameters $\\mu=0$ and $\\sigma=0.01$. The corresponding losses are then defined as $L_t = -R_t$. The use of a fixed seed, $s=20240517$, ensures the absolute reproducibility of this generated data.\n\n2.  **Threshold Selection**: The problem requires an analysis of sensitivity to the threshold $u$. For this purpose, a series of thresholds is chosen based on the empirical quantiles of the generated loss data $L_t$. For each quantile level $q$ in the test suite $\\{0.80, 0.90, 0.95, 0.975, 0.99, 0.995, 0.999\\}$, the threshold $u$ is set to the value such that $P(L_t \\le u) = q$.\n\n3.  **Exceedance Calculation**: For each threshold $u$, the set of exceedances is compiled. These are the values $Y_i = L_i - u$ for all observations $L_i$ that are strictly greater than $u$. This set of $Y_i$ values represents the data to which the GPD will be fitted.\n\n4.  **GPD Parameter Estimation**: The tail index $\\xi$ of the GPD is estimated from the series of exceedances $\\{Y_i\\}$. The standard and most reliable method for this estimation is Maximum Likelihood Estimation (MLE), which will be employed here. The GPD is characterized by a shape parameter (the tail index $\\xi$) and a scale parameter $\\beta$. According to the POT model, exceedances are inherently positive, so the GPD's location parameter is theoretically $0$. This constraint is enforced during the fitting process (using `floc=0` in the `scipy` implementation) to improve the stability and theoretical correctness of the estimate. The MLE procedure computes the value of $\\xi$ that maximizes the probability of observing the collected exceedance data.\n\nThis entire process, from threshold selection to GPD fitting, is repeated for each quantile level $q$ in the test suite. The resulting sequence of estimated tail indices, $\\hat{\\xi}(q)$, demonstrates the practical effect of threshold choice on tail risk measurement. It is expected that as $q$ approaches $1$, the threshold $u$ increases, the GPD approximation becomes more accurate, and the estimated $\\hat{\\xi}$ should converge towards the theoretical value of $0.2$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t, genpareto\n\ndef solve():\n    \"\"\"\n    Computes the sensitivity of the GPD tail index estimate to the threshold choice\n    for a synthetic financial loss series based on the Peaks-Over-Threshold method.\n    \"\"\"\n    # --- Problem Parameters ---\n    n = 6000\n    mu = 0.0\n    sigma = 0.01\n    nu = 5.0\n    seed = 20240517\n    \n    # --- Test Suite ---\n    # The quantile levels for threshold selection define the test cases.\n    test_cases = [0.80, 0.90, 0.95, 0.975, 0.99, 0.995, 0.999]\n    \n    # --- Data Generation ---\n    # Use a specific random number generator for reproducibility as per problem statement.\n    rng = np.random.default_rng(seed)\n    \n    # Generate random variates from Student's t-distribution with nu degrees of freedom.\n    # T_nu is a standard t-distribution (location=0, scale=1).\n    T_nu = t.rvs(df=nu, size=n, random_state=rng)\n    \n    # Calculate log-returns R_t and losses L_t based on the generated variates.\n    R_t = mu + sigma * T_nu\n    L_t = -R_t\n    \n    # --- Main Logic: POT Analysis for each threshold ---\n    results = []\n    for q in test_cases:\n        # 1. Set the threshold 'u' as the empirical q-quantile of the losses.\n        u = np.quantile(L_t, q)\n        \n        # 2. Identify all losses exceeding the threshold and compute the exceedance values.\n        # Exceedances are defined as Y_i = L_i - u for all L_i > u.\n        exceedances = L_t[L_t > u] - u\n        \n        # 3. Fit a Generalized Pareto Distribution (GPD) to the exceedances.\n        # We use Maximum Likelihood Estimation (MLE), as implemented in scipy.\n        # The POT model implies a location parameter of 0 for exceedances, which we fix\n        # using the 'floc=0' argument for theoretical consistency and numerical stability.\n        # The 'c' shape parameter returned by the fit corresponds to the tail index 'xi'.\n        xi, _, _ = genpareto.fit(exceedances, floc=0)\n        \n        results.append(xi)\n\n    # --- Final Output Formatting ---\n    # The problem requires the results to be rounded to 6 decimal places and\n    # formatted as a comma-separated list within square brackets.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"}, {"introduction": "A key test of understanding a model is to see if its parameters behave as theory predicts under transformation. The tail index $\\xi$ represents a fundamental property of a distribution's extremes, and theory states that when a heavy-tailed process is combined with an independent, light-tailed one, the tail index of the sum is governed by the heavy-tailed component. In this practice [@problem_id:2418692], you will test this powerful result in a tangible financial context by modeling an asset's returns in both a stable currency and a high-inflation one. This provides a concrete demonstration of the robustness of the tail index and deepens your intuition about what it truly measures.", "id": "2418692", "problem": "Consider an asset priced in United States Dollar (USD) and a foreign currency undergoing high inflation, referred to here as the High-Inflation Currency (HIC). Let $r^{USD}_t$ denote the asset's single-period log-return in USD and let $r^{FX}_t$ denote the single-period log-return of the exchange rate measured in HIC per USD. The single-period log-return of the asset in the HIC is defined as $r^{HIC}_t = r^{USD}_t + r^{FX}_t$. Assume that $\\{r^{USD}_t\\}_{t=1}^N$ are independent and identically distributed draws from a Student-$t$ distribution with $\\nu$ degrees of freedom, scaled by $\\sigma_S$ and with mean $0$, and that $\\{r^{FX}_t\\}_{t=1}^N$ are independent and identically distributed draws from a Normal distribution with mean $\\mu_{FX}$ and standard deviation $\\sigma_{FX}$. Assume independence between $\\{r^{USD}_t\\}$ and $\\{r^{FX}_t\\}$. All returns and parameters are dimensionless decimals per period (for example, $0.01$ denotes one percent per period and must be treated as a decimal, not with a percentage sign). Angles do not appear in this problem.\n\nFor each test case, you must perform the following steps using the provided parameters $(\\text{seed}, N, \\nu, \\sigma_S, \\mu_{FX}, \\sigma_{FX}, q)$:\n1. Initialize a pseudorandom number generator with the given integer $\\text{seed}$ and draw $N$ independent realizations of $r^{USD}_t$ and $r^{FX}_t$ as specified above. Construct $r^{HIC}_t = r^{USD}_t + r^{FX}_t$ for $t = 1,\\dots,N$.\n2. For each series $X \\in \\{r^{USD}, r^{HIC}\\}$, compute the empirical $q$-quantile threshold $u^X$ and form the exceedances $Y^X = \\{X_t - u^X \\mid X_t > u^X\\}$.\n3. Under the Peaks-Over-Threshold (POT) framework, assume the exceedances $Y^X$ follow a Generalized Pareto Distribution (GPD) with location fixed at $0$, scale $\\beta^X > 0$, and shape (tail index) $\\xi^X \\in \\mathbb{R}$. Estimate the tail index $\\xi^X$ by maximum likelihood for each $X \\in \\{r^{USD}, r^{HIC}\\}$ based solely on $Y^X$.\n4. For each test case, produce the triple $(\\hat{\\xi}^{USD}, \\hat{\\xi}^{HIC}, \\hat{\\xi}^{HIC} - \\hat{\\xi}^{USD})$.\n\nUse the following test suite, which is designed to probe different regimes, including a general case, an equality boundary, a high-threshold edge case, and an almost light-tailed case:\n- Case A (general heavy-tailed USD with high inflation FX): $(\\text{seed} = 12345, N = 200000, \\nu = 4, \\sigma_S = 0.02, \\mu_{FX} = 0.01, \\sigma_{FX} = 0.015, q = 0.95)$.\n- Case B (boundary: no FX component so USD and HIC coincide): $(\\text{seed} = 67890, N = 200000, \\nu = 4, \\sigma_S = 0.02, \\mu_{FX} = 0.0, \\sigma_{FX} = 0.0, q = 0.95)$.\n- Case C (edge: very high threshold with fewer exceedances): $(\\text{seed} = 54321, N = 50000, \\nu = 3, \\sigma_S = 0.03, \\mu_{FX} = 0.01, \\sigma_{FX} = 0.02, q = 0.995)$.\n- Case D (near-light-tailed USD; small positive tail index): $(\\text{seed} = 98765, N = 150000, \\nu = 50, \\sigma_S = 0.01, \\mu_{FX} = 0.005, \\sigma_{FX} = 0.01, q = 0.98)$.\n\nYour program must output a single line containing a list of the four triples in the order A, B, C, D. Each triple must be a list of three floating-point numbers rounded to six decimal places, representing $(\\hat{\\xi}^{USD}, \\hat{\\xi}^{HIC}, \\hat{\\xi}^{HIC} - \\hat{\\xi}^{USD})$. The final output must therefore be a single line of the form\n\"[ [a1,a2,a3],[b1,b2,b3],[c1,c2,c3],[d1,d2,d3] ]\"\nwith no additional characters or whitespace requirements beyond commas and brackets. The output must contain only this single line.", "solution": "We compare the tail index estimated via the Peaks-Over-Threshold (POT) paradigm in two currencies. The POT method states that if an independent and identically distributed series has a sufficiently high threshold $u$, the distribution of exceedances $Y = X - u \\mid X > u$ is approximately Generalized Pareto Distribution (GPD). The GPD with location fixed at $0$ has density\n$$\nf(y \\mid \\xi, \\beta) =\n\\begin{cases}\n\\dfrac{1}{\\beta}\\left(1 + \\dfrac{\\xi y}{\\beta}\\right)^{-1/\\xi - 1}, & \\xi \\neq 0,\\; y \\ge 0,\\; 1 + \\dfrac{\\xi y}{\\beta} > 0,\\\n$$8pt]\n\\dfrac{1}{\\beta}\\exp\\!\\left(-\\dfrac{y}{\\beta}\\right), & \\xi = 0,\\; y \\ge 0,\n\\end{cases}\n$$\nwith $\\beta > 0$. The shape parameter $\\xi$ is the tail index: $\\xi > 0$ corresponds to heavy tails (Fréchet domain), $\\xi = 0$ corresponds to light exponential tails (Gumbel domain), and $\\xi < 0$ corresponds to a finite right endpoint (Weibull domain).\n\nModeling the returns:\n- The USD log-return $r^{USD}_t$ is drawn from a Student-$t$ distribution with degrees of freedom $\\nu$, scaled by $\\sigma_S$ and centered at $0$. Let $Z_t \\sim t_\\nu$ with the standard parameterization; then $r^{USD}_t = \\sigma_S Z_t$.\n- The FX log-return $r^{FX}_t$ is independent and distributed as Normal$(\\mu_{FX}, \\sigma^2_{FX})$.\n- The HIC log-return is $r^{HIC}_t = r^{USD}_t + r^{FX}_t$.\n\nFor each series $X \\in \\{r^{USD}, r^{HIC}\\}$, we compute the empirical $q$-quantile $u^X$ to define exceedances $Y^X = \\{X_t - u^X : X_t > u^X\\}$. Under the POT assumption, we estimate $(\\xi^X, \\beta^X)$ by maximum likelihood using the exceedances $Y^X$ and a Generalized Pareto Distribution (GPD) with location fixed at $0$. The log-likelihood for $k$ exceedances $y_1,\\dots,y_k$ is\n$$\n\\ell(\\xi, \\beta) =\n\\begin{cases}\n- k \\log \\beta - \\left(1 + \\dfrac{1}{\\xi}\\right) \\sum_{i=1}^{k} \\log\\!\\left(1 + \\dfrac{\\xi y_i}{\\beta}\\right), & \\xi \\neq 0,\\\n$$8pt]\n- k \\log \\beta - \\dfrac{1}{\\beta}\\sum_{i=1}^{k} y_i, & \\xi = 0,\n\\end{cases}\n$$\nfor $\\beta > 0$ and $1 + \\dfrac{\\xi y_i}{\\beta} > 0$ for all $i$. The maximum likelihood estimator (MLE) $\\hat{\\xi}$ is obtained by maximizing $\\ell(\\xi,\\beta)$ (equivalently minimizing the negative log-likelihood) with respect to $(\\xi, \\beta)$ subject to these constraints.\n\nAlgorithmic outline integrated with the statistical principles:\n1. For each test case, set the random number generator seed to the provided integer value to ensure reproducibility, then draw $N$ independent samples $Z_t \\sim t_\\nu$ and $W_t \\sim \\mathcal{N}(0,1)$. Construct $r^{USD}_t = \\sigma_S Z_t$ and $r^{FX}_t = \\mu_{FX} + \\sigma_{FX} W_t$, and finally $r^{HIC}_t = r^{USD}_t + r^{FX}_t$.\n2. For $X \\in \\{r^{USD}, r^{HIC}\\}$, compute $u^X$ as the empirical $q$-quantile of the $N$ draws $\\{X_t\\}_{t=1}^{N}$, then form the exceedances $Y^X$ by subtracting $u^X$ from all observations exceeding $u^X$.\n3. Fit a GPD with location fixed at $0$ to each exceedance sample $Y^X$ by maximum likelihood to obtain $\\hat{\\xi}^X$. Since the negative log-likelihood is well-known and the constraints are explicit, standard optimization routines or specialized extreme value distribution fitting routines can be used to obtain the MLEs.\n4. Report $(\\hat{\\xi}^{USD}, \\hat{\\xi}^{HIC}, \\hat{\\xi}^{HIC} - \\hat{\\xi}^{USD})$.\n\nInterpretation expectations grounded in first principles:\n- In Cases A and C, $r^{USD}$ is heavy-tailed due to the Student-$t$ component with $\\nu \\in \\{4, 3\\}$, so $\\xi^{USD} > 0$. The HIC return adds an independent Gaussian component with positive mean $\\mu_{FX} > 0$; Gaussian summands are light-tailed and do not change the tail index of the sum in the heavy-tailed regime. Therefore, $\\hat{\\xi}^{HIC}$ should be close to $\\hat{\\xi}^{USD}$ in large samples, and the reported difference should be small.\n- In Case B, the FX component is identically zero, so $r^{HIC} \\equiv r^{USD}$; the estimated tail indices must coincide up to numerical tolerance, implying a difference extremely close to $0$.\n- In Case D, the Student-$t$ distribution has $\\nu = 50$; its tail index satisfies $\\xi \\approx 1/\\nu$, so $\\xi \\approx 0.02$, reflecting an almost light-tailed regime. Adding a Gaussian component again should not change the estimated tail index materially.\n\nThe final program implements these steps for the specified test suite and prints a single line containing the list of four triples, each entry rounded to six decimal places, as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import genpareto\n\ndef simulate_returns(seed, N, nu, sigma_S, mu_fx, sigma_fx):\n    \"\"\"\n    Simulate USD and HIC log-returns.\n\n    Parameters:\n        seed (int): RNG seed\n        N (int): sample size\n        nu (float): degrees of freedom for Student-t\n        sigma_S (float): scale for USD returns\n        mu_fx (float): mean for FX returns\n        sigma_fx (float): std dev for FX returns\n\n    Returns:\n        r_usd (np.ndarray), r_hic (np.ndarray)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Student-t random variates (standard parameterization), scaled by sigma_S.\n    r_usd = sigma_S * rng.standard_t(df=nu, size=N)\n    # Gaussian FX returns.\n    if sigma_fx == 0.0:\n        r_fx = np.full(N, mu_fx, dtype=float)\n    else:\n        r_fx = mu_fx + sigma_fx * rng.standard_normal(size=N)\n    r_hic = r_usd + r_fx\n    return r_usd, r_hic\n\ndef fit_gpd_xi(exceedances):\n    \"\"\"\n    Fit GPD to exceedances with location fixed at 0 and return the shape xi.\n\n    Parameters:\n        exceedances (np.ndarray): positive exceedances (X - u) | X > u\n\n    Returns:\n        xi_hat (float): estimated tail index (shape)\n    \"\"\"\n    # Use SciPy's MLE for genpareto with loc fixed at 0\n    # genpareto.fit returns (c=xi, loc, scale)\n    # We fix loc=0 as required by POT exceedances definition.\n    # Add a small jitter if necessary (but here exceedances are strictly >=0).\n    # Ensure numeric stability: if all exceedances are 0 (degenerate), fall back to xi=0\n    y = np.asarray(exceedances, dtype=float)\n    if y.size == 0:\n        # No exceedances: undefined; return NaN\n        return float(\"nan\")\n    if np.all(y == 0.0):\n        return 0.0\n    try:\n        c, loc, scale = genpareto.fit(y, floc=0.0)\n        xi_hat = float(c)\n    except Exception:\n        # Fallback: method-of-moments-like rough estimate using PWM (Pickands estimator variant)\n        # Pickands estimator using 1/2 and 1/4 quantiles of exceedances (requires enough data).\n        ys = np.sort(y)\n        k = len(ys)\n        # Use indices at 1/2 and 1/4 fraction positions\n        i2 = max(1, int(np.floor(0.5 * k)) - 1)\n        i4 = max(1, int(np.floor(0.25 * k)) - 1)\n        if ys[i2] == 0:\n            xi_hat = 0.0\n        else:\n            xi_hat = (ys[i4] - ys[i2]) / (ys[i2] - ys[0]) - 1.0\n    return xi_hat\n\ndef estimate_tail_indices_for_case(case):\n    \"\"\"\n    For a single test case, simulate returns, compute thresholds, exceedances, and estimate xi.\n\n    Parameters:\n        case (tuple): (seed, N, nu, sigma_S, mu_fx, sigma_fx, q)\n\n    Returns:\n        (xi_usd_hat, xi_hic_hat, delta) as floats\n    \"\"\"\n    seed, N, nu, sigma_S, mu_fx, sigma_fx, q = case\n    r_usd, r_hic = simulate_returns(seed, N, nu, sigma_S, mu_fx, sigma_fx)\n\n    # Compute thresholds\n    u_usd = np.quantile(r_usd, q)\n    u_hic = np.quantile(r_hic, q)\n\n    # Exceedances\n    y_usd = r_usd[r_usd > u_usd] - u_usd\n    y_hic = r_hic[r_hic > u_hic] - u_hic\n\n    # Fit GPD and get xi\n    xi_usd = fit_gpd_xi(y_usd)\n    xi_hic = fit_gpd_xi(y_hic)\n    delta = xi_hic - xi_usd\n    return xi_usd, xi_hic, delta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (seed, N, nu, sigma_S, mu_fx, sigma_fx, q)\n    test_cases = [\n        (12345, 200000, 4.0, 0.02, 0.01, 0.015, 0.95),   # Case A\n        (67890, 200000, 4.0, 0.02, 0.0, 0.0, 0.95),      # Case B\n        (54321, 50000, 3.0, 0.03, 0.01, 0.02, 0.995),    # Case C\n        (98765, 150000, 50.0, 0.01, 0.005, 0.01, 0.98),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        xi_usd, xi_hic, delta = estimate_tail_indices_for_case(case)\n        # Round to six decimals for output formatting\n        results.append([xi_usd, xi_hic, delta])\n\n    # Format as required: a single line list of four triples, with six decimals.\n    def fmt(x):\n        # Ensure proper rounding and fixed number of decimals\n        if np.isnan(x):\n            return \"nan\"\n        return f\"{x:.6f}\"\n\n    formatted = \"[\" + \",\".join(\n        [\"[\" + \",\".join(fmt(x) for x in trip) + \"]\" for trip in results]\n    ) + \"]\"\n\n    # Final print statement in the exact required format.\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```"}, {"introduction": "Moving beyond estimation, a primary goal of financial modeling is to determine whether underlying risk processes have changed over time, for example, after a major financial crisis. By estimating the GPD tail index $\\xi$ for two different periods, we can use formal statistical hypothesis testing to assess if an observed difference is significant or simply due to random chance. This final practice [@problem_id:2418723] walks you through the complete process of conducting a Wald test for a structural break in tail risk. You will learn not only to estimate the parameters but also to calculate their standard errors and use them to draw statistically rigorous conclusions, a cornerstone skill in quantitative analysis.", "id": "2418723", "problem": "Consider two independent samples of simulated daily log-returns representing pre-crisis and post-crisis periods of a financial market. The peaks-over-threshold framework assumes that for a high threshold $u$, the conditional distribution of exceedances $Y = X - u \\mid X > u$ is approximately Generalized Pareto with shape parameter $\\xi$ and scale parameter $\\beta$. The Generalized Pareto Distribution (GPD) with parameters $(\\xi,\\beta)$ has support $y \\ge 0$ when $\\xi \\ge 0$ and $0 \\le y < -\\beta/\\xi$ when $\\xi < 0$, and cumulative distribution function\n$$\nF(y \\mid \\xi,\\beta) = \n\\begin{cases}\n1 - \\left(1 + \\dfrac{\\xi y}{\\beta}\\right)^{-1/\\xi}, & \\xi \\ne 0, \\\\\n1 - \\exp\\!\\left(-\\dfrac{y}{\\beta}\\right), & \\xi = 0,\n\\end{cases}\n\\quad \\text{for } \\beta > 0.\n$$\nThe finite-sample negative log-likelihood for exceedances $(y_1,\\dots,y_k)$ under the GPD model is\n$$\n\\ell(\\xi,\\beta; y_{1:k}) =\n\\begin{cases}\nk \\log \\beta + \\left(1+\\dfrac{1}{\\xi}\\right)\\displaystyle\\sum_{i=1}^k \\log\\!\\left(1 + \\dfrac{\\xi y_i}{\\beta}\\right), & \\xi \\ne 0, \\\\\nk \\log \\beta + \\dfrac{1}{\\beta}\\displaystyle\\sum_{i=1}^k y_i, & \\xi = 0,\n\\end{cases}\n$$\nsubject to the support constraint $1 + \\xi y_i/\\beta > 0$ for all $i$ and $\\beta > 0$. The maximum likelihood estimator $(\\hat{\\xi},\\hat{\\beta})$ is defined as any pair that minimizes $\\ell(\\xi,\\beta; y_{1:k})$. The observed information matrix is the Hessian of $\\ell$ evaluated at $(\\hat{\\xi},\\hat{\\beta})$, and its inverse approximates the covariance matrix of $(\\hat{\\xi},\\hat{\\beta})$; in particular, the approximate variance of $\\hat{\\xi}$ is the $(1,1)$ entry of that inverse.\n\nYou will test whether the shape (tail index) parameter changed between pre- and post-crisis periods using a two-sided Wald test at significance level $\\alpha = 0.05$:\n$$\nH_0: \\ \\xi_{\\text{pre}} = \\xi_{\\text{post}}\n\\quad \\text{versus} \\quad\nH_1: \\ \\xi_{\\text{pre}} \\ne \\xi_{\\text{post}}.\n$$\nLet $\\hat{\\xi}_{\\text{pre}}$ and $\\hat{\\xi}_{\\text{post}}$ be the maximum likelihood estimators from the respective samples, with approximate standard errors $s_{\\text{pre}}$ and $s_{\\text{post}}$ obtained from the observed-information covariance matrices. The Wald statistic is\n$$\nZ = \\dfrac{\\hat{\\xi}_{\\text{pre}} - \\hat{\\xi}_{\\text{post}}}{\\sqrt{s_{\\text{pre}}^2 + s_{\\text{post}}^2}},\n$$\nand $H_0$ is rejected if $|Z| \\ge z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution.\n\nData generation for each period is as follows. Fix a base threshold level $u_0 = 0$ and a tail mixing probability $p_{\\text{tail}} \\in (0,1)$. Generate $n$ independent observations by concatenating:\n- A “body” of size $n - m$ with $m = \\lfloor p_{\\text{tail}} n \\rceil$ drawn from a continuous distribution supported on $(-\\infty, u_0]$; use the uniform distribution on $[-3, u_0]$.\n- A “tail” of size $m$ constructed as $u_0 + Y$, where $Y$ are independent GPD$(\\xi,\\beta)$ random variables with parameters $(\\xi,\\beta)$ specified for the period.\n\nFor each period and test case, set the threshold $u$ to the empirical $q$-quantile of the simulated sample, with $q = 0.9$. Define exceedances as $Y_i = X_i - u$ for all $X_i > u$.\n\nYour program must, for each test case below:\n1. Simulate the pre-crisis and post-crisis samples using the specified parameters and independent random seeds.\n2. Compute the empirical threshold $u$ at quantile level $q = 0.9$ separately for each period.\n3. Form the exceedances $Y_i$ above the respective thresholds.\n4. Compute the maximum likelihood estimator $(\\hat{\\xi},\\hat{\\beta})$ for each period by minimizing the exact finite-sample negative log-likelihood $\\ell(\\xi,\\beta; y_{1:k})$ under the support constraint.\n5. Approximate the standard error of $\\hat{\\xi}$ for each period using the inverse observed information matrix at the maximum likelihood estimator.\n6. Perform the two-sided Wald test of $H_0: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$ at significance level $\\alpha = 0.05$.\n7. Output a boolean indicating whether there is a statistically significant change in the tail index (output true if $H_0$ is rejected, and false otherwise).\n\nTest suite (each tuple corresponds to one test case in the order: $(\\text{seed}_{\\text{pre}}, \\text{seed}_{\\text{post}}, n_{\\text{pre}}, n_{\\text{post}}, \\xi_{\\text{pre}}, \\beta_{\\text{pre}}, \\xi_{\\text{post}}, \\beta_{\\text{post}})$), with common $u_0 = 0$, $p_{\\text{tail}} = 0.25$, $q = 0.9$, and $\\alpha = 0.05$:\n- Case A: $(12345, 54321, 5000, 5000, 0.2, 1.0, 0.6, 1.0)$.\n- Case B: $(111, 222, 4000, 4000, 0.2, 1.0, 0.2, 1.0)$.\n- Case C: $(333, 444, 6000, 6000, 0.01, 1.0, 0.0, 1.0)$.\n- Case D: $(555, 666, 6000, 6000, -0.15, 1.2, 0.15, 1.2)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example: \"[true,false,true,false]\". The booleans must be all lowercase.", "solution": "The problem requires performing a statistical hypothesis test to determine if the tail behavior of financial returns, modeled by a Generalized Pareto Distribution (GPD), has changed between two periods. This is a standard application of Extreme Value Theory (EVT) in quantitative finance. The validation process confirms that the problem statement is scientifically sound, well-posed, and contains all necessary information for a unique, verifiable solution.\n\nThe solution proceeds systematically through data simulation, parameter estimation via Maximum Likelihood, and hypothesis testing using a Wald test.\n\n**1. Data Simulation and Preprocessing**\n\nThe data for each period (pre- and post-crisis) are generated from a mixture distribution designed to have a specified tail behavior. For a sample of size $n$, a proportion $p_{\\text{tail}}$ of the data points constitute the \"tail\" and are drawn from a GPD. The remaining points form the \"body\" of the distribution.\n\n-   **Body**: $n - m$ samples are drawn from a Uniform distribution on $[-3, u_0]$, where $m = \\lfloor p_{\\text{tail}} n \\rceil$ and the base threshold is $u_0 = 0$.\n-   **Tail**: $m$ samples are generated as $u_0 + Y_i$, where $Y_i$ are independent GPD$(\\xi, \\beta)$ random variates.\n\nTo generate a random variate $Y$ from a GPD$(\\xi, \\beta)$, we use the inverse transform sampling method. The quantile function, $F^{-1}(p)$, is derived by inverting the GPD cumulative distribution function (CDF), $F(y)$. Given a uniform random variate $U \\sim U(0,1)$, a GPD variate $Y$ is generated as:\n$$\nY = F^{-1}(U) = \n\\begin{cases}\n\\dfrac{\\beta}{\\xi} \\left( (1-U)^{-\\xi} - 1 \\right), & \\xi \\ne 0, \\\\\n-\\beta \\log(1-U), & \\xi = 0.\n\\end{cases}\n$$\nSince $1-U$ is also uniformly distributed on $(0,1)$, this is equivalent to using $U$ directly in the expressions.\n\nAfter simulating the full sample $X = \\{X_1, \\dots, X_n\\}$, we apply the Peaks-Over-Threshold (POT) method. We establish a high threshold $u$ as the empirical $q$-quantile of the sample, with $q=0.9$. The exceedances are then defined as the positive values $Y_i = X_i - u$ for all $X_i > u$. These exceedances form the dataset for fitting the GPD model.\n\n**2. Maximum Likelihood Estimation (MLE)**\n\nThe parameters $(\\xi, \\beta)$ of the GPD are estimated for each period by maximizing the log-likelihood function or, equivalently, minimizing the negative log-likelihood function, $\\ell(\\xi, \\beta)$. For a set of $k$ exceedances $\\{y_1, \\dots, y_k\\}$, the negative log-likelihood is given by:\n$$\n\\ell(\\xi,\\beta; y_{1:k}) =\n\\begin{cases}\nk \\log \\beta + \\left(1+\\dfrac{1}{\\xi}\\right)\\displaystyle\\sum_{i=1}^k \\log\\!\\left(1 + \\dfrac{\\xi y_i}{\\beta}\\right), & \\xi \\ne 0, \\\\\nk \\log \\beta + \\dfrac{1}{\\beta}\\displaystyle\\sum_{i=1}^k y_i, & \\xi = 0.\n\\end{cases}\n$$\nThis minimization is a numerical optimization problem. The function for $\\xi \\ne 0$ converges to the function for $\\xi = 0$ as $\\xi \\to 0$. To ensure numerical stability, we implement the objective function with a conditional branch, using the limiting form for values of $\\xi$ close to zero (e.g., $|\\xi| < 10^{-8}$).\n\nThe minimization is subject to constraints: $\\beta > 0$, and for the logarithmic term to be well-defined, $1 + \\xi y_i/\\beta > 0$ for all exceedances $y_i$. The latter constraint implies $y_i < -\\beta/\\xi$ when $\\xi < 0$. These constraints are enforced within the objective function by returning a large value (representing infinity) if they are violated, effectively creating a barrier that guides the optimizer towards the valid parameter space. The optimization is performed using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm provided by `scipy.optimize.minimize`.\n\n**3. Standard Error Approximation**\n\nAccording to large-sample theory for MLE, the asymptotic covariance matrix of the estimators $(\\hat{\\xi}, \\hat{\\beta})$ is approximated by the inverse of the observed information matrix, $I(\\hat{\\xi}, \\hat{\\beta})$. The observed information matrix is the Hessian matrix of the negative log-likelihood function evaluated at the MLE:\n$$\n\\text{Cov}(\\hat{\\xi}, \\hat{\\beta}) \\approx [I(\\hat{\\xi}, \\hat{\\beta})]^{-1} = \\left[ \\nabla^2 \\ell(\\hat{\\xi}, \\hat{\\beta}) \\right]^{-1}.\n$$\nThe BFGS algorithm, being a quasi-Newton method, computes an approximation to the inverse of the Hessian matrix as part of its procedure. This approximation is readily available from the optimization result. The variance of the shape parameter estimator, $\\text{Var}(\\hat{\\xi})$, is approximated by the top-left element of this inverse Hessian matrix. The corresponding standard error is its square root:\n$$\ns_{\\hat{\\xi}} = \\sqrt{\\left( [I(\\hat{\\xi}, \\hat{\\beta})]^{-1} \\right)_{1,1}}.\n$$\n\n**4. Wald Test for Parameter Equality**\n\nTo test the hypothesis of no change in the tail index, $H_0: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$, against the alternative $H_1: \\xi_{\\text{pre}} \\ne \\xi_{\\text{post}}$, we use a two-sided Wald test. The test statistic is constructed from the MLEs and their standard errors from the two independent samples (pre-crisis and post-crisis):\n$$\nZ = \\dfrac{\\hat{\\xi}_{\\text{pre}} - \\hat{\\xi}_{\\text{post}}}{\\sqrt{s_{\\text{pre}}^2 + s_{\\text{post}}^2}}.\n$$\nUnder the null hypothesis, the statistic $Z$ follows an asymptotic standard normal distribution, $N(0,1)$. We reject $H_0$ at a significance level $\\alpha$ if the absolute value of the observed statistic, $|Z|$, exceeds the critical value $z_{1-\\alpha/2}$, which is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For $\\alpha = 0.05$, the critical value is $z_{0.975} \\approx 1.96$.\n\nThe entire procedure is encapsulated in a program that iterates through the provided test cases, performing simulation, estimation, and testing for each, and reports whether the null hypothesis is rejected.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the GPD tail index comparison.\n    \"\"\"\n    test_cases = [\n        # (seed_pre, seed_post, n_pre, n_post, xi_pre, beta_pre, xi_post, beta_post)\n        (12345, 54321, 5000, 5000, 0.2, 1.0, 0.6, 1.0),\n        (111, 222, 4000, 4000, 0.2, 1.0, 0.2, 1.0),\n        (333, 444, 6000, 6000, 0.01, 1.0, 0.0, 1.0),\n        (555, 666, 6000, 6000, -0.15, 1.2, 0.15, 1.2),\n    ]\n\n    common_params = {\n        'u0': 0.0,\n        'p_tail': 0.25,\n        'q': 0.9,\n        'alpha': 0.05,\n    }\n\n    results = []\n    for case in test_cases:\n        seed_pre, seed_post, n_pre, n_post, xi_pre, beta_pre, xi_post, beta_post = case\n        \n        # Fit pre-crisis period\n        xi_hat_pre, se_pre = fit_gpd_for_period(\n            seed_pre, n_pre, xi_pre, beta_pre, common_params\n        )\n        \n        # Fit post-crisis period\n        xi_hat_post, se_post = fit_gpd_for_period(\n            seed_post, n_post, xi_post, beta_post, common_params\n        )\n        \n        # Perform Wald test\n        wald_statistic = (xi_hat_pre - xi_hat_post) / np.sqrt(se_pre**2 + se_post**2)\n        critical_value = norm.ppf(1 - common_params['alpha'] / 2)\n        \n        reject_h0 = np.abs(wald_statistic) >= critical_value\n        results.append(str(reject_h0).lower())\n\n    print(f\"[{','.join(results)}]\")\n\ndef fit_gpd_for_period(seed, n, xi, beta, params):\n    \"\"\"\n    Simulates data and fits a GPD model for a single period.\n    Returns the estimated shape parameter and its standard error.\n    \"\"\"\n    X = simulate_data(seed, n, xi, beta, params['p_tail'], params['u0'])\n    \n    u = np.quantile(X, params['q'])\n    Y = X[X > u] - u\n    \n    # It's possible, though unlikely, that there are no exceedances\n    if len(Y) == 0:\n        raise ValueError(\"No exceedances found for GPD fitting.\")\n\n    # Objective function: negative log-likelihood for GPD\n    def nll_gpd(p, y_data):\n        _xi, _beta = p\n        \n        # Constraint: beta > 0\n        if _beta <= 1e-6:\n            return np.inf\n            \n        # Support constraint: 1 + xi*y/beta > 0\n        terms = 1 + _xi * y_data / _beta\n        if np.any(terms <= 0):\n            return np.inf\n\n        k = len(y_data)\n        \n        if abs(_xi) < 1e-8:\n            # Case xi -> 0 (Exponential distribution)\n            neg_log_lik = k * np.log(_beta) + np.sum(y_data) / _beta\n        else:\n            # Case xi != 0\n            log_of_terms = np.log(terms)\n            neg_log_lik = k * np.log(_beta) + (1 + 1/_xi) * np.sum(log_of_terms)\n\n        if not np.isfinite(neg_log_lik):\n            return np.inf\n            \n        return neg_log_lik\n\n    # Initial guess for optimization\n    initial_guess = [0.1, np.std(Y) if len(Y) > 1 else 1.0]\n\n    # Run optimizer to find MLE\n    res = minimize(\n        nll_gpd,\n        initial_guess,\n        args=(Y,),\n        method='BFGS',\n        options={'gtol': 1e-8}\n    )\n\n    if not res.success:\n        # A failed optimization might require more robust initial values or optimizer choice\n        # For this problem, we assume `BFGS` with this initial guess suffices.\n        pass\n\n    xi_hat, _ = res.x\n    \n    # Approximate variance from the inverse Hessian\n    var_xi = res.hess_inv[0, 0]\n    \n    # Handle potential numerical instability if variance is negative\n    if var_xi < 0:\n        # This shouldn't happen with BFGS, which maintains a positive definite Hess approx.\n        # But as a safeguard:\n        var_xi = np.abs(var_xi)\n\n    se_xi = np.sqrt(var_xi)\n    \n    return xi_hat, se_xi\n\ndef simulate_data(seed, n, xi, beta, p_tail, u0):\n    \"\"\"\n    Generates a sample from the mixture distribution.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    m = int(round(p_tail * n))\n    n_body = n - m\n    \n    # Generate the \"body\" of the distribution\n    body = rng.uniform(-3.0, u0, size=n_body)\n    \n    # Generate the \"tail\" using GPD inverse transform sampling\n    U = rng.uniform(size=m)\n    if abs(xi) < 1e-8:\n        tail_excess = -beta * np.log(U)\n    else:\n        tail_excess = (beta / xi) * (np.power(U, -xi) - 1)\n        \n    tail = u0 + tail_excess\n    \n    return np.concatenate((body, tail))\n\nif __name__ == \"__main__\":\n    solve()\n```"}]}