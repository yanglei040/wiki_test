{"hands_on_practices": [{"introduction": "This first exercise is a cornerstone of credit risk modeling: building a binary classifier from the ground up. You will implement a logistic regression model to predict the likelihood of a financial advisor being sanctioned, a proxy for default or misconduct. This practice moves beyond simply using a pre-built library function; by implementing the feature standardization, the maximum likelihood framework, and the Newton's method optimization routine with regularization yourself, you will gain a profound understanding of how these critical models work under the hood [@problem_id:2407536].", "id": "2407536", "problem": "You are tasked with implementing a binary classifier using logistic regression to predict whether a financial advisor will be sanctioned by a regulator in the next year, based on their customer complaint history and employment instability. The goal is to build the model from first principles in the context of computational economics and finance, using a maximum likelihood framework for binary outcomes and the concept of penalization for regularization. Your program must be a complete, runnable script that performs the following steps using the provided data and test suite, and outputs the aggregate results in the specified format.\n\nYou must build the classifier starting from fundamental principles: a binary outcome modeled as a Bernoulli random variable with probability that depends on a linear index transformed by a logistic link, and parameters estimated by maximizing the likelihood of the observed data. Interpretability constraints typical in financial risk modeling apply: include an intercept, standardize features to have zero mean and unit variance using only the training set statistics, and use an isotropic squared norm penalty (ridge regularization) on the slope coefficients to guard against overfitting. The intercept must not be penalized. No shortcut formulas or off-the-shelf model-fitting routines may be used; derive the update equations for the parameters and implement a numerically stable solver using Newton’s method.\n\nData description:\n- Features per advisor: \n  - $c$: number of customer complaints (count),\n  - $r$: average complaints per year (nonnegative real),\n  - $s$: average severity of complaints (normalized on $[0,1]$),\n  - $f$: number of firm changes in the last $5$ years (count),\n  - $t$: tenure in current firm in years (nonnegative real).\n- Target: $y \\in \\{0,1\\}$ indicating whether the advisor was sanctioned within the next year.\n\nTraining dataset (each item is $(c,r,s,f,t; y)$):\n- $(0,\\, 0.0,\\, 0.1,\\, 0,\\, 15;\\, 0)$\n- $(1,\\, 0.1,\\, 0.2,\\, 0,\\, 12;\\, 0)$\n- $(2,\\, 0.2,\\, 0.3,\\, 1,\\, 10;\\, 0)$\n- $(0,\\, 0.0,\\, 0.2,\\, 1,\\, 18;\\, 0)$\n- $(1,\\, 0.1,\\, 0.1,\\, 0,\\, 20;\\, 0)$\n- $(2,\\, 0.2,\\, 0.15,\\, 1,\\, 16;\\, 0)$\n- $(1,\\, 0.0,\\, 0.25,\\, 0,\\, 14;\\, 0)$\n- $(0,\\, 0.05,\\, 0.05,\\, 0,\\, 22;\\, 0)$\n- $(3,\\, 0.5,\\, 0.4,\\, 1,\\, 9;\\, 0)$\n- $(5,\\, 0.8,\\, 0.7,\\, 2,\\, 5;\\, 1)$\n- $(7,\\, 1.2,\\, 0.9,\\, 3,\\, 3;\\, 1)$\n- $(4,\\, 0.6,\\, 0.6,\\, 2,\\, 6;\\, 1)$\n- $(8,\\, 1.5,\\, 0.85,\\, 4,\\, 2;\\, 1)$\n- $(6,\\, 1.0,\\, 0.8,\\, 3,\\, 4;\\, 1)$\n- $(9,\\, 1.8,\\, 0.95,\\, 5,\\, 1;\\, 1)$\n- $(3,\\, 0.4,\\, 0.55,\\, 2,\\, 8;\\, 1)$\n\nModeling and training requirements:\n- Standardize each feature $c$, $r$, $s$, $f$, and $t$ to zero mean and unit variance using only the training set. For each feature $x$, the standardized value is computed as $(x - \\mu_x)/\\sigma_x$, where $\\mu_x$ and $\\sigma_x$ are the training mean and standard deviation for that feature. If any $\\sigma_x$ equals $0$, replace it with $1$ to avoid division by zero.\n- Augment the standardized feature matrix with an intercept column of ones.\n- Fit the logistic regression parameters by minimizing the penalized negative log-likelihood using Newton’s method with a stopping criterion based on the Euclidean norm of the parameter update being less than $10^{-8}$ or after $50$ iterations, whichever comes first.\n- Use a ridge penalty with strength $\\lambda \\ge 0$ applied only to the slope coefficients (intercept excluded).\n\nClassification rule:\n- For a given advisor with features $(c,r,s,f,t)$, compute the predicted sanction probability as a decimal in $[0,1]$. Classify as sanctioned if the probability is at least $0.5$; otherwise classify as not sanctioned. All probabilities, if reported, must be decimals; do not use percentage signs.\n\nTest suite:\nYour program must train the model multiple times as specified and return the classifications for the following four cases. Each case provides a penalty level $\\lambda$ and a test advisor’s features $(c,r,s,f,t)$. For each case, fit the model on the training data with the provided $\\lambda$ and then classify the given test advisor using the $0.5$ threshold.\n\n- Case $1$: $\\lambda = 1.0$, $(c,r,s,f,t) = (6,\\, 1.0,\\, 0.8,\\, 3,\\, 3)$\n- Case $2$: $\\lambda = 0.0$, $(c,r,s,f,t) = (0,\\, 0.0,\\, 0.1,\\, 0,\\, 15)$\n- Case $3$: $\\lambda = 10.0$, $(c,r,s,f,t) = (10,\\, 1.5,\\, 0.9,\\, 5,\\, 2)$\n- Case $4$: $\\lambda = 0.5$, $(c,r,s,f,t) = (1,\\, 0.1,\\, 0.2,\\, 0,\\, 12)$\n\nFinal output format:\n- Your program should produce a single line of output containing the classification results as a comma-separated list of integers enclosed in square brackets, in the order of the cases above, for example, $\"[1,0,1,0]\"$.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of statistical learning, specifically logistic regression with ridge regularization. The setup is well-posed, providing all necessary data, a clear objective function, and a specified numerical method for optimization. It is an objective and formalizable problem in computational finance. We will proceed with a solution derived from first principles.\n\nLet the set of training data be $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, where $\\mathbf{x}_i \\in \\mathbb{R}^D$ is the feature vector for the $i$-th advisor and $y_i \\in \\{0, 1\\}$ is the corresponding binary outcome (sanctioned or not). Here, $N=16$ is the number of training observations and $D=5$ is the number of features.\n\nThe logistic regression model postulates that the probability of a positive outcome ($y_i=1$) is given by the logistic (sigmoid) function applied to a linear combination of the features:\n$$\nP(y_i=1 | \\mathbf{x}_i^*; \\boldsymbol{\\beta}) = p_i = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}\n$$\nHere, $\\mathbf{x}_i^*$ is the $i$-th feature vector augmented with a leading $1$ to accommodate the intercept term, so $\\mathbf{x}_i^* \\in \\mathbb{R}^{D+1}$. The vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{D+1}$ contains the model parameters, with $\\beta_0$ being the intercept and $\\beta_1, \\dots, \\beta_D$ being the slope coefficients.\n\nThe parameters $\\boldsymbol{\\beta}$ are estimated by maximizing the log-likelihood of the observed data, with an added penalty term for regularization. The likelihood for a single observation is given by the Bernoulli probability mass function: $L_i(\\boldsymbol{\\beta}) = p_i^{y_i} (1-p_i)^{1-y_i}$. The total log-likelihood for $N$ independent observations is:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\ln(L_i(\\boldsymbol{\\beta})) = \\sum_{i=1}^N [y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]\n$$\nSubstituting the expressions for $p_i$ and $\\ln(1-p_i) = \\ln(1-\\sigma(z_i)) = -z_i - \\ln(1+e^{z_i})$ and $\\ln(p_i) = \\ln(\\sigma(z_i)) = -\\ln(1+e^{-z_i}) = z_i - \\ln(1+e^{z_i})$ for $z_i=\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}$, we can simplify the log-likelihood to a more convenient form:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N [y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) - \\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}})]\n$$\nWe aim to minimize the penalized *negative* log-likelihood. The penalty is the squared $\\ell_2$-norm of the slope coefficients (ridge regularization), which discourages overly large parameter values and helps prevent overfitting. The intercept $\\beta_0$ is not penalized. The objective function to minimize is:\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\sum_{j=1}^D \\beta_j^2 = \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^T \\mathbf{I}' \\boldsymbol{\\beta}\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength and $\\mathbf{I}'$ is a $(D+1) \\times (D+1)$ diagonal matrix with $I'_{00} = 0$ and $I'_{jj} = 1$ for $j=1, \\dots, D$.\n\nTo minimize $J(\\boldsymbol{\\beta})$, we use Newton's method, which is an iterative algorithm with the update rule:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla J(\\boldsymbol{\\beta}^{(t)})\n$$\nwhere $\\nabla J$ is the gradient of the objective function and $\\mathbf{H}$ is its Hessian matrix.\n\nFirst, we derive the gradient $\\nabla J(\\boldsymbol{\\beta})$. The $j$-th component of the gradient of the unpenalized part is:\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left( \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] \\right) = \\sum_{i=1}^N \\left[ \\frac{e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}} x_{ij}^* - y_i x_{ij}^* \\right] = \\sum_{i=1}^N (p_i - y_i) x_{ij}^*\n$$\nIn matrix notation, where $\\mathbf{X}^*$ is the $N \\times (D+1)$ augmented design matrix, $\\mathbf{p}$ is the vector of probabilities, and $\\mathbf{y}$ is the vector of outcomes, this is $\\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y})$.\nThe gradient of the penalty term is $\\lambda \\mathbf{I}' \\boldsymbol{\\beta}$.\nThe full gradient is therefore:\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{I}' \\boldsymbol{\\beta}\n$$\nNext, we derive the Hessian matrix $\\mathbf{H}(\\boldsymbol{\\beta})$. The entry $H_{jk}$ is $\\frac{\\partial^2 J(\\boldsymbol{\\beta})}{\\partial \\beta_j \\partial \\beta_k}$. For the unpenalized part:\n$$\n\\frac{\\partial^2}{\\partial \\beta_j \\partial \\beta_k} \\left( \\sum_{i=1}^N \\dots \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^N p_i x_{ij}^* \\right) = \\sum_{i=1}^N x_{ij}^* \\frac{\\partial p_i}{\\partial \\beta_k}\n$$\nSince $\\frac{\\partial p_i}{\\partial \\beta_k} = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})(1 - \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})) x_{ik}^* = p_i(1-p_i)x_{ik}^*$, the Hessian of the unpenalized part is:\n$$\nH_{jk} = \\sum_{i=1}^N x_{ij}^* p_i(1-p_i) x_{ik}^*\n$$\nIn matrix form, this is $\\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^*$, where $\\mathbf{W}$ is an $N \\times N$ diagonal matrix with $W_{ii} = p_i(1-p_i)$.\nThe Hessian of the penalty term is simply $\\lambda \\mathbf{I}'$.\nThe full Hessian is:\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^* + \\lambda \\mathbf{I}'\n$$\nThis Hessian is positive definite for $\\lambda > 0$, ensuring a unique minimum and numerical stability for the Newton's method.\n\nThe implementation will proceed as follows:\n1.  Compute the mean $\\boldsymbol{\\mu}$ and standard deviation $\\boldsymbol{\\sigma}$ for each of the $D=5$ features from the training data. If any $\\sigma_j = 0$, it will be set to $1$.\n2.  Standardize the training data matrix $\\mathbf{X}$ to get $\\mathbf{X}_{std}$, where each column has zero mean and unit variance.\n3.  Augment $\\mathbf{X}_{std}$ with a column of ones to form the design matrix $\\mathbf{X}^*$.\n4.  For each test case, initialize the parameter vector $\\boldsymbol{\\beta}$ (e.g., to zeros).\n5.  Iteratively apply the Newton's method update:\n    a. Calculate probabilities $\\mathbf{p} = \\sigma(\\mathbf{X}^* \\boldsymbol{\\beta}^{(t)})$ and the weight matrix $\\mathbf{W}^{(t)}$.\n    b. Compute the gradient $\\nabla J(\\boldsymbol{\\beta}^{(t)})$ and Hessian $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})$.\n    c. Solve the linear system $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\Delta \\boldsymbol{\\beta} = - \\nabla J(\\boldsymbol{\\beta}^{(t)})$ for the update step $\\Delta \\boldsymbol{\\beta}$.\n    d. Update the parameters: $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$.\n    e. Terminate if the Euclidean norm of the update, $\\|\\Delta \\boldsymbol{\\beta}\\|_2$, is less than $10^{-8}$ or after $50$ iterations.\n6.  For each test vector $\\mathbf{x}_{\\text{test}}$, standardize it using the training set's $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, augment it to $\\mathbf{x}_{\\text{test}}^*$, calculate the predicted probability $p_{\\text{test}} = \\sigma(\\mathbf{x}_{\\text{test}}^{*T} \\boldsymbol{\\beta}_{\\text{final}})$, and classify as $1$ if $p_{\\text{test}} \\ge 0.5$, and $0$ otherwise.\nThe final output will be a list of these classifications.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and solves the logistic regression problem as specified.\n    \"\"\"\n    # Training dataset from the problem statement\n    # (c, r, s, f, t; y)\n    training_data = np.array([\n        [0.0, 0.0, 0.1, 0.0, 15.0, 0.0],\n        [1.0, 0.1, 0.2, 0.0, 12.0, 0.0],\n        [2.0, 0.2, 0.3, 1.0, 10.0, 0.0],\n        [0.0, 0.0, 0.2, 1.0, 18.0, 0.0],\n        [1.0, 0.1, 0.1, 0.0, 20.0, 0.0],\n        [2.0, 0.2, 0.15, 1.0, 16.0, 0.0],\n        [1.0, 0.0, 0.25, 0.0, 14.0, 0.0],\n        [0.0, 0.05, 0.05, 0.0, 22.0, 0.0],\n        [3.0, 0.5, 0.4, 1.0, 9.0, 0.0],\n        [5.0, 0.8, 0.7, 2.0, 5.0, 1.0],\n        [7.0, 1.2, 0.9, 3.0, 3.0, 1.0],\n        [4.0, 0.6, 0.6, 2.0, 6.0, 1.0],\n        [8.0, 1.5, 0.85, 4.0, 2.0, 1.0],\n        [6.0, 1.0, 0.8, 3.0, 4.0, 1.0],\n        [9.0, 1.8, 0.95, 5.0, 1.0, 1.0],\n        [3.0, 0.4, 0.55, 2.0, 8.0, 1.0],\n    ])\n\n    X_train_raw = training_data[:, :-1]\n    y_train = training_data[:, -1].reshape(-1, 1)\n\n    # Test cases from the problem statement\n    # Each is (lambda, (c, r, s, f, t))\n    test_cases = [\n        (1.0, (6.0, 1.0, 0.8, 3.0, 3.0)),\n        (0.0, (0.0, 0.0, 0.1, 0.0, 15.0)),\n        (10.0, (10.0, 1.5, 0.9, 5.0, 2.0)),\n        (0.5, (1.0, 0.1, 0.2, 0.0, 12.0)),\n    ]\n\n    # Step 1: Standardize features using training data statistics\n    mean_X = np.mean(X_train_raw, axis=0)\n    std_X = np.std(X_train_raw, axis=0)\n    # As per instructions, replace std=0 with 1 to avoid division by zero\n    std_X[std_X == 0] = 1.0\n    \n    X_train_std = (X_train_raw - mean_X) / std_X\n\n    # Step 2: Augment the standardized feature matrix with an intercept column\n    intercept_col = np.ones((X_train_std.shape[0], 1))\n    X_train_aug = np.hstack((intercept_col, X_train_std))\n\n    def numerically_stable_sigmoid(z):\n        \"\"\"Computes the sigmoid function in a numerically stable way.\"\"\"\n        # Using a vectorized implementation for efficiency\n        # This prevents overflow with large positive z and underflow with large negative z\n        pos_mask = (z >= 0)\n        neg_mask = (z < 0)\n        p = np.zeros_like(z, dtype=float)\n        p[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        p[neg_mask] = np.exp(z[neg_mask]) / (1.0 + np.exp(z[neg_mask]))\n        return p\n\n    results = []\n    \n    num_features = X_train_aug.shape[1]\n\n    for lambda_val, test_features_raw in test_cases:\n        # Step 3: Fit the logistic regression model using Newton's method\n        \n        # Initialize parameters (beta) to zeros\n        beta = np.zeros((num_features, 1))\n        \n        # Regularization matrix I'\n        # Diagonal matrix with 0 for intercept and lambda for slopes\n        penalty_matrix = lambda_val * np.eye(num_features)\n        penalty_matrix[0, 0] = 0.0\n        \n        max_iter = 50\n        tolerance = 1e-8\n        \n        for i in range(max_iter):\n            # Calculate linear predictors and probabilities\n            z = X_train_aug @ beta\n            p = numerically_stable_sigmoid(z)\n            \n            # W is a diagonal matrix of weights p_i * (1 - p_i)\n            # Implemented with a 1D array for efficiency\n            weights = p * (1 - p)\n            W = np.diag(weights.flatten())\n            \n            # Calculate gradient of the penalized negative log-likelihood\n            # grad = X^T * (p - y) + lambda * I' * beta\n            gradient = X_train_aug.T @ (p - y_train) + penalty_matrix @ beta\n            \n            # Calculate Hessian of the penalized negative log-likelihood\n            # H = X^T * W * X + lambda * I'\n            hessian = X_train_aug.T @ W @ X_train_aug + penalty_matrix\n            \n            # Solve the linear system H * delta_beta = -gradient\n            # This is the Newton-Raphson update step\n            try:\n                # np.linalg.solve is more stable and efficient than inverting the matrix\n                delta_beta = np.linalg.solve(hessian, -gradient)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular, for robustness\n                delta_beta = np.linalg.pinv(hessian) @ -gradient\n            \n            # Update parameters\n            beta += delta_beta\n            \n            # Check for convergence\n            if np.linalg.norm(delta_beta) < tolerance:\n                break\n\n        # Step 4: Classify the test advisor\n        x_test_std = (np.array(test_features_raw) - mean_X) / std_X\n        x_test_aug = np.hstack(([1.0], x_test_std))\n        \n        # Calculate predicted probability for the test case\n        prob_test = numerically_stable_sigmoid(x_test_aug @ beta).item()\n        \n        # Classify based on the 0.5 threshold\n        classification = 1 if prob_test >= 0.5 else 0\n        results.append(classification)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Once a model is built, how do we interpret its decisions and provide actionable feedback? This exercise tackles that question by introducing the concept of algorithmic recourse, where you will find the smallest and most feasible change to a denied loan applicant's profile to secure an 'approve' decision [@problem_id:2385810]. This practice delves into the practical implications of credit models, applying constrained optimization to understand a model's sensitivity and explore pathways for individuals to improve their outcomes.", "id": "2385810", "problem": "You are given a binary credit decision model, trained using Machine Learning (ML), that maps a feature vector $\\mathbf{x} \\in \\mathbb{R}^{d}$ to an approval or denial. The model is logistic regression with probability $p(\\mathbf{x}) = \\left(1 + \\exp\\left(-(\\mathbf{w}^{\\top}\\mathbf{x} + b)\\right)\\right)^{-1}$. The decision rule is: approve if $p(\\mathbf{x}) \\geq 0.5$ and deny otherwise. This is equivalent to approving if and only if $\\mathbf{w}^{\\top}\\mathbf{x} + b \\geq 0$. Consider an applicant with current features $\\mathbf{x}_{0}$ that is denied. You are asked to compute a minimally costly adversarial modification (also called recourse) to reach approval, under realistic constraints on what can and cannot be changed.\n\nThe cost of a modification from $\\mathbf{x}_{0}$ to $\\mathbf{z}$ is measured by a weighted quadratic norm\n$$\n\\|\\mathbf{z} - \\mathbf{x}_{0}\\|_{\\mathbf{C}} \\equiv \\sqrt{(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0})},\n$$\nwhere $\\mathbf{C}$ is a positive definite diagonal matrix expressing the heterogeneous difficulty of changing each feature. Some features are immutable due to regulatory or factual reasons; immutability is summarized by a mask $\\mathbf{m} \\in \\{0,1\\}^{d}$, where $m_{i} = 0$ means feature $i$ cannot be changed and thus $z_{i} = (x_{0})_{i}$ must hold. All features must also remain within given lower and upper bounds $\\boldsymbol{\\ell} \\le \\mathbf{z} \\le \\mathbf{u}$, interpreted componentwise.\n\nYour task is to write a program that, for each specified test case, solves the following constrained optimization problem from first principles:\n- minimize $\\tfrac{1}{2}(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0})$,\n- subject to $\\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0$,\n- and $z_{i} = (x_{0})_{i}$ for all indices $i$ with $m_{i} = 0$,\n- and $\\ell_{i} \\le z_{i} \\le u_{i}$ for all indices $i$.\n\nIf the current point $\\mathbf{x}_{0}$ already satisfies $\\mathbf{w}^{\\top}\\mathbf{x}_{0} + b \\ge 0$, then output the minimal cost $0$ for that case, because no change is needed. If, due to immutability and box constraints, approval is impossible (that is, no $\\mathbf{z}$ satisfies the constraints and $\\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0$), output $+\\infty$ represented as the string $inf$ for that case.\n\nUse only the model and constraints provided below. All numeric data are unitless and must be treated as pure numbers.\n\nModel and metric parameters (shared by all test cases):\n- Dimension: $d = 4$.\n- Weight vector: $\\mathbf{w} = [\\,1.2,\\,-0.7,\\,0.3,\\,1.5\\,]$.\n- Intercept: $b = -0.2$.\n- Cost matrix: $\\mathbf{C} = \\mathrm{diag}([\\,1.0,\\,1.0,\\,4.0,\\,0.25\\,])$.\n- Lower bounds: $\\boldsymbol{\\ell} = [\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$.\n- Upper bounds: $\\mathbf{u} = [\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$.\n\nTest suite (each case provides $\\mathbf{x}_{0}$ and mask $\\mathbf{m}$):\n- Case $1$ (general feasible case): $\\mathbf{x}_{0} = [\\,0.2,\\,0.8,\\,0.1,\\,0.1\\,]$, $\\mathbf{m} = [\\,1,\\,1,\\,1,\\,1\\,]$.\n- Case $2$ (already approved, zero modification): $\\mathbf{x}_{0} = [\\,0.9,\\,0.1,\\,0.1,\\,0.1\\,]$, $\\mathbf{m} = [\\,1,\\,1,\\,1,\\,1\\,]$.\n- Case $3$ (immutability increases difficulty but feasible): $\\mathbf{x}_{0} = [\\,0.2,\\,0.8,\\,0.1,\\,0.1\\,]$, $\\mathbf{m} = [\\,1,\\,1,\\,1,\\,0\\,]$.\n- Case $4$ (infeasible under bounds and immutability): $\\mathbf{x}_{0} = [\\,0.0,\\,1.0,\\,0.0,\\,0.0\\,]$, $\\mathbf{m} = [\\,0,\\,0,\\,1,\\,0\\,]$.\n\nProgram requirements:\n- For each case, compute the minimal cost $\\|\\mathbf{z}^{\\star} - \\mathbf{x}_{0}\\|_{\\mathbf{C}}$ that achieves approval subject to the constraints, or determine that it is infeasible.\n- Results must be returned as floating-point numbers rounded to $6$ decimal places. If infeasible, return $inf$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $\\texttt{[0.123456,0.000000,0.234567,inf]}$) in the order of Cases $1$ through $4$.\n\nNotes:\n- Express any angles or percentages as pure numbers if they appear; none appear here.\n- You must not require any user input; use the parameters specified above.\n- Design your algorithm from fundamental definitions of logistic regression classification and convex optimization principles, and implement an exact or numerical solver that is appropriate for convex quadratic problems with linear constraints.", "solution": "The problem statement is examined and deemed valid. It presents a standard convex quadratic program under linear constraints, which is a well-posed and scientifically sound problem in the field of algorithmic recourse. We now proceed to the derivation and implementation of the solution.\n\nThe problem is to find a modification vector $\\mathbf{z} \\in \\mathbb{R}^{d}$ for an initial applicant feature vector $\\mathbf{x}_{0}$ that satisfies a set of constraints while minimizing a quadratic cost. The optimization problem is formulated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & f(\\mathbf{z}) = \\frac{1}{2}(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0}) \\\\\n\\text{subject to} \\quad & \\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0 \\\\\n& z_{i} = (x_0)_{i} \\quad \\forall i \\text{ such that } m_i = 0 \\\\\n& \\boldsymbol{\\ell} \\le \\mathbf{z} \\le \\mathbf{u}\n\\end{aligned}\n$$\nThe objective function $f(\\mathbf{z})$ is strictly convex because the cost matrix $\\mathbf{C}$ is positive definite. The constraints are all linear, defining a convex feasible set. This structure ensures that if a solution exists, it is unique. The final cost to be reported is $\\|\\mathbf{z}^{\\star} - \\mathbf{x}_{0}\\|_{\\mathbf{C}} = \\sqrt{2f(\\mathbf{z}^{\\star})}$.\n\nFirst, we handle the preliminary cases. If the initial point $\\mathbf{x}_{0}$ already satisfies the approval constraint, $\\mathbf{w}^{\\top}\\mathbf{x}_{0} + b \\ge 0$, then no modification is necessary. The optimal solution is $\\mathbf{z}^{\\star} = \\mathbf{x}_{0}$ and the cost is $0$.\n\nIf $\\mathbf{w}^{\\top}\\mathbf{x}_{0} + b < 0$, we must find a new vector $\\mathbf{z}$. Before proceeding, we must check for feasibility. A solution is feasible only if there exists at least one vector $\\mathbf{z}$ that satisfies all constraints simultaneously. To check this, we find the maximum possible value of the score function $\\mathbf{w}^{\\top}\\mathbf{z} + b$ subject to the immutability and box constraints. Let this maximum score be $S_{max}$. This score is achieved by choosing $z_i$ for each mutable feature $i$ (where $m_i = 1$) to be $u_i$ if $w_i > 0$ and $\\ell_i$ if $w_i < 0$. If $S_{max} < 0$, no possible modification can achieve approval, and the problem is infeasible, resulting in an infinite cost.\n\nIf the problem is feasible and requires modification, we solve the full constrained optimization problem. We use the method of Lagrange multipliers. The Lagrangian for this problem is:\n$$\nL(\\mathbf{z}, \\lambda, \\boldsymbol{\\mu}_L, \\boldsymbol{\\mu}_U) = \\frac{1}{2}(\\mathbf{z} - \\mathbf{x}_{0})^{\\top}\\mathbf{C}(\\mathbf{z} - \\mathbf{x}_{0}) - \\lambda(\\mathbf{w}^{\\top}\\mathbf{z} + b) - \\boldsymbol{\\mu}_L^{\\top}(\\mathbf{z} - \\boldsymbol{\\ell}) - \\boldsymbol{\\mu}_U^{\\top}(\\mathbf{u} - \\mathbf{z})\n$$\nThis is formulated over the mutable features only, as immutable features are fixed. Let $I_{mut} = \\{i \\mid m_i = 1\\}$. The gradient of the Lagrangian with respect to the mutable components of $\\mathbf{z}$ must be zero at the optimum:\n$$\n\\nabla_{\\mathbf{z}_{mut}} L = \\mathbf{C}_{mut}(\\mathbf{z}_{mut} - \\mathbf{x}_{0,mut}) - \\lambda \\mathbf{w}_{mut} - \\boldsymbol{\\mu}_{L,mut} + \\boldsymbol{\\mu}_{U,mut} = \\mathbf{0}\n$$\nHere, $\\lambda \\ge 0$, $\\boldsymbol{\\mu}_L \\ge 0$, and $\\boldsymbol{\\mu}_U \\ge 0$ are the Lagrange multipliers for the approval, lower-bound, and upper-bound constraints, respectively. Since $\\mathbf{C}$ is diagonal, we can solve for each component $z_i$ where $i \\in I_{mut}$:\n$$\nz_i = (x_0)_i + \\frac{1}{C_{ii}}(\\lambda w_i + (\\mu_L)_i - (\\mu_U)_i)\n$$\nThe Karush-Kuhn-Tucker (KKT) complementary slackness conditions state that a multiplier is non-zero only if its corresponding inequality constraint is active (holds with equality). For the box constraints:\n- If $\\ell_i < z_i < u_i$, then $(\\mu_L)_i = 0$ and $(\\mu_U)_i = 0$, which gives $z_i = (x_0)_i + \\lambda \\frac{w_i}{C_{ii}}$.\n- If $z_i = \\ell_i$, then $(\\mu_U)_i = 0$.\n- If $z_i = u_i$, then $(\\mu_L)_i = 0$.\nThis allows us to express the optimal $z_i$ as a function of only $\\lambda$. The solution for $z_i$ is the projection of the unconstrained optimum onto the interval $[\\ell_i, u_i]$:\n$$\nz_i^{\\star}(\\lambda) = \\mathrm{clip}\\left((x_0)_i + \\lambda \\frac{w_i}{C_{ii}}, \\ell_i, u_i\\right) = \\max\\left(\\ell_i, \\min\\left(u_i, (x_0)_i + \\lambda \\frac{w_i}{C_{ii}}\\right)\\right)\n$$\nFor immutable features $j \\notin I_{mut}$, we have $z_j^{\\star} = (x_0)_j$.\n\nSince the initial point is denied, the approval constraint $\\mathbf{w}^{\\top}\\mathbf{z} + b \\ge 0$ must be active at the solution, meaning $\\mathbf{w}^{\\top}\\mathbf{z}^{\\star} + b = 0$. This gives us an equation to find the optimal $\\lambda^{\\star}$:\n$$\ng(\\lambda) = \\mathbf{w}^{\\top}\\mathbf{z}^{\\star}(\\lambda) + b = \\sum_{i=0}^{d-1} w_i z_i^{\\star}(\\lambda) + b = 0\n$$\nThe function $g(\\lambda)$ is continuous and monotonically non-decreasing with respect to $\\lambda \\ge 0$. We know $g(0) = \\mathbf{w}^{\\top}\\mathbf{x}_{0} + b < 0$, and for a feasible problem, there exists some large $\\lambda$ such that $g(\\lambda) \\ge 0$. Therefore, a unique root $\\lambda^{\\star} \\ge 0$ exists and can be found efficiently using a numerical root-finding algorithm, such as the bisection method.\n\nThe complete algorithm is as follows:\n1. For a given test case $(\\mathbf{x}_{0}, \\mathbf{m})$, compute the initial score $S_0 = \\mathbf{w}^{\\top}\\mathbf{x}_{0} + b$. If $S_0 \\ge 0$, the cost is $0$.\n2. If $S_0 < 0$, check for feasibility by finding the maximum possible score $S_{max}$ under the box and immutability constraints. If $S_{max} < 0$, the problem is infeasible; the cost is $\\infty$.\n3. If the problem is feasible, find the root $\\lambda^{\\star}$ of $g(\\lambda) = 0$ using the bisection method on an interval $[\\lambda_{low}, \\lambda_{high}]$ (e.g., $[0, 1000]$).\n4. With the optimal $\\lambda^{\\star}$, compute the optimal solution vector $\\mathbf{z}^{\\star}$ using the clipping formula for each mutable feature.\n5. Calculate the final cost $\\|\\mathbf{z}^{\\star} - \\mathbf{x}_{0}\\|_{\\mathbf{C}} = \\sqrt{\\sum_{i=0}^{d-1} C_{ii}(z_i^{\\star} - (x_0)_i)^2}$.\nThis procedure yields an exact solution to the convex optimization problem, limited only by the numerical precision of the bisection method.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained optimization problem for multiple test cases.\n    \"\"\"\n    # Model and metric parameters (shared by all test cases)\n    w = np.array([1.2, -0.7, 0.3, 1.5])\n    b = -0.2\n    C_diag = np.array([1.0, 1.0, 4.0, 0.25])\n    ell = np.array([0.0, 0.0, 0.0, 0.0])\n    u = np.array([1.0, 1.0, 1.0, 1.0])\n\n    # Test suite\n    test_cases = [\n        # Case 1: general feasible case\n        {'x0': np.array([0.2, 0.8, 0.1, 0.1]), 'm': np.array([1, 1, 1, 1])},\n        # Case 2: already approved, zero modification\n        {'x0': np.array([0.9, 0.1, 0.1, 0.1]), 'm': np.array([1, 1, 1, 1])},\n        # Case 3: immutability increases difficulty but feasible\n        {'x0': np.array([0.2, 0.8, 0.1, 0.1]), 'm': np.array([1, 1, 1, 0])},\n        # Case 4: infeasible under bounds and immutability\n        {'x0': np.array([0.0, 1.0, 0.0, 0.0]), 'm': np.array([0, 0, 1, 0])},\n    ]\n\n    results = [\n        _solve_single_case(\n            case['x0'], case['m'], w, b, C_diag, ell, u\n        ) for case in test_cases\n    ]\n\n    # Format output according to problem specification\n    formatted_results = []\n    for r in results:\n        if r == np.inf:\n            formatted_results.append(\"inf\")\n        else:\n            formatted_results.append(f\"{r:.6f}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _solve_single_case(x0, m, w, b, C_diag, ell, u):\n    \"\"\"\n    Solves the recourse optimization problem for a single applicant.\n    \"\"\"\n    # Step 1: Check if already approved\n    score_x0 = w.T @ x0 + b\n    if score_x0 >= 0:\n        return 0.0\n\n    mutable_idx = np.where(m == 1)[0]\n    \n    # Step 2: Check for feasibility\n    # Find the maximum possible score under immutability and box constraints.\n    z_best = np.copy(x0)\n    for i in mutable_idx:\n        if w[i] > 0:\n            z_best[i] = u[i]\n        elif w[i] < 0:\n            z_best[i] = ell[i]\n    \n    max_score = w.T @ z_best + b\n    if max_score < 0:\n        return np.inf\n\n    # Step 3: Find the optimal Lagrange multiplier lambda using bisection\n    \n    # Define the score function g(lambda) = w^T z(lambda) + b = 0\n    def g(lam):\n        z_lam = np.copy(x0)\n        for i in mutable_idx:\n            z_i_unconstrained = x0[i] + lam * w[i] / C_diag[i]\n            z_lam[i] = np.clip(z_i_unconstrained, ell[i], u[i])\n        return w.T @ z_lam + b\n\n    # Bisection search for lambda\n    lam_low = 0.0\n    lam_high = 1.0\n    # Establish a safe upper bound for lambda\n    while g(lam_high) < 0:\n        lam_high *= 2.0\n        if lam_high > 1e12: # Safety break against infinite loop\n            return np.inf\n\n    # Perform bisection for a fixed number of iterations for high precision\n    for _ in range(100):\n        lam_mid = (lam_low + lam_high) / 2\n        if g(lam_mid) < 0:\n            lam_low = lam_mid\n        else:\n            lam_high = lam_mid\n    \n    lambda_star = (lam_low + lam_high) / 2\n\n    # Step 4: Compute the optimal solution z_star and its cost\n    z_star = np.copy(x0)\n    for i in mutable_idx:\n        z_i_unconstrained = x0[i] + lambda_star * w[i] / C_diag[i]\n        z_star[i] = np.clip(z_i_unconstrained, ell[i], u[i])\n\n    delta_z = z_star - x0\n    cost_squared = np.sum(C_diag * (delta_z**2))\n    cost = np.sqrt(cost_squared)\n\n    return cost\n\nif __name__ == \"__main__\":\n    solve()\n```"}, {"introduction": "Credit risk extends beyond individual entities to the stability of the entire financial system. This final practice moves from single-obligor analysis to systemic risk by simulating a financial contagion cascade [@problem_id:2385774]. By modeling how an initial default can propagate through an interconnected network of firms, you will explore the dynamics of stress testing and understand how interconnectedness can amplify losses, a crucial concept in modern financial regulation.", "id": "2385774", "problem": "You are asked to formalize and implement a stress testing procedure for a loan portfolio that incorporates second-order contagion effects generated by an exogenous counterparty default. The design must start from core credit risk definitions and proceed by explicit logical rules. Consider a system with $N$ obligors indexed by $i \\in \\{0,1,\\dots,N-1\\}$. The relevant primitives are:\n- A vector of the bank’s exposures at default $b \\in \\mathbb{R}_{\\ge 0}^{N}$ to these obligors, where $b_i$ is the bank’s exposure to obligor $i$.\n- A vector of initial equity buffers $K^{(0)} \\in \\mathbb{R}_{> 0}^{N}$, where $K_i^{(0)}$ is obligor $i$’s equity before contagion.\n- An inter-obligor exposure matrix $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$ with $X_{ii} = 0$ for all $i$, where $X_{i,j}$ is the exposure of obligor $i$ to obligor $j$.\n- A loss-given-default for the bank’s loans $L^{\\text{bank}} \\in [0,1]$ and a loss-given-default for inter-obligor claims $L^{\\text{inter}} \\in [0,1]$.\n- A systemic loss multiplier $m \\ge 1$ that amplifies inter-obligor losses during stress.\n- An initial default set $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$ representing exogenous defaults at time $t=0$.\n\nFundamental definitions and rules:\n- Exposure at Default (EAD) is the outstanding exposure subject to loss when a counterparty defaults; here, the bank’s EAD to obligor $i$ is $b_i$, and inter-obligor EAD from $i$ to $j$ is $X_{i,j}$.\n- Loss Given Default (LGD) is the fraction of EAD not recovered upon default; here, the bank’s LGD on loans is $L^{\\text{bank}}$, and inter-obligor LGD is $L^{\\text{inter}}$.\n- Equity is reduced by realized credit losses. At each contagion iteration $t \\in \\{1,2,\\dots\\}$, define $K^{(t)}$ by deducting only the losses caused by counterparties that newly defaulted at iteration $t-1$. For an obligor $i$ not yet defaulted,\n$$\nK_i^{(t)} \\;=\\; K_i^{(t-1)} \\;-\\; \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big),\n$$\nwhere $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ is the set of obligors that became defaulted at iteration $t-1$ (with $D^{(-1)} := \\emptyset$). The default rule is: obligor $i$ defaults at iteration $t$ if and only if $K_i^{(t)} \\le 0$. Use a numerical tolerance $\\varepsilon = 10^{-12}$ and treat $K_i^{(t)} \\le \\varepsilon$ as defaulted to avoid floating point artifacts.\n- The contagion process terminates at the first iteration $T$ such that $\\Delta D^{(T)} = \\emptyset$. The final default set is $D^{(\\infty)} = D^{(T)}$.\n- The bank’s portfolio loss is\n$$\n\\text{Loss} \\;=\\; \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}.\n$$\n\nYour task is to implement a program that, for each test case described below, computes:\n- the bank’s total loss as a real number (express the result as a decimal, not a percentage), and\n- the total number of defaulted obligors (an integer), counting both the exogenous initial defaults and any contagion-induced defaults.\n\nIndices are zero-based in all inputs. No physical units are involved. Angles are not used. Percentages must be expressed as decimals (for example, write $0.4$ rather than $40\\%$).\n\nTest suite:\n- Case A (second-order contagion, moderate amplification):\n  - $N = 4$.\n  - $b = [10.0,\\, 5.0,\\, 8.0,\\, 6.0]$.\n  - $K^{(0)} = [2.0,\\, 2.0,\\, 1.0,\\, 1.1]$.\n  - $X$ has nonzero entries $X_{1,0} = 1.6$, $X_{2,0} = 1.8$, $X_{3,0} = 0.6$, $X_{1,2} = 0.9$, $X_{3,2} = 1.2$; all other entries are $0$.\n  - $L^{\\text{bank}} = 0.45$, $L^{\\text{inter}} = 0.5$, $m = 1.2$.\n  - $D^{(0)} = \\{0\\}$.\n- Case B (no contagion network):\n  - $N = 3$.\n  - $b = [5.0,\\, 5.0,\\, 5.0]$.\n  - $K^{(0)} = [1.0,\\, 1.0,\\, 1.0]$.\n  - $X$ is the $3 \\times 3$ zero matrix.\n  - $L^{\\text{bank}} = 0.4$, $L^{\\text{inter}} = 0.5$, $m = 1.2$.\n  - $D^{(0)} = \\{1\\}$.\n- Case C (boundary condition at default threshold, full cascade):\n  - $N = 3$.\n  - $b = [1.0,\\, 1.0,\\, 1.0]$.\n  - $K^{(0)} = [1.0,\\, 0.5,\\, 0.5]$.\n  - $X$ has nonzero entries $X_{1,0} = 1.0$, $X_{2,1} = 1.0$; all other entries are $0$.\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.5$, $m = 1.0$.\n  - $D^{(0)} = \\{0\\}$.\n- Case D (amplification creates contagion that would not occur at $m = 1$):\n  - $N = 4$.\n  - $b = [2.0,\\, 2.0,\\, 2.0,\\, 2.0]$.\n  - $K^{(0)} = [1.2,\\, 0.9,\\, 0.15,\\, 0.5]$.\n  - $X$ has nonzero entries $X_{0,3} = 0.3$, $X_{1,3} = 0.3$, $X_{2,3} = 0.2$, $X_{0,2} = 0.7$, $X_{1,2} = 0.6$; all other entries are $0$.\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.4$, $m = 2.0$.\n  - $D^{(0)} = \\{3\\}$.\n\nProgramming task:\n- Implement the above contagion dynamics precisely as stated.\n- For each case, return a list $[\\text{Loss},\\, \\text{Count}]$, where $\\text{Loss}$ is a floating-point number and $\\text{Count}$ is an integer. You may round intermediate computations as needed, but your final $\\text{Loss}$ should be computed using standard floating-point arithmetic and is expected to be accurate to at least $10^{-6}$ in absolute error.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s result is itself a two-element list. For example, your output must look like\n$[\\,[\\text{Loss}_A,\\text{Count}_A],[\\text{Loss}_B,\\text{Count}_B],[\\text{Loss}_C,\\text{Count}_C],[\\text{Loss}_D,\\text{Count}_D]\\,]$\nwith no spaces in the printed line.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens.\n- The number of obligors is $N$.\n- The bank's exposures at default vector is $b \\in \\mathbb{R}_{\\ge 0}^{N}$.\n- The initial equity buffer vector is $K^{(0)} \\in \\mathbb{R}_{> 0}^{N}$.\n- The inter-obligor exposure matrix is $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$ with $X_{i,j}$ being the exposure of obligor $i$ to obligor $j$, and $X_{ii} = 0$.\n- The bank's loss-given-default is $L^{\\text{bank}} \\in [0,1]$.\n- The inter-obligor loss-given-default is $L^{\\text{inter}} \\in [0,1]$.\n- The systemic loss multiplier is $m \\ge 1$.\n- The initial set of exogenously defaulted obligors is $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$.\n- The numerical tolerance for default is $\\varepsilon = 10^{-12}$. An obligor $i$ defaults if its equity $K_i^{(t)} \\le \\varepsilon$.\n- The contagion dynamics are governed by the iterative rule for equity update:\n$$K_i^{(t)} = K_i^{(t-1)} - \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)$$\nwhere $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ and $D^{(-1)} := \\emptyset$.\n- Contagion terminates at the first iteration $T$ where $\\Delta D^{(T)} = \\emptyset$. The final default set is $D^{(\\infty)} = D^{(T)}$.\n- The bank's portfolio loss is $\\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}$.\n- Four specific test cases (A, B, C, D) are provided with all necessary numerical values for the above parameters.\n\nStep 2: Validate Using Extracted Givens.\n- **Scientifically Grounded**: The problem describes a discrete-time network model of financial contagion. The concepts of exposure, equity, loss-given-default, and default cascades are standard in the field of financial risk management and computational economics. The model, while a simplification, is based on the established principle that losses on assets (interbank exposures) can deplete a firm's equity, leading to its own failure. Thus, the problem is scientifically sound.\n- **Well-Posed**: The process is an iterative algorithm. The state of the system is the set of defaulted obligors. This set is monotonically non-decreasing. Since the total number of obligors $N$ is finite, the contagion process must terminate in at most $N$ iterations. The final state (set of all defaulted obligors) is uniquely determined by the initial conditions and the rules of the system. Therefore, a unique, stable solution for the total loss and the number of defaults exists. The problem is well-posed.\n- **Objective**: The problem is defined with precise mathematical notation and unambiguous rules. All parameters are given, and the required outputs are clearly specified. There is no subjective language.\n\nStep 3: Verdict and Action.\nThe problem is valid as it is scientifically grounded, well-posed, and objective. A solution will be provided.\n\nThe contagion process is modeled as a sequence of discrete time steps, indexed by $t \\in \\{0, 1, 2, \\dots\\}$. We must track the state of each obligor, which involves its equity capital and its default status.\n\nThe state variables at iteration $t$ are:\n- $K^{(t)} \\in \\mathbb{R}^N$: The vector of equity for all obligors.\n- $D^{(t)} \\subseteq \\{0, 1, \\dots, N-1\\}$: The cumulative set of all obligors that have defaulted up to and including iteration $t$.\n- $\\Delta D^{(t)} = D^{(t)} \\setminus D^{(t-1)}$: The set of obligors that newly defaulted at iteration $t$.\n\nThe algorithm proceeds as follows:\n1.  **Initialization ($t=0$)**:\n    - The equity vector is the initial equity, $K^{(0)}$.\n    - The cumulative default set $D^{(-1)}$ is defined as the empty set, $\\emptyset$.\n    - The initial default set $D^{(0)}$ is given exogenously.\n    - The set of newly defaulted obligors at $t=0$ is $\\Delta D^{(0)} = D^{(0)} \\setminus D^{(-1)} = D^{(0)}$.\n\n2.  **Iterative Contagion ($t=1, 2, \\dots, T$)**: The core of the simulation is a loop that continues as long as a new wave of defaults occurs. Let us define the state at the beginning of iteration $t$ as $(K^{(t-1)}, D^{(t-1)})$, with $\\Delta D^{(t-1)}$ being the set of firms that just defaulted.\n    - If $\\Delta D^{(t-1)} = \\emptyset$, the contagion has ceased. The process terminates. The final state is reached.\n    - Otherwise, we must calculate the impact on the remaining solvent obligors. For each obligor $i$ that is not yet in default (i.e., $i \\notin D^{(t-1)}$), we calculate the loss it incurs from counterparties in $\\Delta D^{(t-1)}$:\n    $$\n    L_{i, t} = \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)\n    $$\n    - The equity for these solvent obligors is then updated:\n    $$\n    K_i^{(t)} = K_i^{(t-1)} - L_{i, t}\n    $$\n    For obligors already in default or for obligors that do not have their equity updated at this step, their equity value remains unchanged, i.e., $K_i^{(t)} = K_i^{(t-1)}$.\n    - A new default set for this iteration is identified. An obligor $i$ that was previously solvent ($i \\notin D^{(t-1)}$) now defaults if its equity is wiped out, i.e., $K_i^{(t)} \\le \\varepsilon$. Let this set be $\\Delta D_{\\text{new}}^{(t)}$.\n    - The cumulative default set is updated: $D^{(t)} = D^{(t-1)} \\cup \\Delta D_{\\text{new}}^{(t)}$.\n    - The set of newly defaulted obligors for the *next* iteration is $\\Delta D^{(t)} = \\Delta D_{\\text{new}}^{(t)}$. The process then repeats for $t+1$.\n\n3.  **Termination and Final Calculation**: The loop terminates at an iteration $T$ when $\\Delta D^{(T-1)} = \\emptyset$.\n    - The final set of all defaulted obligors is $D^{(\\infty)} = D^{(T-1)}$.\n    - The total number of defaults is the cardinality of this set, $|D^{(\\infty)}|$.\n    - The total loss to the bank is calculated based on its exposures to the obligors in the final default set:\n    $$\n    \\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}\n    $$\n\nThis algorithm is implemented for each of the four test cases provided. The parameters for each case are used to initialize the simulation, which is then run to completion to find the final loss and the count of defaulted obligors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n\n    # Numerical tolerance for default check\n    EPSILON = 1e-12\n\n    test_cases = [\n        # Case A: second-order contagion, moderate amplification\n        {\n            \"N\": 4,\n            \"b\": np.array([10.0, 5.0, 8.0, 6.0]),\n            \"K0\": np.array([2.0, 2.0, 1.0, 1.1]),\n            \"X_sparse\": {(1, 0): 1.6, (2, 0): 1.8, (3, 0): 0.6, (1, 2): 0.9, (3, 2): 1.2},\n            \"L_bank\": 0.45,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {0},\n        },\n        # Case B: no contagion network\n        {\n            \"N\": 3,\n            \"b\": np.array([5.0, 5.0, 5.0]),\n            \"K0\": np.array([1.0, 1.0, 1.0]),\n            \"X_sparse\": {},\n            \"L_bank\": 0.4,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {1},\n        },\n        # Case C: boundary condition at default threshold, full cascade\n        {\n            \"N\": 3,\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"K0\": np.array([1.0, 0.5, 0.5]),\n            \"X_sparse\": {(1, 0): 1.0, (2, 1): 1.0},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.5,\n            \"m\": 1.0,\n            \"D0\": {0},\n        },\n        # Case D: amplification creates contagion that would not occur at m=1\n        {\n            \"N\": 4,\n            \"b\": np.array([2.0, 2.0, 2.0, 2.0]),\n            \"K0\": np.array([1.2, 0.9, 0.15, 0.5]),\n            \"X_sparse\": {(0, 3): 0.3, (1, 3): 0.3, (2, 3): 0.2, (0, 2): 0.7, (1, 2): 0.6},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.4,\n            \"m\": 2.0,\n            \"D0\": {3},\n        },\n    ]\n\n    def run_simulation(case):\n        \"\"\"\n        Runs the contagion simulation for a single test case.\n        \"\"\"\n        N = case[\"N\"]\n        b = case[\"b\"]\n        K = case[\"K0\"].copy()\n        X_sparse = case[\"X_sparse\"]\n        L_bank = case[\"L_bank\"]\n        L_inter = case[\"L_inter\"]\n        m = case[\"m\"]\n        D0 = case[\"D0\"]\n\n        # Construct the dense exposure matrix X\n        X = np.zeros((N, N))\n        for (i, j), val in X_sparse.items():\n            X[i, j] = val\n\n        # State variables for the simulation\n        D_final = set(D0)\n        newly_defaulted = set(D0)\n        \n        effective_lgd = m * L_inter\n\n        while newly_defaulted:\n            last_wave_defaults = list(newly_defaulted)\n            newly_defaulted = set()\n            \n            solvent_obligors = [i for i in range(N) if i not in D_final]\n            \n            if not solvent_obligors:\n                break\n                \n            # Calculate losses for solvent obligors from the last wave of defaults\n            losses = X[solvent_obligors, :][:, last_wave_defaults].sum(axis=1) * effective_lgd\n\n            # Update equity and check for new defaults\n            for idx, obligor_idx in enumerate(solvent_obligors):\n                K[obligor_idx] -= losses[idx]\n                if K[obligor_idx] <= EPSILON:\n                    newly_defaulted.add(obligor_idx)\n\n            D_final.update(newly_defaulted)\n\n        # Calculate final results\n        final_loss = b[list(D_final)].sum() * L_bank\n        num_defaults = len(D_final)\n        \n        return [final_loss, num_defaults]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(case)\n        results.append(f\"[{result[0]},{result[1]}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}