{"hands_on_practices": [{"introduction": "Calculating Value at Risk for a portfolio is a fundamental skill in finance. This exercise provides hands-on practice with the historical simulation method, a widely used technique to estimate VaR. You will construct a portfolio of correlated assets, a realistic scenario that requires modeling the interdependencies between asset returns, and then apply the empirical quantile method to determine the portfolio's risk exposure [@problem_id:2390037].", "id": "2390037", "problem": "You are given a portfolio of $N=10$ assets. Let the portfolio weight vector be $\\mathbf{w} \\in \\mathbb{R}^{10}$, and let the historical returns over $T$ days be the matrix $\\mathbf{R} \\in \\mathbb{R}^{T \\times 10}$ whose $t$-th row is $\\mathbf{r}_t^\\top$. The daily portfolio return is $R^p_t = \\sum_{i=1}^{10} w_i r_{t,i}$ and the daily portfolio loss is $L_t = - R^p_t$. The $99\\%$ Value at Risk (Value at Risk (VaR)) is defined for a loss random variable $L$ as the smallest number $v$ such that $\\mathbb{P}(L \\le v) \\ge 0.99$. Under historical simulation, replace the distribution of $L$ with its empirical distribution from the sample $\\{L_t\\}_{t=1}^T$. Using the empirical distribution function, define the empirical $p$-quantile as the order statistic at index $k = \\lceil p \\cdot T \\rceil$ (with $1$-based indexing), that is, sort $\\{L_t\\}$ in nondecreasing order to obtain $L_{(1)} \\le \\cdots \\le L_{(T)}$, and set $\\widehat{q}_p = L_{(k)}$ with $k = \\lceil p \\cdot T \\rceil$. Then the historical simulation estimate of the $99\\%$ VaR is $\\widehat{\\mathrm{VaR}}_{0.99} = \\widehat{q}_{0.99}$. All outputs must be reported as decimals (for example, report $0.025$ instead of $2.5\\%$).\n\nTo ensure a scientifically sound and reproducible dataset that captures cross-asset correlation, construct the historical returns as follows. Let the per-asset daily volatility vector be $\\boldsymbol{\\sigma} = (\\sigma_1,\\ldots,\\sigma_{10})$ with $\\sigma_i = 0.01 + 0.002 \\cdot (i-1)$ for $i \\in \\{1,\\ldots,10\\}$. Define the correlation matrix $\\mathbf{C} \\in \\mathbb{R}^{10 \\times 10}$ by $C_{ij} = \\rho^{|i-j|}$ with $\\rho = 0.6$, and define the covariance matrix $\\boldsymbol{\\Sigma} = \\mathbf{D} \\mathbf{C} \\mathbf{D}$, where $\\mathbf{D} = \\mathrm{diag}(\\boldsymbol{\\sigma})$. For a given $T$, generate a historical sample by drawing $\\mathbf{Z} \\in \\mathbb{R}^{T \\times 10}$ with independent and identically distributed standard normal entries and then setting $\\mathbf{R} = \\mathbf{Z} \\mathbf{L}^\\top$, where $\\mathbf{L}$ is the lower-triangular Cholesky factor of $\\boldsymbol{\\Sigma}$ such that $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^\\top$. Use a fixed pseudo-random seed $12345$ for every dataset generation. This process induces correlated asset returns with the specified covariance, serving as the historical sample for the historical simulation. Angles are not involved in this task. No physical units apply; report all losses as unitless decimal numbers.\n\nImplement a program that computes $\\widehat{\\mathrm{VaR}}_{0.99}$ by the empirical order statistic definition above for each of the following test cases. In all cases, use the generation method and the fixed seed $12345$ to produce the $\\mathbf{R}$ matrix for the specified $T$.\n\nTest Suite:\n- Case A (general case): $T = 1000$, weights $w_i = 1/10$ for all $i \\in \\{1,\\ldots,10\\}$.\n- Case B (concentration edge case): $T = 500$, weights $w_1 = 1$ and $w_i = 0$ for all $i \\in \\{2,\\ldots,10\\}$.\n- Case C (risk-parity style case): $T = 252$, weights given by $w_i \\propto 1/\\sigma_i$ for $i \\in \\{1,\\ldots,10\\}$, normalized so that $\\sum_{i=1}^{10} w_i = 1$.\n\nAlgorithmic requirements:\n- Use the empirical quantile with $k = \\lceil 0.99 \\cdot T \\rceil$ as defined above (no interpolation).\n- For each case, compute the loss series $\\{L_t\\}_{t=1}^T$ from the generated returns and the specified weights, and then compute $\\widehat{\\mathrm{VaR}}_{0.99}$.\n- Round each final $\\widehat{\\mathrm{VaR}}_{0.99}$ to $6$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the three rounded results for Cases A, B, and C, in that order, as a comma-separated list enclosed in square brackets. For example, the output must look like $[x_A,x_B,x_C]$ with each $x$ shown to exactly $6$ decimal places.", "solution": "The problem is well-defined, scientifically sound, and internally consistent. We shall proceed with a formal solution.\n\nThe objective is to calculate the $99\\%$ Value at Risk ($\\mathrm{VaR}_{0.99}$) for a portfolio of $N=10$ assets under three distinct scenarios. The calculation will use the historical simulation method, where the historical data is synthetically generated according to a specified statistical model.\n\n**1. Synthetic Generation of Asset Returns**\n\nThe core of the simulation is the generation of a $T \\times N$ matrix of historical returns, $\\mathbf{R}$, where $T$ is the number of days and $N=10$ is the number of assets. The returns are modeled as draws from a multivariate normal distribution with a mean of zero and a specified covariance matrix $\\boldsymbol{\\Sigma}$. This covariance structure is crucial for capturing the interdependencies between asset price movements.\n\nFirst, we define the components of the covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{10 \\times 10}$.\nThe per-asset daily volatility vector is $\\boldsymbol{\\sigma} = (\\sigma_1, \\sigma_2, \\dots, \\sigma_{10})^\\top$, where the volatility of the $i$-th asset is given by the formula:\n$$\n\\sigma_i = 0.01 + 0.002 \\cdot (i-1) \\quad \\text{for } i \\in \\{1, 2, \\dots, 10\\}\n$$\nThis creates a spectrum of volatilities, starting from $\\sigma_1 = 0.01$ and increasing to $\\sigma_{10} = 0.028$. These volatilities form the diagonal of a matrix $\\mathbf{D} = \\mathrm{diag}(\\boldsymbol{\\sigma})$.\n\nNext, the correlation structure is defined by a matrix $\\mathbf{C} \\in \\mathbb{R}^{10 \\times 10}$, where the correlation between asset $i$ and asset $j$ is:\n$$\nC_{ij} = \\rho^{|i-j|}\n$$\nThe parameter $\\rho$ is given as $0.6$. This structure implies that assets closer in index are more strongly correlated.\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ is then assembled as:\n$$\n\\boldsymbol{\\Sigma} = \\mathbf{D} \\mathbf{C} \\mathbf{D}\n$$\nTo generate correlated random variates, we use the Cholesky decomposition of $\\boldsymbol{\\Sigma}$. We find a lower-triangular matrix $\\mathbf{L}$ such that $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^\\top$.\n\nThe historical return matrix $\\mathbf{R}$ is generated by first creating a matrix $\\mathbf{Z} \\in \\mathbb{R}^{T \\times 10}$ of independent and identically distributed standard normal random variables ($\\mathcal{N}(0,1)$). The correlated returns are then obtained by the transformation:\n$$\n\\mathbf{R} = \\mathbf{Z} \\mathbf{L}^\\top\n$$\nEach row $\\mathbf{r}_t^\\top$ of $\\mathbf{R}$ is a random vector representing the daily returns of the $10$ assets, and these vectors are drawn from $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$. For reproducibility, the pseudo-random number generator is initialized with a fixed seed of $12345$ for each data generation process.\n\n**2. Historical Simulation for Value at Risk ($\\mathrm{VaR}$)**\n\nGiven a portfolio weight vector $\\mathbf{w} = (w_1, w_2, \\dots, w_{10})^\\top$, the daily return of the portfolio, $R^p_t$, is the weighted sum of the individual asset returns:\n$$\nR^p_t = \\mathbf{r}_t^\\top \\mathbf{w} = \\sum_{i=1}^{10} w_i r_{t,i}\n$$\nThe corresponding daily portfolio loss is defined as the negative of the return: $L_t = -R^p_t$. This calculation is performed for each day $t$ from $1$ to $T$, yielding a sample of historical losses $\\{L_t\\}_{t=1}^T$.\n\nThe historical simulation approach estimates the $\\mathrm{VaR}$ from the empirical distribution of this loss sample. The $p$-quantile of the loss distribution is estimated by finding a specific value from the sorted loss data. We sort the losses in non-decreasing order:\n$$\nL_{(1)} \\le L_{(2)} \\le \\cdots \\le L_{(T)}\n$$\nThe problem specifies the empirical $p$-quantile, $\\widehat{q}_p$, to be the $k$-th order statistic, $L_{(k)}$, where the index $k$ is given by:\n$$\nk = \\lceil p \\cdot T \\rceil\n$$\nThis is a 1-based index. For the $99\\%$ $\\mathrm{VaR}$, we set $p=0.99$, so the historical estimate is:\n$$\n\\widehat{\\mathrm{VaR}}_{0.99} = L_{(k)} \\quad \\text{with} \\quad k = \\lceil 0.99 \\cdot T \\rceil\n$$\n\n**3. Application to Test Cases**\n\nWe now apply this methodology to the three specified test cases. The covariance matrix $\\boldsymbol{\\Sigma}$ and its Cholesky factor $\\mathbf{L}$ are common to all cases as they depend only on $N=10$ and the fixed parameters $\\boldsymbol{\\sigma}$ and $\\rho$.\n\n**Case A: General Case**\n-   Time periods: $T = 1000$.\n-   Portfolio weights: An equally weighted portfolio, $w_i = 1/10 = 0.1$ for all $i$.\n-   $\\mathrm{VaR}$ index: $k = \\lceil 0.99 \\cdot 1000 \\rceil = \\lceil 990 \\rceil = 990$.\n-   Procedure:\n    1.  Generate $\\mathbf{R} \\in \\mathbb{R}^{1000 \\times 10}$ using the seed $12345$.\n    2.  Compute the loss series $\\{L_t\\}_{t=1}^{1000}$ using $\\mathbf{w} = (0.1, \\dots, 0.1)^\\top$.\n    3.  Sort the losses and select the $990$-th value, $L_{(990)}$.\n\n**Case B: Concentration Edge Case**\n-   Time periods: $T = 500$.\n-   Portfolio weights: A portfolio fully concentrated in the first asset, $\\mathbf{w} = (1, 0, \\dots, 0)^\\top$.\n-   $\\mathrm{VaR}$ index: $k = \\lceil 0.99 \\cdot 500 \\rceil = \\lceil 495 \\rceil = 495$.\n-   Procedure:\n    1.  Generate $\\mathbf{R} \\in \\mathbb{R}^{500 \\times 10}$ using the seed $12345$.\n    2.  Compute the loss series $\\{L_t\\}_{t=1}^{500}$. Here, $L_t = -r_{t,1}$.\n    3.  Sort the losses and select the $495$-th value, $L_{(495)}$.\n\n**Case C: Risk-Parity Style Case**\n-   Time periods: $T = 252$ (approximating one trading year).\n-   Portfolio weights: Weights are inversely proportional to asset volatility, $w_i \\propto 1/\\sigma_i$. This is a \"risk-parity\" inspired allocation, where less volatile assets receive higher weight. The weights must be normalized to sum to $1$:\n    $$\n    w_i = \\frac{1/\\sigma_i}{\\sum_{j=1}^{10} (1/\\sigma_j)}\n    $$\n-   $\\mathrm{VaR}$ index: $k = \\lceil 0.99 \\cdot 252 \\rceil = \\lceil 249.48 \\rceil = 250$.\n-   Procedure:\n    1.  Calculate the normalized weights $\\mathbf{w}$.\n    2.  Generate $\\mathbf{R} \\in \\mathbb{R}^{252 \\times 10}$ using the seed $12345$.\n    3.  Compute the loss series $\\{L_t\\}_{t=1}^{252}$ using the calculated $\\mathbf{w}$.\n    4.  Sort the losses and select the $250$-th value, $L_{(250)}$.\n\nFor each case, the computed $\\widehat{\\mathrm{VaR}}_{0.99}$ will be rounded to $6$ decimal places as required. The implementation will use 0-based array indexing, so the value at index $k$ in a 1-based system corresponds to the value at index $k-1$ in code.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef generate_returns(T, N, sigma_vec, corr_matrix, seed):\n    \"\"\"\n    Generates a matrix of correlated asset returns.\n\n    Args:\n        T (int): Number of time periods (days).\n        N (int): Number of assets.\n        sigma_vec (np.ndarray): Vector of asset volatilities.\n        corr_matrix (np.ndarray): Asset correlation matrix.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        np.ndarray: A T x N matrix of simulated daily returns.\n    \"\"\"\n    # Construct the covariance matrix: Sigma = D * C * D\n    D = np.diag(sigma_vec)\n    cov_matrix = D @ corr_matrix @ D\n\n    # Perform Cholesky decomposition: Sigma = L * L^T\n    # numpy.linalg.cholesky returns the lower-triangular matrix L.\n    L = np.linalg.cholesky(cov_matrix)\n\n    # Generate independent standard normal random variables\n    rng = np.random.default_rng(seed)\n    Z = rng.standard_normal((T, N))\n\n    # Generate correlated returns: R = Z * L^T\n    R = Z @ L.T\n    return R\n\ndef calculate_var(T, weights, returns):\n    \"\"\"\n    Calculates the 99% Value at Risk using historical simulation.\n\n    Args:\n        T (int): Number of time periods.\n        weights (np.ndarray): Portfolio weights vector.\n        returns (np.ndarray): Matrix of historical returns.\n\n    Returns:\n        float: The 99% VaR, rounded to 6 decimal places.\n    \"\"\"\n    # Calculate daily portfolio returns: Rp_t = R_t^T * w\n    portfolio_returns = returns @ weights\n    \n    # Calculate daily portfolio losses: L_t = -Rp_t\n    losses = -portfolio_returns\n    \n    # Sort losses in non-decreasing order\n    sorted_losses = np.sort(losses)\n    \n    # Calculate the index k for the 99% empirical quantile\n    # k = ceil(p * T), with 1-based indexing\n    p = 0.99\n    k = math.ceil(p * T)\n    \n    # Get the VaR value. Index is k-1 due to 0-based indexing in Python.\n    var_99 = sorted_losses[k - 1]\n    \n    return round(var_99, 6)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three test cases.\n    \"\"\"\n    N = 10\n    rho = 0.6\n    \n    # Define asset volatilities\n    sigma_vec = np.array([0.01 + 0.002 * (i - 1) for i in range(1, N + 1)])\n    \n    # Define correlation matrix\n    indices = np.arange(N)\n    corr_matrix = rho ** np.abs(indices - indices[:, np.newaxis])\n    \n    # Fixed seed for all data generation\n    seed = 12345\n    \n    # Define test cases\n    test_cases = [\n        {'name': 'A', 'T': 1000, 'weights_def': 'equal'},\n        {'name': 'B', 'T': 500, 'weights_def': 'concentrated'},\n        {'name': 'C', 'T': 252, 'weights_def': 'risk-parity'}\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        T = case['T']\n        \n        # Generate returns for the specific T\n        returns = generate_returns(T, N, sigma_vec, corr_matrix, seed)\n        \n        # Define weights for the case\n        weights = np.zeros(N)\n        if case['weights_def'] == 'equal':\n            weights = np.full(N, 1.0 / N)\n        elif case['weights_def'] == 'concentrated':\n            weights[0] = 1.0\n        elif case['weights_def'] == 'risk-parity':\n            inv_sigma = 1.0 / sigma_vec\n            weights = inv_sigma / np.sum(inv_sigma)\n        \n        # Calculate VaR for the case\n        var_result = calculate_var(T, weights, returns)\n        results.append(var_result)\n\n    # Format the final output string as [x_A,x_B,x_C]\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n\n```"}, {"introduction": "A reliable risk measure should reflect the benefits of diversification, meaning the risk of a portfolio should not exceed the sum of its parts. This exercise challenges you to investigate this property, known as subadditivity, for Value at Risk. Through carefully constructed scenarios, you will use simulation to demonstrate that VaR can fail this fundamental test, providing a powerful insight into one of its most significant theoretical limitations [@problem_id:2412240].", "id": "2412240", "problem": "Construct a self-contained program that computes, from first principles, whether Value-at-Risk (VaR) is subadditive in specifically defined cases. Let a loss be a real-valued random variable. For a loss random variable $X$ with cumulative distribution function $F_X(x)$ and a level $\\\\alpha \\\\in (0,1)$, define the Value-at-Risk at level $\\\\alpha$ as\n$$\n\\\\operatorname{VaR}_{\\\\alpha}(X) = \\\\inf\\\\{x \\\\in \\\\mathbb{R} : F_X(x) \\\\ge \\\\alpha\\\\}.\n$$\nGiven an independent and identically distributed sample $L_1,\\\\dots,L_n$ from $X$, define the empirical distribution function $F_n(x) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\mathbf{1}\\\\{L_i \\\\le x\\\\}$ and the empirical Value-at-Risk estimator\n$$\n\\\\widehat{\\\\operatorname{VaR}}_{\\\\alpha}(X) = \\\\inf\\\\{x \\\\in \\\\mathbb{R} : F_n(x) \\\\ge \\\\alpha\\\\}.\n$$\nConsider two loss random variables $A$ and $B$ for each test case below, and define the portfolio loss $P = A + B$. For each test case $j$, compute the boolean outcome\n$$\nI_j = \\\\big[\\\\widehat{\\\\operatorname{VaR}}_{\\\\alpha}(P) > \\\\widehat{\\\\operatorname{VaR}}_{\\\\alpha}(A) + \\\\widehat{\\\\operatorname{VaR}}_{\\\\alpha}(B)\\\\big].\n$$\nUse the same sample size $n$ and the same fixed pseudorandom seed $s$ for all cases. All probabilities are to be interpreted as decimals in $[0,1]$. No physical units apply.\n\nYour program must implement the following test suite. In all cases, the draw size is $n = \\\\;400{,}000$ and the pseudorandom seed is $s = \\\\;20231407$. All Bernoulli events and standard normal variables are to be drawn according to the specified dependence structure.\n\n- Test case $1$ (independent, rare large losses): $\\\\alpha = \\\\;0.95$. Asset $A$ has loss $10$ with probability $0.03$ and loss $0$ otherwise. Asset $B$ has the same distribution. $A$ and $B$ are independent.\n- Test case $2$ (independent, asymmetric rare losses): $\\\\alpha = \\\\;0.975$. Asset $A$ has loss $8$ with probability $0.02$ and loss $0$ otherwise. Asset $B$ has loss $12$ with probability $0.02$ and loss $0$ otherwise. $A$ and $B$ are independent.\n- Test case $3$ (comonotonic rare losses): $\\\\alpha = \\\\;0.95$. Asset $A$ has loss $10$ with probability $0.03$ and loss $0$ otherwise. Asset $B$ has the same marginal distribution. The dependence is comonotonic: there exists a uniform $U$ on $[0,1]$ such that both $A$ and $B$ equal their loss level if and only if $U < 0.03$, and otherwise both equal $0$.\n- Test case $4$ (correlated Gaussian losses): $\\\\alpha = \\\\;0.99$. Asset $A$ is Gaussian with mean $0$ and standard deviation $1$. Asset $B$ is Gaussian with mean $0$ and standard deviation $1.2$. The correlation between $A$ and $B$ is $0.6$.\n- Test case $5$ (perfectly negatively dependent Gaussian losses): $\\\\alpha = \\\\;0.99$. Asset $A$ is Gaussian with mean $0$ and standard deviation $1$. Asset $B$ equals $-A$ almost surely (perfect negative dependence), so the portfolio loss is identically $0$.\n\nFor each case, generate exactly $n$ paired draws $(A_i,B_i)$ consistent with the stated distributions and dependence. Compute the empirical Value-at-Risk at the specified $\\\\alpha$ for $A$, $B$, and $P = A+B$ using the definition of $\\\\widehat{\\\\operatorname{VaR}}_{\\\\alpha}$ above. Then compute $I_j$ as defined.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,true,false]\") but using capitalized Python boolean literals. That is, the exact output format must be\n\"[result1,result2,result3,result4,result5]\" with each result either \"True\" or \"False\".", "solution": "The problem requires a computational investigation into the subadditivity of the Value-at-Risk ($\\operatorname{VaR}$) measure. A risk measure $\\rho$ is defined as subadditive if for any two loss random variables $X$ and $Y$, the inequality $\\rho(X+Y) \\le \\rho(X) + \\rho(Y)$ holds. This property formalizes the principle of diversification: the risk of a combined portfolio should not be greater than the sum of the risks of its individual components. Value-at-Risk is a widely used risk measure, but it is not a coherent risk measure in the sense of Artzner et al. (1999) precisely because it can fail to be subadditive. This exercise demonstrates this failure through specific, constructed examples.\n\nThe task is to compute the boolean outcome of the inequality $\\widehat{\\operatorname{VaR}}_{\\alpha}(P) > \\widehat{\\operatorname{VaR}}_{\\alpha}(A) + \\widehat{\\operatorname{VaR}}_{\\alpha}(B)$ for five distinct test cases, where $A$ and $B$ are loss random variables, $P=A+B$ is the portfolio loss, and $\\widehat{\\operatorname{VaR}}_{\\alpha}$ is the empirical Value-at-Risk estimator at a confidence level $\\alpha$.\n\nThe empirical $\\operatorname{VaR}$ is defined as $\\widehat{\\operatorname{VaR}}_{\\alpha}(X) = \\inf\\{x \\in \\mathbb{R} : F_n(x) \\ge \\alpha\\}$, where $F_n(x)$ is the empirical cumulative distribution function from a sample of size $n$. For a sample of losses $L_1, \\dots, L_n$, let the sorted sample be $L_{(1)} \\le L_{(2)} \\le \\dots \\le L_{(n)}$. The empirical CDF $F_n(x)$ is a step function that jumps by $1/n$ at each data point. The condition $F_n(x) \\ge \\alpha$ is equivalent to saying that at least $n\\alpha$ of the sample points are less than or equal to $x$. This means $x$ must be at least as large as the $\\lceil n\\alpha \\rceil$-th order statistic. Therefore, the empirical VaR is precisely the value of this order statistic:\n$$\n\\widehat{\\operatorname{VaR}}_{\\alpha}(X) = L_{(\\lceil n\\alpha \\rceil)}\n$$\nOur general procedure for each test case is as follows:\n1.  Initialize a pseudorandom number generator with the fixed seed $s = 20231407$.\n2.  Generate a sample of $n = 400,000$ paired draws $(A_i, B_i)$ for $i=1, \\dots, n$, according to the specified marginal distributions and dependence structure.\n3.  Create the portfolio loss sample $P_i = A_i + B_i$.\n4.  For each of the three samples $\\{A_i\\}$, $\\{B_i\\}$, and $\\{P_i\\}$, compute the empirical $\\operatorname{VaR}$ at the specified level $\\alpha$. This involves sorting the sample and selecting the element at the index $k-1$ (using $0$-based indexing), where $k = \\lceil n\\alpha \\rceil$.\n5.  Evaluate the boolean expression $I_j = [\\widehat{\\operatorname{VaR}}_{\\alpha}(P) > \\widehat{\\operatorname{VaR}}_{\\alpha}(A) + \\widehat{\\operatorname{VaR}}_{\\alpha}(B)]$.\n\nThe specific generation methods for each case are:\n\nTest case $1$ (independent, rare large losses): $\\alpha = 0.95$. The losses for $A$ and $B$ are drawn from a Bernoulli distribution, representing a loss of $10$ with probability $p=0.03$ and $0$ otherwise. Independence is modeled by using two separate streams of uniform random numbers, $U_A$ and $U_B$, to generate the samples for $A$ and $B$. For each $i \\in \\{1, \\dots, n\\}$, $A_i=10$ if $U_{A,i} < 0.03$ and $A_i=0$ otherwise, and similarly for $B_i$ using $U_{B,i}$. The index for the VaR calculation is $k = \\lceil 400,000 \\times 0.95 \\rceil = 380,000$.\n\nTest case $2$ (independent, asymmetric rare losses): $\\alpha = 0.975$. This case is structurally similar to the first, but with asymmetric parameters. Asset $A$ has loss $8$ with probability $0.02$, and asset $B$ has loss $12$ with probability $0.02$. Independence is again modeled with separate random number streams. The VaR index is $k = \\lceil 400,000 \\times 0.975 \\rceil = 390,000$.\n\nTest case $3$ (comonotonic rare losses): $\\alpha = 0.95$. The marginal distributions for $A$ and $B$ are identical to case $1$. However, the dependence is comonotonic. This is modeled by using a single stream of uniform random numbers, $U$. For each $i \\in \\{1, \\dots, n\\}$, both $A_i$ and $B_i$ are set to $10$ if $U_i < 0.03$, and both are $0$ otherwise. This represents a perfect positive dependence in the tail events. The VaR index is $k = \\lceil 400,000 \\times 0.95 \\rceil = 380,000$.\n\nTest case $4$ (correlated Gaussian losses): $\\alpha = 0.99$. $A \\sim \\mathcal{N}(0, 1^2)$ and $B \\sim \\mathcal{N}(0, 1.2^2)$ with correlation $\\rho(A, B) = 0.6$. We generate paired samples from this bivariate normal distribution. This can be achieved by first generating pairs of independent standard normal variates $(Z_{1,i}, Z_{2,i})$ and then applying a linear transformation corresponding to the Cholesky decomposition of the covariance matrix $\\Sigma$:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_A^2 & \\rho \\sigma_A \\sigma_B \\\\ \\rho \\sigma_A \\sigma_B & \\sigma_B^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.72 \\\\ 0.72 & 1.44 \\end{pmatrix}\n$$\nA simpler, equivalent construction is $A_i = Z_{1,i}$ and $B_i = \\sigma_B (\\rho Z_{1,i} + \\sqrt{1-\\rho^2} Z_{2,i})$. The VaR index is $k = \\lceil 400,000 \\times 0.99 \\rceil = 396,000$. For elliptical distributions like the multivariate normal, VaR is known to be subadditive.\n\nTest case $5$ (perfectly negatively dependent Gaussian losses): $\\alpha = 0.99$. Asset $A \\sim \\mathcal{N}(0, 1^2)$ and $B = -A$. This is a case of perfect hedging. We generate a sample of standard normal variates for $A$ and set the sample for $B$ to be its element-wise negation. The portfolio loss is $P = A+B = 0$ for all outcomes. Consequently, its empirical $\\widehat{\\operatorname{VaR}}_{\\alpha}(P)$ must be $0$. The VaR index is $k = \\lceil 400,000 \\times 0.99 \\rceil = 396,000$.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes whether Value-at-Risk (VaR) is subadditive in five specific cases.\n    \"\"\"\n    \n    # Global parameters\n    N_SAMPLES = 400_000\n    SEED = 20231407\n    \n    # Initialize a single random number generator for all simulations\n    rng = np.random.default_rng(SEED)\n\n    test_cases = [\n        # Case 1: Independent, rare large losses\n        {\n            \"alpha\": 0.95,\n            \"A\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"B\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"dep\": \"independent\"\n        },\n        # Case 2: Independent, asymmetric rare losses\n        {\n            \"alpha\": 0.975,\n            \"A\": {\"type\": \"bernoulli\", \"loss\": 8.0, \"p\": 0.02},\n            \"B\": {\"type\": \"bernoulli\", \"loss\": 12.0, \"p\": 0.02},\n            \"dep\": \"independent\"\n        },\n        # Case 3: Comonotonic rare losses\n        {\n            \"alpha\": 0.95,\n            \"A\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"B\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"dep\": \"comonotonic\"\n        },\n        # Case 4: Correlated Gaussian losses\n        {\n            \"alpha\": 0.99,\n            \"A\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.0},\n            \"B\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.2},\n            \"dep\": \"correlated\", \"rho\": 0.6\n        },\n        # Case 5: Perfectly negatively dependent Gaussian losses\n        {\n            \"alpha\": 0.99,\n            \"A\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.0},\n            \"B\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.0},\n            \"dep\": \"negatively_dependent\"\n        }\n    ]\n\n    results = []\n\n    def get_empirical_var(samples, alpha):\n        \"\"\"\n        Calculates the empirical Value-at-Risk.\n        Defined as the k-th order statistic, where k = ceil(n*alpha).\n        \"\"\"\n        n = len(samples)\n        k = math.ceil(n * alpha)\n        # Using k-1 for 0-based indexing\n        sorted_samples = np.sort(samples)\n        return sorted_samples[k - 1]\n\n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        \n        # --- Generate samples for A and B based on the case ---\n        if case[\"dep\"] == \"independent\":\n            u_a = rng.random(size=N_SAMPLES)\n            u_b = rng.random(size=N_SAMPLES)\n            params_a = case[\"A\"]\n            samples_a = np.where(u_a < params_a[\"p\"], params_a[\"loss\"], 0.0)\n            params_b = case[\"B\"]\n            samples_b = np.where(u_b < params_b[\"p\"], params_b[\"loss\"], 0.0)\n\n        elif case[\"dep\"] == \"comonotonic\":\n            u = rng.random(size=N_SAMPLES)\n            params_a = case[\"A\"]\n            samples_a = np.where(u < params_a[\"p\"], params_a[\"loss\"], 0.0)\n            params_b = case[\"B\"]\n            samples_b = np.where(u < params_b[\"p\"], params_b[\"loss\"], 0.0)\n\n        elif case[\"dep\"] == \"correlated\":\n            params_a = case[\"A\"]\n            params_b = case[\"B\"]\n            rho = case[\"rho\"]\n            \n            mean = [params_a[\"mean\"], params_b[\"mean\"]]\n            cov = [[params_a[\"std\"]**2, rho * params_a[\"std\"] * params_b[\"std\"]],\n                   [rho * params_a[\"std\"] * params_b[\"std\"], params_b[\"std\"]**2]]\n            \n            samples = rng.multivariate_normal(mean, cov, size=N_SAMPLES)\n            samples_a = samples[:, 0]\n            samples_b = samples[:, 1]\n            \n        elif case[\"dep\"] == \"negatively_dependent\":\n            params_a = case[\"A\"]\n            samples_a = rng.normal(loc=params_a[\"mean\"], scale=params_a[\"std\"], size=N_SAMPLES)\n            samples_b = -samples_a\n        \n        # --- Create portfolio samples ---\n        samples_p = samples_a + samples_b\n        \n        # --- Calculate VaR for A, B, and Portfolio P ---\n        var_a = get_empirical_var(samples_a, alpha)\n        var_b = get_empirical_var(samples_b, alpha)\n        var_p = get_empirical_var(samples_p, alpha)\n        \n        # --- Check for subadditivity violation ---\n        is_violated = var_p > (var_a + var_b)\n        results.append(is_violated)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "While VaR tells us the loss that is unlikely to be exceeded, it says nothing about the *magnitude* of the loss when it is. To address this, we turn to Expected Shortfall ($ES$), a measure that quantifies the average loss in the worst-case scenarios. This practice will guide you to compare $\\operatorname{VaR}$ and $\\operatorname{ES}$ by simulating portfolio returns from distributions with varying tail thickness, revealing how $\\operatorname{ES}$ provides a more comprehensive assessment of tail risk [@problem_id:2390682].", "id": "2390682", "problem": "Construct a complete, runnable program that, for a fixed equally weighted portfolio of $M$ assets with independent and identically distributed returns, estimates and compares the Value at Risk (VaR) and Expected Shortfall (ES) at confidence level $0.95$ when each asset return is distributed as a Student's $t$-distribution with varying degrees of freedom. All returns are to be interpreted as dimensionless decimal returns.\n\nLet $R_{i}$ denote the return of asset $i$ in a single scenario, for $i \\in \\{1,\\dots,M\\}$. For a given degrees-of-freedom parameter $\\nu$, assume $R_{i} = s(\\nu)\\,T_{i}$ where $T_{i}$ has a Student's $t$-distribution with $\\nu$ degrees of freedom and unit scale, and $s(\\nu)$ is the scaling factor\n$$\ns(\\nu) \\equiv \\sqrt{\\frac{\\nu - 2}{\\nu}},\n$$\nwhich ensures $\\operatorname{Var}(R_{i}) = 1$ for $\\nu &gt; 2$. Define the equally weighted portfolio return in a scenario by\n$$\nR_{p} \\equiv \\frac{1}{M}\\sum_{i=1}^{M} R_{i},\n$$\nand the corresponding portfolio loss by\n$$\nL \\equiv -R_{p}.\n$$\n\nDefine the Value at Risk (VaR) at confidence level $\\alpha \\in (0,1)$ as\n$$\n\\operatorname{VaR}_{\\alpha}(L) \\equiv \\inf\\{x \\in \\mathbb{R} : \\mathbb{P}(L \\le x) \\ge \\alpha \\},\n$$\nand the Expected Shortfall (ES), also called Conditional Value at Risk, as\n$$\n\\operatorname{ES}_{\\alpha}(L) \\equiv \\mathbb{E}\\!\\left[\\,L \\,\\middle|\\, L \\ge \\operatorname{VaR}_{\\alpha}(L)\\right].\n$$\n\nYour program must:\n- Generate $N$ independent scenarios of the vector $(R_{1},\\dots,R_{M})$ for each specified $\\nu$, using the construction above with $T_{i}$ independent across $i$ and across scenarios, and compute the associated portfolio loss $L$ in each scenario.\n- Use the empirical distribution of the simulated $L$ to compute the empirical estimators of $\\operatorname{VaR}_{\\alpha}(L)$ and $\\operatorname{ES}_{\\alpha}(L)$, where the empirical $\\operatorname{VaR}_{\\alpha}(L)$ is the order statistic corresponding to the index $k \\equiv \\lceil \\alpha N \\rceil$, and the empirical $\\operatorname{ES}_{\\alpha}(L)$ is the sample average of all simulated losses greater than or equal to that empirical $\\operatorname{VaR}_{\\alpha}(L)$.\n\nTest Suite and parameters to use:\n- Number of assets: $M = 10$.\n- Confidence level: $\\alpha = 0.95$.\n- Number of scenarios per test: $N = 100000$.\n- Random number generator seed must be fixed at $123456$.\n- Degrees of freedom to test (each is a separate test case): $\\nu \\in \\{2.2,\\,3.0,\\,5.0,\\,30.0,\\,100.0\\}$.\n\nYour program should produce a single line of output containing the results aggregated across all test cases as a Python list literal. For each test case, return a two-element list $[\\widehat{\\operatorname{VaR}}_{0.95},\\widehat{\\operatorname{ES}}_{0.95}]$ of floating-point numbers in decimal form (not percentages). Aggregate the $5$ test cases in the order of the list of $\\nu$ values given above. The final output format must therefore be a single line of the form\n[[v_1,e_1],[v_2,e_2],[v_3,e_3],[v_4,e_4],[v_5,e_5]]\nwhere each $v_j$ and $e_j$ is the empirical estimate of $\\operatorname{VaR}_{0.95}$ and $\\operatorname{ES}_{0.95}$ for the corresponding $\\nu$, expressed as floating-point numbers.", "solution": "The task is to construct a program to estimate Value at Risk ($\\operatorname{VaR}$) and Expected Shortfall ($\\operatorname{ES}$) for an equally weighted portfolio. The returns of the constituent assets are modeled as independent and identically distributed random variables following a scaled Student's $t$-distribution. The estimation is to be performed for several values of the degrees-of-freedom parameter, $\\nu$.\n\nBefore proceeding to a solution, a rigorous validation of the problem statement is mandatory.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   Portfolio composition: Equally weighted, with $M = 10$ assets.\n-   Asset returns: $R_i$ for $i \\in \\{1,\\dots,M\\}$ are independent and identically distributed (i.i.d.).\n-   Return distribution model: $R_i = s(\\nu) T_i$, where $T_i$ is a random variable with a Student's $t$-distribution with $\\nu$ degrees of freedom and unit scale.\n-   Scaling factor: $s(\\nu) = \\sqrt{\\frac{\\nu - 2}{\\nu}}$, defined for $\\nu > 2$. This ensures $\\operatorname{Var}(R_i) = 1$.\n-   Portfolio return: $R_p = \\frac{1}{M}\\sum_{i=1}^{M} R_i$.\n-   Portfolio loss: $L = -R_p$.\n-   Value at Risk definition: $\\operatorname{VaR}_{\\alpha}(L) = \\inf\\{x \\in \\mathbb{R} : \\mathbb{P}(L \\le x) \\ge \\alpha \\}$.\n-   Expected Shortfall definition: $\\operatorname{ES}_{\\alpha}(L) = \\mathbb{E}[L \\mid L \\ge \\operatorname{VaR}_{\\alpha}(L)]$.\n-   Simulation parameters: Number of scenarios $N = 100000$; confidence level $\\alpha = 0.95$; random number generator seed fixed at $123456$.\n-   Test cases: Degrees of freedom $\\nu \\in \\{2.2, 3.0, 5.0, 30.0, 100.0\\}$.\n-   Empirical estimators:\n    -   $\\widehat{\\operatorname{VaR}}_{\\alpha}(L)$ is the order statistic corresponding to index $k = \\lceil \\alpha N \\rceil$.\n    -   $\\widehat{\\operatorname{ES}}_{\\alpha}(L)$ is the sample average of all simulated losses greater than or equal to the empirical $\\operatorname{VaR}$.\n-   Output format: A single line representing a Python list of lists: `[[v_1,e_1],[v_2,e_2],...,[v_5,e_5]]`.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientific and Factual Soundness**: The problem is scientifically sound. The use of the Student's $t$-distribution to model heavy-tailed financial returns is a standard technique in quantitative finance. The definitions of $\\operatorname{VaR}$ and $\\operatorname{ES}$ are textbook definitions. The scaling factor $s(\\nu)$ is correctly formulated to ensure unit variance for the asset returns for $\\nu > 2$, as $\\operatorname{Var}(T_i) = \\frac{\\nu}{\\nu-2}$. The test cases for $\\nu$, starting from $2.2$, all satisfy this condition.\n2.  **Well-Posedness**: The problem is well-posed. It provides all necessary parameters ($M, \\alpha, N$, seed, test values for $\\nu$), defines the model unambiguously, and specifies the exact estimators to be used. This ensures that a unique numerical result can be obtained.\n3.  **Objectivity**: The problem statement is expressed in precise, objective, and formal mathematical language, free from any subjective or ambiguous claims.\n4.  **Completeness and Consistency**: The problem is self-contained and internally consistent. All required definitions and data are provided. No contradictions are present.\n5.  **Feasibility**: The specified computation ($5$ test cases, each with $N=100000$ scenarios for $M=10$ assets) is computationally feasible with modern hardware and standard scientific libraries.\n\n**Verdict and Action**\nThe problem statement is **valid** as it is scientifically grounded, well-posed, objective, complete, and computationally feasible. We will now proceed with the derivation and implementation of the solution.\n\n**Solution Derivation**\n\nThe core of the problem is to perform a Monte Carlo simulation to estimate risk measures for a portfolio loss $L$. The distribution of $L$ is inherited from the properties of the asset returns $R_i$.\n\n**1. Properties of the Portfolio Loss**\nThe return of asset $i$, $R_i$, has a mean of $\\mathbb{E}[R_i] = 0$ for $\\nu > 1$ and a variance of $\\operatorname{Var}(R_i) = 1$ for $\\nu > 2$. The portfolio return is $R_p = \\frac{1}{M}\\sum_{i=1}^{M} R_i$. Due to the i.i.d. nature of the asset returns, the expectation and variance of the portfolio return are:\n$$\n\\mathbb{E}[R_p] = \\mathbb{E}\\left[\\frac{1}{M}\\sum_{i=1}^{M} R_i\\right] = \\frac{1}{M}\\sum_{i=1}^{M} \\mathbb{E}[R_i] = 0\n$$\n$$\n\\operatorname{Var}(R_p) = \\operatorname{Var}\\left(\\frac{1}{M}\\sum_{i=1}^{M} R_i\\right) = \\frac{1}{M^2}\\sum_{i=1}^{M} \\operatorname{Var}(R_i) = \\frac{M}{M^2} = \\frac{1}{M}\n$$\nThe portfolio loss is defined as $L = -R_p$. Consequently, its statistical properties are $\\mathbb{E}[L] = 0$ and $\\operatorname{Var}(L) = \\frac{1}{M}$. For this problem, $M=10$, so $\\operatorname{Var}(L) = 0.1$. The distribution of $L$, being a sum of scaled $t$-distributed variables, does not have a simple closed form, which necessitates the use of simulation.\n\n**2. Monte Carlo Simulation Algorithm**\nFor each given value of $\\nu \\in \\{2.2, 3.0, 5.0, 30.0, 100.0\\}$, the following procedure is executed:\n\na. **Random Variate Generation**: Generate an $N \\times M$ matrix, $\\mathbf{T}$, where $N = 100000$ and $M=10$. Each element $T_{ki}$ (for scenario $k$ and asset $i$) is an independent draw from the standard Student's $t$-distribution with $\\nu$ degrees of freedom. A fixed seed of $123456$ ensures reproducibility.\n\nb. **Return Calculation**: Compute the scaling factor $s(\\nu) = \\sqrt{(\\nu - 2)/\\nu}$. Then, construct the matrix of asset returns $\\mathbf{R}$ by element-wise multiplication: $\\mathbf{R} = s(\\nu) \\mathbf{T}$.\n\nc. **Portfolio Loss Calculation**: For each scenario $k=1, \\dots, N$, calculate the portfolio return $R_{p,k} = \\frac{1}{M}\\sum_{i=1}^{M} R_{ki}$. This is achieved by taking the mean of each row of $\\mathbf{R}$. The corresponding portfolio losses are given by the vector $\\mathbf{L}$ where $L_k = -R_{p,k}$.\n\n**3. Empirical Estimation of VaR and ES**\nUsing the generated vector $\\mathbf{L}$ of $N$ portfolio loss samples:\n\na. **VaR Estimation**: The vector $\\mathbf{L}$ is sorted in non-decreasing order to obtain the order statistics $L_{(1)} \\le L_{(2)} \\le \\dots \\le L_{(N)}$. The empirical $\\operatorname{VaR}$ at confidence level $\\alpha = 0.95$ is the $k$-th order statistic, where the index $k$ is given by:\n$$\nk = \\lceil \\alpha N \\rceil = \\lceil 0.95 \\times 100000 \\rceil = 95000\n$$\nIn a zero-indexed array, this corresponds to the element at index $k-1 = 94999$. Thus, $\\widehat{\\operatorname{VaR}}_{0.95} = L_{(95000)}$.\n\nb. **ES Estimation**: The empirical $\\operatorname{ES}$ is defined as the sample average of all simulated losses that are greater than or equal to the estimated $\\operatorname{VaR}$.\n$$\n\\widehat{\\operatorname{ES}}_{0.95} = \\frac{\\sum_{k=1}^{N} L_k \\cdot \\mathbf{1}_{\\{L_k \\ge \\widehat{\\operatorname{VaR}}_{0.95}\\}}}{\\sum_{k=1}^{N} \\mathbf{1}_{\\{L_k \\ge \\widehat{\\operatorname{VaR}}_{0.95}\\}}}\n$$\nwhere $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function. This is implemented by filtering the original loss vector $\\mathbf{L}$ for all values satisfying the condition and then computing their arithmetic mean.\n\nAs $\\nu \\to \\infty$, the Student's $t$-distribution converges to the standard normal distribution. Therefore, for large $\\nu$, we expect the results to approach the theoretical values for a Gaussian portfolio loss $L \\sim \\mathcal{N}(0, 1/M)$. With $M=10$ and $\\alpha=0.95$, these are $\\operatorname{VaR}_{0.95} = \\frac{1}{\\sqrt{10}} \\Phi^{-1}(0.95) \\approx 0.5202$ and $\\operatorname{ES}_{0.95} = \\frac{1}{\\sqrt{10}} \\frac{\\phi(\\Phi^{-1}(0.95))}{1-0.95} \\approx 0.6521$, where $\\Phi$ and $\\phi$ are the standard normal CDF and PDF, respectively. For smaller $\\nu$, the fatter tails of the $t$-distribution should lead to higher values for both $\\operatorname{VaR}$ and $\\operatorname{ES}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Estimates and compares Value at Risk (VaR) and Expected Shortfall (ES)\n    for an equally weighted portfolio with asset returns following a scaled\n    Student's t-distribution with varying degrees of freedom.\n    \"\"\"\n    \n    # Define the test cases and parameters from the problem statement.\n    M = 10  # Number of assets\n    alpha = 0.95  # Confidence level\n    N = 100000  # Number of scenarios\n    seed = 123456  # Random number generator seed\n    \n    test_cases = [\n        2.2,\n        3.0,\n        5.0,\n        30.0,\n        100.0,\n    ]\n\n    # Initialize a random number generator with the specified seed for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for nu in test_cases:\n        # Main logic to calculate the result for one case goes here.\n\n        # 1. Generate N x M matrix of random variates from Student's t-distribution.\n        # The random_state parameter is passed the generator instance to ensure it is used.\n        T_matrix = t.rvs(df=nu, size=(N, M), random_state=rng)\n\n        # 2. Calculate the scaling factor s(nu) to ensure Var(R_i) = 1.\n        # This is valid for nu > 2, which holds for all test cases.\n        s_nu = np.sqrt((nu - 2) / nu)\n        \n        # 3. Scale the random variates to get the asset returns matrix R.\n        R_matrix = s_nu * T_matrix\n\n        # 4. Compute the equally weighted portfolio return for each scenario.\n        # This is the mean of each row (axis=1).\n        R_p_vector = np.mean(R_matrix, axis=1)\n\n        # 5. Compute the portfolio loss L = -R_p.\n        L_vector = -R_p_vector\n\n        # 6. Estimate VaR at confidence level alpha.\n        # First, sort the losses in ascending order.\n        L_sorted = np.sort(L_vector)\n        \n        # The index for VaR is k = ceil(alpha*N).\n        # In 0-based Python indexing, this corresponds to index k-1.\n        k_index = int(np.ceil(alpha * N)) - 1\n        var_estimate = L_sorted[k_index]\n\n        # 7. Estimate ES at confidence level alpha.\n        # ES is the sample average of all losses greater than or equal to the empirical VaR.\n        # This is implemented by boolean indexing on the original loss vector.\n        es_estimate = np.mean(L_vector[L_vector >= var_estimate])\n        \n        # Append the [VaR, ES] pair for the current test case.\n        results.append([var_estimate, es_estimate])\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists, e.g., [[v1, e1], [v2, e2]].\n    # The str() function on a list provides the correct formatting for each inner list.\n    # The ','.join() method combines these strings with a comma separator.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}