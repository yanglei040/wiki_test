{"hands_on_practices": [{"introduction": "Theory finds its purpose in application. This first practice bridges the gap between macroeconomic theory and financial modeling by building an affine term structure model directly from an economic principle. We will model the short-term interest rate using a Taylor-type policy rule, where the weights on inflation and unemployment serve as the latent factors driving the yield curve [@problem_id:2370016]. This exercise provides hands-on experience in deriving the model's recursive pricing structure and using observed yields to infer the unobserved state of the economy as captured by the policy rule.", "id": "2370016", "problem": "You are asked to implement and test a calibration routine for a Gaussian Affine Term Structure Model (ATSM) in which the short rate is generated by a Taylor-type policy rule whose coefficients are the latent state variables. The model must be specified and solved under the risk-neutral measure. All interest rates and yields must be handled as decimals (for example, an annual yield of $2$ percent is written as $0.02$), and no percentage sign is allowed in the output. The goal is to recover, from synthetic yield curve observations at a single date, the latent Taylor-rule weights on inflation and unemployment that enter the short rate.\n\nFundamental base and model setup:\n- Work under the risk-neutral measure, with the absence of arbitrage implying that a zero-coupon bond price at time $t$ is the conditional expectation of the discounted payoff. Formally, for a zero-coupon bond maturing in $n$ years, the price is $P_t(n) \\equiv \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp\\left(-\\sum_{j=0}^{n-1} r_{t+j}\\right)\\right]$, where $\\mathbb{Q}$ denotes the risk-neutral measure.\n- The short rate is generated by a Taylor-type rule with time-varying coefficients treated as the state, with $r_t = \\delta_0 + \\delta_1^\\top x_t$, where $x_t \\in \\mathbb{R}^2$ collects the Taylor-rule weights on inflation and unemployment, respectively.\n- The state is Gaussian and follows a first-order linear process under the risk-neutral measure, $x_{t+1} = \\mu + \\Phi x_t + \\Sigma \\varepsilon_{t+1}$, where $\\varepsilon_{t+1} \\sim \\mathcal{N}(0, I)$, $\\mu \\in \\mathbb{R}^2$, $\\Phi \\in \\mathbb{R}^{2\\times 2}$ is stable, and $\\Sigma \\in \\mathbb{R}^{2\\times 2}$ is lower-triangular. The market price of risk is zero so that the physical and risk-neutral dynamics coincide.\n\nTasks to implement:\n1) Starting from the risk-neutral pricing identity and the Gaussian linear state dynamics, derive and implement the exponential-affine solution for zero-coupon bond prices. In particular, show that there exist sequences $\\{A_n\\}_{n\\ge 0}$ and $\\{B_n\\}_{n\\ge 0}$ such that $\\log P_t(n) = -A_n - B_n^\\top x_t$, and obtain their recursions by conditioning forward one step. Then define model-implied $n$-year zero-coupon yields as $y_t(n) \\equiv -\\frac{1}{n}\\log P_t(n)$.\n2) Given a cross-section of $N$ maturities at a single date $t$, show how to estimate the latent state $x_t$ via least squares by exploiting the linear relation $y_t(n)\\,n - A_n = B_n^\\top x_t$. Implement a numerically stable algorithm to compute the least-squares estimate $\\hat{x}_t$ from the overidentified system.\n3) For each parameter set in the test suite below, generate synthetic observed yields $y^{\\text{obs}}(n)$ by evaluating the affine yield formula at the true state $x_t^{\\star}$ and adding a deterministic measurement disturbance $\\epsilon_n$. Then estimate $\\hat{x}_t$ by least squares and compute the in-sample root mean squared error (RMSE) across maturities between model-implied and observed yields.\n\nCalibration objective and reporting:\n- For each test case, with maturities $\\mathcal{N} = \\{1,2,5,10\\}$ years, construct $y^{\\text{obs}}(n) = \\frac{A_n + B_n^\\top x_t^{\\star}}{n} + \\epsilon_n$, estimate $\\hat{x}_t$, and compute $\\text{RMSE} \\equiv \\sqrt{\\frac{1}{|\\mathcal{N}|}\\sum_{n\\in \\mathcal{N}} \\left(y^{\\text{obs}}(n) - \\frac{A_n + B_n^\\top \\hat{x}_t}{n}\\right)^2}$.\n- Your program must output, for each test case, the three-tuple comprising the two components of $\\hat{x}_t$ followed by the RMSE, all rounded to six decimal places. Aggregate the results for all test cases in the order they are listed below into a single flat list.\n- Final output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if there are three test cases, the output must look like \"[x1_case1,x2_case1,rmse_case1,x1_case2,x2_case2,rmse_case2,x1_case3,x2_case3,rmse_case3]\".\n\nTest suite (all numbers are decimals and must be used exactly as specified):\n- Common maturities: $\\mathcal{N} = \\{\\,1,\\,2,\\,5,\\,10\\,\\}$ (years).\n\n- Test Case 1:\n  - Short-rate loadings: $\\delta_0 = 0.005$, $\\delta_1 = [\\pi, u]^\\top$ with $\\pi = 0.02$, $u = 0.06$.\n  - Risk-neutral state dynamics: $\\mu = [0.3,\\, -0.1]^\\top$, $\\Phi = \\begin{bmatrix}0.8 & 0\\\\0 & 0.5\\end{bmatrix}$, $\\Sigma = \\operatorname{diag}(0.1,\\,0.1)$.\n  - True state at time $t$: $x_t^{\\star} = [1.0,\\,0.5]^\\top$.\n  - Measurement disturbances across maturities: $\\epsilon = [0.0000,\\,0.0000,\\,0.0000,\\,0.0000]^\\top$.\n\n- Test Case 2 (near-boundary persistence):\n  - Short-rate loadings: $\\delta_0 = 0.004$, $\\delta_1 = [\\pi, u]^\\top$ with $\\pi = 0.03$, $u = 0.05$.\n  - Risk-neutral state dynamics: $\\mu = [-0.05,\\,0.02]^\\top$, $\\Phi = \\begin{bmatrix}0.99 & 0\\\\0 & 0.95\\end{bmatrix}$, $\\Sigma = \\operatorname{diag}(0.05,\\,0.02)$.\n  - True state at time $t$: $x_t^{\\star} = [0.7,\\,-0.3]^\\top$.\n  - Measurement disturbances: $\\epsilon = [0.0001,\\,-0.0001,\\,0.0001,\\,-0.0001]^\\top$.\n\n- Test Case 3 (heterogeneous persistence and larger volatility):\n  - Short-rate loadings: $\\delta_0 = 0.002$, $\\delta_1 = [\\pi, u]^\\top$ with $\\pi = 0.01$, $u = 0.08$.\n  - Risk-neutral state dynamics: $\\mu = [-0.2,\\,0.4]^\\top$, $\\Phi = \\begin{bmatrix}0.3 & 0\\\\0 & 0.7\\end{bmatrix}$, $\\Sigma = \\operatorname{diag}(0.2,\\,0.1)$.\n  - True state at time $t$: $x_t^{\\star} = [1.2,\\,0.2]^\\top$.\n  - Measurement disturbances: $\\epsilon = [0.0002,\\,0.0000,\\,-0.0002,\\,0.0000]^\\top$.\n\nProgram requirements:\n- Implement the affine recursion for bond prices from first principles as described above.\n- Use numerically stable linear algebra to solve the least-squares problem for $\\hat{x}_t$.\n- For each test case, return the three numbers $(\\hat{x}_{t,1}, \\hat{x}_{t,2}, \\text{RMSE})$, all rounded to six decimal places.\n- Final output: A single line with a flat list of $9$ floats (three per test case) in the exact order of the test suite and with the exact formatting described above. No other output is permitted.", "solution": "The problem statement is a valid exercise in computational finance. It describes the calibration of a discrete-time Gaussian Affine Term Structure Model (ATSM) using synthetic yield curve data. The model is fully specified, the objectives are clear, and the problem is scientifically sound and well-posed. We proceed with the solution.\n\nThe solution is structured in three parts: first, the derivation of the recursive formulas for the affine coefficients; second, the formulation of the state vector estimation problem via ordinary least squares; and third, the specification of the algorithm to be implemented.\n\n**1. Derivation of Affine Recursions**\n\nThe model is defined under the risk-neutral measure $\\mathbb{Q}$. The price of a zero-coupon bond at time $t$ with $n$ periods to maturity, $P_t(n)$, is given by the risk-neutral expectation of the discounted payoff:\n$$\nP_t(n) = \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp\\left(-\\sum_{j=0}^{n-1} r_{t+j}\\right)\\right]\n$$\nwhere $r_t$ is the short rate. By the law of iterated expectations, this can be written recursively:\n$$\nP_t(n) = \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp(-r_t) \\cdot \\exp\\left(-\\sum_{j=1}^{n-1} r_{t+j}\\right)\\right] = \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp(-r_t) \\cdot P_{t+1}(n-1)\\right]\n$$\nThe model assumes an exponential-affine form for the bond price, which we write as:\n$$\nP_t(n) = \\exp(-A_n - B_n^\\top x_t)\n$$\nwhere $A_n$ is a scalar and $B_n \\in \\mathbb{R}^2$. The short rate $r_t$ and the state vector $x_t$ are specified as:\n$$\nr_t = \\delta_0 + \\delta_1^\\top x_t\n$$\n$$\nx_{t+1} = \\mu + \\Phi x_t + \\Sigma \\varepsilon_{t+1}, \\quad \\varepsilon_{t+1} \\sim \\mathcal{N}(0, I)\n$$\nSubstituting these into the recursive pricing equation:\n$$\n\\exp(-A_n - B_n^\\top x_t) = \\mathbb{E}_t^{\\mathbb{Q}}\\left[ \\exp\\left(-(\\delta_0 + \\delta_1^\\top x_t)\\right) \\exp\\left(-A_{n-1} - B_{n-1}^\\top x_{t+1}\\right) \\right]\n$$\nSince terms involving $t$-dated variables are known at time $t$, they can be taken out of the expectation:\n$$\n\\exp(-A_n - B_n^\\top x_t) = \\exp(-\\delta_0 - \\delta_1^\\top x_t - A_{n-1}) \\mathbb{E}_t^{\\mathbb{Q}}\\left[ \\exp\\left(-B_{n-1}^\\top x_{t+1}\\right) \\right]\n$$\nWe now evaluate the expectation by substituting the dynamics of $x_{t+1}$:\n$$\n\\mathbb{E}_t^{\\mathbb{Q}}\\left[ \\exp\\left(-B_{n-1}^\\top (\\mu + \\Phi x_t + \\Sigma \\varepsilon_{t+1})\\right) \\right] = \\exp\\left(-B_{n-1}^\\top \\mu - B_{n-1}^\\top \\Phi x_t\\right) \\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp\\left(-B_{n-1}^\\top \\Sigma \\varepsilon_{t+1}\\right)\\right]\n$$\nThe remaining expectation involves the random variable $Z = -B_{n-1}^\\top \\Sigma \\varepsilon_{t+1}$. Since $\\varepsilon_{t+1} \\sim \\mathcal{N}(0, I)$, $Z$ is a scalar normal variable with mean $\\mathbb{E}[Z]=0$ and variance $\\text{Var}(Z) = (-B_{n-1}^\\top \\Sigma) \\mathbb{E}[\\varepsilon_{t+1}\\varepsilon_{t+1}^\\top] (-B_{n-1}^\\top \\Sigma)^\\top = B_{n-1}^\\top \\Sigma \\Sigma^\\top B_{n-1}$. The moment-generating function of a normal variable $Y \\sim \\mathcal{N}(m, \\sigma^2)$ at point $s$ is $\\mathbb{E}[\\exp(sY)] = \\exp(sm + \\frac{1}{2}s^2\\sigma^2)$. For $Z$, evaluated at $s=1$, we have:\n$$\n\\mathbb{E}_t^{\\mathbb{Q}}\\left[\\exp(Z)\\right] = \\exp\\left(\\frac{1}{2}\\text{Var}(Z)\\right) = \\exp\\left(\\frac{1}{2} B_{n-1}^\\top \\Sigma \\Sigma^\\top B_{n-1}\\right)\n$$\nSubstituting this back and taking logarithms of both sides yields:\n$$\n-A_n - B_n^\\top x_t = (-\\delta_0 - A_{n-1} - B_{n-1}^\\top \\mu + \\frac{1}{2} B_{n-1}^\\top \\Sigma \\Sigma^\\top B_{n-1}) - (\\delta_1^\\top + B_{n-1}^\\top \\Phi) x_t\n$$\nThis identity must hold for any state $x_t$. By matching the intercept and the coefficient of $x_t$, we obtain the following recursions for $A_n$ and $B_n$:\n$$\nB_n = \\Phi^\\top B_{n-1} + \\delta_1\n$$\n$$\nA_n = A_{n-1} + \\delta_0 + B_{n-1}^\\top \\mu - \\frac{1}{2} B_{n-1}^\\top \\Sigma \\Sigma^\\top B_{n-1}\n$$\nThe recursions are initialized for a zero-maturity bond, $P_t(0)=1$, for which $\\log P_t(0)=0$. This implies $-A_0 - B_0^\\top x_t=0$ for all $x_t$, which requires the initial conditions:\n$$\nA_0 = 0, \\quad B_0 = \\mathbf{0} \\in \\mathbb{R}^2\n$$\n\n**2. State Estimation via Least Squares**\n\nThe model-implied yield for an $n$-period bond is defined as $y_t(n) = -\\frac{1}{n} \\log P_t(n)$. Using the affine structure:\n$$\ny_t(n) = \\frac{A_n + B_n^\\top x_t}{n}\n$$\nGiven a set of observed yields $\\{y^{\\text{obs}}(n_i)\\}_{i=1}^N$ for maturities $\\mathcal{N}=\\{n_1, \\dots, n_N\\}$, we aim to estimate the unobserved state vector $x_t$. Rearranging the yield equation provides a linear relationship:\n$$\nn \\cdot y_t(n) - A_n = B_n^\\top x_t\n$$\nObserved yields are assumed to be generated by the model at the true state $x_t^\\star$ plus a measurement disturbance $\\epsilon_n$:\n$$\ny^{\\text{obs}}(n) = \\frac{A_n + B_n^\\top x_t^\\star}{n} + \\epsilon_n\n$$\nSubstituting this into the linear relationship gives:\n$$\nn \\cdot y^{\\text{obs}}(n) - A_n = B_n^\\top x_t^\\star + n \\cdot \\epsilon_n\n$$\nFor a set of $N$ maturities, we can stack these equations to form an overdetermined linear system. Let $Z$ be an $N \\times 1$ vector with elements $Z_i = n_i \\cdot y^{\\text{obs}}(n_i) - A_{n_i}$, and let $\\mathbf{B}$ be an $N \\times 2$ matrix whose rows are $B_{n_i}^\\top$. The system is:\n$$\nZ = \\mathbf{B} x_t + \\nu\n$$\nwhere $\\nu$ is a vector of pricing errors. The least-squares estimate $\\hat{x}_t$ minimizes the sum of squared errors, $\\|Z - \\mathbf{B} x_t\\|^2$, and is given by the solution to the normal equations:\n$$\n\\hat{x}_t = (\\mathbf{B}^\\top \\mathbf{B})^{-1} \\mathbf{B}^\\top Z\n$$\nThis system should be solved using a numerically stable method, such as QR decomposition or Singular Value Decomposition (SVD), which is implemented in standard scientific computing libraries.\n\n**3. Algorithmic Implementation and Evaluation**\n\nFor each test case, the calibration procedure is as follows:\n1.  Initialize $A_0 = 0$ and $B_0 = [0, 0]^\\top$.\n2.  Iteratively compute and store the coefficients $\\{A_n, B_n\\}$ for $n=1, \\dots, 10$ using the derived recursions.\n3.  For the specified maturities $\\mathcal{N} = \\{1, 2, 5, 10\\}$, generate the synthetic \"observed\" yields $y^{\\text{obs}}(n)$ using the provided true state $x_t^{\\star}$ and measurement disturbances $\\epsilon_n$.\n4.  Construct the $4 \\times 1$ target vector $Z$ where the $i$-th element is $n_i \\cdot y^{\\text{obs}}(n_i) - A_{n_i}$.\n5.  Construct the $4 \\times 2$ regressor matrix $\\mathbf{B}$ where the $i$-th row is $B_{n_i}^\\top$.\n6.  Solve the linear least-squares problem $Z = \\mathbf{B} \\hat{x}_t$ to find the estimated state vector $\\hat{x}_t$.\n7.  Calculate the model-implied yields $y^{\\text{model}}(n) = (A_n + B_n^\\top \\hat{x}_t)/n$ for each $n \\in \\mathcal{N}$ using the estimated $\\hat{x}_t$.\n8.  Compute the in-sample Root Mean Squared Error (RMSE) between the observed and model-implied yields:\n    $$\n    \\text{RMSE} = \\sqrt{\\frac{1}{|\\mathcal{N}|}\\sum_{n\\in \\mathcal{N}} (y^{\\text{obs}}(n) - y^{\\text{model}}(n))^2}\n    $$\n9.  The final result for the test case is the tuple $(\\hat{x}_{t,1}, \\hat{x}_{t,2}, \\text{RMSE})$, with each value rounded to six decimal places.\n\nThis procedure is deterministic and directly implements the theoretical framework on the provided data.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the ATSM calibration problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"delta0\": 0.005,\n            \"delta1\": np.array([0.02, 0.06]),\n            \"mu\": np.array([0.3, -0.1]),\n            \"Phi\": np.array([[0.8, 0.0], [0.0, 0.5]]),\n            \"Sigma\": np.array([[0.1, 0.0], [0.0, 0.1]]),\n            \"x_true\": np.array([1.0, 0.5]),\n            \"epsilon\": np.array([0.0000, 0.0000, 0.0000, 0.0000]),\n        },\n        # Test Case 2\n        {\n            \"delta0\": 0.004,\n            \"delta1\": np.array([0.03, 0.05]),\n            \"mu\": np.array([-0.05, 0.02]),\n            \"Phi\": np.array([[0.99, 0.0], [0.0, 0.95]]),\n            \"Sigma\": np.array([[0.05, 0.0], [0.0, 0.02]]),\n            \"x_true\": np.array([0.7, -0.3]),\n            \"epsilon\": np.array([0.0001, -0.0001, 0.0001, -0.0001]),\n        },\n        # Test Case 3\n        {\n            \"delta0\": 0.002,\n            \"delta1\": np.array([0.01, 0.08]),\n            \"mu\": np.array([-0.2, 0.4]),\n            \"Phi\": np.array([[0.3, 0.0], [0.0, 0.7]]),\n            \"Sigma\": np.array([[0.2, 0.0], [0.0, 0.1]]),\n            \"x_true\": np.array([1.2, 0.2]),\n            \"epsilon\": np.array([0.0002, 0.0000, -0.0002, 0.0000]),\n        },\n    ]\n\n    maturities = np.array([1, 2, 5, 10])\n    max_maturity = np.max(maturities)\n\n    all_results = []\n\n    for case in test_cases:\n        delta0 = case[\"delta0\"]\n        delta1 = case[\"delta1\"]\n        mu = case[\"mu\"]\n        Phi = case[\"Phi\"]\n        Sigma = case[\"Sigma\"]\n        x_true = case[\"x_true\"]\n        epsilon = case[\"epsilon\"]\n\n        # 1. Compute affine coefficients A_n and B_n\n        A = np.zeros(max_maturity + 1)\n        B = np.zeros((max_maturity + 1, 2))\n        \n        A[0] = 0.0\n        B[0, :] = 0.0\n\n        Sigma_SigmaT = Sigma @ Sigma.T\n\n        for n in range(1, max_maturity + 1):\n            B[n, :] = Phi.T @ B[n-1, :] + delta1\n            A[n] = A[n-1] + delta0 + B[n-1, :].T @ mu - 0.5 * B[n-1, :].T @ Sigma_SigmaT @ B[n-1, :]\n\n        # 2. Generate synthetic observed yields\n        y_obs = np.zeros(len(maturities))\n        for i, n in enumerate(maturities):\n            y_obs[i] = (A[n] + B[n, :].T @ x_true) / n + epsilon[i]\n            \n        # 3. Set up and solve the least squares problem\n        num_maturities = len(maturities)\n        Z_vector = np.zeros(num_maturities)\n        B_matrix = np.zeros((num_maturities, 2))\n\n        for i, n in enumerate(maturities):\n            Z_vector[i] = n * y_obs[i] - A[n]\n            B_matrix[i, :] = B[n, :]\n            \n        # Solve for x_hat using numerically stable lstsq\n        x_hat, _, _, _ = np.linalg.lstsq(B_matrix, Z_vector, rcond=None)\n        \n        # 4. Compute model-implied yields and RMSE\n        y_model = np.zeros(num_maturities)\n        for i, n in enumerate(maturities):\n            y_model[i] = (A[n] + B[n, :].T @ x_hat) / n\n\n        rmse = np.sqrt(np.mean((y_obs - y_model)**2))\n\n        # 5. Append results rounded to 6 decimal places\n        all_results.append(round(x_hat[0], 6))\n        all_results.append(round(x_hat[1], 6))\n        all_results.append(round(rmse, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}, {"introduction": "Standard calibration often minimizes the squared difference between model and market prices, but what if we think of calibration as aligning probability distributions? This practice introduces a more general and powerful approach to parameter estimation using a concept from information theory: the Kullback-Leibler (KL) divergence. You will calibrate a one-factor Vasicek model by minimizing the KL-divergence between the model-implied cross-sectional distribution of bond prices and an empirically observed one [@problem_id:2370055]. This method provides a fresh perspective on what it means for a model to be 'close' to the data.", "id": "2370055", "problem": "Consider an Affine Term Structure Model (ATSM) of the one-factor Vasicek type for the risk-neutral short rate process, where the short rate $r_t$ evolves as $dr_t = \\kappa(\\theta - r_t)\\,dt + \\sigma\\,dW_t$ under the risk-neutral measure. The parameters are the mean-reversion speed $\\kappa$, the long-run mean $\\theta$, the volatility $\\sigma$, and the initial short rate $r_0$. For a maturity $T \\ge 0$, the model-implied zero-coupon bond price at time $0$ is given by\n$$\nP(0,T) = \\exp\\!\\big(A(T) - B(T)\\,r_0\\big),\n$$\nwhere\n$$\nB(T) = \\frac{1 - e^{-\\kappa T}}{\\kappa},\n\\qquad\nA(T) = \\left(\\theta - \\frac{\\sigma^2}{2\\kappa^2}\\right)\\big(B(T) - T\\big) - \\frac{\\sigma^2}{4\\kappa}\\,B(T)^2,\n$$\nwith the conventions that for $T=0$ one has $B(0)=0$ and $A(0)=0$. For a finite set of maturities $\\{T_i\\}_{i=1}^n$ with $T_i \\ge 0$, define the model-implied cross-sectional distribution over these maturities by normalizing the bond prices:\n$$\nq_i(\\kappa,\\theta,\\sigma,r_0) = \\frac{P(0,T_i)}{\\sum_{j=1}^n P(0,T_j)} \\quad \\text{for } i=1,\\dots,n.\n$$\nLet $\\{p_i\\}_{i=1}^n$ be an empirical cross-sectional distribution over the same maturities, with $p_i > 0$ and $\\sum_{i=1}^n p_i = 1$. Define the Kullback–Leibler (KL) divergence (using the natural logarithm) from the empirical distribution to the model-implied distribution by\n$$\nD_{\\mathrm{KL}}\\big(p \\,\\|\\, q(\\kappa,\\theta,\\sigma,r_0)\\big) \\;=\\; \\sum_{i=1}^n p_i \\,\\log\\!\\left(\\frac{p_i}{q_i(\\kappa,\\theta,\\sigma,r_0)}\\right).\n$$\nFormulate the calibration problem as the minimization of $D_{\\mathrm{KL}}$ over the parameter vector $(\\kappa,\\theta,\\sigma,r_0)$, subject to $\\kappa>0$ and $\\sigma>0$. Assume no-arbitrage and risk-neutral valuation hold. There are no physical units in this problem. Angles are not involved.\n\nTest Suite. For each of the following three cases, the maturities $\\{T_i\\}$ and empirical distributions $\\{p_i\\}$ are specified. In every case, use the maturities exactly as given (in years), and use the empirical probabilities exactly as provided (they already sum to $1$):\n\n- Case $1$:\n  - Maturities: $\\{0.25,\\,0.5,\\,1.0,\\,2.0,\\,5.0,\\,10.0\\}$.\n  - Empirical distribution: $\\{0.27,\\,0.25,\\,0.20,\\,0.15,\\,0.08,\\,0.05\\}$.\n\n- Case $2$ (includes a boundary maturity at zero):\n  - Maturities: $\\{0.0,\\,0.25,\\,0.5,\\,1.0,\\,4.0,\\,7.0\\}$.\n  - Empirical distribution: $\\{0.35,\\,0.25,\\,0.18,\\,0.12,\\,0.06,\\,0.04\\}$.\n\n- Case $3$ (includes very long maturities):\n  - Maturities: $\\{0.5,\\,1.0,\\,1.5,\\,2.0,\\,5.0,\\,15.0,\\,30.0\\}$.\n  - Empirical distribution: $\\{0.26,\\,0.21,\\,0.17,\\,0.14,\\,0.10,\\,0.07,\\,0.05\\}$.\n\nParameter Domain. Restrict the search to the economically plausible set\n$$\n\\kappa \\in [10^{-4},\\,5.0],\\quad \\theta \\in [-0.05,\\,0.15],\\quad \\sigma \\in [10^{-5},\\,0.2],\\quad r_0 \\in [-0.02,\\,0.2].\n$$\n\nRequired Output. For each case in the order given above, compute the minimized value of $D_{\\mathrm{KL}}$ over $(\\kappa,\\theta,\\sigma,r_0)$ subject to the stated domain. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each minimized value rounded to $10$ decimal places, for example, $\\big[\\text{result}_1,\\text{result}_2,\\text{result}_3\\big]$.", "solution": "The problem presented is a well-defined task in computational finance. It requires the calibration of a one-factor Vasicek term structure model to empirically observed data. The calibration is formulated as a numerical optimization problem, specifically, the minimization of the Kullback-Leibler (KL) divergence between a model-implied probability distribution and a given empirical distribution. This is a rigorous and standard approach. We will proceed with a principled solution.\n\nThe core of the problem is to find the parameter vector $\\mathbf{x} = (\\kappa, \\theta, \\sigma, r_0)$ that minimizes the objective function $D_{\\mathrm{KL}}(p \\,\\|\\, q(\\mathbf{x}))$. The vector $\\mathbf{x}$ consists of the Vasicek model parameters: $\\kappa$ (mean-reversion speed), $\\theta$ (long-run mean), $\\sigma$ (volatility), and the initial short rate $r_0$. The optimization is performed over a specified, bounded domain for these parameters.\n\nThe objective function, the KL divergence, is given by:\n$$\nD_{\\mathrm{KL}}\\big(p \\,\\|\\, q(\\mathbf{x})\\big) = \\sum_{i=1}^n p_i \\log\\left(\\frac{p_i}{q_i(\\mathbf{x})}\\right)\n$$\nwhere $\\{p_i\\}_{i=1}^n$ is the fixed empirical distribution and $\\{q_i(\\mathbf{x})\\}_{i=1}^n$ is the model-implied distribution, which depends on the parameters $\\mathbf{x}$. The model-implied probabilities are derived from the zero-coupon bond prices $P(0, T_i)$ for a set of maturities $\\{T_i\\}_{i=1}^n$:\n$$\nq_i(\\mathbf{x}) = \\frac{P(0,T_i; \\mathbf{x})}{\\sum_{j=1}^n P(0,T_j; \\mathbf{x})}\n$$\nIn the Vasicek model, under the risk-neutral measure, the bond price is an exponential-affine function of the short rate:\n$$\nP(0,T) = \\exp\\big(A(T) - B(T)r_0\\big)\n$$\nThe functions $A(T)$ and $B(T)$ are themselves functions of the model parameters. Specifically, for a maturity $T > 0$:\n$$\nB(T; \\kappa) = \\frac{1 - e^{-\\kappa T}}{\\kappa}\n$$\n$$\nA(T; \\kappa, \\theta, \\sigma) = \\left(\\theta - \\frac{\\sigma^2}{2\\kappa^2}\\right)\\big(B(T) - T\\big) - \\frac{\\sigma^2}{4\\kappa}\\,B(T)^2\n$$\nFor the boundary case $T=0$, the conventions $A(0)=0$ and $B(0)=0$ are used, which correctly implies $P(0,0)=1$.\n\nThe problem is to solve the constrained non-linear optimization problem:\n$$\n\\min_{\\mathbf{x} \\in \\mathcal{D}} D_{\\mathrm{KL}}(p \\,\\|\\, q(\\mathbf{x}))\n$$\nThe search domain $\\mathcal{D}$ is the hyper-rectangle defined by:\n$\\kappa \\in [10^{-4}, 5.0]$, $\\theta \\in [-0.05, 0.15]$, $\\sigma \\in [10^{-5}, 0.2]$, and $r_0 \\in [-0.02, 0.2]$.\n\nThe objective function is non-linear and its dependencies on the parameters are complex, potentially leading to a landscape with multiple local minima. A simple gradient-based local optimizer might fail to find the global minimum. Therefore, a global optimization algorithm is the appropriate choice. We will use the `differential_evolution` algorithm provided by the `scipy.optimize` library. This is a population-based stochastic optimization method that is robust for finding the global minimum of functions over box-constrained domains.\n\nFor numerical stability, especially when dealing with exponentials, it is prudent to work with logarithms. The objective function can be rewritten as:\n$$\nD_{\\mathrm{KL}} = \\sum_{i=1}^n p_i (\\log p_i - \\log q_i)\n$$\nThe term $\\log q_i$ is computed as $\\log q_i = \\log P(0,T_i) - \\log(\\sum_j P(0,T_j))$. The sum of exponentials in the denominator is numerically unstable if computed directly. We can use the log-sum-exp stabilization trick:\n$$\n\\log\\left(\\sum_{j=1}^n P(0,T_j)\\right) = \\log\\left(\\sum_{j=1}^n \\exp\\big(\\log P(0,T_j)\\big)\\right)\n$$\nwhere $\\log P(0,T_j) = A(T_j) - B(T_j)r_0$. The `scipy.special.logsumexp` function implements this computation robustly.\n\nThe implementation will consist of a primary function that defines the objective $D_{\\mathrm{KL}}$ as a function of the parameter vector $\\mathbf{x}$. This function will be passed to the `differential_evolution` solver along with the specified bounds for the parameters. This process will be repeated for each of the three test cases, and the minimized KL divergence value will be recorded. A fixed seed for the random number generator is used to ensure the reproducibility of the optimization results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import differential_evolution\nfrom scipy.special import logsumexp\n\ndef vasicek_bond_price_components(params, T_values):\n    \"\"\"\n    Calculates the A(T) and B(T) components for the Vasicek model bond price.\n    \n    Args:\n        params (list or np.ndarray): A list of parameters [kappa, theta, sigma, r0].\n            r0 is not used in this function but is part of the standard parameter vector.\n        T_values (np.ndarray): An array of maturities T.\n    \n    Returns:\n        (np.ndarray, np.ndarray): A tuple containing the A(T) and B(T) arrays.\n    \"\"\"\n    kappa, theta, sigma, _ = params\n    T_values = np.asarray(T_values, dtype=float)\n    \n    A_T = np.zeros_like(T_values)\n    B_T = np.zeros_like(T_values)\n    \n    # Isolate non-zero maturities to avoid division by zero in formulas.\n    # The convention for T=0 is A(0)=0, B(0)=0, which is handled by the initialization.\n    non_zero_T_mask = T_values > 1e-9\n\n    if np.any(non_zero_T_mask):\n        T = T_values[non_zero_T_mask]\n        \n        # The parameter domain for kappa is [1e-4, 5.0], so kappa is never zero.\n        # Direct computation of B(T) is safe.\n        exp_minus_kappa_T = np.exp(-kappa * T)\n        B_T_vals = (1.0 - exp_minus_kappa_T) / kappa\n        \n        # Calculate A(T) using the formula from the problem statement.\n        term_B_minus_T = B_T_vals - T\n        kappa_sq = kappa * kappa\n        sigma_sq = sigma * sigma\n        \n        A_T_vals = (theta - sigma_sq / (2.0 * kappa_sq)) * term_B_minus_T - \\\n                   (sigma_sq / (4.0 * kappa)) * (B_T_vals**2)\n        \n        # Assign calculated values to the corresponding positions in the arrays.\n        B_T[non_zero_T_mask] = B_T_vals\n        A_T[non_zero_T_mask] = A_T_vals\n        \n    return A_T, B_T\n\ndef kl_divergence_objective(params, T_values, p_dist):\n    \"\"\"\n    Objective function calculating the KL divergence for Vasicek model calibration.\n    \n    Args:\n        params (list or np.ndarray): A list of parameters [kappa, theta, sigma, r0].\n        T_values (np.ndarray): An array of maturities T.\n        p_dist (np.ndarray): The empirical probability distribution.\n        \n    Returns:\n        float: The KL divergence D_KL(p || q).\n    \"\"\"\n    kappa, theta, sigma, r0 = params\n    \n    # Calculate A(T) and B(T)\n    A_T, B_T = vasicek_bond_price_components(params, T_values)\n    \n    # Calculate the logarithm of bond prices\n    log_P_0_T = A_T - B_T * r0\n\n    # Handle potential numerical overflows from intermediate calculations\n    if np.any(np.isnan(log_P_0_T)) or np.any(np.isinf(log_P_0_T)):\n        return np.inf\n\n    # Calculate log of model-implied probabilities q_i using log-sum-exp for stability\n    log_S = logsumexp(log_P_0_T)\n    log_q_dist = log_P_0_T - log_S\n    \n    # Calculate KL divergence: D_KL(p || q) = sum(p_i * (log(p_i) - log(q_i)))\n    # The problem specifies p_i > 0, so log(p_dist) is safe.\n    kl_div = np.sum(p_dist * (np.log(p_dist) - log_q_dist))\n    \n    return kl_div\n\ndef solve():\n    \"\"\"\n    Main function to solve the calibration problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0.25, 0.5, 1.0, 2.0, 5.0, 10.0]),\n            np.array([0.27, 0.25, 0.20, 0.15, 0.08, 0.05])\n        ),\n        (\n            np.array([0.0, 0.25, 0.5, 1.0, 4.0, 7.0]),\n            np.array([0.35, 0.25, 0.18, 0.12, 0.06, 0.04])\n        ),\n        (\n            np.array([0.5, 1.0, 1.5, 2.0, 5.0, 15.0, 30.0]),\n            np.array([0.26, 0.21, 0.17, 0.14, 0.10, 0.07, 0.05])\n        )\n    ]\n\n    # Define the parameter search domain (bounds for the optimizer)\n    bounds = [\n        (1e-4, 5.0),    # kappa\n        (-0.05, 0.15),  # theta\n        (1e-5, 0.2),    # sigma\n        (-0.02, 0.2)    # r0\n    ]\n\n    results = []\n    for maturities, emp_dist in test_cases:\n        # Define the objective function for the current case\n        objective_func = lambda p: kl_divergence_objective(p, maturities, emp_dist)\n        \n        # Perform global optimization using differential evolution to find the minimum KL divergence\n        # A seed is used for reproducibility of the stochastic optimization process.\n        result = differential_evolution(objective_func, bounds, seed=42)\n        \n        # The minimized value of the objective function\n        min_kl_divergence = result.fun\n        results.append(min_kl_divergence)\n\n    # Format the results to 10 decimal places as required.\n    results_str = [f\"{r:.10f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"}, {"introduction": "After performing a calibration, how can we diagnose the result? This final, advanced practice turns the calibration problem on its head to build a deeper intuition for the mechanics of optimization. Instead of finding the parameters that minimize a given objective function, you will start with a known 'optimal' parameter set and reverse-engineer the implicit weights of the weighted least-squares objective that could have produced it [@problem_id:2370072]. This powerful technique relies on the first-order condition that the gradient must be zero at a minimum, providing a unique way to deconstruct and interpret calibration outcomes.", "id": "2370072", "problem": "Consider an exponential-affine term structure model for zero-coupon bond prices. For a maturity $T \\ge 0$, define the instantaneous forward rate by\n$$\nf(0,T) \\equiv \\phi_0 + \\phi_1 e^{-\\lambda_1 T} + \\phi_2 e^{-\\lambda_2 T},\n$$\nwhere $\\phi_0$, $\\phi_1$, and $\\phi_2$ are parameters, and $\\lambda_1$ and $\\lambda_2$ are strictly positive constants. The corresponding model-implied zero-coupon bond price for maturity $T$ is\n$$\nP_{\\text{model}}(T;\\boldsymbol{\\phi}) \\equiv \\exp\\!\\Bigg(-\\int_0^T f(0,u)\\,du\\Bigg) \n= \\exp\\!\\Bigg(-\\phi_0 T - \\frac{\\phi_1}{\\lambda_1}\\big(1-e^{-\\lambda_1 T}\\big) - \\frac{\\phi_2}{\\lambda_2}\\big(1-e^{-\\lambda_2 T}\\big)\\Bigg),\n$$\nwith parameter vector $\\boldsymbol{\\phi} \\equiv (\\phi_0,\\phi_1,\\phi_2)^\\top$.\n\nSuppose a weighted least squares objective is used in calibration against observed zero-coupon bond prices $\\{P_{\\text{obs}}(T_i)\\}_{i=1}^n$ at maturities $\\{T_i\\}_{i=1}^n$:\n$$\nS(\\boldsymbol{\\phi}) \\equiv \\sum_{i=1}^n w_i \\Big(P_{\\text{model}}(T_i;\\boldsymbol{\\phi}) - P_{\\text{obs}}(T_i)\\Big)^2,\n$$\nwhere the weights satisfy $w_i \\ge 0$ and $\\sum_{i=1}^n w_i = 1$. Assume there are no constraints on $\\boldsymbol{\\phi}$ in the optimization.\n\nYou are given a parameter vector $\\boldsymbol{\\phi}^\\star$ that is known to be a first-order stationary point (unconstrained) of $S(\\boldsymbol{\\phi})$ for some unknown nonnegative weights $\\{w_i\\}_{i=1}^n$ summing to $1$, along with the maturities and observed prices. Your task is to reverse-engineer one such weight vector $\\boldsymbol{w}$ that makes $\\boldsymbol{\\phi}^\\star$ satisfy the first-order stationarity conditions.\n\nUse the model definition above and fundamental calculus to derive the necessary conditions for first-order stationarity, and then compute a weight vector $\\boldsymbol{w}$ that satisfies those conditions and the normalization $\\sum_{i=1}^n w_i = 1$. If multiple solutions exist, select the solution that exactly satisfies the first-order conditions and normalization in the sense of solving the corresponding linear system; when this square system is singular, return the minimum Euclidean norm solution that satisfies the conditions in the least-squares sense, and then renormalize it to sum to $1$. All outputs must be expressed as decimals (no percentage signs).\n\nTest Suite. For each test case below, compute a corresponding weight vector $\\boldsymbol{w} \\in \\mathbb{R}^n$:\n- Case $1$ (general case):\n  - $\\lambda_1 = 0.7$, $\\lambda_2 = 0.15$.\n  - Maturities (in years): $T = [0.5, 1.0, 2.5, 5.0]$.\n  - Given stationary-point parameters: $\\boldsymbol{\\phi}^\\star = (0.018, -0.012, 0.020)$.\n  - Observed prices generated from $\\boldsymbol{\\phi}^{\\text{true}} = (0.019, -0.011, 0.0185)$ by the same model: $P_{\\text{obs}}(T_i) = P_{\\text{model}}(T_i; \\boldsymbol{\\phi}^{\\text{true}})$ for each $T_i$.\n\n- Case $2$ (boundary price included at $T=0$):\n  - $\\lambda_1 = 1.0$, $\\lambda_2 = 0.3$.\n  - Maturities (in years): $T = [0.0, 0.25, 1.0, 3.0]$.\n  - Given stationary-point parameters: $\\boldsymbol{\\phi}^\\star = (0.015, 0.002, 0.010)$.\n  - Observed prices generated from $\\boldsymbol{\\phi}^{\\text{true}} = (0.0155, 0.0015, 0.0105)$ as above.\n\n- Case $3$ (near-collinearity in loadings):\n  - $\\lambda_1 = 5.0$, $\\lambda_2 = 4.0$.\n  - Maturities (in years): $T = [0.5, 0.9, 1.1, 1.5]$.\n  - Given stationary-point parameters: $\\boldsymbol{\\phi}^\\star = (0.020, 0.005, -0.004)$.\n  - Observed prices generated from $\\boldsymbol{\\phi}^{\\text{true}} = (0.0195, 0.0040, -0.0035)$ as above.\n\nFinal Output Format. Your program should produce a single line of output containing a list of the three weight vectors, one per test case, each weight vector presented as a list of $n$ decimals, all enclosed in a single outer list. For example:\n$[$$[w_{1,1},w_{1,2},w_{1,3},w_{1,4}]$$,$$[w_{2,1},w_{2,2},w_{2,3},w_{2,4}]$$,$$[w_{3,1},w_{3,2},w_{3,3},w_{3,4}]$$]$.", "solution": "The problem requires us to reverse-engineer a set of weights $\\boldsymbol{w} = (w_1, \\dots, w_n)^\\top$ used in a weighted least squares calibration, given that a specific parameter vector $\\boldsymbol{\\phi}^\\star$ is a first-order stationary point of the objective function.\n\nThe objective function to be minimized is given by:\n$$\nS(\\boldsymbol{\\phi}) \\equiv \\sum_{i=1}^n w_i \\Big(P_{\\text{model}}(T_i;\\boldsymbol{\\phi}) - P_{\\text{obs}}(T_i)\\Big)^2\n$$\nwhere $\\boldsymbol{\\phi} = (\\phi_0, \\phi_1, \\phi_2)^\\top$ is the vector of parameters. The weights satisfy $w_i \\ge 0$ and $\\sum_{i=1}^n w_i = 1$. The problem states that the optimization is unconstrained.\n\nA point $\\boldsymbol{\\phi}^\\star$ is a first-order stationary point if the gradient of $S(\\boldsymbol{\\phi})$ with respect to $\\boldsymbol{\\phi}$ vanishes at that point:\n$$\n\\nabla_{\\boldsymbol{\\phi}} S(\\boldsymbol{\\phi}) \\Big|_{\\boldsymbol{\\phi}=\\boldsymbol{\\phi}^\\star} = \\boldsymbol{0}\n$$\nThis vector equation corresponds to three scalar equations, one for each component of $\\boldsymbol{\\phi}$:\n$$\n\\frac{\\partial S}{\\partial \\phi_j} \\Bigg|_{\\boldsymbol{\\phi}=\\boldsymbol{\\phi}^\\star} = 0 \\quad \\text{for } j=0, 1, 2.\n$$\nUsing the chain rule, we compute the partial derivative of $S(\\boldsymbol{\\phi})$:\n$$\n\\frac{\\partial S}{\\partial \\phi_j} = \\sum_{i=1}^n w_i \\cdot 2 \\Big(P_{\\text{model}}(T_i;\\boldsymbol{\\phi}) - P_{\\text{obs}}(T_i)\\Big) \\cdot \\frac{\\partial P_{\\text{model}}(T_i;\\boldsymbol{\\phi})}{\\partial \\phi_j}\n$$\nTo proceed, we must compute the derivative of the model bond price, $P_{\\text{model}}(T;\\boldsymbol{\\phi})$, with respect to each parameter $\\phi_j$. The bond price is given by:\n$$\nP_{\\text{model}}(T;\\boldsymbol{\\phi}) = \\exp\\!\\Bigg(-\\phi_0 T - \\frac{\\phi_1}{\\lambda_1}\\big(1-e^{-\\lambda_1 T}\\big) - \\frac{\\phi_2}{\\lambda_2}\\big(1-e^{-\\lambda_2 T}\\big)\\Bigg) = \\exp(A(T;\\boldsymbol{\\phi}))\n$$\nThe derivative is:\n$$\n\\frac{\\partial P_{\\text{model}}(T;\\boldsymbol{\\phi})}{\\partial \\phi_j} = P_{\\text{model}}(T;\\boldsymbol{\\phi}) \\cdot \\frac{\\partial A(T;\\boldsymbol{\\phi})}{\\partial \\phi_j}\n$$\nThe partial derivatives of the exponent term $A(T;\\boldsymbol{\\phi})$ are the factor loadings:\n$$\nL_{i,0} = \\frac{\\partial A(T_i;\\boldsymbol{\\phi})}{\\partial \\phi_0} = -T_i\n$$\n$$\nL_{i,1} = \\frac{\\partial A(T_i;\\boldsymbol{\\phi})}{\\partial \\phi_1} = -\\frac{1}{\\lambda_1}\\big(1-e^{-\\lambda_1 T_i}\\big)\n$$\n$$\nL_{i,2} = \\frac{\\partial A(T_i;\\boldsymbol{\\phi})}{\\partial \\phi_2} = -\\frac{1}{\\lambda_2}\\big(1-e^{-\\lambda_2 T_i}\\big)\n$$\nLet us define the pricing error at the stationary point $\\boldsymbol{\\phi}^\\star$ for maturity $T_i$ as $e_i = P_{\\text{model}}(T_i;\\boldsymbol{\\phi}^\\star) - P_{\\text{obs}}(T_i)$.\nThe first-order conditions, evaluated at $\\boldsymbol{\\phi} = \\boldsymbol{\\phi}^\\star$, become a system of linear equations in the unknown weights $w_i$:\n$$\n\\sum_{i=1}^n w_i \\cdot 2 \\cdot e_i \\cdot P_{\\text{model}}(T_i;\\boldsymbol{\\phi}^\\star) \\cdot L_{i,j} = 0 \\quad \\text{for } j=0, 1, 2.\n$$\nThe constant factor of $2$ can be dropped. Let us define the coefficients $c_{ij}$ for $i=1, \\dots, n$ and $j=0, 1, 2$ as:\n$$\nc_{ij} = e_i \\cdot P_{\\text{model}}(T_i;\\boldsymbol{\\phi}^\\star) \\cdot L_{i,j}\n$$\nThe first-order conditions form a homogeneous system of $3$ linear equations for the $n$ unknown weights $w_i$:\n$$\n\\begin{cases}\n\\sum_{i=1}^n c_{i0} w_i = 0 \\\\\n\\sum_{i=1}^n c_{i1} w_i = 0 \\\\\n\\sum_{i=1}^n c_{i2} w_i = 0\n\\end{cases}\n$$\nIn addition to these three equations, the weights must satisfy the normalization constraint:\n$$\n\\sum_{i=1}^n w_i = 1\n$$\nWe now have a system of $4$ linear equations for $n$ unknowns. For the test cases provided, $n=4$, so we have a square system of $4$ equations in $4$ unknowns. We can write this system in matrix form as $\\boldsymbol{A}\\boldsymbol{w} = \\boldsymbol{b}$:\n$$\n\\begin{pmatrix}\nc_{10} & c_{20} & \\dots & c_{n0} \\\\\nc_{11} & c_{21} & \\dots & c_{n1} \\\\\nc_{12} & c_{22} & \\dots & c_{n2} \\\\\n1 & 1 & \\dots & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nw_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 1\n\\end{pmatrix}\n$$\nThe solution vector $\\boldsymbol{w}$ can be found by solving this linear system. The algorithmic procedure is as follows:\n1.  For a given test case, compute the observed prices $P_{\\text{obs}}(T_i)$ using the provided true parameters $\\boldsymbol{\\phi}^{\\text{true}}$.\n2.  Compute the model prices $P_{\\text{model}}(T_i;\\boldsymbol{\\phi}^\\star)$ using the given stationary parameters $\\boldsymbol{\\phi}^\\star$.\n3.  Calculate the pricing errors $e_i = P_{\\text{model}}(T_i;\\boldsymbol{\\phi}^\\star) - P_{\\text{obs}}(T_i)$ for each maturity $T_i$.\n4.  Compute the factor loadings $L_{ij}$ for each maturity $T_i$ and each parameter index $j$.\n5.  Construct the coefficient matrix $\\boldsymbol{C}$ of size $n \\times 3$ with entries $C_{ij} = c_{ij} = e_i \\cdot P_{\\text{model}}(T_i;\\boldsymbol{\\phi}^\\star) \\cdot L_{ij}$.\n6.  Assemble the $4 \\times 4$ matrix $\\boldsymbol{A}$ and the $4 \\times 1$ vector $\\boldsymbol{b}$. The first three rows of $\\boldsymbol{A}$ are the transpose of $\\boldsymbol{C}$, and the fourth row is a vector of ones. $\\boldsymbol{b}=(0,0,0,1)^\\top$.\n7.  Solve the system $\\boldsymbol{A}\\boldsymbol{w} = \\boldsymbol{b}$.\n    - If $\\boldsymbol{A}$ is non-singular, a unique solution exists and can be found using a standard linear solver.\n    - If $\\boldsymbol{A}$ is singular, as indicated by the problem statement, we must find the minimum Euclidean norm solution to the least-squares problem $\\min \\|\\boldsymbol{A}\\boldsymbol{w}-\\boldsymbol{b}\\|_2$. Let this solution be $\\boldsymbol{w}_{\\text{ls}}$. We then renormalize it to ensure the sum-to-one constraint holds: $\\boldsymbol{w} = \\boldsymbol{w}_{\\text{ls}} / \\sum_i (\\boldsymbol{w}_{\\text{ls}})_i$. This accommodates cases where the system is inconsistent due to collinearity between the first-order conditions and the normalization constraint.\n\nThis procedure will be implemented for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the weight vector w in a weighted least squares calibration\n    of an affine term structure model, given a stationary point.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"l1\": 0.7, \"l2\": 0.15,\n            \"T\": np.array([0.5, 1.0, 2.5, 5.0]),\n            \"phi_star\": np.array([0.018, -0.012, 0.020]),\n            \"phi_true\": np.array([0.019, -0.011, 0.0185]),\n        },\n        {\n            \"l1\": 1.0, \"l2\": 0.3,\n            \"T\": np.array([0.0, 0.25, 1.0, 3.0]),\n            \"phi_star\": np.array([0.015, 0.002, 0.010]),\n            \"phi_true\": np.array([0.0155, 0.0015, 0.0105]),\n        },\n        {\n            \"l1\": 5.0, \"l2\": 4.0,\n            \"T\": np.array([0.5, 0.9, 1.1, 1.5]),\n            \"phi_star\": np.array([0.020, 0.005, -0.004]),\n            \"phi_true\": np.array([0.0195, 0.0040, -0.0035]),\n        }\n    ]\n\n    results = []\n\n    def p_model(T, phi, l1, l2):\n        \"\"\"Calculates the model zero-coupon bond price.\"\"\"\n        phi0, phi1, phi2 = phi\n        \n        # Use np.divide to handle T=0 or l=0 cases gracefully, though problem states l > 0\n        term1 = phi0 * T\n        term2 = np.divide(phi1, l1, out=np.zeros_like(T, dtype=float), where=l1!=0) * (1 - np.exp(-l1 * T))\n        term3 = np.divide(phi2, l2, out=np.zeros_like(T, dtype=float), where=l2!=0) * (1 - np.exp(-l2 * T))\n\n        return np.exp(-term1 - term2 - term3)\n\n    for case in test_cases:\n        l1, l2 = case[\"l1\"], case[\"l2\"]\n        T_arr = case[\"T\"]\n        phi_star = case[\"phi_star\"]\n        phi_true = case[\"phi_true\"]\n        n = len(T_arr)\n        k = len(phi_star)\n\n        # 1. Calculate observed and model prices\n        p_obs = p_model(T_arr, phi_true, l1, l2)\n        p_model_at_star = p_model(T_arr, phi_star, l1, l2)\n\n        # 2. Calculate pricing errors\n        errors = p_model_at_star - p_obs\n\n        # 3. Calculate factor loadings L (n x k matrix)\n        L = np.zeros((n, k))\n        L[:, 0] = -T_arr\n        L[:, 1] = -1/l1 * (1 - np.exp(-l1 * T_arr))\n        L[:, 2] = -1/l2 * (1 - np.exp(-l2 * T_arr))\n\n        # 4. Calculate coefficients c_ij (n x k matrix)\n        C = np.zeros((n, k))\n        for j in range(k):\n            C[:, j] = errors * p_model_at_star * L[:, j]\n\n        # 5. Assemble the matrix A and vector b for the system Aw = b\n        A = np.zeros((k+1, n))\n        A[:k, :] = C.T\n        A[k, :] = 1.0\n        \n        b = np.zeros(k+1)\n        b[k] = 1.0\n\n        # 6. Solve the linear system\n        # As per problem, if square system is singular, use lstsq and renormalize.\n        # We handle this by attempting a direct solve, and falling back to lstsq.\n        try:\n            # np.linalg.cond(A) could be used to check for ill-conditioning,\n            # but catching the error is more direct for singularity.\n            if np.linalg.det(A) == 0:\n                 raise np.linalg.LinAlgError(\"Singular matrix\")\n            w = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Handle singular case as per instruction\n            w_ls = np.linalg.lstsq(A, b, rcond=None)[0]\n            # Renormalize to ensure sum-to-one constraint is met\n            w = w_ls / np.sum(w_ls)\n\n        results.append(list(w))\n\n    # Format the final output string\n    # e.g., [[w11,w12,...],[w21,w22,...]]\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"}]}