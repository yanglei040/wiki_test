{"hands_on_practices": [{"introduction": "This first exercise establishes the foundational mechanics of Historical Simulation Value at Risk (VaR). You will begin by synthetically generating a realistic dataset for a portfolio of correlated assets, a core skill in financial modeling. You will then apply the historical simulation algorithm to this data to calculate the $99\\%$ VaR for various portfolio weighting schemes, providing a solid, from-the-ground-up understanding of the entire workflow ([@problem_id:2390037]).", "id": "2390037", "problem": "You are given a portfolio of $N=10$ assets. Let the portfolio weight vector be $\\mathbf{w} \\in \\mathbb{R}^{10}$, and let the historical returns over $T$ days be the matrix $\\mathbf{R} \\in \\mathbb{R}^{T \\times 10}$ whose $t$-th row is $\\mathbf{r}_t^\\top$. The daily portfolio return is $R^p_t = \\sum_{i=1}^{10} w_i r_{t,i}$ and the daily portfolio loss is $L_t = - R^p_t$. The $99\\%$ Value at Risk (Value at Risk (VaR)) is defined for a loss random variable $L$ as the smallest number $v$ such that $\\mathbb{P}(L \\le v) \\ge 0.99$. Under historical simulation, replace the distribution of $L$ with its empirical distribution from the sample $\\{L_t\\}_{t=1}^T$. Using the empirical distribution function, define the empirical $p$-quantile as the order statistic at index $k = \\lceil p \\cdot T \\rceil$ (with $1$-based indexing), that is, sort $\\{L_t\\}$ in nondecreasing order to obtain $L_{(1)} \\le \\cdots \\le L_{(T)}$, and set $\\widehat{q}_p = L_{(k)}$ with $k = \\lceil p \\cdot T \\rceil$. Then the historical simulation estimate of the $99\\%$ VaR is $\\widehat{\\mathrm{VaR}}_{0.99} = \\widehat{q}_{0.99}$. All outputs must be reported as decimals (for example, report $0.025$ instead of $2.5\\%$).\n\nTo ensure a scientifically sound and reproducible dataset that captures cross-asset correlation, construct the historical returns as follows. Let the per-asset daily volatility vector be $\\boldsymbol{\\sigma} = (\\sigma_1,\\ldots,\\sigma_{10})$ with $\\sigma_i = 0.01 + 0.002 \\cdot (i-1)$ for $i \\in \\{1,\\ldots,10\\}$. Define the correlation matrix $\\mathbf{C} \\in \\mathbb{R}^{10 \\times 10}$ by $C_{ij} = \\rho^{|i-j|}$ with $\\rho = 0.6$, and define the covariance matrix $\\boldsymbol{\\Sigma} = \\mathbf{D} \\mathbf{C} \\mathbf{D}$, where $\\mathbf{D} = \\mathrm{diag}(\\boldsymbol{\\sigma})$. For a given $T$, generate a historical sample by drawing $\\mathbf{Z} \\in \\mathbb{R}^{T \\times 10}$ with independent and identically distributed standard normal entries and then setting $\\mathbf{R} = \\mathbf{Z} \\mathbf{L}^\\top$, where $\\mathbf{L}$ is the lower-triangular Cholesky factor of $\\boldsymbol{\\Sigma}$ such that $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^\\top$. Use a fixed pseudo-random seed $12345$ for every dataset generation. This process induces correlated asset returns with the specified covariance, serving as the historical sample for the historical simulation. Angles are not involved in this task. No physical units apply; report all losses as unitless decimal numbers.\n\nImplement a program that computes $\\widehat{\\mathrm{VaR}}_{0.99}$ by the empirical order statistic definition above for each of the following test cases. In all cases, use the generation method and the fixed seed $12345$ to produce the $\\mathbf{R}$ matrix for the specified $T$.\n\nTest Suite:\n- Case A (general case): $T = 1000$, weights $w_i = 1/10$ for all $i \\in \\{1,\\ldots,10\\}$.\n- Case B (concentration edge case): $T = 500$, weights $w_1 = 1$ and $w_i = 0$ for all $i \\in \\{2,\\ldots,10\\}$.\n- Case C (risk-parity style case): $T = 252$, weights given by $w_i \\propto 1/\\sigma_i$ for $i \\in \\{1,\\ldots,10\\}$, normalized so that $\\sum_{i=1}^{10} w_i = 1$.\n\nAlgorithmic requirements:\n- Use the empirical quantile with $k = \\lceil 0.99 \\cdot T \\rceil$ as defined above (no interpolation).\n- For each case, compute the loss series $\\{L_t\\}_{t=1}^T$ from the generated returns and the specified weights, and then compute $\\widehat{\\mathrm{VaR}}_{0.99}$.\n- Round each final $\\widehat{\\mathrm{VaR}}_{0.99}$ to $6$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the three rounded results for Cases A, B, and C, in that order, as a comma-separated list enclosed in square brackets. For example, the output must look like $[x_A,x_B,x_C]$ with each $x$ shown to exactly $6$ decimal places.", "solution": "The problem is well-defined, scientifically sound, and internally consistent. We shall proceed with a formal solution.\n\nThe objective is to calculate the $99\\%$ Value at Risk ($\\mathrm{VaR}_{0.99}$) for a portfolio of $N=10$ assets under three distinct scenarios. The calculation will use the historical simulation method, where the historical data is synthetically generated according to a specified statistical model.\n\n**1. Synthetic Generation of Asset Returns**\n\nThe core of the simulation is the generation of a $T \\times N$ matrix of historical returns, $\\mathbf{R}$, where $T$ is the number of days and $N=10$ is the number of assets. The returns are modeled as draws from a multivariate normal distribution with a mean of zero and a specified covariance matrix $\\boldsymbol{\\Sigma}$. This covariance structure is crucial for capturing the interdependencies between asset price movements.\n\nFirst, we define the components of the covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{10 \\times 10}$.\nThe per-asset daily volatility vector is $\\boldsymbol{\\sigma} = (\\sigma_1, \\sigma_2, \\dots, \\sigma_{10})^\\top$, where the volatility of the $i$-th asset is given by the formula:\n$$\n\\sigma_i = 0.01 + 0.002 \\cdot (i-1) \\quad \\text{for } i \\in \\{1, 2, \\dots, 10\\}\n$$\nThis creates a spectrum of volatilities, starting from $\\sigma_1 = 0.01$ and increasing to $\\sigma_{10} = 0.028$. These volatilities form the diagonal of a matrix $\\mathbf{D} = \\mathrm{diag}(\\boldsymbol{\\sigma})$.\n\nNext, the correlation structure is defined by a matrix $\\mathbf{C} \\in \\mathbb{R}^{10 \\times 10}$, where the correlation between asset $i$ and asset $j$ is:\n$$\nC_{ij} = \\rho^{|i-j|}\n$$\nThe parameter $\\rho$ is given as $0.6$. This structure implies that assets closer in index are more strongly correlated.\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ is then assembled as:\n$$\n\\boldsymbol{\\Sigma} = \\mathbf{D} \\mathbf{C} \\mathbf{D}\n$$\nTo generate correlated random variates, we use the Cholesky decomposition of $\\boldsymbol{\\Sigma}$. We find a lower-triangular matrix $\\mathbf{L}$ such that $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^\\top$.\n\nThe historical return matrix $\\mathbf{R}$ is generated by first creating a matrix $\\mathbf{Z} \\in \\mathbb{R}^{T \\times 10}$ of independent and identically distributed standard normal random variables ($\\mathcal{N}(0,1)$). The correlated returns are then obtained by the transformation:\n$$\n\\mathbf{R} = \\mathbf{Z} \\mathbf{L}^\\top\n$$\nEach row $\\mathbf{r}_t^\\top$ of $\\mathbf{R}$ is a random vector representing the daily returns of the $10$ assets, and these vectors are drawn from $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$. For reproducibility, the pseudo-random number generator is initialized with a fixed seed of $12345$ for each data generation process.\n\n**2. Historical Simulation for Value at Risk ($\\mathrm{VaR}$)**\n\nGiven a portfolio weight vector $\\mathbf{w} = (w_1, w_2, \\dots, w_{10})^\\top$, the daily return of the portfolio, $R^p_t$, is the weighted sum of the individual asset returns:\n$$\nR^p_t = \\mathbf{r}_t^\\top \\mathbf{w} = \\sum_{i=1}^{10} w_i r_{t,i}\n$$\nThe corresponding daily portfolio loss is defined as the negative of the return: $L_t = -R^p_t$. This calculation is performed for each day $t$ from $1$ to $T$, yielding a sample of historical losses $\\{L_t\\}_{t=1}^T$.\n\nThe historical simulation approach estimates the $\\mathrm{VaR}$ from the empirical distribution of this loss sample. The $p$-quantile of the loss distribution is estimated by finding a specific value from the sorted loss data. We sort the losses in non-decreasing order:\n$$\nL_{(1)} \\le L_{(2)} \\le \\cdots \\le L_{(T)}\n$$\nThe problem specifies the empirical $p$-quantile, $\\widehat{q}_p$, to be the $k$-th order statistic, $L_{(k)}$, where the index $k$ is given by:\n$$\nk = \\lceil p \\cdot T \\rceil\n$$\nThis is a 1-based index. For the $99\\%$ $\\mathrm{VaR}$, we set $p=0.99$, so the historical estimate is:\n$$\n\\widehat{\\mathrm{VaR}}_{0.99} = L_{(k)} \\quad \\text{with} \\quad k = \\lceil 0.99 \\cdot T \\rceil\n$$\n\n**3. Application to Test Cases**\n\nWe now apply this methodology to the three specified test cases. The covariance matrix $\\boldsymbol{\\Sigma}$ and its Cholesky factor $\\mathbf{L}$ are common to all cases as they depend only on $N=10$ and the fixed parameters $\\boldsymbol{\\sigma}$ and $\\rho$.\n\n**Case A: General Case**\n-   Time periods: $T = 1000$.\n-   Portfolio weights: An equally weighted portfolio, $w_i = 1/10 = 0.1$ for all $i$.\n-   $\\mathrm{VaR}$ index: $k = \\lceil 0.99 \\cdot 1000 \\rceil = \\lceil 990 \\rceil = 990$.\n-   Procedure:\n    1.  Generate $\\mathbf{R} \\in \\mathbb{R}^{1000 \\times 10}$ using the seed $12345$.\n    2.  Compute the loss series $\\{L_t\\}_{t=1}^{1000}$ using $\\mathbf{w} = (0.1, \\dots, 0.1)^\\top$.\n    3.  Sort the losses and select the $990$-th value, $L_{(990)}$.\n\n**Case B: Concentration Edge Case**\n-   Time periods: $T = 500$.\n-   Portfolio weights: A portfolio fully concentrated in the first asset, $\\mathbf{w} = (1, 0, \\dots, 0)^\\top$.\n-   $\\mathrm{VaR}$ index: $k = \\lceil 0.99 \\cdot 500 \\rceil = \\lceil 495 \\rceil = 495$.\n-   Procedure:\n    1.  Generate $\\mathbf{R} \\in \\mathbb{R}^{500 \\times 10}$ using the seed $12345$.\n    2.  Compute the loss series $\\{L_t\\}_{t=1}^{500}$. Here, $L_t = -r_{t,1}$.\n    3.  Sort the losses and select the $495$-th value, $L_{(495)}$.\n\n**Case C: Risk-Parity Style Case**\n-   Time periods: $T = 252$ (approximating one trading year).\n-   Portfolio weights: Weights are inversely proportional to asset volatility, $w_i \\propto 1/\\sigma_i$. This is a \"risk-parity\" inspired allocation, where less volatile assets receive higher weight. The weights must be normalized to sum to $1$:\n    $$\n    w_i = \\frac{1/\\sigma_i}{\\sum_{j=1}^{10} (1/\\sigma_j)}\n    $$\n-   $\\mathrm{VaR}$ index: $k = \\lceil 0.99 \\cdot 252 \\rceil = \\lceil 249.48 \\rceil = 250$.\n-   Procedure:\n    1.  Calculate the normalized weights $\\mathbf{w}$.\n    2.  Generate $\\mathbf{R} \\in \\mathbb{R}^{252 \\times 10}$ using the seed $12345$.\n    3.  Compute the loss series $\\{L_t\\}_{t=1}^{252}$ using the calculated $\\mathbf{w}$.\n    4.  Sort the losses and select the $250$-th value, $L_{(250)}$.\n\nFor each case, the computed $\\widehat{\\mathrm{VaR}}_{0.99}$ will be rounded to $6$ decimal places as required. The implementation will use 0-based array indexing, so the value at index $k$ in a 1-based system corresponds to the value at index $k-1$ in code.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef generate_returns(T, N, sigma_vec, corr_matrix, seed):\n    \"\"\"\n    Generates a matrix of correlated asset returns.\n\n    Args:\n        T (int): Number of time periods (days).\n        N (int): Number of assets.\n        sigma_vec (np.ndarray): Vector of asset volatilities.\n        corr_matrix (np.ndarray): Asset correlation matrix.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        np.ndarray: A T x N matrix of simulated daily returns.\n    \"\"\"\n    # Construct the covariance matrix: Sigma = D * C * D\n    D = np.diag(sigma_vec)\n    cov_matrix = D @ corr_matrix @ D\n\n    # Perform Cholesky decomposition: Sigma = L * L^T\n    # numpy.linalg.cholesky returns the lower-triangular matrix L.\n    L = np.linalg.cholesky(cov_matrix)\n\n    # Generate independent standard normal random variables\n    rng = np.random.default_rng(seed)\n    Z = rng.standard_normal((T, N))\n\n    # Generate correlated returns: R = Z * L^T\n    R = Z @ L.T\n    return R\n\ndef calculate_var(T, weights, returns):\n    \"\"\"\n    Calculates the 99% Value at Risk using historical simulation.\n\n    Args:\n        T (int): Number of time periods.\n        weights (np.ndarray): Portfolio weights vector.\n        returns (np.ndarray): Matrix of historical returns.\n\n    Returns:\n        float: The 99% VaR, rounded to 6 decimal places.\n    \"\"\"\n    # Calculate daily portfolio returns: Rp_t = R_t^T * w\n    portfolio_returns = returns @ weights\n    \n    # Calculate daily portfolio losses: L_t = -Rp_t\n    losses = -portfolio_returns\n    \n    # Sort losses in non-decreasing order\n    sorted_losses = np.sort(losses)\n    \n    # Calculate the index k for the 99% empirical quantile\n    # k = ceil(p * T), with 1-based indexing\n    p = 0.99\n    k = math.ceil(p * T)\n    \n    # Get the VaR value. Index is k-1 due to 0-based indexing in Python.\n    var_99 = sorted_losses[k - 1]\n    \n    return round(var_99, 6)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three test cases.\n    \"\"\"\n    N = 10\n    rho = 0.6\n    \n    # Define asset volatilities\n    sigma_vec = np.array([0.01 + 0.002 * (i - 1) for i in range(1, N + 1)])\n    \n    # Define correlation matrix\n    indices = np.arange(N)\n    corr_matrix = rho ** np.abs(indices - indices[:, np.newaxis])\n    \n    # Fixed seed for all data generation\n    seed = 12345\n    \n    # Define test cases\n    test_cases = [\n        {'name': 'A', 'T': 1000, 'weights_def': 'equal'},\n        {'name': 'B', 'T': 500, 'weights_def': 'concentrated'},\n        {'name': 'C', 'T': 252, 'weights_def': 'risk-parity'}\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        T = case['T']\n        \n        # Generate returns for the specific T\n        returns = generate_returns(T, N, sigma_vec, corr_matrix, seed)\n        \n        # Define weights for the case\n        weights = np.zeros(N)\n        if case['weights_def'] == 'equal':\n            weights = np.full(N, 1.0 / N)\n        elif case['weights_def'] == 'concentrated':\n            weights[0] = 1.0\n        elif case['weights_def'] == 'risk-parity':\n            inv_sigma = 1.0 / sigma_vec\n            weights = inv_sigma / np.sum(inv_sigma)\n        \n        # Calculate VaR for the case\n        var_result = calculate_var(T, weights, returns)\n        results.append(var_result)\n\n    # Format the final output string as [x_A,x_B,x_C]\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n\n```"}, {"introduction": "Historical financial data is rarely pristine and often requires careful pre-processing to be economically meaningful. This practice addresses a common real-world data challenge: a stock split. You will learn how to correctly adjust a historical price series to remove the distortionary effects of a corporate action before calculating returns, ensuring your VaR estimate ([@problem_id:2400150]) reflects genuine market risk rather than data artifacts.", "id": "2400150", "problem": "You are given a portfolio composed of a small number of equities with known daily closing prices over a finite lookback window of length $T$ days. One of the equities undergoes a $2$-for-$1$ stock split at a known effective date within the lookback period, meaning that at the split date $t_s$ the recorded closing price is mechanically halved while the number of shares doubles. For the purpose of measuring risk, you must compute the one-day Historical Simulation (HS) Value at Risk (VaR) at a confidence level of $0.95$ for specified portfolios using the following first-principles definitions and requirements.\n\nDefinitions and requirements:\n- Let there be $N$ assets. Let $P^{(i)}_t$ denote the recorded (unadjusted) closing price of asset $i$ at date $t$, for $t \\in \\{0,1,\\dots,T\\}$ and $i \\in \\{1,\\dots,N\\}$. Let $h_i$ denote the number of shares currently held (at date $T$) of asset $i$.\n- A $2$-for-$1$ split for asset $i$ occurring at date $t_s$ is defined as follows: the recorded price at $t_s$ reflects the split, while recorded prices for $t &lt; t_s$ do not. To form economically comparable price relatives that exclude the mechanical effect of the split, define adjusted prices $\\tilde{P}^{(i)}_t$ by\n$$\n\\tilde{P}^{(i)}_t \\;=\\;\n\\begin{cases}\nP^{(i)}_t / s, & \\text{if } t &lt; t_s \\text{ and asset } i \\text{ has a split at } t_s \\text{ with factor } s, \\\\\nP^{(i)}_t, & \\text{otherwise},\n\\end{cases}\n$$\nwith $s = 2$ for a $2$-for-$1$ split. If an asset has no split, then $\\tilde{P}^{(i)}_t = P^{(i)}_t$ for all $t$.\n- For each day $t \\in \\{1,\\dots,T\\}$, define the day-$t$ simple return for asset $i$ using the adjusted prices:\n$$\nR^{(i)}_t \\;=\\; \\frac{\\tilde{P}^{(i)}_t}{\\tilde{P}^{(i)}_{t-1}} - 1.\n$$\n- Let the current adjusted price vector at date $T$ be $\\tilde{\\mathbf{P}}_T = (\\tilde{P}^{(1)}_T,\\dots,\\tilde{P}^{(N)}_T)$. The portfolio one-day profit-or-loss (P&amp;L) under historical scenario $t \\in \\{1,\\dots,T\\}$ is\n$$\n\\Pi_t \\;=\\; \\sum_{i=1}^N h_i \\, \\tilde{P}^{(i)}_T \\, R^{(i)}_t.\n$$\n- Let $\\{\\Pi_t\\}_{t=1}^T$ be the empirical sample of one-day P&amp;L scenarios. The one-day HS VaR at confidence level $\\alpha = 0.95$ is the positive number\n$$\n\\mathrm{VaR}_{\\alpha} \\;=\\; - Q_p(\\{\\Pi_t\\}),\n$$\nwhere $p = 1 - \\alpha = 0.05$ and $Q_p$ denotes the left-continuous empirical quantile defined by\n$$\nQ_p(\\{\\Pi_t\\}) \\;=\\; \\Pi_{(k)}, \\quad \\text{with } k = \\left\\lceil p \\, T \\right\\rceil,\n$$\nand $\\Pi_{(1)} \\le \\Pi_{(2)} \\le \\dots \\le \\Pi_{(T)}$ are the order statistics of the sample. If $\\left\\lceil p T \\right\\rceil &lt; 1$, take $k = 1$.\n- All VaR values must be expressed in the same currency units as the input prices (e.g., dollars), rounded to $2$ decimal places, and represented as decimal numbers (no percent signs).\n\nTest suite:\nFor each test case below, compute the one-day HS VaR at confidence level $\\alpha = 0.95$ according to the definitions above.\n\n- Test Case $1$:\n  - Number of assets $N = 2$, lookback length $T = 7$.\n  - Asset $1$ recorded prices $P^{(1)}_t$ for $t = 0,\\dots,7$: $[98.0,\\,101.0,\\,99.0,\\,100.0,\\,50.0,\\,51.0,\\,50.5,\\,52.0]$.\n  - Asset $2$ recorded prices $P^{(2)}_t$ for $t = 0,\\dots,7$: $[50.0,\\,49.5,\\,50.5,\\,51.0,\\,50.8,\\,51.4,\\,51.2,\\,52.0]$.\n  - A $2$-for-$1$ split occurs for asset $1$ at date $t_s = 4$ (i.e., between $t=3$ and $t=4$), so $s = 2$ for asset $1$ and no split for asset $2$.\n  - Holdings: $h_1 = 200$, $h_2 = 100$.\n  - Output: one float, the VaR in currency units rounded to $2$ decimals.\n\n- Test Case $2$:\n  - Number of assets $N = 1$, lookback length $T = 4$.\n  - Asset $1$ recorded prices $P^{(1)}_t$ for $t = 0,\\dots,4$: $[120.0,\\,60.0,\\,61.2,\\,60.0,\\,62.4]$.\n  - A $2$-for-$1$ split occurs for asset $1$ at date $t_s = 1$, so $s = 2$ for asset $1$.\n  - Holdings: $h_1 = 80$.\n  - Output: one float, the VaR in currency units rounded to $2$ decimals.\n\n- Test Case $3$:\n  - Number of assets $N = 3$, lookback length $T = 5$.\n  - Asset $1$ recorded prices $P^{(1)}_t$ for $t = 0,\\dots,5$: $[40.0,\\,41.2,\\,40.8,\\,41.0,\\,41.5,\\,41.0]$.\n  - Asset $2$ recorded prices $P^{(2)}_t$ for $t = 0,\\dots,5$: $[200.0,\\,198.0,\\,199.0,\\,201.0,\\,202.0,\\,200.0]$.\n  - Asset $3$ recorded prices $P^{(3)}_t$ for $t = 0,\\dots,5$: $[10.0,\\,10.1,\\,10.2,\\,10.2,\\,10.1,\\,10.2]$.\n  - No splits for any asset in this case.\n  - Holdings: $h_1 = 100$, $h_2 = 10$, $h_3 = 0$.\n  - Output: one float, the VaR in currency units rounded to $2$ decimals.\n\n- Test Case $4$:\n  - Number of assets $N = 2$, lookback length $T = 5$.\n  - Asset $1$ recorded prices $P^{(1)}_t$ for $t = 0,\\dots,5$: $[100.0,\\,102.0,\\,101.0,\\,103.0,\\,104.0,\\,52.0]$.\n  - Asset $2$ recorded prices $P^{(2)}_t$ for $t = 0,\\dots,5$: $[30.0,\\,30.3,\\,29.7,\\,30.0,\\,30.6,\\,30.9]$.\n  - A $2$-for-$1$ split occurs for asset $1$ at date $t_s = 5$, so $s = 2$ for asset $1$ and no split for asset $2$.\n  - Holdings: $h_1 = 150$, $h_2 = 200$.\n  - Output: one float, the VaR in currency units rounded to $2$ decimals.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as Test Cases $1$ through $4$, for example, $[x_1,x_2,x_3,x_4]$, where each $x_j$ is the requested VaR rounded to $2$ decimals.", "solution": "The problem statement submitted for analysis is determined to be **valid**. It is scientifically grounded in the established principles of financial risk management, specifically the Historical Simulation (HS) methodology for Value at Risk (VaR) calculation. The problem is well-posed, providing a complete and consistent set of definitions, data, and constraints that permit a unique and verifiable solution. All terms are defined with mathematical precision, and the numerical examples are physically and economically plausible.\n\nThe solution proceeds by implementing the prescribed algorithm, which is a standard industry approach. The core principle of Historical Simulation is to estimate the potential one-day profit-and-loss (P&L) distribution of a portfolio by subjecting the current portfolio to a set of historical market price movements. The VaR is then determined as a specific low-end quantile of this simulated P&L distribution, representing a potential loss that is not expected to be exceeded with a high degree of confidence.\n\nA crucial prerequisite for this method is the creation of an economically meaningful history of asset returns. Corporate actions such as stock splits cause mechanical shifts in price levels that do not reflect any change in the underlying economic value of the asset. To prevent these distortions from contaminating the historical return data, a price adjustment is necessary.\n\nThe problem defines an adjusted price series, $\\tilde{P}^{(i)}_t$, for each asset $i$. For an asset that undergoes a $s$-for-$1$ stock split at date $t_s$, the historical prices recorded before the split, $P^{(i)}_t$ for $t < t_s$, are divided by the split factor $s$. For a $2$-for-$1$ split, $s=2$. Prices at and after the split date are not adjusted. In mathematical terms:\n$$\n\\tilde{P}^{(i)}_t =\n\\begin{cases}\nP^{(i)}_t / s, & \\text{if } t < t_s \\text{ and asset } i \\text{ has a split at } t_s \\text{ with factor } s, \\\\\nP^{(i)}_t, & \\text{otherwise}.\n\\end{cases}\n$$\nFor assets with no splits, $\\tilde{P}^{(i)}_t = P^{(i)}_t$ for all $t$.\n\nFrom this consistent, adjusted price series, the historical simple returns for each asset $i$ over the lookback window $t \\in \\{1, \\dots, T\\}$ are computed:\n$$\nR^{(i)}_t = \\frac{\\tilde{P}^{(i)}_t}{\\tilde{P}^{(i)}_{t-1}} - 1.\n$$\nThese returns represent the true historical percentage changes in value.\n\nThe next step is to generate the P&L scenarios. The P&L for a historical scenario $t$ is calculated by applying the historical returns $R^{(i)}_t$ to the current market value of each holding. The current value of the holding in asset $i$ is $V_i = h_i \\, P^{(i)}_T$, where $h_i$ is the number of shares and $P^{(i)}_T$ is the last recorded price. Note that the adjusted price at time $T$, $\\tilde{P}^{(i)}_T$, is always equal to the recorded price $P^{(i)}_T$ per its definition, as $T \\not< t_s$. The portfolio P&L for scenario $t$ is the sum of the P&L contributions from all $N$ assets:\n$$\n\\Pi_t = \\sum_{i=1}^N V_i \\, R^{(i)}_t = \\sum_{i=1}^N h_i \\, P^{(i)}_T \\, R^{(i)}_t.\n$$\nThis calculation is performed for each day $t \\in \\{1, \\dots, T\\}$ in the lookback period, yielding an empirical sample of $T$ potential P&L values, $\\{\\Pi_t\\}_{t=1}^T$.\n\nFinally, the Value at Risk is extracted from this P&L distribution. The problem defines the one-day HS VaR at confidence level $\\alpha = 0.95$ as the negative of the $p$-quantile of the P&L sample, where $p = 1 - \\alpha = 0.05$. The quantile $Q_p(\\{\\Pi_t\\})$ is found by first sorting the P&L sample in ascending order to obtain the order statistics $\\Pi_{(1)} \\le \\Pi_{(2)} \\le \\dots \\le \\Pi_{(T)}$. The quantile is then the $k$-th element of this sorted sample:\n$$\nQ_p(\\{\\Pi_t\\}) = \\Pi_{(k)}, \\quad \\text{where } k = \\lceil p T \\rceil.\n$$\nThe problem specifies that if $\\lceil p T \\rceil < 1$, one should take $k=1$. The VaR is then reported as a positive value representing a loss:\n$$\n\\mathrm{VaR}_{\\alpha} = -Q_p(\\{\\Pi_t\\}).\n$$\nThe following is a detailed computation for Test Case $1$, which illustrates the application of this methodology. The remaining cases are solved by identical logic.\n\n**Test Case 1 Detailed Calculation:**\n- **Given:** $N=2$, $T=7$, $\\alpha=0.95$, $p=0.05$. Asset $1$ has a $2$-for-$1$ split ($s=2$) at $t_s=4$.\n- **Holdings:** $h_1 = 200$, $h_2 = 100$.\n- **Prices:**\n  - $P^{(1)} = [98.0, 101.0, 99.0, 100.0, 50.0, 51.0, 50.5, 52.0]$ for $t=0, \\dots, 7$.\n  - $P^{(2)} = [50.0, 49.5, 50.5, 51.0, 50.8, 51.4, 51.2, 52.0]$ for $t=0, \\dots, 7$.\n\n1.  **Price Adjustment:** Asset $1$ prices for $t \\in \\{0, 1, 2, 3\\}$ are divided by $s=2$.\n    - $\\tilde{P}^{(1)} = [98/2, 101/2, 99/2, 100/2, 50.0, 51.0, 50.5, 52.0] = [49.0, 50.5, 49.5, 50.0, 50.0, 51.0, 50.5, 52.0]$.\n    - Asset $2$ has no split: $\\tilde{P}^{(2)} = P^{(2)}$.\n\n2.  **Return Calculation ($t=1, \\dots, 7$):**\n    - $R^{(1)} \\approx [0.03061, -0.01980, 0.01010, 0.00000, 0.02000, -0.00980, 0.02970]$.\n    - $R^{(2)} \\approx [-0.01000, 0.02020, 0.00990, -0.00392, 0.01181, -0.00389, 0.01563]$.\n\n3.  **P&L Calculation:**\n    - Current values: $V_1 = h_1 P^{(1)}_7 = 200 \\times 52.0 = 10400$. $V_2 = h_2 P^{(2)}_7 = 100 \\times 52.0 = 5200$.\n    - $\\Pi_t = 10400 \\cdot R^{(1)}_t + 5200 \\cdot R^{(2)}_t$.\n    - $\\{\\Pi_t\\}_{t=1}^7 \\approx [266.37, -100.89, 156.54, -20.39, 269.42, -122.19, 390.16]$.\n\n4.  **VaR Calculation:**\n    - Sorted P&L: $\\{\\Pi_{(t)}\\}_{t=1}^7 \\approx [-122.19, -100.89, -20.39, 156.54, 266.37, 269.42, 390.16]$.\n    - Index calculation: $k = \\lceil pT \\rceil = \\lceil 0.05 \\times 7 \\rceil = \\lceil 0.35 \\rceil = 1$.\n    - Quantile: $Q_{0.05}(\\{\\Pi_t\\}) = \\Pi_{(1)} \\approx -122.19$.\n    - VaR: $\\mathrm{VaR}_{0.95} = - (-122.19) = 122.19$. After rounding to $2$ decimal places, the VaR is $122.19$.\n\nThe computations for the other test cases follow the same procedure, yielding the final consolidated results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_var(prices_unadjusted, holdings, splits, T, alpha):\n    \"\"\"\n    Computes the one-day Historical Simulation VaR for a given portfolio.\n\n    Args:\n        prices_unadjusted (list of list of floats): A list where each element is a list of\n            unadjusted closing prices for an asset, from t=0 to T.\n        holdings (list of floats): Number of shares held for each asset.\n        splits (dict): A dictionary mapping asset index (0-based) to a tuple\n            (split_date, split_factor).\n        T (int): The lookback period in days for calculating returns.\n        alpha (float): The confidence level for VaR.\n\n    Returns:\n        float: The calculated VaR, rounded to 2 decimal places.\n    \"\"\"\n    N = len(holdings)\n    p = 1.0 - alpha\n\n    # Step 1: Adjust prices for stock splits\n    prices_adjusted = []\n    for i in range(N):\n        p_asset = np.array(prices_unadjusted[i], dtype=float)\n        if i in splits:\n            t_s, s = splits[i]\n            # Prices before the split date are adjusted.\n            # a_s is 1-based index in problem, convert to 0-based\n            if t_s > 0:\n                p_asset[:t_s] /= s\n        prices_adjusted.append(p_asset)\n\n    prices_adjusted = np.array(prices_adjusted)\n\n    # Step 2: Calculate historical simple returns\n    # prices_adjusted is N x (T+1). Returns will be N x T.\n    # returns[:, t] corresponds to R_{t+1}\n    returns = prices_adjusted[:, 1:] / prices_adjusted[:, :-1] - 1\n\n    # Step 3: Calculate portfolio P&L scenarios\n    # Get current prices (at time T)\n    current_prices = prices_adjusted[:, -1]\n    \n    # Calculate current values of holdings\n    current_values = np.array(holdings) * current_prices\n    \n    # Calculate P&L for each historical day t = 1..T\n    # The shape of returns.T is T x N. The shape of current_values is N.\n    # The dot product for each row of returns.T gives the P&L for that day.\n    pnl_scenarios = returns.T.dot(current_values)\n\n    # Step 4: Calculate VaR from the P&L distribution\n    # Sort P&L scenarios in ascending order\n    sorted_pnl = np.sort(pnl_scenarios)\n\n    # Calculate the index k for the quantile\n    # Per problem: k = ceil(p*T). If < 1, take k=1.\n    k = int(np.ceil(p * T))\n    if k < 1:\n        k = 1\n    \n    # Get the quantile. In a 0-indexed array, this is element k-1.\n    quantile = sorted_pnl[k - 1]\n\n    # VaR is the negative of the quantile, representing a loss\n    var = -quantile\n    \n    return var\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases from the problem statement.\n    \"\"\"\n    test_cases = [\n        {\n            \"prices\": [\n                [98.0, 101.0, 99.0, 100.0, 50.0, 51.0, 50.5, 52.0],\n                [50.0, 49.5, 50.5, 51.0, 50.8, 51.4, 51.2, 52.0]\n            ],\n            \"holdings\": [200, 100],\n            \"splits\": {0: (4, 2)},  # Asset 1 (index 0) splits at t=4\n            \"T\": 7,\n            \"alpha\": 0.95\n        },\n        {\n            \"prices\": [\n                [120.0, 60.0, 61.2, 60.0, 62.4]\n            ],\n            \"holdings\": [80],\n            \"splits\": {0: (1, 2)},  # Asset 1 (index 0) splits at t=1\n            \"T\": 4,\n            \"alpha\": 0.95\n        },\n        {\n            \"prices\": [\n                [40.0, 41.2, 40.8, 41.0, 41.5, 41.0],\n                [200.0, 198.0, 199.0, 201.0, 202.0, 200.0],\n                [10.0, 10.1, 10.2, 10.2, 10.1, 10.2]\n            ],\n            \"holdings\": [100, 10, 0],\n            \"splits\": {}, # No splits\n            \"T\": 5,\n            \"alpha\": 0.95\n        },\n        {\n            \"prices\": [\n                [100.0, 102.0, 101.0, 103.0, 104.0, 52.0],\n                [30.0, 30.3, 29.7, 30.0, 30.6, 30.9]\n            ],\n            \"holdings\": [150, 200],\n            \"splits\": {0: (5, 2)},  # Asset 1 (index 0) splits at t=5\n            \"T\": 5,\n            \"alpha\": 0.95\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_var(\n            case[\"prices\"],\n            case[\"holdings\"],\n            case[\"splits\"],\n            case[\"T\"],\n            case[\"alpha\"]\n        )\n        results.append(result)\n\n    # Format output to ensure two decimal places are always shown\n    formatted_results = [f'{r:.2f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "While simple Historical Simulation is intuitive, its reliance on a fixed window of past returns makes it slow to adapt to new market volatility regimes. This advanced exercise introduces Filtered Historical Simulation (FHS), a powerful hybrid approach that rectifies this weakness. By using a GARCH($1,1$) model to filter historical returns, you will create a VaR model ([@problem_id:2400188]) that combines the non-parametric benefit of real historical shocks with the dynamic responsiveness of a conditional volatility forecast.", "id": "2400188", "problem": "You are given several finite sequences of daily log-returns and parameter values for a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model of order $(1,1)$. For each sequence and parameter set, compute the one-day-ahead filtered historical simulation Value at Risk (VaR) at a specified tail probability for a portfolio with current value equal to one monetary unit. Report each VaR as a non-negative real number in monetary units.\n\nThe filtered historical simulation VaR at tail probability $\\alpha$ for a unit-valued portfolio is defined as follows. Let the observed log-returns be $\\{r_0,\\dots,r_{T-1}\\}$, with $T \\geq 1$. Let the conditional variance evolve according to the GARCH$(1,1)$ recursion\n$$\n\\sigma_{i+1}^2 \\;=\\; \\omega \\;+\\; \\alpha \\, r_i^2 \\;+\\; \\beta \\, \\sigma_i^2 \\quad \\text{for } i=0,1,\\dots,T-1,\n$$\nwith initial variance $\\sigma_0^2 = s_0$, where $\\omega \\ge 0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta < 1$. Define the filtered (standardized) returns by\n$$\nz_i \\;=\\; \\frac{r_i}{\\sigma_i} \\quad \\text{for } i=0,1,\\dots,T-1,\n$$\nwhere $\\sigma_i = \\sqrt{\\sigma_i^2}$. The one-step-ahead conditional standard deviation is $\\sigma_T = \\sqrt{\\sigma_T^2}$, where $\\sigma_T^2$ is obtained from the recursion above. Construct the set of scaled scenarios for the next-day return as\n$$\n\\{\\tilde{r}_i\\}_{i=0}^{T-1}, \\quad \\text{where } \\tilde{r}_i = \\sigma_T \\, z_i.\n$$\nLet $N = T$ denote the number of scenarios. Order the scenarios in nondecreasing order to obtain $\\tilde{r}_{(1)} \\le \\tilde{r}_{(2)} \\le \\dots \\le \\tilde{r}_{(N)}$. Define the empirical lower-$\\alpha$ quantile by the nearest-rank rule\n$$\nk \\;=\\; \\max\\{1, \\lceil \\alpha N \\rceil\\}, \\qquad q_\\alpha \\;=\\; \\tilde{r}_{(k)}.\n$$\nThe filtered historical simulation VaR at tail probability $\\alpha$ for a unit-valued portfolio is\n$$\nV_\\alpha \\;=\\; -\\, q_\\alpha.\n$$\nReport $V_\\alpha$ in monetary units, expressed as a real number rounded to six decimal places.\n\nUse the above definitions to compute $V_\\alpha$ for each of the following test cases. All log-returns are dimensionless (daily log-returns), and all VaR outputs must be expressed in monetary units per unit portfolio value, rounded to six decimal places. The tail probability must be expressed as a decimal.\n\nTest Suite:\n\n- Test Case A (general case):\n  - Returns $\\{r_i\\}_{i=0}^{39}$:\n    $0.0042$, $-0.0061$, $0.0015$, $0.0023$, $-0.0038$, $0.0095$, $-0.0123$, $0.0071$, $-0.0022$, $0.0009$, $-0.0047$, $0.0033$, $0.0028$, $-0.0019$, $0.0056$, $-0.0082$, $0.0104$, $-0.0067$, $0.0011$, $-0.0026$, $0.0039$, $-0.0041$, $0.0062$, $-0.0008$, $0.0017$, $-0.0059$, $0.0044$, $-0.0031$, $0.0021$, $-0.0075$, $0.0089$, $-0.0064$, $0.0005$, $0.0012$, $-0.0029$, $0.0031$, $-0.0045$, $0.0026$, $-0.0014$, $0.0020$.\n  - Parameters: $\\omega = 0.000001$, $\\alpha = 0.07$, $\\beta = 0.92$, $s_0 = 0.0001$.\n  - Tail probability: $\\alpha = 0.05$.\n\n- Test Case B (small sample, boundary nearest-rank):\n  - Returns $\\{r_i\\}_{i=0}^{11}$:\n    $-0.0020$, $0.0015$, $-0.0010$, $0.0007$, $-0.0035$, $0.0021$, $-0.0009$, $0.0012$, $-0.0048$, $0.0004$, $0.0009$, $-0.0022$.\n  - Parameters: $\\omega = 0.000002$, $\\alpha = 0.10$, $\\beta = 0.85$, $s_0 = 0.00008$.\n  - Tail probability: $\\alpha = 0.10$.\n\n- Test Case C (high volatility clustering, extreme tail):\n  - Returns $\\{r_i\\}_{i=0}^{29}$:\n    $0.0150$, $-0.0220$, $0.0080$, $-0.0120$, $0.0060$, $-0.0180$, $0.0110$, $-0.0090$, $0.0050$, $-0.0130$, $0.0170$, $-0.0210$, $0.0090$, $-0.0140$, $0.0070$, $-0.0190$, $0.0120$, $-0.0110$, $0.0060$, $-0.0160$, $0.0180$, $-0.0240$, $0.0100$, $-0.0200$, $0.0080$, $-0.0150$, $0.0070$, $-0.0170$, $0.0130$, $-0.0180$.\n  - Parameters: $\\omega = 0.000005$, $\\alpha = 0.08$, $\\beta = 0.91$, $s_0 = 0.0002$.\n  - Tail probability: $\\alpha = 0.01$.\n\n- Test Case D (low volatility, smooth dynamics):\n  - Returns $\\{r_i\\}_{i=0}^{19}$:\n    $0.0004$, $-0.0006$, $0.0003$, $-0.0002$, $0.0005$, $-0.0004$, $0.0006$, $-0.0003$, $0.0002$, $-0.0005$, $0.0004$, $-0.0001$, $0.0003$, $-0.0002$, $0.0001$, $-0.0004$, $0.0005$, $-0.0002$, $0.0003$, $-0.0003$.\n  - Parameters: $\\omega = 0.0000005$, $\\alpha = 0.05$, $\\beta = 0.90$, $s_0 = 0.00003$.\n  - Tail probability: $\\alpha = 0.05$.\n\nYour program must compute $V_\\alpha$ for each test case in the order A, B, C, D, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[v_A,v_B,v_C,v_D]}$. Each value must be rounded to six decimal places.", "solution": "The problem is determined to be valid. It is scientifically sound, well-posed, and provides all necessary information to compute the requested quantities. The task is to calculate the one-day-ahead filtered historical simulation Value at Risk (VaR) for several time series of log-returns using a GARCH($1$,$1$) model for volatility.\n\nThe methodology follows the precise definition provided in the problem statement. Let the set of $T$ observed log-returns be $\\{r_i\\}_{i=0}^{T-1}$. The conditional variance, $\\sigma_i^2$, is modeled using a GARCH($1$,$1$) process. To avoid ambiguity with the GARCH parameter $\\alpha$, the tail probability for VaR calculation will be denoted $\\alpha_{\\text{tail}}$. The recursion for the conditional variance starts with an initial value $\\sigma_0^2 = s_0$ and proceeds as:\n$$\n\\sigma_{i+1}^2 \\;=\\; \\omega \\;+\\; \\alpha \\, r_i^2 \\;+\\; \\beta \\, \\sigma_i^2 \\quad \\text{for } i=0,1,\\dots,T-1\n$$\nThe parameters $\\omega$, $\\alpha$, and $\\beta$ are given constants satisfying $\\omega \\ge 0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta < 1$, which ensures non-negative variance and a stationary process.\n\nAt each step $i$, the standardized (or filtered) return $z_i$ is computed by scaling the observed log-return $r_i$ by the conditional standard deviation $\\sigma_i = \\sqrt{\\sigma_i^2}$:\n$$\nz_i \\;=\\; \\frac{r_i}{\\sigma_i}\n$$\nThis process is iterated for all $T$ returns, from $i=0$ to $i=T-1$, yielding a series of standardized returns $\\{z_i\\}_{i=0}^{T-1}$ and the one-step-ahead conditional variance forecast, $\\sigma_T^2$. The corresponding standard deviation is $\\sigma_T = \\sqrt{\\sigma_T^2}$.\n\nThe core of the filtered historical simulation method is to construct a set of scenarios for the next day's return by combining the current volatility forecast with the historical distribution of standardized returns. The scenarios, $\\{\\tilde{r}_i\\}_{i=0}^{T-1}$, are generated by rescaling the historical filtered returns $\\{z_i\\}$ with the one-step-ahead volatility forecast $\\sigma_T$:\n$$\n\\tilde{r}_i = \\sigma_T \\, z_i\n$$\nThis creates $N=T$ scenarios for the potential next-day return.\n\nTo find the VaR, these $N$ scenarios are sorted in non-decreasing order to form the ordered set $\\{\\tilde{r}_{(1)}, \\tilde{r}_{(2)}, \\dots, \\tilde{r}_{(N)}\\}$. The lower $\\alpha_{\\text{tail}}$-quantile of this empirical distribution, $q_{\\alpha_{\\text{tail}}}$, is found using the nearest-rank method. The rank index $k$ is calculated as:\n$$\nk \\;=\\; \\max\\{1, \\lceil \\alpha_{\\text{tail}} N \\rceil\\}\n$$\nThe quantile is then the $k$-th element of the sorted scenarios:\n$$\nq_\\alpha \\;=\\; \\tilde{r}_{(k)}\n$$\nFinally, the one-day Value at Risk at the $\\alpha_{\\text{tail}}$ confidence level for a portfolio of value $1$ monetary unit is the negative of this quantile:\n$$\nV_{\\alpha_{\\text{tail}}} \\;=\\; -\\, q_\\alpha\n$$\nThe result is reported as a non-negative number in monetary units, rounded to six decimal places. The procedure is applied to each test case as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the one-day-ahead filtered historical simulation VaR for multiple test cases.\n    \"\"\"\n\n    def calculate_fhs_var(returns, omega, alpha_garch, beta, s0, alpha_tail):\n        \"\"\"\n        Calculates the Filtered Historical Simulation VaR using a GARCH(1,1) model.\n\n        Args:\n            returns (list): Sequence of daily log-returns.\n            omega (float): GARCH(1,1) omega parameter.\n            alpha_garch (float): GARCH(1,1) alpha parameter.\n            beta (float): GARCH(1,1) beta parameter.\n            s0 (float): Initial variance, sigma_0^2.\n            alpha_tail (float): Tail probability for VaR calculation.\n\n        Returns:\n            float: The calculated VaR, rounded to six decimal places.\n        \"\"\"\n        T = len(returns)\n        returns_arr = np.array(returns, dtype=float)\n\n        # Array to store conditional variances sigma_i^2\n        variances = np.zeros(T + 1)\n        variances[0] = s0\n\n        # Array to store standardized returns z_i\n        std_returns = np.zeros(T)\n\n        # GARCH filtering loop\n        for i in range(T):\n            sigma_i_sq = variances[i]\n            r_i = returns_arr[i]\n\n            # sigma_i is guaranteed to be positive due to omega >= 0, alpha >= 0, beta >= 0 and s0 > 0.\n            sigma_i = np.sqrt(sigma_i_sq)\n            \n            std_returns[i] = r_i / sigma_i\n\n            variances[i+1] = omega + alpha_garch * (r_i**2) + beta * sigma_i_sq\n\n        # One-step-ahead volatility forecast\n        sigma_T_sq = variances[T]\n        sigma_T = np.sqrt(sigma_T_sq)\n\n        # Generate scaled scenarios\n        scaled_scenarios = sigma_T * std_returns\n\n        # Sort scenarios to find the quantile\n        scaled_scenarios.sort()\n\n        # Calculate quantile using the nearest-rank rule\n        N = T\n        # k is the 1-based rank\n        k = int(max(1, np.ceil(alpha_tail * N)))\n        # q_alpha is the k-th smallest scenario (using 0-based index k-1)\n        q_alpha = scaled_scenarios[k - 1]\n\n        # VaR is the negative of the quantile\n        var_alpha = -q_alpha\n\n        return var_alpha\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"returns\": [\n                0.0042, -0.0061, 0.0015, 0.0023, -0.0038, 0.0095, -0.0123, 0.0071, -0.0022, 0.0009, \n                -0.0047, 0.0033, 0.0028, -0.0019, 0.0056, -0.0082, 0.0104, -0.0067, 0.0011, -0.0026, \n                0.0039, -0.0041, 0.0062, -0.0008, 0.0017, -0.0059, 0.0044, -0.0031, 0.0021, -0.0075, \n                0.0089, -0.0064, 0.0005, 0.0012, -0.0029, 0.0031, -0.0045, 0.0026, -0.0014, 0.0020\n            ],\n            \"params\": {\"omega\": 0.000001, \"alpha_garch\": 0.07, \"beta\": 0.92, \"s0\": 0.0001},\n            \"alpha_tail\": 0.05\n        },\n        {\n            \"returns\": [\n                -0.0020, 0.0015, -0.0010, 0.0007, -0.0035, 0.0021, -0.0009, 0.0012, -0.0048, 0.0004, \n                0.0009, -0.0022\n            ],\n            \"params\": {\"omega\": 0.000002, \"alpha_garch\": 0.10, \"beta\": 0.85, \"s0\": 0.00008},\n            \"alpha_tail\": 0.10\n        },\n        {\n            \"returns\": [\n                0.0150, -0.0220, 0.0080, -0.0120, 0.0060, -0.0180, 0.0110, -0.0090, 0.0050, -0.0130, \n                0.0170, -0.0210, 0.0090, -0.0140, 0.0070, -0.0190, 0.0120, -0.0110, 0.0060, -0.0160, \n                0.0180, -0.0240, 0.0100, -0.0200, 0.0080, -0.0150, 0.0070, -0.0170, 0.0130, -0.0180\n            ],\n            \"params\": {\"omega\": 0.000005, \"alpha_garch\": 0.08, \"beta\": 0.91, \"s0\": 0.0002},\n            \"alpha_tail\": 0.01\n        },\n        {\n            \"returns\": [\n                0.0004, -0.0006, 0.0003, -0.0002, 0.0005, -0.0004, 0.0006, -0.0003, 0.0002, -0.0005, \n                0.0004, -0.0001, 0.0003, -0.0002, 0.0001, -0.0004, 0.0005, -0.0002, 0.0003, -0.0003\n            ],\n            \"params\": {\"omega\": 0.0000005, \"alpha_garch\": 0.05, \"beta\": 0.90, \"s0\": 0.00003},\n            \"alpha_tail\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        var_result = calculate_fhs_var(case[\"returns\"], **case[\"params\"], alpha_tail=case[\"alpha_tail\"])\n        # Format the result to exactly six decimal places\n        results.append(f\"{var_result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}