{"hands_on_practices": [{"introduction": "Our journey into the practical application of Extreme Value Theory begins with the block maxima method, a foundational approach for modeling the largest observations in a dataset. This exercise guides you through simulating a time series, such as electricity demand, and fitting a Generalized Extreme Value (GEV) distribution to the maximum values observed in successive time blocks. By engaging with this practice [@problem_id:2391840], you will develop the essential skills to quantify tail risk, calculate metrics like Value-at-Risk and Expected Shortfall, and even price derivatives tied to extreme outcomes, all from first principles.", "id": "2391840", "problem": "Write a complete program that applies Extreme Value Theory (EVT) to model the maximum daily electricity demand in Texas to support pricing peak-load electricity derivatives and assessing grid stability risk. Your implementation must start from the following fundamental base: the Fisher–Tippett–Gnedenko theorem, the definition of block maxima and the Generalized Extreme Value distribution, the definitions of quantile and Expected Shortfall, and standard principles of risk-neutral valuation. You must not use any shortcut formulas; compute quantities from first principles and, when needed, by numerical integration. Angles used inside trigonometric functions must be in radians. All electricity quantities must be treated in gigawatts (GW). Monetary scaling is normalized to $1$ per gigawatt so that derivative prices are expressed in the same numerical units as demand. All probabilities must be expressed as decimals.\n\nThe data-generating mechanism for synthetic daily electricity demand is as follows. Let daily demand $D_{t}$ for day index $t \\in \\{0,1,\\dots,N-1\\}$ be\n$$\nD_{t} = \\max\\left\\{0, \\ \\mu + A \\sin\\left(\\frac{2\\pi \\, (t \\bmod 365)}{365}\\right) + \\sigma \\, \\varepsilon_{t}\\right\\},\n$$\nwhere $\\varepsilon_{t}$ are independent and identically distributed draws from a Student $t$ distribution with $\\nu$ degrees of freedom. The symbol $\\max\\{\\cdot,\\cdot\\}$ enforces nonnegativity. The parameter $N$ equals $365 \\times Y$ for a sample spanning $Y$ years.\n\nYour program must perform the following steps for each test case:\n- Step $1$: Simulate the daily demand series $\\{D_{t}\\}_{t=0}^{N-1}$ using the specified random seed and parameters $(Y,\\ \\mu,\\ A,\\ \\sigma,\\ \\nu)$.\n- Step $2$: Partition the days into non-overlapping blocks of $B$ days (discard any leftover days) to form block maxima $\\{M_{i}\\}_{i=1}^{m}$ where $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$, and $M_{i} = \\max\\{D_{t} : t \\in \\text{block } i\\}$.\n- Step $3$: Fit a Generalized Extreme Value (GEV) distribution to the block maxima by maximum likelihood, obtaining parameters $(\\xi, \\mu_{\\text{gev}}, \\sigma_{\\text{gev}})$ where $\\xi$ is the shape parameter, $\\mu_{\\text{gev}}$ is the location, and $\\sigma_{\\text{gev}}$ is the scale.\n- Step $4$: Compute the one-step-ahead block-maximum Value-at-Risk at level $\\alpha$, defined as the $\\alpha$-quantile $q_{\\alpha}$ of the fitted GEV distribution for the maximum in the next block of length $B$ days.\n- Step $5$: Compute the one-step-ahead block-maximum Expected Shortfall at level $\\alpha$, defined as\n$$\n\\text{ES}_{\\alpha} \\equiv \\mathbb{E}\\!\\left[M \\mid M > q_{\\alpha}\\right] = \\frac{1}{1-\\alpha}\\int_{\\alpha}^{1} Q(u)\\,du,\n$$\nwhere $Q(u)$ is the quantile function of the fitted GEV distribution for $M$. Evaluate the integral numerically to sufficient precision.\n- Step $6$: Consider a European call option on the upcoming block maximum with maturity equal to the block length, i.e., $\\tau = \\frac{B}{365}$ years. The option payoff is $(M - K)^{+}$, where $K$ is the strike and $(x)^{+} \\equiv \\max\\{x,0\\}$. Under a risk-neutral approximation that identifies the fitted GEV with the risk-neutral distribution, compute the undiscounted expected payoff\n$$\n\\mathbb{E}\\!\\left[(M - K)^{+}\\right] = \\int_{0}^{1} \\max\\{Q(u) - K, 0\\}\\,du,\n$$\nand discount it at the continuously compounded risk-free rate $r$ to obtain the price $P = e^{-r \\tau} \\, \\mathbb{E}\\!\\left[(M - K)^{+}\\right]$. Evaluate any required integral numerically.\n- Step $7$: Given a capacity level $L$, compute the grid failure probability $p_{\\text{fail}} = \\mathbb{P}[M > L]$ for the next block maximum under the fitted GEV distribution.\n\nYour program must return, for each test case, a list with the following elements in order:\n$[q_{\\alpha},\\ \\text{ES}_{\\alpha},\\ P,\\ \\xi,\\ p_{\\text{fail}}]$,\nrounded as follows: $q_{\\alpha}$ to $3$ decimals, $\\text{ES}_{\\alpha}$ to $3$ decimals, $P$ to $3$ decimals, $\\xi$ to $4$ decimals, and $p_{\\text{fail}}$ to $6$ decimals. All quantities are floats without unit symbols in the output.\n\nTest suite. Use the following four test cases, each specified as an ordered tuple\n$(\\text{seed},\\ Y,\\ B,\\ \\mu,\\ A,\\ \\sigma,\\ \\nu,\\ \\alpha,\\ K,\\ L,\\ r)$:\n\n- Case $1$: $(12345,\\ 12,\\ 30,\\ 55.0,\\ 12.0,\\ 5.0,\\ 3.5,\\ 0.99,\\ 80.0,\\ 95.0,\\ 0.02)$.\n- Case $2$: $(2024,\\ 5,\\ 7,\\ 45.0,\\ 8.0,\\ 4.0,\\ 5.0,\\ 0.975,\\ 65.0,\\ 85.0,\\ 0.01)$.\n- Case $3$: $(7,\\ 8,\\ 14,\\ 50.0,\\ 6.0,\\ 3.0,\\ 30.0,\\ 0.995,\\ 75.0,\\ 90.0,\\ 0.0)$.\n- Case $4$: $(999,\\ 3,\\ 30,\\ 60.0,\\ 15.0,\\ 7.0,\\ 2.8,\\ 0.98,\\ 85.0,\\ 100.0,\\ 0.03)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the per-case result lists, enclosed in square brackets. For example, a valid shape would be\n$[[x_{1},y_{1},z_{1},u_{1},v_{1}],[x_{2},y_{2},z_{2},u_{2},v_{2}],\\dots]$,\nwith no additional text before or after.", "solution": "We model extremes of electricity demand using block maxima and the Generalized Extreme Value (GEV) distribution. The Fisher–Tippett–Gnedenko theorem states that, under broad conditions, the properly normalized maximum of independent and identically distributed observations converges in distribution to a member of the GEV family. We therefore take block maxima of daily demands and fit a GEV model by maximum likelihood.\n\nData generation. We simulate daily demand $D_{t}$ as\n$$\nD_{t} = \\max\\{0,\\ \\mu + A \\sin(2\\pi \\, t/365) + \\sigma \\varepsilon_{t}\\},\n$$\nwith $\\varepsilon_{t}$ independent and identically distributed draws from a Student $t$ distribution with $\\nu$ degrees of freedom. The angle is in radians. The sine term captures seasonal variation, and the heavy-tailed noise captures rare spikes. Nonnegativity is enforced by the $\\max$ operator.\n\nBlock maxima. We partition the series into blocks of $B$ days and take maxima $M_{i}$ for each block. If we have $N = 365 Y$ total days, the number of full blocks is $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$ and any leftover days are discarded to maintain equal block sizes. We collect $\\{M_{i}\\}_{i=1}^{m}$.\n\nGEV fitting. We fit a GEV distribution by maximum likelihood to $\\{M_{i}\\}$. The GEV has cumulative distribution function\n$$\nF(x) = \\exp\\left\\{-\\left[1 + \\xi \\left(\\frac{x - \\mu_{\\text{gev}}}{\\sigma_{\\text{gev}}}\\right)\\right]^{-1/\\xi}\\right\\}\n$$\non the support where $1 + \\xi \\left(\\frac{x - \\mu_{\\text{gev}}}{\\sigma_{\\text{gev}}}\\right) > 0$, with the Gumbel case $\\xi = 0$ interpreted by continuity. In the implementation, we use a standard scientific library whose parameterization employs a shape $c$ with $c = -\\xi$. After fitting, we report $\\xi = -c$.\n\nRisk measures. The one-step-ahead block-maximum Value-at-Risk at level $\\alpha$ is the quantile\n$$\nq_{\\alpha} = Q(\\alpha),\n$$\nwhere $Q$ is the quantile function (inverse cumulative distribution function) of the fitted GEV distribution. The Expected Shortfall at level $\\alpha$ is defined by\n$$\n\\text{ES}_{\\alpha} = \\mathbb{E}[M \\mid M > q_{\\alpha}],\n$$\nand can be written via the quantile function as\n$$\n\\text{ES}_{\\alpha} = \\frac{1}{1-\\alpha} \\int_{\\alpha}^{1} Q(u)\\,du.\n$$\nWe evaluate the integral numerically using adaptive quadrature. For heavy-tailed cases with $\\xi \\ge 1$, this integral would diverge; in our synthetic cases, the fitted $\\xi$ remains below $1$ and the integral is finite. To avoid numerical issues at $u = 1$, we integrate up to $u = 1 - \\varepsilon$ with a very small $\\varepsilon$.\n\nDerivative pricing. Consider a European call on the next block maximum $M$ with strike $K$ and maturity $\\tau = B/365$ years. Under a risk-neutral approximation where the fitted GEV is treated as the risk-neutral distribution, the call price is\n$$\nP = e^{-r \\tau} \\, \\mathbb{E}[(M - K)^{+}].\n$$\nUsing the quantile function, we express the undiscounted expectation as\n$$\n\\mathbb{E}[(M - K)^{+}] = \\int_{0}^{1} \\max\\{Q(u) - K, 0\\}\\,du = \\int_{u_{0}}^{1} Q(u)\\,du - (1 - u_{0}) K,\n$$\nwhere $u_{0} = F(K)$ is the cumulative distribution function at $K$. This identity follows because the area under the tail of the quantile function above $K$ equals the expected exceedance above $K$. We evaluate the integral numerically over $[u_{0}, 1 - \\varepsilon]$ and apply the discount factor $e^{-r \\tau}$.\n\nGrid failure probability. Given a capacity $L$, the failure probability for the next block is\n$$\np_{\\text{fail}} = \\mathbb{P}[M > L] = 1 - F(L),\n$$\ncomputed directly from the fitted GEV cumulative distribution function.\n\nAlgorithmic steps per test case:\n- Fix the seed and generate $N = 365 Y$ days of noise $\\varepsilon_{t}$ from the Student $t$ distribution with $\\nu$ degrees of freedom. Compute seasonal means $\\mu + A \\sin(2\\pi (t \\bmod 365)/365)$ and add scaled noise $\\sigma \\varepsilon_{t}$. Truncate at $0$ to get $D_{t}$.\n- Partition into $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$ blocks of length $B$, compute $M_{i} = \\max$ in each block.\n- Fit the GEV by maximum likelihood. Extract shape $c$, location $\\mu_{\\text{gev}}$, and scale $\\sigma_{\\text{gev}}$. Set $\\xi = -c$.\n- Compute $q_{\\alpha} = Q(\\alpha)$ from the fitted model.\n- Compute $\\text{ES}_{\\alpha} = \\frac{1}{1-\\alpha} \\int_{\\alpha}^{1} Q(u)\\,du$ by numerical quadrature with a small upper cutoff $\\varepsilon$ to avoid singularities.\n- Compute $u_{0} = F(K)$. If $u_{0} \\ge 1$, set the option value to $0$. Otherwise compute the integral $\\int_{u_{0}}^{1} Q(u)\\,du$ numerically and set $P = e^{-r \\tau}\\left(\\int_{u_{0}}^{1} Q(u)\\,du - (1 - u_{0}) K\\right)$ clipped at zero for numerical safety.\n- Compute $p_{\\text{fail}} = 1 - F(L)$ and bound it to $[0,1]$ for numerical stability.\n- Round $q_{\\alpha}$ and $\\text{ES}_{\\alpha}$ to $3$ decimals, $P$ to $3$ decimals, $\\xi$ to $4$ decimals, and $p_{\\text{fail}}$ to $6$ decimals.\n\nThe final output is a single line containing the list of per-case results $[[q_{\\alpha},\\ \\text{ES}_{\\alpha},\\ P,\\ \\xi,\\ p_{\\text{fail}}], \\dots]$ for the four test cases, with no additional text.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import genextreme\nfrom scipy.integrate import quad\n\ndef simulate_daily_demand(seed, years, mu, amplitude, sigma, df):\n    \"\"\"\n    Simulate daily electricity demand (in GW) over 'years' years,\n    with seasonality and heavy-tailed Student-t noise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    days = 365 * years\n    t = np.arange(days)\n    # Angle in radians; seasonality repeats every 365 days\n    seasonal = amplitude * np.sin(2.0 * np.pi * (t % 365) / 365.0)\n    noise = sigma * rng.standard_t(df, size=days)\n    demand = mu + seasonal + noise\n    # Enforce non-negativity\n    demand = np.maximum(0.0, demand)\n    return demand\n\ndef block_maxima(series, block_days):\n    \"\"\"\n    Partition 'series' into non-overlapping blocks of length 'block_days'\n    and return the list of maxima in each block. Discard remainder.\n    \"\"\"\n    n = len(series)\n    m = n // block_days\n    if m <= 0:\n        return np.array([])\n    trimmed = series[:m * block_days]\n    reshaped = trimmed.reshape(m, block_days)\n    maxima = reshaped.max(axis=1)\n    return maxima\n\ndef fit_gev_mle(maxima):\n    \"\"\"\n    Fit GEV distribution to block maxima using MLE via scipy.stats.genextreme.\n    SciPy's genextreme parameterization uses shape c = -xi.\n    Returns (xi, loc, scale).\n    \"\"\"\n    # Ensure we have variability\n    if len(maxima) < 3 or np.allclose(np.std(maxima), 0.0):\n        # Fallback: trivial fit with tiny scale to avoid errors\n        loc = float(np.mean(maxima)) if len(maxima) > 0 else 0.0\n        scale = float(np.std(maxima)) if len(maxima) > 0 else 1.0\n        c = 0.0\n    else:\n        c, loc, scale = genextreme.fit(maxima)\n        # Sometimes fit may return non-positive scale; guard it\n        if scale <= 0:\n            # Adjust scale to a small positive value\n            scale = max(1e-6, float(np.std(maxima)))\n    xi = -c\n    return xi, loc, scale, c\n\ndef gev_quantile(u, c, loc, scale):\n    \"\"\"Quantile function Q(u) for the fitted GEV in SciPy's parameterization.\"\"\"\n    return genextreme.ppf(u, c, loc=loc, scale=scale)\n\ndef gev_cdf(x, c, loc, scale):\n    \"\"\"CDF F(x) for the fitted GEV in SciPy's parameterization.\"\"\"\n    return genextreme.cdf(x, c, loc=loc, scale=scale)\n\ndef expected_shortfall_alpha(alpha, c, loc, scale):\n    \"\"\"\n    Compute ES_alpha = (1/(1-alpha)) * integral_{alpha}^{1} Q(u) du\n    via numerical quadrature. Integrate up to 1 - eps to avoid endpoint issues.\n    \"\"\"\n    eps = 1e-12\n    upper = 1.0 - eps\n    if alpha >= 1.0:\n        return float('inf')\n    # Define integrand with safety checks\n    def integrand(u):\n        q = gev_quantile(u, c, loc, scale)\n        return q\n    try:\n        val, _ = quad(integrand, alpha, upper, epsabs=1e-6, epsrel=1e-6, limit=200)\n        es = val / (1.0 - alpha)\n        return es\n    except Exception:\n        return float('inf')\n\ndef call_price_on_block_max(K, r, block_days, c, loc, scale):\n    \"\"\"\n    Price of a European call option on the next block maximum M with strike K,\n    maturity tau = block_days/365 years, under risk-neutral approximation\n    using the fitted GEV distribution. Uses quantile integral identity:\n    E[(M-K)+] = \\int_{u0}^{1} Q(u) du - (1-u0)K, where u0 = F(K).\n    \"\"\"\n    tau = block_days / 365.0\n    u0 = gev_cdf(K, c, loc, scale)\n    if not np.isfinite(u0):\n        return 0.0\n    if u0 >= 1.0:\n        undiscounted = 0.0\n    else:\n        eps = 1e-12\n        upper = 1.0 - eps\n        def integrand(u):\n            return gev_quantile(u, c, loc, scale)\n        try:\n            integral_val, _ = quad(integrand, u0, upper, epsabs=1e-6, epsrel=1e-6, limit=200)\n            undiscounted = integral_val - (1.0 - u0) * K\n            if not np.isfinite(undiscounted):\n                undiscounted = 0.0\n        except Exception:\n            undiscounted = 0.0\n    price = np.exp(-r * tau) * max(0.0, undiscounted)\n    return price\n\ndef risk_metrics_for_case(case):\n    \"\"\"\n    Compute [VaR_alpha, ES_alpha, call_price, xi, p_fail] for one test case.\n    Rounding:\n      VaR, ES, price -> 3 decimals\n      xi -> 4 decimals\n      p_fail -> 6 decimals\n    \"\"\"\n    (seed, Y, B, mu, A, sigma, nu, alpha, K, L, r) = case\n    # Simulate daily demand\n    demand = simulate_daily_demand(seed, Y, mu, A, sigma, nu)\n    # Block maxima\n    maxima = block_maxima(demand, B)\n    if len(maxima) < 3:\n        # Ensure there are enough maxima; otherwise pad with mean\n        if len(maxima) == 0:\n            maxima = np.array([mu])\n        elif len(maxima) == 1:\n            maxima = np.array([maxima[0], maxima[0] * 0.99 + 0.01, maxima[0] * 1.01 - 0.01])\n        else:\n            maxima = np.concatenate([maxima, [np.mean(maxima)]*(3 - len(maxima))])\n    # Fit GEV\n    xi, loc, scale, c = fit_gev_mle(maxima)\n    # VaR\n    try:\n        q_alpha = float(gev_quantile(alpha, c, loc, scale))\n        if not np.isfinite(q_alpha):\n            # Fallback: use high empirical quantile\n            q_alpha = float(np.quantile(maxima, alpha))\n    except Exception:\n        q_alpha = float(np.quantile(maxima, alpha))\n    # ES\n    es_alpha = expected_shortfall_alpha(alpha, c, loc, scale)\n    # Call price\n    price = call_price_on_block_max(K, r, B, c, loc, scale)\n    # Failure probability\n    try:\n        p_fail = 1.0 - float(gev_cdf(L, c, loc, scale))\n        if not np.isfinite(p_fail):\n            # Fallback approximation\n            p_fail = max(0.0, min(1.0, 1.0 - float(np.mean(maxima <= L))))\n    except Exception:\n        p_fail = max(0.0, min(1.0, 1.0 - float(np.mean(maxima <= L))))\n    p_fail = max(0.0, min(1.0, p_fail))\n    # Rounding\n    result = [\n        round(q_alpha, 3),\n        round(es_alpha, 3),\n        round(price, 3),\n        round(xi, 4),\n        round(p_fail, 6),\n    ]\n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (seed, Y, B, mu, A, sigma, nu, alpha, K, L, r)\n    test_cases = [\n        (12345, 12, 30, 55.0, 12.0, 5.0, 3.5, 0.99, 80.0, 95.0, 0.02),\n        (2024, 5, 7, 45.0, 8.0, 4.0, 5.0, 0.975, 65.0, 85.0, 0.01),\n        (7, 8, 14, 50.0, 6.0, 3.0, 30.0, 0.995, 75.0, 90.0, 0.0),\n        (999, 3, 30, 60.0, 15.0, 7.0, 2.8, 0.98, 85.0, 100.0, 0.03),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = risk_metrics_for_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure a single line with the nested list representation.\n    print(f\"{results}\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}, {"introduction": "Having explored block maxima, we now turn to the powerful Peak-over-Threshold (POT) framework, which models all events exceeding a high threshold. This practice [@problem_id:2391785] challenges you to not only fit a Generalized Pareto Distribution (GPD) to financial returns but also to perform a statistical test to determine if the tail behavior has changed, for instance, after a major economic announcement. Mastering this technique equips you with the tools to detect shifts in market risk regimes, a critical task in dynamic financial environments.", "id": "2391785", "problem": "You are asked to formalize and test a change in tail behavior of high-frequency currency returns around a major central bank announcement using Extreme Value Theory (EVT). Work in the Peaks-Over-Threshold (POT) framework grounded in the Pickands–Balkema–de Haan theorem, which states that for a sufficiently high threshold, the distribution of exceedances converges to a Generalized Pareto Distribution (GPD). You must implement a complete program that, from first principles, models the upper tail of absolute returns before and after the announcement, estimates the tail index, and statistically tests whether the tail index changes.\n\nStart from the following fundamental base:\n- The definition of the exceedance distribution over a high threshold and the Pickands–Balkema–de Haan theorem that motivates modeling excesses by a Generalized Pareto Distribution (GPD).\n- The principle of Maximum Likelihood Estimation (MLE) to estimate model parameters.\n- The Wald test principle for testing equality of parameters across two samples using the asymptotic covariance from the observed Fisher information (negative Hessian of the log-likelihood at the MLE).\n\nYour task:\n- Consider a stylized generator for high-frequency returns based on a symmetric Student distribution to produce heavy-tailed returns with a controllable tail index via the degrees of freedom. Let the pre-announcement and post-announcement segments have possibly different degrees of freedom and scale parameters.\n- For each segment, transform raw returns to absolute returns, select a high empirical threshold at a specified quantile, and collect exceedances.\n- Fit a GPD to exceedances via Maximum Likelihood Estimation. Use the observed Fisher information (the Hessian of the negative log-likelihood at the MLE) to produce an asymptotic variance estimate for the tail index estimator.\n- Formulate a Wald test for the null hypothesis that the pre-announcement and post-announcement tail indices are equal, at a specified significance level. Decide whether to reject the null hypothesis.\n- Implement careful parameter constraint handling for the GPD to ensure valid likelihood calculations.\n\nMathematical objects and constraints to respect:\n- Absolute returns are denoted by $x \\in \\mathbb{R}_{+}$.\n- For a chosen threshold $u$, exceedances are $y = x - u$ for $x > u$.\n- The Generalized Pareto Distribution (GPD) has shape (tail index) $\\xi$ and scale $\\beta$, with $\\beta &gt; 0$ and support defined by $1 + \\xi y / \\beta &gt; 0$.\n- The Maximum Likelihood Estimation (MLE) for $(\\xi,\\beta)$ must satisfy the GPD support constraints during optimization.\n- Use the observed Fisher information matrix computed as the Hessian of the negative log-likelihood evaluated at the MLE to approximate the variance of the tail index estimate $\\widehat{\\xi}$.\n- For the Wald test of $H_{0}: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$ vs $H_{1}: \\xi_{\\text{pre}} \\neq \\xi_{\\text{post}}$, use the statistic \n$$\nW = \\frac{\\left(\\widehat{\\xi}_{\\text{pre}} - \\widehat{\\xi}_{\\text{post}}\\right)^{2}}{\\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}}) + \\operatorname{Var}(\\widehat{\\xi}_{\\text{post}})},\n$$\nwhich is asymptotically $\\chi^{2}$ with $1$ degree of freedom under $H_{0}$. Reject $H_{0}$ when the upper-tail probability of the $\\chi^{2}$ distribution at $W$ is strictly less than the given significance level $\\alpha$.\n\nData generation for universality and testability:\n- For each segment, generate independent and identically distributed returns from a scaled symmetric Student distribution with degrees of freedom $\\nu$ and scale $s$, denoted $t_{\\nu}$, then multiply by $s$. Use absolute values of the simulated returns to define $x$.\n- No physical units are involved. Work purely with dimensionless numeric data.\n\nImplementation requirements:\n- For each case, use the given pseudorandom seed to ensure reproducibility.\n- For each segment, compute the empirical threshold $u$ as the sample quantile at level $q \\in (0,1)$ of $x$.\n- Estimate $(\\xi,\\beta)$ by MLE on exceedances $y = x - u$.\n- Compute the observed Fisher information via the Hessian of the negative log-likelihood at the MLE and use its inverse to approximate the variance of $\\widehat{\\xi}$.\n- Conduct the Wald test at level $\\alpha$ and return a boolean decision.\n\nTest suite:\nImplement your program to run the following five test cases. Each case defines the pre- and post-announcement data-generating process and test settings. For each case, output a boolean indicating whether you reject $H_{0}: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$ at the stated level $\\alpha$.\n\n- Case A (no change, moderate tails): seed $= 12345$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 8$, $\\nu_{\\text{post}} = 8$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.975$, $\\alpha = 0.05$.\n- Case B (clear change, heavier post tail): seed $= 202405$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 8$, $\\nu_{\\text{post}} = 3$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.975$, $\\alpha = 0.01$.\n- Case C (near thin tails, no change): seed $= 424242$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 1000$, $\\nu_{\\text{post}} = 1000$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.990$, $\\alpha = 0.05$.\n- Case D (few exceedances, very similar tails): seed $= 777777$, $N_{\\text{pre}} = 5000$, $N_{\\text{post}} = 5000$, $\\nu_{\\text{pre}} = 5.0$, $\\nu_{\\text{post}} = 5.1$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.990$, $\\alpha = 0.05$.\n- Case E (scale change only, no tail-index change): seed $= 314159$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 4$, $\\nu_{\\text{post}} = 4$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 2.0$, $q = 0.975$, $\\alpha = 0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five cases as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False,True]\". Each entry is a boolean corresponding to a case in the same order as listed above.", "solution": "The problem as stated is subjected to validation and is found to be valid. It is scientifically sound, resting on established principles of extreme value theory, maximum likelihood estimation, and statistical hypothesis testing. The problem is well-posed, with all necessary data and conditions specified for a unique, verifiable solution. The language is objective and formal. We may therefore proceed with the solution.\n\nThe problem requires a statistical test for a structural break in the tail behavior of financial return series. We are to use the Peaks-Over-Threshold (POT) framework, which is justified by the Pickands–Balkema–de Haan theorem. This theorem states that for a random variable $X$ with a sufficiently high threshold $u$, the distribution of exceedances $Y = X-u$ conditional on $X > u$ can be approximated by a Generalized Pareto Distribution (GPD).\n\nThe probability density function (PDF) of the GPD is given by:\n$$\nf(y; \\xi, \\beta) = \n\\begin{cases} \n\\frac{1}{\\beta} \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-(1/\\xi + 1)} & \\text{for } \\xi \\neq 0 \\\\\n\\frac{1}{\\beta} \\exp\\left(-\\frac{y}{\\beta}\\right) & \\text{for } \\xi = 0 \n\\end{cases}\n$$\nThe parameters are the shape parameter (or tail index) $\\xi \\in \\mathbb{R}$ and the scale parameter $\\beta > 0$. The support of the distribution is $y \\geq 0$ for $\\xi \\geq 0$, and $0 \\leq y \\leq -\\beta/\\xi$ for $\\xi < 0$. The parameter $\\xi$ governs the heaviness of the tail: $\\xi > 0$ corresponds to heavy (Pareto-like) tails, $\\xi = 0$ corresponds to medium (exponential-like) tails, and $\\xi < 0$ corresponds to light, bounded tails. For financial returns, we typically expect $\\xi > 0$. The theoretical tail index for a Student's t-distribution with $\\nu$ degrees of freedom is $\\xi = 1/\\nu$.\n\nOur first step is to estimate the parameters $(\\xi, \\beta)$ for the pre-announcement and post-announcement return series. We use the method of Maximum Likelihood Estimation (MLE). For a set of $k$ independent exceedances $y_1, y_2, \\ldots, y_k$, the log-likelihood function is $\\ell(\\xi, \\beta) = \\sum_{i=1}^k \\log f(y_i; \\xi, \\beta)$. The MLE estimates $(\\widehat{\\xi}, \\widehat{\\beta})$ are the values of $(\\xi, \\beta)$ that maximize this function. This is numerically equivalent to minimizing the negative log-likelihood:\n$$\n-\\ell(\\xi, \\beta) = \n\\begin{cases}\nk\\log\\beta + \\left(\\frac{1}{\\xi} + 1\\right)\\sum_{i=1}^k \\log\\left(1 + \\frac{\\xi y_i}{\\beta}\\right) & \\text{for } \\xi \\neq 0 \\\\\nk\\log\\beta + \\frac{1}{\\beta}\\sum_{i=1}^k y_i & \\text{for } \\xi = 0\n\\end{cases}\n$$\nThe optimization must respect the parameter constraints $\\beta > 0$ and $1 + \\xi y_i/\\beta > 0$ for all $i$. These constraints are enforced within the objective function provided to a numerical optimizer. If the parameters violate these conditions during an optimization step, the function returns a value representing infinity to steer the optimizer away from invalid regions.\n\nThe second step is to quantify the uncertainty of our estimate for the tail index, $\\widehat{\\xi}$. Under standard regularity conditions, the MLE $\\widehat{\\theta} = (\\widehat{\\xi}, \\widehat{\\beta})$ is asymptotically normally distributed with a mean equal to the true parameter value $\\theta$ and a covariance matrix given by the inverse of the Fisher information matrix, $\\mathcal{I}(\\theta)^{-1}$. We approximate $\\mathcal{I}(\\theta)$ with the observed Fisher information, $J(\\widehat{\\theta})$, which is the Hessian of the negative log-likelihood function evaluated at the MLE:\n$$\nJ(\\widehat{\\theta}) = -\\left. \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta \\partial \\theta^T} \\right|_{\\theta=\\widehat{\\theta}} = \\left. \\nabla^2 (-\\ell(\\theta)) \\right|_{\\theta=\\widehat{\\theta}}\n$$\nThis Hessian matrix is computed numerically using a second-order central difference formula. The asymptotic covariance matrix is then $\\operatorname{Cov}(\\widehat{\\theta}) \\approx J(\\widehat{\\theta})^{-1}$. The variance of the tail index estimator, $\\operatorname{Var}(\\widehat{\\xi})$, is the first diagonal element of this inverted matrix.\n\nThe third and final step is to perform a Wald test for the null hypothesis $H_0: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$ against the alternative $H_1: \\xi_{\\text{pre}} \\neq \\xi_{\\text{post}}$. Since the pre- and post-announcement samples are independent, their respective MLEs are also independent. The variance of the difference of the estimators is the sum of their variances: $\\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}} - \\widehat{\\xi}_{\\text{post}}) = \\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}}) + \\operatorname{Var}(\\widehat{\\xi}_{\\text{post}})$. The Wald test statistic is:\n$$\nW = \\frac{(\\widehat{\\xi}_{\\text{pre}} - \\widehat{\\xi}_{\\text{post}})^2}{\\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}}) + \\operatorname{Var}(\\widehat{\\xi}_{\\text{post}})}\n$$\nUnder the null hypothesis, $W$ asymptotically follows a chi-squared distribution with $1$ degree of freedom, $W \\\n\\xrightarrow{d} \\chi^2(1)$. We compute the p-value, which is the probability of observing a test statistic as extreme or more extreme than $W$ under $H_0$. This is given by $P(\\chi^2(1) \\geq W)$. We reject the null hypothesis at a significance level $\\alpha$ if this p-value is strictly less than $\\alpha$.\n\nThe complete algorithm proceeds as follows for each test case:\n1.  Generate absolute returns from the specified scaled Student's t-distributions for the pre- and post-announcement periods using the provided random seed.\n2.  For each period:\n    a. Determine the threshold $u$ as the empirical quantile of the absolute returns at the specified level $q$.\n    b. Identify all exceedances $y = x - u$ for data points $x > u$.\n    c. Perform MLE to obtain estimates $(\\widehat{\\xi}, \\widehat{\\beta})$.\n    d. Numerically compute the Hessian of the negative log-likelihood at the MLE.\n    e. Invert the Hessian to find the covariance matrix and extract the variance of the tail index estimator, $\\operatorname{Var}(\\widehat{\\xi})$.\n3.  Use the estimates $\\widehat{\\xi}_{\\text{pre}}, \\widehat{\\xi}_{\\text{post}}$ and their variances to compute the Wald statistic $W$.\n4.  Calculate the p-value from the $\\chi^2(1)$ distribution.\n5.  Return `True` if the p-value is less than the significance level $\\alpha$, indicating rejection of $H_0$, and `False` otherwise.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.stats import t as student_t\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for changes in tail behavior.\n    \"\"\"\n\n    def _gpd_neg_log_likelihood(params, y, epsilon=1e-8):\n        \"\"\"\n        Computes the negative log-likelihood for the Generalized Pareto Distribution.\n        Handles parameter constraints by returning infinity.\n        \"\"\"\n        xi, beta = params\n        k = len(y)\n\n        # Constraint: beta > 0\n        if beta <= 0:\n            return np.inf\n\n        # Constraint: 1 + xi * y / beta > 0\n        try:\n            with np.errstate(divide='ignore', invalid='ignore'):\n                term = 1 + xi * y / beta\n            if np.any(term <= 0):\n                return np.inf\n        except (FloatingPointError, ValueError):\n            return np.inf\n\n        # Case: xi is close to 0 (Exponential distribution)\n        if abs(xi) < epsilon:\n            return k * np.log(beta) + np.sum(y) / beta\n\n        # Case: xi is not 0\n        log_term = np.log(term)\n        return k * np.log(beta) + (1 / xi + 1) * np.sum(log_term)\n\n    def _calculate_hessian(func, params, args, h=1e-5):\n        \"\"\"\n        Numerically calculates the Hessian matrix of a function using central differences.\n        \"\"\"\n        n_params = len(params)\n        hessian = np.zeros((n_params, n_params))\n        \n        for i in range(n_params):\n            for j in range(i, n_params):\n                # Diagonal elements\n                if i == j:\n                    p_plus = np.copy(params)\n                    p_plus[i] += h\n                    p_minus = np.copy(params)\n                    p_minus[i] -= h\n                    f_0 = func(params, *args)\n                    f_plus = func(p_plus, *args)\n                    f_minus = func(p_minus, *args)\n                    hessian[i, i] = (f_plus - 2 * f_0 + f_minus) / (h * h)\n                # Off-diagonal elements\n                else:\n                    p_pp = np.copy(params); p_pp[i] += h; p_pp[j] += h\n                    p_pm = np.copy(params); p_pm[i] += h; p_pm[j] -= h\n                    p_mp = np.copy(params); p_mp[i] -= h; p_mp[j] += h\n                    p_mm = np.copy(params); p_mm[i] -= h; p_mm[j] -= h\n                    \n                    f_pp = func(p_pp, *args)\n                    f_pm = func(p_pm, *args)\n                    f_mp = func(p_mp, *args)\n                    f_mm = func(p_mm, *args)\n\n                    val = (f_pp - f_pm - f_mp + f_mm) / (4 * h * h)\n                    hessian[i, j] = val\n                    hessian[j, i] = val\n        return hessian\n\n    def analyze_segment(returns, q):\n        \"\"\"\n        Analyzes a single segment of returns to estimate GPD parameters and tail index variance.\n        \"\"\"\n        u = np.quantile(returns, q)\n        exceedances = returns[returns > u] - u\n\n        if len(exceedances) == 0:\n            return np.nan, np.nan\n\n        # Initial guess for optimization\n        beta_init = np.mean(exceedances)\n        xi_init = 0.1  # A small positive value, common for financial data.\n        initial_params = [xi_init, beta_init]\n\n        # Perform MLE\n        res = optimize.minimize(\n            _gpd_neg_log_likelihood,\n            initial_params,\n            args=(exceedances,),\n            method='Nelder-Mead',\n            options={'maxiter': 2000, 'adaptive': True}\n        )\n\n        if not res.success:\n            # Fallback to L-BFGS-B if Nelder-Mead fails\n            res = optimize.minimize(\n                _gpd_neg_log_likelihood,\n                initial_params,\n                args=(exceedances,),\n                method='L-BFGS-B',\n                bounds=((-0.5, 1.5), (1e-6, None))\n            )\n            if not res.success:\n                return np.nan, np.nan\n        \n        mle_params = res.x\n        xi_hat, _ = mle_params\n\n        # Compute observed Fisher information (Hessian of neg-log-likelihood)\n        try:\n            hessian = _calculate_hessian(_gpd_neg_log_likelihood, mle_params, args=(exceedances,))\n            cov_matrix = np.linalg.inv(hessian)\n            var_xi = cov_matrix[0, 0]\n            # Ensure variance is non-negative\n            if var_xi < 0:\n                return xi_hat, np.nan\n        except (np.linalg.LinAlgError, ValueError):\n            return xi_hat, np.nan\n\n        return xi_hat, var_xi\n\n    def run_one_case(case_params):\n        \"\"\"\n        Runs one full test case for the Wald test.\n        \"\"\"\n        seed, N_pre, N_post, nu_pre, nu_post, s_pre, s_post, q, alpha = case_params\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        returns_pre = np.abs(student_t.rvs(df=nu_pre, scale=s_pre, size=N_pre, random_state=rng))\n        returns_post = np.abs(student_t.rvs(df=nu_post, scale=s_post, size=N_post, random_state=rng))\n\n        # Analyze each segment\n        xi_pre, var_xi_pre = analyze_segment(returns_pre, q)\n        xi_post, var_xi_post = analyze_segment(returns_post, q)\n        \n        if np.isnan(xi_pre) or np.isnan(var_xi_pre) or np.isnan(xi_post) or np.isnan(var_xi_post):\n            return False # Cannot reject if estimation fails\n\n        # Wald test\n        numerator = (xi_pre - xi_post)**2\n        denominator = var_xi_pre + var_xi_post\n\n        if denominator <= 0:\n            return False # Cannot perform test if total variance is not positive\n\n        W = numerator / denominator\n        p_value = chi2.sf(W, df=1)\n\n        return p_value < alpha\n\n    test_cases = [\n        # (seed, N_pre, N_post, nu_pre, nu_post, s_pre, s_post, q, alpha)\n        (12345, 10000, 10000, 8, 8, 1.0, 1.0, 0.975, 0.05),\n        (202405, 10000, 10000, 8, 3, 1.0, 1.0, 0.975, 0.01),\n        (424242, 10000, 10000, 1000, 1000, 1.0, 1.0, 0.990, 0.05),\n        (777777, 5000, 5000, 5.0, 5.1, 1.0, 1.0, 0.990, 0.05),\n        (314159, 10000, 10000, 4, 4, 1.0, 2.0, 0.975, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = run_one_case(case)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Financial crises are often defined by the simultaneous occurrence of multiple extreme events, a phenomenon that univariate models cannot capture. This final practice [@problem_id:2391764] introduces you to the world of multivariate extreme value theory by using copulas to link the tail behavior of different risk factors, such as oil prices and stock markets. By implementing a bivariate model, you will learn how to compute the probability of joint extreme events, providing a more holistic and realistic assessment of portfolio and systemic risk.", "id": "2391764", "problem": "You are tasked with implementing a program that computes the joint tail probability of two financial risk events using multivariate extreme value theory. Let $X$ denote the one-day oil spot price jump in United States Dollar (USD) per barrel (right tail, large positive moves), and let $Y$ denote the one-day equity index loss magnitude in percentage points (right tail, large losses expressed as absolute percent points, not with a percentage sign). The objective is to compute, for specified thresholds $x^\\star$ (in USD) and $y^\\star$ (in percentage points), the probability $\\mathbb{P}(X > x^\\star, Y > y^\\star)$ under the following model assumptions.\n\nModel specification:\n- Marginal tails: For $X$ and $Y$, use threshold exceedance models with the Generalized Pareto Distribution (GPD) beyond fixed thresholds $u_X$ and $u_Y$ respectively. For $X$, define the excess $Z_X = X - u_X$ given $X > u_X$. For $Y$, define the excess $Z_Y = Y - u_Y$ given $Y > u_Y$. The conditional survival function for a GPD excess is\n  $$ \\mathbb{P}(Z > z \\mid Z \\ge 0) = \\begin{cases}\n  \\left(1 + \\dfrac{\\xi z}{\\beta}\\right)^{-1/\\xi}, & \\xi \\ne 0, \\, 1 + \\dfrac{\\xi z}{\\beta} > 0, \\\\\n  \\exp\\!\\left(-\\dfrac{z}{\\beta}\\right), & \\xi = 0,\n  \\end{cases} $$\n  where $\\beta > 0$ is the scale parameter and $\\xi$ is the shape parameter. For $\\xi < 0$, the support is bounded above by $z < -\\beta/\\xi$; beyond this bound, the tail probability is zero. The unconditional tail probability at any $t \\ge u$ is\n  $$ \\mathbb{P}(X > t) = \\mathbb{P}(X > u_X) \\cdot \\mathbb{P}(Z_X > t - u_X \\mid X > u_X), \\quad \\mathbb{P}(Y > t) = \\mathbb{P}(Y > u_Y) \\cdot \\mathbb{P}(Z_Y > t - u_Y \\mid Y > u_Y). $$\n  The quantities $\\mathbb{P}(X > u_X)$ and $\\mathbb{P}(Y > u_Y)$ are given as inputs.\n- Dependence structure: The joint distribution of $(X,Y)$ is specified by an extreme value copula of Gumbel–Hougaard form with parameter $\\theta \\ge 1$:\n  $$ C(u,v) = \\exp\\!\\left(-\\left((-\\ln u)^{\\theta} + (-\\ln v)^{\\theta}\\right)^{1/\\theta}\\right), \\quad u,v \\in (0,1]. $$\n- Joint tail probability: For any thresholds $x^\\star \\ge u_X$ and $y^\\star \\ge u_Y$, with marginal distribution functions $F_X$ and $F_Y$, the joint tail probability satisfies\n  $$ \\mathbb{P}(X > x^\\star, Y > y^\\star) = 1 - F_X(x^\\star) - F_Y(y^\\star) + C(F_X(x^\\star), F_Y(y^\\star)). $$\n\nGlobal thresholds:\n- Oil price jump threshold $u_X = 10$ USD.\n- Equity loss threshold $u_Y = 2$ percentage points.\n\nYour task:\n- For each test case below, compute the value of $\\mathbb{P}(X > x^\\star, Y > y^\\star)$ as a decimal number.\n- If a threshold $t$ exceeds the finite upper endpoint implied by a negative shape parameter $\\xi < 0$ (i.e., $t - u \\ge -\\beta/\\xi$), the corresponding marginal tail probability is zero.\n- All results must be expressed as decimals (no percentage sign) and rounded to six decimal places.\n\nTest suite (each line specifies $(\\beta_X, \\xi_X, p_{X0}, \\beta_Y, \\xi_Y, p_{Y0}, \\theta, x^\\star, y^\\star)$, where $p_{X0} = \\mathbb{P}(X > u_X)$ and $p_{Y0} = \\mathbb{P}(Y > u_Y)$):\n1. $(\\beta_X = 8, \\, \\xi_X = 0.2, \\, p_{X0} = 0.05, \\, \\beta_Y = 1.5, \\, \\xi_Y = 0.1, \\, p_{Y0} = 0.10, \\, \\theta = 2, \\, x^\\star = 20, \\, y^\\star = 5)$\n2. $(\\beta_X = 5, \\, \\xi_X = 0, \\, p_{X0} = 0.02, \\, \\beta_Y = 1.2, \\, \\xi_Y = 0.2, \\, p_{Y0} = 0.04, \\, \\theta = 1, \\, x^\\star = 25, \\, y^\\star = 3)$\n3. $(\\beta_X = 10, \\, \\xi_X = 0.3, \\, p_{X0} = 0.03, \\, \\beta_Y = 2.0, \\, \\xi_Y = 0.3, \\, p_{Y0} = 0.08, \\, \\theta = 10, \\, x^\\star = 10, \\, y^\\star = 2)$\n4. $(\\beta_X = 6, \\, \\xi_X = 0.2, \\, p_{X0} = 0.04, \\, \\beta_Y = 1.5, \\, \\xi_Y = -0.2, \\, p_{Y0} = 0.03, \\, \\theta = 1.5, \\, x^\\star = 20, \\, y^\\star = 10)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4]\"), in the same order as the test cases. Each $r_i$ must be a float rounded to six decimal places.", "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded in the established principles of multivariate extreme value theory, a standard methodology in computational finance for risk management. The problem is well-posed, providing all necessary definitions, parameters, and a clear, objective computational goal. It is self-contained and free of contradictions or ambiguities. Therefore, a solution will be provided.\n\nThe objective is to compute the joint tail probability $\\mathbb{P}(X > x^\\star, Y > y^\\star)$ for two random variables, $X$ and $Y$, representing financial risk events. The problem specifies a model for the marginal tails and their dependence structure. The solution is derived by systematically applying the provided formulas.\n\nThe fundamental formula for the joint tail probability, based on the inclusion-exclusion principle and the definition of a copula, is given as:\n$$ \\mathbb{P}(X > x^\\star, Y > y^\\star) = 1 - F_X(x^\\star) - F_Y(y^\\star) + C(F_X(x^\\star), F_Y(y^\\star)) $$\nwhere $F_X$ and $F_Y$ are the marginal cumulative distribution functions (CDFs) of $X$ and $Y$, respectively, and $C(u,v)$ is their copula.\n\nThe process to compute this probability consists of three main steps:\n1.  Compute the marginal CDF value for $X$, $F_X(x^\\star)$.\n2.  Compute the marginal CDF value for $Y$, $F_Y(y^\\star)$.\n3.  Substitute these values into the copula function and then into the main joint probability formula.\n\n**Step 1 & 2: Calculation of Marginal CDFs**\n\nThe CDF is related to the survival function (or tail probability) by $F(t) = 1 - \\mathbb{P}(X > t)$. The problem provides a model for the unconditional tail probability for any threshold $t$ greater than or equal to a high threshold $u$. For variable $X$, this is:\n$$ \\mathbb{P}(X > t) = \\mathbb{P}(X > u_X) \\cdot \\mathbb{P}(Z_X > t - u_X \\mid X > u_X) $$\nwhere $u_X = 10$ is the fixed threshold for $X$. Let $p_{X0} = \\mathbb{P}(X > u_X)$ be the given probability of exceeding this threshold. The term $\\mathbb{P}(Z_X > t - u_X \\mid X > u_X)$ is the survival function of the excess $Z_X = X - u_X$ over the threshold $u_X$. This excess is modeled by a Generalized Pareto Distribution (GPD).\n\nThe survival function for a GPD-distributed random variable $Z$ with scale parameter $\\beta > 0$ and shape parameter $\\xi$ is given for an excess $z > 0$ as:\n$$ S_{GPD}(z; \\beta, \\xi) = \\mathbb{P}(Z > z \\mid Z \\ge 0) = \\begin{cases}\n\\left(1 + \\dfrac{\\xi z}{\\beta}\\right)^{-1/\\xi}, & \\text{if } \\xi \\ne 0 \\\\\n\\exp\\left(-\\dfrac{z}{\\beta}\\right), & \\text{if } \\xi = 0\n\\end{cases} $$\nThis formula is valid for $z$ such that $1 + \\xi z / \\beta > 0$. If $\\xi < 0$, the distribution has a finite upper endpoint at $z_{max} = -\\beta/\\xi$. For any excess $z \\ge z_{max}$, the survival probability is $0$.\n\nThus, for a given threshold $x^\\star \\ge u_X$, we first calculate the excess $z_X = x^\\star - u_X$. Then we find the conditional survival probability $S_{GPD}(z_X; \\beta_X, \\xi_X)$. The unconditional tail probability for $X$ is:\n$$ \\mathbb{P}(X > x^\\star) = p_{X0} \\cdot S_{GPD}(z_X; \\beta_X, \\xi_X) $$\nFinally, the marginal CDF value is $F_X(x^\\star) = 1 - \\mathbb{P}(X > x^\\star)$.\n\nAn identical procedure is followed for variable $Y$, using its specific parameters $(\\beta_Y, \\xi_Y, p_{Y0})$ and threshold $y^\\star$ relative to $u_Y = 2$.\n\n**Step 3: Calculation of Joint Probability**\n\nThe dependence between the marginal distributions is modeled by the Gumbel-Hougaard copula with parameter $\\theta \\ge 1$:\n$$ C(u,v) = \\exp\\left(-\\left((-\\ln u)^{\\theta} + (-\\ln v)^{\\theta}\\right)^{1/\\theta}\\right) $$\nwhere $u = F_X(x^\\star)$ and $v = F_Y(y^\\star)$ are the marginal CDF values computed in the previous steps.\n\nThe algorithm to solve for each test case is as follows:\n1.  For a given set of parameters $(\\beta_X, \\xi_X, p_{X0}, \\beta_Y, \\xi_Y, p_{Y0}, \\theta, x^\\star, y^\\star)$ and global thresholds $u_X=10, u_Y=2$:\n2.  Calculate the excess for $X$: $z_X = x^\\star - u_X$.\n3.  Calculate the GPD survival probability for $z_X$, $S_{GPD,X}$, accounting for the cases $\\xi_X = 0$ and $\\xi_X < 0$.\n4.  Calculate the unconditional tail probability for $X$: $S_X = p_{X0} \\cdot S_{GPD,X}$.\n5.  Calculate the marginal CDF for $X$: $F_X = 1 - S_X$.\n6.  Repeat steps 2-5 for variable $Y$ to find $F_Y = 1 - p_{Y0} \\cdot S_{GPD}(y^\\star - u_Y; \\beta_Y, \\xi_Y)$.\n7.  If either $F_X=1$ or $F_Y=1$, the joint tail probability $\\mathbb{P}(X > x^\\star, Y > y^\\star)$ is $0$, as the event $(X > x^\\star, Y > y^\\star)$ is a subset of an event with probability zero.\n8.  Otherwise, calculate the copula value $C(F_X, F_Y)$ using the Gumbel-Hougaard formula.\n9.  Compute the final joint tail probability: $P_{joint} = 1 - F_X - F_Y + C(F_X, F_Y)$.\n10. Round the result to six decimal places as required.\n\nThis procedure rigorously integrates the specified models for marginal tails and dependence to arrive at the desired risk measure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the joint tail probability of two financial risk events\n    using multivariate extreme value theory.\n    \"\"\"\n\n    # Global thresholds for the models.\n    u_X = 10.0  # USD\n    u_Y = 2.0   # percentage points\n\n    # Test cases: (beta_X, xi_X, p_X0, beta_Y, xi_Y, p_Y0, theta, x_star, y_star)\n    test_cases = [\n        (8.0, 0.2, 0.05, 1.5, 0.1, 0.10, 2.0, 20.0, 5.0),\n        (5.0, 0.0, 0.02, 1.2, 0.2, 0.04, 1.0, 25.0, 3.0),\n        (10.0, 0.3, 0.03, 2.0, 0.3, 0.08, 10.0, 10.0, 2.0),\n        (6.0, 0.2, 0.04, 1.5, -0.2, 0.03, 1.5, 20.0, 10.0),\n    ]\n\n    def gpd_survival(z, beta, xi):\n        \"\"\"\n        Calculates the survival function P(Z > z) for a GPD.\n        \"\"\"\n        # Excess z must be non-negative as z = t - u, and problem states t >= u.\n        if z < 0:\n            return 1.0\n\n        # Case 1: xi < 0 (distribution has a finite upper endpoint)\n        if xi < 0:\n            upper_endpoint = -beta / xi\n            if z >= upper_endpoint:\n                return 0.0\n        \n        # Case 2: xi = 0 (Exponential distribution)\n        if xi == 0.0:\n            return np.exp(-z / beta)\n\n        # Case 3: xi != 0 (General GPD case)\n        # The check for xi < 0 above ensures base > 0 for valid z.\n        base = 1.0 + xi * z / beta\n        return base**(-1.0 / xi)\n\n    results = []\n    for case in test_cases:\n        beta_X, xi_X, p_X0, beta_Y, xi_Y, p_Y0, theta, x_star, y_star = case\n\n        # --- Calculate marginal probability for X ---\n        if x_star < u_X:\n            # Per problem, x_star >= u_X, but handle for robustness\n            S_X = 1.0\n        else:\n            z_X = x_star - u_X\n            cond_surv_X = gpd_survival(z_X, beta_X, xi_X)\n            S_X = p_X0 * cond_surv_X\n        F_X = 1.0 - S_X\n\n        # --- Calculate marginal probability for Y ---\n        if y_star < u_Y:\n            # Per problem, y_star >= u_Y, but handle for robustness\n            S_Y = 1.0\n        else:\n            z_Y = y_star - u_Y\n            cond_surv_Y = gpd_survival(z_Y, beta_Y, xi_Y)\n            S_Y = p_Y0 * cond_surv_Y\n        F_Y = 1.0 - S_Y\n\n        # If one of the marginal tail probabilities is zero, the joint probability is zero.\n        if S_X == 0.0 or S_Y == 0.0:\n            joint_tail_prob = 0.0\n        else:\n            # --- Calculate Gumbel-Hougaard copula value ---\n            # Handles special cases of independence (theta=1) and comonotonicity (theta->inf)\n            term_u = (-np.log(F_X))**theta\n            term_v = (-np.log(F_Y))**theta\n            copula_val = np.exp(-((term_u + term_v)**(1.0 / theta)))\n\n            # --- Calculate joint tail probability ---\n            # P(X > x, Y > y) = 1 - F_X(x) - F_Y(y) + C(F_X(x), F_Y(y))\n            joint_tail_prob = 1.0 - F_X - F_Y + copula_val\n        \n        results.append(round(joint_tail_prob, 6))\n\n    # Final print statement in the exact required format.\n    # The output format requires string representations of floats, rounded.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"}]}