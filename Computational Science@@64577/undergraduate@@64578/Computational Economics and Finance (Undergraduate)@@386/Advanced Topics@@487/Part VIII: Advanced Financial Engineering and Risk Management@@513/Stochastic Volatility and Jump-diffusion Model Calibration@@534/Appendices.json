{"hands_on_practices": [{"introduction": "To begin our journey into advanced financial models, we first isolate the impact of sudden market shocks, or \"jumps.\" This exercise provides a foundational understanding of how to estimate the core parameters of a jump process—its frequency ($\\lambda$) and average size ($\\mu_J$)—within the context of an idealized interest rate model. By working with a simplified, hypothetical scenario where the underlying short-rate changes are given, you will master the fundamental statistical logic for parameter estimation before tackling more complex, real-world data [@problem_id:2370067].", "id": "2370067", "problem": "You are given a one-factor affine short rate model with jumps under the risk-neutral measure, where the short rate $r_t$ follows the Stochastic Differential Equation (SDE)\n$$\ndr_t \\;=\\; \\kappa \\,(\\theta - r_t)\\,dt \\;+\\; \\sigma\\, dW_t \\;+\\; J\\, dN_t,\n$$\nwith mean-reversion speed $\\kappa &gt; 0$, long-run level $\\theta$, volatility $\\sigma \\ge 0$, standard Wiener process $W_t$, and a Poisson jump process $N_t$ with constant intensity $\\lambda \\ge 0$. The jump size $J$ is deterministic and equal to a constant $\\mu_J$ (in decimal rate units per year). For continuously compounded zero-coupon yields $y(t,\\tau)$ at maturity $\\tau$, the model implies an affine relation\n$$\ny(t,\\tau) \\;=\\; -\\frac{A(\\tau)}{\\tau} \\;+\\; \\frac{B(\\tau)}{\\tau}\\, r_t,\n$$\nwhere $B(\\tau) \\;=\\; \\frac{1 - e^{-\\kappa \\tau}}{\\kappa}$ and $A(\\tau)$ is the standard Vasicek function. Over a very short interval of length $\\Delta t$, assume that the change in yield at maturity $\\tau$ satisfies\n$$\n\\Delta y(t,\\tau) \\;=\\; \\frac{B(\\tau)}{\\tau} \\, \\Delta r_t,\n$$\nwith $\\Delta r_t$ denoting the change of the short rate over the interval. In each interval, assume that at most one jump can occur, the diffusion and drift contributions are negligible relative to jumps, and the jump size is deterministic $\\mu_J$. Thus, $\\Delta r_t$ is either $0$ (no jump) or $\\mu_J$ (one jump). Let $p = \\lambda \\Delta t$ denote the probability of a jump in a single interval.\n\nYour task is to calibrate the jump intensity $\\lambda$ (per year) and the deterministic jump magnitude $\\mu_J$ (decimal rate per year) from cross-sectional yield changes observed over $K$ daily intervals that include a major market shock. Use the known relation between $\\Delta y$ and $\\Delta r$ above and the model assumptions described to infer $\\lambda$ and $\\mu_J$ from the data.\n\nUse the following fixed parameters and conventions:\n- Maturities (in years): $\\{\\tau_i\\}_{i=1}^6 = \\{0.25,\\, 0.5,\\, 1.0,\\, 2.0,\\, 5.0,\\, 10.0\\}$.\n- Mean-reversion speed: $\\kappa = 0.3$ (per year).\n- Interval length: $\\Delta t = \\frac{1}{252}$ (years).\n- For each interval $k$, the loadings are $L(\\tau_i) = \\frac{B(\\tau_i)}{\\tau_i}$ with $B(\\tau) = \\frac{1 - e^{-\\kappa \\tau}}{\\kappa}$.\n- For each dataset, the observed yield change vectors $\\{\\Delta y_k(\\tau_i)\\}_{i=1}^6$ are generated by $\\Delta y_k(\\tau_i) = L(\\tau_i)\\,\\Delta r_k$ from specified short-rate increments $\\{\\Delta r_k\\}$.\n\nTest Suite (three datasets covering distinct scenarios):\n1. Dataset $\\mathbf{1}$ (shock within $K=5$ days): short-rate increments $\\{\\Delta r_k\\}_{k=1}^5 = [0.0,\\, 0.0,\\, -0.015,\\, 0.0,\\, 0.0]$.\n2. Dataset $\\mathbf{2}$ (no changes over $K=4$ days): short-rate increments $\\{\\Delta r_k\\}_{k=1}^4 = [0.0,\\, 0.0,\\, 0.0,\\, 0.0]$.\n3. Dataset $\\mathbf{3}$ (single-day extreme shock, $K=1$): short-rate increments $\\{\\Delta r_k\\}_{k=1}^1 = [-0.02]$.\n\nFor each dataset:\n- Construct the yield change cross-sections $\\{\\Delta y_k(\\tau_i)\\}_{i=1}^6$ using the loadings $L(\\tau_i)$ and the given $\\Delta r_k$.\n- From these yield changes and the model relation, estimate the jump intensity $\\lambda$ (per year) and the deterministic jump magnitude $\\mu_J$ (decimal rate per year).\n\nYour program must produce a single line of output containing the results as a comma-separated list of three elements, where each element is itself a list of two floating-point numbers $[\\lambda,\\mu_J]$ for the corresponding dataset, both rounded to exactly six decimal places. The required final output format is:\n$$\n[[\\lambda_1,\\mu_{J,1}],[\\lambda_2,\\mu_{J,2}],[\\lambda_3,\\mu_{J,3}]]\n$$\nFor example, the output must look like $[[a,b],[c,d],[e,f]]$ with each of $a,b,c,d,e,f$ printed to six decimal places.", "solution": "The problem has been scrutinized and found to be valid. It is a well-defined exercise in parameter estimation for a specified stochastic model. The data generation process, where the underlying short-rate increments are provided directly, simplifies the estimation task into a direct application of statistical definitions. We shall proceed to the solution.\n\nThe short rate $r_t$ is modeled by the stochastic differential equation:\n$$\ndr_t = \\kappa (\\theta - r_t)dt + \\sigma dW_t + J dN_t\n$$\nwhere $\\kappa$ is the mean-reversion speed, $\\theta$ is the long-run mean, $\\sigma$ is the volatility, $W_t$ is a standard Wiener process, and $N_t$ is a Poisson process with constant intensity $\\lambda$. The jump size $J$ is a deterministic constant, denoted $\\mu_J$.\n\nThe problem states that for a short time interval $\\Delta t$, the change in the short rate, $\\Delta r_t$, can be approximated by considering only the jump component, as drift and diffusion are assumed negligible. Furthermore, at most one jump can occur in any interval $\\Delta t$. Therefore, the change in the short rate is binary:\n$$\n\\Delta r_t =\n\\begin{cases}\n\\mu_J & \\text{if a jump occurs} \\\\\n0 & \\text{if no jump occurs}\n\\end{cases}\n$$\nThe probability of a jump in a single interval is given as $p = \\lambda \\Delta t$.\n\nThe change in the zero-coupon yield $y(t,\\tau)$ for maturity $\\tau$ is linked to the short rate change by the affine relation:\n$$\n\\Delta y(t,\\tau) = \\frac{B(\\tau)}{\\tau} \\Delta r_t = L(\\tau) \\Delta r_t\n$$\nwhere the loading $L(\\tau) = B(\\tau)/\\tau$ is a function of maturity $\\tau$ and the parameter $\\kappa$.\n\nThe task is to estimate the jump intensity $\\lambda$ and jump magnitude $\\mu_J$ from given time series of short-rate increments $\\{\\Delta r_k\\}_{k=1}^K$ for three distinct datasets. The problem instructs to first construct yield changes and then estimate the parameters. However, since the underlying short-rate increments $\\Delta r_k$ are provided, and the relationship $\\Delta y_k(\\tau_i) = L(\\tau_i) \\Delta r_k$ is exact, any attempt to estimate $\\Delta r_k$ from the constructed $\\Delta y_k(\\tau_i)$ would simply recover the provided $\\Delta r_k$ values. Thus, the problem reduces to estimating $(\\lambda, \\mu_J)$ directly from the sequence $\\{\\Delta r_k\\}_{k=1}^K$.\n\nThe estimation logic is as follows:\n$1$. **Estimation of Jump Magnitude $\\mu_J$**: The jump magnitude $\\mu_J$ is deterministic. Any non-zero observed value in the sequence $\\{\\Delta r_k\\}$ must correspond to a jump and therefore must be equal to $\\mu_J$.\n    - If the number of non-zero increments is greater than zero, the estimate $\\hat{\\mu}_J$ is the unique value of these increments. The problem setup guarantees all non-zero increments within a dataset are identical, as only one jump event is simulated per dataset.\n    - If there are no non-zero increments (no jumps observed), the parameter $\\mu_J$ is not statistically identifiable from the data. In such a case, a standard and logical convention is to set its estimate to zero, $\\hat{\\mu}_J = 0$, as its value is irrelevant if jumps do not occur.\n\n$2$. **Estimation of Jump Intensity $\\lambda$**: The observation of a jump in any of the $K$ intervals constitutes a Bernoulli trial. Let $N_J$ be the total number of jumps observed, which is the count of non-zero elements in the sequence $\\{\\Delta r_k\\}_{k=1}^K$. The maximum likelihood estimator for the probability of a jump in a single interval, $p$, is the sample proportion:\n$$\n\\hat{p} = \\frac{N_J}{K}\n$$\nFrom the relationship $p = \\lambda \\Delta t$, the estimator for the jump intensity $\\lambda$ is:\n$$\n\\hat{\\lambda} = \\frac{\\hat{p}}{\\Delta t} = \\frac{N_J}{K \\Delta t}\n$$\nWe are given the fixed parameter $\\Delta t = 1/252$.\n\nWe now apply this methodology to each dataset.\n\n**Dataset 1:**\n- Short-rate increments: $\\{\\Delta r_k\\}_{k=1}^5 = [0.0, 0.0, -0.015, 0.0, 0.0]$.\n- Number of intervals: $K = 5$.\n- Number of observed jumps: $N_J = 1$. The non-zero increment is $-0.015$.\n- Estimation of $\\mu_J$: From the single observed jump, we directly identify the jump magnitude.\n$$\n\\hat{\\mu}_{J,1} = -0.015\n$$\n- Estimation of $\\lambda$:\n$$\n\\hat{\\lambda}_1 = \\frac{N_J}{K \\Delta t} = \\frac{1}{5 \\times (1/252)} = \\frac{252}{5} = 50.4\n$$\n\n**Dataset 2:**\n- Short-rate increments: $\\{\\Delta r_k\\}_{k=1}^4 = [0.0, 0.0, 0.0, 0.0]$.\n- Number of intervals: $K = 4$.\n- Number of observed jumps: $N_J = 0$.\n- Estimation of $\\mu_J$: No jumps are observed, so $\\mu_J$ is not identifiable. We apply the convention.\n$$\n\\hat{\\mu}_{J,2} = 0.0\n$$\n- Estimation of $\\lambda$:\n$$\n\\hat{\\lambda}_2 = \\frac{N_J}{K \\Delta t} = \\frac{0}{4 \\times (1/252)} = 0.0\n$$\n\n**Dataset 3:**\n- Short-rate increments: $\\{\\Delta r_k\\}_{k=1}^1 = [-0.02]$.\n- Number of intervals: $K = 1$.\n- Number of observed jumps: $N_J = 1$. The non-zero increment is $-0.02$.\n- Estimation of $\\mu_J$:\n$$\n\\hat{\\mu}_{J,3} = -0.02\n$$\n- Estimation of $\\lambda$:\n$$\n\\hat{\\lambda}_3 = \\frac{N_J}{K \\Delta t} = \\frac{1}{1 \\times (1/252)} = 252.0\n$$\n\nIn summary, the estimated parameters are:\n- For Dataset $1$: $(\\hat{\\lambda}_1, \\hat{\\mu}_{J,1}) = (50.4, -0.015)$\n- For Dataset $2$: $(\\hat{\\lambda}_2, \\hat{\\mu}_{J,2}) = (0.0, 0.0)$\n- For Dataset $3$: $(\\hat{\\lambda}_3, \\hat{\\mu}_{J,3}) = (252.0, -0.02)$\n\nThese results will be formatted to six decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Estimates jump intensity (lambda) and jump magnitude (mu_J) for a one-factor\n    affine short-rate model with jumps, based on provided short-rate increments.\n    \"\"\"\n    # Define fixed parameters from the problem statement.\n    delta_t = 1.0 / 252.0\n\n    # Test suite: three datasets of short-rate increments {delta_r_k}.\n    test_cases = [\n        [0.0, 0.0, -0.015, 0.0, 0.0],  # Dataset 1: K=5, one jump\n        [0.0, 0.0, 0.0, 0.0],         # Dataset 2: K=4, no jumps\n        [-0.02]                       # Dataset 3: K=1, one jump\n    ]\n\n    results = []\n    for dr_increments in test_cases:\n        # K is the total number of observation intervals in the dataset.\n        K = len(dr_increments)\n        \n        # We use a numpy array for efficient vectorized operations.\n        dr_array = np.array(dr_increments)\n        \n        # N_J is the number of observed jumps, counted as non-zero increments.\n        num_jumps = np.count_nonzero(dr_array)\n        \n        # Estimate lambda using the maximum likelihood estimator for a sequence of Bernoulli trials.\n        # lambda_hat = (N_J / K) / delta_t\n        if K > 0:\n            lambda_est = num_jumps / (K * delta_t)\n        else:\n            # This case will not be reached with the given test data.\n            lambda_est = 0.0\n\n        # Estimate mu_J, the deterministic jump magnitude.\n        if num_jumps > 0:\n            # If jumps are observed, mu_J is identified as the magnitude of these jumps.\n            # As mu_J is deterministic, all non-zero increments must be identical.\n            # We can take the first non-zero element.\n            mu_j_est = dr_array[dr_array != 0][0]\n        else:\n            # If no jumps are observed, mu_J is not statistically identifiable.\n            # We follow the standard convention of setting its estimate to 0,\n            # as its value is irrelevant in this scenario.\n            mu_j_est = 0.0\n            \n        results.append([lambda_est, mu_j_est])\n\n    # Format the final results into the required string format.\n    # The output is a list of lists, e.g., [[lambda1,mu_J1],[lambda2,mu_J2],...],\n    # with each float formatted to exactly six decimal places.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # The final print statement must produce only the required single-line output.\n    print(final_output_string)\n\nsolve()\n```"}, {"introduction": "Building on the concept of jumps, we now apply it to equity option pricing using the seminal Merton jump-diffusion model. This practice challenges you to price options where the underlying asset can experience sudden, large movements, a feature absent in the Black-Scholes world. Crucially, you will also implement Tikhonov regularization, a powerful technique to ensure your calibration routine produces stable and economically sensible parameters even when faced with the sparse option data common in real markets [@problem_id:2434399].", "id": "2434399", "problem": "You are given a European call option pricing model under a jump-diffusion with stochastic jumps, specifically the Merton jump-diffusion model. Under the risk-neutral measure, the underlying asset price process $\\{S_t\\}_{t \\ge 0}$ follows the stochastic differential equation\n$$\n\\frac{\\,\\mathrm{d}S_t\\,}{S_{t^-}} \\;=\\; \\bigl(r - q - \\lambda k\\bigr)\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_t \\;+\\; (J - 1)\\,\\mathrm{d}N_t,\n$$\nwhere $r$ is the continuously compounded risk-free rate (expressed as a decimal), $q$ is the continuously compounded dividend yield (expressed as a decimal), $\\sigma$ is the diffusive volatility (expressed as a decimal), $\\lambda$ is the jump intensity (events per unit time), $N_t$ is a Poisson process with intensity $\\lambda$, and $J$ is the jump multiplier with $\\ln J \\sim \\mathcal{N}(\\mu_J, \\delta_J^2)$. The jump compensator is $k \\equiv \\mathbb{E}[J - 1] = \\exp\\!\\bigl(\\mu_J + \\tfrac{1}{2}\\delta_J^2\\bigr) - 1$. All rates and volatilities must be treated as decimals, not percentages.\n\nFor a European call option with spot $S$, strike $K$, maturity $T$ (in years), risk-free rate $r$, and dividend yield $q$, the no-arbitrage price under this model can be written as the Poisson mixture of Black–Scholes prices:\n$$\nC_{\\text{MJD}}(S,K,T;r,q;\\sigma,\\lambda,\\mu_J,\\delta_J) \\;=\\;\n\\sum_{n=0}^{\\infty} \\mathrm{e}^{-\\lambda T} \\frac{(\\lambda T)^n}{n!}\\;\nC_{\\text{BS}}\\!\\Bigl(S \\mathrm{e}^{n\\mu_J},K,T;\\; r - \\lambda k,\\; q,\\; \\sqrt{\\sigma^2 + \\tfrac{n\\,\\delta_J^2}{T}} \\Bigr),\n$$\nwhere $C_{\\text{BS}}$ is the Black–Scholes–Merton price with continuous dividend yield $q$, and $k = \\exp(\\mu_J + \\tfrac{1}{2}\\delta_J^2) - 1$. The Black–Scholes–Merton call price satisfies\n$$\nC_{\\text{BS}}(S,K,T;\\, r, q, \\sigma)\n=\nS\\,\\mathrm{e}^{-qT}\\,\\Phi(d_1) - K\\,\\mathrm{e}^{-rT}\\,\\Phi(d_2),\n\\quad\nd_1 = \\frac{\\ln(S/K) + (r - q + \\tfrac{1}{2}\\sigma^2)T}{\\sigma\\sqrt{T}},\n\\quad\nd_2 = d_1 - \\sigma\\sqrt{T},\n$$\nwith $\\Phi$ denoting the standard normal cumulative distribution function.\n\nCalibration is posed as a regularized nonlinear least-squares problem with Tikhonov regularization. For a given set of observed market option prices $\\{C_i^{\\text{mkt}}\\}_{i=1}^M$ at strikes $\\{K_i\\}_{i=1}^M$ and common maturity $T$, define the parameter vector $\\boldsymbol{\\theta} = [\\sigma,\\lambda,\\mu_J,\\delta_J]^\\top$ and a prior (reference) vector $\\boldsymbol{\\theta}_0$. The calibration problem is\n$$\n\\min_{\\boldsymbol{\\theta}} \\;\\; \\sum_{i=1}^{M} \\bigl(C_{\\text{MJD}}(S,K_i,T;\\, r,q;\\boldsymbol{\\theta}) - C_i^{\\text{mkt}}\\bigr)^2\n\\;+\\;\n\\alpha \\,\\bigl\\| \\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0 \\bigr\\|_2^2,\n$$\nwhere $\\alpha \\ge 0$ is the Tikhonov regularization strength. The components of $\\boldsymbol{\\theta}$ must satisfy the bounds $\\sigma \\in [0.01, 1.50]$, $\\lambda \\in [0.00, 2.00]$, $\\mu_J \\in [-0.50, 0.50]$, and $\\delta_J \\in [0.01, 1.00]$.\n\nYour task is to implement a program that, for each test case below, constructs the synthetic market prices from the stated “true” parameters via the model above (with no noise), calibrates $\\boldsymbol{\\theta}$ by solving the optimization problem, and reports the root-mean-square error (RMSE) between the calibrated model prices and the synthetic market prices. All rates and volatilities must be treated and reported as decimals. Angles are not involved. The final numeric results must be printed as floating-point numbers.\n\nTest Suite:\n- Case $1$ (general case):\n  - $S = 100$, $r = 0.01$, $q = 0.00$, $T = 0.50$, strikes $K \\in \\{90, 100, 110\\}$.\n  - True parameters $\\boldsymbol{\\theta}^{\\star} = [0.20,\\, 0.30,\\, -0.10,\\, 0.20]$.\n  - Prior $\\boldsymbol{\\theta}_0 = [0.18,\\, 0.10,\\, 0.00,\\, 0.25]$, regularization $\\alpha = 10^{-4}$.\n- Case $2$ (sparse data, underdetermined):\n  - $S = 100$, $r = 0.00$, $q = 0.00$, $T = 0.50$, strikes $K \\in \\{90, 110\\}$.\n  - True parameters $\\boldsymbol{\\theta}^{\\star} = [0.18,\\, 0.40,\\, -0.05,\\, 0.25]$.\n  - Prior $\\boldsymbol{\\theta}_0 = [0.20,\\, 0.20,\\, 0.00,\\, 0.20]$, regularization $\\alpha = 10^{-2}$.\n- Case $3$ (edge case, no jumps):\n  - $S = 100$, $r = 0.01$, $q = 0.00$, $T = 1.00$, strikes $K \\in \\{80, 100, 120\\}$.\n  - True parameters $\\boldsymbol{\\theta}^{\\star} = [0.25,\\, 0.00,\\, 0.00,\\, 0.20]$.\n  - Prior $\\boldsymbol{\\theta}_0 = [0.22,\\, 0.30,\\, -0.10,\\, 0.25]$, regularization $\\alpha = 10^{-4}$.\n\nImplementation requirements:\n- Use the definitions above directly, without assuming any additional formulas not stated here.\n- In evaluating $C_{\\text{MJD}}$, approximate the infinite sum by a finite sum that yields numerically stable and accurate values.\n- For each case, synthesize $C_i^{\\text{mkt}}$ by evaluating $C_{\\text{MJD}}$ at $\\boldsymbol{\\theta}^{\\star}$ with the specified $(S,r,q,T,K_i)$ and then calibrate $\\boldsymbol{\\theta}$ using the stated prior and regularization.\n- For each case, compute the RMSE\n$$\n\\mathrm{RMSE} = \\sqrt{ \\frac{1}{M} \\sum_{i=1}^{M} \\bigl( C_{\\text{MJD}}(S,K_i,T;\\, r,q;\\widehat{\\boldsymbol{\\theta}}) - C_i^{\\text{mkt}} \\bigr)^2 }.\n$$\n\nFinal Output Format:\n- Your program should produce a single line of output containing the three RMSE values for Cases $1$, $2$, and $3$, in that order, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets. For example, a valid output format is\n$[x_1,x_2,x_3]$\nwhere each $x_j$ is a floating-point number rounded to $6$ decimal places and there is no additional text.", "solution": "The problem requires calibrating the parameters of the Merton jump-diffusion model by minimizing a regularized least-squares objective across a sparse set of option prices. The objective is built from first principles of risk-neutral valuation and the structure of the Merton model.\n\nFirst, under the risk-neutral measure, the asset price dynamics are given by\n$$\n\\frac{\\,\\mathrm{d}S_t\\,}{S_{t^-}} \\;=\\; \\bigl(r - q - \\lambda k\\bigr)\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_t \\;+\\; (J - 1)\\,\\mathrm{d}N_t,\n$$\nwith $\\ln J \\sim \\mathcal{N}(\\mu_J,\\delta_J^2)$ and $k = \\exp(\\mu_J + \\tfrac{1}{2}\\delta_J^2) - 1$. This specification ensures that the discounted price process is a martingale after compensating the drift by subtracting $\\lambda k$.\n\nSecond, the European call payoff at maturity is $\\max(S_T - K, 0)$. The risk-neutral price at time $t=0$ is the discounted expectation\n$$\nC_{\\text{MJD}}(S,K,T;\\, r,q;\\boldsymbol{\\theta}) \\;=\\; \\mathrm{e}^{-rT}\\,\\mathbb{E}^{\\mathbb{Q}}\\bigl[(S_T - K)^+ \\,\\big|\\, S_0 = S \\bigr].\n$$\nBy conditioning on $N_T = n$, we use the law of total expectation and the independence of jump times and sizes to write the distribution of $\\ln S_T$ as a mixture of normals: when $N_T=n$, the jump component contributes a normal shift of mean $n\\mu_J$ and variance $n\\delta_J^2$, while the diffusive component contributes variance $\\sigma^2 T$ and drift $r - q - \\lambda k$. Therefore the call price can be expressed as a Poisson-weighted sum of Black–Scholes–Merton prices,\n$$\nC_{\\text{MJD}}(S,K,T;\\, r,q;\\sigma,\\lambda,\\mu_J,\\delta_J) \\;=\\;\n\\sum_{n=0}^{\\infty} \\mathrm{e}^{-\\lambda T} \\frac{(\\lambda T)^n}{n!}\\;\nC_{\\text{BS}}\\!\\Bigl(S \\mathrm{e}^{n\\mu_J},K,T;\\; r - \\lambda k,\\; q,\\; \\sqrt{\\sigma^2 + \\tfrac{n\\,\\delta_J^2}{T}} \\Bigr),\n$$\nwhere $k = \\exp(\\mu_J + \\tfrac{1}{2}\\delta_J^2) - 1$. The Black–Scholes–Merton formula with continuous dividend yield is\n$$\nC_{\\text{BS}}(S,K,T;\\, r,q,\\sigma) = S\\,\\mathrm{e}^{-qT}\\,\\Phi(d_1) - K\\,\\mathrm{e}^{-rT}\\,\\Phi(d_2),\n$$\nwith\n$$\nd_1 = \\frac{\\ln(S/K) + (r - q + \\tfrac{1}{2}\\sigma^2)T}{\\sigma\\sqrt{T}},\n\\qquad\nd_2 = d_1 - \\sigma\\sqrt{T},\n$$\nand $\\Phi$ the standard normal cumulative distribution function.\n\nThird, the calibration constructs a parameter vector $\\boldsymbol{\\theta} = [\\sigma,\\lambda,\\mu_J,\\delta_J]^\\top$ and minimizes a regularized residual sum of squares:\n$$\n\\min_{\\boldsymbol{\\theta}} \\;\\; \\sum_{i=1}^{M} \\bigl(C_{\\text{MJD}}(S,K_i,T;\\, r,q;\\boldsymbol{\\theta}) - C_i^{\\text{mkt}}\\bigr)^2\n\\;+\\;\n\\alpha \\,\\bigl\\| \\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0 \\bigr\\|_2^2.\n$$\nThis is a Tikhonov (ridge) regularization term, with strength $\\alpha \\ge 0$ and identity penalty matrix (so the penalty is $\\ell_2$ on deviations from the prior $\\boldsymbol{\\theta}_0$). The inclusion of the regularization term discourages unstable solutions in sparse-data settings by shrinking toward the prior.\n\nFourth, the optimization is performed subject to physically and statistically reasonable bounds:\n$$\n\\sigma \\in [0.01, 1.50],\\quad \\lambda \\in [0.00, 2.00],\\quad \\mu_J \\in [-0.50, 0.50],\\quad \\delta_J \\in [0.01, 1.00].\n$$\nThese bounds enforce positivity of scale parameters and limit jump magnitudes to numerically stable ranges consistent with advanced undergraduate practice problems.\n\nFifth, for each test case, synthetic “market” prices $\\{C_i^{\\text{mkt}}\\}_{i=1}^M$ are computed by evaluating $C_{\\text{MJD}}$ at the stated true parameters $\\boldsymbol{\\theta}^\\star$ and the given $(S,r,q,T,K_i)$. This ensures an internally consistent and noise-free dataset, enabling the assessment of calibration accuracy purely as an optimization and regularization exercise.\n\nSixth, the infinite series for $C_{\\text{MJD}}$ is computed via truncation. Accuracy is achieved by summing a sufficient number of Poisson terms. A practical approach is to sum up to a maximum index $N_{\\max}$ large enough to make the omitted tail negligible for the parameter ranges at hand. For example, using $N_{\\max}$ as the larger of a fixed cap and a function of $\\lambda T$ (such as a mean plus several standard deviations) ensures robust approximation.\n\nSeventh, after solving the optimization for each case and obtaining $\\widehat{\\boldsymbol{\\theta}}$, the root-mean-square error is computed as\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{M}\\sum_{i=1}^M \\bigl(C_{\\text{MJD}}(S,K_i,T;\\, r,q;\\widehat{\\boldsymbol{\\theta}}) - C_i^{\\text{mkt}}\\bigr)^2}.\n$$\nBecause the synthetic data are noise-free and generated by the same model, the RMSE values are expected to be close to $0$ in well-posed cases (Case $1$ and Case $3$) and nonzero but small in the sparse, underdetermined case (Case $2$), where the regularization influences the solution.\n\nFinally, the program aggregates the RMSE values for Cases $1$, $2$, and $3$ into a single line formatted as $[x_1,x_2,x_3]$, where each $x_j$ is rounded to $6$ decimal places. All rates and volatilities are used and interpreted as decimals, with no percentage signs.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import log, sqrt, exp, erf\nfrom scipy.optimize import minimize\nfrom math import factorial\n\ndef std_norm_cdf(x: float) -> float:\n    # Standard normal CDF via error function\n    return 0.5 * (1.0 + erf(x / sqrt(2.0)))\n\ndef bs_call_price(S: float, K: float, r: float, q: float, sigma: float, T: float) -> float:\n    # Black-Scholes-Merton call with continuous dividend yield\n    if sigma <= 0.0 or T <= 0.0:\n        # In the limiting case, treat as forward intrinsic value\n        forward = S * np.exp(-q * T)\n        discK = K * np.exp(-r * T)\n        return max(forward - discK, 0.0)\n    vol_sqrt_T = sigma * sqrt(T)\n    d1 = (log(S / K) + (r - q + 0.5 * sigma * sigma) * T) / vol_sqrt_T\n    d2 = d1 - vol_sqrt_T\n    return S * np.exp(-q * T) * std_norm_cdf(d1) - K * np.exp(-r * T) * std_norm_cdf(d2)\n\ndef merton_call_price(S: float, K: float, r: float, q: float, T: float,\n                      sigma: float, lam: float, mu_j: float, delta_j: float) -> float:\n    # Merton jump-diffusion call via Poisson mixture of BSM prices\n    # Compute jump compensator\n    k = np.exp(mu_j + 0.5 * (delta_j ** 2)) - 1.0\n    r_eff = r - lam * k\n    # Poisson mean\n    m = lam * T\n    # Choose truncation level: mean + 10 stddevs, but at least 50 terms\n    Nmax = max(50, int(np.ceil(m + 10.0 * sqrt(m + 1e-12))))\n    # Compute Poisson weights iteratively to avoid overflow\n    # Start with p0 = exp(-m)\n    p = np.exp(-m)\n    price = 0.0\n    for n in range(0, Nmax + 1):\n        # Effective sigma and shifted spot for the nth term\n        sigma_n_sq = sigma * sigma + (n * delta_j * delta_j) / max(T, 1e-16)\n        sigma_n = sqrt(max(sigma_n_sq, 1e-16))\n        S_n = S * np.exp(n * mu_j)\n        price += p * bs_call_price(S_n, K, r_eff, q, sigma_n, T)\n        # Update Poisson weight for next n\n        # p_{n+1} = p_n * m / (n + 1)\n        if n < Nmax:\n            p = p * (m / (n + 1.0))\n    return price\n\ndef synthesize_market_prices(S, r, q, T, Ks, theta_true):\n    sigma, lam, mu_j, delta_j = theta_true\n    prices = []\n    for K in Ks:\n        prices.append(merton_call_price(S, K, r, q, T, sigma, lam, mu_j, delta_j))\n    return np.array(prices, dtype=float)\n\ndef calibrate_case(S, r, q, T, Ks, market_prices, theta0, alpha):\n    Ks = np.array(Ks, dtype=float)\n    M = len(Ks)\n    theta0 = np.array(theta0, dtype=float)\n\n    # Bounds: sigma in [0.01, 1.50], lambda in [0.0, 2.0], mu in [-0.5, 0.5], delta in [0.01, 1.0]\n    bounds = [(0.01, 1.50), (0.0, 2.0), (-0.5, 0.5), (0.01, 1.0)]\n\n    def objective(theta):\n        sigma, lam, mu_j, delta_j = theta\n        model_prices = np.array(\n            [merton_call_price(S, K, r, q, T, sigma, lam, mu_j, delta_j) for K in Ks],\n            dtype=float\n        )\n        resid = model_prices - market_prices\n        data_term = float(np.dot(resid, resid))\n        reg = float(alpha) * float(np.dot(theta - theta0, theta - theta0))\n        return data_term + reg\n\n    # Use prior as initial guess\n    x0 = theta0.copy()\n    result = minimize(objective, x0, method='L-BFGS-B', bounds=bounds)\n    theta_hat = result.x\n    # Compute RMSE\n    model_prices_hat = np.array(\n        [merton_call_price(S, K, r, q, T, theta_hat[0], theta_hat[1], theta_hat[2], theta_hat[3]) for K in Ks],\n        dtype=float\n    )\n    rmse = float(np.sqrt(np.mean((model_prices_hat - market_prices) ** 2)))\n    return theta_hat, rmse\n\ndef solve():\n    # Define test cases as specified in the problem statement.\n\n    # Case 1: general case\n    S1, r1, q1, T1 = 100.0, 0.01, 0.0, 0.50\n    Ks1 = [90.0, 100.0, 110.0]\n    theta_true_1 = [0.20, 0.30, -0.10, 0.20]\n    theta0_1 = [0.18, 0.10, 0.00, 0.25]\n    alpha_1 = 1e-4\n    market1 = synthesize_market_prices(S1, r1, q1, T1, Ks1, theta_true_1)\n\n    # Case 2: sparse data\n    S2, r2, q2, T2 = 100.0, 0.00, 0.00, 0.50\n    Ks2 = [90.0, 110.0]\n    theta_true_2 = [0.18, 0.40, -0.05, 0.25]\n    theta0_2 = [0.20, 0.20, 0.00, 0.20]\n    alpha_2 = 1e-2\n    market2 = synthesize_market_prices(S2, r2, q2, T2, Ks2, theta_true_2)\n\n    # Case 3: no jumps\n    S3, r3, q3, T3 = 100.0, 0.01, 0.00, 1.00\n    Ks3 = [80.0, 100.0, 120.0]\n    theta_true_3 = [0.25, 0.00, 0.00, 0.20]\n    theta0_3 = [0.22, 0.30, -0.10, 0.25]\n    alpha_3 = 1e-4\n    market3 = synthesize_market_prices(S3, r3, q3, T3, Ks3, theta_true_3)\n\n    # Calibrate each case\n    _, rmse1 = calibrate_case(S1, r1, q1, T1, Ks1, market1, theta0_1, alpha_1)\n    _, rmse2 = calibrate_case(S2, r2, q2, T2, Ks2, market2, theta0_2, alpha_2)\n    _, rmse3 = calibrate_case(S3, r3, q3, T3, Ks3, market3, theta0_3, alpha_3)\n\n    results = [rmse1, rmse2, rmse3]\n    # Print in required format with 6 decimal places\n    print(f\"[{results[0]:.6f},{results[1]:.6f},{results[2]:.6f}]\")\n\nsolve()\n```"}, {"introduction": "This final practice elevates our analysis to one of the cornerstones of modern quantitative finance: the Heston stochastic volatility model. Here, we move beyond constant or jumping volatility to a framework where volatility itself is a random process, allowing us to capture the persistent smiles and skews observed in option markets. You will develop a sophisticated option pricer using Fourier inversion of the model's characteristic function and calibrate the key parameters governing the interaction between asset price and its volatility, providing deep insights into market dynamics [@problem_id:2434416].", "id": "2434416", "problem": "You are to write a complete, runnable program that calibrates a Heston stochastic volatility model separately for two equities per test case and compares the correlations and vol-of-vol parameters. You must implement European call option pricing under the Heston model using a Fourier-transform-based method and calibrate only the correlation parameter between asset and variance Brownian motions, denoted by $\\rho$, and the volatility-of-variance parameter, denoted by $\\nu$. All other parameters are fixed. Your program must generate synthetic “observed” option prices from specified ground-truth Heston parameters, then recover $\\rho$ and $\\nu$ by minimizing the squared pricing error over a specified option grid for each stock. Finally, it must report the absolute differences $|\\rho_{A} - \\rho_{B}|$ and $|\\nu_{A} - \\nu_{B}|$ for each test case as floats rounded to six decimals.\n\nFundamental base. Under the Heston model in the risk-neutral measure, the asset price process $S_{t}$ and instantaneous variance $v_{t}$ follow\n$$\ndS_{t} = r S_{t} \\, dt + \\sqrt{v_{t}} \\, S_{t} \\, dW^{(1)}_{t},\n\\qquad\ndv_{t} = \\kappa \\left(\\theta - v_{t}\\right) dt + \\nu \\sqrt{v_{t}} \\, dW^{(2)}_{t},\n$$\nwith $\\mathbb{E}\\left[dW^{(1)}_{t}\\, dW^{(2)}_{t}\\right] = \\rho \\, dt$, where $r$ is the continuously compounded risk-free rate, $\\kappa$ is the variance mean-reversion speed, $\\theta$ is the long-run variance, $\\nu$ is the volatility of variance, and $\\rho$ is the instantaneous correlation between the asset and variance Brownian motions. By the no-arbitrage principle and risk-neutral valuation, the time-$0$ price of a European call option with maturity $T$ and strike $K$ is the discounted risk-neutral expectation of its payoff.\n\nYour implementation requirements:\n- Implement a numerically stable Heston European call pricer based on the risk-neutral characteristic function and Fourier inversion with numerical quadrature. You must not assume any given closed-form option price beyond the risk-neutral valuation principle and the use of characteristic functions.\n- Use no dividends (dividend yield equal to $0$).\n- Calibrate $\\rho$ and $\\nu$ by nonlinear least squares with simple bounds, holding the other Heston parameters fixed and shared between the two stocks in each test case. Use bounds $-0.999 < \\rho < 0.999$ and $10^{-4} \\le \\nu \\le 2.0$.\n- The “observed” option prices against which you calibrate must be generated by your own Heston pricer using the ground-truth parameters for each stock in each test case.\n\nCalibration design:\n- Option grid used for both stocks in every test case:\n  - Maturities $T \\in \\{\\,0.25,\\,0.5,\\,1.0\\,\\}$ (in years).\n  - Strikes $K$ are given by strike multipliers applied to spot, with multipliers $\\{\\,0.8,\\,0.9,\\,1.1,\\,1.2\\,\\}$. For example, $K = 0.8 \\times S_{0}$ produces a deep in-the-money call for the buyer.\n- For each stock, form the residual vector as the difference between model and observed prices across all maturity–strike combinations, and minimize the sum of squared residuals with respect to $(\\rho,\\nu)$ subject to the bounds above.\n\nTest suite:\nProvide three test cases. In each case, the two stocks share the same $(S_{0}, r, \\kappa, \\theta, v_{0})$ but have different ground-truth $(\\rho, \\nu)$. Your program must synthesize observed prices from these ground truths and then independently calibrate $(\\rho, \\nu)$ for Stock A and Stock B in each case.\n\n- Case $1$:\n  - Shared: $S_{0} = 100$, $r = 0.01$, $\\kappa = 1.5$, $\\theta = 0.04$, $v_{0} = 0.04$.\n  - Stock A truth: $\\rho_{A} = -0.7$, $\\nu_{A} = 0.6$.\n  - Stock B truth: $\\rho_{B} = -0.6$, $\\nu_{B} = 0.55$.\n- Case $2$:\n  - Shared: $S_{0} = 100$, $r = 0.005$, $\\kappa = 2.0$, $\\theta = 0.03$, $v_{0} = 0.03$.\n  - Stock A truth: $\\rho_{A} = -0.95$, $\\nu_{A} = 0.3$.\n  - Stock B truth: $\\rho_{B} = -0.90$, $\\nu_{B} = 0.28$.\n- Case $3$:\n  - Shared: $S_{0} = 100$, $r = 0.02$, $\\kappa = 1.2$, $\\theta = 0.05$, $v_{0} = 0.05$.\n  - Stock A truth: $\\rho_{A} = -0.5$, $\\nu_{A} = 0.9$.\n  - Stock B truth: $\\rho_{B} = -0.48$, $\\nu_{B} = 1.0$.\n\nAnswer specification and output format:\n- For each test case, after calibrating both stocks, compute two floats:\n  - $d_{\\rho} = \\left|\\hat{\\rho}_{A} - \\hat{\\rho}_{B}\\right|$,\n  - $d_{\\nu} = \\left|\\hat{\\nu}_{A} - \\hat{\\nu}_{B}\\right|$,\n  where hats denote calibrated values.\n- Round each of these floats to exactly six decimals.\n- Your program should produce a single line of output containing all results, flattened in the order of cases and within-case parameters as\n  $[d_{\\rho}^{(1)}, d_{\\nu}^{(1)}, d_{\\rho}^{(2)}, d_{\\nu}^{(2)}, d_{\\rho}^{(3)}, d_{\\nu}^{(3)}]$,\n  where the superscript denotes the case index. For example, a valid output might look like\n  $[0.100000,0.050000,0.050000,0.020000,0.020000,0.100000]$.\n\nNotes:\n- Angles are not involved.\n- There are no physical units; all quantities are dimensionless under the risk-neutral valuation framework used here.\n- The implementation must be fully deterministic and must not use any randomness.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Heston Model Dynamics**: Under a risk-neutral measure $\\mathbb{Q}$, the asset price $S_t$ and its variance $v_t$ follow the stochastic differential equations:\n$$\ndS_{t} = r S_{t} \\, dt + \\sqrt{v_{t}} \\, S_{t} \\, dW^{(1)}_{t}\n$$\n$$\ndv_{t} = \\kappa \\left(\\theta - v_{t}\\right) dt + \\nu \\sqrt{v_{t}} \\, dW^{(2)}_{t}\n$$\nwhere $dW^{(1)}_{t}$ and $dW^{(2)}_{t}$ are Wiener processes with correlation $\\mathbb{E}\\left[dW^{(1)}_{t}\\, dW^{(2)}_{t}\\right] = \\rho \\, dt$.\n- **Parameters**:\n    - $r$: continuously compounded risk-free rate.\n    - $\\kappa$: speed of mean reversion of variance.\n    - $\\theta$: long-run mean of variance.\n    - $\\nu$: volatility of variance (vol-of-vol).\n    - $\\rho$: correlation between asset and variance processes.\n    - $S_0$: initial asset price.\n    - $v_0$: initial variance.\n- **Task**: Calibrate parameters $(\\rho, \\nu)$ for two stocks, A and B, per test case.\n- **Fixed Parameters**: For each test case, $(S_0, r, \\kappa, \\theta, v_0)$ are shared between stocks A and B.\n- **Calibration Method**:\n    - Objective: Minimize the sum of squared errors between model-predicted European call option prices and synthetically generated \"observed\" prices.\n    - Optimization variables: $(\\rho, \\nu)$.\n    - Bounds: $-0.999 < \\rho < 0.999$ and $10^{-4} \\le \\nu \\le 2.0$.\n- **Observed Data Generation**: \"Observed\" prices are generated using the provided Heston model pricer with ground-truth parameters for each stock.\n- **Option Grid for Calibration**:\n    - Maturities $T \\in \\{0.25, 0.5, 1.0\\}$ years.\n    - Strikes $K = S_0 \\times m$, where multipliers $m \\in \\{0.8, 0.9, 1.1, 1.2\\}$.\n- **Test Cases**:\n    - **Case 1**: Shared: $S_{0} = 100$, $r = 0.01$, $\\kappa = 1.5$, $\\theta = 0.04$, $v_{0} = 0.04$. Stock A truth: $(\\rho_{A}, \\nu_{A}) = (-0.7, 0.6)$. Stock B truth: $(\\rho_{B}, \\nu_{B}) = (-0.6, 0.55)$.\n    - **Case 2**: Shared: $S_{0} = 100$, $r = 0.005$, $\\kappa = 2.0$, $\\theta = 0.03$, $v_{0} = 0.03$. Stock A truth: $(\\rho_{A}, \\nu_{A}) = (-0.95, 0.3)$. Stock B truth: $(\\rho_{B}, \\nu_{B}) = (-0.90, 0.28)$.\n    - **Case 3**: Shared: $S_{0} = 100$, $r = 0.02$, $\\kappa = 1.2$, $\\theta = 0.05$, $v_{0} = 0.05$. Stock A truth: $(\\rho_{A}, \\nu_{A}) = (-0.5, 0.9)$. Stock B truth: $(\\rho_{B}, \\nu_{B}) = (-0.48, 1.0)$.\n- **Output**: For each case, compute $d_{\\rho} = |\\hat{\\rho}_{A} - \\hat{\\rho}_{B}|$ and $d_{\\nu} = |\\hat{\\nu}_{A} - \\hat{\\nu}_{B}|$, where hatted variables are calibrated values. Report a flattened list of these values, rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the Heston stochastic volatility model, a canonical model in quantitative finance. The pricing method via Fourier inversion of the characteristic function is a standard and rigorous technique. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. It requires calibrating a model against synthetic data generated from the same model, which is a standard procedure for testing the implementation of a pricing and calibration routine. A unique solution to the optimization problem is expected, especially given the synthetic nature of the data.\n- **Objective**: The problem is stated using precise mathematical definitions, numerical values, and an unambiguous objective. It is free of subjectivity.\n- **Completeness and Consistency**: The problem provides all necessary data and constraints to implement the solution. The parameters, including those that violate the Feller condition ($2\\kappa\\theta > \\nu^2$), are well within the domain where the characteristic function is defined, making the problem specification consistent with the chosen mathematical framework.\n- **Other Flaws**: The problem is not metaphorical, trivial, or outside the scope of scientific verifiability.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Design\n\nThe core of the solution is the implementation of a European call option pricer for the Heston model and its use within a numerical optimization routine for calibration.\n\n**1. Heston Model Option Pricing**\n\nThe price of a European call option $C$ with strike $K$ and maturity $T$ is given by the risk-neutral valuation formula:\n$$\nC(S_0, v_0, T) = e^{-rT} \\mathbb{E}_{\\mathbb{Q}}[\\max(S_T - K, 0) | \\mathcal{F}_0]\n$$\nThis expectation can be efficiently computed using Fourier methods. The price can be decomposed as:\n$$\nC(S_0, K, T) = S_0 P_1 - K e^{-rT} P_2\n$$\nwhere $P_1$ and $P_2$ are probabilities in different numeraires. Heston showed that these probabilities can be calculated by inverting the conditional characteristic function of the log-price, $x_T = \\ln(S_T)$. The formula, based on the Gil-Pelaez inversion theorem, is:\n$$\nP_j(\\phi; K) = \\frac{1}{2} + \\frac{1}{\\pi} \\int_0^\\infty \\text{Re}\\left[ \\frac{e^{-i\\phi \\ln K} f_j(\\phi)}{i\\phi} \\right] d\\phi\n$$\nfor $j \\in \\{1, 2\\}$, where $f_j$ are the relevant characteristic functions.\n\nThe characteristic function of the log-price $x_T = \\ln(S_T)$ under the risk-neutral measure is given by $f(\\phi) = \\mathbb{E}[e^{i\\phi x_T}]$. For the Heston model, it has a known semi-closed form:\n$$\nf(\\phi, T) = \\exp(C(\\phi, T) + D(\\phi, T)v_0 + i\\phi (\\ln S_0 + rT))\n$$\nwhere\n$$\nC(\\phi, T) = \\frac{\\kappa\\theta}{\\nu^2} \\left[ (\\kappa - \\rho\\nu i\\phi - d)T - 2\\ln\\left(\\frac{1 - ge^{-dT}}{1-g}\\right) \\right]\n$$\n$$\nD(\\phi, T) = \\frac{\\kappa - \\rho\\nu i\\phi - d}{\\nu^2} \\left(\\frac{1 - e^{-dT}}{1-ge^{-dT}}\\right)\n$$\nwith auxiliary variables:\n$$\nd = \\sqrt{(\\kappa - \\rho\\nu i\\phi)^2 + (\\phi^2 + i\\phi)\\nu^2}\n$$\n$$\ng = \\frac{\\kappa - \\rho\\nu i\\phi - d}{\\kappa - \\rho\\nu i\\phi + d}\n$$\nThe characteristic functions required for $P_1$ and $P_2$ are:\n- $f_2(\\phi) = f(\\phi, T)$: The characteristic function of $\\ln(S_T)$ under the risk-neutral measure.\n- $f_1(\\phi) = f(\\phi-i, T)$: A related characteristic function corresponding to the stock numeraire. This is equivalent to evaluating $f(\\phi, T)$ with $\\phi$ replaced by $\\phi-i$.\n\nThe integrals for $P_1$ and $P_2$ are computed numerically using quadrature, for which `scipy.integrate.quad` is suitable.\n\n**2. Calibration Procedure**\n\nThe calibration aims to find the parameters $(\\hat{\\rho}, \\hat{\\nu})$ that best match the model's output to a set of observed market prices. The problem specifies that these \"observed\" prices are synthetically generated using the same pricing model with known ground-truth parameters.\n\nLet $\\Theta = (\\rho, \\nu)$ be the vector of parameters to be calibrated. Let the set of options used for calibration be indexed by $i=1, \\dots, N$, with strikes $K_i$ and maturities $T_i$.\nThe objective function to be minimized is the sum of squared errors (SSE):\n$$\n\\text{SSE}(\\Theta) = \\sum_{i=1}^{N} \\left[ C_{\\text{model}}(K_i, T_i; \\Theta) - C_{\\text{observed}}(K_i, T_i) \\right]^2\n$$\nwhere $C_{\\text{model}}$ is the price from our Heston pricer and $C_{\\text{observed}}$ is the synthetic target price.\n\nThis minimization is a nonlinear least-squares problem subject to box constraints:\n$$\n-0.999 \\le \\rho \\le 0.999 \\quad \\text{and} \\quad 10^{-4} \\le \\nu \\le 2.0\n$$\nWe employ the L-BFGS-B algorithm, available in `scipy.optimize.minimize`, which is a quasi-Newton method well-suited for this type of constrained optimization problem. The strict inequality for $\\rho$ is handled by using the specified values as inclusive bounds, which is standard practice.\n\n**3. Algorithmic Implementation**\n\nThe overall algorithm is as follows:\n1. For each of the three test cases provided:\n    a. Define the shared parameters $(S_0, r, \\kappa, \\theta, v_0)$ and the ground-truth parameters $(\\rho_A, \\nu_A)$ for Stock A and $(\\rho_B, \\nu_B)$ for Stock B.\n    b. Construct the option grid consisting of $3 \\times 4 = 12$ call options from the specified maturities and strike multipliers.\n    c. For Stock A:\n        i. Generate the $12$ \"observed\" prices using the Heston pricer with the ground-truth parameters $(\\rho_A, \\nu_A)$.\n        ii. Define an objective function that computes the SSE for a given pair $(\\rho, \\nu)$.\n        iii. Use `scipy.optimize.minimize` with the L-BFGS-B method, bounds, and a reasonable initial guess (e.g., $\\rho=-0.5, \\nu=0.5$) to find the calibrated parameters $\\hat{\\rho}_A$ and $\\hat{\\nu}_A$.\n    d. For Stock B:\n        i. Repeat the process from (c) using the ground-truth parameters $(\\rho_B, \\nu_B)$ to find the calibrated parameters $\\hat{\\rho}_B$ and $\\hat{\\nu}_B$.\n    e. Compute the absolute differences $d_{\\rho} = |\\hat{\\rho}_A - \\hat{\\rho}_B|$ and $d_{\\nu} = |\\hat{\\nu}_A - \\hat{\\nu}_B|$.\n2. Collect the six resulting difference values ($d_{\\rho}^{(1)}, d_{\\nu}^{(1)}, d_{\\rho}^{(2)}, d_{\\nu}^{(2)}, d_{\\rho}^{(3)}, d_{\\nu}^{(3)}$).\n3. Format these values as a string of comma-separated numbers, each rounded to six decimal places, enclosed in square brackets. This is the final output.\nThis deterministic procedure strictly adheres to the problem requirements.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the Heston model calibration problem.\n    \"\"\"\n\n    def heston_char_func(phi, T, r, kappa, theta, nu, rho, v0, S0):\n        \"\"\"\n        Computes the Heston characteristic function for the log-price ln(S_T).\n        f(phi, T) = E[exp(i*phi*ln(S_T))]\n        \"\"\"\n        x0 = np.log(S0)\n        \n        # d and g are auxiliary variables in the Heston characteristic function\n        d = np.sqrt((kappa - rho * nu * 1j * phi)**2 + (phi**2 + 1j * phi) * nu**2)\n        g = (kappa - rho * nu * 1j * phi - d) / (kappa - rho * nu * 1j * phi + d)\n        \n        # C and D are the main components of the characteristic function's exponent\n        C = r * 1j * phi * T + (kappa * theta / nu**2) * \\\n            ((kappa - rho * nu * 1j * phi - d) * T - 2 * np.log((1 - g * np.exp(-d * T)) / (1 - g)))\n        \n        D = (kappa - rho * nu * 1j * phi - d) / nu**2 * ((1 - np.exp(-d * T)) / (1 - g * np.exp(-d * T)))\n        \n        return np.exp(C + D * v0 + 1j * phi * x0)\n\n    def heston_price_integrand(phi, j, S0, K, T, r, kappa, theta, nu, rho, v0):\n        \"\"\"\n        Computes the integrand for calculating P_j in the Heston pricing formula.\n        j=1 or j=2.\n        \"\"\"\n        if j == 1:\n            # Use characteristic function f(phi-i) for P1\n            char_val = heston_char_func(phi - 1j, T, r, kappa, theta, nu, rho, v0, S0)\n        else: # j == 2\n            # Use characteristic function f(phi) for P2\n            char_val = heston_char_func(phi, T, r, kappa, theta, nu, rho, v0, S0)\n        \n        integrand = np.real(np.exp(-1j * phi * np.log(K)) * char_val / (1j * phi))\n        return integrand\n        \n    def heston_call_price(S0, K, T, r, kappa, theta, nu, rho, v0):\n        \"\"\"\n        Calculates the Heston European call price using Fourier inversion.\n        \"\"\"\n        # Integration limit for the semi-infinite integral\n        integration_limit = 200.0\n\n        # Integral for P1\n        args1 = (1, S0, K, T, r, kappa, theta, nu, rho, v0)\n        integral1, _ = quad(heston_price_integrand, 0, integration_limit, args=args1)\n        P1 = 0.5 + integral1 / np.pi\n\n        # Integral for P2\n        args2 = (2, S0, K, T, r, kappa, theta, nu, rho, v0)\n        integral2, _ = quad(heston_price_integrand, 0, integration_limit, args=args2)\n        P2 = 0.5 + integral2 / np.pi\n        \n        price = S0 * P1 - K * np.exp(-r * T) * P2\n        return price\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"shared\": {\"S0\": 100, \"r\": 0.01, \"kappa\": 1.5, \"theta\": 0.04, \"v0\": 0.04},\n            \"A\": {\"rho\": -0.7, \"nu\": 0.6},\n            \"B\": {\"rho\": -0.6, \"nu\": 0.55}\n        },\n        {\n            \"shared\": {\"S0\": 100, \"r\": 0.005, \"kappa\": 2.0, \"theta\": 0.03, \"v0\": 0.03},\n            \"A\": {\"rho\": -0.95, \"nu\": 0.3},\n            \"B\": {\"rho\": -0.90, \"nu\": 0.28}\n        },\n        {\n            \"shared\": {\"S0\": 100, \"r\": 0.02, \"kappa\": 1.2, \"theta\": 0.05, \"v0\": 0.05},\n            \"A\": {\"rho\": -0.5, \"nu\": 0.9},\n            \"B\": {\"rho\": -0.48, \"nu\": 1.0}\n        }\n    ]\n    \n    # Define the option grid structure\n    maturities = [0.25, 0.5, 1.0]\n    strike_multipliers = [0.8, 0.9, 1.1, 1.2]\n    \n    results = []\n    \n    for case in test_cases:\n        shared_params = case[\"shared\"]\n        S0, r, kappa, theta, v0 = shared_params.values()\n        \n        params_A_truth = case[\"A\"]\n        params_B_truth = case[\"B\"]\n        \n        # Construct the option grid for this case\n        option_grid = []\n        for T in maturities:\n            for mult in strike_multipliers:\n                K = S0 * mult\n                option_grid.append({\"T\": T, \"K\": K})\n\n        calibrated_params = {}\n        for stock_label, truth_params in [(\"A\", params_A_truth), (\"B\", params_B_truth)]:\n            \n            # 1. Generate synthetic \"observed\" prices\n            observed_prices = np.array([\n                heston_call_price(S0, opt['K'], opt['T'], r, kappa, theta,\n                                   truth_params['nu'], truth_params['rho'], v0)\n                for opt in option_grid\n            ])\n            \n            # 2. Define the objective function for minimization\n            def objective_function(params):\n                rho, nu = params\n                model_prices = np.array([\n                    heston_call_price(S0, opt['K'], opt['T'], r, kappa, theta, nu, rho, v0)\n                    for opt in option_grid\n                ])\n                return np.sum((model_prices - observed_prices)**2)\n\n            # 3. Calibrate parameters rho and nu\n            initial_guess = [-0.5, 0.5]\n            bounds = ((-0.999, 0.999), (1e-4, 2.0))\n            \n            opt_result = minimize(objective_function, initial_guess, method='L-BFGS-B', bounds=bounds)\n            \n            calibrated_params[stock_label] = opt_result.x\n\n        rho_A_cal, nu_A_cal = calibrated_params[\"A\"]\n        rho_B_cal, nu_B_cal = calibrated_params[\"B\"]\n        \n        # 4. Compute and store absolute differences\n        d_rho = abs(rho_A_cal - rho_B_cal)\n        d_nu = abs(nu_A_cal - nu_B_cal)\n        \n        results.append(d_rho)\n        results.append(d_nu)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"}]}