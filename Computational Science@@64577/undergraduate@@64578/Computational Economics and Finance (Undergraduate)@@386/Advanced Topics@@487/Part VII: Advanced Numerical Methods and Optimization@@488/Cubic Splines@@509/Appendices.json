{"hands_on_practices": [{"introduction": "The essence of a cubic spline's smoothness lies in how the cubic segments are joined together. This is controlled by the second derivatives, or curvatures, at the data points (knots). This first exercise provides a focused, hands-on calculation of this crucial component, asking you to determine the second derivative at an interior knot for a natural cubic spline ([@problem_id:2164958]). Mastering this step is fundamental to understanding how splines achieve their characteristic smooth and stable fit.", "id": "2164958", "problem": "A function $S(x)$ is a natural cubic spline that interpolates a set of data points. By definition, a cubic spline is a piecewise function composed of cubic polynomials, and it is continuous along with its first and second derivatives. A \"natural\" cubic spline has the additional property that its second derivative is zero at the two endpoints of the interval of interpolation.\n\nConsider a natural cubic spline $S(x)$ that passes through the following three data points: $(x_0, y_0) = (0, 0)$, $(x_1, y_1) = (1, 1)$, and $(x_2, y_2) = (2, 0)$.\n\nCalculate the value of the second derivative of the spline at $x=1$, which is denoted by $S''(1)$.\n\n", "solution": "Let $x_{0}=0$, $x_{1}=1$, $x_{2}=2$ with $y_{0}=0$, $y_{1}=1$, $y_{2}=0$. For a natural cubic spline, the second derivatives at the knots $M_{i}=S''(x_{i})$ satisfy $M_{0}=0$ and $M_{2}=0$, and the interior equations are\n$$\nh_{i-1}M_{i-1}+2(h_{i-1}+h_{i})M_{i}+h_{i}M_{i+1}\n=6\\left(\\frac{y_{i+1}-y_{i}}{h_{i}}-\\frac{y_{i}-y_{i-1}}{h_{i-1}}\\right),\n$$\nfor $i=1,\\ldots,n-1$, where $h_{i}=x_{i+1}-x_{i}$. Here $h_{0}=x_{1}-x_{0}=1$ and $h_{1}=x_{2}-x_{1}=1$, and there is a single interior equation at $i=1$:\n$$\n1\\cdot M_{0}+2(1+1)M_{1}+1\\cdot M_{2}\n=6\\left(\\frac{y_{2}-y_{1}}{1}-\\frac{y_{1}-y_{0}}{1}\\right).\n$$\nSubstituting $M_{0}=0$, $M_{2}=0$, $y_{2}-y_{1}=0-1=-1$, and $y_{1}-y_{0}=1-0=1$ gives\n$$\n4M_{1}=6(-1-1)=6(-2)=-12,\n$$\nhence\n$$\nM_{1}=-3.\n$$\nTherefore $S''(1)=M_{1}=-3$.", "answer": "$$\\boxed{-3}$$"}, {"introduction": "After determining the second derivatives, the next step is to construct the spline itself and use it for interpolation. This practice builds directly on the previous concept by having you interpolate a value using a natural cubic spline and compare it with a simpler piecewise linear model ([@problem_id:2164998]). This comparison is a powerful way to see the practical difference between a simple, kinked model and the smooth curve generated by a spline, illustrating why splines are often a superior choice for modeling continuous processes.", "id": "2164998", "problem": "An engineer is modeling the shape of a thin, flexible rod that is constrained to pass through three points in a 2D Cartesian plane: $P_0=(-1, 1)$, $P_1=(0, 0)$, and $P_2=(1, 1)$. Two simple models are proposed to estimate the rod's vertical position, $y$, at other horizontal locations, $x$.\n\nModel A is a piecewise linear interpolant, which connects the specified points with straight line segments.\nModel B is a natural cubic spline, which ensures the curve is smooth by matching first and second derivatives at the interior points, and has zero second derivatives at the endpoints.\n\nCalculate the vertical position of the rod at $x=0.5$ as predicted by both Model A and Model B. Let these values be $y_A$ and $y_B$, respectively. Present your answer as a pair of exact fractions $(y_A, y_B)$.\n\n", "solution": "We label the nodes as $x_{0}=-1$, $x_{1}=0$, $x_{2}=1$ with corresponding values $y_{0}=1$, $y_{1}=0$, $y_{2}=1$.\n\nModel A (piecewise linear interpolant): For $x \\in [x_{1},x_{2}]$, the line through $(0,0)$ and $(1,1)$ has slope\n$$\nm=\\frac{y_{2}-y_{1}}{x_{2}-x_{1}}=\\frac{1-0}{1-0}=1,\n$$\nso the interpolant is\n$$\ny(x)=y_{1}+m(x-x_{1})=0+1\\cdot(x-0)=x.\n$$\nEvaluating at $x=\\frac{1}{2}$ gives\n$$\ny_{A}=y\\!\\left(\\frac{1}{2}\\right)=\\frac{1}{2}.\n$$\n\nModel B (natural cubic spline): Let $M_{i}=S''(x_{i})$. Natural end conditions give $M_{0}=0$ and $M_{2}=0$. With $h_{0}=x_{1}-x_{0}=1$ and $h_{1}=x_{2}-x_{1}=1$, the cubic spline system at the interior node $i=1$ is\n$$\nh_{0}M_{0}+2(h_{0}+h_{1})M_{1}+h_{1}M_{2}=6\\left(\\frac{y_{2}-y_{1}}{h_{1}}-\\frac{y_{1}-y_{0}}{h_{0}}\\right).\n$$\nSubstituting the values,\n$$\n1\\cdot 0+2(1+1)M_{1}+1\\cdot 0=6\\left(\\frac{1-0}{1}-\\frac{0-1}{1}\\right)=6(1-(-1))=12,\n$$\nso\n$$\n4M_{1}=12 \\quad \\Rightarrow \\quad M_{1}=3.\n$$\nOn the interval $[x_{1},x_{2}]=[0,1]$, the spline is\n$$\nS(x)=\\frac{M_{1}}{6h_{1}}(x_{2}-x)^{3}+\\frac{M_{2}}{6h_{1}}(x-x_{1})^{3}+\\left(y_{1}-\\frac{M_{1}h_{1}^{2}}{6}\\right)\\frac{x_{2}-x}{h_{1}}+\\left(y_{2}-\\frac{M_{2}h_{1}^{2}}{6}\\right)\\frac{x-x_{1}}{h_{1}}.\n$$\nWith $h_{1}=1$, $M_{1}=3$, $M_{2}=0$, $y_{1}=0$, and $y_{2}=1$, this simplifies to\n$$\nS(x)=\\frac{3}{6}(1-x)^{3}+0+\\left(0-\\frac{3}{6}\\right)(1-x)+\\left(1-0\\right)x\n=\\frac{1}{2}(1-x)^{3}-\\frac{1}{2}(1-x)+x.\n$$\nExpanding and combining like terms,\n$$\nS(x)=\\frac{1}{2}-\\frac{3}{2}x+\\frac{3}{2}x^{2}-\\frac{1}{2}x^{3}-\\frac{1}{2}+\\frac{1}{2}x+x\n=\\frac{3}{2}x^{2}-\\frac{1}{2}x^{3}.\n$$\nEvaluating at $x=\\frac{1}{2}$ gives\n$$\ny_{B}=S\\!\\left(\\frac{1}{2}\\right)=\\frac{3}{2}\\left(\\frac{1}{2}\\right)^{2}-\\frac{1}{2}\\left(\\frac{1}{2}\\right)^{3}\n=\\frac{3}{2}\\cdot\\frac{1}{4}-\\frac{1}{2}\\cdot\\frac{1}{8}\n=\\frac{3}{8}-\\frac{1}{16}\n=\\frac{5}{16}.\n$$\n\nThus, the required pair is $\\left(\\frac{1}{2},\\frac{5}{16}\\right)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{5}{16}\\end{pmatrix}}$$"}, {"introduction": "While splines offer smoothness, why are they often preferred over other smooth functions like a single high-degree polynomial? This advanced practice tackles that question head-on by exploring the Runge phenomenon, where high-degree polynomial interpolants can oscillate wildly between data points ([@problem_id:2386520]). Through a computational exercise modeling a financial volatility curve, you will see firsthand how splines provide a stable and accurate fit while a global polynomial fails, demonstrating their vital role in quantitative finance and computational modeling.", "id": "2386520", "problem": "Consider the function $v(k)$ mapping standardized log-moneyness $k$ to an implied volatility level, defined by\n$$\nv(k) \\equiv \\alpha + \\frac{\\beta}{1 + \\gamma k^2},\n$$\nwith parameters $\\alpha = 0.2$, $\\beta = 0.1$, and $\\gamma = 25$. For a given integer $n \\ge 2$, define a data set of $n$ equally spaced nodes on the closed interval $[-1,1]$ as $k_i = -1 + \\frac{2(i-1)}{n-1}$ for $i \\in \\{1,2,\\dots,n\\}$, and observed values $y_i = v(k_i)$.\n\nFor each specified $n$, consider two interpolants constructed from the same $\\{(k_i,y_i)\\}_{i=1}^n$:\n1. $p_n(k)$ is the unique polynomial interpolant of degree at most $n-1$ satisfying $p_n(k_i) = y_i$ for all $i \\in \\{1,2,\\dots,n\\}$.\n2. $s_n(k)$ is the natural cubic spline interpolant with knots at the nodes $\\{k_i\\}_{i=1}^n$, satisfying $s_n(k_i) = y_i$ for all $i \\in \\{1,2,\\dots,n\\}$, being twice continuously differentiable on $[-1,1]$, piecewise cubic between consecutive knots, and obeying the natural boundary conditions $s_n''(-1) = 0$ and $s_n''(1) = 0$.\n\nFor an evaluation grid $\\{x_j\\}_{j=1}^m$ contained in $[-1,1]$, define the root mean squared error (RMSE) of an approximant $a(k)$ to $v(k)$ as\n$$\n\\mathrm{RMSE}(a) \\equiv \\left( \\frac{1}{m} \\sum_{j=1}^m \\left[a(x_j) - v(x_j)\\right]^2 \\right)^{1/2},\n$$\nand the maximum absolute error as\n$$\n\\mathrm{MAX}(a) \\equiv \\max_{1 \\le j \\le m} \\left|a(x_j) - v(x_j)\\right|.\n$$\n\nUsing the definition above, evaluate the following four test cases:\n\n- Case A (general case): $n = 9$, evaluation grid size $m = 1001$ with equidistant points on $[-1,1]$.\n- Case B (general case): $n = 17$, evaluation grid size $m = 1001$ with equidistant points on $[-1,1]$.\n- Case C (boundary stressing high degree): $n = 25$, evaluation grid size $m = 1001$ with equidistant points on $[-1,1]$.\n- Case D (interpolation identity check): $n = 17$, evaluation grid consisting exactly of the $n$ nodes, i.e., $m = n$ and $\\{x_j\\}_{j=1}^m = \\{k_i\\}_{i=1}^n$.\n\nFor each case, compute four quantities in the following order:\n1. $\\mathrm{RMSE}(p_n)$,\n2. $\\mathrm{RMSE}(s_n)$,\n3. $\\mathrm{MAX}(p_n)$,\n4. $\\mathrm{MAX}(s_n)$.\n\nFinal output format requirement: Your program should produce a single line of output containing a comma-separated list of four-element lists, one per test case, enclosed in square brackets. Specifically, the output must be\n$$\n\\left[\\left[r_1,r_2,r_3,r_4\\right],\\left[r_1',r_2',r_3',r_4'\\right],\\left[r_1'',r_2'',r_3'',r_4''\\right],\\left[r_1''',r_2''',r_3''',r_4'''\\right]\\right],\n$$\nwhere each $r_\\cdot$ is a real number, and the four inner lists correspond respectively to Cases A, B, C, and D in the order stated above. All numerical results must be expressed as real numbers without units. The output must appear exactly on a single line as specified, for example as $[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] ]$ but without any additional explanatory text, and without any percentage signs or angle units.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It is scientifically grounded in the field of numerical analysis, well-posed, objective, and contains all necessary information for a unique solution. No inconsistencies, contradictions, or ambiguities are present. We may therefore proceed with the solution.\n\nThe problem requires a comparative analysis of two interpolation schemes—global polynomial interpolation and piecewise cubic spline interpolation—for approximating a given function. The function to be approximated models an implied volatility curve and is defined as:\n$$\nv(k) \\equiv \\alpha + \\frac{\\beta}{1 + \\gamma k^2}\n$$\nwith given parameters $\\alpha = 0.2$, $\\beta = 0.1$, and $\\gamma = 25$. This is a rational function, which is infinitely differentiable ($C^\\infty$) on the entire real line. However, its analytic continuation into the complex plane has poles at $k = \\pm i/\\sqrt{\\gamma} = \\pm i/5 = \\pm 0.2i$. The proximity of these poles to the interpolation interval $[-1, 1]$ is a critical factor influencing the convergence characteristics of polynomial interpolants.\n\nFor a given number of nodes $n$, we are provided with a data set $\\{(k_i, y_i)\\}_{i=1}^n$, where the nodes $k_i = -1 + \\frac{2(i-1)}{n-1}$ are uniformly spaced on $[-1, 1]$, and the values are $y_i = v(k_i)$.\n\nThe first approximant, $p_n(k)$, is the unique interpolating polynomial of degree at most $n-1$. For such a polynomial to exist and be unique, the $k_i$ must be distinct, which is guaranteed by their definition. While a unique solution exists, it is a well-established result in approximation theory that for functions like $v(k)$ with singularities close to the real interval, high-degree polynomial interpolation on equidistant nodes leads to large oscillations, especially near the endpoints. This behavior is known as Runge's phenomenon. To compute this polynomial, we shall employ a numerically stable method, the Barycentric interpolation formula, which is preferable to constructing and solving the ill-conditioned Vandermonde system for the monomial coefficients.\n\nThe second approximant, $s_n(k)$, is the natural cubic spline. This method constructs a piecewise cubic polynomial on each subinterval $[k_i, k_{i+1}]$. To ensure smoothness, the spline is required to be twice continuously differentiable ($C^2$) over the entire interval $[-1, 1]$. This is achieved by enforcing that the function values, first derivatives, and second derivatives of adjacent cubic pieces match at the interior knots. The system is closed by the \"natural\" boundary conditions, $s_n''(-1) = 0$ and $s_n''(1) = 0$. This setup leads to a well-conditioned, tridiagonal system of linear equations for the second derivatives at the knots, which can be solved efficiently and stably. Cubic splines are generally superior for data fitting and interpolation as they avoid the oscillatory behavior of high-degree polynomials.\n\nThe performance of each interpolant, $a(k) \\in \\{p_n(k), s_n(k)\\}$, is quantified using two error metrics evaluated on a discrete grid $\\{x_j\\}_{j=1}^m$:\n1.  Root Mean Squared Error: $\\mathrm{RMSE}(a) = \\left( \\frac{1}{m} \\sum_{j=1}^m [a(x_j) - v(x_j)]^2 \\right)^{1/2}$\n2.  Maximum Absolute Error: $\\mathrm{MAX}(a) = \\max_{1 \\le j \\le m} |a(x_j) - v(x_j)|$\n\nThe computational procedure for each test case is as follows:\n1.  For a given $n$, establish the set of $n$ interpolation nodes $\\{k_i\\}$ and compute the corresponding values $\\{y_i = v(k_i)\\}$.\n2.  Construct the polynomial interpolant $p_n(k)$ using the Barycentric form from the set $\\{(k_i, y_i)\\}$.\n3.  Construct the natural cubic spline $s_n(k)$ from the same data set, imposing $s_n''(-1) = s_n''(1) = 0$.\n4.  Define the evaluation grid $\\{x_j\\}_{j=1}^m$ as specified for the test case.\n5.  Evaluate $v(x_j)$, $p_n(x_j)$, and $s_n(x_j)$ on the evaluation grid.\n6.  Compute the error vectors for both the polynomial and the spline.\n7.  Calculate the four required metrics: $\\mathrm{RMSE}(p_n)$, $\\mathrm{RMSE}(s_n)$, $\\mathrm{MAX}(p_n)$, and $\\mathrm{MAX}(s_n)$.\n\nFor Case C, with a high node count of $n=25$, we expect a significant manifestation of Runge's phenomenon, leading to large errors for $p_{25}(k)$. Conversely, the spline's error should decrease as $n$ increases. Case D serves as a fundamental check of correctness: since the evaluation grid coincides with the interpolation nodes, the error for both interpolants must be zero (or within machine epsilon), as $a(k_i) = y_i = v(k_i)$ by definition of interpolation. The following program implements this procedure.", "answer": "```python\nimport numpy as np\nfrom scipy.interpolate import BarycentricInterpolator, CubicSpline\n\ndef solve():\n    \"\"\"\n    Computes and compares errors for polynomial and cubic spline interpolants.\n    \"\"\"\n    # Define parameters for the function v(k)\n    alpha = 0.2\n    beta = 0.1\n    gamma = 25.0\n\n    def v(k: np.ndarray) -> np.ndarray:\n        \"\"\"\n        The target function to be interpolated.\n        v(k) = alpha + beta / (1 + gamma * k^2)\n        \"\"\"\n        return alpha + beta / (1.0 + gamma * np.power(k, 2))\n\n    # Define the four test cases from the problem statement\n    # Each tuple is (n, m, grid_type)\n    # grid_type 'equidistant': m points on [-1, 1]\n    # grid_type 'nodes': m=n points, evaluation grid is the interpolation nodes\n    test_cases = [\n        (9, 1001, 'equidistant'),    # Case A\n        (17, 1001, 'equidistant'),   # Case B\n        (25, 1001, 'equidistant'),   # Case C\n        (17, 17, 'nodes'),           # Case D\n    ]\n\n    all_results = []\n    \n    for n, m, grid_type in test_cases:\n        # Step 1: Generate interpolation data\n        # n equally spaced nodes on the closed interval [-1, 1]\n        k_nodes = np.linspace(-1.0, 1.0, n)\n        # Observed values y_i = v(k_i)\n        y_nodes = v(k_nodes)\n\n        # Step 2: Construct the interpolants\n        # p_n(k): unique polynomial interpolant of degree at most n-1\n        # Using Barycentric form for numerical stability\n        p_n = BarycentricInterpolator(k_nodes, y_nodes)\n\n        # s_n(k): natural cubic spline interpolant\n        # bc_type='natural' sets the second derivatives at endpoints to zero\n        s_n = CubicSpline(k_nodes, y_nodes, bc_type='natural')\n\n        # Step 3: Define the evaluation grid\n        if grid_type == 'equidistant':\n            x_grid = np.linspace(-1.0, 1.0, m)\n        elif grid_type == 'nodes':\n            x_grid = k_nodes\n        else:\n            raise ValueError(\"Invalid grid_type specified.\")\n\n        # Step 4: Evaluate the true function and interpolants on the grid\n        v_true = v(x_grid)\n        p_n_vals = p_n(x_grid)\n        s_n_vals = s_n(x_grid)\n        \n        # Step 5: Compute errors\n        errors_p = p_n_vals - v_true\n        errors_s = s_n_vals - v_true\n\n        # Step 6: Calculate the required error metrics\n        # RMSE\n        rmse_p = np.sqrt(np.mean(np.square(errors_p)))\n        rmse_s = np.sqrt(np.mean(np.square(errors_s)))\n        \n        # MAX\n        max_p = np.max(np.abs(errors_p))\n        max_s = np.max(np.abs(errors_s))\n\n        # Store the four computed quantities for the current case\n        case_results = [rmse_p, rmse_s, max_p, max_s]\n        all_results.append(case_results)\n\n    # Final Output Formatting:\n    # Build a list of strings, where each string represents an inner list, e.g., \"[r1,r2,r3,r4]\"\n    results_as_strings = [\n        f\"[{','.join(map(str, res))}]\" for res in all_results\n    ]\n    \n    # Join the list of strings with commas and enclose in the outermost brackets\n    final_output = f\"[{','.join(results_as_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"}]}