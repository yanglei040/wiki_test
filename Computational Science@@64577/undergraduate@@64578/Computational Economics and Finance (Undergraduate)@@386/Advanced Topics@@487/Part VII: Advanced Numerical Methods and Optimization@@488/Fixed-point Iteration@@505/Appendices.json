{"hands_on_practices": [{"introduction": "The journey to find a root of an equation $f(x)=0$ using fixed-point iteration begins with a crucial first step: rewriting the equation into the form $x = g(x)$. However, many such rearrangements are possible, and not all are created equal. This exercise [@problem_id:2214069] demonstrates how the choice of the function $g(x)$ dramatically impacts the speed of convergence by having you compare the convergence rates, determined by $|g'(x)|$, of two different iterative schemes designed to find the same root.", "id": "2214069", "problem": "The equation $f(x) = e^x - 2x - 1 = 0$ has a trivial root at $x=0$ and a unique positive root, which we denote by $\\alpha$. To numerically approximate this positive root $\\alpha$, two different fixed-point iteration schemes are proposed.\n\nScheme A is defined by the iteration function $g_A(x) = \\frac{e^x - 1}{2}$.\nScheme B is defined by the iteration function $g_B(x) = \\ln(2x+1)$.\n\nThe local behavior of a fixed-point iteration method, whether it converges to or diverges from a root, is characterized by its asymptotic rate constant. For an iteration function $g(x)$ and a root $\\alpha$, this constant is given by $C = |g'(\\alpha)|$. A value of $C < 1$ indicates convergence, while $C > 1$ indicates divergence.\n\nLet the rate constants for Scheme A and Scheme B at the root $\\alpha$ be $C_A = |g'_A(\\alpha)|$ and $C_B = |g'_B(\\alpha)|$, respectively.\n\nGiven that the positive root is $\\alpha \\approx 1.256431$, calculate the numerical value of the ratio $R = \\frac{C_A}{C_B}$.\n\nRound your final answer to four significant figures.\n\n", "solution": "We are given $f(x)=\\exp(x)-2x-1$ with a positive root $\\alpha$. Two fixed-point iterations are defined by $g_{A}(x)=\\frac{\\exp(x)-1}{2}$ and $g_{B}(x)=\\ln(2x+1)$. For a fixed-point iteration $x_{n+1}=g(x_{n})$ converging to $\\alpha$, the asymptotic rate constant is $C=|g'(\\alpha)|$.\n\nCompute derivatives:\n$$\ng_{A}'(x)=\\frac{1}{2}\\exp(x), \\quad g_{B}'(x)=\\frac{2}{2x+1}.\n$$\nTherefore,\n$$\nC_{A}=|g_{A}'(\\alpha)|=\\frac{1}{2}\\exp(\\alpha), \\quad C_{B}=|g_{B}'(\\alpha)|=\\frac{2}{2\\alpha+1}.\n$$\nThe ratio is\n$$\nR=\\frac{C_{A}}{C_{B}}=\\frac{\\frac{1}{2}\\exp(\\alpha)}{\\frac{2}{2\\alpha+1}}=\\frac{\\exp(\\alpha)\\,(2\\alpha+1)}{4}.\n$$\nSince $\\alpha$ satisfies $f(\\alpha)=0$, we have $\\exp(\\alpha)-2\\alpha-1=0$, hence $\\exp(\\alpha)=2\\alpha+1$. Substituting,\n$$\nR=\\frac{(2\\alpha+1)^{2}}{4}.\n$$\nWith $\\alpha\\approx 1.256431$,\n$$\n2\\alpha+1=2(1.256431)+1=3.512862,\n$$\n$$\n(2\\alpha+1)^{2}=(3.512862)^{2}=12.340199431044,\n$$\n$$\nR\\approx \\frac{12.340199431044}{4}=3.085049857761.\n$$\nRounding to four significant figures gives $R\\approx 3.085$.", "answer": "$$\\boxed{3.085}$$"}, {"introduction": "Many fundamental concepts in economics, such as market-clearing prices, are defined by equilibrium conditions where supply equals demand. These conditions often lead to equations that cannot be solved easily with pen and paper. This practice [@problem_id:2393811] provides a hands-on guide to translating a classic supply-and-demand model into a concrete fixed-point iteration scheme, illustrating a powerful method for numerically finding economic equilibrium points.", "id": "2393811", "problem": "Consider a competitive partial-equilibrium market for a non-durable good with price $p>0$. Market demand is given by $D(p)=\\dfrac{10}{p}$ and market supply is given by $S(p)=\\dfrac{3}{2}\\,p$. In equilibrium, the market-clearing price $p^{\\ast}$ satisfies the equation $f(p)=0$, where $f(p)=D(p)-S(p)=\\dfrac{10}{p}-\\dfrac{3}{2}\\,p$.\n\nUsing only the equation $f(p)=0$ as the starting point, define a fixed-point iteration $x_{n+1}=g(x_n)$ of the form\n$x_{n+1}=x_n-\\lambda\\,(x_n-c)\\,f(x_n)$\nwith the constants $c=1$ and $\\lambda=\\dfrac{1}{10}$. This construction yields an iteration with an extraneous fixed point at $p=c$ that is not a root of $f(p)$.\n\nProvide the explicit closed-form expression for the mapping $g(p)$ as a function of $p$. Your final answer must be a single closed-form analytic expression for $g(p)$ in terms of $p$. No numerical rounding is required.", "solution": "The problem requires the derivation of the explicit closed-form expression for the mapping $g(p)$, which defines a fixed-point iteration of the form $p_{n+1} = g(p_n)$. This iteration is designed to find a root of the function $f(p)=0$.\n\nThe specific form of the iteration mapping $g(p)$ is provided as:\n$$g(p) = p - \\lambda(p-c)f(p)$$\nThe problem supplies the function $f(p)$ and the constants $\\lambda$ and $c$.\n\nFirst, let us identify the provided components. The function $f(p)$ represents the excess demand in the market and is defined as $f(p) = D(p) - S(p)$. The demand function is given by $D(p)=\\dfrac{10}{p}$ and the supply function is $S(p)=\\dfrac{3}{2}p$. Therefore, the function $f(p)$ is:\n$$f(p) = \\dfrac{10}{p} - \\dfrac{3}{2}p$$\nThe constants for the iterative scheme are given as:\n$$c = 1$$\n$$\\lambda = \\dfrac{1}{10}$$\nOur task is to substitute these components into the definition of $g(p)$ and simplify the resulting expression.\n\nSubstituting the values of $\\lambda$ and $c$, and the expression for $f(p)$:\n$$g(p) = p - \\left(\\dfrac{1}{10}\\right)(p - 1)\\left(\\dfrac{10}{p} - \\dfrac{3}{2}p\\right)$$\nTo obtain the final closed-form expression, we must perform the algebraic expansion. We begin by expanding the product of the terms $(p - 1)$ and $\\left(\\dfrac{10}{p} - \\dfrac{3}{2}p\\right)$:\n$$(p - 1)\\left(\\dfrac{10}{p} - \\dfrac{3}{2}p\\right) = p\\left(\\dfrac{10}{p}\\right) - p\\left(\\dfrac{3}{2}p\\right) - 1\\left(\\dfrac{10}{p}\\right) + 1\\left(\\dfrac{3}{2}p\\right)$$\n$$= 10 - \\dfrac{3}{2}p^2 - \\dfrac{10}{p} + \\dfrac{3}{2}p$$\nNow, this expanded form is substituted back into the expression for $g(p)$:\n$$g(p) = p - \\dfrac{1}{10}\\left(10 - \\dfrac{3}{2}p^2 + \\dfrac{3}{2}p - \\dfrac{10}{p}\\right)$$\nNext, we distribute the factor of $-\\dfrac{1}{10}$ across the terms within the parentheses:\n$$g(p) = p - \\dfrac{10}{10} - \\left(\\dfrac{1}{10}\\right)\\left(-\\dfrac{3}{2}p^2\\right) - \\left(\\dfrac{1}{10}\\right)\\left(\\dfrac{3}{2}p\\right) - \\left(\\dfrac{1}{10}\\right)\\left(-\\dfrac{10}{p}\\right)$$\n$$g(p) = p - 1 + \\dfrac{3}{20}p^2 - \\dfrac{3}{20}p + \\dfrac{10}{10p}$$\n$$g(p) = p - 1 + \\dfrac{3}{20}p^2 - \\dfrac{3}{20}p + \\dfrac{1}{p}$$\nFinally, we collect terms with the same power of $p$. The terms linear in $p$ are $p$ and $-\\dfrac{3}{20}p$:\n$$g(p) = \\dfrac{3}{20}p^2 + \\left(1 - \\dfrac{3}{20}\\right)p - 1 + \\dfrac{1}{p}$$\nSimplifying the coefficient of the linear term:\n$$1 - \\dfrac{3}{20} = \\dfrac{20}{20} - \\dfrac{3}{20} = \\dfrac{17}{20}$$\nThis yields the final simplified expression for $g(p)$:\n$$g(p) = \\dfrac{3}{20}p^2 + \\dfrac{17}{20}p - 1 + \\dfrac{1}{p}$$\nThis is the required explicit closed-form analytic expression for the mapping $g(p)$.", "answer": "$$\\boxed{\\frac{3}{20}p^{2} + \\frac{17}{20}p - 1 + \\frac{1}{p}}$$"}, {"introduction": "While the Banach Fixed-Point Theorem guarantees convergence for contraction mappings, the speed can be frustratingly slow if the contraction factor is close to one, a common scenario in economic models. This advanced practice [@problem_id:2393814] introduces Aitken's $\\Delta^2$ method, a powerful technique for accelerating linearly convergent sequences. You will implement this method to significantly speed up the calculation of an asset price, demonstrating a vital tool in the computational economist's toolkit.", "id": "2393814", "problem": "Consider the linear asset-pricing problem for a perpetuity with constant dividend. In a risk-neutral, one-period model, the no-arbitrage pricing restriction for the time-$0$ price $p$ of a consol with constant dividend $d$ and discount factor $\\beta$ implies that the price satisfies the fixed-point relation $p = g(p)$, where $g(p)$ is a self-map on $\\mathbb{R}$. Specifically, under constant discount factor $\\beta$ with $|\\beta| &lt; 1$, and constant dividend $d$, the mapping is $g(p) = d + \\beta p$. The mapping $g$ is a contraction on $\\mathbb{R}$ with modulus $|\\beta|$, and hence the fixed-point iteration $p_{k+1} = g(p_k)$ converges linearly to the unique fixed point for any initial guess $p_0 \\in \\mathbb{R}$. The purpose of this exercise is to implement and compare the basic fixed-point iteration with an acceleration scheme based on Aitkenâ€™s $\\Delta^2$ process, which combines three consecutive iterates to accelerate a linearly converging sequence.\n\nYour tasks are:\n- From first principles, implement the basic fixed-point iteration for the mapping $g(p) = d + \\beta p$.\n- Derive and implement an Aitken $\\Delta^2$ acceleration step that uses three consecutive iterates from the basic fixed-point iteration to construct an accelerated iterate. Your implementation must be numerically robust: if the denominator required by the acceleration step is zero or numerically too small in magnitude (leading to potential division instability), then skip the acceleration in that cycle by proceeding with the unaccelerated iterate produced by the basic fixed-point iteration.\n- Use the absolute fixed-point residual $|g(x) - x|$ as the stopping criterion. Stop when $|g(x) - x| \\le \\text{tol}$, where $\\text{tol} &gt; 0$ is a given tolerance.\n\nImplementation details to enforce:\n- For the basic fixed-point iteration, each application of $g$ counts as one function evaluation. Report the number of iterations (which equals the number of function evaluations in this case).\n- For the Aitken-accelerated procedure, organize the computation in cycles. In each cycle, produce two consecutive iterates from the basic map $g$ starting from the current $x_0$ to obtain $x_1$ and $x_2$, then compute one accelerated iterate from $(x_0, x_1, x_2)$. Counting of function evaluations must include the two evaluations to produce $x_1$ and $x_2$, plus one more evaluation to compute the residual at the accelerated iterate (namely, evaluate $g(x_{\\text{acc}})$). Thus, a successful acceleration cycle typically uses three evaluations of $g$. If acceleration is skipped due to numerical safety, still evaluate the residual at the chosen iterate and count this evaluation. Continue cycling until the residual tolerance is met or a maximum number of cycles is reached.\n- Use the absolute fixed-point residual $|g(x) - x|$ with a given tolerance $\\text{tol}$ as the termination condition for both methods. Use the same maximum-iteration cap $N_{\\max}$ for both the number of basic iterations and the number of Aitken cycles, respectively.\n\nTest suite:\nFor each parameter tuple $(\\beta, d, p_0, \\text{tol}, N_{\\max})$ below, run both methods and collect results.\n\n1. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (0.9, 1.0, 0.0, 10^{-12}, 10000)$\n2. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (0.99, 1.0, 0.0, 10^{-12}, 10000)$\n3. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (-0.8, 1.0, 0.0, 10^{-12}, 10000)$\n4. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (0.0, 2.0, 5.0, 10^{-12}, 10000)$\n\nFor each test case, produce a list with the following six entries:\n- The approximate fixed point returned by the basic iteration (a float).\n- The total number of basic iterations performed (an integer).\n- The total number of function evaluations in the basic method (an integer).\n- The approximate fixed point returned by the Aitken-accelerated procedure (a float).\n- The total number of Aitken cycles performed (an integer).\n- The total number of function evaluations in the Aitken method (an integer).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of four inner lists (one for each test case) enclosed in square brackets. Each inner list must have the six entries in the order specified above. All floats must be rounded to $12$ decimals. For example, the output should look like $[[x_{11},x_{12},\\dots],[x_{21},x_{22},\\dots],\\dots]$ with the exact values for this problemâ€™s test cases.\n- No additional text should be printed.", "solution": "The problem presented is a well-defined exercise in computational economics, specifically in asset pricing, combined with a standard problem from numerical analysis concerning the acceleration of convergence for fixed-point iterations. The validation of the problem statement confirms that it is scientifically grounded, mathematically consistent, and computationally tractable. We shall therefore proceed with a full solution.\n\nThe core of the problem is to find the fixed point $p^*$ of the mapping $g: \\mathbb{R} \\to \\mathbb{R}$ defined by:\n$$ g(p) = d + \\beta p $$\nwhere $p$ is the price of a perpetuity, $d$ is a constant dividend, and $\\beta$ is a constant discount factor with $|\\beta| < 1$. The fixed point $p^*$ represents the no-arbitrage equilibrium price of the asset. Analytically, the solution is trivial to find by solving the equation $p = d + \\beta p$:\n$$ p(1 - \\beta) = d \\implies p^* = \\frac{d}{1 - \\beta} $$\nThis analytical solution serves as a benchmark for our numerical methods. The condition $|\\beta| < 1$ ensures that the mapping $g(p)$ is a contraction on $\\mathbb{R}$ with contraction factor $|\\beta|$, which guarantees the existence of a unique fixed point and the convergence of the basic fixed-point iteration from any starting point $p_0 \\in \\mathbb{R}$, according to the Banach Fixed-Point Theorem.\n\nWe will implement and compare two methods for finding this fixed point.\n\n**Method 1: Basic Fixed-Point Iteration**\n\nThis is the most direct application of the contraction mapping principle. Starting with an initial guess $p_0$, we generate a sequence $\\{p_k\\}_{k=0}^\\infty$ using the recurrence relation:\n$$ p_{k+1} = g(p_k) = d + \\beta p_k $$\nThe iteration proceeds until the absolute fixed-point residual is smaller than a specified tolerance $\\text{tol} > 0$. The stopping criterion is given by $|g(p_k) - p_k| \\le \\text{tol}$. However, to maintain that the number of iterations equals the number of function evaluations, we check the condition $|p_{k+1} - p_k| \\le \\text{tol}$, which is equivalent.\n\nThe algorithm is as follows:\n1. Initialize $p \\leftarrow p_0$, $k \\leftarrow 0$.\n2. For $k = 1, 2, \\dots, N_{\\max}$:\n   a. Compute $p_{\\text{new}} = g(p)$.\n   b. Calculate the residual: $R = |p_{\\text{new}} - p|$.\n   c. Update the iterate: $p \\leftarrow p_{\\text{new}}$.\n   d. If $R \\le \\text{tol}$, terminate the iteration and return $p$ as the solution. The number of iterations and function evaluations is $k$.\n3. If the loop completes without convergence, the procedure has failed.\nThe rate of convergence of this method is linear, with the error decreasing by a factor of approximately $|\\beta|$ at each step. When $|\\beta|$ is close to $1$, convergence can be exceedingly slow, which motivates the use of acceleration techniques.\n\n**Method 2: Aitken's $\\Delta^2$ Acceleration**\n\nAitken's $\\Delta^2$ process is a method to accelerate the convergence of a sequence that is converging linearly. Given three consecutive terms of a sequence, $(x_n, x_{n+1}, x_{n+2})$, an improved estimate of the limit, $x'_n$, is constructed.\n\nLet us derive the formula. For a linearly convergent sequence $\\{x_n\\}$ with limit $x^*$, we have $x_{n+1} - x^* \\approx c(x_n - x^*)$ for some constant $c$ with $|c|<1$. This implies $(x_{n+1} - x_n) \\approx (c-1)(x_n - x^*)$. By expressing $x_n - x^*$ from this and substituting back into $x_{n+1} - x^* \\approx c(x_n-x^*)$, we can solve for $x^*$. A more direct algebraic manipulation yields the standard formula. Let $\\Delta$ be the forward difference operator, $\\Delta x_n = x_{n+1} - x_n$. The accelerated iterate is given by:\n$$ x'_n = x_n - \\frac{(\\Delta x_n)^2}{\\Delta^2 x_n} = x_n - \\frac{(x_{n+1} - x_n)^2}{(x_{n+2} - x_{n+1}) - (x_{n+1} - x_n)} = x_n - \\frac{(x_{n+1} - x_n)^2}{x_{n+2} - 2x_{n+1} + x_n} $$\nThe denominator can become zero or numerically unstable if $x_{n+2} - 2x_{n+1} + x_n \\approx 0$. A robust implementation must check for this condition.\n\nThe computation is structured in cycles. Within each cycle, starting from a point $x_0$:\n1. Generate two standard fixed-point iterates: $x_1 = g(x_0)$ and $x_2 = g(x_1)$. This requires two function evaluations.\n2. Compute the denominator $D = x_2 - 2x_1 + x_0$.\n3. For numerical stability, if $|D|$ is smaller than a small threshold (e.g., $10^{-16}$), we deem the acceleration step unsafe. In this case, we discard the acceleration and proceed with the unaccelerated iterate, setting the next point $x_{\\text{next}} \\leftarrow x_2$.\n4. If the denominator is safe, compute the accelerated iterate: $x_{\\text{acc}} = x_0 - (x_1 - x_0)^2 / D$, and set $x_{\\text{next}} \\leftarrow x_{\\text{acc}}$.\n5. Check the stopping criterion. This requires a third function evaluation to compute the residual at the new point: $R = |g(x_{\\text{next}}) - x_{\\text{next}}|$.\n6. The next cycle begins with the new point $x_0 \\leftarrow x_{\\text{next}}$.\nEach cycle thus requires three function evaluations. The process continues until $R \\le \\text{tol}$ or the maximum number of cycles $N_{\\max}$ is reached.\n\nWe now proceed to implement these two algorithms and apply them to the specified test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares basic fixed-point iteration and Aitken-accelerated\n    iteration for an asset-pricing problem.\n    \"\"\"\n\n    test_cases = [\n        (0.9, 1.0, 0.0, 1e-12, 10000),\n        (0.99, 1.0, 0.0, 1e-12, 10000),\n        (-0.8, 1.0, 0.0, 1e-12, 10000),\n        (0.0, 2.0, 5.0, 1e-12, 10000),\n    ]\n\n    all_results = []\n\n    for beta, d, p0, tol, n_max in test_cases:\n        \n        # Define the mapping function g(p)\n        g = lambda p: d + beta * p\n\n        # --- Method 1: Basic Fixed-Point Iteration ---\n        p_basic = p0\n        evals_basic = 0\n        iters_basic = 0\n        for i in range(1, n_max + 1):\n            p_next = g(p_basic)\n            evals_basic += 1\n            iters_basic += 1\n            \n            residual = abs(p_next - p_basic)\n            p_basic = p_next\n            \n            if residual <= tol:\n                break\n        \n        # --- Method 2: Aitken's Delta^2 Acceleration ---\n        p_aitken = p0\n        evals_aitken = 0\n        cycles_aitken = 0\n        denominator_threshold = 1e-16 # For numerical stability\n\n        for k in range(1, n_max + 1):\n            cycles_aitken += 1\n            \n            x0 = p_aitken\n            \n            # Step 1: Generate two standard iterates\n            x1 = g(x0)\n            x2 = g(x1)\n            evals_aitken += 2\n            \n            # Step 2 & 3: Compute denominator and check for stability\n            denominator = x2 - 2 * x1 + x0\n            \n            p_next = 0.0\n            if abs(denominator) < denominator_threshold:\n                # Skip acceleration, proceed with standard iterate\n                p_next = x2\n            else:\n                # Step 4: Compute accelerated iterate\n                numerator = (x1 - x0)**2\n                p_next = x0 - numerator / denominator\n            \n            # Step 5: Check stopping criterion\n            g_p_next = g(p_next)\n            evals_aitken += 1\n            residual = abs(g_p_next - p_next)\n            \n            p_aitken = p_next\n            \n            if residual <= tol:\n                break\n        \n        case_results = [\n            p_basic, \n            iters_basic, \n            evals_basic,\n            p_aitken, \n            cycles_aitken, \n            evals_aitken\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in all_results:\n        p_b, i_b, e_b, p_a, c_a, e_a = res\n        s = f\"[{p_b:.12f},{i_b},{e_b},{p_a:.12f},{c_a},{e_a}]\"\n        formatted_results.append(s)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}]}