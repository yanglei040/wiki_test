{"hands_on_practices": [{"introduction": "This exercise takes you under the hood to work on the computational engine of an interior-point method. You will apply Newton's method to the barrier problem for a given portfolio optimization model, calculating the search direction and the associated Newton decrement. This practice solidifies the understanding of the core mechanics of a single IPM iteration and how progress towards the central path is measured. [@problem_id:2402656]", "id": "2402656", "problem": "Consider a portfolio allocation vector $w \\in \\mathbb{R}^3_{++}$ over $n=3$ assets with expected returns vector $r \\in \\mathbb{R}^3$. The objective is to maximize the Shannon entropy of the portfolio, given by $-\\sum_{i=1}^3 w_i \\ln(w_i)$, subject to a budget constraint and a target expected return constraint. Equivalently, this can be posed as the minimization problem\n$\\min_{w \\in \\mathbb{R}^3_{++}} \\sum_{i=1}^3 w_i \\ln(w_i)$\nsubject to the linear equality constraints $1^{\\top}w = 1$ and $r^{\\top}w = \\mu$, where $\\ln(\\cdot)$ denotes the natural logarithm. Consider the Interior-Point Method (IPM) with a barrier scaling parameter $t>0$ applied to the equality-constrained minimization of $t \\sum_{i=1}^3 w_i \\ln(w_i)$.\n\nAt the current iterate, you are given:\n- $r = \\begin{pmatrix}0.02 \\\\ 0.01 \\\\ 0.03\\end{pmatrix}$,\n- $\\mu = 0.02$,\n- $w^{(0)} = \\begin{pmatrix}0.4 \\\\ 0.4 \\\\ 0.2\\end{pmatrix}$,\n- $t = 10$,\n- the current dual multipliers for the constraints are $\\nu^{(0)} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\nUsing first principles from constrained optimization and the definition of the equality-constrained Newton step for the $t$-scaled problem (that is, by forming the Lagrangian with the linear constraints, writing down the first-order optimality conditions, and linearizing them at the given iterate), compute the Newton decrement squared\n$\\lambda^2 = \\Delta w^{\\top} \\nabla^2\\!\\big(t \\sum_{i=1}^3 w_i \\ln(w_i)\\big)\\, \\Delta w$\nat the given iterate $(w^{(0)}, \\nu^{(0)})$, where $\\Delta w$ is the equality-constrained Newton step direction for the $t$-scaled problem at $(w^{(0)}, \\nu^{(0)})$. Round your answer to four significant figures.", "solution": "The problem as stated is a standard exercise in the application of interior-point methods to a convex optimization problem, specifically entropy maximization under linear constraints, which is common in portfolio theory. The problem is well-posed, scientifically grounded, and contains all necessary information. It is therefore valid.\n\nWe are tasked with computing the Newton decrement squared, $\\lambda^2$, for an equality-constrained optimization problem. The objective function is $f_t(w) = t \\sum_{i=1}^3 w_i \\ln(w_i)$ with $t=10$. The constraints are linear and can be written in matrix form as $Aw = b$.\n\nFirst, we formalize the components of the problem.\nThe objective function is:\n$$ f_t(w) = 10 \\sum_{i=1}^3 w_i \\ln(w_i) $$\nThe constraints are $\\boldsymbol{1}^{\\top}w = 1$ and $r^{\\top}w = \\mu$. We define the constraint matrix $A$ and vector $b$ as:\n$$ A = \\begin{pmatrix} 1 & 1 & 1 \\\\ r_1 & r_2 & r_3 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0.02 & 0.01 & 0.03 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ \\mu \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.02 \\end{pmatrix} $$\nThe Newton step $(\\Delta w, \\nu^{+})$ at a given primal-dual point $(w, \\nu)$ is found by solving the linearized first-order optimality (KKT) conditions. The KKT system is:\n$$ \\begin{pmatrix} \\nabla^2 f_t(w) & A^\\top \\\\ A & 0 \\end{pmatrix} \\begin{pmatrix} \\Delta w \\\\ \\nu^{+} \\end{pmatrix} = \\begin{pmatrix} -\\nabla f_t(w) \\\\ b \\end{pmatrix} $$\nThis system is for a feasible starting point. Since our given point $w^{(0)}$ is not feasible (i.e., $A w^{(0)} \\neq b$), we solve for the step towards feasibility and optimality simultaneously:\n$$ \\begin{pmatrix} \\nabla^2 f_t(w^{(0)}) & A^\\top \\\\ A & 0 \\end{pmatrix} \\begin{pmatrix} \\Delta w \\\\ \\nu^{+} \\end{pmatrix} = \\begin{pmatrix} -\\nabla f_t(w^{(0)}) \\\\ b - A w^{(0)} \\end{pmatrix} $$\nHere, $\\nu^{+}$ is the next iterate for the dual variables, which we compute directly. The problem states $\\nu^{(0)}=\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, but this value is not directly needed for the standard Newton step formulation unless one is computing a step for $\\nu$ as well.\n\nWe proceed to compute the necessary components at the iterate $w^{(0)} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.2 \\end{pmatrix}$.\n\nThe gradient of $f_t(w)$ is $\\nabla f_t(w) = t (\\boldsymbol{1} + \\ln(w))$, where $\\ln(w)$ is applied element-wise.\n$$ g = \\nabla f_t(w^{(0)}) = 10 \\begin{pmatrix} 1 + \\ln(0.4) \\\\ 1 + \\ln(0.4) \\\\ 1 + \\ln(0.2) \\end{pmatrix} \\approx \\begin{pmatrix} 10 \\times (1 - 0.91629) \\\\ 10 \\times (1 - 0.91629) \\\\ 10 \\times (1 - 1.60944) \\end{pmatrix} = \\begin{pmatrix} 0.8371 \\\\ 0.8371 \\\\ -6.0944 \\end{pmatrix} $$\nThe Hessian of $f_t(w)$ is a diagonal matrix $\\nabla^2 f_t(w) = t \\cdot \\text{diag}(1/w_1, \\dots, 1/w_n)$.\n$$ H = \\nabla^2 f_t(w^{(0)}) = 10 \\cdot \\text{diag}\\left(\\frac{1}{0.4}, \\frac{1}{0.4}, \\frac{1}{0.2}\\right) = \\text{diag}(25, 25, 50) $$\nThe inverse Hessian is $H^{-1} = \\text{diag}(1/25, 1/25, 1/50) = \\text{diag}(0.04, 0.04, 0.02)$.\n\nThe constraint residual is:\n$$ r_{\\text{prim}} = b - A w^{(0)} = \\begin{pmatrix} 1 \\\\ 0.02 \\end{pmatrix} - \\begin{pmatrix} 1 & 1 & 1 \\\\ 0.02 & 0.01 & 0.03 \\end{pmatrix} \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.02 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0.018 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.002 \\end{pmatrix} $$\nFrom the KKT system, we can first solve for $\\nu^{+}$.\n$$ A \\Delta w = r_{\\text{prim}} $$\n$$ H \\Delta w + A^\\top \\nu^{+} = -g \\implies \\Delta w = -H^{-1}(g + A^\\top \\nu^{+}) $$\nSubstituting $\\Delta w$ into the first equation gives:\n$$ A(-H^{-1}(g + A^\\top \\nu^{+})) = r_{\\text{prim}} \\implies (A H^{-1} A^\\top) \\nu^{+} = -A H^{-1} g - r_{\\text{prim}} $$\nWe compute the matrix $A H^{-1} A^\\top$:\n$$ A H^{-1} A^\\top = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0.02 & 0.01 & 0.03 \\end{pmatrix} \\begin{pmatrix} 0.04 & 0 & 0 \\\\ 0 & 0.04 & 0 \\\\ 0 & 0 & 0.02 \\end{pmatrix} \\begin{pmatrix} 1 & 0.02 \\\\ 1 & 0.01 \\\\ 1 & 0.03 \\end{pmatrix} = \\begin{pmatrix} 0.1 & 0.0018 \\\\ 0.0018 & 0.000038 \\end{pmatrix} $$\nAnd the vector $A H^{-1} g$:\n$$ A H^{-1} g = \\begin{pmatrix} 0.04 & 0.04 & 0.02 \\\\ 0.0008 & 0.0004 & 0.0006 \\end{pmatrix} \\begin{pmatrix} 0.8371 \\\\ 0.8371 \\\\ -6.0944 \\end{pmatrix} \\approx \\begin{pmatrix} -0.05492 \\\\ -0.002652 \\end{pmatrix} $$\nThe system for $\\nu^{+}$ is:\n$$ \\begin{pmatrix} 0.1 & 0.0018 \\\\ 0.0018 & 0.000038 \\end{pmatrix} \\nu^{+} = -\\begin{pmatrix} -0.05492 \\\\ -0.002652 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.002 \\end{pmatrix} = \\begin{pmatrix} 0.05492 \\\\ 0.000652 \\end{pmatrix} $$\nSolving this $2 \\times 2$ system yields $\\nu^{+} \\approx \\begin{pmatrix} 1.6304 \\\\ -60.066 \\end{pmatrix}$.\n\nNow, we compute the Newton step $\\Delta w$:\n$$ \\Delta w = -H^{-1}(g + A^\\top \\nu^{+}) $$\n$$ A^\\top \\nu^{+} \\approx \\begin{pmatrix} 1 & 0.02 \\\\ 1 & 0.01 \\\\ 1 & 0.03 \\end{pmatrix} \\begin{pmatrix} 1.6304 \\\\ -60.066 \\end{pmatrix} = \\begin{pmatrix} 1.6304 - 1.20132 \\\\ 1.6304 - 0.60066 \\\\ 1.6304 - 1.80198 \\end{pmatrix} = \\begin{pmatrix} 0.42908 \\\\ 1.02974 \\\\ -0.17158 \\end{pmatrix} $$\n$$ g + A^\\top \\nu^{+} \\approx \\begin{pmatrix} 0.8371 \\\\ 0.8371 \\\\ -6.0944 \\end{pmatrix} + \\begin{pmatrix} 0.42908 \\\\ 1.02974 \\\\ -0.17158 \\end{pmatrix} = \\begin{pmatrix} 1.26618 \\\\ 1.86684 \\\\ -6.26598 \\end{pmatrix} $$\n$$ \\Delta w = -\\begin{pmatrix} 0.04 \\\\ 0.04 \\\\ 0.02 \\end{pmatrix} \\odot \\begin{pmatrix} 1.26618 \\\\ 1.86684 \\\\ -6.26598 \\end{pmatrix} = \\begin{pmatrix} -0.0506472 \\\\ -0.0746736 \\\\ 0.1253196 \\end{pmatrix} $$\nFinally, we compute the Newton decrement squared, $\\lambda^2$, using its definition:\n$$ \\lambda^2 = \\Delta w^{\\top} H \\Delta w $$\n$$ \\lambda^2 \\approx \\begin{pmatrix} -0.0506472 & -0.0746736 & 0.1253196 \\end{pmatrix} \\begin{pmatrix} 25 & 0 & 0 \\\\ 0 & 25 & 0 \\\\ 0 & 0 & 50 \\end{pmatrix} \\begin{pmatrix} -0.0506472 \\\\ -0.0746736 \\\\ 0.1253196 \\end{pmatrix} $$\n$$ \\lambda^2 \\approx 25(-0.0506472)^2 + 25(-0.0746736)^2 + 50(0.1253196)^2 $$\n$$ \\lambda^2 \\approx 25(0.00256514) + 25(0.00557615) + 50(0.0157050) $$\n$$ \\lambda^2 \\approx 0.0641285 + 0.13940375 + 0.78525 $$\n$$ \\lambda^2 \\approx 0.98878225 $$\nRounding the result to four significant figures gives $0.9888$.", "answer": "$$\\boxed{0.9888}$$"}, {"introduction": "Not every well-posed model has a feasible solution, and a robust algorithm must be able to detect and prove this. This practice demonstrates how to use the final dual iterates from an IPM that has declared a problem infeasible to construct a formal proof of infeasibility, known as a Farkas certificate. This bridges the gap between the computational output of a solver and the fundamental theory of linear inequalities. [@problem_id:2402685]", "id": "2402685", "problem": "Consider the long-only portfolio feasibility problem expressed in linear programming form: find $x \\in \\mathbb{R}^{3}$ such that $x \\geq \\mathbf{0}$ and $A x = b$, where\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 1 & 1 \\\\\n0.10 & 0.20 & 0.15\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\\n0.22\n\\end{pmatrix}.\n$$\nThe first row $1 \\cdot x_{1} + 1 \\cdot x_{2} + 1 \\cdot x_{3} = 1$ is the budget constraint, and the second row $0.10 x_{1} + 0.20 x_{2} + 0.15 x_{3} = 0.22$ is a target expected return requirement. An interior-point method applied to the homogeneous self-dual embedding of this feasibility problem terminates declaring infeasibility and returns a final dual iterate approximately\n$$\n\\tilde{y} \\;=\\; \\begin{pmatrix} 0.1997 \\\\ -1.0030 \\end{pmatrix}.\n$$\nUsing the sign information in $\\tilde{y}$ to fix a normalization, construct a Farkas certificate of infeasibility $y \\in \\mathbb{R}^{2}$ satisfying $A^{\\top} y \\geq \\mathbf{0}$ and $b^{\\top} y < 0$, with the normalization $y_{2} = -1$ and with $y_{1}$ chosen as small as possible subject to these inequalities. Compute the scalar $b^{\\top} y$. No rounding is required; provide an exact value.", "solution": "The problem requires the construction of a Farkas certificate of infeasibility for a given linear system. The feasibility problem is to find a vector $x \\in \\mathbb{R}^{3}$ such that $A x = b$ and $x \\geq \\mathbf{0}$. The provided matrices are\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 1 & 1 \\\\\n0.10 & 0.20 & 0.15\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\\n0.22\n\\end{pmatrix}.\n$$\nAccording to Farkas's Lemma (a theorem of the alternative), the system $A x = b, x \\geq \\mathbf{0}$ is infeasible if and only if there exists a vector $y \\in \\mathbb{R}^{2}$ satisfying the two conditions:\n$1.$ $A^{\\top} y \\geq \\mathbf{0}$\n$2.$ $b^{\\top} y < 0$\nSuch a vector $y$ is called a Farkas certificate of infeasibility. The problem instructs us to find such a certificate.\n\nFirst, we write the transpose of matrix $A$:\n$$\nA^{\\top} \\;=\\; \\begin{pmatrix}\n1 & 0.10 \\\\\n1 & 0.20 \\\\\n1 & 0.15\n\\end{pmatrix}\n$$\nLet the certificate vector be $y = \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix}$. The condition $A^{\\top} y \\geq \\mathbf{0}$ expands into a system of three linear inequalities:\n$$\n\\begin{pmatrix}\n1 & 0.10 \\\\\n1 & 0.20 \\\\\n1 & 0.15\n\\end{pmatrix}\n\\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\ny_{1} + 0.10 y_{2} \\\\\ny_{1} + 0.20 y_{2} \\\\\ny_{1} + 0.15 y_{2}\n\\end{pmatrix}\n\\;\\geq\\;\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives us:\n(i) $y_{1} + 0.10 y_{2} \\geq 0$\n(ii) $y_{1} + 0.20 y_{2} \\geq 0$\n(iii) $y_{1} + 0.15 y_{2} \\geq 0$\n\nThe second condition, $b^{\\top} y < 0$, becomes:\n$$\nb^{\\top} y \\;=\\; \\begin{pmatrix} 1 & 0.22 \\end{pmatrix} \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix} \\;=\\; y_{1} + 0.22 y_{2} < 0\n$$\n\nThe problem specifies a normalization for the certificate, $y_{2} = -1$. This normalization is consistent with the sign structure of the approximate dual iterate $\\tilde{y}$ provided. Substituting $y_{2} = -1$ into our system of inequalities:\n(i) $y_{1} + 0.10(-1) \\geq 0 \\implies y_{1} \\geq 0.10$\n(ii) $y_{1} + 0.20(-1) \\geq 0 \\implies y_{1} \\geq 0.20$\n(iii) $y_{1} + 0.15(-1) \\geq 0 \\implies y_{1} \\geq 0.15$\n(iv) $y_{1} + 0.22(-1) < 0 \\implies y_{1} < 0.22$\n\nTo satisfy inequalities (i), (ii), and (iii) simultaneously, $y_{1}$ must be greater than or equal to the maximum of the lower bounds:\n$$\ny_{1} \\geq \\max(0.10, 0.20, 0.15) = 0.20\n$$\nCombining this with inequality (iv), we obtain the full condition on $y_{1}$:\n$$\n0.20 \\leq y_{1} < 0.22\n$$\nThe problem demands that we choose $y_{1}$ to be as small as possible. The set of valid values for $y_{1}$ is the interval $[0.20, 0.22)$. The minimum value in this set is its lower bound, $y_{1} = 0.20$.\n\nWith this selection, our Farkas certificate is:\n$$\ny = \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix} = \\begin{pmatrix} 0.20 \\\\ -1 \\end{pmatrix}\n$$\nWe must verify that this vector satisfies all conditions.\n$A^{\\top}y = \\begin{pmatrix} 0.20 + 0.10(-1) \\\\ 0.20 + 0.20(-1) \\\\ 0.20 + 0.15(-1) \\end{pmatrix} = \\begin{pmatrix} 0.10 \\\\ 0 \\\\ 0.05 \\end{pmatrix}$, which is component-wise greater than or equal to $\\mathbf{0}$.\n$b^{\\top}y = 0.20 + 0.22(-1) = 0.20 - 0.22 = -0.02$, which is strictly less than $0$.\nBoth conditions are satisfied.\n\nThe final task is to compute the scalar $b^{\\top} y$. We have already calculated this value.\n$$\nb^{\\top} y = 0.20 - 0.22 = -0.02\n$$\nAlternatively, using fractions:\n$y_{1} \\geq \\max(\\frac{1}{10}, \\frac{1}{5}, \\frac{3}{20}) = \\frac{1}{5}$.\n$y_{1} < \\frac{22}{100} = \\frac{11}{50}$.\nThe minimum value is $y_{1} = \\frac{1}{5}$.\n$b^{\\top} y = y_{1} + \\frac{11}{50}y_{2} = \\frac{1}{5} + \\frac{11}{50}(-1) = \\frac{10}{50} - \\frac{11}{50} = -\\frac{1}{50}$.\nThe values $-0.02$ and $-\\frac{1}{50}$ are identical.", "answer": "$$\\boxed{-0.02}$$"}, {"introduction": "The theoretical convergence of an IPM is one thing; its practical performance is another, and it can depend heavily on the starting point. This exercise explores the crucial, high-level concepts of centrality and numerical conditioning that govern an IPM's efficiency. You will analyze how different initial feasible points can lead to faster or slower convergence, teaching the 'art' of effectively using these powerful algorithms. [@problem_id:2402652]", "id": "2402652", "problem": "Consider a discrete-time optimal trade execution problem over $T \\in \\mathbb{N}$ periods. Let $x \\in \\mathbb{R}^T$ denote the vector of trades (positive values are sells) and suppose the risk-adjusted expected implementation cost is modeled by the convex quadratic objective\n$$\n\\min_{x \\in \\mathbb{R}^T} \\;\\; \\tfrac{1}{2} x^{\\top} Q x + g^{\\top} x,\n$$\nwhere $Q \\in \\mathbb{R}^{T \\times T}$ is symmetric positive definite, aggregating temporary price impact and a risk penalty, and $g \\in \\mathbb{R}^T$ captures exogenous drift or fees. The liquidation requirement is encoded by the linear equality $a^{\\top} x = q$, where $a = \\mathbf{1} \\in \\mathbb{R}^T$ and $q \\in \\mathbb{R}$. Per-period participation constraints impose bound inequalities $l < x < u$ with vectors $l,u \\in \\mathbb{R}^T$ satisfying $l_i < u_i$ for all $i \\in \\{1,\\dots,T\\}$, and such that there exists at least one strictly feasible $x$ with $a^{\\top} x = q$ and $l < x < u$.\n\nIntroduce lower and upper slack variables $s_{\\ell} = x - l \\in \\mathbb{R}^T$ and $s_{u} = u - x \\in \\mathbb{R}^T$, and Lagrange multipliers $y \\in \\mathbb{R}$ for the equality, and $z_{\\ell}, z_{u} \\in \\mathbb{R}^T$ for the bounds. The Karush-Kuhn-Tucker (KKT) optimality conditions for this convex quadratic program are\n$$\n\\begin{aligned}\n&\\text{primal feasibility:} && a^{\\top} x = q,\\;\\; s_{\\ell} = x - l,\\;\\; s_{u} = u - x,\\;\\; s_{\\ell} \\ge 0,\\;\\; s_{u} \\ge 0,\\\\\n&\\text{dual feasibility:} && Q x + g + a y - z_{\\ell} + z_{u} = 0,\\;\\; z_{\\ell} \\ge 0,\\;\\; z_{u} \\ge 0,\\\\\n&\\text{complementarity:} && s_{\\ell,i} z_{\\ell,i} = 0,\\;\\; s_{u,i} z_{u,i} = 0 \\quad \\text{for all } i \\in \\{1,\\dots,T\\}.\n\\end{aligned}\n$$\nA primal-dual Interior-Point Method (IPM) aims to approach the central path, which replaces complementarity by the barrier relations $s_{\\ell,i} z_{\\ell,i} = \\mu$ and $s_{u,i} z_{u,i} = \\mu$ for all $i$ with barrier parameter $\\mu > 0$, while maintaining strict positivity $s_{\\ell} > 0$, $s_{u} > 0$, $z_{\\ell} > 0$, $z_{u} > 0$. In each iteration, a Newton step for the perturbed KKT system is computed from the current strictly feasible primal-dual point $(x, y, s_{\\ell}, s_{u}, z_{\\ell}, z_{u})$, and a step length is chosen to preserve strict positivity.\n\nSuppose you run a primal-dual IPM on the problem above. You consider different choices of initial strictly feasible points $x_0$ (with associated strictly positive slacks $s_{\\ell,0}, s_{u,0}$ and multipliers $z_{\\ell,0}, z_{u,0}$ satisfying $a^{\\top} x_0 = q$, $l < x_0 < u$). How can the choice of $x_0$ affect the first few IPM iterations?\n\nSelect all statements that are valid consequences of the governing equations and feasibility-positivity requirements:\n\nA. If the initial complementarity products $\\{s_{\\ell,0,i} z_{\\ell,0,i}\\}_{i=1}^T$ and $\\{s_{u,0,i} z_{u,0,i}\\}_{i=1}^T$ are highly dispersed across $i$ (some much larger, some much smaller than their average), then the first one or two primal-dual steps are typically forced to use smaller step lengths to maintain $s_{\\ell} > 0$, $s_{u} > 0$, $z_{\\ell} > 0$, $z_{u} > 0$.\n\nB. If $(x_0, y_0, s_{\\ell,0}, s_{u,0}, z_{\\ell,0}, z_{u,0})$ lies close to the central path in the sense that $s_{\\ell,0,i} z_{\\ell,0,i} \\approx \\bar{\\mu}$ and $s_{u,0,i} z_{u,0,i} \\approx \\bar{\\mu}$ for a common $\\bar{\\mu} > 0$ and all $i$, then the coefficient scalings in the first Newton system are more balanced, which tends to allow larger primal-dual step lengths and more rapid early reduction in the duality gap.\n\nC. Choosing $x_0$ that nearly minimizes the quadratic objective, regardless of its associated slacks and multipliers, always reduces the number of IPM iterations needed to reach a given accuracy.\n\nD. If $x_0$ is strictly feasible but poorly scaled relative to $Q$ (for example, some components of $x_0$ are extremely close to $l$ while others are extremely close to $u$, leading to extreme values in the ratios $z_{\\ell,0,i} / s_{\\ell,0,i}$ and $z_{u,0,i} / s_{u,0,i}$), then the initial linear systems solved in the Newton step can be more ill-conditioned, potentially slowing progress in the first few iterations.\n\nE. For convex quadratic optimal execution with equality and bound constraints as above, the first Newton direction in a primal-dual IPM is independent of the particular strictly feasible $x_0$ and depends only on the choice of barrier parameter $\\mu$.", "solution": "The problem presented is a standard convex Quadratic Program (QP) arising in the field of quantitative finance, specifically for optimal trade execution. The formulation is mathematically sound, well-posed, and objective. The matrix $Q$ being symmetric positive definite guarantees a strictly convex objective function, which in turn ensures a unique optimal solution. The assumption of the existence of a strictly feasible point is standard and necessary for the application of a primal-dual Interior-Point Method (IPM). The problem is valid, and we may proceed with the analysis.\n\nThe core of each iteration in a primal-dual IPM is the calculation of a Newton step $(\\Delta x, \\Delta y, \\Delta s_{\\ell}, \\Delta s_{u}, \\Delta z_{\\ell}, \\Delta z_{u})$ from the current strictly feasible iterate $(x, y, s_{\\ell}, s_{u}, z_{\\ell}, z_{u})$. This step is found by solving a linear system derived from the first-order Taylor expansion of the perturbed Karush-Kuhn-Tucker (KKT) conditions. The perturbed KKT system, with barrier parameter $\\mu > 0$, includes the complementarity conditions $s_{\\ell,i} z_{\\ell,i} = \\mu$ and $s_{u,i} z_{u,i} = \\mu$ for all $i \\in \\{1,\\dots,T\\}$.\n\nBy eliminating the slack and dual variable steps $\\Delta s_{\\ell}, \\Delta s_{u}, \\Delta z_{\\ell}, \\Delta z_{u}$, the Newton step for the primary variables $(\\Delta x, \\Delta y)$ is determined by solving the so-called augmented system or KKT system:\n$$\n\\begin{bmatrix}\nQ + D & a \\\\\na^{\\top} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta x \\\\\n\\Delta y\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nr_1 \\\\\nr_2\n\\end{bmatrix}\n$$\nwhere $D$ is a diagonal matrix given by $D = S_{\\ell}^{-1}Z_{\\ell} + S_{u}^{-1}Z_{u}$, with $S_{\\ell} = \\text{diag}(s_{\\ell})$, $Z_{\\ell} = \\text{diag}(z_{\\ell})$, $S_{u} = \\text{diag}(s_{u})$, and $Z_{u} = \\text{diag}(z_{u})$. The diagonal entries of $D$ are $D_{ii} = \\frac{z_{\\ell,i}}{s_{\\ell,i}} + \\frac{z_{u,i}}{s_{u,i}}$. The right-hand side vector $(r_1, r_2)$ depends on the current iterate and the residuals of the KKT conditions. The variables $s_{\\ell}$ and $s_{u}$ are themselves functions of $x$, namely $s_{\\ell} = x-l$ and $s_{u}=u-x$.\n\nThe new iterate is then found by taking a step of length $\\alpha > 0$:\n$$ (x, y, s, z)^{k+1} = (x, y, s, z)^{k} + \\alpha (\\Delta x, \\Delta y, \\Delta s, \\Delta z)^{k} $$\nThe step length $\\alpha$ is chosen to be less than $1$ and small enough to ensure the strict positivity of all slack variables and their corresponding dual multipliers, i.e., $s_{\\ell} > 0$, $s_{u} > 0$, $z_{\\ell} > 0$, and $z_{u} > 0$. Specifically, $\\alpha$ is limited by the smallest ratio of a variable to its negative change:\n$$ \\alpha \\le \\eta \\min \\left( \\min_{i: \\Delta s_{\\ell,i}<0} \\frac{-s_{\\ell,i}}{\\Delta s_{\\ell,i}}, \\min_{i: \\Delta s_{u,i}<0} \\frac{-s_{u,i}}{\\Delta s_{u,i}}, \\min_{i: \\Delta z_{\\ell,i}<0} \\frac{-z_{\\ell,i}}{\\Delta z_{\\ell,i}}, \\min_{i: \\Delta z_{u,i}<0} \\frac{-z_{u,i}}{\\Delta z_{u,i}} \\right) $$\nwhere $\\eta \\in (0,1)$ is a safety factor (e.g., $\\eta = 0.995$).\n\nWith this framework, we shall evaluate each statement.\n\n**A. If the initial complementarity products $\\{s_{\\ell,0,i} z_{\\ell,0,i}\\}_{i=1}^T$ and $\\{s_{u,0,i} z_{u,0,i}\\}_{i=1}^T$ are highly dispersed across $i$ (some much larger, some much smaller than their average), then the first one or two primal-dual steps are typically forced to use smaller step lengths to maintain $s_{\\ell} > 0$, $s_{u} > 0$, $z_{\\ell} > 0$, $z_{u} > 0$.**\n\nThis statement addresses the \"centrality\" of the starting point. The central path is defined by points where all complementarity products are equal. High dispersion means the point is far from the central path. The Newton step aims to correct both for feasibility and centrality. The centrality correction part of the step will attempt to push the small products up and the large products down. This can result in a large Newton step component $(\\Delta s_i, \\Delta z_i)$ relative to the current value $(s_i, z_i)$ for some component $i$. If, for instance, a variable $s_{\\ell,i}$ is small and its corresponding step $\\Delta s_{\\ell,i}$ is large and negative, the ratio $-s_{\\ell,i}/\\Delta s_{\\ell,i}$ will be very small. This severely restricts the maximum allowable step length $\\alpha$ to maintain positivity. Thus, starting far from the central path, as characterized by high dispersion, forces the algorithm to take short, corrective steps to move closer to the path before longer steps that reduce the duality gap can be taken.\n\nVerdict: **Correct**.\n\n**B. If $(x_0, y_0, s_{\\ell,0}, s_{u,0}, z_{\\ell,0}, z_{u,0})$ lies close to the central path in the sense that $s_{\\ell,0,i} z_{\\ell,0,i} \\approx \\bar{\\mu}$ and $s_{u,0,i} z_{u,0,i} \\approx \\bar{\\mu}$ for a common $\\bar{\\mu} > 0$ and all $i$, then the coefficient scalings in the first Newton system are more balanced, which tends to allow larger primal-dual step lengths and more rapid early reduction in the duality gap.**\n\nIf the starting point is close to the central path, the complementarity residuals are small, and the Newton step is dominated by the affine-scaling component, which is primarily directed at reducing the duality gap. The required correction to restore centrality is minimal. Consequently, the search direction is less likely to aggressively point towards the boundary of the feasible region for any particular component. This allows for a larger step length $\\alpha$, often close to $1$. A larger $\\alpha$ means a more substantial reduction in the barrier parameter $\\mu$ and thus a faster reduction in the duality gap, which is proportional to the average of the complementarity products. The claim about \"balanced scalings\" is also related; while centrality alone does not guarantee well-conditioned matrices, it avoids the extreme imbalances that arise when trying to center a point very near a boundary. The primary consequence is the ability to take large steps.\n\nVerdict: **Correct**.\n\n**C. Choosing $x_0$ that nearly minimizes the quadratic objective, regardless of its associated slacks and multipliers, always reduces the number of IPM iterations needed to reach a given accuracy.**\n\nThis statement is fundamentally misguided. The efficiency of an IPM depends critically on staying \"in the interior\" of the feasible set, not on how close the initial point is to the optimal solution. An optimal solution $x^*$ often lies on the boundary of the feasible region, meaning some inequality constraints are active (i.e., $x^*_i = l_i$ or $x^*_i = u_i$ for some $i$). Choosing an initial $x_0$ very close to $x^*$ will likely mean that for some $i$, $s_{\\ell,0,i} = x_{0,i} - l_i$ or $s_{u,0,i} = u_i - x_{0,i}$ is extremely small. This \"hugging\" of the boundary is precisely what IPMs are designed to avoid. It leads to extremely small step lengths to maintain positivity (a phenomenon called \"stalling\") and, as discussed in option D, severe ill-conditioning of the linear systems. A central point with a much worse objective value is a far superior starting point for an IPM. The word \"always\" makes the statement particularly strong, and it is demonstrably false.\n\nVerdict: **Incorrect**.\n\n**D. If $x_0$ is strictly feasible but poorly scaled relative to $Q$ (for example, some components of $x_0$ are extremely close to $l$ while others are extremely close to $u$, leading to extreme values in the ratios $z_{\\ell,0,i} / s_{\\ell,0,i}$ and $z_{u,0,i} / s_{u,0,i}$), then the initial linear systems solved in the Newton step can be more ill-conditioned, potentially slowing progress in the first few iterations.**\n\nThis statement correctly identifies a primary source of numerical difficulty in IPMs. The conditioning of the KKT matrix that is inverted at each step, specifically the block $Q + D = Q + S_{\\ell}^{-1}Z_{\\ell} + S_{u}^{-1}Z_{u}$, is critical. If a component $x_{0,i}$ is very close to its lower bound $l_i$, its slack $s_{\\ell,0,i}$ is very small. To satisfy the perturbed complementarity condition $s_{\\ell,0,i} z_{\\ell,0,i} \\approx \\mu_0$, the corresponding dual variable $z_{\\ell,0,i}$ must be large. This causes the ratio $\\frac{z_{\\ell,0,i}}{s_{\\ell,0,i}}$, which is a diagonal entry in $D$, to become extremely large. If different components of $x_0$ have vastly different proximities to their bounds, the diagonal matrix $D$ will have entries spanning many orders of magnitude. This makes the matrix $Q+D$ ill-conditioned. Solving a linear system with an ill-conditioned matrix is numerically unstable and computationally expensive, which slows down or even halts the progress of the IPM.\n\nVerdict: **Correct**.\n\n**E. For convex quadratic optimal execution with equality and bound constraints as above, the first Newton direction in a primal-dual IPM is independent of the particular strictly feasible $x_0$ and depends only on the choice of barrier parameter $\\mu$.**\n\nThis statement is patently false. The Newton step is, by its very definition, a local quantity computed at the current iterate. Let us examine the KKT matrix and the right-hand side of the Newton system. The matrix contains the term $D = S_{\\ell}^{-1}Z_{\\ell} + S_{u}^{-1}Z_{u}$, which is explicitly a function of the current iterate's slacks and dual variables, $(s, z)$, which in turn depend on the primal variable $x$. The right-hand side vector $(r_1, r_2)$ is composed of the residuals of the KKT conditions at the current point, which also fundamentally depend on $(x, y, s, z)$. Since both the matrix defining the linear system and its right-hand side depend on the initial point $(x_0, y_0, s_{\\ell,0}, s_{u,0}, z_{\\ell,0}, z_{u,0})$, the resulting solution—the Newton direction—must also depend on this initial point. A different starting position results in a different linearization of the KKT system and thus a different search direction.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ABD}$$"}]}