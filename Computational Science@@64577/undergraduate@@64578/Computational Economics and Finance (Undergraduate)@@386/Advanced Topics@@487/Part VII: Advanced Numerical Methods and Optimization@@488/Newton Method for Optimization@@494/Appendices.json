{"hands_on_practices": [{"introduction": "Newton's method offers remarkable speed, but its convergence is not unconditional. This exercise explores one of its classic failure modes: a stable cycle. By analyzing the update rule, you will discover how a seemingly reasonable utility function can trap the algorithm in an oscillation between two points, preventing it from ever reaching the optimum. This thought experiment is crucial for understanding why the pure, unguarded Newton's method is rarely used in production and why safeguards are essential. [@problem_id:2414715]", "id": "2414715", "problem": "In computational economics and finance, iterative second-order methods are often used to maximize a univariate utility index. Let $u:\\mathbb{R}\\to\\mathbb{R}$ be twice continuously differentiable. The Newton update for maximizing $u$ from a current iterate $x_k$ is defined by\n$$\nx_{k+1}\\;=\\;x_k\\;-\\;\\frac{u'(x_k)}{u''(x_k)}\\,,\n$$\nwhenever $u''(x_k)\\neq 0$. A strict $2$-cycle between two points $x=a$ and $x=b$ means that $x_{k+1}=b$ when $x_k=a$ and $x_{k+1}=a$ when $x_k=b$, with $u'(a)\\neq 0$, $u'(b)\\neq 0$, $u''(a)\\neq 0$, and $u''(b)\\neq 0$.\n\nWhich of the following utility functions generates a strict $2$-cycle between $x=0$ and $x=1$ under the Newton update for maximization (that is, $x_{k+1}=1$ when $x_k=0$ and $x_{k+1}=0$ when $x_k=1$), with well-defined updates and nonzero first and second derivatives at $x=0$ and $x=1$?\n\nA. $u(x)=\\dfrac{x^3}{3}-\\dfrac{x^2}{2}+x$\n\nB. $u(x)=-\\dfrac{1}{2}\\left(x-\\dfrac{1}{2}\\right)^2$\n\nC. $u(x)=\\dfrac{x^3}{3}+\\dfrac{x^2}{2}+x$\n\nD. $u(x)=\\dfrac{x^3}{3}-x$", "solution": "The problem statement will first be subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe explicit information provided in the problem statement is as follows:\n- The utility index is a function $u:\\mathbb{R}\\to\\mathbb{R}$ which is twice continuously differentiable.\n- The Newton update for maximizing $u(x)$ from an iterate $x_k$ is given by the formula:\n$$\nx_{k+1} = x_k - \\frac{u'(x_k)}{u''(x_k)}\n$$\nThis update is defined only when $u''(x_k) \\neq 0$.\n- A strict $2$-cycle between two distinct points $x=a$ and $x=b$ is defined by the following conditions:\n    1.  When $x_k=a$, then $x_{k+1}=b$.\n    2.  When $x_k=b$, then $x_{k+1}=a$.\n    3.  $u'(a) \\neq 0$ and $u'(b) \\neq 0$.\n    4.  $u''(a) \\neq 0$ and $u''(b) \\neq 0$.\n- The specific problem requires identifying a utility function that generates a strict $2$-cycle between $a=0$ and $b=1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to scrutiny based on the established criteria.\n- **Scientifically Grounded**: The problem is based on Newton's method, a fundamental algorithm in numerical analysis, applied to optimization. The concept of fixed points and cycles is a standard topic in the study of dynamical systems generated by such iterative methods. This is an entirely valid scientific and mathematical context.\n- **Well-Posed**: The problem provides a clear and unambiguous definition of a strict $2$-cycle and the specific iterative map. It asks to identify which of the given functions satisfies these explicitly stated mathematical conditions. The task is well-defined and allows for a unique determination among the given options.\n- **Objective**: The problem is stated using formal mathematical language and definitions. There is no subjectivity, ambiguity, or opinion-based content.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. There are no contradictions, missing information, or other logical flaws. We may therefore proceed with the derivation of a solution.\n\n### Solution Derivation\nThe conditions for a strict $2$-cycle between the points $a=0$ and $b=1$ must be translated into equations involving the utility function $u(x)$ and its derivatives.\n\nThe first condition is that an iteration starting at $x_k=0$ results in $x_{k+1}=1$. Applying the Newton update formula:\n$$\n1 = 0 - \\frac{u'(0)}{u''(0)}\n$$\nThis simplifies to:\n$$\n1 = -\\frac{u'(0)}{u''(0)} \\implies u''(0) = -u'(0)\n$$\n\nThe second condition is that an iteration starting at $x_k=1$ results in $x_{k+1}=0$. Applying the Newton update formula again:\n$$\n0 = 1 - \\frac{u'(1)}{u''(1)}\n$$\nThis simplifies to:\n$$\n1 = \\frac{u'(1)}{u''(1)} \\implies u''(1) = u'(1)\n$$\n\nAdditionally, the definition of a *strict* $2$-cycle with *well-defined* updates imposes the following non-zero constraints:\n1.  $u'(0) \\neq 0$\n2.  $u'(1) \\neq 0$\n3.  $u''(0) \\neq 0$\n4.  $u''(1) \\neq 0$\n\nNotice that if $u'(0) \\neq 0$, the condition $u''(0) = -u'(0)$ automatically implies $u''(0) \\neq 0$. Similarly, if $u'(1) \\neq 0$, the condition $u''(1) = u'(1)$ implies $u''(1) \\neq 0$. Therefore, we only need to check four primary conditions for each candidate function:\n1.  $u''(0) = -u'(0)$\n2.  $u''(1) = u'(1)$\n3.  $u'(0) \\neq 0$\n4.  $u'(1) \\neq 0$\n\nWe shall now evaluate each option against these four conditions.\n\n### Option-by-Option Analysis\n\n**A. $u(x)=\\dfrac{x^3}{3}-\\dfrac{x^2}{2}+x$**\nFirst, we compute the first and second derivatives:\n$u'(x) = \\frac{d}{dx}\\left(\\frac{x^3}{3}-\\frac{x^2}{2}+x\\right) = x^2 - x + 1$\n$u''(x) = \\frac{d}{dx}(x^2 - x + 1) = 2x - 1$\n\nNow, we evaluate these derivatives at $x=0$ and $x=1$.\nAt $x=0$:\n$u'(0) = 0^2 - 0 + 1 = 1$\n$u''(0) = 2(0) - 1 = -1$\nCheck conditions at $x=0$:\n- Is $u''(0) = -u'(0)$? Yes, $-1 = -1$.\n- Is $u'(0) \\neq 0$? Yes, $1 \\neq 0$.\n\nAt $x=1$:\n$u'(1) = 1^2 - 1 + 1 = 1$\n$u''(1) = 2(1) - 1 = 1$\nCheck conditions at $x=1$:\n- Is $u''(1) = u'(1)$? Yes, $1 = 1$.\n- Is $u'(1) \\neq 0$? Yes, $1 \\neq 0$.\nAll four conditions are satisfied.\nVerdict: **Correct**\n\n**B. $u(x)=-\\dfrac{1}{2}\\left(x-\\dfrac{1}{2}\\right)^2$**\nFirst, we compute the derivatives:\n$u'(x) = \\frac{d}{dx}\\left(-\\frac{1}{2}\\left(x-\\frac{1}{2}\\right)^2\\right) = -\\left(x-\\frac{1}{2}\\right) = -x + \\frac{1}{2}$\n$u''(x) = \\frac{d}{dx}\\left(-x + \\frac{1}{2}\\right) = -1$\n\nNow, we evaluate at $x=0$:\n$u'(0) = -0 + \\frac{1}{2} = \\frac{1}{2}$\n$u''(0) = -1$\nCheck condition $1$: Is $u''(0) = -u'(0)$? No, $-1 \\neq -\\frac{1}{2}$.\nThe first condition fails. There is no need to proceed further.\nVerdict: **Incorrect**\n\n**C. $u(x)=\\dfrac{x^3}{3}+\\dfrac{x^2}{2}+x$**\nFirst, we compute the derivatives:\n$u'(x) = \\frac{d}{dx}\\left(\\frac{x^3}{3}+\\frac{x^2}{2}+x\\right) = x^2 + x + 1$\n$u''(x) = \\frac{d}{dx}(x^2 + x + 1) = 2x + 1$\n\nNow, we evaluate at $x=0$:\n$u'(0) = 0^2 + 0 + 1 = 1$\n$u''(0) = 2(0) + 1 = 1$\nCheck condition $1$: Is $u''(0) = -u'(0)$? No, $1 \\neq -1$.\nThe first condition fails.\nVerdict: **Incorrect**\n\n**D. $u(x)=\\dfrac{x^3}{3}-x$**\nFirst, we compute the derivatives:\n$u'(x) = \\frac{d}{dx}\\left(\\frac{x^3}{3}-x\\right) = x^2 - 1$\n$u''(x) = \\frac{d}{dx}(x^2 - 1) = 2x$\n\nNow, we evaluate at $x=0$:\n$u''(0) = 2(0) = 0$\nOne of the defining conditions of a strict $2$-cycle is that $u''(a)\\neq 0$ and $u''(b)\\neq 0$. Here, $u''(0)=0$, which means the Newton update is not defined at $x=0$. Therefore, this function cannot generate a cycle involving the point $x=0$.\nVerdict: **Incorrect**\n\nBased on the exhaustive analysis, only option A satisfies all required conditions for a strict $2$-cycle between $x=0$ and $x=1$.", "answer": "$$\\boxed{A}$$"}, {"introduction": "One of the most powerful safeguards for Newton’s method is the use of a line search to ensure that each step makes sufficient progress. This practice puts you in the role of a quantitative developer debugging a faulty optimization routine. By carefully examining the diagnostic output from a failing algorithm, you will need to pinpoint the logical error in its implementation of the Armijo sufficient decrease condition, a cornerstone of modern optimization software. [@problem_id:2414722]", "id": "2414722", "problem": "A financial institution estimates a binary default probability model by minimizing the negative log-likelihood of a logistic specification using Newton's method with backtracking line search that is intended to enforce Armijo sufficient decrease. The parameter vector is denoted by $\\beta \\in \\mathbb{R}^p$, the objective by $f(\\beta)$, the gradient by $\\nabla f(\\beta)$, and the Hessian by $\\nabla^2 f(\\beta)$. The implementation prints the following diagnostics.\n\nAt iteration $k=0$:\n- $f(\\beta_0) = 120.35$.\n- $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$.\n- The minimum and maximum eigenvalues of $\\nabla^2 f(\\beta_0)$ are $\\lambda_{\\min} = 3.1$ and $\\lambda_{\\max} = 214.2$.\n- The linear system for the Newton direction is solved, and the reported inner product is $\\nabla f(\\beta_0)^\\top s_0 = -28.9$.\n- The backtracking line search with parameter $c_1 = 10^{-4}$ accepts $t_0 = 1$ with the message “Armijo satisfied,” and the code reports $f(\\beta_0 + t_0 s_0) = 134.7$.\n\nAt iteration $k=1$:\n- $f(\\beta_1) = 134.7$.\n- $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$.\n- The minimum eigenvalue of $\\nabla^2 f(\\beta_1)$ is reported as $\\lambda_{\\min} = 2.7$.\n- The computed direction satisfies $\\nabla f(\\beta_1)^\\top s_1 = -31.4$.\n- The line search again accepts $t_1 = 1$ with the message “Armijo satisfied,” and $f(\\beta_1 + t_1 s_1) = 150.2$.\n\nAssume the negative log-likelihood of the logistic model is correctly specified and twice continuously differentiable, and that the dataset is of moderate scale and conditioning, so that numerical linear algebra is stable when the Hessian is positive definite.\n\nBased on these diagnostics and standard properties of Newton's method for unconstrained minimization with Armijo backtracking, which implementation issue is most consistent with the observed non-convergence?\n\nA. The Hessian was assembled without cross-partial terms, making it indefinite and producing non-descent directions.\n\nB. The Newton system was solved with the wrong sign, effectively using $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$, which is an ascent direction.\n\nC. The Armijo sufficient decrease test in the line search was implemented with the inequality reversed, so steps that increase $f$ are erroneously accepted.\n\nD. The algorithm’s stopping criterion on $\\lVert \\nabla f(\\beta_k) \\rVert_2$ is too strict, causing it to continue iterating instead of terminating near a minimizer.", "solution": "The user has provided a problem statement describing the output of a faulty implementation of Newton's method for minimizing a negative log-likelihood function. I must validate the statement and then diagnose the most likely implementation error based on the provided data.\n\n**Problem Validation**\n\nFirst, I will extract the given information and validate the problem statement.\n\n**Givens:**\n-   **Algorithm**: Newton's method for minimizing an objective function $f(\\beta)$.\n-   **Update Rule**: $\\beta_{k+1} = \\beta_k + t_k s_k$, where $s_k$ is the Newton direction.\n-   **Newton Direction**: $s_k$ is the solution to $\\nabla^2 f(\\beta_k) s_k = - \\nabla f(\\beta_k)$.\n-   **Line Search**: Backtracking to enforce the Armijo sufficient decrease condition with parameter $c_1 = 10^{-4}$.\n-   **Armijo Condition**: $f(\\beta_k + t_k s_k) \\le f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$.\n-   **Assumptions**: The function $f$ is the twice continuously differentiable negative log-likelihood of a logistic model. Numerical stability is assumed when the Hessian is positive definite.\n\n**Data at Iteration $k=0$:**\n-   $f(\\beta_0) = 120.35$\n-   $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$\n-   Eigenvalues of $\\nabla^2 f(\\beta_0)$: $\\lambda_{\\min} = 3.1$, $\\lambda_{\\max} = 214.2$.\n-   $\\nabla f(\\beta_0)^\\top s_0 = -28.9$\n-   Accepted step size: $t_0 = 1$.\n-   Resulting function value: $f(\\beta_1) = f(\\beta_0 + t_0 s_0) = 134.7$.\n-   Algorithm message: \"Armijo satisfied\".\n\n**Data at Iteration $k=1$:**\n-   $f(\\beta_1) = 134.7$\n-   $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$\n-   Minimum eigenvalue of $\\nabla^2 f(\\beta_1)$: $\\lambda_{\\min} = 2.7$.\n-   $\\nabla f(\\beta_1)^\\top s_1 = -31.4$\n-   Accepted step size: $t_1 = 1$.\n-   Resulting function value: $f(\\beta_2) = f(\\beta_1 + t_1 s_1) = 150.2$.\n-   Algorithm message: \"Armijo satisfied\".\n\n**Validation Analysis:**\nThe problem statement is scientifically grounded in numerical optimization and statistics. The provided data appears contradictory to the stated goal of the algorithm (minimization via Armijo rule), but this is the basis of the diagnostic question, not a flaw in the problem statement itself. The problem is well-posed, objective, and contains sufficient information for a logical diagnosis. For a logistic regression negative log-likelihood, the Hessian matrix is guaranteed to be positive semi-definite, and for a non-degenerate dataset, positive definite. The given positive eigenvalues ($\\lambda_{\\min} = 3.1$ and $\\lambda_{\\min} = 2.7$) are consistent with this property. The problem is therefore valid.\n\n**Derivation of Solution**\n\nThe objective of the algorithm is to minimize $f(\\beta)$. This requires that the function value decreases at each step, i.e., $f(\\beta_{k+1}) < f(\\beta_k)$. The Armijo condition is designed to guarantee this, and even more, to ensure a \"sufficient\" decrease.\n\nLet us analyze the Armijo condition at iteration $k=0$:\nThe condition is $f(\\beta_0 + t_0 s_0) \\le f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0$.\nSubstituting the given values:\n-   Left-hand side (LHS): $f(\\beta_0 + s_0) = 134.7$.\n-   Right-hand side (RHS): $f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0 = 120.35 + (10^{-4})(1)(-28.9) = 120.35 - 0.00289 = 120.34711$.\n\nThe correct Armijo condition requires $134.7 \\le 120.34711$, which is false. Despite this, the algorithm reports \"Armijo satisfied\". Furthermore, the function value has increased: $f(\\beta_1) = 134.7 > f(\\beta_0) = 120.35$. This is a failure for a minimization algorithm.\n\nLet us perform the same analysis for iteration $k=1$:\nThe condition is $f(\\beta_1 + t_1 s_1) \\le f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1$.\nSubstituting the given values:\n-   LHS: $f(\\beta_1 + s_1) = 150.2$.\n-   RHS: $f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1 = 134.7 + (10^{-4})(1)(-31.4) = 134.7 - 0.00314 = 134.69686$.\n\nThe correct Armijo condition requires $150.2 \\le 134.69686$, which is also false. Again, the algorithm erroneously reports \"Armijo satisfied\" and the function value increases: $f(\\beta_2) = 150.2 > f(\\beta_1) = 134.7$.\n\nThe algorithm is systematically failing. It is taking steps that increase the objective function and moving away from a minimum, as also indicated by the increasing gradient norm ($\\lVert \\nabla f(\\beta_1) \\rVert_2 > \\lVert \\nabla f(\\beta_0) \\rVert_2$). The only way \"Armijo satisfied\" could be printed while the function increases is if the logical test for the condition is implemented incorrectly.\n\nLet us consider the possibility of a reversed inequality in the implementation:\n$f(\\beta_k + t_k s_k) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$.\n\n-   At $k=0$, this would test if $134.7 \\ge 120.34711$. This is true.\n-   At $k=1$, this would test if $150.2 \\ge 134.69686$. This is also true.\n\nThis reversed inequality perfectly explains why a step size of $t=1$ is accepted in both iterations despite leading to a substantial increase in the objective function.\n\n**Option-by-Option Analysis**\n\nA. The Hessian was assembled without cross-partial terms, making it indefinite and producing non-descent directions.\n**Analysis**: This claim is contradicted by the data. The minimum eigenvalues of the Hessian at both iterations are given as $\\lambda_{\\min} = 3.1$ and $\\lambda_{\\min} = 2.7$, respectively. Since the minimum eigenvalues are positive, the Hessians are positive definite, not indefinite. Furthermore, the computed directions are descent directions because the inner products with the gradient, $\\nabla f(\\beta_0)^\\top s_0 = -28.9$ and $\\nabla f(\\beta_1)^\\top s_1 = -31.4$, are negative.\n**Verdict**: Incorrect.\n\nB. The Newton system was solved with the wrong sign, effectively using $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$, which is an ascent direction.\n**Analysis**: If this were the case, the search direction $s_k$ would be an ascent direction. For a positive definite Hessian $\\nabla^2 f(\\beta_k)$, the directional derivative $\\nabla f(\\beta_k)^\\top s_k$ would be $\\nabla f(\\beta_k)^\\top [\\nabla^2 f(\\beta_k)]^{-1} \\nabla f(\\beta_k) > 0$. However, the provided data shows $\\nabla f(\\beta_k)^\\top s_k < 0$ for both iterations. Thus, the computed directions are descent directions, not ascent directions.\n**Verdict**: Incorrect.\n\nC. The Armijo sufficient decrease test in the line search was implemented with the inequality reversed, so steps that increase $f$ are erroneously accepted.\n**Analysis**: As demonstrated in the derivation above, if the Armijo condition check was implemented as $f(\\beta_{k+1}) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$, the algorithm would accept the steps $t_0=1$ and $t_1=1$, and report \"Armijo satisfied\" at each iteration. This hypothesis is fully consistent with the observed increase in the function value ($120.35 \\to 134.7 \\to 150.2$) and matches all numerical data provided.\n**Verdict**: Correct.\n\nD. The algorithm’s stopping criterion on $\\lVert \\nabla f(\\beta_k) \\rVert_2$ is too strict, causing it to continue iterating instead of terminating near a minimizer.\n**Analysis**: The nature of the stopping criterion is irrelevant to the erroneous behavior observed *within* each iteration. The core problem is that the algorithm is diverging (function value and gradient norm are increasing), not that it is converging slowly and failing to stop. A strict stopping criterion does not cause divergence; it merely means the algorithm runs longer if it is converging. The observed behavior is a fundamental failure of the optimization step itself.\n**Verdict**: Incorrect.", "answer": "$$\\boxed{C}$$"}, {"introduction": "Having explored how Newton's method can fail and how its safeguards work, it is time to build a robust solver from the ground up. This exercise challenges you to implement a hybrid algorithm that intelligently chooses between a fast Newton step and a reliable steepest-descent step. By incorporating a backtracking line search to guarantee progress, you will construct a practical and powerful tool for tackling the diverse optimization problems encountered in economics and finance. [@problem_id:2414720]", "id": "2414720", "problem": "Consider the unconstrained minimization of smooth real-valued functions under an update rule that enforces a sufficient decrease condition at every iteration. Let $f:\\mathbb{R}^n \\to \\mathbb{R}$ be twice continuously differentiable. At an iterate $x_k \\in \\mathbb{R}^n$, define the gradient $\\nabla f(x_k)$ and the Hessian matrix $\\nabla^2 f(x_k)$. A candidate update $x_{k+1} = x_k + \\alpha_k p_k$ must satisfy the Armijo sufficient decrease condition with constants $c_1 \\in (0,1)$ and $\\rho \\in (0,1)$, namely\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\nwhere $\\alpha_k$ is chosen by backtracking of the form $\\alpha_k \\in \\{ \\rho^j : j \\in \\{0,1,2,\\ldots\\} \\}$, starting from $\\alpha_k = 1$. For a given $x_k$, first attempt to select $p_k$ as the solution to the linear system\n$$\n\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k).\n$$\nIf this candidate direction fails to produce a step length $\\alpha_k$ in the backtracking sequence that satisfies the sufficient decrease condition, then reject that direction and instead select $p_k = -\\nabla f(x_k)$, and determine $\\alpha_k$ via the same backtracking sufficient decrease condition. The iteration terminates when $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ or when a maximum number of $K$ iterations has been performed.\n\nImplement this iterative scheme with the following fixed constants: $c_1 = 10^{-4}$, $\\rho = 0.5$, tolerance $\\varepsilon = 10^{-8}$, backtracking limit of $M = 50$ reductions per iteration, and maximum iterations $K = 50$. If the linear system is singular or ill-posed at some $x_k$, treat the direction defined by $\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)$ as failed and proceed with $p_k = -\\nabla f(x_k)$ as above. All computations are to be performed in real arithmetic without any external data input.\n\nApply this scheme to the following test suite. In all cases, use the Euclidean norm $\\|\\cdot\\|_2$ for the termination criterion, and start the backtracking at step size $\\alpha_k = 1$ at each iteration. Report the minimizer estimate returned by the termination condition for each case. Express every reported number as a decimal rounded to exactly six digits after the decimal point.\n\nTest case A (binary choice negative log-likelihood in two parameters). Let the parameter be $x = (b_0, b_1)^\\top \\in \\mathbb{R}^2$. Let the data be $\\{(x_i,y_i)\\}_{i=1}^5$ with\n$$\n(x_1,y_1) = (-2,0),\\quad (x_2,y_2) = (-1,0),\\quad (x_3,y_3) = (0,1),\\quad (x_4,y_4) = (1,1),\\quad (x_5,y_5) = (2,0).\n$$\nDefine the negative log-likelihood\n$$\nf(x) = \\sum_{i=1}^5 \\left[ \\log\\!\\left(1 + e^{b_0 + b_1 x_i}\\right) - y_i\\,(b_0 + b_1 x_i) \\right].\n$$\nUse the initial point $x_0 = (0,0)^\\top$.\n\nTest case B (ill-conditioned quadratic). Let $x \\in \\mathbb{R}^2$. Define\n$$\nf(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x,\\quad Q = \\begin{bmatrix} 10^{-6} & 0 \\\\ 0 & 1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 10^{-6} \\\\ 1 \\end{bmatrix}.\n$$\nUse the initial point $x_0 = (10,-10)^\\top$.\n\nTest case C (nonconvex one-dimensional quartic). Let $x \\in \\mathbb{R}$. Define\n$$\nf(x) = x^4 - 3 x^2 + x.\n$$\nUse the initial point $x_0 = 0.2$.\n\nOutput specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the result for each test case appears in order as:\n- For Test case A: a list with two numbers $[b_0^\\star,b_1^\\star]$,\n- For Test case B: a list with two numbers $[x_1^\\star,x_2^\\star]$,\n- For Test case C: a single number $x^\\star$,\nwith every number rounded to exactly six digits after the decimal point. For example, the format must be\n$$\n\\big[ [b_0^\\star,b_1^\\star], [x_1^\\star,x_2^\\star], x^\\star \\big].\n$$\nNo spaces are permitted in the printed line. All numbers should be expressed as plain decimals.", "solution": "The problem presented is a well-defined task in numerical optimization. It requires the implementation of a hybrid iterative algorithm combining Newton's method with the steepest descent method, augmented by a backtracking line search to ensure sufficient decrease at each step. The problem is scientifically sound, resting on established principles of nonlinear optimization, and provides all necessary parameters, initial conditions, and objective functions for three distinct test cases. The problem is therefore deemed valid.\n\nThe algorithm to be implemented is an unconstrained minimization procedure for a twice continuously differentiable function $f: \\mathbb{R}^n \\to \\mathbb{R}$. Given an iterate $x_k$, the next iterate $x_{k+1}$ is found by $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k$ is the step length.\n\nThe core of the algorithm is the choice of the search direction $p_k$. The primary choice is the Newton direction, obtained by solving the linear system:\n$$\n\\nabla^2 f(x_k) p_k = - \\nabla f(x_k)\n$$\nwhere $\\nabla f(x_k)$ is the gradient and $\\nabla^2 f(x_k)$ is the Hessian of $f$ at $x_k$. This direction is optimal for quadratic functions and provides rapid local convergence (quadratic) near a minimum where the Hessian is positive definite. However, the Newton direction may be undefined if the Hessian is singular, or it may not be a descent direction if the Hessian is not positive definite. A direction $p_k$ is a descent direction if $\\nabla f(x_k)^\\top p_k < 0$. For the Newton direction, $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$, which is negative only if $\\nabla^2 f(x_k)$ is positive definite. If the Newton direction is not a descent direction or fails to yield a suitable step length, the algorithm must switch to a robust alternative.\n\nThe fallback direction is the steepest descent direction, $p_k = -\\nabla f(x_k)$. This direction is guaranteed to be a descent direction as long as the gradient is non-zero, since $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2 < 0$. While its convergence is generally slower (linear), it ensures progress from any point where $\\nabla f(x_k) \\neq 0$.\n\nThe step length $\\alpha_k$ is determined by a backtracking line search. Starting with a trial step $\\alpha = 1$, the step is successively reduced by a factor $\\rho \\in (0,1)$ until the Armijo sufficient decrease condition is met:\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k\n$$\nfor a constant $c_1 \\in (0,1)$. The problem sets $c_1 = 10^{-4}$ and $\\rho = 0.5$. A limit of $M=50$ backtracking steps is imposed.\n\nThe overall iterative procedure is as follows:\nFor $k = 0, 1, 2, \\ldots, K-1$:\n1. Compute the gradient $g_k = \\nabla f(x_k)$. Terminate if $\\|g_k\\|_2 \\le \\varepsilon$.\n2. Attempt to compute the Newton direction $p_{\\text{Newton}}$. This attempt fails if $\\nabla^2f(x_k)$ is singular, or if the resulting direction is not one of descent ($\\nabla f(x_k)^\\top p_{\\text{Newton}} \\ge 0$).\n3. If the Newton direction is valid, perform a backtracking line search. If a step length $\\alpha_k$ is found within $M$ reductions, the update is performed with $p_k = p_{\\text{Newton}}$ and $\\alpha_k$.\n4. If the Newton direction step fails for any reason (singularity, not a descent direction, or backtracking failure), the algorithm reverts to the steepest descent direction $p_k = -\\nabla f(x_k)$ and performs another backtracking line search to find $\\alpha_k$.\n5. Update the iterate: $x_{k+1} = x_k + \\alpha_k p_k$.\nThe process terminates if the gradient norm is below a tolerance $\\varepsilon=10^{-8}$ or if the maximum number of iterations $K=50$ is reached.\n\nThe implementation will apply this logic to three test cases. For each case, we must provide the analytical forms of the objective function $f(x)$, its gradient $\\nabla f(x)$, and its Hessian matrix $\\nabla^2 f(x)$.\n\n**Test Case A: Binary Choice Negative Log-Likelihood**\nThe objective function is the negative log-likelihood for a logistic regression model.\n$f(b_0, b_1) = \\sum_{i=1}^5 \\left[ \\log(1 + e^{b_0 + b_1 x_i}) - y_i(b_0 + b_1 x_i) \\right]$.\nLet $u_i(b) = b_0 + b_1 x_i$. The sigmoid function is $\\sigma(u) = 1/(1+e^{-u})$.\nThe gradient components are:\n$$\n\\frac{\\partial f}{\\partial b_0} = \\sum_{i=1}^5 (\\sigma(u_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial b_1} = \\sum_{i=1}^5 x_i (\\sigma(u_i) - y_i)\n$$\nThe Hessian components are (using $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$):\n$$\n\\frac{\\partial^2 f}{\\partial b_0^2} = \\sum_{i=1}^5 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1 \\partial b_0} = \\sum_{i=1}^5 x_i \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1^2} = \\sum_{i=1}^5 x_i^2 \\sigma(u_i)(1-\\sigma(u_i))\n$$\nThis Hessian is always positive semi-definite, ensuring that the Newton direction (if defined) is a descent direction.\n\n**Test Case B: Ill-Conditioned Quadratic**\nThe objective function is $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$.\nThe gradient is $\\nabla f(x) = Qx - b$.\nThe Hessian is constant: $\\nabla^2 f(x) = Q$.\nSince $Q$ is a constant, positive definite matrix, Newton's method with a full step ($\\alpha=1$) will find the exact minimum $x^*=Q^{-1}b$ in a single iteration.\n\n**Test Case C: Nonconvex One-Dimensional Quartic**\nThe objective function is $f(x) = x^4 - 3x^2 + x$.\nThe gradient (derivative) is $f'(x) = 4x^3 - 6x + 1$.\nThe Hessian (second derivative) is $f''(x) = 12x^2 - 6$.\nAt the initial point $x_0 = 0.2$, the Hessian is $f''(0.2) = 12(0.04) - 6 = -5.52$, which is negative. Therefore, the Newton direction at $x_0$ will be an ascent direction, and the algorithm must fall back to steepest descent for the first iteration.\n\nThe following Python code implements the described algorithm and applies it to the three test cases, formatting the output as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Fixed constants for the algorithm\n    C1 = 1e-4\n    RHO = 0.5\n    EPS = 1e-8\n    MAX_ITER = 50\n    BT_LIMIT = 50\n\n    def optimizer(f, grad_f, hess_f, x0):\n        \"\"\"\n        Implements the hybrid Newton/Gradient-Descent optimization algorithm.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n\n        for _ in range(MAX_ITER):\n            g = grad_f(x)\n            if np.linalg.norm(g) <= EPS:\n                return x\n\n            p = None\n            alpha = None\n            newton_step_succeeded = False\n\n            # --- Attempt Newton's Method ---\n            try:\n                H = hess_f(x)\n                p_newton = np.linalg.solve(H, -g)\n\n                # Newton direction must be a descent direction\n                if np.dot(g, p_newton) < 0:\n                    # Backtracking line search for Newton's direction\n                    alpha_bt = 1.0\n                    fk = f(x)\n                    g_dot_p = np.dot(g, p_newton)\n                    for _ in range(BT_LIMIT):\n                        if f(x + alpha_bt * p_newton) <= fk + C1 * alpha_bt * g_dot_p:\n                            p = p_newton\n                            alpha = alpha_bt\n                            newton_step_succeeded = True\n                            break\n                        alpha_bt *= RHO\n            except np.linalg.LinAlgError:\n                # Hessian is singular or ill-posed\n                pass\n\n            # --- Fallback to Steepest Descent ---\n            if not newton_step_succeeded:\n                p_sd = -g\n                \n                # Backtracking line search for steepest descent\n                alpha_bt = 1.0\n                fk = f(x)\n                g_dot_p = np.dot(g, p_sd)\n                for _ in range(BT_LIMIT):\n                    if f(x + alpha_bt * p_sd) <= fk + C1 * alpha_bt * g_dot_p:\n                        p = p_sd\n                        alpha = alpha_bt\n                        break\n                    alpha_bt *= RHO\n            \n            # If no step could be found (highly unlikely for steepest descent)\n            if p is None or alpha is None:\n                return x\n\n            # Update x\n            x = x + alpha * p\n        \n        return x\n\n    # --- Test Case A: Logistic Regression ---\n    x_data_A = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data_A = np.array([0.0, 0.0, 1.0, 1.0, 0.0])\n    x0_A = np.array([0.0, 0.0])\n\n    def f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Use np.logaddexp for numerical stability: log(1+exp(x)) = logaddexp(0,x)\n        return np.sum(np.logaddexp(0, u) - y_data_A * u)\n\n    def grad_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Sigmoid function: 1 / (1 + exp(-u))\n        sigma = 1 / (1 + np.exp(-u))\n        diff = sigma - y_data_A\n        return np.array([np.sum(diff), np.sum(diff * x_data_A)])\n\n    def hess_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        sigma = 1 / (1 + np.exp(-u))\n        weights = sigma * (1 - sigma)\n        h00 = np.sum(weights)\n        h01 = np.sum(weights * x_data_A)\n        h11 = np.sum(weights * x_data_A**2)\n        return np.array([[h00, h01], [h01, h11]])\n\n    result_A = optimizer(f_A, grad_f_A, hess_f_A, x0_A)\n\n    # --- Test Case B: Ill-Conditioned Quadratic ---\n    Q_B = np.array([[1e-6, 0.0], [0.0, 1.0]])\n    b_vec_B = np.array([1e-6, 1.0])\n    x0_B = np.array([10.0, -10.0])\n\n    def f_B(x):\n        return 0.5 * x.T @ Q_B @ x - b_vec_B.T @ x\n    \n    def grad_f_B(x):\n        return Q_B @ x - b_vec_B\n    \n    def hess_f_B(x):\n        return Q_B\n\n    result_B = optimizer(f_B, grad_f_B, hess_f_B, x0_B)\n\n    # --- Test Case C: Nonconvex Quartic ---\n    x0_C = np.array([0.2])\n\n    def f_C(x):\n        xv = x[0]\n        return xv**4 - 3 * xv**2 + xv\n    \n    def grad_f_C(x):\n        xv = x[0]\n        return np.array([4 * xv**3 - 6 * xv + 1])\n    \n    def hess_f_C(x):\n        xv = x[0]\n        return np.array([[12 * xv**2 - 6]])\n    \n    result_C = optimizer(f_C, grad_f_C, hess_f_C, x0_C)\n\n    # --- Format Output ---\n    results_list = [result_A, result_B, result_C]\n    formatted_parts = []\n    for res in results_list:\n        if res.size > 1:\n            num_strs = [f\"{val:.6f}\" for val in res]\n            formatted_parts.append(f\"[{','.join(num_strs)}]\")\n        else:\n            formatted_parts.append(f\"{res.item():.6f}\")\n\n    final_output = f\"[{','.join(formatted_parts)}]\"\n    print(final_output)\n\nsolve()\n```"}]}