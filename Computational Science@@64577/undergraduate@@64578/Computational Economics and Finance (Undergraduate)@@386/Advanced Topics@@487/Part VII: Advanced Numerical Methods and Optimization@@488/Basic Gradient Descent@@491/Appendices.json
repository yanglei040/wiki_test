{"hands_on_practices": [{"introduction": "We begin our hands-on journey with a classic problem from computational finance: determining an optimal hedging strategy. This exercise asks you to use gradient descent to minimize the risk associated with fluctuating fuel costs, a tangible application with clear real-world value [@problem_id:2375264]. The underlying objective function is convex, which guarantees a single global minimum, making this the perfect environment to focus on mastering the core mechanics of gradient descent: deriving the gradient and implementing the iterative update rule.", "id": "2375264", "problem": "An airline seeks to hedge its jet fuel cost exposure by trading oil futures contracts. Let a time index be denoted by $t \\in \\{1, \\dots, T\\}$. For each period $t$, let $s_t$ denote the observed change in the airlineâ€™s jet fuel cost per unit volume (for example, a change in United States dollars (USD) per gallon), and let $f_t$ denote the corresponding change in the futures price per contract-unit over the same period. The airline chooses a scalar hedging ratio $h \\in \\mathbb{R}$, interpreted as the number of futures contract-units per unit of fuel exposure, to reduce fluctuations in the hedged cost change $x_t(h) = s_t - h f_t$. To discourage excessively large positions, the airline incurs a small quadratic position penalty with coefficient $\\gamma \\ge 0$. The objective is to choose $h$ that minimizes the penalized sample mean squared hedged change\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2.\n$$\nStarting from a given initial guess $h_0 \\in \\mathbb{R}$, use basic gradient descent with a fixed step size $\\alpha > 0$ to minimize $J(h)$. At iteration $k$, update $h_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)$, and terminate when the absolute change in iterates satisfies $|h_{k+1} - h_k| < \\varepsilon$ or when a maximum of $N$ iterations is reached. Your implementation must:\n- Derive the gradient $\\nabla J(h)$ from first principles based on the definition of $J(h)$.\n- Ensure numerical robustness by using the provided tolerance $\\varepsilon$ and iteration cap $N$.\n- Return the final iterate as the estimated optimal hedging ratio $h^\\star$.\n\nImplement a program that computes $h^\\star$ for each of the following test cases. For each case, $s$ and $f$ are given as ordered lists of real numbers, and you are provided with $\\gamma$, $\\alpha$, $h_0$, $\\varepsilon$, and $N$.\n\nTest Suite:\n- Case $1$:\n  - $s = [\\,\\$1.0,\\,-\\$0.5,\\,\\$0.75,\\,-\\$1.25,\\,\\$0.6,\\,-\\$0.8\\,]$\n  - $f = [\\,\\$0.9,\\,-\\$0.4,\\,\\$0.8,\\,-\\$1.1,\\,\\$0.5,\\,-\\$0.7\\,]$\n  - $\\gamma = 0.01$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- Case $2$:\n  - $s = [\\,\\$1.0,\\,-\\$1.0,\\,\\$1.0,\\,-\\$1.0,\\,\\$0.5,\\,-\\$0.5\\,]$\n  - $f = [\\,-\\$0.8,\\,\\$0.8,\\,-\\$0.7,\\,\\$0.7,\\,-\\$0.4,\\,\\$0.4\\,]$\n  - $\\gamma = 0.02$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- Case $3$ (weak hedging instrument; tests regularization):\n  - $s = [\\,\\$1.0,\\,\\$0.5,\\,-\\$0.5,\\,-\\$1.0,\\,\\$0.3,\\,-\\$0.2\\,]$\n  - $f = [\\,\\$0.1,\\,-\\$0.1,\\,\\$0.1,\\,-\\$0.1,\\,\\$0.05,\\,-\\$0.05\\,]$\n  - $\\gamma = 0.5$, $\\alpha = 0.02$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- Case $4$ (longer sample, negligible penalty):\n  - $s = [\\,\\$0.2,\\,-\\$0.1,\\,\\$0.3,\\,-\\$0.25,\\,\\$0.15,\\,-\\$0.05,\\,\\$0.4,\\,-\\$0.35\\,]$\n  - $f = [\\,\\$0.18,\\,-\\$0.12,\\,\\$0.28,\\,-\\$0.22,\\,\\$0.14,\\,-\\$0.06,\\,\\$0.39,\\,-\\$0.33\\,]$\n  - $\\gamma = 0.0$, $\\alpha = 0.08$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n\nAll monetary quantities are in United States dollars (USD) but the output hedging ratios are unitless. Your program should produce a single line of output containing the four estimated hedging ratios as a comma-separated list enclosed in square brackets, each rounded to exactly $6$ decimal places (for example, $[1.234567,-0.123456,0.000001,2.000000]$). No other text should be printed.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n\nThe objective is to find the hedging ratio $h \\in \\mathbb{R}$ that minimizes the following penalized cost function:\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2\n$$\nwhere:\n- $t \\in \\{1, \\dots, T\\}$ is the time index.\n- $s_t$ is the change in jet fuel cost per unit volume.\n- $f_t$ is the change in the futures price per contract-unit.\n- $\\gamma \\ge 0$ is the quadratic position penalty coefficient.\n\nThe optimization is to be performed using basic gradient descent with the update rule:\n$$\nh_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)\n$$\nwhere:\n- $k$ is the iteration index.\n- $h_k$ is the estimate of the hedging ratio at iteration $k$.\n- $\\alpha > 0$ is a fixed step size.\n- $\\nabla J(h_k)$ is the gradient of $J(h)$ evaluated at $h_k$.\n\nThe algorithm starts from an initial guess $h_0$ and terminates under one of two conditions:\n1.  Convergence: $|h_{k+1} - h_k| < \\varepsilon$, where $\\varepsilon$ is a specified tolerance.\n2.  Maximum iterations: The number of iterations reaches a maximum value $N$.\n\nThe provided test cases are:\n- **Case 1**:\n  - $s = [1.0, -0.5, 0.75, -1.25, 0.6, -0.8]$\n  - $f = [0.9, -0.4, 0.8, -1.1, 0.5, -0.7]$\n  - $\\gamma = 0.01$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **Case 2**:\n  - $s = [1.0, -1.0, 1.0, -1.0, 0.5, -0.5]$\n  - $f = [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4]$\n  - $\\gamma = 0.02$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **Case 3**:\n  - $s = [1.0, 0.5, -0.5, -1.0, 0.3, -0.2]$\n  - $f = [0.1, -0.1, 0.1, -0.1, 0.05, -0.05]$\n  - $\\gamma = 0.5$, $\\alpha = 0.02$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **Case 4**:\n  - $s = [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35]$\n  - $f = [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33]$\n  - $\\gamma = 0.0$, $\\alpha = 0.08$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is an application of regularized linear regression, a fundamental technique in statistics and machine learning, to a standard problem in financial engineering (hedging). The objective function is a form of Ridge regression, and gradient descent is a canonical method for its optimization. The formulation is scientifically and mathematically sound.\n- **Well-Posed**: The objective function $J(h)$ is a quadratic function of $h$. Specifically, $J(h) = A h^2 - B h + C$, where the coefficient of the quadratic term is $A = (\\frac{1}{T} \\sum_{t=1}^T f_t^2) + \\gamma$. Since $\\gamma \\ge 0$ and $f_t^2 \\ge 0$, the coefficient $A$ is strictly positive as long as not all $f_t$ are zero or $\\gamma > 0$. In all test cases, the vector $f$ is not a zero vector, thus $A > 0$. This means $J(h)$ is a strictly convex function and possesses a unique global minimum. The problem is therefore well-posed.\n- **Objective**: The problem is stated in precise mathematical terms, devoid of ambiguity, subjectivity, or opinion.\n- **Completeness**: All required data and parameters ($s, f, \\gamma, \\alpha, h_0, \\varepsilon, N$) are provided for each test case. The problem is self-contained.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A solution will be furnished.\n\n**Solution Derivation**\n\nThe core of the gradient descent algorithm is the computation of the gradient of the objective function, $\\nabla J(h)$. We derive this from first principles by differentiating $J(h)$ with respect to $h$.\n\nThe objective function is:\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2\n$$\n\nThe gradient $\\nabla J(h)$ is the ordinary derivative $\\frac{dJ}{dh}$:\n$$\n\\nabla J(h) = \\frac{d}{dh} \\left( \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2 \\right)\n$$\n\nBy linearity of differentiation, we can differentiate term by term:\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{d}{dh} (s_t - h f_t)^2 + \\frac{d}{dh} (\\gamma h^2)\n$$\n\nApplying the chain rule to the first term and the power rule to the second:\n$$\n\\frac{d}{dh} (s_t - h f_t)^2 = 2 (s_t - h f_t) \\cdot \\frac{d}{dh}(s_t - h f_t) = 2 (s_t - h f_t) (-f_t) = -2s_t f_t + 2h f_t^2\n$$\n$$\n\\frac{d}{dh} (\\gamma h^2) = 2 \\gamma h\n$$\n\nSubstituting these back into the expression for the gradient:\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} (-2s_t f_t + 2h f_t^2) + 2 \\gamma h\n$$\n\nWe can separate the terms inside the summation:\n$$\n\\nabla J(h) = \\frac{1}{T} \\left( \\sum_{t=1}^{T} (-2s_t f_t) + \\sum_{t=1}^{T} (2h f_t^2) \\right) + 2 \\gamma h\n$$\n$$\n\\nabla J(h) = -\\frac{2}{T} \\sum_{t=1}^{T} s_t f_t + \\frac{2h}{T} \\sum_{t=1}^{T} f_t^2 + 2 \\gamma h\n$$\n\nFactoring out $2h$:\n$$\n\\nabla J(h) = 2h \\left( \\frac{1}{T} \\sum_{t=1}^{T} f_t^2 + \\gamma \\right) - \\frac{2}{T} \\sum_{t=1}^{T} s_t f_t\n$$\nThis is the analytical expression for the gradient. This expression will be used in the iterative update rule.\n\nThe algorithm proceeds as follows:\n1.  Initialize $h_k$ with $h_0$.\n2.  For $k$ from $0$ to $N-1$:\n    a. Compute the gradient $\\nabla J(h_k)$ using the derived formula. To do this efficiently, we first pre-compute the sample moments $\\frac{1}{T} \\sum s_t f_t$ and $\\frac{1}{T} \\sum f_t^2$.\n    b. Update the estimate: $h_{k+1} = h_k - \\alpha \\nabla J(h_k)$.\n    c. Check for convergence: if $|h_{k+1} - h_k| < \\varepsilon$, the algorithm has converged. The final estimate is $h^\\star = h_{k+1}$. Terminate.\n3.  If the loop completes without meeting the convergence criterion, the algorithm terminates due to reaching the maximum number of iterations. The final estimate is $h^\\star = h_N$.\n\nThis procedure will be implemented for each test case to find the optimal hedging ratio $h^\\star$. The provided step sizes $\\alpha$ are small enough to ensure stable convergence toward the unique minimum of the convex objective function.\nThe analytical solution for the minimum, found by setting $\\nabla J(h) = 0$, is\n$h^\\star = \\frac{\\sum s_t f_t}{\\sum f_t^2 + T\\gamma}$, which serves as a validation for the convergence of the iterative algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal hedging ratio using gradient descent for multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"s\": [1.0, -0.5, 0.75, -1.25, 0.6, -0.8],\n            \"f\": [0.9, -0.4, 0.8, -1.1, 0.5, -0.7],\n            \"gamma\": 0.01,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, -1.0, 1.0, -1.0, 0.5, -0.5],\n            \"f\": [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4],\n            \"gamma\": 0.02,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, 0.5, -0.5, -1.0, 0.3, -0.2],\n            \"f\": [0.1, -0.1, 0.1, -0.1, 0.05, -0.05],\n            \"gamma\": 0.5,\n            \"alpha\": 0.02,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35],\n            \"f\": [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33],\n            \"gamma\": 0.0,\n            \"alpha\": 0.08,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n    ]\n\n    def compute_h_star(s_data, f_data, gamma, alpha, h0, epsilon, N):\n        \"\"\"\n        Computes the optimal hedging ratio h* using gradient descent.\n        \"\"\"\n        s = np.array(s_data)\n        f = np.array(f_data)\n        T = len(s)\n\n        # Pre-compute sample moments for efficiency\n        # E[f^2] = (1/T) * sum(f_t^2)\n        mean_f_sq = np.sum(f**2) / T\n        # E[sf] = (1/T) * sum(s_t * f_t)\n        mean_s_f = np.sum(s * f) / T\n        \n        h_k = h0\n\n        for _ in range(N):\n            # Gradient: grad_J(h) = 2 * (h * (E[f^2] + gamma) - E[sf])\n            grad_J = 2 * (h_k * (mean_f_sq + gamma) - mean_s_f)\n            \n            h_k_plus_1 = h_k - alpha * grad_J\n            \n            if np.abs(h_k_plus_1 - h_k) < epsilon:\n                return h_k_plus_1\n            \n            h_k = h_k_plus_1\n            \n        return h_k\n\n    results = []\n    for case in test_cases:\n        h_star = compute_h_star(\n            case[\"s\"],\n            case[\"f\"],\n            case[\"gamma\"],\n            case[\"alpha\"],\n            case[\"h0\"],\n            case[\"epsilon\"],\n            case[\"N\"],\n        )\n        results.append(h_star)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each number rounded to 6 decimal places, enclosed in brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```"}, {"introduction": "Having mastered the basic algorithm on a well-behaved problem, we now explore one of the most significant challenges in optimization: non-convexity. This practice is a thought experiment designed to build your intuition for the 'landscapes' that gradient descent navigates [@problem_id:2375265]. You will analyze how an infinitesimally small change in the starting point can dramatically alter the final outcome, leading the algorithm into a suboptimal local minimum. This exercise is crucial for a deeper understanding of why initialization strategies are so important and why gradient descent does not guarantee finding the best possible solution.", "id": "2375265", "problem": "A researcher in computational finance is calibrating a scalar preference parameter $\\theta$ in a representative agent asset-pricing model by minimizing a stylized nonconvex loss that captures misspecification and a weak empirical tilt. The loss function is\n$$\nL(\\theta) \\;=\\; (\\theta^{2} - 1)^{2} \\;-\\; 0.2\\,\\theta.\n$$\nThe researcher applies standard gradient descent with a constant step size $\\alpha$, using the update\n$$\n\\theta_{t+1} \\;=\\; \\theta_{t} \\;-\\; \\alpha\\,\\nabla L(\\theta_{t}).\n$$\nConsider $\\alpha = 0.1$ and the two initializations $\\theta_{0}^{(-)} = -0.051$ and $\\theta_{0}^{(+)} = -0.049$, which differ by $0.002$. Which of the following best characterizes the asymptotic behavior of the two gradient descent runs and identifies whether a suboptimal minimum is reached?\n\nA. Both $\\theta_{0}^{(-)}$ and $\\theta_{0}^{(+)}$ converge to the left local minimum near $\\theta \\approx -1$, which is the global minimum.\n\nB. $\\theta_{0}^{(-)}$ converges to the right local minimum near $\\theta \\approx 1$ (global), while $\\theta_{0}^{(+)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal).\n\nC. $\\theta_{0}^{(-)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal), while $\\theta_{0}^{(+)}$ converges to the right local minimum near $\\theta \\approx 1$ (global).\n\nD. Both sequences diverge to $\\pm\\infty$ because $\\alpha = 0.1$ is too large for stability on this loss.", "solution": "The first duty in any scientific inquiry is to validate the problem statement. A flawed premise leads to a meaningless conclusion.\n\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n- Loss function: $L(\\theta) = (\\theta^{2} - 1)^{2} - 0.2\\,\\theta$.\n- Gradient descent update rule: $\\theta_{t+1} = \\theta_{t} - \\alpha\\,\\nabla L(\\theta_{t})$.\n- Step size: $\\alpha = 0.1$.\n- Initial conditions: $\\theta_{0}^{(-)} = -0.051$ and $\\theta_{0}^{(+)} = -0.049$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is analyzed against the required criteria for validity.\n- **Scientifically Grounded**: The problem describes the minimization of a scalar polynomial function using the standard gradient descent algorithm. This is a fundamental and well-understood problem in numerical optimization and calculus. The function is nonconvex, which presents a common challenge in optimization, making the problem non-trivial and relevant. The context of \"computational finance\" is merely a setting; the core problem is purely mathematical. It is scientifically sound.\n- **Well-Posed**: The loss function is explicitly defined. The update rule, step size, and initial conditions are given as precise numerical values. The question asks for the asymptotic behavior of the sequences, which is a deterministic outcome of the given setup. The problem is well-posed.\n- **Objective**: The problem is stated with objective and precise mathematical language. There are no subjective or ambiguous terms.\n\nThe problem statement does not violate any criteria for validity. It is a standard, self-contained, and solvable mathematical problem.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. We proceed to the solution.\n\n### Derivation of Solution\n\nThe solution requires an analysis of the topology of the loss function $L(\\theta)$ and the dynamics of the gradient descent algorithm.\n\nFirst, we must identify the critical points of the loss function by finding the roots of its gradient. The loss function is $L(\\theta) = \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta$.\nThe gradient (first derivative) is:\n$$\n\\nabla L(\\theta) = L'(\\theta) = \\frac{d}{d\\theta} \\left( \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta \\right) = 4\\theta^3 - 4\\theta - 0.2\n$$\nThe critical points $\\theta^*$ are the solutions to $L'(\\theta^*) = 0$:\n$$\n4(\\theta^*)^3 - 4\\theta^* - 0.2 = 0\n$$\nThis is a cubic equation. To understand its roots, we observe that for the unperturbed equation $4\\theta^3 - 4\\theta = 0$, the roots are $\\theta = 0, \\pm 1$. The term $-0.2$ is a small perturbation.\n- Near $\\theta = 0$, $L'(\\theta) \\approx -4\\theta - 0.2$. Setting this to $0$ gives $-4\\theta \\approx 0.2$, so $\\theta \\approx -0.05$.\n- Near $\\theta = 1$, let $\\theta = 1+\\epsilon$. $L'(1+\\epsilon) = 4(1+\\epsilon)^3 - 4(1+\\epsilon) - 0.2 \\approx 4(1+3\\epsilon) - 4(1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$. Setting this to $0$ gives $8\\epsilon \\approx 0.2$, so $\\epsilon \\approx 0.025$, yielding a root near $\\theta \\approx 1.025$.\n- Near $\\theta = -1$, let $\\theta = -1+\\epsilon$. $L'(-1+\\epsilon) = 4(-1+\\epsilon)^3 - 4(-1+\\epsilon) - 0.2 \\approx 4(-1+3\\epsilon) - 4(-1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$. Setting this to $0$ gives $8\\epsilon \\approx 0.2$, so $\\epsilon \\approx 0.025$, yielding a root near $\\theta \\approx -1 + 0.025 = -0.975$.\n\nSo, we have three critical points, approximately $\\theta_1^* \\approx -0.975$, $\\theta_2^* \\approx -0.05$, and $\\theta_3^* \\approx 1.025$.\n\nNext, we classify these critical points using the second derivative test. The second derivative is:\n$$\nL''(\\theta) = \\frac{d}{d\\theta} (4\\theta^3 - 4\\theta - 0.2) = 12\\theta^2 - 4\n$$\n- At $\\theta_1^* \\approx -0.975$: $L''(-0.975) = 12(-0.975)^2 - 4 > 12(0.9)^2 - 4 = 12(0.81) - 4 = 9.72 - 4 > 0$. This is a local minimum.\n- At $\\theta_2^* \\approx -0.05$: $L''(-0.05) = 12(-0.05)^2 - 4 = 12(0.0025) - 4 = 0.03 - 4 < 0$. This is a local maximum.\n- At $\\theta_3^* \\approx 1.025$: $L''(1.025) = 12(1.025)^2 - 4 > 12(1)^2 - 4 = 8 > 0$. This is a local minimum.\n\nWe have two local minima and one local maximum. To determine the global minimum, we must compare the values of $L(\\theta)$ at the two minima.\n- $L(\\theta_1^*) \\approx L(-0.975) = ((-0.975)^2 - 1)^2 - 0.2(-0.975) \\approx (0.95 - 1)^2 + 0.195 = 0.0025 + 0.195 = 0.1975$.\n- $L(\\theta_3^*) \\approx L(1.025) = ((1.025)^2 - 1)^2 - 0.2(1.025) \\approx (1.05 - 1)^2 - 0.205 = 0.0025 - 0.205 = -0.2025$.\nSince $L(\\theta_3^*) < L(\\theta_1^*)$, the local minimum near $\\theta \\approx 1.025$ is the **global minimum**, and the local minimum near $\\theta \\approx -0.975$ is a **suboptimal local minimum**.\n\nThe local maximum at $\\theta_2^* \\approx -0.05$ acts as a separatrix, or a \"watershed,\" for the gradient flow. Initial points to the left of this maximum will descend into the left basin of attraction, while points to the right will descend into the right basin.\n\nThe initializations are $\\theta_{0}^{(-)} = -0.051$ and $\\theta_{0}^{(+)} = -0.049$. These are strategically placed on either side of the local maximum at $\\theta_2^* \\approx -0.05$.\n- For $\\theta_{0}^{(-)} = -0.051$: This point is slightly to the left of the local maximum. The gradient $L'(\\theta)$ must be positive in the interval immediately to the left of a local maximum. Let's verify: $L'(-0.051) = 4(-0.051)^3 - 4(-0.051) - 0.2 \\approx 4(-0.00013) + 0.204 - 0.2 = -0.00052 + 0.004 > 0$.\nThe gradient descent update is $\\theta_{1}^{(-)} = \\theta_{0}^{(-)} - \\alpha L'(\\theta_{0}^{(-)})$. Since $L'(\\theta_{0}^{(-)}) > 0$, the update will subtract a positive quantity, moving the parameter to the left: $\\theta_{1}^{(-)} < \\theta_{0}^{(-)}$. This trajectory will converge to the left local minimum, which is suboptimal.\n- For $\\theta_{0}^{(+)} = -0.049$: This point is slightly to the right of the local maximum. The gradient $L'(\\theta)$ must be negative in the interval immediately to the right of a local maximum. Let's verify: $L'(-0.049) = 4(-0.049)^3 - 4(-0.049) - 0.2 \\approx 4(-0.00012) + 0.196 - 0.2 = -0.00048 - 0.004 < 0$.\nThe update is $\\theta_{1}^{(+)} = \\theta_{0}^{(+)} - \\alpha L'(\\theta_{0}^{(+)})$. Since $L'(\\theta_{0}^{(+)}) < 0$, the update will add a positive quantity, moving the parameter to the right: $\\theta_{1}^{(+)} > \\theta_{0}^{(+)}$. This trajectory will converge to the right local minimum, which is the global minimum.\n\nFinally, we assess the claim of divergence. For gradient descent to converge to a minimum $\\theta^*$, a sufficient condition on the step size is $0 < \\alpha < 2/L''(\\theta^*)$.\n- At the left minimum ($\\theta_1^*$), $L''(\\theta_1^*) \\approx 7.4$. The condition is $\\alpha < 2/7.4 \\approx 0.27$. Our $\\alpha = 0.1$ satisfies this.\n- At the right minimum ($\\theta_3^*$), $L''(\\theta_3^*) \\approx 8.6$. The condition is $\\alpha < 2/8.6 \\approx 0.23$. Our $\\alpha = 0.1$ satisfies this as well.\nThe step size $\\alpha = 0.1$ is sufficiently small to ensure convergence, so the sequences will not diverge.\n\n### Option-by-Option Analysis\n\nA. **Both $\\theta_{0}^{(-)}$ and $\\theta_{0}^{(+)}$ converge to the left local minimum near $\\theta \\approx -1$, which is the global minimum.**\nThe initial points are on opposite sides of a local maximum and thus fall into different basins of attraction. They do not converge to the same minimum. Furthermore, the left minimum is suboptimal, not global.\nVerdict: **Incorrect**.\n\nB. **$\\theta_{0}^{(-)}$ converges to the right local minimum near $\\theta \\approx 1$ (global), while $\\theta_{0}^{(+)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal).**\nThis is the reverse of our derived behavior. $\\theta_{0}^{(-)}$ is pushed left, and $\\theta_{0}^{(+)}$ is pushed right.\nVerdict: **Incorrect**.\n\nC. **$\\theta_{0}^{(-)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal), while $\\theta_{0}^{(+)}$ converges to the right local minimum near $\\theta \\approx 1$ (global).**\nThis matches our analysis precisely. $\\theta_{0}^{(-)} = -0.051$ lies in the basin of attraction of the suboptimal left minimum. $\\theta_{0}^{(+)} = -0.049$ lies in the basin of attraction of the global right minimum.\nVerdict: **Correct**.\n\nD. **Both sequences diverge to $\\pm\\infty$ because $\\alpha = 0.1$ is too large for stability on this loss.**\nOur analysis of the step size condition shows that $\\alpha = 0.1$ is well within the stable range for convergence to either minimum. The claim of divergence is false.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{C}$$"}, {"introduction": "Our final practice synthesizes the preceding lessons into a sophisticated, multi-dimensional application: finding the optimal location for a new business to maximize customer traffic [@problem_id:2375271]. The objective function in this scenario is non-convex, meaning multiple local maxima exist, directly demonstrating the practical implications of the concepts explored in our prior exercise on local optima. You will adapt your skills from minimization to maximization using gradient *ascent* and implement a more robust adaptive step-size method, the backtracking line search, a technique essential for efficiently navigating complex optimization landscapes.", "id": "2375271", "problem": "You are given a smooth, unconstrained objective in two dimensions constructed from spatial demand centers. Let there be $m \\in \\mathbb{N}$ centers, each with location $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^2$, positive weight $w_i \\in \\mathbb{R}_{>0}$, and isotropic spread $s_i \\in \\mathbb{R}_{>0}$. Define the customer traffic intensity function $T: \\mathbb{R}^2 \\to \\mathbb{R}$ by\n$$\nT(\\boldsymbol{x}) \\equiv \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right),\n$$\nwhere $\\left\\| \\cdot \\right\\|_2$ denotes the Euclidean norm. Your task is to use first principles to derive the gradient $\\nabla T(\\boldsymbol{x})$ and then implement gradient ascent with a backtracking line search to estimate a maximizer $\\boldsymbol{x}^\\star \\in \\mathbb{R}^2$ of $T(\\boldsymbol{x})$ for each test instance below.\n\nStart from the following foundational bases:\n- The definition of the gradient as the vector of partial derivatives.\n- The chain rule of differentiation for compositions of differentiable functions.\n- The fact that the directional derivative in the direction of the gradient gives the maximal instantaneous rate of increase.\n- The Armijo condition for backtracking line search: for a step size $\\alpha > 0$ and ascent direction $\\boldsymbol{d}$, accept $\\alpha$ if\n$$\nT(\\boldsymbol{x} + \\alpha \\boldsymbol{d}) \\ge T(\\boldsymbol{x}) + c \\alpha \\, \\nabla T(\\boldsymbol{x})^\\top \\boldsymbol{d}\n$$\nfor a chosen constant $c \\in (0,1)$.\n\nAlgorithmic requirements:\n- Use steepest ascent with direction $\\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)$ and iterate $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$.\n- Use backtracking line search starting from an initial step size $\\alpha_0$, shrinking by factor $\\rho \\in (0,1)$ until the Armijo condition is satisfied or until the step size falls below a minimum threshold, at which point you should stop.\n- Stop when the gradient norm is below a tolerance $\\varepsilon$ or when the maximum number of iterations is reached.\n- There are no constraints on $\\boldsymbol{x}$ beyond $\\boldsymbol{x} \\in \\mathbb{R}^2$, and there are no physical units to report.\n\nTest suite:\n- Case $1$ (general, well-separated peaks):\n  - $m = 3$,\n  - $\\boldsymbol{\\mu}_1 = (1, 2)$, $w_1 = 100$, $s_1 = 1.0$;\n  - $\\boldsymbol{\\mu}_2 = (-2, -1)$, $w_2 = 80$, $s_2 = 1.5$;\n  - $\\boldsymbol{\\mu}_3 = (3, -3)$, $w_3 = 120$, $s_3 = 0.7$;\n  - Initialization $\\boldsymbol{x}_0 = (0, 0)$.\n- Case $2$ (symmetric, flat gradient at initialization):\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (-2, 0)$, $w_1 = 50$, $s_1 = 0.5$;\n  - $\\boldsymbol{\\mu}_2 = (2, 0)$, $w_2 = 50$, $s_2 = 0.5$;\n  - Initialization $\\boldsymbol{x}_0 = (0, 0)$.\n- Case $3$ (local maximum versus global maximum, sensitivity to initialization):\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (5, 5)$, $w_1 = 200$, $s_1 = 2.0$;\n  - $\\boldsymbol{\\mu}_2 = (-5, -5)$, $w_2 = 180$, $s_2 = 1.0$;\n  - Initialization $\\boldsymbol{x}_0 = (-10, -10)$.\n\nUse the same line search and stopping parameters for all cases:\n- Initial step size $\\alpha_0 = 1.0$,\n- Shrink factor $\\rho = 0.5$,\n- Armijo coefficient $c = 10^{-4}$,\n- Gradient-norm tolerance $\\varepsilon = 10^{-8}$,\n- Maximum iterations $K_{\\max} = 10000$,\n- Minimum step size threshold for backtracking $\\alpha_{\\min} = 10^{-12}$.\n\nOutput specification:\n- For each case, return the estimated maximizer and objective value as a list $[\\hat{x}, \\hat{y}, T(\\hat{\\boldsymbol{x}})]$, where $\\hat{\\boldsymbol{x}} = (\\hat{x}, \\hat{y})$.\n- Each scalar must be a floating-point number rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of the three case results, enclosed in square brackets and preserving the internal list structure. Concretely, print\n  $$\n  \\big[ [\\hat{x}_1,\\hat{y}_1,T_1],\\; [\\hat{x}_2,\\hat{y}_2,T_2],\\; [\\hat{x}_3,\\hat{y}_3,T_3] \\big],\n  $$\n  rounded as specified, with no extra whitespace or text. For example, a syntactically valid output would look like\n  $$\n  [[0.123456,-1.234567,9.876543],[\\dots],[\\dots]].\n  $$\nThe expected answer types are lists of floats, and the final output aggregates these three lists into one top-level list on a single line.", "solution": "The problem presented is a standard unconstrained nonlinear optimization problem. The task is to find a local maximum of a given objective function $T(\\boldsymbol{x})$ using the method of steepest ascent, a specific form of gradient ascent. The problem is well-defined, scientifically sound, and all necessary parameters and conditions for a numerical solution are provided. Therefore, the problem is valid.\n\nThe solution proceeds in two stages. First, we derive the analytical form of the gradient of the objective function. Second, we implement the gradient ascent algorithm with a backtracking line search to numerically find a maximizer for each specified test case.\n\n**1. Derivation of the Gradient $\\nabla T(\\boldsymbol{x})$**\n\nThe objective function is given as:\n$$\nT(\\boldsymbol{x}) = \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right)\n$$\nwhere $\\boldsymbol{x} = (x_1, x_2)^\\top \\in \\mathbb{R}^2$ is the variable position.\n\nThe gradient of $T(\\boldsymbol{x})$, denoted $\\nabla T(\\boldsymbol{x})$, is a vector of its partial derivatives with respect to the components of $\\boldsymbol{x}$:\n$$\n\\nabla T(\\boldsymbol{x}) = \\begin{pmatrix} \\frac{\\partial T}{\\partial x_1} \\\\ \\frac{\\partial T}{\\partial x_2} \\end{pmatrix}\n$$\n\nDue to the linearity of the differentiation operator, the gradient of a sum is the sum of the gradients. We can thus compute the gradient of each term in the summation independently:\n$$\n\\nabla T(\\boldsymbol{x}) = \\nabla \\left( \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right) = \\sum_{i=1}^{m} \\nabla \\left( w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right)\n$$\n\nLet us analyze a single term $T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x}))$, where the exponent is $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2$.\nUsing the chain rule for vector functions, $\\nabla (\\exp(f(\\boldsymbol{x}))) = \\exp(f(\\boldsymbol{x})) \\nabla f(\\boldsymbol{x})$. Applying this, we get:\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x})) \\nabla f_i(\\boldsymbol{x})\n$$\n\nNow, we must compute the gradient of $f_i(\\boldsymbol{x})$. The squared Euclidean norm is $\\| \\boldsymbol{z} \\|_2^2 = \\boldsymbol{z}^\\top \\boldsymbol{z}$. Let $\\boldsymbol{z} = \\boldsymbol{x} - \\boldsymbol{\\mu}_i$. Then $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)^\\top (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)$.\nThe gradient of a quadratic form $\\boldsymbol{x}^\\top A \\boldsymbol{x}$ is $(A+A^\\top)\\boldsymbol{x}$. Here, we have a simpler case. Let $\\boldsymbol{\\mu}_i = (\\mu_{i1}, \\mu_{i2})^\\top$.\n$$\nf_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\left( (x_1 - \\mu_{i1})^2 + (x_2 - \\mu_{i2})^2 \\right)\n$$\nThe partial derivatives are:\n$$\n\\frac{\\partial f_i}{\\partial x_1} = -\\frac{1}{2s_i^2} \\cdot 2(x_1 - \\mu_{i1}) \\cdot 1 = -\\frac{1}{s_i^2}(x_1 - \\mu_{i1})\n$$\n$$\n\\frac{\\partial f_i}{\\partial x_2} = -\\frac{1}{2s_i^2} \\cdot 2(x_2 - \\mu_{i2}) \\cdot 1 = -\\frac{1}{s_i^2}(x_2 - \\mu_{i2})\n$$\nCombining these into the gradient vector $\\nabla f_i(\\boldsymbol{x})$:\n$$\n\\nabla f_i(\\boldsymbol{x}) = \\begin{pmatrix} -\\frac{1}{s_i^2}(x_1 - \\mu_{i1}) \\\\ -\\frac{1}{s_i^2}(x_2 - \\mu_{i2}) \\end{pmatrix} = -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)\n$$\n\nSubstituting this back into the expression for $\\nabla T_i(\\boldsymbol{x})$:\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) \\left( -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right)\n$$\n\nFinally, summing over all $i = 1, \\dots, m$ gives the complete gradient of the objective function $T(\\boldsymbol{x})$:\n$$\n\\nabla T(\\boldsymbol{x}) = \\sum_{i=1}^{m} \\left[ - \\frac{w_i}{s_i^2} \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right]\n$$\nThis expression is fundamental for the implementation of the gradient ascent algorithm.\n\n**2. Algorithmic Implementation**\n\nThe method of steepest ascent iteratively updates the current estimate $\\boldsymbol{x}_k$ by taking a step in the direction of the gradient $\\nabla T(\\boldsymbol{x}_k)$. The update rule is:\n$$\n\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k, \\quad \\text{where} \\quad \\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)\n$$\nThe step size $\\alpha_k$ is determined at each iteration using a backtracking line search to ensure sufficient increase in the objective function, as specified by the Armijo condition.\n\n**Algorithm: Gradient Ascent with Backtracking Line Search**\n\n1.  **Initialization**:\n    -   Set iteration counter $k = 0$.\n    -   Set initial point $\\boldsymbol{x}_0$.\n    -   Set parameters: initial step size $\\alpha_0$, shrink factor $\\rho$, Armijo constant $c$, tolerance $\\varepsilon$, max iterations $K_{\\max}$, minimum step size $\\alpha_{\\min}$.\n\n2.  **Iteration**: For $k = 0, 1, 2, \\ldots, K_{\\max}-1$:\n    a.  **Compute Gradient**: Calculate the gradient vector $\\boldsymbol{g}_k = \\nabla T(\\boldsymbol{x}_k)$ using the derived formula.\n    b.  **Check for Convergence**: If $\\| \\boldsymbol{g}_k \\|_2 < \\varepsilon$, the gradient is sufficiently small. Terminate and return $\\boldsymbol{x}_k$ as the estimated maximizer.\n    c.  **Backtracking Line Search**: Find a step size $\\alpha_k$.\n        i.   Initialize step size $\\alpha = \\alpha_0$.\n        ii.  The ascent direction is $\\boldsymbol{d}_k = \\boldsymbol{g}_k$.\n        iii. Calculate the right-hand side of the Armijo condition: $RHS = T(\\boldsymbol{x}_k) + c \\alpha \\boldsymbol{g}_k^\\top \\boldsymbol{d}_k = T(\\boldsymbol{x}_k) + c \\alpha \\| \\boldsymbol{g}_k \\|_2^2$.\n        iv.  While $\\alpha \\ge \\alpha_{\\min}$:\n             -   Compute the candidate point: $\\boldsymbol{x}_{\\text{new}} = \\boldsymbol{x}_k + \\alpha \\boldsymbol{d}_k$.\n             -   Evaluate the objective at the new point: $LHS = T(\\boldsymbol{x}_{\\text{new}})$.\n             -   If $LHS \\ge RHS$, the Armijo condition is satisfied. Set $\\alpha_k = \\alpha$ and break the inner while-loop.\n             -   Else, shrink the step size: $\\alpha = \\rho \\alpha$.\n        v.   If the loop terminates because $\\alpha < \\alpha_{\\min}$, the step size is too small to make progress. Terminate the main algorithm and return the current point $\\boldsymbol{x}_k$.\n    d.  **Update Position**: Update the estimate: $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$.\n\n3.  **Termination**: If the loop completes by reaching $K_{\\max}$, terminate and return the final point $\\boldsymbol{x}_{K_{\\max}}$.\n\nThis algorithm is applied to each of the three test cases using the specified parameters.\n\n-   **Case 1** starts at $\\boldsymbol{x}_0 = (0,0)$ and is expected to converge to one of the three local maxima. The combination of weights, spreads, and distance from the origin determines which basin of attraction the algorithm follows.\n-   **Case 2** starts at $\\boldsymbol{x}_0 = (0,0)$, which is a saddle point due to the symmetry of the setup. At this point, $\\nabla T(0,0) = \\boldsymbol{0}$. The algorithm is expected to terminate immediately at iteration $0$, as the gradient norm is below the tolerance $\\varepsilon$.\n-   **Case 3** demonstrates sensitivity to initialization. The starting point $\\boldsymbol{x}_0 = (-10,-10)$ is in the basin of attraction of the local maximum near $\\boldsymbol{\\mu}_2 = (-5,-5)$, even though the global maximum is near $\\boldsymbol{\\mu}_1 = (5,5)$ due to its higher weight $w_1=200 > w_2=180$. The algorithm is expected to converge to this local maximum.\n\nThe implementation will consist of functions to compute $T(\\boldsymbol{x})$ and $\\nabla T(\\boldsymbol{x})$, and a main solver function that orchestrates the iterative process described above for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases and prints the result.\n    \"\"\"\n\n    # Define test cases as a list of dictionaries.\n    # Each dictionary contains: centers, initial_x, m\n    # 'centers' is a list of tuples (mu, w, s)\n    test_cases = [\n        {\n            \"m\": 3,\n            \"centers\": [\n                (np.array([1.0, 2.0]), 100.0, 1.0),\n                (np.array([-2.0, -1.0]), 80.0, 1.5),\n                (np.array([3.0, -3.0]), 120.0, 0.7),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([-2.0, 0.0]), 50.0, 0.5),\n                (np.array([2.0, 0.0]), 50.0, 0.5),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([5.0, 5.0]), 200.0, 2.0),\n                (np.array([-5.0, -5.0]), 180.0, 1.0),\n            ],\n            \"initial_x\": np.array([-10.0, -10.0]),\n        },\n    ]\n\n    # Algorithmic parameters\n    params = {\n        \"alpha_0\": 1.0,\n        \"rho\": 0.5,\n        \"c\": 1e-4,\n        \"epsilon\": 1e-8,\n        \"k_max\": 10000,\n        \"alpha_min\": 1e-12,\n    }\n\n    results = []\n    for case in test_cases:\n        result_x = run_gradient_ascent(case, params)\n        final_T = T_func(result_x, case[\"centers\"])\n        # Format output as [x, y, T(x)] rounded to 6 decimal places\n        formatted_result = [\n            round(result_x[0], 6),\n            round(result_x[1], 6),\n            round(final_T, 6),\n        ]\n        results.append(formatted_result)\n    \n    # Final print statement in the exact required format\n    # Example: [[0.123456,-1.234567,9.876543],[...],[...]]\n    print(str(results).replace(\" \", \"\"))\n\n\ndef T_func(x, centers):\n    \"\"\"\n    Computes the objective function T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    total_intensity = 0.0\n    for mu_i, w_i, s_i in centers:\n        norm_sq = np.sum((x - mu_i)**2)\n        exponent = -1.0 / (2.0 * s_i**2) * norm_sq\n        total_intensity += w_i * np.exp(exponent)\n    return total_intensity\n\ndef grad_T_func(x, centers):\n    \"\"\"\n    Computes the gradient of the objective function nabla T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    grad = np.zeros(2)\n    for mu_i, w_i, s_i in centers:\n        diff = x - mu_i\n        norm_sq = np.sum(diff**2)\n        s_i_sq = s_i**2\n        exp_term = np.exp(-1.0 / (2.0 * s_i_sq) * norm_sq)\n        grad += (-w_i / s_i_sq) * exp_term * diff\n    return grad\n\ndef run_gradient_ascent(case, params):\n    \"\"\"\n    Performs gradient ascent with backtracking line search for a single case.\n    \"\"\"\n    x_k = case[\"initial_x\"].copy()\n    centers = case[\"centers\"]\n    \n    alpha_0 = params[\"alpha_0\"]\n    rho = params[\"rho\"]\n    c = params[\"c\"]\n    epsilon = params[\"epsilon\"]\n    k_max = params[\"k_max\"]\n    alpha_min = params[\"alpha_min\"]\n\n    for _ in range(k_max):\n        grad_k = grad_T_func(x_k, centers)\n        grad_norm = np.linalg.norm(grad_k)\n\n        if grad_norm < epsilon:\n            break\n\n        # Backtracking line search\n        alpha = alpha_0\n        d_k = grad_k\n        T_k = T_func(x_k, centers)\n        armijo_rhs_const = c * np.dot(grad_k, d_k) # same as c * grad_norm**2\n\n        while alpha >= alpha_min:\n            x_new = x_k + alpha * d_k\n            T_new = T_func(x_new, centers)\n            \n            if T_new >= T_k + alpha * armijo_rhs_const:\n                x_k = x_new\n                break\n            \n            alpha *= rho\n        \n        if alpha < alpha_min:\n            # Cannot find a suitable step, terminate.\n            break\n            \n    return x_k\n\nsolve()\n```"}]}