{"hands_on_practices": [{"introduction": "Financial decisions often rely on models with parameters that are estimated from noisy data. This exercise explores the concept of *conditioning* by examining how a project's Net Present Value ($NPV$) can be highly sensitive to its estimated beta ($\\beta$) in the Capital Asset Pricing Model (CAPM). By determining the smallest estimation error that flips a crucial investment decision, you will gain a tangible intuition for how small input errors can lead to dramatically different outcomes, a hallmark of ill-conditioned problems. [@problem_id:2370897]", "id": "2370897", "problem": "A firm considers an all-equity project requiring an initial outlay of $I = 199$ at time $t=0$. The project is expected to generate a stream of free cash flows starting at time $t=1$ equal to $C_{1} = 1$, which then grow at a constant rate $g = 0.06$ each period forever. The firm discounts cash flows using the Capital Asset Pricing Model (CAPM), where the cost of equity is $r(\\beta) = r_{f} + \\beta\\big(E[R_{m}] - r_{f}\\big)$. The risk-free rate is $r_{f} = 0.02$, and the market risk premium is $E[R_{m}] - r_{f} = 0.05$. The firmâ€™s true beta is $\\beta^{\\ast} = 0.9$.\n\nThe investment rule is to accept the project if the net present value computed using the measured beta $\\widehat{\\beta}$ is nonnegative, and to reject otherwise. A wrong decision occurs if the rule based on $\\widehat{\\beta}$ differs from the decision that would be made using the true $\\beta^{\\ast}$.\n\nCompute the smallest absolute estimation error in beta, $|\\widehat{\\beta} - \\beta^{\\ast}|$, that would flip the decision from acceptance under the true $\\beta^{\\ast}$ to rejection when using $\\widehat{\\beta}$. Provide your answer as an exact fraction.", "solution": "The problem statement must first be validated for scientific soundness, well-posedness, and objectivity.\n\n**Step 1: Extract Givens**\n- Initial outlay: $I = 199$\n- Initial cash flow at $t=1$: $C_{1} = 1$\n- Constant growth rate of cash flows: $g = 0.06$\n- Cost of equity model (CAPM): $r(\\beta) = r_{f} + \\beta\\big(E[R_{m}] - r_{f}\\big)$\n- Risk-free rate: $r_{f} = 0.02$\n- Market risk premium: $E[R_{m}] - r_{f} = 0.05$\n- True project beta: $\\beta^{\\ast} = 0.9$\n- Measured project beta: $\\widehat{\\beta}$\n- Investment rule: Accept if Net Present Value $NPV(\\widehat{\\beta}) \\ge 0$, reject otherwise.\n- A wrong decision occurs if the outcome with $\\widehat{\\beta}$ is different from the outcome with $\\beta^{\\ast}$.\n- The objective is to find the smallest absolute estimation error, $|\\widehat{\\beta} - \\beta^{\\ast}|$, that leads to a decision flip from acceptance to rejection.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is based on standard, well-established principles of corporate finance: the growing perpetuity model for valuation (Gordon Growth Model) and the Capital Asset Pricing Model (CAPM) for determining the discount rate. The provided data is self-contained and internally consistent. The perpetuity formula requires the discount rate $r$ to be greater than the growth rate $g$. Using the true beta $\\beta^{\\ast} = 0.9$, the discount rate is $r(\\beta^{\\ast}) = 0.02 + 0.9(0.05) = 0.02 + 0.045 = 0.065$. This value is greater than the growth rate $g=0.06$, so the valuation is well-defined. The problem is thus scientifically grounded and well-posed. The language is objective and quantitative. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will now be provided.\n\n**Solution Derivation**\n\nThe Net Present Value (NPV) of a project with an initial outlay $I$ and a perpetual stream of cash flows starting at $C_1$ and growing at a rate $g$ is given by the formula for a growing perpetuity:\n$$\nNPV = \\frac{C_1}{r - g} - I\n$$\nwhere $r$ is the appropriate discount rate. This formula is valid only if $r > g$.\n\nThe discount rate $r$ is the cost of equity, determined by the Capital Asset Pricing Model (CAPM) as a function of the project's beta, $\\beta$:\n$$\nr(\\beta) = r_{f} + \\beta\\big(E[R_{m}] - r_{f}\\big)\n$$\nSubstituting the given values, we have:\n$$\nr(\\beta) = 0.02 + \\beta(0.05)\n$$\nThe NPV as a function of $\\beta$ is therefore:\n$$\nNPV(\\beta) = \\frac{C_1}{r_f + \\beta(E[R_m] - r_f) - g} - I = \\frac{1}{0.02 + 0.05\\beta - 0.06} - 199 = \\frac{1}{0.05\\beta - 0.04} - 199\n$$\n\nFirst, we determine the correct investment decision using the true beta, $\\beta^{\\ast} = 0.9$.\nThe true discount rate is:\n$$\nr(\\beta^{\\ast}) = r(0.9) = 0.02 + 0.9(0.05) = 0.02 + 0.045 = 0.065\n$$\nSince $r(\\beta^{\\ast}) = 0.065 > g = 0.06$, the valuation is valid. The NPV under the true beta is:\n$$\nNPV(\\beta^{\\ast}) = \\frac{1}{0.065 - 0.06} - 199 = \\frac{1}{0.005} - 199 = 200 - 199 = 1\n$$\nSince $NPV(\\beta^{\\ast}) = 1 > 0$, the correct decision is to **accept** the project.\n\nThe problem asks for the smallest estimation error $|\\widehat{\\beta} - \\beta^{\\ast}|$ that would flip this decision to **rejection**. A rejection occurs when the calculated NPV is negative, i.e., $NPV(\\widehat{\\beta}) < 0$. The decision flips at the boundary where $NPV(\\widehat{\\beta}) = 0$. Let us find the critical beta, $\\widehat{\\beta}_{crit}$, for which this is true.\n$$\nNPV(\\widehat{\\beta}_{crit}) = \\frac{1}{r(\\widehat{\\beta}_{crit}) - g} - I = 0\n$$\n$$\n\\frac{1}{r(\\widehat{\\beta}_{crit}) - 0.06} = 199\n$$\n$$\nr(\\widehat{\\beta}_{crit}) - 0.06 = \\frac{1}{199}\n$$\n$$\nr(\\widehat{\\beta}_{crit}) = 0.06 + \\frac{1}{199}\n$$\nNow, we substitute the CAPM formula for $r(\\widehat{\\beta}_{crit})$:\n$$\n0.02 + 0.05\\widehat{\\beta}_{crit} = 0.06 + \\frac{1}{199}\n$$\nSolving for $\\widehat{\\beta}_{crit}$:\n$$\n0.05\\widehat{\\beta}_{crit} = 0.06 - 0.02 + \\frac{1}{199} = 0.04 + \\frac{1}{199}\n$$\n$$\n\\widehat{\\beta}_{crit} = \\frac{0.04 + \\frac{1}{199}}{0.05} = \\frac{0.04}{0.05} + \\frac{1}{199 \\times 0.05} = \\frac{4}{5} + \\frac{1}{9.95} = \\frac{4}{5} + \\frac{1}{199/20} = \\frac{4}{5} + \\frac{20}{199}\n$$\nTo combine these fractions, we find a common denominator:\n$$\n\\widehat{\\beta}_{crit} = \\frac{4 \\times 199 + 20 \\times 5}{5 \\times 199} = \\frac{796 + 100}{995} = \\frac{896}{995}\n$$\nThe $NPV(\\beta)$ function, $NPV(\\beta) = \\frac{1}{0.05\\beta - 0.04} - 199$, is a decreasing function of $\\beta$ for $\\beta > 0.04/0.05 = 0.8$. Since $\\beta^{\\ast} = 0.9 > 0.8$, we are in the region where a higher beta leads to a lower NPV.\nThe correct decision is acceptance ($NPV > 0$). A flip to rejection ($NPV < 0$) requires the estimated beta $\\widehat{\\beta}$ to be greater than the critical beta $\\widehat{\\beta}_{crit}$.\nSince $NPV(\\beta^{\\ast}) > 0$ and $NPV(\\widehat{\\beta}_{crit}) = 0$, and $NPV$ is a decreasing function of $\\beta$, it must be that $\\widehat{\\beta}_{crit} > \\beta^{\\ast}$.\nLet's verify: $\\widehat{\\beta}_{crit} = \\frac{896}{995} \\approx 0.9005025$ and $\\beta^{\\ast} = 0.9$. Indeed, $\\widehat{\\beta}_{crit} > \\beta^{\\ast}$.\n\nThe smallest absolute estimation error, $|\\widehat{\\beta} - \\beta^{\\ast}|$, that would cause the decision to flip is the difference between the critical beta and the true beta. For any error larger than this, rejection will occur if the error is positive ($\\widehat{\\beta} > \\beta^{\\ast}$).\n$$\n|\\widehat{\\beta}_{crit} - \\beta^{\\ast}| = \\widehat{\\beta}_{crit} - \\beta^{\\ast} = \\frac{896}{995} - 0.9 = \\frac{896}{995} - \\frac{9}{10}\n$$\nWe compute the difference using a common denominator of $9950$:\n$$\n\\frac{896 \\times 10}{9950} - \\frac{9 \\times 995}{9950} = \\frac{8960 - 8955}{9950} = \\frac{5}{9950}\n$$\nSimplifying this fraction by dividing the numerator and denominator by $5$:\n$$\n\\frac{5 \\div 5}{9950 \\div 5} = \\frac{1}{1990}\n$$\nThus, the smallest absolute estimation error in beta that flips the decision from acceptance to rejection is $\\frac{1}{1990}$.", "answer": "$$\n\\boxed{\\frac{1}{1990}}\n$$"}, {"introduction": "Many complex problems in economics and finance are linearized and solved as a system of equations $Ax=b$. This practice powerfully illustrates the critical distinction between a problem's intrinsic sensitivity (conditioning) and the reliability of the algorithm used to solve it (stability). You will analyze a scenario where a well-conditioned system, one that is not inherently sensitive to small changes, yields a completely inaccurate solution when solved with a naive algorithm, demonstrating the vital importance of stable numerical methods like Gaussian elimination with pivoting. [@problem_id:2370924]", "id": "2370924", "problem": "Consider a linear system of equations that encodes a pairwise no-arbitrage balance between two synthetic portfolios in a frictional market with a small proportional distortion. Let the unknown vector be $x \\in \\mathbb{R}^{2}$, and for a given parameter $\\varepsilon \\in (0,1)$ define the coefficient matrix and right-hand side by\n$$\nA(\\varepsilon) \\equiv \\begin{bmatrix}\n\\varepsilon & 1 \\\\\n1 & 1\n\\end{bmatrix}, \\qquad \nb \\equiv \\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}.\n$$\nThis system can be interpreted as a linearized set of two no-arbitrage consistency equations linking two replicating strategies with a small friction parameter $\\varepsilon$ appearing only in the first equation. The equations are non-dimensional and require no physical units.\n\nDefine the spectral condition number with respect to the Euclidean norm by\n$$\n\\kappa_{2}\\!\\left(A(\\varepsilon)\\right) \\equiv \\|A(\\varepsilon)\\|_{2}\\,\\|A(\\varepsilon)^{-1}\\|_{2}.\n$$\n\nDefine two computed approximations to the solution $x$ as follows:\n1. $x_{\\mathrm{np}}(\\varepsilon)$ is the result of solving $A(\\varepsilon)\\,x=b$ by a triangular factorization that keeps the row order fixed at every step (no row exchanges).\n2. $x_{\\mathrm{pp}}(\\varepsilon)$ is the result of solving $A(\\varepsilon)\\,x=b$ by a triangular factorization that, at each elimination step, reorders rows within the active submatrix so that the pivot in the current column has maximal absolute value (row exchanges allowed within the column).\n\nLet the exact solution be denoted $x^{\\star}(\\varepsilon)$, and define the relative error of any approximation $y \\in \\mathbb{R}^{2}$ by\n$$\n\\mathcal{E}(y;\\varepsilon) \\equiv \\frac{\\|y - x^{\\star}(\\varepsilon)\\|_{2}}{\\|x^{\\star}(\\varepsilon)\\|_{2}}.\n$$\n\nYour program must, for each specified test case value of $\\varepsilon$, compute the triple \n$$\n\\left(\\kappa_{2}\\!\\left(A(\\varepsilon)\\right), \\ \\mathcal{E}\\!\\left(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon\\right), \\ \\mathcal{E}\\!\\left(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon\\right)\\right),\n$$\nand then evaluate the boolean\n$$\nS(\\varepsilon) \\equiv \\Big(\\kappa_{2}\\!\\left(A(\\varepsilon)\\right) \\leq 10\\Big) \\ \\wedge \\ \\Big(\\mathcal{E}\\!\\left(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon\\right) \\geq 10^{-3}\\Big) \\ \\wedge \\ \\Big(\\mathcal{E}\\!\\left(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon\\right) \\leq 10^{-12}\\Big).\n$$\nThe intended interpretation is that $S(\\varepsilon)$ is true precisely when the system is well-conditioned while the fixed-row-order elimination produces a large forward error and the row-exchanging elimination remains highly accurate.\n\nTest Suite:\n- $\\varepsilon = 10^{-16}$\n- $\\varepsilon = 10^{-12}$\n- $\\varepsilon = 10^{-8}$\n- $\\varepsilon = 10^{-4}$\n- $\\varepsilon = 10^{-1}$\n\nAnswer specification:\n- For each test case in the order listed above, compute $S(\\varepsilon)$ as defined.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_{1},r_{2},r_{3},r_{4},r_{5}]$, where each $r_{i}$ is either the literal token True or False corresponding to $S(\\varepsilon)$ for the $i$-th test case.\n\nAll calculations must be carried out in standard double-precision floating-point arithmetic. Angles and physical units are not involved in this task, and no percentages are required; any ratios are to be reported as pure decimal numbers.", "solution": "The problem statement submitted for analysis is deemed valid. It is a well-posed, scientifically grounded problem in numerical linear algebra, formulated with objective and precise language. All necessary data, definitions, and constraints are provided, and there are no internal contradictions or logical flaws. The problem asks for an analysis of algorithmic stability, a fundamental concept in computational science. We shall proceed with the solution.\n\nThe objective is to investigate the numerical stability of solving the linear system $A(\\varepsilon)x = b$ for various small values of the parameter $\\varepsilon$. Specifically, we must compare the accuracy of two methods based on triangular factorization: one that maintains a fixed row order (Gaussian elimination without pivoting) and one that reorders rows to use the pivot of largest magnitude (Gaussian elimination with partial pivoting).\n\nFirst, let us determine the exact solution, $x^{\\star}(\\varepsilon)$, against which the numerical approximations will be compared. The matrix $A(\\varepsilon)$ is given by\n$$\nA(\\varepsilon) = \\begin{bmatrix} \\varepsilon & 1 \\\\ 1 & 1 \\end{bmatrix}.\n$$\nThe determinant is $\\det(A(\\varepsilon)) = \\varepsilon \\cdot 1 - 1 \\cdot 1 = \\varepsilon - 1$. Since the problem specifies $\\varepsilon \\in (0,1)$, the determinant is non-zero, and the matrix is always invertible. The inverse is\n$$\nA(\\varepsilon)^{-1} = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} 1 & -1 \\\\ -1 & \\varepsilon \\end{bmatrix}.\n$$\nThe exact solution $x^{\\star}(\\varepsilon) = A(\\varepsilon)^{-1}b$ is therefore\n$$\nx^{\\star}(\\varepsilon) = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} 1 & -1 \\\\ -1 & \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} 1 - 2 \\\\ -1 + 2\\varepsilon \\end{bmatrix} = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} -1 \\\\ -1 + 2\\varepsilon \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{1-\\varepsilon} \\\\ \\frac{1-2\\varepsilon}{1-\\varepsilon} \\end{bmatrix}.\n$$\nAs $\\varepsilon \\to 0$, the exact solution approaches $x^{\\star}(0) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. The solution itself is well-behaved for small $\\varepsilon$.\n\nNext, we analyze the conditioning of the problem. The spectral condition number $\\kappa_{2}(A(\\varepsilon))$ measures the sensitivity of the solution $x$ to perturbations in $A$ or $b$. For $\\varepsilon \\to 0$, the matrix $A(\\varepsilon)$ approaches $A(0) = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$. The singular values of $A(0)$ are the square roots of the eigenvalues of $A(0)^T A(0) = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\end{bmatrix}$. The eigenvalues of this matrix are $\\lambda = (3 \\pm \\sqrt{5})/2$. The singular values are $\\sigma_{\\max} = \\sqrt{(3 + \\sqrt{5})/2}$ and $\\sigma_{\\min} = \\sqrt{(3 - \\sqrt{5})/2}$. The condition number is\n$$\n\\kappa_{2}(A(0)) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{\\sqrt{3+\\sqrt{5}}}{\\sqrt{3-\\sqrt{5}}} = \\frac{3+\\sqrt{5}}{2} \\approx 2.618.\n$$\nThis is a small number, indicating that for small $\\varepsilon$, the matrix $A(\\varepsilon)$ is well-conditioned. The problem is not inherently sensitive to small perturbations. The first condition in $S(\\varepsilon)$, which is $\\kappa_{2}(A(\\varepsilon)) \\leq 10$, will therefore be satisfied for all test values of $\\varepsilon$. Any large errors in a computed solution must therefore arise from the instability of the algorithm used, not from the ill-conditioning of the problem itself.\n\nLet us now analyze the first numerical method, $x_{\\mathrm{np}}(\\varepsilon)$, which corresponds to Gaussian elimination with no pivoting. We perform an LU factorization of $A(\\varepsilon) = LU$.\n$$\nA(\\varepsilon) = \\begin{bmatrix} \\varepsilon & 1 \\\\ 1 & 1 \\end{bmatrix}.\n$$\nTo eliminate the entry $a_{21}=1$, we use the multiplier $m_{21} = a_{21}/a_{11} = 1/\\varepsilon$. The resulting upper triangular matrix $U$ is\n$$\nU = \\begin{bmatrix} \\varepsilon & 1 \\\\ 0 & 1 - 1/\\varepsilon \\end{bmatrix}.\n$$\nAnd the lower triangular matrix $L$ is\n$$\nL = \\begin{bmatrix} 1 & 0 \\\\ 1/\\varepsilon & 1 \\end{bmatrix}.\n$$\nFor a small $\\varepsilon$ (e.g., $\\varepsilon \\approx 10^{-16}$), the multiplier $1/\\varepsilon$ becomes extremely large. In standard double-precision floating-point arithmetic, which has a machine epsilon of approximately $2.2 \\times 10^{-16}$, the computation of the term $\\mathrm{fl}(1 - 1/\\varepsilon)$ suffers from catastrophic cancellation. The number $1$ is insignificant compared to $1/\\varepsilon$, so the result is simply $\\mathrm{fl}(-1/\\varepsilon)$.\nThe system is solved via forward substitution $Ly=b$ and backward substitution $Ux=y$.\nSolving $Ly=b$:\n$y_1 = b_1 = 1$.\n$(1/\\varepsilon)y_1 + y_2 = b_2 \\implies y_2 = 2 - 1/\\varepsilon$.\nIn floating-point arithmetic, $\\mathrm{fl}(y_2) = \\mathrm{fl}(2 - 1/\\varepsilon) \\approx -1/\\varepsilon$.\nSolving $Ux=y$:\nThe $(2,2)$ element of the computed $U$ matrix is $\\tilde{u}_{22} = \\mathrm{fl}(1-1/\\varepsilon) \\approx -1/\\varepsilon$.\n$\\tilde{u}_{22} x_2 = \\tilde{y}_2 \\implies (-1/\\varepsilon) x_2 \\approx -1/\\varepsilon \\implies x_2 \\approx 1$.\n$\\varepsilon x_1 + x_2 = y_1 \\implies \\varepsilon x_1 + 1 \\approx 1 \\implies \\varepsilon x_1 \\approx 0 \\implies x_1 \\approx 0$.\nThe computed solution is $x_{\\mathrm{np}}(\\varepsilon) \\approx \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nComparing this to the exact solution for small $\\varepsilon$, $x^{\\star}(\\varepsilon) \\approx \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, the error is substantial. The relative error is\n$$\n\\mathcal{E}(x_{\\mathrm{np}};\\varepsilon) = \\frac{\\|x_{\\mathrm{np}} - x^{\\star}(\\varepsilon)\\|_{2}}{\\|x^{\\star}(\\varepsilon)\\|_{2}} \\approx \\frac{\\|\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\|_{2}}{\\|\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\|_{2}} = \\frac{\\|\\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}\\|_{2}}{\\sqrt{1^2+1^2}} = \\frac{1}{\\sqrt{2}} \\approx 0.707.\n$$\nThis is a very large error, far exceeding $10^{-3}$. This illustrates the numerical instability of Gaussian elimination without pivoting when a small pivot is encountered. This will hold for any $\\varepsilon$ small enough to cause this floating-point behavior. This instability disappears only when $\\varepsilon$ is large enough that $1/\\varepsilon$ is not large. For $\\varepsilon = 10^{-1}$, the multiplier is $10$, and the calculation $1-10=-9$ is exact in floating-point. In this case, no-pivot elimination is accurate. Hence, the condition $\\mathcal{E}(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon) \\geq 10^{-3}$ will be true for small $\\varepsilon$ but false for $\\varepsilon = 10^{-1}$.\n\nNow, we analyze the second method, $x_{\\mathrm{pp}}(\\varepsilon)$, corresponding to Gaussian elimination with partial pivoting. At each step, rows are exchanged to ensure the pivot element (the diagonal element) is the largest in its column within the active submatrix.\nFor $A(\\varepsilon) = \\begin{bmatrix} \\varepsilon & 1 \\\\ 1 & 1 \\end{bmatrix}$ and $\\varepsilon \\in (0,1)$, we have $|a_{21}| = 1 > |\\varepsilon| = |a_{11}|$. Thus, we must swap row $1$ and row $2$. This is equivalent to multiplying by a permutation matrix $P = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$. The system becomes $PAx=Pb$.\n$$\nPA = \\begin{bmatrix} 1 & 1 \\\\ \\varepsilon & 1 \\end{bmatrix}, \\quad Pb = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.\n$$\nNow we perform elimination on this new system. The multiplier is $m_{21} = \\varepsilon/1 = \\varepsilon$. Since $|\\varepsilon|<1$, the multiplier is small, which is the hallmark of a stable elimination step.\nThe upper triangular matrix $U$ becomes\n$$\nU = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1-\\varepsilon \\end{bmatrix}.\n$$\nThe operations are $y_1=2$, $y_2=1-2\\varepsilon$, then $(1-\\varepsilon)x_2 = 1-2\\varepsilon \\implies x_2 = (1-2\\varepsilon)/(1-\\varepsilon)$, and $x_1+x_2=2 \\implies x_1=2-x_2 = 1/(1-\\varepsilon)$. The computed solution is identical to the analytical form of the exact solution. As all operations involve well-conditioned arithmetic (no subtractions of nearly equal large numbers), the floating-point calculation will be highly accurate. The relative error $\\mathcal{E}(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon)$ will be on the order of machine epsilon (around $10^{-16}$), thus satisfying the condition $\\mathcal{E}(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon) \\leq 10^{-12}$ for all test cases.\n\nIn summary, for $\\varepsilon \\in \\{10^{-16}, 10^{-12}, 10^{-8}, 10^{-4}\\}$:\n1.  $\\kappa_{2}(A(\\varepsilon)) \\leq 10$ is `True`.\n2.  $\\mathcal{E}(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon) \\geq 10^{-3}$ is `True` due to instability.\n3.  $\\mathcal{E}(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon) \\leq 10^{-12}$ is `True` due to stability.\nThus, $S(\\varepsilon)$ evaluates to `True`.\n\nFor $\\varepsilon=10^{-1}$:\n1.  $\\kappa_{2}(A(0.1)) \\leq 10$ is `True`.\n2.  $\\mathcal{E}(x_{\\mathrm{np}}(0.1);\\varepsilon) \\geq 10^{-3}$ is `False` because the multiplier $1/0.1=10$ is not large enough to cause significant floating-point error, and the calculation is accurate.\n3.  $\\mathcal{E}(x_{\\mathrm{pp}}(0.1);\\varepsilon) \\leq 10^{-12}$ is `True`.\nSince the second condition is false, $S(0.1)$ evaluates to `False`.\n\nThe program will implement these calculations and confirm this reasoning.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# The problem can be solved with numpy; scipy is not strictly necessary but is permitted.\n\ndef solve():\n    \"\"\"\n    Solves the specified problem by analyzing the numerical stability of solving\n    a linear system with and without pivoting.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1e-16,\n        1e-12,\n        1e-8,\n        1e-4,\n        1e-1,\n    ]\n\n    results = []\n    \n    for eps in test_cases:\n        # Define the matrix A and vector b for the current epsilon\n        A = np.array([[eps, 1.0], [1.0, 1.0]], dtype=np.float64)\n        b = np.array([1.0, 2.0], dtype=np.float64)\n\n        # --- 1. Calculate the 'exact' solution x_star ---\n        # Using the analytical formula derived from the problem.\n        # This gives a high-precision reference for error calculation.\n        x_star = np.array([1.0 / (1.0 - eps), (1.0 - 2.0 * eps) / (1.0 - eps)], dtype=np.float64)\n\n        # --- 2. Calculate the condition number kappa_2 ---\n        kappa_2 = np.linalg.cond(A, 2)\n\n        # --- 3. Calculate the solution with no pivoting (x_np) ---\n        # This requires manually implementing Gaussian elimination without row swaps.\n        def solve_no_pivot(mat_A, vec_b):\n            n = len(vec_b)\n            A_np = mat_A.copy()\n            b_np = vec_b.copy()\n\n            # Forward elimination to create an upper triangular matrix\n            # This is a 2x2 specific implementation for simplicity and clarity.\n            if A_np[0, 0] == 0:\n                # This case isn't hit for eps > 0 but is a necessary check\n                # for a general algorithm. A zero pivot without pivoting fails.\n                return np.array([np.nan, np.nan])\n            \n            m = A_np[1, 0] / A_np[0, 0]\n            A_np[1, :] -= m * A_np[0, :]\n            b_np[1] -= m * b_np[0]\n            \n            # Backward substitution\n            x = np.zeros(n, dtype=np.float64)\n            if A_np[1, 1] == 0:\n                # Another fail condition, singular matrix after elimination.\n                return np.array([np.nan, np.nan])\n                \n            x[1] = b_np[1] / A_np[1, 1]\n            x[0] = (b_np[0] - A_np[0, 1] * x[1]) / A_np[0, 0]\n            \n            return x\n\n        x_np = solve_no_pivot(A, b)\n        \n        # --- 4. Calculate the solution with partial pivoting (x_pp) ---\n        # np.linalg.solve uses LAPACK routines which employ partial pivoting by default.\n        x_pp = np.linalg.solve(A, b)\n\n        # --- 5. Calculate relative errors ---\n        norm_x_star = np.linalg.norm(x_star, 2)\n        if norm_x_star == 0:\n             # Avoid division by zero, though not relevant for this problem.\n            rel_error_np = np.linalg.norm(x_np - x_star, 2)\n            rel_error_pp = np.linalg.norm(x_pp - x_star, 2)\n        else:\n            rel_error_np = np.linalg.norm(x_np - x_star, 2) / norm_x_star\n            rel_error_pp = np.linalg.norm(x_pp - x_star, 2) / norm_x_star\n            \n        # --- 6. Evaluate the boolean condition S(epsilon) ---\n        S = (kappa_2 <= 10.0) and (rel_error_np >= 1e-3) and (rel_error_pp <= 1e-12)\n        results.append(S)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Discretization methods, such as the binomial tree model for option pricing, are foundational tools in computational finance. This hands-on coding problem investigates a subtle form of algorithmic instability known as oscillatory convergence, which can arise when approximating continuous-time models. You will observe how the calculated price for a deep out-of-the-money option can fail to converge smoothly, instead oscillating as the number of time steps in the model is increased, highlighting the numerical complexities that can appear in practical financial modeling. [@problem_id:2370925]", "id": "2370925", "problem": "Consider an arbitrage-free market in which the underlying asset price process is modeled under the risk-neutral probability measure as a recombining binomial tree that approximates a continuous-time geometric Brownian motion over maturity horizon $T$ with continuously compounded risk-free rate $r$, continuous dividend yield $q$, and volatility $\\sigma$. For a given number of time steps $N \\in \\mathbb{N}$, let $\\Delta t = T/N$, and define the Coxâ€“Rossâ€“Rubinstein (CRR) up and down multipliers by $u = \\exp(\\sigma \\sqrt{\\Delta t})$ and $d = 1/u$, and the risk-neutral probability by $p = \\dfrac{\\exp\\left((r - q)\\Delta t\\right) - d}{u - d}$, with $0 < p < 1$. For a European call option with spot $S_0$, strike $K$, and maturity $T$, define the $N$-step binomial price $V_N$ by the risk-neutral valuation\n$$\nV_N = \\exp(-r T) \\sum_{k=0}^{N} \\binom{N}{k} p^k (1-p)^{N-k} \\max\\!\\left(S_0\\, u^{k} d^{N-k} - K, 0\\right).\n$$\nYou will examine the stability of $V_N$ as a function of the time-step count $N$ for several parameter sets, focusing on deep out-of-the-money conditions. For a fixed parameter set, define the sequence $\\{V_{N_i}\\}$ for $N_i \\in \\{1,2,3,4,5,6,7,8,9,10,11,12\\}$. Using this sequence, compute the following two scalar diagnostics that quantify oscillation and instability when too few steps are used:\n- The oscillation count $O$, defined as the number of strict sign changes in the first differences of the sequence, that is, let $\\Delta_i = V_{N_{i+1}} - V_{N_{i}}$ for $i = 1,\\dots,11$, ignore any $\\Delta_i$ with $|\\Delta_i| \\le \\varepsilon$ for $\\varepsilon = 10^{-12}$, and count the number of indices $i$ such that $\\Delta_i \\Delta_{i-1} < 0$ in the remaining subsequence.\n- The instability ratio $R$, defined by\n$$\nR = \\frac{\\sum_{i=1}^{11} |\\Delta_i|}{\\left|V_{N_{12}} - V_{N_{1}}\\right| + \\varepsilon},\n$$\nwith the same $\\varepsilon = 10^{-12}$. Values of $R$ greater than $1$ indicate oscillatory behavior that increases the total variation beyond the net change from $N=1$ to $N=12$.\n\nUse the following test suite of parameter sets, each specifying $(S_0, K, r, q, \\sigma, T)$:\n- Test case $1$ (deep out-of-the-money): $(S_0, K, r, q, \\sigma, T) = (100, 150, 0.05, 0, 0.20, 1)$.\n- Test case $2$ (at-the-money control): $(S_0, K, r, q, \\sigma, T) = (100, 100, 0.05, 0, 0.20, 1)$.\n- Test case $3$ (deeper out-of-the-money): $(S_0, K, r, q, \\sigma, T) = (100, 200, 0.05, 0, 0.20, 1)$.\n- Test case $4$ (short-maturity, deep out-of-the-money): $(S_0, K, r, q, \\sigma, T) = (100, 150, 0.05, 0, 0.20, 0.25)$.\n\nYour program must, for each test case, compute the sequence $\\{V_{N}\\}_{N=1}^{12}$, then compute the pair $(O, R)$ as defined above. The required final output format is a single line containing a comma-separated list enclosed in square brackets, aggregating the results for the four test cases in order as\n$[O_1, R_1, O_2, R_2, O_3, R_3, O_4, R_4]$,\nwhere each $O_j$ is an integer and each $R_j$ is a floating-point number rounded to six decimal places. No other text should be printed.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the established principles of financial engineering, specifically the Cox-Ross-Rubinstein binomial options pricing model. The problem is well-posed, with all parameters, definitions, and objective functions stated clearly and unambiguously. There are no internal contradictions, factual errors, or sources of ambiguity. We may therefore proceed with the derivation of the solution.\n\nThe objective is to analyze the numerical stability of the binomial option pricing formula for a small number of time steps, $N$. The price of a European call option, $V_N$, is given by the risk-neutral valuation formula:\n$$\nV_N = \\exp(-r T) \\sum_{k=0}^{N} \\binom{N}{k} p^k (1-p)^{N-k} \\max\\!\\left(S_0\\, u^{k} d^{N-k} - K, 0\\right)\n$$\nwhere the parameters are defined as follows:\n- The number of time steps is $N$, and the time increment is $\\Delta t = T/N$.\n- The up and down factors for the underlying asset price are $u = \\exp(\\sigma \\sqrt{\\Delta t})$ and $d = 1/u$.\n- The risk-neutral probability of an up-move is $p = \\dfrac{\\exp\\left((r - q)\\Delta t\\right) - d}{u - d}$. The condition $0 < p < 1$ is an essential no-arbitrage requirement, which is satisfied by the provided test data for the specified range of $N$.\n\nThe computational procedure for each of the $4$ test cases is as follows:\n\nFirst, we must generate the sequence of option prices $\\{V_{N}\\}$ for $N \\in \\{1, 2, \\dots, 12\\}$. For each integer $N$ from $1$ to $12$, we perform these steps:\n1.  Compute the time step $\\Delta t = T/N$.\n2.  Compute the CRR parameters $u$, $d$, and $p$ according to their definitions.\n3.  Evaluate the sum in the formula for $V_N$. This involves iterating from $k=0$ to $N$. In each iteration $k$:\n    a. We compute the probability of reaching the $k$-th terminal state, which is given by the binomial probability mass function $B(k; N, p) = \\binom{N}{k} p^k (1-p)^{N-k}$. The binomial coefficient $\\binom{N}{k}$ is computed using standard library functions for numerical stability.\n    b. We compute the terminal asset price for the $k$-th state: $S_T(k) = S_0 u^k d^{N-k}$.\n    c. We determine the option payoff at this state: $\\max(S_T(k) - K, 0)$.\n    d. The contribution to the total expected payoff is the product of the probability and the payoff.\n4.  The final option price $V_N$ is obtained by discounting the sum of these probability-weighted payoffs from maturity $T$ to the present time, using the factor $\\exp(-rT)$.\nThis process yields the desired sequence $V_1, V_2, \\dots, V_{12}$.\n\nSecond, with the sequence $\\{V_N\\}$ computed, we proceed to calculate the two diagnostic metrics, the oscillation count $O$ and the instability ratio $R$.\n1.  Compute the sequence of first differences, $\\Delta_i = V_{i+1} - V_{i}$ for $i \\in \\{1, 2, \\dots, 11\\}$.\n2.  To compute the oscillation count $O$, we interpret the directive \"count... in the remaining subsequence\" to mean that the sequence of differences $\\Delta$ is first filtered to exclude elements for which $|\\Delta_i| \\le \\varepsilon$ (where the tolerance $\\varepsilon$ is given as $10^{-12}$), and then sign changes between adjacent elements of this new, shorter sequence are counted. That is, $O$ is the number of indices $j$ such that $\\Delta'_j \\Delta'_{j+1} < 0$, where $\\Delta'$ is the filtered sequence. This procedure robustly identifies genuine price oscillations by ignoring numerically insignificant fluctuations.\n3.  To compute the instability ratio $R$, we use the formula:\n$$\nR = \\frac{\\sum_{i=1}^{11} |\\Delta_i|}{\\left|V_{12} - V_{1}\\right| + \\varepsilon}\n$$\nThe numerator represents the total variation (the sum of the magnitudes of all price changes), while the denominator is the magnitude of the net change from the first to the last step, stabilized by $\\varepsilon$. A ratio $R > 1$ indicates that the path from $V_1$ to $V_{12}$ is not monotonic; the total variation exceeds the net variation, which is characteristic of oscillatory convergence.\n\nThis entire procedure is implemented for each of the $4$ specified parameter sets. The final output is an aggregation of the computed $(O, R)$ pairs.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\n\ndef calculate_vn(s0, k_strike, r, q, sigma, t, n_steps):\n    \"\"\"\n    Calculates the European call option price using the N-step CRR binomial model.\n    The implementation is vectorized for efficiency.\n    \"\"\"\n    if n_steps == 0:\n        return max(s0 - k_strike, 0)\n    \n    dt = t / n_steps\n    u = np.exp(sigma * np.sqrt(dt))\n    d = 1.0 / u\n    p = (np.exp((r - q) * dt) - d) / (u - d)\n\n    # Vector of possible numbers of up-moves\n    k_moves = np.arange(0, n_steps + 1)\n    \n    # Vector of terminal stock prices\n    st_values = s0 * (u**k_moves) * (d**(n_steps - k_moves))\n    \n    # Vector of payoffs at terminal nodes\n    payoff_values = np.maximum(st_values - k_strike, 0)\n    \n    # Vector of probabilities for each terminal node\n    probs = comb(n_steps, k_moves, exact=False) * (p**k_moves) * ((1 - p)**(n_steps - k_moves))\n    \n    # Expected payoff is the sum of probability-weighted payoffs\n    expected_payoff = np.sum(probs * payoff_values)\n    \n    # Discount the expected payoff to present value\n    vn = np.exp(-r * t) * expected_payoff\n    return vn\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (S0, K, r, q, sigma, T)\n        (100, 150, 0.05, 0, 0.20, 1),    # Test case 1\n        (100, 100, 0.05, 0, 0.20, 1),    # Test case 2\n        (100, 200, 0.05, 0, 0.20, 1),    # Test case 3\n        (100, 150, 0.05, 0, 0.20, 0.25), # Test case 4\n    ]\n\n    all_results = []\n    epsilon = 1e-12\n    n_values = range(1, 13)\n\n    for case in test_cases:\n        s0, k_strike, r, q, sigma, t = case\n        \n        # 1. Compute the sequence of option prices {V_N} for N=1 to 12.\n        v_sequence = [calculate_vn(s0, k_strike, r, q, sigma, t, n) for n in n_values]\n        \n        # 2. Compute the first differences.\n        deltas = np.diff(v_sequence)\n        \n        # 3. Calculate the Instability Ratio (R).\n        sum_abs_deltas = np.sum(np.abs(deltas))\n        net_change = np.abs(v_sequence[-1] - v_sequence[0])\n        instability_ratio = sum_abs_deltas / (net_change + epsilon)\n        \n        # 4. Calculate the Oscillation Count (O).\n        # Filter out differences that are close to zero.\n        filtered_deltas = [d for d in deltas if np.abs(d) > epsilon]\n        \n        oscillation_count = 0\n        # A sign change can only occur if there are at least two significant differences.\n        if len(filtered_deltas) > 1:\n            for i in range(len(filtered_deltas) - 1):\n                # Check for a strict sign change between adjacent elements in the filtered sequence.\n                if filtered_deltas[i] * filtered_deltas[i+1] < 0:\n                    oscillation_count += 1\n        \n        all_results.append(oscillation_count)\n        all_results.append(f\"{instability_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}]}