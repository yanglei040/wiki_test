{"hands_on_practices": [{"introduction": "In computational finance and economics, simulating economic scenarios or financial instrument behavior is a fundamental task. These simulations often require a source of randomness to model unpredictable market shocks or measurement errors. This exercise provides a concrete look at how to generate a realization of a Gaussian white noise process, a common model for such randomness, starting from the basic output of a standard random number generator. By applying the Box-Muller transform, you will bridge the gap between the theoretical definition of white noise and its practical implementation in a simulation environment [@problem_id:1350034].", "id": "1350034", "problem": "An engineer is developing a simulation for a digital communication system and needs to model channel noise. The noise is modeled as a discrete-time Gaussian white noise process, $\\{W[n]\\}_{n=1,2,...}$, which is a sequence of independent and identically distributed random variables, each following a normal distribution with a mean of zero and a specific variance.\n\nThe engineer uses a standard pseudo-random number generator that produces a sequence of numbers $\\{U_n\\}$ drawn from a uniform distribution on the interval $[0, 1)$. To convert these into standard normal random variables $\\{Z_n\\}$ (with mean $\\mu=0$ and variance $\\sigma^2=1$), the engineer employs the Box-Muller transform. This method takes two independent uniform random variables, $U_1$ and $U_2$, and produces two independent standard normal random variables, $Z_1$ and $Z_2$, using the following equations:\n$$Z_1 = \\sqrt{-2 \\ln(U_1)} \\cos(2\\pi U_2)$$\n$$Z_2 = \\sqrt{-2 \\ln(U_1)} \\sin(2\\pi U_2)$$\nThe Gaussian white noise process $\\{W[n]\\}$ is then generated by appropriately scaling the sequence of standard normal variables $\\{Z[n]\\}$ to achieve a desired variance of $\\sigma_W^2 = 7.5$.\n\nSuppose the first two numbers produced by the uniform random number generator are $U_1 = \\exp(-4.5)$ and $U_2 = 1/3$. Using the first standard normal variable $Z_1$ generated by these two uniform numbers, calculate the corresponding first sample of the white noise process, $W[1]$.\n\nExpress your answer as a single real number rounded to three significant figures.\n\n", "solution": "We use the Box–Muller transform. Given $U_{1}=\\exp(-4.5)$ and $U_{2}=\\frac{1}{3}$, the first standard normal variate is\n$$\nZ_{1}=\\sqrt{-2\\ln(U_{1})}\\,\\cos(2\\pi U_{2}).\n$$\nSince $\\ln(U_{1})=\\ln(\\exp(-4.5))=-4.5$, we have\n$$\n-2\\ln(U_{1})=-2(-4.5)=9,\\quad \\sqrt{-2\\ln(U_{1})}=\\sqrt{9}=3.\n$$\nAlso,\n$$\n\\cos(2\\pi U_{2})=\\cos\\!\\left(2\\pi\\cdot \\frac{1}{3}\\right)=\\cos\\!\\left(\\frac{2\\pi}{3}\\right)=-\\frac{1}{2}.\n$$\nTherefore,\n$$\nZ_{1}=3\\left(-\\frac{1}{2}\\right)=-\\frac{3}{2}.\n$$\nTo obtain the white noise sample with variance $\\sigma_{W}^{2}=7.5$, scale the standard normal by $\\sigma_{W}=\\sqrt{7.5}$:\n$$\nW[1]=\\sqrt{\\sigma_{W}^{2}}\\,Z_{1}=\\sqrt{7.5}\\left(-\\frac{3}{2}\\right)=-\\frac{3}{2}\\sqrt{7.5}.\n$$\nNumerically, $\\sqrt{7.5}\\approx 2.738612787$, hence\n$$\nW[1]\\approx -1.5\\times 2.738612787\\approx -4.107919181,\n$$\nwhich rounded to three significant figures is $-4.11$.", "answer": "$$\\boxed{-4.11}$$"}, {"introduction": "White noise is rarely a final model for economic data, which often exhibits trends and correlations. Instead, its power lies in its role as a fundamental building block for more sophisticated time series models. This practice demonstrates this key principle by constructing a Moving Average (MA) process, where the current value is a combination of a current and a past white noise shock. By calculating the autocovariance of this new process, you will see precisely how a simple, uncorrelated process can give rise to a more complex process with predictable short-term dependence, a cornerstone concept in time series analysis [@problem_id:1350040].", "id": "1350040", "problem": "A digital signal processing engineer is analyzing the output of a newly designed filter. The output signal at discrete time points $t$, denoted as $X_t$, is modeled by a stochastic process. This model describes how the output is influenced by a stream of unpredictable, random electronic fluctuations. The relationship is given by the equation:\n\n$$X_t = W_t + \\theta W_{t-1}$$\n\nHere, the sequence $\\{W_t\\}$ for all integer times $t$ represents a discrete-time white noise process. This means that each $W_t$ is a random variable with the following properties:\n1.  The mean (expected value) is zero for all $t$: $E[W_t] = 0$.\n2.  The variance is a constant value $\\sigma_W^2$ for all $t$: $\\text{Var}(W_t) = \\sigma_W^2$.\n3.  The values at different time points are uncorrelated: $\\text{Cov}(W_t, W_s) = 0$ for any $t \\neq s$.\n\nThe parameter $\\theta$ is a fixed, real-valued constant that characterizes the filter's design. To understand how a random fluctuation at one moment affects the signal at the next, the engineer needs to compute the covariance between the signal's value at time $t$ and its value at the immediately succeeding time step, $t+1$.\n\nDetermine the covariance $\\text{Cov}(X_t, X_{t+1})$. Your final answer should be a symbolic expression in terms of $\\theta$ and $\\sigma_W^2$.\n\n", "solution": "We are given the moving-average process of order one:\n$$X_{t} = W_{t} + \\theta W_{t-1}, \\quad X_{t+1} = W_{t+1} + \\theta W_{t}.$$\nThe covariance at lag one is defined by\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\mathbb{E}[X_{t}X_{t+1}] - \\mathbb{E}[X_{t}]\\,\\mathbb{E}[X_{t+1}].$$\nSince $\\mathbb{E}[W_{t}] = 0$ for all $t$, we have\n$$\\mathbb{E}[X_{t}] = \\mathbb{E}[W_{t} + \\theta W_{t-1}] = 0 + \\theta \\cdot 0 = 0,$$\nand similarly $\\mathbb{E}[X_{t+1}] = 0$. Therefore,\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\mathbb{E}[X_{t}X_{t+1}].$$\nCompute the product:\n$$X_{t}X_{t+1} = (W_{t} + \\theta W_{t-1})(W_{t+1} + \\theta W_{t})\n= W_{t}W_{t+1} + \\theta W_{t}^{2} + \\theta W_{t-1}W_{t+1} + \\theta^{2} W_{t-1}W_{t}.$$\nTaking expectations and using the white-noise properties $\\mathbb{E}[W_{s}W_{t}] = 0$ for $s \\neq t$ and $\\mathbb{E}[W_{t}^{2}] = \\text{Var}(W_{t}) = \\sigma_{W}^{2}$, we obtain\n$$\\mathbb{E}[X_{t}X_{t+1}] = 0 + \\theta\\,\\mathbb{E}[W_{t}^{2}] + 0 + 0 = \\theta \\sigma_{W}^{2}.$$\nThus,\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\theta \\sigma_{W}^{2}.$$", "answer": "$$\\boxed{\\theta \\sigma_{W}^{2}}$$"}, {"introduction": "After developing and fitting a time series model to data—for example, to explain stock returns or forecast sales—how do we know if the model is any good? A crucial diagnostic step is to examine the model's residuals, which are the parts of the data the model couldn't explain. If the model has successfully captured the predictable patterns, the residuals should be unpredictable, resembling white noise. This advanced practice guides you through implementing the formal statistical tests—the Student's $t$-test for the mean and the Ljung-Box test for autocorrelation—that are used daily by quantitative analysts to determine if a residual series is consistent with white noise, thereby validating the fit of their models [@problem_id:2448045].", "id": "2448045", "problem": "You are analyzing whether seasonality-adjusted daily sales residuals from a firm behave like weak white noise in the sense of zero mean and no linear autocorrelation. Use the following fundamental base: by definition, a weak white noise process $\\{e_t\\}$ satisfies $\\mathbb{E}[e_t] = 0$ and $\\operatorname{Cov}(e_t, e_{t-k}) = 0$ for all integers $k \\neq 0$. To decide if a finite observed sequence is consistent with weak white noise at a given significance level, proceed from first principles as follows: test the mean using a two-sided Student's $t$-test for $\\mathbb{E}[e_t]=0$, and test the joint null of zero autocorrelation up to lag $h$ using the Ljung-Box portmanteau statistic.\n\nImplementation details to be followed by your program:\n- Mean-zero test. Let the sample size be $n$, the sample mean be $\\bar{e} = \\frac{1}{n}\\sum_{t=1}^{n} e_t$, and the unbiased sample standard deviation be $s = \\sqrt{\\frac{1}{n-1}\\sum_{t=1}^{n}(e_t - \\bar{e})^2}$. Under the null hypothesis $\\mathbb{E}[e_t] = 0$, the statistic\n$$\nT = \\frac{\\bar{e}}{s/\\sqrt{n}}\n$$\nhas a Student's $t$ distribution with $n-1$ degrees of freedom for independent, identically distributed data with finite second moment. Compute the two-sided $p$-value and compare to the given significance level $\\alpha$.\n- Autocorrelation test. For each lag $k \\in \\{1,\\dots,h^\\ast\\}$ where $h^\\ast = \\min(h, n-1)$, compute the sample autocorrelation\n$$\n\\hat{r}_k = \\frac{\\sum_{t=k+1}^{n}(e_t - \\bar{e})(e_{t-k} - \\bar{e})}{\\sum_{t=1}^{n}(e_t - \\bar{e})^2}.\n$$\nThen compute the Ljung-Box statistic\n$$\nQ = n(n+2)\\sum_{k=1}^{h^\\ast} \\frac{\\hat{r}_k^2}{n-k}.\n$$\nUnder the joint null hypothesis that $\\hat{r}_k = 0$ for all $k=1,\\dots,h^\\ast$, $Q$ is approximately distributed as chi-square with $h^\\ast$ degrees of freedom for large $n$. Compute the corresponding $p$-value and compare to $\\alpha$.\n- Decision rule. Declare a residual series \"weak white noise\" if and only if both tests fail to reject at level $\\alpha$, that is, both $p_{\\text{mean}} \\ge \\alpha$ and $p_{\\text{LB}} \\ge \\alpha$ hold.\n- Degenerate variance handling. If the unbiased sample variance is exactly zero, then all $e_t$ are equal. In that case, declare the series \"not white noise\" unless all $e_t$ are exactly zero, in which case treat it as white noise by convention for this exercise. This rule prevents undefined divisions in the test statistics.\n\nAngle unit requirement: any angles used below are in radians.\n\nTest suite. Apply the above decision rule to the following four seasonality-adjusted residual sequences, each with its specified significance level $\\alpha$ and maximum autocorrelation lag $h$:\n\n- Case 1 (happy path: sparse uncorrelated residuals): $n=60$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = 0$ for all $t$ except $e_{6} = 1.0$ and $e_{51} = -1.0$.\n- Case 2 (autocorrelated leftovers from weekly pattern): $n=56$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = \\sin\\!\\left(\\frac{2\\pi t}{7}\\right)$ for $t=0,1,\\dots,55$, where $\\pi$ is the mathematical constant and the angle is in radians.\n- Case 3 (misadjusted constant bias; boundary case with zero variance): $n=30$, $h=5$, $\\alpha=0.05$. The residuals are constant $e_t = 0.3$ for all $t=1,\\dots,30$.\n- Case 4 (small sample, large $h$ close to $n$; boundary on $h$): $n=12$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = 0$ for all $t$ except $e_{1} = 1.0$ and $e_{12} = -1.0$.\n\nYour program must compute, for each case, a boolean indicating whether the residuals are weak white noise at level $\\alpha$ according to the above rule. The final output format must be exactly one line containing the four boolean results in order as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False]\". No additional text should be printed.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Definition**: A weak white noise process $\\{e_t\\}$ satisfies $\\mathbb{E}[e_t] = 0$ and $\\operatorname{Cov}(e_t, e_{t-k}) = 0$ for all integers $k \\neq 0$.\n- **Test 1 (Mean)**: Two-sided Student's $t$-test for the null hypothesis $\\mathbb{E}[e_t]=0$. The statistic $T = \\frac{\\bar{e}}{s/\\sqrt{n}}$ follows a Student's $t$ distribution with $n-1$ degrees of freedom, where $\\bar{e}$ is the sample mean, $s$ is the unbiased sample standard deviation, and $n$ is the sample size.\n- **Test 2 (Autocorrelation)**: Ljung-Box test for the joint null hypothesis of zero autocorrelations up to lag $h$. The statistic $Q = n(n+2)\\sum_{k=1}^{h^\\ast} \\frac{\\hat{r}_k^2}{n-k}$ is approximately chi-square distributed with $h^\\ast$ degrees of freedom, where $h^\\ast = \\min(h, n-1)$ and $\\hat{r}_k$ is the sample autocorrelation at lag $k$.\n- **Decision Rule**: A series is classified as weak white noise if and only if the $p$-values from both the mean test ($p_{\\text{mean}}$) and the Ljung-Box test ($p_{\\text{LB}}$) are greater than or equal to the significance level $\\alpha$.\n- **Degenerate Case**: If the sample variance is zero, the series is weak white noise only if all its elements are zero; otherwise, it is not.\n- **Test Cases**:\n    - Case 1: $n=60, h=10, \\alpha=0.05$. Residuals $e_t=0$ except $e_{6} = 1.0, e_{51} = -1.0$.\n    - Case 2: $n=56, h=10, \\alpha=0.05$. Residuals $e_t = \\sin(\\frac{2\\pi t}{7})$ for $t=0,1,\\dots,55$.\n    - Case 3: $n=30, h=5, \\alpha=0.05$. Residuals $e_t = 0.3$ for all $t$.\n    - Case 4: $n=12, h=10, \\alpha=0.05$. Residuals $e_t=0$ except $e_{1} = 1.0, e_{12} = -1.0$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is based on established statistical methods for time series analysis, namely the Student's $t$-test and the Ljung-Box test. The definitions and formulas for weak white noise, the test statistics, and their distributions are standard and correct.\n- **Well-Posed**: The problem provides all necessary data and parameters for each test case. The decision rule is unambiguous. A unique boolean result is obtainable for each case. The specification for handling the degenerate case of zero variance is clear and prevents division-by-zero errors.\n- **Objective**: The problem is stated in precise, objective, and quantitative terms. The classification is based on comparing computed $p$-values to a given significance level, which is an objective criterion.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It contains no logical contradictions, unspecified parameters, or non-formalizable instructions. The slight ambiguity in the indexing of the series in Case 2 (i.e., $t=0, \\dots, 55$) is a minor notational issue that is reasonably interpreted as defining a 0-indexed sequence of $56$ values, which is standard in computational practice. Therefore, the problem is deemed **valid**. A solution will be constructed.\n\n**Solution Design**\n\nThe solution will be implemented as a program that evaluates each of the four specified time series against the criteria for weak white noise. The core of the program will be a function that encapsulates the logic for a single time series. This function will proceed as follows.\n\nFirst, for a given series $\\{e_t\\}_{t=1}^n$, we compute fundamental sample statistics: the sample size $n$, the sample mean $\\bar{e} = \\frac{1}{n}\\sum_{t=1}^{n} e_t$, and the sum of squared deviations from the mean, $\\sum_{t=1}^{n}(e_t - \\bar{e})^2$.\n\nA special case is handled first, as stipulated. If the sum of squared deviations is zero (within floating-point tolerance), it implies all $e_t$ are identical and equal to $\\bar{e}$. According to the problem's convention, this degenerate series is considered weak white noise if and only if $\\bar{e}=0$. If the variance is non-zero, the analysis proceeds to the statistical tests.\n\nThe first test addresses the zero-mean condition, $\\mathbb{E}[e_t]=0$. The Student's $t$-statistic is calculated:\n$$\nT = \\frac{\\bar{e}}{s/\\sqrt{n}}\n$$\nwhere $s = \\sqrt{\\frac{1}{n-1}\\sum_{t=1}^{n}(e_t - \\bar{e})^2}$ is the unbiased sample standard deviation. The two-sided $p$-value, $p_{\\text{mean}}$, is then computed from the Student's $t$ distribution with $n-1$ degrees of freedom.\n\nThe second test addresses the no-autocorrelation condition, $\\operatorname{Cov}(e_t, e_{t-k}) = 0$ for $k \\ne 0$. This is done using the Ljung-Box test. We first determine the effective maximum lag, $h^\\ast = \\min(h, n-1)$. Then, for each lag $k$ from $1$ to $h^\\ast$, we compute the sample autocorrelation:\n$$\n\\hat{r}_k = \\frac{\\sum_{t=k+1}^{n}(e_t - \\bar{e})(e_{t-k} - \\bar{e})}{\\sum_{t=1}^{n}(e_t - \\bar{e})^2}\n$$\nThe Ljung-Box statistic $Q$ is then constructed as:\n$$\nQ = n(n+2)\\sum_{k=1}^{h^\\ast} \\frac{\\hat{r}_k^2}{n-k}\n$$\nThe corresponding $p$-value, $p_{\\text{LB}}$, is derived from the chi-square ($\\chi^2$) distribution with $h^\\ast$ degrees of freedom.\n\nFinally, the decision rule is applied. The series is classified as weak white noise if and only if both conditions, $p_{\\text{mean}} \\ge \\alpha$ and $p_{\\text{LB}} \\ge \\alpha$, are met. This boolean result is computed for each of the four test cases provided in the problem statement. The anaylsis for each case proceeds as scripted:\n- **Case 1**: A sparse series with two non-zero entries that sum to zero. The sample mean is exactly zero ($p_{\\text{mean}}=1.0$). The non-zero entries are far apart, so autocorrelations up to lag $h=10$ are zero. Thus, $Q=0$ ($p_{\\text{LB}}=1.0$). Both tests pass.\n- **Case 2**: A perfect sinusoid over an integer number of periods. The sample mean is nearly zero ($p_{\\text{mean}} \\approx 1.0$), but the series is strongly periodic. The autocorrelation at lag $k=7$ will be high, leading to a large $Q$ statistic and a very small $p_{\\text{LB}}$. The autocorrelation test will fail.\n- **Case 3**: A constant non-zero series. This falls into the degenerate variance category. Since the mean is non-zero, it is classified as not white noise by the special rule.\n- **Case 4**: A sparse series similar to Case 1. The sample mean is zero ($p_{\\text{mean}}=1.0$). The non-zero entries are at the extremes of the series ($e_1, e_{12}$). The distance between them is $11$. Since we test up to lag $h=10$, their interaction is not captured, and all relevant sample autocorrelations are zero. $Q=0$ ($p_{\\text{LB}}=1.0$). Both tests pass.\n\nThe implementation will utilize the `numpy` library for numerical operations and `scipy.stats` for obtaining the $p$-values from the $t$ and $\\chi^2$ distributions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, chi2\n\ndef is_weak_white_noise(e: np.ndarray, h: int, alpha: float) -> bool:\n    \"\"\"\n    Tests if a time series behaves like weak white noise.\n\n    Args:\n        e: A numpy array representing the time series of residuals.\n        h: The maximum lag for the Ljung-Box test.\n        alpha: The significance level for the tests.\n\n    Returns:\n        True if the series is classified as weak white noise, False otherwise.\n    \"\"\"\n    n = len(e)\n    if n == 0:\n        return True # Convention for empty series\n\n    e_bar = np.mean(e)\n    sum_sq_dev = np.sum((e - e_bar)**2)\n\n    # Degenerate variance handling rule\n    if np.isclose(sum_sq_dev, 0):\n        # Variance is zero, so all elements are equal to the mean.\n        # It's white noise by convention only if the mean is also zero.\n        return np.isclose(e_bar, 0)\n\n    # 1. Mean-zero test (Student's t-test)\n    # Unbiased sample standard deviation\n    s = np.sqrt(sum_sq_dev / (n - 1))\n    \n    # Check for s=0 shouldn't be needed due to sum_sq_dev check, but for robustness:\n    if np.isclose(s, 0):\n        # This case is already covered by the sum_sq_dev check, but as a safeguard.\n        return np.isclose(e_bar, 0)\n        \n    t_statistic = e_bar / (s / np.sqrt(n))\n    df_t = n - 1\n    p_mean = t.sf(np.abs(t_statistic), df=df_t) * 2\n\n    # 2. Autocorrelation test (Ljung-Box)\n    h_star = min(h, n - 1)\n    \n    lb_sum = 0.0\n    for k in range(1, h_star + 1):\n        # Numerator: sum_{i=k to n-1} (e[i] - e_bar) * (e[i-k] - e_bar)\n        # using 0-indexed array 'e'\n        numerator = np.dot(e[k:] - e_bar, e[:-k] - e_bar)\n        r_k = numerator / sum_sq_dev\n        lb_sum += (r_k**2) / (n - k)\n\n    q_statistic = n * (n + 2) * lb_sum\n    df_q = h_star\n    \n    # Handle df_q=0 case (e.g., n=1, so h_star=0)\n    if df_q == 0:\n        p_lb = 1.0 # No autocorrelations to test\n    else:\n        p_lb = chi2.sf(q_statistic, df=df_q)\n\n    # 3. Decision rule\n    return p_mean >= alpha and p_lb >= alpha\n\ndef solve():\n    \"\"\"\n    Solves the problem by applying the white noise test to four specified cases.\n    \"\"\"\n    \n    # Case 1 (happy path: sparse uncorrelated residuals)\n    n1, h1, alpha1 = 60, 10, 0.05\n    e1 = np.zeros(n1)\n    e1[5] = 1.0   # e_6 in 1-based indexing\n    e1[50] = -1.0 # e_51 in 1-based indexing\n    \n    # Case 2 (autocorrelated leftovers from weekly pattern)\n    n2, h2, alpha2 = 56, 10, 0.05\n    t2 = np.arange(n2)\n    e2 = np.sin(2 * np.pi * t2 / 7)\n    \n    # Case 3 (misadjusted constant bias; boundary case with zero variance)\n    n3, h3, alpha3 = 30, 5, 0.05\n    e3 = np.full(n3, 0.3)\n    \n    # Case 4 (small sample, large h close to n; boundary on h)\n    n4, h4, alpha4 = 12, 10, 0.05\n    e4 = np.zeros(n4)\n    e4[0] = 1.0   # e_1 in 1-based indexing\n    e4[11] = -1.0 # e_12 in 1-based indexing\n\n    test_cases = [\n        (e1, h1, alpha1),\n        (e2, h2, alpha2),\n        (e3, h3, alpha3),\n        (e4, h4, alpha4),\n    ]\n\n    results = []\n    for e, h, alpha in test_cases:\n        result = is_weak_white_noise(e, h, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}