{"hands_on_practices": [{"introduction": "This exercise demonstrates one of the most critical issues in applied econometrics: omitted variable bias. By simulating a data generating process where an unobserved factor like \"ability\" is correlated with an observed regressor like education, you will gain a concrete understanding of how this violates the zero conditional mean assumption, $E(u|X)=0$. This practice will allow you to computationally verify how a confounding variable can lead to biased and inconsistent OLS estimators, making it essential for any analysis aiming for causal inference [@problem_id:2417165].", "id": "2417165", "problem": "Consider a data generating process for individual wages in which an econometrician regresses wages on years of education, but an unobserved ability affects both education and wages. Let the true structural model be\n$$\nw_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i,\n$$\nwhere $w_i$ is the wage of individual $i$, $\\text{edu}_i$ is the years of education of individual $i$, $a_i$ is an unobserved ability, and $u_i$ is an idiosyncratic error. Suppose education is determined by\n$$\n\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i.\n$$\nAssume $a_i \\sim \\mathcal{N}(0,\\sigma_a^2)$, $v_i \\sim \\mathcal{N}(0,\\sigma_v^2)$, and $u_i \\sim \\mathcal{N}(0,\\sigma_u^2)$, with $a_i$, $v_i$, and $u_i$ mutually independent. The econometrician estimates the regression of $w_i$ on $\\text{edu}_i$ with an intercept, omitting $a_i$.\n\nYour task is to write a complete, runnable program that, for a given test suite of parameter values, simulates data according to the process above, estimates the coefficient on $\\text{edu}_i$ from the regression of $w_i$ on $\\text{edu}_i$ with an intercept for each test case, and reports for each case: the estimated coefficient $\\hat{\\beta}_1$, the bias $\\hat{\\beta}_1 - \\beta_1$, and whether the estimate strictly exceeds the true $\\beta_1$.\n\nRandomness and reproducibility: For test case index $i \\in \\{1,2,3,4,5\\}$, use the pseudo-random seed $s_i = s_0 + i$ with base $s_0 = 12345$. Use the specified Normal distributions exactly as stated. All quantities are dimensionless and require no physical units.\n\nTest suite of parameter values to be used:\n- Case $1$ (positive correlation between ability and education; overestimation expected): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $2$ (no correlation between ability and education; no omitted variable bias in probability limit): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.0$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $3$ (negative correlation between ability and education; underestimation expected): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = -0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $4$ (zero true return to education; spurious positive estimate expected): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.0$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $5$ (boundary where ability has zero variance; omitted variable bias vanishes): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 0.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order $1$ through $5$, append, in sequence, the estimated coefficient $\\hat{\\beta}_1$ (a float), the bias $\\hat{\\beta}_1 - \\beta_1$ (a float), and an indicator (a boolean) of whether $\\hat{\\beta}_1$ strictly exceeds $\\beta_1$. Thus the output will be a flat list of length $15$ in the order $[\\hat{\\beta}_1^{(1)}, \\text{bias}^{(1)}, \\text{over}^{(1)}, \\ldots, \\hat{\\beta}_1^{(5)}, \\text{bias}^{(5)}, \\text{over}^{(5)}]$.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n\nThe structural model for wage is given by:\n$w_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i$\n\nThe model for education is:\n$\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i$\n\nThe distributional assumptions are:\n- $a_i \\sim \\mathcal{N}(0, \\sigma_a^2)$\n- $v_i \\sim \\mathcal{N}(0, \\sigma_v^2)$\n- $u_i \\sim \\mathcal{N}(0, \\sigma_u^2)$\n- $a_i$, $v_i$, and $u_i$ are mutually independent.\n\nThe econometrician estimates an incomplete model by regressing $w_i$ on $\\text{edu}_i$ with an intercept, omitting $a_i$.\n\nRandomness specification:\nFor test case index $i \\in \\{1, 2, 3, 4, 5\\}$, the pseudo-random seed is $s_i = s_0 + i$, with the base seed $s_0 = 12345$.\n\nTest Suite Parameters:\n- Case $1$: $N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.8$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$.\n- Case $2$: $N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.0$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$.\n- Case $3$: $N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=-0.8$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$.\n- Case $4$: $N=100000$, $\\beta_0=0$, $\\beta_1=0.0$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.8$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$.\n- Case $5$: $N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.8$, $\\sigma_a=0.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$.\n\nRequired outputs for each case: estimated coefficient $\\hat{\\beta}_1$, bias $\\hat{\\beta}_1 - \\beta_1$, and a boolean indicator for $\\hat{\\beta}_1 > \\beta_1$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem describes a canonical example of omitted variable bias in linear regression, a fundamental concept in econometrics and statistics. The models and assumptions are standard. The problem is scientifically sound.\n- **Well-Posed**: The problem provides a complete data generating process, a specific estimation task, and fully specified parameters for all test cases. A unique solution exists for a given random seed. The problem is well-posed.\n- **Objective**: All elements of the problem are defined with mathematical precision. There are no subjective or ambiguous statements. The problem is objective.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A solution will be constructed.\n\n**Solution Design**\n\nThe core of this problem is the bias that arises in an Ordinary Least Squares (OLS) estimator when a relevant variable is omitted from the regression model, and this omitted variable is correlated with an included regressor.\n\nLet the true model be\n$$w_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i$$\nThe misspecified model estimated by the econometrician is\n$$w_i = \\gamma_0 + \\gamma_1 \\cdot \\text{edu}_i + \\epsilon_i$$\nThe term $\\epsilon_i$ in the estimated model is a composite error term that includes the omitted variable and the original idiosyncratic error: $\\epsilon_i = \\beta_a \\cdot a_i + u_i$.\n\nThe OLS estimator for $\\gamma_1$, which we denote $\\hat{\\beta}_1$, is given by\n$$\\hat{\\beta}_1 = \\frac{\\text{Cov}(\\text{edu}, w)}{\\text{Var}(\\text{edu})}$$\nTo understand the properties of this estimator, we examine its probability limit:\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\text{Cov}(\\text{edu}_i, w_i)}{\\text{Var}(\\text{edu}_i)}$$\nSubstituting the true structural equation for $w_i$:\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\text{Cov}(\\text{edu}_i, \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i)}{\\text{Var}(\\text{edu}_i)}$$\nBy the linearity of covariance and the fact that $\\text{Cov}(\\text{edu}_i, \\beta_0) = 0$:\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\beta_1 \\cdot \\text{Cov}(\\text{edu}_i, \\text{edu}_i) + \\beta_a \\cdot \\text{Cov}(\\text{edu}_i, a_i) + \\text{Cov}(\\text{edu}_i, u_i)}{\\text{Var}(\\text{edu}_i)}$$\nGiven the structural model for education, $\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i$, and the mutual independence of $a_i, v_i, u_i$, we have $\\text{Cov}(\\text{edu}_i, u_i) = \\text{Cov}(e_0 + \\phi \\cdot a_i + v_i, u_i) = 0$.\nThe expression simplifies to:\n$$\\text{plim}(\\hat{\\beta}_1) = \\beta_1 + \\beta_a \\frac{\\text{Cov}(\\text{edu}_i, a_i)}{\\text{Var}(\\text{edu}_i)}$$\nThe term $\\beta_a \\frac{\\text{Cov}(\\text{edu}_i, a_i)}{\\text{Var}(\\text{edu}_i)}$ represents the asymptotic omitted variable bias. We can derive its components from the given assumptions:\n$$\\text{Cov}(\\text{edu}_i, a_i) = \\text{Cov}(e_0 + \\phi \\cdot a_i + v_i, a_i) = \\phi \\cdot \\text{Var}(a_i) = \\phi \\sigma_a^2$$\n$$\\text{Var}(\\text{edu}_i) = \\text{Var}(e_0 + \\phi \\cdot a_i + v_i) = \\phi^2 \\cdot \\text{Var}(a_i) + \\text{Var}(v_i) = \\phi^2 \\sigma_a^2 + \\sigma_v^2$$\nThus, the asymptotic bias is:\n$$\\text{Bias} = \\beta_a \\frac{\\phi \\sigma_a^2}{\\phi^2 \\sigma_a^2 + \\sigma_v^2}$$\nThe estimator $\\hat{\\beta}_1$ is consistent (i.e., the bias is zero) if and only if $\\beta_a = 0$ or $\\phi = 0$ or $\\sigma_a^2 = 0$. This means the omitted variable either does not affect the dependent variable ($w_i$), or it is uncorrelated with the included regressor ($\\text{edu}_i$), or it has no variance at all.\n\nThe simulation will proceed as follows for each test case:\n$1$. Set the pseudo-random seed to $s_0 + i$ to ensure reproducibility.\n$2$. Generate $N$ draws for the random components $a_i$, $v_i$, and $u_i$ from their specified normal distributions.\n$3$. Construct the observed data vectors for $\\text{edu}$ and $w$ according to their structural equations using the generated random components and the case-specific parameters.\n$4$. Estimate the coefficients of the misspecified model by regressing $w$ on $\\text{edu}$ and an intercept. This is achieved using the standard OLS matrix formula $\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}$, where $\\mathbf{y}$ is the vector of $w_i$ values and $\\mathbf{X}$ is the $N \\times 2$ design matrix with a column of ones and a column of $\\text{edu}_i$ values. The estimated coefficient $\\hat{\\beta}_1$ is the second element of the vector $\\hat{\\mathbf{\\beta}}$.\n$5$. Calculate the bias as $\\hat{\\beta}_1 - \\beta_1$ and determine if $\\hat{\\beta}_1 > \\beta_1$.\n$6$. Store the three resulting values ($\\hat{\\beta}_1$, bias, over-estimation indicator) for final reporting.\n\nThis procedure will be repeated for all five test cases, demonstrating the theoretical effects of omitted variable bias under different conditions. The large sample size $N=100000$ ensures that the simulated finite-sample estimates will be very close to their theoretical probability limits.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates data to demonstrate omitted variable bias in OLS regression,\n    estimates coefficients for several parameter sets, and reports the results.\n    \"\"\"\n    # Base seed for pseudo-random number generation.\n    base_seed = 12345\n\n    # Test suite of parameter values.\n    # Each tuple contains:\n    # (N, beta_0, beta_1, beta_a, e_0, phi, sigma_a, sigma_v, sigma_u)\n    test_cases = [\n        (100000, 0, 0.08, 0.5, 12, 0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, 0.0, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, -0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.0, 0.5, 12, 0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, 0.8, 0.0, 1.0, 1.0),\n    ]\n\n    results = []\n\n    for i, case in enumerate(test_cases, 1):\n        # Unpack parameters for the current case.\n        N, beta_0, beta_1, beta_a, e_0, phi, sigma_a, sigma_v, sigma_u = case\n        \n        # Set the seed for reproducibility for the current test case.\n        seed = base_seed + i\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate the underlying random variables from their distributions.\n        #    a_i ~ N(0, sigma_a^2)\n        #    v_i ~ N(0, sigma_v^2)\n        #    u_i ~ N(0, sigma_u^2)\n        a = rng.normal(loc=0.0, scale=sigma_a, size=N)\n        v = rng.normal(loc=0.0, scale=sigma_v, size=N)\n        u = rng.normal(loc=0.0, scale=sigma_u, size=N)\n\n        # 2. Construct the variables 'edu' and 'wage' based on the structural model.\n        #    edu_i = e_0 + phi * a_i + v_i\n        edu = e_0 + phi * a + v\n        \n        #    w_i = beta_0 + beta_1 * edu_i + beta_a * a_i + u_i\n        wage = beta_0 + beta_1 * edu + beta_a * a + u\n\n        # 3. Perform OLS regression of wage on edu and an intercept.\n        #    We estimate the model: w_i = gamma_0 + gamma_1 * edu_i + error\n        #    The OLS solution is beta_hat = (X'X)^-1 * X'y\n        \n        # Construct the design matrix X with a column of ones for the intercept.\n        X = np.vstack([np.ones(N), edu]).T\n        \n        # Calculate OLS coefficients.\n        try:\n            # Using the standard matrix formula for OLS coefficients.\n            # beta_hat = [gamma_0_hat, gamma_1_hat]\n            XtX = X.T @ X\n            XtY = X.T @ wage\n            beta_hat_vector = np.linalg.inv(XtX) @ XtY\n        except np.linalg.LinAlgError:\n            # In case of singular matrix, use pseudo-inverse\n            beta_hat_vector = np.linalg.pinv(X) @ wage\n\n        estimated_beta_1 = beta_hat_vector[1]\n\n        # 4. Calculate the bias and the overestimation indicator.\n        bias = estimated_beta_1 - beta_1\n        is_over = estimated_beta_1 > beta_1\n\n        # 5. Append results for this case to the master list.\n        results.extend([estimated_beta_1, bias, is_over])\n\n    # Final print statement in the exact required format.\n    # The list is flattened and elements are converted to strings.\n    formatted_results = [f\"{x:.8f}\" if isinstance(x, float) else str(x) for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "This practice explores a classic textbook violation of the OLS assumptions known as the \"dummy variable trap.\" You will programmatically construct a design matrix that includes both an intercept and a full set of categorical dummy variables [@problem_id:2417199]. This provides a hands-on demonstration of why this configuration creates perfect multicollinearity, making the Gram matrix $X^{\\intercal}X$ singular and thus preventing the calculation of the OLS estimator.", "id": "2417199", "problem": "Consider a linear regression model estimated by Ordinary Least Squares (OLS), where the design matrix is constructed from a constant (intercept) and dummy variables for a single categorical regressor. Let there be $n$ observations indexed by $i \\in \\{1,\\dots,n\\}$, and let $\\ell_i$ denote the categorical label for observation $i$. Let the set of distinct labels in $(\\ell_1,\\dots,\\ell_n)$ be $\\{c_1,\\dots,c_K\\}$, ordered so that $c_1 < \\cdots < c_K$ and hence $K \\ge 1$. For each $j \\in \\{1,\\dots,K\\}$, define the dummy column $D_j \\in \\mathbb{R}^n$ by $(D_j)_i = 1$ if $\\ell_i = c_j$ and $(D_j)_i = 0$ otherwise. Let $1_n \\in \\mathbb{R}^n$ denote the vector of ones. The design matrix $X \\in \\mathbb{R}^{n \\times p}$ is to be constructed according to two binary flags:\n- include\\_intercept $\\in \\{0,1\\}$, indicating whether the constant column $1_n$ is included,\n- drop\\_baseline $\\in \\{0,1\\}$, indicating whether the dummy column corresponding to $c_1$ is dropped from $X$; this flag is relevant only when include\\_intercept $= 1$.\n\nFormally, construct $X$ as follows:\n- If include\\_intercept $= 1$ and drop\\_baseline $= 0$, then $X = [\\,1_n \\;\\; D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$ so that $p = K+1$.\n- If include\\_intercept $= 1$ and drop\\_baseline $= 1$, then $X = [\\,1_n \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$ so that $p = K$.\n- If include\\_intercept $= 0$ (regardless of drop\\_baseline), then $X = [\\,D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$ so that $p = K$.\n\nFor each test case defined below, build $X$ from the provided label sequence and flags, then determine whether the Gram matrix $X^{\\top} X \\in \\mathbb{R}^{p \\times p}$ is singular, where a square matrix $M$ is singular if and only if it is not invertible (equivalently, $\\det(M) = 0$).\n\nYou must write a program that, for each test case, outputs an integer equal to $1$ if $X^{\\top} X$ is singular and equal to $0$ otherwise. Use exact logic over the constructed $X$; do not read any external input.\n\nTest suite (each case is a triple $(\\ell, \\text{include\\_intercept}, \\text{drop\\_baseline})$):\n- Case $1$: $\\ell = (1,2,3,1,2,3)$, include\\_intercept $= 1$, drop\\_baseline $= 0$.\n- Case $2$: $\\ell = (1,2,3,1,2,3)$, include\\_intercept $= 1$, drop\\_baseline $= 1$.\n- Case $3$: $\\ell = (1,2,3,1,2,3)$, include\\_intercept $= 0$, drop\\_baseline $= 0$.\n- Case $4$: $\\ell = (5,5,5,5)$, include\\_intercept $= 1$, drop\\_baseline $= 0$.\n- Case $5$: $\\ell = (5,5,5,5)$, include\\_intercept $= 1$, drop\\_baseline $= 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). Each result must be the integer $1$ or $0$ as specified above.", "solution": "The problem requires an analysis of the singularity of the Gram matrix $X^{\\top} X$, where $X$ is a design matrix constructed from dummy variables and an optional intercept. A square matrix is singular if and only if its columns are linearly dependent. The Gram matrix $X^{\\top} X$ is singular if and only if the columns of the matrix $X$ are linearly dependent. Our task, therefore, reduces to investigating the conditions under which the columns of $X$ exhibit linear dependence.\n\nLet $n$ be the number of observations and let the set of unique categorical labels be $\\{c_1, \\dots, c_K\\}$, where $K \\ge 1$. For each $j \\in \\{1, \\dots, K\\}$, the column vector $D_j \\in \\mathbb{R}^n$ is a dummy variable where the $i$-th element, $(D_j)_i$, is $1$ if observation $i$ has label $c_j$, and $0$ otherwise. A fundamental property of this construction is that for any observation, exactly one dummy variable is active. This implies a linear relationship among the full set of dummy variables and the intercept vector $1_n \\in \\mathbb{R}^n$, which is a column of ones:\n$$ \\sum_{j=1}^{K} D_j = 1_n $$\nThis equation is the key to determining the singularity of $X^{\\top} X$. We analyze the three cases for the construction of $X$ as specified.\n\nCase $1$: `include_intercept` $= 1$, `drop_baseline` $= 0$.\nThe design matrix is $X = [\\,1_n \\;\\; D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$. The columns of $X$ are the intercept vector $1_n$ and all $K$ dummy vectors $D_1, \\dots, D_K$. The fundamental relationship can be rewritten as a linear combination of the columns of $X$:\n$$ (1) \\cdot 1_n - (1) \\cdot D_1 - (1) \\cdot D_2 - \\cdots - (1) \\cdot D_K = \\mathbf{0}_n $$\nThis is a non-trivial linear combination (with coefficient vector $[1, -1, \\dots, -1]^{\\top}$) of the columns of $X$ that equals the zero vector $\\mathbf{0}_n$. Thus, the columns of $X$ are linearly dependent. This multicollinearity is often termed the \"dummy variable trap\". Consequently, the Gram matrix $X^{\\top} X$ is singular.\n\nCase $2$: `include_intercept` $= 1$, `drop_baseline` $= 1$.\nThe design matrix is $X = [\\,1_n \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$. The dummy variable for the baseline category, $D_1$, is omitted. If $K=1$, the set of included dummy vectors is empty, so $X = [\\,1_n\\,]$. A single non-zero column vector constitutes a linearly independent set. Its Gram matrix $X^{\\top}X = [n]$, which is non-singular for $n \\ge 1$.\nIf $K > 1$, we test for linear independence of the columns $\\{1_n, D_2, \\dots, D_K\\}$ by setting a linear combination to zero:\n$$ \\alpha_0 \\cdot 1_n + \\sum_{j=2}^{K} \\alpha_j D_j = \\mathbf{0}_n $$\nBy the problem's definition, for each category $c_j$, there exists at least one observation $i$ with that label. For an observation in the baseline category $c_1$, the corresponding row of the equation becomes $\\alpha_0 \\cdot 1 + \\sum_{j=2}^{K} \\alpha_j \\cdot 0 = 0$, which implies $\\alpha_0 = 0$. For an observation in any other category $c_j$ (with $j \\ge 2$), the equation becomes $\\alpha_0 \\cdot 1 + \\alpha_j \\cdot 1 = 0$. Since we have found $\\alpha_0 = 0$, it follows that $\\alpha_j = 0$ for all $j \\in \\{2, \\dots, K\\}$. The only solution is the trivial one where all coefficients are zero. Therefore, the columns of $X$ are linearly independent, and $X^{\\top} X$ is non-singular.\n\nCase $3$: `include_intercept` $= 0$.\nThe design matrix is $X = [\\,D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$. We test for linear independence of the columns $\\{D_1, \\dots, D_K\\}$:\n$$ \\sum_{j=1}^{K} \\alpha_j D_j = \\mathbf{0}_n $$\nFor an observation in category $c_j$, the corresponding row of this vector equation is $\\alpha_j \\cdot 1 = 0$, which implies $\\alpha_j = 0$. As this holds for each category $j \\in \\{1, \\dots, K\\}$, all coefficients must be zero. The columns of $X$ are linearly independent, and $X^{\\top} X$ is non-singular.\n\nSummary of results based on this analysis:\n- Case $1$: $\\ell = (1,2,3,1,2,3)$, `include_intercept` $= 1$, `drop_baseline` $= 0$. This configuration matches Case $1$ analysis. Result: singular ($1$).\n- Case $2$: $\\ell = (1,2,3,1,2,3)$, `include_intercept` $= 1$, `drop_baseline` $= 1$. This matches Case $2$ analysis. Result: non-singular ($0$).\n- Case $3$: $\\ell = (1,2,3,1,2,3)$, `include_intercept` $= 0$, `drop_baseline` $= 0$. This matches Case $3$ analysis. Result: non-singular ($0$).\n- Case $4$: $\\ell = (5,5,5,5)$, `include_intercept` $= 1$, `drop_baseline` $= 0$. Here $K=1$. The matrix is $X = [\\,1_n \\;\\; D_1\\,]$. Since every observation is in the same category, $D_1=1_n$, so $X=[\\,1_n \\;\\; 1_n\\,]$. The columns are identical, hence linearly dependent. Result: singular ($1$).\n- Case $5$: $\\ell = (5,5,5,5)$, `include_intercept` $= 1$, `drop_baseline` $= 1$. Here $K=1$. This is an instance of Case $2$ where $X=[\\,1_n\\,]$. It has one column and rank $1$. Result: non-singular ($0$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite. For each case, it constructs\n    the design matrix X and determines if the Gram matrix X'X is singular\n    by checking if the rank of X is less than its number of columns.\n    \"\"\"\n    test_cases = [\n        # Case 1: (labels, include_intercept, drop_baseline)\n        ((1, 2, 3, 1, 2, 3), 1, 0),\n        # Case 2\n        ((1, 2, 3, 1, 2, 3), 1, 1),\n        # Case 3\n        ((1, 2, 3, 1, 2, 3), 0, 0),\n        # Case 4\n        ((5, 5, 5, 5), 1, 0),\n        # Case 5\n        ((5, 5, 5, 5), 1, 1),\n    ]\n\n    results = []\n    for l, include_intercept, drop_baseline in test_cases:\n        n = len(l)\n        # The problem statement implies n >= 1 and K >= 1, so no need for n=0 check.\n\n        # Identify unique categories and map them to indices 0, 1, ..., K-1\n        categories = sorted(list(set(l)))\n        K = len(categories)\n        cat_map = {cat: i for i, cat in enumerate(categories)}\n\n        # Construct the full set of K dummy variables efficiently\n        all_dummies = np.zeros((n, K))\n        rows = np.arange(n)\n        cols = [cat_map[label] for label in l]\n        all_dummies[rows, cols] = 1\n\n        # Build the list of columns for the design matrix X\n        X_cols = []\n        \n        if include_intercept == 1:\n            # Add intercept column\n            X_cols.append(np.ones((n, 1)))\n            \n            if drop_baseline == 0:\n                # Add all K dummy variables\n                if K > 0:\n                    X_cols.append(all_dummies)\n            else:  # drop_baseline == 1\n                # Add dummies D_2, ..., D_K\n                if K > 1:\n                    X_cols.append(all_dummies[:, 1:])\n        else:  # include_intercept == 0\n            # Add all K dummy variables\n            if K > 0:\n                X_cols.append(all_dummies)\n\n        # If no columns were selected, the matrix has 0 columns.\n        if not X_cols:\n            p = 0\n            rank = 0\n        else:\n            # Horizontally stack columns to form X\n            X = np.hstack(X_cols)\n            p = X.shape[1]\n            # Calculate the rank of the design matrix X\n            rank = np.linalg.matrix_rank(X)\n\n        # X'X is singular if and only if the columns of X are linearly dependent,\n        # which is true if and only if rank(X) < number of columns of X.\n        is_singular = 1 if rank < p else 0\n        results.append(is_singular)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Violating the homoscedasticity assumption does not bias OLS point estimates, but it invalidates our standard errors, leading to unreliable statistical inference. This exercise guides you through simulating data with heteroskedasticity, where the error variance is not constant [@problem_id:2417150]. By implementing and comparing classical OLS, White's heteroskedasticity-consistent, and bootstrap standard errors, you will learn to diagnose this common issue and apply modern robust methods to ensure your hypothesis tests are valid.", "id": "2417150", "problem": "Consider the simple linear regression model with a single regressor and an intercept in a computational economics and finance setting. The response variable $y_i$ is generated from the data-generating process\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i,\n$$\nwhere $x_i \\sim \\mathcal{N}(0,1)$ independently and identically distributed across $i$, and the disturbance $u_i$ exhibits heteroskedasticity according to\n$$\nu_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,1)\\ \\text{independently of}\\ x_i.\n$$\nAssume the coefficient of interest is the slope $\\beta_1$. For each test case defined below, generate a sample $\\{(y_i,x_i)\\}_{i=1}^n$ using the specified parameters, and compute three estimates of the standard error of the ordinary least squares (OLS) estimator of $\\beta_1$:\n- the classical homoskedasticity-based OLS standard error,\n- the heteroskedasticity-consistent (HC$0$) standard error (also known as the White-robust standard error),\n- a bootstrap standard error based on nonparametric resampling of observation pairs $(y_i,x_i)$ with replacement using $B$ bootstrap replications.\n\nUse the same fixed random number generator seed $s$ for all random draws within each test case to ensure reproducibility. For all computations, include an intercept in the regression. Express each reported standard error as a real number rounded to six decimal places.\n\nTest suite. Use the following parameter sets; each bullet describes one test case:\n- Case $1$: $n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=17$.\n- Case $2$: $n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=0.0$, $B=400$, $s=23$.\n- Case $3$: $n=50$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=123$.\n- Case $4$: $n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=10.0$, $B=400$, $s=2023$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. Each inner list corresponds to one test case, in the same order as above, and contains three real numbers in the order [OLS standard error, White-robust standard error, bootstrap standard error], each rounded to six decimal places. For example, the output must have the form\n$[[a_{1,1},a_{1,2},a_{1,3}],[a_{2,1},a_{2,2},a_{2,3}],[a_{3,1},a_{3,2},a_{3,3}],[a_{4,1},a_{4,2},a_{4,3}]]$\nwith no spaces.", "solution": "The problem presented is a well-defined computational exercise in econometrics. It is scientifically grounded, logically consistent, and provides all necessary information to proceed with a solution. The task is to compute three different estimators for the standard error of an Ordinary Least Squares (OLS) regression coefficient under a specified data-generating process that includes heteroskedasticity.\n\nThe model is a simple linear regression:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n$$\nfor $i = 1, \\dots, n$. In matrix notation, this is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of observations of the response variable, $\\mathbf{X}$ is the $n \\times 2$ design matrix with the first column being a vector of ones and the second column being the observations of the regressor $x_i$, $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ is the $2 \\times 1$ vector of coefficients, and $\\mathbf{u}$ is the $n \\times 1$ vector of disturbances.\n\nThe OLS estimator for $\\boldsymbol{\\beta}$ is given by:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nThe variance-covariance matrix of this estimator, conditional on $\\mathbf{X}$, is:\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Omega} \\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\nwhere $\\boldsymbol{\\Omega} = \\text{E}[\\mathbf{u}\\mathbf{u}^T | \\mathbf{X}]$. In this problem, the disturbances $u_i$ are independent across observations, so $\\boldsymbol{\\Omega}$ is a diagonal matrix. The heteroskedastic structure $u_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ implies that the diagonal elements of $\\boldsymbol{\\Omega}$ are $\\text{Var}(u_i|x_i) = \\sigma_i^2 = \\sigma_0^2(1 + \\gamma x_i^2)$.\n\nThe task is to estimate the standard error of $\\hat{\\beta}_1$, the OLS estimator for the slope coefficient, which is the square root of the second diagonal element of $\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X})$. We will compute three different estimates for this quantity.\n\n1.  **Classical Homoskedasticity-Based OLS Standard Error**\n\nThis estimator incorrectly assumes that the disturbances are homoskedastic, meaning $\\text{Var}(u_i|x_i) = \\sigma^2$ for all $i$. Under this assumption, $\\boldsymbol{\\Omega} = \\sigma^2\\mathbf{I}_n$, and the variance-covariance matrix simplifies to:\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\nThe unknown error variance $\\sigma^2$ is estimated using the OLS residuals, $\\hat{u}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$. The unbiased estimator for $\\sigma^2$ is:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-k} \\sum_{i=1}^{n} \\hat{u}_i^2\n$$\nwhere $k$ is the number of regressors, which is $2$ in this case (intercept and slope). The estimated classical variance-covariance matrix is $\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}$. The standard error for $\\hat{\\beta}_1$ is the square root of the second diagonal element of this matrix:\n$$\n\\text{SE}_{OLS}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\n\n2.  **Heteroskedasticity-Consistent (HC0) Standard Error**\n\nThis estimator, also known as the White or robust standard error, does not assume homoskedasticity. It provides a consistent estimate of the variance-covariance matrix even when $\\boldsymbol{\\Omega}$ is not a multiple of the identity matrix. It is based on the \"sandwich\" formula for the variance. The middle part of the sandwich, $\\mathbf{S} = \\mathbf{X}^T \\boldsymbol{\\Omega} \\mathbf{X}$, is estimated by replacing the unknown variances $\\sigma_i^2$ with the squared OLS residuals $\\hat{u}_i^2$. This gives the HC$0$ estimator for $\\mathbf{S}$:\n$$\n\\hat{\\mathbf{S}}_0 = \\sum_{i=1}^{n} \\hat{u}_i^2 \\mathbf{x}_i \\mathbf{x}_i^T\n$$\nwhere $\\mathbf{x}_i = [1, x_i]^T$ is the $i$-th row of the design matrix $\\mathbf{X}$. The HC$0$ variance-covariance estimator is then:\n$$\n\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\hat{\\mathbf{S}}_0 (\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\nThe corresponding standard error for $\\hat{\\beta}_1$ is:\n$$\n\\text{SE}_{HC0}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\nWhen $\\gamma > 0$, the errors are truly heteroskedastic, and this estimator is expected to be more accurate than the classical one, especially in large samples. When $\\gamma=0.0$, the errors are homoskedastic, and both estimators should be similar.\n\n3.  **Bootstrap Standard Error**\n\nThe bootstrap provides a nonparametric method for estimating the standard error. We use the \"pairs bootstrap,\" which is appropriate in the presence of heteroskedasticity because it resamples the pairs $(x_i, y_i)$ together, thus preserving the unknown relationship between the regressor and the variance of the error term. The algorithm is as follows:\n    1.  From the original sample $\\{(y_i, x_i)\\}_{i=1}^n$, draw a \"bootstrap sample\" of size $n$ by sampling pairs with replacement.\n    2.  Using this bootstrap sample, compute the OLS estimate of the slope coefficient, denoted $\\hat{\\beta}_{1,b}^*$.\n    3.  Repeat steps $1$ and $2$ for a large number of replications, $B$. This yields a distribution of bootstrap estimates $\\{\\hat{\\beta}_{1,1}^*, \\dots, \\hat{\\beta}_{1,B}^*\\}$.\n    4.  The bootstrap standard error is the sample standard deviation of these $B$ estimates:\n        $$\n        \\text{SE}_{Boot}(\\hat{\\beta}_1) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\beta}_{1,b}^* - \\bar{\\beta}_1^*)^2}\n        $$\n        where $\\bar{\\beta}_1^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\beta}_{1,b}^*$. This procedure is computationally intensive but provides a reliable estimate of the sampling variability of $\\hat{\\beta}_1$.\n\nThe implementation will proceed by first generating the data for each test case according to the specified parameters ($n, \\beta_0, \\beta_1, \\sigma_0, \\gamma$) and the random seed $s$. Then, for each generated dataset, the OLS estimates will be computed, followed by the calculation of the three specified standard errors. The results will be rounded and formatted as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It generates data, computes OLS and three types of standard errors,\n    and prints the results in the required format.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=17\n        (500, 0.7, 1.5, 1.0, 2.0, 400, 17),\n        # Case 2: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=0.0, B=400, s=23\n        (500, 0.7, 1.5, 1.0, 0.0, 400, 23),\n        # Case 3: n=50, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=123\n        (50, 0.7, 1.5, 1.0, 2.0, 400, 123),\n        # Case 4: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=10.0, B=400, s=2023\n        (500, 0.7, 1.5, 1.0, 10.0, 400, 2023),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n, beta0, beta1, sigma0, gamma, B, s = case\n        \n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(s)\n\n        # 1. Data Generation\n        x = rng.normal(0, 1, size=n)\n        epsilon = rng.normal(0, 1, size=n)\n        u = sigma0 * np.sqrt(1 + gamma * x**2) * epsilon\n        y = beta0 + beta1 * x + u\n\n        # 2. OLS Estimation\n        X = np.vstack((np.ones(n), x)).T  # Design matrix [1, x_i]\n        \n        try:\n            # Pre-compute (X'X)^-1\n            inv_XTX = np.linalg.inv(X.T @ X)\n            beta_hat = inv_XTX @ X.T @ y\n        except np.linalg.LinAlgError:\n            # Handle rare case of singular matrix\n            all_results.append([np.nan, np.nan, np.nan])\n            continue\n            \n        residuals = y - X @ beta_hat\n        k = X.shape[1] # Number of regressors (intercept + slope)\n\n        # 3. Compute Standard Errors\n\n        # 3.1. Classical (Homoskedastic) OLS Standard Error\n        sigma2_hat = np.sum(residuals**2) / (n - k)\n        var_cov_ols = sigma2_hat * inv_XTX\n        se_ols = np.sqrt(var_cov_ols[1, 1])\n\n        # 3.2. HC0 (White-Robust) Standard Error\n        # S_0 = sum(u_i^2 * x_i * x_i')\n        S0 = X.T @ np.diag(residuals**2) @ X\n        var_cov_hc0 = inv_XTX @ S0 @ inv_XTX\n        se_hc0 = np.sqrt(var_cov_hc0[1, 1])\n\n        # 3.3. Bootstrap Standard Error (Pairs Bootstrap)\n        bootstrap_betas = np.zeros(B)\n        for b in range(B):\n            # Resample pairs (y_i, x_i) with replacement\n            indices = rng.choice(n, size=n, replace=True)\n            y_star = y[indices]\n            X_star = X[indices]\n            \n            try:\n                # OLS on bootstrap sample\n                inv_XTX_star = np.linalg.inv(X_star.T @ X_star)\n                beta_hat_star = inv_XTX_star @ X_star.T @ y_star\n                bootstrap_betas[b] = beta_hat_star[1]\n            except np.linalg.LinAlgError:\n                bootstrap_betas[b] = np.nan\n        \n        # Standard deviation of bootstrap estimates\n        se_boot = np.nanstd(bootstrap_betas, ddof=1)\n\n        # 4. Store rounded results\n        case_results = [\n            round(se_ols, 6),\n            round(se_hc0, 6),\n            round(se_boot, 6)\n        ]\n        all_results.append(case_results)\n\n    # Format output string to be a list of lists with no spaces\n    formatted_results = [repr(r).replace(' ', '') for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}]}