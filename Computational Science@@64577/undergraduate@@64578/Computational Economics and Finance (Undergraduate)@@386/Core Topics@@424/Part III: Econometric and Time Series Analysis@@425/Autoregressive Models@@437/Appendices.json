{"hands_on_practices": [{"introduction": "A key feature of a stationary autoregressive process is its tendency to revert to a long-run average. This exercise introduces the fundamental concept of the unconditional mean of an AR(1) model, which represents the central value around which the time series fluctuates. By working through this simple calculation, you will learn how to derive this crucial long-run expectation directly from the model's intercept and autoregressive coefficient, providing a foundational skill for model interpretation [@problem_id:1897485].", "id": "1897485", "problem": "An economic research group is modeling the quarterly inflation anomaly, denoted by $I_t$, for a certain country. The anomaly is defined as the deviation of the observed inflation rate from the central bank's target rate. The group's analysis suggests that this time series can be effectively described by a first-order autoregressive (AR(1)) model of the form:\n\n$I_t = \\alpha + \\beta I_{t-1} + \\epsilon_t$\n\nHere, $I_t$ represents the inflation anomaly in percentage points during quarter $t$. The model includes a constant intercept $\\alpha = 0.4$, and an autoregressive coefficient $\\beta = 0.75$, which captures the persistence of the anomaly from the previous quarter. The term $\\epsilon_t$ is a white noise process, characterized by a mean of zero and constant variance, representing unpredictable shocks to inflation.\n\nAssuming the time series process for the inflation anomaly is stationary, calculate its long-run expected value. Express your answer in percentage points.\n\n", "solution": "We are given the AR(1) process for the inflation anomaly:\n$$\nI_{t} = \\alpha + \\beta I_{t-1} + \\epsilon_{t},\n$$\nwith $\\alpha = 0.4$, $\\beta = 0.75$, and $\\epsilon_{t}$ a white noise process with $\\mathbb{E}[\\epsilon_{t}] = 0$. Under the stationarity assumption (which requires $|\\beta| < 1$), the long-run expected value, or unconditional mean, is constant and equal to $\\mu = \\mathbb{E}[I_{t}]$.\n\nTaking expectations on both sides of the model:\n$$\n\\mathbb{E}[I_{t}] = \\mathbb{E}[\\alpha + \\beta I_{t-1} + \\epsilon_{t}] = \\alpha + \\beta \\mathbb{E}[I_{t-1}] + \\mathbb{E}[\\epsilon_{t}].\n$$\nUsing $\\mathbb{E}[\\epsilon_{t}] = 0$ and stationarity $\\mathbb{E}[I_{t}] = \\mathbb{E}[I_{t-1}] = \\mu$, we get:\n$$\n\\mu = \\alpha + \\beta \\mu.\n$$\nRearranging,\n$$\n\\mu - \\beta \\mu = \\alpha \\quad \\Rightarrow \\quad \\mu (1 - \\beta) = \\alpha \\quad \\Rightarrow \\quad \\mu = \\frac{\\alpha}{1 - \\beta}.\n$$\nSubstituting $\\alpha = 0.4$ and $\\beta = 0.75$,\n$$\n\\mu = \\frac{0.4}{1 - 0.75} = \\frac{0.4}{0.25} = 1.6.\n$$\nThus, the long-run expected value of the inflation anomaly is $1.6$ percentage points.", "answer": "$$\\boxed{1.6}$$"}, {"introduction": "While understanding a model's properties from its given parameters is crucial, the more common task in finance and economics is to estimate those parameters from observed data. This practice introduces the Yule-Walker equations, a classic method that connects a time series's autocovariance structure directly to the coefficients of its underlying AR process. By solving a system of these equations for an AR(2) model, you will gain hands-on experience with one of the foundational techniques for model estimation [@problem_id:2373810].", "id": "2373810", "problem": "A demeaned, weakly stationary monthly excess return series $\\{r_{t}\\}$ is modeled as an autoregressive (AR) model of order $2$, that is,\n$r_{t} = \\phi_{1} r_{t-1} + \\phi_{2} r_{t-2} + \\varepsilon_{t}$,\nwhere $\\{\\varepsilon_{t}\\}$ is a zero-mean white noise innovation process with variance $\\sigma_{\\varepsilon}^{2}$ and $\\varepsilon_{t}$ is uncorrelated with $\\{r_{s}\\}$ for all $s < t$. From a long sample, the following sample autocovariances at lags $0$, $1$, and $2$ are computed and will be treated as population values for estimation:\n$\\hat{\\gamma}(0) = 0.010$, $\\hat{\\gamma}(1) = 0.006$, $\\hat{\\gamma}(2) = 0.002$.\nUsing the Yule–Walker equations associated with the autoregressive structure and the given autocovariances, determine the Yule–Walker estimates of the parameters $\\phi_{1}$, $\\phi_{2}$, and the innovation variance $\\sigma_{\\varepsilon}^{2}$. Express your final answer as a single row matrix $\\left(\\phi_{1} \\ \\phi_{2} \\ \\sigma_{\\varepsilon}^{2}\\right)$. No rounding is required; provide exact values.", "solution": "The problem requires the determination of the parameters of a weakly stationary autoregressive model of order $2$, or AR($2$), using the Yule-Walker equations. This constitutes a standard application of the method of moments in time series analysis.\n\nFirst, we must validate the problem statement.\nThe given information is as follows:\n- The process $\\{r_{t}\\}$ is a demeaned, weakly stationary AR($2$) process: $r_{t} = \\phi_{1} r_{t-1} + \\phi_{2} r_{t-2} + \\varepsilon_{t}$.\n- The innovation process $\\{\\varepsilon_{t}\\}$ is zero-mean white noise with variance $\\sigma_{\\varepsilon}^{2}$.\n- The population autocovariances are taken to be the given sample values: $\\gamma(0) = 0.010$, $\\gamma(1) = 0.006$, and $\\gamma(2) = 0.002$.\n\nThe problem is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data sufficient for a unique solution. The autocovariance values are physically plausible for a stationary process; the variance $\\gamma(0)$ is positive, and the corresponding autocorrelations $\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = 0.6$ and $\\rho(2) = \\frac{\\gamma(2)}{\\gamma(0)} = 0.2$ are within the valid range of $[-1, 1]$. The problem is therefore deemed valid.\n\nWe proceed with the solution. The Yule-Walker equations for a general AR($p$) process are derived by multiplying the defining equation by $r_{t-k}$ for $k > 0$ and taking the expectation. This yields:\n$$E[r_{t}r_{t-k}] = \\sum_{i=1}^{p} \\phi_{i} E[r_{t-i}r_{t-k}] + E[\\varepsilon_{t}r_{t-k}]$$\nGiven weak stationarity and the fact that $\\varepsilon_{t}$ is uncorrelated with past values of $r_s$ (i.e., $E[\\varepsilon_{t}r_{t-k}]=0$ for $k>0$), this simplifies to:\n$$\\gamma(k) = \\sum_{i=1}^{p} \\phi_{i} \\gamma(k-i)$$\nFor the given AR($2$) process, we set $p=2$ and use $k=1$ and $k=2$:\nFor $k=1$: $\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(-1)$. Since $\\gamma(k) = \\gamma(-k)$, this is $\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(1)$.\nFor $k=2$: $\\gamma(2) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(0)$.\n\nThis gives a system of two linear equations in the two unknown parameters $\\phi_{1}$ and $\\phi_{2}$:\n$$\n\\begin{cases}\n\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(1) \\\\\n\\gamma(2) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(0)\n\\end{cases}\n$$\nIn matrix form, this is:\n$$\n\\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\end{pmatrix} =\n\\begin{pmatrix} \\gamma(0) & \\gamma(1) \\\\ \\gamma(1) & \\gamma(0) \\end{pmatrix}\n\\begin{pmatrix} \\phi_{1} \\\\ \\phi_{2} \\end{pmatrix}\n$$\nThe problem states to use the provided sample autocovariances as population values. To maintain precision, we convert the decimal values to fractions:\n$\\gamma(0) = 0.010 = \\frac{10}{1000} = \\frac{1}{100}$\n$\\gamma(1) = 0.006 = \\frac{6}{1000} = \\frac{3}{500}$\n$\\gamma(2) = 0.002 = \\frac{2}{1000} = \\frac{1}{500}$\n\nSubstituting these values into the system of equations:\n$$\n\\begin{cases}\n\\frac{3}{500} = \\phi_{1} \\frac{1}{100} + \\phi_{2} \\frac{3}{500} \\\\\n\\frac{1}{500} = \\phi_{1} \\frac{3}{500} + \\phi_{2} \\frac{1}{100}\n\\end{cases}\n$$\nTo simplify, we multiply both equations by $500$:\n$$\n\\begin{cases}\n3 = 5\\phi_{1} + 3\\phi_{2} \\\\\n1 = 3\\phi_{1} + 5\\phi_{2}\n\\end{cases}\n$$\nWe can solve this system. From the second equation, $3\\phi_{1} = 1 - 5\\phi_{2}$, so $\\phi_{1} = \\frac{1 - 5\\phi_{2}}{3}$. Substituting this into the first equation:\n$$3 = 5\\left(\\frac{1 - 5\\phi_{2}}{3}\\right) + 3\\phi_{2}$$\nMultiplying by $3$ gives:\n$$9 = 5(1 - 5\\phi_{2}) + 9\\phi_{2}$$\n$$9 = 5 - 25\\phi_{2} + 9\\phi_{2}$$\n$$4 = -16\\phi_{2}$$\n$$\\phi_{2} = -\\frac{4}{16} = -\\frac{1}{4}$$\nNow, we find $\\phi_{1}$:\n$$\\phi_{1} = \\frac{1 - 5(-\\frac{1}{4})}{3} = \\frac{1 + \\frac{5}{4}}{3} = \\frac{\\frac{9}{4}}{3} = \\frac{9}{12} = \\frac{3}{4}$$\nThe Yule-Walker estimates for the autoregressive coefficients are $\\phi_{1} = \\frac{3}{4}$ and $\\phi_{2} = -\\frac{1}{4}$.\n\nNext, we must find the innovation variance, $\\sigma_{\\varepsilon}^{2}$. This is obtained from the Yule-Walker equation for $k=0$, which involves the variance of the process, $\\gamma(0)$. We multiply the AR($2$) equation by $r_t$ and take expectations:\n$$E[r_{t}^2] = \\phi_{1} E[r_{t}r_{t-1}] + \\phi_{2} E[r_{t}r_{t-2}] + E[r_{t}\\varepsilon_{t}]$$\n$$\\gamma(0) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(2) + E[(\\phi_{1}r_{t-1} + \\phi_{2}r_{t-2} + \\varepsilon_{t})\\varepsilon_{t}]$$\nSince $\\varepsilon_{t}$ is uncorrelated with past values of $r$, $E[r_{t-1}\\varepsilon_{t}]=0$ and $E[r_{t-2}\\varepsilon_{t}]=0$. This leads to:\n$$\\gamma(0) = \\phi_{1}\\gamma(1) + \\phi_{2}\\gamma(2) + E[\\varepsilon_{t}^2]$$\n$$\\gamma(0) = \\phi_{1}\\gamma(1) + \\phi_{2}\\gamma(2) + \\sigma_{\\varepsilon}^{2}$$\nSolving for $\\sigma_{\\varepsilon}^{2}$:\n$$\\sigma_{\\varepsilon}^{2} = \\gamma(0) - \\phi_{1}\\gamma(1) - \\phi_{2}\\gamma(2)$$\nSubstituting the known values:\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\left(\\frac{3}{4}\\right)\\left(\\frac{3}{500}\\right) - \\left(-\\frac{1}{4}\\right)\\left(\\frac{1}{500}\\right)$$\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\frac{9}{2000} + \\frac{1}{2000}$$\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\frac{8}{2000} = \\frac{1}{100} - \\frac{1}{250}$$\nUsing a common denominator of $500$:\n$$\\sigma_{\\varepsilon}^{2} = \\frac{5}{500} - \\frac{2}{500} = \\frac{3}{500}$$\nThus, the innovation variance is $\\sigma_{\\varepsilon}^{2} = \\frac{3}{500}$.\n\nThe requested parameters are $\\phi_{1} = \\frac{3}{4}$, $\\phi_{2} = -\\frac{1}{4}$, and $\\sigma_{\\varepsilon}^{2} = \\frac{3}{500}$. We present these in the required row matrix format.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{4} & -\\frac{1}{4} & \\frac{3}{500}\n\\end{pmatrix}\n}\n$$"}, {"introduction": "The methods used in the previous practices, such as calculating a long-run mean and applying Yule-Walker equations, critically depend on the assumption that the time series is stationary. But what if it is not? This advanced hands-on practice challenges you to address this fundamental question by implementing a Dickey-Fuller test from first principles to detect a \"unit root,\" the statistical hallmark of non-stationarity. By simulating data, calculating the test statistic, and generating your own critical values, you will gain a deep, practical understanding of one of the most essential tools in modern time series analysis [@problem_id:2373818].", "id": "2373818", "problem": "You are asked to implement, from first principles, a one-sided left-tail Dickey–Fuller test for a unit root in an autoregressive process of order one. The setting is an autoregressive model of order one, where a univariate time series $\\{y_t\\}_{t=1}^T$ obeys the recursion $y_t = \\phi y_{t-1} + \\varepsilon_t$ with $\\varepsilon_t$ independently and identically distributed as a Gaussian random variable with mean $0$ and variance $\\sigma^2$, and initial value $y_0 = 0$. The null hypothesis is that the process has a unit root, that is $H_0: \\phi = 1$. The Dickey–Fuller test is formed by first differencing and regressing the first difference $\\Delta y_t$ on the lagged level $y_{t-1}$ without an intercept or trend. The statistical decision is based on the one-sided left-tail test against the alternative $H_1: \\phi < 1$ (equivalently $H_1: \\rho < 0$ for $\\rho \\equiv \\phi - 1$), rejecting when the test statistic is smaller than a critical value.\n\nStarting only from the definitions of the autoregressive model of order one and ordinary least squares, implement the full testing procedure without using any pre-built unit root test functions. The distribution of the Dickey–Fuller test statistic under the null hypothesis is non-standard; therefore, you must approximate the critical value at significance level $\\alpha$ by Monte Carlo simulation under the null using a random walk with Gaussian innovations.\n\nYour program must:\n- Implement ordinary least squares for the regression $\\Delta y_t$ on $y_{t-1}$, $t = 2, \\dots, T$, with no intercept and no trend, and compute the associated $t$-statistic for the slope coefficient.\n- Approximate the Dickey–Fuller left-tail critical value at significance level $\\alpha$ by simulating $M$ independent random walks of length $T$ under $H_0$ and computing the empirical $\\alpha$-quantile of the simulated test statistics.\n- For each test case, simulate a data set from the autoregressive model of order one with the specified parameters, compute the Dickey–Fuller test statistic, compare it to the simulated critical value, and output a boolean indicating whether $H_0$ is rejected at level $\\alpha$.\n\nUse the following foundational definitions and facts as your base:\n- An autoregressive model of order one is given by $y_t = \\phi y_{t-1} + \\varepsilon_t$ with $|\\phi| \\leq 1$ for a stationary or unit root process, and $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ independent across $t$.\n- The first difference is $\\Delta y_t \\equiv y_t - y_{t-1}$.\n- Ordinary least squares estimator for a simple regression without intercept, regressing $u_t$ on $v_t$ for $t = 1, \\dots, n$, is $\\hat{\\beta} = \\frac{\\sum_{t=1}^n v_t u_t}{\\sum_{t=1}^n v_t^2}$, with residual variance estimate $s^2 = \\frac{1}{n-1} \\sum_{t=1}^n (u_t - \\hat{\\beta} v_t)^2$ and standard error $\\operatorname{se}(\\hat{\\beta}) = \\sqrt{s^2 / \\sum_{t=1}^n v_t^2}$, yielding the $t$-statistic $t = \\hat{\\beta} / \\operatorname{se}(\\hat{\\beta})$.\n\nImplementation requirements:\n- Use the significance level $\\alpha = 0.05$ and Monte Carlo replications $M = 2000$ for critical value approximation.\n- For data simulation in each test case, use the provided data-generation random seed to ensure reproducibility. For the Monte Carlo approximation under the null, use the provided null-simulation random seed specific to that test case to ensure reproducibility of the critical value. Do not reuse any pre-built unit root or Dickey–Fuller functions.\n- Your program must be fully deterministic and must not require any user input.\n\nTest suite:\n- Each test case is a tuple $(T,\\phi,\\sigma,\\text{data\\_seed},\\text{null\\_seed})$ with the following values:\n  - Case A (happy path, unit root): $(T=\\;200,\\;\\phi=\\;1.0,\\;\\sigma=\\;1.0,\\;\\text{data\\_seed}=\\;123,\\;\\text{null\\_seed}=\\;9001)$.\n  - Case B (near unit root, stationary): $(T=\\;200,\\;\\phi=\\;0.95,\\;\\sigma=\\;1.0,\\;\\text{data\\_seed}=\\;456,\\;\\text{null\\_seed}=\\;9002)$.\n  - Case C (strongly stationary): $(T=\\;200,\\;\\phi=\\;0.0,\\;\\sigma=\\;1.0,\\;\\text{data\\_seed}=\\;789,\\;\\text{null\\_seed}=\\;9003)$.\n  - Case D (boundary with shorter sample): $(T=\\;60,\\;\\phi=\\;1.0,\\;\\sigma=\\;1.0,\\;\\text{data\\_seed}=\\;321,\\;\\text{null\\_seed}=\\;9004)$.\n\nOutput specification:\n- For each case, compute whether to reject $H_0$ at level $\\alpha=\\;0.05$ using the Monte Carlo critical value with $M=\\;2000$ replications. Rejection occurs if the Dickey–Fuller statistic is less than the empirical $\\alpha$-quantile computed under $H_0$.\n- Your program should produce a single line of output containing the four boolean results in order A, B, C, D as a comma-separated list enclosed in square brackets, for example, $\\texttt{[True,False,True,False]}$.\n\nNo physical units or angle units apply. Express any fractions or proportions as decimals, for example $\\alpha=\\;0.05$ is a decimal in $\\left[0,1\\right]$.", "solution": "The user's problem statement has been validated and is deemed valid. It constitutes a well-posed and scientifically grounded problem in computational econometrics, specifically concerning unit root testing in time series analysis. All necessary components, including the model, hypotheses, test statistic formulation, and simulation parameters, are provided, free of contradiction or ambiguity. The problem requests an implementation from first principles, which is a standard pedagogical exercise.\n\nThe core task is to implement a one-sided left-tail Dickey-Fuller test for an autoregressive process of order one, denoted AR($1$).\n\nFirst, we establish the theoretical framework. The AR($1$) model for a time series $\\{y_t\\}_{t=1}^T$ is given by the equation:\n$$y_t = \\phi y_{t-1} + \\varepsilon_t$$\nwhere the initial value is $y_0 = 0$, and the innovations $\\varepsilon_t$ are independent and identically distributed (i.i.d.) Gaussian random variables with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$. The parameter $\\phi$ governs the dynamics of the process. If $|\\phi| < 1$, the process is stationary. If $|\\phi| = 1$, the process contains a unit root and is non-stationary.\n\nThe problem is to test for the presence of a unit root. The null hypothesis, $H_0$, is that the process has a unit root, against the one-sided alternative, $H_1$, that the process is stationary. This is formulated as:\n$$H_0: \\phi = 1 \\quad \\text{vs.} \\quad H_1: \\phi < 1$$\nUnder the null hypothesis, the model becomes $y_t = y_{t-1} + \\varepsilon_t$, which is a random walk.\n\nThe Dickey-Fuller test procedure involves reparameterizing the model. By subtracting $y_{t-1}$ from both sides of the AR($1$) equation, we obtain:\n$$y_t - y_{t-1} = (\\phi - 1) y_{t-1} + \\varepsilon_t$$\nLet $\\Delta y_t = y_t - y_{t-1}$ be the first difference and $\\rho = \\phi - 1$. The equation transforms into the Dickey-Fuller regression model:\n$$\\Delta y_t = \\rho y_{t-1} + \\varepsilon_t$$\nIn this form, the hypotheses are equivalent to:\n$$H_0: \\rho = 0 \\quad \\text{vs.} \\quad H_1: \\rho < 0$$\nWe test this hypothesis by running a simple ordinary least squares (OLS) regression of $\\Delta y_t$ on $y_{t-1}$ for $t = 2, \\dots, T$, without an intercept. The number of observations in this regression is $n = T-1$.\n\nThe OLS estimator for $\\rho$ is given by:\n$$\\hat{\\rho} = \\frac{\\sum_{t=2}^T y_{t-1} \\Delta y_t}{\\sum_{t=2}^T y_{t-1}^2}$$\nThe residuals from this regression are $e_t = \\Delta y_t - \\hat{\\rho} y_{t-1}$. The unbiased estimator for the variance of the innovations, $s^2$, is calculated using $n-k$ degrees of freedom, where $n=T-1$ is the number of observations and $k=1$ is the number of estimated parameters (only $\\rho$).\n$$s^2 = \\frac{1}{T-2} \\sum_{t=2}^T e_t^2$$\nThe standard error of the OLS estimator $\\hat{\\rho}$ is:\n$$\\operatorname{se}(\\hat{\\rho}) = \\sqrt{\\frac{s^2}{\\sum_{t=2}^T y_{t-1}^2}}$$\nThe Dickey-Fuller test statistic is the $t$-statistic for the coefficient $\\hat{\\rho}$:\n$$t_{\\hat{\\rho}} = \\frac{\\hat{\\rho}}{\\operatorname{se}(\\hat{\\rho})}$$\n\nUnder the null hypothesis $H_0: \\rho=0$, the regressor $y_{t-1}$ is a non-stationary process (a sum of i.i.d. random variables). Consequently, the test statistic $t_{\\hat{\\rho}}$ does not follow the standard Student's $t$-distribution. Its limiting distribution is a functional of Brownian motion, known as the Dickey-Fuller distribution. Since this distribution is non-standard, critical values must be obtained through simulation.\n\nThe procedure to approximate the critical value at a significance level $\\alpha$ is as follows:\n1.  Generate a large number, $M$, of independent time series of length $T$, each under the null hypothesis ($H_0: \\phi=1$, a random walk). The distribution of the $t$-statistic is invariant to the innovation variance $\\sigma^2$, so we can conveniently use $\\sigma^2=1$ for these simulations.\n2.  For each of the $M$ simulated time series, compute the Dickey-Fuller test statistic $t_{\\hat{\\rho}}$.\n3.  Collect the $M$ computed statistics. The empirical $\\alpha$-quantile of this collection of statistics serves as the estimated critical value, $c_\\alpha$. For a left-tail test, this is the value such that $\\alpha$ proportion of the simulated statistics are less than it.\n\nThe complete algorithm for each test case is:\n1.  **Critical Value Simulation**: Given the sample size $T$, significance level $\\alpha=0.05$, and number of replications $M=2000$, use the provided `null_seed` to simulate the critical value $c_{0.05}$.\n2.  **Data Generation**: Given the parameters $T$, $\\phi$, $\\sigma$, use the provided `data_seed` to generate a single time series $\\{y_t\\}_{t=0}^T$ according to the AR($1$) model $y_t = \\phi y_{t-1} + \\varepsilon_t$ with $y_0 = 0$.\n3.  **Test Statistic Calculation**: Compute the Dickey-Fuller statistic $t_{\\text{data}}$ from the series generated in the previous step.\n4.  **Hypothesis Test Decision**: Compare the computed test statistic to the simulated critical value. Since the alternative hypothesis is $H_1: \\rho < 0$, we perform a left-tail test. We reject the null hypothesis $H_0$ if $t_{\\text{data}} < c_{0.05}$. The result is a boolean value indicating rejection.\n\nThis principled approach allows for the construction of the entire testing procedure from fundamental definitions, as required by the problem statement.", "answer": "```python\nimport numpy as np\n\ndef compute_df_statistic(y_series, T):\n    \"\"\"\n    Computes the Dickey-Fuller t-statistic for a given time series.\n    The regression is Delta y_t on y_{t-1} without an intercept.\n    \n    Args:\n        y_series (np.ndarray): The time series {y_t} from t=0 to T. Length must be T+1.\n        T (int): The number of time periods in the original series {y_t}_{t=1}^T.\n\n    Returns:\n        float: The computed t-statistic.\n    \"\"\"\n    # The regression uses observations from t=2 to T.\n    # y_lagged corresponds to y_{t-1} for t=2,...,T, so it is y_1, ..., y_{T-1}.\n    y_lagged = y_series[1:T]\n    \n    # delta_y corresponds to Delta y_t for t=2,...,T.\n    y_current = y_series[2:T+1]\n    delta_y = y_current - y_lagged\n\n    # Number of observations in the regression is n = T-1.\n    n_obs = T - 1\n\n    # OLS estimation for rho in Delta y_t = rho * y_{t-1} + e_t\n    sum_sq_y_lag = np.sum(y_lagged**2)\n    \n    # Avoid division by zero, although highly unlikely with random data.\n    if sum_sq_y_lag == 0:\n        return np.nan\n\n    rho_hat = np.sum(y_lagged * delta_y) / sum_sq_y_lag\n    \n    # Calculate residuals and estimate of error variance s^2\n    residuals = delta_y - rho_hat * y_lagged\n    # Degrees of freedom is n_obs - k, where k=1 (one parameter, rho)\n    # So, df = (T-1) - 1 = T-2.\n    df = n_obs - 1\n    if df <= 0: # Should not happen for T > 2\n        return np.nan\n        \n    s2 = np.sum(residuals**2) / df\n    \n    # Calculate standard error of rho_hat\n    se_rho_hat = np.sqrt(s2 / sum_sq_y_lag)\n    \n    # Avoid division by zero for the t-statistic itself.\n    if se_rho_hat == 0:\n        return np.nan # Or a large number, but NaN is more informative.\n\n    t_stat = rho_hat / se_rho_hat\n    return t_stat\n\ndef run_dickey_fuller_test(T, phi, sigma, data_seed, null_seed):\n    \"\"\"\n    Performs the complete Dickey-Fuller test for a single case.\n    \n    Args:\n        T (int): Sample size.\n        phi (float): Autoregressive coefficient.\n        sigma (float): Standard deviation of innovations.\n        data_seed (int): Random seed for data generation.\n        null_seed (int): Random seed for critical value simulation.\n\n    Returns:\n        bool: True if H0 is rejected, False otherwise.\n    \"\"\"\n    ALPHA = 0.05\n    M = 2000\n\n    # Step 1: Simulate the critical value under the null hypothesis (phi=1)\n    null_rng = np.random.default_rng(null_seed)\n    simulated_stats = np.empty(M)\n    for i in range(M):\n        # Generate a random walk (phi=1). The t-stat is invariant to sigma, so we use 1.0.\n        eps_null = null_rng.normal(scale=1.0, size=T)\n        y_null = np.zeros(T + 1)\n        # Using a loop for direct implementation of the model definition\n        for t in range(1, T + 1):\n            y_null[t] = y_null[t-1] + eps_null[t-1]\n        \n        simulated_stats[i] = compute_df_statistic(y_null, T)\n\n    # The critical value is the alpha-quantile of the simulated statistics distribution.\n    critical_value = np.quantile(simulated_stats, ALPHA)\n    \n    # Step 2: Generate the data for the specific test case\n    data_rng = np.random.default_rng(data_seed)\n    eps_data = data_rng.normal(scale=sigma, size=T)\n    y_data = np.zeros(T + 1)\n    for t in range(1, T + 1):\n        y_data[t] = phi * y_data[t-1] + eps_data[t-1]\n\n    # Step 3: Compute the Dickey-Fuller statistic for the generated data\n    t_stat_data = compute_df_statistic(y_data, T)\n    \n    # Step 4: Make the decision. Reject H0 if the statistic is in the left tail.\n    return t_stat_data < critical_value\n    \ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite: (T, phi, sigma, data_seed, null_seed)\n    test_cases = [\n        (200, 1.0, 1.0, 123, 9001),   # Case A (Unit root)\n        (200, 0.95, 1.0, 456, 9002),  # Case B (Near unit root)\n        (200, 0.0, 1.0, 789, 9003),   # Case C (Strongly stationary)\n        (60, 1.0, 1.0, 321, 9004),    # Case D (Unit root, shorter sample)\n    ]\n\n    results = []\n    for case in test_cases:\n        T, phi, sigma, data_seed, null_seed = case\n        reject_h0 = run_dickey_fuller_test(T, phi, sigma, data_seed, null_seed)\n        results.append(reject_h0)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"}]}