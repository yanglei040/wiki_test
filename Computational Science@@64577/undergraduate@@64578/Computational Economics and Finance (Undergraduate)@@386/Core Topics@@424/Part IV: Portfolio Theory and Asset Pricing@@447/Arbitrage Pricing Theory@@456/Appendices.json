{"hands_on_practices": [{"introduction": "The Arbitrage Pricing Theory provides a framework for understanding how systematic factors drive asset returns. A crucial skill in modern finance is the ability to construct portfolios that isolate these factors. This first exercise [@problem_id:2372091] provides hands-on practice in this by asking you to build a 'pure factor' portfolioâ€”one designed to have a unit exposure to a single target factor and zero exposure to all others, all while minimizing portfolio variance. By solving this constrained quadratic optimization problem, you will learn the fundamental technique for creating the building blocks of factor-based investment strategies.", "id": "2372091", "problem": "Design a program that constructs a \"pure factor\" portfolio in the sense of Arbitrage Pricing Theory (APT) (Arbitrage Pricing Theory (APT)) using quadratic optimization from first principles. You are given, for each test case, a factor loading matrix $B \\in \\mathbb{R}^{N \\times K}$, a symmetric positive-definite asset return covariance matrix $\\Sigma \\in \\mathbb{R}^{N \\times N}$, and a target factor index $j \\in \\{1,\\dots,K\\}$. The task is to compute portfolio weights $w \\in \\mathbb{R}^{N}$ that minimize portfolio variance subject to achieving unit exposure to the target factor and zero exposure to all other factors, and also being zero-investment (dollar-neutral).\n\nFormally, for each test case, solve the following optimization problem:\nMinimize the portfolio variance\n$$\n\\min_{w \\in \\mathbb{R}^{N}} \\; w^{\\top} \\Sigma \\, w\n$$\nsubject to the linear equality constraints\n$$\nB^{\\top} w \\;=\\; e_j, \\quad \\mathbf{1}^{\\top} w \\;=\\; 0,\n$$\nwhere $e_j \\in \\mathbb{R}^{K}$ is the $j$-th standard basis vector (i.e., the vector with a $1$ in position $j$ and $0$ elsewhere), and $\\mathbf{1} \\in \\mathbb{R}^{N}$ denotes the vector of ones. All quantities are unitless; any \"percentages\" must be represented in decimal form.\n\nYour program must produce three diagnostics for each test case:\n- The maximum absolute factor exposure deviation\n$$\n\\epsilon_{\\text{fac}} \\;=\\; \\max_{1 \\le k \\le K} \\left| \\left(B^{\\top} w\\right)_k - (e_j)_k \\right|.\n$$\n- The absolute net investment\n$$\n\\epsilon_{\\text{net}} \\;=\\; \\left| \\mathbf{1}^{\\top} w \\right|.\n$$\n- The achieved minimum variance\n$$\nv \\;=\\; w^{\\top} \\Sigma \\, w.\n$$\n\nYour outputs for each test case must be the list $[\\epsilon_{\\text{fac}}, \\epsilon_{\\text{net}}, v]$ with each value rounded to $8$ decimal places.\n\nTest suite. Use exactly the following three test cases:\n\nTest case A (happy path):\n- $N = 5$, $K = 2$, $j = 1$.\n- Factor loadings\n$$\nB \\;=\\;\n\\begin{bmatrix}\n0.5 & 1.0 \\\\\n1.2 & -0.3 \\\\\n-0.7 & 0.8 \\\\\n0.0 & -0.5 \\\\\n0.9 & 0.2\n\\end{bmatrix}.\n$$\n- Covariance (diagonal, positive-definite)\n$$\n\\Sigma \\;=\\; \\mathrm{diag}\\!\\left(0.04, \\; 0.09, \\; 0.025, \\; 0.16, \\; 0.0625\\right).\n$$\n\nTest case B (boundary: $N = K+1$ with unique feasible solution):\n- $N = 3$, $K = 2$, $j = 1$.\n- Factor loadings\n$$\nB \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.5 & 1.0 \\\\\n-0.5 & 0.5\n\\end{bmatrix}.\n$$\n- Covariance (diagonal, positive-definite)\n$$\n\\Sigma \\;=\\; \\mathrm{diag}\\!\\left(0.01, \\; 0.02, \\; 0.03\\right).\n$$\n\nTest case C (ill-conditioned covariance but positive-definite):\n- $N = 8$, $K = 3$, $j = 1$.\n- Factor loadings\n$$\nB \\;=\\;\n\\begin{bmatrix}\n0.2 & -0.1 & 0.3 \\\\\n1.1 & 0.4 & -0.2 \\\\\n-0.3 & 0.9 & 0.5 \\\\\n0.0 & -0.2 & 0.1 \\\\\n0.7 & 0.0 & -0.4 \\\\\n-0.6 & 0.3 & 0.2 \\\\\n0.4 & -0.5 & 0.0 \\\\\n0.8 & 0.1 & 0.6\n\\end{bmatrix}.\n$$\n- Covariance defined by\n$$\n\\Sigma \\;=\\; D \\;+\\; 0.001 \\cdot \\mathbf{1}\\mathbf{1}^{\\top},\n$$\nwhere $D = \\mathrm{diag}\\!\\left(0.04, \\; 0.01, \\; 0.09, \\; 0.0025, \\; 0.0225, \\; 0.01, \\; 0.04, \\; 0.0025\\right)$ and $\\mathbf{1}\\mathbf{1}^{\\top}$ is the $N \\times N$ matrix of ones.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of three-element lists, each inner list corresponding to one test case in the order A, B, C. Each number must be rounded to $8$ decimal places. For example, the output format must be\n$$\n\\texttt{[[a\\_1,a\\_2,a\\_3],[b\\_1,b\\_2,b\\_3],[c\\_1,c\\_2,c\\_3]]}\n$$\nwith no extra text, where each symbol stands for a decimal number as specified above.", "solution": "The problem presented is a constrained quadratic optimization task, which is a fundamental problem in computational finance, specifically in the context of portfolio construction under the Arbitrage Pricing Theory (APT). The objective is to find a portfolio weight vector $w$ that minimizes risk, quantified as portfolio variance, while satisfying a set of linear constraints that define the portfolio's exposure to systematic risk factors and its net investment.\n\nThe problem can be formally stated as follows. Find the vector of portfolio weights $w \\in \\mathbb{R}^{N}$ that solves:\n$$\n\\min_{w} f(w) = \\frac{1}{2} w^{\\top} \\Sigma w\n$$\nsubject to the linear equality constraints:\n$$\nB^{\\top} w = e_j\n$$\n$$\n\\mathbf{1}^{\\top} w = 0\n$$\nHere, $w$ is the $N \\times 1$ vector of asset weights, $\\Sigma$ is the $N \\times N$ symmetric positive-definite covariance matrix of asset returns, $B$ is the $N \\times K$ matrix of factor loadings, $\\mathbf{1}$ is the $N \\times 1$ vector of ones, and $e_j$ is the $K \\times 1$ standard basis vector with a $1$ at the $j$-th position and zeros elsewhere. The factor of $\\frac{1}{2}$ is included for mathematical convenience and does not alter the location of the minimum.\n\nThe constraints can be consolidated into a single matrix equation $C w = d$, where $C$ is a $(K+1) \\times N$ matrix and $d$ is a $(K+1) \\times 1$ vector:\n$$\nC = \\begin{bmatrix} B^{\\top} \\\\ \\mathbf{1}^{\\top} \\end{bmatrix}, \\quad d = \\begin{bmatrix} e_j \\\\ 0 \\end{bmatrix}\n$$\nThe problem is thus a standard equality-constrained quadratic program. Since the covariance matrix $\\Sigma$ is positive-definite, the objective function $f(w)$ is strictly convex. This property, combined with the linear nature of the constraints, guarantees the existence of a unique global minimum, provided the constraint system is feasible and the rows of $C$ are linearly independent.\n\nTo find the analytical solution, we employ the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ for this problem is:\n$$\n\\mathcal{L}(w, \\lambda) = \\frac{1}{2} w^{\\top} \\Sigma w + \\lambda^{\\top} (d - Cw)\n$$\nwhere $\\lambda$ is a $(K+1) \\times 1$ vector of Lagrange multipliers. The first-order necessary conditions for optimality require the gradient of the Lagrangian with respect to $w$ and $\\lambda$ to be zero.\n\nThe gradient with respect to $w$ is:\n$$\n\\nabla_w \\mathcal{L} = \\Sigma w - C^{\\top} \\lambda = 0\n$$\nSince $\\Sigma$ is positive-definite, it is invertible. We can therefore express $w$ in terms of $\\lambda$:\n$$\nw = \\Sigma^{-1} C^{\\top} \\lambda\n$$\nThe gradient with respect to $\\lambda$ simply recovers the linear constraints:\n$$\n\\nabla_\\lambda \\mathcal{L} = d - Cw = 0 \\implies Cw = d\n$$\nSubstituting the expression for $w$ into the constraint equation yields:\n$$\nC (\\Sigma^{-1} C^{\\top} \\lambda) = d\n$$\nThis can be written as a linear system for the Lagrange multipliers $\\lambda$:\n$$\n(C \\Sigma^{-1} C^{\\top}) \\lambda = d\n$$\nThe matrix $M = C \\Sigma^{-1} C^{\\top}$ is a $(K+1) \\times (K+1)$ matrix. Given that $\\Sigma^{-1}$ is positive-definite and we assume the constraint matrix $C$ has full row rank (i.e., its rows are linearly independent), the matrix $M$ is also positive-definite and therefore invertible. We can solve for $\\lambda$:\n$$\n\\lambda = (C \\Sigma^{-1} C^{\\top})^{-1} d\n$$\nFinally, substituting this expression for $\\lambda$ back into the equation for $w$ provides the closed-form solution for the optimal portfolio weights:\n$$\nw = \\Sigma^{-1} C^{\\top} (C \\Sigma^{-1} C^{\\top})^{-1} d\n$$\nA special case arises when $N = K+1$, as in Test Case B. Here, the constraint matrix $C$ is square. If it is invertible, the system $Cw=d$ has a unique solution $w=C^{-1}d$. This single feasible point is, by definition, the minimum. The general formula correctly degenerates to this case, as $w = \\Sigma^{-1} C^{\\top} (C^{\\top})^{-1} \\Sigma C^{-1} d = C^{-1}d$.\n\nThe computational procedure is as follows:\n$1$. For each test case, construct the matrices $B$, $\\Sigma$, and the parameters $N$, $K$, $j$.\n$2$. Construct the composite constraint matrix $C$ and the target vector $d$. Note that the problem specifies $j$ is $1$-indexed, which must be converted to $0$-indexed for array manipulation.\n$3$. Compute the inverse of the covariance matrix, $\\Sigma^{-1}$.\n$4$. Form the matrix $M = C \\Sigma^{-1} C^{\\top}$.\n$5$. Solve the linear system $M \\lambda = d$ for the vector $\\lambda$. Using a linear solver is numerically superior to explicit inversion of $M$.\n$6$. Calculate the optimal weight vector $w = \\Sigma^{-1} C^{\\top} \\lambda$.\n$7$. Using the computed $w$, calculate the three required diagnostic quantities:\n    - The maximum absolute factor exposure deviation: $\\epsilon_{\\text{fac}} = \\max_{k} |(B^{\\top} w)_k - (e_j)_k|$.\n    - The absolute net investment: $\\epsilon_{\\text{net}} = |\\mathbf{1}^{\\top} w|$.\n    - The minimum portfolio variance: $v = w^{\\top} \\Sigma w$.\n\nThe numerical results for $\\epsilon_{\\text{fac}}$ and $\\epsilon_{\\text{net}}$ should be close to machine precision, confirming the accuracy of the implementation in satisfying the constraints.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_single_case(B, Sigma, j, N, K):\n    \"\"\"\n    Solves the quadratic optimization problem for a single test case.\n\n    Args:\n        B (np.ndarray): Factor loading matrix (N x K).\n        Sigma (np.ndarray): Asset return covariance matrix (N x N).\n        j (int): 1-based index of the target factor.\n        N (int): Number of assets.\n        K (int): Number of factors.\n\n    Returns:\n        list: A list containing [eps_fac, eps_net, variance].\n    \"\"\"\n    # Convert 1-based target factor index to 0-based\n    target_idx = j - 1\n\n    # Construct the constraint matrix C of size (K+1) x N\n    C_mat = np.vstack([B.T, np.ones(N)])\n\n    # Construct the constraint target vector d of size (K+1)\n    d_vec = np.zeros(K + 1)\n    d_vec[target_idx] = 1.0\n\n    # Since Sigma is positive definite, its inverse exists.\n    Sigma_inv = np.linalg.inv(Sigma)\n\n    # Form the matrix for the linear system for Lagrange multipliers.\n    # M = C * Sigma_inv * C.T, which is of size (K+1) x (K+1).\n    M = C_mat @ Sigma_inv @ C_mat.T\n\n    # Solve the system M * lambda = d for the Lagrange multipliers lambda.\n    # This is numerically more stable than computing inv(M).\n    try:\n        lambda_vec = np.linalg.solve(M, d_vec)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudoinverse if M is singular, though not expected\n        # for valid problems.\n        lambda_vec = np.linalg.pinv(M) @ d_vec\n\n    # Calculate the optimal portfolio weights w = Sigma_inv * C.T * lambda\n    w = Sigma_inv @ C_mat.T @ lambda_vec\n\n    # --- Calculate Diagnostics ---\n\n    # 1. Maximum absolute factor exposure deviation (eps_fac)\n    ej_vec = np.zeros(K)\n    ej_vec[target_idx] = 1.0\n    factor_exposures = B.T @ w\n    eps_fac = np.max(np.abs(factor_exposures - ej_vec))\n\n    # 2. Absolute net investment (eps_net)\n    net_investment = np.sum(w)\n    eps_net = np.abs(net_investment)\n\n    # 3. Achieved minimum variance (v)\n    variance = w.T @ Sigma @ w\n\n    return [eps_fac, eps_net, variance]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the solver, and print results.\n    \"\"\"\n    # Test case A (happy path)\n    B_A = np.array([\n        [0.5, 1.0], [1.2, -0.3], [-0.7, 0.8], [0.0, -0.5], [0.9, 0.2]\n    ])\n    Sigma_A = np.diag([0.04, 0.09, 0.025, 0.16, 0.0625])\n    N_A, K_A, j_A = 5, 2, 1\n\n    # Test case B (boundary: N = K+1)\n    B_B = np.array([[1.0, 0.0], [0.5, 1.0], [-0.5, 0.5]])\n    Sigma_B = np.diag([0.01, 0.02, 0.03])\n    N_B, K_B, j_B = 3, 2, 1\n\n    # Test case C (ill-conditioned covariance)\n    B_C = np.array([\n        [0.2, -0.1, 0.3], [1.1, 0.4, -0.2], [-0.3, 0.9, 0.5],\n        [0.0, -0.2, 0.1], [0.7, 0.0, -0.4], [-0.6, 0.3, 0.2],\n        [0.4, -0.5, 0.0], [0.8, 0.1, 0.6]\n    ])\n    N_C, K_C, j_C = 8, 3, 1\n    D_C = np.diag([0.04, 0.01, 0.09, 0.0025, 0.0225, 0.01, 0.04, 0.0025])\n    ones_mat_C = np.ones((N_C, N_C))\n    Sigma_C = D_C + 0.001 * ones_mat_C\n\n    test_cases = [\n        (B_A, Sigma_A, j_A, N_A, K_A),\n        (B_B, Sigma_B, j_B, N_B, K_B),\n        (B_C, Sigma_C, j_C, N_C, K_C),\n    ]\n\n    results = []\n    for case in test_cases:\n        B, Sigma, j, N, K = case\n        res = solve_single_case(B, Sigma, j, N, K)\n        results.append(res)\n\n    # Format the final output string as per requirements\n    formatted_results = []\n    for res_list in results:\n        # Format each number to 8 decimal places\n        formatted_list = [f\"{val:.8f}\" for val in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"}, {"introduction": "While controlling factor risk is essential, the ultimate goal for many investment managers is to generate 'alpha' ($\\alpha$)â€”returns that are not explained by exposure to systematic factors. This practice [@problem_id:2372080] challenges you to construct a portfolio that is explicitly designed to hunt for $\\alpha$. You will use linear programming to find the optimal weights that maximize your portfolio's $\\alpha$ while simultaneously ensuring it is 'factor-neutral,' meaning its returns are, by design, uncorrelated with the risk factors. This exercise operationalizes the 'arbitrage' aspect of APT, demonstrating how to systematically exploit potential mispricings.", "id": "2372080", "problem": "You are given a finite set of risky assets indexed by $i \\in \\{1,\\dots,n\\}$ and a set of $m$ pre-defined factors in the Arbitrage Pricing Theory (APT). Let the expected return vector be $\\boldsymbol{\\mu} \\in \\mathbb{R}^{n}$ (entries expressed as decimals, e.g., $0.05$ means five hundredths), the risk-free rate be $r_f \\in \\mathbb{R}$, the factor loading matrix be $\\mathbf{B} \\in \\mathbb{R}^{n \\times m}$ whose $i$-th row contains the factor loadings of asset $i$, and the factor risk premia be $\\boldsymbol{\\lambda} \\in \\mathbb{R}^{m}$. The APT specification for each asset $i$ is\n$$\n\\mathbb{E}[R_i] \\;=\\; r_f \\;+\\; \\mathbf{B}_{i,\\cdot}\\,\\boldsymbol{\\lambda} \\;+\\; \\alpha_i,\n$$\nwhere $\\alpha_i$ is the pricing error (Jensenâ€™s alpha relative to the factor model). Define the vector of alphas $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{n}$ by\n$$\n\\boldsymbol{\\alpha} \\;=\\; \\boldsymbol{\\mu} \\;-\\; r_f \\,\\mathbf{1} \\;-\\; \\mathbf{B}\\,\\boldsymbol{\\lambda}.\n$$\nConsider a portfolio with weights $\\mathbf{w} \\in \\mathbb{R}^{n}$ that is fully invested, $\\mathbf{1}^\\top \\mathbf{w} = 1$, and factor-neutral, $\\mathbf{B}^\\top \\mathbf{w} = \\mathbf{0}$. Each weight is bounded by given lower and upper bounds $\\ell_i \\le w_i \\le u_i$. The Jensenâ€™s alpha of the factor-neutral portfolio relative to the given factors is\n$$\n\\alpha_p \\;=\\; \\boldsymbol{\\alpha}^\\top \\mathbf{w}.\n$$\nYour task is to write a complete program that, for each test case below, computes the maximum achievable portfolio Jensenâ€™s alpha $\\alpha_p$ over all portfolios $\\mathbf{w}$ that satisfy the fully-invested, factor-neutral, and box constraints. All rates must be treated as decimals (for example, $0.05$ represents five hundredths). The final output must list the maximum $\\alpha_p$ for each test case as floating-point numbers rounded to exactly $6$ decimal places.\n\nTest Suite (four independent cases):\n\n- Test Case A:\n  - Assets $n = 4$, factors $m = 2$.\n  - Risk-free rate $r_f = 0.02$.\n  - Factor premia $\\boldsymbol{\\lambda} = (0.05,\\,0.02)$.\n  - Factor loadings\n    $$\n    \\mathbf{B} \\;=\\;\n    \\begin{bmatrix}\n    0.8 & 0.2\\\\\n    -0.4 & 0.5\\\\\n    0.1 & -0.7\\\\\n    -0.5 & -0.1\n    \\end{bmatrix}.\n    $$\n  - Expected returns $\\boldsymbol{\\mu} = (0.074,\\,0.005,\\,0.014,\\,-0.007)$.\n  - Bounds $\\ell_i = -0.3$ and $u_i = 0.8$ for all $i \\in \\{1,2,3,4\\}$.\n\n- Test Case B:\n  - Assets $n = 3$, factors $m = 2$.\n  - Risk-free rate $r_f = 0.01$.\n  - Factor premia $\\boldsymbol{\\lambda} = (0.03,\\,0.01)$.\n  - Factor loadings\n    $$\n    \\mathbf{B} \\;=\\;\n    \\begin{bmatrix}\n    0.2 & -0.1\\\\\n    0.5 & 0.4\\\\\n    -0.7 & -0.3\n    \\end{bmatrix}.\n    $$\n  - Expected returns $\\boldsymbol{\\mu} = r_f \\mathbf{1} + \\mathbf{B}\\boldsymbol{\\lambda} = (0.015,\\,0.029,\\,-0.014)$.\n  - Bounds $\\ell_i = -0.5$ and $u_i = 0.8$ for all $i \\in \\{1,2,3\\}$.\n\n- Test Case C:\n  - Assets $n = 4$, factors $m = 2$.\n  - Risk-free rate $r_f = 0.01$.\n  - Factor premia $\\boldsymbol{\\lambda} = (0.04,\\,0.02)$.\n  - Factor loadings\n    $$\n    \\mathbf{B} \\;=\\;\n    \\begin{bmatrix}\n    0.5 & 0.4\\\\\n    -0.5 & -0.4\\\\\n    0.3 & -0.2\\\\\n    -0.3 & 0.2\n    \\end{bmatrix}.\n    $$\n  - Expected returns $\\boldsymbol{\\mu} = (0.04,\\,-0.019,\\,0.021,\\,0.0)$.\n  - Bounds $\\ell_i = 0.0$ and $u_i = 0.7$ for all $i \\in \\{1,2,3,4\\}$.\n\n- Test Case D:\n  - Assets $n = 4$, factors $m = 3$.\n  - Risk-free rate $r_f = 0.015$.\n  - Factor premia $\\boldsymbol{\\lambda} = (0.03,\\,0.01,\\,0.02)$.\n  - Factor loadings\n    $$\n    \\mathbf{B} \\;=\\;\n    \\begin{bmatrix}\n    0.6 & 0.1 & -0.2\\\\\n    -0.4 & 0.2 & 0.3\\\\\n    -0.2 & -0.3 & -0.4\\\\\n    0.0 & 0.0 & 0.3\n    \\end{bmatrix}.\n    $$\n  - Expected returns $\\boldsymbol{\\mu} = (0.026,\\,0.009,\\,-0.005,\\,0.02)$.\n  - Bounds $\\ell_i = -0.6$ and $u_i = 0.6$ for all $i \\in \\{1,2,3,4\\}$.\n\nProgram Requirements:\n\n- For each test case, compute $\\boldsymbol{\\alpha} = \\boldsymbol{\\mu} - r_f \\mathbf{1} - \\mathbf{B}\\boldsymbol{\\lambda}$.\n- Over all $\\mathbf{w} \\in \\mathbb{R}^{n}$ satisfying $\\mathbf{1}^\\top \\mathbf{w} = 1$, $\\mathbf{B}^\\top \\mathbf{w} = \\mathbf{0}$, and $\\ell_i \\le w_i \\le u_i$ for all $i$, compute the maximal portfolio Jensenâ€™s alpha $\\alpha_p^{\\star} = \\max \\{\\boldsymbol{\\alpha}^\\top \\mathbf{w}\\}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3,x_4]$), where each $x_k$ is the maximal $\\alpha_p^{\\star}$ for the $k$-th test case, rounded to exactly $6$ decimal places. No other text should be printed.", "solution": "The problem presented is a constrained optimization problem within the framework of Arbitrage Pricing Theory (APT). It is correctly formulated as a linear program, which is a class of problems that is well-posed and for which robust solution algorithms exist. The validation process confirms that the problem statement is scientifically grounded, self-contained, and objective. We shall proceed with a formal solution.\n\nThe task is to find the maximum achievable Jensen's alpha, $\\alpha_p$, of a portfolio. The portfolio alpha is a linear function of the portfolio weights $\\mathbf{w} \\in \\mathbb{R}^{n}$:\n$$\n\\alpha_p(\\mathbf{w}) = \\boldsymbol{\\alpha}^\\top \\mathbf{w}\n$$\nThe weights $\\mathbf{w}$ must satisfy a set of linear constraints:\n1.  Full investment constraint: $\\mathbf{1}^\\top \\mathbf{w} = 1$, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is a vector of ones.\n2.  Factor-neutrality constraint: $\\mathbf{B}^\\top \\mathbf{w} = \\mathbf{0}$, where $\\mathbf{B} \\in \\mathbb{R}^{n \\times m}$ is the factor loading matrix and $\\mathbf{0} \\in \\mathbb{R}^{m}$ is the zero vector.\n3.  Box constraints: $\\ell_i \\le w_i \\le u_i$ for each asset $i \\in \\{1, \\dots, n\\}$.\n\nThe vector of asset-specific alphas, $\\boldsymbol{\\alpha}$, is determined by the APT model's pricing errors:\n$$\n\\boldsymbol{\\alpha} = \\boldsymbol{\\mu} - r_f \\mathbf{1} - \\mathbf{B}\\boldsymbol{\\lambda}\n$$\nwhere $\\boldsymbol{\\mu}$ is the vector of expected returns, $r_f$ is the risk-free rate, and $\\boldsymbol{\\lambda}$ is the vector of factor risk premia.\n\nThis problem can be formally stated as the following linear program (LP):\n$$\n\\begin{aligned}\n\\underset{\\mathbf{w} \\in \\mathbb{R}^n}{\\text{maximize}} \\quad & \\boldsymbol{\\alpha}^\\top \\mathbf{w} \\\\\n\\text{subject to} \\quad & \\mathbf{1}^\\top \\mathbf{w} = 1 \\\\\n& \\mathbf{B}^\\top \\mathbf{w} = \\mathbf{0} \\\\\n& \\boldsymbol{\\ell} \\le \\mathbf{w} \\le \\mathbf{u}\n\\end{aligned}\n$$\nThe two equality constraints can be combined into a single matrix equation. We define a matrix $\\mathbf{A}_{eq} \\in \\mathbb{R}^{(1+m) \\times n}$ and a vector $\\mathbf{b}_{eq} \\in \\mathbb{R}^{1+m}$ as follows:\n$$\n\\mathbf{A}_{eq} = \\begin{bmatrix} \\mathbf{1}^\\top \\\\ \\mathbf{B}^\\top \\end{bmatrix}, \\quad \\mathbf{b}_{eq} = \\begin{bmatrix} 1 \\\\ \\mathbf{0} \\end{bmatrix}\n$$\nThe LP is then expressed in a standard form suitable for computational solvers:\n$$\n\\begin{aligned}\n\\underset{\\mathbf{w}}{\\text{maximize}} \\quad & \\boldsymbol{\\alpha}^\\top \\mathbf{w} \\\\\n\\text{subject to} \\quad & \\mathbf{A}_{eq} \\mathbf{w} = \\mathbf{b}_{eq} \\\\\n& \\boldsymbol{\\ell} \\le \\mathbf{w} \\le \\mathbf{u}\n\\end{aligned}\n$$\nTo solve this problem, we will utilize the `linprog` function from the `scipy.optimize` library. This function is designed to solve minimization problems. To achieve maximization of $\\boldsymbol{\\alpha}^\\top \\mathbf{w}$, we will minimize its negative, $-\\boldsymbol{\\alpha}^\\top \\mathbf{w}$. The objective function coefficients for the solver will be $\\mathbf{c} = -\\boldsymbol{\\alpha}$.\n\nThe procedural steps for each test case are as follows:\n\n1.  **Compute Alpha Vector:** Given the parameters for a test case ($\\boldsymbol{\\mu}$, $r_f$, $\\mathbf{B}$, $\\boldsymbol{\\lambda}$), the vector $\\boldsymbol{\\alpha}$ is computed. For Test Case B, the expected returns are given as $\\boldsymbol{\\mu} = r_f \\mathbf{1} + \\mathbf{B}\\boldsymbol{\\lambda}$. This directly implies that $\\boldsymbol{\\alpha} = \\boldsymbol{\\mu} - r_f \\mathbf{1} - \\mathbf{B}\\boldsymbol{\\lambda} = \\mathbf{0}$. Consequently, the objective function is $\\boldsymbol{\\alpha}^\\top \\mathbf{w} = 0$ for any $\\mathbf{w}$, and the maximum alpha is trivially $0$, provided the set of feasible portfolios is not empty.\n\n2.  **Construct LP Inputs:** The matrices and vectors for the `linprog` function are constructed.\n    - `c`: The coefficient vector for the objective function to be minimized, $\\mathbf{c} = -\\boldsymbol{\\alpha}$.\n    - `A_eq`: The equality constraint matrix $\\mathbf{A}_{eq}$, formed by vertically stacking a row of ones and the transpose of the factor loading matrix, $\\mathbf{B}^\\top$.\n    - `b_eq`: The equality constraint vector $\\mathbf{b}_{eq}$, which is a vector with $1$ as its first element followed by $m$ zeros.\n    - `bounds`: A sequence of tuples `(l_i, u_i)` for each weight $w_i$, representing the box constraints.\n\n3.  **Solve the LP:** The `scipy.optimize.linprog` function is called with these inputs. The function returns an optimization result object.\n\n4.  **Extract Solution:** The optimal value found by the solver, `res.fun`, corresponds to the minimum of $-\\boldsymbol{\\alpha}^\\top \\mathbf{w}$. Therefore, the maximum portfolio alpha is $\\alpha_p^{\\star} = -(\\text{res.fun})$.\n\nThis procedure is deterministic and will be applied to each test case specified in the problem statement. The final program implements these steps, calculates the maximum alpha for each case, and formats the output according to the specified requirements.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves for the maximum portfolio Jensen's alpha for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case A\n        {\n            \"n\": 4, \"m\": 2,\n            \"r_f\": 0.02,\n            \"lambda_vec\": np.array([0.05, 0.02]),\n            \"B\": np.array([\n                [0.8, 0.2],\n                [-0.4, 0.5],\n                [0.1, -0.7],\n                [-0.5, -0.1]\n            ]),\n            \"mu\": np.array([0.074, 0.005, 0.014, -0.007]),\n            \"bounds\": (-0.3, 0.8)\n        },\n        # Test Case B\n        {\n            \"n\": 3, \"m\": 2,\n            \"r_f\": 0.01,\n            \"lambda_vec\": np.array([0.03, 0.01]),\n            \"B\": np.array([\n                [0.2, -0.1],\n                [0.5, 0.4],\n                [-0.7, -0.3]\n            ]),\n            \"mu\": np.array([0.015, 0.029, -0.014]), # mu = r_f * 1 + B * lambda\n            \"bounds\": (-0.5, 0.8)\n        },\n        # Test Case C\n        {\n            \"n\": 4, \"m\": 2,\n            \"r_f\": 0.01,\n            \"lambda_vec\": np.array([0.04, 0.02]),\n            \"B\": np.array([\n                [0.5, 0.4],\n                [-0.5, -0.4],\n                [0.3, -0.2],\n                [-0.3, 0.2]\n            ]),\n            \"mu\": np.array([0.04, -0.019, 0.021, 0.0]),\n            \"bounds\": (0.0, 0.7)\n        },\n        # Test Case D\n        {\n            \"n\": 4, \"m\": 3,\n            \"r_f\": 0.015,\n            \"lambda_vec\": np.array([0.03, 0.01, 0.02]),\n            \"B\": np.array([\n                [0.6, 0.1, -0.2],\n                [-0.4, 0.2, 0.3],\n                [-0.2, -0.3, -0.4],\n                [0.0, 0.0, 0.3]\n            ]),\n            \"mu\": np.array([0.026, 0.009, -0.005, 0.02]),\n            \"bounds\": (-0.6, 0.6)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n, m = case[\"n\"], case[\"m\"]\n        r_f, lambda_vec = case[\"r_f\"], case[\"lambda_vec\"]\n        B, mu = case[\"B\"], case[\"mu\"]\n        bounds_tuple = case[\"bounds\"]\n\n        # Step 1: Compute the alpha vector\n        ones_n = np.ones(n)\n        alpha = mu - r_f * ones_n - B @ lambda_vec\n\n        # The problem is to maximize alpha.T @ w, which is equivalent to\n        # minimizing -alpha.T @ w.\n        c = -alpha\n\n        # Step 2: Set up the constraints for linprog\n        # Equality constraints: A_eq @ w = b_eq\n        # 1. Sum of weights is 1: np.ones(n).T @ w = 1\n        # 2. Factor-neutral: B.T @ w = 0\n        A_eq = np.vstack([np.ones(n), B.T])\n        \n        b_eq = np.zeros(1 + m)\n        b_eq[0] = 1\n\n        # Box constraints (bounds) for each weight w_i\n        bounds = [bounds_tuple for _ in range(n)]\n\n        # Step 3: Solve the linear programming problem\n        # Using 'highs' method, which is the current recommended solver in SciPy\n        res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n        \n        # Step 4: Extract the result\n        if res.success:\n            # res.fun is the minimum value of -alpha.T @ w.\n            # The maximum of alpha.T @ w is -res.fun.\n            max_alpha_p = -res.fun\n        else:\n            # In case the problem is infeasible or unbounded, although not expected\n            # for the given test cases.\n            max_alpha_p = np.nan\n\n        results.append(max_alpha_p)\n\n    # Final print statement in the exact required format.\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "The portfolio construction techniques we've explored rely on accurate estimates of factor loadings, the $b_{ik}$ coefficients that measure an asset's sensitivity to each factor. However, real-world financial data rarely conforms to the clean assumptions of normal distributions, often exhibiting 'fat tails' that can severely distort standard estimation methods like Ordinary Least Squares (OLS). This final exercise [@problem_id:2372129] delves into this critical practical challenge by having you compare the performance of OLS against a robust regression technique in estimating these loadings from simulated data with fat-tailed errors. This practice will solidify your understanding of why robust econometrics is indispensable for the successful application of any asset pricing model.", "id": "2372129", "problem": "Consider the linear multi-factor representation of Arbitrage Pricing Theory (APT), where for each asset index $i \\in \\{1,\\dots,N\\}$ and time index $t \\in \\{1,\\dots,T\\}$ the return $R_{i,t}$ satisfies\n$$\nR_{i,t} \\;=\\; a_i \\;+\\; \\sum_{k=1}^{K} b_{i k}\\, f_{k,t} \\;+\\; \\varepsilon_{i,t}.\n$$\nAssume the following data-generating process.\n- The intercepts satisfy $a_i = 0$ for all $i$.\n- The factor loadings satisfy $b_{i k} \\sim \\mathcal{N}(0,1)$ independently across $i$ and $k$.\n- The factors satisfy $f_{k,t} \\sim \\mathcal{N}(0,1)$ independently across $k$ and $t$, and are independent of all other random elements.\n- The idiosyncratic errors are fat-tailed. Let $z_{i,t}$ be independent and identically distributed (i.i.d.) standard Studentâ€™s $t$ variates with degrees of freedom $\\nu$, with the usual variance $\\nu/(\\nu-2)$ for $\\nu>2$. Define the unit-variance transformation $e_{i,t} = \\sqrt{\\frac{\\nu-2}{\\nu}}\\, z_{i,t}$ so that $\\mathrm{Var}(e_{i,t}) = 1$. Let the error standard deviation be $\\sigma_{\\varepsilon} = 0.5$ and set $\\varepsilon_{i,t} = \\sigma_{\\varepsilon} e_{i,t}$.\n\nFor each asset $i$, consider estimating the $K$-dimensional factor loading vector $(b_{i1},\\dots,b_{iK})$ via two estimators based on the $T$ observations $\\{(R_{i,t}, f_{1,t},\\dots,f_{K,t})\\}_{t=1}^{T}$ and an intercept. Let $X_t = (1, f_{1,t}, \\dots, f_{K,t})$ and $\\theta_i = (a_i, b_{i1},\\dots,b_{iK})$.\n- The Ordinary Least Squares (OLS) estimator (Ordinary Least Squares (OLS)) is the minimizer of the objective\n$$\n\\sum_{t=1}^{T} \\left(R_{i,t} - X_t \\theta_i\\right)^2.\n$$\n- The Huber robust estimator is the minimizer of the objective\n$$\n\\sum_{t=1}^{T} \\rho_{\\kappa}\\!\\left(R_{i,t} - X_t \\theta_i\\right),\n$$\nwhere the Huber loss with threshold $\\kappa$ is\n$$\n\\rho_{\\kappa}(u) \\;=\\; \\begin{cases}\n\\frac{1}{2}u^2, & \\text{if } |u| \\le \\kappa,\\\n$$6pt]\n\\kappa\\left(|u| - \\frac{1}{2}\\kappa\\right), & \\text{if } |u| > \\kappa,\n\\end{cases}\n$$\nwith $\\kappa = c\\,\\sigma_{\\varepsilon}$ and $c = 1.345$.\n\nFor each estimator, define the root mean squared error (Root Mean Squared Error (RMSE)) of the factor loading estimates over all assets and factors as\n$$\n\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\left(\\widehat{b}_{i k} - b_{i k}\\right)^2},\n$$\nwhere $\\widehat{b}_{ik}$ is the estimated factor loading for asset $i$ and factor $k$ (excluding the intercept).\n\nSimulation protocol and test suite:\n- Use a fixed base pseudorandom seed $s = 20231105$. For test case index $j \\in \\{0,1,2,3\\}$, set the pseudorandom seed equal to $s+j$ before generating all random elements for that test case.\n- For each test case below, generate one simulated panel $\\{R_{i,t}\\}$ from the model and compute the RMSE for the OLS estimator and for the Huber robust estimator. For that test case, output the boolean indicator\n$$\nI_j \\;=\\; \\mathbf{1}\\!\\left\\{\\mathrm{RMSE}_{\\text{robust}} < \\mathrm{RMSE}_{\\text{OLS}}\\right\\}.\n$$\n\nTest suite (each test case is specified as $(N, K, T, \\nu)$):\n1. $(50, 3, 250, 30)$.\n2. $(50, 3, 250, 3)$.\n3. $(50, 3, 60, 4)$.\n4. $(200, 5, 250, 3)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite, namely $[I_0,I_1,I_2,I_3]$ (for example, $[{\\tt True},{\\tt False},{\\tt True},{\\tt True}]$). No other text should be printed.", "solution": "The problem statement has been subjected to rigorous validation and is deemed valid. It is scientifically grounded in the principles of financial econometrics, specifically Arbitrage Pricing Theory (APT), and presents a well-posed, objective, and complete computational task. All parameters, distributions, and procedures are defined with sufficient precision for a unique and reproducible solution.\n\nThe task is to compare the performance of two estimators for the factor loadings in an APT model: Ordinary Least Squares (OLS) and a robust M-estimator using the Huber loss function. The comparison is to be performed via a simulation study under different parameterizations of the model, particularly concerning the tail thickness of the idiosyncratic error distribution.\n\nThe model for asset returns is given by\n$$\nR_{i,t} \\;=\\; a_i \\;+\\; \\sum_{k=1}^{K} b_{i k}\\, f_{k,t} \\;+\\; \\varepsilon_{i,t}\n$$\nwhere $a_i$ is the intercept, $b_{ik}$ are the factor loadings, $f_{k,t}$ are systematic factors, and $\\varepsilon_{i,t}$ is the idiosyncratic error. The parameters for the data generating process are specified as: $a_i = 0$, $b_{i k} \\sim \\mathcal{N}(0,1)$, and $f_{k,t} \\sim \\mathcal{N}(0,1)$.\n\nA critical feature of the model is the distribution of the idiosyncratic errors, $\\varepsilon_{i,t}$. These are specified to be fat-tailed, following a scaled Student's $t$-distribution. Specifically, $\\varepsilon_{i,t} = \\sigma_{\\varepsilon} e_{i,t}$, where $\\sigma_{\\varepsilon} = 0.5$ and $e_{i,t}$ is a transformation of a standard Student's $t$-variate $z_{i,t}$ with $\\nu$ degrees of freedom, $e_{i,t} = \\sqrt{(\\nu-2)/\\nu} \\, z_{i,t}$. This transformation ensures that $\\mathrm{Var}(e_{i,t}) = 1$ for $\\nu > 2$, and thus $\\mathrm{Var}(\\varepsilon_{i,t}) = \\sigma_{\\varepsilon}^2$. The degrees of freedom parameter $\\nu$ controls the tail thickness of the error distribution; smaller values of $\\nu$ correspond to heavier tails and a higher probability of observing extreme values (outliers).\n\nFor each asset $i$, the vector of parameters $\\theta_i = (a_i, b_{i1},\\dots,b_{iK})^T$ is estimated using $T$ time-series observations. The regression model for asset $i$ is $R_i = X \\theta_i + \\varepsilon_i$, where $R_i$ is the $T \\times 1$ vector of returns, $X$ is the $T \\times (K+1)$ design matrix with columns $(1, f_{1,\\cdot}, \\dots, f_{K,\\cdot})$, and $\\varepsilon_i$ is the $T \\times 1$ vector of errors.\n\nThe two estimators are:\n1.  **Ordinary Least Squares (OLS)**: This estimator minimizes the sum of squared residuals, $\\sum_{t=1}^{T} (R_{i,t} - X_t \\theta_i)^2$. The solution is given in closed form by $\\widehat{\\theta}_{i,\\text{OLS}} = (X^T X)^{-1} X^T R_i$. While OLS is the best linear unbiased estimator under the Gauss-Markov assumptions (which hold here, as the errors are independent and homoscedastic), its efficiency deteriorates significantly when the error distribution is not normal but fat-tailed, due to its sensitivity to large residuals.\n\n2.  **Huber Robust Estimator**: This M-estimator minimizes the sum of Huber losses, $\\sum_{t=1}^{T} \\rho_{\\kappa}(R_{i,t} - X_t \\theta_i)$. The Huber loss function, $\\rho_{\\kappa}(u)$, is quadratic for small arguments ($|u| \\le \\kappa$) and linear for large arguments ($|u| > \\kappa$). This hybrid nature allows the estimator to behave like OLS for observations with small residuals but reduces the influence of observations with large residuals (outliers), providing robustness. The threshold is set to $\\kappa = c\\,\\sigma_{\\varepsilon} = 1.345 \\times 0.5 = 0.6725$. The minimization of this convex objective function does not have a closed-form solution and requires numerical optimization. A quasi-Newton method such as BFGS is a suitable algorithm for this task.\n\nThe performance of the estimators is evaluated using the Root Mean Squared Error (RMSE) of the factor loading estimates, $\\widehat{b}_{ik}$, defined as:\n$$\n\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\left(\\widehat{b}_{i k} - b_{i k}\\right)^2}\n$$\nFor each test case, we compute a boolean indicator $I_j = \\mathbf{1}\\{\\mathrm{RMSE}_{\\text{robust}} < \\mathrm{RMSE}_{\\text{OLS}}\\}$. We expect the robust estimator to outperform OLS (i.e., yield a smaller RMSE) when the error distribution has heavy tails (small $\\nu$), while OLS may have a slight advantage when the errors are nearly normal (large $\\nu$).\n\nThe solution proceeds by implementing the specified simulation protocol. For each test case, all random elements are generated based on a fixed seed. Then, for each of the $N$ assets, both OLS and Huber estimates of the factor loadings are computed. The Huber estimation is performed using the `minimize` function from the `scipy.optimize` library. After collecting the estimates for all assets, the respective RMSEs are calculated and compared to determine the boolean indicator.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating asset returns from an APT model\n    and comparing the performance of OLS and Huber robust regression estimators.\n    \"\"\"\n    \n    # Simulation protocol parameters\n    base_seed = 20231105\n    c = 1.345\n    sigma_eps = 0.5\n    kappa = c * sigma_eps\n\n    test_cases = [\n        (50, 3, 250, 30),  # Case j=0\n        (50, 3, 250, 3),   # Case j=1\n        (50, 3, 60, 4),    # Case j=2\n        (200, 5, 250, 3),  # Case j=3\n    ]\n\n    results = []\n\n    def huber_loss(u, k):\n        \"\"\"Vectorized Huber loss function.\"\"\"\n        abs_u = np.abs(u)\n        is_small = abs_u <= k\n        \n        quadratic_loss = 0.5 * u[is_small]**2\n        linear_loss = k * (abs_u[~is_small] - 0.5 * k)\n        \n        loss = np.zeros_like(u)\n        loss[is_small] = quadratic_loss\n        loss[~is_small] = linear_loss\n        \n        return loss\n\n    def huber_objective(theta, X, y, k):\n        \"\"\"Objective function for Huber regression.\"\"\"\n        residuals = y - X @ theta\n        return np.sum(huber_loss(residuals, k))\n\n    for j, (N, K, T, nu) in enumerate(test_cases):\n        # Set the seed for reproducibility for the current test case\n        np.random.seed(base_seed + j)\n\n        # 1. Generate data from the specified DGP\n        # True factor loadings b_ik ~ N(0,1)\n        b_true = np.random.randn(N, K)\n        \n        # Factors f_kt ~ N(0,1)\n        factors = np.random.randn(K, T)\n        \n        # Idiosyncratic errors e_it with heavy tails\n        # z_it ~ Student's t with nu degrees of freedom\n        z = np.random.standard_t(df=nu, size=(N, T))\n        # Scale z to have unit variance\n        e = z * np.sqrt((nu - 2) / nu)\n        # Scale by error standard deviation\n        eps = sigma_eps * e\n        \n        # Generate asset returns R = b*f + eps (since a_i=0)\n        # R is an N x T matrix\n        R = b_true @ factors + eps\n\n        # 2. Prepare for estimation\n        # Design matrix X is T x (K+1), common for all assets\n        X = np.hstack([np.ones((T, 1)), factors.T])\n        \n        # Matrices to store estimated factor loadings (excluding intercept)\n        b_hat_ols = np.zeros((N, K))\n        b_hat_huber = np.zeros((N, K))\n\n        # 3. Estimate parameters for each asset\n        for i in range(N):\n            R_i = R[i, :]  # Returns for asset i (T x 1)\n            \n            # OLS Estimation\n            # theta_ols = (X'X)^-1 X'R_i\n            try:\n                theta_ols = np.linalg.solve(X.T @ X, X.T @ R_i)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if X'X is singular, though unlikely\n                theta_ols = np.linalg.pinv(X) @ R_i\n            \n            b_hat_ols[i, :] = theta_ols[1:]\n            \n            # Huber Robust Estimation\n            # Use OLS estimate as a starting point for the optimization\n            res = minimize(\n                fun=huber_objective,\n                x0=theta_ols,\n                args=(X, R_i, kappa),\n                method='BFGS',\n                options={'gtol': 1e-6}\n            )\n            theta_huber = res.x\n            b_hat_huber[i, :] = theta_huber[1:]\n\n        # 4. Calculate RMSE for factor loadings\n        rmse_ols = np.sqrt(np.mean((b_hat_ols - b_true)**2))\n        rmse_huber = np.sqrt(np.mean((b_hat_huber - b_true)**2))\n        \n        # 5. Compare RMSEs and store the boolean indicator\n        results.append(rmse_huber < rmse_ols)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}