{"hands_on_practices": [{"introduction": "We begin our hands-on journey with a classic problem in dynamic decision-making: the optimal replacement of a productive asset. This exercise asks you to determine when it is best to replace a machine whose condition deteriorates over time and whose replacement cost is itself a random variable [@problem_id:2419699]. This setup introduces the core challenge of balancing immediate costs against uncertain future costs and benefits, a hallmark of problems solved by dynamic programming. While often tackled with Value Function Iteration, this problem provides an excellent, intuitive foundation for understanding the structure of Bellman equations in an infinite-horizon context, setting the stage for the more advanced Policy Function Iteration method.", "id": "2419699", "problem": "Consider an infinite-horizon, discrete-time decision problem for a single productive machine. Time is indexed by $t \\in \\{0,1,2,\\dots\\}$. At the beginning of each period $t$, the decision-maker observes the machine’s condition state $s_t \\in \\{1,2,\\dots,S\\}$ and a replacement cost state $i_t \\in \\{1,2,\\dots,M\\}$. The state $s_t$ indexes condition from best ($s_t=1$) to worst ($s_t=S$). The replacement cost state $i_t$ determines the immediate cost of replacement in that period.\n\nAction set: in each period, the decision-maker chooses $a_t \\in \\{0,1\\}$, where $a_t=1$ means “replace now” and $a_t=0$ means “keep operating this period.”\n\nCosts and dynamics:\n- If $a_t=1$, an immediate cost equal to $C_{i_t}$ is paid, the machine is reset to the best condition next period so that $s_{t+1}=1$, and the replacement cost state evolves according to a Markov transition matrix $\\Pi \\in \\mathbb{R}^{M \\times M}$ with entries $\\pi_{i j} = \\mathbb{P}(i_{t+1}=j \\mid i_t=i)$.\n- If $a_t=0$, an immediate cost $m_{s_t}$ is paid. With probability $q_{s_t}$, the machine breaks during the period; in that case, there is an additional immediate failure penalty $f$ and an immediate replacement at the current period’s replacement cost $C_{i_t}$ is required to restore functionality by next period. With probability $1 - q_{s_t}$, there is no breakdown. The condition next period follows:\n  - If a breakdown occurs, $s_{t+1}=1$.\n  - If no breakdown occurs, $s_{t+1}=\\min\\{s_t+1, S\\}$.\n  The replacement cost state $i_t$ always evolves according to $\\Pi$, independent of the action $a_t$ and of breakdown realizations.\n\nObjective: minimize the expected, discounted sum of costs with discount factor $\\beta \\in (0,1)$:\n$$\n\\min_{\\{a_t\\}_{t \\ge 0}} \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t \\cdot \\text{cost}(s_t,i_t,a_t)\\right],\n$$\nwhere $\\text{cost}(s_t,i_t,1) = C_{i_t}$ and $\\text{cost}(s_t,i_t,0) = m_{s_t} + q_{s_t}\\,(C_{i_t} + f)$.\n\nLet the value function be $V(s,i)$, the minimal expected discounted cost starting from state $(s,i)$ at the beginning of a period. The optimal stationary policy is a mapping $\\pi^{\\star}(s,i) \\in \\{0,1\\}$ that minimizes the Bellman objective at every state $(s,i)$.\n\nState ordering for reporting results: for a given test case with $S$ and $M$, define the ordered list of states as\n$$\n[(1,1), (1,2), \\dots, (1,M), (2,1), (2,2), \\dots, (2,M), \\dots, (S,1), \\dots, (S,M)].\n$$\nYour program must compute the optimal stationary policy $\\pi^{\\star}(s,i)$ for all states in this order and report it as a list of integers (where $1$ denotes “replace now” and $0$ denotes “keep”).\n\nTest suite of parameter sets:\n- Test case $1$:\n  - $S = 4$, $M = 2$.\n  - Discount factor $\\beta = 0.95$.\n  - Maintenance costs $m = [1.0, 2.0, 3.0, 4.0]$ corresponding to $s \\in \\{1,2,3,4\\}$.\n  - Breakdown probabilities $q = [0.05, 0.10, 0.20, 0.40]$ corresponding to $s \\in \\{1,2,3,4\\}$.\n  - Failure penalty $f = 5.0$.\n  - Replacement costs $C = [8.0, 14.0]$ corresponding to $i \\in \\{1,2\\}$.\n  - Replacement cost transition matrix\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.9 & 0.1 \\\\\n    0.2 & 0.8\n    \\end{bmatrix}.\n    $$\n- Test case $2$:\n  - $S = 4$, $M = 2$.\n  - Discount factor $\\beta = 0.95$.\n  - Maintenance costs $m = [0.0, 1.0, 2.0, 3.0]$ for $s \\in \\{1,2,3,4\\}$.\n  - Breakdown probabilities $q = [0.20, 0.40, 0.60, 0.80]$ for $s \\in \\{1,2,3,4\\}$.\n  - Failure penalty $f = 20.0$.\n  - Replacement costs $C = [8.0, 12.0]$ for $i \\in \\{1,2\\}$.\n  - Replacement cost transition matrix\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.85 & 0.15 \\\\\n    0.25 & 0.75\n    \\end{bmatrix}.\n    $$\n- Test case $3$:\n  - $S = 4$, $M = 2$.\n  - Discount factor $\\beta = 0.90$.\n  - Maintenance costs $m = [0.5, 0.6, 0.7, 0.8]$ for $s \\in \\{1,2,3,4\\}$.\n  - Breakdown probabilities $q = [0.0, 0.0, 0.0, 0.0]$ for $s \\in \\{1,2,3,4\\}$.\n  - Failure penalty $f = 0.0$.\n  - Replacement costs $C = [10.0, 20.0]$ for $i \\in \\{1,2\\}$.\n  - Replacement cost transition matrix\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.95 & 0.05 \\\\\n    0.05 & 0.95\n    \\end{bmatrix}.\n    $$\n\nRequired output: For each test case, output a list of length $S \\cdot M$ containing the optimal binary decisions $\\pi^{\\star}(s,i)$ in the specified state order. Aggregate the results over all test cases into a single line as a list of lists, with no spaces, for example $[[\\dots],[\\dots],[\\dots]]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact format described above.", "solution": "The problem as stated is valid. It constitutes a standard, well-posed, infinite-horizon, discrete-time dynamic programming problem, commonly known as a Markov Decision Process (MDP). All parameters, state-transition dynamics, and cost structures are explicitly defined, allowing for the computation of a unique optimal stationary policy. The problem is grounded in the established theory of optimal control and computational economics.\n\nThe objective is to find the optimal stationary policy $\\pi^{\\star}(s,i)$ that minimizes the total expected discounted cost for an infinite horizon. The state of the system at any time $t$ is given by the pair $(s_t, i_t)$, where $s_t \\in \\{1, 2, \\dots, S\\}$ is the machine condition and $i_t \\in \\{1, 2, \\dots, M\\}$ is the replacement cost state. The decision-maker chooses an action $a_t \\in \\{0, 1\\}$, where $a_t=0$ is 'keep' and $a_t=1$ is 'replace'.\n\nThe solution to this problem is characterized by the Bellman equation for the value function $V(s,i)$, which represents the minimal expected discounted cost starting from state $(s,i)$. The Bellman equation is:\n$$\nV(s,i) = \\min \\{ V_0(s,i), V_1(s,i) \\}\n$$\nwhere $V_a(s,i)$ is the value associated with taking action $a$ in state $(s,i)$. These are often called the action-value functions.\n\nLet us define the expected one-period ahead value, contingent on the current cost state $i$ and the next-period machine state $s'$, as:\n$$\nE(s', i) = \\sum_{j=1}^{M} \\pi_{ij} V(s', j)\n$$\nwhere $\\pi_{ij}$ is the probability of transitioning from cost state $i$ to cost state $j$, given by the transition matrix $\\Pi$.\n\nThe action-value function for 'replace' ($a=1$) is given by:\n$$\nV_1(s,i) = C_i + \\beta E(1, i)\n$$\nThis is because replacing the machine incurs an immediate cost $C_i$ and transitions the machine state to the best condition, $s'=1$, in the next period.\n\nThe action-value function for 'keep' ($a=0$) is more complex. The expected immediate cost is $m_s + q_s(f+C_i)$. The machine state in the next period depends on whether a breakdown occurs. A breakdown, with probability $q_s$, resets the machine to state $s'=1$. If no breakdown occurs, with probability $1-q_s$, the machine deteriorates to state $s'=\\min\\{s+1, S\\}$. Therefore, the value of keeping is:\n$$\nV_0(s,i) = \\left( m_s + q_s(f+C_i) \\right) + \\beta \\left[ q_s E(1, i) + (1-q_s) E(\\min\\{s+1, S\\}, i) \\right]\n$$\nThe problem is solved by finding the fixed point of the Bellman operator, which can be accomplished using the method of value iteration. This iterative algorithm proceeds as follows:\n$1$. Initialize the value function for all states, for instance, $V_k(s,i) = 0$ for all $(s,i)$ at iteration $k=0$.\n$2$. For each subsequent iteration $k=1, 2, \\dots$, update the value function for all states $(s,i)$ using the previous iteration's values $V_{k-1}$:\n$$\nV_k(s,i) = \\min \\left\\{ \\left( m_s + q_s(f+C_i) \\right) + \\beta \\left[ q_s E_{k-1}(1, i) + (1-q_s) E_{k-1}(\\min\\{s+1, S\\}, i) \\right], \\quad C_i + \\beta E_{k-1}(1, i) \\right\\}\n$$\nwhere $E_{k-1}(s',i) = \\sum_{j=1}^{M} \\pi_{ij} V_{k-1}(s', j)$.\n$3$. The iteration continues until the value function converges, i.e., when the maximum change across all states is smaller than a pre-defined tolerance $\\epsilon$: $\\max_{s,i} |V_k(s,i) - V_{k-1}(s,i)| < \\epsilon$. Since the discount factor $\\beta \\in (0,1)$, the Bellman operator is a contraction mapping, which guarantees convergence to a unique fixed point $V^\\star$.\n\nOnce the converged value function $V^\\star$ is obtained, the optimal stationary policy $\\pi^\\star(s,i)$ is determined by choosing the action that minimizes a single-period Bellman lookahead for each state:\n$$\n\\pi^\\star(s,i) = \\arg\\min_{a \\in \\{0,1\\}} V_a(s,i)\n$$\nSpecifically, the policy is to replace if the value of replacing is less than the value of keeping. Adhering to the convention of keeping in case of a tie:\n$$\n\\pi^\\star(s,i) = \\begin{cases} 0 & \\text{if } V_0(s,i) \\le V_1(s,i) \\\\ 1 & \\text{if } V_0(s,i) > V_1(s,i) \\end{cases}\n$$\nThe implementation will use vectorized operations with NumPy for computational efficiency. The value function $V(s,i)$ will be stored in an $S \\times M$ matrix. The expected future values will be calculated using matrix multiplication with the transpose of the transition matrix $\\Pi$. The optimal policy will be derived from the final value function and formatted as a flattened list, following the specified state ordering.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_optimal_policy(S, M, beta, m, q, f, C, Pi):\n    \"\"\"\n    Solves for the optimal replacement policy using value iteration.\n\n    Args:\n        S (int): Number of machine condition states.\n        M (int): Number of replacement cost states.\n        beta (float): Discount factor.\n        m (np.ndarray): Vector of maintenance costs for each condition state.\n        q (np.ndarray): Vector of breakdown probabilities for each condition state.\n        f (float): Failure penalty.\n        C (np.ndarray): Vector of replacement costs for each cost state.\n        Pi (np.ndarray): Transition matrix for replacement cost states.\n\n    Returns:\n        list: A flattened list of optimal actions (0 for keep, 1 for replace)\n              ordered by state (s, i) in row-major order.\n    \"\"\"\n    # Initialize value function\n    V = np.zeros((S, M))\n\n    # Value iteration parameters\n    tolerance = 1e-9\n    max_iterations = 10000\n\n    for _ in range(max_iterations):\n        V_old = V.copy()\n\n        # Calculate expected future values for all states (s,i)\n        # ExpectedV[s, i] = sum_{j} Pi[i, j] * V[s, j]\n        ExpectedV = V @ Pi.T\n\n        # --- Value of replacing (a=1) ---\n        # Immediate cost C_i + discounted future value from state s=1\n        # The expected future value for s=1 is the first row of ExpectedV.\n        EV_s1_vec = ExpectedV[0, :]\n        # val_replace is a 1xM row vector, will be broadcast to SxM for comparison\n        val_replace = C + beta * EV_s1_vec\n\n        # --- Value of keeping (a=0) ---\n        # Reshape parameter vectors for broadcasting\n        m_col = m.reshape(-1, 1)  # S x 1\n        q_col = q.reshape(-1, 1)  # S x 1\n        C_row = C.reshape(1, -1)  # 1 x M\n\n        # Expected immediate cost: m_s + q_s * (f + C_i)\n        immediate_cost_keep = m_col + q_col * (f + C_row)\n\n        # Expected future cost: beta * [q_s * E[V(1,j)] + (1-q_s) * E[V(min{s+1,S},j)]]\n        s_indices = np.arange(S)\n        s_next_keep_indices = np.minimum(s_indices + 1, S - 1)\n        EV_s_next_keep_matrix = ExpectedV[s_next_keep_indices, :]\n        \n        future_cost_keep = beta * (\n            q_col * EV_s1_vec.reshape(1, -1) + (1 - q_col) * EV_s_next_keep_matrix\n        )\n\n        val_keep = immediate_cost_keep + future_cost_keep\n\n        # Bellman update\n        V = np.minimum(val_keep, val_replace)\n\n        # Check for convergence\n        if np.max(np.abs(V - V_old)) < tolerance:\n            break\n    \n    # After convergence, determine the optimal policy using the final val_keep and val_replace\n    # Policy is 1 (replace) if val_keep > val_replace, 0 (keep) otherwise.\n    policy = (val_keep > val_replace).astype(int)\n\n    # Flatten the policy matrix to a list in row-major order\n    return policy.flatten().tolist()\n\ndef solve():\n    \"\"\"\n    Defines the test cases, solves for the optimal policy for each,\n    and prints the results in the required format.\n    \"\"\"\n    test_cases = [\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.95,\n            \"m\": np.array([1.0, 2.0, 3.0, 4.0]),\n            \"q\": np.array([0.05, 0.10, 0.20, 0.40]),\n            \"f\": 5.0,\n            \"C\": np.array([8.0, 14.0]),\n            \"Pi\": np.array([[0.9, 0.1], [0.2, 0.8]]),\n        },\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.95,\n            \"m\": np.array([0.0, 1.0, 2.0, 3.0]),\n            \"q\": np.array([0.20, 0.40, 0.60, 0.80]),\n            \"f\": 20.0,\n            \"C\": np.array([8.0, 12.0]),\n            \"Pi\": np.array([[0.85, 0.15], [0.25, 0.75]]),\n        },\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.90,\n            \"m\": np.array([0.5, 0.6, 0.7, 0.8]),\n            \"q\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"f\": 0.0,\n            \"C\": np.array([10.0, 20.0]),\n            \"Pi\": np.array([[0.95, 0.05], [0.05, 0.95]]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        policy = compute_optimal_policy(\n            params[\"S\"], params[\"M\"], params[\"beta\"], params[\"m\"],\n            params[\"q\"], params[\"f\"], params[\"C\"], params[\"Pi\"]\n        )\n        results.append(policy)\n\n    # Format the final output string without spaces\n    result_strings = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"}, {"introduction": "Having established the structure of a dynamic problem, we now turn to the heart of our topic: implementing the Policy Function Iteration (PFI) algorithm itself. You will apply PFI to solve a cornerstone model of macroeconomics—the neoclassical growth model—where an agent decides how much to save versus consume over an infinite lifetime [@problem_id:2419733]. This practice will guide you through the two key steps of PFI: evaluating the long-term value of a given savings plan (policy evaluation) and then using that valuation to find an even better plan (policy improvement). The exercise also introduces the practical challenge of an occasionally binding constraint, a common feature in economic models, and teaches you to assess the accuracy of your solution using the Euler equation residual.", "id": "2419733", "problem": "Consider the infinite-horizon, deterministic dynamic programming problem of a representative agent who chooses next-period capital to maximize lifetime utility. The agent has utility over consumption given by the Constant Relative Risk Aversion (CRRA) utility function $u(c)$, defined as $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ when $\\sigma \\neq 1$ and $u(c) = \\log(c)$ when $\\sigma = 1$. The resource constraint is $c + k' = A k^{\\alpha} + (1-\\delta) k$, where $k$ is current capital, $k'$ is next-period capital, $A$ is total factor productivity, $\\alpha$ is the capital share, and $\\delta$ is the depreciation rate. The agent discounts the future at factor $\\beta \\in (0,1)$. The next-period capital is subject to an occasionally binding inequality constraint $k' \\ge \\bar{k}$. Feasibility also requires nonnegative consumption, which implies $k' \\le A k^{\\alpha} + (1-\\delta) k$. The state space for capital is restricted to a grid $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$ with $k_1 = \\bar{k}$ and $k_N = k_{\\max}$, where the grid is evenly spaced with $N$ points.\n\nFor each parameter set below, compute the stationary optimal policy function $g: \\mathcal{K} \\to \\mathcal{K}$ that maps each grid point $k \\in \\mathcal{K}$ to an optimal choice $k' = g(k)$ on the same grid, subject to $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$, and define the following diagnostics:\n\n- Diagnostic $1$: the fraction of grid points at which the lower bound is binding, computed as the fraction of $k \\in \\mathcal{K}$ for which $g(k) \\le \\bar{k} + \\tau$, where the binding tolerance is $\\tau = 10^{-10}$.\n\n- Diagnostic $2$: the policy value at the median grid index, i.e., $g(k_m)$ where $m = \\lfloor (N-1)/2 \\rfloor + 1$ is the median index in $\\{1,2,\\dots,N\\}$.\n\n- Diagnostic $3$: the maximum absolute Euler equation residual across the grid under the computed policy, defined for each $k \\in \\mathcal{K}$ as\n$$\nr(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|,\n$$\nwhere $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$ and $u'(c)$ is the marginal utility of consumption, given by $u'(c) = c^{-\\sigma}$ when $\\sigma \\neq 1$ and $u'(c) = \\frac{1}{c}$ when $\\sigma = 1$. The diagnostic $3$ to report is $\\max_{k \\in \\mathcal{K}} r(k)$.\n\nThe capital grid is defined as the evenly spaced set $\\mathcal{K}$ with $N$ points between $\\bar{k}$ and $k_{\\max}$ inclusive. For all computations, use the feasibility condition $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$ at each $k \\in \\mathcal{K}$, and only evaluate $u(c)$ when $c > 0$.\n\nTest Suite:\n- Test $1$: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$.\n- Test $2$: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$.\n- Test $3$: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$.\n\nYour program must compute the optimal stationary policy function $g$ on the specified grid for each test and return, for each test in order, the three diagnostics described above, as real numbers. The final output format must aggregate all tests into a single line containing a comma-separated list enclosed in square brackets, consisting of the $3$ diagnostics for Test $1$, followed by the $3$ diagnostics for Test $2$, followed by the $3$ diagnostics for Test $3$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\").", "solution": "The user has provided a well-defined problem from computational macroeconomics. I will first confirm its validity and then present a complete solution.\n\n### Step 1: Extract Givens\n\n- **Utility Function**: $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ for $\\sigma \\neq 1$, and $u(c) = \\log(c)$ for $\\sigma = 1$.\n- **Resource Constraint**: $c + k' = A k^{\\alpha} + (1-\\delta) k$.\n- **Parameters**: Total factor productivity $A$, capital share $\\alpha$, depreciation rate $\\delta$, CRRA coefficient $\\sigma$, and discount factor $\\beta \\in (0,1)$.\n- **State and Choice Variables**: Current capital $k$ (state), next-period capital $k'$ (choice).\n- **Constraints**: $k' \\ge \\bar{k}$ (lower bound) and $k' \\le A k^{\\alpha} + (1-\\delta) k$ (feasibility). $c>0$ is required.\n- **State Space**: A discrete grid $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$, which is an evenly spaced set of $N$ points from $k_1 = \\bar{k}$ to $k_N = k_{\\max}$.\n- **Policy Function**: A stationary mapping $g: \\mathcal{K} \\to \\mathcal{K}$ such that $k' = g(k)$.\n- **Diagnostic 1**: Fraction of grid points $k \\in \\mathcal{K}$ where $g(k) \\le \\bar{k} + \\tau$ with tolerance $\\tau = 10^{-10}$.\n- **Diagnostic 2**: Policy value $g(k_m)$ at the median grid index $m = \\lfloor (N-1)/2 \\rfloor + 1$.\n- **Diagnostic 3**: Maximum absolute Euler equation residual, $\\max_{k \\in \\mathcal{K}} r(k)$, where\n  $r(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|$,\n  with $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$ and $u'(c)$ being the marginal utility.\n- **Test Cases**:\n  - Test 1: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$\n  - Test 2: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$\n  - Test 3: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem describes the standard neoclassical growth model (also called the Ramsey-Cass-Koopmans model) in a discrete-time, deterministic setting. This is a foundational model in macroeconomics. All components—CRRA utility, Cobb-Douglas production technology, capital accumulation dynamics—are standard and rigorously established. The problem is scientifically sound.\n- **Well-Posed**: The problem is to find a stationary optimal policy function for a discounted dynamic programming problem. The state space $\\mathcal{K}$ is a compact set, and the utility function is bounded for any feasible policy. By the Contraction Mapping Theorem, a unique stationary optimal policy function exists. The problem is well-posed.\n- **Objective**: The problem is stated using precise mathematical definitions and objective language. There are no subjective or ambiguous terms.\n- **Conclusion**: The problem does not violate any of the specified invalidity criteria. It is a complete, consistent, and solvable scientific problem.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe problem requires finding the stationary optimal policy function $g(k)$ for an infinite-horizon dynamic programming problem. The Bellman equation for this problem is:\n$$\nV(k) = \\max_{k' \\in \\mathcal{K} \\text{ and feasible}} \\left\\{ u(A k^{\\alpha} + (1-\\delta)k - k') + \\beta V(k') \\right\\}\n$$\nwhere $V(k)$ is the value function. The optimal policy function $g(k)$ is the function that gives the $k'$ that achieves the maximum for each $k$.\n\nWe will solve for the policy function $g(k)$ using the **Policy Function Iteration (PFI)** algorithm. This iterative method finds the fixed point of the Bellman operator, which corresponds to the stationary optimal policy.\n\nThe PFI algorithm proceeds in the following steps:\n1.  **Initialization**:\n    -   Define the capital grid $\\mathcal{K}$ as an array of $N$ points from $\\bar{k}$ to $k_{\\max}$.\n    -   Start with an initial guess for the policy function, $g_0(k)$. A simple and feasible initial policy is to choose the minimum possible capital for the next period, i.e., $g_0(k) = \\bar{k}$ for all $k \\in \\mathcal{K}$.\n\n2.  **Iteration**: Repeat the following two steps until the policy function converges. Let the current iteration be $j$.\n    a. **Policy Evaluation**: Given the current policy $g_j(k)$, compute the corresponding value function $V_j(k)$. This value function is the solution to the linear functional equation:\n       $$\n       V_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k)) + \\beta V_j(g_j(k))\n       $$\n       This represents a system of $N$ linear equations for the $N$ values $\\{V_j(k_i)\\}_{i=1}^N$. Instead of solving this system directly via matrix inversion, which is computationally intensive ($O(N^3)$), we can find $V_j$ by iterating on the value function for a fixed policy:\n       $$\n       V_{j, s+1}(k) = R_j(k) + \\beta V_{j,s}(g_j(k))\n       $$\n       where $R_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k))$ is the reward under policy $g_j$. This inner loop is iterated until $V_{j,s}$ converges to $V_j$.\n\n    b. **Policy Improvement**: With the value function $V_j$ from the evaluation step, find an improved policy $g_{j+1}(k)$ by solving the maximization problem for each state $k \\in \\mathcal{K}$:\n       $$\n       g_{j+1}(k) = \\underset{k' \\in \\mathcal{K} \\text{ s.t. } \\bar{k} \\le k' < Y(k)}{\\arg\\max} \\left\\{ u(Y(k) - k') + \\beta V_j(k') \\right\\}\n       $$\n       where $Y(k) = A k^{\\alpha} + (1-\\delta)k$ is the total available resource. The search for the optimal $k'$ is performed over all grid points that satisfy the feasibility constraints. The strict inequality $k' < Y(k)$ ensures that consumption is always positive, as required for the logarithmic and CRRA utility functions.\n\n3.  **Convergence**: The algorithm terminates when the policy function no longer changes between iterations, i.e., $g_{j+1}(k) = g_j(k)$ for all $k \\in \\mathcal{K}$. The resulting policy is the stationary optimal policy, $g(k)$.\n\nOnce the optimal policy function $g(k)$ is computed, the three diagnostics are calculated as follows:\n- **Diagnostic 1**: The fraction of grid points where the lower bound on capital is binding. Since the policy choices $g(k)$ must lie on the grid $\\mathcal{K}$ and the first grid point is $\\bar{k}$, the condition $g(k) \\le \\bar{k} + \\tau$ for a small $\\tau$ is equivalent to $g(k) = \\bar{k}$. We compute the fraction of states $k \\in \\mathcal{K}$ for which $g(k) = \\bar{k}$.\n- **Diagnostic 2**: The value of the policy function at the median capital grid point, $k_m$. The index $m$ is $1$-based in the problem, so for a $0$-indexed array of size $N$, this corresponds to index $\\lfloor(N-1)/2\\rfloor$. We report $g(k_{\\lfloor(N-1)/2\\rfloor})$.\n- **Diagnostic 3**: The maximum absolute Euler equation error across the grid. For each state $k$, we compute the consumption $c(k) = Y(k) - g(k)$ and the consumption in the next period, $c(g(k)) = Y(g(k)) - g(g(k))$. Then, we calculate the residual $r(k)$ using the provided formula and find the maximum value over all $k \\in \\mathcal{K}$. This serves as a measure of the accuracy of our computed policy.\n\nThis principled approach guarantees finding the discrete optimal policy and allows for a straightforward computation of the required diagnostics.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the dynamic programming problem for all test cases\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0),\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0),\n        (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        alpha, beta, delta, A, sigma, k_bar, N, k_max = params\n        diagnostics = compute_policy_and_diagnostics(\n            alpha, beta, delta, A, sigma, k_bar, int(N), k_max\n        )\n        results.extend(diagnostics)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_policy_and_diagnostics(alpha, beta, delta, A, sigma, k_bar, N, k_max):\n    \"\"\"\n    Computes the stationary optimal policy function using Policy Function Iteration\n    and calculates the required diagnostics.\n    \"\"\"\n    # 1. Setup the economic environment\n    k_grid = np.linspace(k_bar, k_max, N)\n\n    # Define utility and marginal utility functions based on sigma\n    if sigma == 1.0:\n        u = lambda c: np.log(c)\n        u_prime = lambda c: 1.0 / c\n    else:\n        u = lambda c: (c**(1.0 - sigma)) / (1.0 - sigma)\n        u_prime = lambda c: c**(-sigma)\n\n    # 2. Policy Function Iteration (PFI)\n    policy_idx = np.zeros(N, dtype=int)  # Initial policy: g(k) = k_bar for all k\n    \n    max_pfi_iter = 200\n    pfi_tol = 1e-9\n\n    V = np.zeros(N) # Initialize value function\n\n    for pfi_iter in range(max_pfi_iter):\n        # --- Policy Evaluation ---\n        # Iteratively find the value function for the current policy\n        max_v_iter = 5000\n        v_tol = 1e-9\n        \n        k_prime_eval = k_grid[policy_idx]\n        c_eval = A * k_grid**alpha + (1.0 - delta) * k_grid - k_prime_eval\n        \n        # Consumption must be positive\n        R = np.full(N, -np.inf)\n        positive_c_mask = c_eval > 0\n        R[positive_c_mask] = u(c_eval[positive_c_mask])\n        \n        for v_iter in range(max_v_iter):\n            V_next = R + beta * V[policy_idx]\n            if np.max(np.abs(V_next - V)) < v_tol:\n                break\n            V = V_next\n        else: # This 'else' belongs to the for-loop, runs if no 'break'\n            pass # Convergence not reached, but proceed with current V\n\n        # --- Policy Improvement ---\n        new_policy_idx = np.zeros(N, dtype=int)\n        y = A * k_grid**alpha + (1.0 - delta) * k_grid\n\n        for i in range(N):\n            # Find feasible choices for k' (must allow for positive consumption)\n            # k_prime < y[i]\n            valid_k_prime_end_idx = np.searchsorted(k_grid, y[i], side='left')\n\n            if valid_k_prime_end_idx == 0:\n                # No feasible choice leads to positive consumption\n                new_policy_idx[i] = 0 # Default to k_bar\n                continue\n\n            choice_indices = np.arange(valid_k_prime_end_idx)\n            k_prime_choices = k_grid[choice_indices]\n            c_choices = y[i] - k_prime_choices\n            \n            value_choices = u(c_choices) + beta * V[choice_indices]\n            \n            best_choice_local_idx = np.argmax(value_choices)\n            new_policy_idx[i] = choice_indices[best_choice_local_idx]\n\n        # --- Convergence Check ---\n        if np.max(np.abs(new_policy_idx - policy_idx)) < 1:\n            policy_idx = new_policy_idx\n            break\n        \n        policy_idx = new_policy_idx\n\n    # 3. Calculate Diagnostics\n    # Final policy g(k) = k'\n    g_k = k_grid[policy_idx]\n    \n    # Diagnostic 1: Fraction of grid points where the lower bound is binding\n    # Since g(k) is on the grid and k_bar is the first grid point,\n    # g(k) <= k_bar + tau is equivalent to g(k) == k_bar.\n    diag1 = np.mean(policy_idx == 0)\n    \n    # Diagnostic 2: Policy value at the median grid index\n    # Problem: m = floor((N-1)/2) + 1 (1-based), Python: (N-1)//2 (0-based)\n    median_idx = (N - 1) // 2\n    diag2 = g_k[median_idx]\n    \n    # Diagnostic 3: Maximum absolute Euler equation residual\n    c = A * k_grid**alpha + (1.0 - delta) * k_grid - g_k\n    \n    # Find g(g(k))\n    g_of_g_k_policy_idx = policy_idx[policy_idx]\n    g_of_g_k = k_grid[g_of_g_k_policy_idx]\n    \n    # Consumption at the next state, c' = c(g(k))\n    c_prime = A * g_k**alpha + (1.0 - delta) * g_k - g_of_g_k\n    \n    # Ensure all consumptions are positive before taking logs or negative powers\n    if np.any(c <= 0) or np.any(c_prime <= 0):\n        # This indicates a problem, as feasible policies should yield c > 0.\n        # However, to prevent crashes, we'll signal an error with a large residual.\n        # In a well-behaved model, this branch is not taken.\n        return diag1, diag2, np.inf\n\n    u_prime_c = u_prime(c)\n    u_prime_c_prime = u_prime(c_prime)\n    \n    # Marginal return on capital (k')\n    return_on_k_prime = alpha * A * g_k**(alpha - 1.0) + (1.0 - delta)\n    \n    residuals = np.abs(u_prime_c - beta * u_prime_c_prime * return_on_k_prime)\n    diag3 = np.max(residuals)\n    \n    return diag1, diag2, diag3\n\nif __name__ == '__main__':\n    solve()\n```"}, {"introduction": "Our final practice elevates the analysis to a more realistic and complex scenario by incorporating uncertainty directly into the economic environment. You will solve a stochastic optimal growth model where the economy is subject to random productivity shocks that follow a Markov process [@problem_id:2419647]. This capstone exercise demonstrates the power of Policy Function Iteration in handling stochastic dynamic problems. Beyond simply computing the optimal policy, you will use it to characterize the economy's long-run behavior by computing the stationary distribution of capital and productivity, allowing you to calculate key aggregate statistics like the average capital stock and consumption. This practice bridges the gap between algorithmic implementation and genuine computational economic analysis.", "id": "2419647", "problem": "You are given a stationary infinite-horizon dynamic programming problem of optimal capital accumulation with discrete state and action spaces. The state is a pair $(k,z)$ where $k$ is current capital and $z$ is current total factor productivity. The choice is next-period capital $k'$. The per-period utility is $u(c)$ with constant relative risk aversion (CRRA), and the resource constraint is $c = z k^{\\alpha} + (1-\\delta)k - k'$. The intertemporal objective is the unique fixed point of the Bellman equation\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\},\n$$\nwhere $\\mathcal{K}$ is a finite grid of capital choices and $z$ follows a finite-state time-homogeneous Markov chain with transition matrix $P$. The utility function is $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ for $c > 0$ and $u(c) = -10^{12}$ for $c \\le 0$ (a large negative number to encode infeasibility). The expectation $\\mathbb{E}[V(k',z') \\mid z]$ is taken with respect to the next-period productivity $z'$ conditional on current $z$, using the given Markov transition matrix $P$. The action space is restricted to the grid $\\mathcal{K}$.\n\nThe state space is discrete: $z$ takes values in a finite set $\\mathcal{Z} = \\{z_1,\\dots,z_S\\}$ and $k$ takes values in the grid $\\mathcal{K} = \\{k_1,\\dots,k_N\\}$, where $k_1 = \\underline{k}$, $k_N = \\overline{k}$, and the grid is evenly spaced between these bounds. The Markov transition matrix $P$ is an $S \\times S$ row-stochastic matrix with entries $P_{ij} = \\Pr(z' = z_j \\mid z = z_i)$.\n\nFor each parameter set below, compute:\n1. The optimal stationary policy $\\pi(k,z) \\in \\mathcal{K}$ that solves the Bellman equation on the given grids, with the tie-breaking rule that if multiple $k'$ attain the maximum, select the smallest such $k'$ in $\\mathcal{K}$.\n2. The corresponding value function $V(k,z)$ associated with $\\pi$.\n3. The Markov transition matrix $Q$ on the joint state space $\\mathcal{K} \\times \\mathcal{Z}$ induced by $(\\pi,P)$, which has entries $Q\\left((k,z),(k',z')\\right) = \\mathbf{1}\\{k' = \\pi(k,z)\\} \\cdot P(z,z')$.\n4. The unique stationary distribution $\\mu$ on $\\mathcal{K} \\times \\mathcal{Z}$ satisfying $\\mu = \\mu Q$, obtained as the limit of repeated application of $Q$ to an initial uniform distribution on $\\mathcal{K} \\times \\mathcal{Z}$.\n5. The expected capital $\\mathbb{E}_{\\mu}[k]$ and expected consumption $\\mathbb{E}_{\\mu}[c]$ under the stationary distribution, where $c(k,z) = z k^{\\alpha} + (1-\\delta)k - \\pi(k,z)$.\n6. The Bellman residual at the computed solution, defined as\n$$\nR = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} P(z,z') V(k',z') \\right\\} \\right) - V(k,z) \\right|.\n$$\n\nYour program must compute the above for each parameter set in the test suite. For each parameter set, produce a list of three real numbers: $[\\mathbb{E}_{\\mu}[k], \\mathbb{E}_{\\mu}[c], R]$, with each entry rounded to $6$ decimals. The final program output must be a single line containing the concatenation of these lists for all test cases, as a single comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3,x_4,x_5,x_6,\\dots]$.\n\nUse the following test suite of parameter sets:\n- Test case $1$ (baseline):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.08$, $\\sigma = 2.0$.\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$ with transition matrix $P = \\begin{bmatrix}0.9 & 0.1 \\\\ 0.1 & 0.9\\end{bmatrix}$.\n  - Capital grid $\\mathcal{K}$: $N = 200$ evenly spaced points from $\\underline{k} = 0.001$ to $\\overline{k} = 5.0$.\n- Test case $2$ (high patience):\n  - $\\alpha = 0.35$, $\\beta = 0.995$, $\\delta = 0.08$, $\\sigma = 2.0$.\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$ with transition matrix $P = \\begin{bmatrix}0.9 & 0.1 \\\\ 0.1 & 0.9\\end{bmatrix}$.\n  - Capital grid $\\mathcal{K}$: $N = 200$ evenly spaced points from $\\underline{k} = 0.001$ to $\\overline{k} = 5.0$.\n- Test case $3$ (low volatility, different curvature and depreciation):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.10$, $\\sigma = 3.0$.\n  - $\\mathcal{Z} = \\{1.0, 1.0\\}$ with transition matrix $P = \\begin{bmatrix}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{bmatrix}$.\n  - Capital grid $\\mathcal{K}$: $N = 180$ evenly spaced points from $\\underline{k} = 0.001$ to $\\overline{k} = 4.0$.\n\nAngle units are not applicable. No physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries ordered as $[\\mathbb{E}_{\\mu}[k]^{(1)},\\mathbb{E}_{\\mu}[c]^{(1)},R^{(1)},\\mathbb{E}_{\\mu}[k]^{(2)},\\mathbb{E}_{\\mu}[c]^{(2)},R^{(2)},\\mathbb{E}_{\\mu}[k]^{(3)},\\mathbb{E}_{\\mu}[c]^{(3)},R^{(3)}]$, where the superscripts indicate the test case index. Each numeric entry must be rounded to $6$ decimals.", "solution": "The problem presented is a canonical stationary infinite-horizon dynamic programming problem, specifically a discretized version of the stochastic optimal growth model. The task is to find the optimal savings policy and characterize the long-run behavior of the system. The problem is well-posed; the Bellman operator is a contraction mapping under the given parameters, guaranteeing the existence and uniqueness of the value function and an associated optimal stationary policy. We will solve this problem using the Policy Function Iteration (PFI) algorithm.\n\nThe state of the system is a pair $(k,z)$, where $k \\in \\mathcal{K}$ is the capital stock and $z \\in \\mathcal{Z}$ is the productivity level. The agent chooses the next-period capital stock $k' \\in \\mathcal{K}$ to maximize the discounted sum of future utilities. The objective is to find the value function $V(k,z)$ that is the unique fixed point of the Bellman equation:\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\}\n$$\nThe utility function is of the Constant Relative Risk Aversion (CRRA) form, $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$, with a large penalty for non-positive consumption. The expectation is taken with respect to the Markov process for productivity $z$, $\\mathbb{E}[V(k',z') \\mid z_i] = \\sum_{j=1}^S P_{ij} V(k', z_j)$.\n\nThe Policy Function Iteration (PFI) algorithm is an iterative procedure that finds the optimal policy $\\pi(k,z)$ by alternating between two steps:\n\n1.  **Policy Evaluation**: For a given policy function $\\pi_j$, we compute the value $V^{\\pi_j}$ associated with following this policy indefinitely. This value function is the solution to the linear system of equations defined by the Bellman operator for a fixed policy:\n    $$\n    V^{\\pi_j}(k,z) = u\\left(z k^{\\alpha} + (1-\\delta)k - \\pi_j(k,z)\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(\\pi_j(k,z), z_{s'})\n    $$\n    This system can be solved either by matrix inversion, $(I - \\beta M_{\\pi_j})^{-1} u_{\\pi_j}$, where $M_{\\pi_j}$ is the transition matrix induced by the policy, or, more practically for large state spaces, by iterating the operator above until the value function converges. We shall employ the latter iterative method within each evaluation step.\n\n2.  **Policy Improvement**: Given the value function $V^{\\pi_j}$ from the evaluation step, we find a new, improved policy $\\pi_{j+1}$ by solving the one-step maximization problem for each state $(k,z)$:\n    $$\n    \\pi_{j+1}(k,z) = \\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(k', z_{s'}) \\right\\}\n    $$\n    The specified tie-breaking rule—selecting the smallest $k'$ among optimizers—is applied.\n\nThe algorithm starts with an initial guess for the policy, $\\pi_0$, and repeats these two steps until the policy function no longer changes, i.e., $\\pi_{j+1} = \\pi_j$. At this point, the optimal policy $\\pi^*$ and its corresponding value function $V^*$ have been found.\n\nAfter obtaining the converged policy $\\pi^*$ and value function $V^*$, we compute the required statistics:\n\n-   **State Transition Matrix $Q$**: The Markov transition matrix on the joint discrete state space $\\mathcal{K} \\times \\mathcal{Z}$ is constructed. Its elements are $Q((k,z),(k',z')) = \\mathbf{1}\\{k' = \\pi^*(k,z)\\} \\cdot P(z,z')$. This is an $(NS \\times NS)$ matrix, where $N=|\\mathcal{K}|$ and $S=|\\mathcal{Z}|$.\n\n-   **Stationary Distribution $\\mu$**: The unique stationary distribution $\\mu$ is a row vector of length $NS$ satisfying the property $\\mu = \\mu Q$. It is computed using the power method: starting with an initial uniform distribution $\\mu_0$, we iterate $\\mu_{m+1} = \\mu_m Q$ until convergence.\n\n-   **Expected Values**: With the stationary distribution $\\mu$ represented as a matrix of probabilities $\\mu_{is}$ for each state $(k_i, z_s)$, we compute the expected capital and consumption:\n    $$\n    \\mathbb{E}_{\\mu}[k] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} k_i\n    $$\n    $$\n    \\mathbb{E}_{\\mu}[c] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} \\left( z_s k_i^{\\alpha} + (1-\\delta)k_i - k_{\\pi^*(i,s)} \\right)\n    $$\n    where $k_{\\pi^*(i,s)}$ is the capital level corresponding to the index returned by the policy function.\n\n-   **Bellman Residual $R$**: This quantity measures the precision of the numerical solution. It is defined as the maximum absolute difference between the computed value function $V^*$ and the result of applying the full Bellman operator one more time:\n    $$\n    R = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(c(k,z,k')\\right) + \\beta \\sum_{z'} P(z,z') V^*(k',z') \\right\\} \\right) - V^*(k,z) \\right|\n    $$\n\nThe implementation will use vectorized `NumPy` operations to ensure computational efficiency. The tie-breaking rule is naturally handled by `numpy.argmax`, which returns the index of the first maximum.", "answer": "```python\nimport numpy as np\n\ndef solve_model(alpha, beta, delta, sigma, z_grid, P, k_grid):\n    \"\"\"\n    Solves the optimal growth model using Policy Function Iteration.\n    \"\"\"\n    N = len(k_grid)\n    S = len(z_grid)\n    \n    # Utility function\n    # Note: The case sigma = 1 is not in test cases, so we ignore it.\n    def u(c):\n        utility = np.full(c.shape, -1e12)\n        positive_c = c > 0\n        utility[positive_c] = (c[positive_c]**(1 - sigma)) / (1 - sigma)\n        return utility\n\n    # --- Policy Function Iteration Setup ---\n    # V: value function (N, S)\n    # pi: policy function, stores index of k' (N, S)\n    V = np.zeros((N, S))\n    pi = np.zeros((N, S), dtype=int) \n\n    # PFI convergence parameters\n    max_pfi_iter = 200\n    pfi_iter = 0\n    policy_converged = False\n\n    # Policy Evaluation convergence parameters\n    max_eval_iter = 2000\n    eval_tol = 1e-9\n\n    # Pre-calculate consumption for all state-action pairs\n    # Shape: (N_k, N_z, N_k') -> (i, s, j)\n    k_matrix_i = k_grid[:, np.newaxis, np.newaxis]\n    z_matrix_s = z_grid[np.newaxis, :, np.newaxis]\n    k_matrix_j = k_grid[np.newaxis, np.newaxis, :]\n    \n    consumption = z_matrix_s * (k_matrix_i**alpha) + (1 - delta) * k_matrix_i - k_matrix_j\n    utility_all_choices = u(consumption)\n\n    # --- PFI Loop ---\n    while not policy_converged and pfi_iter < max_pfi_iter:\n        pfi_iter += 1\n        pi_old = pi.copy()\n\n        # 1. Policy Evaluation Step\n        V_eval = V.copy() # Use V from previous PFI step as starting point\n        for _ in range(max_eval_iter):\n            k_prime_indices = pi # Shape (N, S)\n            \n            # Expected value term: E[V(k', z')|z]\n            # V @ P.T gives E_z'[V(k, z')] for each k, z\n            EV = V_eval @ P.T # Shape (N, S)\n            \n            # Select the expected value for the policy-dictated k'\n            # EV[pi[i, s], s] for each (i,s)\n            expected_val_at_pi = EV[pi, np.arange(S)] # Shape (N, S)\n\n            # Consumption under current policy\n            k_prime_values = k_grid[k_prime_indices] # Shape (N, S)\n            c_at_pi = z_grid[None,:] * (k_grid[:,None]**alpha) + (1-delta)*k_grid[:,None] - k_prime_values\n            \n            # Value function update\n            V_new = u(c_at_pi) + beta * expected_val_at_pi\n            \n            if np.max(np.abs(V_new - V_eval)) < eval_tol:\n                break\n            V_eval = V_new\n        V = V_eval\n\n        # 2. Policy Improvement Step\n        EV = V @ P.T # Expected value, shape (N,S)\n        \n        # RHS of Bellman equation for all choices\n        # utility_all_choices (N, S, N)\n        # EV.T (S, N) -> broadcast to (N, S, N)\n        rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n\n        # Find new policy and new value function\n        # argmax provides the tie-breaking rule (smallest index/k')\n        pi = np.argmax(rhs_bellman, axis=2)\n        \n        # Check for convergence\n        if np.array_equal(pi, pi_old):\n            policy_converged = True\n\n    if not policy_converged:\n        print(\"Warning: Policy function iteration did not converge.\")\n\n    # --- Post-convergence calculations ---\n    # Final value function from one VFI step on optimal policy\n    EV = V @ P.T\n    rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n    V_final = np.max(rhs_bellman, axis=2)\n\n    # Bellman Residual\n    residual = np.max(np.abs(V_final - V))\n\n    # Construct the transition matrix Q on the joint state space\n    NS = N * S\n    Q = np.zeros((NS, NS))\n    for i in range(N):\n        for s in range(S):\n            old_idx = s * N + i\n            policy_k_idx = pi[i, s]\n            for t in range(S):\n                new_idx = t * N + policy_k_idx\n                Q[old_idx, new_idx] = P[s, t]\n\n    # Stationary Distribution mu\n    mu = np.ones(NS) / NS\n    max_mu_iter = 5000\n    mu_tol = 1e-10\n    for _ in range(max_mu_iter):\n        mu_new = mu @ Q\n        if np.max(np.abs(mu_new - mu)) < mu_tol:\n            mu = mu_new\n            break\n        mu = mu_new\n    \n    mu_matrix = mu.reshape((S, N)).T # Shape (N, S)\n\n    # Expected Capital\n    expected_k = np.sum(mu_matrix * k_grid[:, np.newaxis])\n\n    # Expected Consumption\n    k_prime_indices_opt = pi\n    k_prime_values_opt = k_grid[k_prime_indices_opt]\n    consumption_opt = z_grid[None,:] * (k_grid[:,None]**alpha) + \\\n                      (1 - delta) * k_grid[:,None] - k_prime_values_opt\n    expected_c = np.sum(mu_matrix * consumption_opt)\n    \n    return [round(expected_k, 6), round(expected_c, 6), round(residual, 6)]\n\ndef solve():\n    test_cases = [\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.995, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.10, \"sigma\": 3.0,\n            \"z_grid\": np.array([1.0, 1.0]),\n            \"P\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n            \"N\": 180, \"k_min\": 0.001, \"k_max\": 4.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k_grid = np.linspace(case[\"k_min\"], case[\"k_max\"], case[\"N\"])\n        \n        result_triplet = solve_model(\n            case[\"alpha\"], case[\"beta\"], case[\"delta\"], case[\"sigma\"],\n            case[\"z_grid\"], case[\"P\"], k_grid\n        )\n        results.extend(result_triplet)\n    \n    # Format the output as per problem specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}