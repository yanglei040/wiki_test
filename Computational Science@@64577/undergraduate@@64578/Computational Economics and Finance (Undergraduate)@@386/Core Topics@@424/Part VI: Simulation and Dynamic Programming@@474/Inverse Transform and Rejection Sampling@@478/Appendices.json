{"hands_on_practices": [{"introduction": "The inverse transform method is a cornerstone of random variate generation, offering a direct path from a uniform random number to a sample from a desired distribution. This first practice challenges you to connect the underlying theory with a robust numerical implementation for the standard logistic distribution. By deriving and coding the inverse cumulative distribution function (CDF), you will not only build a functional sampler but also uncover a fundamental relationship between this distribution and the log-odds, or logit, function so prevalent in statistical modeling [@problem_id:2403666].", "id": "2403666", "problem": "You are asked to implement the inverse transform sampling method for the standard logistic distribution and to demonstrate its equivalence to taking the logarithm of the odds ratio (also known as the log-odds or logit). Your implementation must be a complete, runnable program.\n\nStart from the following foundational base:\n\n- If $U$ is a random variable distributed as a continuous uniform distribution on the unit interval, written $U \\sim \\mathrm{Uniform}(0,1)$, and $F$ is a continuous and strictly increasing cumulative distribution function, then the random variable $X = F^{-1}(U)$ has the cumulative distribution function $F$. This is the principle behind inverse transform sampling and follows from the definition of the cumulative distribution function and the monotonicity of $F$.\n- The standard logistic distribution has cumulative distribution function $F(x) = \\dfrac{1}{1 + e^{-x}}$ for all real $x \\in \\mathbb{R}$. Its mean is $0$ and its variance is $\\dfrac{\\pi^2}{3}$.\n\nYour tasks:\n\n1. Using only the base facts above, derive an explicit inverse transform $F^{-1}$ for the standard logistic cumulative distribution function and explain why this inverse equals the log-odds of the corresponding cumulative probability. The log-odds transformation for a probability $p \\in (0,1)$ is defined as $\\log\\!\\left(\\dfrac{p}{1-p}\\right)$, where $\\log$ denotes the natural logarithm.\n2. Implement numerically stable functions for:\n   - The logistic cumulative distribution function $F(x)$.\n   - The logistic inverse transform $F^{-1}(u)$ on $(0,1)$.\n3. Verify the inverse transform and the log-odds relationship across a small test suite that probes typical and edge-case values.\n\nTest suite and required checks:\n\n- Use the following probabilities $u$ to test the inverse transform: $u \\in \\{0.1, 0.5, 0.9, 10^{-12}, 1 - 10^{-12}\\}$. For each $u$ in this set, compute $x = F^{-1}(u)$ and verify numerically that $|F(x) - u| \\leq \\varepsilon$ with tolerance $\\varepsilon = 10^{-10}$. Record a boolean for each $u$ indicating whether the condition holds.\n- Use the following real values $x$ to test the log-odds relationship: $x \\in \\{-6.0, -1.0, 0.0, 2.5, 10.0\\}$. For each $x$ in this set, compute $u = F(x)$ and verify that $\\left|\\log\\!\\left(\\dfrac{u}{1-u}\\right) - x\\right| \\leq \\varepsilon$ with the same tolerance $\\varepsilon = 10^{-10}$. Record a boolean for each $x$ indicating whether the condition holds.\n- Use inverse transform sampling to generate $N = 100000$ independent and identically distributed (IID) draws from the standard logistic distribution by transforming $U_1, \\dots, U_N$ where $U_i \\sim \\mathrm{Uniform}(0,1)$ into $X_i = F^{-1}(U_i)$. Let $\\bar{X}_N$ be the sample mean and $S_N^2$ be the sample variance. Verify the following conditions with tolerances $\\delta_{\\mathrm{mean}} = 0.02$ and $\\delta_{\\mathrm{var}} = 0.05$:\n  - $|\\bar{X}_N - 0| \\leq \\delta_{\\mathrm{mean}}$,\n  - $\\left|S_N^2 - \\dfrac{\\pi^2}{3}\\right| \\leq \\delta_{\\mathrm{var}}$.\n  Record one boolean for the mean check and one boolean for the variance check.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true1,true2,...]\"). The order must be:\n  - The five booleans for the inverse-transform checks at $u \\in \\{0.1, 0.5, 0.9, 10^{-12}, 1 - 10^{-12}\\}$ (in that order),\n  - followed by the five booleans for the log-odds checks at $x \\in \\{-6.0, -1.0, 0.0, 2.5, 10.0\\}$ (in that order),\n  - followed by the two booleans for the Monte Carlo checks on the sample mean and variance (in that order).", "solution": "The problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded in established principles of probability theory and statistics, well-posed with clear objectives and constraints, and formulated in objective, unambiguous language. The required tasks are standard exercises in computational statistics. We will proceed with the solution.\n\nThe solution is organized into three parts: the analytical derivation of the inverse transform, a discussion of its numerically stable implementation, and the protocol for its numerical verification.\n\n### 1. Analytical Derivation\n\nThe core of the inverse transform sampling method rests on the inversion of the Cumulative Distribution Function (CDF).\n\n#### 1.1. Inverse of the Standard Logistic CDF\n\nThe standard logistic distribution is defined by its CDF, $F(x)$:\n$$F(x) = \\frac{1}{1 + e^{-x}}$$\nTo apply the inverse transform sampling method, we must find the inverse function, $x = F^{-1}(u)$, by solving for $x$ given a probability $u = F(x)$, where $u \\in (0, 1)$.\n\nWe begin with the equation for the CDF:\n$$u = \\frac{1}{1 + e^{-x}}$$\nWe perform algebraic manipulation to isolate $x$:\n$$1 + e^{-x} = \\frac{1}{u}$$\n$$e^{-x} = \\frac{1}{u} - 1$$\n$$e^{-x} = \\frac{1 - u}{u}$$\nTaking the natural logarithm of both sides yields:\n$$\\ln(e^{-x}) = \\ln\\left(\\frac{1 - u}{u}\\right)$$\n$$-x = \\ln\\left(\\frac{1 - u}{u}\\right)$$\nFinally, solving for $x$:\n$$x = -\\ln\\left(\\frac{1 - u}{u}\\right) = \\ln\\left(\\left(\\frac{1 - u}{u}\\right)^{-1}\\right) = \\ln\\left(\\frac{u}{1 - u}\\right)$$\nThus, the inverse function, or quantile function, is:\n$$F^{-1}(u) = \\ln\\left(\\frac{u}{1 - u}\\right)$$\n\n#### 1.2. Equivalence to the Log-Odds (Logit) Function\n\nThe log-odds, or logit transformation, of a probability $p$ is defined as $\\log\\left(\\frac{p}{1-p}\\right)$. By setting the probability $p$ to be the cumulative probability $u = F(x)$, the log-odds of $u$ becomes:\n$$\\text{logit}(u) = \\log\\left(\\frac{u}{1 - u}\\right)$$\nComparing this with our derived inverse function $F^{-1}(u)$, we see they are identical:\n$$x = F^{-1}(u) = \\log\\left(\\frac{u}{1 - u}\\right) = \\text{logit}(u)$$\nThis demonstrates a fundamental property of the logistic distribution: a random variable $X$ from a standard logistic distribution is equal to the log-odds of its own cumulative probability, $U=F(X)$. This relationship is central to its application in models such as logistic regression.\n\n### 2. Numerical Implementation Strategy\n\nCorrectness in numerical computation requires attention to potential instabilities such as overflow and loss of precision.\n\n#### 2.1. The CDF Function $F(x)$\n\nThe direct implementation of $F(x) = \\frac{1}{1 + e^{-x}}$ is prone to numerical overflow if $x$ is a large negative number, as $e^{-x}$ will become excessively large. A more stable implementation utilizes an alternative but algebraically equivalent form, $F(x) = \\frac{e^x}{1 + e^x}$, which is stable for negative $x$ as $e^x$ approaches $0$. A robust function should therefore branch based on the sign of $x$:\n- For $x \\ge 0$, use $F(x) = \\frac{1}{1 + e^{-x}}$. Here, $e^{-x}$ is between $0$ and $1$, posing no overflow risk.\n- For $x < 0$, use $F(x) = \\frac{e^x}{1 + e^x}$. Here, $e^{x}$ is between $0$ and $1$, similarly posing no overflow risk.\nThis ensures stability across the entire real line $\\mathbb{R}$.\n\n#### 2.2. The Inverse CDF Function $F^{-1}(u)$\n\nThe inverse CDF, $F^{-1}(u) = \\ln\\left(\\frac{u}{1 - u}\\right)$, can be implemented as $\\ln(u) - \\ln(1 - u)$. For values of $u$ extremely close to $1$, the term $1-u$ could suffer from catastrophic cancellation if machine precision is insufficient. However, for standard double-precision floating-point arithmetic and the test values provided (e.g., $u = 1 - 10^{-12}$), the calculation of $1-u$ is exact. Therefore, a direct implementation is sufficient and accurate for this problem.\n\n### 3. Verification Protocol\n\nThe problem specifies three sets of tests to verify the correctness of our derivation and implementation.\n\n1.  **Inverse Transform Check**: We will compute $x = F^{-1}(u)$ for a set of probabilities $u$ and verify that $F(x)$ recovers the original $u$ within a specified tolerance $\\varepsilon = 10^{-10}$. This confirms that our implemented functions are indeed inverses of each other.\n\n2.  **Log-Odds Relationship Check**: For a set of values $x$, we will compute $u=F(x)$ and verify that the log-odds of $u$, $\\log\\left(\\frac{u}{1-u}\\right)$, recovers the original $x$ within the tolerance $\\varepsilon = 10^{-10}$. This numerically validates the identity $x = \\text{logit}(F(x))$.\n\n3.  **Monte Carlo Simulation**: We will generate $N=100000$ random variates from the standard logistic distribution using inverse transform sampling on a uniform random number stream. We will then compute the sample mean $\\bar{X}_N$ and sample variance $S_N^2$. These statistics will be compared against the known theoretical mean ($\\mu=0$) and variance ($\\sigma^2 = \\pi^2/3$) of the standard logistic distribution. The adherence to specified tolerances ($\\delta_{\\mathrm{mean}} = 0.02$ and $\\delta_{\\mathrm{var}} = 0.05$) provides empirical evidence that the sampling method correctly generates variates from the target distribution. A fixed random seed will be used to ensure reproducibility of the results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and verifies the inverse transform sampling for the standard logistic distribution.\n    \"\"\"\n\n    # --- Task 2: Implement numerically stable functions ---\n\n    def logistic_cdf(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Numerically stable computation of the standard logistic CDF.\n        F(x) = 1 / (1 + exp(-x))\n        \"\"\"\n        x = np.asanyarray(x, dtype=float)\n        res = np.empty_like(x)\n        \n        # For non-negative x, the standard formula is stable.\n        pos_mask = (x >= 0)\n        res[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        \n        # For negative x, use the equivalent form F(x) = exp(x) / (1 + exp(x)) to avoid overflow.\n        neg_mask = ~pos_mask\n        exp_x_neg = np.exp(x[neg_mask])\n        res[neg_mask] = exp_x_neg / (1.0 + exp_x_neg)\n        \n        return res\n\n    def logistic_inv_cdf(u: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the inverse of the standard logistic CDF (quantile function or logit).\n        F_inv(u) = log(u / (1 - u))\n        \"\"\"\n        u = np.asanyarray(u, dtype=float)\n        # The form log(u) - log(1-u) is equivalent and standard.\n        return np.log(u) - np.log(1 - u)\n\n    # --- Task 3: Verify the implementation ---\n\n    results = []\n    \n    # Test Suite 1: Inverse transform check\n    # For each u, compute x = F_inv(u) and check |F(x) - u| <= epsilon\n    epsilon = 1e-10\n    u_vals = np.array([0.1, 0.5, 0.9, 1e-12, 1.0 - 1e-12])\n    \n    x_from_u = logistic_inv_cdf(u_vals)\n    u_recalc = logistic_cdf(x_from_u)\n    inv_transform_checks = np.abs(u_recalc - u_vals) <= epsilon\n    results.extend(inv_transform_checks)\n\n    # Test Suite 2: Log-odds relationship check\n    # For each x, compute u = F(x) and check |log(u/(1-u)) - x| <= epsilon\n    x_vals = np.array([-6.0, -1.0, 0.0, 2.5, 10.0])\n    \n    u_from_x = logistic_cdf(x_vals)\n    # The log-odds is identical to the inverse CDF function.\n    x_recalc = logistic_inv_cdf(u_from_x)\n    log_odds_checks = np.abs(x_recalc - x_vals) <= epsilon\n    results.extend(log_odds_checks)\n\n    # Test Suite 3: Monte Carlo simulation check\n    N = 100000\n    delta_mean = 0.02\n    delta_var = 0.05\n    \n    # Use a fixed seed for reproducibility.\n    np.random.seed(42)\n    \n    # Generate N uniform samples from (0, 1)\n    uniform_samples = np.random.uniform(0.0, 1.0, N)\n    \n    # Apply inverse transform sampling\n    logistic_samples = logistic_inv_cdf(uniform_samples)\n    \n    # Calculate sample mean and variance (with ddof=1 for unbiased estimator)\n    sample_mean = np.mean(logistic_samples)\n    sample_var = np.var(logistic_samples, ddof=1)\n    \n    # Theoretical properties of the standard logistic distribution\n    true_mean = 0.0\n    true_var = np.pi**2 / 3.0\n    \n    # Perform checks\n    mean_check = np.abs(sample_mean - true_mean) <= delta_mean\n    var_check = np.abs(sample_var - true_var) <= delta_var\n    results.extend([mean_check, var_check])\n\n    # Final output formatting\n    # Convert boolean values to lowercase strings \"true\" or \"false\"\n    results_str = [str(b).lower() for b in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"}, {"introduction": "While powerful, the inverse transform method is only applicable when the inverse CDF can be derived and computed. For more complex target distributions, we turn to more general techniques like rejection sampling. This exercise provides a complete, hands-on walkthrough of building a rejection sampler from the ground up for a non-standard target density involving the sine function [@problem_id:2403643]. You will even implement the specified pseudo-random number generator, gaining a deep appreciation for every moving part of this essential Monte Carlo algorithm.", "id": "2403643", "problem": "Construct a program that generates independent draws from a probability distribution on the interval $[0,4\\pi]$ with density proportional to $x \\mapsto \\frac{1}{1+\\lvert \\sin(x)\\rvert}$, and computes specified summary statistics for a set of parameter configurations. All angles are in radians. Your program must rely only on the definition of the target distribution and an explicitly specified source of independent Uniform on $[0,1)$ random numbers, without reading any external input.\n\nThe target density is defined by\n$$\nf(x) \\;=\\; c \\cdot \\frac{1}{1+\\lvert \\sin(x)\\rvert}, \\quad x \\in [0,4\\pi],\n$$\nwith $f(x)=0$ outside this interval, and where the normalization constant $c$ is determined uniquely by the requirement $\\int_0^{4\\pi} f(x)\\,dx = 1$.\n\nFor the purpose of generating candidate points and reporting acceptance rates, consider a proposal density $g$ that is uniform on $[0,4\\pi]$, that is,\n$$\ng(x) \\;=\\; \\frac{1}{4\\pi} \\quad \\text{for } x\\in[0,4\\pi], \\quad \\text{and } g(x)=0 \\text{ otherwise},\n$$\ntogether with a positive constant $M$ such that $f(x)\\le M\\,g(x)$ for all $x\\in[0,4\\pi]$.\n\nRandom number generation must use the following Linear Congruential Generator (LCG) to produce a reproducible sequence of Uniform on $[0,1)$ variables. Let the integer state be $(s_n)_{n\\ge 0}$ with\n$$\ns_{n+1} \\;=\\; (a\\, s_n + c_0) \\bmod m,\n$$\nwhere $a=1664525$, $c_0=1013904223$, and $m=2^{32}$. Each Uniform on $[0,1)$ variate is given by $u_n = s_n/m$. For each test case below, initialize $s_0$ to the specified seed value, and use the generated sequence in order.\n\nTest suite:\n- Test case $1$ (boundary-size sample): Use seed $s_0=314159265$, constant $M=\\frac{\\pi}{2}$, and produce $N=1$ accepted draw. Output the single accepted value $x_1$ rounded to $6$ decimal places.\n- Test case $2$ (moderate sample, exact $M$): Use seed $s_0=271828182$, constant $M=\\frac{\\pi}{2}$, and produce $N=1000$ accepted draws. Output two quantities rounded to $6$ decimal places: first, the overall acceptance rate (accepted draws divided by the total number of proposed points), and second, the sample mean of $x\\mapsto \\sin(x)$ over the accepted draws.\n- Test case $3$ (large sample, suboptimal $M$): Use seed $s_0=141421356$, constant $M=\\frac{\\pi}{2}+\\frac{1}{2}$, and produce $N=50000$ accepted draws. Output two quantities rounded to $6$ decimal places: first, the overall acceptance rate, and second, the empirical probability that an accepted draw lies in the interval $[\\pi,2\\pi]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[x_1,\\ \\text{acc\\_2},\\ \\overline{\\sin}_2,\\ \\text{acc\\_3},\\ p_{[\\pi,2\\pi],3}],\n$$\nwhere $x_1$ is from test case $1$, $\\text{acc\\_2}$ and $\\overline{\\sin}_2$ are from test case $2$, and $\\text{acc\\_3}$ and $p_{[\\pi,2\\pi],3}$ are from test case $3$. Every numeric item must be rounded to $6$ decimal places, and no spaces are permitted inside the brackets.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Target probability density function (PDF): $f(x) = c \\cdot \\frac{1}{1+\\lvert \\sin(x)\\rvert}$ for $x \\in [0,4\\pi]$, and $f(x)=0$ otherwise. The constant $c$ is defined by $\\int_0^{4\\pi} f(x)\\,dx = 1$.\n- Proposal probability density function: $g(x) = \\frac{1}{4\\pi}$ for $x \\in [0,4\\pi]$, and $g(x)=0$ otherwise.\n- Rejection sampling constant: $M$, a positive constant such that $f(x) \\le M\\,g(x)$ for all $x \\in [0,4\\pi]$.\n- Random number source: A Linear Congruential Generator (LCG) defined by $s_{n+1} = (a\\, s_n + c_0) \\bmod m$, with parameters $a=1664525$, $c_0=1013904223$, $m=2^{32}$. Uniform deviates are $u_n = s_n/m$.\n- Test Case 1: $s_0=314159265$, $M=\\frac{\\pi}{2}$, $N=1$. Output is the single accepted value $x_1$.\n- Test Case 2: $s_0=271828182$, $M=\\frac{\\pi}{2}$, $N=1000$. Outputs are the acceptance rate and the sample mean of $\\sin(x)$.\n- Test Case 3: $s_0=141421356$, $M=\\frac{\\pi}{2}+\\frac{1}{2}$, $N=50000$. Outputs are the acceptance rate and the empirical probability that an accepted draw is in $[\\pi,2\\pi]$.\n- Formatting: All numerical outputs must be rounded to $6$ decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem describes the application of rejection sampling, a standard and fundamental algorithm in computational statistics. The target function $x \\mapsto \\frac{1}{1+\\lvert \\sin(x)\\rvert}$ is non-negative and continuous on $[0, 4\\pi]$. Its integral over this domain is finite, ensuring a valid PDF can be constructed. The normalization constant $c$ is determined by finding this integral. The function $|\\sin(x)|$ has a period of $\\pi$, so the integral is $4 \\int_0^\\pi \\frac{dx}{1+\\sin(x)}$. Using the Weierstrass substitution $t = \\tan(x/2)$, this integral evaluates to $2$. Thus, $\\int_0^{4\\pi} \\frac{dx}{1+|\\sin(x)|} = 4 \\times 2 = 8$. The normalization constant is $c=1/8$, and the target PDF is $f(x) = \\frac{1}{8(1+|\\sin x|)}$ on the domain $[0, 4\\pi]$. The problem is scientifically sound.\n\n- **Well-Posedness:** The rejection sampling method requires a constant $M$ such that $f(x) \\le M g(x)$. This inequality is $\\frac{1}{8(1+|\\sin x|)} \\le M \\frac{1}{4\\pi}$, which simplifies to $M \\ge \\frac{\\pi}{2} \\frac{1}{1+|\\sin x|}$. The expression on the right is maximized when $|\\sin x|$ is minimized, i.e., $|\\sin x|=0$. Thus, we must have $M \\ge \\frac{\\pi}{2}$. The problem provides $M=\\frac{\\pi}{2}$ (the optimal value) and $M=\\frac{\\pi}{2}+\\frac{1}{2}$ (a valid suboptimal value), both of which satisfy the condition. The LCG parameters are standard and produce a deterministic, pseudo-random sequence, making the results reproducible. All required parameters ($s_0$, $M$, $N$) for each test case are explicitly provided. The problem is well-posed and self-contained.\n\n- **Objectivity:** The problem is stated using precise mathematical definitions and objective computational tasks. There are no subjective or ambiguous elements.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A reasoned solution will be provided.\n\n**Solution Derivation**\nThe core of this problem is to implement the rejection sampling algorithm. This method generates samples from a target distribution with density $f(x)$ by using a simpler proposal distribution with density $g(x)$. The method requires a constant $M$ such that the scaled proposal density $M g(x)$ serves as an envelope for $f(x)$, i.e., $f(x) \\le M g(x)$ for all $x$.\n\nThe algorithm to generate one sample is as follows:\n1.  Draw a candidate sample $Y$ from the proposal distribution $g(x)$.\n2.  Draw a random number $U$ from the uniform distribution on $[0,1]$.\n3.  If $U \\le \\frac{f(Y)}{M g(Y)}$, accept the candidate $Y$ as a sample from $f(x)$. Otherwise, reject $Y$ and return to step 1.\n\nFor this specific problem:\n- The proposal distribution $g(x)$ is uniform on $[0, 4\\pi]$. A candidate $Y$ can be generated by drawing a uniform variate $U_1 \\sim U(0,1)$ and setting $Y = 4\\pi \\cdot U_1$.\n- The target density is $f(x) = \\frac{1}{8(1+|\\sin x|)}$.\n- The proposal density is $g(x) = \\frac{1}{4\\pi}$.\n- The acceptance condition check involves the ratio:\n$$\n\\frac{f(Y)}{M g(Y)} = \\frac{\\frac{1}{8(1+|\\sin Y|)}}{M \\frac{1}{4\\pi}} = \\frac{4\\pi}{8M} \\cdot \\frac{1}{1+|\\sin Y|} = \\frac{\\pi}{2M} \\cdot \\frac{1}{1+|\\sin Y|}\n$$\nThe sequence of uniform variates required for generating candidates and for the acceptance check must be produced by the specified LCG: $s_{n+1} = (1664525 \\cdot s_n + 1013904223) \\pmod{2^{32}}$, with $u_n = s_n / 2^{32}$.\n\nFor each test case, the procedure is:\n1.  Initialize the LCG with the given seed $s_0$.\n2.  Initialize an empty list for accepted samples and set a proposal counter to $0$.\n3.  Loop until $N$ samples are collected:\n    a. Increment the proposal counter.\n    b. Generate $U_1$ from the LCG to create a candidate $Y = 4\\pi \\cdot U_1$.\n    c. Generate $U_2$ from the LCG for the acceptance test.\n    d. Calculate the threshold $T = \\frac{\\pi}{2M(1+|\\sin Y|)}$.\n    e. If $U_2 \\le T$, add $Y$ to the list of accepted samples.\n4.  After collecting $N$ samples, compute the required statistics.\n\nThe statistics for each case are:\n- Case 1: The value of the single sample, $x_1$.\n- Case 2: The acceptance rate, defined as $\\frac{N}{\\text{total proposals}}$, and the sample mean $\\frac{1}{N} \\sum_{i=1}^{N} \\sin(x_i)$.\n- Case 3: The acceptance rate and the empirical probability $P(\\pi \\le X \\le 2\\pi)$, calculated as $\\frac{1}{N} \\times (\\text{count of samples } x_i \\text{ such that } \\pi \\le x_i \\le 2\\pi)$.\n\nAll final numerical results must be formatted to six decimal places. The implementation will follow this logic precisely to produce the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing rejection sampling for the given\n    distribution and computing the required statistics for three test cases.\n    \"\"\"\n    \n    # LCG parameters are defined as specified in the problem.\n    A_LCG = 1664525\n    C0_LCG = 1013904223\n    M_LCG = 2**32\n\n    def lcg_generator(seed):\n        \"\"\"\n        Creates a generator that yields a sequence of U(0,1) random numbers\n        using the specified Linear Congruential Generator (LCG).\n        \"\"\"\n        state = seed\n        while True:\n            state = (A_LCG * state + C0_LCG) % M_LCG\n            yield state / M_LCG\n\n    def rejection_sampler(lcg, M, N):\n        \"\"\"\n        Generates N samples from the target distribution using rejection sampling.\n\n        Args:\n            lcg (generator): The random number generator.\n            M (float): The constant for the rejection sampling envelope.\n            N (int): The number of samples to generate.\n\n        Returns:\n            tuple: A tuple containing a list of accepted samples and the total\n                   number of proposals made.\n        \"\"\"\n        accepted_samples = []\n        total_proposals = 0\n        \n        # Pre-compute the constant factor for the acceptance threshold calculation\n        acceptance_factor = np.pi / (2.0 * M)\n\n        while len(accepted_samples) < N:\n            total_proposals += 1\n            \n            # Step 1: Generate candidate Y from proposal g(x) = U[0, 4*pi]\n            u1 = next(lcg)\n            y_candidate = u1 * 4.0 * np.pi\n            \n            # Step 2: Generate U for acceptance check from U[0, 1]\n            u2 = next(lcg)\n            \n            # Step 3: Calculate acceptance threshold and check condition\n            # T(y) = f(y) / (M*g(y)) = (pi/(2*M)) * 1/(1+|sin(y)|)\n            threshold = acceptance_factor / (1.0 + np.abs(np.sin(y_candidate)))\n            \n            if u2 <= threshold:\n                accepted_samples.append(y_candidate)\n                \n        return accepted_samples, total_proposals\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, M, N)\n        (314159265, np.pi / 2.0, 1),\n        (271828182, np.pi / 2.0, 1000),\n        (141421356, np.pi / 2.0 + 0.5, 50000),\n    ]\n\n    all_results = []\n\n    # --- Test Case 1 ---\n    seed1, M1, N1 = test_cases[0]\n    lcg1 = lcg_generator(seed1)\n    samples1, _ = rejection_sampler(lcg1, M1, N1)\n    x1 = samples1[0]\n    all_results.append(x1)\n\n    # --- Test Case 2 ---\n    seed2, M2, N2 = test_cases[1]\n    lcg2 = lcg_generator(seed2)\n    samples2, total_proposals2 = rejection_sampler(lcg2, M2, N2)\n    \n    # Calculate acceptance rate\n    acc_2 = N2 / total_proposals2\n    all_results.append(acc_2)\n    \n    # Calculate sample mean of sin(x)\n    mean_sin_2 = np.mean([np.sin(x) for x in samples2])\n    all_results.append(mean_sin_2)\n\n    # --- Test Case 3 ---\n    seed3, M3, N3 = test_cases[2]\n    lcg3 = lcg_generator(seed3)\n    samples3, total_proposals3 = rejection_sampler(lcg3, M3, N3)\n    \n    # Calculate acceptance rate\n    acc_3 = N3 / total_proposals3\n    all_results.append(acc_3)\n    \n    # Calculate empirical probability of x in [pi, 2*pi]\n    count_in_interval = sum(1 for x in samples3 if np.pi <= x <= 2.0 * np.pi)\n    prob_3 = count_in_interval / N3\n    all_results.append(prob_3)\n    \n    # Format all results to 6 decimal places and join them into the final string\n    formatted_results = [f'{r:.6f}' for r in all_results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "A functional sampler is a great start, but in computational finance and economics, efficiency is paramount. This practice moves beyond basic implementation to the critical task of a sampler's design and optimization. You will analytically determine how to best configure a proposal distribution to maximize the acceptance rate when sampling from a standard normal target [@problem_id:2403626]. This exercise sharpens your mathematical toolkit and instills the key principle that a well-chosen proposal distribution, one that closely mimics the target, is the secret to an efficient simulation.", "id": "2403626", "problem": "In a Monte Carlo valuation of a derivative driven by a standard normal innovation, suppose the target density is the standard normal $f(x) = \\mathcal{N}(0, 1)$ on $\\mathbb{R}$, and that a rejection sampler is employed with a normal proposal $g(x) = \\mathcal{N}(\\mu_{p}, \\sigma_{p}^{2})$, where $\\sigma_{p}^{2} > 1$ is fixed and $\\mu_{p} \\in \\mathbb{R}$ is a free design parameter. The proposal draws are obtained from $g$ by any valid generator (for instance, inverse transform sampling applied to the cumulative distribution function of the normal distribution), and the rejection sampler uses the smallest admissible envelope constant $M(\\mu_{p})$ satisfying $f(x) \\leq M(\\mu_{p})\\, g(x)$ for all $x \\in \\mathbb{R}$. Define the sampling efficiency as the asymptotic acceptance probability, namely the expected acceptance probability under $g$ when using $M(\\mu_{p})$.\n\nDetermine the value of $\\mu_{p}$ that maximizes this efficiency subject to the stated constraints. Express your answer as a single real number. No rounding is required.", "solution": "The problem requires finding the value of the parameter $\\mu_{p}$ that maximizes the efficiency of a rejection sampler. The target probability density function (PDF) is the standard normal distribution, $f(x) = \\mathcal{N}(0, 1)$, and the proposal PDF is a normal distribution, $g(x) = \\mathcal{N}(\\mu_{p}, \\sigma_{p}^{2})$, with the given constraint that $\\sigma_{p}^{2} > 1$.\n\nFirst, we must formalize the problem. The PDFs are given by their explicit forms:\n$$ f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) $$\n$$ g(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{p}} \\exp\\left(-\\frac{(x-\\mu_{p})^2}{2\\sigma_{p}^2}\\right) $$\n\nThe rejection sampler requires an envelope constant $M(\\mu_{p})$ such that $f(x) \\leq M(\\mu_{p}) g(x)$ for all $x \\in \\mathbb{R}$. The problem specifies using the smallest admissible constant, which is given by the supremum of the ratio of the two densities over the entire domain of $x$:\n$$ M(\\mu_{p}) = \\sup_{x \\in \\mathbb{R}} \\frac{f(x)}{g(x)} $$\n\nLet us compute the ratio $\\frac{f(x)}{g(x)}$:\n$$ \\frac{f(x)}{g(x)} = \\frac{\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{\\sqrt{2\\pi}\\sigma_{p}} \\exp\\left(-\\frac{(x-\\mu_{p})^2}{2\\sigma_{p}^2}\\right)} = \\sigma_{p} \\exp\\left(-\\frac{x^2}{2} + \\frac{(x-\\mu_{p})^2}{2\\sigma_{p}^2}\\right) $$\nTo find the supremum, we must maximize the argument of the exponential function with respect to $x$. Let this argument be denoted by $h(x)$:\n$$ h(x) = -\\frac{x^2}{2} + \\frac{x^2 - 2\\mu_{p}x + \\mu_{p}^2}{2\\sigma_{p}^2} $$\nCombining the terms over a common denominator gives:\n$$ h(x) = \\frac{-\\sigma_{p}^2 x^2 + x^2 - 2\\mu_{p}x + \\mu_{p}^2}{2\\sigma_{p}^2} = \\frac{(1-\\sigma_{p}^2)x^2 - 2\\mu_{p}x + \\mu_{p}^2}{2\\sigma_{p}^2} $$\nThis function $h(x)$ is a quadratic in the variable $x$. The coefficient of the $x^2$ term is $\\frac{1-\\sigma_{p}^2}{2\\sigma_{p}^2}$. According to the problem statement, $\\sigma_{p}^{2} > 1$, which implies that $1-\\sigma_{p}^2 < 0$. Therefore, the quadratic represents a parabola opening downwards, and it possesses a unique maximum value.\n\nTo find the value of $x$ that maximizes $h(x)$, we compute the derivative of $h(x)$ with respect to $x$ and set it to zero:\n$$ \\frac{dh}{dx} = \\frac{2(1-\\sigma_{p}^2)x - 2\\mu_{p}}{2\\sigma_{p}^2} = \\frac{(1-\\sigma_{p}^2)x - \\mu_{p}}{\\sigma_{p}^2} $$\nSetting $\\frac{dh}{dx} = 0$ yields the location of the maximum:\n$$ (1-\\sigma_{p}^2)x_{\\text{max}} - \\mu_{p} = 0 \\implies x_{\\text{max}} = \\frac{\\mu_{p}}{1-\\sigma_{p}^2} $$\nThe maximum value of $h(x)$ is obtained by substituting $x_{\\text{max}}$ back into the expression for $h(x)$:\n$$ h(x_{\\text{max}}) = \\frac{(1-\\sigma_{p}^2)\\left(\\frac{\\mu_{p}}{1-\\sigma_{p}^2}\\right)^2 - 2\\mu_{p}\\left(\\frac{\\mu_{p}}{1-\\sigma_{p}^2}\\right) + \\mu_{p}^2}{2\\sigma_{p}^2} $$\n$$ h(x_{\\text{max}}) = \\frac{\\frac{\\mu_{p}^2}{1-\\sigma_{p}^2} - \\frac{2\\mu_{p}^2}{1-\\sigma_{p}^2} + \\mu_{p}^2}{2\\sigma_{p}^2} = \\frac{-\\frac{\\mu_{p}^2}{1-\\sigma_{p}^2} + \\mu_{p}^2}{2\\sigma_{p}^2} $$\n$$ h(x_{\\text{max}}) = \\frac{\\mu_{p}^2 \\left(1 - \\frac{1}{1-\\sigma_{p}^2}\\right)}{2\\sigma_{p}^2} = \\frac{\\mu_{p}^2 \\left(\\frac{1-\\sigma_{p}^2 - 1}{1-\\sigma_{p}^2}\\right)}{2\\sigma_{p}^2} = \\frac{\\mu_{p}^2 \\left(\\frac{-\\sigma_{p}^2}{1-\\sigma_{p}^2}\\right)}{2\\sigma_{p}^2} = \\frac{-\\mu_{p}^2}{2(1-\\sigma_{p}^2)} = \\frac{\\mu_{p}^2}{2(\\sigma_{p}^2 - 1)} $$\nThus, the smallest envelope constant $M(\\mu_p)$ is:\n$$ M(\\mu_{p}) = \\sigma_{p} \\exp\\left( h(x_{\\text{max}}) \\right) = \\sigma_{p} \\exp\\left(\\frac{\\mu_{p}^2}{2(\\sigma_{p}^2 - 1)}\\right) $$\n\nThe problem defines sampling efficiency, $\\eta(\\mu_p)$, as the asymptotic acceptance probability. For rejection sampling, this efficiency is the reciprocal of the envelope constant $M$:\n$$ \\eta(\\mu_{p}) = \\frac{1}{M(\\mu_{p})} = \\frac{1}{\\sigma_{p}} \\exp\\left(-\\frac{\\mu_{p}^2}{2(\\sigma_{p}^2 - 1)}\\right) $$\nThis relation is a standard result, derived from the fact that the overall probability of acceptance is $\\int_{-\\infty}^{\\infty} \\frac{f(x)}{M g(x)} g(x) dx = \\frac{1}{M} \\int_{-\\infty}^{\\infty} f(x) dx = \\frac{1}{M}$, since $f(x)$ is a normalized PDF.\n\nOur objective is to find the value of $\\mu_{p}$ that maximizes the efficiency $\\eta(\\mu_{p})$. Maximizing $\\eta(\\mu_{p})$ is equivalent to minimizing its denominator, $M(\\mu_{p})$. Let us examine the expression for $M(\\mu_{p})$:\n$$ M(\\mu_{p}) = \\sigma_{p} \\exp\\left(\\frac{\\mu_{p}^2}{2(\\sigma_{p}^2 - 1)}\\right) $$\nIn this expression, $\\sigma_{p}$ is a fixed positive constant. The exponential function $\\exp(y)$ is a strictly increasing function of its argument $y$. Therefore, to minimize $M(\\mu_{p})$, we must minimize its exponent:\n$$ \\frac{\\mu_{p}^2}{2(\\sigma_{p}^2 - 1)} $$\nGiven the constraint $\\sigma_{p}^2 > 1$, the denominator $2(\\sigma_{p}^2 - 1)$ is a positive constant. The numerator is $\\mu_{p}^2$. The expression is a simple quadratic function of $\\mu_{p}$, $C \\cdot \\mu_p^2$ where $C > 0$. This function has a global minimum at the vertex of the parabola. The minimum value of $\\mu_{p}^2$ is $0$, which occurs if and only if $\\mu_{p} = 0$.\n\nTherefore, the function $M(\\mu_{p})$ is minimized when $\\mu_{p}=0$. As a direct consequence, the efficiency $\\eta(\\mu_p)$ is maximized when $\\mu_{p}=0$. This result is intuitively correct: to most efficiently sample a target distribution, the proposal distribution should be centered at the same location as the target's mode, which for the standard normal distribution is at zero.", "answer": "$$\\boxed{0}$$"}]}