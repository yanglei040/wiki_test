{"hands_on_practices": [{"introduction": "The best way to understand Monte Carlo integration is to apply it directly. This first practice focuses on the core mechanic: approximating the value of a complex, multi-dimensional integral by averaging the integrand's value at a few randomly chosen points. By working through this manual calculation [@problem_id:1376816], you will gain a concrete understanding of how the sum of sampled values, scaled by the integration volume, provides a powerful estimate for quantities that are often difficult to compute analytically.", "id": "1376816", "problem": "A materials scientist is studying a new alloy created in a cubical mold of side length 1 meter. The coordinate system is aligned with the mold, such that it occupies the region defined by $0 \\le x \\le 1$, $0 \\le y \\le 1$, and $0 \\le z \\le 1$. The concentration of a special hardening agent, $C$, is found to be non-zero only in a specific sub-region of the mold defined by the inequalities $0 \\le z \\le y \\le x \\le 1$. Within this sub-region, the concentration at a point $(x,y,z)$ is described by the function $C(x,y,z) = k x y z$, where $k$ is a constant. Outside this region, the concentration is zero. The total mass of the agent in the mold is given by the integral of the concentration function over the entire volume of the 1-meter-cubed mold.\n\nTo estimate this total mass, an automated measurement system probes the concentration at a set of $N=5$ sample points, which are assumed to be representative of a uniform random sampling within the unit cube. The coordinates of these five points are:\n$P_1 = (0.8, 0.7, 0.2)$\n$P_2 = (0.9, 0.5, 0.6)$\n$P_3 = (0.6, 0.8, 0.3)$\n$P_4 = (0.5, 0.4, 0.3)$\n$P_5 = (0.7, 0.9, 0.8)$\n\nGiven the concentration constant $k = 4.8 \\text{ kg/m}^6$, calculate the numerical estimate for the total mass of the hardening agent in the mold based on this set of five sample points. Express your answer in kilograms (kg) and round to three significant figures.\n\n", "solution": "The total mass is the volume integral of the concentration over the unit cube:\n$$\nM=\\iiint_{[0,1]^{3}} C(x,y,z)\\,dV.\n$$\nWith uniform random sampling over a domain of volume $V=1$, the Monte Carlo estimator with $N$ samples $\\{P_{i}\\}_{i=1}^{N}$ is\n$$\n\\widehat{M}=\\frac{V}{N}\\sum_{i=1}^{N} C(P_{i})=\\frac{1}{N}\\sum_{i=1}^{N} C(P_{i}).\n$$\nHere $C(x,y,z)=k\\,x y z$ if $0 \\le z \\le y \\le x \\le 1$ and $C=0$ otherwise. Evaluate the indicator $0 \\le z \\le y \\le x \\le 1$ for each sample:\n- $P_{1}=(0.8,0.7,0.2)$: $0.2 \\le 0.7 \\le 0.8 \\le 1$ is true, so contributes $k\\cdot 0.8\\cdot 0.7\\cdot 0.2=0.112\\,k$.\n- $P_{2}=(0.9,0.5,0.6)$: $0.6 \\le 0.5$ is false, contributes $0$.\n- $P_{3}=(0.6,0.8,0.3)$: $0.8 \\le 0.6$ is false, contributes $0$.\n- $P_{4}=(0.5,0.4,0.3)$: $0.3 \\le 0.4 \\le 0.5 \\le 1$ is true, contributes $k\\cdot 0.5\\cdot 0.4\\cdot 0.3=0.06\\,k$.\n- $P_{5}=(0.7,0.9,0.8)$: $0.9 \\le 0.7$ is false, contributes $0$.\nTherefore\n$$\n\\sum_{i=1}^{5} C(P_{i})=(0.112+0.06)\\,k=0.172\\,k,\n$$\nand\n$$\n\\widehat{M}=\\frac{1}{5}\\cdot 0.172\\,k=0.0344\\,k.\n$$\nSubstituting $k=4.8$ gives\n$$\n\\widehat{M}=0.0344\\times 4.8=0.16512,\n$$\nwhich, rounded to three significant figures, is $0.165$ kilograms.", "answer": "$$\\boxed{0.165}$$"}, {"introduction": "While the basic Monte Carlo method is robust, its efficiency can often be dramatically improved. This practice explores a powerful variance reduction technique known as importance sampling, which involves sampling from a distribution that mimics the integrand to focus computational effort where it matters most. By analyzing how the choice of sampling distribution affects the estimator's variance [@problem_id:2414609], you will learn the critical lesson that a well-chosen strategy can yield faster, more accurate results, while a poor choice can be counterproductive.", "id": "2414609", "problem": "Consider the integral of a nonnegative integrand over a finite interval. Let the target integral be\n$$\nI = \\int_{0}^{1} f(x)\\,dx,\n$$\nwith $f(x) = x^{m}$ for a fixed exponent $m$ satisfying $m > -1$. Define the crude Monte Carlo estimator as the random variable $Y_{\\mathrm{c}} = f(U)$ with $U \\sim \\mathrm{Uniform}(0,1)$, and define the importance sampling estimator with a probability density function $p(x)$ on $[0,1]$ as $Y_{\\mathrm{is}} = f(X)/p(X)$ with $X \\sim p$. The per-sample variances of these unbiased estimators are, by definition,\n$$\n\\mathrm{Var}(Y_{\\mathrm{c}}) = \\mathbb{E}[Y_{\\mathrm{c}}^{2}] - I^{2}, \\qquad \\mathrm{Var}(Y_{\\mathrm{is}}) = \\mathbb{E}[Y_{\\mathrm{is}}^{2}] - I^{2}.\n$$\nFor the importance sampling density, consider the Beta distribution on $[0,1]$ with parameters $\\alpha > 0$ and $\\beta > 0$,\n$$\np(x) = \\frac{x^{\\alpha - 1}(1 - x)^{\\beta - 1}}{B(\\alpha,\\beta)},\n$$\nwhere $B(\\alpha,\\beta)$ is the Beta function. Assume parameter choices that make all quantities finite; in particular, assume $\\beta < 2$ and $\\alpha < 2m + 2$ to ensure that $\\mathbb{E}[Y_{\\mathrm{is}}^{2}]$ exists.\n\nFor each parameter set $(m,\\alpha,\\beta)$ in the following test suite, compute the ratio\n$$\nr = \\frac{\\mathrm{Var}(Y_{\\mathrm{is}})}{\\mathrm{Var}(Y_{\\mathrm{c}})},\n$$\nand produce the results in the specified format.\n\nTest suite (each triple is $(m,\\alpha,\\beta)$):\n\n- Case $1$: $(2,\\tfrac{1}{2},\\tfrac{3}{2})$.\n- Case $2$: $(2,1,1)$.\n- Case $3$: $(2,\\tfrac{5}{2},1)$.\n- Case $4$: $(2,3,1)$.\n\nFinal Output Format: Your program should produce a single line of output containing the results for the cases, in the order listed above, as a comma-separated list enclosed in square brackets (for example, $[r_{1},r_{2}]$). Each number must be rounded to $6$ decimal places. No additional text should be printed.", "solution": "The problem requires the computation of the ratio $r = \\mathrm{Var}(Y_{\\mathrm{is}})/\\mathrm{Var}(Y_{\\mathrm{c}})$, where $\\mathrm{Var}(Y_{\\mathrm{c}})$ is the variance of the crude Monte Carlo estimator and $\\mathrm{Var}(Y_{\\mathrm{is}})$ is the variance of the importance sampling estimator for the integral $I = \\int_{0}^{1} f(x)\\,dx$ with $f(x) = x^{m}$. The analysis proceeds by deriving analytical expressions for each variance.\n\nFirst, we calculate the exact value of the integral $I$. For $m > -1$:\n$$\nI = \\int_{0}^{1} x^{m} \\, dx = \\left[ \\frac{x^{m+1}}{m+1} \\right]_{0}^{1} = \\frac{1}{m+1}\n$$\n\nNext, we derive the variance of the crude Monte Carlo estimator, $Y_{\\mathrm{c}} = f(U) = U^m$, where $U \\sim \\mathrm{Uniform}(0,1)$. The variance is defined as $\\mathrm{Var}(Y_{\\mathrm{c}}) = \\mathbb{E}[Y_{\\mathrm{c}}^{2}] - I^{2}$. The second moment $\\mathbb{E}[Y_{\\mathrm{c}}^{2}]$ is:\n$$\n\\mathbb{E}[Y_{\\mathrm{c}}^{2}] = \\mathbb{E}[(U^m)^2] = \\mathbb{E}[U^{2m}] = \\int_{0}^{1} u^{2m} \\cdot 1 \\, du = \\frac{1}{2m+1}\n$$\nThis requires $2m+1 > 0$, or $m > -1/2$. The given condition $m > -1$ and the test case $m=2$ satisfy this. The variance is therefore:\n$$\n\\mathrm{Var}(Y_{\\mathrm{c}}) = \\frac{1}{2m+1} - \\left(\\frac{1}{m+1}\\right)^2 = \\frac{(m+1)^2 - (2m+1)}{(2m+1)(m+1)^2} = \\frac{m^2 + 2m + 1 - 2m - 1}{(2m+1)(m+1)^2} = \\frac{m^2}{(2m+1)(m+1)^2}\n$$\n\nNow, we derive the variance of the importance sampling estimator, $Y_{\\mathrm{is}} = f(X)/p(X)$, where $X$ is drawn from the Beta distribution with density $p(x) = x^{\\alpha - 1}(1 - x)^{\\beta - 1} / B(\\alpha, \\beta)$. The variance is $\\mathrm{Var}(Y_{\\mathrm{is}}) = \\mathbb{E}[Y_{\\mathrm{is}}^{2}] - I^{2}$. The second moment $\\mathbb{E}[Y_{\\mathrm{is}}^{2}]$ is calculated as:\n$$\n\\mathbb{E}[Y_{\\mathrm{is}}^{2}] = \\int_{0}^{1} \\left(\\frac{f(x)}{p(x)}\\right)^2 p(x) \\, dx = \\int_{0}^{1} \\frac{f(x)^2}{p(x)} \\, dx\n$$\nSubstituting $f(x) = x^m$ and the density $p(x)$:\n$$\n\\mathbb{E}[Y_{\\mathrm{is}}^{2}] = \\int_{0}^{1} \\frac{(x^m)^2}{\\frac{x^{\\alpha - 1}(1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}} \\, dx = B(\\alpha, \\beta) \\int_{0}^{1} x^{2m - \\alpha + 1} (1 - x)^{1 - \\beta} \\, dx\n$$\nThe integral is a Beta function integral, $B(a,b) = \\int_0^1 t^{a-1}(1-t)^{b-1}dt$. We rewrite the integrand's exponents accordingly:\n$$\n\\mathbb{E}[Y_{\\mathrm{is}}^{2}] = B(\\alpha, \\beta) \\int_{0}^{1} x^{(2m - \\alpha + 2) - 1} (1 - x)^{(2 - \\beta) - 1} \\, dx\n$$\nThis integral is equal to $B(2m - \\alpha + 2, 2 - \\beta)$. The existence of this integral is guaranteed by the problem constraints $\\alpha < 2m + 2$ and $\\beta < 2$. Thus, the second moment is:\n$$\n\\mathbb{E}[Y_{\\mathrm{is}}^{2}] = B(\\alpha, \\beta) B(2m - \\alpha + 2, 2 - \\beta)\n$$\nThe variance for the importance sampling estimator is:\n$$\n\\mathrm{Var}(Y_{\\mathrm{is}}) = B(\\alpha, \\beta) B(2m - \\alpha + 2, 2 - \\beta) - \\frac{1}{(m+1)^2}\n$$\n\nFinally, the ratio $r$ is given by the general formula:\n$$\nr = \\frac{\\mathrm{Var}(Y_{\\mathrm{is}})}{\\mathrm{Var}(Y_{\\mathrm{c}})} = \\frac{B(\\alpha, \\beta) B(2m - \\alpha + 2, 2 - \\beta) - \\frac{1}{(m+1)^2}}{\\frac{m^2}{(2m+1)(m+1)^2}}\n$$\n\nFor all test cases, the exponent is $m=2$. We can specialize the formulas:\n$I = \\frac{1}{2+1} = \\frac{1}{3}$.\n$\\mathrm{Var}(Y_{\\mathrm{c}}) = \\frac{2^2}{(2(2)+1)(2+1)^2} = \\frac{4}{5 \\cdot 9} = \\frac{4}{45}$.\nThe ratio $r$ for $m=2$ becomes:\n$$\nr = \\frac{B(\\alpha, \\beta) B(6 - \\alpha, 2 - \\beta) - \\frac{1}{9}}{\\frac{4}{45}} = \\frac{45}{4} \\left( B(\\alpha, \\beta) B(6 - \\alpha, 2 - \\beta) - \\frac{1}{9} \\right)\n$$\n\nNow we compute the ratio for each test case.\n\nCase $1$: $(m,\\alpha,\\beta) = (2, \\frac{1}{2}, \\frac{3}{2})$.\nWe have $\\alpha = 0.5$ and $\\beta = 1.5$.\n$r = \\frac{45}{4} \\left( B(0.5, 1.5) B(5.5, 0.5) - \\frac{1}{9} \\right)$.\nUsing the relation $B(x,y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$:\n$B(0.5, 1.5) = B(\\frac{1}{2}, \\frac{3}{2}) = \\frac{\\Gamma(1/2)\\Gamma(3/2)}{\\Gamma(2)} = \\frac{\\sqrt{\\pi} \\cdot (1/2)\\sqrt{\\pi}}{1!} = \\frac{\\pi}{2}$.\n$B(5.5, 0.5) = B(\\frac{11}{2}, \\frac{1}{2}) = \\frac{\\Gamma(11/2)\\Gamma(1/2)}{\\Gamma(6)} = \\frac{(945/32)\\sqrt{\\pi} \\cdot \\sqrt{\\pi}}{5!} = \\frac{945\\pi}{32 \\cdot 120} = \\frac{63\\pi}{256}$.\n$r = \\frac{45}{4} \\left( \\frac{\\pi}{2} \\cdot \\frac{63\\pi}{256} - \\frac{1}{9} \\right) = \\frac{45}{4} \\frac{63\\pi^2}{512} - \\frac{45}{36} = \\frac{2835\\pi^2}{2048} - \\frac{5}{4} = \\frac{2835\\pi^2 - 2560}{2048}$.\nNumerically, this is approximately $12.408172$.\n\nCase $2$: $(m,\\alpha,\\beta) = (2, 1, 1)$.\nHere, $\\alpha=1, \\beta=1$, so $p(x) = \\frac{x^0(1-x)^0}{B(1,1)} = 1$, which is the uniform distribution. The importance sampling scheme is identical to the crude Monte Carlo scheme. Therefore, $\\mathrm{Var}(Y_{\\mathrm{is}}) = \\mathrm{Var}(Y_{\\mathrm{c}})$ and the ratio $r$ must be $1$.\nVerifying with the formula:\n$r = \\frac{45}{4} \\left( B(1, 1) B(5, 1) - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( 1 \\cdot \\frac{1}{5} - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( \\frac{1}{5} - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( \\frac{9-5}{45} \\right) = \\frac{45}{4} \\frac{4}{45} = 1$.\n\nCase $3$: $(m,\\alpha,\\beta) = (2, \\frac{5}{2}, 1)$.\nWe have $\\alpha = 2.5$ and $\\beta = 1$. The formula for $B(a,1)$ is $1/a$.\n$r = \\frac{45}{4} \\left( B(2.5, 1) B(3.5, 1) - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( \\frac{1}{2.5} \\cdot \\frac{1}{3.5} - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( \\frac{2}{5} \\cdot \\frac{2}{7} - \\frac{1}{9} \\right)$.\n$r = \\frac{45}{4} \\left( \\frac{4}{35} - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( \\frac{36 - 35}{315} \\right) = \\frac{45}{4 \\cdot 315} = \\frac{1}{4 \\cdot 7} = \\frac{1}{28}$.\nNumerically, this is approximately $0.035714$.\n\nCase $4$: $(m,\\alpha,\\beta) = (2, 3, 1)$.\nHere, $\\alpha = 3$ and $\\beta = 1$. The sampling density is $p(x) = \\frac{x^{3-1}}{B(3,1)} = \\frac{x^2}{1/3} = 3x^2$. The integrand is $f(x)=x^2$. Since $p(x) \\propto f(x)$, the estimator $Y_{\\mathrm{is}} = \\frac{f(X)}{p(X)} = \\frac{X^2}{3X^2} = \\frac{1}{3}$ is a constant. The variance of a constant is zero. Thus, $r=0$.\nVerifying with the formula:\n$r = \\frac{45}{4} \\left( B(3, 1) B(3, 1) - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( \\frac{1}{3} \\cdot \\frac{1}{3} - \\frac{1}{9} \\right) = \\frac{45}{4} \\left( \\frac{1}{9} - \\frac{1}{9} \\right) = 0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import beta\n\ndef solve():\n    \"\"\"\n    Computes the ratio of variances for importance sampling vs. crude Monte Carlo\n    for the integral of f(x) = x^m from 0 to 1.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2, 0.5, 1.5),  # Case 1\n        (2, 1, 1),      # Case 2\n        (2, 2.5, 1),    # Case 3\n        (2, 3, 1)       # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        m, alpha, beta_p = case\n\n        # General formula for the variance of the crude Monte Carlo estimator.\n        # Var(Y_c) = E[f(U)^2] - I^2 = integral(x^(2m))dx - (integral(x^m)dx)^2\n        #          = 1/(2m+1) - 1/((m+1)^2)\n        #          = m^2 / ((2m+1)*(m+1)^2)\n        var_c = m**2 / ((2 * m + 1) * (m + 1)**2)\n\n        # General formula for the variance of the importance sampling estimator.\n        # Var(Y_is) = E[(f(X)/p(X))^2] - I^2 = integral(f(x)^2/p(x))dx - I^2\n        #           = B(alpha, beta) * B(2m - alpha + 2, 2 - beta) - 1/((m+1)^2)\n        # The conditions m > -1, alpha > 0, beta > 0, alpha < 2m+2, beta < 2 are met.\n        term1 = beta(alpha, beta_p)\n        term2 = beta(2 * m - alpha + 2, 2 - beta_p)\n        var_is = term1 * term2 - 1 / ((m + 1)**2)\n        \n        # The ratio r = Var(Y_is) / Var(Y_c).\n        # var_c is non-zero since m=2 is used in all test cases.\n        ratio = var_is / var_c\n        \n        results.append(ratio)\n\n    # Format the results to 6 decimal places as required.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "Monte Carlo methods extend far beyond calculating simple integrals; they are powerful tools for estimating expected values in complex systems. This exercise applies the Monte Carlo framework to a classic problem in cooperative game theory: determining the Shapley value to ensure a fair allocation of a startup's equity among its founders [@problem_id:2411557]. This practice demonstrates how simulation can provide actionable answers to sophisticated economic questions that involve combinatorial complexity and principles of fairness.", "id": "2411557", "problem": "A startup’s founding team decides to allocate equity according to each founder’s marginal contribution in a cooperative production environment formalized by cooperative game theory. Let $N=\\{1,\\dots,n\\}$ be the set of founders and let $v:2^N \\to \\mathbb{R}_{\\ge 0}$ be a characteristic function that maps each coalition $S \\subseteq N$ to a nonnegative, real-valued output. The Shapley value $\\phi_i(v)$ of founder $i \\in N$ is defined as the expected marginal contribution of $i$ to a coalition of predecessors when founders arrive in a uniformly random order. Using Monte Carlo (MC) simulation, approximate the Shapley value for each founder by sampling independent Uniform(“all permutations of $N$”) permutations, computing each sampled marginal contribution, and averaging.\n\nBase principles to use:\n- Cooperative game definition: the value function $v$ assigns a well-defined payoff to each coalition $S \\subseteq N$, with $v(\\varnothing)=0$.\n- Shapley value definition via permutations: for any $i \\in N$, \n$$\n\\phi_i(v)=\\mathbb{E}_{\\pi}\\left[v\\!\\left(P_i^{\\pi}\\cup\\{i\\}\\right)-v\\!\\left(P_i^{\\pi}\\right)\\right],\n$$\nwhere $\\pi$ is a uniformly random permutation of $N$ and $P_i^{\\pi}$ is the set of players preceding $i$ in $\\pi$.\n- Monte Carlo integration (sample average approximation): if $X_1,\\dots,X_M$ are independent and identically distributed (i.i.d.) samples from a distribution with finite mean $\\mu$, then by the Law of Large Numbers the sample average $\\frac{1}{M}\\sum_{m=1}^M X_m$ converges to $\\mu$ as $M \\to \\infty$.\n\nAlgorithmic task:\n- For each test case below, estimate the Shapley value vector $\\phi(v)=(\\phi_1(v),\\dots,\\phi_n(v))$ by drawing $M$ i.i.d. permutations, computing the marginal contribution at each position in each permutation, and averaging. Use the specified pseudo-random seeds for reproducibility. Represent coalitions in any convenient manner (e.g., bit masks). Precompute the value function on all coalitions $S \\subseteq N$ if helpful.\n\nNumerical settings common to all tests:\n- For each test case, draw exactly $M=100000$ i.i.d. permutations uniformly from the $n!$ possible permutations.\n- Use the specified seed $s$ for the pseudo-random number generator at the start of each test case.\n- Report each component of the estimated Shapley value rounded to exactly $6$ decimal places.\n- No physical units are involved. All outputs are to be real numbers.\n\nTest suite:\n- Test A (additive, baseline “happy path”): let $n=3$. Let $a=(0.2,0.3,0.5)$. Define\n$$\nv_A(S)=\\sum_{i\\in S} a_i,\n$$\nwith $v_A(\\varnothing)=0$. Use seed $s_A=314159$.\n- Test B (pairwise synergies with diminishing returns): let $n=4$. Let $a=(0.8,0.6,0.4,0.2)$ and symmetric pairwise synergy matrix $B=(b_{ij})$ with $b_{ii}=0$ and the nonzero entries\n$$\nb_{12}=0.3,\\quad b_{13}=0.05,\\quad b_{23}=0.1,\\quad b_{34}=0.2,\n$$\nwith symmetry $b_{ij}=b_{ji}$ and all other $b_{ij}=0$. Define\n$$\nv_B(S)=\\log\\!\\Big(1+\\sum_{i\\in S} a_i+\\sum_{i<j,\\, i,j\\in S} b_{ij}\\Big),\n$$\nwhere $\\log$ denotes the natural logarithm, and $v_B(\\varnothing)=0$. Use seed $s_B=271828$.\n- Test C (threshold output, edge case with a zero-contribution founder): let $n=4$. Let $a=(0.4,0.35,0.25,0.0)$ and threshold $T=0.9$. Define\n$$\nv_C(S)=\\max\\!\\Big(0,\\sum_{i\\in S} a_i - T\\Big),\n$$\nwith $v_C(\\varnothing)=0$. Use seed $s_C=161803$.\n\nRequired final output format:\n- Your program should produce a single line of output that is a JSON-like list of lists. The line must contain three inner lists, one per test case in the order A, B, C. Each inner list must contain the $n$ estimated Shapley values for that test case, each rounded to $6$ decimal places, and written with no spaces. For example, the structural form must be\n- outer list: opening bracket, inner lists separated by commas, closing bracket,\n- inner list: opening bracket, $n$ real numbers separated by commas, closing bracket.", "solution": "The problem statement is parsed and determined to be valid. It is scientifically grounded in cooperative game theory and numerical analysis, well-posed with all necessary parameters defined, and phrased in objective, formal language. There are no contradictions, ambiguities, or unsound premises. The task is to implement a standard Monte Carlo simulation to estimate Shapley values, a well-defined computational problem.\n\nThe theoretical foundation for the solution is the definition of the Shapley value, $\\phi_i(v)$, for a player $i$ in a cooperative game with characteristic function $v$. The value $\\phi_i(v)$ represents the fair allocation of the total surplus generated by the grand coalition $N$, and it is uniquely characterized by a set of axioms (efficiency, symmetry, null player, and additivity). An equivalent and computationally useful definition is based on an expectation over random orderings of players. Specifically, the Shapley value for player $i$ is the expected marginal contribution of player $i$ to the coalition of players preceding them, averaged over all possible permutations of the set of players $N$. The expectation is taken with respect to a uniform probability distribution over the $n!$ possible permutations.\n\n$$\n\\phi_i(v)=\\mathbb{E}_{\\pi}\\left[v(P_i^{\\pi}\\cup\\{i\\}) - v(P_i^{\\pi})\\right]\n$$\n\nHere, $\\pi$ is a permutation of $N=\\{1, \\dots, n\\}$ chosen uniformly at random, and $P_i^{\\pi}$ is the set of players who appear before player $i$ in the permutation $\\pi$.\n\nDirect enumeration of all $n!$ permutations is computationally infeasible for even moderately large $n$. Therefore, we employ a Monte Carlo method. By the Law of Large Numbers, the expectation $\\mathbb{E}[X]$ of a random variable $X$ can be approximated by the sample mean of a large number of independent and identically distributed (i.i.d.) draws of $X$. In this context, the random variable for player $i$ is their marginal contribution $X_i(\\pi) = v(P_i^{\\pi}\\cup\\{i\\}) - v(P_i^{\\pi})$ for a randomly chosen permutation $\\pi$. We generate a large number, $M$, of i.i.d. random permutations $\\pi_1, \\pi_2, \\dots, \\pi_M$. The Shapley value $\\phi_i(v)$ is then estimated by the sample average:\n\n$$\n\\hat{\\phi}_i(v) = \\frac{1}{M} \\sum_{m=1}^{M} \\left[ v(P_i^{\\pi_m}\\cup\\{i\\}) - v(P_i^{\\pi_m}) \\right]\n$$\n\nThe algorithm proceeds as follows for each test case, which is defined by the number of players $n$, the characteristic function $v$, a pseudo-random seed $s$, and the number of Monte Carlo samples $M=100000$.\n\n1.  **Initialization**: A pseudo-random number generator is initialized with the specified seed $s$ for reproducibility. An array, `total_contributions`, of size $n$ is initialized to all zeros. This array will accumulate the marginal contributions for each player across all sampled permutations.\n\n2.  **Pre-computation of Characteristic Function**: For a small number of players $n$, the number of distinct coalitions, $2^n$, is manageable. To optimize the simulation, we pre-compute the value $v(S)$ for every possible coalition $S \\subseteq N$. Coalitions are efficiently represented using integer bitmasks, where an integer $k$ from $0$ to $2^n-1$ represents a coalition. The $j$-th bit of $k$ is $1$ if player $j$ (using $0$-based indexing) is in the coalition, and $0$ otherwise. A table, `v_values`, of size $2^n$ is populated such that `v_values[k]` stores the value $v(S_k)$, where $S_k$ is the coalition corresponding to the mask $k$.\n\n3.  **Monte Carlo Simulation Loop**: The main loop iterates $M$ times. In each iteration $m=1, \\dots, M$:\n    a. A new random permutation $\\pi_m$ of the players $\\{0, 1, \\dots, n-1\\}$ is generated using the initialized random number generator.\n    b. We iterate through the players in the order specified by $\\pi_m$. A variable, `predecessor_mask`, is maintained, starting at $0$ (representing the empty set $v(\\varnothing)=0$).\n    c. For each player $i$ at position $j$ in the permutation $\\pi_m$, their marginal contribution is calculated. The coalition of predecessors $P_i^{\\pi_m}$ is represented by `predecessor_mask`. The value $v(P_i^{\\pi_m})$ is looked up from the pre-computed table as `v_values[predecessor_mask]`.\n    d. The new coalition $S' = P_i^{\\pi_m} \\cup \\{i\\}$ is formed by setting the $i$-th bit in `predecessor_mask`. The value $v(S')$ is then looked up.\n    e. The marginal contribution, $\\Delta_i = v(S') - v(P_i^{\\pi_m})$, is computed and added to `total_contributions[i]`.\n    f. The `predecessor_mask` is updated to include player $i$ for the next player in the permutation.\n\n4.  **Final Estimation**: After the loop completes, the estimated Shapley value for each player $i$ is calculated by dividing the total accumulated marginal contribution by the number of samples $M$: $\\hat{\\phi}_i(v) = \\text{total\\_contributions}[i] / M$.\n\nThis general algorithm is applied to each of the three test cases, using their specific characteristic functions for the pre-computation step:\n\n-   **Test A**: $n=3$, $a=(0.2,0.3,0.5)$, $v_A(S)=\\sum_{i\\in S} a_i$. This is an additive game, where the analytical Shapley value is $\\phi_i(v_A) = a_i$. The Monte Carlo estimate should be very close to $(0.2, 0.3, 0.5)$.\n\n-   **Test B**: $n=4$, $a=(0.8,0.6,0.4,0.2)$, $B$ is a symmetric synergy matrix. The function $v_B(S)=\\log(1+\\sum_{i\\in S} a_i+\\sum_{i<j,\\, i,j\\in S} b_{ij})$ models individual contributions and pairwise synergies with diminishing returns, captured by the natural logarithm.\n\n-   **Test C**: $n=4$, $a=(0.4,0.35,0.25,0.0)$, $T=0.9$. The function $v_C(S)=\\max(0,\\sum_{i\\in S} a_i - T)$ represents a threshold project. Note that player $4$ (index $3$) has an individual contribution of $a_4=0.0$. Consequently, their marginal contribution is always $v_C(S \\cup \\{4\\}) - v_C(S) = \\max(0, \\sum_{i \\in S} a_i - T) - \\max(0, \\sum_{i \\in S} a_i - T) = 0$. Thus, their analytical Shapley value is $\\phi_4(v_C) = 0.0$, which provides a clear check for the implementation.\n\nThe final results are rounded to $6$ decimal places and formatted as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by running Monte Carlo simulations for each test case.\n    \"\"\"\n\n    def estimate_shapley_values(n, v_func, M, seed):\n        \"\"\"\n        Estimates Shapley values for a given game using Monte Carlo simulation.\n\n        Args:\n            n (int): Number of players.\n            v_func (function): The characteristic function v(S_mask).\n            M (int): Number of Monte Carlo samples (permutations).\n            seed (int): Seed for the pseudo-random number generator.\n\n        Returns:\n            np.ndarray: An array of estimated Shapley values for each player.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # 1. Pre-compute characteristic function values for all 2^n coalitions\n        num_coalitions = 1 << n\n        v_values = np.zeros(num_coalitions, dtype=float)\n        for mask in range(num_coalitions):\n            v_values[mask] = v_func(mask, n)\n\n        # 2. Monte Carlo simulation\n        total_marginal_contributions = np.zeros(n, dtype=float)\n        players = np.arange(n)\n\n        for _ in range(M):\n            # Generate a random permutation of players\n            perm = rng.permutation(players)\n            \n            predecessor_mask = 0\n            for player in perm:\n                # Marginal contribution of 'player' to the coalition of its predecessors\n                # Value of coalition without the player\n                v_predecessors = v_values[predecessor_mask]\n\n                # Value of coalition with the player\n                new_coalition_mask = predecessor_mask | (1 << player)\n                v_with_player = v_values[new_coalition_mask]\n\n                marginal_contribution = v_with_player - v_predecessors\n                total_marginal_contributions[player] += marginal_contribution\n\n                # Add player to the coalition for the next iteration\n                predecessor_mask = new_coalition_mask\n\n        # 3. Average the contributions to get the Shapley value estimates\n        shapley_values = total_marginal_contributions / M\n        return shapley_values\n\n    # Common numerical settings\n    M = 100000\n\n    # Test Case A\n    n_A = 3\n    a_A = np.array([0.2, 0.3, 0.5])\n    seed_A = 314159\n    def v_A(mask, n):\n        val = 0.0\n        for i in range(n):\n            if (mask >> i) & 1:\n                val += a_A[i]\n        return val\n\n    # Test Case B\n    n_B = 4\n    a_B = np.array([0.8, 0.6, 0.4, 0.2])\n    B = np.zeros((n_B, n_B))\n    B[0, 1] = B[1, 0] = 0.3\n    B[0, 2] = B[2, 0] = 0.05\n    B[1, 2] = B[2, 1] = 0.1\n    B[2, 3] = B[3, 2] = 0.2\n    seed_B = 271828\n    def v_B(mask, n):\n        sum_a = 0.0\n        sum_b = 0.0\n        players_in_coalition = []\n        for i in range(n):\n            if (mask >> i) & 1:\n                sum_a += a_B[i]\n                players_in_coalition.append(i)\n        \n        for i in range(len(players_in_coalition)):\n            for j in range(i + 1, len(players_in_coalition)):\n                p1 = players_in_coalition[i]\n                p2 = players_in_coalition[j]\n                sum_b += B[p1, p2]\n                \n        return np.log(1.0 + sum_a + sum_b)\n\n    # Test Case C\n    n_C = 4\n    a_C = np.array([0.4, 0.35, 0.25, 0.0])\n    T_C = 0.9\n    seed_C = 161803\n    def v_C(mask, n):\n        sum_a = 0.0\n        for i in range(n):\n            if (mask >> i) & 1:\n                sum_a += a_C[i]\n        return max(0.0, sum_a - T_C)\n\n    test_cases = [\n        {'n': n_A, 'v_func': v_A, 'seed': seed_A},\n        {'n': n_B, 'v_func': v_B, 'seed': seed_B},\n        {'n': n_C, 'v_func': v_C, 'seed': seed_C},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        shapley_values = estimate_shapley_values(case['n'], case['v_func'], M, case['seed'])\n        \n        # Format results to 6 decimal places\n        formatted_results = [f\"{val:.6f}\" for val in shapley_values]\n        all_results.append(f\"[{','.join(formatted_results)}]\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```"}]}