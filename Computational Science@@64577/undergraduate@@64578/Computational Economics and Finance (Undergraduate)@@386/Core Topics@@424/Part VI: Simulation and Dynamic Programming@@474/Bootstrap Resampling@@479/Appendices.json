{"hands_on_practices": [{"introduction": "To truly master bootstrap resampling, we must move from theory to application. This first practice provides a hands-on coding challenge: estimating the confidence interval for the median of a financial dataset. We focus on the median because, unlike the mean, its sampling distribution is often difficult to derive analytically, especially for non-normal data like venture capital returns. This exercise [@problem_id:2377547] will guide you through implementing the non-parametric percentile bootstrap, a powerful and intuitive method for placing bounds on a statistic when theoretical shortcuts are unavailable.", "id": "2377547", "problem": "Consider a dataset of venture capital deal-level simple returns defined for each deal $i$ as $r_i = \\dfrac{\\text{cash\\_out}_i - \\text{cash\\_in}_i}{\\text{cash\\_in}_i}$, so that $r_i \\in [-1, +\\infty)$. Assume the observed returns are realizations from an unknown distribution, and that the data are independent and identically distributed (IID). Let the sample median of a multiset $x_1, x_2, \\ldots, x_n$ be defined as the middle order statistic when $n$ is odd, and as the average of the two middle order statistics when $n$ is even.\n\nYour task is to compute, for each test case below, a two-sided confidence interval for the population median based on the following definition of the bootstrap percentile interval. Let $T$ denote the sample median functional. For a given dataset $D = [x_1, x_2, \\ldots, x_n]$, define the empirical distribution $\\hat{F}_n$ that places mass $1/n$ on each observed value (counting multiplicities). For a given integer $B \\ge 1$, draw $B$ IID bootstrap resamples, each of size $n$, from $\\hat{F}_n$ (sampling with replacement from the multiset $D$). For each resample $b \\in \\{1, 2, \\ldots, B\\}$, compute $T_b = T(D^{\\ast}_b)$, the sample median of the $b$-th resample. Let $T_{(1)} \\le T_{(2)} \\le \\cdots \\le T_{(B)}$ be the sorted values. For a nominal two-sided confidence level $1 - \\alpha \\in (0,1)$, define the lower endpoint as the empirical $p$-quantile with $p = \\alpha/2$, and the upper endpoint as the empirical $q$-quantile with $q = 1 - \\alpha/2$, where for any probability level $u \\in [0,1]$ the empirical quantile $Q(u)$ is defined by linear interpolation on sorted indices:\n- Let $k = 1 + (B - 1)u$.\n- If $k \\le 1$, set $Q(u) = T_{(1)}$; if $k \\ge B$, set $Q(u) = T_{(B)}$.\n- Otherwise, write $k = m + \\delta$ with $m = \\lfloor k \\rfloor$ and $\\delta \\in (0,1)$, and set $Q(u) = (1 - \\delta) T_{(m)} + \\delta T_{(m+1)}$.\n\nFor reproducibility, for each test case use a pseudo-random number generator initialized with the specified integer seed before drawing the $B$ bootstrap resamples.\n\nTest suite:\n- Case $1$: $D_1 = [-1.0, -0.8, -0.6, -0.5, -0.3, -0.1, 0.0, 0.2, 0.25, 0.35, 0.4, 0.5, 0.6, 0.9, 1.2, 1.5, 2.0, 3.0, 5.0]$, $B_1 = 20000$, $\\alpha_1 = 0.10$, seed $S_1 = 202311$.\n- Case $2$: $D_2 = [-1.0, -0.9, -0.9, -0.5, -0.2, -0.1, 0.0, 0.05]$, $B_2 = 15000$, $\\alpha_2 = 0.20$, seed $S_2 = 7$.\n- Case $3$: $D_3 = [-1.0, -1.0, -0.5, -0.5, 0.0, 0.0, 0.4, 0.4, 0.4, 1.0, 1.0, 2.5]$, $B_3 = 30000$, $\\alpha_3 = 0.05$, seed $S_3 = 12345$.\n- Case $4$: $D_4 = [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]$, $B_4 = 10000$, $\\alpha_4 = 0.10$, seed $S_4 = 99$.\n\nYour program must compute for each case the pair $[\\ell, u] = [Q(\\alpha/2), Q(1 - \\alpha/2)]$ as defined above. Each bound must be reported as a decimal number rounded to six digits after the decimal point.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, where each case is a two-element list $[\\ell, u]$. For example: $[[\\ell_1,u_1],[\\ell_2,u_2],[\\ell_3,u_3],[\\ell_4,u_4]]$ with each numeric value rounded to six digits after the decimal point.", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of givens for a standard computational statistics problem. The task is to compute bootstrap percentile confidence intervals for the population median, given several datasets and parameters.\n\nThe method to be implemented is the bootstrap percentile interval. This is a non-parametric statistical technique used to estimate the sampling distribution of a statistic, and thereby construct confidence intervals, without making strong assumptions about the underlying population distribution. The fundamental principle of the bootstrap is to use the empirical distribution function, $\\hat{F}_n$, of the observed sample $D = \\{x_1, x_2, \\ldots, x_n\\}$ as an approximation to the true, unknown population distribution $F$. In $\\hat{F}_n$, each observed data point $x_i$ is assigned a probability mass of $1/n$.\n\nThe algorithm proceeds as follows:\n$1$.\nFor a given dataset $D$ of size $n$, we draw a large number, $B$, of bootstrap resamples. Each resample, denoted $D^{\\ast}_b$ for $b \\in \\{1, 2, \\ldots, B\\}$, is of size $n$ and is obtained by sampling with replacement from the original dataset $D$. This process is a computational realization of drawing IID samples from the empirical distribution $\\hat{F}_n$.\n\n$2$.\nFor each bootstrap resample $D^{\\ast}_b$, we compute the statistic of interest. In this problem, the statistic is the sample median, denoted as $T$. Let $T_b = T(D^{\\ast}_b)$ be the sample median of the $b$-th resample. The sample median is defined as the middle order statistic for an odd-sized sample and the arithmetic mean of the two middle order statistics for an even-sized sample.\n\n$3$.\nThe collection of $B$ bootstrap statistics, $\\{T_1, T_2, \\ldots, T_B\\}$, serves as an empirical approximation to the sampling distribution of the sample median, $T$.\n\n$4$.\nTo construct a two-sided confidence interval with a nominal confidence level of $1-\\alpha$, we use the percentile method. This involves finding the appropriate quantiles of the sorted bootstrap statistics, $T_{(1)} \\le T_{(2)} \\le \\cdots \\le T_{(B)}$. The lower bound of the confidence interval, $\\ell$, is the empirical $(\\alpha/2)$-quantile of this distribution, and the upper bound, $u$, is the empirical $(1 - \\alpha/2)$-quantile.\n\n$5$.\nThe problem provides a precise definition for the calculation of the empirical quantile, $Q(u)$, for any probability level $u \\in [0,1]$. It uses linear interpolation between order statistics. The procedure involves calculating an index $k = 1 + (B - 1)u$ and, if $1 < k < B$, interpolating between the values $T_{(\\lfloor k \\rfloor)}$ and $T_{(\\lfloor k \\rfloor + 1)}$. This specific rule corresponds to the standard linear interpolation method for quantiles, implemented for instance in the `numpy.quantile` function with the `interpolation='linear'` option. Using this specific function ensures adherence to the problem definition.\n\n$6$.\nFor scientific reproducibility, the pseudo-random number generator used for resampling must be initialized with a specific seed for each test case. This ensures that the sequence of bootstrap resamples is deterministic and the result can be independently verified.\n\nThe following implementation will execute this algorithm for each of the four test cases provided, computing the specified confidence interval bounds and rounding them to six decimal places as required. For the special case where all data points in the original sample are identical, such as in Case $4$, any bootstrap resample will also consist of identical values. Consequently, the median of every bootstrap resample will be this same value, and the resulting confidence interval will collapse to a single point.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are required or permitted.\n\ndef solve():\n    \"\"\"\n    Computes bootstrap percentile confidence intervals for the median for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            'D': [-1.0, -0.8, -0.6, -0.5, -0.3, -0.1, 0.0, 0.2, 0.25, 0.35, 0.4, 0.5, 0.6, 0.9, 1.2, 1.5, 2.0, 3.0, 5.0],\n            'B': 20000,\n            'alpha': 0.10,\n            'seed': 202311\n        },\n        # Case 2\n        {\n            'D': [-1.0, -0.9, -0.9, -0.5, -0.2, -0.1, 0.0, 0.05],\n            'B': 15000,\n            'alpha': 0.20,\n            'seed': 7\n        },\n        # Case 3\n        {\n            'D': [-1.0, -1.0, -0.5, -0.5, 0.0, 0.0, 0.4, 0.4, 0.4, 1.0, 1.0, 2.5],\n            'B': 30000,\n            'alpha': 0.05,\n            'seed': 12345\n        },\n        # Case 4\n        {\n            'D': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n            'B': 10000,\n            'alpha': 0.10,\n            'seed': 99\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract parameters for the current case.\n        D = np.array(case['D'])\n        B = case['B']\n        alpha = case['alpha']\n        seed = case['seed']\n        n = len(D)\n\n        # 1. Initialize the pseudo-random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 2. Generate B bootstrap resamples and compute the median for each.\n        # Pre-allocate array for efficiency.\n        bootstrap_medians = np.zeros(B)\n        for i in range(B):\n            # Draw a bootstrap resample of size n with replacement.\n            resample = rng.choice(D, size=n, replace=True)\n            # Compute the median of the resample.\n            bootstrap_medians[i] = np.median(resample)\n\n        # 3. Sort the bootstrap medians to prepare for quantile calculation.\n        bootstrap_medians.sort()\n\n        # 4. Compute the lower and upper quantiles for the confidence interval.\n        # The problem's quantile definition matches numpy's 'linear' interpolation.\n        p_lower = alpha / 2.0\n        p_upper = 1.0 - alpha / 2.0\n\n        lower_bound = np.quantile(bootstrap_medians, p_lower, interpolation='linear')\n        upper_bound = np.quantile(bootstrap_medians, p_upper, interpolation='linear')\n\n        # 5. Round the results to six decimal places as required by the problem.\n        l_rounded = round(lower_bound, 6)\n        u_rounded = round(upper_bound, 6)\n        \n        results.append([l_rounded, u_rounded])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, with numeric values rounded to 6 decimal places.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function.\nsolve()\n```"}, {"introduction": "A powerful tool is only effective when its limitations are understood. The standard non-parametric bootstrap is built on the crucial assumption that data are independent and identically distributed (i.i.d.). This exercise [@problem_id:2377555] challenges you to think critically about what happens when this assumption is violated, specifically in the context of autoregressive time series data common in finance. By analyzing why the i.i.d. bootstrap fails in this context, you will develop a deeper understanding of the \"why\" behind the method, ensuring you choose the right tool for the right data structure.", "id": "2377555", "problem": "You observe a univariate time series $\\{y_t\\}_{t=0}^{T}$ generated by the autoregressive model of order $1$, $\\text{AR}(1)$, given by\n$$\ny_t = \\beta\\, y_{t-1} + \\epsilon_t,\\quad t=1,\\dots,T,\n$$\nwith $|\\beta|<1$, where $\\{\\epsilon_t\\}$ are independent and identically distributed (i.i.d.) with $\\mathbb{E}[\\epsilon_t]=0$, $\\mathbb{V}\\mathrm{ar}(\\epsilon_t)=\\sigma^2 \\in (0,\\infty)$, and a finite fourth moment. You estimate $\\beta$ by ordinary least squares (OLS) from the regression of $y_t$ on $y_{t-1}$. A colleague proposes a “standard non-parametric i.i.d. bootstrap” as follows: sample with replacement from $\\{y_1,\\dots,y_T\\}$ to produce a bootstrap series $\\{y_t^{\\ast}\\}_{t=1}^{T}$, then compute a bootstrap estimate $\\hat{\\beta}^{\\ast}$ by regressing $y_t^{\\ast}$ on $y_{t-1}^{\\ast}$. The colleague plans to use the empirical distribution of $\\hat{\\beta}^{\\ast}$ to form confidence intervals for $\\beta$.\n\nWhich option best explains why this bootstrap procedure fails to provide asymptotically valid inference for $\\beta$?\n\nA. The i.i.d. resampling of $\\{y_t\\}$ destroys the serial dependence and the recursive relation $y_t=\\beta y_{t-1}+\\epsilon_t$, so the bootstrap world does not replicate the joint dependence structure that determines the sampling distribution of the OLS estimator; hence the bootstrap approximation is inconsistent.\n\nB. The failure arises only because $\\{\\epsilon_t\\}$ need to be Gaussian; if $\\epsilon_t\\sim \\mathcal{N}(0,\\sigma^2)$, the i.i.d. bootstrap would be exact for $\\hat{\\beta}$.\n\nC. The OLS estimator of $\\beta$ in an $\\text{AR}(1)$ is superconsistent, so no bootstrap can approximate its sampling distribution.\n\nD. The procedure may be inaccurate in very small samples, but for large $T$ the i.i.d. bootstrap becomes valid because the dependence becomes negligible and the shuffled series approximates the original process.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n-   The observed data is a univariate time series $\\{y_t\\}_{t=0}^{T}$.\n-   The data generating process (DGP) is an autoregressive model of order $1$, $\\text{AR}(1)$: $y_t = \\beta\\, y_{t-1} + \\epsilon_t$, for $t=1,\\dots,T$.\n-   The parameter $\\beta$ is constrained by $|\\beta|<1$, which ensures the process is stationary.\n-   The error terms $\\{\\epsilon_t\\}$ are independent and identically distributed (i.i.d.) with $\\mathbb{E}[\\epsilon_t]=0$, $\\mathbb{V}\\mathrm{ar}(\\epsilon_t)=\\sigma^2 \\in (0,\\infty)$, and a finite fourth moment.\n-   The parameter $\\beta$ is estimated by an ordinary least squares (OLS) estimator, $\\hat{\\beta}$, from the regression of $y_t$ on $y_{t-1}$.\n-   A proposed bootstrap procedure is described as a “standard non-parametric i.i.d. bootstrap”:\n    1.  A bootstrap series $\\{y_t^{\\ast}\\}_{t=1}^{T}$ is generated by sampling with replacement from the observed data $\\{y_1,\\dots,y_T\\}$.\n    2.  A bootstrap estimate $\\hat{\\beta}^{\\ast}$ is computed by regressing $y_t^{\\ast}$ on $y_{t-1}^{\\ast}$.\n    3.  The empirical distribution of $\\hat{\\beta}^{\\ast}$ is to be used for forming confidence intervals for $\\beta$.\n-   The question asks for the reason why this bootstrap procedure fails to provide asymptotically valid inference for $\\beta$.\n\nStep 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem deals with standard concepts in time series econometrics: the $\\text{AR}(1)$ model, OLS estimation, and bootstrap resampling. The model and its properties are fundamental in the field. The question pertains to the correct application of statistical methods. The setup is scientifically sound.\n-   **Well-Posed:** The problem is clearly stated. It specifies the model, the estimator, and the proposed inferential procedure, and asks for a specific diagnosis of why the procedure is invalid. A unique and correct explanation exists within the theory of bootstrap methods for dependent data.\n-   **Objective:** The problem is phrased in objective, technical language, free from ambiguity or subjective content.\n-   **Self-Contained and Consistent:** All necessary information is provided. The stationarity condition $|\\beta|<1$ is given, and the properties of the error term are standard for asymptotic analysis. There are no contradictions.\n\nStep 3: Verdict and Action\nThe problem statement is valid. It is a well-formulated question in econometrics. I will proceed with a full derivation and analysis of the options.\n\nThe core principle of the bootstrap is that the bootstrap data generating process must mimic the true data generating process. The sampling distribution of an estimator is determined by the joint probability distribution of the sample data. For a bootstrap procedure to be valid, the relationship between the bootstrap sample and the original sample must replicate the relationship between the original sample and the true population or process.\n\nThe true data generating process is an $\\text{AR}(1)$ model:\n$$\ny_t = \\beta y_{t-1} + \\epsilon_t\n$$\nThis equation defines the temporal dependence structure of the series $\\{y_t\\}$. The value of $y_t$ is directly and causally dependent on its immediate past value, $y_{t-1}$. This serial dependence is the defining characteristic of the process. The OLS estimator of $\\beta$ is\n$$\n\\hat{\\beta} = \\frac{\\sum_{t=1}^T y_t y_{t-1}}{\\sum_{t=1}^T y_{t-1}^2}\n$$\nThe sampling distribution of $\\hat{\\beta}$ (specifically, the distribution of $\\sqrt{T}(\\hat{\\beta} - \\beta)$) is a consequence of this serial dependence.\n\nNow, consider the proposed bootstrap procedure. It creates a bootstrap sample $\\{y_t^{\\ast}\\}_{t=1}^T$ by sampling with replacement from the original observations $\\{y_1, \\dots, y_T\\}$. This is the standard non-parametric bootstrap designed for i.i.d. data. The crucial consequence of this resampling scheme is that the temporal ordering of the original data is destroyed. In the bootstrap sample, the value of $y_t^{\\ast}$ is a random draw from $\\{y_1, \\dots, y_T\\}$, and the value of $y_{t-1}^{\\ast}$ is another, independent random draw from the same set. Therefore, there is no systematic relationship between $y_t^{\\ast}$ and $y_{t-1}^{\\ast}$ that resembles the original $\\text{AR}(1)$ structure.\n\nIn the bootstrap world, the pair $(y_t^{\\ast}, y_{t-1}^{\\ast})$ consists of two independent draws from the empirical distribution function of the original data. The \"true\" regression coefficient between two such independent variables is zero (assuming $\\mathbb{E}[y_t]=0$). Therefore, the bootstrap estimator\n$$\n\\hat{\\beta}^{\\ast} = \\frac{\\sum_{t=1}^T y_t^{\\ast} y_{t-1}^{\\ast}}{\\sum_{t=1}^T (y_{t-1}^{\\ast})^2}\n$$\nwill not converge to a quantity related to $\\beta$. Instead, its distribution will be centered around $0$. Since the distribution of $\\hat{\\beta}^{\\ast}$ does not approximate the distribution of $\\hat{\\beta}$, the bootstrap procedure is inconsistent and fails to provide valid inference.\n\nCorrect bootstrap procedures for autoregressive models, such as the residual bootstrap or the moving block bootstrap, are specifically designed to preserve the dependence structure of the time series. The proposed i.i.d. bootstrap fails precisely because it neglects this structure.\n\nNow, let us evaluate each option:\n\nA. The i.i.d. resampling of $\\{y_t\\}$ destroys the serial dependence and the recursive relation $y_t=\\beta y_{t-1}+\\epsilon_t$, so the bootstrap world does not replicate the joint dependence structure that determines the sampling distribution of the OLS estimator; hence the bootstrap approximation is inconsistent.\nThis statement accurately diagnoses the fundamental flaw. The validity of the bootstrap hinges on replicating the properties of the DGP. For time series, this includes the dependence structure. The i.i.d. resampling scheme treats the data as if they were independent, which they are not, thereby breaking the very structure that the model aims to capture. This leads to an inconsistent bootstrap.\n**Verdict: Correct.**\n\nB. The failure arises only because $\\{\\epsilon_t\\}$ need to be Gaussian; if $\\epsilon_t\\sim \\mathcal{N}(0,\\sigma^2)$, the i.i.d. bootstrap would be exact for $\\hat{\\beta}$.\nThis is incorrect. The failure is due to the violation of the dependence structure, a problem that is unrelated to the distributional form of the error terms $\\{\\epsilon_t\\}$. Assuming Gaussian errors would not rectify the fact that i.i.d. resampling of $\\{y_t\\}$ destroys the autoregressive dynamics. The problem persists regardless of whether the errors are Gaussian or follow some other distribution.\n**Verdict: Incorrect.**\n\nC. The OLS estimator of $\\beta$ in an $\\text{AR}(1)$ is superconsistent, so no bootstrap can approximate its sampling distribution.\nThis statement is factually incorrect on two counts. First, for a stationary $\\text{AR}(1)$ process where $|\\beta|<1$, the OLS estimator $\\hat{\\beta}$ is $\\sqrt{T}$-consistent, not superconsistent. Superconsistency (convergence at a rate of $T$) occurs in the unit root case where $\\beta=1$. The problem explicitly states $|\\beta|<1$. Second, the claim that \"no bootstrap can approximate\" the distribution of a superconsistent estimator is too strong and generally false; specialized bootstrap methods can be designed for such cases.\n**Verdict: Incorrect.**\n\nD. The procedure may be inaccurate in very small samples, but for large $T$ the i.i.d. bootstrap becomes valid because the dependence becomes negligible and the shuffled series approximates the original process.\nThis is the opposite of the truth. The failure of this bootstrap procedure is asymptotic. As the sample size $T$ grows, the inconsistency becomes more, not less, evident. The distribution of the bootstrap estimator $\\hat{\\beta}^{\\ast}$ will converge to a distribution centered at or near $0$, while the sampling distribution of the actual estimator $\\hat{\\beta}$ is centered at the true parameter $\\beta$. The dependence in a stationary $\\text{AR}(1)$ process with $\\beta \\neq 0$ is a defining feature and does not become negligible as $T$ increases. Shuffling the series is precisely what one must not do.\n**Verdict: Incorrect.**\n\nThe only correct and precise explanation for the failure of the proposed bootstrap procedure is provided in option A.", "answer": "$$\\boxed{A}$$"}, {"introduction": "After learning how to apply the bootstrap and when to be cautious, a final practical question remains: how do we ensure our results are reliable? The bootstrap is a Monte Carlo simulation, and its output has inherent randomness that depends on the number of replications, $B$. This exercise [@problem_id:2377512] provides a framework for investigating the stability of your bootstrap confidence intervals. By quantifying how the interval endpoints fluctuate with the random seed, you will gain an intuition for how to choose a sufficiently large $B$ to achieve stable, reproducible results, a critical skill for any serious practitioner.", "id": "2377512", "problem": "Consider a single asset with independent and identically distributed (i.i.d.) daily log-returns. Let a realized sample be denoted by $\\{X_i\\}_{i=1}^n$. Define the sample mean $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n X_i$. For a given number of bootstrap replications $B$, define the bootstrap sampling distribution of $\\hat{\\mu}$ by repeatedly resampling, with replacement, $n$ observations from the realized sample and computing the mean for each resample. The two-sided confidence interval at confidence level $0.95$ is defined by the empirical quantiles at levels $0.025$ and $0.975$ of the bootstrap distribution of $\\hat{\\mu}$, using linear interpolation between order statistics. Denote these endpoints by $L(B)$ (lower) and $U(B)$ (upper).\n\nTo quantify the stability of the endpoints with respect to $B$, consider $\\mathcal{R}$ independent repetitions of the entire bootstrap procedure using the same realized sample but different random seeds; label the endpoints in repetition $j \\in \\{1,\\dots,\\mathcal{R}\\}$ by $\\ell_j(B)$ and $u_j(B)$. Define the stability measures as population standard deviations\n$$\ns_\\ell(B) = \\left(\\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}}\\left(\\ell_j(B) - \\bar{\\ell}(B)\\right)^2\\right)^{1/2},\\quad\n\\bar{\\ell}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}} \\ell_j(B),\n$$\nand\n$$\ns_u(B) = \\left(\\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}}\\left(u_j(B) - \\bar{u}(B)\\right)^2\\right)^{1/2},\\quad\n\\bar{u}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}} u_j(B).\n$$\n\nFor reproducibility, the realized sample for each test case must be generated synthetically as specified below, and it must be reused unchanged across all $\\mathcal{R}$ repetitions for that test case. In each repetition $j \\in \\{0,1,\\dots,\\mathcal{R}-1\\}$, the bootstrap procedure must use a pseudorandom seed equal to the given base seed plus $j$. For the generation of the realized sample, use the provided sample seed exactly. All pseudorandom number generation must be based on a modern pseudorandom generator with the given integer seeds.\n\nReturn models:\n- Gaussian returns: $X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$.\n- Heavy-tailed returns: $X_i = \\mu + \\sigma \\sqrt{\\frac{\\nu - 2}{\\nu}}\\, T_{\\nu}$, where $T_{\\nu}$ has a Student’s $t$ distribution with $\\nu > 2$ degrees of freedom and unit scale, so that $\\mathrm{Var}(X_i)$ equals $\\sigma^2$.\n\nStatistic of interest:\n- The parameter is the mean return $\\mu$ and the statistic is the sample mean $\\hat{\\mu}$.\n\nConfidence interval:\n- Endpoints are the empirical quantiles at levels $0.025$ and $0.975$ of the bootstrap distribution of $\\hat{\\mu}$, with linear interpolation between order statistics.\n\nStability metric:\n- Report $s_\\ell(B)$ and $s_u(B)$ as defined above for each test case, rounded to $6$ decimals.\n\nTest suite:\nFor each test case, parameters are listed as $(\\text{dist}, n, \\mu, \\sigma, \\nu, B, \\mathcal{R}, \\text{sample\\_seed}, \\text{bootstrap\\_base\\_seed})$; when $\\text{dist}=\\text{N}$, the entry $\\nu$ is not used.\n1. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 25,\\, 32,\\, 20231101,\\, 770000)$.\n2. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 200,\\, 32,\\, 20231101,\\, 770100)$.\n3. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 1000,\\, 32,\\, 20231101,\\, 770200)$.\n4. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 2000,\\, 32,\\, 20231101,\\, 770300)$.\n5. $(\\text{N},\\, 30,\\, 0.0005,\\, 0.01,\\, -,\\, 200,\\, 32,\\, 20231111,\\, 880100)$.\n6. $(\\text{T},\\, 252,\\, 0.0005,\\, 0.02,\\, 3,\\, 1000,\\, 32,\\, 20231221,\\, 990100)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[s_\\ell(B), s_u(B)]$ in the same order as the test cases, with each value rounded to $6$ decimals. For example, a valid output for three cases would look like $[[0.001234,0.001987],[0.000456,0.000765],[0.000123,0.000321]]$.\n\nNo physical units are involved. All numeric results must be floats.", "solution": "The problem presents a valid and well-posed task in computational statistics, specifically concerning the stability analysis of bootstrap confidence intervals. I will provide a systematic solution.\n\nThe core of the problem is to quantify the variability of bootstrap confidence interval endpoints. This variability arises because the bootstrap procedure itself is a random process, dependent on a random seed. By repeating the entire bootstrap procedure multiple times with different seeds, we can observe the distribution of the interval endpoints and measure their stability, for which the standard deviation is a natural metric.\n\nThe overall procedure for each test case is as follows:\n1.  Synthetically generate a single realized sample of asset log-returns, $\\{X_i\\}_{i=1}^n$, of size $n$. This sample is generated once using a specified `sample_seed` and remains fixed throughout the subsequent steps for a given test case.\n2.  Perform $\\mathcal{R}$ independent repetitions of the bootstrap confidence interval estimation. For each repetition $j \\in \\{0, 1, \\dots, \\mathcal{R}-1\\}$:\n    a.  Set the seed for the pseudorandom number generator (PRNG) to `bootstrap_base_seed` $+ j$. This ensures that each of the $\\mathcal{R}$ repetitions is statistically independent yet computationally reproducible.\n    b.  Generate $B$ bootstrap samples. Each bootstrap sample is created by drawing $n$ observations from the original realized sample $\\{X_i\\}_{i=1}^n$ with replacement.\n    c.  For each of the $B$ bootstrap samples, compute the sample mean. This collection of $B$ means forms the empirical bootstrap distribution of the statistic $\\hat{\\mu}$.\n    d.  From this empirical distribution, determine the two-sided $95\\%$ confidence interval. The problem specifies that the lower and upper endpoints, denoted $\\ell_j(B)$ and $u_j(B)$, are the empirical quantiles at levels $q_{low} = 0.025$ and $q_{high} = 0.975$, respectively. Linear interpolation is used to estimate the quantiles if they fall between ordered data points.\n3.  After completing all $\\mathcal{R}$ repetitions, we will have two sets of data: $\\{\\ell_j(B)\\}_{j=0}^{\\mathcal{R}-1}$ and $\\{u_j(B)\\}_{j=0}^{\\mathcal{R}-1}$.\n4.  Finally, calculate the stability of these endpoints. The problem defines the stability measures as the population standard deviations of these two sets of endpoints. Let the collection of lower endpoints be $\\boldsymbol{\\ell} = (\\ell_0(B), \\dots, \\ell_{\\mathcal{R}-1}(B))$ and upper endpoints be $\\boldsymbol{u} = (u_0(B), \\dots, u_{\\mathcal{R}-1}(B))$. The means are $\\bar{\\ell}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1} \\ell_j(B)$ and $\\bar{u}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1} u_j(B)$. The stability metrics are then:\n$$\ns_\\ell(B) = \\sqrt{\\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1}\\left(\\ell_j(B) - \\bar{\\ell}(B)\\right)^2}\n$$\n$$\ns_u(B) = \\sqrt{\\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1}\\left(u_j(B) - \\bar{u}(B)\\right)^2}\n$$\nThis corresponds to a standard deviation calculation with a divisor of $\\mathcal{R}$, not $\\mathcal{R}-1$.\n\nThe generation of the realized sample depends on the specified distribution.\n- For Gaussian returns, $X_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we draw from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$.\n- For heavy-tailed returns, $X_i = \\mu + \\sigma \\sqrt{\\frac{\\nu - 2}{\\nu}}\\, T_{\\nu}$, where $T_{\\nu}$ follows a standard Student's t-distribution with $\\nu$ degrees of freedom. This formulation is chosen so that $\\mathrm{E}[X_i] = \\mu$ and $\\mathrm{Var}(X_i) = \\sigma^2$, provided $\\nu > 2$.\n\nComputationally, the implementation will adhere to these steps. A modern pseudorandom number generator, specifically `numpy.random.PCG64`, will be used for all random number generation to ensure reproducibility as specified. The calculation of quantiles with linear interpolation is handled by `numpy.quantile` with its default `method='linear'`. The population standard deviation is computed using `numpy.std` with `ddof=0`. The entire process is encapsulated in a function that iterates through the provided test suite, calculates the pair $(s_\\ell(B), s_u(B))$ for each case, and formats the output according to the strict requirements.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef compute_stability_for_case(dist, n, mu, sigma, nu, B, R, sample_seed, bootstrap_base_seed):\n    \"\"\"\n    Computes the stability of bootstrap confidence interval endpoints for a single test case.\n\n    Args:\n        dist (str): Distribution type ('N' for Gaussian, 'T' for Student's-t).\n        n (int): Size of the realized sample.\n        mu (float): Mean of the return distribution.\n        sigma (float): Standard deviation of the return distribution.\n        nu (int): Degrees of freedom for the Student's-t distribution.\n        B (int): Number of bootstrap replications.\n        R (int): Number of independent repetitions of the bootstrap procedure.\n        sample_seed (int): Seed for generating the realized sample.\n        bootstrap_base_seed (int): Base seed for the bootstrap procedure repetitions.\n\n    Returns:\n        list: A list containing [s_l, s_u], the stability measures for the lower and upper\n              confidence interval endpoints, rounded to 6 decimals.\n    \"\"\"\n    # Step 1: Generate the realized sample\n    rng_sample = np.random.Generator(np.random.PCG64(sample_seed))\n    \n    if dist == 'N':\n        realized_sample = rng_sample.normal(loc=mu, scale=sigma, size=n)\n    elif dist == 'T':\n        # Generate standard Student's t-distributed random variables\n        t_samples = t.rvs(df=nu, size=n, random_state=rng_sample)\n        # Scale to match mean mu and variance sigma^2\n        scaling_factor = sigma * np.sqrt((nu - 2) / nu)\n        realized_sample = mu + scaling_factor * t_samples\n    else:\n        raise ValueError(\"Invalid distribution type specified.\")\n\n    lower_endpoints = []\n    upper_endpoints = []\n\n    # Step 2: Outer loop for R independent repetitions\n    for j in range(R):\n        current_seed = bootstrap_base_seed + j\n        rng_bootstrap = np.random.Generator(np.random.PCG64(current_seed))\n\n        # Perform B bootstrap resamples efficiently\n        # Generate all indices for B resamples at once\n        bootstrap_indices = rng_bootstrap.choice(n, size=(B, n), replace=True)\n        \n        # Create bootstrap samples and compute their means\n        bootstrap_samples = realized_sample[bootstrap_indices]\n        bootstrap_means = bootstrap_samples.mean(axis=1)\n\n        # Step 2d: Calculate confidence interval endpoints\n        q_low, q_high = np.quantile(bootstrap_means, [0.025, 0.975])\n        \n        lower_endpoints.append(q_low)\n        upper_endpoints.append(q_high)\n\n    # Step 4: Calculate stability measures (population standard deviation)\n    s_l = np.std(lower_endpoints, ddof=0)\n    s_u = np.std(upper_endpoints, ddof=0)\n\n    return [round(s_l, 6), round(s_u, 6)]\n\ndef solve():\n    \"\"\"\n    Main solver function that runs all test cases and prints the final result.\n    \"\"\"\n    test_cases = [\n        # (dist, n, mu, sigma, nu, B, R, sample_seed, bootstrap_base_seed)\n        ('N', 252, 0.0005, 0.01, None, 25, 32, 20231101, 770000),\n        ('N', 252, 0.0005, 0.01, None, 200, 32, 20231101, 770100),\n        ('N', 252, 0.0005, 0.01, None, 1000, 32, 20231101, 770200),\n        ('N', 252, 0.0005, 0.01, None, 2000, 32, 20231101, 770300),\n        ('N', 30, 0.0005, 0.01, None, 200, 32, 20231111, 880100),\n        ('T', 252, 0.0005, 0.02, 3, 1000, 32, 20231221, 990100),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_stability_for_case(*case)\n        results.append(result)\n\n    # Format the final output string exactly as required, without spaces after commas.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"}]}