{"hands_on_practices": [{"introduction": "The best way to truly grasp Value Function Iteration (VFI) is to build it from the ground up. This first exercise guides you through implementing a VFI solver for a canonical stochastic optimal growth model, a true workhorse in modern computational macroeconomics. By solving for an agent's optimal consumption and savings decisions in the face of random productivity shocks, you will build a foundational understanding of the core VFI mechanics: discretizing state spaces, iterating on the Bellman equation, and extracting the resulting value and policy functions [@problem_id:2446471].", "id": "2446471", "problem": "Consider an infinite-horizon consumption–savings problem with a single representative agent who faces a two-dimensional state comprised of physical capital and an exogenous productivity level. Time is discrete and indexed by $t \\in \\{0,1,2,\\dots\\}$. The period utility is Constant Relative Risk Aversion (CRRA), defined for consumption $c_t$ by\n$$\nu(c_t) = \n\\begin{cases}\n\\dfrac{c_t^{1-\\sigma}}{1-\\sigma}, & \\text{if } \\sigma \\neq 1, \\\\\n\\log(c_t), & \\text{if } \\sigma = 1,\n\\end{cases}\n$$\nwhere $\\sigma > 0$ is the coefficient of relative risk aversion. The production technology is Cobb–Douglas with capital share parameter $\\alpha \\in (0,1)$ and depreciation rate $\\delta \\in (0,1)$, so that output is $y_t = z_t k_t^{\\alpha}$ and the law of motion for capital satisfies\n$$\nc_t + k_{t+1} = z_t k_t^{\\alpha} + (1 - \\delta) k_t,\n$$\nwith the nonnegativity constraint $c_t \\ge 0$ and a borrowing constraint $k_{t+1} \\ge 0$. The discount factor is $\\beta \\in (0,1)$.\n\nThe exogenous productivity state $z_t$ takes values on a finite grid $\\{z_1,\\dots,z_{N_z}\\}$ and follows a time-homogeneous Markov chain with transition matrix $P \\in \\mathbb{R}^{N_z \\times N_z}$, where $P_{ij} = \\mathbb{P}(z_{t+1}=z_j \\mid z_t=z_i)$ and each row of $P$ sums to $1$.\n\nDefine the value function $V(k,z)$ as the supremum over feasible plans of the expected discounted sum of utilities starting from state $(k,z)$. The value function satisfies the Bellman equation\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\!\\left(z k^{\\alpha} + (1-\\delta)k - k' \\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} \\mathbb{P}(z' \\mid z) V(k', z') \\right\\},\n$$\nwhere $\\mathcal{Z} = \\{z_1,\\dots,z_{N_z}\\}$ and $\\mathcal{K}$ is a discrete grid of capital choices. The feasible set is characterized by $k' \\in \\mathcal{K}$ such that $z k^{\\alpha} + (1-\\delta)k - k' \\ge 0$.\n\nYour program must:\n- Discretize the capital space on an equally spaced grid $\\mathcal{K} = \\{k_{\\min}, k_{\\min} + \\Delta k, \\dots, k_{\\max}\\}$ with $N_k$ points (including endpoints).\n- Compute the unique fixed point of the Bellman equation on the Cartesian product $\\mathcal{K} \\times \\mathcal{Z}$, and the associated policy function for next-period capital $k'(k,z)$ restricted to $\\mathcal{K}$.\n- For each test case below, report the following five quantities:\n  1. The number of iterations taken to satisfy the stopping criterion.\n  2. The value function evaluated at the median capital grid index and the median shock index, i.e., $V\\!\\left(k_{\\lfloor (N_k-1)/2 \\rfloor+1}, z_{\\lfloor (N_z-1)/2 \\rfloor+1}\\right)$.\n  3. The optimal next-period capital at the lowest capital and lowest shock, i.e., $k'(k_{\\min}, z_{\\min})$.\n  4. A boolean indicating whether, for each fixed shock $z$, the policy $k'(k,z)$ is weakly increasing in $k$ across the entire capital grid.\n  5. The maximum absolute Euler equation residual over interior states (excluding states where the policy at $(k,z)$ selects the lowest or highest capital grid point, or where any implied consumption is nonpositive), where the residual at a state $(k,z)$ is defined as\n     $$\n     \\left| 1 - \\dfrac{\\beta \\, \\mathbb{E}\\!\\left[ u'(c') \\left( \\alpha z' k'^{\\alpha - 1} + 1 - \\delta \\right) \\,\\big|\\, z \\right]}{u'(c)} \\right|,\n     $$\n     with $c = z k^{\\alpha} + (1-\\delta)k - k'$, $k' = k'(k,z)$, $c' = z' k'^{\\alpha} + (1-\\delta)k' - k''$, and $k'' = k'(k', z')$. Here $u'(c)$ denotes the marginal utility of consumption.\n\nUse the following test suite. For each test case, use the specified parameters, shock grid, transition matrix, and capital grid:\n\n- Test Case A (baseline):\n  - $\\beta = 0.96$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.08$.\n  - Shock grid: $N_z = 3$ with $z = [0.9, 1.0, 1.1]$.\n  - Transition matrix\n    $$\n    P = \\begin{bmatrix}\n    0.90 & 0.09 & 0.01 \\\\\n    0.09 & 0.82 & 0.09 \\\\\n    0.01 & 0.09 & 0.90\n    \\end{bmatrix}.\n    $$\n  - Capital grid: $N_k = 80$, $k_{\\min} = 0.01$, $k_{\\max} = 3.0$.\n\n- Test Case B (lower patience and more volatile shocks):\n  - $\\beta = 0.90$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.08$.\n  - Shock grid: $N_z = 3$ with $z = [0.8, 1.0, 1.2]$.\n  - Transition matrix\n    $$\n    P = \\begin{bmatrix}\n    0.85 & 0.10 & 0.05 \\\\\n    0.10 & 0.80 & 0.10 \\\\\n    0.05 & 0.10 & 0.85\n    \\end{bmatrix}.\n    $$\n  - Capital grid: $N_k = 60$, $k_{\\min} = 0.01$, $k_{\\max} = 3.0$.\n\n- Test Case C (higher patience and lower depreciation):\n  - $\\beta = 0.985$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.02$.\n  - Shock grid: $N_z = 3$ with $z = [0.95, 1.0, 1.05]$.\n  - Transition matrix\n    $$\n    P = \\begin{bmatrix}\n    0.92 & 0.07 & 0.01 \\\\\n    0.07 & 0.86 & 0.07 \\\\\n    0.01 & 0.07 & 0.92\n    \\end{bmatrix}.\n    $$\n  - Capital grid: $N_k = 100$, $k_{\\min} = 0.01$, $k_{\\max} = 4.0$.\n\nStopping criterion and numerical conventions:\n- Initialize the value function arbitrarily on $\\mathcal{K} \\times \\mathcal{Z}$.\n- Iterate on the Bellman equation until the sup-norm distance between successive value function iterates is less than $\\varepsilon = 10^{-6}$, or until a maximum of $N_{\\text{iter},\\max} = 1000$ iterations is reached.\n- When computing utilities, treat any consumption $c \\le 0$ as infeasible. You must enforce $c \\ge 0$; any infeasible choice must not be selected in the maximization.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with three sublists, one per test case in the order A, B, C. Each sublist must contain the five quantities in the order specified above: $[\\text{iterations}, V_{\\text{mid}}, k'_{\\text{min,low}~z}, \\text{is\\_monotone}, \\text{max\\_Euler\\_residual}]$.\n- All floating-point numbers must be rounded to six decimal places; the boolean must be either True or False.\n- Concretely, the output must look like\n  $$\n  \\big[ [n_A, v_A, k_A, b_A, e_A], [n_B, v_B, k_B, b_B, e_B], [n_C, v_C, k_C, b_C, e_C] \\big],\n  $$\n  printed as a single line with brackets and commas exactly as shown, where $n_\\cdot$ are integers, $v_\\cdot, k_\\cdot, e_\\cdot$ are floats rounded to six decimals, and $b_\\cdot$ are booleans.", "solution": "The problem is a stationary, infinite-horizon dynamic program with a compact, discretized state space and a bounded reward function on the feasible set. The Bellman operator $T$ mapping bounded functions into bounded functions is defined pointwise for $(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}$ by\n$$\n(TV)(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\!\\left(z k^{\\alpha} + (1 - \\delta)k - k' \\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} \\mathbb{P}(z' \\mid z) V(k', z') \\right\\}.\n$$\nBy Blackwell's sufficient conditions, $T$ is a contraction mapping on the space of bounded functions under the sup norm: it is monotone because if $V \\le W$ then $TV \\le TW$, and it satisfies discounting since $(TV + a)(k,z) \\le (TV)(k,z) + \\beta a$ for any constant $a$, with modulus $\\beta \\in (0,1)$. Therefore, $T$ has a unique fixed point $V^\\star$, and for any initial $V_0$, the sequence $V_{n+1} = TV_n$ converges to $V^\\star$ in the sup norm.\n\nTo compute the fixed point numerically, we discretize both the capital state and the decision $k'$ on the same finite grid $\\mathcal{K} = \\{k_1,\\dots,k_{N_k}\\}$, and the exogenous shock space on $\\mathcal{Z} = \\{z_1,\\dots,z_{N_z}\\}$. For each shock $z_j$ and each current capital $k_i$, we enumerate all candidate next-period capitals $k' \\in \\mathcal{K}$, compute the implied consumption $c = z_j k_i^{\\alpha} + (1 - \\delta) k_i - k'$, discard infeasible candidates with $c \\le 0$, evaluate period utility $u(c)$, and add the discounted expected continuation value $\\beta \\sum_{m=1}^{N_z} P_{j m} V(k', z_m)$. The maximizer over $k'$ yields the updated value and the corresponding policy index. This construction delivers the Bellman operator applied to a value function iterate in a vectorized manner. Repeating this update until the sup-norm distance between successive iterates falls below $\\varepsilon = 10^{-6}$ (or until the iteration cap is reached) yields an $\\varepsilon$-accurate approximation to $V^\\star$ and its associated policy function $k'(k,z)$.\n\nOnce the policy is obtained, we evaluate additional diagnostics:\n\n- The value at the median indices: Let $i^\\star = \\left\\lfloor \\dfrac{N_k - 1}{2} \\right\\rfloor + 1$ and $j^\\star = \\left\\lfloor \\dfrac{N_z - 1}{2} \\right\\rfloor + 1$. Report $V(k_{i^\\star}, z_{j^\\star})$.\n\n- The optimal next-period capital at the boundary state $(k_{\\min}, z_{\\min})$: Report $k'(k_1, z_1)$.\n\n- Monotonicity of the policy in capital: For each fixed shock $z_j$, verify whether the mapping $i \\mapsto k'(k_i, z_j)$ is weakly increasing for $i = 1,\\dots,N_k$. This property is theoretically supported by the monotonicity and concavity of the Bellman operator in standard concave growth models, although numerical discretization can introduce minor violations. We compute the boolean by checking all adjacent pairs.\n\n- The Euler equation residual: The first-order condition for interior solutions asserts\n$$\nu'(c_t) = \\beta \\, \\mathbb{E} \\left[ u'(c_{t+1}) \\left( \\alpha z_{t+1} k_{t+1}^{\\alpha - 1} + 1 - \\delta \\right) \\bigg| z_t \\right].\n$$\nUsing the discrete policy $k' = k'(k,z)$ and the Markov chain over $z$, we approximate the residual at each interior state $(k_i, z_j)$ as\n$$\nR(k_i,z_j) = \\left| 1 - \\dfrac{\\beta \\sum_{m=1}^{N_z} P_{j m} \\, u'(c'(k_i,z_j; z_m)) \\, \\left( \\alpha z_m \\, k'(k_i,z_j)^{\\alpha - 1} + 1 - \\delta \\right)}{u'(c(k_i,z_j))} \\right|,\n$$\nwhere $c(k_i,z_j) = z_j k_i^{\\alpha} + (1 - \\delta) k_i - k'(k_i,z_j)$ and $c'(k_i,z_j; z_m) = z_m k'(k_i,z_j)^{\\alpha} + (1 - \\delta) k'(k_i,z_j) - k''$, with $k'' = k'(k'(k_i,z_j), z_m)$. We exclude states where the policy chooses the lowest or highest $k'$ (to avoid corner solutions) and any states where $c$ or $c'$ is nonpositive. The maximum residual across the remaining states measures the global quality of the discrete policy relative to the Euler condition.\n\nNumerical details:\n- For CRRA with $\\sigma \\neq 1$, $u'(c) = c^{-\\sigma}$. For $\\sigma = 1$, one can use $u(c) = \\log(c)$ and $u'(c) = 1/c$; the implementation accommodates both cases.\n- Feasibility $c \\ge 0$ is enforced by assigning a value of negative infinity to infeasible choices in the maximization step, ensuring they are never selected.\n- The expected continuation value for a fixed current shock $z_j$ and candidate $k'$ is computed as a row-wise product of $P$ with the stacked value function over $z'$ evaluated at $k'$.\n\nApplying this procedure to each of the three specified test cases with their given parameters, shock grids, transition matrices, and capital grids yields, for each case, the five requested outputs: the iteration count, the value at median indices, the policy at $(k_{\\min}, z_{\\min})$, the monotonicity boolean, and the maximum Euler residual. All floating-point outputs are rounded to six decimal places, and the final print is a single-line list structured as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef crra_utility(c, sigma):\n    \"\"\"CRRA utility u(c); returns -inf for nonpositive c to enforce feasibility.\"\"\"\n    c = np.asarray(c)\n    u = np.full_like(c, -np.inf, dtype=np.float64)\n    positive = c > 0\n    if sigma == 1.0:\n        u[positive] = np.log(c[positive])\n    else:\n        u[positive] = (np.power(c[positive], 1.0 - sigma)) / (1.0 - sigma)\n    return u\n\ndef crra_marginal_utility(c, sigma):\n    \"\"\"Marginal utility u'(c) for CRRA; returns nan for nonpositive c.\"\"\"\n    c = np.asarray(c)\n    mu = np.full_like(c, np.nan, dtype=np.float64)\n    positive = c > 0\n    if sigma == 1.0:\n        mu[positive] = 1.0 / c[positive]\n    else:\n        mu[positive] = np.power(c[positive], -sigma)\n    return mu\n\ndef vfi(k_grid, z_grid, P, beta, sigma, alpha, delta, tol=1e-6, max_iter=1000):\n    \"\"\"\n    Perform value function iteration on discretized grids.\n    Returns:\n        V: value function array shape (Nz, Nk)\n        policy_idx: optimal k' indices shape (Nz, Nk)\n        iters: iterations performed\n    \"\"\"\n    Nz = len(z_grid)\n    Nk = len(k_grid)\n    V = np.zeros((Nz, Nk), dtype=np.float64)\n    V_new = np.empty_like(V)\n    policy_idx = np.zeros((Nz, Nk), dtype=np.int64)\n\n    # Precompute resources y[j, i] = z_j * k_i^alpha + (1 - delta) * k_i\n    k_alpha = np.power(k_grid, alpha)\n    y = (z_grid[:, None] * k_alpha[None, :]) + (1.0 - delta) * k_grid[None, :]\n\n    # For each shock z_j, continuation value over k' is beta * P[j] @ V_old\n    iters = 0\n    diff = np.inf\n    neg_inf = -1.0e12  # numerical -inf surrogate for infeasible choices\n    while iters < max_iter and diff > tol:\n        # Expected continuation values for choosing each k' at current shock j\n        # EV has shape (Nz, Nk): for each j, EV[j, k_idx] = beta * sum_m P[j, m] * V[m, k_idx]\n        EV = beta * (P @ V)\n        # Loop over shocks to form return matrices and maximize w.r.t k'\n        for j in range(Nz):\n            # consumption matrix for current shock j: C[i, kprime] = y[j,i] - k_grid[kprime]\n            C = y[j, :, None] - k_grid[None, :]\n            U = crra_utility(C, sigma)\n            # mask infeasible as neg_inf to avoid NaNs in max\n            U[~np.isfinite(U)] = neg_inf\n            # add continuation value for each k' (broadcast across current k_i)\n            # total return W[i, kprime]\n            W = U + EV[j][None, :]\n            # maximize over k' axis=1\n            pol_idx = np.argmax(W, axis=1)\n            V_new[j, :] = W[np.arange(Nk), pol_idx]\n            policy_idx[j, :] = pol_idx\n\n        diff = np.max(np.abs(V_new - V))\n        V[:, :] = V_new\n        iters += 1\n\n    return V, policy_idx, iters\n\ndef policy_monotone(policy_idx):\n    \"\"\"\n    Check monotonicity of policy in k for each z: nondecreasing indices across k.\n    Returns True if monotone for all z, else False.\n    \"\"\"\n    Nz, Nk = policy_idx.shape\n    for j in range(Nz):\n        if np.any(np.diff(policy_idx[j, :]) < 0):\n            return False\n    return True\n\ndef euler_residual_max(k_grid, z_grid, P, beta, sigma, alpha, delta, policy_idx):\n    \"\"\"\n    Compute the maximum absolute Euler residual over interior states.\n    Excludes states where policy picks boundary k' or any implied c or c' is nonpositive.\n    Residual at (k_i, z_j): |1 - RHS / LHS| with LHS = u'(c), RHS = beta * E[u'(c') * (alpha z' k'^{alpha-1} + 1 - delta)].\n    \"\"\"\n    Nz = len(z_grid)\n    Nk = len(k_grid)\n    # Precompute\n    k_alpha = np.power(k_grid, alpha)\n    y = (z_grid[:, None] * k_alpha[None, :]) + (1.0 - delta) * k_grid[None, :]\n\n    max_resid = 0.0\n    # Iterate over states\n    for j in range(Nz):\n        for i in range(Nk):\n            kp_idx = policy_idx[j, i]\n            # exclude boundary policies\n            if kp_idx == 0 or kp_idx == Nk - 1:\n                continue\n            k = k_grid[i]\n            z = z_grid[j]\n            kp = k_grid[kp_idx]\n            # Current consumption\n            c = z * (k ** alpha) + (1.0 - delta) * k - kp\n            if c <= 0:\n                continue\n            mu_c = (1.0 / c) if sigma == 1.0 else c ** (-sigma)\n            # Next period marginal utility and returns term\n            # For each z' compute c' and k'' = policy(k', z')\n            kp_alpha = kp ** alpha\n            # Precompute gross return at next period given z'\n            R_terms = alpha * z_grid * (kp ** (alpha - 1.0)) + (1.0 - delta)\n            # Compute c' for each z'\n            y_next = z_grid * kp_alpha + (1.0 - delta) * kp  # shape (Nz,)\n            # For each z' state m, need k'' index policy_idx[m, kp_idx]\n            kpp_idx_vec = policy_idx[:, kp_idx]  # shape (Nz,)\n            kpp_vec = k_grid[kpp_idx_vec]        # shape (Nz,)\n            c_next = y_next - kpp_vec            # shape (Nz,)\n            # Exclude if any c' is nonpositive by setting their marginal utility to nan and ignoring via weights\n            mu_c_next = crra_marginal_utility(c_next, sigma)  # nan where nonpositive\n            # Expected RHS conditional on current z_j using P[j, :]\n            valid = np.isfinite(mu_c_next)\n            if not np.any(valid):\n                continue\n            RHS = beta * np.sum(P[j, valid] * mu_c_next[valid] * R_terms[valid])\n            resid = abs(1.0 - RHS / mu_c)\n            if np.isfinite(resid):\n                if resid > max_resid:\n                    max_resid = resid\n    return float(max_resid)\n\ndef run_test_case(params):\n    beta = params[\"beta\"]\n    sigma = params[\"sigma\"]\n    alpha = params[\"alpha\"]\n    delta = params[\"delta\"]\n    z = np.array(params[\"z\"], dtype=np.float64)\n    P = np.array(params[\"P\"], dtype=np.float64)\n    Nk = params[\"Nk\"]\n    k_min = params[\"k_min\"]\n    k_max = params[\"k_max\"]\n    k_grid = np.linspace(k_min, k_max, Nk, dtype=np.float64)\n\n    V, pol_idx, iters = vfi(k_grid, z, P, beta, sigma, alpha, delta, tol=1e-6, max_iter=1000)\n\n    # Median indices\n    mid_k_idx = (Nk - 1) // 2\n    mid_z_idx = (len(z) - 1) // 2\n    V_mid = V[mid_z_idx, mid_k_idx]\n    # Policy at (k_min, z_min)\n    kp_min_lowz = k_grid[pol_idx[0, 0]]\n    # Monotonicity\n    is_mono = policy_monotone(pol_idx)\n    # Euler residual\n    max_euler_resid = euler_residual_max(k_grid, z, P, beta, sigma, alpha, delta, pol_idx)\n\n    # Round floats to six decimals\n    result = [\n        int(iters),\n        round(float(V_mid), 6),\n        round(float(kp_min_lowz), 6),\n        bool(is_mono),\n        round(float(max_euler_resid), 6),\n    ]\n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"beta\": 0.96,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.08,\n            \"z\": [0.9, 1.0, 1.1],\n            \"P\": [\n                [0.90, 0.09, 0.01],\n                [0.09, 0.82, 0.09],\n                [0.01, 0.09, 0.90],\n            ],\n            \"Nk\": 80,\n            \"k_min\": 0.01,\n            \"k_max\": 3.0,\n        },\n        {\n            \"name\": \"B\",\n            \"beta\": 0.90,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.08,\n            \"z\": [0.8, 1.0, 1.2],\n            \"P\": [\n                [0.85, 0.10, 0.05],\n                [0.10, 0.80, 0.10],\n                [0.05, 0.10, 0.85],\n            ],\n            \"Nk\": 60,\n            \"k_min\": 0.01,\n            \"k_max\": 3.0,\n        },\n        {\n            \"name\": \"C\",\n            \"beta\": 0.985,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.02,\n            \"z\": [0.95, 1.0, 1.05],\n            \"P\": [\n                [0.92, 0.07, 0.01],\n                [0.07, 0.86, 0.07],\n                [0.01, 0.07, 0.92],\n            ],\n            \"Nk\": 100,\n            \"k_min\": 0.01,\n            \"k_max\": 4.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure booleans and numbers are printed without extra spaces.\n    def format_item(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        elif isinstance(x, (int, np.integer)):\n            return str(int(x))\n        elif isinstance(x, float) or isinstance(x, np.floating):\n            # Ensure fixed rounding to 6 decimals; preserve trailing zeros\n            return f\"{x:.6f}\"\n        else:\n            return str(x)\n\n    formatted = []\n    for res in results:\n        formatted.append(\"[\" + \",\".join(format_item(x) for x in res) + \"]\")\n    print(\"[\" + \",\".join(formatted) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}, {"introduction": "Economic models gain realism when their constraints reflect the complexities of the real world. For instance, an individual's ability to borrow money often depends on their current income or wealth. This practice challenges you to adapt the basic VFI algorithm to handle such state-dependent constraints [@problem_id:2446389]. You will implement a solution where the set of feasible actions, $\\mathcal{A}(s)$, changes with the current state $s$, developing a crucial skill for building more nuanced and practical dynamic models.", "id": "2446389", "problem": "Consider an infinite-horizon consumption-savings problem with discrete assets and two income states. The state is a pair $s = (a, z)$ where $a$ is current assets chosen from a finite grid and $z \\in \\{z_{L}, z_{H}\\}$ is the current income realization. The per-period utility is $u(c) = \\log(c)$, where $c$ is consumption. The resource constraint is\n$$\nc + a' = (1 + r)a + z,\n$$\nwith the borrowing constraint depending on the current income state,\n$$\na' \\ge \\underline{b}(z),\n$$\nand the feasibility requirement $c > 0$. The transition for the exogenous income process is Markov with transition matrix $P$, where $P_{ij} = \\Pr(z' = j \\mid z = i)$. The objective is to maximize expected discounted utility with discount factor $\\beta \\in (0,1)$, and the optimal value function $V(a,z)$ satisfies the Bellman equation\n$$\nV(a,z) = \\max_{a' \\in \\mathcal{A}(z)} \\left\\{ \\log\\left((1+r)a + z - a'\\right) + \\beta \\sum_{z' \\in \\{z_L, z_H\\}} P(z,z') V(a', z') \\right\\},\n$$\nwhere the feasible action set depends on the state via\n$$\n\\mathcal{A}(z) = \\left\\{ a' \\in \\mathcal{G}_A \\,:\\, a' \\ge \\underline{b}(z) \\text{ and } (1+r)a + z - a' > 0 \\right\\},\n$$\nand $\\mathcal{G}_A$ is the given asset grid. All choices are restricted to the grid $\\mathcal{G}_A$.\n\nYour task is to compute, for each parameter set below, the optimal value $V(a^{\\star}, z^{\\star})$ and the corresponding optimal next asset choice $a'^{\\star}$ at the specified query state $(a^{\\star}, z^{\\star})$, under the model defined above. If multiple maximizers exist, select the smallest $a'^{\\star}$ among them. All numerical outputs must be rounded to $6$ decimal places.\n\nTest Suite:\n- Case $1$:\n  - Asset grid $\\mathcal{G}_A = \\{-0.5,\\,-0.25,\\,0.0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\}$.\n  - Income values $(z_L, z_H) = (1.0,\\,2.0)$.\n  - Transition matrix $P = \\begin{bmatrix}0.9 & 0.1\\\\ 0.2 & 0.8\\end{bmatrix}$, where the first row corresponds to $z_L$.\n  - Borrowing limits $(\\underline{b}(z_L), \\underline{b}(z_H)) = (0.0,\\,-0.5)$.\n  - Discount factor $\\beta = 0.95$.\n  - Interest rate $r = 0.04$.\n  - Query state $(a^{\\star}, z^{\\star}) = (0.25,\\, z_H)$.\n- Case $2$:\n  - Asset grid $\\mathcal{G}_A = \\{-0.5,\\,-0.25,\\,0.0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\}$.\n  - Income values $(z_L, z_H) = (1.0,\\,2.0)$.\n  - Transition matrix $P = \\begin{bmatrix}0.9 & 0.1\\\\ 0.2 & 0.8\\end{bmatrix}$.\n  - Borrowing limits $(\\underline{b}(z_L), \\underline{b}(z_H)) = (0.0,\\,-0.5)$.\n  - Discount factor $\\beta = 0.95$.\n  - Interest rate $r = 0.04$.\n  - Query state $(a^{\\star}, z^{\\star}) = (-0.25,\\, z_L)$.\n- Case $3$:\n  - Asset grid $\\mathcal{G}_A = \\{-0.25,\\,0.0,\\,0.25,\\,0.5,\\,0.75\\}$.\n  - Income values $(z_L, z_H) = (0.5,\\,1.5)$.\n  - Transition matrix $P = \\begin{bmatrix}0.95 & 0.05\\\\ 0.1 & 0.9\\end{bmatrix}$.\n  - Borrowing limits $(\\underline{b}(z_L), \\underline{b}(z_H)) = (0.0,\\,-0.25)$.\n  - Discount factor $\\beta = 0.96$.\n  - Interest rate $r = 0.02$.\n  - Query state $(a^{\\star}, z^{\\star}) = (0.0,\\, z_L)$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets. For each case, report two values in order: the optimal value $V(a^{\\star}, z^{\\star})$ and the corresponding optimal next asset $a'^{\\star}$, each rounded to $6$ decimal places. Aggregate all three cases sequentially into a single flat list. For example, the output format must be\n$$\n[\\text{V1},\\text{a1\\_next},\\text{V2},\\text{a2\\_next},\\text{V3},\\text{a3\\_next}],\n$$\nwhere each item is a decimal number rounded to $6$ decimal places.", "solution": "The problem presented is a standard finite-state, infinite-horizon, discrete-time dynamic programming problem, a cornerstone of modern quantitative economics. We are tasked with finding the solution to an agent's consumption-savings problem by computing the value function and associated policy function. The prescribed method for such a problem is Value Function Iteration (VFI).\n\nThe state of the system is defined by the pair $s = (a, z)$, where $a$ is the asset level from a discrete grid $\\mathcal{G}_A$ and $z$ is the stochastic income realization from a discrete set $\\{z_L, z_H\\}$. The agent's objective is to choose a sequence of next-period asset holdings, $a'$, to maximize the expected sum of discounted single-period utilities, given by the logarithmic utility function $u(c) = \\log(c)$.\n\nThe core of the problem lies in solving the Bellman equation, which is a functional equation that the optimal value function $V(a,z)$ must satisfy:\n$$V(a,z) = \\max_{a' \\in \\mathcal{A}(a,z)} \\left\\{ \\log\\left((1+r)a + z - a'\\right) + \\beta \\sum_{z' \\in \\{z_L, z_H\\}} P(z,z') V(a', z') \\right\\}$$\nHere, the set of feasible choices for next-period assets, $\\mathcal{A}(a,z)$, is constrained by the agent's borrowing limit $\\underline{b}(z)$ and the necessity of positive consumption, $c > 0$. The available cash-on-hand is $(1+r)a + z$, which determines the upper bound for consumption and thus for the choice of $a'$.\n\nThe Bellman equation defines a mapping, let us call it the Bellman operator $T$, which takes a candidate value function $V_k$ and returns an updated function $V_{k+1} = T(V_k)$. For a discount factor $\\beta \\in (0,1)$, this operator is a contraction mapping with respect to the supremum norm. The Banach fixed-point theorem consequently guarantees that there exists a unique fixed point $V^\\star$ such that $V^\\star = T(V^\\star)$. Furthermore, iterating the operator from an arbitrary initial bounded function $V_0$ will converge to this unique solution: $\\lim_{k \\to \\infty} T^k(V_0) = V^\\star$.\n\nThe algorithm for Value Function Iteration proceeds as follows:\n\n1.  **Initialization**: The state space is discretized by the given asset grid $\\mathcal{G}_A$ and income states $\\{z_L, z_H\\}$. This results in a finite number of states $(a_i, z_j)$. We represent the value function as a matrix $\\mathbf{V}$ of size $N_a \\times N_z$, where $N_a$ is the number of points in $\\mathcal{G}_A$ and $N_z=2$. We initialize this matrix, for example, with all zeros: $V_0(a,z) = 0$ for all $(a,z)$. We also define a small tolerance threshold $\\epsilon > 0$ for convergence.\n\n2.  **Iteration**: We repeat the following update step for each iteration $k=0, 1, 2, \\dots$ until convergence. For each state $(a_i, z_j)$ in our state space:\n    a. We compute the right-hand side of the Bellman equation using the current value function approximation $V_k$. This involves a maximization problem. For each possible choice of next-period assets $a'_l \\in \\mathcal{G}_A$:\n        i. We first check if the choice $a'_l$ is feasible. It must satisfy the state-dependent borrowing limit, $a'_l \\ge \\underline{b}(z_j)$, and ensure positive consumption, $c_l = (1+r)a_i + z_j - a'_l > 0$.\n        ii. If feasible, we calculate the value of this choice, which is the sum of current utility and the discounted expected continuation value:\n            $$W(a_i, z_j, a'_l) = \\log(c_l) + \\beta \\left( P(z_j, z_L) V_k(a'_l, z_L) + P(z_j, z_H) V_k(a'_l, z_H) \\right)$$\n    b. The updated value for state $(a_i, z_j)$ is the maximum over all such feasible choices:\n        $$V_{k+1}(a_i, z_j) = \\max_{a'_l \\in \\mathcal{A}(a_i,z_j)} W(a_i, z_j, a'_l)$$\n    c. We also store the choice $a'_l$ that achieves this maximum. This forms the policy function for the next iteration, $g_{k+1}(a_i, z_j)$. The problem specifies that in case of a tie between multiple maximizers, the smallest value of $a'$ should be chosen.\n\n3.  **Convergence Check**: After updating the value for all states, we obtain a new value function matrix $V_{k+1}$. We check for convergence by measuring the distance between successive iterates, using the supremum norm:\n    $$\\text{dist} = \\max_{i,j} | V_{k+1}(a_i, z_j) - V_k(a_i, z_j) |$$\n    If $\\text{dist} < \\epsilon$, the process is terminated. The final matrix $V_{k+1}$ is our numerical approximation of the true value function $V^\\star$, and the associated policy function $g_{k+1}$ is the optimal policy $g^\\star$.\n\n4.  **Reporting Results**: Once the algorithm has converged, we identify the indices corresponding to the query state $(a^\\star, z^\\star)$ for each test case. We then extract the converged value $V^\\star(a^\\star, z^\\star)$ and the optimal policy choice $a'^\\star = g^\\star(a^\\star, z^\\star)$ from our final computed matrices.\n\nThis numerical procedure will be implemented for each of the three supplied parameter sets. The core logic is identical, only the parameters differ between cases. The computation is executed using matrix operations for efficiency, particularly for calculating the expected continuation value across all possible next-period asset choices. The final numerical results are then rounded to $6$ decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef run_vfi(params):\n    \"\"\"\n    Solves the consumption-savings problem using Value Function Iteration.\n    \"\"\"\n    asset_grid, z_values, P, b_limits, beta, r, query_state = params\n    a_star, z_star_val = query_state\n\n    n_a = len(asset_grid)\n    n_z = len(z_values)\n\n    # Map query state to indices\n    try:\n        a_star_idx = np.where(asset_grid == a_star)[0][0]\n        z_star_idx = np.where(z_values == z_star_val)[0][0]\n    except IndexError:\n        raise ValueError(\"Query state not found in grids.\")\n\n    # VFI setup\n    V = np.zeros((n_a, n_z))\n    policy = np.zeros((n_a, n_z), dtype=int)\n    \n    max_iter = 5000\n    tol = 1e-9\n\n    for i in range(max_iter):\n        V_old = V.copy()\n        \n        # E[V(a',z') | z] for all a' and z\n        # (V_old @ P.T)[i, j] = sum_k V_old[i, k] * P.T[k, j] = sum_k V_old(a_i, z_k) * P(z_k | z_j)\n        expected_cont_value_matrix = V_old @ P.T\n\n        for z_idx, z_val in enumerate(z_values):\n            # E[V(a',z') | z_j] for all a' for this specific z_j\n            expected_cont_value_vector = expected_cont_value_matrix[:, z_idx]\n            borrowing_limit = b_limits[z_idx]\n\n            for a_idx, a_val in enumerate(asset_grid):\n                cash_on_hand = (1 + r) * a_val + z_val\n                consumption = cash_on_hand - asset_grid\n                \n                feasible_mask = (consumption > 0) & (asset_grid >= borrowing_limit)\n                \n                value_options = -np.inf * np.ones(n_a)\n                \n                if np.any(feasible_mask):\n                    utility = np.log(consumption[feasible_mask])\n                    cont_vals = beta * expected_cont_value_vector[feasible_mask]\n                    value_options[feasible_mask] = utility + cont_vals\n\n                V[a_idx, z_idx] = np.max(value_options)\n                # np.argmax returns the index of the first max value. Since asset_grid\n                # is sorted, this correctly implements the tie-breaking rule.\n                policy[a_idx, z_idx] = np.argmax(value_options)\n    \n        dist = np.max(np.abs(V - V_old))\n        if dist < tol:\n            break\n\n    # After convergence, extract and round results\n    optimal_value = V[a_star_idx, z_star_idx]\n    optimal_policy_idx = policy[a_star_idx, z_star_idx]\n    optimal_asset_choice = asset_grid[optimal_policy_idx]\n    \n    return optimal_value, optimal_asset_choice\n\ndef solve():\n    test_cases = [\n        # Case 1\n        (\n            np.array([-0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]),\n            np.array([1.0, 2.0]),\n            np.array([[0.9, 0.1], [0.2, 0.8]]),\n            np.array([0.0, -0.5]),\n            0.95,\n            0.04,\n            (0.25, 2.0)\n        ),\n        # Case 2\n        (\n            np.array([-0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0]),\n            np.array([1.0, 2.0]),\n            np.array([[0.9, 0.1], [0.2, 0.8]]),\n            np.array([0.0, -0.5]),\n            0.95,\n            0.04,\n            (-0.25, 1.0)\n        ),\n        # Case 3\n        (\n            np.array([-0.25, 0.0, 0.25, 0.5, 0.75]),\n            np.array([0.5, 1.5]),\n            np.array([[0.95, 0.05], [0.1, 0.9]]),\n            np.array([0.0, -0.25]),\n            0.96,\n            0.02,\n            (0.0, 0.5)\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        v, a_next = run_vfi(case)\n        results.append(round(v, 6))\n        results.append(round(a_next, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"}, {"introduction": "While VFI is a robust and reliable tool, is it always the most efficient? This final practice introduces a powerful alternative: Policy Function Iteration (PFI), also known as Howard's Policy Improvement Algorithm. Instead of repeatedly maximizing, PFI alternates between improving the policy rule and fully evaluating the value of that fixed rule. This exercise [@problem_id:2446390] asks you to implement both algorithms and compare their computational costs, moving you from simply solving a model to thinking critically about the most effective numerical strategy to do so.", "id": "2446390", "problem": "Consider the infinite-horizon, deterministic, discrete-state, discrete-choice capital accumulation problem defined as follows. The state is capital $k \\in \\mathcal{K}$, where $\\mathcal{K}$ is a finite grid on an interval $[k_{\\min}, k_{\\max}]$. The per-period utility is $u(c) = \\log(c)$ for $c > 0$ and $u(c) = -\\infty$ otherwise. Production is $f(k) = z k^{\\alpha}$ with $z = 1$, capital depreciates at rate $\\delta \\in (0,1)$, and the discount factor is $\\beta \\in (0,1)$. The law of motion is\n$$\nk' \\in \\mathcal{K}, \\quad c = f(k) + (1-\\delta)k - k'.\n$$\nLet the Bellman equation be\n$$\nV(k) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V(k') \\right\\}.\n$$\nUse the following discrete approximation protocol:\n- The grid $\\mathcal{K}$ is a set of $N_k$ evenly spaced points on $[k_{\\min}, k_{\\max}]$.\n- Define the deterministic steady state $k^{\\star}$ by the first-order condition $f'(k^{\\star}) = \\frac{1}{\\beta} - 1 + \\delta$, i.e.,\n$$\n\\alpha (k^{\\star})^{\\alpha-1} = \\frac{1}{\\beta} - 1 + \\delta, \\quad \\text{so} \\quad k^{\\star} = \\left(\\frac{\\alpha}{\\frac{1}{\\beta} - 1 + \\delta}\\right)^{\\frac{1}{1-\\alpha}}.\n$$\n- For each test case, the interval endpoints are $k_{\\min} = a \\cdot k^{\\star}$ and $k_{\\max} = b \\cdot k^{\\star}$ with given $a$ and $b$, and $\\mathcal{K}$ is the uniform grid on this interval with $N_k$ points.\n- The initial value function is $V_0(k) = 0$ for all $k \\in \\mathcal{K}$.\n- Infeasible choices with $c \\le 0$ are evaluated as $u(c) = -\\infty$ and are therefore never optimal.\n\nDefine two iteration methods to compute an approximate fixed point of the Bellman equation, both using the same tolerance $\\varepsilon$ and the same uniform grid:\n- Method A (full maximization at every iteration): Construct the sequence $\\{V_n\\}_{n \\ge 0}$ by $V_{n+1}(k) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V_n(k') \\right\\}$ for all $k \\in \\mathcal{K}$. Stop at the smallest integer $n \\ge 0$ such that $\\|V_{n+1} - V_n\\|_{\\infty} < \\varepsilon$. Let $M_A$ be the number of iterations performed, which equals the number of full state-wise maximization sweeps applied.\n- Method B (alternating improvement and evaluation): Starting from $V_0$, repeat the following until convergence. First, for each $k \\in \\mathcal{K}$, compute a decision rule $\\pi(k) \\in \\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V(k') \\right\\}$. Then, holding $\\pi$ fixed, perform exactly $m$ policy evaluation sweeps, each sweep updating $V$ synchronously by $V_{\\text{new}}(k) = u\\big(f(k) + (1-\\delta)k - \\pi(k)\\big) + \\beta V\\big(\\pi(k)\\big)$ for all $k \\in \\mathcal{K}$. After every block of $m$ evaluation sweeps, test convergence by comparing the current value function to the one at the start of the block, and stop when the $\\ell_{\\infty}$ distance is less than $\\varepsilon$. Let $M_B$ be the total number of improvement steps performed, which equals the number of times the decision rule $\\pi$ is recomputed (and therefore the number of full state-wise maximization sweeps).\n\nFor each test case below, compute and report the tuple containing:\n- $M_A$,\n- $M_B$,\n- a boolean indicating whether $M_B < M_A$,\n- the $\\ell_{\\infty}$ distance between the final value functions obtained by Method A and Method B.\n\nAll logarithms are natural. The convergence tolerance is $\\varepsilon = 10^{-5}$. If an algorithm does not converge within $I_{\\max}$ outer iterations, terminate it and return the number of iterations performed up to termination. Use $I_{\\max} = 10{,}000$ for Method A, and $I_{\\max} = 10{,}000$ improvement steps for Method B. Angles do not appear in this problem. There are no physical units to report.\n\nTest Suite:\n- Case $1$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.33, 0.96, 0.08, 200, 0.5, 1.5, 20)$.\n- Case $2$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.36, 0.99, 0.025, 300, 0.25, 1.75, 25)$.\n- Case $3$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.30, 0.95, 0.10, 80, 0.30, 1.70, 1)$.\n- Case $4$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.40, 0.97, 0.05, 250, 0.40, 1.60, 50)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order $[M_A, M_B, \\text{boolean}, \\text{distance}]$. For example, a syntactically valid output looks like\n\"[ [12,3,True,0.00001], [ ... ], [ ... ], [ ... ] ]\" (the specific numbers must be those computed by your program).", "solution": "The problem is a deterministic, infinite-horizon dynamic programming model typically referred to as a discrete-state, discrete-choice version of the neoclassical growth model. The objective is to compute an approximate fixed point of the Bellman equation on a finite grid using two distinct iteration schemes and to compare their performance using a well-defined metric.\n\nFundamental setup:\n- The state space is a finite grid $\\mathcal{K} = \\{k_1, k_2, \\ldots, k_{N_k}\\}$ over $[k_{\\min}, k_{\\max}]$, built from $k_{\\min} = a \\cdot k^{\\star}$ and $k_{\\max} = b \\cdot k^{\\star}$. The steady state $k^{\\star}$ is defined by the stationarity requirement implied by the Euler equation in this deterministic setting: $f'(k^{\\star}) = \\frac{1}{\\beta} - 1 + \\delta$, leading to \n$$\nk^{\\star} = \\left(\\frac{\\alpha}{\\frac{1}{\\beta} - 1 + \\delta}\\right)^{\\frac{1}{1-\\alpha}}.\n$$\n- The Bellman operator $\\mathcal{T}$ on the space of bounded functions $V:\\mathcal{K} \\to \\mathbb{R}$ is\n$$\n(\\mathcal{T}V)(k) = \\max_{k' \\in \\mathcal{K}} \\Big\\{ u\\big( f(k) + (1-\\delta)k - k' \\big) + \\beta V(k') \\Big\\}.\n$$\n- With $u(c) = \\log(c)$ for $c>0$ and $u(c) = -\\infty$ for $c \\le 0$, infeasible choices are automatically eliminated by the maximization. On a finite grid, this operator is a contraction mapping under the sup norm with modulus $\\beta \\in (0,1)$ when rewards are bounded above; thus, iterating the operator from any bounded initial guess converges to the unique fixed point.\n\nDiscrete approximation and precomputation:\n- Since $\\mathcal{K}$ is finite and fixed, one can precompute the matrix of instantaneous utilities $U \\in \\mathbb{R}^{N_k \\times N_k}$ with entries\n$$\nU_{ij} = \\begin{cases}\n\\log\\!\\Big( f(k_i) + (1-\\delta)k_i - k_j \\Big), & \\text{if } f(k_i) + (1-\\delta)k_i - k_j > 0,\\\\\n-\\infty, & \\text{otherwise.}\n\\end{cases}\n$$\n- The Bellman update can then be written in vectorized form as \n$$\nV_{\\text{new}} = \\max_{j \\in \\{1,\\ldots,N_k\\}} \\left\\{ U_{\\cdot j} + \\beta V_j \\right\\},\n$$\nwhere $U_{\\cdot j}$ is the $j$th column of $U$ and $V_j$ is the scalar $V(k_j)$ broadcast across states.\n\nMethod A (full maximization at every iteration):\n- Initialize $V_0(k) = 0$ for all $k \\in \\mathcal{K}$. For $n = 0,1,2,\\ldots$, compute $V_{n+1} = \\mathcal{T} V_n$ using the discrete maximization over all $k' \\in \\mathcal{K}$ at each state. Stop at the smallest $n$ for which $\\|V_{n+1} - V_n\\|_{\\infty} < \\varepsilon$, and set $M_A = n+1$, the count of full maximization sweeps needed.\n\nMethod B (alternating improvement and evaluation):\n- Given a current value $V$, compute an improved decision rule $\\pi:\\mathcal{K}\\to\\mathcal{K}$ by\n$$\n\\pi(k_i) \\in \\arg\\max_{k_j \\in \\mathcal{K}} \\left\\{ U_{ij} + \\beta V(k_j) \\right\\}.\n$$\n- Define the policy evaluation operator associated with $\\pi$ as\n$$\n(\\mathcal{T}_{\\pi}V)(k_i) = U_{i,\\pi(i)} + \\beta V\\big(\\pi(k_i)\\big),\n$$\nwhere $U_{i,\\pi(i)} = u\\big(f(k_i) + (1-\\delta)k_i - \\pi(k_i)\\big)$. Holding $\\pi$ fixed, apply $\\mathcal{T}_{\\pi}$ exactly $m$ times, synchronously updating all states each sweep. This is a contraction with modulus $\\beta$ since it is affine in $V$ with slope $\\beta$ along the mapping induced by $\\pi$.\n- After $m$ sweeps, compare the new value function to the value at the start of this block using the sup norm. If the distance is below $\\varepsilon$, stop. Otherwise, recompute $\\pi$ using the current value, increment the improvement counter, and repeat. The number $M_B$ is the total number of improvement steps, equal to the total number of full maximization sweeps. Since each improvement requires only one maximization across choices, and policy evaluation contains no maximization, this count isolates precisely the expensive maximization operations.\n\nPerformance metric and comparison:\n- The metric $M_A$ reflects the number of full maximization sweeps required by the full maximization iteration to reach the tolerance. The metric $M_B$ reflects the number of full maximization sweeps when interleaving policy evaluation sweeps between improvements. Theory and practice indicate that holding a policy fixed for $m \\ge 1$ sweeps accelerates convergence of the value function between improvements, often reducing the count $M_B$ relative to $M_A$ because improvements are needed less frequently.\n- To verify that both methods are computing the same fixed point, compute the $\\ell_{\\infty}$ distance between their final value functions. With identical discretizations, discount factor $\\beta \\in (0,1)$, and a bounded reward structure on a finite grid, both methods converge to the same fixed point within tolerance, so the distance should be small, typically on the order of $\\varepsilon$.\n\nImplementation details consistent with the statement:\n- Construct $\\mathcal{K}$ using the provided $(\\alpha,\\beta,\\delta,N_k,a,b)$ via $k^{\\star}$ and uniform spacing on $[a k^{\\star}, b k^{\\star}]$.\n- Precompute $U$ safely, assigning a very large negative value in place of $-\\infty$ for numerical maximization while guaranteeing such entries are never selected when feasible entries exist.\n- Use tolerance $\\varepsilon = 10^{-5}$ and iteration caps as specified. For Method B, test convergence after each block of $m$ policy evaluation sweeps.\n- For each test case, record $M_A$, $M_B$, the boolean $(M_B < M_A)$, and the sup norm distance between the final value functions from the two methods, and format the results as a single-line list of lists.\n\nThe expected qualitative outcome across the provided test cases is:\n- For $m > 1$, $M_B$ is typically strictly less than $M_A$, especially when $\\beta$ is close to $1$ and $N_k$ is larger, due to stronger benefits from policy evaluation sweeps.\n- When $m = 1$, the methods become closely related, and $M_B$ is expected to be similar to $M_A$.\n- The sup norm distance between the final value functions is expected to be very small, typically on the order of $\\varepsilon$, confirming that both methods compute the same fixed point within the prescribed tolerance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef steady_state_k(alpha, beta, delta):\n    # k* = (alpha / (1/beta - 1 + delta))^(1/(1-alpha))\n    num = alpha\n    den = (1.0 / beta) - 1.0 + delta\n    kstar = (num / den) ** (1.0 / (1.0 - alpha))\n    return kstar\n\ndef build_grid(kstar, a, b, Nk):\n    kmin = a * kstar\n    kmax = b * kstar\n    return np.linspace(kmin, kmax, Nk)\n\ndef precompute_utility_matrix(kgrid, alpha, delta):\n    # U[i, j] = log(c) if c > 0 else a large negative sentinel\n    # c = k_i^alpha + (1 - delta) * k_i - k'_j\n    k = kgrid\n    kp = kgrid\n    K = k[:, None]            # shape (N, 1)\n    KP = kp[None, :]          # shape (1, N)\n    output = (K ** alpha)\n    C = output + (1.0 - delta) * K - KP\n    # Use a large negative number as -inf sentinel\n    neg_inf = -1.0e12\n    with np.errstate(divide='ignore', invalid='ignore'):\n        U = np.where(C > 0.0, np.log(C), neg_inf)\n    return U  # shape (N, N)\n\ndef method_A_vfi(U, beta, tol, max_iter):\n    \"\"\"\n    Full maximization at every iteration:\n    V_{n+1}(i) = max_j { U[i,j] + beta * V_n[j] }\n    Returns: V, iterations (number of full maximization sweeps)\n    \"\"\"\n    N = U.shape[0]\n    V = np.zeros(N, dtype=float)\n    count = 0\n    for it in range(max_iter):\n        # Compute RHS matrix: U + beta * V_j (broadcast along rows)\n        RHS = U + beta * V[None, :]\n        V_new = RHS.max(axis=1)\n        count += 1\n        diff = np.max(np.abs(V_new - V))\n        V = V_new\n        if diff < tol:\n            break\n    return V, count\n\ndef method_B_policy_improvement(U, beta, tol, m_eval, max_improve):\n    \"\"\"\n    Alternating improvement and evaluation:\n    - Improvement: pi(i) = argmax_j { U[i,j] + beta * V[j] }\n    - Evaluation: V_{t+1}(i) = U[i, pi(i)] + beta * V_t[pi(i)]  (apply m_eval times)\n    Convergence test after each block of m_eval evaluation sweeps:\n    ||V_after - V_before||_inf < tol\n    Returns: V, improvements (number of improvement steps = full maximization sweeps)\n    \"\"\"\n    N = U.shape[0]\n    V = np.zeros(N, dtype=float)\n    improvements = 0\n    for imp in range(max_improve):\n        V_before = V.copy()\n        # Improvement step: one full maximization sweep\n        RHS = U + beta * V[None, :]\n        pi = np.argmax(RHS, axis=1)  # tie-break: first maximizer (np.argmax behavior)\n        # m_eval policy evaluation sweeps with fixed pi\n        idx = np.arange(N)\n        U_pi = U[idx, pi]  # immediate utility under policy\n        for _ in range(m_eval):\n            V = U_pi + beta * V[pi]\n        improvements += 1\n        # Convergence check after the block of evaluations\n        diff = np.max(np.abs(V - V_before))\n        if diff < tol:\n            break\n    return V, improvements\n\ndef solve():\n    # Tolerance and iteration caps\n    tol = 1e-5\n    max_iter_A = 10000\n    max_improve_B = 10000\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (alpha, beta, delta, N_k, a, b, m_eval)\n    test_cases = [\n        (0.33, 0.96, 0.08, 200, 0.5, 1.5, 20),\n        (0.36, 0.99, 0.025, 300, 0.25, 1.75, 25),\n        (0.30, 0.95, 0.10, 80, 0.30, 1.70, 1),\n        (0.40, 0.97, 0.05, 250, 0.40, 1.60, 50),\n    ]\n\n    results = []\n    for (alpha, beta, delta, Nk, a, b, m_eval) in test_cases:\n        # Build grid and utilities\n        kstar = steady_state_k(alpha, beta, delta)\n        kgrid = build_grid(kstar, a, b, Nk)\n        U = precompute_utility_matrix(kgrid, alpha, delta)\n\n        # Method A\n        V_A, it_A = method_A_vfi(U, beta, tol, max_iter_A)\n\n        # Method B\n        V_B, imp_B = method_B_policy_improvement(U, beta, tol, m_eval, max_improve_B)\n\n        # Compare value functions\n        dist = float(np.max(np.abs(V_A - V_B)))\n\n        results.append([it_A, imp_B, imp_B < it_A, dist])\n\n    # Final print statement in the exact required format.\n    # Ensure booleans and floats/ints are properly represented.\n    # Convert nested lists to string representation as required.\n    def format_element(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        elif isinstance(x, float):\n            # Use repr to avoid excessive rounding; ensure it's a valid Python float literal\n            return repr(x)\n        else:\n            return str(x)\n\n    formatted_cases = []\n    for case in results:\n        formatted_cases.append(\"[\" + \",\".join(format_element(x) for x in case) + \"]\")\n    print(\"[\" + \",\".join(formatted_cases) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}]}