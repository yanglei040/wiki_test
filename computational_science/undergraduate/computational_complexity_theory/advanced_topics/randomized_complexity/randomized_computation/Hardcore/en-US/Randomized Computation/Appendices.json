{
    "hands_on_practices": [
        {
            "introduction": "Randomized algorithms offer powerful strategies for tackling computationally hard problems, and this practice explores one of the simplest yet most elegant examples. You will analyze the performance of a completely random truth assignment for the Maximum 2-Satisfiability (MAX-2-SAT) problem . By applying the principle of linearity of expectation, you will discover the surprisingly effective performance guarantee of this naive approach, providing a foundational insight into the power of randomized approximation algorithms.",
            "id": "1441265",
            "problem": "In the field of computational complexity, the Maximum 2-Satisfiability (MAX-2-SAT) problem is a fundamental optimization challenge. An instance of this problem is defined by a set of $n$ Boolean variables, $V = \\{x_1, x_2, \\dots, x_n\\}$, and a collection of $m$ clauses. Each clause is a disjunction (an OR operation) of exactly two literals. A literal is either a variable $x_i$ or its negation $\\neg x_i$. For any given clause, assume its two literals involve distinct variables (e.g., clauses of the form $x_i \\lor \\neg x_i$ or $x_i \\lor x_i$ are excluded). The goal of MAX-2-SAT is to find a truth assignment—a mapping of each variable to TRUE or FALSE—that maximizes the total number of satisfied clauses.\n\nConsider the simplest randomized algorithm for approximating a solution: each variable $x_i \\in V$ is independently assigned a value of TRUE with probability $1/2$ and FALSE with probability $1/2$.\n\nFor an arbitrary MAX-2-SAT instance with $m > 0$ clauses satisfying the stated conditions, what is the expected fraction of clauses that will be satisfied by applying this randomized assignment once? Express your answer as a single, closed-form analytic expression.",
            "solution": "Let a clause be $C=(\\ell_{i} \\lor \\ell_{j})$ where $\\ell_{i}$ is a literal on variable $x_{i}$ and $\\ell_{j}$ is a literal on variable $x_{j}$ with $i \\neq j$. Under the randomized assignment where each $x_{k}$ is independently TRUE with probability $\\frac{1}{2}$ and FALSE with probability $\\frac{1}{2}$, any literal (whether $x_{k}$ or $\\neg x_{k}$) is TRUE with probability $\\frac{1}{2}$ and FALSE with probability $\\frac{1}{2}$. Since the literals involve distinct variables and assignments are independent, the events that $\\ell_{i}$ is FALSE and that $\\ell_{j}$ is FALSE are independent. Therefore,\n$$\nP(\\text{$C$ is unsatisfied}) = P(\\ell_{i}\\ \\text{FALSE and}\\ \\ell_{j}\\ \\text{FALSE}) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}.\n$$\nHence,\n$$\nP(\\text{$C$ is satisfied}) = 1 - \\frac{1}{4} = \\frac{3}{4}.\n$$\nLet $X_{k}$ be the indicator that the $k$-th clause is satisfied. Then $E[X_{k}] = \\frac{3}{4}$ for all $k$, and by linearity of expectation the expected number of satisfied clauses is\n$$\nE\\Big[\\sum_{k=1}^{m} X_{k}\\Big] = \\sum_{k=1}^{m} E[X_{k}] = m \\cdot \\frac{3}{4}.\n$$\nThe expected fraction of satisfied clauses is\n$$\nE\\left[\\frac{1}{m} \\sum_{k=1}^{m} X_{k}\\right] = \\frac{1}{m} \\cdot m \\cdot \\frac{3}{4} = \\frac{3}{4},\n$$\nwhich is independent of the specific instance, provided $m0$.",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        },
        {
            "introduction": "The power of randomized computation often relies on a source of perfectly unbiased random bits, but what if your only source of randomness is flawed? This classic problem , first tackled by John von Neumann, challenges you to analyze a procedure for creating a perfectly fair outcome from a biased source. You will determine the expected number of times the biased source must be used to generate a single unbiased result, a key measure of the algorithm's efficiency and practicality.",
            "id": "1441270",
            "problem": "A deep-space probe is designed for astrobiological analysis on a distant exoplanet. The probe is equipped with a sensor that performs a binary measurement on atmospheric samples. A \"positive\" detection, which we can label 'H', occurs with a constant but unknown probability $p$, where $0  p  1$. A \"negative\" detection, labeled 'T', occurs with probability $1-p$. The probe's internal computer must generate perfectly unbiased random bits (0 or 1, each with probability 1/2) for data encryption, using only the output from this biased sensor.\n\nThe procedure implemented by the probe's software is as follows:\n1.  Take two consecutive readings from the sensor.\n2.  If the two readings are different (HT or TH), a random bit is successfully generated. The sequence HT produces a '0', and the sequence TH produces a '1'.\n3.  If the two readings are the same (HH or TT), the pair is discarded, and the process restarts from step 1. This is repeated until a bit is generated.\n\nYour task is to determine the expected number of sensor readings the probe must perform to generate a single fair random bit. Express your answer as a function of the sensor's positive detection probability, $p$.",
            "solution": "Let each sensor reading be modeled as a Bernoulli trial with success probability $p$ and failure probability $1-p$, and assume successive readings are independent and identically distributed.\n\nIn one trial (one pair of readings), a bit is generated if and only if the outcomes are different. The success probability per pair is\n$$\nq = \\Pr(\\text{HT or TH}) = \\Pr(\\text{HT}) + \\Pr(\\text{TH}) = p(1-p) + (1-p)p = 2p(1-p).\n$$\nThe number of pairs $N$ needed to obtain the first success follows a geometric distribution with success probability $q$, so\n$$\n\\mathbb{E}[N] = \\frac{1}{q} = \\frac{1}{2p(1-p)}.\n$$\nEach trial uses exactly two readings, hence the total number of readings $R$ is $R = 2N$, and by linearity of expectation,\n$$\n\\mathbb{E}[R] = 2\\,\\mathbb{E}[N] = \\frac{2}{2p(1-p)} = \\frac{1}{p(1-p)}.\n$$\nTherefore, the expected number of sensor readings to generate one fair random bit is $\\frac{1}{p(1-p)}$.",
            "answer": "$$\\boxed{\\frac{1}{p(1-p)}}$$"
        },
        {
            "introduction": "In the era of big data, processing massive data streams with limited memory is a critical challenge, and randomization offers a path forward. This exercise  introduces a probabilistic counter, a clever algorithm that approximates a very large count using a small amount of memory. Your task is to analyze an estimator for the true count and, by using the law of total expectation on a transformed variable, prove a remarkable property about its average behavior.",
            "id": "1441272",
            "problem": "In the analysis of large-scale data streams, it is often necessary to count a vast number of events using limited memory. One technique is to use a probabilistic counter. Consider such a counter whose state is represented by an integer value $C$. The counter is initialized to $C_0 = 0$. When the $k$-th signal (for $k = 1, 2, \\dots, N$) arrives, the counter's value $C_{k-1}$ is updated to $C_k$ according to the following rule: the counter increments by one (i.e., $C_k = C_{k-1} + 1$) with probability $p = q^{-C_{k-1}}$, and it remains unchanged (i.e., $C_k = C_{k-1}$) with probability $1-p$. Here, $q$ is a constant real number greater than 1.\n\nThis process is repeated for a total of $N$ incoming signals. The final state of the counter is $C_N$. An analyst proposes an estimator, $X_N$, for the true number of signals $N$. The estimator is defined as a function of the final counter value $C_N$:\n$$X_N = \\frac{q^{C_N} - 1}{q-1}$$\nDetermine the exact expected value of this estimator, $E[X_N]$, after $N$ signals have been received. Express your answer in terms of $N$ and $q$.",
            "solution": "We analyze the counter as a Markov process with state $C_{k}$ and transition rule: given $C_{k-1}=c$, the next state is $C_{k}=c+1$ with probability $q^{-c}$ and $C_{k}=c$ with probability $1-q^{-c}$, where $q1$. Define the transformed process $Z_{k}=q^{C_{k}}$. We compute the conditional expectation of $Z_{k}$ given $C_{k-1}=c$:\n$$\n\\mathbb{E}\\!\\left[Z_{k}\\mid C_{k-1}=c\\right]\n= q^{-c}\\,q^{c+1} + \\left(1-q^{-c}\\right)q^{c}\n= q + q^{c} - 1\n= q^{C_{k-1}} + (q-1)\n= Z_{k-1} + (q-1).\n$$\nApplying the law of total expectation (tower property),\n$$\n\\mathbb{E}[Z_{k}] = \\mathbb{E}\\!\\left[\\mathbb{E}[Z_{k}\\mid C_{k-1}]\\right]\n= \\mathbb{E}[Z_{k-1} + (q-1)]\n= \\mathbb{E}[Z_{k-1}] + (q-1).\n$$\nWith the initial condition $C_{0}=0$, we have $Z_{0}=q^{C_{0}}=1$, hence\n$$\n\\mathbb{E}[Z_{N}] = \\mathbb{E}[Z_{0}] + N(q-1) = 1 + N(q-1).\n$$\nThe estimator is $X_{N} = \\frac{q^{C_{N}} - 1}{q-1} = \\frac{Z_{N}-1}{q-1}$. Using linearity of expectation,\n$$\n\\mathbb{E}[X_{N}] = \\frac{\\mathbb{E}[Z_{N}] - 1}{q-1} = \\frac{\\left(1 + N(q-1)\\right) - 1}{q-1} = N.\n$$\nTherefore, the estimator is exactly unbiased, with expected value equal to the true count $N$.",
            "answer": "$$\\boxed{N}$$"
        }
    ]
}