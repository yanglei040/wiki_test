{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on journey with a foundational problem in probability and computer science: the Coupon Collector's Problem. This exercise, set in the context of a hypothetical game, demonstrates how to analyze the expected time to completion for a randomized process by breaking it down into a series of simpler steps. Mastering the analysis in this problem  is crucial, as it introduces the powerful tool of linearity of expectation and its application to sequences of geometric random variables, a pattern that appears frequently in the analysis of randomized algorithms.",
            "id": "1441266",
            "problem": "In the popular online game \"Galaxy of Heroes,\" players can acquire new characters by obtaining \"Hero Shards\" from randomized \"Chrono-Capsules.\" A new character is unlocked only when a player has collected one of each of the distinct shard types associated with that character.\n\nConsider the character \"Zeta Prime,\" who requires a complete set of 5 distinct types of Hero Shards to be unlocked. Each Chrono-Capsule guarantees exactly one Hero Shard for Zeta Prime. The type of shard obtained from a capsule is chosen uniformly at random from the 5 possible types, and each acquisition is an independent event.\n\nWhat is the expected number of Chrono-Capsules a player must open to collect at least one of each of the 5 distinct Hero Shard types for Zeta Prime? Your answer should be an exact value.",
            "solution": "Let $T$ be the total number of Chrono-Capsules needed to obtain all $5$ distinct shard types. Decompose $T$ as a sum of waiting times:\n$$\nT=\\sum_{k=0}^{4} X_{k},\n$$\nwhere $X_{k}$ is the number of additional capsules needed to obtain a new shard type given that $k$ distinct types have already been collected.\n\nWhen $k$ distinct types are collected, the probability that the next capsule yields a new type is\n$$\np_{k}=\\frac{5-k}{5},\n$$\nsince there are $5-k$ unseen types out of $5$ equally likely types. Because each capsule result is independent and $p_{k}$ remains constant while $k$ is fixed, $X_{k}$ has a geometric distribution (counting the number of trials until the first success), with expectation\n$$\n\\mathbb{E}[X_{k}]=\\frac{1}{p_{k}}=\\frac{5}{5-k}.\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[T]=\\sum_{k=0}^{4} \\mathbb{E}[X_{k}]=\\sum_{k=0}^{4} \\frac{5}{5-k}=5 \\sum_{j=1}^{5} \\frac{1}{j}=5 H_{5},\n$$\nwhere $H_{5}$ is the fifth harmonic number. Compute $H_{5}$ exactly:\n$$\nH_{5}=1+\\frac{1}{2}+\\frac{1}{3}+\\frac{1}{4}+\\frac{1}{5}=\\frac{60}{60}+\\frac{30}{60}+\\frac{20}{60}+\\frac{15}{60}+\\frac{12}{60}=\\frac{137}{60}.\n$$\nTherefore,\n$$\n\\mathbb{E}[T]=5 \\cdot \\frac{137}{60}=\\frac{685}{60}=\\frac{137}{12}.\n$$",
            "answer": "$$\\boxed{\\frac{137}{12}}$$"
        },
        {
            "introduction": "Building on the concept of expectation, we now tackle a core challenge in computational complexity: the Maximum 2-Satisfiability (MAX-2-SAT) problem. This practice illustrates the power of the probabilistic method, where an extremely simple randomized strategy can yield a surprisingly strong and guaranteed performance baseline for an NP-hard problem. By calculating the expected fraction of satisfied clauses , you will gain direct insight into how randomization serves as a potent tool for designing and analyzing approximation algorithms.",
            "id": "1441265",
            "problem": "In the field of computational complexity, the Maximum 2-Satisfiability (MAX-2-SAT) problem is a fundamental optimization challenge. An instance of this problem is defined by a set of $n$ Boolean variables, $V = \\{x_1, x_2, \\dots, x_n\\}$, and a collection of $m$ clauses. Each clause is a disjunction (an OR operation) of exactly two literals. A literal is either a variable $x_i$ or its negation $\\neg x_i$. For any given clause, assume its two literals involve distinct variables (e.g., clauses of the form $x_i \\lor \\neg x_i$ or $x_i \\lor x_i$ are excluded). The goal of MAX-2-SAT is to find a truth assignment—a mapping of each variable to TRUE or FALSE—that maximizes the total number of satisfied clauses.\n\nConsider the simplest randomized algorithm for approximating a solution: each variable $x_i \\in V$ is independently assigned a value of TRUE with probability $1/2$ and FALSE with probability $1/2$.\n\nFor an arbitrary MAX-2-SAT instance with $m > 0$ clauses satisfying the stated conditions, what is the expected fraction of clauses that will be satisfied by applying this randomized assignment once? Express your answer as a single, closed-form analytic expression.",
            "solution": "Let a clause be $C=(\\ell_{i} \\lor \\ell_{j})$ where $\\ell_{i}$ is a literal on variable $x_{i}$ and $\\ell_{j}$ is a literal on variable $x_{j}$ with $i \\neq j$. Under the randomized assignment where each $x_{k}$ is independently TRUE with probability $\\frac{1}{2}$ and FALSE with probability $\\frac{1}{2}$, any literal (whether $x_{k}$ or $\\neg x_{k}$) is TRUE with probability $\\frac{1}{2}$ and FALSE with probability $\\frac{1}{2}$. Since the literals involve distinct variables and assignments are independent, the events that $\\ell_{i}$ is FALSE and that $\\ell_{j}$ is FALSE are independent. Therefore,\n$$\nP(\\text{$C$ is unsatisfied}) = P(\\ell_{i}\\ \\text{FALSE and}\\ \\ell_{j}\\ \\text{FALSE}) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}.\n$$\nHence,\n$$\nP(\\text{$C$ is satisfied}) = 1 - \\frac{1}{4} = \\frac{3}{4}.\n$$\nLet $X_{k}$ be the indicator that the $k$-th clause is satisfied. Then $E[X_{k}] = \\frac{3}{4}$ for all $k$, and by linearity of expectation the expected number of satisfied clauses is\n$$\nE\\Big[\\sum_{k=1}^{m} X_{k}\\Big] = \\sum_{k=1}^{m} E[X_{k}] = m \\cdot \\frac{3}{4}.\n$$\nThe expected fraction of satisfied clauses is\n$$\nE\\left[\\frac{1}{m} \\sum_{k=1}^{m} X_{k}\\right] = \\frac{1}{m} \\cdot m \\cdot \\frac{3}{4} = \\frac{3}{4},\n$$\nwhich is independent of the specific instance, provided $m>0$.",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        },
        {
            "introduction": "Our final practice explores a sophisticated application of randomization within the modern domain of big data and streaming algorithms. This problem introduces a probabilistic counter, a clever device for estimating large counts with very little memory, framed here as a hypothetical scenario. The analysis requires more than a direct application of expectation; it challenges you to discover a creative transformation of the system's state to make the problem tractable. By proving that the proposed estimator is unbiased , you will appreciate how elegant mathematical techniques enable efficient computation under significant resource constraints.",
            "id": "1441272",
            "problem": "In the analysis of large-scale data streams, it is often necessary to count a vast number of events using limited memory. One technique is to use a probabilistic counter. Consider such a counter whose state is represented by an integer value $C$. The counter is initialized to $C_0 = 0$. When the $k$-th signal (for $k = 1, 2, \\dots, N$) arrives, the counter's value $C_{k-1}$ is updated to $C_k$ according to the following rule: the counter increments by one (i.e., $C_k = C_{k-1} + 1$) with probability $p = q^{-C_{k-1}}$, and it remains unchanged (i.e., $C_k = C_{k-1}$) with probability $1-p$. Here, $q$ is a constant real number greater than 1.\n\nThis process is repeated for a total of $N$ incoming signals. The final state of the counter is $C_N$. An analyst proposes an estimator, $X_N$, for the true number of signals $N$. The estimator is defined as a function of the final counter value $C_N$:\n$$X_N = \\frac{q^{C_N} - 1}{q-1}$$\nDetermine the exact expected value of this estimator, $E[X_N]$, after $N$ signals have been received. Express your answer in terms of $N$ and $q$.",
            "solution": "We analyze the counter as a Markov process with state $C_{k}$ and transition rule: given $C_{k-1}=c$, the next state is $C_{k}=c+1$ with probability $q^{-c}$ and $C_{k}=c$ with probability $1-q^{-c}$, where $q>1$. Define the transformed process $Z_{k}=q^{C_{k}}$. We compute the conditional expectation of $Z_{k}$ given $C_{k-1}=c$:\n$$\n\\mathbb{E}\\!\\left[Z_{k}\\mid C_{k-1}=c\\right]\n= q^{-c}\\,q^{c+1} + \\left(1-q^{-c}\\right)q^{c}\n= q + q^{c} - 1\n= q^{C_{k-1}} + (q-1)\n= Z_{k-1} + (q-1).\n$$\nApplying the law of total expectation (tower property),\n$$\n\\mathbb{E}[Z_{k}] = \\mathbb{E}\\!\\left[\\mathbb{E}[Z_{k}\\mid C_{k-1}]\\right]\n= \\mathbb{E}[Z_{k-1} + (q-1)]\n= \\mathbb{E}[Z_{k-1}] + (q-1).\n$$\nWith the initial condition $C_{0}=0$, we have $Z_{0}=q^{C_{0}}=1$, hence\n$$\n\\mathbb{E}[Z_{N}] = \\mathbb{E}[Z_{0}] + N(q-1) = 1 + N(q-1).\n$$\nThe estimator is $X_{N} = \\frac{q^{C_{N}} - 1}{q-1} = \\frac{Z_{N}-1}{q-1}$. Using linearity of expectation,\n$$\n\\mathbb{E}[X_{N}] = \\frac{\\mathbb{E}[Z_{N}] - 1}{q-1} = \\frac{\\left(1 + N(q-1)\\right) - 1}{q-1} = N.\n$$\nTherefore, the estimator is exactly unbiased, with expected value equal to the true count $N$.",
            "answer": "$$\\boxed{N}$$"
        }
    ]
}