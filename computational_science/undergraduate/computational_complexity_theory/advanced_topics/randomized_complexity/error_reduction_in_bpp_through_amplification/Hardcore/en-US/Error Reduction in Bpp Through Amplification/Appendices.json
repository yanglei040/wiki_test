{
    "hands_on_practices": [
        {
            "introduction": "The power of probabilistic algorithms lies in our ability to make them arbitrarily reliable. This first exercise provides a direct, hands-on calculation of this principle, known as amplification. By determining the number of independent trials needed to reduce a significant initial error to a near-infinitesimal level, you will gain a concrete appreciation for why the \"probabilistic\" nature of BPP algorithms does not compromise their practical utility .",
            "id": "1422498",
            "problem": "A research group in computational theory has designed a new probabilistic algorithm for a decision problem. This algorithm is known to belong to the complexity class BPP (Bounded-error Probabilistic Polynomial-time), which means that for any given input, it produces the correct 'yes' or 'no' answer with a probability of at least $2/3$. Consequently, the probability of an error on any single run, $\\epsilon_0$, is at most $1/3$.\n\nTo increase confidence in the result, the researchers employ an amplification strategy: they run the algorithm $k$ times independently on the same input and take the majority vote as the final answer. They want to achieve a high degree of certainty for a particularly important computation, requiring that the final probability of an incorrect majority vote is less than $2^{-100}$.\n\nFor this problem, you may use the following standard result from complexity theory, which is a consequence of the Chernoff bound: The error probability of the majority-vote procedure, $\\delta_k$, after $k$ trials is bounded by the inequality:\n$$ \\delta_k \\le \\exp\\left(-2k\\left(\\frac{1}{2} - \\epsilon_0\\right)^2\\right) $$\nwhere $\\epsilon_0$ is the error probability of a single trial, which you should take as its worst-case value.\n\nDetermine the minimum integer number of trials, $k$, required to ensure the final error probability is less than $2^{-100}$. For your calculation, use the approximation $\\ln(2) \\approx 0.69315$.",
            "solution": "We are given the Chernoff-type bound for the majority-vote error after $k$ independent runs:\n$$\n\\delta_{k} \\le \\exp\\left(-2k\\left(\\frac{1}{2}-\\epsilon_{0}\\right)^{2}\\right),\n$$\nand the worst-case single-run error is $\\epsilon_{0}=\\frac{1}{3}$. Substituting $\\epsilon_{0}=\\frac{1}{3}$ gives\n$$\n\\left(\\frac{1}{2}-\\epsilon_{0}\\right)=\\frac{1}{2}-\\frac{1}{3}=\\frac{1}{6},\n$$\nso\n$$\n\\delta_{k} \\le \\exp\\left(-2k\\left(\\frac{1}{6}\\right)^{2}\\right)=\\exp\\left(-\\frac{k}{18}\\right).\n$$\nTo ensure the final error probability is less than $2^{-100}$, it suffices to require\n$$\n\\exp\\left(-\\frac{k}{18}\\right)  2^{-100}.\n$$\nTaking natural logarithms of both sides yields\n$$\n-\\frac{k}{18}  -100\\,\\ln(2),\n$$\nwhich is equivalent to\n$$\n\\frac{k}{18}  100\\,\\ln(2) \\quad \\Longrightarrow \\quad k  1800\\,\\ln(2).\n$$\nSince $k$ must be an integer, the minimum admissible value is\n$$\nk_{\\min}=\\left\\lceil 1800\\,\\ln(2)\\right\\rceil.\n$$\nUsing the given approximation $\\ln(2)\\approx 0.69315$,\n$$\n1800\\,\\ln(2)\\approx 1800\\times 0.69315=1247.67\\ldots,\n$$\nhence\n$$\nk_{\\min}=1248.\n$$",
            "answer": "$$\\boxed{1248}$$"
        },
        {
            "introduction": "The definition of BPP requires an algorithm to be correct with some probability bounded away from $1/2$, often stated as $2/3$. This problem invites you to explore the theoretical robustness of this definition by comparing two algorithms with different initial error rates. By calculating the ratio of work required to amplify them to the same high standard of reliability, you will uncover a key insight: the initial error constant only affects the total runtime by a constant factor, highlighting why BPP is a well-defined and stable complexity class .",
            "id": "1422496",
            "problem": "Consider two probabilistic algorithms, $\\mathcal{A}_1$ and $\\mathcal{A}_2$, designed to solve the same decision problem for inputs of size $n$. The runtime for a single execution of either algorithm is given by a polynomial function $T(n)$.\n\nFor any given input, algorithm $\\mathcal{A}_1$ returns the correct answer with a probability of at least $3/4$. In contrast, algorithm $\\mathcal{A}_2$ is less reliable, returning the correct answer with a probability of at least $2/3$. The error in both algorithms is two-sided, meaning they can be incorrect on both 'yes' and 'no' instances of the problem.\n\nTo improve their reliability, a standard amplification procedure is employed for both algorithms. This procedure consists of running the algorithm $k$ independent times and taking the majority vote of the outcomes as the final answer. For each algorithm, we want to determine the minimum number of repetitions, $k_1$ for $\\mathcal{A}_1$ and $k_2$ for $\\mathcal{A}_2$, required to ensure that the final error probability of the amplified algorithm is at most $2^{-n}$.\n\nLet $Time_1(n) = k_1 T(n)$ and $Time_2(n) = k_2 T(n)$ be the total time complexities of the amplified versions of $\\mathcal{A}_1$ and $\\mathcal{A}_2$, respectively. Assuming $n$ is large, determine the value of the ratio $\\frac{Time_2(n)}{Time_1(n)}$.\n\nExpress your answer as an exact fraction.",
            "solution": "The problem asks for the asymptotic ratio of the total runtimes, $\\frac{Time_2(n)}{Time_1(n)}$, required to amplify two probabilistic algorithms to a target error probability of $2^{-n}$. This ratio is equivalent to the ratio of the number of repetitions required for each algorithm, $\\frac{k_2}{k_1}$, since the single-run time $T(n)$ is the same for both.\n\nFirst, let's establish the error probabilities for a single run of each algorithm.\nFor algorithm $\\mathcal{A}_1$, the success probability is $p_1 = 3/4$, so the error probability is $\\epsilon_1 = 1 - p_1 = 1 - 3/4 = 1/4$.\nFor algorithm $\\mathcal{A}_2$, the success probability is $p_2 = 2/3$, so the error probability is $\\epsilon_2 = 1 - p_2 = 1 - 2/3 = 1/3$.\nBoth error probabilities satisfy $\\epsilon_i  1/2$, which is a necessary condition for amplification by majority voting to be effective.\n\nThe amplification process involves running an algorithm with error $\\epsilon$ for $k$ independent trials and taking the majority vote. Let $X_i$ be an indicator random variable for the $i$-th trial, where $X_i=1$ if the algorithm returns an incorrect answer and $X_i=0$ otherwise. We have $P(X_i=1) = \\epsilon$. The total number of incorrect answers is $S_k = \\sum_{i=1}^k X_i$. The majority vote is incorrect if more than half of the trials are incorrect, i.e., if $S_k  k/2$.\n\nTo bound the probability $P(S_k  k/2)$, we can use a version of the Chernoff bound known as Hoeffding's inequality. For a sum of $k$ independent Bernoulli variables $S_k$ with mean $E[S_k] = k\\epsilon$, the inequality states:\n$$ P(S_k \\ge (1+\\delta)E[S_k]) \\le \\exp\\left(-\\frac{\\delta^2}{3}E[S_k]\\right) $$\nA more direct and commonly used form for BPP amplification states that the probability of the majority outcome being wrong is bounded by:\n$$ P(\\text{error}) \\le \\exp(-2k(1/2 - \\epsilon)^2) $$\nWe want this final error probability to be at most $2^{-n}$. So, we set up the inequality:\n$$ \\exp(-2k(1/2 - \\epsilon)^2) \\le 2^{-n} $$\nTaking the natural logarithm of both sides:\n$$ -2k(1/2 - \\epsilon)^2 \\le n \\ln(2^{-1}) = -n \\ln(2) $$\nMultiplying by $-1$ and reversing the inequality sign:\n$$ 2k(1/2 - \\epsilon)^2 \\ge n \\ln(2) $$\nSolving for $k$, we find the minimum number of repetitions required:\n$$ k \\ge \\frac{n \\ln(2)}{2(1/2 - \\epsilon)^2} $$\nThis can be simplified:\n$$ k \\ge \\frac{n \\ln(2)}{2\\left(\\frac{1-2\\epsilon}{2}\\right)^2} = \\frac{n \\ln(2)}{2 \\frac{(1-2\\epsilon)^2}{4}} = \\frac{2n \\ln(2)}{(1-2\\epsilon)^2} $$\nFor large $n$, we can approximate the minimum required repetitions $k$ by the R.H.S value, as the ceiling function's effect on the ratio becomes negligible.\n$$ k \\approx \\frac{2n \\ln(2)}{(1-2\\epsilon)^2} $$\n\nNow, we calculate the required number of repetitions for each algorithm.\n\nFor algorithm $\\mathcal{A}_1$, with $\\epsilon_1 = 1/4$:\n$$ k_1 \\approx \\frac{2n \\ln(2)}{(1 - 2(1/4))^2} = \\frac{2n \\ln(2)}{(1 - 1/2)^2} = \\frac{2n \\ln(2)}{(1/2)^2} = \\frac{2n \\ln(2)}{1/4} = 8n \\ln(2) $$\n\nFor algorithm $\\mathcal{A}_2$, with $\\epsilon_2 = 1/3$:\n$$ k_2 \\approx \\frac{2n \\ln(2)}{(1 - 2(1/3))^2} = \\frac{2n \\ln(2)}{(1 - 2/3)^2} = \\frac{2n \\ln(2)}{(1/3)^2} = \\frac{2n \\ln(2)}{1/9} = 18n \\ln(2) $$\n\nFinally, we compute the ratio of the total runtimes.\n$$ \\frac{Time_2(n)}{Time_1(n)} = \\frac{k_2 T(n)}{k_1 T(n)} = \\frac{k_2}{k_1} $$\nSubstituting the expressions for $k_1$ and $k_2$:\n$$ \\frac{k_2}{k_1} \\approx \\frac{18n \\ln(2)}{8n \\ln(2)} = \\frac{18}{8} = \\frac{9}{4} $$\nThe ratio is constant and does not depend on $n$ or $T(n)$.",
            "answer": "$$\\boxed{\\frac{9}{4}}$$"
        },
        {
            "introduction": "While running an algorithm a fixed number of times is a valid amplification strategy, it may not always be the most efficient. This advanced problem introduces a more sophisticated, adaptive scheme that can potentially save computational effort by making a decision based on an intermediate \"margin of confidence.\" Analyzing this two-stage process requires a deeper application of Chernoff bounds and challenges you to think critically about designing smarter, resource-aware probabilistic procedures .",
            "id": "1422501",
            "problem": "Consider a language $L$ in the complexity class BPP (Bounded-error Probabilistic Polynomial time). This means there exists a probabilistic Turing machine (PTM) $M$ that decides $L$ in polynomial time with an error probability of at most $1/3$. That is, for any input string $x$, the probability that $M(x)$ outputs the correct answer (i.e., 'yes' if $x \\in L$, and 'no' if $x \\notin L$) is at least $2/3$.\n\nTo reduce the probability of error for a given input $x$, a novel adaptive amplification scheme is proposed. The scheme works as follows:\n\n1.  Run the machine $M$ on input $x$ for $k$ independent trials. Let $N_1$ be the number of 'yes' outputs and $N_0$ be the number of 'no' outputs, where $N_1 + N_0 = k$.\n2.  Calculate the margin of votes, defined as $|N_1 - N_0|$.\n3.  If the margin $|N_1 - N_0|$ is greater than $\\sqrt{k}$, the algorithm halts and outputs the majority vote from these $k$ trials.\n4.  If the margin $|N_1 - N_0|$ is less than or equal to $\\sqrt{k}$, the algorithm performs an additional $k$ independent trials. It then outputs the majority vote from the total of $2k$ trials.\n\nAssume that for a sufficiently large number of independent Bernoulli trials $m$, with each trial having a success probability $p$, the number of successes $Z$ is concentrated around its mean $mp$. You may use the following version of the Chernoff bound to analyze this concentration: for any $\\epsilon  0$,\n$$\n\\Pr\\left[\\left|\\frac{Z}{m} - p\\right|  \\epsilon\\right] \\le 2 \\exp(-2m\\epsilon^2)\n$$\n\nDetermine a closed-form analytic expression for an upper bound on the total probability that this adaptive scheme outputs an incorrect answer. Your expression should be a function of $k$.",
            "solution": "Let $x$ be fixed and let $Y_{i}$ be the indicator that the $i$-th run of $M$ on $x$ outputs the correct answer. For both cases $x \\in L$ and $x \\notin L$, we have $\\Pr[Y_{i}=1]=p$ with $p \\ge \\frac{2}{3}$ by the BPP guarantee. Let $S_{m}=\\sum_{i=1}^{m} Y_{i}$ denote the number of correct outputs in $m$ independent trials.\n\nDefine the two error events corresponding to the adaptive scheme:\n- $E_{1}$: after $k$ trials, the margin exceeds the threshold and the majority is wrong, i.e.,\n$$\nE_{1}=\\left\\{|2S_{k}-k|\\sqrt{k}\\ \\text{ and }\\ S_{k}\\frac{k}{2}\\right\\}.\n$$\n- $E_{2}$: the first $k$ trials do not exceed the margin threshold, and after $2k$ trials the majority is wrong, i.e.,\n$$\nE_{2}=\\left\\{|2S_{k}-k|\\le \\sqrt{k}\\ \\text{ and }\\ S_{2k}k\\right\\}.\n$$\nThe total error probability satisfies\n$$\n\\Pr[\\text{error}]=\\Pr[E_{1}]+\\Pr[E_{2}]\\le \\Pr[E_{1}]+\\Pr\\left[S_{2k}k\\right],\n$$\nsince $E_{2}\\subseteq\\{S_{2k}k\\}$.\n\nWe now bound the two terms using the provided Chernoff/Hoeffding inequality: for any $\\epsilon0$ and $Z\\sim \\text{Bin}(m,p)$,\n$$\n\\Pr\\left[\\left|\\frac{Z}{m}-p\\right|\\epsilon\\right]\\le 2\\exp(-2m\\epsilon^{2}).\n$$\n\nBound for $\\Pr[E_{1}]$. The condition $|2S_{k}-k|\\sqrt{k}$ and $S_{k}\\frac{k}{2}$ implies\n$$\nS_{k}\\le \\frac{k-\\sqrt{k}}{2},\\quad\\text{so}\\quad \\frac{S_{k}}{k}\\le \\frac{1}{2}-\\frac{1}{2\\sqrt{k}}.\n$$\nHence\n$$\n\\Pr[E_{1}]\\le \\Pr\\left[\\frac{S_{k}}{k}\\le \\frac{1}{2}-\\frac{1}{2\\sqrt{k}}\\right]\n=\\Pr\\left[\\left|\\frac{S_{k}}{k}-p\\right|\\ge \\left(p-\\frac{1}{2}+\\frac{1}{2\\sqrt{k}}\\right)\\right].\n$$\nWith $p\\ge \\frac{2}{3}$, the deviation satisfies\n$$\np-\\frac{1}{2}+\\frac{1}{2\\sqrt{k}}\\ge \\frac{1}{6}+\\frac{1}{2\\sqrt{k}}.\n$$\nTherefore,\n$$\n\\Pr[E_{1}]\\le 2\\exp\\left(-2k\\left(\\frac{1}{6}+\\frac{1}{2\\sqrt{k}}\\right)^{2}\\right)\n=2\\exp\\left(-\\frac{k}{18}-\\frac{1}{3}\\sqrt{k}-\\frac{1}{2}\\right).\n$$\n\nBound for $\\Pr[S_{2k}k]$. Here\n$$\n\\Pr\\left[S_{2k}k\\right]=\\Pr\\left[\\frac{S_{2k}}{2k}\\le \\frac{1}{2}\\right]\n=\\Pr\\left[\\left|\\frac{S_{2k}}{2k}-p\\right|\\ge \\left(p-\\frac{1}{2}\\right)\\right].\n$$\nWith $p\\ge \\frac{2}{3}$ we have $p-\\frac{1}{2}\\ge \\frac{1}{6}$, so\n$$\n\\Pr\\left[S_{2k}k\\right]\\le 2\\exp\\left(-2(2k)\\left(\\frac{1}{6}\\right)^{2}\\right)=2\\exp\\left(-\\frac{k}{9}\\right).\n$$\n\nCombining the two bounds gives the desired closed-form upper bound, uniform over all inputs $x$,\n$$\n\\Pr[\\text{incorrect output}]\\le 2\\exp\\left(-\\frac{k}{18}-\\frac{1}{3}\\sqrt{k}-\\frac{1}{2}\\right)+2\\exp\\left(-\\frac{k}{9}\\right),\n$$\nwhich is a function of $k$ only and uses only the provided concentration inequality and the BPP guarantee $p\\ge \\frac{2}{3}$.",
            "answer": "$$\\boxed{2\\exp\\left(-\\frac{k}{18}-\\frac{1}{3}\\sqrt{k}-\\frac{1}{2}\\right)+2\\exp\\left(-\\frac{k}{9}\\right)}$$"
        }
    ]
}