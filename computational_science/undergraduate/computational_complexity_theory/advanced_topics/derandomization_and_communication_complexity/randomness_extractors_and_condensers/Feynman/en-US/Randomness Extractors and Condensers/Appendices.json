{
    "hands_on_practices": [
        {
            "introduction": "Let's begin by quantifying the randomness of a weak source using the concept of min-entropy, denoted as $H_{\\infty}(X)$. This powerful metric measures the \"worst-case\" unpredictability of a source, which is determined by its most likely outcome. This first exercise provides a direct application of the min-entropy formula to a hypothetical faulty generator, helping you build a concrete understanding of how a source's quality is formally assessed. ",
            "id": "1441868",
            "problem": "In the field of computational complexity theory, a \"weak random source\" is a process that generates outputs with some degree of randomness, but its probability distribution is not uniform. The quality of such a source is often quantified by its min-entropy, which measures the amount of \"worst-case\" randomness in bits.\n\nConsider a simplified model of a faulty hardware random number generator. The generator, denoted as a random source $X$, is designed to produce an integer from a set of possible outcomes. Due to a defect, the generator can only output one of five specific integer values: $\\{0, 1, 2, 3, 4\\}$. The probability distribution for these outcomes has been characterized as follows:\n- The probability of generating the value 0 is $P(X=0) = \\frac{1}{3}$.\n- The probability of generating any of the other values is equal, with $P(X=k) = \\frac{1}{6}$ for each $k \\in \\{1, 2, 3, 4\\}$.\n\nThe min-entropy of a discrete random variable $X$ with possible outcomes $\\{x_1, x_2, \\dots, x_n\\}$ is defined by the formula:\n$$H_{\\infty}(X) = -\\log_{2}\\left(\\max_{i} P(X=x_i)\\right)$$\nwhere $P(X=x_i)$ is the probability of outcome $x_i$.\n\nCalculate the min-entropy $H_{\\infty}(X)$ of this faulty generator. Present your answer as a single closed-form analytic expression in terms of logarithms.",
            "solution": "By definition, the min-entropy of a discrete random variable $X$ is given by\n$$\nH_{\\infty}(X) = -\\log_{2}\\left(\\max_{i} P(X=x_i)\\right).\n$$\nThe given probabilities are $P(X=0) = \\frac{1}{3}$ and $P(X=k) = \\frac{1}{6}$ for each $k \\in \\{1,2,3,4\\}$. Since $\\frac{1}{3} > \\frac{1}{6}$, the maximum probability is\n$$\n\\max_{i} P(X=x_i) = \\frac{1}{3}.\n$$\nSubstituting into the definition, we obtain\n$$\nH_{\\infty}(X) = -\\log_{2}\\left(\\frac{1}{3}\\right).\n$$\nUsing the logarithmic identity $\\log_{b}(x^{-1}) = -\\log_{b}(x)$, we simplify\n$$\n-\\log_{2}\\left(\\frac{1}{3}\\right) = \\log_{2}(3).\n$$\nTherefore, the min-entropy is\n$$\nH_{\\infty}(X) = \\log_{2}(3).\n$$",
            "answer": "$$\\boxed{\\log_{2}(3)}$$"
        },
        {
            "introduction": "We now apply the concept of min-entropy to a more structured and common model of an imperfect source found in theoretical computer science: the \"bit-fixing source.\" By calculating the min-entropy for this type of source, you will discover a clear and intuitive connection between the formal definition of $H_{\\infty}(X)$ and the effective number of truly random bits the source produces. This exercise bridges the gap between abstract theory and practical models of weak randomness. ",
            "id": "1441918",
            "problem": "In the study of computational complexity and cryptography, we often analyze imperfect sources of randomness. One such model is a \"bit-fixing source.\"\n\nConsider a source that generates $n$-bit binary strings. This source is known to be partially compromised: a specific set of $t$ bit positions are always fixed to the value 0, while the remaining $n-t$ bit positions are chosen independently and uniformly at random (i.e., each of these $n-t$ bits is 0 with probability $1/2$ and 1 with probability $1/2$).\n\nThe quality of a random source can be measured by its min-entropy. For a random variable $X$ that takes values from a set of possible outcomes $\\Omega$, its min-entropy, denoted $H_{\\infty}(X)$, is defined as:\n$$H_{\\infty}(X) = -\\log_{2}\\left(\\max_{x \\in \\Omega} P(X=x)\\right)$$\nwhere $P(X=x)$ is the probability of a specific outcome $x$.\n\nSuppose a specific bit-fixing source generates $n=128$ bit strings, with $t=20$ of its bit positions fixed. Calculate the min-entropy of this source.",
            "solution": "Let $X$ be the random variable representing the $n$-bit string generated by the source. The set of all possible outcomes is denoted by $\\Omega$.\n\nThe source generates $n$-bit strings where $t$ bits are fixed and $n-t$ bits are random. The random bits are chosen uniformly and independently. This means that for each of the $n-t$ non-fixed positions, there are 2 equally likely possibilities (0 or 1).\n\nThe total number of distinct strings that the source can produce is determined by the number of combinations for the random part. This is $2^{n-t}$. So, the size of the sample space is $|\\Omega| = 2^{n-t}$.\n\nSince the $n-t$ bits are chosen uniformly at random, every possible string that the source can generate is equally likely. The probability of any specific outcome $x \\in \\Omega$ is given by:\n$$P(X=x) = \\frac{1}{\\text{Total number of possible outcomes}} = \\frac{1}{|\\Omega|} = \\frac{1}{2^{n-t}}$$\n\nThe definition of min-entropy requires us to find the maximum probability among all possible outcomes. Since all outcomes are equally likely, the maximum probability is the same as the probability of any single outcome:\n$$\\max_{x \\in \\Omega} P(X=x) = \\frac{1}{2^{n-t}}$$\n\nNow we substitute this maximum probability into the formula for min-entropy:\n$$H_{\\infty}(X) = -\\log_{2}\\left(\\max_{x \\in \\Omega} P(X=x)\\right)$$\n$$H_{\\infty}(X) = -\\log_{2}\\left(\\frac{1}{2^{n-t}}\\right)$$\n\nUsing the logarithm property $\\log(1/a) = -\\log(a)$, we get:\n$$H_{\\infty}(X) = - \\left(-\\log_{2}\\left(2^{n-t}\\right)\\right)$$\n$$H_{\\infty}(X) = \\log_{2}\\left(2^{n-t}\\right)$$\n\nUsing the property $\\log_b(b^y) = y$, we can simplify this to:\n$$H_{\\infty}(X) = n-t$$\n\nThe problem provides the numerical values $n=128$ and $t=20$. We can substitute these into our derived expression:\n$$H_{\\infty}(X) = 128 - 20 = 108$$\n\nThus, the min-entropy of the source is 108.",
            "answer": "$$\\boxed{108}$$"
        },
        {
            "introduction": "A primary goal of randomness extraction is to produce an output that is statistically \"indistinguishable\" from a perfectly uniform distribution. The formal tool we use to measure this closeness is the statistical distance, $\\Delta$. This practice problem will guide you through a hands-on calculation of the statistical distance between an ideal source and a flawed one, making this abstract concept tangible and demonstrating how we can precisely quantify the quality of an extractor's output. ",
            "id": "1441905",
            "problem": "In the field of cryptography and computational complexity, the quality of a random source is crucial. A common way to quantify the difference between a real-world, imperfect random source and a truly uniform random source is by calculating their statistical distance.\n\nConsider a simple random number generator designed to produce 2-bit integers. An ideal version of this generator would produce each of the four possible outcomes in the set $\\{0, 1, 2, 3\\}$ with equal likelihood. Let this ideal probability distribution be denoted by $U$.\n\nA particular physical implementation of this generator is found to be defective. An analysis reveals that the outcome $0$ occurs in exactly half of all trials. The other three outcomes, $\\{1, 2, 3\\}$, are observed to occur with equal frequency among the remaining trials. Let this flawed probability distribution be denoted by $X$.\n\nThe statistical distance, $\\Delta(P, Q)$, between two discrete probability distributions $P$ and $Q$ over the same finite sample space $\\Omega$ is defined as:\n$$ \\Delta(P, Q) = \\frac{1}{2} \\sum_{\\omega \\in \\Omega} |P(\\omega) - Q(\\omega)| $$\n\nCalculate the statistical distance $\\Delta(U, X)$ between the ideal distribution $U$ and the flawed distribution $X$. Express your answer as an exact fraction.",
            "solution": "We consider the finite sample space $\\Omega=\\{0,1,2,3\\}$. The ideal distribution $U$ assigns $U(\\omega)=\\frac{1}{4}$ for each $\\omega\\in\\Omega$. The flawed distribution $X$ has $X(0)=\\frac{1}{2}$ and, since the remaining probability mass $\\frac{1}{2}$ is split equally among $\\{1,2,3\\}$, we have $X(1)=X(2)=X(3)=\\frac{1}{6}$.\n\nBy definition, the statistical distance is\n$$\n\\Delta(U,X)=\\frac{1}{2}\\sum_{\\omega\\in\\Omega}\\left|U(\\omega)-X(\\omega)\\right|.\n$$\nWe compute the absolute differences term by term:\n$$\n|U(0)-X(0)|=\\left|\\frac{1}{4}-\\frac{1}{2}\\right|=\\frac{1}{4},\n$$\nand for each $i\\in\\{1,2,3\\}$,\n$$\n|U(i)-X(i)|=\\left|\\frac{1}{4}-\\frac{1}{6}\\right|=\\left|\\frac{3-2}{12}\\right|=\\frac{1}{12}.\n$$\nSumming these four terms gives\n$$\n\\sum_{\\omega\\in\\Omega}\\left|U(\\omega)-X(\\omega)\\right|=\\frac{1}{4}+3\\cdot\\frac{1}{12}=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nTherefore,\n$$\n\\Delta(U,X)=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        }
    ]
}