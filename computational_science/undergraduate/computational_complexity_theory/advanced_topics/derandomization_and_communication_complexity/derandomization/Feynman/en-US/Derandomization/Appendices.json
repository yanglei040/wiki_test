{
    "hands_on_practices": [
        {
            "introduction": "Generating truly random bits can be a bottleneck in computation. Derandomization explores how to achieve the power of randomized algorithms using far less, or even zero, randomness. This first practice introduces pairwise independence, a foundational concept where we require that any pair of random variables in a set be independent, a much weaker condition than full independence. By working through this exercise, you will gain a concrete understanding of this property and see how a very small sample space can successfully mimic some key aspects of true randomness. ",
            "id": "1420514",
            "problem": "In the field of theoretical computer science, particularly in the study of randomized algorithms, the concept of pairwise independence serves as a powerful tool for derandomization. Fully independent random variables require a large sample space, which can be computationally expensive. Pairwise independent variables, a weaker notion, can often be used instead with a significantly smaller sample space.\n\nLet $S$ be a sample space consisting of four distinct strings, each of length three. We define a uniform probability distribution over $S$, meaning each string has a probability of $\\frac{1}{4}$ of being chosen. Let $X_1, X_2,$ and $X_3$ be random variables representing the first, second, and third bits, respectively, of a string chosen uniformly at random from $S$.\n\nFor these variables to be useful in many applications, they must satisfy two conditions:\n1.  **Uniformity**: Each variable $X_i$ for $i \\in \\{1, 2, 3\\}$ must be uniformly distributed, meaning the probability of it being 0 is equal to the probability of it being 1. That is, $P(X_i=0) = P(X_i=1) = \\frac{1}{2}$.\n2.  **Pairwise Independence**: For every distinct pair of indices $i, j \\in \\{1, 2, 3\\}$, the random variables $X_i$ and $X_j$ must be independent. This means that for all values $b_1, b_2 \\in \\{0, 1\\}$, the joint probability satisfies $P(X_i=b_1, X_j=b_2) = P(X_i=b_1)P(X_j=b_2)$.\n\nWhich of the following sample spaces $S$ satisfies both the uniformity and pairwise independence conditions for the random variables $X_1, X_2, X_3$?\n\nA. $S = \\{000, 001, 010, 100\\}$\n\nB. $S = \\{000, 011, 100, 111\\}$\n\nC. $S = \\{000, 011, 101, 110\\}$\n\nD. $S = \\{000, 111, 001, 110\\}$",
            "solution": "We work under the uniform distribution on a sample space with four strings, so each string has probability $\\frac{1}{4}$. For $X_{i}$ to be uniform, for each $i \\in \\{1,2,3\\}$ we need $P(X_{i}=0)=P(X_{i}=1)=\\frac{1}{2}$. Since the distribution assigns equal weight to the four strings, this requires that in the multiset of the $i$-th coordinates across $S$ there are exactly two zeros and two ones. For pairwise independence of $X_{i}$ and $X_{j}$, because the marginals must be uniform, we need for every $b_{1},b_{2} \\in \\{0,1\\}$ that\n$$\nP(X_{i}=b_{1},X_{j}=b_{2})=P(X_{i}=b_{1})P(X_{j}=b_{2})=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$\nUnder the uniform distribution on four strings, this condition holds if and only if, among the four strings, each ordered pair $(b_{1},b_{2})$ appears exactly once as the $(i,j)$-coordinates.\n\nCheck option A: $S=\\{000,001,010,100\\}$. The first coordinates are $0,0,0,1$, hence\n$$\nP(X_{1}=0)=\\frac{3}{4}\\neq \\frac{1}{2},\n$$\nso uniformity fails and thus A is invalid.\n\nCheck option B: $S=\\{000,011,100,111\\}$. The coordinates for $X_1$, $X_2$, and $X_3$ are $(0,0,1,1)$, $(0,1,0,1)$, and $(0,1,0,1)$ respectively. Each variable is uniform since each column has two zeros and two ones. We must check all pairs for independence. For the pair $(X_{2},X_{3})$: from 000 we get (0,0); from 011 we get (1,1); from 100 we get (0,0); from 111 we get (1,1). Here only the pairs $(0,0)$ and $(1,1)$ appear, each twice. Thus\n$$\nP(X_{2}=0,X_{3}=1)=0 \\neq \\frac{1}{4}=\\frac{1}{2}\\cdot\\frac{1}{2},\n$$\nso pairwise independence fails; B is invalid.\n\nCheck option D: $S=\\{000,111,001,110\\}$. Each position has two zeros and two ones, so each $X_{i}$ is uniform. For $(X_{1},X_{2})$, the pairs are: from 000 get (0,0); from 111 get (1,1); from 001 get (0,0); from 110 get (1,1). Thus\n$$\nP(X_{1}=0,X_{2}=1)=0 \\neq \\frac{1}{4},\n$$\nso pairwise independence fails; D is invalid.\n\nCheck option C: $S=\\{000,011,101,110\\}$. The first, second, and third coordinates across $S$ are each two zeros and two ones, so for each $i$,\n$$\nP(X_{i}=0)=P(X_{i}=1)=\\frac{1}{2}.\n$$\nFor pairwise independence, enumerate pairs:\n- For $(X_{1},X_{2})$: from 000 get (0,0); from 011 get (0,1); from 101 get (1,0); from 110 get (1,1). Hence for each $(b_{1},b_{2}) \\in \\{0,1\\}^{2}$,\n$$\nP(X_{1}=b_{1},X_{2}=b_{2})=\\frac{1}{4}=\\frac{1}{2}\\cdot\\frac{1}{2}.\n$$\n- For $(X_{1},X_{3})$: from 000 get (0,0); from 011 get (0,1); from 101 get (1,1); from 110 get (1,0), again each pair once, so\n$$\nP(X_{1}=b_{1},X_{3}=b_{2})=\\frac{1}{4}=\\frac{1}{2}\\cdot\\frac{1}{2}.\n$$\n- For $(X_{2},X_{3})$: from 000 get (0,0); from 011 get (1,1); from 101 get (0,1); from 110 get (1,0), each pair once, so\n$$\nP(X_{2}=b_{1},X_{3}=b_{2})=\\frac{1}{4}=\\frac{1}{2}\\cdot\\frac{1}{2}.\n$$\nThus all marginals are uniform and all pairs are independent.\n\nTherefore, the unique valid choice among the given options is C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "While verifying properties of a given distribution is a good start, the real power comes from constructing these distributions ourselves. This practice delves into one of the most elegant and widely used constructions: the family of linear congruential hash functions. You will see how simple modular arithmetic allows us to create a vast family of functions where any randomly chosen function is guaranteed to be 2-wise independent, a property essential for analyzing randomized data structures and algorithms. ",
            "id": "1420528",
            "problem": "In the design of randomized algorithms and data structures, 2-wise independent families of hash functions are a fundamental tool for providing theoretical guarantees with a limited amount of randomness. A family of functions $H$, where each function $h \\in H$ maps a universe $U$ to a range $V$, is called 2-wise independent if for any two distinct elements $x_1, x_2 \\in U$ and any two (not necessarily distinct) elements $y_1, y_2 \\in V$, the probability of a randomly chosen $h$ from $H$ satisfying both $h(x_1) = y_1$ and $h(x_2) = y_2$ is exactly $1/|V|^2$.\n\nA common construction for such a family involves modular arithmetic. Consider a universe of keys given by the integers $U = \\{0, 1, ..., 99\\}$. We wish to map these keys to a hash table. For this purpose, we select the smallest prime number $p$ such that $p \\ge |U|$, which is $p = 101$.\n\nThe family of hash functions, $H$, is defined as the set of all functions $h_{a,b}$ from the set $\\{0, 1, ..., 100\\}$ to itself, of the form:\n$$h_{a,b}(x) = (ax + b) \\pmod{101}$$\nwhere the coefficients $a$ and $b$ can be any integers in the set $\\{0, 1, ..., 100\\}$.\n\nA key property of this construction is that for any two distinct keys $x_1, x_2$ and any two hash values $y_1, y_2$, there exists exactly one function $h_{a,b}$ in the family $H$ that satisfies both conditions $h_{a,b}(x_1) = y_1$ and $h_{a,b}(x_2) = y_2$.\n\nYour task is to find the specific coefficient $a$ for the unique hash function $h_{a,b}$ that satisfies the following two mapping requirements simultaneously:\n1. It maps the key $x_1 = 10$ to the hash value $y_1 = 50$.\n2. It maps the key $x_2 = 20$ to the hash value $y_2 = 70$.\n\nProvide the value of the coefficient $a$. The answer must be an integer.",
            "solution": "We are working in the finite field of integers modulo the prime $101$. The hash family is $h_{a,b}(x) \\equiv ax + b \\pmod{101}$. The two mapping requirements give the simultaneous congruences:\n$$10a + b \\equiv 50 \\pmod{101},$$\n$$20a + b \\equiv 70 \\pmod{101}.$$\n\nSubtract the first congruence from the second to eliminate $b$:\n$$\\left(20a + b\\right) - \\left(10a + b\\right) \\equiv 70 - 50 \\pmod{101},$$\nwhich simplifies to\n$$10a \\equiv 20 \\pmod{101}.$$\n\nSince $101$ is prime and $10 \\not\\equiv 0 \\pmod{101}$, $10$ has a multiplicative inverse modulo $101$. To find $10^{-1} \\pmod{101}$, use the extended Euclidean algorithm:\n$$101 = 10 \\cdot 10 + 1 \\implies 1 = 101 - 10 \\cdot 10.$$\nReducing modulo $101$ gives\n$$10 \\cdot (-10) \\equiv 1 \\pmod{101},$$\nso\n$$10^{-1} \\equiv -10 \\equiv 91 \\pmod{101}.$$\n\nMultiply both sides of $10a \\equiv 20 \\pmod{101}$ by $10^{-1}$ to solve for $a$:\n$$a \\equiv 20 \\cdot 10^{-1} \\equiv 20 \\cdot 91 \\pmod{101}.$$\nCompute the product modulo $101$:\n$$20 \\cdot 91 = 1820 \\equiv 1820 - 101 \\cdot 18 = 1820 - 1818 = 2 \\pmod{101}.$$\n\nThus, the required coefficient is $a \\equiv 2 \\pmod{101}$. Taking the representative in $\\{0,1,\\ldots,100\\}$ gives $a = 2$. A quick check: with $a=2$, from $10a + b \\equiv 50$ we get $b \\equiv 50 - 20 \\equiv 30$, and then $20a + b \\equiv 40 + 30 \\equiv 70$, as required.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Beyond constructing objects with limited independence, a more general derandomization technique is the method of conditional expectations. This powerful method provides a deterministic recipe for finding an object that is guaranteed to exist by a probabilistic argument. This exercise walks you through applying this method to a geometric partitioning problem, demonstrating how to make a sequence of deterministic, locally optimal choices that lead to a globally desirable outcome, effectively removing the need for random coin flips. ",
            "id": "1420518",
            "problem": "In computational geometry and machine learning, a fundamental task is to partition a set of points using a hyperplane. Given a set of $n$ data vectors $v_1, v_2, \\dots, v_n$ in $\\mathbb{R}^d$, we wish to find a separating hyperplane through the origin, defined by a normal vector $u$, that bisects the set as evenly as possible. The two partitions are formed by points with a positive dot product with $u$ and points with a negative dot product. The \"imbalance\" of a partition is the absolute difference between the number of points on each side.\n\nWe restrict our search for the normal vector to the discrete set $u \\in \\{-1, 1\\}^d$. A simple randomized approach would be to pick a vector $u$ uniformly at random from this set. However, we seek a deterministic algorithm. The method of conditional expectations provides a way to derandomize such a process.\n\nInstead of working with the imbalance directly, which is difficult, we use a surrogate \"potential function\" $W(u)$. We will choose the components of $u = (\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_d)$ one by one, from $k=1$ to $d$. At each step $k$, we choose $\\epsilon_k \\in \\{-1, 1\\}$ to minimize the conditional expectation of $W(u)$, given the choices made so far. If there is a tie, we choose $\\epsilon_k = 1$.\n\nYour task is to apply this algorithm to find the vector $u \\in \\{-1, 1\\}^3$ for the following specific case:\nThe dimension is $d=3$.\nThe set of vectors consists of $n=2$ vectors:\n$v_1 = (2, -1, 2)$\n$v_2 = (1, 3, -1)$\n\nThe potential function is given by $W(u) = \\sum_{i=1}^{2} \\cosh(\\lambda (v_i \\cdot u))$, where $\\lambda$ is a given positive constant. You can assume that for any choice of $u$, the dot products $v_i \\cdot u$ are non-zero.\n\nWhich of the following vectors $u$ is found by this deterministic algorithm?\n\nA. $(1, 1, 1)$\n\nB. $(1, 1, -1)$\n\nC. $(1, -1, 1)$\n\nD. $(1, -1, -1)$\n\nE. $(-1, 1, 1)$\n\nF. $(-1, 1, -1)$\n\nG. $(-1, -1, 1)$\n\nH. $(-1, -1, -1)$",
            "solution": "We use the method of conditional expectations with the random process that selects each coordinate independently and uniformly from $\\{-1,1\\}$. Let $u=(\\epsilon_{1},\\epsilon_{2},\\epsilon_{3})$ and for $i\\in\\{1,2\\}$ define $s_{i}=v_{i}\\cdot u$. The potential is $W(u)=\\sum_{i=1}^{2}\\cosh(\\lambda s_{i})$ with $\\lambda>0$. A key identity for independent Rademacher variables $\\delta_{j}$ is\n$$\n\\mathbb{E}\\!\\left[\\cosh\\!\\left(\\lambda\\left(a+\\sum_{j}b_{j}\\delta_{j}\\right)\\right)\\right]\n=\\frac{\\mathbb{E}\\!\\left[\\exp\\!\\left(\\lambda\\left(a+\\sum_{j}b_{j}\\delta_{j}\\right)\\right)\\right]+\\mathbb{E}\\!\\left[\\exp\\!\\left(-\\lambda\\left(a+\\sum_{j}b_{j}\\delta_{j}\\right)\\right)\\right]}{2}\n=\\cosh(\\lambda a)\\prod_{j}\\cosh(\\lambda b_{j}),\n$$\nwhich follows from independence and $\\mathbb{E}[\\exp(t b \\delta)]=\\cosh(t b)$.\n\nStep $k=1$. We choose $\\epsilon_{1}$ to minimize $\\mathbb{E}[W\\mid \\epsilon_{1}]$, where the expectation is over $\\epsilon_{2},\\epsilon_{3}$. Here $v_{1}=(2,-1,2)$ and $v_{2}=(1,3,-1)$. For $i=1$, $s_{1}=2\\epsilon_{1}-\\epsilon_{2}+2\\epsilon_{3}$; for $i=2$, $s_{2}=\\epsilon_{1}+3\\epsilon_{2}-\\epsilon_{3}$. Using the identity with $a=v_{i,1}\\epsilon_{1}$ and random terms $b_{2}=v_{i,2}$, $b_{3}=v_{i,3}$,\n$$\n\\mathbb{E}\\!\\left[\\cosh(\\lambda s_{i})\\mid \\epsilon_{1}\\right]=\\cosh(\\lambda v_{i,1}\\epsilon_{1})\\cosh(\\lambda v_{i,2})\\cosh(\\lambda v_{i,3}).\n$$\nSince $\\cosh$ is even, $\\cosh(\\lambda v_{i,1}\\epsilon_{1})=\\cosh(\\lambda v_{i,1})$, so $\\mathbb{E}[W\\mid \\epsilon_{1}]$ is independent of $\\epsilon_{1}$. By the tie rule, we set $\\epsilon_{1}=1$.\n\nStep $k=2$. With $\\epsilon_{1}=1$ fixed, choose $\\epsilon_{2}$ to minimize $\\mathbb{E}[W\\mid \\epsilon_{1}=1,\\epsilon_{2}]$, expecting only over $\\epsilon_{3}$. For each $i$,\n$$\n\\mathbb{E}\\!\\left[\\cosh(\\lambda s_{i})\\mid \\epsilon_{1}=1,\\epsilon_{2}\\right]=\\cosh\\!\\left(\\lambda(v_{i,1}\\cdot 1+v_{i,2}\\epsilon_{2})\\right)\\cosh(\\lambda v_{i,3}).\n$$\nThus\n$$\nF(\\epsilon_{2})=\\cosh(\\lambda(2-\\epsilon_{2}))\\cosh(2\\lambda)+\\cosh(\\lambda(1+3\\epsilon_{2}))\\cosh(\\lambda).\n$$\nEvaluate the two choices:\n$$\nF(1)=\\cosh(\\lambda)\\cosh(2\\lambda)+\\cosh(4\\lambda)\\cosh(\\lambda),\\quad\nF(-1)=\\cosh(3\\lambda)\\cosh(2\\lambda)+\\cosh(2\\lambda)\\cosh(\\lambda).\n$$\nTheir difference simplifies using $\\cosh x\\cosh y=\\frac{\\cosh(x+y)+\\cosh(x-y)}{2}$:\n$$\nF(1)-F(-1)=\\cosh(4\\lambda)\\cosh(\\lambda)-\\cosh(3\\lambda)\\cosh(2\\lambda)\n=\\frac{\\cosh(5\\lambda)+\\cosh(3\\lambda)-\\cosh(5\\lambda)-\\cosh(\\lambda)}{2}\n=\\frac{\\cosh(3\\lambda)-\\cosh(\\lambda)}{2}>0,\n$$\nsince $\\lambda>0$. Hence $F(-1)$ is smaller and we choose $\\epsilon_{2}=-1$.\n\nStep $k=3$. With $\\epsilon_{1}=1$, $\\epsilon_{2}=-1$, we choose $\\epsilon_{3}$ to minimize the actual $W$ (no expectation remains). Compute\n$$\ns_{1}=2\\cdot 1-(-1)+2\\epsilon_{3}=3+2\\epsilon_{3},\\quad s_{2}=1+3(-1)-\\epsilon_{3}=-2-\\epsilon_{3}.\n$$\nThus\n$$\nW(\\epsilon_{3}=1)=\\cosh(5\\lambda)+\\cosh(3\\lambda),\\quad\nW(\\epsilon_{3}=-1)=\\cosh(\\lambda)+\\cosh(\\lambda)=2\\cosh(\\lambda).\n$$\nSince $\\cosh(5\\lambda)+\\cosh(3\\lambda)>2\\cosh(\\lambda)$ for $\\lambda>0$, the minimum occurs at $\\epsilon_{3}=-1$.\n\nTherefore the deterministic algorithm yields $u=(1,-1,-1)$, which corresponds to option D.",
            "answer": "$$\\boxed{D}$$"
        }
    ]
}