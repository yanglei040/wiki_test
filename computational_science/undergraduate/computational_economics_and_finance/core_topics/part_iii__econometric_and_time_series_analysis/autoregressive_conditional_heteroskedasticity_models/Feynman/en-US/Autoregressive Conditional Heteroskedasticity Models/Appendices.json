{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of using GARCH models for financial forecasting is the assumption of covariance stationarity, which ensures that the unconditional variance of the process is finite and constant over time. This practice roots your understanding in both theory and simulation, guiding you to first derive the famous stationarity condition $\\alpha_1 + \\beta_1 \\lt 1$. You will then bring this condition to life by simulating processes that obey, border, and violate it, providing a concrete visualization of what stationarity means in practice .",
            "id": "2373513",
            "problem": "Consider the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model of order $\\left(1,1\\right)$ for a zero-mean return process $\\left\\{r_t\\right\\}$ defined by the stochastic difference equations\n$$r_t \\;=\\; \\sigma_t\\,\\varepsilon_t,$$\n$$\\sigma_t^2 \\;=\\; \\omega \\;+\\; \\alpha_1\\,r_{t-1}^2 \\;+\\; \\beta_1\\,\\sigma_{t-1}^2,$$\nwhere $\\left\\{\\varepsilon_t\\right\\}$ is Independent and Identically Distributed (IID) with $\\mathbb{E}\\!\\left[\\varepsilon_t\\right]=0$ and $\\mathbb{E}\\!\\left[\\varepsilon_t^2\\right]=1$, and where $\\omega \\gt 0$, $\\alpha_1 \\ge 0$, $\\beta_1 \\ge 0$. In computational economics and finance, a key question is covariance stationarity, i.e., the existence of a finite, time-invariant unconditional second moment $\\mathbb{E}\\!\\left[r_t^2\\right]$.\n\nYour tasks are as follows.\n\nTask A: Starting only from the model definitions and the laws of iterated expectations and independence, derive the scalar linear recursion satisfied by the unconditional second moment $\\mu_t \\equiv \\mathbb{E}\\!\\left[r_t^2\\right] = \\mathbb{E}\\!\\left[\\sigma_t^2\\right]$, and determine a necessary and sufficient condition, in terms of $\\alpha_1$ and $\\beta_1$, under which $\\mu_t$ converges to a finite limit as $t \\to \\infty$. If the limit exists, express it symbolically in terms of $\\omega$, $\\alpha_1$, and $\\beta_1$.\n\nTask B: Design an algorithm that, given parameters $\\left(\\omega,\\alpha_1,\\beta_1\\right)$, uses a long simulation of the GARCH$\\left(1,1\\right)$ process to empirically assess covariance stationarity by verifying two properties:\n- Convergence: the long-run sample mean of $r_t^2$ is close to the theoretical unconditional second moment from Task A when that moment exists.\n- Stability: block means of $r_t^2$ computed over equal-sized, contiguous time blocks in the post-burn-in sample are stable relative to each other and do not exhibit persistent drift. Your algorithm should operationalize stability with a quantitative diagnostic that compares early and late segments of the series and a dispersion measure across blocks.\n\nTask C: Implement a complete, runnable program that:\n- Simulates the model using IID standard normal innovations for each parameter set in the test suite below, with total length $T = 120{,}000$ and a burn-in of $B = 20{,}000$ observations discarded prior to diagnostics. Initialize $\\sigma_0^2$ to a strictly positive finite value that is compatible with all cases. Use a fixed random seed to ensure reproducibility.\n- Uses $K = 8$ equal-sized blocks on the post-burn-in sample to compute block means of $r_t^2$.\n- Classifies each test case as covariance-stationary or not according to the following quantitative criteria:\n  - Let $\\widehat{m}$ denote the post-burn-in sample mean of $r_t^2$ and let $m^\\star$ denote the theoretical unconditional second moment from Task A whenever it exists. Define the relative error $\\left|\\widehat{m} - m^\\star\\right|/m^\\star$.\n  - Let $m_{\\text{first}}$ and $m_{\\text{last}}$ be the sample means of $r_t^2$ on the first and last quarters, respectively, of the post-burn-in sample, and define the growth ratio $d \\equiv m_{\\text{last}}/m_{\\text{first}}$.\n  - Let $m_1,\\dots,m_K$ be the block means, and define the coefficient of variation $\\mathrm{CV} \\equiv \\mathrm{sd}\\!\\left(m_1,\\dots,m_K\\right)/\\overline{m}$ with $\\overline{m}$ the arithmetic mean of the $K$ block means.\n  - Classify a parameter set as covariance-stationary if and only if the theoretical unconditional second moment exists and all of the following hold simultaneously: the relative error is at most $\\tau = 0.10$, the growth ratio satisfies $d \\le \\rho = 1.20$, and the block stability satisfies $\\mathrm{CV} \\le \\kappa = 0.20$.\n- Produces a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean corresponding to the classification for the associated test case in the order given below, with no spaces.\n\nTest Suite:\n- Case A (sum slightly less than $1$): $\\omega = 0.1$, $\\alpha_1 = 0.05$, $\\beta_1 = 0.90$.\n- Case B (sum equal to $1$): $\\omega = 0.1$, $\\alpha_1 = 0.05$, $\\beta_1 = 0.95$.\n- Case C (sum slightly greater than $1$): $\\omega = 0.1$, $\\alpha_1 = 0.05$, $\\beta_1 = 0.98$.\n\nAnswer specification:\n- The final output must be a single line containing a list of three boolean values in the exact order $\\left[\\text{Case A},\\text{Case B},\\text{Case C}\\right]$, e.g., `[True,False,False]`.\n- No physical units are involved. All numerical thresholds must be implemented exactly as specified above.",
            "solution": "The problem requires a two-part analysis of the GARCH$\\left(1,1\\right)$ model: first, a theoretical derivation of the condition for covariance stationarity, and second, the design and implementation of an empirical test for this property based on simulation.\n\nWe begin with the theoretical derivation as specified in Task A. The GARCH$\\left(1,1\\right)$ model is defined by the equations:\n$$r_t = \\sigma_t \\varepsilon_t$$\n$$\\sigma_t^2 = \\omega + \\alpha_1 r_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2$$\nwhere $\\left\\{\\varepsilon_t\\right\\}$ is an IID process with $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\mathbb{E}[\\varepsilon_t^2] = 1$. The parameters satisfy $\\omega > 0$, $\\alpha_1 \\ge 0$, and $\\beta_1 \\ge 0$. We are interested in the unconditional second moment, $\\mu_t \\equiv \\mathbb{E}[r_t^2]$.\n\nLet $\\mathcal{F}_{t-1}$ denote the sigma-algebra representing the information available at time $t-1$, which includes all past values of $\\varepsilon_t$. By definition, $\\sigma_t^2$ is a function of past returns and variances, and is therefore $\\mathcal{F}_{t-1}$-measurable. The innovation $\\varepsilon_t$ is independent of $\\mathcal{F}_{t-1}$.\n\nFirst, we establish the identity $\\mu_t = \\mathbb{E}[\\sigma_t^2]$. Using the law of iterated expectations:\n$$\\mu_t = \\mathbb{E}[r_t^2] = \\mathbb{E}[\\sigma_t^2 \\varepsilon_t^2] = \\mathbb{E}\\left[\\mathbb{E}[\\sigma_t^2 \\varepsilon_t^2 | \\mathcal{F}_{t-1}]\\right]$$\nSince $\\sigma_t^2$ is $\\mathcal{F}_{t-1}$-measurable and $\\varepsilon_t$ is independent of $\\mathcal{F}_{t-1}$:\n$$\\mathbb{E}[\\sigma_t^2 \\varepsilon_t^2 | \\mathcal{F}_{t-1}] = \\sigma_t^2 \\mathbb{E}[\\varepsilon_t^2 | \\mathcal{F}_{t-1}] = \\sigma_t^2 \\mathbb{E}[\\varepsilon_t^2] = \\sigma_t^2 \\cdot 1 = \\sigma_t^2$$\nSubstituting this back, we obtain:\n$$\\mu_t = \\mathbb{E}[\\sigma_t^2]$$\nThis confirms the identity provided in the problem statement.\n\nNext, we derive the recurrence relation for $\\mu_t$. We take the unconditional expectation of the variance equation:\n$$\\mathbb{E}[\\sigma_t^2] = \\mathbb{E}[\\omega + \\alpha_1 r_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2]$$\nBy the linearity of the expectation operator:\n$$\\mathbb{E}[\\sigma_t^2] = \\omega + \\alpha_1 \\mathbb{E}[r_{t-1}^2] + \\beta_1 \\mathbb{E}[\\sigma_{t-1}^2]$$\nUsing our definitions, $\\mu_t = \\mathbb{E}[\\sigma_t^2]$ and $\\mu_{t-1} = \\mathbb{E}[r_{t-1}^2] = \\mathbb{E}[\\sigma_{t-1}^2]$, we arrive at the scalar linear recursion for the unconditional second moment:\n$$\\mu_t = \\omega + (\\alpha_1 + \\beta_1) \\mu_{t-1}$$\nThis is a first-order linear non-homogeneous recurrence relation. For $\\mu_t$ to converge to a finite, time-invariant limit $\\mu$ as $t \\to \\infty$, the coefficient of the dynamic term must have a magnitude less than $1$. Given the non-negativity constraints $\\alpha_1 \\ge 0$ and $\\beta_1 \\ge 0$, the necessary and sufficient condition for the existence of a finite, time-invariant second moment is:\n$$\\alpha_1 + \\beta_1 < 1$$\nIf this condition holds, the process is said to be weakly or covariance-stationary. The limiting unconditional variance, which we denote $m^\\star$, is found by setting $\\mu_t = \\mu_{t-1} = m^\\star$ in the recurrence, yielding the fixed-point equation:\n$$m^\\star = \\omega + (\\alpha_1 + \\beta_1) m^\\star$$\nSolving for $m^\\star$ gives the expression for the theoretical unconditional second moment:\n$$m^\\star = \\frac{\\omega}{1 - \\alpha_1 - \\beta_1}$$\nThis theoretical foundation directly informs the algorithmic design for Tasks B and C.\n\nThe algorithm to empirically assess covariance stationarity proceeds as follows.\n1.  **Theoretical Pre-check**: For a given parameter set $(\\omega, \\alpha_1, \\beta_1)$, the condition $\\alpha_1 + \\beta_1 < 1$ is first checked. If it is not met, the process is immediately classified as not covariance-stationary, as the theoretical unconditional second moment does not exist.\n2.  **Simulation**: If the pre-check passes, a GARCH$\\left(1,1\\right)$ process of length $T = 120,000$ is simulated. The innovations $\\varepsilon_t$ are drawn from an IID standard normal distribution. To ensure reproducibility, a fixed random seed is used. The simulation starts from an initial variance $\\sigma_0^2 > 0$.\n3.  **Data Preparation**: The first $B = 20,000$ data points are discarded as a burn-in period to remove dependence on the initial conditions. The analysis is conducted on the subsequent sample of $N = 100,000$ squared returns, $\\{r_t^2\\}_{t=B}^{T-1}$.\n4.  **Diagnostic Evaluation**: Three quantitative metrics are computed to assess convergence and stability.\n    a.  **Convergence criterion**: The sample mean of the post-burn-in squared returns, $\\widehat{m}$, is computed. Its relative error with respect to the theoretical value $m^\\star$, given by $|\\widehat{m} - m^\\star|/m^\\star$, must be less than or equal to a tolerance $\\tau = 0.10$.\n    b.  **Stability criterion (Trend)**: The post-burn-in sample is divided into four equal quarters. The ratio of the mean of the last quarter ($m_{\\text{last}}$) to the mean of the first quarter ($m_{\\text{first}}$), denoted by $d = m_{\\text{last}}/m_{\\text{first}}$, is calculated. A stationary series should not exhibit significant drift, so this ratio must be less than or equal to a threshold $\\rho = 1.20$.\n    c.  **Stability criterion (Dispersion)**: The post-burn-in sample is partitioned into $K=8$ non-overlapping blocks of equal size. The mean of each block, $m_k$, is computed. The stability of the process is assessed by the coefficient of variation (CV) of these block means, $\\mathrm{CV} = \\mathrm{sd}(m_1, \\dots, m_K) / \\overline{m}$, where $\\mathrm{sd}(\\cdot)$ is the sample standard deviation and $\\overline{m}$ is the mean of the block means. A low CV indicates that the variance is stable across the sample. This value must be less than or equal to a threshold $\\kappa = 0.20$.\n5.  **Final Classification**: A parameter set is classified as covariance-stationary if and only if the theoretical condition $\\alpha_1 + \\beta_1 < 1$ is satisfied AND all three empirical criteria (relative error, growth ratio, block CV) meet their specified thresholds. Otherwise, it is classified as not covariance-stationary. This complete procedure is implemented for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_garch_analysis(omega, alpha_1, beta_1, T, B, K, tau, rho, kappa, eps_series):\n    \"\"\"\n    Simulates a GARCH(1,1) process and assesses covariance stationarity.\n\n    Args:\n        omega (float): GARCH parameter.\n        alpha_1 (float): GARCH parameter.\n        beta_1 (float): GARCH parameter.\n        T (int): Total length of the simulation.\n        B (int): Length of the burn-in period.\n        K (int): Number of blocks for stability analysis.\n        tau (float): Relative error threshold.\n        rho (float): Growth ratio threshold.\n        kappa (float): Block stability (CV) threshold.\n        eps_series (np.ndarray): Pre-generated standard normal innovations.\n\n    Returns:\n        bool: True if classified as covariance-stationary, False otherwise.\n    \"\"\"\n    # Step 1: Theoretical Pre-check\n    if alpha_1 + beta_1 >= 1:\n        return False\n\n    # Theoretical unconditional second moment\n    m_star = omega / (1 - alpha_1 - beta_1)\n\n    # Step 2: Simulation\n    sigma_sq = np.zeros(T)\n    r_sq = np.zeros(T)\n    \n    # Initialization: choose a positive finite value compatible with all cases,\n    # as required by the problem.\n    sigma_sq[0] = 0.1\n\n    for t in range(T - 1):\n        r_sq[t] = sigma_sq[t] * eps_series[t]**2\n        sigma_sq[t+1] = omega + alpha_1 * r_sq[t] + beta_1 * sigma_sq[t]\n    \n    # Compute the last value of r_sq\n    r_sq[T-1] = sigma_sq[T-1] * eps_series[T-1]**2\n\n    # Step 3: Data Preparation\n    analysis_sample = r_sq[B:]\n    N = T - B\n    \n    # Step 4: Diagnostic Evaluation\n    \n    # a. Convergence Criterion\n    m_hat = np.mean(analysis_sample)\n    relative_error = np.abs(m_hat - m_star) / m_star\n    \n    # b. Stability Criterion (Trend)\n    n_quarter = N // 4\n    m_first = np.mean(analysis_sample[:n_quarter])\n    m_last = np.mean(analysis_sample[-n_quarter:])\n    \n    # To avoid division by zero if m_first happens to be pathologically small\n    if m_first = 0:\n        growth_ratio = float('inf')\n    else:\n        growth_ratio = m_last / m_first\n\n    # c. Stability Criterion (Dispersion)\n    block_size = N // K\n    block_means = np.array([\n        np.mean(analysis_sample[k*block_size : (k+1)*block_size]) for k in range(K)\n    ])\n    mean_of_block_means = np.mean(block_means) # This is equivalent to m_hat\n    \n    if mean_of_block_means = 0:\n        cv = float('inf')\n    else:\n        # Use ddof=1 for sample standard deviation\n        cv = np.std(block_means, ddof=1) / mean_of_block_means\n\n    # Step 5: Final Classification\n    is_conv_ok = relative_error = tau\n    is_growth_ok = growth_ratio = rho\n    is_stability_ok = cv = kappa\n\n    return is_conv_ok and is_growth_ok and is_stability_ok\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    # Define parameters from the problem statement.\n    T = 120000\n    B = 20000\n    K = 8\n    tau = 0.10\n    rho = 1.20\n    kappa = 0.20\n\n    # Define the test cases.\n    test_cases = [\n        {'name': 'Case A', 'omega': 0.1, 'alpha_1': 0.05, 'beta_1': 0.90},\n        {'name': 'Case B', 'omega': 0.1, 'alpha_1': 0.05, 'beta_1': 0.95},\n        {'name': 'Case C', 'omega': 0.1, 'alpha_1': 0.05, 'beta_1': 0.98},\n    ]\n\n    # Set a fixed random seed for reproducibility of the entire program run.\n    # The same stream of random numbers will be used sequentially for all cases.\n    np.random.seed(42)\n    \n    # Pre-generate all random numbers needed.\n    eps_series = np.random.standard_normal(T)\n\n    results = []\n    for case in test_cases:\n        is_stationary = run_garch_analysis(\n            omega=case['omega'],\n            alpha_1=case['alpha_1'],\n            beta_1=case['beta_1'],\n            T=T, B=B, K=K,\n            tau=tau, rho=rho, kappa=kappa,\n            eps_series=eps_series\n        )\n        results.append(is_stationary)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we understand a model's foundational properties, the practical task of model selection begins. This exercise immerses you in the trade-off between model complexity and fit by pitting a GARCH(1,1) model against its simpler ARCH(1) counterpart . By using the Bayesian Information Criterion (BIC) on simulated data, you will discover how the principle of parsimony can sometimes favor a simpler (but incorrect) model, a crucial lesson for any practitioner navigating real-world data.",
            "id": "2373512",
            "problem": "You are to write a complete, runnable program that constructs simulated financial return series and evaluates model selection between two volatility models using Gaussian Quasi-Maximum Likelihood Estimation (QMLE). The task is framed within autoregressive conditional heteroskedasticity, specifically comparing an autoregressive conditional heteroskedasticity of order one (ARCH(1)) against a generalized autoregressive conditional heteroskedasticity of order one, one (GARCH(1,1)) when the data are generated by a low-persistence GARCH(1,1) process.\n\nStart from the following core definitions and widely accepted facts:\n\n- A mean-zero return series $\\{r_t\\}_{t=1}^T$ with conditional heteroskedasticity is modeled as $r_t = \\sigma_t z_t$, where $\\{z_t\\}$ are independent and identically distributed standard normal variables, and $\\sigma_t^2$ is the conditional variance.\n- The ARCH($p$) model specifies conditional variance as $\\sigma_t^2 = \\omega + \\sum_{i=1}^p \\alpha_i r_{t-i}^2$, with $\\omega  0$, $\\alpha_i \\ge 0$, and $\\sum_{i=1}^p \\alpha_i  1$ for covariance stationarity.\n- The GARCH(1,1) model specifies conditional variance as $\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2$, with $\\omega  0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta  1$ for covariance stationarity.\n- Under the Gaussian QMLE, given a parametric volatility recursion producing $\\{\\sigma_t^2(\\theta)\\}$ for parameters $\\theta$, the log-likelihood is\n$$\n\\ell_T(\\theta) = -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log(2\\pi) + \\log\\left(\\sigma_t^2(\\theta)\\right) + \\frac{r_t^2}{\\sigma_t^2(\\theta)} \\right).\n$$\n- The Bayesian Information Criterion (BIC) for a model with $k$ free parameters and sample size $n$ is\n$$\n\\mathrm{BIC} = -2 \\ell_T(\\hat{\\theta}) + k \\log(n),\n$$\nwhere $\\hat{\\theta}$ is the parameter that maximizes the log-likelihood.\n\nYour program must:\n\n1. For each test case, simulate a return series $\\{r_t\\}_{t=1}^T$ from a GARCH(1,1) process with the given parameters and low persistence (that is, $\\alpha + \\beta$ strictly less than $1$ and numerically small). Use standard normal innovations $\\{z_t\\}$ and a fixed burn-in of $B = 1000$ observations discarded before keeping the last $T$ observations to mitigate the effect of initial conditions. Use the unconditional variance as the initial variance, namely $\\sigma_0^2 = \\omega / (1 - \\alpha - \\beta)$.\n2. Fit by Gaussian QMLE both:\n   - An ARCH($p$) model with the specified order $p$.\n   - A GARCH(1,1) model.\n   The estimation must respect positivity and stationarity constraints as stated above. You may use any numerically stable, differentiable reparameterization to impose these constraints during optimization.\n3. For each fitted model, compute its maximized log-likelihood and its BIC. Use $k = 1 + p$ for ARCH($p$) and $k = 3$ for GARCH(1,1), and $n = T$.\n4. Define the indicator that the simulated dataset “fools” the standard ARCH($p$) model as a boolean that is true if the ARCH($p$) BIC is strictly smaller than the GARCH(1,1) BIC when the data are generated by GARCH(1,1). That is, output true if $\\mathrm{BIC}_{\\mathrm{ARCH}(p)}  \\mathrm{BIC}_{\\mathrm{GARCH}(1,1)}$.\n\nImplementation requirements and numerical details:\n\n- Use the unconditional variance associated with the current parameter guess to initialize recursions during likelihood evaluation for both models. For the ARCH($p$) likelihood recursion, initialize pre-sample squared returns $r_{t-i}^2$ for $i  t$ with this unconditional variance.\n- Use standard normal innovations for simulation.\n- Ensure that conditional variances remain strictly positive for all time steps during simulation and likelihood evaluation.\n- Use any deterministic numerical optimization routine to maximize the Gaussian log-likelihood for each model, with constraints enforced via smooth transformations of unconstrained parameters.\n\nTest suite:\n\nFor each of the following three parameter sets, run the full simulation-estimation-selection pipeline described above and return a boolean result indicating whether the ARCH($p$) model is selected by BIC over the GARCH(1,1) model:\n\n- Test case 1 (happy path, low persistence): $T = 1200$, $\\omega = 0.05$, $\\alpha = 0.08$, $\\beta = 0.18$, $p = 1$, simulation seed $= 11$.\n- Test case 2 (smaller sample): $T = 800$, $\\omega = 0.10$, $\\alpha = 0.05$, $\\beta = 0.25$, $p = 1$, simulation seed $= 22$.\n- Test case 3 (near white noise volatility): $T = 1000$, $\\omega = 0.02$, $\\alpha = 0.03$, $\\beta = 0.05$, $p = 1$, simulation seed $= 33$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the three boolean results corresponding to the three test cases as a comma-separated list enclosed in square brackets (for example, `[True,False,True]`).",
            "solution": "The problem requires the implementation of a numerical experiment in financial econometrics. The objective is to evaluate the performance of the Bayesian Information Criterion (BIC) in selecting between a GARCH($1,1$) model and a simpler ARCH($p$) model when the data are, in fact, generated by a GARCH($1,1$) process with low persistence. This exercise is fundamental to understanding model misspecification and the trade-off between model parsimony and goodness-of-fit.\n\nThe procedure will be executed for each specified test case, following a rigorous three-step process: data simulation, model estimation, and model selection.\n\n**1. Data Simulation**\n\nThe foundation of this analysis is a simulated financial return series $\\{r_t\\}_{t=1}^T$. This series is generated from a GARCH($1,1$) process, defined by the equations:\n$$\nr_t = \\sigma_t z_t\n$$\n$$\n\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2\n$$\nwhere $\\{z_t\\}_{t=1}^T$ is a sequence of independent and identically distributed (i.i.d.) random variables drawn from the standard normal distribution, $z_t \\sim \\mathcal{N}(0, 1)$. The parameters $(\\omega, \\alpha, \\beta)$ are provided for each test case and satisfy the covariance stationarity conditions: $\\omega  0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta  1$.\n\nTo mitigate the influence of initial conditions on the generated series, we first simulate a longer series of length $T + B$, where $T$ is the desired sample size and $B = 1000$ is the burn-in period. The recursion is initialized using the unconditional variance of the process, $\\sigma_0^2 = \\frac{\\omega}{1 - \\alpha - \\beta}$. The first $B$ simulated observations are then discarded, leaving the final series $\\{r_t\\}_{t=1}^T$ for analysis.\n\n**2. Parameter Estimation via Quasi-Maximum Likelihood (QMLE)**\n\nFor each simulated series, we fit two competing models: an ARCH($p$) model and a GARCH($1,1$) model. The estimation is performed by maximizing the Gaussian quasi-log-likelihood function. Assuming Gaussian innovations, the log-likelihood for a sample of size $T$ given a parameter vector $\\theta$ is:\n$$\n\\ell_T(\\theta) = -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log(2\\pi) + \\log\\left(\\sigma_t^2(\\theta)\\right) + \\frac{r_t^2}{\\sigma_t^2(\\theta)} \\right)\n$$\nMaximizing $\\ell_T(\\theta)$ is equivalent to minimizing its negative. For optimization purposes, the constant term $-\\frac{T}{2}\\log(2\\pi)$ can be ignored, as it does not affect the location of the maximum. The objective function to be minimized is therefore:\n$$\n\\mathcal{L}(\\theta) = \\sum_{t=1}^T \\left( \\log\\left(\\sigma_t^2(\\theta)\\right) + \\frac{r_t^2}{\\sigma_t^2(\\theta)} \\right)\n$$\nA critical aspect of the estimation is enforcing the parameter constraints required for positivity of variance and stationarity. This is achieved through smooth, differentiable reparameterizations, which allow an unconstrained numerical optimizer to search over the valid parameter space.\n\nFor the GARCH($1,1$) model, the parameters are $\\theta_{GARCH} = (\\omega, \\alpha, \\beta)$. The constraints are $\\omega  0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta  1$. We map from an unconstrained vector $\\psi = (\\psi_0, \\psi_1, \\psi_2) \\in \\mathbb{R}^3$ to the constrained space as follows:\n- $\\omega = \\exp(\\psi_0)$\n- $\\alpha = \\frac{\\psi_1^2}{1 + \\psi_1^2}$\n- $\\beta = \\left(1 - \\alpha\\right) \\frac{\\psi_2^2}{1 + \\psi_2^2}$\nThis construction guarantees all constraints are met.\n\nFor the ARCH($p$) model, where $p=1$ in all test cases, the parameters are $\\theta_{ARCH} = (\\omega, \\alpha_1)$. The constraints are $\\omega  0$, $0 \\le \\alpha_1  1$. We map from an unconstrained vector $\\phi = (\\phi_0, \\phi_1) \\in \\mathbb{R}^2$:\n- $\\omega = \\exp(\\phi_0)$\n- $\\alpha_1 = \\frac{\\exp(\\phi_1)}{1 + \\exp(\\phi_1)}$\nThis transformation ensures $\\omega  0$ and $\\alpha_1 \\in (0, 1)$.\n\nDuring each evaluation of the likelihood function within the optimization routine, the conditional variance series $\\{\\sigma_t^2(\\theta)\\}_{t=1}^T$ must be computed. The recursion is initialized using the\nunconditional variance implied by the *current* parameter guess $\\theta$. For GARCH($1,1$), pre-sample values $r_0^2$ and $\\sigma_0^2$ are set to $\\frac{\\omega}{1-\\alpha-\\beta}$. For ARCH($1$), the pre-sample $r_0^2$ is set to $\\frac{\\omega}{1-\\alpha_1}$.\n\n**3. Model Selection via Bayesian Information Criterion (BIC)**\n\nAfter obtaining the maximized log-likelihood value $\\ell_T(\\hat{\\theta})$ for each model, we compute the BIC:\n$$\n\\mathrm{BIC} = -2 \\ell_T(\\hat{\\theta}) + k \\log(n)\n$$\nwhere $\\hat{\\theta}$ is the vector of estimated parameters, $k$ is the number of free parameters in the model, and $n=T$ is the sample size. The BIC penalizes models for complexity, with the term $k \\log(n)$ representing the penalty. A lower BIC value indicates a more preferred model.\n- For the ARCH($p$) model, $k = p + 1$. Since $p=1$, $k_{ARCH} = 2$.\n- For the GARCH($1,1$) model, $k_{GARCH} = 3$.\n\nThe final output for each test case is a boolean value indicating whether the ARCH($p$) model \"fools\" the selection criterion. This is true if its BIC is strictly less than the BIC of the GARCH($1,1$) model, i.e., $\\mathrm{BIC}_{\\mathrm{ARCH}(p)}  \\mathrm{BIC}_{\\mathrm{GARCH}(1,1)}$. This outcome suggests that, despite the data originating from a GARCH process, the more parsimonious ARCH model is favored due to the BIC's penalty on the extra parameter of the GARCH model. This is more likely when the true GARCH process has low persistence (small $\\alpha+\\beta$), making it difficult to distinguish from a simpler ARCH process, especially with smaller sample sizes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef simulate_garch11(T, B, omega, alpha, beta, seed):\n    \"\"\"\n    Simulates a GARCH(1,1) process.\n    \"\"\"\n    np.random.seed(seed)\n    total_len = T + B\n    \n    # Generate standard normal innovations\n    z = np.random.randn(total_len)\n    \n    r = np.zeros(total_len)\n    sigma2 = np.zeros(total_len)\n    \n    # Initial variance (unconditional variance)\n    uncond_var = omega / (1 - alpha - beta)\n    sigma2_t_minus_1 = uncond_var\n    # Use E[r_{t-1}^2] = sigma_{t-1}^2 for initialization of first r_t\n    r_t_minus_1_sq = uncond_var\n    \n    for t in range(total_len):\n        sigma2[t] = omega + alpha * r_t_minus_1_sq + beta * sigma2_t_minus_1\n        r[t] = np.sqrt(sigma2[t]) * z[t]\n        \n        sigma2_t_minus_1 = sigma2[t]\n        r_t_minus_1_sq = r[t]**2\n        \n    # Discard burn-in period\n    return r[B:]\n\ndef garch11_neg_loglike_factory(r):\n    \"\"\"\n    Factory for the negative log-likelihood of a GARCH(1,1) model.\n    \"\"\"\n    T = len(r)\n    r_sq = r**2\n\n    def neg_loglike(unconstrained_params):\n        # 1. Reparameterize to enforce constraints\n        psi_0, psi_1, psi_2 = unconstrained_params\n        omega = np.exp(psi_0)\n        # alpha is in [0, 1)\n        alpha = psi_1**2 / (1 + psi_1**2)\n        # beta is in [0, 1-alpha)\n        beta = (1 - alpha) * (psi_2**2 / (1 + psi_2**2))\n        \n        # 2. Check for stationarity to avoid division by zero\n        if (alpha + beta) >= 1.0:\n            return 1e9 # Return a large value if non-stationary\n\n        # 3. Calculate conditional variances\n        sigma2 = np.zeros(T)\n        uncond_var = omega / (1 - alpha - beta)\n        \n        # Initialize with unconditional variance\n        sigma2[0] = omega + alpha * uncond_var + beta * uncond_var\n        \n        for t in range(1, T):\n            sigma2[t] = omega + alpha * r_sq[t-1] + beta * sigma2[t-1]\n\n        # Prevent numerical issues with very small variances\n        sigma2 = np.maximum(sigma2, 1e-9)\n\n        # 4. Calculate log-likelihood\n        log_likelihood_sum = np.sum(np.log(sigma2) + r_sq / sigma2)\n        \n        return 0.5 * log_likelihood_sum\n\n    return neg_loglike\n\ndef arch1_neg_loglike_factory(r):\n    \"\"\"\n    Factory for the negative log-likelihood of an ARCH(1) model.\n    \"\"\"\n    T = len(r)\n    r_sq = r**2\n\n    def neg_loglike(unconstrained_params):\n        # 1. Reparameterize to enforce constraints\n        phi_0, phi_1 = unconstrained_params\n        omega = np.exp(phi_0)\n        # alpha1 is in (0, 1)\n        alpha1 = np.exp(phi_1) / (1 + np.exp(phi_1))\n\n        # 2. Check for stationarity\n        if alpha1 >= 1.0:\n            return 1e9\n        \n        # 3. Calculate conditional variances\n        sigma2 = np.zeros(T)\n        uncond_var = omega / (1 - alpha1)\n        \n        # Initialize with unconditional variance for pre-sample r_0^2\n        sigma2[0] = omega + alpha1 * uncond_var\n        \n        for t in range(1, T):\n            sigma2[t] = omega + alpha1 * r_sq[t-1]\n        \n        # Prevent numerical issues\n        sigma2 = np.maximum(sigma2, 1e-9)\n\n        # 4. Calculate log-likelihood\n        log_likelihood_sum = np.sum(np.log(sigma2) + r_sq / sigma2)\n        \n        return 0.5 * log_likelihood_sum\n\n    return neg_loglike\n    \ndef solve():\n    \"\"\"\n    Main function to run test cases and generate final output.\n    \"\"\"\n    test_cases = [\n        # T, omega, alpha, beta, p, seed\n        (1200, 0.05, 0.08, 0.18, 1, 11),\n        (800, 0.10, 0.05, 0.25, 1, 22),\n        (1000, 0.02, 0.03, 0.05, 1, 33),\n    ]\n\n    results = []\n    B = 1000 # Burn-in period\n\n    for case in test_cases:\n        T, omega_true, alpha_true, beta_true, p, seed = case\n        \n        # 1. Simulate data from GARCH(1,1)\n        r_sim = simulate_garch11(T, B, omega_true, alpha_true, beta_true, seed)\n        \n        # 2. Fit GARCH(1,1) model\n        garch_objective = garch11_neg_loglike_factory(r_sim)\n        # Initial guess for unconstrained params (psi_0, psi_1, psi_2)\n        # Corresponds roughly to omega=0.1, alpha=0.1, beta=0.8\n        x0_garch = [-2.3, 0.33, 2.8] \n        garch_res = minimize(garch_objective, x0_garch, method='BFGS')\n        \n        # maximized log-likelihood value (without constant)\n        min_neg_ll_garch = garch_res.fun\n        \n        # 3. Fit ARCH(p) model (here p=1)\n        arch_objective = arch1_neg_loglike_factory(r_sim)\n        # Initial guess for unconstrained params (phi_0, phi_1)\n        # Corresponds roughly to omega=0.1, alpha1=0.2\n        x0_arch = [-2.3, -1.38] \n        arch_res = minimize(arch_objective, x0_arch, method='BFGS')\n        \n        min_neg_ll_arch = arch_res.fun\n\n        # 4. Compute BIC for both models\n        k_garch = 3\n        # We need 2 * min_neg_ll since optimizer minimizes 0.5 * sum(...)\n        # The constant part of log-likelihood cancels out in comparison\n        bic_garch = 2 * min_neg_ll_garch + k_garch * np.log(T)\n        \n        k_arch = 1 + p\n        bic_arch = 2 * min_neg_ll_arch + k_arch * np.log(T)\n        \n        # 5. Compare BICs and record result\n        # True if ARCH(p) is wrongly selected\n        results.append(bic_arch  bic_garch)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Fitting a model is not the end of the econometrician's journey; rigorous validation is a critical final step. This practice focuses on post-estimation diagnostics, where you will implement the Ljung-Box test to check if your GARCH model has successfully captured all the volatility clustering in the data . Through carefully designed scenarios, you will not only learn how to execute this essential check but also appreciate its potential limitations, fostering a critical perspective on model validation.",
            "id": "2395745",
            "problem": "You are given the task of building a complete program that estimates a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model and applies a Ljung-Box diagnostic test to the squared standardized residuals to detect remaining Autoregressive Conditional Heteroskedasticity (ARCH) effects. The program must implement from first principles the following components grounded in core definitions: (i) estimation of a GARCH model via maximum likelihood under a Gaussian assumption, (ii) construction of standardized residuals using the estimated conditional variance sequence, and (iii) computation of the Ljung-Box test statistic using sample autocorrelations of a transformed series derived from the standardized residuals. The program must not rely on any prepackaged GARCH or diagnostic routines and must be reproducible with a fixed pseudorandom seed.\n\nConsider a univariate zero-mean return series $\\{r_t\\}_{t=1}^T$ with conditional variance $\\{\\sigma_t^2\\}_{t=1}^T$. A GARCH model specifies the conditional variance as a recursion driven by past squared innovations and past conditional variances. Standardized residuals are defined by dividing each innovation by the square root of its conditional variance. The Ljung-Box diagnostic is a portmanteau test that evaluates whether the sample autocorrelations of a series are collectively zero at a set of lags. Under a suitable null hypothesis and asymptotics, the Ljung-Box statistic has a reference distribution that enables calculation of a tail probability used for a decision rule at a given significance level.\n\nYour program must do the following for each test case in the suite below:\n1. Simulate a return series with a specified conditional variance recursion and normal innovations. Use a fixed pseudorandom seed $12345$ and a positive burn-in before collecting the last $T$ observations to reduce initialization effects.\n2. Fit a GARCH($1,1$) model to the simulated series by Quasi-Maximum Likelihood Estimation (QMLE) under a Gaussian likelihood, imposing positivity and a strict stationarity-type bound for parameters.\n3. Compute standardized residuals by dividing each innovation by the square root of the fitted conditional variance at that time.\n4. Apply the Ljung-Box test to the squared standardized residuals using the specified maximum lag $m$ for that test case. Use the standard definition of the sample autocorrelation based on sample covariances and the large-sample reference distribution to obtain a tail probability. Decide to reject the null if this tail probability is strictly less than the significance level $\\alpha = 0.05$.\n5. Record a boolean result for each test case indicating whether the diagnostic rejects the null hypothesis of no residual autocorrelation in the squared standardized residuals at the specified lags.\n\nTest suite:\n- Case A (well-specified baseline, happy path):\n  - Data generating process: GARCH($1,1$) with parameters $(\\omega,\\alpha,\\beta) = (0.05,0.05,0.90)$.\n  - Sample size: $T=3000$.\n  - Ljung-Box maximum lag: $m=20$.\n- Case B (misspecified model with remaining ARCH effects that should be detected):\n  - Data generating process: GARCH($2,1$) with parameters $(\\omega,\\alpha_1,\\alpha_2,\\beta) = (0.02,0.04,0.08,0.86)$.\n  - Sample size: $T=4000$.\n  - Ljung-Box maximum lag: $m=20$.\n- Case C (misspecified model with remaining ARCH effects that the Ljung-Box test may fail to detect because of an ill-chosen lag truncation and limited sample size):\n  - Data generating process: an ARCH process with a single long-lag effect at lag $L=10$, defined by $\\sigma_t^2 = \\omega + a_L \\cdot r_{t-L}^2$ with $(\\omega,a_L)=(0.05,0.80)$.\n  - Sample size: $T=800$.\n  - Ljung-Box maximum lag: $m=1$.\n\nImplementation details:\n- Use a Gaussian likelihood for estimation and Gaussian innovations for simulation. Explicitly constrain the GARCH($1,1$) parameters $(\\omega,\\alpha,\\beta)$ to satisfy $\\omega0$, $\\alpha\\ge 0$, $\\beta\\ge 0$, and $\\alpha+\\beta1$.\n- Standardize residuals using the fitted conditional variance path from the estimated GARCH($1,1$) model.\n- Implement the Ljung-Box statistic using the standard definition of sample autocorrelations based on centered data, and derive the tail probability under the large-sample reference distribution at $m$ lags.\n- Use a burn-in of at least $1000$ observations for all simulations before keeping the last $T$ observations.\n\nFinal output format:\n- Your program should produce a single line of output containing the three boolean decisions for Cases A, B, and C in that order, formatted as a comma-separated list enclosed in square brackets, for example, `[True,False,True]`.\n\nYour answer must be a complete, runnable program. No user input is required or allowed. There are no physical units involved in this problem; all numerical outputs are dimensionless booleans in the specified list format.",
            "solution": "The problem presented is a well-defined exercise in computational econometrics, requiring the implementation of a full pipeline for GARCH model analysis. It is scientifically grounded in established theories of time series analysis, is self-contained, and presents an objective, formalizable task. There are no identifiable flaws; therefore, a rigorous solution is warranted.\n\nThe solution involves several sequential steps: data simulation, model estimation via Quasi-Maximum Likelihood Estimation (QMLE), calculation of standardized residuals, and a diagnostic check using the Ljung-Box test.\n\nA general GARCH($p,q$) process for a zero-mean return series $\\{r_t\\}$ is defined by the equations:\n$$ r_t = \\sigma_t z_t $$\n$$ \\sigma_t^2 = \\omega + \\sum_{i=1}^{q} \\alpha_i r_{t-i}^2 + \\sum_{j=1}^{p} \\beta_j \\sigma_{t-j}^2 $$\nHere, $\\{z_t\\}$ is a sequence of independent and identically distributed (i.i.d.) random variables with zero mean and unit variance, which we assume to be standard normal, $z_t \\sim N(0,1)$. The parameters must satisfy $\\omega  0$, $\\alpha_i \\ge 0$, and $\\beta_j \\ge 0$ to ensure non-negative variance. For the conditional variance process to be weakly stationary, the condition $\\sum_{i=1}^{q} \\alpha_i + \\sum_{j=1}^{p} \\beta_j  1$ is required.\n\nThe problem requires estimating a GARCH($1,1$) model, for which the conditional variance recursion simplifies to:\n$$ \\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2 $$\nThe parameter vector to be estimated is $\\theta = (\\omega, \\alpha, \\beta)$.\n\nThe estimation is performed using QMLE. Assuming the innovations are conditionally normal, the log-likelihood for observation $t$, conditional on the information set $\\mathcal{F}_{t-1}$, is:\n$$ l_t(\\theta) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma_t^2(\\theta)) - \\frac{r_t^2}{2\\sigma_t^2(\\theta)} $$\nThe total log-likelihood for a sample of size $T$ is the sum $\\mathcal{L}(\\theta) = \\sum_{t=1}^T l_t(\\theta)$. The QMLE estimator $\\hat{\\theta}$ is the value of $\\theta$ that maximizes $\\mathcal{L}(\\theta)$, subject to the parameter constraints. This is equivalent to minimizing the negative log-likelihood, $-\\mathcal{L}(\\theta)$. The optimization is performed numerically, typically using a quasi-Newton method such as L-BFGS-B, which can handle box constraints on the parameters. The strict stationarity-type constraint, $\\alpha + \\beta  1$, is enforced within the objective function by returning an infinite penalty if it is violated. The recursion for $\\sigma_t^2$ is initialized by setting $\\sigma_1^2$ to the sample variance of the return series, a common and robust practice.\n\nOnce the parameters $\\hat{\\theta} = (\\hat{\\omega}, \\hat{\\alpha}, \\hat{\\beta})$ are estimated, the fitted conditional variance series $\\{\\hat{\\sigma}_t^2\\}_{t=1}^T$ is constructed using the GARCH($1,1$) recursion with these estimates. The standardized residuals are then computed as:\n$$ \\hat{\\epsilon}_t = \\frac{r_t}{\\hat{\\sigma}_t} $$\nIf the GARCH($1,1$) model is correctly specified, the sequence $\\{\\hat{\\epsilon}_t\\}$ should be approximately i.i.d. with unit variance. Consequently, the squared standardized residuals, $\\{\\hat{\\epsilon}_t^2\\}$, should exhibit no significant autocorrelation.\n\nTo test this hypothesis, we employ the Ljung-Box test on the series $x_t = \\hat{\\epsilon}_t^2$. First, we compute the sample autocorrelations of $\\{x_t\\}$ up to a specified maximum lag $m$. The sample autocorrelation at lag $k  0$ is defined as:\n$$ \\hat{\\rho}_k = \\frac{\\sum_{t=k+1}^T (x_t - \\bar{x})(x_{t-k} - \\bar{x})}{\\sum_{t=1}^T (x_t - \\bar{x})^2} $$\nwhere $\\bar{x}$ is the sample mean of $\\{x_t\\}$.\n\nThe Ljung-Box Q-statistic is then calculated as:\n$$ Q = T(T+2) \\sum_{k=1}^m \\frac{\\hat{\\rho}_k^2}{T-k} $$\nUnder the null hypothesis ($H_0$) that there is no autocorrelation in the series (i.e., $\\rho_1 = \\dots = \\rho_m = 0$), the statistic $Q$ asymptotically follows a chi-squared distribution with $m$ degrees of freedom, $Q \\sim \\chi^2(m)$. We calculate the p-value as $P(\\chi^2(m)  Q_{\\text{obs}})$, where $Q_{\\text{obs}}$ is the observed value of the statistic. If this p-value is strictly less than the specified significance level $\\alpha = 0.05$, we reject the null hypothesis, indicating the presence of remaining ARCH effects.\n\nThis entire procedure is applied to three test cases:\n1.  **Case A**: The data is generated from a GARCH($1,1$) process and a GARCH($1,1$) model is fitted. This is a well-specified case, and we anticipate that the diagnostic test will fail to reject the null hypothesis.\n2.  **Case B**: The data is generated from a GARCH($2,1$) process, but a GARCH($1,1$) model is fitted. This misspecification is expected to leave uncaptured dynamics, leading to autocorrelation in the squared standardized residuals. We anticipate that the Ljung-Box test will reject the null hypothesis.\n3.  **Case C**: The data is generated from an ARCH process with a specific lag structure at lag $L=10$. We fit a GARCH($1,1$) model and apply the Ljung-Box test with a maximum lag of only $m=1$. This test is poorly suited to detect the specific form of misspecification, as it only examines lag $1$. We anticipate that the test will fail to reject the null hypothesis, demonstrating a limitation of the diagnostic tool when misconfigured.\n\nThe implementation will synthesize these components into a single program that executes the analysis for all three cases and reports the boolean decision (reject/fail to reject) for each.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\ndef simulate_series(omega, alpha_coeffs, beta_coeffs, T, burn_in, seed):\n    \"\"\"\n    Simulates a time series from a general GARCH(p,q) process.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    q = len(alpha_coeffs)\n    p = len(beta_coeffs)\n    total_T = T + burn_in\n\n    r = np.zeros(total_T)\n    sigma2 = np.zeros(total_T)\n    z = rng.normal(size=total_T)\n\n    # Initialize with unconditional variance, assuming stationarity\n    uncond_var_denom = 1.0 - np.sum(alpha_coeffs) - np.sum(beta_coeffs)\n    # Ensure denominator is positive, preventing division by zero for non-stationary cases\n    uncond_var = omega / uncond_var_denom if uncond_var_denom > 0 else 1.0\n\n    # max_lag is the number of past values needed for the recursion\n    max_lag = max(p, q)\n    if max_lag == 0: # handle simple case of constant variance\n        r = np.sqrt(omega) * z\n        return r[burn_in:]\n\n    sigma2[:max_lag] = uncond_var\n    r[:max_lag] = np.sqrt(sigma2[:max_lag]) * z[:max_lag]\n    \n    # Convert lists to numpy arrays for vectorized operations\n    alpha_arr = np.array(alpha_coeffs)\n    beta_arr = np.array(beta_coeffs)\n\n    for t in range(max_lag, total_T):\n        arch_term = np.sum(alpha_arr * np.flip(r[t-q:t]**2)) if q > 0 else 0\n        garch_term = np.sum(beta_arr * np.flip(sigma2[t-p:t])) if p > 0 else 0\n        sigma2[t] = omega + arch_term + garch_term\n        r[t] = np.sqrt(max(1e-9, sigma2[t])) * z[t] # Failsafe for numerical stability\n\n    return r[burn_in:]\n\n\ndef garch11_neg_log_likelihood(params, r_series):\n    \"\"\"\n    Computes the negative of the log-likelihood for a GARCH(1,1) model.\n    \"\"\"\n    omega, alpha, beta = params\n    \n    # Parameter constraints\n    if omega = 0 or alpha  0 or beta  0 or (alpha + beta) >= 1.0:\n        return np.inf\n\n    T = len(r_series)\n    sigma2 = np.zeros(T)\n    \n    # Initialize variance with sample variance\n    sigma2[0] = np.var(r_series)\n\n    for t in range(1, T):\n        sigma2[t] = omega + alpha * r_series[t-1]**2 + beta * sigma2[t-1]\n    \n    # Add a small constant to sigma2 to avoid log(0)\n    sigma2[sigma2 = 0] = 1e-9\n\n    log_likelihood = -0.5 * np.sum(np.log(2 * np.pi) + np.log(sigma2) + r_series**2 / sigma2)\n    \n    return -log_likelihood\n\n\ndef estimate_garch11(r_series):\n    \"\"\"\n    Estimates GARCH(1,1) parameters using QMLE.\n    \"\"\"\n    initial_params = np.array([np.var(r_series)*0.05, 0.05, 0.9])\n    bounds = [(1e-9, None), (0, 1), (0, 1)]\n    \n    result = minimize(garch11_neg_log_likelihood, initial_params, args=(r_series,),\n                      method='L-BFGS-B', bounds=bounds)\n    \n    return result.x\n\n\ndef ljung_box_test(series, m):\n    \"\"\"\n    Computes the Ljung-Box Q-statistic and its p-value.\n    \"\"\"\n    T = len(series)\n    x_mean = np.mean(series)\n    \n    # Sample variance (autocovariance at lag 0)\n    gamma0 = np.sum((series - x_mean)**2) / T\n    if gamma0 == 0:\n        return 1.0 # No variation, so no autocorrelation\n    \n    acf_sq_terms = []\n    for k in range(1, m + 1):\n        # Sample autocovariance at lag k\n        gamma_k = np.sum((series[k:] - x_mean) * (series[:-k] - x_mean)) / T\n        rho_k = gamma_k / gamma0\n        acf_sq_terms.append(rho_k**2 / (T - k))\n        \n    Q = T * (T + 2) * np.sum(acf_sq_terms)\n    \n    # p-value from chi-squared distribution\n    p_value = chi2.sf(Q, df=m)\n    return p_value\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    burn_in = 1000\n    seed = 12345\n    alpha_level = 0.05\n\n    test_cases = [\n        {\n            'name': 'Case A',\n            'dgp_params': {'omega': 0.05, 'alpha': [0.05], 'beta': [0.90]},\n            'T': 3000,\n            'm': 20\n        },\n        {\n            'name': 'Case B',\n            'dgp_params': {'omega': 0.02, 'alpha': [0.04, 0.08], 'beta': [0.86]},\n            'T': 4000,\n            'm': 20\n        },\n        {\n            'name': 'Case C',\n            'dgp_params': {'omega': 0.05, 'alpha': [0]*9 + [0.80], 'beta': []},\n            'T': 800,\n            'm': 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Simulate return series\n        dgp = case['dgp_params']\n        r_series = simulate_series(dgp['omega'], dgp['alpha'], dgp['beta'], case['T'], burn_in, seed)\n        \n        # 2. Fit GARCH(1,1) model\n        est_omega, est_alpha, est_beta = estimate_garch11(r_series)\n        \n        # 3. Compute standardized residuals\n        T = case['T']\n        sigma2_hat = np.zeros(T)\n        sigma2_hat[0] = np.var(r_series)\n        for t in range(1, T):\n            sigma2_hat[t] = est_omega + est_alpha * r_series[t-1]**2 + est_beta * sigma2_hat[t-1]\n        \n        std_residuals = r_series / np.sqrt(np.maximum(1e-9, sigma2_hat))\n        sq_std_residuals = std_residuals**2\n        \n        # 4. Apply Ljung-Box test\n        p_value = ljung_box_test(sq_std_residuals, case['m'])\n        \n        # 5. Record boolean decision\n        reject_null = p_value  alpha_level\n        results.append(reject_null)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}