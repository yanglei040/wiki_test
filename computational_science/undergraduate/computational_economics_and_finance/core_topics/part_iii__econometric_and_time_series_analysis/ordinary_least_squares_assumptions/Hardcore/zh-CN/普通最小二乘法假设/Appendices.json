{
    "hands_on_practices": [
        {
            "introduction": "普通最小二乘法（OLS）的核心是寻找最佳的 *线性* 拟合。但如果真实世界的关系本质上是非线性的，会发生什么呢？这个练习  将通过一个引人注目的例子来回答这个问题，你将看到一个完美确定的非线性关系如何被线性回归完全忽略，从而得到接近于零的斜率估计值和判定系数 $R^2$。这个实践旨在强调在应用线性模型之前，检验和理解数据关系形态的重要性。",
            "id": "2417149",
            "problem": "构建一个程序，从第一性原理出发，演示线性普通最小二乘法 (OLS) 回归如何无法检测出确定性的非线性关系。对于下方的每个测试用例，你必须生成一个合成数据集，并计算带截距项的 OLS 拟合。拟合结果定义为能使残差平方和 $\\sum_{i=1}^{N}\\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2$ 最小化的实数对 $(\\hat{\\beta}_0, \\hat{\\beta}_1)$。然后计算决定系数 $R^2$，其公式为 $1 - \\dfrac{\\sum_{i=1}^{N}\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}$，其中 $\\hat{\\varepsilon}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$ 且 $\\bar{y} = \\dfrac{1}{N}\\sum_{i=1}^{N} y_i$。所有计算必须使用实数进行精确算术运算，且所有生成的数据都是确定性的。\n\n生成以下三个数据集，每个数据集在指定区间上包含 $N=1001$ 个均匀间隔的点 $x_i$ 以及按规定对应的 $y_i$。任何测试中均不含随机性：\n\n- 测试用例 A（非线性周期性，对称域）：$x_i$ 位于 $[-\\pi,\\pi]$ 上，$y_i = \\cos(x_i)$。\n- 测试用例 B（非线性凸函数，对称域）：$x_i$ 位于 $[-2,2]$ 上，$y_i = x_i^2$。\n- 测试用例 C（正确设定的线性基准）：$x_i$ 位于 $[-5,5]$ 上，$y_i = 3 + 2 x_i$。\n\n你的程序必须为每个测试用例计算并返回数值对 $(\\hat{\\beta}_1, R^2)$，并遵循此顺序。这三个数值对必须按顺序汇总成单行输出，形式为一个包含三个子列表的逗号分隔列表，不含空格，每个实数四舍五入至 $6$ 位小数。例如，要求的格式是\n\"[[b1_A,R2_A],[b1_B,R2_B],[b1_C,R2_C]]\"\n所有数字都显示 $6$ 位小数。\n\n测试套件与答案规范：\n- 输入参数已如上文固定给出；没有用户输入。\n- 输出为三个有序实数对，每个测试用例一个：A、B 和 C 的 $(\\hat{\\beta}_1, R^2)$。\n- 最终输出必须打印为单行，其中包含一个含三个子列表的列表，每个子列表包含两个四舍五入到 $6$ 位小数的浮点数，且不含空格。\n\n覆盖性设计：\n- 测试用例 A 探究了对称域上的一个确定性、非线性周期关系，在这种情况下，尽管存在完全的函数依赖关系，线性回归仍可能得出 $\\hat{\\beta}_1 \\approx 0$ 和 $R^2 \\approx 0$ 的结果。\n- 测试用例 B 探究了对称域上的一个确定性、非线性凸关系，在这种情况下，最佳线性拟合是一个常数，从而得出 $\\hat{\\beta}_1 = 0$ 和 $R^2 = 0$。\n- 测试用例 C 是一个设定正确的线性模型，它会得出 $\\hat{\\beta}_1 = 2$ 和 $R^2 = 1$ 的结果，用作基准。",
            "solution": "对所述问题进行验证。\n\n**步骤 1：提取已知条件**\n- **目标**：演示线性普通最小二乘法 (OLS) 回归在检测确定性非线性关系时的失效情况。\n- **模型**：带截距项的简单线性回归。\n- **最小化准则**：最小化残差平方和 $SSR = \\sum_{i=1}^{N}\\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2$，以求得估计量 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$。\n- **度量指标**：决定系数 $R^2 = 1 - \\dfrac{\\sum_{i=1}^{N}\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}$，其中 $\\hat{\\varepsilon}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$ 且 $\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$。\n- **数据生成**：所有数据集都是确定性的，包含 $N=1001$ 个均匀间隔的点。\n- **测试用例 A**：$x_i$ 位于 $[-\\pi, \\pi]$ 上，$y_i = \\cos(x_i)$。\n- **测试用例 B**：$x_i$ 位于 $[-2, 2]$ 上，$y_i = x_i^2$。\n- **测试用例 C**：$x_i$ 位于 $[-5, 5]$ 上，$y_i = 3 + 2 x_i$。\n- **要求输出**：对于每个用例，输出数值对 $(\\hat{\\beta}_1, R^2)$，汇总成一个列表的列表，数字四舍五入至 $6$ 位小数。\n\n**步骤 2：使用提取的已知条件进行验证**\n根据既定标准对问题进行评估：\n1.  **科学性**：该问题基于普通最小二乘法回归的基本原理，这是统计学和计量经济学的基石。所呈现的公式和概念都是标准的且事实正确。\n2.  **适定性**：对于简单线性回归，只要自变量存在方差，OLS 估计问题就存在唯一的解析解，数据集的构造满足此条件。该问题表述明确，并为求解提供了所有必要信息。\n3.  **客观性**：该问题以精确的数学术语陈述，没有主观性或模糊性。\n4.  **其他缺陷**：该问题没有违反任何其他验证标准。它内容完整、逻辑一致、计算上可行，并且与其所述的 OLS 假设主题直接相关。\n\n**步骤 3：结论与行动**\n该问题是 **有效的**。有必要提供完整解答。\n\n我们从第一性原理出发。目标是找到使残差平方和 $S(\\beta_0, \\beta_1) = \\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i)^2$ 最小化的参数 $(\\hat{\\beta}_0, \\hat{\\beta}_1)$。为求最小值，我们对 $\\beta_0$ 和 $\\beta_1$ 求偏导数并令其为零。这就得到了正规方程组：\n$$\n\\frac{\\partial S}{\\partial \\beta_0} = -2 \\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i) = 0 \\implies \\sum y_i - N\\beta_0 - \\beta_1 \\sum x_i = 0\n$$\n$$\n\\frac{\\partial S}{\\partial \\beta_1} = -2 \\sum_{i=1}^{N}x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0 \\implies \\sum x_i y_i - \\beta_0 \\sum x_i - \\beta_1 \\sum x_i^2 = 0\n$$\n求解这个关于 $\\beta_0$ 和 $\\beta_1$ 的线性方程组，可得到 OLS 估计量，我们用帽子符号表示：\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N}(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}\n$$\n$$\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n$$\n其中 $\\bar{x} = \\frac{1}{N}\\sum x_i$ 且 $\\bar{y} = \\frac{1}{N}\\sum y_i$。\n\n决定系数 $R^2$ 衡量因变量方差中可由自变量预测的部分所占的比例。其定义为：\n$$\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{TSS}} = 1 - \\frac{\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}\n$$\n其中 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ 是拟合值，SSR 是残差平方和 (Sum of Squared Residuals)，TSS 是总平方和 (Total Sum of Squares)。\n\n对于所有三个测试用例，一个关键点是自变量 $x$ 的生成方式。这 $N=1001$ 个点均匀分布在 $[-L, L]$ 形式的对称区间上。由于 $N$ 是奇数，中心点是 $x_{(N-1)/2} = 0$，并且对于数据集中的每个点 $x_i$，其相反数 $-x_i$ 也存在。这种对称性意味着 $x$ 的均值恰好为零：$\\bar{x} = 0$。这极大地简化了估计量的计算：\n$$\n\\hat{\\beta}_1 = \\frac{\\sum x_i y_i}{\\sum x_i^2}\n$$\n$$\n\\hat{\\beta}_0 = \\bar{y}\n$$\n\n我们现在使用这些简化公式来分析每个案例。\n\n**测试用例 A：非线性周期性，对称域（$y_i = \\cos(x_i)$ 在 $[-\\pi, \\pi]$ 上）**\n函数 $y = f(x) = \\cos(x)$ 是一个偶函数，意味着 $f(-x) = f(x)$。$\\hat{\\beta}_1$ 分子中的项是 $g(x_i) = x_i \\cos(x_i)$ 的和。函数 $g(x)$ 是一个奇函数 ($x$) 和一个偶函数 ($\\cos(x)$) 的乘积，结果是一个奇函数，$g(-x) = (-x)\\cos(-x) = -x\\cos(x) = -g(x)$。\n一个奇函数在一个以零为中心的对称点集上的和恰好为零。\n$$\n\\sum_{i=1}^{N} x_i y_i = \\sum_{i=1}^{N} x_i \\cos(x_i) = 0\n$$\n因此，斜率系数为零：\n$$\n\\hat{\\beta}_1 = \\frac{0}{\\sum x_i^2} = 0\n$$\n当 $\\hat{\\beta}_1 = 0$ 时，回归线是水平的，即 $\\hat{y}_i = \\hat{\\beta}_0 = \\bar{y}$。残差平方和为 $SSR = \\sum(y_i - \\bar{y})^2$，这与总平方和 TSS 相等。\n$$\nR^2 = 1 - \\frac{SSR}{TSS} = 1 - \\frac{TSS}{TSS} = 0\n$$\n尽管 $x$ 和 $y$ 之间存在完美的确定性关系，线性回归报告的解释力完全为零。\n\n**测试用例 B：非线性凸函数，对称域（$y_i = x_i^2$ 在 $[-2, 2]$ 上）**\n函数 $y = f(x) = x^2$ 也是一个偶函数。分析与案例 A 相同。乘积 $g(x_i) = x_i y_i = x_i \\cdot x_i^2 = x_i^3$ 是一个奇函数。将这个函数在一个对称点集上求和，结果为零。\n$$\n\\sum_{i=1}^{N} x_i y_i = \\sum_{i=1}^{N} x_i^3 = 0\n$$\n同样，这导致 $\\hat{\\beta}_1 = 0$，因此 $R^2 = 0$。OLS 回归完全无法察觉这种完美的二次关系。这种失败是根本性的，并突显了零协方差并不意味着独立性，这是初学者常犯的错误。例如，在金融领域，简单的线性模型可能会忽略诸如波动率聚集之类的非线性依赖关系。\n\n**测试用例 C：正确设定的线性基准（$y_i = 3 + 2x_i$ 在 $[-5, 5]$ 上）**\n这个案例用作对照组。数据由一个完美的线性模型生成，其真实参数为 $\\beta_0=3$ 和 $\\beta_1=2$。我们期望 OLS 能够精确地恢复这些参数。\n使用 $\\hat{\\beta}_1$ 的简化公式：\n$$\n\\hat{\\beta}_1 = \\frac{\\sum x_i y_i}{\\sum x_i^2} = \\frac{\\sum x_i (3 + 2x_i)}{\\sum x_i^2} = \\frac{3\\sum x_i + 2\\sum x_i^2}{\\sum x_i^2}\n$$\n由于 $\\sum x_i = N\\bar{x} = 0$，表达式简化为：\n$$\n\\hat{\\beta}_1 = \\frac{2\\sum x_i^2}{\\sum x_i^2} = 2\n$$\n截距为 $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\bar{y} - 2 \\cdot 0 = \\bar{y}$。$y$ 的均值为 $\\bar{y} = \\frac{1}{N}\\sum(3+2x_i) = \\frac{1}{N}(3N + 2\\sum x_i) = 3$。所以，$\\hat{\\beta}_0 = 3$。\n拟合模型为 $\\hat{y}_i = 3 + 2x_i$，与真实模型完全相同。所有 $i$ 的残差均为 $\\hat{\\varepsilon}_i = y_i - \\hat{y}_i = 0$。因此，$SSR = 0$。\n总平方和 $TSS = \\sum(y_i - \\bar{y})^2 = \\sum((3+2x_i)-3)^2 = \\sum(2x_i)^2 = 4\\sum x_i^2$。由于并非所有 $x_i$ 都为零，所以 $TSS > 0$。\n$$\nR^2 = 1 - \\frac{0}{TSS} = 1\n$$\n正如预期的那样，对于一个没有噪声的、设定正确的线性模型，OLS 提供了完美的拟合。\n\n实现过程将把这些解析公式转化为代码。它将为每个案例构建数据集，计算估计量和 $R^2$，并按规定格式化结果。由于浮点运算，计算结果可能与理论精确值 $0$、$1$ 和 $2$ 有微小偏差，但在要求的精度下这些偏差是微不足道的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes OLS coefficients for three deterministic datasets to demonstrate\n    the limitations of linear regression for non-linear relationships.\n    \"\"\"\n\n    # Define the problem parameters for each test case.\n    # Each tuple contains: (N, x_min, x_max, y_function, description)\n    test_cases = [\n        (1001, -np.pi, np.pi, lambda x: np.cos(x), \"A\"),\n        (1001, -2.0, 2.0, lambda x: x**2, \"B\"),\n        (1001, -5.0, 5.0, lambda x: 3.0 + 2.0 * x, \"C\"),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, x_min, x_max, y_func, _ = case\n\n        # Generate the deterministic dataset\n        x = np.linspace(x_min, x_max, N)\n        y = y_func(x)\n\n        # Compute means\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n\n        # Compute OLS estimator for the slope (beta_1)\n        # beta_1 = Cov(x, y) / Var(x)\n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        \n        # Avoid division by zero, though not expected here\n        if denominator == 0:\n            beta_1 = 0.0\n        else:\n            beta_1 = numerator / denominator\n\n        # Compute OLS estimator for the intercept (beta_0)\n        beta_0 = y_mean - beta_1 * x_mean\n\n        # Compute the fitted values\n        y_pred = beta_0 + beta_1 * x\n\n        # Compute Sum of Squared Residuals (SSR) and Total Sum of Squares (TSS)\n        ssr = np.sum((y - y_pred)**2)\n        tss = np.sum((y - y_mean)**2)\n\n        # Compute the coefficient of determination (R^2)\n        # If TSS is zero, all y values are the same.\n        # R^2 is undefined, but can be treated as 1 if the model is perfect (ssr=0)\n        # or 0 if it's not (ssr>0). Here, TSS will not be zero.\n        if tss == 0:\n            r_squared = 1.0 if ssr == 0 else 0.0\n        else:\n            r_squared = 1.0 - (ssr / tss)\n\n        results.append([beta_1, r_squared])\n\n    # Format the final output string as per problem specification.\n    # Example: [[b1_A,R2_A],[b1_B,R2_B],[b1_C,R2_C]]\n    # Numbers are rounded to 6 decimal places.\n    sublist_strs = []\n    for res_pair in results:\n        # Using f-string for precise decimal formatting\n        s = f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\"\n        sublist_strs.append(s)\n    \n    final_output = f\"[{','.join(sublist_strs)}]\"\n\n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "在计量经济学中，最关键的假设之一是外生性假设，即误差项与解释变量不相关。当这个假设被违背时——通常是由于遗漏了与模型中变量相关的关键变量——OLS 估计量将产生偏误且不一致。这个模拟练习  将抽象的遗漏变量偏误（OVB）概念变得生动具体，你将亲手构建一个场景，观察像“能力”这样不可观测的因素是如何系统性地扭曲我们对教育回报率的估计。",
            "id": "2417165",
            "problem": "考虑一个个人工资的数据生成过程，其中一位计量经济学家将工资对受教育年限进行回归，但一个未被观测到的能力同时影响着受教育程度和工资。设真实的结构模型为\n$$\nw_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i,\n$$\n其中 $w_i$ 是个体 $i$ 的工资，$\\text{edu}_i$ 是个体 $i$ 的受教育年限，$a_i$ 是一个未观测到的能力，$u_i$ 是一个特异性误差。假设受教育程度由以下公式决定：\n$$\n\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i.\n$$\n假设 $a_i \\sim \\mathcal{N}(0,\\sigma_a^2)$，$v_i \\sim \\mathcal{N}(0,\\sigma_v^2)$ 和 $u_i \\sim \\mathcal{N}(0,\\sigma_u^2)$，且 $a_i$，$v_i$ 和 $u_i$ 相互独立。该计量经济学家估计了 $w_i$ 对 $\\text{edu}_i$ 带截距项的回归，并忽略了 $a_i$。\n\n你的任务是编写一个完整、可运行的程序，该程序针对一组给定的参数值测试套件，根据上述过程模拟数据，为每个测试用例通过 $w_i$ 对 $\\text{edu}_i$ 带截距项的回归估计 $\\text{edu}_i$ 的系数，并为每个案例报告：估计系数 $\\hat{\\beta}_1$、偏差 $\\hat{\\beta}_1 - \\beta_1$ 以及估计值是否严格大于真实的 $\\beta_1$。\n\n随机性与可复现性：对于测试用例索引 $i \\in \\{1,2,3,4,5\\}$，使用伪随机种子 $s_i = s_0 + i$，其中基础种子 $s_0 = 12345$。请严格按照规定使用指定的正态分布。所有量均为无量纲，无需物理单位。\n\n待使用的参数值测试套件：\n- 案例 1（能力与受教育程度之间存在正相关；预期会产生高估）：$N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$。\n- 案例 2（能力与受教育程度之间无相关性；在概率极限下无遗漏变量偏误）：$N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.0$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$。\n- 案例 3（能力与受教育程度之间存在负相关；预期会产生低估）：$N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = -0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$。\n- 案例 4（受教育的真实回报为零；预期会产生虚假的正向估计）：$N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.0$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$。\n- 案例 5（能力方差为零的边界情况；遗漏变量偏误消失）：$N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 0.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$。\n\n要求的最终输出格式：你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。对于从 1 到 5 的每个案例，请依次附加估计系数 $\\hat{\\beta}_1$（一个浮点数）、偏差 $\\hat{\\beta}_1 - \\beta_1$（一个浮点数）以及一个指示 $\\hat{\\beta}_1$ 是否严格大于 $\\beta_1$ 的指示符（一个布尔值）。因此，输出将是一个长度为 15 的扁平列表，其顺序为 $[\\hat{\\beta}_1^{(1)}, \\text{bias}^{(1)}, \\text{over}^{(1)}, \\ldots, \\hat{\\beta}_1^{(5)}, \\text{bias}^{(5)}, \\text{over}^{(5)}]$。",
            "solution": "对问题陈述进行验证。\n\n**步骤 1：提取给定条件**\n\n工资的结构模型由以下公式给出：\n$w_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i$\n\n受教育程度的模型为：\n$\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i$\n\n分布假设为：\n- $a_i \\sim \\mathcal{N}(0, \\sigma_a^2)$\n- $v_i \\sim \\mathcal{N}(0, \\sigma_v^2)$\n- $u_i \\sim \\mathcal{N}(0, \\sigma_u^2)$\n- $a_i$，$v_i$ 和 $u_i$ 相互独立。\n\n计量经济学家通过将 $w_i$ 对 $\\text{edu}_i$ 进行带截距项的回归来估计一个不完整的模型，其中忽略了 $a_i$。\n\n随机性规范：\n对于测试用例索引 $i \\in \\{1, 2, 3, 4, 5\\}$，伪随机种子为 $s_i = s_0 + i$，基础种子 $s_0 = 12345$。\n\n测试套件参数：\n- 案例 1：$N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.8$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$。\n- 案例 2：$N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.0$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$。\n- 案例 3：$N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=-0.8$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$。\n- 案例 4：$N=100000$, $\\beta_0=0$, $\\beta_1=0.0$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.8$, $\\sigma_a=1.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$。\n- 案例 5：$N=100000$, $\\beta_0=0$, $\\beta_1=0.08$, $\\beta_a=0.5$, $e_0=12$, $\\phi=0.8$, $\\sigma_a=0.0$, $\\sigma_v=1.0$, $\\sigma_u=1.0$。\n\n每个案例所需的输出：估计系数 $\\hat{\\beta}_1$、偏差 $\\hat{\\beta}_1 - \\beta_1$ 以及 $\\hat{\\beta}_1 > \\beta_1$ 的布尔指示符。\n\n**步骤 2：使用提取的给定条件进行验证**\n\n- **科学依据**：该问题描述了线性回归中遗漏变量偏误的一个典型例子，这是计量经济学和统计学中的一个基本概念。模型和假设都是标准的。该问题在科学上是合理的。\n- **适定性**：该问题提供了一个完整的数据生成过程、一个特定的估计任务以及所有测试用例的完全指定的参数。对于给定的随机种子，存在唯一的解。该问题是适定的。\n- **客观性**：问题的所有元素都以数学精度定义。没有主观或模棱两可的陈述。该问题是客观的。\n\n**步骤 3：结论与行动**\n\n问题有效。将构建一个解决方案。\n\n**解决方案设计**\n\n这个问题的核心是当一个相关变量从回归模型中被遗漏，并且这个遗漏的变量与一个包含的回归量相关时，普通最小二乘（OLS）估计量中出现的偏误。\n\n设真实模型为\n$$w_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i$$\n计量经济学家估计的设定不当的模型是\n$$w_i = \\gamma_0 + \\gamma_1 \\cdot \\text{edu}_i + \\epsilon_i$$\n估计模型中的 $\\epsilon_i$ 项是一个复合误差项，它包括了被遗漏的变量和原始的特异性误差：$\\epsilon_i = \\beta_a \\cdot a_i + u_i$。\n\n$\\gamma_1$ 的 OLS 估计量（我们记作 $\\hat{\\beta}_1$）由下式给出\n$$\\hat{\\beta}_1 = \\frac{\\text{Cov}(\\text{edu}, w)}{\\text{Var}(\\text{edu})}$$\n为了理解这个估计量的性质，我们考察它的概率极限：\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\text{Cov}(\\text{edu}_i, w_i)}{\\text{Var}(\\text{edu}_i)}$$\n将 $w_i$ 的真实结构方程代入：\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\text{Cov}(\\text{edu}_i, \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i)}{\\text{Var}(\\text{edu}_i)}$$\n根据协方差的线性性质以及 $\\text{Cov}(\\text{edu}_i, \\beta_0) = 0$ 这一事实：\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\beta_1 \\cdot \\text{Cov}(\\text{edu}_i, \\text{edu}_i) + \\beta_a \\cdot \\text{Cov}(\\text{edu}_i, a_i) + \\text{Cov}(\\text{edu}_i, u_i)}{\\text{Var}(\\text{edu}_i)}$$\n给定受教育程度的结构模型 $\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i$，以及 $a_i, v_i, u_i$ 的相互独立性，我们有 $\\text{Cov}(\\text{edu}_i, u_i) = \\text{Cov}(e_0 + \\phi \\cdot a_i + v_i, u_i) = 0$。\n表达式简化为：\n$$\\text{plim}(\\hat{\\beta}_1) = \\beta_1 + \\beta_a \\frac{\\text{Cov}(\\text{edu}_i, a_i)}{\\text{Var}(\\text{edu}_i)}$$\n$\\beta_a \\frac{\\text{Cov}(\\text{edu}_i, a_i)}{\\text{Var}(\\text{edu}_i)}$ 项代表了渐近遗漏变量偏误。我们可以从给定的假设中推导出其组成部分：\n$$\\text{Cov}(\\text{edu}_i, a_i) = \\text{Cov}(e_0 + \\phi \\cdot a_i + v_i, a_i) = \\phi \\cdot \\text{Var}(a_i) = \\phi \\sigma_a^2$$\n$$\\text{Var}(\\text{edu}_i) = \\text{Var}(e_0 + \\phi \\cdot a_i + v_i) = \\phi^2 \\cdot \\text{Var}(a_i) + \\text{Var}(v_i) = \\phi^2 \\sigma_a^2 + \\sigma_v^2$$\n因此，渐近偏误为：\n$$\\text{Bias} = \\beta_a \\frac{\\phi \\sigma_a^2}{\\phi^2 \\sigma_a^2 + \\sigma_v^2}$$\n估计量 $\\hat{\\beta}_1$ 是一致的（即偏误为零）当且仅当 $\\beta_a = 0$ 或 $\\phi = 0$ 或 $\\sigma_a^2 = 0$。这意味着被遗漏的变量要么不影响因变量（$w_i$），要么与包含的回归量（$\\text{edu}_i$）不相关，要么其方差为零。\n\n对于每个测试用例，模拟将按以下步骤进行：\n1. 将伪随机种子设置为 $s_0 + i$ 以确保可复现性。\n2. 从指定的正态分布中为随机分量 $a_i$，$v_i$ 和 $u_i$ 生成 $N$ 个抽样。\n3. 使用生成的随机分量和特定案例的参数，根据其结构方程构建观测数据向量 $\\text{edu}$ 和 $w$。\n4. 通过将 $w$ 对 $\\text{edu}$ 和一个截距项进行回归，来估计设定不当模型的系数。这可以通过标准的 OLS 矩阵公式 $\\hat{\\mathbf{\\beta}} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}$ 来实现，其中 $\\mathbf{y}$ 是 $w_i$ 值的向量，$\\mathbf{X}$ 是一个 $N \\times 2$ 的设计矩阵，包含一列 1 和一列 $\\text{edu}_i$ 值。估计系数 $\\hat{\\beta}_1$ 是向量 $\\hat{\\mathbf{\\beta}}$ 的第二个元素。\n5. 计算偏差为 $\\hat{\\beta}_1 - \\beta_1$，并确定是否 $\\hat{\\beta}_1 > \\beta_1$。\n6. 存储这三个结果值（$\\hat{\\beta}_1$、偏差、高估指示符）以供最终报告。\n\n该过程将对所有五个测试用例重复进行，以展示在不同条件下遗漏变量偏误的理论效应。$N=100000$ 的大样本容量确保了模拟的有限样本估计值将非常接近其理论概率极限。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates data to demonstrate omitted variable bias in OLS regression,\n    estimates coefficients for several parameter sets, and reports the results.\n    \"\"\"\n    # Base seed for pseudo-random number generation.\n    base_seed = 12345\n\n    # Test suite of parameter values.\n    # Each tuple contains:\n    # (N, beta_0, beta_1, beta_a, e_0, phi, sigma_a, sigma_v, sigma_u)\n    test_cases = [\n        (100000, 0, 0.08, 0.5, 12, 0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, 0.0, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, -0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.0, 0.5, 12, 0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, 0.8, 0.0, 1.0, 1.0),\n    ]\n\n    results = []\n\n    for i, case in enumerate(test_cases, 1):\n        # Unpack parameters for the current case.\n        N, beta_0, beta_1, beta_a, e_0, phi, sigma_a, sigma_v, sigma_u = case\n        \n        # Set the seed for reproducibility for the current test case.\n        seed = base_seed + i\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate the underlying random variables from their distributions.\n        #    a_i ~ N(0, sigma_a^2)\n        #    v_i ~ N(0, sigma_v^2)\n        #    u_i ~ N(0, sigma_u^2)\n        a = rng.normal(loc=0.0, scale=sigma_a, size=N)\n        v = rng.normal(loc=0.0, scale=sigma_v, size=N)\n        u = rng.normal(loc=0.0, scale=sigma_u, size=N)\n\n        # 2. Construct the variables 'edu' and 'wage' based on the structural model.\n        #    edu_i = e_0 + phi * a_i + v_i\n        edu = e_0 + phi * a + v\n        \n        #    w_i = beta_0 + beta_1 * edu_i + beta_a * a_i + u_i\n        wage = beta_0 + beta_1 * edu + beta_a * a + u\n\n        # 3. Perform OLS regression of wage on edu and an intercept.\n        #    We estimate the model: w_i = gamma_0 + gamma_1 * edu_i + error\n        #    The OLS solution is beta_hat = (X'X)^-1 * X'y\n        \n        # Construct the design matrix X with a column of ones for the intercept.\n        X = np.vstack([np.ones(N), edu]).T\n        \n        # Calculate OLS coefficients.\n        try:\n            # Using the standard matrix formula for OLS coefficients.\n            # beta_hat = [gamma_0_hat, gamma_1_hat]\n            XtX = X.T @ X\n            XtY = X.T @ wage\n            beta_hat_vector = np.linalg.inv(XtX) @ XtY\n        except np.linalg.LinAlgError:\n            # In case of singular matrix, use pseudo-inverse\n            beta_hat_vector = np.linalg.pinv(X) @ wage\n\n        estimated_beta_1 = beta_hat_vector[1]\n\n        # 4. Calculate the bias and the overestimation indicator.\n        bias = estimated_beta_1 - beta_1\n        is_over = estimated_beta_1 > beta_1\n\n        # 5. Append results for this case to the master list.\n        results.extend([estimated_beta_1, bias, is_over])\n\n    # Final print statement in the exact required format.\n    # The list is flattened and elements are converted to strings.\n    formatted_results = [f\"{x:.8f}\" if isinstance(x, float) else str(x) for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "即使 OLS 估计量是无偏的，我们对其进行的统计推断（如假设检验）的有效性也依赖于另一个关键假设：同方差性，即误差项具有恒定的方差。当数据表现出异方差性（误差方差随观测值变化）时，传统的标准误计算公式将不再可靠，导致推断失效。在这个实践中 ，你将生成异方差数据，并并排比较有缺陷的经典标准误、稳健的怀特（White）标准误以及通过自助法（bootstrap）得到的标准误，从而深刻理解异方差性的后果与对策。",
            "id": "2417150",
            "problem": "考虑一个计算经济学和金融学背景下的简单线性回归模型，该模型包含一个回归量和一个截距。响应变量 $y_i$ 由以下数据生成过程生成\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i,\n$$\n其中 $x_i \\sim \\mathcal{N}(0,1)$ 在观测值 $i$ 之间独立同分布，并且扰动项 $u_i$ 根据以下公式表现出异方差性\n$$\nu_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,1)\\ \\text{且与}\\ x_i\\ \\text{独立}.\n$$\n假设我们感兴趣的系数是斜率 $\\beta_1$。对于下文定义的每个测试用例，请使用指定的参数生成一个样本 $\\{(y_i,x_i)\\}_{i=1}^n$，并计算 $\\beta_1$ 的普通最小二乘 (OLS) 估计量的三种标准误估计：\n- 经典的基于同方差性的 OLS 标准误，\n- 异方差一致性 (HC0) 标准误（也称为 White-稳健标准误），\n- 基于对观测对 $(y_i,x_i)$ 进行 $B$ 次自助法重复有放回抽样的非参数自助法标准误。\n\n为确保可复现性，在每个测试用例中，所有随机抽样均使用相同的固定随机数生成器种子 $s$。在所有计算中，回归模型均应包含截距。将报告的每个标准误表示为四舍五入到六位小数的实数。\n\n测试集。使用以下参数集；每个项目符号描述一个测试用例：\n- 用例 1：$n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=17$。\n- 用例 2：$n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=0.0$, $B=400$, $s=23$。\n- 用例 3：$n=50$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=123$。\n- 用例 4：$n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=10.0$, $B=400$, $s=2023$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表，并用方括号括起来。每个内部列表对应一个测试用例，顺序与上文相同，并按 [OLS 标准误, White-稳健标准误, 自助法标准误] 的顺序包含三个实数，每个实数都四舍五入到六位小数。例如，输出必须采用\n$[[a_{1,1},a_{1,2},a_{1,3}],[a_{2,1},a_{2,2},a_{2,3}],[a_{3,1},a_{3,2},a_{3,3}],[a_{4,1},a_{4,2},a_{4,3}]]$\n的形式，且不含空格。",
            "solution": "所提出的问题是计量经济学中一个定义明确的计算练习。它具有科学依据，逻辑上一致，并为求解提供了所有必要的信息。任务是在一个包含异方差性的特定数据生成过程下，计算普通最小二乘 (OLS) 回归系数标准误的三种不同估计量。\n\n模型是一个简单线性回归：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n$$\n对于 $i = 1, \\dots, n$。用矩阵表示法，即为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}$，其中 $\\mathbf{y}$ 是响应变量观测值的 $n \\times 1$ 向量，$\\mathbf{X}$ 是 $n \\times 2$ 的设计矩阵，其第一列是全为 1 的向量，第二列是回归量 $x_i$ 的观测值，$\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ 是 $2 \\times 1$ 的系数向量，$\\mathbf{u}$ 是 $n \\times 1$ 的扰动项向量。\n\n$\\boldsymbol{\\beta}$ 的 OLS 估计量由下式给出：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n给定 $\\mathbf{X}$ 的条件下，该估计量的方差-协方差矩阵为：\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Omega} \\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\n其中 $\\boldsymbol{\\Omega} = \\text{E}[\\mathbf{u}\\mathbf{u}^T | \\mathbf{X}]$。在此问题中，扰动项 $u_i$ 在不同观测值之间是独立的，因此 $\\boldsymbol{\\Omega}$ 是一个对角矩阵。异方差结构 $u_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i$ 及 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 意味着 $\\boldsymbol{\\Omega}$ 的对角元素为 $\\text{Var}(u_i|x_i) = \\sigma_i^2 = \\sigma_0^2(1 + \\gamma x_i^2)$。\n\n任务是估计斜率系数的 OLS 估计量 $\\hat{\\beta}_1$ 的标准误，即 $\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X})$ 第二个对角元素的平方根。我们将计算该量的三种不同估计值。\n\n1.  **经典的基于同方差性的 OLS 标准误**\n\n该估计量错误地假设扰动项是同方差的，即对所有 $i$ 都有 $\\text{Var}(u_i|x_i) = \\sigma^2$。在此假设下，$\\boldsymbol{\\Omega} = \\sigma^2\\mathbf{I}_n$，方差-协方差矩阵简化为：\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\n未知的误差方差 $\\sigma^2$ 使用 OLS 残差 $\\hat{u}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$ 进行估计。$\\sigma^2$ 的无偏估计量为：\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-k} \\sum_{i=1}^{n} \\hat{u}_i^2\n$$\n其中 $k$ 是回归量的数量，本例中为 2（截距和斜率）。估计的经典方差-协方差矩阵为 $\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}$。$\\hat{\\beta}_1$ 的标准误是该矩阵第二个对角元素的平方根：\n$$\n\\text{SE}_{OLS}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\n\n2.  **异方差一致性 (HC0) 标准误**\n\n该估计量也称为 White 稳健标准误或简称为稳健标准误，它不假设同方差性。即使当 $\\boldsymbol{\\Omega}$ 不是单位矩阵的倍数时，它也能提供方差-协方差矩阵的一致估计。它基于方差的“三明治”公式。三明治的中间部分 $\\mathbf{S} = \\mathbf{X}^T \\boldsymbol{\\Omega} \\mathbf{X}$，通过用 OLS 残差的平方 $\\hat{u}_i^2$ 替换未知的方差 $\\sigma_i^2$ 来进行估计。这给出了 $\\mathbf{S}$ 的 HC$0$ 估计量：\n$$\n\\hat{\\mathbf{S}}_0 = \\sum_{i=1}^{n} \\hat{u}_i^2 \\mathbf{x}_i \\mathbf{x}_i^T\n$$\n其中 $\\mathbf{x}_i = [1, x_i]^T$ 是设计矩阵 $\\mathbf{X}$ 的第 $i$ 行。那么，HC$0$ 方差-协方差估计量为：\n$$\n\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\hat{\\mathbf{S}}_0 (\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\n$\\hat{\\beta}_1$ 相应的标准误为：\n$$\n\\text{SE}_{HC0}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\n当 $\\gamma > 0$ 时，误差确实存在异方差性，预计该估计量会比经典估计量更准确，尤其是在大样本中。当 $\\gamma=0.0$ 时，误差是同方差的，两种估计量应该相似。\n\n3.  **自助法标准误**\n\n自助法提供了一种用于估计标准误的非参数方法。我们使用“配对自助法”，该方法在存在异方差性的情况下是合适的，因为它将观测对 $(x_i, y_i)$ 一起进行重抽样，从而保留了回归量与误差项方差之间未知的关系。算法如下：\n    1.  从原始样本 $\\{(y_i, x_i)\\}_{i=1}^n$ 中，通过有放回地抽样观测对来抽取一个大小为 $n$ 的“自助样本”。\n    2.  使用这个自助样本，计算斜率系数的 OLS 估计值，记为 $\\hat{\\beta}_{1,b}^*$。\n    3.  重复步骤 1 和 2，进行大量的重复，次数为 $B$。这将产生一个自助法估计值的分布 $\\{\\hat{\\beta}_{1,1}^*, \\dots, \\hat{\\beta}_{1,B}^*\\}$。\n    4.  自助法标准误是这 $B$ 个估计值的样本标准差：\n        $$\n        \\text{SE}_{Boot}(\\hat{\\beta}_1) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\beta}_{1,b}^* - \\bar{\\beta}_1^*)^2}\n        $$\n        其中 $\\bar{\\beta}_1^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\beta}_{1,b}^*$。这个过程计算量很大，但能为 $\\hat{\\beta}_1$ 的抽样变异性提供一个可靠的估计。\n\n实施时，将首先根据指定的参数（$n, \\beta_0, \\beta_1, \\sigma_0, \\gamma$）和随机种子 $s$ 为每个测试用例生成数据。然后，对每个生成的数据集，将计算 OLS 估计值，随后计算三种指定的标准误。结果将按要求进行四舍五入和格式化。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It generates data, computes OLS and three types of standard errors,\n    and prints the results in the required format.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=17\n        (500, 0.7, 1.5, 1.0, 2.0, 400, 17),\n        # Case 2: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=0.0, B=400, s=23\n        (500, 0.7, 1.5, 1.0, 0.0, 400, 23),\n        # Case 3: n=50, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=123\n        (50, 0.7, 1.5, 1.0, 2.0, 400, 123),\n        # Case 4: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=10.0, B=400, s=2023\n        (500, 0.7, 1.5, 1.0, 10.0, 400, 2023),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n, beta0, beta1, sigma0, gamma, B, s = case\n        \n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(s)\n\n        # 1. Data Generation\n        x = rng.normal(0, 1, size=n)\n        epsilon = rng.normal(0, 1, size=n)\n        u = sigma0 * np.sqrt(1 + gamma * x**2) * epsilon\n        y = beta0 + beta1 * x + u\n\n        # 2. OLS Estimation\n        X = np.vstack((np.ones(n), x)).T  # Design matrix [1, x_i]\n        \n        try:\n            # Pre-compute (X'X)^-1\n            inv_XTX = np.linalg.inv(X.T @ X)\n            beta_hat = inv_XTX @ X.T @ y\n        except np.linalg.LinAlgError:\n            # Handle rare case of singular matrix\n            all_results.append([np.nan, np.nan, np.nan])\n            continue\n            \n        residuals = y - X @ beta_hat\n        k = X.shape[1] # Number of regressors (intercept + slope)\n\n        # 3. Compute Standard Errors\n\n        # 3.1. Classical (Homoskedastic) OLS Standard Error\n        sigma2_hat = np.sum(residuals**2) / (n - k)\n        var_cov_ols = sigma2_hat * inv_XTX\n        se_ols = np.sqrt(var_cov_ols[1, 1])\n\n        # 3.2. HC0 (White-Robust) Standard Error\n        # S_0 = sum(u_i^2 * x_i * x_i')\n        S0 = X.T @ np.diag(residuals**2) @ X\n        var_cov_hc0 = inv_XTX @ S0 @ inv_XTX\n        se_hc0 = np.sqrt(var_cov_hc0[1, 1])\n\n        # 3.3. Bootstrap Standard Error (Pairs Bootstrap)\n        bootstrap_betas = np.zeros(B)\n        for b in range(B):\n            # Resample pairs (y_i, x_i) with replacement\n            indices = rng.choice(n, size=n, replace=True)\n            y_star = y[indices]\n            X_star = X[indices]\n            \n            try:\n                # OLS on bootstrap sample\n                inv_XTX_star = np.linalg.inv(X_star.T @ X_star)\n                beta_hat_star = inv_XTX_star @ X_star.T @ y_star\n                bootstrap_betas[b] = beta_hat_star[1]\n            except np.linalg.LinAlgError:\n                bootstrap_betas[b] = np.nan\n        \n        # Standard deviation of bootstrap estimates\n        se_boot = np.nanstd(bootstrap_betas, ddof=1)\n\n        # 4. Store rounded results\n        case_results = [\n            round(se_ols, 6),\n            round(se_hc0, 6),\n            round(se_boot, 6)\n        ]\n        all_results.append(case_results)\n\n    # Format output string to be a list of lists with no spaces\n    formatted_results = [repr(r).replace(' ', '') for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}