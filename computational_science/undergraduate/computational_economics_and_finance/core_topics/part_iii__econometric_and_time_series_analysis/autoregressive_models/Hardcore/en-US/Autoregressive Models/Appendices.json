{
    "hands_on_practices": [
        {
            "introduction": "Before we can build and forecast with complex models, we must understand their fundamental properties. For a stationary autoregressive process, one of the most important characteristics is its long-run mean, the value to which the series is expected to revert over time. This exercise provides practice in calculating this essential feature for a simple AR(1) model, laying the groundwork for more advanced analysis.",
            "id": "1897485",
            "problem": "An economic research group is modeling the quarterly inflation anomaly, denoted by $I_t$, for a certain country. The anomaly is defined as the deviation of the observed inflation rate from the central bank's target rate. The group's analysis suggests that this time series can be effectively described by a first-order autoregressive (AR(1)) model of the form:\n\n$$I_t = \\alpha + \\beta I_{t-1} + \\epsilon_t$$\n\nHere, $I_t$ represents the inflation anomaly in percentage points during quarter $t$. The model includes a constant intercept $\\alpha = 0.4$, and an autoregressive coefficient $\\beta = 0.75$, which captures the persistence of the anomaly from the previous quarter. The term $\\epsilon_t$ is a white noise process, characterized by a mean of zero and constant variance, representing unpredictable shocks to inflation.\n\nAssuming the time series process for the inflation anomaly is stationary, calculate its long-run expected value. Express your answer in percentage points.",
            "solution": "We are given the AR(1) process for the inflation anomaly:\n$$\nI_{t} = \\alpha + \\beta I_{t-1} + \\epsilon_{t},\n$$\nwith $\\alpha = 0.4$, $\\beta = 0.75$, and $\\epsilon_{t}$ a white noise process with $\\mathbb{E}[\\epsilon_{t}] = 0$. Under the stationarity assumption (which requires $|\\beta| < 1$), the long-run expected value, or unconditional mean, is constant and equal to $\\mu = \\mathbb{E}[I_{t}]$.\n\nTaking expectations on both sides of the model:\n$$\n\\mathbb{E}[I_{t}] = \\mathbb{E}[\\alpha + \\beta I_{t-1} + \\epsilon_{t}] = \\alpha + \\beta \\mathbb{E}[I_{t-1}] + \\mathbb{E}[\\epsilon_{t}].\n$$\nUsing $\\mathbb{E}[\\epsilon_{t}] = 0$ and stationarity $\\mathbb{E}[I_{t}] = \\mathbb{E}[I_{t-1}] = \\mu$, we get:\n$$\n\\mu = \\alpha + \\beta \\mu.\n$$\nRearranging,\n$$\n\\mu - \\beta \\mu = \\alpha \\quad \\Rightarrow \\quad \\mu (1 - \\beta) = \\alpha \\quad \\Rightarrow \\quad \\mu = \\frac{\\alpha}{1 - \\beta}.\n$$\nSubstituting $\\alpha = 0.4$ and $\\beta = 0.75$,\n$$\n\\mu = \\frac{0.4}{1 - 0.75} = \\frac{0.4}{0.25} = 1.6.\n$$\nThus, the long-run expected value of the inflation anomaly is $1.6$ percentage points.",
            "answer": "$$\\boxed{1.6}$$"
        },
        {
            "introduction": "Once we grasp a model's theoretical properties, the next practical step is estimating its parameters from observed data. The Yule-Walker equations offer a classic method to bridge this gap, establishing a direct link between a process's autocovariances and its underlying autoregressive coefficients. This practice will guide you through this crucial estimation procedure, demonstrating how to recover the structure of an AR(2) model from its statistical fingerprints.",
            "id": "2373810",
            "problem": "A demeaned, weakly stationary monthly excess return series $\\{r_{t}\\}$ is modeled as an autoregressive (AR) model of order $2$, that is,\n$$r_{t} = \\phi_{1} r_{t-1} + \\phi_{2} r_{t-2} + \\varepsilon_{t}$$,\nwhere $\\{\\varepsilon_{t}\\}$ is a zero-mean white noise innovation process with variance $\\sigma_{\\varepsilon}^{2}$ and $\\varepsilon_{t}$ is uncorrelated with $\\{r_{s}\\}$ for all $s < t$. From a long sample, the following sample autocovariances at lags $0$, $1$, and $2$ are computed and will be treated as population values for estimation:\n$\\hat{\\gamma}(0) = 0.010$, $\\hat{\\gamma}(1) = 0.006$, $\\hat{\\gamma}(2) = 0.002$.\nUsing the Yule–Walker equations associated with the autoregressive structure and the given autocovariances, determine the Yule–Walker estimates of the parameters $\\phi_{1}$, $\\phi_{2}$, and the innovation variance $\\sigma_{\\varepsilon}^{2}$. Express your final answer as a single row matrix $\\begin{pmatrix} \\phi_{1} & \\phi_{2} & \\sigma_{\\varepsilon}^{2} \\end{pmatrix}$. No rounding is required; provide exact values.",
            "solution": "The Yule-Walker equations for a general AR($p$) process are derived by multiplying the defining equation by $r_{t-k}$ for $k > 0$ and taking the expectation. This yields:\n$$E[r_{t}r_{t-k}] = \\sum_{i=1}^{p} \\phi_{i} E[r_{t-i}r_{t-k}] + E[\\varepsilon_{t}r_{t-k}]$$\nGiven weak stationarity and the fact that $\\varepsilon_{t}$ is uncorrelated with past values of $r_s$ (i.e., $E[\\varepsilon_{t}r_{t-k}]=0$ for $k>0$), this simplifies to:\n$$\\gamma(k) = \\sum_{i=1}^{p} \\phi_{i} \\gamma(k-i)$$\nFor the given AR($2$) process, we set $p=2$ and use $k=1$ and $k=2$:\nFor $k=1$: $\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(-1)$. Since $\\gamma(k) = \\gamma(-k)$, this is $\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(1)$.\nFor $k=2$: $\\gamma(2) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(0)$.\n\nThis gives a system of two linear equations in the two unknown parameters $\\phi_{1}$ and $\\phi_{2}$:\n$$\n\\begin{cases}\n\\gamma(1) = \\phi_{1} \\gamma(0) + \\phi_{2} \\gamma(1) \\\\\n\\gamma(2) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(0)\n\\end{cases}\n$$\nIn matrix form, this is:\n$$\n\\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\end{pmatrix} =\n\\begin{pmatrix} \\gamma(0) & \\gamma(1) \\\\ \\gamma(1) & \\gamma(0) \\end{pmatrix}\n\\begin{pmatrix} \\phi_{1} \\\\ \\phi_{2} \\end{pmatrix}\n$$\nThe problem provides the autocovariances: $\\gamma(0) = 0.010$, $\\gamma(1) = 0.006$, and $\\gamma(2) = 0.002$. To maintain precision, we convert the decimal values to fractions:\n$\\gamma(0) = 0.010 = \\frac{10}{1000} = \\frac{1}{100}$\n$\\gamma(1) = 0.006 = \\frac{6}{1000} = \\frac{3}{500}$\n$\\gamma(2) = 0.002 = \\frac{2}{1000} = \\frac{1}{500}$\n\nSubstituting these values into the system of equations:\n$$\n\\begin{cases}\n\\frac{3}{500} = \\phi_{1} \\frac{1}{100} + \\phi_{2} \\frac{3}{500} \\\\\n\\frac{1}{500} = \\phi_{1} \\frac{3}{500} + \\phi_{2} \\frac{1}{100}\n\\end{cases}\n$$\nTo simplify, we multiply both equations by $500$:\n$$\n\\begin{cases}\n3 = 5\\phi_{1} + 3\\phi_{2} \\\\\n1 = 3\\phi_{1} + 5\\phi_{2}\n\\end{cases}\n$$\nWe can solve this system. From the second equation, $3\\phi_{1} = 1 - 5\\phi_{2}$, so $\\phi_{1} = \\frac{1 - 5\\phi_{2}}{3}$. Substituting this into the first equation:\n$$3 = 5\\left(\\frac{1 - 5\\phi_{2}}{3}\\right) + 3\\phi_{2}$$\nMultiplying by $3$ gives:\n$$9 = 5(1 - 5\\phi_{2}) + 9\\phi_{2}$$\n$$9 = 5 - 25\\phi_{2} + 9\\phi_{2}$$\n$$4 = -16\\phi_{2}$$\n$$\\phi_{2} = -\\frac{4}{16} = -\\frac{1}{4}$$\nNow, we find $\\phi_{1}$:\n$$\\phi_{1} = \\frac{1 - 5(-\\frac{1}{4})}{3} = \\frac{1 + \\frac{5}{4}}{3} = \\frac{\\frac{9}{4}}{3} = \\frac{9}{12} = \\frac{3}{4}$$\nThe Yule-Walker estimates for the autoregressive coefficients are $\\phi_{1} = \\frac{3}{4}$ and $\\phi_{2} = -\\frac{1}{4}$.\n\nNext, we must find the innovation variance, $\\sigma_{\\varepsilon}^{2}$. This is obtained from the Yule-Walker equation for $k=0$, which involves the variance of the process, $\\gamma(0)$. We multiply the AR($2$) equation by $r_t$ and take expectations:\n$$E[r_{t}^2] = \\phi_{1} E[r_{t}r_{t-1}] + \\phi_{2} E[r_{t}r_{t-2}] + E[r_{t}\\varepsilon_{t}]$$\n$$\\gamma(0) = \\phi_{1} \\gamma(1) + \\phi_{2} \\gamma(2) + E[(\\phi_{1}r_{t-1} + \\phi_{2}r_{t-2} + \\varepsilon_{t})\\varepsilon_{t}]$$\nSince $\\varepsilon_{t}$ is uncorrelated with past values of $r$, $E[r_{t-1}\\varepsilon_{t}]=0$ and $E[r_{t-2}\\varepsilon_{t}]=0$. This leads to:\n$$\\gamma(0) = \\phi_{1}\\gamma(1) + \\phi_{2}\\gamma(2) + E[\\varepsilon_{t}^2]$$\n$$\\gamma(0) = \\phi_{1}\\gamma(1) + \\phi_{2}\\gamma(2) + \\sigma_{\\varepsilon}^{2}$$\nSolving for $\\sigma_{\\varepsilon}^{2}$:\n$$\\sigma_{\\varepsilon}^{2} = \\gamma(0) - \\phi_{1}\\gamma(1) - \\phi_{2}\\gamma(2)$$\nSubstituting the known values:\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\left(\\frac{3}{4}\\right)\\left(\\frac{3}{500}\\right) - \\left(-\\frac{1}{4}\\right)\\left(\\frac{1}{500}\\right)$$\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\frac{9}{2000} + \\frac{1}{2000}$$\n$$\\sigma_{\\varepsilon}^{2} = \\frac{1}{100} - \\frac{8}{2000} = \\frac{1}{100} - \\frac{1}{250}$$\nUsing a common denominator of $500$:\n$$\\sigma_{\\varepsilon}^{2} = \\frac{5}{500} - \\frac{2}{500} = \\frac{3}{500}$$\nThus, the innovation variance is $\\sigma_{\\varepsilon}^{2} = \\frac{3}{500}$.\n\nThe requested parameters are $\\phi_{1} = \\frac{3}{4}$, $\\phi_{2} = -\\frac{1}{4}$, and $\\sigma_{\\varepsilon}^{2} = \\frac{3}{500}$. We present these in the required row matrix format.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{4} & -\\frac{1}{4} & \\frac{3}{500}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In applied work, we rarely know the true data-generating process and must choose a model, often balancing complexity and accuracy. But what are the consequences if our simplifying assumptions are wrong? This advanced, hands-on exercise delves into the critical issue of model misspecification, using both theoretical derivation and computation to quantify the bias and forecast efficiency loss that arise when a simple AR(1) model is incorrectly fitted to a more complex AR(3) reality.",
            "id": "2373867",
            "problem": "Consider a strictly stationary Autoregressive (AR) process of finite order defined on a probability space with zero mean. Let the data-generating process be an autoregressive process of order three, denoted AR($3$), given by\n$$\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3} + \\varepsilon_t,\n$$\nwhere $\\{\\varepsilon_t\\}$ is an independent and identically distributed sequence with $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\mathrm{Var}(\\varepsilon_t) = \\sigma^2$, and where the autoregressive polynomial has all roots outside the unit circle so that the process is stationary. Let $\\gamma(k) = \\mathbb{E}[y_t y_{t-k}]$ denote the population autocovariance at lag $k$. In many empirical applications in computational economics and finance, a lower-order autoregressive model is estimated for parsimony or due to limited data. This problem asks you to quantify, from first principles, the consequences of omitting relevant lags by estimating an AR($1$) when the truth is AR($3$).\n\nStart from the following foundational bases:\n- The definition of stationary autoregression and its population moment conditions linking $\\gamma(k)$ and the autoregressive coefficients.\n- The definition of the linear least squares projection as the minimizer of mean squared error.\n- The fact that including additional valid regressors in a linear least squares projection cannot increase the minimum achievable mean squared error.\n\nTasks:\n1. Using only these bases, derive the population normal equations implied by the AR($3$) model that relate $\\gamma(0)$, $\\gamma(1)$, $\\gamma(2)$, and $\\gamma(3)$ to $\\phi_1$, $\\phi_2$, $\\phi_3$, and $\\sigma^2$. Then, deduce a linear system that can be solved to obtain $\\gamma(0)$, $\\gamma(1)$, $\\gamma(2)$, and $\\gamma(3)$.\n2. Using the definition of linear least squares projection, derive the pseudo-true AR($1$) coefficient $\\theta^\\star$ that minimizes $\\mathbb{E}[(y_t - \\theta y_{t-1})^2]$ under the AR($1$) restriction, expressed in terms of the population autocovariances.\n3. Derive the one-step-ahead mean squared forecast error when using the mis-specified AR($1$) predictor for $y_{t+1}$, and express it in terms of $\\gamma(0)$ and $\\gamma(1)$. Compare it to the minimal one-step-ahead mean squared forecast error attainable under the correctly specified AR($3$) model, expressed in terms of $\\sigma^2$. Define the forecast efficiency loss as the ratio of these two mean squared errors.\n\nImplementation requirements:\n- Implement a program that, for each parameter set in the test suite below, computes:\n  - the pseudo-true AR($1$) coefficient $\\theta^\\star$;\n  - the asymptotic bias $\\theta^\\star - \\phi_1$ induced by omitting $y_{t-2}$ and $y_{t-3}$;\n  - the one-step-ahead forecast mean squared error ratio\n  $$\n  R \\equiv \\frac{\\text{MSE under AR($1$) restriction}}{\\text{MSE under correctly specified AR($3$)}}.\n  $$\n- Your program must construct and solve the linear system for $\\gamma(0)$, $\\gamma(1)$, $\\gamma(2)$, and $\\gamma(3)$ that follows from the AR($3$) moment equations; it must not rely on simulation or numerical integration.\n\nTest suite (each case is $(\\phi_1,\\phi_2,\\phi_3,\\sigma^2)$):\n- Case A (general “happy path”): $(0.5, 0.3, -0.2, 1.0)$.\n- Case B (no omitted lags, boundary check): $(0.6, 0.0, 0.0, 1.0)$.\n- Case C (only the third lag matters, omitted-lag edge case): $(0.0, 0.0, 0.7, 1.0)$.\n- Case D (small positive coefficients): $(0.3, 0.2, 0.1, 1.0)$.\n\nFinal output format:\n- For each case, output three numbers in this order: $\\theta^\\star$, $\\theta^\\star - \\phi_1$, and $R$.\n- Aggregate the results of all cases into a single flat list in the order A, B, C, D, so the output contains $12$ numbers.\n- Express all numbers rounded to $6$ decimal places and without scientific notation.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,\\dots,x_{12}]$).",
            "solution": "The strictly stationary data-generating process is an autoregressive process of order three, AR($3$):\n$$\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3} + \\varepsilon_t\n$$\nwhere $\\{\\varepsilon_t\\}$ is a white noise process with $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\mathrm{Var}(\\varepsilon_t) = \\sigma^2$. The process is zero-mean, $\\mathbb{E}[y_t] = 0$. The population autocovariance at lag $k$ is $\\gamma(k) = \\mathbb{E}[y_t y_{t-k}]$. Due to stationarity, $\\gamma(k)$ is independent of time $t$, and $\\gamma(k) = \\gamma(-k)$.\n\n**Task 1: Derivation of the Linear System for Autocovariances**\n\nThe Yule-Walker equations provide the relationship between the autoregressive coefficients and the autocovariances. They are derived by multiplying the AR($p$) equation by $y_{t-k}$ for $k \\ge 0$ and taking the expectation. For our AR($3$) process, we consider $k \\in \\{0, 1, 2, 3\\}$.\n\nA key property is that the innovation $\\varepsilon_t$ is uncorrelated with all past values of the process, a consequence of it being the error in the projection of $y_t$ onto the past. Thus, for $k > 0$, $\\mathbb{E}[\\varepsilon_t y_{t-k}] = 0$. For $k=0$, we have $\\mathbb{E}[\\varepsilon_t y_t] = \\mathbb{E}[\\varepsilon_t(\\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3} + \\varepsilon_t)] = \\mathbb{E}[\\varepsilon_t^2] = \\sigma^2$.\n\nFor $k=1$:\n$$\n\\mathbb{E}[y_t y_{t-1}] = \\phi_1 \\mathbb{E}[y_{t-1} y_{t-1}] + \\phi_2 \\mathbb{E}[y_{t-2} y_{t-1}] + \\phi_3 \\mathbb{E}[y_{t-3} y_{t-1}] + \\mathbb{E}[\\varepsilon_t y_{t-1}]\n$$\n$$\n\\gamma(1) = \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) + \\phi_3 \\gamma(2)\n$$\n\nFor $k=2$:\n$$\n\\mathbb{E}[y_t y_{t-2}] = \\phi_1 \\mathbb{E}[y_{t-1} y_{t-2}] + \\phi_2 \\mathbb{E}[y_{t-2} y_{t-2}] + \\phi_3 \\mathbb{E}[y_{t-3} y_{t-2}] + \\mathbb{E}[\\varepsilon_t y_{t-2}]\n$$\n$$\n\\gamma(2) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0) + \\phi_3 \\gamma(1)\n$$\n\nFor $k=3$:\n$$\n\\mathbb{E}[y_t y_{t-3}] = \\phi_1 \\mathbb{E}[y_{t-1} y_{t-3}] + \\phi_2 \\mathbb{E}[y_{t-2} y_{t-3}] + \\phi_3 \\mathbb{E}[y_{t-3} y_{t-3}] + \\mathbb{E}[\\varepsilon_t y_{t-3}]\n$$\n$$\n\\gamma(3) = \\phi_1 \\gamma(2) + \\phi_2 \\gamma(1) + \\phi_3 \\gamma(0)\n$$\n\nFor $k=0$ (the variance of the process):\n$$\n\\mathbb{E}[y_t y_t] = \\phi_1 \\mathbb{E}[y_{t-1} y_t] + \\phi_2 \\mathbb{E}[y_{t-2} y_t] + \\phi_3 \\mathbb{E}[y_{t-3} y_t] + \\mathbb{E}[\\varepsilon_t y_t]\n$$\n$$\n\\gamma(0) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + \\phi_3 \\gamma(3) + \\sigma^2\n$$\n\nTo solve for $\\gamma(0), \\gamma(1), \\gamma(2), \\gamma(3)$, we rearrange these four equations into a linear system with the autocovariances as unknowns.\n1.  $-\\phi_1 \\gamma(0) + (1-\\phi_2) \\gamma(1) - \\phi_3 \\gamma(2) + 0 \\cdot \\gamma(3) = 0$\n2.  $-\\phi_2 \\gamma(0) - (\\phi_1+\\phi_3) \\gamma(1) + \\gamma(2) + 0 \\cdot \\gamma(3) = 0$\n3.  $-\\phi_3 \\gamma(0) - \\phi_2 \\gamma(1) - \\phi_1 \\gamma(2) + \\gamma(3) = 0$\n4.  $\\gamma(0) - \\phi_1 \\gamma(1) - \\phi_2 \\gamma(2) - \\phi_3 \\gamma(3) = \\sigma^2$\n\nThis constitutes a $4 \\times 4$ linear system, $M \\boldsymbol{\\gamma} = \\boldsymbol{b}$, where $\\boldsymbol{\\gamma} = [\\gamma(0), \\gamma(1), \\gamma(2), \\gamma(3)]^T$:\n$$\n\\begin{pmatrix}\n-\\phi_1 & 1-\\phi_2 & -\\phi_3 & 0 \\\\\n-\\phi_2 & -(\\phi_1+\\phi_3) & 1 & 0 \\\\\n-\\phi_3 & -\\phi_2 & -\\phi_1 & 1 \\\\\n1 & -\\phi_1 & -\\phi_2 & -\\phi_3\n\\end{pmatrix}\n\\begin{pmatrix}\n\\gamma(0) \\\\\n\\gamma(1) \\\\\n\\gamma(2) \\\\\n\\gamma(3)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n\\sigma^2\n\\end{pmatrix}\n$$\nSolving this system yields the required autocovariances.\n\n**Task 2: Derivation of the Pseudo-True AR(1) Coefficient $\\theta^\\star$**\n\nWhen an AR($1$) model, $y_t = \\theta y_{t-1} + u_t$, is fitted to the data, the parameter $\\theta$ is estimated by minimizing the mean squared error. The population analog of this estimation is the linear least squares projection of $y_t$ onto $y_{t-1}$. We seek to find the coefficient $\\theta^\\star$ that minimizes the objective function $S(\\theta) = \\mathbb{E}[(y_t - \\theta y_{t-1})^2]$.\n\nTo find the minimum, we set the first derivative with respect to $\\theta$ to zero:\n$$\n\\frac{d S(\\theta)}{d\\theta} = \\mathbb{E}\\left[ \\frac{d}{d\\theta} (y_t - \\theta y_{t-1})^2 \\right] = \\mathbb{E}[-2(y_t - \\theta y_{t-1})y_{t-1}] = 0\n$$\n$$\n\\mathbb{E}[-y_t y_{t-1} + \\theta y_{t-1}^2] = 0\n$$\nThis is the population normal equation for the AR($1$) projection. Expressing it in terms of autocovariances:\n$$\n-\\mathbb{E}[y_t y_{t-1}] + \\theta \\mathbb{E}[y_{t-1}^2] = 0\n$$\n$$\n-\\gamma(1) + \\theta \\gamma(0) = 0\n$$\nSolving for $\\theta$ yields the pseudo-true coefficient $\\theta^\\star$:\n$$\n\\theta^\\star = \\frac{\\gamma(1)}{\\gamma(0)}\n$$\nThis is simply the first autocorrelation coefficient of the process, $\\rho(1)$. The asymptotic bias from omitting $y_{t-2}$ and $y_{t-3}$ is the difference between this pseudo-true value and the true coefficient of $y_{t-1}$, which is $\\theta^\\star - \\phi_1$.\n\n**Task 3: Derivation of Forecast Mean Squared Errors and their Ratio**\n\nFirst, we determine the mean squared error (MSE) of the one-step-ahead forecast from the mis-specified AR($1$) model. The forecast is $\\hat{y}_t = \\theta^\\star y_{t-1}$. The MSE is the value of the minimized objective function $S(\\theta^\\star)$:\n$$\n\\mathrm{MSE}_{\\text{AR(1)}} = \\mathbb{E}[(y_t - \\theta^\\star y_{t-1})^2] = \\mathbb{E}[y_t^2 - 2\\theta^\\star y_t y_{t-1} + (\\theta^\\star)^2 y_{t-1}^2]\n$$\n$$\n\\mathrm{MSE}_{\\text{AR(1)}} = \\gamma(0) - 2\\theta^\\star \\gamma(1) + (\\theta^\\star)^2 \\gamma(0)\n$$\nSubstituting $\\theta^\\star = \\gamma(1)/\\gamma(0)$:\n$$\n\\mathrm{MSE}_{\\text{AR(1)}} = \\gamma(0) - 2\\frac{\\gamma(1)}{\\gamma(0)}\\gamma(1) + \\left(\\frac{\\gamma(1)}{\\gamma(0)}\\right)^2 \\gamma(0) = \\gamma(0) - \\frac{2\\gamma(1)^2}{\\gamma(0)} + \\frac{\\gamma(1)^2}{\\gamma(0)}\n$$\n$$\n\\mathrm{MSE}_{\\text{AR(1)}} = \\gamma(0) - \\frac{\\gamma(1)^2}{\\gamma(0)}\n$$\nNext, we determine the MSE of the forecast from the correctly specified AR($3$) model. The optimal linear predictor for $y_t$ given the past is $\\hat{y}_t = \\mathbb{E}[y_t | y_{t-1}, y_{t-2}, y_{t-3}] = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3}$. The forecast error is:\n$$\ny_t - \\hat{y}_t = ( \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3} + \\varepsilon_t ) - (\\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3}) = \\varepsilon_t\n$$\nThe MSE is the variance of this error:\n$$\n\\mathrm{MSE}_{\\text{AR(3)}} = \\mathbb{E}[(y_t - \\hat{y}_t)^2] = \\mathbb{E}[\\varepsilon_t^2] = \\sigma^2\n$$\nThis is the minimum achievable MSE, as stated in the problem's premises. The forecast efficiency loss is the ratio of these two MSEs:\n$$\nR \\equiv \\frac{\\mathrm{MSE}_{\\text{AR(1)}}}{\\mathrm{MSE}_{\\text{AR(3)}}} = \\frac{\\gamma(0) - \\gamma(1)^2/\\gamma(0)}{\\sigma^2}\n$$\nThe computational procedure will involve solving the $4 \\times 4$ linear system for the autocovariances for each parameter set and then substituting the obtained values of $\\gamma(0)$ and $\\gamma(1)$ into the derived formulae for $\\theta^\\star$ and $R$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a suite of test cases, calculating metrics\n    related to AR(1) misspecification of a true AR(3) process.\n    \"\"\"\n\n    # Test suite: each case is a tuple (phi1, phi2, phi3, sigma2)\n    test_cases = [\n        (0.5, 0.3, -0.2, 1.0),    # Case A\n        (0.6, 0.0, 0.0, 1.0),    # Case B\n        (0.0, 0.0, 0.7, 1.0),    # Case C\n        (0.3, 0.2, 0.1, 1.0),    # Case D\n    ]\n\n    all_results = []\n    for params in test_cases:\n        phi1, phi2, phi3, sigma2 = params\n        \n        # Task 1: Construct and solve the linear system for autocovariances.\n        # The system is M * gamma_vec = b, where gamma_vec = [gamma(0), gamma(1), gamma(2), gamma(3)]'.\n        \n        M = np.array([\n            [-phi1, 1 - phi2, -phi3, 0],\n            [-phi2, -(phi1 + phi3), 1, 0],\n            [-phi3, -phi2, -phi1, 1],\n            [1, -phi1, -phi2, -phi3]\n        ])\n        \n        b = np.array([0, 0, 0, sigma2])\n        \n        try:\n            # Solve for gamma_vec = [gamma(0), gamma(1), gamma(2), gamma(3)]\n            gamma_vec = np.linalg.solve(M, b)\n            gamma0 = gamma_vec[0]\n            gamma1 = gamma_vec[1]\n        except np.linalg.LinAlgError:\n            # This should not happen for stationary processes, but is included for robustness.\n            # Assign NaN to indicate failure to solve.\n            theta_star, bias, R = np.nan, np.nan, np.nan\n            all_results.extend([theta_star, bias, R])\n            continue\n            \n        # Ensure gamma0 is not zero to avoid division by zero.\n        # For a stationary process with non-zero innovation variance, gamma(0) > 0.\n        if gamma0 == 0:\n            theta_star, bias, R = np.nan, np.nan, np.nan\n            all_results.extend([theta_star, bias, R])\n            continue\n\n        # Task 2: Calculate the pseudo-true AR(1) coefficient and the bias.\n        # theta_star = gamma(1) / gamma(0)\n        theta_star = gamma1 / gamma0\n        \n        # bias = theta_star - phi1\n        bias = theta_star - phi1\n        \n        # Task 3: Calculate the forecast efficiency loss ratio R.\n        # R = MSE_AR1 / MSE_AR3 = (gamma(0) - gamma(1)^2 / gamma(0)) / sigma^2\n        mse_ar1 = gamma0 - (gamma1**2 / gamma0)\n        # MSE for AR(3) is sigma2\n        R = mse_ar1 / sigma2\n        \n        all_results.extend([theta_star, bias, R])\n\n    # Final print statement in the exact required format.\n    # The format string ensures 6 decimal places and no scientific notation.\n    print(f\"[{','.join(f'{x:.6f}' for x in all_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}