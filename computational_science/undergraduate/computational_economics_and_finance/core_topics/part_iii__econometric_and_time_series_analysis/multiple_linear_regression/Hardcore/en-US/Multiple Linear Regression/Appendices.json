{
    "hands_on_practices": [
        {
            "introduction": "This first exercise serves as a comprehensive, end-to-end application of multiple linear regression. You will step into the shoes of a data analyst tasked with predicting mobile app demand, a common scenario in computational finance and marketing. By building a model from the ground up to relate downloads to factors like price and advertising, you will practice not only the mechanics of OLS estimation but also prediction, coefficient interpretation, and initial model diagnostics .",
            "id": "2413185",
            "problem": "You are asked to implement and apply a multiple linear regression in a computational finance setting. The task is to build an Ordinary Least Squares (OLS) model of mobile application demand, where the target variable is the number of downloads. The predictors are price in United States Dollars (USD), user rating on a $1$ to $5$ scale, and advertising spend on social media in USD. The model must include an intercept. All predicted downloads must be expressed as counts (unitless), and all currency inputs are in USD. No angles are involved. All outputs must be numeric in the types boolean, integer, or float as specified.\n\nStarting from fundamental definitions, you must treat OLS as the solution to minimizing the sum of squared residuals across the training data. You must not rely on pre-canned black-box models beyond numeric linear algebra primitives. You must use the provided training data to estimate the model, then answer the test suite that follows.\n\nTraining data: Each row gives $(\\text{price}, \\text{user\\_rating}, \\text{advertising\\_spend\\_on\\_social\\_media}) \\to \\text{downloads}$, where downloads are counts.\n\n- Row $1$: $(0.99, 4.5, 12000) \\to 31320$\n- Row $2$: $(1.99, 4.2, 8000) \\to 24620$\n- Row $3$: $(0.00, 3.8, 5000) \\to 25700$\n- Row $4$: $(2.99, 4.7, 15000) \\to 29620$\n- Row $5$: $(4.99, 4.9, 20000) \\to 30970$\n- Row $6$: $(1.49, 3.5, 0) \\to 17370$\n- Row $7$: $(0.00, 4.0, 7000) \\to 27950$\n- Row $8$: $(3.99, 4.4, 11000) \\to 23820$\n- Row $9$: $(2.49, 3.9, 6000) \\to 21570$\n- Row $10$: $(0.99, 4.8, 18000) \\to 36720$\n- Row $11$: $(1.99, 4.1, 4000) \\to 21440$\n- Row $12$: $(0.49, 3.6, 2000) \\to 21540$\n\nValidation data for out-of-sample evaluation: Each row gives $(\\text{price}, \\text{user\\_rating}, \\text{advertising\\_spend\\_on\\_social\\_media}) \\to \\text{downloads}$.\n\n- V$1$: $(2.99, 4.0, 9000) \\to 23320$\n- V$2$: $(0.00, 4.9, 16000) \\to 37300$\n- V$3$: $(1.49, 3.7, 4000) \\to 21370$\n\nYou must implement the following, grounded in first principles of multiple linear regression:\n\n- Fit a linear model for downloads as a function of an intercept, price, user rating, and advertising spend by minimizing the sum of squared residuals over the training set.\n- Compute the estimated coefficient vector and residual variance using the training set only.\n- Use the fitted model to compute required quantities for the test suite below.\n\nTest suite and required outputs:\n\nLet $n$ denote the number of training observations and $k$ the number of model parameters, including the intercept. Here $n = 12$ and $k = 4$.\n\nYour program must compute the following five results, in order:\n\n$1.$ Prediction task (happy path): Using the fitted model, predict downloads for a new app with $(\\text{price} = 1.99,\\ \\text{user\\_rating} = 4.5,\\ \\text{advertising\\_spend\\_on\\_social\\_media} = 10000)$. Output the prediction rounded to the nearest integer (count).\n\n$2.$ Coefficient extraction: Output the estimated coefficient for price as a float rounded to $2$ decimal places.\n\n$3.$ Out-of-sample goodness-of-fit: Compute the coefficient of determination on the validation set, defined as $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$, where $\\bar{y}_{\\text{test}}$ is the mean of the validation targets. Output this as a float rounded to $4$ decimal places.\n\n$4.$ Statistical significance of price (edge case: small residual variance): Using a two-sided Student’s $t$-test at significance level $\\alpha = 0.05$ with degrees of freedom $n - k$, test the null hypothesis that the price coefficient equals $0$ against the alternative that it differs from $0$. Output a boolean: $True$ if significant, $False$ otherwise.\n\n$5.$ Numerical stability diagnostic (multicollinearity check): Compute the spectral condition number of the training design matrix with intercept using the $2$-norm, i.e., the ratio of its largest to smallest singular values. Output this as a float rounded to $2$ decimal places.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). The results must appear in the exact order specified: prediction (integer), price coefficient (float rounded to $2$ decimals), test $R^{2}$ (float rounded to $4$ decimals), price-significance boolean, condition number (float rounded to $2$ decimals).",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Task**: Implement and apply multiple linear regression using Ordinary Least Squares (OLS) to model mobile application demand.\n- **Target Variable ($y$)**: Number of downloads (counts).\n- **Predictors ($x_j$)**: Price (USD), user rating ($1-5$ scale), advertising spend (USD).\n- **Model Specification**: A linear model including an intercept term.\n- **Methodology**: Minimize the sum of squared residuals. No \"black-box\" models are to be used, only linear algebra primitives.\n- **Training Data** ($n=12$ observations):\n  - $(\\text{price}, \\text{user\\_rating}, \\text{ad\\_spend}) \\to \\text{downloads}$\n  - $(0.99, 4.5, 12000) \\to 31320$\n  - $(1.99, 4.2, 8000) \\to 24620$\n  - $(0.00, 3.8, 5000) \\to 25700$\n  - $(2.99, 4.7, 15000) \\to 29620$\n  - $(4.99, 4.9, 20000) \\to 30970$\n  - $(1.49, 3.5, 0) \\to 17370$\n  - $(0.00, 4.0, 7000) \\to 27950$\n  - $(3.99, 4.4, 11000) \\to 23820$\n  - $(2.49, 3.9, 6000) \\to 21570$\n  - $(0.99, 4.8, 18000) \\to 36720$\n  - $(1.99, 4.1, 4000) \\to 21440$\n  - $(0.49, 3.6, 2000) \\to 21540$\n- **Validation Data** ($3$ observations):\n  - $(2.99, 4.0, 9000) \\to 23320$\n  - $(0.00, 4.9, 16000) \\to 37300$\n  - $(1.49, 3.7, 4000) \\to 21370$\n- **Constants**: Number of training observations $n=12$, number of model parameters $k=4$.\n- **Test Suite**:\n  1.  Predict downloads for $(\\text{price}=1.99, \\text{user\\_rating}=4.5, \\text{ad\\_spend}=10000)$. Output: integer.\n  2.  Extract the estimated coefficient for price. Output: float, $2$ decimal places.\n  3.  Compute $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$ on the validation set. Output: float, $4$ decimal places.\n  4.  Test significance of the price coefficient using a two-sided t-test at $\\alpha = 0.05$ with $n-k$ degrees of freedom. Output: boolean.\n  5.  Compute the spectral condition number of the training design matrix. Output: float, $2$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard, well-defined application of multiple linear regression, a fundamental method in statistics and econometrics.\n- **Scientifically Grounded**: The problem is based on Ordinary Least Squares (OLS), a core and well-established statistical theory. The context is realistic for computational economics and finance.\n- **Well-Posed**: The problem provides sufficient data (12 observations for 4 parameters) to estimate the model coefficients. The tasks in the test suite are specific, unambiguous, and have unique solutions based on standard statistical formulas.\n- **Objective**: The problem statement is written in precise, objective language. The data are numerical, and the required calculations are based on established mathematical formulas, not subjective interpretation.\n- The problem does not violate any of the invalidity criteria. It is not based on false premises, it is formalizable, the data is complete, and the\ntasks are verifiable.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\n### Solution\nThe problem requires fitting a multiple linear regression model using the Ordinary Least Squares (OLS) method. The model is of the form:\n$$ \\text{downloads} = \\beta_0 + \\beta_1 \\cdot \\text{price} + \\beta_2 \\cdot \\text{user\\_rating} + \\beta_3 \\cdot \\text{ad\\_spend} + \\epsilon $$\nIn matrix notation, this is expressed as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y}$ is the vector of observed downloads, $\\mathbf{X}$ is the design matrix, $\\boldsymbol{\\beta}$ is the vector of coefficients, and $\\boldsymbol{\\epsilon}$ is the vector of errors.\n\nThe OLS method finds the coefficient vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes the sum of squared residuals (SSR), $S = \\sum_{i=1}^{n} \\epsilon_i^2$.\n$$ S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) $$\nTo minimize this quantity, we take the gradient with respect to $\\boldsymbol{\\beta}$ and set it to zero:\n$$ \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0} $$\nThis yields the normal equations:\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y} $$\nThe solution for the OLS estimator $\\hat{\\boldsymbol{\\beta}}$ is given by:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\nThis equation will be solved using numerically stable linear algebra routines.\n\nFirst, we construct the design matrix $\\mathbf{X}_{\\text{train}}$ and the target vector $\\mathbf{y}_{\\text{train}}$ from the $n=12$ training observations. The matrix $\\mathbf{X}_{\\text{train}}$ has $k=4$ columns: one for the intercept ($\\beta_0$) and one for each of the three predictors.\n$$\n\\mathbf{y}_{\\text{train}} = \\begin{pmatrix} 31320 \\\\ 24620 \\\\ 25700 \\\\ 29620 \\\\ 30970 \\\\ 17370 \\\\ 27950 \\\\ 23820 \\\\ 21570 \\\\ 36720 \\\\ 21440 \\\\ 21540 \\end{pmatrix},\n\\quad\n\\mathbf{X}_{\\text{train}} = \\begin{pmatrix}\n1 & 0.99 & 4.5 & 12000 \\\\\n1 & 1.99 & 4.2 & 8000 \\\\\n1 & 0.00 & 3.8 & 5000 \\\\\n1 & 2.99 & 4.7 & 15000 \\\\\n1 & 4.99 & 4.9 & 20000 \\\\\n1 & 1.49 & 3.5 & 0 \\\\\n1 & 0.00 & 4.0 & 7000 \\\\\n1 & 3.99 & 4.4 & 11000 \\\\\n1 & 2.49 & 3.9 & 6000 \\\\\n1 & 0.99 & 4.8 & 18000 \\\\\n1 & 1.99 & 4.1 & 4000 \\\\\n1 & 0.49 & 3.6 & 2000 \\\\\n\\end{pmatrix}\n$$\nSolving the normal equations for $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3]^T$ gives the estimated coefficient vector. Let's proceed to solve the test suite questions.\n\n**1. Prediction Task**\nA new observation is given by the predictor vector $(\\text{price}=1.99, \\text{user\\_rating}=4.5, \\text{ad\\_spend}=10000)$. We form a design vector $\\mathbf{x}_{\\text{new}}$ with an intercept:\n$$ \\mathbf{x}_{\\text{new}} = \\begin{pmatrix} 1 & 1.99 & 4.5 & 10000 \\end{pmatrix}^T $$\nThe predicted number of downloads $\\hat{y}_{\\text{new}}$ is calculated as:\n$$ \\hat{y}_{\\text{new}} = \\mathbf{x}_{\\text{new}}^T \\hat{\\boldsymbol{\\beta}} $$\nThe result is rounded to the nearest integer.\n\n**2. Coefficient Extraction**\nThe estimated coefficient for price, $\\hat{\\beta}_1$, is the second element of the vector $\\hat{\\boldsymbol{\\beta}}$. This value is extracted and rounded to $2$ decimal places.\n\n**3. Out-of-sample Goodness-of-Fit**\nThe coefficient of determination for the validation set, $R^{2}_{\\text{test}}$, is computed as:\n$$ R^{2}_{\\text{test}} = 1 - \\frac{SSR_{\\text{test}}}{SST_{\\text{test}}} $$\nwhere $SSR_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2$ is the sum of squared residuals on the validation set, and $SST_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2$ is the total sum of squares for the validation set. $\\bar{y}_{\\text{test}}$ is the mean of the observed downloads in the validation data. Predictions $\\hat{y}_i$ are generated using the fitted model: $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$. The result is rounded to $4$ decimal places.\n\n**4. Statistical Significance of Price**\nWe test the null hypothesis $H_0: \\beta_1 = 0$ against the alternative $H_1: \\beta_1 \\neq 0$ using a two-sided Student's t-test. The t-statistic is:\n$$ t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} $$\nThe standard error of the coefficient, $SE(\\hat{\\beta}_1)$, is the square root of the corresponding diagonal element of the coefficient covariance matrix, $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}_{\\text{train}}^T \\mathbf{X}_{\\text{train}})^{-1}$. The residual variance, $\\hat{\\sigma}^2$, is an unbiased estimator of the error variance $\\sigma^2$ and is calculated from the training data:\n$$ \\hat{\\sigma}^2 = \\frac{SSR_{\\text{train}}}{n-k} = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-k} $$\nThe degrees of freedom for the t-distribution are $df = n-k = 12-4=8$. We compare the absolute value of the calculated t-statistic, $|t|$, to the critical value $t_{\\text{crit}}$ from the t-distribution for a significance level $\\alpha = 0.05$ and $df=8$. If $|t| > t_{\\text{crit}}$, we reject $H_0$, and the coefficient is statistically significant.\n\n**5. Numerical Stability Diagnostic**\nThe spectral condition number of the training design matrix $\\mathbf{X}_{\\text{train}}$ is the ratio of its largest singular value ($\\sigma_{\\max}$) to its smallest singular value ($\\sigma_{\\min}$):\n$$ \\kappa_2(\\mathbf{X}_{\\text{train}}) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} $$\nThis value is a measure of multicollinearity in the predictors. A high value indicates potential numerical instability in estimating the coefficients. The result is rounded to $2$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Implements and applies a multiple linear regression model from first principles.\n    \"\"\"\n    # Define the problem data as specified.\n    # Training Data (n=12)\n    y_train_list = [31320, 24620, 25700, 29620, 30970, 17370, 27950, 23820, 21570, 36720, 21440, 21540]\n    X_train_predictors_list = [\n        [0.99, 4.5, 12000], [1.99, 4.2, 8000], [0.00, 3.8, 5000], [2.99, 4.7, 15000],\n        [4.99, 4.9, 20000], [1.49, 3.5, 0], [0.00, 4.0, 7000], [3.99, 4.4, 11000],\n        [2.49, 3.9, 6000], [0.99, 4.8, 18000], [1.99, 4.1, 4000], [0.49, 3.6, 2000]\n    ]\n\n    # Validation Data\n    y_val_list = [23320, 37300, 21370]\n    X_val_predictors_list = [\n        [2.99, 4.0, 9000], [0.00, 4.9, 16000], [1.49, 3.7, 4000]\n    ]\n\n    # New app data for prediction\n    x_new_predictors_list = [1.99, 4.5, 10000]\n    \n    # Convert lists to numpy arrays\n    y_train = np.array(y_train_list, dtype=float)\n    X_train_predictors = np.array(X_train_predictors_list, dtype=float)\n    y_val = np.array(y_val_list, dtype=float)\n    X_val_predictors = np.array(X_val_predictors_list, dtype=float)\n    x_new_predictors = np.array(x_new_predictors_list, dtype=float)\n\n    # Add intercept column to design matrices\n    n_train = X_train_predictors.shape[0]\n    n_val = X_val_predictors.shape[0]\n    X_train = np.hstack([np.ones((n_train, 1)), X_train_predictors])\n    X_val = np.hstack([np.ones((n_val, 1)), X_val_predictors])\n    x_new = np.hstack([1, x_new_predictors])\n    \n    # --- Model Fitting: Solve Normal Equations ---\n    # (X.T @ X) @ beta = X.T @ y\n    XTX = X_train.T @ X_train\n    XTy = X_train.T @ y_train\n    beta_hat = np.linalg.solve(XTX, XTy)\n\n    # --- Test Suite Computations ---\n    results = []\n\n    # 1. Prediction task\n    y_pred_new = x_new @ beta_hat\n    result1 = int(round(y_pred_new))\n    results.append(result1)\n\n    # 2. Coefficient extraction (price)\n    price_coeff = beta_hat[1]\n    result2 = round(price_coeff, 2)\n    results.append(result2)\n\n    # 3. Out-of-sample R-squared\n    y_pred_val = X_val @ beta_hat\n    residuals_val = y_val - y_pred_val\n    ssr_val = np.sum(residuals_val**2)\n    y_mean_val = np.mean(y_val)\n    sst_val = np.sum((y_val - y_mean_val)**2)\n    r2_val = 1 - (ssr_val / sst_val)\n    result3 = round(r2_val, 4)\n    results.append(result3)\n    \n    # 4. Statistical significance of price\n    n = n_train\n    k = X_train.shape[1] # number of parameters\n    df = n - k\n    y_pred_train = X_train @ beta_hat\n    residuals_train = y_train - y_pred_train\n    ssr_train = np.sum(residuals_train**2)\n    sigma_sq_hat = ssr_train / df\n    XTX_inv = np.linalg.inv(XTX)\n    var_beta_hat = sigma_sq_hat * XTX_inv\n    se_price_coeff = np.sqrt(var_beta_hat[1, 1])\n    t_statistic = price_coeff / se_price_coeff\n    alpha = 0.05\n    t_critical = t.ppf(1 - alpha/2, df)\n    result4 = bool(abs(t_statistic) > t_critical)\n    results.append(result4)\n    \n    # 5. Numerical stability diagnostic (condition number)\n    cond_num = np.linalg.cond(X_train, 2)\n    result5 = round(cond_num, 2)\n    results.append(result5)\n\n    # Final print statement in the exact required format.\n    # Example format: [21438,-1803.97,0.6216,True,1367.65]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our first practice introduced the condition number as a diagnostic for multicollinearity. This exercise provides a deeper, more intuitive understanding of *why* high collinearity is problematic. Through a controlled simulation, you will directly observe how increasing the correlation between two predictors causes the variances of their estimated coefficients to 'inflate,' making your estimates less precise and reliable .",
            "id": "2413145",
            "problem": "Consider the multiple linear regression model with an intercept, where the response and covariates are generated by the following data-generating process. For each observation index $i \\in \\{1,\\dots,n\\}$, let\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_{1i} \\;+\\; \\beta_2 x_{2i} \\;+\\; \\varepsilon_i,\n$$\nwith $x_{1i} \\sim \\mathcal{N}(0,1)$, $u_i \\sim \\mathcal{N}(0,\\sigma_u^2)$, $x_{2i} \\;=\\; 3 x_{1i} \\;+\\; u_i$, and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$. All random variables are mutually independent across $i$ and across types. The parameter vector is fixed at $\\beta_0 = 0.5$, $\\beta_1 = 2.0$, $\\beta_2 = -1.5$, and the error variance is fixed at $\\sigma_\\varepsilon^2 = 1.0$. The sample size is $n = 800$. The random number generator must be initialized with the seed $2025$ for reproducibility; for each test case $j$ in the test suite below, use the seed $2025 + j$.\n\nFor each specified value of $\\sigma_u$ in the test suite below, generate a dataset according to the process above and fit the ordinary least squares model with an intercept and regressors $x_1$ and $x_2$. From the fitted model, form the estimated variance-covariance matrix of the coefficient vector $\\widehat{\\beta}$ under the classical linear model assumptions. Then compute the following two scalar diagnostics:\n1. The spectral condition number $\\kappa$ of $X^\\top X$, defined as the ratio of its largest eigenvalue to its smallest eigenvalue.\n2. The sum of the marginal variances of the slope estimators, defined as $V_{\\text{slopes}} \\equiv \\mathbb{V}[\\widehat{\\beta}_1] + \\mathbb{V}[\\widehat{\\beta}_2]$, where the variances are taken from the diagonal entries of the estimated variance-covariance matrix of $\\widehat{\\beta}$.\n\nTest suite. Use the following four values for $\\sigma_u$: $(5.0,\\; 1.0,\\; 0.1,\\; 10^{-6})$. These cover, respectively, a case with weak collinearity, a typical case, a strong collinearity case, and a near-perfect collinearity edge case. For each case $j \\in \\{1,2,3,4\\}$, set the seed to $2025 + j$, generate $n = 800$ observations, and compute $(\\kappa_j, V_{\\text{slopes},j})$.\n\nRequired final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the entries are ordered as $[\\kappa_1, V_{\\text{slopes},1}, \\kappa_2, V_{\\text{slopes},2}, \\kappa_3, V_{\\text{slopes},3}, \\kappa_4, V_{\\text{slopes},4}]$. Each number must be rendered as a floating-point decimal rounded to exactly six digits after the decimal point. No additional text or lines should be printed.",
            "solution": "The problem statement is found to be valid. It presents a well-defined simulation exercise from the field of econometrics to study the consequences of multicollinearity in a multiple linear regression model. All parameters, distributions, and procedural steps are specified with sufficient clarity and precision to permit a unique, reproducible solution. The problem is scientifically grounded, internally consistent, and programmatically feasible.\n\nThe objective is to analyze how the degree of linear dependency between two regressors, $x_1$ and $x_2$, impacts two key diagnostics of an Ordinary Least Squares (OLS) regression: the condition number of the design matrix and the precision of the estimated slope coefficients.\n\nThe multiple linear regression model is specified in vector form as:\n$$\nY = X\\beta + \\varepsilon\n$$\nwhere $Y$ is the $n \\times 1$ vector of the response variable, $X$ is the $n \\times p$ design matrix of regressors (with $p=3$ parameters: intercept, $\\beta_1$, $\\beta_2$), $\\beta$ is the $p \\times 1$ vector of true coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of stochastic error terms. In this problem, $n=800$, the first column of $X$ is a vector of ones for the intercept $\\beta_0=0.5$, the second column contains observations of $x_1$, and the third column contains observations of $x_2$. The true slope coefficients are $\\beta_1=2.0$ and $\\beta_2=-1.5$. The error terms $\\varepsilon_i$ are independent and identically distributed as $\\mathcal{N}(0, \\sigma_\\varepsilon^2)$ with $\\sigma_\\varepsilon^2=1.0$.\n\nMulticollinearity is intentionally introduced through the data-generating process for the regressors. The second regressor, $x_{2i}$, is a linear function of the first, $x_{1i}$, plus a random component $u_i$:\n$$\nx_{2i} = 3 x_{1i} + u_i\n$$\nHere, $x_{1i} \\sim \\mathcal{N}(0,1)$ and $u_i \\sim \\mathcal{N}(0, \\sigma_u^2)$, with all random variables being mutually independent. The severity of the collinearity between $x_1$ and $x_2$ is controlled by the variance of $u_i$, $\\sigma_u^2$. The theoretical correlation between $x_1$ and $x_2$ is given by:\n$$\n\\text{Corr}(x_1, x_2) = \\frac{\\text{Cov}(x_1, x_2)}{\\sqrt{\\text{Var}(x_1)\\text{Var}(x_2)}} = \\frac{\\text{Cov}(x_1, 3x_1 + u)}{\\sqrt{\\text{Var}(x_1) \\cdot \\text{Var}(3x_1 + u)}} = \\frac{3\\text{Var}(x_1)}{\\sqrt{\\text{Var}(x_1) \\cdot (9\\text{Var}(x_1) + \\text{Var}(u))}} = \\frac{3(1)}{\\sqrt{1 \\cdot (9(1) + \\sigma_u^2)}} = \\frac{3}{\\sqrt{9 + \\sigma_u^2}}\n$$\nAs $\\sigma_u \\rightarrow 0$, the error term $u_i$ vanishes, the relationship $x_2 \\approx 3x_1$ becomes nearly deterministic, and $\\text{Corr}(x_1, x_2) \\rightarrow 1$. The test suite systematically decreases $\\sigma_u$ from $5.0$ to $10^{-6}$ to examine the effects of this increasing collinearity.\n\nThe OLS estimator for the coefficient vector is $\\widehat{\\beta} = (X^\\top X)^{-1}X^\\top Y$. Its theoretical variance-covariance matrix is $\\text{Var}(\\widehat{\\beta} | X) = \\sigma_\\varepsilon^2 (X^\\top X)^{-1}$. In practice, $\\sigma_\\varepsilon^2$ is unknown and is estimated from the sample residuals $e = Y - X\\widehat{\\beta}$. The estimator for the error variance is $\\widehat{\\sigma}^2 = \\frac{e^\\top e}{n-p}$. Thus, the estimated variance-covariance matrix of the coefficients is:\n$$\n\\widehat{\\text{Var}}(\\widehat{\\beta}) = \\widehat{\\sigma}^2 (X^\\top X)^{-1}\n$$\n\nWe are tasked with computing two diagnostics:\n$1$. The spectral condition number, $\\kappa$, of the matrix $X^\\top X$. It is defined as the ratio of the largest to the smallest eigenvalue of this matrix: $\\kappa(X^\\top X) = \\frac{\\lambda_{\\text{max}}(X^\\top X)}{\\lambda_{\\text{min}}(X^\\top X)}$. A large condition number signifies that $X^\\top X$ is ill-conditioned or nearly singular, a direct consequence of high multicollinearity. This numerical instability makes the inversion of $X^\\top X$ highly sensitive to small perturbations in the data.\n$2$. The sum of the marginal variances of the slope estimators, $V_{\\text{slopes}} = \\mathbb{V}[\\widehat{\\beta}_1] + \\mathbb{V}[\\widehat{\\beta}_2]$. These variances are the diagonal elements of the estimated variance-covariance matrix $\\widehat{\\text{Var}}(\\widehat{\\beta})$ corresponding to $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$. A primary consequence of multicollinearity is the inflation of these variances, which implies that the coefficient estimates become less precise and statistically less reliable.\n\nThe computational procedure for each value of $\\sigma_u$ is as follows:\n$1$. Set the random number generator seed to $2025+j$ for test case $j \\in \\{1,2,3,4\\}$.\n$2$. Generate a sample of size $n=800$ for $x_1$, $u$, and $\\varepsilon$ from their respective normal distributions.\n$3$. Construct the vectors $x_2$ and $y$ according to the data-generating process.\n$4$. Form the $800 \\times 3$ design matrix $X = [\\mathbf{1}, x_1, x_2]$.\n$5$. Compute the matrix $A = X^\\top X$.\n$6$. Calculate the eigenvalues of $A$ and compute the condition number $\\kappa = \\lambda_{\\text{max}}(A) / \\lambda_{\\text{min}}(A)$.\n$7$. Compute the OLS coefficient estimates $\\widehat{\\beta} = (X^\\top X)^{-1}X^\\top Y$. A numerically stable method, such as solving the system of linear equations $X^\\top X \\widehat{\\beta} = X^\\top Y$, is recommended.\n$8$. Calculate the vector of residuals $e = Y - X\\widehat{\\beta}$.\n$9$. Compute the estimated error variance $\\widehat{\\sigma}^2 = \\frac{e^\\top e}{n-3}$.\n$10$. Compute the inverse matrix $(X^\\top X)^{-1}$.\n$11$. Calculate the estimated variance-covariance matrix $\\widehat{V} = \\widehat{\\sigma}^2 (X^\\top X)^{-1}$.\n$12$. Extract the variances $\\widehat{\\mathbb{V}}[\\widehat{\\beta}_1] = \\widehat{V}_{22}$ and $\\widehat{\\mathbb{V}}[\\widehat{\\beta}_2] = \\widehat{V}_{33}$ (using $1$-based indexing for the matrix) and compute their sum, $V_{\\text{slopes}}$.\n$13$. Store the pair $(\\kappa, V_{\\text{slopes}})$ for the current test case.\n\nAs $\\sigma_u$ decreases, we expect a dramatic increase in both $\\kappa$ and $V_{\\text{slopes}}$, illustrating the progressive degradation of the OLS estimation due to intensifying multicollinearity. The final case with $\\sigma_u = 10^{-6}$ represents an extreme scenario where $X^\\top X$ is nearly singular, posing a significant numerical challenge.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing multicollinearity effects in OLS.\n\n    For different levels of induced collinearity between two regressors, this function\n    generates data, fits a multiple linear regression model, and computes two key\n    diagnostics: the spectral condition number of X'X and the sum of the variances\n    of the slope coefficient estimators.\n    \"\"\"\n\n    # Define fixed parameters from the problem statement\n    beta_0 = 0.5\n    beta_1 = 2.0\n    beta_2 = -1.5\n    sigma_eps_sq = 1.0\n    n = 800\n    \n    # Define the test suite for the standard deviation of u\n    test_suite_sigma_u = [5.0, 1.0, 0.1, 1e-6]\n    \n    # Base seed for the random number generator\n    base_seed = 2025\n    \n    results = []\n    \n    # Iterate through the test cases\n    for j, sigma_u in enumerate(test_suite_sigma_u):\n        # Set the seed for reproducibility for each test case j in {1,2,3,4}\n        # The loop index j is 0-based, so we use j+1.\n        seed = base_seed + (j + 1)\n        rng = np.random.default_rng(seed)\n        \n        # --- Step 1: Data Generation ---\n        \n        # Generate x_1i ~ N(0, 1)\n        x1 = rng.normal(loc=0.0, scale=1.0, size=n)\n        \n        # Generate u_i ~ N(0, sigma_u^2)\n        u = rng.normal(loc=0.0, scale=sigma_u, size=n)\n        \n        # Construct x_2i = 3 * x_1i + u_i to induce collinearity\n        x2 = 3.0 * x1 + u\n        \n        # Generate epsilon_i ~ N(0, 1.0)\n        epsilon = rng.normal(loc=0.0, scale=np.sqrt(sigma_eps_sq), size=n)\n        \n        # Construct the response variable y_i\n        y = beta_0 + beta_1 * x1 + beta_2 * x2 + epsilon\n        \n        # --- Step 2: OLS Estimation and Diagnostics ---\n        \n        # Construct the design matrix X (n x 3) with an intercept column\n        X = np.column_stack([np.ones(n), x1, x2])\n        \n        # Compute the matrix X'X\n        XTX = X.T @ X\n        \n        # Diagnostic 1: Spectral condition number of X'X\n        # Use eigvalsh for symmetric matrices, which returns sorted eigenvalues\n        eigenvalues = np.linalg.eigvalsh(XTX)\n        kappa = eigenvalues[-1] / eigenvalues[0]\n        \n        # Diagnostic 2: Sum of marginal variances of slope estimators\n        \n        # Compute OLS coefficients. Using np.linalg.solve is numerically\n        # more stable than inverting XTX directly to find beta_hat.\n        beta_hat = np.linalg.solve(XTX, X.T @ y)\n\n        # Calculate residuals\n        residuals = y - X @ beta_hat\n        \n        # Degrees of freedom for the variance estimator: n - (number of parameters)\n        dof = n - X.shape[1]  # n - 3 = 797\n        \n        # Estimate the error variance, sigma_hat^2\n        sigma_hat_sq = (residuals.T @ residuals) / dof\n        \n        # For the variance-covariance matrix, we need the inverse of X'X\n        # Use np.linalg.inv. For the extreme case, this might be sensitive,\n        # but modern libraries handle it with high precision.\n        XTX_inv = np.linalg.inv(XTX)\n        \n        # Estimated variance-covariance matrix of beta_hat\n        var_cov_beta_hat = sigma_hat_sq * XTX_inv\n        \n        # The sum of variances for the slope coefficients (beta_1 and beta_2)\n        # These are the diagonal elements with index 1 and 2 (0-based)\n        v_slopes = var_cov_beta_hat[1, 1] + var_cov_beta_hat[2, 2]\n        \n        # Append the computed diagnostics for this test case\n        results.extend([kappa, v_slopes])\n\n    # --- Step 3: Format and Print Output ---\n    \n    # Format each result to exactly six decimal places as a string\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    \n    # Print the final output in the required single-line format\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond multicollinearity, another critical assumption of the classical linear model is homoskedasticity—the idea that the variance of the errors is constant across observations. This practice explores what happens when this assumption fails, a common situation known as heteroskedasticity. Using a Monte Carlo simulation, you will discover why classical standard errors can be misleading in this context and validate the performance of heteroskedasticity-consistent (or 'robust') standard errors, a vital tool in the modern econometrician's toolkit .",
            "id": "2413193",
            "problem": "Consider the linear regression model with heteroskedastic disturbances. For each observation $i \\in \\{1,\\dots,n\\}$, let the data-generating process be\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i,\n$$\nwith the exogeneity condition $E[\\varepsilon_i \\mid X] = 0$ and a heteroskedastic variance structure\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma^2 \\cdot \\text{income}_i,\n$$\nwhere $X$ denotes the full regressor matrix including the intercept. The goal is to use Monte Carlo simulation to demonstrate that the classical Ordinary Least Squares (OLS) standard errors fail under heteroskedasticity, while Heteroskedasticity-Consistent (HC) standard errors maintain correct coverage asymptotically.\n\nYou must write a complete, runnable program that:\n- Generates synthetic data according to the description above.\n- Estimates the regression by OLS, computes both classical homoskedasticity-only standard errors and Heteroskedasticity-Consistent (HC) robust standard errors.\n- Constructs two-sided confidence intervals for $\\beta_1$ with nominal coverage $0.95$ using the standard normal critical value.\n- Repeats the experiment over $R$ independent replications and reports the empirical coverage for each standard error method (the fraction of confidence intervals that contain the true $\\beta_1$).\n\nUse the following fundamental base for design and justification:\n- The OLS estimator is defined as the minimizer of the sum of squared residuals. The residuals are the deviations between observed $y_i$ and the fitted values implied by the linear combination of regressors.\n- The classical homoskedasticity-only standard errors require the assumption $\\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^2 I$, and are inconsistent when the variance depends on regressors.\n- Heteroskedasticity-Consistent (HC) standard errors replace the incorrect homoskedastic residual covariance with an estimator that remains consistent when $\\operatorname{Var}(\\varepsilon \\mid X)$ is diagonal but not proportional to the identity matrix.\n\nData generation details for each replication:\n- Draw $\\text{income}_i$ independently from a Gamma distribution with shape parameter $k = 4$ and scale parameter $\\theta = 12$ (so $E[\\text{income}_i] = k \\theta = 48$). This choice ensures strictly positive $\\text{income}_i$ and realistic right-skewness.\n- Draw $\\text{educ}_i$ independently from a Normal distribution with mean $14$ and standard deviation $2$, then clip to the interval $[8, 22]$ to avoid implausible values.\n- Set the true parameter vector to $(\\beta_0,\\beta_1,\\beta_2) = (1, 0.8, 0.5)$.\n- For each $i$, draw $\\varepsilon_i$ from a Normal distribution with mean $0$ and variance $\\sigma^2 \\cdot \\text{income}_i$.\n- Construct $y_i$ accordingly.\n\nConfidence intervals:\n- For each replication, after estimating the model by OLS, compute two confidence intervals for $\\beta_1$: one using the classical homoskedasticity-only standard error and one using the Heteroskedasticity-Consistent (HC) robust standard error. In both cases, use the standard normal critical value corresponding to a nominal coverage of $0.95$ (i.e., the two-sided critical value $z_{0.975}$).\n- Record whether each interval contains the true $\\beta_1$.\n\nTest suite:\nRun the Monte Carlo simulation for the following parameter sets $(n, \\sigma^2, R)$:\n- Case A (happy path): $(n, \\sigma^2, R) = (500, 1.0, 1000)$.\n- Case B (small sample): $(n, \\sigma^2, R) = (60, 1.0, 1000)$.\n- Case C (more severe heteroskedasticity): $(n, \\sigma^2, R) = (500, 3.0, 1000)$.\n- Case D (milder heteroskedasticity): $(n, \\sigma^2, R) = (500, 0.2, 1000)$.\n\nRandomness and reproducibility:\n- Use a fixed seed for the pseudo-random number generator equal to $20240519$ so that results are exactly reproducible.\n\nProgram output requirements:\n- For each test case, report three items: the empirical coverage using homoskedasticity-only standard errors (a float), the empirical coverage using HC robust standard errors (a float), and a boolean indicating whether the robust coverage is closer to the nominal $0.95$ than the homoskedastic coverage (i.e., whether $|\\text{robust} - 0.95| < |\\text{classical} - 0.95|$).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one inner list per test case, for example:\n\"[[cA_classical,cA_robust,bA],[cB_classical,cB_robust,bB],[cC_classical,cC_robust,bC],[cD_classical,cD_robust,bD]]\"\n- All coverages must be reported as decimals (not percentages).\n\nAngle units are not applicable. No physical units are needed.\n\nYour program must be complete and runnable as is, without any user input or external files, and must strictly adhere to the specified output format.",
            "solution": "The problem as stated is valid. It presents a well-posed and standard econometric exercise for demonstrating the consequences of heteroskedasticity on the inference produced by the Ordinary Least Squares (OLS) estimator. The data-generating process is fully specified, the theoretical concepts are sound, and the objectives are clear and quantifiable. The task is to perform a Monte Carlo simulation, which is a fundamental technique in computational statistics and econometrics for evaluating the finite-sample properties of estimators and statistical tests.\n\nWe begin by formalizing the specified linear regression model. For each observation $i=1, \\dots, n$, the model is:\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i\n$$\nIn matrix notation, this can be written as $y = X\\beta + \\varepsilon$, where $y$ is the $n \\times 1$ vector of observations on the dependent variable, $X$ is the $n \\times p$ matrix of regressors (with $p=3$), $\\beta$ is the $p \\times 1$ vector of true parameters, and $\\varepsilon$ is the $n \\times 1$ vector of disturbances. The regressor matrix for observation $i$ is $x_i^T = [1, \\text{income}_i, \\text{educ}_i]$.\n\nThe problem specifies two key assumptions about the error term $\\varepsilon$:\n$1$. The exogeneity condition: $E[\\varepsilon \\mid X] = 0$. This ensures that the OLS estimator is unbiased.\n$2$. A specific form of heteroskedasticity: $\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma_i^2 = \\sigma^2 \\cdot \\text{income}_i$. This means the variance of the error term is not constant but depends on one of the regressors. The full covariance matrix of the error vector is $\\Omega = \\operatorname{Var}(\\varepsilon \\mid X) = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$, which is not proportional to the identity matrix $I_n$.\n\nThe OLS estimator for $\\beta$ is derived by minimizing the sum of squared residuals, $S(\\beta) = (y - X\\beta)^T(y - X\\beta)$. The well-known solution is:\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y\n$$\nUnder the exogeneity assumption, this estimator is unbiased, i.e., $E[\\hat{\\beta}_{OLS} \\mid X] = \\beta$. However, for valid statistical inference (constructing confidence intervals and performing hypothesis tests), we need a consistent estimator for the covariance matrix of $\\hat{\\beta}_{OLS}$. The true covariance matrix of the OLS estimator, conditional on $X$, is:\n$$\n\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid X) = (X^T X)^{-1} X^T \\Omega X (X^T X)^{-1}\n$$\n\nThe simulation compares two estimators for this covariance matrix.\n\nFirst, the classical, homoskedasticity-only variance estimator. This estimator incorrectly assumes that $\\Omega = \\sigma^2 I_n$. It is given by:\n$$\n\\widehat{\\operatorname{Var}}_{classical}(\\hat{\\beta}) = \\hat{s}^2 (X^T X)^{-1}\n$$\nwhere $\\hat{s}^2 = \\frac{1}{n-p} \\sum_{i=1}^n \\hat{\\varepsilon}_i^2$ is the standard unbiased estimator of the error variance, and $\\hat{\\varepsilon}_i = y_i - x_i^T \\hat{\\beta}$ are the OLS residuals. When heteroskedasticity is present, this formula is incorrect, and the resulting standard errors are biased and inconsistent. Confidence intervals constructed using these standard errors will not have the correct nominal coverage probability.\n\nSecond, the Heteroskedasticity-Consistent (HC) variance estimator, specifically the HC0 estimator proposed by White (1980). This estimator does not require the homoskedasticity assumption and provides a consistent estimate of the true covariance matrix. It replaces the unknown $\\Omega$ in the true formula with a diagonal matrix of squared OLS residuals, $\\hat{\\Omega} = \\operatorname{diag}(\\hat{\\varepsilon}_1^2, \\dots, \\hat{\\varepsilon}_n^2)$. The HC0 estimator is:\n$$\n\\widehat{\\operatorname{Var}}_{HC0}(\\hat{\\beta}) = (X^T X)^{-1} (X^T \\hat{\\Omega} X) (X^T X)^{-1}\n$$\nThe term $X^T \\hat{\\Omega} X$ is often called the \"sandwich meat\". The standard errors derived from this estimator are \"robust\" to heteroskedasticity of an unknown form. Asymptotically, confidence intervals based on HC standard errors will achieve the correct nominal coverage.\n\nThe simulation will proceed as follows for each test case $(n, \\sigma^2, R)$:\n$1$. The process is repeated for $R$ replications.\n$2$. In each replication, a synthetic dataset $(y, X)$ of size $n$ is generated according to the specified data-generating process with true parameters $(\\beta_0, \\beta_1, \\beta_2) = (1, 0.8, 0.5)$.\n$3$. The model is estimated using OLS to obtain $\\hat{\\beta}$ and the residuals $\\hat{\\varepsilon}$.\n$4$. The standard error for $\\hat{\\beta}_1$ is computed using both the classical formula and the HC0 robust formula. This involves taking the square root of the second diagonal element (index $1$) of the respective estimated covariance matrices.\n$5$. Two $95\\%$ confidence intervals for $\\beta_1$ are constructed: $[\\hat{\\beta}_1 \\pm z_{0.975} \\cdot SE(\\hat{\\beta}_1)]$, where $z_{0.975}$ is the critical value from the standard normal distribution ($z_{0.975} \\approx 1.95996$) and $SE(\\hat{\\beta}_1)$ is the corresponding standard error estimate.\n$6$. For each interval, we check if it contains the true value $\\beta_1 = 0.8$.\n$7$. After all replications, the empirical coverage for each method is calculated as the fraction of intervals that contained the true parameter. These coverages are compared to the nominal level of $0.95$.\nThe expectation is that the empirical coverage of the robust confidence intervals will be close to $0.95$, while the coverage of the classical intervals will deviate significantly, demonstrating their failure under heteroskedasticity.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_simulation(n, sigma2, R, true_beta, rng):\n    \"\"\"\n    Runs a single Monte Carlo simulation for a given parameter set.\n    \"\"\"\n    beta1_true = true_beta[1]\n    num_params = len(true_beta)\n    \n    # Pre-calculate the critical value for 95% CI\n    # z_crit = 1.959964 is norm.ppf(0.975)\n    z_crit = norm.ppf(1 - 0.05 / 2.0)\n\n    # Counters for coverage\n    classical_hits = 0\n    robust_hits = 0\n\n    for _ in range(R):\n        # 1. Generate data\n        income = rng.gamma(shape=4, scale=12, size=n)\n        educ = rng.normal(loc=14, scale=2, size=n)\n        educ = np.clip(educ, 8, 22)\n        \n        X = np.column_stack((np.ones(n), income, educ))\n        \n        # Generate heteroskedastic errors\n        error_variances = sigma2 * income\n        epsilon = rng.normal(loc=0, scale=np.sqrt(error_variances))\n        \n        y = X @ true_beta + epsilon\n\n        # 2. OLS Estimation\n        try:\n            XTX = X.T @ X\n            XTX_inv = np.linalg.inv(XTX)\n            beta_hat = XTX_inv @ X.T @ y\n        except np.linalg.LinAlgError:\n            continue # Skip iteration if X is singular\n\n        beta1_hat = beta_hat[1]\n        residuals = y - X @ beta_hat\n\n        # 3. Calculate Classical (Homoskedastic) Standard Errors\n        s2_classical = np.sum(residuals**2) / (n - num_params)\n        var_beta_classical = s2_classical * XTX_inv\n        se_beta1_classical = np.sqrt(var_beta_classical[1, 1])\n\n        # 4. Calculate HC0 (Robust) Standard Errors\n        # Efficiently compute the sandwich meat: X^T * diag(res^2) * X\n        sandwich_meat = X.T @ (X * (residuals**2)[:, np.newaxis])\n        var_beta_hc0 = XTX_inv @ sandwich_meat @ XTX_inv\n        \n        # Ensure variance is non-negative due to potential floating point errors\n        var_beta1_hc0 = var_beta_hc0[1, 1]\n        if var_beta1_hc0  0:\n            var_beta1_hc0 = 0\n        se_beta1_hc0 = np.sqrt(var_beta1_hc0)\n\n        # 5. Construct Confidence Intervals and check coverage\n        # Classical CI\n        ci_classical_lower = beta1_hat - z_crit * se_beta1_classical\n        ci_classical_upper = beta1_hat + z_crit * se_beta1_classical\n        if ci_classical_lower = beta1_true = ci_classical_upper:\n            classical_hits += 1\n\n        # Robust CI\n        ci_robust_lower = beta1_hat - z_crit * se_beta1_hc0\n        ci_robust_upper = beta1_hat + z_crit * se_beta1_hc0\n        if ci_robust_lower = beta1_true = ci_robust_upper:\n            robust_hits += 1\n\n    # 6. Calculate empirical coverages\n    coverage_classical = classical_hits / R\n    coverage_robust = robust_hits / R\n\n    # 7. Compare performance\n    is_robust_better = abs(coverage_robust - 0.95)  abs(coverage_classical - 0.95)\n\n    return [coverage_classical, coverage_robust, is_robust_better]\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (n, sigma^2, R) = (500, 1.0, 1000)\n        (500, 1.0, 1000),\n        # Case B: (n, sigma^2, R) = (60, 1.0, 1000)\n        (60, 1.0, 1000),\n        # Case C: (n, sigma^2, R) = (500, 3.0, 1000)\n        (500, 3.0, 1000),\n        # Case D: (n, sigma^2, R) = (500, 0.2, 1000)\n        (500, 0.2, 1000),\n    ]\n\n    true_beta = np.array([1.0, 0.8, 0.5])\n    seed = 20240519\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for n, sigma2, R in test_cases:\n        result = run_simulation(n, sigma2, R, true_beta, rng)\n        results.append(result)\n\n    # Format the output string exactly as required\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        cov_c, cov_r, is_better = res\n        # Format boolean as lowercase 'true'/'false'\n        bool_str = 'true' if is_better else 'false'\n        output_str += f\"[{cov_c},{cov_r},{bool_str}]\"\n        if i  len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}