{
    "hands_on_practices": [
        {
            "introduction": "离散时间随机过程为理解随时间演化的系统提供了强大的框架。本练习将通过一个经典的金融应用——信用评级动态模型，来实践马尔可夫链的核心概念。你将学习如何根据观测到的评级历史数据，估计状态之间的转移概率矩阵，并进一步计算出系统在长期会趋于的稳定状态，即平稳分布。这个练习不仅能巩固你对马尔可夫链理论的理解，还能让你掌握从数据中构建和分析随机模型的基本技能。",
            "id": "2388997",
            "problem": "考虑一个离散时间的信用评级系统，该系统被建模为在有限状态空间上的一阶马尔可夫链。状态空间是有序的 $S=\\{\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{Default}\\}$，我们分别将其索引为 $0,1,2,3,4$。你观察到一个按时间排序的评级序列 $\\{X_t\\}_{t=0}^{T}$，其取值于 $S$ 中。令 $N_{ij}$ 表示在观测序列中从状态 $i$ 到状态 $j$ 的一步转移计数，即 $N_{ij}=\\#\\{t \\in \\{0,\\ldots,T-1\\}\\,:\\,X_t=i,\\,X_{t+1}=j\\}$。\n\n你必须使用伪计数 $\\alpha=1$ 的拉普拉斯平滑最大似然方法来估计 $5\\times 5$ 的转移概率矩阵 $P=\\left[P_{ij}\\right]$。具体而言，对于每个 $i\\in\\{0,1,2,3,4\\}$ 和 $j\\in\\{0,1,2,3,4\\}$，定义\n$$\n\\widehat{P}_{ij}=\\frac{N_{ij}+\\alpha}{\\sum_{k=0}^{4} N_{ik}+5\\alpha},\n$$\n其中 $\\alpha=1$。这将产生一个所有元素都严格为正的行随机矩阵。\n\n计算长期（平稳）分布 $\\pi$，它是一个满足以下条件的唯一非负元素行向量：\n$$\n\\pi \\widehat{P}=\\pi,\\quad \\sum_{i=0}^{4}\\pi_i=1.\n$$\n以指定的状态顺序 $[\\pi_{\\text{AAA}},\\pi_{\\text{AA}},\\pi_{\\text{A}},\\pi_{\\text{BBB}},\\pi_{\\text{Default}}]$，将平稳分布报告为一个小数列表，每个数四舍五入到六位小数。\n\n使用以下观测序列的测试套件（每个都是 $S$ 中的标签列表）：\n\n- 测试用例 1（包含升级和降级的一般情况）：\n  $[\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{Default},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB}]$。\n\n- 测试用例 2（信息最少的边界情况）：\n  $[\\text{A},\\text{A}]$。\n\n- 测试用例 3（评级集中于高级别的边缘情况）：\n  $[\\text{AAA},\\text{AA},\\text{AAA},\\text{AAA},\\text{AA},\\text{A},\\text{AAA},\\text{AAA},\\text{AAA},\\text{AA},\\text{A},\\text{AAA},\\text{AAA},\\text{AAA},\\text{AAA},\\text{AAA}]$。\n\n对于每个测试用例，你必须：\n1. 对所有元素使用 $\\alpha=1$ 构建平滑估计量 $\\widehat{P}$。\n2. 计算满足 $\\pi \\widehat{P}=\\pi$ 和 $\\sum_i \\pi_i=1$ 的平稳分布 $\\pi$。\n3. 将 $\\pi$ 的每个分量四舍五入到六位小数。\n\n最终输出格式：你的程序应该生成单行输出，其中包含所有三个测试用例的结果。结果是一个用方括号括起来的逗号分隔列表，其中每个元素本身是按指定状态顺序表示的平稳分布的列表。例如，一个可接受的格式是\n$[\\,[\\pi^{(1)}_{\\text{AAA}},\\ldots,\\pi^{(1)}_{\\text{Default}}],[\\pi^{(2)}_{\\text{AAA}},\\ldots,\\pi^{(2)}_{\\text{Default}}],[\\pi^{(3)}_{\\text{AAA}},\\ldots,\\pi^{(3)}_{\\text{Default}}]\\,]$，\n每个数字都四舍五入到六位小数。此问题中没有物理单位、角度或百分比；所有输出都必须是小数。",
            "solution": "所提出的问题是计算统计学和随机过程中的一个有效练习。它具有科学依据，是良定的和客观的。我们将给出完整的解法。\n\n该问题要求我们估计一个离散时间一阶马尔可夫链的平稳分布。状态空间为 $S=\\{\\text{AAA}, \\text{AA}, \\text{A}, \\text{BBB}, \\text{Default}\\}$，我们将分别用整数 $i \\in \\{0, 1, 2, 3, 4\\}$ 对其进行索引。给定一个观测时间序列 $\\{X_t\\}_{t=0}^{T}$。\n\n首先，我们必须估计转移概率矩阵，记为 $P = [P_{ij}]$，其中 $P_{ij} = \\mathbb{P}(X_{t+1}=j | X_t=i)$。问题指定使用拉普拉斯平滑最大似然估计量。这是一种贝叶斯估计方法，其中对转移矩阵的行施加狄利克雷先验。对于给定的行 $i$，概率向量 $[P_{i0}, \\dots, P_{i4}]$ 的先验是集中参数为 $\\alpha$ 的对称狄利克雷分布。\n\n令 $N_{ij}$ 为序列中从状态 $i$ 到状态 $j$ 的观测到的一步转移次数。平滑估计量 $\\widehat{P}_{ij}$ 的公式为：\n$$\n\\widehat{P}_{ij} = \\frac{N_{ij} + \\alpha}{\\sum_{k=0}^{4} N_{ik} + |S|\\alpha}\n$$\n其中 $|S|$ 是状态数，为 $5$。问题指定伪计数 $\\alpha=1$。令 $N_i = \\sum_{k=0}^{4} N_{ik}$ 为观测到的从状态 $i$ 开始的总转移次数。估计量简化为：\n$$\n\\widehat{P}_{ij} = \\frac{N_{ij} + 1}{N_i + 5}\n$$\n此过程确保估计的转移矩阵 $\\widehat{P}$ 中的每个元素都严格为正，即对于所有 $i, j \\in S$，$\\widehat{P}_{ij} > 0$。转移矩阵所有元素均为正的马尔可夫链称为正则马尔可夫链。有限状态空间上的正则马尔可夫链的一个关键属性是它是遍历的，并拥有唯一的平稳分布。\n\n平稳分布是一个行向量 $\\pi = [\\pi_0, \\pi_1, \\pi_2, \\pi_3, \\pi_4]$，满足两个条件：\n$1.$ $\\pi \\widehat{P} = \\pi$\n$2.$ $\\sum_{i=0}^{4} \\pi_i = 1$\n\n第一个条件 $\\pi \\widehat{P} = \\pi$ 意味着 $\\pi$ 是矩阵 $\\widehat{P}$ 的一个左特征向量，其对应的特征值为 $\\lambda=1$。这可以重写为一个齐次线性方程组：\n$$\n\\pi (\\widehat{P} - I) = \\mathbf{0}\n$$\n其中 $I$ 是 $5 \\times 5$ 的单位矩阵，$\\mathbf{0}$ 是一个零行向量。其转置形式为 $(\\widehat{P}^T - I^T) \\pi^T = \\mathbf{0}^T$，或者更简单地写为 $(\\widehat{P}^T - I) \\pi^T = \\mathbf{0}$。\n\n由于任何行随机矩阵都有一个特征值 $\\lambda=1$，所以矩阵 $(\\widehat{P}^T - I)$ 是奇异的，其零空间是非平凡的。对于正则马尔可夫链，对应于 $\\lambda=1$ 的特征空间维度为 $1$。因此，方程组 $(\\widehat{P}^T - I) \\pi^T = \\mathbf{0}$ 的解空间维度为 $1$。为了找到唯一的平稳分布 $\\pi$，我们必须施加归一化条件 $\\sum_{i=0}^{4} \\pi_i = 1$。\n\n在数值上，我们可以通过构建一个线性方程组来求解。我们从 $(\\widehat{P}^T - I) \\pi^T = \\mathbf{0}$ 中取出前 $|S|-1=4$ 个线性方程，并加上归一化方程。设 $A$ 是一个矩阵，其前 $4$ 行是 $(\\widehat{P}^T - I)$ 的前 $4$ 行，最后一行是全1向量。设 $b$ 是一个列向量，其前 $4$ 个元素为零，最后一个元素为一。需要求解的方程组是：\n$$\nA \\pi^T = b\n$$\n解由 $\\pi^T = A^{-1}b$ 给出。解的存在性和唯一性由 $\\widehat{P}$ 的正则性保证。\n\n我们将对三个测试用例中的每一个应用此过程。\n\n**步骤 1：状态映射和转移计数**\n对于每个测试序列，我们将字符串标签映射到整数索引 $\\{0, 1, 2, 3, 4\\}$。然后，我们遍历长度为 $T+1$ 的序列来计算 $T$ 次转移，并填充 $5 \\times 5$ 的计数矩阵 $N = [N_{ij}]$。\n\n**步骤 2：转移矩阵估计**\n使用计数矩阵 $N$ 和 $\\alpha=1$，我们通过公式 $\\widehat{P}_{ij} = (N_{ij} + 1) / (N_i + 5)$ 计算平滑转移矩阵 $\\widehat{P}$。\n\n**步骤 3：平稳分布计算**\n我们如上所述从 $\\widehat{P}^T$ 构建矩阵 $A$ 和向量 $b = [0, 0, 0, 0, 1]^T$。然后我们使用标准线性代数求解器求解线性系统 $A \\pi^T = b$ 以得到 $\\pi^T$。\n\n**步骤 4：格式化**\n将所得向量 $\\pi$ 的分量按要求四舍五入到六位小数。最终输出是一个列表，其中包含每个测试用例的四舍五入后的平稳分布向量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    state_map = {'AAA': 0, 'AA': 1, 'A': 2, 'BBB': 3, 'Default': 4}\n    num_states = len(state_map)\n    alpha = 1.0\n\n    test_cases = [\n        ['BBB','A','AA','AAA','AA','A','BBB','A','AA','AAA','AA','A','BBB','A','AA','AAA','AA','A','BBB','Default','A','AA','AAA','AA','A','BBB','A','AA','AAA','AA','A','BBB'],\n        ['A','A'],\n        ['AAA','AA','AAA','AAA','AA','A','AAA','AAA','AAA','AA','A','AAA','AAA','AAA','AAA','AAA']\n    ]\n\n    all_results = []\n\n    for sequence in test_cases:\n        # Step 1: Count transitions\n        counts = np.zeros((num_states, num_states), dtype=int)\n        int_sequence = [state_map[s] for s in sequence]\n        \n        for i in range(len(int_sequence) - 1):\n            from_state = int_sequence[i]\n            to_state = int_sequence[i+1]\n            counts[from_state, to_state] += 1\n\n        # Step 2: Estimate smoothed transition matrix P_hat\n        p_hat = np.zeros((num_states, num_states), dtype=float)\n        row_totals = np.sum(counts, axis=1)\n        \n        for i in range(num_states):\n            denominator = row_totals[i] + num_states * alpha\n            for j in range(num_states):\n                numerator = counts[i, j] + alpha\n                p_hat[i, j] = numerator / denominator\n\n        # Step 3: Compute the stationary distribution pi\n        # We need to solve pi * P_hat = pi, or pi * (P_hat - I) = 0,\n        # which is equivalent to (P_hat^T - I^T) * pi^T = 0^T.\n        # Let A = P_hat^T - I. We solve for the null space of A.\n        # We replace the last equation with the normalization condition sum(pi) = 1.\n        \n        A = (p_hat.T - np.identity(num_states))\n        A[-1, :] = 1.0  # Last row is for sum(pi_i) = 1\n        \n        b = np.zeros(num_states)\n        b[-1] = 1.0  # Corresponds to sum(pi_i) = 1\n        \n        try:\n            # Solve the linear system A * pi^T = b\n            pi = np.linalg.solve(A, b)\n            \n            # Ensure non-negativity and re-normalize for robustness\n            pi[pi  0] = 0\n            pi /= np.sum(pi)\n\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix if something unexpected happens\n            # For a regular P_hat, this shouldn't be reached.\n            # We can use eigenvector method as a backup.\n            # Find the right eigenvector of P_hat.T for eigenvalue 1\n            eigenvalues, eigenvectors = np.linalg.eig(p_hat.T)\n            # Find the eigenvector corresponding to eigenvalue 1\n            idx = np.argmin(np.abs(eigenvalues - 1.0))\n            pi = np.real(eigenvectors[:, idx])\n            # Normalize to get the probability distribution\n            pi = pi / np.sum(pi)\n\n        # Step 4: Round and format the result\n        rounded_pi = np.round(pi, 6).tolist()\n        all_results.append(rounded_pi)\n\n    # Final print statement in the exact required format.\n    # The default str() representation of a list of lists works. e.g., [[...], [...]]\n    print(str(all_results).replace(\" \", \"\"))\n\n\nsolve()\n```"
        },
        {
            "introduction": "在许多现实场景中，我们关心的核心变量（“状态”）是无法直接观测的，只能通过带有噪声的测量数据来推断。卡尔曼滤波器为这类问题提供了最优的线性估计算法。在这个练习中，你将扮演体育数据分析师的角色，应用卡尔曼滤波器来追踪一名篮球运动员潜在的“真实”投篮能力。通过处理每场比赛的得分数据（这些数据本身是有噪声的），你将学习如何动态地更新你对运动员能力的估计，并理解模型是如何优雅地处理数据质量差异的。",
            "id": "2389012",
            "problem": "考虑一个单一隐藏状态，它代表一名篮球运动员在一系列比赛中潜在的投篮能力。在每个离散时间步 $t \\in \\{1,2,\\dots,T\\}$，该运动员投篮 $n_t$ 次，命中 $m_t$ 次。将观测到的投篮命中率定义为 $y_t = m_t / n_t$（当 $n_t  0$ 时），并在 $n_t = 0$ 时将 $y_t$ 视为缺失值。使用以下线性高斯状态空间模型来模拟隐藏能力和观测值：\n- 隐藏状态动态：$ \\theta_t = \\mu + \\phi \\left(\\theta_{t-1} - \\mu\\right) + w_t $，其中 $w_t \\sim \\mathcal{N}(0,q)$。\n- 观测方程（当 $n_t  0$ 时）：$ y_t = \\theta_t + v_t $，其中 $v_t \\sim \\mathcal{N}(0, R_t)$。\n\n假设 $R_t$ 是已知的，并透过一个受二项式抽样启发的样本均值方差近似法来决定，其与 $n_t$ 的关系为：$ R_t = \\bar{p}(1-\\bar{p}) / n_t $，其中 $\\bar{p}$ 是一个固定的参考概率。当 $n_t = 0$ 时，将观测值视为缺失，并且只执行状态预测步骤（不进行更新）。所有概率和方差必须表示为小数（例如，写成 $0.45$ 而不是 $45\\%$）。\n\n您将获得以下固定参数：\n- 长期均值：$\\mu = 0.45$。\n- 自回归系数：$\\phi = 0.90$。\n- 状态创新方差：$q = 0.0005$。\n- 用于观测方差的参考概率：$\\bar{p} = 0.45$（在 $R_t$ 中使用此值）。\n- 初始状态的先验：$\\theta_0 \\sim \\mathcal{N}(m_0, P_0)$，其中 $m_0 = 0.45$ 且 $P_0 = 0.01$。\n\n从先验 $(m_0, P_0)$ 开始，实现一个递归滤波程序，在每个时间 $t$，计算状态及其方差的一步预测，然后（如果 $n_t  0$）使用观测值 $y_t$ 和上述线性高斯模型来更新它们。如果 $n_t = 0$，则跳过更新步骤，并将预测值作为该时期的滤波后状态。\n\n您的任务是编写一个完整的程序，该程序：\n1. 针对上述指定的模型，实现所述的时变方差卡尔曼滤波器。\n2. 对以下每个测试套组的比赛运行该滤波器（每个套组都是一系列 $(m_t, n_t)$ 配对）：\n   - 测试案例 A（中等尝试次数，结果多样）：\n     - $[(5,11), (4,10), (7,12), (6,14), (2,6), (8,15), (3,8), (6,12), (7,16), (5,9)]$。\n   - 测试案例 B（高尝试次数，结果相对稳定）：\n     - $[(9,20), (10,22), (8,18), (11,24), (12,25), (9,19), (10,21), (12,26), (11,24), (13,28)]$。\n   - 测试案例 C（部分比赛尝试次数为零；将这些视为缺失观测）：\n     - $[(0,0), (3,5), (0,0), (4,4), (0,0), (2,10), (0,0), (5,10)]$。\n   - 测试案例 D（结果和尝试次数不稳定）：\n     - $[(1,2), (0,5), (7,10), (1,12), (9,10), (0,3), (6,15), (2,2), (0,8), (10,12)]$。\n3. 对于每个测试案例，在处理完序列中的最后一场比赛后，输出最终的滤波后验平均值 $\\hat{\\theta}_T$ 和方差 $P_T$。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含以方括号括住的逗号分隔列表形式的结果。每个测试案例的结果本身必须是一个双元素列表 $[\\hat{\\theta}_T, P_T]$，且两个值都四舍五入到恰好六位小。例如，整体输出应如下所示：\n- $[[\\hat{\\theta}_T^{(A)}, P_T^{(A)}],[\\hat{\\theta}_T^{(B)}, P_T^{(B)}],[\\hat{\\theta}_T^{(C)}, P_T^{(C)}],[\\hat{\\theta}_T^{(D)}, P_T^{(D)}]]$，\n以单行印出。\n\n角度单位不适用。百分比必须表示为小数，而不是使用百分比符号。此问题中没有物理单位。",
            "solution": "此问题要求实现卡尔曼滤波器，这是一种递归算法，用于从一系列带有噪声的测量中估计线性动力系统的隐藏状态。该系统将篮球运动员的投篮能力 $\\theta_t$ 建模为一个随时间演变的潜在状态。\n\n首先，我们在线性高斯状态空间模型的框架下将问题形式化。该模型由两个方程组成：一个状态转移方程和一个观测方程。\n\n系统的状态是运动员在时间 $t$ 的投篮能力，用标量 $\\theta_t$ 表示。此状态的演变由一个一阶自回归过程 AR($1$) 描述，该过程会向长期均值 $\\mu$ 进行均值回归。\n\n状态方程：\n隐藏状态动态由下式给出：\n$$ \\theta_t = \\mu + \\phi \\left(\\theta_{t-1} - \\mu\\right) + w_t $$\n这可以重新排列成标准的线性形式：\n$$ \\theta_t = \\phi \\theta_{t-1} + (1 - \\phi)\\mu + w_t $$\n其中：\n- $\\theta_t$ 是时间 $t$ 的状态。\n- $\\phi$ 是自回归系数，决定状态的持续性。给定值为 $\\phi=0.90$。\n- $\\mu$ 是过程的长期均值，给定值为 $\\mu=0.45$。\n- $w_t$ 是过程噪声，假设为白噪声过程，满足 $w_t \\sim \\mathcal{N}(0, q)$，其中 $q$ 是状态创新方差，给定值为 $q=0.0005$。\n\n观测方程：\n时间 $t$ 的观测值是运动员的观测投篮命中率 $y_t = m_t / n_t$，其中 $m_t$ 是 $n_t$ 次尝试中命中的次数。此观测值仅在 $n_t > 0$ 时可用。观测值被建模为对真实潜在能力 $\\theta_t$ 的带有噪声的测量。\n$$ y_t = \\theta_t + v_t $$\n其中：\n- $y_t$ 是时间 $t$ 的观测值。\n- $v_t$ 是测量噪声，假设为白噪声过程，满足 $v_t \\sim \\mathcal{N}(0, R_t)$。方差 $R_t$ 是随时间变化的。\n\n测量噪声方差 $R_t$ 是基于二项分布样本比例的方差来近似的。给定 $n_t$ 次试验，样本比例 $y_t$ 的方差约为 $p(1-p)/n_t$。问题指定在此计算中使用一个固定的参考概率 $\\bar{p}=0.45$：\n$$ R_t = \\frac{\\bar{p}(1 - \\bar{p})}{n_t} = \\frac{0.45(1 - 0.45)}{n_t} = \\frac{0.2475}{n_t} $$\n这种形式使得 $R_t$ 是时变的，因为它取决于每场比赛的投篮次数 $n_t$。当 $n_t$ 很大时，$R_t$ 很小，反映了对观测值的更高信赖度。\n\n卡尔曼滤波器为估计状态的后验分布 $p(\\theta_t | y_{1:t})$ 提供了一个递归解。由于模型是线性和高斯的，这个后验分布也是高斯的，并且可以完全由其均值 $\\hat{\\theta}_{t|t}$ 和方差 $P_{t|t}$ 来表征。\n\n滤波过程从初始状态 $\\theta_0 \\sim \\mathcal{N}(m_0, P_0)$ 的先验分布开始，给定参数为 $m_0 = 0.45$ 和 $P_0 = 0.01$。在每个时间步 $t=1, 2, \\dots, T$，算法执行两个步骤：预测步骤和更新步骤。\n\n设时间 $t-1$ 的滤波后验分布为 $\\mathcal{N}(\\hat{\\theta}_{t-1|t-1}, P_{t-1|t-1})$。\n\n步骤 1：预测（时间更新）\n在此步骤中，我们根据截至时间 $t-1$ 的所有信息来预测时间 $t$ 的状态分布。计算预测（先验）均值 $\\hat{\\theta}_{t|t-1}$ 和方差 $P_{t|t-1}$。\n\n对状态方程取期望值：\n$$ \\hat{\\theta}_{t|t-1} = \\mathbb{E}[\\phi \\theta_{t-1} + (1 - \\phi)\\mu + w_t | y_{1:t-1}] = \\phi \\hat{\\theta}_{t-1|t-1} + (1 - \\phi)\\mu $$\n预测误差的方差是：\n$$ P_{t|t-1} = \\text{Var}(\\theta_t - \\hat{\\theta}_{t|t-1}) = \\text{Var}(\\phi(\\theta_{t-1} - \\hat{\\theta}_{t-1|t-1}) + w_t) $$\n由于误差 $(\\theta_{t-1} - \\hat{\\theta}_{t-1|t-1})$ 与过程噪声 $w_t$ 不相关，方差相加：\n$$ P_{t|t-1} = \\phi^2 \\text{Var}(\\theta_{t-1} - \\hat{\\theta}_{t-1|t-1}) + \\text{Var}(w_t) = \\phi^2 P_{t-1|t-1} + q $$\n\n步骤 2：更新（测量更新）\n此步骤使用时间 $t$ 的新观测值 $y_t$ 来精化预测。这仅在有观测值可用时（即 $n_t > 0$）执行。\n\n首先，我们计算新息（innovation），即实际观测值 $y_t$ 与其预测值之间的差异：\n$$ \\tilde{y}_t = y_t - \\mathbb{E}[y_t | y_{1:t-1}] = y_t - \\mathbb{E}[\\theta_t + v_t | y_{1:t-1}] = y_t - \\hat{\\theta}_{t|t-1} $$\n新息的方差，或称新息协方差，是：\n$$ S_t = \\text{Var}(\\tilde{y}_t) = \\text{Var}((\\theta_t - \\hat{\\theta}_{t|t-1}) + v_t) = P_{t|t-1} + R_t $$\n最佳卡尔曼增益 $K_t$ 决定了基于新息对预测进行多大程度的调整。它的计算旨在最小化后验误差方差：\n$$ K_t = \\frac{\\text{Cov}(\\theta_t, \\tilde{y}_t)}{\\text{Var}(\\tilde{y}_t)} = \\frac{\\text{Cov}(\\theta_t, \\theta_t - \\hat{\\theta}_{t|t-1} + v_t)}{S_t} = \\frac{P_{t|t-1}}{S_t} = \\frac{P_{t|t-1}}{P_{t|t-1} + R_t} $$\n更新后的（后验）状态均值是预测均值和观测值的加权平均：\n$$ \\hat{\\theta}_{t|t} = \\hat{\\theta}_{t|t-1} + K_t \\tilde{y}_t = \\hat{\\theta}_{t|t-1} + K_t (y_t - \\hat{\\theta}_{t|t-1}) $$\n更新后的（后验）误差方差是：\n$$ P_{t|t} = (1 - K_t) P_{t|t-1} $$\n\n处理缺失观测：\n如果 $n_t = 0$，则观测值 $y_t$ 缺失。在这种情况下，无法执行更新。对时间 $t$ 状态的最佳估计就是上一步的预测。因此，时间 $t$ 的后验被设定为等于其先验：\n$$ \\hat{\\theta}_{t|t} = \\hat{\\theta}_{t|t-1} $$\n$$ P_{t|t} = P_{t|t-1} $$\n\n对每个测试案例，整体算法的实现如下：\n1. 用先验均值 $\\hat{\\theta}_{0|0} = m_0 = 0.45$ 和方差 $P_{0|0} = P_0 = 0.01$ 初始化滤波器。\n2. 对于每个时间步 $t=1, \\dots, T$：\n   a. 执行预测步骤以计算 $\\hat{\\theta}_{t|t-1}$ 和 $P_{t|t-1}$。\n   b. 检查是否 $n_t > 0$：\n      i. 如果为真，计算 $y_t = m_t / n_t$ 和 $R_t = \\bar{p}(1-\\bar{p}) / n_t$。执行更新步骤以计算 $\\hat{\\theta}_{t|t}$ 和 $P_{t|t}$。\n      ii. 如果为假，跳过更新并设定 $\\hat{\\theta}_{t|t} = \\hat{\\theta}_{t|t-1}$ 和 $P_{t|t} = P_{t|t-1}$。\n3. 处理完整个序列后的最终值 $\\hat{\\theta}_{T|T}$ 和 $P_{T|T}$ 是每个测试案例所需的输出。",
            "answer": "```python\nimport numpy as np\n\ndef run_kalman_filter(data, mu, phi, q, p_bar, m0, P0):\n    \"\"\"\n    Implements the Kalman filter for the given state-space model.\n\n    Args:\n        data (list of tuples): A sequence of (m_t, n_t) pairs.\n        mu (float): Long-run mean of the state process.\n        phi (float): Autoregressive coefficient of the state process.\n        q (float): State innovation variance.\n        p_bar (float): Reference probability for observation variance.\n        m0 (float): Prior mean of the initial state.\n        P0 (float): Prior variance of the initial state.\n\n    Returns:\n        tuple: A tuple containing the final filtered posterior mean and variance.\n    \"\"\"\n    # Initialize the filtered state mean and variance with the prior\n    theta_filt = m0\n    P_filt = P0\n    \n    # Pre-calculate the numerator for the observation variance R_t\n    obs_var_numerator = p_bar * (1.0 - p_bar)\n\n    # Iterate through each time step (game)\n    for m_t, n_t in data:\n        # --- 1. Prediction Step ---\n        # Predict the next state mean\n        theta_pred = phi * theta_filt + mu * (1.0 - phi)\n        # Predict the next state variance\n        P_pred = phi**2 * P_filt + q\n\n        # --- 2. Update Step ---\n        # Check if there is an observation (n_t > 0)\n        if n_t > 0:\n            # Calculate the observation y_t\n            y_t = m_t / n_t\n            # Calculate the time-varying observation variance R_t\n            R_t = obs_var_numerator / n_t\n            \n            # Calculate the innovation covariance S_t\n            S_t = P_pred + R_t\n            \n            # Calculate the optimal Kalman gain K_t\n            K_t = P_pred / S_t\n            \n            # Update the state mean\n            theta_filt = theta_pred + K_t * (y_t - theta_pred)\n            \n            # Update the state variance\n            P_filt = (1.0 - K_t) * P_pred\n        else:\n            # If observation is missing (n_t = 0), the posterior is the prior\n            theta_filt = theta_pred\n            P_filt = P_pred\n            \n    return theta_filt, P_filt\n\ndef solve():\n    \"\"\"\n    Main function to define parameters, run test cases, and print results.\n    \"\"\"\n    # Fixed model parameters\n    mu = 0.45\n    phi = 0.90\n    q = 0.0005\n    p_bar = 0.45\n    \n    # Prior for the initial state\n    m0 = 0.45\n    P0 = 0.01\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Test case A\n        [(5, 11), (4, 10), (7, 12), (6, 14), (2, 6), (8, 15), (3, 8), (6, 12), (7, 16), (5, 9)],\n        # Test case B\n        [(9, 20), (10, 22), (8, 18), (11, 24), (12, 25), (9, 19), (10, 21), (12, 26), (11, 24), (13, 28)],\n        # Test case C\n        [(0, 0), (3, 5), (0, 0), (4, 4), (0, 0), (2, 10), (0, 0), (5, 10)],\n        # Test case D\n        [(1, 2), (0, 5), (7, 10), (1, 12), (9, 10), (0, 3), (6, 15), (2, 2), (0, 8), (10, 12)]\n    ]\n\n    results = []\n    # Process each test case\n    for data in test_cases:\n        theta_T, P_T = run_kalman_filter(data, mu, phi, q, p_bar, m0, P0)\n        # Format the result for the current test case as a string\n        # with values rounded to six decimal places, enclosed in brackets.\n        results.append(f\"[{theta_T:.6f},{P_T:.6f}]\")\n\n    # Final print statement in the exact required format.\n    # The output is a single line: a list of lists.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "金融市场的行为常常被认为是在几个不同的“状态”或“机制”之间切换，例如高波动期和低波动期，但这些状态是不可见的。隐马尔可夫模型（HMM）是识别这些潜在市场机制的有力工具。这个高级练习要求你从零开始，实现完整的HMM框架，包括使用Baum-Welch算法从收益率时间序列中估计模型参数，以及运用Viterbi算法解码最可能的隐藏状态序列。完成此练习将让你深入掌握一种前沿的计算经济学建模技术，并能够将其应用于识别金融数据中的结构性变化。",
            "id": "2388979",
            "problem": "您会获得一个实值回报率的离散时间序列，该序列被假定由一个双态隐马尔可夫模型 (HMM) 生成，其中隐藏状态代表一个不可观测的市场机制，例如低波动性机制与高波动性机制。状态过程是一个一阶马尔可夫链。在每个时间点，以隐藏状态为条件，观测到的回报率是从具有特定状态均值和方差的高斯分布中抽取的。您的任务是根据第一性原理，通过最大似然法估计 HMM，并为每个数据集解码最可能的隐藏机制序列。然后，使用具有经济意义的标记约定，根据解码出的机制将每个时间索引分类为高波动性或低波动性，并报告这些分类。\n\n基本原理：\n- 离散时间马尔可夫链是一个随机变量序列 $\\{S_t\\}_{t=0}^{T-1}$，其取值于一个有限集合，且对于所有 $t \\in \\{1,\\dots,T-1\\}$，满足 $\\mathbb{P}(S_t \\mid S_{t-1},\\dots,S_0) = \\mathbb{P}(S_t \\mid S_{t-1})$。\n- 隐马尔可夫模型 (HMM) 由一个隐藏的马尔可夫链 $S_t \\in \\{0,1\\}$ 和可观测的发射 $R_t \\in \\mathbb{R}$ 组成，其中给定隐藏状态，$R_t$ 是条件独立的。初始分布为 $\\boldsymbol{\\pi} \\in [0,1]^2$ 且 $\\sum_{i=0}^1 \\pi_i = 1$，转移矩阵为 $\\mathbf{A} \\in [0,1]^{2 \\times 2}$ 且每行之和为 1，发射分布为高斯分布：$R_t \\mid (S_t=i) \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$，其中 $i \\in \\{0,1\\}$。\n\n问题要求：\n- 令 $\\mathbf{r} = (r_0,\\dots,r_{T-1})$ 表示一个长度为 $T \\in \\mathbb{N}$ 的回报率数据集。\n- HMM 参数为 $\\boldsymbol{\\theta} = (\\boldsymbol{\\pi}, \\mathbf{A}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2)$，其中 $\\boldsymbol{\\mu} = (\\mu_0,\\mu_1)$ 且 $\\boldsymbol{\\sigma}^2 = (\\sigma_0^2,\\sigma_1^2)$。\n- 从提供的初始值开始，在 HMM 假设下通过最大化似然函数 $\\mathcal{L}(\\boldsymbol{\\theta}; \\mathbf{r})$ 来估计 $\\boldsymbol{\\theta}$。您必须实现一个与这些基本定义一致、并利用条件独立性和马尔可夫性质的数值稳定算法。您的估计只能使用提供的初始参数、数据和上述定律；不允许使用外部建模捷径或黑盒 HMM 库。\n- 为每个数据集估计出 $\\boldsymbol{\\theta}$ 后，在估计出的参数下解码最可能出现的隐藏状态序列 $\\hat{\\mathbf{s}} = (\\hat{s}_0,\\dots,\\hat{s}_{T-1})$，这需要与最大化状态路径和观测值的联合概率保持一致。\n- 将估计出方差较大的状态标记为高波动性机制。具体来说，通过 $i_{\\text{high}} = \\arg\\max_{i \\in \\{0,1\\}} \\sigma_i^2$ 定义 $i_{\\text{high}} \\in \\{0,1\\}$。如果对于一个数值容差 $\\varepsilon  0$，有 $|\\sigma_0^2 - \\sigma_1^2| \\le \\varepsilon$，则通过确定性地设置 $i_{\\text{high}} = 1$ 来打破平局。生成最终的分类向量 $\\mathbf{c} = (c_0,\\dots,c_{T-1})$，其中如果 $\\hat{s}_t = i_{\\text{high}}$，则 $c_t = 1$，否则 $c_t = 0$。\n- 数值稳定性：您必须以避免下溢或上溢的方式实现概率和似然的计算，例如通过在对数域中操作并使用稳定的求和技术。确保方差保持严格为正；如有需要，可以强制设定一个小的正下界 $\\underline{\\sigma}^2  0$。\n\n测试套件：\n对于下面的每个测试用例，您将获得：\n- 一个回报率数组 $\\mathbf{r}$。\n- 初始参数 $(\\boldsymbol{\\pi}^{(0)}, \\mathbf{A}^{(0)}, \\boldsymbol{\\mu}^{(0)}, \\boldsymbol{\\sigma}^{2(0)})$。\n- 用于您的估计算法的最大迭代次数 $N_{\\max}$ 和收敛容差 $\\tau$。\n\n您的程序必须处理所有测试用例，并为每个用例输出分类向量 $\\mathbf{c}$ 作为整数列表。\n\n测试用例 1 (正常路径，机制区分良好)：\n- $\\mathbf{r} = [0.004, -0.003, 0.002, -0.004, 0.001, -0.002, 0.003, -0.001, 0.002, -0.002, 0.0015, -0.0035, 0.025, -0.018, 0.015, -0.022, 0.028, -0.019, 0.017, -0.021, 0.019, -0.016, 0.018, -0.024]$。\n- $\\boldsymbol{\\pi}^{(0)} = [0.5, 0.5]$, $\\mathbf{A}^{(0)} = \\begin{bmatrix}0.9  0.1 \\\\ 0.1  0.9\\end{bmatrix}$, $\\boldsymbol{\\mu}^{(0)} = [0.0, 0.0]$, $\\boldsymbol{\\sigma}^{2(0)} = [0.00002, 0.0004]$。\n- $N_{\\max} = 100$, $\\tau = 10^{-6}$。\n\n测试用例 2 (边界条件，机制几乎无法区分)：\n- $\\mathbf{r} = [0.001, -0.002, 0.002, -0.001, 0.0, -0.002, 0.002, -0.001, 0.001, -0.002]$。\n- $\\boldsymbol{\\pi}^{(0)} = [0.5, 0.5]$, $\\mathbf{A}^{(0)} = \\begin{bmatrix}0.5  0.5 \\\\ 0.5  0.5\\end{bmatrix}$, $\\boldsymbol{\\mu}^{(0)} = [0.0, 0.0]$, $\\boldsymbol{\\sigma}^{2(0)} = [0.00001, 0.00001]$。\n- $N_{\\max} = 100$, $\\tau = 10^{-6}$。\n\n测试用例 3 (边缘情况，序列非常短)：\n- $\\mathbf{r} = [0.01, -0.02, 0.015]$。\n- $\\boldsymbol{\\pi}^{(0)} = [0.5, 0.5]$, $\\mathbf{A}^{(0)} = \\begin{bmatrix}0.6  0.4 \\\\ 0.4  0.6\\end{bmatrix}$, $\\boldsymbol{\\mu}^{(0)} = [0.0, 0.0]$, $\\boldsymbol{\\sigma}^{2(0)} = [0.0001, 0.0004]$。\n- $N_{\\max} = 100$, $\\tau = 10^{-6}$。\n\n测试用例 4 (持续性机制，每个状态都有长时间的运行)：\n- $\\mathbf{r} = [0.002, -0.0015, 0.0018, -0.0022, 0.0012, -0.0018, 0.0021, -0.0017, 0.03, -0.024, 0.022, -0.028, 0.027, -0.023, 0.025, -0.026, 0.0019, -0.0021, 0.0016, -0.0019, 0.0020, -0.0018, 0.0017, -0.0022]$。\n- $\\boldsymbol{\\pi}^{(0)} = [0.5, 0.5]$, $\\mathbf{A}^{(0)} = \\begin{bmatrix}0.98  0.02 \\\\ 0.02  0.98\\end{bmatrix}$, $\\boldsymbol{\\mu}^{(0)} = [0.0, 0.0]$, $\\boldsymbol{\\sigma}^{2(0)} = [0.00001, 0.0005]$。\n- $N_{\\max} = 100$, $\\tau = 10^{-6}$。\n\n最终输出格式：\n- 对于每个测试用例，将分类向量 $\\mathbf{c}$ 作为由 0 和 1 组成的整数列表输出，其中根据上述标记规则，1 表示高波动性机制，0 表示低波动性机制。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，形式为用方括号括起来的逗号分隔列表，不含空格。例如，包含三个测试用例的输出将如下所示：$[[1,0,1],[0,0],[1,1,1,0]]$。\n- 程序必须是完全自包含的，不得读取任何输入，并且只能使用指定的环境。不涉及物理单位或角度。在代码中以标准十进制形式表示所有实数。",
            "solution": "所提出的问题是计算金融和信号处理领域中一个适定且标准的任务：从观测到的回报率时间序列中估计一个具有高斯发射的双态隐马尔可夫模型 (HMM)。解决方案需要实现两种基本算法：用于最大似然参数估计的 Baum-Welch 算法和用于解码最可能隐藏状态序列的 Viterbi 算法。整个过程必须从第一性原理出发，确保数值稳定性。\n\n设隐藏状态的数量为 $N=2$，观测数量为 $T$。HMM 参数表示为 $\\boldsymbol{\\theta} = (\\boldsymbol{\\pi}, \\mathbf{A}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2)$，其中 $\\boldsymbol{\\pi}$ 是初始状态分布，$\\mathbf{A}$ 是状态转移矩阵，发射分布是均值为 $\\boldsymbol{\\mu} = (\\mu_0, \\mu_1)$、方差为 $\\boldsymbol{\\sigma}^2 = (\\sigma_0^2, \\sigma_1^2)$ 的高斯分布。观测到的回报率序列为 $\\mathbf{r} = (r_0, r_1, \\dots, r_{T-1})$。\n\n为防止长观测序列出现数值下溢，所有概率计算都将在对数域中进行。`log-sum-exp` 操作对此至关重要：$\\text{logsumexp}(x_1, \\dots, x_k) = \\log(\\sum_{i=1}^k e^{x_i})$。\n\n从状态 $i$ 发射观测值 $r_t$ 的高斯概率密度函数 (PDF) 由下式给出：\n$$ p(r_t | S_t=i; \\mu_i, \\sigma_i^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{(r_t - \\mu_i)^2}{2\\sigma_i^2}} $$\n其对数为 $\\log p(r_t | S_t=i) = -0.5\\log(2\\pi\\sigma_i^2) - \\frac{(r_t - \\mu_i)^2}{2\\sigma_i^2}$。\n\n首先，我们使用 Baum-Welch 算法来估计模型参数 $\\boldsymbol{\\theta}$。这是专为 HMM 定制的期望最大化 (EM) 算法的一个实例。它通过迭代方式优化参数，以增加观测数据的似然。\n\n**参数估计：Baum-Welch 算法**\n\n该算法从一组初始参数 $\\boldsymbol{\\theta}^{(0)}$ 开始，并在期望 (E) 步骤和最大化 (M) 步骤之间迭代。\n\n**E-步骤：**在此步骤中，我们根据观测值 $\\mathbf{r}$ 和当前参数估计 $\\boldsymbol{\\theta}^{(k)}$ 计算潜变量（隐藏状态）的期望值。这需要计算两个关键的后验概率：\n1.  $\\gamma_t(i) = P(S_t=i | \\mathbf{r}, \\boldsymbol{\\theta}^{(k)})$：在时间 $t$ 处于状态 $i$ 的概率。\n2.  $\\xi_t(i, j) = P(S_t=i, S_{t+1}=j | \\mathbf{r}, \\boldsymbol{\\theta}^{(k)})$：在时间 $t$ 从状态 $i$ 转移到状态 $j$ 的概率。\n\n这些是使用**前向-后向算法**计算的。令 $\\log \\alpha_t(i) = \\log P(r_0, \\dots, r_t, S_t=i)$ 和 $\\log \\beta_t(i) = \\log P(r_{t+1}, \\dots, r_{T-1} | S_t=i)$。\n\n**前向传递**过程如下：\n-   **初始化 ($t=0$):**\n    $$ \\log \\alpha_0(i) = \\log \\pi_i + \\log p(r_0 | S_0=i) $$\n-   **递归 ($t=1, \\dots, T-1$):**\n    $$ \\log \\alpha_t(j) = \\log p(r_t | S_t=j) + \\underset{i \\in \\{0,1\\}}{\\text{logsumexp}} \\left( \\log \\alpha_{t-1}(i) + \\log A_{ij} \\right) $$\n\n**后向传递**过程如下：\n-   **初始化 ($t=T-1$):**\n    $$ \\log \\beta_{T-1}(i) = 0 \\quad (\\text{因为 } \\beta_{T-1}(i) = 1) $$\n-   **递归 ($t=T-2, \\dots, 0$):**\n    $$ \\log \\beta_t(i) = \\underset{j \\in \\{0,1\\}}{\\text{logsumexp}} \\left( \\log A_{ij} + \\log p(r_{t+1} | S_{t+1}=j) + \\log \\beta_{t+1}(j) \\right) $$\n\n观测序列的对数似然为 $\\log \\mathcal{L}(\\boldsymbol{\\theta}; \\mathbf{r}) = \\underset{i \\in \\{0,1\\}}{\\text{logsumexp}} (\\log \\alpha_{T-1}(i))$。监测该值以判断收敛。\n\n然后计算后验概率 $\\gamma_t(i)$ 和 $\\xi_t(i, j)$ (首先在对数空间中)：\n$$ \\log \\gamma_t(i) = \\log \\alpha_t(i) + \\log \\beta_t(i) - \\log \\mathcal{L}(\\boldsymbol{\\theta}; \\mathbf{r}) $$\n$$ \\log \\xi_t(i, j) = \\log \\alpha_t(i) + \\log A_{ij} + \\log p(r_{t+1} | S_{t+1}=j) + \\log \\beta_{t+1}(j) - \\log \\mathcal{L}(\\boldsymbol{\\theta}; \\mathbf{r}) $$\n然后我们得到 $\\gamma_t(i) = \\exp(\\log \\gamma_t(i))$ 和 $\\xi_t(i, j) = \\exp(\\log \\xi_t(i, j))$。\n\n**M-步骤：**我们通过最大化 E-步骤中求出的期望对数似然来更新参数 $\\boldsymbol{\\theta}^{(k+1)}$。更新公式如下：\n-   **初始分布：** $\\pi_i^{(k+1)} = \\gamma_0(i)$\n-   **转移矩阵：** $A_{ij}^{(k+1)} = \\frac{\\sum_{t=0}^{T-2} \\xi_t(i, j)}{\\sum_{t=0}^{T-2} \\gamma_t(i)}$\n-   **均值：** $\\mu_i^{(k+1)} = \\frac{\\sum_{t=0}^{T-1} \\gamma_t(i) r_t}{\\sum_{t=0}^{T-1} \\gamma_t(i)}$\n-   **方差：** $(\\sigma_i^2)^{(k+1)} = \\frac{\\sum_{t=0}^{T-1} \\gamma_t(i) (r_t - \\mu_i^{(k+1)})^2}{\\sum_{t=0}^{T-1} \\gamma_t(i)}$\n\n为了保持数值稳定性，对方差强制施加一个小的正下界 $\\underline{\\sigma}^2$。重复 E-步骤和 M-步骤，直到对数似然的改善低于容差 $\\tau$ 或达到最大迭代次数 $N_{\\max}$。\n\n**状态解码：Viterbi 算法**\n\n在 Baum-Welch 算法收敛到估计参数 $\\hat{\\boldsymbol{\\theta}}$ 后，我们寻找最可能的隐藏状态序列 $\\hat{\\mathbf{s}} = (\\hat{s}_0, \\dots, \\hat{s}_{T-1})$。这通过使用 Viterbi 算法实现，它是一种动态规划方法，用于寻找使联合概率 $P(\\mathbf{s}, \\mathbf{r} | \\hat{\\boldsymbol{\\theta}})$ 最大化的路径。\n\n令 $\\log \\delta_t(i)$ 为截至时间 $t$、结束于状态 $i$ 的最可能状态序列的对数概率。令 $\\psi_t(j)$ 存储此路径上的前驱状态。\n\n-   **初始化 ($t=0$):**\n    $$ \\log \\delta_0(i) = \\log \\hat{\\pi}_i + \\log p(r_0 | S_0=i; \\hat{\\mu}_i, \\hat{\\sigma}_i^2) $$\n-   **递归 ($t=1, \\dots, T-1$):**\n    $$ \\log \\delta_t(j) = \\max_{i \\in \\{0,1\\}} \\left( \\log \\delta_{t-1}(i) + \\log \\hat{A}_{ij} \\right) + \\log p(r_t | S_t=j; \\hat{\\mu}_j, \\hat{\\sigma}_j^2) $$\n    $$ \\psi_t(j) = \\underset{i \\in \\{0,1\\}}{\\arg\\max} \\left( \\log \\delta_{t-1}(i) + \\log \\hat{A}_{ij} \\right) $$\n-   **终止：**最可能路径的最终状态是 $\\hat{s}_{T-1} = \\underset{i \\in \\{0,1\\}}{\\arg\\max} (\\log \\delta_{T-1}(i))$。\n-   **回溯 ($t=T-2, \\dots, 0$):** 路径被反向重构：$\\hat{s}_t = \\psi_{t+1}(\\hat{s}_{t+1})$。\n\n**机制分类**\n\n利用估计出的方差 $\\hat{\\sigma}_0^2$ 和 $\\hat{\\sigma}_1^2$，我们定义一个“高波动性”机制。该机制的状态索引 $i_{\\text{high}}$ 由以下公式确定：\n$$ i_{\\text{high}} = \\underset{i \\in \\{0,1\\}}{\\arg\\max} \\, \\hat{\\sigma}_i^2 $$\n应用平局打破规则：如果对于一个小的容差 $\\varepsilon > 0$，有 $|\\hat{\\sigma}_0^2 - \\hat{\\sigma}_1^2| \\le \\varepsilon$，我们确定性地设置 $i_{\\text{high}} = 1$。最后，从解码的路径 $\\hat{\\mathbf{s}}$ 构建分类向量 $\\mathbf{c}$：\n$$ c_t = \\begin{cases} 1  \\text{如果 } \\hat{s}_t = i_{\\text{high}} \\\\ 0  \\text{否则} \\end{cases} $$\n此过程应用于每个测试用例以生成所需的输出。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases for HMM estimation and decoding.\n    \"\"\"\n    test_cases = [\n        {\n            \"r\": np.array([0.004, -0.003, 0.002, -0.004, 0.001, -0.002, 0.003, -0.001, 0.002, -0.002, 0.0015, -0.0035, 0.025, -0.018, 0.015, -0.022, 0.028, -0.019, 0.017, -0.021, 0.019, -0.016, 0.018, -0.024]),\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"var0\": np.array([0.00002, 0.0004]),\n            \"N_max\": 100,\n            \"tau\": 1e-6\n        },\n        {\n            \"r\": np.array([0.001, -0.002, 0.002, -0.001, 0.0, -0.002, 0.002, -0.001, 0.001, -0.002]),\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"var0\": np.array([0.00001, 0.00001]),\n            \"N_max\": 100,\n            \"tau\": 1e-6\n        },\n        {\n            \"r\": np.array([0.01, -0.02, 0.015]),\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.6, 0.4], [0.4, 0.6]]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"var0\": np.array([0.0001, 0.0004]),\n            \"N_max\": 100,\n            \"tau\": 1e-6\n        },\n        {\n            \"r\": np.array([0.002, -0.0015, 0.0018, -0.0022, 0.0012, -0.0018, 0.0021, -0.0017, 0.03, -0.024, 0.022, -0.028, 0.027, -0.023, 0.025, -0.026, 0.0019, -0.0021, 0.0016, -0.0019, 0.0020, -0.0018, 0.0017, -0.0022]),\n            \"pi0\": np.array([0.5, 0.5]),\n            \"A0\": np.array([[0.98, 0.02], [0.02, 0.98]]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"var0\": np.array([0.00001, 0.0005]),\n            \"N_max\": 100,\n            \"tau\": 1e-6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        c = solve_hmm_case(case)\n        results.append(c)\n\n    results_str = [f\"[{','.join(map(str, r))}]\" for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\n\ndef log_gaussian_pdf(x, mu, var):\n    \"\"\"\n    Computes the log of the Gaussian probability density function.\n    \"\"\"\n    return -0.5 * np.log(2 * np.pi * var) - (x - mu)**2 / (2 * var)\n\n\ndef solve_hmm_case(case):\n    \"\"\"\n    Solves a single HMM problem case: estimation, decoding, and classification.\n    \"\"\"\n    r, pi, A, mu, var, N_max, tau = \\\n        case[\"r\"], case[\"pi0\"], case[\"A0\"], case[\"mu0\"], case[\"var0\"], case[\"N_max\"], case[\"tau\"]\n\n    T = len(r)\n    N = len(pi)\n    var_floor = 1e-9\n    \n    log_likelihood_old = -np.inf\n\n    for _ in range(N_max):\n        # E-Step: Forward-Backward Algorithm\n        log_emission_probs = np.zeros((T, N))\n        for i in range(N):\n            log_emission_probs[:, i] = log_gaussian_pdf(r, mu[i], var[i])\n\n        # Forward pass\n        log_alpha = np.zeros((T, N))\n        log_pi = np.log(pi)\n        log_A = np.log(A)\n        \n        log_alpha[0, :] = log_pi + log_emission_probs[0, :]\n        for t in range(1, T):\n            for j in range(N):\n                log_alpha[t, j] = log_emission_probs[t, j] + logsumexp(log_alpha[t - 1, :] + log_A[:, j])\n\n        log_likelihood = logsumexp(log_alpha[T - 1, :])\n        \n        if abs(log_likelihood - log_likelihood_old)  tau:\n            break\n        log_likelihood_old = log_likelihood\n\n        # Backward pass\n        log_beta = np.zeros((T, N))\n        for t in range(T - 2, -1, -1):\n            for i in range(N):\n                 log_beta[t, i] = logsumexp(log_A[i, :] + log_emission_probs[t + 1, :] + log_beta[t + 1, :])\n\n        # Compute posteriors\n        log_gamma = log_alpha + log_beta - log_likelihood\n        gamma = np.exp(log_gamma)\n\n        log_xi = np.zeros((T - 1, N, N))\n        for t in range(T - 1):\n            for i in range(N):\n                for j in range(N):\n                    log_xi[t, i, j] = log_alpha[t, i] + log_A[i, j] + log_emission_probs[t + 1, j] + log_beta[t + 1, j]\n        log_xi -= log_likelihood\n        xi = np.exp(log_xi)\n\n        # M-Step: Re-estimation\n        pi = gamma[0, :]\n        \n        A_numerator = np.sum(xi, axis=0)\n        A_denominator = np.sum(gamma[:-1, :], axis=0)[:, np.newaxis]\n        A = A_numerator / A_denominator\n\n        gamma_sum = np.sum(gamma, axis=0)\n        \n        mu = np.sum(gamma * r[:, np.newaxis], axis=0) / gamma_sum\n        \n        var_numerator = np.sum(gamma * (r[:, np.newaxis] - mu)**2, axis=0)\n        var = var_numerator / gamma_sum\n        var = np.maximum(var, var_floor)\n\n    # Viterbi Algorithm for decoding\n    log_delta = np.zeros((T, N))\n    psi = np.zeros((T, N), dtype=int)\n\n    log_pi = np.log(pi)\n    log_A = np.log(A)\n\n    for i in range(N):\n        log_emission_probs[:, i] = log_gaussian_pdf(r, mu[i], var[i])\n\n    log_delta[0, :] = log_pi + log_emission_probs[0, :]\n\n    for t in range(1, T):\n        for j in range(N):\n            log_probs = log_delta[t - 1, :] + log_A[:, j]\n            psi[t, j] = np.argmax(log_probs)\n            log_delta[t, j] = np.max(log_probs) + log_emission_probs[t, j]\n\n    # Backtracking\n    s_hat = np.zeros(T, dtype=int)\n    s_hat[T - 1] = np.argmax(log_delta[T - 1, :])\n    for t in range(T - 2, -1, -1):\n        s_hat[t] = psi[t + 1, s_hat[t + 1]]\n\n    # Classification\n    epsilon_var = 1e-9\n    if abs(var[0] - var[1]) = epsilon_var:\n        i_high = 1\n    else:\n        i_high = np.argmax(var)\n        \n    c = (s_hat == i_high).astype(int).tolist()\n    \n    return c\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}