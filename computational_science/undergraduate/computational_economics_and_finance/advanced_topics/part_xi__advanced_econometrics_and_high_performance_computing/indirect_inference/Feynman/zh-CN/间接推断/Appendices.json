{
    "hands_on_practices": [
        {
            "introduction": "理论是根基，但真正的理解来自于实践。本节将通过一系列动手练习，带你从零开始构建并应用间接推断。第一个练习将我们带到一个经典的概率模型中：高尔顿板（或称“Plinko”板）。这个练习旨在通过一个直观的例子，让你掌握间接推断的核心流程：如何通过观察最终结果（小球的分布）来反向推断系统内部的未知参数（小球在每个钉子上向左或向右的概率）。通过解决这个问题 ()，你将练习建立结构模型参数与辅助统计量（如均值和方差）之间的联系，并实现一个完整的参数估计过程。",
            "id": "2401820",
            "problem": "考虑一个简化的“plinko”式高尔顿板模型，用于说明随机过程。大量相同的球从顶部逐一落下。每个球会经过 $T$ 排钉子。在每个钉子处，球会向右或向左偏转。设 $S_{it}\\in\\{-d, +d\\}$ 表示球 $i$ 在第 $t$ 排的水平增量，其中 $d>0$ 是固定的水平步长。假设 $\\mathbb{P}(S_{it}=+d)=p$ 且 $\\mathbb{P}(S_{it}=-d)=1-p$，其中 $0\\le p\\le 1$，且各球各排之间独立。球 $i$ 经过 $T$ 排后的最终水平位置为 $X_i=\\sum_{t=1}^{T} S_{it}$。\n\n设 $\\theta=(p,d)$ 是未知的结构参数向量。您观测到一个由真实参数 $\\theta_0=(p_0,d_0)$ 生成的包含 $N_{\\text{obs}}$ 个独立最终位置的数据集 $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$。您需要使用间接推断（Indirect Inference, II）来估计 $\\theta$，其定义如下。选择一个辅助模型：一个正态分布 $\\mathcal{N}(\\mu,\\sigma^2)$，其辅助参数是从最终位置计算出的样本均值和样本方差。将辅助统计量映射 $m(\\cdot)$ 定义为 $m(\\{x_i\\}) = \\big(\\bar{x}, s^2\\big)$，其中 $\\bar{x}$ 是样本均值，而 $s^2$ 是除数为 $N$（总体方差）的样本方差。对于一个候选参数 $\\theta$，设 $m_{\\text{sim}}(\\theta)$ 表示由参数为 $\\theta$ 的结构模型生成的数据计算出的辅助统计量。II 估计量通过最小化二次距离\n$$\nQ(\\theta)=\\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big)^{\\top} W \\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big),\n$$\n来实现，其中权重矩阵 $W=I_2$，$m_{\\text{obs}}=m(\\{X_i\\}_{i=1}^{N_{\\text{obs}}})$。\n\n您的任务是编写一个完整的程序，对下面指定的每个测试用例，从第一性原理出发完成以下所有操作：\n- 使用给定的真实参数 $\\theta_0$ 和指定的随机种子，通过结构模型生成观测数据集 $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$。\n- 计算观测到的辅助统计量 $m_{\\text{obs}}$。\n- 将 $Q(\\theta)$ 在可行集 $\\{(p,d): 0 \\le p \\le 1, d > 0\\}$ 上最小化，以计算间接推断估计值 $\\hat{\\theta}=(\\hat{p},\\hat{d})$。\n- 按照最终输出格式的要求，为每个测试用例返回 $\\hat{\\theta}$。\n\n所有概率必须表示为 $[0,1]$ 区间内的小数。不涉及物理单位。任何角度（如果存在）都与此问题无关。估计必须严格按照上述定义进行。\n\n测试集。对于每一项，都给定了行数 $T$、真实概率 $p_0$、真实步长 $d_0$、观测样本量 $N_{\\text{obs}}$ 以及用于生成观测数据的随机种子。必须使用提供的种子来初始化随机数生成器以生成观测数据。最终的样本方差必须使用除数 $N_{\\text{obs}}$（总体方差）。\n- 测试用例 1：$T=20$，$p_0=0.6$，$d_0=1.2$，$N_{\\text{obs}}=50000$，种子 $=13579$。\n- 测试用例 2：$T=30$，$p_0=0.5$，$d_0=1.0$，$N_{\\text{obs}}=50000$，种子 $=24680$。\n- 测试用例 3：$T=25$，$p_0=0.9$，$d_0=0.8$，$N_{\\text{obs}}=50000$，种子 $=11235$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个列表的列表，每个内部列表对应一个测试用例，顺序与上面列出的顺序相同。每个内部列表必须包含两个小数 $[\\hat{p},\\hat{d}]$，四舍五入到小数点后恰好六位。例如，输出行必须如下所示：\n[[p1,d1],[p2,d2],[p3,d3]]\n不含空格，其中每个 $p_j$ 和 $d_j$ 都是四舍五入到六位小数的小数。",
            "solution": "所提出的问题是使用间接推断方法进行参数估计的一个有效且适定的练习。它在科学上基于概率论和统计估计，并为其解决提供了所有必要的信息。我们将提供一个完整的解决方案。\n\n问题的核心是，给定一组过程最终状态的观测值，估计该随机过程的结构参数 $\\theta=(p,d)$。结构模型描述了一个在具有 $T$ 排钉子的高尔顿板上运动的球。在每一排 $t \\in \\{1, \\dots, T\\}$，球的水平位置发生增量 $S_t$，该增量以概率 $p$ 为 $+d$，以概率 $1-p$ 为 $-d$。经过 $T$ 排后的最终水平位置为 $X = \\sum_{t=1}^{T} S_t$。\n\n首先，我们建立结构参数 $\\theta=(p,d)$ 与最终位置 $X$ 的矩之间的解析关系。设 $K$ 是一个随机变量，表示向右偏转的次数，它服从二项分布，$K \\sim \\text{Binomial}(T,p)$。总偏转次数为 $T$，因此有 $T-K$ 次向左偏转。最终位置 $X$ 可以表示为：\n$$\nX = K \\cdot (+d) + (T-K) \\cdot (-d) = d(K - (T-K)) = d(2K - T)\n$$\n二项变量 $K$ 的均值和方差分别为 $\\mathbb{E}[K] = Tp$ 和 $\\text{Var}(K) = Tp(1-p)$。利用期望和方差的性质，我们可以求出最终位置 $X$ 的均值 $\\mu_X$ 和方差 $\\sigma_X^2$：\n$$\n\\mu(p, d) = \\mathbb{E}[X] = \\mathbb{E}[d(2K - T)] = d(2\\mathbb{E}[K] - T) = d(2Tp - T) = Td(2p-1)\n$$\n$$\n\\sigma^2(p, d) = \\text{Var}(X) = \\text{Var}(d(2K - T)) = d^2 \\text{Var}(2K) = 4d^2\\text{Var}(K) = 4d^2Tp(1-p)\n$$\n这两个方程构成了将结构参数 $\\theta=(p,d)$ 映射到可观测数据理论矩的约束函数。\n\n该问题要求通过间接推断（II）进行估计。我们给定一个辅助模型，即正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$，并且辅助统计量是样本均值 $\\bar{x}$ 和样本方差 $s^2$（除数为 $N_{\\text{obs}}$）。我们有一组由真实参数 $\\theta_0=(p_0,d_0)$ 生成的 $N_{\\text{obs}}$ 个观测值 $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$。我们计算观测到的辅助统计量向量 $m_{\\text{obs}} = (\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$。\n\nII 估计量 $\\hat{\\theta}$ 是通过最小化目标函数找到的：\n$$\nQ(\\theta) = (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))^{\\top} W (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))\n$$\n其中权重矩阵是单位矩阵，$W=I_2$。项 $m_{\\text{sim}}(\\theta)$ 表示由参数为 $\\theta$ 的模型产生的辅助统计量。评估 $m_{\\text{sim}}(\\theta)$ 是一个关键点。虽然人们可以在优化过程中对每个 $\\theta$ 值进行模拟，但这会将随机性引入目标函数，使最小化过程变得复杂，并需要指定内部模拟的规模，而题目并未提供。在这种情况下，正确且标准的方法是使用上面推导出的解析矩，这对应于当模拟规模趋于无穷大时辅助统计量的期望值。\n因此，$m_{\\text{sim}}(\\theta) = (\\mu(p,d), \\sigma^2(p,d))$。目标函数变为：\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - \\mu(p,d))^2 + (s^2_{\\text{obs}} - \\sigma^2(p,d))^2\n$$\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - Td(2p-1))^2 + (s^2_{\\text{obs}} - 4Td^2p(1-p))^2\n$$\n估计任务是找到使函数 $Q(p,d)$ 在约束条件 $p \\in [0,1]$ 和 $d > 0$ 下最小化的值 $(\\hat{p}, \\hat{d})$。这是一个标准的非线性约束优化问题。\n\n解决每个测试用例的算法流程如下：\n1.  **生成观测数据**：对于给定的测试用例参数 $T, p_0, d_0, N_{\\text{obs}}$ 和特定的随机种子，生成 $N_{\\text{obs}}$ 个观测值。通过首先从分布 $\\text{Binomial}(T, p_0)$ 中抽取 $N_{\\text{obs}}$ 个值 $\\{K_i\\}_{i=1}^{N_{\\text{obs}}}$ 来高效地实现这一点。然后，将观测到的最终位置计算为 $X_i = d_0(2K_i - T)$。\n2.  **计算观测统计量**：从生成的数据集 $\\{X_i\\}$ 中，计算观测到的辅助统计量 $m_{\\text{obs}}=(\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$。具体来说，$\\bar{x}_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} X_i$ 且 $s^2_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} (X_i - \\bar{x}_{\\text{obs}})^2$。\n3.  **数值优化**：关于 $p$ 和 $d$ 最小化目标函数 $Q(p,d)$。我们采用适合约束问题的数值优化算法，例如带箱式约束的拟牛顿法（如 L-BFGS-B）。搜索空间由边界 $p \\in [0,1]$ 和 $d \\in (0, \\infty)$ 定义。为了数值稳定性，对 $d$ 使用一个小的正数作为下界。\n4.  **报告估计值**：最小化 $Q(p,d)$ 的值对 $(\\hat{p}, \\hat{d})$ 构成了给定测试用例的间接推断估计。然后收集所有测试用例的结果，并按要求格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the Indirect Inference estimation problem for all test cases.\n    \"\"\"\n\n    # Test cases as specified in the problem statement.\n    test_cases = [\n        # (T, p0, d0, N_obs, seed)\n        (20, 0.6, 1.2, 50000, 13579),\n        (30, 0.5, 1.0, 50000, 24680),\n        (25, 0.9, 0.8, 50000, 11235),\n    ]\n\n    all_results = []\n\n    for T, p0, d0, N_obs, seed in test_cases:\n        # Step 1: Generate the observed dataset {X_i}\n        # Use a more efficient method based on the Binomial distribution.\n        # k ~ Binomial(T, p) represents the number of right steps.\n        # X = k * (+d) + (T - k) * (-d) = d * (2k - T)\n        rng = np.random.default_rng(seed)\n        k_obs = rng.binomial(T, p0, size=N_obs)\n        x_obs = d0 * (2 * k_obs - T)\n\n        # Step 2: Compute the observed auxiliary statistics m_obs = (mean, variance)\n        mu_obs = np.mean(x_obs)\n        # Use population variance (divisor N), as specified (ddof=0 is default for np.var)\n        var_obs = np.var(x_obs)\n        m_obs = (mu_obs, var_obs)\n        \n        # Step 3: Define the objective function Q(theta) for minimization.\n        # theta is a tuple (p, d).\n        def objective_function(theta, T_val, m_obs_val):\n            p, d = theta\n            mu_obs_val, var_obs_val = m_obs_val\n\n            # Analytical moments from the structural model\n            mu_sim = T_val * d * (2 * p - 1)\n            var_sim = 4 * T_val * (d**2) * p * (1 - p)\n            \n            # Quadratic objective function Q(theta)\n            q_val = (mu_obs_val - mu_sim)**2 + (var_obs_val - var_sim)**2\n            return q_val\n\n        # Step 4: Perform numerical optimization to find the II estimate.\n        # Initial guess for the parameters (p, d)\n        initial_guess = [0.5, 1.0]\n\n        # Bounds for the parameters: 0 = p = 1 and d > 0.\n        # Use a small positive number for the lower bound of d for numerical stability.\n        bounds = [(0.0, 1.0), (1e-9, None)]\n\n        # Minimize the objective function. L-BFGS-B is suitable for box constraints.\n        result = minimize(\n            fun=objective_function,\n            x0=initial_guess,\n            args=(T, m_obs),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n\n        # The optimized parameters are the II estimates\n        p_hat, d_hat = result.x\n        all_results.append([p_hat, d_hat])\n\n    # Final print statement in the exact required format.\n    # Create the list of lists with rounded values.\n    # e.g., [[0.600012, 1.199998], [0.500001, 1.000003], [0.900005, 0.799989]]\n    # Then convert to string and remove spaces to match the output format.\n    final_list_formatted = [[round(p, 6), round(d, 6)] for p, d in all_results]\n    output_str = str(final_list_formatted).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n\n```"
        },
        {
            "introduction": "在掌握了基础流程之后，我们来挑战一个更贴近经济学研究实践的场景。在许多情况下，辅助模型的参数本身就需要通过一个相对复杂的统计方法来估计，例如广义矩估计（GMM）。这个练习 () 将结构模型设定为一个标准的一阶自回归（AR(1)）过程，但其辅助模型采用了一个过识别的 GMM 框架进行估计。通过完成这项任务，你不仅能深化对 GMM 的理解，还能学会如何在间接推断的“内循环”中嵌套另一个估计器，并掌握使用“共同随机数”这一关键的模拟技巧来提高估计的稳定性和效率。",
            "id": "2401827",
            "problem": "您会得到一个结构性时间序列模型和一个辅助模型，其参数通过广义矩估计 (GMM) 进行估计。您的任务是实现一个间接推断估计量，其中辅助参数是通过 GMM 而非最大似然估计 (MLE) 获得的，并为指定的一组数据生成过程和模拟设置生成间接推断估计值。\n\n结构模型是一阶自回归模型，其已知创新项方差等于 1。对于每个时间指数 $t$，该过程满足\n$$\ny_t = \\theta \\, y_{t-1} + \\varepsilon_t,\n$$\n其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的。使用初始条件 $y_0 = 0$ 和在收集 $T$ 个观测值之前的 $B = 200$ 个周期的预烧期 (burn-in)。对于观测样本路径和辅助样本路径的模拟，角度单位不适用，也不涉及物理单位。\n\n辅助模型是线性回归\n$$\ny_t = \\beta_0 + \\beta_1 \\, y_{t-1} + u_t,\n$$\n辅助参数 $\\beta = (\\beta_0,\\beta_1)^\\top$ 是通过基于工具向量的过度识别矩条件来定义的\n$$\nz_t = \\begin{bmatrix} 1 \\\\ y_{t-1} \\\\ y_{t-2} \\end{bmatrix}.\n$$\n对于每个样本 $y_1,\\dots,y_T$，将 $t=3,\\dots,T$ 的样本矩函数定义为\n$$\ng_T(\\beta) = \\frac{1}{n} \\sum_{t=3}^T z_t \\, \\left(y_t - \\beta_0 - \\beta_1 \\, y_{t-1}\\right),\n$$\n其中 $n = T-2$。$\\beta$ 的 GMM 估计量使用维度为 $3 \\times 3$ 的单位权重矩阵定义为\n$$\\widehat{\\beta}(y_{1:T}) = \\arg \\min_{\\beta \\in \\mathbb{R}^2} \\; g_T(\\beta)^\\top g_T(\\beta).$$\n候选结构参数 $\\theta$ 的间接推断目标使用从具有相同预烧期 $B$ 的结构模型生成的 $S$ 个长度为 $T$ 的独立模拟样本。设 $\\widehat{\\beta}^{\\text{obs}}$ 表示从观测数据集中获得的辅助 GMM 估计值。对于给定的 $\\theta$，定义\n$$\n\\overline{\\beta}(\\theta) = \\frac{1}{S} \\sum_{s=1}^S \\widehat{\\beta}(y^{(s)}_{1:T}(\\theta)),\n$$\n其中 $y^{(s)}_{1:T}(\\theta)$ 是在 $\\theta$ 下生成的第 $s$ 个长度为 $T$ 的模拟样本。间接推断准则为\n$$Q(\\theta) = \\left(\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\right)^\\top W \\left(\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\right),$$\n其中 $W = I_2$。对于下文的每个测试案例，将 $\\theta$ 限制在离散搜索网格上\n$$\n\\mathcal{G}_i = \\{-0.95, -0.95 + \\delta_i, \\dots, 0.95\\},\n$$\n其中 $\\delta_i$ 是在测试案例中指定的网格步长。间接推断估计值为\n$$\\widehat{\\theta}_i \\in \\arg \\min_{\\theta \\in \\mathcal{G}_i} Q(\\theta),$$\n若存在多个最优解，则选择最小的 $\\theta$。\n\n模拟与可复现性要求如下：\n- 对于每个测试案例 $i$，使用真实参数 $\\theta^{\\star}_i$、样本量 $T_i$、等于 $1$ 的创新项方差、预烧期 $B=200$ 以及指定的观测数据种子 $s^{\\text{obs}}_i$ 来生成观测数据集。\n- 为了在网格上评估测试案例 $i$ 的 $Q(\\theta)$，通过为每个重复索引 $s \\in \\{1,\\dots,S_i\\}$ 使用种子 $s^{\\text{sim}}_i + s$ 预先生成一个长度为 $T_i + B$ 的标准正态创新项序列，从而在不同的 $\\theta$ 值之间使用共同随机数。对于所有的 $\\theta \\in \\mathcal{G}_i$，重用相同的预生成创新项。\n\n实现上述过程并为以下四个测试案例计算 $\\widehat{\\theta}_i$：\n- 测试案例 1：$T_1 = 300$，$\\theta^{\\star}_1 = 0.6$，$S_1 = 20$，$\\delta_1 = 0.01$，$s^{\\text{obs}}_1 = 1729$，$s^{\\text{sim}}_1 = 20231$。\n- 测试案例 2：$T_2 = 300$，$\\theta^{\\star}_2 = 0.9$，$S_2 = 25$，$\\delta_2 = 0.01$，$s^{\\text{obs}}_2 = 1730$，$s^{\\text{sim}}_2 = 20232$。\n- 测试案例 3：$T_3 = 150$，$\\theta^{\\star}_3 = 0.0$，$S_3 = 40$，$\\delta_3 = 0.02$，$s^{\\text{obs}}_3 = 1731$，$s^{\\text{sim}}_3 = 20233$。\n- 测试案例 4：$T_4 = 300$，$\\theta^{\\star}_4 = -0.5$，$S_4 = 20$，$\\delta_4 = 0.01$，$s^{\\text{obs}}_4 = 1732$，$s^{\\text{sim}}_4 = 20234$。\n\n您的程序应生成单行输出，包含四个估计值 $\\widehat{\\theta}_1, \\widehat{\\theta}_2, \\widehat{\\theta}_3, \\widehat{\\theta}_4$，格式为逗号分隔的列表，并用方括号括起来，每个值四舍五入到小数点后四位，例如 $[\\widehat{\\theta}_1,\\widehat{\\theta}_2,\\widehat{\\theta}_3,\\widehat{\\theta}_4]$。",
            "solution": "该问题要求为一阶自回归过程 AR(1) 的参数 $\\theta$ 实现一个间接推断估计量。该估计过程基于一个辅助模型，其参数通过广义矩估计 (GMM) 进行估计。首先评估问题陈述的有效性。\n\n### 步骤 1：提取给定信息\n\n- **结构模型**：$y_t = \\theta \\, y_{t-1} + \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ 为独立同分布 (i.i.d.) 的创新项。\n- **初始条件**：$y_0 = 0$。\n- **模拟参数**：预烧期 $B = 200$。预烧后收集样本量为 $T$ 的观测值。\n- **辅助模型**：$y_t = \\beta_0 + \\beta_1 \\, y_{t-1} + u_t$，参数为 $\\beta = (\\beta_0,\\beta_1)^\\top$。\n- **工具向量**：$z_t = [1, y_{t-1}, y_{t-2}]^\\top$。\n- **GMM 样本矩函数**：$g_T(\\beta) = \\frac{1}{n} \\sum_{t=3}^T z_t \\, (y_t - \\beta_0 - \\beta_1 y_{t-1})$，其中 $n = T-2$。\n- **GMM 估计量**：$\\widehat{\\beta}(y_{1:T}) = \\arg \\min_{\\beta \\in \\mathbb{R}^2} \\; g_T(\\beta)^\\top g_T(\\beta)$。这对应于使用单位权重矩阵。\n- **间接推断准则**：$Q(\\theta) = (\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta))^\\top W (\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta))$，权重矩阵 $W = I_2$。\n- **模拟的辅助参数**：$\\overline{\\beta}(\\theta) = \\frac{1}{S} \\sum_{s=1}^S \\widehat{\\beta}(y^{(s)}_{1:T}(\\theta))$，其中 $y^{(s)}_{1:T}(\\theta)$ 是从参数为 $\\theta$ 的结构模型生成的 $S$ 个独立样本。\n- **估计网格**：$\\mathcal{G}_i = \\{-0.95, -0.95 + \\delta_i, \\dots, 0.95\\}$。\n- **间接推断估计量**：$\\widehat{\\theta}_i \\in \\arg \\min_{\\theta \\in \\mathcal{G}_i} Q(\\theta)$，若存在多个最优解，则选择最小的 $\\theta$。\n- **可复现性**：在评估不同 $\\theta$ 值的 $Q(\\theta)$ 时，使用共同随机数。对于测试案例 $i$，观测数据使用种子 $s^{\\text{obs}}_i$ 生成。$S_i$ 条模拟路径使用种子 $s^{\\text{sim}}_i + 1, \\dots, s^{\\text{sim}}_i + S_i$ 生成。\n- **测试案例**：\n    1.  $T_1 = 300$，$\\theta^{\\star}_1 = 0.6$，$S_1 = 20$，$\\delta_1 = 0.01$，$s^{\\text{obs}}_1 = 1729$，$s^{\\text{sim}}_1 = 20231$。\n    2.  $T_2 = 300$，$\\theta^{\\star}_2 = 0.9$，$S_2 = 25$，$\\delta_2 = 0.01$，$s^{\\text{obs}}_2 = 1730$，$s^{\\text{sim}}_2 = 20232$。\n    3.  $T_3 = 150$，$\\theta^{\\star}_3 = 0.0$，$S_3 = 40$，$\\delta_3 = 0.02$，$s^{\\text{obs}}_3 = 1731$，$s^{\\text{sim}}_3 = 20233$。\n    4.  $T_4 = 300$，$\\theta^{\\star}_4 = -0.5$，$S_4 = 20$，$\\delta_4 = 0.01$，$s^{\\text{obs}}_4 = 1732$，$s^{\\text{sim}}_4 = 20234$。\n\n### 步骤 2：使用提取的给定信息进行验证\n\n- **科学依据**：该问题在计量经济学和统计学原理上有坚实的基础。AR(1) 模型、GMM 估计和间接推断都是标准且广泛使用的方法论。\n- **适定性**：该问题是适定的。目标函数 $Q(\\theta)$ 定义清晰，其在离散网格上的最小化保证了解的存在性。指定的决胜规则确保了解的唯一性。\n- **客观性**：该问题以精确、客观的数学语言陈述，没有任何主观或模糊的术语。\n- **完整性与一致性**：所有必需的模型、参数、种子和程序步骤都已明确提供。设置是内部一致的，没有矛盾。对 GMM 使用单位权重矩阵是一种有效（尽管可能效率不高）的选择，它能导出一个明确定义的估计量。\n\n### 步骤 3：结论与行动\n\n问题陈述是有效的，它在科学上合理、适定且自洽。将提供一个合理的解法。\n\n### 基于原理的解法设计\n\n该解法通过实现指定的间接推断过程来构建。这涉及几个不同的逻辑步骤：数据模拟、辅助参数估计、间接推断准则的评估以及对结构参数的网格搜索。\n\n1.  **辅助参数的解析 GMM 估计量**\n    辅助参数 $\\beta = (\\beta_0, \\beta_1)^\\top$ 通过最小化 $J(\\beta) = g_T(\\beta)^\\top g_T(\\beta)$ 来估计。设样本表示为 $y_1, \\dots, y_T$。矩条件对 $t=3, \\dots, T$ 进行评估，共包含 $n=T-2$ 个观测值。我们可以以矩阵形式表示该问题。令\n    $$\n    y_{\\text{vec}} = \\begin{bmatrix} y_3 \\\\ y_4 \\\\ \\vdots \\\\ y_T \\end{bmatrix}, \\quad\n    X = \\begin{bmatrix} 1  y_2 \\\\ 1  y_3 \\\\ \\vdots  \\vdots \\\\ 1  y_{T-1} \\end{bmatrix}, \\quad\n    Z = \\begin{bmatrix} 1  y_2  y_1 \\\\ 1  y_3  y_2 \\\\ \\vdots  \\vdots  \\vdots \\\\ 1  y_{T-1}  y_{T-2} \\end{bmatrix}.\n    $$\n    GMM 目标函数，忽略常数因子 $1/n^2$，是最小化 $(Z'y_{\\text{vec}} - Z'X\\beta)^\\top (Z'y_{\\text{vec}} - Z'X\\beta)$。这是一个形如 $\\min_{\\beta} \\|b - A\\beta \\|_2^2$ 的线性最小二乘问题，其中 $A = Z'X$ 且 $b = Z'y_{\\text{vec}}$。解由正规方程给出：$\\beta = (A^\\top A)^{-1} A^\\top b$。代回后，GMM 估计量为\n    $$\n    \\widehat{\\beta} = \\left( (X'Z)(Z'X) \\right)^{-1} (X'Z)(Z'y_{\\text{vec}}).\n    $$\n    对于非共线数据，该解析公式计算效率高且数值稳定，将用于估计 $\\widehat{\\beta}$。\n\n2.  **从结构模型进行数据模拟**\n    需要一个函数从结构模型 $y_t = \\theta y_{t-1} + \\varepsilon_t$ 生成时间序列数据。生成单个长度为 $T$ 的样本的步骤如下：\n    - 初始化一个长度为 $B+T+1$ 的路径，其中 $y_0 = 0$。\n    - 使用一个预生成的、长度为 $B+T$ 的独立同分布的标准正态创新项序列 $\\varepsilon_1, \\dots, \\varepsilon_{B+T}$。\n    - 从 $t=1$ 迭代到 $B+T$：$y_t = \\theta y_{t-1} + \\varepsilon_t$。\n    - 最终样本由最后 $T$ 个观测值组成，即 $\\{y_{B+1}, \\dots, y_{B+T}\\}$，这实际上是丢弃了最初的 B 个预烧期和 $y_0$。\n\n3.  **间接推断过程**\n    问题的核心是针对每个测试案例执行的四步过程：\n    - **步骤 A：“观测”数据与参数**。对于给定的测试案例 $i$，其真实参数为 $\\theta^\\star_i$，样本量为 $T_i$，种子为 $s^{\\text{obs}}_i$，生成单个“观测”时间序列 $y^{\\text{obs}}$。然后将 GMM 估计量应用于此序列以获得观测的辅助参数向量 $\\widehat{\\beta}^{\\text{obs}}$。\n    - **步骤 B：生成共同随机数**。对于模拟部分，使用种子 $s^{\\text{sim}}_i + 1, \\dots, s^{\\text{sim}}_i + S_i$ 预先生成 $S_i$ 组独立同分布的标准正态创新项，每组长度为 $B+T_i$。在评估网格 $\\mathcal{G}_i$ 中所有候选 $\\theta$ 值的 $Q(\\theta)$ 时，这些创新项组保持固定（共同）。这是一种方差缩减技术，可以提高估计量的稳定性。\n    - **步骤 C：评估准则函数 $Q(\\theta)$**。对于网格 $\\mathcal{G}_i$ 上的每个候选 $\\theta$，执行以下操作：\n        - 使用参数 $\\theta$ 和 $S_i$ 组预生成的创新项，通过结构模型模拟 $S_i$ 个时间序列。\n        - 对于 $S_i$ 个模拟序列中的每一个，使用解析 GMM 公式估计辅助参数向量 $\\widehat{\\beta}^{(s)}(\\theta)$。\n        - 计算平均辅助参数向量 $\\overline{\\beta}(\\theta) = \\frac{1}{S_i} \\sum_{s=1}^{S_i} \\widehat{\\beta}^{(s)}(\\theta)$。\n        - 间接推断准则计算为欧几里得距离的平方：$Q(\\theta) = \\|\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\|_2^2$。\n    - **步骤 D：网格搜索**。为 $\\mathcal{G}_i$ 中的每个 $\\theta$ 计算 $Q(\\theta)$ 的值。估计值 $\\widehat{\\theta}_i$ 是使 $Q(\\theta)$ 最小化的 $\\theta$ 值。指定的决胜规则（选择最小的 $\\theta$）可以通过按升序遍历网格并仅在找到严格更小的 $Q(\\theta)$ 值时更新最小值来自然处理。\n\n4.  **实现**\n    将使用 Python 中的 `numpy` 库进行数值运算来开发一个实现。一个主函数 `solve` 将协调整个过程。将创建以下辅助函数：\n    - 模拟 AR(1) 过程 (`generate_ar1`)。\n    - 估计 GMM 参数 (`gmm_estimator`)。\n    - 为单个测试案例运行整个过程 (`run_test_case`)。\n    主函数将遍历四个指定的测试案例，调用案例运行器，收集估计的 $\\widehat{\\theta}_i$，并按规定格式化最终输出。",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef generate_ar1(theta, T, B, innovations):\n    \"\"\"\n    Generates a time series from an AR(1) model.\n    \n    Args:\n        theta (float): The AR(1) parameter.\n        T (int): The number of observations in the final sample.\n        B (int): The number of burn-in periods.\n        innovations (np.ndarray): A vector of T+B standard normal innovations.\n\n    Returns:\n        np.ndarray: The simulated time series of length T.\n    \"\"\"\n    path_len = T + B\n    path = np.zeros(path_len + 1)  # path[0] is y_0\n    \n    for t in range(path_len):\n        path[t + 1] = theta * path[t] + innovations[t]\n        \n    return path[B + 1:] # Returns y_{B+1}, ..., y_{B+T} (length T)\n\ndef gmm_estimator(y):\n    \"\"\"\n    Computes the GMM estimator for the auxiliary model.\n    y_t = beta_0 + beta_1 * y_{t-1} + u_t\n    z_t = [1, y_{t-1}, y_{t-2}]\n    \n    Args:\n        y (np.ndarray): The time series data (y_1, ..., y_T), length T.\n        \n    Returns:\n        np.ndarray: The GMM estimate [beta_0_hat, beta_1_hat].\n    \"\"\"\n    T = len(y)\n    n = T - 2 # Number of observations for moment conditions (t=3 to T)\n    \n    if n = 0:\n        raise ValueError(\"Time series is too short for GMM estimation.\")\n\n    # y_vec corresponds to y_t for t in {3..T}\n    y_vec = y[2:T] # shape (n,)\n    \n    # X corresponds to [1, y_{t-1}] for t in {3..T}\n    X = np.zeros((n, 2))\n    X[:, 0] = 1.0\n    X[:, 1] = y[1:T-1] # y_2, ..., y_{T-1}\n    \n    # Z corresponds to [1, y_{t-1}, y_{t-2}] for t in {3..T}\n    Z = np.zeros((n, 3))\n    Z[:, 0] = 1.0\n    Z[:, 1] = y[1:T-1] # y_2, ..., y_{T-1}\n    Z[:, 2] = y[0:T-2] # y_1, ..., y_{T-2}\n    \n    # Analytical solution for GMM with W = I:\n    # beta_hat = ( (X'Z Z'X)^-1 ) * ( X'Z Z'y )\n    XT_Z = X.T @ Z\n    ZT_X = Z.T @ X\n    ZT_y = Z.T @ y_vec\n    \n    # Matrix to be inverted\n    M = XT_Z @ ZT_X\n    \n    try:\n        M_inv = np.linalg.inv(M)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if matrix is singular\n        M_inv = np.linalg.pinv(M)\n        \n    beta_hat = M_inv @ (XT_Z @ ZT_y)\n    \n    return beta_hat\n\ndef run_test_case(params):\n    \"\"\"\n    Performs the full indirect inference estimation for one test case.\n    \n    Args:\n        params (dict): A dictionary with all parameters for the test case.\n        \n    Returns:\n        float: The indirect inference estimate of theta.\n    \"\"\"\n    T, theta_star, S, delta, s_obs, s_sim = params.values()\n    B = 200\n\n    # 1. Generate \"observed\" data and estimate beta_obs\n    rng_obs = np.random.default_rng(s_obs)\n    innovs_obs = rng_obs.standard_normal(T + B)\n    y_obs = generate_ar1(theta_star, T, B, innovs_obs)\n    beta_obs = gmm_estimator(y_obs)\n\n    # 2. Pre-generate common random numbers for simulations\n    sim_innovations = []\n    for s in range(1, S + 1):\n        rng_sim = np.random.default_rng(s_sim + s)\n        sim_innovations.append(rng_sim.standard_normal(T + B))\n\n    # 3. Grid search for theta\n    grid_start = -0.95\n    grid_end = 0.95\n    num_points = int(round((grid_end - grid_start) / delta)) + 1\n    theta_grid = np.linspace(grid_start, grid_end, num_points)\n\n    min_Q = np.inf\n    best_theta = None\n\n    for theta_candidate in theta_grid:\n        beta_sim_list = []\n        for s in range(S):\n            y_sim = generate_ar1(theta_candidate, T, B, sim_innovations[s])\n            beta_sim = gmm_estimator(y_sim)\n            beta_sim_list.append(beta_sim)\n        \n        beta_bar = np.mean(beta_sim_list, axis=0)\n        \n        Q = np.sum((beta_obs - beta_bar)**2)\n        \n        # Tie-breaking: select the smallest theta, so only update on strictly smaller Q\n        if Q  min_Q:\n            min_Q = Q\n            best_theta = theta_candidate\n            \n    return best_theta\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'T': 300, 'theta_star': 0.6, 'S': 20, 'delta': 0.01, 's_obs': 1729, 's_sim': 20231},\n        {'T': 300, 'theta_star': 0.9, 'S': 25, 'delta': 0.01, 's_obs': 1730, 's_sim': 20232},\n        {'T': 150, 'theta_star': 0.0, 'S': 40, 'delta': 0.02, 's_obs': 1731, 's_sim': 20233},\n        {'T': 300, 'theta_star': -0.5, 'S': 20, 'delta': 0.01, 's_obs': 1732, 's_sim': 20234},\n    ]\n\n    results = []\n    for case in test_cases:\n        estimated_theta = run_test_case(case)\n        results.append(estimated_theta)\n\n    print(f\"[{','.join(f'{x:.4f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现在，让我们将间接推断的威力发挥到极致，用它来解决一个看似棘手的问题：混沌系统的参数估计。许多物理、生物和经济系统表现出复杂的非线性甚至混沌行为，其似然函数往往难以处理，使得传统估计方法失效。这个练习 () 将以著名的逻辑斯蒂映射（Logistic Map）为例，展示如何利用间接推断来估计驱动混沌动态的关键参数 $r$。我们将使用一个简单的自回归模型作为辅助模型，它就像一个“特征提取器”，捕捉混沌序列的某些统计特性。这个练习完美地诠释了间接推断的精髓：即使辅助模型是“错误”的，只要它对我们关心的结构参数是敏感的，它就能成为连接理论模型与现实数据的桥梁。",
            "id": "2401774",
            "problem": "构建一个独立的程序，实现一个基于间接推断 (Indirect Inference, II) 的估计器，用以恢复一个混沌离散时间模型的结构参数。真实数据生成过程 (DGP) 由含单个未知参数的逻辑斯谛映射 (logistic map) 定义。状态方程为\n$$\nx_{t+1} = r \\, x_t \\, (1 - x_t), \\quad t = 0,1,2,\\dots,T-1,\n$$\n初始条件 $x_0 \\in (0,1)$ 为固定的已知值，观测方程为\n$$\ny_t = x_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2), \\quad t = 0,1,\\dots,T-1,\n$$\n其中 $\\varepsilon_t$ 是独立同分布的高斯扰动项，其标准差 $\\sigma > 0$ 已知（也可能 $\\sigma=0$）。唯一未知的结构参数是 $r \\in \\mathbb{R}$。目标是使用间接推断 (II) 方法，通过匹配由观测数据计算出的辅助统计量与由结构模型生成的模拟数据计算出的辅助统计量来估计 $r$。\n\n将辅助模型定义为带截距项的一阶自回归模型 $AR(1)$：\n$$\ny_t = a_0 + a_1 \\, y_{t-1} + u_t, \\quad t=1,2,\\dots,T-1,\n$$\n其中 $u_t$ 是均值为零的残差。对于任意序列 $\\{y_t\\}_{t=0}^{T-1}$，定义辅助统计量向量为\n$$\n\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big),\n$$\n其中 $\\hat{a}_0(y)$ 和 $\\hat{a}_1(y)$ 是 $a_0$ 和 $a_1$ 的普通最小二乘 (OLS) 估计量，并且\n$$\n\\hat{s}_u(y) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} \\hat{u}_t(y)^2},\n$$\n其中 $\\hat{u}_t(y)$ 是 OLS 残差。$r$ 的间接推断估计量被定义为观测数据计算的辅助统计量与使用候选参数值的结构模型生成的模拟数据计算的辅助统计量之间二次距离的最小化子。设 $K \\in \\mathbb{N}$ 为用于求平均的独立模拟数据集的数量。对于一个候选参数 $r$，使用相同的样本量 $T$、相同的初始条件 $x_0$ 以及具有相同已知 $\\sigma$ 的高斯观测噪声，定义 $K$ 个模拟数据集 $\\{y^{(k)}(r)\\}_{k=1}^K$。将模拟辅助统计量的平均值定义为\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right).\n$$\n使用单位权重矩阵 $W = I_3$，准则函数为\n$$Q(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right)^{\\top} W \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right),$$\n其中 $y^{obs}$ 表示观测数据序列。间接推断估计量为\n$$\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r),$$\n在一个预先指定的有限网格 $\\mathcal{R}$ 上。\n\n您的程序必须严格按照上述定义实现该估计器，并采用以下固定的设计元素以确保确定性和可复现性：\n\n- 参数网格：\n$$\n\\mathcal{R} = \\left\\{ 3.50 + 0.0025 \\, j \\,:\\, j = 0,1,2,\\dots,200 \\right\\}.\n$$\n- 模拟重复次数：$K = 15$。\n- 初始条件：$x_0 = 0.123456789$。\n- 权重矩阵：$W = I_3$。\n- 为了在不同候选值 $r \\in \\mathcal{R}$ 之间保证可复现性，对模拟数据集使用共同随机数 (common random numbers)：对于每个测试案例，使用一个带有固定种子 $s_{sim}$ 的伪随机数生成器，生成恰好 $K$ 个长度为 $T$ 的独立高斯噪声序列，并对所有 $r \\in \\mathcal{R}$ 重复使用这 $K$ 个序列。对于观测数据，使用一个带有固定种子 $s_{obs}$ 的伪随机数生成器生成其高斯噪声序列。在所有情况下，将 $\\varepsilon_t$ 抽样为独立的 $\\mathcal{N}(0,\\sigma^2)$ 变量。初始条件 $x_0$ 在所有模拟中必须保持相同。\n\n测试套件。为以下每个测试案例实现并求解该估计器，这些案例中指定了真实的结构参数、样本量、噪声水平和种子：\n\n- 案例 A (一般情况)：$r^{\\star} = 3.8000$, $T = 1000$, $\\sigma = 0.0200$, $s_{obs} = 1729$, $s_{sim} = 2468$。\n- 案例 B (接近混沌边缘，较低噪声)：$r^{\\star} = 3.5700$, $T = 800$, $\\sigma = 0.0100$, $s_{obs} = 1730$, $s_{sim} = 2469$。\n- 案例 C (最大混沌，无测量噪声)：$r^{\\star} = 4.0000$, $T = 1200$, $\\sigma = 0.0000$, $s_{obs} = 1731$, $s_{sim} = 2470$。\n- 案例 D (较短序列，较高噪声)：$r^{\\star} = 3.9500$, $T = 300$, $\\sigma = 0.0500$, $s_{obs} = 1732$, $s_{sim} = 2471$。\n\n对于每个案例，通过使用指定的 $r^{\\star}$、$x_0$ 和 $T$ 模拟逻辑斯谛映射，并使用指定的 $s_{obs}$ 添加具有指定 $\\sigma$ 的高斯噪声，来生成观测数据 $y^{obs}$。然后，如上文定义，在网格 $\\mathcal{R}$ 上计算间接推断估计量 $\\hat{r}$，每个网格点使用 $K$ 个模拟数据集，这些数据集通过来自指定 $s_{sim}$ 的共同随机数生成。\n\n最终输出格式。您的程序应生成单行输出，包含按顺序排列的案例 A–D 的四个估计值 $\\hat{r}$，形式为方括号内以逗号分隔的列表。每个值必须打印为小数，并精确到小数点后六位。例如，输出格式为“[rA,rB,rC,rD]”，其中 $rA$、$rB$、$rC$ 和 $rD$ 是四个四舍五入后的估计值。不涉及单位，且不应打印任何额外文本。",
            "solution": "该问题要求根据间接推断 (II) 的定义构建一个估计器，并将其应用于一个混沌结构模型。结构性 DGP 是带观测噪声的逻辑斯谛映射。该估计器由以下要素定义：\n\n1. 结构模型。状态方程为 $x_{t+1} = r \\, x_t \\, (1 - x_t)$，$t = 0,1,\\dots,T-1$，其中 $x_0 = 0.123456789$。观测数据为 $y_t = x_t + \\varepsilon_t$，其中对于每个 $t$，$\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ 且独立。\n\n2. 辅助模型。辅助模型是带截距项的 $AR(1)$ 模型：$y_t = a_0 + a_1 \\, y_{t-1} + u_t$，$t = 1,\\dots,T-1$。对于任意序列 $y = (y_0,\\dots,y_{T-1})$，使用正规方程定义 OLS 估计值。令 $Y = (y_1,\\dots,y_{T-1})^{\\top}$，$X$ 为一个 $(T-1) \\times 2$ 矩阵，其第一列为全1，第二列为 $(y_0,\\dots,y_{T-2})^{\\top}$。OLS 估计量为\n$$\n\\hat{\\beta}(y) = \\begin{pmatrix} \\hat{a}_0(y) \\\\ \\hat{a}_1(y) \\end{pmatrix} = (X^{\\top}X)^{-1} X^{\\top} Y,\n$$\n残差为 $\\hat{u}(y) = Y - X \\hat{\\beta}(y)$。残差离散度统计量为\n$$\n\\hat{s}_u(y) = \\sqrt{ \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\hat{u}_t(y)^2 }.\n$$\n将辅助统计量收集为 $\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big)$。\n\n3. 间接推断准则。对于一个候选参数 $r$，使用结构模型并以初始状态 $x_0$ 和已知 $\\sigma$ 的高斯噪声，模拟 $K = 15$ 个长度为 $T$ 的数据集 $y^{(k)}(r)$。计算跨模拟的辅助统计量的平均值，\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right),\n$$\n并将使用单位权重矩阵 $W = I_3$ 的二次距离定义为\n$$\nQ(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right)^{\\top} \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right).\n$$\n估计量是 $\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r)$，在有限网格 $\\mathcal{R} = \\{3.50 + 0.0025 \\, j : j = 0,1,\\dots,200\\}$ 上。\n\n4. 可复现性与共同随机数。对于每个测试案例，固定观测噪声种子 $s_{obs}$ 以生成单个观测序列 $\\{\\varepsilon_t^{obs}\\}$, 并固定模拟种子 $s_{sim}$ 以生成恰好 $K = 15$ 个独立的高斯噪声序列 $\\{\\varepsilon_t^{(k)}\\}_{k=1}^K$。对所有候选参数 $r \\in \\mathcal{R}$ 使用这 $K$ 个相同的序列。这实现了共同随机数，从而使得 $Q(r)$ 的变化是由于结构动态学的改变，而不是模拟噪声的变化。初始条件 $x_0 = 0.123456789$ 在所有模拟中保持相同。\n\n5. 与第一性原理一致的实施计划。对于每个测试案例：\n- 首先用 $x_0$ 模拟 $x_{t+1} = r^{\\star} x_t (1-x_t)$，然后对 $t = 0,\\dots,T-1$ 添加 $\\varepsilon_t^{obs} \\sim \\mathcal{N}(0,\\sigma^2)$，从而生成观测序列 $y^{obs}$。\n- 通过上述 OLS 公式计算 $\\hat{b}(y^{obs})$。\n- 使用指定的 $s_{sim}$ 生成 $K$ 个独立的、长度均为 $T$ 的高斯噪声序列。对于每个候选参数 $r \\in \\mathcal{R}$，使用 $x_0$ 从逻辑斯谛映射模拟结构状态路径 $x(r)$，通过 $y^{(k)}(r) = x(r) + \\varepsilon^{(k)}$ 构建 $K$ 个模拟数据集，为每个 $k$ 计算 $\\hat{b}\\!\\left(y^{(k)}(r)\\right)$，得到 $\\bar{b}(r)$，评估 $Q(r)$，并在 $\\mathcal{R}$ 上选择最小化子。如果由于数值上的平局而存在多个最小化子，则选择 $\\mathcal{R}$ 中达到最小值的最小 $r$，这是一个明确定义的规则。\n\n6. 数值和统计考量。对于 $r \\in [3.50,4.00]$，逻辑斯谛映射表现出复杂且通常是混沌的行为。间接推断利用辅助模型来比较观测数据和模拟数据之间的显著特征（截距、持续性、残差离散度）。在一组使用共同随机数的模拟中匹配这些特征，可以得到一个指导 $r$ 选择的准则。有限网格 $\\mathcal{R}$ 定义了对连续参数空间的筛分式 (sieve-type) 近似；网格分辨率 $0.0025$ 意味着在理想条件下，$\\hat{r}$ 的可达精度在 $0.0025$ 以内。测量噪声 $\\sigma$ 和样本量 $T$ 的存在会影响辅助统计量的变异性，从而影响 $\\hat{r}$ 的精度。\n\n7. 输出。最终程序按 A, B, C, D 的顺序计算四个指定案例的 $\\hat{r}$，并打印单行内容，该行包含一个由方括号括起来的、以逗号分隔的列表，其中是四个四舍五入到六位小数的估计值，且无任何额外输出。\n\n本解决方案直接应用了间接推断的定义，具体方法是指定辅助统计量、模拟框架以及在预定网格上的最小化过程，确保每一步都基于数学公式，并通过固定的种子和共同随机数确保结果的可复现性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_logistic_path(r: float, T: int, x0: float) -> np.ndarray:\n    \"\"\"\n    Simulate the logistic map x_{t+1} = r x_t (1 - x_t) for t=0,...,T-2 with x_0 = x0.\n    Returns an array x of length T with x[0]=x0.\n    \"\"\"\n    x = np.empty(T, dtype=float)\n    x[0] = x0\n    for t in range(T - 1):\n        x[t + 1] = r * x[t] * (1.0 - x[t])\n    return x\n\ndef auxiliary_stats_ar1(y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute auxiliary statistics from AR(1) with intercept:\n    y_t = a0 + a1 y_{t-1} + u_t, t=1..T-1.\n    Returns [a0_hat, a1_hat, s_u_hat] where s_u_hat = sqrt( (1/(T-1)) sum u_t^2 ).\n    \"\"\"\n    # Ensure 1D float array\n    y = np.asarray(y, dtype=float).ravel()\n    if y.size  2:\n        # Degenerate case: not enough observations; return NaNs (should not occur in this problem)\n        return np.array([np.nan, np.nan, np.nan], dtype=float)\n    y_curr = y[1:]\n    y_lag = y[:-1]\n    # Design matrix with intercept\n    X = np.column_stack((np.ones_like(y_lag), y_lag))\n    # OLS via normal equations\n    XtX = X.T @ X\n    XtY = X.T @ y_curr\n    beta = np.linalg.solve(XtX, XtY)\n    resid = y_curr - X @ beta\n    s_u = np.sqrt(np.mean(resid ** 2))\n    return np.array([beta[0], beta[1], s_u], dtype=float)\n\ndef indirect_inference_estimate_r(\n    r_grid: np.ndarray,\n    K: int,\n    x0: float,\n    T: int,\n    sigma: float,\n    obs_seed: int,\n    sim_seed: int,\n    r_true: float\n) -> float:\n    \"\"\"\n    Compute the indirect inference estimate of r over the given grid r_grid.\n    - Generate observed data using r_true, x0, T, sigma with RNG seed obs_seed.\n    - Generate K simulation noise sequences using seed sim_seed (common random numbers).\n    - For each r in r_grid, simulate x path and form K simulated datasets by adding the fixed noise sequences.\n    - Compute auxiliary statistics for observed and average over simulated datasets.\n    - Minimize squared Euclidean distance between observed and average simulated auxiliary stats.\n    Returns the minimizing r (tie broken by smallest r).\n    \"\"\"\n    # Generate observed data\n    x_obs = simulate_logistic_path(r_true, T, x0)\n    rng_obs = np.random.default_rng(obs_seed)\n    eps_obs = rng_obs.normal(loc=0.0, scale=sigma, size=T)\n    y_obs = x_obs + eps_obs\n    b_obs = auxiliary_stats_ar1(y_obs)\n\n    # Pre-generate K noise sequences (common random numbers)\n    rng_sim = np.random.default_rng(sim_seed)\n    eps_bank = rng_sim.normal(loc=0.0, scale=sigma, size=(K, T))\n\n    # For each r, compute Q(r)\n    best_r = None\n    best_Q = np.inf\n\n    # Preallocate buffer for speed\n    # Loop over candidate r\n    for r in r_grid:\n        # Simulate state path once for this r\n        x_sim = simulate_logistic_path(r, T, x0)\n        # Add fixed noise sequences to produce K simulated datasets\n        # Each y_k has shape (T,)\n        Q_components = []\n        b_sum = np.zeros(3, dtype=float)\n        # Loop over K replications\n        for k in range(K):\n            y_k = x_sim + eps_bank[k]\n            b_k = auxiliary_stats_ar1(y_k)\n            b_sum += b_k\n        b_bar = b_sum / K\n        diff = b_obs - b_bar\n        Q_r = float(diff @ diff)  # Identity weighting\n        # Update best\n        if Q_r  best_Q - 1e-18 or (np.isclose(Q_r, best_Q) and (best_r is None or r  best_r)):\n            best_Q = Q_r\n            best_r = r\n\n    return float(best_r)\n\ndef solve():\n    # Fixed design elements\n    x0 = 0.123456789\n    K = 15\n    # Grid: {3.50 + 0.0025 * j, j=0..200}\n    r_grid = 3.50 + 0.0025 * np.arange(201, dtype=float)\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (r_true, T, sigma, s_obs, s_sim)\n    test_cases = [\n        (3.8000, 1000, 0.0200, 1729, 2468),  # Case A\n        (3.5700,  800, 0.0100, 1730, 2469),  # Case B\n        (4.0000, 1200, 0.0000, 1731, 2470),  # Case C\n        (3.9500,  300, 0.0500, 1732, 2471),  # Case D\n    ]\n\n    results = []\n    for r_true, T, sigma, s_obs, s_sim in test_cases:\n        r_hat = indirect_inference_estimate_r(\n            r_grid=r_grid,\n            K=K,\n            x0=x0,\n            T=T,\n            sigma=sigma,\n            obs_seed=s_obs,\n            sim_seed=s_sim,\n            r_true=r_true\n        )\n        # Round to 6 decimals in final output formatting\n        results.append(r_hat)\n\n    # Format each result to exactly 6 decimal places\n    formatted = \",\".join(f\"{val:.6f}\" for val in results)\n    # Final print statement in the exact required format.\n    print(f\"[{formatted}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}