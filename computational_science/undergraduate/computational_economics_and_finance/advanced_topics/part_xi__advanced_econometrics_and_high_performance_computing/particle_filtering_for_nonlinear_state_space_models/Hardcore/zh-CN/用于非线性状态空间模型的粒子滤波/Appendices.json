{
    "hands_on_practices": [
        {
            "introduction": "本节练习将引导你从零开始构建一个粒子滤波器，并将其应用于金融领域中一个经典的模型。我们将使用一个随机波动率模型来追踪一项资产回报背后潜在的、不可观测的波动率。通过对比两种不同的初始先验分布——一个宽泛的先验和一个尖锐但错误的先验——来评估它们对滤波器性能的影响，这将帮助你深入理解先验信息在状态估计中的关键作用 。这项实践不仅能巩固你对自举粒子滤波器（bootstrap particle filter）核心机制的理解，还能培养你在实际应用中处理不确定性的直觉。",
            "id": "2418266",
            "problem": "实现并分析一个序列蒙特卡洛（SMC）粒子滤波器，该滤波器用于一个与计算金融相关的非线性状态空间模型，并量化估计性能对初始粒子分布 $p(x_0)$ 的敏感性。您必须比较一个扩散先验和一个尖峰但错误的先验。您的程序必须是一个完整的、可运行的脚本，它能模拟数据、运行滤波器，并为一个小型测试套件输出量化指标。\n\n潜状态模型是一个常用于金融收益率的随机波动率模型：\n- 状态转移（潜在对数波动率）：$x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,1)$ 在 $t$ 上独立。\n- 观测（收益率）：$y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$，在给定 $x_t$ 的条件下，在 $t$ 上独立。\n\n假设以下基础：\n- 潜在马尔可夫过程以及如上所述的给定状态下观测值的条件独立性。\n- 目标是使用粒子滤波器来近似滤波分布序列 $p(x_t \\mid y_{1:t})$。\n- 将要实现的粒子滤波器是自举粒子滤波器（bootstrap particle filter），它使用转移密度作为提议分布。\n\n需要执行的任务：\n- 为每个测试案例从模型中模拟一个数据集 $\\{(x_t, y_t)\\}_{t=1}^T$。从转移动态所蕴含的平稳分布中初始化 $x_0$，即 $x_0 \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$，然后对 $t \\in \\{1,\\dots,T\\}$ 向前生成 $(x_t, y_t)$。\n- 实现具有 $N$ 个粒子、系统性重采样以及有效样本量阈值为 $N/2$ 的自举粒子滤波器。\n- 使用对数权重以避免数值下溢，并计算每个时间点 $t$ 的滤波均值 $\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$，其中 $w_t^{(i)}$ 是时间 $t$ 的归一化权重。\n- 对于每个测试案例，在相同的模拟数据上运行两次粒子滤波器：\n  1. 扩散先验：$x_0^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$，其中 $m_0 = \\mu$ 且 $s_0 = 3.0$。\n  2. 尖峰但错误的先验：$x_0^{(i)} \\sim \\mathcal{N}(m_b, s_b^2)$，其中 $m_b = \\mu + 2\\,\\sigma_x$ 且 $s_b = 0.05$，其中 $\\sigma_x = \\sigma_v / \\sqrt{1-\\phi^2}$ 是 $x_t$ 的平稳标准差。\n- 对于每次运行，计算前 $H$ 步的均方根误差：\n$$\\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}.$$\n- 对于每个测试案例，计算比率\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}},$$\n因此 $R > 1$ 表示扩散先验比尖峰但错误的先验在早期阶段产生更低的误差。\n\n算法要求：\n- 使用系统性重采样。\n- 使用有效样本量 $ESS = \\left(\\sum_{i=1}^N (w_t^{(i)})^2 \\right)^{-1}$ 并在 $ESS  N/2$ 时触发重采样。\n- 内部使用对数权重以保证数值稳定性。\n- 所有模拟和滤波器必须由具有如下指定固定种子的伪随机数生成器驱动，以确保确定性。\n\n测试套件：\n- 对于下面的每个参数集，使用给定的种子模拟一次数据，然后使用单独的固定种子运行两个滤波器（扩散先验和尖峰先验）。对于每个案例，报告比率 $R$。\n- 案例 1（基线持续性和大量粒子）：\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$。\n  - 数据模拟种子：$2023001$。\n  - 粒子滤波器种子：扩散 $= 9020001$，尖峰 $= 9020002$。\n- 案例 2（少量粒子，更高的权重退化风险）：\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 150$。\n  - 数据模拟种子：$2023002$。\n  - 粒子滤波器种子：扩散 $= 9020011$，尖峰 $= 9020012$。\n- 案例 3（高持续性，对 $p(x_0)$ 更强的敏感性）：\n  - $\\mu = -0.7$, $\\phi = 0.985$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$。\n  - 数据模拟种子：$2023003$。\n  - 粒子滤波器种子：扩散 $= 9020021$，尖峰 $= 9020022$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序与测试套件案例的顺序完全一致：$[R_1,R_2,R_3]$。\n- 每个 $R_j$ 必须是精确到六位小数的浮点数。\n- 不应打印任何其他文本。\n\n此问题不涉及物理单位。所有角度（如果在随机数生成中隐式出现）均不相关；不需要角度单位。\n\n您的程序必须是自包含的，无需输入，并且在指定种子下可复现。它必须实现所有必需的步骤，不依赖任何外部数据。输出必须完全由上述规范和您的确定性伪随机数使用方式决定。",
            "solution": "问题陈述已被解析和验证。它被认为是具有科学依据、适定、客观和完整的。它描述了一个标准的计算统计任务：将序列蒙特卡洛（SMC）方法应用于金融计量经济学中常见的状态空间模型。所有参数、算法和评估指标都得到了明确的规定。因此，该问题被视为有效，并将构建一个解决方案。\n\n该问题要求实现一个自举粒子滤波器，以估计随机波动率模型中的潜在对数波动率 $x_t$。该模型由两个方程定义：\n\n$1$. 潜在对数波动率过程 $x_t$ 的状态转移方程。这是一个一阶自回归过程，AR($1$)：\n$$x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1)$$\n在这里，$\\mu$ 是过程的长期均值，$\\phi$ 是持续性参数，$\\sigma_v$ 是对数波动率的波动率。为使过程平稳，要求 $|\\phi|  1$。该过程的平稳分布是高斯分布：\n$$x_t \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$$\n\n$2$. 金融收益率 $y_t$ 的观测方程。给定当前对数波动率，收益率是条件独立的，并遵循均值为 $0$、方差为 $\\exp(x_t)$ 的正态分布：\n$$y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$$\n这种表述捕捉了众所周知的波动率聚集效应，即大（小）的收益率很可能跟随大（小）的收益率。\n\n目标是近似 $t = 1, \\dots, T$ 的滤波分布 $p(x_t \\mid y_{1:t})$。自举粒子滤波器通过顺序更新一组 $N$ 个加权随机样本（或称“粒子”）$\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 来实现这一目标。这些粒子代表了滤波分布的离散近似。该算法对每个时间步 $t=1, \\dots, T$ 进行迭代。\n\n本文实现的算法遵循这些标准步骤，从近似 $p(x_{t-1} \\mid y_{1:t-1})$ 的一组粒子 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 及其相关的归一化权重 $\\{w_{t-1}^{(i)}\\}_{i=1}^N$ 开始：\n\n**步骤 1：重采样。** 权重退化是粒子滤波器中的一个常见问题，即在几次迭代后，一个粒子的权重会接近 $1$，而所有其他粒子的权重接近 $0$。为缓解此问题，我们计算有效样本量 $ESS = \\left(\\sum_{i=1}^N (w_{t-1}^{(i)})^2 \\right)^{-1}$。如果 $ESS$ 低于指定为 $N/2$ 的阈值，则执行重采样步骤。使用了系统性重采样，这是一种高效的方差缩减技术。它涉及从现有集合 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 中有放回地选择 $N$ 个新粒子，其中选择粒子 $i$ 的概率是其权重 $w_{t-1}^{(i)}$。重采样后，新的粒子集是未加权的，因此所有权重都重置为 $w_{t-1}^{(i)} = 1/N$。\n\n**步骤 2：传播。** 每个粒子根据状态转移动态向前演化。由于使用的是自举滤波器，提议分布就是状态转移密度本身。对于每个粒子 $i$，抽取一个新状态：\n$$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathcal{N}\\left(\\mu + \\phi(x_{t-1}^{(i)} - \\mu), \\sigma_v^2\\right)$$\n这产生了一个新的粒子集 $\\{x_t^{(i)}\\}_{i=1}^N$，它代表了时间 $t$ 的先验分布 $p(x_t \\mid y_{1:t-1})$。\n\n**步骤 3：加权。** 更新重要性权重以纳入新的观测值 $y_t$。每个粒子的更新（未归一化）权重是其先前权重与给定新粒子状态 $x_t^{(i)}$ 下观测值 $y_t$ 的似然的乘积：\n$$\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\cdot p(y_t \\mid x_t^{(i)})$$\n似然 $p(y_t \\mid x_t^{(i)})$ 由在 $y_t$ 处求值的 $\\mathcal{N}(0, \\exp(x_t^{(i)}))$ 的概率密度函数给出。为了数值稳定性，计算在对数域中执行：\n$$\\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\log p(y_t \\mid x_t^{(i)})$$\n其中 $\\log p(y_t \\mid x_t^{(i)}) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}x_t^{(i)} - \\frac{y_t^2}{2\\exp(x_t^{(i)})}$。\n\n**步骤 4：归一化和估计。** 未归一化的对数权重被归一化以确保它们的和为一。首先，为防止数值溢出，在取指数之前减去最大对数权重。归一化的权重为：\n$$w_t^{(i)} = \\frac{\\exp(\\log \\tilde{w}_t^{(i)})}{\\sum_{j=1}^N \\exp(\\log \\tilde{w}_t^{(j)})}$$\n然后，时间 $t$ 的状态均值的滤波估计被计算为粒子的加权平均值：\n$$\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$$\n\n分析要求比较滤波器在两种不同初始粒子分布 $p(x_0)$ 下的性能。一个“扩散”先验，$\\mathcal{N}(\\mu, 3.0^2)$，以真实平稳均值为中心但具有高方差。这反映了较弱的先验知识。一个“尖峰但错误”的先验，$\\mathcal{N}(\\mu + 2\\sigma_x, 0.05^2)$，其中 $\\sigma_x$ 是平稳标准差，反映了强烈但错误的置信度。该比较通过初始时间范围 $H$ 内的均方根误差（$\\mathrm{RMSE}$）的比率 $R$ 来量化：\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}}, \\quad \\mathrm{where} \\quad \\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}$$\n比率 $R > 1$ 表示在滤波的初始阶段，扩散先验的性能更优。这是预料之中的，因为一个尖锐且错误的先验可能会误导滤波器，尤其是在状态演化缓慢的高度持续性过程中，需要更多时间让数据来纠正初始误差。实现将遵循这些原则，使用指定的参数和随机种子以确保可复现性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef systematic_resample(weights, rng):\n    \"\"\"\n    Performs systematic resampling.\n\n    Args:\n        weights (np.ndarray): Array of normalized particle weights.\n        rng (np.random.Generator): A numpy random number generator.\n\n    Returns:\n        np.ndarray: Indices of resampled particles.\n    \"\"\"\n    N = len(weights)\n    # Generate N ordered pointers from a single random draw\n    u = rng.uniform(0.0, 1.0 / N)\n    positions = u + np.arange(N) / N\n    \n    # Calculate cumulative sum of weights\n    cumulative_weights = np.cumsum(weights)\n    \n    # Find indices of particles to keep\n    indices = np.searchsorted(cumulative_weights, positions)\n    return indices\n\ndef simulate_data(mu, phi, sigma_v, T, seed):\n    \"\"\"\n    Simulates data from the stochastic volatility model.\n\n    Args:\n        mu (float): Long-run mean of log-volatility.\n        phi (float): Persistence parameter.\n        sigma_v (float): Volatility of log-volatility.\n        T (int): Number of time steps.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: True states x (T+1) and observations y (T).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    \n    # Stationary distribution for x_0\n    if abs(phi)  1:\n        sigma_x_sq = sigma_v**2 / (1 - phi**2)\n        sigma_x = np.sqrt(sigma_x_sq)\n        x[0] = mu + sigma_x * rng.standard_normal()\n    else: # Handle non-stationary case if necessary, here we assume it's stationary.\n        x[0] = mu\n        \n    for t in range(1, T + 1):\n        # State transition\n        x[t] = mu + phi * (x[t-1] - mu) + sigma_v * rng.standard_normal()\n        \n        # Observation\n        obs_std_dev = np.exp(x[t] / 2.0)\n        y[t-1] = obs_std_dev * rng.standard_normal()\n        \n    return x, y\n\ndef run_particle_filter(y_data, mu, phi, sigma_v, N, m_prior, s_prior, seed, H):\n    \"\"\"\n    Runs the bootstrap particle filter for the stochastic volatility model.\n\n    Args:\n        y_data (np.ndarray): Array of observations.\n        mu, phi, sigma_v (float): Model parameters.\n        N (int): Number of particles.\n        m_prior, s_prior (float): Mean and std dev for the initial particle distribution.\n        seed (int): Seed for the filter's random number generator.\n        H (int): Horizon for RMSE calculation (unused in function, for context).\n\n    Returns:\n        np.ndarray: Array of filtered state mean estimates.\n    \"\"\"\n    T = len(y_data)\n    rng = np.random.default_rng(seed)\n    \n    # Initialization (t=0)\n    # This represents the particle approximation of p(x_0)\n    particles_tm1 = rng.normal(loc=m_prior, scale=s_prior, size=N)\n    weights_tm1 = np.full(N, 1.0 / N)\n    \n    estimates = np.zeros(T)\n    \n    # Use precomputed constant for log-likelihood\n    LOG_2PI_HALF = 0.5 * np.log(2 * np.pi)\n\n    # Main loop for t=1,...,T (Python index 0 to T-1)\n    for t in range(T):\n        y_obs = y_data[t]\n        \n        # --- Step 1: Resampling (based on weights from t-1) ---\n        ess = 1.0 / np.sum(weights_tm1**2)\n        if ess  N / 2.0:\n            indices = systematic_resample(weights_tm1, rng)\n            particles_tm1 = particles_tm1[indices]\n            # After resampling, weights are implicitly reset\n            log_weights_tm1 = np.full(N, -np.log(N))\n        else:\n            log_weights_tm1 = np.log(weights_tm1)\n\n        # --- Step 2: Propagation (from t-1 to t) ---\n        noise = rng.standard_normal(N)\n        particles_t = mu + phi * (particles_tm1 - mu) + sigma_v * noise\n\n        # --- Step 3: Weighting (using observation y_t) ---\n        log_var = particles_t\n        log_likelihoods = -LOG_2PI_HALF - 0.5 * log_var - (y_obs**2) / (2.0 * np.exp(log_var))\n        \n        # Update log-weights\n        unnorm_log_weights_t = log_weights_tm1 + log_likelihoods\n        \n        # Normalize weights for stability\n        max_log_weight = np.max(unnorm_log_weights_t)\n        temp_weights = np.exp(unnorm_log_weights_t - max_log_weight)\n        weights_t = temp_weights / np.sum(temp_weights)\n\n        # --- Step 4: Estimation ---\n        estimates[t] = np.sum(weights_t * particles_t)\n\n        # --- Step 5: Prepare for next iteration ---\n        particles_tm1 = particles_t\n        weights_tm1 = weights_t\n        \n    return estimates\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023001, 'pf_seeds': {'diffuse': 9020001, 'peaked': 9020002}},\n        # Case 2\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 150,\n         'data_seed': 2023002, 'pf_seeds': {'diffuse': 9020011, 'peaked': 9020012}},\n        # Case 3\n        {'mu': -0.7, 'phi': 0.985, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023003, 'pf_seeds': {'diffuse': 9020021, 'peaked': 9020022}},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mu, phi, sigma_v, T, H, N = case['mu'], case['phi'], case['sigma_v'], case['T'], case['H'], case['N']\n        \n        # Simulate data once for the case\n        x_true, y_data = simulate_data(mu, phi, sigma_v, T, case['data_seed'])\n\n        # Define prior parameters\n        # Diffuse prior\n        m0_diffuse = mu\n        s0_diffuse = 3.0\n        \n        # Peaked, incorrect prior\n        sigma_x = sigma_v / np.sqrt(1 - phi**2)\n        m0_peaked = mu + 2.0 * sigma_x\n        s0_peaked = 0.05\n        \n        # Run filters\n        estimates_diffuse = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_diffuse, s0_diffuse, case['pf_seeds']['diffuse'], H\n        )\n        estimates_peaked = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_peaked, s0_peaked, case['pf_seeds']['peaked'], H\n        )\n        \n        # Compute RMSE over the first H steps\n        # x_true[0] is x_0, x_true[1:H+1] corresponds to y_data[0:H]\n        errors_diffuse = estimates_diffuse[:H] - x_true[1:H+1]\n        rmse_diffuse = np.sqrt(np.mean(errors_diffuse**2))\n        \n        errors_peaked = estimates_peaked[:H] - x_true[1:H+1]\n        rmse_peaked = np.sqrt(np.mean(errors_peaked**2))\n        \n        # Compute the ratio R\n        ratio_R = rmse_peaked / rmse_diffuse\n        results.append(f\"{ratio_R:.6f}\")\n\n    # Print final output in the required format\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```"
        },
        {
            "introduction": "在掌握了粒子滤波器的基本实现后，我们将其应用到一个更高级的统计推断任务中：模型参数估计。本练习将探讨一个带有非线性关系的菲利普斯曲线模型，该模型在宏观经济学中用于描述通货膨胀与失业率之间的关系。由于模型的非线性，其似然函数没有解析解，我们将利用粒子滤波器来近似计算在不同参数下的似然值，从而找到最大似然估计 。完成这项练习将让你体会到，当卡尔曼滤波器等标准方法失效时，粒子滤波器如何成为分析复杂经济模型的强大工具。",
            "id": "2418262",
            "problem": "考虑一个具有时变非加速通货膨胀失业率（NAIRU）的非线性菲利普斯曲线。设未观测到的状态为NAIRU，记作 $n_t$，观测到的失业率为 $u_t$，观测到的通货膨胀率为 $\\pi_t$。该模型是一个由以下公式定义的非线性状态空间系统：\n$$\n\\pi_t = \\alpha + \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t,\n$$\n$$\nn_t - \\mu = \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t,\n$$\n其中 $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$，$\\eta_t \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$，且 $\\alpha = 0$。初始状态 $n_0$ 根据状态方程所蕴含的平稳分布进行分布，即：\n$$\nn_0 \\sim \\mathcal{N}\\!\\left(\\mu,\\;\\frac{\\sigma_\\eta^2}{1-\\rho^2}\\right).\n$$\n您的任务是为三种不同的参数配置（测试用例）模拟数据，然后对每个测试用例，在指定的有限网格上计算参数向量 $\\theta = (\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$ 的最大似然估计，同时将 $\\alpha=0$、$\\mu$ 和 $\\rho$ 固定在每个测试用例中指定的值。\n\n数据生成必须按以下步骤进行。\n\n- 共同的观测失业过程 $u_t$：\n  - 时间长度 $T = 120$。\n  - 自回归过程 $u_t = \\mu_u + \\phi\\,(u_{t-1}-\\mu_u) + \\nu_t$，其中 $\\nu_t \\sim \\mathcal{N}(0,\\sigma_u^2)$。\n  - 参数：$\\mu_u = 5.5$，$\\phi = 0.8$，$\\sigma_u = 0.3$，初始值 $u_0 = \\mu_u$。\n  - 使用伪随机种子 $100$ 来生成 $\\{u_t\\}_{t=1}^T$。\n- 对于每个测试用例 $i \\in \\{1,2,3\\}$，使用上述模型和特定于该测试用例的真实参数来模拟潜在状态 $\\{n_t\\}_{t=1}^T$ 和通货膨胀率 $\\{\\pi_t\\}_{t=1}^T$。对于每个测试用例 $i$，使用伪随机种子 $200+i$ 来生成状态方程和测量方程中的新息。\n\n测试套件由以下三个案例定义，每个案例都提供了用于数据模拟的固定值 $(\\mu,\\rho)$ 和真实值 $(\\beta^\\star,\\gamma^\\star,\\sigma_\\varepsilon^\\star,\\sigma_\\eta^\\star)$，以及您必须在其上计算并最大化似然函数的有限网格。\n\n- 案例 1（一般情况）：\n  - 固定值：$\\mu = 5.5$，$\\rho = 0.95$。\n  - 真实模拟参数：$\\beta^\\star = -0.5$，$\\gamma^\\star = 0.06$，$\\sigma_\\varepsilon^\\star = 0.2$，$\\sigma_\\eta^\\star = 0.1$。\n  - 估计网格：\n    - $\\beta \\in \\{-0.6,\\,-0.5\\}$，\n    - $\\gamma \\in \\{0.04,\\,0.06\\}$，\n    - $\\sigma_\\varepsilon \\in \\{0.18,\\,0.22\\}$，\n    - $\\sigma_\\eta \\in \\{0.09,\\,0.11\\}$。\n- 案例 2（近线性菲利普斯曲线，小状态噪声）：\n  - 固定值：$\\mu = 5.5$，$\\rho = 0.9$。\n  - 真实模拟参数：$\\beta^\\star = -0.4$，$\\gamma^\\star = 0.0$，$\\sigma_\\varepsilon^\\star = 0.15$，$\\sigma_\\eta^\\star = 0.02$。\n  - 估计网格：\n    - $\\beta \\in \\{-0.5,\\,-0.4\\}$，\n    - $\\gamma \\in \\{0.0,\\,0.02\\}$，\n    - $\\sigma_\\varepsilon \\in \\{0.12,\\,0.18\\}$，\n    - $\\sigma_\\eta \\in \\{0.01,\\,0.03\\}$。\n- 案例 3（高持久性NAIRU和更强的非线性）：\n  - 固定值：$\\mu = 5.5$，$\\rho = 0.98$。\n  - 真实模拟参数：$\\beta^\\star = -0.6$，$\\gamma^\\star = 0.2$，$\\sigma_\\varepsilon^\\star = 0.25$，$\\sigma_\\eta^\\star = 0.15$。\n  - 估计网格：\n    - $\\beta \\in \\{-0.7,\\,-0.6\\}$，\n    - $\\gamma \\in \\{0.18,\\,0.2\\}$，\n    - $\\sigma_\\varepsilon \\in \\{0.22,\\,0.28\\}$，\n    - $\\sigma_\\eta \\in \\{0.13,\\,0.17\\}$。\n\n对于每个测试用例，给定观测对 $\\{(u_t,\\pi_t)\\}_{t=1}^T$ 和固定的 $(\\mu,\\rho)$，计算指定网格中每个参数元组的对数似然值，并选择使该对数似然值最大化的参数元组。您的程序必须对所有测试用例使用相同的观测失业率序列 $\\{u_t\\}_{t=1}^T$。为了可复现性，似然计算中使用的任何额外伪随机数都必须使用伪随机种子 $300+i$（对于测试用例 $i$）生成，并且在同一测试用例的所有参数元组中必须相同地重用。\n\n您的程序必须按测试用例 1、2、3 的顺序输出最大化参数元组的串接列表，其中每个元组按 $(\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$ 的顺序列出。报告的每个数字必须精确到六位小数。最终输出必须是单行，包含用方括号括起来的逗号分隔列表形式的结果，例如 $[\\beta_1,\\gamma_1,\\sigma_{\\varepsilon,1},\\sigma_{\\eta,1},\\beta_2,\\gamma_2,\\sigma_{\\varepsilon,2},\\sigma_{\\eta,2},\\beta_3,\\gamma_3,\\sigma_{\\varepsilon,3},\\sigma_{\\eta,3}]$。\n\n所有测试用例的答案都必须是浮点数。不涉及物理单位；请勿包含任何单位符号。不涉及角度。所有类似分数的值都必须以小数形式提供。输出必须严格遵循上述单行格式。",
            "solution": "问题陈述构成了一个在计算计量经济学领域中定义明确的练习，特别是在非线性状态空间模型的参数估计方面。它具有科学依据，内部一致，并为获得唯一、可验证的解提供了所有必要信息。因此，该问题被认为是有效的。\n\n核心任务是为一个非线性菲利普斯曲线模型找到参数向量 $\\theta = (\\beta, \\gamma, \\sigma_\\varepsilon, \\sigma_\\eta)$ 的最大似然估计，其中搜索空间被限制在一个有限网格内。该模型由通货膨胀 $\\pi_t$ 的测量方程和未观测到的 NAIRU $n_t$ 的状态方程定义：\n$$\n\\pi_t = \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)\n$$\n$$\nn_t = \\mu + \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n$$\n\n测量方程中的三次项 $(u_t - n_t)^3$ 引入的非线性使得似然函数在解析上难以处理。对于线性高斯状态空间模型，卡尔曼滤波器可以精确计算似然函数。然而，对于非线性或非高斯系统，则需要数值近似方法。处理此类问题的标准且有原则的方法是使用序贯蒙特卡洛（SMC）方法，通常称为粒子滤波器。\n\n其方法是为指定网格上的每个参数向量 $\\theta$ 近似计算对数似然函数 $\\log \\mathcal{L}(\\theta | Y_{1:T})$。对数似然可以分解为条件对数似然的和：\n$$\n\\log \\mathcal{L}(\\theta | Y_{1:T}) = \\sum_{t=1}^T \\log p(y_t | Y_{1:t-1}; \\theta)\n$$\n其中 $Y_{1:t} = \\{(\\pi_1, u_1), \\dots, (\\pi_t, u_t)\\}$ 代表截至时间 $t$ 的观测历史。粒子滤波器为每个预测密度项 $p(\\pi_t | Y_{1:t-1}; \\theta)$ 提供一个数值估计。\n\n我们将实现一种称为自助粒子滤波器（Bootstrap Filter）的特定粒子滤波器变体。对于给定的参数向量 $\\theta$ 和一组 $N$ 个粒子，算法流程如下：\n\n1.  **初始化 (t=0)**：从初始状态分布中抽取一组 $N$ 个粒子 $\\{n_0^{(j)}\\}_{j=1}^N$，该分布是状态过程的平稳分布：$n_0 \\sim \\mathcal{N}(\\mu, \\sigma_\\eta^2/(1-\\rho^2))$。总对数似然初始化为 $0$。\n\n2.  **序贯更新 (t = 1 到 T)**：\n    a.  **预测（传播）**：根据状态转移方程，将每个粒子向前传播。为每个粒子抽取一个随机新息：\n        $$\n        n_t^{(j)} = \\mu + \\rho(n_{t-1}^{(j)} - \\mu) + \\eta_t^{(j)}, \\quad \\text{其中 } \\eta_t^{(j)} \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n        $$\n    b.  **加权**：根据每个传播后的粒子 $n_t^{(j)}$ 对当前观测值 $\\pi_t$ 的解释程度，为其分配一个权重 $w_t^{(j)}$。权重是测量误差的概率密度函数（PDF）在观测值处的取值：\n        $$\n        \\hat{\\pi}_t^{(j)} = \\beta(u_t - n_t^{(j)}) + \\gamma(u_t - n_t^{(j)})^3\n        $$\n        $$\n        w_t^{(j)} = p(\\pi_t | n_t^{(j)}, u_t; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\varepsilon^2}} \\exp\\left(-\\frac{(\\pi_t - \\hat{\\pi}_t^{(j)})^2}{2\\sigma_\\varepsilon^2}\\right)\n        $$\n    c.  **似然近似**：条件似然 $p(\\pi_t | Y_{1:t-1}; \\theta)$ 通过未归一化权重的平均值来近似：\n        $$\n        \\hat{p}(\\pi_t | Y_{1:t-1}; \\theta) \\approx \\frac{1}{N} \\sum_{j=1}^N w_t^{(j)}\n        $$\n        通过加上该平均权重的对数来更新总对数似然。如果平均权重为零，则对数似然为负无穷大，表明该参数向量与观测值不兼容。\n    d.  **重采样**：为了解决粒子退化问题（即一个粒子获得了所有权重），从当前集合 $\\{n_t^{(j)}\\}$ 中进行有放回的抽样，以生成一组新的粒子。抽样概率由归一化权重 $W_t^{(j)} = w_t^{(j)} / \\sum_k w_t^{(k)}$ 给出。我们采用系统重采样，这是一种高效且低方差的技术。重采样后的粒子随后用于时间 $t+1$ 的预测步骤。\n\n总体的计算流程如下：\n首先，模拟共同的观测失业率序列 $\\{u_t\\}_{t=1}^{T=120}$。然后，对于三个测试用例中的每一个：\n1.  使用测试用例的真实参数和指定的随机种子模拟通货膨胀序列 $\\{\\pi_t\\}_{t=1}^{T=120}$。\n2.  使用为估计步骤指定的种子，预先生成粒子滤波器所需的随机变量集（用于初始抽取、传播和重采样）。这确保了滤波器本身的随机性对于参数网格上的所有点都是相同的，从而保证了它们似然值的公平比较。\n3.  使用粒子滤波器为测试用例网格中的每个参数元组计算近似的对数似然。\n4.  将产生最高对数似然值的参数元组确定为该案例的最大似然估计。\n\n最后，将所有三个案例的估计参数串接起来，并按照问题规范进行格式化。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main solver function to run simulations and estimations for all test cases.\n    \"\"\"\n    # Global parameters\n    T = 120\n    U_MU = 5.5\n    U_PHI = 0.8\n    U_SIGMA = 0.3\n    U_INITIAL = U_MU\n    U_SEED = 100\n    N_PARTICLES = 2000\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.95},\n            \"true_params\": {\"beta\": -0.5, \"gamma\": 0.06, \"sigma_e\": 0.2, \"sigma_n\": 0.1},\n            \"grid\": {\n                \"beta\": [-0.6, -0.5],\n                \"gamma\": [0.04, 0.06],\n                \"sigma_e\": [0.18, 0.22],\n                \"sigma_n\": [0.09, 0.11],\n            },\n            \"sim_seed\": 201,\n            \"est_seed\": 301,\n        },\n        {\n            \"case_id\": 2,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.9},\n            \"true_params\": {\"beta\": -0.4, \"gamma\": 0.0, \"sigma_e\": 0.15, \"sigma_n\": 0.02},\n            \"grid\": {\n                \"beta\": [-0.5, -0.4],\n                \"gamma\": [0.0, 0.02],\n                \"sigma_e\": [0.12, 0.18],\n                \"sigma_n\": [0.01, 0.03],\n            },\n            \"sim_seed\": 202,\n            \"est_seed\": 302,\n        },\n        {\n            \"case_id\": 3,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.98},\n            \"true_params\": {\"beta\": -0.6, \"gamma\": 0.2, \"sigma_e\": 0.25, \"sigma_n\": 0.15},\n            \"grid\": {\n                \"beta\": [-0.7, -0.6],\n                \"gamma\": [0.18, 0.2],\n                \"sigma_e\": [0.22, 0.28],\n                \"sigma_n\": [0.13, 0.17],\n            },\n            \"sim_seed\": 203,\n            \"est_seed\": 303,\n        }\n    ]\n\n    def generate_unemployment_data(T, mu_u, phi, sigma_u, u0, seed):\n        rng = np.random.default_rng(seed)\n        u = np.zeros(T)\n        u_prev = u0\n        nu = rng.normal(0, sigma_u, T)\n        for t in range(T):\n            u_t = mu_u + phi * (u_prev - mu_u) + nu[t]\n            u[t] = u_t\n            u_prev = u_t\n        return u\n\n    def simulate_model_data(T, u_series, true_params, fixed_params, seed):\n        rng = np.random.default_rng(seed)\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        beta, gamma, sigma_e, sigma_n = true_params.values()\n        \n        n_series = np.zeros(T)\n        pi_series = np.zeros(T)\n        \n        # Initial state n_0 from stationary distribution\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        n_prev = rng.normal(mu, np.sqrt(sigma_n0_sq))\n\n        innov_eta = rng.normal(0, sigma_n, T)\n        innov_eps = rng.normal(0, sigma_e, T)\n\n        for t in range(T):\n            # State equation\n            n_t = mu + rho * (n_prev - mu) + innov_eta[t]\n            n_series[t] = n_t\n            \n            # Measurement equation\n            u_gap = u_series[t] - n_t\n            pi_t = beta * u_gap + gamma * (u_gap**3) + innov_eps[t]\n            pi_series[t] = pi_t\n            \n            n_prev = n_t\n            \n        return pi_series\n\n    def particle_filter_log_likelihood(pi_series, u_series, params, fixed_params, N, rand_draws):\n        beta, gamma, sigma_e, sigma_n = params\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        T = len(pi_series)\n        z_init, z_prop, u_resample = rand_draws\n\n        # Initial state distribution\n        if rho**2 >= 1 or sigma_n = 0 or sigma_e = 0:\n            return -np.inf\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        sigma_n0 = np.sqrt(sigma_n0_sq)\n\n        # Initialization (t=0)\n        particles = mu + sigma_n0 * z_init\n        log_likelihood = 0.0\n\n        for t in range(T):\n            # Prediction/Propagation\n            particles = mu + rho * (particles - mu) + sigma_n * z_prop[t]\n\n            # Weighting\n            u_gap = u_series[t] - particles\n            pi_expected = beta * u_gap + gamma * (u_gap**3)\n            \n            # Using scipy.stats.norm.logpdf for numerical stability with small weights\n            log_weights = norm.logpdf(pi_series[t], loc=pi_expected, scale=sigma_e)\n            \n            # Log-Likelihood update using LogSumExp trick\n            if np.all(np.isneginf(log_weights)):\n                return -np.inf\n                \n            max_log_weight = np.max(log_weights)\n            weights_stable = np.exp(log_weights - max_log_weight)\n            mean_weight = np.mean(weights_stable)\n            \n            log_likelihood += max_log_weight + np.log(mean_weight)\n            \n            # Normalization\n            normalized_weights = weights_stable / np.sum(weights_stable)\n            \n            # Resampling (Systematic)\n            if np.sum(normalized_weights) > 0:\n                positions = (np.arange(N) + u_resample[t]) / N\n                cum_weights = np.cumsum(normalized_weights)\n                indices = np.searchsorted(cum_weights, positions)\n                particles = particles[indices]\n            else: # All weights were zero.\n                return -np.inf\n\n        return log_likelihood\n\n\n    # --- Main Execution ---\n    # Step 1: Generate common unemployment data\n    u_data = generate_unemployment_data(T, U_MU, U_PHI, U_SIGMA, U_INITIAL, U_SEED)\n\n    final_results = []\n    \n    for case in test_cases:\n        # Step 2: Simulate case-specific inflation data\n        pi_data = simulate_model_data(T, u_data, case[\"true_params\"], case[\"fixed_params\"], case[\"sim_seed\"])\n\n        # Step 3: Pre-generate random numbers for the particle filter\n        rng_est = np.random.default_rng(case[\"est_seed\"])\n        rand_draws_pf = (\n            rng_est.standard_normal(N_PARTICLES),  # For initialization\n            rng_est.standard_normal((T, N_PARTICLES)),  # For propagation\n            rng_est.random(T),  # For resampling\n        )\n\n        # Step 4: Grid search for MLE\n        max_log_lik = -np.inf\n        best_params = None\n\n        param_grid = list(itertools.product(\n            case[\"grid\"][\"beta\"],\n            case[\"grid\"][\"gamma\"],\n            case[\"grid\"][\"sigma_e\"],\n            case[\"grid\"][\"sigma_n\"]\n        ))\n        \n        for params_tuple in param_grid:\n            log_lik = particle_filter_log_likelihood(\n                pi_data, u_data, params_tuple, case[\"fixed_params\"], N_PARTICLES, rand_draws_pf\n            )\n            \n            if log_lik > max_log_lik:\n                max_log_lik = log_lik\n                best_params = params_tuple\n        \n        final_results.extend(best_params)\n\n    # Format and print the final output\n    print(f\"[{','.join([f'{val:.6f}' for val in final_results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后一个练习是一个批判性思维挑战，旨在强调在应用复杂算法前仔细分析模型假设的重要性。该问题提出了一个看似需要粒子滤波的结构突变检测任务：一个状态空间模型中的某个参数在未知时间点 $\\tau$ 发生了跳变。然而，一个关键的简化假设——状态方程中的过程噪声为零 ($\\sigma_{\\eta} = 0$)——使得状态演化路径在给定参数下是确定性的 。这项练习揭示了一个深刻的道理：一个看似复杂的问题，有时会因为特定的模型结构而允许使用更为简洁的求解方法，从而避免了不必要的计算复杂性。",
            "id": "2418273",
            "problem": "考虑一个由非线性状态空间模型生成的单变量时间序列，该模型在决定潜变量动态的一个时不变参数上存在单个结构性断点。设潜变量表示为 $x_t$，观测序列表示为 $y_t$，其中 $t \\in \\{1,\\dots,T\\}$。模型规定如下：\n- 状态转移方程：$x_t = \\theta_t + \\rho \\, x_{t-1} + \\sigma_{\\eta} \\, \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,1)$。\n- 观测方程：$y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$。\n- 参数 $\\theta_t$ 的断点结构：存在一个未知断点时间 $\\tau \\in \\{1,\\dots,T\\}$，使得当 $t \\le \\tau$ 时 $\\theta_t = \\theta_1$，当 $t  \\tau$ 时 $\\theta_t = \\theta_2$。$\\tau$ 的先验分布是在 $\\{1,\\dots,T\\}$ 上的均匀分布。\n- 初始条件 $x_0$ 已知。\n\n你的任务是构建一个程序，对于下方的每个测试用例，该程序接收模型和由所提供的数据生成过程所蕴含的观测时间序列，并返回结构性断点时间的最大后验估计 $\\hat{\\tau} \\in \\{1,\\dots,T\\}$，结果以整数形式报告。\n\n对于所有测试用例中的数据生成，以下条件适用：\n- 状态新息的标准差为零，即 $\\sigma_{\\eta} = 0$，因此在给定参数和 $\\tau$ 的情况下，$x_t$ 是确定性演化的。\n- 观测新息的标准差为 $\\sigma_{\\varepsilon} = 0.1$。\n- 自回归系数为 $\\rho = 0.7$。\n- 初始状态为 $x_0 = 0$。\n- 时间范围为 $T = 30$。\n- 观测噪声抽样 $\\{\\varepsilon_t\\}_{t=1}^{T}$ 是固定的，并由以下列表给出：\n$\\big[\\, 0.03,\\,-0.02,\\,0.01,\\,0.00,\\,-0.01,\\,0.02,\\,-0.03,\\,0.04,\\,-0.02,\\,0.01,\\,0.00,\\,-0.04,\\,0.05,\\,-0.01,\\,0.02,\\,-0.02,\\,0.03,\\,-0.03,\\,0.01,\\,0.00,\\,0.02,\\,-0.01,\\,0.04,\\,-0.02,\\,0.01,\\,-0.03,\\,0.02,\\,0.00,\\,-0.01,\\,0.03\\,\\big]$。\n- 对于每个具有指定 $(\\theta_1,\\theta_2,\\tau_{\\text{true}})$ 的测试用例，通过递归式 $x_t = \\theta_t + \\rho \\, x_{t-1}$（其中 $\\theta_t$ 由 $\\tau_{\\text{true}}$ 定义）来确定性地构建潜变量路径 $\\{x_t\\}$，然后使用上述固定的 $\\{\\varepsilon_t\\}$ 通过 $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$ 定义观测序列。生成的 $\\{y_t\\}$ 构成了你的估计过程应使用的数据；你在估计中不得使用 $\\{\\varepsilon_t\\}$ 或 $\\tau_{\\text{true}}$。\n\n测试套件：\n- 用例 A（样本内无断点）：$\\theta_1 = 0.5$，$\\theta_2 = 1.5$，$\\tau_{\\text{true}} = 30$。\n- 用例 B（早期断点）：$\\theta_1 = 0.5$，$\\theta_2 = 1.5$，$\\tau_{\\text{true}} = 8$。\n- 用例 C（晚期断点）：$\\theta_1 = 0.5$，$\\theta_2 = 1.5$，$\\tau_{\\text{true}} = 22$。\n\n对于每个用例，仅基于模型所蕴含的似然和 $\\tau$ 上的均匀先验来计算断点时间的最大后验估计 $\\hat{\\tau}$。你的程序应生成单行输出，其中包含用例 A、B 和 C 的三个估计断点时间，按顺序排列，格式为用方括号括起来的逗号分隔列表，例如 $[\\hat{\\tau}_A,\\hat{\\tau}_B,\\hat{\\tau}_C]$。输出必须是整数。此问题不涉及物理单位，角度也不适用。解决方案不得读取任何外部输入；所有需要的值都在本文中提供，或必须完全按照规定进行硬编码。",
            "solution": "该问题是有效的。这是一个针对具有结构性断点的非线性状态空间模型进行参数估计的适定性练习。我们开始进行求解。\n\n目标是找到未知结构性断点时间 $\\tau$ 的最大后验 (MAP) 估计。参数 $\\tau$ 可以取集合 $\\{1, \\dots, T\\}$ 中的任何整数值，其中时间范围 $T=30$。MAP 估计（表示为 $\\hat{\\tau}$）是在给定观测数据 $y_{1:T} = \\{y_1, \\dots, y_T\\}$ 的条件下，使后验概率最大化的 $\\tau$ 值。根据贝叶斯定理，后验概率为：\n$$\nP(\\tau | y_{1:T}) = \\frac{P(y_{1:T} | \\tau) P(\\tau)}{P(y_{1:T})}\n$$\n我们希望找到 $\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(\\tau | y_{1:T})$。分母 $P(y_{1:T})$ 是一个与 $\\tau$ 无关的归一化常数，在优化过程中可以忽略。问题指定了 $\\tau$ 的均匀先验分布，使得对于所有 $\\tau \\in \\{1, \\dots, T\\}$ 都有 $P(\\tau) = 1/T$。由于这个先验也相对于 $\\tau$ 是常数，它不影响最大值的位置。因此，MAP 估计问题简化为最大似然 (ML) 估计问题：\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(y_{1:T} | \\tau)\n$$\n其中 $P(y_{1:T} | \\tau)$ 是在假设断点时间为 $\\tau$ 的条件下观测数据的似然。处理对数似然 $\\mathcal{L}(\\tau) = \\log P(y_{1:T} | \\tau)$ 通常更为方便。使似然最大化的 $\\tau$ 值同样也会使对数似然最大化。\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} \\mathcal{L}(\\tau)\n$$\n现在我们来定义似然函数。问题提供了一个关键的简化：状态新息的标准差为零，即 $\\sigma_{\\eta} = 0$。这使得对于给定的一组参数，状态转移方程是确定性的。具体来说，对于任何假设的断点时间 $k \\in \\{1, \\dots, T\\}$，整个潜变量路径 $\\{x_t(k)\\}_{t=1}^T$ 都由以下递归式唯一确定：\n$$\nx_t(k) = \\theta_t(k) + \\rho x_{t-1}(k)\n$$\n初始条件为 $x_0(k) = x_0 = 0$。参数 $\\theta_t(k)$ 定义为：当 $t \\le k$ 时 $\\theta_t(k) = \\theta_1$，当 $t  k$ 时 $\\theta_t(k) = \\theta_2$。因为对于给定的 $k$，潜变量路径是确定性的，所以无需通过滤波技术（如粒子滤波器）进行状态估计，而如果 $\\sigma_{\\eta}  0$，这将是必需的。\n\n观测方程为 $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$。这意味着在给定状态 $x_t$ 的条件下，每个观测值 $y_t$ 服从条件正态分布：\n$$\ny_t | x_t \\sim \\mathcal{N}\\left(\\mu_t, \\sigma_{\\varepsilon}^2\\right) \\quad \\text{其中} \\quad \\mu_t = \\frac{1}{2} x_t^2\n$$\n对于由假设 $\\tau=k$ 所确定的状态 $x_t(k)$，单个观测值 $y_t$ 的概率密度函数为：\n$$\np(y_t | x_t(k)) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\varepsilon}^2}} \\exp\\left( -\\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right)\n$$\n观测误差 $\\varepsilon_t$ 在时间上是独立的。因此，序列 $y_{1:T}$ 的总似然是各个概率密度的乘积：\n$$\nP(y_{1:T} | \\tau=k) = \\prod_{t=1}^T p(y_t | x_t(k))\n$$\n相应的对数似然为：\n$$\n\\mathcal{L}(k) = \\log P(y_{1:T} | \\tau=k) = \\sum_{t=1}^T \\log p(y_t | x_t(k)) = \\sum_{t=1}^T \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2) - \\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right]\n$$\n为了最大化关于 $k$ 的 $\\mathcal{L}(k)$，我们可以忽略常数项 $-\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2)$ 和正常数缩放因子 $1/(2\\sigma_{\\varepsilon}^2)$。因此，最大化对数似然等价于最小化观测数据与模型对 $y_t$ 均值的预测之间的残差平方和 (SSE)：\n$$\n\\hat{\\tau} = \\arg\\min_{k \\in \\{1, \\dots, T\\}} \\sum_{t=1}^T \\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2\n$$\n估计过程如下。对于每个测试用例：\n$1.$ 首先，根据问题说明，使用提供的真实参数 $(\\theta_1, \\theta_2, \\tau_{\\text{true}})$ 和固定的噪声序列 $\\{\\varepsilon_t\\}_{t=1}^T$ 生成观测时间序列 $\\{y_t\\}_{t=1}^T$。\n$2.$ 然后，为了估计 $\\hat{\\tau}$，遍历从 $1$ 到 $T=30$ 的每个可能的候选断点时间 $k$。\n$3.$ 对于每个候选 $k$，使用已知的 $\\theta_1$、$\\theta_2$、$\\rho$ 和 $x_0$ 值计算假设的确定性状态路径 $\\{x_t(k)\\}_{t=1}^T$。\n$4.$ 计算残差平方和 $SSE(k) = \\sum_{t=1}^T (y_t - \\frac{1}{2}x_t(k)^2)^2$。\n$5.$ 估计值 $\\hat{\\tau}$ 是产生最小 SSE 的 $k$ 值。如果多个 $k$ 值产生相同的最小 SSE，则按惯例选择其中最小的 $k$（例如，通过 `argmin`）。\n\n对每个测试用例实施此过程，以找到相应的估计值 $\\hat{\\tau}$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the maximum a posteriori estimate of a structural break time\n    in a nonlinear state-space model for three test cases.\n    \"\"\"\n\n    # Global parameters and data generation settings from the problem statement\n    sigma_eps = 0.1\n    rho = 0.7\n    x0 = 0.0\n    T = 30\n    epsilon_t_draws = np.array([\n        0.03, -0.02, 0.01, 0.00, -0.01, 0.02, -0.03, 0.04, -0.02, 0.01,\n        0.00, -0.04, 0.05, -0.01, 0.02, -0.02, 0.03, -0.03, 0.01, 0.00,\n        0.02, -0.01, 0.04, -0.02, 0.01, -0.03, 0.02, 0.00, -0.01, 0.03\n    ])\n\n    test_cases = [\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 30},  # Case A\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 8},   # Case B\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 22}   # Case C\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        theta1 = case['theta1']\n        theta2 = case['theta2']\n        tau_true = case['tau_true']\n\n        #\n        # Step 1: Generate the observed time series y_t for the current case.\n        # This part simulates the data that the estimation procedure will use.\n        #\n        x_true = np.zeros(T + 1)\n        x_true[0] = x0\n        y_obs = np.zeros(T)\n        \n        for t in range(1, T + 1):\n            theta_t = theta1 if t = tau_true else theta2\n            x_true[t] = theta_t + rho * x_true[t-1]\n            # y_obs is 0-indexed, so y_t corresponds to y_obs[t-1]\n            y_obs[t-1] = 0.5 * x_true[t]**2 + sigma_eps * epsilon_t_draws[t-1]\n            \n        #\n        # Step 2: Estimate the break time tau by maximizing the likelihood.\n        # This is equivalent to minimizing the sum of squared errors (SSE).\n        # The estimator only has access to y_obs and the model structure/parameters\n        # (theta1, theta2, rho, sigma_eps), not tau_true or epsilon_t_draws.\n        #\n        \n        sse_values = []\n        possible_taus = range(1, T + 1)\n        \n        for k in possible_taus:\n            # For each candidate break time k, compute the hypothetical state path\n            # and the corresponding SSE.\n            x_hypothetical = np.zeros(T + 1)\n            x_hypothetical[0] = x0\n            current_sse = 0.0\n            \n            for t in range(1, T + 1):\n                theta_t_hyp = theta1 if t = k else theta2\n                x_hypothetical[t] = theta_t_hyp + rho * x_hypothetical[t-1]\n                \n                y_pred = 0.5 * x_hypothetical[t]**2\n                current_sse += (y_obs[t-1] - y_pred)**2\n                \n            sse_values.append(current_sse)\n            \n        # The MAP/ML estimate for tau is the one that minimizes the SSE.\n        # np.argmin returns the 0-based index of the minimum SSE.\n        # Since possible_taus starts from 1, the estimated tau is index + 1.\n        min_sse_index = np.argmin(sse_values)\n        tau_hat = possible_taus[min_sse_index]\n        \n        results.append(tau_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}