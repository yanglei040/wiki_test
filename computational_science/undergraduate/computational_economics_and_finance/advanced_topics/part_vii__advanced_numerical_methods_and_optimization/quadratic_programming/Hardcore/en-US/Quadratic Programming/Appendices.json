{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on exploration with a classic location analysis problem, a scenario frequently encountered in operations research and economics. This exercise  asks you to find the optimal location for a warehouse to minimize transportation costs, subject to a geographical constraint. By solving this problem analytically using the method of Lagrange multipliers, you will gain a foundational understanding of how to tackle equality-constrained quadratic programs and appreciate the elegant geometry of their solutions.",
            "id": "2424375",
            "problem": "A firm plans to open a single warehouse to serve $N$ retail stores located at positions $\\{x_i\\}_{i=1}^N \\subset \\mathbb{R}^d$, where $d \\in \\mathbb{N}$. Each store $i$ has a strictly positive demand weight $w_i \\in \\mathbb{R}_{++}$, and $\\sum_{i=1}^N w_i \\in \\mathbb{R}_{++}$. The firm models aggregate transport cost using the squared Euclidean distance and seeks to place the warehouse at a location $x \\in \\mathbb{R}^d$ that minimizes the total weighted squared distance to the stores. Due to a zoning requirement, the warehouse must lie on a prescribed affine hyperplane given by $a^{\\top} x = b$, where $a \\in \\mathbb{R}^d \\setminus \\{0\\}$ and $b \\in \\mathbb{R}$ are given. Formally, the problem is\n$$\n\\min_{x \\in \\mathbb{R}^d} \\;\\; \\sum_{i=1}^N w_i \\,\\|x - x_i\\|_2^2\n\\quad \\text{subject to} \\quad a^{\\top} x = b.\n$$\nDetermine, in closed form, the optimal warehouse location as a function of $\\{x_i\\}_{i=1}^N$, $\\{w_i\\}_{i=1}^N$, $a$, and $b$. Provide your final answer as a single analytic expression. No rounding is required.",
            "solution": "The problem statement must first be validated for correctness and feasibility.\n\n**Step 1: Extract Givens**\nThe explicit information provided in the problem statement is as follows:\n- Number of retail stores: $N$.\n- Location of store $i$: $x_i \\in \\mathbb{R}^d$, for $i \\in \\{1, 2, \\dots, N\\}$, where $d \\in \\mathbb{N}$.\n- Demand weight for store $i$: $w_i \\in \\mathbb{R}_{++}$ (strictly positive real numbers). The total weight is also strictly positive: $\\sum_{i=1}^N w_i \\in \\mathbb{R}_{++}$.\n- The location of the warehouse to be determined: $x \\in \\mathbb{R}^d$.\n- The objective function to be minimized: $f(x) = \\sum_{i=1}^N w_i \\,\\|x - x_i\\|_2^2$.\n- The constraint on the warehouse location: $a^{\\top} x = b$.\n- The constraint parameters: $a \\in \\mathbb{R}^d \\setminus \\{0\\}$ (a non-zero vector) and $b \\in \\mathbb{R}$ (a scalar).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is subjected to rigorous validation.\n- **Scientifically Grounded**: The problem is a classical constrained optimization problem, specifically a quadratic program (QP). The objective function is the weighted sum of squared Euclidean distances, a standard choice in location analysis and statistics representing variance or cost. The constraint is a linear equality, defining an affine hyperplane. The formulation is firmly rooted in convex optimization and is mathematically sound.\n- **Well-Posed**: The objective function $f(x)$ is a sum of squared norms. It can be expanded as:\n$$f(x) = \\sum_{i=1}^N w_i (x - x_i)^\\top (x - x_i) = \\sum_{i=1}^N w_i (x^\\top x - 2x^\\top x_i + x_i^\\top x_i)$$\n$$f(x) = \\left(\\sum_{i=1}^N w_i\\right) x^\\top x - 2x^\\top \\left(\\sum_{i=1}^N w_i x_i\\right) + \\left(\\sum_{i=1}^N w_i x_i^\\top x_i\\right)$$\nThis is a quadratic function of $x$. The Hessian matrix is $\\nabla^2_x f(x) = 2 \\left(\\sum_{i=1}^N w_i\\right) I_d$, where $I_d$ is the $d \\times d$ identity matrix. Since all $w_i > 0$, the total weight $\\sum w_i > 0$, and the Hessian is positive definite. This confirms that $f(x)$ is a strictly convex function. The constraint set $C = \\{x \\in \\mathbb{R}^d \\mid a^\\top x = b\\}$ is an affine subspace, which is a convex set. Minimizing a strictly convex function over a non-empty convex set guarantees the existence of a unique solution. The problem is therefore well-posed.\n- **Objective**: The problem is stated using precise, unambiguous mathematical language. There are no subjective or opinion-based elements.\n\nThe problem does not exhibit any flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, or ill-posed.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be derived.\n\nThe problem is a constrained optimization problem which can be solved using the method of Lagrange multipliers. The task is to find $x \\in \\mathbb{R}^d$ that solves:\n$$ \\min_{x \\in \\mathbb{R}^d} \\sum_{i=1}^N w_i \\|x - x_i\\|_2^2 \\quad \\text{subject to} \\quad a^\\top x - b = 0 $$\nThe Lagrangian function $\\mathcal{L}(x, \\lambda)$ is constructed as:\n$$ \\mathcal{L}(x, \\lambda) = \\sum_{i=1}^N w_i (x - x_i)^\\top(x - x_i) + \\lambda (a^\\top x - b) $$\nwhere $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier. For an optimal solution $x^*$, the Karush-Kuhn-Tucker (KKT) conditions require that the gradient of the Lagrangian with respect to $x$ vanishes:\n$$ \\nabla_x \\mathcal{L}(x^*, \\lambda^*) = 0 $$\nWe compute the gradient of $\\mathcal{L}(x, \\lambda)$ with respect to $x$:\n$$ \\nabla_x \\mathcal{L}(x, \\lambda) = \\nabla_x \\left( \\sum_{i=1}^N w_i (x^\\top x - 2x^\\top x_i + x_i^\\top x_i) + \\lambda (a^\\top x - b) \\right) $$\n$$ \\nabla_x \\mathcal{L}(x, \\lambda) = \\sum_{i=1}^N w_i (2x - 2x_i) + \\lambda a = 2 \\left( \\sum_{i=1}^N w_i \\right) x - 2 \\sum_{i=1}^N w_i x_i + \\lambda a $$\nLet us define the total weight $W = \\sum_{j=1}^N w_j$ and the weighted mean location (center of mass) $\\bar{x} = \\frac{1}{W} \\sum_{i=1}^N w_i x_i$. The gradient expression simplifies to:\n$$ \\nabla_x \\mathcal{L}(x, \\lambda) = 2W x - 2W \\bar{x} + \\lambda a = 2W(x - \\bar{x}) + \\lambda a $$\nSetting the gradient to zero gives the first-order condition:\n$$ 2W(x^* - \\bar{x}) + \\lambda^* a = 0 $$\nSince $W > 0$, we can solve for the optimal location $x^*$ in terms of the multiplier $\\lambda^*$:\n$$ x^* = \\bar{x} - \\frac{\\lambda^*}{2W} a $$\nThe optimal solution $x^*$ must also satisfy the constraint $a^\\top x^* = b$. We substitute the expression for $x^*$ into the constraint equation to find $\\lambda^*$:\n$$ a^\\top \\left( \\bar{x} - \\frac{\\lambda^*}{2W} a \\right) = b $$\n$$ a^\\top \\bar{x} - \\frac{\\lambda^*}{2W} (a^\\top a) = b $$\nRecognizing that $a^\\top a = \\|a\\|_2^2$, we solve for $\\lambda^*$:\n$$ \\frac{\\lambda^*}{2W} \\|a\\|_2^2 = a^\\top \\bar{x} - b $$\nSince $a \\neq 0$, it follows that $\\|a\\|_2^2 > 0$, and we can isolate the term with the multiplier:\n$$ \\frac{\\lambda^*}{2W} = \\frac{a^\\top \\bar{x} - b}{\\|a\\|_2^2} $$\nNow we substitute this back into the expression for $x^*$:\n$$ x^* = \\bar{x} - \\left( \\frac{a^\\top \\bar{x} - b}{\\|a\\|_2^2} \\right) a $$\nThis expression represents the optimal warehouse location. Geometrically, the unconstrained minimizer of the objective function is the weighted mean $\\bar{x}$. The formula for $x^*$ shows that the constrained optimum is the orthogonal projection of the unconstrained minimizer $\\bar{x}$ onto the affine hyperplane defined by $a^\\top x = b$.\n\nTo provide the final answer in terms of the initial problem parameters, we substitute the definitions of $\\bar{x}$ and $W$:\n$$ \\bar{x} = \\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{j=1}^N w_j} $$\nSubstituting this into the expression for $x^*$:\n$$ x^* = \\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{j=1}^N w_j} - \\frac{a^\\top \\left( \\frac{\\sum_{k=1}^N w_k x_k}{\\sum_{j=1}^N w_j} \\right) - b}{\\|a\\|_2^2} a $$\nTo simplify the fraction, we can multiply the numerator and denominator of the coefficient of $a$ by $\\sum_{j=1}^N w_j$:\n$$ x^* = \\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{j=1}^N w_j} - \\left( \\frac{a^\\top \\left( \\sum_{k=1}^N w_k x_k \\right) - b \\left( \\sum_{j=1}^N w_j \\right)}{\\left( \\sum_{j=1}^N w_j \\right) \\|a\\|_2^2} \\right) a $$\nThis is the final closed-form expression for the optimal warehouse location.",
            "answer": "$$\\boxed{\\frac{\\sum_{i=1}^N w_i x_i}{\\sum_{j=1}^N w_j} - \\left( \\frac{a^\\top \\left( \\sum_{k=1}^N w_k x_k \\right) - b \\left( \\sum_{j=1}^N w_j \\right)}{\\left( \\sum_{j=1}^N w_j \\right) \\|a\\|_2^2} \\right) a}$$"
        },
        {
            "introduction": "Building on the basics, this next problem introduces the practical complexities of inequality constraints, which define the boundaries of most real-world economic decisions. In this exercise , you will determine the optimal allocation of a company's budget to maximize its strategic utility. This practice will sharpen your skills in navigating a feasible region and identifying when the optimal solution lies not in the interior, but on the boundary defined by resource limitations.",
            "id": "2424372",
            "problem": "A technology firm allocates its annual information technology budget between two activities: cybersecurity and new feature development. Let $x$ denote millions of dollars allocated to cybersecurity and let $y$ denote millions of dollars allocated to new feature development. The firmâ€™s net strategic utility (measured in profit-equivalent units) from the allocation is\n$$\nU(x,y) \\;=\\; 12\\,x \\;-\\; 1.5\\,x^{2} \\;+\\; 10\\,y \\;-\\; y^{2} \\;+\\; 0.6\\,x\\,y.\n$$\nThe allocation is subject to the following constraints:\n$$\nx + y \\leq 10,\\quad x \\geq 2,\\quad y \\geq 0,\n$$\nwhere $10$ is the total budget (in millions of dollars), $x \\geq 2$ reflects a regulatory minimum for cybersecurity, and $y \\geq 0$ reflects nonnegativity of investment.\n\nDetermine the value of $x$ that maximizes $U(x,y)$ subject to the constraints. Express your answer in million dollars and round your answer to four significant figures.",
            "solution": "The problem is to find the values of $x$ and $y$ that maximize the utility function $U(x,y)$ subject to a set of linear inequality constraints. This is a constrained optimization problem, specifically a quadratic programming problem. The objective function is\n$$\nU(x,y) = 12x - 1.5x^2 + 10y - y^2 + 0.6xy\n$$\nThe constraints are:\n$$\nx + y \\leq 10 \\\\\nx \\geq 2 \\\\\ny \\geq 0\n$$\n\nFirst, we validate the problem structure. The feasible region defined by the linear inequalities is a closed and bounded set (a compact set). Specifically, it is a triangle in the $xy$-plane with vertices at $(2,0)$, $(10,0)$, and $(2,8)$. Being a polygon, the feasible region is convex.\n\nNext, we analyze the objective function $U(x,y)$. To determine its convexity/concavity, we compute the Hessian matrix, which is the matrix of second partial derivatives.\nThe first partial derivatives are:\n$$\n\\frac{\\partial U}{\\partial x} = 12 - 3x + 0.6y\n$$\n$$\n\\frac{\\partial U}{\\partial y} = 10 - 2y + 0.6x\n$$\nThe second partial derivatives are:\n$$\n\\frac{\\partial^2 U}{\\partial x^2} = -3\n$$\n$$\n\\frac{\\partial^2 U}{\\partial y^2} = -2\n$$\n$$\n\\frac{\\partial^2 U}{\\partial x \\partial y} = \\frac{\\partial^2 U}{\\partial y \\partial x} = 0.6\n$$\nThe Hessian matrix $H$ is:\n$$\nH = \\begin{pmatrix} -3 & 0.6 \\\\ 0.6 & -2 \\end{pmatrix}\n$$\nThe principal minors of $H$ are $D_1 = -3$ and $D_2 = \\det(H) = (-3)(-2) - (0.6)(0.6) = 6 - 0.36 = 5.64$.\nSince $D_1 < 0$ and $D_2 > 0$, the Hessian matrix is negative definite. This implies that the function $U(x,y)$ is strictly concave.\n\nMaximizing a strictly concave function over a compact convex set guarantees the existence of a unique global maximum. This maximum can occur either in the interior of the feasible region or on its boundary.\n\nLet us first find the unconstrained maximum by setting the gradient of $U(x,y)$ to zero:\n$$\n12 - 3x + 0.6y = 0 \\implies 3x - 0.6y = 12\n$$\n$$\n10 - 2y + 0.6x = 0 \\implies -0.6x + 2y = 10\n$$\nSolving this system of linear equations. From the second equation, $2y = 10 + 0.6x$, which gives $y = 5 + 0.3x$. Substituting into the first equation:\n$$\n3x - 0.6(5 + 0.3x) = 12\n$$\n$$\n3x - 3 - 0.18x = 12\n$$\n$$\n2.82x = 15 \\implies x = \\frac{15}{2.82} = \\frac{1500}{282} = \\frac{250}{47} \\approx 5.319\n$$\nThen,\n$$\ny = 5 + 0.3\\left(\\frac{250}{47}\\right) = 5 + \\frac{75}{47} = \\frac{235 + 75}{47} = \\frac{310}{47} \\approx 6.596\n$$\nThe unconstrained maximum is at approximately $(5.319, 6.596)$. We check if this point satisfies the constraints:\n$x = 250/47 \\approx 5.319 \\geq 2$ (satisfied).\n$y = 310/47 \\approx 6.596 \\geq 0$ (satisfied).\n$x + y = \\frac{250}{47} + \\frac{310}{47} = \\frac{560}{47} \\approx 11.915$. This violates the constraint $x + y \\leq 10$.\nSince the unconstrained maximum is outside the feasible region, the constrained maximum must lie on the boundary of the feasible region. The boundary consists of three line segments:\n1. $x + y = 10$, for $x \\in [2, 10]$\n2. $x = 2$, for $y \\in [0, 8]$\n3. $y = 0$, for $x \\in [2, 10]$\n\nWe analyze the function $U(x,y)$ on each of these segments.\n\nCase 1: The boundary $x + y = 10$.\nWe substitute $y = 10 - x$ into $U(x,y)$. The domain for $x$ on this segment is determined by $x \\geq 2$ and $y = 10 - x \\geq 0 \\implies x \\leq 10$. So, $x \\in [2,10]$.\nLet $f(x) = U(x, 10 - x)$:\n$$\nf(x) = 12x - 1.5x^2 + 10(10-x) - (10-x)^2 + 0.6x(10-x)\n$$\n$$\nf(x) = 12x - 1.5x^2 + 100 - 10x - (100 - 20x + x^2) + 6x - 0.6x^2\n$$\n$$\nf(x) = (12 - 10 + 20 + 6)x + (-1.5 - 1 - 0.6)x^2 + (100 - 100)\n$$\n$$\nf(x) = 28x - 3.1x^2\n$$\nThis is a concave parabola. We find its maximum by setting its derivative to zero:\n$$\nf'(x) = 28 - 6.2x = 0 \\implies x = \\frac{28}{6.2} = \\frac{280}{62} = \\frac{140}{31}\n$$\n$x = \\frac{140}{31} \\approx 4.5161$. This value is within the interval $[2, 10]$. Thus, it is a candidate for the maximum. The corresponding $y$ value is $y = 10 - \\frac{140}{31} = \\frac{310 - 140}{31} = \\frac{170}{31}$.\nAt this point, $U\\left(\\frac{140}{31}, \\frac{170}{31}\\right) = f\\left(\\frac{140}{31}\\right) = 28\\left(\\frac{140}{31}\\right) - 3.1\\left(\\frac{140}{31}\\right)^2 = \\frac{3920}{31} - \\frac{31}{10}\\frac{19600}{961} = \\frac{3920}{31} - \\frac{1960}{31} = \\frac{1960}{31} \\approx 63.226$.\n\nCase 2: The boundary $x=2$.\nThe domain for $y$ on this segment is $y \\geq 0$ and $2+y \\leq 10 \\implies y \\leq 8$. So, $y \\in [0,8]$.\nLet $g(y) = U(2,y)$:\n$$\ng(y) = 12(2) - 1.5(2^2) + 10y - y^2 + 0.6(2)y\n$$\n$$\ng(y) = 24 - 6 + 10y - y^2 + 1.2y = 18 + 11.2y - y^2\n$$\nTo find the maximum of this concave parabola, we set its derivative to zero:\n$$\ng'(y) = 11.2 - 2y = 0 \\implies y = 5.6\n$$\nThis value is within the interval $[0, 8]$. The candidate point is $(2, 5.6)$.\nThe utility is $U(2, 5.6) = 18 + 11.2(5.6) - (5.6)^2 = 18 + 62.72 - 31.36 = 49.36$.\n\nCase 3: The boundary $y=0$.\nThe domain for $x$ on this segment is $x \\geq 2$ and $x+0 \\leq 10 \\implies x \\leq 10$. So, $x \\in [2,10]$.\nLet $h(x) = U(x,0)$:\n$$\nh(x) = 12x - 1.5x^2\n$$\nTo find the maximum of this concave parabola, we set its derivative to zero:\n$$\nh'(x) = 12 - 3x = 0 \\implies x = 4\n$$\nThis value is within the interval $[2, 10]$. The candidate point is $(4, 0)$.\nThe utility is $U(4, 0) = 12(4) - 1.5(4^2) = 48 - 1.5(16) = 48 - 24 = 24$.\n\nFinally, we compare the utility values at the candidate points found on the boundary segments. We also implicitly check the corners of the feasible region, as the analysis on segments covers them.\n1. On segment $x+y=10$: $U(\\frac{140}{31}, \\frac{170}{31}) = \\frac{1960}{31} \\approx 63.226$.\n2. On segment $x=2$: $U(2, 5.6) = 49.36$.\n3. On segment $y=0$: $U(4, 0) = 24$.\n\nThe maximum utility value is $\\frac{1960}{31}$, which occurs at $(x, y) = \\left(\\frac{140}{31}, \\frac{170}{31}\\right)$.\nThe problem asks for the value of $x$ that maximizes $U(x,y)$, rounded to four significant figures.\n$$\nx = \\frac{140}{31} \\approx 4.516129...\n$$\nRounding to four significant figures, we get $x = 4.516$.\nThe result, $4.516$, is in millions of dollars as specified.",
            "answer": "$$\n\\boxed{4.516}\n$$"
        },
        {
            "introduction": "Our final practice bridges the gap between analytical theory and computational implementation, a crucial step for any aspiring computational economist. The task  is to project a vector onto the probability simplex, a fundamental QP subroutine in machine learning, statistics, and portfolio construction. This hands-on coding challenge will guide you through implementing an efficient algorithm, transforming a mathematical concept into a practical and reusable tool.",
            "id": "2424310",
            "problem": "Consider the following convex quadratic program that arises in constructing long-only portfolio weights in computational economics and finance. Given a vector $y \\in \\mathbb{R}^n$, find the projection $x^\\star \\in \\mathbb{R}^n$ of $y$ onto the probability simplex by solving\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\|x - y\\|_2^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^n x_i = 1,\\ \\ x_i \\ge 0 \\ \\text{for all } i.\n$$\nFor each test case below, compute the unique optimizer $x^\\star$ of the problem stated above. Round each component of $x^\\star$ to $6$ decimal places. There are no physical units involved. Your program must not read any input and must use the following test suite exactly.\n\nTest suite (each $y$ is listed as an ordered tuple of its components):\n- Case $1$: $y = (0.2, 0.6, 0.7)$.\n- Case $2$: $y = (0.1, 0.2, 0.7)$.\n- Case $3$: $y = (-0.5, 0.3, 1.2)$.\n- Case $4$: $y = (-1.0, -2.0, -3.0)$.\n- Case $5$: $y = (0.5, 0.5, 0.5)$.\n- Case $6$: $y = (0.1, 0.2, -0.3)$.\n- Case $7$: $y = (10.0, -10.0, 0.0)$.\n- Case $8$: $y = (-2.0)$.\n- Case $9$: $y = (0.9, 0.1, 0.1, -0.2, 0.0, 0.1)$.\n\nYour program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets, where each result is itself the list of rounded components of $x^\\star$ for the corresponding test case. For example, the required format is\n$$\n[\\,[x^\\star_{1,1},\\ldots,x^\\star_{1,n_1}],[x^\\star_{2,1},\\ldots,x^\\star_{2,n_2}],\\ldots,[x^\\star_{9,1},\\ldots,x^\\star_{9,n_9}]\\,],\n$$\nwith no spaces anywhere in the output line. The final output type is a list of lists of floats, represented as a single line of text.",
            "solution": "The problem is to find the Euclidean projection of a vector $y \\in \\mathbb{R}^n$ onto the standard, or probability, simplex $\\Delta^n = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, x_i \\ge 0\\}$. This is a convex quadratic programming problem, as the objective function $f(x) = \\|x - y\\|_2^2$ is strictly convex and the feasible region $\\Delta^n$ is a non-empty, closed, and bounded convex set. A unique solution $x^\\star$ is therefore guaranteed to exist.\n\nThe problem can be stated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & \\frac{1}{2}\\sum_{i=1}^n (x_i - y_i)^2 \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^n x_i - 1 = 0 \\\\\n& -x_i \\le 0, \\quad \\text{for } i=1, \\dots, n\n\\end{aligned}\n$$\nThe factor of $1/2$ is introduced for convenience and does not alter the location of the minimum. We can solve this using the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is:\n$$\nL(x, \\lambda, \\mu) = \\frac{1}{2} \\sum_{i=1}^n (x_i - y_i)^2 - \\lambda \\left(\\sum_{i=1}^n x_i - 1\\right) - \\sum_{i=1}^n \\mu_i x_i\n$$\nHere, $\\lambda$ is the Lagrange multiplier for the equality constraint and $\\mu_i \\ge 0$ are the multipliers for the non-negativity constraints.\n\nThe KKT conditions for an optimal solution $(x^\\star, \\lambda^\\star, \\mu^\\star)$ are:\n1.  **Stationarity**: The gradient of the Lagrangian with respect to $x$ must be zero. For each component $x_i$:\n    $$\n    \\frac{\\partial L}{\\partial x_i} = (x_i^\\star - y_i) - \\lambda^\\star - \\mu_i^\\star = 0 \\implies x_i^\\star = y_i + \\lambda^\\star + \\mu_i^\\star\n    $$\n2.  **Primal Feasibility**: The solution $x^\\star$ must lie in the feasible set.\n    $$\n    \\sum_{i=1}^n x_i^\\star = 1 \\quad \\text{and} \\quad x_i^\\star \\ge 0 \\quad \\text{for all } i\n    $$\n3.  **Dual Feasibility**: The multipliers for the inequality constraints must be non-negative.\n    $$\n    \\mu_i^\\star \\ge 0 \\quad \\text{for all } i\n    $$\n4.  **Complementary Slackness**: The product of each inequality multiplier and its corresponding constraint must be zero.\n    $$\n    \\mu_i^\\star x_i^\\star = 0 \\quad \\text{for all } i\n    $$\n\nFrom the complementary slackness condition, if $x_i^\\star > 0$, then $\\mu_i^\\star$ must be $0$. In this case, the stationarity condition simplifies to $x_i^\\star - y_i - \\lambda^\\star = 0$, which gives $x_i^\\star = y_i + \\lambda^\\star$.\nIf $x_i^\\star = 0$, then $\\mu_i^\\star \\ge 0$. The stationarity condition gives $-y_i - \\lambda^\\star - \\mu_i^\\star = 0$, or $\\mu_i^\\star = -y_i - \\lambda^\\star$. The dual feasibility $\\mu_i^\\star \\ge 0$ implies $-y_i - \\lambda^\\star \\ge 0$, or $y_i + \\lambda^\\star \\le 0$.\n\nLet us define a threshold $\\theta = -\\lambda^\\star$. The conditions on $x_i^\\star$ can be unified as:\n$$\nx_i^\\star = \\max(0, y_i - \\theta)\n$$\nThis single expression elegantly combines the two cases from the complementary slackness analysis. If $y_i - \\theta > 0$, then $x_i^\\star = y_i - \\theta > 0$, and the corresponding $\\mu_i^\\star = 0$. If $y_i - \\theta \\le 0$, then $x_i^\\star = 0$, which requires $\\mu_i^\\star \\ge 0$.\n\nThe value of $\\theta$ is determined by the primal feasibility constraint $\\sum_{i=1}^n x_i^\\star = 1$:\n$$\n\\sum_{i=1}^n \\max(0, y_i - \\theta) = 1\n$$\nLet the function $F(\\theta) = \\sum_{i=1}^n \\max(0, y_i - \\theta)$. We need to find the root of $F(\\theta) - 1 = 0$. $F(\\theta)$ is a continuous, piecewise linear, and monotonically non-increasing function of $\\theta$. This property allows for an efficient algorithm to find the unique $\\theta$.\n\nThe algorithm is as follows:\n1.  Sort the components of the input vector $y$ in descending order to obtain a new vector $u$, where $u_1 \\ge u_2 \\ge \\dots \\ge u_n$.\n2.  Find the largest integer $\\rho \\in \\{1, \\dots, n\\}$ such that\n    $$\n    u_j - \\frac{1}{j}\\left(\\sum_{k=1}^j u_k - 1\\right) > 0 \\quad \\text{for } j=1, \\dots, \\rho\n    $$\n    This can be found efficiently. Let $s_j = \\sum_{k=1}^j u_k$ be the cumulative sum. We seek the largest $j$ such that this condition holds.\n3.  With this value of $\\rho$, the required threshold $\\theta$ is given by:\n    $$\n    \\theta = \\frac{1}{\\rho}\\left(\\sum_{k=1}^\\rho u_k - 1\\right) = \\frac{s_\\rho - 1}{\\rho}\n    $$\n4.  Once $\\theta$ is found, the optimal solution $x^\\star$ is computed component-wise using the original vector $y$:\n    $$\n    x_i^\\star = \\max(0, y_i - \\theta) \\quad \\text{for } i=1, \\dots, n\n    $$\n\nThis algorithm is guaranteed to find the unique solution for any given $y \\in \\mathbb{R}^n$. The final step is to round each component of the computed $x^\\star$ to $6$ decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef project_on_simplex(y):\n    \"\"\"\n    Project a vector y onto the probability simplex.\n\n    This function solves the convex quadratic program:\n    min ||x - y||_2^2\n    s.t. sum(x) = 1, x_i >= 0 for all i.\n\n    The algorithm is based on the efficient method described in papers\n    such as \"Efficient Projections onto the L1-Ball for Learning in\n    High Dimensions\" by Duchi et al. (2008) and related works.\n\n    Args:\n        y (np.ndarray): The input vector to project.\n\n    Returns:\n        np.ndarray: The projected vector x_star.\n    \"\"\"\n    y_arr = np.asarray(y)\n    n = len(y_arr)\n\n    # Sort the input vector in descending order.\n    u = np.sort(y_arr)[::-1]\n\n    # Compute the cumulative sum of the sorted vector.\n    s = np.cumsum(u)\n\n    # Find the value rho, which is the number of positive elements in the solution.\n    # This is done by finding the largest j for which u_j - (1/j)(s_j - 1) > 0.\n    j_indices = np.arange(1, n + 1)\n    \n    # The condition will always be met for at least j=1 (1 > 0 unless n=0),\n    # so np.where will not be empty.\n    conditions = u - (s - 1) / j_indices > 0\n    \n    # Get the index of the last element that satisfies the condition.\n    # This corresponds to rho. Since indices are 0-based, rho = index + 1.\n    rho_idx = np.where(conditions)[0][-1]\n    rho = rho_idx + 1\n\n    # Compute the threshold theta.\n    theta = (s[rho_idx] - 1) / rho\n\n    # Compute the projection x_star by applying the threshold.\n    x_star = np.maximum(0, y_arr - theta)\n    \n    return x_star\n\ndef solve():\n    \"\"\"\n    Solves the projection on simplex problem for a suite of test cases\n    and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (0.2, 0.6, 0.7),\n        (0.1, 0.2, 0.7),\n        (-0.5, 0.3, 1.2),\n        (-1.0, -2.0, -3.0),\n        (0.5, 0.5, 0.5),\n        (0.1, 0.2, -0.3),\n        (10.0, -10.0, 0.0),\n        (-2.0,),\n        (0.9, 0.1, 0.1, -0.2, 0.0, 0.1),\n    ]\n\n    results_as_strings = []\n    for case in test_cases:\n        # Calculate the projection\n        x_star = project_on_simplex(case)\n        \n        # Round each component to 6 decimal places\n        rounded_x = [round(val, 6) for val in x_star]\n        \n        # Format the list of numbers into the required string format '[n1,n2,...]'\n        list_str = f\"[{','.join(map(str, rounded_x))}]\"\n        results_as_strings.append(list_str)\n\n    # Join all results into a single string '[[...],[...]]'\n    final_output = f\"[{','.join(results_as_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}