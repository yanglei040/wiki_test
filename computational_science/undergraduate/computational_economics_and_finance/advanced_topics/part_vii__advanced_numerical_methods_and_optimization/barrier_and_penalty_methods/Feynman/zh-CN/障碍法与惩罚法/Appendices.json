{
    "hands_on_practices": [
        {
            "introduction": "罚函数法通过求解一系列罚参数递增的无约束子问题来逼近约束最优化问题的解。这种序列方法的计算效率在很大程度上取决于如何为每个子问题选择初始点。本练习旨在引导你从头开始实现一个序列二次罚函数法，并通过实验来量化“热启动”（warm-starting）策略——即利用上一步的解作为下一步的初始猜测值——所带来的优势，从而让你对算法的实际动态获得直观理解。",
            "id": "2423453",
            "problem": "要求您为约束优化问题实现一种序列二次惩罚法，并通过实验量化温启动（即将前一个子问题的解作为下一个惩罚参数的初始猜测）所带来的益处。实现一个带有回溯 Armijo 线搜索的基于梯度的求解器，以最小化一系列无约束惩罚子问题。对于固定的惩罚参数序列 $\\{\\rho_k\\}_{k=1}^K$（其中 $\\rho_1 < \\rho_2 < \\dots < \\rho_K$），请对每个测试案例比较两种策略：(i) 从相同的初始点对每个子问题进行冷启动，以及 (ii) 从子问题 $k$ 的计算出的极小化点对子问题 $k+1$ 进行温启动。报告加速比，其定义为在整个惩罚序列中，冷启动所用的梯度下降总迭代次数与温启动所用的梯度下降总迭代次数之比。\n\n使用的基本原理和定义：\n- 一个约束最小化问题包含目标函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$，不等式约束 $g_i(x)\\le 0$（$i\\in\\{1,\\dots,m\\}$），以及等式约束 $h_j(x)=0$（$j\\in\\{1,\\dots,p\\}$）。\n- 经典二次惩罚对不等式约束的违背量应用为 $\\max\\{0, g_i(x)\\}^2$，对等式约束应用为 $h_j(x)^2$。\n- 对于给定的 $\\rho>0$，惩罚子问题是最小化\n$$\n\\Phi_\\rho(x)=f(x)+\\rho\\left(\\sum_{i=1}^m \\max\\{0,g_i(x)\\}^2+\\sum_{j=1}^p h_j(x)^2\\right).\n$$\n- 使用带有回溯 Armijo 法则的梯度下降法：给定当前点 $x$、梯度 $\\nabla\\Phi_\\rho(x)$、初始步长 $t_0$、缩减因子 $\\beta\\in(0,1)$ 和 Armijo 参数 $c\\in(0,1)$，从序列 $\\{t_0, \\beta t_0, \\beta^2 t_0,\\dots\\}$ 中选择满足以下条件的最大 $t$：\n$$\n\\Phi_\\rho(x - t \\nabla \\Phi_\\rho(x)) \\le \\Phi_\\rho(x) - c\\,t\\,\\|\\nabla \\Phi_\\rho(x)\\|_2^2.\n$$\n- 当 $\\|\\nabla \\Phi_\\rho(x)\\|_2\\le \\varepsilon$ 时，停止内部求解器。\n\n实现要求：\n- 完全按照上述定义实现二次惩罚法和带有回溯 Armijo 线搜索的梯度下降法。\n- 对于不等式约束，通过在惩罚值及其梯度中均使用 $\\max\\{0,\\cdot\\}$ 结构，仅处理正向违背量。对于等式约束，惩罚其残差的平方。\n- 使用惩罚序列 $\\rho\\in\\{10,10^2,10^3\\}$，即 $\\rho \\in \\{10,100,1000\\}$。\n- 对所有子问题，使用梯度容差 $\\varepsilon=10^{-6}$、Armijo 参数 $c=10^{-4}$、缩减因子 $\\beta=\\tfrac{1}{2}$ 和初始步长 $t_0=1$。每个子问题的最大梯度迭代次数上限为 $N_{\\max}=10^4$。\n- 计算使一个子问题收敛所需的外部梯度下降迭代次数（每次线搜索后接受的步数）；不要单独计算线搜索的回溯步数。\n\n测试套件：\n实现并求解以下三个二维测试案例。在每个案例中，返回加速比\n$$\nS=\\frac{N_{\\mathrm{cold}}}{N_{\\mathrm{warm}}},\n$$\n其中 $N_{\\mathrm{cold}}$ 是从指定初始点对每个子问题进行冷启动时，在所有惩罚参数上累加的梯度下降总迭代次数；$N_{\\mathrm{warm}}$ 是从前一个子问题的解对每个子问题进行温启动时的总迭代次数。\n\n- 案例 $\\mathbf{A}$（带有激活线性不等式约束的凸二次问题）：\n  - 目标函数：$f(x,y)=(x-1)^2+2\\,(y+2)^2$。\n  - 不等式约束：$g_1(x,y)=1-x-y\\le 0$。\n  - 无等式约束。\n  - 初始点：$x_0=(0,0)$。\n\n- 案例 $\\mathbf{B}$（带有等式约束的凸二次问题）：\n  - 目标函数：$f(x,y)=(x-3)^2+(y-1)^2$。\n  - 等式约束：$h_1(x,y)=x-y=0$。\n  - 无不等式约束。\n  - 初始点：$x_0=(0,0)$。\n\n- 案例 $\\mathbf{C}$（带有曲线不等式约束的凸二次问题）：\n  - 目标函数：$f(x,y)=(x+2)^2+y^2$。\n  - 不等式约束：$g_1(x,y)=x^2+y^2-1\\le 0$。\n  - 无等式约束。\n  - 初始点：$x_0=(0,0)$。\n\n输出规范：\n- 对每个案例，计算如上定义的加速比 $S$。\n- 您的程序应生成单行输出，包含三个加速比，按 $\\left[S_A,S_B,S_C\\right]$ 的顺序以逗号分隔的列表形式放在方括号内，其中 $S_A$ 对应案例 $\\mathbf{A}$，$S_B$ 对应案例 $\\mathbf{B}$，$S_C$ 对应案例 $\\mathbf{C}$。例如，输出形式为 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$，其中包含数值。\n- 将每个加速比表示为浮点数。您可以在内部进行四舍五入，但打印出的值必须是标准的十进制浮点数。\n\n不涉及物理单位。不使用角度。不使用百分比。\n\n最终程序必须是自包含的，无需输入，并遵守指定的运行时环境。正确性将通过验证实现是否遵循定义，以及温启动产生的迭代次数是否严格更少或至少不多于冷启动，从而为指定案例生成有意义的加速比来评估。输出必须严格为指定格式的一行。",
            "solution": "该问题要求实现序列二次惩罚法来解决约束优化问题。任务的核心是比较求解无约束子问题序列时两种初始化策略的计算效率：冷启动策略与温启动策略。效率将通过加速比来量化，该加速比定义为梯度下降总迭代次数的比率。\n\n约束优化问题的一般形式是在一组不等式约束 $g_i(x) \\le 0$（$i \\in \\{1, \\dots, m\\}$）和等式约束 $h_j(x) = 0$（$j \\in \\{1, \\dots, p\\}$）的条件下，最小化目标函数 $f(x)$，其中 $x \\in \\mathbb{R}^n$。\n\n二次惩罚法通过求解一系列无约束最小化问题来近似此问题的解。对于给定的惩罚参数 $\\rho > 0$，通过向原始目标函数添加惩罚约束违背的项来构造惩罚目标函数 $\\Phi_\\rho(x)$。该惩罚函数的具体形式为：\n$$\n\\Phi_\\rho(x) = f(x) + \\rho \\left( \\sum_{i=1}^m \\left(\\max\\{0, g_i(x)\\}\\right)^2 + \\sum_{j=1}^p \\left(h_j(x)\\right)^2 \\right)\n$$\n然后，相对于 $x$ 最小化此函数 $\\Phi_\\rho(x)$。通过对一系列递增的惩罚参数 $\\rho_1 < \\rho_2 < \\dots < \\rho_K$ 求解该无约束问题，极小化点序列 $x^*(\\rho_k)$ 将收敛到原始约束问题的解。\n\n为了最小化每个无约束子问题 $\\min_x \\Phi_\\rho(x)$，需要一种基于梯度的方法。惩罚目标函数的梯度 $\\nabla \\Phi_\\rho(x)$ 使用链式法则推导得出。对于不等式约束项 $P_i(x) = \\rho (\\max\\{0, g_i(x)\\})^2$，其梯度为 $\\nabla P_i(x) = 2 \\rho \\max\\{0, g_i(x)\\} \\nabla g_i(x)$。对于等式约束项 $Q_j(x) = \\rho (h_j(x))^2$，其梯度为 $\\nabla Q_j(x) = 2 \\rho h_j(x) \\nabla h_j(x)$。将这些与目标函数的梯度结合，完整的梯度为：\n$$\n\\nabla \\Phi_\\rho(x) = \\nabla f(x) + 2\\rho \\left( \\sum_{i=1}^m \\max\\{0, g_i(x)\\} \\nabla g_i(x) + \\sum_{j=1}^p h_j(x) \\nabla h_j(x) \\right)\n$$\n无约束最小化使用梯度下降法执行。从点 $x_k$ 开始，通过沿负梯度方向移动来找到下一个点 $x_{k+1}$：\n$$\nx_{k+1} = x_k - t \\nabla \\Phi_\\rho(x_k)\n$$\n步长 $t > 0$ 由采用 Armijo 条件的回溯线搜索确定。对于给定的下降方向 $d_k = -\\nabla \\Phi_\\rho(x_k)$，我们从序列 $\\{t_0, \\beta t_0, \\beta^2 t_0, \\dots\\}$ 中寻找满足以下条件的最大 $t$：\n$$\n\\Phi_\\rho(x_k + t d_k) \\le \\Phi_\\rho(x_k) + c \\, t \\, \\nabla \\Phi_\\rho(x_k)^T d_k\n$$\n使用 $d_k = -\\nabla \\Phi_\\rho(x_k)$，该条件简化为问题陈述中给出的形式：\n$$\n\\Phi_\\rho(x_k - t \\nabla \\Phi_\\rho(x_k)) \\le \\Phi_\\rho(x_k) - c \\, t \\, \\|\\nabla \\Phi_\\rho(x_k)\\|_2^2\n$$\n算法迭代进行，直到梯度的范数低于指定的容差 $\\varepsilon$，即 $\\|\\nabla \\Phi_\\rho(x)\\|_2 \\le \\varepsilon$。此求解器的参数是固定的：初始步长 $t_0=1$，Armijo 参数 $c=10^{-4}$，缩减因子 $\\beta=0.5$，以及梯度范数容差 $\\varepsilon=10^{-6}$。每个子问题的最大迭代次数上限为 $N_{\\max}=10^4$。\n\n实验比较了在惩罚参数序列 $\\rho \\in \\{10, 100, 1000\\}$ 上的两种策略：\n1.  **冷启动：** 每个关于 $\\rho_k$ 的子问题都从相同的起始点 $x_0$ 初始化。总迭代次数 $N_{\\mathrm{cold}}$ 是独立求解每个子问题所需迭代次数的总和。\n2.  **温启动：** 第一个子问题（关于 $\\rho_1=10$）从 $x_0$ 初始化。后续每个关于 $\\rho_{k+1}$ 的子问题都使用从前一个关于 $\\rho_k$ 的子问题获得的解来初始化。总迭代次数 $N_{\\mathrm{warm}}$ 是此序列中所有迭代次数的总和。\n\n温启动的基本原理是，解 $x^*(\\rho_k)$ 有望成为 $\\Phi_{\\rho_{k+1}}(x)$ 极小化点的一个良好初始猜测，特别是当 $\\rho_{k+1}$ 不比 $\\rho_k$ 大很多时。这应能带来更快的收敛速度。性能增益通过加速比 $S = N_{\\mathrm{cold}} / N_{\\mathrm{warm}}$ 来衡量。\n\n实现过程将首先为每个测试案例的目标函数、约束函数及其各自的梯度定义 Python 函数。一个通用的求解器函数将执行带有 Armijo 线搜索的梯度下降。一个顶层函数将管理惩罚参数序列，应用冷启动和温启动策略，计算每种策略的总迭代次数，并计算加速比。对所有三个提供的测试案例都将重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    \n    # --- Solver Parameters ---\n    SOLVER_PARAMS = {\n        'epsilon': 1e-6,\n        'c_armijo': 1e-4,\n        'beta': 0.5,\n        't0': 1.0,\n        'n_max': 10000\n    }\n    PENALTY_PARAMS = [10.0, 100.0, 1000.0]\n\n    # --- Test Case Definitions ---\n    \n    # Case A: (x-1)^2 + 2(y+2)^2, s.t. 1-x-y <= 0\n    case_A = {\n        'f': lambda x: (x[0] - 1.0)**2 + 2.0 * (x[1] + 2.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 1.0), 4.0 * (x[1] + 2.0)]),\n        'g': [lambda x: 1.0 - x[0] - x[1]],\n        'grad_g': [lambda x: np.array([-1.0, -1.0])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    # Case B: (x-3)^2 + (y-1)^2, s.t. x-y = 0\n    case_B = {\n        'f': lambda x: (x[0] - 3.0)**2 + (x[1] - 1.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 3.0), 2.0 * (x[1] - 1.0)]),\n        'g': [],\n        'grad_g': [],\n        'h': [lambda x: x[0] - x[1]],\n        'grad_h': [lambda x: np.array([1.0, -1.0])],\n        'x0': np.array([0.0, 0.0])\n    }\n    \n    # Case C: (x+2)^2 + y^2, s.t. x^2+y^2-1 <= 0\n    case_C = {\n        'f': lambda x: (x[0] + 2.0)**2 + x[1]**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] + 2.0), 2.0 * x[1]]),\n        'g': [lambda x: x[0]**2 + x[1]**2 - 1.0],\n        'grad_g': [lambda x: np.array([2.0 * x[0], 2.0 * x[1]])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    \n    def get_penalized_funcs(case, rho):\n        \"\"\"Creates the penalized function and its gradient for a given case and rho.\"\"\"\n        \n        def phi(x):\n            f_val = case['f'](x)\n            g_sum = sum(max(0, g_func(x))**2 for g_func in case['g'])\n            h_sum = sum(h_func(x)**2 for h_func in case['h'])\n            return f_val + rho * (g_sum + h_sum)\n\n        def grad_phi(x):\n            grad_f_val = case['grad_f'](x)\n            \n            grad_g_sum = np.zeros_like(x)\n            for g_func, grad_g_func in zip(case['g'], case['grad_g']):\n                g_val = g_func(x)\n                if g_val > 0:\n                    grad_g_sum += 2.0 * g_val * grad_g_func(x)\n\n            grad_h_sum = np.zeros_like(x)\n            for h_func, grad_h_func in zip(case['h'], case['grad_h']):\n                h_val = h_func(x)\n                grad_h_sum += 2.0 * h_val * grad_h_func(x)\n                \n            return grad_f_val + rho * (grad_g_sum + grad_h_sum)\n        \n        return phi, grad_phi\n\n    def gradient_descent(phi, grad_phi, x_init, params):\n        \"\"\"\n        Performs gradient descent with backtracking Armijo line search.\n        \"\"\"\n        x = np.copy(x_init)\n        n_iters = 0\n        \n        for k in range(params['n_max']):\n            grad = grad_phi(x)\n            grad_norm_sq = np.dot(grad, grad)\n\n            if np.sqrt(grad_norm_sq) <= params['epsilon']:\n                break\n            \n            # Backtracking line search\n            t = params['t0']\n            phi_x = phi(x)\n            \n            while True:\n                x_new = x - t * grad\n                phi_new = phi(x_new)\n                armijo_check = phi_x - params['c_armijo'] * t * grad_norm_sq\n                \n                if phi_new <= armijo_check:\n                    break\n                t *= params['beta']\n            \n            x = x_new\n            n_iters += 1\n        \n        return x, n_iters\n\n    def run_penalty_method(case, solver_params, penalty_params):\n        \"\"\"\n        Runs the full sequential penalty method for a case,\n        calculating iterations for both cold and warm starts.\n        \"\"\"\n        # Cold start\n        total_iters_cold = 0\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            _, n_iters = gradient_descent(phi, grad_phi, case['x0'], solver_params)\n            total_iters_cold += n_iters\n            \n        # Warm start\n        total_iters_warm = 0\n        x_warm = np.copy(case['x0'])\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            x_sol, n_iters = gradient_descent(phi, grad_phi, x_warm, solver_params)\n            total_iters_warm += n_iters\n            x_warm = x_sol\n            \n        if total_iters_warm == 0:\n             # This case should not happen in this problem, but is a safeguard.\n             # If cold is also 0, speedup is 1. If cold > 0, speedup is \"infinite\".\n            return 1.0 if total_iters_cold == 0 else float('inf')\n            \n        return float(total_iters_cold) / float(total_iters_warm)\n\n    results = []\n    for case in test_cases:\n        speedup = run_penalty_method(case, SOLVER_PARAMS, PENALTY_PARAMS)\n        results.append(speedup)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在处理等式约束时，不同的罚函数会带来截然不同的理论性质和计算挑战。光滑的二次（$L_2$）罚函数虽然易于优化，但通常需要将罚参数 $\\rho$ 趋于无穷大才能获得可行解，这会引发严重的数值病态问题。相比之下，非光滑的 $L_1$ 罚函数则具有“精确性”（exactness），即在有限的罚参数下就能得到原问题的精确解。通过本练习的分析对比，你将深入理解在光滑性、精确性和数值稳定性之间的权衡，这是选择和设计高效优化算法的核心。",
            "id": "2423474",
            "problem": "在一个计算工程设计任务中，考虑等式约束优化问题：最小化$f(x)$，约束条件为$h(x)=0$，其中$x \\in \\mathbb{R}^n$，$f:\\mathbb{R}^n \\to \\mathbb{R}$，$h:\\mathbb{R}^n \\to \\mathbb{R}$。假设$f$和$h$是连续可微的，并且在一个约束局部极小值点$x^\\star$处，线性无关约束规范(LICQ)成立且二阶充分条件(SOSC)被满足。采用两种无约束罚函数形式：\n( i ) 二次罚函数 $F_{\\mathrm{QP}}(x;\\rho) = f(x) + \\rho\\,h(x)^2$，$ \\rho > 0$。\n( ii ) $L_1$罚函数 $F_{L_1}(x;\\rho) = f(x) + \\rho\\,|h(x)|$，$\\rho > 0$。\n在光滑性、条件性和在有限与无限$\\rho$值下恢复约束解的能力方面，比较这两种罚函数方法。选择所有正确的陈述。\n\nA. 如果$f$和$h$是光滑的，那么对于所有$x$和任何$\\rho>0$，$F_{\\mathrm{QP}}(x;\\rho)$都是光滑的，但要强制满足可行性通常需要$\\rho \\to \\infty$，这可能导致在可行点附近Hessian矩阵的病态问题。\n\nB. 对于等式约束，$L_1$罚函数可以是精确的：在标准正则性条件下，存在一个有限的$\\bar{\\rho} \\ge |\\lambda^\\star|$，使得对于所有$\\rho \\ge \\bar{\\rho}$，带有Karush–Kuhn–Tucker (KKT)乘子$\\lambda^\\star$的约束局部极小值点$x^\\star$也是$F_{L_1}(x;\\rho)$的一个局部极小值点；然而，$F_{L_1}(x;\\rho)$在$h(x)=0$的点上是非光滑的。\n\nC. 因为$F_{L_1}(x;\\rho)$仅在远离可行域时才不可微，所以直接将牛顿法应用于$F_{L_1}(x;\\rho)$时，在可行点附近享有二次收敛性。\n\nD. 二次罚函数方法随着$\\rho$的增加可以避免病态问题，因为$F_{\\mathrm{QP}}(x;\\rho)$的Hessian矩阵在$\\rho$上保持一致有界。\n\nE. 当$h(x)$是线性时，二次罚函数和$L_1$罚函数是等价的，即对于任何固定的$\\rho>0$，它们对所有光滑的$f$都产生相同的极小值点。",
            "solution": "首先将验证问题陈述的科学性和逻辑完整性。\n\n### 步骤1：提取已知条件\n-   **优化问题**：最小化$f(x)$，约束条件为$h(x)=0$。\n-   **定义域和函数**：$x \\in \\mathbb{R}^n$, $f:\\mathbb{R}^n \\to \\mathbb{R}$, $h:\\mathbb{R}^n \\to \\mathbb{R}$。\n-   **假设**：\n    1.  $f$和$h$是连续可微的。\n    2.  $x^\\star$是一个约束局部极小值点。\n    3.  线性无关约束规范(LICQ)在$x^\\star$处成立。\n    4.  二阶充分条件(SOSC)在$x^\\star$处被满足。\n-   **罚函数形式**：\n    1.  二次罚函数：$F_{\\mathrm{QP}}(x;\\rho) = f(x) + \\rho\\,h(x)^2$，$ \\rho > 0$。\n    2.  $L_1$罚函数：$F_{L_1}(x;\\rho) = f(x) + \\rho\\,|h(x)|$，$ \\rho > 0$。\n-   **任务**：比较两种罚函数形式，并评估给定陈述的正确性。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题设置在非线性约束优化的标准框架内。所描述的函数、假设和方法是计算优化和工程设计研究中的经典主题。\n\n-   **科学依据**：该问题涉及两种基本的罚函数方法：二次罚函数和非光滑的$L_1$精确罚函数。所有概念，如KKT条件、LICQ和SOSC，都是优化理论的基石。该问题是科学严谨的。\n-   **适定性**：该问题定义明确。它要求在标准假设下对标准方法进行定性和理论上的比较。所提供的条件（光滑性、LICQ、SOSC）正是保证唯一拉格朗日乘子$\\lambda^\\star$存在以及解$x^\\star$的局部稳定性所必需的，这对于分析两种罚函数方法至关重要。进行独特而有意义的分析是可能的。\n-   **客观性**：语言是技术性的，没有歧义。\n\n问题陈述没有科学或逻辑上的缺陷。这是数值优化领域一个有效的理论问题。\n\n### 步骤3：结论与行动\n问题陈述是有效的。将进行全面分析。\n\n### 解题推导\n\n该问题要求分析两种不同的罚函数方法，用于解决等式约束问题：\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{subject to} \\quad h(x) = 0\n$$\n在局部极小值点$x^\\star$处的Karush-Kuhn-Tucker (KKT)条件表明，存在一个拉格朗日乘子$\\lambda^\\star \\in \\mathbb{R}$，使得：\n$$\n\\nabla f(x^\\star) + \\lambda^\\star \\nabla h(x^\\star) = 0\n$$\n$$\nh(x^\\star) = 0\n$$\n所提供的假设(LICQ和SOSC)确保$x^\\star$是一个正则点和一个严格局部极小值点。\n\n让我们分析每种罚函数形式。\n\n**1. 二次罚函数: $F_{\\mathrm{QP}}(x;\\rho) = f(x) + \\rho\\,h(x)^2$**\n\n-   **光滑性**：问题陈述$f$和$h$是连续可微的($C^1$)。函数$z \\mapsto z^2$是无限可微的($C^\\infty$)。$C^1$函数的复合和求和仍然是$C^1$的。因此，$F_{\\mathrm{QP}}(x;\\rho)$是连续可微的。如果假设$f$和$h$是二次连续可微的($C^2$)，那么$F_{\\mathrm{QP}}(x;\\rho)$也是$C^2$的。因此，它是一个光滑函数，可以应用标准的无约束优化算法（如牛顿法）。\n\n-   **收敛性与精确性**：设$x(\\rho)$是$F_{\\mathrm{QP}}(x;\\rho)$的一个极小值点。一阶必要条件是$\\nabla F_{\\mathrm{QP}}(x(\\rho);\\rho) = 0$，这给出：\n    $$\n    \\nabla f(x(\\rho)) + 2\\rho h(x(\\rho)) \\nabla h(x(\\rho)) = 0\n    $$\n    一个标准结果是，当$\\rho \\to \\infty$时，极小值点序列$\\{x(\\rho)\\}$收敛于约束极小值点$x^\\star$。将$F_{\\mathrm{QP}}$的最优性条件与KKT条件进行比较，我们可以将$2\\rho h(x(\\rho))$看作是拉格朗日乘子$-\\lambda^\\star$的一个近似。因此，$h(x(\\rho)) \\approx -\\lambda^\\star / (2\\rho)$。对于任何有限的$\\rho > 0$，我们有$h(x(\\rho)) \\neq 0$（除非$\\lambda^\\star=0$，这是一个平凡的情况）。这意味着该方法不是*精确*的；可行性仅在极限$\\rho \\to \\infty$时才能达到。\n\n-   **条件性**：$F_{\\mathrm{QP}}(x;\\rho)$的Hessian矩阵是：\n    $$\n    \\nabla^2 F_{\\mathrm{QP}}(x;\\rho) = \\nabla^2 f(x) + 2\\rho \\nabla h(x) \\nabla h(x)^T + 2\\rho h(x) \\nabla^2 h(x)\n    $$\n    当$\\rho \\to \\infty$时，我们有$x(\\rho) \\to x^\\star$且$2\\rho h(x(\\rho)) \\to -\\lambda^\\star$。在$x(\\rho)$处的Hessian矩阵变为：\n    $$\n    \\nabla^2 F_{\\mathrm{QP}}(x(\\rho);\\rho) \\approx \\left( \\nabla^2 f(x^\\star) + \\lambda^\\star \\nabla^2 h(x^\\star) \\right) + 2\\rho \\nabla h(x^\\star) \\nabla h(x^\\star)^T\n    $$\n    括号中的项是拉格朗日函数$\\nabla_{xx}^2 \\mathcal{L}(x^\\star, \\lambda^\\star)$的Hessian矩阵。第二项$2\\rho \\nabla h(x^\\star) \\nabla h(x^\\star)^T$是一个秩一矩阵，其范数随$\\rho$线性增长。这一项导致Hessian矩阵在$\\nabla h(x^\\star)$方向上的一个特征值趋于无穷大，而在与$\\nabla h(x^\\star)$正交的方向上的特征值保持有界。因此，$\\nabla^2 F_{\\mathrm{QP}}$的条件数随着$\\rho \\to \\infty$而发散，导致严重的数值病态问题。\n\n**2. $L_1$罚函数: $F_{L_1}(x;\\rho) = f(x) + \\rho\\,|h(x)|$**\n\n-   **光滑性**：绝对值函数$|z|$在$z=0$处不可微。因此，$F_{L_1}(x;\\rho)$在所有$h(x)=0$的点$x$处是不可微的，而这恰好是原问题的可行集。这种非光滑性妨碍了像牛顿法这样需要可微性的经典梯度方法的直接应用。\n\n-   **精确性**：这个函数被称为*精确*罚函数。优化理论中的一个关键结果指出，如果$x^\\star$是满足SOSC的严格局部极小值点，那么存在一个阈值$\\bar{\\rho}$，使得对于所有$\\rho > \\bar{\\rho}$，$x^\\star$也是$F_{L_1}(x;\\rho)$的一个严格局部极小值点。该阈值与拉格朗日乘子的大小有关：必须选择$\\rho > |\\lambda^\\star|$。因此，对于一个足够大但有限的$\\rho$，可以通过求解一个无约束（但非光滑）的最小化问题来找到精确的约束解。\n\n### 逐项分析\n\n**A. 如果$f$和$h$是光滑的，那么对于所有$x$和任何$\\rho>0$，$F_{\\mathrm{QP}}(x;\\rho)$都是光滑的，但要强制满足可行性通常需要$\\rho \\to \\infty$，这可能导致在可行点附近Hessian矩阵的病态问题。**\n-   如我们分析所确立的，$F_{\\mathrm{QP}}(x;\\rho)$是光滑的。\n-   如已确立的，可行性仅在极限$\\rho \\to \\infty$时获得。\n-   如已确立的，Hessian矩阵随着$\\rho \\to \\infty$而变得病态。\n-   **结论：正确。**\n\n**B. 对于等式约束，$L_1$罚函数可以是精确的：在标准正则性条件下，存在一个有限的$\\bar{\\rho} \\ge |\\lambda^\\star|$，使得对于所有$\\rho \\ge \\bar{\\rho}$，带有Karush–Kuhn–Tucker (KKT)乘子$\\lambda^\\star$的约束局部极小值点$x^\\star$也是$F_{L_1}(x;\\rho)$的一个局部极小值点；然而，$F_{L_1}(x;\\rho)$在$h(x)=0$的点上是非光滑的。**\n-   陈述的第一部分正确定义了精确罚函数的性质，其对$\\rho$的阈值条件与拉格朗日乘子$\\lambda^\\star$正确相关。问题假设了“标准正则性”(LICQ, SOSC)，这正是该定理成立所需要的。请注意，该条件通常表述为严格不等式$\\rho > |\\lambda^\\star|$，以使$x^\\star$成为*严格*局部极小值点，但对于局部极小值点，陈述中的$\\rho \\ge |\\lambda^\\star|$是有效的。\n-   第二部分正确地指出，该函数在可行集上（其中$h(x)=0$）是非光滑的。\n-   **结论：正确。**\n\n**C. 因为$F_{L_1}(x;\\rho)$仅在远离可行域时才不可微，所以直接将牛顿法应用于$F_{L_1}(x;\\rho)$时，在可行点附近享有二次收敛性。**\n-   前提“$F_{L_1}(x;\\rho)$仅在远离可行域时才不可微”在事实上是错误的。不可微性恰好发生在可行集*上*（其中$h(x)=0$），而不是远离它。在远离可行域的地方，即$h(x) \\neq 0$处，该函数是局部光滑的。\n-   因为解$x^\\star$位于不可微流形上，需要Hessian矩阵的标准牛顿法不适用。需要使用非光滑优化技术。因此，关于二次收敛的结论是没有根据的。\n-   **结论：不正确。**\n\n**D. 二次罚函数方法随着$\\rho$的增加可以避免病态问题，因为$F_{\\mathrm{QP}}(x;\\rho)$的Hessian矩阵在$\\rho$上保持一致有界。**\n-   这个陈述与事实完全相反。如对$F_{\\mathrm{QP}}$的分析所示，Hessian矩阵$\\nabla^2 F_{\\mathrm{QP}}$包含项$2\\rho \\nabla h(x) \\nabla h(x)^T$，它随着$\\rho$无限增长。这导致了严重的病态问题。Hessian矩阵并非一致有界。\n-   **结论：不正确。**\n\n**E. 当$h(x)$是线性时，二次罚函数和$L_1$罚函数是等价的，即对于任何固定的$\\rho>0$，它们对所有光滑的$f$都产生相同的极小值点。**\n-   这个说法是错误的。二次罚项($h^2$)和绝对值罚项($|h|$)之间的根本区别不会因为$h(x)$是线性的就消失。让我们考虑一个简单的问题：最小化$f(x)=x^2$，约束条件为$h(x)=x-1=0$。解是$x^\\star=1$。\n-   二次罚函数的极小值点是$x(\\rho) = (x^2 + \\rho(x-1)^2)' = 2x+2\\rho(x-1)=0 \\implies x=\\frac{\\rho}{1+\\rho}$。\n-   对于足够大的$\\rho$（特别是$\\rho \\ge |\\lambda^\\star|=2$），$L_1$罚函数的极小值点是精确解$x=1$。\n-   对于任何$\\rho > 0$，$\\frac{\\rho}{1+\\rho} \\neq 1$。例如，如果$\\rho=3$，则$F_{\\mathrm{QP}}$的极小值点是$x=3/4$，而$F_{L_1}$的极小值点是$x=1$。极小值点不相同。\n-   **结论：不正确。**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "罚函数法的一个强大应用是为具有复杂约束集的优化问题寻找一个可行的初始点，这通常被称为“第一阶段”（Phase I）方法。通过最小化一个由所有约束的违反程度构成的“优值函数”（merit function），我们可以系统地驱动一个解朝向可行域。本次实践将引导你将这一优化技术应用于计算金融中的一个具体场景：通过构建和最小化一个综合了预算、风险和预期收益等多重约束的罚函数，来寻找一个满足所有条件的初始投资组合，这在实际的投资组合优化中是至关重要的一步。",
            "id": "2374527",
            "problem": "考虑构建一个满足一系列经济和金融约束的初始投资组合向量的任务。令 $n \\in \\mathbb{N}$ 表示资产数量，$\\mu \\in \\mathbb{R}^n$ 为预期收益向量，$\\Sigma \\in \\mathbb{R}^{n \\times n}$ 为收益的对称正定协方差矩阵。令 $u \\in \\mathbb{R}^n$ 为权重分量的上界向量。投资组合权重为 $w \\in \\mathbb{R}^n$。需要满足的约束条件如下：\n- 预算等式：$\\sum_{i=1}^n w_i = 1$。\n- 下界（禁止卖空）：对于所有 $i \\in \\{1,\\dots,n\\}$，有 $w_i \\ge 0$。\n- 上界：对于所有 $i \\in \\{1,\\dots,n\\}$，有 $w_i \\le u_i$。\n- 要求的预期收益：$\\mu^\\top w \\ge R_{\\text{target}}$。\n- 风险上限：$w^\\top \\Sigma w \\le V_{\\max}$。\n\n将不等式函数以标准形式 $g(w) \\le 0$ 定义，等式函数以 $h(w) = 0$ 定义如下：\n- $g_{\\text{ret}}(w) = R_{\\text{target}} - \\mu^\\top w$,\n- $g_{\\text{var}}(w) = w^\\top \\Sigma w - V_{\\max}$,\n- $g_{\\text{lo},i}(w) = -w_i$ 对于每个 $i \\in \\{1,\\dots,n\\}$,\n- $g_{\\text{up},i}(w) = w_i - u_i$ 对于每个 $i \\in \\{1,\\dots,n\\}$,\n- $h_{\\text{bud}}(w) = \\mathbf{1}^\\top w - 1$，其中 $\\mathbf{1}$ 是 $\\mathbb{R}^n$ 中的全1向量。\n\n对于任意惩罚参数 $\\rho > 0$，定义优值函数\n$$\nM_\\rho(w) \\;=\\; \\rho \\left( \\sum_{i=1}^n \\bigl(\\max\\{0, g_{\\text{lo},i}(w)\\}\\bigr)^2 \\;+\\; \\sum_{i=1}^n \\bigl(\\max\\{0, g_{\\text{up},i}(w)\\}\\bigr)^2 \\;+\\; \\bigl(\\max\\{0, g_{\\text{ret}}(w)\\}\\bigr)^2 \\;+\\; \\bigl(\\max\\{0, g_{\\text{var}}(w)\\}\\bigr)^2 \\;+\\; \\bigl(h_{\\text{bud}}(w)\\bigr)^2 \\right) \\;+\\; \\lambda \\,\\|w\\|_2^2,\n$$\n其中 $\\lambda > 0$ 是一个固定的正则化参数，$\\|\\cdot\\|_2$ 表示欧几里得范数。\n\n对于给定的容差 $\\tau > 0$，将任意 $w$ 处的最大约束违反度定义为\n$$\n\\mathrm{vio}(w) \\;=\\; \\max\\!\\left( \\left| h_{\\text{bud}}(w) \\right|, \\;\\max\\{0, g_{\\text{ret}}(w)\\}, \\;\\max\\{0, g_{\\text{var}}(w)\\}, \\;\\max_{i=1,\\dots,n}\\max\\{0, g_{\\text{lo},i}(w)\\}, \\;\\max_{i=1,\\dots,n}\\max\\{0, g_{\\text{up},i}(w)\\} \\right).\n$$\n\n你的程序必须为下方的每个测试实例，从序列 $\\{10^1, 10^2, 10^3, 10^4, 10^5, 10^6\\}$ 中选择一个 $\\rho$ 值，生成一个近似最小化 $M_\\rho(w)$ 的向量 $w$，然后报告在得到的 $w$ 处 $\\mathrm{vio}(w)$ 的值。对于每个测试实例，从序列中选择能使 $\\mathrm{vio}(w) \\le \\tau$ 成立的最小 $\\rho$（如果存在）；如果没有这样的 $\\rho$ 能使 $\\mathrm{vio}(w) \\le \\tau$ 成立，则报告在最大 $\\rho = 10^6$ 处得到的 $\\mathrm{vio}(w)$。使用固定的容差 $\\tau = 10^{-6}$ 和正则化参数 $\\lambda = 10^{-8}$。\n\n测试套件：\n- 测试用例 A:\n  - $n = 3$,\n  - $\\mu = [\\,0.06,\\; 0.10,\\; 0.14\\,]$,\n  - $\\Sigma = \\begin{bmatrix} 0.010 & 0.002 & 0.001 \\\\ 0.002 & 0.020 & 0.003 \\\\ 0.001 & 0.003 & 0.030 \\end{bmatrix}$,\n  - $u = [\\,0.8,\\; 0.8,\\; 0.8\\,]$,\n  - $R_{\\text{target}} = 0.09$,\n  - $V_{\\max} = 0.025$.\n- 测试用例 B:\n  - $n = 3$,\n  - $\\mu = [\\,0.03,\\; 0.05,\\; 0.07\\,]$,\n  - $\\Sigma = \\begin{bmatrix} 0.008 & 0.001 & 0.0005 \\\\ 0.001 & 0.012 & 0.001 \\\\ 0.0005 & 0.001 & 0.015 \\end{bmatrix}$,\n  - $u = [\\,0.8,\\; 0.8,\\; 0.8\\,]$,\n  - $R_{\\text{target}} = 0.07$,\n  - $V_{\\max} = 0.05$.\n- 测试用例 C:\n  - $n = 4$,\n  - $\\mu = [\\,0.05,\\; 0.08,\\; 0.12,\\; 0.04\\,]$,\n  - $\\Sigma = \\begin{bmatrix}\n  0.005 & 0.001 & 0.001 & 0.0005 \\\\\n  0.001 & 0.010 & 0.002 & 0.001 \\\\\n  0.001 & 0.002 & 0.020 & 0.0015 \\\\\n  0.0005 & 0.001 & 0.0015 & 0.004\n  \\end{bmatrix}$,\n  - $u = [\\,0.6,\\; 0.6,\\; 0.5,\\; 1.0\\,]$,\n  - $R_{\\text{target}} = 0.08$,\n  - $V_{\\max} = 0.012$.\n\n所有测试实例的初始条件：使用任意一个确定性的 $w^{(0)} \\in \\mathbb{R}^n$；例如，使用分量为 $w^{(0)}_i = 1/n$ 的等权重向量 $w^{(0)}$。\n\n你的程序必须在单行中，按照 A、B、C 测试用例的顺序，以如下精确格式输出这三个测试用例的最大约束违反度：一个包含三个浮点数的列表，用方括号括起，数字间用逗号分隔，每个浮点数四舍五入到小数点后恰好六位。例如，一个输出行必须形如 $[v_A,v_B,v_C]$，其中 $v_A$、$v_B$ 和 $v_C$ 均为四舍五入到六位小数的浮点数。不得打印任何额外文本。",
            "solution": "该问题要求找到一个投资组合权重向量 $w \\in \\mathbb{R}^n$，使其满足一系列线性和二次等式及不等式约束。这是一个计算金融领域的可行性问题。为找到这样一个向量，提出的方法是罚函数法，该方法将有约束问题转化为一系列无约束优化问题。\n\n该方法的核心在于构建一个必须被最小化的优值函数 $M_\\rho(w)$。对于给定的惩罚参数 $\\rho > 0$，此函数定义为：\n$$\nM_\\rho(w) \\;=\\; \\rho \\cdot P(w) \\;+\\; \\lambda \\,\\|w\\|_2^2\n$$\n其中 $P(w)$ 是惩罚项，$\\lambda \\|w\\|_2^2$ 是一个正则化项。惩罚项汇总了所有约束的违反情况：\n$$\nP(w) \\;=\\; \\sum_{j} \\bigl(\\max\\{0, g_j(w)\\}\\bigr)^2 \\;+\\; \\sum_{k} \\bigl(h_k(w)\\bigr)^2\n$$\n此处，$g_j(w) \\le 0$ 是不等式约束，$h_k(w) = 0$ 是等式约束。问题陈述中为该投资组合问题明确定义了这些约束：\n- 不等式：$g_{\\text{ret}}(w) = R_{\\text{target}} - \\mu^\\top w$，$g_{\\text{var}}(w) = w^\\top \\Sigma w - V_{\\max}$，$g_{\\text{lo},i}(w) = -w_i$ 和 $g_{\\text{up},i}(w) = w_i - u_i$。\n- 等式：$h_{\\text{bud}}(w) = \\mathbf{1}^\\top w - 1$。\n\n函数 $M_\\rho(w)$ 是一个无约束的连续可微（$C^1$）函数。它的性质至关重要。定义约束的函数 $g_j(w)$ 和 $h_k(w)$ 是仿射函数，或者，对于 $g_{\\text{var}}(w)$，由于协方差矩阵 $\\Sigma$ 是正定的，它是一个凸函数。函数 $\\max\\{0, \\cdot\\}$ 是凸且非递减的。一个凸函数与一个非负、非递减的凸函数（如对于 $x \\ge 0$ 的 $x \\mapsto x^2$）的复合会保持凸性。因此，每一项 $\\bigl(\\max\\{0, g_j(w)\\}\\bigr)^2$ 都是凸的。类似地，仿射函数的平方 $\\bigl(h_k(w)\\bigr)^2$ 也是凸的。由于当 $\\lambda > 0$ 时正则化项 $\\lambda \\|w\\|_2^2$ 是强凸的，优值函数 $M_\\rho(w)$ 作为一个包含一个强凸项的凸函数的非负和，其本身也是强凸的。这是一个关键性质，因为它保证了对于任何给定的 $\\rho > 0$，$M_\\rho(w)$ 都存在一个唯一的全局最小值点。\n\n为了通过数值方法找到这个最小值点，我们可以采用一个基于梯度的优化算法。Broyden–Fletcher–Goldfarb–Shanno (BFGS) 算法是解决此类无约束 $C^1$ 最小化问题的合适选择。为使该算法高效且准确，我们必须提供优值函数的解析梯度 $\\nabla M_\\rho(w)$。使用链式法则，梯度为：\n$$\n\\nabla M_\\rho(w) = 2\\rho \\left( \\sum_{j} \\max\\{0, g_j(w)\\} \\nabla g_j(w) \\;+\\; \\sum_{k} h_k(w) \\nabla h_k(w) \\right) \\;+\\; 2\\lambda w\n$$\n各个约束函数的梯度很容易计算：\n- $\\nabla g_{\\text{lo},i}(w) = -e_i$ (其中 $e_i$ 是第 $i$ 个标准基向量)\n- $\\nabla g_{\\text{up},i}(w) = e_i$\n- $\\nabla g_{\\text{ret}}(w) = -\\mu$\n- $\\nabla g_{\\text{var}}(w) = 2 \\Sigma w$ (因为 $\\Sigma$ 是对称的)\n- $\\nabla h_{\\text{bud}}(w) = \\mathbf{1}$ (一个全1向量)\n\n将这些梯度代入 $\\nabla M_\\rho(w)$ 的表达式中，就得到了可以进行数值实现的完整梯度向量公式。\n\n按照问题规定，整个流程如下：\n1. 用等权重向量初始化投资组合，即 $w^{(0)}_i = 1/n$。\n2. 遍历规定的惩罚参数序列 $\\rho \\in \\{10^1, 10^2, \\dots, 10^6\\}$。\n3. 在每次迭代中，使用 BFGS 算法数值求解无约束最小化问题 $w^* = \\arg\\min_w M_\\rho(w)$，并从上一次迭代的解开始（一种热启动策略）。\n4. 在为当前 $\\rho$ 找到最优解 $w^*$ 后，计算问题陈述中定义的最大约束违反度 $\\mathrm{vio}(w^*)$。\n5. 如果 $\\mathrm{vio}(w^*) \\le \\tau = 10^{-6}$，则当前测试用例的处理过程终止，此违反度值即为结果。\n6. 如果未满足容差，则继续使用下一个更大的 $\\rho$ 值。如果遍历完整个循环仍未满足容差，则报告最后一步（$\\rho = 10^6$ 时）的违反度。\n\n这种系统化的方法确保了我们能在给定的 $\\rho$ 序列中找到一个满足所需精度的可行解（如果存在），否则就报告在最高惩罚下得到的“尽力而为”的解。由于所有测试用例的可行域都是非空的，我们预期该算法能够找到一个满足容差 $\\tau$ 的解。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final results.\n    \"\"\"\n    \n    # Global parameters as specified in the problem\n    LAMBDA = 1e-8\n    TAU = 1e-6\n    RHO_SEQUENCE = np.array([1e1, 1e2, 1e3, 1e4, 1e5, 1e6])\n\n    # Test cases data\n    test_cases = [\n        # Case A\n        {\n            \"n\": 3,\n            \"mu\": np.array([0.06, 0.10, 0.14]),\n            \"Sigma\": np.array([[0.010, 0.002, 0.001],\n                               [0.002, 0.020, 0.003],\n                               [0.001, 0.003, 0.030]]),\n            \"u\": np.array([0.8, 0.8, 0.8]),\n            \"R_target\": 0.09,\n            \"V_max\": 0.025,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        },\n        # Case B\n        {\n            \"n\": 3,\n            \"mu\": np.array([0.03, 0.05, 0.07]),\n            \"Sigma\": np.array([[0.008, 0.001, 0.0005],\n                               [0.001, 0.012, 0.001],\n                               [0.0005, 0.001, 0.015]]),\n            \"u\": np.array([0.8, 0.8, 0.8]),\n            \"R_target\": 0.07,\n            \"V_max\": 0.05,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        },\n        # Case C\n        {\n            \"n\": 4,\n            \"mu\": np.array([0.05, 0.08, 0.12, 0.04]),\n            \"Sigma\": np.array([[0.005, 0.001, 0.001, 0.0005],\n                               [0.001, 0.010, 0.002, 0.001],\n                               [0.001, 0.002, 0.020, 0.0015],\n                               [0.0005, 0.001, 0.0015, 0.004]]),\n            \"u\": np.array([0.6, 0.6, 0.5, 1.0]),\n            \"R_target\": 0.08,\n            \"V_max\": 0.012,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        violation = solve_one_case(case_params, RHO_SEQUENCE)\n        # Format to exactly 6 digits after the decimal point\n        results.append(f\"{violation:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef get_constraint_values(w, params):\n    \"\"\"Calculates the values of all constraint functions.\"\"\"\n    mu, Sigma, u, R_target, V_max = params['mu'], params['Sigma'], params['u'], params['R_target'], params['V_max']\n    \n    g_lo = -w\n    g_up = w - u\n    g_ret = R_target - mu.dot(w)\n    g_var = w.dot(Sigma.dot(w)) - V_max\n    h_bud = np.sum(w) - 1.0\n    \n    return g_lo, g_up, g_ret, g_var, h_bud\n\ndef merit_function(w, rho, params):\n    \"\"\"Calculates the value of the merit function M_rho(w).\"\"\"\n    lam = params['lambda']\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    p_lo = np.maximum(0, g_lo)\n    p_up = np.maximum(0, g_up)\n    p_ret = np.maximum(0, g_ret)\n    p_var = np.maximum(0, g_var)\n    \n    penalty_term = np.sum(p_lo**2) + np.sum(p_up**2) + p_ret**2 + p_var**2 + h_bud**2\n    regularization_term = lam * np.sum(w**2)\n    \n    return rho * penalty_term + regularization_term\n\ndef merit_gradient(w, rho, params):\n    \"\"\"Calculates the gradient of the merit function M_rho(w).\"\"\"\n    n, mu, Sigma, lam = params['n'], params['mu'], params['Sigma'], params['lambda']\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    p_lo = np.maximum(0, g_lo)\n    p_up = np.maximum(0, g_up)\n    p_ret = np.maximum(0, g_ret)\n    p_var = np.maximum(0, g_var)\n    \n    # Gradient of penalty term for lo/up bounds\n    grad_bounds = p_up - p_lo # component-wise\n    \n    # Gradient of penalty term for return\n    grad_ret = p_ret * (-mu)\n    \n    # Gradient of penalty term for variance\n    grad_var = p_var * (2 * Sigma.dot(w))\n    \n    # Gradient of penalty term for budget\n    grad_bud = h_bud * np.ones(n)\n    \n    # Combine all gradient components\n    grad = 2 * rho * (grad_bounds + grad_ret + grad_var + grad_bud)\n    \n    # Add gradient of regularization term\n    grad += 2 * lam * w\n    \n    return grad\n\ndef calculate_violation(w, params):\n    \"\"\"Calculates the maximum constraint violation vio(w).\"\"\"\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    vio_lo = np.max(np.maximum(0, g_lo))\n    vio_up = np.max(np.maximum(0, g_up))\n    vio_ret = np.maximum(0, g_ret)\n    vio_var = np.maximum(0, g_var)\n    vio_bud = np.abs(h_bud)\n    \n    return np.max([vio_lo, vio_up, vio_ret, vio_var, vio_bud])\n\ndef solve_one_case(params, rho_sequence):\n    \"\"\"Solves a single test case using the penalty method.\"\"\"\n    n = params['n']\n    tau = params['tau']\n    w0 = np.ones(n) / n\n    final_violation = -1.0\n    \n    for rho in rho_sequence:\n        res = minimize(\n            fun=merit_function,\n            x0=w0,\n            args=(rho, params),\n            method='BFGS',\n            jac=merit_gradient,\n            options={'gtol': 1e-9} \n        )\n        \n        w_opt = res.x\n        final_violation = calculate_violation(w_opt, params)\n        \n        if final_violation <= tau:\n            break\n        \n        w0 = w_opt # Warm start for the next iteration\n        \n    return final_violation\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}