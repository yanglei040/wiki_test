{
    "hands_on_practices": [
        {
            "introduction": "A problem's inherent sensitivity, or conditioning, is only half the story; the stability of the algorithm used to solve it is equally critical. This exercise provides a striking demonstration of algorithmic instability, where a well-conditioned linear system yields a highly inaccurate result when solved with a naive algorithm (Gaussian elimination without pivoting). By comparing this to the result from a stable algorithm (with pivoting), we can isolate the catastrophic effects of a poor algorithmic choice and understand the importance of pivot selection .",
            "id": "2370924",
            "problem": "Consider a linear system of equations that encodes a pairwise no-arbitrage balance between two synthetic portfolios in a frictional market with a small proportional distortion. Let the unknown vector be $x \\in \\mathbb{R}^{2}$, and for a given parameter $\\varepsilon \\in (0,1)$ define the coefficient matrix and right-hand side by\n$$\nA(\\varepsilon) \\equiv \\begin{bmatrix}\n\\varepsilon  1 \\\\\n1  1\n\\end{bmatrix}, \\qquad \nb \\equiv \\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}.\n$$\nThis system can be interpreted as a linearized set of two no-arbitrage consistency equations linking two replicating strategies with a small friction parameter $\\varepsilon$ appearing only in the first equation. The equations are non-dimensional and require no physical units.\n\nDefine the spectral condition number with respect to the Euclidean norm by\n$$\n\\kappa_{2}\\!\\left(A(\\varepsilon)\\right) \\equiv \\|A(\\varepsilon)\\|_{2}\\,\\|A(\\varepsilon)^{-1}\\|_{2}.\n$$\n\nDefine two computed approximations to the solution $x$ as follows:\n1. $x_{\\mathrm{np}}(\\varepsilon)$ is the result of solving $A(\\varepsilon)\\,x=b$ by a triangular factorization that keeps the row order fixed at every step (no row exchanges).\n2. $x_{\\mathrm{pp}}(\\varepsilon)$ is the result of solving $A(\\varepsilon)\\,x=b$ by a triangular factorization that, at each elimination step, reorders rows within the active submatrix so that the pivot in the current column has maximal absolute value (row exchanges allowed within the column).\n\nLet the exact solution be denoted $x^{\\star}(\\varepsilon)$, and define the relative error of any approximation $y \\in \\mathbb{R}^{2}$ by\n$$\n\\mathcal{E}(y;\\varepsilon) \\equiv \\frac{\\|y - x^{\\star}(\\varepsilon)\\|_{2}}{\\|x^{\\star}(\\varepsilon)\\|_{2}}.\n$$\n\nYour program must, for each specified test case value of $\\varepsilon$, compute the triple \n$$\n\\left(\\kappa_{2}\\!\\left(A(\\varepsilon)\\right), \\ \\mathcal{E}\\!\\left(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon\\right), \\ \\mathcal{E}\\!\\left(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon\\right)\\right),\n$$\nand then evaluate the boolean\n$$\nS(\\varepsilon) \\equiv \\Big(\\kappa_{2}\\!\\left(A(\\varepsilon)\\right) \\leq 10\\Big) \\ \\wedge \\ \\Big(\\mathcal{E}\\!\\left(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon\\right) \\geq 10^{-3}\\Big) \\ \\wedge \\ \\Big(\\mathcal{E}\\!\\left(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon\\right) \\leq 10^{-12}\\Big).\n$$\nThe intended interpretation is that $S(\\varepsilon)$ is true precisely when the system is well-conditioned while the fixed-row-order elimination produces a large forward error and the row-exchanging elimination remains highly accurate.\n\nTest Suite:\n- $\\varepsilon = 10^{-16}$\n- $\\varepsilon = 10^{-12}$\n- $\\varepsilon = 10^{-8}$\n- $\\varepsilon = 10^{-4}$\n- $\\varepsilon = 10^{-1}$\n\nAnswer specification:\n- For each test case in the order listed above, compute $S(\\varepsilon)$ as defined.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_{1},r_{2},r_{3},r_{4},r_{5}]$, where each $r_{i}$ is either the literal token True or False corresponding to $S(\\varepsilon)$ for the $i$-th test case.\n\nAll calculations must be carried out in standard double-precision floating-point arithmetic. Angles and physical units are not involved in this task, and no percentages are required; any ratios are to be reported as pure decimal numbers.",
            "solution": "The problem statement submitted for analysis is deemed valid. It is a well-posed, scientifically grounded problem in numerical linear algebra, formulated with objective and precise language. All necessary data, definitions, and constraints are provided, and there are no internal contradictions or logical flaws. The problem asks for an analysis of algorithmic stability, a fundamental concept in computational science. We shall proceed with the solution.\n\nThe objective is to investigate the numerical stability of solving the linear system $A(\\varepsilon)x = b$ for various small values of the parameter $\\varepsilon$. Specifically, we must compare the accuracy of two methods based on triangular factorization: one that maintains a fixed row order (Gaussian elimination without pivoting) and one that reorders rows to use the pivot of largest magnitude (Gaussian elimination with partial pivoting).\n\nFirst, let us determine the exact solution, $x^{\\star}(\\varepsilon)$, against which the numerical approximations will be compared. The matrix $A(\\varepsilon)$ is given by\n$$\nA(\\varepsilon) = \\begin{bmatrix} \\varepsilon  1 \\\\ 1  1 \\end{bmatrix}.\n$$\nThe determinant is $\\det(A(\\varepsilon)) = \\varepsilon \\cdot 1 - 1 \\cdot 1 = \\varepsilon - 1$. Since the problem specifies $\\varepsilon \\in (0,1)$, the determinant is non-zero, and the matrix is always invertible. The inverse is\n$$\nA(\\varepsilon)^{-1} = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} 1  -1 \\\\ -1  \\varepsilon \\end{bmatrix}.\n$$\nThe exact solution $x^{\\star}(\\varepsilon) = A(\\varepsilon)^{-1}b$ is therefore\n$$\nx^{\\star}(\\varepsilon) = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} 1  -1 \\\\ -1  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} 1 - 2 \\\\ -1 + 2\\varepsilon \\end{bmatrix} = \\frac{1}{\\varepsilon - 1} \\begin{bmatrix} -1 \\\\ -1 + 2\\varepsilon \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{1-\\varepsilon} \\\\ \\frac{1-2\\varepsilon}{1-\\varepsilon} \\end{bmatrix}.\n$$\nAs $\\varepsilon \\to 0$, the exact solution approaches $x^{\\star}(0) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. The solution itself is well-behaved for small $\\varepsilon$.\n\nNext, we analyze the conditioning of the problem. The spectral condition number $\\kappa_{2}(A(\\varepsilon))$ measures the sensitivity of the solution $x$ to perturbations in $A$ or $b$. For $\\varepsilon \\to 0$, the matrix $A(\\varepsilon)$ approaches $A(0) = \\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix}$. The singular values of $A(0)$ are the square roots of the eigenvalues of $A(0)^T A(0) = \\begin{bmatrix} 1  1 \\\\ 1  2 \\end{bmatrix}$. The eigenvalues of this matrix are $\\lambda = (3 \\pm \\sqrt{5})/2$. The singular values are $\\sigma_{\\max} = \\sqrt{(3 + \\sqrt{5})/2}$ and $\\sigma_{\\min} = \\sqrt{(3 - \\sqrt{5})/2}$. The condition number is\n$$\n\\kappa_{2}(A(0)) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{\\sqrt{3+\\sqrt{5}}}{\\sqrt{3-\\sqrt{5}}} = \\frac{3+\\sqrt{5}}{2} \\approx 2.618.\n$$\nThis is a small number, indicating that for small $\\varepsilon$, the matrix $A(\\varepsilon)$ is well-conditioned. The problem is not inherently sensitive to small perturbations. The first condition in $S(\\varepsilon)$, which is $\\kappa_{2}(A(\\varepsilon)) \\leq 10$, will therefore be satisfied for all test values of $\\varepsilon$. Any large errors in a computed solution must therefore arise from the instability of the algorithm used, not from the ill-conditioning of the problem itself.\n\nLet us now analyze the first numerical method, $x_{\\mathrm{np}}(\\varepsilon)$, which corresponds to Gaussian elimination with no pivoting. We perform an LU factorization of $A(\\varepsilon) = LU$.\n$$\nA(\\varepsilon) = \\begin{bmatrix} \\varepsilon  1 \\\\ 1  1 \\end{bmatrix}.\n$$\nTo eliminate the entry $a_{21}=1$, we use the multiplier $m_{21} = a_{21}/a_{11} = 1/\\varepsilon$. The resulting upper triangular matrix $U$ is\n$$\nU = \\begin{bmatrix} \\varepsilon  1 \\\\ 0  1 - 1/\\varepsilon \\end{bmatrix}.\n$$\nAnd the lower triangular matrix $L$ is\n$$\nL = \\begin{bmatrix} 1  0 \\\\ 1/\\varepsilon  1 \\end{bmatrix}.\n$$\nFor a small $\\varepsilon$ (e.g., $\\varepsilon \\approx 10^{-16}$), the multiplier $1/\\varepsilon$ becomes extremely large. In standard double-precision floating-point arithmetic, which has a machine epsilon of approximately $2.2 \\times 10^{-16}$, the computation of the term $\\mathrm{fl}(1 - 1/\\varepsilon)$ suffers from catastrophic cancellation. The number $1$ is insignificant compared to $1/\\varepsilon$, so the result is simply $\\mathrm{fl}(-1/\\varepsilon)$.\nThe system is solved via forward substitution $Ly=b$ and backward substitution $Ux=y$.\nSolving $Ly=b$:\n$y_1 = b_1 = 1$.\n$(1/\\varepsilon)y_1 + y_2 = b_2 \\implies y_2 = 2 - 1/\\varepsilon$.\nIn floating-point arithmetic, $\\mathrm{fl}(y_2) = \\mathrm{fl}(2 - 1/\\varepsilon) \\approx -1/\\varepsilon$.\nSolving $Ux=y$:\nThe $(2,2)$ element of the computed $U$ matrix is $\\tilde{u}_{22} = \\mathrm{fl}(1-1/\\varepsilon) \\approx -1/\\varepsilon$.\n$\\tilde{u}_{22} x_2 = \\tilde{y}_2 \\implies (-1/\\varepsilon) x_2 \\approx -1/\\varepsilon \\implies x_2 \\approx 1$.\n$\\varepsilon x_1 + x_2 = y_1 \\implies \\varepsilon x_1 + 1 \\approx 1 \\implies \\varepsilon x_1 \\approx 0 \\implies x_1 \\approx 0$.\nThe computed solution is $x_{\\mathrm{np}}(\\varepsilon) \\approx \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nComparing this to the exact solution for small $\\varepsilon$, $x^{\\star}(\\varepsilon) \\approx \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, the error is substantial. The relative error is\n$$\n\\mathcal{E}(x_{\\mathrm{np}};\\varepsilon) = \\frac{\\|x_{\\mathrm{np}} - x^{\\star}(\\varepsilon)\\|_{2}}{\\|x^{\\star}(\\varepsilon)\\|_{2}} \\approx \\frac{\\|\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\|_{2}}{\\|\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\|_{2}} = \\frac{\\|\\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}\\|_{2}}{\\sqrt{1^2+1^2}} = \\frac{1}{\\sqrt{2}} \\approx 0.707.\n$$\nThis is a very large error, far exceeding $10^{-3}$. This illustrates the numerical instability of Gaussian elimination without pivoting when a small pivot is encountered. This will hold for any $\\varepsilon$ small enough to cause this floating-point behavior. This instability disappears only when $\\varepsilon$ is large enough that $1/\\varepsilon$ is not large. For $\\varepsilon = 10^{-1}$, the multiplier is $10$, and the calculation $1-10=-9$ is exact in floating-point. In this case, no-pivot elimination is accurate. Hence, the condition $\\mathcal{E}(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon) \\geq 10^{-3}$ will be true for small $\\varepsilon$ but false for $\\varepsilon = 10^{-1}$.\n\nNow, we analyze the second method, $x_{\\mathrm{pp}}(\\varepsilon)$, corresponding to Gaussian elimination with partial pivoting. At each step, rows are exchanged to ensure the pivot element (the diagonal element) is the largest in its column within the active submatrix.\nFor $A(\\varepsilon) = \\begin{bmatrix} \\varepsilon  1 \\\\ 1  1 \\end{bmatrix}$ and $\\varepsilon \\in (0,1)$, we have $|a_{21}| = 1  |\\varepsilon| = |a_{11}|$. Thus, we must swap row $1$ and row $2$. This is equivalent to multiplying by a permutation matrix $P = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}$. The system becomes $PAx=Pb$.\n$$\nPA = \\begin{bmatrix} 1  1 \\\\ \\varepsilon  1 \\end{bmatrix}, \\quad Pb = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.\n$$\nNow we perform elimination on this new system. The multiplier is $m_{21} = \\varepsilon/1 = \\varepsilon$. Since $|\\varepsilon|1$, the multiplier is small, which is the hallmark of a stable elimination step.\nThe upper triangular matrix $U$ becomes\n$$\nU = \\begin{bmatrix} 1  1 \\\\ 0  1-\\varepsilon \\end{bmatrix}.\n$$\nThe operations are $y_1=2$, $y_2=1-2\\varepsilon$, then $(1-\\varepsilon)x_2 = 1-2\\varepsilon \\implies x_2 = (1-2\\varepsilon)/(1-\\varepsilon)$, and $x_1+x_2=2 \\implies x_1=2-x_2 = 1/(1-\\varepsilon)$. The computed solution is identical to the analytical form of the exact solution. As all operations involve well-conditioned arithmetic (no subtractions of nearly equal large numbers), the floating-point calculation will be highly accurate. The relative error $\\mathcal{E}(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon)$ will be on the order of machine epsilon (around $10^{-16}$), thus satisfying the condition $\\mathcal{E}(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon) \\leq 10^{-12}$ for all test cases.\n\nIn summary, for $\\varepsilon \\in \\{10^{-16}, 10^{-12}, 10^{-8}, 10^{-4}\\}$:\n1.  $\\kappa_{2}(A(\\varepsilon)) \\leq 10$ is `True`.\n2.  $\\mathcal{E}(x_{\\mathrm{np}}(\\varepsilon);\\varepsilon) \\geq 10^{-3}$ is `True` due to instability.\n3.  $\\mathcal{E}(x_{\\mathrm{pp}}(\\varepsilon);\\varepsilon) \\leq 10^{-12}$ is `True` due to stability.\nThus, $S(\\varepsilon)$ evaluates to `True`.\n\nFor $\\varepsilon=10^{-1}$:\n1.  $\\kappa_{2}(A(0.1)) \\leq 10$ is `True`.\n2.  $\\mathcal{E}(x_{\\mathrm{np}}(0.1);\\varepsilon) \\geq 10^{-3}$ is `False` because the multiplier $1/0.1=10$ is not large enough to cause significant floating-point error, and the calculation is accurate.\n3.  $\\mathcal{E}(x_{\\mathrm{pp}}(0.1);\\varepsilon) \\leq 10^{-12}$ is `True`.\nSince the second condition is false, $S(0.1)$ evaluates to `False`.\n\nThe program will implement these calculations and confirm this reasoning.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# The problem can be solved with numpy; scipy is not strictly necessary but is permitted.\n\ndef solve():\n    \"\"\"\n    Solves the specified problem by analyzing the numerical stability of solving\n    a linear system with and without pivoting.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1e-16,\n        1e-12,\n        1e-8,\n        1e-4,\n        1e-1,\n    ]\n\n    results = []\n    \n    for eps in test_cases:\n        # Define the matrix A and vector b for the current epsilon\n        A = np.array([[eps, 1.0], [1.0, 1.0]], dtype=np.float64)\n        b = np.array([1.0, 2.0], dtype=np.float64)\n\n        # --- 1. Calculate the 'exact' solution x_star ---\n        # Using the analytical formula derived from the problem.\n        # This gives a high-precision reference for error calculation.\n        x_star = np.array([1.0 / (1.0 - eps), (1.0 - 2.0 * eps) / (1.0 - eps)], dtype=np.float64)\n\n        # --- 2. Calculate the condition number kappa_2 ---\n        kappa_2 = np.linalg.cond(A, 2)\n\n        # --- 3. Calculate the solution with no pivoting (x_np) ---\n        # This requires manually implementing Gaussian elimination without row swaps.\n        def solve_no_pivot(mat_A, vec_b):\n            n = len(vec_b)\n            A_np = mat_A.copy()\n            b_np = vec_b.copy()\n\n            # Forward elimination to create an upper triangular matrix\n            # This is a 2x2 specific implementation for simplicity and clarity.\n            if A_np[0, 0] == 0:\n                # This case isn't hit for eps  0 but is a necessary check\n                # for a general algorithm. A zero pivot without pivoting fails.\n                return np.array([np.nan, np.nan])\n            \n            m = A_np[1, 0] / A_np[0, 0]\n            A_np[1, :] -= m * A_np[0, :]\n            b_np[1] -= m * b_np[0]\n            \n            # Backward substitution\n            x = np.zeros(n, dtype=np.float64)\n            if A_np[1, 1] == 0:\n                # Another fail condition, singular matrix after elimination.\n                return np.array([np.nan, np.nan])\n                \n            x[1] = b_np[1] / A_np[1, 1]\n            x[0] = (b_np[0] - A_np[0, 1] * x[1]) / A_np[0, 0]\n            \n            return x\n\n        x_np = solve_no_pivot(A, b)\n        \n        # --- 4. Calculate the solution with partial pivoting (x_pp) ---\n        # np.linalg.solve uses LAPACK routines which employ partial pivoting by default.\n        x_pp = np.linalg.solve(A, b)\n\n        # --- 5. Calculate relative errors ---\n        norm_x_star = np.linalg.norm(x_star, 2)\n        if norm_x_star == 0:\n             # Avoid division by zero, though not relevant for this problem.\n            rel_error_np = np.linalg.norm(x_np - x_star, 2)\n            rel_error_pp = np.linalg.norm(x_pp - x_star, 2)\n        else:\n            rel_error_np = np.linalg.norm(x_np - x_star, 2) / norm_x_star\n            rel_error_pp = np.linalg.norm(x_pp - x_star, 2) / norm_x_star\n            \n        # --- 6. Evaluate the boolean condition S(epsilon) ---\n        S = (kappa_2 = 10.0) and (rel_error_np = 1e-3) and (rel_error_pp = 1e-12)\n        results.append(S)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen how an unstable algorithm can ruin a good problem, we now explore the opposite scenario: what happens when a perfectly stable algorithm meets an ill-conditioned problem? This practice uses the famous Hilbert matrix, $H_n$, a classic example of a matrix whose condition number, $\\kappa_2(H_n)$, grows exponentially with its size. You will observe that even though our modern solver is backward stable (introducing minimal error itself), the forward error in the solution degrades dramatically, revealing the fundamental accuracy limits imposed by the problem's own sensitivity .",
            "id": "2421700",
            "problem": "You are asked to write a complete, runnable program that demonstrates numerical stability and instability when solving a linear system with a Hilbert matrix. A Hilbert matrix of size $n$, denoted by $H_n$, is the $n \\times n$ matrix with entries $H_{ij} = \\dfrac{1}{i + j - 1}$ for $1 \\le i, j \\le n$. This matrix is symmetric and positive definite, but it is famously ill-conditioned, meaning small perturbations (for example, rounding errors from floating-point arithmetic) can cause large changes in the computed solution of a linear system.\n\nFundamental base and definitions: Consider the linear system $A x = b$ with a nonsingular matrix $A$. For the solution $x$, the following notions are fundamental.\n- The forward relative error is $\\dfrac{\\lVert \\hat{x} - x \\rVert_2}{\\lVert x \\rVert_2}$.\n- The backward error (one common normalized choice) is $\\dfrac{\\lVert A \\hat{x} - b \\rVert_2}{\\lVert A \\rVert_2 \\lVert \\hat{x} \\rVert_2 + \\lVert b \\rVert_2}$.\n- The $2$-norm condition number is $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2$.\n\nWell-tested facts to be used as the starting point: \n- In floating-point arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard, stable direct solvers (for example, Gaussian elimination with partial pivoting as implemented in standard libraries) are typically backward stable: the computed solution $\\hat{x}$ is the exact solution to a nearby system $(A + \\Delta A)\\hat{x} = b$ with $\\dfrac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2}$ on the order of the unit roundoff $u$ (about $2^{-53} \\approx 1.11 \\times 10^{-16}$ for double precision).\n- For small perturbations, the forward error is bounded in proportion to the condition number: $\\dfrac{\\lVert \\hat{x} - x \\rVert_2}{\\lVert x \\rVert_2} \\lesssim \\kappa_2(A) \\cdot \\dfrac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2}$.\n\nTask: Implement a program that, for each $n$ in a prescribed test suite, constructs the Hilbert matrix $H_n$, chooses a known exact solution $x_\\text{true}$, constructs $b = H_n x_\\text{true}$, solves the system numerically in double precision to obtain $\\hat{x}$, and reports:\n- the forward relative error $\\dfrac{\\lVert \\hat{x} - x_\\text{true} \\rVert_2}{\\lVert x_\\text{true} \\rVert_2}$,\n- the normalized backward error $\\dfrac{\\lVert H_n \\hat{x} - b \\rVert_2}{\\lVert H_n \\rVert_2 \\lVert \\hat{x} \\rVert_2 + \\lVert b \\rVert_2}$,\n- the $2$-norm condition number $\\kappa_2(H_n)$.\n\nChoice of exact solution: Use $x_\\text{true}$ with entries $x_i = (-1)^{i-1}$ for $i = 1, \\dots, n$, so the entries of $x_\\text{true}$ are $\\{+1, -1, +1, -1, \\dots\\}$.\n\nNumerical environment: Use double-precision floating-point arithmetic. No random numbers are permitted. Angles are not involved. There are no physical units.\n\nTest suite: Run your program for $n \\in \\{2, 5, 8, 12, 14\\}$ in that order. These cases cover a baseline small size ($n = 2$), a typical well-behaved small-to-moderate case ($n = 5$), a clearly ill-conditioned moderate case ($n = 8$), and two increasingly extreme ill-conditioned cases ($n = 12$ and $n = 14$).\n\nRequired final output format: Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case reported as an inner list in the form $[n,\\ \\text{forward\\_error},\\ \\text{backward\\_error},\\ \\kappa_2]$. There must be no spaces anywhere in the output. For example, the overall structure must be\n$[[n_1,\\text{forward\\_error}_1,\\text{backward\\_error}_1,\\kappa_{2,1}],[n_2,\\text{forward\\_error}_2,\\text{backward\\_error}_2,\\kappa_{2,2}],\\dots]$.\nAll numeric values should be printed as decimal floating-point numbers (scientific notation is acceptable). The program must not read any input.",
            "solution": "The problem as stated is a standard, yet fundamental, exercise in numerical linear algebra, a cornerstone of computational science. Its purpose is to demonstrate empirically the critical distinction between the stability of an algorithm and the conditioning of a problem. Before proceeding, a rigorous validation is required.\n\nThe problem requires the analysis of the linear system $H_n x = b$, where $H_n$ is the Hilbert matrix of size $n$. All givens are clearly stated: the definition of $H_n$ as $H_{ij} = (i+j-1)^{-1}$, the choice of the exact solution vector $x_\\text{true}$, the procedure for constructing the vector $b$, the numerical methods to be used (double-precision arithmetic, standard linear solvers), and the precise quantities to be reported: the forward relative error, a normalized backward error, and the $2$-norm condition number. The test suite of matrix sizes $n \\in \\{2, 5, 8, 12, 14\\}$ is specified.\n\nThe problem statement is scientifically grounded, drawing upon established principles of matrix analysis and numerical stability theory. It is well-posed, providing a complete and consistent set of instructions that lead to a unique, verifiable computational result. The language is objective and unambiguous. Therefore, the problem is deemed valid and a solution will be presented.\n\nThe central concept is the conditioning of a mathematical problem. A problem is ill-conditioned if small relative changes in the input data can lead to large relative changes in the output solution. For a linear system $Ax=b$, the sensitivity of the solution $x$ to perturbations in $A$ and $b$ is measured by the condition number of the matrix $A$, defined for the $2$-norm as:\n$$\n\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2\n$$\nA large condition number signifies an ill-conditioned problem. Hilbert matrices are notoriously ill-conditioned, with $\\kappa_2(H_n)$ growing exponentially with $n$.\n\nAn algorithm for solving a problem is numerically stable if it does not introduce significant additional error beyond what is inherent in the problem's conditioning. A backward stable algorithm, such as Gaussian elimination with partial pivoting (the basis for standard library solvers like `numpy.linalg.solve`), computes a solution $\\hat{x}$ that is the exact solution to a slightly perturbed problem:\n$$\n(A + \\Delta A)\\hat{x} = b, \\quad \\text{where} \\quad \\frac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2} \\approx u\n$$\nHere, $u$ is the unit roundoff, or machine precision (for IEEE $754$ double precision, $u = 2^{-53} \\approx 1.11 \\times 10^{-16}$). A small backward error is a manifestation of this property. The problem defines a specific normalized backward error:\n$$\nE_b = \\frac{\\lVert A \\hat{x} - b \\rVert_2}{\\lVert A \\rVert_2 \\lVert \\hat{x} \\rVert_2 + \\lVert b \\rVert_2}\n$$\nThis quantity measures the size of the residual $A\\hat{x}-b$ relative to the scale of the problem. For a backward stable algorithm, we expect $E_b$ to be small, on the order of $u$.\n\nIn contrast, the forward relative error measures the discrepancy in the solution itself:\n$$\nE_f = \\frac{\\lVert \\hat{x} - x_\\text{true} \\rVert_2}{\\lVert x_\\text{true} \\rVert_2}\n$$\nThe fundamental relationship connecting these quantities is approximately given by:\n$$\nE_f \\lesssim \\kappa_2(A) \\cdot (\\text{backward error})\n$$\nThis inequality demonstrates that even with a backward stable algorithm (small backward error), if the problem is ill-conditioned (large $\\kappa_2(A)$), the resulting forward error in the computed solution can be unacceptably large. The Hilbert matrix provides a classic case study of this phenomenon.\n\nThe computational procedure to be implemented is as follows, for each $n$ in the test suite $\\{2, 5, 8, 12, 14\\}$:\n$1$. Construct the $n \\times n$ Hilbert matrix $H_n$ with entries $H_{ij} = (i+j-1)^{-1}$ for $i,j \\in \\{1, \\dots, n\\}$.\n$2$. Construct the true solution vector $x_\\text{true}$ of size $n$ with entries $(x_\\text{true})_i = (-1)^{i-1}$ for $i \\in \\{1, \\dots, n\\}$.\n$3$. Compute the right-hand side vector $b = H_n x_\\text{true}$. This calculation is performed in double-precision arithmetic, and the result is the reference right-hand side for the numerical test.\n$4$. Solve the linear system $H_n x = b$ numerically to obtain the computed solution $\\hat{x}$. This will be done using a standard, stable direct solver.\n$5$. Compute the three required diagnostic quantities:\n    a. The $2$-norm condition number, $\\kappa_2(H_n)$.\n    b. The forward relative error, $E_f = \\frac{\\lVert \\hat{x} - x_\\text{true} \\rVert_2}{\\lVert x_\\text{true} \\rVert_2}$.\n    c. The normalized backward error, $E_b = \\frac{\\lVert H_n \\hat{x} - b \\rVert_2}{\\lVert H_n \\rVert_2 \\lVert \\hat{x} \\rVert_2 + \\lVert b \\rVert_2}$.\n$6$. Collect and report the results $[n, E_f, E_b, \\kappa_2(H_n)]$.\n\nExpected behavior: As $n$ increases, $\\kappa_2(H_n)$ will grow extremely fast. The normalized backward error $E_b$ will remain small, close to machine precision, confirming the stability of the solver. Consequently, the forward error $E_f$ will grow dramatically, roughly tracking the product $\\kappa_2(H_n) \\cdot u$. For $n=12$ and $n=14$, the condition number will be so large that $\\kappa_2(H_n) \\cdot u \\gg 1$, indicating that the matrix is numerically singular and the computed solution $\\hat{x}$ is expected to have no correct significant digits. This program will demonstrate this behavior precisely.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Demonstrates numerical stability and instability when solving a linear system\n    with a Hilbert matrix for various sizes.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [2, 5, 8, 12, 14]\n\n    results = []\n    for n in test_cases:\n        # Construct the Hilbert matrix H_n.\n        # scipy.linalg.hilbert uses 1-based indexing in its definition,\n        # which matches the problem statement.\n        H = linalg.hilbert(n)\n\n        # Construct the known exact solution x_true with entries (-1)^(i-1).\n        # For 0-based array indexing, this is (-1)^i.\n        x_true = np.array([(-1)**i for i in range(n)])\n\n        # Construct the right-hand side vector b = H_n * x_true.\n        # This is performed in standard double-precision floating-point arithmetic.\n        b = H @ x_true\n\n        # Solve the system H_n * x = b numerically to obtain the computed solution x_hat.\n        # numpy.linalg.solve is a backward stable direct solver.\n        x_hat = np.linalg.solve(H, b)\n\n        # Calculate the 2-norm condition number of H_n.\n        # This is kappa_2(H_n) = ||H_n||_2 * ||H_n^-1||_2.\n        cond_num = np.linalg.cond(H, p=2)\n\n        # Calculate the forward relative error: ||x_hat - x_true||_2 / ||x_true||_2.\n        norm_x_true = np.linalg.norm(x_true, ord=2)\n        norm_error_x = np.linalg.norm(x_hat - x_true, ord=2)\n        forward_error = norm_error_x / norm_x_true\n\n        # Calculate the normalized backward error:\n        # ||H_n * x_hat - b||_2 / (||H_n||_2 * ||x_hat||_2 + ||b||_2).\n        residual_vec = H @ x_hat - b\n        norm_residual = np.linalg.norm(residual_vec, ord=2)\n        norm_H = np.linalg.norm(H, ord=2)\n        norm_x_hat = np.linalg.norm(x_hat, ord=2)\n        norm_b = np.linalg.norm(b, ord=2)\n        backward_error = norm_residual / (norm_H * norm_x_hat + norm_b)\n        \n        # Store the results for the current n.\n        results.append([n, forward_error, backward_error, cond_num])\n\n    # Format the final output string exactly as required.\n    # Each sublist is formatted as \"[n,ferr,berr,cond]\" with no spaces.\n    str_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    \n    # All sublist strings are joined by commas and enclosed in brackets.\n    final_output_str = f\"[{','.join(str_results)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n\n```"
        },
        {
            "introduction": "The principles of numerical stability extend far beyond linear algebra, impacting the dynamic models frequently used in finance. This exercise investigates stability in the context of the binomial option pricing model, a widely used method for valuing derivatives. You will discover that for certain parameters, especially for a small number of time steps $N$, the computed option price does not converge smoothly but exhibits oscillatory behavior, a form of instability that highlights the care required when discretizing continuous-time models .",
            "id": "2370925",
            "problem": "Consider an arbitrage-free market in which the underlying asset price process is modeled under the risk-neutral probability measure as a recombining binomial tree that approximates a continuous-time geometric Brownian motion over maturity horizon $T$ with continuously compounded risk-free rate $r$, continuous dividend yield $q$, and volatility $\\sigma$. For a given number of time steps $N \\in \\mathbb{N}$, let $\\Delta t = T/N$, and define the Cox–Ross–Rubinstein (CRR) up and down multipliers by $u = \\exp(\\sigma \\sqrt{\\Delta t})$ and $d = 1/u$, and the risk-neutral probability by $p = \\dfrac{\\exp\\left((r - q)\\Delta t\\right) - d}{u - d}$, with $0  p  1$. For a European call option with spot $S_0$, strike $K$, and maturity $T$, define the $N$-step binomial price $V_N$ by the risk-neutral valuation\n$$\nV_N = \\exp(-r T) \\sum_{k=0}^{N} \\binom{N}{k} p^k (1-p)^{N-k} \\max\\!\\left(S_0\\, u^{k} d^{N-k} - K, 0\\right).\n$$\nYou will examine the stability of $V_N$ as a function of the time-step count $N$ for several parameter sets, focusing on deep out-of-the-money conditions. For a fixed parameter set, define the sequence $\\{V_{N_i}\\}$ for $N_i \\in \\{1,2,3,4,5,6,7,8,9,10,11,12\\}$. Using this sequence, compute the following two scalar diagnostics that quantify oscillation and instability when too few steps are used:\n- The oscillation count $O$, defined as the number of strict sign changes in the first differences of the sequence, that is, let $\\Delta_i = V_{N_{i+1}} - V_{N_{i}}$ for $i = 1,\\dots,11$, ignore any $\\Delta_i$ with $|\\Delta_i| \\le \\varepsilon$ for $\\varepsilon = 10^{-12}$, and count the number of indices $i$ such that $\\Delta_i \\Delta_{i-1}  0$ in the remaining subsequence.\n- The instability ratio $R$, defined by\n$$\nR = \\frac{\\sum_{i=1}^{11} |\\Delta_i|}{\\left|V_{N_{12}} - V_{N_{1}}\\right| + \\varepsilon},\n$$\nwith the same $\\varepsilon = 10^{-12}$. Values of $R$ greater than $1$ indicate oscillatory behavior that increases the total variation beyond the net change from $N=1$ to $N=12$.\n\nUse the following test suite of parameter sets, each specifying $(S_0, K, r, q, \\sigma, T)$:\n- Test case $1$ (deep out-of-the-money): $(S_0, K, r, q, \\sigma, T) = (100, 150, 0.05, 0, 0.20, 1)$.\n- Test case $2$ (at-the-money control): $(S_0, K, r, q, \\sigma, T) = (100, 100, 0.05, 0, 0.20, 1)$.\n- Test case $3$ (deeper out-of-the-money): $(S_0, K, r, q, \\sigma, T) = (100, 200, 0.05, 0, 0.20, 1)$.\n- Test case $4$ (short-maturity, deep out-of-the-money): $(S_0, K, r, q, \\sigma, T) = (100, 150, 0.05, 0, 0.20, 0.25)$.\n\nYour program must, for each test case, compute the sequence $\\{V_{N}\\}_{N=1}^{12}$, then compute the pair $(O, R)$ as defined above. The required final output format is a single line containing a comma-separated list enclosed in square brackets, aggregating the results for the four test cases in order as\n$[O_1, R_1, O_2, R_2, O_3, R_3, O_4, R_4]$,\nwhere each $O_j$ is an integer and each $R_j$ is a floating-point number rounded to six decimal places. No other text should be printed.",
            "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the established principles of financial engineering, specifically the Cox-Ross-Rubinstein binomial options pricing model. The problem is well-posed, with all parameters, definitions, and objective functions stated clearly and unambiguously. There are no internal contradictions, factual errors, or sources of ambiguity. We may therefore proceed with the derivation of the solution.\n\nThe objective is to analyze the numerical stability of the binomial option pricing formula for a small number of time steps, $N$. The price of a European call option, $V_N$, is given by the risk-neutral valuation formula:\n$$\nV_N = \\exp(-r T) \\sum_{k=0}^{N} \\binom{N}{k} p^k (1-p)^{N-k} \\max\\!\\left(S_0\\, u^{k} d^{N-k} - K, 0\\right)\n$$\nwhere the parameters are defined as follows:\n- The number of time steps is $N$, and the time increment is $\\Delta t = T/N$.\n- The up and down factors for the underlying asset price are $u = \\exp(\\sigma \\sqrt{\\Delta t})$ and $d = 1/u$.\n- The risk-neutral probability of an up-move is $p = \\dfrac{\\exp\\left((r - q)\\Delta t\\right) - d}{u - d}$. The condition $0  p  1$ is an essential no-arbitrage requirement, which is satisfied by the provided test data for the specified range of $N$.\n\nThe computational procedure for each of the $4$ test cases is as follows:\n\nFirst, we must generate the sequence of option prices $\\{V_{N}\\}$ for $N \\in \\{1, 2, \\dots, 12\\}$. For each integer $N$ from $1$ to $12$, we perform these steps:\n1.  Compute the time step $\\Delta t = T/N$.\n2.  Compute the CRR parameters $u$, $d$, and $p$ according to their definitions.\n3.  Evaluate the sum in the formula for $V_N$. This involves iterating from $k=0$ to $N$. In each iteration $k$:\n    a. We compute the probability of reaching the $k$-th terminal state, which is given by the binomial probability mass function $B(k; N, p) = \\binom{N}{k} p^k (1-p)^{N-k}$. The binomial coefficient $\\binom{N}{k}$ is computed using standard library functions for numerical stability.\n    b. We compute the terminal asset price for the $k$-th state: $S_T(k) = S_0 u^k d^{N-k}$.\n    c. We determine the option payoff at this state: $\\max(S_T(k) - K, 0)$.\n    d. The contribution to the total expected payoff is the product of the probability and the payoff.\n4.  The final option price $V_N$ is obtained by discounting the sum of these probability-weighted payoffs from maturity $T$ to the present time, using the factor $\\exp(-rT)$.\nThis process yields the desired sequence $V_1, V_2, \\dots, V_{12}$.\n\nSecond, with the sequence $\\{V_N\\}$ computed, we proceed to calculate the two diagnostic metrics, the oscillation count $O$ and the instability ratio $R$.\n1.  Compute the sequence of first differences, $\\Delta_i = V_{i+1} - V_{i}$ for $i \\in \\{1, 2, \\dots, 11\\}$.\n2.  To compute the oscillation count $O$, we interpret the directive \"count... in the remaining subsequence\" to mean that the sequence of differences $\\Delta$ is first filtered to exclude elements for which $|\\Delta_i| \\le \\varepsilon$ (where the tolerance $\\varepsilon$ is given as $10^{-12}$), and then sign changes between adjacent elements of this new, shorter sequence are counted. That is, $O$ is the number of indices $j$ such that $\\Delta'_j \\Delta'_{j+1}  0$, where $\\Delta'$ is the filtered sequence. This procedure robustly identifies genuine price oscillations by ignoring numerically insignificant fluctuations.\n3.  To compute the instability ratio $R$, we use the formula:\n$$\nR = \\frac{\\sum_{i=1}^{11} |\\Delta_i|}{\\left|V_{12} - V_{1}\\right| + \\varepsilon}\n$$\nThe numerator represents the total variation (the sum of the magnitudes of all price changes), while the denominator is the magnitude of the net change from the first to the last step, stabilized by $\\varepsilon$. A ratio $R  1$ indicates that the path from $V_1$ to $V_{12}$ is not monotonic; the total variation exceeds the net variation, which is characteristic of oscillatory convergence.\n\nThis entire procedure is implemented for each of the $4$ specified parameter sets. The final output is an aggregation of the computed $(O, R)$ pairs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\n\ndef calculate_vn(s0, k_strike, r, q, sigma, t, n_steps):\n    \"\"\"\n    Calculates the European call option price using the N-step CRR binomial model.\n    The implementation is vectorized for efficiency.\n    \"\"\"\n    if n_steps == 0:\n        return max(s0 - k_strike, 0)\n    \n    dt = t / n_steps\n    u = np.exp(sigma * np.sqrt(dt))\n    d = 1.0 / u\n    p = (np.exp((r - q) * dt) - d) / (u - d)\n\n    # Vector of possible numbers of up-moves\n    k_moves = np.arange(0, n_steps + 1)\n    \n    # Vector of terminal stock prices\n    st_values = s0 * (u**k_moves) * (d**(n_steps - k_moves))\n    \n    # Vector of payoffs at terminal nodes\n    payoff_values = np.maximum(st_values - k_strike, 0)\n    \n    # Vector of probabilities for each terminal node\n    probs = comb(n_steps, k_moves, exact=False) * (p**k_moves) * ((1 - p)**(n_steps - k_moves))\n    \n    # Expected payoff is the sum of probability-weighted payoffs\n    expected_payoff = np.sum(probs * payoff_values)\n    \n    # Discount the expected payoff to present value\n    vn = np.exp(-r * t) * expected_payoff\n    return vn\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (S0, K, r, q, sigma, T)\n        (100, 150, 0.05, 0, 0.20, 1),    # Test case 1\n        (100, 100, 0.05, 0, 0.20, 1),    # Test case 2\n        (100, 200, 0.05, 0, 0.20, 1),    # Test case 3\n        (100, 150, 0.05, 0, 0.20, 0.25), # Test case 4\n    ]\n\n    all_results = []\n    epsilon = 1e-12\n    n_values = range(1, 13)\n\n    for case in test_cases:\n        s0, k_strike, r, q, sigma, t = case\n        \n        # 1. Compute the sequence of option prices {V_N} for N=1 to 12.\n        v_sequence = [calculate_vn(s0, k_strike, r, q, sigma, t, n) for n in n_values]\n        \n        # 2. Compute the first differences.\n        deltas = np.diff(v_sequence)\n        \n        # 3. Calculate the Instability Ratio (R).\n        sum_abs_deltas = np.sum(np.abs(deltas))\n        net_change = np.abs(v_sequence[-1] - v_sequence[0])\n        instability_ratio = sum_abs_deltas / (net_change + epsilon)\n        \n        # 4. Calculate the Oscillation Count (O).\n        # Filter out differences that are close to zero.\n        filtered_deltas = [d for d in deltas if np.abs(d)  epsilon]\n        \n        oscillation_count = 0\n        # A sign change can only occur if there are at least two significant differences.\n        if len(filtered_deltas)  1:\n            for i in range(len(filtered_deltas) - 1):\n                # Check for a strict sign change between adjacent elements in the filtered sequence.\n                if filtered_deltas[i] * filtered_deltas[i+1]  0:\n                    oscillation_count += 1\n        \n        all_results.append(oscillation_count)\n        all_results.append(f\"{instability_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}