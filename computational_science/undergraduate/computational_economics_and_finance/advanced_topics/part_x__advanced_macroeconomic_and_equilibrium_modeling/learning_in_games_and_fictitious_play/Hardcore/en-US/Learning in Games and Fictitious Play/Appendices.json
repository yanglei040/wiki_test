{
    "hands_on_practices": [
        {
            "introduction": "To truly understand a learning model, there is no substitute for building it from the ground up. This first practice challenges you to implement the classical fictitious play algorithm. Rather than a simple $2 \\times 2$ game, we will use the five-strategy \"Rock-Paper-Scissors-Lizard-Spock\" game to demonstrate how the model scales. This exercise will solidify your understanding of how beliefs are formed from empirical histories and how players select best responses . You will also employ Principal Component Analysis (PCA), a powerful data science technique, to visualize the high-dimensional path of the players' beliefs, offering a glimpse into the geometric dynamics of learning.",
            "id": "2405836",
            "problem": "You are asked to design and implement a complete, runnable program that simulates deterministic fictitious play in a two-player zero-sum Rock-Paper-Scissors-Lizard-Spock game, records the five-dimensional belief trajectory of one player, and produces visualization-ready two-dimensional principal component coordinates at specified time snapshots. The final output must be a single line in a precisely specified format.\n\nFundamental base and definitions:\n- Consider a two-player normal-form zero-sum game with action set $\\mathcal{A}=\\{0,1,2,3,4\\}$ corresponding respectively to Rock, Paper, Scissors, Lizard, and Spock.\n- Let the row player’s payoff matrix be $A\\in\\mathbb{R}^{5\\times 5}$ with entries $A_{ij}\\in\\{-1,0,1\\}$ satisfying anti-symmetry $A=-A^{\\top}$ and $A_{ii}=0$ for all $i$. The dominance relations are given by the set of ordered pairs\n$$\n\\mathcal{W}=\\{(0,2),(0,3),(1,0),(1,4),(2,1),(2,3),(3,4),(3,1),(4,2),(4,0)\\},\n$$\nmeaning that if $(i,j)\\in\\mathcal{W}$ then action $i$ defeats action $j$, so $A_{ij}=1$ and $A_{ji}=-1$; ties yield $A_{ii}=0$.\n- Deterministic fictitious play proceeds in discrete periods $t=1,2,\\dots,T$. Each player forms a belief equal to the empirical frequency of the opponent’s past actions and then chooses a pure best response to that belief. Let the row player’s belief about the column player’s period-$t$ mixed strategy be the empirical frequency vector $q_t\\in\\Delta^4$ computed from the column player’s actions in periods $\\{1,\\dots,t-1\\}$. Similarly, let the column player’s belief about the row player’s period-$t$ mixed strategy be $p_t\\in\\Delta^4$ computed from the row player’s actions in periods $\\{1,\\dots,t-1\\}$. The row player’s expected payoff vector against belief $q_t$ is $v_t=A\\,q_t$, and a pure best response is any $i\\in\\arg\\max_{k\\in\\mathcal{A}} (v_t)_k$. The column player seeks to minimize the row player’s payoff; its expected row-payoff vector given $p_t$ is $w_t=p_t^{\\top}A\\in\\mathbb{R}^5$, and a pure best response is any $j\\in\\arg\\min_{\\ell\\in\\mathcal{A}} (w_t)_{\\ell}$. You must implement deterministic tie-breaking by always selecting the smallest index within the best-response argmax or argmin set.\n- Let the row player’s belief trajectory be the sequence $(b_t)_{t=1}^T$, where $b_t\\in\\Delta^4$ is the empirical frequency of the column player’s actions over periods $\\{1,\\dots,t\\}$. This is a five-dimensional time series that lies in the $5$-simplex.\n\nComputational tasks to implement:\n1. Build the payoff matrix $A$ using the dominance set $\\mathcal{W}$.\n2. Simulate deterministic fictitious play for $T$ periods starting from specified initial pure actions for the row and column players in period $t=1$. For each period $t\\ge 2$, both players simultaneously choose their actions as pure best responses to beliefs formed from the opponent’s actions in periods $\\{1,\\dots,t-1\\}$ with the specified deterministic tie-breaking. Record the full sequence of the row player’s beliefs $(b_t)_{t=1}^T$.\n3. Define the uniform mixed strategy $u=\\left(\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5}\\right)$. Compute the final Euclidean distance $\\|b_T-u\\|_2$.\n4. For “visualization,” compute a two-dimensional principal component projection of the belief trajectory. Let $X\\in\\mathbb{R}^{T\\times 5}$ be the matrix whose $t$-th row is $b_t^{\\top}$. Center $X$ by subtracting the column-wise mean to obtain $X_c$. Using a singular value decomposition (SVD) $X_c=U\\Sigma V^{\\top}$ with singular values on the diagonal of $\\Sigma$, define the fraction of variance explained by the first two principal components as\n$$\n\\rho_2=\\frac{\\sigma_1^2+\\sigma_2^2}{\\sum_{k}\\sigma_k^2},\n$$\nwith the convention that $\\rho_2=0$ if $\\sum_k \\sigma_k^2=0$. Define the two-dimensional coordinates for time $t$ as the $t$-th row of $U_{[:,1:2]}\\,\\mathrm{diag}(\\sigma_1,\\sigma_2)$, i.e., the scores on the first two principal components.\n5. Return the two-dimensional coordinates at specified sample times as “visualization-ready” data.\n\nTest suite:\nImplement the above for the following three parameter sets. For each case, use the provided number of periods, initial actions, and sample times for projection output. Action indices follow Rock $=0$, Paper $=1$, Scissors $=2$, Lizard $=3$, Spock $=4$.\n- Case $1$ (general “happy path”): $T=400$, initial row action $=0$, initial column action $=1$, sample times $=\\{1,2,50,200,400\\}$.\n- Case $2$ (small-sample boundary): $T=5$, initial row action $=0$, initial column action $=0$, sample times $=\\{1,2,5\\}$.\n- Case $3$ (longer horizon, different start): $T=1000$, initial row action $=4$, initial column action $=2$, sample times $=\\{1,10,100,500,1000\\}$.\n\nRequired outputs per test case:\n- Let $d=\\|b_T-u\\|_2$ be the final Euclidean distance from the uniform strategy.\n- Let $\\rho_2$ be the fraction of variance explained by the first two principal components as defined above.\n- Let $C$ be the list of two-dimensional coordinates at the specified sample times, in the same order as the sample times, where each coordinate is a pair $[c_{t,1},c_{t,2}]$.\n\nFinal output format:\n- Your program must produce a single line containing a comma-separated list of the three test case results enclosed in square brackets. Each test case result must be a list of the form $[d,\\rho_2,C]$, where $C$ is a list of $2$-element lists. For example, a syntactically valid overall output has the form\n$[[d_1,\\rho_{2,1},C_1],[d_2,\\rho_{2,2},C_2],[d_3,\\rho_{2,3},C_3]]$.\n- No input is provided to the program; the test suite is hard-coded.\n- Angles and physical units are not applicable; all outputs are real numbers without units.",
            "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in the fields of game theory and statistics, well-posed with a unique deterministic solution, and objective in its definitions and requirements. All necessary data and conditions are provided, and there are no internal contradictions. We may therefore proceed with the solution.\n\nThe problem requires the implementation of a simulation of deterministic fictitious play for a two-player, zero-sum Rock-Paper-Scissors-Lizard-Spock game. The process involves constructing the game's payoff matrix, simulating the learning dynamics over a specified number of periods, and analyzing the resulting belief trajectory of one player using Principal Component Analysis (PCA).\n\nFirst, we construct the row player's payoff matrix $A \\in \\mathbb{R}^{5 \\times 5}$. The action set is $\\mathcal{A} = \\{0, 1, 2, 3, 4\\}$. The problem provides the set of winning-losing pairs, $\\mathcal{W}$. For each pair $(i, j) \\in \\mathcal{W}$, action $i$ defeats action $j$, yielding a payoff of $A_{ij} = 1$. The game is zero-sum, which implies anti-symmetry, $A = -A^\\top$, so $A_{ji} = -A_{ij} = -1$. Ties result in zero payoff, $A_{ii} = 0$. These rules uniquely define the matrix $A$.\n\nSecond, we simulate the fictitious play dynamics for $T$ periods. The simulation starts with given initial actions $i_1$ and $j_1$ at period $t=1$. For each subsequent period $t \\in \\{2, \\dots, T\\}$, the players form beliefs and choose a best response. The row player's belief at time $t$, denoted $q_t \\in \\Delta^4$, is the empirical frequency of the column player's actions in all preceding periods $\\{1, \\dots, t-1\\}$. The expected payoff for the row player for choosing action $k \\in \\mathcal{A}$ is the $k$-th component of the vector $v_t = A q_t$. The row player then selects an action $i_t$ that maximizes this payoff:\n$$\ni_t \\in \\arg\\max_{k \\in \\mathcal{A}} (A q_t)_k\n$$\nSimilarly, the column player forms a belief $p_t \\in \\Delta^4$ based on the row player's past actions. The column player aims to minimize the row player's payoff. The expected payoff for each of the column player's choices $\\ell \\in \\mathcal{A}$ is given by the vector $w_t = p_t^\\top A$. The column player selects an action $j_t$ that minimizes this value:\n$$\nj_t \\in \\arg\\min_{\\ell \\in \\mathcal{A}} (p_t^\\top A)_\\ell\n$$\nA strict tie-breaking rule is enforced: in case of multiple best responses, the action with the smallest index is chosen. This makes the simulation fully deterministic.\n\nThird, we analyze the row player's belief trajectory $(b_t)_{t=1}^T$. Note that the belief $b_t$ for analysis is defined as the empirical frequency of the column player's actions over periods $\\{1, \\dots, t\\}$, which includes the action at time $t$. This is distinct from the belief $q_t$ used for decision-making at time $t$. The final belief deviation is measured by the Euclidean distance $d = \\|b_T - u\\|_2$, where $u = (\\frac{1}{5}, \\dots, \\frac{1}{5})^\\top$ is the uniform mixed strategy.\n\nFourth, we perform PCA on the belief trajectory to obtain a two-dimensional representation. The trajectory is organized into a data matrix $X \\in \\mathbb{R}^{T \\times 5}$, where the $t$-th row is $b_t^\\top$. The data is centered by subtracting the column-wise mean vector from each row to obtain $X_c$. The SVD of the centered matrix, $X_c = U \\Sigma V^\\top$, provides the principal components. The singular values $\\sigma_k$ on the diagonal of $\\Sigma$ are used to compute the fraction of variance explained by the first two principal components:\n$$\n\\rho_2 = \\frac{\\sigma_1^2 + \\sigma_2^2}{\\sum_{k=1}^{\\text{rank}(X_c)} \\sigma_k^2}\n$$\nIf the total variance $\\sum_k \\sigma_k^2 = 0$, we set $\\rho_2=0$. The two-dimensional coordinates at time $t$, also known as scores, are given by the $t$-th row of the matrix product $U_{[:,1:2]} \\mathrm{diag}(\\sigma_1, \\sigma_2)$. This represents the projection of the centered data onto the first two principal component axes.\n\nFinally, for each test case, we compute the distance $d$, the variance fraction $\\rho_2$, and the list $C$ of two-dimensional coordinates at specified sample times. These results are then formatted into the required output structure.",
            "answer": "```python\nimport numpy as np\n\ndef run_case(T, initial_row_action, initial_col_action, sample_times):\n    \"\"\"\n    Runs a single test case for the fictitious play simulation.\n    \"\"\"\n    # Task 1: Build the payoff matrix A\n    A = np.zeros((5, 5), dtype=np.float64)\n    dominance_pairs = [\n        (0, 2), (0, 3), (1, 0), (1, 4), (2, 1),\n        (2, 3), (3, 4), (3, 1), (4, 2), (4, 0)\n    ]\n    for i, j in dominance_pairs:\n        A[i, j] = 1.0\n        A[j, i] = -1.0\n\n    # Task 2: Simulate deterministic fictitious play\n    row_actions = [initial_row_action]\n    col_actions = [initial_col_action]\n\n    row_counts = np.zeros(5, dtype=np.float64)\n    row_counts[initial_row_action] = 1.0\n    col_counts = np.zeros(5, dtype=np.float64)\n    col_counts[initial_col_action] = 1.0\n\n    for t in range(2, T + 1):\n        # Beliefs for period t are based on actions from 1 to t-1\n        q_t = col_counts / (t - 1)\n        p_t = row_counts / (t - 1)\n\n        # Row player's best response (maximizes payoff)\n        row_expected_payoffs = A @ q_t\n        i_t = np.argmax(row_expected_payoffs)\n\n        # Column player's best response (minimizes row's payoff)\n        col_expected_payoffs = p_t @ A\n        j_t = np.argmin(col_expected_payoffs)\n\n        # Record actions and update counts\n        row_actions.append(i_t)\n        col_actions.append(j_t)\n        row_counts[i_t] += 1.0\n        col_counts[j_t] += 1.0\n\n    # Compute row player's belief trajectory (b_t)\n    # b_t is the empirical frequency of column actions up to time t.\n    X = np.zeros((T, 5), dtype=np.float64)\n    temp_col_counts = np.zeros(5, dtype=np.float64)\n    for t in range(T):\n        action_idx = col_actions[t]\n        temp_col_counts[action_idx] += 1.0\n        X[t, :] = temp_col_counts / (t + 1)\n    \n    # Task 3: Compute final Euclidean distance from uniform strategy\n    b_T = X[-1, :]\n    u = np.full(5, 1.0/5.0)\n    d = np.linalg.norm(b_T - u)\n\n    # Task 4: Perform Principal Component Analysis\n    X_c = X - X.mean(axis=0)\n\n    # Use SVD for PCA. full_matrices=False is efficient.\n    try:\n        U, S, Vh = np.linalg.svd(X_c, full_matrices=False)\n    except np.linalg.LinAlgError: # Safety for unusual matrices, though not expected here\n        U, S, Vh = np.zeros((T,1)), np.array([]), np.zeros((1,5))\n\n    # Calculate fraction of variance explained by first two PCs\n    s2 = S**2\n    total_variance = np.sum(s2)\n    rho_2 = 0.0\n    if total_variance > 1e-15:\n        rho_2 = np.sum(s2[:2]) / total_variance\n\n    # Calculate 2D coordinates (scores on first two PCs)\n    scores = np.zeros((T, 2))\n    num_components = min(2, S.shape[0])\n    if num_components > 0:\n        scores[:, :num_components] = U[:, :num_components] * S[:num_components]\n\n    # Task 5: Return coordinates at specified sample times\n    # Sample times are 1-indexed, so we access index t-1\n    C = [scores[t - 1].tolist() for t in sample_times]\n\n    return [d, rho_2, C]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        {'T': 400, 'initial_row_action': 0, 'initial_col_action': 1, 'sample_times': [1, 2, 50, 200, 400]},\n        {'T': 5, 'initial_row_action': 0, 'initial_col_action': 0, 'sample_times': [1, 2, 5]},\n        {'T': 1000, 'initial_row_action': 4, 'initial_col_action': 2, 'sample_times': [1, 10, 100, 500, 1000]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(\n            case['T'],\n            case['initial_row_action'],\n            case['initial_col_action'],\n            case['sample_times']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation includes spaces, which we remove.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Fictitious play operates on the assumption that opponents are drawing their actions from a fixed, stationary distribution. But what happens when this assumption is violated? This exercise pits fictitious play against a more robust learning algorithm, Hedge (also known as exponential weights), in a game against a non-stationary opponent whose strategy changes over time . By comparing their performance using the metric of \"external regret,\" you will gain crucial insights into the relative strengths and weaknesses of these learning rules and understand why algorithms with no-regret guarantees are fundamental in computational economics and machine learning.",
            "id": "2405816",
            "problem": "Let there be a standard zero-sum Rock–Paper–Scissors game in normal form with the row player’s payoff matrix given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & -1 & 1 \\\\\n1 & 0 & -1 \\\\\n-1 & 1 & 0\n\\end{pmatrix},\n$$\nwhere action indices $1$, $2$, and $3$ correspond to Rock, Paper, and Scissors, respectively. The row player (the learner) faces a deterministic, non-stationary opponent (the column player) who selects a pure action $j(t)\\in\\{1,2,3\\}$ at each round $t\\in\\{1,\\dots,T\\}$. The interaction lasts for $T$ rounds.\n\nTwo learning rules for the learner are to be evaluated:\n\n$1.$ Fictitious play. For $t=1$, choose action $i(1)=1$. For $t\\ge 2$, let $\\hat{q}(t-1)\\in\\mathbb{R}^3$ be the empirical frequency vector of the opponent’s past actions through period $t-1$, that is, $\\hat{q}_k(t-1)=\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$ for each $k\\in\\{1,2,3\\}$. Choose a pure best response $i(t)\\in\\arg\\max_{i\\in\\{1,2,3\\}} \\left(A\\,\\hat{q}(t-1)\\right)_i$, breaking ties by selecting the smallest index. The realized payoff at round $t$ is $r^{\\mathrm{FP}}(t)=A_{i(t),\\,j(t)}$. Define the empirical mixed strategy $\\bar{p}^{\\mathrm{FP}}\\in\\mathbb{R}^3$ by $\\bar{p}^{\\mathrm{FP}}_i=\\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$.\n\n$2.$ Hedge (exponential weights) with full-information feedback. Initialize weights $w_i(1)=1$ for each $i\\in\\{1,2,3\\}$. At each round $t\\in\\{1,\\dots,T\\}$, form the mixed strategy $p_i(t)=\\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$. Let $g(t)\\in\\mathbb{R}^3$ be the vector of counterfactual payoffs against the opponent’s realized pure action $j(t)$, i.e., $g_i(t)=A_{i,\\,j(t)}$. The expected payoff is $r^{\\mathrm{H}}(t)=\\sum_{i=1}^3 p_i(t)\\,g_i(t)$. Update weights by $w_i(t+1)=w_i(t)\\,\\exp\\!\\left(\\eta\\,g_i(t)\\right)$, where the learning rate is $\\eta=\\sqrt{\\frac{2\\ln n}{T}}$ with $n=3$. Define the time-averaged strategy $\\bar{p}^{\\mathrm{H}}\\in\\mathbb{R}^3$ by $\\bar{p}^{\\mathrm{H}}=\\frac{1}{T}\\sum_{t=1}^T p(t)$.\n\nFor both learning rules, compute two summary statistics at horizon $T$:\n\n$1.$ Average external regret,\n$$\n\\frac{R_T}{T}\n\\;=\\;\n\\frac{1}{T}\\left(\n\\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t)\n\\;-\\;\n\\sum_{t=1}^T r(t)\n\\right),\n$$\nwhere for fictitious play $r(t)=r^{\\mathrm{FP}}(t)$ and for Hedge $r(t)=r^{\\mathrm{H}}(t)$, and $g_i(t)=A_{i,\\,j(t)}$ in both cases.\n\n$2.$ The $\\ell_1$ distance of the learner’s time-averaged strategy to the unique mixed-strategy Nash equilibrium $u^\\star=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$, namely\n$$\nD(\\bar{p})\n\\;=\\;\n\\sum_{i=1}^3 \\left| \\bar{p}_i - \\tfrac{1}{3} \\right|.\n$$\n\nTest suite. Your program must evaluate the two learning rules on the following four opponent schedules, each specified by a horizon $T$ and a piecewise-constant pure-action schedule:\n\n$1.$ Case A (happy path): $T=90$. Opponent plays Rock for $30$ rounds, then Paper for $30$ rounds, then Scissors for $30$ rounds, i.e., $j(t)=1$ for $t\\in\\{1,\\dots,30\\}$, $j(t)=2$ for $t\\in\\{31,\\dots,60\\}$, and $j(t)=3$ for $t\\in\\{61,\\dots,90\\}$.\n\n$2.$ Case B (boundary): $T=3$. Opponent plays Rock at $t=1$, Paper at $t=2$, Scissors at $t=3$, i.e., $j(1)=1$, $j(2)=2$, $j(3)=3$.\n\n$3.$ Case C (non-stationary edge): $T=60$. Opponent plays Rock for $20$ rounds, then Paper for $10$ rounds, then Scissors for $20$ rounds, then Paper for $10$ rounds, i.e., $j(t)=1$ for $t\\in\\{1,\\dots,20\\}$, $j(t)=2$ for $t\\in\\{21,\\dots,30\\}$, $j(t)=3$ for $t\\in\\{31,\\dots,50\\}$, and $j(t)=2$ for $t\\in\\{51,\\dots,60\\}$.\n\n$4.$ Case D (stationary edge): $T=60$. Opponent plays Rock in all rounds, i.e., $j(t)=1$ for all $t\\in\\{1,\\dots,60\\}$.\n\nRequired outputs. For each case, produce a list of four real numbers\n$$\n\\big[\\, \\tfrac{R_T^{\\mathrm{H}}}{T},\\;\\tfrac{R_T^{\\mathrm{FP}}}{T},\\; D(\\bar{p}^{\\mathrm{H}}),\\; D(\\bar{p}^{\\mathrm{FP}})\\,\\big],\n$$\nwhere the superscripts $\\mathrm{H}$ and $\\mathrm{FP}$ denote Hedge and fictitious play, respectively. Each number must be rounded to $6$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list of these four-element lists, enclosed in square brackets, with no additional text. For example,\n$$\n[\\,[x_1,x_2,x_3,x_4],[y_1,y_2,y_3,y_4],\\dots]\\,,\n$$\nwith each $x_k$ and $y_k$ a decimal rounded to $6$ places. No physical units are involved in this problem, and all angles are irrelevant. There must be no input; compute outputs only for the specified test suite.",
            "solution": "The problem is valid. It presents a well-defined computational task in the domain of game theory and online learning, which is a subfield of computational economics. All parameters, algorithms, and evaluation metrics are specified with sufficient precision to allow for a unique and verifiable solution. The problem is scientifically grounded, using standard models (Rock-Paper-Scissors), algorithms (fictitious play, Hedge), and performance metrics (regret, distance to Nash equilibrium).\n\nThe task is to simulate two learning algorithms, Fictitious Play and Hedge, for a row player in a Rock-Paper-Scissors game against a deterministic, non-stationary column player. We are given the row player's payoff matrix $A$:\n$$\nA = \\begin{pmatrix} 0 & -1 & 1 \\\\ 1 & 0 & -1 \\\\ -1 & 1 & 0 \\end{pmatrix}\n$$\nwhere rows and columns correspond to actions Rock (index $1$), Paper (index $2$), and Scissors (index $3$). The simulation is to be run for $T$ rounds for four different pre-defined opponent action schedules $j(t)$.\n\nThe solution involves implementing the two algorithms and the specified summary statistics. The simulation proceeds iteratively for $t = 1, \\dots, T$.\n\n**1. Fictitious Play (FP) Algorithm**\n\nFictitious play is an iterative learning rule where a player assumes their opponent is playing a stationary mixed strategy, estimated by the empirical frequency of their past actions. The player then chooses a pure best response to this estimated strategy.\n\n- **Initialization ($t=1$):** The learner is prescribed to play Rock, so $i(1)=1$.\n- **Iteration ($t \\ge 2$):**\n    1.  The learner calculates the empirical frequency of the opponent's actions up to round $t-1$. This forms the belief vector $\\hat{q}(t-1) \\in \\mathbb{R}^3$, where each component $\\hat{q}_k(t-1)$ is given by $\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$.\n    2.  The learner calculates the expected payoff for each of their three pure actions against this belief. The expected payoff for action $i$ is $(A\\hat{q}(t-1))_i$.\n    3.  The learner chooses the action $i(t)$ that maximizes this expected payoff: $i(t) \\in \\arg\\max_{i\\in\\{1,2,3\\}} (A\\hat{q}(t-1))_i$. Ties are broken by selecting the action with the smallest index.\n- **Payoff:** The realized payoff at round $t$ is $r^{\\mathrm{FP}}(t) = A_{i(t), j(t)}$.\n\n**2. Hedge (Exponential Weights) Algorithm**\n\nHedge is an algorithm for online decision-making that maintains a distribution over a set of experts (in this case, pure actions). It updates this distribution based on the performance of each expert.\n\n- **Initialization ($t=1$):** The learner starts with equal weights for each action: $w_i(1) = 1$ for $i \\in \\{1, 2, 3\\}$. The learning rate $\\eta$ is set to $\\sqrt{\\frac{2\\ln n}{T}}$ with $n=3$.\n- **Iteration ($t = 1, \\dots, T$):**\n    1.  The learner forms a mixed strategy $p(t)$ by normalizing the current weights: $p_i(t) = \\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$.\n    2.  After the opponent plays action $j(t)$, the learner observes the counterfactual payoffs for all of their own actions. This is given by the vector $g(t) \\in \\mathbb{R}^3$, where $g_i(t) = A_{i, j(t)}$. This vector corresponds to the $j(t)$-th column of the matrix $A$.\n    3.  The weights for the next round are updated multiplicatively: $w_i(t+1) = w_i(t) \\exp(\\eta g_i(t))$. Actions that would have yielded a higher payoff receive a larger weight increase.\n- **Payoff:** The algorithm's performance at round $t$ is measured by its expected payoff under the mixed strategy $p(t)$, which is $r^{\\mathrm{H}}(t) = \\sum_{i=1}^3 p_i(t) g_i(t)$.\n\n**3. Summary Statistics**\n\nFor each algorithm and each opponent schedule, two metrics are computed at the horizon $T$:\n\n- **Average External Regret:** This measures the average per-round difference between the algorithm's cumulative payoff and the cumulative payoff of the best single action in hindsight. It is calculated as:\n$$\n\\frac{R_T}{T} = \\frac{1}{T}\\left( \\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t) - \\sum_{t=1}^T r(t) \\right)\n$$\nFor Fictitious Play, $r(t) = r^{\\mathrm{FP}}(t)$, and for Hedge, $r(t) = r^{\\mathrm{H}}(t)$. The term $\\sum_{t=1}^T g_i(t)$ represents the total payoff that would have been accumulated by consistently playing action $i$.\n\n- **$\\ell_1$ Distance to Nash Equilibrium:** This metric quantifies how far the learner's time-averaged strategy is from the unique mixed-strategy Nash equilibrium of Rock-Paper-Scissors, which is $u^\\star = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$. The distance is:\n$$\nD(\\bar{p}) = \\sum_{i=1}^3 \\left| \\bar{p}_i - \\frac{1}{3} \\right|\n$$\nFor Fictitious Play, the time-averaged strategy $\\bar{p}^{\\mathrm{FP}}$ is the empirical frequency of the actions played: $\\bar{p}^{\\mathrm{FP}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$. For Hedge, it is the average of the mixed strategies used in each round: $\\bar{p}^{\\mathrm{H}} = \\frac{1}{T}\\sum_{t=1}^T p(t)$.\n\nThe implementation will simulate these processes for the four specified test cases, computing and collecting the four required statistics ($R_T^{\\mathrm{H}}/T$, $R_T^{\\mathrm{FP}}/T$, $D(\\bar{p}^{\\mathrm{H}})$, $D(\\bar{p}^{\\mathrm{FP}})$) for each.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    # Payoff matrix for the row player (learner).\n    # Actions: 0=Rock, 1=Paper, 2=Scissors\n    A = np.array([[0, -1, 1],\n                  [1, 0, -1],\n                  [-1, 1, 0]])\n    n_actions = 3\n    u_star = np.full(n_actions, 1.0 / n_actions)\n\n    def generate_schedule(case, T):\n        \"\"\"Generates the opponent's action schedule for a given case.\"\"\"\n        # Note: actions are 1-indexed in problem, 0-indexed in code.\n        # This function returns 0-indexed actions.\n        if case == 'A':  # T=90\n            return np.concatenate([np.full(30, 0), np.full(30, 1), np.full(30, 2)])\n        elif case == 'B':  # T=3\n            return np.array([0, 1, 2])\n        elif case == 'C':  # T=60\n            return np.concatenate([np.full(20, 0), np.full(10, 1), np.full(20, 2), np.full(10, 1)])\n        elif case == 'D':  # T=60\n            return np.full(60, 0)\n        return None\n\n    def run_fictitious_play(T, opponent_schedule, A):\n        \"\"\"Simulates the Fictitious Play algorithm.\"\"\"\n        opponent_action_counts = np.zeros(n_actions)\n        learner_action_counts = np.zeros(n_actions)\n        total_payoff_fp = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n\n            if t == 0:\n                learner_action_idx = 0  # Rule: i(1)=1 (Rock)\n            else:\n                # Belief is the empirical frequency of opponent's past actions\n                opponent_empirical_freq = opponent_action_counts / t\n                expected_payoffs = A @ opponent_empirical_freq\n                # Best response, ties broken by smallest index (np.argmax default)\n                learner_action_idx = np.argmax(expected_payoffs)\n\n            # Record actions and payoffs\n            learner_action_counts[learner_action_idx] += 1\n            total_payoff_fp += A[learner_action_idx, opponent_action_idx]\n            opponent_action_counts[opponent_action_idx] += 1\n\n        # Calculate summary statistics\n        p_bar_fp = learner_action_counts / T\n        d_fp = np.sum(np.abs(p_bar_fp - u_star))\n\n        return total_payoff_fp, d_fp\n\n    def run_hedge(T, opponent_schedule, A):\n        \"\"\"Simulates the Hedge (Exponential Weights) algorithm.\"\"\"\n        eta = np.sqrt(2 * np.log(n_actions) / T)\n        weights = np.ones(n_actions)\n        \n        sum_p_t = np.zeros(n_actions)\n        total_expected_payoff_h = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            \n            # Form mixed strategy\n            p_t = weights / np.sum(weights)\n            sum_p_t += p_t\n            \n            # Counterfactual payoffs\n            g_t = A[:, opponent_action_idx]\n            \n            # Expected payoff for this round\n            total_expected_payoff_h += p_t @ g_t\n            \n            # Update weights\n            weights *= np.exp(eta * g_t)\n            \n        # Calculate summary statistics\n        p_bar_h = sum_p_t / T\n        d_h = np.sum(np.abs(p_bar_h - u_star))\n\n        return total_expected_payoff_h, d_h\n\n    def calculate_regret(T, total_payoff, opponent_schedule, A):\n        \"\"\"Calculates the average external regret.\"\"\"\n        # Payoff of best fixed action in hindsight\n        total_g = np.zeros(n_actions)\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            total_g += A[:, opponent_action_idx]\n        \n        max_fixed_payoff = np.max(total_g)\n        avg_regret = (max_fixed_payoff - total_payoff) / T\n        return avg_regret\n\n    test_cases = [\n        ('A', 90),\n        ('B', 3),\n        ('C', 60),\n        ('D', 60)\n    ]\n\n    all_results = []\n\n    for case_label, T in test_cases:\n        opponent_schedule = generate_schedule(case_label, T)\n\n        # Run simulations\n        total_payoff_fp, d_fp = run_fictitious_play(T, opponent_schedule, A)\n        total_payoff_h, d_h = run_hedge(T, opponent_schedule, A)\n        \n        # Calculate regrets\n        avg_regret_fp = calculate_regret(T, total_payoff_fp, opponent_schedule, A)\n        avg_regret_h = calculate_regret(T, total_payoff_h, opponent_schedule, A)\n        \n        # Store results in the specified order\n        results = [avg_regret_h, avg_regret_fp, d_h, d_fp]\n        all_results.append(results)\n    \n    # Format the output string as required\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_nums = [f\"{num:.6f}\" for num in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "This final practice invites a fascinating shift in perspective: instead of modeling a learning agent, you will become a sophisticated player who strategically plays against one. If an opponent follows a predictable rule like fictitious play, their future actions can be anticipated, and their beliefs can be manipulated. This exercise  challenges you to find the optimal sequence of plays to maximize your own total payoff by exploiting the opponent's learning process. Solving this problem requires dynamic programming, a core technique in economics for sequential decision-making, and provides a powerful illustration of the concept of strategic teaching.",
            "id": "2405888",
            "problem": "Consider a repeated two-player, two-action strategic-form game played over a finite horizon of $T$ discrete periods indexed by $t \\in \\{1,2,\\dots,T\\}$. Player $1$ (the sophisticated player) chooses actions from $\\{0,1\\}$ and seeks to maximize the sum of stage payoffs over the horizon. Player $2$ (the opponent) chooses actions from $\\{0,1\\}$ and follows classical fictitious play: before each period $t$, Player $2$ forms a belief about Player $1$’s probability of playing action $0$ based on an initial prior and the empirical frequency of Player $1$’s past actions, and then plays a best response to that belief to maximize Player $2$’s own expected stage payoff. Player $1$ knows Player $2$’s learning rule and payoff matrix and chooses actions to best respond to the learning process, anticipating the effect of current actions on future beliefs of Player $2$.\n\nThe one-shot stage payoffs are as follows. Let $U_1$ be the $2 \\times 2$ matrix of Player $1$’s payoffs and $U_2$ be the $2 \\times 2$ matrix of Player $2$’s payoffs, where the row index corresponds to Player $1$’s action and the column index corresponds to Player $2$’s action. If Player $1$ chooses action $a_1 \\in \\{0,1\\}$ and Player $2$ chooses action $a_2 \\in \\{0,1\\}$ in a period, then Player $1$ receives payoff $U_1[a_1,a_2]$ and Player $2$ receives payoff $U_2[a_1,a_2]$.\n\nPlayer $2$’s fictitious play belief at the beginning of period $t$ is given by\n$$\np_t \\equiv \\frac{\\alpha_0 + n_0(t-1)}{\\alpha_0 + \\alpha_1 + (t-1)},\n$$\nwhere $\\alpha_0,\\alpha_1 \\in \\mathbb{N}_0$ are fixed prior pseudo-counts for actions $0$ and $1$ respectively, and $n_0(t-1)$ is the total number of times Player $1$ has chosen action $0$ in periods $1$ through $t-1$. Assume $\\alpha_0 + \\alpha_1 \\ge 1$ so that the denominator is strictly positive at $t=1$. In each period $t$, Player $2$ chooses a best response action \n$$\na_2(t) \\in \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ \\mathbb{E}[U_2[a_1,b] \\mid a_1 \\sim \\text{Bernoulli}(p_t)] \\right\\} \n= \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ p_t \\cdot U_2[0,b] + (1-p_t)\\cdot U_2[1,b] \\right\\}.\n$$\nIf the argmax is not unique, Player $2$ breaks ties by selecting the smaller action index, i.e., $a_2(t)=0$ in case of a tie.\n\nIn each period $t$, the sequence of events is:\n- The belief $p_t$ is formed from the prior and Player $1$’s past actions.\n- Player $2$ chooses $a_2(t)$ as a best response to $p_t$ as defined above.\n- Player $1$ chooses a pure action $a_1(t) \\in \\{0,1\\}$.\n- Payoffs $U_1[a_1(t),a_2(t)]$ and $U_2[a_1(t),a_2(t)]$ are realized.\n- The count $n_0(t)$ is updated by $n_0(t)=n_0(t-1)+\\mathbf{1}\\{a_1(t)=0\\}$.\n\nPlayer $1$ observes the entire history and chooses actions adaptively to maximize the undiscounted total payoff\n$$\n\\sum_{t=1}^{T} U_1[a_1(t),a_2(t)],\n$$\nanticipating Player $2$’s belief updates and best responses.\n\nYour task is to write a program that, for each test case below, computes the maximum total payoff attainable by Player $1$ over the horizon $T$ under the above dynamics, assuming Player $1$ can commit to any pure action at each period while fully anticipating Player $2$’s fictitious play behavior and tie-breaking rule.\n\nNo physical quantities or angles appear, so no units are required. All outputs must be exact integers.\n\nInput is not provided to your program; instead, your program must internally use the following test suite, where each case specifies $U_1$, $U_2$, the horizon $T$, and the prior $(\\alpha_0,\\alpha_1)$:\n\n- Test case $1$ (manipulation beneficial):\n  - $$U_1 = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\end{bmatrix}$$,\n  - $$U_2 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$$,\n  - $T = 5$,\n  - $(\\alpha_0,\\alpha_1) = (0,1)$.\n\n- Test case $2$ (zero-sum matching pennies):\n  - $$U_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}$$,\n  - $$U_2 = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix}$$,\n  - $T = 6$,\n  - $(\\alpha_0,\\alpha_1) = (1,1)$.\n\n- Test case $3$ (boundary tie at the start, coordination):\n  - $$U_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$$,\n  - $$U_2 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}$$,\n  - $T = 3$,\n  - $(\\alpha_0,\\alpha_1) = (1,1)$.\n\n- Test case $4$ (threshold exactly at the prior):\n  - $$U_1 = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\end{bmatrix}$$,\n  - $$U_2 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$$,\n  - $T = 2$,\n  - $(\\alpha_0,\\alpha_1) = (2,3)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, e.g., $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the maximum total payoff (an integer) for test case $i$.",
            "solution": "The problem is subjected to validation and is deemed valid. It is a well-posed problem in computational game theory, specifically modeling a sophisticated player's optimal strategy against an opponent using a fictitious play learning rule. The setup is mathematically consistent, self-contained, and free of scientific or logical flaws. The problem can be solved algorithmically using dynamic programming.\n\nThe problem requires finding the optimal strategy for Player $1$, who is a sophisticated agent facing a predictable opponent, Player $2$, over a finite horizon of $T$ periods. Player $1$’s actions influence Player $2$’s future beliefs and, consequently, future actions. This is a sequential decision-making problem under certainty, as Player $2$'s behavior is deterministic, making it amenable to solution by dynamic programming and backward induction.\n\nLet the state of the system at the beginning of period $t \\in \\{1, 2, \\dots, T\\}$ be defined by the history of Player $1$'s actions. The only aspect of the history that influences future dynamics is the count of times Player $1$ has chosen action $0$. Thus, a sufficient state representation is the pair $(t, k)$, where $t$ is the current period number and $k = n_0(t-1)$ is the number of times Player $1$ has played action $0$ in periods $1$ through $t-1$. The number of times Player $1$ has played action $1$ is $(t-1)-k$.\n\nLet $V(t, k)$ denote the maximum total payoff Player $1$ can obtain from period $t$ to the end of the horizon $T$, given that the system is in state $(t, k)$. Our goal is to compute $V(1, 0)$, as at the start of the game ($t=1$), no actions have been taken ($k=0$).\n\nThe dynamic programming solution proceeds via backward induction, starting from the final period.\n\nThe base case is for periods after the horizon ends. At $t=T+1$, the game is over, and no more payoffs can be accumulated.\n$$V(T+1, k) = 0 \\quad \\text{for all valid } k \\in \\{0, 1, \\dots, T\\}.$$\n\nFor any period $t \\in \\{T, T-1, \\dots, 1\\}$ and any valid state $(t, k)$ where $k \\in \\{0, 1, \\dots, t-1\\}$, we can write a Bellman equation for $V(t, k)$. First, we determine Player $2$'s action, $a_2(t)$, which is a deterministic function of the state $(t, k)$. Player $2$'s belief about Player $1$ playing action $0$ is:\n$$p_t = \\frac{\\alpha_0 + k}{\\alpha_0 + \\alpha_1 + t - 1}.$$\nPlayer $2$ chooses $a_2(t) \\in \\{0,1\\}$ to maximize their expected payoff. They will choose action $0$ if their expected payoff from choosing $0$ is greater than or equal to (due to the tie-breaking rule) their expected payoff from choosing $1$:\n$$ p_t \\cdot U_2[0,0] + (1-p_t)\\cdot U_2[1,0] \\ge p_t \\cdot U_2[0,1] + (1-p_t)\\cdot U_2[1,1]. $$\nTo avoid floating-point arithmetic and potential precision errors, we can express this inequality using integers. Let $N_t = \\alpha_0 + k$ and $D_t = \\alpha_0 + \\alpha_1 + t - 1$, so $p_t = N_t / D_t$. Since $D_t > 0$, we can multiply the inequality by $D_t$ and rearrange to get:\n$$ N_t \\cdot (U_2[0,0] - U_2[1,0] - U_2[0,1] + U_2[1,1]) \\ge D_t \\cdot (U_2[1,1] - U_2[1,0]). $$\nThis comparison involves only integer arithmetic and fully determines $a_2(t)$ for any state $(t, k)$.\n\nGiven $a_2(t)$, Player $1$ chooses $a_1(t) \\in \\{0,1\\}$ to maximize the sum of the current period's payoff and the value of the subsequent state. The Bellman equation is:\n$$ V(t, k) = \\max \\left\\{\n    \\underbrace{U_1[0, a_2(t)] + V(t+1, k+1)}_{\\text{Value if } a_1(t)=0},\n    \\underbrace{U_1[1, a_2(t)] + V(t+1, k)}_{\\text{Value if } a_1(t)=1}\n\\right\\}. $$\nHere, if Player $1$ chooses action $0$, the count of zeros for the next period, $t+1$, becomes $k+1$. If Player $1$ chooses action $1$, the count remains $k$.\n\nThe algorithm consists of computing $V(t, k)$ for all relevant states by iterating $t$ from $T$ down to $1$, and for each $t$, iterating $k$ from $0$ to $t-1$. A two-dimensional array can be used to store the computed values of $V(t, k)$, a technique known as memoization. The final answer is the value $V(1, 0)$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the repeated game problem for a sophisticated player against a \n    fictitious play opponent for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: manipulation beneficial\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 5,\n            \"prior\": (0, 1),\n        },\n        # Case 2: zero-sum matching pennies\n        {\n            \"U1\": np.array([[1, -1], [-1, 1]], dtype=int),\n            \"U2\": np.array([[-1, 1], [1, -1]], dtype=int),\n            \"T\": 6,\n            \"prior\": (1, 1),\n        },\n        # Case 3: boundary tie at the start, coordination\n        {\n            \"U1\": np.array([[1, 0], [0, 0]], dtype=int),\n            \"U2\": np.array([[2, 0], [0, 2]], dtype=int),\n            \"T\": 3,\n            \"prior\": (1, 1),\n        },\n        # Case 4: threshold exactly at the prior\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 2,\n            \"prior\": (2, 3),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        U1 = case[\"U1\"]\n        U2 = case[\"U2\"]\n        T = case[\"T\"]\n        alpha0, alpha1 = case[\"prior\"]\n\n        # V[t][k] stores the max payoff from period t to T, given that Player 1\n        # has played action 0 a total of k times up to period t-1.\n        # Dimensions are (T+2) x (T+1) to handle t=T+1 and k=T.\n        V = np.zeros((T + 2, T + 1), dtype=int)\n\n        # Precompute constants for Player 2's best response to avoid redundant calculations.\n        # P2 plays 0 if: N * D_U2 >= D * C_U2\n        D_U2 = (U2[0, 0] - U2[1, 0]) - (U2[0, 1] - U2[1, 1])\n        C_U2 = U2[1, 1] - U2[1, 0]\n\n        # Backward induction from period T down to 1\n        for t in range(T, 0, -1):\n            # In period t, t-1 rounds have passed, so k (count of '0's) can be from 0 to t-1\n            for k in range(t):\n                # Belief p_t = N/D. We use integer arithmetic to determine P2's action.\n                N = alpha0 + k\n                D = alpha0 + alpha1 + t - 1\n                \n                # Determine Player 2's action a2_t\n                # Tie-breaking rule: P2 chooses action 0 in case of indifference.\n                # This is handled by '>=' in the main comparison.\n                \n                a2_t = 0 # Default action\n                if D_U2 > 0:\n                    if not (N * D_U2 >= D * C_U2):\n                        a2_t = 1\n                elif D_U2 < 0:\n                    if not (N * D_U2 <= D * C_U2):  # Inequality flips\n                        a2_t = 1\n                else: # D_U2 == 0, p_t does not affect choice\n                    if not (0 >= C_U2):\n                        a2_t = 1\n                \n                # Player 1's Bellman equation\n                # Value if P1 chooses action 0\n                val_if_0 = U1[0, a2_t] + V[t + 1, k + 1]\n                \n                # Value if P1 chooses action 1\n                val_if_1 = U1[1, a2_t] + V[t + 1, k]\n                \n                V[t, k] = max(val_if_0, val_if_1)\n        \n        # The result is the value at the beginning of the game (t=1, k=0)\n        results.append(V[1, 0])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}