{
    "hands_on_practices": [
        {
            "introduction": "要真正理解一个理论，最好的方式莫过于亲手实现它。这个练习将引导你从零开始构建“虚拟对策”（fictitious play）这个在博弈论中至关重要的学习模型。通过在经典的“石头-剪刀-布-蜥蜴-斯波克”（Rock-Paper-Scissors-Lizard-Spock）游戏中模拟玩家的行为，你不仅能掌握虚拟对策的核心机制——即根据对手过去行为的经验频率形成信念并做出最优反应，还将学习使用主成分分析（Principal Component Analysis, PCA）这一强大的数据分析工具来可视化高维信念的动态演化过程。",
            "id": "2405836",
            "problem": "要求您设计并实现一个完整的、可运行的程序，该程序在一个双人零和的剪刀石头布-蜥蜴-斯波克（Rock-Paper-Scissors-Lizard-Spock）游戏中模拟确定性虚拟对策，记录一个玩家的五维信念轨迹，并在指定的时间快照生成可供可视化的二维主成分坐标。最终输出必须是遵循精确指定格式的单行文本。\n\n基本根据与定义：\n- 考虑一个双人范式零和博弈，其行动集 $\\mathcal{A}=\\{0,1,2,3,4\\}$ 分别对应石头、布、剪刀、蜥蜴和斯波克。\n- 设行玩家的支付矩阵为 $A\\in\\mathbb{R}^{5\\times 5}$，其条目 $A_{ij}\\in\\{-1,0,1\\}$ 满足反对称性 $A=-A^{\\top}$ 且对所有 $i$ 都有 $A_{ii}=0$。优胜关系由有序对集合给出\n$$\n\\mathcal{W}=\\{(0,2),(0,3),(1,0),(1,4),(2,1),(2,3),(3,4),(3,1),(4,2),(4,0)\\},\n$$\n意味着若 $(i,j)\\in\\mathcal{W}$，则行动 $i$ 击败行动 $j$，因此 $A_{ij}=1$ 且 $A_{ji}=-1$；平局则产生 $A_{ii}=0$。\n- 确定性虚拟对策在离散时期 $t=1,2,\\dots,T$ 中进行。每个玩家根据对手过去行动的经验频率形成信念，然后选择一个纯最佳响应。设行玩家关于列玩家在时期 $t$ 的混合策略的信念为经验频率向量 $q_t\\in\\Delta^4$，该向量根据列玩家在时期 $\\{1,\\dots,t-1\\}$ 的行动计算得出。类似地，设列玩家关于行玩家在时期 $t$ 的混合策略的信念为 $p_t\\in\\Delta^4$，该向量根据行玩家在时期 $\\{1,\\dots,t-1\\}$ 的行动计算得出。行玩家对抗信念 $q_t$ 的期望支付向量是 $v_t=A\\,q_t$，其纯最佳响应为任意 $i\\in\\arg\\max_{k\\in\\mathcal{A}} (v_t)_k$。列玩家旨在最小化行玩家的支付；给定 $p_t$ 时，其期望行支付向量为 $w_t=p_t^{\\top}A\\in\\mathbb{R}^5$，其纯最佳响应为任意 $j\\in\\arg\\min_{\\ell\\in\\mathcal{A}} (w_t)_{\\ell}$。您必须实现确定性的平局打破规则，即始终在最佳响应的 argmax 或 argmin 集合中选择最小的索引。\n- 设行玩家的信念轨迹为序列 $(b_t)_{t=1}^T$，其中 $b_t\\in\\Delta^4$ 是列玩家在时期 $\\{1,\\dots,t\\}$ 内行动的经验频率。这是一个位于 $5$-单纯形中的五维时间序列。\n\n需实现的计算任务：\n1. 使用优胜集合 $\\mathcal{W}$ 构建支付矩阵 $A$。\n2. 从时期 $t=1$ 行玩家和列玩家指定的初始纯行动开始，模拟确定性虚拟对策共 $T$ 个时期。对于每个时期 $t\\ge 2$，双方玩家同时根据对手在时期 $\\{1,\\dots,t-1\\}$ 内的行动所形成的信念，使用指定的确定性平局打破规则选择其纯最佳响应。记录行玩家信念的完整序列 $(b_t)_{t=1}^T$。\n3. 定义均匀混合策略 $u=\\left(\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5}\\right)$。计算最终的欧几里得距离 $\\|b_T-u\\|_2$。\n4. 为进行“可视化”，计算信念轨迹的一个二维主成分投影。设 $X\\in\\mathbb{R}^{T\\times 5}$ 为一个矩阵，其第 $t$ 行为 $b_t^{\\top}$。通过减去列均值将 $X$ 中心化，得到 $X_c$。使用奇异值分解 (SVD) $X_c=U\\Sigma V^{\\top}$，其中奇异值位于 $\\Sigma$ 的对角线上，将前两个主成分解释的方差比例定义为\n$$\n\\rho_2=\\frac{\\sigma_1^2+\\sigma_2^2}{\\sum_{k}\\sigma_k^2},\n$$\n并约定如果 $\\sum_k \\sigma_k^2=0$，则 $\\rho_2=0$。将时间 $t$ 的二维坐标定义为矩阵 $U_{[:,1:2]}\\,\\mathrm{diag}(\\sigma_1,\\sigma_2)$ 的第 $t$ 行，即在前两个主成分上的得分。\n5. 在指定的采样时间点，返回二维坐标作为“可视化就绪”数据。\n\n测试套件：\n为以下三个参数集实现上述任务。对于每个案例，使用提供的时期数、初始行动和用于投影输出的采样时间。行动索引遵循：石头 $=0$，布 $=1$，剪刀 $=2$，蜥蜴 $=3$，斯波克 $=4$。\n- 案例 $1$ （通用“理想路径”）：$T=400$，初始行行动 $=0$，初始列行动 $=1$，采样时间 $=\\{1,2,50,200,400\\}$。\n- 案例 $2$ （小样本边界）：$T=5$，初始行行动 $=0$，初始列行动 $=0$，采样时间 $=\\{1,2,5\\}$。\n- 案例 $3$ （更长的时间范围，不同起点）：$T=1000$，初始行行动 $=4$，初始列行动 $=2$，采样时间 $=\\{1,10,100,500,1000\\}$。\n\n每个测试案例的所需输出：\n- 设 $d=\\|b_T-u\\|_2$ 为与均匀策略的最终欧几里得距离。\n- 设 $\\rho_2$ 为如上定义的前两个主成分所解释的方差比例。\n- 设 $C$ 为在指定采样时间点的二维坐标列表，其顺序与采样时间相同，其中每个坐标是一对 $[c_{t,1},c_{t,2}]$。\n\n最终输出格式：\n- 您的程序必须生成单行文本，其中包含一个由方括号括起来的、包含三个测试案例结果的逗号分隔列表。每个测试案例的结果必须是 $[d,\\rho_2,C]$ 形式的列表，其中 $C$ 是一个包含 2 元素列表的列表。例如，一个语法上有效的整体输出形式为\n$[[d_1,\\rho_{2,1},C_1],[d_2,\\rho_{2,2},C_2],[d_3,\\rho_{2,3},C_3]]$。\n- 程序没有输入；测试套件是硬编码的。\n- 角度和物理单位不适用；所有输出均为无单位的实数。",
            "solution": "该问题陈述已经过严格验证，并被确定为有效。它在博弈论和统计学领域具有科学依据，问题设定良好，具有唯一的确定性解，并且其定义和要求是客观的。所有必要的数据和条件均已提供，不存在内部矛盾。因此，我们可以着手进行求解。\n\n该问题要求实现一个针对双人零和剪刀石头布-蜥蜴-斯波克游戏的确定性虚拟对策模拟。该过程涉及构建游戏的支付矩阵，在指定数量的时期内模拟学习动态，并使用主成分分析（PCA）分析其中一个玩家所产生的信念轨迹。\n\n首先，我们构建行玩家的支付矩阵 $A \\in \\mathbb{R}^{5 \\times 5}$。行动集为 $\\mathcal{A} = \\{0, 1, 2, 3, 4\\}$。问题提供了表示胜负关系的集合 $\\mathcal{W}$。对于每对 $(i, j) \\in \\mathcal{W}$，行动 $i$ 击败行动 $j$，产生支付 $A_{ij} = 1$。该博弈是零和的，这意味着反对称性 $A = -A^\\top$，因此 $A_{ji} = -A_{ij} = -1$。平局导致零支付，即 $A_{ii} = 0$。这些规则唯一地定义了矩阵 $A$。\n\n其次，我们模拟 $T$ 个时期的虚拟对策动态。模拟从时期 $t=1$ 给定的初始行动 $i_1$ 和 $j_1$ 开始。对于随后的每个时期 $t \\in \\{2, \\dots, T\\}$，玩家形成信念并选择最佳响应。行玩家在时间 $t$ 的信念，记为 $q_t \\in \\Delta^4$，是列玩家在所有先前时期 $\\{1, \\dots, t-1\\}$ 中行动的经验频率。行玩家选择行动 $k \\in \\mathcal{A}$ 的期望支付是向量 $v_t = A q_t$ 的第 $k$ 个分量。然后，行玩家选择一个能最大化此支付的行动 $i_t$：\n$$\ni_t \\in \\arg\\max_{k \\in \\mathcal{A}} (A q_t)_k\n$$\n类似地，列玩家根据行玩家的过去行动形成信念 $p_t \\in \\Delta^4$。列玩家旨在最小化行玩家的支付。对于列玩家的每个选择 $\\ell \\in \\mathcal{A}$，其期望支付由向量 $w_t = p_t^\\top A$ 给出。列玩家选择一个能最小化此值的行动 $j_t$：\n$$\nj_t \\in \\arg\\min_{\\ell \\in \\mathcal{A}} (p_t^\\top A)_\\ell\n$$\n强制执行严格的平局打破规则：如果存在多个最佳响应，则选择索引最小的行动。这使得模拟过程完全确定。\n\n第三，我们分析行玩家的信念轨迹 $(b_t)_{t=1}^T$。请注意，用于分析的信念 $b_t$ 定义为列玩家在时期 $\\{1, \\dots, t\\}$ 内行动的经验频率，这包括了时间 $t$ 的行动。这与用于在时间 $t$ 进行决策的信念 $q_t$ 是不同的。最终的信念偏差通过欧几里得距离 $d = \\|b_T - u\\|_2$ 来衡量，其中 $u = (\\frac{1}{5}, \\dots, \\frac{1}{5})^\\top$ 是均匀混合策略。\n\n第四，我们对信念轨迹执行 PCA 以获得二维表示。该轨迹被组织成一个数据矩阵 $X \\in \\mathbb{R}^{T \\times 5}$，其中第 $t$ 行是 $b_t^\\top$。通过从每行中减去列均值向量来对数据进行中心化，得到 $X_c$。中心化矩阵的奇异值分解（SVD），即 $X_c = U \\Sigma V^\\top$，提供了主成分。$\\Sigma$ 对角线上的奇异值 $\\sigma_k$ 用于计算前两个主成分解释的方差比例：\n$$\n\\rho_2 = \\frac{\\sigma_1^2 + \\sigma_2^2}{\\sum_{k=1}^{\\text{rank}(X_c)} \\sigma_k^2}\n$$\n如果总方差 $\\sum_k \\sigma_k^2 = 0$，我们设 $\\rho_2=0$。时间 $t$ 的二维坐标（也称为得分）由矩阵乘积 $U_{[:,1:2]} \\mathrm{diag}(\\sigma_1, \\sigma_2)$ 的第 $t$ 行给出。这表示中心化数据在前两个主成分轴上的投影。\n\n最后，对于每个测试案例，我们计算距离 $d$、方差比例 $\\rho_2$ 以及在指定采样时间的二维坐标列表 $C$。然后将这些结果格式化为所需的输出结构。",
            "answer": "```python\nimport numpy as np\n\ndef run_case(T, initial_row_action, initial_col_action, sample_times):\n    \"\"\"\n    Runs a single test case for the fictitious play simulation.\n    \"\"\"\n    # Task 1: Build the payoff matrix A\n    A = np.zeros((5, 5), dtype=np.float64)\n    dominance_pairs = [\n        (0, 2), (0, 3), (1, 0), (1, 4), (2, 1),\n        (2, 3), (3, 4), (3, 1), (4, 2), (4, 0)\n    ]\n    for i, j in dominance_pairs:\n        A[i, j] = 1.0\n        A[j, i] = -1.0\n\n    # Task 2: Simulate deterministic fictitious play\n    row_actions = [initial_row_action]\n    col_actions = [initial_col_action]\n\n    row_counts = np.zeros(5, dtype=np.float64)\n    row_counts[initial_row_action] = 1.0\n    col_counts = np.zeros(5, dtype=np.float64)\n    col_counts[initial_col_action] = 1.0\n\n    for t in range(2, T + 1):\n        # Beliefs for period t are based on actions from 1 to t-1\n        q_t = col_counts / (t - 1)\n        p_t = row_counts / (t - 1)\n\n        # Row player's best response (maximizes payoff)\n        row_expected_payoffs = A @ q_t\n        i_t = np.argmax(row_expected_payoffs)\n\n        # Column player's best response (minimizes row's payoff)\n        col_expected_payoffs = p_t @ A\n        j_t = np.argmin(col_expected_payoffs)\n\n        # Record actions and update counts\n        row_actions.append(i_t)\n        col_actions.append(j_t)\n        row_counts[i_t] += 1.0\n        col_counts[j_t] += 1.0\n\n    # Compute row player's belief trajectory (b_t)\n    # b_t is the empirical frequency of column actions up to time t.\n    X = np.zeros((T, 5), dtype=np.float64)\n    temp_col_counts = np.zeros(5, dtype=np.float64)\n    for t in range(T):\n        action_idx = col_actions[t]\n        temp_col_counts[action_idx] += 1.0\n        X[t, :] = temp_col_counts / (t + 1)\n    \n    # Task 3: Compute final Euclidean distance from uniform strategy\n    b_T = X[-1, :]\n    u = np.full(5, 1.0/5.0)\n    d = np.linalg.norm(b_T - u)\n\n    # Task 4: Perform Principal Component Analysis\n    X_c = X - X.mean(axis=0)\n\n    # Use SVD for PCA. full_matrices=False is efficient.\n    try:\n        U, S, Vh = np.linalg.svd(X_c, full_matrices=False)\n    except np.linalg.LinAlgError: # Safety for unusual matrices, though not expected here\n        U, S, Vh = np.zeros((T,1)), np.array([]), np.zeros((1,5))\n\n    # Calculate fraction of variance explained by first two PCs\n    s2 = S**2\n    total_variance = np.sum(s2)\n    rho_2 = 0.0\n    if total_variance > 1e-15:\n        rho_2 = np.sum(s2[:2]) / total_variance\n\n    # Calculate 2D coordinates (scores on first two PCs)\n    scores = np.zeros((T, 2))\n    num_components = min(2, S.shape[0])\n    if num_components > 0:\n        scores[:, :num_components] = U[:, :num_components] * S[:num_components]\n\n    # Task 5: Return coordinates at specified sample times\n    # Sample times are 1-indexed, so we access index t-1\n    C = [scores[t - 1].tolist() for t in sample_times]\n\n    return [d, rho_2, C]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        {'T': 400, 'initial_row_action': 0, 'initial_col_action': 1, 'sample_times': [1, 2, 50, 200, 400]},\n        {'T': 5, 'initial_row_action': 0, 'initial_col_action': 0, 'sample_times': [1, 2, 5]},\n        {'T': 1000, 'initial_row_action': 4, 'initial_col_action': 2, 'sample_times': [1, 10, 100, 500, 1000]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(\n            case['T'],\n            case['initial_row_action'],\n            case['initial_col_action'],\n            case['sample_times']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation includes spaces, which we remove.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "一个学习算法的优劣通常是在比较中显现的。在掌握了虚拟对策的基础后，这个练习将我们带入一个更广阔的视角：它与其他在线学习策略相比表现如何？ 你将实现并对比虚拟对策和另一个在线学习领域的基石算法——Hedge（指数权重算法），特别是在面对一个非平稳（non-stationary）对手时的表现。通过这次对比实践，你将深刻理解不同学习模型（如最优反应动态与后悔最小化）在适应性和性能保证上的根本差异，并学会使用“外部遗憾”（external regret）等标准指标来量化评估它们的表现。",
            "id": "2405816",
            "problem": "假设有一个标准的零和石头剪刀布范式博弈，行参与人的收益矩阵由下式给出\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & -1 & 1 \\\\\n1 & 0 & -1 \\\\\n-1 & 1 & 0\n\\end{pmatrix},\n$$\n其中行动索引 $1$、$2$ 和 $3$ 分别对应于石头、布和剪刀。行参与人（学习者）面对一个确定性的、非平稳的对手（列参与人），对手在每一轮 $t\\in\\{1,\\dots,T\\}$ 选择一个纯行动 $j(t)\\in\\{1,2,3\\}$。互动持续 $T$ 轮。\n\n将对学习者的两种学习规则进行评估：\n\n$1.$ 虚拟对策 (Fictitious play)。当 $t=1$ 时，选择行动 $i(1)=1$。当 $t\\ge 2$ 时，令 $\\hat{q}(t-1)\\in\\mathbb{R}^3$ 为对手截至第 $t-1$ 期过去行动的经验频率向量，即对每个 $k\\in\\{1,2,3\\}$，有 $\\hat{q}_k(t-1)=\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$。选择一个纯最佳响应 $i(t)\\in\\arg\\max_{i\\in\\{1,2,3\\}} \\left(A\\,\\hat{q}(t-1)\\right)_i$，若有多个最优解，则选择索引最小的。在第 $t$ 轮实现的收益为 $r^{\\mathrm{FP}}(t)=A_{i(t),\\,j(t)}$。定义经验混合策略 $\\bar{p}^{\\mathrm{FP}}\\in\\mathbb{R}^3$ 为 $\\bar{p}^{\\mathrm{FP}}_i=\\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$。\n\n$2.$ Hedge (指数权重) 算法，具有完全信息反馈。对每个 $i\\in\\{1,2,3\\}$，初始化权重 $w_i(1)=1$。在每一轮 $t\\in\\{1,\\dots,T\\}$，形成混合策略 $p_i(t)=\\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$。令 $g(t)\\in\\mathbb{R}^3$ 为针对对手已实现的纯行动 $j(t)$ 的反事实收益向量，即 $g_i(t)=A_{i,\\,j(t)}$。期望收益为 $r^{\\mathrm{H}}(t)=\\sum_{i=1}^3 p_i(t)\\,g_i(t)$。通过 $w_i(t+1)=w_i(t)\\,\\exp\\!\\left(\\eta\\,g_i(t)\\right)$ 更新权重，其中学习率 $\\eta=\\sqrt{\\frac{2\\ln n}{T}}$ 且 $n=3$。定义时间平均策略 $\\bar{p}^{\\mathrm{H}}\\in\\mathbb{R}^3$ 为 $\\bar{p}^{\\mathrm{H}}=\\frac{1}{T}\\sum_{t=1}^T p(t)$。\n\n对于这两种学习规则，在时间跨度 $T$ 结束时计算两个汇总统计量：\n\n$1.$ 平均外部遗憾 (Average external regret)，\n$$\n\\frac{R_T}{T}\n\\;=\\;\n\\frac{1}{T}\\left(\n\\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t)\n\\;-\\;\n\\sum_{t=1}^T r(t)\n\\right),\n$$\n其中，对于虚拟对策，$r(t)=r^{\\mathrm{FP}}(t)$；对于 Hedge 算法，$r(t)=r^{\\mathrm{H}}(t)$。在两种情况下，$g_i(t)=A_{i,\\,j(t)}$。\n\n$2.$ 学习者的时间平均策略与唯一混合策略纳什均衡 $u^\\star=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$ 之间的 $\\ell_1$ 距离，即\n$$\nD(\\bar{p})\n\\;=\\;\n\\sum_{i=1}^3 \\left| \\bar{p}_i - \\tfrac{1}{3} \\right|.\n$$\n\n测试套件。您的程序必须在以下四种对手策略安排上评估这两种学习规则，每种安排都由一个时间跨度 $T$ 和一个分段常数纯行动策略指定：\n\n$1.$ 情况 A (理想路径)：$T=90$。对手出 30 轮石头，然后出 30 轮布，再出 30 轮剪刀，即 $j(t)=1$ 对 $t\\in\\{1,\\dots,30\\}$，$j(t)=2$ 对 $t\\in\\{31,\\dots,60\\}$，以及 $j(t)=3$ 对 $t\\in\\{61,\\dots,90\\}$。\n\n$2.$ 情况 B (边界情况)：$T=3$。对手在 $t=1$ 时出石头，在 $t=2$ 时出布，在 $t=3$ 时出剪刀，即 $j(1)=1$，$j(2)=2$，$j(3)=3$。\n\n$3.$ 情况 C (非平稳边缘情况)：$T=60$。对手出 20 轮石头，然后出 10 轮布，再出 20 轮剪刀，最后出 10 轮布，即 $j(t)=1$ 对 $t\\in\\{1,\\dots,20\\}$，$j(t)=2$ 对 $t\\in\\{21,\\dots,30\\}$，$j(t)=3$ 对 $t\\in\\{31,\\dots,50\\}$，以及 $j(t)=2$ 对 $t\\in\\{51,\\dots,60\\}$。\n\n$4.$ 情况 D (平稳边缘情况)：$T=60$。对手在所有轮次中都出石头，即对所有 $t\\in\\{1,\\dots,60\\}$，$j(t)=1$。\n\n要求输出。对于每种情况，生成一个包含四个实数的列表\n$$\n\\big[\\, \\tfrac{R_T^{\\mathrm{H}}}{T},\\;\\tfrac{R_T^{\\mathrm{FP}}}{T},\\; D(\\bar{p}^{\\mathrm{H}}),\\; D(\\bar{p}^{\\mathrm{FP}})\\,\\big],\n$$\n其中上标 $\\mathrm{H}$ 和 $\\mathrm{FP}$ 分别表示 Hedge 和虚拟对策。每个数字必须四舍五入到 $6$ 位小数。您的程序应生成单行输出，其中包含这些四元素列表的逗号分隔列表形式的结果，并用方括号括起来，不含任何额外文本。例如：\n$$\n[\\,[x_1,x_2,x_3,x_4],[y_1,y_2,y_3,y_4],\\dots]\\,,\n$$\n其中每个 $x_k$ 和 $y_k$ 是四舍五入到 $6$ 位小数的十进制数。本问题不涉及物理单位，所有角度均无关。不得有输入；仅为指定的测试套件计算输出。",
            "solution": "该问题是有效的。它提出了一个在博弈论和在线学习领域的明确定义的计算任务，该领域是计算经济学的一个子领域。所有参数、算法和评估指标都以足够的精度进行了指定，从而可以得到唯一且可验证的解。该问题具有科学依据，使用了标准模型（石头剪刀布）、算法（虚拟对策、Hedge）和性能指标（遗憾、与纳什均衡的距离）。\n\n任务是为一个在石头剪刀布博弈中对抗一个确定性的、非平稳的列参与人的行参与人，模拟两种学习算法：虚拟对策 (Fictitious Play) 和 Hedge。我们给出了行参与人的收益矩阵 $A$：\n$$\nA = \\begin{pmatrix} 0 & -1 & 1 \\\\ 1 & 0 & -1 \\\\ -1 & 1 & 0 \\end{pmatrix}\n$$\n其中行和列分别对应行动石头（索引 $1$）、布（索引 $2$）和剪刀（索引 $3$）。模拟将针对四种不同的预定义对手行动策略 $j(t)$ 运行 $T$ 轮。\n\n解法涉及实现这两种算法和指定的汇总统计量。模拟以迭代方式进行，从 $t = 1$ 到 $T$。\n\n**1. 虚拟对策 (Fictitious Play, FP) 算法**\n\n虚拟对策是一种迭代学习规则，其中参与人假设其对手正在使用一个平稳的混合策略，该策略通过对手过去行动的经验频率来估计。然后，该参与人选择对这个估计策略的纯最佳响应。\n\n- **初始化 ($t=1$)：** 学习者被规定出石头，因此 $i(1)=1$。\n- **迭代 ($t \\ge 2$)：**\n    1.  学习者计算对手截至第 $t-1$ 轮的行动经验频率。这构成了信念向量 $\\hat{q}(t-1) \\in \\mathbb{R}^3$，其中每个分量 $\\hat{q}_k(t-1)$ 由 $\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$ 给出。\n    2.  学习者计算其三种纯行动中每一种针对该信念的期望收益。行动 $i$ 的期望收益是 $(A\\hat{q}(t-1))_i$。\n    3.  学习者选择行动 $i(t)$ 以最大化此期望收益：$i(t) \\in \\arg\\max_{i\\in\\{1,2,3\\}} (A\\hat{q}(t-1))_i$。平局通过选择索引最小的行动来打破。\n- **收益：** 第 $t$ 轮实现的收益为 $r^{\\mathrm{FP}}(t) = A_{i(t), j(t)}$。\n\n**2. Hedge (指数权重) 算法**\n\nHedge 是一种用于在线决策的算法，它在一组专家（在此案例中是纯行动）上维护一个概率分布。它根据每个专家的表现来更新这个分布。\n\n- **初始化 ($t=1$)：** 学习者为每个行动设置相等的初始权重：对 $i \\in \\{1, 2, 3\\}$，有 $w_i(1) = 1$。学习率 $\\eta$ 设置为 $\\sqrt{\\frac{2\\ln n}{T}}$，其中 $n=3$。\n- **迭代 ($t = 1, \\dots, T$)：**\n    1.  学习者通过归一化当前权重来形成一个混合策略 $p(t)$：$p_i(t) = \\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$。\n    2.  在对手采取行动 $j(t)$ 后，学习者观察到自己所有行动的反事实收益。这由向量 $g(t) \\in \\mathbb{R}^3$ 给出，其中 $g_i(t) = A_{i, j(t)}$。该向量对应于矩阵 $A$ 的第 $j(t)$ 列。\n    3.  下一轮的权重以乘法方式更新：$w_i(t+1) = w_i(t) \\exp(\\eta g_i(t))$。本可以产生更高收益的行动会获得更大的权重增加。\n- **收益：** 算法在第 $t$ 轮的性能由其在混合策略 $p(t)$ 下的期望收益来衡量，即 $r^{\\mathrm{H}}(t) = \\sum_{i=1}^3 p_i(t) g_i(t)$。\n\n**3. 汇总统计量**\n\n对于每种算法和每个对手策略安排，在时间跨度 $T$ 结束时计算两个指标：\n\n- **平均外部遗憾：** 该指标衡量算法的累积收益与事后最佳单一固定行动的累积收益之间的每轮平均差异。其计算公式为：\n$$\n\\frac{R_T}{T} = \\frac{1}{T}\\left( \\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t) - \\sum_{t=1}^T r(t) \\right)\n$$\n对于虚拟对策，$r(t) = r^{\\mathrm{FP}}(t)$；对于 Hedge 算法，$r(t) = r^{\\mathrm{H}}(t)$。项 $\\sum_{t=1}^T g_i(t)$ 表示通过持续选择行动 $i$ 所能累积的总收益。\n\n- **与纳什均衡的 $\\ell_1$ 距离：** 该指标量化了学习者的时间平均策略与石头剪刀布博弈中唯一的混合策略纳什均衡 $u^\\star = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$ 之间的距离。该距离为：\n$$\nD(\\bar{p}) = \\sum_{i=1}^3 \\left| \\bar{p}_i - \\frac{1}{3} \\right|\n$$\n对于虚拟对策，时间平均策略 $\\bar{p}^{\\mathrm{FP}}$ 是所玩行动的经验频率：$\\bar{p}^{\\mathrm{FP}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$。对于 Hedge 算法，它是每轮使用的混合策略的平均值：$\\bar{p}^{\\mathrm{H}} = \\frac{1}{T}\\sum_{t=1}^T p(t)$。\n\n实现将为四个指定的测试用例模拟这些过程，并为每个用例计算和收集所需的四个统计数据（$R_T^{\\mathrm{H}}/T$、$R_T^{\\mathrm{FP}}/T$、$D(\\bar{p}^{\\mathrm{H}})$、$D(\\bar{p}^{\\mathrm{FP}})$）。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    # Payoff matrix for the row player (learner).\n    # Actions: 0=Rock, 1=Paper, 2=Scissors\n    A = np.array([[0, -1, 1],\n                  [1, 0, -1],\n                  [-1, 1, 0]])\n    n_actions = 3\n    u_star = np.full(n_actions, 1.0 / n_actions)\n\n    def generate_schedule(case, T):\n        \"\"\"Generates the opponent's action schedule for a given case.\"\"\"\n        # Note: actions are 1-indexed in problem, 0-indexed in code.\n        # This function returns 0-indexed actions.\n        if case == 'A':  # T=90\n            return np.concatenate([np.full(30, 0), np.full(30, 1), np.full(30, 2)])\n        elif case == 'B':  # T=3\n            return np.array([0, 1, 2])\n        elif case == 'C':  # T=60\n            return np.concatenate([np.full(20, 0), np.full(10, 1), np.full(20, 2), np.full(10, 1)])\n        elif case == 'D':  # T=60\n            return np.full(60, 0)\n        return None\n\n    def run_fictitious_play(T, opponent_schedule, A):\n        \"\"\"Simulates the Fictitious Play algorithm.\"\"\"\n        opponent_action_counts = np.zeros(n_actions)\n        learner_action_counts = np.zeros(n_actions)\n        total_payoff_fp = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n\n            if t == 0:\n                learner_action_idx = 0  # Rule: i(1)=1 (Rock)\n            else:\n                # Belief is the empirical frequency of opponent's past actions\n                opponent_empirical_freq = opponent_action_counts / t\n                expected_payoffs = A @ opponent_empirical_freq\n                # Best response, ties broken by smallest index (np.argmax default)\n                learner_action_idx = np.argmax(expected_payoffs)\n\n            # Record actions and payoffs\n            learner_action_counts[learner_action_idx] += 1\n            total_payoff_fp += A[learner_action_idx, opponent_action_idx]\n            opponent_action_counts[opponent_action_idx] += 1\n\n        # Calculate summary statistics\n        p_bar_fp = learner_action_counts / T\n        d_fp = np.sum(np.abs(p_bar_fp - u_star))\n\n        return total_payoff_fp, d_fp\n\n    def run_hedge(T, opponent_schedule, A):\n        \"\"\"Simulates the Hedge (Exponential Weights) algorithm.\"\"\"\n        eta = np.sqrt(2 * np.log(n_actions) / T)\n        weights = np.ones(n_actions)\n        \n        sum_p_t = np.zeros(n_actions)\n        total_expected_payoff_h = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            \n            # Form mixed strategy\n            p_t = weights / np.sum(weights)\n            sum_p_t += p_t\n            \n            # Counterfactual payoffs\n            g_t = A[:, opponent_action_idx]\n            \n            # Expected payoff for this round\n            total_expected_payoff_h += p_t @ g_t\n            \n            # Update weights\n            weights *= np.exp(eta * g_t)\n            \n        # Calculate summary statistics\n        p_bar_h = sum_p_t / T\n        d_h = np.sum(np.abs(p_bar_h - u_star))\n\n        return total_expected_payoff_h, d_h\n\n    def calculate_regret(T, total_payoff, opponent_schedule, A):\n        \"\"\"Calculates the average external regret.\"\"\"\n        # Payoff of best fixed action in hindsight\n        total_g = np.zeros(n_actions)\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            total_g += A[:, opponent_action_idx]\n        \n        max_fixed_payoff = np.max(total_g)\n        avg_regret = (max_fixed_payoff - total_payoff) / T\n        return avg_regret\n\n    test_cases = [\n        ('A', 90),\n        ('B', 3),\n        ('C', 60),\n        ('D', 60)\n    ]\n\n    all_results = []\n\n    for case_label, T in test_cases:\n        opponent_schedule = generate_schedule(case_label, T)\n\n        # Run simulations\n        total_payoff_fp, d_fp = run_fictitious_play(T, opponent_schedule, A)\n        total_payoff_h, d_h = run_hedge(T, opponent_schedule, A)\n        \n        # Calculate regrets\n        avg_regret_fp = calculate_regret(T, total_payoff_fp, opponent_schedule, A)\n        avg_regret_h = calculate_regret(T, total_payoff_h, opponent_schedule, A)\n        \n        # Store results in the specified order\n        results = [avg_regret_h, avg_regret_fp, d_h, d_fp]\n        all_results.append(results)\n    \n    # Format the output string as required\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_nums = [f\"{num:.6f}\" for num in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "掌握了基础学习模型的运作方式后，一个自然而深刻的问题是：我们能否利用对手学习过程的可预测性来为自己谋利？这个练习将你的角色从一个学习者转变为一个“成熟的”策略家，旨在最大化地利用一个天真的、遵循虚拟对策规则的对手。 这项富有挑战性的实践将引导你使用动态规划（dynamic programming）这一强大的计算工具，通过反向归纳法来求解最优的剥削策略。这个过程不仅是对一个静态策略的最佳回应，而是对一个完整学习过程的最佳回应，从而揭示了计算博弈论中一个核心且迷人的主题。",
            "id": "2405888",
            "problem": "考虑一个在有限期 $T$ 内进行的重复双人双行动策略式博弈，博弈的离散时期由 $t \\in \\{1,2,\\dots,T\\}$ 索引。参与者 $1$（老练的参与者）从 $\\{0,1\\}$ 中选择行动，并寻求最大化整个博弈期间的阶段收益总和。参与者 $2$（对手）从 $\\{0,1\\}$ 中选择行动，并遵循经典的虚拟对策策略：在每个时期 $t$ 开始前，参与者 $2$ 基于初始先验和参与者 $1$ 过去行动的经验频率，形成关于参与者 $1$ 会选择行动 $0$ 的概率信念，然后对此信念采取最佳应对，以最大化参与者 $2$ 自身的期望阶段收益。参与者 $1$ 知道参与者 $2$ 的学习规则和收益矩阵，并选择行动以最佳地应对该学习过程，同时预测当前行动对参与者 $2$ 未来信念的影响。\n\n单次阶段收益如下。设 $U_1$ 为参与者 $1$ 的 $2 \\times 2$ 收益矩阵， $U_2$ 为参与者 $2$ 的 $2 \\times 2$ 收益矩阵，其中行索引对应参与者 $1$ 的行动，列索引对应参与者 $2$ 的行动。如果在某个时期，参与者 $1$ 选择行动 $a_1 \\in \\{0,1\\}$，参与者 $2$ 选择行动 $a_2 \\in \\{0,1\\}$，那么参与者 $1$ 获得收益 $U_1[a_1,a_2]$，参与者 $2$ 获得收益 $U_2[a_1,a_2]$。\n\n在时期 $t$ 开始时，参与者 $2$ 的虚拟对策信念由下式给出：\n$$\np_t \\equiv \\frac{\\alpha_0 + n_0(t-1)}{\\alpha_0 + \\alpha_1 + (t-1)},\n$$\n其中 $\\alpha_0,\\alpha_1 \\in \\mathbb{N}_0$ 分别是行动 $0$ 和 $1$ 的固定先验伪计数，$n_0(t-1)$ 是参与者 $1$ 在时期 $1$ 到 $t-1$ 期间选择行动 $0$ 的总次数。假设 $\\alpha_0 + \\alpha_1 \\ge 1$，以确保在 $t=1$ 时分母为严格正数。在每个时期 $t$，参与者 $2$ 选择一个最佳应对行动\n$$\na_2(t) \\in \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ \\mathbb{E}[U_2[a_1,b] \\mid a_1 \\sim \\text{Bernoulli}(p_t)] \\right\\} \n= \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ p_t \\cdot U_2[0,b] + (1-p_t)\\cdot U_2[1,b] \\right\\}.\n$$\n如果 `argmax` 不唯一，参与者 $2$ 通过选择较小的行动索引来打破平局，即在平局情况下选择 $a_2(t)=0$。\n\n在每个时期 $t$，事件的顺序如下：\n- 根据先验和参与者 $1$ 的过去行动形成信念 $p_t$。\n- 参与者 $2$ 如上所述，选择 $a_2(t)$ 作为对 $p_t$ 的最佳应对。\n- 参与者 $1$ 选择一个纯行动 $a_1(t) \\in \\{0,1\\}$。\n- 收益 $U_1[a_1(t),a_2(t)]$ 和 $U_2[a_1(t),a_2(t)]$ 得以实现。\n- 计数 $n_0(t)$ 通过 $n_0(t)=n_0(t-1)+\\mathbf{1}\\{a_1(t)=0\\}$ 进行更新。\n\n参与者 $1$ 观察整个历史并自适应地选择行动，以最大化未贴现的总收益\n$$\n\\sum_{t=1}^{T} U_1[a_1(t),a_2(t)],\n$$\n同时预测参与者 $2$ 的信念更新和最佳应对。\n\n你的任务是编写一个程序，对于下述每个测试用例，计算在上述动态下，参与者 $1$ 在整个博弈期 $T$ 内可获得的最大总收益。假设参与者 $1$ 可以在每个时期承诺采取任何纯行动，同时完全预测参与者 $2$ 的虚拟对策行为和打破平局的规则。\n\n问题不涉及物理量或角度，因此不需要单位。所有输出必须是精确的整数。\n\n程序没有输入；相反，你的程序必须在内部使用以下测试套件，其中每个案例指定了 $U_1$，$U_2$，博弈期 $T$ 和先验 $(\\alpha_0,\\alpha_1)$：\n\n- 测试用例 1 (操纵有利)：\n  - $U_1 = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$,\n  - $T = 5$,\n  - $(\\alpha_0,\\alpha_1) = (0,1)$.\n\n- 测试用例 2 (零和匹配硬币)：\n  - $U_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix}$,\n  - $T = 6$,\n  - $(\\alpha_0,\\alpha_1) = (1,1)$.\n\n- 测试用例 3 (初始边界平局，协调博弈)：\n  - $U_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}$,\n  - $T = 3$,\n  - $(\\alpha_0,\\alpha_1) = (1,1)$.\n\n- 测试用例 4 (阈值恰好在先验处)：\n  - $U_1 = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$,\n  - $T = 2$,\n  - $(\\alpha_0,\\alpha_1) = (2,3)$.\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果顺序与测试用例相同，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是测试用例 $i$ 的最大总收益（一个整数）。",
            "solution": "该问题已经过验证并被认为是有效的。这是一个在计算博弈论中定义完善的问题，具体来说，它为一个老练的参与者针对采用虚拟对策学习规则的对手，建立了最优策略模型。该设定在数学上是一致的、自洽的，并且没有科学或逻辑上的缺陷。该问题可以使用动态规划通过算法解决。\n\n该问题要求为参与者 $1$ 找到最优策略，他是一个老练的代理人，在 $T$ 个时期的有限期内面对一个可预测的对手——参与者 $2$。参与者 $1$ 的行动会影响参与者 $2$ 的未来信念，并因此影响其未来的行动。这是一个确定性环境下的序列决策问题，因为参与者 $2$ 的行为是确定性的，这使得它适合用动态规划和逆向归纳法来解决。\n\n设系统在时期 $t \\in \\{1, 2, \\dots, T\\}$ 开始时的状态由参与者 $1$ 的行动历史定义。历史中唯一影响未来动态的方面是参与者 $1$ 选择行动 $0$ 的次数。因此，一个充分的状态表示是序对 $(t, k)$，其中 $t$ 是当前时期编号，$k = n_0(t-1)$ 是参与者 $1$ 在时期 $1$ 到 $t-1$ 中选择行动 $0$ 的次数。参与者 $1$ 选择行动 $1$ 的次数是 $(t-1)-k$。\n\n令 $V(t, k)$ 表示在系统处于状态 $(t, k)$ 的情况下，参与者 $1$ 从时期 $t$ 到博弈期结束时 $T$ 所能获得的最大总收益。我们的目标是计算 $V(1, 0)$，因为在博弈开始时（$t=1$），还没有采取任何行动（$k=0$）。\n\n动态规划的解决方案通过逆向归纳进行，从最后一个时期开始。\n\n基本情况是博弈期结束后的时期。在 $t=T+1$ 时，博弈结束，不再有收益累积。\n$$V(T+1, k) = 0 \\quad \\text{for all valid } k \\in \\{0, 1, \\dots, T\\}.$$\n\n对于任何时期 $t \\in \\{T, T-1, \\dots, 1\\}$ 和任何有效状态 $(t, k)$，其中 $k \\in \\{0, 1, \\dots, t-1\\}$，我们可以写出 $V(t, k)$ 的贝尔曼方程。首先，我们确定参与者 $2$ 的行动 $a_2(t)$，它是状态 $(t, k)$ 的一个确定性函数。参与者 $2$ 关于参与者 $1$ 会选择行动 $0$ 的信念是：\n$$p_t = \\frac{\\alpha_0 + k}{\\alpha_0 + \\alpha_1 + t - 1}.$$\n参与者 $2$ 选择 $a_2(t) \\in \\{0,1\\}$ 以最大化其期望收益。如果选择行动 $0$ 的期望收益大于或等于（由于打破平局的规则）选择行动 $1$ 的期望收益，他们将选择行动 $0$：\n$$ p_t \\cdot U_2[0,0] + (1-p_t)\\cdot U_2[1,0] \\ge p_t \\cdot U_2[0,1] + (1-p_t)\\cdot U_2[1,1]. $$\n为了避免浮点数运算和潜在的精度误差，我们可以使用整数来表示这个不等式。设 $N_t = \\alpha_0 + k$ 和 $D_t = \\alpha_0 + \\alpha_1 + t - 1$，因此 $p_t = N_t / D_t$。由于 $D_t > 0$，我们可以将不等式两边乘以 $D_t$ 并重新整理得到：\n$$ N_t \\cdot (U_2[0,0] - U_2[1,0] - U_2[0,1] + U_2[1,1]) \\ge D_t \\cdot (U_2[1,1] - U_2[1,0]). $$\n这个比较仅涉及整数运算，并完全确定了任何状态 $(t,k)$ 下的 $a_2(t)$。\n\n给定 $a_2(t)$，参与者 $1$ 选择 $a_1(t) \\in \\{0,1\\}$ 以最大化当前时期收益和后续状态值的总和。贝尔曼方程是：\n$$ V(t, k) = \\max \\left\\{\n    \\underbrace{U_1[0, a_2(t)] + V(t+1, k+1)}_{\\text{Value if } a_1(t)=0},\n    \\underbrace{U_1[1, a_2(t)] + V(t+1, k)}_{\\text{Value if } a_1(t)=1}\n\\right\\}. $$\n这里，如果参与者 $1$ 选择行动 $0$，下一个时期 $t+1$ 的零计数变为 $k+1$。如果参与者 $1$ 选择行动 $1$，则计数保持为 $k$。\n\n该算法包括通过从 $T$ 向下迭代到 $1$ 的方式计算所有相关状态的 $V(t, k)$，并且对每个 $t$，从 $0$ 到 $t-1$ 迭代 $k$。可以使用一个二维数组来存储计算出的 $V(t, k)$ 值，这种技术称为记忆化。最终答案是值 $V(1, 0)$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the repeated game problem for a sophisticated player against a \n    fictitious play opponent for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: manipulation beneficial\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 5,\n            \"prior\": (0, 1),\n        },\n        # Case 2: zero-sum matching pennies\n        {\n            \"U1\": np.array([[1, -1], [-1, 1]], dtype=int),\n            \"U2\": np.array([[-1, 1], [1, -1]], dtype=int),\n            \"T\": 6,\n            \"prior\": (1, 1),\n        },\n        # Case 3: boundary tie at the start, coordination\n        {\n            \"U1\": np.array([[1, 0], [0, 0]], dtype=int),\n            \"U2\": np.array([[2, 0], [0, 2]], dtype=int),\n            \"T\": 3,\n            \"prior\": (1, 1),\n        },\n        # Case 4: threshold exactly at the prior\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 2,\n            \"prior\": (2, 3),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        U1 = case[\"U1\"]\n        U2 = case[\"U2\"]\n        T = case[\"T\"]\n        alpha0, alpha1 = case[\"prior\"]\n\n        # V[t][k] stores the max payoff from period t to T, given that Player 1\n        # has played action 0 a total of k times up to period t-1.\n        # Dimensions are (T+2) x (T+1) to handle t=T+1 and k=T.\n        V = np.zeros((T + 2, T + 1), dtype=int)\n\n        # Precompute constants for Player 2's best response to avoid redundant calculations.\n        # P2 plays 0 if: N * D_U2 >= D * C_U2\n        D_U2 = (U2[0, 0] - U2[1, 0]) - (U2[0, 1] - U2[1, 1])\n        C_U2 = U2[1, 1] - U2[1, 0]\n\n        # Backward induction from period T down to 1\n        for t in range(T, 0, -1):\n            # In period t, t-1 rounds have passed, so k (count of '0's) can be from 0 to t-1\n            for k in range(t):\n                # Belief p_t = N/D. We use integer arithmetic to determine P2's action.\n                N = alpha0 + k\n                D = alpha0 + alpha1 + t - 1\n                \n                # Determine Player 2's action a2_t\n                # Tie-breaking rule: P2 chooses action 0 in case of indifference.\n                # This is handled by '>=' in the main comparison.\n                \n                a2_t = 0 # Default action\n                if D_U2 > 0:\n                    if not (N * D_U2 >= D * C_U2):\n                        a2_t = 1\n                elif D_U2  0:\n                    if not (N * D_U2 = D * C_U2):  # Inequality flips\n                        a2_t = 1\n                else: # D_U2 == 0, p_t does not affect choice\n                    if not (0 >= C_U2):\n                        a2_t = 1\n                \n                # Player 1's Bellman equation\n                # Value if P1 chooses action 0\n                val_if_0 = U1[0, a2_t] + V[t + 1, k + 1]\n                \n                # Value if P1 chooses action 1\n                val_if_1 = U1[1, a2_t] + V[t + 1, k]\n                \n                V[t, k] = max(val_if_0, val_if_1)\n        \n        # The result is the value at the beginning of the game (t=1, k=0)\n        results.append(V[1, 0])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}