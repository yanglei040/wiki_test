{
    "hands_on_practices": [
        {
            "introduction": "理论学习的最终目的是解决实际问题。本节的第一个实践将带你进入一个经典的库存管理场景。你将运用动态规划（具体来说是价值迭代法）来为一个公司制定最优的订货策略，其核心是在持有成本、订货成本和缺货惩罚之间找到平衡。这项练习不仅能让你熟练掌握将问题构建为马尔可夫决策过程（MDP）并求解的方法，还会让你接触到一个重要的实用技巧：如何将连续的状态变量（库存水平）离散化为一个可计算的、非均匀的（对数）状态空间 。",
            "id": "2388580",
            "problem": "一家单一产品公司在离散时间内面临随机需求，时间范围为无限期。库存为非负，并以单位计量。该公司在每个时期需求实现之前选择一个离散的订购量。未满足的需求会流失（无缺货积压）。库存的状态空间是对数离散化的，而不是线性离散化的。\n\n模型定义如下。\n\n- 状态空间。库存状态 $s$ 属于一个有限网格 $\\mathcal{S}$，该网格由 $\\{0\\}$ 和一组对数间隔且四舍五入为整数的严格正点构成。给定严格正整数 $s_{\\min}^+$ 和 $s_{\\max}$，其中 $s_{\\min}^+ \\le s_{\\max}$，以及一个正整数 $N_+$，其中 $N_+ \\ge 2$，定义\n$$\n\\tilde{s}_i = \\exp\\!\\left(\\log(s_{\\min}^+) + (i-1)\\frac{\\log(s_{\\max}) - \\log(s_{\\min}^+)}{N_+ - 1}\\right), \\quad i \\in \\{1,\\dots,N_+\\},\n$$\n然后四舍五入为整数 $r_i = \\operatorname{round}(\\tilde{s}_i)$，去掉重复项，并设置 $\\mathcal{S} = \\{0\\} \\cup \\{r_i\\}_{i=1}^{N_+}$，按升序排序。网格 $\\mathcal{S}$ 包含 $0$ 和最高（并包括）$s_{\\max}$ 的严格正整数。\n\n- 行动空间。公司从离散集合 $\\mathcal{A} = \\{0,1,2,\\dots,A_{\\max}\\}$ 中选择一个订购量 $a$，其中 $A_{\\max}$ 是一个给定的正整数。\n\n- 需求。在每个时期，需求 $D$ 是一个离散随机变量，其支持集为非负整数的有限集合 $\\{d_1,\\dots,d_K\\}$，关联概率为 $\\{p_1,\\dots,p_K\\}$，满足 $\\sum_{k=1}^K p_k = 1$。\n\n- 时间顺序和转移。给定当前库存 $s \\in \\mathcal{S}$ 和行动 $a \\in \\mathcal{A}$，需求 $D$ 实现。销售额为 $\\min\\{s+a, D\\}$。离散化之前的期末库存为\n$$\ns'_{\\text{cont}} = \\max\\{s + a - D, 0\\}。\n$$\n下一个状态是通过使用最近邻投影将 $s'_{\\text{cont}}$ 投影到网格 $\\mathcal{S}$ 上获得的\n$$\n\\Pi(s'_{\\text{cont}}) = \\operatorname*{arg\\,min}_{g \\in \\mathcal{S}} |g - s'_{\\text{cont}}|,\n$$\n平局则倾向于较低的网格点。\n\n- 单位周期成本。瞬时成本为\n$$\ng(s,a,D) = K \\cdot \\mathbf{1}\\{a > 0\\} + c\\,a + h\\,s'_{\\text{cont}} + p\\,\\max\\{D - (s+a), 0\\},\n$$\n其中 $K$ 是固定订购成本，$c$ 是单位订购成本，$h$ 是期末库存 $s'_{\\text{cont}}$ 的单位持有成本，$p$ 是单位销售流失惩罚。\n\n- 目标。对于折扣因子 $\\beta \\in (0,1)$，公司寻求一个平稳策略，以最小化在所有可测策略（这些策略选择 $a_t \\in \\mathcal{A}$ 作为 $s_t \\in \\mathcal{S}$ 的函数）上的预期无限期折扣成本 $\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t g(s_t,a_t,D_t)\\right]$。\n\n您的任务是实现一个程序，对于下面的测试套件中的每个参数集，计算在上述精确离散模型（包括投影 $\\Pi(\\cdot)$）下，初始库存 $s_0 = 0$ 时的最优平稳行动。所需的输出是整数形式的最优行动。\n\n测试套件。对于每个测试用例，请精确使用给定的参数。所有指定为数字的量都是无量纲的。\n\n- 测试用例 $1$（一般情况）：\n  - 折扣因子 $\\beta = 0.95$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 50$, $N_+ = 10$。\n  - 行动上限 $A_{\\max} = 20$。\n  - 成本 $K = 5$, $c = 1$, $h = 0.1$, $p = 2$。\n  - 需求支持集和概率：$\\{0,1,2,3,4\\}$，概率为 $\\{0.1,0.2,0.3,0.25,0.15\\}$。\n\n- 测试用例 $2$（无固定订购成本，惩罚更高）：\n  - 折扣因子 $\\beta = 0.95$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 30$, $N_+ = 8$。\n  - 行动上限 $A_{\\max} = 15$。\n  - 成本 $K = 0$, $c = 1$, $h = 0.5$, $p = 5$。\n  - 需求支持集和概率：$\\{0,2,5,8\\}$，概率为 $\\{0.2,0.3,0.3,0.2\\}$。\n\n- 测试用例 $3$（零需求边缘情况）：\n  - 折扣因子 $\\beta = 0.9$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 20$, $N_+ = 6$。\n  - 行动上限 $A_{\\max} = 10$。\n  - 成本 $K = 3$, $c = 1$, $h = 0.2$, $p = 4$。\n  - 需求支持集和概率：$\\{0\\}$，概率为 $\\{1.0\\}$。\n\n- 测试用例 $4$（确定性高需求，销售流失惩罚强）：\n  - 折扣因子 $\\beta = 0.9$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 40$, $N_+ = 12$。\n  - 行动上限 $A_{\\max} = 20$。\n  - 成本 $K = 1$, $c = 1$, $h = 0.1$, $p = 10$。\n  - 需求支持集和概率：$\\{10\\}$，概率为 $\\{1.0\\}$。\n\n最终输出格式。您的程序应生成单行输出，其中包含测试用例 1 到 4 在初始库存 $s_0 = 0$ 时的最优行动，按顺序排列，格式为方括号括起来的逗号分隔列表（例如，$\\texttt{[a_1,a_2,a_3,a_4]}$）。所需的输出是整数。",
            "solution": "在尝试求解之前，需对问题进行验证。\n\n### 步骤1：提取给定信息\n\n- **状态空间 $\\mathcal{S}$**：库存水平的有限网格。它由一个包含 $N_+ \\ge 2$ 个严格正整数的主集合 $\\{r_i\\}_{i=1}^{N_+}$ 和状态 $\\{0\\}$ 构成。点 $r_i$ 是在 $s_{\\min}^+  0$ 和 $s_{\\max} \\ge s_{\\min}^+$ 之间对数间隔的点 $\\tilde{s}_i$ 的整数舍入值。对于 $i \\in \\{1,\\dots,N_+\\}$，$\\tilde{s}_i$ 的公式为：\n$$\n\\tilde{s}_i = \\exp\\!\\left(\\log(s_{\\min}^+) + (i-1)\\frac{\\log(s_{\\max}) - \\log(s_{\\min}^+)}{N_+ - 1}\\right)\n$$\n网格的正数部分为 $\\{r_i = \\operatorname{round}(\\tilde{s}_i)\\}_{i=1}^{N_+}$，并移除了重复项。完整的状态空间是 $\\mathcal{S} = \\{0\\} \\cup \\{r_i\\}$，按升序排列。\n\n- **行动空间 $\\mathcal{A}$**：可能的订购量集合，由 $\\mathcal{A} = \\{0,1,2,\\dots,A_{\\max}\\}$ 给出，其中 $A_{\\max}$ 是给定的正整数。\n\n- **需求 $D$**：一个离散随机变量，其有限支持集为 $\\{d_1,\\dots,d_K\\}$，相应的概率为 $\\{p_1,\\dots,p_K\\}$，其中 $\\sum_{k=1}^K p_k = 1$。\n\n- **状态转移**：给定状态 $s \\in \\mathcal{S}$ 和行动 $a \\in \\mathcal{A}$，投影前的期末库存水平为 $s'_{\\text{cont}} = \\max\\{s + a - D, 0\\}$。下一个状态 $s' \\in \\mathcal{S}$ 是通过使用最近邻投影将 $s'_{\\text{cont}}$ 投影到网格 $\\mathcal{S}$ 上来确定的：\n$$\ns' = \\Pi(s'_{\\text{cont}}) = \\operatorname*{arg\\,min}_{g \\in \\mathcal{S}} |g - s'_{\\text{cont}}|\n$$\n平局通过选择较小的网格点来解决。\n\n- **单位周期成本 $g(s,a,D)$**：单个周期内产生的成本为：\n$$\ng(s,a,D) = K \\cdot \\mathbf{1}\\{a  0\\} + c\\,a + h\\,s'_{\\text{cont}} + p\\,\\max\\{D - (s+a), 0\\}\n$$\n其中 $K$ 是固定订购成本，$c$ 是单位订购成本，$h$ 是单位持有成本，$p$ 是单位销售流失惩罚。\n\n- **目标**：以折扣因子 $\\beta \\in (0,1)$ 最小化无限期预期折扣成本 $\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t g(s_t,a_t,D_t)\\right]$。目标是找到一个平稳最优策略 $\\pi^*: \\mathcal{S} \\to \\mathcal{A}$。\n\n- **任务**：对于四个不同的参数集（测试用例），确定初始状态 $s_0=0$ 时的最优平稳行动 $a^*$。\n\n### 步骤2：使用提取的给定信息进行验证\n\n- **科学依据**：该问题是单一商品库存控制问题在不确定性下的标准表述，这是运筹学和计算经济学中的一个经典课题。使用离散状态空间（特别是对数网格）和离散行动是使连续状态问题在计算上易于处理的常用且有效技术。该模型基于动态规划的成熟原理。它在科学上是合理的。\n\n- **适定性**：该问题是一个有限状态、有限行动、无限期折扣动态规划。对于非负参数，单位周期成本 $g(s,a,D)$ 是非负的，因此有下界。折扣因子 $\\beta$ 严格介于 $0$ 和 $1$ 之间。在这些条件下，贝尔曼算子是一个收缩映射。动态规划中的标准定理（例如，来自 Blackwell 的定理）保证了存在一个唯一的有界价值函数，它是贝尔曼算子的不动点，并且存在一个平稳最优策略。价值迭代等算法保证收敛到这个唯一解。该问题是适定的。\n\n- **客观性**：所有变量、参数和函数都以数学精度定义。语言正式且无歧义。该问题是客观的。\n\n### 步骤3：结论与行动\n\n问题陈述有效。它具有科学依据、适定性、客观性，并具有完整且一致的设置。将提供解决方案。\n\n### 解决方案\n\n所描述的问题是一个无限期折扣成本随机动态规划问题。最优策略通过求解贝尔曼方程找到，该方程刻画了每个状态 $s \\in \\mathcal{S}$ 的最小期望成本，即价值函数 $V(s)$。价值函数必须满足：\n$$\nV(s) = \\min_{a \\in \\mathcal{A}} \\left\\{ \\mathbb{E}_D[g(s,a,D)] + \\beta \\mathbb{E}_D[V(\\Pi(\\max\\{s+a-D, 0\\}))] \\right\\}\n$$\n该方程表明，处于状态 $s$ 的价值是所有可能行动 $a$ 的即期期望成本与折扣后未来期望成本之和的最小值。期望 $\\mathbb{E}_D[\\cdot]$ 是对需求 $D$ 的分布取的。\n\n让我们将状态-行动价值函数（或称 $Q$ 函数）定义为最小化中的项：\n$$\nQ(s,a) = \\mathbb{E}_D[g(s,a,D)] + \\beta \\mathbb{E}_D[V(\\Pi(\\max\\{s+a-D, 0\\}))]\n$$\n贝尔曼方程可以写成 $V(s) = \\min_{a \\in \\mathcal{A}} Q(s,a)$。\n\n该问题使用**价值迭代**算法求解，该算法保证收敛到最优价值函数 $V^*$，因为对于 $\\beta \\in (0,1)$，贝尔曼算子是一个收缩映射。算法过程如下：\n\n1.  **初始化**：\n    - 根据指定的对数网格生成过程构建离散状态空间 $\\mathcal{S}$。设 $N_s = |\\mathcal{S}|$。\n    - 初始化价值函数向量 $V_0 \\in \\mathbb{R}^{N_s}$，通常对于所有 $s \\in \\mathcal{S}$ 设置 $V_0(s) = 0$。\n    - 设置迭代计数器 $n=0$ 和一个小的收敛容差 $\\epsilon  0$。\n\n2.  **迭代**：对 $n=0, 1, 2, \\dots$ 重复：\n    - 对于每个状态 $s \\in \\mathcal{S}$，通过使用上一迭代的价值函数 $V_n$ 求解单步贝尔曼方程来计算更新后的值 $V_{n+1}(s)$：\n    $$\n    V_{n+1}(s) = \\min_{a \\in \\mathcal{A}} \\left\\{ \\mathbb{E}_D[g(s,a,D)] + \\beta \\sum_{k=1}^K p_k V_n\\left(\\Pi(\\max\\{s+a-d_k, 0\\})\\right) \\right\\}\n    $$\n    状态-行动对 $(s,a)$ 的即期期望成本为：\n    $$\n    \\mathbb{E}_D[g(s,a,D)] = K \\cdot \\mathbf{1}\\{a  0\\} + c\\,a + \\sum_{k=1}^K p_k \\left( h\\,\\max\\{s+a-d_k, 0\\} + p\\,\\max\\{d_k-(s+a), 0\\} \\right)\n    $$\n    - 在迭代 $n$ 时的平稳策略 $\\pi_n(s)$ 是为每个状态 $s$ 实现最小值的行动 $a$。\n\n3.  **终止**：当价值函数收敛时，迭代停止。这通过检查连续价值函数之间差异的上确界范数是否小于容差来确定：\n    $$\n    \\max_{s \\in \\mathcal{S}} |V_{n+1}(s) - V_n(s)|  \\epsilon\n    $$\n    一个足够小的容差，例如 $\\epsilon=10^{-8}$，确保所得策略是最优的。\n\n4.  **结果**：收敛后，算法得出最优价值函数 $V^*$ 和最优平稳策略 $\\pi^*$。问题要求在特定初始状态 $s_0=0$ 时的最优行动，即 $\\pi^*(0)$。\n\n计算实现涉及每个测试用例的以下步骤：\n\na.  一个函数生成排序后的状态网格 $\\mathcal{S}$ 以及从状态值到其索引的映射，以实现高效查找。\nb.  实现一个投影函数 $\\Pi(x)$。给定一个值 $x$ 和排序后的网格 $\\mathcal{S}$，它使用 `numpy.argmin(numpy.abs(S - x))` 找到最近网格点的索引。平局规则（取较小值）由 `argmin` 自动处理，因为它返回最小值的第一个索引。\nc.  执行主价值迭代循环。在此循环内，嵌套循环遍历每个状态 $s \\in \\mathcal{S}$ 和每个行动 $a \\in \\mathcal{A}$。对于每个 $(s,a)$ 对，一个更深层的循环遍历需求结果 $\\{d_k\\}$ 来计算完整的 $Q(s,a)$ 值。\nd.  在为给定状态找到所有行动的 $Q$ 值后，将最小值存储为 $V_{n+1}(s)$，并将相应的行动存储为该状态的当前最佳策略。\ne.  遍历所有状态后，检查收敛标准。如果未满足，则将 $V_n$ 更新为 $V_{n+1}$ 并重复该过程。\nf.  一旦达到收敛，从最终策略数组中提取状态 $s=0$ 的最优行动。对所有提供的测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the inventory management problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"beta\": 0.95, \"s_min_plus\": 1, \"s_max\": 50, \"N_plus\": 10,\n            \"A_max\": 20, \"K\": 5, \"c\": 1, \"h\": 0.1, \"p\": 2,\n            \"demand_support\": [0, 1, 2, 3, 4],\n            \"demand_probs\": [0.1, 0.2, 0.3, 0.25, 0.15]\n        },\n        {\n            \"beta\": 0.95, \"s_min_plus\": 1, \"s_max\": 30, \"N_plus\": 8,\n            \"A_max\": 15, \"K\": 0, \"c\": 1, \"h\": 0.5, \"p\": 5,\n            \"demand_support\": [0, 2, 5, 8],\n            \"demand_probs\": [0.2, 0.3, 0.3, 0.2]\n        },\n        {\n            \"beta\": 0.9, \"s_min_plus\": 1, \"s_max\": 20, \"N_plus\": 6,\n            \"A_max\": 10, \"K\": 3, \"c\": 1, \"h\": 0.2, \"p\": 4,\n            \"demand_support\": [0], \"demand_probs\": [1.0]\n        },\n        {\n            \"beta\": 0.9, \"s_min_plus\": 1, \"s_max\": 40, \"N_plus\": 12,\n            \"A_max\": 20, \"K\": 1, \"c\": 1, \"h\": 0.1, \"p\": 10,\n            \"demand_support\": [10], \"demand_probs\": [1.0]\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        optimal_action = compute_optimal_policy(**params)\n        results.append(optimal_action)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef create_state_grid(s_min_plus, s_max, N_plus):\n    \"\"\"\n    Generates the logarithmically spaced state grid.\n    \"\"\"\n    if s_min_plus > s_max:\n        raise ValueError(\"s_min_plus must be less than or equal to s_max\")\n    if N_plus  2:\n        raise ValueError(\"N_plus must be 2 or greater\")\n\n    log_s_min = np.log(s_min_plus)\n    log_s_max = np.log(s_max)\n    \n    # Generate log-spaced points\n    grid_points = np.exp(log_s_min + (np.arange(N_plus)) * (log_s_max - log_s_min) / (N_plus - 1))\n    \n    # Round to nearest integer\n    rounded_points = np.round(grid_points).astype(int)\n    \n    # Remove duplicates and form the final grid with 0\n    positive_grid = sorted(list(set(rounded_points)))\n    \n    final_grid = np.array([0] + positive_grid)\n    \n    return final_grid\n\ndef compute_optimal_policy(beta, s_min_plus, s_max, N_plus, A_max, K, c, h, p, demand_support, demand_probs):\n    \"\"\"\n    Computes the optimal policy for a given set of parameters using value iteration.\n    \"\"\"\n    \n    # 1. Setup\n    S = create_state_grid(s_min_plus, s_max, N_plus)\n    s_to_idx = {val: i for i, val in enumerate(S)}\n    \n    A = np.arange(A_max + 1)\n    D = np.array(demand_support)\n    P_d = np.array(demand_probs)\n    \n    V = np.zeros(len(S))\n    policy = np.zeros(len(S), dtype=int)\n    tol = 1e-8\n    max_iter = 5000\n\n    def project(x_cont, grid):\n        \"\"\" Projects a continuous value onto the discrete grid S. \"\"\"\n        idx = np.argmin(np.abs(grid - x_cont))\n        return grid[idx]\n\n    # Pre-calculate components of one-period cost that depend only on (s,a,D)\n    # This can be vectorized for efficiency.\n    s_plus_a_tensor = S[:, np.newaxis] + A[np.newaxis, :] # Shape (|S|, |A|)\n    s_cont_prime_tensor = np.maximum(s_plus_a_tensor[:, :, np.newaxis] - D[np.newaxis, np.newaxis, :], 0)\n    \n    holding_costs_exp = h * np.sum(P_d * s_cont_prime_tensor, axis=2)\n    penalty_costs_exp = p * np.sum(P_d * np.maximum(D - s_plus_a_tensor[:, :, np.newaxis], 0), axis=2)\n    \n    fixed_costs = K * (A > 0)\n    unit_costs = c * A\n    \n    E_g = fixed_costs[np.newaxis, :] + unit_costs[np.newaxis, :] + holding_costs_exp + penalty_costs_exp\n    \n    # 2. Value Iteration\n    for _ in range(max_iter):\n        V_new = np.copy(V)\n        \n        # Calculate expected future value term\n        E_V = np.zeros((len(S), len(A)))\n        for s_idx, s in enumerate(S):\n            for a_idx, a in enumerate(A):\n                future_value = 0\n                for d_val, p_val in zip(D, P_d):\n                    s_cont_prime = max(s + a - d_val, 0)\n                    s_prime = project(s_cont_prime, S)\n                    s_prime_idx = s_to_idx[s_prime]\n                    future_value += p_val * V[s_prime_idx]\n                E_V[s_idx, a_idx] = future_value\n\n        Q_values = E_g + beta * E_V\n        \n        V_new = np.min(Q_values, axis=1)\n        policy = A[np.argmin(Q_values, axis=1)]\n\n        # 3. Check convergence\n        if np.max(np.abs(V - V_new))  tol:\n            break\n        \n        V = V_new\n\n    # 4. Return optimal action for s=0\n    return policy[0]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "在许多现实世界的场景中，我们无法预先知道环境的完整模型（如状态转移概率）。这时，强化学习（RL）便显示出其强大的威力。接下来的练习将指导你构建一个简单的算法交易智能体，它通过与环境的互动来学习交易策略，而无需预知市场价格的“规则” 。这个实践将理论与应用相结合，通过实现经典的Q学习算法，让你亲身体验智能体如何从“试错”中学习，并理解状态-动作值函数 $Q(s, a)$ 在决策中的核心作用。",
            "id": "2388619",
            "problem": "你的任务是实现一个完整的、可运行的程序，该程序使用离散状态和动作空间来训练一个表格型强化学习（RL）智能体，使其仅基于相对强弱指数（RSI）这一技术指标来交易单一股票。该程序必须遵循马尔可夫决策过程（MDP）的形式化，并使用从贝尔曼最优方程推导出的第一性原理来优化行为，本声明中不给出捷径公式。\n\n环境定义如下。智能体观察到的状态是由 RSI 区间和其当前头寸组成的一对。RSI 区间是根据价格序列计算得出的，它是一个关于回看窗口 $w$ 的函数，使用最近 $w$ 个单周期价格变化的平均收益和平均损失的标准定义：\n- 设价格序列为 $\\{P_t\\}_{t=0}^{T-1}$，其中 $T \\geq w + 2$，并定义单步价格变化 $\\Delta_t = P_t - P_{t-1}$，对于 $t \\in \\{1, \\dots, T-1\\}$。\n- 对于每个时间 $t$ 且 $t \\geq w$，将平均收益定义为在 $k \\in \\{t-w+1, \\dots, t\\}$ 范围内 $\\max(\\Delta_k, 0)$ 的算术平均值，将平均损失定义为在同一窗口内 $\\max(-\\Delta_k, 0)$ 的算术平均值。\n- 定义相对强度为 $RS_t = \\dfrac{\\text{平均收益}}{\\text{平均损失}}$，并采用以下约定以确保数学上的良定义性：如果平均损失为 $0$ 且平均收益为严格正数，则设 $RS_t = +\\infty$，这意味着相对强弱指数（RSI）$RSI_t = 100$；如果平均收益为 $0$ 且平均损失为严格正数，则设 $RSI_t = 0$；如果两个平均值都为 $0$，则设 $RSI_t = 50$。否则，当两个平均值都为正数时，设 $RSI_t = 100 - \\dfrac{100}{1 + RS_t}$。\n- 使用阈值 $30$ 和 $70$ 将 RSI 离散化为多个区间，如下所示：如果 $RSI_t \\leq 30$，则为超卖（Oversold）；如果 $30  RSI_t  70$，则为中性（Neutral）；如果 $RSI_t \\geq 70$，则为超买（Overbought）。\n\n在时间 $t$ 的完整离散状态是 $s_t = (r_t, p_t)$，其中 $r_t \\in \\{\\text{超卖}, \\text{中性}, \\text{超买}\\}$ 是 RSI 区间，而 $p_t \\in \\{0,1\\}$ 是当前头寸（0 为空仓，1 为持有一个单位的多头）。离散动作空间是 $A = \\{\\text{持有}, \\text{买入}, \\text{卖出}\\}$。\n\n周期内动态如下：\n- 在每个时间 $t$ 且 $t \\in \\{w, \\dots, T-2\\}$，智能体观察 $s_t$，选择一个动作 $a_t \\in A$，其头寸确定性地更新：\n  - 如果 $a_t = \\text{买入}$，则 $p_{t+} = 1$。\n  - 如果 $a_t = \\text{卖出}$，则 $p_{t+} = 0$。\n  - 如果 $a_t = \\text{持有}$，则 $p_{t+} = p_t$。\n- 仅当 $p_{t+} \\neq p_t$ 时（即只有当头寸实际发生变化时），才会产生交易成本 $c \\geq 0$。\n- 单步奖励为 $r_t = p_{t+}\\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\}\\, c$。\n- 下一个状态 $s_{t+1}$ 使用下一个 RSI 区间和更新后的头寸 $p_{t+}$。\n\n回合（Episodes）定义为从 $p_w = 0$ 开始，对时间索引 $t \\in \\{w, \\dots, T-2\\}$ 的一次从左到右的遍历。学习智能体必须实现源自贝尔曼最优原理和样本备份的表格型 $Q$ 学习。你必须使用一个 $\\epsilon$-贪心行为策略，在探索时对三个动作进行均匀随机探索，在利用时通过选择使当前动作价值估计最大化的任何动作来进行贪心动作选择。以确定性的方式通过选择索引最小的动作来解决平局。对所有随机化过程使用固定的伪随机种子 $12345$，以确保结果是可复现的。\n\n在训练指定数量的回合后，在用于训练的相同价格路径上评估贪心策略（在评估期间设置 $\\epsilon = 0$），从 $p_w = 0$ 开始，对 $t \\in \\{w, \\dots, T-2\\}$ 使用相同的奖励构造。在评估结束时，如果最终头寸为 $1$，则通过卖出进行强制平仓，并减去一次交易成本 $c$；平仓时没有额外的价格变化。测试用例的评估指标是最终累计财富，它等于评估期间的奖励总和加上结束时可能的强制平仓成本。不涉及任何物理单位。\n\n你的程序必须实现上述内容，并运行以下测试套件。对于每个测试用例，你将获得价格路径 $\\{P_t\\}$、窗口长度 $w$、学习率 $\\alpha \\in (0,1]$、折扣因子 $\\gamma \\in [0,1]$、探索率 $\\epsilon \\in [0,1]$、训练回合数 $E \\in \\mathbb{N}$ 和交易成本 $c \\geq 0$。\n\n测试套件：\n- 案例 1（带有小幅回撤的趋势，顺利路径）：价格 $[100, 101, 102, 101, 103, 105, 104, 106, 108, 110]$，$w = 3$，$\\alpha = 0.3$，$\\gamma = 0.9$，$\\epsilon = 0.1$，$E = 200$，$c = 0.05$。\n- 案例 2（横盘且多噪声，边界 $\\gamma = 0$）：价格 $[100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2]$，$w = 3$，$\\alpha = 0.5$，$\\gamma = 0.0$，$\\epsilon = 0.2$，$E = 300$，$c = 0.05$。\n- 案例 3（从一开始就贪心，无探索）：价格 $[10, 11, 12, 13, 14, 15]$，$w = 2$，$\\alpha = 0.3$，$\\gamma = 0.9$，$\\epsilon = 0.0$，$E = 50$，$c = 0.1$。\n- 案例 4（边缘 RSI 区间，包括 $0$ 和 $100$）：价格 $[50, 49, 48, 47, 46, 47, 48, 49, 50, 49]$，$w = 3$，$\\alpha = 0.4$，$\\gamma = 0.8$，$\\epsilon = 0.15$，$E = 250$，$c = 0.05$。\n\n实现要求：\n- 状态空间是离散的，由 3 个 RSI 区间和 2 个头寸状态的笛卡尔积构成，恰好有 6 个状态。\n- 动作空间是离散的，恰好有 3 个动作，顺序为 $\\{\\text{持有}, \\text{买入}, \\text{卖出}\\}$，索引为 $\\{0, 1, 2\\}$。\n- 将所有动作价值 $Q(s,a)$ 初始化为 0。\n- 对所有随机性使用相同的固定伪随机种子 $12345$。\n- 评估为上述 4 个案例中的每一个生成最终财富。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[x_1,x_2,x_3,x_4]$。\n- 每个 $x_i$ 必须是一个四舍五入到恰好 6 位小数的浮点数。\n\n你的任务：完全按照规定实现程序，使其按顺序运行这 4 个案例并打印上述单行描述。不需要用户输入，也不允许使用外部文件。实现应是自包含且纯算法的。确保所有随机化都使用固定的种子 $12345$，以便结果是可复现的。",
            "solution": "所提出的问题是强化学习在金融交易场景中的一个形式严谨的应用。它在科学上是合理的，内部一致，并且有足够的细节来支持一个唯一的算法解决方案。因此，我们将着手对其进行形式化分析和实现。\n\n问题是训练一个智能体，使用从 $Q$ 学习算法派生的策略来交易单一股票。环境被建模为一个有限状态马尔可夫决策过程（MDP），这对于离散性质的状态和动作是合适的。一个 MDP 由一个元组 $(S, A, P, R, \\gamma)$ 正式定义，其中：\n- $S$ 是状态集。\n- $A$ 是动作集。\n- $P$ 是状态转移概率函数，$P(s'|s, a) = \\text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)$。\n- $R$ 是奖励函数，$R(s, a, s') = \\mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$。\n- $\\gamma \\in [0, 1]$ 是未来奖励的折扣因子。\n\n在这个具体问题中，这些组件定义如下：\n\n状态空间 $S$ 是相对强弱指数（RSI）区间集和可能头寸集的笛卡尔积。设 RSI 区间为 $R_{reg} = \\{\\text{超卖}, \\text{中性}, \\text{超买}\\}$，头寸为 $P_{pos} = \\{0, 1\\}$。时间 $t$ 的状态是 $s_t = (r_t, p_t) \\in R_{reg} \\times P_{pos}$。这导致了 $|S| = 3 \\times 2 = 6$ 个不同的状态。\n\n动作空间是 $A = \\{\\text{持有}, \\text{买入}, \\text{卖出}\\}$，这是一个包含 3 个离散动作的集合。\n\n状态转移由智能体的动作和外生价格序列 $\\{P_t\\}_{t=0}^{T-1}$ 决定。给定一个状态 $s_t = (r_t, p_t)$ 和一个动作 $a_t$，智能体的头寸确定性地转移到一个新头寸 $p_{t+}$。下一个 RSI 区间 $r_{t+1}$ 由截至时间 $t+1$ 的价格序列决定。因此，下一个状态是 $s_{t+1} = (r_{t+1}, p_{t+})$。由于价格序列是固定的，对于任何给定的动作，转移动态都是确定性的。\n\n在时间 $t$，从状态 $s_t=(r_t, p_t)$ 采取动作 $a_t$ 的奖励函数，由价格变化带来的盈利或亏损给出，并根据交易成本进行调整。动作后的头寸 $p_{t+}$ 决定了对价格变化 $P_{t+1} - P_t$ 的敞口。奖励 $r_t$ 是：\n$$r_t = p_{t+} \\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\} \\cdot c$$\n其中 $c$ 是交易成本，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n智能体的目标是学习一个最优策略 $\\pi^*: S \\to A$，以最大化从任何给定状态开始的预期累积折扣奖励。这是通过学习最优动作价值函数 $Q^*(s, a)$ 来实现的，它代表了在状态 $s$ 中采取动作 $a$ 并在此后以最优方式行动所能获得的最大预期折扣未来奖励。$Q^*$ 函数满足贝尔曼最优方程：\n$$Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a' \\in A} Q^*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]$$\n对于这个问题中的确定性转移（给定价格路径），期望算子是多余的。对于由动作 $a_t$ 引起的从 $s_t$ 到 $s_{t+1}$ 的转移，伴随着即时奖励 $r_t$，贝尔曼方程简化为：\n$$Q^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in A} Q^*(s_{t+1}, a')$$\n$Q$ 学习是一种无模型的时序差分（TD）控制算法，它迭代地逼近 $Q^*(s, a)$。智能体不需要先验地知道转移或奖励函数。它从样本转移 $(s_t, a_t, r_t, s_{t+1})$ 中学习。在每次这样的转移之后，动作价值表中的条目 $Q(s_t, a_t)$ 会根据以下规则更新：\n$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a' \\in A} Q(s_{t+1}, a') - Q(s_t, a_t) \\right)$$\n这里，$\\alpha \\in (0, 1]$ 是学习率，它控制更新的步长。项 $r_t + \\gamma \\max_{a'} Q(s_{t+1}, a')$ 是 TD 目标，它作为贝尔曼方程右侧的基于样本的估计。\n\n为了确保对状态-动作空间的充分探索，必须选择一个允许非贪心动作的行为策略。指定的 $\\epsilon$-贪心策略通过以概率 $\\epsilon$ 选择一个随机动作和以概率 $1-\\epsilon$ 选择贪心动作（最大化当前 $Q(s, \\cdot)$ 的动作）来实现这一点。\n\n实现将按以下步骤进行：\n1.  将根据指定公式预先计算给定价格序列的 RSI 值，包括对零值平均收益或损失的约定。然后，这些连续的 RSI 值将被映射到三个离散区间。\n2.  状态空间和动作空间将被映射为整数索引，以便与 NumPy 数组高效地配合使用，该数组将表示 $Q$ 表，并初始化为全零。具体来说，一个状态 $(r, p)$，其中 $r \\in \\{0, 1, 2\\}$ 且 $p \\in \\{0, 1\\}$，将被映射到一个索引 $i_s = r \\cdot 2 + p$。动作将被索引为 $0, 1, 2$。$Q$ 表的大小将为 $6 \\times 3$。\n3.  训练过程将迭代指定的 $E$ 个回合。每个回合包括对价格序列的有效时间步（从 $t=w$ 到 $t=T-2$）进行一次遍历。\n4.  在每个回合内，智能体从零头寸开始。在每个时间步 $t$，它观察状态 $s_t$，通过 $\\epsilon$-贪心策略选择一个动作 $a_t$，观察奖励 $r_t$ 和下一个状态 $s_{t+1}$，并使用 $Q$ 学习规则更新 $Q$ 表。\n5.  训练完成后，评估学习到的策略。评估遵循相同的时间轨迹，但 $\\epsilon=0$，意味着智能体总是根据学习到的 $Q$ 值采取贪心行动。在此次遍历中累积总财富。如果在评估期结束时智能体持有头寸，则应用最终的平仓成本。最终的累计财富是性能指标。\n\n所有随机化都由一个固定的种子控制，以确保可复现性，这是科学验证的强制要求。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the Q-learning trading agent problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"prices\": [100.0, 101.0, 102.0, 101.0, 103.0, 105.0, 104.0, 106.0, 108.0, 110.0],\n            \"w\": 3, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.1, \"E\": 200, \"c\": 0.05\n        },\n        {\n            \"prices\": [100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2],\n            \"w\": 3, \"alpha\": 0.5, \"gamma\": 0.0, \"epsilon\": 0.2, \"E\": 300, \"c\": 0.05\n        },\n        {\n            \"prices\": [10.0, 11.0, 12.0, 13.0, 14.0, 15.0],\n            \"w\": 2, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.0, \"E\": 50, \"c\": 0.1\n        },\n        {\n            \"prices\": [50.0, 49.0, 48.0, 47.0, 46.0, 47.0, 48.0, 49.0, 50.0, 49.0],\n            \"w\": 3, \"alpha\": 0.4, \"gamma\": 0.8, \"epsilon\": 0.15, \"E\": 250, \"c\": 0.05\n        }\n    ]\n\n    results = []\n    \n    # --- Mappings ---\n    # RSI Regimes: {0: Oversold, 1: Neutral, 2: Overbought}\n    # Positions: {0: Flat, 1: Long}\n    # States: (rsi_regime, position_state) -> rsi_regime * 2 + position_state\n    # Actions: {0: Hold, 1: Buy, 2: Sell}\n    NUM_STATES = 6\n    NUM_ACTIONS = 3\n    \n    for case in test_cases:\n        prices_np = np.array(case[\"prices\"], dtype=np.float64)\n        w = case[\"w\"]\n        alpha = case[\"alpha\"]\n        gamma = case[\"gamma\"]\n        epsilon = case[\"epsilon\"]\n        E = case[\"E\"]\n        c = case[\"c\"]\n        T = len(prices_np)\n\n        # --- 1. Pre-calculate RSI and Regimes ---\n        rsi_values = np.full(T, np.nan)\n        deltas = prices_np[1:] - prices_np[:-1]\n        \n        for t in range(w, T):\n            window_deltas = deltas[t-w:t]\n            gains = np.maximum(window_deltas, 0)\n            losses = np.maximum(-window_deltas, 0)\n            \n            avg_gain = np.mean(gains)\n            avg_loss = np.mean(losses)\n\n            if avg_loss == 0:\n                if avg_gain == 0:\n                    rsi_values[t] = 50.0\n                else:\n                    rsi_values[t] = 100.0\n            else:\n                rs = avg_gain / avg_loss\n                rsi_values[t] = 100.0 - (100.0 / (1.0 + rs))\n\n        rsi_regimes = np.full(T, -1, dtype=int)\n        rsi_regimes[rsi_values = 30] = 0  # Oversold\n        rsi_regimes[(rsi_values > 30)  (rsi_values  70)] = 1 # Neutral\n        rsi_regimes[rsi_values >= 70] = 2  # Overbought\n\n        # --- 2. Training Phase ---\n        q_table = np.zeros((NUM_STATES, NUM_ACTIONS))\n        rng = np.random.RandomState(12345)\n\n        for _ in range(E):\n            current_pos = 0\n            for t in range(w, T - 1):\n                # Current state\n                current_rsi_regime = rsi_regimes[t]\n                current_state_idx = current_rsi_regime * 2 + current_pos\n                \n                # Action selection (epsilon-greedy)\n                if rng.rand()  epsilon:\n                    action_idx = rng.randint(0, NUM_ACTIONS)\n                else:\n                    action_idx = np.argmax(q_table[current_state_idx, :])\n\n                # State transition and reward\n                prev_pos = current_pos\n                if action_idx == 0: # Hold\n                    next_pos = prev_pos\n                elif action_idx == 1: # Buy\n                    next_pos = 1\n                else: # Sell\n                    next_pos = 0\n                \n                transaction_cost = c if next_pos != prev_pos else 0.0\n                reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n\n                # Next state\n                next_rsi_regime = rsi_regimes[t+1]\n                next_state_idx = next_rsi_regime * 2 + next_pos\n                \n                # Q-table update\n                old_q_value = q_table[current_state_idx, action_idx]\n                next_max_q = np.max(q_table[next_state_idx, :])\n                td_target = reward + gamma * next_max_q\n                new_q_value = old_q_value + alpha * (td_target - old_q_value)\n                q_table[current_state_idx, action_idx] = new_q_value\n\n                # Update position for next step in episode\n                current_pos = next_pos\n        \n        # --- 3. Evaluation Phase ---\n        total_wealth = 0.0\n        current_pos = 0\n        final_pos = 0\n\n        for t in range(w, T - 1):\n            # Current state\n            current_rsi_regime = rsi_regimes[t]\n            current_state_idx = current_rsi_regime * 2 + current_pos\n\n            # Action selection (greedy)\n            action_idx = np.argmax(q_table[current_state_idx, :])\n\n            # State transition and reward\n            prev_pos = current_pos\n            if action_idx == 0: # Hold\n                next_pos = prev_pos\n            elif action_idx == 1: # Buy\n                next_pos = 1\n            else: # Sell\n                next_pos = 0\n            \n            transaction_cost = c if next_pos != prev_pos else 0.0\n            reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n            total_wealth += reward\n\n            # Update position for next step in evaluation\n            current_pos = next_pos\n        \n        final_pos = current_pos\n        \n        # Final liquidation\n        if final_pos == 1:\n            total_wealth -= c\n\n        results.append(round(total_wealth, 6))\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "定义离散状态和行动空间不仅仅是理论上的抽象，它对模型的预测结果有着直接而深远的影响。最后一个练习将通过一个简化的劳动力市场模型来阐明这一点。你将分析一项政策干预——最低工资法——如何通过限制公司的行动空间（即可提供的工资选项），来改变整个市场的均衡结果 。通过计算这项政策对稳态失业率的影响，你将深刻体会到，对行动空间的精确建模是进行有意义的经济分析和政策评估的关键一步。",
            "id": "2388625",
            "problem": "考虑一个离散时间的搜寻匹配环境，其中有连续统的同质风险中性工人和企业。时间由 $t=0,1,2,\\dots$ 索引。每个工人要么失业，要么就业。一个失业工人在一个时期内遇到一个企业的概率为 $\\lambda$，其中 $\\lambda=0.6$。一个就业工人在期末失去工作的概率为 $\\delta$，其中 $\\delta=0.1$，此事件与其他一切无关。\n\n当一个失业工人遇到一个企业时，匹配特定的生产率 $y$ 会被实现并被企业观察到。生产率 $y$ 在有限集 $Y=\\{1.0,\\,1.2,\\,1.4\\}$ 中取值，并且在不同会面之间是独立同分布的（i.i.d.），其概率为 $\\mathbb{P}(y=1.0)=0.3$，$\\mathbb{P}(y=1.2)=0.4$ 和 $\\mathbb{P}(y=1.4)=0.3$。然后，企业从离散行动集 $W=\\{0.9,\\,1.1,\\,1.3\\}$ 中选择一个工资报价 $w$。工人接受任何满足 $w\\ge r$ 的报价 $w$，其中保留工资为 $r=1.0$。如果工人接受，则该匹配在该时期内产生产出 $y$，工人获得报酬 $w$，企业的单期利润为 $y-w$。如果 $y-w0$，企业在该时期内将获得负利润。企业总可以选择不雇佣（等价于，不提供可接受的报价）并获得利润 $0$。\n\n企业是短视的，每次选择 $w$ 以最大化当期利润 $y-w$，同时需要满足接受条件 $w\\ge r$ 和可行性条件 $w\\in W$。引入了一项最低工资法，通过要求 $w\\ge m$（其中 $m=1.2$）从下方截断了行动空间。在该法律下，企业的可行行动集变为 $W_m=\\{w\\in W:\\,w\\ge m\\}$。\n\n假设该法律得到完美执行，且环境的所有其他方面均如所述，那么在最低工资法下的稳态失业率是多少？四舍五入到三位小数。\n\nA. $u=0.357$\n\nB. $u=0.192$\n\nC. $u=0.294$\n\nD. $u=0.125$",
            "solution": "该环境具有离散的状态和行动空间：生产率状态 $y\\in\\{1.0,1.2,1.4\\}$ 和工资行动 $w\\in\\{0.9,1.1,1.3\\}$，并被最低工资 $m=1.2$ 从下方截断。在给定的 $y$ 下，企业的单期问题是选择 $w$ 来最大化 $y-w$，同时满足接受条件 $w\\ge r$ 和可行性，并且企业可以选择不雇佣以避免负利润。\n\n在最低工资法 $m=1.2$ 下，可行的工资集合变为\n$$\nW_m=\\{w\\in W:\\,w\\ge m\\}=\\{1.3\\}.\n$$\n因为工人的保留工资是 $r=1.0$，所以接受约束 $w\\ge r$ 被 $w=1.3$ 自动满足。只有当 $y-w\\ge 0$ 时，即只有当 $y\\ge 1.3$ 时，雇佣才是有利可图的。给定 $y\\in\\{1.0,1.2,1.4\\}$，盈利性仅在 $y=1.4$ 时成立。因此，在会面的条件下，当且仅当 $y=1.4$ 时才会发生雇佣。观察到 $y=1.4$ 的概率是 $\\mathbb{P}(y=1.4)=0.3$。\n\n因此，失业工人每个时期的工作发现概率为\n$$\nf=\\lambda\\cdot \\mathbb{P}(\\text{hire} \\mid \\text{meeting})=\\lambda\\cdot \\mathbb{P}(y=1.4)=0.6\\times 0.3=0.18.\n$$\n\n在稳态下，流入失业的人数等于流出失业的人数：\n$$\n\\delta(1-u)=fu.\n$$\n求解 $u$ 可得\n$$\nu=\\frac{\\delta}{\\delta+f}=\\frac{0.1}{0.1+0.18}=\\frac{0.1}{0.28}=\\frac{5}{14}\\approx 0.357142857\\approx 0.357 \\text{ (保留三位小数)}.\n$$\n\n逐项分析：\n\nA. $u=0.357$。这与使用截断的行动空间和流量平衡方程从第一性原理计算出的稳态失业率相匹配。正确。\n\nB. $u=0.192$。这等于 $\\frac{0.1}{0.1+0.42}$，这是没有最低工资时的稳态失业率（此时，在 $y\\in\\{1.2,1.4\\}$ 且工资为 $1.1$ 时会发生雇佣）。这不适用于最低工资 $m=1.2$ 将 $W$ 截断为 $\\{1.3\\}$ 的情况。错误。\n\nC. $u=0.294$。这对应于使用了不正确的工作发现概率，例如 $f=0.24$（例如，错误地假设在 $y\\ge 1.2$ 时以工资 $1.2$ 雇佣，而不是遵守离散行动集 $W_m=\\{1.3\\}$），得出 $u=\\frac{0.1}{0.1+0.24}\\approx 0.294$。这忽略了离散行动空间被截断为 $w=1.3$ 的情况。错误。\n\nD. $u=0.125$。如果错误地忽略了会面概率 $\\lambda$ 并取 $f=\\mathbb{P}(y\\ge 1.2)=0.7$，就会得到这个结果，导致 $u=\\frac{0.1}{0.1+0.7}=0.125$。这既忽略了会面摩擦，也忽略了离散截断至 $w=1.3$ 的情况。错误。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}