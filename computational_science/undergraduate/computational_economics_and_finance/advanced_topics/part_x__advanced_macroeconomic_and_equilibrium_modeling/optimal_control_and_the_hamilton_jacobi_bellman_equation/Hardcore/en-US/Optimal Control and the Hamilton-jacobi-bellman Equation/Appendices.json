{
    "hands_on_practices": [
        {
            "introduction": "The Merton portfolio problem is a landmark application of optimal control theory in finance, providing a foundational model for how an individual should allocate wealth between a risky and a risk-free asset. This exercise guides you through a specific, elegant case using a logarithmic utility function. By applying the Hamilton-Jacobi-Bellman (HJB) equation, you will derive the famous result that the optimal fraction of wealth to invest in the risky asset is constant, depending only on the parameters of the market and not on the investor's wealth or time.",
            "id": "2416529",
            "problem": "Consider a continuous-time, frictionless financial market with two assets. A risk-free asset (money market account) $B_{t}$ evolves according to $dB_{t} = r B_{t} \\, dt$ with constant risk-free rate $r \\ge 0$. A risky asset $S_{t}$ follows geometric Brownian motion given by $dS_{t} = S_{t} \\left( \\mu \\, dt + \\sigma \\, dW_{t} \\right)$, where $\\mu  r$ and $\\sigma  0$ are constants, and $\\{ W_{t} \\}_{t \\ge 0}$ is a standard Brownian motion on a filtered probability space satisfying the usual conditions. An investor with initial wealth $W_{0} = w  0$ chooses a consumption process $\\{ c_{t} \\}_{t \\ge 0}$ and a portfolio process $\\{ \\pi_{t} \\}_{t \\ge 0}$, where $\\pi_{t}$ denotes the fraction of wealth allocated to the risky asset at time $t$, and the remaining fraction $1 - \\pi_{t}$ is allocated to the risk-free asset. Wealth evolves as\n$$\ndW_{t} = \\left[ r W_{t} + \\pi_{t} W_{t} (\\mu - r) - c_{t} \\right] dt + \\pi_{t} W_{t} \\sigma \\, dW_{t},\n$$\nwith the admissibility requirements that $W_{t} \\ge 0$ and $c_{t} \\ge 0$ almost surely for all $t \\ge 0$, and that $\\{ \\pi_{t}, c_{t} \\}$ are progressively measurable and integrable so that the stochastic integral is well-defined.\n\nThe investor has time-separable preferences over consumption with instantaneous utility $U(c) = \\ln(c)$ and subjective discount rate $\\delta  0$. The objective is to maximize expected discounted utility\n$$\n\\sup_{\\{ \\pi_{t}, c_{t} \\}_{t \\ge 0}} \\; \\mathbb{E} \\left[ \\int_{0}^{\\infty} \\exp(-\\delta t) \\, \\ln(c_{t}) \\, dt \\right].\n$$\n\nDetermine the unique constant (time- and wealth-invariant) optimal fraction $\\pi^{\\star}$ of wealth invested in the risky asset that solves this problem. Express your final answer as a single closed-form analytic expression in terms of $\\mu$, $r$, and $\\sigma$. No rounding is required. Do not include units in your answer.",
            "solution": "The posed problem is a classic, well-defined problem in continuous-time finance, specifically the Merton portfolio problem with an infinite time horizon. The problem is scientifically grounded, mathematically well-posed, and all parameters and objectives are stated with clarity and precision. It is thus deemed a valid problem. We will solve it using the method of dynamic programming, which leads to the Hamilton-Jacobi-Bellman (HJB) equation.\n\nLet $J(W)$ be the value function, representing the maximum achievable expected utility for an investor with current wealth $W$. Due to the infinite time horizon and constant parameters, the value function is independent of time $t$. It is defined as:\n$$\nJ(W) = \\sup_{\\{ \\pi_{t}, c_{t} \\}_{t \\ge 0}} \\; \\mathbb{E} \\left[ \\int_{0}^{\\infty} \\exp(-\\delta t) \\, \\ln(c_{t}) \\, dt \\;\\middle|\\; W_{0}=W \\right]\n$$\nThe state variable is wealth $W_{t}$, and the control variables are consumption $c_{t}$ and the fraction of wealth in the risky asset $\\pi_{t}$. The dynamics of wealth are given by the stochastic differential equation:\n$$\ndW_{t} = \\left[ r W_{t} + \\pi_{t} W_{t} (\\mu - r) - c_{t} \\right] dt + \\pi_{t} W_{t} \\sigma \\, dW_{t}\n$$\nThe Hamilton-Jacobi-Bellman equation for this stochastic optimal control problem is:\n$$\n\\delta J(W) = \\sup_{c \\ge 0, \\pi} \\left\\{ \\ln(c) + J'(W) \\left[ r W + \\pi W (\\mu - r) - c \\right] + \\frac{1}{2} J''(W) \\left( \\pi W \\sigma \\right)^{2} \\right\\}\n$$\nwhere $J'(W) = \\frac{dJ}{dW}$ and $J''(W) = \\frac{d^{2}J}{dW^{2}}$. The term $\\delta J(W)$ on the left side represents the \"flow value\" that the optimal policy must generate to match the total value $J(W)$ over time, with $\\delta$ being the discount rate. The right side is the maximization of the sum of the instantaneous utility from consumption, $\\ln(c)$, and the expected instantaneous change in the value function, which is given by applying the generator of the process to $J(W)$.\n\nTo find the optimal controls, we take the first-order conditions of the expression inside the supremum with respect to $c$ and $\\pi$.\nFirst, differentiating with respect to consumption $c$:\n$$\n\\frac{\\partial}{\\partial c} \\left( \\ln(c) - c J'(W) \\right) = \\frac{1}{c} - J'(W) = 0\n$$\nThis yields the optimal consumption rule:\n$$\nc^{\\star}(W) = \\frac{1}{J'(W)}\n$$\nThe second-order condition is $-\\frac{1}{c^{2}}  0$, which confirms this is a maximum.\n\nNext, differentiating with respect to the portfolio fraction $\\pi$:\n$$\n\\frac{\\partial}{\\partial \\pi} \\left( J'(W) \\pi W (\\mu - r) + \\frac{1}{2} J''(W) \\pi^{2} W^{2} \\sigma^{2} \\right) = J'(W) W (\\mu - r) + J''(W) \\pi W^{2} \\sigma^{2} = 0\n$$\nSolving for $\\pi$, we find the optimal portfolio rule:\n$$\n\\pi^{\\star}(W) = - \\frac{J'(W) W (\\mu - r)}{J''(W) W^{2} \\sigma^{2}} = - \\frac{\\mu - r}{\\sigma^{2}} \\frac{J'(W)}{W J''(W)}\n$$\nThe second-order condition requires $J''(W) W^{2} \\sigma^{2}  0$. Since $W^{2}  0$ and $\\sigma^{2}  0$, this requires $J''(W)  0$. This is consistent with a concave value function, which is expected for a risk-averse investor.\n\nThe structure of the problem, where utility is logarithmic and wealth dynamics are multiplicative, suggests a solution for the value function of the form:\n$$\nJ(W) = A \\ln(W) + B\n$$\nwhere $A$ and $B$ are constants to be determined. The concavity requirement $J''(W)  0$ implies that $A  0$.\nThe derivatives of this conjectured value function are:\n$$\nJ'(W) = \\frac{A}{W}\n$$\n$$\nJ''(W) = -\\frac{A}{W^{2}}\n$$\nSubstituting these derivatives into the expression for the optimal portfolio fraction $\\pi^{\\star}(W)$:\n$$\n\\pi^{\\star} = - \\frac{\\mu - r}{\\sigma^{2}} \\frac{A/W}{W(-A/W^{2})} = - \\frac{\\mu - r}{\\sigma^{2}} \\frac{A/W}{-A/W} = \\frac{\\mu - r}{\\sigma^{2}}\n$$\nThis result is a constant, independent of wealth $W$ and time $t$, which is consistent with the problem's request for a \"unique constant optimal fraction\". The problem only asks for this fraction.\n\nFor completeness, we can determine the constant $A$ by substituting the optimal controls and the value function ansatz back into the HJB equation.\nThe optimal consumption is $c^{\\star} = \\frac{1}{J'(W)} = \\frac{W}{A}$.\nThe HJB equation becomes:\n$$\n\\delta (A \\ln(W) + B) = \\ln\\left(\\frac{W}{A}\\right) + \\frac{A}{W}\\left[ rW + \\left(\\frac{\\mu - r}{\\sigma^{2}}\\right)W(\\mu-r) - \\frac{W}{A} \\right] + \\frac{1}{2}\\left(-\\frac{A}{W^{2}}\\right)\\left(\\frac{\\mu - r}{\\sigma^{2}}\\right)^{2} W^{2} \\sigma^{2}\n$$\nSimplifying the expression:\n$$\n\\delta A \\ln(W) + \\delta B = (\\ln(W) - \\ln(A)) + A\\left[r + \\frac{(\\mu-r)^{2}}{\\sigma^{2}} - \\frac{1}{A}\\right] - \\frac{A}{2}\\frac{(\\mu-r)^{2}}{\\sigma^{2}}\n$$\n$$\n\\delta A \\ln(W) + \\delta B = \\ln(W) - \\ln(A) + Ar + \\frac{A(\\mu-r)^{2}}{\\sigma^{2}} - 1 - \\frac{A(\\mu-r)^{2}}{2\\sigma^{2}}\n$$\n$$\n\\delta A \\ln(W) + \\delta B = \\ln(W) + \\left[ - \\ln(A) - 1 + Ar + \\frac{A(\\mu-r)^{2}}{2\\sigma^{2}} \\right]\n$$\nFor this equation to hold for all $W  0$, the coefficients of $\\ln(W)$ and the constant terms on both sides must be equal.\nEquating the coefficients of $\\ln(W)$:\n$$\n\\delta A = 1 \\implies A = \\frac{1}{\\delta}\n$$\nSince $\\delta  0$, we have $A  0$, which is consistent with our earlier requirement for concavity.\nThe existence of a solution for the constant $B$ confirms the validity of our ansatz, but its value is not needed to answer the question. The optimal fraction of wealth invested in the risky asset depends only on the market parameters $\\mu$, $r$, and $\\sigma$.\n\nThe final answer is therefore the constant expression derived for $\\pi^{\\star}$.",
            "answer": "$$\n\\boxed{\\frac{\\mu - r}{\\sigma^{2}}}\n$$"
        },
        {
            "introduction": "While solving unconstrained problems is a crucial first step, many real-world scenarios involve limits, such as a borrowing constraint that prevents wealth from becoming negative. This practice  shifts the focus from calculation to conceptual understanding, asking you to identify the correct mathematical formulation of the HJB equation when a state constraint is present. Correctly handling these boundaries is a key skill for building more realistic and robust economic models.",
            "id": "2416539",
            "problem": "Consider a continuous-time consumption-saving problem with a borrowing constraint in which an agent chooses measurable consumption $c_t \\ge 0$ to maximize the discounted utility\n$$\\int_0^{\\infty} e^{-\\rho t} u(c_t) \\, dt,$$\nwhere $u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma}$ for $\\gamma  0$, $\\gamma \\ne 1$, and $\\rho  0$. Wealth $x_t$ evolves according to\n$$\\dot{x}_t = r x_t + y - c_t,$$\nwith $x_0 = \\bar{x} \\ge 0$, constant interest rate $r \\ge 0$, and constant income flow $y \\ge 0$. The borrowing constraint requires $x_t \\ge 0$ for all $t \\ge 0$. Let $V(x)$ denote the value function. Using the principle of optimality and the Hamilton-Jacobi-Bellman (HJB) framework, which of the following correctly characterizes the HJB equation in the interior $x  0$ together with the appropriate boundary condition at the state-constraint boundary $x = 0$?\n\nA. For all $x  0$,\n$$\\rho V(x) = \\max_{c \\ge 0} \\left\\{ u(c) + V_x(x)\\,(r x + y - c) \\right\\},$$\nand at $x = 0$,\n$$\\rho V(0) = \\max_{0 \\le c \\le y} \\left\\{ u(c) + V_x(0)\\,(y - c) \\right\\}.$$\n\nB. For all $x  0$,\n$$\\rho V(x) = \\max_{c \\ge 0} \\left\\{ u(c) + V_x(x)\\,(r x + y - c) \\right\\},$$\nand the state-constraint induces a Neumann boundary condition $V_x(0) = u'(y)$.\n\nC. For all $x  0$,\n$$\\rho V(x) = \\max_{c \\ge 0} \\left\\{ u(c) + V_x(x)\\,(r x + y - c) \\right\\},$$\nand the boundary condition at $x = 0$ is Dirichlet with fixed value\n$$V(0) = \\int_0^{\\infty} e^{-\\rho t} u(y)\\, dt = \\frac{u(y)}{\\rho}.$$\n\nD. The borrowing constraint appears in the HJB through a Lagrange multiplier $\\lambda \\ge 0$ so that, for all $x \\ge 0$,\n$$\\rho V(x) = \\max_{c \\ge 0,\\, \\lambda \\ge 0} \\left\\{ u(c) + V_x(x)\\,(r x + y - c) + \\lambda\\, x \\right\\}, \\quad \\lambda\\, x = 0.$$\n\nE. For all $x  0$,\n$$\\rho V(x) = \\max_{c \\ge 0} \\left\\{ u(c) + V_x(x)\\,(r x + y - c) \\right\\},$$\nand at $x = 0$ the state-constraint boundary condition is the inequality\n$$\\rho V(0) \\ge \\max_{c \\ge 0} \\left\\{ u(c) + V_x(0)\\,(y - c) \\right\\},$$\nwith no restriction on $c$ at $x = 0$.",
            "solution": "We start from first principles. The problem is an infinite-horizon deterministic control problem with state $x_t$ and control $c_t$. The objective is\n$$\\max_{c_{\\cdot} \\ge 0} \\int_0^{\\infty} e^{-\\rho t} u(c_t)\\, dt,$$\nsubject to the law of motion\n$$\\dot{x}_t = r x_t + y - c_t, \\quad x_0 = \\bar{x} \\ge 0,$$\nand the state constraint\n$$x_t \\ge 0 \\quad \\text{for all } t \\ge 0.$$\n\nBy the dynamic programming principle, if $V(x)$ denotes the value function, then for small $\\Delta  0$ and admissible controls over $[0,\\Delta)$,\n$$V(x) = \\max_{c \\in \\mathcal{A}(x)} \\left\\{ \\int_0^{\\Delta} e^{-\\rho t} u(c)\\, dt + e^{-\\rho \\Delta} V\\left(x + \\Delta \\cdot (r x + y - c) + o(\\Delta)\\right) \\right\\},$$\nwhere $\\mathcal{A}(x)$ denotes the set of controls that keep the state feasible over the interval $[0,\\Delta)$ beginning at $x$. Using a first-order expansion and dividing by $\\Delta$, letting $\\Delta \\to 0$, we obtain the Hamilton-Jacobi-Bellman equation\n$$\\rho V(x) = \\max_{c \\in \\mathcal{A}(x)} \\left\\{ u(c) + V_x(x)\\,(r x + y - c) \\right\\}.$$\n\nThe key is to characterize $\\mathcal{A}(x)$:\n- For interior states $x  0$, feasibility in an infinitesimal interval does not restrict $c$ beyond $c \\ge 0$, so $\\mathcal{A}(x) = \\{ c \\in \\mathbb{R}_+ \\}$.\n- At the boundary $x = 0$, the state constraint requires that the instantaneous drift cannot point outward from the feasible set. Specifically, to avoid violating $x_t \\ge 0$ immediately, we must have $\\dot{x}_t \\ge 0$ at $x = 0$. Therefore, admissible controls at $x = 0$ satisfy\n$$r \\cdot 0 + y - c \\ge 0 \\quad \\Longleftrightarrow \\quad 0 \\le c \\le y.$$\nThus $\\mathcal{A}(0) = [0,y]$.\n\nHence, the correct HJB and boundary characterization are:\n- For all $x  0$,\n$$\\rho V(x) = \\max_{c \\ge 0} \\left\\{ u(c) + V_x(x)\\,(r x + y - c) \\right\\}.$$\n- At $x = 0$,\n$$\\rho V(0) = \\max_{0 \\le c \\le y} \\left\\{ u(c) + V_x(0)\\,(y - c) \\right\\}.$$\n\nThis matches Option A. Next, we evaluate each option.\n\nOption A: This matches the derivation. The interior HJB uses the unrestricted nonnegativity constraint on $c$, and the boundary condition uses the state-constraint restriction $0 \\le c \\le y$ to prevent immediate exit from the feasible set. Equality holds because the dynamic programming principle applies with the admissible control set adapted to the state. Verdict — Correct.\n\nOption B: It posits $V_x(0) = u'(y)$. This condition would arise only if the optimizer at $x = 0$ chooses $c = y$ as an interior maximizer of $u(c) + V_x(0) (y - c)$, which requires $u'(y) = V_x(0)$ and that $c = y$ is not at a corner. However, whether $c = y$ is optimal at $x = 0$ depends on parameters and the shape of $V$; in general, the optimal choice at the boundary may be $c  y$ to move into the interior (saving) or $c = y$ to remain at the boundary, determined endogenously by $V_x(0)$. Therefore, imposing $V_x(0) = u'(y)$ as a boundary condition is not generally valid. Verdict — Incorrect.\n\nOption C: It fixes $V(0) = \\frac{u(y)}{\\rho}$, which is the value obtained by holding $c_t \\equiv y$ forever. This is not generally optimal under the borrowing constraint; the agent may choose $c_t  y$ for some time to accumulate assets (since $r \\ge 0$) and then consume more later, potentially achieving a higher value than $\\frac{u(y)}{\\rho}$. Conversely, parameter configurations could make $c_t \\equiv y$ optimal. In either case, one cannot a priori impose a Dirichlet boundary value independent of $V_x(0)$. Verdict — Incorrect.\n\nOption D: It introduces the state constraint via a Lagrange multiplier $\\lambda$ in the HJB and includes the complementarity condition $\\lambda x = 0$. In dynamic programming, state constraints are enforced by restricting the admissible control set as a function of the state, not by adding multipliers times the state to the HJB integrand. The term $\\lambda x$ and $\\lambda x = 0$ do not correctly encode the instantaneous feasibility requirement at the boundary; instead, one must restrict $c$ so that the drift points inward when $x = 0$. Verdict — Incorrect.\n\nOption E: It uses an inequality at $x = 0$ but allows maximization over $c \\ge 0$ without the essential restriction $c \\le y$. Allowing $c  y$ at $x = 0$ would imply an outward-pointing drift $\\dot{x} = y - c  0$, which immediately violates feasibility. The correct admissible set at $x = 0$ is $[0,y]$, and with the dynamic programming principle the equation at the boundary holds with equality using that restricted set. Verdict — Incorrect.\n\nTherefore, the correct choice is Option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The power of the HJB framework extends far beyond finance. This exercise  demonstrates its versatility by modeling a student's dilemma: how to optimally allocate study time between two subjects to maximize their final GPA. The problem's linear structure leads to a different kind of solution—a \"bang-bang\" control—highlighting how the same dynamic programming principles can be applied to a wide array of optimization challenges.",
            "id": "2416576",
            "problem": "Consider a student who allocates continuous study time between two subjects, indexed by $i \\in \\{1,2\\}$, over a fixed horizon $[0,T]$. Let $X_i(t)$ denote the student’s understanding in subject $i$ at time $t$, and let $u(t) \\in [0,1]$ denote the fraction of instantaneous study effort devoted to subject $1$ at time $t$ (so subject $2$ receives $1-u(t)$). The dynamics of understanding are modeled as controlled Itô processes\n$$\n\\mathrm{d}X_1(t) = \\left(-b_1 X_1(t) + a_1 u(t)\\right)\\mathrm{d}t + \\sigma_1 \\mathrm{d}W_1(t), \\quad\n\\mathrm{d}X_2(t) = \\left(-b_2 X_2(t) + a_2 \\left(1 - u(t)\\right)\\right)\\mathrm{d}t + \\sigma_2 \\mathrm{d}W_2(t),\n$$\nwhere $W_1(t)$ and $W_2(t)$ are independent standard Brownian motions, $a_1,a_2 \\ge 0$ are learning productivity coefficients, $b_1,b_2 \\ge 0$ are forgetting rates, and $\\sigma_1,\\sigma_2 \\ge 0$ are volatility parameters. The terminal Grade Point Average (GPA) proxy is modeled as a linear payoff\n$$\nG = \\varphi_1 X_1(T) + \\varphi_2 X_2(T),\n$$\nwith weights $\\varphi_1,\\varphi_2  0$. The objective is to choose an admissible control $u(\\cdot)$, measurable and adapted to the filtration generated by $(W_1,W_2)$, to maximize the expected terminal GPA $\\mathbb{E}[G]$.\n\nYour task is to write a complete, runnable program that, for each parameter set below, computes the optimal expected terminal GPA $\\sup_{u(\\cdot)\\in[0,1]} \\mathbb{E}[G]$ given the initial state $(X_1(0),X_2(0)) = (x_{10},x_{20})$. Angles do not appear, and there are no physical units; report all numerical answers as floating-point numbers.\n\nUse the following test suite of parameter values. For each test case $k$, the parameters are ordered as\n$(a_1,a_2,b_1,b_2,\\sigma_1,\\sigma_2,\\varphi_1,\\varphi_2,T,x_{10},x_{20})$:\n\n- Test 1 (general case): $(1.0,0.8,0.2,0.3,0.3,0.2,2.0,1.5,2.0,0.5,0.7)$.\n- Test 2 (boundary, zero horizon): $(1.0,1.2,0.1,0.3,0.0,0.0,1.0,1.0,0.0,1.0,2.0)$.\n- Test 3 (tie in marginal effectiveness, identical decay): $(1.0,0.4,0.5,0.5,0.5,0.1,1.0,2.5,3.0,0.0,0.0)$.\n- Test 4 (interior switching in optimal allocation): $(1.0,1.0,0.2,1.0,0.2,0.4,1.0,2.0,2.0,0.0,0.0)$.\n- Test 5 (zero learning in one subject and zero decay in the other): $(0.0,1.0,0.3,0.0,0.7,0.5,1.0,1.0,1.5,0.2,0.1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[3.141593,2.718282]$), where the $k$-th entry is the optimal expected terminal GPA for Test $k$, each reported as a floating-point number rounded to six decimal places.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\nStep 1: Extract Givens\n- State variables: $X_1(t)$, $X_2(t)$.\n- Control variable: $u(t) \\in [0,1]$.\n- State dynamics:\n$$\n\\mathrm{d}X_1(t) = \\left(-b_1 X_1(t) + a_1 u(t)\\right)\\mathrm{d}t + \\sigma_1 \\mathrm{d}W_1(t)\n$$\n$$\n\\mathrm{d}X_2(t) = \\left(-b_2 X_2(t) + a_2 \\left(1 - u(t)\\right)\\right)\\mathrm{d}t + \\sigma_2 \\mathrm{d}W_2(t)\n$$\n- $W_1(t), W_2(t)$ are independent standard Brownian motions.\n- Parameters: $a_1, a_2, b_1, b_2, \\sigma_1, \\sigma_2 \\ge 0$, $\\varphi_1, \\varphi_2  0$.\n- Time horizon: $[0, T]$.\n- Terminal payoff: $G = \\varphi_1 X_1(T) + \\varphi_2 X_2(T)$.\n- Objective: Maximize $\\mathbb{E}[G]$.\n- Initial state: $(X_1(0), X_2(0)) = (x_{10}, x_{20})$.\n- Test cases: Five distinct parameter sets are provided.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard stochastic optimal control problem with linear dynamics (controlled Ornstein-Uhlenbeck processes) and a linear objective function. This is a well-established model in financial engineering and economics, known as a Merton-type problem. It is mathematically and scientifically sound.\n- **Well-Posed**: The problem is well-posed. The dynamics are linear, the objective is linear, and the control is constrained to a compact set. Standard existence and uniqueness theorems for such problems apply. The Hamilton-Jacobi-Bellman (HJB) framework is directly applicable.\n- **Objective**: The problem is stated with precise mathematical definitions and without ambiguity or subjective content.\n- **Flaw Analysis**: The problem does not violate any fundamental principles, is not metaphorical, is complete and consistent, is not physically infeasible, is well-structured, and is scientifically verifiable. All specified parameters are in valid ranges.\n\nStep 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\nThe problem is to find an optimal allocation of study time $u(t)$ to maximize the expected terminal GPA. This is a continuous-time stochastic optimal control problem. The standard method for solving such problems is the Hamilton-Jacobi-Bellman (HJB) equation.\n\nLet $V(t, x_1, x_2)$ be the value function, representing the maximum expected terminal GPA, given the state $(X_1(t), X_2(t)) = (x_1, x_2)$.\n$$ V(t, x_1, x_2) = \\sup_{u(\\cdot) \\in [0,1]} \\mathbb{E}_{t,x_1,x_2} \\left[ \\varphi_1 X_1(T) + \\varphi_2 X_2(T) \\right] $$\nThe value function must satisfy the terminal condition given by the payoff function:\n$$ V(T, x_1, x_2) = \\varphi_1 x_1 + \\varphi_2 x_2 $$\nThe HJB equation for this problem is:\n$$ \\frac{\\partial V}{\\partial t} + \\sup_{u \\in [0,1]} \\left\\{ \\left(-b_1 x_1 + a_1 u\\right) \\frac{\\partial V}{\\partial x_1} + \\left(-b_2 x_2 + a_2(1-u)\\right) \\frac{\\partial V}{\\partial x_2} + \\frac{1}{2} \\sigma_1^2 \\frac{\\partial^2 V}{\\partial x_1^2} + \\frac{1}{2} \\sigma_2^2 \\frac{\\partial^2 V}{\\partial x_2^2} \\right\\} = 0 $$\nThe cross-derivative term is absent because the Brownian motions $W_1(t)$ and $W_2(t)$ are independent.\n\nGiven the linear structure of the dynamics and the terminal condition, we propose a linear ansatz for the value function:\n$$ V(t, x_1, x_2) = p_1(t) x_1 + p_2(t) x_2 + q(t) $$\nThe partial derivatives are:\n$$ \\frac{\\partial V}{\\partial t} = p_1'(t) x_1 + p_2'(t) x_2 + q'(t), \\quad \\frac{\\partial V}{\\partial x_1} = p_1(t), \\quad \\frac{\\partial V}{\\partial x_2} = p_2(t), \\quad \\frac{\\partial^2 V}{\\partial x_1^2} = 0, \\quad \\frac{\\partial^2 V}{\\partial x_2^2} = 0 $$\nA crucial observation is that the second derivatives are zero. Consequently, the terms involving $\\sigma_1$ and $\\sigma_2$ vanish from the HJB equation. This implies that the optimal control and the expected value are independent of the volatility parameters, a characteristic of problems with linear objectives (risk neutrality).\n\nSubstituting the derivatives into the HJB equation:\n$$ p_1' x_1 + p_2' x_2 + q' + \\sup_{u \\in [0,1]} \\left\\{ (-b_1 x_1 + a_1 u) p_1 + (-b_2 x_2 + a_2(1-u)) p_2 \\right\\} = 0 $$\nWe can separate the terms containing the control variable $u$:\n$$ (p_1' - b_1 p_1) x_1 + (p_2' - b_2 p_2) x_2 + q' + a_2 p_2 + \\sup_{u \\in [0,1]} \\left\\{ u \\left( a_1 p_1 - a_2 p_2 \\right) \\right\\} = 0 $$\nFor this equation to hold for all $x_1, x_2$, the coefficients of $x_1$ and $x_2$ must be zero. This yields a system of ordinary differential equations (ODEs) for $p_1(t)$ and $p_2(t)$:\n$$ p_1'(t) - b_1 p_1(t) = 0 \\implies p_1'(t) = b_1 p_1(t) $$\n$$ p_2'(t) - b_2 p_2(t) = 0 \\implies p_2'(t) = b_2 p_2(t) $$\nFrom the terminal condition $V(T, x_1, x_2) = \\varphi_1 x_1 + \\varphi_2 x_2$, we obtain the terminal conditions for the ODEs: $p_1(T) = \\varphi_1$ and $p_2(T) = \\varphi_2$. We also set $q(T)=0$.\nSolving these backward-time ODEs gives:\n$$ p_1(t) = \\varphi_1 e^{-b_1(T-t)} $$\n$$ p_2(t) = \\varphi_2 e^{-b_2(T-t)} $$\nThe optimal control $u^*(t)$ is chosen to maximize the term $u(a_1 p_1(t) - a_2 p_2(t))$. This is a linear function of $u \\in [0,1]$. The optimal control is therefore of a \"bang-bang\" nature:\n$$ u^*(t) = \\begin{cases} 1  \\text{if } a_1 p_1(t) - a_2 p_2(t)  0 \\\\ 0  \\text{if } a_1 p_1(t) - a_2 p_2(t)  0 \\\\ \\text{any } u \\in [0,1]  \\text{if } a_1 p_1(t) - a_2 p_2(t) = 0 \\end{cases} $$\nThe switching function $S(t) = a_1 p_1(t) - a_2 p_2(t)$ determines the allocation. The value of the supremum is $\\max(a_1 p_1(t) - a_2 p_2(t), 0)$.\nThe remaining part of the HJB equation gives the ODE for $q(t)$:\n$$ q'(t) + a_2 p_2(t) + \\max(a_1 p_1(t) - a_2 p_2(t), 0) = 0 $$\nThis simplifies to:\n$$ q'(t) + \\max(a_1 p_1(t), a_2 p_2(t)) = 0 $$\nIntegrating from $t$ to $T$ with $q(T)=0$, we find $q(t)$:\n$$ q(t) = \\int_t^T \\max(a_1 p_1(\\tau), a_2 p_2(\\tau)) d\\tau $$\nThe desired quantity is the value function at time $t=0$, $V(0, x_{10}, x_{20})$:\n$$ V(0, x_{10}, x_{20}) = p_1(0)x_{10} + p_2(0)x_{20} + q(0) $$\nSubstituting the expressions for $p_1(0)$, $p_2(0)$, and $q(0)$:\n$$ \\sup \\mathbb{E}[G] = \\varphi_1 e^{-b_1 T} x_{10} + \\varphi_2 e^{-b_2 T} x_{20} + \\int_0^T \\max\\left(a_1 \\varphi_1 e^{-b_1(T-\\tau)}, a_2 \\varphi_2 e^{-b_2(T-\\tau)}\\right) d\\tau $$\nTo compute the integral, we must find if and where the two functions inside the maximum are equal. Let $f_1(\\tau) = a_1 \\varphi_1 e^{-b_1(T-\\tau)}$ and $f_2(\\tau) = a_2 \\varphi_2 e^{-b_2(T-\\tau)}$. A potential switching time $\\tau^* \\in [0,T]$ exists where $f_1(\\tau^*) = f_2(\\tau^*)$.\nIf $b_1 \\neq b_2$ and $a_1, a_2, \\varphi_1, \\varphi_2  0$, this occurs at:\n$$ \\tau^* = T - \\frac{1}{b_1 - b_2} \\ln\\left(\\frac{a_1 \\varphi_1}{a_2 \\varphi_2}\\right) $$\nIf $0  \\tau^*  T$, the integral is split into two parts. Otherwise, one function dominates over the entire interval $[0,T]$. The integral of each component function is computed analytically. Let $C_i = a_i\\varphi_i$. For $b_i \\neq 0$, the indefinite integral is $\\int C_i e^{-b_i(T-\\tau)} d\\tau = \\frac{C_i}{b_i} e^{-b_i(T-\\tau)}$. For $b_i=0$, the integral is $\\int C_i d\\tau = C_i \\tau$. The overall optimal expected GPA is found by summing the contribution from the initial state and the computed value of the control integral.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the student's optimal study allocation problem for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # (a1, a2, b1, b2, sigma1, sigma2, phi1, phi2, T, x10, x20)\n        (1.0, 0.8, 0.2, 0.3, 0.3, 0.2, 2.0, 1.5, 2.0, 0.5, 0.7),  # Test 1\n        (1.0, 1.2, 0.1, 0.3, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0),  # Test 2\n        (1.0, 0.4, 0.5, 0.5, 0.5, 0.1, 1.0, 2.5, 3.0, 0.0, 0.0),  # Test 3\n        (1.0, 1.0, 0.2, 1.0, 0.2, 0.4, 1.0, 2.0, 2.0, 0.0, 0.0),  # Test 4\n        (0.0, 1.0, 0.3, 0.0, 0.7, 0.5, 1.0, 1.0, 1.5, 0.2, 0.1),  # Test 5\n    ]\n\n    results = []\n    for params in test_cases:\n        result = calculate_optimal_gpa(params)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_optimal_gpa(params):\n    \"\"\"\n    Calculates the optimal expected GPA for a single set of parameters.\n    The formula is V(0, x10, x20) = p1(0)*x10 + p2(0)*x20 + q(0),\n    where:\n    p1(0) = phi1 * exp(-b1*T)\n    p2(0) = phi2 * exp(-b2*T)\n    q(0) = integral from 0 to T of max(f1(tau), f2(tau)) d(tau)\n    f1(tau) = a1*phi1*exp(-b1*(T-tau))\n    f2(tau) = a2*phi2*exp(-b2*(T-tau))\n    \"\"\"\n    (a1, a2, b1, b2, _, _, phi1, phi2, T, x10, x20) = params\n\n    # Contribution from the initial state\n    initial_gpa = phi1 * np.exp(-b1 * T) * x10 + phi2 * np.exp(-b2 * T) * x20\n\n    if T == 0:\n        return initial_gpa\n\n    # Helper function to compute the definite integral of f_i(tau) from c to d\n    def integrate_f(a, b, phi, T_val, c, d):\n        if d = c:\n            return 0.0\n        \n        # Marginal effectiveness\n        C = a * phi\n        if C == 0:\n            return 0.0\n\n        if b == 0:\n            return C * (d - c)\n        else:\n            # Integral of C*exp(-b*(T-tau)) dtau is (C/b)*exp(-b*(T-tau))\n            val_at_d = (C / b) * np.exp(-b * (T_val - d))\n            val_at_c = (C / b) * np.exp(-b * (T_val - c))\n            return val_at_d - val_at_c\n\n    # Contribution from the optimal control\n    control_integral = 0.0\n    \n    C1 = a1 * phi1\n    C2 = a2 * phi2\n\n    # If one subject has zero marginal effectiveness, always focus on the other\n    if C1 = 0:\n        control_integral = integrate_f(a2, b2, phi2, T, 0, T)\n    elif C2 = 0:\n        control_integral = integrate_f(a1, b1, phi1, T, 0, T)\n    # Case with identical decay rates\n    elif b1 == b2:\n        if C1 = C2:\n            control_integral = integrate_f(a1, b1, phi1, T, 0, T)\n        else:\n            control_integral = integrate_f(a2, b2, phi2, T, 0, T)\n    # General case with different decay rates\n    else:\n        # Switching time tau_star is where f1(tau) = f2(tau)\n        tau_star = T - np.log(C1 / C2) / (b1 - b2)\n\n        if tau_star = 0:\n            # Switch is at or before t=0. One function dominates over [0,T].\n            # h(tau) = f1/f2 is monotonic. Sign at tau=0 determines dominance.\n            if b1  b2:  # h(tau) is increasing. f1f2 for tau  tau_star.\n                control_integral = integrate_f(a1, b1, phi1, T, 0, T)\n            else:  # b1  b2, h(tau) is decreasing. f2f1 for tau  tau_star.\n                control_integral = integrate_f(a2, b2, phi2, T, 0, T)\n        elif tau_star = T:\n            # Switch is at or after t=T. One function dominates over [0,T].\n            if b1  b2: # h(tau) is increasing. f2f1 for tau  tau_star.\n                control_integral = integrate_f(a2, b2, phi2, T, 0, T)\n            else: # b1  b2, h(tau) is decreasing. f1f2 for tau  tau_star.\n                control_integral = integrate_f(a1, b1, phi1, T, 0, T)\n        else:\n            # Interior switch at tau_star. Integral is split.\n            if b1  b2: # h increases, f2f1 before tau_star, f1f2 after\n                integ1 = integrate_f(a2, b2, phi2, T, 0, tau_star)\n                integ2 = integrate_f(a1, b1, phi1, T, tau_star, T)\n                control_integral = integ1 + integ2\n            else: # b1  b2, h decreases, f1f2 before, f2f1 after\n                integ1 = integrate_f(a1, b1, phi1, T, 0, tau_star)\n                integ2 = integrate_f(a2, b2, phi2, T, tau_star, T)\n                control_integral = integ1 + integ2\n\n    return initial_gpa + control_integral\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}