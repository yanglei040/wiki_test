{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in any Peaks-over-threshold (POT) analysis is selecting an appropriate threshold, $u$. This choice involves a fundamental trade-off: a threshold that is too low violates the asymptotic theory underlying the Generalized Pareto Distribution (GPD) approximation, leading to biased estimates, while a threshold that is too high leaves too few data points for a stable, low-variance estimation. This exercise provides hands-on practice with this \"bias-variance tradeoff\" by asking you to analyze how the estimated tail index, $\\xi$, varies across a range of thresholds, a common diagnostic procedure in extreme value analysis .",
            "id": "2418694",
            "problem": "Consider a synthetic daily log-return series intended to approximate the distributional features of the Standard and Poor’s 500 (SP 500) daily returns for the purpose of tail risk analysis. Let the daily log-returns be denoted by $R_t$ for $t = 1, \\dots, n$, and define $R_t = \\mu + \\sigma \\cdot T_\\nu$, where $T_\\nu$ is a Student’s $t$ random variable with degrees of freedom $\\nu$, location $0$, and unit scale. Use $n = 6000$, $\\mu = 0$, $\\sigma = 0.01$, $\\nu = 5$, and a fixed pseudorandom number generator seed $s = 20240517$ to ensure reproducibility. Define losses by $L_t = -R_t$.\n\nFor any threshold $u$, define the exceedances by $Y_i = L_i - u$ for all indices $i$ such that $L_i  u$. Under the Peaks-Over-Threshold (POT) model, assume the exceedances $Y_i$ follow a Generalized Pareto Distribution (GPD) with tail index $\\xi$ and scale $\\beta$.\n\nYour task is to quantify the sensitivity of the estimated tail index $\\xi$ to the choice of threshold $u$ by computing, for each threshold in the test suite described below, the estimate of $\\xi$ obtained from the exceedances over $u$. For each threshold, set $u$ equal to the empirical $q$-quantile of the losses $L_t$ for the given quantile level $q$.\n\nTest suite (quantile levels for thresholds): $q \\in \\{0.80, 0.90, 0.95, 0.975, 0.99, 0.995, 0.999\\}$.\n\nFor each $q$ in the test suite, compute the corresponding estimate of $\\xi$ as a real number. The required final output is a single line containing the results for the thresholds in the specified order, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets. For example, an output with three results should look like $[x_1,x_2,x_3]$ where each $x_j$ is a decimal number rounded to $6$ decimal places.\n\nYour program must produce a single line of output containing the estimates in the exact required format and order, with no additional text. No physical units are involved. Angles are not involved. Percentages are not involved. The answers must be real-valued floats.",
            "solution": "The problem statement submitted for analysis is deemed valid. It presents a well-posed, scientifically grounded problem in computational finance, specifically concerning the application of the Peaks-Over-Threshold (POT) method from Extreme Value Theory (EVT). All parameters and conditions are specified, allowing for a unique and reproducible solution. The methodology rests on established statistical principles.\n\nThe core of the problem lies in the application of the Pickands–Balkema–de Haan theorem. This theorem posits that, for a wide class of distributions with heavy tails, the conditional distribution of exceedances over a sufficiently high threshold converges to a Generalized Pareto Distribution (GPD). The data model specified, $R_t = \\mu + \\sigma \\cdot T_\\nu$, where $T_\\nu$ is a random variable from a Student's $t$-distribution with $\\nu$ degrees of freedom, is a canonical example of such a heavy-tailed process. For a Student's $t$-distribution with $\\nu$ degrees of freedom, the theoretical GPD tail index is $\\xi = 1/\\nu$. In this problem, with $\\nu=5$, the expected theoretical value for the tail index is $\\xi = 1/5 = 0.2$. The exercise is to estimate this parameter from a finite sample and observe its sensitivity to the threshold choice.\n\nThe computational procedure is as follows:\n\n1.  **Data Generation**: A synthetic dataset is created to model financial losses. A sample of $n=6000$ points is generated from a Student's $t$-distribution with $\\nu=5$ degrees of freedom, location $0$, and unit scale. These points, denoted $T_t$, are transformed into log-returns $R_t = \\mu + \\sigma \\cdot T_t$ using the provided parameters $\\mu=0$ and $\\sigma=0.01$. The corresponding losses are then defined as $L_t = -R_t$. The use of a fixed seed, $s=20240517$, ensures the absolute reproducibility of this generated data.\n\n2.  **Threshold Selection**: The problem requires an analysis of sensitivity to the threshold $u$. For this purpose, a series of thresholds is chosen based on the empirical quantiles of the generated loss data $L_t$. For each quantile level $q$ in the test suite $\\{0.80, 0.90, 0.95, 0.975, 0.99, 0.995, 0.999\\}$, the threshold $u$ is set to the value such that $P(L_t \\le u) = q$.\n\n3.  **Exceedance Calculation**: For each threshold $u$, the set of exceedances is compiled. These are the values $Y_i = L_i - u$ for all observations $L_i$ that are strictly greater than $u$. This set of $Y_i$ values represents the data to which the GPD will be fitted.\n\n4.  **GPD Parameter Estimation**: The tail index $\\xi$ of the GPD is estimated from the series of exceedances $\\{Y_i\\}$. The standard and most reliable method for this estimation is Maximum Likelihood Estimation (MLE), which will be employed here. The GPD is characterized by a shape parameter (the tail index $\\xi$) and a scale parameter $\\beta$. According to the POT model, exceedances are inherently positive, so the GPD's location parameter is theoretically $0$. This constraint is enforced during the fitting process (using `floc=0` in the `scipy` implementation) to improve the stability and theoretical correctness of the estimate. The MLE procedure computes the value of $\\xi$ that maximizes the probability of observing the collected exceedance data.\n\nThis entire process, from threshold selection to GPD fitting, is repeated for each quantile level $q$ in the test suite. The resulting sequence of estimated tail indices, $\\hat{\\xi}(q)$, demonstrates the practical effect of threshold choice on tail risk measurement. It is expected that as $q$ approaches $1$, the threshold $u$ increases, the GPD approximation becomes more accurate, and the estimated $\\hat{\\xi}$ should converge towards the theoretical value of $0.2$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t, genpareto\n\ndef solve():\n    \"\"\"\n    Computes the sensitivity of the GPD tail index estimate to the threshold choice\n    for a synthetic financial loss series based on the Peaks-Over-Threshold method.\n    \"\"\"\n    # --- Problem Parameters ---\n    n = 6000\n    mu = 0.0\n    sigma = 0.01\n    nu = 5.0\n    seed = 20240517\n    \n    # --- Test Suite ---\n    # The quantile levels for threshold selection define the test cases.\n    test_cases = [0.80, 0.90, 0.95, 0.975, 0.99, 0.995, 0.999]\n    \n    # --- Data Generation ---\n    # Use a specific random number generator for reproducibility as per problem statement.\n    rng = np.random.default_rng(seed)\n    \n    # Generate random variates from Student's t-distribution with nu degrees of freedom.\n    # T_nu is a standard t-distribution (location=0, scale=1).\n    T_nu = t.rvs(df=nu, size=n, random_state=rng)\n    \n    # Calculate log-returns R_t and losses L_t based on the generated variates.\n    R_t = mu + sigma * T_nu\n    L_t = -R_t\n    \n    # --- Main Logic: POT Analysis for each threshold ---\n    results = []\n    for q in test_cases:\n        # 1. Set the threshold 'u' as the empirical q-quantile of the losses.\n        u = np.quantile(L_t, q)\n        \n        # 2. Identify all losses exceeding the threshold and compute the exceedance values.\n        # Exceedances are defined as Y_i = L_i - u for all L_i  u.\n        exceedances = L_t[L_t  u] - u\n        \n        # 3. Fit a Generalized Pareto Distribution (GPD) to the exceedances.\n        # We use Maximum Likelihood Estimation (MLE), as implemented in scipy.\n        # The POT model implies a location parameter of 0 for exceedances, which we fix\n        # using the 'floc=0' argument for theoretical consistency and numerical stability.\n        # The 'c' shape parameter returned by the fit corresponds to the tail index 'xi'.\n        xi, _, _ = genpareto.fit(exceedances, floc=0)\n        \n        results.append(xi)\n\n    # --- Final Output Formatting ---\n    # The problem requires the results to be rounded to 6 decimal places and\n    # formatted as a comma-separated list within square brackets.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "While the Pickands–Balkema–de Haan theorem provides strong theoretical justification for using the GPD to model exceedances, it is always wise to validate a model's performance. This practice sets up a direct comparison between the POT-GPD approach and an alternative model, the stretched exponential (Weibull) distribution. By fitting both models to data and evaluating their predictive accuracy using out-of-sample log-likelihood, you will learn a quantitative method for model selection and gain a deeper appreciation for why and when the GPD is the preferred choice for tail modeling .",
            "id": "2418756",
            "problem": "Consider modeling large financial losses using the peaks-over-threshold method. Let $X \\ge 0$ denote a nonnegative loss random variable. Given a high threshold $u$, define the exceedances $Y = X - u \\mid X  u$. You will compare two tail models:\n\n- Peaks-over-threshold (POT) with the Generalized Pareto Distribution (GPD) for $Y$: this is justified by the Pickands–Balkema–de Haan theorem, which states that for a wide class of underlying distributions, the conditional exceedance distribution converges to the GPD as $u$ increases.\n\n- A simple stretched exponential model for the tail, implemented as a Weibull distribution for $X$ with shape $\\beta  0$ and scale $\\lambda  0$; under this model, the conditional density of $X$ given $X  u$ is $f_{\\text{Weibull}}(x;\\beta,\\lambda)/S_{\\text{Weibull}}(u;\\beta,\\lambda)$, where $f_{\\text{Weibull}}$ is the Weibull density and $S_{\\text{Weibull}}$ is the Weibull survival function.\n\nYou must implement maximum likelihood estimation for both models using only exceedances above a threshold $u$ computed from the training sample at a specified percentile $p \\in (0,1)$, and then evaluate out-of-sample predictive performance on a separate test sample using the same $u$.\n\nFoundational facts and definitions to use:\n\n- Generalized Pareto Distribution (GPD) for exceedances $Y \\ge 0$ with shape $\\xi \\in \\mathbb{R}$ and scale $\\sigma  0$ has density\n$$\ng(y;\\xi,\\sigma) = \\begin{cases}\n\\dfrac{1}{\\sigma}\\left(1 + \\dfrac{\\xi y}{\\sigma}\\right)^{-(1/\\xi+1)},  \\xi \\ne 0,\\; 1+\\dfrac{\\xi y}{\\sigma}0 \\\\\n\\dfrac{1}{\\sigma}\\exp\\!\\left(-\\dfrac{y}{\\sigma}\\right),  \\xi = 0,\\; y \\ge 0\n\\end{cases}\n$$\n\n- Weibull (stretched exponential) distribution for $X \\ge 0$ with shape $\\beta  0$ and scale $\\lambda  0$ has density and survival function\n$$\nf_{\\text{Weibull}}(x;\\beta,\\lambda) = \\dfrac{\\beta}{\\lambda}\\left(\\dfrac{x}{\\lambda}\\right)^{\\beta-1} \\exp\\!\\left(-\\left(\\dfrac{x}{\\lambda}\\right)^{\\beta}\\right), \\quad\nS_{\\text{Weibull}}(x;\\beta,\\lambda) = \\exp\\!\\left(-\\left(\\dfrac{x}{\\lambda}\\right)^{\\beta}\\right).\n$$\nTherefore, the conditional density of $X$ given $Xu$ is $f_{\\text{Weibull}}(x;\\beta,\\lambda)/S_{\\text{Weibull}}(u;\\beta,\\lambda)$ for $xu$.\n\nYou will use deterministic data generation by inverse transform, to avoid stochastic variability. For a sample size $n \\in \\mathbb{N}$, define a deterministic grid $u_k = (k-0.5)/n$ for $k = 1,2,\\dots,n$. To simulate from a distribution with cumulative distribution function $F$, set $x_k = F^{-1}(u_k)$.\n\nInverse cumulative distribution functions to use:\n\n- For GPD with $\\xi \\ne 0$ and $\\sigma  0$,\n$$\nF^{-1}(u) = \\dfrac{\\sigma}{\\xi}\\left((1-u)^{-\\xi} - 1\\right), \\quad u \\in (0,1).\n$$\nFor $\\xi = 0$ (exponential case), use $F^{-1}(u) = -\\sigma \\ln(1-u)$.\n\n- For Weibull with shape $\\beta0$ and scale $\\lambda0$,\n$$\nF^{-1}(u) = \\lambda\\left(-\\ln(1-u)\\right)^{1/\\beta}, \\quad u \\in (0,1).\n$$\n\nTask:\n\n- For each test case described below:\n    1. Generate a training sample $\\{x_k^{\\text{train}}\\}_{k=1}^{n_{\\text{train}}}$ and a test sample $\\{x_k^{\\text{test}}\\}_{k=1}^{n_{\\text{test}}}$ from the specified data generating process (DGP) using the inverse cumulative distribution function on the deterministic grid $u_k = (k - 0.5)/n$.\n    2. Compute the threshold $u$ as the empirical quantile at percentile $p$ of the training sample. Use the standard linear interpolation if needed.\n    3. Form the training exceedance set $\\mathcal{I}_{\\text{train}} = \\{i: x_i^{\\text{train}}  u\\}$ with exceedances $y_i^{\\text{train}} = x_i^{\\text{train}} - u$ for $i \\in \\mathcal{I}_{\\text{train}}$. Similarly, form the test exceedance set $\\mathcal{I}_{\\text{test}} = \\{j: x_j^{\\text{test}}  u\\}$ with exceedances $y_j^{\\text{test}} = x_j^{\\text{test}} - u$ for $j \\in \\mathcal{I}_{\\text{test}}$.\n    4. Fit the GPD parameters $(\\xi,\\sigma)$ by maximum likelihood using only $\\{y_i^{\\text{train}}\\}_{i \\in \\mathcal{I}_{\\text{train}}}$. Fit the Weibull parameters $(\\beta,\\lambda)$ by maximizing the conditional likelihood of $\\{x_i^{\\text{train}}: i \\in \\mathcal{I}_{\\text{train}}\\}$ under $X \\mid Xu$, i.e., the product of $f_{\\text{Weibull}}(x_i;\\beta,\\lambda)/S_{\\text{Weibull}}(u;\\beta,\\lambda)$ over $i \\in \\mathcal{I}_{\\text{train}}$.\n    5. Evaluate predictive performance on the test exceedances by computing the average log-likelihood under each model:\n        - POT (GPD): $\\dfrac{1}{|\\mathcal{I}_{\\text{test}}|} \\sum_{j \\in \\mathcal{I}_{\\text{test}}} \\log g\\!\\left(y_j^{\\text{test}}; \\hat{\\xi}, \\hat{\\sigma}\\right)$.\n        - Stretched exponential (Weibull conditional): $\\dfrac{1}{|\\mathcal{I}_{\\text{test}}|} \\sum_{j \\in \\mathcal{I}_{\\text{test}}} \\left[\\log f_{\\text{Weibull}}\\!\\left(x_j^{\\text{test}}; \\hat{\\beta}, \\hat{\\lambda}\\right) - \\log S_{\\text{Weibull}}\\!\\left(u; \\hat{\\beta}, \\hat{\\lambda}\\right)\\right]$.\n       If $|\\mathcal{I}_{\\text{test}}| = 0$, define both averages to be $0$.\n    6. Decision rule for this test case: output the integer $1$ if the POT model has strictly higher average log-likelihood than the stretched exponential model, and $0$ otherwise.\n\nTest suite:\n\n- Case A (stretched exponential DGP):\n    - DGP: Weibull with shape $\\beta = 0.7$ and scale $\\lambda = 1.0$.\n    - Training size $n_{\\text{train}} = 5000$, test size $n_{\\text{test}} = 5000$.\n    - Threshold percentile $p = 0.95$.\n\n- Case B (heavy-tailed GPD DGP):\n    - DGP: GPD with shape $\\xi = 0.3$ and scale $\\sigma = 1.0$.\n    - Training size $n_{\\text{train}} = 5000$, test size $n_{\\text{test}} = 5000$.\n    - Threshold percentile $p = 0.95$.\n\n- Case C (boundary light tail):\n    - DGP: Exponential, i.e., Weibull with $\\beta = 1.0$, $\\lambda = 1.0$ (equivalently GPD with $\\xi = 0$ and $\\sigma = 1.0$).\n    - Training size $n_{\\text{train}} = 5000$, test size $n_{\\text{test}} = 5000$.\n    - Threshold percentile $p = 0.99$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $\\left[\\text{Case A}, \\text{Case B}, \\text{Case C}\\right]$. For each case, print the integer $1$ if POT is better (higher average log-likelihood) and $0$ otherwise. For example, an output might look like $[0,1,0]$.",
            "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in the principles of extreme value theory, mathematically well-posed, and all necessary parameters and procedures are unambiguously defined. The task is a standard exercise in computational statistics, comparing the goodness-of-fit of two tail models—the Generalized Pareto Distribution (GPD) and a conditional Weibull distribution—on deterministically generated data.\n\nThe solution proceeds by implementing the steps outlined in the problem statement for each of the three test cases.\n\n1.  **Data Generation**: For each case, a training sample $\\{x_k^{\\text{train}}\\}_{k=1}^{n_{\\text{train}}}$ and a test sample $\\{x_k^{\\text{test}}\\}_{k=1}^{n_{\\text{test}}}$ are generated from the specified Data Generating Process (DGP). The generation is deterministic, using the inverse transform sampling method on a uniform grid $u_k = (k-0.5)/n$ for $k=1, \\dots, n$. Since $n_{\\text{train}} = n_{\\text{test}}$ and the generation process is identical, the training and test samples will be identical. This is a direct consequence of the problem's specification.\n\n2.  **Threshold Selection**: A high threshold $u$ is computed as the quantile of the training sample corresponding to the specified percentile $p$. Linear interpolation is used as is standard.\n\n3.  **Model Fitting via Maximum Likelihood Estimation (MLE)**: Both models are fitted using only the exceedances from the training sample, $\\{x_i^{\\text{train}}  u\\}$.\n\n    *   **Generalized Pareto Distribution (GPD)**: The GPD is fitted to the exceedances $y_i^{\\text{train}} = x_i^{\\text{train}} - u$. The parameters $(\\xi, \\sigma)$ are estimated by maximizing the GPD log-likelihood. The log-likelihood function for a sample of $N_u$ exceedances $\\{y_i\\}_{i=1}^{N_u}$ is given by:\n        $$\n        \\ell(\\xi, \\sigma; \\mathbf{y}) = \\sum_{i=1}^{N_u} \\log g(y_i; \\xi, \\sigma) = \n        \\begin{cases}\n        -N_u \\log \\sigma - \\left(\\frac{1}{\\xi} + 1\\right) \\sum_{i=1}^{N_u} \\log\\left(1 + \\frac{\\xi y_i}{\\sigma}\\right),  \\text{if } \\xi \\ne 0 \\\\\n        -N_u \\log \\sigma - \\frac{1}{\\sigma} \\sum_{i=1}^{N_u} y_i,  \\text{if } \\xi = 0\n        \\end{cases}\n        $$\n        subject to constraints $\\sigma  0$ and $1 + \\xi y_i / \\sigma  0$ for all $i$. The estimation is performed using the `fit` method from `scipy.stats.genpareto`, which provides a robust MLE implementation.\n\n    *   **Conditional Weibull Model**: The Weibull distribution parameters $(\\beta, \\lambda)$ are estimated by maximizing the conditional log-likelihood of the observations $x_i^{\\text{train}}$ given that they exceed the threshold $u$. The log-likelihood function to be maximized is:\n        $$\n        \\ell(\\beta, \\lambda; \\mathbf{x}, u) = \\sum_{i: x_i  u} \\left[ \\log f_{\\text{Weibull}}(x_i; \\beta, \\lambda) - \\log S_{\\text{Weibull}}(u; \\beta, \\lambda) \\right]\n        $$\n        Substituting the expressions for the Weibull density $f_{\\text{Weibull}}$ and survival function $S_{\\text{Weibull}}$, we get:\n        $$\n        \\ell(\\beta, \\lambda) = N_u \\log \\beta - N_u \\beta \\log \\lambda + (\\beta-1)\\sum_{i: x_i  u} \\log x_i - \\frac{1}{\\lambda^\\beta} \\sum_{i: x_i  u} (x_i^\\beta - u^\\beta)\n        $$\n        This function is maximized numerically with respect to $\\beta  0$ and $\\lambda  0$ using the `minimize` function from the `scipy.optimize` library. To handle the positivity constraints, the optimization is performed over the log-transformed parameters $\\log\\beta$ and $\\log\\lambda$.\n\n4.  **Performance Evaluation and Decision**: The predictive performance of each fitted model is evaluated on the test set exceedances. The metric is the average log-likelihood.\n    *   For the GPD model, the score is $\\mathcal{S}_{\\text{GPD}} = \\frac{1}{|\\mathcal{I}_{\\text{test}}|} \\sum_{j \\in \\mathcal{I}_{\\text{test}}} \\log g(y_j^{\\text{test}}; \\hat{\\xi}, \\hat{\\sigma})$.\n    *   For the Weibull model, the score is $\\mathcal{S}_{\\text{Weibull}} = \\frac{1}{|\\mathcal{I}_{\\text{test}}|} \\sum_{j \\in \\mathcal{I}_{\\text{test}}} \\left[ \\log f_{\\text{Weibull}}(x_j^{\\text{test}}; \\hat{\\beta}, \\hat{\\lambda}) - \\log S_{\\text{Weibull}}(u; \\hat{\\beta}, \\hat{\\lambda}) \\right]$.\n    \n    If there are no exceedances in the test set ($|\\mathcal{I}_{\\text{test}}| = 0$), both scores are defined to be $0$. The decision rule is to output $1$ if $\\mathcal{S}_{\\text{GPD}}  \\mathcal{S}_{\\text{Weibull}}$, and $0$ otherwise. This entire procedure is repeated for all three test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import genpareto, weibull_min\n\ndef solve():\n    \"\"\"\n    Solves the problem by running three test cases to compare POT-GPD and\n    conditional Weibull models for financial loss tail modeling.\n    \"\"\"\n\n    def generate_gpd_data(n, xi, sigma):\n        \"\"\"Generates deterministic data from a GPD distribution.\"\"\"\n        u_k = (np.arange(1, n + 1) - 0.5) / n\n        if np.abs(xi)  1e-9:\n            return -sigma * np.log(1 - u_k)\n        else:\n            return (sigma / xi) * (np.power(1 - u_k, -xi) - 1)\n\n    def generate_weibull_data(n, beta, lambda_):\n        \"\"\"Generates deterministic data from a Weibull distribution.\"\"\"\n        u_k = (np.arange(1, n + 1) - 0.5) / n\n        return lambda_ * np.power(-np.log(1 - u_k), 1.0 / beta)\n    \n    def neg_log_lik_weibull_cond(log_params, x_exceed, u):\n        \"\"\"\n        Calculates the negative conditional log-likelihood for the Weibull\n        distribution, given observations x  u.\n        Optimization is performed over log-transformed parameters.\n        \"\"\"\n        log_beta, log_lambda = log_params\n        beta = np.exp(log_beta)\n        lambda_ = np.exp(log_lambda)\n\n        if beta = 0 or lambda_ = 0:\n            return np.inf\n        \n        n_u = len(x_exceed)\n        log_x_exceed = np.log(x_exceed)\n\n        # Using properties of logarithms for numerical stability\n        # lambda_**(-beta) * x**beta = (x/lambda_)**beta = exp(beta * (log(x) - log(lambda_)))\n        log_lambda_val = np.log(lambda_)\n        \n        term1 = n_u * np.log(beta)\n        term2 = -n_u * beta * log_lambda_val\n        term3 = (beta - 1) * np.sum(log_x_exceed)\n        \n        v_i = np.power(x_exceed / lambda_, beta)\n        v_u = np.power(u / lambda_, beta)\n        \n        term4 = -np.sum(v_i)\n        term5 = n_u * v_u\n        \n        log_likelihood = term1 + term2 + term3 + term4 + term5\n        \n        if np.isnan(log_likelihood) or np.isinf(log_likelihood):\n            return np.inf\n\n        return -log_likelihood\n\n    def solve_case(dgp_type, dgp_params, n_train, n_test, p):\n        \"\"\"Processes a single test case.\"\"\"\n        # 1. Generate data\n        if dgp_type == 'weibull':\n            beta_true, lambda_true = dgp_params\n            train_data = generate_weibull_data(n_train, beta_true, lambda_true)\n            test_data = generate_weibull_data(n_test, beta_true, lambda_true)\n        elif dgp_type == 'gpd':\n            xi_true, sigma_true = dgp_params\n            train_data = generate_gpd_data(n_train, xi_true, sigma_true)\n            test_data = generate_gpd_data(n_test, xi_true, sigma_true)\n        else:\n            raise ValueError(\"Unknown DGP type\")\n\n        # 2. Compute threshold\n        u = np.quantile(train_data, p, interpolation='linear')\n\n        # 3. Form exceedance sets\n        train_exceed_indices = np.where(train_data  u)\n        x_train_exceed = train_data[train_exceed_indices]\n        y_train_exceed = x_train_exceed - u\n\n        test_exceed_indices = np.where(test_data  u)\n        x_test_exceed = test_data[test_exceed_indices]\n        y_test_exceed = x_test_exceed - u\n        \n        n_test_exceed = len(y_test_exceed)\n\n        if n_test_exceed == 0:\n            return 1 if 0.0  0.0 else 0\n\n        # 4. Fit models\n        # GPD model (POT)\n        try:\n            # Fit xi and sigma, with location fixed at 0\n            gpd_params = genpareto.fit(y_train_exceed, floc=0)\n            xi_hat, _, sigma_hat = gpd_params\n        except Exception:\n            # Fallback if fitting fails (unlikely)\n            xi_hat, sigma_hat = 0.0, 1.0\n\n        # Weibull model (Stretched Exponential)\n        # Initial guess for log-parameters\n        initial_guess = [np.log(1.0), np.log(np.mean(x_train_exceed))] \n        res = minimize(\n            neg_log_lik_weibull_cond,\n            x0=initial_guess,\n            args=(x_train_exceed, u),\n            method='Nelder-Mead'\n        )\n        if res.success:\n            log_beta_hat, log_lambda_hat = res.x\n            beta_hat, lambda_hat = np.exp(log_beta_hat), np.exp(log_lambda_hat)\n        else:\n            # Fallback if fitting fails\n            beta_hat, lambda_hat = 1.0, 1.0\n            \n        # 5. Evaluate predictive performance\n        # GPD (POT)\n        avg_loglik_gpd = np.mean(genpareto.logpdf(y_test_exceed, c=xi_hat, scale=sigma_hat, loc=0))\n\n        # Conditional Weibull\n        logpdf_weibull = weibull_min.logpdf(x_test_exceed, c=beta_hat, scale=lambda_hat)\n        logsf_weibull_u = weibull_min.logsf(u, c=beta_hat, scale=lambda_hat)\n        avg_loglik_weibull = np.mean(logpdf_weibull - logsf_weibull_u)\n\n        if np.isnan(avg_loglik_gpd): avg_loglik_gpd = -np.inf\n        if np.isnan(avg_loglik_weibull): avg_loglik_weibull = -np.inf\n\n        # 6. Decision rule\n        return 1 if avg_loglik_gpd  avg_loglik_weibull else 0\n\n    # Test suite definition\n    test_cases = [\n        # Case A\n        {'dgp_type': 'weibull', 'dgp_params': (0.7, 1.0), 'n_train': 5000, 'n_test': 5000, 'p': 0.95},\n        # Case B\n        {'dgp_type': 'gpd', 'dgp_params': (0.3, 1.0), 'n_train': 5000, 'n_test': 5000, 'p': 0.95},\n        # Case C\n        {'dgp_type': 'weibull', 'dgp_params': (1.0, 1.0), 'n_train': 5000, 'n_test': 5000, 'p': 0.99},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(\n            dgp_type=case['dgp_type'],\n            dgp_params=case['dgp_params'],\n            n_train=case['n_train'],\n            n_test=case['n_test'],\n            p=case['p']\n        )\n        results.append(result)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond parameter estimation, this final practice demonstrates how to use the POT framework for statistical inference to answer tangible questions in finance. You will tackle the classic question of whether a major financial crisis induced a structural change in market tail risk. This involves not only estimating the tail index $\\xi$ for two different periods but also calculating its standard error to construct a formal Wald test, allowing you to determine if the observed difference between the estimates is statistically significant .",
            "id": "2418723",
            "problem": "Consider two independent samples of simulated daily log-returns representing pre-crisis and post-crisis periods of a financial market. The peaks-over-threshold framework assumes that for a high threshold $u$, the conditional distribution of exceedances $Y = X - u \\mid X  u$ is approximately Generalized Pareto with shape parameter $\\xi$ and scale parameter $\\beta$. The Generalized Pareto Distribution (GPD) with parameters $(\\xi,\\beta)$ has support $y \\ge 0$ when $\\xi \\ge 0$ and $0 \\le y  -\\beta/\\xi$ when $\\xi  0$, and cumulative distribution function\n$$\nF(y \\mid \\xi,\\beta) = \n\\begin{cases}\n1 - \\left(1 + \\dfrac{\\xi y}{\\beta}\\right)^{-1/\\xi},  \\xi \\ne 0, \\\\\n1 - \\exp\\!\\left(-\\dfrac{y}{\\beta}\\right),  \\xi = 0,\n\\end{cases}\n\\quad \\text{for } \\beta  0.\n$$\nThe finite-sample negative log-likelihood for exceedances $(y_1,\\dots,y_k)$ under the GPD model is\n$$\n\\ell(\\xi,\\beta; y_{1:k}) =\n\\begin{cases}\nk \\log \\beta + \\left(1+\\dfrac{1}{\\xi}\\right)\\displaystyle\\sum_{i=1}^k \\log\\!\\left(1 + \\dfrac{\\xi y_i}{\\beta}\\right),  \\xi \\ne 0, \\\\\nk \\log \\beta + \\dfrac{1}{\\beta}\\displaystyle\\sum_{i=1}^k y_i,  \\xi = 0,\n\\end{cases}\n$$\nsubject to the support constraint $1 + \\xi y_i/\\beta  0$ for all $i$ and $\\beta  0$. The maximum likelihood estimator $(\\hat{\\xi},\\hat{\\beta})$ is defined as any pair that minimizes $\\ell(\\xi,\\beta; y_{1:k})$. The observed information matrix is the Hessian of $\\ell$ evaluated at $(\\hat{\\xi},\\hat{\\beta})$, and its inverse approximates the covariance matrix of $(\\hat{\\xi},\\hat{\\beta})$; in particular, the approximate variance of $\\hat{\\xi}$ is the $(1,1)$ entry of that inverse.\n\nYou will test whether the shape (tail index) parameter changed between pre- and post-crisis periods using a two-sided Wald test at significance level $\\alpha = 0.05$:\n$$\nH_0: \\ \\xi_{\\text{pre}} = \\xi_{\\text{post}}\n\\quad \\text{versus} \\quad\nH_1: \\ \\xi_{\\text{pre}} \\ne \\xi_{\\text{post}}.\n$$\nLet $\\hat{\\xi}_{\\text{pre}}$ and $\\hat{\\xi}_{\\text{post}}$ be the maximum likelihood estimators from the respective samples, with approximate standard errors $s_{\\text{pre}}$ and $s_{\\text{post}}$ obtained from the observed-information covariance matrices. The Wald statistic is\n$$\nZ = \\dfrac{\\hat{\\xi}_{\\text{pre}} - \\hat{\\xi}_{\\text{post}}}{\\sqrt{s_{\\text{pre}}^2 + s_{\\text{post}}^2}},\n$$\nand $H_0$ is rejected if $|Z| \\ge z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution.\n\nData generation for each period is as follows. Fix a base threshold level $u_0 = 0$ and a tail mixing probability $p_{\\text{tail}} \\in (0,1)$. Generate $n$ independent observations by concatenating:\n- A “body” of size $n - m$ with $m = \\lfloor p_{\\text{tail}} n \\rceil$ drawn from a continuous distribution supported on $(-\\infty, u_0]$; use the uniform distribution on $[-3, u_0]$.\n- A “tail” of size $m$ constructed as $u_0 + Y$, where $Y$ are independent GPD$(\\xi,\\beta)$ random variables with parameters $(\\xi,\\beta)$ specified for the period.\n\nFor each period and test case, set the threshold $u$ to the empirical $q$-quantile of the simulated sample, with $q = 0.9$. Define exceedances as $Y_i = X_i - u$ for all $X_i  u$.\n\nYour program must, for each test case below:\n1. Simulate the pre-crisis and post-crisis samples using the specified parameters and independent random seeds.\n2. Compute the empirical threshold $u$ at quantile level $q = 0.9$ separately for each period.\n3. Form the exceedances $Y_i$ above the respective thresholds.\n4. Compute the maximum likelihood estimator $(\\hat{\\xi},\\hat{\\beta})$ for each period by minimizing the exact finite-sample negative log-likelihood $\\ell(\\xi,\\beta; y_{1:k})$ under the support constraint.\n5. Approximate the standard error of $\\hat{\\xi}$ for each period using the inverse observed information matrix at the maximum likelihood estimator.\n6. Perform the two-sided Wald test of $H_0: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$ at significance level $\\alpha = 0.05$.\n7. Output a boolean indicating whether there is a statistically significant change in the tail index (output true if $H_0$ is rejected, and false otherwise).\n\nTest suite (each tuple corresponds to one test case in the order: $(\\text{seed}_{\\text{pre}}, \\text{seed}_{\\text{post}}, n_{\\text{pre}}, n_{\\text{post}}, \\xi_{\\text{pre}}, \\beta_{\\text{pre}}, \\xi_{\\text{post}}, \\beta_{\\text{post}})$), with common $u_0 = 0$, $p_{\\text{tail}} = 0.25$, $q = 0.9$, and $\\alpha = 0.05$:\n- Case A: $(12345, 54321, 5000, 5000, 0.2, 1.0, 0.6, 1.0)$.\n- Case B: $(111, 222, 4000, 4000, 0.2, 1.0, 0.2, 1.0)$.\n- Case C: $(333, 444, 6000, 6000, 0.01, 1.0, 0.0, 1.0)$.\n- Case D: $(555, 666, 6000, 6000, -0.15, 1.2, 0.15, 1.2)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example: \"[true,false,true,false]\". The booleans must be all lowercase.",
            "solution": "The problem requires performing a statistical hypothesis test to determine if the tail behavior of financial returns, modeled by a Generalized Pareto Distribution (GPD), has changed between two periods. This is a standard application of Extreme Value Theory (EVT) in quantitative finance. The validation process confirms that the problem statement is scientifically sound, well-posed, and contains all necessary information for a unique, verifiable solution.\n\nThe solution proceeds systematically through data simulation, parameter estimation via Maximum Likelihood, and hypothesis testing using a Wald test.\n\n**1. Data Simulation and Preprocessing**\n\nThe data for each period (pre- and post-crisis) are generated from a mixture distribution designed to have a specified tail behavior. For a sample of size $n$, a proportion $p_{\\text{tail}}$ of the data points constitute the \"tail\" and are drawn from a GPD. The remaining points form the \"body\" of the distribution.\n\n-   **Body**: $n - m$ samples are drawn from a Uniform distribution on $[-3, u_0]$, where $m = \\lfloor p_{\\text{tail}} n \\rceil$ and the base threshold is $u_0 = 0$.\n-   **Tail**: $m$ samples are generated as $u_0 + Y_i$, where $Y_i$ are independent GPD$(\\xi, \\beta)$ random variates.\n\nTo generate a random variate $Y$ from a GPD$(\\xi, \\beta)$, we use the inverse transform sampling method. The quantile function, $F^{-1}(p)$, is derived by inverting the GPD cumulative distribution function (CDF), $F(y)$. Given a uniform random variate $U \\sim U(0,1)$, a GPD variate $Y$ is generated as:\n$$\nY = F^{-1}(U) = \n\\begin{cases}\n\\dfrac{\\beta}{\\xi} \\left( (1-U)^{-\\xi} - 1 \\right),  \\xi \\ne 0, \\\\\n-\\beta \\log(1-U),  \\xi = 0.\n\\end{cases}\n$$\nSince $1-U$ is also uniformly distributed on $(0,1)$, this is equivalent to using $U$ directly in the expressions.\n\nAfter simulating the full sample $X = \\{X_1, \\dots, X_n\\}$, we apply the Peaks-Over-Threshold (POT) method. We establish a high threshold $u$ as the empirical $q$-quantile of the sample, with $q=0.9$. The exceedances are then defined as the positive values $Y_i = X_i - u$ for all $X_i  u$. These exceedances form the dataset for fitting the GPD model.\n\n**2. Maximum Likelihood Estimation (MLE)**\n\nThe parameters $(\\xi, \\beta)$ of the GPD are estimated for each period by maximizing the log-likelihood function or, equivalently, minimizing the negative log-likelihood function, $\\ell(\\xi, \\beta)$. For a set of $k$ exceedances $\\{y_1, \\dots, y_k\\}$, the negative log-likelihood is given by:\n$$\n\\ell(\\xi,\\beta; y_{1:k}) =\n\\begin{cases}\nk \\log \\beta + \\left(1+\\dfrac{1}{\\xi}\\right)\\displaystyle\\sum_{i=1}^k \\log\\!\\left(1 + \\dfrac{\\xi y_i}{\\beta}\\right),  \\xi \\ne 0, \\\\\nk \\log \\beta + \\dfrac{1}{\\beta}\\displaystyle\\sum_{i=1}^k y_i,  \\xi = 0.\n\\end{cases}\n$$\nThis minimization is a numerical optimization problem. The function for $\\xi \\ne 0$ converges to the function for $\\xi = 0$ as $\\xi \\to 0$. To ensure numerical stability, we implement the objective function with a conditional branch, using the limiting form for values of $\\xi$ close to zero (e.g., $|\\xi|  10^{-8}$).\n\nThe minimization is subject to constraints: $\\beta  0$, and for the logarithmic term to be well-defined, $1 + \\xi y_i/\\beta  0$ for all exceedances $y_i$. The latter constraint implies $y_i  -\\beta/\\xi$ when $\\xi  0$. These constraints are enforced within the objective function by returning a large value (representing infinity) if they are violated, effectively creating a barrier that guides the optimizer towards the valid parameter space. The optimization is performed using the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm provided by `scipy.optimize.minimize`.\n\n**3. Standard Error Approximation**\n\nAccording to large-sample theory for MLE, the asymptotic covariance matrix of the estimators $(\\hat{\\xi}, \\hat{\\beta})$ is approximated by the inverse of the observed information matrix, $I(\\hat{\\xi}, \\hat{\\beta})$. The observed information matrix is the Hessian matrix of the negative log-likelihood function evaluated at the MLE:\n$$\n\\text{Cov}(\\hat{\\xi}, \\hat{\\beta}) \\approx [I(\\hat{\\xi}, \\hat{\\beta})]^{-1} = \\left[ \\nabla^2 \\ell(\\hat{\\xi}, \\hat{\\beta}) \\right]^{-1}.\n$$\nThe BFGS algorithm, being a quasi-Newton method, computes an approximation to the inverse of the Hessian matrix as part of its procedure. This approximation is readily available from the optimization result. The variance of the shape parameter estimator, $\\text{Var}(\\hat{\\xi})$, is approximated by the top-left element of this inverse Hessian matrix. The corresponding standard error is its square root:\n$$\ns_{\\hat{\\xi}} = \\sqrt{\\left( [I(\\hat{\\xi}, \\hat{\\beta})]^{-1} \\right)_{1,1}}.\n$$\n\n**4. Wald Test for Parameter Equality**\n\nTo test the hypothesis of no change in the tail index, $H_0: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$, against the alternative $H_1: \\xi_{\\text{pre}} \\ne \\xi_{\\text{post}}$, we use a two-sided Wald test. The test statistic is constructed from the MLEs and their standard errors from the two independent samples (pre-crisis and post-crisis):\n$$\nZ = \\dfrac{\\hat{\\xi}_{\\text{pre}} - \\hat{\\xi}_{\\text{post}}}{\\sqrt{s_{\\text{pre}}^2 + s_{\\text{post}}^2}}.\n$$\nUnder the null hypothesis, the statistic $Z$ follows an asymptotic standard normal distribution, $N(0,1)$. We reject $H_0$ at a significance level $\\alpha$ if the absolute value of the observed statistic, $|Z|$, exceeds the critical value $z_{1-\\alpha/2}$, which is the $(1-\\alpha/2)$-quantile of the standard normal distribution. For $\\alpha = 0.05$, the critical value is $z_{0.975} \\approx 1.96$.\n\nThe entire procedure is encapsulated in a program that iterates through the provided test cases, performing simulation, estimation, and testing for each, and reports whether the null hypothesis is rejected.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the GPD tail index comparison.\n    \"\"\"\n    test_cases = [\n        # (seed_pre, seed_post, n_pre, n_post, xi_pre, beta_pre, xi_post, beta_post)\n        (12345, 54321, 5000, 5000, 0.2, 1.0, 0.6, 1.0),\n        (111, 222, 4000, 4000, 0.2, 1.0, 0.2, 1.0),\n        (333, 444, 6000, 6000, 0.01, 1.0, 0.0, 1.0),\n        (555, 666, 6000, 6000, -0.15, 1.2, 0.15, 1.2),\n    ]\n\n    common_params = {\n        'u0': 0.0,\n        'p_tail': 0.25,\n        'q': 0.9,\n        'alpha': 0.05,\n    }\n\n    results = []\n    for case in test_cases:\n        seed_pre, seed_post, n_pre, n_post, xi_pre, beta_pre, xi_post, beta_post = case\n        \n        # Fit pre-crisis period\n        xi_hat_pre, se_pre = fit_gpd_for_period(\n            seed_pre, n_pre, xi_pre, beta_pre, common_params\n        )\n        \n        # Fit post-crisis period\n        xi_hat_post, se_post = fit_gpd_for_period(\n            seed_post, n_post, xi_post, beta_post, common_params\n        )\n        \n        # Perform Wald test\n        wald_statistic = (xi_hat_pre - xi_hat_post) / np.sqrt(se_pre**2 + se_post**2)\n        critical_value = norm.ppf(1 - common_params['alpha'] / 2)\n        \n        reject_h0 = np.abs(wald_statistic) = critical_value\n        results.append(str(reject_h0).lower())\n\n    print(f\"[{','.join(results)}]\")\n\ndef fit_gpd_for_period(seed, n, xi, beta, params):\n    \"\"\"\n    Simulates data and fits a GPD model for a single period.\n    Returns the estimated shape parameter and its standard error.\n    \"\"\"\n    X = simulate_data(seed, n, xi, beta, params['p_tail'], params['u0'])\n    \n    u = np.quantile(X, params['q'])\n    Y = X[X  u] - u\n    \n    # It's possible, though unlikely, that there are no exceedances\n    if len(Y) == 0:\n        raise ValueError(\"No exceedances found for GPD fitting.\")\n\n    # Objective function: negative log-likelihood for GPD\n    def nll_gpd(p, y_data):\n        _xi, _beta = p\n        \n        # Constraint: beta  0\n        if _beta = 1e-6:\n            return np.inf\n            \n        # Support constraint: 1 + xi*y/beta  0\n        terms = 1 + _xi * y_data / _beta\n        if np.any(terms = 0):\n            return np.inf\n\n        k = len(y_data)\n        \n        if abs(_xi)  1e-8:\n            # Case xi - 0 (Exponential distribution)\n            neg_log_lik = k * np.log(_beta) + np.sum(y_data) / _beta\n        else:\n            # Case xi != 0\n            log_of_terms = np.log(terms)\n            neg_log_lik = k * np.log(_beta) + (1 + 1/_xi) * np.sum(log_of_terms)\n\n        if not np.isfinite(neg_log_lik):\n            return np.inf\n            \n        return neg_log_lik\n\n    # Initial guess for optimization\n    initial_guess = [0.1, np.std(Y) if len(Y)  1 else 1.0]\n\n    # Run optimizer to find MLE\n    res = minimize(\n        nll_gpd,\n        initial_guess,\n        args=(Y,),\n        method='BFGS',\n        options={'gtol': 1e-8}\n    )\n\n    if not res.success:\n        # A failed optimization might require more robust initial values or optimizer choice\n        # For this problem, we assume `BFGS` with this initial guess suffices.\n        pass\n\n    xi_hat, _ = res.x\n    \n    # Approximate variance from the inverse Hessian\n    var_xi = res.hess_inv[0, 0]\n    \n    # Handle potential numerical instability if variance is negative\n    if var_xi  0:\n        # This shouldn't happen with BFGS, which maintains a positive definite Hess approx.\n        # But as a safeguard:\n        var_xi = np.abs(var_xi)\n\n    se_xi = np.sqrt(var_xi)\n    \n    return xi_hat, se_xi\n\ndef simulate_data(seed, n, xi, beta, p_tail, u0):\n    \"\"\"\n    Generates a sample from the mixture distribution.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    m = int(round(p_tail * n))\n    n_body = n - m\n    \n    # Generate the \"body\" of the distribution\n    body = rng.uniform(-3.0, u0, size=n_body)\n    \n    # Generate the \"tail\" using GPD inverse transform sampling\n    U = rng.uniform(size=m)\n    if abs(xi)  1e-8:\n        tail_excess = -beta * np.log(U)\n    else:\n        tail_excess = (beta / xi) * (np.power(U, -xi) - 1)\n        \n    tail = u0 + tail_excess\n    \n    return np.concatenate((body, tail))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}