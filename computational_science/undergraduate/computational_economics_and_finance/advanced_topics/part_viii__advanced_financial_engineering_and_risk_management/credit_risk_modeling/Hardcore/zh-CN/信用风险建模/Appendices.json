{
    "hands_on_practices": [
        {
            "introduction": "逻辑回归是信用风险建模的基石，常用于预测违约概率。本练习将引导你从第一性原理出发，构建一个逻辑回归分类器，而不仅仅是调用现成的库。通过亲手实现基于最大似然估计和正则化的模型，你将深入理解这些分类器在底层是如何工作的，这对于在实际金融应用中进行模型验证和定制至关重要。",
            "id": "2407536",
            "problem": "您的任务是使用逻辑回归实现一个二元分类器，根据财务顾问的客户投诉历史和就业不稳定性，预测其在下一年是否会受到监管机构的处罚。目标是在计算经济学和金融学的背景下，从第一性原理出发构建模型，使用针对二元结果的最大似然框架和用于正则化的惩罚概念。您的程序必须是一个完整、可运行的脚本，使用所提供的数据和测试套件执行以下步骤，并以指定格式输出汇总结果。\n\n您必须从基本原理出发构建分类器：将二元结果建模为一个伯努利随机变量，其概率取决于经逻辑连接函数转换的线性指数，并通过最大化观测数据的似然来估计参数。应用金融风险建模中典型的可解释性约束：包含一个截距项，仅使用训练集的统计数据将特征标准化为零均值和单位方差，并对斜率系数使用各向同性平方范数惩罚（岭正则化）以防止过拟合。截距项不得被惩罚。不得使用任何快捷公式或现成的模型拟合程序；推导参数的更新方程，并使用牛顿法实现一个数值稳定的求解器。\n\n数据描述：\n- 每位顾问的特征：\n  - $c$：客户投诉次数（计数），\n  - $r$：年均投诉次数（非负实数），\n  - $s$：投诉平均严重程度（归一化到 $[0,1]$），\n  - $f$：过去 $5$ 年内更换公司的次数（计数），\n  - $t$：在当前公司的任职年限（非负实数）。\n- 目标：$y \\in \\{0,1\\}$ 表示该顾问在下一年内是否受到处罚。\n\n训练数据集（每个项目为 $(c,r,s,f,t; y)$）：\n- $(0,\\, 0.0,\\, 0.1,\\, 0,\\, 15;\\, 0)$\n- $(1,\\, 0.1,\\, 0.2,\\, 0,\\, 12;\\, 0)$\n- $(2,\\, 0.2,\\, 0.3,\\, 1,\\, 10;\\, 0)$\n- $(0,\\, 0.0,\\, 0.2,\\, 1,\\, 18;\\, 0)$\n- $(1,\\, 0.1,\\, 0.1,\\, 0,\\, 20;\\, 0)$\n- $(2,\\, 0.2,\\, 0.15,\\, 1,\\, 16;\\, 0)$\n- $(1,\\, 0.0,\\, 0.25,\\, 0,\\, 14;\\, 0)$\n- $(0,\\, 0.05,\\, 0.05,\\, 0,\\, 22;\\, 0)$\n- $(3,\\, 0.5,\\, 0.4,\\, 1,\\, 9;\\, 0)$\n- $(5,\\, 0.8,\\, 0.7,\\, 2,\\, 5;\\, 1)$\n- $(7,\\, 1.2,\\, 0.9,\\, 3,\\, 3;\\, 1)$\n- $(4,\\, 0.6,\\, 0.6,\\, 2,\\, 6;\\, 1)$\n- $(8,\\, 1.5,\\, 0.85,\\, 4,\\, 2;\\, 1)$\n- $(6,\\, 1.0,\\, 0.8,\\, 3,\\, 4;\\, 1)$\n- $(9,\\, 1.8,\\, 0.95,\\, 5,\\, 1;\\, 1)$\n- $(3,\\, 0.4,\\, 0.55,\\, 2,\\, 8;\\, 1)$\n\n建模和训练要求：\n- 使用仅来自训练集的数据，将每个特征 $c$、$r$、$s$、$f$ 和 $t$ 标准化为零均值和单位方差。对于每个特征 $x$，其标准化值计算为 $(x - \\mu_x)/\\sigma_x$，其中 $\\mu_x$ 和 $\\sigma_x$ 是该特征的训练集均值和标准差。如果任何 $\\sigma_x$ 等于 $0$，则将其替换为 $1$ 以避免除以零。\n- 用一个全为 1 的截距列来增广标准化特征矩阵。\n- 使用牛顿法，通过最小化带惩罚的负对数似然来拟合逻辑回归参数，停止准则为参数更新量的欧几里得范数小于 $10^{-8}$ 或达到 $50$ 次迭代，以先到者为准。\n- 使用强度为 $\\lambda \\ge 0$ 的岭惩罚，仅应用于斜率系数（截距项除外）。\n\n分类规则：\n- 对于一个具有特征 $(c,r,s,f,t)$ 的给定顾问，计算其预测的处罚概率，结果为 $[0,1]$ 内的一个小数。如果概率至少为 $0.5$，则分类为受处罚；否则分类为未受处罚。所有报告的概率必须是小数；不要使用百分号。\n\n测试套件：\n您的程序必须按规定多次训练模型，并返回以下四种情况的分类结果。每种情况提供一个惩罚水平 $\\lambda$ 和一个测试顾问的特征 $(c,r,s,f,t)$。对于每种情况，使用提供的 $\\lambda$ 在训练数据上拟合模型，然后使用 $0.5$ 的阈值对给定的测试顾问进行分类。\n\n- 情况 $1$：$\\lambda = 1.0$，$(c,r,s,f,t) = (6,\\, 1.0,\\, 0.8,\\, 3,\\, 3)$\n- 情况 $2$：$\\lambda = 0.0$，$(c,r,s,f,t) = (0,\\, 0.0,\\, 0.1,\\, 0,\\, 15)$\n- 情况 $3$：$\\lambda = 10.0$，$(c,r,s,f,t) = (10,\\, 1.5,\\, 0.9,\\, 5,\\, 2)$\n- 情况 $4$：$\\lambda = 0.5$，$(c,r,s,f,t) = (1,\\, 0.1,\\, 0.2,\\, 0,\\, 12)$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含分类结果，格式为方括号内以逗号分隔的整数列表，顺序与上述情况一致，例如 `\"[1,0,1,0]\"`。",
            "solution": "所述问题是有效的。它在科学上基于统计学习的原理，特别是带岭正则化的逻辑回归。该设置是适定的，提供了所有必要的数据、一个清晰的目标函数以及一个指定的优化数值方法。这是一个计算金融领域中的客观且可形式化的问题。我们将从第一性原理出发推导解决方案。\n\n设训练数据集为 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^D$ 是第 $i$ 位顾问的特征向量，$y_i \\in \\{0, 1\\}$ 是对应的二元结果（受处罚或未受处罚）。这里，$N=16$ 是训练观测值的数量，$D=5$ 是特征的数量。\n\n逻辑回归模型假定，正向结果（$y_i=1$）的概率由应用于特征线性组合的逻辑（sigmoid）函数给出：\n$$\nP(y_i=1 | \\mathbf{x}_i^*; \\boldsymbol{\\beta}) = p_i = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}\n$$\n这里，$\\mathbf{x}_i^*$ 是第 $i$ 个特征向量，其前端增补了一个 $1$ 以容纳截距项，因此 $\\mathbf{x}_i^* \\in \\mathbb{R}^{D+1}$。向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{D+1}$ 包含模型参数，其中 $\\beta_0$ 是截距，$\\beta_1, \\dots, \\beta_D$ 是斜率系数。\n\n参数 $\\boldsymbol{\\beta}$ 是通过最大化观测数据的对数似然来估计的，并为正则化添加了一个惩罚项。单个观测的似然由伯努利概率质量函数给出：$L_i(\\boldsymbol{\\beta}) = p_i^{y_i} (1-p_i)^{1-y_i}$。$N$ 个独立观测的总对数似然为：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\ln(L_i(\\boldsymbol{\\beta})) = \\sum_{i=1}^N [y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]\n$$\n将 $p_i$ 的表达式以及 $\\ln(1-p_i) = \\ln(1-\\sigma(z_i)) = -z_i - \\ln(1+e^{z_i})$ 和 $\\ln(p_i) = \\ln(\\sigma(z_i)) = -\\ln(1+e^{-z_i}) = z_i - \\ln(1+e^{z_i})$（其中 $z_i=\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}$）代入，我们可以将对数似然简化为更方便的形式：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N [y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) - \\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}})]\n$$\n我们的目标是最小化带惩罚的*负*对数似然。惩罚项是斜率系数的平方 $\\ell_2$ 范数（岭正则化），这会抑制过大的参数值并有助于防止过拟合。截距 $\\beta_0$ 不被惩罚。要最小化的目标函数是：\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\sum_{j=1}^D \\beta_j^2 = \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^T \\mathbf{I}' \\boldsymbol{\\beta}\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，$\\mathbf{I}'$ 是一个 $(D+1) \\times (D+1)$ 的对角矩阵，其中 $I'_{00} = 0$ 且对于 $j=1, \\dots, D$ 有 $I'_{jj} = 1$。\n\n为了最小化 $J(\\boldsymbol{\\beta})$，我们使用牛顿法，这是一种迭代算法，其更新规则如下：\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla J(\\boldsymbol{\\beta}^{(t)})\n$$\n其中 $\\nabla J$ 是目标函数的梯度，$\\mathbf{H}$ 是其海森矩阵。\n\n首先，我们推导梯度 $\\nabla J(\\boldsymbol{\\beta})$。未惩罚部分的梯度的第 $j$ 个分量是：\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left( \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] \\right) = \\sum_{i=1}^N \\left[ \\frac{e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}} x_{ij}^* - y_i x_{ij}^* \\right] = \\sum_{i=1}^N (p_i - y_i) x_{ij}^*\n$$\n在矩阵表示法中，其中 $\\mathbf{X}^*$ 是 $N \\times (D+1)$ 的增广设计矩阵，$\\mathbf{p}$ 是概率向量，$\\mathbf{y}$ 是结果向量，这可以表示为 $\\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y})$。\n惩罚项的梯度是 $\\lambda \\mathbf{I}' \\boldsymbol{\\beta}$。因此，完整梯度为：\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{I}' \\boldsymbol{\\beta}\n$$\n接下来，我们推导海森矩阵 $\\mathbf{H}(\\boldsymbol{\\beta})$。元素 $H_{jk}$ 是 $\\frac{\\partial^2 J(\\boldsymbol{\\beta})}{\\partial \\beta_j \\partial \\beta_k}$。对于未惩罚部分：\n$$\n\\frac{\\partial^2}{\\partial \\beta_j \\partial \\beta_k} \\left( \\sum_{i=1}^N \\dots \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^N p_i x_{ij}^* \\right) = \\sum_{i=1}^N x_{ij}^* \\frac{\\partial p_i}{\\partial \\beta_k}\n$$\n由于 $\\frac{\\partial p_i}{\\partial \\beta_k} = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})(1 - \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})) x_{ik}^* = p_i(1-p_i)x_{ik}^*$，未惩罚部分的海森矩阵是：\n$$\nH_{jk} = \\sum_{i=1}^N x_{ij}^* p_i(1-p_i) x_{ik}^*\n$$\n在矩阵形式中，这是 $\\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^*$，其中 $\\mathbf{W}$ 是一个 $N \\times N$ 的对角矩阵，其对角元素为 $W_{ii} = p_i(1-p_i)$。\n惩罚项的海森矩阵就是 $\\lambda \\mathbf{I}'$。完整的海森矩阵是：\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^* + \\lambda \\mathbf{I}'\n$$\n对于 $\\lambda > 0$，此海森矩阵是正定的，这确保了牛顿法存在唯一最小值和数值稳定性。\n\n实现将按以下步骤进行：\n1.  从训练数据中计算每个（共 $D=5$ 个）特征的均值 $\\boldsymbol{\\mu}$ 和标准差 $\\boldsymbol{\\sigma}$。如果任何 $\\sigma_j = 0$，则将其设为 $1$。\n2.  标准化训练数据矩阵 $\\mathbf{X}$ 得到 $\\mathbf{X}_{std}$，其中每一列都具有零均值和单位方差。\n3.  用一个全为 1 的列增广 $\\mathbf{X}_{std}$，形成设计矩阵 $\\mathbf{X}^*$。\n4.  对于每个测试用例，初始化参数向量 $\\boldsymbol{\\beta}$（例如，初始化为零）。\n5.  迭代应用牛顿法更新：\n    a. 计算概率 $\\mathbf{p} = \\sigma(\\mathbf{X}^* \\boldsymbol{\\beta}^{(t)})$ 和权重矩阵 $\\mathbf{W}^{(t)}$。\n    b. 计算梯度 $\\nabla J(\\boldsymbol{\\beta}^{(t)})$ 和海森矩阵 $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})$。\n    c. 求解线性系统 $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\Delta \\boldsymbol{\\beta} = - \\nabla J(\\boldsymbol{\\beta}^{(t)})$ 以获得更新步长 $\\Delta \\boldsymbol{\\beta}$。\n    d. 更新参数：$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$。\n    e. 如果更新量 $\\|\\Delta \\boldsymbol{\\beta}\\|_2$ 的欧几里得范数小于 $10^{-8}$ 或达到 $50$ 次迭代，则终止。\n6.  对于每个测试向量 $\\mathbf{x}_{\\text{test}}$，使用训练集的 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\sigma}$ 对其进行标准化，将其增广为 $\\mathbf{x}_{\\text{test}}^*$，计算预测概率 $p_{\\text{test}} = \\sigma(\\mathbf{x}_{\\text{test}}^{*T} \\boldsymbol{\\beta}_{\\text{final}})$，如果 $p_{\\text{test}} \\ge 0.5$ 则分类为 $1$，否则分类为 $0$。\n最终输出将是这些分类结果的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and solves the logistic regression problem as specified.\n    \"\"\"\n    # Training dataset from the problem statement\n    # (c, r, s, f, t; y)\n    training_data = np.array([\n        [0.0, 0.0, 0.1, 0.0, 15.0, 0.0],\n        [1.0, 0.1, 0.2, 0.0, 12.0, 0.0],\n        [2.0, 0.2, 0.3, 1.0, 10.0, 0.0],\n        [0.0, 0.0, 0.2, 1.0, 18.0, 0.0],\n        [1.0, 0.1, 0.1, 0.0, 20.0, 0.0],\n        [2.0, 0.2, 0.15, 1.0, 16.0, 0.0],\n        [1.0, 0.0, 0.25, 0.0, 14.0, 0.0],\n        [0.0, 0.05, 0.05, 0.0, 22.0, 0.0],\n        [3.0, 0.5, 0.4, 1.0, 9.0, 0.0],\n        [5.0, 0.8, 0.7, 2.0, 5.0, 1.0],\n        [7.0, 1.2, 0.9, 3.0, 3.0, 1.0],\n        [4.0, 0.6, 0.6, 2.0, 6.0, 1.0],\n        [8.0, 1.5, 0.85, 4.0, 2.0, 1.0],\n        [6.0, 1.0, 0.8, 3.0, 4.0, 1.0],\n        [9.0, 1.8, 0.95, 5.0, 1.0, 1.0],\n        [3.0, 0.4, 0.55, 2.0, 8.0, 1.0],\n    ])\n\n    X_train_raw = training_data[:, :-1]\n    y_train = training_data[:, -1].reshape(-1, 1)\n\n    # Test cases from the problem statement\n    # Each is (lambda, (c, r, s, f, t))\n    test_cases = [\n        (1.0, (6.0, 1.0, 0.8, 3.0, 3.0)),\n        (0.0, (0.0, 0.0, 0.1, 0.0, 15.0)),\n        (10.0, (10.0, 1.5, 0.9, 5.0, 2.0)),\n        (0.5, (1.0, 0.1, 0.2, 0.0, 12.0)),\n    ]\n\n    # Step 1: Standardize features using training data statistics\n    mean_X = np.mean(X_train_raw, axis=0)\n    std_X = np.std(X_train_raw, axis=0)\n    # As per instructions, replace std=0 with 1 to avoid division by zero\n    std_X[std_X == 0] = 1.0\n    \n    X_train_std = (X_train_raw - mean_X) / std_X\n\n    # Step 2: Augment the standardized feature matrix with an intercept column\n    intercept_col = np.ones((X_train_std.shape[0], 1))\n    X_train_aug = np.hstack((intercept_col, X_train_std))\n\n    def numerically_stable_sigmoid(z):\n        \"\"\"Computes the sigmoid function in a numerically stable way.\"\"\"\n        # Using a vectorized implementation for efficiency\n        # This prevents overflow with large positive z and underflow with large negative z\n        pos_mask = (z >= 0)\n        neg_mask = (z  0)\n        p = np.zeros_like(z, dtype=float)\n        p[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        p[neg_mask] = np.exp(z[neg_mask]) / (1.0 + np.exp(z[neg_mask]))\n        return p\n\n    results = []\n    \n    num_features = X_train_aug.shape[1]\n\n    for lambda_val, test_features_raw in test_cases:\n        # Step 3: Fit the logistic regression model using Newton's method\n        \n        # Initialize parameters (beta) to zeros\n        beta = np.zeros((num_features, 1))\n        \n        # Regularization matrix I'\n        # Diagonal matrix with 0 for intercept and lambda for slopes\n        penalty_matrix = lambda_val * np.eye(num_features)\n        penalty_matrix[0, 0] = 0.0\n        \n        max_iter = 50\n        tolerance = 1e-8\n        \n        for i in range(max_iter):\n            # Calculate linear predictors and probabilities\n            z = X_train_aug @ beta\n            p = numerically_stable_sigmoid(z)\n            \n            # W is a diagonal matrix of weights p_i * (1 - p_i)\n            # Implemented with a 1D array for efficiency\n            weights = p * (1 - p)\n            W = np.diag(weights.flatten())\n            \n            # Calculate gradient of the penalized negative log-likelihood\n            # grad = X^T * (p - y) + lambda * I' * beta\n            gradient = X_train_aug.T @ (p - y_train) + penalty_matrix @ beta\n            \n            # Calculate Hessian of the penalized negative log-likelihood\n            # H = X^T * W * X + lambda * I'\n            hessian = X_train_aug.T @ W @ X_train_aug + penalty_matrix\n            \n            # Solve the linear system H * delta_beta = -gradient\n            # This is the Newton-Raphson update step\n            try:\n                # np.linalg.solve is more stable and efficient than inverting the matrix\n                delta_beta = np.linalg.solve(hessian, -gradient)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular, for robustness\n                delta_beta = np.linalg.pinv(hessian) @ -gradient\n            \n            # Update parameters\n            beta += delta_beta\n            \n            # Check for convergence\n            if np.linalg.norm(delta_beta)  tolerance:\n                break\n\n        # Step 4: Classify the test advisor\n        x_test_std = (np.array(test_features_raw) - mean_X) / std_X\n        x_test_aug = np.hstack(([1.0], x_test_std))\n        \n        # Calculate predicted probability for the test case\n        prob_test = numerically_stable_sigmoid(x_test_aug @ beta).item()\n        \n        # Classify based on the 0.5 threshold\n        classification = 1 if prob_test >= 0.5 else 0\n        results.append(classification)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在预测违约之后，我们需要关心接下来会发生什么。本练习引入“治愈率”这一概念，即已违约贷款恢复正常履约状态的概率。你将运用生存分析的原理来模拟这一随时间变化的动态过程，从而对信贷损失获得更精细的认识，并学习处理在金融建模中常见的时间依赖结果和右删失数据。",
            "id": "2385819",
            "problem": "要求您为违约贷款建立一个简约的离散时间治愈模型，以便在月度监控框架内估计借款人从不良状态转换到正常履约状态的概率。请基于生存分析和伯努利试验的基本原理进行推导，而不使用任何用于目标参数的现成公式。您将实现一个程序，通过最大似然估计（MLE）来估计模型参数，并为一组固定的测试套件生成特定的数值输出。\n\n假设一笔贷款在第 $t=1$ 个月初违约后，其向治愈状态的转换由每月独立的伯努利试验决定，每个借款人具有特定的月度治愈概率 $h_i \\in (0,1)$。如果一个借款人被观察了 $n_i \\in \\mathbb{N}$ 个月并在第 $n_i$ 个月末治愈，则事件指示符为 $d_i=1$；如果借款人在观察期间未治愈，则该观察为右删失，事件指示符为 $d_i=0$。在每月试验独立的假设下，对于借款人 $i$，其似然贡献的计算方式为：当 $d_i=1$ 时，将前 $(n_i-1)$ 个月未治愈的概率与第 $n_i$ 个月治愈的概率相乘；当 $d_i=0$ 时，则是 $n_i$ 个月均未治愈的概率。这就得到了一个可以用 $h_i$、$n_i$ 和 $d_i$ 表示的似然函数。\n\n使用带有单个协变量 $x_i \\in \\mathbb{R}$ 和一个截距项的逻辑斯蒂连接函数对月度治愈概率进行建模：\n$$\nh_i \\;=\\; \\sigma(\\eta_i) \\;=\\; \\frac{1}{1 + e^{-\\eta_i}}, \n\\quad \\text{其中} \\quad \\eta_i \\;=\\; \\alpha + \\beta x_i,\n$$\n其中参数为 $\\alpha \\in \\mathbb{R}$ 和 $\\beta \\in \\mathbb{R}$。利用借款人之间的独立性，将全样本似然函数写成各个借款人似然函数的乘积，然后取对数以推导出对数似然函数。\n\n从上述定义和仅有的这些核心假设出发，完成以下任务：\n- 推导样本对数似然函数作为 $(\\alpha,\\beta)$ 的函数。\n- 推导得分函数（即对数似然函数关于 $(\\alpha,\\beta)$ 的梯度）。\n- 推导 Hessian 矩阵（二阶导数矩阵），并用它来设计一个牛顿类算法，该算法能够沿凹的对数似然函数爬升至其最大值。为您选择的任何用于确保数值稳定性的阻尼或线性搜索方法提供理由。\n- 实现一个程序，在提供的数据集上通过最大化对数似然函数来估计 $(\\alpha,\\beta)$。请使用双精度算术。所有概率必须以小数形式表示，而不是百分比。\n\n使用以下借款人数据集，其中每个元组为 $(x_i, n_i, d_i)$，$x_i$ 无单位，$n_i$ 以月为单位，$d_i \\in \\{0,1\\}$。数据集如下：\n- $(-1.5, 6, 0)$\n- $(-1.2, 6, 0)$\n- $(-1.0, 6, 0)$\n- $(-0.8, 5, 1)$\n- $(-0.5, 6, 1)$\n- $(-0.2, 4, 1)$\n- $(0.0, 3, 1)$\n- $(0.2, 2, 1)$\n- $(0.5, 1, 1)$\n- $(0.8, 1, 1)$\n- $(1.0, 1, 1)$\n- $(1.2, 1, 1)$\n- $(1.5, 1, 1)$\n- $(1.8, 1, 1)$\n- $(-0.3, 6, 0)$\n- $(0.3, 2, 1)$\n\n在估计出 $(\\hat{\\alpha},\\hat{\\beta})$ 后，计算以下测试套件输出：\n- 估计的参数 $\\hat{\\alpha}$ 和 $\\hat{\\beta}$。\n- 使用 $h(x) = \\sigma(\\hat{\\alpha} + \\hat{\\beta} x)$ 计算 $x \\in \\{-1.0, 0.0, 1.0\\}$ 时的预测月度治愈概率。\n- 使用 $1 - (1 - h(x))^{6}$ 计算 $x \\in \\{-1.0, 1.0\\}$ 时的预测六个月治愈概率。\n- 边界条件检查：使用相同公式计算 $x \\in \\{-3.0, 3.0\\}$ 时的预测六个月治愈概率。\n\n所有概率必须以小数形式报告。不涉及物理单位。不涉及角度。\n\n您的程序必须生成单行输出，其中包含 $9$ 个结果，顺序完全如下：\n$[\\hat{\\alpha}, \\hat{\\beta}, h(-1.0), h(0.0), h(1.0), \\text{cure6}(-1.0), \\text{cure6}(1.0), \\text{cure6}(-3.0), \\text{cure6}(3.0)]$，\n四舍五入到六位小数，以逗号分隔，并用方括号括起来。例如，一个有效的格式是 $[\\dots]$，其中包含恰好 $9$ 个十进制数。\n\n您的程序需要计算的测试套件摘要：\n- 在提供的数据集上进行参数估计。\n- 针对月度治愈概率在 $x=-1.0$、$x=0.0$、$x=1.0$ 处的预测。\n- 针对六个月治愈概率在 $x=-1.0$、$x=1.0$、$x=-3.0$、$x=3.0$ 处的预测。\n\n您的最终答案必须是一个完整、可运行的程序，该程序执行所有步骤，并完全按照指定格式打印所需的单行输出。",
            "solution": "我们从离散时间生存分析的构造开始。借款人 $i$ 在 $t=0$ 时违约，并从 $t=1$ 开始按月监控。设 $h_i \\in (0,1)$ 表示借款人 $i$ 的恒定月度治愈概率。月度治愈试验的独立性意味着等待治愈的时间服从参数为 $h_i$ 的几何分布。如果借款人在第 $n_i$ 个月治愈（事件指示符 $d_i=1$），此结果的概率为\n$$\n\\Pr(T_i = n_i \\mid h_i) \\;=\\; (1-h_i)^{n_i-1} h_i.\n$$\n如果借款人在 $n_i$ 个月后被右删失（事件指示符 $d_i=0$），观察到截至并包括第 $n_i$ 个月均未治愈的概率为\n$$\n\\Pr(T_i > n_i \\mid h_i) \\;=\\; (1-h_i)^{n_i}.\n$$\n结合两种情况，借款人级别的似然函数可以紧凑地写为\n$$\nL_i(h_i; n_i, d_i) \\;=\\; \\left[ (1-h_i)^{n_i-1} h_i \\right]^{d_i} \\left[(1-h_i)^{n_i}\\right]^{1-d_i}\n\\;=\\; h_i^{d_i} (1-h_i)^{n_i-d_i}.\n$$\n假设各借款人之间相互独立，则样本似然函数为 $L(\\{h_i\\}) = \\prod_{i=1}^N L_i(h_i;n_i,d_i)$，样本对数似然函数为\n$$\n\\ell(\\{h_i\\}) \\;=\\; \\sum_{i=1}^N \\left[ d_i \\log h_i + (n_i - d_i) \\log(1 - h_i) \\right].\n$$\n\n接下来，我们通过一个带有截距 $\\alpha$ 和单个协变量 $x_i$ 的斜率 $\\beta$ 的逻辑斯蒂连接函数，将 $h_i$ 与观测到的异质性联系起来：\n$$\n\\eta_i \\;=\\; \\alpha + \\beta x_i, \\qquad\nh_i \\;=\\; \\sigma(\\eta_i) \\;=\\; \\frac{1}{1 + e^{-\\eta_i}}.\n$$\n因此，作为参数 $(\\alpha,\\beta)$ 的函数的对数似然函数变为\n$$\n\\ell(\\alpha,\\beta) \\;=\\; \\sum_{i=1}^N \\left[ d_i \\log \\sigma(\\alpha + \\beta x_i) + (n_i - d_i) \\log\\left(1 - \\sigma(\\alpha + \\beta x_i)\\right) \\right].\n$$\n\n为了应用最大似然估计（MLE），我们推导得分函数（梯度）和 Hessian 矩阵。设 $h_i = \\sigma(\\eta_i)$，并回顾 $\\frac{d\\sigma}{d\\eta} = h_i(1 - h_i)$。根据链式法则，\n$$\n\\frac{\\partial \\ell}{\\partial \\eta_i}\n= \\frac{d_i}{h_i} \\frac{d h_i}{d \\eta_i} - \\frac{n_i - d_i}{1 - h_i} \\frac{d h_i}{d \\eta_i}\n= \\left( \\frac{d_i}{h_i} - \\frac{n_i - d_i}{1 - h_i} \\right) h_i(1 - h_i)\n= d_i - n_i h_i.\n$$\n使用 $\\eta_i = \\alpha + \\beta x_i$，得分向量为\n$$\n\\nabla \\ell(\\alpha,\\beta)\n= \\sum_{i=1}^N (d_i - n_i h_i)\n\\begin{bmatrix}\n\\frac{\\partial \\eta_i}{\\partial \\alpha} \\\\\n\\frac{\\partial \\eta_i}{\\partial \\beta}\n\\end{bmatrix}\n= \\sum_{i=1}^N (d_i - n_i h_i)\n\\begin{bmatrix}\n1 \\\\ x_i\n\\end{bmatrix}.\n$$\nHessian 矩阵通过对得分函数再次求导得出。由于 $\\frac{\\partial}{\\partial \\eta_i}(d_i - n_i h_i) = -n_i h_i (1 - h_i)$，我们得到\n$$\n\\nabla^2 \\ell(\\alpha,\\beta)\n= \\sum_{i=1}^N \\left( - n_i h_i (1 - h_i) \\right)\n\\begin{bmatrix}\n1 \\\\ x_i\n\\end{bmatrix}\n\\begin{bmatrix}\n1  x_i\n\\end{bmatrix}\n= - \\sum_{i=1}^N n_i h_i (1 - h_i)\n\\begin{bmatrix}\n1  x_i\\\\\nx_i  x_i^2\n\\end{bmatrix}.\n$$\n该 Hessian 矩阵是负半定的，因为 $n_i h_i (1 - h_i) \\ge 0$，这保证了对数似然函数是凹函数，并且任何驻点都是全局最大值点。\n\n一个用于最大化 $\\ell(\\alpha,\\beta)$ 的牛顿类算法在每次迭代 $k$ 中求解以下线性系统\n$$\n\\nabla^2 \\ell(\\theta^{(k)}) \\, s^{(k)} \\;=\\; - \\nabla \\ell(\\theta^{(k)}),\n\\quad \\text{with} \\quad \\theta^{(k+1)} \\;=\\; \\theta^{(k)} + \\gamma^{(k)} s^{(k)},\n$$\n其中 $\\theta = \\begin{bmatrix}\\alpha \\\\ \\beta\\end{bmatrix}$ 且 $\\gamma^{(k)} \\in (0,1]$ 是通过回溯线性搜索选择的步长，以确保单调递增，即 $\\ell(\\theta^{(k+1)}) > \\ell(\\theta^{(k)})$。为保持数值稳定性，在计算 $\\log h_i$ 和 $\\log(1-h_i)$ 时，使用一个很小的 $\\varepsilon$ 将 $h_i$ 的值限制在远离 $0$ 和 $1$ 的范围内，例如，在双精度下强制 $h_i \\in [\\varepsilon, 1-\\varepsilon]$。\n\n有了最大似然估计值 $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta})$，对于协变量值 $x$，预测的月度治愈概率为\n$$\nh(x) \\;=\\; \\sigma(\\hat{\\alpha} + \\hat{\\beta} x),\n$$\n而在 $K$ 个月内治愈的预测概率可以从几何等待时间分布中获得：\n$$\n\\Pr(T \\le K \\mid x) \\;=\\; 1 - (1 - h(x))^{K}.\n$$\n在测试套件中，我们取 $K=6$，因此 $\\text{cure6}(x) = 1 - (1 - h(x))^{6}$。\n\n算法设计摘要：\n- 将 $\\theta^{(0)}$ 初始化为一个有限值，例如 $\\theta^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 迭代：\n  - 计算 $h_i = \\sigma(\\alpha + \\beta x_i)$ 并为了稳定性将其值限制在 $[\\varepsilon, 1 - \\varepsilon]$ 范围内。\n  - 计算 $\\ell(\\theta)$、$\\nabla \\ell(\\theta)$ 和 $\\nabla^2 \\ell(\\theta)$。\n  - 求解 $\\nabla^2 \\ell(\\theta) s = - \\nabla \\ell(\\theta)$；如果该系统是病态的，则向 $\\nabla^2 \\ell$ 添加一个小的脊项 $\\lambda I$。\n  - 对 $\\gamma \\in (0,1]$ 执行回溯线性搜索以确保函数值上升。\n  - 当 $\\lVert \\nabla \\ell(\\theta) \\rVert_2$ 低于某个容差或 $\\ell(\\theta)$ 的变化可忽略不计时停止。\n- 报告 $\\hat{\\alpha}$ 和 $\\hat{\\beta}$。\n- 计算在 $x \\in \\{-1.0, 0.0, 1.0\\}$ 处的 $h(x)$ 和在 $x \\in \\{-1.0, 1.0, -3.0, 3.0\\}$ 处的 $\\text{cure6}(x)$。\n- 将所有结果以小数形式输出，四舍五入到六位小数，并遵循指定的顺序和格式。\n\n该方法基于带有右删失的离散时间治愈的几何等待时间模型、用于概率的逻辑斯蒂连接函数，以及用于凹似然函数的标准 Newton 优化。最终的程序实现了这些步骤，在提供的数据集上拟合模型，并打印所要求的单行输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(z):\n    # Stable sigmoid\n    # For large negative z, exp(-z) overflows, so we use numerically stable form\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef loglik_grad_hess(theta, X, n, d, eps=1e-12):\n    # theta: [alpha, beta]\n    # X: array of shape (N, 2) with columns [1, x]\n    # n: durations (N,)\n    # d: event indicators (N,)\n    eta = X @ theta\n    h = sigmoid(eta)\n    # clamp h for numerical stability in logs\n    h_clamped = np.clip(h, eps, 1.0 - eps)\n    # log-likelihood\n    ll = np.sum(d * np.log(h_clamped) + (n - d) * np.log(1.0 - h_clamped))\n    # gradient: sum_i (d_i - n_i h_i) * [1, x_i]\n    residual = d - n * h\n    grad = X.T @ residual\n    # Hessian: - sum_i n_i h_i(1-h_i) * [[1, x_i],[x_i, x_i^2]]\n    w = n * h * (1.0 - h)  # (N,)\n    # Build Hessian as - X^T diag(w) X\n    # Compute X^T * (w * X) efficiently\n    WX = X * w[:, None]\n    hess = -(X.T @ WX)\n    return ll, grad, hess\n\ndef newton_maximize(initial_theta, X, n, d, tol=1e-9, max_iter=100, ls_beta=0.5, ls_c=1e-4):\n    theta = initial_theta.copy()\n    prev_ll = -np.inf\n    for it in range(max_iter):\n        ll, grad, hess = loglik_grad_hess(theta, X, n, d)\n        grad_norm = np.linalg.norm(grad, ord=2)\n        if grad_norm  tol:\n            break\n        # Ensure Hessian is not singular; add small ridge if needed\n        # Try Cholesky on -hess (which should be positive semi-definite)\n        # We'll solve hess * s = -grad; if fails, add ridge\n        ridge = 1e-8\n        for _ in range(8):\n            try:\n                # Solve linear system\n                step = np.linalg.solve(hess - ridge * np.eye(hess.shape[0]), -grad)\n                break\n            except np.linalg.LinAlgError:\n                ridge *= 10.0\n        else:\n            # Fallback to gradient ascent direction if Hessian is too ill-conditioned\n            step = grad / max(grad_norm, 1e-12)\n\n        # Backtracking line search to ensure ascent: ll(theta + t*step) >= ll + ls_c * t * grad^T * step\n        t = 1.0\n        current_ll = ll\n        directional_derivative = grad @ step\n        # If directional derivative is negative (should be ascent), flip step\n        if directional_derivative  0:\n            step = -step\n            directional_derivative = -directional_derivative\n\n        while t > 1e-8:\n            candidate = theta + t * step\n            new_ll, _, _ = loglik_grad_hess(candidate, X, n, d)\n            if new_ll >= current_ll + ls_c * t * directional_derivative:\n                theta = candidate\n                prev_ll = new_ll\n                break\n            t *= ls_beta\n        else:\n            # If line search fails, stop to avoid infinite loop\n            theta = theta  # unchanged\n            break\n    return theta\n\ndef solve():\n    # Define the dataset: list of (x, n, d)\n    data = [\n        (-1.5, 6, 0),\n        (-1.2, 6, 0),\n        (-1.0, 6, 0),\n        (-0.8, 5, 1),\n        (-0.5, 6, 1),\n        (-0.2, 4, 1),\n        (0.0, 3, 1),\n        (0.2, 2, 1),\n        (0.5, 1, 1),\n        (0.8, 1, 1),\n        (1.0, 1, 1),\n        (1.2, 1, 1),\n        (1.5, 1, 1),\n        (1.8, 1, 1),\n        (-0.3, 6, 0),\n        (0.3, 2, 1),\n    ]\n    # Prepare arrays\n    x = np.array([row[0] for row in data], dtype=float)\n    n = np.array([row[1] for row in data], dtype=float)\n    d = np.array([row[2] for row in data], dtype=float)\n    N = x.shape[0]\n    X = np.column_stack([np.ones(N), x])\n\n    # Estimate parameters via Newton maximization\n    theta0 = np.array([0.0, 0.0], dtype=float)\n    theta_hat = newton_maximize(theta0, X, n, d, tol=1e-9, max_iter=100)\n\n    alpha_hat, beta_hat = theta_hat.tolist()\n\n    # Prediction functions\n    def h_of_x(xv):\n        return float(sigmoid(alpha_hat + beta_hat * xv))\n\n    def cure_k_of_x(xv, K=6):\n        h = h_of_x(xv)\n        return float(1.0 - (1.0 - h) ** K)\n\n    # Test suite predictions\n    xs_monthly = [-1.0, 0.0, 1.0]\n    xs_cure6_main = [-1.0, 1.0]\n    xs_cure6_edge = [-3.0, 3.0]\n\n    monthly_probs = [h_of_x(v) for v in xs_monthly]\n    cure6_main = [cure_k_of_x(v, K=6) for v in xs_cure6_main]\n    cure6_edge = [cure_k_of_x(v, K=6) for v in xs_cure6_edge]\n\n    # Aggregate results in required order:\n    # [alpha_hat, beta_hat, h(-1.0), h(0.0), h(1.0), cure6(-1.0), cure6(1.0), cure6(-3.0), cure6(3.0)]\n    results = [alpha_hat, beta_hat] + monthly_probs + cure6_main + cure6_edge\n\n    # Round to six decimals\n    results_str = [f\"{v:.6f}\" for v in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现在，让我们将视野从单个贷款扩展到相互关联的金融系统。本练习将通过一个基于网络的压力测试来模拟金融传染，即一个实体的违约可能引发一连串的连锁失败。通过模拟这些动态过程，你将洞察系统性风险——这是单个贷款模型无法捕捉的关键概念，并体会到理解投资组合中二阶效应的重要性。",
            "id": "2385774",
            "problem": "要求您为贷款组合形式化并实施一个压力测试程序，该程序包含由外生交易对手违约产生的二阶传染效应。设计必须从核心信用风险定义开始，并遵循明确的逻辑规则进行。考虑一个包含 $N$ 个债务人的系统，债务人由 $i \\in \\{0,1,\\dots,N-1\\}$ 索引。相关原语如下：\n- 银行对这些债务人的违约风险暴露向量 $b \\in \\mathbb{R}_{\\ge 0}^{N}$，其中 $b_i$ 是银行对债务人 $i$ 的风险暴露。\n- 初始股权缓冲向量 $K^{(0)} \\in \\mathbb{R}_{ 0}^{N}$，其中 $K_i^{(0)}$ 是债务人 $i$ 在传染发生前的股权。\n- 债务人间的风险暴露矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$，其中对所有 $i$ 都有 $X_{ii} = 0$，$X_{i,j}$ 是债务人 $i$ 对债务人 $j$ 的风险暴露。\n- 银行贷款的违约损失率 $L^{\\text{bank}} \\in [0,1]$ 和债务人间债权的违约损失率 $L^{\\text{inter}} \\in [0,1]$。\n- 系统性损失乘数 $m \\ge 1$，用于在压力期间放大债务人间的损失。\n- 初始违约集合 $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$，代表在时间 $t=0$ 时的外生违约。\n\n基本定义和规则：\n- 违约风险暴露 (EAD) 是指当交易对手违约时面临损失的未偿风险暴露；在此，银行对债务人 $i$ 的 EAD 为 $b_i$，债务人 $i$ 对 $j$ 的 EAD 为 $X_{i,j}$。\n- 违约损失率 (LGD) 是指违约时 EAD 未能收回的比例；在此，银行贷款的 LGD 为 $L^{\\text{bank}}$，债务人间的 LGD 为 $L^{\\text{inter}}$。\n- 股权因已实现的信用损失而减少。在每次传染迭代 $t \\in \\{1,2,\\dots\\}$ 中，通过扣除仅由在迭代 $t-1$ 时新违约的交易对手造成的损失来定义 $K^{(t)}$。对于尚未违约的债务人 $i$，\n$$\nK_i^{(t)} \\;=\\; K_i^{(t-1)} \\;-\\; \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big),\n$$\n其中 $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ 是在迭代 $t-1$ 时变为违约的债务人集合（其中 $D^{(-1)} := \\emptyset$）。违约规则是：债务人 $i$ 在迭代 $t$ 时违约，当且仅当 $K_i^{(t)} \\le 0$。使用数值容差 $\\varepsilon = 10^{-12}$，并将 $K_i^{(t)} \\le \\varepsilon$ 视为违约，以避免浮点误差。\n- 当首次迭代 $T$ 满足 $\\Delta D^{(T)} = \\emptyset$ 时，传染过程终止。最终违约集合为 $D^{(\\infty)} = D^{(T)}$。\n- 银行的投资组合损失为\n$$\n\\text{Loss} \\;=\\; \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}.\n$$\n\n您的任务是实现一个程序，为下述每个测试用例计算：\n- 银行的总损失，以实数形式表示（结果表示为小数，而非百分比），以及\n- 违约债务人的总数（一个整数），包括外生初始违约和任何由传染引发的违约。\n\n所有输入中的索引都是从零开始的。不涉及物理单位。不使用角度。百分比必须以小数形式表示（例如，写 $0.4$ 而不是 $40\\%$）。\n\n测试套件：\n- Case A (二阶传染，中度放大):\n  - $N = 4$.\n  - $b = [10.0,\\, 5.0,\\, 8.0,\\, 6.0]$.\n  - $K^{(0)} = [2.0,\\, 2.0,\\, 1.0,\\, 1.1]$.\n  - $X$ 的非零项为 $X_{1,0} = 1.6$, $X_{2,0} = 1.8$, $X_{3,0} = 0.6$, $X_{1,2} = 0.9$, $X_{3,2} = 1.2$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.45$, $L^{\\text{inter}} = 0.5$, $m = 1.2$.\n  - $D^{(0)} = \\{0\\}$.\n- Case B (无传染网络):\n  - $N = 3$.\n  - $b = [5.0,\\, 5.0,\\, 5.0]$.\n  - $K^{(0)} = [1.0,\\, 1.0,\\, 1.0]$.\n  - $X$ 是 $3 \\times 3$ 的零矩阵。\n  - $L^{\\text{bank}} = 0.4$, $L^{\\text{inter}} = 0.5$, $m = 1.2$.\n  - $D^{(0)} = \\{1\\}$.\n- Case C (违约阈值的边界条件，完全级联):\n  - $N = 3$.\n  - $b = [1.0,\\, 1.0,\\, 1.0]$.\n  - $K^{(0)} = [1.0,\\, 0.5,\\, 0.5]$.\n  - $X$ 的非零项为 $X_{1,0} = 1.0$, $X_{2,1} = 1.0$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.5$, $m = 1.0$.\n  - $D^{(0)} = \\{0\\}$.\n- Case D (放大效应在 $m=1$ 时不会发生的情况下创造了传染):\n  - $N = 4$.\n  - $b = [2.0,\\, 2.0,\\, 2.0,\\, 2.0]$.\n  - $K^{(0)} = [1.2,\\, 0.9,\\, 0.15,\\, 0.5]$.\n  - $X$ 的非零项为 $X_{0,3} = 0.3$, $X_{1,3} = 0.3$, $X_{2,3} = 0.2$, $X_{0,2} = 0.7$, $X_{1,2} = 0.6$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.4$, $m = 2.0$.\n  - $D^{(0)} = \\{3\\}$.\n\n编程任务：\n- 精确地按照所述实现上述传染动态。\n- 对于每个案例，返回一个列表 $[\\text{Loss},\\, \\text{Count}]$，其中 $\\text{Loss}$ 是一个浮点数，$\\text{Count}$ 是一个整数。您可以根据需要对中间计算进行四舍五入，但最终的 $\\text{Loss}$ 应使用标准浮点运算计算，并期望其绝对误差至少精确到 $10^{-6}$。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个案例的结果本身就是一个双元素列表。例如，您的输出必须类似于\n$[\\,[\\text{Loss}_A,\\text{Count}_A],[\\text{Loss}_B,\\text{Count}_B],[\\text{Loss}_C,\\text{Count}_C],[\\text{Loss}_D,\\text{Count}_D]\\,]$\n打印行中不含空格。",
            "solution": "问题陈述需经过验证。\n\n步骤1：提取给定信息。\n- 债务人数量为 $N$。\n- 银行的违约风险暴露向量为 $b \\in \\mathbb{R}_{\\ge 0}^{N}$。\n- 初始股权缓冲向量为 $K^{(0)} \\in \\mathbb{R}_{ 0}^{N}$。\n- 债务人间的风险暴露矩阵为 $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$，其中 $X_{i,j}$ 是债务人 $i$ 对债务人 $j$ 的风险暴露，且 $X_{ii} = 0$。\n- 银行的违约损失率为 $L^{\\text{bank}} \\in [0,1]$。\n- 债务人间的违约损失率为 $L^{\\text{inter}} \\in [0,1]$。\n- 系统性损失乘数为 $m \\ge 1$。\n- 外生违约的初始债务人集合为 $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$。\n- 违约的数值容差为 $\\varepsilon = 10^{-12}$。如果债务人 $i$ 的股权 $K_i^{(t)} \\le \\varepsilon$，则该债务人违约。\n- 传染动态由股权更新的迭代规则控制：\n$$K_i^{(t)} = K_i^{(t-1)} - \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)$$\n其中 $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ 且 $D^{(-1)} := \\emptyset$。\n- 当首次迭代 $T$ 满足 $\\Delta D^{(T)} = \\emptyset$ 时，传染终止。最终违约集合为 $D^{(\\infty)} = D^{(T)}$。\n- 银行的投资组合损失为 $\\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}$。\n- 提供了四个特定的测试用例（A、B、C、D），以及上述参数所有必需的数值。\n\n步骤2：使用提取的给定信息进行验证。\n- **科学依据**：该问题描述了一个金融传染的离散时间网络模型。风险暴露、股权、违约损失率和违约级联等概念是金融风险管理和计算经济学领域的标准概念。该模型虽然是简化版，但基于一个公认的原则，即资产（银行间风险暴露）损失会消耗公司的股权，导致其自身倒闭。因此，该问题具有科学合理性。\n- **适定性**：该过程是一个迭代算法。系统的状态是违约债务人的集合。该集合是单调不减的。由于债务人总数 $N$ 是有限的，传染过程必须在至多 $N$ 次迭代内终止。最终状态（所有违约债务人的集合）由初始条件和系统规则唯一确定。因此，总损失和违约数量存在唯一的稳定解。该问题是适定的。\n- **客观性**：该问题使用精确的数学符号和明确的规则进行定义。所有参数都已给出，并且明确规定了所需的输出。没有主观性语言。\n\n步骤3：结论与行动。\n该问题具有科学依据、适定性和客观性，因此是有效的。将提供一个解决方案。\n\n传染过程被建模为一系列离散的时间步，由 $t \\in \\{0, 1, 2, \\dots\\}$ 索引。我们必须跟踪每个债务人的状态，包括其股本和违约状态。\n\n在迭代 $t$ 时的状态变量是：\n- $K^{(t)} \\in \\mathbb{R}^N$：所有债务人的股权向量。\n- $D^{(t)} \\subseteq \\{0, 1, \\dots, N-1\\}$：截至并包括迭代 $t$ 时已违约的所有债务人的累积集合。\n- $\\Delta D^{(t)} = D^{(t)} \\setminus D^{(t-1)}$：在迭代 $t$ 时新违约的债务人集合。\n\n算法流程如下：\n1.  **初始化 ($t=0$)**：\n    - 股权向量为初始股权 $K^{(0)}$。\n    - 累积违约集合 $D^{(-1)}$ 定义为空集 $\\emptyset$。\n    - 初始违约集合 $D^{(0)}$ 是外生给定的。\n    - 在 $t=0$ 时新违约的债务人集合为 $\\Delta D^{(0)} = D^{(0)} \\setminus D^{(-1)} = D^{(0)}$。\n\n2.  **迭代传染 ($t=1, 2, \\dots, T$)**：模拟的核心是一个循环，只要新一波违约发生，该循环就会继续。我们将迭代 $t$ 开始时的状态定义为 $(K^{(t-1)}, D^{(t-1)})$，其中 $\\Delta D^{(t-1)}$ 是刚刚违约的公司集合。\n    - 如果 $\\Delta D^{(t-1)} = \\emptyset$，则传染已停止。过程终止。达到最终状态。\n    - 否则，我们必须计算对剩余有偿付能力债务人的影响。对于每个尚未违约的债务人 $i$（即 $i \\notin D^{(t-1)}$），我们计算其因 $\\Delta D^{(t-1)}$ 中的交易对手而产生的损失：\n    $$\n    L_{i, t} = \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)\n    $$\n    - 然后更新这些有偿付能力债务人的股权：\n    $$\n    K_i^{(t)} = K_i^{(t-1)} - L_{i, t}\n    $$\n    对于已经违约的债务人或在此步骤中股权未更新的债务人，其股权值保持不变，即 $K_i^{(t)} = K_i^{(t-1)}$。\n    - 确定本轮迭代的新违约集合。一个先前有偿付能力的债务人 $i$（$i \\notin D^{(t-1)}$）如果其股权被耗尽，即 $K_i^{(t)} \\le \\varepsilon$，则现在违约。令该集合为 $\\Delta D_{\\text{new}}^{(t)}$。\n    - 更新累积违约集合：$D^{(t)} = D^{(t-1)} \\cup \\Delta D_{\\text{new}}^{(t)}$。\n    - *下一*次迭代的新违约债务人集合是 $\\Delta D^{(t)} = \\Delta D_{\\text{new}}^{(t)}$。然后该过程对 $t+1$ 重复。\n\n3.  **终止与最终计算**：当迭代 $T$ 满足 $\\Delta D^{(T-1)} = \\emptyset$ 时，循环终止。\n    - 所有违约债务人的最终集合是 $D^{(\\infty)} = D^{(T-1)}$。\n    - 违约总数是该集合的基数 $|D^{(\\infty)}|$。\n    - 银行的总损失根据其对最终违约集合中债务人的风险暴露计算得出：\n    $$\n    \\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}\n    $$\n\n该算法针对所提供的四个测试用例中的每一个进行实现。每个用例的参数用于初始化模拟，然后运行模拟直至完成，以找到最终损失和违约债务人数量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n\n    # Numerical tolerance for default check\n    EPSILON = 1e-12\n\n    test_cases = [\n        # Case A: second-order contagion, moderate amplification\n        {\n            \"N\": 4,\n            \"b\": np.array([10.0, 5.0, 8.0, 6.0]),\n            \"K0\": np.array([2.0, 2.0, 1.0, 1.1]),\n            \"X_sparse\": {(1, 0): 1.6, (2, 0): 1.8, (3, 0): 0.6, (1, 2): 0.9, (3, 2): 1.2},\n            \"L_bank\": 0.45,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {0},\n        },\n        # Case B: no contagion network\n        {\n            \"N\": 3,\n            \"b\": np.array([5.0, 5.0, 5.0]),\n            \"K0\": np.array([1.0, 1.0, 1.0]),\n            \"X_sparse\": {},\n            \"L_bank\": 0.4,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {1},\n        },\n        # Case C: boundary condition at default threshold, full cascade\n        {\n            \"N\": 3,\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"K0\": np.array([1.0, 0.5, 0.5]),\n            \"X_sparse\": {(1, 0): 1.0, (2, 1): 1.0},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.5,\n            \"m\": 1.0,\n            \"D0\": {0},\n        },\n        # Case D: amplification creates contagion that would not occur at m=1\n        {\n            \"N\": 4,\n            \"b\": np.array([2.0, 2.0, 2.0, 2.0]),\n            \"K0\": np.array([1.2, 0.9, 0.15, 0.5]),\n            \"X_sparse\": {(0, 3): 0.3, (1, 3): 0.3, (2, 3): 0.2, (0, 2): 0.7, (1, 2): 0.6},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.4,\n            \"m\": 2.0,\n            \"D0\": {3},\n        },\n    ]\n\n    def run_simulation(case):\n        \"\"\"\n        Runs the contagion simulation for a single test case.\n        \"\"\"\n        N = case[\"N\"]\n        b = case[\"b\"]\n        K = case[\"K0\"].copy()\n        X_sparse = case[\"X_sparse\"]\n        L_bank = case[\"L_bank\"]\n        L_inter = case[\"L_inter\"]\n        m = case[\"m\"]\n        D0 = case[\"D0\"]\n\n        # Construct the dense exposure matrix X\n        X = np.zeros((N, N))\n        for (i, j), val in X_sparse.items():\n            X[i, j] = val\n\n        # State variables for the simulation\n        D_final = set(D0)\n        newly_defaulted = set(D0)\n        \n        effective_lgd = m * L_inter\n\n        while newly_defaulted:\n            last_wave_defaults = list(newly_defaulted)\n            newly_defaulted = set()\n            \n            solvent_obligors = [i for i in range(N) if i not in D_final]\n            \n            if not solvent_obligors:\n                break\n                \n            # Calculate losses for solvent obligors from the last wave of defaults\n            losses = X[solvent_obligors, :][:, last_wave_defaults].sum(axis=1) * effective_lgd\n\n            # Update equity and check for new defaults\n            for idx, obligor_idx in enumerate(solvent_obligors):\n                K[obligor_idx] -= losses[idx]\n                if K[obligor_idx] = EPSILON:\n                    newly_defaulted.add(obligor_idx)\n\n            D_final.update(newly_defaulted)\n\n        # Calculate final results\n        final_loss = b[list(D_final)].sum() * L_bank\n        num_defaults = len(D_final)\n        \n        return [final_loss, num_defaults]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(case)\n        results.append(f\"[{result[0]},{result[1]}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}