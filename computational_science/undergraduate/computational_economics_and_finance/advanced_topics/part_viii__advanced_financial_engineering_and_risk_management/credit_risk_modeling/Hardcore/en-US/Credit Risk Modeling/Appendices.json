{
    "hands_on_practices": [
        {
            "introduction": "At the heart of credit risk management lies the ability to predict the likelihood of default. This first exercise challenges you to build a logistic regression model, a cornerstone of credit scoring, from its foundational principles. You will not only implement the model but also the optimization routine (Newton's method) with ridge regularization, providing a deep, practical understanding of how this essential classification tool works in a financial context .",
            "id": "2407536",
            "problem": "You are tasked with implementing a binary classifier using logistic regression to predict whether a financial advisor will be sanctioned by a regulator in the next year, based on their customer complaint history and employment instability. The goal is to build the model from first principles in the context of computational economics and finance, using a maximum likelihood framework for binary outcomes and the concept of penalization for regularization. Your program must be a complete, runnable script that performs the following steps using the provided data and test suite, and outputs the aggregate results in the specified format.\n\nYou must build the classifier starting from fundamental principles: a binary outcome modeled as a Bernoulli random variable with probability that depends on a linear index transformed by a logistic link, and parameters estimated by maximizing the likelihood of the observed data. Interpretability constraints typical in financial risk modeling apply: include an intercept, standardize features to have zero mean and unit variance using only the training set statistics, and use an isotropic squared norm penalty (ridge regularization) on the slope coefficients to guard against overfitting. The intercept must not be penalized. No shortcut formulas or off-the-shelf model-fitting routines may be used; derive the update equations for the parameters and implement a numerically stable solver using Newton’s method.\n\nData description:\n- Features per advisor: \n  - $c$: number of customer complaints (count),\n  - $r$: average complaints per year (nonnegative real),\n  - $s$: average severity of complaints (normalized on $[0,1]$),\n  - $f$: number of firm changes in the last $5$ years (count),\n  - $t$: tenure in current firm in years (nonnegative real).\n- Target: $y \\in \\{0,1\\}$ indicating whether the advisor was sanctioned within the next year.\n\nTraining dataset (each item is $(c,r,s,f,t; y)$):\n- $(0, 0.0, 0.1, 0, 15; 0)$\n- $(1, 0.1, 0.2, 0, 12; 0)$\n- $(2, 0.2, 0.3, 1, 10; 0)$\n- $(0, 0.0, 0.2, 1, 18; 0)$\n- $(1, 0.1, 0.1, 0, 20; 0)$\n- $(2, 0.2, 0.15, 1, 16; 0)$\n- $(1, 0.0, 0.25, 0, 14; 0)$\n- $(0, 0.05, 0.05, 0, 22; 0)$\n- $(3, 0.5, 0.4, 1, 9; 0)$\n- $(5, 0.8, 0.7, 2, 5; 1)$\n- $(7, 1.2, 0.9, 3, 3; 1)$\n- $(4, 0.6, 0.6, 2, 6; 1)$\n- $(8, 1.5, 0.85, 4, 2; 1)$\n- $(6, 1.0, 0.8, 3, 4; 1)$\n- $(9, 1.8, 0.95, 5, 1; 1)$\n- $(3, 0.4, 0.55, 2, 8; 1)$\n\nModeling and training requirements:\n- Standardize each feature $c$, $r$, $s$, $f$, and $t$ to zero mean and unit variance using only the training set. For each feature $x$, the standardized value is computed as $(x - \\mu_x)/\\sigma_x$, where $\\mu_x$ and $\\sigma_x$ are the training mean and standard deviation for that feature. If any $\\sigma_x$ equals $0$, replace it with $1$ to avoid division by zero.\n- Augment the standardized feature matrix with an intercept column of ones.\n- Fit the logistic regression parameters by minimizing the penalized negative log-likelihood using Newton’s method with a stopping criterion based on the Euclidean norm of the parameter update being less than $10^{-8}$ or after $50$ iterations, whichever comes first.\n- Use a ridge penalty with strength $\\lambda \\ge 0$ applied only to the slope coefficients (intercept excluded).\n\nClassification rule:\n- For a given advisor with features $(c,r,s,f,t)$, compute the predicted sanction probability as a decimal in $[0,1]$. Classify as sanctioned if the probability is at least $0.5$; otherwise classify as not sanctioned. All probabilities, if reported, must be decimals; do not use percentage signs.\n\nTest suite:\nYour program must train the model multiple times as specified and return the classifications for the following four cases. Each case provides a penalty level $\\lambda$ and a test advisor’s features $(c,r,s,f,t)$. For each case, fit the model on the training data with the provided $\\lambda$ and then classify the given test advisor using the $0.5$ threshold.\n\n- Case $1$: $\\lambda = 1.0$, $(c,r,s,f,t) = (6, 1.0, 0.8, 3, 3)$\n- Case $2$: $\\lambda = 0.0$, $(c,r,s,f,t) = (0, 0.0, 0.1, 0, 15)$\n- Case $3$: $\\lambda = 10.0$, $(c,r,s,f,t) = (10, 1.5, 0.9, 5, 2)$\n- Case $4$: $\\lambda = 0.5$, $(c,r,s,f,t) = (1, 0.1, 0.2, 0, 12)$\n\nFinal output format:\n- Your program should produce a single line of output containing the classification results as a comma-separated list of integers enclosed in square brackets, in the order of the cases above, for example, $\"[1,0,1,0]\"$.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of statistical learning, specifically logistic regression with ridge regularization. The setup is well-posed, providing all necessary data, a clear objective function, and a specified numerical method for optimization. It is an objective and formalizable problem in computational finance. We will proceed with a solution derived from first principles.\n\nLet the set of training data be $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, where $\\mathbf{x}_i \\in \\mathbb{R}^D$ is the feature vector for the $i$-th advisor and $y_i \\in \\{0, 1\\}$ is the corresponding binary outcome (sanctioned or not). Here, $N=16$ is the number of training observations and $D=5$ is the number of features.\n\nThe logistic regression model postulates that the probability of a positive outcome ($y_i=1$) is given by the logistic (sigmoid) function applied to a linear combination of the features:\n$$\nP(y_i=1 | \\mathbf{x}_i^*; \\boldsymbol{\\beta}) = p_i = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}\n$$\nHere, $\\mathbf{x}_i^*$ is the $i$-th feature vector augmented with a leading $1$ to accommodate the intercept term, so $\\mathbf{x}_i^* \\in \\mathbb{R}^{D+1}$. The vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{D+1}$ contains the model parameters, with $\\beta_0$ being the intercept and $\\beta_1, \\dots, \\beta_D$ being the slope coefficients.\n\nThe parameters $\\boldsymbol{\\beta}$ are estimated by maximizing the log-likelihood of the observed data, with an added penalty term for regularization. The likelihood for a single observation is given by the Bernoulli probability mass function: $L_i(\\boldsymbol{\\beta}) = p_i^{y_i} (1-p_i)^{1-y_i}$. The total log-likelihood for $N$ independent observations is:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\ln(L_i(\\boldsymbol{\\beta})) = \\sum_{i=1}^N [y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]\n$$\nSubstituting the expressions for $p_i$ and $\\ln(1-p_i) = \\ln(1-\\sigma(z_i)) = -z_i - \\ln(1+e^{z_i})$ and $\\ln(p_i) = \\ln(\\sigma(z_i)) = -\\ln(1+e^{-z_i}) = z_i - \\ln(1+e^{z_i})$ for $z_i=\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}$, we can simplify the log-likelihood to a more convenient form:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N [y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) - \\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}})]\n$$\nWe aim to minimize the penalized *negative* log-likelihood. The penalty is the squared $\\ell_2$-norm of the slope coefficients (ridge regularization), which discourages overly large parameter values and helps prevent overfitting. The intercept $\\beta_0$ is not penalized. The objective function to minimize is:\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\sum_{j=1}^D \\beta_j^2 = \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^T \\mathbf{I}' \\boldsymbol{\\beta}\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength and $\\mathbf{I}'$ is a $(D+1) \\times (D+1)$ diagonal matrix with $I'_{00} = 0$ and $I'_{jj} = 1$ for $j=1, \\dots, D$.\n\nTo minimize $J(\\boldsymbol{\\beta})$, we use Newton's method, which is an iterative algorithm with the update rule:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla J(\\boldsymbol{\\beta}^{(t)})\n$$\nwhere $\\nabla J$ is the gradient of the objective function and $\\mathbf{H}$ is its Hessian matrix.\n\nFirst, we derive the gradient $\\nabla J(\\boldsymbol{\\beta})$. The $j$-th component of the gradient of the unpenalized part is:\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left( \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] \\right) = \\sum_{i=1}^N \\left[ \\frac{e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}} x_{ij}^* - y_i x_{ij}^* \\right] = \\sum_{i=1}^N (p_i - y_i) x_{ij}^*\n$$\nIn matrix notation, where $\\mathbf{X}^*$ is the $N \\times (D+1)$ augmented design matrix, $\\mathbf{p}$ is the vector of probabilities, and $\\mathbf{y}$ is the vector of outcomes, this is $\\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y})$.\nThe gradient of the penalty term is $\\lambda \\mathbf{I}' \\boldsymbol{\\beta}$.\nThe full gradient is therefore:\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{I}' \\boldsymbol{\\beta}\n$$\nNext, we derive the Hessian matrix $\\mathbf{H}(\\boldsymbol{\\beta})$. The entry $H_{jk}$ is $\\frac{\\partial^2 J(\\boldsymbol{\\beta})}{\\partial \\beta_j \\partial \\beta_k}$. For the unpenalized part:\n$$\n\\frac{\\partial^2}{\\partial \\beta_j \\partial \\beta_k} \\left( \\sum_{i=1}^N \\dots \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^N p_i x_{ij}^* \\right) = \\sum_{i=1}^N x_{ij}^* \\frac{\\partial p_i}{\\partial \\beta_k}\n$$\nSince $\\frac{\\partial p_i}{\\partial \\beta_k} = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})(1 - \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})) x_{ik}^* = p_i(1-p_i)x_{ik}^*$, the Hessian of the unpenalized part is:\n$$\nH_{jk} = \\sum_{i=1}^N x_{ij}^* p_i(1-p_i) x_{ik}^*\n$$\nIn matrix form, this is $\\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^*$, where $\\mathbf{W}$ is an $N \\times N$ diagonal matrix with $W_{ii} = p_i(1-p_i)$.\nThe Hessian of the penalty term is simply $\\lambda \\mathbf{I}'$.\nThe full Hessian is:\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^* + \\lambda \\mathbf{I}'\n$$\nThis Hessian is positive definite for $\\lambda > 0$, ensuring a unique minimum and numerical stability for the Newton's method.\n\nThe implementation will proceed as follows:\n1.  Compute the mean $\\boldsymbol{\\mu}$ and standard deviation $\\boldsymbol{\\sigma}$ for each of the $D=5$ features from the training data. If any $\\sigma_j = 0$, it will be set to $1$.\n2.  Standardize the training data matrix $\\mathbf{X}$ to get $\\mathbf{X}_{std}$, where each column has zero mean and unit variance.\n3.  Augment $\\mathbf{X}_{std}$ with a column of ones to form the design matrix $\\mathbf{X}^*$.\n4.  For each test case, initialize the parameter vector $\\boldsymbol{\\beta}$ (e.g., to zeros).\n5.  Iteratively apply the Newton's method update:\n    a. Calculate probabilities $\\mathbf{p} = \\sigma(\\mathbf{X}^* \\boldsymbol{\\beta}^{(t)})$ and the weight matrix $\\mathbf{W}^{(t)}$.\n    b. Compute the gradient $\\nabla J(\\boldsymbol{\\beta}^{(t)})$ and Hessian $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})$.\n    c. Solve the linear system $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\Delta \\boldsymbol{\\beta} = - \\nabla J(\\boldsymbol{\\beta}^{(t)})$ for the update step $\\Delta \\boldsymbol{\\beta}$.\n    d. Update the parameters: $\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$.\n    e. Terminate if the Euclidean norm of the update, $\\|\\Delta \\boldsymbol{\\beta}\\|_2$, is less than $10^{-8}$ or after $50$ iterations.\n6.  For each test vector $\\mathbf{x}_{\\text{test}}$, standardize it using the training set's $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma}$, augment it to $\\mathbf{x}_{\\text{test}}^*$, calculate the predicted probability $p_{\\text{test}} = \\sigma(\\mathbf{x}_{\\text{test}}^{*T} \\boldsymbol{\\beta}_{\\text{final}})$, and classify as $1$ if $p_{\\text{test}} \\ge 0.5$, and $0$ otherwise.\nThe final output will be a list of these classifications.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and solves the logistic regression problem as specified.\n    \"\"\"\n    # Training dataset from the problem statement\n    # (c, r, s, f, t; y)\n    training_data = np.array([\n        [0.0, 0.0, 0.1, 0.0, 15.0, 0.0],\n        [1.0, 0.1, 0.2, 0.0, 12.0, 0.0],\n        [2.0, 0.2, 0.3, 1.0, 10.0, 0.0],\n        [0.0, 0.0, 0.2, 1.0, 18.0, 0.0],\n        [1.0, 0.1, 0.1, 0.0, 20.0, 0.0],\n        [2.0, 0.2, 0.15, 1.0, 16.0, 0.0],\n        [1.0, 0.0, 0.25, 0.0, 14.0, 0.0],\n        [0.0, 0.05, 0.05, 0.0, 22.0, 0.0],\n        [3.0, 0.5, 0.4, 1.0, 9.0, 0.0],\n        [5.0, 0.8, 0.7, 2.0, 5.0, 1.0],\n        [7.0, 1.2, 0.9, 3.0, 3.0, 1.0],\n        [4.0, 0.6, 0.6, 2.0, 6.0, 1.0],\n        [8.0, 1.5, 0.85, 4.0, 2.0, 1.0],\n        [6.0, 1.0, 0.8, 3.0, 4.0, 1.0],\n        [9.0, 1.8, 0.95, 5.0, 1.0, 1.0],\n        [3.0, 0.4, 0.55, 2.0, 8.0, 1.0],\n    ])\n\n    X_train_raw = training_data[:, :-1]\n    y_train = training_data[:, -1].reshape(-1, 1)\n\n    # Test cases from the problem statement\n    # Each is (lambda, (c, r, s, f, t))\n    test_cases = [\n        (1.0, (6.0, 1.0, 0.8, 3.0, 3.0)),\n        (0.0, (0.0, 0.0, 0.1, 0.0, 15.0)),\n        (10.0, (10.0, 1.5, 0.9, 5.0, 2.0)),\n        (0.5, (1.0, 0.1, 0.2, 0.0, 12.0)),\n    ]\n\n    # Step 1: Standardize features using training data statistics\n    mean_X = np.mean(X_train_raw, axis=0)\n    std_X = np.std(X_train_raw, axis=0)\n    # As per instructions, replace std=0 with 1 to avoid division by zero\n    std_X[std_X == 0] = 1.0\n    \n    X_train_std = (X_train_raw - mean_X) / std_X\n\n    # Step 2: Augment the standardized feature matrix with an intercept column\n    intercept_col = np.ones((X_train_std.shape[0], 1))\n    X_train_aug = np.hstack((intercept_col, X_train_std))\n\n    def numerically_stable_sigmoid(z):\n        \"\"\"Computes the sigmoid function in a numerically stable way.\"\"\"\n        # Using a vectorized implementation for efficiency\n        # This prevents overflow with large positive z and underflow with large negative z\n        pos_mask = (z >= 0)\n        neg_mask = (z  0)\n        p = np.zeros_like(z, dtype=float)\n        p[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        p[neg_mask] = np.exp(z[neg_mask]) / (1.0 + np.exp(z[neg_mask]))\n        return p\n\n    results = []\n    \n    num_features = X_train_aug.shape[1]\n\n    for lambda_val, test_features_raw in test_cases:\n        # Step 3: Fit the logistic regression model using Newton's method\n        \n        # Initialize parameters (beta) to zeros\n        beta = np.zeros((num_features, 1))\n        \n        # Regularization matrix I'\n        # Diagonal matrix with 0 for intercept and lambda for slopes\n        penalty_matrix = lambda_val * np.eye(num_features)\n        penalty_matrix[0, 0] = 0.0\n        \n        max_iter = 50\n        tolerance = 1e-8\n        \n        for i in range(max_iter):\n            # Calculate linear predictors and probabilities\n            z = X_train_aug @ beta\n            p = numerically_stable_sigmoid(z)\n            \n            # W is a diagonal matrix of weights p_i * (1 - p_i)\n            # Implemented with a 1D array for efficiency\n            weights = p * (1 - p)\n            W = np.diag(weights.flatten())\n            \n            # Calculate gradient of the penalized negative log-likelihood\n            # grad = X^T * (p - y) + lambda * I' * beta\n            gradient = X_train_aug.T @ (p - y_train) + penalty_matrix @ beta\n            \n            # Calculate Hessian of the penalized negative log-likelihood\n            # H = X^T * W * X + lambda * I'\n            hessian = X_train_aug.T @ W @ X_train_aug + penalty_matrix\n            \n            # Solve the linear system H * delta_beta = -gradient\n            # This is the Newton-Raphson update step\n            try:\n                # np.linalg.solve is more stable and efficient than inverting the matrix\n                delta_beta = np.linalg.solve(hessian, -gradient)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular, for robustness\n                delta_beta = np.linalg.pinv(hessian) @ -gradient\n            \n            # Update parameters\n            beta += delta_beta\n            \n            # Check for convergence\n            if np.linalg.norm(delta_beta)  tolerance:\n                break\n\n        # Step 4: Classify the test advisor\n        x_test_std = (np.array(test_features_raw) - mean_X) / std_X\n        x_test_aug = np.hstack(([1.0], x_test_std))\n        \n        # Calculate predicted probability for the test case\n        prob_test = numerically_stable_sigmoid(x_test_aug @ beta).item()\n        \n        # Classify based on the 0.5 threshold\n        classification = 1 if prob_test >= 0.5 else 0\n        results.append(classification)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful risk model does more than just make accurate predictions; it informs economically sound decisions. This practice introduces the crucial concept of asymmetric costs, acknowledging that in credit risk, failing to predict a default (a false negative) is typically far more damaging than wrongly flagging a good borrower (a false positive). By constructing a simple decision stump that explicitly minimizes a cost function based on a penalty parameter $k$, you will learn to align your model's objectives with real-world business consequences .",
            "id": "2386953",
            "problem": "You are given a binary classification problem motivated by credit risk assessment in computational economics and finance. Each observation represents a borrower described by two features and a binary outcome. Let the data consist of $n=14$ observations indexed by $i \\in \\{1,\\dots,14\\}$, with feature vector $\\mathbf{x}_i = (x_{i,1}, x_{i,2})$ and label $y_i \\in \\{0,1\\}$. The two features are the debt-to-income ratio $x_{i,1}$ (unitless fraction) and the credit score $x_{i,2}$ (unitless index scale). Defaults are indicated by $y_i=1$, non-defaults by $y_i=0$. The dataset is:\n$(x_{1,1},x_{1,2},y_1)=(0.18, 810, 0)$,\n$(x_{2,1},x_{2,2},y_2)=(0.22, 790, 0)$,\n$(x_{3,1},x_{3,2},y_3)=(0.28, 770, 0)$,\n$(x_{4,1},x_{4,2},y_4)=(0.34, 740, 0)$,\n$(x_{5,1},x_{5,2},y_5)=(0.38, 710, 0)$,\n$(x_{6,1},x_{6,2},y_6)=(0.44, 690, 0)$,\n$(x_{7,1},x_{7,2},y_7)=(0.52, 660, 1)$,\n$(x_{8,1},x_{8,2},y_8)=(0.58, 640, 1)$,\n$(x_{9,1},x_{9,2},y_9)=(0.62, 620, 1)$,\n$(x_{10,1},x_{10,2},y_{10})=(0.68, 600, 1)$,\n$(x_{11,1},x_{11,2},y_{11})=(0.74, 580, 1)$,\n$(x_{12,1},x_{12,2},y_{12})=(0.80, 560, 1)$,\n$(x_{13,1},x_{13,2},y_{13})=(0.36, 705, 1)$,\n$(x_{14,1},x_{14,2},y_{14})=(0.66, 605, 0)$.\n\nA false positive (predicting default when $y_i=0$) incurs cost $c_{\\mathrm{FP}} = 1$. A false negative (predicting non-default when $y_i=1$) incurs cost $c_{\\mathrm{FN}} = k$, where $k \\ge 0$ is a given scalar.\n\nConsider classifiers restricted to decision stumps: a single-threshold partition on one feature. For feature index $j \\in \\{0,1\\}$ corresponding to $x_{i,1}$ and $x_{i,2}$ respectively, and a threshold $t \\in \\mathbb{R}$, define the left group $L(j,t)=\\{i : x_{i,j} \\le t\\}$ and the right group $R(j,t)=\\{i : x_{i,j} > t\\}$. The stump assigns a single label to $L(j,t)$ and a single label to $R(j,t)$. For any group $S \\subseteq \\{1,\\dots,14\\}$ and for any constant prediction $\\hat{y} \\in \\{0,1\\}$ made for all $i \\in S$, the in-sample misclassification cost is\n$$\nC(S,\\hat{y};k) = \\begin{cases}\nk \\cdot \\left|\\{i \\in S : y_i=1\\}\\right|,  \\text{if } \\hat{y}=0,\\\\\n1 \\cdot \\left|\\{i \\in S : y_i=0\\}\\right|,  \\text{if } \\hat{y}=1.\n\\end{cases}\n$$\nThe optimal constant prediction for $S$ is the one that minimizes $C(S,\\hat{y};k)$, yielding a minimal group cost\n$$\nC^\\star(S;k) = \\min\\big(k \\cdot n_{1}(S), 1 \\cdot n_{0}(S)\\big),\n$$\nwhere $n_{1}(S) = \\left|\\{i \\in S : y_i=1\\}\\right|$ and $n_{0}(S) = \\left|\\{i \\in S : y_i=0\\}\\right|$. The in-sample cost of a stump $(j,t)$ is then\n$$\n\\mathcal{C}(j,t;k) = C^\\star(L(j,t);k) + C^\\star(R(j,t);k).\n$$\nCandidate thresholds $t$ for each feature $j$ are restricted to midpoints between consecutive sorted unique observed values of $\\{x_{i,j}\\}_{i=1}^{14}$, ensuring $L(j,t)$ and $R(j,t)$ are both nonempty.\n\nAdditionally, consider the trivial classifier with no split that predicts a single label for all $i$, incurring cost\n$$\n\\mathcal{C}_{\\mathrm{none}}(k) = C^\\star(\\{1,\\dots,14\\};k).\n$$\n\nSelection rule and tie-breaking conventions must be followed exactly:\n1. Among the set containing all candidate stumps $(j,t)$ and the no-split classifier, choose the one with the smallest in-sample cost. If multiple options tie in cost, prefer the no-split classifier. If the tie does not involve the no-split classifier, prefer the stump with the smaller feature index $j$. If there is still a tie, prefer the smaller threshold $t$.\n2. For each group $S$ assigned a label, when $k \\cdot n_{1}(S) = n_{0}(S)$, choose $\\hat{y}=0$.\n\nFor each selected classifier, report:\n- the feature index $j$ as an integer in $\\{0,1\\}$, or $-1$ if the no-split classifier is selected,\n- the threshold $t$ as a real number, which must be reported as $0$ if the no-split classifier is selected,\n- the predicted label for the left group $L(j,t)$, denoted $\\hat{y}_L \\in \\{0,1\\}$, or the single global prediction if no split is selected,\n- the predicted label for the right group $R(j,t)$, denoted $\\hat{y}_R \\in \\{0,1\\}$, or $-1$ if no split is selected,\n- the total in-sample cost of the selected classifier.\n\nAll real numbers in the final output must be rounded to three decimal places.\n\nTest suite. Evaluate the above for the following five values of $k$: $k \\in \\{0, 0.5, 1, 2, 5\\}$. Your program should produce a single line of output containing the results for these five cases as a comma-separated list enclosed in square brackets, with no spaces, where each case is itself encoded as a list $[j,t,\\hat{y}_L,\\hat{y}_R,\\mathrm{cost}]$. For the no-split classifier, encode the result as $[-1,0,\\hat{y},-1,\\mathrm{cost}]$. For example, a valid overall output shape is\n$[[j_1,t_1,\\hat{y}_{L,1},\\hat{y}_{R,1},\\mathrm{cost}_1],[j_2,t_2,\\hat{y}_{L,2},\\hat{y}_{R,2},\\mathrm{cost}_2],\\dots]$.",
            "solution": "The user has submitted a problem requiring the determination of an optimal classifier for a binary classification task under a custom, asymmetric cost function. I will first validate the problem statement for scientific soundness, completeness, and objectivity.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n\n- **Dataset**: $n=14$ observations $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i = (x_{i,1}, x_{i,2})$ and $y_i \\in \\{0, 1\\}$.\n  - Data points:\n    $(0.18, 810, 0)$, $(0.22, 790, 0)$, $(0.28, 770, 0)$, $(0.34, 740, 0)$, $(0.38, 710, 0)$, $(0.44, 690, 0)$, $(0.52, 660, 1)$, $(0.58, 640, 1)$, $(0.62, 620, 1)$, $(0.68, 600, 1)$, $(0.74, 580, 1)$, $(0.80, 560, 1)$, $(0.36, 705, 1)$, $(0.66, 605, 0)$.\n- **Cost Structure**:\n  - False Positive Cost: $c_{\\mathrm{FP}} = 1$.\n  - False Negative Cost: $c_{\\mathrm{FN}} = k$, for a given scalar $k \\ge 0$.\n- **Group Cost Function**: For a group $S$ and a constant prediction $\\hat{y} \\in \\{0,1\\}$, the cost is:\n$$\nC(S,\\hat{y};k) = \\begin{cases}\nk \\cdot \\left|\\{i \\in S : y_i=1\\}\\right|,  \\text{if } \\hat{y}=0,\\\\\n1 \\cdot \\left|\\{i \\in S : y_i=0\\}\\right|,  \\text{if } \\hat{y}=1.\n\\end{cases}\n$$\n- **Minimal Group Cost**: The optimal cost for a group $S$ is:\n$$\nC^\\star(S;k) = \\min\\big(k \\cdot n_{1}(S), 1 \\cdot n_{0}(S)\\big),\n$$\nwhere $n_{1}(S)$ and $n_{0}(S)$ are the counts of class $1$ and class $0$ samples in $S$, respectively.\n- **Classifier Types**:\n  1.  **Decision Stump**: A partition on one feature $j \\in \\{0,1\\}$ at a threshold $t$. The total in-sample cost is $\\mathcal{C}(j,t;k) = C^\\star(L(j,t);k) + C^\\star(R(j,t);k)$.\n      - Thresholds $t$ are midpoints between consecutive sorted unique observed feature values.\n  2.  **No-split Classifier**: A single label is predicted for all data points. The cost is $\\mathcal{C}_{\\mathrm{none}}(k) = C^\\star(\\{1,\\dots,14\\};k)$.\n- **Selection and Tie-Breaking Rules**:\n  1.  Choose the classifier (stump or no-split) with the minimum in-sample cost.\n  2.  If costs are tied, prefer the no-split classifier.\n  3.  If stumps are tied in cost, prefer the one with the smaller feature index $j$.\n  4.  If still tied (same cost, same $j$), prefer the smaller threshold $t$.\n  5.  When determining the optimal prediction for a single group $S$, if $k \\cdot n_{1}(S) = n_{0}(S)$, the prediction must be $\\hat{y}=0$.\n- **Test Suite**: Evaluate for $k \\in \\{0, 0.5, 1, 2, 5\\}$.\n- **Output Format**: For each $k$, report $[j, t, \\hat{y}_L, \\hat{y}_R, \\mathrm{cost}]$. For the no-split classifier, the format is $[-1, 0, \\hat{y}, -1, \\mathrm{cost}]$. Real numbers must be rounded to three decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is an application of decision theory to classification, a standard topic in machine learning and computational statistics. The use of asymmetric costs is fundamental in domains like credit risk, where the cost of a false negative (granting a loan to someone who defaults) is typically much higher than a false positive (denying a loan to someone who would not have defaulted). The problem is mathematically well-defined and requires no a-scientific assumptions.\n- **Well-Posed**: The problem is well-posed. The search space for classifiers is finite (one no-split option and a finite number of unique stumps). The objective function (total cost) is clearly defined for every candidate classifier. The comprehensive tie-breaking rules ensure that a unique solution exists for any given $k$.\n- **Objective**: The problem is stated in precise, objective mathematical and algorithmic terms. All data, functions, and rules are explicitly provided, leaving no room for subjective interpretation.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-defined, self-contained, and scientifically sound optimization problem. I will now proceed with the solution.\n\n## Solution\n\nThe problem requires us to find an optimal classifier—either a no-split model or a decision stump—for a given binary classification dataset by minimizing a specified cost function. The cost function is parameterized by a scalar $k$, which controls the penalty for false negative errors. We must perform this optimization for several values of $k$.\n\nThe core of the methodology is a systematic search over all valid classifiers. For each value of $k$ from the test suite, we will:\n1.  Establish a baseline by computing the cost and optimal prediction of the no-split classifier. This becomes our initial best solution.\n2.  Systematically generate and evaluate every possible decision stump.\n3.  Compare each stump's cost to the current best cost, updating our choice of optimal classifier strictly according to the provided selection and tie-breaking rules.\n\nLet us first formalize the calculation of cost and prediction for any given subset of data $S$. The minimal cost for group $S$ is $C^\\star(S;k) = \\min(k \\cdot n_{1}(S), n_{0}(S))$. The optimal prediction $\\hat{y}$ for this group is determined as follows, incorporating the specified tie-breaking rule:\n$$\n\\hat{y}(S; k) =\n\\begin{cases}\n0,  \\text{if } k \\cdot n_{1}(S) \\le n_{0}(S) \\\\\n1,  \\text{if } k \\cdot n_{1}(S) > n_{0}(S)\n\\end{cases}\n$$\n\nThe solution algorithm proceeds as follows for each tested value of $k$:\n\n**Step 1: The No-Split Classifier**\nLet $S_{all} = \\{1, \\dots, 14\\}$ be the entire dataset. We compute the total counts of class $0$ and class $1$ instances, $n_0(S_{all})$ and $n_1(S_{all})$. The cost of the no-split classifier is $\\mathcal{C}_{\\mathrm{none}}(k) = C^\\star(S_{all}; k)$, and its single prediction is $\\hat{y}(S_{all}; k)$. This configuration, `(j=-1, t=0, y_L=\\hat{y}, y_R=-1, cost=\\mathcal{C}_{\\mathrm{none}}(k))`, is set as the initial best solution.\n\n**Step 2: The Decision Stump Classifiers**\nWe iterate through each feature $j \\in \\{0, 1\\}$. For each feature, we identify the unique sorted values $\\{v_1, v_2, \\dots, v_m\\}$. The set of candidate thresholds $\\{t_1, t_2, \\dots, t_{m-1}\\}$ is formed by the midpoints $t_i = (v_i + v_{i+1})/2$.\n\nFor each candidate stump, defined by the pair $(j, t)$, we perform the following:\n- Partition the dataset into a left group $L = \\{i \\mid x_{i,j} \\le t\\}$ and a right group $R = \\{i \\mid x_{i,j} > t\\}$.\n- For each group, calculate the class counts: $n_0(L), n_1(L)$ and $n_0(R), n_1(R)$.\n- Compute the minimal cost for each group: $C^\\star(L;k)$ and $C^\\star(R;k)$.\n- Determine the optimal prediction for each group: $\\hat{y}_L = \\hat{y}(L; k)$ and $\\hat{y}_R = \\hat{y}(R; k)$.\n- The total cost for the stump is $\\mathcal{C}(j,t;k) = C^\\star(L;k) + C^\\star(R;k)$.\n\n**Step 3: Selection and Tie-Breaking**\nThe cost $\\mathcal{C}(j,t;k)$ of the current stump is compared against the cost of the best classifier found so far. The best classifier is updated based on the following prioritized rules:\n1.  A lower cost always wins.\n2.  If costs are equal, the current best solution is kept if it is the no-split classifier.\n3.  If costs are equal and both are stumps, the one with the smaller feature index $j$ is chosen.\n4.  If costs and feature indices are also equal, the stump with the smaller threshold $t$ is chosen.\n\nThis rigorous procedure ensures that for each $k$, we find the unique optimal classifier as defined by the problem statement. The algorithm will be implemented in a Python program to compute the results for the entire test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the decision stump optimization problem for a range of k values.\n    \"\"\"\n    # The dataset provided in the problem statement.\n    data = np.array([\n        [0.18, 810., 0.], [0.22, 790., 0.], [0.28, 770., 0.],\n        [0.34, 740., 0.], [0.38, 710., 0.], [0.44, 690., 0.],\n        [0.52, 660., 1.], [0.58, 640., 1.], [0.62, 620., 1.],\n        [0.68, 600., 1.], [0.74, 580., 1.], [0.80, 560., 1.],\n        [0.36, 705., 1.], [0.66, 605., 0.]\n    ])\n    \n    X = data[:, :2]\n    y = data[:, 2]\n    n_samples = X.shape[0]\n    \n    test_cases = [0.0, 0.5, 1.0, 2.0, 5.0]\n    final_results = []\n\n    for k in test_cases:\n        #\n        # Step 1: Evaluate the no-split classifier\n        #\n        n1_total = np.sum(y)\n        n0_total = n_samples - n1_total\n        \n        cost_if_0 = k * n1_total\n        cost_if_1 = 1 * n0_total\n        \n        # Tie-breaking rule for prediction: if k * n1 = n0, predict 0.\n        if cost_if_0 = cost_if_1:\n            best_cost = cost_if_0\n            best_pred = 0\n        else:\n            best_cost = cost_if_1\n            best_pred = 1\n\n        best_classifier = {\n            'j': -1,\n            't': 0.0,\n            'y_L': int(best_pred),\n            'y_R': -1,\n            'cost': best_cost\n        }\n\n        #\n        # Step 2: Evaluate all possible decision stumps\n        #\n        for j in range(X.shape[1]):\n            # Get unique sorted feature values to create thresholds\n            feature_values = np.unique(X[:, j])\n            thresholds = (feature_values[:-1] + feature_values[1:]) / 2.0\n            \n            # Sort data by the current feature to efficiently calculate splits\n            sort_indices = np.argsort(X[:, j])\n            y_sorted = y[sort_indices]\n            x_sorted = X[sort_indices, j]\n\n            for i in range(len(thresholds)):\n                t = thresholds[i]\n                \n                # Split based on sorted values to avoid re-sorting\n                split_idx = np.searchsorted(x_sorted, t, side='right')\n                y_L = y_sorted[:split_idx]\n                y_R = y_sorted[split_idx:]\n                \n                n1_L = np.sum(y_L)\n                n0_L = len(y_L) - n1_L\n                \n                n1_R = np.sum(y_R)\n                n0_R = len(y_R) - n1_R\n\n                # Calculate cost and prediction for the Left group\n                cost_L_if_0 = k * n1_L\n                cost_L_if_1 = 1 * n0_L\n                if cost_L_if_0 = cost_L_if_1:\n                    cost_L = cost_L_if_0\n                    pred_L = 0\n                else:\n                    cost_L = cost_L_if_1\n                    pred_L = 1\n                \n                # Calculate cost and prediction for the Right group\n                cost_R_if_0 = k * n1_R\n                cost_R_if_1 = 1 * n0_R\n                if cost_R_if_0 = cost_R_if_1:\n                    cost_R = cost_R_if_0\n                    pred_R = 0\n                else:\n                    cost_R = cost_R_if_1\n                    pred_R = 1\n                    \n                current_cost = cost_L + cost_R\n                \n                #\n                # Step 3: Apply selection and tie-breaking rules\n                #\n                if current_cost  best_classifier['cost']:\n                    best_classifier = {\n                        'j': j, 't': t, 'y_L': int(pred_L), \n                        'y_R': int(pred_R), 'cost': current_cost\n                    }\n                elif current_cost == best_classifier['cost']:\n                    # Rule: Prefer no-split on tie\n                    if best_classifier['j'] == -1:\n                        continue\n                    # Rule: Prefer smaller feature index j\n                    if j  best_classifier['j']:\n                        best_classifier = {\n                            'j': j, 't': t, 'y_L': int(pred_L), \n                            'y_R': int(pred_R), 'cost': current_cost\n                        }\n                    # Rule: Prefer smaller threshold t\n                    elif j == best_classifier['j'] and t  best_classifier['t']:\n                        best_classifier = {\n                            'j': j, 't': t, 'y_L': int(pred_L), \n                            'y_R': int(pred_R), 'cost': current_cost\n                        }\n\n        # Format and store the result for the current k\n        res = [\n            best_classifier['j'],\n            round(best_classifier['t'], 3),\n            best_classifier['y_L'],\n            best_classifier['y_R'],\n            round(best_classifier['cost'], 3)\n        ]\n        final_results.append(res)\n    \n    # Custom formatting to match the exact output requirement\n    # [[-1,0.0,0,-1,0.0],[0,0.48,0,1,1.5],...]\n    def format_list(lst):\n        items = []\n        for item in lst:\n            if isinstance(item, float):\n                # Ensure .0 for integers represented as floats\n                items.append(f\"{item:.3f}\")\n            else:\n                items.append(str(item))\n        return f\"[{','.join(items)}]\"\n    \n    output_str = f\"[{','.join(map(format_list, final_results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Individual credit events rarely happen in a vacuum. This final practice expands our scope from individual obligors to the financial system as a whole, tackling the critical issue of systemic risk. You will implement a network contagion model to simulate how the failure of one entity can trigger a cascade of defaults among its interconnected counterparties. This exercise provides a tangible introduction to stress testing and the dynamics of financial contagion, a primary focus for modern financial regulation .",
            "id": "2385774",
            "problem": "You are asked to formalize and implement a stress testing procedure for a loan portfolio that incorporates second-order contagion effects generated by an exogenous counterparty default. The design must start from core credit risk definitions and proceed by explicit logical rules. Consider a system with $N$ obligors indexed by $i \\in \\{0,1,\\dots,N-1\\}$. The relevant primitives are:\n- A vector of the bank’s exposures at default $b \\in \\mathbb{R}_{\\ge 0}^{N}$ to these obligors, where $b_i$ is the bank’s exposure to obligor $i$.\n- A vector of initial equity buffers $K^{(0)} \\in \\mathbb{R}_{ 0}^{N}$, where $K_i^{(0)}$ is obligor $i$’s equity before contagion.\n- An inter-obligor exposure matrix $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$ with $X_{ii} = 0$ for all $i$, where $X_{i,j}$ is the exposure of obligor $i$ to obligor $j$.\n- A loss-given-default for the bank’s loans $L^{\\text{bank}} \\in [0,1]$ and a loss-given-default for inter-obligor claims $L^{\\text{inter}} \\in [0,1]$.\n- A systemic loss multiplier $m \\ge 1$ that amplifies inter-obligor losses during stress.\n- An initial default set $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$ representing exogenous defaults at time $t=0$.\n\nFundamental definitions and rules:\n- Exposure at Default (EAD) is the outstanding exposure subject to loss when a counterparty defaults; here, the bank’s EAD to obligor $i$ is $b_i$, and inter-obligor EAD from $i$ to $j$ is $X_{i,j}$.\n- Loss Given Default (LGD) is the fraction of EAD not recovered upon default; here, the bank’s LGD on loans is $L^{\\text{bank}}$, and inter-obligor LGD is $L^{\\text{inter}}$.\n- Equity is reduced by realized credit losses. At each contagion iteration $t \\in \\{1,2,\\dots\\}$, define $K^{(t)}$ by deducting only the losses caused by counterparties that newly defaulted at iteration $t-1$. For an obligor $i$ not yet defaulted,\n$$\nK_i^{(t)} \\;=\\; K_i^{(t-1)} \\;-\\; \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big),\n$$\nwhere $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ is the set of obligors that became defaulted at iteration $t-1$ (with $D^{(-1)} := \\emptyset$). The default rule is: obligor $i$ defaults at iteration $t$ if and only if $K_i^{(t)} \\le 0$. Use a numerical tolerance $\\varepsilon = 10^{-12}$ and treat $K_i^{(t)} \\le \\varepsilon$ as defaulted to avoid floating point artifacts.\n- The contagion process terminates at the first iteration $T$ such that $\\Delta D^{(T)} = \\emptyset$. The final default set is $D^{(\\infty)} = D^{(T)}$.\n- The bank’s portfolio loss is\n$$\n\\text{Loss} \\;=\\; \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}.\n$$\n\nYour task is to implement a program that, for each test case described below, computes:\n- the bank’s total loss as a real number (express the result as a decimal, not a percentage), and\n- the total number of defaulted obligors (an integer), counting both the exogenous initial defaults and any contagion-induced defaults.\n\nIndices are zero-based in all inputs. No physical units are involved. Angles are not used. Percentages must be expressed as decimals (for example, write $0.4$ rather than $40\\%$).\n\nTest suite:\n- Case A (second-order contagion, moderate amplification):\n  - $N = 4$.\n  - $b = [10.0, 5.0, 8.0, 6.0]$.\n  - $K^{(0)} = [2.0, 2.0, 1.0, 1.1]$.\n  - $X$ has nonzero entries $X_{1,0} = 1.6$, $X_{2,0} = 1.8$, $X_{3,0} = 0.6$, $X_{1,2} = 0.9$, $X_{3,2} = 1.2$; all other entries are $0$.\n  - $L^{\\text{bank}} = 0.45$, $L^{\\text{inter}} = 0.5$, $m = 1.2$.\n  - $D^{(0)} = \\{0\\}$.\n- Case B (no contagion network):\n  - $N = 3$.\n  - $b = [5.0, 5.0, 5.0]$.\n  - $K^{(0)} = [1.0, 1.0, 1.0]$.\n  - $X$ is the $3 \\times 3$ zero matrix.\n  - $L^{\\text{bank}} = 0.4$, $L^{\\text{inter}} = 0.5$, $m = 1.2$.\n  - $D^{(0)} = \\{1\\}$.\n- Case C (boundary condition at default threshold, full cascade):\n  - $N = 3$.\n  - $b = [1.0, 1.0, 1.0]$.\n  - $K^{(0)} = [1.0, 0.5, 0.5]$.\n  - $X$ has nonzero entries $X_{1,0} = 1.0$, $X_{2,1} = 1.0$; all other entries are $0$.\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.5$, $m = 1.0$.\n  - $D^{(0)} = \\{0\\}$.\n- Case D (amplification creates contagion that would not occur at $m = 1$):\n  - $N = 4$.\n  - $b = [2.0, 2.0, 2.0, 2.0]$.\n  - $K^{(0)} = [1.2, 0.9, 0.15, 0.5]$.\n  - $X$ has nonzero entries $X_{0,3} = 0.3$, $X_{1,3} = 0.3$, $X_{2,3} = 0.2$, $X_{0,2} = 0.7$, $X_{1,2} = 0.6$; all other entries are $0$.\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.4$, $m = 2.0$.\n  - $D^{(0)} = \\{3\\}$.\n\nProgramming task:\n- Implement the above contagion dynamics precisely as stated.\n- For each case, return a list $[\\text{Loss}, \\text{Count}]$, where $\\text{Loss}$ is a floating-point number and $\\text{Count}$ is an integer. You may round intermediate computations as needed, but your final $\\text{Loss}$ should be computed using standard floating-point arithmetic and is expected to be accurate to at least $10^{-6}$ in absolute error.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s result is itself a two-element list. For example, your output must look like\n$[\\,[\\text{Loss}_A,\\text{Count}_A],[\\text{Loss}_B,\\text{Count}_B],[\\text{Loss}_C,\\text{Count}_C],[\\text{Loss}_D,\\text{Count}_D]\\,]$\nwith no spaces in the printed line.",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens.\n- The number of obligors is $N$.\n- The bank's exposures at default vector is $b \\in \\mathbb{R}_{\\ge 0}^{N}$.\n- The initial equity buffer vector is $K^{(0)} \\in \\mathbb{R}_{ 0}^{N}$.\n- The inter-obligor exposure matrix is $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$ with $X_{i,j}$ being the exposure of obligor $i$ to obligor $j$, and $X_{ii} = 0$.\n- The bank's loss-given-default is $L^{\\text{bank}} \\in [0,1]$.\n- The inter-obligor loss-given-default is $L^{\\text{inter}} \\in [0,1]$.\n- The systemic loss multiplier is $m \\ge 1$.\n- The initial set of exogenously defaulted obligors is $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$.\n- The numerical tolerance for default is $\\varepsilon = 10^{-12}$. An obligor $i$ defaults if its equity $K_i^{(t)} \\le \\varepsilon$.\n- The contagion dynamics are governed by the iterative rule for equity update:\n$$K_i^{(t)} = K_i^{(t-1)} - \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)$$\nwhere $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ and $D^{(-1)} := \\emptyset$.\n- Contagion terminates at the first iteration $T$ where $\\Delta D^{(T)} = \\emptyset$. The final default set is $D^{(\\infty)} = D^{(T)}$.\n- The bank's portfolio loss is $\\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}$.\n- Four specific test cases (A, B, C, D) are provided with all necessary numerical values for the above parameters.\n\nStep 2: Validate Using Extracted Givens.\n- **Scientifically Grounded**: The problem describes a discrete-time network model of financial contagion. The concepts of exposure, equity, loss-given-default, and default cascades are standard in the field of financial risk management and computational economics. The model, while a simplification, is based on the established principle that losses on assets (interbank exposures) can deplete a firm's equity, leading to its own failure. Thus, the problem is scientifically sound.\n- **Well-Posed**: The process is an iterative algorithm. The state of the system is the set of defaulted obligors. This set is monotonically non-decreasing. Since the total number of obligors $N$ is finite, the contagion process must terminate in at most $N$ iterations. The final state (set of all defaulted obligors) is uniquely determined by the initial conditions and the rules of the system. Therefore, a unique, stable solution for the total loss and the number of defaults exists. The problem is well-posed.\n- **Objective**: The problem is defined with precise mathematical notation and unambiguous rules. All parameters are given, and the required outputs are clearly specified. There is no subjective language.\n\nStep 3: Verdict and Action.\nThe problem is valid as it is scientifically grounded, well-posed, and objective. A solution will be provided.\n\nThe contagion process is modeled as a sequence of discrete time steps, indexed by $t \\in \\{0, 1, 2, \\dots\\}$. We must track the state of each obligor, which involves its equity capital and its default status.\n\nThe state variables at iteration $t$ are:\n- $K^{(t)} \\in \\mathbb{R}^N$: The vector of equity for all obligors.\n- $D^{(t)} \\subseteq \\{0, 1, \\dots, N-1\\}$: The cumulative set of all obligors that have defaulted up to and including iteration $t$.\n- $\\Delta D^{(t)} = D^{(t)} \\setminus D^{(t-1)}$: The set of obligors that newly defaulted at iteration $t$.\n\nThe algorithm proceeds as follows:\n1.  **Initialization ($t=0$)**:\n    - The equity vector is the initial equity, $K^{(0)}$.\n    - The cumulative default set $D^{(-1)}$ is defined as the empty set, $\\emptyset$.\n    - The initial default set $D^{(0)}$ is given exogenously.\n    - The set of newly defaulted obligors at $t=0$ is $\\Delta D^{(0)} = D^{(0)} \\setminus D^{(-1)} = D^{(0)}$.\n\n2.  **Iterative Contagion ($t=1, 2, \\dots, T$)**: The core of the simulation is a loop that continues as long as a new wave of defaults occurs. Let us define the state at the beginning of iteration $t$ as $(K^{(t-1)}, D^{(t-1)})$, with $\\Delta D^{(t-1)}$ being the set of firms that just defaulted.\n    - If $\\Delta D^{(t-1)} = \\emptyset$, the contagion has ceased. The process terminates. The final state is reached.\n    - Otherwise, we must calculate the impact on the remaining solvent obligors. For each obligor $i$ that is not yet in default (i.e., $i \\notin D^{(t-1)}$), we calculate the loss it incurs from counterparties in $\\Delta D^{(t-1)}$:\n    $$\n    L_{i, t} = \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)\n    $$\n    - The equity for these solvent obligors is then updated:\n    $$\n    K_i^{(t)} = K_i^{(t-1)} - L_{i, t}\n    $$\n    For obligors already in default or for obligors that do not have their equity updated at this step, their equity value remains unchanged, i.e., $K_i^{(t)} = K_i^{(t-1)}$.\n    - A new default set for this iteration is identified. An obligor $i$ that was previously solvent ($i \\notin D^{(t-1)}$) now defaults if its equity is wiped out, i.e., $K_i^{(t)} \\le \\varepsilon$. Let this set be $\\Delta D_{\\text{new}}^{(t)}$.\n    - The cumulative default set is updated: $D^{(t)} = D^{(t-1)} \\cup \\Delta D_{\\text{new}}^{(t)}$.\n    - The set of newly defaulted obligors for the *next* iteration is $\\Delta D^{(t)} = \\Delta D_{\\text{new}}^{(t)}$. The process then repeats for $t+1$.\n\n3.  **Termination and Final Calculation**: The loop terminates at an iteration $T$ when $\\Delta D^{(T-1)} = \\emptyset$.\n    - The final set of all defaulted obligors is $D^{(\\infty)} = D^{(T-1)}$.\n    - The total number of defaults is the cardinality of this set, $|D^{(\\infty)}|$.\n    - The total loss to the bank is calculated based on its exposures to the obligors in the final default set:\n    $$\n    \\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}\n    $$\n\nThis algorithm is implemented for each of the four test cases provided. The parameters for each case are used to initialize the simulation, which is then run to completion to find the final loss and the count of defaulted obligors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n\n    # Numerical tolerance for default check\n    EPSILON = 1e-12\n\n    test_cases = [\n        # Case A: second-order contagion, moderate amplification\n        {\n            \"N\": 4,\n            \"b\": np.array([10.0, 5.0, 8.0, 6.0]),\n            \"K0\": np.array([2.0, 2.0, 1.0, 1.1]),\n            \"X_sparse\": {(1, 0): 1.6, (2, 0): 1.8, (3, 0): 0.6, (1, 2): 0.9, (3, 2): 1.2},\n            \"L_bank\": 0.45,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {0},\n        },\n        # Case B: no contagion network\n        {\n            \"N\": 3,\n            \"b\": np.array([5.0, 5.0, 5.0]),\n            \"K0\": np.array([1.0, 1.0, 1.0]),\n            \"X_sparse\": {},\n            \"L_bank\": 0.4,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {1},\n        },\n        # Case C: boundary condition at default threshold, full cascade\n        {\n            \"N\": 3,\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"K0\": np.array([1.0, 0.5, 0.5]),\n            \"X_sparse\": {(1, 0): 1.0, (2, 1): 1.0},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.5,\n            \"m\": 1.0,\n            \"D0\": {0},\n        },\n        # Case D: amplification creates contagion that would not occur at m=1\n        {\n            \"N\": 4,\n            \"b\": np.array([2.0, 2.0, 2.0, 2.0]),\n            \"K0\": np.array([1.2, 0.9, 0.15, 0.5]),\n            \"X_sparse\": {(0, 3): 0.3, (1, 3): 0.3, (2, 3): 0.2, (0, 2): 0.7, (1, 2): 0.6},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.4,\n            \"m\": 2.0,\n            \"D0\": {3},\n        },\n    ]\n\n    def run_simulation(case):\n        \"\"\"\n        Runs the contagion simulation for a single test case.\n        \"\"\"\n        N = case[\"N\"]\n        b = case[\"b\"]\n        K = case[\"K0\"].copy()\n        X_sparse = case[\"X_sparse\"]\n        L_bank = case[\"L_bank\"]\n        L_inter = case[\"L_inter\"]\n        m = case[\"m\"]\n        D0 = case[\"D0\"]\n\n        # Construct the dense exposure matrix X\n        X = np.zeros((N, N))\n        for (i, j), val in X_sparse.items():\n            X[i, j] = val\n\n        # State variables for the simulation\n        D_final = set(D0)\n        newly_defaulted = set(D0)\n        \n        effective_lgd = m * L_inter\n\n        while newly_defaulted:\n            last_wave_defaults = list(newly_defaulted)\n            newly_defaulted = set()\n            \n            solvent_obligors = [i for i in range(N) if i not in D_final]\n            \n            if not solvent_obligors:\n                break\n                \n            # Calculate losses for solvent obligors from the last wave of defaults\n            losses = X[solvent_obligors, :][:, last_wave_defaults].sum(axis=1) * effective_lgd\n\n            # Update equity and check for new defaults\n            for idx, obligor_idx in enumerate(solvent_obligors):\n                K[obligor_idx] -= losses[idx]\n                if K[obligor_idx] = EPSILON:\n                    newly_defaulted.add(obligor_idx)\n\n            D_final.update(newly_defaulted)\n\n        # Calculate final results\n        final_loss = b[list(D_final)].sum() * L_bank\n        num_defaults = len(D_final)\n        \n        return [final_loss, num_defaults]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(case)\n        results.append(f\"[{result[0]},{result[1]}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}