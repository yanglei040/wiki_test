{
    "hands_on_practices": [
        {
            "introduction": "Understanding the theoretical performance of an algorithm is a crucial first step before implementation. This practice will guide you through an analysis of the computational complexity of the Longstaff-Schwartz Monte Carlo (LSMC) method. By deriving its scaling properties and comparing them to those of a traditional finite difference (FD) approach, you will gain a deeper appreciation for the trade-offs involved and understand the scenarios where LSMC provides a significant computational advantage, especially in high-dimensional settings .",
            "id": "2442266",
            "problem": "Consider pricing an American-style contingent claim with a two-dimensional state variable using two different numerical methods: the Longstaff–Schwartz Monte Carlo method (LSMC) and a finite difference method. Assume the following setup and conventions.\n\n- The state is two-dimensional, and there are $T$ discrete exercise dates. For the Longstaff–Schwartz Monte Carlo method (LSMC), $M$ independent sample paths are simulated with $T$ exercise dates. At each exercise date prior to maturity, one performs a least-squares regression to estimate the continuation value using a polynomial basis in the two state variables of total degree at most $p$. The regression is implemented via the normal equations, which are formed by computing $X^{\\top} X$ and $X^{\\top} y$ and then solved by Gaussian elimination at each exercise date. Assume the worst-case that all $M$ paths are in-the-money at all exercise dates, and each arithmetic operation on real numbers counts as unit cost.\n- For the finite difference method, the time domain is discretized into $N_{t}$ steps, and the spatial grid is $G \\times G$. At each time step, the American early-exercise constraint is enforced by solving a projected linear complementarity condition via Projected Successive Over-Relaxation (PSOR) with $I$ iterations per time step, where $I$ does not depend on $G$. Each PSOR iteration visits every grid point once with $\\mathcal{O}(1)$ arithmetic work per point.\n\nAssume that $N_{t} = T$ so that both methods use the same number of time levels. Ignore constant factors and lower-order terms. Work in the Random Access Machine model where forming $X^{\\top} X$ for an $M \\times K$ design matrix is $\\mathcal{O}(M K^{2})$, forming $X^{\\top} y$ is $\\mathcal{O}(M K)$, and solving a dense $K \\times K$ linear system by Gaussian elimination is $\\mathcal{O}(K^{3})$. Let $K$ denote the number of polynomial basis functions of total degree at most $p$ in two variables, including the constant term, as implied by combinatorial counting.\n\nDefine $C_{\\mathrm{LSMC}}$ and $C_{\\mathrm{FD}}$ as the leading-order arithmetic operation counts (ignoring constant factors) for LSMC and finite difference, respectively. Under the asymptotic regime where $M K^{2}$ dominates $K^{3}$ and $M$ in $C_{\\mathrm{LSMC}}$, and with $I$ treated as a constant in $C_{\\mathrm{FD}}$, derive the asymptotic ratio\n$$\nR \\equiv \\frac{C_{\\mathrm{LSMC}}}{C_{\\mathrm{FD}}}\n$$\nand simplify it fully in terms of $M$, $p$, and $G$ only.\n\nProvide your final answer as a single closed-form analytic expression for $R$. No rounding is required.",
            "solution": "The problem requires the derivation of the ratio of computational costs, $R \\equiv \\frac{C_{\\mathrm{LSMC}}}{C_{\\mathrm{FD}}}$, for two numerical methods used in pricing American-style contingent claims: the Longstaff-Schwartz Monte Carlo (LSMC) method and a finite difference (FD) method.\n\nFirst, we determine the leading-order arithmetic operation count for the LSMC method, $C_{\\mathrm{LSMC}}$.\nThe LSMC algorithm proceeds backward in time from exercise date $T-1$ down to $1$. The number of time steps where a regression is performed is $T-1$.\nAt each of these time steps, a least-squares regression is used to estimate the continuation value. The basis for the regression consists of polynomials in two state variables of total degree at most $p$. The number of such basis functions, denoted by $K$, can be determined by a combinatorial argument. The number of non-negative integer solutions to the inequality $i_1 + i_2 \\le p$ is equivalent to the number of non-negative integer solutions to $i_1 + i_2 + s = p$, where $s$ is a slack variable. This is a classic stars and bars problem, and the number of solutions is given by $\\binom{p+d}{d}$ where the number of variables is $d=2$.\nThus, the number of basis functions is:\n$$\nK = \\binom{p+2}{2} = \\frac{(p+2)!}{2!p!} = \\frac{(p+2)(p+1)}{2}\n$$\nThe regression is performed on $M$ paths, so the design matrix $X$ has dimensions $M \\times K$. The cost of the regression procedure is specified as the sum of costs for forming $X^{\\top}X$, forming $X^{\\top}y$, and solving the resulting linear system. These costs are given as $\\mathcal{O}(M K^2)$, $\\mathcal{O}(M K)$, and $\\mathcal{O}(K^3)$, respectively. The problem states that we are in an asymptotic regime where the $M K^2$ term dominates. Therefore, the cost of the regression at a single time step is taken to be proportional to $M K^2$.\nThe total cost for the LSMC method, $C_{\\mathrm{LSMC}}$, is the cost per time step multiplied by the number of time steps, which is $T-1$. Ignoring constant factors and lower-order terms, the leading-order complexity is:\n$$\nC_{\\mathrm{LSMC}} \\propto (T-1) M K^2 \\sim T M K^2\n$$\nSubstituting the expression for $K$:\n$$\nC_{\\mathrm{LSMC}} = T M \\left( \\frac{(p+1)(p+2)}{2} \\right)^2\n$$\nHere we define $C_{\\mathrm{LSMC}}$ as the dominant term in the operation count, setting the proportionality constant to $1$ as implied by the problem's instruction to \"ignore constant factors.\"\n\nNext, we determine the leading-order arithmetic operation count for the finite difference (FD) method, $C_{\\mathrm{FD}}$.\nThe FD scheme also proceeds backward in time. The number of time steps is given as $N_t$, and we are instructed to set $N_t = T$. So there are $T-1$ backward steps.\nThe spatial domain is a grid of size $G \\times G$, which contains $G^2$ points.\nAt each time step, the early-exercise constraint is handled by solving a projected linear complementarity problem via Projected Successive Over-Relaxation (PSOR). This procedure requires $I$ iterations. Each iteration visits every one of the $G^2$ grid points and performs $\\mathcal{O}(1)$ work per point. Thus, the cost of one PSOR iteration is $\\mathcal{O}(G^2)$. The total cost per time step is the cost per iteration multiplied by the number of iterations, $I$, which is $\\mathcal{O}(I G^2)$.\nThe problem states that $I$ is a constant. Therefore, the leading-order cost per time step is proportional to $G^2$.\nThe total cost for the FD method, $C_{\\mathrm{FD}}$, is the cost per time step multiplied by the number of time steps, $T-1$.\n$$\nC_{\\mathrm{FD}} \\propto (T-1) G^2 \\sim T G^2\n$$\nAgain, ignoring constant factors (which includes $I$), we define the leading-order count as:\n$$\nC_{\\mathrm{FD}} = T G^2\n$$\n\nFinally, we compute the ratio $R \\equiv \\frac{C_{\\mathrm{LSMC}}}{C_{\\mathrm{FD}}}$.\n$$\nR = \\frac{T M \\left( \\frac{(p+1)(p+2)}{2} \\right)^2}{T G^2}\n$$\nThe factor of $T$ in the numerator and denominator cancels out:\n$$\nR = \\frac{M \\left( \\frac{(p+1)(p+2)}{2} \\right)^2}{G^2}\n$$\nSimplifying this expression yields the final result:\n$$\nR = \\frac{M (p+1)^2 (p+2)^2}{4 G^2}\n$$\nThis expression gives the asymptotic ratio of the computational costs in terms of $M$, $p$, and $G$, as required.",
            "answer": "$$\n\\boxed{\\frac{M(p+1)^{2}(p+2)^{2}}{4G^{2}}}\n$$"
        },
        {
            "introduction": "The Longstaff-Schwartz algorithm is powerful, but it contains a subtle pitfall: a tendency towards an optimistic, or upwardly biased, price estimate. This bias arises from using the same set of simulated paths to both determine the optimal exercise strategy and to value the option, a form of in-sample overfitting. This hands-on coding exercise will allow you to build an LSMC pricer from scratch and directly measure this positive bias by comparing the in-sample price with a more robust out-of-sample estimate, a fundamental technique for validating results from simulation-based methods .",
            "id": "2442310",
            "problem": "Implement a complete program to quantify and illustrate the positive bias that arises in the Longstaff–Schwartz Monte Carlo (LSMC) price estimate for an American put option when the number of simulated paths is small relative to the number of basis functions used in the regression. Your implementation must be grounded in the following fundamental base: under no-arbitrage and risk-neutral pricing, the value of an American option equals the supremum over stopping times of the discounted expected payoff, and dynamic programming with conditional expectations characterizes the optimal stopping rule. In LSMC, conditional expectations are approximated by projections onto a finite-dimensional basis of functions of the state, estimated by least squares using simulated paths.\n\nYou must proceed in purely mathematical and algorithmic terms as follows:\n\n- Model dynamics under the risk-neutral probability measure by geometric Brownian motion: for stock price process $\\{S_t\\}_{t \\in [0,T]}$, simulate paths on an equidistant grid with $M$ time steps using the exact discretization of the stochastic differential equation $\\mathrm{d}S_t = r S_t \\,\\mathrm{d}t + \\sigma S_t \\,\\mathrm{d}W_t$, where $r$ is the continuously compounded risk-free rate, $\\sigma$ is the volatility, and $\\{W_t\\}$ is a standard Wiener process. Use the exact transition $S_{t+\\Delta t} = S_t \\exp\\!\\left((r - \\tfrac{1}{2}\\sigma^2)\\Delta t + \\sigma \\sqrt{\\Delta t}\\, Z\\right)$ with $Z \\sim \\mathcal{N}(0,1)$ independent across time and paths.\n\n- Consider an American put option with strike $K$ and payoff $\\max(K - S_t, 0)$ at any exercise time $t \\in [0,T]$. Let the time grid be $t_m = m \\Delta t$ with $\\Delta t = T/M$ and $m \\in \\{0,1,\\dots,M\\}$.\n\n- Implement the Longstaff–Schwartz algorithm to estimate the continuation value via least-squares projection onto a polynomial basis of the form $\\{\\phi_j(x)\\}_{j=0}^{d}$ with $\\phi_j(x) = x^j$ applied to the scaled state $x = S_t / K$, where $d$ is the polynomial degree. At each exercise time $t_m$ (excluding $t_0$), regress the discounted next-step realized cash flows on the basis evaluated at states that are currently in the money. Use ordinary least squares without regularization. Use the same simulated paths to both estimate the regression coefficients and to compute the resulting exercise strategy and cash flows to obtain the in-sample LSMC price estimate.\n\n- To remove in-sample (look-ahead) bias, perform an out-of-sample evaluation by applying the fitted regression coefficients from the training sample at each time step to an independent set of simulated paths (with the same model parameters and time grid) to determine the exercise decisions and realized discounted cash flows. The mean of these discounted cash flows gives an out-of-sample lower-bound estimate of the option price.\n\n- Define the measured bias for a given configuration as the difference between the in-sample estimate and the out-of-sample lower-bound estimate.\n\nParameter values to use (all numbers must be interpreted in the same monetary units as $S_0$ and $K$; no physical units are involved):\n- Initial stock price $S_0 = 100$.\n- Strike $K = 100$.\n- Risk-free rate $r = 0.06$.\n- Volatility $\\sigma = 0.2$.\n- Maturity $T = 1$.\n- Time steps $M = 50$ (so $\\Delta t = T/M$).\n- For numerical stability, scale the basis by $S_t/K$ as described above.\n\nRandomness and reproducibility:\n- Use a fixed pseudo-random seed for the out-of-sample (evaluation) simulations equal to $4444$.\n- For training simulations, use a per-test-case seed provided in the test suite below.\n- Use independent random numbers for training and out-of-sample evaluation.\n\nExercise timing convention:\n- Allow exercise decisions on time points $t_m$ for $m \\in \\{1,2,\\dots,M-1\\}$, and discount once more to $t_0$ for valuation at $0$. Finally, at $t_0$, take the maximum of the discounted continuation value and the immediate exercise value $\\max(K - S_0, 0)$ to ensure consistency with American exercise rights at $t_0$ for both in-sample and out-of-sample estimates.\n\nBasis specification:\n- For polynomial degree $d$, use basis size $d+1$ with monomials $\\{1, x, x^2, \\dots, x^d\\}$ where $x = S_t/K$.\n\nTest suite:\nCompute the measured bias (defined as in-sample estimate minus out-of-sample estimate) for each of the following three configurations, using the seeds as specified. In each case, use an out-of-sample evaluation set size of $N_{\\text{eval}} = 8000$ independent paths.\n1. Case A (small sample, high complexity): training paths $N_{\\text{train}} = 120$, polynomial degree $d = 6$, training seed $111$.\n2. Case B (larger sample, same complexity): training paths $N_{\\text{train}} = 1000$, polynomial degree $d = 6$, training seed $222$.\n3. Case C (small sample, lower complexity): training paths $N_{\\text{train}} = 120$, polynomial degree $d = 2$, training seed $333$.\n\nNumerical output:\n- For each case, compute the scalar bias and round it to $4$ decimal places.\n- Your program should produce a single line of output containing the three rounded biases as a comma-separated list enclosed in square brackets (for example, \"[0.1234,0.5678,0.9012]\").\n\nDesign for coverage:\n- Case A explores potential strong positive bias due to small $N_{\\text{train}}$ and relatively large basis.\n- Case B serves as a \"happy path\" where the bias should be smaller as $N_{\\text{train}}$ increases.\n- Case C serves as a contrasting edge where fewer basis functions are used with the same small $N_{\\text{train}}$ to illustrate reduced overfitting.\n\nYour implementation must be entirely self-contained, require no user input, and adhere strictly to the specified output format.",
            "solution": "We begin from the fundamental no-arbitrage principle under the risk-neutral probability measure. Let $\\{S_t\\}_{t \\in [0,T]}$ follow geometric Brownian motion under the risk-neutral measure:\n$$\n\\mathrm{d}S_t = r S_t \\,\\mathrm{d}t + \\sigma S_t \\,\\mathrm{d}W_t,\n$$\nwith continuously compounded risk-free rate $r$, volatility $\\sigma$, and standard Wiener process $\\{W_t\\}$. The exact transition over a discretization step $\\Delta t$ is\n$$\nS_{t+\\Delta t} = S_t \\exp\\left((r - \\tfrac{1}{2}\\sigma^2)\\Delta t + \\sigma \\sqrt{\\Delta t}\\, Z\\right),\n$$\nwith $Z \\sim \\mathcal{N}(0,1)$ independent over time and paths. For an American option with payoff process $\\{g(S_t)\\}$, here $g(s) = \\max(K - s, 0)$ for a put, the no-arbitrage price at time $0$ equals\n$$\nV_0 = \\sup_{\\tau \\in \\mathcal{T}} \\mathbb{E}^{\\mathbb{Q}}\\left[ e^{-r \\tau} g(S_{\\tau}) \\right],\n$$\nwhere $\\mathcal{T}$ is the set of stopping times taking values in $[0,T]$ with respect to the filtration generated by $\\{S_t\\}$ and $\\mathbb{Q}$ is the risk-neutral measure. On a discrete time grid $t_m = m \\Delta t$, $m \\in \\{0,1,\\dots,M\\}$, the dynamic programming recursion for the optimal stopping problem is\n$$\nV_m(S_{t_m}) = \\max\\left( g(S_{t_m}), \\, \\mathbb{E}^{\\mathbb{Q}}\\left[ e^{-r \\Delta t} V_{m+1}(S_{t_{m+1}}) \\mid S_{t_m} \\right] \\right),\n$$\nwith terminal condition $V_M(S_{t_M}) = g(S_{t_M})$.\n\nThe Longstaff–Schwartz Monte Carlo (LSMC) method approximates the conditional expectation $\\mathbb{E}^{\\mathbb{Q}}\\left[ e^{-r \\Delta t} V_{m+1}(S_{t_{m+1}}) \\mid S_{t_m} \\right]$ by an $L^2$-projection onto a finite-dimensional linear span of basis functions in the state (here $S_{t_m}$). Concretely, one estimates at each time step $t_m$ (excluding the initial time $t_0$) the regression\n$$\nY = \\beta_0 \\phi_0(X) + \\beta_1 \\phi_1(X) + \\cdots + \\beta_d \\phi_d(X) + \\varepsilon,\n$$\nwhere $X := S_{t_m}/K$ is the scaled state and $Y := e^{-r \\Delta t} \\hat{V}_{m+1}$ is the discounted realized cash flow from next time step onward under the current policy on simulated paths that are in the money (that is, $g(S_{t_m}) > 0$). The basis functions are the monomials $\\phi_j(x) = x^j$ for $j = 0,1,\\dots,d$. The coefficients $\\{\\beta_j\\}$ are obtained by ordinary least squares on the simulated training sample. The estimated continuation value at time $t_m$ is then\n$$\n\\widehat{C}_m(S_{t_m}) = \\sum_{j=0}^d \\widehat{\\beta}_j \\phi_j(S_{t_m}/K).\n$$\nThe estimated policy exercises at $t_m$ if $g(S_{t_m}) > \\widehat{C}_m(S_{t_m})$ for in-the-money states; otherwise, it continues. This backward induction proceeds from $t_{M-1}$ down to $t_1$, discounting realized cash flows by $e^{-r \\Delta t}$ at each step. Finally, valuation at $t_0$ discounts once more by $e^{-r \\Delta t}$ and takes the maximum with $g(S_0)$ to respect early exercise at $t_0$:\n$$\n\\widehat{V}_0 = \\max\\left(g(S_0), \\, e^{-r \\Delta t} \\cdot \\frac{1}{N} \\sum_{n=1}^N \\widehat{C}_0^{(n)} \\right),\n$$\nwhere $\\widehat{C}_0^{(n)}$ denotes the pathwise discounted continuation cash flow under the estimated policy.\n\nBias mechanism when the number of paths is small relative to the number of basis functions:\n- The regression step estimates coefficients $\\{\\widehat{\\beta}_j\\}$ using noisy targets $Y$ from the same simulated sample. When the sample size $N$ is small relative to the basis dimension $d+1$, the regression tends to overfit the noise in $Y$ on the training set, particularly because the regression is conditioned on the in-the-money subset which may be quite small at many time steps. This reduces training residuals in-sample, artificially increasing the estimated continuation values where convenient for in-sample valuation and, crucially, affecting the exercise decision so as to select pathwise outcomes that look more favorable in-sample.\n- Because the exercise decision is based on comparing $g(S_{t_m})$ to an overfitted $\\widehat{C}_m(S_{t_m})$, the decision boundary can be distorted to reduce early exercise in-sample, thereby increasing the realized discounted cash flows measured on the same sample. This introduces a positive look-ahead (or in-sample) bias in the LSMC price estimate.\n- Out-of-sample evaluation applies the same regression coefficients to an independent set of paths. Overfitting does not transfer to new data; thus, the policy typically does not exhibit the same artificially high continuation values. The out-of-sample estimate is a valid lower bound on the true price and removes the in-sample optimism, so the difference between in-sample and out-of-sample estimates provides an empirical measure of the positive bias.\n\nAlgorithmic design for the program:\n1. Simulate training paths $\\{S^{\\text{train}}_{t_m}\\}$ using the exact discretization with given $S_0$, $r$, $\\sigma$, $T$, and $M$, under a fixed training seed for reproducibility.\n2. Backward induction for training:\n   - Initialize pathwise cash flows at maturity $t_M$ as $g(S^{\\text{train}}_{t_M})$.\n   - For $m = M-1, M-2, \\dots, 1$:\n     - Discount the next-step cash flows by $e^{-r \\Delta t}$.\n     - Identify in-the-money paths where $g(S^{\\text{train}}_{t_m}) > 0$.\n     - Regress the discounted cash flows on $\\{\\phi_j(S^{\\text{train}}_{t_m}/K)\\}_{j=0}^d$ using only in-the-money paths to obtain $\\{\\widehat{\\beta}_{m,j}\\}_{j=0}^d$.\n     - Compute $\\widehat{C}_m$ for all training paths using these coefficients and update cash flows by exercising on in-the-money paths where $g(S^{\\text{train}}_{t_m}) > \\widehat{C}_m$; otherwise, continue with the discounted cash flows.\n   - After finishing at $m=1$, discount once more to $t_0$ and take $\\widehat{V}^{\\text{in}}_0 = \\max\\big(g(S_0), \\text{mean of discounted cash flows at } t_0\\big)$.\n   - Store the regression coefficients $\\{\\widehat{\\beta}_{m,j}\\}$ for all $m \\in \\{1,\\dots,M-1\\}$.\n3. Out-of-sample evaluation:\n   - Simulate an independent evaluation set $\\{S^{\\text{eval}}_{t_m}\\}$ with $N_{\\text{eval}}$ paths using the fixed evaluation seed.\n   - Initialize evaluation cash flows at $t_M$ as $g(S^{\\text{eval}}_{t_M})$.\n   - For $m = M-1, M-2, \\dots, 1$:\n     - Discount by $e^{-r \\Delta t}$.\n     - Use stored $\\{\\widehat{\\beta}_{m,j}\\}$ to compute $\\widehat{C}_m$ at $S^{\\text{eval}}_{t_m}$.\n     - Exercise on in-the-money paths where $g(S^{\\text{eval}}_{t_m}) > \\widehat{C}_m$, updating the cash flows; otherwise continue.\n   - After finishing at $m=1$, discount once more to $t_0$ and take $\\widehat{V}^{\\text{out}}_0 = \\max\\big(g(S_0), \\text{mean of discounted cash flows at } t_0\\big)$.\n4. The measured bias is $B = \\widehat{V}^{\\text{in}}_0 - \\widehat{V}^{\\text{out}}_0$, rounded to $4$ decimal places.\n\nTest-suite rationale:\n- Case A: $N_{\\text{train}} = 120$ and degree $d = 6$ means $d+1 = 7$ basis functions, which is substantial relative to the in-the-money sample size at many time steps. We expect a noticeably positive bias.\n- Case B: $N_{\\text{train}} = 1000$ with the same degree $d = 6$ increases the sample size and should reduce overfitting; thus, the measured bias should be smaller than in Case A.\n- Case C: $N_{\\text{train}} = 120$ with degree $d = 2$ reduces model complexity relative to Case A, mitigating overfitting and yielding a smaller bias than Case A, despite the same small $N_{\\text{train}}$.\n\nThe program must produce a single line with a list of the three biases in the order of Cases A, B, C, each rounded to $4$ decimal places and separated by commas. This output succinctly quantifies the effect of the ratio of $N_{\\text{train}}$ to basis dimension on in-sample optimism in LSMC.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_gbm_paths(S0, r, sigma, T, M, N, rng):\n    dt = T / M\n    # Generate standard normal increments\n    Z = rng.standard_normal(size=(N, M))\n    drift = (r - 0.5 * sigma * sigma) * dt\n    vol = sigma * np.sqrt(dt)\n    # Log increments\n    log_increments = drift + vol * Z\n    # Cumulative sum along time, prepend zeros for t0\n    log_paths = np.concatenate(\n        [np.zeros((N, 1)), np.cumsum(log_increments, axis=1)], axis=1\n    )\n    S_paths = S0 * np.exp(log_paths)\n    return S_paths  # shape (N, M+1)\n\ndef payoff_put(S, K):\n    return np.maximum(K - S, 0.0)\n\ndef basis_matrix(S_scaled, degree):\n    # S_scaled: shape (n,) or (n,1), build [1, x, x^2, ..., x^degree]\n    x = S_scaled.reshape(-1)\n    # Use Vandermonde with increasing powers\n    return np.vander(x, N=degree+1, increasing=True)\n\ndef lsmc_train_and_coeffs(S_paths, K, r, T, M, degree):\n    \"\"\"\n    Train LSMC on given paths:\n    - Returns in-sample price and list of regression coefficients per time step m=1..M-1.\n    \"\"\"\n    N = S_paths.shape[0]\n    dt = T / M\n    disc = np.exp(-r * dt)\n\n    # Initialize cashflows at maturity\n    cf = payoff_put(S_paths[:, M], K).copy()\n\n    # Store coefficients for each time step m (1..M-1). We'll use None if regression is skipped.\n    coeffs = [None] * (M + 1)\n\n    # Backward induction from m = M-1 down to 1\n    for m in range(M-1, 0, -1):\n        # Discount cashflows to time m\n        cf *= disc\n\n        S_m = S_paths[:, m]\n        immediate = payoff_put(S_m, K)\n        itm_mask = immediate > 0.0\n        idx = np.where(itm_mask)[0]\n\n        if idx.size > 0:\n            # Regression on in-the-money paths only\n            X = basis_matrix(S_m[idx] / K, degree)\n            Y = cf[idx]\n            # Ordinary least squares (minimum-norm solution if underdetermined)\n            beta, *_ = np.linalg.lstsq(X, Y, rcond=None)\n            coeffs[m] = beta\n            # Continuation estimates for all paths at time m\n            X_all = basis_matrix(S_m / K, degree)\n            cont = X_all.dot(beta)\n            # Exercise decision for in-the-money paths\n            exercise = itm_mask  (immediate > cont)\n            # Update cashflows: for exercised paths, set to immediate payoff; others keep continuation cf\n            cf[exercise] = immediate[exercise]\n        else:\n            # No in-the-money paths; keep coeffs[m] as None and no exercise updates\n            coeffs[m] = np.zeros(degree+1, dtype=float)  # zero continuation\n\n    # Discount once more to time 0\n    cf0 = cf * disc\n    # Enforce American right at t=0\n    price_in_sample = max(payoff_put(np.array([S_paths[0, 0]]), K)[0], float(np.mean(cf0)))\n    return price_in_sample, coeffs\n\ndef lsmc_evaluate_oos(S_paths_eval, K, r, T, M, degree, coeffs):\n    \"\"\"\n    Apply stored regression coefficients to an independent evaluation set to compute out-of-sample price.\n    \"\"\"\n    N = S_paths_eval.shape[0]\n    dt = T / M\n    disc = np.exp(-r * dt)\n\n    cf = payoff_put(S_paths_eval[:, M], K).copy()\n\n    for m in range(M-1, 0, -1):\n        cf *= disc\n        S_m = S_paths_eval[:, m]\n        immediate = payoff_put(S_m, K)\n        itm_mask = immediate > 0.0\n\n        beta = coeffs[m]\n        if beta is None:\n            # No regression info; continuation is zero -> immediate exercise if in-the-money\n            exercise = itm_mask  # since cont = 0\n            cf[exercise] = immediate[exercise]\n        else:\n            X_all = basis_matrix(S_m / K, degree)\n            cont = X_all.dot(beta)\n            # Exercise decision for in-the-money paths\n            exercise = itm_mask  (immediate > cont)\n            cf[exercise] = immediate[exercise]\n\n    cf0 = cf * disc\n    price_oos = max(payoff_put(np.array([S_paths_eval[0, 0]]), K)[0], float(np.mean(cf0)))\n    return price_oos\n\ndef run_case(S0, K, r, sigma, T, M, degree, N_train, N_eval, seed_train, seed_eval):\n    rng_train = np.random.default_rng(seed_train)\n    rng_eval = np.random.default_rng(seed_eval)\n\n    # Simulate training and evaluation paths\n    S_train = simulate_gbm_paths(S0, r, sigma, T, M, N_train, rng_train)\n    S_eval = simulate_gbm_paths(S0, r, sigma, T, M, N_eval, rng_eval)\n\n    # Train LSMC and get in-sample price and coefficients\n    price_in, coeffs = lsmc_train_and_coeffs(S_train, K, r, T, M, degree)\n    # Evaluate out-of-sample lower bound\n    price_oos = lsmc_evaluate_oos(S_eval, K, r, T, M, degree, coeffs)\n\n    bias = price_in - price_oos\n    return bias\n\ndef solve():\n    # Model and grid parameters\n    S0 = 100.0\n    K = 100.0\n    r = 0.06\n    sigma = 0.2\n    T = 1.0\n    M = 50\n\n    # Evaluation (out-of-sample) set size and seed\n    N_eval = 8000\n    seed_eval = 4444\n\n    # Test cases: (N_train, degree, seed_train)\n    test_cases = [\n        (120, 6, 111),   # Case A: small N_train, high degree\n        (1000, 6, 222),  # Case B: larger N_train, same degree\n        (120, 2, 333),   # Case C: small N_train, low degree\n    ]\n\n    results = []\n    for N_train, degree, seed_train in test_cases:\n        bias = run_case(S0, K, r, sigma, T, M, degree, N_train, N_eval, seed_train, seed_eval)\n        results.append(bias)\n\n    # Round to 4 decimals and print in required format\n    formatted = \"[\" + \",\".join(f\"{x:.4f}\" for x in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The accuracy of the Longstaff-Schwartz algorithm hinges on how well the chosen basis functions can approximate the true continuation value. This choice is one of the most critical implementation details, directly influencing the quality of the estimated optimal exercise strategy. In this exercise, you will move beyond simply calculating a price to investigate how the estimated early exercise boundary—the heart of the American option problem—is affected by using two different families of basis functions: simple monomials versus the more theoretically-suited Laguerre polynomials .",
            "id": "2442309",
            "problem": "Consider pricing an American put option under the risk-neutral measure using the Longstaff–Schwartz algorithm (Least Squares Monte Carlo, Longstaff–Schwartz algorithm). The underlying asset price process is modeled as geometric Brownian motion with risk-neutral dynamics given by the stochastic differential equation under the risk-neutral probability measure: $$dS_t = (r - q) S_t \\, dt + \\sigma S_t \\, dW_t,$$ where $S_t$ is the asset price, $r$ is the continuously compounded risk-free rate, $q$ is the continuous dividend yield, $\\sigma$ is the volatility, and $W_t$ is a standard Brownian motion. The American put has strike $K$ and maturity $T$, with payoff at time $t$ given by $$\\max\\{K - S_t, 0\\}.$$ The Longstaff–Schwartz algorithm approximates the conditional continuation value at each exercise time by projecting the discounted realized cash flows onto a chosen family of basis functions of the current state. The choice of basis family creates approximation error that can affect the estimated optimal stopping policy and, in particular, the estimated exercise boundary. Your task is to quantify the sensitivity of the estimated exercise boundary to the basis family by running the algorithm twice on the same simulated paths, once with simple monomials and once with Laguerre polynomials, and then comparing the resulting estimated boundaries at selected times.\n\nYou must implement the following, starting from first principles in risk-neutral valuation and optimal stopping:\n\n1) Simulate $S_t$ on a uniform time grid using the exact solution of geometric Brownian motion. For $N$ time steps of size $\\Delta t = T/N$, the recursion for each path is $$S_{t+\\Delta t} = S_t \\exp\\left(\\left(r - q - \\tfrac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t} \\, Z\\right),$$ where $Z \\sim \\mathcal{N}(0,1)$ are independent standard normal draws.\n\n2) Implement the Longstaff–Schwartz algorithm for an American put:\n   a) At maturity $t_N = T$, set the cash flow for each path to the payoff $C_N = \\max\\{K - S_{t_N}, 0\\}$.\n   b) Proceed backward for $i = N-1, N-2, \\dots, 1$. First discount existing cash flows by the single-step discount factor $\\exp(-r \\Delta t)$ to obtain their value at $t_i$. Among paths that are in the money at $t_i$ (i.e., those with $\\max\\{K - S_{t_i}, 0\\}  0$), fit a least squares regression of the discounted cash flows on a chosen family of basis functions of the scaled state $x = S_{t_i}/K$. Use the regression to estimate the continuation value $\\widehat{C}_{\\text{cont}}(S_{t_i})$ for the in-the-money paths. Exercise on those in-the-money paths for which the immediate exercise value $\\max\\{K - S_{t_i}, 0\\}$ is greater than or equal to the estimated continuation value; otherwise, continue and keep the discounted cash flow unchanged.\n   c) After the backward induction ends at $t_1$, discount the remaining cash flows one more step to time $t_0$ to obtain the time-$0$ present value, and compute the option price estimate as the average across paths. This price is not the target output here but is needed for internal consistency of the algorithm.\n\n3) For the regression in step 2b, use two different basis families on the same simulated paths:\n   a) Monomials: use the basis vector $$[1, x, x^2, x^3]$$ where $x = S_{t_i}/K$.\n   b) Laguerre polynomials: use the first four (physicists’) Laguerre polynomials evaluated at $x = S_{t_i}/K$, namely $$L_0(x) = 1, \\; L_1(x) = 1 - x, \\; L_2(x) = 1 - 2x + \\tfrac{x^2}{2}, \\; L_3(x) = 1 - 3x + \\tfrac{3}{2}x^2 - \\tfrac{1}{6}x^3.$$ Construct the regression design matrix with these columns.\n\n4) At three interior exercise times specified by the fractions $\\{1/4, 1/2, 3/4\\}$ of maturity, estimate the exercise boundary for each basis family as follows. Let $t_i$ be the time index nearest to each fraction of $T$ (rounded to the nearest integer index in $\\{1, \\dots, N-1\\}$). At time $t_i$, given the fitted regression function $\\widehat{C}_{\\text{cont}}(S)$, define the function $$f(S) = (K - S) - \\widehat{C}_{\\text{cont}}(S).$$ On the grid $S \\in [0, K]$ discretized uniformly with $M$ points, find the supremum of $S$ on this grid such that $f(S) \\ge 0$. If there exists an index $j$ such that $f(S_j) \\ge 0$ and $f(S_{j+1})  0$, linearly interpolate between $S_j$ and $S_{j+1}$ to refine the root estimate; otherwise, if $f(S) \\ge 0$ for all grid points set the boundary to $K$, and if $f(S)  0$ for all grid points set the boundary to $0$. This defines an estimated exercise boundary $\\widehat{S}^*_{\\text{family}}(t_i)$ for each basis family at each selected time.\n\n5) Sensitivity metric: For each test case, compute the maximum absolute difference across the three selected times between the boundaries obtained with Laguerre and monomial bases: $$D = \\max_{t \\in \\{T/4, T/2, 3T/4\\}} \\left| \\widehat{S}^*_{\\text{Laguerre}}(t) - \\widehat{S}^*_{\\text{Monomials}}(t) \\right|.$$\n\nImplementation details and constraints:\n- Use exactly $N$ time steps as specified per test case, and exactly $P$ independent paths per test case (see below). Use the same random number seed and the same simulated paths for both basis families within each test case to isolate the effect of the basis choice.\n- Use the exact geometric Brownian motion discretization outlined in item $1$.\n- Use least squares via a numerically stable method. If there are no in-the-money paths at a regression step, define the boundary estimate at that time (if needed) to be $0$ by convention. If there is at least one in-the-money path, proceed with regression; if ill-conditioned, use the least squares solution that minimizes the squared residuals.\n- For the boundary grid use $M = 400$ evenly spaced points on $[0, K]$.\n- All rates and volatilities are expressed as decimals (for example, $0.05$ for $5$ percent), and there are no physical units involved.\n\nTest suite:\nProvide results for the following three test cases. In all cases, use $P = 8{,}192$ paths, the boundary grid size $M = 400$, and the time step count $N$ as specified.\n\n1) Case A (balanced, moderate volatility): $S_0 = 100$, $K = 100$, $r = 0.05$, $q = 0$, $\\sigma = 0.2$, $T = 1$, $N = 50$, random seed $= 12345$.\n2) Case B (out-of-the-money start, high volatility): $S_0 = 90$, $K = 100$, $r = 0.03$, $q = 0$, $\\sigma = 0.5$, $T = 1$, $N = 50$, random seed $= 2023$.\n3) Case C (in-the-money start, low volatility, shorter maturity): $S_0 = 110$, $K = 100$, $r = 0.01$, $q = 0$, $\\sigma = 0.1$, $T = 0.5$, $N = 40$, random seed $= 42$.\n\nFinal output format:\nYour program should produce a single line of output containing the three sensitivity values $D$ for the test cases A, B, and C, respectively, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets, for example, [0.123456,0.234567,0.345678].",
            "solution": "The problem requires the implementation of the Longstaff-Schwartz algorithm to analyze the sensitivity of the American put option's exercise boundary to the choice of basis functions. The solution proceeds logically from first principles.\n\n**1. Risk-Neutral Simulation of Asset Paths**\nThe foundation of derivative pricing is the principle of risk-neutral valuation. We simulate the evolution of the underlying asset price $S_t$ under the risk-neutral measure $\\mathbb{Q}$. The dynamics are described by the stochastic differential equation:\n$$dS_t = (r - q) S_t \\, dt + \\sigma S_t \\, dW_t^{\\mathbb{Q}}$$\nwhere $r$ is the risk-free rate, $q$ is the dividend yield, $\\sigma$ is the volatility, and $W_t^{\\mathbb{Q}}$ is a standard Brownian motion under $\\mathbb{Q}$. This equation has an exact solution, which allows for precise simulation over a discrete time grid $t_0, t_1, \\dots, t_N$, where $t_i = i \\Delta t$ and $\\Delta t = T/N$. For each of $P$ paths, we generate the price sequence as:\n$$S_{t_{i+1}} = S_{t_i} \\exp\\left(\\left(r - q - \\frac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t} Z_{i+1}\\right)$$\nHere, $\\{Z_i\\}_{i=1}^N$ are independent random variables drawn from a standard normal distribution, $\\mathcal{N}(0,1)$. The same set of $P$ simulated paths is used for both basis function analyses to ensure a controlled comparison.\n\n**2. The Longstaff-Schwartz Algorithm: Backward Induction and Regression**\nThe value of an American option at any time $t$ is the maximum of its immediate exercise value and its continuation value (the expected value of holding the option). This optimal stopping problem is solved using dynamic programming via backward induction.\n\nLet $C_i(S_{t_i})$ be the value of the option at time $t_i$ for a given state $S_{t_i}$.\nAt maturity, $t_N = T$, the option value is simply its intrinsic value:\n$$C_N(S_{t_N}) = \\max(K - S_{t_N}, 0)$$\nWe then step backward in time from $i = N-1$ to $1$. At each time $t_i$, the continuation value, $C_{\\text{cont}}(S_{t_i})$, is the discounted expected value of the option at the next step, conditional on the information at $t_i$:\n$$C_{\\text{cont}}(S_{t_i}) = \\mathbb{E}^{\\mathbb{Q}}\\left[e^{-r\\Delta t} C_{i+1}(S_{t_{i+1}}) \\mid S_{t_i}\\right]$$\nThe core innovation of the Longstaff-Schwartz algorithm is to approximate this conditional expectation using a linear regression. The regression is performed only for paths that are \"in-the-money\" ($S_{t_i}  K$), as for out-of-the-money paths the decision is always to continue. For these in-the-money paths, we regress the discounted future cash flows $\\{e^{-r\\Delta t} C_{i+1}^{(j)}\\}$ (where $j$ indexes the path) onto a set of basis functions $\\{f_k(S_{t_i}^{(j)}/K)\\}_{k=0}^d$ of the scaled state variable $x = S_{t_i}/K$. This yields an estimated continuation value function:\n$$\\widehat{C}_{\\text{cont}}(S_{t_i}) = \\sum_{k=0}^{d} \\beta_{i,k} f_k(S_{t_i}/K)$$\nThe coefficients $\\boldsymbol{\\beta}_i = \\{\\beta_{i,k}\\}$ are found by minimizing the sum of squared errors. We employ two different basis function families as specified:\n-   **Monomials**: A basis of $[1, x, x^2, x^3]$.\n-   **Laguerre Polynomials**: A basis of $[L_0(x), L_1(x), L_2(x), L_3(x)]$.\nOnce $\\widehat{C}_{\\text{cont}}(S_{t_i})$ is estimated, the optimal stopping rule is applied for each in-the-money path:\n$$\n\\text{If } \\max(K - S_{t_i}, 0) \\geq \\widehat{C}_{\\text{cont}}(S_{t_i}), \\text{ then exercise; otherwise, continue.}\n$$\nIf exercise is optimal, the cash flow for that path at time $t_i$ is set to $K - S_{t_i}$, and this value is carried backward (after discounting) in subsequent steps. If continuation is optimal, the cash flow at $t_i$ is the discounted cash flow from $t_{i+1}$. This process defines the vector of cash flows $\\{C_i^{(j)}\\}$ for all paths.\n\n**3. Estimation of the Exercise Boundary**\nThe exercise boundary, $S^*(t)$, is the critical stock price at which an option holder is indifferent between immediate exercise and continuation. This indifference point is defined by the equality of the exercise value and the continuation value:\n$$K - S^*(t) = C_{\\text{cont}}(S^*(t), t)$$\nWe estimate this boundary $\\widehat{S}^*(t_i)$ at specified times $t_i$ by finding the root of the function $f(S) = (K - S) - \\widehat{C}_{\\text{cont}}(S)$, where $\\widehat{C}_{\\text{cont}}(S)$ is constructed using the regression coefficients $\\boldsymbol{\\beta}_i$ computed at step $t_i$. The root is located numerically by evaluating $f(S)$ on a fine grid of $M=400$ points over the interval $[0, K]$. Where a sign change in $f(S)$ is detected between two adjacent grid points $S_j$ and $S_{j+1}$, the root is approximated with high precision using linear interpolation. If $f(S) \\geq 0$ across the entire grid, the boundary is set to $K$. If $f(S)  0$ everywhere, it is set to $0$.\n\n**4. Sensitivity Analysis**\nThe entire procedure is executed twice for each test case—once with monomial basis functions and once with Laguerre basis functions, on the identical set of simulated asset paths. This isolates the effect of the basis function choice on the estimated exercise boundary. For each specified time point $t_i \\in \\{T/4, T/2, 3T/4\\}$, we compute the two boundary estimates, $\\widehat{S}^*_{\\text{Monomials}}(t_i)$ and $\\widehat{S}^*_{\\text{Laguerre}}(t_i)$. The final sensitivity metric, $D$, is the maximum absolute difference between these estimates across the three time points:\n$$D = \\max_{i \\in \\text{indices for } \\{T/4, T/2, 3T/4\\}} \\left| \\widehat{S}^*_{\\text{Laguerre}}(t_i) - \\widehat{S}^*_{\\text{Monomials}}(t_i) \\right|$$\nThis metric quantifies the model risk associated with the choice of basis functions in the Longstaff-Schwartz algorithm. The following code implements this complete methodology.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_basis_functions(x, basis_type):\n    \"\"\"\n    Generates the design matrix for given states and basis type.\n\n    Args:\n        x (np.ndarray): Scaled asset prices (S/K).\n        basis_type (str): 'monomials' or 'laguerre'.\n\n    Returns:\n        np.ndarray: The design matrix (number of paths x number of basis functions).\n    \"\"\"\n    if basis_type == 'monomials':\n        return np.vstack([np.ones_like(x), x, x**2, x**3]).T\n    elif basis_type == 'laguerre':\n        L0 = np.ones_like(x)\n        L1 = 1.0 - x\n        L2 = 1.0 - 2.0 * x + 0.5 * x**2\n        L3 = 1.0 - 3.0 * x + 1.5 * x**2 - (1.0 / 6.0) * x**3\n        return np.vstack([L0, L1, L2, L3]).T\n    else:\n        raise ValueError(\"Unknown basis type specified.\")\n\ndef run_lsm_and_get_boundaries(S_paths, K, r, dt, N, M, basis_type, boundary_times_indices):\n    \"\"\"\n    Runs the Longstaff-Schwartz algorithm and computes exercise boundaries at specified times.\n\n    Args:\n        S_paths (np.ndarray): Simulated asset price paths.\n        K (float): Strike price.\n        r (float): Risk-free rate.\n        dt (float): Time step size.\n        N (int): Number of time steps.\n        M (int): Number of points in the boundary estimation grid.\n        basis_type (str): 'monomials' or 'laguerre'.\n        boundary_times_indices (set): Set of time indices to estimate the boundary.\n\n    Returns:\n        dict: A dictionary mapping time index to the estimated boundary value.\n    \"\"\"\n    P = S_paths.shape[0]\n    cash_flows = np.maximum(K - S_paths[:, -1], 0)\n    boundary_estimates = {}\n\n    for i in range(N - 1, 0, -1):\n        cash_flows *= np.exp(-r * dt)\n        S_t = S_paths[:, i]\n        itm_indices = np.where(S_t  K)[0]\n\n        if len(itm_indices) > 0:\n            X = get_basis_functions(S_t[itm_indices] / K, basis_type)\n            y = cash_flows[itm_indices]\n\n            try:\n                beta = np.linalg.lstsq(X, y, rcond=None)[0]\n            except np.linalg.LinAlgError:\n                # In case of a fatal error, which is unlikely with lstsq, \n                # we assume continuation value is 0 (or some other robust handling).\n                # This would mean exercising whenever in the money.\n                beta = np.zeros(X.shape[1])\n\n            if i in boundary_times_indices:\n                S_grid = np.linspace(1e-6, K, M)\n                x_grid = S_grid / K\n                basis_grid = get_basis_functions(x_grid, basis_type)\n                cont_val_grid = basis_grid @ beta\n                f_S = (K - S_grid) - cont_val_grid\n\n                positive_f = np.where(f_S >= 0)[0]\n                if len(positive_f) == 0:\n                    boundary = 0.0\n                elif len(positive_f) == M:\n                    boundary = K\n                else:\n                    root_idx = positive_f[-1]\n                    if root_idx + 1  M:\n                        f1, f2 = f_S[root_idx], f_S[root_idx + 1]\n                        s1, s2 = S_grid[root_idx], S_grid[root_idx + 1]\n                        boundary = s1 - f1 * (s2 - s1) / (f2 - f1)\n                    else:\n                        boundary = K\n                boundary_estimates[i] = boundary\n\n            continuation_value = get_basis_functions(S_t[itm_indices] / K, basis_type) @ beta\n            exercise_value = K - S_t[itm_indices]\n            exercise_indices = itm_indices[exercise_value >= continuation_value]\n            cash_flows[exercise_indices] = K - S_t[exercise_indices]\n        \n        else:\n            if i in boundary_times_indices:\n                boundary_estimates[i] = 0.0\n\n    return boundary_estimates\n\ndef calculate_sensitivity(S0, K, r, q, sigma, T, N, P, M, seed):\n    \"\"\"\n    Calculates the sensitivity metric D for a single test case.\n\n    Args:\n        S0, K, r, q, sigma, T, N, P, M, seed: All problem parameters.\n\n    Returns:\n        float: The sensitivity metric D.\n    \"\"\"\n    np.random.seed(seed)\n    dt = T / N\n    S_paths = np.zeros((P, N + 1))\n    S_paths[:, 0] = S0\n    drift = (r - q - 0.5 * sigma**2) * dt\n    diffusion = sigma * np.sqrt(dt)\n\n    for i in range(1, N + 1):\n        Z = np.random.standard_normal(P)\n        S_paths[:, i] = S_paths[:, i-1] * np.exp(drift + diffusion * Z)\n\n    time_fractions = [0.25, 0.5, 0.75]\n    boundary_times_indices = {int(round(f * N)) for f in time_fractions}\n    boundary_times_indices = {max(1, min(N-1, idx)) for idx in boundary_times_indices}\n\n    boundaries_mono = run_lsm_and_get_boundaries(S_paths, K, r, dt, N, M, 'monomials', boundary_times_indices)\n    boundaries_laguerre = run_lsm_and_get_boundaries(S_paths, K, r, dt, N, M, 'laguerre', boundary_times_indices)\n    \n    differences = []\n    for idx in sorted(list(boundary_times_indices)):\n        b_mono = boundaries_mono.get(idx, 0.0)\n        b_lag = boundaries_laguerre.get(idx, 0.0)\n        differences.append(np.abs(b_lag - b_mono))\n        \n    return np.max(differences) if differences else 0.0\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'S0': 100., 'K': 100., 'r': 0.05, 'q': 0., 'sigma': 0.2, 'T': 1.0, 'N': 50, 'seed': 12345},\n        {'S0': 90., 'K': 100., 'r': 0.03, 'q': 0., 'sigma': 0.5, 'T': 1.0, 'N': 50, 'seed': 2023},\n        {'S0': 110., 'K': 100., 'r': 0.01, 'q': 0., 'sigma': 0.1, 'T': 0.5, 'N': 40, 'seed': 42}\n    ]\n    common_params = {'P': 8192, 'M': 400}\n    \n    results = []\n    for case_params in test_cases:\n        params = {**case_params, **common_params}\n        D = calculate_sensitivity(**params)\n        results.append(D)\n        \n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}