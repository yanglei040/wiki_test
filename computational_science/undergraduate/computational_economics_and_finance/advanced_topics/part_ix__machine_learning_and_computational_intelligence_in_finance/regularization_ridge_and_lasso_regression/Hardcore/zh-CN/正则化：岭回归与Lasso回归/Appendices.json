{
    "hands_on_practices": [
        {
            "introduction": "为了真正理解正则化惩罚项 $\\lambda$ 的作用，一个富有启发性的方法是研究其极端情况。这个练习将探讨当惩罚力度变得无限大时，模型会发生什么，这迫使模型呈现出其最简单的形式。通过从第一性原理出发进行推导，你将加深对系数收缩效应以及未受惩罚的截距项角色的理解。",
            "id": "2426322",
            "problem": "一位计量经济学家将一项资产的超额收益建模为在 $n$ 个时间段内观察到的 $p$ 个预测变量的线性函数。令响应向量为 $y \\in \\mathbb{R}^{n}$，预测变量矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中第 $i$ 行为 $x_{i}^{\\top} \\in \\mathbb{R}^{p}$。考虑带有截距 $ \\beta_{0} \\in \\mathbb{R} $ 和斜率向量 $\\beta \\in \\mathbb{R}^{p}$ 的线性模型，其中截距不被惩罚。定义给定惩罚水平 $\\lambda \\geq 0$ 的 Ridge 目标函数为\n$$\nQ_{\\lambda}^{\\mathrm{R}}(\\beta_{0},\\beta) \\equiv \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-x_{i}^{\\top}\\beta\\right)^{2}+\\lambda \\|\\beta\\|_{2}^{2},\n$$\n以及最小绝对收缩和选择算子 (LASSO) 目标函数为\n$$\nQ_{\\lambda}^{\\mathrm{L}}(\\beta_{0},\\beta) \\equiv \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-x_{i}^{\\top}\\beta\\right)^{2}+\\lambda \\|\\beta\\|_{1}.\n$$\n对于每个 $\\lambda \\geq 0$，令 $(\\beta_{0}^{\\mathrm{R}}(\\lambda),\\beta^{\\mathrm{R}}(\\lambda))$ 和 $(\\beta_{0}^{\\mathrm{L}}(\\lambda),\\beta^{\\mathrm{L}}(\\lambda))$ 分别是 $Q_{\\lambda}^{\\mathrm{R}}$ 和 $Q_{\\lambda}^{\\mathrm{L}}$ 的最小化子。用以下方式表示样本均值\n$$\n\\bar{y} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} y_{i}\n\\quad\\text{and}\\quad\n\\bar{x} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} x_{i}.\n$$\n仅假设 $n \\geq 1$ 和 $p \\geq 1$，并且在两个目标函数中截距都不被惩罚。\n\n从第一性原理出发，证明当 $\\lambda \\to \\infty$ 时，$\\beta^{\\mathrm{R}}(\\lambda)$ 和 $\\beta^{\\mathrm{L}}(\\lambda)$ 都收敛到 $\\mathbb{R}^{p}$ 中的零向量。然后，确定截距参数 $\\beta_{0}^{\\mathrm{R}}(\\lambda)$ (等价于 $\\beta_{0}^{\\mathrm{L}}(\\lambda)$) 在 $\\lambda \\to \\infty$ 时的精确极限的封闭形式，该形式仅用 $\\bar{y}$ 和 $\\bar{x}$ 表示。\n\n你的最终答案应该是一个给出截距在 $\\lambda \\to \\infty$ 时极限的单一解析表达式。不需要数值近似。",
            "solution": "该问题提法清晰，具有科学依据，并包含了进行严格数学推导所需的所有信息。因此，该问题被认为是有效的。我们将继续进行解答，该解答根据问题陈述的要求分为两部分。首先，我们将证明当惩罚参数 $\\lambda$ 趋于无穷大时，Ridge 和 LASSO 回归的斜率向量都收敛到零向量。其次，我们将在相同条件下确定截距参数的极限。\n\n令通用目标函数表示为 $Q_{\\lambda}(\\beta_{0}, \\beta)$，它代表 Ridge 目标函数 $Q_{\\lambda}^{\\mathrm{R}}$ 或 LASSO 目标函数 $Q_{\\lambda}^{\\mathrm{L}}$。这可以写成：\n$$\nQ_{\\lambda}(\\beta_{0}, \\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - x_i^\\top \\beta)^2 + \\lambda P(\\beta)\n$$\n其中，对于 Ridge 回归，$P(\\beta) = \\|\\beta\\|_{2}^{2}$；对于 LASSO 回归，$P(\\beta) = \\|\\beta\\|_{1}$。令 $(\\beta_0(\\lambda), \\beta(\\lambda))$ 表示在给定 $\\lambda \\geq 0$ 时最小化该函数的参数对 $(\\beta_0, \\beta)$。\n\n首先，我们证明 $\\lim_{\\lambda \\to \\infty} \\beta(\\lambda) = \\mathbf{0}$。\n根据最小化子的定义，对于任何其他参数对 $(\\beta_0', \\beta')$，以下不等式成立：\n$$\nQ_{\\lambda}(\\beta_0(\\lambda), \\beta(\\lambda)) \\leq Q_{\\lambda}(\\beta_0', \\beta')\n$$\n我们选择一个特定的、方便的点进行比较：$(\\beta_0', \\beta') = (\\bar{y}, \\mathbf{0})$，其中 $\\mathbf{0}$ 是 $\\mathbb{R}^p$ 中的零向量。在此点，目标函数的值为：\n$$\nQ_{\\lambda}(\\bar{y}, \\mathbf{0}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y} - x_i^\\top \\mathbf{0})^2 + \\lambda P(\\mathbf{0})\n$$\n对于 Ridge 和 LASSO 惩罚项，$P(\\mathbf{0}) = 0$。因此，表达式简化为：\n$$\nQ_{\\lambda}(\\bar{y}, \\mathbf{0}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n$$\n这个量是一个有限常数，仅取决于数据 $y$，而不取决于 $\\lambda$。我们用 $C$ 表示这个常数。来自最小化子定义的不等式变为：\n$$\nQ_{\\lambda}(\\beta_0(\\lambda), \\beta(\\lambda)) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0(\\lambda) - x_i^\\top \\beta(\\lambda))^2 + \\lambda P(\\beta(\\lambda)) \\leq C\n$$\n第一项，即均方误差，必然是非负的。因此，我们可以为惩罚项推导出一个不等式：\n$$\n\\lambda P(\\beta(\\lambda)) \\leq C\n$$\n对于 $\\lambda > 0$，我们可以写成：\n$$\nP(\\beta(\\lambda)) \\leq \\frac{C}{\\lambda}\n$$\n当 $\\lambda \\to \\infty$ 时，右侧的 $\\frac{C}{\\lambda}$ 趋近于 $0$。由于惩罚函数 $P(\\beta)$ 总是非负的，根据夹逼定理，我们必有 $\\lim_{\\lambda \\to \\infty} P(\\beta(\\lambda)) = 0$。\n\n对于 Ridge 回归，$P(\\beta) = \\|\\beta\\|_2^2 = \\sum_{j=1}^{p} (\\beta_j)^2$。条件 $\\lim_{\\lambda \\to \\infty} \\|\\beta^{\\mathrm{R}}(\\lambda)\\|_2^2 = 0$ 意味着向量的每个分量都必须趋于零。因此，$\\lim_{\\lambda \\to \\infty} \\beta^{\\mathrm{R}}(\\lambda) = \\mathbf{0}$。\n对于 LASSO 回归，$P(\\beta) = \\|\\beta\\|_1 = \\sum_{j=1}^{p} |\\beta_j|$。条件 $\\lim_{\\lambda \\to \\infty} \\|\\beta^{\\mathrm{L}}(\\lambda)\\|_1 = 0$ 也意味着每个分量都必须趋于零。因此，$\\lim_{\\lambda \\to \\infty} \\beta^{\\mathrm{L}}(\\lambda) = \\mathbf{0}$。\n这就完成了问题第一部分的证明。\n\n其次，我们确定截距参数 $\\beta_0(\\lambda)$ 在 $\\lambda \\to \\infty$ 时的极限。\n对于任何固定的 $\\lambda \\geq 0$，最优截距 $\\beta_0(\\lambda)$ 是通过对给定的斜率向量 $\\beta$ 最小化关于 $\\beta_0$ 的目标函数来找到的。由于惩罚项不依赖于 $\\beta_0$，我们只需要对均方误差项求导。\n$$\n\\frac{\\partial Q_{\\lambda}}{\\partial \\beta_0} = \\frac{\\partial}{\\partial \\beta_0} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - x_i^\\top \\beta)^2 \\right] = \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\beta_0 - x_i^\\top \\beta)(-1)\n$$\n将此偏导数设为零，以找到与最优 $\\beta(\\lambda)$ 对应的最优 $\\beta_0(\\lambda)$：\n$$\n-\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0(\\lambda) - x_i^\\top \\beta(\\lambda)) = 0\n$$\n$$\n\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0(\\lambda) - \\sum_{i=1}^{n} x_i^\\top \\beta(\\lambda) = 0\n$$\n使用样本均值的定义，$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ 和 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$：\n$$\nn\\bar{y} - n\\beta_0(\\lambda) - \\left( \\sum_{i=1}^{n} x_i \\right)^\\top \\beta(\\lambda) = 0\n$$\n$$\nn\\bar{y} - n\\beta_0(\\lambda) - (n\\bar{x})^\\top \\beta(\\lambda) = 0\n$$\n由于 $n \\geq 1$，我们可以除以 $n$：\n$$\n\\bar{y} - \\beta_0(\\lambda) - \\bar{x}^\\top \\beta(\\lambda) = 0\n$$\n这给出了对于任何 $\\lambda \\geq 0$ 的最优截距的精确表达式：\n$$\n\\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\beta(\\lambda)\n$$\n这个关系对 Ridge 和 LASSO 估计量都有效，因为推导过程不依赖于惩罚项 $P(\\beta)$ 的形式。为了找到当 $\\lambda \\to \\infty$ 时的极限，我们将极限算子应用于此表达式：\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\lim_{\\lambda \\to \\infty} (\\bar{y} - \\bar{x}^\\top \\beta(\\lambda))\n$$\n由于 $\\bar{y}$ 和 $\\bar{x}$ 是关于 $\\lambda$ 的常数，并且内积是一个连续函数，我们可以将极限移入：\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\left( \\lim_{\\lambda \\to \\infty} \\beta(\\lambda) \\right)\n$$\n从我们证明的第一部分，我们已经确定 $\\lim_{\\lambda \\to \\infty} \\beta(\\lambda) = \\mathbf{0}$。代入这个结果即可得到最终答案：\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\mathbf{0} = \\bar{y}\n$$\n因此，当惩罚参数 $\\lambda$ 趋于无穷大时，Ridge 和 LASSO 的斜率系数都收缩到零，而截距项收敛于响应变量的样本均值 $\\bar{y}$。",
            "answer": "$$\\boxed{\\bar{y}}$$"
        },
        {
            "introduction": "在理解了惩罚项如何简化模型之后，我们现在转向一个关键问题：当预测变量相关时，岭回归和 LASSO 的行为有何不同。这个练习通过一个精心构建的场景，展示了 LASSO 在变量选择上的一个惊人表现，而岭回归则表现出更稳定的“平均”效应。这个计算实践将用具体的数字来揭示 $L_1$ 和 $L_2$ 惩罚在处理相关数据时的本质区别。",
            "id": "2426291",
            "problem": "一位分析师正在估计一个投资组合的单期超额收益的线性预测模型，该模型使用两个高度相关的预测变量，涵盖两个时间段，并且不含截距项。设响应向量为 $y \\in \\mathbb{R}^{2}$，预测变量矩阵为 $X \\in \\mathbb{R}^{2 \\times 2}$，其列向量 $x_{1}$ 和 $x_{2}$ 由下式给出\n$$\ny=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{1}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{2}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}, \\quad X=\\begin{pmatrix}1  2 \\\\ 0  1\\end{pmatrix}.\n$$\n数据生成过程为 $y=x_{1}$（无噪声）。考虑两种正则化估计量：\n- 最小绝对收缩和选择算子 (LASSO) 估计量 $\\hat{\\beta}^{\\text{LASSO}}(\\lambda)$，定义为以下最小化问题的任意解\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n惩罚参数为 $\\lambda=\\frac{6}{5}$。\n- 岭回归估计量 $\\hat{\\beta}^{\\text{Ridge}}(\\alpha)$，定义为以下最小化问题的唯一解\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2},\n$$\n惩罚参数为 $\\alpha=\\frac{1}{2}$。\n\n精确计算这两个系数向量，并将其表示为单个行向量 $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$。给出精确值，不要四舍五入。你的最终答案必须是这个单行向量。",
            "solution": "该问题要求计算两种正则化线性回归的系数向量：岭回归估计量和 LASSO 估计量。该问题定义明确，科学上合理，并提供了所有必要的数据。我们将分别求解每个估计量。\n\n给定的数据是：\n响应向量 $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n预测变量矩阵 $X = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$。\n系数向量为 $\\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}$。\n\n首先，我们计算一些必要的矩阵乘积。\n$X$ 的转置是 $X^T = \\begin{pmatrix} 1  0 \\\\ 2  1 \\end{pmatrix}$。\n格拉姆矩阵是 $X^T X = \\begin{pmatrix} 1  0 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  2 \\\\ 2  5 \\end{pmatrix}$。\n乘积 $X^T y$ 是 $X^T y = \\begin{pmatrix} 1  0 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n\n**1. 岭回归估计量**\n\n岭回归的目标函数由下式给出：\n$$L_{\\text{Ridge}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2}$$\n惩罚参数为 $\\alpha = \\frac{1}{2}$。\n该目标函数是严格凸且可微的。通过将关于 $\\beta$ 的梯度设为零，可以找到最小化解 $\\hat{\\beta}^{\\text{Ridge}}$：\n$$\\nabla_{\\beta} L_{\\text{Ridge}} = -X^T(y - X\\beta) + \\alpha\\beta = 0$$\n整理各项可得岭回归的正规方程：\n$$(X^T X + \\alpha I) \\beta = X^T y$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。因此解为：\n$$\\hat{\\beta}^{\\text{Ridge}} = (X^T X + \\alpha I)^{-1} X^T y$$\n我们代入给定的值：\n$$X^T X + \\alpha I = \\begin{pmatrix} 1  2 \\\\ 2  5 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{2}  2 \\\\ 2  5 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  2 \\\\ 2  \\frac{11}{2} \\end{pmatrix}$$\n为了求该矩阵的逆，我们首先计算其行列式：\n$$\\det(X^T X + \\alpha I) = \\left(\\frac{3}{2}\\right)\\left(\\frac{11}{2}\\right) - (2)(2) = \\frac{33}{4} - 4 = \\frac{33 - 16}{4} = \\frac{17}{4}$$\n其逆矩阵为：\n$$(X^T X + \\alpha I)^{-1} = \\frac{1}{\\frac{17}{4}} \\begin{pmatrix} \\frac{11}{2}  -2 \\\\ -2  \\frac{3}{2} \\end{pmatrix} = \\frac{4}{17} \\begin{pmatrix} \\frac{11}{2}  -2 \\\\ -2  \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17}  -\\frac{8}{17} \\\\ -\\frac{8}{17}  \\frac{6}{17} \\end{pmatrix}$$\n现在我们可以计算岭回归估计量：\n$$\\hat{\\beta}^{\\text{Ridge}} = \\begin{pmatrix} \\frac{22}{17}  -\\frac{8}{17} \\\\ -\\frac{8}{17}  \\frac{6}{17} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} - \\frac{16}{17} \\\\ -\\frac{8}{17} + \\frac{12}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{4}{17} \\end{pmatrix}$$\n因此，$\\hat{\\beta}_{1}^{\\text{Ridge}} = \\frac{6}{17}$ 且 $\\hat{\\beta}_{2}^{\\text{Ridge}} = \\frac{4}{17}$。\n\n**2. LASSO 估计量**\n\nLASSO 的目标函数为：\n$$L_{\\text{LASSO}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\n惩罚参数为 $\\lambda = \\frac{6}{5}$。由于 $L_1$ 范数的不可微性，我们使用次梯度最优性条件。最小化解 $\\hat{\\beta}^{\\text{LASSO}}$ 必须满足 $0 \\in \\partial L_{\\text{LASSO}}(\\hat{\\beta})$。\n对于每个分量 $j \\in \\{1, 2\\}$，次梯度条件为 $X^T(y - X\\hat{\\beta})_j \\in \\lambda \\cdot \\partial |\\beta_j| |_{\\hat{\\beta}_j}$。这可以写成：\n$$\n\\begin{cases}\n(X^T(y - X\\hat{\\beta}))_j = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)  \\text{若 } \\hat{\\beta}_j \\neq 0 \\\\\n|(X^T(y - X\\hat{\\beta}))_j| \\le \\lambda  \\text{若 } \\hat{\\beta}_j = 0\n\\end{cases}\n$$\n让我们计算与残差相关的向量 $c(\\beta) = X^T(y - X\\beta)$：\n$$c(\\beta) = X^T y - X^T X \\beta = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1  2 \\\\ 2  5 \\end{pmatrix}\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\beta_1 - 2\\beta_2 \\\\ 2 - 2\\beta_1 - 5\\beta_2 \\end{pmatrix}$$\n我们分析系数的活动集可能出现的几种情况。\n\n情况 1：$\\hat{\\beta}_1 = 0$，$\\hat{\\beta}_2 \\ne 0$。\n条件是 $|c_1(\\hat{\\beta})| \\le \\lambda$ 和 $c_2(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_2)$。\n当 $\\hat\\beta_1 = 0$ 时，我们有 $c_2 = 2 - 5\\hat{\\beta}_2$。\n如果 $\\hat{\\beta}_2 > 0$：$2 - 5\\hat{\\beta}_2 = \\lambda = \\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5} \\implies \\hat{\\beta}_2 = \\frac{4}{25}$。这与 $\\hat{\\beta}_2 > 0$ 一致。\n我们必须检查 $\\hat{\\beta}_1=0$ 的条件：$|c_1| \\le \\lambda$。\n$|c_1| = |1 - 0 - 2\\hat{\\beta}_2| = |1 - 2(\\frac{4}{25})| = |1 - \\frac{8}{25}| = |\\frac{17}{25}| = \\frac{17}{25}$。\n条件是 $\\frac{17}{25} \\le \\frac{6}{5}$。由于 $\\frac{6}{5} = \\frac{30}{25}$，我们有 $\\frac{17}{25} \\le \\frac{30}{25}$，这是成立的。\n因此，$\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$ 是一个有效的解。\n\n如果 $\\hat{\\beta}_2  0$：$2 - 5\\hat{\\beta}_2 = -\\lambda = -\\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 + \\frac{6}{5} = \\frac{16}{5} \\implies \\hat{\\beta}_2 = \\frac{16}{25}$。这与假设 $\\hat{\\beta}_2  0$ 相矛盾。\n\n情况 2：$\\hat{\\beta}_1 \\ne 0$，$\\hat{\\beta}_2 = 0$。\n条件是 $c_1(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$ 和 $|c_2(\\hat{\\beta})| \\le \\lambda$。\n当 $\\hat\\beta_2 = 0$ 时，我们有 $c_1 = 1 - \\hat\\beta_1$。\n如果 $\\hat{\\beta}_1 > 0$：$1 - \\hat{\\beta}_1 = \\lambda = \\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 - \\frac{6}{5} = -\\frac{1}{5}$。矛盾。\n如果 $\\hat{\\beta}_1  0$：$1 - \\hat{\\beta}_1 = -\\lambda = -\\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 + \\frac{6}{5} = \\frac{11}{5}$。矛盾。\n这种情况无解。\n\n情况 3：$\\hat{\\beta}_1 \\ne 0$，$\\hat{\\beta}_2 \\ne 0$。\n这需要求解 $j=1,2$ 时的 $c_j(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$。\n1) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2 > 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$。两个正数之和不能为负。矛盾。\n2) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2  0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$ 且 $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - (-\\frac{6}{5}) = \\frac{16}{5}$。解此方程组得 $\\hat{\\beta}_2 = \\frac{18}{5}$，这与 $\\hat{\\beta}_2  0$ 相矛盾。\n3) $\\hat{\\beta}_1  0, \\hat{\\beta}_2 > 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$ 且 $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5}$。解此方程组得 $\\hat{\\beta}_2 = -\\frac{18}{5}$，这与 $\\hat{\\beta}_2 > 0$ 相矛盾。\n4) $\\hat{\\beta}_1  0, \\hat{\\beta}_2  0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$。两个负数之和不能为正。矛盾。\n不存在两个系数都非零的解。\n\n由于 LASSO 目标函数是严格凸的（因为 $X$ 是满秩的），所以必须存在唯一的最小化解。我们对所有可能的活动集的分析只得出了一个有效的解。\n因此，LASSO 估计量为 $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$。\n所以，$\\hat{\\beta}_{1}^{\\text{LASSO}} = 0$ 且 $\\hat{\\beta}_{2}^{\\text{LASSO}} = \\frac{4}{25}$。\n\n**最终答案组合**\n问题要求最终答案为单个行向量 $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$。\n代入计算出的值：\n$$\\begin{pmatrix} 0  \\frac{4}{25}  \\frac{6}{17}  \\frac{4}{17} \\end{pmatrix}$$\n这就是最终结果。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\frac{4}{25}  \\frac{6}{17}  \\frac{4}{17} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "理论推导和手动计算是必不可少的，但现代计量经济学依赖于高效的算法来解决实际问题。这个练习将指导你为弹性网络（Elastic Net）实现坐标下降算法，它不仅是实践中拟合这些模型的标准方法，也统一了岭回归和 LASSO。通过编程实践，你将掌握核心的计算技能，并理解这三种方法在算法层面的内在联系。",
            "id": "2426260",
            "problem": "给定弹性网络回归问题定义如下。对于一个数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个响应向量 $y \\in \\mathbb{R}^{n}$，考虑以下优化问题\n$$\n\\min_{b \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 \\;+\\; \\lambda_1 \\lVert b \\rVert_1 \\;+\\; \\frac{\\lambda_2}{2} \\lVert b \\rVert_2^2,\n$$\n其中 $n$ 和 $p$ 是正整数，$X$ 和 $y$ 是给定的，$\\lambda_1 \\ge 0$，$\\lambda_2 \\ge 0$ 是惩罚参数。该任务的动机是计算经济学和金融学中的模型选择和过拟合控制，其中预测变量可能高度相关，且变量数量可能与观测数量相当或更多。\n\n您的程序必须针对下面指定的测试实例，使用一种每次更新一个系数同时保持其他系数固定的方法，来计算所述优化问题的解，并且必须展示每次坐标更新相对于岭回归（Ridge regression）和最小绝对收缩和选择算子（LASSO）的混合性质。程序必须产生下面规定的输出，并且不得需要任何用户输入。\n\n所有计算都是纯数值计算，不涉及物理单位。此问题中不出现角度和百分比。\n\n使用以下包含四个案例的测试套件。在每个案例中，$n$ 和 $p$ 由 $X$ 和 $y$ 的形状指定。\n\n- 案例 1 (在通用设计上的仅岭回归一致性)：\n  - 数据：\n    $$\n    X_1 = \\begin{bmatrix}\n    1  0  1 \\\\\n    1  1  1 \\\\\n    1  2  1 \\\\\n    1  3  2 \\\\\n    1  4  3 \\\\\n    1  5  5\n    \\end{bmatrix}, \\quad\n    b^{\\mathrm{true}} = \\begin{bmatrix} 0.7 \\\\ 1.3 \\\\ -0.8 \\end{bmatrix}, \\quad\n    y_1 = X_1 b^{\\mathrm{true}}.\n    $$\n  - 惩罚：$\\lambda_1 = 0$，$\\lambda_2 = 0.8$。\n  - 此案例所需的标量结果：一个布尔值 $r_1$。如果您算法获得的解在每个系数上与唯一的岭回归闭式解 $b^{\\mathrm{ridge}}$（通过求解 $(X^\\top X / n + \\lambda_2 I) b = (X^\\top y)/n$ 定义）的绝对容差在 $10^{-9}$ 以内相匹配，则 $r_1$ 为真，否则为假。\n\n- 案例 2 (在正交规范设计上的仅 LASSO)：\n  - 数据：\n    $$\n    X_2 = \\begin{bmatrix}\n    2  0  0 \\\\\n    0  2  0 \\\\\n    0  0  2 \\\\\n    0  0  0\n    \\end{bmatrix}, \\quad\n    y_2 = \\begin{bmatrix} 3.2 \\\\ -0.5 \\\\ 1.1 \\\\ 0 \\end{bmatrix}.\n    $$\n    注意，当 $n = 4$ 时，$(1/n) X_2^\\top X_2 = I_3$。\n  - 惩罚：$\\lambda_1 = 0.6$，$\\lambda_2 = 0$。\n  - 此案例所需的标量结果：一个布尔值 $r_2$。如果您算法获得的解在每个系数上与正交规范设计的已知 LASSO 解（即普通最小二乘估计的逐系数软阈值）的绝对容差在 $10^{-12}$ 以内相匹配，则 $r_2$ 为真，否则为假。\n\n- 案例 3 (在案例 1 的通用设计上的通用弹性网络系数)：\n  - 数据：$X_3 = X_1$，$y_3 = y_1$。\n  - 惩罚：$\\lambda_1 = 0.5$，$\\lambda_2 = 0.3$。\n  - 此案例所需的结果：此案例的弹性网络解 $b^{\\mathrm{EN}}$ 的三个系数，每个系数均数值上四舍五入到六位小数。将它们表示为 $b^{(3)}_1$、$b^{(3)}_2$、$b^{(3)}_3$。\n\n- 案例 4 (通过大的 $\\ell_1$ 惩罚实现的零解边界情况)：\n  - 数据：$X_4 = X_1$，$y_4 = y_1$。\n  - 惩罚：$\\lambda_1 = 8.0$，$\\lambda_2 = 0.1$。\n  - 此案例所需的标量结果：一个布尔值 $r_4$。如果计算出的弹性网络解在每个系数上与零向量的绝对容差在 $10^{-10}$ 以内相匹配，则 $r_4$ 为真，否则为假。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含\n$$\n\\big[ r_1, \\; r_2, \\; b^{(3)}_1, \\; b^{(3)}_2, \\; b^{(3)}_3, \\; r_4 \\big].\n$$",
            "solution": "问题陈述已经过验证，并被证实是有效的。它在科学上基于已建立的正则化线性回归理论，是适定的、客观的，并为一项可解的数值任务提供了一套完整且一致的定义和数据。\n\n该问题要求解弹性网络优化问题，该问题由以下目标函数定义：\n$$\nL(b) = \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 \\;+\\; \\lambda_1 \\lVert b \\rVert_1 \\;+\\; \\frac{\\lambda_2}{2} \\lVert b \\rVert_2^2\n$$\n此处，$y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是数据矩阵，$b \\in \\mathbb{R}^{p}$ 是待优化的系数向量，$\\lambda_1 \\ge 0$ 和 $\\lambda_2 \\ge 0$ 是非负正则化参数。$\\lVert \\cdot \\rVert_1$ 项是 $\\ell_1$-范数（绝对值之和），$\\lVert \\cdot \\rVert_2$ 项是 $\\ell_2$-范数（欧几里得范数）。\n\n该问题指定使用逐坐标更新方法，通常称为坐标下降法。该方法每次迭代优化目标函数关于单个系数 $b_j$ 的值，同时保持所有其他系数 $b_{k \\neq j}$ 固定。\n\n为了推导单个系数 $b_j$ 的更新规则，我们将目标函数 $L(b)$ 仅视为 $b_j$ 的函数，并将 $b$ 的所有其他分量视为常数。\n令 $r = y - Xb$ 为当前残差向量。我们可以将平方和项表示为：\n$$\n\\lVert y - X b \\rVert_2^2 = \\left\\lVert y - \\sum_{k=1}^p X_k b_k \\right\\rVert_2^2 = \\left\\lVert \\left(y - \\sum_{k \\neq j} X_k b_k\\right) - X_j b_j \\right\\rVert_2^2\n$$\n令偏残差为 $r^{(j)} = y - \\sum_{k \\neq j} X_k b_k$。这可以根据完整残差 $r$ 和 $b_j$ 的先前值（表示为 $b_j^{\\text{old}}$）计算得出，即 $r^{(j)} = r + X_j b_j^{\\text{old}}$。\n目标函数作为 $b_j$ 的函数是：\n$$\nL(b_j) = \\frac{1}{2n} \\lVert r^{(j)} - X_j b_j \\rVert_2^2 + \\lambda_1 |b_j| + \\frac{\\lambda_2}{2} b_j^2 + \\text{constants}\n$$\n展开平方范数，我们得到：\n$$\nL(b_j) = \\frac{1}{2n} \\left( (r^{(j)})^\\top r^{(j)} - 2b_j (X_j)^\\top r^{(j)} + b_j^2 (X_j)^\\top X_j \\right) + \\lambda_1 |b_j| + \\frac{\\lambda_2}{2} b_j^2 + \\text{constants}\n$$\n为求最小值，我们取 $L(b_j)$ 关于 $b_j$ 的次梯度，并将其设为 0。次梯度 $\\partial L(b_j) / \\partial b_j$ 是：\n$$\n\\frac{\\partial L(b_j)}{\\partial b_j} = \\frac{1}{n} \\left( - (X_j)^\\top r^{(j)} + b_j (X_j)^\\top X_j \\right) + \\lambda_1 \\text{sgn}(b_j) + \\lambda_2 b_j = 0\n$$\n其中 $\\text{sgn}(b_j)$ 是 $|b_j|$ 次梯度的一部分，当 $b_j=0$ 时，其取值范围在 $[-1, 1]$ 内。\n令 $\\rho_j = \\frac{1}{n} (X_j)^\\top r^{(j)}$ 和 $a_j = \\frac{1}{n} (X_j)^\\top X_j$。方程变为：\n$$\n-\\rho_j + b_j a_j + \\lambda_2 b_j + \\lambda_1 \\text{sgn}(b_j) = 0 \\implies b_j (a_j + \\lambda_2) = \\rho_j - \\lambda_1 \\text{sgn}(b_j)\n$$\n这是一个一维的 LASSO 类型问题。其解由软阈值算子 $S(z, \\gamma) = \\text{sgn}(z) \\max(|z|-\\gamma, 0)$ 给出。$b_j$ 的更新规则是：\n$$\nb_j \\leftarrow \\frac{S(\\rho_j, \\lambda_1)}{a_j + \\lambda_2}\n$$\n为提高计算效率，$\\rho_j = \\frac{1}{n} (X_j)^\\top (y - \\sum_{k \\neq j} X_k b_k)$ 可以计算为 $\\rho_j = \\frac{1}{n}(X^\\top y)_j - \\sum_{k \\neq j} \\frac{1}{n}(X^\\top X)_{jk} b_k$。通过预先计算矩阵 $\\frac{1}{n}X^\\top X$ 和 $\\frac{1}{n}X^\\top y$，在迭代循环中每次更新 $b_j$ 的计算成本会变得很低。算法会重复遍历所有系数 $j=1, \\dots, p$，直到系数向量 $b$ 收敛。\n\n该算法将按规定应用于四个测试案例。\n\n案例 1：仅岭回归的一致性。\n当惩罚参数 $\\lambda_1 = 0$ 且 $\\lambda_2 = 0.8$ 时，问题简化为岭回归。由于 $S(\\rho_j, 0) = \\rho_j$，更新规则简化为 $b_j \\leftarrow \\rho_j / (a_j + \\lambda_2)$。坐标下降算法预计将收敛到岭回归目标函数的唯一全局最小值。将此解与岭回归的解析闭式解 $b^{\\mathrm{ridge}} = ( \\frac{1}{n} X^\\top X + \\lambda_2 I )^{-1} (\\frac{1}{n} X^\\top y)$ 进行比较。如果计算出的系数与解析解在 $10^{-9}$ 的容差范围内相匹配，布尔结果 $r_1$ 将为真。\n\n案例 2：在正交规范设计上的仅 LASSO。\n当惩罚参数 $\\lambda_1 = 0.6$ 且 $\\lambda_2 = 0$ 时，问题是 LASSO 回归。设计矩阵 $X_2$ 具有属性 $\\frac{1}{n} X_2^\\top X_2 = I_3$，其中 $n=4$。对于这样的设计，目标函数在各个系数上是解耦的，其精确解已知为软阈值处理后的普通最小二乘（OLS）估计。OLS 估计为 $b^{\\mathrm{OLS}} = (\\frac{1}{n}X_2^\\top X_2)^{-1}(\\frac{1}{n}X_2^\\top y_2) = \\frac{1}{n}X_2^\\top y_2$。因此，LASSO 解为 $b^{\\mathrm{LASSO}}_j = S((b^{\\mathrm{OLS}})_j, \\lambda_1)$。所实现的坐标下降算法预计将收敛到此精确解。如果匹配结果在 $10^{-12}$ 的容差范围内，布尔结果 $r_2$ 将为真。\n\n案例 3：通用弹性网络。\n当 $\\lambda_1 = 0.5$ 且 $\\lambda_2 = 0.3$ 时，这是一个通用的弹性网络问题。应用所推导的完整坐标下降算法。向量 $b^{\\mathrm{EN}}$ 的最终系数，表示为 $b^{(3)}_1$、$b^{(3)}_2$、$b^{(3)}_3$，是所需的输出，四舍五入到六位小数。\n\n案例 4：零解边界情况。\n当 $\\ell_1$ 惩罚较大（$\\lambda_1 = 8.0$）且 $\\lambda_2 = 0.1$ 时，解可能为零向量，即 $b=0$。$b=0$ 为最优解的条件是目标函数在 $b=0$ 处的次梯度必须包含零向量。此条件为对所有 $j=1, \\dots, p$，都有 $| \\frac{1}{n}(X^\\top y)_j | \\le \\lambda_1$。对于给定的数据和 $\\lambda_1 = 8.0$，我们计算 $\\frac{1}{n} X_1^\\top y_1$ 并发现该条件对所有三个分量均成立。因此，坐标下降算法预计将收敛到 $b=0$。如果计算出的每个系数的范数小于容差 $10^{-10}$，布尔结果 $r_4$ 将为真。\n\n实现将首先为所有案例定义数据，然后为每个参数集执行坐标下降求解器，并执行指定的验证检查或提取所需的系数值。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(rho, lam):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(rho) * np.maximum(np.abs(rho) - lam, 0.)\n\ndef solve_elastic_net(X, y, lambda1, lambda2, max_iter=100000, tol=1e-15):\n    \"\"\"\n    Solves the Elastic Net problem using coordinate descent.\n\n    min_b (1/2n) ||y - Xb||^2_2 + lambda1 ||b||_1 + (lambda2/2) ||b||^2_2\n    \"\"\"\n    n, p = X.shape\n    b = np.zeros(p)\n    \n    # Pre-compute matrices for efficiency\n    XTX = (X.T @ X) / n\n    XTy = (X.T @ y) / n\n    \n    # Denominators for the update rule\n    denominators = np.diag(XTX) + lambda2\n\n    for _ in range(max_iter):\n        b_old_cycle = b.copy()\n        \n        for j in range(p):\n            # Calculate rho_j using pre-computed matrices\n            # rho_j = XTy[j] - (XTX_row_j . b - XTX_jj * b_j)\n            rho_j = XTy[j] - (np.dot(XTX[j, :], b) - XTX[j, j] * b[j])\n            \n            if denominators[j] == 0:\n                 # This case is unlikely with non-zero columns in X or lambda2 > 0\n                b[j] = 0.\n            else:\n                b[j] = soft_threshold(rho_j, lambda1) / denominators[j]\n        \n        # Check for convergence\n        max_change = np.max(np.abs(b - b_old_cycle))\n        if max_change  tol:\n            break\n            \n    return b\n\ndef solve():\n    # --- Define common data ---\n    X1 = np.array([\n        [1.0, 0.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 2.0, 1.0],\n        [1.0, 3.0, 2.0],\n        [1.0, 4.0, 3.0],\n        [1.0, 5.0, 5.0]\n    ])\n    b_true = np.array([0.7, 1.3, -0.8])\n    y1 = X1 @ b_true  # As per problem, y1 is defined by this product\n\n    # --- Case 1: Ridge-only consistency ---\n    lambda1_c1, lambda2_c1 = 0.0, 0.8\n    b1_my = solve_elastic_net(X1, y1, lambda1_c1, lambda2_c1)\n    \n    n1, p1 = X1.shape\n    A = (X1.T @ X1) / n1 + lambda2_c1 * np.identity(p1)\n    v = (X1.T @ y1) / n1\n    b1_ridge = np.linalg.solve(A, v)\n    \n    r1 = np.allclose(b1_my, b1_ridge, atol=1e-9)\n\n    # --- Case 2: LASSO-only on an orthonormal design ---\n    X2 = np.array([\n        [2.0, 0.0, 0.0],\n        [0.0, 2.0, 0.0],\n        [0.0, 0.0, 2.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y2 = np.array([3.2, -0.5, 1.1, 0.0])\n    lambda1_c2, lambda2_c2 = 0.6, 0.0\n    \n    b2_my = solve_elastic_net(X2, y2, lambda1_c2, lambda2_c2)\n    \n    n2 = X2.shape[0]\n    # For orthonormal design (1/n)X'X = I, the OLS estimate is (1/n)X'y\n    b_ols = (X2.T @ y2) / n2\n    # The LASSO solution is the soft-thresholded OLS solution\n    b2_lasso = soft_threshold(b_ols, lambda1_c2)\n    \n    r2 = np.allclose(b2_my, b2_lasso, atol=1e-12)\n\n    # --- Case 3: General Elastic Net coefficients ---\n    X3, y3 = X1, y1\n    lambda1_c3, lambda2_c3 = 0.5, 0.3\n    \n    b3_en = solve_elastic_net(X3, y3, lambda1_c3, lambda2_c3)\n    b3_1 = round(b3_en[0], 6)\n    b3_2 = round(b3_en[1], 6)\n    b3_3 = round(b3_en[2], 6)\n\n    # --- Case 4: Zero-solution edge case ---\n    X4, y4 = X1, y1\n    lambda1_c4, lambda2_c4 = 8.0, 0.1\n    \n    b4_en = solve_elastic_net(X4, y4, lambda1_c4, lambda2_c4)\n    r4 = np.allclose(b4_en, np.zeros(X4.shape[1]), atol=1e-10)\n    \n    # --- Final Output Formatting ---\n    results = [\n        str(r1).lower(),\n        str(r2).lower(),\n        f\"{b3_1:.6f}\",\n        f\"{b3_2:.6f}\",\n        f\"{b3_3:.6f}\",\n        str(r4).lower()\n    ]\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}