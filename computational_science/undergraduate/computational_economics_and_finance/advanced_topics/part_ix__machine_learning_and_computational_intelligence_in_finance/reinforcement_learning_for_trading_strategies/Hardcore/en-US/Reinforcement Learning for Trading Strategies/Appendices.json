{
    "hands_on_practices": [
        {
            "introduction": "Before an agent can learn a complex sequence of actions, it must first understand the fundamental trade-offs at each step. This practice explores the critical balance between capturing expected profits and incurring execution costs from market impact, or slippage. By solving for the optimal trade size in a single-step scenario , you will build a core intuition for the penalized reward functions that guide the behavior of sophisticated RL agents.",
            "id": "2426687",
            "problem": "You are given a one-step trading decision problem in which a trading agent chooses a signed trade size $v$ (with buy represented by positive $v$ and sell represented by negative $v$) to maximize a penalized expected reward. The immediate expected price change per unit is $\\mu$, and the market volatility parameter is $\\sigma$. The instantaneous slippage penalty is modeled as a per-unit effect that increases with both the volatility and the square root of the trade size magnitude, following empirical square-root impact: the per-unit slippage is $s_{\\text{per}}(v,\\sigma) = k\\,\\sigma\\,|v|^{1/2}$, where $k$ is a positive coefficient. The total slippage cost in monetary units is $C(v,\\sigma) = s_{\\text{per}}(v,\\sigma)\\,|v| = k\\,\\sigma\\,|v|^{3/2}$. The penalized expected reward for taking action $v$ is therefore\n$$\nR(v;\\mu,\\sigma,k) = \\mu\\,v \\;-\\; k\\,\\sigma\\,|v|^{3/2}.\n$$\nThe choice variable $v$ is constrained by a hard trading limit $|v| \\le V_{\\max}$.\n\nTask: For each parameter set in the test suite below, compute the unique optimal trade size $v^\\star$ that maximizes $R(v;\\mu,\\sigma,k)$ over all real $v$ satisfying $|v|\\le V_{\\max}$. When multiple maximizers exist due to degeneracy, select $v^\\star = 0$ as the tie-breaking convention. Your program must implement this from the definition of $R(v;\\mu,\\sigma,k)$ and output the optimal $v^\\star$ for each test case.\n\nTest suite (each item is a tuple $(\\mu,\\sigma,k,V_{\\max})$):\n- Case $1$: $(0.01, 0.02, 0.5, 10)$\n- Case $2$: $(-0.015, 0.03, 0.7, 20)$\n- Case $3$: $(0.02, 0, 1, 5)$\n- Case $4$: $(0, 0.05, 0.5, 100)$\n- Case $5$: $(0.05, 0.02, 0.2, 1)$\n- Case $6$: $(0.01, 0.5, 2, 100)$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above. Round each result to $6$ decimal places. For example, a valid output with three hypothetical results would look like $[0.123456,-0.500000,1.000000]$.",
            "solution": "The problem statement is submitted for validation.\n\n### Step 1: Extract Givens\n- **Objective Function**: The penalized expected reward is $R(v;\\mu,\\sigma,k) = \\mu\\,v \\;-\\; k\\,\\sigma\\,|v|^{3/2}$.\n- **Choice Variable**: $v$, a real number representing the signed trade size.\n- **Parameters**:\n    - $\\mu$: expected price change per unit.\n    - $\\sigma$: market volatility parameter.\n    - $k$: a positive coefficient, $k0$.\n- **Constraint**: The trade size is limited by $|v| \\le V_{\\max}$.\n- **Tie-breaking Rule**: If multiple maximizers exist due to degeneracy, select $v^\\star = 0$.\n- **Task**: Compute the optimal trade size $v^\\star$ that maximizes $R(v;\\mu,\\sigma,k)$ subject to the constraint.\n- **Test Suite**:\n    - Case $1$: $(\\mu,\\sigma,k,V_{\\max}) = (0.01, 0.02, 0.5, 10)$\n    - Case $2$: $(\\mu,\\sigma,k,V_{\\max}) = (-0.015, 0.03, 0.7, 20)$\n    - Case $3$: $(\\mu,\\sigma,k,V_{\\max}) = (0.02, 0, 1, 5)$\n    - Case $4$: $(\\mu,\\sigma,k,V_{\\max}) = (0, 0.05, 0.5, 100)$\n    - Case $5$: $(\\mu,\\sigma,k,V_{\\max}) = (0.05, 0.02, 0.2, 1)$\n    - Case $6$: $(\\mu,\\sigma,k,V_{\\max}) = (0.01, 0.5, 2, 100)$\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem describes a classical trade-off in financial execution: maximizing expected profit from a predictable price move ($\\mu v$) while minimizing transaction costs. The cost function, $C(v,\\sigma) = k\\,\\sigma\\,|v|^{3/2}$, is a representation of the \"square-root market impact\" law, an empirically observed phenomenon in financial markets. The model is a standard simplified representation used in computational finance. It is scientifically grounded.\n2.  **Well-Posed**: The task is to maximize a continuous function $R(v)$ over a compact set $[-V_{\\max}, V_{\\max}]$. By the Extreme Value Theorem, a maximum is guaranteed to exist. The uniqueness of the solution must be verified. The problem is well-posed.\n3.  **Objective**: The problem is stated in precise mathematical terms, using unambiguous definitions and objective criteria.\n4.  **Completeness**: All necessary parameters ($\\mu, \\sigma, k, V_{\\max}$) are provided for each case. The objective function and constraints are fully specified. The setup is complete.\n5.  **Flaws**: The problem exhibits none of the disqualifying flaws. The parameters are physically plausible in a financial context. One case includes $\\sigma=0$, which is a valid and important limit case representing a market with no volatility, and must be handled correctly.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous solution will be provided.\n\nThe objective is to find $v^\\star$ that maximizes the function $R(v) = \\mu v - k \\sigma |v|^{3/2}$ on the interval $v \\in [-V_{\\max}, V_{\\max}]$. The function $R(v)$ is continuous, but its derivative is not defined at $v=0$. Therefore, we analyze the problem by considering the cases $v0$, $v0$, and $v=0$ separately. The value at $v=0$ is $R(0)=0$.\n\nCase 1: $v  0$.\nThe function is $R(v) = \\mu v - k \\sigma v^{3/2}$. We seek its maximum on $(0, V_{\\max}]$.\nThe first and second derivatives with respect to $v$ are:\n$$\nR'(v) = \\mu - \\frac{3}{2} k \\sigma v^{1/2}\n$$\n$$\nR''(v) = -\\frac{3}{4} k \\sigma v^{-1/2}\n$$\nGiven that $k0$ and we provisionally assume $\\sigma0$, $R''(v)  0$ for all $v0$. This indicates that $R(v)$ is strictly concave for $v0$, so it has at most one local maximum in this domain.\n\nTo find the unconstrained maximizer, we set $R'(v) = 0$:\n$$\n\\mu - \\frac{3}{2} k \\sigma v^{1/2} = 0 \\implies v^{1/2} = \\frac{2\\mu}{3k\\sigma}\n$$\nA real, positive solution for $v^{1/2}$ exists only if $\\mu  0$. In this case, the unconstrained optimal trade is $v_{\\text{unc}} = \\left(\\frac{2\\mu}{3k\\sigma}\\right)^2$.\nIf $\\mu \\le 0$, then $R'(v) \\le 0$ for all $v0$, meaning $R(v)$ is non-increasing on $(0, \\infty)$. The maximum on $[0, V_{\\max}]$ is thus at $v=0$.\nIf $\\mu  0$, the concavity of $R(v)$ implies the maximum on $[0, V_{\\max}]$ occurs at $v^\\star_+ = \\min(v_{\\text{unc}}, V_{\\max})$. The resulting reward $R(v^\\star_+)$ will be non-negative.\n\nCase 2: $v  0$.\nLet $u = -v  0$. The function in terms of $u$ is $R(u) = \\mu(-u) - k\\sigma u^{3/2} = -\\mu u - k\\sigma u^{3/2}$. We maximize this for $u \\in (0, V_{\\max}]$.\nThe first derivative with respect to $u$ is:\n$$\nR'(u) = -\\mu - \\frac{3}{2} k \\sigma u^{1/2}\n$$\nSetting $R'(u) = 0$ gives $u^{1/2} = \\frac{-2\\mu}{3k\\sigma}$. A real, positive solution for $u^{1/2}$ exists only if $\\mu  0$. The unconstrained optimal size is $u_{\\text{unc}} = \\left(\\frac{-2\\mu}{3k\\sigma}\\right)^2 = \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2$.\nIf $\\mu \\ge 0$, then $R'(u)  0$ for all $u0$, meaning the function is decreasing. The maximum for negative $v$ (positive $u$) values is at $v \\to 0^-$, yielding $R(0)=0$.\nIf $\\mu  0$, the optimal positive size $u$ is $u^\\star = \\min(u_{\\text{unc}}, V_{\\max})$, which corresponds to a negative trade $v^\\star_- = -u^\\star$. The reward will be non-negative.\n\nOverall Solution Synthesis:\n- If $\\mu  0$, any trade $v  0$ yields $R(v)  0$, while the optimal trade $v^\\star_+  0$ yields $R(v^\\star_+) \\ge 0$. Thus, the global optimum is $v^\\star = v^\\star_+$.\n- If $\\mu  0$, any trade $v  0$ yields $R(v)  0$, while the optimal trade $v^\\star_-  0$ yields $R(v^\\star_-) \\ge 0$. Thus, the global optimum is $v^\\star = v^\\star_-$.\n- If $\\mu = 0$ (and $\\sigma  0$), the function is $R(v) = -k\\sigma|v|^{3/2}$, which is maximized at $v^\\star = 0$.\n\nCombining these results for $\\sigma  0$:\nThe optimal trade size magnitude is $v_{\\text{mag}} = \\min\\left( \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2, V_{\\max} \\right)$. The sign of the trade must match the sign of $\\mu$. If $\\mu=0$, the magnitude is $0$.\nSo, $v^\\star = \\text{sign}(\\mu) \\cdot \\min\\left( \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2, V_{\\max} \\right)$.\n\nSpecial Case: $\\sigma = 0$.\nThe objective function simplifies to $R(v) = \\mu v$. We must maximize this linear function over $v \\in [-V_{\\max}, V_{\\max}]$.\n- If $\\mu  0$, $R(v)$ is maximized at the upper bound, $v^\\star = V_{\\max}$.\n- If $\\mu  0$, $R(v)$ is maximized at the lower bound, $v^\\star = -V_{\\max}$.\n- If $\\mu = 0$, $R(v)=0$ for all $v$. This is a degeneracy. All $v \\in [-V_{\\max}, V_{\\max}]$ are maximizers. According to the stated tie-breaking rule, we must select $v^\\star = 0$.\nThis can be compactly written as $v^\\star = V_{\\max} \\cdot \\text{sign}(\\mu)$.\n\nFinal Algorithm:\n1. For a given set of parameters $(\\mu, \\sigma, k, V_{\\max})$:\n2. If $\\sigma = 0$, the optimal trade is $v^\\star = V_{\\max} \\cdot \\text{sign}(\\mu)$.\n3. If $\\sigma  0$:\n   - If $\\mu = 0$, the optimal trade is $v^\\star = 0$.\n   - If $\\mu \\ne 0$, calculate the unconstrained optimal magnitude $v_{\\text{unc\\_mag}} = \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2$. The constrained magnitude is $v_{\\text{mag}} = \\min(v_{\\text{unc\\_mag}}, V_{\\max})$. The optimal trade is $v^\\star = \\text{sign}(\\mu) \\cdot v_{\\text{mag}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal trade size v* for a series of one-step trading problems.\n    The problem is to maximize R(v) = mu*v - k*sigma*|v|^(3/2) subject to |v| = V_max.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (mu, sigma, k, V_max).\n    test_cases = [\n        (0.01, 0.02, 0.5, 10),\n        (-0.015, 0.03, 0.7, 20),\n        (0.02, 0, 1, 5),\n        (0, 0.05, 0.5, 100),\n        (0.05, 0.02, 0.2, 1),\n        (0.01, 0.5, 2, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, sigma, k, V_max = case\n        \n        v_star = 0.0\n\n        if sigma == 0:\n            # Special case: no volatility implies no slippage penalty.\n            # Maximize R(v) = mu*v.\n            # Optimal trade is at the boundary, with sign matching mu.\n            # np.sign(0) is 0, correctly handling the mu=0 degenerate case.\n            v_star = V_max * np.sign(mu)\n        else:\n            # Standard case with volatility and slippage.\n            if mu == 0:\n                # If mu is 0, R(v) = -k*sigma*|v|^(3/2), which is maximized at v=0.\n                v_star = 0.0\n            else:\n                # Unconstrained optimal trade magnitude from setting R'(v)=0.\n                v_unc_mag = (2 * abs(mu) / (3 * k * sigma))**2\n                \n                # The optimal magnitude is constrained by V_max.\n                v_mag = min(v_unc_mag, V_max)\n                \n                # The sign of the trade should match the sign of the expected return.\n                v_star = np.sign(mu) * v_mag\n        \n        results.append(v_star)\n    \n    # Format the results to 6 decimal places and join into a single string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key challenge in reinforcement learning is understanding *what* the agent has learned, making interpretability a crucial goal. This exercise tackles that challenge by training an agent with a linear model, allowing you to directly inspect its decision-making logic. By learning a linear $Q$-function from a batch of historical data , you will connect RL to the familiar framework of regression and see how the learned coefficients quantify the importance of different market signals.",
            "id": "2426627",
            "problem": "You are tasked with constructing an explainable Reinforcement Learning (RL) trading agent by using a linear model for the action-value function in a stylized trading environment. The trading environment is modeled as a discrete-time Markov Decision Process (MDP) with time indices $t \\in \\{0,1,\\dots,T-1\\}$, an action set $\\mathcal{A}=\\{-1,0,1\\}$ representing short, flat, and long positions, and a stochastic return process for a single risky asset.\n\nEnvironment dynamics and features:\n- The asset’s simple return $r_t$ follows a first-order autoregressive process:\n$$r_t = \\mu + \\varphi \\, r_{t-1} + \\sigma \\, \\varepsilon_t,$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed, $r_{-1}=0$, and the parameters are $\\mu \\in \\mathbb{R}$, $\\varphi \\in \\mathbb{R}$, and $\\sigma  0$.\n- Define the Exponential Moving Average (EMA) of returns $m_t$ recursively as\n$$m_t = (1-\\alpha) m_{t-1} + \\alpha r_t,$$\nwith $m_{-1}=0$ and smoothing parameter $\\alpha \\in (0,1]$.\n- The state feature vector at time $t$ is\n$$\\phi(s_t) = \\begin{bmatrix} 1 \\\\ r_{t-1} \\\\ m_{t-1} \\end{bmatrix},$$\nwith the conventions $r_{-1} = 0$ and $m_{-1} = 0$.\n\nBehavior policy and reward:\n- The behavior policy that generates data is independent of the state and draws $a_t \\in \\{-1,0,1\\}$ uniformly at random.\n- Let $a_{-1}=0$. The one-step reward at time $t$ is\n$$u_t = a_t \\, r_t \\;-\\; c \\, |a_t - a_{t-1}|,$$\nwhere $c \\ge 0$ is the per-unit transaction cost rate (expressed as a decimal, not as a percentage).\n\nDiscounted returns and linear action-value model:\n- For a given discount factor $\\gamma \\in [0,1)$ and horizon $H \\in \\mathbb{N}$, define the truncated discounted return starting at $t$ as\n$$G_t \\;=\\; \\sum_{k=0}^{K_t-1} \\gamma^k \\, u_{t+k}, \\quad \\text{where } K_t = \\min\\{H,\\, T-t\\}.$$\n- Consider the linear action-value function (linear $Q$-function) parameterized by $\\theta \\in \\mathbb{R}^3$:\n$$Q_\\theta(s_t,a_t) \\;=\\; \\theta^\\top \\big(a_t \\, \\phi(s_t)\\big).$$\n\nEstimation objective:\n- Given a trajectory $\\{(s_t,a_t,u_t)\\}_{t=0}^{T-1}$ generated by the behavior policy and environment above, define the regularized empirical risk\n$$J(\\theta) \\;=\\; \\sum_{t=0}^{T-1} \\Big(Q_\\theta(s_t,a_t) - G_t\\Big)^2 \\;+\\; \\lambda \\, \\|\\theta\\|_2^2,$$\nwhere $\\lambda \\ge 0$ is a regularization parameter. Assume that all parameters satisfy conditions ensuring that the objective is strictly convex so that a unique minimizer exists.\n\nTask:\n- For each test case (parameter set listed below), simulate the environment and behavior policy using the specified random seed to generate the Gaussian shocks $\\varepsilon_t$ and the random actions $a_t$.\n- Compute the features $\\phi(s_t)$, the rewards $u_t$, and the discounted returns $G_t$.\n- Compute the unique minimizer $\\hat{\\theta}$ of $J(\\theta)$ for that test case.\n- Your program must output, for each test case, the learned coefficient vector $\\hat{\\theta}$ as a list $[\\hat{\\theta}_0,\\hat{\\theta}_1,\\hat{\\theta}_2]$ of real numbers. Aggregate the results for all test cases into a single list in the same order.\n\nTest suite (four cases to ensure coverage):\n- Case $1$ (baseline): \n  - $T=200$, $\\mu=0$, $\\varphi=0.1$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0.1$, $\\gamma=0.95$, $c=0.0005$, $H=50$, seed $=42$.\n- Case $2$ (myopic, no regularization):\n  - $T=200$, $\\mu=0$, $\\varphi=0$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0$, $\\gamma=0$, $c=0.0005$, $H=1$, seed $=7$.\n- Case $3$ (high regularization):\n  - $T=200$, $\\mu=0$, $\\varphi=0.3$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=10$, $\\gamma=0.9$, $c=0.001$, $H=50$, seed $=123$.\n- Case $4$ (high transaction costs):\n  - $T=200$, $\\mu=0$, $\\varphi=0.1$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0.1$, $\\gamma=0.95$, $c=0.01$, $H=50$, seed $=99$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the three-dimensional list for a test case, in the same order as above. For example, an admissible format is\n$$\\big[\\,[\\hat{\\theta}_{0}^{(1)},\\hat{\\theta}_{1}^{(1)},\\hat{\\theta}_{2}^{(1)}],\\;[\\hat{\\theta}_{0}^{(2)},\\hat{\\theta}_{1}^{(2)},\\hat{\\theta}_{2}^{(2)}],\\;[\\hat{\\theta}_{0}^{(3)},\\hat{\\theta}_{1}^{(3)},\\hat{\\theta}_{2}^{(3)}],\\;[\\hat{\\theta}_{0}^{(4)},\\hat{\\theta}_{1}^{(4)},\\hat{\\theta}_{2}^{(4)}]\\big].$$\n- All numerical values are to be printed as real numbers. No units or angles are involved in this problem. Percentages, when present as input parameters, are to be provided and treated as decimal fractions as specified above.",
            "solution": "The problem requires us to find the parameter vector $\\hat{\\theta}$ that minimizes a regularized empirical risk function $J(\\theta)$. This is a standard problem in statistical learning, specifically, it takes the form of a Ridge Regression.\n\nThe objective function to be minimized is given by:\n$$J(\\theta) = \\sum_{t=0}^{T-1} \\Big(Q_\\theta(s_t,a_t) - G_t\\Big)^2 + \\lambda \\, \\|\\theta\\|_2^2$$\nwhere the linear action-value function is $Q_\\theta(s_t,a_t) = \\theta^\\top \\big(a_t \\, \\phi(s_t)\\big)$.\n\nThis objective function can be expressed more conveniently in matrix-vector notation. Let $d=3$ be the dimension of the parameter vector $\\theta$. We define a design matrix $X$ of size $T \\times d$ and a target vector $y$ of size $T \\times 1$. The $t$-th row of $X$, denoted by $X_t$, corresponds to the feature vector for the prediction at time $t$:\n$$X_t = \\big(a_t \\, \\phi(s_t)\\big)^\\top = a_t \\begin{bmatrix} 1  r_{t-1}  m_{t-1} \\end{bmatrix}$$\nThe target vector $y$ consists of the sampled truncated discounted returns:\n$$y = \\begin{bmatrix} G_0 \\\\ G_1 \\\\ \\vdots \\\\ G_{T-1} \\end{bmatrix}$$\nWith these definitions, the objective function $J(\\theta)$ can be rewritten in the standard form for Ridge Regression:\n$$J(\\theta) = \\|X\\theta - y\\|_2^2 + \\lambda \\|\\theta\\|_2^2 = (X\\theta - y)^\\top(X\\theta - y) + \\lambda \\theta^\\top\\theta$$\nThis function $J(\\theta)$ is convex. The problem statement guarantees that conditions are met for a unique minimizer to exist. To find this minimizer, $\\hat{\\theta}$, we compute the gradient of $J(\\theta)$ with respect to $\\theta$ and set it to zero:\n$$\\nabla_\\theta J(\\theta) = \\frac{\\partial}{\\partial \\theta} \\left( \\theta^\\top X^\\top X \\theta - 2y^\\top X \\theta + y^\\top y + \\lambda \\theta^\\top \\theta \\right)$$\n$$\\nabla_\\theta J(\\theta) = 2 X^\\top X \\theta - 2 X^\\top y + 2 \\lambda I \\theta = 0$$\nwhere $I$ is the $d \\times d$ identity matrix.\nRearranging the terms, we arrive at the normal equations:\n$$(X^\\top X + \\lambda I) \\theta = X^\\top y$$\nThe unique minimizer $\\hat{\\theta}$ is the solution to this system of linear equations.\n\nThe computational procedure to find $\\hat{\\theta}$ for each test case is as follows:\n1.  **Data Generation**: We first simulate the time series data according to the specified dynamics for $t \\in \\{0, 1, \\dots, T-1\\}$.\n    - A random number generator is initialized with the given seed for reproducibility.\n    - Two random sequences are generated: Gaussian shocks $\\{\\varepsilon_t\\}_{t=0}^{T-1}$ from $\\mathcal{N}(0,1)$ for the return process, and actions $\\{a_t\\}_{t=0}^{T-1}$ from the uniform distribution over $\\{-1, 0, 1\\}$.\n    - The asset returns $r_t$ are generated iteratively using the AR($1$) process: $r_t = \\mu + \\varphi r_{t-1} + \\sigma \\varepsilon_t$, with the initial condition $r_{-1}=0$.\n    - The Exponential Moving Average (EMA) of returns, $m_t$, is generated iteratively: $m_t = (1-\\alpha) m_{t-1} + \\alpha r_t$, with the initial condition $m_{-1}=0$.\n    - The one-step rewards $u_t$ are calculated as $u_t = a_t r_t - c|a_t - a_{t-1}|$, with the initial condition $a_{-1}=0$.\n\n2.  **Target and Feature Construction**: With the simulated trajectories, we construct the components of the regression problem.\n    - The target values $G_t$ (the truncated discounted returns) are calculated for each time step $t \\in \\{0, \\dots, T-1\\}$ using the formula $G_t = \\sum_{k=0}^{K_t-1} \\gamma^k u_{t+k}$, where $K_t = \\min(H, T-t)$.\n    - The $T \\times 3$ design matrix $X$ is assembled, where its $t$-th row is given by $a_t [1, r_{t-1}, m_{t-1}]$.\n    - The $T \\times 1$ target vector $y$ is formed, with its $t$-th element being $G_t$.\n\n3.  **Solving for $\\hat{\\theta}$**: The optimal parameter vector $\\hat{\\theta}$ is found by solving the linear system $(X^\\top X + \\lambda I) \\hat{\\theta} = X^\\top y$. This is accomplished numerically using a standard linear algebra solver, which is generally more stable than computing the matrix inverse directly. The resulting vector $\\hat{\\theta} = [\\hat{\\theta}_0, \\hat{\\theta}_1, \\hat{\\theta}_2]^\\top$ constitutes the learned coefficients for the linear action-value function approximation.\n\nThis entire procedure is implemented for each of the four specified test cases, yielding four distinct coefficient vectors.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def compute_theta(T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed):\n        \"\"\"\n        Computes the optimal theta_hat for a given set of parameters.\n\n        This function implements the three-step procedure:\n        1. Data Generation: Simulates returns, EMAs, actions, and rewards.\n        2. Target and Feature Construction: Computes discounted returns (G_t) and\n           builds the design matrix (X) and target vector (y).\n        3. Solving for theta_hat: Solves the Ridge Regression normal equations.\n        \"\"\"\n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        \n        # Generate random shocks and actions for the entire trajectory\n        epsilons = rng.standard_normal(T)\n        actions_random = rng.choice([-1, 0, 1], size=T)\n        \n        # History arrays: index t corresponds to time t-1.\n        # Size T+1 to hold values from t=-1 to t=T-1.\n        r_hist = np.zeros(T + 1)  # r_hist[t] = r_{t-1}\n        m_hist = np.zeros(T + 1)  # m_hist[t] = m_{t-1}\n        a_hist = np.zeros(T + 1)  # a_hist[t] = a_{t-1}\n\n        # Reward vector: index t corresponds to time t.\n        u_vec = np.zeros(T)\n        \n        # Time-stepping simulation from t=0 to T-1\n        for t in range(T):\n            r_prev = r_hist[t]\n            m_prev = m_hist[t]\n            a_prev = a_hist[t]\n\n            # Calculate r_t, m_t, a_t at time t\n            r_t = mu + phi_r * r_prev + sigma * epsilons[t]\n            m_t = (1 - alpha) * m_prev + alpha * r_t\n            a_t = actions_random[t]\n            \n            # Calculate reward u_t\n            u_t = a_t * r_t - c * np.abs(a_t - a_prev)\n            \n            # Store new values in history arrays\n            r_hist[t + 1] = r_t\n            m_hist[t + 1] = m_t\n            a_hist[t + 1] = a_t\n            u_vec[t] = u_t\n            \n        # 2. Target and Feature Construction\n        \n        # Compute truncated discounted returns G_t\n        G_vec = np.zeros(T)\n        gam_powers = np.power(gam, np.arange(H))\n        \n        for t in range(T):\n            K_t = min(H, T - t)\n            # Sum of discounted future rewards\n            G_vec[t] = np.sum(gam_powers[:K_t] * u_vec[t : t + K_t])\n            \n        # Construct design matrix X and target vector y\n        d = 3  # Dimension of theta\n        X = np.zeros((T, d))\n        y = G_vec\n        \n        for t in range(T):\n            # State features are from time t-1\n            r_prev = r_hist[t]\n            m_prev = m_hist[t]\n            phi_st = np.array([1.0, r_prev, m_prev])\n            \n            # Action is from time t\n            a_t = a_hist[t + 1]\n            \n            # The t-th row of the design matrix\n            X[t, :] = a_t * phi_st\n            \n        # 3. Solving for theta_hat\n        \n        # Formulate the normal equations: A * theta = b\n        A = X.T @ X + lam * np.identity(d)\n        b = X.T @ y\n        \n        # Solve the linear system for theta\n        theta_hat = np.linalg.solve(A, b)\n        \n        return theta_hat.tolist()\n\n    # Test suite (four cases to ensure coverage):\n    # T, mu, phi, sigma, alpha, lambda, gamma, c, H, seed\n    test_cases = [\n        # Case 1 (baseline):\n        (200, 0.0, 0.1, 0.02, 0.2, 0.1, 0.95, 0.0005, 50, 42),\n        # Case 2 (myopic, no regularization):\n        (200, 0.0, 0.0, 0.02, 0.2, 0.0, 0.0, 0.0005, 1, 7),\n        # Case 3 (high regularization):\n        (200, 0.0, 0.3, 0.02, 0.2, 10.0, 0.9, 0.001, 50, 123),\n        # Case 4 (high transaction costs):\n        (200, 0.0, 0.1, 0.02, 0.2, 0.1, 0.95, 0.01, 50, 99),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        # Unpack parameters and call the computation function\n        T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed = params\n        theta_hat = compute_theta(T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed)\n        all_results.append(theta_hat)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An RL agent's performance hinges on its perception of the market, which is determined by its state representation. This capstone exercise moves from isolated concepts to a full end-to-end system, challenging you to build, train, and evaluate a complete trading agent. By systematically investigating how the length of the observation window impacts performance , you will engage in the practical workflow of a quantitative researcher, tuning crucial design parameters to build a more effective agent.",
            "id": "2426648",
            "problem": "Construct a complete, runnable program that studies how the observation window length in the state representation of a trading agent affects out-of-sample performance. The trading environment is a discrete-time, frictional market with a synthetic return process and a finite action set. The agent must be trained separately under each specified window length and then evaluated on an independent test trajectory. The final output must aggregate the results for all cases into a single line as specified below.\n\nDefine the market as follows. Time is indexed by integers $t \\in \\{0,1,\\dots\\}$. Let $r_t$ denote the per-step log-return (expressed as a decimal, not as a percentage) of a synthetic asset. The return process switches deterministically between a positively autocorrelated (trending) regime and a negatively autocorrelated (mean-reverting) regime every $L$ steps. Specifically, for a given $t$, define the regime index $z_t = \\left\\lfloor \\frac{t}{L} \\right\\rfloor \\bmod 2$. Let $(\\mu_0,\\phi_0)$ be the parameters for the trending regime and $(\\mu_1,\\phi_1)$ the parameters for the mean-reverting regime. The return process is\n$$\nr_t \\;=\\; \\mu_{z_t} \\;+\\; \\phi_{z_t}\\, r_{t-1} \\;+\\; \\sigma\\, \\varepsilon_t,\n$$\nwith $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ independent and identically distributed. The initial condition is $r_{-1} = 0$. Use the constants $L=200$, $\\mu_0=0.0005$, $\\phi_0=0.9$, $\\mu_1=0.0$, $\\phi_1=-0.9$, and $\\sigma=0.002$. The agent’s actions at each $t$ are $a_t \\in \\{-1,0,+1\\}$, representing fully short, flat, or fully long positions, respectively. The immediately realized reward per step is\n$$\nR_t \\;=\\; a_t \\, r_t \\;-\\; c \\, |a_t - a_{t-1}| \\;-\\; \\lambda \\, \\hat{v}_t \\, a_t^2,\n$$\nwhere $a_{-1}=0$, $c=0.0005$ is the transaction cost per unit change in position, and $\\lambda=10.0$ is a risk-aversion coefficient applied to a rolling variance estimate $\\hat{v}_t$ of returns computed over the most recent $W$ observed returns available prior to choosing $a_t$ (if fewer than $W$ past returns are available, use all available). The initial variance estimate may be taken as $0$ when insufficient data exist. The return process is exogenous and independent of the agent’s actions.\n\nFor state representation, the agent must form at time $t$ a feature vector using only information available prior to choosing $a_t$. Let $W$ denote the observation window length. The state summary at decision time $t$ is a vector comprising:\n- a constant bias term equal to $1$,\n- the mean of the last up to $W$ realized returns $\\{r_{t-1}, r_{t-2}, \\dots\\}$,\n- the most recent realized return $r_{t-1}$ (taken as $0$ if none),\n- the standard deviation of the last up to $W$ returns,\n- the previous position $a_{t-1}$.\n\nTraining objective: for each specified window length $W$, from first principles, construct a stationary policy $\\pi_W$ that approximately maximizes the expected discounted sum of rewards,\n$$\n\\mathbb{E}\\!\\left[ \\sum_{t=0}^{T_{\\text{train}}-1} \\gamma^t R_t \\right],\n$$\nwith discount factor $\\gamma = 0.95$, by interacting with the environment on training trajectories. The training horizon per episode is $T_{\\text{train}} = 1000$ steps, and the number of training episodes is $E_{\\text{train}} = 8$. The randomness in training must be generated with independent and identically distributed standard normal shocks and be reproducible across window lengths: use the training seeds $s_{\\text{train}} + e$ for episode index $e \\in \\{0,1,\\dots,E_{\\text{train}}-1\\}$ with $s_{\\text{train}} = 12345$. After training $\\pi_W$, evaluate out-of-sample performance by applying the resulting stationary policy greedily without exploration on an independent test trajectory of length $T_{\\text{test}} = 3000$, generated with a distinct seed $s_{\\text{test}} = 54321$ and the same data-generating process. The evaluation metric for each $W$ is the average realized reward per step on the test trajectory,\n$$\n\\bar{R}(W) \\;=\\; \\frac{1}{T_{\\text{test}}} \\sum_{t=0}^{T_{\\text{test}}-1} R_t,\n$$\nexpressed as a decimal.\n\nTest suite. Train and evaluate the agent independently for each observation window length in the list\n$$\n\\mathcal{W} \\;=\\; [\\,1,\\,5,\\,30,\\,240,\\,600\\,].\n$$\n\nYour program must:\n- implement the environment and learning agent strictly according to the definitions above,\n- produce deterministic results that depend only on the specified seeds and parameters,\n- for each $W \\in \\mathcal{W}$, output the corresponding $\\bar{R}(W)$ as a floating-point number rounded to six decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as $\\mathcal{W}$. For example, an output over five test cases must look like\n$$\n[\\bar{R}(1),\\bar{R}(5),\\bar{R}(30),\\bar{R}(240),\\bar{R}(600)],\n$$\nwith each entry rounded to six decimal places. No other text should be printed.\n\nNotes:\n- There are no physical units involved; all returns and rewards must be expressed as decimals.\n- Angles do not appear in this problem.\n- Percentages, if mentioned, must be expressed as decimals (for example, $0.01$ instead of $1\\%$).",
            "solution": "The problem requires the construction and evaluation of a reinforcement learning agent for a synthetic financial trading task. The primary objective is to study the effect of the observation window length, $W$, on the agent's out-of-sample performance. Before proceeding, we must validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Synthetic Return Process**: $r_t = \\mu_{z_t} + \\phi_{z_t} r_{t-1} + \\sigma \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,1)$, $r_{-1} = 0$.\n- **Regime Switching**: Index $z_t = \\lfloor t/L \\rfloor \\bmod 2$, with period $L=200$.\n- **Regime Parameters**: Trending ($z_t=0$): $\\mu_0=0.0005, \\phi_0=0.9$. Mean-reverting ($z_t=1$): $\\mu_1=0.0, \\phi_1=-0.9$.\n- **Volatility**: $\\sigma=0.002$.\n- **Agent Actions**: $a_t \\in \\{-1, 0, +1\\}$, with initial position $a_{-1}=0$.\n- **Reward Function**: $R_t = a_t r_t - c |a_t - a_{t-1}| - \\lambda \\hat{v}_t a_t^2$.\n- **Reward Parameters**: Transaction cost $c=0.0005$, risk aversion $\\lambda=10.0$.\n- **Rolling Variance**: $\\hat{v}_t$ is the variance of the last up to $W$ returns available before time $t$.\n- **State Vector ($s_t$ at decision time $t$)**: A vector of 5 features: [$1$ (bias), mean of recent $\\le W$ returns, $r_{t-1}$, standard deviation of recent $\\le W$ returns, $a_{t-1}$].\n- **Training Objective**: Maximize $\\mathbb{E}[\\sum_{t=0}^{T_{\\text{train}}-1} \\gamma^t R_t]$.\n- **Training Parameters**: Discount factor $\\gamma = 0.95$, training horizon $T_{\\text{train}} = 1000$, number of episodes $E_{\\text{train}} = 8$. Training seeds are $s_{\\text{train}} + e$ for episode $e$, where $s_{\\text{train}} = 12345$.\n- **Evaluation**: Greedy policy (no exploration) on a test trajectory of length $T_{\\text{test}} = 3000$ using seed $s_{\\text{test}} = 54321$.\n- **Evaluation Metric**: Average per-step reward $\\bar{R}(W) = \\frac{1}{T_{\\text{test}}} \\sum_{t=0}^{T_{\\text{test}}-1} R_t$.\n- **Test Suite**: $W \\in \\mathcal{W} = [1, 5, 30, 240, 600]$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded within the field of computational finance, employing standard models like autoregressive processes for returns and a common reward structure. It is well-posed, with all necessary parameters, initial conditions, and evaluation metrics clearly defined. The language is objective and unambiguous. While the specific reinforcement learning algorithm is not prescribed, the directive to \"from first principles, construct a stationary policy\" in this context points toward a fundamental, model-free algorithm. Given the discrete action space and continuous (vector) state space, Q-learning with linear function approximation is the most direct and standard choice. This interpretation is consistent with the level of detail provided elsewhere. The problem is complete, consistent, and computationally feasible.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be constructed.\n\n**Methodology**\nThe task is to train a trading agent using reinforcement learning. We will implement Q-learning, a model-free temporal difference (TD) learning algorithm, with a linear function approximator for the action-value function, $Q(s, a)$.\n\n**1. State and Action-Value Function**\nThe state $s_t$ at time $t$ is a $5$-dimensional feature vector as defined in the problem. The action space is discrete: $\\mathcal{A} = \\{-1, 0, 1\\}$. We approximate the Q-function for each action $a \\in \\mathcal{A}$ as a linear combination of the state features:\n$$\nQ(s_t, a; \\theta_a) = s_t^T \\theta_a\n$$\nwhere $\\theta_a$ is a $5$-dimensional weight vector for action $a$. The agent's goal is to learn the optimal weight vectors $\\theta_a^*$ for all actions.\n\n**2. Learning Algorithm: Q-Learning**\nDuring training, the agent interacts with the environment. At each step $t$, it is in state $s_t$, takes an action $a_t$, receives a reward $R_t$, and observes the next state $s_{t+1}$. The weight vector for the chosen action $a_t$ is updated using the TD error:\n$$\n\\text{TD Error: } \\delta_t = R_t + \\gamma \\max_{a' \\in \\mathcal{A}} Q(s_{t+1}, a'; \\theta_{a'}) - Q(s_t, a_t; \\theta_{a_t})\n$$\n$$\n\\text{Update Rule: } \\theta_{a_t} \\leftarrow \\theta_{a_t} + \\alpha \\cdot \\delta_t \\cdot \\nabla_{\\theta_{a_t}} Q(s_t, a_t; \\theta_{a_t})\n$$\nSince $Q(s_t, a_t; \\theta_{a_t}) = s_t^T \\theta_{a_t}$, the gradient is simply the state vector $s_t$. The update becomes:\n$$\n\\theta_{a_t} \\leftarrow \\theta_{a_t} + \\alpha \\cdot \\delta_t \\cdot s_t\n$$\nHere, $\\alpha$ is the learning rate. We select a standard constant value $\\alpha = 0.01$.\n\n**3. Exploration**\nTo ensure sufficient exploration of the state-action space during training, an $\\epsilon$-greedy policy is used. With probability $\\epsilon$, the agent chooses a random action; otherwise, it chooses the action with the highest estimated Q-value (greedily):\n$$\na_t = \\arg\\max_{a' \\in \\mathcal{A}} Q(s_t, a'; \\theta_{a'})\n$$\nWe use a standard constant exploration rate of $\\epsilon = 0.1$.\n\n**4. Implementation Details**\n- **Reproducibility**: To isolate the effect of the window length $W$, the sequence of random shocks $(\\varepsilon_t)$ for the return process and the random draws for the $\\epsilon$-greedy exploration are pre-generated for each training episode and for the test run. This ensures that for a given episode index, the market dynamics and exploration choices are identical across all experiments for different values of $W$.\n- **State Calculation**: The state vector $s_t$ is constructed using returns realized up to time $t-1$. For $t=0$, where no prior returns exist, the statistical features (mean, standard deviation, last return) are set to $0$. When the number of available returns is less than $W$, all available returns are used for calculations. The standard deviation, and thus the variance term $\\hat{v}_t$ in the reward, is $0$ if fewer than two data points are available.\n- **Training and Evaluation**: For each $W \\in \\mathcal{W}$, the agent's weights are initialized to zero and then trained over $E_{\\text{train}}=8$ episodes, each of length $T_{\\text{train}}=1000$. After training, the learned policy is evaluated on a separate, longer test trajectory of $T_{\\text{test}}=3000$ steps. During evaluation, the policy is purely greedy ($\\epsilon=0$) to measure its out-of-sample performance. The final metric is the average reward per step over this test trajectory.\n\nThis systematic approach allows for a direct comparison of performance attributable solely to the change in the agent's observation window length $W$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates a reinforcement learning trading agent for various\n    observation window lengths, as specified in the problem statement.\n    \"\"\"\n    # Problem Parameters\n    L = 200\n    MU0, PHI0 = 0.0005, 0.9\n    MU1, PHI1 = 0.0, -0.9\n    SIGMA = 0.002\n    C = 0.0005\n    LAMBDA = 10.0\n    GAMMA = 0.95\n    T_TRAIN = 1000\n    E_TRAIN = 8\n    S_TRAIN = 12345\n    T_TEST = 3000\n    S_TEST = 54321\n    WINDOW_LENGTHS = [1, 5, 30, 240, 600]\n    ACTIONS = [-1, 0, 1]\n\n    # RL Hyperparameters\n    ALPHA = 0.01\n    EPSILON = 0.1\n\n    def get_state(t, W, rt_history, at_history):\n        \"\"\"\n        Computes the state vector at time t.\n        State vector: [bias, mean(r), r_{t-1}, std(r), a_{t-1}]\n        \"\"\"\n        if t == 0:\n            return np.array([1.0, 0.0, 0.0, 0.0, 0.0])\n\n        realized_returns = rt_history[1:]  # Exclude r_{-1}\n        \n        window = realized_returns[max(0, len(realized_returns) - W):]\n        \n        r_mean = np.mean(window)\n        r_std = np.std(window) if len(window) > 1 else 0.0\n        r_tm1_feat = realized_returns[-1]\n        a_tm1 = at_history[-1]\n        \n        return np.array([1.0, r_mean, r_tm1_feat, r_std, a_tm1])\n\n    # Pre-generate random numbers for reproducibility across different W\n    train_market_shocks = []\n    train_expl_draws = []\n    train_expl_actions = []\n    for e in range(E_TRAIN):\n        rng_episode = np.random.default_rng(S_TRAIN + e)\n        train_market_shocks.append(rng_episode.standard_normal(size=T_TRAIN))\n        train_expl_draws.append(rng_episode.random(size=T_TRAIN))\n        train_expl_actions.append(rng_episode.choice(ACTIONS, size=T_TRAIN))\n\n    rng_test = np.random.default_rng(S_TEST)\n    test_market_shocks = rng_test.standard_normal(size=T_TEST)\n\n    all_results = []\n    \n    for W in WINDOW_LENGTHS:\n        # --- Training Phase ---\n        weights = {a: np.zeros(5) for a in ACTIONS}\n        \n        for e in range(E_TRAIN):\n            at_history = [0]\n            rt_history = [0.0]\n            for t in range(T_TRAIN):\n                s_t = get_state(t, W, rt_history, at_history)\n                \n                # 1. Choose action a_t using epsilon-greedy policy\n                q_values = {a: np.dot(s_t, weights[a]) for a in ACTIONS}\n                if train_expl_draws[e][t]  EPSILON:\n                    a_t = train_expl_actions[e][t]\n                else:\n                    q_vals_list = [q_values[a] for a in ACTIONS]\n                    best_action_idx = np.argmax(q_vals_list)\n                    a_t = ACTIONS[best_action_idx]\n\n                # 2. Environment step: generate r_t and calculate R_t\n                z_t = (t // L) % 2\n                mu, phi = (MU0, PHI0) if z_t == 0 else (MU1, PHI1)\n                r_tm1 = rt_history[-1]\n                r_t = mu + phi * r_tm1 + SIGMA * train_market_shocks[e][t]\n                \n                a_tm1 = at_history[-1]\n                v_hat_t = s_t[3]**2\n                R_t = a_t * r_t - C * abs(a_t - a_tm1) - LAMBDA * v_hat_t * (a_t**2)\n\n                rt_history.append(r_t)\n                at_history.append(a_t)\n                \n                # 3. Perform Q-learning update\n                s_tp1 = get_state(t + 1, W, rt_history, at_history)\n                q_values_tp1 = {a: np.dot(s_tp1, weights[a]) for a in ACTIONS}\n                max_q_tp1 = max(q_values_tp1.values())\n                td_target = R_t + GAMMA * max_q_tp1\n                td_error = td_target - q_values[a_t]\n                weights[a_t] += ALPHA * td_error * s_t\n\n        # --- Evaluation Phase ---\n        total_reward = 0.0\n        at_history = [0]\n        rt_history = [0.0]\n        for t in range(T_TEST):\n            s_t = get_state(t, W, rt_history, at_history)\n\n            # 1. Choose action a_t greedily\n            q_values = {a: np.dot(s_t, weights[a]) for a in ACTIONS}\n            q_vals_list = [q_values[a] for a in ACTIONS]\n            best_action_idx = np.argmax(q_vals_list)\n            a_t = ACTIONS[best_action_idx]\n            \n            # 2. Environment step\n            z_t = (t // L) % 2\n            mu, phi = (MU0, PHI0) if z_t == 0 else (MU1, PHI1)\n            r_tm1 = rt_history[-1]\n            r_t = mu + phi * r_tm1 + SIGMA * test_market_shocks[t]\n\n            a_tm1 = at_history[-1]\n            v_hat_t = s_t[3]**2\n            R_t = a_t * r_t - C * abs(a_t - a_tm1) - LAMBDA * v_hat_t * (a_t**2)\n            total_reward += R_t\n            \n            rt_history.append(r_t)\n            at_history.append(a_t)\n\n        avg_reward = total_reward / T_TEST\n        all_results.append(round(avg_reward, 6))\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}