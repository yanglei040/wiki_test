{
    "hands_on_practices": [
        {
            "introduction": "强化学习智能体的核心是其奖励函数，它将我们的目标转化为智能体可以优化的数学信号。在进行复杂的策略学习之前，至关重要的一步是精确地定义在单步决策中什么构成“好”或“坏”的结果。本练习将指导您构建一个单步奖励函数，该函数需要平衡几个在现实交易中相互冲突的目标：以优于中间价的价格执行、最小化穿越买卖价差带来的成本，以及惩罚未完成的交易库存。通过将这些金融概念量化，您将为训练一个有效的交易智能体奠定基础。",
            "id": "2423592",
            "problem": "考虑强化学习（RL）中用于动作价值函数（Q函数）的单步奖励设计，该设计应用于限价订单市场中单一资产的最优执行。设决策时刻的状态包括当前中间价 $m$、瞬时买卖价差 $s>0$ 以及智能体的目标方向 $\\sigma \\in \\{+1,-1\\}$，其中 $\\sigma=+1$ 表示买入目标（获取股票），$\\sigma=-1$ 表示卖出目标（清算股票）。在该步骤中，智能体以平均执行价格 $p$ 执行了 $x \\ge 0$ 股，产生一个指标 $I_{\\text{cross}} \\in \\{0,1\\}$，如果该操作穿过价差（市价操作），则该指标等于 $1$，如果该操作是被动的（非市价操作），则等于 $0$。该步骤之后，剩余库存为 $R \\ge 0$。参数 $\\lambda_{\\text{cross}} \\ge 0$ 和 $\\lambda_{\\text{rem}} \\ge 0$ 是给定的非负权重。\n\n将每步奖励 $r$ 定义为单位股价改善和穿越价差指标的唯一仿射函数，该函数同时满足以下原则：\n- 价格改善原则：单位股的贡献为 $\\sigma (m - p)$，奖励那些相对于中间价、与目标方向一致且获得更优价格的执行。\n- 穿越价差抑制原则：穿越价差的操作会产生一个额外的单位股惩罚，该惩罚与价差 $s$ 成正比，由 $\\lambda_{\\text{cross}}$ 加权，并由 $I_{\\text{cross}}$ 激活。\n- 未执行库存惩罚原则：剩余库存会产生一个与 $R$ 成正比的惩罚，由 $\\lambda_{\\text{rem}}$ 加权。\n\n这些原则意味着奖励为\n$$\nr = x [\\sigma (m - p) - \\lambda_{\\text{cross}} I_{\\text{cross}} s] - \\lambda_{\\text{rem}} R.\n$$\n\n实现一个程序，对下面的每个测试用例，使用 $(m, s, \\sigma, p, x, I_{\\text{cross}}, R, \\lambda_{\\text{cross}}, \\lambda_{\\text{rem}})$ 的输入值计算 $r$，所有算术运算均以实数进行。输出必须四舍五入到六位小数。\n\n使用以下测试套件：\n- 测试用例 1 (买入，在买价被动成交): $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=99.99$, $x=100$, $I_{\\text{cross}}=0$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 测试用例 2 (买入，在卖价市价成交): $m=100.00$, $s=0.02$, $\\sigma=+1$, $p=100.01$, $x=100$, $I_{\\text{cross}}=1$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 测试用例 3 (卖出，在卖价被动成交): $m=250.50$, $s=0.10$, $\\sigma=-1$, $p=250.55$, $x=50$, $I_{\\text{cross}}=0$, $R=0$, $\\lambda_{\\text{cross}}=0.25$, $\\lambda_{\\text{rem}}=0.0$。\n- 测试用例 4 (零价差边界): $m=10.00$, $s=0.00$, $\\sigma=+1$, $p=10.00$, $x=10$, $I_{\\text{cross}}=1$, $R=0$, $\\lambda_{\\text{cross}}=999.0$, $\\lambda_{\\text{rem}}=0.0$。\n- 测试用例 5 (无执行，仅有剩余库存惩罚): $m=100.00$, $s=0.20$, $\\sigma=+1$, $p=100.00$, $x=0$, $I_{\\text{cross}}=0$, $R=1000$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.01$。\n- 测试用例 6 (卖出，在买价市价成交并部分完成): $m=50.00$, $s=0.04$, $\\sigma=-1$, $p=49.98$, $x=200$, $I_{\\text{cross}}=1$, $R=300$, $\\lambda_{\\text{cross}}=0.1$, $\\lambda_{\\text{rem}}=0.05$。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[r_1,r_2,\\dots]$），每个 $r_i$ 都四舍五入到六位小数，并且不包含任何额外文本。",
            "solution": "该问题要求为一个从事最优执行的Q学习智能体计算单步奖励，记为 $r$。在尝试求解之前，需对问题陈述进行验证。\n\n首先，我们提取给定信息。\n状态和动作空间变量定义如下：\n- 中间价：$m$\n- 买卖价差：$s > 0$\n- 智能体的目标方向：$\\sigma \\in \\{+1, -1\\}$，其中 $+1$ 为买入，$-1$ 为卖出。\n- 执行股数：$x \\ge 0$\n- 平均执行价格：$p$\n- 穿越价差指标：$I_{\\text{cross}} \\in \\{0, 1\\}$\n- 剩余库存：$R \\ge 0$\n- 非负权重：$\\lambda_{\\text{cross}} \\ge 0$ 和 $\\lambda_{\\text{rem}} \\ge 0$。\n\n单步奖励函数 $r$ 被明确定义为：\n$$\nr = x [\\sigma (m - p) - \\lambda_{\\text{cross}} I_{\\text{cross}} s] - \\lambda_{\\text{rem}} R.\n$$\n问题是为给定的参数测试套件计算 $r$。\n\n接下来，我们验证该问题。\n1.  **科学依据**：该公式在计算金融和算法交易中是标准的。奖励函数是三个关键绩效指标的线性组合：执行差额（价格改善）、穿越价差成本和库存惩罚。这些都是市场微观结构中基本且公认的概念。该模型是对最优执行中权衡取舍的一个有效（尽管简化了）的表示。\n2.  **良构性**：该问题要求直接对一个给定的代数表达式进行求值。对于测试用例中提供的每一组输入，该公式都会产生一个唯一的、明确定义的实数。该设置既不是欠约束也不是过约束。\n3.  **客观性**：问题以精确的数学语言陈述，没有歧义、主观性或观点。一个测试用例包含 $s=0$，这在技术上违反了前提 $s>0$。然而，这是一个边界情况，并不会使公式本身失效，因为公式对于 $s=0$ 仍有良好定义。因此，该问题被认为是有效的。\n\n该问题是有效的。我们继续进行求解。计算过程是对每个测试用例直接应用所提供的公式。\n\n根据指导原则，奖励函数可以分解为三个部分：\n1.  **价格改善项**：$x \\cdot \\sigma (m - p)$。该项衡量相对于决策时中间价 $m$ 的表现。如果 $\\sigma = +1$（买入），当执行价格 $p$ 低于 $m$ 时，会获得正奖励。如果 $\\sigma = -1$（卖出），当 $p$ 高于 $m$ 时，会获得正奖励。这与低买高卖的目标一致。总贡献按执行股数 $x$ 进行缩放。\n2.  **穿越价差惩罚项**：$-x \\cdot \\lambda_{\\text{cross}} \\cdot I_{\\text{cross}} \\cdot s$。此项惩罚为获取即时执行而“穿越价差”的激进操作。指标 $I_{\\text{cross}}=1$ 会激活该惩罚。其大小与股数 $x$、当前价差 $s$ 和参数 $\\lambda_{\\text{cross}}$ 成正比。如果操作是被动的（$I_{\\text{cross}}=0$），则此项为零。\n3.  **剩余库存惩罚项**：$-\\lambda_{\\text{rem}} \\cdot R$。此项惩罚智能体未能完成其执行目标，表现为剩余库存 $R$。其大小由参数 $\\lambda_{\\text{rem}}$ 控制。\n\n我们现在为每个测试用例计算奖励 $r$。\n\n**测试用例 1**：\n- 输入：$m=100.00$, $s=0.02$, $\\sigma=+1$, $p=99.99$, $x=100$, $I_{\\text{cross}}=0$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 计算：\n$$\nr_1 = 100 \\cdot [+1 \\cdot (100.00 - 99.99) - 0.5 \\cdot 0 \\cdot 0.02] - 0.001 \\cdot 900\n$$\n$$\nr_1 = 100 \\cdot (0.01 - 0) - 0.9 = 1.0 - 0.9 = 0.1\n$$\n\n**测试用例 2**：\n- 输入：$m=100.00$, $s=0.02$, $\\sigma=+1$, $p=100.01$, $x=100$, $I_{\\text{cross}}=1$, $R=900$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.001$。\n- 计算：\n$$\nr_2 = 100 \\cdot [+1 \\cdot (100.00 - 100.01) - 0.5 \\cdot 1 \\cdot 0.02] - 0.001 \\cdot 900\n$$\n$$\nr_2 = 100 \\cdot (-0.01 - 0.01) - 0.9 = 100 \\cdot (-0.02) - 0.9 = -2.0 - 0.9 = -2.9\n$$\n\n**测试用例 3**：\n- 输入：$m=250.50$, $s=0.10$, $\\sigma=-1$, $p=250.55$, $x=50$, $I_{\\text{cross}}=0$, $R=0$, $\\lambda_{\\text{cross}}=0.25$, $\\lambda_{\\text{rem}}=0.0$。\n- 计算：\n$$\nr_3 = 50 \\cdot [-1 \\cdot (250.50 - 250.55) - 0.25 \\cdot 0 \\cdot 0.10] - 0.0 \\cdot 0\n$$\n$$\nr_3 = 50 \\cdot [-1 \\cdot (-0.05) - 0] - 0 = 50 \\cdot (0.05) = 2.5\n$$\n\n**测试用例 4**：\n- 输入：$m=10.00$, $s=0.00$, $\\sigma=+1$, $p=10.00$, $x=10$, $I_{\\text{cross}}=1$, $R=0$, $\\lambda_{\\text{cross}}=999.0$, $\\lambda_{\\text{rem}}=0.0$。\n- 计算：\n$$\nr_4 = 10 \\cdot [+1 \\cdot (10.00 - 10.00) - 999.0 \\cdot 1 \\cdot 0.00] - 0.0 \\cdot 0\n$$\n$$\nr_4 = 10 \\cdot (0 - 0) - 0 = 0.0\n$$\n\n**测试用例 5**：\n- 输入：$m=100.00$, $s=0.20$, $\\sigma=+1$, $p=100.00$, $x=0$, $I_{\\text{cross}}=0$, $R=1000$, $\\lambda_{\\text{cross}}=0.5$, $\\lambda_{\\text{rem}}=0.01$。\n- 计算：由于 $x=0$，整个第一项为 $0$。\n$$\nr_5 = 0 \\cdot [\\dots] - 0.01 \\cdot 1000 = 0 - 10.0 = -10.0\n$$\n\n**测试用例 6**：\n- 输入：$m=50.00$, $s=0.04$, $\\sigma=-1$, $p=49.98$, $x=200$, $I_{\\text{cross}}=1$, $R=300$, $\\lambda_{\\text{cross}}=0.1$, $\\lambda_{\\text{rem}}=0.05$。\n- 计算：\n$$\nr_6 = 200 \\cdot [-1 \\cdot (50.00 - 49.98) - 0.1 \\cdot 1 \\cdot 0.04] - 0.05 \\cdot 300\n$$\n$$\nr_6 = 200 \\cdot [-1 \\cdot (0.02) - 0.004] - 15.0 = 200 \\cdot (-0.02 - 0.004) - 15.0\n$$\n$$\nr_6 = 200 \\cdot (-0.024) - 15.0 = -4.8 - 15.0 = -19.8\n$$\n\n最终结果四舍五入到六位小数为：$r_1=0.100000$，$r_2=-2.900000$，$r_3=2.500000$，$r_4=0.000000$，$r_5=-10.000000$，以及 $r_6=-19.800000$。这些结果将在程序中实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the single-step reward 'r' for a series of test cases in an\n    optimal execution problem, based on a given formula.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple represents a test case with parameters:\n    # (m, s, sigma, p, x, I_cross, R, lambda_cross, lambda_rem)\n    test_cases = [\n        (100.00, 0.02, +1, 99.99, 100, 0, 900, 0.5, 0.001),  # Test case 1\n        (100.00, 0.02, +1, 100.01, 100, 1, 900, 0.5, 0.001),  # Test case 2\n        (250.50, 0.10, -1, 250.55, 50, 0, 0, 0.25, 0.0),      # Test case 3\n        (10.00, 0.00, +1, 10.00, 10, 1, 0, 999.0, 0.0),       # Test case 4\n        (100.00, 0.20, +1, 100.00, 0, 0, 1000, 0.5, 0.01),     # Test case 5\n        (50.00, 0.04, -1, 49.98, 200, 1, 300, 0.1, 0.05),     # Test case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack the parameters for clarity\n        m, s, sigma, p, x, I_cross, R, lambda_cross, lambda_rem = case\n        \n        # Calculate the reward 'r' using the provided formula:\n        # r = x * [sigma * (m - p) - lambda_cross * I_cross * s] - lambda_rem * R\n        \n        price_improvement_term = sigma * (m - p)\n        spread_crossing_penalty = lambda_cross * I_cross * s\n        execution_component = x * (price_improvement_term - spread_crossing_penalty)\n        \n        inventory_penalty = lambda_rem * R\n        \n        reward = execution_component - inventory_penalty\n        \n        # Round the result to six decimal places as required.\n        rounded_reward = round(reward, 6)\n        results.append(f\"{rounded_reward:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在设计了单步奖励函数之后，下一步是理解智能体操作的完整环境。最优执行问题不仅涉及瞬时奖励，还涉及当前行为对未来市场状态和机会的长期影响。本练习要求您模拟一个完整的交易执行过程，其中包含了临时和永久价格影响等关键市场动态。通过为一个给定的动作序列计算总回报，您将深入理解智能体决策的累积后果，这是评估任何学习策略有效性的必要前提。",
            "id": "2423600",
            "problem": "给定一个离散时间执行环境，用于在有限的时间范围内出售固定库存。在每个时间步，交易者选择一个非负整数操作，指定要出售的数量。该交易以包含暂时性价格影响的执行价格产生即时现金流，并永久性地移动后续步骤中使用的中间价。如果在截止时间前未清算全部库存，则会施加最终惩罚。目标是为每个指定的参数集计算给定操作序列所获得的总回合奖励。\n\n形式上，令时间指数为 $t \\in \\{0,1,\\dots,T-1\\}$，截止时间为 $T$。状态包含当前中间价 $S_t \\in \\mathbb{R}$ 和剩余库存 $x_t \\in \\mathbb{N}_0$。交易者选择一个意向操作 $a_t \\in \\mathbb{N}_0$，代表要出售的单位数。执行数量为\n$$\na'_t \\equiv \\min\\{a_t, x_t\\}.\n$$\n在时间 $t$ 的执行价格是\n$$\n\\tilde{S}_t \\equiv S_t - \\eta a'_t,\n$$\n其中 $\\eta \\ge 0$ 是暂时性影响系数。在时间 $t$ 的即时奖励（现金流）是\n$$\nr_t \\equiv a'_t \\tilde{S}_t = a'_t \\left(S_t - \\eta a'_t\\right).\n$$\n库存和价格演变如下\n$$\nx_{t+1} = x_t - a'_t,\\qquad S_{t+1} = S_t - \\kappa a'_t + \\epsilon_t,\n$$\n其中 $\\kappa \\ge 0$ 是永久性影响系数，$\\{\\epsilon_t\\}_{t=0}^{T-1}$ 是给定的外生价格冲击的确定性序列。在最终时间 $T$，对任何剩余库存施加惩罚：\n$$\nR_T \\equiv -\\lambda x_T^2,\n$$\n其中 $\\lambda \\ge 0$。总回合奖励为\n$$\nG \\equiv \\sum_{t=0}^{T-1} r_t + R_T.\n$$\n所有价格、成本和奖励均以任意货币单位计；不涉及物理单位。\n\n实现一个程序，对于下面测试套件中的每个参数集，根据提供的操作序列计算 $G$。对于每种情况，使用上述动态模型和给定参数。您必须将 $a_t$ 视为意向操作，并按规定执行 $a'_t = \\min\\{a_t, x_t\\}$。\n\n测试套件（每种情况指定 $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$）：\n- 情况1：$S_0 = 100.0$, $x_0 = 5$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, 操作 $[2,2,1]$, 冲击 $[0.0,0.0,0.0]$。\n- 情况2：$S_0 = 100.0$, $x_0 = 5$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, 操作 $[1,1,1]$, 冲击 $[0.0,0.0,0.0]$。\n- 情况3：$S_0 = 100.0$, $x_0 = 0$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, 操作 $[2,2,2]$, 冲击 $[0.0,0.0,0.0]$。\n- 情况4：$S_0 = 50.0$, $x_0 = 3$, $T = 2$, $\\kappa = 0.3$, $\\eta = 1.0$, $\\lambda = 5.0$, 操作 $[2,2]$, 冲击 $[1.5,0.0]$。\n- 情况5：$S_0 = 20.0$, $x_0 = 10$, $T = 2$, $\\kappa = 0.0$, $\\eta = 0.0$, $\\lambda = 100.0$, 操作 $[0,0]$, 冲击 $[0.0,0.0]$。\n- 情况6：$S_0 = 100.0$, $x_0 = 4$, $T = 2$, $\\kappa = 0.5$, $\\eta = 0.25$, $\\lambda = 1.0$, 操作 $[3,1]$, 冲击 $[-10.0,0.0]$。\n\n您的程序应生成单行输出，其中包含六种情况的结果，结果为用方括号括起来的逗号分隔列表。每个结果必须四舍五入到六位小数。输出必须采用以下确切格式：\n- 仅一行。\n- 用方括号括起列表。\n- 值用逗号分隔。\n- 每个值显示小数点后恰好六位数。",
            "solution": "问题陈述是有效的。它提出了一个在计算金融领域内定义明确、有科学依据的任务，具体涉及最优执行模型。该问题要求在一个离散的有限时间范围内模拟交易过程，其所有控制方程、参数和初始条件都已明确无误地给出。目标是为给定的操作序列计算总回合奖励，这是一个基于指定模型动态的确定性计算。\n\n该模型通过中间价 $S_t$ 和剩余库存 $x_t$ 来描述系统在时间 $t$ 的状态。该过程在时间步 $t \\in \\{0, 1, \\dots, T-1\\}$ 上进行模拟。\n\n对于给定的参数集 $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$，计算总回合奖励 $G$ 的过程如下。\n\n首先，将总奖励累加器初始化为零，我们称之为 $G_{acc} = 0$。初始状态由 $(S_0, x_0)$ 给出。\n\n接下来，我们从 $0$ 到 $T-1$ 遍历每个时间步 $t$。在每一步中：\n$1$. 确定执行数量 $a'_t$。由于交易者卖出的库存不能超过可用库存，执行数量是意向操作 $a_t$ 和当前库存 $x_t$ 的最小值。\n$$\na'_t = \\min\\{a_t, x_t\\}\n$$\n$2$. 计算在时间 $t$ 获得的即时奖励 $r_t$。这是销售产生的现金流，即执行数量 $a'_t$ 和执行价格 $\\tilde{S}_t$ 的乘积。执行价格是中间价 $S_t$ 根据与交易规模成正比的暂时性价格影响进行调整后的价格，系数为 $\\eta$。\n$$\nr_t = a'_t \\cdot \\tilde{S}_t = a'_t \\cdot (S_t - \\eta \\cdot a'_t)\n$$\n$3$. 将此即时奖励加到累积的总奖励中：\n$$\nG_{acc} \\leftarrow G_{acc} + r_t\n$$\n$4$. 更新下一时间步 $t+1$ 的状态变量。库存因执行数量 $a'_t$ 而减少。中间价的更新考虑了与 $a'_t$ 成正比（系数为 $\\kappa$）的永久性价格影响，以及外生价格冲击 $\\epsilon_t$。\n$$\nx_{t+1} = x_t - a'_t\n$$\n$$\nS_{t+1} = S_t - \\kappa \\cdot a'_t + \\epsilon_t\n$$\n此循环一直持续到 $t=T-1$。循环的最后一步之后，状态为 $(S_T, x_T)$。\n\n最后，我们对在截止时间 $T$ 仍未售出的任何库存 $x_T$ 施加最终惩罚 $R_T$。该惩罚是剩余库存的二次函数，惩罚系数为 $\\lambda$。\n$$\nR_T = -\\lambda \\cdot x_T^2\n$$\n总回合奖励 $G$ 是在整个交易时段内累积的所有即时奖励与最终惩罚之和。\n$$\nG = G_{acc} + R_T = \\left(\\sum_{t=0}^{T-1} r_t\\right) + R_T\n$$\n对于每个给定的测试用例，此过程都会为 $G$ 产生一个唯一的确定性值。该计算是给定公式的直接应用。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total episodic reward for a series of optimal execution problems.\n    \"\"\"\n    # Test suite (S0, x0, T, kappa, eta, lambda, actions, shocks)\n    test_cases = [\n        # Case 1\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [2, 2, 1], [0.0, 0.0, 0.0]),\n        # Case 2\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [1, 1, 1], [0.0, 0.0, 0.0]),\n        # Case 3\n        (100.0, 0, 3, 0.2, 0.5, 10.0, [2, 2, 2], [0.0, 0.0, 0.0]),\n        # Case 4\n        (50.0, 3, 2, 0.3, 1.0, 5.0, [2, 2], [1.5, 0.0]),\n        # Case 5\n        (20.0, 10, 2, 0.0, 0.0, 100.0, [0, 0], [0.0, 0.0]),\n        # Case 6\n        (100.0, 4, 2, 0.5, 0.25, 1.0, [3, 1], [-10.0, 0.0]),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        s0, x0, T, kappa, eta, lambd, actions, shocks = case\n        \n        # Initialize state variables\n        s_t = s0\n        x_t = x0\n        total_reward = 0.0\n\n        # Simulate the execution process over the time horizon\n        for t in range(T):\n            action_t = actions[t]\n            shock_t = shocks[t]\n\n            # 1. Determine executed quantity\n            executed_quantity = min(action_t, x_t)\n\n            # 2. Calculate immediate reward\n            if executed_quantity > 0:\n                execution_price = s_t - eta * executed_quantity\n                immediate_reward = executed_quantity * execution_price\n                total_reward += immediate_reward\n            \n            # 3. Update state for the next time step\n            x_t_plus_1 = x_t - executed_quantity\n            s_t_plus_1 = s_t - kappa * executed_quantity + shock_t\n            \n            # Move to the next state\n            x_t = x_t_plus_1\n            s_t = s_t_plus_1\n\n        # 4. Calculate terminal penalty\n        terminal_penalty = -lambd * (x_t ** 2)\n        total_reward += terminal_penalty\n        \n        results.append(total_reward)\n\n    # Format the results as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现在，您已经掌握了奖励函数和环境动力学，是时候构建一个能自主学习最优策略的完整Q-learning智能体了。这个综合性练习将引导您从零开始实施表格型Q-learning算法，以解决一个程式化的最优执行问题。您的任务不仅是训练智能体，还要研究一个关键参数——折扣因子 $\\gamma$——如何影响其行为。通过观察智能体在不同时间偏好下的清仓策略，您将亲身体验到在短期交易成本和长期持仓风险之间进行权衡的微妙之处。",
            "id": "2423640",
            "problem": "给定一个程式化的最优执行问题，该问题被建模为一个离散时间的马尔可夫决策过程 (MDP)，其中一个强化学习 (RL) 智能体通过标准的Q学习递归来学习一个动作价值函数。目标是检验折扣因子 $\\gamma$ 如何影响智能体的有效交易期限，该期限定义为在学习到的动作价值函数所导出的贪婪策略下，完全清算一个固定库存所需的时间步数。\n\n该MDP的定义如下。\n\n- 时间：$t \\in \\{0,1,\\dots,T\\}$，具有固定的期限 $T$。当 $t=T$ 或库存降至零时，以先发生者为准，回合终止。\n- 状态：$s_t=(t,x_t)$，其中库存 $x_t \\in \\{0,1,\\dots,X_0\\}$ 是在时间 $t$ 剩余的离散单位数量。\n- 动作：在状态 $s_t=(t,x_t)$，智能体选择一个整数交易规模 $a_t \\in \\{0,1,\\dots,\\min(s_{\\max},x_t)\\}$，代表在时期 $t$ 内卖出的单位数量。\n- 转移：库存确定性地演化为 $x_{t+1}=x_t - a_t$，时间则递增 $t \\mapsto t+1$。\n- 即时奖励：在状态 $s_t=(t,x_t)$ 采取动作 $a_t$ 的奖励为\n$$\nr_t = -(k a_t^2 + \\lambda x_t^2),\n$$\n其中 $k>0$ 是交易成本曲率，$\\lambda>0$ 是库存持有风险惩罚。终止状态的延续价值为零。\n- 目标：对于一个固定的折扣因子 $\\gamma \\in [0,1]$，智能体寻求最大化期望折扣回报 $\\sum_{t=0}^{T-1} \\gamma^t r_t$。\n\nQ学习智能体根据以下递归更新动作价值估计：\n$$\nQ(s_t,a_t) \\leftarrow (1-\\alpha) Q(s_t,a_t) + \\alpha \\left[r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1},a')\\right],\n$$\n按照惯例，如果 $s_{t+1}$ 是终止状态（即 $t+1=T$ 或 $x_{t+1}=0$），则目标值减少为 $r_t$，因为没有延续价值。这里 $\\alpha \\in (0,1]$ 是学习率，$\\mathcal{A}(s)$ 表示在状态 $s$ 下的可行动作集合。在学习过程中，智能体遵循 $\\varepsilon$-贪婪行为策略：以概率 $\\varepsilon$ 均匀随机选择一个可行动作，以概率 $1-\\varepsilon$ 选择一个使当前 $Q$ 估计值最大化的贪婪动作。$\\arg\\max$ 中的平局总是通过选择最大的动作来打破（即，在所有最大化动作中选择最大的 $a$）。\n\n训练期间用于动作选择的所有随机性都必须由一个以种子 $0$ 初始化的伪随机数生成器产生，以确保可复现性。\n\n要使用的参数值如下：\n- 期限 $T=10$。\n- 初始库存 $X_0=10$。\n- 每期最大卖出规模 $s_{\\max}=3$。\n- 交易成本曲率 $k=0.05$。\n- 库存持有风险惩罚 $\\lambda=0.10$。\n- 学习率 $\\alpha=0.10$。\n- 探索概率 $\\varepsilon=0.20$。\n- 训练回合数 $N_{\\text{episodes}}=20000$。\n- 每个训练回合都从固定的初始状态 $s_0=(0,X_0)$ 开始。\n\n对于下面测试集中的每个折扣因子 $\\gamma$，你必须：\n- 根据上述规格训练Q学习智能体。\n- 训练后，导出由学习到的动作价值函数所引出的贪婪策略。\n- 从固定的初始状态 $s_0=(0,X_0)$ 开始，在没有探索的情况下模拟贪婪策略，以获得清算时间 $L(\\gamma)$，定义为使得 $x_{\\ell}=0$ 的最小整数 $\\ell \\in \\{0,1,\\dots,T\\}$。如果在 $T$ 之前库存未能完全清算，则定义 $L(\\gamma)=T$。\n\n折扣因子测试集：\n- $\\gamma \\in \\{0.0, 0.3, 0.6, 0.9, 0.99, 1.0\\}$。\n\n你的程序必须输出六个清算时间，形式为单行、用方括号括起来的逗号分隔列表，顺序与测试集中的顺序一致。要求的最终输出格式是：\n- 单独一行：例如，$[1,2,3,4,5,6]$，其中每个条目是与测试集中相同位置的折扣因子相对应的整数清算时间。输出行中不允许有空格。",
            "solution": "所提出的问题是一个在计算金融领域，特别是最优交易执行领域内，有效且适定的最优控制问题。它要求实施Q学习算法来解决一个离散时间的马尔可夫决策过程 (MDP)。我将首先阐明其理论基础，然后描述获得解决方案的算法流程。\n\n该问题被建模为一个由元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ 定义的 MDP。\n- 状态空间 $\\mathcal{S}$ 由成对的 $s_t=(t,x_t)$ 组成，其中 $t \\in \\{0, 1, \\dots, T\\}$ 是时间步，而 $x_t \\in \\{0, 1, \\dots, X_0\\}$ 是剩余库存。当 $T=10$ 和 $X_0=10$ 时，状态数量为 $(T+1) \\times (X_0+1) = 11 \\times 11 = 121$。\n- 在状态 $s_t=(t,x_t)$ 的动作空间 $\\mathcal{A}(s_t)$ 由允许的交易规模集合 $a_t \\in \\{0, 1, \\dots, \\min(s_{\\max}, x_t)\\}$ 组成，其中 $s_{\\max}=3$。\n- 转移函数是确定性的。给定状态 $s_t=(t,x_t)$ 和动作 $a_t$，下一个状态是 $s_{t+1}=(t+1, x_t - a_t)$。\n- 即时奖励函数 $r(s_t, a_t)$ 由 $r_t = -(k a_t^2 + \\lambda x_t^2)$ 给出。该函数捕捉了最优执行中的基本权衡：卖出大量 $a_t$ 会产生高昂的二次交易成本 ($k a_t^2$)，而持有大量库存 $x_t$ 则会产生高昂的二次持有风险成本 ($\\lambda x_t^2$)。智能体的目标是最小化累积总成本，这等同于最大化累积总负奖励。\n- 折扣因子 $\\gamma \\in [0,1]$ 决定了未来奖励的现值。\n\n目标是找到一个最优策略 $\\pi^*: \\mathcal{S} \\to \\mathcal{A}$，对于任何起始状态，该策略都能最大化期望的折扣奖励总和，即价值函数。我们使用Q学习，一种无模型的强化学习算法，来找到最优动作价值函数 $Q^*(s,a)$，它代表从状态 $s$ 开始，采取动作 $a$，然后遵循最优策略所能获得的最大期望回报。最优动作价值函数满足贝尔曼最优方程：\n$$\nQ^*(s,a) = \\mathbb{E}\\left[r + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^*(s', a') \\mid s, a\\right]\n$$\n其中 $s'$ 是在状态 $s$ 采取动作 $a$ 之后的下一个状态。考虑到确定性转移，这可以简化为：\n$$\nQ^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q^*(s_{t+1}, a')\n$$\nQ学习通过使用与环境交互的样本，迭代地逼近 $Q^*$ 来更新一个估计值 $Q(s,a)$。更新规则是：\n$$\nQ(s_t, a_t) \\leftarrow (1-\\alpha)Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1}, a')\\right]\n$$\n这里，$\\alpha=0.10$ 是学习率。在训练期间使用 $\\varepsilon=0.20$ 的 $\\varepsilon$-贪婪策略以确保对状态-动作空间的探索。为了可复现性，随机种子被固定为 $0$。\n\n折扣因子 $\\gamma$ 的作用是这个问题的核心。它决定了智能体的时间偏好。\n- 对于 $\\gamma \\approx 0$，智能体是短视的，会严重折扣未来的奖励（成本）。它主要寻求最大化即时奖励 $r_t = -k a_t^2 - \\lambda x_t^2$。由于在状态 $s_t$ 下 $x_t$ 是固定的，这简化为最大化 $-k a_t^2$，即通过最小化 $a_t$ 来实现。智能体学会尽可能少地交易，导致清算时间非常长或无法完成清算。清算时间 $L(\\gamma)$ 应该很大，可能为 $T=10$。\n- 对于 $\\gamma \\approx 1$，智能体是有远见的。项 $\\gamma \\max_{a'} Q(s_{t+1}, a')$ 变得非常重要。这一项携带了所有未来成本的信息。智能体认识到持有库存 ($x > 0$) 将会产生一系列未来的持有成本。为了最小化总成本，它有动力更快地清算库存，即使这意味着要承担更高的即时交易成本。这将导致更短的清算时间。\n\n因此，我们预期清算时间 $L(\\gamma)$ 是折扣因子 $\\gamma$ 的一个非增函数。\n\n解决方案按以下步骤实现：\n1. 对于每个给定的 $\\gamma$ 值，将一个维度为 $(T+1, X_0+1, s_{\\max}+1)$ 的Q表初始化为零。\n2. 智能体被训练 $N_{\\text{episodes}} = 20000$ 个回合。在每个回合中，从 $s_0=(0, 10)$ 开始，智能体与环境交互，直到达到终止状态（$t=T$ 或 $x=0$）。\n3. 在每一步，通过 $\\varepsilon$-贪婪策略选择一个动作。严格执行平局打破规则，即在具有相同Q值的动作中，优先选择最大的交易规模。\n4. 根据指定的递归关系更新Q表。\n5. 训练结束后，提取贪婪策略 $\\pi_G(s) = \\arg\\max_a Q(s,a)$。同样的平局打破规则适用。\n6. 从 $s_0=(0, 10)$ 开始，运行一次确定性模拟，并遵循 $\\pi_G$。将库存 $x_t$ 降至 $0$ 所需的时间步数记录为清算时间 $L(\\gamma)$。如果到 $t=T$ 时库存不为零，则 $L(\\gamma)$ 设置为 $T$。\n对所有指定的 $\\gamma$ 值重复此过程，并报告最终的清算时间。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal execution problem using Q-learning for a suite of discount factors.\n    \"\"\"\n    \n    # Problem parameters\n    T = 10\n    X0 = 10\n    S_MAX = 3\n    K = 0.05\n    LAMBDA = 0.10\n\n    # Q-learning parameters\n    ALPHA = 0.10\n    EPSILON = 0.20\n    N_EPISODES = 20000\n    SEED = 0\n    \n    # Test suite for the discount factor\n    gammas = [0.0, 0.3, 0.6, 0.9, 0.99, 1.0]\n    \n    liquidation_times = []\n\n    for gamma in gammas:\n        # Initialize Q-table: Q(t, x, a)\n        # Dimensions: (time_steps, inventory_levels, action_choices)\n        q_table = np.zeros((T + 1, X0 + 1, S_MAX + 1))\n        \n        # Initialize pseudo-random number generator for reproducibility\n        rng = np.random.default_rng(SEED)\n\n        # Main training loop\n        for _ in range(N_EPISODES):\n            t = 0\n            x = X0\n            \n            # An episode runs until a terminal state is reached\n            while t  T and x > 0:\n                current_state_t = t\n                current_state_x = x\n\n                # Determine the set of feasible actions\n                feasible_actions = list(range(min(S_MAX, current_state_x) + 1))\n\n                # Epsilon-greedy action selection\n                if rng.random()  EPSILON:\n                    # Exploration: choose a random feasible action\n                    action = rng.choice(feasible_actions)\n                else:\n                    # Exploitation: choose the best action based on Q-values\n                    q_values_for_state = q_table[current_state_t, current_state_x, feasible_actions]\n                    max_q = np.max(q_values_for_state)\n                    \n                    # Tie-breaking: choose the largest action among those with max Q-value\n                    best_actions = [a for i, a in enumerate(feasible_actions) if q_values_for_state[i] == max_q]\n                    action = max(best_actions)\n\n                # Execute action: calculate reward and find next state\n                reward = -(K * action**2 + LAMBDA * current_state_x**2)\n                next_t = t + 1\n                next_x = x - action\n\n                # Q-table update\n                is_next_state_terminal = (next_t == T) or (next_x == 0)\n                \n                if is_next_state_terminal:\n                    q_max_next = 0.0\n                else:\n                    next_feasible_actions = list(range(min(S_MAX, next_x) + 1))\n                    q_max_next = np.max(q_table[next_t, next_x, next_feasible_actions])\n                \n                target = reward + gamma * q_max_next\n                \n                old_q_value = q_table[current_state_t, current_state_x, action]\n                q_table[current_state_t, current_state_x, action] = \\\n                    (1 - ALPHA) * old_q_value + ALPHA * target\n\n                # Transition to the next state\n                t = next_t\n                x = next_x\n        \n        # After training, simulate the greedy policy to find liquidation time\n        t_sim = 0\n        x_sim = X0\n        liquidation_time = T # Default if not liquidated by T\n\n        while t_sim  T and x_sim > 0:\n            current_state_t_sim = t_sim\n            current_state_x_sim = x_sim\n\n            # Select greedy action with the specified tie-breaking rule\n            sim_feasible_actions = list(range(min(S_MAX, current_state_x_sim) + 1))\n            q_values_sim = q_table[current_state_t_sim, current_state_x_sim, sim_feasible_actions]\n            max_q_sim = np.max(q_values_sim)\n            best_actions_sim = [a for i, a in enumerate(sim_feasible_actions) if q_values_sim[i] == max_q_sim]\n            action_sim = max(best_actions_sim)\n\n            x_sim -= action_sim\n            t_sim += 1\n            \n            if x_sim == 0:\n                liquidation_time = t_sim\n                break\n        \n        liquidation_times.append(liquidation_time)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, liquidation_times))}]\")\n\nsolve()\n```"
        }
    ]
}