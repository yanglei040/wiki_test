{
    "hands_on_practices": [
        {
            "introduction": "Our journey into the practicalities of floating-point arithmetic begins with a fundamental question: what is the smallest positive number that a computer recognizes as being greater than zero when added to one? In this exercise , you will move beyond textbook definitions to algorithmically discover this value, often related to 'machine epsilon,' in the context of a minimal interest rate. This hands-on discovery process reveals the discrete nature of floating-point numbers and provides a tangible baseline for understanding the precision limits you will encounter in financial models.",
            "id": "2394219",
            "problem": "You are tasked with writing a complete, runnable program that determines how finite-precision arithmetic affects the detectability of small interest rates in single-precision floating point arithmetic. Work within the International Electrotechnical Commission (IEC) and Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) model for single precision (base-2, rounding to nearest with ties to even) and treat interest rates as pure decimals (no percent signs). In this model, the representable numbers near $1$ are discretely spaced, and an interest rate $r$ is computationally distinguishable from $0$ at unit scale if and only if the single-precision result of adding $r$ to $1$ is strictly greater than $1$. Your objective is to infer the smallest such $r > 0$ robustly, using only floating-point operations that themselves obey single-precision semantics.\n\nFundamental base to use:\n- IEEE 754 single-precision arithmetic uses base-2 with a fixed precision and rounds results to the nearest representable value, with ties resolved toward the even significand. You may assume this rounding rule, monotonicity of rounding, and the existence of subnormal numbers as well-tested facts.\n\nConstraints:\n- All additions, multiplications, and divisions intended to reflect single-precision behavior must be performed in single precision. Do not rely on analytical formulas for the answer. Instead, implement an algorithmic search based on the IEEE 754 rounding model to find the minimal $r > 0$ such that the single-precision computation of $1 + r$ is strictly greater than $1$.\n- Express interest rates as decimals (e.g., write $0.05$ for five percent). No units are required.\n\nRequired tasks to implement:\n1. Compute the smallest rate $r > 0$ such that the single-precision result of $1 + r$ is strictly greater than $1$. Use an iterative halving (bisection-style) approach driven entirely by single-precision operations: start from a single-precision $r$ and decrease $r$ until adding half of it no longer increases $1$ in single precision; the last $r$ that still increased $1$ when halved is the desired value.\n2. Verify the boundary condition by checking that with $r_{\\min}$ from step $1$, the single-precision result of $1 + \\frac{r_{\\min}}{2}$ equals $1$.\n3. Cross-check that the computed $r_{\\min}$ matches the single-precision spacing immediately above $1$ obtained from a trusted floating-point metadata source for single precision.\n4. Confirm that adding the smallest positive subnormal single-precision number $s$ to $1$ leaves the single-precision result equal to $1$.\n5. Applied finance check: for a single payoff of magnitude $C = 1$ received at time $T = 360$ (interpreted as months, but with no units required), compute in single precision the difference $\\frac{1}{(1 + r_{\\min})^{T}} - 1$ and return this single number. Report the value as a decimal number (not a percentage).\n\nTest suite and expected outputs:\n- The program must compute the following five outputs in order, aggregating them into a single list:\n  - Case A (happy path): the value of $r_{\\min}$ from task $1$ as a floating-point number.\n  - Case B (boundary condition): a boolean for whether $1 + \\frac{r_{\\min}}{2} = 1$ in single precision.\n  - Case C (consistency): a boolean for whether $r_{\\min}$ equals the single-precision metadata spacing above $1$.\n  - Case D (edge case): a boolean for whether $1 + s = 1$ in single precision for the smallest positive subnormal $s$.\n  - Case E (applied): the value of $\\frac{1}{(1 + r_{\\min})^{360}} - 1$ as a floating-point number, computed entirely in single precision.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_A,result\\_B,result\\_C,result\\_D,result\\_E]$). The list must contain exactly the five values described above, in that order. All floating-point arithmetic relevant to tasks $1$ through $5$ must be performed in single precision, and all comparisons must reflect single-precision results. All outputs must be reported as plain numbers or booleans, without units and without percent signs.",
            "solution": "The problem requires an investigation into the limitations of finite-precision arithmetic, specifically the IEEE 754 single-precision standard, in the context of financial calculations involving small interest rates. The core task is to determine the smallest positive interest rate $r$, denoted $r_{\\min}$, that is computationally distinguishable from $0$ when added to $1$. This is equivalent to finding the smallest positive single-precision number $r_{\\min}$ such that the floating-point operation $fl(1 + r_{\\min})$ yields a result strictly greater than $1$. We will then use this value to perform several verification checks and a simple financial calculation.\n\nIn the IEEE 754 standard for single-precision floating-point numbers (binary32), a number is represented using a sign bit, an 8-bit biased exponent, and a 23-bit fraction. A normalized number is of the form $(-1)^S \\times 2^{E-127} \\times (1.f)_{2}$, where $S$ is the sign bit, $E$ is the exponent, and $f$ is the fraction.\n\nThe number $1.0$ is represented exactly in this system. Its sign is $0$, its unbiased exponent is $0$ (so the biased exponent $E$ is $127$), and its fraction is all zeros. The significand is implicitly $1.0$. The next larger representable number has the same exponent but increments the last bit of the fraction. This number is $1 + 2^{-23}$. The gap between $1.0$ and the next representable number is therefore $2^{-23}$. This quantity is known as the Unit in the Last Place, or ulp(1.0). Any positive number $x$ such that $x  \\text{ulp}(1.0)/2 = 2^{-24}$ added to $1.0$ will be rounded back down to $1.0$ by the default round-to-nearest-ties-to-even rule. If $x = 2^{-24}$, a tie occurs, and the result is rounded to the \"even\" significand, which is that of $1.0$. Therefore, the smallest number $r_{\\min}$ such that $fl(1 + r_{\\min}) > 1$ must be $ulp(1.0) = 2^{-23}$.\n\nThe problem mandates an algorithmic discovery of this value, not its analytical derivation.\n\n**Task 1: Algorithmic Computation of $r_{\\min}$**\n\nWe are instructed to find $r_{\\min}$ using an iterative search. The search must find the smallest number $r$ such that $fl(1+r) > 1$. The method described is a power-of-two descent, which we implement by starting with a candidate $r = 1.0$ and repeatedly halving it. The logic is to find the value $r$ for which $fl(1 + r) > 1$ but $fl(1 + r/2) = 1$. This can be implemented with a loop that continues as long as adding half of the current candidate $r$ to $1$ still produces a result greater than $1$. All arithmetic must be performed in single precision.\n\nLet $r_0 = 1.0_{f32}$. We generate a sequence $r_{k+1} = r_k / 2.0_{f32}$. The loop continues as long as $fl(1.0_{f32} + fl(r_k / 2.0_{f32})) > 1.0_{f32}$. When the condition fails, the current $r_k$ is our desired $r_{\\min}$.\n\n**Task 2: Verification of the Boundary Condition**\n\nBy the construction in Task 1, $r_{\\min}$ is the smallest value in the sequence $1, 1/2, 1/4, \\dots$ for which $fl(1+r_{\\min})>1$. The algorithm terminates precisely when $fl(1 + r_{\\min}/2) = 1$. This check serves to confirm that our algorithm correctly identified the boundary of machine precision relative to $1$. We will compute $fl(1.0_{f32} + fl(r_{\\min} / 2.0_{f32}))$ and verify it equals $1.0_{f32}$.\n\n**Task 3: Cross-check with Floating-Point Metadata**\n\nThe value $r_{\\min}$ we have algorithmically determined should be identical to the fundamental machine constant known as machine epsilon for single precision, which is the distance from $1.0$ to the next larger representable floating-point number. Standard numerical libraries provide access to this value. We will compare our computed $r_{\\min}$ to the value of `numpy.spacing(numpy.float32(1.0))`, which is a trusted source for ulp(1.0). This confirms the correctness of our algorithm and understanding.\n\n**Task 4: Edge Case with Smallest Subnormal Number**\n\nSubnormal (or denormalized) numbers are used to represent values smaller than the smallest normalized number, filling the gap between $0$ and $\\pm 2^{E_{min}}$. The smallest positive single-precision subnormal number, let's call it $s$, is $2^{-149}$. This value is astronomically smaller than $r_{\\min} = 2^{-23}$. When we compute $fl(1.0 + s)$, the result is rounded to the nearest representable number. Since $s \\ll r_{\\min}/2$, the sum $1.0 + s$ is vastly closer to $1.0$ than to the next representable number, $1.0 + r_{\\min}$. Thus, the result of the floating-point addition must be $1.0$. This task verifies our understanding of how extremely small numbers are handled in floating-point addition.\n\n**Task 5: Applied Finance Check**\n\nThis task requires us to compute the effect of this minimal interest rate, $r_{\\min}$, over a long period. We must calculate the value of a financial expression, $D = \\frac{1}{(1 + r_{\\min})^T} - 1$, where the time period $T$ is $360$ (e.g., months in a $30$-year term). All operations must be performed in single precision. This demonstrates that even the smallest computationally detectable interest rate, when compounded, leads to a measurable deviation from a zero-rate scenario. The sequence of operations is:\n1.  Compute $v_1 = fl(1.0_{f32} + r_{\\min})$.\n2.  Compute $v_2 = fl(v_1^{360.0_{f32}})$. This is a repeated multiplication or a library power function, all in single precision.\n3.  Compute $v_3 = fl(1.0_{f32} / v_2)$.\n4.  Compute the final result $D = fl(v_3 - 1.0_{f32})$.\nThe result will be a small negative number, representing the present value discount factor's deviation from $1$ for a cash flow at time $T=360$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a series of tasks related to single-precision floating-point\n    arithmetic and its application in finance, adhering to strict computational\n    and formatting rules.\n    \"\"\"\n\n    # Define single-precision constants to ensure all arithmetic is performed\n    # in the correct domain.\n    one_f32 = np.float32(1.0)\n    two_f32 = np.float32(2.0)\n\n    # --- Task 1: Compute the smallest rate r_min  0 ---\n    # We are looking for the smallest r  0 such that 1 + r  1 in single precision.\n    # The problem specifies a bisection-style search. We start with r=1 and\n    # repeatedly halve it until adding r/2 to 1 no longer produces a result  1.\n    # At that point, the current r is the minimal value, r_min.\n    r = one_f32\n    while (one_f32 + (r / two_f32))  one_f32:\n        r = r / two_f32\n    r_min = r\n    result_A = r_min\n\n    # --- Task 2: Verify the boundary condition ---\n    # Check that adding half of r_min to 1 results in 1, confirming r_min is\n    # the minimal representable increment.\n    is_boundary_correct = (one_f32 + (r_min / two_f32)) == one_f32\n    result_B = is_boundary_correct\n\n    # --- Task 3: Cross-check with floating-point metadata ---\n    # Compare r_min with the value of ULP(1.0) (unit in the last place) for\n    # single precision, also known as machine epsilon relative to 1.\n    # np.spacing(1.0) gives the distance between 1.0 and the next larger float.\n    spacing_at_one = np.spacing(one_f32)\n    is_consistent = (r_min == spacing_at_one)\n    result_C = is_consistent\n\n    # --- Task 4: Confirm edge case with smallest subnormal number ---\n    # The smallest positive single-precision number is np.finfo.tiny.\n    # Adding this to 1 should result in 1 due to rounding.\n    s_subnormal = np.finfo(np.float32).tiny\n    is_subnormal_rounded = (one_f32 + s_subnormal) == one_f32\n    result_D = is_subnormal_rounded\n\n    # --- Task 5: Applied finance check ---\n    # Compute the discount factor deviation over 360 periods using r_min.\n    # All calculations must be performed in single precision.\n    T = np.float32(360.0)\n    one_plus_r_min = one_f32 + r_min\n    # The power operation must also respect single precision.\n    # numpy's ** operator on float32 types maintains float32 precision.\n    compounded_factor = one_plus_r_min ** T\n    inverse_factor = one_f32 / compounded_factor\n    finance_result = inverse_factor - one_f32\n    result_E = finance_result\n\n    # Aggregate results into a list for final output.\n    results = [\n        result_A,\n        result_B,\n        result_C,\n        result_D,\n        result_E\n    ]\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) converts each element, including booleans, to its\n    # string representation ('True', 'False', or the number as a string).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established the finite nature of machine precision, we now explore one of its most treacherous consequences: catastrophic cancellation. This exercise  demonstrates how the seemingly simple subtraction of two very close numbers, such as $F(b) - F(a)$ for a cumulative distribution function where $b \\approx a$, can lead to a dramatic and unacceptable loss of accuracy. By contrasting this naive approach with a more robust numerical integration, you will gain a critical intuition for identifying and avoiding operations that are mathematically sound but numerically unstable.",
            "id": "2394200",
            "problem": "You are to study numerical loss of significance when computing the area of a very thin interval under a probability density function, in the context of models that assume normally distributed returns. Let $f(x)$ be the standard normal probability density function given by $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$, and let $F(x)$ be its cumulative distribution function defined by $F(x) = \\int_{-\\infty}^{x} f(t)\\,dt$. For a given pair $(a,b)$ with $a  b$, the theoretical probability mass in the interval $(a,b]$ is both $F(b) - F(a)$ and $\\int_{a}^{b} f(x)\\,dx$.\n\nDefine the machine epsilon $\\varepsilon$ of the floating-point arithmetic used by your programming environment as the smallest positive real number such that $1 + \\varepsilon  1$ holds in that arithmetic.\n\nFor each of the following test cases (values of $(a,b)$ given below), compute:\n- The relative discrepancy $\\delta = \\frac{\\left| \\left(F(b) - F(a)\\right) - \\int_{a}^{b} f(x)\\,dx \\right|}{\\int_{a}^{b} f(x)\\,dx}$.\n- The ratio $\\rho = \\delta / \\varepsilon$.\n\nTest suite (all numbers are real values):\n- $(a,b) = (0.0, 0.1)$\n- $(a,b) = (0.0, 10^{-8})$\n- $(a,b) = (0.0, 10^{-12})$\n- $(a,b) = (5.0, 5.0 + 10^{-4})$\n- $(a,b) = (8.0, 8.0 + 10^{-6})$\n- $(a,b) = (-12.0, -12.0 + 10^{-3})$\n\nYour program must produce a single line of output containing all results aggregated into one list, in the following order: for each test case in the sequence above, output first the $\\delta$ value and then the $\\rho$ value, yielding a flat list of length $12$. The format must be a comma-separated list enclosed in square brackets, for example, $[\\delta_1,\\rho_1,\\delta_2,\\rho_2,\\dots,\\delta_6,\\rho_6]$. All outputs must be real numbers; no units are involved, and no angle units are required. Percentages, if any arise conceptually, must be expressed as decimals.",
            "solution": "The problem as stated is valid. It is a well-posed exercise in numerical analysis, grounded in the established principles of probability theory and floating-point computation. The objective is to demonstrate and quantify the phenomenon of numerical loss of significance, specifically catastrophic cancellation, which is a critical consideration in scientific and financial computing.\n\nLet us begin with the theoretical foundation. The problem defines two quantities for the probability mass over an interval $(a, b]$ under the standard normal distribution. The first is derived from the cumulative distribution function (CDF), $F(x)$, as the difference $P_1 = F(b) - F(a)$. The second is the direct definite integral of the probability density function (PDF), $f(x)$, over the interval: $P_2 = \\int_{a}^{b} f(x)\\,dx$. By the Fundamental Theorem of Calculus, since $F'(x) = f(x)$, these two quantities are analytically identical. That is, $F(b) - F(a) = \\int_{a}^{b} f(x)\\,dx$.\n\nHowever, in the world of finite-precision arithmetic, analytical equivalence does not guarantee computational equivalence. The core of the problem lies in the numerical evaluation of $P_1$ when the interval is thin, i.e., when $b$ is very close to $a$. In such cases, $F(b)$ will be very close to $F(a)$. Standard floating-point numbers, such as the IEEE 754 double-precision format used in Python, store a value as a sign, a mantissa (significand), and an exponent. The mantissa holds a fixed number of significant digits. When two numbers of nearly equal magnitude are subtracted, the leading digits of their mantissas cancel out. For example, if we have $x = 1.23456789$ and $y = 1.23456700$, the difference $x-y = 0.00000089$. The result has far fewer significant digits than the original operands. This loss of relative precision is known as catastrophic cancellation. The computation of $F(b) - F(a)$ is susceptible to this error when $b \\approx a$.\n\nTo quantify this error, we require a high-accuracy reference value. The direct numerical integration of the PDF, $P_2$, serves this purpose. High-quality numerical quadrature algorithms, such as those implemented in `scipy.integrate.quad`, are designed to be numerically stable. They operate by summing the area of many small slices of the interval $[a, b]$. This process avoids the direct subtraction of two large, nearly equal numbers and thus provides a result that is typically accurate to near machine precision. We will therefore treat the value from numerical integration as our ground truth.\n\nThe problem asks for two metrics. The first is the relative discrepancy, defined as\n$$\n\\delta = \\frac{\\left| \\left(F(b) - F(a)\\right) - \\int_{a}^{b} f(x)\\,dx \\right|}{\\int_{a}^{b} f(x)\\,dx}\n$$\nThis measures the relative error of the subtraction method against our more accurate integration-based reference. The second metric is the ratio $\\rho = \\delta / \\varepsilon$, where $\\varepsilon$ is the machine epsilon. Machine epsilon is the smallest number such that $1 + \\varepsilon  1$ in floating-point arithmetic; it represents the fundamental limit of relative precision for number representation. The ratio $\\rho$ thus normalizes the observed error, telling us how many times worse our computational error is compared to this fundamental limit. A value of $\\rho \\approx 1$ indicates a numerically stable computation, while a large $\\rho$ signifies severe loss of precision.\n\nThe analysis of the test cases reveals the nuances of this effect:\n1.  For intervals near the mean of the distribution (e.g., $(0.0, 10^{-8})$), $F(a)$ and $F(b)$ are near $0.5$. Subtracting them produces catastrophic cancellation.\n2.  For intervals in the far-right tail (e.g., $(8.0, 8.0 + 10^{-6})$), $F(a)$ and $F(b)$ are both extremely close to $1$. Again, subtracting them leads to catastrophic cancellation and a very large $\\rho$.\n3.  Interestingly, for intervals in the far-left tail (e.g., $(-12.0, -12.0 + 10^{-3})$), $F(a)$ and $F(b)$ are both very small positive numbers, close to $0$. The subtraction of two small numbers does not suffer from the same catastrophic cancellation issue, as their relative precision is preserved. In this case, we expect $\\rho$ to be small, close to $1$.\n\nThe algorithm to be implemented is as follows:\n1.  Obtain the machine epsilon $\\varepsilon$ for double-precision floating-point numbers. In Python/NumPy, this is `np.finfo(float).eps`.\n2.  For each test case pair $(a, b)$:\n    a. Compute $V_{\\text{sub}} = F(b) - F(a)$ using `scipy.stats.norm.cdf`.\n    b. Compute the reference value $V_{\\text{int}} = \\int_{a}^{b} f(x)\\,dx$ using `scipy.integrate.quad` with `scipy.stats.norm.pdf`.\n    c. Calculate $\\delta = |V_{\\text{sub}} - V_{\\text{int}}| / V_{\\text{int}}$.\n    d. Calculate $\\rho = \\delta / \\varepsilon$.\n3.  Aggregate all computed $\\delta$ and $\\rho$ values into a single list and format as required.\nThis procedure will be encoded into the provided Python script.",
            "answer": "```python\nimport numpy as np\nimport scipy.stats\nimport scipy.integrate\n\ndef solve():\n    \"\"\"\n    Computes numerical discrepancy and error ratios for calculating probability\n    mass in a thin interval under the standard normal distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.1),\n        (0.0, 1e-8),\n        (0.0, 1e-12),\n        (5.0, 5.0 + 1e-4),\n        (8.0, 8.0 + 1e-6),\n        (-12.0, -12.0 + 1e-3),\n    ]\n\n    # Get the machine epsilon for standard double-precision floats.\n    machine_epsilon = np.finfo(float).eps\n\n    results = []\n    for a, b in test_cases:\n        # Method 1: Subtraction of CDF values.\n        # This method is prone to catastrophic cancellation when a is close to b.\n        val_subtraction = scipy.stats.norm.cdf(b) - scipy.stats.norm.cdf(a)\n\n        # Method 2: Numerical integration of the PDF.\n        # This is used as the high-accuracy reference \"true\" value.\n        # The function to integrate is the standard normal PDF.\n        pdf_func = scipy.stats.norm.pdf\n        val_integral, _ = scipy.integrate.quad(pdf_func, a, b)\n\n        # The relative discrepancy, delta, measures the relative error of the\n        # subtraction method compared to the more accurate integration method.\n        # A check for val_integral == 0.0 is prudent, though not expected for\n        # these non-zero width intervals where the pdf is positive.\n        if val_integral == 0.0:\n            # If the true integral is zero, the discrepancy is zero only if\n            # the subtraction method also yields zero. Otherwise, it's infinite.\n            delta = 0.0 if val_subtraction == 0.0 else np.inf\n        else:\n            delta = np.abs(val_subtraction - val_integral) / val_integral\n\n        # The ratio, rho, normalizes the discrepancy by the machine epsilon,\n        # quantifying the severity of the numerical error in a standard way.\n        rho = delta / machine_epsilon\n\n        results.extend([delta, rho])\n\n    # Final print statement must produce a single line with the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In our final practice, we synthesize these lessons to tackle a realistic challenge in computational finance: the robust valuation of a floating-rate bond. Naively implementing the standard pricing formula can cause the financial impact of a small interest rate margin to vanish entirely due to rounding errors, a phenomenon explored in this exercise . You will implement both the naive formula and a cleverly rearranged, algebraically equivalent version to see how a stable algorithm preserves accuracy, demonstrating a powerful strategy for building reliable financial software.",
            "id": "2394271",
            "problem": "You are asked to write a complete, runnable program that values a floating-rate bond under risk-neutral valuation while exposing the impact of floating-point precision and machine epsilon on the computation. Work from first principles of discounted cash flow under the risk-neutral measure: the present value is the sum over coupon dates of the discounted expected cash flows plus the discounted principal repayment. Use the following fundamental base.\n1. Present value under the risk-neutral measure: the present value equals the sum of cash flows multiplied by corresponding discount factors.\n2. A floating-rate coupon in period $i$ equals $(L_i + m)\\,\\alpha_i\\,N$, where $L_i$ is the given forward rate for period $i$, $m$ is a constant margin added to each reset, $\\alpha_i$ is the accrual fraction of the period, and $N$ is notional.\n3. When discount factors are constructed from simple-compounding forward rates $L_i$ and accruals $\\alpha_i$, they satisfy the recursion $D_0 = 1$ and $D_i = \\dfrac{D_{i-1}}{1 + L_i \\alpha_i}$ for $i = 1,\\dots,T$, where $D_i$ is the discount factor to time $i$ and $T$ is the total number of periods.\n4. The present value is then $PV = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)$, where $D_i$ are obtained from the above recursion and $D_T$ discounts the redemption of principal $N$.\n\nYour task is to implement two numerically distinct computations of the present value:\n- A naive evaluation that directly uses $(L_i + m)$ inside each coupon computation.\n- An algebraically equivalent evaluation that avoids adding a very small number to a much larger number by rearranging terms using identities valid in real arithmetic; in particular, compute the contribution of the margin without first adding it to $L_i$. You must adhere to the same discount factors $D_i$ for both methods.\n\nYour program must execute the following test suite, each test case being fully specified by the notional $N$, the margin $m$, the list or rule for forward rates $\\{L_i\\}_{i=1}^T$, and the list of accrual fractions $\\{\\alpha_i\\}_{i=1}^T$. The discount factors $D_i$ used for valuation in all cases must be derived from the given $L_i$ and $\\alpha_i$ via the recursion $D_0 = 1$, $D_i = \\dfrac{D_{i-1}}{1 + L_i \\alpha_i}$ for $i = 1,\\dots,T$.\n\nTest Case A (happy path, non-negligible margin):\n- $N = 1{,}000{,}000$.\n- $T = 8$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates: $L = [0.021,\\, 0.022,\\, 0.0235,\\, 0.024,\\, 0.025,\\, 0.026,\\, 0.027,\\, 0.028]$.\n- Margin: $m = 0.0005$.\n\nTest Case B (margin far below the rounding unit near typical rates; illustrates loss due to floating-point precision):\n- $N = 1{,}000{,}000{,}000$.\n- $T = 8$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates: use the same list as in Test Case A.\n- Margin: $m = 1 \\times 10^{-20}$.\n\nTest Case C (boundary: zero margin):\n- $N = 2{,}000{,}000$.\n- $T = 8$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates: $L = [0.018,\\, 0.019,\\, 0.0195,\\, 0.020,\\, 0.0205,\\, 0.021,\\, 0.0215,\\, 0.022]$.\n- Margin: $m = 0$.\n\nTest Case D (many periods with small, gradually changing forward rates; tiny but not vanishing margin):\n- $N = 100{,}000{,}000$.\n- $T = 40$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates given by the rule $L_i = 0.015 + 0.0001\\,i$ for $i = 1,\\dots,40$.\n- Margin: $m = 1 \\times 10^{-12}$.\n\nYour program must, for each test case, compute:\n- The naive present value $PV_{\\text{naive}} = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)$.\n- An alternative present value $PV_{\\text{stable}}$ computed without adding $m$ to $L_i$ inside the coupon term, while respecting algebraic equivalence in exact arithmetic and using the same $D_i$.\n- The absolute difference $\\lvert PV_{\\text{naive}} - PV_{\\text{stable}} \\rvert$.\n\nThere are no physical units in this problem. All rates must be treated as decimals, not as percentages. Angles are not involved. The final output format must be a single line containing a comma-separated list enclosed in square brackets, concatenating for the four test cases the triplets in order: $[PV_{\\text{naive}}^{(A)}, PV_{\\text{stable}}^{(A)}, \\lvert \\cdot \\rvert^{(A)}, PV_{\\text{naive}}^{(B)}, PV_{\\text{stable}}^{(B)}, \\lvert \\cdot \\rvert^{(B)}, PV_{\\text{naive}}^{(C)}, PV_{\\text{stable}}^{(C)}, \\lvert \\cdot \\rvert^{(C)}, PV_{\\text{naive}}^{(D)}, PV_{\\text{stable}}^{(D)}, \\lvert \\cdot \\rvert^{(D)}]$.",
            "solution": "The problem is subjected to validation.\n\nStep 1: Extract Givens.\n- The present value ($PV$) under the risk-neutral measure is the sum of discounted cash flows.\n- A floating-rate coupon in period $i$ is given by the formula $C_i = (L_i + m)\\,\\alpha_i\\,N$, where $L_i$ is the forward rate, $m$ is the margin, $\\alpha_i$ is the accrual fraction, and $N$ is the notional.\n- Discount factors $D_i$ are derived from the recursion $D_0 = 1$ and $D_i = \\dfrac{D_{i-1}}{1 + L_i \\alpha_i}$ for $i = 1,\\dots,T$.\n- The present value using naive evaluation is $PV_{\\text{naive}} = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)$.\n- The task is to implement this naive method and an algebraically equivalent, but numerically more stable, method ($PV_{\\text{stable}}$) which avoids the direct addition of $m$ to $L_i$ within the sum.\n- The program must compute $PV_{\\text{naive}}$, $PV_{\\text{stable}}$, and the absolute difference $\\lvert PV_{\\text{naive}} - PV_{\\text{stable}} \\rvert$ for four test cases.\n- Test Case A: $N = 1,000,000$, $T = 8$, $\\alpha_i = 0.25$ for all $i$, $L = [0.021, 0.022, 0.0235, 0.024, 0.025, 0.026, 0.027, 0.028]$, $m = 0.0005$.\n- Test Case B: $N = 1,000,000,000$, $T = 8$, $\\alpha_i = 0.25$ for all $i$, $L$ is the same as in Test Case A, $m = 1 \\times 10^{-20}$.\n- Test Case C: $N = 2,000,000$, $T = 8$, $\\alpha_i = 0.25$ for all $i$, $L = [0.018, 0.019, 0.0195, 0.020, 0.0205, 0.021, 0.0215, 0.022]$, $m = 0$.\n- Test Case D: $N = 100,000,000$, $T = 40$, $\\alpha_i = 0.25$ for all $i$, $L_i = 0.015 + 0.0001 \\cdot i$ for $i = 1,\\dots,40$, $m = 1 \\times 10^{-12}$.\n- The final output format must be a single-line string representation of a list containing the concatenated results.\n\nStep 2: Validate Using Extracted Givens.\n- The problem is scientifically grounded in the principles of financial mathematics and computational science. The valuation of a floating-rate note and the analysis of floating-point arithmetic errors are standard topics.\n- The problem is well-posed. All parameters and formulas are explicitly given for each test case, leading to a unique, computable solution.\n- The problem is objective, stated with clear, unambiguous technical language.\n- The setup is complete and consistent, with no missing information or contradictions.\n- The parameters, while chosen to illustrate numerical effects (e.g., $m = 1 \\times 10^{-20}$), are valid for a computational problem and do not represent a physical impossibility.\n- The structure is sound and requires derivation of a numerically stable formula, which is a standard technique in numerical analysis.\n\nStep 3: Verdict and Action.\nThe problem is valid. A complete solution will be developed.\n\nThe core of the problem is the comparison of two computational methods for the present value of a floating-rate bond.\n\nMethod 1: Naive Evaluation\nThe first method is a direct implementation of the provided formula:\n$$\nPV_{\\text{naive}} = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)\n$$\nHere, for each coupon period $i$, the margin $m$ is added to the forward rate $L_i$ before multiplication. When $m$ is significantly smaller than $L_i$, this addition can lead to a loss of floating-point precision. For example, in standard double-precision arithmetic (IEEE $754$), which has approximately $15$â€“$17$ decimal digits of precision, if $L_i \\approx 10^{-2}$ and $m = 10^{-20}$, the sum $L_i + m$ will be computationally indistinguishable from $L_i$. This phenomenon is known as absorption or catastrophic cancellation, where the smaller number is effectively lost.\n\nMethod 2: Stable Evaluation\nTo construct a more stable method, we must rearrange the formula to avoid the direct addition of $m$ to $L_i$. We begin with the same expression for present value and apply algebraic manipulation, which is exact in real arithmetic.\n$$\nPV = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)\n$$\nWe distribute the terms within the summation:\n$$\nPV = N \\left( \\sum_{i=1}^{T} L_i\\alpha_i D_i + \\sum_{i=1}^{T} m\\alpha_i D_i + D_T \\right)\n$$\nThis rearrangement separates the calculation involving $L_i$ from the contribution of the margin $m$. To further simplify and enhance numerical stability, we analyze the term $\\sum_{i=1}^{T} L_i\\alpha_i D_i$. The discount factors are defined by the recursion $D_i = \\frac{D_{i-1}}{1 + L_i \\alpha_i}$. Rearranging this gives $D_i(1 + L_i \\alpha_i) = D_{i-1}$, which leads to $D_i + L_i \\alpha_i D_i = D_{i-1}$. This provides the critical identity:\n$$\nL_i \\alpha_i D_i = D_{i-1} - D_i\n$$\nSubstituting this identity into the summation results in a telescoping series:\n$$\n\\sum_{i=1}^{T} L_i \\alpha_i D_i = \\sum_{i=1}^{T} (D_{i-1} - D_i) = (D_0 - D_1) + (D_1 - D_2) + \\dots + (D_{T-1} - D_T) = D_0 - D_T\n$$\nBy definition, $D_0 = 1$. Therefore, the sum simplifies to:\n$$\n\\sum_{i=1}^{T} L_i \\alpha_i D_i = 1 - D_T\n$$\nNow, we substitute this result back into the expanded expression for $PV$:\n$$\nPV = N \\left( (1 - D_T) + m \\sum_{i=1}^{T} \\alpha_i D_i + D_T \\right)\n$$\nThe $D_T$ terms cancel, yielding the final, numerically stable formula:\n$$\nPV_{\\text{stable}} = N \\left( 1 + m \\sum_{i=1}^{T} \\alpha_i D_i \\right)\n$$\nThis formula is superior because it completely avoids the addition of $L_i$ and $m$. The total contribution of the margin is calculated separately by first summing the discounted accrual fractions and then multiplying by $m$. This preserves the precision of the minuscule margin term, ensuring its contribution to the final price is not lost due to floating-point arithmetic limitations.\n\nThe implementation will proceed as follows for each test case:\n1.  Establish the parameters $N$, $m$, $T$, and the arrays for $L_i$ and $\\alpha_i$.\n2.  Compute the array of discount factors $D_i$ for $i=0, \\dots, T$ using the specified recursion.\n3.  Calculate $PV_{\\text{naive}}$ using the first formula.\n4.  Calculate $PV_{\\text{stable}}$ using the derived second formula.\n5.  Compute the absolute difference between the two results.\n\nThis procedure will be executed for all four test cases, and the results will be formatted into the required output string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the floating-rate bond valuation problem for all test cases\n    and prints the results in the specified format.\n    \"\"\"\n\n    def generate_test_cases():\n        \"\"\"\n        Generates and returns the test cases as a list of dictionaries.\n        \"\"\"\n        # Test Case A\n        case_a = {\n            \"N\": 1_000_000.0,\n            \"T\": 8,\n            \"alpha\": np.full(8, 0.25),\n            \"L\": np.array([0.021, 0.022, 0.0235, 0.024, 0.025, 0.026, 0.027, 0.028]),\n            \"m\": 0.0005\n        }\n\n        # Test Case B\n        case_b = {\n            \"N\": 1_000_000_000.0,\n            \"T\": 8,\n            \"alpha\": np.full(8, 0.25),\n            \"L\": np.array([0.021, 0.022, 0.0235, 0.024, 0.025, 0.026, 0.027, 0.028]),\n            \"m\": 1e-20\n        }\n\n        # Test Case C\n        case_c = {\n            \"N\": 2_000_000.0,\n            \"T\": 8,\n            \"alpha\": np.full(8, 0.25),\n            \"L\": np.array([0.018, 0.019, 0.0195, 0.020, 0.0205, 0.021, 0.0215, 0.022]),\n            \"m\": 0.0\n        }\n\n        # Test Case D\n        T_d = 40\n        L_d = np.array([0.015 + 0.0001 * (i + 1) for i in range(T_d)])\n        case_d = {\n            \"N\": 100_000_000.0,\n            \"T\": T_d,\n            \"alpha\": np.full(T_d, 0.25),\n            \"L\": L_d,\n            \"m\": 1e-12\n        }\n\n        return [case_a, case_b, case_c, case_d]\n\n    def calculate_pvs(N, m, L, alpha):\n        \"\"\"\n        Calculates the present value of a floating-rate bond using two different methods.\n\n        Args:\n            N (float): Notional amount.\n            m (float): Margin.\n            L (np.ndarray): Array of forward rates.\n            alpha (np.ndarray): Array of accrual fractions.\n\n        Returns:\n            tuple: A tuple containing (pv_naive, pv_stable, abs_diff).\n        \"\"\"\n        T = len(L)\n        \n        # Calculate discount factors D_i for i=0,...,T\n        # D_0 = 1, D_i = D_{i-1} / (1 + L_i * alpha_i)\n        D = np.empty(T + 1, dtype=np.float64)\n        D[0] = 1.0\n        for i in range(1, T + 1):\n            D[i] = D[i - 1] / (1.0 + L[i - 1] * alpha[i - 1])\n        \n        # We need discount factors D_1, ..., D_T for the sums\n        D_coupon_periods = D[1:]\n\n        # Method 1: Naive PV calculation\n        # PV_naive = N * ( sum_{i=1 to T} (L_i + m) * alpha_i * D_i + D_T )\n        coupon_sum_naive = np.sum((L + m) * alpha * D_coupon_periods)\n        pv_naive = N * (coupon_sum_naive + D[T])\n\n        # Method 2: Stable PV calculation\n        # PV_stable = N * ( 1 + m * sum_{i=1 to T} alpha_i * D_i )\n        margin_sum_stable = np.sum(alpha * D_coupon_periods)\n        pv_stable = N * (1.0 + m * margin_sum_stable)\n\n        # Absolute difference\n        abs_diff = np.abs(pv_naive - pv_stable)\n\n        return pv_naive, pv_stable, abs_diff\n\n    test_cases = generate_test_cases()\n    results = []\n    \n    for case in test_cases:\n        pv_naive, pv_stable, abs_diff = calculate_pvs(\n            N=case[\"N\"], \n            m=case[\"m\"], \n            L=case[\"L\"], \n            alpha=case[\"alpha\"]\n        )\n        results.extend([pv_naive, pv_stable, abs_diff])\n\n    # Format the final output as a comma-separated list in a single line.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}