{
    "hands_on_practices": [
        {
            "introduction": "The most direct way to observe the performance implications of NUMA is through serialized memory access patterns, where the latency of each access directly adds to the total execution time. This exercise uses a simple linked-list traversal, a classic example of pointer-chasing, to demonstrate the stark difference between local and remote memory latencies. By calculating the average access time for a pathologically poor memory layout and comparing it to an optimized one, you will gain a quantitative understanding of the critical importance of data locality .",
            "id": "3686974",
            "problem": "A two-socket Non-Uniform Memory Access (NUMA) system has sockets $\\mathrm{S0}$ and $\\mathrm{S1}$, each with its own local dynamic random-access memory (DRAM). A single-threaded program is pinned to run on $\\mathrm{S0}$. Consider traversing a singly linked list whose nodes are $64$ bytes each and aligned on $64$-byte cache lines, so that following the $\\texttt{next}$ pointer of each node causes one last-level-cache miss and a DRAM access for the next node. The working set is much larger than cache, and hardware prefetchers cannot anticipate pointer-chasing, so memory-level parallelism is effectively $1$ (serialized misses).\n\nThe following latencies are well characterized on this machine:\n- Local DRAM miss latency when the thread on $\\mathrm{S0}$ accesses memory allocated on $\\mathrm{S0}$: $L_{\\mathrm{local}} = 90$ nanoseconds.\n- Remote DRAM miss latency when the thread on $\\mathrm{S0}$ accesses memory allocated on $\\mathrm{S1}$ over the inter-socket interconnect, including coherence and directory overheads: $L_{\\mathrm{remote}} = 180$ nanoseconds.\n\nUniform Memory Access (UMA) refers to architectures in which the memory access latency is independent of the location of the memory with respect to the processor. In contrast, in NUMA architectures, the access latency depends on whether the memory is local or remote.\n\nConstruct the following pathologically bad NUMA layout: node $i$ of the linked list is allocated on socket $\\mathrm{S0}$ if $i$ is odd, and on socket $\\mathrm{S1}$ if $i$ is even, for $i = 1, 2, \\dots, N$, with the head node on $\\mathrm{S0}$. Under this layout, half of the traversed nodes are local and half are remote on average.\n\nStarting from the core definitions of UMA and NUMA and the observation that serialized pointer-chasing makes the traversal time equal to the sum of individual access latencies, derive an expression for the average per-node memory access latency $L_{\\mathrm{avg,bad}}$ under the pathological layout. Then, propose a redesign that avoids the pathological alternation by using first-touch placement to allocate all list nodes on $\\mathrm{S0}$, yielding an average per-node latency $L_{\\mathrm{avg,good}}$.\n\nFinally, compute the multiplicative speedup $S$ of the redesigned layout over the pathological layout, defined as\n$$\nS = \\frac{L_{\\mathrm{avg,bad}}}{L_{\\mathrm{avg,good}}}.\n$$\nProvide $S$ as a single real number rounded to four significant figures. No units are required in the final answer; all intermediate latencies should be expressed in nanoseconds in your derivation. Assume $N$ is large enough that start-up and tail effects are negligible, and that the list is read-only during traversal (no writes), so write-induced coherence traffic can be ignored. Express the final speedup as a dimensionless number rounded to four significant figures.",
            "solution": "The user wants me to analyze the performance of a linked list traversal on a Non-Uniform Memory Access (NUMA) system under two different memory allocation layouts and compute the speedup of the optimized layout over the pathological one.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- System: Two-socket NUMA system with sockets $\\mathrm{S0}$ and $\\mathrm{S1}$.\n- Program: Single-threaded, pinned to run on $\\mathrm{S0}$.\n- Data Structure: Singly linked list.\n- Node size: $64$ bytes, aligned on $64$-byte cache lines.\n- Traversal characteristics: Pointer-chasing causes one last-level-cache miss and one DRAM access per node. The working set is large, and hardware prefetchers are ineffective. Memory-level parallelism is $1$.\n- Latencies:\n  - Local DRAM miss latency: $L_{\\mathrm{local}} = 90$ nanoseconds.\n  - Remote DRAM miss latency: $L_{\\mathrm{remote}} = 180$ nanoseconds.\n- Pathological layout: For a list of $N$ nodes indexed $i = 1, 2, \\dots, N$, node $i$ is on socket $\\mathrm{S0}$ if $i$ is odd, and on socket $\\mathrm{S1}$ if $i$ is even. The head node ($i=1$) is on $\\mathrm{S0}$.\n- Redesigned layout: All list nodes are allocated on $\\mathrm{S0}$ using first-touch placement.\n- Definition of speedup: $S = \\frac{L_{\\mathrm{avg,bad}}}{L_{\\mathrm{avg,good}}}$.\n- Assumptions: $N$ is large enough to ignore start-up and tail effects. The list is read-only during traversal.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded in the principles of computer architecture, specifically memory hierarchies and multi-socket processor systems. The concepts of NUMA, UMA, cache misses, pointer-chasing, memory-level parallelism, and memory placement policies like first-touch are standard and well-defined. The latency values provided ($L_{\\mathrm{local}} = 90 \\, \\text{ns}$, $L_{\\mathrm{remote}} = 180 \\, \\text{ns}$) are realistic for modern server hardware, representing a NUMA factor of $2$. The problem is well-posed, objective, and self-contained, providing all necessary information and assumptions to derive a unique solution. The described \"pathological\" scenario is a classic example used to illustrate the performance pitfalls of NUMA-unaware memory allocation. The problem does not violate any fundamental principles, is not ambiguous, and is directly solvable using the provided model.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation and solution.\n\nThe core of the problem lies in calculating the average memory access time for traversing a single node in two different scenarios. The problem states that the linked list traversal is serialized due to pointer-chasing, meaning the access to node $i+1$ cannot begin until the access to node $i$ completes. Therefore, the total traversal time for $N$ nodes is the sum of the individual memory access latencies for each node. The average per-node latency is this total time divided by $N$.\n\nLet's first analyze the pathological NUMA layout. The thread executes on socket $\\mathrm{S0}$. The memory allocation for the linked list nodes alternates between the two sockets:\n- Node $i=1$ is on $\\mathrm{S0}$ (local access). Latency is $L_{\\mathrm{local}}$.\n- Node $i=2$ is on $\\mathrm{S1}$ (remote access). Latency is $L_{\\mathrm{remote}}$.\n- Node $i=3$ is on $\\mathrm{S0}$ (local access). Latency is $L_{\\mathrm{local}}$.\n- Node $i=4$ is on $\\mathrm{S1}$ (remote access). Latency is $L_{\\mathrm{remote}}$.\n- ...and so on.\n\nThe sequence of access latencies is $L_{\\mathrm{local}}, L_{\\mathrm{remote}}, L_{\\mathrm{local}}, L_{\\mathrm{remote}}, \\dots$.\nSince the number of nodes $N$ is assumed to be large, we can neglect any startup or tail effects. Over a large number of accesses, the number of local accesses will be approximately equal to the number of remote accesses. Specifically, for a large even $N$, there are exactly $\\frac{N}{2}$ local accesses and $\\frac{N}{2}$ remote accesses. For a large odd $N$, there are $\\frac{N+1}{2}$ local and $\\frac{N-1}{2}$ remote accesses. In the limit as $N \\to \\infty$, both types of accesses constitute $50\\%$ of the total.\n\nThe total time to traverse $N$ nodes in the pathological layout, $T_{\\mathrm{bad}}$, is:\n$$T_{\\mathrm{bad}} \\approx \\frac{N}{2} \\cdot L_{\\mathrm{local}} + \\frac{N}{2} \\cdot L_{\\mathrm{remote}}$$\nThe average per-node memory access latency, $L_{\\mathrm{avg,bad}}$, is $T_{\\mathrm{bad}}$ divided by $N$:\n$$L_{\\mathrm{avg,bad}} = \\frac{T_{\\mathrm{bad}}}{N} = \\frac{\\frac{N}{2} (L_{\\mathrm{local}} + L_{\\mathrm{remote}})}{N} = \\frac{1}{2} (L_{\\mathrm{local}} + L_{\\mathrm{remote}})$$\nSubstituting the given values, $L_{\\mathrm{local}} = 90 \\, \\text{ns}$ and $L_{\\mathrm{remote}} = 180 \\, \\text{ns}$:\n$$L_{\\mathrm{avg,bad}} = \\frac{1}{2} (90 \\, \\text{ns} + 180 \\, \\text{ns}) = \\frac{270}{2} \\, \\text{ns} = 135 \\, \\text{ns}$$\n\nNext, we analyze the redesigned, NUMA-aware layout. This layout uses a \"first-touch\" placement policy. In such a policy, the operating system allocates a memory page on the NUMA node where it is first accessed (written to or read from). Since the single-threaded program responsible for creating and traversing the list is pinned to socket $\\mathrm{S0}$, a first-touch policy will ensure that all memory pages for the linked list nodes are allocated on the local memory of $\\mathrm{S0}$.\n\nConsequently, every access to a list node by the thread on $\\mathrm{S0}$ will be a local memory access.\n- Node $i=1$ is on $\\mathrm{S0}$ (local access). Latency is $L_{\\mathrm{local}}$.\n- Node $i=2$ is on $\\mathrm{S0}$ (local access). Latency is $L_{\\mathrm{local}}$.\n- ...and so on for all $N$ nodes.\n\nThe total time to traverse $N$ nodes in this good layout, $T_{\\mathrm{good}}$, is:\n$$T_{\\mathrm{good}} = N \\cdot L_{\\mathrm{local}}$$\nThe average per-node memory access latency, $L_{\\mathrm{avg,good}}$, is $T_{\\mathrm{good}}$ divided by $N$:\n$$L_{\\mathrm{avg,good}} = \\frac{T_{\\mathrm{good}}}{N} = \\frac{N \\cdot L_{\\mathrm{local}}}{N} = L_{\\mathrm{local}}$$\nSubstituting the given value:\n$$L_{\\mathrm{avg,good}} = 90 \\, \\text{ns}$$\n\nFinally, we compute the multiplicative speedup $S$, which is the ratio of the average latency of the pathological layout to that of the redesigned layout.\n$$S = \\frac{L_{\\mathrm{avg,bad}}}{L_{\\mathrm{avg,good}}}$$\nSubstituting the derived expressions and values:\n$$S = \\frac{\\frac{1}{2} (L_{\\mathrm{local}} + L_{\\mathrm{remote}})}{L_{\\mathrm{local}}} = \\frac{L_{\\mathrm{local}} + L_{\\mathrm{remote}}}{2 \\cdot L_{\\mathrm{local}}}$$\n$$S = \\frac{135 \\, \\text{ns}}{90 \\, \\text{ns}} = \\frac{135}{90} = \\frac{3 \\times 45}{2 \\times 45} = \\frac{3}{2} = 1.5$$\nThe problem requires the answer to be a real number rounded to four significant figures.\n$$S = 1.500$$\nThe speedup of $1.500$ indicates that the NUMA-aware layout, which ensures all memory accesses are local, makes the linked list traversal $50\\%$ faster than the pathological layout, which forces half of the accesses to be remote.",
            "answer": "$$\\boxed{1.500}$$"
        },
        {
            "introduction": "Beyond single-threaded latency, NUMA performance is deeply intertwined with the cache coherence protocol that manages data consistency across sockets. This practice explores the subtle but significant issue of \"false sharing,\" where independent data items happen to reside on the same cache line, leading to costly, unnecessary invalidation traffic between nodes. By comparing the Array-of-Structs (AoS) and Struct-of-Arrays (SoA) data layouts, you will learn to quantify this effect by counting remote invalidation messages, revealing how thoughtful data organization can eliminate hidden performance bottlenecks .",
            "id": "3687062",
            "problem": "Consider a two-socket Non-Uniform Memory Access (NUMA) system. Each socket has private caches, and the system enforces a directory-based cache coherence protocol consistent with Modified, Exclusive, Shared, Invalid (MESI). Assume write-invalidate semantics: when a core on one socket performs a store to a cache line that is currently present in the other socket’s cache, the coherence protocol issues exactly one invalidation to that other socket for each ownership transfer. The cache line size is $L = 64$ bytes. All arrays are $64$-byte aligned at their starts.\n\nA program executes a steady-state iterative kernel over an array of $N = 2^{20}$ logical elements. Two data layout variants are considered:\n\n1. Array-of-Structs (AoS): Each element is a struct of size $s = 32$ bytes, consisting of two independent fields, $x$ and $y$, each occupying $8$ bytes, plus padding to reach $32$ bytes total. The array $A$ of $N$ elements is contiguous in memory.\n\n2. Struct-of-Arrays (SoA): Two separate arrays $X$ and $Y$, each of length $N$, with element size $8$ bytes. Pages of $X$ are first touched exclusively by threads running on socket $0$, and pages of $Y$ are first touched exclusively by threads running on socket $1$.\n\nWork assignment per iteration is as follows. Threads on socket $0$ update field $x$ for all even indices $i \\in \\{0, 2, 4, \\dots\\}$, performing a single store to $x$ for each such $i$. Threads on socket $1$ update field $y$ for all odd indices $i \\in \\{1, 3, 5, \\dots\\}$, performing a single store to $y$ for each such $i$. No thread reads or writes any other field or index during the iteration. Consider the steady-state regime where, for cache lines that are touched by both sockets in the AoS layout, those lines reside in both sockets’ caches at the start of each iteration due to the previous iteration’s accesses.\n\nStarting from the above definitions of Non-Uniform Memory Access (NUMA), cache lines, and MESI write-invalidate semantics, derive the exact number of cross-socket (remote) invalidation messages generated per iteration for the AoS layout and for the SoA layout. Express your final answer as a two-component row vector $\\left(I_{\\mathrm{AoS}}, I_{\\mathrm{SoA}}\\right)$, where each component is the number of remote invalidations per iteration. No rounding is required.",
            "solution": "The problem requires the derivation of the number of cross-socket invalidation messages per iteration for two data layouts, Array-of-Structs (AoS) and Struct-of-Arrays (SoA), on a two-socket NUMA system with a MESI-based write-invalidate cache coherence protocol.\n\nFirst, let us establish the key parameters provided:\n- Cache line size: $L = 64$ bytes.\n- Number of logical elements: $N = 2^{20}$.\n- AoS struct size: $s = 32$ bytes.\n- SoA element size: $8$ bytes.\n\nThe coherence protocol generates exactly one remote invalidation message for each ownership transfer of a cache line that is present in another socket's cache.\n\n**Analysis for the Array-of-Structs (AoS) Layout**\n\nIn the AoS layout, the data is stored in a single contiguous array $A$ of $N$ structs. Each struct has a size of $s = 32$ bytes. The cache line size is $L = 64$ bytes. Therefore, the number of structs that fit into a single cache line is:\n$$ \\frac{L}{s} = \\frac{64 \\text{ bytes}}{32 \\text{ bytes}} = 2 $$\nSince the array is $64$-byte aligned, the $k$-th cache line of the array (where $k$ is a zero-based index) will contain the structs $A[2k]$ and $A[2k+1]$.\n\nThe work assignment is as follows:\n- Threads on socket $0$ perform a store to field $x$ of $A[i]$ for all even indices $i$. This corresponds to accessing $A[0], A[2], A[4], \\dots$.\n- Threads on socket $1$ perform a store to field $y$ of $A[i]$ for all odd indices $i$. This corresponds to accessing $A[1], A[3], A[5], \\dots$.\n\nLet us analyze the accesses to the $k$-th cache line, which contains $A[2k]$ and $A[2k+1]$.\n- A thread on socket $0$ will write to a field within $A[2k]$.\n- A thread on socket $1$ will write to a field within $A[2k+1]$.\nSince both writes target the same cache line, this scenario is known as false sharing.\n\nThe problem states that in the steady state, these conflicted cache lines start each iteration resident in the caches of both sockets. In the MESI protocol, this means the line is in the **Shared (S)** state in both the cache of socket $0$ and the cache of socket $1$.\n\nWe now trace the invalidation events for one such cache line during one iteration:\n1.  **First Store (e.g., from Socket 0):** A thread on socket $0$ performs a store to $A[2k]$.x. The line is in state S. To write, the cache on socket $0$ must gain exclusive ownership. It sends a request to upgrade its state to **Modified (M)**. This action requires invalidating all other shared copies. Consequently, the coherence protocol sends one invalidation message to socket $1$. Socket $1$'s copy of the line transitions to the **Invalid (I)** state. At this point, one remote invalidation has occurred. The line is in state M on socket $0$ and I on socket $1$.\n2.  **Second Store (from Socket 1):** A thread on socket $1$ performs a store to $A[2k+1]$.y. Its local copy is in state I, so this results in a write miss. The cache on socket $1$ sends a request for the cache line with exclusive ownership. The directory forwards this request to socket $0$, which currently holds the line in state M. Socket $0$ must relinquish ownership. This constitutes an ownership transfer. As the line is \"currently present in the other socket's cache\" (socket $0$'s cache), the protocol issues an invalidation to socket $0$. Socket $0$'s copy transitions to state I. At this point, a second remote invalidation has occurred. The line is now in state I on socket $0$ and M on socket $1$.\n\nThus, for each cache line that contains data touched by both sockets, exactly two cross-socket invalidations are generated per iteration.\n\nNow, we must determine how many cache lines exhibit this false sharing. The array $A$ consists of $N = 2^{20}$ structs, each of size $s = 32$ bytes. The total size of the array is $N \\times s = 2^{20} \\times 32$ bytes. The number of cache lines spanned by the array is:\n$$ \\text{Number of lines} = \\frac{N \\times s}{L} = \\frac{2^{20} \\times 32}{64} = \\frac{2^{20}}{2} = 2^{19} $$\nEvery cache line contains an even-indexed struct (written by socket $0$) and an odd-indexed struct (written by socket $1$), so all $2^{19}$ lines are subject to false sharing.\n\nThe total number of invalidations for the AoS layout, $I_{\\mathrm{AoS}}$, is the number of conflicted lines multiplied by the number of invalidations per line:\n$$ I_{\\mathrm{AoS}} = (\\text{Number of lines}) \\times (\\text{Invalidations per line}) = 2^{19} \\times 2 = 2^{20} $$\n\n**Analysis for the Struct-of-Arrays (SoA) Layout**\n\nIn the SoA layout, the data is organized into two separate arrays, $X$ and $Y$. Array $X$ contains all the $x$ fields, and array $Y$ contains all the $y$ fields.\n\nThe work assignment is:\n- Threads on socket $0$ write to $X[i]$ for even indices $i$.\n- Threads on socket $1$ write to $Y[i]$ for odd indices $i$.\n\nCritically, threads on socket $0$ only ever access array $X$, and threads on socket $1$ only ever access array $Y$. There is no overlap in the arrays accessed by the two sockets.\n\n- **Accesses to Array X:** Since only socket $0$ threads access array $X$, cache lines belonging to $X$ will only be loaded into the caches of socket $0$. They will never be present in the cache of socket $1$. Therefore, when a thread on socket $0$ performs a store to a line from $X$, there is no copy on socket $1$ to invalidate. No cross-socket invalidation message is generated.\n- **Accesses to Array Y:** Similarly, since only socket $1$ threads access array $Y$, cache lines belonging to $Y$ will only be loaded into the caches of socket $1$. They will never be present in the cache of socket $0$. When a thread on socket $1$ performs a store, there is no copy on socket $0$ to invalidate. Again, no cross-socket invalidation message is generated.\n\nThe first-touch page placement policy, which allocates pages of $X$ to socket $0$'s local memory and pages of $Y$ to socket $1$'s local memory, reduces memory access latency but does not change the fundamental cache coherence behavior. The decisive factor is the strict separation of data access, which completely eliminates data sharing between sockets at the cache line level.\n\nTherefore, the total number of cross-socket invalidations for the SoA layout, $I_{\\mathrm{SoA}}$, is zero.\n$$ I_{\\mathrm{SoA}} = 0 $$\n\nThe final answer is the two-component row vector $(I_{\\mathrm{AoS}}, I_{\\mathrm{SoA}})$.\n$$ (I_{\\mathrm{AoS}}, I_{\\mathrm{SoA}}) = (2^{20}, 0) $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2^{20} & 0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}