{
    "hands_on_practices": [
        {
            "introduction": "To unlock instruction-level parallelism, a processor must first distinguish between true data dependencies and superficial name dependencies. This exercise provides a foundational understanding of how register renaming eliminates these \"false\" dependencies (WAR and WAW hazards) that arise from the limited number of architectural registers. By comparing the execution time of a code block on a machine with and without this capability, you will directly quantify the dramatic increase in ILP that register renaming enables, a core principle behind the performance of modern out-of-order CPUs .",
            "id": "3651255",
            "problem": "You are analyzing instruction-level parallelism (ILP) in a basic block on two otherwise identical out-of-order (OoO) processors. Machine $\\mathcal{M}_0$ has no register renaming and therefore must respect all name dependences (write-after-write and write-after-read), while machine $\\mathcal{M}_1$ has perfect register renaming that removes all name dependences while preserving true data dependences (read-after-write). Assume an idealized pipeline with unbounded issue width, unbounded functional units, perfect branch prediction, and no cache or structural stalls. Each arithmetic instruction has latency $1$ cycle. Instruction-level parallelism (ILP) is defined as the average number of instructions completed per cycle under these assumptions for the basic block.\n\nConsider the following basic block of $8$ integer additions in the given static program order, where $R_x$ denotes architectural registers and each line is one instruction:\n- $I_1$: $R_1 \\leftarrow R_2 + R_3$\n- $I_2$: $R_4 \\leftarrow R_1 + R_5$\n- $I_3$: $R_4 \\leftarrow R_4 + R_7$\n- $I_4$: $R_8 \\leftarrow R_4 + R_9$\n- $I_5$: $R_1 \\leftarrow R_{10} + R_{11}$\n- $I_6$: $R_4 \\leftarrow R_1 + R_{13}$\n- $I_7$: $R_4 \\leftarrow R_4 + R_{15}$\n- $I_8$: $R_{16} \\leftarrow R_4 + R_{17}$\n\nTasks:\n- Using only the core definitions of dependence types in dynamic scheduling, identify which inter-instruction edges in this block are true data dependences (read-after-write, RAW) and which are false name dependences (write-after-read, WAR; write-after-write, WAW) on $\\mathcal{M}_0$.\n- Based on these dependences, determine the minimum completion time, measured in cycles, to execute the block on $\\mathcal{M}_0$ and on $\\mathcal{M}_1$. For $\\mathcal{M}_1$, assume perfect register renaming so that all WAR and WAW constraints are removed by assigning distinct physical registers to logically independent results while preserving all RAW dependences.\n- Let $ILP_0$ and $ILP_1$ denote the instruction-level parallelism on $\\mathcal{M}_0$ and $\\mathcal{M}_1$, respectively, computed as $ILP = \\dfrac{N}{T}$ where $N$ is the number of instructions in the block and $T$ is the minimum number of cycles to complete the block under the stated model.\n\nWhat is the numerical increase $\\Delta ILP = ILP_1 - ILP_0$? Express your answer as an exact reduced fraction. No rounding is required and no units are needed.",
            "solution": "The problem requires an analysis of instruction-level parallelism (ILP) for a given basic block on two different out-of-order processors, $\\mathcal{M}_0$ and $\\mathcal{M}_1$. The ILP is defined as $ILP = \\frac{N}{T}$, where $N$ is the number of instructions and $T$ is the minimum execution time in cycles. The block contains $N=8$ instructions, and each instruction has a latency of 1 cycle. The result of an instruction starting in cycle $c$ is available for a dependent instruction at the beginning of cycle $c+1$.\n\nFirst, we must identify all inter-instruction dependences for the processor without register renaming, $\\mathcal{M}_0$. These are categorized as true data dependences (Read-After-Write, RAW), anti-dependences (Write-After-Read, WAR), and output dependences (Write-After-Write, WAW).\n\nThe instruction sequence is:\n$I_1: R_1 \\leftarrow R_2 + R_3$\n$I_2: R_4 \\leftarrow R_1 + R_5$\n$I_3: R_4 \\leftarrow R_4 + R_7$\n$I_4: R_8 \\leftarrow R_4 + R_9$\n$I_5: R_1 \\leftarrow R_{10} + R_{11}$\n$I_6: R_4 \\leftarrow R_1 + R_{13}$\n$I_7: R_4 \\leftarrow R_4 + R_{15}$\n$I_8: R_{16} \\leftarrow R_4 + R_{17}$\n\n**1. Dependence Analysis for Machine $\\mathcal{M}_0$ (No Register Renaming)**\n\nWe identify all pairs of instructions $(I_i, I_j)$ with $j > i$ that have a dependence.\n\n- **True Data Dependences (RAW):** $I_i$ writes a register that $I_j$ reads.\n  - $(I_1, I_2)$ on $R_1$: $I_1$ writes $R_1$, which $I_2$ reads.\n  - $(I_2, I_3)$ on $R_4$: $I_2$ writes $R_4$, which $I_3$ reads.\n  - $(I_3, I_4)$ on $R_4$: $I_3$ writes $R_4$, which $I_4$ reads.\n  - $(I_5, I_6)$ on $R_1$: $I_5$ writes $R_1$, which $I_6$ reads.\n  - $(I_6, I_7)$ on $R_4$: $I_6$ writes $R_4$, which $I_7$ reads.\n  - $(I_7, I_8)$ on $R_4$: $I_7$ writes $R_4$, which $I_8$ reads.\n\n- **Anti-Dependences (WAR):** $I_i$ reads a register that $I_j$ writes.\n  - $(I_2, I_5)$ on $R_1$: $I_2$ reads $R_1$, and $I_5$ writes $R_1$.\n  - $(I_3, I_6)$ on $R_4$: $I_3$ reads $R_4$, and $I_6$ writes $R_4$.\n  - $(I_3, I_7)$ on $R_4$: $I_3$ reads $R_4$, and $I_7$ writes $R_4$.\n  - $(I_4, I_6)$ on $R_4$: $I_4$ reads $R_4$, and $I_6$ writes $R_4$.\n  - $(I_4, I_7)$ on $R_4$: $I_4$ reads $R_4$, and $I_7$ writes $R_4$.\n\n- **Output Dependences (WAW):** $I_i$ and $I_j$ both write to the same register. The processor must ensure the final value in the register corresponds to the last write in program order.\n  - $(I_1, I_5)$ on $R_1$.\n  - On $R_4$, instructions $I_2, I_3, I_6, I_7$ all write. This creates the following WAW dependences: $(I_2, I_3)$, $(I_2, I_6)$, $(I_2, I_7)$, $(I_3, I_6)$, $(I_3, I_7)$, and $(I_6, I_7)$. Note that some of these pairs also have RAW dependences (e.g., $(I_2,I_3)$), which are a stronger constraint.\n\n**2. Execution Time on Machine $\\mathcal{M}_0$**\n\nThe minimum execution time is determined by the critical path in the full dependence graph. An edge from $I_i$ to $I_j$ means $I_j$ can start at or after the cycle in which $I_i$ completes. Let $S(I_k)$ be the start cycle of instruction $I_k$ (starting from $S=0$) and $C(I_k) = S(I_k) + 1$ be its completion cycle.\n\n- $S(I_1) = 0$. $C(I_1) = 1$.\n- $S(I_2) \\geq C(I_1)$ (RAW). $\\implies S(I_2) = 1, C(I_2) = 2$.\n- $S(I_3) \\geq C(I_2)$ (RAW). $\\implies S(I_3) = 2, C(I_3) = 3$.\n- $S(I_4) \\geq C(I_3)$ (RAW). $\\implies S(I_4) = 3, C(I_4) = 4$.\n- $S(I_5) \\geq C(I_2)$ (WAR on $R_1$). Also $S(I_5) \\geq C(I_1)$ (WAW on $R_1$). Since $C(I_2) > C(I_1)$, the WAR dependence is the binding constraint. $\\implies S(I_5) = 2, C(I_5) = 3$.\n- $S(I_6)$ depends on multiple predecessors:\n    - $S(I_6) \\geq C(I_5)$ (RAW on $R_1$). So $S(I_6) \\geq 3$.\n    - $S(I_6) \\geq C(I_4)$ (WAR on $R_4$). So $S(I_6) \\geq 4$.\n    - $S(I_6) \\geq C(I_3)$ (WAW and WAR on $R_4$). So $S(I_6) \\geq 3$.\n    The most restrictive constraint is from $I_4$, so $S(I_6) \\geq \\max(3, 4, 3) = 4$. $\\implies S(I_6) = 4, C(I_6) = 5$.\n- $S(I_7) \\geq C(I_6)$ (RAW). $\\implies S(I_7) = 5, C(I_7) = 6$.\n- $S(I_8) \\geq C(I_7)$ (RAW). $\\implies S(I_8) = 6, C(I_8) = 7$.\n\nThe last instruction, $I_8$, completes at cycle $7$. The total time to execute the block on $\\mathcal{M}_0$ is $T_0 = 7$ cycles.\nThe ILP is $ILP_0 = \\frac{N}{T_0} = \\frac{8}{7}$.\n\n**3. Execution Time on Machine $\\mathcal{M}_1$ (Perfect Register Renaming)**\n\nOn machine $\\mathcal{M}_1$, perfect register renaming eliminates all WAR and WAW dependences. Only RAW dependences remain.\nThe RAW dependences partition the instructions into two independent chains:\n- Chain A: $I_1 \\rightarrow_{RAW} I_2 \\rightarrow_{RAW} I_3 \\rightarrow_{RAW} I_4$.\n- Chain B: $I_5 \\rightarrow_{RAW} I_6 \\rightarrow_{RAW} I_7 \\rightarrow_{RAW} I_8$.\n\nWith unbounded resources, these two chains execute in parallel. The total execution time is determined by the length of the longer chain.\n- The length of Chain A is the sum of latencies of its $4$ instructions, which is $1+1+1+1 = 4$ cycles.\n- The length of Chain B is the sum of latencies of its $4$ instructions, which is $1+1+1+1 = 4$ cycles.\n\nBoth chains are of equal length. The execution proceeds as follows:\n- Cycle 1: $I_1$ and $I_5$ execute.\n- Cycle 2: $I_2$ and $I_6$ execute.\n- Cycle 3: $I_3$ and $I_7$ execute.\n- Cycle 4: $I_4$ and $I_8$ execute.\n\nAll instructions complete at the end of cycle $4$. The total execution time is $T_1 = 4$ cycles.\nThe ILP is $ILP_1 = \\frac{N}{T_1} = \\frac{8}{4} = 2$.\n\n**4. Calculation of $\\Delta ILP$**\n\nThe numerical increase in ILP is $\\Delta ILP = ILP_1 - ILP_0$.\n$\\Delta ILP = 2 - \\frac{8}{7}$\nTo subtract, we find a common denominator:\n$\\Delta ILP = \\frac{14}{7} - \\frac{8}{7} = \\frac{14 - 8}{7} = \\frac{6}{7}$.\nThe result is an exact reduced fraction as required.",
            "answer": "$$\\boxed{\\frac{6}{7}}$$"
        },
        {
            "introduction": "Even with perfect register renaming to resolve name hazards, a processor's performance is ultimately capped by its physical resources. This practice shifts the focus from data dependencies to structural hazards, challenging you to identify the performance bottleneck in a system with a fixed number of functional units. By analyzing a trace of independent instructions, you will learn to calculate the maximum achievable ILP by determining which resource—the issue width, the ALUs, or the multiplier—becomes saturated first .",
            "id": "3651236",
            "problem": "Consider an out-of-order superscalar central processing unit (CPU) with dual-issue capability. The machine has the following characteristics:\n- It can issue up to $2$ instructions per cycle into two parallel issue slots, subject to data and structural hazards.\n- It has two identical integer arithmetic logic unit (ALU) pipelines that can each execute addition or subtraction with latency $1$ cycle and are fully pipelined, allowing up to $2$ additions per cycle if both issue slots are available.\n- It has a single shared multiplier pipeline that is fully pipelined with latency $4$ cycles, but due to sharing, the machine can initiate at most $1$ multiply instruction per cycle across both issue slots.\n- It supports perfect register renaming and dynamic scheduling with an effectively unbounded instruction window, no branch mispredictions, and memory with zero-latency hits, so only the stated structural constraints limit throughput.\n\nA program trace contains $N=300$ arithmetic instructions: $X=180$ multiplies and $A=120$ adds. The instructions are independent, except that each multiply and add reads only already-available operands and produces a distinct destination register. Ignore startup and drain transients; assume steady-state execution once the machine is warmed up.\n\nUsing first-principles definitions of Instruction-Level Parallelism (ILP) and instruction throughput governed by resource constraints, derive the minimal number of cycles required to complete the trace under the stated machine model and compute the average Instruction-Level Parallelism (ILP), defined as the number of retired instructions per cycle. Express the final ILP as a simplified fraction with no units. Do not round; provide the exact value as a reduced rational number.",
            "solution": "The problem asks for the minimal number of cycles to execute a given instruction trace and the resulting average Instruction-Level Parallelism (ILP). The analysis must be based on the resource constraints of the specified out-of-order superscalar processor.\n\nThe execution time of the program trace is determined by the most heavily utilized resource, as the processor's out-of-order capabilities with an unbounded instruction window will ensure that instructions are executed as soon as their required resources are available. Since all data dependencies are removed by the problem's premises, the only limiting factors are the structural hazards, i.e., the finite number of functional units and issue slots.\n\nLet $C$ be the minimal number of cycles required to complete the trace. We can establish a lower bound for $C$ based on each of the key resources.\n\n1.  **Issue Width Constraint**: The processor can issue at most $2$ instructions per cycle. With a total of $N = 300$ instructions, the minimum number of cycles required to issue all of them is:\n    $$C_{issue} \\ge \\frac{N}{\\text{Issue Width}} = \\frac{300}{2} = 150 \\text{ cycles}$$\n\n2.  **Integer ALU Constraint**: The processor has $2$ fully pipelined ALUs, capable of executing a total of $2$ additions per cycle. There are $A = 120$ add instructions. The minimum time to execute all additions, considering only this resource, is:\n    $$C_{alu} \\ge \\frac{A}{\\text{ALU Throughput}} = \\frac{120}{2} = 60 \\text{ cycles}$$\n\n3.  **Multiplier Constraint**: The processor has a single multiplier unit that, while pipelined, can only initiate one new multiplication per cycle. There are $X = 180$ multiply instructions. Thus, the minimum number of cycles required to initiate all multiplications is:\n    $$C_{mult} \\ge \\frac{X}{\\text{Multiplier Throughput}} = \\frac{180}{1} = 180 \\text{ cycles}$$\n\nThe total time $C$ to execute the entire trace must be at least the maximum of these individual lower bounds, because all instruction types must be completed.\n$$C \\ge \\max(C_{issue}, C_{alu}, C_{mult})$$\n$$C \\ge \\max(150, 60, 180) = 180 \\text{ cycles}$$\n\nThe bottleneck resource is the multiplier's initiation port. The minimal possible execution time is therefore at least $180$ cycles. We must confirm if an execution schedule of $180$ cycles is feasible.\n\nLet's construct a schedule for $C = 180$ cycles:\n- In each cycle from $1$ to $180$, we must issue one multiply instruction to meet the $180$-cycle target. This uses one of the two available issue slots and the single multiplier initiation port.\n- This leaves one issue slot available in each of these $180$ cycles.\n- We have $A = 120$ add instructions to schedule. These can be issued in the available second issue slot during the first $120$ cycles.\n- For cycles $i=1, \\dots, 120$:\n    - Issue Slot 1: Issue a multiply instruction.\n    - Issue Slot 2: Issue an add instruction.\n    This is feasible because the machine is dual-issue and has two ALUs, so one add and one multiply can be initiated concurrently.\n- For cycles $i=121, \\dots, 180$:\n    - All $120$ adds have been issued.\n    - Issue Slot 1: Issue a multiply instruction.\n    - Issue Slot 2: Remains idle.\n\nThis schedule successfully issues all $180$ multiplies and all $120$ adds within $180$ cycles without violating any resource constraints. Therefore, the minimal number of cycles is indeed $C = 180$.\n\nThe instruction latencies ($1$ for add, $4$ for multiply) do not affect the total execution time because the problem specifies that all instructions are independent. An out-of-order machine with perfect renaming and a large instruction window does not need to wait for an instruction to complete before issuing subsequent, independent instructions. The throughput (initiation rate) of the functional units is the limiting factor, not their latency.\n\nNow, we can compute the average Instruction-Level Parallelism (ILP), defined as the total number of instructions retired per cycle.\n$$\\text{ILP} = \\frac{\\text{Total Instructions}}{\\text{Total Cycles}} = \\frac{N}{C}$$\nSubstituting the values:\n$$\\text{ILP} = \\frac{300}{180}$$\nThis fraction can be simplified:\n$$\\text{ILP} = \\frac{30 \\times 10}{18 \\times 10} = \\frac{30}{18} = \\frac{5 \\times 6}{3 \\times 6} = \\frac{5}{3}$$\nThe average ILP is $\\frac{5}{3}$ instructions per cycle.",
            "answer": "$$\n\\boxed{\\frac{5}{3}}\n$$"
        },
        {
            "introduction": "This final practice synthesizes the concepts of data dependency and resource constraints into a comprehensive scheduling challenge. You are tasked with finding the fastest execution schedule for a set of instructions with both true data dependencies and specific hardware requirements, namely a limited number of integer and floating-point units with varying latencies. This problem simulates the complex task faced by a real processor's scheduler, forcing you to balance critical path constraints with functional unit availability to maximize performance .",
            "id": "3651272",
            "problem": "A superscalar processor exploits Instruction-Level Parallelism (ILP) by issuing multiple independent operations in the same clock cycle, subject to functional unit availability and data-dependence constraints. Consider a single basic block to be scheduled on a processor with the following properties:\n- There are $2$ identical Integer Arithmetic Logic Units (ALU) and $1$ Floating-Point (FP) unit.\n- All units are fully pipelined such that each unit can accept at most one new operation per cycle.\n- Integer ALU operations have latency $1$ cycle.\n- Floating-point addition has latency $3$ cycles, and floating-point multiplication has latency $4$ cycles.\n- Operands produced by an operation become available exactly $d$ cycles after issue, where $d$ is the operation’s latency. A dependent consumer may be issued in the same cycle in which its last operand becomes available.\n- There are no memory operations, no control dependencies, and registers are fully renamed, eliminating false dependencies (no write-after-read or write-after-write hazards). Only true data dependencies apply.\n\nThe basic block contains the following operations. Each line lists the operation label, the operation type, and its explicit true dependencies:\n- $I_1$: integer add, depends on no prior operation.\n- $I_2$: integer add, depends on no prior operation.\n- $I_3$: integer add, depends on $I_1$.\n- $I_4$: integer add, depends on $I_1$ and $I_2$.\n- $I_5$: integer add, depends on $I_3$ and $I_4$.\n- $F_1$: floating-point multiply, depends on no prior operation.\n- $F_2$: floating-point add, depends on no prior operation.\n- $F_3$: floating-point add, depends on $F_1$ and $F_2$.\n- $F_4$: floating-point multiply, depends on $F_3$.\n- $F_5$: floating-point add, depends on $F_4$ and $F_1$.\n\nAssume out-of-order issue is allowed subject to the stated constraints, and integer operations must be assigned to Integer ALUs while floating-point operations must be assigned to the FP unit. Starting from cycle $0$, determine the minimal makespan $M$, defined as the earliest cycle by which all operations in the basic block have completed (all results ready), under an optimal schedule that maximizes ILP subject to the given constraints. Express your answer as a single integer number of cycles with no units.",
            "solution": "The problem requires determining the minimal makespan for a set of instructions to be executed on a superscalar processor with specific functional units and latency characteristics. The makespan, $M$, is defined as the earliest cycle in which all operations have completed, meaning their results are available. The scheduling must respect both true data dependencies and resource constraints.\n\nFirst, let us formalize the problem. We are given a set of operations and a directed acyclic graph (DAG) representing their data dependencies. For each operation $O$, we have its type (integer or floating-point), its latency $d(O)$, and the set of operations it depends on, $Dependencies(O)$.\n\nThe processor has the following resources:\n- $2$ Integer Arithmetic Logic Units (ALUs).\n- $1$ Floating-Point (FP) unit.\n\nThe operation latencies are given as:\n- Integer addition ($d_{int}$): $1$ cycle.\n- Floating-point addition ($d_{fp\\_add}$): $3$ cycles.\n- Floating-point multiplication ($d_{fp\\_mult}$): $4$ cycles.\n\nLet $Issue(O)$ be the clock cycle in which operation $O$ is issued. The result of operation $O$ becomes available at cycle $Ready(O)$, which is defined as:\n$$Ready(O) = Issue(O) + d(O)$$\n\nThe constraints on the schedule are:\n1.  **Data Dependency Constraint**: An operation $O$ cannot be issued until the results of all operations it depends on are available.\n    $$Issue(O) \\ge \\max_{P \\in Dependencies(O)} \\{Ready(P)\\}$$\n    If an operation has no dependencies, it can be issued starting from cycle $0$.\n\n2.  **Resource Constraint**: In any given cycle $c$, the number of issued integer operations cannot exceed $2$, and the number of issued floating-point operations cannot exceed $1$.\n\nThe goal is to find an optimal schedule that minimizes the makespan $M$, where:\n$$M = \\max_{O} \\{Ready(O)\\}$$\n\nA lower bound on the makespan is the length of the critical path in the data dependency graph. The critical path is the longest path from any initial operation (no dependencies) to any final operation, where the path length is the sum of the latencies of all operations on that path. Let's calculate the length of the longest data dependency chain, which represents the earliest completion time for each instruction assuming infinite resources. This is also called the critical path analysis.\n\nLet $C(O)$ be the earliest possible completion time (ready time) for an operation $O$ based solely on data dependencies.\nIf $O$ has no dependencies, $C(O) = d(O)$, assuming issue at cycle $0$.\nOtherwise, $C(O) = \\left( \\max_{P \\in Dependencies(O)} \\{C(P)\\} \\right) + d(O)$.\n\nLet's compute these values for all instructions.\nFor integer operations (all have $d_{int} = 1$):\n- $Dependencies(I_1) = \\emptyset \\implies C(I_1) = 0 + d(I_1) = 1$.\n- $Dependencies(I_2) = \\emptyset \\implies C(I_2) = 0 + d(I_2) = 1$.\n- $Dependencies(I_3) = \\{I_1\\} \\implies C(I_3) = C(I_1) + d(I_3) = 1 + 1 = 2$.\n- $Dependencies(I_4) = \\{I_1, I_2\\} \\implies C(I_4) = \\max(C(I_1), C(I_2)) + d(I_4) = \\max(1, 1) + 1 = 2$.\n- $Dependencies(I_5) = \\{I_3, I_4\\} \\implies C(I_5) = \\max(C(I_3), C(I_4)) + d(I_5) = \\max(2, 2) + 1 = 3$.\n\nFor floating-point operations:\n- $Dependencies(F_1) = \\emptyset \\implies C(F_1) = 0 + d(F_1) = 0 + 4 = 4$.\n- $Dependencies(F_2) = \\emptyset \\implies C(F_2) = 0 + d(F_2) = 0 + 3 = 3$.\n- $Dependencies(F_3) = \\{F_1, F_2\\} \\implies C(F_3) = \\max(C(F_1), C(F_2)) + d(F_3) = \\max(4, 3) + 3 = 4 + 3 = 7$.\n- $Dependencies(F_4) = \\{F_3\\} \\implies C(F_4) = C(F_3) + d(F_4) = 7 + 4 = 11$.\n- $Dependencies(F_5) = \\{F_4, F_1\\} \\implies C(F_5) = \\max(C(F_4), C(F_1)) + d(F_5) = \\max(11, 4) + 3 = 11 + 3 = 14$.\n\nThe maximum of these earliest completion times is $C(F_5) = 14$. This establishes a lower bound for the makespan: $M \\ge 14$. Now, we must determine if a schedule exists that achieves this makespan while respecting resource constraints. We will construct a schedule using a greedy list-scheduling algorithm, prioritizing operations that are ready to be issued.\n\nLet's track the schedule cycle by cycle, starting from cycle $0$.\n\n**Cycle 0:**\n- Ready to issue: $\\{I_1, I_2, F_1, F_2\\}$ (no dependencies).\n- Resources: $2$ ALUs, $1$ FP unit.\n- Schedule: Issue $I_1$ (ALU1), $I_2$ (ALU2), $F_1$ (FP unit). $F_2$ must wait.\n- Completion times for issued ops:\n  - $Ready(I_1) = Issue(I_1) + d(I_1) = 0 + 1 = 1$.\n  - $Ready(I_2) = Issue(I_2) + d(I_2) = 0 + 1 = 1$.\n  - $Ready(F_1) = Issue(F_1) + d(F_1) = 0 + 4 = 4$.\n\n**Cycle 1:**\n- Ready to issue: $\\{I_3, I_4, F_2\\}$.\n  - $I_3$ is ready because its dependency $I_1$ is ready at cycle $1$.\n  - $I_4$ is ready because its dependencies $I_1, I_2$ are ready at cycle $1$.\n  - $F_2$ was ready in cycle $0$ but was not issued due to resource contention.\n- Resources: $2$ ALUs, $1$ FP unit.\n- Schedule: Issue $I_3$ (ALU1), $I_4$ (ALU2), $F_2$ (FP unit).\n- Completion times for issued ops:\n  - $Ready(I_3) = Issue(I_3) + d(I_3) = 1 + 1 = 2$.\n  - $Ready(I_4) = Issue(I_4) + d(I_4) = 1 + 1 = 2$.\n  - $Ready(F_2) = Issue(F_2) + d(F_2) = 1 + 3 = 4$.\n\n**Cycle 2:**\n- Ready to issue: $\\{I_5\\}$.\n  - $I_5$ is ready because its dependencies $I_3, I_4$ are ready at cycle $2$.\n- Resources: $2$ ALUs, $1$ FP unit.\n- Schedule: Issue $I_5$ (ALU1).\n- Completion time for issued op:\n  - $Ready(I_5) = Issue(I_5) + d(I_5) = 2 + 1 = 3$.\n\n**Cycle 3:**\n- Ready to issue: $\\emptyset$. No instructions have all their data dependencies met.\n  - $F_3$ needs $F_1$ (ready at cycle $4$) and $F_2$ (ready at cycle $4$).\n\n**Cycle 4:**\n- Ready to issue: $\\{F_3\\}$.\n  - $F_3$ is ready because $\\max(Ready(F_1), Ready(F_2)) = \\max(4, 4) = 4$.\n- Resources: $1$ FP unit.\n- Schedule: Issue $F_3$ (FP unit).\n- Completion time for issued op:\n  - $Ready(F_3) = Issue(F_3) + d(F_3) = 4 + 3 = 7$.\n\n**Cycles 5, 6:**\n- Ready to issue: $\\emptyset$.\n  - $F_4$ needs $F_3$, which will be ready at cycle $7$.\n\n**Cycle 7:**\n- Ready to issue: $\\{F_4\\}$.\n  - $F_4$ is ready because $Ready(F_3) = 7$.\n- Resources: $1$ FP unit.\n- Schedule: Issue $F_4$ (FP unit).\n- Completion time for issued op:\n  - $Ready(F_4) = Issue(F_4) + d(F_4) = 7 + 4 = 11$.\n\n**Cycles 8, 9, 10:**\n- Ready to issue: $\\emptyset$.\n  - $F_5$ needs $F_4$ (ready at cycle $11$) and $F_1$ (ready at cycle $4$).\n\n**Cycle 11:**\n- Ready to issue: $\\{F_5\\}$.\n  - $F_5$ is ready because $\\max(Ready(F_4), Ready(F_1)) = \\max(11, 4) = 11$.\n- Resources: $1$ FP unit.\n- Schedule: Issue $F_5$ (FP unit).\n- Completion time for issued op:\n  - $Ready(F_5) = Issue(F_5) + d(F_5) = 11 + 3 = 14$.\n\nAll operations have now been scheduled. We can determine the overall makespan by finding the maximum ready time among all operations.\nThe ready times are:\n- $Ready(I_1) = 1$\n- $Ready(I_2) = 1$\n- $Ready(I_3) = 2$\n- $Ready(I_4) = 2$\n- $Ready(I_5) = 3$\n- $Ready(F_1) = 4$\n- $Ready(F_2) = 4$\n- $Ready(F_3) = 7$\n- $Ready(F_4) = 11$\n- $Ready(F_5) = 14$\n\nThe makespan is $M = \\max(1, 1, 2, 2, 3, 4, 4, 7, 11, 14) = 14$.\n\nThis schedule satisfies all resource and data dependency constraints. Since the achieved makespan of $14$ cycles is equal to the lower bound determined by the critical path analysis, the schedule is optimal and the minimal makespan is $14$ cycles.",
            "answer": "$$\n\\boxed{14}\n$$"
        }
    ]
}