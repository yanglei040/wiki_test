{
    "hands_on_practices": [
        {
            "introduction": "This first practice bridges theory and reality by asking you to think like a performance engineer. To solve a problem like false sharing, you first need to reliably detect it. This exercise challenges you to design a scientifically sound experiment using performance counters, the hardware tools that provide insight into a processor's inner workings. By contrasting a workload designed to induce false sharing with a control case, you will learn to identify the specific signatures of this performance issue, such as spikes in Read-For-Ownership ($\\mathrm{L1\\_RFO}$) requests and Hit-on-Modified ($\\mathrm{HITM}$) events. ",
            "id": "3684650",
            "problem": "Consider a shared-memory multiprocessor with $k$ symmetric Central Processing Unit (CPU) cores, each with a private Level One (L1) write-back cache and a shared last-level cache. The machine implements the Modified-Exclusive-Shared-Invalid (MESI) cache coherence protocol. Two threads, denoted $T_1$ and $T_2$, run concurrently on different cores. The false sharing phenomenon is defined as simultaneous accesses by multiple threads to distinct memory locations that reside within the same cache line, causing coherence traffic even though no actual data dependency exists.\n\nFundamental base: Under a snooping MESI protocol, a store by a core to an address not currently in the Exclusive or Modified state in its cache triggers a Read For Ownership, i.e., Level One Read For Ownership ($\\mathrm{L1\\_RFO}$), to obtain the line in an exclusive state. If another core holds the line in Modified state, the request observes a Hit on Modified ($\\mathrm{HITM}$) response and the modified data is supplied by the peer core. Snoop Response ($\\mathrm{SNOOP\\_RESP}$) counters classify coherence responses such as invalidations, hits, or hit-modified events. False sharing typically produces ping-ponging of a cache line between cores: frequent invalidations, repeated transitions to Modified, and high rates of $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ relative to useful memory operations.\n\nYou are tasked with selecting an experimental method and expected counter signatures that best isolate and detect false sharing using performance counters, including $\\mathrm{L1\\_RFO}$, $\\mathrm{HITM}$, and $\\mathrm{SNOOP\\_RESP}$. The experiment should contrast a false sharing scenario with a control that eliminates false sharing. Let the cache line size be $L = 64$ bytes. Assume the workload performs $N$ iterations where each iteration writes an $8$-byte word per thread.\n\nWhich option best describes a scientifically sound experiment and the expected counter behavior that indicates false sharing?\n\nA. Pin $T_1$ and $T_2$ to different cores. Allocate a single array $A$ and assign $T_1$ to update $A[i]$ and $T_2$ to update $A[i+1]$ for each iteration $i$, with $A$ laid out such that $A[i]$ and $A[i+1]$ are distinct $8$-byte words within the same cache line of size $L = 64$. Run for $N$ iterations. Then run a padded version where $A[i]$ and $A[i+1]$ are separated so they fall on different cache lines (e.g., stride or explicit padding of at least $L$ bytes). Expect the false-sharing run to exhibit a large increase in $\\mathrm{L1\\_RFO}$ per store, many invalidation-related $\\mathrm{SNOOP\\_RESP}$ events, and elevated $\\mathrm{HITM}$ responses relative to the padded control. The padded control should show significantly fewer $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ events per store and lower invalidation-related $\\mathrm{SNOOP\\_RESP}$, with similar last-level cache miss rates between runs.\n\nB. Pin $T_1$ and $T_2$ to different cores. Allocate two arrays $A$ and $B$ aligned so that each element updated by $T_1$ and $T_2$ is placed on different cache lines. Perform only reads (no stores) in both threads for $N$ iterations. Expect a surge of $\\mathrm{HITM}$ events during the read-only run, proving false sharing, and expect the padded control to show higher last-level cache ($\\mathrm{LLC}$) miss rate but lower $\\mathrm{L1\\_RFO}$.\n\nC. Pin $T_1$ and $T_2$ to different cores. Use a single shared counter and have both threads perform atomic increments for $N$ iterations. Expect extremely high $\\mathrm{HITM}$ and $\\mathrm{L1\\_RFO}$, and conclude false sharing. As a control, replace atomic increments with relaxed loads to the same shared counter and expect the counters to drop sharply, validating that elevated $\\mathrm{HITM}$ was due to false sharing.\n\nD. Pin $T_1$ and $T_2$ to simultaneous multithreading (SMT) on the same physical core. Use two local variables in each thread mapped to the same cache line. Expect that false sharing will be detectable primarily via increases in last-level cache ($\\mathrm{LLC}$) misses and decreases in $\\mathrm{HITM}$ relative to a padded control, because SMT threads share the L1 cache and thus coherence traffic is minimized. Conclude false sharing if last-level cache misses rise in the unpadded run.\n\nE. Pin $T_1$ and $T_2$ to different cores. Allocate a shared data structure such that both threads frequently write to the exact same $8$-byte word. Expect large $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ counts in this run, and conclude that these counter elevations demonstrate false sharing. Use a control where each thread writes to its own independent word within the same cache line; expect little change in $\\mathrm{HITM}$ and $\\mathrm{L1\\_RFO}$, confirming the presence of false sharing in the first run only.\n\nSelect the single best option.",
            "solution": "The user has requested a meticulous validation of the problem statement, followed by a detailed solution and evaluation of options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **System Configuration**: A shared-memory multiprocessor with $k$ symmetric Central Processing Unit (CPU) cores.\n- **Cache Hierarchy**: Each core possesses a private Level One (L1) write-back cache. A last-level cache is shared among the cores.\n- **Coherence Protocol**: The machine implements the Modified-Exclusive-Shared-Invalid (MESI) protocol.\n- **Execution Context**: Two threads, $T_1$ and $T_2$, run concurrently on different cores.\n- **Definition of False Sharing**: \"simultaneous accesses by multiple threads to distinct memory locations that reside within the same cache line, causing coherence traffic even though no actual data dependency exists.\"\n- **Protocol Mechanics (Fundamental Base)**:\n    - A store operation to an address not in the Exclusive or Modified state triggers a Read For Ownership, specifically a Level One Read For Ownership ($\\mathrm{L1\\_RFO}$).\n    - If another core holds the line in the Modified state, the request observes a Hit on Modified ($\\mathrm{HITM}$) response, and the modified data is supplied by the peer core.\n    - Coherence responses are classified by Snoop Response ($\\mathrm{SNOOP\\_RESP}$) counters.\n    - The characteristic signature of false sharing includes \"ping-ponging\" of a cache line, frequent invalidations, repeated transitions to the Modified state, and high rates of $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ relative to useful memory operations.\n- **Experimental Parameters**:\n    - Cache line size: $L = 64$ bytes.\n    - Workload: $N$ iterations, where each iteration involves each thread writing an $8$-byte word.\n- **Task**: To identify the best experimental method and corresponding performance counter signatures to isolate and detect false sharing.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is evaluated against the established criteria:\n\n1.  **Scientific or Factual Soundness**: The problem is scientifically sound. The description of a shared-memory multiprocessor, the MESI protocol, the definition of false sharing, and the behavior of performance counters like $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ are all standard and factual concepts in the field of computer architecture.\n2.  **Non-Formalizable or Irrelevant**: The problem is directly relevant to its stated topic (*cache coherence problem and false sharing*) and is entirely formalizable.\n3.  **Incomplete or Contradictory Setup**: The setup is self-contained and complete. It provides all necessary definitions, context, and parameters ($L=64$ bytes, $8$-byte words, write-based workload) to reason about the experimental design. There are no internal contradictions.\n4.  **Unrealistic or Infeasible**: The described hardware architecture and programming scenario are common in modern multi-core systems. The parameters are realistic. The phenomenon of false sharing is a well-documented and practical performance issue.\n5.  **Ill-Posed or Poorly Structured**: The question is well-posed, asking to select the *best* method from a set of choices. The terminology is precise and well-defined within the context of the problem.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a nuanced understanding of cache coherence mechanisms and experimental design to distinguish between true sharing, false sharing, and no sharing, and to correctly interpret performance counter data.\n7.  **Outside Scientific Verifiability**: The proposed experiments are verifiable on actual hardware with Performance Monitoring Units (PMUs) or through cycle-accurate simulation, which are standard methodologies in computer architecture research.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is scientifically grounded, well-posed, and provides a clear basis for evaluating the proposed options. The solution process will now proceed.\n\n### Solution Derivation\n\nThe core of the task is to design an experiment that isolates false sharing. A sound scientific experiment requires a test case and a control case.\n- **Test Case (Inducing False Sharing)**: To create false sharing, two threads on different cores must repeatedly write to distinct memory addresses that lie on the same cache line.\n- **Control Case (Eliminating False Sharing)**: To create a valid control, the experiment should be identical to the test case in all aspects (e.g., number of writes, threads, core pinning) except for the specific condition causing false sharing. This is achieved by ensuring the distinct memory addresses accessed by the threads now lie on different cache lines. This is commonly done through data structure padding.\n\nThe MESI protocol dictates the expected signatures:\n- **During False Sharing**: Let thread $T_1$ on core $C_1$ write to address $x$ and thread $T_2$ on core $C_2$ write to address $y$, where $x$ and $y$ are on the same cache line.\n    1.  $T_1$ writes to $x$. $C_1$ issues an $\\mathrm{L1\\_RFO}$ to get the line in the **Modified (M)** state.\n    2.  $T_2$ writes to $y$. $C_2$ issues an $\\mathrm{L1\\_RFO}$.\n    3.  $C_1$'s snoop logic detects the $\\mathrm{L1\\_RFO}$ for the line it holds in **M** state. From $C_2$'s perspective, this is a $\\mathrm{HITM}$ event. $C_1$ must provide the data and invalidate its copy (transition to **Invalid (I)**).\n    4.  $C_2$ receives the line, performs its write, and holds the line in **M** state.\n    5.  $T_1$ writes to $x$ again. Its copy is **I**. It issues another $\\mathrm{L1\\_RFO}$.\n    6.  This triggers a $\\mathrm{HITM}$ on $C_2$, which must invalidate its copy and send the line to $C_1$.\n    This \"ping-ponging\" of the cache line generates one $\\mathrm{L1\\_RFO}$ and one $\\mathrm{HITM}$ (and an invalidation) for nearly every alternating write between the cores. This results in extremely high counts for these coherence-related events.\n\n- **During the Control Case (Padded Data)**:\n    1.  $T_1$ on $C_1$ writes to $x$. $C_1$ issues an $\\mathrm{L1\\_RFO}$ and gets the line for $x$ in **M** state.\n    2.  $T_2$ on $C_2$ writes to $y$. Because $y$ is on a different cache line, $C_2$ issues an $\\mathrm{L1\\_RFO}$ for its own line and gets it in **M** state.\n    3.  Subsequent writes by $T_1$ to $x$ will be L1 cache hits on $C_1$ (since its line is in **M** state). Similarly for $T_2$ on $C_2$. No further inter-core coherence traffic occurs.\n    Therefore, the control case should exhibit only a minimal number of $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ events (from the initial cold misses) and then proceed with cheap L1 writes. The total number of events should be drastically lower than in the false sharing case.\n\n### Option-by-Option Analysis\n\n**A. Pin $T_1$ and $T_2$ to different cores. Allocate a single array $A$ and assign $T_1$ to update $A[i]$ and $T_2$ to update $A[i+1]$ for each iteration $i$, with $A$ laid out such that $A[i]$ and $A[i+1]$ are distinct $8$-byte words within the same cache line of size $L = 64$. Run for $N$ iterations. Then run a padded version where $A[i]$ and $A[i+1]$ are separated so they fall on different cache lines (e.g., stride or explicit padding of at least $L$ bytes). Expect the false-sharing run to exhibit a large increase in $\\mathrm{L1\\_RFO}$ per store, many invalidation-related $\\mathrm{SNOOP\\_RESP}$ events, and elevated $\\mathrm{HITM}$ responses relative to the padded control. The padded control should show significantly fewer $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ events per store and lower invalidation-related $\\mathrm{SNOOP\\_RESP}$, with similar last-level cache miss rates between runs.**\n\nThis option describes a methodologically perfect experiment.\n- **Test Case**: Having $T_1$ write to $A[i]$ and $T_2$ write to $A[i+1]$ where both are $8$-byte elements within a $64$-byte cache line is the canonical example of false sharing. For instance, if $A[i]$ is at address `addr`, $A[i+1]$ is at `addr+8`, and both are well within a $64$-byte boundary.\n- **Control Case**: Using padding to separate the elements onto different cache lines correctly removes the cause of false sharing while keeping the computational work the same.\n- **Expected Outcome**: The prediction of high $\\mathrm{L1\\_RFO}$, $\\mathrm{HITM}$, and invalidation events for the false-sharing run, and low counts for the padded control, is precisely correct based on the MESI protocol analysis. The observation that last-level cache miss rates should be similar is also astute, as false sharing is a coherence issue between private caches, not typically a capacity issue in the shared LLC.\n**Verdict: Correct.**\n\n**B. Pin $T_1$ and $T_2$ to different cores. Allocate two arrays $A$ and $B$ aligned so that each element updated by $T_1$ and $T_2$ is placed on different cache lines. Perform only reads (no stores) in both threads for $N$ iterations. Expect a surge of $\\mathrm{HITM}$ events during the read-only run, proving false sharing, and expect the padded control to show higher last-level cache ($\\mathrm{LLC}$) miss rate but lower $\\mathrm{L1\\_RFO}$.**\n\nThis option is fundamentally flawed.\n- **Experiment**: It starts with data on different cache lines, which is a setup used to *avoid* false sharing. Furthermore, it uses a read-only workload. False sharing and the associated high-cost coherence traffic (RFOs, HITMs) are caused by writes that require exclusive ownership. Multiple cores can hold a line in the **Shared (S)** state for reading without conflict or invalidations.\n- **Expected Outcome**: The expectation of a \"surge of HITM events\" from a read-only workload is incorrect. A read miss will trigger a read request, not a Read For Ownership ($\\mathrm{L1\\_RFO}$). A HITM event specifically responds to an ownership request (like an RFO) finding the line in a Modified state elsewhere. Neither condition is met.\n**Verdict: Incorrect.**\n\n**C. Pin $T_1$ and $T_2$ to different cores. Use a single shared counter and have both threads perform atomic increments for $N$ iterations. Expect extremely high $\\mathrm{HITM}$ and $\\mathrm{L1\\_RFO}$, and conclude false sharing. As a control, replace atomic increments with relaxed loads to the same shared counter and expect the counters to drop sharply, validating that elevated $\\mathrm{HITM}$ was due to false sharing.**\n\nThis option misdiagnoses the phenomenon.\n- **Experiment**: Having two threads update the *exact same* memory location (a shared counter) is an example of **true sharing** or high contention, not false sharing. While the performance signature (high $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$) is similar, the underlying cause is different—the threads have a legitimate data dependency. The definition of false sharing requires accesses to *distinct* memory locations.\n- **Control**: The proposed control (relaxed loads) is invalid as it changes the fundamental operation of the program from a correct parallel counter to a buggy one where updates are lost. A control must perform the same logical work.\n**Verdict: Incorrect.**\n\n**D. Pin $T_1$ and $T_2$ to simultaneous multithreading (SMT) on the same physical core. Use two local variables in each thread mapped to the same cache line. Expect that false sharing will be detectable primarily via increases in last-level cache ($\\mathrm{LLC}$) misses and decreases in $\\mathrm{HITM}$ relative to a padded control, because SMT threads share the L1 cache and thus coherence traffic is minimized. Conclude false sharing if last-level cache misses rise in the unpadded run.**\n\nThis option misunderstands the hardware context of SMT.\n- **Experiment**: SMT threads on the same physical core typically share the L1 data cache. If two variables are on the same cache line, and two SMT threads on the same core access them, the data exchange happens within the shared L1 cache. This does not trigger the inter-core MESI coherence protocol. Consequently, there would be no associated $\\mathrm{L1\\_RFO}$s or $\\mathrm{HITM}$s between cores. SMT on a single core largely *avoids* the false sharing problem.\n- **Expected Outcome**: The prediction of detecting this via LLC misses is misplaced, and the statement that coherence traffic is minimized is correct, but it leads to the conclusion that this is the wrong setup to detect inter-core false sharing in the first place.\n**Verdict: Incorrect.**\n\n**E. Pin $T_1$ and $T_2$ to different cores. Allocate a shared data structure such that both threads frequently write to the exact same $8$-byte word. Expect large $\\mathrm{L1\\_RFO}$ and $\\mathrm{HITM}$ counts in this run, and conclude that these counter elevations demonstrate false sharing. Use a control where each thread writes to its own independent word within the same cache line; expect little change in $\\mathrm{HITM}$ and $\\mathrm{L1\\_RFO}$, confirming the presence of false sharing in the first run only.**\n\nThis option is incorrect and reverses the concepts.\n- **Experiment**: The initial run (writing to the same word) describes **true sharing**. The option incorrectly labels this as the way to demonstrate false sharing.\n- **Control**: The \"control\" case (writing to independent words in the same cache line) is the canonical definition of **false sharing**.\n- **Expected Outcome**: The prediction for the control is catastrophically wrong. It expects \"little change in HITM and L1_RFO\" for the false sharing scenario. As established, the false sharing scenario is precisely what causes the ping-ponging and massive spike in these counters. This option demonstrates a complete misunderstanding of the topic.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once you know how to detect false sharing, a natural next question is: under what conditions does it become a significant problem? This practice moves from experimental detection to probabilistic modeling, allowing you to quantify the likelihood of contention. By treating the random access pattern of threads to cache lines as a classic probability puzzle similar to the \"birthday problem,\" you can calculate the threshold at which performance degradation becomes nearly certain. This exercise helps build a crucial intuition for when to anticipate and proactively design against false sharing in your own parallel programs. ",
            "id": "3684625",
            "problem": "A multicore system implements a byte-value histogram for streaming data. The histogram has $256$ counters, one per possible byte value. The machine uses a write-invalidate cache coherence protocol such as Modified–Exclusive–Shared–Invalid (MESI), and the cache line size is $64\\,\\text{B}$. Counters are laid out contiguously in memory.\n\nAssume $K$ threads, each on a distinct core, concurrently increments a single counter per time step. Each thread independently chooses which counter to increment, uniformly at random over the $256$ counters, and performs a store that obtains exclusive ownership of the cache line containing that counter. Two or more threads that, in the same time step, choose counters residing in the same cache line will contend via coherence invalidations, even if they modify distinct counters, which is an instance of false sharing.\n\nDefine “high contention” to mean that the probability that at least one cache line is chosen by two or more threads in a single time step is at least $0.9$. Treat the single time step as an instantaneous selection of $K$ counters. For $1\\,\\text{B}$ counters, the $256$ counters occupy $256/64 = 4$ cache lines; thus, at the line granularity, each thread independently selects one of $B=4$ lines uniformly at random each time step.\n\nStarting from core principles of probability (e.g., basic counting and independence) and the cache line granularity of coherence, derive the exact expression for the probability of no contention among $K$ threads as a function of $B$, and from it determine the minimal integer threshold $K$ such that the probability of at least one contended line is at least $0.9$ for the $1\\,\\text{B}$-counter layout (i.e., $B=4$). Report this minimal $K$ as your final answer. No numerical rounding is required.\n\nThen, briefly justify (no calculation required for grading) whether widening each counter to $4\\,\\text{B}$, which keeps the histogram contiguous but increases the number of cache lines it spans to $B=256/16=16$, would increase or decrease this threshold and why, in terms of false sharing under MESI.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It describes a classic performance issue in computer architecture—false sharing—and models it as a probabilistic problem analogous to the birthday problem. All necessary parameters and definitions are provided to derive a unique, meaningful solution.\n\nThe problem asks for three items:\n1. The exact expression for the probability of no contention among $K$ threads as a function of the number of cache lines, $B$.\n2. The minimal integer threshold $K$ for which the probability of contention is at least $0.9$ when $B=4$.\n3. A justification for how this threshold $K$ changes if the number of cache lines increases to $B=16$.\n\nLet $K$ be the number of threads and $B$ be the number of available cache lines. Each of the $K$ threads independently and uniformly selects one of the $B$ cache lines. This process is equivalent to drawing $K$ samples with replacement from a set of $B$ items.\n\nThe total number of possible outcomes, which constitutes the size of the sample space, is the number of ways $K$ threads can choose from $B$ lines. Since each thread has $B$ independent choices, the total number of outcomes is $B^K$.\n\nContention occurs if at least two threads select the same cache line. The complementary event, \"no contention,\" occurs if all $K$ threads select distinct cache lines. We first derive the probability of this latter event, denoted $P(\\text{no contention})$.\n\nFor no contention to be possible, the number of threads must not exceed the number of available lines, i.e., $K \\le B$. If $K > B$, by the pigeonhole principle, at least one line must be chosen by more than one thread, making contention certain and $P(\\text{no contention}) = 0$.\n\nAssuming $K \\le B$, the number of ways for all $K$ threads to choose distinct lines is the number of $K$-permutations of a set of $B$ elements.\nThe first thread can choose any of the $B$ lines.\nThe second thread must choose from the remaining $B-1$ lines.\nThis continues until the $K$-th thread, which has $B-(K-1)$ choices.\nThe total number of such favorable outcomes is given by the falling factorial:\n$$ N_{\\text{no contention}} = B \\times (B-1) \\times \\cdots \\times (B-K+1) = \\frac{B!}{(B-K)!} $$\nThe probability of no contention is the ratio of favorable outcomes to the total number of outcomes:\n$$ P(\\text{no contention} | K, B) = \\frac{B!}{B^K (B-K)!} $$\nThis is the requested exact expression for the probability of no contention for $K \\le B$.\n\nNext, we determine the minimal integer $K$ that satisfies the \"high contention\" condition for the specified layout. With $1\\,\\text{B}$ counters and a $64\\,\\text{B}$ cache line, the $256$ counters occupy $B = \\frac{256}{64} = 4$ cache lines.\n\nThe probability of at least one contended line, $P(\\text{contention})$, is the complement of no contention:\n$$ P(\\text{contention}) = 1 - P(\\text{no contention}) $$\nThe condition for \"high contention\" is given as $P(\\text{contention}) \\ge 0.9$. This is equivalent to:\n$$ 1 - P(\\text{no contention}) \\ge 0.9 $$\n$$ P(\\text{no contention}) \\le 0.1 $$\nSubstituting the derived expression for $P(\\text{no contention})$ with $B=4$:\n$$ \\frac{4!}{4^K (4-K)!} \\le 0.1 $$\nWe must find the smallest integer $K \\ge 1$ that satisfies this inequality. We test values of $K$ starting from $1$.\nFor $K > 4$, $P(\\text{no contention}) = 0$, which satisfies the inequality, but we seek the minimal $K$.\n\nFor $K=1$:\n$$ P(\\text{no contention}) = \\frac{4!}{4^1 (3)!} = \\frac{4}{4} = 1 $$\nThis is not $\\le 0.1$.\n\nFor $K=2$:\n$$ P(\\text{no contention}) = \\frac{4!}{4^2 (2)!} = \\frac{24}{16 \\times 2} = \\frac{24}{32} = \\frac{3}{4} = 0.75 $$\nThis is not $\\le 0.1$.\n\nFor $K=3$:\n$$ P(\\text{no contention}) = \\frac{4!}{4^3 (1)!} = \\frac{24}{64} = \\frac{3}{8} = 0.375 $$\nThis is not $\\le 0.1$.\n\nFor $K=4$:\n$$ P(\\text{no contention}) = \\frac{4!}{4^4 (0)!} = \\frac{24}{256} = \\frac{3}{32} = 0.09375 $$\nSince $0.09375 \\le 0.1$, the condition is met for $K=4$.\nThe corresponding probability of contention is $P(\\text{contention}) = 1 - \\frac{3}{32} = \\frac{29}{32} = 0.90625$, which is $\\ge 0.9$.\nSince the condition was not met for $K < 4$, the minimal integer threshold is $K=4$.\n\nFinally, we justify whether widening each counter to $4\\,\\text{B}$ would increase or decrease this threshold.\nIf each of the $256$ counters is widened to $4\\,\\text{B}$, the total histogram size becomes $256 \\times 4\\,\\text{B} = 1024\\,\\text{B}$. With a $64\\,\\text{B}$ cache line size, the number of cache lines occupied by the histogram increases to $B = \\frac{1024}{64} = 16$.\nFalse sharing occurs when multiple threads access distinct data elements that reside on the same cache line. Under a write-invalidate protocol like MESI, a write by one thread requires exclusive ownership of the line, which invalidates copies in other cores' caches. This forces other cores wishing to access that line to suffer a cache miss and fetch the line again, causing performance degradation.\nBy increasing the number of cache lines from $B=4$ to $B=16$, the data is spread out over a larger number of distinct lines. For a fixed number of threads $K$, the probability that any two threads happen to select counters residing on the same cache line decreases. This is because there are more \"bins\" ($B=16$) for the threads' random selections to fall into. The probability of false sharing for any given $K$ is therefore reduced. Consequently, to reach the same high probability of contention (e.g., $0.9$), a larger number of threads will be required. Thus, the threshold $K$ would increase.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Having learned to detect and model false sharing, the final step is to eliminate it. This practice focuses on padding, one of the most direct and effective mitigation techniques. The challenge is to derive a general formula for the minimum padding required to ensure that elements of an array are always placed on separate cache lines, thereby preventing contention. This calculation is fundamental for writing high-performance code, as it provides a precise method to solve the problem without wasting excessive memory. ",
            "id": "3684636",
            "problem": "Consider a shared-memory multiprocessor with private caches and a bus-based cache coherence protocol operating at cache-line granularity. Each cache line has size $B$ bytes, and coherence actions (such as invalidation or update) apply to entire cache lines. False sharing arises when two threads write to distinct memory locations that reside in the same cache line, triggering coherence traffic even though the threads do not truly share data at the semantic level. To eliminate false sharing for an array of elements, it is sufficient to ensure that no two distinct elements ever reside in the same cache line.\n\nSuppose an array consists of contiguous elements, each being a structure of size $s$ bytes. To prevent false sharing, we append padding of $p$ bytes to each element so that the stride $t$ between consecutive element starts is $t = s + p$. Assume the allocator returns a base address aligned to the cache line size, i.e., the base address is a multiple of $B$. The goal is to choose padding $p$ minimally such that, for any cache line size $B$, no two distinct elements share any cache line.\n\nStarting from the fundamental definitions that cache coherence operates at cache-line granularity and that alignment to multiples of $B$ places element starts on cache line boundaries, derive a closed-form expression for the minimum padding $p$ in terms of $s$ and $B$ that guarantees no cache line is shared by two distinct elements. Then compute the memory overhead ratio $\\rho$ defined by $\\rho = (s + p)/s$.\n\nExpress your final results as closed-form analytic expressions in terms of $s$ and $B$. No numerical approximation or rounding is required.",
            "solution": "The problem requires the derivation of a closed-form expression for the minimum padding $p$ needed to prevent false sharing for an array of elements, and the corresponding memory overhead ratio $\\rho$. The elements, each of size $s$ bytes, are to be padded such that the stride $t$ between consecutive elements becomes $t = s + p$. The cache line size is $B$ bytes, and the base address of the array is aligned to a multiple of $B$. The condition to eliminate false sharing is that no two distinct elements share any cache line.\n\nLet the base address of the array be $A_{base}$. The problem states that $A_{base}$ is a multiple of $B$, so $A_{base} = k_0 B$ for some integer $k_0$. We can analyze the memory layout using addresses relative to $A_{base}$, effectively setting the starting address of the array to $0$ without loss of generality.\n\nAn element of size $s$ is padded with $p$ bytes, resulting in a total occupied space or stride of $t = s+p$ bytes between the start of consecutive elements. The address of the start of the $i$-th element (0-indexed) is given by $a_i = i \\cdot t$. The memory occupied by the $i$-th element itself (excluding padding) is the byte range $[a_i, a_i + s - 1]$.\n\nCache coherence operates on cache lines of size $B$. A memory address $x$ resides in the cache line that covers the address range $[B \\cdot \\lfloor x/B \\rfloor, B \\cdot \\lfloor x/B \\rfloor + B - 1]$.\n\nThe condition that no two distinct elements share a cache line must hold for all pairs of elements. It is sufficient to enforce this for any two consecutive elements, say element $i$ and element $i+1$. If this holds for all $i$, then by transitivity, it holds for any two distinct elements.\n\nThe most robust and direct method to guarantee that each element resides in a set of cache lines disjoint from all other elements is to ensure that each element starts on a cache line boundary. Since the base address of the array is already on a cache line boundary, this can be achieved by making the stride $t$ a multiple of the cache line size $B$.\n\nLet us set the stride $t$ to be an integer multiple of $B$, i.e., $t = mB$ for some positive integer $m$. The start address of element $i$ is $a_i = i \\cdot t = i \\cdot mB$. Since $i$ and $m$ are integers, $a_i$ is always a multiple of $B$. Thus, every element begins exactly at the start of a cache line.\n\nNow we must ensure that the data of element $i$ does not extend into the cache line where element $i+1$ begins.\nElement $i$ occupies the address range $[a_i, a_i + s - 1]$.\nElement $i+1$ starts at address $a_{i+1} = a_i + t$.\nThe last byte of element $i$ must be at an address strictly less than the starting address of element $i+1$. This is guaranteed by construction, as $a_i + s - 1 < a_i + s \\le a_i + t = a_{i+1}$ since $p \\ge 0$.\n\nThe crucial condition is that the cache lines occupied by element $i$ do not overlap with those for element $i+1$.\nThe set of cache lines for element $i$ starts with the line containing $a_i$. The last cache line for element $i$ is the one containing address $a_i + s - 1$. The starting address of this last line is $B \\cdot \\lfloor (a_i + s - 1)/B \\rfloor$. The next cache line begins at address $B \\cdot (\\lfloor (a_i + s - 1)/B \\rfloor + 1)$.\nThe start of element $i+1$ is $a_{i+1}$. Since $a_{i+1}$ is a cache line boundary, the requirement is that $a_{i+1}$ must be greater than or equal to the start of the next available cache line after element $i$.\nThat is, $a_{i+1} \\ge B \\cdot (\\lfloor (a_i + s - 1)/B \\rfloor + 1)$.\nSince $a_i$ and $a_{i+1}$ are multiples of $B$, let $a_i = k_i B$ and $a_{i+1} = k_{i+1} B$.\n$k_{i+1} B \\ge B \\cdot (\\lfloor (k_i B + s - 1)/B \\rfloor + 1)$.\n$k_{i+1} \\ge \\lfloor k_i + (s - 1)/B \\rfloor + 1$.\n$k_{i+1} \\ge k_i + \\lfloor (s - 1)/B \\rfloor + 1$.\nUsing $k_{i+1} = a_{i+1}/B = (a_i+t)/B = k_i + t/B$, we get:\n$k_i + t/B \\ge k_i + \\lfloor (s - 1)/B \\rfloor + 1$.\n$t/B \\ge \\lfloor (s - 1)/B \\rfloor + 1$.\n\nFor any integer $s \\ge 1$ and $B > 0$, the identity $\\lceil s/B \\rceil = \\lfloor (s-1)/B \\rfloor + 1$ holds.\nThus, the condition becomes $t/B \\ge \\lceil s/B \\rceil$.\nSince $t$ must be a multiple of $B$, let $t=mB$. Then $m \\ge \\lceil s/B \\rceil$.\nTo minimize padding $p$, we must minimize the stride $t$. This is achieved by choosing the smallest possible integer value for $m$, which is $m_{min} = \\lceil s/B \\rceil$.\n\nThe minimum stride that guarantees no false sharing is therefore:\n$$t_{min} = B \\cdot \\lceil s/B \\rceil$$\nThis stride must also be at least the size of the element itself, $t_{min} \\ge s$.\nSince $\\lceil s/B \\rceil \\ge s/B$, multiplying by $B$ gives $B \\cdot \\lceil s/B \\rceil \\ge s$. So, $t_{min} \\ge s$ is always satisfied.\n\nThe minimum required padding $p$ is the difference between this minimum stride and the element size $s$:\n$$p = t_{min} - s = B \\cdot \\left\\lceil \\frac{s}{B} \\right\\rceil - s$$\n\nThe second part of the problem is to compute the memory overhead ratio $\\rho$, defined as $\\rho = (s+p)/s$.\nSubstituting $s+p = t_{min}$:\n$$\\rho = \\frac{t_{min}}{s} = \\frac{B \\cdot \\lceil s/B \\rceil}{s}$$\n\nThe derived expressions for $p$ and $\\rho$ are in terms of $s$ and $B$ and are in closed form as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nB \\left\\lceil \\frac{s}{B} \\right\\rceil - s & \\frac{B \\left\\lceil \\frac{s}{B} \\right\\rceil}{s}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}