{
    "hands_on_practices": [
        {
            "introduction": "To master the memory hierarchy, we must first understand how memory addresses map to cache locations. This exercise explores the critical concept of conflict misses in a direct-mapped cache, a common and sometimes severe performance bottleneck. By analyzing a pathological access pattern described in , you will see firsthand how data layout can create a \"ping-pong\" effect that evicts useful data and learn how a simple padding strategy can resolve the issue.",
            "id": "3684789",
            "problem": "A byte-addressable system uses a direct-mapped cache with total capacity $C$ bytes and block size $B$ bytes, where $C$ is an integer multiple of $B$. Let the number of cache lines be $L = C/B$. Two arrays, $A$ and $D$, each occupy exactly $C$ bytes and are laid out in main memory as follows: the base address of $A$ is $A_{\\mathrm{base}} = 0$, and the base address of $D$ is $D_{\\mathrm{base}} = C$. Each array stores one byte per element, so the $i$-th element of $A$ is at address $A_{\\mathrm{base}} + i$, and the $i$-th element of $D$ is at address $D_{\\mathrm{base}} + i$. Consider the alternating access pattern that, for all $i \\in \\{0, 1, \\dots, C-1\\}$, performs a read of $A[i]$ followed by a read of $D[i]$. Assume the cache is initially empty.\n\nUsing only the fundamental definition of direct-mapped indexing—namely, that the cache line index for a byte address $x$ is given by $\\left\\lfloor \\frac{x}{B} \\right\\rfloor \\bmod L$—and the notion of compulsory misses, first reason about the total number of cache misses incurred by the above alternating access pattern with the given layout. Next, propose a remapping that eliminates the pathological conflicts by changing only the base address of $D$ to $D_{\\mathrm{base}}' = C + B$ (i.e., padding $B$ bytes between $A$ and $D$), and re-analyze the total number of misses under the same alternating access pattern.\n\nWhat is the multiplicative reduction factor in the total miss count due to this remapping, expressed as a single closed-form expression in terms of $B$ only? Provide your final answer as a single symbolic expression. No rounding is required.",
            "solution": "The problem is deemed valid as it is scientifically grounded in the principles of computer architecture, is well-posed with all necessary information provided, and is stated objectively.\n\nLet us denote the cache capacity by $C$ bytes, the block size by $B$ bytes, and the number of cache lines by $L = \\frac{C}{B}$. The system is byte-addressable and the cache is direct-mapped. The cache line index for a given byte address $x$ is given by the formula $I(x) = \\left\\lfloor \\frac{x}{B} \\right\\rfloor \\pmod L$.\n\nFirst, we will analyze the total number of cache misses for the initial memory layout, and then for the remapped layout.\n\n**Case 1: Initial Memory Layout**\n\nIn the initial configuration, the base address of array $A$ is $A_{\\mathrm{base}} = 0$ and the base address of array $D$ is $D_{\\mathrm{base}} = C$. Each array has a size of $C$ bytes. The access pattern is an alternating read of corresponding elements from $A$ and $D$: for all $i \\in \\{0, 1, \\dots, C-1\\}$, the system reads $A[i]$ followed by a read of $D[i]$.\n\nThe address of the $i$-th element of $A$, denoted $A[i]$, is $x_A(i) = A_{\\mathrm{base}} + i = i$.\nThe cache line index for this access is:\n$$I_A(i) = \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\pmod L$$\n\nThe address of the $i$-th element of $D$, denoted $D[i]$, is $x_D(i) = D_{\\mathrm{base}} + i = C + i$.\nSince $C=LB$, the cache line index for this access is:\n$$I_D(i) = \\left\\lfloor \\frac{C+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor \\frac{LB+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor L + \\frac{i}{B} \\right\\rfloor \\pmod L$$\nUsing the property of the floor function, $\\lfloor n+y \\rfloor = n + \\lfloor y \\rfloor$ for integer $n$:\n$$I_D(i) = \\left( L + \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\right) \\pmod L$$\nSince $L \\pmod L = 0$, this simplifies to:\n$$I_D(i) = \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\pmod L$$\n\nWe observe that $I_A(i) = I_D(i)$ for all $i$. This means that for any given $i$, the access to $A[i]$ and the access to $D[i]$ map to the exact same cache line.\n\nNow, consider the alternating access sequence (read $A[i]$, read $D[i]$).\nLet's trace the state of a cache line. Suppose the access to $A[i]$ occurs. If the block containing $A[i]$ is not in the cache, it is fetched, resulting in a miss. Immediately after, the access to $D[i]$ occurs. Since $D[i]$ maps to the same cache line, and it belongs to a different memory block, its block must be fetched. This action evicts the block for $A[i]$ that was just loaded. This is a conflict miss.\nFor the next iteration, $i+1$, the access to $A[i+1]$ occurs. If $A[i+1]$ is in the same block as $A[i]$, its block is no longer in the cache because it was evicted by the access to $D[i]$. Thus, this access to $A[i+1]$ is also a miss.\nThis \"ping-pong\" effect continues for the entire loop. The access to an element in $A$ loads a block, which is immediately evicted by the access to the corresponding element in $D$, which is then evicted by the next access to $A$, and so on.\n\nConsequently, every single memory access in this sequence results in a cache miss. The loop runs for $i$ from $0$ to $C-1$, and for each $i$, there are two accesses ($A[i]$ and $D[i]$).\nThe total number of accesses is $2 \\times C$.\nThe total number of misses in this initial configuration, $M_1$, is therefore:\n$$M_1 = 2C$$\n\n**Case 2: Remapped Memory Layout**\n\nIn the remapped configuration, the base address of $A$ remains $A_{\\mathrm{base}} = 0$, but the base address of $D$ is changed to $D_{\\mathrm{base}}' = C + B$.\n\nThe cache line index for an access to $A[i]$ is unchanged:\n$$I_A(i) = \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\pmod L$$\n\nThe address of the $i$-th element of $D$, $D[i]$, is now $x_D'(i) = D_{\\mathrm{base}}' + i = C + B + i$.\nThe cache line index for this access is:\n$$I_D'(i) = \\left\\lfloor \\frac{C+B+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor \\frac{LB+B+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor L + 1 + \\frac{i}{B} \\right\\rfloor \\pmod L$$\n$$I_D'(i) = \\left( L + 1 + \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\right) \\pmod L = \\left( 1 + \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\right) \\pmod L$$\n\nWe now have $I_D'(i) = (I_A(i) + 1) \\pmod L$. The accesses to corresponding elements $A[i]$ and $D[i]$ now map to different (adjacent, wrapping around at $L-1$) cache lines. This remapping eliminates the pathological conflict observed in the first case.\n\nLet's analyze the miss pattern with this new layout. We can group the accesses by memory blocks. The loop variable $i$ ranges from $0$ to $C-1$. The block index within the arrays can be defined as $k = \\lfloor i/B \\rfloor$, which ranges from $k=0$ to $k = \\lfloor (C-1)/B \\rfloor = L-1$.\n\nFor each block index $k \\in \\{0, 1, \\dots, L-1\\}$, the loop iterates through $i$ from $kB$ to $(k+1)B-1$.\n- All accesses to $A[i]$ in this range fall into the $k$-th block of $A$. This block maps to cache line $I_A = k \\pmod L = k$.\n- All accesses to $D[i]$ in this range fall into the $k$-th block of $D$ (in terms of array structure, not memory). This block maps to cache line $I_D' = (k+1) \\pmod L$.\n\nConsider the accesses for a given block index $k$:\n- The first access to the $k$-th block of $A$ (e.g., at $i=kB$ for $A[kB]$) will cause a miss, as this block has not been accessed before (or was evicted). This is a compulsory miss. This miss brings the $k$-th block of $A$ into cache line $k$.\n- The subsequent $B-1$ accesses to this block of $A$ (i.e., $A[kB+1], \\dots, A[(k+1)B-1]$) will all be hits, as the block is now in the cache.\n- Similarly, the first access to the $k$-th block of $D$ (e.g., at $i=kB$ for $D[kB]$) will cause a miss, bringing it into cache line $(k+1) \\pmod L$.\n- The subsequent $B-1$ accesses to this block of $D$ will all be hits.\n\nFor each block index $k$ from $0$ to $L-1$, there is exactly one miss for the corresponding block of $A$ and one miss for the corresponding block of $D$. This gives $2$ misses for each of the $L$ block-iterations.\nThe total number of misses in the remapped configuration, $M_2$, is:\n$$M_2 = 2 \\times L$$\nSince $L=C/B$, we can write this as $M_2 = \\frac{2C}{B}$.\n\n**Multiplicative Reduction Factor**\n\nThe problem asks for the multiplicative reduction factor in the total miss count. This is the ratio of the original number of misses to the new number of misses.\n$$\\text{Reduction Factor} = \\frac{M_1}{M_2}$$\nSubstituting the expressions for $M_1$ and $M_2$:\n$$\\text{Reduction Factor} = \\frac{2C}{2L} = \\frac{2C}{2C/B} = B$$\nThe multiplicative reduction factor is $B$. This expression depends only on $B$ as required.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Building on the mechanics of cache organization, we now examine the policies that govern cache behavior on a write miss. This practice contrasts two standard policies, write-allocate and no-write-allocate, to reveal their profound impact on memory bandwidth consumption. In , you will quantify the wasted bandwidth for a streaming workload with poor temporal locality, illustrating the crucial trade-offs architects face when designing a cache's write strategy.",
            "id": "3684724",
            "problem": "Consider a single-level cache in a memory hierarchy with cache line size $L = 64$ bytes. The processor executes a store stream of $N = 1.5 \\times 10^{8}$ stores, each storing a word of size $w = 8$ bytes, to a memory region much larger than the cache. The store addresses are separated by a stride $s = 128$ bytes between consecutive stores, and the stream does not revisit any line once written. Assume the following two policies for handling a store miss:\n\n1. Write-allocate with write-back: on a store miss, the cache performs a Read For Ownership (RFO), reading the entire cache line from memory into the cache before updating the word, and later writes back the line upon eviction.\n2. No-write-allocate with write-around and write-combining: on a store miss, the cache does not allocate the line and writes the word directly to memory without reading the line.\n\nDefine the wasted bandwidth due to line fills as the total number of bytes transferred from memory to the cache that are never subsequently read by the processor and occur solely because of line fills triggered by store misses. Using only the scenario above, derive from first principles the total wasted bandwidth due to line fills under both policies and compute the difference (write-allocate minus no-write-allocate). Express the final difference in gigabytes, where $1$ gigabyte is defined as $10^{9}$ bytes. Round your answer to four significant figures.",
            "solution": "The problem is deemed valid as it is scientifically grounded in cache write policies, is well-posed with all necessary information provided, and is stated objectively. This problem requires us to analyze the memory traffic generated by a streaming store workload under two different cache write policies. The key is to calculate the \"wasted bandwidth,\" defined as the total bytes transferred from memory to the cache that are never subsequently read by the processor.\n\nThe problem gives the following parameters:\n- Cache line size: $L = 64$ bytes\n- Number of stores: $N = 1.5 \\times 10^{8}$\n- Store word size: $w = 8$ bytes\n- Stride between consecutive stores: $s = 128$ bytes\n\nFirst, we analyze the access pattern. The stride $s=128$ bytes is greater than the cache line size $L=64$ bytes. This means that each of the $N$ store operations will access a different cache line. Since the memory region is much larger than the cache and no line is revisited, every store will result in a cache miss.\n\n**Policy 1: Write-allocate with write-back**\n\nUnder this policy, a store miss triggers a \"Read For Ownership\" (RFO). This involves two steps:\n1. The cache controller issues a read request to fetch the entire cache line of size $L$ from main memory into the cache.\n2. The processor then writes the $w$-byte word into the newly fetched cache line.\n\nThe problem defines wasted bandwidth as the data transferred from memory to the cache that is never read by the processor. In this scenario, the workload is a pure store stream; there are no reads. For each of the $N$ store misses, $L$ bytes are transferred from memory to the cache. None of this data is ever read by the processor (the portion corresponding to the written word is immediately overwritten). Therefore, the entire line fill for each store miss constitutes wasted bandwidth.\n\nThe total wasted bandwidth for the write-allocate policy, $B_{W,1}$, is the number of misses multiplied by the size of the data transferred per miss:\n$$B_{W,1} = N \\times L$$\n\n**Policy 2: No-write-allocate with write-around**\n\nUnder this policy, a store miss does not cause the cache line to be allocated in the cache. Instead, the $w$-byte word is sent directly to main memory (or a write buffer), bypassing the cache. This is known as \"write-around.\"\n\nSince no line is fetched from memory into the cache on a store miss, there are no line fills triggered by stores. Consequently, according to the problem's definition, the wasted bandwidth due to line fills is zero.\n$$B_{W,2} = 0$$\n\n**Calculating the Difference**\n\nThe problem asks for the difference in wasted bandwidth between the two policies.\n$$\\Delta B_W = B_{W,1} - B_{W,2} = (N \\times L) - 0 = N \\times L$$\n\nNow, we can substitute the given values to compute the result:\n$$N = 1.5 \\times 10^{8}$$\n$$L = 64 \\,\\text{bytes}$$\n$$\\Delta B_W = (1.5 \\times 10^{8}) \\times 64 \\,\\text{bytes} = 96 \\times 10^{8} \\,\\text{bytes} = 9.6 \\times 10^{9} \\,\\text{bytes}$$\n\nThe final answer should be in gigabytes, where $1$ gigabyte = $10^{9}$ bytes.\n$$\\Delta B_W = \\frac{9.6 \\times 10^{9} \\,\\text{bytes}}{10^{9} \\,\\text{bytes/GB}} = 9.6 \\,\\text{GB}$$\n\nThe problem requires rounding the answer to four significant figures.\n$$\\Delta B_W = 9.600 \\,\\text{GB}$$",
            "answer": "$$\n\\boxed{9.600}\n$$"
        },
        {
            "introduction": "Our final practice shifts from analyzing cache behavior to proactively designing algorithms that exploit it for maximum performance. Using the canonical example of matrix multiplication, this exercise demonstrates the power of cache blocking, or tiling, a fundamental technique for improving data reuse. You will apply your knowledge of cache capacity, $C$, to solve for the optimal block size $b$ that ensures the active working set remains resident, bridging the gap between hardware awareness and high-performance software engineering .",
            "id": "3684821",
            "problem": "A matrix multiplication kernel computes $\\mathbf{C} \\leftarrow \\mathbf{C} + \\mathbf{A}\\mathbf{B}$ for large square matrices using a blocked algorithm. In the blocked version, the kernel repeatedly multiplies two tiles of size $b \\times b$ from $\\mathbf{A}$ and $\\mathbf{B}$ and accumulates into a $b \\times b$ tile of $\\mathbf{C}$, keeping all three tiles resident in the Level-1 (L1) data cache to maximize reuse. Assume the following:\n\n- The L1 data cache has capacity $C = 192 \\,\\text{KiB}$.\n- Each element is a double-precision floating-point number of size $s = 8 \\,\\text{bytes}$.\n- The cache uses write-back and write-allocate, and the tag and metadata overhead can be neglected relative to $C$ for this calculation.\n- You may assume an ideal fully associative capacity model such that the three $b \\times b$ tiles must collectively occupy no more than the available capacity $C$ to be simultaneously resident.\n\nStarting from core definitions of the memory hierarchy (capacity measured in bytes and data footprints computed as element count times element size), determine the largest theoretical block size $b$ such that the total footprint of the three $b \\times b$ tiles does not exceed $C$. Express your final $b$ as a real number and round your answer to four significant figures. Do not include any units in your final number.",
            "solution": "The problem is deemed valid as it is scientifically grounded in cache blocking techniques for high-performance computing, is well-posed with all necessary information provided, and is stated objectively.\n\nThe core principle of this problem is that the total memory footprint of the data that must be simultaneously resident in the cache cannot exceed the cache's capacity. The problem states that three tiles—one from matrix $\\mathbf{A}$, one from $\\mathbf{B}$, and one from $\\mathbf{C}$—must reside in the L1 data cache.\n\nLet $b$ be the dimension of the square block, or tile.\nEach tile has $b \\times b = b^2$ elements.\nEach element is a double-precision floating-point number, with a size $s = 8 \\,\\text{bytes}$.\n\nThe memory footprint of a single $b \\times b$ tile, denoted $F_{tile}$, is the number of elements multiplied by the size of each element:\n$$F_{tile} = b^2 \\cdot s$$\n\nThe blocked algorithm requires three such tiles to be held in the cache at once. Therefore, the total memory footprint, $F_{total}$, required by the algorithm's inner loop is:\n$$F_{total} = 3 \\cdot F_{tile} = 3 \\cdot b^2 \\cdot s$$\n\nThis total footprint must not exceed the L1 data cache capacity, $C$. This gives us the governing inequality:\n$$3 \\cdot b^2 \\cdot s \\le C$$\n\nTo find the largest theoretical block size $b$, we solve for the case where the footprint exactly matches the capacity:\n$$3 \\cdot b^2 \\cdot s = C$$\n\nWe can now solve for $b$:\n$$b^2 = \\frac{C}{3s}$$\n$$b = \\sqrt{\\frac{C}{3s}}$$\n\nThe given values are $C = 192 \\,\\text{KiB}$ and $s = 8 \\,\\text{bytes}$.\nFirst, we must express the cache capacity $C$ in units of bytes to be consistent with the element size $s$. In computer science, a kibibyte (KiB) is defined as $2^{10}$ bytes.\n$$C = 192 \\,\\text{KiB} = 192 \\times 2^{10} \\,\\text{bytes} = 192 \\times 1024 \\,\\text{bytes} = 196608 \\,\\text{bytes}$$\n\nNow, we substitute the numerical values for $C$ and $s$ into the equation for $b$:\n$$b = \\sqrt{\\frac{196608}{3 \\times 8}}$$\n$$b = \\sqrt{\\frac{196608}{24}}$$\n$$b = \\sqrt{8192}$$\n\nTo simplify the radical, we can express $8192$ as a power of $2$. $8192 = 2^{13}$.\n$$b = \\sqrt{2^{13}} = \\sqrt{2^{12} \\times 2^1} = \\sqrt{(2^6)^2 \\times 2} = 2^6 \\sqrt{2} = 64\\sqrt{2}$$\n\nThe problem requires a real number rounded to four significant figures. We compute the numerical value:\n$$b = 64\\sqrt{2} \\approx 64 \\times 1.41421356... \\approx 90.509667...$$\n\nRounding this value to four significant figures gives:\n$$b \\approx 90.51$$\nThis represents the largest theoretical dimension for the block size. While in practice $b$ must be an integer (e.g., $\\lfloor 90.51 \\rfloor = 90$), the problem explicitly asks for a real number representation.",
            "answer": "$$\n\\boxed{90.51}\n$$"
        }
    ]
}