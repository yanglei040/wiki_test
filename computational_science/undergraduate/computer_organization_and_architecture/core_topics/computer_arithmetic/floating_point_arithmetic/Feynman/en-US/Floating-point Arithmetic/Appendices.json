{
    "hands_on_practices": [
        {
            "introduction": "Our journey into the practicalities of floating-point arithmetic begins with a fundamental source of error: base conversion. While we think and work with decimal (base-10) numbers, computers internally use binary (base-2), and many common decimal fractions like $0.1$ cannot be represented exactly in a finite binary format. This exercise  provides a direct comparison between binary and decimal floating-point standards, challenging you to derive the precise representation of $0.1$ in both systems and observe how this initial discrepancy propagates through simple addition.",
            "id": "3642002",
            "problem": "A computing system implements the Institute of Electrical and Electronics Engineers (IEEE) 754 Standard for Floating-Point Arithmetic in both binary single precision ($\\text{binary32}$) and decimal single precision ($\\text{decimal32}$). The system uses rounding to nearest, ties to even, and each elementary operation is correctly rounded to the destination format. The $\\text{binary32}$ format encodes a real value as $(-1)^{s} \\cdot (1.f) \\cdot 2^{E - 127}$ for normal numbers, where $s$ is the sign bit, $E$ is the $8$-bit exponent field with bias $127$, and $f$ is the $23$-bit fraction. The $\\text{decimal32}$ format encodes a real value as $(-1)^{s} \\cdot c \\cdot 10^{q}$, where $c$ is a base-$10$ coefficient with up to $7$ decimal digits and $q$ is a base-$10$ exponent, and it is capable of representing decimal fractions such as $0.1$ exactly.\n\nStarting from these definitions, and without assuming any precomputed constants, perform the following:\n\n1. Derive the exact real value produced when encoding $0.1_{10}$ in $\\text{binary32}$ under rounding to nearest, ties to even. Specifically, determine the normalized exponent and the nearest representable normalized significand, and express the resulting $\\text{binary32}$ value as an exact rational number.\n\n2. Derive the exact real value produced when encoding $0.1_{10}$ in $\\text{decimal32}$ under the same rounding mode, and express it as an exact rational number.\n\n3. In each format, compute the result of the sequential sum $0.1 + 0.1 + 0.1$, where each addition is performed in that format with correct rounding after the operation. Express each final sum as an exact rational number.\n\n4. Compute the absolute difference between the two final sums obtained in part $3$.\n\nRound your final numeric answer to four significant figures. No units are required.",
            "solution": "The problem requires a detailed analysis of floating-point arithmetic in two different IEEE 754 formats, $\\text{binary32}$ and $\\text{decimal32}$, focusing on the representation and summation of the value $0.1$. The analysis will be conducted in four parts, as stipulated.\n\n**Part 1: Derivation of the $\\text{binary32}$ representation of $0.1_{10}$**\n\nThe number to be represented is $0.1_{10}$, which is the rational number $\\frac{1}{10}$. The $\\text{binary32}$ format for a normalized number is $(-1)^s \\cdot (1.f) \\cdot 2^{E-127}$. First, we must determine the exponent. We seek an integer exponent $e$ such that $1 \\le \\frac{1}{10} \\times 2^{-e} < 2$. Since $2^3 = 8$ and $2^4 = 16$, we have $2^3 < 10 < 2^4$. Taking the reciprocal, $2^{-4} < \\frac{1}{10} < 2^{-3}$. To normalize $\\frac{1}{10}$, we write it as $M \\times 2^{-4}$, where $M = \\frac{1}{10} \\times 2^4 = \\frac{16}{10} = 1.6$. Thus, the true exponent is $e = -4$. The biased exponent is $E = e + 127 = -4 + 127 = 123$.\n\nThe significand to be encoded is $1.6_{10}$. The $\\text{binary32}$ format has $1$ implicit bit and $23$ explicit fraction bits, for a total precision of $24$ bits. To determine the $24$-bit representation of the significand, we scale the value $1.6$ by $2^{23}$ and round the result to the nearest integer.\nThe scaled significand is $1.6 \\times 2^{23} = \\frac{16}{10} \\times 2^{23} = \\frac{8}{5} \\times 2^{23} = \\frac{2^3 \\times 2^{23}}{5} = \\frac{2^{26}}{5} = \\frac{67108864}{5} = 13421772.8$.\n\nThe rounding mode is \"rounding to nearest, ties to even\". The value $13421772.8$ is not a tie (i.e., not halfway between two integers). It is closer to $13421773$ than to $13421772$. Thus, we round up to $13421773$.\nThis integer, $13421773$, represents the $24$ bits of the significand. The value of the significand is therefore this integer divided by $2^{23}$.\n$M_{\\text{b32}} = \\frac{13421773}{2^{23}}$.\n\nThe final value represented in $\\text{binary32}$ format, which we denote as $x_{\\text{b32}}$, is the product of this significand and the exponential part:\n$x_{\\text{b32}} = M_{\\text{b32}} \\times 2^{-4} = \\frac{13421773}{2^{23}} \\times 2^{-4} = \\frac{13421773}{2^{27}}$.\nAs a rational number, the $\\text{binary32}$ representation of $0.1$ is $\\frac{13421773}{134217728}$.\n\n**Part 2: Derivation of the $\\text{decimal32}$ representation of $0.1_{10}$**\n\nThe $\\text{decimal32}$ format represents a number as $(-1)^s \\cdot c \\cdot 10^q$, where $c$ is a base-$10$ coefficient of up to $7$ digits. The value $0.1_{10}$ can be expressed as $1 \\times 10^{-1}$. This fits the format perfectly. We can set the sign $s=0$, the coefficient $c=1$, and the exponent $q=-1$. Since the coefficient $c=1$ requires only one digit, it is well within the $7$-digit limit. Therefore, the $\\text{decimal32}$ format represents $0.1$ exactly.\nThe value, which we denote as $x_{\\text{d32}}$, is exactly $0.1$, or the rational number $\\frac{1}{10}$.\n\n**Part 3: Computation of the sequential sum $0.1 + 0.1 + 0.1$**\n\nWe compute the sum in each format, with rounding after each of the two additions.\n\nFor the **$\\text{decimal32}$ format**:\nThe initial value is $x_{\\text{d32}} = \\frac{1}{10}$.\nFirst addition: $S_1 = x_{\\text{d32}} + x_{\\text{d32}} = \\frac{1}{10} + \\frac{1}{10} = \\frac{2}{10} = 0.2$. This value can be represented exactly in $\\text{decimal32}$ format (with $c=2, q=-1$). No rounding is necessary.\nSecond addition: $S_2 = S_1 + x_{\\text{d32}} = \\frac{2}{10} + \\frac{1}{10} = \\frac{3}{10} = 0.3$. This value can also be represented exactly in $\\text{decimal32}$ format (with $c=3, q=-1$). No rounding is necessary.\nThe final sum in the $\\text{decimal32}$ format is $S_{\\text{d32}} = \\frac{3}{10}$.\n\nFor the **$\\text{binary32}$ format**:\nThe initial value is $x_{\\text{b32}} = \\frac{13421773}{2^{27}}$.\nFirst addition: $S'_1 = x_{\\text{b32}} + x_{\\text{b32}} = 2 \\times \\frac{13421773}{2^{27}} = \\frac{13421773}{2^{26}}$.\nThis operation corresponds to incrementing the exponent of $x_{\\text{b32}}$ by $1$ while keeping the significand unchanged. The result $\\frac{13421773}{2^{26}}$ is a valid $\\text{binary32}$ number, so it is stored exactly without rounding.\n\nSecond addition: $S'_2 = S'_1 + x_{\\text{b32}} = \\frac{13421773}{2^{26}} + \\frac{13421773}{2^{27}}$.\nTo perform this addition, we find a common denominator:\n$S'_2 = \\frac{2 \\cdot 13421773}{2^{27}} + \\frac{13421773}{2^{27}} = \\frac{3 \\cdot 13421773}{2^{27}} = \\frac{40265319}{134217728}$.\nThis is the exact mathematical result of the sum. Now, it must be rounded to the $\\text{binary32}$ format. The value is approximately $0.3$. The true exponent $e$ will be $-2$, since $2^{-2} = 0.25$ and $2^{-1} = 0.5$. The normalized form is $M' \\times 2^{-2}$.\nThe significand is $M' = S'_2 \\times 2^2 = \\frac{40265319}{134217728} \\times 4 = \\frac{40265319}{33554432}$.\nTo round this to $24$ bits of precision, we scale it by $2^{23}$:\nScaled significand integer $= M' \\times 2^{23} = \\frac{40265319}{33554432} \\times 2^{23} = \\frac{40265319}{2^{25}} \\times 2^{23} = \\frac{40265319}{2^2} = \\frac{40265319}{4} = 10066329.75$.\nThis result is exactly halfway between the integers $10066329$ and $10066330$, constituting a tie. The \"ties to even\" rule requires rounding to the even integer. The integer part, $10066329$, is odd. Therefore, we must round up to the nearest even integer, which is $10066330$.\nThe rounded $24$-bit significand corresponds to the integer $10066330$. Its value is $M'_{\\text{b32}} = \\frac{10066330}{2^{23}}$.\nThe final sum in the $\\text{binary32}$ format, $S_{\\text{b32}}$, is:\n$S_{\\text{b32}} = M'_{\\text{b32}} \\times 2^{-2} = \\frac{10066330}{2^{23}} \\times 2^{-2} = \\frac{10066330}{2^{25}} = \\frac{5033165}{16777216}$.\n\n**Part 4: Computation of the absolute difference**\n\nThe final sums are $S_{\\text{d32}} = \\frac{3}{10}$ and $S_{\\text{b32}} = \\frac{5033165}{16777216}$. The absolute difference $\\Delta$ is:\n$\\Delta = |S_{\\text{d32}} - S_{\\text{b32}}| = |\\frac{3}{10} - \\frac{5033165}{16777216}|$.\nWe place the terms over a common denominator:\n$\\Delta = |\\frac{3 \\cdot 16777216 - 10 \\cdot 5033165}{10 \\cdot 16777216}| = |\\frac{50331648 - 50331650}{167772160}|$.\n$\\Delta = |\\frac{-2}{167772160}| = \\frac{2}{167772160} = \\frac{1}{83886080}$.\nThis is the exact rational value of the difference.\n\nTo provide the final answer, we must express this value in decimal form and round it to four significant figures.\n$\\Delta = \\frac{1}{83886080} = 0.000000011920928955078125$.\nIn scientific notation, this is $1.19209289... \\times 10^{-8}$.\nRounding to four significant figures gives $1.192 \\times 10^{-8}$.",
            "answer": "$$\n\\boxed{1.192 \\times 10^{-8}}\n$$"
        },
        {
            "introduction": "Having seen how a single decimal value can be misrepresented, we now explore how these small inaccuracies accumulate in a sequence of operations. The expression $(0.1 + 0.2) - 0.3$ is famously counterintuitive in floating-point arithmetic, often not yielding the expected result of zero. This hands-on problem  requires you to step into the role of the processor, meticulously tracking the conversion and rounding at each stage of the calculation to uncover the exact non-zero result and understand why floating-point arithmetic does not obey the associative law of real numbers.",
            "id": "3641928",
            "problem": "A processor implements the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 (double precision) floating-point format with round-to-nearest, ties-to-even. In this format, every normalized finite number has the value\n$$\n(-1)^{s} \\times (1.f)_{2} \\times 2^{E},\n$$\nwhere $s \\in \\{0,1\\}$ is the sign bit, $f$ is a $52$-bit fraction (the hidden leading $1$ makes a $53$-bit significand), and $E = e_{\\mathrm{bias}} - 1023$ with $e_{\\mathrm{bias}}$ the stored $11$-bit biased exponent. The round-to-nearest, ties-to-even rule chooses the representable value closest in $\\mathbb{R}$, and if exactly halfway between two representable values, it chooses the one whose significand is even.\n\nThe decimal constants $0.1$, $0.2$, and $0.3$ are each first converted to the nearest binary64 value under this rounding rule, then the expression $(0.1 + 0.2) - 0.3$ is evaluated in this order, with one rounding after the addition and one after the subtraction, both according to the same rule.\n\nUsing only the definitions above and first principles about binary normalization and rounding, determine the exact real value produced by the hardware for $(0.1 + 0.2) - 0.3$ under binary64 with round-to-nearest, ties-to-even. Express your final answer as an exact power of two (no decimal approximation).",
            "solution": "The principles of the solution are to first convert each decimal constant to its nearest IEEE 754 binary64 representation, and then to perform the specified arithmetic operations in sequence, applying the rounding rule at each step. The binary64 format uses a $53$-bit significand (one implicit leading bit and $52$ explicit fraction bits). Arithmetic can be precisely analyzed by treating the significands as $53$-bit integers.\n\nA normalized floating-point number is represented as $v = (-1)^s \\times M \\times 2^E$, where $M$ is the significand satisfying $1 \\le M < 2$, and $E$ is the exponent. The significand $M$ is stored using $52$ bits for its fractional part, which means it can be written as $M = K \\cdot 2^{-52}$ for some integer $K$ where $2^{52} \\le K < 2^{53}$. The value of the number can therefore be expressed as $v = (-1)^s \\times K \\times 2^{E-52}$. We will determine the integer significand $K$ and the effective exponent for each value. The rounding rule is round-to-nearest, ties-to-even.\n\n\\textbf{Step 1: Conversion of decimal constants to binary64}\n\n\\textbf{Conversion of $0.1$:}\nWe want to represent $0.1$ in the form $M \\times 2^E$, where $1 \\le M < 2$. The exponent $E$ is found by $E = \\lfloor \\log_2(0.1) \\rfloor = \\lfloor -3.3219... \\rfloor = -4$.\nThe ideal significand is $M_{0.1, \\text{ideal}} = 0.1 \\times 2^4 = 1.6$.\nTo find the corresponding $53$-bit integer significand $K$, we scale by $2^{52}$.\n$K_{0.1, \\text{ideal}} = 1.6 \\times 2^{52} = \\frac{8}{5} \\times 2^{52} = \\frac{2^3 \\times 2^{52}}{5} = \\frac{2^{55}}{5}$.\n$2^{55} = 36028797018963968$.\n$K_{0.1, \\text{ideal}} = \\frac{36028797018963968}{5} = 7205759403792793.6$.\nThis value is not an integer. We must round it to the nearest $53$-bit integer. The candidates are $K_a = 7205759403792793$ and $K_b = 7205759403792794$. Since $0.6$ is greater than $0.5$, we round up.\nThe integer significand is $K_{0.1} = 7205759403792794$.\nThe binary64 representation of $0.1$, let's call it $X_{0.1}$, is:\n$X_{0.1} = (K_{0.1} \\cdot 2^{-52}) \\times 2^{-4} = 7205759403792794 \\times 2^{-56}$.\n\n\\textbf{Conversion of $0.2$:}\nWe represent $0.2$ as $M \\times 2^E$. The exponent is $E = \\lfloor \\log_2(0.2) \\rfloor = \\lfloor -2.3219... \\rfloor = -3$.\nThe ideal significand is $M_{0.2, \\text{ideal}} = 0.2 \\times 2^3 = 1.6$.\nThis is the same ideal significand as for $0.1$. Therefore, it rounds to the same integer significand.\n$K_{0.2} = 7205759403792794$.\nThe binary64 representation of $0.2$, denoted $X_{0.2}$, is:\n$X_{0.2} = (K_{0.2} \\cdot 2^{-52}) \\times 2^{-3} = 7205759403792794 \\times 2^{-55}$.\n\n\\textbf{Conversion of $0.3$:}\nWe represent $0.3$ as $M \\times 2^E$. The exponent is $E = \\lfloor \\log_2(0.3) \\rfloor = \\lfloor -1.7369... \\rfloor = -2$.\nThe ideal significand is $M_{0.3, \\text{ideal}} = 0.3 \\times 2^2 = 1.2$.\nThe ideal integer significand is $K_{0.3, \\text{ideal}} = 1.2 \\times 2^{52} = \\frac{6}{5} \\times 2^{52} = \\frac{3 \\times 2^{53}}{5}$.\n$2^{53} = 9007199254740992$.\n$K_{0.3, \\text{ideal}} = \\frac{3 \\times 9007199254740992}{5} = \\frac{27021597764222976}{5} = 5404319552844595.2$.\nThis value is not an integer. We must round it to the nearest integer. The candidates are $K_c = 5404319552844595$ and $K_d = 5404319552844596$. Since $0.2$ is less than $0.5$, we round down.\nThe integer significand is $K_{0.3} = 5404319552844595$.\nThe binary64 representation of $0.3$, denoted $X_{0.3}$, is:\n$X_{0.3} = (K_{0.3} \\cdot 2^{-52}) \\times 2^{-2} = 5404319552844595 \\times 2^{-54}$.\n\n\\textbf{Step 2: Evaluate the sum $(X_{0.1} + X_{0.2})$}\nTo add $X_{0.1}$ and $X_{0.2}$, we must first align their exponents. We use the smaller exponent, $-56$.\n$X_{0.1} = 7205759403792794 \\times 2^{-56}$\n$X_{0.2} = 7205759403792794 \\times 2^{-55} = (2 \\times 7205759403792794) \\times 2^{-56} = 14411518807585588 \\times 2^{-56}$.\nThe exact sum, $S_{\\text{exact}}$, is:\n$S_{\\text{exact}} = (7205759403792794 + 14411518807585588) \\times 2^{-56} = 21617278211378382 \\times 2^{-56}$.\nThe resulting integer significand, $21617278211378382$, is a $55$-bit integer (since $2^{54} < 21617... < 2^{55}$). To normalize, we must express this result with a $53$-bit integer significand. This requires adjusting the exponent.\n$S_{\\text{exact}} = (21617278211378382 \\cdot 2^{-2}) \\times 2^{-54} = 5404319552844595.5 \\times 2^{-54}$.\nThe value to be rounded is represented by the integer significand $5404319552844595.5$ at an exponent of $-54$. This value lies exactly halfway between two representable values whose integer significands are $K_e = 5404319552844595$ and $K_f = 5404319552844596$.\nAccording to the ties-to-even rule, we must choose the one whose significand is even. $K_e$ is odd, and $K_f$ is even. Thus, we choose $K_f$.\nThe rounded sum, $X_{0.1+0.2}$, is:\n$X_{0.1+0.2} = 5404319552844596 \\times 2^{-54}$.\n\n\\textbf{Step 3: Evaluate the subtraction $(X_{0.1+0.2} - X_{0.3})$}\nWe now subtract the stored value for $0.3$ from the result of the addition.\n$X_{0.1+0.2} = 5404319552844596 \\times 2^{-54}$\n$X_{0.3} = 5404319552844595 \\times 2^{-54}$\nThe exponents are already aligned. The subtraction can be performed directly on the integer significands.\n$D = (5404319552844596 - 5404319552844595) \\times 2^{-54}$\n$D = 1 \\times 2^{-54} = 2^{-54}$.\nThe result of the subtraction is the exact value $2^{-54}$. This value must be stored in binary64 format, which may require a final rounding. However, $2^{-54}$ is an exactly representable normalized binary64 number. It can be written as $1.0 \\times 2^{-54}$. Here, the significand is $M=1.0$ (which corresponds to an integer significand $K=2^{52}$ with an exponent of $E-52 = -54-52 = -106$) and the exponent is $E = -54$. The exponent is within the valid range for normalized numbers ($-1022$ to $1023$). Since the value is exactly representable, no rounding is performed.\n\nTherefore, the final value produced by the hardware is exactly $2^{-54}$.",
            "answer": "$$\\boxed{2^{-54}}$$"
        },
        {
            "introduction": "We now turn to a different, yet equally critical, source of error: loss of significance when numbers of vastly different scales are combined. In the world of real numbers, the identity $(a+b)-a=b$ is trivial, but this often fails dramatically in floating-point systems. This practice  introduces the concept of a Unit in the Last Place (ULP) to explain this phenomenon, demonstrating how a small number can be completely \"swamped\" and lost when added to a much larger one. Understanding this behavior is essential for writing numerically stable code that handles a wide dynamic range of inputs.",
            "id": "3641990",
            "problem": "A software engineer writes a numerical kernel in compliance with the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) binary32 (single precision), using the rounding mode \"round to nearest, ties to even.\" Let $\\operatorname{fl}(\\cdot)$ denote the floating-point result of a single operation in this format. Consider the computed quantity\n$$\ny \\;=\\; \\operatorname{fl}\\!\\left(\\,\\operatorname{fl}(a + b)\\;-\\;a\\,\\right),\n$$\nwhich mirrors a naive code pattern for residuals. The engineer notices that in real arithmetic $((a+b)-a)=b$, but in floating-point arithmetic this identity can fail.\n\nWork in the following model:\n- A normalized binary32 number has the form $x = (-1)^{s}\\,(1.f)_2\\,2^{E-127}$, where $s \\in \\{0,1\\}$, $f$ is a $23$-bit fraction, and $E \\in \\{1,2,\\dots,254\\}$ is the biased exponent. The precision is $p=24$ significant bits (including the implicit leading $1$).\n- The \"Unit in the Last Place (ULP)\" at a normalized magnitude $x$ with unbiased exponent $e$ is $\\operatorname{ulp}(x)=2^{e-(p-1)}$.\n- A single correctly rounded operation $\\operatorname{fl}(x \\circ y)$ for $\\circ \\in \\{+,-,\\times,\\div\\}$ returns the unique floating-point number nearest to the exact result, with ties broken toward the even significand.\n\nTasks:\n1. Starting from these definitions, derive an expression for $y$ in terms of $a$, $b$, and the rounding error of the single addition. Use only the definitions above and basic properties of exponent alignment in floating-point addition. Conclude a necessary and sufficient condition under which $y=b$ in this model, expressed in terms of exact representability of $a+b$ in binary32.\n2. Specialize your conclusion to the case where $a$ is a normalized number and $b$ has the same sign as $a$ and satisfies $|b| \\ll |a|$. Express your condition using $\\operatorname{ulp}(a)$ and explain its meaning in terms of the exponent alignment step of a floating-point adder.\n3. Now take $a=10^8$ and $b=1$, interpret these as real inputs to a binary32 computation, and evaluate the binary32 value of $y=\\operatorname{fl}(\\operatorname{fl}(a+b)-a)$ exactly. Your final answer must be a single real number. No rounding is required for the final answer.",
            "solution": "We begin from the core definitions of IEEE 754 binary32 (single precision) and the semantics of correctly rounded arithmetic. A normalized floating-point number has the form $x = (-1)^{s}\\,(1.f)_2\\,2^{E-127}$, with $p=24$ significant bits. The \"Unit in the Last Place (ULP)\" near a normalized $x$ with unbiased exponent $e$ is $\\operatorname{ulp}(x)=2^{e-(p-1)}=2^{e-23}$, which is the spacing between consecutive floating-point numbers in the binade defined by $e$.\n\nStep 1: Expressing $y$ via the rounding error of a single addition and a subtraction.\n\nLet us denote the exact real sum by $s = a+b$. By correct rounding to nearest, ties to even, there exists a rounding error $\\delta$ such that\n$$\n\\operatorname{fl}(a+b) \\;=\\; s + \\delta \\;=\\; a + b + \\delta,\n$$\nwith the constraint $|\\delta| \\le \\tfrac{1}{2}\\,\\operatorname{ulp}(s)$. For $|b|\\ll |a|$ and $a$ normalized, the binade of $s$ typically coincides with that of $a$ (no exponent overflow into the next binade), and then $|\\delta| \\le \\tfrac{1}{2}\\,\\operatorname{ulp}(a)$.\n\nThe computed quantity is\n$$\ny \\;=\\; \\operatorname{fl}\\!\\left(\\operatorname{fl}(a+b) - a\\right) \\;=\\; \\operatorname{fl}\\!\\left((a+b+\\delta)-a\\right) \\;=\\; \\operatorname{fl}\\!\\left(b+\\delta\\right).\n$$\nNow we use the well-known exactness property of subtraction in floating-point arithmetic when two floating-point numbers are within a factor of $2$ of each other. Specifically, by the Sterbenz lemma (a standard result derivable from exponent alignment and normalization), if $x$ and $y$ are floating-point and $\\tfrac{1}{2}\\le \\tfrac{x}{y}\\le 2$, then $x-y$ is exactly representable in the same format. In our setting, $\\operatorname{fl}(a+b)$ and $a$ are both floating-point numbers, and when $|b|<|a|$ they lie within a factor of $2$ of each other, so the subtraction $\\operatorname{fl}(a+b)-a$ is exact. Therefore,\n$$\ny \\;=\\; \\left(\\operatorname{fl}(a+b)\\right)-a \\;=\\; (a+b+\\delta)-a \\;=\\; b+\\delta.\n$$\nIt follows that $y=b$ if and only if $\\delta=0$, that is, if and only if $\\operatorname{fl}(a+b)=a+b$ (exact representability of the sum in binary32 with no rounding). This establishes the necessary and sufficient condition:\n$$\ny=b \\quad\\Longleftrightarrow\\quad a+b \\text{ is exactly representable in binary32}.\n$$\n\nStep 2: Specialization using $\\operatorname{ulp}(a)$ and exponent alignment.\n\nIn a binary adder, to compute $a+b$, the significands are aligned by shifting the significand of the operand with smaller unbiased exponent right by $d$ bit positions, where $d$ is the exponent difference. If $|b|\\ll |a|$ and $a$ and $b$ have the same sign, then after alignment, the contribution of $b$ to the sum at the scale of $a$ is effectively quantized on the $\\operatorname{ulp}(a)$ grid. Concretely, when $a$ remains in the same binade after addition, we have\n$$\n\\operatorname{ulp}(a)=2^{e-23}, \\quad \\text{where } e=\\lfloor \\log_{2}|a| \\rfloor,\n$$\nand the sum $a+b$ is exactly representable if and only if $b$ is an integer multiple of $\\operatorname{ulp}(a)$ and the addition does not cause a renormalization of the exponent to $e+1$. Equivalently, the rounding error $\\delta$ vanishes if and only if $b \\in \\operatorname{ulp}(a)\\,\\mathbb{Z}$ and $a+b$ stays in the binade with unbiased exponent $e$. Intuitively, this states that equality $y=b$ holds precisely when $b$ is \"scaled\" to the local grid defined by the spacing of representable numbers near $a$; if $b$ lies off that grid, alignment plus rounding forces a nonzero $\\delta$ and $y\\ne b$.\n\nStep 3: Computation for $a=10^8$ and $b=1$.\n\nWe now evaluate $y=\\operatorname{fl}(\\operatorname{fl}(a+b)-a)$ in binary32 for $a=10^8$ and $b=1$.\n\nFirst, determine the unbiased exponent $e$ of $a$:\n$$\n2^{26} \\;=\\; 67{,}108{,}864 \\;<\\; 10^{8} \\;<\\; 134{,}217{,}728 \\;=\\; 2^{27},\n$$\nso $e=\\lfloor \\log_{2}(10^{8}) \\rfloor = 26$. Hence the local spacing at $a$ is\n$$\n\\operatorname{ulp}(a) \\;=\\; 2^{e-23} \\;=\\; 2^{3} \\;=\\; 8.\n$$\nBecause $|b|=1<\\tfrac{1}{2}\\operatorname{ulp}(a)=4$, the correctly rounded sum satisfies\n$$\n\\operatorname{fl}(a+b) \\;=\\; a \\quad\\text{(round to nearest, ties to even)}.\n$$\nThus the rounding error on the addition is $\\delta = -b = -1$. As established in Step 1, the final computed value is $y=b+\\delta$, which yields $y=1+(-1)=0$.\nTherefore the computed value $y$ in binary32 is exactly $0$. This exhibits the counterexample in which $((a+b)-a)$ fails to equal $b$ unless $b$ is scaled to the $\\operatorname{ulp}(a)$ grid (here $\\operatorname{ulp}(a)=8$, and $b=1$ is not an integer multiple of $8$).",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}