{
    "hands_on_practices": [
        {
            "introduction": "Before embarking on a sequencing project, it is crucial to estimate the sheer volume of data you will be handling. This exercise walks you through a practical calculation that connects key experimental parameters—like genome size and desired coverage depth—to the final data footprint. By dissecting the structure of a standard FASTQ file, you will gain a tangible understanding of what \"$30\\times$ coverage\" means in terms of gigabytes, a critical skill for experimental design and data management .",
            "id": "2417496",
            "problem": "A whole-genome sequencing experiment targets a haploid human genome of size $3.2\\times 10^{9}$ base pairs at $30\\times$ depth using $150$ base pair paired-end reads on a platform that outputs raw reads in the FASTQ (Fast All-in-one Sequence with Quality) text format. Assume the following:\n\n- Coverage depth is defined as total sequenced bases divided by the haploid genome size.\n- Each read has length $L=150$ bases and is represented in a FASTQ record of four lines: a header line beginning with the character $@$; a sequence line of $L$ nucleotide characters; a plus line beginning with the character $+$ and containing no additional annotation; and a quality line of $L$ American Standard Code for Information Interchange (ASCII) characters encoding per-base quality scores.\n- All characters are single-byte ASCII, and each line is terminated by a single newline character of $1$ byte.\n- The header line for each read contains exactly $40$ ASCII characters (including the initial $@$) before the newline.\n- The FASTQ files are uncompressed.\n- Use the decimal definition of a gigabyte, where $1 \\text{ gigabyte} = 10^{9}$ bytes.\n\nEstimate the total amount of data generated, in gigabytes, for the entire experiment. Round your answer to three significant figures and express it in gigabytes.",
            "solution": "The problem is subjected to validation.\n\nGivens extracted verbatim from the problem statement:\n- Haploid human genome size: $G = 3.2 \\times 10^9$ base pairs\n- Target coverage depth: $C = 30\\times$\n- Read length: $L = 150$ base pairs\n- Read type: paired-end\n- Output format: FASTQ\n- FASTQ record structure:\n    - Header line: $40$ characters (including $@$) + $1$ newline\n    - Sequence line: $L$ characters + $1$ newline\n    - Plus line: $1$ character ($+$) + $1$ newline\n    - Quality line: $L$ characters + $1$ newline\n- Character encoding: single-byte ASCII\n- Line terminator: $1$ byte newline\n- File format: uncompressed\n- Unit conversion: $1 \\text{ gigabyte} = 10^9$ bytes\n- Required precision: round the final answer to three significant figures\n\nValidation verdict:\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard bioinformatics scenario for whole-genome sequencing. The parameters provided, such as genome size ($G$), coverage depth ($C$), and read length ($L$), are realistic values for a human sequencing project. The description of the FASTQ format is accurate and sufficient for the calculation. The problem is self-contained, consistent, and requires a straightforward, formalizable calculation. Thus, the problem is deemed valid and a solution will be provided.\n\nThe objective is to calculate the total size of the data generated by the sequencing experiment in gigabytes.\n\nFirst, we calculate the total number of bases sequenced, denoted by $B_{total}$. This is given by the product of the genome size, $G = 3.2 \\times 10^9$ base pairs, and the coverage depth, $C = 30$.\n$$B_{total} = G \\times C$$\nSubstituting the given values:\n$$B_{total} = (3.2 \\times 10^9) \\times 30 = 9.6 \\times 10^{10} \\text{ bases}$$\n\nNext, we determine the total number of individual reads, $N_{reads}$. Since each read has a length of $L = 150$ bases, the total number of reads is the total number of sequenced bases divided by the length of a single read.\n$$N_{reads} = \\frac{B_{total}}{L}$$\nSubstituting the values for $B_{total}$ and $L$:\n$$N_{reads} = \\frac{9.6 \\times 10^{10}}{150} = \\frac{96 \\times 10^9}{150} = 0.64 \\times 10^9 = 6.4 \\times 10^8 \\text{ reads}$$\nEach read corresponds to one FASTQ record. Therefore, there are $6.4 \\times 10^8$ FASTQ records in total.\n\nNow, we calculate the size of a single FASTQ record, $S_{read}$, in bytes. The problem specifies that all characters are single-byte ASCII ($1$ byte per character) and each line is terminated by a single newline character ($1$ byte). The FASTQ record consists of $4$ lines:\n1.  Header line: This line contains $40$ characters before the newline. The total size is $40 \\text{ bytes} + 1 \\text{ byte} = 41 \\text{ bytes}$.\n2.  Sequence line: This line contains $L = 150$ nucleotide characters. The total size is $150 \\text{ bytes} + 1 \\text{ byte} = 151 \\text{ bytes}$.\n3.  Plus line: This line contains the single character `$+$`. The total size is $1 \\text{ byte} + 1 \\text{ byte} = 2 \\text{ bytes}$.\n4.  Quality line: This line contains $L = 150$ quality score characters. The total size is $150 \\text{ bytes} + 1 \\text{ byte} = 151 \\text{ bytes}$.\n\nThe total size of one FASTQ record is the sum of the sizes of these four lines:\n$$S_{read} = 41 + 151 + 2 + 151 = 345 \\text{ bytes}$$\n\nThe total amount of data generated, $S_{total}$, is the product of the total number of reads, $N_{reads}$, and the size of a single read's record, $S_{read}$.\n$$S_{total} = N_{reads} \\times S_{read}$$\nSubstituting the calculated values:\n$$S_{total} = (6.4 \\times 10^8) \\times 345 = 2208 \\times 10^8 = 2.208 \\times 10^{11} \\text{ bytes}$$\n\nFinally, we convert the total size from bytes to gigabytes (GB), using the given conversion factor $1 \\text{ GB} = 10^9 \\text{ bytes}$.\n$$S_{total, \\text{GB}} = \\frac{S_{total}}{10^9} = \\frac{2.208 \\times 10^{11}}{10^9} = 2.208 \\times 10^2 = 220.8 \\text{ GB}$$\n\nThe problem requires the answer to be rounded to three significant figures. The value $220.8$ rounded to three significant figures is $221$.",
            "answer": "$$\\boxed{221}$$"
        },
        {
            "introduction": "Raw sequencing data is rarely perfect and often contains artifacts from the molecular biology steps of library preparation. One of the most common issues is the formation of \"adapter-dimers,\" which can contaminate a dataset and skew downstream analyses. This problem challenges you to act like a bioinformatician diagnosing a sick dataset: given the tell-tale signs of adapter contamination, you must identify its origin in the lab and select the correct computational workflow to remove it .",
            "id": "2417424",
            "problem": "A paired-end Next-Generation Sequencing (NGS) run yields a high proportion of reads dominated by known adapter motifs. Quality control reports indicate strong adapter content from the start of many reads, and after adapter removal many reads have negligible insert length. Which step in the library construction most plausibly produced these sequences, and what computational strategy is most appropriate to remove them before downstream analysis?\n\nA. Excess adapter molecules during the adapter ligation step led to adapter–adapter ligation products, and insufficient post-ligation size selection allowed these short constructs to persist; computationally, perform adapter-aware trimming using the known adapter sequences on each read (or read pair), then discard any read or read pair whose post-trim insert length is below a minimum threshold $\\ell_{\\min}$, indicating that the sequence is primarily adapter.\n\nB. Over-fragmentation during mechanical shearing generated ultra-short inserts; computationally, increase the base-quality trimming threshold so that reads with average Phred quality below a cutoff $Q_{\\min}$ are discarded.\n\nC. Index hopping during cluster amplification created spurious chimeric reads; computationally, demultiplex with stricter barcode mismatch tolerance by reducing the allowed number of index mismatches to $m_{\\max}$ close to zero.\n\nD. Incomplete end-repair produced hairpin-like molecules that sequenced aberrantly; computationally, align all reads to a reference genome and discard any read that remains unmapped after up to $k$ allowed mismatches.\n\nE. Primer-dimer formation during Polymerase Chain Reaction (PCR) created short amplicons; computationally, remove highly abundant $k$-mers by collapsing reads that share identical $k$-mer profiles to a representative sequence.",
            "solution": "The validity of the problem statement must first be established.\n\n### Step 1: Extract Givens\nThe problem provides the following observations from a paired-end Next-Generation Sequencing (NGS) run:\n1.  A high proportion of reads are dominated by known adapter motifs.\n2.  Quality control reports indicate strong adapter content from the start of many reads.\n3.  After adapter removal, many reads have negligible insert length.\n\nThe problem asks for the most plausible cause of these observations during the library construction phase and the most appropriate computational strategy to remove the resulting artifactual sequences before downstream analysis.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement will now be assessed for scientific validity and clarity.\n- **Scientific Groundedness**: The scenario described is a classic and frequent artifact in NGS library preparation. The concepts presented—paired-end sequencing, adapter motifs, quality control, insert length, adapter ligation, size selection, and various computational filtering strategies—are fundamental and well-established in the field of genomics and bioinformatics. The observations are consistent with the formation of so-called \"adapter-dimers\". This is not speculative.\n- **Well-Posedness**: The problem is well-posed. The set of observations points strongly toward a specific molecular artifact, and the question requests the identification of the generating mechanism and the corresponding corrective computational procedure. A standard, unique, and well-accepted answer exists within the discipline.\n- **Objectivity**: The language is technical and objective. Phrases like \"high proportion,\" \"strong adapter content,\" and \"negligible insert length\" are standard descriptors derived from common QC software outputs (e.g., FastQC). There is no ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem statement is scientifically sound, well-posed, and objective. It is based on a realistic and common scenario in experimental genomics. Therefore, the problem is **valid**. A full solution will be derived.\n\n### Derivation and Option Analysis\n\nThe provided observations must be logically connected to a mechanism in library preparation.\n1.  **\"Adapter content from the start of many reads\"**: In a standard sequencing library, a read begins at the 5' end of the DNA insert. Adapter sequences are ligated to the ends of this insert. Therefore, adapter sequence should only appear at the 3' end of a read, and only if the DNA insert is shorter than the sequencing read length (a phenomenon known as \"read-through\"). The presence of adapter sequence at the very beginning (5' end) of the read indicates that the sequencing process began directly on an adapter molecule, not a DNA insert.\n2.  **\"High proportion of reads dominated by known adapter motifs\" and \"negligible insert length after adapter removal\"**: These two points reinforce the first. If a read consists almost entirely of adapter sequence, then after computationally trimming this adapter sequence away, the remaining \"insert\" will have a length close to zero. The high abundance suggests a systematic issue in the library preparation.\n\nThe only plausible mechanism that produces a sequencing template consisting of two adapters ligated together is the formation of **adapter-dimers**. This occurs during the adapter ligation step. If the molar concentration of adapter molecules is excessively high relative to the concentration of DNA fragments, the ligase is more likely to join two adapter molecules together than to join an adapter to a DNA fragment. These adapter-dimer constructs are short, typically between $120$ and $150$ base pairs.\n\nA subsequent step in library preparation is size selection, which is designed to enrich for the desired fragment size distribution (e.g., $300$-$500$ base pairs) and eliminate very short fragments. If this size selection step is performed suboptimally or is not sufficiently stringent, these short adapter-dimer constructs will be retained in the final library, amplified by PCR, and sequenced.\n\nThe computational strategy to correct this issue must specifically target these artifactual reads. The strategy is twofold:\n1.  **Identification**: Use an adapter trimming tool (e.g., `cutadapt`, `Trimmomatic`) with the known sequences of the adapters to identify and remove adapter content from all reads.\n2.  **Filtering**: After trimming, reads that originated from adapter-dimers will be very short. Therefore, a length filter must be applied to discard any read (or, in this paired-end case, the entire read pair) if the remaining sequence length falls below a specified minimum threshold, $\\ell_{\\min}$.\n\nNow, each option will be evaluated against this derived understanding.\n\n**A. Excess adapter molecules during the adapter ligation step led to adapter–adapter ligation products, and insufficient post-ligation size selection allowed these short constructs to persist; computationally, perform adapter-aware trimming using the known adapter sequences on each read (or read pair), then discard any read or read pair whose post-trim insert length is below a minimum threshold $\\ell_{\\min}$, indicating that the sequence is primarily adapter.**\nThis option correctly identifies the cause: adapter-dimer formation from excess adapter concentration, which is then inadequately removed by size selection. The proposed computational strategy—adapter trimming followed by filtering based on a minimum length threshold $\\ell_{\\min}$—is precisely the standard and most effective method for removing these artifacts.\n**Verdict: Correct.**\n\n**B. Over-fragmentation during mechanical shearing generated ultra-short inserts; computationally, increase the base-quality trimming threshold so that reads with average Phred quality below a cutoff $Q_{\\min}$ are discarded.**\nThe cause is inconsistent with the evidence. Ultra-short inserts would lead to adapter read-through at the 3' end of reads, not adapter content at the 5' start. The computational strategy is also incorrect. The problem is one of sequence content (adapter vs. insert), not base quality. While quality can be a proxy for certain issues, it is not the direct solution here. Discarding reads based on an average quality cutoff $Q_{\\min}$ would not specifically target adapter-dimers.\n**Verdict: Incorrect.**\n\n**C. Index hopping during cluster amplification created spurious chimeric reads; computationally, demultiplex with stricter barcode mismatch tolerance by reducing the allowed number of index mismatches to $m_{\\max}$ close to zero.**\nThis describes a completely different artifact. Index hopping (or index misassignment) leads to reads from one sample being incorrectly assigned to another sample during demultiplexing. It does not create reads that are composed entirely of adapter sequence. The cause and the proposed solution are irrelevant to the problem described.\n**Verdict: Incorrect.**\n\n**D. Incomplete end-repair produced hairpin-like molecules that sequenced aberrantly; computationally, align all reads to a reference genome and discard any read that remains unmapped after up to $k$ allowed mismatches.**\nIncomplete end-repair or A-tailing results in reduced ligation efficiency, leading to lower library yield, not a specific artifact of high abundance. The proposed computational strategy is also suboptimal. Using alignment as the primary filter for adapter-dimers is computationally expensive and presupposes the existence of a reference genome. The standard workflow is to perform adapter/quality trimming and filtering *before* alignment.\n**Verdict: Incorrect.**\n\n**E. Primer-dimer formation during Polymerase Chain Reaction (PCR) created short amplicons; computationally, remove highly abundant $k$-mers by collapsing reads that share identical $k$-mer profiles to a representative sequence.**\nWhile primer-dimers can form during PCR, the problem specifies reads dominated by \"known adapter motifs.\" In this context, \"adapter\" typically refers to the ligation adapters, not the PCR primers that anneal to a site on the adapters. Adapter-dimers form during ligation, which is pre-PCR. The computational strategy described is a form of digital normalization or error correction based on $k$-mer counts, which is not the standard procedure for removing adapter contamination. The goal is complete removal of these artifactual reads, not collapsing them to a single representative sequence.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "At the heart of modern variant calling lies a probabilistic framework that weighs evidence for and against a potential genetic mutation. The Phred quality score of each base is not just a quality metric; it is a critical input that tells the variant caller how much to trust that piece of data. This thought experiment explores a subtle but critical failure mode: what happens when base quality scores are systematically wrong? By analyzing this hypothetical scenario, you will gain a deeper appreciation for how Bayesian statistical models work and why accurate data calibration is essential for reliable genomic discovery .",
            "id": "2417416",
            "problem": "A whole-genome sequencing run produces read base calls whose base-calling software is miscalibrated, assigning the same Phred quality score $Q=40$ to every base. The Phred definition is $Q=-10\\log_{10} p_e$, where $p_e$ is the per-base sequencing error probability. A Genome Analysis Toolkit (GATK)-like Bayesian variant caller uses per-base error probabilities to compute genotype likelihoods from observed reads. Consider a locus with $n=30$ high mapping-quality reads, of which $k=5$ report an alternate base and $n-k$ report the reference base. Assume independence of per-base sequencing errors given the underlying genotype and that there are no other confounders.\n\nWhich statement best describes the expected impact of the uniform $Q=40$ assignment on downstream variant calling in such a model?\n\nA. The caller will produce systematically overconfident evidence for variation (inflated variant quality and genotype quality), increasing false positives by treating mismatches as nearly impossible under the homozygous-reference model and by losing the ability to downweight genuinely low-quality bases.\n\nB. There will be essentially no effect because GATK-like callers use only read counts and allelic balance, not base quality scores, when computing genotype likelihoods.\n\nC. Uniform high $Q$ will force the caller to be conservative, reducing sensitivity and false positives, because it cannot distinguish good from bad bases and thus lowers confidence in all calls.\n\nD. Variant calling will fail entirely because constant $Q$ disables per-read likelihood computation in GATK-like models.",
            "solution": "The Phred quality score definition $Q=-10\\log_{10} p_e$ implies that $Q=40$ corresponds to a per-base error probability $p_e=10^{-4}$. In a Genome Analysis Toolkit (GATK)-like Bayesian framework, per-read likelihoods are computed by treating an observed base that matches the expected allele as having probability approximately $(1-p_e)$ and a base that mismatches the expected allele as having probability approximately $p_e$, with appropriate handling of allele sampling for heterozygous genotypes and haplotype alignment. The total likelihood under a genotype is then the product (or equivalently, the sum of logarithms) of per-read emission probabilities across reads.\n\nTo reason from first principles, compare the likelihood of the data under two competing genotype hypotheses at the site: homozygous reference versus heterozygous reference/alternate. With $n=30$ and $k=5$ alternate observations:\n\n- Under the homozygous-reference hypothesis, the $k$ observed alternate bases must be explained as sequencing errors. With $p_e=10^{-4}$, the contribution from those $k$ reads is approximately $p_e^{k}=(10^{-4})^{5}=10^{-20}$, multiplied by a combinatorial factor corresponding to which reads show errors. The probability of observing exactly $k=5$ such errors among $n=30$ independent reads under the binomial model is\n$$\n\\Pr(K=5\\mid \\text{hom-ref})=\\binom{30}{5} p_e^{5} (1-p_e)^{25}\\approx \\binom{30}{5}\\cdot 10^{-20}\\cdot (1-10^{-4})^{25}\\,,\n$$\nwhich is on the order of $10^{-15}$ after accounting for $\\binom{30}{5}=142{,}506$.\n\n- Under the heterozygous hypothesis, ignoring sequencing errors relative to the chosen haplotype for this qualitative comparison (since $p_e$ is tiny), the count $k$ of alternate reads follows approximately a binomial distribution with success probability $1/2$. Thus,\n$$\n\\Pr(K=5\\mid \\text{het})\\approx \\binom{30}{5}\\left(\\tfrac{1}{2}\\right)^{30}\\,,\n$$\nwhich evaluates to approximately $1.33\\times 10^{-4}$.\n\nThe likelihood ratio favoring the heterozygous over the homozygous-reference model is therefore astronomically large (on the order of $10^{11}$ in this example), driven by the extremely small $p_e$ implied by $Q=40$. If instead the true, properly calibrated qualities were lower (for instance, $Q=20$ so $p_e=10^{-2}$), then\n$$\n\\Pr(K=5\\mid \\text{hom-ref})=\\binom{30}{5} (10^{-2})^{5} (1-10^{-2})^{25}\\approx 1.1\\times 10^{-5}\\,,\n$$\nso the data would still favor the heterozygote but by a much smaller factor. Thus, inflating all base qualities to $Q=40$ makes mismatches appear nearly impossible under the homozygous-reference genotype, disproportionately boosting the relative likelihood of variant genotypes wherever any alternate-supporting reads are present. It also removes the information normally carried by the variability of base qualities across cycles, contexts, and reads, preventing the caller from downweighting genuinely error-prone positions. The net effect is overconfident genotype likelihoods, inflated variant quality and genotype quality scores, and an increased false-positive rate for artifacts not captured by base quality (for example, certain context-specific or mapping-related errors), even as true-positive calls may also receive overinflated confidences.\n\nOption-by-option analysis:\n\n- Option A: This matches the first-principles reasoning. With $Q=40$ for all bases, $p_e=10^{-4}$ is used uniformly, making observed mismatches exceedingly unlikely under the homozygous-reference model. This shifts likelihoods toward variant genotypes and inflates posterior confidences (QUAL, genotype quality), while the loss of base-quality variability removes the caller’s ability to downweight truly low-quality observations. These effects increase false positives, particularly for artifacts not modeled by base quality. Verdict: Correct.\n\n- Option B: Incorrect. GATK-like models explicitly incorporate base quality scores into per-base emission probabilities (for example, in pair-Hidden Markov Model scoring in haplotype-based callers). Ignoring base qualities would contradict how these models compute genotype likelihoods.\n\n- Option C: Incorrect. Uniformly high $Q$ does not make the caller conservative; it makes the caller overconfident because it encodes an extremely low error probability for every base. Confidence increases, not decreases, and the inability to distinguish good from bad bases removes an important downweighting mechanism rather than lowering all confidences.\n\n- Option D: Incorrect. Constant $Q$ does not disable likelihood computation; the model still computes per-read likelihoods using the provided (though miscalibrated) $p_e$. Calls will not fail to be produced; instead, their confidences will be miscalibrated and generally inflated.\n\nTherefore, the best answer is Option A.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}