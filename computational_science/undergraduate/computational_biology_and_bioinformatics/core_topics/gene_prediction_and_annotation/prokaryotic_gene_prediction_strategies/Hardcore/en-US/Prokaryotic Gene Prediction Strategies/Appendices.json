{
    "hands_on_practices": [
        {
            "introduction": "A primary step in identifying genes is to scan a genome for Open Reading Frames (ORFs)—sequences that begin with a start codon and end with an in-frame stop codon. However, a fundamental challenge in bioinformatics is that such features can arise purely by chance. This practice asks you to quantify this effect by calculating the expected number of spurious ORFs that would appear in a random DNA sequence model.\n\nBy working through this problem , you will develop a crucial intuition for the statistical \"noise\" that gene prediction algorithms must overcome. You will see firsthand how factors like genomic base composition and the minimum length threshold ($L_{\\min}$) dramatically influence the rate of false positives, setting the stage for why more sophisticated prediction methods are necessary.",
            "id": "2419180",
            "problem": "You are given a formal model of Open Reading Frame (ORF) prediction applied to Deoxyribonucleic Acid (DNA) sequences from prokaryotes. Assume a genome is generated as an independent and identically distributed sequence over the alphabet $\\{A,C,G,T\\}$ with base probabilities $p_A$, $p_C$, $p_G$, and $p_T$ that sum to $1$. An Open Reading Frame (ORF) is defined on a chosen strand and reading frame as follows: starting at a position aligned to the reading frame, if the triplet equals one of the canonical start codons $\\{ATG,TTG,GTG\\}$, translation proceeds codon by codon in the same frame until the first encountered in-frame stop codon from the set $\\{TAA,TAG,TGA\\}$. The ORF is the sequence from the start codon up to but not including the stop codon. The ORF length (in codons) is defined as the number of codons from and including the start codon up to but not including the stop codon.\n\nA predicted ORF is any such ORF whose length is at least a specified minimum length $L_{\\min}$ (in codons, as defined above). A standard ORF finder is assumed to scan all codon-aligned positions in a specified number of reading frames $F \\in \\{3,6\\}$, where $F=3$ means the three forward frames and $F=6$ means the three forward frames and the three reverse-complement frames. At each frame, it evaluates all possible codon-aligned start positions independently of other frames.\n\nDefine the False Positive Rate (FPR) in this setting to mean the expected number of predicted ORFs per megabase (that is, per $1000000$ nucleotides) in a genome that contains no true genes (so every reported ORF is spurious). Under the independent and identically distributed base model specified by $(p_A,p_T,p_G,p_C)$, you must compute this expectation exactly from first principles, assuming a very long genome so that boundary effects are negligible. For conversion to a per-megabase quantity, use a genome length of $N=1000000$ nucleotides and count only complete codon starts per frame, i.e., use $\\left\\lfloor N/3 \\right\\rfloor$ codon-aligned start positions per frame.\n\nYour task is to write a complete program that, for the following test suite of parameter sets, returns the expected number of spurious predicted ORFs per megabase as a real number for each case. Each test case is specified as $(p_A,p_T,p_G,p_C,L_{\\min},F)$:\n\n- Test case $1$: $(0.45,\\,0.40,\\,0.10,\\,0.05,\\,100,\\,6)$.\n- Test case $2$: $(0.49,\\,0.41,\\,0.06,\\,0.04,\\,50,\\,6)$.\n- Test case $3$: $(0.40,\\,0.40,\\,0.10,\\,0.10,\\,75,\\,6)$.\n- Test case $4$: $(0.25,\\,0.25,\\,0.25,\\,0.25,\\,50,\\,6)$.\n- Test case $5$: $(0.50,\\,0.35,\\,0.10,\\,0.05,\\,30,\\,3)$.\n- Test case $6$: $(0.50,\\,0.35,\\,0.10,\\,0.05,\\,300,\\,6)$.\n\nRequirements and conventions:\n\n- The base probabilities must satisfy $p_A+p_T+p_G+p_C=1$ in each test case; all given cases already satisfy this.\n- Start codon set is exactly $\\{ATG,TTG,GTG\\}$ and stop codon set is exactly $\\{TAA,TAG,TGA\\}$.\n- Report the expected number of spurious predicted ORFs per $1000000$ nucleotides for each test case as a real number.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[x_1,x_2,\\dots,x_6]$), in the same order as the test cases above. No additional text or whitespace is allowed in the output line.",
            "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem in computational biology requiring the application of first principles of probability theory.\n\nThe objective is to compute the expected number of spurious predicted Open Reading Frames (ORFs) per megabase of random Deoxyribonucleic Acid (DNA). This quantity is defined as the False Positive Rate (FPR). The genomic model is an independent and identically distributed (IID) sequence of nucleotides from the alphabet $\\{A, C, G, T\\}$ with given probabilities $p_A$, $p_C$, $p_G$, and $p_T$.\n\nLet $E[N_{\\text{ORF}}]$ be the expected number of predicted ORFs in a sequence of length $N = 1000000$ nucleotides. A predicted ORF is an ORF with a length of at least $L_{\\min}$ codons. The search for ORFs is conducted over $F$ reading frames.\n\nBy the linearity of expectation, the total expected number of predicted ORFs is the sum of the expectations of finding a predicted ORF starting at each possible position. Due to the IID nature of the sequence, the probability of a predicted ORF starting at any given codon-aligned position is constant.\n\nThe total number of codon-aligned positions to be tested is the product of the number of frames, $F$, and the number of start positions per frame. For a sequence of length $N$, there are $\\lfloor N/3 \\rfloor$ non-overlapping codon positions in a single frame.\nThe number of test sites is $F \\times \\lfloor N/3 \\rfloor$.\nGiven $N = 1000000$, the number of positions per frame is $\\lfloor 1000000/3 \\rfloor = 333333$.\n\nThe total expectation is therefore:\n$$ E[N_{\\text{ORF}}] = F \\times \\left\\lfloor \\frac{N}{3} \\right\\rfloor \\times P(\\text{predicted ORF}) $$\nwhere $P(\\text{predicted ORF})$ is the probability that a predicted ORF starts at an arbitrary codon-aligned position.\n\nFor a predicted ORF to start at a given position, two conditions must be met independently:\n$1$. The codon at that position must be a start codon.\n$2$. The length of the resulting ORF must be at least $L_{\\min}$ codons.\n\nLet us calculate the probabilities of these events.\nThe set of start codons is $\\mathcal{S} = \\{ATG, TTG, GTG\\}$. The probability of a start codon, $P(\\text{start})$, is the sum of the probabilities of each start codon, calculated from the base probabilities $p_A, p_T, p_G, p_C$:\n$$ P(\\text{start}) = P(ATG) + P(TTG) + P(GTG) $$\n$$ P(\\text{start}) = (p_A \\cdot p_T \\cdot p_G) + (p_T \\cdot p_T \\cdot p_G) + (p_G \\cdot p_T \\cdot p_G) $$\n\nThe set of stop codons is $\\mathcal{T} = \\{TAA, TAG, TGA\\}$. The probability of a random codon being a stop codon, $P(\\text{stop})$, is:\n$$ P(\\text{stop}) = P(TAA) + P(TAG) + P(TGA) $$\n$$ P(\\text{stop}) = (p_T \\cdot p_A \\cdot p_A) + (p_T \\cdot p_A \\cdot p_G) + (p_T \\cdot p_G \\cdot p_A) $$\nThe probability of a random codon being a non-stop codon is $P(\\text{non-stop}) = 1 - P(\\text{stop})$.\n\nAn ORF is defined as the sequence from a start codon up to, but not including, the first in-frame stop codon. The length of the ORF is the number of codons it contains. For an ORF to have a length of at least $L_{\\min}$, the first $L_{\\min}-1$ codons following the start codon must all be non-stop codons.\nThe sequence of codons following the start position is a series of independent trials. The probability that an ORF has a length of at least $L_{\\min}$ is the probability that a sequence of $L_{\\min}-1$ codons contains no stop codons.\n$$ P(\\text{length} \\ge L_{\\min}) = (P(\\text{non-stop}))^{L_{\\min}-1} = (1 - P(\\text{stop}))^{L_{\\min}-1} $$\n\nThe probability of a predicted ORF starting at a specific site is the product of the probability of a start codon and the probability of the subsequent ORF being long enough:\n$$ P(\\text{predicted ORF}) = P(\\text{start}) \\times P(\\text{length} \\ge L_{\\min}) $$\n$$ P(\\text{predicted ORF}) = P(\\text{start}) \\times (1 - P(\\text{stop}))^{L_{\\min}-1} $$\n\nFinally, we substitute this back into the expression for the total expected number of predicted ORFs:\n$$ E[N_{\\text{ORF}}] = F \\times \\left\\lfloor \\frac{N}{3} \\right\\rfloor \\times P(\\text{start}) \\times (1 - P(\\text{stop}))^{L_{\\min}-1} $$\n\nSubstituting the explicit probability formulas and the value of $N=1000000$:\n$$ E[N_{\\text{ORF}}] = F \\times 333333 \\times (p_A p_T p_G + p_T^2 p_G + p_G p_T p_G) \\times (1 - (p_T p_A^2 + p_T p_A p_G + p_T p_G p_A))^{L_{\\min}-1} $$\n\nThis formula is implemented for each provided test case to compute the required values.",
            "answer": "```python\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the expected number of spurious predicted ORFs per megabase.\n    \"\"\"\n    test_cases = [\n        # (p_A, p_T, p_G, p_C, L_min, F)\n        (0.45, 0.40, 0.10, 0.05, 100, 6),\n        (0.49, 0.41, 0.06, 0.04, 50, 6),\n        (0.40, 0.40, 0.10, 0.10, 75, 6),\n        (0.25, 0.25, 0.25, 0.25, 50, 6),\n        (0.50, 0.35, 0.10, 0.05, 30, 3),\n        (0.50, 0.35, 0.10, 0.05, 300, 6),\n    ]\n\n    results = []\n    \n    # Genome length for calculation (per megabase)\n    N = 1000000\n    \n    # Number of codon-aligned start positions per frame\n    num_positions_per_frame = N // 3\n\n    for case in test_cases:\n        p_A, p_T, p_G, p_C, L_min, F = case\n\n        # Calculate the probability of encountering a start codon {ATG, TTG, GTG}\n        # P(start) = P(ATG) + P(TTG) + P(GTG) = pA*pT*pG + pT*pT*pG + pG*pT*pG\n        p_start = (p_A * p_T * p_G) + (p_T * p_T * p_G) + (p_G * p_T * p_G)\n\n        # Calculate the probability of encountering a stop codon {TAA, TAG, TGA}\n        # P(stop) = P(TAA) + P(TAG) + P(TGA) = pT*pA*pA + pT*pA*pG + pT*pG*pA\n        p_stop = (p_T * p_A * p_A) + (p_T * p_A * p_G) + (p_T * p_G * p_A)\n\n        # The probability of a codon not being a stop codon\n        p_non_stop = 1.0 - p_stop\n\n        # An ORF is predicted if its length is >= L_min. This means the first L_min-1\n        # codons after the start codon must be non-stop codons.\n        # The probability of this event for an ORF is P(non_stop)^(L_min - 1).\n        if L_min > 1:\n            prob_long_enough = p_non_stop ** (L_min - 1)\n        else: # L_min=1 means any start codon is a predicted ORF of sufficient length.\n            prob_long_enough = 1.0\n\n        # The probability of a predicted ORF at a single codon-aligned site\n        # is P(start) * P(length >= L_min).\n        prob_predicted_orf_at_site = p_start * prob_long_enough\n\n        # The total number of sites to check is F * num_positions_per_frame.\n        total_sites = F * num_positions_per_frame\n\n        # By linearity of expectation, the total expected number is num_sites * probability_per_site.\n        expected_orfs = total_sites * prob_predicted_orf_at_site\n        results.append(expected_orfs)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Given that simple ORF scanning yields too many false positives, we must turn to more sophisticated models that can learn to distinguish the statistical signatures of true coding regions from non-coding DNA. This exercise introduces two distinct modeling strategies: a simple, deterministic motif-finding automaton and a more powerful probabilistic framework known as a Hidden Markov Model (HMM). You will implement both approaches to parse a DNA sequence and predict gene boundaries.\n\nThis practice  provides a direct comparison between a rule-based system and a statistical learning model. You will gain hands-on experience implementing the Viterbi algorithm to find the most likely sequence of coding and intergenic states, and you will learn to use standard metrics like the F1-score to quantitatively evaluate and compare the performance of different computational models.",
            "id": "2419168",
            "problem": "You are given the task of implementing and evaluating two distinct sequence-labeling models for identifying gene boundaries in prokaryotic deoxyribonucleic acid (DNA) sequences. Each sequence is a string over the alphabet $\\{A,C,G,T\\}$. A gene is specified here as a contiguous coding segment on the forward strand that begins at a start codon from the set $\\{ATG,GTG,TTG\\}$ and ends at the first subsequent stop codon from the set $\\{TAA,TAG,TGA\\}$. The start boundary index is defined as the zero-based index of the first base of the start codon, and the stop boundary index is defined as the zero-based index of the last base of the stop codon. All indices in this problem are zero-based.\n\nTwo models are to be used to predict gene boundaries as follows.\n\n1) Model A (probabilistic two-state first-order model):\n\n- States: intergenic ($I$) and coding ($C$).\n- Initial state probabilities: $\\pi_I = 0.8$, $\\pi_C = 0.2$.\n- State transition probabilities: $P(I \\to I) = 0.98$, $P(I \\to C) = 0.02$, $P(C \\to C) = 0.98$, $P(C \\to I) = 0.02$.\n- Emission probabilities over $\\{A,C,G,T\\}$:\n  - For $I$: $P(A|I) = 0.3$, $P(C|I) = 0.2$, $P(G|I) = 0.2$, $P(T|I) = 0.3$.\n  - For $C$: $P(A|C) = 0.2$, $P(C|C) = 0.3$, $P(G|C) = 0.3$, $P(T|C) = 0.2$.\n- Given a sequence $s$ of length $L$, the most probable state path $z_{1:L}$ maximizes the joint probability under the above parameters. A predicted start boundary is the index $t$ at which the path transitions from $I$ to $C$ at position $t$, and a predicted stop boundary is the index $t-1$ at which the path transitions from $C$ to $I$ at position $t$. If the most probable path ends in $C$ at position $L-1$, a stop boundary at index $L-1$ is predicted. If it begins in $C$ at position $0$, a start boundary at index $0$ is predicted.\n\n2) Model B (deterministic recurrent motif-driven boundary detector):\n\n- Let $S = \\{ATG,GTG,TTG\\}$ be the set of start codons and $P = \\{TAA,TAG,TGA\\}$ be the set of stop codons.\n- Define indicator functions for a sequence $s$:\n  - $m_s(t) = 1$ if $s[t:t+3] \\in S$, and $m_s(t) = 0$ otherwise. This is defined for $t \\in \\{0,1,\\ldots,L-3\\}$ and $m_s(t)=0$ for $t>L-3$.\n  - $m_p(t) = 1$ if $t \\ge 2$ and $s[t-2:t+1] \\in P$, and $m_p(t) = 0$ otherwise.\n- Initialize a binary inside-gene state $y_{-1} = 0$. For $t$ from $0$ to $L-1$:\n  - If $y_{t-1} = 0$ and $m_s(t) = 1$, then record a start boundary at index $t$ and set $y_t = 1$.\n  - Else if $y_{t-1} = 1$ and $m_p(t) = 1$, then record a stop boundary at index $t$ and set $y_t = 0$.\n  - Else set $y_t = y_{t-1}$.\n- The predicted boundary set for Model B is the collection of all recorded start and stop indices.\n\nEvaluation metric:\n\n- For a given sequence, let $B_{\\text{true}}$ be the set of true boundary indices, and let $B_{\\text{pred}}$ be the set of predicted boundary indices from a model. Define true positives $TP = |B_{\\text{true}} \\cap B_{\\text{pred}}|$, false positives $FP = |B_{\\text{pred}} \\setminus B_{\\text{true}}|$, and false negatives $FN = |B_{\\text{true}} \\setminus B_{\\text{pred}}|$. Define precision $p = \\frac{TP}{TP+FP}$ when $TP+FP > 0$, and recall $r = \\frac{TP}{TP+FN}$ when $TP+FN > 0$. The F1-score is $F1 = \\frac{2pr}{p+r}$ when $p+r > 0$. Special case: if $|B_{\\text{true}}| = 0$ and $|B_{\\text{pred}}| = 0$, define $F1 = 1$.\n- For each test case below, compute the F1-score for Model A and for Model B. Express each F1-score as a decimal rounded to three decimal places.\n\nTest suite:\n\n- Test case $1$: sequence $s_1 = $ \"AAAGGAGGCCATGAAACCCGGGTTTTAATTT\". The true start boundary index is $10$ (the \"A\" in \"ATG\" at indices $10$–$12$), and the true stop boundary index is $27$ (the last \"A\" in \"TAA\" at indices $25$–$27$). Therefore, $B_{\\text{true},1} = \\{10, 27\\}$.\n- Test case $2$: sequence $s_2 = $ \"ATGCCCGGGCCCTAGAAAA\". The true start boundary index is $0$ (the \"A\" in \"ATG\" at indices $0$–$2$), and the true stop boundary index is $14$ (the \"G\" in \"TAG\" at indices $12$–$14$). Therefore, $B_{\\text{true},2} = \\{0, 14\\}$.\n- Test case $3$: sequence $s_3 = $ \"ACACACACACACACACGGCGGCCGCGGCCG\". There are no start or stop codons from the sets above in $s_3$, so $B_{\\text{true},3} = \\emptyset$.\n\nRequired final output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order, output a two-element list $[F1_{\\text{A}}, F1_{\\text{B}}]$ where $F1_{\\text{A}}$ is the F1-score of Model A and $F1_{\\text{B}}$ is the F1-score of Model B, both rounded to three decimal places. For example, the overall output must have the form \"[[a1,b1],[a2,b2],[a3,b3]]\" where each $a_i$ and $b_i$ is a decimal rounded to three decimal places.",
            "solution": "The problem proposed is a well-defined exercise in computational biology, specifically in the domain of prokaryotic gene prediction. It requires the implementation and evaluation of two distinct models for identifying gene boundaries. After a rigorous validation process, the problem is deemed scientifically grounded, well-posed, and objective. All necessary parameters, definitions, and test cases are provided, and there are no internal contradictions or logical flaws. The evaluation metric, the F1-score, is standard, and its calculation, including specified edge cases and standard interpretations for undefined scenarios, is tractable. Therefore, the problem is valid, and a complete solution will be provided.\n\nThe solution involves two main components: implementing the prediction models (Model A and Model B) and then evaluating their predictions against the provided ground truth using the F1-score.\n\n**Model A: Probabilistic Two-State First-Order Model**\n\nThis model is a Hidden Markov Model (HMM) with two states: Intergenic ($I$) and Coding ($C$). The goal is to find the most probable sequence of states for a given DNA sequence, from which gene boundaries can be inferred. The Viterbi algorithm is the standard method for this task.\n\nThe HMM is defined by the following parameters:\n-   **States:** $\\mathcal{S} = \\{I, C\\}$. For computational purposes, we map these to indices $\\{0, 1\\}$.\n-   **Observations (Alphabet):** $\\mathcal{O} = \\{A, C, G, T\\}$, mapped to indices $\\{0, 1, 2, 3\\}$.\n-   **Initial State Probabilities ($\\boldsymbol{\\pi}$):** The probability of the model starting in a given state.\n    $$ \\pi_I = P(z_0=I) = 0.8 $$\n    $$ \\pi_C = P(z_0=C) = 0.2 $$\n-   **Transition Probabilities ($\\mathbf{A}$):** The probability of moving from one state to another.\n    $$ A = \\begin{pmatrix} P(I|I) & P(C|I) \\\\ P(I|C) & P(C|C) \\end{pmatrix} = \\begin{pmatrix} 0.98 & 0.02 \\\\ 0.02 & 0.98 \\end{pmatrix} $$\n-   **Emission Probabilities ($\\mathbf{B}$):** The probability of observing a particular nucleotide from a given state.\n    $$ P(\\text{obs}|I) = \\{P(A|I)=0.3, P(C|I)=0.2, P(G|I)=0.2, P(T|I)=0.3\\} $$\n    $$ P(\\text{obs}|C) = \\{P(A|C)=0.2, P(C|C)=0.3, P(G|C)=0.3, P(T|C)=0.2\\} $$\n\nTo avoid numerical underflow with long sequences, the Viterbi algorithm is implemented in log-space. Let the input sequence be $s$ of length $L$ with indices $0, \\dots, L-1$.\n\n1.  **Initialization ($t=0$):** For each state $k \\in \\{I, C\\}$, the score $\\delta_0(k)$ is the log probability of starting in state $k$ and emitting the first observation $s_0$.\n    $$ \\delta_0(k) = \\log(\\pi_k) + \\log(B_k(s_0)) $$\n    A backpointer table, $\\psi$, is initialized: $\\psi_0(k) = 0$.\n\n2.  **Recursion ($t=1, \\dots, L-1$):** For each state $j \\in \\{I, C\\}$, we find the most probable path ending at state $j$ at time $t$.\n    $$ \\delta_t(j) = \\max_{i \\in \\{I, C\\}} (\\delta_{t-1}(i) + \\log(A_{ij})) + \\log(B_j(s_t)) $$\n    The backpointer stores the state $i$ that maximized the probability:\n    $$ \\psi_t(j) = \\arg\\max_{i \\in \\{I, C\\}} (\\delta_{t-1}(i) + \\log(A_{ij})) $$\n\n3.  **Termination and Backtracking:** The most probable final state is $z_{L-1} = \\arg\\max_{k \\in \\{I, C\\}} (\\delta_{L-1}(k))$. The rest of the path $z_{L-2}, \\dots, z_0$ is found by following the backpointers: $z_{t-1} = \\psi_t(z_t)$.\n\n4.  **Boundary Prediction:** The resulting Viterbi path $z_0, \\dots, z_{L-1}$ is scanned to identify boundaries according to the problem's rules:\n    -   A start boundary is predicted at index $0$ if $z_0 = C$.\n    -   For $t \\in \\{1, \\dots, L-1\\}$, a start boundary is predicted at index $t$ if $z_{t-1}=I$ and $z_t=C$.\n    -   For $t \\in \\{1, \\dots, L-1\\}$, a stop boundary is predicted at index $t-1$ if $z_{t-1}=C$ and $z_t=I$.\n    -   A stop boundary is predicted at index $L-1$ if the path ends with $z_{L-1}=C$.\n\n**Model B: Deterministic Recurrent Motif-Driven Boundary Detector**\n\nThis model is a simple finite-state automaton that scans the sequence for start and stop codons. Its behavior is entirely deterministic.\n\n-   **State:** A single binary variable $y_t \\in \\{0, 1\\}$, where $y_t=0$ indicates an intergenic region and $y_t=1$ indicates an inside-gene region at step $t$. The initial state is $y_{-1}=0$.\n-   **Codon Sets:** Start codons $S = \\{ATG, GTG, TTG\\}$ and stop codons $P = \\{TAA, TAG, TGA\\}$.\n-   **Logic:** The model iterates through the sequence from index $t=0$ to $L-1$. At each position $t$, it updates its state based on the following rules:\n    1.  If the current state is intergenic ($y_{t-1}=0$) and a start codon from $S$ begins at index $t$ (i.e., $s[t:t+3] \\in S$), a start boundary is recorded at index $t$, and the state transitions to inside-gene ($y_t=1$).\n    2.  If the current state is inside-gene ($y_{t-1}=1$) and a stop codon from $P$ ends at index $t$ (i.e., $s[t-2:t+1] \\in P$), a stop boundary is recorded at index $t$, and the state transitions to intergenic ($y_t=0$).\n    3.  Otherwise, the state remains unchanged ($y_t = y_{t-1}$).\n-   The set of all recorded start and stop indices constitutes the predicted boundaries $B_{\\text{pred}}$. This automaton structure ensures that it finds non-overlapping, alternating start and stop sites.\n\n**Evaluation Metric: F1-Score**\n\nFor each model, the set of predicted boundaries, $B_{\\text{pred}}$, is compared to the set of true boundaries, $B_{\\text{true}}$. The performance is quantified by the F1-score.\n\n-   **Counts:**\n    -   True Positives ($TP$): $|B_{\\text{true}} \\cap B_{\\text{pred}}|$\n    -   False Positives ($FP$): $|B_{\\text{pred}} \\setminus B_{\\text{true}}|$\n    -   False Negatives ($FN$): $|B_{\\text{true}} \\setminus B_{\\text{pred}}|$\n-   **Metrics:**\n    -   Precision ($p$): $p = \\frac{TP}{TP+FP}$ if $TP+FP > 0$, otherwise $p=0$.\n    -   Recall ($r$): $r = \\frac{TP}{TP+FN}$ if $TP+FN > 0$, otherwise $r=0$.\n-   **F1-Score:**\n    -   The primary formula is $F1 = \\frac{2pr}{p+r}$ if $p+r > 0$, otherwise $F1=0$.\n    -   A special case is defined: if $|B_{\\text{true}}| = 0$ and $|B_{\\text{pred}}| = 0$, then $F1 = 1$.\n\nThe provided Python code implements these models and the evaluation metric to solve the given test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the gene prediction models on test cases and print the results.\n    \"\"\"\n\n    def model_a_predict(s: str) -> set:\n        \"\"\"\n        Predicts gene boundaries using the probabilistic two-state first-order model (Model A).\n        This is an HMM, and the most probable state path is found using the Viterbi algorithm.\n        \"\"\"\n        L = len(s)\n        if L == 0:\n            return set()\n\n        # Model Parameters\n        # States: 0=Intergenic (I), 1=Coding (C)\n        # Observations: 0=A, 1=C, 2=G, 3=T\n        char_to_int = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n        obs = np.array([char_to_int[c] for c in s], dtype=int)\n        \n        num_states = 2\n        pi = np.array([0.8, 0.2])\n        A = np.array([[0.98, 0.02], [0.02, 0.98]])\n        B = np.array([[0.3, 0.2, 0.2, 0.3], [0.2, 0.3, 0.3, 0.2]])\n\n        # Use log probabilities for numerical stability\n        log_pi = np.log(pi)\n        log_A = np.log(A)\n        log_B = np.log(B)\n\n        # Viterbi algorithm\n        delta = np.zeros((L, num_states))\n        psi = np.zeros((L, num_states), dtype=int)\n\n        # Initialization step\n        delta[0, :] = log_pi + log_B[:, obs[0]]\n\n        # Recursion step\n        for t in range(1, L):\n            for j in range(num_states):\n                trans_prob = delta[t - 1, :] + log_A[:, j]\n                max_prob_idx = np.argmax(trans_prob)\n                max_prob = trans_prob[max_prob_idx]\n                delta[t, j] = max_prob + log_B[j, obs[t]]\n                psi[t, j] = max_prob_idx\n\n        # Backtracking to find the most probable path\n        path = np.zeros(L, dtype=int)\n        path[L - 1] = np.argmax(delta[L - 1, :])\n        for t in range(L - 2, -1, -1):\n            path[t] = psi[t + 1, path[t + 1]]\n\n        # Boundary extraction from the path\n        B_pred = set()\n        # Rule: If path begins in C, start boundary at 0\n        if path[0] == 1:\n            B_pred.add(0)\n        \n        # Rule: Transitions between states\n        for t in range(1, L):\n            if path[t-1] == 0 and path[t] == 1:  # I -> C transition\n                B_pred.add(t)\n            elif path[t-1] == 1 and path[t] == 0:  # C -> I transition\n                B_pred.add(t-1)\n        \n        # Rule: If path ends in C, stop boundary at L-1\n        if path[L - 1] == 1:\n            B_pred.add(L - 1)\n            \n        return B_pred\n\n    def model_b_predict(s: str) -> set:\n        \"\"\"\n        Predicts gene boundaries using the deterministic recurrent motif-driven detector (Model B).\n        \"\"\"\n        L = len(s)\n        if L == 0:\n            return set()\n        \n        start_codons = {\"ATG\", \"GTG\", \"TTG\"}\n        stop_codons = {\"TAA\", \"TAG\", \"TGA\"}\n        \n        B_pred = set()\n        # y=0: intergenic state, y=1: coding state\n        y = 0 \n        \n        for t in range(L):\n            if y == 0:\n                # Look for a start codon\n                if t + 3 <= L and s[t:t+3] in start_codons:\n                    B_pred.add(t)\n                    y = 1\n            else: # y == 1\n                # Look for a stop codon\n                if t >= 2 and s[t-2:t+1] in stop_codons:\n                    B_pred.add(t)\n                    y = 0\n        return B_pred\n\n    def calculate_f1(B_true: set, B_pred: set) -> float:\n        \"\"\"\n        Calculates the F1-score given true and predicted boundary sets.\n        \"\"\"\n        if not B_true and not B_pred:\n            return 1.0\n\n        tp = len(B_true.intersection(B_pred))\n        fp = len(B_pred.difference(B_true))\n        fn = len(B_true.difference(B_pred))\n\n        # Handle division by zero for precision and recall\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n        \n        # Handle division by zero for F1-score\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n        \n        return f1\n\n    # Test suite from the problem statement\n    test_cases = [\n        (\"AAAGGAGGCCATGAAACCCGGGTTTTAATTT\", {10, 27}),\n        (\"ATGCCCGGGCCCTAGAAAA\", {0, 14}),\n        (\"ACACACACACACACACGGCGGCCGCGGCCG\", set())\n    ]\n\n    results = []\n    for s, B_true in test_cases:\n        # Model A\n        B_pred_a = model_a_predict(s)\n        f1_a = calculate_f1(B_true, B_pred_a)\n        \n        # Model B\n        B_pred_b = model_b_predict(s)\n        f1_b = calculate_f1(B_true, B_pred_b)\n        \n        results.append([f\"{f1_a:.3f}\", f\"{f1_b:.3f}\"])\n\n    # Format the final output string as specified\n    final_output = \"[\" + \",\".join([f\"[{a},{b}]\" for a, b in results]) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "State-of-the-art gene prediction systems achieve their high accuracy by integrating multiple, diverse lines of evidence. A raw prediction from an HMM can be strengthened (or weakened) by other signals, such as the quality of a ribosome binding site, intrinsic coding potential measured by hexamer frequencies, and even the type of start codon used. This final practice guides you in building a Naive Bayes classifier to synthesize these features into a single, unified confidence score.\n\nBy implementing this Bayesian framework , you will learn how to formally combine evidence from disparate sources and probability distributions. This process of calculating a posterior probability, $P(G=\\text{True} \\mid \\text{evidence})$, is a cornerstone of modern machine learning in biology and provides a principled way to rank and filter gene predictions, transforming a list of candidates into a high-confidence annotation.",
            "id": "2419184",
            "problem": "You are given a probabilistic model to assign a posterior probability that a gene predicted by a Hidden Markov Model (HMM) is a true protein-coding gene in a prokaryotic genome. For each predicted gene, you observe a feature vector consisting of five components: an HMM log-odds score $s$, a coding hexamer log-likelihood ratio $h$, a ribosome binding site motif score $r$, the gene’s guanine-cytosine (GC) fraction $x \\in (0,1)$, and the start codon category $c \\in \\{\\text{ATG},\\text{GTG},\\text{TTG},\\text{OTHER}\\}$. Let the latent class variable be $G \\in \\{T,F\\}$, where $T$ denotes a true protein-coding gene and $F$ denotes a false prediction. Assume conditional independence of the features given $G$.\n\nLet the prior be $P(G=T) = p_0$ and $P(G=F) = 1 - p_0$, with $p_0 = 0.7$. The conditional distributions of the features given $G$ are specified as follows.\n\n- For the HMM log-odds score $s$:\n  - $s \\mid G=T \\sim \\mathcal{N}(\\mu_s^T,\\ (\\sigma_s^T)^2)$ with $\\mu_s^T = 5$ and $\\sigma_s^T = 2$.\n  - $s \\mid G=F \\sim \\mathcal{N}(\\mu_s^F,\\ (\\sigma_s^F)^2)$ with $\\mu_s^F = 0$ and $\\sigma_s^F = 2$.\n\n- For the coding hexamer log-likelihood ratio $h$:\n  - $h \\mid G=T \\sim \\mathcal{N}(\\mu_h^T,\\ (\\sigma_h^T)^2)$ with $\\mu_h^T = 2$ and $\\sigma_h^T = 1.5$.\n  - $h \\mid G=F \\sim \\mathcal{N}(\\mu_h^F,\\ (\\sigma_h^F)^2)$ with $\\mu_h^F = -1$ and $\\sigma_h^F = 1.5$.\n\n- For the ribosome binding site motif score $r$:\n  - $r \\mid G=T \\sim \\mathcal{N}(\\mu_r^T,\\ (\\sigma_r^T)^2)$ with $\\mu_r^T = 3$ and $\\sigma_r^T = 1$.\n  - $r \\mid G=F \\sim \\mathcal{N}(\\mu_r^F,\\ (\\sigma_r^F)^2)$ with $\\mu_r^F = 0$ and $\\sigma_r^F = 1.5$.\n\n- For the GC fraction $x$:\n  - $x \\mid G=T \\sim \\mathrm{Beta}(\\alpha_T,\\ \\beta_T)$ with $\\alpha_T = 55$ and $\\beta_T = 45$.\n  - $x \\mid G=F \\sim \\mathrm{Beta}(\\alpha_F,\\ \\beta_F)$ with $\\alpha_F = 35$ and $\\beta_F = 65$.\n\n- For the start codon category $c$ (categorical):\n  - $P(c=\\text{ATG} \\mid G=T)=0.83$, $P(c=\\text{GTG} \\mid G=T)=0.12$, $P(c=\\text{TTG} \\mid G=T)=0.04$, $P(c=\\text{OTHER} \\mid G=T)=0.01$.\n  - $P(c=\\text{ATG} \\mid G=F)=0.25$, $P(c=\\text{GTG} \\mid G=F)=0.25$, $P(c=\\text{TTG} \\mid G=F)=0.25$, $P(c=\\text{OTHER} \\mid G=F)=0.25$.\n\nUse the following density and mass functions.\n\n- For a normal distribution, the probability density function of $X \\sim \\mathcal{N}(\\mu,\\ \\sigma^2)$ is\n  $$ f_{\\mathcal{N}}(x \\mid \\mu,\\ \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right). $$\n\n- For a beta distribution, the probability density function of $X \\sim \\mathrm{Beta}(\\alpha,\\ \\beta)$ is\n  $$ f_{\\mathrm{Beta}}(x \\mid \\alpha,\\ \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\ \\beta)}, \\quad \\text{for } x \\in (0,1), $$\n  where $B(\\alpha,\\ \\beta) = \\dfrac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ and $\\Gamma(\\cdot)$ is the gamma function.\n\n- For a categorical variable, the probability mass function is $P(c=k \\mid G)=q_k$ for category $k$ with specified probability $q_k$.\n\nFor an observed feature vector $e=(s,h,r,x,c)$, define the class-conditional likelihoods\n$$ \\ell_T(e) = f_{\\mathcal{N}}(s \\mid \\mu_s^T,\\ \\sigma_s^T) \\cdot f_{\\mathcal{N}}(h \\mid \\mu_h^T,\\ \\sigma_h^T) \\cdot f_{\\mathcal{N}}(r \\mid \\mu_r^T,\\ \\sigma_r^T) \\cdot f_{\\mathrm{Beta}}(x \\mid \\alpha_T,\\ \\beta_T) \\cdot P(c \\mid G=T), $$\n$$ \\ell_F(e) = f_{\\mathcal{N}}(s \\mid \\mu_s^F,\\ \\sigma_s^F) \\cdot f_{\\mathcal{N}}(h \\mid \\mu_h^F,\\ \\sigma_h^F) \\cdot f_{\\mathcal{N}}(r \\mid \\mu_r^F,\\ \\sigma_r^F) \\cdot f_{\\mathrm{Beta}}(x \\mid \\alpha_F,\\ \\beta_F) \\cdot P(c \\mid G=F). $$\n\nUsing Bayes’ theorem, the posterior probability of a true gene given $e$ is\n$$ P(G=T \\mid e) = \\frac{p_0 \\cdot \\ell_T(e)}{p_0 \\cdot \\ell_T(e) + (1-p_0) \\cdot \\ell_F(e)}. $$\n\nYour task is to write a program that, for each test case listed below, computes $P(G=T \\mid e)$ using the model and definitions above.\n\nTest suite of observed feature vectors $(s,h,r,x,c)$ to evaluate, in the stated order:\n- Case Alpha: $(6.0,\\,2.5,\\,3.2,\\,0.56,\\,\\text{ATG})$.\n- Case Beta: $(3.0,\\,1.0,\\,2.0,\\,0.50,\\,\\text{GTG})$.\n- Case Gamma: $(1.0,\\,0.0,\\,0.5,\\,0.45,\\,\\text{TTG})$.\n- Case Delta: $(-1.0,\\,-1.5,\\,-0.5,\\,0.20,\\,\\text{OTHER})$.\n- Case Epsilon: $(12.0,\\,6.0,\\,6.0,\\,0.58,\\,\\text{ATG})$.\n- Case Zeta: $(-5.0,\\,-4.0,\\,-3.0,\\,0.10,\\,\\text{OTHER})$.\n\nFinal output format: Your program should produce a single line of output containing the posterior probabilities for the cases in the exact order above, as a comma-separated list of decimals enclosed in square brackets, with each value rounded to six decimal places (for example, [$0.123456$,$0.654321$,$0.500000$,$0.000001$,$0.999999$,$0.250000$]). No additional text should be printed.",
            "solution": "The problem requires the calculation of the posterior probability that a predicted gene is a true protein-coding gene, given a set of observed features. This is a classic application of Bayesian inference, specifically using a Naive Bayes classifier. The model assumes that the features are conditionally independent given the class label $G \\in \\{T, F\\}$, where $T$ denotes a true gene and $F$ denotes a false prediction.\n\nThe core of the solution is the application of Bayes' theorem. The posterior probability of a gene being true ($G=T$) given the evidence (the feature vector $e=(s,h,r,x,c)$) is given by:\n$$ P(G=T \\mid e) = \\frac{P(e \\mid G=T) P(G=T)}{P(e)} $$\nThe prior probability $P(G=T)$ is given as $p_0 = 0.7$. The term $P(e \\mid G=T)$ is the class-conditional likelihood of observing the evidence $e$ if the gene is true. The denominator $P(e)$ is the total probability of the evidence, which can be expanded using the law of total probability:\n$$ P(e) = P(e \\mid G=T)P(G=T) + P(e \\mid G=F)P(G=F) $$\nSubstituting this into the Bayes' formula, and using the problem's notation $\\ell_G(e) = P(e \\mid G)$ for the likelihood and $P(G=F) = 1-p_0$, we obtain the expression provided in the problem statement:\n$$ P(G=T \\mid e) = \\frac{p_0 \\cdot \\ell_T(e)}{p_0 \\cdot \\ell_T(e) + (1-p_0) \\cdot \\ell_F(e)} $$\nDue to the conditional independence assumption, the class-conditional likelihood $\\ell_G(e)$ is the product of the individual probability density or mass functions for each feature:\n$$ \\ell_G(e) = P(s \\mid G) \\cdot P(h \\mid G) \\cdot P(r \\mid G) \\cdot P(x \\mid G) \\cdot P(c \\mid G) $$\nThe distributions for each feature are specified: continuous features $s, h, r$ follow Normal distributions ($\\mathcal{N}$), feature $x$ follows a Beta distribution ($\\mathrm{Beta}$), and the discrete feature $c$ follows a Categorical distribution.\n\nA direct computation of the likelihoods $\\ell_T(e)$ and $\\ell_F(e)$ by multiplying probability densities may lead to numerical underflow, as these values can be very small. A standard and robust technique is to perform calculations in logarithmic space. The log-likelihood, $L_G(e) = \\ln(\\ell_G(e))$, is the sum of the individual log-probabilities:\n$$ L_G(e) = \\ln(P(s \\mid G)) + \\ln(P(h \\mid G)) + \\ln(P(r \\mid G)) + \\ln(P(x \\mid G)) + \\ln(P(c \\mid G)) $$\nThe log-probability density for a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is:\n$$ \\ln(f_{\\mathcal{N}}(z \\mid \\mu, \\sigma)) = -\\ln(\\sigma) - \\frac{1}{2}\\ln(2\\pi) - \\frac{(z-\\mu)^2}{2\\sigma^2} $$\nThe log-probability density for a Beta distribution $\\mathrm{Beta}(\\alpha, \\beta)$ is:\n$$ \\ln(f_{\\mathrm{Beta}}(z \\mid \\alpha, \\beta)) = (\\alpha-1)\\ln(z) + (\\beta-1)\\ln(1-z) - \\ln(B(\\alpha, \\beta)) $$\nwhere $\\ln(B(\\alpha, \\beta)) = \\ln(\\Gamma(\\alpha)) + \\ln(\\Gamma(\\beta)) - \\ln(\\Gamma(\\alpha+\\beta))$. These calculations are best performed using specialized functions like `scipy.stats.norm.logpdf`, `scipy.stats.beta.logpdf`, and `scipy.special.gammaln` to maintain numerical precision. The log-probability for the categorical variable is simply the logarithm of its given probability mass.\n\nTo compute the posterior probability using log-likelihoods, we can algebraically manipulate the formula:\n$$ P(G=T \\mid e) = \\frac{p_0 \\ell_T(e)}{p_0 \\ell_T(e) + (1-p_0) \\ell_F(e)} = \\frac{1}{1 + \\frac{(1-p_0) \\ell_F(e)}{p_0 \\ell_T(e)}} $$\nThe ratio term in the denominator can be expressed using an exponential of the difference of log-quantities:\n$$ \\frac{(1-p_0) \\ell_F(e)}{p_0 \\ell_T(e)} = \\exp\\left( \\ln\\left( \\frac{(1-p_0) \\ell_F(e)}{p_0 \\ell_T(e)} \\right) \\right) = \\exp\\left( (\\ln(1-p_0) + L_F(e)) - (\\ln(p_0) + L_T(e)) \\right) $$\nThis transformed expression involves a logistic (sigmoid) function, which is numerically stable and mitigates the risk of overflow or underflow.\n\nThe computational procedure is as follows:\nFor each test case vector $e = (s, h, r, x, c)$:\n1.  Calculate the log-likelihood $L_T(e)$ by summing the log-probabilities of each observed feature value, using a set of parameters for the \"True\" class ($G=T$).\n2.  Calculate the log-likelihood $L_F(e)$ similarly, using parameters for the \"False\" class ($G=F$).\n3.  Compute the log of the prior odds, $\\ln(1-p_0) - \\ln(p_0)$.\n4.  Combine these terms to find the log of the posterior odds ratio: $Z = (\\ln(1-p_0) + L_F(e)) - (\\ln(p_0) + L_T(e))$.\n5.  Calculate the final posterior probability as $P(G=T \\mid e) = \\frac{1}{1 + \\exp(Z)}$.\n6.  The results for all test cases are collected and formatted to six decimal places, presented in a comma-separated list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, beta\n\ndef solve():\n    \"\"\"\n    Computes the posterior probability that a predicted gene is a true\n    protein-coding gene based on a Naive Bayes model.\n    \"\"\"\n    # Define the model parameters for true (T) and false (F) gene classes.\n    # The parameters are for features: s, h, r, x, c.\n    params = {\n        'T': {\n            's': {'mu': 5.0, 'sigma': 2.0},\n            'h': {'mu': 2.0, 'sigma': 1.5},\n            'r': {'mu': 3.0, 'sigma': 1.0},\n            'x': {'alpha': 55.0, 'beta': 45.0},\n            'c': {'ATG': 0.83, 'GTG': 0.12, 'TTG': 0.04, 'OTHER': 0.01}\n        },\n        'F': {\n            's': {'mu': 0.0, 'sigma': 2.0},\n            'h': {'mu': -1.0, 'sigma': 1.5},\n            'r': {'mu': 0.0, 'sigma': 1.5},\n            'x': {'alpha': 35.0, 'beta': 65.0},\n            'c': {'ATG': 0.25, 'GTG': 0.25, 'TTG': 0.25, 'OTHER': 0.25}\n        }\n    }\n    \n    # Prior probability of being a true gene.\n    prior_p0 = 0.7\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (s, h, r, x, c)\n        (6.0, 2.5, 3.2, 0.56, 'ATG'),     # Case Alpha\n        (3.0, 1.0, 2.0, 0.50, 'GTG'),     # Case Beta\n        (1.0, 0.0, 0.5, 0.45, 'TTG'),     # Case Gamma\n        (-1.0, -1.5, -0.5, 0.20, 'OTHER'), # Case Delta\n        (12.0, 6.0, 6.0, 0.58, 'ATG'),    # Case Epsilon\n        (-5.0, -4.0, -3.0, 0.10, 'OTHER')  # Case Zeta\n    ]\n\n    results = []\n    \n    # Pre-compute log of priors for efficiency.\n    log_p0 = np.log(prior_p0)\n    log_1_minus_p0 = np.log(1.0 - prior_p0)\n\n    for s_obs, h_obs, r_obs, x_obs, c_obs in test_cases:\n        # Calculate log-likelihood for the \"True\" class (G=T).\n        # This is the sum of the log-probabilities of each feature given G=T.\n        log_L_T = (\n            norm.logpdf(s_obs, loc=params['T']['s']['mu'], scale=params['T']['s']['sigma']) +\n            norm.logpdf(h_obs, loc=params['T']['h']['mu'], scale=params['T']['h']['sigma']) +\n            norm.logpdf(r_obs, loc=params['T']['r']['mu'], scale=params['T']['r']['sigma']) +\n            beta.logpdf(x_obs, a=params['T']['x']['alpha'], b=params['T']['x']['beta']) +\n            np.log(params['T']['c'][c_obs])\n        )\n\n        # Calculate log-likelihood for the \"False\" class (G=F).\n        # This is the sum of the log-probabilities of each feature given G=F.\n        log_L_F = (\n            norm.logpdf(s_obs, loc=params['F']['s']['mu'], scale=params['F']['s']['sigma']) +\n            norm.logpdf(h_obs, loc=params['F']['h']['mu'], scale=params['F']['h']['sigma']) +\n            norm.logpdf(r_obs, loc=params['F']['r']['mu'], scale=params['F']['r']['sigma']) +\n            beta.logpdf(x_obs, a=params['F']['x']['alpha'], b=params['F']['x']['beta']) +\n            np.log(params['F']['c'][c_obs])\n        )\n\n        # Calculate the posterior probability P(G=T|e) using a numerically stable\n        # formula based on log-likelihoods.\n        # P(T|e) = 1 / (1 + exp(log( (1-p0)l_F / (p0 * l_T) )))\n        # log_ratio = log(1-p0) + log_L_F - (log(p0) + log_L_T)\n        \n        log_joint_T = log_p0 + log_L_T\n        log_joint_F = log_1_minus_p0 + log_L_F\n        \n        # This term is log( P(e, F) / P(e, T) )\n        log_odds_ratio = log_joint_F - log_joint_T\n        \n        posterior_T = 1.0 / (1.0 + np.exp(log_odds_ratio))\n        \n        results.append(posterior_T)\n\n    # Format the final output as a comma-separated list of values rounded to\n    # six decimal places, enclosed in square brackets.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}