{
    "hands_on_practices": [
        {
            "introduction": "在单细胞分析中，一个至关重要的初始步骤是区分真实的生物异质性与技术性伪影。本练习提供了一个动手实践的机会，通过模拟来理解像测序深度不均这样的技术问题是如何产生伪集群的，并学习如何开发一种量化方法来识别它们。通过这个过程，你将深刻体会到在相信任何生物学信号之前，理解并处理技术噪声的重要性 。",
            "id": "2371686",
            "problem": "您必须编写一个完整、可运行的程序，该程序在一个生成模型下模拟单细胞RNA测序（scRNA-seq）数据。在该模型中，一个技术性偏差（例如，某个泳道测序质量不佳）会引入一个细胞子集，其捕获效率全局性降低，从而在质量控制（QC）特征空间中导致一个伪簇。然后，程序必须仅使用基于QC指标的逻辑派生准则，来检测是否存在此类由偏差驱动的簇。\n\n请从以下基础定义和经过充分检验的事实开始。在单细胞RNA测序（scRNA-seq）中，信使核糖核酸（mRNA）分子被反转录，并通过唯一分子标识符（UMI）标签进行计数；每个细胞中每个基因的UMI计数通常被建模为泊松分布的抽样，这反映了一个计数过程，其均值与潜在的表达水平和总采样深度成正比。具体来说，令 $X_{i,g}$ 表示细胞 $i$ 和基因 $g$ 的UMI计数。给定细胞 $i$ 的一个生物学簇，其每个基因的表达均值为 $\\mu_{b(i),g} \\gt 0$，以及一个捕捉了采样深度和捕获效率的细胞特异性大小因子 $s_i \\gt 0$，标准做法是建模为\n$$\nX_{i,g} \\sim \\mathrm{Poisson}\\!\\left(s_i \\,\\mu_{b(i),g}\\right),\n$$\n在给定 $(s_i,\\mu_{b(i),\\cdot})$ 的条件下，对于不同的 $g$ 是独立的。一个降低测序深度的技术性泳道偏差可以表示为，对于受影响泳道中的所有细胞，将其 $s_i$ 乘以一个因子 $a$（其中 $0 \\lt a \\lt 1$）。为了进行质量控制（QC），定义细胞 $i$ 的文库大小为 $L_i = \\sum_{g=1}^{G} X_{i,g}$，零值比例为 $Z_i = \\frac{1}{G} \\sum_{g=1}^{G} \\mathbf{1}\\{X_{i,g} = 0\\}$。在泊松模型下，独立泊松随机变量之和仍服从泊松分布，其均值等于各均值之和，因此 $\\mathbb{E}[L_i] = s_i \\sum_{g=1}^{G} \\mu_{b(i),g}$，且单个基因计数值为零的概率是 $\\mathbb{P}(X_{i,g}=0) = \\exp(-s_i \\mu_{b(i),g})$。因此，乘法性降低 $s_i \\mapsto a s_i$ 会使 $\\mathbb{E}[L_i]$ 降低因子 $a$，并增加预期的零值比例，这倾向于在由 $\\log_{10}(L_i+1)$ 和 $Z_i$ 张成的QC特征空间中形成一个独特的簇。\n\n您必须实现以下任务：\n\n1. 生物学异质性和泳道偏差的模拟：\n   - 模拟 $N$ 个细胞和 $G$ 个基因。存在 $B$ 个生物学簇。对于每个生物学簇 $b \\in \\{1,\\dots,B\\}$，通过首先从伽马分布（其形状和尺度参数的选择可产生符合现实的均值）中抽取一个基线表达谱 $\\mu_{1,g}$，然后对每个簇使用独立的对数正态乘数进行扰动以反映生物学变异性，从而为每个基因 $g$ 生成簇特异性的均值表达 $\\mu_{b,g}$。将细胞均匀随机地分配到各个生物学簇中。\n   - 从对数正态分布中抽取细胞大小因子 $s_i$，以反映测序深度的变异性。选取一个比例为 $p$ 的细胞子集（一个“偏差泳道”），并将其 $s_i$ 乘以一个标量 $a$（其中 $0 \\lt a \\le 1$，$a \\lt 1$ 表示存在偏差）。\n   - 独立地在所有基因上生成计数 $X_{i,g} \\sim \\mathrm{Poisson}(s_i \\mu_{b(i),g})$。\n   - 计算每个细胞的QC特征：$F_i = \\left[\\log_{10}(L_i+1), Z_i\\right]$。\n\n2. QC空间中的无监督偏差检测：\n   - 使用 $k=2$ 的 $k$-均值算法在二维QC特征空间中对细胞进行聚类。将两个质心确定性地初始化为具有最小和最大 $\\log_{10}(L_i+1)$ 值的点，并进行迭代直至收敛或达到固定的迭代上限。\n   - 设两个簇的质心为 $C_{\\text{small}}$ 和 $C_{\\text{large}}$，其中 $C_{\\text{small}}$ 对应于细胞数较少的簇，$C_{\\text{large}}$ 对应于另一个簇。将其坐标表示为 $C_{\\text{small}} = [\\ell_s, z_s]$ 和 $C_{\\text{large}} = [\\ell_\\ell, z_\\ell]$。\n   - 当且仅当以下所有条件均成立时，宣布“检测到”偏差驱动的簇：\n     - 质心差异超过最小间距：$\\ell_\\ell - \\ell_s \\ge \\tau_\\ell$ 和 $z_s - z_\\ell \\ge \\tau_z$。\n     - 较小簇的大小比例位于界限之内：$f_{\\min} \\le \\frac{n_{\\text{small}}}{N} \\le f_{\\max}$。\n   - 使用固定阈值 $\\tau_\\ell = 0.2$, $\\tau_z = 0.05$, $f_{\\min} = 0.05$ 和 $f_{\\max} = 0.6$。\n\n3. 确定性：\n   - 每个测试用例使用带有固定种子的伪随机数生成器，以确保确定性。\n\n您的程序必须评估以下参数集测试套件（每个都写成有序元组 $(N,G,B,p,a,\\text{seed})$），进行模拟，然后为每种情况输出一个布尔值，指示是否根据上述规则检测到偏差驱动的簇：\n- 情况A（明显偏差，深度损失严重，中等流行率）：$(N,G,B,p,a,\\text{seed}) = (\\,600,\\,1500,\\,2,\\,0.3,\\,0.25,\\,1\\,)$。\n- 情况B（中等偏差，较低流行率，增加了生物学复杂性）：$(N,G,B,p,a,\\text{seed}) = (\\,500,\\,1200,\\,3,\\,0.2,\\,0.5,\\,2\\,)$。\n- 情况C（无偏差对照）：$(N,G,B,p,a,\\text{seed}) = (\\,500,\\,1200,\\,2,\\,0.0,\\,1.0,\\,3\\,)$。\n- 情况D（偏差过于罕见，无法形成一个连贯的簇）：$(N,G,B,p,a,\\text{seed}) = (\\,400,\\,1000,\\,2,\\,0.02,\\,0.2,\\,4\\,)$。\n\n要求与输出：\n- 您的实现必须精确遵循上述建模和检测设计。\n- 最终程序输出必须为单行文本，包含一个由方括号括起来、用逗号分隔且无空格的4个布尔检测结果列表，顺序为A、B、C、D。例如，仅当输出与您实现的指定测试用例结果相匹配时，像 $[{\\tt True},{\\tt False},{\\tt False},{\\tt False}]$ 这样的输出才是可接受的。",
            "solution": "问题陈述经过审查，被认定是有效的。它构成了生物信息学中一个适定的计算任务，具体涉及单细胞RNA测序（scRNA-seq）数据的模拟与分析。其底层模型——用于UMI计数的泊松模型、通过大小因子缩放来表示技术性偏差的方法，以及使用质量控制（QC）指标进行簇检测——都基于该领域已确立的原则。目标陈述得足够清晰和形式化，允许一个唯一且可验证的解。问题提供了一组参数和一个确定性流程，确保了结果的可复现性。\n\n解决方案通过遵循规定的算法来实现，该算法分为两个主要阶段：数据模拟和偏差检测。\n\n**1. 数据模拟**\n\n模拟过程生成一个合成的scRNA-seq计数矩阵 $X \\in \\mathbb{N}_0^{N \\times G}$，该矩阵包含 $N$ 个细胞和 $G$ 个基因，并融合了生物学和技术两方面的变异来源。\n\n首先，通过定义 $B$ 个不同的细胞簇来建立生物学异质性。通过从伽马分布（因其适合建模非负偏态数据如基因表达水平而被选中）中抽样，生成一个基线平均表达谱 $\\{\\mu_{1,g}\\}_{g=1}^G$。具体而言，$\\mu_{1,g} \\sim \\text{Gamma}(k=2.0, \\theta=0.5)$。对于其余的 $B-1$ 个生物学簇，它们的平均表达谱 $\\{\\mu_{b,g}\\}_{g=1}^G$ 是通过对基线谱应用基因特异性的乘法性扰动来创建的。这些乘数从对数正态分布 $\\text{LogNormal}(\\mu=0, \\sigma^2=0.25)$ 中抽取，以模拟不同细胞类型中基因的上调或下调。然后，将 $N$ 个细胞中的每一个均匀随机地分配到 $B$ 个生物学簇之一。\n\n其次，通过大小因子 $\\{s_i\\}_{i=1}^N$ 引入细胞特异性的技术变异，这些因子用于解释mRNA捕获和测序效率的差异。它们从对数正态分布 $s_i \\sim \\text{LogNormal}(\\mu=0, \\sigma^2=0.25)$ 中抽取。为了模拟指定的技术性偏差，选取一个占总群体比例为 $p$ 的细胞子集。这些被指定为“偏差”细胞的大小因子被一个乘法因子 $a \\in (0, 1]$ 缩放。对于无偏差的情况，$a=1$ 且 $p=0$。\n\n最后，细胞 $i$ 和基因 $g$ 的UMI计数 $X_{i,g}$ 是从泊松分布中独立抽样生成的，其率参数 $\\lambda_{i,g}$ 由该细胞的有效大小因子与该基因在该细胞生物学簇中的平均表达水平的乘积给出。\n$$\nX_{i,g} \\sim \\mathrm{Poisson}(s_i \\mu_{b(i),g})\n$$\n其中 $b(i)$ 是细胞 $i$ 的生物学簇身份。对所有 $N \\times G$ 个细胞-基因对执行此过程，以形成完整的计数矩阵 $X$。\n\n**2. 偏差检测**\n\n检测阶段在从计数矩阵派生的二维QC特征空间上操作。对于每个细胞 $i$，计算两个QC指标：文库大小 $L_i = \\sum_{g=1}^{G} X_{i,g}$ 和零值比例 $Z_i = G^{-1} \\sum_{g=1}^{G} \\mathbf{1}\\{X_{i,g}=0\\}$。细胞 $i$ 的QC特征向量定义为 $F_i = [\\log_{10}(L_i + 1), Z_i]$。\n\n检测算法的核心是应用于所有特征向量集合 $\\{F_i\\}_{i=1}^N$ 的 $k$-均值聚类（$k=2$）。按照规定，该过程被确定性地初始化。两个初始质心被设置为分别拥有最小和最大 $\\log_{10}(L_i+1)$ 值的细胞的特征向量。然后，标准的迭代式$k$-均值算法继续执行，直到簇质心收敛。\n\n收敛后，根据大小标记所产生的两个簇。包含较少细胞的簇被指定为“小”簇，另一个被指定为“大”簇。设它们各自的细胞数为 $n_{\\text{small}}$ 和 $n_{\\text{large}}$，质心为 $C_{\\text{small}} = [\\ell_s, z_s]$ 和 $C_{\\text{large}} = [\\ell_\\ell, z_\\ell]$。\n\n当且仅当一组三个逻辑条件同时满足时，一个偏差驱动的簇被宣布为“已检测到”。这些条件将低质量细胞簇的预期特征形式化。\n1.  **在文库大小轴上有足够的分离度：** 大簇的质心必须比小簇具有足够大的对数文库大小。\n    $$ \\ell_\\ell - \\ell_s \\ge \\tau_\\ell $$\n    阈值为 $\\tau_\\ell = 0.2$。\n2.  **在零值比例轴上有足够的分离度：** 小簇的质心必须比大簇具有足够大的零值比例。这与低质量细胞检测到的基因较少的情况相符。\n    $$ z_s - z_\\ell \\ge \\tau_z $$\n    阈值为 $\\tau_z = 0.05$。\n3.  **合理的簇大小：** 小簇的比例大小 $n_{\\text{small}} / N$ 必须落在一个预定义的范围内，才能被认为是一个连贯的群体，而不是一组可忽略的离群值或大部分数据。\n    $$ f_{\\min} \\le \\frac{n_{\\text{small}}}{N} \\le f_{\\max} $$\n    阈值为 $f_{\\min} = 0.05$ 和 $f_{\\max} = 0.6$。\n\n通过评估这三个条件的逻辑与运算，为每个测试用例计算出表示检测状态的布尔结果。",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(N, G, B, p, a, seed):\n    \"\"\"\n    Simulates scRNA-seq data and detects an artifact-driven cluster.\n\n    Args:\n        N (int): Number of cells.\n        G (int): Number of genes.\n        B (int): Number of biological clusters.\n        p (float): Proportion of cells in the artifact lane.\n        a (float): Multiplicative scaling factor for the artifact.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        bool: True if an artifact-driven cluster is detected, False otherwise.\n    \"\"\"\n    # 3. Determinism: Use a seeded RNG\n    rng = np.random.default_rng(seed)\n\n    # 1. Simulation of biological heterogeneity and lane artifact\n    \n    # Generate biological means mu_b,g\n    # These parameters are chosen to be reasonable but are not specified in the problem.\n    gamma_shape = 2.0\n    gamma_scale = 0.5\n    lognorm_sigma = 0.5\n    \n    # Baseline profile for the first biological cluster\n    mu_baseline = rng.gamma(gamma_shape, gamma_scale, size=G)\n    \n    # Generate profiles for all B clusters\n    mu_profiles = np.zeros((B, G))\n    mu_profiles[0, :] = mu_baseline\n    for b in range(1, B):\n        perturbations = rng.lognormal(mean=0.0, sigma=lognorm_sigma, size=G)\n        mu_profiles[b, :] = mu_baseline * perturbations\n\n    # Assign cells uniformly at random to biological clusters\n    cell_bio_clusters = rng.integers(0, B, size=N)\n\n    # Draw cell size factors s_i from a log-normal distribution\n    # These parameters are chosen to be reasonable. Mean=0 gives a median of 1.\n    size_factors = rng.lognormal(mean=0.0, sigma=lognorm_sigma, size=N)\n    \n    # Select a subset of cells for the artifact lane and apply the artifact\n    num_artifact_cells = int(np.floor(N * p))\n    if num_artifact_cells > 0:\n        artifact_indices = rng.choice(N, size=num_artifact_cells, replace=False)\n        size_factors[artifact_indices] *= a\n\n    # Generate count matrix X_i,g ~ Poisson(s_i * mu_b(i),g)\n    # The rates matrix lambda_ig has dimensions N x G\n    rates = size_factors[:, np.newaxis] * mu_profiles[cell_bio_clusters, :]\n    X = rng.poisson(rates)\n\n    # Compute QC features per cell\n    L = X.sum(axis=1)\n    Z = (X == 0).mean(axis=1)\n    F = np.vstack([np.log10(L + 1), Z]).T\n\n    # 2. Unsupervised artifact detection in QC space\n    \n    # k-means clustering with k=2\n    # Deterministic initialization\n    idx_min_l = np.argmin(F[:, 0])\n    idx_max_l = np.argmax(F[:, 0])\n    centroids = np.array([F[idx_min_l, :], F[idx_max_l, :]])\n\n    # Iterate until convergence or max iterations\n    max_iter = 100\n    for _ in range(max_iter):\n        old_centroids = centroids.copy()\n        \n        # Assign points to the nearest centroid (Euclidean distance)\n        dist_sq_0 = np.sum((F - centroids[0])**2, axis=1)\n        dist_sq_1 = np.sum((F - centroids[1])**2, axis=1)\n        labels = (dist_sq_1  dist_sq_0).astype(int)\n        \n        # Handle case where a cluster becomes empty\n        if np.sum(labels == 0) == 0 or np.sum(labels == 1) == 0:\n            # If a cluster is empty, re-initialization is needed.\n            # However, for this problem's setup, it is highly unlikely.\n            # We can stop here, as centroids cannot be computed.\n            return False # No valid two-cluster structure found\n\n        # Update centroids\n        centroids[0] = F[labels == 0].mean(axis=0)\n        centroids[1] = F[labels == 1].mean(axis=0)\n        \n        if np.allclose(old_centroids, centroids):\n            break\n\n    # Label clusters as 'small' and 'large' based on cell count\n    n_0 = np.sum(labels == 0)\n    n_1 = np.sum(labels == 1)\n\n    if n_0  n_1:\n        n_small, C_small = n_0, centroids[0]\n        n_large, C_large = n_1, centroids[1]\n    else:\n        n_small, C_small = n_1, centroids[1]\n        n_large, C_large = n_0, centroids[0]\n    \n    l_s, z_s = C_small\n    l_l, z_l = C_large\n\n    # Define detection thresholds\n    tau_l = 0.2\n    tau_z = 0.05\n    f_min = 0.05\n    f_max = 0.6\n\n    # Declare an artifact-driven cluster \"detected\" if all criteria are met\n    cond1 = (l_l - l_s) >= tau_l\n    cond2 = (z_s - z_l) >= tau_z\n    frac_small = n_small / N\n    cond3 = (f_min = frac_small) and (frac_small = f_max)\n\n    return cond1 and cond2 and cond3\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: Clear artifact, substantial depth loss, moderate prevalence\n        (600, 1500, 2, 0.3, 0.25, 1),\n        # Case B: Moderate artifact, lower prevalence, added biological complexity\n        (500, 1200, 3, 0.2, 0.5, 2),\n        # Case C: No artifact control\n        (500, 1200, 2, 0.0, 1.0, 3),\n        # Case D: Artifact too rare to be a coherent cluster\n        (400, 1000, 2, 0.02, 0.2, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        N, G, B, p, a, seed = params\n        detection_result = run_simulation(N, G, B, p, a, seed)\n        results.append(detection_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在清理数据并去除技术伪影之后，下一步是通过聚类将细胞分组以识别不同的细胞群体。一个常见的挑战是为聚类选择合适的“粒度”或分辨率。本练习将探讨一种数据驱动的方法，它基于评估聚类结果的稳定性来为Louvain社区发现算法寻找最优的分辨率参数$ \\gamma $ 。",
            "id": "2371617",
            "problem": "您的任务是设计并实现一个算法，用于在分析单细胞相似性图的细胞异质性时，根据所得分区的稳定性，自动建议 Louvain 聚类的最优分辨率参数。您的实现必须是一个完整且可运行的程序，为一组预定义的测试用例执行以下任务。\n\n基本基础和定义：\n- 一个逐个细胞的相似性图表示为一个无向加权图，其邻接矩阵为 $A \\in \\mathbb{R}^{n \\times n}$，其中 $A_{ij} \\ge 0$，$A_{ij} = A_{ji}$，且 $A_{ii} = 0$。\n- 节点 $i$ 的度为 $k_i = \\sum_{j=1}^n A_{ij}$，$m = \\frac{1}{2} \\sum_{i,j=1}^n A_{ij}$ 是总边权。\n- 优化的 Louvain 目标是具有分辨率参数 $\\gamma  0$ 的广义模块度：\n$$\nQ(\\gamma) \\;=\\; \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\Big( A_{ij} \\;-\\; \\gamma \\frac{k_i k_j}{2m} \\Big) \\, \\mathbb{1}\\{c_i = c_j\\},\n$$\n其中 $c_i$ 是节点 $i$ 的社区标签，$\\mathbb{1}\\{\\cdot\\}$ 是指示函数。\n- 聚类稳定性使用调整兰德指数 (Adjusted Rand Index, ARI) 进行量化。对于 $n$ 个项目的两个分区，其列联表条目为 $n_{ij}$，行和为 $a_i = \\sum_j n_{ij}$，列和为 $b_j = \\sum_i n_{ij}$，则调整兰德指数为\n$$\n\\mathrm{ARI} \\;=\\; \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} \\;-\\; \\frac{\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}}{\\binom{n}{2}}}{\\frac{1}{2}\\Big( \\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2} \\Big) \\;-\\; \\frac{\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}}{\\binom{n}{2}}}.\n$$\n- 给定 $\\gamma$ 的稳定性得分定义为在类 Louvain 的局部移动过程的 $R$ 次独立运行（不同的随机种子）中，平均成对调整兰德指数。\n\n算法要求：\n- 实现一个类 Louvain 的局部移动算法，对于固定的 $\\gamma$，该算法从每个节点位于其自身的社区开始，并重复尝试将单个节点移动到相邻社区，前提是这样做会增加 $Q(\\gamma)$。在每一步中：\n  - 以随机顺序访问节点。\n  - 对于一个节点 $i$，考虑将其移动到任何相邻节点的社区中（以及保留在原社区的选项）。对于每个候选移动，计算产生的模块度 $Q(\\gamma)$，并选择产生最大增益的移动。随机打破平局。当且仅当移动严格增加 $Q(\\gamma)$ 时，才应用最佳移动。\n  - 重复遍历，直到完整一轮遍历不再产生任何增加 $Q(\\gamma)$ 的移动。\n- 对于候选集中的每个 $\\gamma$，使用不同的随机种子运行局部移动算法 $R$ 次。计算：\n  - $R$ 个分区之间的平均成对调整兰德指数，作为稳定性得分 $S(\\gamma)$。\n  - $R$ 次运行的平均模块度 $\\overline{Q}(\\gamma)$。\n  - $R$ 次运行的平均聚类数 $\\overline{K}(\\gamma)$。\n- 建议分辨率的选择规则：\n  - 选择最大化 $S(\\gamma)$ 的 $\\gamma^\\star$。\n  - 如果多个 $\\gamma$ 值在 $10^{-8}$ 的容差范围内共享最大 $S(\\gamma)$，则选择具有最大 $\\overline{Q}(\\gamma)$ 的那个。\n  - 如果仍然平局，选择最小的 $\\gamma$。\n\n测试套件：\n在以下三个合成图上实现并评估您的方法。每个图由块结构参数定义，您必须根据这些参数构建 $A$。\n- 案例 1（两个分离良好的组）：\n  - 组大小：$[5, 5]$。\n  - 组内边权：$1.0$。\n  - 组间边权：$0.05$（在来自不同组的任意两个节点之间是均匀的）。\n  - 候选 $\\gamma$ 值：$[0.5, 1.0, 1.5, 2.0]$。\n  - 每个 $\\gamma$ 的运行次数：$R = 6$。\n- 案例 2（同质图）：\n  - 一个包含 $n = 9$ 个节点的完全图，所有非对角线权重为 $1.0$，对角线元素为 $0$。\n  - 候选 $\\gamma$ 值：$[0.1, 0.5, 1.0, 2.0]$。\n  - 每个 $\\gamma$ 的运行次数：$R = 6$。\n- 案例 3（三个中等分离的组）：\n  - 组大小：$[4, 4, 4]$。\n  - 组内边权：$1.0$。\n  - 组间边权：$0.1$（在来自不同组的任意两个节点之间是均匀的）。\n  - 候选 $\\gamma$ 值：$[0.5, 1.0, 1.5, 2.5]$。\n  - 每个 $\\gamma$ 的运行次数：$R = 6$。\n\n块图的 $A$ 构建方法：\n- 对于一个具有组大小 $[s_1, s_2, \\dots, s_g]$ 的块模型，如果节点 $i$ 和 $j$ 在同一个组中且 $i \\ne j$，则设置 $A_{ij} = 1.0$；如果它们在不同的组中，则设置 $A_{ij} = w_{\\text{between}}$。将所有对角线元素设置为 $0$。\n- 对于同质图，对所有 $i \\ne j$ 设置 $A_{ij} = 1.0$，并设置 $A_{ii} = 0$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个案例各自建议的 $\\gamma^\\star$，按顺序排列，形式为用方括号括起来的逗号分隔列表。例如，输出应类似于 $[1.0,0.5,1.5]$。\n- 每个条目必须是单个浮点数，且等于该案例的候选 $\\gamma$ 值之一。\n\n所有计算都是无量纲的；不涉及物理单位。不涉及角度。百分比（如果导出）必须表示为小数，但此处的最终输出是指定格式的浮点数。",
            "solution": "问题陈述已经过严格评估，并被确定为**有效**。它在科学上基于网络科学和计算生物学的既定原则，在数学和算法上是适定的，并为可解的计算任务提供了一套完整且一致的要求。没有违反基本原则、定义模糊或信息缺失的情况，这些情况会妨碍唯一且可验证的解决方案。\n\n任务是通过评估分区的稳定性来确定 Louvain 社区检测的最优分辨率参数 $\\gamma$。该方法是为每个候选 $\\gamma$ 多次执行局部移动算法，然后选择在多次运行中产生最一致（稳定）聚类结果的 $\\gamma$。\n\n该方法通过以下步骤序列为每个测试用例实现。\n\n**1. 图表示**\n\n对于每个测试用例，根据提供的块模型参数构建一个邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$。给定组大小、组内边权 $w_{\\text{in}}$ 和组间边权 $w_{\\text{out}}$，矩阵元素 $A_{ij}$ 定义如下：\n$$\nA_{ij} = \\begin{cases}\n    w_{\\text{in}}  \\text{如果节点 } i \\text{ 和 } j \\text{ 在同一个组中且 } i \\neq j \\\\\n    w_{\\text{out}}  \\text{如果节点 } i \\text{ 和 } j \\text{ 在不同的组中} \\\\\n    0  \\text{如果 } i = j\n\\end{cases}\n$$\n每个节点的度 $k_i = \\sum_{j=1}^n A_{ij}$ 和图的总边权 $m = \\frac{1}{2}\\sum_{i,j} A_{ij}$ 都被预先计算，因为它们对模块度计算至关重要。\n\n**2. 类 Louvain 局部移动算法**\n\n聚类过程的核心是一种局部移动启发式算法，旨在优化广义模块度函数 $Q(\\gamma)$：\n$$\nQ(\\gamma) = \\frac{1}{2m} \\sum_{i,j} \\left( A_{ij} - \\gamma \\frac{k_i k_j}{2m} \\right) \\mathbb{1}\\{c_i = c_j\\}\n$$\n对每个潜在的节点移动直接重新计算 $Q(\\gamma)$ 的计算成本过高。相反，我们计算将一个节点 $i$ 从其当前社区 $C_{\\text{old}}$ 移动到候选社区 $C_{\\text{new}}$ 所导致的模块度变化量 $\\Delta Q$。这个变化由以下高效公式给出：\n$$\n\\Delta Q = \\frac{k_{i, \\text{new}} - k_{i, \\text{old}}}{m} - \\frac{\\gamma k_i}{2m^2} \\left( \\Sigma_{\\text{tot}, \\text{new}} - \\Sigma_{\\text{tot}, \\text{old}} + k_i \\right)\n$$\n其中 $k_{i,C}$ 是连接节点 $i$ 到社区 $C$ 中节点的边的权重之和，$\\Sigma_{\\text{tot}, C}$ 是社区 $C$ 中所有节点的度之和。\n\n算法按以下步骤进行：\n- **初始化**：每个节点 $i$ 都被分配到其自己的唯一社区 $c_i = i$。\n- **迭代**：算法重复遍历所有节点。在每次遍历中：\n    1. 节点以随机顺序访问，以避免固定顺序带来的任何偏差。\n    2. 对于每个节点 $i$，我们考虑将其移动到其某个邻居的社区中。\n    3. 计算每次潜在移动的 $\\Delta Q$。\n    4. 如果最大 $\\Delta Q$ 严格为正，则执行移动。最大增益的平局情况被随机打破。\n    5. 社区级统计数据，如 $\\Sigma_{\\text{tot}, C}$，在每次移动后更新。\n- **终止**：当对所有节点的完整遍历不再产生任何导致 $Q(\\gamma)$ 严格增加的移动时，该过程停止，表明已达到局部最优。\n\n**3. 使用调整兰德指数 (ARI) 进行稳定性量化**\n\n对于每个候选 $\\gamma$，局部移动算法执行 $R$ 次，每次使用不同的随机种子，从而产生一组 $R$ 个分区。在此分辨率下的聚类稳定性通过这些分区所有对之间的平均成对调整兰德指数 (ARI) 来量化。ARI 是衡量两个数据聚类之间相似性的指标，并已针对偶然性进行了校正。给定两个分区，形成一个列联表 $n_{ij}$。ARI 使用以下公式计算：\n$$\n\\mathrm{ARI} = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - \\frac{[\\sum_i \\binom{a_i}{2}][\\sum_j \\binom{b_j}{2}]}{\\binom{n}{2}}}{\\frac{1}{2}[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}] - \\frac{[\\sum_i \\binom{a_i}{2}][\\sum_j \\binom{b_j}{2}]}{\\binom{n}{2}}}\n$$\n其中 $a_i = \\sum_j n_{ij}$ 和 $b_j = \\sum_i n_{ij}$ 是列联表的边际和，$n$ 是节点的总数。二项式系数 $\\binom{k}{2}$ 计算为 $k(k-1)/2$。那么，给定 $\\gamma$ 的稳定性得分是 $S(\\gamma) = \\text{mean}(\\text{ARI}_{jk})$，其中 $1 \\le j  k \\le R$。\n\n**4. 最优分辨率选择**\n\n最优分辨率参数 $\\gamma^\\star$ 是根据一个确定性的三层规则从候选值集合中选择的：\n1.  主要标准：选择最大化稳定性得分 $S(\\gamma)$ 的 $\\gamma$ 值。使用 $10^{-8}$ 的容差来识别最大值的平局情况。\n2.  第一平局决胜规则：如果多个 $\\gamma$ 值并列，则从此子集中选择使平均模块度 $\\overline{Q}(\\gamma)$（在 $R$ 次运行中取平均）最大化的那个。\n3.  第二平局决胜规则：如果仍然存在平局，则在剩余的候选中选择最小的 $\\gamma$ 值。\n\n此过程被系统地应用于问题中指定的三个测试用例中的每一个，以得出最终建议的分辨率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_graph(group_sizes, w_in, w_out):\n    \"\"\"Constructs the adjacency matrix for a block model graph.\"\"\"\n    n_nodes = sum(group_sizes)\n    A = np.zeros((n_nodes, n_nodes))\n    node_to_group = np.zeros(n_nodes, dtype=int)\n    \n    start_idx = 0\n    for i, size in enumerate(group_sizes):\n        end_idx = start_idx + size\n        node_to_group[start_idx:end_idx] = i\n        start_idx = end_idx\n\n    for i in range(n_nodes):\n        for j in range(i + 1, n_nodes):\n            if node_to_group[i] == node_to_group[j]:\n                weight = w_in\n            else:\n                weight = w_out\n            A[i, j] = A[j, i] = weight\n            \n    return A\n\ndef n_choose_2(n):\n    \"\"\"Computes the binomial coefficient C(n, 2).\"\"\"\n    if n  2:\n        return 0\n    return n * (n - 1) // 2\n\ndef calculate_ari(labels_1, labels_2):\n    \"\"\"Calculates the Adjusted Rand Index between two clusterings.\"\"\"\n    n = len(labels_1)\n    if n = 1:\n        return 1.0\n\n    # Relabel to contiguous integers starting from 0\n    _, u_labels_1 = np.unique(labels_1, return_inverse=True)\n    _, u_labels_2 = np.unique(labels_2, return_inverse=True)\n    \n    n_labels_1 = np.max(u_labels_1) + 1 if len(u_labels_1) > 0 else 0\n    n_labels_2 = np.max(u_labels_2) + 1 if len(u_labels_2) > 0 else 0\n\n    contingency = np.zeros((n_labels_1, n_labels_2), dtype=np.int64)\n    for i in range(n):\n        contingency[u_labels_1[i], u_labels_2[i]] += 1\n\n    sum_comb_nij = np.sum([n_choose_2(nij) for nij in contingency.flat])\n    \n    sum_comb_a = np.sum([n_choose_2(k) for k in np.sum(contingency, axis=1)])\n    sum_comb_b = np.sum([n_choose_2(k) for k in np.sum(contingency, axis=0)])\n    \n    total_comb = n_choose_2(n)\n    if total_comb == 0:\n        return 1.0\n\n\n    expected_index = (sum_comb_a * sum_comb_b) / total_comb\n    max_index = (sum_comb_a + sum_comb_b) / 2\n    \n    denominator = max_index - expected_index\n    if denominator == 0:\n        # This occurs on trivial clusterings. Conventionally ARI is 0,\n        # unless it is a perfect match (numerator is also 0).\n        return 0.0\n\n    numerator = sum_comb_nij - expected_index\n    return numerator / denominator\n\n\ndef calculate_modularity(A, gamma, communities, two_m):\n    \"\"\"Calculates the modularity of a given partition.\"\"\"\n    if two_m == 0:\n        return 0.0\n    \n    mod = 0.0\n    k = A.sum(axis=1)\n    unique_comms = np.unique(communities)\n\n    for comm_id in unique_comms:\n         nodes_in_comm = np.where(communities == comm_id)[0]\n         subgraph_A = A[np.ix_(nodes_in_comm, nodes_in_comm)]\n         sum_in = np.sum(subgraph_A)\n         sum_tot = np.sum(k[nodes_in_comm])\n         mod += sum_in - gamma * sum_tot**2 / two_m\n    \n    return mod / two_m\n\n\ndef louvain_local_moving(A, gamma, rng):\n    \"\"\"Performs Louvain-like local moving to optimize modularity.\"\"\"\n    n = A.shape[0]\n    k = A.sum(axis=1)\n    two_m = k.sum()\n\n    if two_m == 0:\n        return np.arange(n)\n\n    communities = np.arange(n)\n    sigma_tot = np.copy(k) # Sigma_tot for each community\n    \n    while True:\n        moved = False\n        node_order = rng.permutation(n)\n        \n        for i in node_order:\n            old_comm_id = communities[i]\n            ki = k[i]\n            \n            # Efficiently compute sum of weights to each community\n            k_i_comms = np.bincount(communities, weights=A[i], minlength=n)\n            k_i_in = k_i_comms[old_comm_id]\n\n            neighbors = np.where(A[i] > 0)[0]\n            cand_comm_ids = np.unique(communities[neighbors])\n            \n            best_gain = 0.0\n            best_comm_id = old_comm_id\n            \n            potential_gains = []\n            potential_comms = []\n\n            for cand_comm_id in cand_comm_ids:\n                if cand_comm_id == old_comm_id:\n                    continue\n\n                k_i_to = k_i_comms[cand_comm_id]\n                sigma_tot_old = sigma_tot[old_comm_id]\n                sigma_tot_new = sigma_tot[cand_comm_id]\n\n                gain = (k_i_to - k_i_in) - gamma * ki * (sigma_tot_new - sigma_tot_old + ki) / two_m\n                potential_gains.append(gain)\n                potential_comms.append(cand_comm_id)\n            \n            if potential_gains:\n                max_gain = np.max(potential_gains)\n                if max_gain > 1e-12: # Strict increase, with a small tolerance for floating point\n                    max_indices = np.where(np.abs(potential_gains - max_gain)  1e-12)[0]\n                    chosen_idx = rng.choice(max_indices)\n                    best_gain = potential_gains[chosen_idx]\n                    best_comm_id = potential_comms[chosen_idx]\n\n            if best_gain > 0:\n                moved = True\n                sigma_tot[old_comm_id] -= ki\n                sigma_tot[best_comm_id] += ki\n                communities[i] = best_comm_id\n        \n        if not moved:\n            break\n            \n    return communities\n\ndef solve():\n    \"\"\"Main solver function.\"\"\"\n    \n    test_cases = [\n        { # Case 1\n            \"group_sizes\": [5, 5], \"w_in\": 1.0, \"w_out\": 0.05,\n            \"gammas\": [0.5, 1.0, 1.5, 2.0], \"R\": 6\n        },\n        { # Case 2\n            \"group_sizes\": [9], \"w_in\": 1.0, \"w_out\": 1.0, # Homogeneous\n            \"gammas\": [0.1, 0.5, 1.0, 2.0], \"R\": 6\n        },\n        { # Case 3\n            \"group_sizes\": [4, 4, 4], \"w_in\": 1.0, \"w_out\": 0.1,\n            \"gammas\": [0.5, 1.0, 1.5, 2.5], \"R\": 6\n        }\n    ]\n    \n    base_seed = 42 # For deterministic \"random\" runs\n    final_gammas = []\n    \n    for case in test_cases:\n        A = construct_graph(case[\"group_sizes\"], case[\"w_in\"], case[\"w_out\"])\n        n = A.shape[0]\n        two_m = A.sum()\n        \n        results_per_gamma = []\n        \n        for gamma in case[\"gammas\"]:\n            partitions = []\n            modularities = []\n            \n            for r in range(case[\"R\"]):\n                rng = np.random.default_rng(base_seed + r)\n                partition = louvain_local_moving(A, gamma, rng)\n                partitions.append(partition)\n                \n                mod = calculate_modularity(A, gamma, partition, two_m)\n                modularities.append(mod)\n\n            aris = []\n            if case[\"R\"] > 1:\n                for i in range(case[\"R\"]):\n                    for j in range(i + 1, case[\"R\"]):\n                        ari = calculate_ari(partitions[i], partitions[j])\n                        aris.append(ari)\n            \n            S_gamma = np.mean(aris) if aris else 1.0\n            Q_bar_gamma = np.mean(modularities)\n            \n            results_per_gamma.append((S_gamma, Q_bar_gamma, gamma))\n            \n        # Selection rule\n        # 1. Maximize S(gamma)\n        max_S = -np.inf\n        for S, Q, g in results_per_gamma:\n            if S > max_S:\n                max_S = S\n                \n        tied_on_S = []\n        for S, Q, g in results_per_gamma:\n            if S >= max_S - 1e-8:\n                tied_on_S.append((S, Q, g))\n\n        # 2. Tie-break with Q_bar\n        max_Q = -np.inf\n        for S, Q, g in tied_on_S:\n            if Q > max_Q:\n                max_Q = Q\n        \n        tied_on_Q = []\n        for S, Q, g in tied_on_S:\n            if np.isclose(Q, max_Q):\n                tied_on_Q.append((S, Q, g))\n\n        # 3. Tie-break with smallest gamma\n        tied_on_Q.sort(key=lambda x: x[2])\n        best_gamma = tied_on_Q[0][2]\n        \n        final_gammas.append(best_gamma)\n\n    print(f\"[{','.join(map(str, final_gammas))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "一旦我们识别出稳健的细胞簇，下一个目标就是确定它们的生物学身份。本练习介绍了一种超越单个标记基因的高级方法，即“基因模块”，它通过利用相关基因的共表达特性，为定义和识别细胞类型提供了一种更稳定、更可靠的方法。这种方法是现代单细胞数据分析中增强信号、降低噪声的核心策略 。",
            "id": "2371687",
            "problem": "您的任务是形式化并实现一个稳健的泛化方法，将单个“标记基因”推广为“标记基因模块”，用于在单细胞基因表达数据中识别目标细胞类型。其核心思想是利用共表达：与一个种子标记基因在统计上共表达的一小组基因，相比任何单个基因，能在异质细胞中产生更稳定的信号。\n\n从以下基本基础开始：\n- 分子生物学中心法则：基因转录为信使核糖核酸 (mRNA)，其数量可以被量化以估计基因表达水平。\n- 一个基因表达实验会产生一个矩阵，其中行是基因，列是细胞。条目 $X_{g,c} \\ge 0$ 是基因 $g$ 在细胞 $c$ 中测得的丰度（例如，计数）。\n- 共表达可以通过皮尔逊相关系数进行量化。跨细胞的标准化将原始表达值转换为无量纲的分数，从而使得不同基因之间具有可比性。\n\n您的程序必须为测试套件中的每个数据集实现以下流程。\n\n定义与步骤：\n1. 数据模型。设 $G = \\{g_1,\\dots,g_p\\}$ 为 $p$ 个基因的集合，$C = \\{c_1,\\dots,c_n\\}$ 为 $n$ 个细胞的集合。给定一个非负矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{p \\times n}$ 和一个二元标签向量 $y \\in \\{0,1\\}^n$，该向量指示每个细胞 $c$ 是否为目标类型 ($y_c = 1$) 或不是 ($y_c = 0$)。同时给定一个种子标记基因 $g^\\ast \\in G$ 和一个相关性阈值 $\\tau \\in [0,1]$。\n2. 共表达与模块构建。\n   - 对于每个基因 $g \\in G$，计算向量 $X_{g^\\ast,\\cdot}$ 和 $X_{g,\\cdot}$ 在 $n$ 个细胞间的皮尔逊相关性 $r(g^\\ast,g)$，使用总体公式：即对中心化变量使用总体标准差（分母为 $n$）。\n   - 排除任何在细胞间方差为零的基因（总体标准差等于 $0$），无论是在计算相关性还是后续评分中。\n   - 定义在阈值 $\\tau$ 下的标记基因模块为\n     $$ M(\\tau) \\equiv \\{ g \\in G \\,:\\, r(g^\\ast,g) \\ge \\tau \\ \\text{and}\\ r(g^\\ast,g)  0 \\} \\cup \\{ g^\\ast \\}. $$\n     如果 $M(\\tau)$ 中的任何基因方差为零，则将其从 $M(\\tau)$ 中移除。种子基因 $g^\\ast$ 总是被包含在内，除非其方差为零；如果方差为零，则仅将其用于单基因基线比较，并在需要时将其从模块中排除，以避免除以零。\n3. 标准化与评分。\n   - 对于每个方差非零的基因 $g$，使用总体均值和总体标准差计算其在各细胞中的标准化 $z$-分数：\n     $$ z_{g,c} \\equiv \\frac{X_{g,c} - \\mu_g}{\\sigma_g}, \\quad \\mu_g \\equiv \\frac{1}{n}\\sum_{c=1}^n X_{g,c}, \\quad \\sigma_g \\equiv \\sqrt{\\frac{1}{n}\\sum_{c=1}^n (X_{g,c} - \\mu_g)^2}. $$\n   - 定义每个细胞 $c$ 的模块分数为模块内基因的标准化表达值的平均值：\n     $$ s_c \\equiv \\frac{1}{|M(\\tau)|}\\sum_{g \\in M(\\tau)} z_{g,c}. $$\n   - 定义每个细胞 $c$ 的单基因分数，仅使用种子基因：\n     $$ s_c^{(1)} \\equiv z_{g^\\ast,c}. $$\n4. 分类规则。\n   - 根据分数的符号，使用模块和单基因分别预测目标细胞类型：\n     $$ \\widehat{y}^{(M)}_c \\equiv \\mathbb{I}[\\, s_c  0 \\,], \\qquad \\widehat{y}^{(1)}_c \\equiv \\mathbb{I}[\\, s_c^{(1)}  0 \\,], $$\n     其中 $\\mathbb{I}[\\cdot]$ 是指示函数，当其参数为真时等于 $1$，否则等于 $0$。\n   - 计算每种情况下的正确预测数量：\n     $$ N^{(M)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(M)}_c = y_c \\,], \\qquad N^{(1)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(1)}_c = y_c \\,]. $$\n5. 通过合并标准化差异（科恩 d 值）进行效应大小比较。计算模块分数和单基因分数的效应大小。设 $S = \\{ c : y_c = 1 \\}$ 和 $T = \\{ c : y_c = 0 \\}$，其大小分别为 $n_S$ 和 $n_T$。对于一个分数向量 $u \\in \\mathbb{R}^n$，设\n   $$ \\overline{u}_S \\equiv \\frac{1}{n_S}\\sum_{c \\in S} u_c, \\quad \\overline{u}_T \\equiv \\frac{1}{n_T}\\sum_{c \\in T} u_c, $$\n   $$ s_S^2 \\equiv \\frac{1}{n_S - 1} \\sum_{c \\in S} (u_c - \\overline{u}_S)^2, \\quad s_T^2 \\equiv \\frac{1}{n_T - 1} \\sum_{c \\in T} (u_c - \\overline{u}_T)^2, $$\n   $$ s_p \\equiv \\sqrt{ \\frac{(n_S - 1)s_S^2 + (n_T - 1)s_T^2}{n_S + n_T - 2} }, \\quad d(u) \\equiv \\frac{\\overline{u}_S - \\overline{u}_T}{s_p}. $$\n   将此方法应用于 $u = s$（模块）和 $u = s^{(1)}$（单基因）。\n6. 每个数据集的输出规格。报告列表\n   $$ \\big[\\, |M(\\tau)|,\\ \\mathrm{round}(d(s^{(1)}), 3),\\ \\mathrm{round}(d(s), 3),\\ N^{(M)} - N^{(1)} \\,\\big], $$\n   其中 $\\mathrm{round}(\\cdot,3)$ 表示四舍五入到三位小数。\n\n测试套件。在以下三个数据集上实现您的解决方案。行按 $(g_1,g_2,g_3,g_4,g_5,g_6)$ 排序，列按 $(c_1,\\dots,c_n)$ 排序。种子基因始终为 $g^\\ast = g_1$。为每个数据集提供了标签向量 $y$。\n\n- 数据集 A（理想情况；强共表达模块）：\n  $$ X^{(A)} = \\begin{bmatrix}\n  10  11  9  10  1  2  1  1 \\\\\n  9  10  8  9  1  1  2  1 \\\\\n  1  1  1  2  9  8  10  9 \\\\\n  1  2  1  1  8  9  9  8 \\\\\n  2  1  2  2  2  2  1  2 \\\\\n  6  7  5  6  1  1  1  2\n  \\end{bmatrix}, \\quad y^{(A)} = [\\,1,1,1,1,0,0,0,0\\,], \\quad \\tau^{(A)} = 0.6. $$\n- 数据集 B（边界条件；阈值强制形成单基因模块）：\n  $$ X^{(B)} = X^{(A)}, \\quad y^{(B)} = y^{(A)}, \\quad \\tau^{(B)} = 1.0. $$\n- 数据集 C（脱落边缘案例；模块辅助恢复）：\n  $$ X^{(C)} = \\begin{bmatrix}\n  0  0  8  1  1  1 \\\\\n  7  6  7  1  1  1 \\\\\n  1  1  1  6  7  6 \\\\\n  1  1  1  5  6  5 \\\\\n  2  2  2  2  2  2 \\\\\n  5  5  6  1  1  1\n  \\end{bmatrix}, \\quad y^{(C)} = [\\,1,1,1,0,0,0\\,], \\quad \\tau^{(C)} = 0.3. $$\n\n最终输出格式。您的程序应生成单行输出，包含数据集 A、B 和 C 的结果，格式完全如下（无空格）：一个由三个数据集结果列表组成的逗号分隔列表，并用方括号括起来，例如\n`[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]`\n其中每个 $a_i$ 是一个整数，每个 $b_i,c_i$ 是一个四舍五入到三位小数的浮点数，而 $d_i$ 是一个整数。不得有任何其他打印文本。\n\n注意：\n- 以上所有数学实体（符号、变量、函数、运算符和数字）均以 LaTeX 格式书写，但您的实现必须遵循其计算定义。\n- 角度和物理单位不适用。",
            "solution": "我们通过聚合标准化的共表达基因，来形式化一个稳健的标记基因模块。其推理过程从分子生物学和统计学的基本定义出发，直至一个具体的算法。\n\n基本原理与动机。分子生物学中心法则指出，基因转录为信使核糖核酸 (mRNA)，通过测量其数量可以获得定量的基因表达。单细胞测量产生一个表达矩阵 $X_{g,c}$，其中跨细胞的变异性反映了生物异质性和技术噪声。单个标记基因 $g^\\ast$ 在出现脱落或细微变化时可能会失效，而一小组共表达基因可以通过平均化来消除噪声并放大真实的生物信号。从统计学上讲，如果共表达基因共享信号但具有部分独立的噪声，那么通过类似于独立性下方差可加性的论证，对其标准化表达值进行平均，可以大约按模块大小的比例减少方差。\n\n通过相关性衡量共表达。共表达的一个标准度量是皮尔逊相关性，它量化了两个基因向量在所有细胞中的线性一致性。对均值和方差都使用总体标准化（分母为 $n$），可以得到总体相关系数。用 $X^\\circ_{g,c} \\equiv X_{g,c} - \\mu_g$ 表示中心化向量，用 $\\sigma_g \\equiv \\sqrt{\\frac{1}{n} \\sum_c (X_{g,c} - \\mu_g)^2}$ 表示总体标准差，则相关性为\n$$ r(g^\\ast,g) \\equiv \\frac{1}{n}\\sum_{c=1}^n \\frac{(X_{g^\\ast,c} - \\mu_{g^\\ast})(X_{g,c} - \\mu_g)}{\\sigma_{g^\\ast}\\sigma_g}. $$\n如果 $\\sigma_g = 0$，则相关性未定义，该基因必须在计算相关性和后续评分中被排除，以避免除以零。\n\n模块定义。对于一个阈值 $\\tau \\in [0,1]$，定义\n$$ M(\\tau) \\equiv \\{ g \\in G \\,:\\, r(g^\\ast,g) \\ge \\tau \\ \\text{and}\\ r(g^\\ast,g)  0 \\} \\cup \\{ g^\\ast \\}, $$\n并移除其中任何方差为零的基因。正相关性约束确保了包含共同上调的基因，并排除了反向标记基因。种子基因 $g^\\ast$ 总是被包含在内，除非其方差为零；这保证了至少单基因基线是可用的。\n\n标准化与评分。对于每个保留的基因，计算其总体 $z$-分数\n$$ z_{g,c} \\equiv \\frac{X_{g,c} - \\mu_g}{\\sigma_g}. $$\n模块分数为标准化表达的平均值\n$$ s_c \\equiv \\frac{1}{|M(\\tau)|}\\sum_{g \\in M(\\tau)} z_{g,c}, $$\n而单基因基线为 $s_c^{(1)} \\equiv z_{g^\\ast,c}$。因为每个 $z_{g,\\cdot}$ 在所有细胞中的总体均值为 $0$，所以平均分数 $s_c$ 的总体均值也为 $0$，因此一个自然的、非参数的分类阈值是在 $0$ 处进行符号检验：\n$$ \\widehat{y}^{(M)}_c \\equiv \\mathbb{I}[\\, s_c  0 \\,], \\qquad \\widehat{y}^{(1)}_c \\equiv \\mathbb{I}[\\, s_c^{(1)}  0 \\,]. $$\n\n性能量化。我们计算两个指标：\n1. 正确预测的数量\n   $$ N^{(M)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(M)}_c = y_c \\,], \\qquad N^{(1)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(1)}_c = y_c \\,]. $$\n   提升量为 $N^{(M)} - N^{(1)}$。\n2. 标准化均值差异（科恩 d 值），反映目标细胞与非目标细胞之间的分离度。对于任何分数向量 $u \\in \\mathbb{R}^n$，设 $S = \\{ c : y_c = 1 \\}$，$T = \\{ c : y_c = 0 \\}$，均值为 $\\overline{u}_S$、$\\overline{u}_T$，样本方差为 $s_S^2$、$s_T^2$（分母分别为 $n_S - 1$ 和 $n_T - 1$），以及合并标准差\n   $$ s_p \\equiv \\sqrt{ \\frac{(n_S - 1)s_S^2 + (n_T - 1)s_T^2}{n_S + n_T - 2} }. $$\n   那么\n   $$ d(u) \\equiv \\frac{\\overline{u}_S - \\overline{u}_T}{s_p}. $$\n   我们报告单基因基线的 $d(s^{(1)})$ 和模块的 $d(s)$，每个值都四舍五入到三位小数。\n\n每个数据集的算法步骤：\n- 计算每个基因的总体均值 $\\mu_g$ 和标准差 $\\sigma_g$；在计算相关性和评分时丢弃零方差基因。\n- 为所有方差非零的基因 $g$ 计算总体皮尔逊相关性 $r(g^\\ast,g)$。\n- 通过包含 $g^\\ast$ 以及所有满足 $r(g^\\ast,g) \\ge \\tau$ 且严格为正的基因来构建 $M(\\tau)$。\n- 为 $g \\in M(\\tau) \\cup \\{g^\\ast\\}$ 计算 $z_{g,c}$，然后计算 $s_c$ 和 $s_c^{(1)}$，接着计算 $\\widehat{y}^{(M)}_c$、$\\widehat{y}^{(1)}_c$、$N^{(M)}$、$N^{(1)}$ 以及 $d(s)$、$d(s^{(1)})$。\n\n测试套件预期：\n- 对于 $\\tau = 0.6$ 的数据集 A，会产生一个有意义的模块，其中包含多个与 $g^\\ast = g_1$ 呈正共表达的基因，因此 $|M(\\tau)|$ 大于 $1$，并且 $d(s)$ 和分类准确率都很高。\n- 数据集 B 使用与 A 相同的数据，但 $\\tau = 1.0$，因此只有 $g^\\ast$ 满足阈值（其他基因无法达到 $1$ 的精确相关性），因此 $|M(\\tau)| = 1$，$d(s) = d(s^{(1)})$，并且 $N^{(M)} - N^{(1)} = 0$。\n- 数据集 C 在某些目标细胞中包含了 $g^\\ast$ 的脱落现象，而其共表达伙伴的表达量仍然很高。当 $\\tau = 0.3$ 时，至少有两个共表达基因加入模块；对 $z$-分数进行平均可以恢复目标细胞的信号，从而相对于单基因基线，提高了 $d(s)$ 和 $N^{(M)} - N^{(1)}$。\n\n程序精确实现这些步骤，并打印包含三个列表的单行输出\n$$ \\big[\\, |M(\\tau)|,\\ \\mathrm{round}(d(s^{(1)}), 3),\\ \\mathrm{round}(d(s), 3),\\ N^{(M)} - N^{(1)} \\,\\big] $$\n分别对应数据集 A、B 和 C，按要求连接成一个无空格的逗号分隔列表。",
            "answer": "```python\nimport numpy as np\n\ndef population_mean_std(X, axis):\n    \"\"\"\n    Compute population mean and population standard deviation (ddof=0).\n    Returns (mean, std).\n    \"\"\"\n    mu = np.mean(X, axis=axis)\n    sigma = np.std(X, axis=axis, ddof=0)\n    return mu, sigma\n\ndef population_pearson_corr(x, Y):\n    \"\"\"\n    Compute population Pearson correlation between a 1D vector x (length n)\n    and each row of 2D array Y (m x n). Uses population std (ddof=0).\n    Returns a 1D array of length m with correlations; returns 0.0 for zero-variance rows.\n    \"\"\"\n    # Center x and Y\n    x = np.asarray(x, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    n = x.shape[0]\n    x_mu = np.mean(x)\n    x_std = np.std(x, ddof=0)\n    # Handle zero variance in x (rare by construction)\n    if x_std == 0:\n        return np.zeros(Y.shape[0], dtype=float)\n    Xc = x - x_mu\n    Y_mu = np.mean(Y, axis=1)\n    Y_std = np.std(Y, axis=1, ddof=0)\n    Yc = Y - Y_mu[:, None]\n    # Compute covariance: mean of product of centered variables\n    cov = (Yc @ Xc) / n  # shape (m,)\n    # Avoid division by zero: where Y_std==0 set corr to 0\n    with np.errstate(divide='ignore', invalid='ignore'):\n        r = np.where(denom > 0, cov / denom, 0.0)\n    return r\n\ndef build_module(X, seed_idx, tau):\n    \"\"\"\n    Build marker gene module M(tau) around seed gene index seed_idx with threshold tau.\n    Exclude genes with zero population variance from module and correlation.\n    Only include positively correlated genes with r >= tau.\n    Always include the seed gene unless its variance is zero.\n    Returns a sorted list of unique gene indices in the module.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    p, n = X.shape\n    # Compute per-gene population std to filter zero-variance genes\n    mu_g = np.mean(X, axis=1)\n    std_g = np.std(X, axis=1, ddof=0)\n    nonzero_var = std_g > 0\n    # Compute correlations for eligible genes\n    r = population_pearson_corr(X[seed_idx, :], X)\n    module = set()\n    # Include genes with positive correlation >= tau and nonzero variance\n    for g in range(p):\n        if nonzero_var[g] and (g == seed_idx or (r[g] > 0 and r[g] >= tau)):\n            module.add(g)\n    # Ensure seed inclusion if it has nonzero variance; if zero, exclude to avoid div-by-zero\n    if nonzero_var[seed_idx]:\n        module.add(seed_idx)\n    else:\n        module.discard(seed_idx)\n    # Remove any zero-variance genes (safety)\n    module = [g for g in module if nonzero_var[g]]\n    module.sort()\n    return module\n\ndef z_scores(X):\n    \"\"\"\n    Compute population z-scores per gene across cells.\n    X is (p x n). Returns Z of same shape.\n    For zero-variance genes, set z-scores to 0 (they will be excluded upstream anyway).\n    \"\"\"\n    mu, sigma = population_mean_std(X, axis=1)\n    # Avoid division by zero: where sigma==0, set to 1 to produce zeros\n    sigma_safe = np.where(sigma == 0, 1.0, sigma)\n    Z = (X - mu[:, None]) / sigma_safe[:, None]\n    return Z, mu, sigma\n\ndef cohen_d(u, y):\n    \"\"\"\n    Compute Cohen's d between groups y==1 and y==0 using pooled sample standard deviation.\n    u: 1D array of scores length n\n    y: 1D array of 0/1 labels length n\n    Returns float d. Assumes both groups have at least 2 samples and nonzero pooled variance.\n    \"\"\"\n    u = np.asarray(u, dtype=float)\n    y = np.asarray(y, dtype=int)\n    S = (y == 1)\n    T = (y == 0)\n    uS = u[S]\n    uT = u[T]\n    nS = uS.size\n    nT = uT.size\n    mS = np.mean(uS) if nS > 0 else 0.0\n    mT = np.mean(uT) if nT > 0 else 0.0\n    # Sample variances\n    sS2 = np.var(uS, ddof=1) if nS > 1 else 0.0\n    sT2 = np.var(uT, ddof=1) if nT > 1 else 0.0\n    # Pooled standard deviation\n    denom_df = (nS + nT - 2)\n    if denom_df = 0:\n        sp = 0.0\n    else:\n        sp = np.sqrt(((nS - 1) * sS2 + (nT - 1) * sT2) / denom_df)\n    if sp == 0.0:\n        return 0.0\n    return (mS - mT) / sp\n\ndef evaluate_dataset(X, y, seed_idx, tau):\n    \"\"\"\n    For a dataset (X, y), seed gene index, and threshold tau:\n    - Build module\n    - Compute z-scores\n    - Compute module score and single-gene score\n    - Classify with threshold > 0\n    - Count correct predictions\n    - Compute Cohen's d for both scores\n    Returns [module_size, d_single_rounded, d_module_rounded, improvement_int]\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=int)\n    p, n = X.shape\n    module = build_module(X, seed_idx, tau)\n    Z, _, _ = z_scores(X)\n    # Single-gene score\n    s_single = Z[seed_idx, :]\n    # Module score: average across genes in module\n    if len(module) == 0:\n        # This case shouldn't happen if seed has non-zero var, but as a fallback\n        s_module = np.zeros_like(s_single)\n    else:\n        s_module = np.mean(Z[module, :], axis=0)\n    # Predictions with threshold > 0\n    yhat_single = (s_single > 0).astype(int)\n    yhat_module = (s_module > 0).astype(int)\n    correct_single = int(np.sum(yhat_single == y))\n    correct_module = int(np.sum(yhat_module == y))\n    # Effect sizes\n    d_single = cohen_d(s_single, y)\n    d_module = cohen_d(s_module, y)\n    # Round to 3 decimals for reporting\n    d_single_r = round(float(d_single), 3)\n    d_module_r = round(float(d_module), 3)\n    improvement = int(correct_module - correct_single)\n    return [len(module), d_single_r, d_module_r, improvement]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Dataset A\n    XA = np.array([\n        [10, 11,  9, 10, 1, 2, 1, 1],\n        [ 9, 10,  8,  9, 1, 1, 2, 1],\n        [ 1,  1,  1,  2, 9, 8,10, 9],\n        [ 1,  2,  1,  1, 8, 9, 9, 8],\n        [ 2,  1,  2,  2, 2, 2, 1, 2],\n        [ 6,  7,  5,  6, 1, 1, 1, 2]\n    ], dtype=float)\n    yA = np.array([1,1,1,1,0,0,0,0], dtype=int)\n    seed_idx = 0  # g1\n    tauA = 0.6\n\n    # Dataset B (same X and y, higher tau)\n    XB = XA.copy()\n    yB = yA.copy()\n    tauB = 1.0\n\n    # Dataset C\n    XC = np.array([\n        [0, 0, 8, 1, 1, 1],\n        [7, 6, 7, 1, 1, 1],\n        [1, 1, 1, 6, 7, 6],\n        [1, 1, 1, 5, 6, 5],\n        [2, 2, 2, 2, 2, 2],\n        [5, 5, 6, 1, 1, 1]\n    ], dtype=float)\n    yC = np.array([1,1,1,0,0,0], dtype=int)\n    tauC = 0.3\n\n    test_cases = [\n        (XA, yA, seed_idx, tauA),\n        (XB, yB, seed_idx, tauB),\n        (XC, yC, seed_idx, tauC),\n    ]\n\n    results = []\n    for X, y, seed, tau in test_cases:\n        res = evaluate_dataset(X, y, seed, tau)\n        results.append(res)\n\n    # Final print statement in the exact required format (no spaces).\n    # Example: [[a1,b1,c1,d1],[a2,b2,c2,d2],[a3,b3,c3,d3]]\n    # Convert to string without spaces.\n    def to_str_list(lst):\n        return \"[\" + \",\".join(str(x) for x in lst) + \"]\"\n    output = \"[\" + \",\".join(to_str_list(r) for r in results) + \"]\"\n    print(output)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}