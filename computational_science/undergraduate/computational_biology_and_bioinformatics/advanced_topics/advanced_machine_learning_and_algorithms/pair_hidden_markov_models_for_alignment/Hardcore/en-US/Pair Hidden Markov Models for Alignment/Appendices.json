{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how a Pair Hidden Markov Model (PHMM) functions, it is essential to work directly with its probabilistic core. This exercise moves beyond abstract diagrams and requires you to perform the fundamental calculations that underpin all PHMM-based alignment algorithms. By enumerating all possible alignment paths for two short sequences and calculating their respective probabilities, you will compute the posterior expected number of insertions, providing a concrete understanding of how the model weighs different evolutionary scenarios .",
            "id": "2411573",
            "problem": "Consider a Pair Hidden Markov Model (pair HMM) for global alignment with five states: a begin state $B$, a match state $M$, two insertion states $I_{x}$ and $I_{y}$ (emitting a symbol from sequence $X$ or $Y$, respectively, aligned to a gap in the other), and an end state $E$. The alphabet is $\\{A,C\\}$. Transitions are allowed only as listed below, with the given probabilities; all other transitions have probability $0$.\n\n- From $B$: $B \\to M$ with probability $0.7$, $B \\to I_{x}$ with probability $0.25$, $B \\to I_{y}$ with probability $0.05$.\n- From $M$: $M \\to M$ with probability $0.4$, $M \\to I_{x}$ with probability $0.3$, $M \\to I_{y}$ with probability $0.1$, $M \\to E$ with probability $0.2$.\n- From $I_{x}$: $I_{x} \\to I_{x}$ with probability $0.5$, $I_{x} \\to M$ with probability $0.25$, $I_{x} \\to I_{y}$ with probability $0.05$, $I_{x} \\to E$ with probability $0.2$.\n- From $I_{y}$: $I_{y} \\to I_{y}$ with probability $0.5$, $I_{y} \\to M$ with probability $0.25$, $I_{y} \\to I_{x}$ with probability $0.05$, $I_{y} \\to E$ with probability $0.2$.\n\nEmission probabilities are as follows. In the match state $M$, a pair $(x,y) \\in \\{A,C\\} \\times \\{A,C\\}$ is emitted with probabilities\n$P_{M}(A,A)=0.5$, $P_{M}(A,C)=0.1$, $P_{M}(C,A)=0.2$, $P_{M}(C,C)=0.2$.\nIn the insertion state $I_{x}$, a single symbol $x \\in \\{A,C\\}$ is emitted with $P_{I_{x}}(A)=0.6$, $P_{I_{x}}(C)=0.4$.\nIn the insertion state $I_{y}$, a single symbol $y \\in \\{A,C\\}$ is emitted with $P_{I_{y}}(A)=0.7$, $P_{I_{y}}(C)=0.3$.\n\nYou are given two observed sequences: $X=AC$ and $Y=A$. A state path starts at $B$, emits exactly the given sequences in order according to the state emissions and their consumption rules (state $M$ consumes one symbol from each sequence, state $I_{x}$ consumes one symbol from $X$ only, and state $I_{y}$ consumes one symbol from $Y$ only), and then transitions to $E$. Define the “number of insertions” for a state path as the total count of emissions produced while in insertion states, that is, the number of emissions from $I_{x}$ plus the number of emissions from $I_{y}$ along the path.\n\nCompute the posterior expected number of insertions given the model and the observed sequences, that is, the expectation of this count under the distribution over all valid state paths conditioned on emitting $X$ and $Y$. Give your answer as an exact reduced fraction. Do not include any units.",
            "solution": "The problem asks for the posterior expected number of insertions for the alignment of sequences $X=AC$ and $Y=A$ given a specific Pair Hidden Markov Model (HMM).\n\nFirst, we formalize the problem. Let $\\Pi$ be the set of all valid state paths that start in the begin state $B$, generate the sequences $X$ and $Y$, and terminate in the end state $E$. For any given path $\\pi \\in \\Pi$, let $P(\\pi, X, Y)$ be the joint probability of the path and the observed sequences. The total probability of observing the sequences is $P(X, Y) = \\sum_{\\pi \\in \\Pi} P(\\pi, X, Y)$.\nLet $\\text{ins}(\\pi)$ denote the number of insertions for a path $\\pi$, which is defined as the total number of emissions from states $I_x$ and $I_y$. The posterior expected number of insertions is given by:\n$$E[\\text{ins} | X, Y] = \\sum_{\\pi \\in \\Pi} \\text{ins}(\\pi) P(\\pi | X, Y) = \\frac{\\sum_{\\pi \\in \\Pi} \\text{ins}(\\pi) P(\\pi, X, Y)}{P(X, Y)}$$\n\nA path must consume all symbols of $X$ and $Y$. Let $|X|=L_x=2$ and $|Y|=L_y=1$. Let $n_M$, $n_{I_x}$, and $n_{I_y}$ be the number of times the path visits states $M$, $I_x$, and $I_y$, respectively. State $M$ consumes one symbol from each sequence. State $I_x$ consumes one symbol from $X$. State $I_y$ consumes one symbol from $Y$. For a global alignment, we must have:\n$$n_M + n_{I_x} = L_x = 2$$\n$$n_M + n_{I_y} = L_y = 1$$\nSince $n_M, n_{I_x}, n_{I_y}$ must be non-negative integers, we can determine the possible values. From the second equation, $n_{I_y} = 1 - n_M$, which implies $n_M$ can only be $0$ or $1$.\n\nCase 1: $n_M = 1$.\nThis implies $n_{I_y} = 1 - 1 = 0$ and $n_{I_x} = 2 - 1 = 1$.\nThe number of insertions for any path in this category is $\\text{ins} = n_{I_x} + n_{I_y} = 1 + 0 = 1$.\n\nCase 2: $n_M = 0$.\nThis implies $n_{I_y} = 1 - 0 = 1$ and $n_{I_x} = 2 - 0 = 2$.\nThe number of insertions for any path in this category is $\\text{ins} = n_{I_x} + n_{I_y} = 2 + 1 = 3$.\n\nLet $P_1$ be the total probability of all paths with $1$ insertion, and $P_3$ be the total probability of all paths with $3$ insertions. Then $P(X, Y) = P_1 + P_3$. The expected value is:\n$$E[\\text{ins} | X, Y] = \\frac{1 \\cdot P_1 + 3 \\cdot P_3}{P_1 + P_3}$$\n\nWe now enumerate all possible paths for each case and calculate their probabilities. Let $a_{kl}$ be the transition probability from state $k$ to $l$, and $P_k(\\cdot)$ be the emission probability for state $k$. The sequences are $X=x_1x_2=AC$ and $Y=y_1=A$.\n\nCalculating $P_1$ (paths with $n_M=1$, $n_{I_x}=1$, $n_{I_y}=0$):\nThere are two possible orderings of the emitting states $\\{M, I_x\\}$:\nPath 1a: Sequence of emitting states is $(M, I_x)$. The full path is $B \\to M \\to I_x \\to E$.\nThis path must emit $(x_1, y_1) = (A, A)$ from state $M$ and $x_2 = C$ from state $I_x$.\nThe probability is $P_{1a} = a_{BM} \\cdot P_M(A,A) \\cdot a_{MI_x} \\cdot P_{I_x}(C) \\cdot a_{I_xE}$.\n$P_{1a} = (0.7) \\cdot (0.5) \\cdot (0.3) \\cdot (0.4) \\cdot (0.2) = \\frac{7}{10} \\cdot \\frac{1}{2} \\cdot \\frac{3}{10} \\cdot \\frac{2}{5} \\cdot \\frac{1}{5} = \\frac{42}{5000} = \\frac{21}{2500}$.\n\nPath 1b: Sequence of emitting states is $(I_x, M)$. The full path is $B \\to I_x \\to M \\to E$.\nThis path must emit $x_1 = A$ from state $I_x$ and $(x_2, y_1) = (C, A)$ from state $M$.\nThe probability is $P_{1b} = a_{BI_x} \\cdot P_{I_x}(A) \\cdot a_{I_xM} \\cdot P_M(C,A) \\cdot a_{ME}$.\n$P_{1b} = (0.25) \\cdot (0.6) \\cdot (0.25) \\cdot (0.2) \\cdot (0.2) = \\frac{1}{4} \\cdot \\frac{3}{5} \\cdot \\frac{1}{4} \\cdot \\frac{1}{5} \\cdot \\frac{1}{5} = \\frac{3}{2000}$.\n\nTotal probability for Case 1: $P_1 = P_{1a} + P_{1b} = \\frac{21}{2500} + \\frac{3}{2000} = \\frac{84}{10000} + \\frac{15}{10000} = \\frac{99}{10000}$.\n\nCalculating $P_3$ (paths with $n_M=0$, $n_{I_x}=2$, $n_{I_y}=1$):\nThere are $\\binom{3}{1}=3$ possible orderings of the emitting states $\\{I_x, I_x, I_y\\}$.\nPath 2a: Sequence of states is $(I_x, I_x, I_y)$. Path: $B \\to I_x \\to I_x \\to I_y \\to E$.\nEmissions: $I_x$ emits $x_1=A$, second $I_x$ emits $x_2=C$, $I_y$ emits $y_1=A$.\n$P_{2a} = a_{BI_x} \\cdot P_{I_x}(A) \\cdot a_{I_xI_x} \\cdot P_{I_x}(C) \\cdot a_{I_xI_y} \\cdot P_{I_y}(A) \\cdot a_{I_yE}$.\n$P_{2a} = (0.25)(0.6)(0.5)(0.4)(0.05)(0.7)(0.2) = \\frac{1}{4}\\frac{3}{5}\\frac{1}{2}\\frac{2}{5}\\frac{1}{20}\\frac{7}{10}\\frac{1}{5} = \\frac{42}{200000} = \\frac{21}{100000}$.\n\nPath 2b: Sequence of states is $(I_x, I_y, I_x)$. Path: $B \\to I_x \\to I_y \\to I_x \\to E$.\nEmissions: $I_x$ emits $x_1=A$, $I_y$ emits $y_1=A$, second $I_x$ emits $x_2=C$.\n$P_{2b} = a_{BI_x} \\cdot P_{I_x}(A) \\cdot a_{I_xI_y} \\cdot P_{I_y}(A) \\cdot a_{I_yI_x} \\cdot P_{I_x}(C) \\cdot a_{I_xE}$.\n$P_{2b} = (0.25)(0.6)(0.05)(0.7)(0.05)(0.4)(0.2) = \\frac{1}{4}\\frac{3}{5}\\frac{1}{20}\\frac{7}{10}\\frac{1}{20}\\frac{2}{5}\\frac{1}{5} = \\frac{42}{2000000} = \\frac{21}{1000000}$.\n\nPath 2c: Sequence of states is $(I_y, I_x, I_x)$. Path: $B \\to I_y \\to I_x \\to I_x \\to E$.\nEmissions: $I_y$ emits $y_1=A$, first $I_x$ emits $x_1=A$, second $I_x$ emits $x_2=C$.\n$P_{2c} = a_{BI_y} \\cdot P_{I_y}(A) \\cdot a_{I_yI_x} \\cdot P_{I_x}(A) \\cdot a_{I_xI_x} \\cdot P_{I_x}(C) \\cdot a_{I_xE}$.\n$P_{2c} = (0.05)(0.7)(0.05)(0.6)(0.5)(0.4)(0.2) = \\frac{1}{20}\\frac{7}{10}\\frac{1}{20}\\frac{3}{5}\\frac{1}{2}\\frac{2}{5}\\frac{1}{5} = \\frac{42}{1000000} = \\frac{21}{500000}$.\n\nTotal probability for Case 2: $P_3 = P_{2a} + P_{2b} + P_{2c} = \\frac{21}{100000} + \\frac{21}{1000000} + \\frac{42}{1000000} = \\frac{210}{1000000} + \\frac{21}{1000000} + \\frac{42}{1000000} = \\frac{273}{1000000}$.\n\nNow we can compute the expected value.\nNumerator: $1 \\cdot P_1 + 3 \\cdot P_3 = \\frac{99}{10000} + 3 \\cdot \\frac{273}{1000000} = \\frac{9900}{1000000} + \\frac{819}{1000000} = \\frac{10719}{1000000}$.\nDenominator: $P_1 + P_3 = \\frac{99}{10000} + \\frac{273}{1000000} = \\frac{9900}{1000000} + \\frac{273}{1000000} = \\frac{10173}{1000000}$.\n\n$E[\\text{ins} | X, Y] = \\frac{\\frac{10719}{1000000}}{\\frac{10173}{1000000}} = \\frac{10719}{10173}$.\n\nTo simplify the fraction, we find the greatest common divisor of the numerator and denominator.\nThe sum of digits of $10719$ is $1+0+7+1+9 = 18$, so it is divisible by $9$.\nThe sum of digits of $10173$ is $1+0+1+7+3 = 12$, so it is divisible by $3$ but not $9$.\nWe can divide both by $3$.\n$10719 \\div 3 = 3573$.\n$10173 \\div 3 = 3391$.\nThe fraction becomes $\\frac{3573}{3391}$.\nUsing the Euclidean algorithm for $\\text{gcd}(3573, 3391)$:\n$3573 = 1 \\cdot 3391 + 182$\n$3391 = 18 \\cdot 182 + 115$\n$182 = 1 \\cdot 115 + 67$\n$115 = 1 \\cdot 67 + 48$\n$67 = 1 \\cdot 48 + 19$\n$48 = 2 \\cdot 19 + 10$\n$19 = 1 \\cdot 10 + 9$\n$10 = 1 \\cdot 9 + 1$\nThe greatest common divisor is $1$. Thus, the fraction $\\frac{3573}{3391}$ is fully reduced.",
            "answer": "$$\\boxed{\\frac{3573}{3391}}$$"
        },
        {
            "introduction": "The power of a PHMM lies not only in its probabilistic framework but also in its tunable structure, where parameters directly encode our assumptions about the alignment process. This practice is a thought experiment on the connection between model topology and alignment output. By analyzing the effect of setting a single transition probability to zero, you will see how specific parameters can enforce hard constraints on the resulting Viterbi alignment, such as preventing consecutive matches .",
            "id": "2411626",
            "problem": "Consider a standard Pair Hidden Markov Model (HMM) used for pairwise sequence alignment with three emitting states: match $M$, insertion in $X$ (denoted $X$), and insertion in $Y$ (denoted $Y$). The match state $M$ emits a pair of symbols $(x_i,y_j)$, the state $X$ emits a single symbol $x_i$ aligned to a gap in $Y$, and the state $Y$ emits a single symbol $y_j$ aligned to a gap in $X$. There are also silent begin $B$ and end $E$ states that occur only at the start and end of an alignment. Let $a_{s,t}$ denote the transition probability from state $s$ to state $t$, and let $e_M(x_i,y_j)$, $e_X(x_i)$, $e_Y(y_j)$ denote the emission probabilities in the corresponding states. Assume that for each emitting state $s \\in \\{M,X,Y\\}$, the outgoing transition probabilities satisfy $\\sum_{t \\in \\{M,X,Y,E\\}} a_{s,t} = 1$, and that all emission probabilities for valid symbols are strictly positive. Suppose that the transition probability from the match state to itself is set to $a_{M,M} = 0$, while all other transitions among $\\{M,X,Y\\}$ and from these states to $E$ remain strictly positive and unchanged.\n\nUnder these conditions, what is the effect on the Viterbi alignment (i.e., the maximum a posteriori state path and its implied alignment) between two given sequences?\n\nA. The Viterbi path will never enter the match state $M$, so the alignment contains only gaps and cannot align any pair of symbols.\n\nB. Any occurrence of the match state $M$ in the Viterbi path must be isolated; two consecutive matches are impossible, so any two aligned symbol pairs are separated by at least one gap state.\n\nC. The Viterbi alignment is unaffected because the most probable path ignores self-transition probabilities when emissions dominate.\n\nD. The model can still produce runs of consecutive matches by routing through silent begin or end states as bridges, so contiguous matches remain possible.\n\nE. The Viterbi dynamic program becomes ill-defined due to zero transitions, so no optimal path can be computed.",
            "solution": "The problem statement must first be validated for correctness and clarity before any attempt at a solution.\n\n### Step 1: Extract Givens\nThe problem describes a Pair Hidden Markov Model (HMM) for pairwise sequence alignment with the following components and conditions:\n- **States**: Three emitting states: Match ($M$), Insertion in sequence $X$ ($X$), and Insertion in sequence $Y$ ($Y$). Two silent states: Begin ($B$) and End ($E$).\n- **Emissions**:\n    - State $M$ emits a pair of symbols $(x_i, y_j)$. Emission probability is $e_M(x_i,y_j)$.\n    - State $X$ emits a symbol $x_i$ (aligned to a gap). Emission probability is $e_X(x_i)$.\n    - State $Y$ emits a symbol $y_j$ (aligned to a gap). Emission probability is $e_Y(y_j)$.\n- **Transitions**:\n    - Transition probability from state $s$ to state $t$ is denoted $a_{s,t}$.\n    - For each emitting state $s \\in \\{M,X,Y\\}$, the outgoing transition probabilities sum to one: $\\sum_{t \\in \\{M,X,Y,E\\}} a_{s,t} = 1$.\n- **Probability Constraints**:\n    - All emission probabilities for valid symbols are strictly positive.\n    - The transition probability from the match state to itself is set to zero: $a_{M,M} = 0$.\n    - All other transitions among $\\{M,X,Y\\}$ and from these states to $E$ remain strictly positive.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is based on the standard pair HMM framework, a cornerstone of computational biology and bioinformatics for sequence alignment. The model described is a simplified but standard version (e.g., the \"three-state pair HMM\"). The premise is scientifically sound.\n- **Well-Posed**: The problem asks for the effect of a specific parameter change ($a_{M,M} = 0$) on the output of the Viterbi algorithm. This is a well-defined question with a unique logical consequence within the HMM framework.\n- **Objective**: The language is precise and devoid of subjectivity. All terms are standard in the field.\n- **Completeness and Consistency**: The model is sufficiently described. The constraint $a_{M,M}=0$ is a specific, consistent modification to an otherwise standard model. The fact that other transitions remain positive ensures that the model does not become trivial or disconnected.\n- **Realism**: Setting a transition probability to zero is a valid modeling choice, often used to enforce certain structural constraints on alignments. It is a realistic theoretical exercise.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-posed, scientifically grounded question about the properties of a standard bioinformatics algorithm. I will proceed with the derivation of the solution.\n\n###\n### Derivation of Solution\nThe Viterbi algorithm finds the most probable path of states (the Viterbi path) that could have generated a given pair of sequences, $(x_1...x_L, y_1...y_N)$. The probability of a specific path $\\pi = (\\pi_1, \\pi_2, ..., \\pi_K)$ and the corresponding observed sequences is given by:\n$$P(\\text{sequences}, \\pi) = a_{B,\\pi_1} \\prod_{k=1}^{K-1} a_{\\pi_k, \\pi_{k+1}} \\prod_{k=1}^{K} P(\\text{emission}_k | \\pi_k)$$\nwhere the final transition is to the end state $E$, i.e. $\\pi_K \\rightarrow E$. For compactness, this can be written as $a_{\\pi_K, E}$.\n\nThe Viterbi algorithm uses dynamic programming to find the path $\\pi^*$ that maximizes this probability. Let $V_s(i, j)$ be the probability of the most probable path ending in state $s \\in \\{M, X, Y\\}$, having generated the prefixes $x_1...x_i$ and $y_1...y_j$. The recurrence relations for these scores are:\n\nFor the match state $M$ at position $(i,j)$:\n$$V_M(i,j) = e_M(x_i, y_j) \\times \\max \\begin{cases} V_M(i-1, j-1) \\cdot a_{M,M} \\\\ V_X(i-1, j-1) \\cdot a_{X,M} \\\\ V_Y(i-1, j-1) \\cdot a_{Y,M} \\end{cases}$$\n\nFor the gap state $X$ at position $(i,j)$:\n$$V_X(i,j) = e_X(x_i) \\times \\max \\begin{cases} V_M(i-1, j) \\cdot a_{M,X} \\\\ V_X(i-1, j) \\cdot a_{X,X} \\\\ V_Y(i-1, j) \\cdot a_{Y,X} \\end{cases}$$\n\nFor the gap state $Y$ at position $(i,j)$:\n$$V_Y(i,j) = e_Y(y_j) \\times \\max \\begin{cases} V_M(i, j-1) \\cdot a_{M,Y} \\\\ V_X(i, j-1) \\cdot a_{X,Y} \\\\ V_Y(i, j-1) \\cdot a_{Y,Y} \\end{cases}$$\nThe calculation starts from the initial state where $V_B(0,0)=1$.\n\nThe problem states that the self-transition probability for the match state is zero: $a_{M,M} = 0$. We must analyze the consequence of this constraint on the Viterbi path.\n\nLet's examine the recurrence for $V_M(i,j)$:\n$$V_M(i,j) = e_M(x_i, y_j) \\times \\max \\begin{cases} V_M(i-1, j-1) \\cdot 0 \\\\ V_X(i-1, j-1) \\cdot a_{X,M} \\\\ V_Y(i-1, j-1) \\cdot a_{Y,M} \\end{cases}$$\nThe term $V_M(i-1, j-1) \\cdot a_{M,M}$ becomes $V_M(i-1, j-1) \\cdot 0 = 0$. This means that the path leading to state $M$ at $(i,j)$ through state $M$ at $(i-1, j-1)$ will always have a probability contribution of $0$.\n\nThe Viterbi algorithm seeks to maximize the path probability. Unless all possible paths into state $M$ have a probability of zero (which is not the case, as $a_{X,M} > 0$ and $a_{Y,M} > 0$), the algorithm will never select the transition from $M$ to $M$ as part of the optimal path. The most probable path, the Viterbi path, is a sequence of states $\\pi^* = (\\pi_1^*, \\pi_2^*, ... , \\pi_K^*)$. The condition $a_{M,M}=0$ means that it is impossible for any two consecutive states in this path to be $M$, i.e., $\\pi_k^*=M$ and $\\pi_{k+1}^*=M$ cannot occur simultaneously.\n\nIn terms of the alignment, a state $M$ corresponds to an aligned pair of symbols. A sequence of two consecutive matches would correspond to a sub-path $M \\to M$. Since this transition has a probability of $0$, any path containing it has an overall probability of $0$. The Viterbi algorithm will discard such paths in favor of any existing path with a non-zero probability.\n\nTherefore, any occurrence of the match state $M$ in the Viterbi path must be preceded and followed by a different state. The other emitting states are $X$ and $Y$, which correspond to gaps in the alignment. This means any two aligned symbol pairs must be separated by at least one gap insertion or deletion.\n\n### Evaluation of Options\n\n**A. The Viterbi path will never enter the match state $M$, so the alignment contains only gaps and cannot align any pair of symbols.**\nThis is **Incorrect**. The model can transition into state $M$ from states $X$ or $Y$, as the problem states that $a_{X,M}$ and $a_{Y,M}$ are strictly positive. For example, a path segment $... \\to X \\to M \\to Y \\to ...$ is possible and may be optimal. This would produce an alignment with an isolated matched pair. It is therefore false that the path will never enter state $M$.\n\n**B. Any occurrence of the match state $M$ in the Viterbi path must be isolated; two consecutive matches are impossible, so any two aligned symbol pairs are separated by at least one gap state.**\nThis is **Correct**. As derived above, the condition $a_{M,M} = 0$ makes any path containing a transition from state $M$ to state $M$ have a total probability of $0$. The Viterbi algorithm will not select such a path if any path with positive probability exists. This forbids consecutive occurrences of state $M$. In an alignment, states $M$ represent aligned symbol pairs, and states $X$ and $Y$ represent gaps. Consequently, any two aligned symbol pairs must be separated by at least one gap.\n\n**C. The Viterbi alignment is unaffected because the most probable path ignores self-transition probabilities when emissions dominate.**\nThis is **Incorrect**. This statement demonstrates a fundamental misunderstanding of the Viterbi algorithm. The path probability is a product of both emission and transition probabilities. A zero transition probability forces the product to zero, regardless of how large the emission probabilities are. The Viterbi algorithm does not \"ignore\" any probabilities; it rigorously computes the full path probability. The alignment is certainly affected by setting $a_{M,M}=0$.\n\n**D. The model can still produce runs of consecutive matches by routing through silent begin or end states as bridges, so contiguous matches remain possible.**\nThis is **Incorrect**. The silent states $B$ and $E$ have specific roles. The Begin state $B$ is the unique starting point of any path, before any symbols are processed (at indices $(0,0)$). The End state $E$ is the unique termination point, entered only after all desired symbols from one or both sequences have been processed. They cannot be used as intermediate \"bridge\" states in the middle of an alignment. The structure of the dynamic programming table enforces a monotonic progression through the sequence indices, making such routing impossible.\n\n**E. The Viterbi dynamic program becomes ill-defined due to zero transitions, so no optimal path can be computed.**\nThis is **Incorrect**. A zero probability is a valid value for a parameter in a probabilistic model. The Viterbi algorithm and its dynamic programming recurrence relations are perfectly well-defined and can be executed without issue. The term involving $a_{M,M}$ simply evaluates to $0$, and the `max` operation proceeds as usual. An optimal path can and will be computed; it will simply be constrained to paths that do not contain the $M \\to M$ transition.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "A key advantage of PHMMs is their ability to be trained from data, allowing their parameters to adapt to the statistical properties of a given set of sequences. This exercise provides a practical window into this process by having you implement the Maximization (M) step of the Baum-Welch training algorithm. You will use expected emission counts—the output of the Expectation (E) step—to re-estimate the model's emission probabilities, demonstrating a core mechanism of machine learning in sequence analysis .",
            "id": "2411616",
            "problem": "You are given a Pair Hidden Markov Model (pair-HMM) for pairwise sequence alignment, consisting of three states: a match state $M$, an insertion-in-$x$ state $I_x$, and an insertion-in-$y$ state $I_y$. The emission process is as follows: state $M$ emits a pair of symbols, state $I_x$ emits a symbol from sequence $x$ paired with a gap in sequence $y$, and state $I_y$ emits a gap in sequence $x$ paired with a symbol from sequence $y$. Consider the deoxyribonucleic acid (DNA) alphabet $\\Sigma=\\{A,C,G,T\\}$.\n\nAfter an Expectation-Maximization (EM) iteration using the Baum-Welch method for Hidden Markov Models (HMMs), you are provided with the expected emission counts from state $I_x$ for each symbol in $\\Sigma$. Denote these nonnegative expected counts by $n_A$, $n_C$, $n_G$, and $n_T$. You are asked to compute the simplified M-step update for the emission probability $e_{I_x}(A)$ under the following specification:\n\n- The updated emission distribution for $I_x$ over $\\Sigma$ must maximize the expected complete-data log-likelihood subject to the constraints that $\\sum_{b\\in\\Sigma} e_{I_x}(b)=1$ and $e_{I_x}(b)\\ge 0$ for all $b\\in\\Sigma$.\n- In the special case that $n_A+n_C+n_G+n_T=0$, set the updated emission distribution to be uniform over $\\Sigma$.\n\nYour task is to write a program that, for each test case below, computes the single float value $e_{I_x}(A)$ according to the specification above. Each output must be rounded to exactly $6$ digits after the decimal point.\n\nTest suite (each test case is a quadruple $(n_A,n_C,n_G,n_T)$):\n\n- Case $1$: $(3,1,1,1)$.\n- Case $2$: $(0,0,0,0)$.\n- Case $3$: $(10,0,0,0)$.\n- Case $4$: $(0,5,5,0)$.\n- Case $5$: $\\left(1\\times 10^{-12},2\\times 10^{-12},3\\times 10^{-12},4\\times 10^{-12}\\right)$.\n\nYour program must produce a single line of output containing the results for the above cases, in order, as a comma-separated list enclosed in square brackets. For example, the output format must be like [r1,r2,r3,r4,r5], where each $r_i$ is a float rounded to exactly $6$ digits after the decimal point.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Model**: A Pair Hidden Markov Model (pair-HMM) with three states: a match state $M$, an insertion-in-$x$ state $I_x$, and an insertion-in-$y$ state $I_y$.\n- **Alphabet**: Deoxyribonucleic acid (DNA) alphabet $\\Sigma=\\{A,C,G,T\\}$.\n- **Data**: Nonnegative expected emission counts from state $I_x$ for each symbol in $\\Sigma$, denoted as $n_A, n_C, n_G, n_T$.\n- **Objective**: Compute the M-step update for the emission probability $e_{I_x}(A)$.\n- **Optimization Criterion**: Maximize the expected complete-data log-likelihood.\n- **Constraints**: $\\sum_{b\\in\\Sigma} e_{I_x}(b)=1$ and $e_{I_x}(b)\\ge 0$ for all $b\\in\\Sigma$.\n- **Special Condition**: If $n_A+n_C+n_G+n_T=0$, the updated emission distribution is uniform over $\\Sigma$.\n- **Test Cases**:\n    - Case 1: $(3,1,1,1)$\n    - Case 2: $(0,0,0,0)$\n    - Case 3: $(10,0,0,0)$\n    - Case 4: $(0,5,5,0)$\n    - Case 5: $(1\\times 10^{-12},2\\times 10^{-12},3\\times 10^{-12},4\\times 10^{-12})$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem describes the M-step of the Baum-Welch algorithm, a standard parameter estimation method for Hidden Markov Models. This is a fundamental and well-established procedure in computational biology and machine learning. The problem is scientifically sound.\n- **Well-Posedness**: The problem specifies a clear objective (maximization of a defined function), a complete set of constraints, and explicit handling of the edge case where total counts are zero. This structure ensures a unique, stable, and meaningful solution exists for any valid input.\n- **Objectivity**: The problem is stated using precise, unambiguous mathematical and scientific terminology. It is free of subjective or speculative content.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard, well-posed problem in bioinformatics. A solution will be provided.\n\nThe M-step of the Baum-Welch algorithm updates the model parameters to maximize the expected complete-data log-likelihood, which is formulated during the E-step. Let the current parameter set be $\\theta^{(t)}$. The objective is to find the new parameter set $\\theta$ that maximizes $Q(\\theta|\\theta^{(t)})$, the expected value of the log-likelihood of the complete data.\n\nWe are concerned only with the emission probabilities for state $I_x$, denoted by the set $\\{e_{I_x}(b) | b \\in \\Sigma\\}$. The portion of the expected complete-data log-likelihood function that depends on these parameters is given by:\n$$ L(e_{I_x}) = \\sum_{b \\in \\Sigma} E[\\text{count of emissions of } b \\text{ from state } I_x] \\cdot \\log(e_{I_x}(b)) $$\nThe problem provides the expected counts of emissions from state $I_x$ as $n_A, n_C, n_G, n_T$. Therefore, our objective is to maximize the function:\n$$ L(e_{I_x}) = n_A \\log(e_{I_x}(A)) + n_C \\log(e_{I_x}(C)) + n_G \\log(e_{I_x}(G)) + n_T \\log(e_{I_x}(T)) $$\nThis maximization is subject to the stochastic constraints:\n$$ \\sum_{b \\in \\Sigma} e_{I_x}(b) = 1 \\quad \\text{and} \\quad e_{I_x}(b) \\ge 0 \\;\\; \\forall b \\in \\Sigma $$\nThis is a standard constrained optimization problem, which can be solved using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(e_{I_x}, \\lambda) = \\left( \\sum_{b \\in \\Sigma} n_b \\log(e_{I_x}(b)) \\right) - \\lambda \\left( \\left( \\sum_{b \\in \\Sigma} e_{I_x}(b) \\right) - 1 \\right) $$\nTo find the extremum, we compute the partial derivative of $\\mathcal{L}$ with respect to each $e_{I_x}(b')$ for $b' \\in \\Sigma$ and set it to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial e_{I_x}(b')} = \\frac{n_{b'}}{e_{I_x}(b')} - \\lambda = 0 $$\nThis implies $n_{b'} = \\lambda e_{I_x}(b')$. Summing over all symbols $b' \\in \\Sigma$:\n$$ \\sum_{b' \\in \\Sigma} n_{b'} = \\sum_{b' \\in \\Sigma} \\lambda e_{I_x}(b') = \\lambda \\sum_{b' \\in \\Sigma} e_{I_x}(b') $$\nApplying the constraint $\\sum_{b' \\in \\Sigma} e_{I_x}(b') = 1$, we find the value of the Lagrange multiplier $\\lambda$:\n$$ \\lambda = \\sum_{b' \\in \\Sigma} n_{b'} = n_A+n_C+n_G+n_T $$\nLet $N = n_A+n_C+n_G+n_T$ be the total expected count. The updated emission probability for any symbol $b$ is then:\n$$ e_{I_x}(b) = \\frac{n_b}{\\lambda} = \\frac{n_b}{N} $$\nThis result is valid provided that $N > 0$. The specific quantity to be computed is $e_{I_x}(A)$:\n$$ e_{I_x}(A) = \\frac{n_A}{n_A + n_C + n_G + n_T} $$\nThe problem defines a special case for when $N = 0$. In this scenario, the updated emission distribution is uniform over the alphabet $\\Sigma = \\{A, C, G, T\\}$, which has size $|\\Sigma| = 4$. Thus, for any symbol $b \\in \\Sigma$:\n$$ e_{I_x}(b) = \\frac{1}{|\\Sigma|} = \\frac{1}{4} = 0.25 $$\nThis also applies to $e_{I_x}(A)$.\n\nWe now apply this logic to the provided test cases.\n\nCase 1: $(n_A, n_C, n_G, n_T) = (3, 1, 1, 1)$\nTotal count $N = 3+1+1+1 = 6$.\nSince $N > 0$, $e_{I_x}(A) = \\frac{n_A}{N} = \\frac{3}{6} = 0.5$.\n\nCase 2: $(n_A, n_C, n_G, n_T) = (0, 0, 0, 0)$\nTotal count $N = 0+0+0+0 = 0$.\nThis is the special case. The distribution is uniform.\n$e_{I_x}(A) = 0.25$.\n\nCase 3: $(n_A, n_C, n_G, n_T) = (10, 0, 0, 0)$\nTotal count $N = 10+0+0+0 = 10$.\nSince $N > 0$, $e_{I_x}(A) = \\frac{n_A}{N} = \\frac{10}{10} = 1.0$.\n\nCase 4: $(n_A, n_C, n_G, n_T) = (0, 5, 5, 0)$\nTotal count $N = 0+5+5+0 = 10$.\nSince $N > 0$, $e_{I_x}(A) = \\frac{n_A}{N} = \\frac{0}{10} = 0.0$.\n\nCase 5: $(n_A, n_C, n_G, n_T) = (1\\times 10^{-12}, 2\\times 10^{-12}, 3\\times 10^{-12}, 4\\times 10^{-12})$\nTotal count $N = (1+2+3+4) \\times 10^{-12} = 10 \\times 10^{-12} = 1 \\times 10^{-11}$.\nSince $N > 0$, $e_{I_x}(A) = \\frac{n_A}{N} = \\frac{1\\times 10^{-12}}{10\\times 10^{-12}} = \\frac{1}{10} = 0.1$.\n\nThe results must be formatted to $6$ decimal places.\n- Case 1: $0.500000$\n- Case 2: $0.250000$\n- Case 3: $1.000000$\n- Case 4: $0.000000$\n- Case 5: $0.100000$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the M-step update for the emission probability e_{I_x}(A)\n    for a series of test cases based on expected emission counts.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of expected counts (n_A, n_C, n_G, n_T).\n    test_cases = [\n        (3.0, 1.0, 1.0, 1.0),\n        (0.0, 0.0, 0.0, 0.0),\n        (10.0, 0.0, 0.0, 0.0),\n        (0.0, 5.0, 5.0, 0.0),\n        (1e-12, 2e-12, 3e-12, 4e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        # The expected count n_A is the first element of the tuple.\n        n_A = case[0]\n        \n        # Calculate the total expected count. Using np.sum is robust for\n        # floating-point arithmetic and sums the elements of the tuple.\n        total_count = np.sum(case)\n        \n        # The M-step update rule for emission probabilities is derived from\n        # maximizing the expected complete-data log-likelihood.\n        # The updated probability e_{I_x}(b) is the normalized expected count: n_b / sum(n_i).\n        \n        # A special case is defined for when the total expected count is zero.\n        # In this situation, the updated distribution is uniform. For an alphabet\n        # of size 4 (A, C, G, T), each probability becomes 1/4 = 0.25.\n        if total_count == 0.0:\n            e_Ix_A = 0.25\n        else:\n            # For non-zero total counts, apply the standard formula.\n            e_Ix_A = n_A / total_count\n            \n        # The problem requires the output to be rounded to exactly 6 digits\n        # after the decimal point. We use an f-string for this formatting.\n        results.append(f\"{e_Ix_A:.6f}\")\n\n    # Final print statement in the exact required format: [r1,r2,r3,r4,r5]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}