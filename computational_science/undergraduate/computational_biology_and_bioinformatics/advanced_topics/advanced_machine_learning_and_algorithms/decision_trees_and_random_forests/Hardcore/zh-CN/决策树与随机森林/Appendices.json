{
    "hands_on_practices": [
        {
            "introduction": "理解一个算法的最好方法就是亲手实现它。这个练习将指导你从零开始构建一个随机森林分类器，用于解决一个生物信息学中的分类任务 。通过这个过程，你将巩固对基尼不纯度 (Gini impurity)、递归分区以及集成投票等核心概念的理解。",
            "id": "2384429",
            "problem": "构建一个完整的、可运行的程序，该程序基于决策树集成实现一个用于化学信息学分类任务的二元分类器。该分类器必须遵循随机森林 (RF) 的数学定义，此处将其解释为对多个轴对齐、二元、有限深度的决策树 (Decision Trees, DT) 进行多数投票。这些决策树使用固定的训练集进行训练，训练集中的化合物由结构描述符表示。任务是预测一个化合物是否具有荧光性，类别标签 $1$ 表示有荧光， $0$ 表示无荧光。\n\n定义与数据：\n- 每个化合物由一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 表示，其中包含 $d=2$ 个特征：\n  - 特征 0：交替键中的最长共轭体系长度（整数）。\n  - 特征 1：电子给体原子数（整数）。\n- 带标签的训练集为 $\\mathcal{D}=\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $n=8$，标签 $y_i \\in \\{0,1\\}$。训练样本如下：\n  - $\\mathbf{x}_1=(3,0)$，其中 $y_1=0$。\n  - $\\mathbf{x}_2=(4,1)$，其中 $y_2=0$。\n  - $\\mathbf{x}_3=(5,0)$，其中 $y_3=0$。\n  - $\\mathbf{x}_4=(6,1)$，其中 $y_4=1$。\n  - $\\mathbf{x}_5=(7,1)$，其中 $y_5=1$。\n  - $\\mathbf{x}_6=(8,2)$，其中 $y_6=1$。\n  - $\\mathbf{x}_7=(9,1)$，其中 $y_7=1$。\n  - $\\mathbf{x}_8=(10,0)$，其中 $y_8=1$。\n- 必须为其返回预测的查询集是以下三个化合物的列表：\n  - $\\mathbf{q}_1=(5,1)$。\n  - $\\mathbf{q}_2=(6,0)$。\n  - $\\mathbf{q}_3=(8,1)$。\n\n决策树分类器 $h$ 的数学规范：\n- 一个节点 $v$ 关联一个索引集 $S_v \\subseteq \\{1,\\dots,n\\}$，该集合包含到达节点 $v$ 的训练样本，以及一个深度 $\\mathrm{depth}(v)\\in \\mathbb{N}_0$。根节点的深度为 $\\mathrm{depth}=0$，其索引集为 $S_{\\text{root}}=\\{1,\\dots,n\\}$。\n- 对于任意节点 $v$，定义类别计数 $c_k(v)=|\\{i\\in S_v: y_i=k\\}|$（其中 $k\\in\\{0,1\\}$）、节点大小 $|S_v|$ 以及经验类别概率 $p_k(v)=c_k(v)/|S_v|$。节点 $v$ 处的基尼不纯度为\n$$G(v)=1-\\sum_{k\\in\\{0,1\\}} p_k(v)^2.$$\n- 节点 $v$ 处的候选轴对齐分裂定义如下。对于每个特征索引 $j\\in\\{0,1\\}$ 和多重集 $\\{x_{i,j} : i\\in S_v\\}$，取其排序后的唯一值列表 $z_1 < z_2 < \\dots < z_m$。候选分裂阈值 $\\tau$ 从中点集合 $\\{ (z_k + z_{k+1})/2 \\}_{k=1}^{m-1}$ 中选取。在所有特征和所有相应候选阈值构成的有效分裂中，选择使加权子节点基尼不纯度最小的分裂。一个分裂是有效的，如果两个子节点的大小都至少为 $n_{\\min}$。平局时，选择阈值 $\\tau$ 最小的分裂，如果仍然平局，则选择特征索引 $j$ 最小的分裂。",
            "solution": "问题陈述已经过严格验证，并被确定为 **有效**。这是一个计算科学领域中自成一体、有科学依据且定义明确的问题，为构建随机森林分类器提供了完整的规范。所有必要的数据、数学定义、超参数和打破平局的规则都已提供，确保了唯一且确定性的解决方案。\n\n任务是构建一个随机森林 ($H$) 分类器，它是由 $T$ 棵决策树 ($h_t$) 组成的集成。森林的预测是单个树预测的多数投票结果。问题陈述提供了三个具有特定超参数的测试用例。对于这三个测试用例，一个关键的观察是 `bootstrap` 参数被设置为 `False`，并且特征子采样参数 $m_{\\text{try}}$ 被设置为 $2$，这等于总特征数 ($d=2$)。这带来了一个重要后果：\n- **无自助采样 (No Bootstrapping)**：森林中的每棵树 $h_t$ 都是在完全相同的完整训练数据集 $\\mathcal{D}$ 上训练的。\n- **无特征子采样 (No Feature Subsampling)**：在每棵树的每个节点上，都会考虑所有 $d=2$ 个特征进行分裂。\n由于树的构建算法（包括所有打破平局的规则）是确定性的，因此在单个随机森林实例中的所有 $T$ 棵树都将是完全相同的。因此，对于任何给定的测试用例，随机森林的预测 $H(\\mathbf{x})$ 简化为使用相应超参数构建的单棵决策树 $h(\\mathbf{x})$ 的预测。参数 $T$（树的数量）对于最终预测变得无关紧要。\n\n我们的流程将是：针对三个测试用例中的每一个，使用指定的参数 ($D_{\\max}, n_{\\min}$) 构建一棵决策树，然后用它来对三个查询向量进行分类。\n\n决策树是递归构建的，从包含所有训练数据索引 $S_{\\text{root}} = \\{1, 2, \\dots, 8\\}$ 的根节点开始。对于任何具有关联训练索引 $S_v$ 的节点 $v$，我们执行以下步骤：\n\n1.  **检查停止标准**：如果满足以下条件之一，节点 $v$ 就成为叶节点：\n    a. 节点是纯的，即其基尼不纯度 $G(v) = 1 - \\sum_{k \\in \\{0,1\\}} p_k(v)^2$ 为 $0$。\n    b. 节点的深度达到最大允许深度，即 $\\mathrm{depth}(v) \\ge D_{\\max}$。\n    c. 从该节点无法进行任何有效分裂。一个分裂是有效的，如果它将大小为 $|S_v|$ 的节点数据划分为两个大小为 $|S_v^{\\text{L}}|$ 和 $|S_v^{\\text{R}}|$ 的子节点，且满足 $|S_v^{\\text{L}}| \\ge n_{\\min}$ 和 $|S_v^{\\text{R}}| \\ge n_{\\min}$。\n\n    如果一个节点是叶节点，其预测 $\\hat{y}(v)$ 是 $S_v$ 中样本的多数类别。平局时选择类别 0。\n\n2.  **寻找最佳分裂**：如果没有达到停止标准，我们搜索最优分裂 $(j^*, \\tau^*)$，以最小化加权基尼不纯度：\n    $$ \\Phi(v; j, \\tau) = \\frac{|S_v^{\\text{L}}(j,\\tau)|}{|S_v|}G(v^{\\text{L}}(j,\\tau)) + \\frac{|S_v^{\\text{R}}(j,\\tau)|}{|S_v|}G(v^{\\text{R}}(j,\\tau)) $$\n    搜索是在所有特征 $j \\in \\{0, 1\\}$ 和所有有效的候选阈值 $\\tau$ 上进行的。阈值是排序后的唯一特征值之间的中点。打破平局的规则是选择具有最小阈值 $\\tau$ 的分裂，然后是最小的特征索引 $j$。\n\n3.  **递归**：节点 $v$ 成为一个存储 $(j^*, \\tau^*)$ 的内部节点。通过将 $S_v$ 划分为 $S_v^{\\text{L}}(j^*,\\tau^*)$ 和 $S_v^{\\text{R}}(j^*,\\tau^*)$ 来创建两个子节点，并对它们在深度 $\\mathrm{depth}(v)+1$ 处递归应用树的构建过程。\n\n训练数据包含 $n=8$ 个样本，其标签 $y \\in \\{0,1\\}$：\n$\\mathcal{D} = \\{(\\mathbf{x}_1=(3,0), y_1=0), (\\mathbf{x}_2=(4,1), y_2=0), (\\mathbf{x}_3=(5,0), y_3=0), (\\mathbf{x}_4=(6,1), y_4=1), (\\mathbf{x}_5=(7,1), y_5=1), (\\mathbf{x}_6=(8,2), y_6=1), (\\mathbf{x}_7=(9,1), y_7=1), (\\mathbf{x}_8=(10,0), y_8=1)\\}$。\n根节点包含索引 $\\{1, \\dots, 8\\}$，对应的标签为 $\\{0,0,0,1,1,1,1,1\\}$。类别计数为 $c_0=3, c_1=5$。初始基尼不纯度为 $G(\\text{root}) = 1 - ((3/8)^2 + (5/8)^2) = 30/64 \\approx 0.46875$。\n\n让我们分析每种情况：\n\n**情况 A: $(T=7, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=1)$ 和情况 B: $(T=1, D_{\\max}=1, m_{\\text{try}}=2, n_{\\min}=1)$**\n\n对于这两种情况，$n_{\\min}=1$，所以任何能创建非空子节点的分裂都是有效的。我们为根节点寻找最佳分裂。\n- **特征 $j=0$**：值为 $\\{3,4,5,6,7,8,9,10\\}$。一个候选分裂在 $\\tau=5.5$ 处。\n  - 左子节点 ($x_0 \\le 5.5$)：索引为 $\\{1,2,3\\}$，标签为 $\\{0,0,0\\}$。这个集合是纯的，所以 $G(v^{\\text{L}})=0$。大小为 3。\n  - 右子节点 ($x_0 > 5.5$)：索引为 $\\{4,5,6,7,8\\}$，标签为 $\\{1,1,1,1,1\\}$。这个集合也是纯的，$G(v^{\\text{R}})=0$。大小为 5。\n  - 加权不纯度为 $\\Phi(v; j=0, \\tau=5.5) = (3/8) \\cdot 0 + (5/8) \\cdot 0 = 0$。\n这是一个完美的分裂，达到了可能的最小不纯度 0。没有其他分裂能做得更好。因此，根节点在特征 $j=0$ 和阈值 $\\tau=5.5$ 上进行分裂。\n\n- **对于情况 B ($D_{\\max}=1$)**：子节点位于深度 1。由于 $\\mathrm{depth} \\ge D_{\\max}$，两个子节点都成为叶节点。\n  - 左叶节点：包含标签为 $\\{0,0,0\\}$ 的样本。多数类别为 0。预测 $\\hat{y}=0$。\n  - 右叶节点：包含标签为 $\\{1,1,1,1,1\\}$ 的样本。多数类别为 1。预测 $\\hat{y}=1$。\n\n- **对于情况 A ($D_{\\max}=3$)**：子节点位于深度 1。我们检查停止标准。两个子节点都是纯节点 ($G=0$)，所以它们立即成为叶节点，无需考虑 $D_{\\max}$。\n  - 生成的树结构与情况 B 的树结构相同。\n\n因此，对于情况 A 和情况 B，决策树为：如果 $x_0 \\le 5.5$，则预测 0；否则，预测 1。\n对于查询集 $\\{\\mathbf{q}_1=(5,1), \\mathbf{q}_2=(6,0), \\mathbf{q}_3=(8,1)\\}$ 的预测：\n- $\\mathbf{q}_1=(5,1)$: $x_0=5 \\le 5.5 \\implies$ 预测 0。\n- $\\mathbf{q}_2=(6,0)$: $x_0=6 > 5.5 \\implies$ 预测 1。\n- $\\mathbf{q}_3=(8,1)$: $x_0=8 > 5.5 \\implies$ 预测 1。\n两种情况的预测向量都是 $[0,1,1]$。\n\n**情况 C: $(T=5, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=5)$**\n\n这里，最小叶节点大小参数为 $n_{\\min}=5$。在包含 $|S_{\\text{root}}|=8$ 个样本的根节点处，我们搜索有效分裂。一个分裂是有效的，当且仅当两个子节点都至少有 $n_{\\min}=5$ 个样本。如果一个分裂产生的子节点大小为 $|S^{\\text{L}}|$ 和 $|S^{\\text{R}}|$，我们必须满足 $|S^{\\text{L}}| \\ge 5$ 且 $|S^{\\text{R}}| \\ge 5$。然而，$|S^{\\text{L}}| + |S^{\\text{R}}| = 8$，所以这是不可能的（例如，$5+5=10>8$）。\n因此，从根节点没有有效的的分裂。根据停止规则，根节点成为一个叶节点。\n- 叶节点预测：根节点的数据计数为 $c_0=3$ 和 $c_1=5$。多数类别是 1。\n情况 C 的树只是一个总是预测 1 的单叶节点。\n对查询集的预测：\n- $\\mathbf{q}_1=(5,1) \\implies$ 预测 1。\n- $\\mathbf{q}_2=(6,0) \\implies$ 预测 1。\n- $\\mathbf{q}_3=(8,1) \\implies$ 预测 1。\n情况 C 的预测向量是 $[1,1,1]$。\n\n**预测总结：**\n- 情况 A: $[0,1,1]$\n- 情况 B: $[0,1,1]$\n- 情况 C: $[1,1,1]$\n最终的输出字符串是根据这些结果构建的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, depth):\n        self.depth = depth\n        self.feature_index = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n        self.is_leaf = False\n        self.prediction = None\n\nclass DecisionTree:\n    \"\"\"Implements a single decision tree classifier.\"\"\"\n    def __init__(self, max_depth, min_samples_leaf):\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n    \n    def _gini(self, y):\n        \"\"\"Calculates the Gini impurity for a set of labels.\"\"\"\n        n_samples = len(y)\n        if n_samples == 0:\n            return 0.0\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / n_samples\n        return 1.0 - np.sum(probabilities**2)\n\n    def _find_best_split(self, X, y, indices):\n        \"\"\"Finds the best split for a node.\"\"\"\n        n_samples = len(indices)\n        if n_samples < 2:\n            return None, None\n            \n        # Gini of the current node before splitting\n        parent_gini = self._gini(y[indices])\n        \n        best_gini = parent_gini\n        best_split = None\n        \n        n_features = X.shape[1]\n        \n        # As per problem, m_try=2, so we iterate through all features\n        for feature_index in range(n_features):\n            feature_values = X[indices, feature_index]\n            unique_values = np.unique(feature_values)\n            \n            if len(unique_values) < 2:\n                continue\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            \n            for threshold in thresholds:\n                left_indices = indices[X[indices, feature_index] <= threshold]\n                right_indices = indices[X[indices, feature_index] > threshold]\n                \n                # Check min_samples_leaf constraint\n                if len(left_indices) < self.min_samples_leaf or len(right_indices) < self.min_samples_leaf:\n                    continue\n\n                gini_left = self._gini(y[left_indices])\n                gini_right = self._gini(y[right_indices])\n                \n                weighted_gini = (len(left_indices) / n_samples) * gini_left + \\\n                                (len(right_indices) / n_samples) * gini_right\n                \n                # Tie-breaking rules from problem: smallest tau, then smallest feature index.\n                # The loop order naturally handles this.\n                if weighted_gini < best_gini:\n                    best_gini = weighted_gini\n                    best_split = (feature_index, threshold)\n\n        if best_split is None:\n            return None, None # No valid split found\n        \n        return best_split\n\n    def _grow_tree(self, X, y, indices, depth):\n        \"\"\"Recursively grows the decision tree.\"\"\"\n        node = Node(depth)\n        \n        # Determine leaf prediction\n        labels_at_node = y[indices]\n        count_1 = np.sum(labels_at_node == 1)\n        count_0 = len(labels_at_node) - count_1\n        # Tie-breaking: favor class 0\n        node.prediction = 1 if count_1 > count_0 else 0\n\n        # Check stopping criteria\n        is_pure = self._gini(labels_at_node) == 0.0\n        if is_pure or depth >= self.max_depth or len(indices) < 2:\n            node.is_leaf = True\n            return node\n\n        feature_index, threshold = self._find_best_split(X, y, indices)\n        \n        if feature_index is None: # No valid split found\n            node.is_leaf = True\n            return node\n\n        node.feature_index = feature_index\n        node.threshold = threshold\n        \n        left_indices = indices[X[indices, feature_index] <= threshold]\n        right_indices = indices[X[indices, feature_index] > threshold]\n        \n        node.left = self._grow_tree(X, y, left_indices, depth + 1)\n        node.right = self._grow_tree(X, y, right_indices, depth + 1)\n        \n        return node\n\n    def fit(self, X, y):\n        \"\"\"Builds the decision tree.\"\"\"\n        initial_indices = np.arange(len(y))\n        self.root = self._grow_tree(X, y, initial_indices, 0)\n        \n    def _predict_one(self, x, node):\n        \"\"\"Predicts class for a single sample.\"\"\"\n        current_node = node\n        while not current_node.is_leaf:\n            if x[current_node.feature_index] <= current_node.threshold:\n                current_node = current_node.left\n            else:\n                current_node = current_node.right\n        return current_node.prediction\n\n    def predict(self, X):\n        \"\"\"Predicts classes for a set of samples.\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])\n\nclass RandomForest:\n    \"\"\"Implements a Random Forest classifier.\"\"\"\n    def __init__(self, n_trees, max_depth, min_samples_leaf, bootstrap, seed):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.bootstrap = bootstrap\n        self.seed = seed\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"Trains the Random Forest.\"\"\"\n        self.trees = []\n        n_samples = len(y)\n        \n        # The problem states m_try=2, so no feature subsampling is needed.\n        # It also sets bootstrap=False, so no data bootstrapping is needed.\n        # This means all trees will be identical for a given hyperparameter set.\n        \n        for i in range(self.n_trees):\n            tree = DecisionTree(max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n            \n            if self.bootstrap:\n                # Per problem, we'd use seed + i for reproducibility\n                rng = np.random.default_rng(self.seed + i)\n                indices = rng.choice(n_samples, n_samples, replace=True)\n                tree.fit(X[indices], y[indices])\n            else:\n                tree.fit(X, y)\n                \n            self.trees.append(tree)\n            \n    def predict(self, X):\n        \"\"\"Makes predictions with the Random Forest.\"\"\"\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        # tree_preds shape is (n_trees, n_samples)\n        \n        # Majority vote; ties go to class 0\n        n_ones = np.sum(tree_preds == 1, axis=0)\n        n_zeros = self.n_trees - n_ones\n        \n        return np.where(n_ones > n_zeros, 1, 0)\n\ndef solve():\n    # Define training data\n    X_train = np.array([\n        [3, 0], [4, 1], [5, 0], [6, 1], [7, 1], [8, 2], [9, 1], [10, 0]\n    ])\n    y_train = np.array([0, 0, 0, 1, 1, 1, 1, 1])\n\n    # Define query data\n    X_query = np.array([\n        [5, 1], [6, 0], [8, 1]\n    ])\n\n    # Define test cases: (T, D_max, m_try, n_min, s, bootstrap)\n    test_cases = [\n        (7, 3, 2, 1, 13, False),  # Case A\n        (1, 1, 2, 1, 7, False),   # Case B\n        (5, 3, 2, 5, 5, False)   # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T, D_max, m_try, n_min, s, bootstrap = case\n        \n        # As per problem, m_try is always 2. It is not passed to the RF, as the DT handles all features.\n        rf = RandomForest(n_trees=T, max_depth=D_max, min_samples_leaf=n_min, bootstrap=bootstrap, seed=s)\n        rf.fit(X_train, y_train)\n        predictions = rf.predict(X_query)\n        all_results.append(list(predictions))\n\n    # Format the final output string exactly as required\n    inner_parts = []\n    for res_list in all_results:\n        inner_parts.append(f\"[{','.join(map(str, res_list))}]\")\n    final_output = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "在构建了决策树之后，理解其内在特性至关重要。本练习旨在探讨决策树的一个关键弱点——结构不稳定性，你将看到一个微小的数据扰动如何彻底改变整个树的结构 。这个实践揭示了为何需要使用像随机森林这样的集成方法，因为它们正是为了缓解此类问题而设计的。",
            "id": "2386935",
            "problem": "考虑一个二元破产预测问题，使用一个确定性的、贪婪的、轴对齐的决策树，该决策树使用基尼不纯度作为其分裂标准。您将展示在一家公司的报告收益受到微小扰动下，深层决策树的结构不稳定性。模型和数据定义如下。\n\n模型和算法规格：\n- 标签是二元的，类别 $y \\in \\{0,1\\}$，其中 $y=1$ 表示破产。\n- 对于任何具有 $n$ 个元素的样本集 $S$ 和经验正例分数 $p=\\frac{1}{n}\\sum_{i\\in S}\\mathbb{1}\\{y_i=1\\}$，基尼不纯度为 $G(S)=2p(1-p)$。\n- 对于将父集 $S$ 分裂为不相交的非空子集 $S_L$ 和 $S_R$ 的候选分裂，不纯度减少（增益）为 $\\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right)$。\n- 树以贪婪方式生长：在每个内部节点，从所有容许的分裂中，选择具有最大 $\\Delta G$ 的分裂。如果 $\\Delta G$ 出现平局，则选择具有较小特征索引的分裂，如果仍然平局，则选择较小的阈值。如果没有容许的分裂产生严格正增益，则成为叶节点。\n- 分裂是二元的，形式为 $x_j \\le t$ 的样本进入左子节点，$x_j > t$ 的样本进入右子节点。\n- 受领域知识约束的阈值：仅可使用特征 $j=0$（收益）和特征 $j=1$（杠杆率）进行分裂，且每个特征只能在阈值 $t=0$ 处分裂。特征 $j=2$ 和 $j=3$ 不用于分裂。一个分裂是容许的，当且仅当两个子节点都非空。叶节点预测其节点中的多数类；平局则预测为类别 $0$。\n- 最大深度固定为 $D=4$（根节点深度为 $0$）。当一个节点是纯的，或达到深度 $D$，或没有容许的分裂产生正增益时，停止生长。\n\n数据：\n- 有 $n=16$ 家公司和 $d=4$ 个特征，顺序为：收益（特征 $0$）、杠杆率（特征 $1$）、流动性（特征 $2$）和利息覆盖率（特征 $3$）。\n- 标签 $y$ 为\n$$\ny = [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0].\n$$\n- 基线（未扰动）特征 $X\\in\\mathbb{R}^{16\\times 4}$ 由行 $i=0,\\dots,15$ 给出：\n    - 收益 $x_{i,0}$：\n        - 对于索引 $i=0,\\dots,7$（前八家 $y=1$ 的公司）：$[0,0,0,0,0,0,1,1]$。\n        - 对于索引 $i=8,\\dots,15$（后八家 $y=0$ 的公司）：$[0,0,1,1,1,1,1,1]$。\n      因此，恰好有 $6$ 家破产公司和 $2$ 家非破产公司的 $x_{i,0}\\le 0$。\n    - 杠杆率 $x_{i,1}$：\n        - 对于索引 $i=0,\\dots,7$：$[-1,-1,1,1,1,1,1,1]$。\n        - 对于索引 $i=8,\\dots,15$：$[-1,-1,-1,-1,-1,-1,1,1]$。\n      因此，恰好有 $2$ 家破产公司和 $6$ 家非破产公司的 $x_{i,1}\\le 0$。\n    - 流动性 $x_{i,2}$ 对所有 $i$ 恒为 $0$。\n    - 利息覆盖率 $x_{i,3}$ 对所有 $i$ 恒为 $0$。\n\n扰动设置：\n- 选择特定的公司索引 $i^\\star=5$（该公司当前收益为 $x_{5,0}=0$，标签为 $y_{5}=1$）。\n- 通过将 $x_{5,0}$ 替换为 $x_{5,0}+\\varepsilon$ 来构建扰动数据集 $X^{(\\varepsilon)}$，保持 $X$ 的所有其他条目不变。考虑以下扰动测试集\n$$\n\\varepsilon \\in \\{0.0,\\;10^{-12},\\;10^{-6},\\;10^{-2}\\}.\n$$\n\n任务：\n1) 实现上述确定性决策树，并在基线数据 $X$ 和 $y$ 上进行训练，以获得基线树 $T^{(0)}$。\n\n2) 对于测试集中的每个 $\\varepsilon$，构建 $X^{(\\varepsilon)}$，在相同规则下训练一棵树 $T^{(\\varepsilon)}$，并计算相对于 $T^{(0)}$ 的两个诊断指标：\n   - 结构差异 $D_{\\text{nodes}}(\\varepsilon)$：当按前序遍历进行位置比较时，$T^{(0)}$ 和 $T^{(\\varepsilon)}$ 之间的内部节点不匹配数量。形式上，递归地比较节点；如果一个是叶节点而另一个是内部节点，则计算两个子树中内部节点的总数；如果两者都是内部节点，若它们的 $(j,t)$ 对不同，则加 $1$，并对对齐的左右子节点进行递归；如果两者都是叶节点，则加 $0$。\n   - 功能差异 $F_{\\text{pred}}(\\varepsilon)$：在 $n=16$ 家训练公司中，$T^{(0)}$ 和 $T^{(\\varepsilon)}$ 预测的标签不同的公司所占的比例。\n\n3) 您的程序应生成单行输出，其中包含测试集的汇总结果，格式为方括号括起来的逗号分隔列表。对于每个 $\\varepsilon$，按顺序附加 $D_{\\text{nodes}}(\\varepsilon)$（一个整数），然后是 $F_{\\text{pred}}(\\varepsilon)$（一个浮点数）。例如，输出格式必须是\n\"[D_nodes(eps1),F_pred(eps1),D_nodes(eps2),F_pred(eps2),D_nodes(eps3),F_pred(eps3),D_nodes(eps4),F_pred(eps4)]\"。\n\n基于核心定义的预期行为说明：\n- 使用 $G(S)=2p(1-p)$ 和上述计数，在根节点上，对于未扰动的数据，收益在阈值 $0$ 处的分裂和杠杆率在阈值 $0$ 处的分裂都获得了相同的不纯度减少，因此平局打破规则选择了收益（特征 $0$）。当用 $\\varepsilon>0$ 的 $x_{5,0}+\\varepsilon$ 替换 $x_{5,0}$ 后，收益在 $0$ 处的分裂严格变差，而杠杆率在 $0$ 处的分裂保持不变，因此根分裂切换到杠杆率（特征 $1$）。顶部的这一变化会级联影响整个深度递归的构建过程，并通常导致内部节点结构发生根本性变化。\n\n您的实现必须严格遵守上述算法规则，并且不得需要任何用户输入。最终结果必须以所描述的精确格式作为单行打印，针对指定的 $\\varepsilon$ 值。不涉及任何物理单位或角度。所有报告的分数必须是 $[0,1]$ 范围内的十进制数。",
            "solution": "如所述，该问题是有效的。它构成了一个定义明确的计算练习，其基础是机器学习的既定原则，特别是决策树的构建。算法规范，包括贪婪优化准则、停止条件和确定性平局打破规则，是完整且无歧义的，允许唯一的解决方案。我们继续进行。\n\n任务是展示决策树在微小扰动下的结构不稳定性。这需要根据提供的规格实现一个确定性的、贪婪的决策树算法。解决方案涉及构建一个基线树 $T^{(0)}$ 和几个扰动树 $T^{(\\varepsilon)}$，然后量化它们之间的差异。\n\n该算法的核心是构建树的递归过程。在每个节点，都必须做出一个决定：要么将其声明为叶节点，要么将其分裂成两个子节点。如果一个节点是纯的（所有样本都属于一个类别），如果它已达到最大深度 $D=4$，或者如果没有容许的分裂可以实现严格为正的基尼不纯度减少，则该节点成为叶节点。一个容许的分裂是在允许的特征（$j=0$ 或 $j=1$）上，于固定阈值 $t=0$ 处进行的分裂，且该分裂产生两个非空子节点。叶节点的预测是多数类，平局则解析为类别 $0$。\n\n如果一个节点不是叶节点，它就会被分裂。贪婪的选择是最大化基尼增益 $\\Delta G$ 的容许分裂 $(j, t)$。具有正样本比例 $p$ 的集合 $S$ 的基尼不纯度是 $G(S) = 2p(1-p)$。对于将 $S$ 分裂为 $S_L$ 和 $S_R$ 的情况，增益是 $\\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right)$。问题指定了一个严格的平局打破规则：如果多个分裂产生相同的最大增益，则选择特征索引 $j$ 较小的那个。\n\n不稳定现象源于根节点。让我们分析未扰动数据（$\\varepsilon=0$）的初始分裂。根节点包含 $n=16$ 个样本，其中有 $n_1=8$ 家破产公司（$y=1$）和 $n_0=8$ 家非破产公司（$y=0$）。正例的比例是 $p=8/16=0.5$，所以根节点的基尼不纯度是 $G(S_{\\text{root}}) = 2(0.5)(1-0.5) = 0.5$。\n\n1.  **按收益分裂 ($j=0, t=0$)**：\n    左子节点 ($x_{i,0} \\le 0$) 包含 $6$ 家破产公司和 $2$ 家非破产公司 ($|S_L|=8, p_L=6/8=0.75$)。右子节点 ($x_{i,0} > 0$) 包含 $2$ 家破产公司和 $6$ 家非破产公司 ($|S_R|=8, p_R=2/8=0.25$)。\n    基尼不纯度为 $G(S_L) = G(S_R) = 2(0.75)(0.25) = 0.375$。\n    增益为 $\\Delta G_0 = 0.5 - \\left(\\frac{8}{16}(0.375) + \\frac{8}{16}(0.375)\\right) = 0.5 - 0.375 = 0.125$。\n\n2.  **按杠杆率分裂 ($j=1, t=0$)**：\n    左子节点 ($x_{i,1} \\le 0$) 包含 $2$ 家破产公司和 $6$ 家非破产公司 ($|S_L|=8, p_L=2/8=0.25$)。右子节点 ($x_{i,1} > 0$) 包含 $6$ 家破产公司和 $2$ 家非破产公司 ($|S_R|=8, p_R=6/8=0.75$)。\n    基尼不纯度同样为 $G(S_L) = G(S_R) = 0.375$。\n    增益为 $\\Delta G_1 = 0.5 - 0.375 = 0.125$。\n\n由于 $\\Delta G_0 = \\Delta G_1$，平局打破规则规定我们选择较小的特征索引 $j=0$。因此 $T^{(0)}$ 的根节点按收益分裂。\n\n现在考虑 $\\varepsilon > 0$ 的扰动。公司 $i=5$（破产）的收益从 $x_{5,0}=0$ 变为 $x_{5,0}=\\varepsilon>0$。这家公司现在会移动到收益分裂的右子节点。杠杆率分裂不受影响。\n杠杆率分裂的增益保持为 $\\Delta G_1 = 0.125$。对于收益分裂，子节点的大小变得不平衡。左子节点有 $|S_L'|=7$（$5$ 家破产，$2$ 家非破产），右子节点有 $|S_R'|=9$（$3$ 家破产，$6$ 家非破产）。\n$G(S_L') = 2(5/7)(2/7) = 20/49$。$G(S_R') = 2(3/9)(6/9) = 4/9$。\n收益分裂的新增益是 $\\Delta G_0' = 0.5 - \\left(\\frac{7}{16}(20/49) + \\frac{9}{16}(4/9)\\right) = 0.5 - (\\frac{5}{28} + \\frac{1}{4}) = 0.5 - \\frac{12}{28} = \\frac{1}{14} \\approx 0.0714$。\n由于 $\\Delta G_1 > \\Delta G_0'$，对于任何 $\\varepsilon > 0$，最优分裂都变成了杠杆率（$j=1$）。根节点的这一变化会向下传播到整棵树，导致一个完全不同的结构。\n\n为了量化这些差异，我们计算两个指标：\n-   **结构差异 $D_{\\text{nodes}}(\\varepsilon)$**：这是通过对 $T^{(0)}$ 和 $T^{(\\varepsilon)}$ 的节点进行前序递归比较来计算的。如果两个对应的节点都是内部节点但具有不同的分裂规则，则增加 $1$ 的成本。如果一个是叶节点而另一个是内部节点，则成本是内部节点子树中内部节点的总数。总差异是在整个树结构上这些成本的总和。\n-   **功能差异 $F_{\\text{pred}}(\\varepsilon)$**：这衡量了在 $16$ 个训练样本中，$T^{(0)}$ 和 $T^{(\\varepsilon)}$ 的预测标签不同的样本比例。每个公司 $i$ 的预测是从相应的树中获得的，即 $T^{(0)}$ 对 $X$ 进行预测，而 $T^{(\\varepsilon)}$ 对 $X^{(\\varepsilon)}$ 进行预测。\n\n实现将包含一个 `DecisionTree` 类，该类封装了 `fit` 和 `predict` 逻辑，并精确遵循指定的算法。一个 `Node` 类表示树的结构。单独的函数将计算结构和功能差异。通过对指定的 $\\varepsilon$ 值套件执行此逻辑，我们生成所需的输出。",
            "answer": "```python\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, predicted_class=None):\n        self.predicted_class = predicted_class  # Used for leaf nodes\n        self.feature_index = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n\n    @property\n    def is_leaf(self):\n        \"\"\"A node is a leaf if it has a predicted class.\"\"\"\n        return self.predicted_class is not None\n\nclass DecisionTree:\n    \"\"\"\n    A deterministic, greedy, axis-aligned decision tree.\n    \"\"\"\n    def __init__(self, max_depth=4):\n        self.max_depth = max_depth\n        self.root = None\n        self.split_features = [0, 1]  # Earnings, Leverage\n        self.split_threshold = 0.0\n\n    def _gini(self, y):\n        \"\"\"Calculates the Gini impurity.\"\"\"\n        n = len(y)\n        if n == 0:\n            return 0.0\n        p = np.sum(y) / n\n        return 2 * p * (1 - p)\n\n    def _make_leaf(self, y):\n        \"\"\"Creates a leaf node and determines its prediction.\"\"\"\n        leaf = Node()\n        if len(y) == 0:\n            leaf.predicted_class = 0\n            return leaf\n        \n        n_pos = np.sum(y)\n        n_neg = len(y) - n_pos\n        # Majority class prediction, with ties predicting class 0\n        leaf.predicted_class = 1 if n_pos > n_neg else 0\n        return leaf\n\n    def _find_best_split(self, X, y, indices):\n        \"\"\"Finds the best admissible split for a set of samples.\"\"\"\n        best_gain = -1.0\n        best_split = None\n        \n        current_y = y[indices]\n        if len(current_y) == 0:\n            return None, -1.0\n            \n        parent_gini = self._gini(current_y)\n        \n        for feature_index in self.split_features:\n            left_indices = indices[X[indices, feature_index] <= self.split_threshold]\n            right_indices = indices[X[indices, feature_index] > self.split_threshold]\n            \n            if len(left_indices) == 0 or len(right_indices) == 0:\n                continue\n\n            y_left, y_right = y[left_indices], y[right_indices]\n            gini_left, gini_right = self._gini(y_left), self._gini(y_right)\n            \n            n = len(current_y)\n            n_left, n_right = len(y_left), len(y_right)\n            child_gini = (n_left / n) * gini_left + (n_right / n) * gini_right\n            gain = parent_gini - child_gini\n            \n            if gain > best_gain:\n                best_gain = gain\n                best_split = (feature_index, self.split_threshold, left_indices, right_indices)\n        \n        return best_split, best_gain\n\n    def _build_tree(self, X, y, indices, depth):\n        \"\"\"Recursively builds the tree.\"\"\"\n        current_y = y[indices]\n        is_pure = (self._gini(current_y) == 0.0)\n        \n        if depth == self.max_depth or is_pure:\n            return self._make_leaf(current_y)\n            \n        best_split, best_gain = self._find_best_split(X, y, indices)\n        \n        if best_gain <= 0:\n            return self._make_leaf(current_y)\n        \n        feature_index, threshold, left_indices, right_indices = best_split\n        \n        node = Node()\n        node.feature_index = feature_index\n        node.threshold = threshold\n        node.left = self._build_tree(X, y, left_indices, depth + 1)\n        node.right = self._build_tree(X, y, right_indices, depth + 1)\n        \n        return node\n        \n    def fit(self, X, y):\n        \"\"\"Trains the decision tree.\"\"\"\n        initial_indices = np.arange(X.shape[0])\n        self.root = self._build_tree(X, y, initial_indices, depth=0)\n        \n    def _predict_one(self, x, node):\n        \"\"\"Predicts the class for a single sample.\"\"\"\n        if node.is_leaf:\n            return node.predicted_class\n        if x[node.feature_index] <= node.threshold:\n            return self._predict_one(x, node.left)\n        else:\n            return self._predict_one(x, node.right)\n\n    def predict(self, X):\n        \"\"\"Predicts classes for a set of samples.\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])\n\ndef count_internal_nodes(node):\n    \"\"\"Recursively counts the number of internal nodes in a subtree.\"\"\"\n    if node is None or node.is_leaf:\n        return 0\n    return 1 + count_internal_nodes(node.left) + count_internal_nodes(node.right)\n\ndef compute_structural_difference(node1, node2):\n    \"\"\"Recursively computes the structural difference between two trees.\"\"\"\n    if node1.is_leaf and node2.is_leaf:\n        return 0\n    \n    if node1.is_leaf and not node2.is_leaf:\n        return count_internal_nodes(node2)\n        \n    if not node1.is_leaf and node2.is_leaf:\n        return count_internal_nodes(node1)\n        \n    # Both are internal nodes\n    cost = 0\n    if node1.feature_index != node2.feature_index or node1.threshold != node2.threshold:\n        cost = 1\n        \n    cost += compute_structural_difference(node1.left, node2.left)\n    cost += compute_structural_difference(node1.right, node2.right)\n    \n    return cost\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment and produce results.\n    \"\"\"\n    # Define baseline data\n    y = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int)\n    X = np.zeros((16, 4))\n    X[0:8, 0] = [0, 0, 0, 0, 0, 0, 1, 1]\n    X[8:16, 0] = [0, 0, 1, 1, 1, 1, 1, 1]\n    X[0:8, 1] = [-1, -1, 1, 1, 1, 1, 1, 1]\n    X[8:16, 1] = [-1, -1, -1, -1, -1, -1, 1, 1]\n    \n    perturbed_company_idx = 5\n    epsilons = [0.0, 1e-12, 1e-6, 1e-2]\n    \n    # Train baseline tree and get predictions\n    tree_0 = DecisionTree(max_depth=4)\n    tree_0.fit(X, y)\n    preds_0 = tree_0.predict(X)\n    \n    results = []\n    \n    for eps in epsilons:\n        if eps == 0.0:\n            # The baseline tree compared to itself has 0 difference\n            d_nodes = 0\n            f_pred = 0.0\n        else:\n            # Create perturbed data\n            X_eps = X.copy()\n            X_eps[perturbed_company_idx, 0] += eps\n            \n            # Train perturbed tree\n            tree_eps = DecisionTree(max_depth=4)\n            tree_eps.fit(X_eps, y)\n            \n            # Compute Structural Difference\n            d_nodes = compute_structural_difference(tree_0.root, tree_eps.root)\n            \n            # Compute Functional Difference\n            # Predictions are made on the data each tree was trained on\n            preds_eps = tree_eps.predict(X_eps)\n            f_pred = np.sum(preds_0 != preds_eps) / len(y)\n\n        results.extend([d_nodes, f_pred])\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "超越模型构建本身，最后的这个练习将深入探讨模型可解释性这一前沿领域。给定一个用于信用评分的现有随机森林模型，你的任务是找出一个被拒申请人的档案需要做出的最小“反事实”改变，从而使模型预测结果翻转为“批准” 。这种反事实解释提供了一种强有力的工具，用以审计和理解复杂模型的决策行为。",
            "id": "2386887",
            "problem": "给定一个用于信贷审批的固定二元分类模型，该模型是一个由三棵决策树（表示为树 $A$、$B$ 和 $C$）构成的随机森林（RF）。每棵树都对一个特征向量 $x \\in \\mathbb{R}^3$ 进行操作，其分量 $x = (x_1,x_2,x_3)$ 定义如下：$x_1$ 是以万美元为单位的年收入，限制在 $x_1 \\in [1.0,20.0]$；$x_2$ 是循环信贷使用率，限制在 $x_2 \\in [0.0,1.0]$；$x_3$ 是一个归一化的信用历史质量得分，限制在 $x_3 \\in [0.0,1.0]$。RF 返回标签 $1$ 表示批准，$0$ 表示拒绝，其预测结果由三棵树的多数投票决定（当且仅当至少有 $2$ 棵树预测为 $1$ 时批准）。每棵树都是一个二元决策树，具有 $x_j \\le t$ 与 $x_j > t$ 形式的轴对齐分裂。树在叶节点输出其二元标签。\n\n对于满足以下两个区域中至少一个的任何输入，树 $A$ 输出批准（$1$），否则输出拒绝（$0$）：\n- 区域 $A_1$：$x_3 > 0.55$ 且 $x_2 \\le 0.70$。\n- 区域 $A_2$：$x_3 \\le 0.55$ 且 $x_2 > 0.30$ 且 $x_1 > 8.00$。\n\n对于满足以下三个区域中至少一个的任何输入，树 $B$ 输出批准（$1$），否则输出拒绝（$0$）：\n- 区域 $B_1$：$x_1 \\le 12.00$ 且 $x_3 > 0.40$ 且 $x_2 \\le 0.50$。\n- 区域 $B_2$：$x_1 > 12.00$ 且 $x_2 \\le 0.60$。\n- 区域 $B_3$：$x_1 > 12.00$ 且 $x_2 > 0.60$ 且 $x_3 \\le 0.80$。\n\n对于满足以下两个区域中至少一个的任何输入，树 $C$ 输出批准（$1$），否则输出拒绝（$0$）：\n- 区域 $C_1$：$x_2 > 0.20$ 且 $x_3 \\le 0.50$ 且 $x_1 \\le 15.00$。\n- 区域 $C_2$：$x_2 > 0.20$ 且 $x_3 > 0.50$。\n\n根据构造，RF 当且仅当至少有两棵树输出批准时，才会输出批准。对于当前被 RF 拒绝的任何给定申请人特征向量 $x$，定义一个反事实变化为一个向量 $\\Delta x \\in \\mathbb{R}^3$，使得修改后的向量 $x' = x + \\Delta x$ 位于定义域盒子 $[1.0,20.0] \\times [0.0,1.0] \\times [0.0,1.0]$ 内，并能获得 RF 的批准。变化的幅度由 $\\ell_1$ 范数 $||\\Delta x||_1 = | \\Delta x_1 | + | \\Delta x_2 | + | \\Delta x_3 |$ 评估。任务是找到一个最小变化的反事实，即一个向量 $\\Delta x$，它在满足 $x'$ 位于定义域内且 RF 对 $x'$ 预测为批准的条件下，最小化 $||\\Delta x||_1$。\n\n树区域中的严格不等式必须以 $\\varepsilon = 10^{-6}$ 的数值容差进行解释。具体来说，对于任何形如 $x_j > t$ 的约束，可行性要求 $x_j \\ge t + \\varepsilon$；对于任何形如 $x_j \\le t$ 的约束，可行性要求 $x_j \\le t$。对于多个约束的交集，所有约束都必须同时满足。如果存在多个最小化解，您必须按照以下确定性的平局打破规则进行选择：首先，最小化欧几里得范数 $||\\Delta x||_2$；如果仍然存在平局，选择字典序最小的 $\\Delta x$（先比较 $\\Delta x_1$，然后是 $\\Delta x_2$，最后是 $\\Delta x_3$）。\n\n您的程序必须为下面的每个测试用例计算一个如上定义的最小变化反事实 $\\Delta x$。所有测试用例的最终输出必须汇总到一行，形式为一个用方括号括起来的逗号分隔列表，其中每个用例的结果是一个包含对应于 $(\\Delta x_1,\\Delta x_2,\\Delta x_3)$ 的三个浮点值的列表。每个浮点值必须四舍五入到六位小数。\n\n测试套件（每个 $x$ 都在定义域内且被 RF 拒绝）：\n- 案例 1：$x = (7.00, 0.95, 0.90)$。\n- 案例 2：$x = (10.00, 0.20, 0.40)$。\n- 案例 3：$x = (19.00, 0.95, 0.85)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来且无空格的逗号分隔列表形式的结果，其中每个元素是对应一个案例的三分量列表，顺序与上文相同。例如，一个可接受的格式是 $[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],[a_{31},a_{32},a_{33}]]$，其中每个 $a_{ij}$ 是一个六位小数的数字。",
            "solution": "用户提供了一个问题，要求为随机森林（RF）分类模型寻找一个最小的反事实变化。该问题定义明确，具有科学依据，并且计算上是可行的。我现在将着手进行正式的求解。\n\n问题在于，对于一个初始被 RF 模型拒绝的给定输入特征向量 $x = (x_1, x_2, x_3)$，找到一个变化向量 $\\Delta x = (\\Delta x_1, \\Delta x_2, \\Delta x_3)$。修改后的向量 $x' = x + \\Delta x$ 必须被分类为“批准”，并且必须保持在有效特征域 $D$ 内。目标是找到一个在特定平局打破规则下，最小化 $\\ell_1$ 范数 $||\\Delta x||_1 = \\sum_{i=1}^3 |\\Delta x_i|$ 的 $\\Delta x$。\n\n首先，让我们将问题的各个组成部分形式化。\n特征域是 $\\mathbb{R}^3$ 中的一个超矩形（一个盒子）：\n$$D = [1.0, 20.0] \\times [0.0, 1.0] \\times [0.0, 1.0]$$\nRF 模型由三棵决策树 $T_A, T_B, T_C$ 组成。设它们对输入 $z$ 的各自预测为 $A(z), B(z), C(z) \\in \\{0, 1\\}$。RF 的预测为批准（1）当且仅当各棵树预测值的总和至少为 $2$：\n$$RF(z) = 1 \\iff A(z) + B(z) + C(z) \\ge 2$$\n给定一个输入 $x$，其 $RF(x) = 0$。我们寻求一个新点 $x' = x + \\Delta x$，使得 $RF(x')=1$ 且 $x' \\in D$。目标是解决以下优化问题：\n$$ \\min_{\\Delta x} ||\\Delta x||_1 \\quad \\text{subject to} \\quad (x + \\Delta x) \\in D \\text{ and } RF(x + \\Delta x) = 1 $$\n平局打破规则是：如果多个 $\\Delta x$ 产生相同的最小 $\\ell_1$ 范数，首先选择具有最小欧几里得范数 $||\\Delta x||_2$ 的那个，然后选择字典序最小的向量 $\\Delta x$。\n\n条件 $RF(x')=1$ 意味着至少有两棵树必须批准 $x'$。这可以通过三种方式发生（这些方式并非相互排斥）：\n1. $A(x')=1$ 且 $B(x')=1$。\n2. $A(x')=1$ 且 $C(x')=1$。\n3. $B(x')=1$ 且 $C(x')=1$。\n\n令 $\\mathcal{A}_A, \\mathcal{A}_B, \\mathcal{A}_C$ 分别表示树 $A$、$B$ 和 $C$ 的批准区域。RF 的总批准区域 $\\mathcal{A}_{RF}$ 是至少有两棵树批准的区域的并集：\n$$ \\mathcal{A}_{RF} = (\\mathcal{A}_A \\cap \\mathcal{A}_B) \\cup (\\mathcal{A}_A \\cap \\mathcal{A}_C) \\cup (\\mathcal{A}_B \\cap \\mathcal{A}_C) $$\n每棵树的批准区域被定义为多个子区域的并集，这些子区域是轴对齐的超矩形（盒子）。例如，$\\mathcal{A}_A = A_1 \\cup A_2$。两个盒子的交集是另一个盒子（可能为空）。盒子的并集产生一个作为盒子并集的集合。因此，总批准区域 $\\mathcal{A}_{RF}$ 是有限个盒子的并集。让我们将这组盒子表示为 $\\{K_k\\}$。\n\n优化问题是在集合 $\\mathcal{A}_{RF} \\cap D$ 中找到一个在 $\\ell_1$ 范数下最接近 $x$ 的点 $x'$。这等同于找到从 $x$ 到构成 $\\mathcal{A}_{RF} \\cap D$ 的任何一个盒子的最小距离。\n$$ \\min_{x' \\in \\mathcal{A}_{RF} \\cap D} ||x' - x||_1 = \\min_k \\left( \\min_{x' \\in K_k \\cap D} ||x' - x||_1 \\right) $$\n\n对于任何单个盒子 $K = [l_1, u_1] \\times [l_2, u_2] \\times [l_3, u_3]$，找到最接近给定点 $x$ 的点 $x' \\in K$ 是一个可分离问题。通过将 $x$ 投影到 $K$ 上可以最小化 $\\ell_p$ 距离。投影点 $x'_{proj}$ 的坐标为：\n$$ (x'_{proj})_i = \\max(l_i, \\min(x_i, u_i)) $$\n变化向量是 $\\Delta x = x'_{proj} - x$，而 $\\ell_1$ 距离是 $||\\Delta x||_1 = \\sum_{i=1}^3 |(x'_{proj})_i - x_i|$。\n\n总体算法如下：\n1. 将每棵树的批准子区域（$A_1, A_2, B_1, \\dots, C_2$）定义为盒子，并为严格不等式引入数值容差 $\\varepsilon = 10^{-6}$。对于约束 $x_j > t$，边界为 $t+\\varepsilon$；对于 $x_j \\le t$，边界为 $t$。\n2. 枚举所有导致 RF 批准的树批准组合：$(A,B), (A,C), (B,C)$。\n3. 对于每对树，比如 $(T_i, T_j)$，通过取 $T_i$ 的每个批准子区域与 $T_j$ 的每个批准子区域的交集，生成所有交集盒子。\n4. 对于每个生成的交集盒子，将其与定义域盒子 $D$ 再作交集。\n5. 如果得到的盒子非空，则计算初始点 $x$ 在此盒子上的投影，以找到候选点 $x'$、变化向量 $\\Delta x = x' - x$、其 $\\ell_1$ 范数和 $\\ell_2$ 范数。\n6. 维护迄今为止找到的最佳候选解的记录，定义为元组 $(||\\Delta x||_1, ||\\Delta x||_2, \\Delta x)$。\n7. 遍历所有生成的盒子，将每个新的候选解与当前最佳解进行比较。如果新候选解的 $\\ell_1$ 范数更小，则它更优；或者如果 $\\ell_1$ 范数相等且其 $\\ell_2$ 范数更小；或者如果两个范数都相等且其 $\\Delta x$ 向量在字典序上更小。\n8. 在检查了所有可能的批准区域后，找到的最佳候选解即为最终解。\n\n这种穷举搜索保证能找到最优解，因为它考虑了所有可能获得 RF 批准的方式，并为每种方式找到了最小变化点。在此问题中，需要检查的区域数量是有限且较小的。具体来说，每个测试用例需要评估 $(2 \\times 3) + (2 \\times 2) + (3 \\times 2) = 6+4+6=16$ 个交集盒子。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the counterfactual explanation problem for all test cases.\n    \"\"\"\n    \n    # Numerical tolerance\n    EPS = 1e-6\n\n    # Feature domain box D\n    domain_box = (np.array([1.0, 0.0, 0.0]), np.array([20.0, 1.0, 1.0]))\n\n    # Define tree approval regions as boxes [low_bounds, high_bounds]\n    # np.inf is used for unbounded dimensions before intersecting with the domain.\n    inf = np.inf\n    \n    # Tree A regions\n    regions_A = [\n        # A1: x3 > 0.55, x2 <= 0.70\n        (np.array([-inf, -inf, 0.55 + EPS]), np.array([inf, 0.70, inf])),\n        # A2: x3 <= 0.55, x2 > 0.30, x1 > 8.00\n        (np.array([8.00 + EPS, 0.30 + EPS, -inf]), np.array([inf, inf, 0.55])),\n    ]\n\n    # Tree B regions\n    regions_B = [\n        # B1: x1 <= 12.00, x3 > 0.40, x2 <= 0.50\n        (np.array([-inf, -inf, 0.40 + EPS]), np.array([12.00, 0.50, inf])),\n        # B2: x1 > 12.00, x2 <= 0.60\n        (np.array([12.00 + EPS, -inf, -inf]), np.array([inf, 0.60, inf])),\n        # B3: x1 > 12.00, x2 > 0.60, x3 <= 0.80\n        (np.array([12.00 + EPS, 0.60 + EPS, -inf]), np.array([inf, inf, 0.80])),\n    ]\n\n    # Tree C regions\n    regions_C = [\n        # C1: x2 > 0.20, x3 <= 0.50, x1 <= 15.00\n        (np.array([-inf, 0.20 + EPS, -inf]), np.array([15.00, inf, 0.50])),\n        # C2: x2 > 0.20, x3 > 0.50\n        (np.array([-inf, 0.20 + EPS, 0.50 + EPS]), np.array([inf, inf, inf])),\n    ]\n\n    tree_regions = {'A': regions_A, 'B': regions_B, 'C': regions_C}\n    target_combinations = [('A', 'B'), ('A', 'C'), ('B', 'C')]\n    \n    def intersect_boxes(box1, box2):\n        \"\"\"Intersects two boxes.\"\"\"\n        low1, high1 = box1\n        low2, high2 = box2\n        intersect_low = np.maximum(low1, low2)\n        intersect_high = np.minimum(high1, high2)\n        return intersect_low, intersect_high\n\n    def get_candidate(x, box):\n        \"\"\"Calculates the change vector and norms for projecting x onto a box.\"\"\"\n        low, high = box\n        if np.any(low > high):\n            return (inf, inf, None) # Invalid box\n        \n        x_prime = np.maximum(low, np.minimum(x, high))\n        delta_x = x_prime - x\n        l1_norm = np.sum(np.abs(delta_x))\n        l2_norm = np.sqrt(np.sum(delta_x**2))\n        return (l1_norm, l2_norm, delta_x)\n\n    def solve_one_case(x_tuple):\n        \"\"\"Solves for a single test case vector x.\"\"\"\n        x = np.array(x_tuple, dtype=np.float64)\n        best_candidate = (inf, inf, None)\n\n        for combo in target_combinations:\n            regions1 = tree_regions[combo[0]]\n            regions2 = tree_regions[combo[1]]\n            \n            for r1 in regions1:\n                for r2 in regions2:\n                    # Intersection of two approval regions\n                    target_box = intersect_boxes(r1, r2)\n                    # Further intersect with domain\n                    final_box = intersect_boxes(target_box, domain_box)\n                    \n                    candidate = get_candidate(x, final_box)\n                    \n                    # Tie-breaking logic\n                    if candidate[0] < best_candidate[0]:\n                        best_candidate = candidate\n                    elif candidate[0] == best_candidate[0]:\n                        if candidate[1] < best_candidate[1]:\n                            best_candidate = candidate\n                        elif candidate[1] == best_candidate[1]:\n                            # Lexicographical comparison\n                            current_best_dx = best_candidate[2]\n                            new_dx = candidate[2]\n                            if new_dx[0] < current_best_dx[0] or \\\n                               (new_dx[0] == current_best_dx[0] and new_dx[1] < current_best_dx[1]) or \\\n                               (new_dx[0] == current_best_dx[0] and new_dx[1] == current_best_dx[1] and new_dx[2] < current_best_dx[2]):\n                                best_candidate = candidate\n        \n        return best_candidate[2]\n\n    # Test cases from the problem statement\n    test_cases = [\n        (7.00, 0.95, 0.90),\n        (10.00, 0.20, 0.40),\n        (19.00, 0.95, 0.85),\n    ]\n\n    results_formatted = []\n    for case in test_cases:\n        delta_x = solve_one_case(case)\n        \n        # Format each component to six decimal places\n        formatted_dx = [f\"{v:.6f}\" for v in delta_x]\n        results_formatted.append(f\"[{','.join(formatted_dx)}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n\n```"
        }
    ]
}