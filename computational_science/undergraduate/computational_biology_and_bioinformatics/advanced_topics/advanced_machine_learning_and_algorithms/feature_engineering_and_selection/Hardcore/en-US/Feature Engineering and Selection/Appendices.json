{
    "hands_on_practices": [
        {
            "introduction": "A fundamental task in bioinformatics is to analyze the statistical properties of a gene's sequence. One powerful property is codon usage bias, the non-uniform use of synonymous codons. This practice will guide you through engineering a feature that quantifies how much a gene's codon usage deviates from the average of its host organism, a key technique for identifying horizontally transferred genes or genes of viral origin. You will learn to compute empirical probability distributions from sequence data and compare them using the $\\ell_1$ distance, a core skill in computational sequence analysis .",
            "id": "2389797",
            "problem": "You are given a formal definition of a deviation feature that quantifies how much a gene’s codon usage differs from the average codon usage of its host organism. Let the codon alphabet be the set of all $64$ possible triplets over the Deoxyribonucleic Acid (DNA) alphabet $\\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$, and let the set of stop codons be $C_{\\mathrm{stop}}=\\{\\mathrm{TAA},\\mathrm{TAG},\\mathrm{TGA}\\}$. Define the set of sense codons as $C = \\{\\text{all DNA triplets}\\} \\setminus C_{\\mathrm{stop}}$, which contains $61$ codons. For any DNA sequence whose length is a multiple of $3$, its codonization is the left-to-right decomposition into non-overlapping triplets $(c_1,c_2,\\dots,c_n)$, where $n$ is the number of codons.\n\nFor any codonized sequence $S$ (with no stop codons in the test suite), define its empirical codon usage distribution $p_S$ over $C$ by\n$$\np_S(c) = \\frac{\\#\\{i \\in \\{1,\\dots,n\\}: c_i = c\\}}{n} \\quad \\text{for each } c \\in C,\n$$\nwhere $n$ is the total number of codons in $S$, and $\\#\\{\\cdot\\}$ denotes the count. For a multiset of host coding sequences $H=\\{H_1,H_2,\\dots,H_m\\}$, define the host’s pooled average codon usage distribution $q_H$ by pooling counts across all host sequences and normalizing by the total number of codons,\n$$\nq_H(c) = \\frac{\\sum_{j=1}^{m} \\#\\{i: (H_j)_i = c\\}}{\\sum_{j=1}^{m} n_j} \\quad \\text{for each } c \\in C,\n$$\nwhere $n_j$ is the number of codons in $H_j$. The deviation feature to compute for a gene sequence $G$ is the $\\ell_1$ distance between its codon usage and the host’s pooled usage,\n$$\nD(G,H) = \\sum_{c \\in C} \\left| p_G(c) - q_H(c) \\right|.\n$$\n\nAll sequences in the test suite are DNA uppercase strings with length divisible by $3$, and none contain stop codons. Codonization must use the reading frame starting at position $1$, with non-overlapping triplets. Use $C$ as the set of sense codons as defined above. Your program must compute $D(G,H)$ exactly as defined, and must output results rounded to $6$ decimal places.\n\nTest suite:\n- Host coding set $H$ consists of the following three sequences:\n  - $H_1 = \\text{ATGGCTGCTGGTGGCATGGCCGCTGGT}$,\n  - $H_2 = \\text{GCTGGTGCTGGCATGGCTGCCGGT}$,\n  - $H_3 = \\text{ATGGCTGGTGCTGCTGGC}$.\n- Four candidate gene sequences $G$ to evaluate against the same host $H$:\n  1. $G_1 = \\text{ATGGCTGCTGGTGGCATGGCCGCTGGTGCTGGTGCTGGCATGGCTGCCGGTATGGCTGGTGCTGCTGGC}$,\n  2. $G_2 = \\text{TTTTTCTTATTGAAAAAGAACAATCAACAC}$,\n  3. $G_3 = \\text{ATGGCTGGTGCTAAGGCTTACGGCGCCTTT}$,\n  4. $G_4 = \\text{ATG}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the four results $[D(G_1,H),D(G_2,H),D(G_3,H),D(G_4,H)]$ as a comma-separated list of decimals, rounded to $6$ decimal places, enclosed in square brackets, with no spaces. For example, the format must be exactly like $[\\dots]$.\n- There are no physical units or angle units involved. All values are dimensionless real numbers.\n\nCoverage notes:\n- The test suite includes a general case, an exact-match case, a disjoint-support boundary case, and a minimal-length gene case. The answers must be decimals rounded to $6$ places.",
            "solution": "The problem statement is scrutinized and found to be valid. It is scientifically grounded in the domain of bioinformatics, specifically concerning codon usage bias. The definitions provided for codon usage distributions and the deviation feature (the $\\ell_1$ distance) are mathematically precise and standard within the field. All necessary data and constraints are provided, forming a well-posed problem with a unique, computable solution. The problem is objective, free of ambiguity, and its computational task is feasible. I will therefore proceed with a complete solution.\n\nThe solution methodology involves three principal steps:\n1.  Establish a canonical representation of the sense codons.\n2.  Compute the host's pooled average codon usage distribution, denoted as $q_H$.\n3.  For each candidate gene sequence $G$, compute its empirical codon usage distribution, $p_G$, and then calculate the deviation feature $D(G,H) = \\sum_{c \\in C} |p_G(c) - q_H(c)|$.\n\nFirst, we define the set of sense codons, $C$. This is the set of all $4^3 = 64$ possible DNA triplets, excluding the three standard stop codons: $\\mathrm{TAA}$, $\\mathrm{TAG}$, and $\\mathrm{TGA}$. This leaves $|C| = 61$ sense codons. For computational purposes, we establish a fixed, ordered list of these $61$ codons and a mapping from each codon string to a unique index. This allows us to represent codon usage distributions as vectors in $\\mathbb{R}^{61}$.\n\nSecond, we compute the distribution $q_H$ for the host organism. The problem defines this as the pooled average usage. This is calculated by first concatenating all host sequences $\\{H_1, H_2, \\dots, H_m\\}$ into a single large sequence. Then, we count the occurrences of each of the $61$ sense codons within this pooled sequence. The distribution $q_H$ is obtained by normalizing these counts by the total number of codons in the pooled sequence.\nThe provided host set is $H = \\{H_1, H_2, H_3\\}$, where:\n$H_1 = \\text{ATGGCTGCTGGTGGCATGGCCGCTGGT}$ (length $27$, $n_1 = 9$ codons)\n$H_2 = \\text{GCTGGTGCTGGCATGGCTGCCGGT}$ (length $24$, $n_2 = 8$ codons)\n$H_3 = \\text{ATGGCTGGTGCTGCTGGC}$ (length $18$, $n_3 = 6$ codons)\n\nThe total number of codons in the host pool is $N_H = n_1 + n_2 + n_3 = 9 + 8 + 6 = 23$.\nThe pooled codon counts are determined by summing the counts from each sequence:\n- Count(ATG) = $4$\n- Count(GCT) = $9$\n- Count(GGT) = $5$\n- Count(GGC) = $3$\n- Count(GCC) = $2$\nAll other codons have a count of $0$.\nThe host distribution $q_H$ is a vector where the component for each codon $c$ is $q_H(c) = \\text{Count}(c) / N_H$. For example, $q_H(\\text{ATG}) = 4/23$.\n\nThird, for each candidate gene $G_k$ ($k=1,2,3,4$), we calculate its empirical codon usage distribution $p_{G_k}$. This involves codonizing the gene sequence, counting the occurrences of each sense codon, and normalizing by the gene's total number of codons, $n_{G_k}$.\n\nFinally, the deviation feature $D(G_k, H)$ is the $\\ell_1$ distance between the two probability distribution vectors, $p_{G_k}$ and $q_H$.\n$D(G_k, H) = \\| p_{G_k} - q_H \\|_1 = \\sum_{i=1}^{61} |(p_{G_k})_i - (q_H)_i|$.\n\nLet us apply this to the four test cases.\n\n**Case 1: $G_1$**\n$G_1$ is the concatenation of $H_1, H_2, H_3$. Thus, its codon composition is identical to the pooled host composition. The number of codons is $n_{G_1} = 23$. The distribution $p_{G_1}$ is therefore identical to $q_H$.\n$D(G_1, H) = \\sum_{c \\in C} |q_H(c) - q_H(c)| = 0$.\n\n**Case 2: $G_2 = \\text{TTTTTCTTATTGAAAAAGAACAATCAACAC}$**\nThis gene has $n_{G_2} = 10$ codons: {TTT, TTC, TTA, TTG, AAA, AAG, AAC, AAT, CAA, CAC}, each occurring once. Thus, for each of these $10$ codons $c$, $p_{G_2}(c) = 1/10$. The set of codons in $G_2$ is disjoint from the set of codons present in the host pool $H$. When two probability distributions have disjoint support, their $\\ell_1$ distance is maximal.\n$D(G_2, H) = \\sum_{c \\in \\text{codons}(G_2)} |p_{G_2}(c) - 0| + \\sum_{c \\in \\text{codons}(H)} |0 - q_H(c)| = \\sum p_{G_2}(c) + \\sum q_H(c) = 1 + 1 = 2$.\n\n**Case 3: $G_3 = \\text{ATGGCTGGTGCTAAGGCTTACGGCGCCTTT}$**\nThis gene has $n_{G_3} = 10$ codons. The codon counts are: ATG(1), GCT(3), GGT(1), AAG(1), TAC(1), GGC(1), GCC(1), TTT(1). The distribution $p_{G_3}$ is found by dividing these counts by $10$. We then compute $\\sum_{c \\in C} |p_{G_3}(c) - q_H(c)|$.\nThe non-zero terms are for codons present in either $G_3$ or $H$.\n- $|p_{G_3}(\\text{ATG}) - q_H(\\text{ATG})| = |1/10 - 4/23| = |23/230 - 40/230| = 17/230$\n- $|p_{G_3}(\\text{GCT}) - q_H(\\text{GCT})| = |3/10 - 9/23| = |69/230 - 90/230| = 21/230$\n- $|p_{G_3}(\\text{GGT}) - q_H(\\text{GGT})| = |1/10 - 5/23| = |23/230 - 50/230| = 27/230$\n- $|p_{G_3}(\\text{GGC}) - q_H(\\text{GGC})| = |1/10 - 3/23| = |23/230 - 30/230| = 7/230$\n- $|p_{G_3}(\\text{GCC}) - q_H(\\text{GCC})| = |1/10 - 2/23| = |23/230 - 20/230| = 3/230$\nTerms for codons only in $G_3$:\n- $|p_{G_3}(\\text{AAG}) - 0| = 1/10 = 23/230$\n- $|p_{G_3}(\\text{TAC}) - 0| = 1/10 = 23/230$\n- $|p_{G_3}(\\text{TTT}) - 0| = 1/10 = 23/230$\nThe sum is $(17+21+27+7+3+23+23+23)/230 = 144/230 = 72/115 \\approx 0.626087$.\n\n**Case 4: $G_4 = \\text{ATG}$**\nThis gene has $n_{G_4} = 1$ codon, ATG. Its distribution is $p_{G_4}(\\text{ATG}) = 1$, and $p_{G_4}(c) = 0$ for all other codons.\nThe distance is $D(G_4, H) = |p_{G_4}(\\text{ATG}) - q_H(\\text{ATG})| + \\sum_{c \\neq \\text{ATG}} |0 - q_H(c)|$.\nThis equals $|1 - 4/23| + (\\sum_{c \\in C} q_H(c) - q_H(\\text{ATG})) = 19/23 + (1 - 4/23) = 19/23 + 19/23 = 38/23 \\approx 1.652174$.\n\nThe implementation will perform these calculations programmatically for all $61$ codons to ensure correctness.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the codon usage deviation feature for a set of genes against a host.\n    \"\"\"\n    # Define test cases as per the problem statement.\n    host_sequences = [\n        \"ATGGCTGCTGGTGGCATGGCCGCTGGT\",\n        \"GCTGGTGCTGGCATGGCTGCCGGT\",\n        \"ATGGCTGGTGCTGCTGGC\",\n    ]\n    gene_sequences = [\n        \"ATGGCTGCTGGTGGCATGGCCGCTGGTGCTGGTGCTGGCATGGCTGCCGGTATGGCTGGTGCTGCTGGC\",\n        \"TTTTTCTTATTGAAAAAGAACAATCAACAC\",\n        \"ATGGCTGGTGCTAAGGCTTACGGCGCCTTT\",\n        \"ATG\",\n    ]\n\n    # Step 1: Establish a canonical representation of sense codons.\n    bases = ['A', 'C', 'G', 'T']\n    all_codons = [b1 + b2 + b3 for b1 in bases for b2 in bases for b3 in bases]\n    stop_codons = {\"TAA\", \"TAG\", \"TGA\"}\n    sense_codons = sorted([c for c in all_codons if c not in stop_codons])\n    codon_to_idx = {codon: i for i, codon in enumerate(sense_codons)}\n    num_sense_codons = len(sense_codons)\n\n    def get_codon_distribution(sequence: str) -> np.ndarray:\n        \"\"\"\n        Calculates the empirical codon usage distribution for a given DNA sequence.\n        \n        Args:\n            sequence: A DNA sequence string with length divisible by 3.\n\n        Returns:\n            A numpy array representing the probability distribution over sense codons.\n        \"\"\"\n        counts = np.zeros(num_sense_codons, dtype=np.float64)\n        num_codons = len(sequence) // 3\n\n        if num_codons == 0:\n            return counts\n\n        for i in range(0, len(sequence), 3):\n            codon = sequence[i:i+3]\n            if codon in codon_to_idx:\n                counts[codon_to_idx[codon]] += 1\n        \n        return counts / num_codons\n\n    # Step 2: Compute the host's pooled average codon usage distribution q_H.\n    pooled_host_sequence = \"\".join(host_sequences)\n    q_H = get_codon_distribution(pooled_host_sequence)\n\n    results = []\n    # Step 3: For each gene, compute its distribution p_G and the deviation D(G, H).\n    for gene_seq in gene_sequences:\n        # Calculate p_G for the current gene.\n        p_G = get_codon_distribution(gene_seq)\n        \n        # Calculate the L1 distance (deviation feature).\n        # D(G,H) = sum |p_G(c) - q_H(c)|\n        deviation = np.sum(np.abs(p_G - q_H))\n        \n        results.append(deviation)\n\n    # Format the final output as a comma-separated list of decimals rounded to 6 places.\n    # The format \"{:.6f}\" handles rounding and ensures 6 decimal places are shown.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Feature engineering often involves integrating different data types, such as genomic coordinates and molecular measurements. This exercise focuses on DNA methylation, an epigenetic mark crucial for gene regulation. You will create a feature by averaging methylation levels not at individual sites, but across biologically meaningful regions known as CpG island \"shores.\" This practice teaches you to handle genomic interval arithmetic and aggregate data based on functional annotations, moving from noisy point measurements to more robust and powerful regional features .",
            "id": "2389842",
            "problem": "You are given synthetic deoxyribonucleic acid (DNA) methylation microarray data with probe genomic coordinates and per-sample methylation values known as beta values. The task is to engineer a single feature per sample: the average beta value across all probes that lie in the Cytosine-phosphate-Guanosine (CpG) island shores, defined as regions flanking each CpG island by a specified window width on both sides, excluding any base pairs that are inside any CpG island. Your solution must be a complete runnable program that computes this feature for multiple test cases and additionally evaluates a simple feature-selection criterion based on variance across samples.\n\nStart from the following fundamental base:\n- A CpG island is represented as a closed genomic interval with integer base-pair coordinates on a chromosome. For an island with start coordinate $s$ and end coordinate $e$ (inclusive), the left shore is the closed interval $[s - w, s - 1]$ and the right shore is the closed interval $[e + 1, e + w]$, where $w$ is a nonnegative integer window width in base pairs. All intervals must be clipped to the chromosome domain $[1, L]$, where $L$ is the chromosome length in base pairs.\n- The union of shore regions over all islands is the set of base-pair positions that lie in at least one such left or right shore, after excluding any positions that lie inside any CpG island.\n- A probe at genomic position $x$ lies in the shore union if and only if $x$ is inside at least one left or right shore interval (after clipping to $[1, L]$) and $x$ is not contained in any CpG island interval.\n- For a given sample $i$, the engineered feature value is the arithmetic mean of beta values over all probes whose positions fall in the shore union, ignoring missing values. If no probe contributes for a sample, the feature value is undefined and should be represented as a not-a-number quantity.\n- The across-sample population variance is defined as $\\sigma^{2} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(m_{i} - \\bar{m}\\right)^{2}$ where $m_{i}$ is the engineered feature value for sample $i$, $\\bar{m}$ is the mean of the $m_{i}$ over samples with defined values, and $n$ is the count of samples with defined values used in $\\bar{m}$. Missing feature values should be excluded from the variance computation. If no samples have defined feature values, the variance is undefined and should be represented as a not-a-number quantity. The feature-selection decision for a test case is to return a boolean indicating whether $\\sigma^{2} \\ge \\tau$, where $\\tau$ is a provided nonnegative threshold.\n\nAll coordinates are integers and measured in base pairs (bp). Beta values are real numbers on $[0, 1]$ or not-a-number. No angles are involved. All outputs must be decimals, not percentages.\n\nYour program must:\n- For each test case, construct the shore union from the given CpG islands, chromosome length $L$, and window width $w$.\n- Select the probes whose genomic positions lie in the shore union and compute, for each sample, the engineered feature $m_{i}$ by averaging over the selected probes, ignoring missing values.\n- Compute the across-sample population variance $\\sigma^{2}$ over defined $m_{i}$ values and compare to the provided threshold $\\tau$ to produce a boolean decision.\n- Round each $m_{i}$ and $\\sigma^{2}$ to $6$ decimal places for output. If any $m_{i}$ or $\\sigma^{2}$ is undefined, output it as a not-a-number.\n\nInput is hardcoded via the following test suite that your program must embed. For each test case, the data consist of:\n- Chromosome length $L$ in base pairs.\n- Window width $w$ in base pairs.\n- A list of CpG island intervals $[(s_{1}, e_{1}), (s_{2}, e_{2}), \\dots]$ with $s_{k} \\le e_{k}$, endpoints inclusive.\n- A list of probes as pairs $(x, [\\beta_{1}, \\beta_{2}, \\dots, \\beta_{S}])$ where $x$ is the probe position and $\\beta_{i}$ is the beta value for sample $i$ (possibly not-a-number).\n- A nonnegative variance threshold $\\tau$.\n\nTest Suite:\n- Test case $1$ (happy path):\n  - $L = 10000$, $w = 2000$.\n  - Islands: $[(4000, 4500)]$.\n  - Probes:\n    - $(1500, [0.1, 0.2, 0.15])$\n    - $(2500, [0.8, 0.7, 0.9])$\n    - $(4000, [0.9, 0.95, 0.85])$\n    - $(4501, [0.3, 0.4, 0.35])$\n    - $(4600, [0.6, 0.5, 0.55])$\n    - $(7000, [0.2, 0.2, 0.2])$\n  - $\\tau = 0.001$.\n- Test case $2$ (boundary clipping and missing values):\n  - $L = 5000$, $w = 2000$.\n  - Islands: $[(100, 300)]$.\n  - Probes:\n    - $(1, [0.5, 0.6, 0.7])$\n    - $(99, [0.4, \\text{NaN}, 0.6])$\n    - $(100, [0.9, 0.8, 0.7])$\n    - $(200, [0.2, 0.2, 0.2])$\n    - $(301, [0.3, 0.3, 0.3])$\n    - $(2299, [0.2, 0.2, \\text{NaN}])$\n    - $(2300, [0.1, \\text{NaN}, 0.1])$\n    - $(2301, [0.0, 0.0, 0.0])$\n  - $\\tau = 0.002$.\n- Test case $3$ (overlapping shores from adjacent islands, excluding island bases):\n  - $L = 20000$, $w = 3000$.\n  - Islands: $[(5000, 5200), (8000, 8200)]$.\n  - Probes:\n    - $(2100, [0.2, 0.4, 0.6])$\n    - $(5100, [0.9, 0.9, 0.9])$\n    - $(6000, [0.5, 0.7, 0.9])$\n    - $(8100, [0.1, 0.2, 0.3])$\n    - $(9000, [0.4, 0.4, 0.4])$\n    - $(15000, [0.0, 0.0, 0.0])$\n    - $(11200, [0.8, 0.6, 0.4])$\n    - $(8201, [0.3, \\text{NaN}, 0.3])$\n  - $\\tau = 0.001$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-free, bracketed list of the three test-case results. Each test-case result is itself a list structured as $[m_{1}, m_{2}, \\dots, m_{S}, \\sigma^{2}, \\text{pass}]$ where $m_{i}$ are the per-sample engineered feature values rounded to $6$ decimal places, $\\sigma^{2}$ is the across-sample population variance rounded to $6$ decimal places, and $\\text{pass}$ is a boolean indicating whether $\\sigma^{2} \\ge \\tau$. The output must have no spaces. For example, a list of three test-case results should look like $[[\\dots],[\\dots],[\\dots]]$ with no spaces anywhere.",
            "solution": "The problem presented is a well-defined exercise in computational biology, specifically in the domain of feature engineering from genomic data. The task requires the computation of a single feature per sample from synthetic DNA methylation microarray data, followed by a simple variance-based feature selection step. The problem is scientifically grounded, logically consistent, and provides all necessary information for a unique solution. Therefore, it is deemed valid.\n\nThe solution proceeds by implementing the specified logic in a sequential, step-by-step manner for each test case.\n\nFirst, we must precisely identify the genomic regions of interest, which are the CpG island shores. A CpG island is given as a closed interval $[s, e]$. The shores are defined as regions of width $w$ flanking the island, specifically the intervals $[s - w, s - 1]$ and $[e + 1, e + w]$. These intervals must be clipped to the valid chromosome domain, which is $[1, L]$, where $L$ is the chromosome length. Thus, for an island $[s, e]$, the left shore is $[\\max(1, s - w), s - 1]$ and the right shore is $[e + 1, \\min(L, e + w)]$.\n\nThe core of the problem lies in defining the set of probe positions that are considered for feature computation. A probe at a genomic coordinate $x$ is selected if and only if two conditions are met:\n1. The position $x$ must fall within the union of all shore regions derived from all CpG islands. This means $x$ must be in at least one left or right shore interval.\n2. The position $x$ must NOT fall within any CpG island interval $[s_k, e_k]$.\n\nTo implement this logic efficiently, we can use boolean masks over the entire chromosome length $L$. Let us define two boolean arrays of length $L+1$ (using 1-based indexing for coordinates): `is_island` and `is_shore`.\n- The `is_island` array is initialized to false. For each island $[s_k, e_k]$, we set the elements from index $s_k$ to $e_k$ (inclusive) to true.\n- The `is_shore` array is also initialized to false. For each island $[s_k, e_k]$, we calculate its clipped left and right shore intervals and set the corresponding elements in `is_shore` to true.\n\nA probe at position $x$ is then selected if `is_shore[x]` is true and `is_island[x]` is false. This approach correctly handles complex cases, such as shores from adjacent islands overlapping each other or even overlapping other islands, by correctly applying the exclusion principle.\n\nOnce the set of relevant probes is identified, we engineer the feature for each sample. Let $S$ be the number of samples. The beta values for the selected probes form a matrix of values, where rows correspond to probes and columns to samples. For each sample $i \\in \\{1, \\dots, S\\}$, the feature value $m_i$ is calculated as the arithmetic mean of the beta values $(\\beta)$ from all selected probes, denoted by the set $P_{\\text{sel}}$. Any missing values, represented as not-a-number (NaN), must be ignored in this calculation.\n$$\nm_i = \\frac{1}{|P'_{\\text{sel}, i}|} \\sum_{p \\in P'_{\\text{sel}, i}} \\beta_{p,i}\n$$\nwhere $P'_{\\text{sel}, i}$ is the subset of selected probes for which the beta value for sample $i$ is not NaN. If for a given sample $i$, all selected probes have NaN beta values or if no probes are selected at all ($P_{\\text{sel}}$ is empty), the set $P'_{\\text{sel}, i}$ is empty and the feature value $m_i$ is defined as NaN.\n\nThe final step is to evaluate the utility of this engineered feature using a variance-based selection criterion. We compute the population variance, $\\sigma^2$, of the feature values $m_i$ across all samples for which $m_i$ is not NaN. Let the set of non-NaN feature values be $M' = \\{m_i | m_i \\text{ is not NaN}\\}$ and let $n = |M'|$ be the count of such values. The mean of these values is $\\bar{m} = \\frac{1}{n} \\sum_{m_i \\in M'} m_i$. The population variance is then:\n$$\n\\sigma^2 = \\frac{1}{n} \\sum_{m_i \\in M'} (m_i - \\bar{m})^2\n$$\nIf $n=0$ (all $m_i$ are NaN), $\\sigma^2$ is also NaN. The feature is selected if this variance meets or exceeds a given threshold $\\tau$:\n$$\n\\text{pass} = (\\sigma^2 \\ge \\tau)\n$$\nIf $\\sigma^2$ is NaN, this comparison evaluates to false.\n\nAll computed values for $m_i$ and $\\sigma^2$ are rounded to $6$ decimal places as required. The procedure is applied independently to each test case provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the feature engineering and selection problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"L\": 10000, \"w\": 2000,\n            \"islands\": [(4000, 4500)],\n            \"probes\": [\n                (1500, [0.1, 0.2, 0.15]),\n                (2500, [0.8, 0.7, 0.9]),\n                (4000, [0.9, 0.95, 0.85]),\n                (4501, [0.3, 0.4, 0.35]),\n                (4600, [0.6, 0.5, 0.55]),\n                (7000, [0.2, 0.2, 0.2]),\n            ],\n            \"tau\": 0.001\n        },\n        # Test case 2 (boundary clipping and missing values)\n        {\n            \"L\": 5000, \"w\": 2000,\n            \"islands\": [(100, 300)],\n            \"probes\": [\n                (1, [0.5, 0.6, 0.7]),\n                (99, [0.4, np.nan, 0.6]),\n                (100, [0.9, 0.8, 0.7]),\n                (200, [0.2, 0.2, 0.2]),\n                (301, [0.3, 0.3, 0.3]),\n                (2299, [0.2, 0.2, np.nan]),\n                (2300, [0.1, np.nan, 0.1]),\n                (2301, [0.0, 0.0, 0.0]),\n            ],\n            \"tau\": 0.002\n        },\n        # Test case 3 (overlapping shores from adjacent islands)\n        {\n            \"L\": 20000, \"w\": 3000,\n            \"islands\": [(5000, 5200), (8000, 8200)],\n            \"probes\": [\n                (2100, [0.2, 0.4, 0.6]),\n                (5100, [0.9, 0.9, 0.9]),\n                (6000, [0.5, 0.7, 0.9]),\n                (8100, [0.1, 0.2, 0.3]),\n                (9000, [0.4, 0.4, 0.4]),\n                (15000, [0.0, 0.0, 0.0]),\n                (11200, [0.8, 0.6, 0.4]),\n                (8201, [0.3, np.nan, 0.3]),\n            ],\n            \"tau\": 0.001\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        L = case[\"L\"]\n        w = case[\"w\"]\n        islands = case[\"islands\"]\n        probes = case[\"probes\"]\n        tau = case[\"tau\"]\n\n        # Step 1: Identify shore and island regions using boolean masks\n        is_island = np.zeros(L + 1, dtype=bool)\n        is_shore = np.zeros(L + 1, dtype=bool)\n\n        for s, e in islands:\n            is_island[s : e + 1] = True\n            \n            # Left shore\n            left_shore_start = max(1, s - w)\n            left_shore_end = s - 1\n            if left_shore_start <= left_shore_end:\n                is_shore[left_shore_start : left_shore_end + 1] = True\n            \n            # Right shore\n            right_shore_start = e + 1\n            right_shore_end = min(L, e + w)\n            if right_shore_start <= right_shore_end:\n                is_shore[right_shore_start : right_shore_end + 1] = True\n\n        # Step 2: Select probes that are in shores but not in islands\n        selected_betas = []\n        for pos, beta_values in probes:\n            if 1 <= pos <= L and is_shore[pos] and not is_island[pos]:\n                selected_betas.append(beta_values)\n        \n        num_samples = len(probes[0][1]) if probes else 0\n        \n        # Step 3: Compute engineered feature m_i for each sample\n        m_features = []\n        if not selected_betas:\n            m_features = [np.nan] * num_samples\n        else:\n            selected_betas_matrix = np.array(selected_betas)\n            for i in range(num_samples):\n                sample_betas = selected_betas_matrix[:, i]\n                # np.nanmean handles empty arrays (after NaN removal) by returning nan\n                mean_val = np.nanmean(sample_betas)\n                m_features.append(mean_val)\n\n        # Step 4: Compute variance and perform feature selection\n        m_features_valid = [m for m in m_features if not np.isnan(m)]\n        \n        if len(m_features_valid) > 0:\n            # np.var computes population variance by default (ddof=0)\n            variance = np.var(m_features_valid)\n        else:\n            variance = np.nan\n\n        pass_criterion = False\n        if not np.isnan(variance):\n            pass_criterion = variance >= tau\n            \n        # Step 5: Format results\n        rounded_m = [round(m, 6) if not np.isnan(m) else m for m in m_features]\n        rounded_var = round(variance, 6) if not np.isnan(variance) else variance\n\n        case_result = rounded_m + [rounded_var, pass_criterion]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format\n    # Custom formatter to handle nan, bool, and float\n    def format_item(item):\n        if isinstance(item, bool):\n            return str(item).lower()\n        if isinstance(item, float) and np.isnan(item):\n            return 'nan'\n        if isinstance(item, float):\n            return f\"{item:.6f}\"\n        return str(item)\n\n    result_str = \"[\"\n    for i, case_res in enumerate(all_results):\n        result_str += \"[\"\n        result_str += \",\".join([format_item(item) for item in case_res])\n        result_str += \"]\"\n        if i < len(all_results) - 1:\n            result_str += \",\"\n    result_str += \"]\"\n    \n    # Another way to achieve the same result without spaces\n    # import json\n    # print(json.dumps(all_results, separators=(',', ':')).replace('NaN', 'nan'))\n\n    print(result_str.replace(\"True\", \"true\").replace(\"False\", \"false\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Features are not always direct transformations of raw data; sometimes, the most powerful features are themselves parameters estimated from a statistical model. This advanced practice introduces the concept of epistasis, where the effect of one genetic variant on a trait is modified by another. You will engineer a feature that quantifies this interaction between two Single Nucleotide Polymorphisms (SNPs) by fitting a logistic regression model and extracting the coefficient of the interaction term, $\\beta_3$. This exercise demonstrates how modeling can be a sophisticated form of feature engineering to capture complex, non-additive biological effects .",
            "id": "2389799",
            "problem": "You are given binary phenotype data and two genotype features representing two single nucleotide polymorphisms (SNPs), denoted by $S_1$ and $S_2$, for multiple individuals. For an individual index $i \\in \\{1,2,\\ldots,n\\}$, the phenotype is $y_i \\in \\{0,1\\}$ and the genotypes are $S_{1,i} \\in \\{0,1,2\\}$ and $S_{2,i} \\in \\{0,1,2\\}$. Consider the logistic regression model in which the log-odds of $y_i=1$ is a linear function of an intercept, the main effects of $S_{1,i}$ and $S_{2,i}$, and their interaction:\n$$\n\\log\\left(\\frac{\\mathbb{P}(y_i=1 \\mid S_{1,i},S_{2,i})}{\\mathbb{P}(y_i=0 \\mid S_{1,i},S_{2,i})}\\right) \\;=\\; \\beta_0 \\;+\\; \\beta_1 S_{1,i} \\;+\\; \\beta_2 S_{2,i} \\;+\\; \\beta_3 \\left(S_{1,i} \\cdot S_{2,i}\\right).\n$$\nThe engineered interaction feature is defined to be the maximum-likelihood estimate of the coefficient $\\beta_3$ obtained by fitting the above model. If the interaction column $\\left(S_{1,i}\\cdot S_{2,i}\\right)_{i=1}^n$ is identically zero (so that it carries no information), define the engineered interaction feature to be $0$ by convention.\n\nYour task is to compute the engineered interaction feature for each of the following test cases. For each case, you must treat the given data as the complete dataset to which the model above is fitted, with no additional covariates. All computations are unitless. Your program should output a single line that aggregates the results from all test cases into a comma-separated list enclosed in square brackets, with each value rounded to exactly $6$ digits after the decimal point.\n\nTest Suite\n\n- Case A (balanced presence of all genotype combinations):\n  The dataset consists of $n=20$ individuals specified as ordered triples $(S_1,S_2,y)$. The samples are\n  $$(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,1),$$\n  $$(1,0,0),(1,0,0),(1,0,1),(1,0,0),(1,0,1),$$\n  $$(0,1,0),(0,1,1),(0,1,0),(0,1,0),(0,1,1),$$\n  $$(1,1,1),(1,1,1),(1,1,0),(1,1,1),(1,1,1).$$\n  For this case, compute the fitted value of $\\hat{\\beta}_3$.\n\n- Case B (approximately additive main effects with minimal interaction):\n  The dataset consists of $n=40$ individuals, partitioned into four groups by $(S_1,S_2)$ with equal group sizes. For $(S_1,S_2)=(0,0)$ there are $10$ individuals with $3$ having $y=1$ and $7$ having $y=0$; for $(S_1,S_2)=(1,0)$ there are $10$ individuals with $5$ having $y=1$ and $5$ having $y=0$; for $(S_1,S_2)=(0,1)$ there are $10$ individuals with $5$ having $y=1$ and $5$ having $y=0$; for $(S_1,S_2)=(1,1)$ there are $10$ individuals with $7$ having $y=1$ and $3$ having $y=0$. Explicitly, the samples are\n  $$(0,0,1),(0,0,1),(0,0,1),(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,0),(0,0,0),$$\n  $$(1,0,1),(1,0,1),(1,0,1),(1,0,1),(1,0,1),(1,0,0),(1,0,0),(1,0,0),(1,0,0),(1,0,0),$$\n  $$(0,1,1),(0,1,1),(0,1,1),(0,1,1),(0,1,1),(0,1,0),(0,1,0),(0,1,0),(0,1,0),(0,1,0),$$\n  $$(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,1),(1,1,0),(1,1,0),(1,1,0).$$\n  For this case, compute the fitted value of $\\hat{\\beta}_3$.\n\n- Case C (degenerate interaction column):\n  The dataset consists of $n=8$ individuals with $S_2$ identically zero. The samples are\n  $$(0,0,0),(1,0,1),(0,0,0),(2,0,1),(1,0,0),(2,0,1),(0,0,1),(1,0,0).$$\n  For this case, return $0$ as the engineered interaction feature by the stated convention.\n\nFinal Output Format\n\nYour program should produce a single line of output containing the three engineered interaction features as a comma-separated list enclosed in square brackets, with each value rounded to exactly $6$ digits after the decimal point, for example: $[\\hat{\\beta}_3^{(A)},\\hat{\\beta}_3^{(B)},\\hat{\\beta}_3^{(C)}]$.",
            "solution": "The problem statement is subjected to rigorous validation.\n\nStep 1: Extract Givens\n- **Model**: Logistic regression with logit link function for a binary phenotype $y_i \\in \\{0, 1\\}$ and two single nucleotide polymorphism (SNP) features $S_{1,i}, S_{2,i} \\in \\{0, 1, 2\\}$. The model is specified as:\n$$ \\log\\left(\\frac{\\mathbb{P}(y_i=1 \\mid S_{1,i},S_{2,i})}{\\mathbb{P}(y_i=0 \\mid S_{1,i},S_{2,i})}\\right) = \\beta_0 + \\beta_1 S_{1,i} + \\beta_2 S_{2,i} + \\beta_3 (S_{1,i} \\cdot S_{2,i}) $$\n- **Engineered Feature Definition**: The feature is the maximum-likelihood estimate (MLE) of the interaction coefficient, $\\hat{\\beta}_3$.\n- **Special Condition**: If the interaction column $(S_{1,i} \\cdot S_{2,i})_{i=1}^n$ is identically zero, the engineered feature is defined to be $0$.\n- **Data Sets**:\n    - **Case A**: $n=20$ samples provided as $(S_1, S_2, y)$ triples.\n    - **Case B**: $n=40$ samples provided as counts for each of the four $(S_1,S_2)$ combinations, $(0,0), (1,0), (0,1), (1,1)$, and also as explicit samples.\n    - **Case C**: $n=8$ samples for which $S_2$ is identically zero.\n- **Task**: Compute $\\hat{\\beta}_3$ for each case.\n\nStep 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded**: The problem is based on logistic regression, a fundamental statistical method, applied to SNP data analysis, which is a standard practice in computational biology and bioinformatics. The model is scientifically sound.\n- **Well-Posed**: The problem asks for the maximum-likelihood estimate of a parameter in a generalized linear model. For the provided datasets in Case A and Case B, the data are not completely separable (i.e., for predictor combinations with multiple samples, both $y=0$ and $y=1$ outcomes are present), which ensures the existence of a unique and finite MLE. For Case C, the problem statement provides a clear rule to handle the non-identifiability that arises from a degenerate design matrix, making the problem well-posed in all cases.\n- **Objective**: The problem is articulated using precise mathematical language, with clearly defined terms and objective data.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, ambiguity, or contradiction. All data and conditions required for a solution are provided.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be derived and implemented.\n\nThe core of the problem is to find the maximum-likelihood estimate for the parameters $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^T$ of the specified logistic regression model.\n\nThe probability that $y_i=1$ is given by the sigmoid function, $\\sigma(\\eta_i)$:\n$$\np_i = \\mathbb{P}(y_i=1 \\mid \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\n$$\nwhere $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$ is the linear predictor. The design vector for the $i$-th individual is $\\mathbf{x}_i = [1, S_{1,i}, S_{2,i}, S_{1,i} \\cdot S_{2,i}]^T$.\n\nThe likelihood of the observed data, assuming independence, is the product of the Bernoulli probabilities for each observation:\n$$\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}\n$$\nMaximizing the likelihood is equivalent to maximizing the log-likelihood, $\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta})$:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\nBy substituting $p_i = e^{\\eta_i} / (1+e^{\\eta_i})$ and $1-p_i = 1 / (1+e^{\\eta_i})$, the log-likelihood function simplifies to:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\eta_i - \\log(1 + e^{\\eta_i}) \\right] = \\sum_{i=1}^n \\left[ y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) - \\log(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) \\right]\n$$\nThere is no closed-form solution for $\\boldsymbol{\\beta}$ that maximizes $\\ell(\\boldsymbol{\\beta})$. The solution must be found numerically. The log-likelihood function for a logistic regression model is globally concave, which guarantees that a unique maximum exists if the data are not perfectly separated. We can find this maximum by minimizing the negative log-likelihood, which is a convex function. A standard and efficient method for this is a quasi-Newton algorithm such as BFGS (Broyden–Fletcher–Goldfarb–Shanno), which we will employ using `scipy.optimize.minimize`. This requires the gradient of the objective function.\n\nThe objective function to minimize is $f(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta})$. Its gradient, which is the negative of the log-likelihood gradient, is:\n$$\n\\nabla f(\\boldsymbol{\\beta}) = -\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -\\sum_{i=1}^n (y_i - p_i) \\mathbf{x}_i = \\sum_{i=1}^n (p_i - y_i) \\mathbf{x}_i = \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y})\n$$\nwhere $\\mathbf{X}$ is the $n \\times 4$ design matrix, $\\mathbf{y}$ is the vector of outcomes, and $\\mathbf{p}$ is the vector of probabilities.\n\nFor each case, we proceed as follows:\n\n**Case A**:\nThe dataset consists of $n=20$ individual samples. We construct the $20 \\times 4$ design matrix $\\mathbf{X}$ and the $20 \\times 1$ response vector $\\mathbf{y}$ directly from the given data. The four columns of $\\mathbf{X}$ correspond to the intercept ($1$), $S_1$, $S_2$, and the interaction product $S_1 \\cdot S_2$. We then numerically minimize the negative log-likelihood using an initial guess of $\\boldsymbol{\\beta} = \\mathbf{0}$ to find the MLE vector $\\hat{\\boldsymbol{\\beta}}$. The desired feature is the fourth component of this vector, $\\hat{\\beta}_3$.\n\n**Case B**:\nThe dataset consists of $n=40$ samples, aggregated into four groups based on $(S_1, S_2)$ values. This structure allows for a more computationally efficient formulation of the log-likelihood. Let $j$ index the four unique groups, $N_j$ be the number of individuals in group $j$, and $k_j$ be the number of cases ($y=1$) in that group. The unique design vectors are $\\mathbf{x}_j$. The log-likelihood is:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{j=1}^4 \\left[ k_j (\\mathbf{x}_j^T \\boldsymbol{\\beta}) - N_j \\log(1 + e^{\\mathbf{x}_j^T \\boldsymbol{\\beta}}) \\right]\n$$\nThe gradient is similarly aggregated:\n$$\n\\nabla f(\\boldsymbol{\\beta}) = \\sum_{j=1}^4 (N_j p_j - k_j) \\mathbf{x}_j\n$$\nWe form the $4 \\times 4$ matrix of unique design vectors and the corresponding vectors for $N_j$ and $k_j$. The optimization then proceeds as in Case A to find $\\hat{\\boldsymbol{\\beta}}$, and we extract $\\hat{\\beta}_3$.\n\n**Case C**:\nThe dataset has $S_{2,i} = 0$ for all $i=1, \\dots, 8$. Consequently, the interaction term $S_{1,i} \\cdot S_{2,i}$ is also $0$ for all individuals. The column in the design matrix corresponding to the interaction coefficient $\\beta_3$ is a vector of zeros. This creates a non-identifiability issue, as any value of $\\beta_3$ would yield the same likelihood. The problem provides an explicit convention for this scenario: the engineered interaction feature is defined to be $0$. Therefore, for this case, $\\hat{\\beta}_3 = 0$ by definition, and no computation is necessary.\n\nThe implementation will use `scipy.optimize.minimize` with the 'BFGS' method, providing both the objective function (negative log-likelihood) and its Jacobian (gradient) for efficiency and accuracy. To ensure numerical stability when computing $\\log(1+e^{\\eta})$ and the sigmoid function, functions from the `scipy.special` module (`logsumexp` and `expit`) will be utilized.",
            "answer": "```python\nimport numpy as np\nimport scipy.optimize\n\ndef solve():\n    \"\"\"\n    Solves the logistic regression problem for three test cases and computes the\n    engineered interaction feature (MLE of beta_3) for each.\n    \"\"\"\n\n    # --- Case A ---\n    data_A = [\n        (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 1),\n        (1, 0, 0), (1, 0, 0), (1, 0, 1), (1, 0, 0), (1, 0, 1),\n        (0, 1, 0), (0, 1, 1), (0, 1, 0), (0, 1, 0), (0, 1, 1),\n        (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 1),\n    ]\n    S1_A = np.array([d[0] for d in data_A])\n    S2_A = np.array([d[1] for d in data_A])\n    y_A = np.array([d[2] for d in data_A])\n    X_A = np.c_[np.ones(len(S1_A)), S1_A, S2_A, S1_A * S2_A]\n    \n    beta_A = _fit_logistic_regression(X_A, y_A)\n    beta3_A = beta_A[3]\n\n    # --- Case B ---\n    # The data can be aggregated for efficiency\n    # (S1, S2): {N: total count, k: count with y=1}\n    data_B_grouped = {\n        (0, 0): {'N': 10, 'k': 3},\n        (1, 0): {'N': 10, 'k': 5},\n        (0, 1): {'N': 10, 'k': 5},\n        (1, 1): {'N': 10, 'k': 7},\n    }\n    X_unique_B = []\n    N_B, k_B = [], []\n    for (s1, s2), counts in data_B_grouped.items():\n        X_unique_B.append([1, s1, s2, s1 * s2])\n        N_B.append(counts['N'])\n        k_B.append(counts['k'])\n    \n    X_B_agg = np.array(X_unique_B)\n    N_B_agg = np.array(N_B)\n    k_B_agg = np.array(k_B)\n\n    beta_B = _fit_logistic_regression(X_B_agg, k_B_agg, N=N_B_agg, grouped=True)\n    beta3_B = beta_B[3]\n\n    # --- Case C ---\n    # For this case, S2 is always 0. The interaction term S1*S2 is always 0.\n    # The problem statement defines the engineered feature to be 0 in this scenario.\n    beta3_C = 0.0\n\n    results = [beta3_A, beta3_B, beta3_C]\n    \n    # Format the output as specified\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\n\ndef _fit_logistic_regression(X, y, N=None, grouped=False):\n    \"\"\"\n    Fits a logistic regression model and returns the estimated coefficients.\n    Can handle both individual and grouped data.\n    \"\"\"\n    from scipy.special import expit\n\n    num_features = X.shape[1]\n    beta_initial = np.zeros(num_features)\n\n    if grouped:\n        # For grouped data\n        def objective(beta, X_grp, k_grp, N_grp):\n            eta = X_grp @ beta\n            # Numerically stable calculation of log(1 + exp(eta))\n            log_1_plus_exp_eta = np.logaddexp(0, eta)\n            neg_log_likelihood = -np.sum(k_grp * eta - N_grp * log_1_plus_exp_eta)\n            return neg_log_likelihood\n\n        def jacobian(beta, X_grp, k_grp, N_grp):\n            eta = X_grp @ beta\n            p = expit(eta)\n            grad = X_grp.T @ (N_grp * p - k_grp)\n            return grad\n        \n        args = (X, y, N)\n\n    else:\n        # For individual data\n        def objective(beta, X_ind, y_ind):\n            eta = X_ind @ beta\n            log_1_plus_exp_eta = np.logaddexp(0, eta)\n            neg_log_likelihood = -np.sum(y_ind * eta - log_1_plus_exp_eta)\n            return neg_log_likelihood\n\n        def jacobian(beta, X_ind, y_ind):\n            eta = X_ind @ beta\n            p = expit(eta)\n            grad = X_ind.T @ (p - y_ind)\n            return grad\n        \n        args = (X, y)\n\n    result = scipy.optimize.minimize(\n        fun=objective,\n        x0=beta_initial,\n        args=args,\n        method='BFGS',\n        jac=jacobian\n    )\n\n    if not result.success:\n        raise RuntimeError(f\"Optimization failed: {result.message}\")\n\n    return result.x\n\nsolve()\n\n```"
        }
    ]
}