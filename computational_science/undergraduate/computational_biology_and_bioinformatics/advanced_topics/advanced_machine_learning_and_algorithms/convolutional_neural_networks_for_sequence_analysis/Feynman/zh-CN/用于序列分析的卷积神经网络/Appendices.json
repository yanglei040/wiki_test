{
    "hands_on_practices": [
        {
            "introduction": "要真正理解一个复杂的模型，最好的方法莫过于亲手构建它。本练习将引导你逐步完成一个卷积神经网络（CNN）对给定 DNA 序列进行预测的全过程。通过亲手实现卷积、激活函数（ReLU）和池化等核心操作，你将揭开 CNN 的“黑箱”，并对其工作机制建立具体而深刻的理解。我们将此过程置于一个实际的生物信息学情境中——预测单核苷酸多态性（SNP）对模型输出的影响，这使得练习不仅富有挑战性，更具现实意义。",
            "id": "2382374",
            "problem": "给定一个小型的、确定性的用于脱氧核糖核酸（DNA）序列分析的卷积神经网络（CNN），以及一个通过改变独热编码输入中的一个位置来模拟单核苷酸多态性（SNP）的协议。您的任务是实现精确的数学流程，并为一组给定的序列和突变测试用例，计算每个SNP引起的模型预测值的有符号变化。您的程序必须以严格指定的格式，将最终结果作为单行输出。\n\n输入域是基于字母表 $\\{A,C,G,T\\}$ 的DNA序列集合。每个长度为 $L$ 的序列通过独热编码表示为一个矩阵 $X \\in \\mathbb{R}^{L \\times 4}$，其映射关系为 $A \\mapsto (1,0,0,0)$，$C \\mapsto (0,1,0,0)$，$G \\mapsto (0,0,1,0)$，$T \\mapsto (0,0,0,1)$。单核苷酸多态性通过将序列中某个确切位置 $p$（0-基索引）的原始碱基更改为另一个目标碱基来模拟，然后重新编码以获得突变后的矩阵 $X' \\in \\mathbb{R}^{L \\times 4}$。\n\n卷积层使用 $F$ 个滤波器执行一维互相关（卷积神经网络中使用的常规操作），步长为 $1$，边界为“valid”（无填充）。设 $K$ 为核宽度，$C=4$ 为输入通道数。对于权重为 $W^{(f)} \\in \\mathbb{R}^{K \\times C}$、偏置为 $b^{(f)} \\in \\mathbb{R}$ 的滤波器 $f \\in \\{0,1,\\dots,F-1\\}$，以及输出位置 $i \\in \\{0,1,\\dots,L-K\\}$，其预激活值为\n$$\nz^{(f)}[i] \\;=\\; \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\, X[i+k,c] \\;+\\; b^{(f)}.\n$$\n非线性函数是修正线性单元（ReLU），定义为\n$$\n\\mathrm{ReLU}(u) \\;=\\; \\max(0,u).\n$$\n由此得到激活值\n$$\na^{(f)}[i] \\;=\\; \\mathrm{ReLU}\\!\\left(z^{(f)}[i]\\right).\n$$\n然后对每个滤波器独立应用全局最大池化，\n$$\nm^{(f)} \\;=\\; \\max_{i} \\, a^{(f)}[i] \\, ,\n$$\n生成一个向量 $m \\in \\mathbb{R}^{F}$。最后，一个全连接线性读出层产生标量预测值\n$$\ny \\;=\\; \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\, m^{(f)} \\;+\\; b^{\\mathrm{fc}} \\, .\n$$\n\n为您提供一个具有 $F=2$ 个滤波器和核宽度 $K=3$ 的特定网络，其权重和偏置如下。所有未标明的条目均为零。\n- 滤波器 $f=0$（检测模式 $A\\!C\\!G$）：\n  $$\n  W^{(0)} \\;=\\; \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 0\n  \\end{bmatrix}, \\quad b^{(0)} \\;=\\; -2.5 \\, .\n  $$\n- 滤波器 $f=1$（检测模式 $T\\!T\\!T$）：\n  $$\n  W^{(1)} \\;=\\; \\begin{bmatrix}\n  0 & 0 & 0 & 1 \\\\\n  0 & 0 & 0 & 1 \\\\\n  0 & 0 & 0 & 1\n  \\end{bmatrix}, \\quad b^{(1)} \\;=\\; -2.5 \\, .\n  $$\n- 全连接读出层：\n  $$\n  w^{\\mathrm{fc}} \\;=\\; \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}, \\quad b^{\\mathrm{fc}} \\;=\\; 0.1 \\, .\n  $$\n\n对于每个测试用例，计算预测值的有符号变化\n$$\n\\Delta \\;=\\; y_{\\mathrm{mut}} \\;-\\; y_{\\mathrm{ref}} \\, ,\n$$\n其中 $y_{\\mathrm{ref}}$ 是原始序列的预测值，$y_{\\mathrm{mut}}$ 是应用SNP后的预测值。位置使用0-基索引。序列只包含字符 $A$、$C$、$G$、$T$。每个SNP都将一个碱基更改为另一个不同的碱基。\n\n需要实现和评估的测试套件：\n- 案例1：序列 $S = \\text{\"AACGTTGA\"}$，位置 $p = 2$，新碱基 $B' = \\text{\"T\"}$。\n- 案例2：序列 $S = \\text{\"TTTACGTT\"}$，位置 $p = 0$，新碱基 $B' = \\text{\"G\"}$。\n- 案例3：序列 $S = \\text{\"ACG\"}$，位置 $p = 1$，新碱基 $B' = \\text{\"G\"}$。\n- 案例4：序列 $S = \\text{\"ACGACG\"}$，位置 $p = 1$，新碱基 $B' = \\text{\"T\"}$。\n\n您的程序必须：\n- 完全按照上述规定实现独热编码、卷积互相关、修正线性单元、全局最大池化和全连接读出层。\n- 为每个测试用例计算所定义的 $\\Delta$。\n- 将每个 $\\Delta$ 四舍五入到6位小数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按测试用例顺序排列、用逗号分隔并用方括号括起来的结果，例如 `[Δ_1,Δ_2,Δ_3,Δ_4]`。每个值必须使用标准四舍五入规则保留6位小数（例如，$0.5$ 变为 $\\text{\"0.500000\"}$）。不应打印任何其他文本。",
            "solution": "问题陈述已经过验证，并被认定为有效。它在科学上基于计算生物学和深度学习的原理，提出了一个具备所有必要参数且定义明确的数学任务，并以客观、无歧义的语言进行阐述。所描述的卷积神经网络的所有组成部分——独热编码、卷积、ReLU激活、最大池化和全连接层——都是该领域的标准元素。计算单核苷酸多态性（SNP）对模型输出的影响，是一项标准的*in silico*（计算机模拟）灵敏度分析技术。因此，我们着手提供一个完整的解决方案。\n\n该问题要求计算一个确定性神经网络输出的变化量 $\\Delta = y_{\\mathrm{mut}} - y_{\\mathrm{ref}}$，该变化由输入DNA序列中的单点突变引起。这通过为参考序列（$S_{\\mathrm{ref}}$）和突变序列（$S_{\\mathrm{mut}}$）实现网络的前向传播，以获得它们各自的标量预测值 $y_{\\mathrm{ref}}$ 和 $y_{\\mathrm{mut}}$ 来实现。\n\n计算流程的结构如下：\n\n**1. 输入编码**\n一个长度为 $L$ 的DNA序列通过独热编码被转换成一个数值矩阵 $X \\in \\mathbb{R}^{L \\times 4}$。每个核苷酸被映射到一个唯一的4维二进制向量，从而建立一个标准基：$A \\mapsto [1, 0, 0, 0]$，$C \\mapsto [0, 1, 0, 0]$，$G \\mapsto [0, 0, 1, 0]$，以及 $T \\mapsto [0, 0, 0, 1]$。\n\n**2. 卷积层**\n该层用于检测序列中的局部模式。它由 $F=2$ 个滤波器组成，每个滤波器的核宽度为 $K=3$，权重为 $W^{(f)} \\in \\mathbb{R}^{3 \\times 4}$。其操作是一维互相关，步长为 $1$，无填充（“valid”卷积）。对于每个滤波器 $f$ 和输入序列中每个可能的起始位置 $i$（其中 $i \\in \\{0, 1, \\dots, L-K\\}$），会计算出一个预激活值 $z^{(f)}[i]$。该值是滤波器权重 $W^{(f)}$ 与输入矩阵相应 $K \\times 4$ 切片 $X[i:i+K, :]$ 的逐元素乘积之和，再加上一个特定于该滤波器的偏置 $b^{(f)}$。其公式为：\n$$\nz^{(f)}[i] = \\left( \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\cdot X[i+k,c] \\right) + b^{(f)}\n$$\n所提供的滤波器权重被设计用来检测基序 \"ACG\" ($f=0$) 和 \"TTT\" ($f=1$)。序列窗口与滤波器权重中编码的基序完美匹配时，点积和为 $3$，加上偏置 $b^{(f)} = -2.5$后，得到预激活值为 $0.5$。\n\n**3. 激活函数**\n预激活值通过一个非线性激活函数，即修正线性单元（ReLU），其定义为 $\\mathrm{ReLU}(u) = \\max(0, u)$。该函数引入了非线性，使模型能够学习更复杂的模式。它有效地将任何负的预激活值设为零，这意味着只有当输入基序提供足够强的匹配以克服负偏置时，滤波器才会“激活”。其激活值为：\n$$\na^{(f)}[i] = \\mathrm{ReLU}(z^{(f)}[i])\n$$\n\n**4. 全局最大池化层**\n对于每个滤波器的激活图 $a^{(f)}$，通过取所有位置 $i$ 上的最大值来提取单个特征值 $m^{(f)}$。这被称为全局最大池化。\n$$\nm^{(f)} = \\max_{i} a^{(f)}[i]\n$$\n该操作使模型的预测对所检测基序的位置具有不变性，并将表示的维度降低到一个固定大小的向量 $m \\in \\mathbb{R}^{F}$，而与输入序列的长度 $L$ 无关。如果没有位置产生正激活值，则最大值为 $0$。\n\n**5. 全连接读出层**\n最终的预测值，一个标量 $y$，是池化特征值 $m^{(f)}$ 的线性组合，通过全连接层的权重 $w^{\\mathrm{fc}}$ 加权，再加上一个最终的偏置项 $b^{\\mathrm{fc}}$ 计算得出。\n$$\ny = \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\cdot m^{(f)} + b^{\\mathrm{fc}}\n$$\n\n**计算示例：案例1**\n我们来逐步计算案例1：$S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$，位置 $p=2$，新碱基 $B'=\\text{\"T\"}$。突变后的序列是 $S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$。\n\n**参考预测值（$y_{\\mathrm{ref}}$）：**\n- 序列：$S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$ ($L=8$)。\n- 卷积窗口 ($K=3$)：\"AAC\"、\"ACG\"、\"CGT\"、\"GTT\"、\"TTG\"、\"TGA\"。\n- **滤波器0（\"ACG\"）：** 位置 $i=1$ 处的窗口 \"ACG\" 完美匹配。点积为 $3$。$z^{(0)}[1] = 3 - 2.5 = 0.5$。所有其他窗口的点积都小于 $2.5$，因此它们的预激活值为负。激活图为 $a^{(0)}_{\\mathrm{ref}} = [0, 0.5, 0, 0, 0, 0]$。\n- **滤波器1（\"TTT\"）：** 没有窗口能够完美匹配 \"TTT\"。最高的点积来自 \"GTT\" 和 \"TTG\"（得分为 $2$），产生的预激活值为 $2 - 2.5 = -0.5$。因此，该滤波器的所有激活值均为 $0$。激活图为 $a^{(1)}_{\\mathrm{ref}} = [0, 0, 0, 0, 0, 0]$。\n- **最大池化：** $m^{(0)}_{\\mathrm{ref}} = \\max(a^{(0)}_{\\mathrm{ref}}) = 0.5$。$m^{(1)}_{\\mathrm{ref}} = \\max(a^{(1)}_{\\mathrm{ref}}) = 0$。池化后的特征向量为 $m_{\\mathrm{ref}} = [0.5, 0]$。\n- **读出：** $y_{\\mathrm{ref}} = (1.0 \\cdot 0.5) + (-0.5 \\cdot 0) + 0.1 = 0.5 + 0 + 0.1 = 0.6$。\n\n**突变后预测值（$y_{\\mathrm{mut}}$）：**\n- 序列：$S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$。位置 $p=2$ 处的突变将 \"C\" 更改为 \"T\"。\n- 原始的 \"ACG\" 基序被破坏。受影响的新窗口是 \"AAT\" ($i=0$)、\"ATG\" ($i=1$) 和 \"TGT\" ($i=2$)。\n- **滤波器0（\"ACG\"）：** 现在来自任何窗口的最高点积是 $2$（来自 \"ATG\"），产生预激活值为 $2 - 2.5 = -0.5$。所有激活值均为 $0$。$a^{(0)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$。\n- **滤波器1（\"TTT\"）：** 突变并未创建 \"TTT\" 基序。最高点积为 $2$（来自 \"TGT\"、\"GTT\"、\"TTG\"），产生预激活值为 $-0.5$。所有激活值均为 $0$。$a^{(1)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$。\n- **最大池化：** $m^{(0)}_{\\mathrm{mut}} = 0$。$m^{(1)}_{\\mathrm{mut}} = 0$。池化后的特征向量为 $m_{\\mathrm{mut}} = [0, 0]$。\n- **读出：** $y_{\\mathrm{mut}} = (1.0 \\cdot 0) + (-0.5 \\cdot 0) + 0.1 = 0.1$。\n\n**案例1的最终计算：**\n预测值的变化为 $\\Delta_1 = y_{\\mathrm{mut}} - y_{\\mathrm{ref}} = 0.1 - 0.6 = -0.5$。\n\n对所有测试用例应用此精确过程，结果四舍五入至6位小数。所提供的程序实现了这一逻辑。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the signed change in a CNN's prediction due to single nucleotide polymorphisms.\n    \"\"\"\n    # Define the fixed network parameters\n    params = {\n        'W0': np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=float),\n        'b0': -2.5,\n        'W1': np.array([[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]], dtype=float),\n        'b1': -2.5,\n        'w_fc': np.array([1.0, -0.5], dtype=float),\n        'b_fc': 0.1\n    }\n    \n    # One-hot encoding mapping\n    one_hot_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_channels = 4\n    kernel_width = 3\n\n    def get_prediction(sequence: str) -> float:\n        \"\"\"\n        Performs a full forward pass of the CNN for a given DNA sequence.\n        \"\"\"\n        seq_len = len(sequence)\n        \n        # Handle sequences shorter than the kernel\n        if seq_len  kernel_width:\n             # No convolution is possible, so all activations are zero by default\n             m = np.array([0.0, 0.0])\n             y = np.sum(m * params['w_fc']) + params['b_fc']\n             return y\n        \n        # 1. One-hot encode the sequence\n        X = np.zeros((seq_len, num_channels), dtype=float)\n        for i, base in enumerate(sequence):\n            if base in one_hot_map:\n                X[i, one_hot_map[base]] = 1.0\n\n        # Define filters and biases\n        filters = [(params['W0'], params['b0']), (params['W1'], params['b1'])]\n        pooled_features = []\n\n        for W_f, b_f in filters:\n            # 2. Convolutional Layer (pre-activations)\n            conv_len = seq_len - kernel_width + 1\n            pre_activations = np.zeros(conv_len, dtype=float)\n            for i in range(conv_len):\n                window = X[i : i + kernel_width, :]\n                pre_activations[i] = np.sum(window * W_f) + b_f\n            \n            # 3. ReLU Activation\n            activations = np.maximum(0, pre_activations)\n            \n            # 4. Global Max Pooling\n            # np.max on an empty array raises error. If activations is empty, this\n            # means conv_len was 0. In this case max is 0.\n            if activations.size > 0:\n                max_activation = np.max(activations)\n            else:\n                max_activation = 0.0\n            \n            pooled_features.append(max_activation)\n        \n        m = np.array(pooled_features)\n        \n        # 5. Fully Connected Readout\n        y = np.sum(m * params['w_fc']) + params['b_fc']\n        \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"AACGTTGA\", 2, \"T\"),\n        (\"TTTACGTT\", 0, \"G\"),\n        (\"ACG\", 1, \"G\"),\n        (\"ACGACG\", 1, \"T\"),\n    ]\n\n    results = []\n    for seq_ref, p, new_base in test_cases:\n        # Get prediction for reference sequence\n        y_ref = get_prediction(seq_ref)\n        \n        # Create mutated sequence\n        seq_list = list(seq_ref)\n        seq_list[p] = new_base\n        seq_mut = \"\".join(seq_list)\n        \n        # Get prediction for mutated sequence\n        y_mut = get_prediction(seq_mut)\n        \n        # Compute the change and round\n        delta = y_mut - y_ref\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了 CNN 如何处理数据之后，一个自然而然的问题是：我们应该给它“喂”什么样的数据？这个练习探讨了生物信息学中的一个关键决策点：如何表示生物序列。本练习将对比两种常见的编码方式：为每个氨基酸提供唯一标识的独热编码，以及基于其理化性质（如疏水性、电荷和极性）的特征编码。通过分析这两种方法的优劣，你将学会批判性地思考不同输入表示对模型参数效率、数据标准化的需求，以及模型泛化能力的影响。",
            "id": "2382354",
            "problem": "您正在使用卷积神经网络 (CNN) 设计一个蛋白质序列分类器。每个蛋白质都是一个由标准氨基酸集合构成的序列。您考虑对每个残基使用两种输入编码方式：(i) 对 $20$ 种经典氨基酸进行独热编码 (one-hot encoding)，以及 (ii) 一个由三个理化性质（疏水性 $h$、净电荷 $q$ 和极性 $p$）组成的向量，因此每个位置都由一个 $\\mathbb{R}^{3}$ 中的向量表示。该 CNN 的起始部分是一个单层的一维卷积层，其卷积核宽度 $k=5$，步长为 $1$，无填充，并有 $F=64$ 个滤波器，之后再接一个点态非线性（pointwise nonlinearity）操作。假设输入序列的长度为 $L$（一个正整数），并忽略任何下游层。除非另有说明，否则将卷积核视为仅包含其权重，而偏置项则分开考虑。\n\n选择所有正确的选项。\n\nA. 不考虑偏置项，对于独热编码输入，第一个卷积层中可训练权重的总数是 $k \\times 20 \\times F$；对于性质输入，总数是 $k \\times 3 \\times F$。因此，当 $k=5$ 且 $F=64$ 时，性质输入正好使用 $960$ 个卷积核权重，而独热编码输入使用 $6400$ 个。\n\nB. 如果将三个性质通道 $(h,q,p)$ 作为可能具有不同尺度的原始数值输入，方差最大的通道可能会在第一个卷积层的学习动态中占据主导地位；将每个通道标准化为零均值和单位方差有助于减轻这种影响。\n\nC. 由于性质编码是根据理化相似性对氨基酸进行聚合的，因此在这种编码下，宽度为 $5$ 的卷积无法检测到位置特异性基序（position-specific motifs），而在独热编码下则可以。\n\nD. 对于一组固定的、已学习的卷积权重，改变三个性质通道的顺序（例如，交换 $h$ 和 $p$）不会改变卷积的输出。\n\nE. 与独热编码相比，性质编码可以使单个学习到的滤波器对那些保持了理化相似性的、未见过的氨基酸替换产生相似的响应，从而可能提高模型对训练期间未见过的变体的泛化能力。",
            "solution": "问题陈述是对计算生物学中应用卷积神经网络 (CNN) 进行蛋白质序列分析的一个标准设置的有效描述。该问题具有科学依据，提法得当，客观且信息完整，提供了足够的信息来评估所给选项。独热编码和理化性质这两种编码方法是标准的表示方法，而所述的 CNN 架构对于序列处理任务也是典型的。\n\n让我们分析一下输入和卷积层的结构。输入是一个长度为 $L$ 的序列。序列中的每个元素都是一个代表氨基酸的向量。\n对于独热编码，每个氨基酸由一个维度为 $C_1 = 20$ 的向量表示。因此，如果采用（长度，通道数）的格式，输入张量的形状为 $(L, 20)$。\n对于理化性质编码，每个氨基酸由一个维度为 $C_2 = 3$ 的向量表示。因此，输入张量的形状为 $(L, 3)$。\n\n第一个层是一个具有以下参数的一维卷积层：\n- 卷积核宽度，$k=5$\n- 步长，$S=1$\n- 滤波器数量（输出通道数），$F=64$\n- 填充，$P=0$\n\n在一维卷积中，单个滤波器的权重维度为（卷积核宽度，输入通道数）。由于有 $F$ 个这样的滤波器，该层的总权重张量维度为 $(F, k, C)$，其中 $C$ 是输入通道数。卷积核中可训练权重的总数（如题所述，不包括偏置项）是这些维度的乘积：$F \\times k \\times C$。\n\n现在，我们来评估每个选项。\n\n**A. 不考虑偏置项，对于独热编码输入，第一个卷积层中可训练权重的总数是 $k \\times 20 \\times F$；对于性质输入，总数是 $k \\times 3 \\times F$。因此，当 $k=5$ 且 $F=64$ 时，性质输入正好使用 $960$ 个卷积核权重，而独热编码输入使用 $6400$ 个。**\n\n可训练权重的数量由公式 $F \\times k \\times C$ 给出。\n- 对于独热编码，输入通道数 $C_1 = 20$。权重总数为 $F \\times k \\times C_1 = 64 \\times 5 \\times 20 = 320 \\times 20 = 6400$。由于乘法满足交换律，所给公式 $k \\times 20 \\times F$ 是等价的。\n- 对于理化性质编码，输入通道数 $C_2 = 3$。权重总数为 $F \\times k \\times C_2 = 64 \\times 5 \\times 3 = 320 \\times 3 = 960$。所给公式 $k \\times 3 \\times F$ 也是等价的。\n\n该陈述基于给定的参数 $k=5$ 和 $F=64$ 提供了正确的公式和数值结果。\n**结论：正确。**\n\n**B. 如果将三个性质通道 $(h,q,p)$ 作为可能具有不同尺度的原始数值输入，方差最大的通道可能会在第一个卷积层的学习动态中占据主导地位；将每个通道标准化为零均值和单位方差有助于减轻这种影响。**\n\n该陈述阐述了训练机器学习模型（包括神经网络）的一项基本原则。卷积操作是输入与学习权重的线性组合。如果输入特征（此处为通道 $h$、$q$ 和 $p$）的数值范围或方差差异巨大，那么值较大的特征将对卷积层的输出以及反向传播期间计算的梯度产生不成比例的巨大影响。这可能导致训练不稳定，并可能使模型主要从高方差通道中学习，而忽略其他通道中可能存在的有用信息。将输入特征（在本例中，是在整个数据集上对每个通道）标准化为均值为 $0$、标准差为 $1$，是一种标准且强烈推荐的预处理步骤。这可以确保所有通道处于可比较的尺度上，通常会带来更稳定、更高效的训练。\n**结论：正确。**\n\n**C. 由于性质编码是根据理化相似性对氨基酸进行聚合的，因此在这种编码下，宽度为 $5$ 的卷积无法检测到位置特异性基序（position-specific motifs），而在独热编码下则可以。**\n\n卷积操作在其感受野内本质上是位置特异性的。一个宽度为 $k=5$ 的滤波器将其 $5$ 个权重向量组成的集合应用于其所覆盖的连续 $5$ 个输入位置。对于性质编码，这 $5$ 个权重向量中的每一个的维度都是 $3$（$h, q, p$ 各有一个权重）。滤波器学习的是这 $5$ 个位置上理化性质的特定模式。例如，它可能学习检测窗口中位置 $1$ 的高疏水性区域，然后是位置 $3$ 的正电荷。这是一种理化性质的“位置特异性基序”。\n\n声称它“无法检测”此类基序是错误的。实际情况是，这种编码使得区分两种恰好具有相同或非常相似的理化向量的不同氨基酸变得不可能。而独热编码是每种氨基酸的唯一标识符，不存在这种模糊性，并允许滤波器学习基于特定氨基酸身份的基序（例如，位置 $1$ 是“丙氨酸”，而不仅仅是“一个小的疏水氨基酸”）。然而，无法区分某些氨基酸并不等同于无法检测位置特异性模式。卷积算子就其本质而言，就是检测其输入中的局部、位置特异性模式。\n**结论：不正确。**\n\n**D. 对于一组固定的、已学习的卷积权重，改变三个性质通道的顺序（例如，交换 $h$ 和 $p$）不会改变卷积的输出。**\n\n设序列位置 $j$ 的输入向量为 $\\mathbf{x}_j = [x_{j,h}, x_{j,q}, x_{j,p}]^T$，表示 $(h,q,p)$ 的值。设单个滤波器在其卷积核内位置 $m$（$m \\in \\{0, 1, ..., 4\\}$）的权重为 $\\mathbf{w}_m = [w_{m,h}, w_{m,q}, w_{m,p}]^T$。序列中位置 $j=i+m$ 对卷积输出的贡献是点积 $\\mathbf{w}_m \\cdot \\mathbf{x}_j = w_{m,h}x_{j,h} + w_{m,q}x_{j,q} + w_{m,p}x_{j,p}$。在位置 $i$ 处，一个滤波器的总输出是在卷积核窗口上对这些点积求和：$y_i = \\sum_{m=0}^{4} \\mathbf{w}_m \\cdot \\mathbf{x}_{i+m}$。\n\n如果我们排列输入通道的顺序，例如交换 $h$ 和 $p$，新的输入向量将是 $\\mathbf{x}'_j = [x_{j,p}, x_{j,q}, x_{j,h}]^T$。使用相同的固定权重 $\\mathbf{w}_m$，新输出的贡献形式为 $\\mathbf{w}_m \\cdot \\mathbf{x}'_j = w_{m,h}x_{j,p} + w_{m,q}x_{j,q} + w_{m,p}x_{j,h}$。\n通常情况下，$w_{m,h}x_{j,h} + w_{m,p}x_{j,p} \\neq w_{m,h}x_{j,p} + w_{m,p}x_{j,h}$。\n只有在满足一些无关紧要的条件时，例如对所有相关的 $j$ 都有 $x_{j,h} = x_{j,p}$，或者学习到的权重具有特定的对称性，例如对所有 $m$ 都有 $w_{m,h} = w_{m,p}$ 时，输出才会保持不变。这两种情况都不能被假定。卷积操作对其输入通道的排列不是不变的，因为每个通道都由一组不同的权重处理。\n**结论：不正确。**\n\n**E. 与独热编码相比，性质编码可以使单个学习到的滤波器对那些保持了理化相似性的、未见过的氨基酸替换产生相似的响应，从而可能提高模型对训练期间未见过的变体的泛化能力。**\n\n该陈述描述了由编码选择引入的归纳偏置（inductive bias）的概念。独热编码将每个氨基酸表示为一个正交向量。模型没有关于哪些氨基酸相似的先验信息。例如，在这种表示中，生物化学上相似的氨基酸 Leucine (L) 和 Isoleucine (I) 之间的区别，与 Leucine (L) 和 Aspartic Acid (D) 之间的区别一样大。一个经过训练用于识别包含 L 的基序的滤波器，不会自动对 L 被 I 替换的相同基序产生响应。\n\n相比之下，理化性质编码是一种密集表示，其中相似的氨基酸被映射到 $\\mathbb{R}^3$ 空间中的邻近点。Leucine 和 Isoleucine 将具有非常相似的 $(h,q,p)$ 向量。一个学习识别性质模式（例如，“高度疏水的残基”）的滤波器，将对该位置的 L 和 I 产生相似的响应。这使得模型能够泛化。如果训练数据中包含一个带有 Leucine 的功能位点，模型可以学习其性质，并正确预测一个带有 Isoleucine（训练中未见过）的变体也是一个功能位点。这是使用基于特征的编码的一个关键动机，可以提高模型在新、未见过数据上的性能。\n**结论：正确。**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "理解了前向传播和输入表示后，我们来深入探讨模型本身的学习过程。这个高级练习将向你介绍正则化（$L_1$ 和 $L_2$）这一强大的技术，它不仅能控制模型复杂度以防止过拟合，在生物信息学中，它更是一种促使模型在卷积核中学习到稀疏、可解释模式（即“基序”）的重要工具。通过从零开始实现梯度下降和近端梯度下降算法，你将亲身体验我们如何引导 CNN 学习具有生物学意义的特征，并将抽象的数学理论付诸实践。",
            "id": "2382359",
            "problem": "你的任务是编写一个完整、可运行的程序，研究正则化惩罚项的选择如何影响使用卷积神经网络（CNN）进行脱氧核糖核酸（DNA）序列分析时，单个一维卷积核所学习到的基序的稀疏性。该模型被形式化为一个带有 logistic 链接函数和平均池化的线性模型。你必须从第一性原理推导出算法，并仅使用线性代数和向量微积分来实现它。在本问题陈述中出现的所有符号、变量、函数、算子和数字都必须使用 LaTeX 数学模式表示。\n\n考虑基于字母表 $\\{A,C,G,T\\}$ 的序列，其在 $\\mathbb{R}^{4}$ 中进行 one-hot 编码。设序列长度为 $S$，核（基序）长度为 $L$。对于一个 one-hot 编码的序列 $X \\in \\{0,1\\}^{S \\times 4}$，定义在位置 $t$ 的一维有效卷积响应为\n$$\ns_t = \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b} \\, X_{t+i-1,\\,b},\n$$\n其中 $W \\in \\mathbb{R}^{L \\times 4}$ 是核权重。然后跨位置进行平均池化，形成一个标量预激活值\n$$\nz = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} s_t + b,\n$$\n其中 $b \\in \\mathbb{R}$ 是一个截距。设二进制标签 $y \\in \\{0,1\\}$ 的预测概率由 logistic 函数给出\n$$\np = \\sigma(z) = \\frac{1}{1+\\exp(-z)}.\n$$\n给定一个数据集 $\\{(X^{(n)},y^{(n)})\\}_{n=1}^{N}$，定义平均 logistic 损失为\n$$\n\\mathcal{L}(W,b) = \\frac{1}{N} \\sum_{n=1}^{N} \\left[ -y^{(n)} \\log p^{(n)} - (1-y^{(n)}) \\log(1-p^{(n)}) \\right],\n$$\n其中 $p^{(n)} = \\sigma(z^{(n)})$ 按上述方式计算。考虑仅对 $W$ 的两种正则化器：$L_1$ 惩罚\n$$\nR_{1}(W) = \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\left| W_{i,b} \\right|\n$$\n和 $L_2$ 惩罚\n$$\nR_{2}(W) = \\frac{1}{2} \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b}^{2}.\n$$\n对于正则化强度 $\\lambda \\ge 0$，要最小化的经验风险是\n$$\nF(W,b) = \\mathcal{L}(W,b) + \\lambda \\, R(W),\n$$\n其中 $R(W)$ 是 $R_{1}(W)$ 或 $R_{2}(W)$。平均池化确保了当模型用 $X$ 的所有滑动窗口的平均值表示时，其对于 $W$ 和 $b$ 是线性的，因此无正则化的部分是凸的。\n\n你的任务是：\n- 从第一性原理出发，使用链式法则推导 $\\mathcal{L}(W,b)$ 关于 $W$ 和 $b$ 的梯度，并根据池化特征的数据矩阵为梯度指定一个由 Lipschitz 常数界证明合理的步长选择。然后你必须实现两种训练过程：\n  - 对于 $L_2$ 正则化，使用梯度下降法对 $W$ 和 $b$ 进行 $T$ 步更新，步长为 $\\alpha$。\n  - 对于 $L_1$ 正则化，使用近端梯度下降法：对 $W$ 的无正则化部分执行一个梯度步，然后应用阈值为 $\\alpha \\lambda$ 的软阈值近端算子；通过无惩罚的梯度下降法更新 $b$。\n- 按如下方式生成合成数据。固定整数 $S$ 和 $L$，并通过为每个位置 $i \\in \\{1,\\dots,L\\}$ 选择一个偏好的碱基 $b_i \\in \\{1,2,3,4\\}$（对应于 $\\{A,C,G,T\\}$）来构建一个真实基序（ground-truth motif）。对于正序列，生成一个长度为 $S$ 的背景序列，其碱基是独立同分布的均匀碱基，然后选择一个起始索引 $t^{\\star} \\in \\{1, \\dots, S-L+1\\}$，并用每个偏移处的偏好碱基覆盖掉长度为 $L$ 的片段。对于负序列，生成一个长度为 $S$ 的背景序列，不覆盖任何片段。将序列编码为 $\\{0,1\\}^{S \\times 4}$ 中的 one-hot 矩阵。正样本标签为 $y=1$，负样本标签为 $y=0$。对于每个序列，计算所有长度为 $L$ 的 one-hot 片段的平均值：\n$$\n\\Phi(X) = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} X_{t:t+L,:} \\in \\mathbb{R}^{L \\times 4},\n$$\n并将其展平为 $\\mathbb{R}^{4L}$ 中的一个向量，作为线性模型的特征向量。这种特征构造方式与上述的平均池化卷积模型完全等价。\n- 对每个指定的测试案例，训练模型直至 $T$ 步。设 $\\tau0$ 是一个很小的阈值。将学习到的核的稀疏度分数定义为\n$$\n\\mathrm{spar}(W) = \\frac{1}{4L} \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\mathbf{1}\\left\\{ |W_{i,b}|  \\tau \\right\\}.\n$$\n为每个测试案例报告此值，形式为 $[0,1]$ 范围内的实数。\n\n实现要求：\n- 你必须仅使用基础线性代数以及所要求的 logistic 链接函数和近端算子来实现训练过程；不要使用任何机器学习库。\n- 使用从无正则化的 logistic 损失的特征矩阵的谱范数推导出的 Lipschitz 常数上界来选择步长 $\\alpha$。对于 $L_2$ 正则化，在你的步长界中适当地考虑其额外的平滑性。\n- 使用 $T=1000$ 个梯度步数，$\\tau=10^{-3}$，并且不要惩罚截距 $b$。\n- 所有的随机选择必须能从一个指定的整数种子进行复现。\n\n测试套件：\n- 在所有案例中，使用 $S=40$， $L=8$，以及包含 $N/2$ 个正样本和 $N/2$ 个负样本的类别均衡数据。\n- 案例 $\\mathbf{1}$ (正常路径, $L_1$): 正则化 $L_1$，$\\lambda = 0.05$，$N=600$，种子 $=7$，有信号（正样本包含基序，负样本不包含）。\n- 案例 $\\mathbf{2}$ (正常路径对照组, $L_2$): 正则化 $L_2$，$\\lambda = 0.05$，$N=600$，种子 $=7$，有信号。\n- 案例 $\\mathbf{3}$ (边缘情况, 无信号): 正则化 $L_1$，$\\lambda = 0.3$，$N=600$，种子 $=21$，无信号（两个类别都作为背景生成；前 $N/2$ 个序列的标签被指定为 $y=1$，其余 $N/2$ 个序列的标签为 $y=0$）。\n- 案例 $\\mathbf{4}$ (边界条件, 正则化消失): 正则化 $L_1$，$\\lambda = 10^{-6}$，$N=600$，种子 $=11$，有信号。\n\n最终输出规范：\n- 你的程序必须产生单行输出，其中包含案例 $\\mathbf{1}$到 $\\mathbf{4}$ 的稀疏度分数，按此顺序排列，形式为一个由方括号括起来的逗号分隔列表。每个值必须四舍五入到小数点后三位。例如，输出格式必须与此完全一样 `[v_1,v_2,v_3,v_4]`，其中每个 `v_k` 是一个小数点后有三位的小数，并且不打印任何额外文本。",
            "solution": "此问题要求实现一个数值实验，以研究 $L_1$ 与 $L_2$ 正则化在一个简化的用于 DNA 序列分析的一维卷积神经网络模型上所产生的稀疏性诱导效应。该模型等价于一个对所有子序列片段进行平均池化得到的特征所做的 logistic 回归。解决方案将从第一性原理推导得出。\n\n首先，我们将模型形式化为一个线性 logistic 回归。一个序列 $X \\in \\{0,1\\}^{S \\times 4}$ 被进行 one-hot 编码。核为 $W \\in \\mathbb{R}^{L \\times 4}$。卷积响应后接平均池化产生一个预激活值 $z$：\n$$\nz = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} \\left( \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b} X_{t+i-1,b} \\right) + b\n$$\n根据求和的线性性质，这可以表示为一个内积。令 $w = \\text{vec}(W) \\in \\mathbb{R}^{4L}$ 为核权重的展平向量。令 $\\phi(X) \\in \\mathbb{R}^{4L}$ 为对应于序列矩阵 $X$ 的所有长度为 $L$ 的 one-hot 片段的平均值的展平向量：\n$$\n\\Phi(X) = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} X_{[t:t+L-1],:} \\in \\mathbb{R}^{L \\times 4}\n$$\n且 $\\phi(X) = \\text{vec}(\\Phi(X))$。这里，$X_{[t:t+L-1],:}$ 表示 $X$ 从第 $t$ 行到第 $t+L-1$ 行的子矩阵。预激活值可以写成一个标准的线性模型：\n$$\nz = w^T \\phi(X) + b\n$$\n给定一个数据集 $\\{(\\phi^{(n)}, y^{(n)})\\}_{n=1}^N$，其中 $\\phi^{(n)} = \\phi(X^{(n)})$，预测模型为 $p^{(n)} = \\sigma(z^{(n)}) = \\sigma(w^T\\phi^{(n)} + b)$。目标是最小化正则化的 logistic 损失：\n$$\nF(w,b) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}^{(n)}(w,b) + \\lambda R(w)\n$$\n其中 $\\mathcal{L}^{(n)} = -y^{(n)} \\log p^{(n)} - (1-y^{(n)}) \\log(1-p^{(n)})$ 且 $R(w)$ 是 $w$ 的 $L_1$ 或 $L_2$ 范数。\n\n为了实现优化算法，我们需要无正则化损失 $\\mathcal{L}(w,b) = \\frac{1}{N} \\sum_n \\mathcal{L}^{(n)}$ 的梯度。使用链式法则，单个样本 $n$ 的损失关于预激活值 $z^{(n)}$ 的梯度是：\n$$\n\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial z^{(n)}} = \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial p^{(n)}} \\frac{\\partial p^{(n)}}{\\partial z^{(n)}} = \\left( -\\frac{y^{(n)}}{p^{(n)}} + \\frac{1-y^{(n)}}{1-p^{(n)}} \\right) \\cdot (p^{(n)}(1-p^{(n)})) = p^{(n)} - y^{(n)}\n$$\n$z^{(n)}$ 关于参数的梯度是 $\\nabla_w z^{(n)} = \\phi^{(n)}$ 和 $\\partial z^{(n)}/\\partial b = 1$。平均损失 $\\mathcal{L}$ 的完整梯度是：\n$$\n\\nabla_w \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} (p^{(n)} - y^{(n)}) \\phi^{(n)}\n$$\n$$\n\\nabla_b \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} (p^{(n)} - y^{(n)})\n$$\n在矩阵表示法中，令 $\\Phi$ 为 $N \\times (4L)$ 的设计矩阵，其行为 $(\\phi^{(n)})^T$，$y$ 为标签向量，$p$ 为预测向量。梯度为 $\\nabla_w \\mathcal{L} = \\frac{1}{N} \\Phi^T (p-y)$ 和 $\\nabla_b \\mathcal{L} = \\frac{1}{N} \\mathbf{1}^T (p-y)$。\n\n对于基于梯度的优化，步长 $\\alpha$ 必须仔细选择。一个稳定的 $\\alpha$ 选择与被优化函数梯度的 Lipschitz 常数成反比。对于 logistic 损失 $\\mathcal{L}(w,b)$，我们求其 Hessian 矩阵的一个上界。令 $\\theta = [w^T, b]^T$ 和 $\\tilde{\\phi}^{(n)}=[(\\phi^{(n)})^T, 1]^T$。Hessian 矩阵为：\n$$\n\\nabla^2_{\\theta} \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial (p^{(n)}-y^{(n)})}{\\partial \\theta^T} \\tilde{\\phi}^{(n)} = \\frac{1}{N} \\sum_{n=1}^{N} p^{(n)}(1-p^{(n)}) \\tilde{\\phi}^{(n)} (\\tilde{\\phi}^{(n)})^T\n$$\n因为 $p(1-p) \\le 1/4$，我们有 $\\nabla^2_{\\theta} \\mathcal{L} \\preceq \\frac{1}{4N} \\sum_{n=1}^{N} \\tilde{\\phi}^{(n)} (\\tilde{\\phi}^{(n)})^T = \\frac{1}{4N} \\tilde{\\Phi}^T \\tilde{\\Phi}$，其中 $\\tilde{\\Phi}$ 是 $N \\times (4L+1)$ 的增广设计矩阵。$\\nabla \\mathcal{L}$ 的 Lipschitz 常数 $L_{\\nabla \\mathcal{L}}$ 受此矩阵的最大特征值限制：$L_{\\nabla \\mathcal{L}} \\le \\frac{1}{4N} \\lambda_{\\text{max}}(\\tilde{\\Phi}^T \\tilde{\\Phi}) = \\frac{\\sigma_{\\text{max}}(\\tilde{\\Phi})^2}{4N}$。我们将基于此上界设置步长，$L_{bound} = \\frac{\\sigma_{\\text{max}}(\\tilde{\\Phi})^2}{4N}$。\n\n对于 $L_2$ 正则化，目标函数 $F(w,b) = \\mathcal{L}(w,b) + \\frac{\\lambda}{2} \\|w\\|_2^2$ 是光滑的。梯度为 $\\nabla_w F = \\nabla_w \\mathcal{L} + \\lambda w$ 和 $\\nabla_b F = \\nabla_b \\mathcal{L}$。此目标函数的 Hessian 矩阵为 $\\nabla^2_\\theta F = \\nabla^2_\\theta \\mathcal{L} + \\text{diag}(\\lambda, \\dots, \\lambda, 0)$。梯度 $\\nabla F$ 是 Lipschitz 连续的，其常数 $L_{\\nabla F} \\le L_{bound} + \\lambda$。梯度下降的更新步骤为：\n$$\nw_{k+1} = w_k - \\alpha (\\nabla_w \\mathcal{L}(w_k, b_k) + \\lambda w_k)\n$$\n$$\nb_{k+1} = b_k - \\alpha \\nabla_b \\mathcal{L}(w_k, b_k)\n$$\n一个安全的步长是 $\\alpha = 1/(L_{bound} + \\lambda)$。\n\n对于 $L_1$ 正则化，目标函数 $F(w,b) = \\mathcal{L}(w,b) + \\lambda \\|w\\|_1$ 是非光滑的。我们使用近端梯度下降，它将光滑部分（$\\mathcal{L}$）的梯度步与非光滑部分（$\\lambda \\|w\\|_1$）的近端步相结合。更新步骤为：\n$$\nw_{k+1} = \\text{prox}_{\\alpha\\lambda, \\|\\cdot\\|_1} ( w_k - \\alpha \\nabla_w \\mathcal{L}(w_k, b_k) )\n$$\n$$\nb_{k+1} = b_k - \\alpha \\nabla_b \\mathcal{L}(w_k, b_k)\n$$\n步长 $\\alpha$ 仅基于光滑部分选择，因此我们设置 $\\alpha = 1/L_{bound}$。$L_1$ 范数的近端算子是软阈值函数，逐元素应用：\n$$\n\\text{prox}_{\\gamma, \\|\\cdot\\|_1}(v)_i = \\text{sign}(v_i) \\max(|v_i| - \\gamma, 0)\n$$\n在我们的情况下，阈值为 $\\gamma = \\alpha\\lambda$。\n\n最终权重核 $W$（由 $w$ 重塑而来）的稀疏度计算为绝对值小于一个很小阈值 $\\tau > 0$ 的权重所占的比例：\n$$\n\\mathrm{spar}(W) = \\frac{1}{4L} \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\mathbf{1}\\left\\{ |W_{i,b}|  \\tau \\right\\}.\n$$\n实现将遵循这些推导来执行所要求的数值实验。",
            "answer": "```python\nimport numpy as np\n\ndef one_hot_encode(seq, alphabet_size=4):\n    \"\"\"Converts a sequence of integers into a one-hot encoded matrix.\"\"\"\n    return np.eye(alphabet_size)[seq]\n\ndef generate_data(N, S, L, seed, signal=True):\n    \"\"\"\n    Generates synthetic DNA sequence data.\n    \n    Args:\n        N (int): Total number of sequences.\n        S (int): Sequence length.\n        L (int): Motif length.\n        seed (int): Random seed for reproducibility.\n        signal (bool): If True, positive samples contain a motif. If False,\n                       all samples are random background.\n\n    Returns:\n        tuple: (sequences, labels)\n            sequences (np.ndarray): N x S x 4 one-hot encoded sequences.\n            labels (np.ndarray): N x 1 binary labels.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_pos = N // 2\n    n_neg = N - n_pos\n    \n    sequences_int = []\n    labels = np.array([1] * n_pos + [0] * n_neg)\n\n    # Ground-truth motif\n    motif_bases = rng.choice(4, size=L)\n\n    # Positive sequences\n    for _ in range(n_pos):\n        seq = rng.choice(4, size=S)\n        if signal:\n            start_pos = rng.integers(0, S - L + 1)\n            seq[start_pos : start_pos + L] = motif_bases\n        sequences_int.append(seq)\n        \n    # Negative sequences\n    for _ in range(n_neg):\n        seq = rng.choice(4, size=S)\n        sequences_int.append(seq)\n\n    sequences_one_hot = np.array([one_hot_encode(s) for s in sequences_int])\n    return sequences_one_hot, labels\n\ndef create_feature_matrix(sequences, S, L):\n    \"\"\"\n    Computes the average patch features for a set of sequences.\n    \n    Args:\n        sequences (np.ndarray): N x S x 4 one-hot encoded sequences.\n        S (int): Sequence length.\n        L (int): Motif length.\n\n    Returns:\n        np.ndarray: N x (4*L) feature matrix.\n    \"\"\"\n    N = sequences.shape[0]\n    num_windows = S - L + 1\n    phi_matrix = np.zeros((N, L, 4))\n\n    for n in range(N):\n        avg_patch = np.zeros((L, 4))\n        for t in range(num_windows):\n            avg_patch += sequences[n, t:t+L, :]\n        phi_matrix[n, :, :] = avg_patch / num_windows\n    \n    return phi_matrix.reshape(N, 4 * L)\n\ndef train_model(phi_matrix, y, N, L, reg_type, lambda_val, T, tau):\n    \"\"\"\n    Trains the logistic regression model and computes sparsity.\n    \n    Args:\n        phi_matrix (np.ndarray): N x (4*L) feature matrix.\n        y (np.ndarray): N-element label vector.\n        N (int): Number of samples.\n        L (int): Kernel length.\n        reg_type (str): 'L1' or 'L2'.\n        lambda_val (float): Regularization strength.\n        T (int): Number of training steps.\n        tau (float): Threshold for sparsity calculation.\n\n    Returns:\n        float: Sparsity fraction of the learned kernel weights.\n    \"\"\"\n    n_features = 4 * L\n    \n    # Augment features for intercept and Lipschitz constant calculation\n    tilde_phi = np.c_[phi_matrix, np.ones(N)]\n    \n    # Calculate Lipschitz constant bound for the gradient of the logistic loss\n    # L = norm(tilde_Phi^T * tilde_Phi) / (4*N) = sigma_max(tilde_Phi)^2 / (4*N)\n    svd_vals = np.linalg.svd(tilde_phi, compute_uv=False)\n    sigma_max = svd_vals[0]\n    lipschitz_L = (sigma_max**2) / (4 * N)\n    \n    # Initialize weights and bias\n    w = np.zeros(n_features)\n    b = 0.0\n\n    if reg_type == 'L1':\n        alpha = 1.0 / lipschitz_L\n    elif reg_type == 'L2':\n        alpha = 1.0 / (lipschitz_L + lambda_val)\n    else:\n        raise ValueError(\"Invalid regularization type\")\n\n    for _ in range(T):\n        z = phi_matrix @ w + b\n        p = 1.0 / (1.0 + np.exp(-z))\n        \n        error = p - y\n        \n        grad_w = (1.0 / N) * phi_matrix.T @ error\n        grad_b = (1.0 / N) * np.sum(error)\n        \n        if reg_type == 'L1':\n            w_temp = w - alpha * grad_w\n            w = np.sign(w_temp) * np.maximum(np.abs(w_temp) - alpha * lambda_val, 0)\n            b = b - alpha * grad_b\n        elif reg_type == 'L2':\n            w = w - alpha * (grad_w + lambda_val * w)\n            b = b - alpha * grad_b\n            \n    sparsity = np.sum(np.abs(w)  tau) / n_features\n    return sparsity\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Global parameters\n    S = 40\n    L = 8\n    T = 1000\n    tau = 1e-3\n\n    # Test cases from the problem statement\n    test_cases = [\n        {'case': 1, 'reg_type': 'L1', 'lambda_val': 0.05, 'N': 600, 'seed': 7,  'signal': True},\n        {'case': 2, 'reg_type': 'L2', 'lambda_val': 0.05, 'N': 600, 'seed': 7, 'signal': True},\n        {'case': 3, 'reg_type': 'L1', 'lambda_val': 0.3,  'N': 600, 'seed': 21, 'signal': False},\n        {'case': 4, 'reg_type': 'L1', 'lambda_val': 1e-6, 'N': 600, 'seed': 11, 'signal': True},\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate data\n        sequences, labels = generate_data(\n            N=case['N'], S=S, L=L, seed=case['seed'], signal=case['signal']\n        )\n        \n        # 2. Create feature matrix\n        phi_matrix = create_feature_matrix(sequences, S, L)\n        \n        # 3. Train model and get sparsity\n        sparsity = train_model(\n            phi_matrix, labels, case['N'], L,\n            case['reg_type'], case['lambda_val'], T, tau\n        )\n        \n        results.append(sparsity)\n\n    # Format and print the final output as specified.\n    print(f\"[{','.join(f'{v:.3f}' for v in results)}]\")\n\nsolve()\n```"
        }
    ]
}