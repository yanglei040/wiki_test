{
    "hands_on_practices": [
        {
            "introduction": "While we often rely on software libraries to train Support Vector Machines, working through the mechanics by hand provides invaluable insight. This exercise demystifies the process by having you calculate the decision function for an RBF kernel SVM in a simple, symmetric setting. By solving for the dual variables and bias term, you will see exactly how the kernel, training data, and model parameters interact to classify a new data point .",
            "id": "2433176",
            "problem": "A bacteriology lab constructs a binary classifier to distinguish pathogenic and commensal bacterial species using a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel. The feature representation for each species is a point in a $2$-dimensional space derived from genome-derived summary statistics. The training set consists of exactly $2$ labeled samples: a pathogenic bacterium at $\\mathbf{x}_{1} = (1, 1)$ with label $y_{1} = +1$, and a commensal at $\\mathbf{x}_{2} = (-1, -1)$ with label $y_{2} = -1$. The SVM uses the RBF kernel\n$$\nK(\\mathbf{x}, \\mathbf{z}) = \\exp\\!\\big(-\\gamma \\|\\mathbf{x} - \\mathbf{z}\\|^{2}\\big),\n$$\nwith kernel parameter $\\gamma > 0$, and the standard hard-margin formulation (i.e., separable case in the induced feature space). Let the resulting decision function be\n$$\nf(\\mathbf{x}) = \\sum_{i=1}^{2} \\alpha_{i} y_{i} K(\\mathbf{x}_{i}, \\mathbf{x}) + b,\n$$\nwith dual variables $\\alpha_{i}$ and bias $b$ determined by the hard-margin SVM optimization.\n\nCompute the exact value of the decision function at the query point $\\mathbf{x}_{\\ast} = (0, 0)$ as a closed-form analytic expression in terms of $\\gamma$. Provide your final answer as a single simplified expression. No rounding is required and no units are involved.",
            "solution": "The hard-margin Support Vector Machine (SVM) optimization in the dual for binary labels $y_{i} \\in \\{-1, +1\\}$ and kernel $K$ is\n$$\n\\max_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^{2}} \\; W(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{2} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K(\\mathbf{x}_{i}, \\mathbf{x}_{j}),\n$$\nsubject to $\\alpha_{i} \\ge 0$ for all $i$ and the equality constraint\n$$\n\\sum_{i=1}^{2} \\alpha_{i} y_{i} = 0.\n$$\nThe decision function is\n$$\nf(\\mathbf{x}) = \\sum_{i=1}^{2} \\alpha_{i} y_{i} K(\\mathbf{x}_{i}, \\mathbf{x}) + b,\n$$\nwith $b$ determined by the Karush–Kuhn–Tucker (KKT) conditions, which in the hard-margin case require that for any support vector $\\mathbf{x}_{i}$,\n$$\ny_{i} f(\\mathbf{x}_{i}) = 1.\n$$\n\nCompute the kernel matrix entries. For the Radial Basis Function (RBF) kernel with parameter $\\gamma > 0$,\n$$\nK(\\mathbf{x}_{1}, \\mathbf{x}_{1}) = \\exp\\!\\big(-\\gamma \\|\\mathbf{x}_{1} - \\mathbf{x}_{1}\\|^{2}\\big) = \\exp(0) = 1,\n$$\n$$\nK(\\mathbf{x}_{2}, \\mathbf{x}_{2}) = \\exp\\!\\big(-\\gamma \\|\\mathbf{x}_{2} - \\mathbf{x}_{2}\\|^{2}\\big) = 1.\n$$\nThe squared distance between $\\mathbf{x}_{1} = (1, 1)$ and $\\mathbf{x}_{2} = (-1, -1)$ is\n$$\n\\|\\mathbf{x}_{1} - \\mathbf{x}_{2}\\|^{2} = (1 - (-1))^{2} + (1 - (-1))^{2} = 2^{2} + 2^{2} = 8,\n$$\nso\n$$\nK(\\mathbf{x}_{1}, \\mathbf{x}_{2}) = K(\\mathbf{x}_{2}, \\mathbf{x}_{1}) = \\exp(-8 \\gamma).\n$$\n\nBy the equality constraint $\\sum_{i=1}^{2} \\alpha_{i} y_{i} = 0$ and the symmetry of the problem, the optimum satisfies $\\alpha_{1} = \\alpha_{2} = a$ for some $a \\ge 0$. Substitute into the dual objective:\n\\begin{align*}\nW(a) &= \\alpha_{1} + \\alpha_{2} - \\frac{1}{2}\\Big( \\alpha_{1}^{2} y_{1}^{2} K_{11} + 2 \\alpha_{1} \\alpha_{2} y_{1} y_{2} K_{12} + \\alpha_{2}^{2} y_{2}^{2} K_{22} \\Big) \\\\\n&= 2 a - \\frac{1}{2}\\Big( a^{2} \\cdot 1 \\cdot 1 + 2 a^{2} \\cdot (+1)(-1) K_{12} + a^{2} \\cdot 1 \\cdot 1 \\Big) \\\\\n&= 2 a - \\frac{1}{2}\\Big( a^{2} + a^{2} - 2 a^{2} K_{12} \\Big) \\\\\n&= 2 a - a^{2} \\big(1 - K_{12}\\big),\n\\end{align*}\nwhere $K_{12} = \\exp(-8 \\gamma)$. Differentiate with respect to $a$ and set to zero:\n$$\n\\frac{\\mathrm{d} W}{\\mathrm{d} a} = 2 - 2 a \\big(1 - K_{12}\\big) = 0 \\quad \\Longrightarrow \\quad a = \\frac{1}{1 - K_{12}} = \\frac{1}{1 - \\exp(-8 \\gamma)}.\n$$\nThus,\n$$\n\\alpha_{1} = \\alpha_{2} = \\frac{1}{1 - \\exp(-8 \\gamma)}.\n$$\n\nDetermine $b$ from the margin conditions. Using $y_{1} f(\\mathbf{x}_{1}) = 1$ and $y_{2} f(\\mathbf{x}_{2}) = 1$:\n\\begin{align*}\n1 &= y_{1} f(\\mathbf{x}_{1}) = (+1)\\Big( \\alpha_{1} y_{1} K_{11} + \\alpha_{2} y_{2} K_{21} + b \\Big) = a \\big( 1 - K_{12} \\big) + b, \\\\\n1 &= y_{2} f(\\mathbf{x}_{2}) = (-1)\\Big( \\alpha_{1} y_{1} K_{12} + \\alpha_{2} y_{2} K_{22} + b \\Big) = a \\big( 1 - K_{12} \\big) - b.\n\\end{align*}\nAdding the two equations gives\n$$\n2 = 2 a \\big(1 - K_{12}\\big) \\quad \\Longrightarrow \\quad a \\big(1 - K_{12}\\big) = 1,\n$$\nwhich is consistent with $a = 1/(1 - K_{12})$, and subtracting yields $b = 0$.\n\nNow evaluate the decision function at $\\mathbf{x}_{\\ast} = (0, 0)$. Compute the kernel values:\n\\begin{align*}\n\\|\\mathbf{x}_{1} - \\mathbf{x}_{\\ast}\\|^{2} &= (1 - 0)^{2} + (1 - 0)^{2} = 1^{2} + 1^{2} = 2, \\\\\n\\|\\mathbf{x}_{2} - \\mathbf{x}_{\\ast}\\|^{2} &= (-1 - 0)^{2} + (-1 - 0)^{2} = 1^{2} + 1^{2} = 2,\n\\end{align*}\nso\n$$\nK(\\mathbf{x}_{1}, \\mathbf{x}_{\\ast}) = \\exp(-2 \\gamma), \\quad K(\\mathbf{x}_{2}, \\mathbf{x}_{\\ast}) = \\exp(-2 \\gamma).\n$$\nTherefore,\n\\begin{align*}\nf(\\mathbf{x}_{\\ast}) &= \\alpha_{1} y_{1} K(\\mathbf{x}_{1}, \\mathbf{x}_{\\ast}) + \\alpha_{2} y_{2} K(\\mathbf{x}_{2}, \\mathbf{x}_{\\ast}) + b \\\\\n&= a \\big( +1 \\cdot \\exp(-2 \\gamma) \\big) + a \\big( -1 \\cdot \\exp(-2 \\gamma) \\big) + 0 \\\\\n&= a \\exp(-2 \\gamma) - a \\exp(-2 \\gamma) \\\\\n&= 0.\n\\end{align*}\nThus, the exact value of the decision function at $\\mathbf{x}_{\\ast} = (0, 0)$ is identically zero for all $\\gamma > 0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The performance of many machine learning models, and especially kernel methods, depends critically on proper data preparation. This practice explores a common scenario in computational biology where features have vastly different scales, such as gene expression levels and mutation counts. You will analyze why applying a distance-based kernel like the Radial Basis Function (RBF) without feature scaling can lead to poor model performance, highlighting the crucial link between the kernel's mathematical formulation and practical data preprocessing .",
            "id": "2433188",
            "problem": "A research team is building a binary classifier to predict whether tumor samples will respond to a targeted therapy based on a heterogeneous feature set that combines messenger ribonucleic acid (mRNA) expression levels and somatic mutation counts. The mRNA features are continuous, measured in transcripts per million and often range up to approximately $10^{4}$, whereas the mutation features are small nonnegative integers, typically between $0$ and $5$. They plan to train a Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel defined by $k(\\mathbf{x},\\mathbf{x}')=\\exp\\!\\left(-\\gamma \\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2}\\right)$ with $\\gamma>0$, where $\\lVert \\cdot \\rVert$ denotes the Euclidean norm. They are considering whether to scale each feature to a common range such as $[0,1]$ before training.\n\nWhich of the following best explains why scaling features to comparable ranges is crucial in this setting?\n\nA. Because the isotropic RBF kernel depends on the squared Euclidean distance $\\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2}$, unscaled high-magnitude mRNA features will dominate the distance, causing $k(\\mathbf{x},\\mathbf{x}')$ to become nearly $0$ for most pairs unless $\\gamma$ is made extremely small. This distorts the geometry, yields an ill-conditioned kernel matrix, and makes the model sensitive to arbitrary measurement units. Scaling restores comparable contributions and numerical conditioning.\n\nB. Because scaling ensures that the soft-margin penalty $C$ has the same physical units across all features, which is required for the SVM optimization problem to be well-defined. Without scaling, the optimization problem lacks a unique solution.\n\nC. Because the RBF kernel assumes each input feature is Gaussian with zero mean and unit variance, and scaling to $[0,1]$ enforces this assumption. If features are not scaled to $[0,1]$, the kernel no longer corresponds to a valid inner product.\n\nD. Because without scaling, the SVM will interpret integer-valued mutation counts as categorical variables and will compute a non-metric distance, violating the requirements of the RBF kernel and preventing convergence.\n\nE. Because the kernel trick automatically compensates for feature scale differences by mapping inputs to a high-dimensional space, scaling is unnecessary and can only reduce model expressiveness.",
            "solution": "The problem statement will first be subjected to rigorous validation.\n\n### Step 1: Extract Givens\n\nThe provided information consists of the following:\n- **Problem Domain**: A binary classification task in computational biology to predict tumor response to therapy.\n- **Input Features**: A heterogeneous set comprising:\n    - Messenger ribonucleic acid (mRNA) expression levels: continuous values, with a range up to approximately $10^4$.\n    - Somatic mutation counts: small non-negative integers, typically in the range $[0, 5]$.\n- **Proposed Model**: A Support Vector Machine (SVM).\n- **Kernel Function**: The Radial Basis Function (RBF) kernel, defined as $k(\\mathbf{x},\\mathbf{x}')=\\exp\\!\\left(-\\gamma \\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2}\\right)$, with the hyperparameter $\\gamma > 0$.\n- **Distance Metric**: The norm $\\lVert \\cdot \\rVert$ is specified as the Euclidean norm.\n- **Central Question**: The problem asks for the best explanation for why scaling features to a common range, such as $[0, 1]$, is crucial in this specific context.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is evaluated against the required criteria.\n\n- **Scientific Grounding**: The problem is grounded in the established fields of machine learning and computational biology. The use of an SVM with an RBF kernel for classification based on genomic data (mRNA expression, mutations) is a standard and scientifically valid methodology. The specified kernel function is correct, and the described feature characteristics are realistic for this type of data. The problem is scientifically sound.\n- **Well-Posed**: The question asks for a technical justification of a standard data preprocessing step (feature scaling) in the context of a specific algorithm (SVM with RBF kernel) and specific data characteristics (heterogeneous scales). The problem is structured to have a single, best-reasoned answer derived from the mathematical properties of the components involved. The problem is well-posed.\n- **Objective**: The problem is stated using precise, objective, and technical language. It requires a deductive explanation based on mathematical principles, not a subjective opinion. The question is objective.\n\n### Step 3: Verdict and Action\n\nThe problem statement is scientifically sound, well-posed, and objective. It contains no contradictions, ambiguities, or factual errors. Therefore, it is deemed **valid**. I will proceed to derive the solution and evaluate the given options.\n\nThe core of the problem lies in the interaction between the feature scales and the definition of the RBF kernel. The RBF kernel function $k(\\mathbf{x},\\mathbf{x}')$ computes the similarity between two data points $\\mathbf{x}$ and $\\mathbf{x}'$ based on the squared Euclidean distance between them, $\\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2}$.\n\nLet a data point $\\mathbf{x}$ be a vector of $d$ features, $\\mathbf{x} = (x_1, x_2, \\dots, x_d)$. The squared Euclidean distance is given by the sum of squared differences along each feature dimension:\n$$\n\\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2} = \\sum_{i=1}^{d} (x_i - x_i')^2\n$$\nIn the given scenario, the feature set is heterogeneous. Let us consider one mRNA feature, say $x_m$, and one mutation count feature, $x_c$.\nThe range for $x_m$ is on the order of $[0, 10^4]$.\nThe range for $x_c$ is on the order of $[0, 5]$.\n\nConsider the difference contribution of each feature to the total squared distance:\n- For the mRNA feature, a plausible difference between two samples could be, for instance, $(x_m - x_m') = 2000$. Its contribution to the sum is $(2000)^2 = 4 \\times 10^6$.\n- For the mutation count feature, the maximum possible difference is $(x_c - x_c') = 5$. Its maximum contribution to the sum is $5^2 = 25$.\n\nThe total squared distance is overwhelmingly dominated by the contribution from the feature with the largest numerical range:\n$$\n\\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2} \\approx (x_m - x_m')^2\n$$\nThe contributions from features with small numerical ranges, such as the mutation counts, become negligible in this sum. The RBF kernel is known as an *isotropic* kernel because it applies a single scaling parameter, $\\gamma$, to the total squared distance. It implicitly assumes that all feature dimensions are of comparable scale and importance.\n\nWhen one feature dominates the distance calculation, the geometry of the data space as perceived by the kernel is severely distorted. The SVM, which relies exclusively on these kernel-computed similarities to find the optimal separating hyperplane in the feature space, will effectively ignore all features except for those with the largest magnitudes. The model's predictive power will become dependent almost entirely on the mRNA features, while the potentially valuable information in the mutation count features is lost.\n\nFurthermore, consider the kernel value $k(\\mathbf{x},\\mathbf{x}') = \\exp(-\\gamma \\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2})$. If the distance $\\lVert \\mathbf{x}-\\mathbf{x}' \\rVert$ is very large for most pairs of distinct points (due to the large-magnitude features), the argument of the exponential function becomes a large negative number. Consequently, the kernel value $k(\\mathbf{x},\\mathbf{x}')$ will approach $0$. The kernel matrix $K$, where $K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$, will have diagonal elements equal to $1$ (since $k(\\mathbf{x}_i, \\mathbf{x}_i) = \\exp(0) = 1$) and off-diagonal elements very close to $0$. Such a matrix is numerically close to the identity matrix, which is often ill-conditioned for the purposes of the dual SVM optimization problem and can lead to a trivial or non-informative model. One could try to compensate by making $\\gamma$ extremely small, but this just shifts the numerical difficulties and can lead to a kernel matrix where all entries are close to $1$, which is also ill-conditioned (nearly singular).\n\nScaling the features, for example by mapping each to the range $[0, 1]$, resolves this issue. If all features are in $[0, 1]$, the maximum contribution of any feature to the squared Euclidean distance is $(1-0)^2 = 1$. All features can now contribute comparably to the distance calculation. This allows the hyperparameter $\\gamma$ to be chosen (e.g., via cross-validation) to control the model's complexity in a meaningful way across all dimensions, rather than just being a fudge factor to counteract poor scaling. It also makes the model robust to the choice of physical units for the measurements.\n\nNow, I will evaluate each option.\n\n**A. Because the isotropic RBF kernel depends on the squared Euclidean distance $\\lVert \\mathbf{x}-\\mathbf{x}' \\rVert^{2}$, unscaled high-magnitude mRNA features will dominate the distance, causing $k(\\mathbf{x},\\mathbf{x}')$ to become nearly $0$ for most pairs unless $\\gamma$ is made extremely small. This distorts the geometry, yields an ill-conditioned kernel matrix, and makes the model sensitive to arbitrary measurement units. Scaling restores comparable contributions and numerical conditioning.**\nThis statement is a precise and comprehensive summary of the reasoning derived above. It correctly identifies the role of the Euclidean distance, the dominance by high-magnitude features, the effect on kernel values, the resulting distortion of geometry, the numerical conditioning of the kernel matrix, and the sensitivity to measurement units.\n\nVerdict: **Correct**.\n\n**B. Because scaling ensures that the soft-margin penalty $C$ has the same physical units across all features, which is required for the SVM optimization problem to be well-defined. Without scaling, the optimization problem lacks a unique solution.**\nThis statement is incorrect. The soft-margin penalty $C$ is a unitless hyperparameter. It balances the trade-off between maximizing the margin width and minimizing the classification error. It is not related to the physical units of the features. The SVM optimization problem is well-defined regardless of feature scaling. While scaling can make the choice of a good $C$ easier in practice, it is not a requirement for the problem's mathematical definition or the existence of a unique solution.\n\nVerdict: **Incorrect**.\n\n**C. Because the RBF kernel assumes each input feature is Gaussian with zero mean and unit variance, and scaling to $[0,1]$ enforces this assumption. If features are not scaled to $[0,1]$, the kernel no longer corresponds to a valid inner product.**\nThis statement is fundamentally flawed. The RBF kernel does *not* assume that the input data follows a Gaussian distribution. The validity of a kernel function (i.e., its correspondence to an inner product in some feature space, as per Mercer's theorem) is a property of the function $k(\\cdot, \\cdot)$ itself, independent of the probability distribution of the data it is applied to. The RBF kernel is a valid positive definite kernel for any real-valued input vector space. Scaling to $[0, 1]$ is a form of min-max scaling, which does not produce a zero mean and unit variance distribution. That would be standardization. The entire premise is incorrect.\n\nVerdict: **Incorrect**.\n\n**D. Because without scaling, the SVM will interpret integer-valued mutation counts as categorical variables and will compute a non-metric distance, violating the requirements of the RBF kernel and preventing convergence.**\nThis statement demonstrates a grave misunderstanding of how SVM algorithms operate. An SVM treats all numerical inputs as continuous (real) values by default. It does not automatically interpret integer-valued features as categorical. It will compute the standard Euclidean distance, which is a metric, on these integer values just as it does for real values. For example, the distance component for mutation counts $3$ and $1$ is correctly computed as $(3-1)^2=4$. The algorithm does not compute a \"non-metric distance,\" and convergence is not prevented for this reason.\n\nVerdict: **Incorrect**.\n\n**E. Because the kernel trick automatically compensates for feature scale differences by mapping inputs to a high-dimensional space, scaling is unnecessary and can only reduce model expressiveness.**\nThis statement is the opposite of the truth. The kernel trick is a computational shortcut that avoids explicit calculation of feature vectors $\\phi(\\mathbf{x})$ in the high-dimensional space. The entire computation relies on the kernel function $k(\\mathbf{x}, \\mathbf{x}') = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle$, which is computed using the original input vectors $\\mathbf{x}$ and $\\mathbf{x}'$ in the original, unscaled space. As demonstrated, the RBF kernel is highly sensitive to the scale of these original features. Therefore, the kernel trick does not compensate for scale differences; it propagates the problems caused by unscaled features into the high-dimensional space. Scaling is essential, not unnecessary.\n\nVerdict: **Incorrect**.\n\nIn summary, option A provides the only correct and complete explanation.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The true power of the kernel trick lies in its flexibility to define custom similarity measures that incorporate domain-specific knowledge. This exercise moves beyond using standard kernels to the task of designing one from scratch for a specific bioinformatics problem: splice-site prediction. You will implement a weighted string kernel that gives more importance to matches in exon regions than in intron regions, demonstrating how to embed biological priors directly into your model's architecture .",
            "id": "2433200",
            "problem": "You are given a binary classification setting relevant to splice-site prediction in computational biology, modeled for a Support Vector Machine (SVM) using the kernel trick. Consider DNA sequences over the alphabet $\\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$ together with a same-length annotation mask over the alphabet $\\{\\text{E}, \\text{I}\\}$ indicating exon and intron positions, respectively. Define a weighted $k$-spectrum string kernel that upweights matches occurring fully within exon regions, as follows.\n\nLet $k \\in \\mathbb{N}$ be fixed. For a sequence $s$ of length $n$ and its mask $m$ of length $n$, for each starting position $p \\in \\{0,1,\\dots,n-k\\}$ define the $k$-mer window $s[p:p+k]$ and its window mask $m[p:p+k]$. Define a positional window weight\n$$\ng_{(s,m)}(p) \\;=\\;\n\\begin{cases}\nw_E & \\text{if all symbols of } m[p:p+k] \\text{ are } \\text{E},\\\\\nw_I & \\text{if all symbols of } m[p:p+k] \\text{ are } \\text{I},\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nFor any $k$-mer $u \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k$, define the weighted feature map component\n$$\n\\phi_u(s,m) \\;=\\; \\sum_{p=0}^{n-k} g_{(s,m)}(p)\\,\\mathbf{1}\\!\\left[s[p:p+k] = u\\right],\n$$\nwhere $\\mathbf{1}[\\cdot]$ is the indicator function. The kernel between two annotated sequences $(s,m)$ and $(t,n)$ is the inner product\n$$\nK\\big((s,m),(t,n)\\big) \\;=\\; \\sum_{u \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k} \\phi_u(s,m)\\,\\phi_u(t,n).\n$$\n\nUse the fixed parameter values $k=2$, $w_E=2$, and $w_I=1$. Consider the following annotated DNA sequences of equal length, each given as a pair $(\\text{sequence}, \\text{mask})$:\n- $A$: (\"ACGTAC\", \"EEEIII\")\n- $B$: (\"ACGTTC\", \"EEIIII\")\n- $C$: (\"TTGTAC\", \"IIIIEE\")\n- $D$: (\"AAAAAA\", \"IIIIII\")\n- $E$: (\"AAAAAA\", \"EEEEEE\")\n\nYour tasks are:\n- Compute the kernel values $K(A,A)$, $K(A,B)$, $K(B,C)$, and $K(D,E)$.\n- Form the Gram matrix $G$ for the set $\\{A,B,C\\}$ with entries $G_{ij} = K(S_i,S_j)$ for $S_1=A$, $S_2=B$, $S_3=C$, and determine whether $G$ is positive semidefinite in the sense that all its eigenvalues are greater than or equal to $0$ (within standard floating-point rounding).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order: `[K(A,A),K(A,B),K(B,C),K(D,E),is_PSD]`, where `is_PSD` is a boolean. For example, the output format must look like `[x,y,z,u,True]` with no spaces. All numerical answers are unitless. The test suite consists of the four kernel evaluations specified above and the positive semidefinite check on the Gram matrix for $\\{A,B,C\\}$, covering a typical case, cross-exon/intron interactions, an all-intron versus all-exon contrast, and a matrix-level validity check.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. It presents a valid computational task based on established principles of machine learning and bioinformatics. All necessary data and definitions are provided, and there are no contradictions or ambiguities. We shall proceed with the solution.\n\nThe core of the problem is the computation of a weighted $k$-spectrum string kernel. The kernel $K$ between two annotated sequences $(s, m)$ and $(t, n')$ is defined as the inner product of their feature vectors, $K\\big((s,m),(t,n')\\big) = \\langle \\phi(s,m), \\phi(t,n') \\rangle$. This can be written as:\n$$\nK\\big((s,m),(t,n')\\big) = \\sum_{u \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k} \\phi_u(s,m)\\,\\phi_u(t,n')\n$$\nwhere $\\phi_u(s, m)$ is the weighted count of the $k$-mer $u$ in sequence $s$ according to its mask $m$. A more direct, computationally efficient formulation, which avoids an explicit enumeration of all possible $k$-mers $u$, is given by:\n$$\nK\\big((s,m),(t,n')\\big) = \\sum_{p=0}^{|s|-k} \\sum_{q=0}^{|t|-k} g_{(s,m)}(p) g_{(t,n')}(q) \\mathbf{1}\\!\\left[s[p:p+k] = t[q:q+k]\\right]\n$$\nWe will utilize the feature map summation approach, as it is conceptually clear and equivalent. The fixed parameters are given as $k=2$, $w_E=2$, and $w_I=1$. All sequences have length $n=6$, so the number of $2$-mers in each is $n-k+1 = 6-2+1=5$, with starting positions $p \\in \\{0, 1, 2, 3, 4\\}$.\n\nFirst, we must compute the feature maps $\\phi(S)$ for each annotated sequence $S \\in \\{A, B, C, D, E\\}$. The feature map component $\\phi_u(S)$ for a $2$-mer $u$ is the sum of its positional weights $g(p)$ over all occurrences in the sequence.\n\n**1. Feature Map Calculations**\n\nFor each sequence, we list the $2$-mers, their mask windows, and the resulting positional weights $g(p)$.\n\n- **Sequence A**: $s_A = \\text{\"ACGTAC\"}$, $m_A = \\text{\"EEEIII\"}$\n  - $p=0$: $s_A[0:2]$=\"AC\", $m_A[0:2]$=\"EE\" (all 'E') $\\implies g_A(0) = w_E = 2$.\n  - $p=1$: $s_A[1:2]$=\"CG\", $m_A[1:2]$=\"EE\" (all 'E') $\\implies g_A(1) = w_E = 2$.\n  - $p=2$: $s_A[2:4]$=\"GT\", $m_A[2:4]$=\"EI\" (mixed) $\\implies g_A(2) = 0$.\n  - $p=3$: $s_A[3:5]$=\"TA\", $m_A[3:5]$=\"II\" (all 'I') $\\implies g_A(3) = w_I = 1$.\n  - $p=4$: $s_A[4:6]$=\"AC\", $m_A[4:6]$=\"II\" (all 'I') $\\implies g_A(4) = w_I = 1$.\n  The non-zero components of the feature map $\\phi(A)$ are:\n  $\\phi_{\\text{AC}}(A) = g_A(0) + g_A(4) = 2 + 1 = 3$.\n  $\\phi_{\\text{CG}}(A) = g_A(1) = 2$.\n  $\\phi_{\\text{TA}}(A) = g_A(3) = 1$.\n\n- **Sequence B**: $s_B = \\text{\"ACGTTC\"}$, $m_B = \\text{\"EEIIII\"}$\n  - $p=0$: \"AC\", \"EE\" (all 'E') $\\implies g_B(0) = w_E = 2$.\n  - $p=1$: \"CG\", \"EI\" (mixed) $\\implies g_B(1) = 0$.\n  - $p=2$: \"GT\", \"II\" (all 'I') $\\implies g_B(2) = w_I = 1$.\n  - $p=3$: \"TT\", \"II\" (all 'I') $\\implies g_B(3) = w_I = 1$.\n  - $p=4$: \"TC\", \"II\" (all 'I') $\\implies g_B(4) = w_I = 1$.\n  The non-zero components of $\\phi(B)$ are:\n  $\\phi_{\\text{AC}}(B) = 2$, $\\phi_{\\text{GT}}(B) = 1$, $\\phi_{\\text{TT}}(B) = 1$, $\\phi_{\\text{TC}}(B) = 1$.\n\n- **Sequence C**: $s_C = \\text{\"TTGTAC\"}$, $m_C = \\text{\"IIIIEE\"}$\n  - $p=0$: \"TT\", \"II\" (all 'I') $\\implies g_C(0) = w_I = 1$.\n  - $p=1$: \"TG\", \"II\" (all 'I') $\\implies g_C(1) = w_I = 1$.\n  - $p=2$: \"GT\", \"II\" (all 'I') $\\implies g_C(2) = w_I = 1$.\n  - $p=3$: \"TA\", \"IE\" (mixed) $\\implies g_C(3) = 0$.\n  - $p=4$: \"AC\", \"EE\" (all 'E') $\\implies g_C(4) = w_E = 2$.\n  The non-zero components of $\\phi(C)$ are:\n  $\\phi_{\\text{TT}}(C) = 1$, $\\phi_{\\text{TG}}(C) = 1$, $\\phi_{\\text{GT}}(C) = 1$, $\\phi_{\\text{AC}}(C) = 2$.\n\n- **Sequence D**: $s_D = \\text{\"AAAAAA\"}$, $m_D = \\text{\"IIIIII\"}$\n  - For all $p \\in \\{0, 1, 2, 3, 4\\}$, the $2$-mer is \"AA\" and the mask window is \"II\".\n  - Thus, $g_D(p) = w_I = 1$ for all $p$.\n  The only non-zero component of $\\phi(D)$ is:\n  $\\phi_{\\text{AA}}(D) = \\sum_{p=0}^4 1 = 5$.\n\n- **Sequence E**: $s_E = \\text{\"AAAAAA\"}$, $m_E = \\text{\"EEEEEE\"}$\n  - For all $p \\in \\{0, 1, 2, 3, 4\\}$, the $2$-mer is \"AA\" and the mask window is \"EE\".\n  - Thus, $g_E(p) = w_E = 2$ for all $p$.\n  The only non-zero component of $\\phi(E)$ is:\n  $\\phi_{\\text{AA}}(E) = \\sum_{p=0}^4 2 = 10$.\n\n**2. Kernel Value Computations**\n\nWe now compute the specified kernel values.\n\n- $K(A,A) = \\langle\\phi(A), \\phi(A)\\rangle = \\sum_u (\\phi_u(A))^2 = (\\phi_{\\text{AC}}(A))^2 + (\\phi_{\\text{CG}}(A))^2 + (\\phi_{\\text{TA}}(A))^2 = 3^2 + 2^2 + 1^2 = 9 + 4 + 1 = 14$.\n\n- $K(A,B) = \\langle\\phi(A), \\phi(B)\\rangle = \\sum_u \\phi_u(A)\\phi_u(B)$. The only common $2$-mer with non-zero weights is \"AC\".\n  $K(A,B) = \\phi_{\\text{AC}}(A)\\phi_{\\text{AC}}(B) = 3 \\times 2 = 6$.\n\n- $K(B,C) = \\langle\\phi(B), \\phi(C)\\rangle$. The common $2$-mers are \"AC\", \"GT\", and \"TT\".\n  $K(B,C) = \\phi_{\\text{AC}}(B)\\phi_{\\text{AC}}(C) + \\phi_{\\text{GT}}(B)\\phi_{\\text{GT}}(C) + \\phi_{\\text{TT}}(B)\\phi_{\\text{TT}}(C) = (2 \\times 2) + (1 \\times 1) + (1 \\times 1) = 4 + 1 + 1 = 6$.\n\n- $K(D,E) = \\langle\\phi(D), \\phi(E)\\rangle$. The only common $2$-mer is \"AA\".\n  $K(D,E) = \\phi_{\\text{AA}}(D)\\phi_{\\text{AA}}(E) = 5 \\times 10 = 50$.\n\n**3. Gram Matrix and Positive Semidefinite Check**\n\nThe Gram matrix $G$ for the set $\\{A, B, C\\}$ is a $3 \\times 3$ symmetric matrix with entries $G_{ij} = K(S_i, S_j)$, where $S_1=A, S_2=B, S_3=C$. We have already computed the off-diagonal entries $K(A,B)=6$ and $K(B,C)=6$. We need $K(A,C)$, $K(B,B)$, and $K(C,C)$.\n\n- $K(A,C) = \\langle\\phi(A), \\phi(C)\\rangle$. The only common $2$-mer is \"AC\".\n  $K(A,C) = \\phi_{\\text{AC}}(A)\\phi_{\\text{AC}}(C) = 3 \\times 2 = 6$.\n\n- $K(B,B) = \\langle\\phi(B), \\phi(B)\\rangle = \\sum_u (\\phi_u(B))^2 = (\\phi_{\\text{AC}}(B))^2 + (\\phi_{\\text{GT}}(B))^2 + (\\phi_{\\text{TT}}(B))^2 + (\\phi_{\\text{TC}}(B))^2 = 2^2 + 1^2 + 1^2 + 1^2 = 4 + 1 + 1 + 1 = 7$.\n\n- $K(C,C) = \\langle\\phi(C), \\phi(C)\\rangle = \\sum_u (\\phi_u(C))^2 = (\\phi_{\\text{AC}}(C))^2 + (\\phi_{\\text{GT}}(C))^2 + (\\phi_{\\text{TT}}(C))^2 + (\\phi_{\\text{TG}}(C))^2 = 2^2 + 1^2 + 1^2 + 1^2 = 4 + 1 + 1 + 1 = 7$.\n\nThe Gram matrix is therefore:\n$$\nG = \\begin{pmatrix} K(A,A) & K(A,B) & K(A,C) \\\\ K(B,A) & K(B,B) & K(B,C) \\\\ K(C,A) & K(C,B) & K(C,C) \\end{pmatrix} = \\begin{pmatrix} 14 & 6 & 6 \\\\ 6 & 7 & 6 \\\\ 6 & 6 & 7 \\end{pmatrix}\n$$\nTo determine if $G$ is positive semidefinite (PSD), we must check if all its eigenvalues $\\lambda$ are non-negative. We solve the characteristic equation $\\det(G - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} 14-\\lambda & 6 & 6 \\\\ 6 & 7-\\lambda & 6 \\\\ 6 & 6 & 7-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(14-\\lambda)((7-\\lambda)^2 - 36) - 6(6(7-\\lambda) - 36) + 6(36 - 6(7-\\lambda)) = 0\n$$\nThis simplifies to the characteristic polynomial $(\\lambda-1)(\\lambda-5)(\\lambda-22)=0$.\nThe eigenvalues are $\\lambda_1 = 1$, $\\lambda_2 = 5$, and $\\lambda_3 = 22$. Since all eigenvalues are strictly positive, G is not only positive semidefinite but also positive definite. This is expected, as any kernel defined as an inner product in a feature space is a valid Mercer kernel and will always produce a positive semidefinite Gram matrix. The result for `is_PSD` is `True`.\n\nSummary of results:\n- $K(A,A) = 14$\n- $K(A,B) = 6$\n- $K(B,C) = 6$\n- $K(D,E) = 50$\n- `is_PSD` = `True`",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes specified kernel values and checks the positive semidefiniteness \n    of a Gram matrix for a custom weighted k-spectrum string kernel.\n    \"\"\"\n    \n    # Define fixed parameters from the problem statement.\n    k = 2\n    w_E = 2.0\n    w_I = 1.0\n\n    # Define the annotated DNA sequences.\n    sequences = {\n        'A': (\"ACGTAC\", \"EEEIII\"),\n        'B': (\"ACGTTC\", \"EEIIII\"),\n        'C': (\"TTGTAC\", \"IIIIEE\"),\n        'D': (\"AAAAAA\", \"IIIIII\"),\n        'E': (\"AAAAAA\", \"EEEEEE\"),\n    }\n\n    def get_phi(seq_tuple, k, w_E, w_I):\n        \"\"\"\n        Computes the feature map phi for a given annotated sequence.\n        The map is a dictionary from k-mers to their weighted counts.\n        \"\"\"\n        s, m = seq_tuple\n        n = len(s)\n        phi = {}\n        for p in range(n - k + 1):\n            kmer = s[p:p + k]\n            mask_window = m[p:p + k]\n            weight = 0.0\n            \n            if all(char == 'E' for char in mask_window):\n                weight = w_E\n            elif all(char == 'I' for char in mask_window):\n                weight = w_I\n            \n            if weight > 0:\n                phi[kmer] = phi.get(kmer, 0.0) + weight\n        return phi\n\n    def kernel(phi_X, phi_Y):\n        \"\"\"\n        Computes the kernel value K(X, Y) as the inner product of their feature maps.\n        \"\"\"\n        val = 0.0\n        # Iterate over the smaller dictionary for efficiency.\n        if len(phi_X) > len(phi_Y):\n            phi_X, phi_Y = phi_Y, phi_X\n        \n        for kmer, value_X in phi_X.items():\n            value_Y = phi_Y.get(kmer, 0.0)\n            val += value_X * value_Y\n        return val\n\n    # Pre-compute all feature maps.\n    phis = {name: get_phi(data, k, w_E, w_I) for name, data in sequences.items()}\n\n    # Compute the four required kernel values.\n    k_AA = kernel(phis['A'], phis['A'])\n    k_AB = kernel(phis['A'], phis['B'])\n    k_BC = kernel(phis['B'], phis['C'])\n    k_DE = kernel(phis['D'], phis['E'])\n    \n    # Construct the Gram matrix G for the set {A, B, C}.\n    gram_keys = ['A', 'B', 'C']\n    N = len(gram_keys)\n    gram_matrix = np.zeros((N, N))\n    for i in range(N):\n        for j in range(i, N):\n            # Kernel function is symmetric, K(X, Y) = K(Y, X).\n            val = kernel(phis[gram_keys[i]], phis[gram_keys[j]])\n            gram_matrix[i, j] = val\n            gram_matrix[j, i] = val\n            \n    # Check if G is positive semidefinite by checking if all eigenvalues are non-negative.\n    # A small tolerance is used for floating-point arithmetic inaccuracies.\n    # np.linalg.eigvalsh is numerically stable and efficient for symmetric matrices.\n    eigenvalues = np.linalg.eigvalsh(gram_matrix)\n    is_psd = np.all(eigenvalues >= -1e-9)\n\n    # Assemble the final results list.\n    # Convert floats to integers if they are whole numbers.\n    results = [\n        int(k_AA) if k_AA == int(k_AA) else k_AA,\n        int(k_AB) if k_AB == int(k_AB) else k_AB,\n        int(k_BC) if k_BC == int(k_BC) else k_BC,\n        int(k_DE) if k_DE == int(k_DE) else k_DE,\n        is_psd\n    ]\n    \n    # Print the results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}