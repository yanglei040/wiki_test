{
    "hands_on_practices": [
        {
            "introduction": "The Hansch equation is a foundational QSAR model that often describes a parabolic relationship between biological potency, measured as $\\log_{10}(1/C)$, and a physicochemical property like hydrophobicity, $\\mathrm{cLogP}$. This mathematical form captures the crucial idea that an optimal property value exists for maximal effect. This exercise  provides direct practice with this concept, using basic calculus to find the ideal property value predicted by the model.",
            "id": "2423851",
            "problem": "In quantitative structure–activity relationship (QSAR) modeling, the hydrophobicity of a compound is often encoded by the calculated logarithm of the octanol–water partition coefficient ($\\mathrm{cLogP}$). Consider a Hansch-type relationship between biological potency and hydrophobicity given by the function\n$$\n\\log_{10}\\!\\left(\\frac{1}{C}\\right) \\;=\\; 0.5 \\cdot \\mathrm{cLogP} \\;-\\; 0.01 \\cdot (\\mathrm{cLogP})^{2} \\;+\\; 1.2,\n$$\nwhere $C$ denotes the molar concentration required to achieve a defined effect level, and $\\mathrm{cLogP}$ is dimensionless. Under the standard interpretation in QSAR, the optimal hydrophobicity corresponds to the value of $\\mathrm{cLogP}$ that maximizes the biological activity, equivalently maximizing $\\log_{10}(1/C)$.\n\nDetermine the optimal $\\mathrm{cLogP}$. Provide the exact value with no rounding. The answer is unitless.",
            "solution": "The problem statement must first be subjected to rigorous validation.\n\n**Step 1: Extract Givens**\nThe givens are listed verbatim as provided:\n- The Hansch-type relationship: $\\log_{10}\\!\\left(\\frac{1}{C}\\right) \\;=\\; 0.5 \\cdot \\mathrm{cLogP} \\;-\\; 0.01 \\cdot (\\mathrm{cLogP})^{2} \\;+\\; 1.2$\n- $C$ is the molar concentration required to achieve a defined effect level.\n- $\\mathrm{cLogP}$ is dimensionless.\n- The objective is to find the value of $\\mathrm{cLogP}$ that maximizes the biological activity, which is equivalent to maximizing the function $\\log_{10}(1/C)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required criteria.\n1.  **Scientifically Grounded**: The problem describes a Hansch analysis, a foundational concept in Quantitative Structure-Activity Relationship (QSAR) studies. The parabolic relationship between a biological response and a physicochemical parameter like hydrophobicity ($\\mathrm{cLogP}$) is a classic model used to explain how properties like membrane permeation and receptor binding contribute to activity. The equation is a standard representation of this model. The problem is therefore scientifically sound and based on established principles in computational drug design.\n2.  **Well-Posed**: The task is to find the maximum of a quadratic function. The function is explicitly defined, and the variable to be optimized is clearly stated. A quadratic function with a negative coefficient for the squared term has a unique global maximum. Thus, the problem is well-posed.\n3.  **Objective and Self-Contained**: The problem is stated using precise, objective mathematical and scientific language. All necessary information to solve the problem is provided. There are no ambiguities or missing definitions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a straightforward optimization problem grounded in a standard model from computational chemistry. I will proceed with the solution.\n\nThe problem requires finding the optimal value of $\\mathrm{cLogP}$ that maximizes the biological activity, which is given by the function for $\\log_{10}(1/C)$. Let us define a function $f(x)$ where $x = \\mathrm{cLogP}$. The function to be maximized is:\n$$\nf(x) = 0.5 x - 0.01 x^2 + 1.2\n$$\nThis is a quadratic function of the form $f(x) = ax^2 + bx + c$, where the coefficients are $a = -0.01$, $b = 0.5$, and $c = 1.2$. Since the coefficient $a$ is negative ($a = -0.01 < 0$), the graph of this function is a parabola opening downwards, which guarantees the existence of a single global maximum.\n\nTo find the value of $x$ that maximizes $f(x)$, we must use differential calculus. The maximum occurs at a critical point where the first derivative of the function with respect to $x$ is equal to zero.\nWe compute the first derivative, $f'(x)$:\n$$\nf'(x) = \\frac{d}{dx} \\left( 0.5 x - 0.01 x^2 + 1.2 \\right)\n$$\nApplying the power rule for differentiation:\n$$\nf'(x) = 0.5 - (2 \\cdot 0.01) x^{1} + 0\n$$\n$$\nf'(x) = 0.5 - 0.02 x\n$$\nNow, we set the first derivative to zero to find the critical point:\n$$\n0.5 - 0.02 x = 0\n$$\nSolving this linear equation for $x$:\n$$\n0.02 x = 0.5\n$$\n$$\nx = \\frac{0.5}{0.02}\n$$\nTo simplify the fraction, we can multiply the numerator and the denominator by $100$:\n$$\nx = \\frac{50}{2} = 25\n$$\nTo confirm that this value of $x$ corresponds to a maximum, we can use the second derivative test. We compute the second derivative, $f''(x)$:\n$$\nf''(x) = \\frac{d}{dx} (0.5 - 0.02 x) = -0.02\n$$\nSince $f''(x) = -0.02 < 0$ for all values of $x$, the function is concave down everywhere, and the critical point at $x=25$ is indeed a global maximum.\n\nTherefore, the optimal value of $\\mathrm{cLogP}$ that maximizes the biological activity is $25$.",
            "answer": "$$\\boxed{25}$$"
        },
        {
            "introduction": "A QSAR model's predictions are only reliable for molecules structurally similar to its training data, a concept formalized as the Applicability Domain (AD). The leverage method is a standard technique to assess this, where a high leverage value for a compound indicates it is an outlier in the descriptor space. This coding practice  is essential for building robust models, as it teaches you to implement a system that automatically identifies when a prediction is an unreliable extrapolation.",
            "id": "2423889",
            "problem": "A dataset of molecular descriptors is used to develop a Quantitative Structure-Activity Relationship (QSAR) model. A new compound yields a large prediction error. Determine whether such a failure would have been predictable using an applicability domain criterion based on leverage computed from the standardized descriptor space.\n\nLet the training descriptor matrix be denoted by $X \\in \\mathbb{R}^{n \\times m}$ with $n=12$ compounds and $m=3$ descriptors. For descriptor column $j \\in \\{1,2,3\\}$, compute the training-set mean $\\mu_j$ and population standard deviation $\\sigma_j$ as\n$$\n\\mu_j \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} X_{ij}, \\qquad\n\\sigma_j \\;=\\; \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} \\bigl(X_{ij}-\\mu_j\\bigr)^2}.\n$$\nStandardize the training matrix columnwise to obtain $Z \\in \\mathbb{R}^{n \\times m}$ with entries $Z_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}$. Define the symmetric positive-definite matrix $S \\in \\mathbb{R}^{m \\times m}$ by\n$$\nS \\;=\\; Z^\\top Z.\n$$\nFor a new compound with raw descriptor vector $x_{\\text{new}} \\in \\mathbb{R}^m$, standardize it using the training statistics to obtain $z_{\\text{new}} \\in \\mathbb{R}^m$ with entries $z_{\\text{new},j} = \\frac{x_{\\text{new},j}-\\mu_j}{\\sigma_j}$. Define its leverage as\n$$\nh_{\\text{new}} \\;=\\; z_{\\text{new}}^\\top S^{-1} z_{\\text{new}}.\n$$\nUse the conventional leverage-based applicability domain threshold\n$$\nh^\\star \\;=\\; \\frac{3m}{n}.\n$$\nDeclare a new compound to be outside the applicability domain if and only if $h_{\\text{new}} > h^\\star$. For this problem, interpret “the failure was predictable” as the event $h_{\\text{new}} > h^\\star$.\n\nUse the following training descriptor matrix $X$ (rows are compounds, columns are descriptors), with all numbers given in the same arbitrary units for each descriptor:\n- Row $1$: $\\bigl[\\,1.2,\\,35.0,\\,2.10\\,\\bigr]$\n- Row $2$: $\\bigl[\\,1.5,\\,40.2,\\,2.30\\,\\bigr]$\n- Row $3$: $\\bigl[\\,0.8,\\,28.7,\\,1.80\\,\\bigr]$\n- Row $4$: $\\bigl[\\,1.1,\\,33.3,\\,2.00\\,\\bigr]$\n- Row $5$: $\\bigl[\\,1.7,\\,45.1,\\,2.60\\,\\bigr]$\n- Row $6$: $\\bigl[\\,1.3,\\,36.9,\\,2.20\\,\\bigr]$\n- Row $7$: $\\bigl[\\,1.0,\\,31.5,\\,1.90\\,\\bigr]$\n- Row $8$: $\\bigl[\\,1.4,\\,38.4,\\,2.40\\,\\bigr]$\n- Row $9$: $\\bigl[\\,1.6,\\,43.0,\\,2.50\\,\\bigr]$\n- Row $10$: $\\bigl[\\,0.9,\\,29.8,\\,1.70\\,\\bigr]$\n- Row $11$: $\\bigl[\\,1.8,\\,47.5,\\,2.80\\,\\bigr]$\n- Row $12$: $\\bigl[\\,1.2,\\,34.1,\\,2.05\\,\\bigr]$\n\nEvaluate the following set of $4$ new compounds (test suite), each provided as a raw descriptor vector $x_{\\text{new}}$:\n- Case A: $\\bigl[\\,1.3,\\,36.0,\\,2.10\\,\\bigr]$\n- Case B: $\\bigl[\\,2.5,\\,70.0,\\,4.5\\,\\bigr]$\n- Case C: $\\bigl[\\,0.7,\\,26.0,\\,1.5\\,\\bigr]$\n- Case D: $\\bigl[\\,1.5,\\,40.2,\\,2.30\\,\\bigr]$\n\nYour program must:\n- Compute $\\mu_j$ and $\\sigma_j$ from the training matrix $X$ using the formulas above with $n=12$.\n- Standardize $X$ to obtain $Z$ and compute $S = Z^\\top Z$.\n- For each new case, standardize $x_{\\text{new}}$ to $z_{\\text{new}}$, compute $h_{\\text{new}} = z_{\\text{new}}^\\top S^{-1} z_{\\text{new}}$, compute $h^\\star = \\frac{3m}{n}$ with $m=3$ and $n=12$, and decide whether $h_{\\text{new}} > h^\\star$.\n- Produce a single line of output containing the results for the four cases as a comma-separated list of booleans enclosed in square brackets, in the order $\\bigl[\\text{A},\\text{B},\\text{C},\\text{D}\\bigr]$, for example $\\bigl[\\text{True},\\text{False},\\text{True},\\text{False}\\bigr]$.\n\nNo physical units are required in the output. Angles are not used. Percentages are not used. The output must be precisely one line in the format described above.",
            "solution": "We are asked to decide, for each new compound, whether a large prediction error would have been predictable by applicability domain analysis grounded in the leverage of the new point in the standardized descriptor space spanned by the training set. The fundamental construction is the hat matrix in descriptor space without an intercept term: for a standardized training matrix $Z \\in \\mathbb{R}^{n \\times m}$, the matrix $H \\in \\mathbb{R}^{n \\times n}$ defined as $H = Z \\bigl(Z^\\top Z\\bigr)^{-1} Z^\\top$ has diagonal elements that measure the leverages of the training compounds. For a new standardized descriptor vector $z_{\\text{new}} \\in \\mathbb{R}^m$, its leverage relative to the training set is\n$$\nh_{\\text{new}} \\;=\\; z_{\\text{new}}^\\top \\bigl(Z^\\top Z\\bigr)^{-1} z_{\\text{new}}.\n$$\nThis expression arises directly from the geometric interpretation of least squares projection in the $m$-dimensional descriptor space spanned by the columns of $Z$. The applicability domain threshold used here is\n$$\nh^\\star \\;=\\; \\frac{3m}{n},\n$$\nwhich scales inversely with the training set size $n$ and proportionally with the dimensionality $m$.\n\nAlgorithmic steps consistent with these principles:\n1. Given $X \\in \\mathbb{R}^{n \\times m}$ with $n=12$ and $m=3$, compute the columnwise means and population standard deviations:\n   $$\n   \\mu_j = \\frac{1}{12}\\sum_{i=1}^{12} X_{ij}, \\quad \\sigma_j = \\sqrt{\\frac{1}{12}\\sum_{i=1}^{12} \\bigl(X_{ij}-\\mu_j\\bigr)^2}, \\quad j \\in \\{1,2,3\\}.\n   $$\n   This yields specific numeric values $\\mu_1, \\mu_2, \\mu_3$ and $\\sigma_1, \\sigma_2, \\sigma_3$.\n2. Form the standardized training matrix $Z$ with entries $Z_{ij} = \\frac{X_{ij}-\\mu_j}{\\sigma_j}$, ensuring each column has zero mean and unit population variance. Compute $S = Z^\\top Z \\in \\mathbb{R}^{3 \\times 3}$ and invert it to obtain $S^{-1}$ (invertibility follows from the columns being linearly independent).\n3. For each new raw descriptor vector $x_{\\text{new}}$, compute $z_{\\text{new}}$ using the same $\\mu_j$ and $\\sigma_j$ as $z_{\\text{new},j} = \\frac{x_{\\text{new},j}-\\mu_j}{\\sigma_j}$, then compute leverage $h_{\\text{new}} = z_{\\text{new}}^\\top S^{-1} z_{\\text{new}}$.\n4. Set $h^\\star = \\frac{3m}{n} = \\frac{3 \\cdot 3}{12}$.\n5. Decide “predictable failure” if $h_{\\text{new}} > h^\\star$, and “not predictable” otherwise.\n\nApplying these steps to the provided data:\n- Using the given $X$, the means and population standard deviations are computed directly from definitions. Standardization yields $Z$, and then $S = Z^\\top Z$ is assembled and inverted.\n- For Case A with $x_{\\text{new}} = \\bigl[\\,1.3,\\,36.0,\\,2.10\\,\\bigr]$, the standardized vector $z_{\\text{new}}$ lies close to the origin; the resulting $h_{\\text{new}}$ is much less than $h^\\star$, so the decision is “not predictable” (boolean $\\text{False}$).\n- For Case B with $x_{\\text{new}} = \\bigl[\\,2.5,\\,70.0,\\,4.5\\,\\bigr]$, $z_{\\text{new}}$ is far from the training cloud; $h_{\\text{new}}$ greatly exceeds $h^\\star$, so the decision is “predictable” (boolean $\\text{True}$).\n- For Case C with $x_{\\text{new}} = \\bigl[\\,0.7,\\,26.0,\\,1.5\\,\\bigr]$, $z_{\\text{new}}$ is moderately far from the center; $h_{\\text{new}}$ exceeds $h^\\star$, so the decision is “predictable” (boolean $\\text{True}$).\n- For Case D with $x_{\\text{new}} = \\bigl[\\,1.5,\\,40.2,\\,2.30\\,\\bigr]$, which coincides with a training point, $h_{\\text{new}}$ is small compared to $h^\\star$, so the decision is “not predictable” (boolean $\\text{False}$).\n\nTherefore, in the order $\\bigl[\\text{A},\\text{B},\\text{C},\\text{D}\\bigr]$, the boolean results are $\\bigl[\\text{False},\\text{True},\\text{True},\\text{False}\\bigr]$. The accompanying program computes these values by following the definitions above and prints them in the required single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef standardize_training(X):\n    \"\"\"\n    Compute column-wise means and population standard deviations (ddof=0),\n    then standardize X using these statistics.\n    Returns Z (standardized), means, stds.\n    \"\"\"\n    means = X.mean(axis=0)\n    # population std: sqrt(mean((x - mean)^2))\n    diffs = X - means\n    vars_pop = (diffs ** 2).mean(axis=0)\n    stds = np.sqrt(vars_pop)\n    # Guard against zero std (not expected here)\n    if np.any(stds == 0):\n        raise ValueError(\"Zero standard deviation encountered in a descriptor column.\")\n    Z = (X - means) / stds\n    return Z, means, stds\n\ndef leverage_new_point(z_new, S_inv):\n    \"\"\"\n    Compute leverage h_new = z_new^T S^{-1} z_new\n    where S = Z^T Z for training standardized matrix Z.\n    \"\"\"\n    return float(z_new @ S_inv @ z_new)\n\ndef solve():\n    # Training descriptor matrix X: n=12, m=3\n    X = np.array([\n        [1.2, 35.0, 2.10],\n        [1.5, 40.2, 2.30],\n        [0.8, 28.7, 1.80],\n        [1.1, 33.3, 2.00],\n        [1.7, 45.1, 2.60],\n        [1.3, 36.9, 2.20],\n        [1.0, 31.5, 1.90],\n        [1.4, 38.4, 2.40],\n        [1.6, 43.0, 2.50],\n        [0.9, 29.8, 1.70],\n        [1.8, 47.5, 2.80],\n        [1.2, 34.1, 2.05],\n    ], dtype=float)\n\n    # Test suite: new compounds (A, B, C, D)\n    new_compounds = [\n        np.array([1.3, 36.0, 2.10], dtype=float),  # A\n        np.array([2.5, 70.0, 4.50], dtype=float),  # B\n        np.array([0.7, 26.0, 1.50], dtype=float),  # C\n        np.array([1.5, 40.2, 2.30], dtype=float),  # D\n    ]\n\n    n, m = X.shape\n    # Standardize training data\n    Z, means, stds = standardize_training(X)\n    # Compute S and its inverse\n    S = Z.T @ Z\n    # Using a robust inverse in case of near-singularity; here it should be fine.\n    S_inv = np.linalg.inv(S)\n\n    # Applicability domain threshold h* = 3m/n\n    h_star = 3.0 * m / n\n\n    results = []\n    for x_new in new_compounds:\n        z_new = (x_new - means) / stds\n        h_new = leverage_new_point(z_new, S_inv)\n        predictable = h_new > h_star\n        results.append(predictable)\n\n    # Final print statement in the exact required format.\n    # Booleans will be printed as True/False without spaces as per join formatting.\n    print(f\"[{','.join('True' if r else 'False' for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While powerful, complex machine learning models like neural networks are often considered 'black boxes', making it difficult to understand *why* they make a certain prediction. SHapley Additive exPlanations (SHAP) is a state-of-the-art technique that explains a model's output by fairly attributing the prediction to the contribution of each input feature, $\\phi_i(x)$. This advanced practice  offers hands-on experience in implementing SHAP from first principles, equipping you with a rigorous method to interpret and trust even the most complex QSAR models.",
            "id": "2423840",
            "problem": "Construct a complete program that computes exact SHapley Additive exPlanations (SHAP) values for a specified Quantitative Structure-Activity Relationship (QSAR) prediction task. Quantitative Structure-Activity Relationship (QSAR) relates molecular structure descriptors to biological activity. Consider a neural network (NN) model that maps a binary molecular fingerprint descriptor vector of dimension $d=6$ to a predicted activity on the negative base-$10$ logarithm concentration scale (commonly referred to as $p\\mathrm{IC}_{50}$, where half maximal inhibitory concentration ($\\text{IC}_{50}$) is a concentration; $p\\mathrm{IC}_{50}$ is dimensionless).\n\nThe model is defined as follows. Let the input be $x \\in \\{0,1\\}^{6}$, the hidden layer be of size $3$, and the activation function be the Rectified Linear Unit (ReLU), defined by $\\mathrm{ReLU}(z)=\\max(0,z)$ applied element-wise. The model output is\n$$\nf(x) = b_2 + W_2 \\cdot \\sigma(W_1 x + b_1),\n$$\nwhere $\\sigma$ denotes the element-wise $\\mathrm{ReLU}$, $W_1 \\in \\mathbb{R}^{3 \\times 6}$, $b_1 \\in \\mathbb{R}^{3}$, $W_2 \\in \\mathbb{R}^{1 \\times 3}$, and $b_2 \\in \\mathbb{R}$. The parameters are fixed as:\n$$\nW_1 =\n\\begin{bmatrix}\n1.0 & -0.5 & 0.0 & 0.5 & 1.0 & -1.0\\\\\n0.0 & 1.0 & 1.0 & -0.5 & 0.5 & 0.0\\\\\n0.5 & 0.5 & -1.0 & 1.0 & 0.0 & 1.0\n\\end{bmatrix},\n\\quad\nb_1 =\n\\begin{bmatrix}\n0.0\\\\\n0.0\\\\\n0.0\n\\end{bmatrix},\n\\quad\nW_2 =\n\\begin{bmatrix}\n1.0 & 0.5 & -0.5\n\\end{bmatrix},\n\\quad\nb_2 = 0.1.\n$$\n\nTo define SHapley Additive exPlanations (SHAP), fix a baseline descriptor vector $b \\in \\{0,1\\}^{6}$ equal to the all-zero vector:\n$$\nb = [0,0,0,0,0,0].\n$$\nFor any subset $S \\subseteq \\{1,2,3,4,5,6\\}$, define the coalition value\n$$\nv(S) = f(x_S),\n$$\nwhere $x_S \\in \\{0,1\\}^{6}$ is constructed by\n$$\n(x_S)_j =\n\\begin{cases}\nx_j, & \\text{if } j \\in S,\\\\\nb_j, & \\text{if } j \\notin S.\n\\end{cases}\n$$\nFor each feature index $i \\in \\{1,\\dots,6\\}$, the SHAP value $\\phi_i(x)$ is the Shapley value of player $i$ in the cooperative game with characteristic function $v$, defined by\n$$\n\\phi_i(x) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(|N|-|S|-1)!}{|N|!} \\left[ v(S \\cup \\{i\\}) - v(S) \\right],\n$$\nwhere $N=\\{1,2,3,4,5,6\\}$ and $|N|=6$.\n\nYour program must compute the exact SHAP value vector\n$$\n\\Phi(x) = [\\phi_1(x), \\phi_2(x), \\phi_3(x), \\phi_4(x), \\phi_5(x), \\phi_6(x)]\n$$\nfor each input descriptor vector $x$ in the following test suite, using the model $f$ and baseline $b$ as specified above:\n\n- Test case $1$ (single potent molecule): $x^{(1)} = [1,1,0,1,1,0]$.\n- Test case $2$ (boundary baseline): $x^{(2)} = [0,0,0,0,0,0]$.\n- Test case $3$ (single-bit presence): $x^{(3)} = [0,0,1,0,0,0]$.\n- Test case $4$ (saturated presence): $x^{(4)} = [1,1,1,1,1,1]$.\n\nNo external data are required. All computations are dimensionless and require no physical units. Angles are not involved. The final results for each test case must be lists of real numbers.\n\nYour program should produce a single line of output containing the results for all test cases as a list of lists, in the exact order $\\{x^{(1)}, x^{(2)}, x^{(3)}, x^{(4)}\\}$, where each inner list is the SHAP value vector $\\Phi(x^{(k)})$ for the corresponding test case $k \\in \\{1,2,3,4\\}$. The format must be a single line:\n$$\n[\\Phi(x^{(1)}), \\Phi(x^{(2)}), \\Phi(x^{(3)}), \\Phi(x^{(4)})]\n$$\nFor example, the output must look like\n$$\n[[$a_1,a_2,a_3,a_4,a_5,a_6$],[$b_1,b_2,b_3,b_4,b_5,b_6$],[$c_1,c_2,c_3,c_4,c_5,c_6$],[$d_1,d_2,d_3,d_4,d_5,d_6$]]\n$$\nwith each $a_i$, $b_i$, $c_i$, and $d_i$ real numbers. You may round to a fixed number of decimal places if desired, but you must output numeric values (not symbols) in this structure.",
            "solution": "The task is to compute SHapley Additive exPlanations (SHAP) values from first principles for a specified Quantitative Structure-Activity Relationship (QSAR) neural network. The cooperative game is set by fixing a model $f$, a baseline $b$, and an input $x$. For any subset $S \\subseteq N=\\{1,\\dots,6\\}$, the coalition value is $v(S)=f(x_S)$, with $x_S$ equal to $x$ on indices in $S$ and equal to $b$ on indices not in $S$.\n\nThe model is a one-hidden-layer neural network with Rectified Linear Unit (ReLU) nonlinearity:\n$$\nf(x) = b_2 + W_2 \\cdot \\sigma(W_1 x + b_1),\n$$\nwhere $\\sigma(z)=\\max(0,z)$ element-wise. The matrices and vectors are explicitly given:\n$$\nW_1 =\n\\begin{bmatrix}\n1.0 & -0.5 & 0.0 & 0.5 & 1.0 & -1.0\\\\\n0.0 & 1.0 & 1.0 & -0.5 & 0.5 & 0.0\\\\\n0.5 & 0.5 & -1.0 & 1.0 & 0.0 & 1.0\n\\end{bmatrix},\n\\quad\nb_1 =\n\\begin{bmatrix}\n0.0\\\\\n0.0\\\\\n0.0\n\\end{bmatrix},\n\\quad\nW_2 =\n\\begin{bmatrix}\n1.0 & 0.5 & -0.5\n\\end{bmatrix},\n\\quad\nb_2 = 0.1,\n\\quad\nb = [0,0,0,0,0,0].\n$$\nGiven any $x \\in \\{0,1\\}^{6}$, the forward evaluation computes $z=W_1 x + b_1$, applies $\\sigma$ to obtain $h=\\sigma(z)$, and then outputs $f(x)=b_2 + W_2 \\cdot h$. Since the baseline $b$ is the all-zero vector, for any subset $S$ the vector $x_S$ can be obtained by zeroing all features not in $S$ and leaving the features in $S$ equal to those of $x$.\n\nThe SHAP value $\\phi_i(x)$ for feature $i$ is defined by the Shapley value formula from cooperative game theory:\n$$\n\\phi_i(x) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!\\,(|N|-|S|-1)!}{|N|!} \\left[ f(x_{S \\cup \\{i\\}}) - f(x_S) \\right],\n$$\nwith $|N|=6$. The factorial-based weight $\\frac{|S|!\\,(|N|-|S|-1)!}{|N|!}$ depends only on the size $|S|$ and ensures the Shapley axioms, including symmetry, dummy, additivity, and the efficiency property:\n$$\n\\sum_{i=1}^{6} \\phi_i(x) = f(x) - f(b).\n$$\n\nPrinciple-based computation proceeds as follows:\n- Enumerate all subsets $S \\subseteq N$; since $|N|=6$, there are $2^{6}=64$ subsets. For computational efficiency and exactness, precompute $f(x_S)$ for all $S$.\n- For each feature $i$, sum over all $S \\subseteq N \\setminus \\{i\\}$ the weighted marginal contribution $f(x_{S \\cup \\{i\\}}) - f(x_S)$ with the exact Shapley weight. This uses only the fundamental definition of the Shapley value and does not require approximations or external libraries.\n- The result is the SHAP vector $\\Phi(x)=[\\phi_1(x),\\dots,\\phi_6(x)]$.\n\nFor verification, one can check that for each test input $x$ the efficiency property holds:\n$$\n\\sum_{i=1}^{6} \\phi_i(x) = f(x) - f(b),\n$$\nwith $b=[0,0,0,0,0,0]$ and the given $f$. Because $\\sigma$ is piecewise linear and $f$ is a sum of affine forms composed with $\\sigma$, all evaluations are deterministic real numbers. The program carries out these steps exactly for the four specified test cases:\n- $x^{(1)} = [1,1,0,1,1,0]$,\n- $x^{(2)} = [0,0,0,0,0,0]$,\n- $x^{(3)} = [0,0,1,0,0,0]$,\n- $x^{(4)} = [1,1,1,1,1,1]$.\n\nThe final output is a single line containing the list of SHAP vectors in the required order:\n$$\n[\\Phi(x^{(1)}), \\Phi(x^{(2)}), \\Phi(x^{(3)}), \\Phi(x^{(4)})].\n$$\nEach inner vector consists of real numbers and can be rounded for readability without changing the mathematical correctness. This approach satisfies the fundamental definition of SHAP values and directly explains the contribution of each binary fingerprint feature to the QSAR model’s $p\\mathrm{IC}_{50}$ prediction relative to the baseline.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef relu(z: np.ndarray) -> np.ndarray:\n    return np.maximum(0.0, z)\n\ndef model_f(x: np.ndarray) -> float:\n    # Define the fixed NN parameters\n    W1 = np.array([\n        [1.0, -0.5, 0.0, 0.5, 1.0, -1.0],\n        [0.0,  1.0, 1.0, -0.5, 0.5,  0.0],\n        [0.5,  0.5, -1.0, 1.0, 0.0,  1.0]\n    ], dtype=float)\n    b1 = np.array([0.0, 0.0, 0.0], dtype=float)\n    W2 = np.array([1.0, 0.5, -0.5], dtype=float)\n    b2 = 0.1\n    h = relu(W1 @ x + b1)\n    return float(b2 + W2 @ h)\n\ndef shap_values_exact(x: np.ndarray, f, baseline: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute exact SHAP values by enumeration for input x given model f and baseline.\n    Uses interventional definition with baseline imputation (features not in S set to baseline).\n    \"\"\"\n    d = x.size\n    # Precompute factorials for weights\n    fact = [math.factorial(k) for k in range(d+1)]\n    d_fact = fact[d]\n\n    # Precompute f for all subset masks to avoid redundant evaluations.\n    # mask is an integer in [0, 2^d), where bit j indicates feature j included.\n    num_masks = 1 << d\n    f_cache = np.empty(num_masks, dtype=float)\n    for mask in range(num_masks):\n        x_S = baseline.copy()\n        # set features present in mask to those of x\n        for j in range(d):\n            if (mask >> j) & 1:\n                x_S[j] = x[j]\n        f_cache[mask] = f(x_S)\n\n    # Compute SHAP values\n    phi = np.zeros(d, dtype=float)\n    for i in range(d):\n        contrib = 0.0\n        for mask in range(num_masks):\n            if (mask >> i) & 1:\n                # skip masks that already include i; we only consider S ⊆ N \\ {i}\n                continue\n            s_size = mask.bit_count()\n            weight = (fact[s_size] * fact[d - s_size - 1]) / d_fact\n            with_i_mask = mask | (1 << i)\n            marginal = f_cache[with_i_mask] - f_cache[mask]\n            contrib += weight * marginal\n        phi[i] = contrib\n    return phi\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([1,1,0,1,1,0], dtype=float),  # Test case 1\n        np.array([0,0,0,0,0,0], dtype=float),  # Test case 2\n        np.array([0,0,1,0,0,0], dtype=float),  # Test case 3\n        np.array([1,1,1,1,1,1], dtype=float),  # Test case 4\n    ]\n    baseline = np.zeros(6, dtype=float)\n\n    # Compute SHAP values for each test case\n    results = []\n    for x in test_cases:\n        phi = shap_values_exact(x, model_f, baseline)\n        # Optional rounding for readability\n        phi = np.round(phi, 6)\n        results.append(phi.tolist())\n\n    # Final print statement in the exact required format: list of lists on a single line.\n    # Example: [[a1,a2,...,a6],[b1,b2,...,b6],...]\n    def format_list_of_lists(lol):\n        # Format without extra spaces to keep it compact and deterministic.\n        inner = []\n        for lst in lol:\n            inner.append(\"[\" + \",\".join(f\"{v:.6f}\".rstrip('0').rstrip('.') if '.' in f\"{v:.6f}\" else str(v) for v in lst) + \"]\")\n        return \"[\" + \",\".join(inner) + \"]\"\n    \n    # Corrected simple formatter to avoid the complex logic for this task\n    def format_list_of_lists_simple(lol):\n        return str(lol).replace(\" \", \"\")\n\n    print(format_list_of_lists_simple(results))\n\n# Simplified solve to ensure the format is exactly as requested by a simple string conversion.\ndef solve():\n    test_cases = [\n        np.array([1,1,0,1,1,0], dtype=float),\n        np.array([0,0,0,0,0,0], dtype=float),\n        np.array([0,0,1,0,0,0], dtype=float),\n        np.array([1,1,1,1,1,1], dtype=float),\n    ]\n    baseline = np.zeros(6, dtype=float)\n    results = []\n    for x in test_cases:\n        phi = shap_values_exact(x, model_f, baseline)\n        # Rounding for clean output\n        results.append(np.round(phi, 6).tolist())\n    # Convert list of lists to string and remove spaces for the required compact format\n    print(str(results).replace(\" \", \"\"))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}