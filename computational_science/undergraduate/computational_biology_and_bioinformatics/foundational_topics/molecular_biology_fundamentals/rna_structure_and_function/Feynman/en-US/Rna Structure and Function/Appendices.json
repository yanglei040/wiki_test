{
    "hands_on_practices": [
        {
            "introduction": "While finding the single Minimum Free Energy (MFE) structure is a fundamental task, an RNA molecule in solution actually exists as a dynamic ensemble of conformations. This exercise moves beyond a single structure to explore the entire folding landscape. By extending a dynamic programming algorithm, you will not just find the optimal structure, but count all suboptimal structures within a given energy threshold $\\Delta$, a quantity that measures the \"frustration\" of the landscape . This practice provides a deep, quantitative understanding of conformational ensembles and their importance in RNA function.",
            "id": "2427180",
            "problem": "You will implement a program that quantifies the \"frustration\" of a Ribonucleic Acid (RNA) folding landscape by counting the number of competing, near-optimal secondary structures under a simplified, yet scientifically standard, model. The goal is to derive from first principles a precise algorithm and compute, for a given set of RNA sequences and energy thresholds, how many suboptimal secondary structures are energetically close to the Minimum Free Energy (MFE) structure.\n\nDefinitions and modeling assumptions:\n- An RNA sequence is a string over the alphabet $\\{ \\mathrm{A}, \\mathrm{U}, \\mathrm{G}, \\mathrm{C} \\}$.\n- A secondary structure is a set of non-crossing base pairs $(i,j)$ with $i < j$, where each nucleotide is in at most one pair, and there are no pseudoknots (that is, no two pairs $(i,j)$ and $(k,\\ell)$ such that $i < k < j < \\ell$).\n- Allowed base pairs and their energies (in kilocalories per mole) are:\n  - $\\mathrm{G}$–$\\mathrm{C}$ or $\\mathrm{C}$–$\\mathrm{G}$: energy $-3$,\n  - $\\mathrm{A}$–$\\mathrm{U}$ or $\\mathrm{U}$–$\\mathrm{A}$: energy $-2$,\n  - $\\mathrm{G}$–$\\mathrm{U}$ or $\\mathrm{U}$–$\\mathrm{G}$: energy $-1$.\n  Unpaired nucleotides contribute energy $0$.\n- Hairpin minimum loop length constraint: a pair $(i,j)$ is allowed only if $j - i - 1 \\geq 3$.\n- Total free energy of a secondary structure $s$ is the sum of its base-pair energies, denoted $E(s)$, in $\\mathrm{kcal/mol}$.\n- The Minimum Free Energy (MFE) is $E_{\\min} = \\min_{s} E(s)$ over all valid secondary structures $s$ for a given sequence.\n- For a nonnegative threshold $\\Delta$ (in $\\mathrm{kcal/mol}$), define the frustration count as\n  $$F(\\Delta) = \\left| \\left\\{ s \\ \\big| \\ E_{\\min} < E(s) \\leq E_{\\min} + \\Delta \\right\\} \\right|,$$\n  that is, the number of distinct near-optimal structures with energy strictly greater than the MFE but within $\\Delta$ of it. Distinct structures are defined by distinct sets of base pairs.\n\nYour task:\n- Using only the above definitions, design an algorithm that:\n  - Computes $E_{\\min}$ for a sequence under the given constraints.\n  - Enumerates all valid secondary structures (without pseudoknots) and their energies.\n  - Computes $F(\\Delta)$ for each test case.\n- Universal applicability requirement: Your algorithm must be presented and implemented in purely mathematical and logical terms, relying on the definitions and constraints above, and should be understandable independent of any specific external biology software package.\n\nUnits and numeric format:\n- Energies are measured in $\\mathrm{kcal/mol}$; however, your program’s output is unitless counts, each an integer.\n- Angles are not used.\n- Percentages are not used.\n\nTest suite:\nCompute $F(\\Delta)$ for each of the following $(\\text{sequence}, \\Delta)$ pairs:\n- Case $1$: sequence \"GCAUCU\", $\\Delta = 1$.\n- Case $2$: sequence \"GCAUCU\", $\\Delta = 3$.\n- Case $3$: sequence \"AAAAAA\", $\\Delta = 5$.\n- Case $4$: sequence \"AUGC\", $\\Delta = 10$.\n- Case $5$: sequence \"GGGAAACCC\", $\\Delta = 2$.\n- Case $6$: sequence \"GCAUUGC\", $\\Delta = 3$.\n- Case $7$: sequence \"GGGCCC\", $\\Delta = 3$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5,r6,r7]\"), where each $r_k$ is the integer frustration count $F(\\Delta)$ for Case $k$ in the specified order.",
            "solution": "The problem requires us to compute the number of suboptimal secondary structures for a given RNA sequence within a specified energy threshold, $\\Delta$, above the Minimum Free Energy (MFE). This is a counting problem defined on the state space of all valid non-pseudoknotted secondary structures. The structure of such problems, constrained by the non-crossing condition, is naturally suited for a dynamic programming approach, as the folding of a sequence can be decomposed into independent folding problems on its subsequences.\n\nWe shall design an algorithm based on this principle. The core of the algorithm is the systematic computation of the energy distribution for every possible subsequence of the given RNA sequence. The energy distribution is not just a single minimum value, but a complete enumeration of all possible energies and the number of distinct structures that can achieve each energy. We will represent this distribution as a map from an energy value $E$ to a count $c$, signifying that $c$ distinct structures have a total free energy of $E$.\n\nLet the RNA sequence be $S$ of length $L$, indexed from $0$ to $L-1$. We define two dynamic programming tables, $N$ and $C$, where each entry $(i,j)$ with $0 \\le i \\le j < L$ stores an energy distribution map.\n\n1.  $N(i,j)$: This map stores the energy distribution for all valid secondary structures on the subsequence $S[i..j]$.\n2.  $C(i,j)$: This map stores the energy distribution for all valid secondary structures on the subsequence $S[i..j]$, conditional on the nucleotides $S[i]$ and $S[j]$ forming a base pair.\n\nThe tables are filled by iterating over the length of the subsequence, $d = j-i$, from $d=0$ to $L-1$.\n\n**Base Cases:**\n- For an empty subsequence, there exists one structure (the empty one) with energy $0$. To handle boundary conditions in our recurrence, we define $N(i, i-1) = \\{0: 1\\}$.\n- For a subsequence of length $1$, $S[i]$, there is only one structure (the unpaired nucleotide) with energy $0$. Thus, $N(i,i) = \\{0: 1\\}$.\n- More generally, for any subsequence $S[i..j]$ that is too short to form a hairpin loop, i.e., $j-i-1 < 3$, the only possible structure is the one with no base pairs, having energy $0$. So, for $d = j-i < 4$, $N(i,j) = \\{0: 1\\}$ and $C(i,j)$ is an empty map.\n\n**Recurrence Relations:**\nFor a subsequence $S[i..j]$ of length $d+1$, where $d = j-i$:\n\nFirst, we compute $C(i,j)$. A structure is formed on $S[i..j]$ with $S[i]$ and $S[j]$ paired only if this is permitted. Let $e(S[i], S[j])$ be the energy of the base pair. This is non-zero only for canonical pairs (G-C, A-U, G-U). The hairpin constraint requires $j - i - 1 \\ge 3$. If these conditions are met, the total energy is the sum of $e(S[i], S[j])$ and the energy of any valid structure on the internal subsequence $S[i+1..j-1]$. The distribution for $C(i,j)$ is therefore the distribution $N(i+1, j-1)$ with each energy shifted by $e(S[i], S[j])$.\n$$C(i,j)[E + e(S[i],S[j])] = N(i+1, j-1)[E]$$\nfor all energies $E$ in the distribution $N(i+1, j-1)$. If the pair $(i,j)$ is not allowed, $C(i,j)$ is empty.\n\nNext, we compute $N(i,j)$. We consider all possibilities for the nucleotide $S[j]$.\n1.  **$S[j]$ is unpaired:** The set of structures for $S[i..j]$ is identical to the set of structures for $S[i..j-1]$. Thus, we initialize the distribution for $N(i,j)$ with $N(i, j-1)$.\n2.  **$S[j]$ is paired with $S[k]$ for some $i \\le k < j$:** The non-crossing constraint implies that such a structure decomposes into two independent parts: the structure on the external subsequence $S[i..k-1]$ and the structure on the subsequence $S[k..j]$ where $(k,j)$ must be a pair. The energy distribution for the latter is given by $C(k,j)$. The total number of structures for a given energy is obtained by the convolution of the energy distributions of the two independent parts.\nThe total energy $E$ is $E_{left} + E_{bifurcation}$, where $E_{left}$ is an energy from a structure on $S[i..k-1]$ and $E_{bifurcation}$ is from a structure on $S[k..j]$ with pair $(k,j)$. The number of ways to form this is $c_{left} \\times c_{bifurcation}$.\n\nCombining these cases, the recurrence for $N(i,j)$ is:\n$$N(i,j) = N(i,j-1) \\oplus \\bigoplus_{k=i}^{j-4} \\left( N(i,k-1) \\otimes C(k,j) \\right)$$\nHere, $\\oplus$ denotes the merging of two energy distribution maps by summing counts for identical keys, and $\\otimes$ denotes the convolution of two distributions, corresponding to the multiplication of counts for all combinations of energies. The summation over $k$ runs up to $j-4$ to respect the hairpin loop constraint $j-k-1 \\ge 3$.\n\n**Final Calculation:**\nAfter filling the DP tables up to $N(0, L-1)$, we have the complete energy distribution for the entire sequence. Let this be $D_{final} = N(0, L-1)$.\n1.  We find the Minimum Free Energy, $E_{\\min} = \\min(\\text{keys}(D_{final}))$.\n2.  We then compute the frustration count $F(\\Delta)$ by summing the counts for all energies $E$ in $D_{final}$ that satisfy the condition $E_{\\min} < E \\leq E_{\\min} + \\Delta$.\n$$F(\\Delta) = \\sum_{E \\in D_{final}, E_{\\min} < E \\leq E_{\\min} + \\Delta} D_{final}[E]$$\nThis procedure gives a precise, verifiable count based entirely on the first principles laid out in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the RNA frustration count problem for a suite of test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n\n    # Allowed base pairs and their energies\n    energy_map = {\n        'AU': -2, 'UA': -2,\n        'GC': -3, 'CG': -3,\n        'GU': -1, 'UG': -1,\n    }\n\n    # Hairpin minimum loop length constraint\n    MIN_LOOP_LENGTH = 3\n\n    # Test suite\n    test_cases = [\n        (\"GCAUCU\", 1),\n        (\"GCAUCU\", 3),\n        (\"AAAAAA\", 5),\n        (\"AUGC\", 10),\n        (\"GGGAAACCC\", 2),\n        (\"GCAUUGC\", 3),\n        (\"GGGCCC\", 3),\n    ]\n\n    # --- Algorithm Implementation ---\n    \n    def get_pair_energy(b1, b2):\n        \"\"\"Returns the energy of a base pair.\"\"\"\n        return energy_map.get(b1 + b2, 0)\n\n    def add_distributions(dist1, dist2):\n        \"\"\"Merges two energy distributions.\"\"\"\n        res = dist1.copy()\n        for e, c in dist2.items():\n            res[e] = res.get(e, 0) + c\n        return res\n\n    def convolve_distributions(dist1, dist2):\n        \"\"\"Computes the convolution of two energy distributions.\"\"\"\n        res = {}\n        if not dist1 or not dist2:\n            return res\n        for e1, c1 in dist1.items():\n            for e2, c2 in dist2.items():\n                new_e = e1 + e2\n                new_c = c1 * c2\n                res[new_e] = res.get(new_e, 0) + new_c\n        return res\n\n    def compute_frustration(sequence, delta):\n        \"\"\"\n        Computes the frustration count F(Delta) for a given RNA sequence.\n        \"\"\"\n        n = len(sequence)\n        \n        # N[i][j]: energy distribution for all structures on subsequence S[i..j]\n        # C[i][j]: energy distribution for structures on S[i..j] with (i,j) paired\n        N = [[{} for _ in range(n)] for _ in range(n)]\n        C = [[{} for _ in range(n)] for _ in range(n)]\n\n        # Base case for empty subsequence (N[i][i-1]) used in recurrence\n        N_pre = {0: 1}\n\n        # Initialize for subsequences of length 1\n        for i in range(n):\n            N[i][i] = {0: 1}\n\n        # Fill DP tables by increasing subsequence length d = j - i\n        for d in range(1, n):\n            for i in range(n - d):\n                j = i + d\n\n                # Compute C[i][j]\n                if d > MIN_LOOP_LENGTH:\n                    pair_energy = get_pair_energy(sequence[i], sequence[j])\n                    if pair_energy < 0:\n                        internal_dist = N[i + 1][j - 1]\n                        C[i][j] = {e + pair_energy: c for e, c in internal_dist.items()}\n                \n                # Compute N[i][j]\n                # Case 1: j is unpaired\n                dist_N = N[i][j - 1].copy()\n\n                # Case 2: j is paired with k\n                # k must be such that j-k-1 >= MIN_LOOP_LENGTH  => k <= j - MIN_LOOP_LENGTH - 1\n                for k in range(i, j - MIN_LOOP_LENGTH):\n                    dist_bifurcation = C[k][j]\n                    if not dist_bifurcation:\n                        continue\n                    \n                    dist_left = N[i][k - 1] if k > i else N_pre\n                    \n                    # Convolve left and bifurcation distributions\n                    convolution_result = convolve_distributions(dist_left, dist_bifurcation)\n                    dist_N = add_distributions(dist_N, convolution_result)\n                \n                N[i][j] = dist_N\n\n        final_distribution = N[0][n - 1]\n        \n        if not final_distribution:\n            return 0\n\n        # Calculate E_min and the frustration count F(Delta)\n        e_min = min(final_distribution.keys())\n        frustration_count = 0\n        for energy, count in final_distribution.items():\n            if e_min < energy <= e_min + delta:\n                frustration_count += count\n        \n        return frustration_count\n\n    results = []\n    for seq, delta_val in test_cases:\n        result = compute_frustration(seq, delta_val)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An RNA's biological role is often dictated by its complex three-dimensional shape, which is stabilized by tertiary contacts between distant parts of the chain. This practice introduces a powerful technique to predict these contacts using evolutionary data. You will analyze a Multiple Sequence Alignment (MSA) to find pairs of positions that mutate in a correlated fashion, a signal known as co-evolution, by calculating their Mutual Information ($I$). Critically, you will also implement the Average Product Correction (APC) to distinguish true, direct couplings from indirect, phylogenetic noise, a vital step in modern structural bioinformatics .",
            "id": "2427140",
            "problem": "You are given the task of predicting tertiary contact points in ribonucleic acid (RNA) using co-evolutionary signals computed from a multiple sequence alignment (MSA). The goal is to design and implement a program that, from first principles, constructs a pairwise co-evolution score using information-theoretic quantities and then applies a bias correction to better isolate direct positional couplings. The final predictions are evaluated against a provided ground-truth set of contacts. Your approach must be derived from the following fundamental bases and core definitions, without using any shortcut formulas not derived from these bases within your reasoning: the Central Dogma of Molecular Biology (deoxyribonucleic acid to ribonucleic acid to protein), the definition of Shannon entropy, and the definition of mutual information.\n\nThe required base definitions are as follows. For a discrete random variable $X$ with probability mass function $p(x)$ over a finite alphabet, the Shannon entropy is defined as\n$$\nH(X) = - \\sum_{x} p(x) \\log p(x),\n$$\nwhere $\\log$ denotes the natural logarithm with base $e$. For a pair of discrete random variables $(X,Y)$ with joint probability mass function $p(x,y)$ and marginals $p(x)$ and $p(y)$, the mutual information is\n$$\nI(X;Y) = \\sum_{x}\\sum_{y} p(x,y)\\,\\log\\frac{p(x,y)}{p(x)\\,p(y)}.\n$$\nYou must use these definitions as the fundamental base to construct your algorithm.\n\nYou are to implement the following conceptual steps in a principled way.\n- From the MSA, treat each alignment column as a discrete random variable over the alphabet $\\{\\texttt{A}, \\texttt{C}, \\texttt{G}, \\texttt{U}, \\texttt{-}\\}$ of size $q=5$, where $\\texttt{-}$ denotes a gap. Let the alignment contain $N$ sequences of equal length $L$.\n- Estimate joint distributions $p_{ij}(x,y)$ for each position pair $(i,j)$ with $1 \\le i < j \\le L$ using count-based frequency estimation with a Dirichlet (additive) pseudocount prior. Specifically, implement Laplace smoothing by adding a pseudocount $\\alpha$ to every pair state $(x,y)$ and then normalizing to obtain a valid joint probability distribution. Let the corresponding marginals be computed by summing the joint probabilities over $y$ or $x$, respectively. In all computations, use the natural logarithm with base $e$.\n- Use the fundamental definition of mutual information $I$ above to compute an initial co-evolution score for each pair $(i,j)$.\n- Derive and apply a correction that removes average background biases per position to better approximate direct couplings before tertiary contact prediction. The correction must be derived from the principle of subtracting a separable average bias component built from per-position means and the global mean of the pairwise score matrix to diminish spurious correlations introduced by overall positional variability and finite-sample effects. Your derivation must make no appeal to a shortcut formula; rather, you must justify the final corrective form you implement from the stated principles.\n- Exclude trivial local neighbors by only considering pairs with sequence separation at least $s$, i.e., pairs satisfying $|i-j|\\ge s$.\n- For each test case, rank the eligible position pairs by the corrected score in descending order. Break ties deterministically by increasing lexicographic order of $(i,j)$ with $i<j$.\n- For each test case, report the precision at top-$k$ predictions, defined as the number of correctly predicted contacts among the top-$k$ ranked pairs divided by $k$. Report this as a decimal (not a percentage).\n\nAll answers are unitless. Angles are not involved. Every fraction should be expressed as a decimal number. Use $s=3$, $k=2$, and a pseudocount $\\alpha=0.5$.\n\nTest suite. Your program must process the following three test cases. Each test case is an independent MSA with its own ground-truth contact set. Positions are indexed starting from $1$. The alphabet is fixed to $\\{\\texttt{A},\\texttt{C},\\texttt{G},\\texttt{U},\\texttt{-}\\}$ for all test cases. For every case, compute the precision at top-$k$ according to the above specification.\n\n- Test case A (happy path; clear co-evolution signal). Let $L=10$. The MSA consists of $N=12$ sequences:\n  - \"AACGUCCGUA\"\n  - \"UCGCGAGUGU\"\n  - \"GGAUAUAACC\"\n  - \"CUUGCGCCAG\"\n  - \"ACUCUUGGGA\"\n  - \"UAGGCCCUUU\"\n  - \"GGCUUAACCG\"\n  - \"CUACAGGAGC\"\n  - \"AAUUCCAUUU\"\n  - \"UCCGGUCCGA\"\n  - \"GGGCUAGACG\"\n  - \"CUUUAGAGAC\"\n  The ground-truth tertiary contacts are the unordered pairs $\\{(2,9),(4,7)\\}$. Evaluate all pairs with $|i-j|\\ge s$.\n\n- Test case B (boundary case; no signal). Let $L=8$. The MSA consists of $N=6$ identical sequences:\n  - \"ACGUACGU\"\n  - \"ACGUACGU\"\n  - \"ACGUACGU\"\n  - \"ACGUACGU\"\n  - \"ACGUACGU\"\n  - \"ACGUACGU\"\n  The ground-truth contact set is empty, i.e., $\\{\\}$. Evaluate all pairs with $|i-j|\\ge s$.\n\n- Test case C (edge case; gaps co-vary with a true distal contact). Let $L=8$. The MSA consists of $N=10$ sequences:\n  - \"ACGUACGU\"\n  - \"CGUAGUCG\"\n  - \"GUACCAUC\"\n  - \"UACGUGAA\"\n  - \"-CGUACG-\"\n  - \"-GUAGUC-\"\n  - \"AUCGAUGU\"\n  - \"CAGUGACG\"\n  - \"GCUACGUC\"\n  - \"UGACUCAA\"\n  The ground-truth tertiary contact is $\\{(1,8)\\}$. Evaluate all pairs with $|i-j|\\ge s$.\n\nFinal output format. Your program should produce a single line of output containing the three precision values for Test cases A, B, and C, respectively, as a comma-separated list enclosed in square brackets. Each value should be rounded to exactly three decimal places. For example, an acceptable output format is \"[0.500,1.000,0.000]\".",
            "solution": "The problem requires the development of a computational method to predict tertiary contacts in an RNA molecule from a multiple sequence alignment (MSA). The approach is to be constructed from first principles of information theory and must include a principled bias correction. The validity of the problem statement is confirmed, as it is scientifically grounded in established concepts of bioinformatics and information theory, is well-posed with all necessary parameters and data specified, and is expressed in objective, formal language. The problem is therefore valid and a solution will be constructed as follows.\n\nThe core of the method is to quantify the co-evolution between pairs of residue positions in an MSA. A strong co-evolutionary signal between two positions suggests that they may be under a compensatory evolutionary pressure to maintain a structural or functional interaction, such as a tertiary contact.\n\nLet the given MSA consist of $N$ sequences, each of length $L$. The alphabet of residues is $\\mathcal{A} = \\{\\texttt{A}, \\texttt{C}, \\texttt{G}, \\texttt{U}, \\texttt{-}\\}$, with size $q=5$. Each column $i$ of the MSA, for $i \\in \\{1, \\dots, L\\}$, can be modeled as a discrete random variable $X_i$ that takes values from $\\mathcal{A}$.\n\nStep 1: Estimation of Probability Distributions\n\nTo quantify co-evolution, we first need to estimate the probability distributions for single positions and pairs of positions. For any pair of columns $(i,j)$ with $1 \\le i < j \\le L$, we estimate the joint probability mass function $p_{ij}(x,y) = P(X_i=x, X_j=y)$ for all $x, y \\in \\mathcal{A}$.\n\nA robust estimation method for these probabilities from a finite sample of $N$ sequences is crucial. We use frequency counting with an additive pseudocount, a Bayesian estimation technique corresponding to a Dirichlet prior. This method, often called Laplace smoothing, prevents zero probabilities for unobserved states, which would cause issues with logarithmic calculations.\n\nLet $f_{ij}(x,y)$ be the observed frequency count of the residue pair $(x,y)$ in columns $i$ and $j$ of the MSA. We add a pseudocount $\\alpha > 0$ to each of the $q^2$ possible pair states. The estimated joint probability is then:\n$$\np_{ij}(x,y) = \\frac{f_{ij}(x,y) + \\alpha}{N + q^2 \\alpha}\n$$\nThe denominator $N + q^2 \\alpha$ is the normalization constant, which is the sum of all raw counts ($N$) plus the sum of all pseudocounts ($q^2 \\alpha$).\n\nFrom the joint probability distribution $p_{ij}(x,y)$, we derive the marginal probability distributions $p_i(x)$ and $p_j(y)$ by summation:\n$$\np_i(x) = \\sum_{y \\in \\mathcal{A}} p_{ij}(x,y) \\quad \\text{and} \\quad p_j(y) = \\sum_{x \\in \\mathcal{A}} p_{ij}(x,y)\n$$\nThese marginals are consistent with the joint distribution, ensuring that $\\sum_{x \\in \\mathcal{A}} p_i(x) = 1$ and $\\sum_{y \\in \\mathcal{A}} p_j(y) = 1$.\n\nStep 2: Mutual Information as a Co-evolution Score\n\nMutual Information (MI) from information theory measures the statistical dependence between two random variables. The MI for the pair of MSA columns $(i,j)$ is defined as:\n$$\nMI_{ij} = I(X_i; X_j) = \\sum_{x \\in \\mathcal{A}}\\sum_{y \\in \\mathcal{A}} p_{ij}(x,y)\\,\\log\\frac{p_{ij}(x,y)}{p_i(x)\\,p_j(y)}\n$$\nwhere the logarithm is the natural logarithm (base $e$). A high $MI_{ij}$ value indicates strong statistical coupling between positions $i$ and $j$. However, MI is susceptible to background noise from phylogenetic history and finite sampling effects, which can create high scores for pairs that are not in direct contact.\n\nStep 3: Derivation of Bias Correction\n\nTo isolate the signal of direct coupling from background noise, a correction must be applied to the raw $MI$ scores. The problem specifies deriving a correction based on subtracting a separable average bias component. We hypothesize that the observed score $MI_{ij}$ is a sum of the true direct coupling signal $D_{ij}$ and a background bias term $B_{ij}$ that is approximately separable into contributions from each position.\n$$\nMI_{ij} = D_{ij} + B_{ij}\n$$\nLet us model the bias term $B_{ij}$ as a product of position-specific factors: $B_{ij} \\approx \\beta_i \\beta_j$. We assume that the direct coupling signal $D_{ij}$ is sparse (i.e., non-zero for only a few pairs corresponding to true contacts), while the background bias $B_{ij}$ is dense. Under this assumption, the average score for a position $i$ will be dominated by the bias contribution.\n\nLet $\\overline{MI}_{i\\cdot}$ denote the average MI score for position $i$ over all other positions $k \\neq i$, and $\\overline{MI}_{\\cdot\\cdot}$ be the global average MI score over all pairs $(k,l)$ with $k \\neq l$.\n$$\n\\overline{MI}_{i\\cdot} = \\frac{1}{L-1} \\sum_{k \\neq i} MI_{ik} \\approx \\frac{1}{L-1} \\sum_{k \\neq i} \\beta_i \\beta_k = \\beta_i \\left(\\frac{1}{L-1} \\sum_{k \\neq i} \\beta_k\\right) = \\beta_i \\bar{\\beta}\n$$\nAnd the global average is:\n$$\n\\overline{MI}_{\\cdot\\cdot} = \\frac{1}{L(L-1)} \\sum_{k \\neq l} MI_{kl} \\approx \\frac{1}{L(L-1)} \\sum_{k \\neq l} \\beta_k \\beta_l \\approx \\bar{\\beta}^2\n$$\nFrom these approximations, we can express the bias factors $\\beta_i$ in terms of the observable averages: $\\beta_i \\approx \\overline{MI}_{i\\cdot} / \\bar{\\beta}$. Substituting this into the bias model $B_{ij} \\approx \\beta_i \\beta_j$:\n$$\nB_{ij} \\approx \\left(\\frac{\\overline{MI}_{i\\cdot}}{\\bar{\\beta}}\\right) \\left(\\frac{\\overline{MI}_{j\\cdot}}{\\bar{\\beta}}\\right) = \\frac{\\overline{MI}_{i\\cdot}\\overline{MI}_{j\\cdot}}{\\bar{\\beta}^2}\n$$\nUsing $\\bar{\\beta}^2 \\approx \\overline{MI}_{\\cdot\\cdot}$, we arrive at an estimate for the bias term:\n$$\nB_{ij} \\approx \\frac{\\overline{MI}_{i\\cdot}\\overline{MI}_{j\\cdot}}{\\overline{MI}_{\\cdot\\cdot}}\n$$\nThe corrected score, known as the Average Product Correction (APC), is obtained by subtracting this bias term from the original MI score:\n$$\nAPC_{ij} = MI_{ij} - \\frac{\\overline{MI}_{i\\cdot}\\overline{MI}_{j\\cdot}}{\\overline{MI}_{\\cdot\\cdot}}\n$$\nThis correction effectively discounts MI scores for pairs of positions that both exhibit high average MI with all other positions, which is characteristic of background noise.\n\nStep 4: Contact Prediction and Evaluation\n\nThe final algorithm for contact prediction is as follows:\n1.  For an MSA with $N$ sequences and length $L$, calculate the raw $MI_{ij}$ score for all pairs $(i,j)$ with $1 \\le i < j \\le L$.\n2.  From the matrix of $MI$ scores, compute the APC-corrected scores, $APC_{ij}$, for all pairs.\n3.  Filter the pairs to consider only potential tertiary contacts, excluding local neighbors. We retain only pairs $(i,j)$ satisfying the sequence separation criterion $|i-j| \\ge s$, with $s=3$.\n4.  Rank the eligible pairs in descending order based on their $APC_{ij}$ score. Ties are broken by lexicographical order of the pair $(i,j)$.\n5.  Select the top $k=2$ pairs from this ranked list as the predicted contacts.\n6.  Evaluate the prediction accuracy by calculating the precision at top-$k$, which is the fraction of the top $k$ predictions that are present in the ground-truth set of contacts.\n$$\n\\text{Precision@k} = \\frac{|\\{\\text{Top-}k \\text{ Predictions}\\} \\cap \\{\\text{True Contacts}\\}|}{k}\n$$\nThis procedure is applied to each test case using the specified parameters: pseudocount $\\alpha=0.5$, separation threshold $s=3$, and evaluation rank $k=2$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the RNA contact prediction problem for all test cases.\n    \"\"\"\n\n    # --- Parameter Definition ---\n    s = 3  # Sequence separation threshold\n    k = 2  # Top-k predictions for precision calculation\n    alpha = 0.5  # Pseudocount for probability estimation\n    alphabet = {'A': 0, 'C': 1, 'G': 2, 'U': 3, '-': 4}\n    q = len(alphabet)\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"msa\": [\n                \"AACGUCCGUA\", \"UCGCGAGUGU\", \"GGAUAUAACC\", \"CUUGCGCCAG\",\n                \"ACUCUUGGGA\", \"UAGGCCCUUU\", \"GGCUUAACCG\", \"CUACAGGAGC\",\n                \"AAUUCCAUUU\", \"UCCGGUCCGA\", \"GGGCUAGACG\", \"CUUUAGAGAC\"\n            ],\n            \"contacts\": {(2, 9), (4, 7)}\n        },\n        {\n            \"name\": \"B\",\n            \"msa\": [\n                \"ACGUACGU\", \"ACGUACGU\", \"ACGUACGU\",\n                \"ACGUACGU\", \"ACGUACGU\", \"ACGUACGU\"\n            ],\n            \"contacts\": set()\n        },\n        {\n            \"name\": \"C\",\n            \"msa\": [\n                \"ACGUACGU\", \"CGUAGUCG\", \"GUACCAUC\", \"UACGUGAA\",\n                \"-CGUACG-\", \"-GUAGUC-\", \"AUCGAUGU\", \"CAGUGACG\",\n                \"GCUACGUC\", \"UGACUCAA\"\n            ],\n            \"contacts\": {(1, 8)}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        msa = case[\"msa\"]\n        true_contacts = case[\"contacts\"]\n        N = len(msa)\n        L = len(msa[0])\n\n        precision = calculate_precision(msa, true_contacts, L, N, q, alpha, s, k, alphabet)\n        results.append(f\"{precision:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_precision(msa, true_contacts, L, N, q, alpha, s, k, alphabet):\n    \"\"\"\n    Calculates the precision@k for a single test case.\n    \"\"\"\n    # 1. Convert MSA characters to integer representation for numpy\n    msa_int = np.array([[alphabet[res] for res in seq] for seq in msa], dtype=int)\n\n    # 2. Calculate Mutual Information (MI) matrix\n    mi_matrix = np.zeros((L, L))\n    for i in range(L):\n        for j in range(i + 1, L):\n            # 2a. Calculate joint frequency counts\n            freq_ij = np.zeros((q, q), dtype=float)\n            for seq_idx in range(N):\n                res_i = msa_int[seq_idx, i]\n                res_j = msa_int[seq_idx, j]\n                freq_ij[res_i, res_j] += 1\n            \n            # 2b. Estimate joint probability distribution with pseudocount\n            prob_ij = (freq_ij + alpha) / (N + q**2 * alpha)\n\n            # 2c. Estimate marginal probabilities by summing over the joint distribution\n            prob_i = prob_ij.sum(axis=1)\n            prob_j = prob_ij.sum(axis=0)\n\n            # 2d. Calculate Mutual Information\n            # Term p_i(x) * p_j(y) is calculated using an outer product.\n            # Due to alpha > 0, all probabilities are non-zero, avoiding log(0).\n            p_i_p_j = np.outer(prob_i, prob_j)\n            log_ratio = np.log(prob_ij / p_i_p_j)\n            \n            # The mutual information is the expectation of the log-ratio\n            mi = np.sum(prob_ij * log_ratio)\n            mi_matrix[i, j] = mi_matrix[j, i] = mi\n\n    # 3. Apply Average Product Correction (APC)\n    apc_matrix = np.zeros((L, L))\n    if L > 1:\n        # Calculate row/column and global averages.\n        # Note: Diagonal of mi_matrix is zero.\n        # Mean for position i is the sum of row i divided by (L-1).\n        mean_mi_i = mi_matrix.sum(axis=1) / (L - 1)\n        # Global mean is the sum of all elements divided by L*(L-1) off-diagonal pairs.\n        mean_mi_global = mi_matrix.sum() / (L * (L - 1)) if (L * (L - 1)) > 0 else 0\n\n        if mean_mi_global > 1e-9: # Avoid division by zero\n            # Correction term is outer product of position-wise means, normalized by global mean.\n            correction = np.outer(mean_mi_i, mean_mi_i) / mean_mi_global\n            apc_matrix = mi_matrix - correction\n        else: # If no background signal, APC score equals MI score\n            apc_matrix = mi_matrix\n        np.fill_diagonal(apc_matrix, 0)\n\n    # 4. Filter, rank pairs, and get top-k predictions\n    candidate_pairs = []\n    for i in range(L):\n        for j in range(i + 1, L):\n            # Apply sequence separation filter: |i-j| >= s\n            if (j - i) >= s:\n                # Store tuple for sorting: (-score, i, j)\n                # Negating score allows for descending order sort.\n                # (i, j) provides lexicographical tie-breaking.\n                candidate_pairs.append((-apc_matrix[i, j], i, j))\n    \n    candidate_pairs.sort()\n    \n    top_k_predictions = []\n    for idx in range(min(k, len(candidate_pairs))):\n        _, pred_i, pred_j = candidate_pairs[idx]\n        top_k_predictions.append(tuple(sorted((pred_i, pred_j))))\n\n    # 5. Calculate Precision@k\n    # Convert 1-based ground truth contacts to 0-based for comparison\n    true_contacts_0based = set()\n    for c1, c2 in true_contacts:\n        true_contacts_0based.add(tuple(sorted((c1 - 1, c2 - 1))))\n    \n    correct_predictions = 0\n    for pred_pair in top_k_predictions:\n        if pred_pair in true_contacts_0based:\n            correct_predictions += 1\n            \n    precision = correct_predictions / k if k > 0 else 0.0\n    return precision\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The ability to predict an RNA's biophysical properties from its sequence is crucial for designing diagnostics, therapeutics, and molecular biology experiments. This exercise guides you through building a machine learning model to predict the melting temperature ($T_m$) of an RNA duplex, a key measure of its stability. You will perform feature engineering by translating sequence information into physically meaningful descriptors, such as GC-content and nearest-neighbor frequencies, and then use ridge regression to learn the relationship between these features and $T_m$ . This hands-on task exemplifies a core workflow in bioinformatics, connecting sequence data to predictive modeling.",
            "id": "2427183",
            "problem": "You are given short ribonucleic acid (RNA) sequences composed of the four ribonucleotides adenine (A), uracil (U), guanine (G), and cytosine (C). Each sequence is assumed to form a perfectly complementary duplex with its reverse-complement at equal strand concentration under fixed buffer conditions, so that any variation in melting temperature arises only from the sequence composition and ordering. The melting temperature, denoted by $T_m$, is the temperature at which half of the duplex is denatured at thermodynamic equilibrium. Well-tested biophysical facts establish that duplex stability arises from Watson–Crick base pairing and base-stacking interactions, and that guanine–cytosine content and nearest-neighbor dinucleotide stacking make substantial contributions to stability. Your task is to implement a machine learning regressor that predicts $T_m$ from the sequence alone by learning from labeled examples.\n\nFundamental bases for the design:\n- The Central Dogma of Molecular Biology states that genetic information flows from deoxyribonucleic acid (DNA) to RNA to protein, but here we focus only on RNA base pairing into a duplex.\n- RNA duplex stability is governed by base pairing and stacking; a nearest-neighbor view treats the duplex as a sum over dinucleotide contributions along the strand, and the number of neighboring pairs is $L-1$ for a sequence of length $L$.\n- Longer duplexes and higher guanine–cytosine fraction are generally more stable, so useful features must encode length, guanine–cytosine content, and nearest-neighbor composition without assuming any specific target formula for $T_m$.\n\nYou must:\n- Represent each sequence by a fixed-length real-valued feature vector using only the sequence. At minimum, include the following:\n  - A vector of length $16$ corresponding to the normalized counts of all dinucleotides from the set $\\{ \\text{AA}, \\text{AU}, \\text{AG}, \\text{AC}, \\text{UA}, \\text{UU}, \\text{UG}, \\text{UC}, \\text{GA}, \\text{GU}, \\text{GG}, \\text{GC}, \\text{CA}, \\text{CU}, \\text{CG}, \\text{CC} \\}$, where each component equals the count of that dinucleotide divided by $(L-1)$ if $L \\ge 2$, and equals $0$ if $L &lt; 2$.\n  - The fraction of guanine–cytosine nucleotides, defined as the number of guanine or cytosine nucleotides divided by $L$ when $L \\ge 1$, and $0$ if $L = 0$.\n  - The length $L$ itself as a real-valued feature.\n  - An intercept term.\n- Fit a linear model with squared loss and $\\ell_2$ regularization (ridge regression) to training data, using a closed-form normal-equation solution. Do not use iterative training or stochastic elements.\n- Use the resulting model to predict $T_m$ for the test suite, rounding each prediction to two decimal places. Express all temperatures in degrees Celsius.\n\nMathematical and algorithmic requirements:\n- Formulate a design matrix $X \\in \\mathbb{R}^{N \\times d}$ from $N$ training sequences and $d$ features per sequence, and a label vector $y \\in \\mathbb{R}^{N}$ containing the given $T_m$ values in degrees Celsius.\n- Use $\\ell_2$ regularization with regularization strength $\\alpha &gt; 0$ applied to all coefficients except the intercept. Compute the ridge regression solution via the normal equations by solving\n$$\n\\left(X^\\top X + \\Lambda\\right) w = X^\\top y,\n$$\nwhere $w \\in \\mathbb{R}^{d}$ is the coefficient vector (including the intercept), and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix with entries $\\alpha$ for non-intercept coefficients and $0$ for the intercept coefficient. No other fitting procedures are permitted.\n\nTraining data (sequence, $T_m$ in degrees Celsius):\n- $\\text{AUAUAU}$, $22.0$\n- $\\text{GCGCGC}$, $54.0$\n- $\\text{AUGCUA}$, $34.0$\n- $\\text{GGGCCC}$, $57.0$\n- $\\text{AAAAAA}$, $18.0$\n- $\\text{CCGGCC}$, $53.0$\n- $\\text{AUGGCAU}$, $39.0$\n- $\\text{GGAUCC}$, $44.0$\n- $\\text{AUGCAUGC}$, $47.0$\n- $\\text{GCAUGCAC}$, $49.0$\n- $\\text{ACGUACGU}$, $45.0$\n- $\\text{UUUUUUUU}$, $20.0$\n- $\\text{GGGGGGGG}$, $68.0$\n- $\\text{AUAUAUAU}$, $30.0$\n- $\\text{GCGCGCGC}$, $62.0$\n- $\\text{AUGCUAGC}$, $48.0$\n- $\\text{CGAUCG}$, $41.0$\n- $\\text{AUGGCCAU}$, $53.0$\n- $\\text{AUGCUAGCUA}$, $56.0$\n- $\\text{GGCAUUGCCG}$, $63.0$\n\nTest suite (predict $T_m$ for each sequence, in degrees Celsius):\n- $\\text{AUAU}$\n- $\\text{GCGC}$\n- $\\text{AUGCAU}$\n- $\\text{GGCAUGCC}$\n- $\\text{ACACACAC}$\n- $\\text{GGAACCUU}$\n- $\\text{GC}$\n\nEdge cases covered include short sequences with $L=2$ and $L=4$, low guanine–cytosine content, and high guanine–cytosine content.\n\nYour program must:\n- Construct features exactly as specified from the sequences.\n- Fit a ridge regression model using the closed-form normal equations with a fixed $\\alpha = 10^{-3}$.\n- Predict $T_m$ for the test suite sequences in the order listed above.\n- Round each predicted $T_m$ to two decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[\\dots]$.\n\nAngle units are not applicable. All temperatures must be interpreted in degrees Celsius and reported as decimal numbers without unit symbols. No user input or external data is allowed; the program must be self-contained and deterministic.",
            "solution": "The problem presented is a classical exercise in bioinformatics and applied machine learning, requiring the construction of a predictive model for the melting temperature, $T_m$, of short RNA duplexes. The task is to be accomplished by applying ridge regression to a set of features derived from the RNA sequence. The problem statement is scientifically grounded, well-posed, and contains all necessary information for a deterministic solution. It is therefore valid. We proceed with the solution.\n\nThe objective is to find a mapping from an RNA sequence to its melting temperature, $T_m$. We will construct a linear model of the form $\\hat{T}_m = w^\\top x$, where $x$ is a feature vector derived from the sequence and $w$ is a vector of coefficients learned from data.\n\n**Step 1: Feature Engineering**\n\nThe first and most critical step is the transformation of a discrete RNA sequence into a continuous, real-valued feature vector. As stipulated, for a sequence $S$ of length $L$, the feature vector $x \\in \\mathbb{R}^{d}$ must encapsulate its length, base composition, and nearest-neighbor content. We define a feature vector of dimension $d=19$, structured as follows:\n\n1.  **Intercept Term ($x_0$)**: A constant value of $1$. This term, $x_0 = 1$, allows the model to learn a baseline $T_m$ value, independent of any sequence-specific properties.\n\n2.  **Guanine-Cytosine (GC) Fraction ($x_1$)**: This feature is defined as the ratio of the count of guanine ($G$) and cytosine ($C$) nucleotides to the total length of the sequence.\n    $$\n    x_1 = \\frac{N_G + N_C}{L} \\text{ for } L \\ge 1\n    $$\n    and $x_1 = 0$ for $L=0$. This feature captures the well-known stabilizing effect of GC pairs, which form three hydrogen bonds, compared to the two bonds in AU pairs.\n\n3.  **Sequence Length ($x_2$)**: The length $L$ of the sequence itself, i.e., $x_2 = L$. Longer duplexes are generally more stable due to a greater number of cumulative base-stacking and hydrogen-bonding interactions.\n\n4.  **Dinucleotide Frequencies ($x_3, \\dots, x_{18}$)**: This is a set of $16$ features representing the normalized frequencies of all possible dinucleotides. The set of dinucleotides is $\\{\\text{AA}, \\text{AU}, \\text{AG}, \\text{AC}, \\ldots, \\text{CC}\\}$. We establish a fixed ordering for these $16$ dinucleotides as specified in the problem statement. For each dinucleotide $D_k$ in this ordered set, its corresponding feature $x_{k+3}$ is computed as:\n    $$\n    x_{k+3} = \\frac{\\text{count}(D_k)}{L-1} \\text{ for } L \\ge 2\n    $$\n    and $x_{k+3} = 0$ for $L < 2$. The divisor $L-1$ is the total number of dinucleotide steps in a sequence of length $L$. These features represent the contribution of nearest-neighbor stacking interactions to duplex stability, which is a cornerstone of thermodynamic models for nucleic acids.\n\nThus, for any given RNA sequence, we can construct a unique $19$-dimensional feature vector $x = [x_0, x_1, \\dots, x_{18}]^\\top$.\n\n**Step 2: Model Formulation and Training**\n\nWe are provided with a training set of $N=20$ sequences and their corresponding measured $T_m$ values. We organize this data into a design matrix $X \\in \\mathbb{R}^{N \\times d}$ and a label vector $y \\in \\mathbb{R}^{N}$. Each row $i$ of $X$ is the feature vector $x^{(i)\\top}$ for the $i$-th training sequence, and the corresponding entry $y_i$ is its measured $T_m$. Here, $d=19$.\n\nWe employ ridge regression to learn the coefficient vector $w \\in \\mathbb{R}^{d}$. This model minimizes a squared-error loss function with an $\\ell_2$-norm penalty on the coefficients:\n$$\n\\min_{w} \\| Xw - y \\|_2^2 + \\alpha \\sum_{j=1}^{d-1} w_j^2\n$$\nNote that the penalty is applied to all coefficients except for the intercept $w_0$, as specified. The regularization parameter $\\alpha > 0$ controls the trade-off between fitting the data and penalizing large coefficient values, which helps to prevent overfitting.\n\nThe optimal weight vector $w$ is found by solving the normal equations, modified for ridge regression:\n$$\n\\left(X^\\top X + \\Lambda\\right) w = X^\\top y\n$$\nHere, $\\Lambda$ is a $d \\times d$ diagonal matrix implementing the selective regularization. Since our intercept term corresponds to the first feature (index $0$), the diagonal entries of $\\Lambda$ are:\n$$\n\\Lambda_{jj} = \\begin{cases} 0 & \\text{if } j=0 \\\\ \\alpha & \\text{if } j \\in \\{1, 2, \\dots, d-1\\} \\end{cases}\n$$\nThe problem specifies a fixed regularization strength $\\alpha = 10^{-3}$.\n\nThe matrix $(X^\\top X + \\Lambda)$ is symmetric and, for $\\alpha > 0$, positive definite, thus ensuring it is invertible. The solution for the weight vector $w$ is obtained by solving this linear system:\n$$\nw = \\left(X^\\top X + \\Lambda\\right)^{-1} X^\\top y\n$$\nComputationally, it is more numerically stable to solve the system directly rather than computing the matrix inverse explicitly.\n\n**Step 3: Prediction**\n\nOnce the weight vector $w$ has been determined from the training data, it constitutes our predictive model. To predict the melting temperature $\\hat{T}_m$ for a new test sequence, we first construct its feature vector $x_{\\text{test}} \\in \\mathbb{R}^{d}$ using the exact same feature engineering process. The prediction is then computed as a dot product:\n$$\n\\hat{T}_m = x_{\\text{test}}^\\top w\n$$\nThe final result for each test sequence is to be rounded to two decimal places. This deterministic procedure guarantees a reproducible outcome. The solution is implemented following these exact specifications.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a ridge regression model to predict RNA melting temperature (Tm)\n    based on sequence features, using a closed-form normal equation solution.\n    \"\"\"\n    \n    # Training data: (sequence, Tm in Celsius)\n    training_data = [\n        (\"AUAUAU\", 22.0),\n        (\"GCGCGC\", 54.0),\n        (\"AUGCUA\", 34.0),\n        (\"GGGCCC\", 57.0),\n        (\"AAAAAA\", 18.0),\n        (\"CCGGCC\", 53.0),\n        (\"AUGGCAU\", 39.0),\n        (\"GGAUCC\", 44.0),\n        (\"AUGCAUGC\", 47.0),\n        (\"GCAUGCAC\", 49.0),\n        (\"ACGUACGU\", 45.0),\n        (\"UUUUUUUU\", 20.0),\n        (\"GGGGGGGG\", 68.0),\n        (\"AUAUAUAU\", 30.0),\n        (\"GCGCGCGC\", 62.0),\n        (\"AUGCUAGC\", 48.0),\n        (\"CGAUCG\", 41.0),\n        (\"AUGGCCAU\", 53.0),\n        (\"AUGCUAGCUA\", 56.0),\n        (\"GGCAUUGCCG\", 63.0)\n    ]\n\n    # Test suite sequences for which to predict Tm\n    test_sequences = [\n        \"AUAU\",\n        \"GCGC\",\n        \"AUGCAU\",\n        \"GGCAUGCC\",\n        \"ACACACAC\",\n        \"GGAACCUU\",\n        \"GC\"\n    ]\n    \n    # Regularization parameter\n    alpha = 1e-3\n    \n    # Ordered list of dinucleotides for feature vector construction\n    dinucl_order = [\n        'AA', 'AU', 'AG', 'AC', 'UA', 'UU', 'UG', 'UC', \n        'GA', 'GU', 'GG', 'GC', 'CA', 'CU', 'CG', 'CC'\n    ]\n    dinucl_map = {d: i for i, d in enumerate(dinucl_order)}\n    num_features = 1 + 1 + 1 + len(dinucl_order) # Intercept, GC-frac, length, dinucleotides\n\n    def featurize(sequence: str) -> np.ndarray:\n        \"\"\"\n        Converts an RNA sequence into a fixed-length feature vector.\n        Feature vector order: [intercept, gc_fraction, length, dinucleotide_freqs...]\n        \"\"\"\n        features = np.zeros(num_features)\n        L = len(sequence)\n\n        # 1. Intercept term (index 0)\n        features[0] = 1.0\n\n        # 2. GC-fraction (index 1)\n        if L > 0:\n            gc_count = sequence.count('G') + sequence.count('C')\n            features[1] = gc_count / L\n        else:\n            features[1] = 0.0\n\n        # 3. Length (index 2)\n        features[2] = float(L)\n\n        # 4. Dinucleotide frequencies (indices 3 to 18)\n        if L >= 2:\n            counts = {d: 0 for d in dinucl_order}\n            for i in range(L - 1):\n                dinuc = sequence[i:i+2]\n                if dinuc in counts:\n                    counts[dinuc] += 1\n            \n            # Normalize counts\n            for dinuc, count in counts.items():\n                features[3 + dinucl_map[dinuc]] = count / (L - 1)\n        \n        return features\n\n    # --- Model Training ---\n\n    # Construct the design matrix X and label vector y\n    num_training_samples = len(training_data)\n    X = np.zeros((num_training_samples, num_features))\n    y = np.zeros(num_training_samples)\n\n    for i, (seq, tm) in enumerate(training_data):\n        X[i, :] = featurize(seq)\n        y[i] = tm\n    \n    # Construct the regularization matrix Lambda\n    # The intercept (at index 0) is not regularized\n    Lambda = np.diag([0.0] + [alpha] * (num_features - 1))\n    \n    # Solve the normal equations: (X^T X + Lambda) w = X^T y\n    XTX = X.T @ X\n    XTy = X.T @ y\n    \n    A = XTX + Lambda\n    \n    # Solve for the weight vector w\n    try:\n        w = np.linalg.solve(A, XTy)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudoinverse if solving fails, though unlikely with regularization\n        w = np.linalg.pinv(A) @ XTy\n\n    # --- Prediction ---\n\n    results = []\n    for seq in test_sequences:\n        # Generate feature vector for the test sequence\n        x_test = featurize(seq)\n        \n        # Predict Tm\n        tm_pred = x_test @ w\n        \n        # Round to two decimal places and store\n        results.append(round(tm_pred, 2))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}