{
    "hands_on_practices": [
        {
            "introduction": "Before applying any optimization algorithm, it is crucial to understand the objective function we aim to minimize. In non-linear least squares, this objective is the sum of squared residuals. This first exercise provides foundational practice in calculating the residual vector, which measures the discrepancy between your model's predictions and the actual observed data for a given set of parameters. ",
            "id": "2214281",
            "problem": "In the context of non-linear least squares fitting, a common task is to minimize the sum of the squares of residuals. Consider a model that describes a certain physical phenomenon, given by the function $f(x, \\beta) = \\beta_1 \\sqrt{x} + \\beta_2$, where $\\beta = (\\beta_1, \\beta_2)^T$ is the vector of parameters to be determined.\n\nAn experiment yields two data points $(x_i, y_i)$: the first point is $(4, 5)$ and the second point is $(9, 7)$.\n\nThe residual for the $i$-th data point is defined as $r_i(\\beta) = y_i - f(x_i, \\beta)$. The residual vector $r(\\beta)$ is a column vector whose components are the individual residuals $r_i(\\beta)$.\n\nGiven an initial estimate for the parameters $\\beta^{(0)} = (1, 3)^T$, calculate the corresponding residual vector $r(\\beta^{(0)})$. Present your final answer as a row matrix with two elements, corresponding to the components of the residual vector.",
            "solution": "We are given the model $f(x,\\beta)=\\beta_{1}\\sqrt{x}+\\beta_{2}$, residuals $r_{i}(\\beta)=y_{i}-f(x_{i},\\beta)$, data points $(x_{1},y_{1})=(4,5)$ and $(x_{2},y_{2})=(9,7)$, and the initial estimate $\\beta^{(0)}=(\\beta_{1}^{(0)},\\beta_{2}^{(0)})^{T}=(1,3)^{T}$.\n\nFirst, we compute the model's predictions for each data point using the initial parameter estimate $\\beta^{(0)}$:\n$$\nf(x_1, \\beta^{(0)}) = 1 \\cdot \\sqrt{4} + 3 = 1 \\cdot 2 + 3 = 5\n$$\n$$\nf(x_2, \\beta^{(0)}) = 1 \\cdot \\sqrt{9} + 3 = 1 \\cdot 3 + 3 = 6\n$$\nNext, we calculate the residuals $r_i(\\beta^{(0)}) = y_i - f(x_i, \\beta^{(0)})$:\n$$\nr_1(\\beta^{(0)}) = y_1 - f(x_1, \\beta^{(0)}) = 5 - 5 = 0\n$$\n$$\nr_2(\\beta^{(0)}) = y_2 - f(x_2, \\beta^{(0)}) = 7 - 6 = 1\n$$\nThe residual vector is $\\mathbf{r}(\\beta^{(0)}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Presented as a row matrix with two elements, this is $\\begin{pmatrix}0  1\\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0  1\\end{pmatrix}}$$"
        },
        {
            "introduction": "With a grasp of residuals, we can now dive into the core mechanics of the Gauss-Newton method itself. This iterative algorithm refines parameter estimates by repeatedly solving a linearized version of the problem. This practice will guide you through a complete, single iteration of the method, from computing the Jacobian matrix to calculating the update step that brings you closer to the best-fit parameters. ",
            "id": "2214282",
            "problem": "In an experimental study, a certain physical process is modeled by the function $f(x; a) = \\frac{x}{1+ax}$, where $a$ is an unknown parameter to be determined. A researcher has collected two data points $(x_i, y_i)$: the first point is $(1, 0.5)$ and the second point is $(2, 0.8)$.\n\nTo find the optimal value of the parameter $a$ that best fits the data in a least-squares sense, the researcher decides to use the Gauss-Newton method. Starting with an initial guess of $a_0 = 1$, perform exactly one iteration of the Gauss-Newton method to find the updated estimate for the parameter, denoted as $a_1$.\n\nExpress your answer for $a_1$ as an exact fraction in simplest form.",
            "solution": "We model the data with the function $f(x;a) = \\frac{x}{1+ax}$. Following the standard definition, the residual for the $i$-th data point is $r_i(a) = y_i - f(x_i; a)$. The Gauss-Newton update step $\\Delta a$ is found by solving the normal equation $(J^T J) \\Delta a = J^T r$, where $J$ is the Jacobian of the model function $f$ and $r$ is the residual vector, both evaluated at the current guess $a_0$. The new estimate is $a_1 = a_0 + \\Delta a$.\n\nFirst, we compute the Jacobian $J$, which contains the partial derivatives of the model function $f(x;a)$ with respect to the parameter $a$:\n$$\n\\frac{\\partial f}{\\partial a} = \\frac{\\partial}{\\partial a} \\left( \\frac{x}{1+ax} \\right) = -\\frac{x^2}{(1+ax)^2}\n$$\nWith data points $(x_1, y_1) = (1, 0.5)$ and $(x_2, y_2) = (2, 0.8)$, and initial guess $a_0 = 1$, we evaluate the Jacobian entries:\n$$\nJ_1 = \\left. -\\frac{x_1^2}{(1+ax_1)^2} \\right|_{a=1} = -\\frac{1^2}{(1+1 \\cdot 1)^2} = -\\frac{1}{4}\n$$\n$$\nJ_2 = \\left. -\\frac{x_2^2}{(1+ax_2)^2} \\right|_{a=1} = -\\frac{2^2}{(1+1 \\cdot 2)^2} = -\\frac{4}{9}\n$$\nNext, we compute the residuals $r_i = y_i - f(x_i; a_0)$:\n$$\nr_1 = 0.5 - f(1; 1) = 0.5 - \\frac{1}{1+1} = 0.5 - 0.5 = 0\n$$\n$$\nr_2 = 0.8 - f(2; 1) = 0.8 - \\frac{2}{1+2} = \\frac{4}{5} - \\frac{2}{3} = \\frac{12-10}{15} = \\frac{2}{15}\n$$\nNow we form the quantities $J^T J$ and $J^T r$:\n$$\nJ^T J = J_1^2 + J_2^2 = \\left(-\\frac{1}{4}\\right)^2 + \\left(-\\frac{4}{9}\\right)^2 = \\frac{1}{16} + \\frac{16}{81} = \\frac{81+256}{1296} = \\frac{337}{1296}\n$$\n$$\nJ^T r = J_1 r_1 + J_2 r_2 = \\left(-\\frac{1}{4}\\right)(0) + \\left(-\\frac{4}{9}\\right)\\left(\\frac{2}{15}\\right) = -\\frac{8}{135}\n$$\nWe solve for the update step $\\Delta a = (J^T J)^{-1} (J^T r)$:\n$$\n\\Delta a = \\frac{J^T r}{J^T J} = \\frac{-8/135}{337/1296} = -\\frac{8}{135} \\cdot \\frac{1296}{337} = -\\frac{384}{1685}\n$$\nFinally, the updated estimate is:\n$$\na_1 = a_0 + \\Delta a = 1 - \\frac{384}{1685} = \\frac{1685-384}{1685} = \\frac{1301}{1685}\n$$",
            "answer": "$$\\boxed{\\frac{1301}{1685}}$$"
        },
        {
            "introduction": "The Gauss-Newton method is powerful, but it is not infallible; its performance can be highly sensitive to the initial guess and the specific nature of the problem. This exercise explores a classic failure mode where the algorithm, instead of converging to the correct solution, becomes trapped in an oscillation. By analyzing this hypothetical scenario, you will gain a deeper appreciation for the method's limitations and understand the motivation behind more robust techniques like damped Gauss-Newton or the Levenberg-Marquardt algorithm. ",
            "id": "2214263",
            "problem": "An engineer is using the Gauss-Newton algorithm to solve a simple non-linear least squares problem. The goal is to fit a model $g(t; x) = A \\arctan(x t)$ to a single data point $(t_1, y_1) = (1, 0)$. The parameter to be determined is $x$, and the constant $A$ is fixed at $A=1$. The objective is to find the value of $x$ that minimizes the sum of squared residuals, defined as $F(x) = \\frac{1}{2} [g(t_1; x) - y_1]^2$. The true minimum for this problem is clearly at $x = 0$.\n\nHowever, when the engineer applies the Gauss-Newton method starting from a specific initial guess $x_0  0$, they observe that the algorithm fails to converge to the minimum. Instead, the iterates become trapped in a stable 2-cycle, oscillating perpetually between the initial guess $x_0$ and its negative, $-x_0$.\n\nDetermine the value of this specific initial guess $x_0$ that causes the Gauss-Newton method to immediately enter this 2-cycle. Express your answer as a number rounded to four significant figures.",
            "solution": "The model is $g(t; x) = \\arctan(x t)$ with $A=1$, and the single datum is $(t_{1}, y_{1}) = (1, 0)$. The residual is therefore\n$$\nr(x) = g(1; x) - y_{1} = \\arctan(x).\n$$\nThe scalar Gauss-Newton step for minimizing $F(x) = \\frac{1}{2} r(x)^{2}$ solves the linearized least squares $r(x) + J(x) p \\approx 0$, where $J(x)$ is the Jacobian of the model function, which in this case is the same as the derivative of the residual, $J(x) = \\frac{dr}{dx}$. Hence, the update step is\n$$\np = -\\frac{r(x)}{J(x)},\n$$\nand the next iterate is\n$$\nx_{+} = x + p = x - \\frac{r(x)}{J(x)}.\n$$\nHere $J(x) = \\frac{d}{dx}\\arctan(x) = \\frac{1}{1 + x^{2}}$, so the Gauss-Newton iteration map is\n$$\nT(x) = x - \\frac{\\arctan(x)}{\\frac{1}{1 + x^{2}}} = x - (1 + x^{2}) \\arctan(x).\n$$\nA 2-cycle $\\{x_{0}, -x_{0}\\}$ occurs immediately when $T(x_{0}) = -x_{0}$, which gives\n$$\n-x_{0} = x_{0} - (1 + x_{0}^{2}) \\arctan(x_{0}) \\;\\;\\Longrightarrow\\;\\; (1 + x_{0}^{2}) \\arctan(x_{0}) = 2 x_{0}.\n$$\nLet $x_{0} = \\tan(\\theta)$ with $\\theta \\in (0, \\frac{\\pi}{2})$. Using $1 + \\tan^{2}(\\theta) = \\sec^{2}(\\theta)$ and $\\arctan(\\tan(\\theta)) = \\theta$, the equation becomes\n$$\n\\sec^{2}(\\theta)\\,\\theta = 2 \\tan(\\theta) \\;\\;\\Longrightarrow\\;\\; \\theta = 2 \\sin(\\theta)\\cos(\\theta) = \\sin(2\\theta).\n$$\nBesides the trivial solution $\\theta = 0$ (which corresponds to $x=0$), the unique nonzero solution in $(0, \\frac{\\pi}{2})$ satisfies $\\theta = \\sin(2\\theta)$. Solving numerically yields\n$$\n\\theta \\approx 0.947745 \\text{ rad},\n$$\nso\n$$\nx_{0} = \\tan(\\theta) \\approx \\tan(0.947745) \\approx 1.391740\\ldots\n$$\nRounded to four significant figures, this gives the required initial guess.",
            "answer": "$$\\boxed{1.392}$$"
        }
    ]
}