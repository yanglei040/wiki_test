{
    "hands_on_practices": [
        {
            "introduction": "要真正理解一个算法，最好的方法莫过于亲手执行它的每一步。这个练习将引导你完成随机梯度下降（SGD）最核心的操作：基于单个数据点计算梯度并更新模型参数。通过这个简单的计算，你将直观地感受 SGD 是如何在一次迭代中“学习”的。",
            "id": "2206637",
            "problem": "一个迭代优化算法被用来寻找一个参数 $x$ 以最小化一个成本函数 $F(x)$。总成本函数是几个分量函数的平均值：$F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$。在这个具体情况下，分量函数是二次的，由 $f_i(x) = (x - c_i)^2$ 给出，其中常数 $c_i = i$，$i = 1, 2, \\dots, 10$，因此 $N=10$。\n\n优化过程从参数的初始值 $x_0$ 开始。在每一步中，新的估计值 $x_{k+1}$ 是通过仅使用一个随机选择的分量函数 $f_j(x)$ 从当前估计值 $x_k$ 计算出来的。更新规则定义如下：\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\n其中 $\\eta$ 是一个常数，称为学习率。\n\n给定初始参数值为 $x_0 = 10.0$ 和学习率为 $\\eta = 0.1$，计算经过一次更新步骤后的参数 $x_1$ 的值。对于这第一步，使用的分量函数是索引为 $j=5$ 的 $f_j(x)$。",
            "solution": "给定的分量函数形式为 $f_{i}(x) = (x - c_{i})^{2}$，其中 $c_{i} = i$。对于第一次更新，选择的索引是 $j=5$，因此 $f_{5}(x) = (x - 5)^{2}$。\n\n更新规则是\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\n使用幂法则和链式法则，所选分量的导数是\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\n在当前迭代值 $x_{0} = 10$ 处求值，得到\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\n当学习率 $\\eta = 0.1$ 时，更新变为\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\n因此，在使用 $f_5$ 进行一次更新步骤后，参数值为 $x_{1} = 9$。",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "我们已经知道如何迈出一步，但这一步的方向“正确”吗？这个练习深入探讨了 SGD 算法中“随机”一词的统计学本质。我们将计算随机梯度估计量的方差，从而量化在单个样本上计算出的梯度与真实全体梯度之间的“噪音”或差异，这正是理解 SGD 为何高效及其“抖动”下降行为的关键。",
            "id": "2206620",
            "problem": "在许多机器学习问题中，目标是最小化一个损失函数 $F(x)$，其形式为该数据集上所有样本损失的平均值。其常见形式为 $F(x) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x)$，其中 $f_i(x)$ 是与第 $i$ 个数据点相关的损失， $x$ 是模型参数。\n\n考虑一个简化的一维问题，我们希望找到最小化损失函数的参数 $x$。该数据集仅包含两个数据点（$N=2$），由此可得以下分量损失函数：\n$$f_1(x) = (x-2)^2$$\n$$f_2(x) = (x+2)^2$$\n因此，总损失函数为 $F(x) = \\frac{1}{2} (f_1(x) + f_2(x))$。\n\n随机梯度下降（SGD）是一种迭代优化算法，在每一步中近似 $F(x)$ 的真实梯度。在其最简单的形式中，一个随机梯度估计量（我们记为 $g(x)$）的计算方法是：首先从 $\\{1, 2\\}$ 中均匀随机地选择一个索引 $i$，然后计算相应分量函数的梯度，即 $g(x) = \\nabla f_i(x)$。在这个一维情况下，梯度算子 $\\nabla$ 就是对 $x$ 的导数，即 $\\frac{d}{dx}$。\n\n计算随机梯度估计量 $g(x)$ 在特定点 $x = 1$ 处的方差。",
            "solution": "我们已知 $f_{1}(x)=(x-2)^{2}$ 和 $f_{2}(x)=(x+2)^{2}$，随机梯度估计量 $g(x)$ 的定义为从 $\\{1,2\\}$ 中均匀选择 $i$，并设 $g(x)=\\frac{d}{dx}f_{i}(x)$。首先计算分量梯度：\n$$\n\\frac{d}{dx}f_{1}(x)=2(x-2), \\quad \\frac{d}{dx}f_{2}(x)=2(x+2).\n$$\n在 $x=1$ 处，$g(1)$ 的取值为\n$$\ng(1)=2(1-2)=-2 \\quad \\text{with probability } \\frac{1}{2}, \\quad g(1)=2(1+2)=6 \\quad \\text{with probability } \\frac{1}{2}.\n$$\n计算 $g(1)$ 的均值：\n$$\n\\mathbb{E}[g(1)]=\\frac{1}{2}(-2)+\\frac{1}{2}(6)=2.\n$$\n这等于 $F$ 在 $x=1$ 处的真实梯度，因为\n$$\nF'(x)=\\frac{1}{2}\\left(2(x-2)+2(x+2)\\right)=2x \\implies F'(1)=2.\n$$\n计算二阶矩：\n$$\n\\mathbb{E}[g(1)^{2}]=\\frac{1}{2}\\left((-2)^{2}+6^{2}\\right)=\\frac{1}{2}(4+36)=20.\n$$\n因此，方差为\n$$\n\\operatorname{Var}(g(1))=\\mathbb{E}[g(1)^{2}]-\\left(\\mathbb{E}[g(1)]\\right)^{2}=20-2^{2}=16.\n$$\n因此，随机梯度估计量在 $x=1$ 处的方差为 $16$。",
            "answer": "$$\\boxed{16}$$"
        },
        {
            "introduction": "基于梯度的优化方法通常被比作“下山”，每一步都应该使我们更接近谷底（即更低的损失）。但对于 SGD，情况总是如此吗？这个练习将挑战这一直观假设，你将通过计算发现，即使一个 SGD 步骤成功降低了单个样本的损失，它也可能导致整体损失函数的意外增加。这个看似矛盾的现象揭示了 SGD 优化路径的一个深刻特性。",
            "id": "2206653",
            "problem": "在机器学习领域，优化算法通过最小化损失函数来训练模型参数。考虑一个具有单一标量参数 $w$ 的简单模型。目标是最小化总损失函数 $F(w)$，它被定义为在一个包含两个数据点的小型数据集上各个损失函数的平均值。总损失函数由下式给出：\n\n$$F(w) = \\frac{1}{2}\\left[f_1(w) + f_2(w)\\right]$$\n\n与这两个数据点相关的各个损失函数是：\n\n$$f_1(w) = \\frac{1}{2}(w - 2)^2$$\n$$f_2(w) = \\frac{1}{2}(w - 10)^2$$\n\n训练过程从初始参数值 $w_0 = 3$ 开始。使用学习率为 $\\eta = 2$ 的随机梯度下降 (SGD) 算法执行单个更新步骤。对于本次特定的更新，梯度仅使用第一个数据点的损失函数 $f_1(w)$ 来计算。\n\n计算由这次 SGD 单步更新导致的总损失函数值的变化量 $F(w_1) - F(w_0)$。将您的最终答案四舍五入到三位有效数字。",
            "solution": "我们已知 $F(w)=\\frac{1}{2}\\left[f_{1}(w)+f_{2}(w)\\right]$，其中 $f_{1}(w)=\\frac{1}{2}(w-2)^{2}$ 且 $f_{2}(w)=\\frac{1}{2}(w-10)^{2}$。初始参数为 $w_{0}=3$。学习率为 $\\eta=2$ 的单步 SGD 仅使用 $f_{1}$ 的梯度。\n\n一维 SGD 更新规则为\n$$\nw_{1}=w_{0}-\\eta\\,\\frac{d f_{1}}{d w}\\bigg|_{w=w_{0}}.\n$$\n计算导数：\n$$\n\\frac{d f_{1}}{d w}=\\frac{d}{d w}\\left[\\frac{1}{2}(w-2)^{2}\\right]=(w-2).\n$$\n在 $w_{0}=3$ 处求值：\n$$\n\\frac{d f_{1}}{d w}\\bigg|_{w=3}=3-2=1.\n$$\n因此更新后的参数为\n$$\nw_{1}=3-2\\cdot 1=1.\n$$\n\n现在计算 $F(w_{0})$：\n$$\nf_{1}(3)=\\frac{1}{2}(3-2)^{2}=\\frac{1}{2},\\quad f_{2}(3)=\\frac{1}{2}(3-10)^{2}=\\frac{1}{2}\\cdot 49=\\frac{49}{2},\n$$\n$$\nF(3)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{49}{2}\\right)=\\frac{1}{2}\\cdot\\frac{50}{2}=\\frac{1}{2}\\cdot 25=\\frac{25}{2}.\n$$\n\n计算 $F(w_{1})$：\n$$\nf_{1}(1)=\\frac{1}{2}(1-2)^{2}=\\frac{1}{2},\\quad f_{2}(1)=\\frac{1}{2}(1-10)^{2}=\\frac{1}{2}\\cdot 81=\\frac{81}{2},\n$$\n$$\nF(1)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{81}{2}\\right)=\\frac{1}{2}\\cdot\\frac{82}{2}=\\frac{1}{2}\\cdot 41=\\frac{41}{2}.\n$$\n\n因此，总损失的变化量为\n$$\nF(w_{1})-F(w_{0})=\\frac{41}{2}-\\frac{25}{2}=\\frac{16}{2}=8.\n$$\n四舍五入到三位有效数字，结果是 $8.00$。",
            "answer": "$$\\boxed{8.00}$$"
        }
    ]
}