{
    "hands_on_practices": [
        {
            "introduction": "Stochastic Gradient Descent is powerful because of its efficiency. Instead of calculating the true gradient from the entire dataset, which can be computationally expensive, we approximate it using just a single data point or a small 'mini-batch'. This first exercise provides hands-on practice with this core mechanic, demonstrating how to compute a single update step based on a single component of the overall objective function.  This calculation is the fundamental building block of the entire SGD process.",
            "id": "2206637",
            "problem": "An iterative optimization algorithm is used to find a parameter $x$ that minimizes a cost function. The total cost function is an average of several component functions: $F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$. In this specific case, the component functions are quadratic and given by $f_i(x) = (x - c_i)^2$, where the constants are $c_i = i$ for $i = 1, 2, \\dots, 10$, and thus $N=10$.\n\nThe optimization process starts with an initial guess for the parameter, $x_0$. At each step, a new estimate, $x_{k+1}$, is calculated from the current estimate, $x_k$, by using only a single, randomly chosen component function, $f_j(x)$. The update rule is defined as:\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\nwhere $\\eta$ is a constant known as the learning rate.\n\nGiven an initial parameter value of $x_0 = 10.0$ and a learning rate of $\\eta = 0.1$, compute the value of the parameter $x_1$ after one update step. For this first step, the component function used is $f_j(x)$ with the index $j=5$.",
            "solution": "We are given component functions of the form $f_{i}(x) = (x - c_{i})^{2}$ with $c_{i} = i$. For the first update, the chosen index is $j=5$, so $f_{5}(x) = (x - 5)^{2}$.\n\nThe update rule is\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\nUsing the power rule and chain rule, the derivative of the chosen component is\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\nEvaluating at the current iterate $x_{0} = 10$ gives\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\nWith learning rate $\\eta = 0.1$, the update becomes\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\nThus, after one update step using $f_{5}$, the parameter value is $x_{1} = 9$.",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "After performing an update step, it is natural to assume we have moved closer to our goal of minimizing the total loss. However, the 'stochastic' nature of SGD introduces a fascinating subtlety. This practice problem directly confronts a common misconception by showing that a single SGD update, while correctly following the gradient of one data point, is not guaranteed to decrease the overall loss function.  Understanding this is key to grasping why SGD's path towards the minimum is often noisy and zig-zagging, rather than a smooth descent.",
            "id": "2206653",
            "problem": "In the field of machine learning, optimization algorithms are used to train model parameters by minimizing a loss function. Consider a simple model with a single scalar parameter, $w$. The goal is to minimize a total loss function, $F(w)$, which is defined as the average of the individual loss functions over a small dataset of two data points. The total loss function is given by:\n\n$$F(w) = \\frac{1}{2}\\left[f_1(w) + f_2(w)\\right]$$\n\nThe individual loss functions associated with the two data points are:\n\n$$f_1(w) = \\frac{1}{2}(w - 2)^2$$\n$$f_2(w) = \\frac{1}{2}(w - 10)^2$$\n\nThe training process begins with an initial parameter value of $w_0 = 3$. A single update step is performed using the Stochastic Gradient Descent (SGD) algorithm with a learning rate of $\\eta = 2$. For this specific update, the gradient is computed using only the loss function of the first data point, $f_1(w)$.\n\nCalculate the change in the value of the total loss function, $F(w_1) - F(w_0)$, that results from this single SGD update. Round your final answer to three significant figures.",
            "solution": "We are given $F(w)=\\frac{1}{2}\\left[f_{1}(w)+f_{2}(w)\\right]$ with $f_{1}(w)=\\frac{1}{2}(w-2)^{2}$ and $f_{2}(w)=\\frac{1}{2}(w-10)^{2}$. The initial parameter is $w_{0}=3$. A single SGD step with learning rate $\\eta=2$ uses only the gradient of $f_{1}$.\n\nThe SGD update rule in one dimension is\n$$\nw_{1}=w_{0}-\\eta\\,\\frac{d f_{1}}{d w}\\bigg|_{w=w_{0}}.\n$$\nCompute the derivative:\n$$\n\\frac{d f_{1}}{d w}=\\frac{d}{d w}\\left[\\frac{1}{2}(w-2)^{2}\\right]=(w-2).\n$$\nEvaluate at $w_{0}=3$:\n$$\n\\frac{d f_{1}}{d w}\\bigg|_{w=3}=3-2=1.\n$$\nThus the updated parameter is\n$$\nw_{1}=3-2\\cdot 1=1.\n$$\n\nNow compute $F(w_{0})$:\n$$\nf_{1}(3)=\\frac{1}{2}(3-2)^{2}=\\frac{1}{2},\\quad f_{2}(3)=\\frac{1}{2}(3-10)^{2}=\\frac{1}{2}\\cdot 49=\\frac{49}{2},\n$$\n$$\nF(3)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{49}{2}\\right)=\\frac{1}{2}\\cdot\\frac{50}{2}=\\frac{1}{2}\\cdot 25=\\frac{25}{2}.\n$$\n\nCompute $F(w_{1})$:\n$$\nf_{1}(1)=\\frac{1}{2}(1-2)^{2}=\\frac{1}{2},\\quad f_{2}(1)=\\frac{1}{2}(1-10)^{2}=\\frac{1}{2}\\cdot 81=\\frac{81}{2},\n$$\n$$\nF(1)=\\frac{1}{2}\\left(\\frac{1}{2}+\\frac{81}{2}\\right)=\\frac{1}{2}\\cdot\\frac{82}{2}=\\frac{1}{2}\\cdot 41=\\frac{41}{2}.\n$$\n\nTherefore, the change in the total loss is\n$$\nF(w_{1})-F(w_{0})=\\frac{41}{2}-\\frac{25}{2}=\\frac{16}{2}=8.\n$$\nRounded to three significant figures, this is $8.00$.",
            "answer": "$$\\boxed{8.00}$$"
        },
        {
            "introduction": "If individual SGD steps don't always reduce the total loss, why does the algorithm work at all? The answer lies in statistics: while any single stochastic gradient is a 'noisy' estimate of the true gradient, it is often an *unbiased* estimate, meaning it points in the correct direction on average. This exercise allows you to quantify this 'noise' by calculating the variance of the stochastic gradient estimator.  This provides a concrete mathematical basis for the seemingly random, yet ultimately effective, behavior of SGD.",
            "id": "2206620",
            "problem": "In many machine learning problems, the goal is to minimize a loss function $F(x)$ that is structured as an average over a dataset. A common form is $F(x) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(x)$, where $f_i(x)$ is the loss associated with the $i$-th data point and $x$ is a model parameter.\n\nConsider a simplified one-dimensional problem where we want to find the parameter $x$ that minimizes a loss function. The dataset consists of just two data points ($N=2$), leading to the following component loss functions:\n$$f_1(x) = (x-2)^2$$\n$$f_2(x) = (x+2)^2$$\nThe total loss function is therefore $F(x) = \\frac{1}{2} (f_1(x) + f_2(x))$.\n\nStochastic Gradient Descent (SGD) is an iterative optimization algorithm that approximates the true gradient of $F(x)$ at each step. In its simplest form, a stochastic gradient estimator, which we will denote as $g(x)$, is computed by first selecting an index $i$ uniformly at random from $\\{1, 2\\}$, and then calculating the gradient of the corresponding component function, $g(x) = \\nabla f_i(x)$. In this one-dimensional case, the gradient operator $\\nabla$ is simply the derivative with respect to $x$, i.e., $\\frac{d}{dx}$.\n\nCalculate the variance of the stochastic gradient estimator $g(x)$ at the specific point $x = 1$.",
            "solution": "We are given $f_{1}(x)=(x-2)^{2}$ and $f_{2}(x)=(x+2)^{2}$, and the stochastic gradient estimator $g(x)$ is defined by choosing $i$ uniformly from $\\{1,2\\}$ and setting $g(x)=\\frac{d}{dx}f_{i}(x)$. First compute the component gradients:\n$$\n\\frac{d}{dx}f_{1}(x)=2(x-2), \\quad \\frac{d}{dx}f_{2}(x)=2(x+2).\n$$\nAt $x=1$, $g(1)$ takes the values\n$$\ng(1)=2(1-2)=-2 \\quad \\text{with probability } \\frac{1}{2}, \\quad g(1)=2(1+2)=6 \\quad \\text{with probability } \\frac{1}{2}.\n$$\nCompute the mean of $g(1)$:\n$$\n\\mathbb{E}[g(1)]=\\frac{1}{2}(-2)+\\frac{1}{2}(6)=2.\n$$\nThis equals the true gradient of $F$ at $x=1$, since\n$$\nF'(x)=\\frac{1}{2}\\left(2(x-2)+2(x+2)\\right)=2x \\implies F'(1)=2.\n$$\nCompute the second moment:\n$$\n\\mathbb{E}[g(1)^{2}]=\\frac{1}{2}\\left((-2)^{2}+6^{2}\\right)=\\frac{1}{2}(4+36)=20.\n$$\nTherefore, the variance is\n$$\n\\operatorname{Var}(g(1))=\\mathbb{E}[g(1)^{2}]-\\left(\\mathbb{E}[g(1)]\\right)^{2}=20-2^{2}=16.\n$$\nHence, the variance of the stochastic gradient estimator at $x=1$ is $16$.",
            "answer": "$$\\boxed{16}$$"
        }
    ]
}