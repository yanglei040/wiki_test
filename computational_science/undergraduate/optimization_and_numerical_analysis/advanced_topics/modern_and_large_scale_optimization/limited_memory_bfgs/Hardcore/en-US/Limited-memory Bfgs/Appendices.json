{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of the L-BFGS algorithm comes from its clever use of memory. Instead of storing a dense and computationally expensive Hessian matrix, it retains only the most recent changes in position and gradient. This practice exercise  will guide you through calculating these core components, the displacement vector $s_k$ and the gradient difference vector $y_k$, which are the fundamental building blocks for approximating the landscape of the objective function.",
            "id": "2184596",
            "problem": "You are analyzing the behavior of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm, a popular quasi-Newton method for unconstrained optimization. The algorithm builds an approximation of the inverse Hessian matrix by storing the $m$ most recent pairs of vectors $(s_k, y_k)$. Here, $x_k$ is the iterate at step $k$, $s_k = x_{k+1} - x_k$ is the displacement vector, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in the gradient vector for some objective function $f(x)$.\n\nConsider the optimization of the two-dimensional convex quadratic function $f(x) = f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. An optimization routine has produced the following sequence of three iterates (position vectors):\n$$\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix}\n$$\nCalculate the two pairs of history vectors, $(s_0, y_0)$ and $(s_1, y_1)$, that an L-BFGS algorithm would store based on this sequence of iterates.\n\nExpress your answer as a single $2 \\times 4$ matrix where the columns represent the vectors $s_0$, $y_0$, $s_1$, and $y_1$ in that specific order. Use fractions for any non-integer values.",
            "solution": "The goal is to compute the displacement vectors $s_k = x_{k+1} - x_k$ and the gradient difference vectors $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ for $k=0$ and $k=1$.\n\nFirst, we need to find the gradient of the objective function $f(x_1, x_2) = (x_1 - 2)^2 + 3(x_2 + 1)^2$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x_1} = 2(x_1 - 2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 3 \\cdot 2(x_2 + 1) = 6(x_2 + 1)\n$$\nSo, the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} 2(x_1 - 2) \\\\ 6(x_2 + 1) \\end{pmatrix}\n$$\n\nNext, we evaluate the gradient at each of the given iterates $x_0$, $x_1$, and $x_2$. Let's denote these gradients as $g_0$, $g_1$, and $g_2$.\n\nFor $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$\ng_0 = \\nabla f(0, 0) = \\begin{pmatrix} 2(0 - 2) \\\\ 6(0 + 1) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix}\n$$\n\nFor $x_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$:\n$$\ng_1 = \\nabla f(1, -2) = \\begin{pmatrix} 2(1 - 2) \\\\ 6(-2 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(-1) \\\\ 6(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix}\n$$\n\nFor $x_2 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\end{pmatrix}$:\n$$\ng_2 = \\nabla f(2, -1.5) = \\begin{pmatrix} 2(2 - 2) \\\\ 6(-1.5 + 1) \\end{pmatrix} = \\begin{pmatrix} 2(0) \\\\ 6(-0.5) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}\n$$\n\nNow we can compute the displacement vectors $s_0$ and $s_1$.\n\nFor $k=0$:\n$$\ns_0 = x_1 - x_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\ns_1 = x_2 - x_1 = \\begin{pmatrix} 2 \\\\ -1.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ -1.5 - (-2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n\nNext, we compute the gradient difference vectors $y_0$ and $y_1$.\n\nFor $k=0$:\n$$\ny_0 = g_1 - g_0 = \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} - \\begin{pmatrix} -4 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} -2 - (-4) \\\\ -6 - 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}\n$$\n\nFor $k=1$:\n$$\ny_1 = g_2 - g_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -2 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 0 - (-2) \\\\ -3 - (-6) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n\nFinally, we assemble the results into a $2 \\times 4$ matrix where the columns are $s_0, y_0, s_1, y_1$.\n$$\ns_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad y_0 = \\begin{pmatrix} 2 \\\\ -12 \\end{pmatrix}, \\quad s_1 = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad y_1 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nThe resulting matrix is:\n$$\n\\begin{pmatrix} 1  2  1  2 \\\\ -2  -12  \\frac{1}{2}  3 \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2  1  2 \\\\ -2  -12  \\frac{1}{2}  3 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "At the core of the L-BFGS algorithm is an elegant and efficient procedure known as the two-loop recursion. This process uses the stored history vectors to implicitly calculate a new search direction without ever forming the large inverse Hessian matrix. This exercise  provides a hands-on opportunity to step through this crucial computation, revealing the mechanics of how L-BFGS generates its powerful updates.",
            "id": "2184578",
            "problem": "The Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm is a popular quasi-Newton method for unconstrained optimization. In each iteration $k$, the algorithm computes a search direction $p_k$ by applying an approximation of the inverse Hessian matrix to the negative of the current gradient, $g_k = \\nabla f(x_k)$. This approximation is constructed implicitly using a limited history of the $m$ most recent steps.\n\nThe history is stored as pairs of vectors $(s_i, y_i)$ for $i=k-m, \\dots, k-1$, where $s_i = x_{i+1} - x_i$ is the change in position and $y_i = g_{i+1} - g_i$ is the change in the gradient. The search direction $p_k$ is then found by a procedure known as the L-BFGS two-loop recursion.\n\nConsider an L-BFGS update at step $k$ with a memory size of $m=2$. The relevant data available from previous steps are:\n- Current gradient: $g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n- History from step $k-1$: $s_{k-1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{k-1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n- History from step $k-2$: $s_{k-2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $y_{k-2} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$\n\nYour task is to compute the search direction vector $p_k$ for this iteration. Express your answer as a $2 \\times 1$ column vector with exact rational components.",
            "solution": "The L-BFGS search direction $p_k$ is computed by approximating the product $-H_k g_k$, where $H_k$ is the inverse Hessian approximation. This is achieved efficiently using the two-loop recursion algorithm. We are given $m=2$, the gradient $g_k$, and the history vectors $(s_{k-1}, y_{k-1})$ and $(s_{k-2}, y_{k-2})$.\n\nThe algorithm is as follows:\n\n1.  Initialize a vector $q$ with the current gradient:\n    $q = g_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\n2.  **First Loop (backward pass):** This loop iterates from $i = k-1$ down to $i = k-m$. In our case, $i$ goes from $k-1$ to $k-2$.\n    We first pre-calculate the scalars $\\rho_i = \\frac{1}{y_i^T s_i}$.\n    For $i = k-1$:\n    $y_{k-1}^T s_{k-1} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (1)(0) = 1$.\n    So, $\\rho_{k-1} = \\frac{1}{1} = 1$.\n\n    For $i = k-2$:\n    $y_{k-2}^T s_{k-2} = \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = (-1)(0) + (2)(1) = 2$.\n    So, $\\rho_{k-2} = \\frac{1}{2}$.\n\n    Now, we perform the loop updates. We will also store the computed $\\alpha_i$ values, as they are needed in the second loop.\n    -   **For $i = k-1$**:\n        $\\alpha_{k-1} = \\rho_{k-1} s_{k-1}^T q = (1) \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = (1)((1)(1) + (0)(-2)) = 1$.\n        $q \\leftarrow q - \\alpha_{k-1} y_{k-1} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} - (1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ -2-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$.\n\n    -   **For $i = k-2$**:\n        $\\alpha_{k-2} = \\rho_{k-2} s_{k-2}^T q = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((0)(0) + (1)(-3)) = -\\frac{3}{2}$.\n        $q \\leftarrow q - \\alpha_{k-2} y_{k-2} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\left(-\\frac{3}{2}\\right) \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} -\\frac{3}{2} \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 - \\frac{3}{2} \\\\ -3 + 3 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix}$.\n\n3.  **Initial Hessian Scaling:** The initial inverse Hessian approximation $H_k^0$ is a diagonal matrix $\\gamma_k I$, where $\\gamma_k = \\frac{s_{k-1}^T y_{k-1}}{y_{k-1}^T y_{k-1}}$. We initialize our result vector $r$ by multiplying this scaled identity matrix with the current $q$.\n    $s_{k-1}^T y_{k-1} = (1)(1) + (0)(1) = 1$.\n    $y_{k-1}^T y_{k-1} = (1)^2 + (1)^2 = 2$.\n    $\\gamma_k = \\frac{1}{2}$.\n    $r = \\gamma_k q = \\frac{1}{2} \\begin{pmatrix} -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix}$.\n\n4.  **Second Loop (forward pass):** This loop iterates from $i = k-m$ up to $i = k-1$. In our case, $i$ goes from $k-2$ to $k-1$.\n    -   **For $i = k-2$**:\n        $\\beta = \\rho_{k-2} y_{k-2}^T r = \\left(\\frac{1}{2}\\right) \\begin{pmatrix} -1  2 \\end{pmatrix} \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} = \\left(\\frac{1}{2}\\right)((-1)(-\\frac{3}{4}) + (2)(0)) = \\frac{3}{8}$.\n        $r \\leftarrow r + s_{k-2} (\\alpha_{k-2} - \\beta) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{3}{2} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{12}{8} - \\frac{3}{8}\\right) = \\begin{pmatrix} -\\frac{3}{4} \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n    -   **For $i = k-1$**:\n        $\\beta = \\rho_{k-1} y_{k-1}^T r = (1) \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = (1)(-\\frac{6}{8}) + (1)(-\\frac{15}{8}) = -\\frac{21}{8}$.\n        $r \\leftarrow r + s_{k-1} (\\alpha_{k-1} - \\beta) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(1 - \\left(-\\frac{21}{8}\\right)\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\left(\\frac{8}{8} + \\frac{21}{8}\\right) = \\begin{pmatrix} -\\frac{6}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} + \\begin{pmatrix} \\frac{29}{8} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix}$.\n\n5.  The final result of the two-loop recursion is the vector $r = H_k g_k$. The search direction is $p_k = -r$.\n    $p_k = - \\begin{pmatrix} \\frac{23}{8} \\\\ -\\frac{15}{8} \\end{pmatrix} = \\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} -\\frac{23}{8} \\\\ \\frac{15}{8} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Why choose a complex quasi-Newton method like L-BFGS over the simpler steepest descent? This practice  directly addresses that question by having you compute the search directions for both algorithms at the same point. By comparing the resulting vectors, you will gain a tangible sense of how L-BFGS incorporates curvature information to find a more efficient path towards the minimum.",
            "id": "2184555",
            "problem": "In the field of numerical optimization, quasi-Newton methods are popular for finding the minimum of a function. One such method is the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, which approximates the inverse Hessian matrix using information from a limited number of previous iterations.\n\nConsider the task of minimizing the quadratic objective function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$f(x_1, x_2) = x_1^2 + 4x_2^2$$\nAn optimization algorithm is currently at the iterate $\\mathbf{x}_1 = (2, -0.5)^T$. The previous iterate was $\\mathbf{x}_0 = (4, 1)^T$. We wish to compare the search direction proposed by two different methods at point $\\mathbf{x}_1$.\n\nThe first method is the method of steepest descent, where the search direction $\\mathbf{p}_{\\text{SD}}$ is simply the negative of the gradient at the current point, i.e., $\\mathbf{p}_{\\text{SD}} = -\\nabla f(\\mathbf{x}_1)$.\n\nThe second method is L-BFGS with a memory of $m=1$. The search direction $\\mathbf{p}_{\\text{L-BFGS}}$ at the current iterate $\\mathbf{x}_1$ is calculated as $\\mathbf{p}_{\\text{L-BFGS}} = -\\mathbf{r}$, where the vector $\\mathbf{r}$ is the result of the following procedure (known as the two-loop recursion):\nLet $\\mathbf{g}_1 = \\nabla f(\\mathbf{x}_1)$ be the current gradient. From the previous step, we have the vectors $\\mathbf{s}_0 = \\mathbf{x}_1 - \\mathbf{x}_0$ and $\\mathbf{y}_0 = \\nabla f(\\mathbf{x}_1) - \\nabla f(\\mathbf{x}_0)$.\n1.  Initialize a temporary vector $\\mathbf{q} \\leftarrow \\mathbf{g}_1$.\n2.  Compute the scalar $\\rho_0 = \\frac{1}{\\mathbf{y}_0^T \\mathbf{s}_0}$.\n3.  Compute the scalar $\\alpha_0 = \\rho_0 (\\mathbf{s}_0^T \\mathbf{q})$.\n4.  Update the temporary vector: $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_0 \\mathbf{y}_0$.\n5.  Compute the initial Hessian scaling factor $\\gamma_0 = \\frac{\\mathbf{s}_0^T \\mathbf{y}_0}{\\mathbf{y}_0^T \\mathbf{y}_0}$.\n6.  Initialize the result vector $\\mathbf{r} \\leftarrow \\gamma_0 \\mathbf{q}$.\n7.  Compute the scalar $\\beta_0 = \\rho_0 (\\mathbf{y}_0^T \\mathbf{r})$.\n8.  Update the result vector: $\\mathbf{r} \\leftarrow \\mathbf{r} + (\\alpha_0 - \\beta_0) \\mathbf{s}_0$.\n\nYour task is to calculate the cosine of the angle $\\theta$ between the steepest descent search direction $\\mathbf{p}_{\\text{SD}}$ and the L-BFGS search direction $\\mathbf{p}_{\\text{L-BFGS}}$ at the point $\\mathbf{x}_1$. Report your answer as a numerical value rounded to four significant figures.",
            "solution": "The objective is $f(x_{1},x_{2})=x_{1}^{2}+4x_{2}^{2}$, so the gradient is $\\nabla f(x_{1},x_{2})=(2x_{1},\\,8x_{2})^{T}$.\n\nAt $\\mathbf{x}_{0}=(4,1)^{T}$, the gradient is $\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=(8,8)^{T}$. At $\\mathbf{x}_{1}=(2,-0.5)^{T}$, the gradient is $\\mathbf{g}_{1}=\\nabla f(\\mathbf{x}_{1})=(4,-4)^{T}$.\n\nThe steepest descent direction at $\\mathbf{x}_{1}$ is\n$$\n\\mathbf{p}_{\\text{SD}}=-\\mathbf{g}_{1}=(-4,\\,4)^{T}.\n$$\n\nFor L-BFGS with $m=1$, define $\\mathbf{s}_{0}=\\mathbf{x}_{1}-\\mathbf{x}_{0}=(-2,\\,-\\tfrac{3}{2})^{T}$ and $\\mathbf{y}_{0}=\\mathbf{g}_{1}-\\mathbf{g}_{0}=(-4,\\,-12)^{T}$. Compute\n$$\n\\mathbf{y}_{0}^{T}\\mathbf{s}_{0}=(-4)(-2)+(-12)\\!\\left(-\\tfrac{3}{2}\\right)=26,\\qquad \\rho_{0}=\\frac{1}{\\mathbf{y}_{0}^{T}\\mathbf{s}_{0}}=\\frac{1}{26}.\n$$\nInitialize $\\mathbf{q}\\leftarrow\\mathbf{g}_{1}=(4,-4)^{T}$ and compute\n$$\n\\alpha_{0}=\\rho_{0}(\\mathbf{s}_{0}^{T}\\mathbf{q})=\\frac{1}{26}\\big[(-2)(4)+\\left(-\\tfrac{3}{2}\\right)(-4)\\big]=-\\frac{1}{13}.\n$$\nUpdate\n$$\n\\mathbf{q}\\leftarrow \\mathbf{q}-\\alpha_{0}\\mathbf{y}_{0}=(4,-4)^{T}-\\left(-\\frac{1}{13}\\right)(-4,-12)^{T}=\\left(\\frac{48}{13},\\,-\\frac{64}{13}\\right)^{T}.\n$$\nCompute the scaling\n$$\n\\gamma_{0}=\\frac{\\mathbf{s}_{0}^{T}\\mathbf{y}_{0}}{\\mathbf{y}_{0}^{T}\\mathbf{y}_{0}}=\\frac{26}{16+144}=\\frac{13}{80},\n$$\nand initialize\n$$\n\\mathbf{r}\\leftarrow \\gamma_{0}\\mathbf{q}=\\frac{13}{80}\\left(\\frac{48}{13},\\,-\\frac{64}{13}\\right)^{T}=\\left(\\frac{3}{5},\\,-\\frac{4}{5}\\right)^{T}.\n$$\nThen\n$$\n\\beta_{0}=\\rho_{0}(\\mathbf{y}_{0}^{T}\\mathbf{r})=\\frac{1}{26}\\left[(-4)\\!\\left(\\frac{3}{5}\\right)+(-12)\\!\\left(-\\frac{4}{5}\\right)\\right]=\\frac{18}{65}.\n$$\nUpdate\n$$\n\\mathbf{r}\\leftarrow \\mathbf{r}+(\\alpha_{0}-\\beta_{0})\\mathbf{s}_{0}=\\left(\\frac{3}{5},-\\frac{4}{5}\\right)^{T}+\\left(-\\frac{1}{13}-\\frac{18}{65}\\right)\\left(-2,-\\frac{3}{2}\\right)^{T}=\\left(\\frac{17}{13},-\\frac{7}{26}\\right)^{T}.\n$$\nHence the L-BFGS direction is\n$$\n\\mathbf{p}_{\\text{L-BFGS}}=-\\mathbf{r}=\\left(-\\frac{17}{13},\\,\\frac{7}{26}\\right)^{T}.\n$$\n\nThe cosine of the angle between $\\mathbf{p}_{\\text{SD}}$ and $\\mathbf{p}_{\\text{L-BFGS}}$ is\n$$\n\\cos\\theta=\\frac{\\mathbf{p}_{\\text{SD}}^{T}\\mathbf{p}_{\\text{L-BFGS}}}{\\|\\mathbf{p}_{\\text{SD}}\\|\\,\\|\\mathbf{p}_{\\text{L-BFGS}}\\|}.\n$$\nCompute the numerator and norms:\n$$\n\\mathbf{p}_{\\text{SD}}^{T}\\mathbf{p}_{\\text{L-BFGS}}=(-4,4)\\cdot\\left(-\\frac{17}{13},\\frac{7}{26}\\right)=\\frac{82}{13},\n$$\n$$\n\\|\\mathbf{p}_{\\text{SD}}\\|=\\sqrt{(-4)^{2}+4^{2}}=4\\sqrt{2},\\qquad\n\\|\\mathbf{p}_{\\text{L-BFGS}}\\|=\\sqrt{\\left(\\frac{17}{13}\\right)^{2}+\\left(\\frac{7}{26}\\right)^{2}}=\\frac{\\sqrt{1205}}{26}.\n$$\nTherefore\n$$\n\\cos\\theta=\\frac{\\frac{82}{13}}{4\\sqrt{2}\\,\\frac{\\sqrt{1205}}{26}}\n=\\frac{41}{\\sqrt{2410}}.\n$$\nNumerically, $\\cos\\theta=\\frac{41}{\\sqrt{2410}}\\approx 0.8352$ when rounded to four significant figures.",
            "answer": "$$\\boxed{0.8352}$$"
        }
    ]
}