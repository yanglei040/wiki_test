{
    "hands_on_practices": [
        {
            "introduction": "Let's begin with a foundational exercise to build your intuition. We will apply the subgradient method to one of the simplest nonsmooth convex functions, the absolute value function $f(x) = |x|$. This practice illustrates the core mechanics of a subgradient update and highlights a critical aspect of the method: how the choice of step size can dramatically affect the algorithm's trajectory, even causing it to overshoot the minimum .",
            "id": "2207140",
            "problem": "Consider the problem of minimizing the nonsmooth convex function $f(x) = |x|$ using the subgradient method. The iterative update rule for the subgradient method is given by $x_{k+1} = x_k - \\alpha_k g_k$, where $x_k$ is the iterate at step $k$, $\\alpha_k$ is the step size, and $g_k$ is a subgradient of the function $f$ at the point $x_k$. For the function $f(x)=|x|$, a valid subgradient at any point $x \\neq 0$ is given by $g = \\text{sgn}(x)$, where $\\text{sgn}(x)$ is the sign function which returns $1$ for $x>0$ and $-1$ for $x<0$.\n\nSuppose we start the iteration at the point $x_0 = 5$ and use a constant step size $\\alpha_k = \\alpha = 10$ for all steps.\n\nCalculate the value of the next iterate, $x_1$.",
            "solution": "We are minimizing the convex nonsmooth function $f(x)=|x|$ using the subgradient method with update rule\n$$\nx_{k+1} = x_{k} - \\alpha_{k} g_{k},\n$$\nwhere $g_{k} \\in \\partial f(x_{k})$ is a subgradient of $f$ at $x_{k}$. For $f(x)=|x|$ and any $x \\neq 0$, a valid subgradient is $g=\\text{sgn}(x)$, where $\\text{sgn}(x)=1$ if $x>0$ and $\\text{sgn}(x)=-1$ if $x<0$.\n\nGiven $x_{0}=5$ and constant step size $\\alpha_{k}=\\alpha=10$, we compute the subgradient at $x_{0}$:\n$$\ng_{0}=\\text{sgn}(x_{0})=\\text{sgn}(5)=1.\n$$\nApplying the update rule,\n$$\nx_{1}=x_{0}-\\alpha g_{0}=5-10\\cdot 1=-5.\n$$\nThus, the next iterate is $x_{1}=-5$.",
            "answer": "$$\\boxed{-5}$$"
        },
        {
            "introduction": "One of the most important distinctions between smooth and nonsmooth optimization is that a subgradient at a point is not necessarily unique. This exercise delves into the concept of the subdifferential, the set of all valid subgradients. By starting at a point where the objective function is 'kinked', you will see how different valid choices for the subgradient can lead to different optimization paths, a key characteristic of subgradient methods .",
            "id": "2207146",
            "problem": "Consider the problem of minimizing the $L_1$-norm function $f(x_1, x_2) = |x_1| + |x_2|$ in $\\mathbb{R}^2$ using the subgradient method. The iterative update rule for the subgradient method is given by:\n$$x_{k+1} = x_k - \\alpha_k g_k$$\nwhere $x_k$ is the current iterate, $\\alpha_k > 0$ is the step size, and $g_k$ is a subgradient of the function $f$ at $x_k$.\n\nThe set of all subgradients of the $L_1$-norm, known as the subdifferential $\\partial f(x)$, is defined as follows: A vector $g = (g_1, g_2)$ is a subgradient of $f(x_1, x_2) = |x_1| + |x_2|$ at $x = (x_1, x_2)$ if its components satisfy:\n$$ g_i = \\begin{cases} \\operatorname{sign}(x_i) & \\text{if } x_i \\neq 0 \\\\ v_i & \\text{if } x_i = 0 \\end{cases} $$\nwhere $\\operatorname{sign}(\\cdot)$ is the sign function, and for any component $i$ where $x_i = 0$, $v_i$ can be any value in the interval $[-1, 1]$.\n\nSuppose we start at the point $x_0 = (1, 0)$ and use a fixed step size of $\\alpha_0 = 0.5$. Since the second component of $x_0$ is zero, the subgradient at this point is not unique.\n\nLet's consider two distinct valid subgradients at $x_0$.\nFirst, choose the subgradient $g^{(a)}$ where the component corresponding to the zero coordinate of $x_0$ is set to its minimum possible value from its allowed interval.\nSecond, choose the subgradient $g^{(b)}$ where the component corresponding to the zero coordinate of $x_0$ is set to its maximum possible value from its allowed interval.\n\nCalculate the next two iterates, $x_1^{(a)}$ and $x_1^{(b)}$, resulting from applying one step of the subgradient method with subgradients $g^{(a)}$ and $g^{(b)}$, respectively.\n\nYour final answer should be a row matrix containing the four coordinates of these two points in the order $(x_{1}^{(a)}, x_{2}^{(a)}, x_{1}^{(b)}, x_{2}^{(b)})$.",
            "solution": "We minimize $f(x_{1},x_{2})=|x_{1}|+|x_{2}|$ with subgradient method $x_{k+1}=x_{k}-\\alpha_{k}g_{k}$, where $g_{k}\\in\\partial f(x_{k})$. The subdifferential is given componentwise by $g_{i}=\\operatorname{sign}(x_{i})$ if $x_{i}\\neq 0$ and $g_{i}\\in[-1,1]$ if $x_{i}=0$.\n\nAt $x_{0}=(1,0)$, the subgradient components satisfy $g_{1}=\\operatorname{sign}(1)=1$ and $g_{2}\\in[-1,1]$. With step size $\\alpha_{0}=\\frac{1}{2}$:\n- Choose $g^{(a)}=(1,-1)$ by taking the minimum permissible value for the second component.\n- Choose $g^{(b)}=(1,1)$ by taking the maximum permissible value for the second component.\n\nApply the update:\n$$x_{1}^{(a)}=x_{0}-\\alpha_{0}g^{(a)}=(1,0)-\\frac{1}{2}(1,-1)=\\left(1-\\frac{1}{2},\\,0-(-\\frac{1}{2})\\right)=\\left(\\frac{1}{2},\\,\\frac{1}{2}\\right),$$\n$$x_{1}^{(b)}=x_{0}-\\alpha_{0}g^{(b)}=(1,0)-\\frac{1}{2}(1,1)=\\left(1-\\frac{1}{2},\\,0-\\frac{1}{2}\\right)=\\left(\\frac{1}{2},\\,-\\frac{1}{2}\\right).$$\n\nThus, in the order $(x_{1}^{(a)}, x_{2}^{(a)}, x_{1}^{(b)}, x_{2}^{(b)})$, the row matrix is $\\left(\\frac{1}{2},\\,\\frac{1}{2},\\,\\frac{1}{2},\\,-\\frac{1}{2}\\right)$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "We now move to a more practical application that is common in fields like supply chain management and robust control. This problem involves minimizing a function defined as the maximum of several other functions, which often arises when optimizing for a worst-case scenario. You will practice identifying the 'active' function to determine the correct subgradient at each step and perform multiple iterations to see how the method progresses towards a solution .",
            "id": "2207196",
            "problem": "A manufacturing firm is optimizing its production plan for two products. Let $x_1$ and $x_2$ be the production levels for product 1 and product 2, respectively. The associated cost function, which the firm seeks to minimize, is given by $C(x_1, x_2) = \\max(3x_1 + x_2 + 5, x_1 + 4x_2 - 2)$. The firm uses an iterative optimization algorithm known as the subgradient method to update its production plan. The update rule for the production vector $\\mathbf{x} = (x_1, x_2)$ from iteration $k$ to $k+1$ is given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{g}_k$, where $\\mathbf{g}_k$ is a subgradient of the cost function $C$ at the point $\\mathbf{x}_k$, and $\\alpha$ is a fixed step size.\n\nThe initial production plan is $\\mathbf{x}_0 = (2, 3)$. The firm uses a constant step size of $\\alpha = 0.5$.\n\nCalculate the production plan vector $\\mathbf{x}_2 = (x_{1,2}, x_{2,2})$ after two complete iterations of the algorithm. Express your answer as a row matrix $\\begin{bmatrix} x_{1,2} & x_{2,2} \\end{bmatrix}$.",
            "solution": "The problem requires us to perform two iterations of the subgradient method to minimize the cost function $C(x_1, x_2) = \\max(3x_1 + x_2 + 5, x_1 + 4x_2 - 2)$. The update rule is $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{g}_k$, with the initial point $\\mathbf{x}_0 = (2, 3)$ and step size $\\alpha = 0.5$.\n\nFirst, let's identify the components of the cost function. Let $f_1(x_1, x_2) = 3x_1 + x_2 + 5$ and $f_2(x_1, x_2) = x_1 + 4x_2 - 2$. Then $C(\\mathbf{x}) = \\max(f_1(\\mathbf{x}), f_2(\\mathbf{x}))$.\n\nFor a function defined as the maximum of several differentiable functions, a subgradient at a point $\\mathbf{x}$ can be chosen as the gradient of any of the functions that are \"active\" at $\\mathbf{x}$ (i.e., whose value equals the maximum value). If only one function is active, the subgradient is unique and is equal to the gradient of that function.\n\nThe gradients of our two linear functions are constant vectors:\n$\\nabla f_1(x_1, x_2) = (3, 1)$\n$\\nabla f_2(x_1, x_2) = (1, 4)$\n\n**First Iteration (k=0)**\n\nWe start with the initial point $\\mathbf{x}_0 = (2, 3)$. We must first determine the active function(s) at this point by evaluating $f_1$ and $f_2$:\n$f_1(\\mathbf{x}_0) = 3(2) + 3 + 5 = 6 + 3 + 5 = 14$\n$f_2(\\mathbf{x}_0) = 2 + 4(3) - 2 = 2 + 12 - 2 = 12$\n\nSince $f_1(\\mathbf{x}_0) = 14$ is strictly greater than $f_2(\\mathbf{x}_0) = 12$, $f_1$ is the only active function at $\\mathbf{x}_0$. Therefore, the subgradient $\\mathbf{g}_0$ is the gradient of $f_1$:\n$\\mathbf{g}_0 = \\nabla f_1 = (3, 1)$\n\nNow we apply the update rule to find the next point, $\\mathbf{x}_1$:\n$\\mathbf{x}_1 = \\mathbf{x}_0 - \\alpha \\mathbf{g}_0 = (2, 3) - 0.5 \\cdot (3, 1)$\n$\\mathbf{x}_1 = (2, 3) - (1.5, 0.5)$\n$\\mathbf{x}_1 = (2 - 1.5, 3 - 0.5) = (0.5, 2.5)$\n\n**Second Iteration (k=1)**\n\nThe new point is $\\mathbf{x}_1 = (0.5, 2.5)$. We repeat the process by evaluating $f_1$ and $f_2$ at $\\mathbf{x}_1$:\n$f_1(\\mathbf{x}_1) = 3(0.5) + 2.5 + 5 = 1.5 + 2.5 + 5 = 9$\n$f_2(\\mathbf{x}_1) = 0.5 + 4(2.5) - 2 = 0.5 + 10 - 2 = 8.5$\n\nAgain, $f_1(\\mathbf{x}_1) = 9$ is strictly greater than $f_2(\\mathbf{x}_1) = 8.5$, so $f_1$ remains the sole active function. The subgradient $\\mathbf{g}_1$ is therefore the gradient of $f_1$:\n$\\mathbf{g}_1 = \\nabla f_1 = (3, 1)$\n\nWe apply the update rule one more time to find $\\mathbf{x}_2$:\n$\\mathbf{x}_2 = \\mathbf{x}_1 - \\alpha \\mathbf{g}_1 = (0.5, 2.5) - 0.5 \\cdot (3, 1)$\n$\\mathbf{x}_2 = (0.5, 2.5) - (1.5, 0.5)$\n$\\mathbf{x}_2 = (0.5 - 1.5, 2.5 - 0.5) = (-1, 2)$\n\nAfter two iterations, the production plan vector is $\\mathbf{x}_2 = (-1, 2)$.",
            "answer": "$$\\boxed{\\begin{bmatrix} -1 & 2 \\end{bmatrix}}$$"
        }
    ]
}