{
    "hands_on_practices": [
        {
            "introduction": "The heart of any trust-region method is replacing a complex, nonlinear objective function with a simpler, local approximation. This exercise provides practice in constructing the standard quadratic model, $m_k(p)$, which is based on the second-order Taylor expansion of the objective function at the current iterate. Mastering this step is fundamental, as the quality of the entire optimization process depends on this local model .",
            "id": "2224506",
            "problem": "In the field of unconstrained optimization, trust-region methods are a class of iterative algorithms used to solve nonlinear programming problems. At each iteration $k$, these methods construct a simpler model function $m_k(p)$ that approximates the true objective function $f(x)$ in a neighborhood (the \"trust region\") around the current point $x_k$. The step $p$ is then found by minimizing this model within the trust region. A common choice for the model is a quadratic function derived from the second-order Taylor expansion of $f(x)$ at $x_k$.\n\nConsider the objective function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$f(x_1, x_2) = \\sin(x_1) + x_2^2$$\nDetermine the quadratic model $m_k(p)$ for this function at the point $x_k = (0, 1)$, where $p = (p_1, p_2)$ is the step from $x_k$. Express your answer as a polynomial in terms of $p_1$ and $p_2$.",
            "solution": "The quadratic trust-region model based on the second-order Taylor expansion of $f$ at $x_{k}$ is\n$$m_{k}(p) = f(x_{k}) + \\nabla f(x_{k})^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^{2} f(x_{k}) p,$$\nwhere $p = (p_{1}, p_{2})$.\n\nGiven $f(x_{1}, x_{2}) = \\sin(x_{1}) + x_{2}^{2}$, compute the gradient and Hessian:\n$$\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} \\cos(x_{1}) \\\\ 2 x_{2} \\end{pmatrix}, \\quad \\nabla^{2} f(x_{1}, x_{2}) = \\begin{pmatrix} -\\sin(x_{1})  0 \\\\ 0  2 \\end{pmatrix}.$$\n\nEvaluate these at $x_{k} = (0, 1)$:\n$$f(x_{k}) = \\sin(0) + 1^{2} = 1,$$\n$$\\nabla f(x_{k}) = \\begin{pmatrix} \\cos(0) \\\\ 2 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad \\nabla^{2} f(x_{k}) = \\begin{pmatrix} -\\sin(0)  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix}.$$\n\nWith $p = (p_{1}, p_{2})$, compute the linear and quadratic terms:\n$$\\nabla f(x_{k})^{\\top} p = 1 \\cdot p_{1} + 2 \\cdot p_{2} = p_{1} + 2 p_{2},$$\n$$p^{\\top} \\nabla^{2} f(x_{k}) p = \\begin{pmatrix} p_{1}  p_{2} \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} p_{1} \\\\ p_{2} \\end{pmatrix} = 2 p_{2}^{2}.$$\n\nTherefore,\n$$m_{k}(p) = 1 + \\left(p_{1} + 2 p_{2}\\right) + \\frac{1}{2} \\left(2 p_{2}^{2}\\right) = 1 + p_{1} + 2 p_{2} + p_{2}^{2}.$$",
            "answer": "$$\\boxed{1 + p_{1} + 2 p_{2} + p_{2}^{2}}$$"
        },
        {
            "introduction": "Once the quadratic model is built, the next task is to find a trial step by solving the trust-region subproblem: minimizing the model within a certain radius. This problem guides you through the core logic for solving this subproblem in a simple one-dimensional case . You will learn to first check the unconstrained minimizer of the model and see how the trust-region boundary dictates the solution when that point is too far away.",
            "id": "2224504",
            "problem": "In the context of unconstrained optimization, trust-region methods iteratively approximate a complex function with a simpler model function $m(p)$ around the current point. The next step $p$ is then determined by solving the trust-region subproblem, which involves minimizing this model within a \"trust region\" of radius $\\Delta  0$, where the model is believed to be a reliable approximation of the original function. The subproblem is formally stated as:\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\nConsider a one-dimensional optimization scenario where the model function for a step $p \\in \\mathbb{R}$ is a quadratic given by:\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\nwith a gradient term $g = 2$, a Hessian term $H = 6$, and an arbitrary constant $c$. The step is constrained by a trust-region radius of $\\Delta = 0.1$.\n\nDetermine the optimal step $p$ that solves this one-dimensional trust-region subproblem. Provide your answer as an exact decimal number.",
            "solution": "We must minimize the quadratic model $m(p) = g p + \\frac{1}{2} H p^{2} + c$ subject to the trust-region constraint $|p| \\le \\Delta$, with given values $g=2$, $H=6$, and $\\Delta=0.1$. The constant $c$ does not affect the minimizer and can be ignored.\n\nConsider the Lagrangian for the trust-region subproblem:\n$$\n\\mathcal{L}(p,\\lambda) = g p + \\frac{1}{2} H p^{2} + \\lambda \\left(p^{2} - \\Delta^{2}\\right),\n$$\nwith $\\lambda \\ge 0$. The Karush-Kuhn-Tucker conditions are:\n1. Stationarity: \n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = g + H p + 2 \\lambda p = 0.\n$$\n2. Primal feasibility: $|p| \\le \\Delta$.\n3. Dual feasibility: $\\lambda \\ge 0$.\n4. Complementary slackness: $\\lambda \\left(p^{2} - \\Delta^{2}\\right) = 0$.\n\nCase 1 (interior solution): If $|p|  \\Delta$, then $\\lambda = 0$ and stationarity gives\n$$\ng + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\nFor $g=2$ and $H=6$, this gives $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$. Check feasibility: $|p_{u}| = \\frac{1}{3}  0.1 = \\Delta$, so the interior solution is infeasible.\n\nCase 2 (boundary solution): Then $p^{2} = \\Delta^{2}$, so $p = \\pm \\Delta$. Stationarity becomes\n$$\ng + H p + 2 \\lambda p = 0 \\quad \\Rightarrow \\quad (H + 2 \\lambda) p = -g.\n$$\nSince $H + 2 \\lambda \\ge 0$, the sign of $p$ must match the sign of $-g$. With $g = 2  0$, we must take $p = -\\Delta = -0.1$. To verify dual feasibility, solve for $\\lambda$:\n$$\n(H + 2 \\lambda)(-\\Delta) = -g \\quad \\Rightarrow \\quad (H + 2 \\lambda)\\Delta = g \\quad \\Rightarrow \\quad 2 \\lambda = \\frac{g}{\\Delta} - H.\n$$\nSubstituting $g=2$, $\\Delta=0.1$, and $H=6$ gives\n$$\n2 \\lambda = \\frac{2}{0.1} - 6 = 20 - 6 = 14 \\quad \\Rightarrow \\quad \\lambda = 7 \\ge 0,\n$$\nwhich satisfies dual feasibility and complementary slackness.\n\nTherefore, the optimal trust-region step is the boundary step in the negative gradient direction:\n$$\np^{\\star} = -\\Delta = -0.1.\n$$",
            "answer": "$$\\boxed{-0.1}$$"
        },
        {
            "introduction": "What happens when the objective function's curvature is negative, leading to a concave quadratic model? This situation poses a challenge for many optimization algorithms, but trust-region methods handle it gracefully. This practice demonstrates the robustness of the trust-region framework by showing how the boundary of the trust region is used to find a meaningful step even when the model does not have an unconstrained minimum .",
            "id": "2224539",
            "problem": "In the context of unconstrained optimization, a trust-region method iteratively builds a quadratic model $m_k(p)$ of the objective function $f(x)$ around the current iterate $x_k$. The next step $p_k$ is found by minimizing this model within a \"trust region\" of radius $\\Delta_k$. The model is defined in one dimension as:\n$$m_k(p) = f(x_k) + f'(x_k)p + \\frac{1}{2}f''(x_k)p^2$$\nwhere $p$ is the step from $x_k$. The step $p_k$ is the solution to the subproblem:\n$$\\min_{p} m_k(p) \\quad \\text{subject to} \\quad |p| \\le \\Delta_k$$\n\nConsider the one-dimensional objective function $f(x) = -\\frac{1}{3}x^3 + 2x^2 - x$. Suppose we are at the iterate $x_k = 5$ and the trust-region radius is $\\Delta_k = 2$.\n\nDetermine the value of the step $p_k$ that solves the trust-region subproblem for this iteration.",
            "solution": "We form the one-dimensional quadratic model at $x_{k}=5$:\n$$m_{k}(p)=f(x_{k})+f'(x_{k})p+\\frac{1}{2}f''(x_{k})p^{2}.$$\nGiven $f(x)=-\\frac{1}{3}x^{3}+2x^{2}-x$, its derivatives are\n$$f'(x)=-x^{2}+4x-1,\\qquad f''(x)=-2x+4.$$\nEvaluating at $x_{k}=5$ gives\n$$g_{k}=f'(5)=-25+20-1=-6,\\qquad B_{k}=f''(5)=-10+4=-6.$$\nHence the model simplifies to\n$$m_{k}(p)=f(x_{k})-6p+\\frac{1}{2}(-6)p^{2}=f(x_{k})-6p-3p^{2}.$$\nThe trust-region subproblem is\n$$\\min_{p} \\, m_{k}(p)\\quad \\text{subject to}\\quad |p|\\le \\Delta_{k}=2.$$\nSince $B_{k}0$, the quadratic $m_{k}(p)$ is concave in $p$. Its stationary point solves $m_{k}'(p)=g_{k}+B_{k}p=0$, giving\n$$p^{\\ast}=-\\frac{g_{k}}{B_{k}}=-\\frac{-6}{-6}=-1,$$\nwhich is a maximizer due to concavity. Therefore, the minimum over the closed interval $\\{|p|\\le 2\\}$ occurs at one of the boundary points $p=\\pm 2$. Evaluate the model at the endpoints:\n$$m_{k}(2)=f(x_{k})-12-12=f(x_{k})-24,\\qquad m_{k}(-2)=f(x_{k})+12-12=f(x_{k}).$$\nThus $m_{k}(2)m_{k}(-2)$, so the minimizing step is\n$$p_{k}=2.$$",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}