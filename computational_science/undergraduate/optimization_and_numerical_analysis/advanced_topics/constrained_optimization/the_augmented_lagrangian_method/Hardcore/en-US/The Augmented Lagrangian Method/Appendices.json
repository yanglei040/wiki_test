{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's walk through a single, complete iteration of the Augmented Lagrangian method. This exercise provides a foundational understanding of the core mechanics: minimizing the augmented Lagrangian function to find the next solution estimate, and then using the result to update the Lagrange multiplier. This first step  is crucial for building intuition before tackling more complex scenarios.",
            "id": "2208360",
            "problem": "Consider the constrained optimization problem of minimizing the objective function $f(x) = x^2$ subject to the equality constraint $h(x) = x - 3 = 0$.\n\nThe augmented Lagrangian method is an iterative algorithm for solving such problems. The augmented Lagrangian function is defined as:\n$$L_A(x, \\lambda; \\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2} [h(x)]^2$$\nwhere $\\lambda$ is the Lagrange multiplier estimate and $\\rho > 0$ is the penalty parameter.\n\nA single iteration of the method, starting from an estimate $\\lambda_k$, consists of two main steps:\n1.  Find the next iterate $x_{k+1}$ by solving the unconstrained minimization problem:\n    $$x_{k+1} = \\arg\\min_{x} L_A(x, \\lambda_k; \\rho)$$\n2.  Update the Lagrange multiplier using the formula:\n    $$\\lambda_{k+1} = \\lambda_k + \\rho h(x_{k+1})$$\n\nPerform one full iteration of the augmented Lagrangian method starting with an initial multiplier estimate $\\lambda_0 = 1$ and using a penalty parameter $\\rho = 2$. Determine the resulting values for the new iterate $x_1$ and the updated multiplier $\\lambda_1$.\n\nExpress your answer as a row matrix $\\begin{pmatrix} x_1 & \\lambda_1 \\end{pmatrix}$ using exact fractions.",
            "solution": "We are given $f(x) = x^{2}$ and $h(x) = x - 3$, with augmented Lagrangian\n$$L_{A}(x,\\lambda;\\rho) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^{2}.$$\nWith $\\lambda_{0} = 1$ and $\\rho = 2$, this becomes\n$$L_{A}(x,1;2) = x^{2} + 1\\cdot(x-3) + \\frac{2}{2}(x-3)^{2} = x^{2} + x - 3 + (x-3)^{2}.$$\nTo obtain $x_{1}$, solve the unconstrained minimization:\n$$x_{1} = \\arg\\min_{x} L_{A}(x,1;2).$$\nDifferentiate and set the derivative to zero:\n$$\\frac{d}{dx}L_{A}(x,1;2) = 2x + 1 + 2(x-3) = 4x - 5,$$\n$$4x - 5 = 0 \\implies x_{1} = \\frac{5}{4}.$$\nThe second derivative is\n$$\\frac{d^{2}}{dx^{2}}L_{A}(x,1;2) = 4 > 0,$$\nso $x_{1} = \\frac{5}{4}$ is the unique minimizer.\n\nUpdate the multiplier using $\\lambda_{1} = \\lambda_{0} + \\rho h(x_{1})$:\n$$h(x_{1}) = \\frac{5}{4} - 3 = -\\frac{7}{4},$$\n$$\\lambda_{1} = 1 + 2\\left(-\\frac{7}{4}\\right) = 1 - \\frac{7}{2} = -\\frac{5}{2}.$$\n\nThus, the resulting values are $x_{1} = \\frac{5}{4}$ and $\\lambda_{1} = -\\frac{5}{2}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{4} & -\\frac{5}{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having mastered a single step, we now explore the iterative power of the method. This practice asks you to track the solution over the first few iterations, revealing how the sequence of minimizers progressively approaches the true constrained solution. By deriving an analytical expression for the iterates , you will gain a deeper insight into the convergence properties of the algorithm and see how it systematically refines its estimate.",
            "id": "2208359",
            "problem": "The method of multipliers, also known as the augmented Lagrangian method, is an iterative algorithm for solving constrained optimization problems. Consider a one-dimensional optimization problem of the form:\nMinimize $f(x)$ subject to the equality constraint $h(x) = 0$.\n\nThe augmented Lagrangian function $L_\\rho(x, \\lambda)$ is defined as:\n$$L_\\rho(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2} (h(x))^2$$\nwhere $\\lambda$ is the Lagrange multiplier estimate and $\\rho > 0$ is a constant penalty parameter.\n\nThe method of multipliers proceeds via the following iterative scheme starting from an initial multiplier $\\lambda_0$:\nFor $k = 0, 1, 2, \\dots$\n1. Find the next iterate $x_{k+1}$ by minimizing the augmented Lagrangian with the current multiplier estimate $\\lambda_k$:\n   $$x_{k+1} = \\arg\\min_x L_\\rho(x, \\lambda_k)$$\n2. Update the Lagrange multiplier:\n   $$\\lambda_{k+1} = \\lambda_k + \\rho h(x_{k+1})$$\n\nApply the first three iterations of this method to the problem of minimizing the function $f(x) = x^2$ subject to the constraint $x - 1 = 0$.\nUse a fixed penalty parameter $\\rho > 0$ and start with an initial multiplier estimate of $\\lambda_0 = 0$.\n\nDetermine the analytical expressions for the first three minimizers, $x_1, x_2$, and $x_3$, in terms of the penalty parameter $\\rho$. Present your answers for $x_1, x_2,$ and $x_3$ in order.",
            "solution": "We are asked to minimize $f(x)=x^{2}$ subject to $h(x)=x-1=0$ using the method of multipliers with augmented Lagrangian\n$$\nL_{\\rho}(x,\\lambda)=x^{2}+\\lambda(x-1)+\\frac{\\rho}{2}(x-1)^{2},\n$$\na fixed $\\rho>0$, and initial multiplier $\\lambda_{0}=0$. The iterations are:\n$$\nx_{k+1}=\\arg\\min_{x}L_{\\rho}(x,\\lambda_{k}),\\quad \\lambda_{k+1}=\\lambda_{k}+\\rho\\,h(x_{k+1})=\\lambda_{k}+\\rho(x_{k+1}-1).\n$$\nTo find $x_{k+1}$, differentiate $L_{\\rho}(x,\\lambda_{k})$ with respect to $x$ and set the derivative to zero:\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x}=2x+\\lambda_{k}+\\rho(x-1)=0.\n$$\nThis gives a linear equation in $x$:\n$$\n(2+\\rho)x+\\lambda_{k}-\\rho=0 \\quad\\Longrightarrow\\quad x_{k+1}=\\frac{\\rho-\\lambda_{k}}{2+\\rho}.\n$$\nUpdate $\\lambda$ using the rule:\n$$\n\\lambda_{k+1}=\\lambda_{k}+\\rho\\left(\\frac{\\rho-\\lambda_{k}}{2+\\rho}-1\\right)=\\lambda_{k}-\\frac{\\rho(\\lambda_{k}+2)}{2+\\rho}.\n$$\nThis simplifies to the affine recursion\n$$\n\\lambda_{k+1}=\\frac{2}{2+\\rho}\\lambda_{k}-\\frac{2\\rho}{2+\\rho}.\n$$\nDefine $a=\\frac{2}{2+\\rho}$. With $\\lambda_{0}=0$, the solution of the recursion is\n$$\n\\lambda_{k}=-2\\sum_{i=0}^{k-1}a^{i}=-2\\frac{1-a^{k}}{1-a}=2a^{k}-2.\n$$\nSubstitute into $x_{k+1}=\\frac{\\rho-\\lambda_{k}}{2+\\rho}$ to obtain\n$$\nx_{k+1}=\\frac{\\rho-(2a^{k}-2)}{2+\\rho}=\\frac{\\rho+2-2a^{k}}{2+\\rho}=1-\\frac{2a^{k}}{2+\\rho}\n=1-\\frac{2^{k+1}}{(2+\\rho)^{k+1}}.\n$$\nNow compute the first three minimizers:\n- For $k=0$,\n$$\nx_{1}=1-\\frac{2}{2+\\rho}=\\frac{\\rho}{2+\\rho}.\n$$\n- For $k=1$,\n$$\nx_{2}=1-\\frac{2^{2}}{(2+\\rho)^{2}}=\\frac{(2+\\rho)^{2}-4}{(2+\\rho)^{2}}=\\frac{\\rho(\\rho+4)}{(2+\\rho)^{2}}.\n$$\n- For $k=2$,\n$$\nx_{3}=1-\\frac{2^{3}}{(2+\\rho)^{3}}=\\frac{(2+\\rho)^{3}-8}{(2+\\rho)^{3}}=\\frac{\\rho(\\rho^{2}+6\\rho+12)}{(2+\\rho)^{3}}.\n$$\nTherefore, the requested expressions for $x_{1},x_{2},x_{3}$ in terms of $\\rho$ are as above.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\rho}{2+\\rho} & \\frac{\\rho(\\rho+4)}{(2+\\rho)^{2}} & \\frac{\\rho(\\rho^{2}+6\\rho+12)}{(2+\\rho)^{3}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Optimization problems in science and engineering are rarely limited to a single variable. This practice extends our skills to a multi-dimensional scenario, demonstrating the method's versatility and power in a more realistic setting. You will see that the fundamental principles remain unchanged, but the unconstrained minimization step now requires working with gradients and solving a system of linear equations , a common task in applied mathematics.",
            "id": "2208379",
            "problem": "Consider the optimization problem of minimizing the function $f(x_1, x_2) = x_1^2 + x_2^2$ subject to the linear equality constraint $x_1 + x_2 - 2 = 0$.\n\nThis problem can be addressed using the Augmented Lagrangian Method. For a general optimization problem with objective function $f(x)$ and an equality constraint $h(x) = 0$, the augmented Lagrangian is defined as:\n$$L_\\rho(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^2$$\nwhere $\\lambda$ is the estimate of the Lagrange multiplier and $\\rho > 0$ is a positive penalty parameter.\n\nThe method involves a sequence of unconstrained minimization subproblems. Starting with an initial multiplier estimate $\\lambda_0$, one finds the vector $x^{(1)}$ that minimizes $L_\\rho(x, \\lambda_0)$.\n\nYour task is to solve this first subproblem. Given an initial Lagrange multiplier estimate $\\lambda_0 = 0$, find the vector $x^{(1)} = (x_1^{(1)}, x_2^{(1)})$ that minimizes the corresponding augmented Lagrangian. Express your answer as a row vector in terms of the penalty parameter $\\rho$.",
            "solution": "We are given $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$ and the equality constraint $h(x) = x_{1} + x_{2} - 2 = 0$. The augmented Lagrangian with parameter $\\rho > 0$ is\n$$\nL_{\\rho}(x, \\lambda) = f(x) + \\lambda h(x) + \\frac{\\rho}{2}\\left[h(x)\\right]^{2}.\n$$\nWith the initial multiplier estimate $\\lambda_{0} = 0$, the first subproblem is the unconstrained minimization of\n$$\nL_{\\rho}(x, 0) = x_{1}^{2} + x_{2}^{2} + \\frac{\\rho}{2}\\left(x_{1} + x_{2} - 2\\right)^{2}.\n$$\nTo find its minimizer, set the gradient to zero. Let $s = x_{1} + x_{2} - 2$. Then\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{1}} = 2x_{1} + \\rho s = 2x_{1} + \\rho(x_{1} + x_{2} - 2) = 0,\n$$\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{2}} = 2x_{2} + \\rho s = 2x_{2} + \\rho(x_{1} + x_{2} - 2) = 0.\n$$\nThese yield the linear system\n$$\n(2+\\rho)x_{1} + \\rho x_{2} = 2\\rho, \\qquad \\rho x_{1} + (2+\\rho)x_{2} = 2\\rho.\n$$\nBy symmetry, the solution satisfies $x_{1} = x_{2} = t$. Substituting gives\n$$\n2t + \\rho(2t - 2) = 0 \\;\\;\\Longrightarrow\\;\\; (2 + 2\\rho)t = 2\\rho \\;\\;\\Longrightarrow\\;\\; t = \\frac{\\rho}{1+\\rho}.\n$$\nHence $x_{1}^{(1)} = x_{2}^{(1)} = \\frac{\\rho}{1+\\rho}$. The Hessian of $L_{\\rho}(x,0)$ is $2I + \\rho\\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix}$, which is positive definite for $\\rho > 0$, so this stationary point is the unique global minimizer.\n\nTherefore,\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{\\rho}{1+\\rho} & \\frac{\\rho}{1+\\rho} \\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\rho}{1+\\rho} & \\frac{\\rho}{1+\\rho}\\end{pmatrix}}$$"
        }
    ]
}