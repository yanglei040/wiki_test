{
    "hands_on_practices": [
        {
            "introduction": "Our first exercise provides a foundational entry point into the power of Lagrange multipliers. By seeking the extreme values of a simple linear function on a circular path, you'll engage with the central concept of constrained optimization in its most intuitive form. This practice is designed to build your skills in setting up the Lagrangian and solving the resulting system of equations, directly connecting algebraic manipulation to the geometric idea of tangency between level curves and the constraint boundary. ",
            "id": "2216764",
            "problem": "A sensor is constrained to move along a circular path in a two-dimensional plane, described by the equation $x^2 + y^2 = 1$. The sensor measures a quantity $f(x,y)$ that varies with position, given by the function $f(x, y) = 2x + 3y$. Determine the absolute minimum and maximum values of the quantity $f(x,y)$ that the sensor will measure along its path. Present your answer as a row matrix $\\begin{pmatrix} \\text{min\\_value} & \\text{max\\_value} \\end{pmatrix}$, where the first entry is the minimum value and the second entry is the maximum value.",
            "solution": "We seek the extrema of $f(x,y) = 2x + 3y$ subject to the constraint $x^{2} + y^{2} = 1$. Use the method of Lagrange multipliers with Lagrangian\n$$\n\\mathcal{L}(x,y,\\lambda) = 2x + 3y - \\lambda \\left(x^{2} + y^{2} - 1\\right).\n$$\nSet the partial derivatives to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x} = 2 - 2\\lambda x = 0,\\quad \\frac{\\partial \\mathcal{L}}{\\partial y} = 3 - 2\\lambda y = 0,\\quad \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(x^{2} + y^{2} - 1) = 0.\n$$\nFrom the first two equations,\n$$\nx = \\frac{1}{\\lambda}, \\quad y = \\frac{3}{2\\lambda}.\n$$\nImpose the constraint $x^{2} + y^{2} = 1$:\n$$\n\\left(\\frac{1}{\\lambda}\\right)^{2} + \\left(\\frac{3}{2\\lambda}\\right)^{2} = 1 \\;\\;\\Rightarrow\\;\\; \\frac{1}{\\lambda^{2}} + \\frac{9}{4\\lambda^{2}} = 1 \\;\\;\\Rightarrow\\;\\; \\frac{13}{4\\lambda^{2}} = 1 \\;\\;\\Rightarrow\\;\\; \\lambda^{2} = \\frac{13}{4}.\n$$\nThus $\\lambda = \\pm \\frac{\\sqrt{13}}{2}$. Evaluate $f$ using $x = \\frac{1}{\\lambda}$ and $y = \\frac{3}{2\\lambda}$:\n$$\nf = 2x + 3y = \\frac{2}{\\lambda} + \\frac{9}{2\\lambda} = \\frac{13}{2\\lambda}.\n$$\nFor $\\lambda = \\frac{\\sqrt{13}}{2}$, \n$$\nf_{\\max} = \\frac{13}{2}\\cdot \\frac{2}{\\sqrt{13}} = \\sqrt{13}.\n$$\nFor $\\lambda = -\\frac{\\sqrt{13}}{2}$,\n$$\nf_{\\min} = \\frac{13}{2}\\cdot \\left(-\\frac{2}{\\sqrt{13}}\\right) = -\\sqrt{13}.\n$$\nTherefore, the absolute minimum and maximum values of $f$ on the unit circle are $-\\sqrt{13}$ and $\\sqrt{13}$, respectively.",
            "answer": "$$\\boxed{\\begin{pmatrix}-\\sqrt{13} & \\sqrt{13}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Having mastered the basics in two dimensions, we now extend our analysis to a more complex three-dimensional scenario. This problem challenges you to find the optimal dimensions of a box inscribed within an ellipsoid to maximize a generalized objective function. A key technique you will practice here is the logarithmic transformation, a powerful trick that simplifies the differentiation of objective functions involving products. This method is widely applicable in fields like economics and statistics, and this exercise offers a perfect opportunity to add it to your problem-solving toolkit. ",
            "id": "2216756",
            "problem": "An architect is designing a special exhibition room inside a building with a large ellipsoidal dome. The room must be a rectangular box with its sides parallel to the axes of the ellipsoid. The eight corners of the rectangular box must lie on the surface of the ellipsoid, which is described by the equation $\\frac{x^2}{a^2} + \\frac{y^2}{b^2} + \\frac{z^2}{c^2} = 1$. The parameters $a, b, c$ are the positive semi-axes of the ellipsoid.\n\nThe \"livability score\" of the room, a measure of its aesthetic and functional quality, is determined by a weighted product of its half-dimensions $(x, y, z)$. This score is given by the function $S(x,y,z) = x^{\\alpha} y^{\\beta} z^{\\gamma}$, where $\\alpha, \\beta, \\gamma$ are positive real constants representing the design priorities for length, width, and height, respectively.\n\nUsing the method of Lagrange multipliers, determine the half-dimensions $(x, y, z)$ of the rectangular box that maximize this livability score. Your answer should be a set of three analytic expressions for $x$, $y$, and $z$ in terms of $a, b, c, \\alpha, \\beta,$ and $\\gamma$.",
            "solution": "We are to maximize the weighted product $S(x,y,z) = x^{\\alpha} y^{\\beta} z^{\\gamma}$ subject to the ellipsoidal constraint\n$$\n\\frac{x^{2}}{a^{2}} + \\frac{y^{2}}{b^{2}} + \\frac{z^{2}}{c^{2}} = 1,\n$$\nwith $x,y,z > 0$ as half-dimensions. Because $\\ln$ is strictly increasing, maximizing $S$ is equivalent to maximizing\n$$\nf(x,y,z) = \\alpha \\ln x + \\beta \\ln y + \\gamma \\ln z\n$$\nunder the same constraint.\n\nIntroduce a Lagrange multiplier $\\lambda$ for the constraint $g(x,y,z) = \\frac{x^{2}}{a^{2}} + \\frac{y^{2}}{b^{2}} + \\frac{z^{2}}{c^{2}} - 1 = 0$, and form the Lagrangian\n$$\n\\mathcal{L}(x,y,z,\\lambda) = \\alpha \\ln x + \\beta \\ln y + \\gamma \\ln z - \\lambda \\left( \\frac{x^{2}}{a^{2}} + \\frac{y^{2}}{b^{2}} + \\frac{z^{2}}{c^{2}} - 1 \\right).\n$$\nStationarity requires the partial derivatives with respect to $x,y,z$ to vanish:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\alpha}{x} - \\lambda \\frac{2x}{a^{2}} = 0,\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial y} = \\frac{\\beta}{y} - \\lambda \\frac{2y}{b^{2}} = 0,\\quad\n\\frac{\\partial \\mathcal{L}}{\\partial z} = \\frac{\\gamma}{z} - \\lambda \\frac{2z}{c^{2}} = 0.\n$$\nSolving each for $x^{2}, y^{2}, z^{2}$ yields\n$$\nx^{2} = \\frac{\\alpha a^{2}}{2\\lambda},\\quad\ny^{2} = \\frac{\\beta b^{2}}{2\\lambda},\\quad\nz^{2} = \\frac{\\gamma c^{2}}{2\\lambda}.\n$$\nSubstitute these into the constraint:\n$$\n\\frac{x^{2}}{a^{2}} + \\frac{y^{2}}{b^{2}} + \\frac{z^{2}}{c^{2}} = \\frac{\\alpha}{2\\lambda} + \\frac{\\beta}{2\\lambda} + \\frac{\\gamma}{2\\lambda} = \\frac{\\alpha + \\beta + \\gamma}{2\\lambda} = 1,\n$$\nwhich implies\n$$\n\\lambda = \\frac{\\alpha + \\beta + \\gamma}{2}.\n$$\nTherefore,\n$$\nx^{2} = \\frac{\\alpha a^{2}}{\\alpha + \\beta + \\gamma},\\quad\ny^{2} = \\frac{\\beta b^{2}}{\\alpha + \\beta + \\gamma},\\quad\nz^{2} = \\frac{\\gamma c^{2}}{\\alpha + \\beta + \\gamma}.\n$$\nSince $x,y,z$ represent half-dimensions, we take the positive square roots:\n$$\nx = a \\sqrt{\\frac{\\alpha}{\\alpha + \\beta + \\gamma}},\\quad\ny = b \\sqrt{\\frac{\\beta}{\\alpha + \\beta + \\gamma}},\\quad\nz = c \\sqrt{\\frac{\\gamma}{\\alpha + \\beta + \\gamma}}.\n$$\nBecause $f(x,y,z)$ is strictly concave on $x,y,z>0$ and the feasible set intersects the positive orthant smoothly, these first-order conditions yield the unique maximizer on the ellipsoidal surface with $x,y,z>0$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\na \\sqrt{\\frac{\\alpha}{\\alpha+\\beta+\\gamma}} & b \\sqrt{\\frac{\\beta}{\\alpha+\\beta+\\gamma}} & c \\sqrt{\\frac{\\gamma}{\\alpha+\\beta+\\gamma}}\n\\end{pmatrix}}$$"
        },
        {
            "introduction": "Our final practice takes a step beyond standard application to explore the theoretical underpinnings of the Lagrange multiplier method. We investigate a problem involving the matrix $1$-norm, where the objective function is not differentiable everywhere, a situation that violates a key assumption of the standard theorem. By analyzing the problem on a 'differentiable patch' and comparing the result to the known true maximum, you will gain a crucial insight into how gradient-based methods can miss optima that occur at non-differentiable points, or 'corners', of the constraint set. This exercise is designed to deepen your understanding of why and when the Lagrange multiplier method works, a critical step toward becoming a sophisticated practitioner of optimization. ",
            "id": "2216763",
            "problem": "The induced 1-norm of a real matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined by the constrained optimization problem $\\|A\\|_1 = \\max_{\\mathbf{x} \\in \\mathbb{R}^n, \\|\\mathbf{x}\\|_1=1} \\|A\\mathbf{x}\\|_1$, where $\\|\\mathbf{v}\\|_1 = \\sum_{i} |v_i|$ is the vector 1-norm. A direct application of the standard method of Lagrange multipliers to this problem is complicated by the fact that the objective function $f(\\mathbf{x}) = \\|A\\mathbf{x}\\|_1$ and the constraint function $g(\\mathbf{x}) = \\|\\mathbf{x}\\|_1 - 1 = 0$ are not differentiable everywhere.\n\nConsider the specific matrix $A = \\begin{pmatrix} 5 & 1 \\\\ 1 & -3 \\end{pmatrix}$. To understand the behavior of this optimization, we can analyze it on a 'patch' of the domain where the functions are differentiable. Let $R$ be the region in $\\mathbb{R}^2$ defined by the set of inequalities:\n$$x_1 > 0, \\quad x_2 > 0, \\quad \\text{and} \\quad x_1 - 3x_2 < 0$$\nWithin this region $R$, both $f(\\mathbf{x})$ and $g(\\mathbf{x})$ are continuously differentiable. Let $V_{cand}$ be the value of the objective function $f(\\mathbf{x})$ for any point $\\mathbf{x}$ that lies both on the constraint surface $\\|\\mathbf{x}\\|_1=1$ and within region $R$, and also satisfies the Lagrange multiplier first-order necessary condition, $\\nabla f(\\mathbf{x}) = \\lambda \\nabla g(\\mathbf{x})$, for some Lagrange multiplier $\\lambda$.\n\nThe true global maximum for this problem, $\\|A\\|_1$, is attained at a point on the boundary of the constraint set where the gradient is not well-defined. Calculate the exact value of the ratio $\\frac{V_{cand}}{\\|A\\|_1}$.",
            "solution": "We are given $A=\\begin{pmatrix}5 & 1 \\\\ 1 & -3\\end{pmatrix}$ and the induced $1$-norm $\\|A\\|_{1}=\\max_{\\mathbf{x}\\in\\mathbb{R}^{2},\\ \\|\\mathbf{x}\\|_{1}=1}\\|A\\mathbf{x}\\|_{1}$, with $f(\\mathbf{x})=\\|A\\mathbf{x}\\|_{1}$ and $g(\\mathbf{x})=\\|\\mathbf{x}\\|_{1}-1=0$. On the region $R=\\{\\mathbf{x}=(x_{1},x_{2})^{\\top}:\\ x_{1}>0,\\ x_{2}>0,\\ x_{1}-3x_{2}<0\\}$, the signs of the components of $A\\mathbf{x}$ are fixed, so $f$ and $g$ are continuously differentiable there.\n\nCompute $A\\mathbf{x}$:\n$$\nA\\mathbf{x}=\\begin{pmatrix}5 & 1 \\\\ 1 & -3\\end{pmatrix}\\begin{pmatrix}x_{1}\\\\x_{2}\\end{pmatrix}\n=\\begin{pmatrix}5x_{1}+x_{2}\\\\x_{1}-3x_{2}\\end{pmatrix}.\n$$\nOn $R$, we have $x_{1}>0$ and $x_{2}>0$, hence $5x_{1}+x_{2}>0$, and we are given $x_{1}-3x_{2}<0$. Therefore the absolute values resolve as\n$$\n|5x_{1}+x_{2}|=5x_{1}+x_{2},\\qquad |x_{1}-3x_{2}|=-(x_{1}-3x_{2})=3x_{2}-x_{1}.\n$$\nThus\n$$\nf(\\mathbf{x})=\\|A\\mathbf{x}\\|_{1}=(5x_{1}+x_{2})+(3x_{2}-x_{1})=4x_{1}+4x_{2}=4(x_{1}+x_{2}).\n$$\nOn the constraint $\\|\\mathbf{x}\\|_{1}=1$ with $x_{1},x_{2}>0$, we have $x_{1}+x_{2}=1$, hence for any feasible $\\mathbf{x}\\in R$,\n$$\nf(\\mathbf{x})=4.\n$$\nCompute gradients in $R$: $\\nabla f(\\mathbf{x})=4\\begin{pmatrix}1\\\\1\\end{pmatrix}$ and $\\nabla g(\\mathbf{x})=\\begin{pmatrix}1\\\\1\\end{pmatrix}$. The Lagrange multiplier first-order condition $\\nabla f(\\mathbf{x})=\\lambda \\nabla g(\\mathbf{x})$ is satisfied with $\\lambda=4$ for every $\\mathbf{x}\\in R$. Therefore any such point is a candidate, and the corresponding objective value is the constant\n$$\nV_{cand}=4.\n$$\n\nNext, compute the induced $1$-norm of $A$. For any matrix, $\\|A\\|_{1}$ equals the maximum absolute column sum:\n$$\n\\|A\\|_{1}=\\max_{j}\\sum_{i}|a_{ij}|.\n$$\nThe column sums are $|5|+|1|=6$ and $|1|+|{-3}|=4$, hence\n$$\n\\|A\\|_{1}=6.\n$$\nTherefore the desired ratio is\n$$\n\\frac{V_{cand}}{\\|A\\|_{1}}=\\frac{4}{6}=\\frac{2}{3}.\n$$",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        }
    ]
}