{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Arnoldi iteration, we must begin with its fundamental mechanics. This first practice exercise guides you through a single, complete step of the algorithm. By manually calculating the first entry of the Hessenberg matrix, $h_{1,1}$, and the next orthonormal vector, $v_2$, you will gain a concrete understanding of the normalization and orthogonalization process that lies at the heart of building a Krylov subspace . Mastering these foundational calculations is key to appreciating the power and elegance of the method.",
            "id": "2154392",
            "problem": "In numerical linear algebra, the Arnoldi iteration is an algorithm for building an orthonormal basis of the Krylov subspace generated by a matrix $A$ and a vector $b$. Consider the square matrix $A$ and the initial vector $b$ given by:\n$$ A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\nThe first step in the Arnoldi process is to normalize the initial vector to obtain the first basis vector, $v_1 = b / \\|b\\|_2$, where $\\|\\cdot\\|_2$ denotes the standard Euclidean norm. The iteration then proceeds to generate subsequent vectors and the entries of an upper Hessenberg matrix $H$.\n\nPerform one complete step of the Arnoldi iteration to compute the Hessenberg matrix entry $h_{1,1}$ and the second Arnoldi basis vector $v_2$.\n\nYour final answer should be given as a single row matrix containing four exact, symbolic values in the following order: the value of $h_{1,1}$, followed by the first, second, and third components of the vector $v_2$.",
            "solution": "We apply one step of the Arnoldi iteration.\n\nFirst, normalize $b$ to obtain $v_{1}$. The Euclidean norm is\n$$\n\\|b\\|_{2}=\\sqrt{1^{2}+2^{2}+2^{2}}=\\sqrt{9}=3,\n$$\nso\n$$\nv_{1}=\\frac{b}{\\|b\\|_{2}}=\\begin{pmatrix}\\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3}\\end{pmatrix}.\n$$\nCompute $w=A v_{1}$:\n$$\nw=\\begin{pmatrix}1 & 1 & 0\\\\ 1 & 1 & 0\\\\ 0 & 0 & 1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3}\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 1 \\\\ \\frac{2}{3}\\end{pmatrix}.\n$$\nThe Hessenberg entry is\n$$\nh_{1,1}=v_{1}^{\\top}w=\\frac{1}{3}\\cdot 1+\\frac{2}{3}\\cdot 1+\\frac{2}{3}\\cdot\\frac{2}{3}= \\frac{13}{9}.\n$$\nOrthogonalize and normalize to get $v_{2}$. Define\n$$\nr=w-h_{1,1}v_{1}=\\begin{pmatrix}1 \\\\ 1 \\\\ \\frac{2}{3}\\end{pmatrix}-\\frac{13}{9}\\begin{pmatrix}\\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3}\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{27} \\\\ \\frac{1}{27} \\\\ -\\frac{8}{27}\\end{pmatrix}.\n$$\nIts norm is\n$$\n\\|r\\|_{2}=\\sqrt{\\left(\\frac{14}{27}\\right)^{2}+\\left(\\frac{1}{27}\\right)^{2}+\\left(-\\frac{8}{27}\\right)^{2}}=\\frac{1}{27}\\sqrt{261}=\\frac{\\sqrt{29}}{9}.\n$$\nThus\n$$\nv_{2}=\\frac{r}{\\|r\\|_{2}}=\\frac{9}{\\sqrt{29}}\\begin{pmatrix}\\frac{14}{27} \\\\ \\frac{1}{27} \\\\ -\\frac{8}{27}\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{3\\sqrt{29}} \\\\ \\frac{1}{3\\sqrt{29}} \\\\ -\\frac{8}{3\\sqrt{29}}\\end{pmatrix}.\n$$\nTherefore, $h_{1,1}=\\frac{13}{9}$ and $v_{2}=\\left(\\frac{14}{3\\sqrt{29}},\\,\\frac{1}{3\\sqrt{29}},\\,-\\frac{8}{3\\sqrt{29}}\\right)^{\\top}$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{13}{9} & \\frac{14}{3\\sqrt{29}} & \\frac{1}{3\\sqrt{29}} & -\\frac{8}{3\\sqrt{29}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "The Arnoldi iteration constructs a basis for the Krylov subspace, but what happens if that subspace is smaller than expected? This thought experiment explores the special case where the iteration terminates after just one step . By determining which starting vector causes this 'early breakdown,' you will uncover a profound connection: the process stops when the starting vector is an eigenvector of the matrix, revealing that the algorithm has found a one-dimensional invariant subspace.",
            "id": "2154381",
            "problem": "The Arnoldi iteration is a numerical algorithm for approximating eigenvalues. For a given square matrix $A$ and a starting vector $v_1$, the first step involves normalizing the vector to $q_1 = v_1 / \\|v_1\\|_2$, computing $w = A q_1$, and then calculating a residual vector $r_1 = w - (q_1^T w) q_1$. The algorithm terminates after this first step if the residual vector $r_1$ is the zero vector.\n\nConsider a $3 \\times 3$ matrix $A$ with elements defined as $A_{11}=1$, $A_{12}=0$, $A_{13}=1$, $A_{21}=1$, $A_{22}=1$, $A_{23}=1$, $A_{31}=1$, $A_{32}=1$, and $A_{33}=-1$. Which of the following column vectors, when chosen as the starting vector $v_1$, will cause the Arnoldi iteration to terminate after just one step?\n\nA. A vector with components (1, 1, 1).\n\nB. A vector with components (1, 2, 1).\n\nC. A vector with components (1, -1, 0).\n\nD. A vector with components (0, 1, 1).\n\nE. A vector with components (1, 0, -1).",
            "solution": "The problem asks to identify which starting vector $v_1$ causes the Arnoldi iteration to terminate after the first step.\nAccording to the problem description, the first step of the Arnoldi iteration terminates if the residual vector $r_1$ is the zero vector.\nThe residual vector is defined as $r_1 = w - h_{11} q_1$, where $q_1 = v_1 / \\|v_1\\|_2$, $w = A q_1$, and $h_{11} = q_1^T w = q_1^T A q_1$.\n\nSetting $r_1 = 0$, we get:\n$$\nw - h_{11} q_1 = 0 \\implies w = h_{11} q_1\n$$\nSubstituting the expression for $w$:\n$$\nA q_1 = h_{11} q_1\n$$\nThis is the definition of an eigenvector and eigenvalue. The equation shows that the vector $q_1$ must be an eigenvector of the matrix $A$, and the scalar $h_{11}$ must be the corresponding eigenvalue.\n\nSince $q_1$ is just a normalized version of the starting vector $v_1$ (i.e., $q_1$ is $v_1$ scaled by $1/\\|v_1\\|_2$), if $q_1$ is an eigenvector, then $v_1$ must also be an eigenvector of $A$. Specifically, if $A q_1 = \\lambda q_1$, then:\n$$\nA \\left(\\frac{v_1}{\\|v_1\\|_2}\\right) = \\lambda \\left(\\frac{v_1}{\\|v_1\\|_2}\\right)\n$$\nMultiplying both sides by the scalar $\\|v_1\\|_2$ gives:\n$$\nA v_1 = \\lambda v_1\n$$\nThus, the problem reduces to finding which of the given vectors is an eigenvector of the matrix $A$.\n\nThe matrix $A$ is given by its components:\n$$\nA = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix}\n$$\nWe now test each option by multiplying the matrix $A$ by the vector $v_1$ from that option and checking if the resulting vector is a scalar multiple of the original vector.\n\nA. For $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(1) + 1(1) \\\\ 1(1) + 1(1) + 1(1) \\\\ 1(1) + 1(1) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? No, because $2/1 \\neq 3/1$.\n\nB. For $v_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(2) + 1(1) \\\\ 1(1) + 1(2) + 1(1) \\\\ 1(1) + 1(2) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? Yes, if we let $\\lambda=2$, we have $2 \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}$. This vector is an eigenvector of $A$.\n\nC. For $v_1 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(-1) + 1(0) \\\\ 1(1) + 1(-1) + 1(0) \\\\ 1(1) + 1(-1) - 1(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$ for some scalar $\\lambda$? No. For example, the second component is $0 = \\lambda(-1)$, which means $\\lambda=0$, but the first component is $1 = \\lambda(1)$, which would mean $\\lambda=1$. This is a contradiction.\n\nD. For $v_1 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(0) + 0(1) + 1(1) \\\\ 1(0) + 1(1) + 1(1) \\\\ 1(0) + 1(1) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\lambda \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? No. The first component requires $1 = \\lambda(0)$, which is impossible.\n\nE. For $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0) + 1(-1) \\\\ 1(1) + 1(0) + 1(-1) \\\\ 1(1) + 1(0) - 1(-1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$ for some scalar $\\lambda$? No. The first component requires $0 = \\lambda(1)$, which means $\\lambda=0$. But the third component requires $2 = \\lambda(-1)$, which would mean $\\lambda=-2$. This is a contradiction.\n\nOnly the vector in option B is an eigenvector of $A$. Therefore, choosing this vector as the starting vector $v_1$ will cause the Arnoldi iteration to terminate after the first step.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Building on the idea of early termination, we now investigate a more general principle governing the size of the Krylov subspace. This exercise examines a matrix $A$ with a specific low-rank structure and asks you to predict when the Arnoldi iteration must terminate . The solution demonstrates that the dimension of the Krylov subspace $\\mathcal{K}_m(A, b)$ is constrained by the dimension of the image of $A$, providing a crucial insight into why the process is guaranteed to stop and produce an exact, finite-dimensional representation for certain classes of matrices.",
            "id": "2154383",
            "problem": "In the analysis of large-scale linear dynamical systems, model order reduction is often achieved by projecting the system dynamics onto a lower-dimensional Krylov subspace. Consider a discrete-time linear system whose state evolution is governed by the matrix $A \\in \\mathbb{R}^{n \\times n}$. The matrix $A$ is constructed as the sum of two outer products:\n$$A = u_1 v_1^T + u_2 v_2^T$$\nwhere $u_1, u_2, v_1, v_2$ are non-zero column vectors in $\\mathbb{R}^n$. The pairs $\\{u_1, u_2\\}$ and $\\{v_1, v_2\\}$ are each linearly independent.\n\nThe Arnoldi iteration is a numerical algorithm used to generate an orthonormal basis $\\{q_1, q_2, \\dots, q_m\\}$ for the Krylov subspace $\\mathcal{K}_m(A, b) = \\text{span}\\{b, Ab, A^2b, \\dots, A^{m-1}b\\}$, starting from an initial vector $b$. The algorithm proceeds as follows:\n1. Initialize: $q_1 = b / \\|b\\|_2$.\n2. Iterate for $j = 1, 2, 3, \\dots, m-1$:\n    a. Compute the next Krylov vector: $w = A q_j$.\n    b. Orthogonalize $w$ against the previous basis vectors (Gram-Schmidt):\n        i. For $i=1, \\dots, j$, compute the projection coefficient: $h_{i,j} = q_i^T w$.\n        ii. Subtract the projection: $w_{\\perp} = w - \\sum_{i=1}^{j} h_{i,j} q_i$.\n    c. Compute the norm of the new orthogonal vector: $h_{j+1,j} = \\|w_{\\perp}\\|_2$.\n    d. If $h_{j+1,j} = 0$, the iteration terminates. Otherwise, normalize to get the next basis vector: $q_{j+1} = w_{\\perp} / h_{j+1,j}$.\n\nThe coefficients $h_{i,j}$ form an upper Hessenberg matrix $H_m$.\n\nSuppose we apply the Arnoldi iteration with a starting vector $b \\in \\mathbb{R}^n$ that is chosen such that the dimensions of the first three Krylov subspaces are $\\dim(\\mathcal{K}_1(A,b))=1$, $\\dim(\\mathcal{K}_2(A,b))=2$, and $\\dim(\\mathcal{K}_3(A,b))=3$.\n\nWhat is the value of the entry $h_{4,3}$?\n\nA. 1\n\nB. 0\n\nC. A non-zero value that depends on the norms of $u_1, u_2, v_1, v_2$.\n\nD. A non-zero value that depends on the inner products between $u_i, v_i$, and $b$.\n\nE. The value cannot be determined without the explicit numerical values of the vectors.",
            "solution": "Write $A$ as $A = u_{1} v_{1}^{T} + u_{2} v_{2}^{T}$. For any $x \\in \\mathbb{R}^{n}$,\n$$\nA x = u_{1}\\big(v_{1}^{T} x\\big) + u_{2}\\big(v_{2}^{T} x\\big) \\in \\operatorname{span}\\{u_{1},u_{2}\\}.\n$$\nHence $\\operatorname{Im}(A) \\subseteq \\operatorname{span}\\{u_{1},u_{2}\\}$ and, for all $k \\geq 1$, $A^{k} b \\in \\operatorname{span}\\{u_{1},u_{2}\\}$.\n\nTherefore, for any $m \\geq 1$,\n$$\n\\mathcal{K}_{m}(A,b) = \\operatorname{span}\\{b,Ab,\\dots,A^{m-1}b\\} \\subseteq \\operatorname{span}\\{b\\} + \\operatorname{span}\\{u_{1},u_{2}\\}.\n$$\nIn particular, $\\dim\\big(\\mathcal{K}_{m}(A,b)\\big) \\leq 3$. The hypothesis gives $\\dim\\big(\\mathcal{K}_{1}(A,b)\\big)=1$, $\\dim\\big(\\mathcal{K}_{2}(A,b)\\big)=2$, and $\\dim\\big(\\mathcal{K}_{3}(A,b)\\big)=3$, which forces\n$$\n\\mathcal{K}_{3}(A,b) = \\operatorname{span}\\{b,Ab,A^{2}b\\} = \\operatorname{span}\\{b\\} \\oplus \\operatorname{span}\\{u_{1},u_{2}\\}.\n$$\nSince $A b \\in \\operatorname{span}\\{u_{1},u_{2}\\}$ and $A$ maps $\\operatorname{span}\\{u_{1},u_{2}\\}$ into itself, it follows that\n$$\nA\\big(\\mathcal{K}_{3}(A,b)\\big) \\subseteq \\operatorname{span}\\{u_{1},u_{2}\\} \\subseteq \\mathcal{K}_{3}(A,b).\n$$\nThus $\\mathcal{K}_{3}(A,b)$ is $A$-invariant, and $A q_{3} \\in \\operatorname{span}\\{q_{1},q_{2},q_{3}\\}$ for any Arnoldi orthonormal basis $\\{q_{1},q_{2},q_{3}\\}$ of $\\mathcal{K}_{3}(A,b)$.\n\nIn the Arnoldi step $j=3$, one computes $w = A q_{3}$ and orthogonalizes against $q_{1},q_{2},q_{3}$, obtaining $w_{\\perp} = w - \\sum_{i=1}^{3} h_{i,3} q_{i}$. Because $w \\in \\operatorname{span}\\{q_{1},q_{2},q_{3}\\}$, it follows that $w_{\\perp} = 0$. Hence\n$$\nh_{4,3} = \\|w_{\\perp}\\|_{2} = 0.\n$$\nTherefore, the correct choice is B.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}