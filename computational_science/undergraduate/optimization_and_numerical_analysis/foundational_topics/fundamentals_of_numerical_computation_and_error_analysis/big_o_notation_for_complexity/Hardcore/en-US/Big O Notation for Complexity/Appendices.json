{
    "hands_on_practices": [
        {
            "introduction": "Our first practice problem applies complexity analysis to a fundamental concept in linear algebra: eigenvectors. Verifying if a vector is an eigenvector of a matrix is a common task in numerical methods, providing a concrete example of how we measure computational cost. This exercise  will guide you through breaking down the verification process into elementary computational steps and determining the overall time complexity, highlighting how a single, computationally intensive step often dominates the total cost.",
            "id": "2156952",
            "problem": "In the development of a numerical linear algebra library, a function is needed to verify the results of an eigenvector algorithm. This verification function takes as input an $n \\times n$ square matrix $A$ with real-valued entries, and a non-zero $n$-dimensional column vector $x$, also with real-valued entries. The function must determine if $x$ is an eigenvector of $A$. The analysis of the algorithm's performance is crucial for large-scale computations.\n\nYour task is to determine the worst-case time complexity of the most efficient algorithm to perform this verification. The time complexity should be expressed using Big O notation as a function of the matrix dimension $n$. Assume that basic arithmetic operations (addition, subtraction, multiplication, division) and comparisons take constant time, $O(1)$.\n\nWhat is the tightest Big O time complexity for this verification procedure?\n\nA. $O(n)$\n\nB. $O(n \\log n)$\n\nC. $O(n^2)$\n\nD. $O(n^3)$\n\nE. $O(n!)$",
            "solution": "We must verify whether the given nonzero vector $x \\in \\mathbb{R}^{n}$ is an eigenvector of the given matrix $A \\in \\mathbb{R}^{n \\times n}$. By definition, $x$ is an eigenvector of $A$ if and only if there exists a scalar $\\lambda \\in \\mathbb{R}$ such that\n$$\nA x = \\lambda x,\n$$\nwith $x \\neq 0$ given.\n\nAn efficient verification algorithm proceeds as follows.\n\n1) Compute the matrix-vector product $y = A x$. For each $i \\in \\{1,\\dots,n\\}$,\n$$\ny_{i} = \\sum_{j=1}^{n} a_{ij} x_{j}.\n$$\nThis requires $n$ multiplications and $n-1$ additions per row, thus a total of $\\Theta(n^{2})$ arithmetic operations. Under the assumption that each arithmetic operation is $O(1)$, this step costs $O(n^{2})$.\n\n2) Determine a candidate eigenvalue $\\lambda$. Find an index $k$ with $x_{k} \\neq 0$ (which must exist since $x \\neq 0$). In the worst case this search costs $O(n)$. Define $ \\lambda = \\frac{y_{k}}{x_{k}} $, which takes $O(1)$ time once $k$ is found.\n\n3) Verify the equality $y = \\lambda x$ componentwise. For each $j \\in \\{1,\\dots,n\\}$:\n- If $x_{j} \\neq 0$, check whether $y_{j} = \\lambda x_{j}$.\n- If $x_{j} = 0$, check whether $y_{j} = 0$.\nThis step uses $O(n)$ comparisons and at most $O(n)$ multiplications.\n\nTherefore, the total running time of this algorithm is\n$$\nO(n^{2}) + O(n) + O(n) = O(n^{2}),\n$$\ndominated by the matrix-vector multiplication.\n\nTo see that no algorithm can asymptotically do better in the worst case, consider a lower bound argument. Suppose $x$ has no zero entries. Any algorithm that inspects fewer than all $n^{2}$ entries of $A$ leaves at least one entry $a_{pq}$ unexamined. Construct two matrices $A$ and $A'$ that agree on all examined entries and differ only in $a_{pq}$ by some $\\delta \\neq 0$. Then $ (A' x)_{p} = (A x)_{p} + \\delta x_{q} $, which changes the $p$-th component by a nonzero amount since $x_{q} \\neq 0$. By choosing $\\delta$ appropriately, one of $A$ or $A'$ can satisfy $A x = \\lambda x$ for some $\\lambda$, while the other does not. An algorithm that did not inspect $a_{pq}$ cannot distinguish these cases, so any correct algorithm must, in the worst case, inspect $\\Omega(n^{2})$ entries, implying a time lower bound of $\\Omega(n^{2})$.\n\nCombining the $O(n^{2})$ upper bound with the $\\Omega(n^{2})$ lower bound gives a tight $\\Theta(n^{2})$ bound. In Big O notation, the tightest choice among the options is $O(n^{2})$, which corresponds to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Many real-world problems, from simulating physical systems to analyzing social networks, involve large matrices that are \"sparse,\" meaning most of their entries are zero. This practice problem  explores how using a specialized data structure to store only the non-zero elements drastically changes the complexity of matrix-vector multiplication. You will learn how the analysis can depend on multiple parameters, in this case, both the matrix dimension and its number of non-zero entries, revealing a key optimization strategy in computational science.",
            "id": "2156941",
            "problem": "In the field of computational science, many problems involve linear systems where the matrices are sparse. A matrix is considered sparse if the number of non-zero elements is significantly smaller than the total number of elements. To improve performance and reduce memory usage, specialized data structures and algorithms are used to handle these matrices.\n\nConsider an $n \\times n$ sparse matrix $A$ which contains exactly $k$ non-zero elements, where $k \\ll n^2$. The matrix $A$ is stored using a Coordinate List (COO) representation. This format uses three lists of length $k$: one for the values of the non-zero elements, one for their corresponding row indices, and one for their corresponding column indices.\n\nYou are tasked with analyzing the time complexity of an algorithm that computes the product $y = Ax$, where $x$ is a dense column vector of size $n$ (i.e., it has $n$ elements, all of which are potentially non-zero).\n\nThe algorithm proceeds as follows:\n1.  An output vector $y$ of size $n$ is created and all its elements are initialized to zero.\n2.  The algorithm then iterates exactly once through the $k$ non-zero elements of $A$ using the COO lists. For each non-zero element with value $v$ at row index $i$ and column index $j$, it performs the update: $y_i \\leftarrow y_i + v \\cdot x_j$.\n\nWhat is the worst-case time complexity of this algorithm, expressed using Big O notation?\n\nA. $O(n)$\n\nB. $O(k)$\n\nC. $O(n+k)$\n\nD. $O(nk)$\n\nE. $O(n^2)$\n\nF. $O(k \\log n)$",
            "solution": "Let $A \\in \\mathbb{R}^{n \\times n}$ have exactly $k$ non-zero entries stored in COO format, and let $x \\in \\mathbb{R}^{n}$ be dense. The algorithm computes $y = Ax$ as follows.\n\nStep 1 (Initialization): Create $y \\in \\mathbb{R}^{n}$ and set all entries to zero. Setting each entry is a constant-time operation. Let the constant cost per write be $c_{1}$. Then the time for this step is $ T_{1}(n) = c_{1} n $, which is $\\Theta(n)$ and thus $O(n)$.\n\nStep 2 (Single pass over non-zeros): Iterate once over the $k$ non-zero entries of $A$. For each non-zero $(v,i,j)$, perform the update $y_{i} \\leftarrow y_{i} + v \\cdot x_{j}$. Accessing $x_{j}$, multiplying by $v$, and adding to $y_{i}$ are all constant-time operations under the standard RAM model. Let the constant cost per non-zero be $c_{2}$. Then the time for this step is $ T_{2}(k) = c_{2} k $, which is $\\Theta(k)$ and thus $O(k)$.\n\nTotal time: The total running time is the sum of the two independent steps, $ T(n,k) = T_{1}(n) + T_{2}(k) = c_{1} n + c_{2} k $. In Big O notation this is\n$$\nT(n,k) \\in O(n + k).\n$$\nTherefore, the worst-case time complexity is $O(n + k)$, which corresponds to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "We now turn our attention from iterative to recursive algorithms, a powerful paradigm that solves problems by breaking them into smaller, self-similar subproblems. This exercise  presents a classic divide-and-conquer scenario, mirroring the structure of highly efficient algorithms like merge sort. By setting up and analyzing the corresponding recurrence relation, you will uncover one of the most important and efficient complexity classes in computer science, $O(n \\log n)$.",
            "id": "2156959",
            "problem": "A software engineer is designing a new algorithm, called 'LogSynthesizer', to process and consolidate server log files. The algorithm operates on a log file containing $n$ entries.\n\nThe LogSynthesizer algorithm is designed as follows:\n1. If the log file has only one entry (i.e., $n=1$), the algorithm performs a constant number of operations and terminates.\n2. If the log file has more than one entry, the algorithm performs the following three steps:\n    a. It divides the log file into two equal halves, each containing $n/2$ entries.\n    b. It recursively calls itself on each of these two halves independently.\n    c. After the two recursive calls return, it merges their processed results. This merging step requires a single pass through all the original $n$ entries to ensure consistency, and this pass takes a total time directly proportional to $n$.\n\nLet $T(n)$ be the function representing the total number of operations required by the LogSynthesizer algorithm for a log file of size $n$. Based on the description, determine the tightest asymptotic upper bound (Big O notation) for the time complexity of this algorithm.\n\nSelect the correct option from the following choices:\n\nA. $O(\\log n)$\n\nB. $O(n)$\n\nC. $O(n \\log n)$\n\nD. $O(n^2)$\n\nE. $O(2^n)$",
            "solution": "Let $T(n)$ denote the total operations. From the algorithm description:\n- Base case: $T(1)=\\Theta(1)$.\n- For $n>1$, the algorithm splits into two subproblems of size $n/2$, solves them recursively, and merges in linear time. Thus there exists a constant $c>0$ such that\n$$\nT(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+c\\,n.\n$$\n\nUsing a recursion tree argument, assume for simplicity that $n=2^{h}$ so that the tree has height $h=\\log_{2}(n)$. At level $i$ ($0 \\leq i \\leq h-1$), there are $2^{i}$ subproblems of size $n/2^{i}$. The total merging cost at level $i$ is $ 2^{i}\\cdot c\\left(\\frac{n}{2^{i}}\\right)=c\\,n $. Since there are $h=\\log_{2}(n)$ such levels before the leaves, the total non-leaf cost is $ c\\,n\\,\\log_{2}(n) $. At the leaves, there are $2^{h}=n$ subproblems of size $1$, each costing $\\Theta(1)$, so the total leaf cost is $\\Theta(n)$.\n\nTherefore,\n$$\nT(n)=\\Theta\\!\\left(n\\log_{2}(n)\\right)+\\Theta(n)=\\Theta\\!\\left(n\\log_{2}(n)\\right).\n$$\nEquivalently, $T(n)=\\Theta\\!\\left(n\\log(n)\\right)$ up to a constant factor in the logarithm base. Hence the tightest asymptotic upper bound is $O(n\\log n)$, which corresponds to option C.\n\nBy the Master Theorem, with $a=2$, $b=2$, and $f(n)=c\\,n$, we have $n^{\\log_{b}a}=n$, and $f(n)=\\Theta\\!\\left(n^{\\log_{b}a}\\right)$, placing the recurrence in Case 2 and yielding the same result $T(n)=\\Theta\\!\\left(n\\log n\\right)$.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}