{
    "hands_on_practices": [
        {
            "introduction": "Solving a quadratic equation is a fundamental task, yet the standard formula can be surprisingly fragile in the face of floating-point arithmetic. This exercise  challenges you to confront \"catastrophic cancellation,\" a major source of numerical error that occurs when subtracting two nearly equal numbers. By analyzing the formula's weak point, you will learn to derive an algebraically equivalent but numerically stable expression, a crucial skill for writing robust scientific code.",
            "id": "2435764",
            "problem": "Consider the quadratic polynomial $p(x) = x^2 - 10^8 x + 1$. Derive exact closed-form expressions for both real roots of $p(x) = 0$ from first principles. Then, using the basic model of rounding in finite precision floating-point arithmetic (as in the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision (binary$64$) standard), explain which of the two exact closed-form expressions is numerically unstable to evaluate and why, identifying the mechanism of catastrophic cancellation. Finally, rewrite the unstable root into an algebraically equivalent exact form that avoids subtractive cancellation. \n\nAnswer requirement: Provide the final answer as exact symbolic expressions for the two roots, without numerical approximation. No rounding is required. Express your final answer as a pair of exact expressions.",
            "solution": "The problem statement is subjected to validation before any attempt at a solution is made.\n\nFirst, we extract the givens of the problem.\nThe quadratic polynomial is given as $p(x) = x^2 - 10^8 x + 1$.\nThe problem requires finding the roots of the equation $p(x) = 0$.\nThe analysis of numerical stability must be performed within the context of finite precision floating-point arithmetic, specifically referencing the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision standard.\nThe task involves identifying which root expression derived from the standard quadratic formula is numerically unstable due to \"catastrophic cancellation\", and subsequently deriving an algebraically equivalent, but numerically stable, expression for that root.\n\nNext, we validate the problem statement.\nThe problem is scientifically grounded, as it deals with a fundamental topic in numerical analysis: the loss of precision when subtracting nearly equal numbers. This phenomenon, known as catastrophic cancellation, is a well-understood consequence of floating-point arithmetic. The polynomial and coefficients are mathematically sound.\nThe problem is well-posed. It provides all necessary information ($a=1$, $b=-10^8$, $c=1$) to find the roots and analyze their numerical properties. The question is unambiguous and leads to a unique set of stable expressions for the roots.\nThe problem is objective, stated in precise mathematical terms without any subjective or speculative content.\nTherefore, the problem is deemed valid and a solution will be furnished.\n\nThe quadratic equation to be solved is $x^2 - 10^8 x + 1 = 0$.\nWe apply the standard quadratic formula for the roots of $ax^2 + bx + c = 0$, which are given by $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$.\nFor the given polynomial, the coefficients are $a=1$, $b=-10^8$, and $c=1$.\nSubstituting these values into the formula yields:\n$$x = \\frac{-(-10^8) \\pm \\sqrt{(-10^8)^2 - 4(1)(1)}}{2(1)}$$\n$$x = \\frac{10^8 \\pm \\sqrt{10^{16} - 4}}{2}$$\nThis gives two exact, real roots:\n$$x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$$\n$$x_2 = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$$\n\nNow, we must analyze the numerical stability of these expressions in finite precision arithmetic. The source of potential instability lies in the subtraction of nearly equal quantities.\nLet us examine the magnitude of the terms involved. The term $10^{16}$ is vastly larger than $4$. Consequently, the value of $\\sqrt{10^{16} - 4}$ is very close to $\\sqrt{10^{16}} = 10^8$.\nTo see this more clearly, we can use a binomial approximation.\n$$\\sqrt{10^{16} - 4} = \\sqrt{10^{16}(1 - 4 \\times 10^{-16})} = 10^8 \\sqrt{1 - 4 \\times 10^{-16}}$$\nFor a small value $\\epsilon$, the approximation $(1 - \\epsilon)^{1/2} \\approx 1 - \\frac{1}{2}\\epsilon$ holds. Here, $\\epsilon = 4 \\times 10^{-16}$, which is very small.\nThus, $\\sqrt{10^{16} - 4} \\approx 10^8 (1 - \\frac{1}{2}(4 \\times 10^{-16})) = 10^8 (1 - 2 \\times 10^{-16}) = 10^8 - 2 \\times 10^{-8}$.\nThe value of $\\sqrt{10^{16} - 4}$ is only slightly less than $10^8$.\n\nConsider the expression for $x_1$: $x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$. This expression involves the addition of two large positive numbers of similar magnitude. In floating-point arithmetic, this operation is numerically stable. The relative error of the sum is small, on the order of the machine precision.\n\nConsider the expression for $x_2$: $x_2 = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$. This expression involves the subtraction of two very nearly equal numbers. Let $y = \\sqrt{10^{16} - 4}$. In floating-point computation, $10^8$ and the computed value of $y$ will agree in many of their leading significant digits. When the subtraction $10^8 - y$ is performed, these leading digits cancel, and the result is determined by the remaining, less significant digits, which are heavily influenced by the rounding errors incurred when computing $y$. This massive increase in relative error is the phenomenon of catastrophic cancellation. Therefore, the expression for $x_2$ is numerically unstable and would yield a highly inaccurate result if evaluated directly using standard double-precision arithmetic.\n\nTo find a numerically stable expression for $x_2$, we use Vieta's formulas, which relate the coefficients of a polynomial to its roots. For a quadratic equation $ax^2 + bx + c = 0$, the product of the roots is given by $x_1 x_2 = \\frac{c}{a}$.\nFor our equation, this gives $x_1 x_2 = \\frac{1}{1} = 1$.\nWe can compute the stable root $x_1$ accurately using its formula. Then, we can find $x_2$ from the relation $x_2 = \\frac{1}{x_1}$.\nSubstituting the stable expression for $x_1$:\n$$x_2 = \\frac{1}{\\frac{10^8 + \\sqrt{10^{16} - 4}}{2}} = \\frac{2}{10^8 + \\sqrt{10^{16} - 4}}$$\nThis revised expression for $x_2$ involves only the addition of positive numbers and a division, both of which are numerically stable operations. It avoids subtractive cancellation and is therefore the preferred form for numerical computation.\n\nWe verify that this new form is algebraically equivalent to the original unstable form for $x_2$ by rationalizing its denominator:\n$$\\frac{2}{10^8 + \\sqrt{10^{16} - 4}} \\cdot \\frac{10^8 - \\sqrt{10^{16} - 4}}{10^8 - \\sqrt{10^{16} - 4}} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{(10^8)^2 - (10^{16} - 4)} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{10^{16} - 10^{16} + 4} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{4} = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$$\nThis confirms the algebraic identity.\n\nThe final required answer consists of the exact symbolic expressions for both roots, rewritten in their numerically stable forms.\nThe larger root is $x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$.\nThe smaller root, in its stable form, is $x_2 = \\frac{2}{10^8 + \\sqrt{10^{16} - 4}}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{10^8 + \\sqrt{10^{16} - 4}}{2} & \\frac{2}{10^8 + \\sqrt{10^{16} - 4}} \\end{pmatrix}}$$"
        },
        {
            "introduction": "In pure mathematics, the sum of a set of numbers is the same regardless of the order of addition. In the world of finite-precision computing, this is not true. This hands-on exercise  demonstrates how summing a list of numbers from smallest-to-largest versus largest-to-smallest can yield different results due to a phenomenon called \"swamping,\" where small values are lost when added to large ones. By comparing these approaches against a high-precision reference, you will gain practical insight into how to minimize accumulated round-off error.",
            "id": "2447450",
            "problem": "You must write a complete, runnable program that demonstrates how finite-precision rounding in floating-point arithmetic causes the result of summing real numbers to depend on the order of operations. The investigation must be grounded in the standard rounding model of floating-point arithmetic. In this model, a floating-point addition of two real numbers is represented as $\\mathrm{fl}(x + y) = (x + y)(1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff (for Institute of Electrical and Electronics Engineers (IEEE) base-$2$ double precision, $u = 2^{-53}$). The program must use IEEE double precision ($64$-bit) as implemented by the host programming language for all floating-point operations.\n\nDefinitions and requirements:\n- Let $\\{a_i\\}_{i=1}^n$ be a finite list of real numbers. The exact real sum is $S = \\sum_{i=1}^n a_i$. A naive floating-point summation computes $\\hat{S}$ by iterating the binary addition $s \\leftarrow \\mathrm{fl}(s + a_i)$ from an initial $s = 0$ in some order.\n- You will compare two orders:\n  1. Ascending-by-magnitude order: sort by $|a_i|$ in ascending order using a stable sort; then sum from smallest magnitude to largest. Denote the result by $\\hat{S}_{\\mathrm{asc}}$.\n  2. Descending-by-magnitude order: sort by $|a_i|$ in descending order using a stable sort; then sum from largest magnitude to smallest. Denote the result by $\\hat{S}_{\\mathrm{desc}}$.\n- As a high-accuracy reference, compute $S_{\\mathrm{ref}}$ using arbitrary precision decimal arithmetic with at least $80$ decimal digits of precision, and treat the inputs as exact decimal rationals. This reference is used to quantify the absolute errors $|\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|$ and $|\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|$. To compare values in a single arithmetic domain, convert the floating-point results to exact decimal rationals before subtraction; that is, convert IEEE double $x$ to its exactly represented decimal `Decimal(x)` and then subtract from $S_{\\mathrm{ref}}$.\n\nYour program must implement the above, and produce a single line of output with the results for the following four test cases (test suite). For each test case, output the triple $[\\hat{S}_{\\mathrm{desc}} - \\hat{S}_{\\mathrm{asc}}, |\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|, |\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|]$ as native floating-point numbers.\n\nTest suite (lists are to be interpreted as exact decimal inputs; a stable sort must preserve the original order for ties in $|a_i|$):\n1. Case A (small numbers swamped by a large one): $\\{1.0, 10 \\text{ copies of } 10^{-16}\\}$, i.e., the list $\\{1.0, 10^{-16}, 10^{-16}, \\dots, 10^{-16}\\}$ with $10$ copies of $10^{-16}$.\n2. Case B (catastrophic cancellation with three terms): $\\{10^{16}, 1.0, -10^{16}\\}$.\n3. Case C (large cancellation plus moderate terms): $\\{10^{16}, -10^{16}, 3.14, 2.71\\}$.\n4. Case D (harmonic sum, $n = 10,000$): $\\left\\{ \\frac{1}{k} : k = 1, 2, \\dots, 10,000 \\right\\}$.\n\nImplementation details:\n- All floating-point summations for $\\hat{S}_{\\mathrm{asc}}$ and $\\hat{S}_{\\mathrm{desc}}$ must use IEEE double precision ($64$-bit) additions. Use a stable sort by absolute value for the two required orders.\n- The reference $S_{\\mathrm{ref}}$ must be computed using arbitrary precision decimal arithmetic with at least $80$ decimal digits. Each input $a_i$ must be interpreted as an exact decimal rational before summation in this high-precision arithmetic.\n- For absolute errors $|\\hat{S} - S_{\\mathrm{ref}}|$, first convert $\\hat{S}$ to its exact decimal representation, then subtract $S_{\\mathrm{ref}}$ in the high-precision decimal domain, and finally report the magnitude as a floating-point number.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a length-$3$ list for one test case. The exact format is\n$[[d_1, e^{\\mathrm{asc}}_1, e^{\\mathrm{desc}}_1], [d_2, e^{\\mathrm{asc}}_2, e^{\\mathrm{desc}}_2], [d_3, e^{\\mathrm{asc}}_3, e^{\\mathrm{desc}}_3], [d_4, e^{\\mathrm{asc}}_4, e^{\\mathrm{desc}}_4]]$,\nwhere $d_i = \\hat{S}_{\\mathrm{desc}} - \\hat{S}_{\\mathrm{asc}}$, $e^{\\mathrm{asc}}_i = |\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|$, and $e^{\\mathrm{desc}}_i = |\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|$. Units do not apply, and all angles, if any, are in radians by default (none are used here).",
            "solution": "The problem statement is valid. It is scientifically sound, well-posed, and provides a clear, quantitative task rooted in the fundamental principles of numerical analysis, specifically the study of round-off errors in floating-point arithmetic. The problem requires a demonstration of the non-associativity of floating-point addition.\n\nThe core of the problem is to compute the sum of a list of numbers, $\\{a_i\\}_{i=1}^n$, using three different methods and to compare the results. The first two methods use standard IEEE $754$ double-precision ($64$-bit) floating-point arithmetic but differ in the order of summation. The third method uses high-precision arithmetic to establish a reference value, considered here as the \"exact\" sum.\n\nThe methodological approach is structured as follows:\n\n1.  **High-Precision Reference Summation ($S_{\\mathrm{ref}}$)**: To establish a ground truth for the sum, we first compute it using arbitrary-precision decimal arithmetic. The precision is set to $100$ decimal digits, which exceeds the requirement of at least $80$ digits. Each input number $a_i$ is treated as an exact decimal rational. For terms like $1/k$, the division is performed within the high-precision context to maintain accuracy. The sum of these high-precision numbers yields the reference sum, $S_{\\mathrm{ref}}$.\n\n2.  **Floating-Point Summation ($\\hat{S}_{\\mathrm{asc}}$ and $\\hat{S}_{\\mathrm{desc}}$)**: The summations in finite precision are performed using standard double-precision floating-point numbers. The problem specifies two orderings based on the magnitude of the terms:\n    *   **Ascending Order ($\\hat{S}_{\\mathrm{asc}}$)**: The list of numbers $\\{a_i\\}$ is sorted according to their absolute values, $|a_i|$, in ascending order. A stable sorting algorithm is used, which ensures that numbers with equal magnitudes maintain their original relative order. The sum is then computed by iterating through this sorted list, accumulating the total with a naive sequential summation: $s_0 = 0$, $s_j = \\mathrm{fl}(s_{j-1} + a'_j)$, where $\\{a'_j\\}$ is the sorted list. The final result is $\\hat{S}_{\\mathrm{asc}} = s_n$.\n    *   **Descending Order ($\\hat{S}_{\\mathrm{desc}}$)**: The procedure is identical, except the list is sorted by $|a_i|$ in descending order. The resulting sum is denoted $\\hat{S}_{\\mathrm{desc}}$.\n\n3.  **Error Analysis**: The analysis involves computing three quantities for each test case.\n    *   The discrepancy between the two floating-point results, $d = \\hat{S}_{\\mathrm{desc}} - \\hat{S}_{\\mathrm{asc}}$. This subtraction is performed in standard double-precision arithmetic.\n    *   The absolute error of the ascending-order sum, $e_{\\mathrm{asc}} = |\\hat{S}_{\\mathrm{asc}} - S_{\\mathrm{ref}}|$.\n    *   The absolute error of the descending-order sum, $e_{\\mathrm{desc}} = |\\hat{S}_{\\mathrm{desc}} - S_{\\mathrm{ref}}|$.\n    To compute these errors accurately, the floating-point results $\\hat{S}_{\\mathrm{asc}}$ and $\\hat{S}_{\\mathrm{desc}}$ are first converted into their exact high-precision decimal representations. The subtraction and absolute value operations are then carried out in the high-precision domain before the final error value is converted back to a standard double-precision float for output.\n\nThis methodology is applied to four distinct test cases designed to highlight specific phenomena of floating-point arithmetic.\n*   **Case A** demonstrates **swamping**, where adding a small number to a very large number results in the loss of the small number's value. Summing in ascending order ($small \\to large$) is expected to be more accurate because the small numbers are first accumulated into a sum that is large enough not to be entirely lost when added to the largest number.\n*   **Case B** and **Case C** involve **catastrophic cancellation**, the subtraction of two nearly equal large numbers. This operation can lead to a dramatic loss of significant digits. The accuracy depends on when the cancellation occurs relative to the addition of other, smaller terms. If the cancellation happens first (as in the descending-magnitude sort), the subsequent addition of small terms to a result near zero is accurate. If a small term is added to a large number before cancellation (as in the ascending sort), its value is swamped and lost.\n*   **Case D**, the partial sum of the harmonic series $\\sum_{k=1}^{10000} \\frac{1}{k}$, involves summing a large number of terms with a wide range of magnitudes. The general heuristic for minimizing accumulated round-off error in such sums is to add the numbers from smallest to largest. Therefore, $\\hat{S}_{\\mathrm{asc}}$ is expected to be significantly more accurate than $\\hat{S}_{\\mathrm{desc}}$.\n\nThe program implements this logic, processing each test case to compute the triple $[d, e_{\\mathrm{asc}}, e_{\\mathrm{desc}}]$ and formats the final output as a list of these results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing summation order effects in floating-point arithmetic.\n    \"\"\"\n\n    def analyze_summation(float_numbers, decimal_numbers):\n        \"\"\"\n        Computes sums in different orders and compares them to a high-precision reference.\n\n        Args:\n            float_numbers (list): A list of numbers as standard Python floats (IEEE 754 double).\n            decimal_numbers (list): A list of numbers as high-precision Decimal objects.\n\n        Returns:\n            list: A list containing [d, e_asc, e_desc] as floats.\n        \"\"\"\n        # 1. Compute the high-precision reference sum S_ref.\n        # The sum is initialized with Decimal(0) to ensure high-precision accumulation.\n        s_ref = sum(decimal_numbers, decimal.Decimal(0))\n\n        # 2. Compute floating-point sums S_asc and S_desc.\n        # The problem requires a stable sort by absolute value. Python's sorted() is stable.\n\n        # Ascending-by-magnitude summation\n        asc_sorted_floats = sorted(float_numbers, key=abs)\n        # Use np.sum for sequential summation as specified by environment.\n        s_asc_float = np.sum(np.array(asc_sorted_floats, dtype=np.float64))\n        \n        # Descending-by-magnitude summation\n        desc_sorted_floats = sorted(float_numbers, key=abs, reverse=True)\n        s_desc_float = np.sum(np.array(desc_sorted_floats, dtype=np.float64))\n\n        # 3. Calculate the required output quantities.\n        # d = S_desc - S_asc, computed in standard floating-point arithmetic.\n        d = s_desc_float - s_asc_float\n\n        # To calculate errors accurately, convert float sums to their exact Decimal representation.\n        # Decimal(float_value) creates a Decimal that exactly represents the binary float.\n        s_asc_decimal = decimal.Decimal(s_asc_float)\n        s_desc_decimal = decimal.Decimal(s_desc_float)\n        \n        # Calculate absolute errors in the high-precision domain.\n        e_asc_decimal = abs(s_asc_decimal - s_ref)\n        e_desc_decimal = abs(s_desc_decimal - s_ref)\n        \n        # Convert final error values back to float for output.\n        return [d, float(e_asc_decimal), float(e_desc_decimal)]\n\n    # Set precision for decimal calculations. 100 digits is safely above the required 80.\n    decimal.getcontext().prec = 100\n    \n    all_results = []\n    \n    # Test Case A: small numbers swamped by a large one\n    inputs_a_str = ['1.0'] + ['1e-16'] * 10\n    floats_a = [float(s) for s in inputs_a_str]\n    decimals_a = [decimal.Decimal(s) for s in inputs_a_str]\n    all_results.append(analyze_summation(floats_a, decimals_a))\n    \n    # Test Case B: catastrophic cancellation with three terms\n    inputs_b_str = ['1e16', '1.0', '-1e16']\n    floats_b = [float(s) for s in inputs_b_str]\n    decimals_b = [decimal.Decimal(s) for s in inputs_b_str]\n    all_results.append(analyze_summation(floats_b, decimals_b))\n\n    # Test Case C: large cancellation plus moderate terms\n    inputs_c_str = ['1e16', '-1e16', '3.14', '2.71']\n    floats_c = [float(s) for s in inputs_c_str]\n    decimals_c = [decimal.Decimal(s) for s in inputs_c_str]\n    all_results.append(analyze_summation(floats_c, decimals_c))\n\n    # Test Case D: harmonic sum, n = 10,000\n    n = 10000\n    # The float numbers for standard summation\n    floats_d = [1.0 / k for k in range(1, n + 1)]\n    # The high-precision decimal numbers for the reference sum.\n    # Division is performed in the high-precision context.\n    decimals_d = [decimal.Decimal(1) / decimal.Decimal(k) for k in range(1, n + 1)]\n    all_results.append(analyze_summation(floats_d, decimals_d))\n    \n    # Final print statement in the exact required format.\n    # str() on a list produces the required '[item1, item2, ...]' format.\n    formatted_results = ','.join(map(str, all_results))\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen that summation order matters, we now explore a more powerful technique for achieving high-accuracy sums. This practice  introduces the Kahan summation algorithm, a classic method for compensated summation that drastically reduces accumulated round-off error. By implementing and testing this algorithm against a naive sum, you will see firsthand how a clever algorithm can overcome the inherent limitations of floating-point hardware and provide more reliable results.",
            "id": "2447409",
            "problem": "You must write a complete, runnable program that evaluates round-off error in summing sequences of real numbers and demonstrates error reduction using the Kahan summation algorithm. All computations are to be carried out in standard double-precision binary floating-point arithmetic. For each test case, compute two sums of the same sequence: a naive left-to-right floating-point sum and a compensated sum using the Kahan summation algorithm. For each sum, compute the absolute error with respect to a high-accuracy reference sum. Your program must then output a single line containing all absolute errors for all test cases in a specified order and format.\n\nDefine absolute error for a computed sum $\\hat{S}$ relative to a reference value $S^{\\star}$ as $E = \\lvert \\hat{S} - S^{\\star} \\rvert$.\n\nThe test suite consists of the following four sequences:\n\n- Test case $1$ (many tiny increments added to a large baseline):\n  - Sequence $S_1$ has length $N_1 + 1$, where $N_1 = 10^{6}$. The first term is $s^{(1)}_0 = 1$, and the remaining $N_1$ terms are $s^{(1)}_k = 10^{-16}$ for $1 \\le k \\le N_1$.\n\n- Test case $2$ (repeated catastrophic cancellation triplets):\n  - Let $M = 2 \\cdot 10^{5}$. Sequence $S_2$ is the concatenation of $M$ blocks of three terms $(1, 10^{-16}, -1)$.\n\n- Test case $3$ (deterministic pseudo-random small-magnitude values with slight bias):\n  - Let the modulus $m = 2^{64}$, multiplier $a = 6364136223846793005$, increment $c = 1442695040888963407$, and seed $x_0 = 123456789123456789$. Define a linear congruential generator by $x_{k+1} \\equiv a x_k + c \\pmod{m}$ for $k \\ge 0$. Let $N_3 = 5 \\cdot 10^{4}$. For $k = 1, 2, \\dots, N_3$, define $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ and the sequence term $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$. The sequence $S_3$ consists of these $N_3$ terms.\n\n- Test case $4$ (dynamic range and cancellation in a short sequence):\n  - Sequence $S_4$ has five terms: $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$.\n\nFor each test case $i \\in \\{1,2,3,4\\}$, compute:\n- The naive sum $\\hat{S}^{\\text{naive}}_i$ by left-to-right accumulation in double-precision floating-point arithmetic.\n- The Kahan-compensated sum $\\hat{S}^{\\text{Kahan}}_i$ using the Kahan summation algorithm in double-precision floating-point arithmetic.\n- A high-accuracy reference $S^{\\star}_i$ computed from the mathematical definition of the sequence using exact arithmetic where possible or using base-$10$ arbitrary precision arithmetic with at least $50$ correct decimal digits such that rounding in double-precision does not contaminate $S^{\\star}_i$.\n\nFor each test case $i$, compute the absolute errors $E^{\\text{naive}}_i = \\lvert \\hat{S}^{\\text{naive}}_i - S^{\\star}_i \\rvert$ and $E^{\\text{Kahan}}_i = \\lvert \\hat{S}^{\\text{Kahan}}_i - S^{\\star}_i \\rvert$.\n\nFinal output format:\n- Produce a single line of output containing a comma-separated list enclosed in square brackets. The list must contain the $8$ numbers in the following order:\n  - $E^{\\text{naive}}_1, E^{\\text{Kahan}}_1, E^{\\text{naive}}_2, E^{\\text{Kahan}}_2, E^{\\text{naive}}_3, E^{\\text{Kahan}}_3, E^{\\text{naive}}_4, E^{\\text{Kahan}}_4$.\n- Each number must be rounded to $12$ significant digits, expressed as a decimal (scientific notation is acceptable).\n- Example of the required single-line format (illustrative only): `[e_1,e_2,e_3,e_4,e_5,e_6,e_7,e_8]`.\n\nNo physical units or angle units are involved in this problem. The program must be self-contained and must not require any user input or external files. The results must be reproducible exactly from the definitions above in any modern programming language that adheres to standard double-precision floating-point semantics.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the principles of numerical analysis, specifically concerning floating-point arithmetic and round-off error. The problem is well-posed, with all necessary data and definitions provided to compute a unique, verifiable solution. It is objective and free from ambiguity.\n\nThe core of this problem is to demonstrate and quantify the loss of precision that occurs during the summation of floating-point numbers of disparate magnitudes and the mitigation of this error using a compensated summation algorithm.\n\nAll calculations are performed using standard double-precision floating-point arithmetic, which corresponds to the IEEE 754 $64$-bit format. This format has approximately $15$ to $17$ decimal digits of precision. The machine epsilon, $\\epsilon$, which is the smallest number such that $1.0 + \\epsilon > 1.0$, is approximately $2.22 \\times 10^{-16}$ for this format. When two numbers of vastly different magnitudes are added, the smaller number may be partially or completely lost. This phenomenon is known as swamping.\n\nThe first method of summation is naive, left-to-right accumulation. For a sequence $s_0, s_1, \\dots, s_N$, the sum $\\hat{S}^{\\text{naive}}$ is computed as $(\\dots((s_0 + s_1) + s_2) + \\dots + s_N)$. This method is highly susceptible to round-off error.\n\nThe second method is the Kahan summation algorithm, a method of compensated summation. It significantly reduces the numerical error in the total obtained by adding a sequence of finite-precision floating-point numbers. The algorithm maintains a running compensation variable, $c$, which accumulates the error that would otherwise be lost. For each term $s_k$ in the sequence, the update rules are:\n$$y_k = s_k - c_{k-1}$$\n$$t_k = \\text{sum}_{k-1} + y_k$$\n$$c_k = (t_k - \\text{sum}_{k-1}) - y_k$$\n$$\\text{sum}_k = t_k$$\nHere, $\\text{sum}_0 = 0$ and $c_0 = 0$. The term $(t_k - \\text{sum}_{k-1})$ recovers the high-order part of $y_k$, and subtracting $y_k$ from this isolates the low-order part (the round-off error), which is stored in $c_k$ and subtracted from the next term $s_{k+1}$.\n\nThe absolute error is defined as $E = \\lvert \\hat{S} - S^{\\star} \\rvert$, where $\\hat{S}$ is a computed sum and $S^{\\star}$ is a high-accuracy reference sum.\n\nAnalysis of Test Cases:\n\nTest Case $1$:\nThe sequence is $s^{(1)}_0 = 1$ followed by $N_1 = 10^6$ terms of $s^{(1)}_k = 10^{-16}$ for $k \\ge 1$.\nThe exact sum is $S^{\\star}_1 = 1 + 10^6 \\times 10^{-16} = 1 + 10^{-10}$.\nIn naive summation, we compute $1 + 10^{-16} + 10^{-16} + \\dots$. The term $10^{-16}$ is very close to the machine epsilon relative to $1.0$. The operation $1.0 + 10^{-16}$ in double-precision arithmetic will suffer from swamping; the result is likely to be rounded back to $1.0$. Thus, most of the small terms will be lost, and $\\hat{S}^{\\text{naive}}_1$ is expected to be very close to $1.0$, resulting in an error close to $10^{-10}$.\nThe Kahan algorithm will capture the lost part $10^{-16}$ in the compensation variable $c$ at each step and reintroduce it, yielding a result $\\hat{S}^{\\text{Kahan}}_1$ that is extremely close to $S^{\\star}_1$. The error $E^{\\text{Kahan}}_1$ should be near machine precision.\n\nTest Case $2$:\nThe sequence consists of $M = 2 \\cdot 10^5$ blocks of $(1, 10^{-16}, -1)$.\nThe exact sum of one block is $1 + 10^{-16} - 1 = 10^{-16}$. The total exact sum is $S^{\\star}_2 = 2 \\cdot 10^5 \\times 10^{-16} = 2 \\cdot 10^{-11}$.\nNaive summation will compute $(1 + 10^{-16}) - 1$. As in the first case, $1 + 10^{-16}$ will likely round to $1.0$, and thus $(1 + 10^{-16}) - 1$ evaluates to $0$. Repeating this for all blocks, $\\hat{S}^{\\text{naive}}_2$ is expected to be $0.0$, leading to an error $E^{\\text{naive}}_2$ of exactly $2 \\cdot 10^{-11}$.\nThe Kahan algorithm will prevent this cancellation error, producing a sum $\\hat{S}^{\\text{Kahan}}_2$ very close to $S^{\\star}_2$, and a much smaller error $E^{\\text{Kahan}}_2$.\n\nTest Case $3$:\nThe sequence consists of $N_3 = 5 \\cdot 10^4$ terms $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$, where $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ and $x_k$ is from an LCG. The values of $u_k$ are pseudo-random in $[-0.5, 0.5)$. The terms $s^{(3)}_k$ are small, with a small positive bias of $10^{-16}$.\nThe exact sum is $S^{\\star}_3 = \\sum_{k=1}^{N_3} (10^{-12} u_k + 10^{-16}) = 10^{-12} \\sum_{k=1}^{N_3} u_k + N_3 \\cdot 10^{-16}$.\nThis sum must be computed using high-precision arithmetic to serve as the reference $S^{\\star}_3$. The LCG states $x_{k+1} \\equiv a x_k + c \\pmod{m}$ are computed using $64$-bit integer arithmetic. The sum $\\sum x_k$ is computed using arbitrary-precision integers, and the final expression for $S^{\\star}_3$ is evaluated using high-precision decimal arithmetic.\nNaive summation will accumulate small round-off errors over the $5 \\cdot 10^4$ additions. The Kahan algorithm is expected to minimize this accumulation, leading to $E^{\\text{Kahan}}_3 \\ll E^{\\text{naive}}_3$.\n\nTest Case $4$:\nThe sequence is $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$.\nThe exact sum is $S^{\\star}_4 = (10^{16} - 10^{16}) + (1 + 3) + 4 \\cdot 10^{-16} = 4 + 4 \\cdot 10^{-16}$.\nNaive left-to-right summation calculates step-by-step:\n1. $10^{16} + 1 = 10^{16}$ (swamping, since $1$ is less than the unit in the last place of $10^{16}$).\n2. $10^{16} - 10^{16} = 0$.\n3. $0 + 3 = 3$.\n4. $3 + 4 \\cdot 10^{-16} = 3$ (swamping, since $4 \\cdot 10^{-16}$ is smaller than machine epsilon relative to $3$).\nSo, $\\hat{S}^{\\text{naive}}_4 = 3$. The error is $E^{\\text{naive}}_4 = \\lvert 3 - (4 + 4 \\cdot 10^{-16}) \\rvert \\approx 1$.\nThe Kahan summation algorithm is designed to handle this. The loss of $1$ in the first step will be captured by the compensation variable. The final sum $\\hat{S}^{\\text{Kahan}}_4$ should be very close to the true sum $S^{\\star}_4$, resulting in a very small error $E^{\\text{Kahan}}_4$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Computes and prints round-off errors for naive and Kahan summations\n    for four specific test cases, adhering to the problem specification.\n    \"\"\"\n\n    def naive_sum(sequence):\n        \"\"\"Computes the naive left-to-right sum of a sequence.\"\"\"\n        s = 0.0\n        for x in sequence:\n            s += x\n        return s\n\n    def kahan_sum(sequence):\n        \"\"\"Computes the sum of a sequence using Kahan's algorithm.\"\"\"\n        s = 0.0\n        c = 0.0\n        for x in sequence:\n            y = x - c\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def generate_test_cases():\n        \"\"\"Generates the sequences for all four test cases.\"\"\"\n        # Test Case 1: Many tiny increments\n        N1 = 10**6\n        seq1 = np.full(N1, 1e-16, dtype=np.float64)\n        seq1 = np.insert(seq1, 0, 1.0)\n        \n        # Test Case 2: Repeated catastrophic cancellation\n        M = 2 * 10**5\n        block = np.array([1.0, 1e-16, -1.0], dtype=np.float64)\n        seq2 = np.tile(block, M)\n        \n        # Test Case 3: LCG-based sequence\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        seq3 = np.zeros(N3, dtype=np.float64)\n        x_current = x0\n        for k in range(N3):\n            x_current = (a * x_current + c) % m\n            u_k = x_current / m - 0.5\n            seq3[k] = 1e-12 * u_k + 1e-16\n\n        # Test Case 4: Dynamic range and cancellation\n        seq4 = np.array([1e16, 1.0, -1e16, 3.0, 4e-16], dtype=np.float64)\n        \n        return [seq1, seq2, seq3, seq4]\n\n    def get_reference_sums():\n        \"\"\"Computes high-accuracy reference sums for all test cases.\"\"\"\n        # Set precision for Decimal calculations\n        decimal.getcontext().prec = 100\n\n        # Reference Sum 1\n        N1 = 10**6\n        s_star_1 = decimal.Decimal(1) + decimal.Decimal(N1) * decimal.Decimal('1e-16')\n\n        # Reference Sum 2\n        M = 2 * 10**5\n        s_star_2 = decimal.Decimal(M) * decimal.Decimal('1e-16')\n\n        # Reference Sum 3\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        sum_x = 0\n        x_current = x0\n        for _ in range(N3):\n            x_current = (a * x_current + c) % m\n            sum_x += x_current\n        \n        D_sum_x = decimal.Decimal(sum_x)\n        D_m = decimal.Decimal(m)\n        D_N3 = decimal.Decimal(N3)\n        D_1e_12 = decimal.Decimal('1e-12')\n        D_1e_16 = decimal.Decimal('1e-16')\n        D_half = decimal.Decimal('0.5')\n        \n        sum_u = D_sum_x / D_m - D_N3 * D_half\n        s_star_3 = D_1e_12 * sum_u + D_N3 * D_1e_16\n\n        # Reference Sum 4\n        s_star_4 = decimal.Decimal('4') + decimal.Decimal('4e-16')\n        \n        return [float(s_star_1), float(s_star_2), float(s_star_3), float(s_star_4)]\n\n    sequences = generate_test_cases()\n    reference_sums = get_reference_sums()\n    \n    results = []\n    \n    for i in range(4):\n        seq = sequences[i]\n        s_star = reference_sums[i]\n        \n        # Naive sum and its error\n        s_naive = naive_sum(seq)\n        e_naive = abs(s_naive - s_star)\n        \n        # Kahan sum and its error\n        s_kahan = kahan_sum(seq)\n        e_kahan = abs(s_kahan - s_star)\n        \n        results.extend([e_naive, e_kahan])\n\n    # Format output to 12 significant digits and print\n    formatted_results = [f\"{res:.12g}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}