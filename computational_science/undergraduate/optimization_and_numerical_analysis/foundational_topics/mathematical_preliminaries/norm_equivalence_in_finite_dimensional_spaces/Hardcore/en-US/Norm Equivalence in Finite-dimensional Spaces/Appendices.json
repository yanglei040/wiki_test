{
    "hands_on_practices": [
        {
            "introduction": "To build a strong intuition for norm equivalence, we begin with a concrete example in a familiar setting. This first practice  invites you to find the precise relationship between the Euclidean norm ($\\|\\cdot\\|_2$) and the maximum norm ($\\|\\cdot\\|_{\\infty}$) in the three-dimensional space $\\mathbb{R}^3$. By calculating the tightest possible constants that bound one norm with the other, you will gain a tangible understanding of what 'equivalence' means in practice.",
            "id": "2191489",
            "problem": "In the analysis of numerical algorithms, it is often necessary to relate different vector norms. For any vector in a finite-dimensional space, all norms are equivalent, meaning they can be bounded by each other.\n\nConsider the real vector space $\\mathbb{R}^3$. For a vector $x = (x_1, x_2, x_3) \\in \\mathbb{R}^3$, the Euclidean norm (or 2-norm) is defined as $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + x_3^2}$, and the maximum norm (or infinity-norm) is defined as $\\|x\\|_{\\infty} = \\max\\{|x_1|, |x_2|, |x_3|\\}$.\n\nFind the best possible (i.e., the tightest) positive real constants $c_1$ and $c_2$ that satisfy the following inequality for any non-zero vector $x \\in \\mathbb{R}^3$:\n$$c_1 \\|x\\|_{\\infty} \\le \\|x\\|_2 \\le c_2 \\|x\\|_{\\infty}$$\nProvide the values of the ordered pair $(c_1, c_2)$ as the final answer.",
            "solution": "Let $x=(x_{1},x_{2},x_{3}) \\in \\mathbb{R}^{3}$ be nonzero, and set $m=\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|\\}$.\n\nLower bound:\nChoose an index $j$ such that $|x_{j}|=m$. Then by the definition of the Euclidean norm,\n$$\n\\|x\\|_{2}=\\sqrt{x_{1}^{2}+x_{2}^{2}+x_{3}^{2}} \\ge \\sqrt{x_{j}^{2}}=|x_{j}|=m=\\|x\\|_{\\infty}.\n$$\nHence, for all nonzero $x$,\n$$\n\\|x\\|_{2} \\ge \\|x\\|_{\\infty}.\n$$\nThis shows that any valid $c_{1}$ must satisfy $c_{1} \\le 1$. The constant $c_{1}=1$ is attained, for example, by vectors with only one nonzero component, e.g., $x=(m,0,0)$, for which $\\|x\\|_{2}=\\|x\\|_{\\infty}=m$. Therefore, the best possible lower constant is\n$$\nc_{1}=1.\n$$\n\nUpper bound:\nSince $|x_{i}|\\le m$ for each $i$, we have $x_{i}^{2}\\le m^{2}$ for each $i$. Summing yields\n$$\n\\|x\\|_{2}^{2}=x_{1}^{2}+x_{2}^{2}+x_{3}^{2}\\le 3m^{2}.\n$$\nTaking square roots gives\n$$\n\\|x\\|_{2}\\le \\sqrt{3}\\,m=\\sqrt{3}\\,\\|x\\|_{\\infty}.\n$$\nThus any valid $c_{2}$ must satisfy $c_{2}\\ge \\sqrt{3}$. Equality is achieved, for example, by $x=(m,m,m)$, where\n$$\n\\|x\\|_{2}=\\sqrt{m^{2}+m^{2}+m^{2}}=\\sqrt{3}\\,m=\\sqrt{3}\\,\\|x\\|_{\\infty}.\n$$\nTherefore, the best possible upper constant is\n$$\nc_{2}=\\sqrt{3}.\n$$\n\nCombining, the tight constants are $c_{1}=1$ and $c_{2}=\\sqrt{3}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1  \\sqrt{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having established the concept in a fixed, low-dimensional space, we now explore how the relationship between norms behaves as the dimension of the space changes. This exercise  asks you to find the equivalence constant relating the 1-norm and the 2-norm in a general $n$-dimensional space. Discovering how this constant depends on $n$ reveals a critical insight: while norms in finite-dimensional spaces are always equivalent, the 'quality' of this equivalence can change significantly with dimension.",
            "id": "2191529",
            "problem": "In the field of data analysis, various norms are used to quantify the magnitude of error vectors. Consider a model that produces an error vector $e = (e_1, e_2, \\dots, e_n)$ in an $n$-dimensional real space, $\\mathbb{R}^n$. The performance of this model is evaluated based on two different error metrics.\n\nThe first metric is the Euclidean norm (or 2-norm), which is related to the root mean square error. It is defined as $\\|e\\|_2 = \\sqrt{\\sum_{i=1}^n e_i^2}$.\nThe second metric is the 1-norm, which corresponds to the sum of absolute errors. It is defined as $\\|e\\|_1 = \\sum_{i=1}^n |e_i|$.\n\nSuppose that through a normalization process, an error vector $e$ is known to have a Euclidean norm of exactly one, i.e., $\\|e\\|_2 = 1$. For a worst-case analysis, we need to determine the maximum possible value for the 1-norm, given this constraint on the 2-norm.\n\nWhat is the maximum possible value of $\\|e\\|_1$ for an error vector $e \\in \\mathbb{R}^n$ that satisfies the condition $\\|e\\|_2 = 1$? Your answer should be a closed-form analytic expression in terms of the dimension $n$.",
            "solution": "We are asked to maximize the 1-norm of $e=(e_{1},\\dots,e_{n})\\in\\mathbb{R}^{n}$ subject to the constraint $\\|e\\|_{2}=1$. Because both $\\|e\\|_{1}$ and $\\|e\\|_{2}$ depend only on the absolute values of the components, define $x_{i}=|e_{i}|$ so that $x_{i}\\geq 0$ for all $i$, and the problem becomes:\nmaximize $\\sum_{i=1}^{n}x_{i}$ subject to $\\sum_{i=1}^{n}x_{i}^{2}=1$ and $x_{i}\\geq 0$.\n\nApply the Cauchy–Schwarz inequality to the vectors $(x_{1},\\dots,x_{n})$ and $(1,\\dots,1)$:\n$$\n\\left(\\sum_{i=1}^{n}x_{i}\\cdot 1\\right)^{2}\\leq \\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)\\left(\\sum_{i=1}^{n}1^{2}\\right)=1\\cdot n=n.\n$$\nTaking square roots yields\n$$\n\\sum_{i=1}^{n}x_{i}\\leq \\sqrt{n}.\n$$\nEquality in Cauchy–Schwarz holds if and only if the two vectors are proportional, i.e., there exists $c\\geq 0$ such that $x_{i}=c$ for all $i$. Imposing the constraint $\\sum_{i=1}^{n}x_{i}^{2}=1$ gives $n c^{2}=1$, hence $c=1/\\sqrt{n}$. For this choice, we have\n$$\n\\|e\\|_{1}=\\sum_{i=1}^{n}|e_{i}|=\\sum_{i=1}^{n}x_{i}=n\\cdot \\frac{1}{\\sqrt{n}}=\\sqrt{n},\n$$\nand $\\|e\\|_{2}=\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}=\\sqrt{n\\cdot \\frac{1}{n}}=1$, so the constraint is satisfied and the bound is achieved.\n\nTherefore, the maximum possible value of $\\|e\\|_{1}$ given $\\|e\\|_{2}=1$ is $\\sqrt{n}$.",
            "answer": "$$\\boxed{\\sqrt{n}}$$"
        },
        {
            "introduction": "The principle of norm equivalence extends beyond the familiar Euclidean spaces to other finite-dimensional vector spaces, such as spaces of functions. This final practice  challenges you to apply the concept to the space of linear polynomials by relating a norm based on polynomial coefficients to an integral norm. This exercise demonstrates the theorem's broad applicability and strengthens your analytical skills by combining concepts from linear algebra and calculus.",
            "id": "2191480",
            "problem": "Consider the vector space $P_1$ of all real polynomials with a degree of at most one. Any polynomial $p$ in this space can be uniquely written as $p(x) = ax+b$ for some real coefficients $a$ and $b$.\n\nOn this vector space, we can define various norms. Let's consider two specific norms:\n1. The coefficient-based infinity norm, defined as $\\|p\\|_{\\infty} = \\max(|a|, |b|)$.\n2. The integral norm, defined as $\\|p\\|_{I} = \\int_{0}^{1} |p(x)| dx$.\n\nWe are interested in finding the tightest possible bounds that relate these two norms. Specifically, we seek the 'best' positive constants, denoted $C_{\\min}$ and $C_{\\max}$, such that the inequality\n$$C_{\\min} \\|p\\|_{\\infty} \\le \\|p\\|_{I} \\le C_{\\max} \\|p\\|_{\\infty}$$\nholds for every non-zero polynomial $p \\in P_1$. The term 'best' implies that $C_{\\min}$ is the largest possible value and $C_{\\max}$ is the smallest possible value for which this inequality is always true.\n\nCalculate the sum of these two optimal constants, $C_{\\min} + C_{\\max}$. Provide your answer as a single numerical value.",
            "solution": "Let $P_{1}$ be the space of real polynomials $p(x)=ax+b$. Define the norms $\\|p\\|_{\\infty}=\\max(|a|,|b|)$ and $\\|p\\|_{I}=\\int_{0}^{1}|ax+b|\\,dx$. We seek the optimal constants\n$$\nC_{\\min}=\\inf_{p\\neq 0}\\frac{\\|p\\|_{I}}{\\|p\\|_{\\infty}},\\qquad\nC_{\\max}=\\sup_{p\\neq 0}\\frac{\\|p\\|_{I}}{\\|p\\|_{\\infty}}.\n$$\nBoth norms are positively homogeneous of degree $1$, so the ratio $\\|p\\|_{I}/\\|p\\|_{\\infty}$ is scale-invariant. Therefore, it suffices to restrict to the set $\\{(a,b):\\max(|a|,|b|)=1\\}$ and analyze\n$$\nI(a,b):=\\int_{0}^{1}|ax+b|\\,dx\n$$\nwith the constraint $\\max(|a|,|b|)=1$, since for such $(a,b)$ we have $\\|p\\|_{\\infty}=1$ and hence $\\|p\\|_{I}=I(a,b)$.\n\nFor $a\\neq 0$, an antiderivative is\n$$\n\\int |ax+b|\\,dx=\\frac{(ax+b)|ax+b|}{2a}+C,\n$$\nso\n$$\nI(a,b)=\\left[\\frac{(ax+b)|ax+b|}{2a}\\right]_{0}^{1}=\\frac{(a+b)|a+b|-b|b|}{2a}.\n$$\nFor $a=0$, one has $I(0,b)=\\int_{0}^{1}|b|\\,dx=|b|$.\n\nWe now optimize $I(a,b)$ over the boundary $\\max(|a|,|b|)=1$, i.e., the union of the edges $|a|=1$ with $|b|\\leq 1$ and $|b|=1$ with $|a|\\leq 1$.\n\n1) Edge $a=1$, $b\\in[-1,1]$. Using the formula,\n- If $b\\geq 0$, then $b|b|=b^{2}$ and $|1+b|=1+b$, hence\n$$\nI(1,b)=\\frac{(1+b)^{2}-b^{2}}{2}=\\frac{1+2b}{2}=\\frac{1}{2}+b,\n$$\nwhich on $[0,1]$ ranges from $1/2$ to $3/2$.\n- If $b\\in[-1,0)$, then $b|b|=-b^{2}$ and $|1+b|=1+b$, hence\n$$\nI(1,b)=\\frac{(1+b)^{2}+b^{2}}{2}=\\frac{1+2b+2b^{2}}{2}.\n$$\nDifferentiating gives $\\frac{d}{db}I(1,b)=1+2b$, which vanishes at $b=-1/2$. This yields the minimum\n$$\nI\\left(1,-\\frac{1}{2}\\right)=\\frac{1+2(-1/2)+2(1/4)}{2}=\\frac{1-1+1/2}{2}=\\frac{1}{4}.\n$$\nAt the endpoints $b=-1,0$, the value is $1/2$.\n\nThus on $a=1$ the range is $[1/4,\\,3/2]$.\n\n2) Edge $a=-1$, $b\\in[-1,1]$. By symmetry, $I(-1,b)=I(1,-b)$. Hence the range is again $[1/4,\\,3/2]$, with the minimum at $b=1/2$ and maximum at $b=-1$.\n\n3) Edge $b=1$, $a\\in[-1,1]$. Since $ax+1\\geq 0$ on $[0,1]$, \n$$\nI(a,1)=\\int_{0}^{1}(ax+1)\\,dx=\\frac{a}{2}+1,\n$$\nwhich ranges from $1/2$ (at $a=-1$) to $3/2$ (at $a=1$).\n\n4) Edge $b=-1$, $a\\in[-1,1]$. Since $ax-1\\leq 0$ on $[0,1]$,\n$$\nI(a,-1)=\\int_{0}^{1}(1-ax)\\,dx=1-\\frac{a}{2},\n$$\nwhich ranges from $1/2$ (at $a=1$) to $3/2$ (at $a=-1$).\n\nFrom these edge analyses, the global minimum over $\\max(|a|,|b|)=1$ is $C_{\\min}=1/4$, attained, for example, at $(a,b)=(1,-1/2)$, and the global maximum is $C_{\\max}=3/2$, attained at the corners $(\\pm 1,\\pm 1)$ with equal signs. Therefore,\n$$\nC_{\\min}+C_{\\max}=\\frac{1}{4}+\\frac{3}{2}=\\frac{7}{4}.\n$$\nThese constants are tight since they are achieved by the exhibited polynomials.",
            "answer": "$$\\boxed{\\frac{7}{4}}$$"
        }
    ]
}