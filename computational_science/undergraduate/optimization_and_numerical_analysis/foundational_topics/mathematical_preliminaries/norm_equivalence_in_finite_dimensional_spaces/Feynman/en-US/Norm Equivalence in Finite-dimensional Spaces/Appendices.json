{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a concrete entry point into the concept of norm equivalence. By working in the familiar space of $\\mathbb{R}^3$, you will determine the precise constants that bind the Euclidean norm ($\\|x\\|_2$) to the maximum norm ($\\|x\\|_{\\infty}$). This practice helps build intuition by demonstrating how to find the 'tightest' possible bounds, a fundamental skill in error analysis and numerical stability proofs .",
            "id": "2191489",
            "problem": "In the analysis of numerical algorithms, it is often necessary to relate different vector norms. For any vector in a finite-dimensional space, all norms are equivalent, meaning they can be bounded by each other.\n\nConsider the real vector space $\\mathbb{R}^3$. For a vector $x = (x_1, x_2, x_3) \\in \\mathbb{R}^3$, the Euclidean norm (or 2-norm) is defined as $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + x_3^2}$, and the maximum norm (or infinity-norm) is defined as $\\|x\\|_{\\infty} = \\max\\{|x_1|, |x_2|, |x_3|\\}$.\n\nFind the best possible (i.e., the tightest) positive real constants $c_1$ and $c_2$ that satisfy the following inequality for any non-zero vector $x \\in \\mathbb{R}^3$:\n$$c_1 \\|x\\|_{\\infty} \\le \\|x\\|_2 \\le c_2 \\|x\\|_{\\infty}$$\nProvide the values of the ordered pair $(c_1, c_2)$ as the final answer.",
            "solution": "Let $x=(x_{1},x_{2},x_{3}) \\in \\mathbb{R}^{3}$ be nonzero, and set $m=\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|\\}$.\n\nLower bound:\nChoose an index $j$ such that $|x_{j}|=m$. Then by the definition of the Euclidean norm,\n$$\n\\|x\\|_{2}=\\sqrt{x_{1}^{2}+x_{2}^{2}+x_{3}^{2}} \\ge \\sqrt{x_{j}^{2}}=|x_{j}|=m=\\|x\\|_{\\infty}.\n$$\nHence, for all nonzero $x$,\n$$\n\\|x\\|_{2} \\ge \\|x\\|_{\\infty}.\n$$\nThis shows that any valid $c_{1}$ must satisfy $c_{1} \\le 1$. The constant $c_{1}=1$ is attained, for example, by vectors with only one nonzero component, e.g., $x=(m,0,0)$, for which $\\|x\\|_{2}=\\|x\\|_{\\infty}=m$. Therefore, the best possible lower constant is\n$$\nc_{1}=1.\n$$\n\nUpper bound:\nSince $|x_{i}|\\le m$ for each $i$, we have $x_{i}^{2}\\le m^{2}$ for each $i$. Summing yields\n$$\n\\|x\\|_{2}^{2}=x_{1}^{2}+x_{2}^{2}+x_{3}^{2}\\le 3m^{2}.\n$$\nTaking square roots gives\n$$\n\\|x\\|_{2}\\le \\sqrt{3}\\,m=\\sqrt{3}\\,\\|x\\|_{\\infty}.\n$$\nThus any valid $c_{2}$ must satisfy $c_{2}\\ge \\sqrt{3}$. Equality is achieved, for example, by $x=(m,m,m)$, where\n$$\n\\|x\\|_{2}=\\sqrt{m^{2}+m^{2}+m^{2}}=\\sqrt{3}\\,m=\\sqrt{3}\\,\\|x\\|_{\\infty}.\n$$\nTherefore, the best possible upper constant is\n$$\nc_{2}=\\sqrt{3}.\n$$\n\nCombining, the tight constants are $c_{1}=1$ and $c_{2}=\\sqrt{3}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 & \\sqrt{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "After establishing the concept in a fixed dimension, we generalize our analysis to an arbitrary $n$-dimensional space, $\\mathbb{R}^n$. This problem challenges you to find the relationship between the 1-norm ($\\|x\\|_1$) and the 2-norm ($\\|x\\|_2$), showing how the equivalence constant can depend on the dimension of the space . Mastering this type of analysis is crucial for understanding the behavior of algorithms in high-dimensional settings, such as those encountered in machine learning and data science.",
            "id": "2191512",
            "problem": "In the field of data analysis, vectors in an $n$-dimensional real space, $\\mathbb{R}^n$, are fundamental objects. The \"magnitude\" of these vectors is often quantified using norms. Two of the most common norms are the L1-norm (or Manhattan norm) and the L2-norm (or Euclidean norm). For a vector $x = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$, these are defined as:\n\n- L1-norm: $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$\n- L2-norm: $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}$\n\nIn finite-dimensional spaces, all norms are equivalent. This means that for any two norms, say $\\|\\cdot\\|_a$ and $\\|\\cdot\\|_b$, there exist positive constants $c$ and $C$ such that $c \\|x\\|_a \\le \\|x\\|_b \\le C \\|x\\|_a$ for all vectors $x$.\n\nConsider the specific relationship $\\|x\\|_1 \\le C \\|x\\|_2$. Your task is to find the smallest possible value of the constant $C$ that makes this inequality true for all vectors $x \\in \\mathbb{R}^n$. Express this optimal constant $C$ as a function of the dimension $n$.",
            "solution": "We seek the smallest constant $C$ such that for all $x=(x_{1},\\dots,x_{n})\\in\\mathbb{R}^{n}$,\n$$\n\\|x\\|_{1}\\leq C\\|x\\|_{2}.\n$$\nBy the Cauchy–Schwarz inequality, for any real sequences $\\{a_{i}\\}$ and $\\{b_{i}\\}$,\n$$\n\\left(\\sum_{i=1}^{n}a_{i}b_{i}\\right)^{2}\\leq\\left(\\sum_{i=1}^{n}a_{i}^{2}\\right)\\left(\\sum_{i=1}^{n}b_{i}^{2}\\right).\n$$\nApply this with $a_{i}=1$ for all $i$ and $b_{i}=|x_{i}|$. Then\n$$\n\\left(\\sum_{i=1}^{n}|x_{i}|\\right)^{2}\\leq\\left(\\sum_{i=1}^{n}1^{2}\\right)\\left(\\sum_{i=1}^{n}|x_{i}|^{2}\\right)=n\\sum_{i=1}^{n}x_{i}^{2}.\n$$\nRecognizing the norms, this gives\n$$\n\\|x\\|_{1}^{2}\\leq n\\|x\\|_{2}^{2}\\quad\\Longrightarrow\\quad \\|x\\|_{1}\\leq \\sqrt{n}\\,\\|x\\|_{2}.\n$$\nThus $C=\\sqrt{n}$ is valid. To show optimality, consider any nonzero vector with equal absolute components, for example $x$ with $x_{i}=t$ for all $i$ where $t\\neq 0$. Then\n$$\n\\|x\\|_{1}=n|t|,\\qquad \\|x\\|_{2}=\\sqrt{n}\\,|t|,\n$$\nso\n$$\n\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}=\\sqrt{n}.\n$$\nTherefore, any $C<\\sqrt{n}$ fails for such $x$, and the smallest possible constant is $C=\\sqrt{n}$.",
            "answer": "$$\\boxed{\\sqrt{n}}$$"
        },
        {
            "introduction": "To demonstrate the broad applicability of norm equivalence, this final practice moves beyond standard vector spaces to a finite-dimensional function space—the space of linear polynomials. You will compare a norm defined by the polynomial's coefficients with an integral norm measuring its 'size' over an interval . This exercise requires the use of calculus and optimization, showcasing how the principles of norm equivalence extend to more abstract mathematical settings.",
            "id": "2191480",
            "problem": "Consider the vector space $P_1$ of all real polynomials with a degree of at most one. Any polynomial $p$ in this space can be uniquely written as $p(x) = ax+b$ for some real coefficients $a$ and $b$.\n\nOn this vector space, we can define various norms. Let's consider two specific norms:\n1. The coefficient-based infinity norm, defined as $\\|p\\|_{\\infty} = \\max(|a|, |b|)$.\n2. The integral norm, defined as $\\|p\\|_{I} = \\int_{0}^{1} |p(x)| dx$.\n\nWe are interested in finding the tightest possible bounds that relate these two norms. Specifically, we seek the 'best' positive constants, denoted $C_{\\min}$ and $C_{\\max}$, such that the inequality\n$$C_{\\min} \\|p\\|_{\\infty} \\le \\|p\\|_{I} \\le C_{\\max} \\|p\\|_{\\infty}$$\nholds for every non-zero polynomial $p \\in P_1$. The term 'best' implies that $C_{\\min}$ is the largest possible value and $C_{\\max}$ is the smallest possible value for which this inequality is always true.\n\nCalculate the sum of these two optimal constants, $C_{\\min} + C_{\\max}$. Provide your answer as a single numerical value.",
            "solution": "Let $P_{1}$ be the space of real polynomials $p(x)=ax+b$. Define the norms $\\|p\\|_{\\infty}=\\max(|a|,|b|)$ and $\\|p\\|_{I}=\\int_{0}^{1}|ax+b|\\,dx$. We seek the optimal constants\n$$\nC_{\\min}=\\inf_{p\\neq 0}\\frac{\\|p\\|_{I}}{\\|p\\|_{\\infty}},\\qquad\nC_{\\max}=\\sup_{p\\neq 0}\\frac{\\|p\\|_{I}}{\\|p\\|_{\\infty}}.\n$$\nBoth norms are positively homogeneous of degree $1$, so the ratio $\\|p\\|_{I}/\\|p\\|_{\\infty}$ is scale-invariant. Therefore, it suffices to restrict to the set $\\{(a,b):\\max(|a|,|b|)=1\\}$ and analyze\n$$\nI(a,b):=\\int_{0}^{1}|ax+b|\\,dx\n$$\nwith the constraint $\\max(|a|,|b|)=1$, since for such $(a,b)$ we have $\\|p\\|_{\\infty}=1$ and hence $\\|p\\|_{I}=I(a,b)$.\n\nFor $a\\neq 0$, an antiderivative is\n$$\n\\int |ax+b|\\,dx=\\frac{(ax+b)|ax+b|}{2a}+C,\n$$\nso\n$$\nI(a,b)=\\left[\\frac{(ax+b)|ax+b|}{2a}\\right]_{0}^{1}=\\frac{(a+b)|a+b|-b|b|}{2a}.\n$$\nFor $a=0$, one has $I(0,b)=\\int_{0}^{1}|b|\\,dx=|b|$.\n\nWe now optimize $I(a,b)$ over the boundary $\\max(|a|,|b|)=1$, i.e., the union of the edges $|a|=1$ with $|b|\\leq 1$ and $|b|=1$ with $|a|\\leq 1$.\n\n1) Edge $a=1$, $b\\in[-1,1]$. Using the formula,\n- If $b\\geq 0$, then $b|b|=b^{2}$ and $|1+b|=1+b$, hence\n$$\nI(1,b)=\\frac{(1+b)^{2}-b^{2}}{2}=\\frac{1+2b}{2}=\\frac{1}{2}+b,\n$$\nwhich on $[0,1]$ ranges from $1/2$ to $3/2$.\n- If $b\\in[-1,0)$, then $b|b|=-b^{2}$ and $|1+b|=1+b$, hence\n$$\nI(1,b)=\\frac{(1+b)^{2}+b^{2}}{2}=\\frac{1+2b+2b^{2}}{2}.\n$$\nDifferentiating gives $\\frac{d}{db}I(1,b)=1+2b$, which vanishes at $b=-1/2$. This yields the minimum\n$$\nI\\left(1,-\\frac{1}{2}\\right)=\\frac{1+2(-1/2)+2(1/4)}{2}=\\frac{1-1+1/2}{2}=\\frac{1}{4}.\n$$\nAt the endpoints $b=-1,0$, the value is $1/2$.\n\nThus on $a=1$ the range is $[1/4,\\,3/2]$.\n\n2) Edge $a=-1$, $b\\in[-1,1]$. By symmetry, $I(-1,b)=I(1,-b)$. Hence the range is again $[1/4,\\,3/2]$, with the minimum at $b=1/2$ and maximum at $b=-1$.\n\n3) Edge $b=1$, $a\\in[-1,1]$. Since $ax+1\\geq 0$ on $[0,1]$, \n$$\nI(a,1)=\\int_{0}^{1}(ax+1)\\,dx=\\frac{a}{2}+1,\n$$\nwhich ranges from $1/2$ (at $a=-1$) to $3/2$ (at $a=1$).\n\n4) Edge $b=-1$, $a\\in[-1,1]$. Since $ax-1\\leq 0$ on $[0,1]$,\n$$\nI(a,-1)=\\int_{0}^{1}(1-ax)\\,dx=1-\\frac{a}{2},\n$$\nwhich ranges from $1/2$ (at $a=1$) to $3/2$ (at $a=-1$).\n\nFrom these edge analyses, the global minimum over $\\max(|a|,|b|)=1$ is $C_{\\min}=1/4$, attained, for example, at $(a,b)=(1,-1/2)$, and the global maximum is $C_{\\max}=3/2$, attained at the corners $(\\pm 1,\\pm 1)$ with equal signs. Therefore,\n$$\nC_{\\min}+C_{\\max}=\\frac{1}{4}+\\frac{3}{2}=\\frac{7}{4}.\n$$\nThese constants are tight since they are achieved by the exhibited polynomials.",
            "answer": "$$\\boxed{\\frac{7}{4}}$$"
        }
    ]
}