{
    "hands_on_practices": [
        {
            "introduction": "The bisection method's primary strength lies in its predictable and guaranteed convergence. This first practice explores the fundamental relationship between the number of iterations and the resulting precision. By analyzing how many additional steps are needed to achieve a tenfold increase in accuracy, you will gain a core intuition for the method's logarithmic efficiency, a property that holds true regardless of the specific function or initial interval .",
            "id": "2169211",
            "problem": "The bisection method is a root-finding algorithm that repeatedly bisects an interval and then selects a sub-interval in which a root must lie for further processing. For a continuous function $f(x)$ on an initial interval $[a, b]$, where $f(a)$ and $f(b)$ have opposite signs, the length of the interval containing the root after $N$ iterations is guaranteed to be no larger than $\\frac{b-a}{2^N}$.\n\nSuppose a numerical analyst has determined that $N$ iterations of the bisection method are sufficient to find a root to within a desired error tolerance $\\epsilon$. A project manager then requests a new calculation with an accuracy that is at least 10 times greater. What is the minimum number of *additional* iterations required to guarantee this new, higher accuracy, regardless of the function or the initial interval $[a, b]$?\n\nA. 2\n\nB. 3\n\nC. 4\n\nD. 10\n\nE. The number of additional iterations cannot be determined without knowing the function and the initial interval.",
            "solution": "Let the initial interval be $[a,b]$. After $N$ iterations, the bisection method guarantees an interval containing the root of length \n$$L_{N} \\leq \\frac{b-a}{2^{N}}.$$\nIf the approximate root is taken as the midpoint of this interval, the maximum absolute error is at most $L_{N}/2$. To reduce the error bound by a factor of 10, we need to perform $k$ additional iterations such that the interval length is reduced by a factor of 10. Each iteration halves the interval length, so after $k$ additional iterations, the error bound is reduced by a factor of $2^k$. We require\n$$2^{k} \\geq 10.$$\nTo find the smallest integer $k$ that satisfies this, we take the base-2 logarithm:\n$$k \\geq \\log_{2}(10) \\approx 3.32$$\nSince the number of iterations must be an integer, the minimum number of additional iterations required is $k=4$.\n\nThis result is independent of the function, the initial interval $[a,b]$, and the initial tolerance $\\epsilon$, as it depends only on the geometric halving property of the bisection method.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Building upon the predictable nature of convergence, this exercise reverses the typical problem-solving approach. Instead of calculating the error from a given setup, you will determine the maximum allowable size of the initial interval, given constraints on the number of iterations and the required final accuracy. This type of \"design problem\" is common in computational science, where one must plan an experiment to ensure the results meet a specific tolerance within a fixed computational budget .",
            "id": "2169215",
            "problem": "A numerical analyst is tasked with finding a root of a continuous function $f(x)$ within a given interval using the bisection method. The computational resources available allow for a maximum of 15 iterations of the algorithm. The final approximation of the root is taken as the midpoint of the final interval. For the result to be acceptable, its absolute error must be guaranteed to be less than $10^{-5}$. What is the maximum possible length of the initial interval $[a, b]$ that the analyst can choose while still ensuring this level of precision is met? Report your answer as a single real number, rounded to four significant figures.",
            "solution": "Let the initial interval be $[a,b]$ with length $L_{0}=b-a$. In the bisection method, each iteration halves the interval length, so after $n$ iterations the interval length is\n$$\nL_{n}=\\frac{L_{0}}{2^{n}}.\n$$\nIf the final approximation is taken as the midpoint of the final interval, then the absolute error is at most half the final interval length:\n$$\n|x_{\\text{approx}}-x^{\\ast}|\\leq \\frac{L_{n}}{2}=\\frac{L_{0}}{2^{n+1}}.\n$$\nTo guarantee an absolute error less than $\\epsilon=10^{-5}$ after $n=15$ iterations, we require\n$$\n\\frac{L_{0}}{2^{15+1}}  10^{-5},\n$$\nwhich gives\n$$\nL_{0}  10^{-5}\\cdot 2^{16}.\n$$\nSince $2^{16}=65536$, the maximal initial length is\n$$\nL_{0,\\max}=65536\\times 10^{-5}=0.65536,\n$$\nwhich, rounded to four significant figures, is $0.6554$.",
            "answer": "$$\\boxed{0.6554}$$"
        },
        {
            "introduction": "The way we define and measure error is a critical aspect of numerical analysis. This practice delves into the important distinction between absolute and relative error stopping criteria, which can lead to different performance characteristics. By analyzing a worst-case scenario, you will derive an expression for the difference in the required number of iterations, highlighting how the choice of error metric interacts with the magnitude of the root being sought . This provides a deeper understanding of how to implement robust and meaningful termination conditions in root-finding algorithms.",
            "id": "2169218",
            "problem": "An engineer is using the bisection method to find a root $r$ of a continuous function $f(x)$ within an initial interval of length $L_0$. The method iteratively reduces the interval length. After $n$ iterations, the interval is $[a_n, b_n]$, its length is $L_n = L_0/2^n$, and its midpoint is $c_n = (a_n+b_n)/2$.\n\nThe engineer considers two alternative stopping criteria, both using a small positive tolerance $\\epsilon$:\n1.  **Absolute Error Criterion**: The algorithm terminates when the interval length becomes smaller than the tolerance, i.e., $L_n  \\epsilon$. Let $N_{abs}$ be the number of iterations required for this criterion.\n2.  **Relative Error Criterion**: The algorithm terminates when the interval length becomes smaller than a fraction of the magnitude of the current root estimate, i.e., $L_n  \\epsilon |c_n|$. Let $N_{rel}$ be the number of iterations required for this criterion.\n\nTo perform a worst-case analysis of the relative error criterion, it is assumed that at the stopping iteration $n$, the midpoint $c_n$ has the minimum possible magnitude consistent with the bracketing requirement that the root $r$ must lie within the interval $[a_n, b_n]$. This minimum magnitude is given by $\\min(|c_n|) = \\max(0, |r| - L_n/2)$. You may assume that for the required number of iterations, the interval length $L_n$ is sufficiently small such that $|r|  L_n/2$.\n\nDetermine an analytic expression for the difference in the number of iterations, $\\Delta N = N_{rel} - N_{abs}$. For the purpose of finding this continuous analytic expression, you may approximate the number of iterations for each criterion by the real value that turns the respective stopping inequality into an equality. Express your answer in terms of the root's magnitude $|r|$ and the tolerance $\\epsilon$.",
            "solution": "The bisection method shrinks the bracketing interval according to $L_{n} = L_{0}/2^{n}$.\n\nFor the absolute error criterion, we approximate the required iteration count by turning the stopping inequality into an equality:\n$$\nL_{n} = \\epsilon \\quad \\Rightarrow \\quad \\frac{L_{0}}{2^{N_{abs}}} = \\epsilon \\quad \\Rightarrow \\quad N_{abs} = \\log_{2}\\!\\left(\\frac{L_{0}}{\\epsilon}\\right).\n$$\n\nFor the relative error criterion under the stated worst-case assumption, the minimum possible midpoint magnitude at iteration $n$ is $\\min(|c_{n}|) = |r| - L_{n}/2$ (since $|r| > L_{n}/2$ is assumed). Turning the stopping inequality into an equality gives\n$$\nL_{n} = \\epsilon\\,\\min(|c_{n}|) = \\epsilon\\left(|r| - \\frac{L_{n}}{2}\\right).\n$$\nSolving for $L_{n}$ yields\n$$\nL_{n}\\left(1 + \\frac{\\epsilon}{2}\\right) = \\epsilon |r| \\quad \\Rightarrow \\quad L_{n} = \\frac{\\epsilon |r|}{1+\\epsilon/2} = \\frac{2\\epsilon |r|}{2+\\epsilon}.\n$$\nThus the corresponding iteration count is\n$$\n\\frac{L_{0}}{2^{N_{rel}}} = \\frac{2\\epsilon |r|}{2+\\epsilon}\n\\quad \\Rightarrow \\quad\nN_{rel} = \\log_{2}\\!\\left(\\frac{L_{0}(2+\\epsilon)}{2\\epsilon |r|}\\right).\n$$\n\nThe difference in iteration counts is therefore\n$$\n\\Delta N \\equiv N_{rel} - N_{abs}\n= \\log_{2}\\!\\left(\\frac{L_{0}(2+\\epsilon)}{2\\epsilon |r|}\\right) - \\log_{2}\\!\\left(\\frac{L_{0}}{\\epsilon}\\right)\n= \\log_{2}\\!\\left(\\frac{L_0(2+\\epsilon)}{2\\epsilon |r|} \\cdot \\frac{\\epsilon}{L_0}\\right)\n= \\log_{2}\\!\\left(\\frac{2+\\epsilon}{2|r|}\\right),\n$$\nwhich depends only on $|r|$ and $\\epsilon$ and is independent of $L_{0}$. The assumption $|r| > L_{n}/2$ is satisfied at the relative stopping point since $L_{n} = 2\\epsilon |r|/(2+\\epsilon)$ implies $|r| > L_{n}/2$ as $1 > \\epsilon/(2+\\epsilon)$.",
            "answer": "$$\\boxed{\\log_{2}\\!\\left(\\frac{2+\\epsilon}{2|r|}\\right)}$$"
        }
    ]
}