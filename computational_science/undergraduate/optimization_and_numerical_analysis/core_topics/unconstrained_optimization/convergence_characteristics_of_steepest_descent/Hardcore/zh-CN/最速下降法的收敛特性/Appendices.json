{
    "hands_on_practices": [
        {
            "introduction": "要理解一个算法的局限性，最好的方法是先观察其在理想情况下的完美表现。对于最速下降法，最理想的情形莫过于在简单的二次函数上，它能一步到位地找到最小值。这个练习  引导我们证明，对于一维二次函数，使用精确线性搜索的最速下降法只需一次迭代就能精确地找到最小值，这为我们理解其更复杂的行为提供了重要的基准。",
            "id": "2162624",
            "problem": "一位优化专家正在使用最速下降算法来寻找一个一维目标函数的最小值。该函数为：\n$f(x) = 3x^2 - 7x + 11$\n\n最速下降算法的迭代过程由以下更新规则定义：\n$x_{k+1} = x_k - \\alpha_k f'(x_k)$\n其中 $x_k$ 是第 $k$ 次迭代时的当前位置，$f'(x_k)$ 是函数在 $x_k$ 处的导数，而 $\\alpha_k$ 是步长。\n\n该专家使用“精确线搜索”来确定每次迭代中的最优步长 $\\alpha_k$。该方法包括选择 $\\alpha_k$ 以便在从 $x_k$ 开始的搜索方向上最小化函数 $f$。换句话说，对于给定的 $x_k$，选择值 $\\alpha_k$ 以最小化关于步长的新函数 $g(\\alpha) = f(x_k - \\alpha f'(x_k))$。\n\n从初始点 $x_0 = 5$ 开始，使用精确线搜索计算最速下降算法单次迭代后的位置 $x_1$。请用分数表示你的答案。",
            "solution": "问题要求计算从 $x_0 = 5$ 开始，经过一次最速下降算法迭代后的位置 $x_1$。需要最小化的函数是 $f(x) = 3x^2 - 7x + 11$。\n\n最速下降算法的更新规则如下：\n$x_{k+1} = x_k - \\alpha_k f'(x_k)$\n\n对于第一次迭代（从 $k=0$ 到 $k=1$），规则是：\n$x_1 = x_0 - \\alpha_0 f'(x_0)$\n\n首先，我们需要计算函数 $f(x)$ 的导数：\n$f'(x) = \\frac{d}{dx}(3x^2 - 7x + 11) = 6x - 7$\n\n接下来，我们在起始点 $x_0 = 5$ 处计算该导数的值：\n$f'(x_0) = f'(5) = 6(5) - 7 = 30 - 7 = 23$\n\n现在我们得到了关于未知步长 $\\alpha_0$ 的更新规则：\n$x_1 = 5 - \\alpha_0 (23)$\n\n题目说明使用精确线搜索来找到 $\\alpha_0$。这意味着我们必须选择 $\\alpha_0$ 以最小化函数 $g(\\alpha) = f(x_0 - \\alpha f'(x_0))$。代入已知值：\n$g(\\alpha) = f(5 - 23\\alpha)$\n\n为了找到最小化 $g(\\alpha)$ 的 $\\alpha$ 值，我们对 $g(\\alpha)$ 关于 $\\alpha$ 求导，并令其为零。我们使用链式法则进行微分：\n$g'(\\alpha) = \\frac{d}{d\\alpha} f(5 - 23\\alpha) = f'(5 - 23\\alpha) \\cdot \\frac{d}{d\\alpha}(5 - 23\\alpha)$\n$g'(\\alpha) = f'(5 - 23\\alpha) \\cdot (-23)$\n\n令 $g'(\\alpha_0) = 0$ 来寻找最优步长 $\\alpha_0$：\n$f'(5 - 23\\alpha_0) \\cdot (-23) = 0$\n\n由于 $-23 \\neq 0$，该条件简化为：\n$f'(5 - 23\\alpha_0) = 0$\n\n导数内的表达式 $5 - 23\\alpha_0$ 正是我们目标点 $x_1$ 的公式。因此，最优步长的条件意味着：\n$f'(x_1) = 0$\n\n这意味着使用精确线搜索进行一次迭代后找到的点 $x_1$ 是原函数 $f(x)$ 导数为零的点。对于一个凸二次函数，这个点就是全局最小值。\n\n我们现在可以使用导数的表达式 $f'(x) = 6x - 7$ 来解出 $x_1$：\n$f'(x_1) = 6x_1 - 7 = 0$\n$6x_1 = 7$\n$x_1 = \\frac{7}{6}$\n\n因此，从 $x_0=5$ 开始，使用精确线搜索进行一次最速下降法迭代，直接导向了函数在 $x_1 = 7/6$ 处的最小值。",
            "answer": "$$\\boxed{\\frac{7}{6}}$$"
        },
        {
            "introduction": "从理想的一维情况过渡到更高维度，最速下降法的行为变得更加复杂，尤其是在处理“病态”问题时。它不再是一步收敛，而是展现出一种经典的“之字形”收敛模式，在通往最小值的路径上反复振荡。通过这个练习 ，我们将亲手计算并分析这一著名的收敛路径，从而直观地理解算法在狭长“山谷”中为何步履维艰。",
            "id": "2162600",
            "problem": "考虑最小化二次函数 $f(x, y) = \\frac{1}{2}(x^2 + \\gamma y^2)$ 的优化问题，其中 $\\gamma$ 是一个正实数常数。我们使用最速下降法来寻找该函数的最小值。该算法从一个初始点 $\\mathbf{x}_0$ 开始，生成一个点序列 $\\{\\mathbf{x}_k\\}$。更新规则由 $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k$ 给出，其中 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ 是 $f$ 在 $\\mathbf{x}_k$ 处的梯度。每一步的步长 $\\alpha_k$ 的选择是为了最小化 $f(\\mathbf{x}_{k+1})$；这种方法被称为精确线搜索。\n\n对于形式为 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x}$ 的二次函数，其中 $Q$ 是一个正定矩阵，在第 $k$ 次迭代时的最优步长 $\\alpha_k$ 由以下公式给出：\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^T \\mathbf{g}_k}{\\mathbf{g}_k^T Q \\mathbf{g}_k}\n$$\n假设我们设置 $\\gamma = 49$ 并从点 $\\mathbf{x}_0 = (49, 1)^T$ 开始算法。可以证明，对于此特定设置，偶数索引的迭代点（$ \\mathbf{x}_0, \\mathbf{x}_2, \\mathbf{x}_4, \\dots $）位于一条穿过原点的直线上，而奇数索引的迭代点（$ \\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_5, \\dots $）位于另一条也穿过原点的直线上。这个点序列形成一条“之”字形路径，收敛到位于 $(0, 0)$ 的最小值。\n\n求这两条直线之间的锐角 $\\theta$ 的余弦值。",
            "solution": "我们有 $f(x,y)=\\frac{1}{2}(x^{2}+\\gamma y^{2})$，其中 $Q=\\mathrm{diag}(1,\\gamma)$，因此对于 $\\mathbf{x}_{k}=(x_{k},y_{k})^{T}$，梯度为 $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})=(x_{k},\\gamma y_{k})^{T}$。精确线搜索的步长为\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{T}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{T}Q\\mathbf{g}_{k}}=\\frac{x_{k}^{2}+\\gamma^{2}y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}.\n$$\n最速下降更新 $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha_{k}\\mathbf{g}_{k}$ 按分量给出\n$$\nx_{k+1}=(1-\\alpha_{k})x_{k},\\qquad y_{k+1}=(1-\\alpha_{k}\\gamma)y_{k}.\n$$\n定义斜率 $m_{k}=y_{k}/x_{k}$（穿过原点的直线）。那么\n$$\nm_{k+1}=\\frac{y_{k+1}}{x_{k+1}}=\\frac{1-\\alpha_{k}\\gamma}{1-\\alpha_{k}}\\,m_{k}.\n$$\n使用 $\\alpha_{k}$ 的表达式，\n$$\n1-\\alpha_{k}=\\frac{\\gamma^{3}y_{k}^{2}-\\gamma^{2}y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}=\\frac{\\gamma^{2}(\\gamma-1)y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}},\n$$\n$$\n1-\\alpha_{k}\\gamma=\\frac{x_{k}^{2}+\\gamma^{3}y_{k}^{2}-\\gamma x_{k}^{2}-\\gamma^{3}y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}=\\frac{(1-\\gamma)x_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}.\n$$\n因此\n$$\n\\frac{1-\\alpha_{k}\\gamma}{1-\\alpha_{k}}=\\frac{(1-\\gamma)x_{k}^{2}}{\\gamma^{2}(\\gamma-1)y_{k}^{2}}=-\\frac{x_{k}^{2}}{\\gamma^{2}y_{k}^{2}},\n$$\n所以\n$$\nm_{k+1}=-\\frac{x_{k}^{2}}{\\gamma^{2}y_{k}^{2}}\\,m_{k}=-\\frac{1}{\\gamma^{2}m_{k}}.\n$$\n由此可得 $m_{k+2}=-\\frac{1}{\\gamma^{2}m_{k+1}}=-\\frac{1}{\\gamma^{2}\\left(-\\frac{1}{\\gamma^{2}m_{k}}\\right)}=m_{k}$，因此所有偶数索引的斜率相等，所有奇数索引的斜率也相等，这证实了存在两条直线。\n\n当 $\\gamma=49$ 且 $\\mathbf{x}_{0}=(49,1)^{T}$ 时，我们有 $m_{0}=y_{0}/x_{0}=1/49=1/\\gamma$，所以\n$$\nm_{1}=-\\frac{1}{\\gamma^{2}m_{0}}=-\\frac{1}{\\gamma^{2}\\cdot(1/\\gamma)}=-\\frac{1}{\\gamma}=-\\frac{1}{49}.\n$$\n因此，这两条直线的斜率分别为 $m$ 和 $-m$，其中 $m=1/49$。斜率分别为 $m$ 和 $-m$ 的直线之间的锐角 $\\theta$ 的余弦值等于\n$$\n\\cos\\theta=\\frac{(1,m)\\cdot(1,-m)}{\\|(1,m)\\|\\,\\|(1,-m)\\|}=\\frac{1-m^{2}}{1+m^{2}}.\n$$\n代入 $m=1/49$，\n$$\n\\cos\\theta=\\frac{1-\\frac{1}{49^{2}}}{1+\\frac{1}{49^{2}}}=\\frac{49^{2}-1}{49^{2}+1}=\\frac{2400}{2402}=\\frac{1200}{1201}.\n$$",
            "answer": "$$\\boxed{\\frac{1200}{1201}}$$"
        },
        {
            "introduction": "算法在狭长“山谷”中缓慢的之字形收敛，不仅影响效率，还给判断何时停止迭代带来了挑战。一个看似已经很小的梯度，可能对应着离真正最小值很远的一个点。这个练习  揭示了一个常见的实践陷阱：基于梯度范数的停止准则在病态问题中可能会产生误导。理解这一点有助于我们设计更可靠的算法终止条件，并正确评估优化结果的质量。",
            "id": "2162662",
            "problem": "一个优化算法被用来寻找凸二次函数 $f(x_1, x_2) = \\frac{1}{2} (x_1^2 + \\lambda x_2^2)$ 的最小值，其中 $\\lambda$ 是一个小的正常数，代表问题的条件状况。该函数的唯一全局最小值点位于 $x^* = (0, 0)$。该算法使用一个标准的终止准则：当梯度 $\\|\\nabla f(x_k)\\|$ 的欧几里得范数小于预定义的容差 $\\epsilon$ 时，算法停止迭代。\n\n在一个具体的应用中，函数参数为 $\\lambda = 10^{-8}$，终止容差设置为 $\\epsilon = 10^{-5}$。算法在一个点 $x_{term} = (x_1, x_2)$ 处停止，该点恰好满足终止条件，即 $\\|\\nabla f(x_{term})\\| = \\epsilon$。这种情况对应于终止区域的边界。\n\n如此小的梯度范数并不能保证点 $x_{term}$ 接近真实的最小值点 $x^*$。你的任务是量化这个最坏情况下的误差。计算对于一个满足终止条件 $\\|\\nabla f(x_{term})\\| = \\epsilon$ 的点，其与最小值点之间的最大可能欧几里得距离 $\\|x_{term} - x^*\\|$。",
            "solution": "我们已知凸二次函数 $f(x_{1}, x_{2}) = \\frac{1}{2}\\left(x_{1}^{2} + \\lambda x_{2}^{2}\\right)$，其唯一最小值点在 $x^{*} = (0, 0)$。它的梯度是：\n$$\n\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} x_{1} \\\\ \\lambda x_{2} \\end{pmatrix}\n$$\n终止条件是 $\\|\\nabla f(x_{term})\\| = \\epsilon$，这明确地施加了约束：\n$$\nx_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}\n$$\n我们被要求计算在所有满足上述约束的 $(x_1, x_2)$ 上，到最小值点的最大可能欧几里得距离，$\\|x_{term} - x^{*}\\| = \\sqrt{x_{1}^{2} + x_{2}^{2}}$。这是一个约束优化问题：\n$$\n\\max_{x_{1}, x_{2}} \\; (x_{1}^{2} + x_{2}^{2}) \\quad \\text{subject to} \\quad x_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}\n$$\n使用拉格朗日乘数法，定义：\n$$\n\\mathcal{L}(x_{1}, x_{2}, \\mu) = x_{1}^{2} + x_{2}^{2} - \\mu \\left(x_{1}^{2} + \\lambda^{2} x_{2}^{2} - \\epsilon^{2}\\right)\n$$\n驻点条件是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{1}} = 2 x_{1} - 2 \\mu x_{1} = 2 x_{1} (1 - \\mu) = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{2}} = 2 x_{2} - 2 \\mu \\lambda^{2} x_{2} = 2 x_{2} (1 - \\mu \\lambda^{2}) = 0\n$$\n以及约束条件 $x_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}$。\n\n从驻点方程可知，要么 $x_{1} = 0$ 要么 $\\mu = 1$，并且要么 $x_{2} = 0$ 要么 $\\mu = \\frac{1}{\\lambda^{2}}$。因为 $\\lambda \\neq 1$，我们不能同时有 $\\mu = 1$ 和 $\\mu = \\frac{1}{\\lambda^{2}}$，所以极值出现在某个坐标为零时。\n\n- 如果 $x_{2} = 0$，则约束条件给出 $x_{1}^{2} = \\epsilon^{2}$，目标函数值为 $x_{1}^{2} + x_{2}^{2} = \\epsilon^{2}$，因此距离为 $\\epsilon$。\n- 如果 $x_{1} = 0$，则约束条件给出 $\\lambda^{2} x_{2}^{2} = \\epsilon^{2}$，所以 $x_{2}^{2} = \\frac{\\epsilon^{2}}{\\lambda^{2}}$，目标函数值为 $x_{1}^{2} + x_{2}^{2} = \\frac{\\epsilon^{2}}{\\lambda^{2}}$，因此距离为 $\\frac{\\epsilon}{\\lambda}$。\n\n在这两个候选值之间，因为 $\\lambda \\in (0, 1)$，所以在 $x_{1} = 0$ 时取到最大值，得到最坏情况下的距离：\n$$\n\\|x_{term} - x^{*}\\|_{\\max} = \\frac{\\epsilon}{\\lambda}.\n$$\n代入给定值 $\\lambda = 10^{-8}$ 和 $\\epsilon = 10^{-5}$，\n$$\n\\|x_{term} - x^{*}\\|_{\\max} = \\frac{10^{-5}}{10^{-8}} = 10^{3} = 1 \\times 10^{3}.\n$$",
            "answer": "$$\\boxed{1 \\times 10^{3}}$$"
        }
    ]
}