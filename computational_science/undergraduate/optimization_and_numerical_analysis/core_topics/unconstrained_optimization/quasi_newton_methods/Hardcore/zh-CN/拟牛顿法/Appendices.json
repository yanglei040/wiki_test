{
    "hands_on_practices": [
        {
            "introduction": "拟牛顿法的每一步迭代都始于计算一个搜索方向。这个基础练习将带你完成这关键的第一步。通过使用最简单的Hessian矩阵近似——单位矩阵，我们将计算初始搜索方向 。这个过程能帮助你理解算法的起点，并揭示了为什么初始步骤通常与最速下降法方向一致。",
            "id": "2195903",
            "problem": "采用一个迭代优化算法来寻找双变量函数 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$ 的一个局部最小值。该算法在点 $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$ 处初始化。在第一步（迭代 $k=0$）中，通过求解线性方程组 $\\mathbf{B}_0 \\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$ 来计算搜索方向 $\\mathbf{p}_0$，其中 $\\nabla f(\\mathbf{x}_0)$ 是 $f$ 在 $\\mathbf{x}_0$ 处的梯度，而 $\\mathbf{B}_0$ 是 Hessian 矩阵的一个近似。在此过程中，初始近似被选为 $2 \\times 2$ 的单位矩阵 $\\mathbf{I}$。确定初始搜索方向向量 $\\mathbf{p}_0$ 的分量。",
            "solution": "问题要求解初始搜索方向向量 $\\mathbf{p}_0$，它是线性方程组 $\\mathbf{B}_0 \\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$ 的解。我们可以通过以下三个主要步骤来解决这个问题：首先，计算函数 $f(x_1, x_2)$ 的梯度；其次，在初始点 $\\mathbf{x}_0$ 处计算该梯度；第三，求解给定的线性方程组以得到 $\\mathbf{p}_0$。\n\n步骤 1：计算函数的梯度。\n函数为 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$。\n$f$ 的梯度，记作 $\\nabla f$，是其偏导数组成的向量：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\n关于 $x_1$ 的偏导数为：\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)\n$$\n关于 $x_2$ 的偏导数为：\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)\n$$\n因此，梯度向量为：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\cos(x_1) \\\\ \\sinh(x_2) \\end{pmatrix}\n$$\n\n步骤 2：在初始点 $\\mathbf{x}_0$ 处计算梯度。\n初始点为 $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$。我们将这些值代入梯度表达式中：\n$$\n\\nabla f(\\mathbf{x}_0) = \\nabla f(0, \\ln(2)) = \\begin{pmatrix} \\cos(0) \\\\ \\sinh(\\ln(2)) \\end{pmatrix}\n$$\n我们计算每个分量。0的余弦是：\n$$\n\\cos(0) = 1\n$$\n双曲正弦函数定义为 $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$。对于 $y = \\ln(2)$：\n$$\n\\sinh(\\ln(2)) = \\frac{\\exp(\\ln(2)) - \\exp(-\\ln(2))}{2} = \\frac{2 - \\exp(\\ln(2^{-1}))}{2} = \\frac{2 - \\frac{1}{2}}{2} = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\n$$\n所以，在初始点处的梯度向量是：\n$$\n\\nabla f(\\mathbf{x}_0) = \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n\n步骤 3：求解线性方程组以得到 $\\mathbf{p}_0$。\n需要求解的方程组是 $\\mathbf{B}_0 \\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$。给定 $\\mathbf{B}_0$ 是 $2 \\times 2$ 的单位矩阵，$\\mathbf{I} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n方程组变为：\n$$\n\\mathbf{I} \\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)\n$$\n任何向量乘以单位矩阵都保持不变，所以 $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$。\n代入我们求得的梯度值：\n$$\n\\mathbf{p}_0 = - \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}\n$$\n因此，初始搜索方向向量 $\\mathbf{p}_0$ 的分量是 $-1$ 和 $-\\frac{3}{4}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}}$$"
        },
        {
            "introduction": "在计算出搜索方向并确定步长后，拟牛顿法的核心在于更新Hessian近似矩阵。这个练习将引导你手动完成一次完整的BFGS迭代过程 。你将从一个点移动到下一个点，并利用梯度信息来构建新的Hessian近似矩阵 $B_1$，从而亲身体验BFGS更新公式的运作机制。",
            "id": "2195901",
            "problem": "一个优化程序被用来寻找二次函数 $f(\\mathbf{x})$ 的最小值，该函数定义在 $\\mathbf{x} \\in \\mathbb{R}^2$ 上，表达式为：\n$$f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^T \\mathbf{A} \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$$\n其中矩阵 $\\mathbf{A}$ 和向量 $\\mathbf{b}$ 如下所示：\n$$\\mathbf{A} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\n该程序采用拟牛顿法，该方法迭代地改进对海森矩阵的近似。该方法从初始点 $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，初始海森矩阵近似为 $\\mathbf{B}_0 = \\mathbf{I}$，其中 $\\mathbf{I}$ 是 $2 \\times 2$ 的单位矩阵。\n\n在每次迭代 $k$ 中，通过求解线性系统 $\\mathbf{B}_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来计算搜索方向 $\\mathbf{p}_k$。然后使用固定的单位步长来确定下一个点，即 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{p}_k$。\n\n确定新点 $\\mathbf{x}_{k+1}$ 后，使用 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新公式将海森矩阵的近似从 $\\mathbf{B}_k$ 更新为 $\\mathbf{B}_{k+1}$：\n$$\\mathbf{B}_{k+1} = \\mathbf{B}_k - \\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^T \\mathbf{B}_k}{\\mathbf{s}_k^T \\mathbf{B}_k \\mathbf{s}_k} + \\frac{\\mathbf{y}_k \\mathbf{y}_k^T}{\\mathbf{y}_k^T \\mathbf{s}_k}$$\n其中 $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ 且 $\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)$。\n\n你的任务是执行该算法的第一次完整迭代，以确定第二步的搜索方向 $\\mathbf{p}_1$。计算向量 $\\mathbf{p}_1$ 的第一个分量的值。将最终答案精确到四位有效数字。",
            "solution": "给定二次函数 $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{T} \\mathbf{A} \\mathbf{x} - \\mathbf{b}^{T} \\mathbf{x}$，其中 $\\mathbf{A} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}$ 且 $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。其梯度为 $\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b}$。\n\n在初始点 $\\mathbf{x}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 且 $\\mathbf{B}_{0} = \\mathbf{I}$ 的条件下，通过求解下式来计算第一个搜索方向 $\\mathbf{p}_{0}$：\n$$\n\\mathbf{B}_{0} \\mathbf{p}_{0} = -\\nabla f(\\mathbf{x}_{0}).\n$$\n由于 $\\nabla f(\\mathbf{x}_{0}) = \\mathbf{A} \\mathbf{x}_{0} - \\mathbf{b} = -\\mathbf{b} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$ 且 $\\mathbf{B}_{0} = \\mathbf{I}$，我们有\n$$\n\\mathbf{p}_{0} = -\\nabla f(\\mathbf{x}_{0}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n采用单位步长，下一个点是\n$$\n\\mathbf{x}_{1} = \\mathbf{x}_{0} + \\mathbf{p}_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n定义 $\\mathbf{s}_{0} = \\mathbf{x}_{1} - \\mathbf{x}_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。计算梯度：\n$$\n\\nabla f(\\mathbf{x}_{1}) = \\mathbf{A} \\mathbf{x}_{1} - \\mathbf{b} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\nabla f(\\mathbf{x}_{0}) = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\n因此\n$$\n\\mathbf{y}_{0} = \\nabla f(\\mathbf{x}_{1}) - \\nabla f(\\mathbf{x}_{0}) = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n使用 BFGS 公式更新海森矩阵近似：\n$$\n\\mathbf{B}_{1} = \\mathbf{B}_{0} - \\frac{\\mathbf{B}_{0} \\mathbf{s}_{0} \\mathbf{s}_{0}^{T} \\mathbf{B}_{0}}{\\mathbf{s}_{0}^{T} \\mathbf{B}_{0} \\mathbf{s}_{0}} + \\frac{\\mathbf{y}_{0} \\mathbf{y}_{0}^{T}}{\\mathbf{y}_{0}^{T} \\mathbf{s}_{0}}.\n$$\n当 $\\mathbf{B}_{0} = \\mathbf{I}$，$\\mathbf{s}_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$\\mathbf{y}_{0} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ 时，我们有\n$$\n\\mathbf{s}_{0}^{T} \\mathbf{s}_{0} = 1, \\quad \\mathbf{s}_{0} \\mathbf{s}_{0}^{T} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}, \\quad \\mathbf{y}_{0} \\mathbf{y}_{0}^{T} = \\begin{pmatrix} 9  3 \\\\ 3  1 \\end{pmatrix}, \\quad \\mathbf{y}_{0}^{T} \\mathbf{s}_{0} = 3,\n$$\n所以\n$$\n\\mathbf{B}_{1} = \\mathbf{I} - \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 9  3 \\\\ 3  1 \\end{pmatrix} = \\begin{pmatrix} 3  1 \\\\ 1  \\frac{4}{3} \\end{pmatrix}.\n$$\n第二步的搜索方向 $\\mathbf{p}_{1}$ 通过求解下式得到\n$$\n\\mathbf{B}_{1} \\mathbf{p}_{1} = -\\nabla f(\\mathbf{x}_{1}) = -\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n$$\n令 $\\mathbf{p}_{1} = \\begin{pmatrix} p \\\\ q \\end{pmatrix}$。则\n$$\n\\begin{cases}\n3p + q = -2, \\\\\np + \\frac{4}{3} q = -1.\n\\end{cases}\n$$\n由第一个方程可得 $q = -2 - 3p$。将其代入第二个方程：\n$$\np + \\frac{4}{3}(-2 - 3p) = -1 \\;\\Rightarrow\\; p - \\frac{8}{3} - 4p = -1 \\;\\Rightarrow\\; -3p - \\frac{8}{3} = -1 \\;\\Rightarrow\\; -3p = \\frac{5}{3} \\;\\Rightarrow\\; p = -\\frac{5}{9}.\n$$\n因此，$\\mathbf{p}_{1}$ 的第一个分量是 $-\\frac{5}{9}$，精确到四位有效数字为 $-0.5556$。",
            "answer": "$$\\boxed{-0.5556}$$"
        },
        {
            "introduction": "从手动计算到算法实现是理解和应用优化方法的关键一步。这个高级实践要求你编写一个完整的SR1拟牛顿法程序，以解决更复杂的优化问题 。你将需要处理现实世界中可能出现的各种挑战，例如数值不稳定性和非正定Hessian矩阵，并通过实现跳过更新、后备方向和线搜索等机制，来构建一个稳健的优化工具。",
            "id": "2417336",
            "problem": "要求您编写一个完整的程序，通过使用对称秩一更新来迭代地近似Hessian矩阵，从而最小化给定的二次连续可微目标函数。该方法必须维护一个对称矩阵序列 $\\{\\mathbf{B}_k\\}_{k \\ge 0}$，使用从 $\\mathbf{B}_k$ 计算出的方向构建步长 $\\{\\mathbf{s}_k\\}$，并应用一个跳过准则，当相关分母过小时避免秩一更新。当梯度范数低于预设容差或达到最大迭代次数时，该方法应终止。\n\n设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 为目标函数，$\\nabla f(\\mathbf{x})$ 表示其梯度，$\\mathbf{B}_k \\in \\mathbb{R}^{n \\times n}$ 为第 $k$ 次迭代时Hessian矩阵的对称近似。给定当前迭代点 $\\mathbf{x}_k \\in \\mathbb{R}^n$ 及其梯度 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$，以及求解 $\\mathbf{B}_k \\mathbf{p}_k = -\\mathbf{g}_k$ 得到的方向 $\\mathbf{p}_k$，定义步长为 $\\mathbf{s}_k = \\alpha_k \\mathbf{p}_k$（其中步长因子 $\\alpha_k \\in (0,1]$），下一个迭代点为 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{s}_k$，梯度位移为 $\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)$。定义 $\\mathbf{u}_k = \\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k$。对称秩一更新为\n$$\n\\mathbf{B}_{k+1} = \\mathbf{B}_k + \\frac{\\mathbf{u}_k \\mathbf{u}_k^\\top}{\\mathbf{s}_k^\\top \\mathbf{u}_k}.\n$$\n引入一个由容差 $\\tau  0$ 参数化的跳过机制，仅在满足\n$$\n|\\mathbf{s}_k^\\top \\mathbf{u}_k|  \\tau \\,\\|\\mathbf{s}_k\\|_2 \\,\\|\\mathbf{u}_k\\|_2,\n$$\n时应用更新，否则设置 $\\mathbf{B}_{k+1} = \\mathbf{B}_k$（跳过更新）。初始时设置 $\\mathbf{B}_0 = \\mathbf{I}$。\n\n您的程序必须为下面的每个测试用例生成以下输出：\n- 执行的总迭代次数 $k_{\\text{end}}$（一个整数），\n- 最终的目标函数值 $f(\\mathbf{x}_{k_{\\text{end}}})$（一个浮点数），\n- 最终的梯度范数 $\\|\\nabla f(\\mathbf{x}_{k_{\\text{end}}})\\|_2$（一个浮点数），\n- 跳过更新的次数（一个整数），\n- 搜索方向 $\\mathbf{p}_k$ 未能成为下降方向（即当 $\\mathbf{g}_k^\\top \\mathbf{p}_k \\ge 0$ 时）并使用备用方向的次数（一个整数）。\n\n如果求解 $\\mathbf{B}_k \\mathbf{p}_k = -\\mathbf{g}_k$ 不可能或产生的方向不是下降方向，则该次迭代必须回退到使用最速下降方向 $\\mathbf{p}_k = -\\mathbf{g}_k$。使用回溯线搜索以强制满足Armijo条件\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\, \\mathbf{g}_k^\\top \\mathbf{p}_k,\n$$\n固定参数为 $c_1 = 10^{-4}$，缩减因子为 $\\beta = \\tfrac{1}{2}$，从 $\\alpha_k = 1$ 开始，每次乘以因子 $\\beta$ 进行缩减，直到条件满足或 $\\alpha_k$ 小于 $10^{-16}$ 为止，在后一种情况下，可直接采用该步长。当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ 或当 $k$ 达到最大迭代预算时终止。\n\n测试套件。将该方法应用于以下六个测试用例。在所有用例中，使用初始Hessian近似 $\\mathbf{B}_0 = \\mathbf{I}$，容差 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$，以及最大迭代次数 $200$。\n- 用例 A（顺利路径，非凸窄谷）：\n  - 目标：二维Rosenbrock函数，\n    $$\n    f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n    $$\n  - 初始点：$\\mathbf{x}_0 = (-1.2,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-8}$。\n- 用例 B（与用例A相同，但跳过更激进）：\n  - 目标：与用例A相同。\n  - 初始点：$\\mathbf{x}_0 = (-1.2,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-2}$。\n- 用例 C（病态严格凸二次函数）：\n  - 目标：\n    $$\n    f(\\mathbf{x}) = \\tfrac{1}{2}\\left(1\\cdot x_1^2 + 1000\\cdot x_2^2\\right).\n    $$\n  - 初始点：$\\mathbf{x}_0 = (1.0,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-8}$。\n- 用例 D（与用例C相同，但跳过更激进）：\n  - 目标：与用例C相同。\n  - 初始点：$\\mathbf{x}_0 = (1.0,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-2}$。\n- 用例 E（具有鞍点结构的非凸函数）：\n  - 目标：\n    $$\n    f(\\mathbf{x}) = x_1^4 - x_1^2 + x_2^2.\n    $$\n  - 初始点：$\\mathbf{x}_0 = (0.5,\\, 0.5)$。\n  - 跳过容差：$\\tau = 10^{-8}$。\n- 用例 F（与用例E相同，但跳过更激进）：\n  - 目标：与用例E相同。\n  - 初始点：$\\mathbf{x}_0 = (0.5,\\, 0.5)$。\n  - 跳过容差：$\\tau = 10^{-2}$。\n\n不涉及角度单位。不出现物理单位。您的程序必须忠实地实现上述数学规范。\n\n最终输出格式。您的程序应生成单行输出，包含一个含六个列表的列表，每个内部列表按A到F的顺序对应一个测试用例，结构如下\n$$\n[\\;k_{\\text{end}},\\; f(\\mathbf{x}_{k_{\\text{end}}}),\\; \\|\\nabla f(\\mathbf{x}_{k_{\\text{end}}})\\|_2,\\; \\text{skipped},\\; \\text{fallback}\\;],\n$$\n其中两个浮点数值必须精确到六位小数。例如，最终打印输出必须如下所示\n$$\n\\big[\\,[k_1, f_1, g_1, s_1, b_1],\\; [k_2, f_2, g_2, s_2, b_2],\\; \\dots,\\; [k_6, f_6, g_6, s_6, b_6]\\,\\big].\n$$",
            "solution": "我们通过将对称秩一更新与稳健的步长计算和充分下降线搜索相结合，来形式化迭代过程。目标是使用跨测试问题的一致终止和线搜索策略，来检验跳过阈值参数如何影响稳定性和速度。\n\n原理。考虑一个目标函数 $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$，其梯度为 $\\nabla f(\\mathbf{x})$。牛顿法使用Hessian矩阵 $\\nabla^2 f(\\mathbf{x}_k)$ 来定义一个局部二次模型，其步长通过求解 $\\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 得到。拟牛顿法用一个近似矩阵 $\\mathbf{B}_k$ 替代精确的Hessian矩阵，该矩阵利用梯度差信息进行更新，以满足一个类割线条件。对称秩一(SR1)更新是通过施加对称性、Frobenius范数下的最小变化以及沿 $\\mathbf{s}_k$ 的割线条件（即 $\\mathbf{B}_{k+1} \\mathbf{s}_k = \\mathbf{y}_k$）推导出来的，其中 $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ 且 $\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)$。强制满足此条件的SR1更新为\n$$\n\\mathbf{B}_{k+1} = \\mathbf{B}_k + \\frac{(\\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k) (\\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k)^\\top}{(\\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k)^\\top \\mathbf{s}_k},\n$$\n只要分母不为零。定义 $\\mathbf{u}_k = \\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k$，并将分母表示为 $d_k = \\mathbf{s}_k^\\top \\mathbf{u}_k$。\n\n稳定性机制。当 $|d_k|$ 很小时，SR1校正项的范数会变得非常大，这可能引入严重的不稳定性（曲率估计的大幅变化、失去下降性以及潜在的数值奇异性）。一个常见的保障措施是在 $|d_k|$ 相对于 $\\mathbf{s}_k$ 和 $\\mathbf{u}_k$ 范数的乘积较小时跳过更新。我们采用准则\n$$\n|\\mathbf{s}_k^\\top \\mathbf{u}_k|  \\tau \\,\\|\\mathbf{s}_k\\|_2 \\,\\|\\mathbf{u}_k\\|_2,\n$$\n其中 $\\tau  0$。如果该准则成立，我们应用秩一更新；否则我们设置 $\\mathbf{B}_{k+1} = \\mathbf{B}_k$（跳过）。\n\n方向计算与备用策略。在每次迭代中，我们通过求解 $\\mathbf{B}_k \\mathbf{p}_k = -\\mathbf{g}_k$ 来寻找 $\\mathbf{p}_k$，其中 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$。由于SR1不保持正定性，$\\mathbf{B}_k$ 可能是非定的或奇异的。如果线性求解失败或 $\\mathbf{g}_k^\\top \\mathbf{p}_k \\ge 0$（不是下降方向），我们回退到使用最速下降方向 $\\mathbf{p}_k = -\\mathbf{g}_k$。这确保我们总能有一个满足 $\\mathbf{g}_k^\\top \\mathbf{p}_k \\le 0$ 的可行方向。\n\n通过充分下降实现全局化。为确保稳定性和目标函数的下降，我们使用带有Armijo条件的回溯法：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\, \\mathbf{g}_k^\\top \\mathbf{p}_k,\n$$\n固定参数 $c_1 = 10^{-4}$ 和缩减因子 $\\beta = \\tfrac{1}{2}$。我们从 $\\alpha = 1$ 开始，并不断缩减 $\\alpha \\leftarrow \\beta \\alpha$，直到条件满足或 $\\alpha$ 变得极小（低于 $10^{-16}$）。选定的 $\\alpha_k$ 定义了 $\\mathbf{s}_k = \\alpha_k \\mathbf{p}_k$ 并得出 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{s}_k$。\n\n终止条件与度量指标。当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ 或当 $k$ 达到最大迭代预算（$200$）时，我们终止算法。对于每个测试，我们返回：\n- 迭代次数 $k_{\\text{end}}$，\n- $f(\\mathbf{x}_{k_{\\text{end}}})$，\n- $\\|\\nabla f(\\mathbf{x}_{k_{\\text{end}}})\\|_2$，\n- 跳过的SR1更新次数，以及\n- 因非下降或求解失败而使用备用策略的次数。\n\n测试套件与覆盖范围。我们使用六个用例：\n- 从 $\\mathbf{x}_0 = (-1.2, 1.0)$ 开始，使用 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$ 的二维Rosenbrock函数。这用于探测一个非凸谷，测试曲率学习能力以及跳过对速度的影响。\n- 从 $\\mathbf{x}_0 = (1,1)$ 开始，使用 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$ 的病态凸二次函数 $f(\\mathbf{x}) = \\tfrac{1}{2} (\\mathbf{x}_1^2 + 1000 \\mathbf{x}_2^2)$。这用于测试对条件数的敏感性以及曲率更新相对于跳过的益处。\n- 从 $\\mathbf{x}_0 = (0.5, 0.5)$ 开始，使用 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$ 的非凸四次函数 $f(\\mathbf{x}) = x_1^4 - x_1^2 + x_2^2$，包含一个鞍点方向，用于探测下降备用策略和稳定性。\n\n权衡分析。当 $\\tau$ 较小（例如 $\\tau = 10^{-8}$）时，该方法会更频繁地应用SR1校正。这倾向于加速收敛，因为曲率信息被频繁更新，但也增加了当 $|\\mathbf{s}_k^\\top \\mathbf{u}_k|$ 很小但未低于阈值时产生巨大校正的风险，可能导致矩阵非定或步长不稳定，我们通过备用策略和线搜索来缓解这些问题。当 $\\tau$ 较大（例如 $\\tau = 10^{-2}$）时，该方法会跳过更多更新。这通过避免被认为是病态的校正来稳定序列 $\\{\\mathbf{B}_k\\}$，但会减慢曲率学习速度，并可能增加迭代次数或对最速下降行为的依赖。该程序通过迭代次数、最终梯度范数、跳过更新的次数以及备用事件的次数来量化这种权衡，从而可以直接比较 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$ 在三个目标函数上的表现。\n\n正确性与第一性原理。该方法直接遵循：\n- 割线条件 $\\mathbf{B}_{k+1} \\mathbf{s}_k = \\mathbf{y}_k$，\n- SR1最小变化推导，\n- Armijo充分下降准则，以及\n- 通过 $\\mathbf{g}_k^\\top \\mathbf{p}_k  0$ 定义的下降方向。\n除了范数相关的跳过阈值外，没有使用任何针对特定问题的启发式方法，所有量均根据 $f$、$\\nabla f$ 和更新规则的定义计算得出。",
            "answer": "```python\nimport numpy as np\n\ndef rosenbrock(x: np.ndarray) - float:\n    # f(x1, x2) = 100 (x2 - x1^2)^2 + (1 - x1)^2\n    x1, x2 = x[0], x[1]\n    return 100.0 * (x2 - x1**2)**2 + (1.0 - x1)**2\n\ndef rosenbrock_grad(x: np.ndarray) - np.ndarray:\n    x1, x2 = x[0], x[1]\n    df_dx1 = -400.0 * (x2 - x1**2) * x1 - 2.0 * (1.0 - x1)\n    df_dx2 = 200.0 * (x2 - x1**2)\n    return np.array([df_dx1, df_dx2], dtype=float)\n\ndef quad_ic(x: np.ndarray) - float:\n    # f(x) = 1/2 (1*x1^2 + 1000*x2^2)\n    x1, x2 = x[0], x[1]\n    return 0.5 * (1.0 * x1**2 + 1000.0 * x2**2)\n\ndef quad_ic_grad(x: np.ndarray) - np.ndarray:\n    x1, x2 = x[0], x[1]\n    return np.array([1.0 * x1, 1000.0 * x2], dtype=float)\n\ndef quartic_saddle(x: np.ndarray) - float:\n    # f(x) = x1^4 - x1^2 + x2^2\n    x1, x2 = x[0], x[1]\n    return x1**4 - x1**2 + x2**2\n\ndef quartic_saddle_grad(x: np.ndarray) - np.ndarray:\n    x1, x2 = x[0], x[1]\n    return np.array([4.0 * x1**3 - 2.0 * x1, 2.0 * x2], dtype=float)\n\ndef armijo_backtracking(f, grad, x, p, gTp, c1=1e-4, beta=0.5, alpha0=1.0, min_alpha=1e-16):\n    alpha = alpha0\n    fx = f(x)\n    while True:\n        xn = x + alpha * p\n        if f(xn) = fx + c1 * alpha * gTp:\n            break\n        alpha *= beta\n        if alpha  min_alpha:\n            break\n    return alpha\n\ndef sr1_optimize(f, grad, x0, tau, max_iter=200, tol=1e-6):\n    n = len(x0)\n    x = x0.copy()\n    B = np.eye(n)\n    skipped_updates = 0\n    fallback_count = 0\n    k = 0\n\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = np.linalg.norm(g)\n        if gnorm = tol:\n            break\n\n        # Attempt to solve B p = -g\n        descent_ok = False\n        try:\n            p = np.linalg.solve(B, -g)\n            gTp = float(np.dot(g, p))\n            if gTp  0.0 and np.all(np.isfinite(p)):\n                descent_ok = True\n        except np.linalg.LinAlgError:\n            descent_ok = False\n\n        if not descent_ok:\n            p = -g\n            gTp = -float(np.dot(g, g))\n            fallback_count += 1\n\n        # Armijo backtracking line search\n        alpha = armijo_backtracking(f, grad, x, p, gTp, c1=1e-4, beta=0.5, alpha0=1.0, min_alpha=1e-16)\n\n        s = alpha * p\n        x_new = x + s\n        g_new = grad(x_new)\n        y = g_new - g\n\n        # SR1 update with skipping\n        u = y - B.dot(s)\n        denom = float(np.dot(s, u))\n        norm_s = np.linalg.norm(s)\n        norm_u = np.linalg.norm(u)\n\n        # Apply skip criterion only if norms are finite and nonzero\n        apply_update = True\n        if norm_s == 0.0 or not np.isfinite(norm_s) or not np.isfinite(norm_u):\n            apply_update = False\n        else:\n            if abs(denom) = tau * norm_s * norm_u:\n                apply_update = False\n\n        if apply_update:\n            # Rank-one update\n            B = B + np.outer(u, u) / denom\n        else:\n            skipped_updates += 1\n\n        x = x_new\n\n    # Final metrics\n    final_f = float(f(x))\n    final_gnorm = float(np.linalg.norm(grad(x)))\n    return k, final_f, final_gnorm, skipped_updates, fallback_count\n\ndef solve():\n    # Define test cases A-F\n    test_cases = [\n        # (name, f, grad, x0, tau)\n        (\"A_rosen_tau1e-8\", rosenbrock, rosenbrock_grad, np.array([-1.2, 1.0], dtype=float), 1e-8),\n        (\"B_rosen_tau1e-2\", rosenbrock, rosenbrock_grad, np.array([-1.2, 1.0], dtype=float), 1e-2),\n        (\"C_quad_tau1e-8\", quad_ic, quad_ic_grad, np.array([1.0, 1.0], dtype=float), 1e-8),\n        (\"D_quad_tau1e-2\", quad_ic, quad_ic_grad, np.array([1.0, 1.0], dtype=float), 1e-2),\n        (\"E_quartic_tau1e-8\", quartic_saddle, quartic_saddle_grad, np.array([0.5, 0.5], dtype=float), 1e-8),\n        (\"F_quartic_tau1e-2\", quartic_saddle, quartic_saddle_grad, np.array([0.5, 0.5], dtype=float), 1e-2),\n    ]\n\n    results = []\n    for _, f, grad, x0, tau in test_cases:\n        k, fval, gnorm, skipped, fallback = sr1_optimize(f, grad, x0, tau, max_iter=200, tol=1e-6)\n        # Round floats to exactly six decimals as required\n        results.append([int(k), float(f\"{fval:.6f}\"), float(f\"{gnorm:.6f}\"), int(skipped), int(fallback)])\n\n    # Print in the exact required format: a single line with the list of lists\n    # Ensure no extra text\n    def format_inner(lst):\n        # lst: [k, f, g, s, b]\n        return f\"[{lst[0]},{lst[1]:.6f},{lst[2]:.6f},{lst[3]},{lst[4]}]\"\n    print(f\"[{','.join(format_inner(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}