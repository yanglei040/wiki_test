{
    "hands_on_practices": [
        {
            "introduction": "Moving from theory to practice, the first question in any umbrella sampling study is how to set up the simulation windows. This involves deciding how many windows are needed and where to place them along the reaction coordinate. The goal is to create a continuous path of overlapping simulations that can be reliably stitched together. This exercise  guides you through a fundamental calculation to determine the minimum number of windows required, balancing the need for sufficient statistical overlap with efficient use of computational resources.",
            "id": "1994857",
            "problem": "An essential technique in computational chemistry for calculating free energy profiles along a reaction coordinate is umbrella sampling. This method is particularly useful for surmounting large energy barriers that would otherwise prevent adequate sampling in a standard molecular dynamics or Monte Carlo simulation.\n\nConsider the dissociation of a simple one-dimensional model of a diatomic molecule. The two particles interact via a Lennard-Jones potential, which also represents the Potential of Mean Force (PMF), $G(\\xi)$, along the reaction coordinate $\\xi = r$, where $r$ is the inter-particle distance. The potential is given by:\n$$G(r) = 4\\epsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right]$$\nIn an umbrella sampling simulation, a series of simulations (called \"windows\") are run. In each window $i$, a harmonic biasing potential, $w_i(r) = \\frac{1}{2} k (r - r_{i,0})^2$, is added to the system's potential energy. Here, $k$ is the spring constant and $r_{i,0}$ is the center of the harmonic potential for window $i$.\n\nYour task is to determine the minimum number of windows needed to compute the PMF over the range from $r_{start} = 1.0\\sigma$ to $r_{end} = 3.0\\sigma$. A common criterion for ensuring sufficient overlap between adjacent windows is to set the separation between their centers, $|r_{i+1,0} - r_{i,0}|$, equal to twice the standard deviation of the particle separation fluctuations observed within a single window. The first window should be centered at $r_{1,0} = r_{start}$, and the subsequent windows should be placed at increasing separation until the center of the last window is at or past $r_{end}$.\n\nFor your calculation, you are to make a key simplifying assumption: the underlying PMF, $G(r)$, is approximately flat within the spatial region sampled by any single window. Therefore, the fluctuations in $r$ within a window are governed solely by the harmonic biasing potential at the given temperature.\n\nUse the following parameters:\n- Thermal energy, $k_B T = 2.5 \\text{ kJ/mol}$\n- LJ energy parameter, $\\epsilon = 1.0 \\text{ kJ/mol}$\n- LJ size parameter, $\\sigma = 0.34 \\text{ nm}$\n- Biasing potential spring constant, $k = 4000 \\text{ kJ mol}^{-1}\\text{nm}^{-2}$\n\nCalculate the minimum integer number of windows required.",
            "solution": "We assume the PMF is locally flat within each window, so the fluctuations in the reaction coordinate within a window are governed solely by the harmonic bias potential. For a one-dimensional harmonic potential $w(r)=\\frac{1}{2}k(r-r_{0})^{2}$ at temperature $T$, the canonical distribution is Gaussian with variance given by equipartition:\n$$\n\\frac{1}{2}k\\langle (r-r_{0})^{2}\\rangle=\\frac{1}{2}k_{B}T \\;\\;\\Rightarrow\\;\\; \\langle (r-r_{0})^{2}\\rangle=\\frac{k_{B}T}{k}.\n$$\nHence the standard deviation within a window is\n$$\ns=\\sqrt{\\frac{k_{B}T}{k}}.\n$$\nThe overlap criterion sets the spacing between neighboring window centers to be\n$$\n\\Delta=2s=2\\sqrt{\\frac{k_{B}T}{k}}.\n$$\nThe total range to cover is from $r_{start}=1.0\\,\\sigma$ to $r_{end}=3.0\\,\\sigma$, so the length is\n$$\nL=r_{end}-r_{start}=(3.0-1.0)\\sigma=2.0\\,\\sigma.\n$$\nConvert to nanometers using $\\sigma=0.34\\,\\text{nm}$:\n$$\nL=2.0\\times 0.34\\,\\text{nm}=0.68\\,\\text{nm}.\n$$\nCompute the standard deviation with $k_{B}T=2.5\\,\\text{kJ mol}^{-1}$ and $k=4000\\,\\text{kJ mol}^{-1}\\text{nm}^{-2}$:\n$$\ns=\\sqrt{\\frac{2.5}{4000}}\\,\\text{nm}=\\sqrt{6.25\\times 10^{-4}}\\,\\text{nm}=0.025\\,\\text{nm},\n$$\nso\n$$\n\\Delta=2s=0.050\\,\\text{nm}.\n$$\nThe minimum number of steps between centers needed to span $L$ is\n$$\nn_{\\text{steps}}=\\left\\lceil \\frac{L}{\\Delta}\\right\\rceil=\\left\\lceil \\frac{0.68}{0.05}\\right\\rceil=\\lceil 13.6\\rceil=14.\n$$\nIncluding the initial window, the minimum number of windows is\n$$\nN=n_{\\text{steps}}+1=14+1=15.\n$$\nTherefore, the minimum integer number of windows required is $15$.",
            "answer": "$$\\boxed{15}$$"
        },
        {
            "introduction": "Once you have a plan for where to place your windows, you face another practical challenge: how to allocate your limited computer time. Given a fixed computational budget, is it better to run many short simulations in densely packed windows, or fewer long simulations in more sparsely placed windows? This exercise  presents a classic strategic dilemma, forcing you to weigh the importance of histogram overlap against the need for statistically converged sampling within each individual window.",
            "id": "2466520",
            "problem": "You are estimating a one-dimensional potential of mean force (PMF) along a reaction coordinate $q$ from $q_{\\min}$ to $q_{\\max}$ using umbrella sampling with harmonic biasing potentials centered at uniformly spaced values of $q$. The span is $L = q_{\\max} - q_{\\min} = 9\\,\\text{\\AA}$. The harmonic bias stiffness has been chosen so that, at the simulation temperature, the stationary biased distribution of $q$ in each umbrella window has standard deviation $\\sigma_q \\approx 1.0\\,\\text{\\AA}$. You will reconstruct the PMF using the Weighted Histogram Analysis Method (WHAM).\n\nAssume the following properties are characteristic of this system under the applied biases:\n- The integrated autocorrelation time of $q$ within an equilibrated biased window is $\\tau = 0.5\\,\\mathrm{ns}$.\n- The time required for the trajectory to reach the stationary biased distribution when each window is started from a configuration near its bias center is $t_{\\mathrm{eq}} = 0.2\\,\\mathrm{ns}$.\n\nYou have a fixed total molecular dynamics simulation budget of $T_{\\text{tot}} = 100\\,\\mathrm{ns}$ to distribute across windows. Consider two strategies with uniformly spaced window centers:\n- Plan I: $M = 10$ windows, each run for $t = 10\\,\\mathrm{ns}$.\n- Plan II: $M = 100$ windows, each run for $t = 1\\,\\mathrm{ns}$.\n\nWhich plan is expected to yield a lower overall statistical uncertainty in the reconstructed PMF, and why?\n\nChoose the single best answer.\n\nA. Plan I, because the window spacing is comparable to the biased width (ensuring sufficient histogram overlap), while the longer per-window runs greatly reduce the fraction of time lost to equilibration and provide more decorrelated samples per window; Plan II wastes a large fraction of the total budget on repeated equilibration and yields too few decorrelated samples per window.\n\nB. Plan II, because using more windows always improves WHAM reconstructions regardless of per-window sampling length; per-window autocorrelation and equilibration do not meaningfully affect the final uncertainty.\n\nC. The two plans will be statistically equivalent under a fixed total budget because WHAM reweighting removes any dependence on overlap quality and time correlations.\n\nD. Plan II, because with $M = 100$ the overlap becomes effectively perfect and therefore dominates the error reduction; the shorter runs per window further reduce time correlations, so Plan II yields the lowest uncertainty under the stated conditions.",
            "solution": "The problem statement asks for a comparison of two different strategies for performing an umbrella sampling calculation to determine a one-dimensional potential of mean force (PMF). The goal is to determine which strategy yields a lower overall statistical uncertainty, given a fixed total computational budget.\n\n### Step 1: Extract Givens\n- Reaction coordinate: $q$\n- Range of reaction coordinate: from $q_{\\min}$ to $q_{\\max}$\n- Total span of reaction coordinate: $L = q_{\\max} - q_{\\min} = 9\\,\\text{\\AA}$\n- Biasing potential: Harmonic, centered at uniformly spaced values of $q$\n- Standard deviation of biased distribution of $q$: $\\sigma_q \\approx 1.0\\,\\text{\\AA}$\n- Reconstruction method: Weighted Histogram Analysis Method (WHAM)\n- Integrated autocorrelation time of $q$: $\\tau = 0.5\\,\\mathrm{ns}$\n- Equilibration time per window: $t_{\\mathrm{eq}} = 0.2\\,\\mathrm{ns}$\n- Total simulation budget: $T_{\\text{tot}} = 100\\,\\mathrm{ns}$\n- Plan I: $M_I = 10$ windows, simulation time per window $t_I = 10\\,\\mathrm{ns}$\n- Plan II: $M_{II} = 100$ windows, simulation time per window $t_{II} = 1\\,\\mathrm{ns}$\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard, well-defined scenario in computational chemistry. The parameters given are physically realistic, and the question posed is a common consideration in designing enhanced sampling simulations. The total simulation time is conserved in both plans: for Plan I, $M_I \\times t_I = 10 \\times 10\\,\\mathrm{ns} = 100\\,\\mathrm{ns} = T_{\\text{tot}}$; for Plan II, $M_{II} \\times t_{II} = 100 \\times 1\\,\\mathrm{ns} = 100\\,\\mathrm{ns} = T_{\\text{tot}}$. The problem is scientifically grounded, well-posed, objective, and internally consistent. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n### Derivation\nThe statistical uncertainty of a PMF reconstructed using WHAM is a complex function of the number of windows, the overlap between adjacent windows, and the number of statistically independent samples collected within each window. An optimal sampling strategy must strike a balance between these factors. We will analyze each plan with respect to these three criteria.\n\n#### Analysis of Plan I\n1.  **Window Spacing and Overlap**:\n    With $M_I = 10$ windows distributed uniformly over a span $L = 9\\,\\text{\\AA}$, the spacing between the centers of adjacent windows is $\\Delta q_I = L / (M_I - 1) = 9\\,\\text{\\AA} / (10 - 1) = 1.0\\,\\text{\\AA}$. The standard deviation of the sampling distribution within each window is given as $\\sigma_q \\approx 1.0\\,\\text{\\AA}$. The ratio of spacing to width is $\\Delta q_I / \\sigma_q \\approx 1.0$. This level of spacing is generally considered optimal or near-optimal, as it ensures sufficient overlap for WHAM to reliably connect the windows without being excessively redundant. Good overlap is critical for minimizing the propagation of statistical error across the reaction coordinate.\n\n2.  **Statistical Sampling**:\n    The total simulation time per window is $t_I = 10\\,\\mathrm{ns}$. Of this, $t_{\\mathrm{eq}} = 0.2\\,\\mathrm{ns}$ is required for equilibration. The productive sampling time per window is thus $t_{\\mathrm{prod}, I} = t_I - t_{\\mathrm{eq}} = 10\\,\\mathrm{ns} - 0.2\\,\\mathrm{ns} = 9.8\\,\\mathrm{ns}$. The integrated autocorrelation time is $\\tau = 0.5\\,\\mathrm{ns}$. The number of statistically independent (or decorrelated) samples, $N_{\\mathrm{eff}}$, can be estimated as the ratio of the productive time to the autocorrelation time.\n    $$N_{\\mathrm{eff}, I} = \\frac{t_{\\mathrm{prod}, I}}{\\tau} = \\frac{9.8\\,\\mathrm{ns}}{0.5\\,\\mathrm{ns}} = 19.6$$\n    This means each window contributes approximately $19-20$ independent samples. This is a reasonable number to construct a local histogram with acceptable statistical properties. The variance of the estimated free energy of a window typically scales as $1/N_{\\mathrm{eff}}$.\n\n3.  **Efficiency**:\n    The total time spent on equilibration for Plan I is $M_I \\times t_{\\mathrm{eq}} = 10 \\times 0.2\\,\\mathrm{ns} = 2.0\\,\\mathrm{ns}$. This represents only $2.0\\,\\mathrm{ns} / 100\\,\\mathrm{ns} = 2\\%$ of the total simulation budget, which is a very efficient use of resources.\n\n#### Analysis of Plan II\n1.  **Window Spacing and Overlap**:\n    With $M_{II} = 100$ windows over the same span $L = 9\\,\\text{\\AA}$, the spacing is $\\Delta q_{II} = L / (M_{II} - 1) = 9\\,\\text{\\AA} / (100 - 1) \\approx 0.091\\,\\text{\\AA}$. The ratio of spacing to width is $\\Delta q_{II} / \\sigma_q \\approx 0.091$. This represents extreme oversampling in the reaction coordinate space. The overlap between adjacent windows is nearly perfect. While WHAM can handle this, the benefit of such dense spacing diminishes rapidly, and it may not be an efficient allocation of a finite computational budget.\n\n2.  **Statistical Sampling**:\n    The total simulation time per window is $t_{II} = 1\\,\\mathrm{ns}$. The productive sampling time is $t_{\\mathrm{prod}, II} = t_{II} - t_{\\mathrm{eq}} = 1\\,\\mathrm{ns} - 0.2\\,\\mathrm{ns} = 0.8\\,\\mathrm{ns}$. The number of statistically independent samples per window is:\n    $$N_{\\mathrm{eff}, II} = \\frac{t_{\\mathrm{prod}, II}}{\\tau} = \\frac{0.8\\,\\mathrm{ns}}{0.5\\,\\mathrm{ns}} = 1.6$$\n    This is a critically low number of independent samples. A histogram constructed from fewer than two effective data points is statistically meaningless and will have enormous variance. The uncertainty in the free energy of each individual window will be very large, and this high local uncertainty will propagate throughout the entire PMF via the WHAM equations, leading to a poor-quality final result despite the excellent overlap.\n\n3.  **Efficiency**:\n    The total time spent on equilibration for Plan II is $M_{II} \\times t_{\\mathrm{eq}} = 100 \\times 0.2\\,\\mathrm{ns} = 20.0\\,\\mathrm{ns}$. This constitutes $20.0\\,\\mathrm{ns} / 100\\,\\mathrm{ns} = 20\\%$ of the total simulation budget. A significant fraction of the computation is wasted on repeated equilibrations, leaving less time for productive sampling.\n\n#### Conclusion\nPlan I represents a well-balanced strategy. The window spacing is well-matched to the sampling width, ensuring good overlap. The simulation time per window is sufficient to overcome equilibration and collect a statistically meaningful number of decorrelated samples. Plan II is a poorly designed strategy. It prioritizes overlap to an unnecessary degree at the severe cost of per-window statistical sampling. The number of effective samples per window is far too low for a reliable reconstruction, and a large portion of the computational budget is wasted on equilibration. Therefore, Plan I is expected to yield a PMF with a significantly lower overall statistical uncertainty.\n\n### Option-by-Option Analysis\n\n**A. Plan I, because the window spacing is comparable to the biased width (ensuring sufficient histogram overlap), while the longer per-window runs greatly reduce the fraction of time lost to equilibration and provide more decorrelated samples per window; Plan II wastes a large fraction of the total budget on repeated equilibration and yields too few decorrelated samples per window.**\nThis statement correctly identifies all the critical factors. It notes that Plan I has sufficient overlap ($\\Delta q_I \\approx \\sigma_q$). It correctly states that the longer runs in Plan I lead to a smaller fraction of time lost to equilibration ($2\\%$ vs $20\\%$) and more decorrelated samples per window ($N_{\\mathrm{eff}, I} \\approx 19.6$ vs $N_{\\mathrm{eff}, II} = 1.6$). It correctly concludes that Plan II is wasteful and provides insufficient sampling per window. This reasoning is entirely correct.\n**Verdict: Correct**\n\n**B. Plan II, because using more windows always improves WHAM reconstructions regardless of per-window sampling length; per-window autocorrelation and equilibration do not meaningfully affect the final uncertainty.**\nThis statement is fundamentally flawed. The notion that \"more windows always improves\" is false; there is a point of diminishing returns, and if it compromises sampling, it becomes detrimental. The assertion that sampling length, autocorrelation, and equilibration are not meaningful is a direct contradiction of the statistical principles underlying error in Monte Carlo and molecular dynamics simulations. The number of effective samples is paramount.\n**Verdict: Incorrect**\n\n**C. The two plans will be statistically equivalent under a fixed total budget because WHAM reweighting removes any dependence on overlap quality and time correlations.**\nThis statement misinterprets the function of WHAM. WHAM is an *optimal* estimator given the available data; it does not and cannot \"remove\" the dependence on the quality of that data. The final uncertainty explicitly depends on the quality of overlap and, most importantly, on the statistical quality (i.e., number of independent samples) of the input histograms. WHAM cannot generate information that was not sampled.\n**Verdict: Incorrect**\n\n**D. Plan II, because with $M = 100$ the overlap becomes effectively perfect and therefore dominates the error reduction; the shorter runs per window further reduce time correlations, so Plan II yields the lowest uncertainty under the stated conditions.**\nThis statement contains a critical error in reasoning. It correctly identifies that overlap becomes \"perfect\" but incorrectly concludes this \"dominates the error reduction.\" The extreme lack of samples in each window will dominate the error, leading to a high overall uncertainty. The claim that \"shorter runs per window further reduce time correlations\" is nonsensical. The intrinsic autocorrelation time, $\\tau$, is a property of the system dynamics in the stationary state, not the length of the simulation run. A shorter run simply means the system is observed for a shorter period, yielding fewer total data points and thus fewer decorrelated samples.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A crucial part of computational science is not just producing results, but critically evaluating them for physical realism. It is common to encounter results that are clearly wrong, such as a potential of mean force (PMF) with an impossibly high energy barrier. This diagnostic exercise  equips you with a systematic troubleshooting checklist, helping you identify the most common and critical errors in simulation setup, execution, and analysis that can lead to such erroneous outcomes.",
            "id": "2466493",
            "problem": "You used umbrella sampling to compute a potential of mean force for ligand escape from a protein along a center-of-mass separation coordinate $r$ using $M$ harmonic windows with biases $w_i(r)=\\tfrac{1}{2}k_i\\left(r-r_i\\right)^2$ and combined them with the Weighted Histogram Analysis Method (WHAM). The resulting profile shows a single barrier of approximately $80\\ \\mathrm{kcal\\ mol^{-1}}$ in the dissociation region, which is physically unrealistic for a small-molecule ligand under typical Molecular Dynamics (MD) conditions near $T\\approx 300\\ \\mathrm{K}$. Which of the following items form a rigorous checklist of simulation or analysis issues you should investigate first to diagnose the inflated barrier? Select all that apply.\n\nA. Inadequate sampling per window and insufficient overlap between adjacent biased windows along $r$, causing poor statistical reconstruction of the unbiased probability density.\n\nB. Omission of the metric (Jacobian) factor for a $3$-dimensional radial coordinate, that is, neglecting the $r^2$ degeneracy when transforming biased histograms into a $1$-dimensional potential of mean force $W(r)$.\n\nC. A periodic simulation box that is too small, such that at large $r$ the ligand or protein interacts with periodic images, corrupting the asymptotic and barrier regions of $W(r)$.\n\nD. Choice of MD integration time step of $2\\ \\mathrm{fs}$ instead of $1\\ \\mathrm{fs}$, while all other simulation parameters are stable and well-converged.\n\nE. Unit mismatch between simulation and analysis, for example treating force constants reported in $\\mathrm{kJ\\ mol^{-1}\\ nm^{-2}}$ as if they were in $\\mathrm{kcal\\ mol^{-1}\\ nm^{-2}}$, or interpreting $r$ in $\\mathrm{nm}$ as if it were in another length unit during analysis.\n\nF. Omission of the standard-state correction when mapping the asymptotic region of $W(r)$ to an absolute binding free energy.\n\nG. A poorly chosen reaction coordinate that enforces an unphysical path (for example, a raw center-of-mass distance that forces the ligand through protein steric occlusions), producing large non-equilibrium work and barriers during biased sampling.\n\nH. Use of an incorrect temperature in WHAM or Multistate Bennett Acceptance Ratio (MBAR) analysis, that is, using a $\\beta=1/\\left(k_{\\mathrm{B}}T\\right)$ inconsistent with the simulated $T$, or a mixture of $\\beta$ values across windows.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Methodology**: Umbrella sampling used to compute a potential of mean force (PMF), $W(r)$.\n- **Reaction Coordinate**: Center-of-mass separation, $r$.\n- **Biasing**: $M$ harmonic windows with bias potentials $w_i(r)=\\tfrac{1}{2}k_i\\left(r-r_i\\right)^2$.\n- **Analysis**: Weighted Histogram Analysis Method (WHAM) was used to combine data.\n- **Result**: The computed PMF shows a single barrier of approximately $80\\ \\mathrm{kcal\\ mol^{-1}}$ in the dissociation region.\n- **Context**: Simulation of a small-molecule ligand dissociating from a protein.\n- **Simulation Conditions**: Molecular Dynamics (MD) near $T\\approx 300\\ \\mathrm{K}$.\n- **Premise**: The barrier height of $80\\ \\mathrm{kcal\\ mol^{-1}}$ is considered physically unrealistic.\n- **Objective**: Identify a rigorous checklist of potential issues to diagnose the inflated barrier.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem describes a common scenario in computational chemistry where an enhanced sampling simulation produces a physically implausible result.\n- **Scientifically Grounded**: The problem is firmly rooted in the principles of statistical mechanics and computational biophysics. Umbrella sampling, WHAM, and PMF calculations are standard techniques. A dissociation barrier of $80\\ \\mathrm{kcal\\ mol^{-1}}$ for a typical non-covalent ligand-protein complex at $300\\ \\mathrm{K}$ is indeed physically unrealistic, as it would imply a dissociation timescale on the order of billions of years, far exceeding what is typical for such systems (microseconds to hours). The premise is factually sound.\n- **Well-Posed**: The problem is well-posed. It presents a specific observation (anomalously high barrier) and asks for the identification of plausible root causes from a given list. A definite set of correct answers can be deduced based on established best practices and common pitfalls in the field.\n- **Objective**: The problem is stated using precise, objective, and technical language common to the field of computational chemistry. It is free of ambiguity and subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. It represents a realistic diagnostic exercise in computational science. I will proceed with the analysis.\n\nThe goal of umbrella sampling combined with WHAM is to compute the potential of mean force, or free energy profile, $W(r)$, along a reaction coordinate $r$. The PMF is related to the unbiased probability distribution function $P(r)$ by the fundamental relation from statistical mechanics:\n$$W(r) = -k_{\\mathrm{B}}T \\ln P(r) + C$$\nwhere $k_{\\mathrm{B}}$ is the Boltzmann constant, $T$ is the absolute temperature, and $C$ is an arbitrary constant. An error of $80\\ \\mathrm{kcal\\ mol^{-1}}$ is exceptionally large, as $k_{\\mathrm{B}}T \\approx 0.6\\ \\mathrm{kcal\\ mol^{-1}}$ at $T=300\\ \\mathrm{K}$. Such an error points not to minor inaccuracies but to a fundamental flaw in the simulation setup, execution, or analysis. Each option must be evaluated as a potential source of such a catastrophic failure.\n\n**A. Inadequate sampling per window and insufficient overlap between adjacent biased windows along $r$, causing poor statistical reconstruction of the unbiased probability density.**\nThe WHAM formalism relies on having sufficient statistical overlap in the sampled distributions $P_i(r)$ from adjacent windows $i$ and $i+1$. If the windows are too far apart, or if the sampling time within each window is too short, the histograms will be noisy and will not overlap sufficiently. This leads to a poorly conditioned system of equations in WHAM, resulting in large statistical errors and potentially creating artificial barriers or dramatically inflating existing ones. A lack of convergence is one of the most common and severe problems in PMF calculations. Therefore, verifying sampling convergence and histogram overlap is a primary and essential diagnostic step.\n**Verdict: Correct**\n\n**B. Omission of the metric (Jacobian) factor for a $3$-dimensional radial coordinate, that is, neglecting the $r^2$ degeneracy when transforming biased histograms into a $1$-dimensional potential of mean force $W(r)$.**\nThe reaction coordinate $r$ is a one-dimensional distance derived from a three-dimensional system. The volume of a spherical shell between $r$ and $r+dr$ is $4\\pi r^2 dr$. The probability of finding the system in this shell is therefore proportional to $r^2$. The one-dimensional probability density $P(r)$ used to define the PMF must account for this geometric factor. The PMF is correctly calculated as $W(r) = -k_{\\mathrm{B}}T \\ln(N(r)/r^2) + C'$, where $N(r)$ is the raw histogram count. This is equivalent to adding a correction term, $-k_{\\mathrm{B}}T \\ln(r^2)$, to the \"naive\" PMF. Omitting this Jacobian correction is a fundamental methodological error. This term represents the change in conformational entropy as the available volume increases with $r$. Its omission makes the free energy at large $r$ artificially high relative to small $r$, thus increasing the apparent barrier height. At $T=300\\ \\mathrm{K}$, the magnitude of this error over a typical dissociation path (e.g., from $r=5\\ \\mathrm{\\AA}$ to $r=25\\ \\mathrm{\\AA}$) is $2k_{\\mathrm{B}}T \\ln(25/5) = 2(0.6\\ \\mathrm{kcal\\ mol^{-1}}) \\ln(5) \\approx 1.9\\ \\mathrm{kcal\\ mol^{-1}}$. While this alone does not explain an $80\\ \\mathrm{kcal\\ mol^{-1}}$ barrier, it is a systematic error that must be on any rigorous checklist for PMF calculations involving radial coordinates.\n**Verdict: Correct**\n\n**C. A periodic simulation box that is too small, such that at large $r$ the ligand or protein interacts with periodic images, corrupting the asymptotic and barrier regions of $W(r)$.**\nIn simulations using periodic boundary conditions, the system is replicated in all directions. If the simulation box is not large enough, as the ligand dissociates from the protein (increasing $r$), it will begin to interact with the periodic image of the protein. The minimum image convention will cause the measured distance to be artificially small, and spurious electrostatic and van der Waals interactions will occur. These finite-size artifacts will prevent the PMF from correctly plateauing at large $r$, often causing it to curve back down or up, which severely corrupts the definition of the unbound state and thus the overall barrier height. This is a critical and common mistake in simulation setup.\n**Verdict: Correct**\n\n**D. Choice of MD integration time step of $2\\ \\mathrm{fs}$ instead of $1\\ \\mathrm{fs}$, while all other simulation parameters are stable and well-converged.**\nA time step of $2\\ \\mathrm{fs}$ is standard practice for biomolecular simulations when using algorithms like SHAKE or LINCS to constrain the motion of bonds involving hydrogen atoms. The problem states that the simulation is stable and well-converged, which implies that this choice of time step did not lead to catastrophic integration failure. While a smaller time step might offer marginally better energy conservation, it is extremely unlikely to be the cause of a systematic error of $80\\ \\mathrm{kcal\\ mol^{-1}}$. Such an error points to a flaw in the physics, statistics, or logic of the calculation, not to minor integration inaccuracies.\n**Verdict: Incorrect**\n\n**E. Unit mismatch between simulation and analysis, for example treating force constants reported in $\\mathrm{kJ\\ mol^{-1}\\ nm^{-2}}$ as if they were in $\\mathrm{kcal\\ mol^{-1}\\ nm^{-2}}$, or interpreting $r$ in $\\mathrm{nm}$ as if it were in another length unit during analysis.**\nThis represents a class of simple but devastating human errors. For example, $1\\ \\mathrm{kcal} \\approx 4.184\\ \\mathrm{kJ}$. If the potential energies from the MD simulation (e.g., in $\\mathrm{kcal\\ mol^{-1}}$) are fed to a WHAM program that expects energies in $\\mathrm{kJ\\ mol^{-1}}$, the resulting PMF barrier will be overestimated by a factor of approximately $4.184$. A true barrier of $\\approx 19\\ \\mathrm{kcal\\ mol^{-1}}$ would then appear as $80\\ \\mathrm{kcal\\ mol^{-1}}$. Similarly, a mismatch in the units of the force constant $k_i$ or the coordinate $r$ (e.g., $\\mathrm{\\AA}$ vs. $\\mathrm{nm}$) would lead to the bias potentials being incorrect by orders of magnitude, destroying the validity of the sampling and subsequent analysis. Such unit-related mistakes are a very common source of large, systematic errors and must be among the first things to be checked.\n**Verdict: Correct**\n\n**F. Omission of the standard-state correction when mapping the asymptotic region of $W(r)$ to an absolute binding free energy.**\nThe standard-state correction, $\\Delta G_{\\mathrm{corr}} = -k_{\\mathrm{B}}T \\ln(C^\\circ V_{\\mathrm{site}})$, is a term added *after* the PMF profile has been computed. It is used to convert the binding free energy derived from the PMF (which corresponds to a specific interaction volume $V_{\\mathrm{site}}$) to a standard concentration $C^\\circ$ (typically $1\\ \\mathrm{M}$). This correction affects the final, single value of the standard binding free energy, $\\Delta G^\\circ_{bind}$, not the shape or barrier height of the $W(r)$ profile itself. The problem is with the barrier within the profile, not the final thermodynamic number derived from it.\n**Verdict: Incorrect**\n\n**G. A poorly chosen reaction coordinate that enforces an unphysical path (for example, a raw center-of-mass distance that forces the ligand through protein steric occlusions), producing large non-equilibrium work and barriers during biased sampling.**\nThe PMF gives the free energy along the chosen path, which is assumed to be a reasonable representation of the true reaction pathway. If the chosen one-dimensional coordinate (e.g., center-of-mass distance) is a poor descriptor of the complex, multi-dimensional dissociation process, it may force the system over an unphysical, high-energy path. For instance, it might pull the ligand directly through a protein loop that it would naturally move around. The computed PMF will correctly report a very high free energy for this sterically hindered, unphysical pathway. This is not a failure of the umbrella sampling method itself, but a failure in the design of the experiment. The result is a large barrier that reflects significant non-equilibrium work rather than an equilibrium free energy barrier. This is a fundamental conceptual error that is a primary suspect for an inflated barrier.\n**Verdict: Correct**\n\n**H. Use of an incorrect temperature in WHAM or Multistate Bennett Acceptance Ratio (MBAR) analysis, that is, using a $\\beta=1/\\left(k_{\\mathrm{B}}T\\right)$ inconsistent with the simulated $T$, or a mixture of $\\beta$ values across windows.**\nThe WHAM equations use the inverse temperature, $\\beta = 1/(k_{\\mathrm{B}}T)$, to unbias the data and reconstruct the free energy profile. The final PMF, $W(r)$, is expressed in units of energy. If the simulation was run at $T_{\\mathrm{sim}}$, but the analysis was performed with an incorrect temperature $T_{\\mathrm{analysis}}$, the resulting free energy profile will be incorrectly scaled. The WHAM equations essentially solve for $W(r) / (k_{\\mathrm{B}}T_{\\mathrm{analysis}})$. If one specifies a temperature $T_{\\mathrm{analysis}}$ that is substantially different from $T_{\\mathrm{sim}}$, the resulting energy profile will be scaled by $T_{\\mathrm{analysis}}/T_{\\mathrm{sim}}$. For instance, if $T_{\\mathrm{sim}} = 300\\ \\mathrm{K}$ but the WHAM analysis is mistakenly performed using energies in units of $\\mathrm{kcal\\ mol^{-1}}$ and a $\\beta$ corresponding to $T=30\\ \\mathrm{K}$, the final barrier would be overestimated tenfold. An $8\\ \\mathrm{kcal\\ mol^{-1}}$ barrier would appear as $80\\ \\mathrm{kcal\\ mol^{-1}}$. This is another simple human error that directly causes massive, systematic scaling of the PMF.\n**Verdict: Correct**",
            "answer": "$$\\boxed{ABCEGH}$$"
        }
    ]
}