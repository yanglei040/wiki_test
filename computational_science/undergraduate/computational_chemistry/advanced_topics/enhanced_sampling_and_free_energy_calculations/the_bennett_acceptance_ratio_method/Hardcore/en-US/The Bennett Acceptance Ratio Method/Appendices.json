{
    "hands_on_practices": [
        {
            "introduction": "The Bennett Acceptance Ratio (BAR) method relies on the Fermi function, $f(x) = (1 + \\exp(x))^{-1}$, to connect statistical ensembles. A direct implementation of this function is prone to numerical overflow errors for large positive arguments, a common pitfall in scientific programming. This first exercise  provides hands-on practice in developing a numerically stable algorithm by reformulating the function, ensuring your calculations are robust and accurate across all possible inputs.",
            "id": "2463465",
            "problem": "You are given the definition of the Fermi function $f(x)$ that appears inside the Bennett acceptance ratio method (BAR) for free-energy differences in computational chemistry:\n$$\nf(x) = \\frac{1}{1 + e^{x}}.\n$$\nDirect evaluation of $e^{x}$ in standard double-precision floating-point arithmetic can lead to overflow for large positive $x$ and severe underflow for large negative $x$. Your task is to implement a numerically stable computation of $f(x)$ for all real $x$, using double-precision arithmetic. Your implementation must return a finite result in $[0, 1]$ for all real inputs $x$.\n\nImplement a program that:\n- Computes $f(x)$ as defined above with a numerically stable scheme in double-precision arithmetic.\n- Applies this computation to the following ordered test suite of inputs:\n  - $x \\in \\{-1000,\\,-745,\\,-100,\\,-50,\\,-10^{-12},\\,0,\\,10^{-12},\\,50,\\,100,\\,709,\\,710,\\,1000\\}$.\n- Produces the results for $f(x)$ in exactly the same order as the inputs.\n\nRequirements and output specification:\n- Use double-precision arithmetic for all calculations.\n- The final program output must be a single line containing the results as a comma-separated list enclosed in square brackets, with the values appearing in the order of the test inputs. For example, the required format is like \"[result1,result2,result3,...]\".\n- The outputs are real numbers (floating-point values). No physical units are involved in this problem.",
            "solution": "A robust solution requires an analysis of the function's behavior and the limitations of floating-point arithmetic. The function is given by:\n$$\nf(x) = \\frac{1}{1 + e^{x}}\n$$\nWe analyze the numerical stability of this expression for different regimes of the input variable $x$.\n\nCase 1: $x$ is large and positive.\nFor large positive $x$, the term $e^x$ grows exponentially. In standard IEEE $754$ double-precision arithmetic, the maximum representable finite value is approximately $1.8 \\times 10^{308}$. The exponential function $e^x$ will overflow when $x > \\ln(1.8 \\times 10^{308}) \\approx 709.78$. A direct computation for $x$ values such as $710$ or $1000$ would lead to an intermediate infinite value, which is poor numerical practice even if the final result is correctly resolved to $0$ by the floating-point unit's rules for `inf`.\n\nTo create a stable algorithm, we must reformulate the expression to avoid computing $e^x$ for positive $x$. We can multiply the numerator and denominator by $e^{-x}$:\n$$\nf(x) = \\frac{1}{1 + e^{x}} = \\frac{e^{-x}}{e^{-x}(1 + e^{x})} = \\frac{e^{-x}}{e^{-x} + 1}\n$$\nIn this alternative form, if $x$ is positive, then $-x$ is negative. The evaluation of $e^{-x}$ will result in a value between $0$ and $1$, thus preventing overflow. This form is numerically stable for $x \\ge 0$.\n\nCase 2: $x$ is large and negative.\nFor large negative $x$, the term $e^x$ approaches $0$. In double-precision, $e^x$ will underflow to exactly $0$ for $x < \\ln(2.2 \\times 10^{-308}) \\approx -708.4$. In this case, the direct computation $f(x) = \\frac{1}{1 + e^x}$ becomes $\\frac{1}{1 + 0} = 1$. This computation is numerically stable and gives the correct limiting value. There is no risk of overflow or catastrophic cancellation.\n\nCase 3: $x$ is near $0$.\nFor $x=0$, both forms yield $f(0) = \\frac{1}{1+e^0} = \\frac{1}{2}$, which is stable.\n\nBased on this analysis, a hybrid algorithm that selects the appropriate formula based on the sign of $x$ provides complete numerical stability across the entire real number line.\n\nThe proposed stable algorithm is as follows:\n-   If $x < 0$, use the original form: $f(x) = \\frac{1}{1 + e^x}$. The argument to the exponential function is negative, ensuring stability.\n-   If $x \\ge 0$, use the algebraically equivalent form: $f(x) = \\frac{e^{-x}}{1 + e^{-x}}$. The argument to the exponential function is non-positive, again ensuring stability.\n\nThis strategy guarantees that the argument passed to the exponential function is always less than or equal to zero, thereby robustly preventing overflow and producing a finite result for any real input $x$. This implementation will now be applied to the provided suite of test inputs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Fermi function f(x) = 1/(1 + exp(x)) for a given set of\n    test inputs using a numerically stable scheme. The final results are\n    printed in the specified format.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        -1000.0,\n        -745.0,\n        -100.0,\n        -50.0,\n        -1.0e-12,\n        0.0,\n        1.0e-12,\n        50.0,\n        100.0,\n        709.0,\n        710.0,\n        1000.0\n    ]\n\n    def stable_fermi(x: float) -> float:\n        \"\"\"\n        Calculates the Fermi function f(x) = 1 / (1 + exp(x)) in a\n        numerically stable manner.\n\n        The method switches between two algebraically equivalent forms of the\n        function based on the sign of x to prevent floating-point overflow.\n\n        Args:\n            x: A real number (float).\n\n        Returns:\n            The value of f(x) as a float.\n        \"\"\"\n        # For x < 0, the argument to exp(x) is negative, so the direct\n        # evaluation is stable and does not risk overflow.\n        if x < 0.0:\n            return 1.0 / (1.0 + np.exp(x))\n        else:\n            # For x >= 0, exp(x) can overflow for large x.\n            # We use the equivalent form f(x) = exp(-x) / (1 + exp(-x)).\n            # The argument to the exponential, -x, is non-positive,\n            # which prevents overflow.\n            exp_neg_x = np.exp(-x)\n            return exp_neg_x / (1.0 + exp_neg_x)\n\n    # Calculate the results for all test cases.\n    results = [stable_fermi(x) for x in test_cases]\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) is used to format numbers without trailing zeros\n    # for cleaner representation, though standard float conversion is sufficient.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a stable Fermi function at our disposal, we can now tackle a complete implementation of the Bennett Acceptance Ratio method. This practice  challenges you to calculate the free energy difference between two states of a particle in a hypothetical two-dimensional potential. By generating samples and solving the core BAR equation, you will gain firsthand experience with the entire workflow, from simulation to analysis, solidifying your understanding of how BAR optimally combines information from different thermodynamic states.",
            "id": "2463503",
            "problem": "Consider a single particle moving in a two-dimensional configuration space with position vector $\\mathbf{r} = (x,y)$. The reduced potential energy (that is, the potential multiplied by the inverse thermal energy, with inverse temperature $\\beta$) for a given scalar parameter $q$ is defined as\n$$\nu(q;\\mathbf{r}) \\equiv \\beta U(q;\\mathbf{r}) = \\tfrac{1}{2} k \\left(x^2 + y^2\\right) - q \\, \\mathbf{E} \\cdot \\mathbf{r},\n$$\nwhere $k$ is a positive scalar spring constant, and $\\mathbf{E} = (E_x, E_y)$ is a fixed electric field vector. Work entirely in reduced, dimensionless units with $\\beta = 1$, so that all energies are dimensionless; no physical units are used or required.\n\nDefine two thermodynamic states, $A$ and $B$, that differ only in the value of the scalar parameter $q$, denoted $q_A$ and $q_B$, respectively. The corresponding reduced potentials are $u_A(\\mathbf{r}) = u(q_A;\\mathbf{r})$ and $u_B(\\mathbf{r}) = u(q_B;\\mathbf{r})$. Let the reduced free energy difference $\\Delta f_{BA}$ be defined as\n$$\n\\Delta f_{BA} \\equiv f_B - f_A = -\\ln Z_B + \\ln Z_A,\n$$\nwhere $Z_A$ and $Z_B$ are the reduced partition functions of states $A$ and $B$, respectively.\n\nGiven independent and identically distributed samples $\\{\\mathbf{r}_i^{(A)}\\}_{i=1}^{N_A}$ drawn from the normalized Boltzmann distribution proportional to $\\exp\\{-u_A(\\mathbf{r})\\}$, and independent and identically distributed samples $\\{\\mathbf{r}_j^{(B)}\\}_{j=1}^{N_B}$ drawn from the normalized Boltzmann distribution proportional to $\\exp\\{-u_B(\\mathbf{r})\\}$, the Bennett acceptance ratio (BAR) method defines $\\Delta f_{BA}$ as the unique solution to the equation\n$$\n\\frac{1}{N_A} \\sum_{i=1}^{N_A} \\frac{1}{1+\\exp\\left(\\Delta u(\\mathbf{r}_i^{(A)}) - \\Delta f_{BA} - c\\right)} \\;=\\; \\frac{1}{N_B} \\sum_{j=1}^{N_B} \\frac{1}{1+\\exp\\left(-\\Delta u(\\mathbf{r}_j^{(B)}) + \\Delta f_{BA} - c\\right)},\n$$\nwhere $\\Delta u(\\mathbf{r}) \\equiv u_B(\\mathbf{r}) - u_A(\\mathbf{r})$ and $c \\equiv \\ln\\!\\left(\\frac{N_B}{N_A}\\right)$.\n\nYour task is to write a complete, runnable program that:\n- For each test case specified below, generates the required independent samples from the exact Boltzmann distributions corresponding to $u_A(\\mathbf{r})$ and $u_B(\\mathbf{r})$ using the specified random number generator seed for reproducibility.\n- Computes the reduced free energy difference $\\Delta f_{BA}$ by solving the BAR defining equation above.\n- Produces the final results for all test cases on a single line, formatted exactly as specified at the end of this problem statement.\n\nAll computations must be carried out in reduced, dimensionless units. Angles are not involved. The required output values are real numbers and must be reported as floating-point values rounded to exactly six digits after the decimal point.\n\nTest Suite:\nFor each case, parameters $(k, q_A, q_B, E_x, E_y, N_A, N_B, \\text{seed})$ are given.\n\n- Case $1$: $(k, q_A, q_B, E_x, E_y, N_A, N_B, \\text{seed}) = (1.0, 0.0, 1.0, 0.8, -0.4, 20000, 20000, 12345)$.\n- Case $2$: $(k, q_A, q_B, E_x, E_y, N_A, N_B, \\text{seed}) = (1.0, 0.7, 0.7, 1.2, -0.3, 15000, 15000, 54321)$.\n- Case $3$: $(k, q_A, q_B, E_x, E_y, N_A, N_B, \\text{seed}) = (1.0, 0.5, 0.9, 1.0, 1.0, 5000, 20000, 2023)$.\n- Case $4$: $(k, q_A, q_B, E_x, E_y, N_A, N_B, \\text{seed}) = (2.0, -0.6, 0.4, 0.0, 0.0, 10000, 10000, 777)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the list of the four BAR estimates of $\\Delta f_{BA}$, in the order of the cases above, as a comma-separated list enclosed in square brackets. Each value must be rounded to exactly six digits after the decimal point. For example, the output must have the form\n\"[v1,v2,v3,v4]\"\nwhere each $v_i$ is a floating-point value with exactly six digits after the decimal point.",
            "solution": "The problem requires the computation of the reduced free energy difference, $\\Delta f_{BA}$, between two thermodynamic states, $A$ and $B$, using the Bennett acceptance ratio (BAR) method. The system consists of a single particle in a two-dimensional potential. Before proceeding to the numerical implementation, a rigorous analysis of the underlying physical model and the numerical method is mandatory.\n\nFirst, we analyze the potential energy function. The reduced potential energy for a state defined by a parameter $q$ is given by:\n$$\nu(q;\\mathbf{r}) = \\tfrac{1}{2} k \\left(x^2 + y^2\\right) - q \\, \\mathbf{E} \\cdot \\mathbf{r}\n$$\nwhere $\\mathbf{r}=(x,y)$, $k>0$, and $\\mathbf{E}=(E_x, E_y)$. To understand the statistical properties of the system, we must identify the form of the Boltzmann distribution, $p(\\mathbf{r}) \\propto \\exp(-u(q;\\mathbf{r}))$. We can rewrite the potential by completing the square for the $x$ and $y$ coordinates:\n$$\n\\begin{aligned}\nu(q;\\mathbf{r}) &= \\left(\\tfrac{1}{2} k x^2 - q E_x x\\right) + \\left(\\tfrac{1}{2} k y^2 - q E_y y\\right) \\\\\n&= \\tfrac{k}{2} \\left[x^2 - \\tfrac{2qE_x}{k} x\\right] + \\tfrac{k}{2} \\left[y^2 - \\tfrac{2qE_y}{k} y\\right] \\\\\n&= \\tfrac{k}{2} \\left[\\left(x - \\tfrac{qE_x}{k}\\right)^2 - \\left(\\tfrac{qE_x}{k}\\right)^2\\right] + \\tfrac{k}{2} \\left[\\left(y - \\tfrac{qE_y}{k}\\right)^2 - \\left(\\tfrac{qE_y}{k}\\right)^2\\right] \\\\\n&= \\tfrac{k}{2} \\left(x - \\tfrac{qE_x}{k}\\right)^2 + \\tfrac{k}{2} \\left(y - \\tfrac{qE_y}{k}\\right)^2 - \\tfrac{q^2}{2k}(E_x^2 + E_y^2)\n\\end{aligned}\n$$\nThe probability density function is therefore:\n$$\np(q;\\mathbf{r}) \\propto \\exp\\left\\{ -\\tfrac{k}{2} \\left[\\left(x - \\tfrac{qE_x}{k}\\right)^2 + \\left(y - \\tfrac{qE_y}{k}\\right)^2\\right] \\right\\}\n$$\nThis is the kernel of a bivariate normal (Gaussian) distribution, $\\mathcal{N}(\\boldsymbol{\\mu}(q), \\boldsymbol{\\Sigma})$. The mean vector $\\boldsymbol{\\mu}(q)$ and covariance matrix $\\boldsymbol{\\Sigma}$ are identified by comparing the exponent to the standard form $-\\frac{1}{2}(\\mathbf{r}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{r}-\\boldsymbol{\\mu})$.\nThe mean vector is:\n$$\n\\boldsymbol{\\mu}(q) = \\begin{pmatrix} \\mu_x(q) \\\\ \\mu_y(q) \\end{pmatrix} = \\begin{pmatrix} qE_x/k \\\\ qE_y/k \\end{pmatrix} = \\frac{q}{k}\\mathbf{E}\n$$\nThe inverse covariance matrix is $\\boldsymbol{\\Sigma}^{-1} = k \\mathbf{I}$, where $\\mathbf{I}$ is the $2 \\times 2$ identity matrix. Thus, the covariance matrix is:\n$$\n\\boldsymbol{\\Sigma} = \\frac{1}{k}\\mathbf{I} = \\begin{pmatrix} 1/k & 0 \\\\ 0 & 1/k \\end{pmatrix}\n$$\nThis analysis proves that the particle's position $(x,y)$ follows a bivariate normal distribution with uncorrelated components, each having a variance of $\\sigma^2 = 1/k$. To generate the required samples for states $A$ and $B$, one must draw from $\\mathcal{N}(\\boldsymbol{\\mu}(q_A), \\boldsymbol{\\Sigma})$ and $\\mathcal{N}(\\boldsymbol{\\mu}(q_B), \\boldsymbol{\\Sigma})$, respectively.\n\nNext, we address the BAR equation itself. The equation requires the evaluation of the potential energy difference, $\\Delta u(\\mathbf{r})$:\n$$\n\\Delta u(\\mathbf{r}) = u_B(\\mathbf{r}) - u_A(\\mathbf{r}) = u(q_B;\\mathbf{r}) - u(q_A;\\mathbf{r})\n$$\n$$\n\\Delta u(\\mathbf{r}) = \\left(\\tfrac{1}{2} k r^2 - q_B \\mathbf{E} \\cdot \\mathbf{r}\\right) - \\left(\\tfrac{1}{2} k r^2 - q_A \\mathbf{E} \\cdot \\mathbf{r}\\right) = -(q_B - q_A) \\mathbf{E} \\cdot \\mathbf{r}\n$$\nLet $\\Delta q = q_B - q_A$. Then $\\Delta u(\\mathbf{r}) = -\\Delta q \\, (\\mathbf{E} \\cdot \\mathbf{r})$.\nThe BAR equation for $\\Delta f_{BA}$ is:\n$$\n\\left\\langle \\frac{1}{1+\\exp\\left(\\Delta u(\\mathbf{r}) - \\Delta f_{BA} - c\\right)} \\right\\rangle_A = \\left\\langle \\frac{1}{1+\\exp\\left(-\\Delta u(\\mathbf{r}) + \\Delta f_{BA} - c\\right)} \\right\\rangle_B\n$$\nwhere $\\langle \\cdot \\rangle_X$ denotes the sample average over configurations drawn from state $X$, and $c = \\ln(N_B/N_A)$. This is a non-linear equation for the unknown $\\Delta f_{BA}$. We can define an objective function $g(\\Delta f)$ whose root is the desired solution:\n$$\ng(\\Delta f) = \\frac{1}{N_A} \\sum_{i=1}^{N_A} \\frac{1}{1+e^{\\Delta u(\\mathbf{r}_i^{(A)}) - \\Delta f - c}} - \\frac{1}{N_B} \\sum_{j=1}^{N_B} \\frac{1}{1+e^{-\\Delta u(\\mathbf{r}_j^{(B)}) + \\Delta f - c}}\n$$\nThe function $g(\\Delta f)$ is continuous and strictly monotonically increasing with respect to $\\Delta f$. As $\\Delta f \\to -\\infty$, $g(\\Delta f) \\to -1$. As $\\Delta f \\to +\\infty$, $g(\\Delta f) \\to 1$. Consequently, a unique root for $g(\\Delta f) = 0$ is guaranteed to exist. This root must be found using a numerical root-finding algorithm, such as Brent's method.\n\nFor this specific problem, an analytical solution for the free energy difference exists, which serves as a crucial check on the numerical results. The reduced partition function $Z(q)$ is:\n$$\nZ(q) = \\int e^{-u(q;\\mathbf{r})} d\\mathbf{r} = e^{\\frac{q^2 E^2}{2k}} \\int \\int e^{-\\frac{k}{2}(x-\\mu_x)^2} e^{-\\frac{k}{2}(y-\\mu_y)^2} dx dy\n$$\nwhere $E^2 = |\\mathbf{E}|^2$. Each integral is a standard Gaussian integral evaluating to $\\sqrt{2\\pi/k}$.\n$$\nZ(q) = e^{\\frac{q^2 E^2}{2k}} \\left(\\frac{2\\pi}{k}\\right)\n$$\nThe reduced free energy is $f(q) = -\\ln Z(q) = -\\frac{q^2 E^2}{2k} - \\ln\\left(\\frac{2\\pi}{k}\\right)$. The exact free energy difference is therefore:\n$$\n\\Delta f_{BA} = f(q_B) - f(q_A) = \\left[-\\frac{q_B^2 E^2}{2k} - \\ln\\left(\\frac{2\\pi}{k}\\right)\\right] - \\left[-\\frac{q_A^2 E^2}{2k} - \\ln\\left(\\frac{2\\pi}{k}\\right)\\right] = -\\frac{(q_B^2 - q_A^2)E^2}{2k}\n$$\nThe numerically estimated $\\Delta f_{BA}$ from the BAR equation using a finite number of samples should be a statistical estimate of this exact theoretical value.\n\nThe computational procedure is as follows:\n1. For each test case, set the parameters $(k, q_A, q_B, \\mathbf{E}, N_A, N_B)$ and the random number generator seed.\n2. Generate $N_A$ samples $\\mathbf{r}_i^{(A)}$ from $\\mathcal{N}(\\boldsymbol{\\mu}(q_A), \\boldsymbol{\\Sigma})$ and $N_B$ samples $\\mathbf{r}_j^{(B)}$ from $\\mathcal{N}(\\boldsymbol{\\mu}(q_B), \\boldsymbol{\\Sigma})$.\n3. Compute the array of potential differences for each set of samples: $\\Delta u_i^{(A)} = -\\Delta q (\\mathbf{E} \\cdot \\mathbf{r}_i^{(A)})$ and $\\Delta u_j^{(B)} = -\\Delta q (\\mathbf{E} \\cdot \\mathbf{r}_j^{(B)})$.\n4. Numerically solve the equation $g(\\Delta f) = 0$ for $\\Delta f$ to find the BAR estimate of $\\Delta f_{BA}$.\n5. Repeat for all test cases and format the output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves the Bennett Acceptance Ratio (BAR) problem for a series of test cases.\n    For each case, it generates samples from the specified Boltzmann distributions,\n    then numerically solves the BAR equation to find the reduced free energy difference.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, q_A, q_B, E_x, E_y, N_A, N_B, seed)\n        (1.0, 0.0, 1.0, 0.8, -0.4, 20000, 20000, 12345),\n        (1.0, 0.7, 0.7, 1.2, -0.3, 15000, 15000, 54321),\n        (1.0, 0.5, 0.9, 1.0, 1.0, 5000, 20000, 2023),\n        (2.0, -0.6, 0.4, 0.0, 0.0, 10000, 10000, 777),\n    ]\n\n    results = []\n    for case in test_cases:\n        k, q_A, q_B, E_x, E_y, N_A, N_B, seed = case\n\n        # Initialize the random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Define system parameters and sampling distributions.\n        # The potential u(q;r) corresponds to a bivariate normal distribution.\n        # Mean vector mu(q) = (q/k) * E\n        # Covariance matrix Sigma = (1/k) * I, so std dev = 1/sqrt(k)\n        E = np.array([E_x, E_y])\n        sigma = 1.0 / np.sqrt(k)\n\n        # 2. Generate samples for states A and B.\n        mu_A = (q_A / k) * E\n        mu_B = (q_B / k) * E\n        \n        # Samples are drawn from N(mu, sigma^2 * I)\n        samples_A = rng.normal(loc=mu_A, scale=sigma, size=(N_A, 2))\n        samples_B = rng.normal(loc=mu_B, scale=sigma, size=(N_B, 2))\n\n        # 3. Calculate potential energy differences for the samples.\n        # delta_u(r) = u_B(r) - u_A(r) = -(q_B - q_A) * E.r\n        delta_q = q_B - q_A\n        # Using np.dot for vectorized dot product.\n        du_samples_from_A = -delta_q * np.dot(samples_A, E)\n        du_samples_from_B = -delta_q * np.dot(samples_B, E)\n        \n        # 4. Solve the BAR equation for the free energy difference delta_f_BA.\n        # The equation is: <sigmoid(du_A - df - c)>_A = <sigmoid(-du_B + df - c)>_B\n        # where sigmoid(x) = 1/(1+exp(x)) and c = ln(N_B/N_A).\n        # We find the root of the function f(df) = LHS - RHS.\n        c = np.log(N_B / N_A)\n\n        def bar_objective_function(delta_f):\n            \"\"\"\n            The objective function for the BAR root-finding problem.\n            The root of this function is the estimated free energy difference.\n            \"\"\"\n            # Argument for the sigmoid function.\n            arg_A = du_samples_from_A - delta_f - c\n            arg_B = -du_samples_from_B + delta_f - c\n\n            # Use np.mean for the expectation values.\n            # np.exp handles potential overflow by returning inf, which is correct\n            # as 1/(1+inf) -> 0.\n            lhs = np.mean(1.0 / (1.0 + np.exp(arg_A)))\n            rhs = np.mean(1.0 / (1.0 + np.exp(arg_B)))\n            \n            return lhs - rhs\n\n        # The objective function is monotonic, guaranteeing a unique root.\n        # A robust root-finding algorithm like Brent's method is suitable.\n        # We provide a wide bracket to ensure the root is found.\n        # As reasoned in the analysis, the function goes from ~-1 to ~+1.\n        try:\n            sol = root_scalar(bar_objective_function, bracket=[-100.0, 100.0], method='brentq')\n            delta_f_BA = sol.root\n        except ValueError:\n            # Fallback for edge cases, though unlikely here\n            delta_f_BA = 0.0\n\n        results.append(delta_f_BA)\n\n    # Final print statement in the exact required format.\n    # Each value is formatted to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After implementing the BAR method, a crucial next step is to understand its performance and limitations as a statistical estimator. While BAR is known for its efficiency, any estimate from finite data will have some statistical error and bias. This final practice  guides you to explore this concept by applying BAR to a simple harmonic oscillator—a system for which the exact free energy difference can be calculated analytically—allowing you to directly quantify the estimator's bias for small sample sizes.",
            "id": "2463439",
            "problem": "You are given two classical one-dimensional harmonic oscillator states, denoted by state $\\mathrm{A}$ and state $\\mathrm{B}$. The reduced potential energy (in units of Boltzmann constant times temperature, $k_{\\mathrm{B}}T$) of a configuration $x \\in \\mathbb{R}$ in state $\\mathrm{A}$ is $u_{\\mathrm{A}}(x) = \\tfrac{1}{2} k_{\\mathrm{A}} (x - \\mu_{\\mathrm{A}})^{2}$, and in state $\\mathrm{B}$ is $u_{\\mathrm{B}}(x) = \\tfrac{1}{2} k_{\\mathrm{B}} (x - \\mu_{\\mathrm{B}})^{2}$. Assume the inverse temperature is $\\beta = 1$ so that all energies are dimensionless in units of $k_{\\mathrm{B}}T$. For such harmonic oscillators, the classical configurational partition function of state $\\mathrm{X} \\in \\{\\mathrm{A},\\mathrm{B}\\}$ is $Z_{\\mathrm{X}} = \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-u_{\\mathrm{X}}(x)\\right)\\,\\mathrm{d}x = \\sqrt{\\tfrac{2\\pi}{k_{\\mathrm{X}}}}$, and the reduced free energy is $F_{\\mathrm{X}} = -\\ln Z_{\\mathrm{X}}$. Hence, the exact reduced free energy difference is $\\Delta F_{\\mathrm{exact}} = F_{\\mathrm{B}} - F_{\\mathrm{A}} = \\tfrac{1}{2}\\ln\\!\\left(\\tfrac{k_{\\mathrm{B}}}{k_{\\mathrm{A}}}\\right)$.\n\nSuppose you independently sample $N_{\\mathrm{A}}$ configurations $\\{x_{i}\\}_{i=1}^{N_{\\mathrm{A}}}$ from the canonical distribution of state $\\mathrm{A}$ and $N_{\\mathrm{B}}$ configurations $\\{y_{j}\\}_{j=1}^{N_{\\mathrm{B}}}$ from the canonical distribution of state $\\mathrm{B}$. For a one-dimensional harmonic oscillator with $u_{\\mathrm{X}}(x) = \\tfrac{1}{2} k_{\\mathrm{X}} (x - \\mu_{\\mathrm{X}})^{2}$ at $\\beta = 1$, the equilibrium distribution of $x$ in state $\\mathrm{X}$ is Gaussian with mean $\\mu_{\\mathrm{X}}$ and variance $1/k_{\\mathrm{X}}$. Define the reduced energy differences\n$\\Delta u_{i} = u_{\\mathrm{B}}(x_{i}) - u_{\\mathrm{A}}(x_{i})$ for $i \\in \\{1,\\dots,N_{\\mathrm{A}}\\}$ and $\\Delta u'_{j} = u_{\\mathrm{A}}(y_{j}) - u_{\\mathrm{B}}(y_{j})$ for $j \\in \\{1,\\dots,N_{\\mathrm{B}}\\}$.\n\nThe Bennett acceptance ratio (BAR) estimator defines the estimate $\\widehat{\\Delta f}$ of the reduced free energy difference $\\Delta F$ to be the unique solution of the equation\n$$\n\\sum_{i=1}^{N_{\\mathrm{A}}} \\frac{1}{1 + \\exp\\!\\left(\\Delta u_{i} - \\widehat{\\Delta f} + \\ln\\!\\left(\\tfrac{N_{\\mathrm{A}}}{N_{\\mathrm{B}}}\\right)\\right)} \\;=\\;\n\\sum_{j=1}^{N_{\\mathrm{B}}} \\frac{1}{1 + \\exp\\!\\left(\\Delta u'_{j} + \\widehat{\\Delta f} - \\ln\\!\\left(\\tfrac{N_{\\mathrm{A}}}{N_{\\mathrm{B}}}\\right)\\right)} \\, .\n$$\nFor a single realization of the samples, define the bias as $b = \\widehat{\\Delta f} - \\Delta F_{\\mathrm{exact}}$.\n\nYour task is to compute the bias $b$ for each of the following test cases. All quantities are dimensionless (in units of $k_{\\mathrm{B}}T$). For each case, draw independent samples from the specified Gaussian equilibrium distributions using the stated random seed, compute $\\widehat{\\Delta f}$ from the BAR equation above, and report $b$ rounded to $6$ decimal places.\n\nTest suite (each tuple is $(k_{\\mathrm{A}}, \\mu_{\\mathrm{A}}, k_{\\mathrm{B}}, \\mu_{\\mathrm{B}}, N_{\\mathrm{A}}, N_{\\mathrm{B}}, \\text{seed})$):\n- Case $1$: $(1.0, 0.0, 2.0, 0.0, 8, 8, 12345)$\n- Case $2$: $(1.0, 0.0, 1.0, 3.0, 5, 5, 54321)$\n- Case $3$: $(0.5, -1.0, 5.0, 1.0, 4, 4, 2024)$\n- Case $4$: $(1.0, 0.0, 3.0, 0.0, 2, 9, 777)$\n- Case $5$: $(1.0, 0.0, 1.0, 0.0, 1, 1, 999)$\n\nYour program must produce a single line of output containing a comma-separated list of the five biases in the order of the cases, enclosed in square brackets, for example, $[\\text{bias}_{1},\\text{bias}_{2},\\text{bias}_{3},\\text{bias}_{4},\\text{bias}_{5}]$, with each $\\text{bias}_{i}$ rounded to $6$ decimal places. The output values are dimensionless. No other text should be printed.",
            "solution": "The task is to compute the bias, $b = \\widehat{\\Delta f} - \\Delta F_{\\mathrm{exact}}$, of the BAR free energy estimator for five distinct cases. The estimator $\\widehat{\\Delta f}$ is the calculated free energy difference from simulated data, and $\\Delta F_{\\mathrm{exact}}$ is the analytical value.\n\nFirst, we establish the analytical quantities. The reduced potential energies for the two states, $\\mathrm{A}$ and $\\mathrm{B}$, are given by:\n$$\nu_{\\mathrm{A}}(x) = \\frac{1}{2} k_{\\mathrm{A}} (x - \\mu_{\\mathrm{A}})^{2}\n$$\n$$\nu_{\\mathrm{B}}(x) = \\frac{1}{2} k_{\\mathrm{B}} (x - \\mu_{\\mathrm{B}})^{2}\n$$\nThe exact reduced free energy difference between these states is:\n$$\n\\Delta F_{\\mathrm{exact}} = F_{\\mathrm{B}} - F_{\\mathrm{A}} = \\frac{1}{2}\\ln\\!\\left(\\frac{k_{\\mathrm{B}}}{k_{\\mathrm{A}}}\\right)\n$$\nThis formula is derived from the classical partition functions of the harmonic oscillators and will serve as the reference value for calculating the bias.\n\nThe computational procedure involves generating finite samples of configurations. For state $\\mathrm{A}$, $N_{\\mathrm{A}}$ configurations $\\{x_i\\}_{i=1}^{N_{\\mathrm{A}}}$ are drawn from its canonical distribution. For a harmonic oscillator, this distribution is a Gaussian with mean $\\mu_{\\mathrm{A}}$ and variance $\\sigma^2_{\\mathrm{A}} = 1/k_{\\mathrm{A}}$. Similarly, for state $\\mathrm{B}$, $N_{\\mathrm{B}}$ configurations $\\{y_j\\}_{j=1}^{N_{\\mathrm{B}}}$ are drawn from a Gaussian distribution with mean $\\mu_{\\mathrm{B}}$ and variance $\\sigma^2_{\\mathrm{B}} = 1/k_{\\mathrm{B}}$. The random samples will be generated using the specified seeds for reproducibility.\n\nOnce the samples are obtained, we calculate the work values. The forward work values are the energy differences evaluated on samples from state $\\mathrm{A}$:\n$$\n\\Delta u_{i} = u_{\\mathrm{B}}(x_{i}) - u_{\\mathrm{A}}(x_{i}), \\quad i = 1, \\dots, N_{\\mathrm{A}}\n$$\nThe reverse work values are evaluated on samples from state $\\mathrm{B}$:\n$$\n\\Delta u'_{j} = u_{\\mathrm{A}}(y_{j}) - u_{\\mathrm{B}}(y_{j}), \\quad j = 1, \\dots, N_{\\mathrm{B}}\n$$\n\nThe BAR estimate of the free energy difference, $\\widehat{\\Delta f}$, is the solution to the following implicit equation:\n$$\n\\sum_{i=1}^{N_{\\mathrm{A}}} \\frac{1}{1 + \\exp\\!\\left(\\Delta u_{i} - \\widehat{\\Delta f} + C \\right)} = \\sum_{j=1}^{N_{\\mathrm{B}}} \\frac{1}{1 + \\exp\\!\\left(\\Delta u'_{j} + \\widehat{\\Delta f} - C \\right)}\n$$\nwhere $C = \\ln(N_{\\mathrm{A}}/N_{\\mathrm{B}})$. To find $\\widehat{\\Delta f}$, we define a function $G(\\widehat{\\Delta f})$ as the difference between the left-hand side (LHS) and right-hand side (RHS) of the equation, which must be equal to zero:\n$$\nG(\\widehat{\\Delta f}) = \\sum_{i=1}^{N_{\\mathrm{A}}} \\frac{1}{1 + \\exp\\!\\left(\\Delta u_{i} - \\widehat{\\Delta f} + C \\right)} - \\sum_{j=1}^{N_{\\mathrm{B}}} \\frac{1}{1 + \\exp\\!\\left(\\Delta u'_{j} + \\widehat{\\Delta f} - C \\right)} = 0\n$$\nThe function $G(\\widehat{\\Delta f})$ is strictly monotonic. Its first derivative with respect to $\\widehat{\\Delta f}$ is:\n$$\n\\frac{dG}{d\\widehat{\\Delta f}} = \\sum_{i=1}^{N_{\\mathrm{A}}} \\frac{\\exp(\\dots)}{(1+\\exp(\\dots))^2} + \\sum_{j=1}^{N_{\\mathrm{B}}} \\frac{\\exp(\\dots)}{(1+\\exp(\\dots))^2}\n$$\nwhere the arguments of the exponentials are the same as in the definition of $G(\\widehat{\\Delta f})$. Since this derivative is a sum of strictly positive terms, $G(\\widehat{\\Delta f})$ is a strictly increasing function of $\\widehat{\\Delta f}$. This property, along with the fact that $G(\\widehat{\\Delta f})$ asymptotes to $-N_{\\mathrm{B}}$ as $\\widehat{\\Delta f} \\to -\\infty$ and to $N_{\\mathrm{A}}$ as $\\widehat{\\Delta f} \\to +\\infty$, guarantees a unique root. This root will be found numerically using a robust algorithm, specifically the Brent-Dekker method (`brentq` from the SciPy library), which is efficient and reliable for a bracketed, monotonic function.\n\nFor each test case, the following steps are executed programmatically:\n1.  Set the random number generator seed.\n2.  Generate $N_{\\mathrm{A}}$ samples from $N(\\mu_{\\mathrm{A}}, 1/k_{\\mathrm{A}})$ and $N_{\\mathrm{B}}$ samples from $N(\\mu_{\\mathrm{B}}, 1/k_{\\mathrm{B}})$.\n3.  Compute the arrays of $\\Delta u_i$ and $\\Delta u'_j$.\n4.  Numerically solve $G(\\widehat{\\Delta f}) = 0$ for $\\widehat{\\Delta f}$.\n5.  Compute $\\Delta F_{\\mathrm{exact}} = \\frac{1}{2} \\ln(k_{\\mathrm{B}}/k_{\\mathrm{A}})$.\n6.  Calculate the bias $b = \\widehat{\\Delta f} - \\Delta F_{\\mathrm{exact}}$.\n7.  The final result for each case is rounded to $6$ decimal places as required. The full set of results is then formatted into the specified list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes the bias of the Bennett Acceptance Ratio (BAR) estimator for a series of test cases\n    involving one-dimensional harmonic oscillators.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (kA, muA, kB, muB, NA, NB, seed)\n        (1.0, 0.0, 2.0, 0.0, 8, 8, 12345),\n        (1.0, 0.0, 1.0, 3.0, 5, 5, 54321),\n        (0.5, -1.0, 5.0, 1.0, 4, 4, 2024),\n        (1.0, 0.0, 3.0, 0.0, 2, 9, 777),\n        (1.0, 0.0, 1.0, 0.0, 1, 1, 999)\n    ]\n\n    results = []\n    for case in test_cases:\n        kA, muA, kB, muB, NA, NB, seed = case\n\n        # Set up random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate samples from equilibrium distributions\n        # State A: Gaussian with mean muA, variance 1/kA\n        std_dev_A = np.sqrt(1.0 / kA)\n        samples_A = rng.normal(loc=muA, scale=std_dev_A, size=NA)\n\n        # State B: Gaussian with mean muB, variance 1/kB\n        std_dev_B = np.sqrt(1.0 / kB)\n        samples_B = rng.normal(loc=muB, scale=std_dev_B, size=NB)\n\n        # Define reduced potential energy functions\n        def uA(x):\n            return 0.5 * kA * (x - muA)**2\n        \n        def uB(x):\n            return 0.5 * kB * (x - muB)**2\n\n        # Calculate energy differences (work values)\n        delta_u_fwd = uB(samples_A) - uA(samples_A)\n        delta_u_rev = uA(samples_B) - uB(samples_B)\n        \n        # Constant term in the BAR equation exponent\n        log_N_ratio = np.log(NA / NB)\n\n        # Define the function whose root is the BAR estimate df_hat\n        def bar_equation(df_hat):\n            \"\"\"\n            Represents the BAR equation in the form G(df_hat) = 0.\n            \"\"\"\n            # Term for forward samples (A -> B)\n            arg_fwd = delta_u_fwd - df_hat + log_N_ratio\n            sum_fwd = np.sum(1.0 / (1.0 + np.exp(arg_fwd)))\n            \n            # Term for reverse samples (B -> A)\n            arg_rev = delta_u_rev + df_hat - log_N_ratio\n            sum_rev = np.sum(1.0 / (1.0 + np.exp(arg_rev)))\n            \n            return sum_fwd - sum_rev\n\n        # Solve for df_hat using a numerical root finder.\n        # A wide bracket [-100, 100] is safe as the function is monotonic\n        # and crosses zero within this range for any reasonable inputs.\n        try:\n            df_hat = brentq(bar_equation, -100.0, 100.0)\n        except ValueError:\n            # This should not be reached due to the function's monotonic properties.\n            # Included for robustness in principle.\n            df_hat = np.nan\n\n        # Calculate exact free energy difference\n        df_exact = 0.5 * np.log(kB / kA)\n\n        # Calculate the bias\n        bias = df_hat - df_exact\n        \n        # Round to 6 decimal places and append to results list\n        results.append(round(bias, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}