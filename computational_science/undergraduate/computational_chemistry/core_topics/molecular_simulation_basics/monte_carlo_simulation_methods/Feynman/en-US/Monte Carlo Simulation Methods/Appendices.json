{
    "hands_on_practices": [
        {
            "introduction": "The Metropolis algorithm is the cornerstone of Monte Carlo simulations in statistical mechanics. This first practice provides a hands-on opportunity to implement this algorithm from scratch in a clear and intuitive setting . By simulating the arrangement of books on a shelf based on a simple \"tidiness\" energy function, you can focus purely on the core logic—proposing a new state, calculating the change in energy $\\Delta E$, and accepting or rejecting the move based on the Metropolis criterion—without the added complexity of a physical system. Mastering this fundamental loop is the first step toward applying Monte Carlo methods to more complex scientific problems.",
            "id": "2458836",
            "problem": "You are given a finite set of discrete objects called books, indexed by an integer set $\\{1,2,\\dots,N\\}$. Each book $i$ has two attributes: a positive real width $w_i$ and a title string $s_i$. For any arrangement of the books represented by a permutation $\\pi$ of $\\{1,2,\\dots,N\\}$, define the following quantities.\n\n1. Define the alphabetical rank $r_i$ of each title $s_i$ by sorting the titles in lexicographic order and assigning $r_i \\in \\{1,2,\\dots,N\\}$ according to that sorted order (rank $1$ is the alphabetically smallest). In all test cases below, all titles are distinct, so the ranks are well-defined.\n\n2. Define the size-mismatch cost\n$$\nM(\\pi) \\;=\\; \\sum_{k=1}^{N-1} \\left|\\, w_{\\pi_{k+1}} - w_{\\pi_{k}} \\,\\right|.\n$$\n\n3. Define the non-alphabetical cost as the inversion count with respect to the ranks\n$$\nI(\\pi) \\;=\\; \\left|\\left\\{\\, (k,\\ell) \\,\\middle|\\, 1 \\le k  \\ell \\le N,\\; r_{\\pi_k}  r_{\\pi_\\ell} \\,\\right\\}\\right|.\n$$\n\nGiven positive weights $a0$ and $b0$, define the total energy\n$$\nE(\\pi) \\;=\\; a \\, M(\\pi) \\;+\\; b \\, I(\\pi).\n$$\n\nConsider the canonical equilibrium distribution over permutations at absolute temperature $T0$ with Boltzmann constant $k_\\mathrm{B}=1$,\n$$\n\\mathbb{P}(\\pi) \\;\\propto\\; \\exp\\!\\big(\\,-E(\\pi)/T\\,\\big).\n$$\n\nA discrete-time Markov chain on the permutation space is defined as follows. Initialize at time step $t=0$ with the identity arrangement $\\pi^{(0)}$, which places the books in the original listed order. For each step $t=1,2,\\dots,M$, choose two distinct positions $u,v \\in \\{1,2,\\dots,N\\}$ uniformly at random, let $\\tilde{\\pi}$ be the permutation obtained from $\\pi^{(t-1)}$ by swapping the entries at positions $u$ and $v$, compute $\\Delta E = E(\\tilde{\\pi}) - E(\\pi^{(t-1)})$, and set\n$$\n\\pi^{(t)} \\;=\\; \\begin{cases}\n\\tilde{\\pi},  \\text{with probability } \\min\\left\\{\\,1,\\; \\exp\\!\\big(-\\Delta E/T\\big)\\,\\right\\},\\\\\n\\pi^{(t-1)},  \\text{otherwise}.\n\\end{cases}\n$$\n\nAll pseudo-random choices must be made using a pseudo-random number generator that is initialized at the beginning of each test case with the specified integer seed $s$, so that the entire trajectory is uniquely determined by the parameters. Let\n$$\nE_{\\min} \\;=\\; \\min\\{\\, E(\\pi^{(t)}) \\,\\mid\\, t = 0,1,\\dots,M \\,\\}\n$$\nbe the lowest energy encountered along the trajectory, including the initial state.\n\nTask: For each test case below, compute $E_{\\min}$ and report it as a real number rounded to six decimal places.\n\nTest suite (each case is a tuple $( (w_1,\\dots,w_N), (s_1,\\dots,s_N), a, b, T, M, s )$):\n\n- Case 1 (general case): `([1.0, 1.5, 0.9, 2.0, 1.2, 1.8], [\"C\", \"A\", \"F\", \"B\", \"D\", \"E\"], 1.0, 0.5, 0.5, 10000, 314159)`.\n\n- Case 2 (boundary: single book): `([1.0], [\"A\"], 2.0, 1.0, 1.0, 10, 7)`.\n\n- Case 3 (two books, reversed alphabetical order): `([1.0, 2.0], [\"B\", \"A\"], 0.1, 5.0, 0.1, 100, 999)`.\n\n- Case 4 (edge: equal sizes, reverse alphabetical starting order): `([1.0, 1.0, 1.0, 1.0], [\"d\", \"c\", \"b\", \"a\"], 2.0, 1.0, 2.0, 5000, 2024)`.\n\nFinal output format: Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, with each entry rounded to six decimal places and without additional whitespace, for example `[$x_1$,$x_2$,$x_3$,$x_4$]` where each $x_i$ is a real number with exactly six digits after the decimal point.",
            "solution": "The problem presented is a well-defined computational task in the domain of statistical mechanics, specifically requiring the implementation of a discrete-state, discrete-time Markov Chain Monte Carlo (MCMC) simulation. The algorithm described is the canonical Metropolis-Hastings algorithm, used here to explore the configuration space of permutations of a set of objects, referred to as books. The objective is to find the minimum energy encountered over a finite simulation trajectory.\n\nBefore proceeding, a validation of the problem statement is required.\n\n**Step 1: Extracted Givens**\n- A finite set of objects (books) is indexed by the set {$1,2,\\dots,N$}.\n- Each book $i$ has a positive real width $w_i > 0$ and a unique title string $s_i$.\n- A permutation of the books is denoted by $\\pi$, a permutation of {$1,2,\\dots,N$}. $\\pi_k$ is the index of the book at position $k$.\n- Alphabetical rank $r_i$ is the rank of title $s_i$ in the lexicographically sorted list of all titles, with $r_i \\in \\{1,2,\\dots,N\\}$.\n- Size-mismatch cost: $M(\\pi) = \\sum_{k=1}^{N-1} |w_{\\pi_{k+1}} - w_{\\pi_k}|$.\n- Non-alphabetical cost (inversion count): $I(\\pi) = |\\{ (k,\\ell) | 1 \\le k  \\ell \\le N, r_{\\pi_k} > r_{\\pi_\\ell} \\}|$.\n- Total energy: $E(\\pi) = a M(\\pi) + b I(\\pi)$ for given positive weights $a>0$, $b>0$.\n- Probability distribution: The canonical equilibrium distribution is given by $P(\\pi) \\propto \\exp(-E(\\pi)/T)$, where $T>0$ is the absolute temperature and the Boltzmann constant $k_\\mathrm{B}=1$.\n- Markov Chain Monte Carlo Simulation:\n    - Initial state ($t=0$): $\\pi^{(0)}$ is the identity permutation, representing the original listed order.\n    - Proposal step: At each time step $t \\in \\{1,\\dots,M\\}$, two distinct positions $u,v \\in \\{1,\\dots,N\\}$ are chosen uniformly at random. A proposal permutation $\\tilde{\\pi}$ is generated by swapping the elements at positions $u,v$ in the current permutation $\\pi^{(t-1)}$.\n    - Acceptance step: The proposal is accepted, i.e., $\\pi^{(t)} = \\tilde{\\pi}$, with probability $p_{acc} = \\min\\{1, \\exp(-\\Delta E/T)\\}$, where $\\Delta E = E(\\tilde{\\pi}) - E(\\pi^{(t-1)})$. Otherwise, the state is unchanged, i.e., $\\pi^{(t)} = \\pi^{(t-1)}$.\n- Pseudo-randomness: The pseudo-random number generator is to be seeded with a specific integer $s$.\n- Objective: Compute $E_{\\min} = \\min\\{ E(\\pi^{(t)}) | t = 0,1,\\dots,M \\}$.\n- Test cases provide tuples of $(\\{w_i\\}, \\{s_i\\}, a, b, T, M, s)$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is evaluated against the required criteria.\n- **Scientifically Grounded**: The problem is fundamentally sound. It describes the Metropolis-Hastings algorithm, a cornerstone of computational statistical physics and chemistry, applied to a combinatorial system. The energy function, Boltzmann distribution, and acceptance criterion are all standard and correctly formulated.\n- **Well-Posed**: The problem is well-posed. For a given set of parameters and a specified pseudo-random number generator seed, the entire trajectory of the system is deterministic. Consequently, the minimum energy encountered, $E_{\\min}$, is a uniquely defined and computable quantity.\n- **Objective**: The problem is stated using precise and unambiguous mathematical language, free from any subjective or opinion-based claims.\n\nAll aspects of the problem (definitions, parameters, algorithm) are specified completely and consistently. There are no scientific flaws, ambiguities, or contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be constructed.\n\n**Principle-Based Solution Design**\n\nThe core of the task is to implement the specified MCMC simulation. The state of our system is the permutation of books. For computational purposes, we represent a permutation as an array of original book indices. Let this array be $p$, of length $N$. The element $p[k]$ (using 0-based indexing) is the original index of the book at position $k+1$.\n\n1.  **Pre-computation of Ranks**: The alphabetical ranks $\\{r_i\\}$ do not change during the simulation. They can be computed once at the beginning. We associate each original book index $i \\in \\{0, \\dots, N-1\\}$ with its title $s_i$, sort these pairs based on the titles, and then assign ranks from $1$ to $N$. The result is stored in an array where the $i$-th element is the rank of the book with original index $i$.\n\n2.  **Energy Calculation**: A function is required to compute the total energy $E(\\pi)$ for any given permutation $p$.\n    - The size-mismatch cost $M(\\pi)$ is calculated as the sum of absolute differences in widths of adjacent books in the permutation:\n    $$\n    M(p) = \\sum_{k=0}^{N-2} |w_{p[k+1]} - w_{p[k]}|\n    $$\n    This is an $O(N)$ computation.\n    - The non-alphabetical cost $I(\\pi)$ is the number of inversions in the sequence of ranks corresponding to the permutation. For a permutation $p$, the sequence of ranks is $(r_{p[0]}, r_{p[1]}, \\dots, r_{p[N-1]})$. The inversion count is:\n    $$\n    I(p) = |\\{ (k,\\ell) | 0 \\le k  \\ell \\le N-1, r_{p[k]} > r_{p[\\ell]} \\}|\n    $$\n    A direct, double-loop implementation for counting inversions has a time complexity of $O(N^2)$. Given that the maximum value of $N$ in the test suite is small (e.g., $N=6$), this complexity is entirely acceptable and its implementation is straightforward, which minimizes the risk of logical error. For larger $N$, an $O(N \\log N)$ algorithm based on merge sort would be preferable for calculating $I(\\pi)$, or an $O(N)$ update for $\\Delta I$ during swaps. Here, recalculation is sufficient.\n    - The total energy is then $E(p) = a M(p) + b I(p)$.\n\n3.  **Simulation Trajectory**:\n    - **Initialization**: We begin with the seed $s$ for the pseudo-random number generator to ensure reproducibility. The initial permutation $p^{(0)}$ is the identity, $p^{(0)} = [0, 1, \\dots, N-1]$. The initial energy $E^{(0)} = E(p^{(0)})$ is calculated, and the minimum energy seen so far is initialized as $E_{\\min} = E^{(0)}$.\n    - **Iteration**: The simulation proceeds for $M$ steps. In each step $t = 1, \\dots, M$:\n        a. The current state is $(\\pi^{(t-1)}, E^{(t-1)})$.\n        b. Two distinct positions, say $u$ and $v$ with $0 \\le u, v  N$, $u \\neq v$, are selected uniformly at random.\n        c. A proposal permutation $\\tilde{p}$ is created by swapping the elements at indices $u$ and $v$ in the current permutation $p^{(t-1)}$.\n        d. The energy of the proposal state, $\\tilde{E} = E(\\tilde{p})$, is computed from scratch, as discussed above. The change in energy is $\\Delta E = \\tilde{E} - E^{(t-1)}$.\n        e. The Metropolis criterion is applied: a random variate $z$ is drawn from the uniform distribution $U(0,1)$. If $z  \\exp(-\\Delta E / T)$, the proposal is accepted. This single condition correctly handles both energy-lowering ($\\Delta E \\le 0$, where $\\exp(-\\Delta E/T) \\ge 1$) and energy-raising ($\\Delta E > 0$) moves.\n        f. If the move is accepted, the new state is $(\\pi^{(t)}, E^{(t)}) = (\\tilde{p}, \\tilde{E})$. If rejected, the state remains unchanged: $(\\pi^{(t)}, E^{(t)}) = (\\pi^{(t-1)}, E^{(t-1)})$.\n        g. The minimum energy is updated: $E_{\\min} = \\min(E_{\\min}, E^{(t)})$.\n\n4.  **Special Cases**:\n    - For $N=1$, the permutation space contains only one element. No swaps are possible. The size-mismatch cost $M(\\pi)$ and inversion count $I(\\pi)$ are both definitionally $0$. Therefore, $E(\\pi) = 0$ and $E_{\\min}$ must be $0$.\n    - If all widths $\\{w_i\\}$ are equal, then $M(\\pi) = 0$ for all permutations $\\pi$. The energy simplifies to $E(\\pi) = b I(\\pi)$, and the simulation's objective becomes solely to minimize the number of alphabetical inversions.\n\nThis structured approach ensures a correct and robust implementation that directly follows the problem's formal specification.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the book arrangement problem using a Metropolis Monte Carlo simulation\n    for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: General case\n        (\n            [1.0, 1.5, 0.9, 2.0, 1.2, 1.8],\n            [\"C\", \"A\", \"F\", \"B\", \"D\", \"E\"],\n            1.0, 0.5, 0.5, 10000, 314159\n        ),\n        # Case 2: Boundary - single book\n        (\n            [1.0],\n            [\"A\"],\n            2.0, 1.0, 1.0, 10, 7\n        ),\n        # Case 3: Two books, reversed alphabetical order\n        (\n            [1.0, 2.0],\n            [\"B\", \"A\"],\n            0.1, 5.0, 0.1, 100, 999\n        ),\n        # Case 4: Edge - equal sizes, reverse alphabetical starting order\n        (\n            [1.0, 1.0, 1.0, 1.0],\n            [\"d\", \"c\", \"b\", \"a\"],\n            2.0, 1.0, 2.0, 5000, 2024\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        widths, titles, a, b, T, M, seed = case\n        N = len(widths)\n        \n        # Consistent RNG for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Handle trivial case N = 1\n        if N = 1:\n            results.append(0.0)\n            continue\n\n        # --- Pre-computation ---\n        # 1. Convert to numpy arrays for efficiency\n        widths_np = np.array(widths, dtype=float)\n        \n        # 2. Determine alphabetical ranks (1-based)\n        # Pair original indices with titles, sort, and extract ranks\n        indexed_titles = sorted(enumerate(titles), key=lambda x: x[1])\n        ranks = np.zeros(N, dtype=int)\n        for rank, (original_index, _) in enumerate(indexed_titles, 1):\n            ranks[original_index] = rank\n\n        # --- Energy Calculation Function ---\n        def calculate_energy(p, widths, ranks, a, b):\n            \"\"\"Calculates the total energy for a given permutation p.\"\"\"\n            p_ranks = ranks[p]\n            p_widths = widths[p]\n            \n            # M(pi): Size-mismatch cost\n            m_cost = np.sum(np.abs(p_widths[1:] - p_widths[:-1]))\n            \n            # I(pi): Non-alphabetical cost (inversion count)\n            # A simple O(N^2) implementation is sufficient for small N.\n            i_cost = 0\n            for i in range(N):\n                for j in range(i + 1, N):\n                    if p_ranks[i]  p_ranks[j]:\n                        i_cost += 1\n            \n            return a * m_cost + b * i_cost\n\n        # --- MCMC Simulation ---\n        # Initial state (identity permutation)\n        current_p = np.arange(N)\n        current_energy = calculate_energy(current_p, widths_np, ranks, a, b)\n        min_energy = current_energy\n\n        # Main simulation loop\n        for _ in range(M):\n            # Propose a move: swap two distinct elements\n            # rng.choice is efficient and ensures u != v\n            u, v = rng.choice(N, size=2, replace=False)\n            \n            proposal_p = current_p.copy()\n            proposal_p[u], proposal_p[v] = proposal_p[v], proposal_p[u]\n            \n            proposal_energy = calculate_energy(proposal_p, widths_np, ranks, a, b)\n            \n            delta_E = proposal_energy - current_energy\n            \n            # Metropolis-Hastings acceptance criterion\n            if delta_E = 0 or rng.random()  np.exp(-delta_E / T):\n                current_p = proposal_p\n                current_energy = proposal_energy\n            \n            # Update minimum energy found\n            if current_energy  min_energy:\n                min_energy = current_energy\n        \n        results.append(min_energy)\n\n    # Format output as specified\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A correct simulation must sample states according to the desired probability distribution, which requires satisfying the detailed balance condition. This exercise  moves beyond simple proposals and delves into a crucial subtlety: simulations in non-Cartesian coordinate systems. When proposing moves in coordinates like spherical angles $(\\theta, \\phi)$, a seemingly symmetric proposal in angle space is not symmetric with respect to the physical surface area. This practice guides you to derive the Jacobian factor needed to correct the acceptance probability $\\alpha$, ensuring you are correctly sampling a uniform distribution on a sphere. Comparing a simulation with and without the correct Jacobian will powerfully demonstrate why a rigorous understanding of the underlying measure is essential for accurate results.",
            "id": "2458841",
            "problem": "A single particle is constrained to move on the surface of a sphere of radius $R$ in three-dimensional Euclidean space. The surface is parametrized by spherical coordinates $(\\theta,\\phi)$, where $\\theta \\in [0,\\pi]$ is the polar angle and $\\phi \\in [0,2\\pi)$ is the azimuthal angle. The equilibrium (target) distribution of the particle is uniform with respect to the surface-area measure on the sphere. A proposal in angle space is constructed by adding independent zero-mean increments $\\Delta \\theta$ and $\\Delta \\phi$ to $(\\theta,\\phi)$; the perturbed angles are then mapped back to the canonical ranges by reflecting $\\theta$ at the boundaries $0$ and $\\pi$ and by wrapping $\\phi$ modulo $2\\pi$. The proposal increments $\\Delta \\theta$ and $\\Delta \\phi$ have Gaussian distributions with prescribed standard deviations. The resulting Markov chain must leave the uniform surface-area distribution invariant. The change-of-variables between $(\\theta,\\phi)$ and surface area induces a Jacobian that affects the proposal density on the surface when the kernel is specified in the angular coordinates.\n\nYour tasks are:\n\n- Using only first principles, determine the Jacobian determinant $J(R,\\theta)$ of the transformation from angular coordinates $(\\theta,\\phi)$ to the surface-area element on the sphere of radius $R$. State your result in terms of $R$ and $\\theta$.\n\n- For symmetric angular perturbations (identical forward and reverse distributions in angle space), determine from first principles the multiplicative factor in the acceptance ratio that arises purely from the proposal-density transformation between $(\\theta,\\phi)$ and the surface-area measure, expressed in terms of $\\theta$ and $\\theta'$ for a move $(\\theta,\\phi) \\to (\\theta',\\phi')$.\n\n- Implement a Monte Carlo (MC) simulation of the particle on the sphere using the above angular proposals. Construct two variants:\n  1. A variant that uses the correct proposal-density transformation implied by your Jacobian in its acceptance decision so that the chain is invariant for the uniform surface-area distribution.\n  2. A variant that incorrectly ignores this transformation in its acceptance decision.\n\n- For each variant, estimate the expectation of the observable $f(\\theta,\\phi) = \\cos^2(\\theta)$ under the chain’s stationary distribution, using a fixed seed for reproducibility.\n\nAngles must be in radians. All numerical answers must be expressed as real numbers without units.\n\nTest suite and required outputs:\n\n1. Evaluate the Jacobian determinant $J(R,\\theta)$ at the following parameter pairs:\n   - $(R,\\theta) = (1,\\pi/6)$,\n   - $(R,\\theta) = (2,\\pi/2)$,\n   - $(R,\\theta) = (3,\\pi)$.\n\n2. For symmetric angular perturbations, evaluate the proposal-density Jacobian ratio factor for the following $(\\theta,\\theta')$ pairs:\n   - $(\\theta,\\theta') = (\\pi/12,\\pi/3)$,\n   - $(\\theta,\\theta') = (\\pi/3,5\\pi/12)$.\n\n3. Run a simulation for $N = 200000$ total steps with burn-in $B = 5000$, radius $R = 1$, starting angles $(\\theta_0,\\phi_0) = (1.234,2.345)$, and Gaussian proposal standard deviations $\\sigma_\\theta = 0.3$ and $\\sigma_\\phi = 0.6$. Use a fixed seed equal to $123$. Produce two estimates for $\\mathbb{E}[\\cos^2(\\theta)]$:\n   - One using the acceptance decision that correctly accounts for the proposal-density transformation due to your Jacobian.\n   - One using an acceptance decision that ignores the proposal-density transformation.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as follows:\n  - The three Jacobian values from item $1$ in the order given.\n  - The two proposal-density Jacobian ratio factors from item $2$ in the order given.\n  - The two Monte Carlo estimates from item $3$ in the order given (first the correct-variant estimate, then the incorrect-variant estimate).\n  For example, the output must have the form `[J1,J2,J3,F1,F2,E_correct,E_wrong]`.",
            "solution": "The problem presented is a well-posed and scientifically grounded exercise in computational statistical mechanics, specifically concerning the application of the Metropolis-Hastings algorithm to a non-Cartesian coordinate system. A rigorous validation of the problem statement finds no inconsistencies, ambiguities, or violations of scientific principles. I will therefore proceed with a complete solution derived from first principles.\n\nThe solution is presented in three parts, corresponding to the tasks outlined in the problem statement.\n\nFirst, we determine the Jacobian determinant of the transformation from spherical angular coordinates to the surface-area element on a sphere. A point on the surface of a sphere of radius $R$ is parametrized in Cartesian coordinates $(x, y, z)$ using spherical coordinates $(\\theta, \\phi)$ as:\n$$\n\\vec{r}(\\theta, \\phi) = (R \\sin\\theta \\cos\\phi, R \\sin\\theta \\sin\\phi, R \\cos\\theta)\n$$\nwhere $\\theta \\in [0, \\pi]$ is the polar angle and $\\phi \\in [0, 2\\pi)$ is the azimuthal angle. An infinitesimal surface-area element, $dA$, is given by the magnitude of the cross product of the tangent vectors $\\frac{\\partial\\vec{r}}{\\partial\\theta}$ and $\\frac{\\partial\\vec{r}}{\\partial\\phi}$.\n\nThe partial derivatives are:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\theta} = (R \\cos\\theta \\cos\\phi, R \\cos\\theta \\sin\\phi, -R \\sin\\theta)\n$$\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\phi} = (-R \\sin\\theta \\sin\\phi, R \\sin\\theta \\cos\\phi, 0)\n$$\nThe cross product is:\n$$\n\\frac{\\partial\\vec{r}}{\\partial\\theta} \\times \\frac{\\partial\\vec{r}}{\\partial\\phi} = (R^2 \\sin^2\\theta \\cos\\phi, R^2 \\sin^2\\theta \\sin\\phi, R^2 \\sin\\theta \\cos\\theta)\n$$\nThe magnitude of this vector, which represents the Jacobian of the transformation, is:\n$$\nJ(R, \\theta) = \\left\\| \\frac{\\partial\\vec{r}}{\\partial\\theta} \\times \\frac{\\partial\\vec{r}}{\\partial\\phi} \\right\\| = \\sqrt{(R^2 \\sin^2\\theta \\cos\\phi)^2 + (R^2 \\sin^2\\theta \\sin\\phi)^2 + (R^2 \\sin\\theta \\cos\\theta)^2}\n$$\n$$\nJ(R, \\theta) = \\sqrt{R^4 \\sin^4\\theta (\\cos^2\\phi + \\sin^2\\phi) + R^4 \\sin^2\\theta \\cos^2\\theta} = \\sqrt{R^4 \\sin^2\\theta (\\sin^2\\theta + \\cos^2\\theta)} = \\sqrt{R^4 \\sin^2\\theta}\n$$\nSince $\\theta \\in [0, \\pi]$, $\\sin\\theta \\ge 0$. Therefore, the Jacobian determinant is:\n$$\nJ(R, \\theta) = R^2 \\sin\\theta\n$$\nThis quantity relates the differential area element $dA$ on the sphere to the differential increments in the angular coordinates: $dA = J(R, \\theta) d\\theta d\\phi = R^2 \\sin\\theta d\\theta d\\phi$.\n\nSecond, we determine the factor in the Metropolis-Hastings acceptance ratio that arises from the coordinate transformation. The acceptance probability $\\alpha$ for a move from a state $s$ to a proposed state $s'$ is given by:\n$$\n\\alpha(s \\to s') = \\min\\left(1, \\frac{\\pi(s')}{\\pi(s)} \\frac{g(s' \\to s)}{g(s \\to s')}\\right)\n$$\nHere, $\\pi(s)$ is the target probability density and $g(s \\to s')$ is the proposal probability density from $s$ to $s'$. The states $s$ and $s'$ are points on the sphere. The target distribution is uniform with respect to the surface-area measure, which implies $\\pi(s) = \\text{constant}$ for all points $s$ on the sphere. Consequently, the ratio of target densities $\\frac{\\pi(s')}{\\pi(s)} = 1$.\n\nThe proposal is made in angular coordinates $(\\theta, \\phi)$, not directly on the surface. Let $g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))$ be the proposal density in the angular coordinate space. The corresponding proposal density on the sphere's surface, $g(s \\to s')$, must be defined with respect to the surface-area measure $dA$. The probability conservation requires $g(s \\to s') dA' = g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi')) d\\theta' d\\phi'$. Using $dA' = J(\\theta') d\\theta' d\\phi'$, we find:\n$$\ng(s \\to s') = \\frac{g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))}{J(\\theta')}\n$$\nSimilarly, for the reverse move:\n$$\ng(s' \\to s) = \\frac{g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi))}{J(\\theta)}\n$$\nThe ratio of proposal densities in the acceptance probability is therefore:\n$$\n\\frac{g(s' \\to s)}{g(s \\to s')} = \\frac{g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi))}{g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))} \\times \\frac{J(\\theta')}{J(\\theta)}\n$$\nThe problem states that the angular perturbations are symmetric, meaning the forward and reverse proposals in angle space have identical distributions. This implies $g_{ang}((\\theta',\\phi') \\to (\\theta,\\phi)) = g_{ang}((\\theta,\\phi) \\to (\\theta',\\phi'))$. The ratio thus simplifies to the ratio of the Jacobians:\n$$\n\\frac{g(s' \\to s)}{g(s \\to s')} = \\frac{J(\\theta')}{J(\\theta)} = \\frac{R^2 \\sin\\theta'}{R^2 \\sin\\theta} = \\frac{\\sin\\theta'}{\\sin\\theta}\n$$\nThis is the required multiplicative factor. The correct acceptance probability is $\\alpha = \\min\\left(1, \\frac{\\sin\\theta'}{\\sin\\theta}\\right)$.\n\nThird, we design the Monte Carlo simulation. The goal is to estimate the expectation value of the observable $f(\\theta, \\phi) = \\cos^2(\\theta)$ for the uniform distribution on the sphere. The theoretical expectation is:\n$$\n\\mathbb{E}[\\cos^2\\theta] = \\frac{\\int_0^{2\\pi} \\int_0^\\pi \\cos^2\\theta \\sin\\theta \\,d\\theta d\\phi}{\\int_0^{2\\pi} \\int_0^\\pi \\sin\\theta \\,d\\theta d\\phi} = \\frac{2\\pi \\int_0^\\pi \\cos^2\\theta \\sin\\theta \\,d\\theta}{4\\pi} = \\frac{1}{2}\\left[-\\frac{\\cos^3\\theta}{3}\\right]_0^\\pi = \\frac{1}{2}\\left(-\\frac{(-1)^3}{3} - \\left(-\\frac{1^3}{3}\\right)\\right) = \\frac{1}{3}\n$$\nThe simulation is implemented as follows:\n1.  Initialize the state $(\\theta_k, \\phi_k)$ at $k=0$ to $(\\theta_0, \\phi_0)$.\n2.  Iterate for $k = 0, \\dots, N-1$:\n    a. Propose a new state $(\\theta_p, \\phi_p)$ by drawing independent increments $\\Delta\\theta$ and $\\Delta\\phi$ from Gaussian distributions $\\mathcal{N}(0, \\sigma_\\theta^2)$ and $\\mathcal{N}(0, \\sigma_\\phi^2)$, respectively.\n    b. Apply boundary conditions. The new polar angle $\\theta_p$ is obtained by reflecting $\\theta_k + \\Delta\\theta$ at the boundaries $0$ and $\\pi$. This is achieved by the transformation $\\theta_p = \\text{mod}(\\theta_k+\\Delta\\theta, 2\\pi)$ followed by $\\theta_p = 2\\pi - \\theta_p$ if $\\theta_p  \\pi$. The new azimuthal angle $\\phi_p$ is obtained by wrapping $\\phi_k + \\Delta\\phi$ modulo $2\\pi$.\n    c. Calculate the acceptance probability $\\alpha$.\n       - **Correct variant**: $\\alpha = \\min\\left(1, \\frac{\\sin\\theta_p}{\\sin\\theta_k}\\right)$.\n       - **Incorrect variant**: The Jacobian factor is ignored. The acceptance probability becomes $\\alpha = \\min(1, 1) = 1$, meaning all moves are accepted.\n    d. Draw a random number $u \\sim U(0,1)$. If $u  \\alpha$, set $(\\theta_{k+1}, \\phi_{k+1}) = (\\theta_p, \\phi_p)$. Otherwise, $(\\theta_{k+1}, \\phi_{k+1}) = (\\theta_k, \\phi_k)$.\n3.  After a burn-in period of $B$ steps, the expectation $\\mathbb{E}[\\cos^2\\theta]$ is estimated by averaging $\\cos^2(\\theta_k)$ over the remaining $N-B$ steps.\n\nThe incorrect variant samples a probability density that is uniform in $(\\theta, \\phi)$ space, i.e., $p(\\theta, \\phi) \\propto 1$. The expected value under this incorrect distribution is $\\mathbb{E}_{incorrect}[\\cos^2\\theta] = \\frac{1}{2\\pi^2} \\int_0^{2\\pi} d\\phi \\int_0^\\pi \\cos^2\\theta \\,d\\theta = \\frac{1}{\\pi} \\int_0^\\pi \\frac{1+\\cos(2\\theta)}{2}d\\theta = \\frac{1}{2\\pi}[\\theta + \\frac{\\sin(2\\theta)}{2}]_0^\\pi = \\frac{1}{2}$. The simulation results should conform to these theoretical predictions of $1/3$ and $1/2$. The implementation will follow this design precisely.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_mc_simulation(params, correct_jacobian, seed):\n    \"\"\"\n    Runs a Monte Carlo simulation of a particle on a sphere.\n\n    Args:\n        params (tuple): A tuple containing simulation parameters:\n                        (N, B, R, theta0, phi0, sigma_theta, sigma_phi).\n        correct_jacobian (bool): If True, use the correct acceptance criterion.\n                                 If False, use the incorrect one.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The estimated expectation value of cos^2(theta).\n    \"\"\"\n    N, B, R, theta0, phi0, sigma_theta, sigma_phi = params\n    \n    # Initialize a new random number generator for each independent run\n    rng = np.random.default_rng(seed)\n    \n    theta = theta0\n    phi = phi0\n    \n    observable_sum = 0.0\n    samples_collected = 0\n    \n    for step in range(N):\n        # Propose a move in angular coordinates\n        d_theta = rng.normal(0.0, sigma_theta)\n        d_phi = rng.normal(0.0, sigma_phi)\n        \n        theta_prop = theta + d_theta\n        phi_prop = phi + d_phi\n        \n        # Apply boundary conditions\n        # For theta: reflection at 0 and pi\n        # This maps the real line to [0, pi] via folding\n        theta_p = np.mod(theta_prop, 2.0 * np.pi)\n        if theta_p  np.pi:\n            theta_p = 2.0 * np.pi - theta_p\n            \n        # For phi: wrapping modulo 2*pi\n        phi_p = np.mod(phi_prop, 2.0 * np.pi)\n        \n        # Calculate acceptance probability\n        if correct_jacobian:\n            # The target distribution is uniform on the sphere, so pi(s')/pi(s) = 1.\n            # The acceptance probability is determined by the Jacobian factor.\n            sin_theta_k = np.sin(theta)\n            sin_theta_p = np.sin(theta_p)\n            \n            # To avoid division by zero if theta is at a pole (0 or pi).\n            if sin_theta_k  1e-12:\n                # If moving from a pole, the volume element is increasing from zero,\n                # so the move should always be accepted unless the proposed\n                # point is also a pole, in which case the ratio is 1.\n                acceptance_ratio = 1.0 if sin_theta_p  1e-12 else np.inf\n            else:\n                acceptance_ratio = sin_theta_p / sin_theta_k\n            \n            alpha = min(1.0, acceptance_ratio)\n        else:\n            # Incorrect variant: ignore the Jacobian factor.\n            # Since the target density is uniform, the acceptance probability is 1.\n            alpha = 1.0\n            \n        # Accept or reject the move\n        if rng.uniform(0.0, 1.0)  alpha:\n            theta = theta_p\n            phi = phi_p\n            \n        # Collect samples after the burn-in period\n        if step = B:\n            observable_sum += np.cos(theta)**2\n            samples_collected += 1\n            \n    if samples_collected == 0:\n        return np.nan\n        \n    return observable_sum / samples_collected\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, calculate all required values,\n    and print the final output in the specified format.\n    \"\"\"\n    # ====== Task 1: Evaluate Jacobian determinant J(R, theta) ======\n    # J(R, theta) = R^2 * sin(theta)\n    \n    # Test case 1: (R, theta) = (1, pi/6)\n    R1, theta1 = 1.0, np.pi/6.0\n    J1 = R1**2 * np.sin(theta1)\n    \n    # Test case 2: (R, theta) = (2, pi/2)\n    R2, theta2 = 2.0, np.pi/2.0\n    J2 = R2**2 * np.sin(theta2)\n    \n    # Test case 3: (R, theta) = (3, pi)\n    R3, theta3 = 3.0, np.pi\n    J3 = R3**2 * np.sin(theta3)\n    \n    # ====== Task 2: Evaluate proposal-density Jacobian ratio factor ======\n    # Factor = sin(theta') / sin(theta)\n    \n    # Test case 1: (theta, theta') = (pi/12, pi/3)\n    theta_a1, theta_a2 = np.pi/12.0, np.pi/3.0\n    F1 = np.sin(theta_a2) / np.sin(theta_a1)\n\n    # Test case 2: (theta, theta') = (pi/3, 5*pi/12)\n    theta_b1, theta_b2 = np.pi/3.0, 5.0*np.pi/12.0\n    F2 = np.sin(theta_b2) / np.sin(theta_b1)\n    \n    # ====== Task 3: Run Monte Carlo simulations ======\n    sim_params = (\n        200000,  # N: total steps\n        5000,    # B: burn-in steps\n        1.0,     # R: radius\n        1.234,   # theta0\n        2.345,   # phi0\n        0.3,     # sigma_theta\n        0.6      # sigma_phi\n    )\n    seed = 123\n    \n    # Run simulation with correct Jacobian factor\n    E_correct = run_mc_simulation(sim_params, correct_jacobian=True, seed=seed)\n    \n    # Run simulation with incorrect (ignored) Jacobian factor\n    E_wrong = run_mc_simulation(sim_params, correct_jacobian=False, seed=seed)\n\n    # Collate results\n    results = [J1, J2, J3, F1, F2, E_correct, E_wrong]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many critical processes in chemistry, such as chemical reactions and protein folding, involve crossing high energy barriers—so-called \"rare events.\" Standard Monte Carlo simulations struggle with these problems, as they tend to get trapped in low-energy states, rarely sampling the important transition regions. This exercise  introduces you to the powerful concept of enhanced sampling, specifically a method known as umbrella sampling. You will explore how to add an artificial bias potential $w(x)$ to the true physical potential $U(x)$, making high-energy regions more accessible, and then use the principle of importance reweighting to remove the effect of the bias and recover exact, unbiased averages for the original system, whose distribution is $\\pi(x) \\propto \\exp[-\\beta U(x)]$. This practice provides insight into how we can cleverly manipulate simulations to study problems that would otherwise be computationally intractable.",
            "id": "2458905",
            "problem": "A single classical particle of mass $m$ moves in a one-dimensional potential energy function $U(x)$ at temperature $T$ in the canonical ensemble. The equilibrium target probability density function (PDF) for the position is $\\pi(x)\\propto \\exp[-\\beta U(x)]$, where $\\beta=1/(k_{\\mathrm{B}}T)$ and $k_{\\mathrm{B}}$ is the Boltzmann constant. The potential $U(x)$ has two minima separated by a high barrier at $x^\\ddagger$, with barrier height $\\Delta U$ much larger than $k_{\\mathrm{B}}T$. As a result, straightforward Metropolis Markov chain Monte Carlo (MCMC) with a symmetric proposal $q(x\\to x')=q(x'\\to x)$ rarely samples the barrier region and transition events.\n\nYou are asked to choose a design for a biased Monte Carlo (MC) scheme that (i) substantially increases sampling in the barrier region and (ii) permits exact recovery of unbiased canonical expectations $\\langle A\\rangle_\\pi$ for any observable $A(x)$, without altering the underlying physical potential $U(x)$.\n\nWhich option below specifies a valid scheme under these requirements?\n\nA. Introduce a static bias potential $w(x)$ chosen to stabilize the barrier region (for example, a harmonic umbrella $w(x)=\\tfrac{1}{2}\\,k\\,(x-x_0)^2$ centered near $x^\\ddagger$ with stiffness $k0$), and run MCMC that targets the modified distribution $\\pi_w(x)\\propto \\exp[-\\beta\\,(U(x)+w(x))]$ using a symmetric proposal and the Metropolis acceptance based on $U(x)+w(x)$. Estimate unbiased canonical expectations by importance reweighting with the factor $\\exp[\\beta\\,w(x)]$, that is, compute $\\langle A\\rangle_\\pi$ as the ratio of the reweighted averages of $A(x)$ and $1$ over samples from $\\pi_w$.\n\nB. Keep the unbiased target $\\pi(x)\\propto \\exp[-\\beta U(x)]$ and a symmetric proposal, but modify the Metropolis acceptance to $\\min\\{1,\\exp[-\\beta (U(x')-U(x))]\\,\\exp[\\gamma\\,(x'-x)]\\}$ with a constant $\\gamma0$ to encourage moves that increase $x$ and thus cross the barrier, and then compute plain (unweighted) sample averages to estimate $\\langle A\\rangle_\\pi$.\n\nC. Use an independent proposal distribution $q(x')$ that is supported only in a narrow interval around the barrier top $x^\\ddagger$ so that proposed moves always lie near the barrier; keep the standard Metropolis acceptance for the unbiased target $\\pi(x)$ with no further modifications, and compute plain (unweighted) sample averages to estimate $\\langle A\\rangle_\\pi$.\n\nD. Add a bias potential $w(x)$ and use the Metropolis acceptance based on $U(x)+w(x)$ to enhance visits to the barrier, but compute observables as simple averages over the biased trajectory without any reweighting, arguing that the bias only improves efficiency and does not change equilibrium expectations.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A single classical particle of mass $m$ moves in one dimension.\n- The potential energy function is $U(x)$.\n- The system is in the canonical ensemble at temperature $T$.\n- The equilibrium target probability density function (PDF) is $\\pi(x) \\propto \\exp[-\\beta U(x)]$, where $\\beta = 1/(k_{\\mathrm{B}}T)$ and $k_{\\mathrm{B}}$ is the Boltzmann constant.\n- The potential $U(x)$ has two minima separated by a high barrier at $x^\\ddagger$.\n- The barrier height $\\Delta U$ is much larger than the thermal energy, $\\Delta U \\gg k_{\\mathrm{B}}T$.\n- Standard Metropolis Markov chain Monte Carlo (MCMC) with a symmetric proposal $q(x \\to x') = q(x' \\to x)$ is inefficient for sampling the barrier region.\n- The task is to identify a biased Monte Carlo (MC) scheme that satisfies two conditions:\n    - (i) Substantially increases sampling in the barrier region.\n    - (ii) Permits exact recovery of unbiased canonical expectations $\\langle A \\rangle_\\pi$ for any observable $A(x)$, without altering the physical potential $U(x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem presented is a classic and fundamentally important problem in computational statistical mechanics, specifically in the field of rare event sampling.\n\n- **Scientifically Grounded:** The problem is based on the core principles of statistical mechanics (the canonical ensemble, Boltzmann distribution) and computational chemistry (Monte Carlo simulations, rare events). The scenario of a high-energy barrier impeding sampling is a realistic and common challenge. All concepts are standard and well-established.\n- **Well-Posed:** The problem provides a clear objective with two precise constraints, (i) and (ii), against which proposed solutions can be unambiguously tested. The question asks for a valid scheme, implying that a unique and correct answer exists among the options based on established theory.\n- **Objective:** The language is precise, using standard scientific terminology ($m$, $U(x)$, $T$, $\\pi(x)$, MCMC, etc.). There are no subjective or ambiguous statements in the problem setup.\n\nThe problem is entirely self-contained and consistent. It does not violate any physical laws or mathematical principles. It is a standard theoretical question about the validity of enhanced sampling algorithms.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived by analyzing the provided options based on the principles of statistical mechanics and Monte Carlo theory.\n\nThe objective is to find a method that samples a modified probability distribution $\\pi_b(x)$ that enhances sampling of the barrier region, while allowing for the exact calculation of the true canonical ensemble average,\n$$\n\\langle A \\rangle_\\pi = \\frac{\\int A(x) \\pi(x) dx}{\\int \\pi(x) dx} = \\frac{\\int A(x) \\exp[-\\beta U(x)] dx}{\\int \\exp[-\\beta U(x)] dx}\n$$\nThe core principle for recovering an average over a target distribution $\\pi(x)$ from samples drawn from a biased distribution $\\pi_b(x)$ is importance sampling. The expectation value $\\langle A \\rangle_\\pi$ is recovered by reweighting:\n$$\n\\langle A \\rangle_\\pi = \\frac{\\left\\langle A(x) \\frac{\\pi(x)}{\\pi_b(x)} \\right\\rangle_{\\pi_b}}{\\left\\langle \\frac{\\pi(x)}{\\pi_b(x)} \\right\\rangle_{\\pi_b}}\n$$\nwhere $\\langle \\cdot \\rangle_{\\pi_b}$ denotes an average over samples drawn from $\\pi_b(x)$. In a simulation with $N$ samples $\\{x_i\\}$ from $\\pi_b(x)$, this is estimated as:\n$$\n\\langle A \\rangle_\\pi \\approx \\frac{\\sum_{i=1}^N A(x_i) W(x_i)}{\\sum_{i=1}^N W(x_i)}\n$$\nwhere $W(x_i) = \\pi(x_i)/\\pi_b(x_i)$ is the importance weight.\n\nWe now evaluate each option.\n\n**Option A:**\nThis option proposes to introduce a static bias potential $w(x)$ and sample a modified (biased) probability distribution $\\pi_w(x) \\propto \\exp[-\\beta(U(x) + w(x))]$.\n- **Condition (i) - Enhanced Sampling:** By choosing an appropriate $w(x)$, for example, a harmonic \"umbrella\" potential $w(x) = \\frac{1}{2}k(x-x_0)^2$ centered near the barrier top $x^\\ddagger$, the effective potential $U_w(x) = U(x) + w(x)$ can be flattened in the barrier region. This lowers the effective energy barrier, thereby substantially increasing the sampling frequency of this region. This condition is satisfied.\n- **Condition (ii) - Unbiased Recovery:** The simulation generates samples from the biased distribution $\\pi_w(x)$. The target distribution is $\\pi(x) \\propto \\exp[-\\beta U(x)]$. The sampling distribution is $\\pi_w(x) \\propto \\exp[-\\beta(U(x) + w(x))]$. The importance weight is therefore:\n$$\nW(x) = \\frac{\\pi(x)}{\\pi_w(x)} = \\frac{C \\exp[-\\beta U(x)]}{C_w \\exp[-\\beta(U(x) + w(x))]} \\propto \\exp[\\beta w(x)]\n$$\nwhere $C$ and $C_w$ are the normalization constants. The formula for recovering the unbiased average is:\n$$\n\\langle A \\rangle_\\pi \\approx \\frac{\\sum_i A(x_i) \\exp[\\beta w(x_i)]}{\\sum_i \\exp[\\beta w(x_i)]}\n$$\nThis precisely matches the procedure described in the option. This method, a form of umbrella sampling, is a standard, correct, and widely used technique in computational science.\n**Verdict: Correct.**\n\n**Option B:**\nThis option proposes modifying the Metropolis acceptance probability to $\\min\\{1, \\exp[-\\beta(U(x') - U(x))] \\exp[\\gamma(x'-x)]\\}$ while using a symmetric proposal.\n- **Condition (i) - Enhanced Sampling:** The factor $\\exp[\\gamma(x'-x)]$ with $\\gamma > 0$ provides a non-physical \"force\" that encourages moves in the positive $x$ direction. This could potentially accelerate barrier crossing from one well to the other, thus satisfying condition (i).\n- **Condition (ii) - Unbiased Recovery:** A Monte Carlo simulation with a given acceptance rule generates samples from a stationary distribution $\\pi_{mod}(x)$ that satisfies the detailed balance condition. For a symmetric proposal, this condition is $\\pi_{mod}(x) a(x \\to x') = \\pi_{mod}(x') a(x' \\to x)$, which implies $\\pi_{mod}(x')/\\pi_{mod}(x) = a(x \\to x')/a(x' \\to x)$. The ratio of acceptance probabilities does not simplify easily, but we can determine the stationary distribution by inspecting the acceptance rule form: $a(x \\to x') = \\min\\{1, \\pi_{mod}(x')/\\pi_{mod}(x)\\}$. Comparing this to the proposed rule, we must have:\n$$\n\\frac{\\pi_{mod}(x')}{\\pi_{mod}(x)} = \\exp[-\\beta(U(x') - U(x))] \\exp[\\gamma(x'-x)] = \\frac{\\exp[-\\beta U(x') + \\gamma x']}{\\exp[-\\beta U(x) + \\gamma x]}\n$$\nThis shows that the stationary distribution is not $\\pi(x)$, but rather $\\pi_{mod}(x) \\propto \\exp[-\\beta U(x) + \\gamma x]$. This is equivalent to simulating a system with an external linear potential $w(x) = -(\\gamma/\\beta)x$. The option states that one should compute \"plain (unweighted) sample averages\". This calculates $\\langle A \\rangle_{\\pi_{mod}}$, not $\\langle A \\rangle_\\pi$. Since $\\pi_{mod}(x) \\neq \\pi(x)$, this procedure yields an incorrect, biased result. Condition (ii) is violated.\n**Verdict: Incorrect.**\n\n**Option C:**\nThis option proposes using an independent proposal distribution $q(x')$ supported only near the barrier top $x^\\ddagger$.\n- **Condition (i) - Enhanced Sampling:** The intention is to force the system to visit the barrier region. However, this scheme is catastrophically inefficient. An independent proposal $q(x \\to x') = q(x')$ is not symmetric, so the correct Metropolis-Hastings acceptance probability must be used:\n$$\na(x \\to x') = \\min\\left(1, \\frac{\\pi(x') q(x' \\to x)}{\\pi(x) q(x \\to x')}\\right) = \\min\\left(1, \\frac{\\pi(x') q(x)}{\\pi(x) q(x')}\\right)\n$$\nThe current state $x$ is almost always in a low-energy well, where $\\pi(x)$ is large. The proposed state $x'$ is always at the high-energy barrier, where $\\pi(x')$ is extremely small. Furthermore, since the proposal distribution $q(\\cdot)$ is supported only near the barrier, proposing a state $x$ located in a well has zero probability, i.e., $q(x)=0$. Consequently, the acceptance probability $a(x \\to x')$ is $0$. The simulation will never move from its initial position. Even if $q(x)$ were small but non-zero, the ratio $\\pi(x')/\\pi(x)$ is exponentially small, leading to a negligible acceptance rate. The scheme fails to enhance sampling; it eliminates it. Condition (i) is severely violated.\n- **Condition (ii) - Unbiased Recovery:** The option states to compute plain averages. If the chain could move, it would indeed sample $\\pi(x)$, and this would be correct. But the dynamics are frozen, so no meaningful sampling occurs.\n**Verdict: Incorrect.**\n\n**Option D:**\nThis option proposes adding a bias potential $w(x)$ to enhance sampling but then computes observables as simple averages without reweighting.\n- **Condition (i) - Enhanced Sampling:** As established in the analysis of Option A, adding a suitable bias potential $w(x)$ is an effective way to enhance sampling of the barrier region. Condition (i) is satisfied.\n- **Condition (ii) - Unbiased Recovery:** The simulation generates samples from the biased distribution $\\pi_w(x) \\propto \\exp[-\\beta(U(x) + w(x))]$. Computing a \"simple average\" means calculating the expectation value with respect to this biased distribution, $\\langle A \\rangle_{\\pi_w}$. This is not the same as the target canonical average, $\\langle A \\rangle_\\pi$. The argument that \"the bias only improves efficiency and does not change equilibrium expectations\" is a fundamental misunderstanding of statistical mechanics. The equilibrium properties of a system are determined by its Hamiltonian or potential energy function. Modifying the potential changes the equilibrium state and all associated expectation values. To recover the properties of the original system, a correction (reweighting) is mandatory, as described in Option A. Condition (ii) is violated.\n**Verdict: Incorrect.**\n\nIn summary, only Option A describes a physically and mathematically sound procedure that both enhances the sampling of a difficult region and provides a rigorous mechanism for recovering the exact, unbiased equilibrium properties of the original system.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}