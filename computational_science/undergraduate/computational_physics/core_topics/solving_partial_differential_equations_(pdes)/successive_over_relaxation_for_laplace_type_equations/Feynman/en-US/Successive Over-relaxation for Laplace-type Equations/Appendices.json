{
    "hands_on_practices": [
        {
            "introduction": "The journey into iterative methods begins with implementation. This foundational practice challenges you to build a Successive Over-Relaxation (SOR) solver for the 2D Laplace equation from the ground up, moving beyond simple Gauss-Seidel iteration to actively accelerate convergence. By empirically searching for the optimal relaxation parameter, $\\omega_{\\mathrm{opt}}$, across various domain geometries, you will gain first-hand experience in a critical aspect of computational performance tuning . This exercise not only solidifies your understanding of the algorithm but also introduces the robust red-black ordering scheme, a key technique for efficient parallel computation.",
            "id": "2397060",
            "problem": "You are to implement a numerical solver for the two-dimensional Laplace equation using Successive Over-Relaxation (SOR) to accelerate Gauss–Seidel iteration. Your implementation must empirically determine the optimal relaxation parameter $\\,\\omega_{\\mathrm{opt}}\\,\\in(0,2)\\,$ for specific rectangular geometries with Dirichlet boundary conditions. The task must be carried out from first principles starting from the definition of the Laplace equation and a standard five-point finite difference discretization on a uniform Cartesian grid.\n\nYou must adhere to the following requirements.\n\n- Start from the definition of the Laplace equation in two spatial dimensions, $\\,\\nabla^2 u = 0\\,$, and the five-point uniform-grid finite difference discretization on a rectangular lattice. Use Gauss–Seidel iteration enhanced by Successive Over-Relaxation (SOR). Do not assume any pre-derived formulas beyond these foundations.\n- Use red–black ordering (checkerboard updates) so that within each iteration you update all red interior points first, then all black interior points, with in-place updates.\n- Adopt the following convergence criterion. Let one “iteration” be one complete red–black sweep. After each sweep, define the iteration-to-iteration change as the maximum absolute difference between the updated values and their previous values over the set of updated interior points (exclude Dirichlet-fixed boundary points and any internal Dirichlet obstacles). Stop when this infinity norm of the update falls below the tolerance $\\,\\tau = 10^{-5}\\,$, or when the number of iterations reaches the maximum $\\,I_{\\max} = 5000\\,$, whichever comes first.\n- Use a zero initial guess for all interior unknowns, i.e., $\\,u_{i,j}=0\\,$ for interior points at the start of each solve. Apply the Dirichlet boundary values and any internal Dirichlet obstacle values before iterations begin and keep them fixed during iterations.\n- Define the empirical search for $\\,\\omega_{\\mathrm{opt}}\\,$ as a two-stage search:\n  - Coarse stage: test $\\,\\omega\\,$ on the grid $\\,\\{1.0, 1.1, 1.2, \\dots, 1.9\\}\\,$.\n  - Fine stage: let $\\,\\omega_c\\,$ be the coarse-stage minimizer of the iteration count to convergence (break ties by choosing the smallest $\\,\\omega\\,$). Then test $\\,\\omega\\,$ on the uniform grid $\\,\\max(1.0,\\omega_c-0.05),\\,\\max(1.0,\\omega_c-0.05)+0.01,\\,\\dots,\\,\\min(1.95,\\omega_c+0.05)\\,$. Choose $\\,\\omega_{\\mathrm{opt}}\\,$ as the minimizer of the iteration count over the fine grid (again breaking ties by choosing the smallest $\\,\\omega\\,$). Report $\\,\\omega_{\\mathrm{opt}}\\,$ rounded to two decimals, and report the corresponding minimal iteration count as an integer. Only round after selecting the minimizer.\n- Use a uniform grid spacing in both directions. You do not need to report physical units, and you should not use angular quantities.\n\nTest suite. Implement your program to compute and report results for the following three ($3$) test cases.\n\n- Case A (square with a hot top edge):\n  - Grid: $\\,N_x = 50\\,$, $\\,N_y = 50\\,$.\n  - Dirichlet boundary values: top edge $\\,u=1.0\\,$, and bottom, left, right edges $\\,u=0.0\\,$.\n  - No internal obstacle.\n- Case B (rectangle with a hot left edge):\n  - Grid: $\\,N_x = 60\\,$, $\\,N_y = 30\\,$.\n  - Dirichlet boundary values: left edge $\\,u=1.0\\,$, and top, bottom, right edges $\\,u=0.0\\,$.\n  - No internal obstacle.\n- Case C (square with a warm internal obstacle):\n  - Grid: $\\,N_x = 40\\,$, $\\,N_y = 40\\,$.\n  - Dirichlet boundary values: all four outer edges $\\,u=0.0\\,$.\n  - Internal Dirichlet obstacle: a centered square of side length $\\,8\\,$ interior grid points (i.e., indices $\\,x\\in\\{16,17,\\dots,23\\}\\,$ and $\\,y\\in\\{16,17,\\dots,23\\}\\,$ in zero-based indexing) held at fixed value $\\,u=0.5\\,$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list $[\\omega_{\\mathrm{opt}}, I_{\\min}]$, where $\\,\\omega_{\\mathrm{opt}}\\,$ is rounded to two decimals and $\\,I_{\\min}\\,$ is the minimal iteration count as an integer. The final line must therefore look like\n$\\,\\big[[\\omega^{(A)}_{\\mathrm{opt}}, I^{(A)}_{\\min}], [\\omega^{(B)}_{\\mathrm{opt}}, I^{(B)}_{\\min}], [\\omega^{(C)}_{\\mathrm{opt}}, I^{(C)}_{\\min}]\\big]\\,$\nwith no additional text. For example, a syntactically correct output would be\n$\\,\\big[[1.80, 250], [1.90, 180], [1.75, 310]\\big]\\,$\nalthough these are not the expected numerical values for this problem.",
            "solution": "We begin from the Laplace equation in two spatial dimensions,\n$$\n\\nabla^2 u \\;=\\; \\frac{\\partial^2 u}{\\partial x^2} \\;+\\; \\frac{\\partial^2 u}{\\partial y^2} \\;=\\; 0,\n$$\nposed on a rectangle with Dirichlet boundary data. On a uniform Cartesian grid with spacing $\\,h\\,$ in both directions and interior indices $\\,i=1,\\dots,N_x-2\\,$ and $\\,j=1,\\dots,N_y-2\\,$ (with the outermost indices reserved for Dirichlet boundaries), the classical five-point finite difference (FD) approximation yields the discrete Laplace operator\n$$\n\\frac{u_{i+1,j} - 2\\,u_{i,j} + u_{i-1,j}}{h^2} \\;+\\; \\frac{u_{i,j+1} - 2\\,u_{i,j} + u_{i,j-1}}{h^2} \\;=\\; 0,\n$$\nwhich simplifies, for uniform $\\,h\\,$ and pure Laplace (no source term), to the fixed-point relation\n$$\nu_{i,j} \\;=\\; \\frac{1}{4}\\,\\big( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\big).\n$$\nThis identity suggests an iterative method: Gauss–Seidel (GS) iteration updates each interior $\\,u_{i,j}\\,$ in-place using the most recently available neighbor values. When some grid points are Dirichlet-fixed (boundaries or internal obstacles), they are excluded from updates and retained as constants so that their values participate in neighbor sums without changing.\n\nSuccessive Over-Relaxation (SOR) accelerates GS by convex extrapolation of the GS update towards the fixed point. Denote the Gauss–Seidel update at $\\,\\{i,j\\}\\,$ by\n$$\nu^{\\mathrm{GS}}_{i,j} \\;=\\; \\frac{1}{4}\\,\\big( u_{i+1,j}^{\\star} + u_{i-1,j}^{\\star} + u_{i,j+1}^{\\star} + u_{i,j-1}^{\\star} \\big),\n$$\nwhere each neighbor $\\,u^{\\star}\\,$ is the current in-place value (some neighbors already updated in the same sweep, others not yet). The SOR update with relaxation parameter $\\,\\omega\\in(0,2)\\,$ is\n$$\nu^{\\mathrm{new}}_{i,j} \\;=\\; (1-\\omega)\\,u^{\\mathrm{old}}_{i,j} \\;+\\; \\omega\\,u^{\\mathrm{GS}}_{i,j}.\n$$\nFor red–black ordering, the grid is partitioned into two interleaved sets based on parity of $\\,i+j\\,$. All red points are updated first using only black neighbors (since each red point’s nearest neighbors are black), then all black points are updated using the newly updated red neighbors, completing one iteration (one sweep). This ordering enables vectorized updates and preserves the Gauss–Seidel dependency structure.\n\nConvergence is monitored by the infinity norm of the per-iteration change. Let $\\,\\Delta^{(k)}\\,$ be the maximum absolute difference between updated and previous values across all updated interior points during iteration $\\,k\\,$. We terminate when\n$$\n\\Delta^{(k)}  \\tau = 10^{-5},\n$$\nor when the number of iterations reaches the cap $\\,I_{\\max}=5000\\,$. The initial guess is set to $\\,u_{i,j}=0\\,$ for all interior points, with Dirichlet boundaries and internal obstacle values imposed and held fixed.\n\nTo empirically determine $\\,\\omega_{\\mathrm{opt}}\\,$, we minimize the number of iterations required to satisfy the stopping criterion over a prescribed set of $\\,\\omega\\,$ values. We adopt a two-stage search to balance robustness and computational cost:\n\n- Coarse stage: evaluate $\\,\\omega \\in \\{1.0, 1.1, \\dots, 1.9\\}\\,$ and select the minimizer $\\,\\omega_c\\,$ of the iteration count (breaking ties towards the smaller $\\,\\omega\\,$).\n- Fine stage: evaluate the neighborhood $\\,\\omega \\in [\\max(1.0,\\omega_c-0.05), \\min(1.95,\\omega_c+0.05)]\\,$ on a uniform grid with step $\\,0.01\\,$, and select the minimizer $\\,\\omega_{\\mathrm{opt}}\\,$ (again breaking ties to the smaller $\\,\\omega\\,$). Only after selecting the minimizer do we round $\\,\\omega_{\\mathrm{opt}}\\,$ to two decimals for reporting.\n\nWe implement three geometries.\n\n- Case A: $\\,N_x=50\\,$, $\\,N_y=50\\,$. Top edge fixed to $\\,u=1.0\\,$; other edges fixed to $\\,u=0.0\\,$; no obstacle.\n- Case B: $\\,N_x=60\\,$, $\\,N_y=30\\,$. Left edge fixed to $\\,u=1.0\\,$; other edges fixed to $\\,u=0.0\\,$; no obstacle.\n- Case C: $\\,N_x=40\\,$, $\\,N_y=40\\,$. All outer edges fixed to $\\,u=0.0\\,$. A centered internal Dirichlet obstacle of side $\\,8\\,$ interior grid points, indices $\\,x\\in\\{16,\\dots,23\\},\\,y\\in\\{16,\\dots,23\\}\\,$, fixed to $\\,u=0.5\\,$.\n\nAlgorithmic details for efficiency and correctness:\n\n- Construct boolean masks over interior points to identify red and black update locations, and optionally to exclude obstacle cells from updates.\n- In each red (respectively black) half-sweep, compute the neighbor sum $\\,u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1}\\,$ over the entire interior, then apply the SOR update only on the red (respectively black) mask. For the red half-sweep, the neighbor sum uses values where all red neighbors are actually black cells; thus the sum correctly employs the most recent black values. After red updates, recompute the neighbor sum so that black updates see the updated red neighbors.\n- Track the maximum absolute change across updates in that sweep as $\\,\\Delta^{(k)}\\,$.\n\nThe program then executes the two-stage search for each test case, records the best $\\,\\omega_{\\mathrm{opt}}\\,$ and its minimal iteration count $\\,I_{\\min}\\,$, rounds $\\,\\omega_{\\mathrm{opt}}\\,$ to two decimals, and prints a single line with the list\n$\\big[[\\omega^{(A)}_{\\mathrm{opt}}, I^{(A)}_{\\min}], [\\omega^{(B)}_{\\mathrm{opt}}, I^{(B)}_{\\min}], [\\omega^{(C)}_{\\mathrm{opt}}, I^{(C)}_{\\min}]\\big].$\nThis procedure is universal for Laplace-type problems on uniform grids with Dirichlet data and demonstrates how Successive Over-Relaxation (SOR) accelerates Gauss–Seidel iteration and how the relaxation parameter $\\,\\omega\\,$ can be empirically tuned for a given discrete geometry.",
            "answer": "```python\nimport numpy as np\n\ndef sor_red_black(\n    nx: int,\n    ny: int,\n    boundary_values: dict,\n    obstacle_mask: np.ndarray | None = None,\n    obstacle_value: float | None = None,\n    omega: float = 1.5,\n    tol: float = 1e-5,\n    max_iters: int = 5000,\n) -> int:\n    \"\"\"\n    Perform red-black SOR for Laplace's equation on an nx-by-ny grid.\n    - boundary_values: dict with any of 'top','bottom','left','right' mapping to floats.\n      Edges not specified default to 0.0.\n    - obstacle_mask: full-size boolean array (ny, nx), True where Dirichlet-fixed internal obstacle exists.\n    - obstacle_value: float value assigned to obstacle cells (required if obstacle_mask is provided).\n    - omega: relaxation parameter in (0,2).\n    - tol: stopping tolerance on infinity norm of per-iteration change.\n    - max_iters: maximum number of red-black sweeps.\n    Returns: number of iterations (sweeps) taken to reach tol (or max_iters if not reached).\n    \"\"\"\n    # Initialize potential field\n    u = np.zeros((ny, nx), dtype=np.float64)\n\n    # Apply boundary conditions (unspecified edges are zero)\n    if 'bottom' in boundary_values:\n        u[0, :] = boundary_values['bottom']\n    if 'top' in boundary_values:\n        u[-1, :] = boundary_values['top']\n    if 'left' in boundary_values:\n        u[:, 0] = boundary_values['left']\n    if 'right' in boundary_values:\n        u[:, -1] = boundary_values['right']\n\n    # Internal Dirichlet obstacle\n    if obstacle_mask is not None:\n        if obstacle_value is None:\n            raise ValueError(\"obstacle_value must be provided when obstacle_mask is given.\")\n        u[obstacle_mask] = obstacle_value\n\n    # Interior masks (exclude outer boundary)\n    # Boolean mask of interior points that are updatable (not obstacle)\n    interior_shape = (ny - 2, nx - 2)\n    if interior_shape[0] = 0 or interior_shape[1] = 0:\n        return 0  # no interior to update\n\n    # Build interior obstacle mask\n    if obstacle_mask is not None:\n        obsI = obstacle_mask[1:-1, 1:-1].copy()\n    else:\n        obsI = np.zeros(interior_shape, dtype=bool)\n\n    update_mask = ~obsI  # True where we update\n\n    # Red-black masks over the interior\n    ii = np.arange(1, ny - 1)[:, None]\n    jj = np.arange(1, nx - 1)[None, :]\n    parity = (ii + jj) % 2  # 0 for even (red), 1 for odd (black)\n    red_mask = (parity == 0)\n    black_mask = ~red_mask\n    red_mask = red_mask  update_mask\n    black_mask = black_mask  update_mask\n\n    # Helper to compute neighbor sum over interior\n    def neighbor_sum(U):\n        return (U[2:, 1:-1] + U[:-2, 1:-1] + U[1:-1, 2:] + U[1:-1, :-2])\n\n    # Iterate\n    iters = 0\n    for k in range(1, max_iters + 1):\n        max_change = 0.0\n\n        # Red half-sweep\n        s = neighbor_sum(u)\n        uI = u[1:-1, 1:-1]\n        if np.any(red_mask):\n            old_vals = uI[red_mask]\n            gs_vals = 0.25 * s[red_mask]\n            new_vals = (1.0 - omega) * old_vals + omega * gs_vals\n            change = np.max(np.abs(new_vals - old_vals)) if new_vals.size else 0.0\n            max_change = max(max_change, float(change))\n            uI[red_mask] = new_vals\n\n        # Black half-sweep\n        s = neighbor_sum(u)  # recompute after red updates\n        if np.any(black_mask):\n            old_vals = uI[black_mask]\n            gs_vals = 0.25 * s[black_mask]\n            new_vals = (1.0 - omega) * old_vals + omega * gs_vals\n            change = np.max(np.abs(new_vals - old_vals)) if new_vals.size else 0.0\n            max_change = max(max_change, float(change))\n            uI[black_mask] = new_vals\n\n        iters = k\n        if max_change  tol:\n            break\n\n    return iters\n\n\ndef empirical_omega_opt(\n    nx: int,\n    ny: int,\n    boundary_values: dict,\n    obstacle_mask: np.ndarray | None,\n    obstacle_value: float | None,\n    tol: float = 1e-5,\n    max_iters: int = 5000,\n):\n    \"\"\"\n    Two-stage empirical search for omega_opt minimizing iteration count.\n    Returns (omega_opt_rounded_to_2_decimals, min_iterations).\n    \"\"\"\n    # Coarse grid: 1.0 to 1.9 step 0.1\n    coarse_candidates = [round(1.0 + 0.1 * i, 10) for i in range(0, 10)]  # [1.0, ..., 1.9]\n    coarse_results = []\n    for w in coarse_candidates:\n        it = sor_red_black(\n            nx, ny, boundary_values,\n            obstacle_mask=obstacle_mask,\n            obstacle_value=obstacle_value,\n            omega=w, tol=tol, max_iters=max_iters\n        )\n        coarse_results.append((w, it))\n    # Select best coarse (min iters, tie-break by smallest omega)\n    min_it_coarse = min(it for _, it in coarse_results)\n    best_coarse = min([w for (w, it) in coarse_results if it == min_it_coarse])\n\n    # Fine grid around best_coarse: step 0.01 within [1.0, 1.95]\n    start = max(1.0, best_coarse - 0.05)\n    end = min(1.95, best_coarse + 0.05)\n    n_steps = int(round((end - start) / 0.01))  # inclusive range\n    fine_candidates = [round(start + 0.01 * i, 10) for i in range(n_steps + 1)]\n    fine_results = []\n    for w in fine_candidates:\n        it = sor_red_black(\n            nx, ny, boundary_values,\n            obstacle_mask=obstacle_mask,\n            obstacle_value=obstacle_value,\n            omega=w, tol=tol, max_iters=max_iters\n        )\n        fine_results.append((w, it))\n    min_it_fine = min(it for _, it in fine_results)\n    best_fine = min([w for (w, it) in fine_results if it == min_it_fine])\n\n    return round(best_fine, 2), int(min_it_fine)\n\n\ndef build_obstacle_mask(ny: int, nx: int, y0: int, y1: int, x0: int, x1: int) -> np.ndarray:\n    \"\"\"\n    Build a full-size boolean mask with True on [y0:y1, x0:x1] inclusive of endpoints if using slice semantics.\n    Here, we assume x1, y1 are exclusive upper bounds for numpy slicing.\n    \"\"\"\n    mask = np.zeros((ny, nx), dtype=bool)\n    mask[y0:y1, x0:x1] = True\n    return mask\n\n\ndef solve():\n    results = []\n\n    # Common solver settings\n    tol = 1e-5\n    max_iters = 5000\n\n    # Case A: Nx=50, Ny=50, top=1.0, others=0.0, no obstacle\n    nx_A, ny_A = 50, 50\n    bvals_A = {'top': 1.0, 'bottom': 0.0, 'left': 0.0, 'right': 0.0}\n    omega_A, it_A = empirical_omega_opt(\n        nx_A, ny_A, bvals_A,\n        obstacle_mask=None, obstacle_value=None,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_A, it_A])\n\n    # Case B: Nx=60, Ny=30, left=1.0, others=0.0, no obstacle\n    nx_B, ny_B = 60, 30\n    bvals_B = {'left': 1.0, 'top': 0.0, 'bottom': 0.0, 'right': 0.0}\n    omega_B, it_B = empirical_omega_opt(\n        nx_B, ny_B, bvals_B,\n        obstacle_mask=None, obstacle_value=None,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_B, it_B])\n\n    # Case C: Nx=40, Ny=40, all edges 0.0, centered 8x8 obstacle at 0.5\n    nx_C, ny_C = 40, 40\n    bvals_C = {'top': 0.0, 'bottom': 0.0, 'left': 0.0, 'right': 0.0}\n    # Center indices for obstacle: 8x8 block\n    side = 8\n    x0 = nx_C // 2 - side // 2\n    x1 = x0 + side\n    y0 = ny_C // 2 - side // 2\n    y1 = y0 + side\n    obstacle_C = build_obstacle_mask(ny_C, nx_C, y0, y1, x0, x1)\n    omega_C, it_C = empirical_omega_opt(\n        nx_C, ny_C, bvals_C,\n        obstacle_mask=obstacle_C, obstacle_value=0.5,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_C, it_C])\n\n    # Final output in the exact required format\n    # Print a single line: [[omegaA,iterA],[omegaB,iterB],[omegaC,iterC]]\n    print(str(results).replace(' ', ''))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A working solver is only useful if we know when to trust its results. This crucial exercise explores the subtle but dangerous pitfall of \"false convergence,\" where an iterative method appears to have stabilized because the change per iteration is small, even though the solution is still far from correct . By constructing scenarios where the update norm $\\|\\Delta \\phi\\|_{\\infty}$ is negligible while the true residual norm $\\|A\\phi - b\\|_{\\infty}$ remains large, you will learn to critically evaluate stopping criteria and appreciate the residual as a more reliable indicator of accuracy.",
            "id": "2397044",
            "problem": "Consider the two-dimensional Laplace equation on a square grid with Dirichlet boundary conditions. The continuous problem is given by $-\\nabla^2 \\phi(\\mathbf{x}) = 0$ in a domain with prescribed boundary values. Discretize the equation on a uniform square grid of size $N \\times N$ with unit grid spacing $h = 1$ using the standard five-point finite difference stencil. Let the interior unknowns be updated by a weighted Gauss–Seidel sweep (also known as Successive Over-Relaxation (SOR) when the relaxation parameter differs from $1$), defined by the point-wise update\n$$\n\\phi_{i,j}^{\\text{new}} = \\phi_{i,j}^{\\text{old}} + \\omega \\left(\\frac{1}{4}\\left(\\phi_{i+1,j}^{\\text{current}} + \\phi_{i-1,j}^{\\text{new}} + \\phi_{i,j+1}^{\\text{current}} + \\phi_{i,j-1}^{\\text{new}}\\right) - \\phi_{i,j}^{\\text{old}} \\right),\n$$\nwhere $\\omega \\in [0,2)$ is the relaxation parameter, and the notation emphasizes the lexicographic use of newly updated neighbors as in the Gauss–Seidel method. Assume a homogeneous right-hand side $f_{i,j} = 0$ (Laplace equation), so that the rearranged discrete equation at interior points is\n$$\n\\phi_{i,j} = \\frac{1}{4}\\left(\\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1}\\right).\n$$\nImpose Dirichlet boundary conditions on the square:\n- Left boundary $x=0$: $\\phi=1$,\n- Right boundary $x=1$: $\\phi=0$,\n- Top boundary $y=1$: $\\phi=0$,\n- Bottom boundary $y=0$: $\\phi=0$.\nInitialize all interior values with $\\phi=0$. Let the grid spacing be $h=1$ in arbitrary units, and let the discrete residual at an interior point be\n$$\nr_{i,j} = \\left(\\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} - 4\\phi_{i,j}\\right),\n$$\nwhich is proportional to the algebraic residual $(A\\phi-b)$ up to the constant factor $1/h^2$. Define the infinity norm of the update on a sweep as\n$$\n\\|\\Delta \\phi\\|_{\\infty} = \\max_{i,j} \\left| \\phi_{i,j}^{\\text{new}} - \\phi_{i,j}^{\\text{old}} \\right|,\n$$\nand the infinity norm of the residual as\n$$\n\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|.\n$$\nYour task is to write a complete, runnable program that:\n- Performs exactly $K$ Gauss–Seidel sweeps with relaxation parameter $\\omega$ on the above problem setup for each test case.\n- After the final sweep, computes $\\|\\Delta \\phi\\|_{\\infty}$ using the last sweep’s updates and computes $\\|r\\|_{\\infty}$ on the resulting field.\n- For each test case, outputs a boolean indicating whether a “false convergence” has occurred, defined here as\n$$\n\\|\\Delta \\phi\\|_{\\infty} \\le \\tau_u \\quad \\text{and} \\quad \\|r\\|_{\\infty} \\ge \\tau_r.\n$$\n\nUse the following test suite of parameter values, designed to probe typical and edge-case behaviors:\n- Case $1$ (happy path convergence): $(N,K,\\omega,\\tau_u,\\tau_r) = (33,8000,1.0,10^{-6},10^{-6})$.\n- Case $2$ (extreme under-relaxation edge case): $(N,K,\\omega,\\tau_u,\\tau_r) = (33,1,0.0,10^{-12},10^{-1})$.\n- Case $3$ (mild under-relaxation illustrating small updates after one sweep while residual remains large): $(N,K,\\omega,\\tau_u,\\tau_r) = (33,1,0.1,3\\times 10^{-2},5\\times 10^{-1})$.\n\nAll angles, if any were present, should be in radians, but no angles are used here. No physical unit conversions are necessary beyond the specified arbitrary units for the grid spacing $h=1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, the output should look like\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3]\n$$\nwhere each $\\text{result}_i$ is either $\\text{True}$ or $\\text{False}$ for the corresponding test case in the order listed above.",
            "solution": "The problem requires an analysis of the convergence behavior of the Successive Over-Relaxation (SOR) method for solving the two-dimensional Laplace equation on a square grid. This is a fundamental problem in computational physics and numerical analysis. The validation of the problem statement must precede any attempt at a solution.\n\nThe problem is scientifically grounded, well-posed, and objective. It describes the discretization of the Laplace equation, $-\\nabla^2 \\phi(\\mathbf{x}) = 0$, using a standard five-point finite difference stencil on an $N \\times N$ grid with unit spacing $h=1$. This results in a system of linear equations for the potential $\\phi_{i,j}$ at interior grid points. The discrete equation at each interior point $(i,j)$ is\n$$\n\\phi_{i,j} = \\frac{1}{4} \\left( \\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} \\right).\n$$\nThis system is to be solved using an iterative method, specifically the SOR method, which includes the Gauss-Seidel method as a special case when the relaxation parameter $\\omega=1$. The update rule is given as:\n$$\n\\phi_{i,j}^{\\text{new}} = \\phi_{i,j}^{\\text{old}} + \\omega \\left(\\frac{1}{4}\\left(\\phi_{i+1,j}^{\\text{current}} + \\phi_{i-1,j}^{\\text{new}} + \\phi_{i,j+1}^{\\text{current}} + \\phi_{i,j-1}^{\\text{new}}\\right) - \\phi_{i,j}^{\\text{old}} \\right).\n$$\nThis can be rewritten as\n$$\n\\phi_{i,j}^{\\text{new}} = (1-\\omega)\\phi_{i,j}^{\\text{old}} + \\frac{\\omega}{4}\\left(\\phi_{i+1,j}^{\\text{current}} + \\phi_{i-1,j}^{\\text{new}} + \\phi_{i,j+1}^{\\text{current}} + \\phi_{i,j-1}^{\\text{new}}\\right).\n$$\nThe superscripts denote values from the previous iteration ('old', 'current') and values already updated in the current iteration ('new'), assuming a lexicographical sweep over the grid points. This corresponds to an in-place update of the potential array $\\phi$. All definitions, including the Dirichlet boundary conditions ($\\phi=1$ on the left edge, $\\phi=0$ elsewhere), initial conditions ($\\phi=0$ for all interior points), and the number of iterations $K$, are specified unambiguously.\n\nThe task is to evaluate a specific condition termed \"false convergence\" for three distinct test cases. This condition is defined as occurring when the infinity norm of the update, $\\|\\Delta \\phi\\|_{\\infty}$, is small, while the infinity norm of the residual, $\\|r\\|_{\\infty}$, remains large. The specific definitions are:\n- Update norm: $\\|\\Delta \\phi\\|_{\\infty} = \\max_{i,j} \\left| \\phi_{i,j}^{\\text{new}} - \\phi_{i,j}^{\\text{old}} \\right|$, calculated during the final ($K$-th) sweep.\n- Residual norm: $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$, where $r_{i,j} = \\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} - 4\\phi_{i,j}$, calculated after the final sweep.\n\nThe problem is valid and we proceed with the solution. The core of the solution is a program that implements the SOR algorithm and computes the required metrics.\n\nThe algorithm proceeds as follows for each test case $(N, K, \\omega, \\tau_u, \\tau_r)$:\n\n1.  **Initialization**: An $N \\times N$ array representing $\\phi$ is created and initialized to zero. The Dirichlet boundary conditions are then enforced: the column corresponding to the left boundary ($j=0$) is set to $1.0$, while the other boundaries remain at $0.0$.\n\n2.  **Iteration**: The program performs exactly $K$ SOR sweeps. A sweep consists of iterating over all interior grid points, $(i,j)$ for $i,j \\in [1, N-2]$, in lexicographical order. At each point, the value $\\phi_{i,j}$ is updated in-place according to the SOR formula. During the final, $K$-th sweep, the maximum absolute difference between the new and old values, $|\\phi_{i,j}^{\\text{new}} - \\phi_{i,j}^{\\text{old}}|$, is tracked to determine $\\|\\Delta \\phi\\|_{\\infty}$.\n\n3.  **Residual Calculation**: After $K$ sweeps, the final potential field $\\phi$ is used to compute the residual $r_{i,j}$ at all interior points. The infinity norm of this residual field, $\\|r\\|_{\\infty}$, is then found. This can be efficiently computed using array slicing.\n\n4.  **Condition Evaluation**: Finally, the boolean condition for \"false convergence\" is evaluated:\n    $$\n    \\|\\Delta \\phi\\|_{\\infty} \\le \\tau_u \\quad \\text{and} \\quad \\|r\\|_{\\infty} \\ge \\tau_r.\n    $$\n    The resulting boolean value (`True` or `False`) is the output for the test case.\n\nThe test cases are chosen to explore different behaviors:\n- **Case 1**: $(N, K, \\omega, \\tau_u, \\tau_r) = (33, 8000, 1.0, 10^{-6}, 10^{-6})$. This represents a standard Gauss-Seidel run with many iterations. We expect the method to be well-converged, meaning both $\\|\\Delta \\phi\\|_{\\infty}$ and $\\|r\\|_{\\infty}$ will be small. The condition should evaluate to `False`.\n- **Case 2**: $(N, K, \\omega, \\tau_u, \\tau_r) = (33, 1, 0.0, 10^{-12}, 10^{-1})$. With $\\omega=0$, no update occurs, so $\\phi$ remains unchanged from its initial state. Thus, $\\|\\Delta \\phi\\|_{\\infty} = 0$. However, the initial state is far from the true solution, so the residual, particularly near the $\\phi=1$ boundary, will be large (of order $1$). This is a trivial case of \"false convergence\" where the update is zero but the solution is incorrect. The condition should evaluate to `True`.\n- **Case 3**: $(N, K, \\omega, \\tau_u, \\tau_r) = (33, 1, 0.1, 3 \\times 10^{-2}, 5 \\times 10^{-1})$. A single sweep with strong under-relaxation ($\\omega=0.1$). The update size is directly proportional to $\\omega$, leading to a small $\\|\\Delta \\phi\\|_{\\infty}$. However, one small update step is insufficient to significantly reduce the large initial residual. Thus, this case should also trigger the \"false convergence\" condition, evaluating to `True`.\n\nThis systematic approach, directly implementing the provided mathematical formulation, will produce the required results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case_simulation(N, K, omega, tau_u, tau_r):\n    \"\"\"\n    Solves the Laplace equation on a square grid using SOR for one test case.\n\n    Args:\n        N (int): The grid size (N x N).\n        K (int): The number of sweeps to perform.\n        omega (float): The relaxation parameter.\n        tau_u (float): The threshold for the update norm.\n        tau_r (float): The threshold for the residual norm.\n\n    Returns:\n        bool: True if false convergence occurred, False otherwise.\n    \"\"\"\n    # 1. Initialize the grid and boundary conditions\n    phi = np.zeros((N, N), dtype=np.float64)\n    # Dirichlet boundary conditions:\n    # phi=0 on top, bottom, right boundaries is default from np.zeros.\n    # phi=1 on left boundary.\n    phi[:, 0] = 1.0\n\n    # 2. Perform K sweeps\n    max_update = 0.0\n    \n    if K == 0:\n        # No sweeps, so no update.\n        delta_phi_inf = 0.0\n    else:\n        for k in range(K):\n            is_final_sweep = (k == K - 1)\n            if is_final_sweep:\n                max_update = 0.0\n\n            for i in range(1, N - 1):\n                for j in range(1, N - 1):\n                    old_val = phi[i, j]\n                    \n                    # Five-point stencil average\n                    term = 0.25 * (phi[i+1, j] + phi[i-1, j] + phi[i, j+1] + phi[i, j-1])\n                    \n                    # SOR update formula\n                    new_val = (1.0 - omega) * old_val + omega * term\n                    phi[i, j] = new_val\n\n                    if is_final_sweep:\n                        update = abs(new_val - old_val)\n                        if update > max_update:\n                            max_update = update\n        delta_phi_inf = max_update\n\n    # 3. Calculate the infinity norm of the residual after all sweeps\n    if N > 2:\n        # Calculate residual r_ij = (phi_{i+1,j} + ...) - 4*phi_{i,j} for interior points\n        residual = (phi[2:, 1:-1] + phi[:-2, 1:-1] +\n                    phi[1:-1, 2:] + phi[1:-1, :-2] -\n                    4.0 * phi[1:-1, 1:-1])\n        r_inf = np.max(np.abs(residual))\n    else:\n        # No interior points, residual is trivially zero.\n        r_inf = 0.0\n\n    # 4. Check for \"false convergence\"\n    is_false_converged = (delta_phi_inf = tau_u) and (r_inf >= tau_r)\n    return is_false_converged\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path convergence)\n        (33, 8000, 1.0, 10**-6, 10**-6),\n        # Case 2 (extreme under-relaxation edge case)\n        (33, 1, 0.0, 10**-12, 10**-1),\n        # Case 3 (mild under-relaxation with small updates but large residual)\n        (33, 1, 0.1, 3 * 10**-2, 5 * 10**-1),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, K, omega, tau_u, tau_r = case\n        result = run_case_simulation(N, K, omega, tau_u, tau_r)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) converts boolean True/False to \"True\"/\"False\" which is the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "True mastery of a numerical method lies in the ability to adapt it beyond textbook examples. This advanced practice moves from the familiar Cartesian grid to a triangular lattice, a common feature in materials science and other fields . The core task is to derive the discrete Laplacian stencil from first principles using Taylor series, reinforcing the fundamental theory behind the finite difference method. Successfully implementing an SOR solver on this new geometry demonstrates a deep, adaptable understanding of the principles of numerical simulation.",
            "id": "2444006",
            "problem": "Consider the two-dimensional Laplace equation $\\,\\nabla^2 u = 0\\,$ on a regular equilateral triangular lattice (equivalently, a hexagonal-neighbor graph) with lattice spacing $\\,a = 1\\,$. Let the lattice be represented by integer indices $\\,i \\in \\{0,1,\\ldots,N_x-1\\}\\,$ and $\\,j \\in \\{0,1,\\ldots,N_y-1\\}\\,$ mapped to Cartesian coordinates by\n$$\nx(i,j) = i + \\tfrac{1}{2} j, \\quad y(i,j) = \\tfrac{\\sqrt{3}}{2} j.\n$$\nAt each interior lattice site $(i,j)$, the six nearest neighbors are $(i-1,j)$, $(i+1,j)$, $(i,j-1)$, $(i,j+1)$, $(i-1,j+1)$, and $(i+1,j-1)$. Impose Dirichlet boundary conditions on the outer rectangular boundary $\\,\\{i=0\\} \\cup \\{i=N_x-1\\} \\cup \\{j=0\\} \\cup \\{j=N_y-1\\}\\,$ using the harmonic function\n$$\nu_{\\text{exact}}(x,y) = x^2 - y^2,\n$$\nwhich satisfies $\\,\\nabla^2 u_{\\text{exact}} = 0\\,$.\n\nTask A (derivation): Starting from the definition of the Laplace operator $\\,\\nabla^2 u = \\partial_{xx} u + \\partial_{yy} u\\,$ and using second-order Taylor expansions along the three unit directions of the triangular lattice separated by $\\,60^\\circ\\,$, derive a consistent second-order finite-difference approximation to $\\,\\nabla^2 u\\,$ on this lattice. From this, obtain the Gauss–Seidel fixed-point update for an interior node in terms of its six neighbors and then the Successive Over-Relaxation (SOR) update for a relaxation parameter $\\,\\omega \\in (0,2)\\,$. Clearly state the final interior-node update stencil you will implement.\n\nTask B (implementation): Implement a solver that applies the Successive Over-Relaxation (SOR) method to compute the steady-state solution $\\,\\phi\\,$ on the triangular lattice with the specified Dirichlet boundary data. Use in-place Gauss–Seidel ordering with over-relaxation parameter $\\,\\omega\\,$ and stop the iteration when the maximum absolute change of $\\,\\phi\\,$ over all interior nodes in a full sweep is less than a prescribed tolerance $\\,\\varepsilon\\,$, or when the number of sweeps reaches a maximum $\\,K_{\\max}\\,$. If there are no interior nodes (i.e., if $\\,N_x \\le 2\\,$ or $\\,N_y \\le 2\\,$), define the iteration count as $\\,0\\,$ and the error measure as $\\,0\\,$.\n\nTask C (error assessment): After convergence, compute the maximum absolute error at interior nodes relative to $\\,u_{\\text{exact}}(x(i,j),y(i,j))\\,$:\n$$\nE_{\\max} = \\max_{1 \\le i \\le N_x-2,\\; 1 \\le j \\le N_y-2} \\left| \\phi(i,j) - u_{\\text{exact}}\\!\\left(x(i,j),y(i,j)\\right) \\right|.\n$$\nIf there are no interior nodes, set $\\,E_{\\max} = 0\\,$.\n\nTest suite: For each parameter tuple $\\,(N_x,N_y,\\omega,\\varepsilon,K_{\\max})\\,$ below, run the solver and return the pair consisting of the number of sweeps required to converge and the corresponding $\\,E_{\\max}\\,$:\n- Case $\\,1$: $\\,(21,\\,21,\\,1.0,\\,10^{-10},\\,10000)\\,$.\n- Case $\\,2$: $\\,(21,\\,21,\\,1.6,\\,10^{-10},\\,10000)\\,$.\n- Case $\\,3$: $\\,(21,\\,21,\\,1.9,\\,10^{-10},\\,10000)\\,$.\n- Case $\\,4$: $\\,(3,\\,3,\\,1.5,\\,10^{-12},\\,1000)\\,$.\n- Case $\\,5$: $\\,(8,\\,2,\\,1.5,\\,10^{-12},\\,1000)\\,$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of pairs, each pair being $[ \\text{sweeps}, \\, E_{\\max} ]$, and the whole list enclosed in square brackets. For example, the format is $[ [r_1,e_1],[r_2,e_2],\\ldots ]$ with no spaces required. No other text should be printed.",
            "solution": "The problem requires the derivation and implementation of a Successive Over-Relaxation (SOR) solver for the two-dimensional Laplace equation, $\\nabla^2 u = 0$, on a regular equilateral triangular lattice.\n\n**Part A: Derivation of the Finite-Difference Stencil and SOR Update**\n\nThe first step is to derive a finite-difference approximation for the Laplace operator $\\nabla^2 u = \\partial_{xx} u + \\partial_{yy} u$ on the specified lattice. The lattice points $(i,j)$ are mapped to Cartesian coordinates by $x(i,j) = i + \\frac{1}{2} j$ and $y(i,j) = \\frac{\\sqrt{3}}{2} j$. The lattice spacing is $a=1$. An interior node at position $\\vec{r}_0$ has six nearest neighbors at positions $\\vec{r}_k = \\vec{r}_0 + \\vec{d}_k$ for $k=1, \\dots, 6$, where the displacement vectors $\\vec{d}_k$ each have magnitude $a=1$. These vectors correspond to the six neighbor indices provided:\n- $\\vec{d}_1 = (1, 0)$ for $(i+1, j)$\n- $\\vec{d}_2 = (-1, 0)$ for $(i-1, j)$\n- $\\vec{d}_3 = (\\frac{1}{2}, \\frac{\\sqrt{3}}{2})$ for $(i, j+1)$\n- $\\vec{d}_4 = (-\\frac{1}{2}, -\\frac{\\sqrt{3}}{2})$ for $(i, j-1)$\n- $\\vec{d}_5 = (-\\frac{1}{2}, \\frac{\\sqrt{3}}{2})$ for $(i-1, j+1)$\n- $\\vec{d}_6 = (\\frac{1}{2}, -\\frac{\\sqrt{3}}{2})$ for $(i+1, j-1)$\n\nThe problem statement suggests using Taylor expansions along three unit directions separated by $60^\\circ$. The vectors $\\vec{d}_1$, $\\vec{d}_3$, and $\\vec{d}_5$ form such a set. A robust and symmetric stencil is derived by considering all six neighbors, which corresponds to using three pairs of opposing directions.\n\nLet $u_0$ be the value of the function at a central node $\\vec{r}_0$, and $u_k$ be the value at neighbor $k$. The second-order Taylor expansion of $u$ at a neighboring point $\\vec{r}_0 + \\vec{d}_k$ is:\n$$u(\\vec{r}_0 + \\vec{d}_k) = u(\\vec{r}_0) + (\\vec{d}_k \\cdot \\nabla)u(\\vec{r}_0) + \\frac{1}{2}(\\vec{d}_k \\cdot \\nabla)^2 u(\\vec{r}_0) + O(a^3)$$\nSumming over all six neighbors ($k=1, \\dots, 6$):\n$$\\sum_{k=1}^6 u_k = 6u_0 + \\left( \\sum_{k=1}^6 \\vec{d}_k \\right) \\cdot \\nabla u_0 + \\frac{1}{2} \\sum_{k=1}^6 (\\vec{d}_k \\cdot \\nabla)^2 u_0 + O(a^4)$$\nThe sum of the displacement vectors is zero due to symmetry: $\\sum_{k=1}^6 \\vec{d}_k = \\vec{0}$. This cancels the first-derivative term.\nThe second-order operator is $(\\vec{d}_k \\cdot \\nabla)^2 = d_{kx}^2 \\partial_{xx} + 2d_{kx}d_{ky} \\partial_{xy} + d_{ky}^2 \\partial_{yy}$. We sum the coefficients over all $k$:\n- $\\sum_{k=1}^6 d_{kx}^2 = (1)^2 + (-1)^2 + (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (\\frac{1}{2})^2 = 3$\n- $\\sum_{k=1}^6 d_{ky}^2 = (0)^2 + (0)^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = 3$\n- $\\sum_{k=1}^6 2 d_{kx}d_{ky} = 0$\nThe sum of the second-order terms simplifies to:\n$$\\sum_{k=1}^6 (\\vec{d}_k \\cdot \\nabla)^2 u_0 = 3\\partial_{xx} u_0 + 3\\partial_{yy} u_0 = 3 \\nabla^2 u_0$$\nSubstituting this into the Taylor sum and using $a=1$:\n$$\\sum_{k=1}^6 u_k = 6u_0 + \\frac{3}{2} \\nabla^2 u_0 + O(a^4)$$\nFor the Laplace equation, we set $\\nabla^2 u = 0$, which gives the discrete equation:\n$$\\sum_{k=1}^6 u_k - 6u_0 = 0 \\quad \\implies \\quad u_0 = \\frac{1}{6} \\sum_{k=1}^6 u_k$$\nThis shows that for a harmonic function on a triangular lattice, the value at a point is the average of its six neighbors. Let $\\phi_{i,j}$ be the numerical solution at node $(i,j)$. For an interior node, this yields the relation:\n$$6\\phi_{i,j} = \\phi_{i-1,j} + \\phi_{i+1,j} + \\phi_{i,j-1} + \\phi_{i,j+1} + \\phi_{i-1,j+1} + \\phi_{i+1,j-1}$$\n\nA Gauss-Seidel iteration solves for $\\phi_{i,j}$ and uses the most recently updated values for neighbors during a sweep:\n$$\\phi_{i,j}^{\\text{GS}} = \\frac{1}{6} \\left( \\phi_{i-1,j} + \\phi_{i+1,j} + \\phi_{i,j-1} + \\phi_{i,j+1} + \\phi_{i-1,j+1} + \\phi_{i+1,j-1} \\right)$$\nThe Successive Over-Relaxation (SOR) method accelerates convergence by mixing the old value $\\phi_{i,j}^{\\text{old}}$ with the Gauss-Seidel update using a relaxation parameter $\\omega \\in (0,2)$:\n$$\\phi_{i,j}^{\\text{new}} = (1-\\omega)\\phi_{i,j}^{\\text{old}} + \\omega \\phi_{i,j}^{\\text{GS}}$$\nSubstituting the expression for $\\phi_{i,j}^{\\text{GS}}$, we obtain the final SOR update stencil to be implemented for each interior node:\n$$\\phi_{i,j}^{\\text{new}} = (1-\\omega)\\phi_{i,j}^{\\text{old}} + \\frac{\\omega}{6} \\left( \\phi_{i-1,j} + \\phi_{i+1,j} + \\phi_{i,j-1} + \\phi_{i,j+1} + \\phi_{i-1,j+1} + \\phi_{i+1,j-1} \\right)$$\n\n**Part B  C: Implementation Strategy and Error Assessment**\n\nThe algorithm is implemented as follows:\n$1$.  **Grid Initialization**: A 2D array $\\phi$ of size $(N_x, N_y)$ is created. The boundary nodes (where $i=0$, $i=N_x-1$, $j=0$, or $j=N_y-1$) are initialized with the values from the exact Dirichlet condition, $u_{\\text{exact}}(x,y) = x^2 - y^2$, using the provided coordinate mapping. Interior nodes are initialized to $0$.\n\n$2$.  **Special Cases**: If $N_x \\le 2$ or $N_y \\le 2$, there are no interior nodes. In this case, the iteration count is defined as $0$ and the maximum error $E_{\\max}$ is $0$.\n\n$3$.  **SOR Iteration**: The solver enters a main loop that runs for a maximum of $K_{\\max}$ sweeps. In each sweep:\n    a. A variable `max_change` is initialized to $0$.\n    b. The algorithm iterates through all interior nodes $(i,j)$, for $j$ from $1$ to $N_y-2$ and $i$ from $1$ to $N_x-2$.\n    c. At each interior node, the value $\\phi_{i,j}$ is updated in-place using the SOR stencil derived above. The use of in-place updates means that values of $\\phi$ at neighboring nodes that have already been visited in the current sweep are used in the calculation, which is the essence of the Gauss-Seidel method.\n    d. The absolute difference between the new and old value of $\\phi_{i,j}$ is computed, and `max_change` is updated to be the maximum such change seen so far in the sweep.\n    e. After iterating through all interior nodes, the convergence criterion is checked. If `max_change` is less than the tolerance $\\varepsilon$, the loop terminates.\n\n$4$.  **Error Assessment**: After the iterative process concludes, the maximum absolute error $E_{\\max}$ is computed. This involves iterating through all interior nodes and finding the maximum of $|\\phi_{i,j} - u_{\\text{exact}}(x(i,j), y(i,j))|$. If there are no interior nodes, $E_{\\max}$ is set to $0$.\n\nThe final output for each test case is the pair $[ \\text{sweeps}, E_{\\max} ]$, where `sweeps` is the number of full sweeps completed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_cartesian_coords(i, j):\n    \"\"\"\n    Computes Cartesian coordinates from triangular lattice indices (i,j).\n    \"\"\"\n    x = float(i) + 0.5 * float(j)\n    y = (np.sqrt(3.0) / 2.0) * float(j)\n    return x, y\n\ndef u_exact(x, y):\n    \"\"\"\n    Computes the exact solution u(x,y) = x^2 - y^2.\n    \"\"\"\n    return x**2 - y**2\n\ndef solve_case(Nx, Ny, omega, epsilon, K_max):\n    \"\"\"\n    Solves the Laplace equation on a triangular lattice for a single set of parameters.\n    \"\"\"\n    # Handle special case: no interior nodes\n    if Nx = 2 or Ny = 2:\n        return 0, 0.0\n\n    # Initialize the grid phi. Using (Nx, Ny) shape to map (i,j) directly.\n    phi = np.zeros((Nx, Ny), dtype=np.float64)\n\n    # Set Dirichlet boundary conditions\n    for i in range(Nx):\n        for j in range(Ny):\n            if i == 0 or i == Nx - 1 or j == 0 or j == Ny - 1:\n                x, y = get_cartesian_coords(i, j)\n                phi[i, j] = u_exact(x, y)\n\n    # Main SOR iteration loop\n    sweeps = 0\n    for k in range(K_max):\n        sweeps = k + 1\n        max_change = 0.0\n        \n        # Iterate over all interior nodes\n        for j in range(1, Ny - 1):\n            for i in range(1, Nx - 1):\n                old_val = phi[i, j]\n                \n                # Sum of the six neighbors on the triangular lattice\n                neighbor_sum = (phi[i - 1, j] + phi[i + 1, j] +\n                                phi[i, j - 1] + phi[i, j + 1] +\n                                phi[i - 1, j + 1] + phi[i + 1, j - 1])\n                \n                # Apply the SOR update formula\n                new_val = (1.0 - omega) * old_val + (omega / 6.0) * neighbor_sum\n                phi[i, j] = new_val\n                \n                # Track the maximum change in this sweep for convergence check\n                max_change = max(max_change, abs(new_val - old_val))\n\n        # Check for convergence\n        if max_change  epsilon:\n            break\n            \n    # Calculate the maximum absolute error E_max at interior nodes\n    max_error = 0.0\n    for j in range(1, Ny - 1):\n        for i in range(1, Nx - 1):\n            x, y = get_cartesian_coords(i, j)\n            exact_val = u_exact(x, y)\n            error = abs(phi[i, j] - exact_val)\n            max_error = max(max_error, error)\n            \n    return sweeps, max_error\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (21, 21, 1.0, 1e-10, 10000),\n        (21, 21, 1.6, 1e-10, 10000),\n        (21, 21, 1.9, 1e-10, 10000),\n        (3, 3, 1.5, 1e-12, 1000),\n        (8, 2, 1.5, 1e-12, 1000),\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny, omega, epsilon, K_max = case\n        sweeps, e_max = solve_case(Nx, Ny, omega, epsilon, K_max)\n        results.append(f\"[{sweeps},{e_max}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}