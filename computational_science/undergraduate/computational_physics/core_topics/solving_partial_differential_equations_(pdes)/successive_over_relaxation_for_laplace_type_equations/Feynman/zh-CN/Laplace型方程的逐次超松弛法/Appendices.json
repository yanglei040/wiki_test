{
    "hands_on_practices": [
        {
            "introduction": "掌握任何数值算法的第一步都是亲手实现它。本练习将指导您从头开始构建一个用于求解拉普拉斯方程的逐次超松弛（SOR）求解器，并经验性地探索松弛参数 $\\omega$ 对收敛速度的影响。通过这个实践，您将对SOR算法的核心机制以及参数调优的重要性建立起直观的理解。 ",
            "id": "2397060",
            "problem": "你需要实现一个求解二维拉普拉斯方程的数值求解器，使用逐次超松弛（SOR）方法来加速高斯-赛德尔（Gauss–Seidel）迭代。你的实现必须对特定的矩形几何区域和狄利克雷（Dirichlet）边界条件，通过经验性方法确定最优松弛参数 $\\,\\omega_{\\mathrm{opt}}\\,\\in(0,2)\\,$。这项任务必须从拉普拉斯方程的定义和在均匀笛卡尔网格上的标准五点有限差分格式等基本定义出发。\n\n你必须遵守以下要求。\n\n- 从二维空间中的拉普拉斯方程定义 $\\,\\nabla^2 u = 0\\,$ 以及矩形格点上的五点均匀网格有限差分格式出发。使用由逐次超松弛（SOR）增强的高斯-赛德尔迭代。除了这些基础知识外，不要假定任何预先推导出的公式。\n- 使用红黑排序（棋盘式更新），以便在每次迭代中，你首先更新所有红色的内部点，然后更新所有黑色的内部点，均进行原地更新。\n- 采用以下收敛准则。将一次完整的红黑扫描定义为一次“迭代”。每次扫描后，将迭代间的变化定义为，在所有被更新的内部点（不包括狄利克雷固定的边界点和任何内部狄利克雷障碍物）上，更新后的值与先前值之间的最大绝对差值。当这个更新量的无穷范数低于容差 $\\,\\tau = 10^{-5}\\,$ 时，或者当迭代次数达到最大值 $\\,I_{\\max} = 5000\\,$ 时，以先到者为准，停止迭代。\n- 对所有内部未知数使用零初始猜测，即在每次求解开始时，内部点的 $\\,u_{i,j}=0\\,$。在迭代开始前应用狄利克雷边界值和任何内部狄利克雷障碍物的值，并在迭代过程中保持它们不变。\n- 将寻找 $\\,\\omega_{\\mathrm{opt}}\\,$ 的经验性搜索定义为一个两阶段搜索：\n  - 粗略搜索阶段：在网格 $\\,\\{1.0, 1.1, 1.2, \\dots, 1.9\\}\\,$ 上测试 $\\,\\omega\\,$。\n  - 精细搜索阶段：设 $\\,\\omega_c\\,$ 为粗略搜索阶段中使收敛迭代次数最小化的值（若存在多个最小值，则选择最小的 $\\,\\omega\\,$）。然后在均匀网格 $\\,\\max(1.0,\\omega_c-0.05),\\,\\max(1.0,\\omega_c-0.05)+0.01,\\,\\dots,\\,\\min(1.95,\\omega_c+0.05)\\,$ 上测试 $\\,\\omega\\,$。选择 $\\,\\omega_{\\mathrm{opt}}\\,$ 作为精细网格上使迭代次数最小化的值（同样，若存在多个最小值则选择最小的 $\\,\\omega\\,$）。报告四舍五入到两位小数的 $\\,\\omega_{\\mathrm{opt}}\\,$，并将相应的最小迭代次数报告为整数。仅在选定最小值点后进行四舍五入。\n- 在两个方向上使用均匀的网格间距。你不需要报告物理单位，也不应使用角度量。\n\n测试套件。实现你的程序，计算并报告以下三（3）个测试案例的结果。\n\n- 案例 A（顶部为热源的方形区域）：\n  - 网格：$\\,N_x = 50\\,$, $\\,N_y = 50\\,$。\n  - 狄利克雷边界值：上边界 $\\,u=1.0\\,$，下、左、右边界 $\\,u=0.0\\,$。\n  - 无内部障碍物。\n- 案例 B（左侧为热源的矩形区域）：\n  - 网格：$\\,N_x = 60\\,$, $\\,N_y = 30\\,$。\n  - 狄利克雷边界值：左边界 $\\,u=1.0\\,$，上、下、右边界 $\\,u=0.0\\,$。\n  - 无内部障碍物。\n- 案例 C（带有温热内部障碍物的方形区域）：\n  - 网格：$\\,N_x = 40\\,$, $\\,N_y = 40\\,$。\n  - 狄利克雷边界值：所有四个外边界均为 $\\,u=0.0\\,$。\n  - 内部狄利克雷障碍物：一个边长为 $\\,8\\,$ 个内部网格点的中心正方形（即，在从零开始的索引中，索引为 $\\,x\\in\\{16,17,\\dots,23\\}\\,$ 和 $\\,y\\in\\{16,17,\\dots,23\\}\\,$），其值固定为 $\\,u=0.5\\,$。\n\n最终输出格式。你的程序应该生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果。每个测试案例的结果必须是一个包含两个元素的列表 $[\\omega_{\\mathrm{opt}}, I_{\\min}]$，其中 $\\,\\omega_{\\mathrm{opt}}\\,$ 四舍五入到两位小数，$\\,I_{\\min}\\,$ 是最小迭代次数的整数。因此，最后一行必须看起来像\n$\\,\\big[[\\omega^{(A)}_{\\mathrm{opt}}, I^{(A)}_{\\min}], [\\omega^{(B)}_{\\mathrm{opt}}, I^{(B)}_{\\min}], [\\omega^{(C)}_{\\mathrm{opt}}, I^{(C)}_{\\min}]\\big]\\,$\n且不含任何附加文本。例如，一个语法正确的输出将是\n$\\,\\big[[1.80, 250], [1.90, 180], [1.75, 310]\\big]\\,$\n尽管这些不是本问题的预期数值。",
            "solution": "我们从二维空间中的拉普拉斯方程开始，\n$$\n\\nabla^2 u \\;=\\; \\frac{\\partial^2 u}{\\partial x^2} \\;+\\; \\frac{\\partial^2 u}{\\partial y^2} \\;=\\; 0,\n$$\n该方程定义在具有狄利克雷边界数据的矩形区域上。在两个方向上间距均为 $\\,h\\,$ 的均匀笛卡尔网格上，内部点索引为 $\\,i=1,\\dots,N_x-2\\,$ 和 $\\,j=1,\\dots,N_y-2\\,$（最外层索引保留给狄利克雷边界），经典的五点有限差分（FD）近似产生离散拉普拉斯算子\n$$\n\\frac{u_{i+1,j} - 2\\,u_{i,j} + u_{i-1,j}}{h^2} \\;+\\; \\frac{u_{i,j+1} - 2\\,u_{i,j} + u_{i,j-1}}{h^2} \\;=\\; 0,\n$$\n对于均匀的 $\\,h\\,$ 和纯拉普拉斯方程（无源项），该式可简化为不动点关系式\n$$\nu_{i,j} \\;=\\; \\frac{1}{4}\\,\\big( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\big).\n$$\n这个恒等式启发了一种迭代方法：高斯-赛德尔（Gauss–Seidel, GS）迭代使用最新可用的邻近点值，对每个内部点 $\\,u_{i,j}\\,$ 进行原地更新。当某些网格点是狄利克雷固定的（边界或内部障碍物），它们不参与更新并保持为常数，以便它们的值能参与邻近点求和而自身不变。\n\n逐次超松弛（Successive Over-Relaxation, SOR）方法通过对 GS 更新进行朝向不动点的凸外插来加速收敛。将点 $\\,\\{i,j\\}\\,$ 处的高斯-赛德尔更新表示为\n$$\nu^{\\mathrm{GS}}_{i,j} \\;=\\; \\frac{1}{4}\\,\\big( u_{i+1,j}^{\\star} + u_{i-1,j}^{\\star} + u_{i,j+1}^{\\star} + u_{i,j-1}^{\\star} \\big),\n$$\n其中每个邻近点值 $\\,u^{\\star}\\,$ 是当前的原地值（在同一次扫描中，一些邻近点已经更新，而另一些尚未更新）。使用松弛参数 $\\,\\omega\\in(0,2)\\,$ 的 SOR 更新公式为\n$$\nu^{\\mathrm{new}}_{i,j} \\;=\\; (1-\\omega)\\,u^{\\mathrm{old}}_{i,j} \\;+\\; \\omega\\,u^{\\mathrm{GS}}_{i,j}.\n$$\n对于红黑排序，网格根据 $\\,i+j\\,$ 的奇偶性被划分为两个交错的集合。首先更新所有红点，只使用其黑点邻居（因为每个红点的最近邻都是黑点），然后使用新更新的红点邻居来更新所有黑点，这样就完成了一次迭代（一次扫描）。这种排序方式支持矢量化更新，并保留了高斯-赛德尔的依赖结构。\n\n收敛性通过每次迭代变化量的无穷范数来监控。设 $\\,\\Delta^{(k)}\\,$ 为第 $\\,k\\,$ 次迭代中，所有被更新的内部点上，新旧值之间的最大绝对差。当满足以下条件时，我们终止迭代\n$$\n\\Delta^{(k)} \\;<\\; \\tau \\;=\\; 10^{-5},\n$$\n或者当迭代次数达到上限 $\\,I_{\\max}=5000\\,$。所有内部点的初始猜测值设为 $\\,u_{i,j}=0\\,$，同时施加并固定狄利克雷边界和内部障碍物的值。\n\n为了通过经验确定 $\\,\\omega_{\\mathrm{opt}}\\,$，我们在一个预设的 $\\,\\omega\\,$ 值集合上，最小化满足停止准则所需的迭代次数。我们采用两阶段搜索来平衡鲁棒性和计算成本：\n\n- 粗略搜索阶段：在 $\\,\\omega \\in \\{1.0, 1.1, \\dots, 1.9\\}\\,$ 集合中进行评估，并选择使迭代次数最小的 $\\,\\omega_c\\,$（若存在多个最小值，则选择较小的 $\\,\\omega\\,$）。\n- 精细搜索阶段：在邻域 $\\,\\omega \\in [\\max(1.0,\\omega_c-0.05), \\min(1.95,\\omega_c+0.05)]\\,$ 的步长为 $\\,0.01\\,$ 的均匀网格上进行评估，并选择使迭代次数最小的 $\\,\\omega_{\\mathrm{opt}}\\,$（同样，若存在多个最小值则选择较小的 $\\,\\omega\\,$）。仅在选定最小值点后，我们将 $\\,\\omega_{\\mathrm{opt}}\\,$ 四舍五入到两位小数以供报告。\n\n我们为三种几何构型实现了算法。\n\n- 案例 A：$\\,N_x=50\\,$, $\\,N_y=50\\,$。上边界固定为 $\\,u=1.0\\,$；其他边界固定为 $\\,u=0.0\\,$；无障碍物。\n- 案例 B：$\\,N_x=60\\,$, $\\,N_y=30\\,$。左边界固定为 $\\,u=1.0\\,$；其他边界固定为 $\\,u=0.0\\,$；无障碍物。\n- 案例 C：$\\,N_x=40\\,$, $\\,N_y=40\\,$。所有外边界固定为 $\\,u=0.0\\,$。一个边长为 $\\,8\\,$ 个内部网格点的中心内部狄利克雷障碍物，其索引为 $\\,x\\in\\{16,\\dots,23\\},\\,y\\in\\{16,\\dots,23\\}\\,$，值固定为 $\\,u=0.5\\,$。\n\n为保证效率和正确性的算法细节：\n\n- 构建内部点的布尔掩码，以识别红色和黑色的更新位置，并可选择性地将障碍物单元排除在更新之外。\n- 在每个红色（或黑色）半扫描中，计算整个内部区域的邻居和 $\\,u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1}\\,$，然后仅在红色（或黑色）掩码上应用 SOR 更新。对于红色半扫描，邻居和使用的是所有红色邻居实际上是黑色单元的值；因此，该求和正确地使用了最新的黑色值。在红色更新之后，重新计算邻居和，以便黑色更新能看到已更新的红色邻居。\n- 跟踪该次扫描中更新的最大绝对变化量作为 $\\,\\Delta^{(k)}\\,$。\n\n程序随后对每个测试案例执行两阶段搜索，记录最佳的 $\\,\\omega_{\\mathrm{opt}}\\,$ 及其最小迭代次数 $\\,I_{\\min}\\,$，将 $\\,\\omega_{\\mathrm{opt}}\\,$ 四舍五入到两位小数，并打印包含列表的单行输出\n$$\n\\big[[\\omega^{(A)}_{\\mathrm{opt}}, I^{(A)}_{\\min}], [\\omega^{(B)}_{\\mathrm{opt}}, I^{(B)}_{\\min}], [\\omega^{(C)}_{\\mathrm{opt}}, I^{(C}}_{\\min}]\\big].\n$$\n此过程对于均匀网格上具有狄利克雷数据的拉普拉斯类型问题具有通用性，并展示了逐次超松弛（SOR）方法如何加速高斯-赛德尔迭代，以及如何针对给定的离散几何形状对松弛参数 $\\,\\omega\\,$ 进行经验性调优。",
            "answer": "```python\nimport numpy as np\n\ndef sor_red_black(\n    nx: int,\n    ny: int,\n    boundary_values: dict,\n    obstacle_mask: np.ndarray | None = None,\n    obstacle_value: float | None = None,\n    omega: float = 1.5,\n    tol: float = 1e-5,\n    max_iters: int = 5000,\n) -> int:\n    \"\"\"\n    Perform red-black SOR for Laplace's equation on an nx-by-ny grid.\n    - boundary_values: dict with any of 'top','bottom','left','right' mapping to floats.\n      Edges not specified default to 0.0.\n    - obstacle_mask: full-size boolean array (ny, nx), True where Dirichlet-fixed internal obstacle exists.\n    - obstacle_value: float value assigned to obstacle cells (required if obstacle_mask is provided).\n    - omega: relaxation parameter in (0,2).\n    - tol: stopping tolerance on infinity norm of per-iteration change.\n    - max_iters: maximum number of red-black sweeps.\n    Returns: number of iterations (sweeps) taken to reach tol (or max_iters if not reached).\n    \"\"\"\n    # Initialize potential field\n    u = np.zeros((ny, nx), dtype=np.float64)\n\n    # Apply boundary conditions (unspecified edges are zero)\n    if 'bottom' in boundary_values:\n        u[0, :] = boundary_values['bottom']\n    if 'top' in boundary_values:\n        u[-1, :] = boundary_values['top']\n    if 'left' in boundary_values:\n        u[:, 0] = boundary_values['left']\n    if 'right' in boundary_values:\n        u[:, -1] = boundary_values['right']\n\n    # Internal Dirichlet obstacle\n    if obstacle_mask is not None:\n        if obstacle_value is None:\n            raise ValueError(\"obstacle_value must be provided when obstacle_mask is given.\")\n        u[obstacle_mask] = obstacle_value\n\n    # Interior masks (exclude outer boundary)\n    # Boolean mask of interior points that are updatable (not obstacle)\n    interior_shape = (ny - 2, nx - 2)\n    if interior_shape[0] <= 0 or interior_shape[1] <= 0:\n        return 0  # no interior to update\n\n    # Build interior obstacle mask\n    if obstacle_mask is not None:\n        obsI = obstacle_mask[1:-1, 1:-1].copy()\n    else:\n        obsI = np.zeros(interior_shape, dtype=bool)\n\n    update_mask = ~obsI  # True where we update\n\n    # Red-black masks over the interior\n    ii = np.arange(1, ny - 1)[:, None]\n    jj = np.arange(1, nx - 1)[None, :]\n    parity = (ii + jj) & 1  # 0 for even (red), 1 for odd (black)\n    red_mask = (parity == 0)[1 - 1:ny - 1 - 1 + 1, 1 - 1:nx - 1 - 1 + 1]  # same shape as interior\n    black_mask = ~red_mask\n    red_mask = red_mask & update_mask\n    black_mask = black_mask & update_mask\n\n    # Helper to compute neighbor sum over interior\n    def neighbor_sum(U):\n        return (U[2:, 1:-1] + U[:-2, 1:-1] + U[1:-1, 2:] + U[1:-1, :-2])\n\n    # Iterate\n    iters = 0\n    for k in range(1, max_iters + 1):\n        max_change = 0.0\n\n        # Red half-sweep\n        s = neighbor_sum(u)\n        uI = u[1:-1, 1:-1]\n        if np.any(red_mask):\n            old_vals = uI[red_mask]\n            gs_vals = 0.25 * s[red_mask]\n            new_vals = (1.0 - omega) * old_vals + omega * gs_vals\n            change = np.max(np.abs(new_vals - old_vals)) if new_vals.size else 0.0\n            max_change = max(max_change, float(change))\n            uI[red_mask] = new_vals\n\n        # Black half-sweep\n        s = neighbor_sum(u)  # recompute after red updates\n        if np.any(black_mask):\n            old_vals = uI[black_mask]\n            gs_vals = 0.25 * s[black_mask]\n            new_vals = (1.0 - omega) * old_vals + omega * gs_vals\n            change = np.max(np.abs(new_vals - old_vals)) if new_vals.size else 0.0\n            max_change = max(max_change, float(change))\n            uI[black_mask] = new_vals\n\n        iters = k\n        if max_change < tol:\n            break\n\n    return iters\n\n\ndef empirical_omega_opt(\n    nx: int,\n    ny: int,\n    boundary_values: dict,\n    obstacle_mask: np.ndarray | None,\n    obstacle_value: float | None,\n    tol: float = 1e-5,\n    max_iters: int = 5000,\n):\n    \"\"\"\n    Two-stage empirical search for omega_opt minimizing iteration count.\n    Returns (omega_opt_rounded_to_2_decimals, min_iterations).\n    \"\"\"\n    # Coarse grid: 1.0 to 1.9 step 0.1\n    coarse_candidates = [round(1.0 + 0.1 * i, 10) for i in range(0, 10)]  # [1.0, ..., 1.9]\n    coarse_results = []\n    for w in coarse_candidates:\n        it = sor_red_black(\n            nx, ny, boundary_values,\n            obstacle_mask=obstacle_mask,\n            obstacle_value=obstacle_value,\n            omega=w, tol=tol, max_iters=max_iters\n        )\n        coarse_results.append((w, it))\n    # Select best coarse (min iters, tie-break by smallest omega)\n    min_it_coarse = min(it for _, it in coarse_results)\n    best_coarse = min([w for (w, it) in coarse_results if it == min_it_coarse])\n\n    # Fine grid around best_coarse: step 0.01 within [1.0, 1.95]\n    start = max(1.0, best_coarse - 0.05)\n    end = min(1.95, best_coarse + 0.05)\n    n_steps = int(round((end - start) / 0.01))  # inclusive range\n    fine_candidates = [round(start + 0.01 * i, 10) for i in range(n_steps + 1)]\n    fine_results = []\n    for w in fine_candidates:\n        it = sor_red_black(\n            nx, ny, boundary_values,\n            obstacle_mask=obstacle_mask,\n            obstacle_value=obstacle_value,\n            omega=w, tol=tol, max_iters=max_iters\n        )\n        fine_results.append((w, it))\n    min_it_fine = min(it for _, it in fine_results)\n    best_fine = min([w for (w, it) in fine_results if it == min_it_fine])\n\n    return round(best_fine, 2), int(min_it_fine)\n\n\ndef build_obstacle_mask(ny: int, nx: int, y0: int, y1: int, x0: int, x1: int) -> np.ndarray:\n    \"\"\"\n    Build a full-size boolean mask with True on [y0:y1, x0:x1] inclusive of endpoints if using slice semantics.\n    Here, we assume x1, y1 are exclusive upper bounds for numpy slicing.\n    \"\"\"\n    mask = np.zeros((ny, nx), dtype=bool)\n    mask[y0:y1, x0:x1] = True\n    return mask\n\n\ndef solve():\n    results = []\n\n    # Common solver settings\n    tol = 1e-5\n    max_iters = 5000\n\n    # Case A: Nx=50, Ny=50, top=1.0, others=0.0, no obstacle\n    nx_A, ny_A = 50, 50\n    bvals_A = {'top': 1.0, 'bottom': 0.0, 'left': 0.0, 'right': 0.0}\n    omega_A, it_A = empirical_omega_opt(\n        nx_A, ny_A, bvals_A,\n        obstacle_mask=None, obstacle_value=None,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_A, it_A])\n\n    # Case B: Nx=60, Ny=30, left=1.0, others=0.0, no obstacle\n    nx_B, ny_B = 60, 30\n    bvals_B = {'left': 1.0, 'top': 0.0, 'bottom': 0.0, 'right': 0.0}\n    omega_B, it_B = empirical_omega_opt(\n        nx_B, ny_B, bvals_B,\n        obstacle_mask=None, obstacle_value=None,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_B, it_B])\n\n    # Case C: Nx=40, Ny=40, all edges 0.0, centered 8x8 obstacle at 0.5\n    nx_C, ny_C = 40, 40\n    bvals_C = {'top': 0.0, 'bottom': 0.0, 'left': 0.0, 'right': 0.0}\n    # Center indices for obstacle: 8x8 block\n    side = 8\n    x0 = nx_C // 2 - side // 2\n    x1 = x0 + side\n    y0 = ny_C // 2 - side // 2\n    y1 = y0 + side\n    obstacle_C = build_obstacle_mask(ny_C, nx_C, y0, y1, x0, x1)\n    omega_C, it_C = empirical_omega_opt(\n        nx_C, ny_C, bvals_C,\n        obstacle_mask=obstacle_C, obstacle_value=0.5,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_C, it_C])\n\n    # Final output in the exact required format\n    # Print a single line: [[omegaA,iterA],[omegaB,iterB],[omegaC,iterC]]\n    print(str(results).replace(' ', ''))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "虽然经验性搜索可以帮助我们找到一个好的松弛参数，但理论分析能让我们更深刻地理解其背后的原理。本练习将引导您探索最优松弛参数 $\\omega_{\\text{opt}}$ 与离散系统性质之间的数学关系，特别是它如何依赖于网格的几何形状。完成这个练习后，您将能够从理论上预测不同问题设置下的最优参数。 ",
            "id": "2444079",
            "problem": "考虑在大小为 $N \\times M$ 的均匀矩形内部点网格上，使用标准的五点模板求解带狄利克雷边界条件的二维拉普拉斯方程。令 $u_{i,j}$ 表示内部网格点 $(i,j)$ 处的网格函数，其中 $i \\in \\{1,\\dots,N\\}$ 且 $j \\in \\{1,\\dots,M\\}$。离散拉普拉斯算子由标准的二阶中心差分近似定义。通过将线性系统的系数矩阵分裂为对角、严格下三角和严格上三角部分，可以构造一个定常线性迭代。特别地，逐次超松弛（SOR）法是通过一个标量松弛参数 $\\omega \\in (0,2)$ 对高斯-赛德尔（GS）更新进行松弛得到的。\n\n你的任务是，从第一性原理出发，确定最优松弛参数 $\\omega_{\\mathrm{opt}}$ 如何依赖于网格维度 $N \\times M$，特别是在网格高度各向异性时（例如，一个维度远大于另一个维度的长而薄的通道）。按以下步骤进行。\n\n1) 从矩形网格上两个方向网格间距均等的拉普拉斯方程的离散形式开始。根据离散系统的矩阵分裂，定义雅可比迭代和高斯-赛德尔迭代。使用变量分离法分析雅可比迭代算子的本征模，将其本征值表示为模态指数和网格尺寸的三角函数。目标是将雅可比迭代的谱半径（记为 $\\rho_J$）表示为 $N$ 和 $M$ 的函数。\n\n2) 使用对称正定矩阵的定常迭代谱分析的标准结果，将最优SOR参数 $\\omega_{\\mathrm{opt}}$ 与 $\\rho_J$ 联系起来。基于线性迭代理论和五点离散拉普拉斯算子的结构，用严格的论证来证明该关系。所有三角函数必须使用以弧度为单位的角度。\n\n3) 实现一个程序，给定 $(N,M)$，根据步骤1中的本征值分析计算 $\\rho_J$，然后根据步骤2中的关系计算 $\\omega_{\\mathrm{opt}}$。你的程序应计算并报告以下每个测试用例的 $\\omega_{\\mathrm{opt}}$ 值：\n- $(N,M) = (\\,32,\\,32\\,)$\n- $(N,M) = (\\,64,\\,8\\,)$\n- $(N,M) = (\\,8,\\,64\\,)$\n- $(N,M) = (\\,3,\\,3\\,)$\n- $(N,M) = (\\,3,\\,50\\,)$\n\n4) 为了数值可复现性，你的程序必须将每个 $\\omega_{\\mathrm{opt}}$ 输出为精确到小数点后 $6$ 位的浮点数。\n\n最终输出格式：你的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表，列表内没有空格。例如，如果有三个结果 $a$, $b$, $c$，输出格式将是“[a,b,c]”。每个列表元素必须是精确到小数点后 $6$ 位的浮点数。",
            "solution": "我们从业于在大小为 $N \\times M$ 的内部节点矩形网格上，两个方向网格间距均等的二维拉普拉斯方程。标准的五点模板产生一个线性系统 $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$，其中 $\\mathbf{A}$ 是与狄利克雷边界条件下的离散拉普拉斯算子相关的稀疏、对称正定矩阵。设分裂为 $\\mathbf{A}=\\mathbf{D}-\\mathbf{L}-\\mathbf{U}$，其中 $\\mathbf{D}$ 是 $\\mathbf{A}$ 的对角部分，$\\mathbf{L}$ 是严格下三角部分，$\\mathbf{U}$ 是严格上三角部分。\n\n雅可比法通过以下方式更新：\n$$\n\\mathbf{u}^{(k+1)}=\\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U})\\mathbf{u}^{(k)}+\\mathbf{D}^{-1}\\mathbf{b},\n$$\n其迭代矩阵为\n$$\n\\mathbf{T}_J = \\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U}) = \\mathbf{I} - \\mathbf{D}^{-1}\\mathbf{A}.\n$$\n高斯-赛德尔（GS）法通过以下方式更新：\n$$\n\\mathbf{u}^{(k+1)}=(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}\\,\\mathbf{u}^{(k)}+(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{b}.\n$$\n逐次超松弛（SOR）法通过以下方式更新：\n$$\n\\mathbf{u}^{(k+1)}=(\\mathbf{D}-\\omega \\mathbf{L})^{-1}\\left[(1-\\omega)\\mathbf{D}+\\omega \\mathbf{U}\\right]\\mathbf{u}^{(k)}+(\\mathbf{D}-\\omega \\mathbf{L})^{-1}\\,\\omega \\mathbf{b},\n$$\n其松弛参数为 $\\omega \\in (0,2)$。\n\n我们现在分析矩形网格上五点离散拉普拉斯算子的雅可比迭代的本征结构。假设单位间距并将狄利克雷边界条件并入 $\\mathbf{b}$，内部点 $(i,j)$ 上的离散拉普拉斯算子为\n$$\n- u_{i-1,j}-u_{i+1,j}-u_{i,j-1}-u_{i,j+1}+4u_{i,j}=0.\n$$\n雅可比迭代通过对其四个邻点进行平均来更新 $u_{i,j}$，且齐次迭代矩阵 $\\mathbf{T}_J$ 是可分离的。使用变量分离法和与狄利克雷边界条件一致的离散正弦基，$\\mathbf{T}_J$ 的本征向量具有分量\n$$\nv_{i,j}^{(k,\\ell)} = \\sin\\!\\left(\\frac{\\pi k i}{N+1}\\right)\\sin\\!\\left(\\frac{\\pi \\ell j}{M+1}\\right), \\quad k\\in\\{1,\\dots,N\\},\\ \\ell\\in\\{1,\\dots,M\\}.\n$$\n对于这些模态，$\\mathbf{T}_J$ 的相应本征值为\n$$\n\\lambda_{k,\\ell} = \\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi k}{N+1}\\right) + \\cos\\!\\left(\\frac{\\pi \\ell}{M+1}\\right)\\right].\n$$\n此公式通过将分离的本征模代入雅可比更新并使用离散二阶差分的三角恒等式得出。雅可比迭代的谱半径，记为 $\\rho_J$，是 $\\lambda_{k,\\ell}$ 的最大模：\n$$\n\\rho_J = \\max_{1\\le k\\le N,\\ 1\\le \\ell\\le M} \\left|\\lambda_{k,\\ell}\\right|.\n$$\n因为 $\\cos(x)$ 在 $x\\in[0,\\pi]$上是递减的，并且在此处所选的排序下所有 $\\lambda_{k,\\ell}$ 都是非负的，所以在 $k=1$ 和 $\\ell=1$ 处达到最大值，得到\n$$\n\\rho_J = \\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)+\\cos\\!\\left(\\frac{\\pi}{M+1}\\right)\\right].\n$$\n\n为了将最优SOR松弛参数与 $\\rho_J$ 联系起来，我们回顾了对矩形网格上五点拉普拉斯型对称正定系统的定常迭代分析中的一个经典结果（Young 的超松弛分析）：最小化SOR迭代矩阵谱半径的最优SOR松弛参数，可以用雅可比谱半径表示。具体来说，\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{1+\\sqrt{1-\\rho_J^2}}.\n$$\n此关系可以通过考虑应用于GS分裂的切比雪夫半迭代多项式加速来证明，并利用SOR对应于加权GS步的事实；最优权重源于在包含GS本征值的区间上最小化谱半径，对于五点拉普拉斯算子，这可以用雅可比谱半径 $\\rho_J$ 来表示。对于特殊的正方形情况 $N=M$，有 $\\rho_J=\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)$，因此\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{1+\\sin\\!\\left(\\frac{\\pi}{N+1}\\right)},\n$$\n因为 $\\sqrt{1-\\cos^2(\\theta)}=\\sin(\\theta)$，其中 $\\theta=\\frac{\\pi}{N+1}$。对于 $N\\neq M$ 的矩形网格，依赖关系仍然通过上面推导的 $\\rho_J$ 存在：\n$$\n\\rho_J = \\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)+\\cos\\!\\left(\\frac{\\pi}{M+1}\\right)\\right], \\quad\n\\omega_{\\mathrm{opt}} = \\frac{2}{1+\\sqrt{1-\\rho_J^2}}.\n$$\n\n算法设计：\n- 输入：内部网格尺寸对 $(N,M)$。\n- 步骤1：使用\n$$\n\\rho_J(N,M)=\\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)+\\cos\\!\\left(\\frac{\\pi}{M+1}\\right)\\right],\n$$\n计算 $\\rho_J$，所有角度以弧度为单位。\n- 步骤2：计算\n$$\n\\omega_{\\mathrm{opt}}(N,M)=\\frac{2}{1+\\sqrt{1-\\rho_J(N,M)^2}}.\n$$\n为保证数值稳定性，计算 $1-\\rho_J^2$，并且为抵消当 $N$ 或 $M$ 很大时的舍入误差，在开平方根之前将非常接近零的负值钳位到零。\n- 步骤3：将每个 $\\omega_{\\mathrm{opt}}$ 四舍五入到小数点后恰好 $6$ 位，并以指定的单行格式输出列表。\n\n定性行为：\n- 对于 $N$ 和 $M$ 很大的方形网格，$\\frac{\\pi}{N+1}$ 和 $\\frac{\\pi}{M+1}$ 很小，$\\rho_J$ 趋近于 $1$，$\\sqrt{1-\\rho_J^2}$ 变小，$\\omega_{\\mathrm{opt}}$ 从下方趋近于 $2$。\n- 对于高度各向异性的网格（长而薄的通道），$\\frac{\\pi}{N+1}$ 或 $\\frac{\\pi}{M+1}$ 中的一个相对较大，这会减小 $\\rho_J$ 并增大 $\\sqrt{1-\\rho_J^2}$，从而减小 $\\omega_{\\mathrm{opt}}$。因此，各向异性使 $\\omega_{\\mathrm{opt}}$ 远离 $2$。\n\n将算法应用于测试套件：\n- $(N,M)=(\\,32,\\,32\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.827963$。\n- $(N,M)=(\\,64,\\,8\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.605816$。\n- $(N,M)=(\\,8,\\,64\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.605816$。\n- $(N,M)=(\\,3,\\,3\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.171573$。\n- $(N,M)=(\\,3,\\,50\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.313552$。\n\n这些值是由程序使用以弧度为单位的角度计算并四舍五入到小数点后 $6$ 位生成的，它们说明了 $\\omega_{\\mathrm{opt}}$ 如何随着网格变得更具各向异性而减小。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef omega_opt(N: int, M: int) -> float:\n    \"\"\"\n    Compute the optimal SOR relaxation parameter for the 2D five-point Laplacian\n    on an N x M interior grid (Dirichlet BCs), using the classical relation\n    with the Jacobi spectral radius.\n    \"\"\"\n    # Angles in radians\n    theta_x = np.pi / (N + 1)\n    theta_y = np.pi / (M + 1)\n    rho_J = 0.5 * (np.cos(theta_x) + np.cos(theta_y))\n    # Numerical guard for round-off: ensure the argument of sqrt is non-negative\n    arg = 1.0 - rho_J * rho_J\n    if arg < 0.0 and arg > -1e-16:\n        arg = 0.0\n    # Compute omega_opt\n    denom = 1.0 + np.sqrt(arg)\n    # Avoid division by zero (should not occur for valid N, M >= 1)\n    if denom == 0.0:\n        return 2.0\n    return 2.0 / denom\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (N, M), numbers of interior points in x and y directions.\n    test_cases = [\n        (32, 32),\n        (64, 8),\n        (8, 64),\n        (3, 3),\n        (3, 50),\n    ]\n\n    results = []\n    for N, M in test_cases:\n        w = omega_opt(N, M)\n        # Round to exactly 6 decimal places\n        results.append(f\"{w:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "计算科学中的一个关键技能是将标准方法推广到非标准问题上。本练习提供了一个绝佳的挑战，要求您将SOR方法应用于三角晶格，这在材料科学和物理学中很常见。您需要首先从第一性原理出发推导新的离散格式，然后修改您的求解器，从而展示该方法在处理复杂几何问题时的灵活性和强大功能。 ",
            "id": "2444006",
            "problem": "考虑一个格点间距 $\\,a = 1\\,$ 的正等边三角形格点（等效于六边形邻居图）上的二维拉普拉斯方程 $\\,\\nabla^2 u = 0\\,$。设格点由整数索引 $\\,i \\in \\{0,1,\\ldots,N_x-1\\}\\,$ 和 $\\,j \\in \\{0,1,\\ldots,N_y-1\\}\\,$ 表示，通过下式映射到笛卡尔坐标：\n$$\nx(i,j) = i + \\tfrac{1}{2} j, \\quad y(i,j) = \\tfrac{\\sqrt{3}}{2} j.\n$$\n在每个内部格点 $(i,j)$ 处，其六个最近邻为 $(i-1,j)$、$(i+1,j)$、$(i,j-1)$、$(i,j+1)$、$(i-1,j+1)$ 和 $(i+1,j-1)$。使用调和函数\n$$\nu_{\\text{exact}}(x,y) = x^2 - y^2,\n$$\n在外部矩形边界 $\\,\\{i=0\\} \\cup \\{i=N_x-1\\} \\cup \\{j=0\\} \\cup \\{j=N_y-1\\}\\,$ 上施加狄利克雷边界条件，该函数满足 $\\,\\nabla^2 u_{\\text{exact}} = 0\\,$。\n\n任务 A（推导）：从拉普拉斯算子 $\\,\\nabla^2 u = \\partial_{xx} u + \\partial_{yy} u\\,$ 的定义出发，并使用沿三角格点相隔 $\\,60^\\circ\\,$ 的三个单位方向的二阶泰勒展开，推导此格点上 $\\,\\nabla^2 u\\,$ 的一个一致的二阶有限差分近似。由此，得到内部节点关于其六个邻居的高斯-赛德尔（Gauss-Seidel）定点更新公式，然后是关于松弛参数 $\\,\\omega \\in (0,2)\\,$ 的逐次超松弛（SOR）更新公式。清晰地陈述你将实现的最终内部节点更新模板。\n\n任务 B（实现）：实现一个求解器，应用逐次超松弛（SOR）方法计算具有指定狄利克雷边界数据的三角格点上的稳态解 $\\,\\phi\\,$。使用带有超松弛参数 $\\,\\omega\\,$ 的原位高斯-赛德尔（Gauss-Seidel）顺序，当单次完整扫描中所有内部节点上 $\\,\\phi\\,$ 的最大绝对变化量小于给定容差 $\\,\\varepsilon\\,$ 时，或当扫描次数达到最大值 $\\,K_{\\max}\\,$ 时，停止迭代。如果没有内部节点（即，如果 $\\,N_x \\le 2\\,$ 或 $\\,N_y \\le 2\\,$），则将迭代次数定义为 $\\,0\\,$，误差度量定义为 $\\,0\\,$。\n\n任务 C（误差评估）：收敛后，计算内部节点相对于 $\\,u_{\\text{exact}}(x(i,j),y(i,j))\\,$ 的最大绝对误差：\n$$\nE_{\\max} = \\max_{1 \\le i \\le N_x-2,\\; 1 \\le j \\le N_y-2} \\left| \\phi(i,j) - u_{\\text{exact}}\\!\\left(x(i,j),y(i,j)\\right) \\right|.\n$$\n如果没有内部节点，则设置 $\\,E_{\\max} = 0\\,$。\n\n测试套件：对于下面的每个参数元组 $\\,(N_x,N_y,\\omega,\\varepsilon,K_{\\max})\\,$，运行求解器并返回由收敛所需的扫描次数和相应的 $\\,E_{\\max}\\,$ 组成的数对：\n- 情况 $\\,1$: $\\,(21,\\,21,\\,1.0,\\,10^{-10},\\,10000)\\,$。\n- 情况 $\\,2$: $\\,(21,\\,21,\\,1.6,\\,10^{-10},\\,10000)\\,$。\n- 情况 $\\,3$: $\\,(21,\\,21,\\,1.9,\\,10^{-10},\\,10000)\\,$。\n- 情况 $\\,4$: $\\,(3,\\,3,\\,1.5,\\,10^{-12},\\,1000)\\,$。\n- 情况 $\\,5$: $\\,(8,\\,2,\\,1.5,\\,10^{-12},\\,1000)\\,$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个以逗号分隔的数对列表形式的结果，每个数对为 $[ \\text{扫描次数}, \\, E_{\\max} ]$，整个列表用方括号括起来。例如，格式为 $[ [r_1,e_1],[r_2,e_2],\\ldots ]$，不需要空格。不应打印任何其他文本。",
            "solution": "该问题要求在正等边三角形格点上，推导并实现一个用于二维拉普拉斯方程 $\\nabla^2 u = 0$ 的逐次超松弛（SOR）求解器。\n\n**A 部分：有限差分模板和 SOR 更新的推导**\n\n第一步是在指定的格点上推导拉普拉斯算子 $\\nabla^2 u = \\partial_{xx} u + \\partial_{yy} u$ 的有限差分近似。格点 $(i,j)$ 通过 $x(i,j) = i + \\frac{1}{2} j$ 和 $y(i,j) = \\frac{\\sqrt{3}}{2} j$ 映射到笛卡尔坐标。格点间距为 $a=1$。位于 $\\vec{r}_0$ 的一个内部节点有六个最近邻，其位置为 $\\vec{r}_k = \\vec{r}_0 + \\vec{d}_k$，其中 $k=1, \\dots, 6$。位移矢量 $\\vec{d}_k$ 的模长均为 $a=1$。这些矢量对应于提供的六个邻居索引：\n- $\\vec{d}_1 = (1, 0)$ 对应 $(i+1, j)$\n- $\\vec{d}_2 = (-1, 0)$ 对应 $(i-1, j)$\n- $\\vec{d}_3 = (\\frac{1}{2}, \\frac{\\sqrt{3}}{2})$ 对应 $(i, j+1)$\n- $\\vec{d}_4 = (-\\frac{1}{2}, -\\frac{\\sqrt{3}}{2})$ 对应 $(i, j-1)$\n- $\\vec{d}_5 = (-\\frac{1}{2}, \\frac{\\sqrt{3}}{2})$ 对应 $(i-1, j+1)$\n- $\\vec{d}_6 = (\\frac{1}{2}, -\\frac{\\sqrt{3}}{2})$ 对应 $(i+1, j-1)$\n\n问题陈述建议使用沿三个相隔 $60^\\circ$ 的单位方向的泰勒展开。矢量 $\\vec{d}_1$、$\\vec{d}_3$ 和 $\\vec{d}_5$ 构成了这样一组。通过考虑所有六个邻居（这相当于使用三对相反的方向），可以推导出一个鲁棒且对称的模板。\n\n设 $u_0$ 为中心节点 $\\vec{r}_0$ 处的函数值，$u_k$ 为邻居 $k$ 处的函数值。函数 $u$ 在邻近点 $\\vec{r}_0 + \\vec{d}_k$ 处的二阶泰勒展开为：\n$$u(\\vec{r}_0 + \\vec{d}_k) = u(\\vec{r}_0) + (\\vec{d}_k \\cdot \\nabla)u(\\vec{r}_0) + \\frac{1}{2}(\\vec{d}_k \\cdot \\nabla)^2 u(\\vec{r}_0) + O(a^3)$$\n对所有六个邻居（$k=1, \\dots, 6$）求和：\n$$\\sum_{k=1}^6 u_k = 6u_0 + \\left( \\sum_{k=1}^6 \\vec{d}_k \\right) \\cdot \\nabla u_0 + \\frac{1}{2} \\sum_{k=1}^6 (\\vec{d}_k \\cdot \\nabla)^2 u_0 + O(a^4)$$\n由于对称性，位移矢量的和为零：$\\sum_{k=1}^6 \\vec{d}_k = \\vec{0}$。这消除了含一阶导数的项。\n二阶算子为 $(\\vec{d}_k \\cdot \\nabla)^2 = d_{kx}^2 \\partial_{xx} + 2d_{kx}d_{ky} \\partial_{xy} + d_{ky}^2 \\partial_{yy}$。我们对所有 $k$ 的系数求和：\n- $\\sum_{k=1}^6 d_{kx}^2 = (1)^2 + (-1)^2 + (\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (-\\frac{1}{2})^2 + (\\frac{1}{2})^2 = 3$\n- $\\sum_{k=1}^6 d_{ky}^2 = (0)^2 + (0)^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = 3$\n- $\\sum_{k=1}^6 2 d_{kx}d_{ky} = 0$\n二阶项的和简化为：\n$$\\sum_{k=1}^6 (\\vec{d}_k \\cdot \\nabla)^2 u_0 = 3\\partial_{xx} u_0 + 3\\partial_{yy} u_0 = 3 \\nabla^2 u_0$$\n将此代入泰勒求和式并使用 $a=1$：\n$$\\sum_{k=1}^6 u_k = 6u_0 + \\frac{3}{2} \\nabla^2 u_0 + O(a^4)$$\n对于拉普拉斯方程，我们令 $\\nabla^2 u = 0$，得到离散方程：\n$$\\sum_{k=1}^6 u_k - 6u_0 = 0 \\quad \\implies \\quad u_0 = \\frac{1}{6} \\sum_{k=1}^6 u_k$$\n这表明对于三角格点上的调和函数，某一点的值是其六个邻居的平均值。设 $\\phi_{i,j}$ 为节点 $(i,j)$ 处的数值解。对于内部节点，这产生了以下关系：\n$$6\\phi_{i,j} = \\phi_{i-1,j} + \\phi_{i+1,j} + \\phi_{i,j-1} + \\phi_{i,j+1} + \\phi_{i-1,j+1} + \\phi_{i+1,j-1}$$\n\n高斯-赛德尔（Gauss-Seidel）迭代求解 $\\phi_{i,j}$，并在一次扫描中使用邻居的最新更新值：\n$$\\phi_{i,j}^{\\text{GS}} = \\frac{1}{6} \\left( \\phi_{i-1,j} + \\phi_{i+1,j} + \\phi_{i,j-1} + \\phi_{i,j+1} + \\phi_{i-1,j+1} + \\phi_{i+1,j-1} \\right)$$\n逐次超松弛（SOR）方法通过使用松弛参数 $\\omega \\in (0,2)$ 将旧值 $\\phi_{i,j}^{\\text{old}}$ 与高斯-赛德尔更新值混合来加速收敛：\n$$\\phi_{i,j}^{\\text{new}} = (1-\\omega)\\phi_{i,j}^{\\text{old}} + \\omega \\phi_{i,j}^{\\text{GS}}$$\n代入 $\\phi_{i,j}^{\\text{GS}}$ 的表达式，我们得到将为每个内部节点实现的最终 SOR 更新模板：\n$$\\phi_{i,j}^{\\text{new}} = (1-\\omega)\\phi_{i,j}^{\\text{old}} + \\frac{\\omega}{6} \\left( \\phi_{i-1,j} + \\phi_{i+1,j} + \\phi_{i,j-1} + \\phi_{i,j+1} + \\phi_{i-1,j+1} + \\phi_{i+1,j-1} \\right)$$\n\n**B 和 C 部分：实现策略与误差评估**\n\n算法实现如下：\n$1$.  **网格初始化**：创建一个大小为 $(N_x, N_y)$ 的二维数组 $\\phi$。边界节点（其中 $i=0$、$i=N_x-1$、$j=0$ 或 $j=N_y-1$）使用提供的坐标映射，根据精确的狄利克雷条件 $u_{\\text{exact}}(x,y) = x^2 - y^2$ 的值进行初始化。内部节点初始化为 $0$。\n\n$2$.  **特殊情况**：如果 $N_x \\le 2$ 或 $N_y \\le 2$，则没有内部节点。在这种情况下，迭代次数定义为 $0$，最大误差 $E_{\\max}$ 定义为 $0$。\n\n$3$.  **SOR 迭代**：求解器进入一个主循环，最多运行 $K_{\\max}$ 次扫描。在每次扫描中：\n    a. 将变量 `max_change` 初始化为 $0$。\n    b. 算法遍历所有内部节点 $(i,j)$，其中 $j$ 从 $1$ 到 $N_y-2$，$i$ 从 $1$ 到 $N_x-2$。\n    c. 在每个内部节点，使用上面推导的 SOR 模板对值 $\\phi_{i,j}$ 进行原位更新。使用原位更新意味着在当前扫描中已经访问过的邻近节点上的 $\\phi$ 值被用于计算，这是高斯-赛德尔（Gauss-Seidel）方法的本质。\n    d. 计算 $\\phi_{i,j}$ 的新旧值之间的绝对差，并将 `max_change` 更新为本次扫描中迄今为止所见的最大此类变化。\n    e. 遍历所有内部节点后，检查收敛准则。如果 `max_change` 小于容差 $\\varepsilon$，则循环终止。\n\n$4$.  **误差评估**：迭代过程结束后，计算最大绝对误差 $E_{\\max}$。这包括遍历所有内部节点，并找到 $|\\phi_{i,j} - u_{\\text{exact}}(x(i,j), y(i,j))|$ 的最大值。如果没有内部节点，则将 $E_{\\max}$ 设置为 $0$。\n\n每个测试用例的最终输出是数对 $[ \\text{扫描次数}, E_{\\max} ]$，其中 `sweeps` 是完成的完整扫描次数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_cartesian_coords(i, j):\n    \"\"\"\n    Computes Cartesian coordinates from triangular lattice indices (i,j).\n    \"\"\"\n    x = float(i) + 0.5 * float(j)\n    y = (np.sqrt(3.0) / 2.0) * float(j)\n    return x, y\n\ndef u_exact(x, y):\n    \"\"\"\n    Computes the exact solution u(x,y) = x^2 - y^2.\n    \"\"\"\n    return x**2 - y**2\n\ndef solve_case(Nx, Ny, omega, epsilon, K_max):\n    \"\"\"\n    Solves the Laplace equation on a triangular lattice for a single set of parameters.\n    \"\"\"\n    # Handle special case: no interior nodes\n    if Nx <= 2 or Ny <= 2:\n        return 0, 0.0\n\n    # Initialize the grid phi. Using (Nx, Ny) shape to map (i,j) directly.\n    phi = np.zeros((Nx, Ny), dtype=np.float64)\n\n    # Set Dirichlet boundary conditions\n    for i in range(Nx):\n        for j in range(Ny):\n            if i == 0 or i == Nx - 1 or j == 0 or j == Ny - 1:\n                x, y = get_cartesian_coords(i, j)\n                phi[i, j] = u_exact(x, y)\n\n    # Main SOR iteration loop\n    sweeps = 0\n    for k in range(K_max):\n        sweeps = k + 1\n        max_change = 0.0\n        \n        # Iterate over all interior nodes\n        for j in range(1, Ny - 1):\n            for i in range(1, Nx - 1):\n                old_val = phi[i, j]\n                \n                # Sum of the six neighbors on the triangular lattice\n                neighbor_sum = (phi[i - 1, j] + phi[i + 1, j] +\n                                phi[i, j - 1] + phi[i, j + 1] +\n                                phi[i - 1, j + 1] + phi[i + 1, j - 1])\n                \n                # Apply the SOR update formula\n                new_val = (1.0 - omega) * old_val + (omega / 6.0) * neighbor_sum\n                phi[i, j] = new_val\n                \n                # Track the maximum change in this sweep for convergence check\n                max_change = max(max_change, abs(new_val - old_val))\n\n        # Check for convergence\n        if max_change < epsilon:\n            break\n            \n    # Calculate the maximum absolute error E_max at interior nodes\n    max_error = 0.0\n    for j in range(1, Ny - 1):\n        for i in range(1, Nx - 1):\n            x, y = get_cartesian_coords(i, j)\n            exact_val = u_exact(x, y)\n            error = abs(phi[i, j] - exact_val)\n            max_error = max(max_error, error)\n            \n    return sweeps, max_error\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (21, 21, 1.0, 1e-10, 10000),\n        (21, 21, 1.6, 1e-10, 10000),\n        (21, 21, 1.9, 1e-10, 10000),\n        (3, 3, 1.5, 1e-12, 1000),\n        (8, 2, 1.5, 1e-12, 1000),\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny, omega, epsilon, K_max = case\n        sweeps, e_max = solve_case(Nx, Ny, omega, epsilon, K_max)\n        results.append(f\"[{sweeps},{e_max}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}