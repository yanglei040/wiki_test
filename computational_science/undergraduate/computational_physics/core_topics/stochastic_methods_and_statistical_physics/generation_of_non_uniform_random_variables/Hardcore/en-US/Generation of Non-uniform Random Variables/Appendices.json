{
    "hands_on_practices": [
        {
            "introduction": "Mastering the inverse transform method begins with hands-on application. This first practice focuses on generating samples from a triangular distribution, a process that requires you to derive the cumulative distribution function (CDF) and its inverse from first principles . This exercise is fundamental, as it builds the core skill of translating a probability density function into a direct sampling algorithm, a common task in computational modeling.",
            "id": "2398091",
            "problem": "You are to write a complete, runnable program that generates independent samples from a triangular probability distribution without using any library function that directly generates such samples. Only independent samples from a continuous uniform distribution on the unit interval are permitted as primitives.\n\nLet the triangular distribution be defined by three real parameters $a$, $c$, and $b$ with $a  b$ and $a \\le c \\le b$. Its probability density function is zero outside $[a,b]$ and within $[a,b]$ is linear-increasing on $[a,c]$ and linear-decreasing on $[c,b]$, scaled so that the integral over $[a,b]$ equals $1$. Explicitly, for $x \\in \\mathbb{R}$,\n$$\nf(x; a,c,b) =\n\\begin{cases}\n\\dfrac{2(x-a)}{(b-a)(c-a)},  a \\le x \\le c,\\quad \\text{if } c > a,\\\\[6pt]\n\\dfrac{2(b-x)}{(b-a)(b-c)},  c  x \\le b,\\quad \\text{if } c  b,\\\\[6pt]\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n\nYour program must, for each specified test case, do all of the following using only first principles:\n1. Generate $N$ independent samples from the triangular distribution with the given $(a,c,b)$.\n2. Compute the sample mean and the sample variance (use the population normalization, that is, divide by $N$).\n3. Compute the exact cumulative distribution function $F(x; a,c,b)$ by integrating the given density, and then compute the Kolmogorov–Smirnov statistic $D_N = \\sup_x \\lvert F_N(x) - F(x)\\rvert$, where $F_N$ is the empirical cumulative distribution function of the generated samples.\n4. Compare the absolute error in the sample mean with the specified tolerance $\\delta_\\mu$, the absolute error in the sample variance with the specified tolerance $\\delta_{\\sigma^2}$, and the Kolmogorov–Smirnov statistic with the specified tolerance $\\delta_{\\mathrm{KS}}$. A test case is considered to pass if and only if all three comparisons are satisfied.\n\nUse only independent uniform variates on $[0,1]$ to construct the non-uniform samples. Your program must be deterministic: use the fixed seed $123456$ (base-10 integer) for the underlying uniform random number generator.\n\nTest suite. For each tuple $(a,c,b,N,\\delta_\\mu,\\delta_{\\sigma^2},\\delta_{\\mathrm{KS}})$ below, perform the procedure described above:\n- Test 1: $(0.0,\\,0.5,\\,1.0,\\,200000,\\,0.002,\\,0.003,\\,0.01)$\n- Test 2: $(0.0,\\,0.0,\\,5.0,\\,150000,\\,0.01,\\,0.03,\\,0.02)$\n- Test 3: $(-2.0,\\,1.0,\\,1.0,\\,140000,\\,0.01,\\,0.03,\\,0.02)$\n- Test 4: $(-3.0,\\,-1.0,\\,2.0,\\,160000,\\,0.008,\\,0.03,\\,0.02)$\n\nAnswer specification and final output format. For each of the four tests, return a boolean indicating whether the test passed. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests; for example, a valid output line looks like\n`[True,False,True,True]`.",
            "solution": "The problem statement is subjected to validation and is deemed valid. It constitutes a well-posed problem in computational physics, specifically in the generation of non-uniform random variables. The task is to generate samples from a triangular distribution using the inverse transform sampling method, and then to validate the generated sample against theoretical properties using standard statistical tests. The problem is scientifically grounded, objective, and provides all necessary information for a unique, verifiable solution. The slight lack of explicit definition for the probability density function (PDF) at the boundary cases where the mode $c$ equals one of the endpoints $a$ or $b$ is a minor imprecision that can be rigorously resolved by considering the limit or the geometry of the distribution, a standard exercise for any competent practitioner.\n\nThe solution proceeds in several steps: First, we derive the cumulative distribution function (CDF) and its inverse, which are essential for the inverse transform sampling method. Second, we state the theoretical mean and variance of the distribution. Third, we detail the algorithm for computing the Kolmogorov-Smirnov statistic.\n\nThe triangular distribution is defined by the probability density function (PDF) $f(x; a,c,b)$ over the support $x \\in [a,b]$:\n$$\nf(x; a,c,b) =\n\\begin{cases}\n\\dfrac{2(x-a)}{(b-a)(c-a)},  a \\le x \\le c \\\\\n\\dfrac{2(b-x)}{(b-a)(b-c)},  c  x \\le b\n\\end{cases}\n$$\nThis is valid for $a  c  b$. We will address the edge cases $c=a$ and $c=b$ during the derivation.\n\nThe chosen method for generating samples is the Inverse Transform Sampling method. This method relies on the fact that if $X$ is a continuous random variable with CDF $F_X(x)$, then the random variable $U = F_X(X)$ is uniformly distributed on $[0,1]$. Consequently, we can generate samples of $X$ by generating uniform random variates $u \\in [0,1]$ and computing $x = F_X^{-1}(u)$.\n\nDerivation of the Cumulative Distribution Function (CDF), $F(x) = \\int_{-\\infty}^x f(t) dt$:\nFor $x  a$, $F(x)=0$.\nFor $a \\le x \\le c$:\n$$\nF(x) = \\int_a^x \\frac{2(t-a)}{(b-a)(c-a)} dt = \\frac{2}{(b-a)(c-a)} \\left[ \\frac{(t-a)^2}{2} \\right]_a^x = \\frac{(x-a)^2}{(b-a)(c-a)}\n$$\nThe value at the mode $c$ is $F(c) = \\frac{(c-a)^2}{(b-a)(c-a)} = \\frac{c-a}{b-a}$.\n\nFor $c  x \\le b$:\n$$\nF(x) = F(c) + \\int_c^x \\frac{2(b-t)}{(b-a)(b-c)} dt = \\frac{c-a}{b-a} + \\left[ \\frac{-(b-t)^2}{(b-a)(b-c)} \\right]_c^x\n$$\n$$\nF(x) = \\frac{c-a}{b-a} + \\frac{-(b-x)^2 - (-(b-c)^2)}{(b-a)(b-c)} = \\frac{c-a}{b-a} + \\frac{(b-c)^2 - (b-x)^2}{(b-a)(b-c)}\n$$\nThis expression simplifies to a more elegant form:\n$$\nF(x) = 1 - \\frac{(b-x)^2}{(b-a)(b-c)}\n$$\nFor $x  b$, $F(x)=1$.\nIn summary, for the general case $a  c  b$:\n$$\nF(x; a,c,b) =\n\\begin{cases}\n0,  x  a \\\\\n\\frac{(x-a)^2}{(b-a)(c-a)},  a \\le x \\le c \\\\\n1 - \\frac{(b-x)^2}{(b-a)(b-c)},  c  x \\le b \\\\\n1,  x  b\n\\end{cases}\n$$\nFor the edge case $c=a$ (a right-triangular distribution decreasing from $a$ to $b$), the PDF is non-zero only for $x \\in [a,b]$, given by $f(x) = \\frac{2(b-x)}{(b-a)^2}$. The CDF becomes $F(x) = 1 - \\frac{(b-x)^2}{(b-a)^2}$ for $x \\in [a,b]$.\nFor the edge case $c=b$ (a right-triangular distribution increasing from $a$ to $b$), the PDF is $f(x) = \\frac{2(x-a)}{(b-a)^2}$ for $x \\in [a,b]$. The CDF becomes $F(x) = \\frac{(x-a)^2}{(b-a)^2}$ for $x \\in [a,b]$.\n\nDerivation of the Inverse CDF, $x = F^{-1}(u)$:\nWe set $u = F(x)$ for $u \\in [0,1]$ and solve for $x$.\nLet $F_c = F(c) = \\frac{c-a}{b-a}$.\nIf $0 \\le u \\le F_c$, this corresponds to $a \\le x \\le c$.\n$$ u = \\frac{(x-a)^2}{(b-a)(c-a)} \\implies x = a + \\sqrt{u(b-a)(c-a)} $$\nIf $F_c  u \\le 1$, this corresponds to $c  x \\le b$.\n$$ u = 1 - \\frac{(b-x)^2}{(b-a)(b-c)} \\implies x = b - \\sqrt{(1-u)(b-a)(b-c)} $$\nFor the edge cases, these formulas simplify.\nIf $c=a$, then $F_c=0$, and for any $u \\in [0,1]$ we use the second form, which becomes $x = b - \\sqrt{(1-u)(b-a)^2} = b - (b-a)\\sqrt{1-u}$.\nIf $c=b$, then $F_c=1$, and for any $u \\in [0,1]$ we use the first form, which becomes $x = a + \\sqrt{u(b-a)^2} = a + (b-a)\\sqrt{u}$.\n\nTheoretical and Sample Statistics:\nThe theoretical mean $\\mu$ and variance $\\sigma^2$ for a triangular distribution are given by:\n$$ \\mu = \\frac{a+b+c}{3} $$\n$$ \\sigma^2 = \\frac{(a-b)^2 + (b-c)^2 + (c-a)^2}{36} $$\nGiven a set of $N$ generated samples $\\{X_i\\}_{i=1}^N$, the sample mean $\\bar{X}$ and sample variance $S_N^2$ (with population normalization) are:\n$$ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i $$\n$$ S_N^2 = \\frac{1}{N} \\sum_{i=1}^N (X_i - \\bar{X})^2 $$\nThe absolute errors are then $|\\bar{X} - \\mu|$ and $|S_N^2 - \\sigma^2|$.\n\nKolmogorov-Smirnov Statistic:\nThe Kolmogorov-Smirnov (KS) statistic $D_N$ measures the maximum distance between the empirical distribution function (EDF) $F_N(x)$ and the theoretical CDF $F(x)$.\n$$ D_N = \\sup_x |F_N(x) - F(x)| $$\nThe EDF for a set of samples is $F_N(x) = \\frac{1}{N}\\sum_{i=1}^N I(X_i \\le x)$, where $I(\\cdot)$ is the indicator function.\nFor computational purposes, if we sort the samples such that $X_{(1)} \\le X_{(2)} \\le \\dots \\le X_{(N)}$, the statistic $D_N$ can be calculated efficiently using the formula:\n$$ D_N = \\max_{i=1,\\dots,N} \\left( \\frac{i}{N} - F(X_{(i)}) , F(X_{(i)}) - \\frac{i-1}{N} \\right) $$\nThe procedure for each test case is to generate $N$ samples, compute the sample mean, sample variance, and the KS statistic, and verify if their respective deviations from the theoretical values are within the specified tolerances $\\delta_\\mu$, $\\delta_{\\sigma^2}$, and $\\delta_{\\mathrm{KS}}$. A test passes if and only if all three conditions are met.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main solver function that executes the validation tests for generating\n    samples from a triangular distribution.\n    \"\"\"\n    test_cases = [\n        (0.0, 0.5, 1.0, 200000, 0.002, 0.003, 0.01),\n        (0.0, 0.0, 5.0, 150000, 0.01, 0.03, 0.02),\n        (-2.0, 1.0, 1.0, 140000, 0.01, 0.03, 0.02),\n        (-3.0, -1.0, 2.0, 160000, 0.008, 0.03, 0.02)\n    ]\n\n    results = []\n    \n    # Use a fixed seed for the random number generator for deterministic output.\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    for case in test_cases:\n        a, c, b, N, tol_mu, tol_var, tol_ks = case\n        \n        # Step 1: Generate N independent samples from the triangular distribution.\n        samples = _generate_triangular_samples(a, c, b, N, rng)\n        \n        # Step 2: Compute the sample mean and the sample variance.\n        sample_mean = np.mean(samples)\n        sample_var = np.var(samples) # Uses N in the denominator by default (ddof=0).\n\n        # Compute theoretical mean and variance.\n        true_mean = (a + b + c) / 3.0\n        true_var = ((a - b)**2 + (b - c)**2 + (c - a)**2) / 36.0\n\n        # Step 3: Compute the Kolmogorov–Smirnov statistic.\n        sorted_samples = np.sort(samples)\n        cdf_at_samples = _get_triangular_cdf(sorted_samples, a, c, b)\n        \n        i_vals = np.arange(1, N + 1)\n        d_plus = np.max(i_vals / N - cdf_at_samples)\n        d_minus = np.max(cdf_at_samples - (i_vals - 1) / N)\n        ks_stat = max(d_plus, d_minus)\n        \n        # Step 4: Compare errors with tolerances.\n        err_mean = abs(sample_mean - true_mean)\n        err_var = abs(sample_var - true_var)\n        \n        test_passed = (err_mean = tol_mu) and \\\n                      (err_var = tol_var) and \\\n                      (ks_stat = tol_ks)\n                      \n        results.append(test_passed)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_triangular_samples(a, c, b, N, rng):\n    \"\"\"\n    Generates N samples from a triangular distribution using inverse transform sampling.\n    \"\"\"\n    u = rng.uniform(0.0, 1.0, N)\n    samples = np.zeros(N, dtype=float)\n    \n    # Handle the edge cases of right-triangular distributions\n    if a == c:\n        # Decreasing right-triangular distribution\n        samples = b - (b - a) * np.sqrt(1 - u)\n    elif b == c:\n        # Increasing right-triangular distribution\n        samples = a + (b - a) * np.sqrt(u)\n    else:\n        # General scalene-triangular distribution\n        fc = (c - a) / (b - a)\n        term1_factor = (b - a) * (c - a)\n        term2_factor = (b - a) * (b - c)\n        \n        # Create a boolean mask for u = fc\n        mask = u = fc\n        \n        # For u = fc, use the first part of the inverse CDF\n        samples[mask] = a + np.sqrt(u[mask] * term1_factor)\n        \n        # For u  fc, use the second part\n        samples[~mask] = b - np.sqrt((1.0 - u[~mask]) * term2_factor)\n        \n    return samples\n\ndef _get_triangular_cdf(x, a, c, b):\n    \"\"\"\n    Computes the exact Cumulative Distribution Function (CDF) for a triangular distribution.\n    The input x can be a scalar or a numpy array for vectorized computation.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    cdf_vals = np.zeros_like(x, dtype=float)\n    \n    # Handle the edge cases of right-triangular distributions\n    if a == c:\n        # Mask for values within the support [a, b]\n        mask_mid = (x = a)  (x = b)\n        cdf_vals[mask_mid] = 1.0 - ((b - x[mask_mid])**2 / (b - a)**2)\n    elif b == c:\n        # Mask for values within the support [a, b]\n        mask_mid = (x = a)  (x = b)\n        cdf_vals[mask_mid] = (x[mask_mid] - a)**2 / (b - a)**2\n    else:\n        # General scalene-triangular distribution\n        mask1 = (x = a)  (x = c)\n        mask2 = (x  c)  (x = b)\n        \n        cdf_vals[mask1] = (x[mask1] - a)**2 / ((b - a) * (c - a))\n        cdf_vals[mask2] = 1.0 - (b - x[mask2])**2 / ((b - a) * (b - c))\n\n    # For values outside the support [a, b]\n    cdf_vals[x  b] = 1.0\n    cdf_vals[x  a] = 0.0\n\n    return cdf_vals\n\nsolve()\n```"
        },
        {
            "introduction": "We now advance from simple one-dimensional distributions to generating arguably the most important distribution in all of science: the Gaussian, or normal, distribution. This practice introduces the elegant Box-Muller transform, a classic technique for producing two independent standard normal variables from two uniform random numbers . Furthermore, you will learn the critical skill of transforming these independent variables into correlated Gaussian variables, an essential step for simulating complex systems where components interact and influence one another.",
            "id": "2398116",
            "problem": "You are to write a complete, runnable program that constructs non-uniform random variables in two dimensions starting only from independent uniform random numbers on the open interval $(0,1)$, and then validates the resulting samples against specified target properties. The tasks are as follows.\n\nTask. Given independent uniform random inputs on $(0,1)$, generate samples of a two-dimensional vector $\\mathbf{X} \\in \\mathbb{R}^{2}$ such that:\n- In the independent case, $\\mathbf{X}$ has the standard bivariate normal distribution with mean vector $\\boldsymbol{\\mu}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$ and covariance matrix $\\boldsymbol{\\Sigma}=\\mathbf{I}_{2}$, where $\\mathbf{I}_{2}$ denotes the $2 \\times 2$ identity matrix.\n- In the correlated case, $\\mathbf{X}$ has a bivariate normal distribution with a specified mean vector $\\boldsymbol{\\mu}\\in\\mathbb{R}^{2}$ and a specified symmetric positive-definite covariance matrix $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{2\\times 2}$.\n\nValidation. For a given sample size $N$, compute the empirical mean\n$$\n\\hat{\\boldsymbol{\\mu}}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{X}_{i}\n$$\nand the empirical covariance (using the population normalization)\n$$\n\\hat{\\boldsymbol{\\Sigma}}=\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbf{X}_{i}-\\hat{\\boldsymbol{\\mu}}\\right)\\left(\\mathbf{X}_{i}-\\hat{\\boldsymbol{\\mu}}\\right)^{\\top}.\n$$\nDefine the mean error $e_{\\mu}=\\max_{j\\in\\{1,2\\}}\\left|\\hat{\\mu}_{j}-\\mu_{j}\\right|$ and the covariance error $e_{\\Sigma}=\\max_{j,k\\in\\{1,2\\}}\\left|\\hat{\\Sigma}_{jk}-\\Sigma_{jk}\\right|$. A test is considered to pass if both $e_{\\mu}lt;\\varepsilon_{\\mu}$ and $e_{\\Sigma}lt;\\varepsilon_{\\Sigma}$ for the tolerances $\\varepsilon_{\\mu}$ and $\\varepsilon_{\\Sigma}$ specified for that test.\n\nRandomness and reproducibility. Use a fixed pseudorandom seed $s$ to initialize the uniform generator so that results are reproducible. Use the integer seed $s=123456789$.\n\nTest suite. Your program must run the following four tests in order, each test producing a single boolean according to the pass/fail rule above.\n\n- Test $\\mathbf{T1}$ (independent, large sample):\n  - Target parameters: $\\boldsymbol{\\mu}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}=\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$.\n  - Sample size: $N=200{,}000$.\n  - Tolerances: $\\varepsilon_{\\mu}=0.02$, $\\varepsilon_{\\Sigma}=0.02$.\n\n- Test $\\mathbf{T2}$ (independent, small sample edge case):\n  - Target parameters: $\\boldsymbol{\\mu}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}=\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}$.\n  - Sample size: $N=100$.\n  - Tolerances: $\\varepsilon_{\\mu}=0.30$, $\\varepsilon_{\\Sigma}=0.30$.\n\n- Test $\\mathbf{T3}$ (correlated, general case):\n  - Target parameters: $\\boldsymbol{\\mu}=\\begin{bmatrix}1.5 \\\\ -0.5\\end{bmatrix}$, $\\boldsymbol{\\Sigma}=\\begin{bmatrix}4.0  1.2 \\\\ 1.2  1.0\\end{bmatrix}$.\n  - Sample size: $N=300{,}000$.\n  - Tolerances: $\\varepsilon_{\\mu}=0.03$, $\\varepsilon_{\\Sigma}=0.03$.\n\n- Test $\\mathbf{T4}$ (correlated, near-boundary correlation):\n  - Target parameters: $\\boldsymbol{\\mu}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $\\boldsymbol{\\Sigma}=\\begin{bmatrix}1.0  -0.95 \\\\ -0.95  1.0\\end{bmatrix}$.\n  - Sample size: $N=300{,}000$.\n  - Tolerances: $\\varepsilon_{\\mu}=0.03$, $\\varepsilon_{\\Sigma}=0.04$.\n\nOutput specification. Your program should produce a single line of output containing the results of the four tests $\\mathbf{T1}$, $\\mathbf{T2}$, $\\mathbf{T3}$, and $\\mathbf{T4}$, respectively, as a comma-separated list of booleans enclosed in square brackets, for example\n`[True,False,True,True]`.\nNo other output is permitted.",
            "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- The task is to generate samples of a two-dimensional vector $\\mathbf{X} \\in \\mathbb{R}^{2}$ from independent uniform random numbers on the open interval $(0,1)$.\n- For the independent case, the target is a standard bivariate normal distribution with mean vector $\\boldsymbol{\\mu}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$ and covariance matrix $\\boldsymbol{\\Sigma}=\\mathbf{I}_{2}$.\n- For the correlated case, the target is a bivariate normal distribution with a specified mean vector $\\boldsymbol{\\mu}\\in\\mathbb{R}^{2}$ and a specified symmetric positive-definite covariance matrix $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{2\\times 2}$.\n- Validation requires computing the empirical mean $\\hat{\\boldsymbol{\\mu}}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{X}_{i}$ and empirical covariance $\\hat{\\boldsymbol{\\Sigma}}=\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbf{X}_{i}-\\hat{\\boldsymbol{\\mu}}\\right)\\left(\\mathbf{X}_{i}-\\hat{\\boldsymbol{\\mu}}\\right)^{\\top}$ for a sample of size $N$.\n- The errors are defined as $e_{\\mu}=\\max_{j\\in\\{1,2\\}}\\left|\\hat{\\mu}_{j}-\\mu_{j}\\right|$ and $e_{\\Sigma}=\\max_{j,k\\in\\{1,2\\}}\\left|\\hat{\\Sigma}_{jk}-\\Sigma_{jk}\\right|$.\n- A test passes if $e_{\\mu}lt;\\varepsilon_{\\mu}$ and $e_{\\Sigma}lt;\\varepsilon_{\\Sigma}$.\n- A fixed pseudorandom seed $s=123456789$ must be used.\n- Four test cases ($\\mathbf{T1}$, $\\mathbf{T2}$, $\\mathbf{T3}$, $\\mathbf{T4}$) are specified with their respective parameters ($\\boldsymbol{\\mu}$, $\\boldsymbol{\\Sigma}$, $N$, $\\varepsilon_{\\mu}$, $\\varepsilon_{\\Sigma}$).\n\nThe problem is validated against the required criteria. It is scientifically grounded, well-posed, and objective. It concerns a standard and fundamental technique in computational physics and statistics: the generation of non-uniform random variates. The theoretical basis, leveraging methods like the Box-Muller transform and Cholesky decomposition, is sound. All parameters and definitions required for a unique, verifiable solution are provided. The covariance matrices specified for the correlated tests are confirmed to be symmetric and positive-definite, ensuring a valid Cholesky decomposition exists. For test $\\mathbf{T3}$, $\\det(\\boldsymbol{\\Sigma}) = 4.0 \\times 1.0 - 1.2^2 = 2.56  0$. For test $\\mathbf{T4}$, $\\det(\\boldsymbol{\\Sigma}) = 1.0 \\times 1.0 - (-0.95)^2 = 0.0975  0$. The problem is therefore deemed valid.\n\nThe solution is constructed in three parts: generation of standard normal random variates, transformation to a general bivariate normal distribution, and empirical validation.\n\nFirst, we address the generation of independent standard normal random variables from a uniform distribution. The problem mandates starting from independent random numbers $U_{1}, U_{2}$ drawn from a uniform distribution on the open interval $(0,1)$. A standard and efficient method for this is the Box-Muller transform. This transform takes two such uniform variates and produces two independent standard normal variates, $Z_{1}, Z_{2} \\sim \\mathcal{N}(0,1)$, via the following relations:\n$$\nZ_1 = \\sqrt{-2 \\ln U_1} \\cos(2 \\pi U_2)\n$$\n$$\nZ_2 = \\sqrt{-2 \\ln U_1} \\sin(2 \\pi U_2)\n$$\nThese two variables form a random vector $\\mathbf{Z} = \\begin{bmatrix} Z_1 \\\\ Z_2 \\end{bmatrix}$ which follows the standard bivariate normal distribution, $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_2)$. This procedure is sufficient for generating the samples required in tests $\\mathbf{T1}$ and $\\mathbf{T2}$, where the target distribution is indeed $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_2)$.\n\nSecond, we develop the method for generating samples from a general bivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, as required for tests $\\mathbf{T3}$ and $\\mathbf{T4}$. This is achieved by applying an affine transformation to the standard normal vector $\\mathbf{Z}$ produced in the first step. Let the desired random vector be $\\mathbf{X}$. We seek a transformation of the form $\\mathbf{X} = \\mathbf{A}\\mathbf{Z} + \\mathbf{b}$ for some matrix $\\mathbf{A}$ and vector $\\mathbf{b}$. The expectation of $\\mathbf{X}$ is $E[\\mathbf{X}] = E[\\mathbf{A}\\mathbf{Z} + \\mathbf{b}] = \\mathbf{A}E[\\mathbf{Z}] + \\mathbf{b}$. Since $E[\\mathbf{Z}] = \\mathbf{0}$, this simplifies to $E[\\mathbf{X}] = \\mathbf{b}$. To match the target mean $\\boldsymbol{\\mu}$, we must set $\\mathbf{b} = \\boldsymbol{\\mu}$. The covariance matrix of $\\mathbf{X}$ is given by:\n$$\n\\text{Cov}(\\mathbf{X}) = E\\left[ (\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})^{\\top} \\right] = E\\left[ (\\mathbf{A}\\mathbf{Z})(\\mathbf{A}\\mathbf{Z})^{\\top} \\right] = E\\left[ \\mathbf{A}\\mathbf{Z}\\mathbf{Z}^{\\top}\\mathbf{A}^{\\top} \\right] = \\mathbf{A} E[\\mathbf{Z}\\mathbf{Z}^{\\top}] \\mathbf{A}^{\\top}\n$$\nThe matrix $E[\\mathbf{Z}\\mathbf{Z}^{\\top}]$ is the covariance matrix of $\\mathbf{Z}$, which is the identity matrix $\\mathbf{I}_2$. Thus, $\\text{Cov}(\\mathbf{X}) = \\mathbf{A}\\mathbf{A}^{\\top}$. To match the target covariance matrix $\\boldsymbol{\\Sigma}$, we must find a matrix $\\mathbf{A}$ such that $\\mathbf{A}\\mathbf{A}^{\\top} = \\boldsymbol{\\Sigma}$. Since the problem specifies that $\\boldsymbol{\\Sigma}$ is a symmetric positive-definite matrix, a solution for $\\mathbf{A}$ can be found using the Cholesky decomposition. The Cholesky decomposition of $\\boldsymbol{\\Sigma}$ yields a unique lower triangular matrix $\\mathbf{L}$ with positive diagonal entries such that $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^{\\top}$. We choose $\\mathbf{A} = \\mathbf{L}$.\nThe complete transformation is therefore:\n$$\n\\mathbf{X} = \\mathbf{L}\\mathbf{Z} + \\boldsymbol{\\mu}\n$$\nwhere $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_2)$ and $\\mathbf{L}$ is the Cholesky factor of $\\boldsymbol{\\Sigma}$. To generate $N$ samples $\\mathbf{X}_i$ for a given test, we first generate $N$ independent vectors $\\mathbf{Z}_i$ using the Box-Muller transform, then apply the affine transformation to each.\n\nThird, the validation procedure is implemented as specified. For each test, after generating a sample of size $N$, $\\{\\mathbf{X}_1, \\dots, \\mathbf{X}_N\\}$, the empirical mean vector $\\hat{\\boldsymbol{\\mu}}$ is calculated as the arithmetic average of the samples. The data is then centered by subtracting this empirical mean. Let the centered data matrix be $\\mathbf{D}_{\\text{cent}}$, of size $N \\times 2$, where each row is $(\\mathbf{X}_i - \\hat{\\boldsymbol{\\mu}})^T$. The empirical covariance matrix $\\hat{\\boldsymbol{\\Sigma}}$ is then computed efficiently using matrix multiplication, which is equivalent to the provided summation formula:\n$$\n\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\mathbf{D}_{\\text{cent}}^{\\top} \\mathbf{D}_{\\text{cent}}\n$$\nFinally, the maximum absolute element-wise errors $e_{\\mu} = \\max_{j} |\\hat{\\mu}_j - \\mu_j|$ and $e_{\\Sigma} = \\max_{j,k} |\\hat{\\Sigma}_{jk} - \\Sigma_{jk}|$ are computed and compared against the specified tolerances $\\varepsilon_{\\mu}$ and $\\varepsilon_{\\Sigma}$ to determine if the test passes. The entire process is executed for each of the four test cases using the fixed random seed $s=123456789$ for reproducibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of generating and validating non-uniform random variables.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # T1: independent, large sample\n        {\n            \"name\": \"T1\",\n            \"mu\": np.array([0.0, 0.0]),\n            \"Sigma\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"N\": 200000,\n            \"eps_mu\": 0.02,\n            \"eps_Sigma\": 0.02,\n        },\n        # T2: independent, small sample\n        {\n            \"name\": \"T2\",\n            \"mu\": np.array([0.0, 0.0]),\n            \"Sigma\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"N\": 100,\n            \"eps_mu\": 0.30,\n            \"eps_Sigma\": 0.30,\n        },\n        # T3: correlated, general case\n        {\n            \"name\": \"T3\",\n            \"mu\": np.array([1.5, -0.5]),\n            \"Sigma\": np.array([[4.0, 1.2], [1.2, 1.0]]),\n            \"N\": 300000,\n            \"eps_mu\": 0.03,\n            \"eps_Sigma\": 0.03,\n        },\n        # T4: correlated, near-boundary correlation\n        {\n            \"name\": \"T4\",\n            \"mu\": np.array([0.0, 0.0]),\n            \"Sigma\": np.array([[1.0, -0.95], [-0.95, 1.0]]),\n            \"N\": 300000,\n            \"eps_mu\": 0.03,\n            \"eps_Sigma\": 0.04,\n        },\n    ]\n\n    # Initialize the random number generator with the specified seed for reproducibility.\n    seed = 123456789\n    rng = np.random.default_rng(seed)\n\n    results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        mu_target = case[\"mu\"]\n        Sigma_target = case[\"Sigma\"]\n        eps_mu = case[\"eps_mu\"]\n        eps_Sigma = case[\"eps_Sigma\"]\n\n        # Step 1: Generate uniform random numbers in (0, 1).\n        # We need N pairs of uniform numbers for the Box-Muller transform.\n        # To strictly be in (0, 1), we can use np.nextafter, but for practical purposes\n        # rng.uniform is sufficient as P(U=0)=0.\n        u = rng.uniform(low=np.nextafter(0, 1), high=1, size=(N, 2))\n        U1 = u[:, 0]\n        U2 = u[:, 1]\n        \n        # Step 2: Apply Box-Muller transform to get standard normal variates.\n        R = np.sqrt(-2.0 * np.log(U1))\n        theta = 2.0 * np.pi * U2\n        \n        Z1 = R * np.cos(theta)\n        Z2 = R * np.sin(theta)\n        \n        # Z is an (N, 2) matrix of standard normal samples.\n        Z = np.vstack([Z1, Z2]).T\n\n        # Step 3: Transform standard normal samples to the target distribution.\n        # X = L @ Z.T + mu, where L is the Cholesky factor of Sigma.\n        if np.array_equal(Sigma_target, np.eye(2)):\n            # For the standard normal case, L is the identity matrix.\n            L = np.eye(2)\n        else:\n            # For the general case, compute the Cholesky decomposition.\n            L = np.linalg.cholesky(Sigma_target)\n\n        # Apply the affine transformation.\n        # Z.T is (2, N). L @ Z.T is (2, N). Transpose to get (N, 2).\n        # Add mu (broadcasts over rows).\n        X = (L @ Z.T).T + mu_target\n\n        # Step 4: Validate the generated samples.\n        # Compute empirical mean.\n        mu_hat = np.mean(X, axis=0)\n\n        # Compute empirical covariance with population normalization (ddof=0).\n        # np.cov calculates this directly. rowvar=False treats columns as variables.\n        Sigma_hat = np.cov(X, rowvar=False, ddof=0)\n        \n        # Compute errors.\n        e_mu = np.max(np.abs(mu_hat - mu_target))\n        e_Sigma = np.max(np.abs(Sigma_hat - Sigma_target))\n        \n        # Check against tolerances.\n        test_passed = (e_mu  eps_mu) and (e_Sigma  eps_Sigma)\n        results.append(test_passed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice addresses a common real-world scenario: what happens when the inverse CDF cannot be expressed with a simple analytical formula? This exercise challenges you to sample from a general polynomial distribution, where direct inversion is often intractable . By combining the theoretical framework of the inverse transform method with numerical root-finding techniques, you will build a robust sampler, demonstrating the power and generality of this approach for virtually any well-behaved continuous distribution.",
            "id": "2398130",
            "problem": "You are given a nonnegative polynomial $P(x) = \\sum_{i=0}^{N} c_i x^i$ defined on a finite interval $[a,b]$, where $N$ is a nonnegative integer and $c_i \\in \\mathbb{R}$ for all $i \\in \\{0,1,\\dots,N\\}$. Assume that $P(x) \\ge 0$ for all $x \\in [a,b]$ and that $\\int_a^b P(x)\\,dx  0$. Define the normalized probability density function $p(x)$ on $[a,b]$ by\n$$\np(x) = \\frac{P(x)}{Z}, \\quad \\text{where } Z = \\int_a^b P(x)\\,dx.\n$$\nLet the cumulative distribution function (CDF) $F(x)$ of $p(x)$ be\n$$\nF(x) = \\int_a^x p(t)\\,dt, \\quad x \\in [a,b].\n$$\nYour task is to implement a program that, for each test case described below, generates independent random variates distributed according to $p(x)$ on $[a,b]$ and then evaluates quantitative diagnostics comparing the generated samples to the exact distribution derived from $P(x)$.\n\nFor each test case, you must:\n- Use the provided polynomial coefficients $\\{c_i\\}_{i=0}^N$, interval endpoints $a$ and $b$, a sample size $n$, and a random seed $s$.\n- Generate $n$ independent samples with density $p(x)$ on $[a,b]$.\n- Compute the exact normalization constant $Z = \\int_a^b P(x)\\,dx$.\n- Compute the exact moments $\\mathbb{E}[X]$ and $\\mathbb{E}[X^2]$ under $p(x)$ via\n$$\n\\mathbb{E}[X^m] = \\frac{\\int_a^b x^m P(x)\\,dx}{\\int_a^b P(x)\\,dx}, \\quad \\text{for } m \\in \\{1,2\\}.\n$$\n- From the generated samples, compute the empirical mean and the empirical second moment (that is, the average of $X^2$ over the samples).\n- Report, for each test case, three real numbers:\n  1) the absolute error in the sample mean relative to the exact $\\mathbb{E}[X]$,\n  2) the absolute error in the sample second moment relative to the exact $\\mathbb{E}[X^2]$,\n  3) the Kolmogorov–Smirnov statistic $D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F(x)|$, where $F_n$ is the empirical cumulative distribution function based on the $n$ samples and $F$ is the exact CDF induced by $P(x)$ on $[a,b]$ and extended by $0$ for $x  a$ and $1$ for $x  b$.\n\nTest suite:\n- Case $1$: $N = 0$, coefficients $[c_0] = [1.0]$, interval $[a,b] = [0.0, 1.0]$, sample size $n = 20000$, seed $s = 12345$.\n- Case $2$: $N = 1$, coefficients $[c_0,c_1] = [0.0, 1.0]$, interval $[a,b] = [0.0, 1.0]$, sample size $n = 30000$, seed $s = 2024$.\n- Case $3$: $N = 2$, coefficients $[c_0,c_1,c_2] = [0.35, -1.0, 1.0]$, interval $[a,b] = [0.0, 1.0]$, sample size $n = 40000$, seed $s = 424242$.\n- Case $4$: $N = 2$, coefficients $[c_0,c_1,c_2] = [2.0, 1.0, -1.0]$, interval $[a,b] = [-1.0, 2.0]$, sample size $n = 50000$, seed $s = 731$.\n\nFinal output format:\n- Your program must produce a single line of output containing all results aggregated in order, as a comma-separated list enclosed in square brackets.\n- The order must be, for Case $1$ through Case $4$: $[\\Delta \\mu_1, \\Delta m_{2,1}, D_{n,1}, \\Delta \\mu_2, \\Delta m_{2,2}, D_{n,2}, \\Delta \\mu_3, \\Delta m_{2,3}, D_{n,3}, \\Delta \\mu_4, \\Delta m_{2,4}, D_{n,4}]$, where $\\Delta \\mu_k$ is the absolute error in the sample mean for Case $k$, $\\Delta m_{2,k}$ is the absolute error in the sample second moment for Case $k$, and $D_{n,k}$ is the Kolmogorov–Smirnov statistic for Case $k$.\n- Each real number in the output must be rounded to exactly $6$ decimal places.\n- For example, the format is like [$r_1, r_2, \\dots, r_{12}$] with each $r_j$ a real number printed with exactly $6$ decimal places.",
            "solution": "The problem statement has been validated and is deemed acceptable. It is scientifically grounded, well-posed, and contains all necessary information to derive a unique and verifiable solution. The problem lies within the standard domain of computational physicsconcerned with the generation of non-uniform random variables. The premises for each test case—specifically, the non-negativity of the polynomial $P(x)$ on the specified interval $[a,b]$—have been verified and hold true.\n\nThe solution proceeds systematically, based on fundamental principles of probability theory and numerical methods.\n\nFirst, we formalize the probability distribution associated with the given polynomial $P(x) = \\sum_{i=0}^{N} c_i x^i$ on the interval $[a,b]$. The unnormalized density is $P(x)$. To create a valid probability density function (PDF), $p(x)$, we must normalize $P(x)$ by its integral over the domain. The normalization constant, $Z$, is given by:\n$$\nZ = \\int_a^b P(x) \\,dx\n$$\nSince $P(x)$ is a polynomial, its indefinite integral, which we denote $Q(x) = \\int P(x) \\,dx$, is also a polynomial. Specifically,\n$$\nQ(x) = \\sum_{i=0}^{N} c_i \\frac{x^{i+1}}{i+1}\n$$\nBy the fundamental theorem of calculus, the normalization constant is simply $Z = Q(b) - Q(a)$. The PDF is then correctly defined as $p(x) = P(x) / Z$ for $x \\in [a,b]$, and $p(x) = 0$ otherwise.\n\nThe cumulative distribution function (CDF), $F(x)$, is the integral of the PDF from the lower bound of the interval to $x$:\n$$\nF(x) = \\int_a^x p(t) \\,dt = \\frac{1}{Z} \\int_a^x P(t) \\,dt = \\frac{Q(x) - Q(a)}{Z}\n$$\nThis function $F(x)$ maps the interval $[a,b]$ to $[0,1]$ and is monotonically non-decreasing.\n\nSecond, we address the generation of random variates from the distribution $p(x)$. The most direct and efficient method for this task, when the CDF is known, is the inverse transform sampling method. This method states that if $U$ is a random variable drawn from a uniform distribution on $[0,1]$, then the random variable $X = F^{-1}(U)$ will be distributed according to the PDF $p(x)$ with CDF $F(x)$. To generate a sample, we first draw a value $u$ from $\\mathcal{U}(0,1)$ and then solve the equation $F(x) = u$ for $x$. This is equivalent to solving the polynomial equation:\n$$\n\\frac{Q(x) - Q(a)}{Z} = u \\implies Q(x) - (Z u + Q(a)) = 0\n$$\nFor the polynomial degrees specified in the problem ($N \\le 2$), the degree of $Q(x)$ is at most $3$. While analytical solutions exist for cubic equations, a numerical approach is more robust and general. Since $F(x)$ is a monotonic function, for any $u \\in [0,1]$, there exists a unique root $x \\in [a,b]$ for the equation $F(x) - u = 0$. This root can be reliably found using a numerical root-finding algorithm, such as Brent's method, which guarantees convergence provided the root is bracketed, as it is here by $[a,b]$.\n\nThird, we calculate the exact moments of the distribution for comparison. The $m$-th moment of the distribution, $\\mathbb{E}[X^m]$, is defined as:\n$$\n\\mathbb{E}[X^m] = \\int_a^b x^m p(x) \\,dx = \\frac{1}{Z} \\int_a^b x^m P(x) \\,dx\n$$\nThe integrand $x^m P(x)$ is another polynomial, so its definite integral can be calculated exactly without numerical error.\n$$\n\\int_a^b x^m P(x) \\,dx = \\int_a^b \\left( \\sum_{i=0}^{N} c_i x^{i+m} \\right) \\,dx = \\sum_{i=0}^{N} c_i \\left[ \\frac{x^{i+m+1}}{i+m+1} \\right]_a^b\n$$\nThis allows for the precise computation of the theoretical mean $\\mathbb{E}[X]$ (for $m=1$) and the second moment $\\mathbb{E}[X^2]$ (for $m=2$).\n\nFinally, we evaluate the quality of the generated sample set $\\{x_j\\}_{j=1}^n$. The empirical mean $\\hat{\\mu}$ and empirical second moment $\\hat{m}_2$ are computed as:\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^n x_j \\quad \\text{and} \\quad \\hat{m}_2 = \\frac{1}{n} \\sum_{j=1}^n x_j^2\n$$\nThe absolute errors $|\\hat{\\mu} - \\mathbb{E}[X]|$ and $|\\hat{m}_2 - \\mathbb{E}[X^2]|$ quantify the deviation of sample moments from their theoretical values. Furthermore, the Kolmogorov-Smirnov (KS) statistic, $D_n$, is used to measure the overall goodness-of-fit. It is defined as the maximum absolute difference between the empirical CDF $F_n(x)$ and the theoretical CDF $F(x)$ over the entire real line:\n$$\nD_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F(x)|\n$$\nHere, $F_n(x) = \\frac{1}{n}\\sum_{j=1}^{n}\\mathbb{I}(x_j \\le x)$, where $\\mathbb{I}$ is the indicator function. The KS statistic provides a robust measure of the quality of the random number generation process.\n\nThe implementation will follow these steps precisely for each test case, using the provided parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It generates samples from polynomial-based probability distributions,\n    calculates exact and empirical statistics, and reports the errors.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'N': 0, 'coeffs': [1.0], 'interval': [0.0, 1.0], 'n': 20000, 's': 12345},\n        {'N': 1, 'coeffs': [0.0, 1.0], 'interval': [0.0, 1.0], 'n': 30000, 's': 2024},\n        {'N': 2, 'coeffs': [0.35, -1.0, 1.0], 'interval': [0.0, 1.0], 'n': 40000, 's': 424242},\n        {'N': 2, 'coeffs': [2.0, 1.0, -1.0], 'interval': [-1.0, 2.0], 'n': 50000, 's': 731}\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # Note: coeffs are [c0, c1, ..., cN]. numpy.poly1d wants [cN, ..., c0].\n        coeffs = np.array(case['coeffs'])\n        a, b = case['interval']\n        n = case['n']\n        seed = case['s']\n\n        # 1. Theoretical Calculations\n        # Represent the polynomial P(x)\n        P_poly = np.poly1d(coeffs[::-1])\n        \n        # Q(x) is the indefinite integral of P(x)\n        Q_poly = P_poly.integ()\n        \n        # Z is the normalization constant\n        Z = Q_poly(b) - Q_poly(a)\n        \n        # Calculate theoretical moments E[X^m] = (1/Z) * integral(x^m * P(x) dx)\n        # The integrand for E[X^m] is x^m * P(x)\n        P_moment1 = np.poly1d([1.0, 0.0]) * P_poly  # This is x * P(x)\n        Q_moment1 = P_moment1.integ()\n        exact_mean = (Q_moment1(b) - Q_moment1(a)) / Z\n        \n        P_moment2 = np.poly1d([1.0, 0.0, 0.0]) * P_poly # This is x^2 * P(x)\n        Q_moment2 = P_moment2.integ()\n        exact_mean_sq = (Q_moment2(b) - Q_moment2(a)) / Z\n        \n        # Define the theoretical CDF, F(x)\n        def cdf(x):\n            # Handle array inputs for kstest\n            x = np.asarray(x)\n            # Clip x to the interval [a, b] for calculation, assuming 0 before a and 1 after b\n            vals = np.clip(x, a, b)\n            f_vals = (Q_poly(vals) - Q_poly(a)) / Z\n            # Apply boundary conditions of a CDF\n            f_vals[x  a] = 0.0\n            f_vals[x  b] = 1.0\n            return f_vals\n\n        # 2. Sample Generation\n        rng = np.random.default_rng(seed)\n        uniform_samples = rng.random(size=n)\n        \n        # Function to find root of: F(x) - u = 0\n        def root_fn(x, u):\n            return (Q_poly(x) - Q_poly(a)) / Z - u\n        \n        # Generate samples using inverse transform sampling with a numerical root finder\n        samples = np.zeros(n)\n        for i in range(n):\n            u = uniform_samples[i]\n            # Brent's method is robust for finding the unique root in [a,b]\n            samples[i] = brentq(root_fn, a, b, args=(u,))\n        \n        # 3. Empirical Diagnostics\n        sample_mean = np.mean(samples)\n        sample_mean_sq = np.mean(samples**2)\n        \n        mean_abs_error = abs(sample_mean - exact_mean)\n        mean_sq_abs_error = abs(sample_mean_sq - exact_mean_sq)\n        \n        # Kolmogorov-Smirnov test\n        ks_result = stats.kstest(samples, cdf)\n        ks_stat = ks_result.statistic\n        \n        # 4. Store the results for this case\n        all_results.extend([mean_abs_error, mean_sq_abs_error, ks_stat])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}