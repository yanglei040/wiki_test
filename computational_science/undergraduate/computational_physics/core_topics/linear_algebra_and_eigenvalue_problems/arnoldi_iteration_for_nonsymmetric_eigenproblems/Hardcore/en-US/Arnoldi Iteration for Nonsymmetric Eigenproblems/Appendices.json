{
    "hands_on_practices": [
        {
            "introduction": "This first exercise focuses on building the Arnoldi iteration from the ground up. You will implement the algorithm in a \"matrix-free\" context, where the matrix $A$ is only accessible through a function that computes the product $Ax$. This approach is fundamental in computational physics for tackling large-scale eigenproblems where storing the matrix explicitly is impossible or impractical .",
            "id": "2373518",
            "problem": "You are given the task of computing approximations to the spectral radii of several real, nonsymmetric linear operators using only the ability to apply each operator to a vector. For a finite-dimensional real vector space of dimension $n$, let $A:\\mathbb{R}^n \\to \\mathbb{R}^n$ be a linear operator. For a chosen positive integer $m$ and a nonzero starting vector $v_0 \\in \\mathbb{R}^n$, consider the $m$-dimensional Krylov subspace $K_m(A, v_0) = \\mathrm{span}\\{v_0, A v_0, A^2 v_0, \\dots, A^{m-1} v_0\\}$. Let $V_m \\in \\mathbb{R}^{n \\times m}$ denote a column-orthonormal basis for $K_m(A, v_0)$ and let $H_m = V_m^\\top A V_m \\in \\mathbb{R}^{m \\times m}$ denote the orthogonal projection of $A$ onto this subspace. Define the Ritz values as the eigenvalues of $H_m$, and define the Ritz estimate of the spectral radius as the largest magnitude among these Ritz values. The true spectral radius of $A$ is $\\rho(A) = \\max\\{|\\lambda| : \\lambda \\in \\sigma(A)\\}$, where $\\sigma(A)$ is the spectrum of $A$.\n\nYour program must compute, for each of the test cases listed below, the absolute error between the Ritz estimate (computed from the eigenvalues of $H_m$) and the true spectral radius $\\rho(A)$, using a prescribed initial vector. The program must not access matrix entries of $A$ directly; it must only use the action of $A$ on a vector. All angles in trigonometric functions are to be interpreted in radians. The final output must be a single line containing a list of floating-point numbers corresponding to the absolute errors for each test case, rounded to eight decimal places.\n\nUse the following test suite. In every case, the initial vector $v_0 \\in \\mathbb{R}^n$ is defined componentwise by\n$$\n(v_0)_i = \\sin(i) + \\tfrac{1}{2}\\cos(2 i), \\quad i=1,2,\\dots,n,\n$$\nfollowed by normalization to unit Euclidean norm $\\|(v_0)\\|_2 = 1$.\n\n- Test case $1$ (tridiagonal Toeplitz operator):\n  - Dimension: $n = 80$.\n  - Parameters: $a = 1$, $b = 2$, $c = \\tfrac{1}{2}$.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x \\in \\mathbb{R}^n$ by\n    $$\n    y_1 = a x_1 + b x_2,\\quad\n    y_i = c x_{i-1} + a x_i + b x_{i+1}\\ \\text{for}\\ i=2,\\dots,n-1,\\quad\n    y_n = c x_{n-1} + a x_n.\n    $$\n  - Subspace dimension: $m = 25$.\n  - True spectral radius: for this operator, the eigenvalues are $a + 2\\sqrt{b c}\\cos\\left(\\tfrac{j\\pi}{n+1}\\right)$ for $j=1,2,\\dots,n$, hence\n    $$\n    \\rho(A) = \\max\\Big\\{\\left|a + 2\\sqrt{b c}\\right|,\\ \\left|a - 2\\sqrt{b c}\\right|\\Big\\}.\n    $$\n\n- Test case $2$ (Jordan block):\n  - Dimension: $n = 60$.\n  - Parameter: $\\lambda = 3$.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x \\in \\mathbb{R}^n$ by\n    $$\n    y_i = \\lambda x_i + x_{i+1}\\ \\text{for}\\ i=1,2,\\dots,n-1,\\quad\n    y_n = \\lambda x_n.\n    $$\n  - Subspace dimension: $m = 15$.\n  - True spectral radius: $\\rho(A) = |\\lambda|$.\n\n- Test case $3$ (similarity transform with diagonal spectrum):\n  - Dimension: $n = 50$.\n  - Diagonal spectrum: define $d_{\\max} = 5$, $d_{\\min} = 1$, and\n    $$\n    d_i = d_{\\min} + (d_{\\max} - d_{\\min})\\frac{n - i}{n - 1},\\quad i=1,2,\\dots,n.\n    $$\n    Let $D \\in \\mathbb{R}^{n \\times n}$ be diagonal with entries $d_i$.\n  - Similarity matrix: define $S \\in \\mathbb{R}^{n \\times n}$ by\n    $$\n    S_{ij} = \\delta_{ij} + 10^{-2}\\sin\\!\\Big(\\frac{(i)(j)}{n+1}\\Big),\\quad i,j=1,2,\\dots,n,\n    $$\n    where $\\delta_{ij}$ is the Kronecker delta.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x$ by $y = S D S^{-1} x$.\n  - Subspace dimension: $m = 30$.\n  - True spectral radius: $\\rho(A) = \\max_i |d_i| = d_{\\max}$.\n\n- Test case $4$ (banded nilpotent perturbation of a scalar multiple of the identity):\n  - Dimension: $n = 100$.\n  - Parameters: $\\alpha = 2$, bandwidth $p = 3$.\n  - Operator action: for any $x \\in \\mathbb{R}^n$, define $y = A x \\in \\mathbb{R}^n$ by\n    $$\n    y = \\alpha x + \\sum_{k=1}^{p} N_k x,\n    $$\n    where $(N_k x)_i = x_{i+k}$ for $i=1,2,\\dots,n-k$, and $(N_k x)_i = 0$ for $i>n-k$.\n  - Subspace dimension: $m = 50$.\n  - True spectral radius: since $A = \\alpha I + N$ with $N$ nilpotent, $\\rho(A) = |\\alpha|$.\n\nFor each test case, compute the absolute error\n$$\n\\varepsilon = \\left|\\max_{\\mu \\in \\sigma(H_m)} |\\mu| - \\rho(A)\\right|,\n$$\nwhere $H_m$ is the orthogonal projection of $A$ onto the $m$-dimensional Krylov subspace generated by the normalized $v_0$. If the Krylov subspace has dimension strictly less than $m$ due to exact or near breakdown, use the actual constructed subspace dimension in forming $H_m$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above, with each number rounded to eight decimal places (for example, $[0.12345678,0.00000000,0.5,1.23456789]$). No other text should be printed.",
            "solution": "The problem requires the computation of the absolute error between the true spectral radius of a linear operator $A$ and an approximation obtained via the Arnoldi iteration. The spectral radius, $\\rho(A)$, is defined as the maximum absolute value of the eigenvalues of $A$. The Arnoldi iteration is a numerical method for finding a subset of the eigenvalues of a general, typically large and sparse, nonsymmetric matrix or linear operator. A critical constraint is that the operator $A$ is accessible only through its action on a vector, i.e., as a function that returns $Ax$ for a given $x$.\n\nThe Arnoldi iteration constructs an orthonormal basis for the Krylov subspace $K_m(A, v_1) = \\mathrm{span}\\{v_1, A v_1, A^2 v_1, \\dots, A^{m-1} v_1\\}$, where $v_1$ is a starting vector of unit norm and $m$ is the dimension of the subspace. Let the orthonormal basis vectors be the columns of the matrix $V_m = [v_1, v_2, \\dots, v_m] \\in \\mathbb{R}^{n \\times m}$. The process also generates an $m \\times m$ upper Hessenberg matrix $H_m$, which represents the orthogonal projection of the operator A onto the Krylov subspace: $H_m = V_m^\\top A V_m$.\n\nThe eigenvalues of this smaller matrix $H_m$, denoted $\\{\\mu_k\\}_{k=1}^m$, are called the Ritz values. They serve as approximations to the eigenvalues of the original operator $A$. The Ritz estimate of the spectral radius of $A$ is given by $\\hat{\\rho}_m(A) = \\max_{k=1,\\dots,m} |\\mu_k|$. The objective is to calculate the absolute error $\\varepsilon = |\\hat{\\rho}_m(A) - \\rho(A)|$ for several test cases.\n\nThe construction of the basis $V_m$ and the matrix $H_m$ is performed via a stabilized Gram-Schmidt orthogonalization procedure. Given a normalized starting vector $v_1$, the algorithm proceeds as follows for $j=1, 2, \\dots, m$:\n1.  Compute the next vector in the Krylov sequence: $w \\gets A v_j$.\n2.  Orthogonalize $w$ against the previously generated basis vectors $\\{v_1, \\dots, v_j\\}$. This is achieved by iterating for $i = 1, \\dots, j$:\n    a.  Compute the projection coefficient: $h_{ij} \\gets v_i^\\top w$.\n    b.  Subtract the projection: $w \\gets w - h_{ij} v_i$.\n3.  The norm of the resulting vector $w$ is the next subdiagonal element of the Hessenberg matrix: $h_{j+1, j} \\gets \\|w\\|_2$.\n4.  If $h_{j+1, j}$ is numerically zero, this indicates a breakdown. The Krylov subspace is invariant under $A$ and its dimension is $j$, which is less than the desired $m$. The algorithm terminates, and the resulting projection is a $j \\times j$ matrix.\n5.  If no breakdown occurs, normalize the new vector to obtain the next basis vector: $v_{j+1} \\gets w / h_{j+1, j}$.\n\nThe coefficients $h_{ij}$ for $i \\in \\{1,\\dots,j\\}$ and $h_{j+1,j}$ form the entries of an $(m+1) \\times m$ upper Hessenberg matrix $\\tilde{H}_m$. The desired matrix $H_m$ is the upper $m \\times m$ submatrix of $\\tilde{H}_m$.\n\nFor each test case, the implementation requires a specific function that computes the action of the operator $A$ on an arbitrary vector $x$. The initial vector $v_0$ is defined component-wise as $(v_0)_i = \\sin(i) + \\frac{1}{2}\\cos(2i)$ for $i=1, \\dots, n$, and is then normalized to $v_1 = v_0/\\|v_0\\|_2$ to start the iteration.\n\n-   **Test Case 1 (Tridiagonal Toeplitz operator)**: The action of this operator is defined by a simple stencil, which can be implemented efficiently using vector operations. The true spectral radius is given by an analytical formula based on its parameters $a, b, c$.\n-   **Test Case 2 (Jordan block)**: The operator action corresponds to multiplication by a matrix $J = \\lambda I + N$, where $N$ is the nilpotent matrix with ones on the first superdiagonal. Its action on a vector is straightforward to compute. The true spectral radius is simply $|\\lambda|$.\n-   **Test Case 3 (Similarity transform)**: The operator is $A = S D S^{-1}$, where $D$ is diagonal and $S$ is a specified dense matrix. Its action $y = A x$ must be computed as a sequence of operations: first, solve the linear system $S z = x$ to obtain $z=S^{-1}x$; second, compute the element-wise product $w=Dz$; finally, compute the matrix-vector product $y=Sw$. The true spectral radius is the largest absolute value of the diagonal entries of $D$.\n-   **Test Case 4 (Perturbed identity)**: The operator is $A = \\alpha I + N$, where $N$ is a sum of shift operators, making it a strictly upper-triangular and thus nilpotent matrix. The eigenvalues of $A$ are all equal to $\\alpha$, so $\\rho(A) = |\\alpha|$. The action is computed by scaling the input vector and adding shifted versions of it.\n\nFor each case, the Arnoldi iteration is performed to construct $H_m$. The eigenvalues of $H_m$ are computed numerically, and the largest magnitude gives the Ritz estimate $\\hat{\\rho}_m(A)$. This is then compared to the known true spectral radius $\\rho(A)$ to find the required absolute error.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_iteration(A_op, v_start, m):\n    \"\"\"\n    Performs Arnoldi iteration to find the Hessenberg projection H_m.\n    This implementation uses a modified Gram-Schmidt process for stability.\n\n    Args:\n        A_op: A function that computes A @ x.\n        v_start: The normalized starting vector of size n.\n        m: The desired dimension of the Krylov subspace.\n\n    Returns:\n        H: The m_eff x m_eff upper Hessenberg matrix, where m_eff = m is\n           the effective dimension upon breakdown.\n    \"\"\"\n    n = v_start.shape[0]\n    V = np.zeros((n, m + 1), dtype=np.float64)\n    H = np.zeros((m + 1, m), dtype=np.float64)\n    \n    # A small tolerance for breakdown detection\n    tolerance = np.finfo(np.float64).eps * n\n    \n    V[:, 0] = v_start\n    \n    for j in range(m):\n        w = A_op(V[:, j])\n        \n        # Orthogonalize w against the existing basis V using modified Gram-Schmidt\n        for i in range(j + 1):\n            h_ij = np.dot(V[:, i], w)\n            w -= h_ij * V[:, i]\n            H[i, j] = h_ij\n        \n        h_jp1_j = np.linalg.norm(w)\n        \n        if h_jp1_j  tolerance:\n            # Breakdown: Krylov subspace has dimension j+1\n            m_eff = j + 1\n            return H[:m_eff, :m_eff]\n            \n        H[j + 1, j] = h_jp1_j\n        V[:, j + 1] = w / h_jp1_j\n        \n    return H[:m, :m]\n\ndef solve():\n    \"\"\"\n    Solves the problem for all specified test cases.\n    \"\"\"\n    # Define operator factories and true spectral radii for each test case\n    \n    def get_op1(n, params):\n        a, b, c = params['a'], params['b'], params['c']\n        def op(x):\n            y = np.zeros_like(x)\n            y[0] = a * x[0] + b * x[1]\n            y[1:-1] = c * x[:-2] + a * x[1:-1] + b * x[2:]\n            y[-1] = c * x[-2] + a * x[-1]\n            return y\n        return op\n    \n    def get_op2(n, params):\n        lam = params['lambda']\n        def op(x):\n            y = np.zeros_like(x)\n            y[:-1] = lam * x[:-1] + x[1:]\n            y[-1] = lam * x[-1]\n            return y\n        return op\n        \n    def get_op3(n, params):\n        d_max, d_min = params['d_max'], params['d_min']\n        i_vec = np.arange(1, n + 1)\n        d = d_min + (d_max - d_min) * (n - i_vec) / (n - 1)\n        \n        i_v = i_vec[:, None]\n        j_h = i_vec[None, :]\n        S = np.eye(n) + 1e-2 * np.sin(i_v * j_h / (n + 1))\n        \n        # Return a closure that has pre-computed S and d\n        def op(x):\n            # y = S * D * S_inv * x\n            z = np.linalg.solve(S, x)\n            w = d * z\n            y = S @ w\n            return y\n        return op\n\n    def get_op4(n, params):\n        alpha, p = params['alpha'], params['p']\n        def op(x):\n            y = alpha * x.copy()\n            for k in range(1, p + 1):\n                y[:n-k] += x[k:]\n            return y\n        return op\n\n    test_cases = [\n        {\n            \"n\": 80, \"m\": 25, \"params\": {\"a\": 1.0, \"b\": 2.0, \"c\": 0.5},\n            \"A_op_factory\": get_op1,\n            \"true_rho\": lambda p: max(abs(p['a'] + 2 * np.sqrt(p['b'] * p['c'])), \n                                      abs(p['a'] - 2 * np.sqrt(p['b'] * p['c'])))\n        },\n        {\n            \"n\": 60, \"m\": 15, \"params\": {\"lambda\": 3.0},\n            \"A_op_factory\": get_op2,\n            \"true_rho\": lambda p: abs(p['lambda'])\n        },\n        {\n            \"n\": 50, \"m\": 30, \"params\": {\"d_max\": 5.0, \"d_min\": 1.0},\n            \"A_op_factory\": get_op3,\n            \"true_rho\": lambda p: p['d_max']\n        },\n        {\n            \"n\": 100, \"m\": 50, \"params\": {\"alpha\": 2.0, \"p\": 3},\n            \"A_op_factory\": get_op4,\n            \"true_rho\": lambda p: abs(p['alpha'])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n, m, params = case[\"n\"], case[\"m\"], case[\"params\"]\n        \n        # 1. Define initial vector\n        i_vals = np.arange(1, n + 1)\n        v0_unnormalized = np.sin(i_vals) + 0.5 * np.cos(2 * i_vals)\n        v0_norm = np.linalg.norm(v0_unnormalized)\n        v0_normalized = v0_unnormalized / v0_norm\n\n        # 2. Get operator and true spectral radius\n        A_op = case[\"A_op_factory\"](n, params)\n        true_rho = case[\"true_rho\"](params)\n        \n        # 3. Run Arnoldi iteration\n        H_m = arnoldi_iteration(A_op, v0_normalized, m)\n        \n        # 4. Compute Ritz estimate and error\n        if H_m.shape[0] > 0:\n            ritz_values = np.linalg.eigvals(H_m)\n            ritz_rho = np.max(np.abs(ritz_values))\n        else: # This case happens if m=0 or breakdown with j=0\n            ritz_rho = 0.0\n\n        error = abs(ritz_rho - true_rho)\n        results.append(f\"{error:.8f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having implemented the Arnoldi iteration, a natural question arises: how accurate are the resulting Ritz vectors? This practice delves into the geometry of the approximation, asking you to quantify the alignment between the computed Ritz vector and the true eigenvector within the confines of the Krylov subspace. By calculating this angle, you will gain a deeper intuition for how the subspace method captures the essential properties of the operator .",
            "id": "2373564",
            "problem": "You are given a nonsymmetric square matrix $A \\in \\mathbb{C}^{n \\times n}$ and a nonzero starting vector $b \\in \\mathbb{C}^{n}$. For a positive integer $m \\le n$, define the $m$-dimensional Krylov subspace generated by $A$ and $b$ as\n$$\n\\mathcal{K}_m(A,b) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{m-1} b\\}.\n$$\nLet $V_m \\in \\mathbb{C}^{n \\times m}$ have orthonormal columns forming a basis of $\\mathcal{K}_m(A,b)$, and define the projected matrix\n$$\nH_m = V_m^\\ast A V_m \\in \\mathbb{C}^{m \\times m}.\n$$\nLet $(\\theta, y)$ be an eigenpair of $H_m$ with $|\\theta|$ maximal among the eigenvalues of $H_m$, and define the corresponding Ritz vector\n$$\nr = V_m y \\in \\mathbb{C}^{n}.\n$$\nLet $u \\in \\mathbb{C}^{n}$ be a unit-norm right eigenvector of $A$ associated with an eigenvalue $\\lambda$ of $A$ satisfying $|\\lambda| = \\max\\{|\\mu| : \\mu \\text{ is an eigenvalue of } A\\}$. Consider the orthogonal projection of $u$ onto $\\mathcal{K}_m(A,b)$,\n$$\nu_{\\mathrm{proj}} = V_m V_m^\\ast u.\n$$\nIf $\\|u_{\\mathrm{proj}}\\|_2 \\neq 0$, define the subspace comparison angle $\\alpha$ between the Ritz vector $r$ and the projected true eigenvector $u_{\\mathrm{proj}}$ as\n$$\n\\alpha = \\arccos\\left( \\left| \\frac{r^\\ast u_{\\mathrm{proj}}}{\\|r\\|_2 \\, \\|u_{\\mathrm{proj}}\\|_2} \\right| \\right),\n$$\nand if $\\|u_{\\mathrm{proj}}\\|_2 = 0$, define $\\alpha = \\pi/2$. The quantity $\\alpha$ is to be reported in radians.\n\nYour task is, for each test case below, to compute the angle $\\alpha$ as defined above.\n\nTest suite:\n- Case $1$:\n  - Size $n = 6$.\n  - Matrix\n    $$\n    A_1 = \\begin{bmatrix}\n    6.0  -2.0  0.0  0.0  0.0  0.0 \\\\\n    1.0  3.5  1.0  0.0  0.0  0.0 \\\\\n    0.0  -1.0  2.7  0.5  0.0  0.0 \\\\\n    0.0  0.0  -0.5  -1.2  1.0  0.0 \\\\\n    0.0  0.0  0.0  -1.0  0.3  2.0 \\\\\n    0.0  0.0  0.0  0.0  -1.0  -0.8\n    \\end{bmatrix}.\n    $$\n  - Starting vector\n    $$\n    b_1 = \\begin{bmatrix}\n    1.0 \\\\ 2.0 \\\\ -1.0 \\\\ 0.5 \\\\ -0.2 \\\\ 0.3\n    \\end{bmatrix}.\n    $$\n  - Krylov dimension $m_1 = 4$.\n- Case $2$:\n  - Size $n = 5$.\n  - Matrix\n    $$\n    A_2 = \\begin{bmatrix}\n    5.0  0.5  0.0  0.0  0.0 \\\\\n    2.0  4.0  0.5  0.0  0.0 \\\\\n    0.0  2.0  3.0  0.5  0.0 \\\\\n    0.0  0.0  2.0  2.0  0.5 \\\\\n    0.0  0.0  0.0  2.0  1.0\n    \\end{bmatrix}.\n    $$\n  - Starting vector\n    $$\n    b_2 = \\begin{bmatrix}\n    1.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\n    \\end{bmatrix}.\n    $$\n  - Krylov dimension $m_2 = 1$.\n- Case $3$:\n  - Size $n = 8$.\n  - Matrix\n    $$\n    A_3 = \\begin{bmatrix}\n    2.2  0.3  -0.1  0.0  0.0  0.0  0.0  0.0 \\\\\n    0.0  -3.1  0.4  0.2  0.0  0.0  0.0  0.0 \\\\\n    0.0  0.0  4.5  -0.3  0.1  0.0  0.0  0.0 \\\\\n    0.0  0.0  0.0  -1.7  0.2  0.1  0.0  0.0 \\\\\n    0.0  0.0  0.0  0.0  0.9  -0.4  0.3  0.0 \\\\\n    0.0  0.0  0.0  0.0  0.0  1.3  0.2  -0.1 \\\\\n    0.0  0.0  0.0  0.0  0.0  0.0  -2.8  0.5 \\\\\n    0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.2\n    \\end{bmatrix}.\n    $$\n  - Starting vector\n    $$\n    b_3 = \\begin{bmatrix}\n    1.0 \\\\ -1.0 \\\\ 0.5 \\\\ 0.7 \\\\ -0.3 \\\\ 0.2 \\\\ -0.4 \\\\ 0.6\n    \\end{bmatrix}.\n    $$\n  - Krylov dimension $m_3 = 5$.\n\nFor each case:\n1. Use the specified $A$, $b$, and $m$ to construct $V_m$ with orthonormal columns spanning $\\mathcal{K}_m(A,b)$, form $H_m = V_m^\\ast A V_m$, select the eigenpair $(\\theta,y)$ of $H_m$ with $|\\theta|$ maximal, and compute the Ritz vector $r = V_m y$.\n2. Compute a unit-norm right eigenvector $u$ of $A$ associated with a $|\\lambda|$-maximizing eigenvalue $\\lambda$ of $A$.\n3. Compute $u_{\\mathrm{proj}} = V_m V_m^\\ast u$ and the angle $\\alpha$ as defined above (in radians).\n\nYour program should produce a single line of output containing the three angles for the three cases, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example\n$$\n[\\alpha_1,\\alpha_2,\\alpha_3].\n$$\nNo physical units are involved. Angles must be expressed in radians. The final output must strictly follow this single-line format.",
            "solution": "The problem requires the computation of an angle, denoted by $\\alpha$, that quantifies the alignment between a Ritz vector from a Krylov subspace and the projection of a true dominant eigenvector onto that same subspace. The provided problem statement is scientifically grounded, well-posed, and objective. It contains all necessary data and definitions for a unique solution to be computed for each test case. The definitions correspond to standard concepts in numerical linear algebra, specifically the Arnoldi iteration for nonsymmetric eigenproblems. The problem is therefore deemed valid.\n\nWe will carry out the computation by following the sequence of steps defined in the problem statement. The overall procedure for a given matrix $A \\in \\mathbb{C}^{n \\times n}$, starting vector $b \\in \\mathbb{C}^{n}$, and Krylov dimension $m \\in \\mathbb{Z}^+$ is as follows.\n\nStep $1$: Construction of an Orthonormal Basis for the Krylov Subspace\n\nThe $m$-dimensional Krylov subspace is defined as $\\mathcal{K}_m(A,b) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{m-1} b\\}$. We need to construct a matrix $V_m \\in \\mathbb{C}^{n \\times m}$ whose columns form an orthonormal basis for this subspace. This can be achieved using a procedure equivalent to the Arnoldi iteration, which is a numerically stable method based on the modified Gram-Schmidt algorithm.\n\nLet the columns of $V_m$ be denoted by $v_1, v_2, \\dots, v_m$. The process is iterative:\n$1$. The first basis vector is the normalized starting vector:\n$$\nv_1 = \\frac{b}{\\|b\\|_2}\n$$\n$2$. For $j = 2, 3, \\dots, m$, the subsequent basis vectors are generated by taking the vector $A v_{j-1}$, orthogonalizing it against all previously generated basis vectors $\\{v_1, \\dots, v_{j-1}\\}$, and normalizing the result. Let $w_j = A v_{j-1}$. The orthogonalization step proceeds as:\n$$\nw'_j = w_j - \\sum_{i=1}^{j-1} (v_i^\\ast w_j) v_i\n$$\n$3$. The new orthonormal basis vector is then obtained by normalization:\n$$\nv_j = \\frac{w'_j}{\\|w'_j\\|_2}\n$$\nThis process guarantees that the resulting matrix $V_m = [v_1, v_2, \\dots, v_m]$ has orthonormal columns spanning $\\mathcal{K}_m(A,b)$. If at any step $\\|w'_j\\|_2$ is close to zero, it signifies that the dimension of the Krylov subspace is less than $m$; the process terminates, yielding a basis for the true subspace.\n\nStep $2$: Computation of the Ritz Vector\n\nWith the basis $V_m$, we project the matrix $A$ onto the Krylov subspace to obtain a smaller matrix $H_m \\in \\mathbb{C}^{m \\times m}$:\n$$\nH_m = V_m^\\ast A V_m\n$$\nThe eigenvalues of $H_m$ are called Ritz values, and they serve as approximations to the eigenvalues of $A$. We solve the smaller eigenvalue problem for $H_m$:\n$$\nH_m y = \\theta y\n$$\nWe identify the eigenpair $(\\theta, y)$ for which the eigenvalue's magnitude, $|\\theta|$, is maximal among all eigenvalues of $H_m$. The eigenvector $y \\in \\mathbb{C}^m$ is normalized such that $\\|y\\|_2 = 1$. The corresponding Ritz vector $r \\in \\mathbb{C}^n$ is the approximation of the true eigenvector in the original $n$-dimensional space, and is computed by mapping $y$ back using $V_m$:\n$$\nr = V_m y\n$$\nSince $V_m$ has orthonormal columns and $\\|y\\|_2 = 1$, the Ritz vector $r$ is also a unit vector, i.e., $\\|r\\|_2 = 1$.\n\nStep $3$: Computation of the True Dominant Eigenvector\n\nWe must find the eigenvalue of the original matrix $A$ with the largest magnitude. This requires solving the full eigenvalue problem:\n$$\nA u = \\lambda u\n$$\nLet $\\lambda$ be an eigenvalue of $A$ such that $|\\lambda|$ is maximal. We find the corresponding right eigenvector $u \\in \\mathbb{C}^n$ and normalize it to have unit norm, $\\|u\\|_2 = 1$.\n\nStep $4$: Projection and Angle Calculation\n\nThe true dominant eigenvector $u$ is projected orthogonally onto the Krylov subspace $\\mathcal{K}_m(A,b)$. The projection operator onto the column space of $V_m$ is $P_m = V_m V_m^\\ast$. The projected vector is:\n$$\nu_{\\mathrm{proj}} = V_m V_m^\\ast u\n$$\nFinally, we compute the angle $\\alpha$ between the Ritz vector $r$ and the projected true eigenvector $u_{\\mathrm{proj}}$. The problem defines this angle via the formula:\n$$\n\\alpha = \\arccos\\left( \\left| \\frac{r^\\ast u_{\\mathrm{proj}}}{\\|r\\|_2 \\, \\|u_{\\mathrm{proj}}\\|_2} \\right| \\right)\n$$\nThis formula computes the angle between the one-dimensional subspaces (complex lines) spanned by the vectors $r$ and $u_{\\mathrm{proj}}$. A special case occurs if $u_{\\mathrm{proj}}$ is the zero vector (i.e., $\\|u_{\\mathrm{proj}}\\|_2 = 0$), which happens when the true eigenvector $u$ is orthogonal to the Krylov subspace. In this case, the angle is defined as $\\alpha = \\pi/2$.\n\nThis complete procedure will be applied to each of the three test cases specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 6,\n            \"A\": np.array([\n                [6.0, -2.0, 0.0, 0.0, 0.0, 0.0],\n                [1.0, 3.5, 1.0, 0.0, 0.0, 0.0],\n                [0.0, -1.0, 2.7, 0.5, 0.0, 0.0],\n                [0.0, 0.0, -0.5, -1.2, 1.0, 0.0],\n                [0.0, 0.0, 0.0, -1.0, 0.3, 2.0],\n                [0.0, 0.0, 0.0, 0.0, -1.0, -0.8]\n            ], dtype=float),\n            \"b\": np.array([1.0, 2.0, -1.0, 0.5, -0.2, 0.3], dtype=float),\n            \"m\": 4\n        },\n        {\n            \"n\": 5,\n            \"A\": np.array([\n                [5.0, 0.5, 0.0, 0.0, 0.0],\n                [2.0, 4.0, 0.5, 0.0, 0.0],\n                [0.0, 2.0, 3.0, 0.5, 0.0],\n                [0.0, 0.0, 2.0, 2.0, 0.5],\n                [0.0, 0.0, 0.0, 2.0, 1.0]\n            ], dtype=float),\n            \"b\": np.array([1.0, 0.0, 0.0, 0.0, 0.0], dtype=float),\n            \"m\": 1\n        },\n        {\n            \"n\": 8,\n            \"A\": np.array([\n                [2.2, 0.3, -0.1, 0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, -3.1, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 4.5, -0.3, 0.1, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, -1.7, 0.2, 0.1, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.9, -0.4, 0.3, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.3, 0.2, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.8, 0.5],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.2]\n            ], dtype=float),\n            \"b\": np.array([1.0, -1.0, 0.5, 0.7, -0.3, 0.2, -0.4, 0.6], dtype=float),\n            \"m\": 5\n        }\n    ]\n\n    results = []\n    # Machine precision tolerance for norm checks\n    TOL = 1e-12\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        m = case[\"m\"]\n        n = case[\"n\"]\n\n        # Step 1: Construct the orthonormal basis V_m for the Krylov subspace\n        V_cols = []\n        q = b / np.linalg.norm(b)\n        V_cols.append(q)\n\n        effective_m = 1\n        for j in range(1, m):\n            w = A @ V_cols[-1]\n            # Modified Gram-Schmidt\n            for v_i in V_cols:\n                w -= np.vdot(v_i, w) * v_i\n            \n            norm_w = np.linalg.norm(w)\n            if norm_w  TOL:\n                # Invariant subspace found (breakdown)\n                break\n            \n            q = w / norm_w\n            V_cols.append(q)\n            effective_m += 1\n\n        Vm = np.column_stack(V_cols)\n\n        # Step 2: Compute the Ritz vector r\n        # Since A and b are real, Vm is real. Vm.T.conj() is just Vm.T\n        Hm = Vm.T @ A @ Vm\n        hm_eigvals, hm_eigvecs = np.linalg.eig(Hm)\n        \n        idx_theta = np.argmax(np.abs(hm_eigvals))\n        y = hm_eigvecs[:, idx_theta]\n        \n        r = Vm @ y\n        \n        # Step 3: Compute the true dominant eigenvector u\n        a_eigvals, a_eigvecs = np.linalg.eig(A)\n        idx_lambda = np.argmax(np.abs(a_eigvals))\n        u = a_eigvecs[:, idx_lambda]\n\n        # Step 4: Compute the projection u_proj and the angle alpha\n        # Vm is real, so Vm* = Vm.T\n        u_proj = Vm @ (Vm.T @ u)\n        \n        norm_u_proj = np.linalg.norm(u_proj)\n        \n        if norm_u_proj  TOL:\n            alpha = np.pi / 2.0\n        else:\n            norm_r = np.linalg.norm(r)\n            # Use np.vdot for Hermitian inner product (r* u_proj)\n            numerator = np.abs(np.vdot(r, u_proj))\n            denominator = norm_r * norm_u_proj\n            \n            # Clip to handle potential floating point inaccuracies\n            cos_val = np.clip(numerator / denominator, -1.0, 1.0)\n            \n            alpha = np.arccos(cos_val)\n            \n        results.append(alpha)\n\n    # Format the output as specified\n    formatted_results = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To make Arnoldi iteration a truly powerful tool, we must adapt it to efficiently find multiple eigenpairs, not just the single most dominant one. This advanced practice challenges you to implement and test an explicit \"locking and deflation\" mechanism, a technique used in professional-grade software to prevent the algorithm from repeatedly converging to the same eigenpairs. You will quantify the significant performance gains this enhancement provides, especially in challenging cases with clustered eigenvalues .",
            "id": "2373565",
            "problem": "Implement a complete and runnable program that compares two approaches for approximating the eigenpairs of largest magnitude of given real, nonsymmetric matrices: one approach that maintains and enforces orthogonality to a set of previously converged approximate eigenvectors (locking with deflation), and one approach that does not maintain such a locked set across restarts. The goal is to quantify how locking reduces the number of matrix-vector products required to obtain a prescribed number of dominant eigenpairs. Your program must be deterministic and must not read input.\n\nYou are given the following specifications.\n\n1) Mathematical setting.\n\n- Let $A \\in \\mathbb{R}^{n \\times n}$ be a real, nonsymmetric matrix. For any nonzero vector $u \\in \\mathbb{C}^{n}$ and scalar $\\theta \\in \\mathbb{C}$, define the residual of an approximate eigenpair $(\\theta,u)$ by $r \\equiv A u - \\theta u$. The residual norm is $\\lVert r \\rVert_{2}$.\n- A pair $(\\theta,u)$ with $\\lVert u \\rVert_{2} = 1$ is considered converged if $\\lVert A u - \\theta u \\rVert_{2} \\le \\varepsilon$, where $\\varepsilon$ is a given tolerance.\n- A dominant eigenpair is one whose eigenvalue has maximal absolute value relative to the rest. For a requested number $k \\in \\mathbb{N}$, the target set is the $k$ eigenpairs corresponding to the eigenvalues with the $k$ largest absolute values (ties may be resolved arbitrarily).\n\n2) Subspace construction with and without locking.\n\n- Across iterations, construct Krylov subspaces of dimension at most $m \\in \\mathbb{N}$ generated by repeated applications of $A$ to an initial vector. Each application of $A$ to a vector counts as one matrix-vector product. After a subspace of dimension at most $m$ is built and processed, you may restart by discarding the subspace and starting from a new initial vector. This defines a cycle. You may perform multiple cycles until the requested number $k$ of dominant eigenpairs has converged.\n- Locking with deflation: maintain a set $\\mathcal{L}$ of currently converged unit-norm approximate eigenvectors. At all times, ensure any newly generated vector $w$ used to expand the subspace satisfies $u^{*} w = 0$ for all $u \\in \\mathcal{L}$ (orthogonality with respect to the standard complex inner product), i.e., project $w$ onto the orthogonal complement of $\\mathrm{span}(\\mathcal{L})$. When new approximate eigenpairs converge, append their vectors (after orthonormalization) to $\\mathcal{L}$. Continue until $|\\mathcal{L}| = k$.\n- No locking: do not maintain any $\\mathcal{L}$ across cycles. In each cycle, identify converged approximate eigenpairs inside the current subspace; accumulate their eigenvalues across cycles as a set of unique values (distinct up to a small threshold in absolute difference) until $k$ unique converged eigenvalues have been collected. Do not deflate previously discovered vectors when constructing new subspaces.\n\n3) Convergence assessment inside a subspace.\n\n- In any subspace of dimension $j \\le m$ with an orthonormal basis $V_{j} \\in \\mathbb{C}^{n \\times j}$, form a projected matrix $H_{j} \\in \\mathbb{C}^{j \\times j}$, and compute its eigenpairs $(\\theta, y)$ with $y \\in \\mathbb{C}^{j}$. For each such pair, form the Ritz vector $u = V_{j} y / \\lVert V_{j} y \\rVert_{2}$ and the residual norm $\\lVert A u - \\theta u \\rVert_{2}$. Use these residual norms to decide convergence with the tolerance $\\varepsilon$.\n- Within any cycle, when identifying which converged approximate eigenpairs to record (and to lock, in the locking variant), prioritize those associated with the largest absolute values $|\\theta|$, and avoid counting or locking duplicates by comparing eigenvalues in absolute difference against a small threshold.\n\n4) Test suite.\n\nFor each test case, construct $A$ as $A = S D S^{-1}$, where $D$ is diagonal with prescribed diagonal entries and $S$ is a dense real matrix generated pseudorandomly with a fixed seed and entries drawn uniformly from the interval $[-1,1]$ until an invertible $S$ is obtained. All initial vectors for cycles are also drawn pseudorandomly using the same seed specification for the case, and are projected to be orthogonal to the locked set when locking is active. All random draws must be reproducible.\n\nThe test suite consists of three cases:\n\n- Case 1 (happy path): $n = 8$, $D = [5.0, 3.0, 2.0, 1.0, 0.5, -0.2, -1.5, 4.0]$, $k = 3$, $m = 4$, $\\varepsilon = 10^{-8}$, maximum number of cycles $C_{\\max} = 200$, random seed $10$.\n- Case 2 (close eigenvalues): $n = 10$, $D = [4.0, 3.99, 1.0, 0.1, -0.1, 2.0, -2.0, 0.5, 0.49, -3.0]$, $k = 3$, $m = 5$, $\\varepsilon = 10^{-8}$, $C_{\\max} = 200$, random seed $21$.\n- Case 3 (single dominant, small subspace): $n = 6$, $D = [-5.0, -1.0, -2.0, 0.1, 0.2, 0.3]$, $k = 1$, $m = 2$, $\\varepsilon = 10^{-8}$, $C_{\\max} = 200$, random seed $7$.\n\nAll numbers above are dimensionless. Express any absolute differences and tolerances as real numbers.\n\n5) Required outputs.\n\n- For each case, run both approaches until the requested number $k$ of dominant eigenpairs have converged according to the criterion $\\lVert A u - \\theta u \\rVert_{2} \\le \\varepsilon$, or until the cycle limit $C_{\\max}$ is reached. Count matrix-vector products as the total number of applications of $A$ to a vector incurred during subspace expansions across cycles, separately for the no-locking and locking approaches.\n- If $C_{\\max}$ is reached without meeting the convergence target in either approach, return the number of matrix-vector products accrued so far for that approach.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a two-element list $[M_{\\mathrm{no\\_lock}}, M_{\\mathrm{lock}}]$ of integers representing the matrix-vector product counts for the no-locking and locking approaches, respectively. The final format must be a single line like $[[a,b],[c,d],[e,f]]$ with integers $a,b,c,d,e,f$.",
            "solution": "The problem as stated is valid. It presents a well-posed, scientifically grounded task within the field of numerical linear algebra, a cornerstone of computational physics. The problem asks for an empirical comparison of two standard variants of a restarted Krylov subspace method for finding dominant eigenpairs of a nonsymmetric matrix. All parameters and procedures are specified with sufficient clarity and objectivity to permit a unique, verifiable solution.\n\nThe core of the task is to approximate a specified number, $k$, of eigenpairs $(\\lambda, u)$ corresponding to the eigenvalues $\\lambda$ of largest magnitude for a given real nonsymmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. The fundamental equation is $A u = \\lambda u$. The proposed method is a restarted Krylov subspace iteration, which is a practical realization of the Arnoldi method.\n\nLet us first formalize the common components of the algorithm. The method generates a sequence of Krylov subspaces $\\mathcal{K}_j(A, v_1) = \\mathrm{span}\\{v_1, Av_1, \\dots, A^{j-1}v_1\\}$, where $v_1$ is a starting vector with $\\lVert v_1 \\rVert_2 = 1$. The Arnoldi process constructs an orthonormal basis $V_j = [v_1, v_2, \\dots, v_j]$ for $\\mathcal{K}_j(A, v_1)$. This process simultaneously generates an upper Hessenberg matrix $H_j = V_j^* A V_j \\in \\mathbb{C}^{j \\times j}$, where the asterisk denotes the conjugate transpose. The key Arnoldi relation after $m$ steps is:\n$$\nA V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^T\n$$\nwhere $V_m \\in \\mathbb{C}^{n \\times m}$ has orthonormal columns, $H_m \\in \\mathbb{C}^{m \\times m}$ is upper Hessenberg, $v_{m+1}$ is a unit vector orthogonal to the columns of $V_m$, $h_{m+1,m}$ is a non-negative scalar, and $e_m$ is the $m$-th standard basis vector in $\\mathbb{R}^m$. Each column of $V_m$ requires one matrix-vector product with $A$, which is the primary measure of computational cost.\n\nFrom the small $m \\times m$ eigenproblem for $H_m$, $H_m y_i = \\theta_i y_i$, we extract approximate eigenpairs for $A$. These are called Ritz pairs $(\\theta_i, u_i)$, where $\\theta_i$ is a Ritz value and $u_i = V_m y_i$ is the corresponding Ritz vector. The problem requires assessing convergence by checking if the residual norm $\\lVert A u_i - \\theta_i u_i \\rVert_2 \\le \\varepsilon$ for a given tolerance $\\varepsilon$. Using the Arnoldi relation, this norm can be calculated efficiently without an additional matrix-vector product:\n$$\n\\begin{aligned}\nA u_i - \\theta_i u_i = A(V_m y_i) - \\theta_i(V_m y_i) \\\\\n= (AV_m) y_i - V_m(\\theta_i y_i) \\\\\n= (V_m H_m + h_{m+1,m} v_{m+1} e_m^T) y_i - V_m(H_m y_i) \\\\\n= V_m H_m y_i + h_{m+1,m} v_{m+1} (e_m^T y_i) - V_m H_m y_i \\\\\n= (h_{m+1,m} \\cdot (y_i)_m) v_{m+1}\n\\end{aligned}\n$$\nwhere $(y_i)_m$ is the last component of the eigenvector $y_i$. Since $\\lVert v_{m+1} \\rVert_2 = 1$, the residual norm is exactly $\\lVert A u_i - \\theta_i u_i \\rVert_2 = |h_{m+1,m}| \\cdot |(y_i)_m|$.\n\nThe process is restarted after building a subspace of dimension $m$ (a cycle). The two approaches differ in how they utilize information from previous cycles.\n\n1.  **No-Locking Approach:** This is the simpler strategy. Each cycle is independent. It starts with a new, randomly generated vector $v_1$. Within a cycle, it identifies all converged Ritz pairs. The corresponding Ritz values $\\theta_i$ are collected. To avoid duplicates, a newly found $\\theta_i$ is added to the set of converged eigenvalues only if it is sufficiently different from all previously found values, i.e., $|\\theta_i - \\theta_{found}| \\ge \\delta$ for all $\\theta_{found}$ in the set, where $\\delta$ is a small uniqueness tolerance. The process terminates when $k$ unique eigenvalues have been found or the maximum number of cycles $C_{\\max}$ is reached. This method may repeatedly find the same dominant eigenpairs in successive cycles, which can be inefficient.\n\n2.  **Locking with Deflation Approach:** This method aims to improve efficiency by preventing the re-computation of converged eigenpairs. It maintains an explicit, orthonormal set $\\mathcal{L} = \\{u_1, \\dots, u_p\\}$ of previously converged and \"locked\" eigenvectors, where $p  k$. At the start of a new cycle, the initial vector $v_1$ is generated randomly and then explicitly orthogonalized against the space spanned by $\\mathcal{L}$: $v_1 \\leftarrow v_1 - \\sum_{i=1}^p (u_i^* v_1) u_i$. The Arnoldi iteration then proceeds in the subspace orthogonal to $\\mathrm{span}(\\mathcal{L})$, effectively \"deflating\" the known eigenvectors from the problem. When new Ritz pairs converge, their Ritz vectors are candidates for being locked. To maintain the properties of $\\mathcal{L}$, a new candidate vector $u_{new}$ is first orthogonalized against the current set $\\mathcal{L}$ and then normalized before being added. This ensures that the algorithm focuses its search on the yet-uncovered part of the spectrum. The process continues until $|\\mathcal{L}| = k$.\n\nFor both methods, the test matrices are constructed as $A = S D S^{-1}$, where $D$ is a diagonal matrix with prescribed eigenvalues and $S$ is a randomly generated, invertible real matrix. The entire procedure, including matrix generation and the selection of starting vectors, is made deterministic by using a fixed-seed pseudorandom number generator. For a fair comparison, both algorithms are executed with identical sequences of random numbers by re-initializing the generator for each run.",
            "answer": "```python\nimport numpy as np\n\ndef arnoldi_iteration(A, v_start, m, locked_vectors):\n    \"\"\"\n    Performs m steps of the Arnoldi iteration to build a Krylov subspace.\n    Handles explicit deflation against locked vectors.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, m + 1), dtype=np.complex128)\n    H = np.zeros((m + 1, m), dtype=np.complex128)\n    \n    mat_vec_count = 0\n    \n    # Start vector processing\n    v = v_start\n    if locked_vectors.shape[1] > 0:\n        v = v - locked_vectors @ (locked_vectors.conj().T @ v)\n    \n    v_norm = np.linalg.norm(v)\n    if v_norm  1e-12:\n        return None, None, None, mat_vec_count, 0 # Start vector is in locked space\n\n    V[:, 0] = v / v_norm\n    \n    actual_m = m\n    for j in range(m):\n        # Apply operator and count\n        w = A @ V[:, j]\n        mat_vec_count += 1\n        \n        # Deflate against locked vectors\n        if locked_vectors.shape[1] > 0:\n            w = w - locked_vectors @ (locked_vectors.conj().T @ w)\n            \n        # Modified Gram-Schmidt against current Krylov basis V\n        for i in range(j + 1):\n            H[i, j] = V[:, i].conj().T @ w\n            w = w - H[i, j] * V[:, i]\n            \n        H[j + 1, j] = np.linalg.norm(w)\n        \n        if H[j + 1, j]  1e-12:  # Breakdown\n            actual_m = j + 1\n            break\n            \n        V[:, j + 1] = w / H[j + 1, j]\n        \n    return V[:, :actual_m], H[:actual_m, :actual_m], H[actual_m, actual_m - 1], mat_vec_count, actual_m\n\ndef run_without_locking(A, k, m, tol, max_cycles, rng):\n    \"\"\"\n    Runs restarted Arnoldi without locking/deflation.\n    \"\"\"\n    n = A.shape[0]\n    total_mat_vecs = 0\n    found_eigenvalues = set()\n    \n    for _ in range(max_cycles):\n        if len(found_eigenvalues) >= k:\n            break\n            \n        v_start = rng.random(n)\n        \n        # We pass an empty locked_vectors set, so no deflation occurs.\n        V, H, h_next, mv_cycle, actual_m = arnoldi_iteration(\n            A, v_start, m, np.zeros((n, 0))\n        )\n        \n        total_mat_vecs += mv_cycle\n        if V is None or actual_m == 0:\n            continue\n            \n        ritz_vals, ritz_vecs_H = np.linalg.eig(H)\n        \n        converged_pairs = []\n        for theta, y in zip(ritz_vals, ritz_vecs_H.T):\n            residual_norm = abs(h_next) * abs(y[-1])\n            if residual_norm  tol:\n                converged_pairs.append(theta)\n\n        # Prioritize by magnitude and add unique values\n        converged_pairs.sort(key=abs, reverse=True)\n        for theta in converged_pairs:\n            is_new = True\n            for found_val in found_eigenvalues:\n                if abs(theta - found_val)  1e-6:\n                    is_new = False\n                    break\n            if is_new and len(found_eigenvalues)  k:\n                found_eigenvalues.add(theta)\n                \n    return total_mat_vecs\n\ndef run_with_locking(A, k, m, tol, max_cycles, rng):\n    \"\"\"\n    Runs restarted Arnoldi with explicit locking and deflation.\n    \"\"\"\n    n = A.shape[0]\n    total_mat_vecs = 0\n    locked_vectors = np.zeros((n, 0), dtype=np.complex128)\n    locked_eigenvalues = []\n\n    for _ in range(max_cycles):\n        if len(locked_eigenvalues) >= k:\n            break\n\n        v_start = rng.random(n)\n        \n        V, H, h_next, mv_cycle, actual_m = arnoldi_iteration(\n            A, v_start, m, locked_vectors\n        )\n        \n        total_mat_vecs += mv_cycle\n        if V is None or actual_m == 0:\n            continue\n\n        ritz_vals, ritz_vecs_H = np.linalg.eig(H)\n\n        newly_converged = []\n        for theta, y in zip(ritz_vals, ritz_vecs_H.T):\n            residual_norm = abs(h_next) * abs(y[-1])\n            is_new = True\n            for locked_val in locked_eigenvalues:\n                if abs(theta - locked_val)  1e-6:\n                    is_new = False\n                    break\n            if is_new and residual_norm  tol:\n                u = V @ y\n                newly_converged.append({'val': theta, 'vec': u / np.linalg.norm(u)})\n        \n        # Sort candidates by magnitude before attempting to lock\n        newly_converged.sort(key=lambda p: abs(p['val']), reverse=True)\n\n        for p in newly_converged:\n            if len(locked_eigenvalues) >= k:\n                break\n            \n            u_candidate = p['vec']\n            # Deflate candidate against current locked set\n            u_proj = u_candidate - locked_vectors @ (locked_vectors.conj().T @ u_candidate)\n            \n            # If it's not in the span of locked vectors, add it.\n            if np.linalg.norm(u_proj) > 1e-6:\n                u_ortho = (u_proj / np.linalg.norm(u_proj)).reshape(-1, 1)\n                locked_vectors = np.hstack([locked_vectors, u_ortho])\n                locked_eigenvalues.append(p['val'])\n                \n    return total_mat_vecs\n\ndef run_case(n, d_diag, k, m, tol, max_cycles, seed):\n    \"\"\"\n    Sets up a test case and runs both algorithms.\n    \"\"\"\n    # Create the test matrix A = S D S^-1\n    # Use a separate RNG for matrix generation to not interfere with start vectors\n    rng_A = np.random.default_rng(seed)\n    while True:\n        S = rng_A.uniform(-1, 1, size=(n, n))\n        if np.linalg.cond(S)  1 / np.finfo(S.dtype).eps:\n            break\n    D = np.diag(d_diag)\n    A = S @ D @ np.linalg.inv(S)\n\n    # Run without locking. Create a new RNG seeded for this run.\n    rng_no_lock = np.random.default_rng(seed)\n    mat_vec_no_lock = run_without_locking(A, k, m, tol, max_cycles, rng_no_lock)\n    \n    # Run with locking. Create another new RNG with the same seed.\n    # This ensures both algorithms use the same sequence of random start vectors.\n    rng_lock = np.random.default_rng(seed)\n    mat_vec_lock = run_with_locking(A, k, m, tol, max_cycles, rng_lock)\n    \n    return [mat_vec_no_lock, mat_vec_lock]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (8, [5.0, 3.0, 2.0, 1.0, 0.5, -0.2, -1.5, 4.0], 3, 4, 1e-8, 200, 10),\n        (10, [4.0, 3.99, 1.0, 0.1, -0.1, 2.0, -2.0, 0.5, 0.49, -3.0], 3, 5, 1e-8, 200, 21),\n        (6, [-5.0, -1.0, -2.0, 0.1, 0.2, 0.3], 1, 2, 1e-8, 200, 7),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result_pair = run_case(*params)\n        all_results.append(result_pair)\n\n    formatted_results = \",\".join([f\"[{r[0]},{r[1]}]\" for r in all_results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        }
    ]
}