{
    "hands_on_practices": [
        {
            "introduction": "To build a solid intuition for ill-conditioning, we begin with a hands-on analytical problem. This exercise challenges you to construct a simple $2 \\times 2$ linear system where a mere $1\\%$ perturbation in the data vector $\\mathbf{b}$ causes the solution vector $\\mathbf{x}$ to jump into an entirely different quadrant . By working through this, you will gain a tangible sense of the instability that a large condition number implies and connect this dramatic numerical effect to the underlying geometry of the linear system.",
            "id": "2428587",
            "problem": "In computational engineering, the conditioning of a linear system is assessed by how sensitively its solution responds to small perturbations in the data. Consider the linear system $A \\mathbf{x} = \\mathbf{b}$ in $\\mathbb{R}^{2}$, where\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix}, \\qquad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}.\n$$\nUse the Euclidean vector norm and the induced matrix norm (matrix $2$-norm) throughout.\n\n1) Starting from the definitions of solution sensitivity and induced norms, verify by explicit construction that there exists a perturbation $\\Delta \\mathbf{b}$ with relative size $\\|\\Delta \\mathbf{b}\\|_{2}/\\|\\mathbf{b}\\|_{2} = 0.01$ such that the solution $\\tilde{\\mathbf{x}}$ to $A \\tilde{\\mathbf{x}} = \\mathbf{b} + \\Delta \\mathbf{b}$ lies in a different open quadrant of $\\mathbb{R}^{2}$ than the solution $\\mathbf{x}$ to $A \\mathbf{x} = \\mathbf{b}$. You must build $\\Delta \\mathbf{b}$ explicitly and justify every step from first principles.\n\n2) Using the definition of the condition number in the matrix $2$-norm, and without invoking any unproven shortcut formulas, compute the condition number $\\kappa_{2}(A)$ for the given matrix $A$.\n\nProvide as your final answer only the numerical value of $\\kappa_{2}(A)$, rounded to four significant figures. No units are required.",
            "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim are:\n- Linear system: $A \\mathbf{x} = \\mathbf{b}$ in $\\mathbb{R}^{2}$.\n- Matrix: $A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix}$.\n- Vector: $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}$.\n- Norms: Euclidean vector norm ($\\|\\cdot\\|_{2}$) and the induced matrix norm ($2$-norm).\n- Perturbed system: $A \\tilde{\\mathbf{x}} = \\mathbf{b} + \\Delta \\mathbf{b}$.\n- Perturbation constraint: $\\|\\Delta \\mathbf{b}\\|_{2}/\\|\\mathbf{b}\\|_{2} = 0.01$.\n- Task 1: Construct $\\Delta \\mathbf{b}$ such that $\\tilde{\\mathbf{x}}$ and $\\mathbf{x}$ are in different open quadrants.\n- Task 2: Compute the condition number $\\kappa_{2}(A)$ from its definition.\n\nValidation Verdict:\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard exercise in numerical linear algebra concerning the sensitivity of linear systems, a core topic in computational engineering. The matrix $A$ is invertible as its determinant is $\\det(A) = (1)(1.05) - (1)(1) = 0.05 \\neq 0$, ensuring a unique solution exists. All data and constraints are provided, and there are no contradictions or ambiguities. The problem is valid.\n\nThe solution proceeds.\n\nPart 1: Construction of a suitable perturbation $\\Delta \\mathbf{b}$.\n\nFirst, we solve the unperturbed system $A \\mathbf{x} = \\mathbf{b}$ to find the original solution $\\mathbf{x}$.\n$$\n\\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}\n$$\nThis corresponds to the system of equations:\n$x_1 + x_2 = 1$\n$x_1 + 1.05 x_2 = 1.015$\n\nSubtracting the first equation from the second yields:\n$(x_1 + 1.05 x_2) - (x_1 + x_2) = 1.015 - 1$\n$0.05 x_2 = 0.015$\n$x_2 = \\frac{0.015}{0.05} = \\frac{15}{50} = \\frac{3}{10} = 0.3$.\n\nSubstituting $x_2 = 0.3$ into the first equation:\n$x_1 + 0.3 = 1 \\implies x_1 = 0.7$.\n\nThe solution is $\\mathbf{x} = \\begin{pmatrix} 0.7 \\\\ 0.3 \\end{pmatrix}$. Since $x_1 > 0$ and $x_2 > 0$, this vector lies in the first open quadrant of $\\mathbb{R}^{2}$.\n\nNext, we analyze the perturbed system $A \\tilde{\\mathbf{x}} = \\mathbf{b} + \\Delta \\mathbf{b}$. The solution is $\\tilde{\\mathbf{x}} = A^{-1}(\\mathbf{b} + \\Delta \\mathbf{b}) = A^{-1}\\mathbf{b} + A^{-1}\\Delta \\mathbf{b} = \\mathbf{x} + \\Delta \\mathbf{x}$, where $\\Delta \\mathbf{x} = A^{-1}\\Delta \\mathbf{b}$. We must construct a perturbation $\\Delta \\mathbf{b}$ such that $\\tilde{\\mathbf{x}}$ lies in a different open quadrant. For example, we can aim for the fourth quadrant, which requires $\\tilde{x}_1 > 0$ and $\\tilde{x}_2 < 0$.\n\nThe new solution's components are $\\tilde{x}_1 = x_1 + \\Delta x_1 = 0.7 + \\Delta x_1$ and $\\tilde{x}_2 = x_2 + \\Delta x_2 = 0.3 + \\Delta x_2$. To move to the fourth quadrant, we need $\\tilde{x}_2 < 0$, which implies $0.3 + \\Delta x_2 < 0$, or $\\Delta x_2 < -0.3$.\n\nTo find the relationship between $\\Delta \\mathbf{x}$ and $\\Delta \\mathbf{b}$, we must compute the inverse of $A$.\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} 1.05 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\frac{1}{0.05} \\begin{pmatrix} 1.05 & -1 \\\\ -1 & 1 \\end{pmatrix} = 20 \\begin{pmatrix} 1.05 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 21 & -20 \\\\ -20 & 20 \\end{pmatrix}.\n$$\nThe change in the solution is $\\Delta \\mathbf{x} = \\begin{pmatrix} \\Delta x_1 \\\\ \\Delta x_2 \\end{pmatrix} = A^{-1} \\Delta \\mathbf{b} = \\begin{pmatrix} 21 & -20 \\\\ -20 & 20 \\end{pmatrix} \\begin{pmatrix} \\Delta b_1 \\\\ \\Delta b_2 \\end{pmatrix}$.\nThe second component is $\\Delta x_2 = -20 \\Delta b_1 + 20 \\Delta b_2 = 20(\\Delta b_2 - \\Delta b_1)$.\nOur condition $\\Delta x_2 < -0.3$ becomes $20(\\Delta b_2 - \\Delta b_1) < -0.3$, which simplifies to $\\Delta b_2 - \\Delta b_1 < -0.015$.\n\nThe perturbation $\\Delta \\mathbf{b}$ must also satisfy the relative size constraint $\\|\\Delta \\mathbf{b}\\|_{2} / \\|\\mathbf{b}\\|_{2} = 0.01$. First, we compute the norm of $\\mathbf{b}$.\n$$\n\\|\\mathbf{b}\\|_{2}^{2} = 1^{2} + 1.015^{2} = 1 + 1.030225 = 2.030225.\n$$\nSo, $\\|\\mathbf{b}\\|_{2} = \\sqrt{2.030225}$.\nThe constraint on $\\Delta \\mathbf{b}$ is $\\|\\Delta \\mathbf{b}\\|_{2} = 0.01 \\|\\mathbf{b}\\|_{2} = 0.01 \\sqrt{2.030225}$.\nThis implies $\\|\\Delta \\mathbf{b}\\|_{2}^{2} = (0.01)^{2} (2.030225) = 0.0002030225$.\n\nWe need to find a vector $\\Delta \\mathbf{b} = \\begin{pmatrix} \\Delta b_1 \\\\ \\Delta b_2 \\end{pmatrix}$ that satisfies both $\\Delta b_2 - \\Delta b_1 < -0.015$ and $\\Delta b_1^2 + \\Delta b_2^2 = 0.0002030225$.\nTo satisfy the first condition as strongly as possible for a fixed norm, we should choose $\\Delta \\mathbf{b}$ such that the quantity $\\Delta b_2 - \\Delta b_1$ is minimized. This occurs when $\\Delta \\mathbf{b}$ is proportional to the vector $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ but with a negative projection on the y-axis and positive on the x-axis. Thus, we choose $\\Delta \\mathbf{b}$ to be in the direction of the vector $\\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$.\n\nLet $\\Delta \\mathbf{b} = R \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$, where $R = \\|\\Delta \\mathbf{b}\\|_{2} = 0.01 \\sqrt{2.030225}$.\nSo, $\\Delta b_1 = \\frac{R}{\\sqrt{2}}$ and $\\Delta b_2 = -\\frac{R}{\\sqrt{2}}$.\nThe vector is explicitly:\n$$\n\\Delta \\mathbf{b} = \\frac{0.01 \\sqrt{2.030225}}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nLet us verify this construction. The norm is $\\|\\Delta \\mathbf{b}\\|_{2} = \\sqrt{(\\frac{R}{\\sqrt{2}})^2 + (-\\frac{R}{\\sqrt{2}})^2} = \\sqrt{\\frac{R^2}{2} + \\frac{R^2}{2}} = R$, which is correct by construction.\nNow check the condition on the components:\n$\\Delta b_2 - \\Delta b_1 = -\\frac{R}{\\sqrt{2}} - \\frac{R}{\\sqrt{2}} = -\\frac{2R}{\\sqrt{2}} = -R\\sqrt{2}$.\nSubstituting $R = 0.01 \\sqrt{2.030225}$:\n$$\n\\Delta b_2 - \\Delta b_1 = -(0.01 \\sqrt{2.030225})\\sqrt{2} = -0.01 \\sqrt{4.06045}.\n$$\nSince $2^2=4$, we have $\\sqrt{4.06045} > 2$. Thus, $\\Delta b_2 - \\Delta b_1 < -0.01 \\times 2 = -0.02$.\nAs $-0.02 < -0.015$, our condition is satisfied.\nLet's compute the change in the solution vector $\\mathbf{x}$.\n$\\Delta x_2 = 20(\\Delta b_2 - \\Delta b_1) = 20(-0.01\\sqrt{4.06045}) = -0.2\\sqrt{4.06045}$.\nNumerically, $\\Delta x_2 \\approx -0.2 \\times 2.01505... \\approx -0.403$.\nThe new component $\\tilde{x}_2 = x_2 + \\Delta x_2 = 0.3 + \\Delta x_2 \\approx 0.3 - 0.403 = -0.103 < 0$.\n\nFor completeness, we check $\\tilde{x}_1$.\n$\\Delta x_1 = 21 \\Delta b_1 - 20 \\Delta b_2 = 21(\\frac{R}{\\sqrt{2}}) - 20(-\\frac{R}{\\sqrt{2}}) = \\frac{41R}{\\sqrt{2}} = \\frac{41(0.01\\sqrt{2.030225})}{\\sqrt{2}} > 0$.\nSo $\\tilde{x}_1 = x_1 + \\Delta x_1 > 0.7 > 0$.\nThe new solution $\\tilde{\\mathbf{x}}$ has components $\\tilde{x}_1 > 0$ and $\\tilde{x}_2 < 0$, placing it in the fourth quadrant. The construction is valid.\n\nPart 2: Computation of the condition number $\\kappa_{2}(A)$.\n\nThe condition number in the matrix $2$-norm is defined as $\\kappa_{2}(A) = \\|A\\|_{2}\\|A^{-1}\\|_{2}$.\nThe problem requires this to be computed from first principles, not by invoking unproven shortcuts. The matrix $2$-norm (or spectral norm) of a matrix $M$ is defined as its largest singular value, $\\|M\\|_{2} = \\sigma_{\\max}(M) = \\sqrt{\\lambda_{\\max}(M^{T}M)}$, where $\\lambda_{\\max}(M^{T}M)$ is the largest eigenvalue of $M^{T}M$.\n\nThe given matrix $A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix}$ is a symmetric matrix, i.e., $A = A^{T}$.\nFor a symmetric matrix $A$, the product $A^{T}A$ becomes $A^{2}$.\nThe eigenvalues of $A^{2}$ are the squares of the eigenvalues of $A$. If $\\lambda$ is an eigenvalue of $A$ with eigenvector $\\mathbf{v}$, then $A^{2}\\mathbf{v} = A(A\\mathbf{v}) = A(\\lambda \\mathbf{v}) = \\lambda(A\\mathbf{v}) = \\lambda^2 \\mathbf{v}$.\nThus, $\\lambda_{\\max}(A^{T}A) = \\lambda_{\\max}(A^{2}) = (\\max_i |\\lambda_i(A)|)^{2}$, where $\\lambda_i(A)$ are the eigenvalues of $A$.\nTherefore, for a symmetric matrix $A$, its $2$-norm is the maximum absolute eigenvalue:\n$$\n\\|A\\|_{2} = \\sqrt{(\\max_i |\\lambda_i(A)|)^{2}} = \\max_i |\\lambda_i(A)|.\n$$\nThe inverse $A^{-1}$ is also symmetric. Its eigenvalues are the reciprocals of the eigenvalues of $A$.\nSo, $\\|A^{-1}\\|_{2} = \\max_i |1/\\lambda_i(A)| = 1 / \\min_i |\\lambda_i(A)|$.\nThis holds provided that $A$ is invertible, which it is.\nCombining these results, the condition number for a symmetric matrix $A$ is the ratio of its largest to its smallest absolute eigenvalues:\n$$\n\\kappa_{2}(A) = \\frac{\\max_i |\\lambda_i(A)|}{\\min_i |\\lambda_i(A)|}.\n$$\nWe now find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} 1-\\lambda & 1 \\\\ 1 & 1.05-\\lambda \\end{pmatrix} = (1-\\lambda)(1.05-\\lambda) - 1 = 0.\n$$\n$$\n\\lambda^{2} - 1.05\\lambda - \\lambda + 1.05 - 1 = 0\n$$\n$$\n\\lambda^{2} - 2.05\\lambda + 0.05 = 0.\n$$\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$$\n\\lambda = \\frac{2.05 \\pm \\sqrt{(-2.05)^{2} - 4(1)(0.05)}}{2} = \\frac{2.05 \\pm \\sqrt{4.2025 - 0.2}}{2} = \\frac{2.05 \\pm \\sqrt{4.0025}}{2}.\n$$\nBoth the trace ($2.05 > 0$) and determinant ($0.05 > 0$) of $A$ are positive, so $A$ is positive definite, and its eigenvalues are positive.\nThe maximum and minimum eigenvalues are:\n$$\n\\lambda_{\\max} = \\frac{2.05 + \\sqrt{4.0025}}{2}\n$$\n$$\n\\lambda_{\\min} = \\frac{2.05 - \\sqrt{4.0025}}{2}\n$$\nThe condition number is their ratio:\n$$\n\\kappa_{2}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{(2.05 + \\sqrt{4.0025})/2}{(2.05 - \\sqrt{4.0025})/2} = \\frac{2.05 + \\sqrt{4.0025}}{2.05 - \\sqrt{4.0025}}.\n$$\nTo compute the numerical value:\n$\\sqrt{4.0025} \\approx 2.0006249377...$\n$$\n\\kappa_{2}(A) \\approx \\frac{2.05 + 2.0006249377}{2.05 - 2.0006249377} = \\frac{4.0506249377}{0.0493750623} \\approx 82.038160...\n$$\nRounding to four significant figures, we get $82.04$.",
            "answer": "$$\\boxed{82.04}$$"
        },
        {
            "introduction": "Moving from analytical examples to computational practice, we now explore the Hilbert matrix, a classic case of extreme ill-conditioning. This exercise will guide you to write a program that demonstrates a critical, and often counter-intuitive, aspect of numerical computation: a very small residual does not guarantee an accurate solution . You will observe firsthand how, for an ill-conditioned system, the computed solution can be wildly incorrect even when it appears to satisfy the original equation almost perfectly.",
            "id": "2381734",
            "problem": "You are asked to investigate numerical sensitivity in solving linear systems by using the Hilbert matrix, a canonical example of an ill-conditioned matrix. Work exclusively in pure mathematical terms and report numerical results without physical units. All mathematical symbols in this statement are to be interpreted in the standard way: a linear system is written as $A \\mathbf{x} = \\mathbf{b}$, the two-norm of a vector or matrix as $\\|\\cdot\\|_2$, and the two-norm condition number as $\\kappa_2(A)$. The Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ is defined entrywise by $H_{ij} = 1/(i + j - 1)$ for $i,j \\in \\{1,\\dots,n\\}$.\n\nTask. For each test case, you will:\n1. Construct the Hilbert matrix $H$ of size $n$.\n2. Compute the two-norm condition number $\\kappa_2(H)$ using its definition in terms of singular values.\n3. Set the true solution to $\\mathbf{x}_{\\text{true}} = \\mathbf{1} \\in \\mathbb{R}^n$ (the vector of all ones) and form the right-hand side $\\mathbf{b} = H \\mathbf{x}_{\\text{true}}$.\n4. Solve the linear system $H \\hat{\\mathbf{x}} = \\mathbf{b}$ using a direct solver to obtain the numerical solution $\\hat{\\mathbf{x}}$ in floating-point arithmetic.\n5. Compute the relative forward error $e_{\\text{fwd}} = \\lVert \\hat{\\mathbf{x}} - \\mathbf{x}_{\\text{true}} \\rVert_2 / \\lVert \\mathbf{x}_{\\text{true}} \\rVert_2$.\n6. Compute the relative residual $r_{\\text{rel}} = \\lVert H \\hat{\\mathbf{x}} - \\mathbf{b} \\rVert_2 / \\lVert \\mathbf{b} \\rVert_2$.\n7. Construct a deterministic perturbation to $\\mathbf{b}$ of relative size $\\varepsilon_{\\text{rel}}$ as follows. Let $\\mathbf{v} \\in \\mathbb{R}^n$ have components $v_i = (-1)^{i-1}$ for $i \\in \\{1,\\dots,n\\}$. Define a scaled perturbation direction $\\mathbf{p} = \\lVert \\mathbf{b} \\rVert_2 \\, \\mathbf{v} / \\lVert \\mathbf{v} \\rVert_2$ so that $\\lVert \\mathbf{p} \\rVert_2 = \\lVert \\mathbf{b} \\rVert_2$. Form the perturbed right-hand side $\\mathbf{b}_{\\text{pert}} = \\mathbf{b} + \\varepsilon_{\\text{rel}} \\, \\mathbf{p}$.\n8. Solve $H \\tilde{\\mathbf{x}} = \\mathbf{b}_{\\text{pert}}$ for $\\tilde{\\mathbf{x}}$ using the same direct solver.\n9. Quantify the observed amplification factor of the perturbation in the solution as $A = \\big( \\lVert \\tilde{\\mathbf{x}} - \\hat{\\mathbf{x}} \\rVert_2 / \\lVert \\mathbf{x}_{\\text{true}} \\rVert_2 \\big) / \\varepsilon_{\\text{rel}}$.\n\nRationale. Steps $5$ and $6$ distinguish between the forward error in the computed solution and the residual of the linear system, highlighting that a small residual does not necessarily imply a small forward error for ill-conditioned matrices. Steps $7$â€“$9$ empirically assess sensitivity of the solution with respect to perturbations in $\\mathbf{b}$ by comparing the relative change in the solution to the relative change in the data.\n\nTest Suite. Use the following list of parameter pairs $(n,\\varepsilon_{\\text{rel}})$:\n- $(n,\\varepsilon_{\\text{rel}}) = (3, 1\\times 10^{-12})$,\n- $(n,\\varepsilon_{\\text{rel}}) = (10, 1\\times 10^{-10})$,\n- $(n,\\varepsilon_{\\text{rel}}) = (12, 1\\times 10^{-10})$.\n\nAnswer Specification. For each test case, compute the four scalars in this order:\n- $\\kappa_2(H)$,\n- $e_{\\text{fwd}}$,\n- $r_{\\text{rel}}$,\n- $A$.\n\nFinal Output Format. Your program should produce a single line of output containing a comma-separated list of all twelve numbers (four numbers per test case, ordered by the test cases as listed above) enclosed in square brackets. That is, the output must be of the form\n$[ \\kappa_2(H_{n_1}), e_{\\text{fwd},n_1}, r_{\\text{rel},n_1}, A_{n_1}, \\kappa_2(H_{n_2}), e_{\\text{fwd},n_2}, r_{\\text{rel},n_2}, A_{n_2}, \\kappa_2(H_{n_3}), e_{\\text{fwd},n_3}, r_{\\text{rel},n_3}, A_{n_3} ]$.\nNo additional text should be printed. Angles and physical units are not applicable; all values are dimensionless real numbers. The program must be a complete, runnable script that performs all computations and prints the final line exactly as specified.",
            "solution": "The problem statement is valid. It is scientifically grounded in the principles of numerical linear algebra, specifically concerning the analysis of ill-conditioned linear systems. The problem is well-posed, objective, and provides all necessary information to perform the computations and arrive at a unique, verifiable numerical result. The task is to investigate the numerical sensitivity of solving a linear system $A \\mathbf{x} = \\mathbf{b}$ where the matrix $A$ is the Hilbert matrix $H$, a classic example of severe ill-conditioning.\n\nThe solution proceeds by implementing the sequence of tasks specified for each test case $(n, \\varepsilon_{\\text{rel}})$. All calculations are performed using standard double-precision floating-point arithmetic.\n\nStep 1: Hilbert Matrix Construction\nFor a given dimension $n$, the Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ is constructed. Its entries are defined by the formula:\n$$\nH_{ij} = \\frac{1}{i + j - 1} \\quad \\text{for } i, j \\in \\{1, 2, \\dots, n\\}\n$$\nIn a computational environment with $0$-based indexing, this corresponds to $H_{ij} = 1 / ((i+1) + (j+1) - 1) = 1 / (i+j+1)$ for $i,j \\in \\{0, 1, \\dots, n-1\\}$.\n\nStep 2: Condition Number Calculation\nThe two-norm condition number, $\\kappa_2(H)$, is a measure of how sensitive the solution of a linear system is to changes in the input data. It is defined as the product of the two-norm of the matrix and its inverse, $\\kappa_2(H) = \\lVert H \\rVert_2 \\lVert H^{-1} \\rVert_2$. A more numerically stable way to compute it, as specified, is via the singular values of the matrix. Let the singular values of $H$ be $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n > 0$. Then the condition number is the ratio of the largest to the smallest singular value:\n$$\n\\kappa_2(H) = \\frac{\\sigma_{\\max}(H)}{\\sigma_{\\min}(H)} = \\frac{\\sigma_1}{\\sigma_n}\n$$\nThe singular values will be computed using a Singular Value Decomposition (SVD) algorithm.\n\nStep 3: Linear System Setup\nThe true solution is defined as a vector of all ones, $\\mathbf{x}_{\\text{true}} = \\mathbf{1} \\in \\mathbb{R}^n$. The right-hand side vector $\\mathbf{b}$ is then determined by performing the matrix-vector multiplication:\n$$\n\\mathbf{b} = H \\mathbf{x}_{\\text{true}}\n$$\n\nStep 4: Numerical Solution\nThe linear system $H \\hat{\\mathbf{x}} = \\mathbf{b}$ is solved for $\\hat{\\mathbf{x}}$ using a standard direct numerical solver, such as one based on LU decomposition with partial pivoting. Due to the ill-conditioning of $H$ and the limitations of floating-point arithmetic, the computed solution $\\hat{\\mathbf{x}}$ will deviate from the true solution $\\mathbf{x}_{\\text{true}}$.\n\nStep 5: Relative Forward Error\nThe relative forward error, $e_{\\text{fwd}}$, quantifies the deviation of the computed solution $\\hat{\\mathbf{x}}$ from the true solution $\\mathbf{x}_{\\text{true}}$, relative to the norm of the true solution.\n$$\ne_{\\text{fwd}} = \\frac{\\lVert \\hat{\\mathbf{x}} - \\mathbf{x}_{\\text{true}} \\rVert_2}{\\lVert \\mathbf{x}_{\\text{true}} \\rVert_2}\n$$\n\nStep 6: Relative Residual\nThe relative residual, $r_{\\text{rel}}$, measures how well the computed solution $\\hat{\\mathbf{x}}$ satisfies the original equation. It is the norm of the residual vector $H \\hat{\\mathbf{x}} - \\mathbf{b}$ relative to the norm of the original right-hand side $\\mathbf{b}$. This is also known as a measure of backward error.\n$$\nr_{\\text{rel}} = \\frac{\\lVert H \\hat{\\mathbf{x}} - \\mathbf{b} \\rVert_2}{\\lVert \\mathbf{b} \\rVert_2}\n$$\nFor ill-conditioned systems, it is characteristic for $r_{\\text{rel}}$ to be small (near machine precision) while $e_{\\text{fwd}}$ is large, illustrating that a small residual does not guarantee an accurate solution.\n\nStep 7: Perturbation Construction\nTo empirically test the system's sensitivity, a deterministic perturbation is introduced into the right-hand side vector $\\mathbf{b}$. A perturbation direction vector $\\mathbf{v} \\in \\mathbb{R}^n$ is defined with alternating components, $v_i = (-1)^{i-1}$ for $i \\in \\{1, \\dots, n\\}$. This vector is scaled to form a perturbation $\\mathbf{p}$ with a norm equal to that of $\\mathbf{b}$:\n$$\n\\mathbf{p} = \\frac{\\lVert \\mathbf{b} \\rVert_2}{\\lVert \\mathbf{v} \\rVert_2} \\mathbf{v}\n$$\nThe perturbed right-hand side, $\\mathbf{b}_{\\text{pert}}$, is then constructed by adding this perturbation, scaled by the relative factor $\\varepsilon_{\\text{rel}}$:\n$$\n\\mathbf{b}_{\\text{pert}} = \\mathbf{b} + \\varepsilon_{\\text{rel}} \\mathbf{p}\n$$\nThe relative magnitude of this perturbation is $\\lVert \\mathbf{b}_{\\text{pert}} - \\mathbf{b} \\rVert_2 / \\lVert \\mathbf{b} \\rVert_2 = \\varepsilon_{\\text{rel}}$. The choice of $\\mathbf{v}$ with alternating signs is designed to excite the eigenvector corresponding to the smallest eigenvalue of $H$ (since $H$ is symmetric positive definite, its singular vectors are its eigenvectors), thereby maximizing the response of the system to the perturbation.\n\nStep 8: Solving the Perturbed System\nThe new linear system involving the perturbed right-hand side, $H \\tilde{\\mathbf{x}} = \\mathbf{b}_{\\text{pert}}$, is solved for $\\tilde{\\mathbf{x}}$ using the same direct solver.\n\nStep 9: Amplification Factor\nThe observed amplification factor, $A$, is computed. This factor measures the ratio of the relative change in the solution to the relative change in the right-hand side data. The change in the solution is measured as $\\lVert \\tilde{\\mathbf{x}} - \\hat{\\mathbf{x}} \\rVert_2$, and this is normalized by $\\lVert \\mathbf{x}_{\\text{true}} \\rVert_2$.\n$$\nA = \\frac{\\left( \\frac{\\lVert \\tilde{\\mathbf{x}} - \\hat{\\mathbf{x}} \\rVert_2}{\\lVert \\mathbf{x}_{\\text{true}} \\Vert_2} \\right)}{\\varepsilon_{\\text{rel}}}\n$$\nThe theory of condition numbers predicts that this amplification factor $A$ can be as large as the condition number $\\kappa_2(H)$. The experiment is designed to observe this relationship.\n\nBy executing these steps for each $(n, \\varepsilon_{\\text{rel}})$ pair, we will generate the required set of twelve numerical values that characterize the behavior of this ill-conditioned system. For larger $n$, we expect $\\kappa_2(H)$ to grow extremely rapidly, leading to a large forward error $e_{\\text{fwd}}$ and an amplification factor $A$ of a similar magnitude to $\\kappa_2(H)$, even while the relative residual $r_{\\text{rel}}$ remains small.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (3, 1e-12),\n        (10, 1e-10),\n        (12, 1e-10),\n    ]\n\n    results = []\n    for n, eps_rel in test_cases:\n        # Task 1: Construct the Hilbert matrix H.\n        # Use 0-based indexing for numpy arrays.\n        # H_ij = 1 / ((i+1) + (j+1) - 1) = 1 / (i + j + 1)\n        i, j = np.ogrid[0:n, 0:n]\n        H = 1.0 / (i + j + 1)\n\n        # Task 2: Compute the two-norm condition number kappa_2(H).\n        # This is done via singular values.\n        singular_values = np.linalg.svd(H, compute_uv=False)\n        kappa_2_H = singular_values[0] / singular_values[-1]\n\n        # Task 3: Set up the true solution and the right-hand side.\n        x_true = np.ones(n)\n        b = H @ x_true\n\n        # Task 4: Solve the linear system to get the numerical solution.\n        x_hat = np.linalg.solve(H, b)\n\n        # Task 5: Compute the relative forward error e_fwd.\n        norm_x_true = np.linalg.norm(x_true, 2)\n        e_fwd = np.linalg.norm(x_hat - x_true, 2) / norm_x_true\n\n        # Task 6: Compute the relative residual r_rel.\n        norm_b = np.linalg.norm(b, 2)\n        r_rel = np.linalg.norm(H @ x_hat - b, 2) / norm_b\n\n        # Task 7: Construct a deterministic perturbation to b.\n        # v_i = (-1)^(i-1) for 1-based index i.\n        # v_i = (-1)^i for 0-based index i.\n        v = (-1.0)**np.arange(n)\n        norm_v = np.linalg.norm(v, 2)\n        p = (norm_b / norm_v) * v\n        b_pert = b + eps_rel * p\n\n        # Task 8: Solve the perturbed system.\n        x_tilde = np.linalg.solve(H, b_pert)\n\n        # Task 9: Quantify the observed amplification factor A.\n        # Note: the problem defines the change relative to x_hat (tilde{x} - hat{x})\n        # but normalizes by x_true.\n        # A = ( ||x_tilde - x_hat|| / ||x_true|| ) / eps_rel\n        A = (np.linalg.norm(x_tilde - x_hat, 2) / norm_x_true) / eps_rel\n        \n        # Append the four required scalars for the current test case.\n        results.extend([kappa_2_H, e_fwd, r_rel, A])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Having learned to recognize and diagnose ill-conditioning, we turn to a powerful technique for mitigating its effects, particularly when dealing with noisy data from physical experiments. This practice introduces Truncated Singular Value Decomposition (TSVD) as a method of regularization, allowing you to systematically filter out the \"noisy\" components of the solution that correspond to tiny singular values . This exercise provides a practical framework for solving ill-posed inverse problems, where the goal is to find a stable and physically meaningful approximate solution rather than an exact but noise-dominated one.",
            "id": "2381778",
            "problem": "You are given a family of square linear systems that are known to be ill-conditioned, constructed from the Hilbert matrix. Consider, for a positive integer $n$, the matrix $A \\in \\mathbb{R}^{n \\times n}$ with entries $A_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1,2,\\ldots,n\\}$. Let the unknown vector be $\\mathbf{x}^\\star \\in \\mathbb{R}^n$ with components $x^\\star_j = \\dfrac{(-1)^{j+1}}{j}$ for $j \\in \\{1,2,\\ldots,n\\}$. The clean data vector is $\\mathbf{b}_{\\mathrm{clean}} = A \\mathbf{x}^\\star$. To model experimental noise in a deterministic and reproducible way, define $\\mathbf{w} \\in \\mathbb{R}^n$ by $w_i = \\cos(i)$ for $i \\in \\{1,2,\\ldots,n\\}$, and then define the noise vector $\\mathbf{e} = \\eta \\, \\lVert \\mathbf{b}_{\\mathrm{clean}} \\rVert_2 \\, \\dfrac{\\mathbf{w}}{\\lVert \\mathbf{w} \\rVert_2}$ for a given relative noise level $\\eta > 0$. The measured data are $\\mathbf{b} = \\mathbf{b}_{\\mathrm{clean}} + \\mathbf{e}$.\n\nLet the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^\\top$, where $\\Sigma = \\operatorname{diag}(\\sigma_1,\\sigma_2,\\ldots,\\sigma_n)$ with singular values ordered so that $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n > 0$, and $U,V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices. For a given relative threshold $\\tau \\in (0,1)$, define the index set of retained modes $K_\\tau = \\{ i \\in \\{1,2,\\ldots,n\\} \\mid \\sigma_i / \\sigma_1 \\ge \\tau \\}$. Define the truncated SVD estimate of $\\mathbf{x}^\\star$ by\n$$\n\\hat{\\mathbf{x}}_\\tau \\;=\\; \\sum_{i \\in K_\\tau} \\frac{\\mathbf{u}_i^\\top \\mathbf{b}}{\\sigma_i} \\, \\mathbf{v}_i,\n$$\nwhere $\\mathbf{u}_i$ and $\\mathbf{v}_i$ denote the $i$-th columns of $U$ and $V$, respectively. Define the effective numerical rank by $r_\\tau = |K_\\tau|$. Define the filtered condition number\n$$\n\\kappa_\\tau(A) \\;=\\; \\frac{\\max_{i \\in K_\\tau} \\sigma_i}{\\min_{i \\in K_\\tau} \\sigma_i},\n$$\nwith the convention that if $K_\\tau = \\varnothing$ then $\\kappa_\\tau(A) = +\\infty$. Finally, define the relative reconstruction error\n$$\nE_\\tau \\;=\\; \\frac{\\lVert \\hat{\\mathbf{x}}_\\tau - \\mathbf{x}^\\star \\rVert_2}{\\lVert \\mathbf{x}^\\star \\rVert_2}.\n$$\n\nYour task is to compute, for each test case below, the triplet consisting of $(E_\\tau, r_\\tau, \\kappa_\\tau(A))$. All vector and matrix norms are the Euclidean norm ($2$-norm). All computations are dimensionless.\n\nTest suite (each test case is a tuple $(n,\\eta,\\tau)$):\n- Test $1$: $(n,\\eta,\\tau) = (10, 10^{-6}, 10^{-12})$.\n- Test $2$: $(n,\\eta,\\tau) = (10, 10^{-4}, 10^{-8})$.\n- Test $3$: $(n,\\eta,\\tau) = (12, 10^{-4}, 10^{-4})$.\n- Test $4$: $(n,\\eta,\\tau) = (12, 10^{-2}, 10^{-4})$.\n- Test $5$: $(n,\\eta,\\tau) = (12, 10^{-2}, 10^{-2})$.\n\nFinal output format requirements:\n- For each test case, output a list $[E_\\tau, r_\\tau, \\kappa_\\tau(A)]$ where $E_\\tau$ and $\\kappa_\\tau(A)$ are floating-point numbers rounded to $8$ decimal places, and $r_\\tau$ is an integer.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element being the list for a test case, in the same order as specified above. For example: $[[E_1,r_1,\\kappa_1],[E_2,r_2,\\kappa_2],\\ldots]$.",
            "solution": "The problem requires the analysis of a linear system $A \\mathbf{x} = \\mathbf{b}$ where the matrix $A$ is a Hilbert matrix, which is known to be severely ill-conditioned. The ill-conditioning implies that small perturbations in the data vector $\\mathbf{b}$ can lead to large variations in the solution vector $\\mathbf{x}$. To address this, a regularization technique known as Truncated Singular Value Decomposition (TSVD) is employed. We are tasked with computing several quantities that characterize the performance of this method for given system parameters. The analysis will be performed for several test cases, each defined by a triplet $(n, \\eta, \\tau)$, where $n$ is the dimension of the system, $\\eta$ is the relative noise level, and $\\tau$ is the regularization threshold.\n\nThe procedure for each test case $(n, \\eta, \\tau)$ is as follows:\n\nStep $1$: Construct the linear system components.\nThe Hilbert matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined by its entries\n$$A_{ij} = \\frac{1}{i + j - 1}$$\nfor $i, j \\in \\{1, 2, \\ldots, n\\}$. The true, unknown solution vector $\\mathbf{x}^\\star \\in \\mathbb{R}^n$ is given by\n$$x^\\star_j = \\frac{(-1)^{j+1}}{j}$$\nfor $j \\in \\{1, 2, \\ldots, n\\}$. The \"clean\" data vector $\\mathbf{b}_{\\mathrm{clean}} \\in \\mathbb{R}^n$ is the result of applying the matrix $A$ to the true solution $\\mathbf{x}^\\star$:\n$$\\mathbf{b}_{\\mathrm{clean}} = A \\mathbf{x}^\\star$$\nTo simulate measurement noise, a deterministic noise vector $\\mathbf{e} \\in \\mathbb{R}^n$ is constructed. First, an auxiliary vector $\\mathbf{w} \\in \\mathbb{R}^n$ is defined as $w_i = \\cos(i)$ for $i \\in \\{1, 2, \\ldots, n\\}$. The noise vector $\\mathbf{e}$ is then scaled relative to the norm of the clean data:\n$$\\mathbf{e} = \\eta \\, \\lVert \\mathbf{b}_{\\mathrm{clean}} \\rVert_2 \\, \\frac{\\mathbf{w}}{\\lVert \\mathbf{w} \\rVert_2}$$\nwhere $\\eta > 0$ is the relative noise level and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm (or $2$-norm). The measured, or noisy, data vector $\\mathbf{b} \\in \\mathbb{R}^n$ is the sum of the clean data and the noise:\n$$\\mathbf{b} = \\mathbf{b}_{\\mathrm{clean}} + \\mathbf{e}$$\n\nStep $2$: Perform the Singular Value Decomposition (SVD).\nThe SVD of the matrix $A$ is computed, yielding the factorization $A = U \\Sigma V^\\top$. Here, $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices whose columns are the left and right singular vectors, respectively. $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix containing the singular values $\\sigma_i$ of $A$, ordered non-increasingly: $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n > 0$. The positivity of all singular values is guaranteed as the Hilbert matrix is non-singular.\n\nStep $3$: Apply the TSVD regularization.\nThe core of TSVD is to filter out the singular modes that are most sensitive to noise. These correspond to the smallest singular values. A relative threshold $\\tau \\in (0,1)$ is used to define the set of retained modes:\n$$K_\\tau = \\{ i \\in \\{1, 2, \\ldots, n\\} \\mid \\sigma_i / \\sigma_1 \\ge \\tau \\}$$\nThis set includes indices of singular values that are not \"too small\" relative to the largest singular value $\\sigma_1$. The cardinality of this set, $r_\\tau = |K_\\tau|$, is the effective numerical rank of the matrix $A$ at the given threshold $\\tau$.\n\nStep $4$: Compute the TSVD solution and associated quantities.\nThe TSVD-regularized solution, $\\hat{\\mathbf{x}}_\\tau$, is constructed by summing only over the retained modes:\n$$\\hat{\\mathbf{x}}_\\tau = \\sum_{i \\in K_\\tau} \\frac{\\mathbf{u}_i^\\top \\mathbf{b}}{\\sigma_i} \\mathbf{v}_i$$\nwhere $\\mathbf{u}_i$ and $\\mathbf{v}_i$ are the $i$-th columns of $U$ and $V$. This formula effectively inverts the well-behaved part of the system while discarding the ill-behaved part.\n\nWith the set $K_\\tau$ determined, the filtered condition number $\\kappa_\\tau(A)$ is calculated as the ratio of the largest retained singular value to the smallest retained singular value:\n$$\\kappa_\\tau(A) = \\frac{\\max_{i \\in K_\\tau} \\sigma_i}{\\min_{i \\in K_\\tau} \\sigma_i} = \\frac{\\sigma_1}{\\min_{i \\in K_\\tau} \\sigma_i}$$\nThe equality holds because $\\sigma_1$ is the maximum singular value and its index $i=1$ is always in $K_\\tau$ for $\\tau \\le 1$.\n\nFinally, the quality of the reconstruction is assessed by computing the relative reconstruction error $E_\\tau$:\n$$E_\\tau = \\frac{\\lVert \\hat{\\mathbf{x}}_\\tau - \\mathbf{x}^\\star \\rVert_2}{\\lVert \\mathbf{x}^\\star \\rVert_2}$$\nThis metric compares the distance between the estimated solution and the true solution, relative to the magnitude of the true solution.\n\nFor each test case, we will numerically implement these steps using standard double-precision floating-point arithmetic to obtain the triplet $(E_\\tau, r_\\tau, \\kappa_\\tau(A))$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a suite of test cases involving ill-conditioned\n    linear systems regularized with Truncated SVD.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 1e-6, 1e-12),\n        (10, 1e-4, 1e-8),\n        (12, 1e-4, 1e-4),\n        (12, 1e-2, 1e-4),\n        (12, 1e-2, 1e-2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, eta, tau = case\n\n        # Step 1: Construct the linear system components\n        # Construct Hilbert matrix A\n        i = np.arange(1, n + 1).reshape(-1, 1)\n        j = np.arange(1, n + 1).reshape(1, -1)\n        A = 1.0 / (i + j - 1)\n\n        # Construct true solution vector x_star\n        j_vec = np.arange(1, n + 1)\n        x_star = ((-1)**(j_vec + 1)) / j_vec\n\n        # Calculate clean data vector b_clean\n        b_clean = A @ x_star\n        norm_b_clean = np.linalg.norm(b_clean)\n\n        # Construct noise vector e\n        i_vec = np.arange(1, n + 1)\n        w = np.cos(i_vec)\n        norm_w = np.linalg.norm(w)\n        e = eta * norm_b_clean * w / norm_w\n\n        # Calculate measured data vector b\n        b = b_clean + e\n\n        # Step 2: Perform Singular Value Decomposition (SVD)\n        U, s, Vt = np.linalg.svd(A)\n        # V from SVD is Vt.T\n        V = Vt.T\n\n        # Step 3: Apply TSVD regularization\n        # Find the set of retained modes K_tau\n        sigma_1 = s[0]\n        retained_indices = np.where(s / sigma_1 >= tau)[0]\n        \n        # Calculate effective numerical rank r_tau\n        r_tau = len(retained_indices)\n\n        # Step 4: Compute the TSVD solution and associated quantities\n        \n        # Calculate filtered condition number kappa_tau(A)\n        # Note: s is sorted descending, so the smallest retained singular\n        # value is at the last index of retained_indices.\n        min_retained_sigma = s[retained_indices[-1]]\n        kappa_tau = sigma_1 / min_retained_sigma\n        \n        # Calculate TSVD solution hat_x_tau\n        # The sum is over i in K_tau of (u_i^T * b / sigma_i) * v_i\n        # In matrix terms: V * Sigma_inv_trunc * U^T * b\n        hat_x_tau = np.zeros(n)\n        for idx in retained_indices:\n            u_i = U[:, idx]\n            v_i = V[:, idx]\n            sigma_i = s[idx]\n            hat_x_tau += (u_i.T @ b / sigma_i) * v_i\n            \n        # Calculate relative reconstruction error E_tau\n        norm_x_star = np.linalg.norm(x_star)\n        err_norm = np.linalg.norm(hat_x_tau - x_star)\n        E_tau = err_norm / norm_x_star\n\n        # Store the result triplet, rounded as specified\n        result_triplet = [\n            round(E_tau, 8),\n            int(r_tau),\n            round(kappa_tau, 8)\n        ]\n        results.append(result_triplet)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}