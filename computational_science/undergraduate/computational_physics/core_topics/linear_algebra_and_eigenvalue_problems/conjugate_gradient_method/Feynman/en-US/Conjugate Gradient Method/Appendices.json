{
    "hands_on_practices": [
        {
            "introduction": "The best way to truly grasp an iterative algorithm like the Conjugate Gradient method is to perform an iteration by hand. This first practice exercise guides you through one complete cycle, starting from an initial guess of the zero vector, which is a common starting point in practice. By calculating the initial residual, search direction, optimal step size, and finally the first updated solution $x_1$, you will build a concrete, step-by-step understanding of the algorithm's mechanics.",
            "id": "1393666",
            "problem": "Consider the linear system of equations $Ax=b$, where the matrix $A$ and the vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\nThe matrix $A$ is symmetric and positive-definite.\n\nStarting with an initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one iteration of the Conjugate Gradient method to find the first updated solution, $x_1$.\n\nExpress your answer as a row matrix containing the components of $x_1$ as exact fractions.",
            "solution": "We apply the Conjugate Gradient method for a symmetric positive-definite matrix $A$ starting from $x_{0}$. The standard first-iteration formulas are:\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\nGiven $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$, compute\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\nNext, compute $A p_{0}$:\n$$\nA p_{0} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\nCompute the scalar products:\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\nThus,\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\nUpdate the solution:\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\nExpressed as a row matrix, the components of $x_{1}$ are $\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having completed a full iteration, we now focus on one of its most critical components: the step size $\\alpha_k$. This value is not arbitrary; it is carefully chosen to minimize the error in the current search direction, which is the key to the method's efficiency. This exercise isolates the calculation of $\\alpha_k$, allowing you to see how the residual and the matrix's properties (via the term $p_k^T A p_k$) determine the optimal step to take.",
            "id": "1393661",
            "problem": "In an application of the Conjugate Gradient (CG) method to solve a system of linear equations $Ax=b$, a crucial step is to determine the optimal length to move along a given search direction. At the $k$-th iteration of the algorithm, the current approximation of the solution is updated via $x_{k+1} = x_k + \\alpha_k p_k$, where $\\alpha_k$ is the step size and $p_k$ is the search direction.\n\nSuppose that for a particular problem, the system matrix $A$ is given by:\n$$A = \\begin{pmatrix} 4 & 1 & 1 \\\\ 1 & 3 & -1 \\\\ 1 & -1 & 5 \\end{pmatrix}$$\nAt the $k$-th iteration, the residual vector is $r_k = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix}$ and the A-orthogonal (or conjugate) search direction is $p_k = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\nCalculate the optimal step size $\\alpha_k$ for this iteration. Round your final answer to four significant figures.",
            "solution": "In the Conjugate Gradient method for symmetric positive definite $A$, the step size $\\alpha_{k}$ is chosen to minimize the quadratic functional along the search direction $p_{k}$. This yields the standard formula\n$$\\alpha_{k}=\\frac{r_{k}^{T}r_{k}}{p_{k}^{T}A p_{k}}.$$\n\nGiven $r_{k}=\\begin{pmatrix}2\\\\-1\\\\1\\end{pmatrix}$ and $p_{k}=\\begin{pmatrix}3\\\\0\\\\1\\end{pmatrix}$, first compute the numerator:\n$$r_{k}^{T}r_{k}=2^{2}+(-1)^{2}+1^{2}=4+1+1=6.$$\n\nNext compute $A p_{k}$ with\n$$A=\\begin{pmatrix} 4 & 1 & 1 \\\\ 1 & 3 & -1 \\\\ 1 & -1 & 5 \\end{pmatrix},\\quad p_{k}=\\begin{pmatrix}3\\\\0\\\\1\\end{pmatrix},$$\nso\n$$A p_{k}=\\begin{pmatrix} 4\\cdot 3+1\\cdot 0+1\\cdot 1 \\\\ 1\\cdot 3+3\\cdot 0+(-1)\\cdot 1 \\\\ 1\\cdot 3+(-1)\\cdot 0+5\\cdot 1 \\end{pmatrix}=\\begin{pmatrix}13\\\\2\\\\8\\end{pmatrix}.$$\n\nThen compute the denominator:\n$$p_{k}^{T}A p_{k}=\\begin{pmatrix}3&0&1\\end{pmatrix}\\begin{pmatrix}13\\\\2\\\\8\\end{pmatrix}=3\\cdot 13+0\\cdot 2+1\\cdot 8=39+8=47.$$\n\nTherefore,\n$$\\alpha_{k}=\\frac{6}{47}.$$\n\nRounding to four significant figures gives\n$$\\alpha_{k}\\approx 0.1277.$$",
            "answer": "$$\\boxed{0.1277}$$"
        },
        {
            "introduction": "The theoretical elegance of the Conjugate Gradient method rests on the assumption that the system matrix $A$ is symmetric and positive-definite. But what happens in a computational setting when this condition is violated? This final practice moves from manual calculation to coding, challenging you to implement the CG algorithm and detect its points of failure, specifically when the denominator $p_k^T A p_k$ becomes non-positive. Mastering this diagnostic is an essential skill for applying numerical methods to complex, real-world problems where ideal conditions are not guaranteed.",
            "id": "2382410",
            "problem": "Implement a program that, for each provided symmetric matrix and right-hand side vector, runs the Conjugate Gradient (CG) method starting from the zero vector and identifies the first iteration index at which the denominator used to compute the next step length is nonpositive. Specifically, at iteration index $k$ (with $k$ starting at $0$), define the search direction $p_k$ and evaluate the quadratic form $p_k^{\\mathsf{T}} A p_k$. The algorithm is said to break down at the smallest $k$ such that $p_k^{\\mathsf{T}} A p_k \\le 0$. If no such index occurs within a prescribed maximum number of iterations, return $-1$.\n\nBackground and required reasoning base: Start from the quadratic minimization problem with a symmetric matrix $A$ and vector $b$, namely the energy functional $\\phi(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. The Conjugate Gradient (CG) method can be understood as producing iterates $x_k$ by line-search minimization of $\\phi(x_k + \\alpha p_k)$ along $A$-conjugate directions $p_k$, where the gradient is $\\nabla \\phi(x) = A x - b$ and the residual is defined as $r_k = b - A x_k$. In the strongly convex case (symmetric positive definite, SPD), every nonzero vector $v$ satisfies $v^{\\mathsf{T}} A v > 0$, which guarantees positive curvature and well-defined step lengths. In the indefinite or semidefinite case, directions can exist with $v^{\\mathsf{T}} A v \\le 0$, destroying the guarantee of a unique minimizer along $p_k$ and causing algorithmic breakdown. Your program must embody this principle by computing $p_k^{\\mathsf{T}} A p_k$ at each iteration and reporting the first $k$ where it is nonpositive.\n\nPrecise computational task: For each test case below, run the Conjugate Gradient iterations with the standard residual and direction recurrences, using the zero vector as the initial guess $x_0 = 0$. At each iteration, before computing the step length, evaluate $p_k^{\\mathsf{T}} A p_k$. If $p_k^{\\mathsf{T}} A p_k \\le 0$, report the current index $k$ for that test case and stop processing that case. If convergence is reached without encountering $p_k^{\\mathsf{T}} A p_k \\le 0$, or if no such event occurs within a maximum of $N_{\\max}$ iterations, report $-1$ for that case. Use a relative stopping test based on the residual norm $\\lVert r_k \\rVert_2 \\le \\varepsilon \\lVert b \\rVert_2$ with tolerance $\\varepsilon = 10^{-12}$. Use $N_{\\max} = 5n$, where $n$ is the dimension of $A$.\n\nTest suite to ensure coverage:\n- Case $\\mathbf{1}$ (SPD, no breakdown expected): \n  - $A_1 = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n  - $b_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$.\n- Case $\\mathbf{2}$ (indefinite, delayed breakdown):\n  - $A_2 = \\begin{bmatrix} 2 & 1 \\\\ 1 & -2 \\end{bmatrix}$,\n  - $b_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n- Case $\\mathbf{3}$ (singular positive semidefinite, zero curvature at start):\n  - $A_3 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$,\n  - $b_3 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n- Case $\\mathbf{4}$ (indefinite, immediate negative curvature):\n  - $A_4 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$,\n  - $b_4 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n\nAlgorithmic conventions:\n- Initialize $x_0 = \\mathbf{0}$, $r_0 = b - A x_0 = b$, and $p_0 = r_0$.\n- At iteration $k$, evaluate $p_k^{\\mathsf{T}} A p_k$. If $p_k^{\\mathsf{T}} A p_k \\le 0$, return $k$ for that test.\n- Otherwise, proceed with the Conjugate Gradient iteration and continue until convergence or $N_{\\max}$ iterations, whichever comes first.\n- If no breakdown occurs, return $-1$.\n\nFinal required output: Your program should produce a single line of output containing the results for the four test cases as a comma-separated list of integers enclosed in square brackets, in the order of the cases above; that is, a single line of the form $[k_1,k_2,k_3,k_4]$, where $k_j$ is the first iteration index with $p_k^{\\mathsf{T}} A p_k \\le 0$ for case $j$, or $-1$ if no such index occurs within $N_{\\max}$ iterations for that case. No units are involved in this problem, and no angles or percentages are required.",
            "solution": "The problem requires the implementation of a modified Conjugate Gradient (CG) algorithm to detect breakdown when the system matrix $A$ is not symmetric positive definite (SPD). The breakdown is defined as the first iteration $k$ at which the quadratic form $p_k^{\\mathsf{T}} A p_k$ is non-positive. We will first review the theoretical foundation of the CG method and its failure modes, then detail the specific algorithm to be implemented, and finally analyze the provided test cases.\n\nThe Conjugate Gradient method is an iterative algorithm for solving systems of linear equations of the form $A x = b$, where the matrix $A$ is symmetric and positive definite. The method can be derived by considering the equivalent optimization problem of minimizing the quadratic energy functional:\n$$\n\\phi(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x\n$$\nThe gradient of this functional is $\\nabla \\phi(x) = A x - b$. The solution to $A x = b$ is the unique minimizer of $\\phi(x)$, where the gradient is zero. The negative of the gradient, $r = b - A x$, is defined as the residual.\n\nThe standard CG algorithm, starting from an initial guess $x_0$, generates a sequence of iterates $x_k$ that progressively minimize $\\phi(x)$. The update from $x_k$ to $x_{k+1}$ is performed by a line search along a carefully chosen search direction $p_k$. The search directions $\\{p_0, p_1, \\dots\\}$ are constructed to be $A$-conjugate (or $A$-orthogonal), meaning $p_i^{\\mathsf{T}} A p_j = 0$ for all $i \\neq j$.\n\nThe canonical algorithm for an $n \\times n$ SPD matrix $A$ is as follows:\nInitialize:\n$x_0 = \\mathbf{0}$\n$r_0 = b - A x_0 = b$\n$p_0 = r_0$\n\nFor $k = 0, 1, 2, \\dots$ until convergence:\n1.  Compute the step length $\\alpha_k$, which is the minimizer of $\\phi(x_k + \\alpha p_k)$:\n    $$\n    \\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}\n    $$\n2.  Update the solution estimate:\n    $$\n    x_{k+1} = x_k + \\alpha_k p_k\n    $$\n3.  Update the residual using a computationally efficient recurrence:\n    $$\n    r_{k+1} = r_k - \\alpha_k A p_k\n    $$\n4.  Compute the coefficient for the next search direction:\n    $$\n    \\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} r_{k+1}}{r_k^{\\mathsf{T}} r_k}\n    $$\n5.  Update the search direction to be $A$-conjugate to previous directions:\n    $$\n    p_{k+1} = r_{k+1} + \\beta_k p_k\n    $$\n\nThe validity of this algorithm fundamentally relies on the matrix $A$ being SPD. The positive-definiteness ensures that for any non-zero vector $v$, the quadratic form $v^{\\mathsf{T}} A v > 0$. This guarantees that the denominator in the expression for $\\alpha_k$, namely $p_k^{\\mathsf{T}} A p_k$, is strictly positive for any non-zero search direction $p_k$. A positive denominator ensures that $\\alpha_k$ is well-defined and positive (since $r_k^{\\mathsf{T}} r_k > 0$ unless the solution is found), guaranteeing that each step is a descent step for the functional $\\phi(x)$.\n\nThe problem at hand concerns the behavior of the CG algorithm when $A$ is not SPD, i.e., when it is symmetric indefinite or positive semidefinite. In such cases, a search direction $p_k$ may be generated for which $p_k^{\\mathsf{T}} A p_k \\le 0$. This leads to an algorithmic breakdown.\n-   **Case 1: $p_k^{\\mathsf{T}} A p_k = 0$ (Zero Curvature).** This occurs if $A$ is positive semidefinite and $p_k$ is a direction of zero curvature (i.e., $A p_k$ is orthogonal to $p_k$). If $r_k \\neq 0$, the formula for the step length $\\alpha_k$ involves division by zero. The algorithm cannot proceed. This situation is encountered if $A$ is singular and a search direction is generated that has a component in the null space of $A$.\n-   **Case 2: $p_k^{\\mathsf{T}} A p_k < 0$ (Negative Curvature).** This occurs if $A$ is indefinite. A negative value for $p_k^{\\mathsf{T}} A p_k$ implies that the functional $\\phi(x)$ is unbounded below along the direction $p_k$. Minimization along this line is not possible, as $\\phi(x)$ decreases without bound as we move in the direction given by $\\text{sign}(\\alpha_k) p_k$. The CG algorithm, in its standard form, is not equipped to handle this and must terminate.\n\nThe task is to implement a modified CG procedure that explicitly checks for this breakdown condition. At each iteration $k$, starting with $k=0$, we must first compute $p_k^{\\mathsf{T}} A p_k$ and test its sign.\n\nThe specific algorithm is therefore:\nInitialize:\n$x_0 = \\mathbf{0}$, $r_0 = b$, $p_0 = r_0$. Let $n$ be the dimension of $A$. Set maximum iterations $N_{\\max} = 5n$ and tolerance $\\varepsilon = 10^{-12}$. Check initial convergence: if $\\lVert r_0 \\rVert_2 \\le \\varepsilon \\lVert b \\rVert_2$, terminate and report $-1$.\n\nFor $k = 0, 1, 2, \\dots, N_{\\max}-1$:\n1.  Compute the matrix-vector product $v_k = A p_k$.\n2.  Compute the quadratic form $d_k = p_k^{\\mathsf{T}} v_k = p_k^{\\mathsf{T}} A p_k$.\n3.  **Breakdown Check:** If $d_k \\le 0$, the algorithm has broken down. Report the current iteration index $k$ and terminate for this test case.\n4.  If no breakdown, proceed with the standard CG update:\n    $\\alpha_k = (r_k^{\\mathsf{T}} r_k) / d_k$\n    $x_{k+1} = x_k + \\alpha_k p_k$\n    $r_{k+1} = r_k - \\alpha_k v_k$\n5.  **Convergence Check:** If $\\lVert r_{k+1} \\rVert_2 \\le \\varepsilon \\lVert b \\rVert_2$, the algorithm has converged. Report $-1$ and terminate for this test case.\n6.  Compute $\\beta_k = (r_{k+1}^{\\mathsf{T}} r_{k+1}) / (r_k^{\\mathsf{T}} r_k)$.\n7.  Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n\nIf the loop completes without breakdown or convergence, report $-1$.\n\nWe now analyze the given test cases:\n-   **Case 1:** $A_1$ is an SPD matrix. Its eigenvalues are approximately $4.44$, $2.56$, and $2.0$. Since all are positive, $p_k^{\\mathsf{T}} A_1 p_k$ will always be positive for any non-zero $p_k$. No breakdown is expected. The algorithm should converge. The expected result is $-1$.\n-   **Case 2:** $A_2$ is an indefinite matrix with eigenvalues $\\pm\\sqrt{5}$. Breakdown is possible. For $k=0$, $p_0 = b_2 = [1, 1]^{\\mathsf{T}}$. Then $p_0^{\\mathsf{T}} A_2 p_0 = [1, 1] [2, 1; 1, -2] [1; 1] = 2 > 0$. The algorithm proceeds. In the next step, for $k=1$, a search direction $p_1$ is generated for which $p_1^{\\mathsf{T}} A_2 p_1 < 0$. Breakdown is expected at $k=1$.\n-   **Case 3:** $A_3$ is a singular positive semidefinite matrix with eigenvalues $1$ and $0$. Breakdown is possible. For $k=0$, $p_0 = b_3 = [0, 1]^{\\mathsf{T}}$. This vector is an eigenvector of $A_3$ corresponding to the eigenvalue $0$ (i.e., it is in the null space of $A_3$). Thus, $p_0^{\\mathsf{T}} A_3 p_0 = 0$. The breakdown condition $p_k^{\\mathsf{T}} A p_k \\le 0$ is met immediately. The expected result is $k=0$.\n-   **Case 4:** $A_4$ is an indefinite matrix with eigenvalues $\\pm 1$. Breakdown is possible. For $k=0$, $p_0 = b_4 = [1, -1]^{\\mathsf{T}}$. We compute $p_0^{\\mathsf{T}} A_4 p_0 = [1, -1] [0, 1; 1, 0] [1; -1] = -2 < 0$. The breakdown condition is met immediately. The expected result is $k=0$.\n\nThe implementation will follow this logic precisely for each case.",
            "answer": "$$\\boxed{[-1,1,0,0]}$$"
        }
    ]
}