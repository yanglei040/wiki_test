{
    "hands_on_practices": [
        {
            "introduction": "The power of the Lanczos method stems from a compact three-term recurrence relation that generates the tridiagonal matrix $T_k$. This mathematical structure is not unique; it is also the defining characteristic of orthogonal polynomials. This exercise illuminates this profound connection by having you implement the Lanczos algorithm on a specially constructed matrix whose characteristic polynomial is exactly a Chebyshev polynomial, allowing you to see the algorithm's mechanics unfold with analytical precision .",
            "id": "2406049",
            "problem": "You will implement and analyze the Lanczos method for real symmetric eigenproblems using a purposely constructed test matrix whose characteristic polynomial coincides with a classical orthogonal polynomial. The goal is to connect the three-term recurrence underlying orthogonal polynomials to the tridiagonal form produced by the Lanczos process, and to validate the construction numerically through a small test suite.\n\nStart from the following fundamental bases:\n- The eigenvalue problem for a real symmetric matrix: given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, its eigenvalues are real, and the Lanczos method constructs an orthonormal Krylov basis that yields a real symmetric tridiagonal projection $T_m \\in \\mathbb{R}^{m \\times m}$ whose eigenvalues (Ritz values) approximate those of $A$.\n- The Laplace expansion (cofactor expansion) of a determinant, and the fact that determinants of tridiagonal matrices obey a three-term recurrence.\n- The definition of classical orthogonal polynomials on the real line via a three-term recurrence. In particular, for the Chebyshev polynomials of the second kind $U_k(x)$, the recurrence is $U_0(x)=1$, $U_1(x)=2x$, and $U_k(x)=2x\\,U_{k-1}(x)-U_{k-2}(x)$ for $k \\ge 2$.\n\nConstruct the $n \\times n$ real symmetric tridiagonal matrix $A_n$ with $0$ on the main diagonal and $1$ on the first sub- and super-diagonals, that is,\n$$\nA_n = \\begin{bmatrix}\n0  1  0  \\cdots  0 \\\\\n1  0  1  \\ddots  \\vdots \\\\\n0  1  0  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  1 \\\\\n0  \\cdots  0  1  0\n\\end{bmatrix}.\n$$\nFrom the determinant expansion for tridiagonal matrices and no other special-purpose formulas, derive that the characteristic polynomial of $A_n$ satisfies a three-term recurrence that matches that of the Chebyshev polynomials of the second kind evaluated at a scaled argument. Conclude the exact identity of the characteristic polynomial of $A_n$ with a suitable scaling of $U_n(x)$.\n\nImplement the Lanczos method for a real symmetric matrix $A$ and an initial vector $v_0 \\in \\mathbb{R}^n$ with $v_0 \\ne 0$. The algorithm must generate orthonormal vectors $v_1,\\dots,v_m$ spanning the Krylov subspace of order $m$, and the tridiagonal matrix $T_m$ with diagonal entries $\\alpha_j$ and sub/super-diagonal entries $\\beta_j$ produced by the three-term recurrence. Use only standard linear algebra operations and ensure numerical robustness for breakdown checks.\n\nYour program must do the following, for each specified test case:\n\n- Build $A_n$ as above.\n- Run $m$ iterations of Lanczos starting from the given $v_0$, producing $T_m$ with entries $\\{\\alpha_j\\}_{j=1}^m$ and $\\{\\beta_j\\}_{j=1}^{m-1}$.\n- For selected comparisons, compute the exact eigenvalues of $A_n$ using the identity between the characteristic polynomial of $A_n$ and the Chebyshev polynomials of the second kind. Use these exact eigenvalues to define quantitative error measures for the Ritz values (eigenvalues of $T_m$).\n- Where required, evaluate the characteristic polynomial of a tridiagonal matrix $T_m$ at real arguments $\\lambda$ using the determinant three-term recurrence $D_0(\\lambda)=1$, $D_1(\\lambda)=\\lambda-\\alpha_1$, and $D_k(\\lambda)=(\\lambda-\\alpha_k) D_{k-1}(\\lambda)-\\beta_{k-1}^2 D_{k-2}(\\lambda)$ for $k \\ge 2$. Compare this with the evaluation of $U_m(\\lambda/2)$ computed via its defining three-term recurrence.\n\nUse the following test suite; all vectors are in $\\mathbb{R}^n$:\n- Test Case $1$: $n=6$, $m=6$, $v_0 = e_1$ where $e_1$ is the first standard basis vector in $\\mathbb{R}^n$. Define the result as the maximum absolute difference between the Ritz values of $T_m$ and the exact eigenvalues of $A_n$.\n- Test Case $2$: $n=10$, $m=5$, $v_0 = e_1$. Define two quantities: (i) the maximum deviation of the computed $\\alpha_j$ from $0$ and of the computed $\\beta_j$ from $1$, aggregated as the maximum of these two maxima; and (ii) the maximum over a uniform grid of $17$ points in the interval $[-2.5,2.5]$ of the absolute difference between $\\det(\\lambda I - T_m)$ and $U_m(\\lambda/2)$, with both evaluated by three-term recurrences. The result for this test case is the maximum of these two quantities.\n- Test Case $3$: $n=20$, $m=12$, $v_0$ is a deterministic pseudo-random vector with entries drawn from the standard normal distribution with a fixed seed $314159$, then normalized to unit Euclidean norm. Define the result as the absolute difference between the largest Ritz value of $T_m$ and the largest exact eigenvalue of $A_n$.\n- Test Case $4$: $n=8$, $m=1$, $v_0=e_1$. Define the result as the maximum absolute difference, over the set of points $\\{-3,-1,0,1,3\\}$, between $\\det(\\lambda I - T_1)$ and $U_1(\\lambda/2)$.\n\nAll answers are pure numbers; no physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers enclosed in square brackets, in the order of the four test cases, for example, \"[$r_1,r_2,r_3,r_4$]\". Each $r_j$ should be a floating-point number. You may round results to within an absolute tolerance of $10^{-12}$ for readability. No other output should be printed.",
            "solution": "The problem as stated is scientifically sound, well-posed, and objective. It is a standard exercise in numerical linear algebra, connecting the theory of orthogonal polynomials with the behavior of the Lanczos algorithm. All parameters and objectives are clearly defined. The problem is valid. We proceed with the solution.\n\nThe solution is partitioned into two components: first, the analytical derivation of the identity between the characteristic polynomial of the matrix $A_n$ and the Chebyshev polynomials of the second kind; second, the description of the numerical implementation of the Lanczos method and the evaluation of the specified test cases.\n\n**1. Derivation of the Characteristic Polynomial**\n\nLet $A_n$ be the $n \\times n$ real symmetric tridiagonal matrix defined by $(A_n)_{i,i} = 0$, $(A_n)_{i,i+1} = (A_n)_{i+1,i} = 1$, and all other entries being $0$. The characteristic polynomial of $A_n$ is $p_n(\\lambda) = \\det(\\lambda I_n - A_n)$. The matrix $\\lambda I_n - A_n$ is:\n$$\n\\lambda I_n - A_n = \\begin{bmatrix}\n\\lambda  -1  0  \\cdots  0 \\\\\n-1  \\lambda  -1  \\ddots  \\vdots \\\\\n0  -1  \\lambda  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1 \\\\\n0  \\cdots  0  -1  \\lambda\n\\end{bmatrix}\n$$\nLet $p_k(\\lambda)$ denote the determinant of the leading principal $k \\times k$ submatrix of $\\lambda I_n - A_n$, which is $\\lambda I_k - A_k$.\nFor $k \\ge 2$, we can compute $p_k(\\lambda)$ using the Laplace (cofactor) expansion along the last row. This yields:\n$$\np_k(\\lambda) = \\lambda \\cdot p_{k-1}(\\lambda) - (-1) \\cdot \\det(M_{k,k-1})\n$$\nwhere $M_{k,k-1}$ is the submatrix obtained by removing row $k$ and column $k-1$. The matrix $M_{k,k-1}$ is an upper triangular matrix with diagonal entries $-1, -1, ..., -1$ ($k-2$ times) and $p_{k-2}(\\lambda)$ in an off-diagonal block structure. More simply, by expanding along the last column of $\\lambda I_k - A_k$:\n$$\np_k(\\lambda) = \\lambda \\cdot p_{k-1}(\\lambda) - (-1) \\cdot \\det\\begin{pmatrix} \\lambda I_{k-2}-A_{k-2}  \\mathbf{0} \\\\ \\mathbf{c}^T  -1 \\end{pmatrix}\n$$\nThe determinant of this block lower triangular matrix is $-p_{k-2}(\\lambda)$. A more direct expansion on the last column gives the terms coming from entries $(k,k)$ and $(k-1,k)$.\nThe entry at $(k,k)$ is $\\lambda$, giving the term $\\lambda \\cdot p_{k-1}(\\lambda)$.\nThe entry at $(k-1,k)$ is $-1$. Its cofactor is $(-1)^{(k-1)+k}$ times the determinant of the matrix with row $k-1$ and column $k$ removed. This matrix is block triangular with a determinant of $-1 \\cdot p_{k-2}(\\lambda)$.\nThus, the contribution is $(-1) \\cdot (-1)^{2k-1} \\cdot (-p_{k-2}(\\lambda)) = -p_{k-2}(\\lambda)$.\nThe full recurrence relation is:\n$$\np_k(\\lambda) = \\lambda p_{k-1}(\\lambda) - p_{k-2}(\\lambda) \\quad \\text{for } k \\ge 2.\n$$\nWe establish the base cases:\n$p_1(\\lambda) = \\det([\\lambda]) = \\lambda$.\n$p_2(\\lambda) = \\det \\begin{pmatrix} \\lambda  -1 \\\\ -1  \\lambda \\end{pmatrix} = \\lambda^2 - 1$.\nTo initiate the recurrence, we require a value for $p_0(\\lambda)$. Using the recurrence for $k=2$: $p_2(\\lambda) = \\lambda p_1(\\lambda) - p_0(\\lambda)$, which implies $\\lambda^2 - 1 = \\lambda(\\lambda) - p_0(\\lambda)$, so we must define $p_0(\\lambda) = 1$.\n\nNow, consider the Chebyshev polynomials of the second kind, $U_k(x)$, defined by the recurrence:\n$U_0(x)=1$, $U_1(x)=2x$, and $U_k(x) = 2x U_{k-1}(x) - U_{k-2}(x)$ for $k \\ge 2$.\nLet us make the substitution $\\lambda = 2x$, so $x = \\lambda/2$. The recurrence for $p_k(\\lambda)$ becomes $p_k(2x) = 2x p_{k-1}(2x) - p_{k-2}(2x)$.\nThis is identical in form to the recurrence for $U_k(x)$. We check if the initial conditions for the sequence $\\{p_k(2x)\\}$ match those for $\\{U_k(x)\\}$.\n- For $k=0$: $p_0(2x) = 1$, which matches $U_0(x)=1$.\n- For $k=1$: $p_1(2x) = 2x$, which matches $U_1(x)=2x$.\nSince the recurrence relations and the first two terms are identical, we conclude by induction that $p_k(2x) = U_k(x)$ for all $k \\ge 0$. Substituting $x=\\lambda/2$ back, we have established the identity for the characteristic polynomial of $A_k$:\n$$\np_k(\\lambda) = U_k(\\lambda/2).\n$$\nThe eigenvalues of $A_n$ are the roots of $p_n(\\lambda) = 0$, which are the roots of $U_n(\\lambda/2)=0$. The roots of $U_n(x)$ are $x_j = \\cos\\left(\\frac{j\\pi}{n+1}\\right)$ for $j=1, \\dots, n$. Therefore, the exact eigenvalues of $A_n$ are $\\lambda_j = 2x_j = 2\\cos\\left(\\frac{j\\pi}{n+1}\\right)$ for $j=1, \\dots, n$.\n\n**2. Numerical Implementation and Analysis**\n\nThe implementation consists of four main components:\n1.  A function to construct the matrix $A_n$ for a given dimension $n$.\n2.  An implementation of the Lanczos algorithm for a real symmetric matrix $A$, a starting vector $v_0$, and a number of iterations $m$. The algorithm generates the coefficients $\\{\\alpha_j\\}_{j=1}^m$ and $\\{\\beta_j\\}_{j=1}^{m-1}$ of the tridiagonal matrix $T_m$. The process starts by normalizing the initial vector $v_0$ to obtain the first Lanczos vector $q_1$.\n3.  A function to evaluate the characteristic polynomial $\\det(\\lambda I_k - T_k)$ of a given tridiagonal matrix $T_k$ at a point $\\lambda$, using the three-term recurrence $D_k(\\lambda)=(\\lambda-\\alpha_k) D_{k-1}(\\lambda)-\\beta_{k-1}^2 D_{k-2}(\\lambda)$, with initial conditions $D_0(\\lambda)=1$ and $D_1(\\lambda)=\\lambda-\\alpha_1$.\n4.  A function to evaluate the Chebyshev polynomial $U_k(x)$ at a point $x$ using its defining recurrence.\n\nThese components are used to solve the four test cases specified.\n\n- For Test Case $1$ ($n=6, m=6, v_0=e_1$), the Lanczos algorithm on $(A_n, e_1)$ for $n$ steps theoretically yields $T_n=A_n$. Thus, the Ritz values (eigenvalues of $T_6$) should match the exact eigenvalues of $A_6$ to within machine precision.\n- For Test Case $2$ ($n=10, m=5, v_0=e_1$), the algorithm generates the leading principal $5 \\times 5$ submatrix of $A_{10}$, which is $A_5$. Thus, the coefficients $\\alpha_j$ must be $0$ and $\\beta_j$ must be $1$. The characteristic polynomial of $T_5 = A_5$ is exactly $U_5(\\lambda/2)$. The specified error quantities should both be near zero.\n- For Test Case $3$ ($n=20, m=12, v_0$ is a random vector), the Lanczos algorithm provides an approximation. The largest Ritz value of $T_{12}$ is expected to be a very good approximation to the largest eigenvalue of $A_{20}$, as the method converges fastest for extremal eigenvalues.\n- For Test Case $4$ ($n=8, m=1, v_0=e_1$), a single step of the Lanczos method is performed. The resulting matrix is $T_1 = [\\alpha_1]$. The calculation gives $\\alpha_1 = v_1^T A v_1 = e_1^T A_8 e_1 = 0$. So $T_1=[0]$ and its characteristic polynomial is $\\det(\\lambda I_1 - T_1) = \\lambda$. This is identical to $U_1(\\lambda/2) = 2(\\lambda/2) = \\lambda$. The difference is analytically zero.\n\nThe final code implements these procedures and computes the required results for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the Lanczos method for a specific real symmetric matrix\n    whose characteristic polynomial corresponds to Chebyshev polynomials.\n    \"\"\"\n\n    def build_A(n):\n        \"\"\"Builds the n x n test matrix A_n.\"\"\"\n        A = np.zeros((n, n), dtype=float)\n        A += np.diag(np.ones(n - 1), k=1)\n        A += np.diag(np.ones(n - 1), k=-1)\n        return A\n\n    def lanczos_gvl(A, v_start, m):\n        \"\"\"\n        Performs m iterations of the Lanczos algorithm.\n        Based on algorithm 10.1.1 from Golub  Van Loan, \"Matrix Computations\" 4th ed.\n        \n        Args:\n            A (np.ndarray): The symmetric matrix.\n            v_start (np.ndarray): The starting vector.\n            m (int): The number of iterations.\n            \n        Returns:\n            tuple[np.ndarray, np.ndarray]: (alphas, betas) for the tridiagonal matrix T_m.\n        \"\"\"\n        n = A.shape[0]\n        q_prev = np.zeros(n, dtype=float)\n        r = v_start.copy()\n        beta = np.linalg.norm(r)\n\n        alphas = np.zeros(m, dtype=float)\n        betas = np.zeros(m - 1, dtype=float)\n\n        for j in range(m):\n            if beta  1e-15:  # Breakdown check\n                # For this problem, breakdown is not expected.\n                # If it occurs, we would return truncated T matrix coeffs.\n                # Here we assume it runs for m steps successfully.\n                alphas = alphas[:j]\n                betas = betas[:j - 1] if j  0 else np.array([])\n                break\n\n            q = r / beta\n            u = A @ q\n            alpha = np.dot(q, u)\n            alphas[j] = alpha\n\n            r = u - alpha * q - beta * q_prev\n\n            if j  m - 1:\n                beta_next = np.linalg.norm(r)\n                betas[j] = beta_next\n                beta = beta_next\n            \n            q_prev = q\n\n        return alphas, betas\n\n    def get_exact_eigenvalues(n):\n        \"\"\"Computes the exact eigenvalues of the matrix A_n.\"\"\"\n        j = np.arange(1, n + 1, dtype=float)\n        eigvals = 2 * np.cos(j * np.pi / (n + 1))\n        return np.sort(eigvals)\n\n    def eval_det_poly(alphas, betas, lam):\n        \"\"\"\n        Evaluates the characteristic polynomial det(lambda*I - T) of a tridiagonal matrix T\n        using the three-term recurrence for determinants.\n        \"\"\"\n        m = len(alphas)\n        if m == 0:\n            return 1.0\n        \n        d_prev = 1.0  # D_0(lambda)\n        d_curr = lam - alphas[0]  # D_1(lambda)\n\n        for k in range(1, m):\n            beta_sq = betas[k-1]**2\n            d_next = (lam - alphas[k]) * d_curr - beta_sq * d_prev\n            d_prev = d_curr\n            d_curr = d_next\n            \n        return d_curr\n\n    def eval_U(k, x):\n        \"\"\"\n        Evaluates the Chebyshev polynomial of the second kind U_k(x) via its recurrence relation.\n        \"\"\"\n        if k == 0:\n            return 1.0\n        if k == 1:\n            return 2.0 * x\n        \n        u_prev = 1.0  # U_0\n        u_curr = 2.0 * x  # U_1\n        \n        for _ in range(2, k + 1):\n            u_next = 2.0 * x * u_curr - u_prev\n            u_prev = u_curr\n            u_curr = u_next\n            \n        return u_curr\n\n    test_cases = [\n        {'n': 6, 'm': 6, 'v0_type': 'e1'},\n        {'n': 10, 'm': 5, 'v0_type': 'e1'},\n        {'n': 20, 'm': 12, 'v0_type': 'random', 'seed': 314159},\n        {'n': 8, 'm': 1, 'v0_type': 'e1'}\n    ]\n    results = []\n\n    # Test Case 1\n    params = test_cases[0]\n    n, m = params['n'], params['m']\n    A = build_A(n)\n    v0 = np.zeros(n)\n    v0[0] = 1.0\n    alphas, betas = lanczos_gvl(A, v0, m)\n    T_m = np.diag(alphas) + np.diag(betas, k=1) + np.diag(betas, k=-1)\n    ritz_values = np.linalg.eigvalsh(T_m)\n    exact_eigvals = get_exact_eigenvalues(n)\n    res1 = np.max(np.abs(ritz_values - exact_eigvals))\n    results.append(res1)\n\n    # Test Case 2\n    params = test_cases[1]\n    n, m = params['n'], params['m']\n    A = build_A(n)\n    v0 = np.zeros(n)\n    v0[0] = 1.0\n    alphas, betas = lanczos_gvl(A, v0, m)\n    \n    max_dev_alpha = np.max(np.abs(alphas)) if len(alphas)  0 else 0.0\n    max_dev_beta = np.max(np.abs(betas - 1.0)) if len(betas)  0 else 0.0\n    q1 = max(max_dev_alpha, max_dev_beta)\n    \n    grid = np.linspace(-2.5, 2.5, 17)\n    max_diff_poly = 0.0\n    for lam in grid:\n        det_val = eval_det_poly(alphas, betas, lam)\n        cheby_val = eval_U(m, lam / 2.0)\n        max_diff_poly = max(max_diff_poly, np.abs(det_val - cheby_val))\n    \n    res2 = max(q1, max_diff_poly)\n    results.append(res2)\n\n    # Test Case 3\n    params = test_cases[2]\n    n, m, seed = params['n'], params['m'], params['seed']\n    A = build_A(n)\n    rng = np.random.default_rng(seed)\n    v0_unnormalized = rng.standard_normal(n)\n    v0 = v0_unnormalized / np.linalg.norm(v0_unnormalized)\n    \n    alphas, betas = lanczos_gvl(A, v0, m)\n    T_m = np.diag(alphas) + np.diag(betas, k=1) + np.diag(betas, k=-1)\n    ritz_values = np.linalg.eigvalsh(T_m)\n    exact_eigvals = get_exact_eigenvalues(n)\n    \n    max_ritz = np.max(ritz_values)\n    max_exact = np.max(exact_eigvals)\n    res3 = np.abs(max_ritz - max_exact)\n    results.append(res3)\n\n    # Test Case 4\n    params = test_cases[3]\n    n, m = params['n'], params['m']\n    A = build_A(n)\n    v0 = np.zeros(n)\n    v0[0] = 1.0\n    alphas, betas = lanczos_gvl(A, v0, m)\n    \n    test_points = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])\n    max_diff_poly = 0.0\n    for lam in test_points:\n        det_val = eval_det_poly(alphas, betas, lam)\n        cheby_val = eval_U(m, lam / 2.0)\n        max_diff_poly = max(max_diff_poly, np.abs(det_val - cheby_val))\n    \n    res4 = max_diff_poly\n    results.append(res4)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A key reason for the Lanczos method's widespread use is its remarkably fast convergence to extremal eigenvalues. But how fast is 'fast'? The Kaniel-Paige-Saad (KPS) convergence theory provides a rigorous answer by bounding the error of the approximate eigenvalues, known as Ritz values. In this practice, you will numerically compute these bounds and compare them against the actual error, gaining a quantitative understanding of why the Lanczos method is so effective for finding the largest or smallest eigenvalues of a system .",
            "id": "2406031",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a real symmetric matrix with eigenvalues $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{n}$ and an orthonormal eigenbasis $\\{u_{i}\\}_{i=1}^{n}$. For a nonzero starting vector $v \\in \\mathbb{R}^{n}$, define the Krylov subspace of order $m \\in \\mathbb{N}$ by\n$$\n\\mathcal{K}_{m}(A,v) = \\operatorname{span}\\{v, Av, A^{2}v, \\dots, A^{m-1}v\\}.\n$$\nThe largest Ritz value of $A$ in $\\mathcal{K}_{m}(A,v)$ is\n$$\n\\theta_{1}^{(m)} = \\max_{x \\in \\mathcal{K}_{m}(A,v),\\, x \\ne 0} \\frac{x^{\\top} A x}{x^{\\top} x}.\n$$\nWrite the unit-norm starting vector as $v / \\|v\\|_{2} = \\sum_{i=1}^{n} \\alpha_{i} u_{i}$, and define the angle $\\phi \\in [0, \\pi/2)$ with $\\tan \\phi = \\sqrt{\\sum_{i=2}^{n} \\alpha_{i}^{2}} / |\\alpha_{1}|$. Define\n$$\nc = \\frac{\\lambda_{2} + \\lambda_{n}}{2}, \\quad d = \\frac{\\lambda_{2} - \\lambda_{n}}{2}, \\quad t = \\frac{\\lambda_{1} - c}{d},\n$$\nand let $T_{k}(x)$ be the Chebyshev polynomial of the first kind of degree $k$, defined for $x \\ge 1$ by $T_{k}(x) = \\cosh(k \\operatorname{arccosh}(x))$. The Kaniel–Paige–Saad (KPS) bound for the largest Ritz value error is\n$$\n0 \\le \\lambda_{1} - \\theta_{1}^{(m)} \\le \\frac{\\left(\\lambda_{1} - \\lambda_{n}\\right) \\tan^{2}\\phi}{\\left[T_{m-1}(t)\\right]^{2}}.\n$$\n\nTask. For each test case below, and for each listed subspace order $m$, numerically evaluate the largest Ritz value $\\theta_{1}^{(m)}$, the error $e_{m} = \\lambda_{1} - \\theta_{1}^{(m)}$, and the KPS bound\n$$\nB_{m} = \\frac{\\left(\\lambda_{1} - \\lambda_{n}\\right) \\tan^{2}\\phi}{\\left[T_{m-1}(t)\\right]^{2}}.\n$$\nAssess whether $e_{m} \\le B_{m} + \\tau$ holds with absolute tolerance $\\tau = 10^{-10}$ for all $m$ in the set specified for that test. For each test case, report a single boolean indicating whether the inequality holds for all its $m$ values. Normalize each starting vector $v$ to unit Euclidean norm before use. Angles are measured in radians.\n\nTest suite. Use the following matrices $A$, starting vectors $v$, and sets of subspace dimensions $M$:\n\n1. Case A: $n = 4$, $A = \\operatorname{diag}([5, 3, 1, -2])$, $v = [0.2, 0.5, 0.3, 0.4]$, $M = \\{1, 2, 3\\}$.\n2. Case B: $n = 6$, $A = \\operatorname{diag}([2.0, 1.999, 1.5, 0.5, 0.0, -1.0])$, $v = [1, 1, 1, 1, 1, 1]$, $M = \\{1, 2, 3, 4\\}$.\n3. Case C: $n = 5$, $A$ is tridiagonal with entries $A_{ii} = 2$ for $i \\in \\{1, 2, 3, 4, 5\\}$, $A_{i,i+1} = A_{i+1,i} = -1$ for $i \\in \\{1, 2, 3, 4\\}$, and $A_{ij} = 0$ otherwise, $v = [1, 2, 3, 4, 5]$, $M = \\{1, 2, 3, 4\\}$.\n4. Case D: $n = 5$, $A = \\operatorname{diag}([10.0, 1.0, 0.5, 0.2, 0.1])$, $v = [10^{-3}, 1.0, 0.0, 0.0, 0.0]$, $M = \\{1, 2, 3\\}$.\n\nRequired final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true_case_A,true_case_B,true_case_C,true_case_D]\"). Each entry must be the boolean truth value for the corresponding case, in order A, B, C, D, using the programming language’s canonical boolean literals.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the established theory of numerical linear algebra, specifically the Lanczos method and its convergence properties for symmetric eigenproblems. The problem is well-posed, providing all necessary definitions, data, and constraints to arrive at a unique, verifiable solution. The language is objective and mathematically precise. Therefore, a solution will be provided.\n\nThe core of the problem is to numerically verify the Kaniel–Paige–Saad (KPS) error bound for the largest Ritz value computed via the Lanczos algorithm. This requires implementing the Lanczos procedure to find the Ritz values and then computing the theoretical bound for comparison.\n\nFirst, let us formalize the procedure for finding the largest Ritz value, $\\theta_{1}^{(m)}$. The Lanczos algorithm is an iterative method that generates an orthonormal basis for the Krylov subspace $\\mathcal{K}_{m}(A,v) = \\operatorname{span}\\{v, Av, \\dots, A^{m-1}v\\}$. Let $v_1$ be the initial vector $v$ normalized to unit Euclidean norm, i.e., $v_1 = v/\\|v\\|_2$. The algorithm generates a sequence of orthonormal vectors $\\{q_j\\}_{j=1}^m$ that form a basis for $\\mathcal{K}_m(A, v_1)$. A key property of this process is that the projection of the matrix $A$ onto this subspace, represented in the basis $\\{q_j\\}$, is an $m \\times m$ real symmetric tridiagonal matrix $T_m$.\n$$\nT_m = Q_m^\\top A Q_m = \\begin{pmatrix}\n\\alpha_1  \\beta_1   \\\\\n\\beta_1  \\alpha_2  \\ddots  \\\\\n \\ddots  \\ddots  \\beta_{m-1} \\\\\n  \\beta_{m-1}  \\alpha_m\n\\end{pmatrix}\n$$\nwhere $Q_m = [q_1, q_2, \\dots, q_m]$. The diagonal entries $\\alpha_j$ and off-diagonal entries $\\beta_j$ are generated by the three-term recurrence relation at the heart of the Lanczos iteration:\n$$\n\\beta_j q_{j+1} = A q_j - \\alpha_j q_j - \\beta_{j-1} q_{j-1}\n$$\nwith $\\alpha_j = q_j^\\top A q_j$ and $\\beta_j = \\|A q_j - \\alpha_j q_j - \\beta_{j-1} q_{j-1}\\|_2$. We initialize with $q_1 = v_1$, $\\beta_0=0$, and $q_0=0$.\n\nThe eigenvalues of this tridiagonal matrix $T_m$ are the Ritz values of $A$ in the subspace $\\mathcal{K}_m(A,v)$. The largest Ritz value, $\\theta_{1}^{(m)}$, is simply the largest eigenvalue of $T_m$. We must compute this value for each specified order $m$.\n\nSecond, we evaluate the KPS bound, $B_m$. The formula is given as:\n$$\nB_m = \\frac{(\\lambda_{1} - \\lambda_{n}) \\tan^{2}\\phi}{[T_{m-1}(t)]^{2}}\n$$\nTo compute $B_m$, we need several quantities derived from the eigensystem of $A$:\n1.  The eigenvalues of $A$, sorted as $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n$. We will compute the full eigensystem of $A$ to obtain all $\\lambda_i$ and the corresponding orthonormal eigenvectors $u_i$. For a given matrix $A$, this is a standard numerical task.\n2.  The angle $\\phi$, defined via $\\tan \\phi$. The initial normalized vector $v_1 = v/\\|v\\|_2$ can be expressed in the eigenbasis of $A$ as $v_1 = \\sum_{i=1}^{n} \\alpha_{i} u_{i}$. The coefficient $\\alpha_1$ is the projection of $v_1$ onto the eigenvector $u_1$ corresponding to the largest eigenvalue $\\lambda_1$, i.e., $\\alpha_1 = u_1^\\top v_1$. Since $\\{u_i\\}$ is an orthonormal basis, $\\sum_{i=1}^n \\alpha_i^2 = \\|v_1\\|_2^2 = 1$. The term $\\sum_{i=2}^{n} \\alpha_{i}^{2}$ is therefore equal to $1 - \\alpha_1^2$. The expression for $\\tan^2 \\phi$ simplifies to:\n    $$\n    \\tan^2\\phi = \\frac{\\sum_{i=2}^{n} \\alpha_{i}^{2}}{\\alpha_{1}^{2}} = \\frac{1 - \\alpha_1^2}{\\alpha_1^2}\n    $$\n    This is valid as the problem states $\\phi \\in [0, \\pi/2)$, which implies $\\alpha_1 \\ne 0$.\n3.  The parameters $c = (\\lambda_2 + \\lambda_n)/2$ and $d = (\\lambda_2 - \\lambda_n)/2$, which define a conditioning parameter $t = (\\lambda_1 - c)/d$. As $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_n$, it can be shown that $t = 1 + 2(\\lambda_1 - \\lambda_2)/(\\lambda_2 - \\lambda_n) \\ge 1$ (provided $\\lambda_2  \\lambda_n$, which holds for all test cases).\n4.  The Chebyshev polynomial of the first kind of degree $k=m-1$, $T_{m-1}(t)$. For $t \\ge 1$, this is given by $T_k(t) = \\cosh(k \\operatorname{arccosh}(t))$.\n\nThe overall procedure for each test case $(A, v, M)$ is as follows:\n\n1.  Given $A$, compute its full eigensystem to find eigenvalues $\\lambda_i$ and eigenvectors $u_i$. Sort them such that $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n$ and the columns of the eigenvector matrix correspond to this order.\n2.  Normalize the starting vector: $v_1 = v / \\|v\\|_2$.\n3.  Calculate the parameters for the KPS bound:\n    -   $\\alpha_1 = u_1^\\top v_1$.\n    -   $\\tan^2\\phi = (1 - \\alpha_1^2) / \\alpha_1^2$.\n    -   $c = (\\lambda_2 + \\lambda_n)/2$.\n    -   $d = (\\lambda_2 - \\lambda_n)/2$.\n    -   $t = (\\lambda_1 - c)/d$.\n4.  Initialize a boolean flag, `case_holds = true`.\n5.  Iterate through each subspace order $m \\in M$:\n    a. Perform $m$ steps of the Lanczos algorithm starting with $v_1$ to generate the tridiagonal matrix $T_m$.\n    b. Compute the eigenvalues of $T_m$. The largest of these is the Ritz value $\\theta_1^{(m)}$.\n    c. Calculate the true error $e_m = \\lambda_1 - \\theta_1^{(m)}$.\n    d. Calculate the Chebyshev polynomial value $T_{m-1}(t) = \\cosh((m-1)\\operatorname{arccosh}(t))$.\n    e. Compute the KPS bound $B_m = (\\lambda_1 - \\lambda_n) \\tan^2\\phi / [T_{m-1}(t)]^2$.\n    f. Check if the condition $e_m \\le B_m + \\tau$ holds for the tolerance $\\tau=10^{-10}$. If it fails, set `case_holds = false` and terminate the iteration for this test case.\n6.  The final result for the test case is the value of `case_holds`. This entire procedure is then repeated for all test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef build_tridiag_case_c():\n    \"\"\"Builds the specific tridiagonal matrix for Case C.\"\"\"\n    n = 5\n    A = np.zeros((n, n))\n    np.fill_diagonal(A, 2.0)\n    off_diag = np.full(n - 1, -1.0)\n    A += np.diag(off_diag, k=1)\n    A += np.diag(off_diag, k=-1)\n    return A\n\ndef chebyshev_T(k, x):\n    \"\"\"\n    Computes the Chebyshev polynomial of the first kind T_k(x) for x = 1.\n    \"\"\"\n    if k == 0:\n        return 1.0\n    # Guard against float-point error making x slightly less than 1\n    if x  1.0:\n        x = 1.0\n    return np.cosh(k * np.arccosh(x))\n\ndef lanczos_iteration(A, v_start, m):\n    \"\"\"\n    Performs m steps of the Lanczos iteration.\n    Returns the diagonal (alpha) and off-diagonal (beta) elements of the\n    tridiagonal matrix T_m.\n    \"\"\"\n    n = A.shape[0]\n    q = np.zeros((n, m + 1))\n    alphas = np.zeros(m)\n    betas = np.zeros(m) # betas[0] is beta_1, etc.\n\n    q[:, 0] = v_start / np.linalg.norm(v_start)\n    \n    # Lanczos 3-term recurrence\n    w_prime = A @ q[:, 0]\n    alphas[0] = q[:, 0].T @ w_prime\n    w = w_prime - alphas[0] * q[:, 0]\n    \n    for j in range(1, m):\n        betas[j-1] = np.linalg.norm(w)\n        if betas[j-1]  1e-14: # Lucky breakdown\n             # The rest of T_m will be zero, we can stop early\n             # and return the smaller matrix's components\n             return alphas[:j], betas[:j-1]\n        \n        q[:, j] = w / betas[j-1]\n        \n        w_prime = A @ q[:, j]\n        alphas[j] = q[:, j].T @ w_prime\n        w = w_prime - alphas[j] * q[:, j] - betas[j-1] * q[:, j-1]\n\n    return alphas, betas[:-1]\n\n\ndef process_case(A, v, M, tau):\n    \"\"\"\n    Processes a single test case to verify the KPS bound.\n    \"\"\"\n    n = A.shape[0]\n    \n    # 1. Eigendecomposition of A\n    evals, evecs = np.linalg.eigh(A)\n    sort_indices = np.argsort(evals)[::-1]\n    evals = evals[sort_indices]\n    evecs = evecs[:, sort_indices]\n    \n    lambda_1 = evals[0]\n    lambda_2 = evals[1]\n    lambda_n = evals[n-1]\n    u_1 = evecs[:, 0]\n\n    # 2. Normalize starting vector\n    v_norm = v / np.linalg.norm(v)\n\n    # 3. Calculate KPS bound parameters\n    alpha_1 = np.abs(u_1.T @ v_norm)\n    if alpha_1  1e-15: # Starting vector is orthogonal to u_1\n        # The bound is infinite, so the inequality holds trivially.\n        return True\n        \n    tan_sq_phi = (1 - alpha_1**2) / alpha_1**2\n    \n    if abs(lambda_2 - lambda_n)  1e-15:\n        # All eigenvalues except lambda_1 are clustered. The bound is not well-defined/infinite.\n        # This implies any finite error is below an infinite bound.\n        return True\n\n    c = (lambda_2 + lambda_n) / 2.0\n    d = (lambda_2 - lambda_n) / 2.0\n    t = (lambda_1 - c) / d\n\n    # 4. Iterate over subspace dimensions m\n    for m in M:\n        # 5a. Lanczos iteration\n        alphas, betas = lanczos_iteration(A, v_norm, m)\n\n        # 5b. Compute largest Ritz value\n        if m == 1:\n            ritz_vals = np.array([alphas[0]])\n        else:\n            ritz_vals = eigh_tridiagonal(alphas, betas, eigvals_only=True)\n        theta_1_m = np.max(ritz_vals)\n\n        # 5c. Compute error\n        e_m = lambda_1 - theta_1_m\n\n        # 5d/e. Compute KPS bound\n        T_m_minus_1_val = chebyshev_T(m - 1, t)\n        if abs(T_m_minus_1_val)  1e-15:\n             # Bound is infinite, inequality holds\n             B_m = np.inf\n        else:\n             B_m = ((lambda_1 - lambda_n) * tan_sq_phi) / (T_m_minus_1_val**2)\n\n        # 5f. Check inequality\n        if not (e_m = B_m + tau):\n            return False\n\n    return True\n\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    tau = 1e-10\n\n    test_cases = [\n        {\n            \"A\": np.diag(np.array([5, 3, 1, -2], dtype=float)),\n            \"v\": np.array([0.2, 0.5, 0.3, 0.4], dtype=float),\n            \"M\": {1, 2, 3},\n        },\n        {\n            \"A\": np.diag(np.array([2.0, 1.999, 1.5, 0.5, 0.0, -1.0], dtype=float)),\n            \"v\": np.ones(6, dtype=float),\n            \"M\": {1, 2, 3, 4},\n        },\n        {\n            \"A\": build_tridiag_case_c(),\n            \"v\": np.array([1, 2, 3, 4, 5], dtype=float),\n            \"M\": {1, 2, 3, 4},\n        },\n        {\n            \"A\": np.diag(np.array([10.0, 1.0, 0.5, 0.2, 0.1], dtype=float)),\n            \"v\": np.array([1e-3, 1.0, 0.0, 0.0, 0.0], dtype=float),\n            \"M\": {1, 2, 3},\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case[\"A\"], case[\"v\"], case[\"M\"], tau)\n        results.append(str(result).lower())\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Theoretical error bounds are crucial for analysis, but practical implementations require a computable and robust rule for when to stop iterating. This exercise transitions from theory to practice by guiding you to implement a standard termination criterion based on the residual norm of a Ritz pair. You will use an elegant and highly efficient formula to calculate this norm, a technique that is fundamental to building production-quality iterative eigensolvers .",
            "id": "2406056",
            "problem": "Given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a nonzero starting vector $v_{1} \\in \\mathbb{R}^{n}$ with $\\|v_{1}\\|_{2} = 1$, consider the $k$-dimensional Krylov subspace $\\mathcal{K}_{k}(A, v_{1}) = \\mathrm{span}\\{v_{1}, A v_{1}, \\ldots, A^{k-1} v_{1}\\}$. Let $Q_{k} \\in \\mathbb{R}^{n \\times k}$ have orthonormal columns that form a basis of $\\mathcal{K}_{k}(A, v_{1})$, and let $T_{k} = Q_{k}^{\\top} A Q_{k} \\in \\mathbb{R}^{k \\times k}$ be the Rayleigh–Ritz projection of $A$ onto $\\mathcal{K}_{k}(A, v_{1})$. Denote by $(\\theta_{i}^{(k)}, u_{i}^{(k)})$ the eigenpairs of $T_{k}$ with $\\|u_{i}^{(k)}\\|_{2} = 1$, and define the corresponding Ritz vectors in the original space by $y_{i}^{(k)} = Q_{k} u_{i}^{(k)}$ with $\\|y_{i}^{(k)}\\|_{2} = 1$. For each Ritz pair $(\\theta_{i}^{(k)}, y_{i}^{(k)})$, define the residual $r_{i}^{(k)} = A y_{i}^{(k)} - \\theta_{i}^{(k)} y_{i}^{(k)}$ and its norm $\\|r_{i}^{(k)}\\|_{2}$.\n\nDevelop a robust termination criterion that accepts a Ritz pair $(\\theta_{i}^{(k)}, y_{i}^{(k)})$ when the inequality\n$$\n\\|r_{i}^{(k)}\\|_{2} \\leq \\max\\big(\\mathrm{atol}, \\ \\mathrm{rtol} \\cdot \\max(1, |\\theta_{i}^{(k)}|)\\big)\\, \\|A\\|_{2}\n$$\nholds, where $\\mathrm{rtol}  0$ and $\\mathrm{atol} \\ge 0$ are user-specified tolerances and $\\|A\\|_{2}$ denotes the spectral norm (the largest singular value, which for symmetric $A$ equals the largest absolute eigenvalue). For a given target specification, let $\\mathcal{I}_{k}$ denote the index set of the $r$ Ritz values chosen according to the target rule:\n- If the target is “smallest,” then $\\mathcal{I}_{k}$ comprises the indices of the $r$ algebraically smallest Ritz values among $\\{\\theta_{i}^{(k)}\\}_{i=1}^{k}$.\n- If the target is “largest,” then $\\mathcal{I}_{k}$ comprises the indices of the $r$ algebraically largest Ritz values among $\\{\\theta_{i}^{(k)}\\}_{i=1}^{k}$.\n\nDefine the minimal iteration count $k_{\\mathrm{acc}}$ as the smallest $k \\in \\{1, 2, \\ldots, k_{\\max}\\}$ for which at least $r$ Ritz pairs in the target set $\\mathcal{I}_{k}$ satisfy the acceptance inequality above. If no such $k$ exists up to $k_{\\max}$, define $k_{\\mathrm{acc}} = -1$.\n\nImplement a program that, for each of the following test cases, constructs the specified symmetric matrix $A$, uses the starting vector $v_{1}$ with components $v_{1,j} = 1$ for $j \\in \\{1,\\ldots,n\\}$ normalized to unit $2$-norm, and returns the corresponding value of $k_{\\mathrm{acc}}$.\n\nTest suite:\n- Test case $1$: $A$ is the discrete one-dimensional Dirichlet Laplacian of size $n = 50$, that is, a tridiagonal matrix with diagonal entries $2$ and sub- and super-diagonal entries $-1$. Target: “smallest,” with $r = 2$. Tolerances: $\\mathrm{rtol} = 10^{-8}$, $\\mathrm{atol} = 10^{-12}$. Maximum iteration $k_{\\max} = 50$.\n- Test case $2$: $A$ is the symmetric Toeplitz matrix of size $n = 60$ defined by $A_{ij} = \\exp\\!\\big(-((i-j)/\\sigma)^{2}\\big)$ with $\\sigma = 5$. Target: “largest,” with $r = 1$. Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-10}$. Maximum iteration $k_{\\max} = 60$.\n- Test case $3$: $A = c I$ with $c = 7$ and size $n = 20$. Target: “smallest,” with $r = 1$. Tolerances: $\\mathrm{rtol} = 10^{-12}$, $\\mathrm{atol} = 0$. Maximum iteration $k_{\\max} = 5$.\n- Test case $4$: $A$ is the discrete one-dimensional Dirichlet Laplacian of size $n = 40$. Target: “smallest,” with $r = 1$. Tolerances: $\\mathrm{rtol} = 10^{-14}$, $\\mathrm{atol} = 0$. Maximum iteration $k_{\\max} = 5$.\n\nYour program should produce a single line of output containing the four values $[k_{\\mathrm{acc}}^{(1)}, k_{\\mathrm{acc}}^{(2)}, k_{\\mathrm{acc}}^{(3)}, k_{\\mathrm{acc}}^{(4)}]$ as a comma-separated list enclosed in square brackets, in that order. Each $k_{\\mathrm{acc}}^{(t)}$ must be an integer, with $-1$ indicating that no acceptance occurred within the prescribed $k_{\\max}$ for test case $t$.",
            "solution": "The problem as stated is a well-defined exercise in computational physics and numerical linear algebra. It concerns the implementation of the Lanczos algorithm for finding eigenvalues of real symmetric matrices, coupled with a practical termination criterion based on the norm of the residual. All parameters and matrices are specified unambiguously, and the problem is scientifically sound, resting on foundational principles of matrix computations. Therefore, the problem is valid and we shall proceed with a complete solution.\n\nThe core of the problem is the Lanczos algorithm. For a given real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a starting vector $v_1$ with $\\|v_1\\|_2 = 1$, the algorithm iteratively constructs an orthonormal basis $\\{v_1, v_2, \\ldots, v_k\\}$ for the Krylov subspace $\\mathcal{K}_k(A, v_1) = \\mathrm{span}\\{v_1, A v_1, \\ldots, A^{k-1} v_1\\}$. The matrix of these basis vectors is $Q_k = [v_1 | v_2 | \\ldots | v_k] \\in \\mathbb{R}^{n \\times k}$.\n\nThe procedure is governed by a three-term recurrence relation:\n$$\n\\beta_{j+1} v_{j+1} = A v_j - \\alpha_j v_j - \\beta_j v_{j-1}\n$$\nwith initial conditions $v_0 = 0$ and $\\beta_1 = 0$. The coefficients are determined as $\\alpha_j = v_j^\\top A v_j$ and $\\beta_{j+1} = \\|A v_j - \\alpha_j v_j - \\beta_j v_{j-1}\\|_2$. These coefficients form a symmetric tridiagonal matrix $T_k \\in \\mathbb{R}^{k \\times k}$:\n$$\nT_k = Q_k^\\top A Q_k = \\begin{pmatrix}\n\\alpha_1  \\beta_2   \\\\\n\\beta_2  \\alpha_2  \\ddots  \\\\\n \\ddots  \\ddots  \\beta_k \\\\\n  \\beta_k  \\alpha_k\n\\end{pmatrix}\n$$\nThe eigenpairs $(\\theta_i^{(k)}, u_i^{(k)})$ of this small matrix $T_k$ are known as Ritz pairs. The Ritz values $\\theta_i^{(k)}$ are approximations to the eigenvalues of $A$, and the Ritz vectors $y_i^{(k)} = Q_k u_i^{(k)}$ are approximations to the corresponding eigenvectors of $A$.\n\nA critical component of an efficient implementation is the calculation of the residual norm $\\|r_i^{(k)}\\|_2 = \\|A y_i^{(k)} - \\theta_i^{(k)} y_i^{(k)}\\|_2$. A naive calculation would be computationally expensive. Instead, we exploit a fundamental property of the Lanczos factorization, $A Q_k = Q_k T_k + \\beta_{k+1} v_{k+1} e_k^\\top$, where $e_k$ is the $k$-th canonical basis vector in $\\mathbb{R}^k$. By substituting $y_i^{(k)} = Q_k u_i^{(k)}$ and using the eigen-relation $T_k u_i^{(k)} = \\theta_i^{(k)} u_i^{(k)}$, we derive:\n$$\nA y_i^{(k)} = A (Q_k u_i^{(k)}) = (Q_k T_k + \\beta_{k+1} v_{k+1} e_k^\\top) u_i^{(k)} = Q_k (T_k u_i^{(k)}) + \\beta_{k+1} v_{k+1} (e_k^\\top u_i^{(k)})\n$$\n$$\nA y_i^{(k)} = Q_k (\\theta_i^{(k)} u_i^{(k)}) + \\beta_{k+1} u_{i,k}^{(k)} v_{k+1} = \\theta_i^{(k)} (Q_k u_i^{(k)}) + \\beta_{k+1} u_{i,k}^{(k)} v_{k+1}\n$$\nwhere $u_{i,k}^{(k)}$ is the $k$-th (last) component of the eigenvector $u_i^{(k)}$. The residual vector is therefore $r_i^{(k)} = A y_i^{(k)} - \\theta_i^{(k)} y_i^{(k)} = \\beta_{k+1} u_{i,k}^{(k)} v_{k+1}$. Since the Lanczos vectors are orthonormal ($_2 = 1$), the residual norm simplifies elegantly to:\n$$\n\\|r_i^{(k)}\\|_2 = |\\beta_{k+1}| \\cdot |u_{i,k}^{(k)}|\n$$\nThis formula provides a computationally inexpensive way to check the termination criterion at each iteration $k$.\n\nThe overall algorithm to find the minimal iteration count $k_{\\mathrm{acc}}$ is as follows:\n$1$. For each test case, construct the matrix $A$ and the normalized starting vector $v_1$.\n$2$. Compute the spectral norm $\\|A\\|_2$. For a symmetric matrix, this is $\\max(|\\lambda_{\\min}|, |\\lambda_{\\max}|)$.\n$3$. Iterate $k$ from $1$ to $k_{\\max}$. In each iteration:\n    a. Perform one step of the Lanczos algorithm to compute $\\alpha_k$ and $\\beta_{k+1}$, extending the tridiagonal matrix $T_{k-1}$ to $T_k$.\n    b. Solve the eigenproblem for the $k \\times k$ matrix $T_k$ to obtain its full set of Ritz pairs $\\{(\\theta_i^{(k)}, u_i^{(k)})\\}_{i=1}^k$. For this, we use a specialized and efficient solver for symmetric tridiagonal matrices.\n    c. Identify the $r$ target Ritz pairs according to the 'smallest' or 'largest' criterion specified in the test case.\n    d. For each of these $r$ target pairs, calculate the residual norm using the formula $\\|r_i^{(k)}\\|_2 = |\\beta_{k+1}| |u_{i,k}^{(k)}|$.\n    e. Compare this norm to the threshold $\\tau_i^{(k)} = \\max(\\mathrm{atol}, \\mathrm{rtol} \\cdot \\max(1, |\\theta_i^{(k)}|)) \\cdot \\|A\\|_2$.\n    f. Count the number of target pairs that satisfy $\\|r_i^{(k)}\\|_2 \\leq \\tau_i^{(k)}$.\n$4$. If this count is greater than or equal to $r$, the current iteration number $k$ is the answer, $k_{\\mathrm{acc}}$. The search for this test case is complete.\n$5$. If the loop finishes without meeting the condition for any $k \\le k_{\\max}$, then $k_{\\mathrm{acc}} = -1$.\nThis procedure is repeated for all four test cases to obtain the final result vector.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef calculate_k_acc(A, v1_unnormalized, r, target, rtol, atol, k_max):\n    \"\"\"\n    Calculates the minimal Lanczos iteration count k_acc for convergence.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Normalize the starting vector to have a 2-norm of 1.\n    v1 = v1_unnormalized / np.linalg.norm(v1_unnormalized)\n\n    # Pre-compute the spectral norm of A for the tolerance calculation.\n    norm_A = np.linalg.norm(A, 2)\n    \n    # Lists to store Lanczos coefficients\n    alphas = []\n    betas_offdiag = []  # Stores beta_2, beta_3, ... for T_k's off-diagonal\n\n    # Lanczos algorithm initialization\n    v_prev = np.zeros(n)\n    v_curr = v1\n    beta_curr = 0.0  # beta_1 is defined as 0\n\n    for k in range(1, k_max + 1):\n        # Perform one step of the Lanczos iteration\n        w = A @ v_curr - beta_curr * v_prev\n        \n        alpha_curr = np.dot(v_curr, w)\n        alphas.append(alpha_curr)\n        \n        w -= alpha_curr * v_curr\n        \n        v_prev = v_curr\n        beta_next = np.linalg.norm(w)\n        \n        # At iteration k, we have T_k of size k x k.\n        # Its diagonal is 'alphas' (k elements).\n        # Its off-diagonal is 'betas_offdiag' (k-1 elements).\n        diag = np.array(alphas)\n        \n        if k == 1:\n            ritz_values = diag\n            # Eigenvector of a 1x1 matrix is [1].\n            ritz_vecs = np.array([[1.0]])\n        else:\n            offdiag = np.array(betas_offdiag)\n            # eigh_tridiagonal is efficient for this task and returns sorted eigenvalues.\n            ritz_values, ritz_vecs = eigh_tridiagonal(diag, offdiag, eigvals_only=False)\n        \n        # Only check for convergence if we have at least r Ritz values.\n        if k = r:\n            # Identify the indices of the r target Ritz pairs.\n            if target == 'smallest':\n                target_indices = range(r)\n            else:  # 'largest'\n                target_indices = range(k - r, k)\n            \n            num_converged = 0\n            for i in target_indices:\n                theta_i = ritz_values[i]\n                # Last component of the i-th eigenvector of T_k.\n                u_ik = ritz_vecs[-1, i]\n                \n                # Calculate the residual norm efficiently.\n                res_norm = abs(beta_next * u_ik)\n                \n                # Calculate the dynamic tolerance threshold.\n                threshold = max(atol, rtol * max(1.0, abs(theta_i))) * norm_A\n                \n                if res_norm = threshold:\n                    num_converged += 1\n            \n            if num_converged = r:\n                return k\n\n        # If breakdown occurs (beta_next is zero), the Krylov subspace is exhausted.\n        # If convergence was not declared, it means the required pairs did not converge.\n        if np.isclose(beta_next, 0.0):\n            return -1\n        \n        # Prepare for the next iteration.\n        v_curr = w / beta_next\n        beta_curr = beta_next\n        betas_offdiag.append(beta_curr)\n\n    # If the loop completes, convergence was not achieved within k_max iterations.\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_suite = [\n        # Test case 1\n        {'n': 50, 'type': 'laplacian', 'r': 2, 'target': 'smallest', 'rtol': 1e-8, 'atol': 1e-12, 'k_max': 50},\n        # Test case 2\n        {'n': 60, 'type': 'toeplitz', 'sigma': 5.0, 'r': 1, 'target': 'largest', 'rtol': 1e-6, 'atol': 1e-10, 'k_max': 60},\n        # Test case 3\n        {'n': 20, 'type': 'identity', 'c': 7.0, 'r': 1, 'target': 'smallest', 'rtol': 1e-12, 'atol': 0.0, 'k_max': 5},\n        # Test case 4\n        {'n': 40, 'type': 'laplacian', 'r': 1, 'target': 'smallest', 'rtol': 1e-14, 'atol': 0.0, 'k_max': 5},\n    ]\n\n    results = []\n    for params in test_suite:\n        n = params['n']\n        \n        # Construct the matrix A based on the test case type.\n        if params['type'] == 'laplacian':\n            A = 2.0 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n        elif params['type'] == 'toeplitz':\n            sigma = params['sigma']\n            indices = np.arange(n, dtype=float).reshape(-1, 1)\n            A = np.exp(-(((indices - indices.T) / sigma) ** 2))\n        elif params['type'] == 'identity':\n            c = params['c']\n            A = c * np.eye(n)\n        \n        # The unnormalized starting vector has all components equal to 1.\n        v1_unnormalized = np.ones(n)\n        \n        # Calculate k_acc for the current test case.\n        k_acc = calculate_k_acc(A, v1_unnormalized, params['r'], params['target'], params['rtol'], params['atol'], params['k_max'])\n        results.append(k_acc)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}