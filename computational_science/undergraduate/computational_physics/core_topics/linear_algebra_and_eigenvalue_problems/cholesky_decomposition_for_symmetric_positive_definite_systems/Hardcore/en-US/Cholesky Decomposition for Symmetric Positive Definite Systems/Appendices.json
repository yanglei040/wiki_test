{
    "hands_on_practices": [
        {
            "introduction": "A key property of a symmetric matrix $A$ that guarantees a unique Cholesky factorization is positive definiteness. This exercise provides a direct, computational method for verifying this property. By attempting to perform the factorization, you will see how the algorithm's success or failure serves as a definitive test for whether a matrix is symmetric positive definite (SPD), building your intuition across a range of illustrative examples .",
            "id": "2379910",
            "problem": "You are given a finite set of real square matrices that are each symmetric. A real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is called symmetric positive definite (SPD) if and only if for every nonzero vector $\\mathbf{x} \\in \\mathbb{R}^n$ one has $\\mathbf{x}^\\mathsf{T} A \\mathbf{x} > 0$. For each matrix in the test suite below, determine whether it is SPD. For each matrix, return a boolean value indicating whether it is SPD, using the mathematical definition. Your program must consider the input matrices exactly as provided without any rescaling or unit conversion.\n\nTest suite (each $A_k$ is symmetric and given explicitly):\n- $A_1 = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}$.\n- $A_2 = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}$.\n- $A_3 = \\begin{bmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix}$.\n- $A_4 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$.\n- $A_5 = \\begin{bmatrix} 10^{-12} & 0 \\\\ 0 & 1 \\end{bmatrix}$.\n- $A_6 = \\begin{bmatrix} -1 \\end{bmatrix}$.\n- $A_7 = \\begin{bmatrix} 2 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of boolean literals enclosed in square brackets, in the order $\\left[A_1, A_2, A_3, A_4, A_5, A_6, A_7\\right]$. For example, an output line must have the form $[\\text{True},\\text{False},\\dots]$ with no spaces.\n\nThe final output of your program must be exactly one line in the format described above.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and self-contained. It presents a standard task from linear algebra without any factual unsoundness, ambiguity, or contradiction.\n\nA real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined as symmetric positive definite (SPD) if the quadratic form $\\mathbf{x}^\\mathsf{T} A \\mathbf{x}$ is strictly positive for every non-zero vector $\\mathbf{x} \\in \\mathbb{R}^n$. That is,\n$$\n\\mathbf{x}^\\mathsf{T} A \\mathbf{x} > 0, \\quad \\forall \\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{x} \\neq \\mathbf{0}\n$$\nWhile this is the fundamental definition, its direct application is impractical as it requires checking an infinite set of vectors. For computational purposes, we rely on several equivalent and verifiable criteria for a symmetric matrix to be SPD:\n$1$. All eigenvalues of the matrix $A$ must be strictly positive.\n$2$. The matrix $A$ must have a unique Cholesky decomposition $A = L L^\\mathsf{T}$, where $L$ is a lower triangular matrix with strictly positive diagonal entries.\n$3$. All leading principal minors of $A$ must have strictly positive determinants (Sylvester's Criterion).\n\nThe Cholesky decomposition provides the most computationally efficient and numerically stable method for testing positive definiteness. An attempt to compute the Cholesky decomposition will fail if and only if the matrix is not symmetric positive definite. This is the method that will be implemented. We now analyze each matrix from the test suite.\n\n- $A_1 = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}$:\nThis is a symmetric $2 \\times 2$ matrix. We apply Sylvester's criterion. The first leading principal minor has determinant $\\det([4]) = 4 > 0$. The second leading principal minor has determinant $\\det(A_1) = (4)(3) - (1)(1) = 11 > 0$. Since all leading principal minors are positive, the matrix is SPD. Result: **True**.\n\n- $A_2 = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}$:\nWe check the eigenvalues. The characteristic polynomial is $\\det(A_2 - \\lambda I) = (1-\\lambda)^2 - 4 = \\lambda^2 - 2\\lambda - 3 = (\\lambda-3)(\\lambda+1) = 0$. The eigenvalues are $\\lambda_1 = 3$ and $\\lambda_2 = -1$. The presence of a negative eigenvalue means the matrix is not SPD. Result: **False**.\n\n- $A_3 = \\begin{bmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix}$:\nThis is a symmetric tridiagonal matrix. We use Sylvester's criterion.\nThe $1^{st}$ leading principal minor is $\\det([2]) = 2 > 0$.\nThe $2^{nd}$ leading principal minor is $\\det\\left(\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}\\right) = (2)(2) - (-1)(-1) = 3 > 0$.\nThe $3^{rd}$ leading principal minor is $\\det(A_3) = 2(3) - (-1)((-1)(2) - (0)(-1)) = 6 - 2 = 4 > 0$.\nAll leading principal minors are positive, thus the matrix is SPD. Result: **True**.\n\n- $A_4 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$:\nThis matrix is singular, as its determinant is $\\det(A_4) = (1)(1) - (1)(1) = 0$. A singular matrix has at least one eigenvalue equal to $0$. For a matrix to be SPD, all eigenvalues must be *strictly* positive. A matrix with a $0$ eigenvalue is positive semi-definite, but not positive definite. If $\\mathbf{v}$ is the eigenvector corresponding to the eigenvalue $0$, then $\\mathbf{v} \\neq \\mathbf{0}$ but $\\mathbf{v}^\\mathsf{T} A \\mathbf{v} = \\mathbf{v}^\\mathsf{T} (A\\mathbf{v}) = \\mathbf{v}^\\mathsf{T} (0\\mathbf{v}) = 0$, violating the strict inequality. Result: **False**.\n\n- $A_5 = \\begin{bmatrix} 10^{-12} & 0 \\\\ 0 & 1 \\end{bmatrix}$:\nThis is a diagonal matrix. The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues are $\\lambda_1 = 10^{-12}$ and $\\lambda_2 = 1$. Both are strictly positive. Therefore, the matrix is SPD. The small magnitude of an eigenvalue does not matter, only its sign. Result: **True**.\n\n- $A_6 = \\begin{bmatrix} -1 \\end{bmatrix}$:\nThis is a $1 \\times 1$ matrix. The only eigenvalue is $-1$, which is negative. Let $\\mathbf{x} = [c]$ for any non-zero real number $c$. Then $\\mathbf{x}^\\mathsf{T} A_6 \\mathbf{x} = c(-1)c = -c^2 < 0$. The matrix is not SPD. Result: **False**.\n\n- $A_7 = \\begin{bmatrix} 2 \\end{bmatrix}$:\nThis is a $1 \\times 1$ matrix. The only eigenvalue is $2$, which is positive. For any non-zero $\\mathbf{x}=[c]$, $\\mathbf{x}^\\mathsf{T} A_7 \\mathbf{x} = c(2)c = 2c^2 > 0$. The matrix is SPD. Result: **True**.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, LinAlgError\n\ndef solve():\n    \"\"\"\n    Solves the problem of determining whether a set of symmetric matrices\n    are positive definite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([[4, 1], [1, 3]]),\n        np.array([[1, 2], [2, 1]]),\n        np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]]),\n        np.array([[1, 1], [1, 1]]),\n        np.array([[1e-12, 0], [0, 1]]),\n        np.array([[-1]]),\n        np.array([[2]]),\n    ]\n\n    def is_spd(matrix: np.ndarray) -> bool:\n        \"\"\"\n        Checks if a symmetric matrix is positive definite using Cholesky decomposition.\n        A real symmetric matrix is positive definite if and only if it has a\n        Cholesky decomposition. The scipy.linalg.cholesky function raises\n        a LinAlgError if the decomposition fails.\n\n        Args:\n            matrix: A numpy array representing a symmetric matrix.\n\n        Returns:\n            True if the matrix is symmetric positive definite, False otherwise.\n        \"\"\"\n        try:\n            # Attempting the Cholesky decomposition is a standard and efficient\n            # numerical test for positive definiteness.\n            cholesky(matrix, lower=True)\n            return True\n        except LinAlgError:\n            # The matrix is not positive definite if Cholesky decomposition fails.\n            return False\n\n    results = [is_spd(A) for A in test_cases]\n\n    # Final print statement in the exact required format.\n    # The format is a comma-separated list of boolean literals.\n    # e.g., [True,False,...] with no spaces.\n    # The map(str, results) converts each boolean to its string representation\n    # ('True' or 'False'), and join combines them with a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a matrix is confirmed to be SPD, the Cholesky decomposition provides a highly efficient and numerically stable path to solving the linear system $A \\mathbf{x} = \\mathbf{b}$. This practice guides you through the implementation of a complete Cholesky solver from first principles . You will code the factorization $A = L L^{\\mathsf{T}}$ and then use the resulting triangular factor $L$ to find the solution $\\mathbf{x}$ through forward and backward substitution, a cornerstone technique in computational physics.",
            "id": "2379908",
            "problem": "You will implement and test a program that solves symmetric positive definite linear systems using the Cholesky decomposition. In computational physics, many elliptic partial differential equation discretizations lead to linear systems of the form $A \\mathbf{x} = \\mathbf{b}$ where $A$ is symmetric positive definite. A fundamental characterization is that $A$ is symmetric positive definite if and only if $\\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} > 0$ for every nonzero vector $\\mathbf{x}$. Under this condition, the quadratic energy $E(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} - \\mathbf{b}^{\\mathsf{T}} \\mathbf{x}$ is strictly convex and has a unique minimizer that satisfies the normal equations $A \\mathbf{x} = \\mathbf{b}$. Your task is to build a solver from first principles that exploits this structure.\n\nImplement a function that, given a symmetric positive definite matrix $A$ and a vector $\\mathbf{b}$, returns the solution $\\mathbf{x}$ by:\n- Computing a Cholesky factorization $A = L L^{\\mathsf{T}}$ with $L$ lower triangular and strictly positive diagonal entries, without calling any library routine for Cholesky factorization.\n- Solving the lower-triangular system $L \\mathbf{y} = \\mathbf{b}$ by forward substitution.\n- Solving the upper-triangular system $L^{\\mathsf{T}} \\mathbf{x} = \\mathbf{y}$ by back substitution.\n\nRequirements for the implementation:\n- Input $A$ must be real, square, and symmetric. Your implementation must check symmetry numerically.\n- If $A$ is not positive definite, your factorization should detect this via a nonpositive pivot and signal an error.\n- Use double-precision floating point arithmetic.\n- Do not call any built-in Cholesky solver; implement the factorization and the triangular solves using basic operations.\n\nTest suite:\nYour program must run the solver on the following seven test cases and aggregate the results.\n\n- Case $1$ (small, well-conditioned):\n  $$A_1 = \\begin{bmatrix} 4 & 1 & 1 \\\\ 1 & 3 & 0 \\\\ 1 & 0 & 2 \\end{bmatrix}, \\quad \\mathbf{x}^{\\star}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{b}_1 = A_1 \\mathbf{x}^{\\star}_1.$$\n  Report the maximum absolute error $e_1 = \\|\\mathbf{x}_1 - \\mathbf{x}^{\\star}_1\\|_{\\infty}$ as a floating-point number.\n\n- Case $2$ (identity):\n  $$A_2 = I_{5}, \\quad \\mathbf{x}^{\\star}_2 = \\begin{bmatrix} -2 \\\\ 0.5 \\\\ 3.5 \\\\ 0 \\\\ -1 \\end{bmatrix}, \\quad \\mathbf{b}_2 = \\mathbf{x}^{\\star}_2.$$\n  Report $e_2 = \\|\\mathbf{x}_2 - \\mathbf{x}^{\\star}_2\\|_{\\infty}$.\n\n- Case $3$ (diagonal with wide scaling):\n  $$A_3 = \\operatorname{diag}\\!\\left(10^{-3},\\,10^{-1},\\,1,\\,10,\\,10^{3}\\right), \\quad \\mathbf{x}^{\\star}_3 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\\\ -2 \\\\ 0.5 \\end{bmatrix}, \\quad \\mathbf{b}_3 = A_3 \\mathbf{x}^{\\star}_3.$$\n  Report $e_3 = \\|\\mathbf{x}_3 - \\mathbf{x}^{\\star}_3\\|_{\\infty}$.\n\n- Case $4$ (one-dimensional Poisson operator with Dirichlet boundaries, banded symmetric positive definite):\n  For $n = 6$, let $A_4 \\in \\mathbb{R}^{n \\times n}$ be tridiagonal with main diagonal equal to $2$ and first sub- and super-diagonals equal to $-1$, i.e.,\n  $$A_4 = \\begin{bmatrix}\n  2 & -1 & 0 & 0 & 0 & 0 \\\\\n  -1 & 2 & -1 & 0 & 0 & 0 \\\\\n  0 & -1 & 2 & -1 & 0 & 0 \\\\\n  0 & 0 & -1 & 2 & -1 & 0 \\\\\n  0 & 0 & 0 & -1 & 2 & -1 \\\\\n  0 & 0 & 0 & 0 & -1 & 2\n  \\end{bmatrix}.$$\n  Take $\\mathbf{x}^{\\star}_4 = \\mathbf{1} \\in \\mathbb{R}^{6}$ (all ones), and $\\mathbf{b}_4 = A_4 \\mathbf{x}^{\\star}_4$. Report $e_4 = \\|\\mathbf{x}_4 - \\mathbf{x}^{\\star}_4\\|_{\\infty}$.\n\n- Case $5$ (random symmetric positive definite):\n  Construct $A_5$ as follows. Set a deterministic seed, generate $M \\in \\mathbb{R}^{6 \\times 6}$ with independent standard normal entries, and let\n  $$A_5 = M^{\\mathsf{T}} M + 10^{-3} I_{6}.$$\n  Choose $\\mathbf{x}^{\\star}_5 \\in \\mathbb{R}^{6}$ with independent standard normal entries, and set $\\mathbf{b}_5 = A_5 \\mathbf{x}^{\\star}_5$. Report $e_5 = \\|\\mathbf{x}_5 - \\mathbf{x}^{\\star}_5\\|_{\\infty}$.\n\n- Case $6$ (ill-conditioned symmetric positive definite, Hilbert matrix):\n  Let $A_6 \\in \\mathbb{R}^{5 \\times 5}$ be the Hilbert matrix with entries $(A_6)_{ij} = 1/(i+j+1)$ for zero-based indices $i,j \\in \\{0,1,2,3,4\\}$. Let\n  $$\\mathbf{x}^{\\star}_6 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{b}_6 = A_6 \\mathbf{x}^{\\star}_6.$$\n  Report $e_6 = \\|\\mathbf{x}_6 - \\mathbf{x}^{\\star}_6\\|_{\\infty}$.\n\n- Case $7$ (symmetric but not positive definite):\n  $$A_7 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}, \\quad \\mathbf{b}_7 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.$$\n  Run your solver and report a boolean $r_7$ that is $\\text{True}$ if your solver correctly detects non-positive definiteness and signals an error, and $\\text{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries in the order of the cases above, namely $[e_1,e_2,e_3,e_4,e_5,e_6,r_7]$. For example, a schematic format is [$r_1,r_2,r_3,r_4,r_5,r_6,r_7$], where $r_k$ denotes the result for case $k$.\n\nThe final outputs for cases $1$ through $6$ are floating-point numbers. The final output for case $7$ is a boolean. No angles or physical units are involved in this problem; all quantities are dimensionless real numbers.",
            "solution": "We begin from the definition of a symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$: $A$ is symmetric ($A = A^{\\mathsf{T}}$) and satisfies $\\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} > 0$ for all nonzero $\\mathbf{x} \\in \\mathbb{R}^{n}$. In computational physics, such matrices arise as stiffness matrices for discretized elliptic operators. Consider the quadratic energy\n$$\nE(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} - \\mathbf{b}^{\\mathsf{T}} \\mathbf{x}.\n$$\nSince $A$ is positive definite, $E$ is strictly convex and coercive. The unique minimizer $\\mathbf{x}^{\\star}$ satisfies the first-order optimality condition $\\nabla E(\\mathbf{x}) = A \\mathbf{x} - \\mathbf{b} = 0$, which is equivalent to the linear system $A \\mathbf{x} = \\mathbf{b}$.\n\nThe Cholesky factorization leverages positive definiteness to guarantee the existence and uniqueness of a factorization\n$$\nA = L L^{\\mathsf{T}},\n$$\nwhere $L$ is lower triangular with strictly positive diagonal entries. This factorization can be computed via a constructive algorithm that processes $A$ one column at a time. The core identities come from expanding $A = L L^{\\mathsf{T}}$ componentwise. For $k \\in \\{0,\\dots,n-1\\}$, write\n$$\nA_{kk} = \\sum_{j=0}^{k} L_{kj}^{2} \\quad \\text{and} \\quad A_{ik} = \\sum_{j=0}^{k} L_{ij} L_{kj} \\quad \\text{for } i > k.\n$$\nSubtracting the contribution from previously computed entries yields the recursive formulas\n$$\nL_{kk} = \\sqrt{A_{kk} - \\sum_{j=0}^{k-1} L_{kj}^{2}}, \\quad\nL_{ik} = \\frac{A_{ik} - \\sum_{j=0}^{k-1} L_{ij} L_{kj}}{L_{kk}} \\quad \\text{for } i = k+1,\\dots,n-1.\n$$\nThe term under the square root is strictly positive by positive definiteness, which ensures well-defined real square roots and strictly positive pivots.\n\nOnce $L$ is computed, solving $A \\mathbf{x} = \\mathbf{b}$ reduces to two triangular solves:\n- Forward substitution solves $L \\mathbf{y} = \\mathbf{b}$. For $i = 0,\\dots,n-1$,\n$$\ny_i = \\frac{1}{L_{ii}} \\left( b_i - \\sum_{j=0}^{i-1} L_{ij} y_j \\right).\n$$\n- Back substitution solves $L^{\\mathsf{T}} \\mathbf{x} = \\mathbf{y}$. For $i = n-1,\\dots,0$,\n$$\nx_i = \\frac{1}{L_{ii}} \\left( y_i - \\sum_{j=i+1}^{n-1} L_{ji} x_j \\right).\n$$\n\nAlgorithmic design:\n- Validate that $A$ is square and numerically symmetric; a practical check is that $\\|A - A^{\\mathsf{T}}\\|_{\\infty}$ is below a small multiple of machine precision times $\\|A\\|_{\\infty}$.\n- Perform the Cholesky factorization using the recursive formulas above. If at any step the computed pivot $A_{kk} - \\sum_{j=0}^{k-1} L_{kj}^{2}$ is nonpositive (e.g., less than or equal to zero within a numerical tolerance), declare that $A$ is not positive definite and signal an error.\n- Carry out forward and back substitutions using the formulas above.\n- For each symmetric positive definite test case, compute the maximum absolute error $e = \\|\\mathbf{x} - \\mathbf{x}^{\\star}\\|_{\\infty} = \\max_i |x_i - x^{\\star}_i|$.\n- For the symmetric but not positive definite test case, catch the signaled error and report a boolean indicating successful detection.\n\nRationale and numerical considerations:\n- No pivoting is required: for symmetric positive definite matrices, Cholesky factorization without pivoting is backward stable in floating-point arithmetic and guarantees positive pivots in exact arithmetic.\n- The work complexity is $\\mathcal{O}(n^{3}/3)$ floating-point operations for the factorization and $\\mathcal{O}(n^{2})$ for the triangular solves.\n- The test suite spans well-conditioned small systems, trivial identity, widely scaled diagonal systems, banded systems typical of elliptic operators, random symmetric positive definite systems, and an ill-conditioned Hilbert matrix to probe numerical robustness. The final test ensures that the solver properly detects the violation of positive definiteness.\n\nThe program implements these steps without calling a library Cholesky routine, constructs each test case exactly as specified, computes the error metrics $e_1,\\dots,e_6$ and the boolean $r_7$, and prints a single line with the list $[e_1,e_2,e_3,e_4,e_5,e_6,r_7]$.",
            "answer": "```python\nimport numpy as np\n\ndef cholesky_solve(A: np.ndarray, b: np.ndarray, symmetry_tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Solve A x = b for symmetric positive definite A using Cholesky factorization A = L L^T,\n    followed by forward and back substitution. Raises ValueError if A is not SPD.\n\n    Parameters:\n        A (np.ndarray): Real symmetric positive definite matrix of shape (n, n).\n        b (np.ndarray): Right-hand side vector of shape (n,).\n        symmetry_tol (float): Tolerance for numerical symmetry check.\n\n    Returns:\n        x (np.ndarray): Solution vector of shape (n,).\n    \"\"\"\n    A = np.array(A, dtype=np.float64, copy=True)\n    b = np.array(b, dtype=np.float64, copy=False)\n    n = A.shape[0]\n\n    if A.ndim != 2 or A.shape[1] != n:\n        raise ValueError(\"Matrix A must be square.\")\n    if b.shape != (n,):\n        raise ValueError(\"Vector b must have shape (n,).\")\n\n    # Symmetry check\n    if not np.allclose(A, A.T, atol=symmetry_tol, rtol=0.0):\n        raise ValueError(\"Matrix A is not symmetric within the given tolerance.\")\n\n    # Cholesky factorization: compute lower-triangular L such that A = L L^T\n    L = np.zeros_like(A)\n    for k in range(n):\n        # Compute diagonal element\n        if k == 0:\n            sumsq = 0.0\n        else:\n            sumsq = float(np.dot(L[k, :k], L[k, :k]))\n        diag = A[k, k] - sumsq\n        if not np.isfinite(diag) or diag <= 0.0:\n            raise ValueError(\"Matrix A is not positive definite (nonpositive pivot encountered).\")\n        Lkk = np.sqrt(diag)\n        L[k, k] = Lkk\n\n        # Compute subdiagonal elements in column k\n        if k < n - 1:\n            # Vectorized computation of the remaining entries in column k\n            for i in range(k + 1, n):\n                if k == 0:\n                    cross = 0.0\n                else:\n                    cross = float(np.dot(L[i, :k], L[k, :k]))\n                L[i, k] = (A[i, k] - cross) / Lkk\n\n    # Forward substitution: solve L y = b\n    y = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        if i == 0:\n            acc = 0.0\n        else:\n            acc = float(np.dot(L[i, :i], y[:i]))\n        y[i] = (b[i] - acc) / L[i, i]\n\n    # Back substitution: solve L^T x = y\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        if i == n - 1:\n            acc = 0.0\n        else:\n            acc = float(np.dot(L[i + 1:, i], x[i + 1:]))\n        x[i] = (y[i] - acc) / L[i, i]\n\n    return x\n\n\ndef make_tridiagonal_poisson(n: int) -> np.ndarray:\n    \"\"\"Construct the 1D Poisson tridiagonal SPD matrix with 2 on diagonal and -1 on off-diagonals.\"\"\"\n    A = np.zeros((n, n), dtype=np.float64)\n    diag = 2.0\n    off = -1.0\n    for i in range(n):\n        A[i, i] = diag\n        if i + 1 < n:\n            A[i, i + 1] = off\n            A[i + 1, i] = off\n    return A\n\n\ndef make_hilbert(n: int) -> np.ndarray:\n    \"\"\"Construct the Hilbert matrix of size n with zero-based indexing: A[i,j] = 1 / (i + j + 1).\"\"\"\n    i = np.arange(n, dtype=np.float64).reshape(-1, 1)\n    j = np.arange(n, dtype=np.float64).reshape(1, -1)\n    return 1.0 / (i + j + 1.0)\n\n\ndef solve():\n    results = []\n\n    # Case 1\n    A1 = np.array([[4.0, 1.0, 1.0],\n                   [1.0, 3.0, 0.0],\n                   [1.0, 0.0, 2.0]], dtype=np.float64)\n    x1_true = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n    b1 = A1 @ x1_true\n    x1 = cholesky_solve(A1, b1)\n    e1 = float(np.max(np.abs(x1 - x1_true)))\n    results.append(e1)\n\n    # Case 2\n    A2 = np.eye(5, dtype=np.float64)\n    x2_true = np.array([-2.0, 0.5, 3.5, 0.0, -1.0], dtype=np.float64)\n    b2 = x2_true.copy()\n    x2 = cholesky_solve(A2, b2)\n    e2 = float(np.max(np.abs(x2 - x2_true)))\n    results.append(e2)\n\n    # Case 3\n    A3 = np.diag([1e-3, 1e-1, 1.0, 10.0, 1e3]).astype(np.float64)\n    x3_true = np.array([1.0, -1.0, 2.0, -2.0, 0.5], dtype=np.float64)\n    b3 = A3 @ x3_true\n    x3 = cholesky_solve(A3, b3)\n    e3 = float(np.max(np.abs(x3 - x3_true)))\n    results.append(e3)\n\n    # Case 4\n    n4 = 6\n    A4 = make_tridiagonal_poisson(n4)\n    x4_true = np.ones(n4, dtype=np.float64)\n    b4 = A4 @ x4_true\n    x4 = cholesky_solve(A4, b4)\n    e4 = float(np.max(np.abs(x4 - x4_true)))\n    results.append(e4)\n\n    # Case 5\n    np.random.seed(7)\n    M = np.random.randn(6, 6).astype(np.float64)\n    A5 = M.T @ M + 1e-3 * np.eye(6, dtype=np.float64)\n    x5_true = np.random.randn(6).astype(np.float64)\n    b5 = A5 @ x5_true\n    x5 = cholesky_solve(A5, b5)\n    e5 = float(np.max(np.abs(x5 - x5_true)))\n    results.append(e5)\n\n    # Case 6\n    A6 = make_hilbert(5)\n    x6_true = np.array([1.0, -1.0, 1.0, -1.0, 1.0], dtype=np.float64)\n    b6 = A6 @ x6_true\n    x6 = cholesky_solve(A6, b6)\n    e6 = float(np.max(np.abs(x6 - x6_true)))\n    results.append(e6)\n\n    # Case 7 (non-SPD detection)\n    A7 = np.array([[0.0, 1.0],\n                   [1.0, 0.0]], dtype=np.float64)\n    b7 = np.array([1.0, 2.0], dtype=np.float64)\n    detected = False\n    try:\n        _ = cholesky_solve(A7, b7)\n        detected = False\n    except ValueError:\n        detected = True\n    results.append(detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many dynamic physical systems, the governing matrix $A$ is not static but receives small updates over time. Recomputing the entire Cholesky factorization after each change is computationally expensive. This advanced exercise introduces an elegant and efficient solution: the rank-1 update, which modifies an existing Cholesky factor in $\\mathcal{O}(n^2)$ time to reflect a change of the form $A \\rightarrow A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$ , a crucial optimization in fields like signal processing and control theory.",
            "id": "2379848",
            "problem": "You are given a symmetric positive definite matrix factorization in lower-triangular form, namely a matrix $L \\in \\mathbb{R}^{n \\times n}$ with strictly positive diagonal such that $A = L L^{\\mathsf{T}}$, where $A$ is symmetric positive definite. Consider a rank-$1$ update of the form $A \\leftarrow A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$ for a nonzero vector $\\mathbf{v} \\in \\mathbb{R}^{n}$. Your task is to implement an algorithm that updates $L$ to a new lower-triangular factor $L^{\\prime}$ with positive diagonal, satisfying $A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}} = L^{\\prime} L^{\\prime \\mathsf{T}}$, using only $\\mathcal{O}(n^{2})$ operations and without recomputing a Cholesky factorization from scratch.\n\nFundamental base and definitions to use:\n- A matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite if and only if $\\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} > 0$ for all nonzero $\\mathbf{x} \\in \\mathbb{R}^{n}$, and if and only if there exists a unique lower-triangular $L$ with positive diagonal such that $A = L L^{\\mathsf{T}}$ (Cholesky factorization).\n- For any $\\mathbf{v} \\in \\mathbb{R}^{n}$, the matrix $A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$ is symmetric positive definite if $A$ is symmetric positive definite.\n\nYour program must:\n- Implement an in-place style rank-$1$ update routine operating on a lower-triangular factor $L$ and a vector $\\mathbf{v}$ to produce $L^{\\prime}$ such that $L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} = L L^{\\mathsf{T}} + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$, using only $\\mathcal{O}(n^{2})$ arithmetic.\n- Verify correctness for each test by computing the Frobenius norm of the difference between the target matrix $A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$ and the reconstructed matrix $L^{\\prime} {L^{\\prime}}^{\\mathsf{T}}$, i.e., compute $\\| L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} - (A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}) \\|_{F}$.\n- Return these norms as floating-point numbers.\n\nTest suite:\nUse the following five test cases. In each case, $L$ is lower-triangular with positive diagonal. All entries are real-valued.\n\n1) Happy-path case ($n = 4$):\n- $L = \\begin{bmatrix}\n2.0 & 0 & 0 & 0 \\\\\n0.5 & 1.8 & 0 & 0 \\\\\n1.2 & -0.3 & 1.5 & 0 \\\\\n-0.7 & 0.4 & 0.6 & 1.1\n\\end{bmatrix}$\n- $\\mathbf{v} = \\begin{bmatrix} 0.8 \\\\ -1.2 \\\\ 0.3 \\\\ 0.5 \\end{bmatrix}$\n\n2) Boundary case ($n = 1$):\n- $L = \\beginbmatrix} 2.0 \\end{bmatrix}$\n- $\\mathbf{v} = \\beginbmatrix} 3.0 \\end{bmatrix}$\n\n3) Zero-update edge case ($n = 3$):\n- $L = \\begin{bmatrix}\n1.2 & 0 & 0 \\\\\n0.1 & 1.1 & 0 \\\\\n-0.2 & 0.05 & 0.9\n\\end{bmatrix}$\n- $\\mathbf{v} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$\n\n4) Small-perturbation case ($n = 3$):\n- $L = \\begin{bmatrix}\n3.0 & 0 & 0 \\\\\n1.0 & 2.5 & 0 \\\\\n-1.2 & 0.7 & 2.2\n\\end{bmatrix}$\n- $\\mathbf{v} = \\begin{bmatrix} 1\\times 10^{-8} \\\\ -2\\times 10^{-8} \\\\ 3\\times 10^{-8} \\end{bmatrix}$\n\n5) Ill-conditioned but valid case ($n = 5$):\n- $L = \\begin{bmatrix}\n1\\times 10^{-6} & 0 & 0 & 0 & 0 \\\\\n2\\times 10^{-7} & 1\\times 10^{-3} & 0 & 0 & 0 \\\\\n-1\\times 10^{-7} & 3\\times 10^{-4} & 1\\times 10^{-1} & 0 & 0 \\\\\n5\\times 10^{-8} & -2\\times 10^{-4} & 4\\times 10^{-2} & 1.0 & 0 \\\\\n1\\times 10^{-8} & 1\\times 10^{-5} & -3\\times 10^{-3} & 2\\times 10^{-1} & 10.0\n\\end{bmatrix}$\n- $\\mathbf{v} = \\begin{bmatrix} 1\\times 10^{-6} \\\\ -1\\times 10^{-4} \\\\ 1\\times 10^{-2} \\\\ -1.0 \\\\ 2.0 \\end{bmatrix}$\n\nYour program should:\n- For each test case, compute $A = L L^{\\mathsf{T}}$, compute $L^{\\prime}$ via your rank-$1$ update routine using the given $L$ and $\\mathbf{v}$, then compute the Frobenius norm $\\| L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} - (A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}) \\|_{F}$ as a floating-point value.\n- Produce a single line of output containing the five results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4},r_{5}]$).\n\nNotes:\n- No physical units are involved in this problem.\n- Do not call a generic full Cholesky decomposition on $A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$ to obtain $L^{\\prime}$. You must implement the $\\mathcal{O}(n^{2})$ rank-$1$ update algorithm that operates directly on the factor $L$ and the vector $\\mathbf{v}$.",
            "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the domain of numerical linear algebra, well-posed, objective, and contains all necessary information for a unique and verifiable solution. We may therefore proceed with the derivation and implementation of the required algorithm.\n\nThe problem requires the computation of an updated Cholesky factorization following a rank-$1$ update to a symmetric positive definite (SPD) matrix. Given a lower-triangular matrix $L \\in \\mathbb{R}^{n \\times n}$ with positive diagonal elements such that $A = L L^{\\mathsf{T}}$, and a vector $\\mathbf{v} \\in \\mathbb{R}^{n}$, we seek a new lower-triangular matrix $L^{\\prime} \\in \\mathbb{R}^{n \\times n}$, also with positive diagonal elements, such that $L^{\\prime} {L^{\\prime}}^{\\mathsf{T}} = A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$. A naive approach of first computing $A' = A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$ and then performing a full Cholesky decomposition of $A'$ would cost $\\mathcal{O}(n^3)$ operations. The requirement is to devise an algorithm with complexity $\\mathcal{O}(n^2)$.\n\nThe fundamental relationship to be satisfied is:\n$$\nL^{\\prime} {L^{\\prime}}^{\\mathsf{T}} = L L^{\\mathsf{T}} + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}\n$$\nThis equation can be expressed in terms of matrix products. Consider the augmented matrix $\\begin{bmatrix} L & \\mathbf{v} \\end{bmatrix} \\in \\mathbb{R}^{n \\times (n+1)}$. Then, the updated matrix $A' = A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$ can be written as:\n$$\nA' = \\begin{bmatrix} L & \\mathbf{v} \\end{bmatrix} \\begin{bmatrix} L^\\mathsf{T} \\\\ \\mathbf{v}^\\mathsf{T} \\end{bmatrix}\n$$\nThis representation does not immediately lead to a triangular factor. A more productive formulation is to consider the transpose, where we have an $(n+1) \\times n$ matrix $M$:\n$$\nM = \\begin{bmatrix} L^\\mathsf{T} \\\\ \\mathbf{v}^\\mathsf{T} \\end{bmatrix}\n$$\nThen, the updated matrix is $A' = M^\\mathsf{T} M = (L^\\mathsf{T})^\\mathsf{T} L^\\mathsf{T} + (\\mathbf{v}^\\mathsf{T})^\\mathsf{T} \\mathbf{v}^\\mathsf{T} = L L^\\mathsf{T} + \\mathbf{v} \\mathbf{v}^\\mathsf{T}$.\nWe are seeking a factorization $A' = L' {L'}^{\\mathsf{T}}$. By identifying ${L'}^{\\mathsf{T}}$ with an upper-triangular matrix $R$, we have $A' = R^\\mathsf{T} R$. This suggests that $R$ is the upper-triangular factor from a QR decomposition of the matrix $M$. That is, if we find an orthogonal matrix $Q \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ such that:\n$$\nQ M = \\begin{bmatrix} R \\\\ 0 \\end{bmatrix}\n$$\nwhere $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular, then $M = Q^\\mathsf{T} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix}$. Consequently,\n$$\nA' = M^\\mathsf{T} M = \\left( Q^\\mathsf{T} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\right)^\\mathsf{T} \\left( Q^\\mathsf{T} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} R^\\mathsf{T} & 0 \\end{bmatrix} Q Q^\\mathsf{T} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} R^\\mathsf{T} & 0 \\end{bmatrix} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} = R^\\mathsf{T} R\n$$\nWe can therefore identify $L'^\\mathsf{T} = R$. The matrix $M$ has a special structure: its top $n \\times n$ block, $L^\\mathsf{T}$, is already upper triangular.\n$$\nM = \\begin{bmatrix} L^\\mathsf{T} \\\\ \\mathbf{v}^\\mathsf{T} \\end{bmatrix} = \\begin{bmatrix}\nl_{11} & l_{21} & \\dots & l_{n1} \\\\\n0 & l_{22} & \\dots & l_{n2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & l_{nn} \\\\\nv_1 & v_2 & \\dots & v_n\n\\end{bmatrix}\n$$\nThis structure is \"almost\" upper triangular, except for the last row. We can introduce zeros into this last row, one element at a time, using a sequence of $n$ Givens rotations. For each column $j=1, \\dots, n$, we apply a rotation in the plane of row $j$ and row $n+1$ to zero out the element $(n+1, j)$, which is $v_j$. This process must be done sequentially from $j=1$ to $n$.\n\nFor the sake of direct implementation on the lower-triangular matrix $L$, we can derive an equivalent column-wise algorithm. This avoids explicit formation of $L^\\mathsf{T}$. The algorithm proceeds by updating one column of $L$ at a time, from $j=1$ to $n$. Let $\\mathbf{p}$ be an auxiliary vector, initialized to $\\mathbf{v}$. At each step $j$, we use a Givens rotation to combine the diagonal element $L_{jj}$ with the current $j$-th element of $\\mathbf{p}$, denoted $p_j$. This rotation updates $L_{jj}$ and annihilates $p_j$. The same rotation must then be applied to the rest of the $j$-th column of $L$ (elements $L_{ij}$ for $i > j$) and the corresponding elements of $\\mathbf{p}$ (elements $p_i$ for $i > j$).\n\nThe algorithm is as follows:\nInitialize an auxiliary vector $\\mathbf{p} \\leftarrow \\mathbf{v}$. The matrix $L'$ is to be computed, which can be done in-place on a copy of $L$.\n\nFor $j = 1, 2, \\dots, n$:\n1.  Extract the diagonal element $\\alpha = L_{jj}$ and the vector element $\\beta = p_j$.\n2.  Compute the new diagonal element of $L'$, which is the Euclidean norm:\n    $$\n    L'_{jj} = \\sqrt{\\alpha^2 + \\beta^2}\n    $$\n    Since $L_{jj} > 0$ by definition of the Cholesky factor, $L'_{jj}$ will also be positive (unless both $\\alpha$ and $\\beta$ were zero, which is not the case).\n3.  Define the cosine and sine for the Givens rotation:\n    $$\n    c = \\frac{\\alpha}{L'_{jj}}, \\quad s = \\frac{\\beta}{L'_{jj}}\n    $$\n    These satisfy $c^2 + s^2 = 1$. The transformation $\\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} L'_{jj} \\\\ 0 \\end{pmatrix}$ effectively incorporates $\\beta$ into the diagonal and zeroes the targeted element of $\\mathbf{p}$.\n4.  Apply this same rotation to the remaining elements of the $j$-th column of $L$ and the vector $\\mathbf{p}$. For each $i = j+1, \\dots, n$:\n    $$\n    \\begin{pmatrix} L'_{ij} \\\\ p'_{i} \\end{pmatrix} = \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} L_{ij} \\\\ p_{i} \\end{pmatrix}\n    $$\n    This translates to the updates:\n    $$\n    L'_{ij} \\leftarrow c \\cdot L_{ij} + s \\cdot p_i\n    $$\n    $$\n    p_i \\leftarrow -s \\cdot L_{ij} + c \\cdot p_i\n    $$\n    These updates must use the values of $L_{ij}$ and $p_i$ from before the start of step $j$. When implementing in-place, one must be careful to use the \"old\" values for both updates.\n\nThis process is repeated for all columns. The total number of arithmetic operations is dominated by step 4. For each $j$, the update loop for $i$ runs $n-j$ times, each time performing a constant number of multiplications and additions. The total complexity is therefore proportional to $\\sum_{j=1}^{n-1} (n-j) \\approx n^2/2$, which is $\\mathcal{O}(n^2)$ as required. The resulting matrix $L'$ is the desired lower-triangular Cholesky factor of $A + \\mathbf{v} \\mathbf{v}^{\\mathsf{T}}$.",
            "answer": "```python\nimport numpy as np\n\ndef chol_rank1_update(L: np.ndarray, v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs a rank-1 update of a Cholesky factorization.\n\n    Given a lower-triangular matrix L and a vector v, computes the \n    lower-triangular matrix L_prime such that:\n    L_prime @ L_prime.T = L @ L.T + v @ v.T\n\n    The algorithm has O(n^2) complexity.\n\n    Args:\n        L: An (n, n) lower-triangular numpy array with a positive diagonal.\n        v: An (n,) numpy array representing the update vector.\n\n    Returns:\n        An (n, n) lower-triangular numpy array L_prime.\n    \"\"\"\n    n = L.shape[0]\n    # Make copies to avoid modifying the original inputs\n    L_prime = L.copy()\n    p = v.copy().astype(float) # Ensure p is float for computations\n\n    for j in range(n):\n        # Extract the diagonal element of L and the j-th element of p\n        alpha = L_prime[j, j]\n        beta = p[j]\n\n        # Compute the new diagonal element for L_prime\n        # np.hypot calculates sqrt(alpha^2 + beta^2) robustly\n        L_prime_jj_new = np.hypot(alpha, beta)\n\n        # Calculate Givens rotation parameters\n        if L_prime_jj_new == 0.0:\n            # This case occurs if alpha and beta are both zero. Since L has a\n            # positive diagonal, alpha > 0, so this branch is unlikely.\n            c = 1.0\n            s = 0.0\n        else:\n            c = alpha / L_prime_jj_new\n            s = beta / L_prime_jj_new\n        \n        # Update the diagonal element of L_prime\n        L_prime[j, j] = L_prime_jj_new\n\n        # Apply the rotation to the rest of the column j and vector p\n        if j < n - 1:\n            # Select the sub-column of L and sub-vector of p to be updated\n            L_subcol = L_prime[j+1:n, j]\n            p_subvec = p[j+1:n]\n            \n            # The update for the new p sub-vector must use the OLD L sub-column.\n            # The update for the new L sub-column must use the OLD p sub-vector.\n            # We compute the new p sub-vector first before updating L_prime's column.\n            new_p_subvec = -s * L_subcol + c * p_subvec\n            L_prime[j+1:n, j] = c * L_subcol + s * p_subvec\n            p[j+1:n] = new_p_subvec\n            \n    return L_prime\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the Cholesky rank-1 update validation.\n    \"\"\"\n    test_cases = [\n        # 1) Happy-path case (n=4)\n        (np.array([[2.0, 0, 0, 0], \n                   [0.5, 1.8, 0, 0], \n                   [1.2, -0.3, 1.5, 0], \n                   [-0.7, 0.4, 0.6, 1.1]]), \n         np.array([0.8, -1.2, 0.3, 0.5])),\n        # 2) Boundary case (n=1)\n        (np.array([[2.0]]), \n         np.array([3.0])),\n        # 3) Zero-update edge case (n=3)\n        (np.array([[1.2, 0, 0], \n                   [0.1, 1.1, 0], \n                   [-0.2, 0.05, 0.9]]), \n         np.array([0.0, 0.0, 0.0])),\n        # 4) Small-perturbation case (n=3)\n        (np.array([[3.0, 0, 0], \n                   [1.0, 2.5, 0], \n                   [-1.2, 0.7, 2.2]]), \n         np.array([1e-8, -2e-8, 3e-8])),\n        # 5) Ill-conditioned but valid case (n=5)\n        (np.array([[1e-6, 0, 0, 0, 0],\n                   [2e-7, 1e-3, 0, 0, 0],\n                   [-1e-7, 3e-4, 1e-1, 0, 0],\n                   [5e-8, -2e-4, 4e-2, 1.0, 0],\n                   [1e-8, 1e-5, -3e-3, 2e-1, 10.0]]),\n         np.array([1e-6, -1e-4, 1e-2, -1.0, 2.0])),\n    ]\n\n    results = []\n    for L, v in test_cases:\n        # Compute the target matrix A_prime = L L^T + v v^T\n        A = L @ L.T\n        v_col = v[:, np.newaxis]\n        A_prime_target = A + v_col @ v_col.T\n\n        # Get the updated Cholesky factor L_prime using the O(n^2) algorithm\n        L_prime = chol_rank1_update(L, v)\n        \n        # Reconstruct the matrix from the updated factor\n        A_prime_reconstructed = L_prime @ L_prime.T\n\n        # Compute the Frobenius norm of the difference to verify correctness\n        error_norm = np.linalg.norm(A_prime_reconstructed - A_prime_target, 'fro')\n        results.append(error_norm)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}