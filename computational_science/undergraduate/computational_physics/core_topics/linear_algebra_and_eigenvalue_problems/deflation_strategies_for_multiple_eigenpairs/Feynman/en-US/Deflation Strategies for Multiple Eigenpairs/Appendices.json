{
    "hands_on_practices": [
        {
            "introduction": "This problem provides a fundamental, hands-on demonstration of deflation in action. You will implement the power method to find a matrix's dominant eigenvalue and see how its convergence rate is dramatically affected by the spectral gap—the ratio $|\\lambda_2 / \\lambda_1|$. By then implementing Hotelling's deflation, you will observe how removing the found eigenpair makes finding the next eigenvalue significantly more efficient, a core motivation for using deflation strategies .",
            "id": "2384610",
            "problem": "Consider the problem of computing multiple eigenpairs of a real symmetric matrix by iterative methods. Start from the core definitions of eigenvalues and eigenvectors: for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, an eigenpair $(\\lambda, v)$ satisfies $A v = \\lambda v$ with $v \\neq 0$. The Rayleigh quotient of a nonzero vector $x$ is $r(x) = \\frac{x^{\\top} A x}{x^{\\top} x}$, and for a unit vector $x$ one has $r(x) = x^{\\top} A x$. The power method constructs a sequence by repeated application of $A$ followed by normalization, $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}$, and monitors $r(x_k)$ for convergence. Deflation is the transformation of $A$ after an approximate dominant eigenpair $(\\widehat{\\lambda}_1, \\widehat{v}_1)$ has been computed, by forming a deflated matrix $A_{\\text{def}} = A - \\widehat{\\lambda}_1 \\widehat{v}_1 \\widehat{v}_1^{\\top}$, where $\\widehat{v}_1$ is normalized to unit length. For a symmetric $A$, if $(\\widehat{\\lambda}_1, \\widehat{v}_1)$ is accurate, the largest-magnitude eigenvalue of $A_{\\text{def}}$ approximates the second-largest eigenvalue of $A$.\n\nYour task is to implement the following, using only the definitions above and basic linear algebra operations:\n\n1) Implement the power method for a real symmetric matrix $A$:\n- Initialize a random vector $x_0 \\in \\mathbb{R}^n$ with independent standard normal entries, then normalize it to unit length.\n- Iterate $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}$.\n- At each iteration compute the Rayleigh quotient $r_k = x_k^{\\top} A x_k$.\n- Stop when the absolute change in Rayleigh quotient satisfies $|r_k - r_{k-1}| < \\tau$, where $\\tau$ is a given tolerance.\n- Count the number of iterations $k$ required to satisfy the stopping condition.\n\n2) Implement Hotelling deflation on a normalized eigenvector approximation: given $(\\widehat{\\lambda}_1, \\widehat{v}_1)$ with $\\|\\widehat{v}_1\\|_2 = 1$, form $A_{\\text{def}} = A - \\widehat{\\lambda}_1 \\widehat{v}_1 \\widehat{v}_1^{\\top}$.\n\n3) Demonstrate slow convergence of the power method when the spectral gap is small, and show dramatic acceleration for finding the second eigenpair via deflation, by constructing matrices with prescribed spectra. Construct $A$ as $A = Q \\Lambda Q^{\\top}$ with $\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_n)$ and $Q$ an orthogonal matrix obtained from the orthonormal-triangular decomposition (QR) of a random matrix with standard normal entries. Use a pseudorandom number generator with a fixed integer seed to make $Q$ reproducible.\n\n4) For each test case below:\n- Build $A$ from the specified spectrum.\n- Apply the power method to $A$ to compute $(\\widehat{\\lambda}_1, \\widehat{v}_1)$ and record the iteration count $k_1$ required to meet the stopping condition.\n- Form $A_{\\text{def}} = A - \\widehat{\\lambda}_1 \\widehat{v}_1 \\widehat{v}_1^{\\top}$.\n- Apply the power method to $A_{\\text{def}}$ to compute an approximation to the second eigenvalue and record the iteration count $k_2$ required to meet the stopping condition.\n\nUse the following parameter values as the test suite:\n- Common parameters: dimension $n = 5$; tolerance $\\tau = 10^{-10}$; maximum allowed iterations per power method call $k_{\\max} = 200000$; for each case, generate the orthogonal matrix $Q$ as the $Q$ factor of the QR decomposition of an $n \\times n$ standard normal matrix with the stated seed, and generate the initial vector $x_0$ for each power method call from the same pseudorandom generator immediately after $Q$ is formed (normalize $x_0$ to unit length). All random numbers are dimensionless.\n- Case $\\mathrm{A}$ (small spectral gap): spectrum $\\{\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, \\lambda_5\\} = \\{1, 1 - 10^{-2}, 0.2, 0.1, 0.05\\}$, seed $s = 123$.\n- Case $\\mathrm{B}$ (very small spectral gap): spectrum $\\{\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, \\lambda_5\\} = \\{1, 1 - 10^{-3}, 0.2, 0.1, 0.05\\}$, seed $s = 456$.\n- Case $\\mathrm{C}$ (large spectral gap): spectrum $\\{\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, \\lambda_5\\} = \\{1, 0.5, 0.2, 0.1, 0.05\\}$, seed $s = 789$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n- $[k_1^{(\\mathrm{A})}, k_2^{(\\mathrm{A})}, k_1^{(\\mathrm{B})}, k_2^{(\\mathrm{B})}, k_1^{(\\mathrm{C})}, k_2^{(\\mathrm{C})}]$,\nwhere $k_1^{(\\cdot)}$ is the iteration count for the first call to the power method on $A$, and $k_2^{(\\cdot)}$ is the iteration count for the call on $A_{\\text{def}}$. Each $k$ must be reported as an integer. No physical units are involved in this problem, and no angles or percentages are required.",
            "solution": "The problem requires an implementation and demonstration of the power method in conjunction with Hotelling deflation to compute the first two eigenpairs of a real symmetric matrix. The core of the problem is to illustrate the dependency of the power method's convergence rate on the spectral gap and to show how deflation can circumvent slow convergence when finding subdominant eigenpairs.\n\nFirst, we establish the theoretical background. For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigenvalues $|\\lambda_1| > |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$ and a corresponding orthonormal set of eigenvectors $\\{v_1, v_2, \\dots, v_n\\}$, the power method is an iterative algorithm designed to find the dominant eigenpair $(\\lambda_1, v_1)$. The iterative step is given by\n$$\nx_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}\n$$\nwhere $x_0$ is an initial vector that is not orthogonal to $v_1$. For a randomly chosen $x_0$, this condition holds with probability $1$. The sequence of vectors $\\{x_k\\}$ converges to $\\pm v_1$. The rate of convergence is determined by the ratio $|\\lambda_2 / \\lambda_1|$. If this ratio is close to $1$, the spectral gap is small, and convergence is slow.\n\nThe eigenvalue $\\lambda_1$ can be estimated using the Rayleigh quotient, defined for a non-zero vector $x$ as $r(x) = \\frac{x^{\\top} A x}{x^{\\top} x}$. As $x_k \\to v_1$, the Rayleigh quotient $r(x_k)$ converges to $\\lambda_1$. The problem specifies the stopping criterion as $|r_k - r_{k-1}| < \\tau$, where $r_k = x_k^{\\top} A x_k$ (assuming $x_k$ is a unit vector) and $\\tau$ is a small tolerance. The algorithm to be implemented is thus:\n1. Initialize a normalized random vector $x_0$. Let $x \\leftarrow x_0$.\n2. Compute an initial Rayleigh quotient $r_{\\text{old}} = x^{\\top} A x$.\n3. For $k=1, 2, \\dots, k_{\\max}$:\n    a. Compute the next iterate: $x_{\\text{next}} = A x / \\|A x\\|_2$.\n    b. Update the vector: $x \\leftarrow x_{\\text{next}}$.\n    c. Compute the new Rayleigh quotient: $r_{\\text{new}} = x^{\\top} A x$.\n    d. Check for convergence: if $|r_{\\text{new}} - r_{\\text{old}}| < \\tau$, terminate and report the number of iterations, $k$.\n    e. Update for the next iteration: $r_{\\text{old}} \\leftarrow r_{\\text{new}}$.\nThis literal interpretation of the problem statement requires two matrix-vector products per iteration, which is computationally inefficient. A more standard implementation would reuse the intermediate product $A x$. However, we will adhere strictly to the stated formulation.\n\nOnce an approximation to the dominant eigenpair, $(\\widehat{\\lambda}_1, \\widehat{v}_1)$, is found (with $\\|\\widehat{v}_1\\|_2=1$), Hotelling deflation constructs a new matrix:\n$$\nA_{\\text{def}} = A - \\widehat{\\lambda}_1 \\widehat{v}_1 \\widehat{v}_1^{\\top}\n$$\nIf $(\\widehat{\\lambda}_1, \\widehat{v}_1)$ is an exact eigenpair $(\\lambda_1, v_1)$, then the eigenvectors $v_2, \\dots, v_n$ of $A$ are also eigenvectors of $A_{\\text{def}}$ with the same eigenvalues $\\lambda_2, \\dots, \\lambda_n$. The vector $v_1$ becomes an eigenvector of $A_{\\text{def}}$ with eigenvalue $0$. Consequently, the dominant eigenvalue of $A_{\\text{def}}$ is $\\lambda_2$. Applying the power method to $A_{\\text{def}}$ will thus compute the second eigenpair of the original matrix $A$. The convergence rate for this second stage will depend on the ratio $|\\lambda_3 / \\lambda_2|$. For the spectra provided in the problem, this ratio is significantly smaller than unity, which predicts a dramatic acceleration in convergence compared to finding $\\lambda_1$ in the presence of a small spectral gap.\n\nThe overall procedure for each test case is as follows:\n1.  Set the seed for the pseudorandom number generator to ensure reproducibility.\n2.  Construct the symmetric matrix $A = Q \\Lambda Q^{\\top}$, where $\\Lambda$ is the diagonal matrix of specified eigenvalues and $Q$ is an orthogonal matrix obtained from the QR decomposition of a random matrix of size $n \\times n$ with standard normal entries.\n3.  Generate a random, normalized initial vector $x_0$ for the first power method run.\n4.  Apply the power method to $A$ with initial vector $x_0$ to find $(\\widehat{\\lambda}_1, \\widehat{v}_1)$ and the iteration count $k_1$.\n5.  Construct the deflated matrix $A_{\\text{def}} = A - \\widehat{\\lambda}_1 \\widehat{v}_1 \\widehat{v}_1^{\\top}$.\n6.  Generate a new random, normalized initial vector for the second power method run.\n7.  Apply the power method to $A_{\\text{def}}$ to find an approximation to the second eigenvalue and record the iteration count $k_2$.\n8.  The final output collates the iteration counts $\\{k_1, k_2\\}$ for all specified test cases.\n\nThe test cases are designed to illustrate the theory: Cases A and B have small spectral gaps ($1 - \\lambda_2/\\lambda_1$ is $10^{-2}$ and $10^{-3}$, respectively), which should lead to large values for $k_1$. Case C has a large spectral gap, which should result in a small $k_1$. In all cases, the gap between $\\lambda_2$ and $\\lambda_3$ is large, so $k_2$ is expected to be small, demonstrating the effectiveness of deflation.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef power_method(A, x0, tolerance, max_iter):\n    \"\"\"\n    Implements the power method for a real symmetric matrix.\n\n    Args:\n        A (np.ndarray): The symmetric matrix.\n        x0 (np.ndarray): The initial normalized vector.\n        tolerance (float): The stopping condition tolerance.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing:\n            - r_new (float): The estimated eigenvalue.\n            - x (np.ndarray): The estimated eigenvector.\n            - k (int): The number of iterations performed.\n        Returns (None, None, max_iter) if convergence is not achieved.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n    \n    # This implementation follows the problem description literally, which requires\n    # two matrix-vector products per iteration. One to update the vector,\n    # and one to compute the new Rayleigh quotient.\n    r_old = x.T @ A @ x\n\n    for k in range(1, max_iter + 1):\n        # Update step for the vector\n        Ax = A @ x\n        norm_Ax = np.linalg.norm(Ax)\n        if norm_Ax == 0:\n            # This can happen for the deflated matrix if x aligns with v1\n            return 0.0, x, k\n        x = Ax / norm_Ax\n\n        # Compute new Rayleigh quotient and check for convergence\n        r_new = x.T @ A @ x\n        \n        if np.abs(r_new - r_old) < tolerance:\n            return r_new, x, k\n        \n        r_old = r_new\n\n    return r_old, x, max_iter\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the power method and deflation experiments.\n    \"\"\"\n    # Common parameters\n    n = 5\n    tolerance = 1e-10\n    max_iter = 200000\n\n    test_cases = [\n        # Case A (small spectral gap)\n        (123, np.array([1.0, 1.0 - 1e-2, 0.2, 0.1, 0.05])),\n        # Case B (very small spectral gap)\n        (456, np.array([1.0, 1.0 - 1e-3, 0.2, 0.1, 0.05])),\n        # Case C (large spectral gap)\n        (789, np.array([1.0, 0.5, 0.2, 0.1, 0.05])),\n    ]\n\n    results = []\n\n    for seed, spectrum in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # 1. Construct the matrix A = Q * Lambda * Q^T\n        Lambda = np.diag(spectrum)\n        M = rng.standard_normal((n, n))\n        Q, _ = qr(M)\n        A = Q @ Lambda @ Q.T\n\n        # 2. Apply power method to A to find the first eigenpair\n        x0_1 = rng.standard_normal(n)\n        x0_1 /= np.linalg.norm(x0_1)\n        lambda_hat_1, v_hat_1, k1 = power_method(A, x0_1, tolerance, max_iter)\n        \n        # 3. Form the deflated matrix A_def\n        A_def = A - lambda_hat_1 * np.outer(v_hat_1, v_hat_1)\n\n        # 4. Apply power method to A_def to find the second eigenpair\n        x0_2 = rng.standard_normal(n)\n        x0_2 /= np.linalg.norm(x0_2)\n        _, _, k2 = power_method(A_def, x0_2, tolerance, max_iter)\n        \n        results.extend([k1, k2])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To deepen our understanding of how deflation modifies a matrix's structure, we now tackle the \"inverse\" problem. Instead of removing an eigenpair, this exercise challenges you to construct a matrix $A$ that is guaranteed to have a specified eigenpair $(\\lambda_0, v_0)$ while its remaining spectrum is inherited from a smaller, given matrix $A_0$. This \"matrix inflation\" task  requires a solid grasp of similarity transformations and block-diagonal structures, reinforcing the geometric intuition behind deflation from a constructive point of view.",
            "id": "2384661",
            "problem": "Construct a program that, for each test case, builds a real square matrix $A \\in \\mathbb{R}^{n \\times n}$ having a prescribed target eigenpair $(\\lambda_0, v_0)$ and whose remaining eigenvalues coincide with the spectrum of a given smaller real square matrix $A_0 \\in \\mathbb{R}^{(n-1) \\times (n-1)}$. For each test case, you are given a real scalar $\\lambda_0 \\in \\mathbb{R}$, a nonzero real vector $v_0 \\in \\mathbb{R}^n$, and a real matrix $A_0 \\in \\mathbb{R}^{(n-1) \\times (n-1)}$. Your program must, for each case, construct a matrix $A \\in \\mathbb{R}^{n \\times n}$ such that:\n- $(\\lambda_0, v_0)$ is an eigenpair of $A$, meaning $A v_0 = \\lambda_0 v_0$, and\n- the multiset of eigenvalues of $A$ is equal to $\\{\\lambda_0\\} \\cup \\sigma(A_0)$, where $\\sigma(A_0)$ denotes the multiset of eigenvalues of $A_0$.\n\nFor numerical verification, define a tolerance $\\tau = 10^{-8}$. For each constructed $A$, your program must verify both of the following conditions:\n1. The eigenpair residual condition: the Euclidean norm of the residual satisfies $\\|A v_0 - \\lambda_0 v_0\\|_2 \\le \\tau \\cdot \\max\\{1, (\\|A\\|_2 + |\\lambda_0|)\\|v_0\\|_2\\}$.\n2. The spectrum match condition: after removing exactly one eigenvalue of $A$ that is closest in modulus to $\\lambda_0$ (to account for the target eigenvalue), the remaining multiset of eigenvalues of $A$ matches the multiset $\\sigma(A_0)$ within tolerance in the following sense: for each $\\mu \\in \\sigma(A_0)$ there exists a distinct eigenvalue $\\nu$ from the remaining eigenvalues of $A$ such that $|\\nu - \\mu| \\le \\tau \\cdot \\max\\{1, |\\mu|\\}$.\n\nThe values for the test suite are as follows. For each case, $n$ is the length of $v_0$ and $A_0 \\in \\mathbb{R}^{(n-1)\\times(n-1)}$:\n- Case 1: $\\lambda_0 = -1.5$, $v_0 = [2,-1,0,2]^\\top$, $$A_0 = \\begin{bmatrix} 0 & 2 & 0 \\\\ -2 & 0 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}.$$\n- Case 2: $\\lambda_0 = 0$, $v_0 = [1,1,1]^\\top$, $$A_0 = \\begin{bmatrix} 5 & 0 \\\\ 0 & 5 \\end{bmatrix}.$$\n- Case 3: $\\lambda_0 = 7$, $v_0 = [0,1]^\\top$, $$A_0 = \\begin{bmatrix} -3 \\end{bmatrix}.$$\n- Case 4: $\\lambda_0 = 2$, $v_0 = [1,0,0]^\\top$, $$A_0 = \\begin{bmatrix} 4 & 1 \\\\ 0 & 4 \\end{bmatrix}.$$\n\nYour program must output, in order, for each case a boolean indicating whether both verification conditions hold. The final output must be a single line containing a comma-separated list of these booleans enclosed in square brackets. For example, the output format must be like \"[result1,result2,result3,result4]\" (without spaces). No angles or physical units are involved in this problem. All numerical comparisons must use the tolerance $\\tau = 10^{-8}$ as specified.",
            "solution": "The problem statement is reviewed and found to be valid. It presents a well-defined task in numerical linear algebra, free of scientific or logical inconsistencies. The objective is to construct a real matrix $A \\in \\mathbb{R}^{n \\times n}$ that satisfies two conditions: it must possess a specified eigenpair $(\\lambda_0, v_0)$ and its remaining spectrum must match that of a given matrix $A_0 \\in \\mathbb{R}^{(n-1) \\times (n-1)}$. We proceed with the derivation of a constructive solution.\n\nThe core of our method lies in the use of a similarity transformation. A matrix $A$ is similar to a matrix $B$ if $A = P B P^{-1}$ for some invertible matrix $P$. Similar matrices share the same spectrum (eigenvalues and their algebraic multiplicities). Our goal is to choose $P$ and $B$ judiciously to enforce the given eigenpair and spectral properties.\n\nLet the given eigenvector be $v_0 \\in \\mathbb{R}^n$, where $v_0 \\neq 0$. We begin by constructing a special basis for $\\mathbb{R}^n$. Let $u_0 = v_0 / \\|v_0\\|_2$ be the normalized eigenvector. We then complete $\\{u_0\\}$ to an orthonormal basis for $\\mathbb{R}^n$, which we denote as $\\{u_0, u_1, \\dots, u_{n-1}\\}$. Let $U$ be the orthogonal matrix whose columns are these basis vectors: $U = [u_0 | u_1 | \\dots | u_{n-1}]$. Since $U$ is orthogonal, its inverse is its transpose, $U^{-1} = U^\\top$.\n\nWe define the matrix $A$ via the similarity transformation $A = U B U^\\top$. We must now determine the structure of $B$ to satisfy the problem's conditions.\n\nFirst, let us impose the eigenpair condition, $A v_0 = \\lambda_0 v_0$. Substituting the definition of $A$ and $v_0 = \\|v_0\\|_2 u_0$, we get:\n$$\n(U B U^\\top) (\\|v_0\\|_2 u_0) = \\lambda_0 (\\|v_0\\|_2 u_0)\n$$\nSince $\\|v_0\\|_2$ is a non-zero scalar, we can simplify this to $U B U^\\top u_0 = \\lambda_0 u_0$. Multiplying from the left by $U^\\top$ yields:\n$$\nB (U^\\top u_0) = \\lambda_0 (U^\\top u_0)\n$$\nBy construction, $u_0$ is the first column of $U$. Therefore, $U^\\top u_0$ is the vector $e_1 = [1, 0, \\dots, 0]^\\top$. The condition simplifies to $B e_1 = \\lambda_0 e_1$. This equation implies that the first column of the matrix $B$ must be $[\\lambda_0, 0, \\dots, 0]^\\top$. Consequently, $B$ must have an upper-triangular block structure:\n$$\nB = \\begin{bmatrix} \\lambda_0 & w^\\top \\\\ 0 & C \\end{bmatrix}\n$$\nwhere $w \\in \\mathbb{R}^{n-1}$ is an arbitrary row vector and $C \\in \\mathbb{R}^{(n-1) \\times (n-1)}$ is some matrix.\n\nSecond, we consider the spectral condition. The spectrum of $A$, denoted $\\sigma(A)$, must be $\\{\\lambda_0\\} \\cup \\sigma(A_0)$. Since $A$ and $B$ are similar, $\\sigma(A) = \\sigma(B)$. The eigenvalues of a block-triangular matrix are the union of the eigenvalues of its diagonal blocks. Thus, the spectrum of $B$ is $\\{\\lambda_0\\} \\cup \\sigma(C)$.\nTo satisfy the problem's requirement, we must have $\\sigma(C) = \\sigma(A_0)$. While there are many matrices $C$ that are similar to $A_0$ and thus share its spectrum, the simplest and most direct choice is to set $C = A_0$.\n\nThe row vector $w^\\top$ does not influence the eigenvalues of $B$. Its choice affects the eigenvectors of $A$ (other than $v_0$) but not the required spectral properties. For simplicity, we choose $w=0$. This leads to the block-diagonal form for $B$:\n$$\nB = \\begin{bmatrix} \\lambda_0 & 0 \\\\ 0 & A_0 \\end{bmatrix}\n$$\n\nWith these choices, the final construction for the matrix $A$ is:\n$$\nA = U \\begin{bmatrix} \\lambda_0 & 0 \\\\ 0 & A_0 \\end{bmatrix} U^\\top\n$$\nThe procedure for constructing $A$ is as follows:\n1.  Given $v_0$, normalize it to obtain $u_0 = v_0 / \\|v_0\\|_2$.\n2.  Construct an orthonormal basis for the orthogonal complement of $u_0$. This is equivalent to finding an orthonormal basis for the null space of $u_0^\\top$. Let the matrix whose columns are these basis vectors be $U_1 \\in \\mathbb{R}^{n \\times (n-1)}$.\n3.  Form the orthogonal matrix $U = [u_0 | U_1]$.\n4.  Form the block-diagonal matrix $B = \\text{scipy.linalg.block\\_diag}(\\lambda_0, A_0)$.\n5.  Compute $A = U B U^\\top$. Since $\\lambda_0$, $v_0$, and $A_0$ are real, $U$ and $B$ can be constructed as real matrices, ensuring $A$ is also real.\n\nFor numerical verification, two conditions must be checked against a tolerance $\\tau = 10^{-8}$.\n1.  The eigenpair residual condition, $\\|A v_0 - \\lambda_0 v_0\\|_2 \\le \\tau \\cdot \\max\\{1, (\\|A\\|_2 + |\\lambda_0|)\\|v_0\\|_2\\}$, is checked by direct computation of the norms involved.\n2.  The spectrum match condition requires comparing the multiset of eigenvalues of $A_0$, $\\sigma(A_0)$, with the multiset of eigenvalues of $A$ after removing the one eigenvalue closest in modulus to $\\lambda_0$. Let the remaining eigenvalues of $A$ be the multiset $S'_A$. We must verify that a bijection $f: \\sigma(A_0) \\to S'_A$ exists such that for every $\\mu \\in \\sigma(A_0)$, its image $\\nu = f(\\mu)$ satisfies $|\\nu - \\mu| \\le \\tau \\cdot \\max\\{1, |\\mu|\\}$. This is a minimum weight perfect matching problem on a bipartite graph, which is solved by finding a permutation of the elements of $\\sigma(A_0)$ that minimizes the sum of distances to the elements of $S'_A$. Given the small dimensions in the test cases, iterating through all permutations is feasible.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import null_space, block_diag\nfrom itertools import permutations\n\ndef solve():\n    \"\"\"\n    Constructs and verifies a matrix A for each test case.\n    \"\"\"\n    test_cases = [\n        {\n            \"lambda0\": -1.5,\n            \"v0\": np.array([2.0, -1.0, 0.0, 2.0]),\n            \"A0\": np.array([[0.0, 2.0, 0.0], [-2.0, 0.0, 0.0], [0.0, 0.0, 3.0]])\n        },\n        {\n            \"lambda0\": 0.0,\n            \"v0\": np.array([1.0, 1.0, 1.0]),\n            \"A0\": np.array([[5.0, 0.0], [0.0, 5.0]])\n        },\n        {\n            \"lambda0\": 7.0,\n            \"v0\": np.array([0.0, 1.0]),\n            \"A0\": np.array([[-3.0]])\n        },\n        {\n            \"lambda0\": 2.0,\n            \"v0\": np.array([1.0, 0.0, 0.0]),\n            \"A0\": np.array([[4.0, 1.0], [0.0, 4.0]])\n        },\n    ]\n\n    results = []\n    tau = 1e-8\n\n    for case in test_cases:\n        lambda0 = case[\"lambda0\"]\n        v0 = case[\"v0\"]\n        A0 = case[\"A0\"]\n        n = len(v0)\n\n        # Step 1: Construct the orthogonal matrix U\n        u0 = v0 / np.linalg.norm(v0)\n        u0_col = u0.reshape(-1, 1)\n\n        # Find an orthonormal basis for the orthogonal complement of u0\n        # This is the null space of u0.T\n        U1 = null_space(u0.reshape(1, -1))\n\n        # Form the full orthogonal matrix U\n        U = np.hstack((u0_col, U1))\n        \n        # Step 2: Construct the block matrix B\n        B = block_diag(lambda0, A0)\n\n        # Step 3: Construct the matrix A\n        A = U @ B @ U.T\n\n        # Step 4: Verification\n        # Condition 1: Eigenpair residual\n        residual_norm = np.linalg.norm(A @ v0 - lambda0 * v0)\n        bound = tau * max(1.0, (np.linalg.norm(A, 2) + np.abs(lambda0)) * np.linalg.norm(v0))\n        cond1_holds = residual_norm <= bound\n        \n        # Condition 2: Spectrum match\n        eigs_A = np.linalg.eigvals(A)\n        eigs_A0 = np.linalg.eigvals(A0)\n        \n        # Find and remove the eigenvalue of A closest in modulus to lambda0\n        idx_to_remove = np.argmin(np.abs(np.abs(eigs_A) - np.abs(lambda0)))\n        eigs_A_rem = np.delete(eigs_A, idx_to_remove)\n        \n        # Sort remaining eigenvalues for a canonical order\n        eigs_A_rem = np.sort(eigs_A_rem)\n        \n        cond2_holds = False\n        # Check all permutations of eigs_A0 to find a valid matching\n        for p in permutations(eigs_A0):\n            p_arr = np.array(p)\n            # Check the tolerance for this permutation\n            errors = np.abs(eigs_A_rem - p_arr)\n            tolerances = tau * np.maximum(1.0, np.abs(p_arr))\n            if np.all(errors <= tolerances):\n                cond2_holds = True\n                break\n        \n        results.append(cond1_holds and cond2_holds)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond the power method, eigenvalues can be found using variational principles by finding the stationary points of the Rayleigh quotient $R_A(x) = (x^\\top A x) / (x^\\top x)$. This advanced practice guides you through implementing a modern, optimization-based approach to find eigenpairs using gradient ascent, where gradients are computed via automatic differentiation rather than analytical formulas. You will then integrate this optimizer with a deflationary scheme, demonstrating the modularity of deflation and its applicability within sophisticated computational frameworks .",
            "id": "2384622",
            "problem": "You are given real, symmetric matrices and asked to compute multiple eigenpairs using a principled variational approach. Consider the Rayleigh quotient defined for a nonzero vector $x \\in \\mathbb{R}^n$ and a real, symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ by\n$$\nR_A(x) = \\frac{x^\\top A x}{x^\\top x}.\n$$\nBy first principles, the stationary points of $R_A(x)$ on the unit sphere $S^{n-1} = \\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$ correspond to eigenvectors of $A$, and the Rayleigh quotient value at such a point equals the corresponding eigenvalue. Your task is to construct a program that:\n- Computes gradients of $R_A(x)$ with respect to $x$ using an automatic differentiation routine, not by substituting a prederived analytic gradient formula.\n- Uses these gradients to find the two largest eigenpairs of $A$ by optimizing $R_A(x)$ over the unit sphere.\n- After identifying each eigenpair $(\\lambda, v)$ with $\\|v\\|_2 = 1$, deflates the operator in a way that preserves symmetry to remove the contribution of the found eigenpair, so that the next optimization identifies a distinct eigenpair of the deflated operator.\n- Ensures the eigenvectors corresponding to distinct eigenvalues are orthonormal, and in the case of repeated eigenvalues, returns any orthonormal set spanning the relevant eigenspace.\n\nInitialization for the optimization must use the unit-normalized all-ones vector $x_0 = \\frac{1}{\\sqrt{n}} (1,1,\\dots,1)^\\top$. The optimization should proceed deterministically with the following numerical parameters: step size $s = 0.2$, iteration cap $M = 20000$, and stopping tolerance $\\varepsilon = 10^{-10}$ for the absolute change in the Rayleigh quotient value between consecutive iterates. At each iteration, the current iterate must be renormalized to maintain $\\|x\\|_2 = 1$. Angles are not used in this problem, so no angle unit is required. No physical units are involved.\n\nTest Suite:\nFor each test matrix below, compute the two largest eigenvalues via the described gradient-based procedure with deflation. The matrices are:\n1. Size $5$: the discrete one-dimensional Laplacian with Dirichlet-type stencil\n$$\nA_1 = \\begin{bmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{bmatrix}.\n$$\n2. Size $3$: a matrix with a repeated largest eigenvalue,\n$$\nA_2 = \\begin{bmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix}.\n$$\n3. Size $3$: an indefinite symmetric matrix,\n$$\nA_3 = \\begin{bmatrix}\n0 & 2 & 0 \\\\\n2 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{bmatrix}.\n$$\n\nFor each matrix $A_i$, return the two largest eigenvalues produced by your method as an ordered pair $[\\lambda^{(i)}_1,\\lambda^{(i)}_2]$ with $\\lambda^{(i)}_1 \\ge \\lambda^{(i)}_2$, each rounded to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list of these ordered pairs, enclosed in square brackets. For example, the overall output format must be:\n[[lam1_case1,lam2_case1],[lam1_case2,lam2_case2],[lam1_case3,lam2_case3]]\nwhere each lam is a decimal with six digits after the decimal point. The outputs are pure numbers with no units.\n\nYour program must not accept any external input. All matrices are specified above, and the program must print the single required output line when run. The answers for each test case are floats. The entire output line must follow the exact format described, with no additional text.",
            "solution": "We begin from the definition of the Rayleigh quotient for a real, symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a nonzero vector $x \\in \\mathbb{R}^n$:\n$$\nR_A(x) = \\frac{x^\\top A x}{x^\\top x}.\n$$\nThe stationary points of $R_A(x)$ constrained to the unit sphere $S^{n-1}$ satisfy the Lagrange multiplier condition. Define the Lagrangian\n$$\n\\mathcal{L}(x,\\mu) = x^\\top A x - \\mu (x^\\top x - 1).\n$$\nSetting the gradient with respect to $x$ to zero yields\n$$\n\\nabla_x \\mathcal{L}(x,\\mu) = 2 A x - 2 \\mu x = 0 \\quad \\Rightarrow \\quad A x = \\mu x.\n$$\nThus, stationary points correspond to eigenvectors $x$ with eigenvalues $\\mu$, and the Rayleigh quotient at an eigenvector $v$ evaluates to the corresponding eigenvalue:\n$$\nR_A(v) = \\frac{v^\\top A v}{v^\\top v} = \\frac{v^\\top (\\lambda v)}{\\|v\\|_2^2} = \\lambda.\n$$\nTherefore, optimizing $R_A(x)$ over $S^{n-1}$ yields eigenpairs. Specifically, the global maximizer achieves the largest eigenvalue and the global minimizer achieves the smallest eigenvalue. Intermediate critical points correspond to other eigenpairs.\n\nTo compute multiple eigenpairs, a deflation step removes the contribution of an already found eigenpair. For a symmetric matrix $A$ and a found eigenpair $(\\lambda, v)$ with $\\|v\\|_2 = 1$, Hotelling’s deflation constructs a new matrix\n$$\nA_{\\text{new}} = A - \\lambda v v^\\top.\n$$\nThis transformation is symmetric and annihilates the component in the direction of $v$. If $A v = \\lambda v$, then $A_{\\text{new}} v = A v - \\lambda v (v^\\top v) = \\lambda v - \\lambda v = 0$. The remaining eigenpairs of $A$ that are orthogonal to $v$ are preserved in $A_{\\text{new}}$. Iterating this deflation allows recovery of additional eigenpairs.\n\nWe now discuss gradients. The gradient of $R_A(x)$ with respect to $x$ can be derived analytically, but the problem requires computing it through automatic differentiation. The Rayleigh quotient is a composition of bilinear forms and rational operations. For $x \\in \\mathbb{R}^n$, write $f(x) = x^\\top A x$ and $g(x) = x^\\top x$. Then $R_A(x) = f(x)/g(x)$. Automatic differentiation computes $\\nabla R_A(x)$ by applying the chain rule to the elementary operations that define $f$ and $g$. A forward-mode automatic differentiation can be implemented by augmenting scalar values with their directional derivatives and overloading arithmetic operations:\n- For Dual numbers $(u, \\dot{u})$ and $(v, \\dot{v})$ representing values and derivatives, addition and multiplication obey $(u, \\dot{u}) + (v, \\dot{v}) = (u + v, \\dot{u} + \\dot{v})$ and $(u, \\dot{u})(v, \\dot{v}) = (u v, u \\dot{v} + v \\dot{u})$.\n- Division obeys $(u, \\dot{u})/(v, \\dot{v}) = \\left(u/v, (\\dot{u} v - u \\dot{v})/v^2\\right)$.\n\nFor a vector $x = (x_1,\\dots,x_n)$ we seed the automatic differentiation with basis directions, so that the derivative part stores a full gradient vector. Constructing $x_i$ as a Dual number with derivative $\\mathbf{e}_i$ yields the gradient of a scalar output directly.\n\nTo optimize $R_A(x)$ on the unit sphere without parameterizing the sphere, a simple approach is to take gradient steps and renormalize the vector:\n1. Start with $x_0 = \\frac{1}{\\sqrt{n}} (1,\\dots,1)^\\top$ to ensure determinism and avoid zero initialization.\n2. At iteration $k$, compute $R_A(x_k)$ and its gradient $\\nabla R_A(x_k)$ using automatic differentiation.\n3. Project the gradient onto the tangent space of the sphere at $x_k$ by subtracting its radial component:\n$$\ng_k^{\\text{tan}} = \\nabla R_A(x_k) - (x_k^\\top \\nabla R_A(x_k)) x_k.\n$$\n4. Update by $x_{k+1/2} = x_k + s \\, g_k^{\\text{tan}}$ for maximization, with a fixed step size $s > 0$, then renormalize $x_{k+1} = x_{k+1/2} / \\|x_{k+1/2}\\|_2$.\n5. Terminate when $|R_A(x_{k+1}) - R_A(x_k)| < \\varepsilon$ or a fixed maximum number of iterations is reached. The limit point $x_\\star$ is an approximate eigenvector, and $\\lambda_\\star = R_A(x_\\star)$ is the corresponding eigenvalue.\n\nFor multiple eigenpairs, after computing $(\\lambda_1, v_1)$, form $A_1 = A - \\lambda_1 v_1 v_1^\\top$ and repeat the procedure on $A_1$ to find $(\\lambda_2, v_2)$, and so on. For real symmetric $A$ with distinct eigenvalues, the found eigenvectors are orthogonal due to the Rayleigh-Ritz characterization and the preservation of symmetry in the deflated operator. For repeated eigenvalues, the method identifies another vector in the eigenspace; orthonormality can be enforced by normalization and tangent projection.\n\nApplying this to the test matrices:\n- For $A_1 \\in \\mathbb{R}^{5 \\times 5}$, the discrete Laplacian has known eigenvalues $2 - 2 \\cos\\left(\\frac{k \\pi}{6}\\right)$ for $k=1,\\dots,5$, but our program will not use this closed form; it will compute via the gradient-based iteration.\n- For $A_2 \\in \\mathbb{R}^{3 \\times 3}$, there is a repeated largest eigenvalue, so the two largest eigenvalues are both equal, and deflation will recover another eigenvector in the same eigenspace.\n- For $A_3 \\in \\mathbb{R}^{3 \\times 3}$, the matrix is indefinite; the largest eigenvalue corresponds to the dominant direction in the $(x_1,x_2)$ plane, and deflation reveals the next largest eigenvalue, which, due to indefiniteness, is negative.\n\nThe program implements:\n- A minimal forward-mode automatic differentiation for scalar computations aggregating a full gradient vector with each scalar via Dual numbers.\n- Gradient-based maximization of $R_A(x)$ on the sphere using tangent projection and renormalization with fixed step size $s = 0.2$, tolerance $\\varepsilon = 10^{-10}$, and iteration cap $M = 20000$.\n- Hotelling’s deflation $A \\leftarrow A - \\lambda v v^\\top$ to extract two largest eigenpairs.\n\nFinally, for each test matrix, it outputs the two largest eigenvalues rounded to six decimal places, in the specified list-of-lists single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Minimal forward-mode automatic differentiation (Dual numbers)\nclass Dual:\n    __array_priority__ = 1000.0  # ensure our ops take precedence over numpy\n\n    def __init__(self, val: float, der: np.ndarray):\n        self.val = float(val)\n        # Ensure derivative is a 1D numpy array\n        self.der = np.array(der, dtype=float)\n\n    def _zeros_like(self):\n        return np.zeros_like(self.der)\n\n    # Addition\n    def __add__(self, other):\n        if isinstance(other, Dual):\n            return Dual(self.val + other.val, self.der + other.der)\n        else:\n            return Dual(self.val + float(other), self.der.copy())\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    # Subtraction\n    def __sub__(self, other):\n        if isinstance(other, Dual):\n            return Dual(self.val - other.val, self.der - other.der)\n        else:\n            return Dual(self.val - float(other), self.der.copy())\n\n    def __rsub__(self, other):\n        if isinstance(other, Dual):\n            return Dual(other.val - self.val, other.der - self.der)\n        else:\n            return Dual(float(other) - self.val, -self.der.copy())\n\n    # Multiplication\n    def __mul__(self, other):\n        if isinstance(other, Dual):\n            return Dual(self.val * other.val, self.val * other.der + other.val * self.der)\n        else:\n            otherf = float(other)\n            return Dual(self.val * otherf, self.der * otherf)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    # True division\n    def __truediv__(self, other):\n        if isinstance(other, Dual):\n            # (u/v)' = (u' v - u v') / v^2\n            denom = other.val * other.val\n            num_der = self.der * other.val - self.val * other.der\n            return Dual(self.val / other.val, num_der / denom)\n        else:\n            otherf = float(other)\n            return Dual(self.val / otherf, self.der / otherf)\n\n    def __rtruediv__(self, other):\n        # other / self\n        if isinstance(other, Dual):\n            return other.__truediv__(self)\n        else:\n            otherf = float(other)\n            # (c / u)' = -c u' / u^2\n            denom = self.val * self.val\n            return Dual(otherf / self.val, (-otherf * self.der) / denom)\n\n    # Negation\n    def __neg__(self):\n        return Dual(-self.val, -self.der)\n\ndef matvec_dual(A: np.ndarray, x_dual: list) -> list:\n    n, m = A.shape\n    assert m == len(x_dual)\n    res = []\n    zero_der = np.zeros_like(x_dual[0].der)\n    for i in range(n):\n        s = Dual(0.0, zero_der)\n        # accumulate A[i, j] * x_j\n        for j in range(m):\n            s = s + x_dual[j] * A[i, j]\n        res.append(s)\n    return res\n\ndef dot_dual(u_dual: list, v_dual: list) -> Dual:\n    assert len(u_dual) == len(v_dual)\n    zero_der = np.zeros_like(u_dual[0].der)\n    s = Dual(0.0, zero_der)\n    for i in range(len(u_dual)):\n        s = s + u_dual[i] * v_dual[i]\n    return s\n\ndef rayleigh_value_and_grad(x: np.ndarray, A: np.ndarray):\n    # Compute R_A(x) and its gradient via forward-mode AD\n    n = x.shape[0]\n    I = np.eye(n)\n    x_dual = [Dual(x[i], I[i]) for i in range(n)]\n    Ax_dual = matvec_dual(A, x_dual)\n    num = dot_dual(x_dual, Ax_dual)\n    den = dot_dual(x_dual, x_dual)\n    q = num / den\n    return q.val, q.der\n\ndef extreme_eigenpair(A: np.ndarray, mode: str = 'max', step: float = 0.2,\n                      tol: float = 1e-10, maxiter: int = 20000):\n    \"\"\"\n    Find an extreme eigenpair by maximizing ('max') or minimizing ('min') the Rayleigh quotient\n    on the unit sphere using gradient steps computed via AD and tangent projection.\n    \"\"\"\n    n = A.shape[0]\n    x = np.ones(n, dtype=float)\n    x /= np.linalg.norm(x)\n    prev_val = None\n\n    for _ in range(maxiter):\n        val, grad = rayleigh_value_and_grad(x, A)\n        # Project gradient onto tangent space of the sphere at x\n        grad_tan = grad - (x @ grad) * x\n\n        if mode == 'max':\n            x_new = x + step * grad_tan\n        else:\n            x_new = x - step * grad_tan\n\n        # Renormalize to maintain ||x|| = 1\n        norm = np.linalg.norm(x_new)\n        if norm == 0.0 or not np.isfinite(norm):\n            # reset to initial direction if degenerate\n            x_new = np.ones(n, dtype=float)\n            x_new /= np.linalg.norm(x_new)\n        else:\n            x_new /= norm\n\n        new_val, _ = rayleigh_value_and_grad(x_new, A)\n\n        # Check convergence in Rayleigh quotient value\n        if prev_val is not None and abs(new_val - prev_val) < tol:\n            x = x_new\n            val = new_val\n            break\n\n        x = x_new\n        prev_val = new_val\n\n    # Final value and normalized eigenvector\n    val, _ = rayleigh_value_and_grad(x, A)\n    # Ensure unit norm\n    x = x / np.linalg.norm(x)\n    return float(val), x\n\ndef deflate(A: np.ndarray, lam: float, v: np.ndarray) -> np.ndarray:\n    # Hotelling's deflation: A_new = A - lam * v v^T\n    return A - lam * np.outer(v, v)\n\ndef top_k_eigenvalues_with_deflation(A: np.ndarray, k: int = 2) -> list:\n    A_curr = A.copy().astype(float)\n    vals = []\n    vecs = []\n    for _ in range(k):\n        lam, v = extreme_eigenpair(A_curr, mode='max', step=0.2, tol=1e-10, maxiter=20000)\n        # Normalize v for safety\n        v = v / np.linalg.norm(v)\n        vals.append(lam)\n        vecs.append(v)\n        A_curr = deflate(A_curr, lam, v)\n    # Round to six decimals as required\n    return [round(val, 6) for val in vals]\n\ndef format_output(list_of_lists: list) -> str:\n    # Format as [[a,b],[c,d],[e,f]] with six decimals and no spaces\n    inner_strs = []\n    for sub in list_of_lists:\n        sub_str = \",\".join(f\"{v:.6f}\" for v in sub)\n        inner_strs.append(f\"[{sub_str}]\")\n    return f\"[{','.join(inner_strs)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A1 = np.array([\n        [2, -1, 0, 0, 0],\n        [-1, 2, -1, 0, 0],\n        [0, -1, 2, -1, 0],\n        [0, 0, -1, 2, -1],\n        [0, 0, 0, -1, 2]\n    ], dtype=float)\n\n    A2 = np.array([\n        [2, 1, 0],\n        [1, 2, 0],\n        [0, 0, 3]\n    ], dtype=float)\n\n    A3 = np.array([\n        [0, 2, 0],\n        [2, 0, 0],\n        [0, 0, -1]\n    ], dtype=float)\n\n    test_matrices = [A1, A2, A3]\n\n    results = []\n    for A in test_matrices:\n        vals = top_k_eigenvalues_with_deflation(A, k=2)\n        results.append(vals)\n\n    # Final print statement in the exact required format.\n    print(format_output(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}