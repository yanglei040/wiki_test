{
    "hands_on_practices": [
        {
            "introduction": "在理想化的优化过程中，梯度下降可能会陷入局部最小值，就像一个滚下山坡的小球卡在小坑里一样。本练习通过引入一个源自物理学的概念——热噪声——来探索如何帮助我们的模型“跳出”这些陷阱。我们将模拟随机梯度下降中的噪声如何扮演类似于模拟退火中温度的角色，从而更有效地探索复杂的损失函数景观 。",
            "id": "2373958",
            "problem": "您将实现并分析一个计算实验，该实验将通过反向传播进行的人工神经网络训练与热噪声辅助的局部最小值逃逸联系起来，并与模拟退火进行类比。考虑一个具有单个参数 $w \\in \\mathbb{R}$ 的标量人工神经网络，对于单个固定输入，其输出为 $y(w) = w$。通过以下公式定义一个非凸训练目标（解释为能量景观）：\n$$\nU(w) = (w^2 - 1)^2 + \\alpha\\, w + \\beta \\cos(k\\, w),\n$$\n其中 $\\alpha$、$\\beta$ 和 $k$ 是固定的已知常数。假设该标量网络通过最小化 $U(w)$ 进行训练，其训练动力学由关于 $w$ 的梯度 $\\frac{dU}{dw}$ 驱动。您将把热噪声引入梯度信号中以模拟热涨落，这与过阻尼朗之万动力学和模拟退火直接类似。\n\n您必须使用的基本原理：\n- 基于梯度的标量参数训练的定义：标量参数的反向传播梯度为 $\\frac{dU}{dw}$。\n- 在温度为 $T$ 的能量景观 $U(w)$ 中，坐标 $w$ 的过阻尼朗之万动力学具有连续时间形式 $dw/dt = -\\mu \\frac{dU}{dw} + \\sqrt{2 D}\\,\\xi(t)$，其中迁移率 $\\mu$ 和扩散常数 $D$ 通过爱因斯坦关系式 $D = \\mu k_{\\mathrm{B}} T$ 相关联，$k_{\\mathrm{B}}$ 是玻尔兹曼常数，$\\xi(t)$ 是标准白噪声。在离散时间中，步长为 $\\Delta t$，并将学习率等同于 $\\eta = \\mu \\Delta t$，这将产生一个带有热缩放高斯噪声项的时间离散化更新。\n\n您的任务：\n1. 从上述基础出发，推导出从 $w_t$ 到 $w_{t+1}$ 的离散时间更新，该更新模拟了在（可能随时间变化的）温度 $T(t)$ 下带有附加热噪声项的类梯度下降反向传播。使用一个恒定的学习率 $\\eta > 0$。清晰地论证高斯噪声项的精确方差与 $\\eta$ 和 $T(t)$ 的关系。\n2. 使用您推导出的更新规则，实现一个模拟，该模拟将参数从 $w_0 = 0.95$ 开始，并执行固定数量的更新步骤 $N_{\\mathrm{steps}}$。将最终落在 $w \\approx -1$ 附近的更深的全局势阱中定义为一次成功的逃逸事件，您必须将其操作化为条件 $w_{N_{\\mathrm{steps}}} \\le -0.9$。\n3. 对于每个实验，使用不同的随机种子运行 $N_{\\mathrm{trials}}$ 次独立试验，并报告成功逃逸的比例，结果为一个在 $[0,1]$ 区间内的浮点数。每次试验必须使用相同的初始条件 $w_0 = 0.95$ 和相同的温度调度 $T(t)$，但高斯热噪声的实现是独立的。使用以下固定常数：$\\alpha = 0.3$，$\\beta = 0.1$，$k = 5.0$，$\\eta = 0.01$，$N_{\\mathrm{steps}} = 4000$，$N_{\\mathrm{trials}} = 64$。\n4. 考虑以下模拟不同训练方案的温度调度 $T(t)$。对于每个调度，计算并报告所描述的逃逸比例。\n   - 情况 A（无热噪声）：对于所有 $t$，$T(t) = 0$。\n   - 情况 B（恒定热浴）：对于所有 $t$，$T(t) = 0.02$。\n   - 情况 C（指数模拟退火，慢速）：$T(t) = T_0 \\exp(-t/\\tau)$，其中 $T_0 = 0.05$ 且 $\\tau = 1500$。\n   - 情况 D（指数模拟退火，高温且快速）：$T(t) = T_0 \\exp(-t/\\tau)$，其中 $T_0 = 0.15$ 且 $\\tau = 300$。\n5. 您的程序必须按此顺序计算情况 A–D 的逃逸比例，并将它们输出到单行中，形式为用方括号括起来的逗号分隔列表。每个值必须四舍五入到恰好三位小数。例如，四个情况的输出应如下所示：[0.000,0.312,0.516,0.203]。\n\n附加说明：\n- 三角函数中的角度是无量纲的弧度。\n- 输出中不需要任何物理单位；报告纯数字。\n- 随机数生成必须是可复现的：选择一个主种子，然后为每个独立试验派生出不同的种子。\n- 根据给定的 $U(w)$ 精确实现梯度 $\\frac{dU}{dw}$。不要数值近似导数。\n- 您的代码必须是自包含的，并且不能读取任何输入。\n\n测试套件规范：\n- 使用上面列出的确切常数和调度。这些构成了您的程序要运行的测试用例。\n- 输出格式要求：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，并四舍五入到三位小数，顺序为 [情况A,情况B,情况C,情况D]。\n- 四个数字中的每一个都必须是浮点类型，并且必须在 $[0,1]$ 区间内。",
            "solution": "该问题要求推导一个参数 $w$ 经历类似于过阻尼朗之万动力学训练的离散时间更新规则，然后使用此规则模拟从能量景观 $U(w)$ 的局部最小值逃逸的过程。\n\n**第1部分：离散时间更新规则的推导**\n\n我们从势能 $U(w)$ 中参数 $w$ 在温度 $T$ 下的连续时间过阻尼朗之万方程开始：\n$$\n\\frac{dw}{dt} = -\\mu \\frac{dU}{dw} + \\sqrt{2 D}\\,\\xi(t)\n$$\n这里，$\\mu$ 是迁移率，$D$ 是扩散常数，$\\xi(t)$ 是标准高斯白噪声，满足 $\\langle \\xi(t) \\rangle = 0$ 和 $\\langle \\xi(t)\\xi(t') \\rangle = \\delta(t-t')$。迁移率和扩散通过爱因斯坦关系式关联，我们将其取为 $D = \\mu T$，这是通过将玻尔兹曼常数 $k_B$ 吸收到温度 $T$ 的定义中（将 $T$ 视为具有能量单位）。\n\n为了获得离散时间更新规则，我们采用欧拉-丸山方法来对这个随机微分方程在一个小的时间步长 $\\Delta t$ 上进行积分。从时间 $t$ 到 $t+\\Delta t$，$w$ 的变化近似为：\n$$\nw_{t+1} - w_t = \\Delta w \\approx \\int_{t}^{t+\\Delta t} \\left( -\\mu \\frac{dU}{dw}\\bigg|_{w_s} + \\sqrt{2 D}\\,\\xi(s) \\right) ds\n$$\n假设 $\\Delta t$ 很小，梯度项可以被视为在该区间内是恒定的，并在 $w_t = w(t)$ 处求值。噪声项的积分是一个维纳过程增量，$\\Delta W_t = \\int_{t}^{t+\\Delta t} \\xi(s) ds$。这个增量是一个高斯随机变量，其均值为 $0$，方差为 $\\Delta t$。我们可以写成 $\\Delta W_t = \\sqrt{\\Delta t} Z_t$，其中 $Z_t$ 是从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取的随机变量。\n\n因此，离散化的更新方程变为：\n$$\nw_{t+1} = w_t - \\mu \\Delta t \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 D} \\sqrt{\\Delta t} Z_t\n$$\n问题陈述通过 $\\eta = \\mu \\Delta t$ 提供了学习率 $\\eta$ 和物理参数之间的类比。将此关系以及 $D = \\mu T$ 代入方程中得到：\n$$\nw_{t+1} = w_t - \\eta \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 (\\mu T)} \\sqrt{\\Delta t} Z_t\n$$\n为了纯粹用 $\\eta$ 和 $T$ 来表示噪声项，我们重新整理 $\\eta = \\mu \\Delta t$ 得到 $\\mu = \\eta / \\Delta t$。将其代入平方根中：\n$$\n\\sqrt{2 (\\mu T) \\Delta t} = \\sqrt{2 \\left(\\frac{\\eta}{\\Delta t} T\\right) \\Delta t} = \\sqrt{2 \\eta T}\n$$\n因此，$w_t$ 的最终离散时间更新规则是：\n$$\nw_{t+1} = w_t - \\eta \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 \\eta T(t)} Z_t\n$$\n其中 $Z_t \\sim \\mathcal{N}(0, 1)$，温度 $T(t)$ 可以是随时间变化的。第一项 $-\\eta \\frac{dU}{dw}$ 对应于机器学习中的标准梯度下降更新。第二项 $\\sqrt{2 \\eta T(t)} Z_t$ 是附加的热噪声。该项是一个高斯随机变量，均值为 $0$，方差为 $(\\sqrt{2\\eta T(t)})^2 = 2\\eta T(t)$。\n\n**第2部分：梯度计算**\n\n能量景观（或训练目标）由以下公式给出：\n$$\nU(w) = (w^2 - 1)^2 + \\alpha w + \\beta \\cos(k w)\n$$\n为了实现更新规则，我们必须计算 $U(w)$ 关于 $w$ 的解析梯度：\n$$\n\\frac{dU}{dw} = \\frac{d}{dw} \\left( w^4 - 2w^2 + 1 + \\alpha w + \\beta \\cos(k w) \\right)\n$$\n应用微分法则，我们得到：\n$$\n\\frac{dU}{dw} = 4w^3 - 4w + \\alpha - \\beta k \\sin(k w)\n$$\n在模拟中，将使用给定的常数 $\\alpha=0.3$、$\\beta=0.1$ 和 $k=5.0$ 直接使用此表达式。\n\n**第3部分：模拟过程**\n\n该模拟旨在计算参数 $w$ 从 $w \\approx 1$ 附近的局部最小值开始，逃逸到 $w \\approx -1$ 附近的全局最小值盆地的次数比例。\n\n对于四种指定的温度调度 $T(t)$ 中的每一种，我们执行以下过程：\n1.  将成功逃逸的计数器初始化为 $0$。\n2.  运行 $N_{\\mathrm{trials}} = 64$ 次独立试验。对于每次试验：\n    a. 将参数 $w$ 初始化为其起始值 $w_0 = 0.95$。\n    b. 为热噪声生成一个独立的随机数序列。这是通过为每次试验使用从主生成器派生的唯一种子来为随机数生成器播种，以确保试验之间和试验内部的可复现性与独立性。\n    c. 执行 $N_{\\mathrm{steps}} = 4000$ 次更新步骤。对于从 $0$ 到 $N_{\\mathrm{steps}}-1$ 的每个步骤 $t$：\n        i.  根据给定的调度计算温度 $T_t = T(t)$。\n        ii. 使用上面推导的公式计算梯度 $g_t = \\frac{dU}{dw}\\big|_{w_t}$。\n        iii. 计算噪声项的标准差 $\\sigma_t = \\sqrt{2 \\eta T_t}$。如果 $T_t = 0$，则 $\\sigma_t=0$。\n        iv. 从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取一个随机样本 $Z_t$。\n        v.  更新参数：$w_{t+1} = w_t - \\eta g_t + \\sigma_t Z_t$。\n    d. 在 $N_{\\mathrm{steps}}$ 步之后，检查参数的最终状态 $w_{N_{\\mathrm{steps}}}$。如果 $w_{N_{\\mathrm{steps}}} \\le -0.9$，则该试验被计为一次成功的逃逸。\n3.  该温度调度的最终逃逸比例是成功逃逸的总次数除以 $N_{\\mathrm{trials}}$。\n\n对所有四种情况重复此过程：\n- 情况 A（无噪声）：$T(t) = 0$。\n- 情况 B（恒定温度）：$T(t) = 0.02$。\n- 情况 C（慢速退火）：$T(t) = 0.05 \\exp(-t/1500)$。\n- 情况 D（快速退火）：$T(t) = 0.15 \\exp(-t/300)$。\n\n实现将直接遵循此逻辑，使用给定的常数：$\\alpha = 0.3$，$\\beta = 0.1$，$k = 5.0$ 和 $\\eta = 0.01$。最终结果四舍五入到三位小数，并按指定格式呈现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from the problem statement\nALPHA = 0.3\nBETA = 0.1\nK = 5.0\nETA = 0.01\nW0 = 0.95\nN_STEPS = 4000\nN_TRIALS = 64\nSUCCESS_THRESHOLD = -0.9\nMASTER_SEED = 42 # For reproducible results\n\ndef gradient_U(w):\n    \"\"\"\n    Computes the analytical gradient of the potential U(w).\n    dU/dw = 4*w^3 - 4*w + alpha - beta*k*sin(k*w)\n    \"\"\"\n    return 4 * w**3 - 4 * w + ALPHA - BETA * K * np.sin(K * w)\n\ndef calculate_escape_fraction(T_func, master_rng):\n    \"\"\"\n    Runs N_trials simulations for a given temperature schedule T_func.\n    Returns the fraction of successful escapes.\n    \"\"\"\n    success_count = 0\n    for i in range(N_TRIALS):\n        # Derive a distinct seed for each trial from the master RNG for independence\n        trial_seed = master_rng.integers(2**32)\n        trial_rng = np.random.default_rng(seed=trial_seed)\n\n        w = W0\n        for t in range(N_STEPS):\n            # Calculate gradient\n            grad = gradient_U(w)\n\n            # Calculate temperature and noise term\n            temp = T_func(t)\n            noise = 0.0\n            if temp > 0:\n                # Variance of noise is 2*eta*T, so std_dev is sqrt(2*eta*T)\n                std_dev = np.sqrt(2 * ETA * temp)\n                noise = trial_rng.normal(0.0, std_dev)\n\n            # Update w using the derived discrete-time Langevin equation\n            # w_{t+1} = w_t - eta * dU/dw + noise\n            w = w - ETA * grad + noise\n        \n        # Check for successful escape\n        if w = SUCCESS_THRESHOLD:\n            success_count += 1\n\n    return success_count / N_TRIALS\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run simulations, and print results.\n    \"\"\"\n    # Master RNG for generating trial seeds, ensuring independence between cases A, B, C, D\n    master_rng = np.random.default_rng(seed=MASTER_SEED)\n\n    # Define the four temperature schedules (test cases)\n    # Case A: No thermal noise\n    T_A = lambda t: 0.0\n    # Case B: Constant thermal bath\n    T_B = lambda t: 0.02\n    # Case C: Exponential simulated annealing, slow\n    T0_C, TAU_C = 0.05, 1500.0\n    T_C = lambda t: T0_C * np.exp(-t / TAU_C)\n    # Case D: Exponential simulated annealing, hot and fast\n    T0_D, TAU_D = 0.15, 300.0\n    T_D = lambda t: T0_D * np.exp(-t / TAU_D)\n\n    schedules = [T_A, T_B, T_C, T_D]\n\n    results = []\n    for schedule in schedules:\n        fraction = calculate_escape_fraction(schedule, master_rng)\n        results.append(fraction)\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n\n```"
        },
        {
            "introduction": "虽然理论模型通常假设无限的计算精度，但实际的计算机使用有限的位数来表示数字，这会对反向传播等算法产生深远影响。本练习将通过量化梯度，系统性地降低其数值精度，来研究学习过程的鲁棒性 。你将亲手发现学习能力在何处“崩溃”，这个临界点类似于物理学中的相变，揭示了算法对计算硬件限制的敏感性。",
            "id": "2373947",
            "problem": "构建一个量化实验，通过训练一个具有量化梯度的单一线性神经元，来研究有限精度反向传播下的学习过程。考虑一个数据集，其输入矩阵为 $X \\in \\mathbb{R}^{N \\times d}$，输出向量为 $y \\in \\mathbb{R}^{N}$，其中 $N=d$ 且 $X$ 是单位矩阵 $I_{d}$。设目标参数向量为 $w^{\\star} \\in \\mathbb{R}^{d}$，其分量为 $w^{\\star} = [1,-2,3,-4,5]^{\\top}$，初始参数向量为 $w_{0}=\\mathbf{0} \\in \\mathbb{R}^{d}$。定义经验均方误差损失函数\n$$\nL(w) = \\frac{1}{N}\\,\\lVert Xw - y \\rVert_{2}^{2}.\n$$\n使用固定的学习率 $\\eta$ 和量化梯度 $Q_{b}(\\nabla L(w))$ 执行全批量梯度下降。该量化梯度精确地具有 $b$ 个有符号位的精度（包括符号位），并逐分量应用。设 $N=5$, $d=5$, $X = I_{5}$, $y = w^{\\star}$, $w_{0}=\\mathbf{0}$, 且 $\\eta=1$。精确训练 $T$ 次迭代，其中 $T=2000$。\n\n量化器定义如下。计算初始的全精度梯度 $g_{0} = \\nabla L(w_{0})$，并设置固定的动态范围 $s_{\\mathrm{fixed}} = \\max_{j} |(g_{0})_{j}|$。对于 $b \\ge 2$，定义均匀量化步长\n$$\n\\Delta_{b} = \\frac{s_{\\mathrm{fixed}}}{2^{\\,b-1}-1},\n$$\n并应用逐分量的向零舍入量化\n$$\n\\left[Q_{b}(g)\\right]_{j} = \\operatorname{sign}(g_{j}) \\cdot \\Delta_{b}\\,\\left\\lfloor \\frac{|g_{j}|}{\\Delta_{b}} \\right\\rfloor,\n$$\n约定 $\\operatorname{sign}(0)=0$。对于边界情况 $b=1$，使用一位符号量化\n$$\n\\left[Q_{1}(g)\\right]_{j} = s_{\\mathrm{fixed}} \\cdot \\operatorname{sign}(g_{j}),\n$$\n其中 $\\operatorname{sign}(0)=0$。训练更新规则为\n$$\nw_{t+1} = w_{t} - \\eta \\, Q_{b}\\big(\\nabla L(w_{t})\\big),\n$$\n应用于 $t=0,1,\\dots,T-1$。\n\n将学习崩溃准则定义为在第 $T$ 次迭代时未能达到目标损失阈值 $\\tau$，即如果 $L(w_{T})  \\tau$ 则发生崩溃。使用固定阈值 $\\tau = 0.002$。\n\n测试套件和要求的输出：\n- 使用位精度测试用例 $b \\in \\{32,16,8,7,5,3,2,1\\}$。\n- 对于此集合中的每个 $b$，运行上述训练并记录一个指示是否崩溃的布尔值：如果 $L(w_{T})  \\tau$ 则输出 $\\mathrm{True}$，否则输出 $\\mathrm{False}$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[False,False,False,True,True,True,True,True]”），其顺序与测试用例 $[32,16,8,7,5,3,2,1]$ 的顺序完全一致。\n\n所有量都是无单位的实数。不使用角度。要求的最终输出格式是包含指定布尔值列表的单行，无任何附加文本。",
            "solution": "该问题陈述具有科学依据，表述清晰且客观。它提出了一个在优化算法中研究数值精度的清晰、可验证的计算实验。所有参数和函数都有明确定义，其前提与数学和机器学习原理一致。该问题是有效的。\n\n任务是模拟使用全批量梯度下降法训练一个单一线性神经元的过程，其中梯度被量化。我们将分析不同位精度下的行为，并确定训练过程何时会崩溃，崩溃定义为未能达到特定的损失阈值。\n\n首先，我们构建损失函数及其梯度。该模型是一个线性神经元，对于输入矩阵 $X$ 和权重向量 $w$，其输出为 $Xw$。损失是经验均方误差：\n$$\nL(w) = \\frac{1}{N} \\lVert Xw - y \\rVert_2^2\n$$\n问题指定 $N=5$，$d=5$，$X = I_5$（$5 \\times 5$ 的单位矩阵），并且目标输出 $y$ 被设定为等于目标权重 $w^{\\star}$。因此，损失函数简化为：\n$$\nL(w) = \\frac{1}{5} \\lVert I_5 w - w^\\star \\rVert_2^2 = \\frac{1}{5} \\lVert w - w^\\star \\rVert_2^2\n$$\n该损失的全局最小值为 $L=0$，在 $w = w^\\star$ 处取得。损失函数关于 $w$ 的梯度是：\n$$\n\\nabla L(w) = \\frac{2}{N} X^{\\top}(Xw - y) = \\frac{2}{5} I_5^{\\top}(I_5 w - w^\\star) = \\frac{2}{5} (w - w^\\star)\n$$\n\n训练过程从初始参数向量 $w_0 = \\mathbf{0}$ 开始。因此，初始梯度为：\n$$\ng_0 = \\nabla L(w_0) = \\frac{2}{5} (w_0 - w^\\star) = -\\frac{2}{5} w^\\star\n$$\n给定 $w^{\\star} = [1, -2, 3, -4, 5]^{\\top}$，我们计算 $g_0$：\n$$\ng_0 = -\\frac{2}{5} [1, -2, 3, -4, 5]^{\\top} = [-0.4, 0.8, -1.2, 1.6, -2.0]^{\\top}\n$$\n量化方案使用一个固定的动态范围 $s_{\\mathrm{fixed}}$，该范围由初始梯度分量的最大绝对值确定：\n$$\ns_{\\mathrm{fixed}} = \\max_{j} |(g_0)_j| = \\max\\{0.4, 0.8, 1.2, 1.6, 2.0\\} = 2.0\n$$\n\n训练使用以下更新规则进行 $T=2000$ 次迭代：\n$$\nw_{t+1} = w_t - \\eta \\, Q_b(\\nabla L(w_t))\n$$\n其中学习率为 $\\eta=1$，$Q_b$ 是用于 $b$ 位精度的量化算子。\n\n对于 $b \\ge 2$，均匀量化步长 $\\Delta_b$ 定义为：\n$$\n\\Delta_b = \\frac{s_{\\mathrm{fixed}}}{2^{b-1}-1} = \\frac{2.0}{2^{b-1}-1}\n$$\n逐分量的量化函数是：\n$$\n[Q_b(g)]_j = \\operatorname{sign}(g_j) \\cdot \\Delta_b \\cdot \\left\\lfloor \\frac{|g_j|}{\\Delta_b} \\right\\rfloor\n$$\n该函数将任何梯度分量 $g_j$ 映射到离零更近的 $\\Delta_b$ 的最近倍数。在零附近存在一个“死区”：如果 $|g_j|  \\Delta_b$，则 $[Q_b(g)]_j = 0$。\n\n对于边界情况 $b=1$，量化方式为：\n$$\n[Q_1(g)]_j = s_{\\mathrm{fixed}} \\cdot \\operatorname{sign}(g_j) = 2.0 \\cdot \\operatorname{sign}(g_j)\n$$\n这将任何非零梯度分量映射到 $+2.0$ 或 $-2.0$。\n\n将对每个位精度 $b \\in \\{32, 16, 8, 7, 5, 3, 2, 1\\}$ 运行模拟。在 $T=2000$ 次迭代后，我们计算最终损失 $L(w_T)$ 并检查是否发生崩溃，即 $L(w_T)  \\tau=0.002$。\n\n导致崩溃的一个关键原因是量化死区。如果初始梯度分量的绝对值 $|(g_0)_j|$ 小于量化步长 $\\Delta_b$，该分量从一开始就会被量化为零。相应的权重 $w_j$ 将永远不会从其初始值 $0$ 更新。$g_0$ 中分量的最小绝对值为 $|(g_0)_0| = 0.4$。如果 $\\Delta_b  0.4$，则必然发生崩溃：\n$$\n\\frac{2.0}{2^{b-1}-1}  0.4 \\implies 5  2^{b-1}-1 \\implies 6  2^{b-1} \\implies b-1  \\log_2(6) \\approx 2.585 \\implies b  3.585\n$$\n因此，对于 $b=3, 2, 1$，我们预计会发生崩溃，因为至少有一个权重分量将无法学习。\n\n对于更高的 $b$ 值，当 $\\Delta_b$ 小到足以让所有分量开始学习时，量化误差仍然会限制最终的精度。当每个分量的梯度大小 $|\\nabla L(w_t)_j|$ 降到 $\\Delta_b$ 以下时，训练实际上会停滞。这会在权重中留下一个残余误差 $w_T - w^\\star$，其中每个分量大约受 $\\frac{N}{2}\\Delta_b$ 的限制。最终损失大致与 $\\Delta_b^2$ 成正比。如果这个可达到的最小损失大于阈值 $\\tau$，就会发生崩溃。我们预计这会发生在像 $7$ 和 $5$ 这样的中等 $b$ 值上。\n\n对于大的 $b$ 值（例如 $32, 16$），$\\Delta_b$ 非常小，量化误差可以忽略不计，算法收敛得很好，最终损失低于 $\\tau$。所提供的程序精确地实现了这个模拟，以找到崩溃点。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates training a single linear neuron with quantized gradients\n    to study learning under finite precision.\n    \"\"\"\n    # Define problem parameters\n    N = 5.0\n    d = 5\n    w_star = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    w0 = np.zeros(d)\n    eta = 1.0\n    T = 2000\n    tau = 0.002\n\n    # Define the bit-precision test cases\n    b_cases = [32, 16, 8, 7, 5, 3, 2, 1]\n    \n    results = []\n\n    # Calculate the fixed dynamic range s_fixed from the initial gradient.\n    # This is constant for all test cases.\n    g0 = (2.0 / N) * (w0 - w_star)\n    s_fixed = np.max(np.abs(g0))\n\n    # Iterate over each bit-precision case\n    for b in b_cases:\n        # Initialize weights for the current simulation\n        w = w0.copy()\n\n        # Define the quantizer for the current bit precision b\n        def quantize_gradient(g: np.ndarray, b_val: int) - np.ndarray:\n            \"\"\"\n            Applies component-wise quantization to the gradient vector.\n            \"\"\"\n            # Case b=1: Sign quantization\n            if b_val == 1:\n                return s_fixed * np.sign(g)\n\n            # Case b = 2: Uniform quantization\n            # The denominator 2**(b-1) - 1 is greater than 0 for b = 2.\n            denominator = (2**(b_val - 1)) - 1\n            \n            # For very large b like 32, delta_b is very small but non-zero.\n            # If hypothetically b was so large that denominator overflows,\n            # delta_b would tend to 0, meaning no quantization.\n            if denominator == 0:  # Should not happen for b=2\n                return g\n            \n            delta_b = s_fixed / denominator\n\n            # The quantizer function is Q_b(g)_j = sign(g_j) * delta_b * floor(|g_j|/delta_b)\n            # This is equivalent to rounding towards zero (truncation).\n            sign_g = np.sign(g)\n            abs_g = np.abs(g)\n            \n            # np.floor implements the rounding-to-zero logic when combined with sign.\n            quantized_magnitude = delta_b * np.floor(abs_g / delta_b)\n            \n            return sign_g * quantized_magnitude\n\n        # Main training loop\n        for _ in range(T):\n            # 1. Calculate the full-precision gradient\n            gradient = (2.0 / N) * (w - w_star)\n            \n            # 2. Quantize the gradient\n            quantized_grad = quantize_gradient(gradient, b)\n            \n            # 3. Update the weights\n            w = w - eta * quantized_grad\n            \n        # After training, calculate the final loss\n        # L(w) = (1/N) * ||w - w_star||^2\n        final_loss = (1.0 / N) * np.linalg.norm(w - w_star)**2\n        \n        # Determine if a breakdown occurred based on the threshold tau\n        breakdown = final_loss  tau\n        results.append(breakdown)\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a Python list of booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的梯度下降在参数空间中沿着最陡的“欧几里得”路径下降，但这不一定是信息论意义上最有效的路径。本练习将带你超越传统的优化范式，在由费雪信息度量（Fisher Information Metric）定义的黎曼流形上进行梯度下降 。通过推导和实现自然梯度下降，你将理解如何根据模型本身的统计结构来调整更新方向，从而实现更高效、更稳健的学习。",
            "id": "2373930",
            "problem": "实现、分析和比较在小型确定性数据集上训练单层人工神经网络（ANN）分类器（逻辑回归）的两种一阶优化方法：(i) 标准欧几里得梯度下降法和 (ii) 遵循由费雪信息度量（FIM）流形导出的测地线方向的黎曼最速下降法。您的推导和实现必须从第一性原理出发：模型的定义、负对数似然、Kullback–Leibler散度（KL散度）以及费雪信息。不得假设任何预先推导出的更新规则。您必须：(a) 从这些定义中推导出适当的黎曼梯度方向，(b) 实现这两种方法，以及 (c) 在提供的测试集上评估它们。本问题不涉及物理单位。所有角度（如有）均应视为无量纲实数。本问题中的所有常数均为实数。\n\n模型与数据。考虑一个二元分类器，其输入为 $x \\in \\mathbb{R}^d$，二元标签为 $y \\in \\{0,1\\}$，参数向量为 $\\theta \\in \\mathbb{R}^d$。该模型为逻辑回归ANN，由下式给出\n$$\np_\\theta(y=1 \\mid x) = \\sigma(\\theta^\\top x), \\quad \\text{其中 } \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\n在数据集 $\\{(x_i,y_i)\\}_{i=1}^N$ 上的平均负对数似然为\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^N \\left[ y_i \\log p_\\theta(y_i=1 \\mid x_i) + (1-y_i)\\log\\left(1 - p_\\theta(y_i=1 \\mid x_i)\\right) \\right].\n$$\n\n基本定义。在推导中仅使用以下内容作为起点：\n- 两个分布 $P$ 和 $Q$ 之间的Kullback–Leibler散度（KL散度）的定义，\n$$\n\\mathrm{KL}(P \\,\\|\\, Q) = \\mathbb{E}_P\\!\\left[ \\log \\frac{dP}{dQ} \\right].\n$$\n- 参数 $\\theta$ 处的费雪信息度量（FIM）张量，\n$$\nF(\\theta) = \\mathbb{E}\\!\\left[ \\nabla_\\theta \\log p_\\theta(Y \\mid X) \\, \\nabla_\\theta \\log p_\\theta(Y \\mid X)^\\top \\right],\n$$\n其中期望是针对 $(X,Y)$ 的数据生成过程计算的。\n\n任务。\n1. 根据上述定义和Kullback–Leibler散度的局部二阶展开，推导在参数 $\\theta$ 处、由费雪信息度量（FIM）导出的用于最小化 $\\mathcal{L}(\\theta)$ 的黎曼最速下降方向。用 $\\nabla_\\theta \\mathcal{L}(\\theta)$ 和 $F(\\theta)$ 表示最终的更新，且不得假设任何先前的简化公式。\n2. 将上述结果应用于逻辑回归模型，得到关于数据集 $\\{(x_i,y_i)\\}$ 和 $\\theta$ 的平均欧几里得梯度 $\\nabla_\\theta \\mathcal{L}(\\theta)$ 和费雪信息矩阵 $F(\\theta)$ 的显式表达式。\n3. 实现两种训练过程，每种都从初始向量 $\\theta_0$ 开始执行 $T$ 步：\n   - 步长为 $\\alpha_{\\mathrm{E}}$ 的欧几里得梯度下降法。\n   - 使用步长 $\\alpha_{\\mathrm{N}}$ 和应用于度量的吉洪诺夫阻尼参数 $\\lambda  0$ 的费雪-黎曼最速下降法，这意味着步长中使用的度量张量为 $F(\\theta) + \\lambda I$，其中 $I$ 是单位矩阵。\n4. 为保证实现的数值稳定性，您必须确保 $\\mathcal{L}(\\theta)$ 中 sigmoid 函数和对数函数的计算是稳定的，并且必须对度量系统使用线性求解器或数值稳定的逆矩阵计算方法。\n5. 对于下面的每个测试用例，返回两种方法训练后的最终平均负对数似然。您的程序必须输出单行内容，即一个包含6个浮点数的列表，四舍五入到六位小数，顺序为 $[\\mathcal{L}^{(1)}_{\\mathrm{E,final}}, \\mathcal{L}^{(1)}_{\\mathrm{N,final}}, \\mathcal{L}^{(2)}_{\\mathrm{E,final}}, \\mathcal{L}^{(2)}_{\\mathrm{N,final}}, \\mathcal{L}^{(3)}_{\\mathrm{E,final}}, \\mathcal{L}^{(3)}_{\\mathrm{N,final}}]$。\n\n测试集。请严格使用以下数据集、初始化和超参数。在所有情况下，使用 $d=2$。对于每个测试，使用 $T$ 步、指定的学习率和指定的阻尼因子 $\\lambda$。\n- 测试 $1$ (平衡，中等尺度):\n$$\nX^{(1)} = \\begin{bmatrix}\n2  1 \\\\\n2  -1 \\\\\n-2  1 \\\\\n-2  -1 \\\\\n1  2 \\\\\n-1  -2\n\\end{bmatrix}, \\quad\ny^{(1)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\quad\n\\theta^{(1)}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad\nT^{(1)} = 100, \\quad\n\\alpha^{(1)}_{\\mathrm{E}} = 0.1, \\quad\n\\alpha^{(1)}_{\\mathrm{N}} = 0.1, \\quad\n\\lambda^{(1)} = 10^{-2}.\n$$\n- 测试 $2$ (共线性特征；度量近乎奇异，需要阻尼):\n$$\nX^{(2)} = \\begin{bmatrix}\n1  2 \\\\\n2  4 \\\\\n-1  -2 \\\\\n-2  -4\n\\end{bmatrix}, \\quad\ny^{(2)} = \\begin{bmatrix}\n1 \\\\ 1 \\\\ 0 \\\\ 0\n\\end{bmatrix}, \\quad\n\\theta^{(2)}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad\nT^{(2)} = 100, \\quad\n\\alpha^{(2)}_{\\mathrm{E}} = 0.1, \\quad\n\\alpha^{(2)}_{\\mathrm{N}} = 0.1, \\quad\n\\lambda^{(2)} = 10^{-2}.\n$$\n- 测试 $3$ (饱和初始化；极小的费雪因子 $p(1-p)$):\n$$\nX^{(3)} = \\begin{bmatrix}\n3  1 \\\\\n-3  -1 \\\\\n1  3 \\\\\n-1  -3\n\\end{bmatrix}, \\quad\ny^{(3)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1 \\\\ 0\n\\end{bmatrix}, \\quad\n\\theta^{(3)}_0 = \\begin{bmatrix} 20 \\\\ -20 \\end{bmatrix}, \\quad\nT^{(3)} = 50, \\quad\n\\alpha^{(3)}_{\\mathrm{E}} = 0.05, \\quad\n\\alpha^{(3)}_{\\mathrm{N}} = 0.05, \\quad\n\\lambda^{(3)} = 10^{-2}.\n$$\n\n关于反向传播的说明。对于欧几里得梯度，通过应用链式法则（在ANN的背景下即为反向传播）来计算 $\\nabla_\\theta \\mathcal{L}(\\theta)$。对于费雪-黎曼方法，使用模型和数据计算费雪信息矩阵，应用阻尼 $\\lambda I$，并沿着您推导出的测地线最速下降方向进行更新。\n\n输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。该列表必须是\n$$\n\\left[\n\\mathcal{L}^{(1)}_{\\mathrm{E,final}}, \\mathcal{L}^{(1)}_{\\mathrm{N,final}}, \\mathcal{L}^{(2)}_{\\mathrm{E,final}}, \\mathcal{L}^{(2)}_{\\mathrm{N,final}}, \\mathcal{L}^{(3)}_{\\mathrm{E,final}}, \\mathcal{L}^{(3)}_{\\mathrm{N,final}}\n\\right],\n$$\n每个条目四舍五入到六位小数。不允许有其他输出。",
            "solution": "该问题要求推导、实现和比较两种用于训练逻辑回归模型的优化方法：标准欧几里得梯度下降法和基于费雪信息度量（FIM）的黎曼最速下降法。推导必须从第一性原理开始。\n\n首先，解决黎曼最速下降方向的推导问题。目标是找到一个参数更新 $\\delta\\theta$，使得损失函数 $\\mathcal{L}(\\theta)$ 的下降幅度最大。损失函数的一阶泰勒展开给出了变化量 $\\Delta \\mathcal{L} \\approx \\nabla_\\theta \\mathcal{L}(\\theta)^\\top \\delta\\theta$。为了找到最速下降方向，必须在步长 $\\delta\\theta$ 的“大小”约束下最小化这个变化量。在标准的欧几里得设置中，这个约束是关于欧几里得范数的平方 $||\\delta\\theta||_2^2$，这导致了我们熟悉的梯度下降更新 $\\delta\\theta \\propto -\\nabla_\\theta \\mathcal{L}(\\theta)$。\n\n在概率模型的背景下，参数空间的几何结构更自然地由概率分布 $p_\\theta$ 变化的程度来描述。衡量两个分布 $p_\\theta$ 和 $p_{\\theta'}$ 之间差异的一个基本度量是Kullback–Leibler (KL) 散度，即 $\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta'})$。对于一个无穷小步长 $\\delta\\theta$，使得 $\\theta' = \\theta + \\delta\\theta$，KL散度为参数之间的距离提供了一个局部度量。我们对小的 $\\delta\\theta$ 展开 $\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta})$。\nKL散度的定义是 $\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta}) = \\mathbb{E}_{Y,X \\sim p_\\theta}[\\log p_\\theta(Y|X) - \\log p_{\\theta+\\delta\\theta}(Y|X)]$。\n我们对 $\\log p_{\\theta+\\delta\\theta}(Y|X)$ 这一项在 $\\theta$ 附近进行二阶泰勒展开：\n$$\n\\log p_{\\theta+\\delta\\theta}(Y|X) \\approx \\log p_\\theta(Y|X) + (\\nabla_\\theta \\log p_\\theta(Y|X))^\\top \\delta\\theta + \\frac{1}{2} \\delta\\theta^\\top (\\nabla^2_\\theta \\log p_\\theta(Y|X)) \\delta\\theta\n$$\n将此式代入KL散度表达式中，得到：\n$$\n\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta}) \\approx -\\mathbb{E}[(\\nabla_\\theta \\log p_\\theta)^\\top \\delta\\theta] - \\frac{1}{2} \\mathbb{E}[\\delta\\theta^\\top (\\nabla^2_\\theta \\log p_\\theta) \\delta\\theta]\n$$\n现在使用信息论的两个关键恒等式。首先，得分函数（对数似然的梯度）的期望为零：$\\mathbb{E}[\\nabla_\\theta \\log p_\\theta] = 0$。其次，费雪信息矩阵（FIM），$F(\\theta)$，由信息矩阵等式给出：$F(\\theta) = \\mathbb{E}[(\\nabla_\\theta \\log p_\\theta)(\\nabla_\\theta \\log p_\\theta)^\\top] = -\\mathbb{E}[\\nabla^2_\\theta \\log p_\\theta]$。\n应用这些恒等式，KL散度简化为：\n$$\n\\mathrm{KL}(p_\\theta \\,\\|\\, p_{\\theta+\\delta\\theta}) \\approx \\frac{1}{2} \\delta\\theta^\\top F(\\theta) \\delta\\theta\n$$\n这个二次型在参数流形上定义了一个黎曼度量，其中度量张量是 FIM。黎曼最速下降方向是在此几何结构中固定步长（即 $\\frac{1}{2} \\delta\\theta^\\top F(\\theta) \\delta\\theta = \\text{常数}$）的约束下，最小化 $\\nabla_\\theta \\mathcal{L}(\\theta)^\\top \\delta\\theta$ 的方向。使用拉格朗日乘数法，我们最小化 $\\mathcal{J}(\\delta\\theta) = \\nabla_\\theta \\mathcal{L}(\\theta)^\\top \\delta\\theta + \\mu (\\frac{1}{2} \\delta\\theta^\\top F(\\theta) \\delta\\theta)$。对 $\\delta\\theta$ 求导并令其为零，得到 $\\nabla_\\theta \\mathcal{L}(\\theta) + \\mu F(\\theta) \\delta\\theta = 0$。这给出了最优步长方向 $\\delta\\theta \\propto -F(\\theta)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta)$。向量 $d_N = -F(\\theta)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta)$ 被称为自然梯度。费雪-黎曼方法的更新规则是 $\\theta_{t+1} = \\theta_t - \\alpha_N F(\\theta_t)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta_t)$。为保证数值稳定性，加入吉洪诺夫阻尼项 $\\lambda I$，更新变为：\n$$\n\\theta_{t+1} = \\theta_t - \\alpha_N (F(\\theta_t) + \\lambda I)^{-1} \\nabla_\\theta \\mathcal{L}(\\theta_t)\n$$\n\n其次，我们为给定的逻辑回归模型具体化所需的量。模型预测为 $p_i = p_\\theta(y=1|x_i) = \\sigma(\\theta^\\top x_i)$，其中 $\\sigma(z) = (1+e^{-z})^{-1}$。sigmoid函数的导数是 $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$。\n平均负对数似然 $\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_i [y_i \\log p_i + (1-y_i)\\log(1-p_i)]$ 的欧几里得梯度可通过应用链式法则求得。对于单个数据点 $(x_i, y_i)$，其对 $\\mathcal{L}$ 中求和项的贡献的梯度为：\n$$\n\\nabla_\\theta [y_i \\log p_i + (1-y_i)\\log(1-p_i)] = \\left(\\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i}\\right)\\nabla_\\theta p_i = \\frac{y_i - p_i}{p_i(1-p_i)} (p_i(1-p_i)x_i) = (y_i - p_i)x_i\n$$\n因此，完整的梯度为 $\\nabla_\\theta \\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^N (y_i - p_i)x_i = \\frac{1}{N}\\sum_{i=1}^N (p_i - y_i)x_i$。用矩阵表示法，即为 $\\nabla_\\theta \\mathcal{L}(\\theta) = \\frac{1}{N}X^\\top(p-y)$。\n\n单个数据点 $x_i$ 的FIM，记为 $F_{x_i}(\\theta)$，是通过对模型预测的标签分布 $Y_i \\sim \\text{Bernoulli}(p_i)$ 取期望来计算的。单个样本的对数似然梯度为 $\\nabla_\\theta \\log p_\\theta(y_i|x_i) = (y_i-p_i)x_i$。\n$$\nF_{x_i}(\\theta) = \\mathbb{E}_{Y_i \\sim p_i}[ (Y_i-p_i)^2 x_i x_i^\\top ] = x_i x_i^\\top \\mathbb{E}[(Y_i-p_i)^2]\n$$\n期望项是伯努利随机变量的方差，即 $p_i(1-p_i)$。因此，$F_{x_i}(\\theta) = p_i(1-p_i)x_i x_i^\\top$。数据集的总FIM是其平均值，称为经验FIM：\n$$\nF(\\theta) = \\frac{1}{N}\\sum_{i=1}^N p_i(1-p_i) x_i x_i^\\top\n$$\n用矩阵表示法，即为 $F(\\theta) = \\frac{1}{N}X^\\top W X$，其中 $W$ 是一个对角矩阵，其对角线元素为 $W_{ii} = p_i(1-p_i)$。\n\n最后，实现过程是为每个测试用例设置两个优化循环，一个用于欧几里得梯度下降，另一个用于费雪-黎曼方法。在黎曼方法的每一步中，计算欧几里得梯度 $\\nabla_\\theta \\mathcal{L}(\\theta)$ 和经验FIM $F(\\theta)$。构造阻尼度量 $M = F(\\theta) + \\lambda I$，并求解线性系统 $Mv = \\nabla_\\theta \\mathcal{L}(\\theta)$ 以获得自然梯度方向 $v$。然后使用该方向和步长 $\\alpha_N$ 更新参数。为了数值稳定性，损失计算会将概率值裁剪，使其远离精确的0和1。在两种方法都进行 T 次迭代后，计算并报告每个测试用例的最终负对数似然值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements, analyzes, and compares Euclidean gradient descent and\n    Fisher-Riemannian steepest descent for training a logistic regression model.\n    \"\"\"\n\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        # The standard implementation is sufficient due to numpy's handling of large exponents.\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def calculate_loss(y, p_hat):\n        \"\"\"\n        Calculates the average negative log-likelihood (binary cross-entropy).\n        Clips probabilities to avoid log(0).\n        \"\"\"\n        epsilon = 1e-15\n        p_hat = np.clip(p_hat, epsilon, 1.0 - epsilon)\n        return -np.mean(y * np.log(p_hat) + (1.0 - y) * np.log(1.0 - p_hat))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": np.array([[2., 1.], [2., -1.], [-2., 1.], [-2., -1.], [1., 2.], [-1., -2.]]),\n            \"y\": np.array([[1.], [0.], [1.], [0.], [1.], [0.]]),\n            \"theta0\": np.array([[0.], [0.]]),\n            \"T\": 100,\n            \"alpha_E\": 0.1,\n            \"alpha_N\": 0.1,\n            \"lambda\": 1e-2,\n        },\n        {\n            \"X\": np.array([[1., 2.], [2., 4.], [-1., -2.], [-2., -4.]]),\n            \"y\": np.array([[1.], [1.], [0.], [0.]]),\n            \"theta0\": np.array([[0.], [0.]]),\n            \"T\": 100,\n            \"alpha_E\": 0.1,\n            \"alpha_N\": 0.1,\n            \"lambda\": 1e-2,\n        },\n        {\n            \"X\": np.array([[3., 1.], [-3., -1.], [1., 3.], [-1., -3.]]),\n            \"y\": np.array([[1.], [0.], [1.], [0.]]),\n            \"theta0\": np.array([[20.], [-20.]]),\n            \"T\": 50,\n            \"alpha_E\": 0.05,\n            \"alpha_N\": 0.05,\n            \"lambda\": 1e-2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, theta0 = case[\"X\"], case[\"y\"], case[\"theta0\"]\n        T, alpha_E, alpha_N, lam = case[\"T\"], case[\"alpha_E\"], case[\"alpha_N\"], case[\"lambda\"]\n        N, d = X.shape\n\n        # --- Euclidean Gradient Descent ---\n        theta_E = theta0.copy()\n        for _ in range(T):\n            z_E = X @ theta_E\n            p_E = sigmoid(z_E)\n            grad_E = (1.0 / N) * X.T @ (p_E - y)\n            theta_E -= alpha_E * grad_E\n\n        p_final_E = sigmoid(X @ theta_E)\n        loss_E = calculate_loss(y, p_final_E)\n        results.append(loss_E)\n\n        # --- Fisher-Riemannian Steepest Descent (Natural Gradient) ---\n        theta_N = theta0.copy()\n        I = np.identity(d)\n        for _ in range(T):\n            z_N = X @ theta_N\n            p_N = sigmoid(z_N)\n\n            # Euclidean gradient\n            grad_N = (1.0 / N) * X.T @ (p_N - y)\n\n            # Fisher Information Matrix (empirical)\n            weights = p_N * (1.0 - p_N)\n            W = np.diag(weights.flatten())\n            F = (1.0 / N) * (X.T @ W @ X)\n\n            # Damped metric and natural gradient direction calculation\n            M = F + lam * I\n            # Solve Mv = grad_N for v, where v is the natural gradient direction\n            v = np.linalg.solve(M, grad_N)\n            \n            theta_N -= alpha_N * v\n        \n        p_final_N = sigmoid(X @ theta_N)\n        loss_N = calculate_loss(y, p_final_N)\n        results.append(loss_N)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}