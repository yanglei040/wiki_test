{
    "hands_on_practices": [
        {
            "introduction": "Before we can effectively use neural networks to approximate functions, we must first understand their fundamental limitations. This practice introduces the \"curse of dimensionality,\" a critical concept explaining why approximating functions in high-dimensional spaces is inherently difficult. You will empirically discover how the amount of data needed to approximate a target function to a fixed accuracy grows dramatically as the number of input dimensions increases, providing a tangible grasp of this core challenge in computational science. ",
            "id": "2417291",
            "problem": "You are asked to empirically demonstrate the curse of dimensionality in neural-network function approximation by measuring how the required number of training samples grows with input dimension to achieve a fixed approximation accuracy. Implement a complete, runnable program that does the following.\n\nFrom first principles, consider the following setup.\n\n- Domain and target function:\n  - For each input dimension $D \\in \\{\\,1,2,4,8\\,\\}$, define the domain as the unit hypercube $[0,1]^D$.\n  - Define the target function $f_D:[0,1]^D \\to \\mathbb{R}$ by\n    $$f_D(\\mathbf{x}) \\;=\\; \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin\\!\\big(2\\pi x_i\\big),$$\n    where angles are in radians. The multiplicative factor $\\sqrt{2}$ ensures that the mean squared magnitude of $f_D$ under the uniform distribution does not vanish or blow up with $D$.\n\n- Neural-network model and training protocol:\n  - Use a one-hidden-layer feedforward neural network with Rectified Linear Unit (ReLU) activation. The hidden layer has $M$ fixed random features, and only the output layer is trained by ridge-regularized linear least squares. This is a valid neural-network function approximator and a well-known approximation to kernel machines using random features.\n  - Let $\\phi:\\mathbb{R}^D \\to \\mathbb{R}^M$ be the hidden feature map defined by\n    $$\\phi_j(\\mathbf{x}) \\;=\\; \\max\\!\\big(0,\\,\\mathbf{w}_j^\\top \\mathbf{x} + b_j\\big), \\quad j\\in\\{1,\\dots,M\\},$$\n    where each $\\mathbf{w}_j \\in \\mathbb{R}^D$ and $b_j \\in \\mathbb{R}$ are drawn independently from a standard normal distribution. The overall model is\n    $$\\hat{f}_D(\\mathbf{x}) \\;=\\; \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c,$$\n    where $\\mathbf{a}\\in\\mathbb{R}^M$ and $c\\in\\mathbb{R}$ are trained by minimizing ridge-regularized empirical mean squared error on a training set of size $N$.\n  - Training reduces to solving the ridge-regularized normal equations\n    $$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} \\;=\\; \\Phi^\\top \\mathbf{y},$$\n    where $\\Phi\\in\\mathbb{R}^{N\\times(M+1)}$ is the design matrix whose $n$-th row is $[\\phi(\\mathbf{x}^{(n)})^\\top,\\;1]$, $\\mathbf{y}\\in\\mathbb{R}^N$ contains targets $f_D(\\mathbf{x}^{(n)})$, $\\boldsymbol{\\theta} \\in \\mathbb{R}^{M+1}$ concatenates $\\mathbf{a}$ and $c$, and $\\lambda>0$ is the ridge regularization strength. Use a fixed $\\lambda$ for all experiments.\n  - Use independent, identically distributed (i.i.d.) samples from the uniform distribution on $[0,1]^D$ for training and validation.\n\n- Accuracy criterion:\n  - Measure the root mean squared error (RMSE) on an i.i.d. validation set:\n    $$\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N_{\\mathrm{val}}}\\sum_{n=1}^{N_{\\mathrm{val}}}\\big(\\hat{f}_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})-f_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})\\big)^2}.$$\n  - Declare that a given training set size $N$ is sufficient if $\\mathrm{RMSE} \\le \\varepsilon$ on the validation set.\n\n- Fixed hyperparameters and reproducibility:\n  - Use $M=256$ random hidden units.\n  - Use ridge parameter $\\lambda=10^{-3}$.\n  - Use validation set size $N_{\\mathrm{val}}=2000$.\n  - Use candidate training sizes $N \\in \\{\\,32,64,128,256,512,1024,2048\\,\\}$ and select the smallest $N$ that meets the accuracy criterion.\n  - Use accuracy threshold $\\varepsilon=0.20$.\n  - Use independent random number generators with fixed seeds so that results are exactly reproducible. Angles in the sine are in radians.\n\n- Test suite and output specification:\n  - For each $D \\in \\{\\,1,2,4,8\\,\\}$, with fixed $M$, $\\lambda$, $N_{\\mathrm{val}}$, and $\\varepsilon$ as above, determine the minimal $N$ from the candidate list that achieves $\\mathrm{RMSE}\\le\\varepsilon$. If no candidate $N$ achieves the threshold for a given $D$, return $-1$ for that $D$.\n  - Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the order of $D=\\{\\,1,2,4,8\\,\\}$, for example:\n    $$[N_1,N_2,N_3,N_4],$$\n    where each $N_k$ is either the minimal sufficient training size for that dimension or $-1$ if none suffices.\n\nScientific realism and foundational base: You must reason from the definition of mean squared approximation error, the structure of a one-hidden-layer neural network with ReLU activation, and the properties of random features and ridge-regularized least squares. Do not use any data-dependent heuristics beyond what is specified. All angles must be in radians. No physical units are involved in this problem. Express any internal percentages, if any arise, as decimals or fractions, not with the percent sign.",
            "solution": "We design an empirical study to expose the curse of dimensionality by quantifying how the required training set size grows with input dimension for a fixed accuracy target when using a fixed-capacity neural-network approximator. The foundational basis is the definition of mean squared error, the structure of a one-hidden-layer neural network, and ridge-regularized least squares.\n\n1. Function and domain. For each dimension $D \\in \\{\\,1,2,4,8\\,\\}$, we consider the target function\n$$f_D(\\mathbf{x}) \\;=\\; \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin\\!\\big(2\\pi x_i\\big), \\quad \\mathbf{x}\\in[0,1]^D.$$\nAngles are in radians. The factor $\\sqrt{2}$ ensures that the expected squared magnitude under the uniform measure is constant in $D$:\n$$\\mathbb{E}\\!\\left[f_D(\\mathbf{x})^2\\right] \\;=\\; \\prod_{i=1}^{D} \\mathbb{E}\\!\\left[2\\sin^2(2\\pi X_i)\\right] \\;=\\; \\prod_{i=1}^{D} 1 \\;=\\; 1,$$\nwhere $X_i \\sim \\mathrm{Uniform}[0,1]$ independently and we used $\\mathbb{E}[\\sin^2(2\\pi X_i)] = \\tfrac{1}{2}$. This normalization removes trivial scaling effects with $D$ and focuses on representational and sampling difficulty.\n\n2. Neural-network approximator with random features. Consider a one-hidden-layer network with Rectified Linear Unit (ReLU) activation defined by\n$$\\phi_j(\\mathbf{x}) \\;=\\; \\max\\!\\big(0,\\,\\mathbf{w}_j^\\top \\mathbf{x} + b_j\\big), \\quad j=1,\\dots,M,$$\nwith $\\mathbf{w}_j \\in \\mathbb{R}^D$ and $b_j\\in\\mathbb{R}$. The model is linear in these features:\n$$\\hat{f}_D(\\mathbf{x}) \\;=\\; \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c.$$\nWe fix the hidden parameters $\\{\\mathbf{w}_j,b_j\\}_{j=1}^{M}$ by sampling i.i.d. from a standard normal distribution. This yields a random features model that retains the universal approximation property as $M$ grows and allows fast, convex training of the output layer.\n\n3. Training by ridge-regularized least squares. Given a training set $\\{(\\mathbf{x}^{(n)}, y^{(n)})\\}_{n=1}^{N}$ with $y^{(n)} = f_D(\\mathbf{x}^{(n)})$, define the design matrix $\\Phi \\in \\mathbb{R}^{N\\times(M+1)}$ by augmenting the hidden features with a column of ones:\n$$\\Phi_{n,:} \\;=\\; \\big[\\phi(\\mathbf{x}^{(n)})^\\top,\\,1\\big].$$\nLet $\\boldsymbol{\\theta} \\in \\mathbb{R}^{M+1}$ be the concatenation of the output weights and bias, and let $\\mathbf{y} \\in \\mathbb{R}^{N}$ be the vector of targets. The ridge-regularized empirical risk minimization problem\n$$\\min_{\\boldsymbol{\\theta}} \\;\\frac{1}{N}\\|\\Phi \\boldsymbol{\\theta} - \\mathbf{y}\\|_2^2 \\;+\\; \\lambda \\|\\boldsymbol{\\theta}\\|_2^2$$\nhas the unique solution given by the normal equations\n$$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} \\;=\\; \\Phi^\\top \\mathbf{y},$$\nwhere $I$ is the $(M+1)\\times(M+1)$ identity matrix and $\\lambda>0$ is the ridge parameter. Solving this linear system yields the trained output layer parameters.\n\n4. Accuracy and validation. To quantify approximation quality we use the root mean squared error (RMSE) on an independent validation set of size $N_{\\mathrm{val}}$:\n$$\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N_{\\mathrm{val}}}\\sum_{n=1}^{N_{\\mathrm{val}}}\\big(\\hat{f}_D(\\mathbf{x}^{(n)}_{\\mathrm{val}}) - f_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})\\big)^2}.$$\nWe declare success for a given $N$ if $\\mathrm{RMSE} \\le \\varepsilon$.\n\n5. Experimental protocol. For each $D \\in \\{\\,1,2,4,8\\,\\}$:\n   - Fix the number of hidden units to $M = 256$ and the ridge parameter to $\\lambda = 10^{-3}$.\n   - Use $N_{\\mathrm{val}} = 2000$ validation samples drawn i.i.d. uniformly from $[0,1]^D$.\n   - Consider candidate training sizes $N \\in \\{\\,32,64,128,256,512,1024,2048\\,\\}$.\n   - For each $N$ in increasing order:\n     - Draw $N$ i.i.d. training inputs from $[0,1]^D$, compute targets using $f_D$.\n     - Construct the feature matrix $\\Phi$ and solve $(\\Phi^\\top \\Phi + \\lambda I)\\boldsymbol{\\theta} = \\Phi^\\top \\mathbf{y}$.\n     - Compute the validation $\\mathrm{RMSE}$.\n     - If $\\mathrm{RMSE} \\le \\varepsilon$ with $\\varepsilon = 0.20$, record this $N$ as sufficient and stop increasing $N$ for this $D$.\n   - If no candidate $N$ satisfies the criterion, record $-1$ for this $D$.\n   - Use fixed random seeds for feature initialization and for data sampling to ensure reproducibility across runs.\n\n6. Rationale with respect to the curse of dimensionality. The curse of dimensionality manifests because the volume of the input space grows exponentially with $D$, so that covering the domain with a fixed density of samples requires exponentially many points. Additionally, functions such as products of oscillatory factors can exhibit increasingly complex variation over $[0,1]^D$ as $D$ increases. With fixed model capacity $M$, the sample size $N$ required to achieve a target $\\mathrm{RMSE}$ generally increases with $D$, and there may exist dimensions for which no $N$ in a practical range meets the threshold because the approximation is representation-limited by $M$.\n\n7. Output. The program aggregates the minimal sufficient $N$ for each $D$ in order $D=\\{\\,1,2,4,8\\,\\}$ into a single line, formatted as a comma-separated list of integers enclosed in square brackets:\n$$[N_1,N_2,N_3,N_4].$$\nIf no candidate $N$ suffices for a particular $D$, the corresponding entry is $-1$.\n\nThis design starts from the definition of mean squared error and the linear algebraic solution to ridge-regularized least squares, specifies a concrete neural-network function approximator via random ReLU features, and implements an empirical protocol to measure how required data size changes with dimension, thereby demonstrating the curse of dimensionality.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef target_function(X):\n    \"\"\"\n    Compute f_D(x) = prod_i sqrt(2) * sin(2*pi*x_i) for a batch of inputs X.\n    X: array of shape (N, D)\n    Returns: array of shape (N,)\n    \"\"\"\n    return np.prod(np.sqrt(2.0) * np.sin(2.0 * np.pi * X), axis=1)\n\ndef generate_random_features(D, M, seed):\n    \"\"\"\n    Generate random ReLU feature parameters (W, b) for dimension D and M features.\n    W: shape (M, D), entries ~ N(0,1)\n    b: shape (M,), entries ~ N(0,1)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(M, D))\n    b = rng.standard_normal(size=(M,))\n    return W, b\n\ndef relu_features(X, W, b):\n    \"\"\"\n    Compute ReLU features Phi = max(0, X @ W^T + b) for batch X.\n    X: (N, D), W: (M, D), b: (M,)\n    Returns: Phi of shape (N, M)\n    \"\"\"\n    Z = X @ W.T + b  # broadcasting b over rows\n    return np.maximum(0.0, Z)\n\ndef ridge_solve(Phi, y, lam):\n    \"\"\"\n    Solve (Phi^T Phi + lam I) theta = Phi^T y for theta.\n    Phi should already include a bias column (i.e., augmented with ones).\n    \"\"\"\n    # Normal equations with Tikhonov regularization\n    A = Phi.T @ Phi\n    # Add ridge on all parameters (including bias for simplicity)\n    A.flat[::A.shape[0]+1] += lam  # add lam to diagonal\n    b = Phi.T @ y\n    theta = np.linalg.solve(A, b)\n    return theta\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\ndef minimal_training_size_for_dimension(D, M, lam, eps, N_candidates, N_val,\n                                        seed_features, seed_train_base, seed_val_base):\n    \"\"\"\n    For a given input dimension D, find the minimal N in N_candidates such that\n    RMSE <= eps on a validation set, using fixed random features and ridge training.\n    Returns the minimal N, or -1 if none suffices.\n    \"\"\"\n    # Fixed random features for this D\n    W, b = generate_random_features(D, M, seed_features + D)\n\n    # Fixed validation set for this D\n    rng_val = np.random.default_rng(seed_val_base + D)\n    X_val = rng_val.random(size=(N_val, D))\n    y_val = target_function(X_val)\n    Phi_val = relu_features(X_val, W, b)\n    Phi_val_aug = np.hstack([Phi_val, np.ones((N_val, 1))])\n\n    for N in N_candidates:\n        rng_train = np.random.default_rng(seed_train_base + D + N)\n        X_train = rng_train.random(size=(N, D))\n        y_train = target_function(X_train)\n        Phi_train = relu_features(X_train, W, b)\n        Phi_train_aug = np.hstack([Phi_train, np.ones((N, 1))])\n\n        theta = ridge_solve(Phi_train_aug, y_train, lam)\n        y_pred_val = Phi_val_aug @ theta\n        e = rmse(y_val, y_pred_val)\n        if e <= eps:\n            return N\n    return -1\n\ndef solve():\n    # Experimental settings as specified\n    dims = [1, 2, 4, 8]              # D values\n    M = 256                          # number of random hidden units\n    lam = 1e-3                       # ridge regularization strength\n    eps = 0.20                       # RMSE threshold\n    N_candidates = [32, 64, 128, 256, 512, 1024, 2048]\n    N_val = 2000\n\n    # Fixed seeds for reproducibility\n    seed_features = 12345\n    seed_train_base = 54321\n    seed_val_base = 99999\n\n    results = []\n    for D in dims:\n        N_min = minimal_training_size_for_dimension(\n            D=D, M=M, lam=lam, eps=eps, N_candidates=N_candidates, N_val=N_val,\n            seed_features=seed_features, seed_train_base=seed_train_base, seed_val_base=seed_val_base\n        )\n        results.append(N_min)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Having seen the challenges of high-dimensional approximation, we now explore a powerful strategy to overcome them: incorporating prior knowledge into the model architecture. This practice demonstrates the concept of \"inductive bias\" by comparing a generic Multilayer Perceptron (MLP) with a Convolutional Neural Network (CNN) on a task with underlying translation symmetry. By training both models to solve a simple partial differential equation, you will see firsthand how a model with the correct structural assumptions (the CNN) can learn and generalize far more effectively than one without. ",
            "id": "2417315",
            "problem": "Consider the one-dimensional, translation-invariant elliptic partial differential equation (PDE) on the periodic domain given by $-u''(x) + a\\,u(x) = f(x)$ with periodic boundary conditions on $[0,1)$. Discretize the problem on a uniform grid of $N$ points with spacing $h = 1/N$. The standard centered second-difference yields a circulant linear system for the discrete solution vector $u \\in \\mathbb{R}^N$ and forcing vector $f \\in \\mathbb{R}^N$. The discrete operator is diagonalized by the discrete Fourier transform: each mode indexed by integer $k \\in \\{0,1,\\dots,N-1\\}$ has eigenvalue $\\lambda_k = a + \\dfrac{4}{h^2}\\sin^2\\!\\left(\\dfrac{\\pi k}{N}\\right)$. Consequently, the exact discrete solution is obtained by dividing the Fourier coefficients of $f$ by $\\lambda_k$ and transforming back to physical space.\n\nIn this problem you will implement from first principles two neural network surrogates for the solution operator $T: f \\mapsto u$, trained only on a single input-output example, and compare their performance to demonstrate the effect of inductive bias for translation-invariant operators:\n\n- A fully connected multilayer perceptron (MLP) with no hidden layers (i.e., a single linear map), represented by a dense matrix $W \\in \\mathbb{R}^{N \\times N}$ acting as $u_{\\text{MLP}} = W f$.\n\n- A one-dimensional circular convolutional neural network (CNN) with one linear convolutional layer having an odd-length kernel of size $K$, represented by a kernel vector $w \\in \\mathbb{R}^{K}$ that acts via circular convolution $u_{\\text{CNN}}[i] = \\sum_{s=-m}^{m} w[s+m]\\; f[(i - s) \\bmod N]$, where $m = (K-1)/2$.\n\nBoth models will be trained by minimizing the mean-squared error loss between the model output and the exact discrete solution produced by the Fourier diagonalization described above. Use simple gradient descent with fixed learning rates and a fixed number of iterations. Initialize all trainable parameters to zero. Angles in any trigonometric expressions must be in radians.\n\nYour implementation must respect the following fixed, scientifically consistent parameter choices and dataset definitions:\n\n- Grid size: $N = 64$ and spacing $h = 1/N$.\n\n- PDE parameter: $a = 1$.\n\n- CNN kernel size: $K = 31$ (so $m = 15$).\n\n- Training set: a single training pair $(f^{\\text{train}}, u^{\\text{train}})$ where $f^{\\text{train}}$ is the unit impulse at index $0$ (i.e., $f^{\\text{train}}[0] = 1$ and $f^{\\text{train}}[i] = 0$ for all $i \\neq 0$) and $u^{\\text{train}}$ is the exact discrete solution for that forcing computed by Fourier diagonalization.\n\n- Optimization details: use constant-step gradient descent with $T_{\\text{MLP}} = 2000$ iterations and learning rate $\\eta_{\\text{MLP}} = 0.2$ for the MLP, and $T_{\\text{CNN}} = 2000$ iterations and learning rate $\\eta_{\\text{CNN}} = 0.05$ for the CNN. For the MLP with loss $L = \\dfrac{1}{N} \\|W f - u\\|_2^2$, update by $W \\leftarrow W - \\eta_{\\text{MLP}} \\dfrac{2}{N} (W f - u) f^\\top$. For the CNN with loss $L = \\dfrac{1}{N} \\|w \\circledast f - u\\|_2^2$, where $\\circledast$ denotes circular convolution as defined above, update by $w[s+m] \\leftarrow w[s+m] - \\eta_{\\text{CNN}} \\dfrac{2}{N} \\sum_{i=0}^{N-1} \\big( (w \\circledast f)[i] - u[i] \\big)\\, f[(i - s) \\bmod N]$ for all $s \\in \\{-m,\\dots,m\\}$.\n\nDesign a test suite to assess translation generalization and frequency response using four fixed forcings $f^{(j)}$ and their exact discrete solutions $u^{(j)}$:\n\n- Case $1$: $f^{(1)}$ is the unit impulse at index $0$.\n\n- Case $2$: $f^{(2)}$ is the unit impulse at index $16$.\n\n- Case $3$: $f^{(3)}[i] = \\cos\\!\\left(2 \\pi \\cdot 4 \\cdot \\dfrac{i}{N}\\right)$ for all $i \\in \\{0,\\dots,N-1\\}$ (angles in radians).\n\n- Case $4$: $f^{(4)}[i] = 1$ for all $i \\in \\{0,\\dots,N-1\\}$.\n\nFor each case, compute the relative $\\ell_2$ error of each model with respect to the exact solution, defined by $\\varepsilon_{\\text{model}} = \\dfrac{\\|u_{\\text{model}} - u^{(j)}\\|_2}{\\|u^{(j)}\\|_2}$, for the MLP and CNN respectively.\n\nYour program must implement all of the above, using the specified hyperparameters, and produce a single line of output containing the eight floating-point results in the following order: $[\\varepsilon_{\\text{MLP}}^{(1)}, \\varepsilon_{\\text{CNN}}^{(1)}, \\varepsilon_{\\text{MLP}}^{(2)}, \\varepsilon_{\\text{CNN}}^{(2)}, \\varepsilon_{\\text{MLP}}^{(3)}, \\varepsilon_{\\text{CNN}}^{(3)}, \\varepsilon_{\\text{MLP}}^{(4)}, \\varepsilon_{\\text{CNN}}^{(4)}]$. The line must be printed as a comma-separated list enclosed in square brackets, with no additional text.\n\nAll quantities are dimensionless under the chosen nondimensionalization of the PDE. Ensure your code is a complete, runnable program that requires no input and uses only the allowed libraries. The final answer must be expressed as floats in the specified single-line format.",
            "solution": "The problem presented is valid. It is scientifically grounded in numerical analysis and computational physics, specifically concerning the numerical solution of partial differential equations and the modern application of machine learning methods as surrogate models. The problem is well-posed, with all parameters, methods, and objectives clearly and unambiguously defined. It constitutes a standard, albeit simplified, exercise in scientific machine learning, designed to demonstrate the concept of inductive bias in neural network architectures. I will therefore proceed with a complete solution.\n\nThe problem requires a comparison of two neural network models, a Multilayer Perceptron (MLP) and a Convolutional Neural Network (CNN), for learning the solution operator of a one-dimensional elliptic PDE. The analysis will proceed in four stages: first, establishing the exact numerical solution to the PDE to serve as ground truth; second, constructing and training the MLP model; third, constructing and training the CNN model; and finally, evaluating both trained models on a suite of test cases to assess their generalization capabilities.\n\n**1. The Governing Equation and its Exact Discrete Solution**\n\nThe physical system is described by the one-dimensional, second-order elliptic partial differential equation with periodic boundary conditions on the domain $x \\in [0,1)$:\n$$ -u''(x) + a\\,u(x) = f(x) $$\nHere, $u(x)$ is the solution field, $f(x)$ is a source or forcing term, and $a$ is a positive constant, given as $a=1$. Discretizing this equation on a uniform grid of $N=64$ points $x_i = i h$ for $i=0, \\dots, N-1$ with spacing $h=1/N$, we use the standard second-order accurate centered finite difference approximation for the second derivative:\n$$ -u''(x_i) \\approx \\frac{-u_{i-1} + 2u_i - u_{i+1}}{h^2} $$\nwhere $u_i = u(x_i)$. Applying this to the PDE results in a system of linear equations for the discrete solution vector $\\mathbf{u} \\in \\mathbb{R}^N$. Due to the periodic boundary conditions, the resulting system matrix is a circulant matrix. A fundamental property of circulant matrices is that they are diagonalized by the Discrete Fourier Transform (DFT). This means that if we transform the equation into the Fourier domain, the differential operator becomes a simple multiplication.\n\nLet $\\hat{\\mathbf{u}}$ and $\\hat{\\mathbf{f}}$ be the DFTs of $\\mathbf{u}$ and $\\mathbf{f}$, respectively. The linear system in the Fourier domain is:\n$$ \\lambda_k \\hat{u}_k = \\hat{f}_k $$\nfor each Fourier mode $k \\in \\{0, 1, \\dots, N-1\\}$. The eigenvalues $\\lambda_k$ of the discrete operator are given by the problem statement:\n$$ \\lambda_k = a + \\frac{4}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right) $$\nWith $a=1$ and $N=64$, all eigenvalues $\\lambda_k$ are strictly positive, guaranteeing that the operator is invertible and a unique solution exists for any forcing $\\mathbf{f}$. The exact discrete solution $\\mathbf{u}$ is found by solving for $\\hat{u}_k = \\hat{f}_k / \\lambda_k$ in the Fourier domain and then applying the inverse DFT to return to the physical domain:\n$$ \\mathbf{u} = \\text{IDFT}\\left(\\frac{\\text{DFT}(\\mathbf{f})}{\\mathbf{\\lambda}}\\right) $$\nThis method provides the ground truth data for training and testing our neural network surrogates.\n\n**2. The Multilayer Perceptron (MLP) Model**\n\nThe first model is a fully connected network with no hidden layers, which is equivalent to a single linear transformation represented by a dense matrix $W \\in \\mathbb{R}^{N \\times N}$. The model's prediction is:\n$$ \\mathbf{u}_{\\text{MLP}} = W \\mathbf{f} $$\nThis model is completely general and assumes no prior structure for the solution operator. It has $N^2 = 64^2 = 4096$ trainable parameters.\n\nThe model is trained on a single data pair $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$, where $\\mathbf{f}^{\\text{train}}$ is a unit impulse at index $0$ (i.e., $\\mathbf{f}^{\\text{train}} = \\mathbf{e}_0$, the first standard basis vector). Training is performed using gradient descent to minimize the mean squared error loss $L = \\frac{1}{N} \\|\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}\\|_2^2$. The gradient of the loss with respect to the weight matrix $W$ is $\\nabla_W L = \\frac{2}{N} (\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}) (\\mathbf{f}^{\\text{train}})^\\top$. Given $\\mathbf{f}^{\\text{train}} = \\mathbf{e}_0$, the outer product $(\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}) \\mathbf{e}_0^\\top$ is a matrix whose first column is the error vector $(\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}})$ and all other columns are zero.\n\nCrucially, since the weights are initialized to $W=0$ and the training input is always $\\mathbf{e}_0$, the gradient is non-zero only for the first column of $W$. Consequently, only the first column of $W$ is ever updated during training. The MLP effectively learns to map the first component of an input vector, $f_0$, to an output vector, and ignores all other input components $f_i$ for $i>0$. This architecture lacks the *inductive bias* for translation invariance, a key property of the underlying physical system. Therefore, we predict it will fail to generalize to inputs that are translated or have different frequency content.\n\n**3. The Convolutional Neural Network (CNN) Model**\n\nThe second model is a 1D circular convolutional layer. The prediction is given by the circular convolution of the input $\\mathbf{f}$ with a learnable kernel $\\mathbf{w} \\in \\mathbb{R}^K$:\n$$ \\mathbf{u}_{\\text{CNN}} = \\mathbf{w} \\circledast \\mathbf{f} $$\nThe kernel size is $K=31$. This operation is defined as:\n$$ u_{\\text{CNN}}[i] = \\sum_{s=-m}^{m} w[s+m]\\; f[(i - s) \\bmod N] $$\nwhere $m = (K-1)/2 = 15$. The CNN model has only $K=31$ trainable parameters. This architecture explicitly builds in the assumption of translation invariance, as a convolution is an operation that is equivariant to translation. This is the correct inductive bias for the given physical operator, which is linear and translation-invariant (LTI). The response of an LTI system to an arbitrary input is the convolution of the input with the system's impulse response (Green's function).\n\nThe CNN is trained on the same impulse-response pair $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$. Since convolving a kernel with a delta impulse yields the kernel itself (appropriately padded or truncated), the training process effectively teaches the kernel $\\mathbf{w}$ to become an approximation of the system's true impulse response, $\\mathbf{u}^{\\text{train}}$. Because the CNN has learned an approximation of the underlying LTI operator's kernel, it is expected to generalize correctly to any input, including translated impulses and inputs of varying frequencies.\n\n**4. Model Evaluation and Expected Outcome**\n\nBoth models are evaluated on four test cases:\n1.  The training impulse at index $0$: tests the ability to fit the training data.\n2.  A translated impulse at index $16$: tests generalization to translation.\n3.  A low-frequency cosine wave ($\\cos(2\\pi \\cdot 4x)$): tests generalization to different frequencies.\n4.  A constant input (zero-frequency wave): tests generalization to the lowest frequency mode.\n\nThe expected outcome is a clear demonstration of the power of inductive bias. The MLP, lacking this bias, should perform well only on the first test case (its training data) and fail on all others. The CNN, possessing the correct translation-invariance bias, should perform well on all four test cases, demonstrating robust generalization from a single training example. The numerical results will quantify this difference in performance via the relative $\\ell_2$ error.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Implements and compares MLP and CNN surrogates for a 1D elliptic PDE.\n    \"\"\"\n    # -- 1. Define constants and problem parameters --\n    N = 64\n    h = 1.0 / N\n    a = 1.0\n    K = 31\n    m = (K - 1) // 2\n    T_MLP = 2000\n    eta_MLP = 0.2\n    T_CNN = 2000\n    eta_CNN = 0.05\n\n    # -- 2. Define helper functions --\n    def exact_solver(f_vec):\n        \"\"\"\n        Computes the exact discrete solution using Fourier diagonalization.\n        \"\"\"\n        k = np.arange(N)\n        lambda_k = a + (4 / h**2) * np.sin(np.pi * k / N)**2\n        # Handle the case where a mode might be numerically zero, though not here.\n        # Add a small epsilon to the denominator for stability if needed.\n        f_hat = scipy.fft.fft(f_vec)\n        u_hat = f_hat / lambda_k\n        u_vec = scipy.fft.ifft(u_hat)\n        # The solution for a real forcing must be real.\n        return u_vec.real\n\n    def circular_conv(w_kernel, f_vec):\n        \"\"\"\n        Performs 1D circular convolution.\n        w_kernel has length K, f_vec has length N.\n        \"\"\"\n        y = np.zeros_like(f_vec, dtype=np.float64)\n        s_vals = np.arange(-m, m + 1)\n        for s_idx, s in enumerate(s_vals):\n            y += w_kernel[s_idx] * np.roll(f_vec, s)\n        return y\n\n    # -- 3. Generate training data --\n    f_train = np.zeros(N)\n    f_train[0] = 1.0\n    u_train = exact_solver(f_train)\n\n    # -- 4. Train the MLP model --\n    W = np.zeros((N, N))\n    for _ in range(T_MLP):\n        u_pred_mlp = W @ f_train\n        error_mlp = u_pred_mlp - u_train\n        # Gradient update for L = (1/N) * ||Wf - u||^2\n        grad_W = (2.0 / N) * np.outer(error_mlp, f_train)\n        W -= eta_MLP * grad_W\n\n    # -- 5. Train the CNN model --\n    w = np.zeros(K)\n    for _ in range(T_CNN):\n        u_pred_cnn = circular_conv(w, f_train)\n        error_cnn = u_pred_cnn - u_train\n        grad_w = np.zeros(K)\n        s_vals = np.arange(-m, m + 1)\n        \n        # Gradient for L = (1/N) * ||w*f - u||^2\n        # dL/dw_j = (2/N) * sum_i ( (w*f)_i - u_i ) * f_{i-j_s}\n        # where j_s is the shift corresponding to kernel element j.\n        for s_idx, s in enumerate(s_vals):\n            # The sum is the cross-correlation of the error and the input signal\n            grad_w[s_idx] = np.dot(error_cnn, np.roll(f_train, s))\n        \n        grad_w *= (2.0 / N)\n        w -= eta_CNN * grad_w\n\n    # -- 6. Define test cases and evaluate models --\n    # Case 1: Unit impulse at index 0 (training case)\n    f1 = np.zeros(N); f1[0] = 1.0\n    # Case 2: Unit impulse at index 16\n    f2 = np.zeros(N); f2[16] = 1.0\n    # Case 3: Cosine wave\n    i_indices = np.arange(N)\n    f3 = np.cos(2 * np.pi * 4 * i_indices / N)\n    # Case 4: Constant function (zero-frequency wave)\n    f4 = np.ones(N)\n\n    test_forcings = [f1, f2, f3, f4]\n    test_solutions = [exact_solver(f) for f in test_forcings]\n    \n    results = []\n    for f_test, u_exact in zip(test_forcings, test_solutions):\n        # Prevent division by zero, although not expected here\n        norm_u_exact = np.linalg.norm(u_exact)\n        if norm_u_exact == 0:\n            norm_u_exact = 1.0\n\n        # MLP prediction and error\n        u_mlp = W @ f_test\n        err_mlp = np.linalg.norm(u_mlp - u_exact) / norm_u_exact\n        results.append(err_mlp)\n        \n        # CNN prediction and error\n        u_cnn = circular_conv(w, f_test)\n        err_cnn = np.linalg.norm(u_cnn - u_exact) / norm_u_exact\n        results.append(err_cnn)\n\n    # -- 7. Print final results in the specified format --\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Now we apply our knowledge of function approximation to a concrete problem from condensed matter physics. This exercise involves modeling the non-equilibrium steady-state distribution of electrons in a conductor, a function governed by the Boltzmann transport equation. You will use a neural network with random features to approximate the analytical solution described, demonstrating how these models can serve as effective surrogates for complex functions derived from fundamental physical principles. ",
            "id": "2417301",
            "problem": "Consider a spatially uniform one-dimensional electron gas in a conductor driven by a constant electric field. Let the single-particle distribution function be $f(v)$ over the velocity variable $v$ (in $\\mathrm{m/s}$). In equilibrium at temperature $T$, the distribution is the one-dimensional Maxwell–Boltzmann distribution\n$$\nf_0(v) = n \\sqrt{\\frac{m}{2\\pi k_{\\mathrm{B}} T}} \\exp\\!\\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right),\n$$\nwhere $n$ is the number density (in $\\mathrm{m}^{-3}$), $m$ is the electron mass (in $\\mathrm{kg}$), and $k_{\\mathrm{B}}$ is the Boltzmann constant (in $\\mathrm{J/K}$). In a steady state under a uniform electric field $E$ (in $\\mathrm{V/m}$), and under the Relaxation Time Approximation (RTA) with relaxation time $\\tau$ (in $\\mathrm{s}$), the Boltzmann transport equation in one dimension for a spatially uniform system reads\n$$\n\\frac{qE}{m}\\,\\frac{\\partial f}{\\partial v} = -\\frac{f - f_0}{\\tau},\n$$\nwhere $q$ is the signed electron charge (with $q=-e$, $e>0$). In the regime of linear response (small field), the standard approximation is to evaluate the velocity derivative at the equilibrium distribution, i.e., replace $\\partial f/\\partial v$ by $\\partial f_0/\\partial v$ on the left-hand side. This yields an inhomogeneous linear relation that determines the non-equilibrium steady state distribution function $f(v)$.\n\nYour tasks are:\n- Derive, starting from the steady-state Boltzmann transport equation under the Relaxation Time Approximation and the linear response assumption, a closed-form expression for the non-equilibrium steady state distribution function $f(v)$ in terms of $f_0(v)$, its velocity derivative $\\partial f_0/\\partial v$, and the parameters $q$, $E$, $m$, $\\tau$.\n- Implement a neural-network function approximator $\\hat f(v)$ for $f(v)$ over a finite velocity interval $v\\in[-v_{\\max},v_{\\max}]$, where $v_{\\max} = 5 \\sqrt{k_{\\mathrm{B}} T/m}$ (in $\\mathrm{m/s}$). Use a single-hidden-layer network with $N_h$ fixed random features and hyperbolic tangent activation (these fixed features form the hidden layer), and train only the linear output layer by minimizing the mean-squared residual of the linearized steady-state Boltzmann equation on a set of collocation points in $[-v_{\\max},v_{\\max}]$ with a small $\\ell_2$ ridge regularization. Explicitly, standardize the input with the thermal speed $v_{\\mathrm{th}} = \\sqrt{k_{\\mathrm{B}} T/m}$ via $x = v / v_{\\mathrm{th}}$, define features $\\phi_j(v) = \\tanh(w_j x + b_j)$ for $j=1,\\dots,N_h$ with fixed random $w_j$ and $b_j$, and fit coefficients so that $\\hat f$ approximately satisfies the linearized equation at the collocation points.\n- Evaluate the quality of your approximation by computing the relative $L^2$ error\n$$\n\\varepsilon = \\frac{\\left(\\int_{-v_{\\max}}^{v_{\\max}} \\left[\\hat f(v) - f(v)\\right]^2 \\, dv\\right)^{1/2}}{\\left(\\int_{-v_{\\max}}^{v_{\\max}} f(v)^2 \\, dv\\right)^{1/2}},\n$$\napproximated numerically by the composite trapezoidal rule over a fine uniform grid in $[-v_{\\max},v_{\\max}]$. This error is dimensionless and must be reported as a floating-point number.\n\nFundamental constants to be used in your implementation:\n- Electron charge magnitude $e = 1.602176634\\times 10^{-19}$ (in $\\mathrm{C}$), with signed electron charge $q = -e$.\n- Electron mass $m = 9.1093837015\\times 10^{-31}$ (in $\\mathrm{kg}$).\n- Boltzmann constant $k_{\\mathrm{B}} = 1.380649\\times 10^{-23}$ (in $\\mathrm{J/K}$).\n\nUse the number density $n = 1.0\\times 10^{21}$ (in $\\mathrm{m}^{-3}$).\n\nAngle units are not used. All physical quantities that require units are specified in the International System of Units (SI). The relative $L^2$ error $\\varepsilon$ is unitless.\n\nTest suite. Your program must run the neural-network approximation and report the relative $L^2$ error for the following parameter sets, each expressed as a triple $(E, \\tau, T)$:\n- Case 1 (baseline equilibrium): $(E, \\tau, T) = (0.0,\\ 1.0\\times 10^{-14},\\ 300.0)$.\n- Case 2 (moderate drive): $(E, \\tau, T) = (1.0\\times 10^{4},\\ 1.0\\times 10^{-14},\\ 300.0)$.\n- Case 3 (short relaxation): $(E, \\tau, T) = (1.0\\times 10^{4},\\ 5.0\\times 10^{-16},\\ 300.0)$.\n- Case 4 (opposite field, hotter): $(E, \\tau, T) = (-5.0\\times 10^{3},\\ 2.0\\times 10^{-14},\\ 600.0)$.\n\nNeural-network design requirements:\n- Use $N_h = 128$ fixed random features with $\\tanh$ activation and a single linear output neuron. Standardize inputs by $v_{\\mathrm{th}}$ to form $x=v/v_{\\mathrm{th}}$ before applying the random affine map and nonlinearity.\n- Train only the linear output layer by minimizing the mean-squared residual of the linearized steady-state Boltzmann equation on a uniform grid of collocation points in $[-v_{\\max},v_{\\max}]$, with a small ridge penalty $\\lambda = 10^{-8}$.\n- Use uniform collocation with at least $N_c=801$ points and evaluate the error on a finer uniform grid with at least $N_e=2001$ points.\n\nFinal output specification:\n- Your program should produce a single line of output containing the four relative $L^2$ errors for Cases $1$–$4$, rounded to six decimal places, as a comma-separated list enclosed in square brackets. For example, an allowable output format is like $[\\varepsilon_1,\\varepsilon_2,\\varepsilon_3,\\varepsilon_4]$ where each $\\varepsilon_i$ is a floating-point number with six digits after the decimal point.",
            "solution": "The problem statement has been rigorously validated and is deemed **valid**. It is scientifically grounded in the principles of statistical mechanics and transport theory, specifically the Boltzmann transport equation. The problem is well-posed, objective, and provides all necessary information for a unique, verifiable solution.\n\nThe task is to first derive the analytical form of the one-dimensional electron distribution function $f(v)$ under a constant electric field $E$ in the linear response regime, and then to train a neural-network-based function approximator $\\hat f(v)$ to learn this function. The training objective is to minimize the residual of the governing physical equation. Finally, the accuracy of the approximator is to be quantified by the relative $L^2$ error against the analytical solution.\n\n**1. Analytical Solution for the Distribution Function**\n\nThe starting point is the steady-state, spatially uniform Boltzmann transport equation in one dimension, under the relaxation time approximation (RTA):\n$$\n\\frac{qE}{m}\\,\\frac{\\partial f}{\\partial v} = -\\frac{f - f_0}{\\tau}\n$$\nHere, $f(v)$ is the non-equilibrium distribution function, $f_0(v)$ is the equilibrium Maxwell-Boltzmann distribution, $q$ is the electron charge, $E$ is the electric field, $m$ is the electron mass, and $\\tau$ is the relaxation time.\n\nThe problem specifies the use of the linear response approximation, which assumes the electric field $E$ is small. Under this assumption, the deviation of $f$ from $f_0$ is small, and the derivative $\\partial f/\\partial v$ on the left-hand side can be approximated by the derivative of the equilibrium distribution, $\\partial f_0/\\partial v$. The equation becomes:\n$$\n\\frac{qE}{m}\\,\\frac{\\partial f_0}{\\partial v} \\approx -\\frac{f - f_0}{\\tau}\n$$\nThis is now an algebraic equation for $f(v)$, which can be solved directly:\n$$\nf - f_0 = -\\tau \\left( \\frac{qE}{m} \\right) \\frac{\\partial f_0}{\\partial v}\n$$\n$$\nf(v) = f_0(v) - \\frac{qE\\tau}{m} \\frac{\\partial f_0}{\\partial v}\n$$\nThis is the closed-form expression for the non-equilibrium distribution function in the linear response regime. To make this fully explicit, we need the derivative of the one-dimensional Maxwell-Boltzmann distribution $f_0(v)$:\n$$\nf_0(v) = n \\sqrt{\\frac{m}{2\\pi k_{\\mathrm{B}} T}} \\exp\\!\\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right)\n$$\nThe derivative with respect to velocity $v$ is:\n$$\n\\frac{\\partial f_0}{\\partial v} = \\left( n \\sqrt{\\frac{m}{2\\pi k_{\\mathrm{B}} T}} \\exp\\!\\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right) \\right) \\cdot \\frac{d}{dv} \\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right)\n$$\n$$\n\\frac{\\partial f_0}{\\partial v} = f_0(v) \\cdot \\left(-\\frac{2mv}{2 k_{\\mathrm{B}} T}\\right) = -f_0(v) \\frac{mv}{k_{\\mathrm{B}} T}\n$$\nSubstituting this derivative back into the expression for $f(v)$:\n$$\nf(v) = f_0(v) - \\frac{qE\\tau}{m} \\left( -f_0(v) \\frac{mv}{k_{\\mathrm{B}} T} \\right)\n$$\nSimplifying by canceling the mass $m$:\n$$\nf(v) = f_0(v) + f_0(v) \\frac{qE\\tau v}{k_{\\mathrm{B}} T}\n$$\n$$\nf(v) = f_0(v) \\left( 1 + \\frac{qE\\tau v}{k_{\\mathrm{B}} T} \\right)\n$$\nThis final expression is the analytical ground truth that will be used to evaluate the neural network approximation.\n\n**2. Neural Network Approximation and Training**\n\nThe problem requires approximating $f(v)$ with a function $\\hat f(v)$ realized by a single-hidden-layer neural network with fixed random features. The structure of the approximator is:\n$$\n\\hat f(v) = c_0 + \\sum_{j=1}^{N_h} c_j \\phi_j(v)\n$$\nwhere $\\{c_j\\}_{j=0}^{N_h}$ are trainable coefficients of the linear output layer, and $\\phi_j(v)$ are $N_h = 128$ fixed basis functions. These basis functions are defined as:\n$$\n\\phi_j(v) = \\tanh(w_j x + b_j), \\quad \\text{where} \\quad x = \\frac{v}{v_{\\mathrm{th}}} \\quad \\text{and} \\quad v_{\\mathrm{th}} = \\sqrt{\\frac{k_{\\mathrm{B}} T}{m}}\n$$\nThe weights $w_j$ and biases $b_j$ are drawn from a random distribution and held constant during training.\n\nThe training objective is to minimize the mean-squared residual of the linearized Boltzmann equation, with an added $\\ell_2$ regularization term. The residual for the approximator $\\hat f(v)$ is:\n$$\n\\mathcal{R}(v) = \\frac{qE}{m}\\,\\frac{\\partial f_0}{\\partial v} + \\frac{\\hat f(v) - f_0(v)}{\\tau}\n$$\nIt is clear that setting the residual to zero, $\\mathcal{R}(v)=0$, recovers the analytical solution for $\\hat f(v)$. Minimizing the squared residual is therefore equivalent to finding the best fit of $\\hat f(v)$ to the analytical solution $f(v)$. The loss function to be minimized over a set of $N_c$ collocation points $\\{v_i\\}$ is:\n$$\n\\mathcal{L} = \\frac{1}{N_c} \\sum_{i=1}^{N_c} \\left[ \\frac{\\hat f(v_i) - f_0(v_i)}{\\tau} + \\frac{qE}{m}\\frac{\\partial f_0}{\\partial v_i} \\right]^2 + \\lambda \\sum_{j=0}^{N_h} c_j^2\n$$\nThis is a standard ridge regression problem. Let $\\mathbf{C}$ be the vector of coefficients $[c_0, c_1, \\dots, c_{N_h}]^T$. Let $\\mathbf{A}$ be the design matrix of size $N_c \\times (N_h+1)$, where $A_{i,0} = 1$ and $A_{i,j} = \\phi_j(v_i)$ for $j > 0$. Let $\\mathbf{y}$ be the target vector with elements $y_i = f_0(v_i) - \\frac{qE\\tau}{m}\\frac{\\partial f_0}{\\partial v_i} = f(v_i)$. The objective is to find $\\mathbf{C}$ that minimizes $||\\mathbf{AC} - \\mathbf{y}||_2^2 + \\lambda' ||\\mathbf{C}||_2^2$ (where $\\lambda'$ incorporates scaling factors like $\\tau$ and $N_c$). The solution is given by the normal equations for ridge regression:\n$$\n(\\mathbf{A}^T\\mathbf{A} + \\lambda \\mathbf{I})\\mathbf{C} = \\mathbf{A}^T\\mathbf{y}\n$$\nThis linear system is solved for the coefficient vector $\\mathbf{C}$, which fully defines the trained approximator $\\hat f(v)$.\n\n**3. Error Evaluation**\n\nThe quality of the approximation is measured by the relative $L^2$ error, $\\varepsilon$, on the interval $[-v_{\\max}, v_{\\max}]$, where $v_{\\max} = 5 v_{\\mathrm{th}}$. The error is defined as:\n$$\n\\varepsilon = \\frac{\\left(\\int_{-v_{\\max}}^{v_{\\max}} \\left[\\hat f(v) - f(v)\\right]^2 \\, dv\\right)^{1/2}}{\\left(\\int_{-v_{\\max}}^{v_{\\max}} f(v)^2 \\, dv\\right)^{1/2}}\n$$\nNumerically, these integrals are computed using the composite trapezoidal rule over a fine, uniform grid of $N_e = 2001$ points spanning the velocity interval. This procedure is repeated for each test case to obtain the required error values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Boltzmann transport equation for an electron gas using a neural\n    network approximator and evaluates the approximation error.\n    \"\"\"\n    # Define fundamental constants\n    e = 1.602176634e-19  # Electron charge magnitude in C\n    q = -e               # Signed electron charge in C\n    m = 9.1093837015e-31 # Electron mass in kg\n    k_B = 1.380649e-23   # Boltzmann constant in J/K\n\n    # Define problem parameters\n    n = 1.0e21           # Number density in m^-3\n\n    # Neural network design requirements\n    N_h = 128            # Number of hidden neurons (fixed random features)\n    lambda_reg = 1e-8    # L2 ridge regularization parameter\n    N_c = 801            # Number of collocation points for training\n    N_e = 2001           # Number of evaluation points for error calculation\n\n    # For reproducibility of the fixed random features, a seed is used.\n    rng = np.random.default_rng(42)\n    W = rng.normal(size=N_h)  # Weights of the random features\n    b = rng.normal(size=N_h)  # Biases of the random features\n\n    # Test suite: parameter sets (E in V/m, tau in s, T in K)\n    test_cases = [\n        # Case 1 (baseline equilibrium)\n        (0.0, 1.0e-14, 300.0),\n        # Case 2 (moderate drive)\n        (1.0e4, 1.0e-14, 300.0),\n        # Case 3 (short relaxation)\n        (1.0e4, 5.0e-16, 300.0),\n        # Case 4 (opposite field, hotter)\n        (-5.0e3, 2.0e-14, 600.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        E, tau, T = case\n\n        # Calculate derived physical quantities for the current case\n        v_th = np.sqrt(k_B * T / m)\n        v_max = 5 * v_th\n\n        # Define the velocity grids for collocation (training) and evaluation (testing)\n        v_collocation = np.linspace(-v_max, v_max, N_c)\n        v_evaluation = np.linspace(-v_max, v_max, N_e)\n\n        # Pre-calculate constants for f0(v) to improve efficiency\n        _f0_const = n * np.sqrt(m / (2 * np.pi * k_B * T))\n        _f0_exp_factor = -m / (2 * k_B * T)\n\n        def f0(v):\n            \"\"\"Computes the equilibrium Maxwell-Boltzmann distribution f_0(v).\"\"\"\n            return _f0_const * np.exp(_f0_exp_factor * v**2)\n\n        def f_analytic(v):\n            \"\"\"Computes the analytical non-equilibrium distribution f(v) in the linear response regime.\"\"\"\n            correction_factor = 1 + (q * E * tau * v) / (k_B * T)\n            return f0(v) * correction_factor\n\n        # --- Training Phase: Solve for the linear output layer coefficients ---\n        \n        # 1. Construct the design matrix A (also known as the feature matrix)\n        # The input velocity is first standardized by the thermal velocity.\n        x_c = v_collocation / v_th\n        \n        # A has shape (N_c, N_h + 1). The first column of ones corresponds to the bias term c_0.\n        A = np.ones((N_c, N_h + 1))\n        \n        # Vectorized computation of features for all points and neurons\n        # Broadcasting W (N_h,) and x_c (N_c,) gives a feature block of shape (N_c, N_h).\n        A[:, 1:] = np.tanh(W[None, :] * x_c[:, None] + b[None, :])\n        \n        # 2. Construct the target vector y from the analytical solution\n        y = f_analytic(v_collocation)\n        \n        # 3. Solve the ridge regression system (A.T @ A + lambda * I) @ C = A.T @ y\n        # to find the coefficient vector C = [c_0, c_1, ..., c_N_h].\n        LHS = A.T @ A + lambda_reg * np.identity(N_h + 1)\n        RHS = A.T @ y\n        C = np.linalg.solve(LHS, RHS)\n\n        # --- Evaluation Phase: Compute the relative L2 error ---\n        \n        def f_nn(v):\n            \"\"\"Evaluates the trained neural network approximator.\"\"\"\n            x = v / v_th\n            # features has shape (len(v), N_h)\n            features = np.tanh(W[None, :] * x[:, None] + b[None, :])\n            # C[1:] has shape (N_h,); result of @ is a vector of length len(v)\n            return C[0] + features @ C[1:]\n        \n        # Get predictions and true values on the fine evaluation grid\n        f_hat_predictions = f_nn(v_evaluation)\n        f_analytic_values = f_analytic(v_evaluation)\n        \n        # Compute the numerator and denominator of the relative L2 error\n        # using the composite trapezoidal rule (implemented by np.trapz).\n        integrand_numerator = (f_hat_predictions - f_analytic_values)**2\n        integrand_denominator = f_analytic_values**2\n        \n        integral_numerator = np.trapz(integrand_numerator, x=v_evaluation)\n        integral_denominator = np.trapz(integrand_denominator, x=v_evaluation)\n        \n        # Calculate the final error.\n        if integral_denominator == 0:\n            error = 0.0 if integral_numerator == 0 else np.inf\n        else:\n            error = np.sqrt(integral_numerator / integral_denominator)\n            \n        results.append(error)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}