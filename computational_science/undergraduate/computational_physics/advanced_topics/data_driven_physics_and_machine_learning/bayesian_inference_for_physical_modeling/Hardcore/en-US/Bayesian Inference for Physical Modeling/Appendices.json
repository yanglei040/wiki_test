{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a quintessential example of Bayesian parameter estimation. We will infer the decay rate, $\\lambda$, of a radioactive source from Geiger counter clicks, a classic problem involving Poisson statistics. This exercise is foundational because it utilizes a conjugate prior, allowing us to derive an analytical form for the posterior distribution and build clear intuition on how our initial beliefs are updated by experimental data .",
            "id": "2375997",
            "problem": "A Geiger counter records discrete detection counts from a stationary radioactive source. The detection process is modeled as a homogeneous Poisson process of constant decay rate $\\lambda$ in $\\mathrm{s^{-1}}$. Over non-overlapping time windows of lengths $\\{t_i\\}$ in $\\mathrm{s}$, the counts $\\{k_i\\}$ are assumed to be conditionally independent given $\\lambda$ with $k_i$ distributed as $\\mathrm{Poisson}(\\lambda t_i)$.\n\nAssume a prior on $\\lambda$ that follows the Gamma distribution with shape $\\alpha_0$ and rate $\\beta_0$, parameterized by the density\n$$\np(\\lambda \\mid \\alpha_0,\\beta_0) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}, \\quad \\lambda > 0,\n$$\nwhere $\\alpha_0 > 0$ is dimensionless and $\\beta_0 > 0$ has units $\\mathrm{s}$.\n\nTask: For each test case below, compute the posterior mean $\\mathbb{E}[\\lambda \\mid \\{(t_i,k_i)\\}]$ in $\\mathrm{s^{-1}}$. Express each result in $\\mathrm{s^{-1}}$, rounded to six decimal places.\n\nTest suite:\n- Case 1 (general): counts $k = (1,0,2)$, durations $t = (1.0,2.0,3.0)\\,\\mathrm{s}$, prior $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n- Case 2 (zero counts): counts $k = (0,0)$, durations $t = (10.0,20.0)\\,\\mathrm{s}$, prior $\\alpha_0 = 2.0$, $\\beta_0 = 5.0\\,\\mathrm{s}$.\n- Case 3 (long exposure, high counts): counts $k = (130,250,410)$, durations $t = (100.0,200.0,300.0)\\,\\mathrm{s}$, prior $\\alpha_0 = 0.5$, $\\beta_0 = 0.1\\,\\mathrm{s}$.\n- Case 4 (very short windows): counts $k = (0,1)$, durations $t = (0.001,0.002)\\,\\mathrm{s}$, prior $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, ordered as Case $1$, Case $2$, Case $3$, Case $4$. For example, a valid format is $[x_1,x_2,x_3,x_4]$, where each $x_j$ is the posterior mean in $\\mathrm{s^{-1}}$ rounded to six decimal places.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of nuclear physics and Bayesian statistics, well-posed, and contains all necessary information for a unique solution. We shall proceed with the derivation.\n\nThe objective is to find the posterior mean of the decay rate parameter $\\lambda$ of a Poisson process. The foundation of this task is Bayes' theorem, which states that the posterior probability is proportional to the product of the likelihood and the prior probability.\n$$\np(\\text{parameter} \\mid \\text{data}) \\propto p(\\text{data} \\mid \\text{parameter}) \\cdot p(\\text{parameter})\n$$\nIn this problem, the parameter is $\\lambda$ and the data, which we denote as $\\mathcal{D}$, consists of a set of observations $\\{ (t_i, k_i) \\}_{i=1}^N$, where $k_i$ is the number of detection counts in a time interval of length $t_i$.\n\nFirst, we define the likelihood function, $p(\\mathcal{D} \\mid \\lambda)$. The problem states that each count $k_i$ is drawn from a Poisson distribution with parameter $\\lambda t_i$. The probability mass function for a single observation is:\n$$\nP(k_i \\mid \\lambda, t_i) = \\frac{(\\lambda t_i)^{k_i} e^{-\\lambda t_i}}{k_i!}\n$$\nGiven that the observations are conditionally independent, the likelihood for the entire dataset $\\mathcal{D}$ is the product of the individual probabilities:\n$$\np(\\mathcal{D} \\mid \\lambda) = \\prod_{i=1}^{N} P(k_i \\mid \\lambda, t_i) = \\prod_{i=1}^{N} \\frac{(\\lambda t_i)^{k_i} e^{-\\lambda t_i}}{k_i!}\n$$\nIn Bayesian analysis, we are interested in the functional form of the likelihood with respect to the parameter $\\lambda$. We can therefore discard terms that do not depend on $\\lambda$, absorbing them into a normalization constant.\n$$\np(\\mathcal{D} \\mid \\lambda) \\propto \\prod_{i=1}^{N} (\\lambda t_i)^{k_i} e^{-\\lambda t_i} = \\left( \\prod_{i=1}^{N} t_i^{k_i} \\right) \\left( \\prod_{i=1}^{N} \\lambda^{k_i} \\right) \\left( \\prod_{i=1}^{N} e^{-\\lambda t_i} \\right)\n$$\n$$\np(\\mathcal{D} \\mid \\lambda) \\propto \\lambda^{\\sum_{i=1}^{N} k_i} e^{-\\lambda \\sum_{i=1}^{N} t_i}\n$$\nLet us define the total counts $K = \\sum_{i=1}^{N} k_i$ and the total observation time $T = \\sum_{i=1}^{N} t_i$. The likelihood simplifies to:\n$$\np(\\mathcal{D} \\mid \\lambda) \\propto \\lambda^K e^{-\\lambda T}\n$$\n\nNext, we consider the prior distribution for $\\lambda$, which is given as a Gamma distribution with shape parameter $\\alpha_0 > 0$ and rate parameter $\\beta_0 > 0$.\n$$\np(\\lambda \\mid \\alpha_0, \\beta_0) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}\n$$\nThe kernel of the prior distribution, ignoring the normalization constant, is:\n$$\np(\\lambda \\mid \\alpha_0, \\beta_0) \\propto \\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}\n$$\n\nNow, we combine the likelihood and the prior to find the posterior distribution, $p(\\lambda \\mid \\mathcal{D}, \\alpha_0, \\beta_0)$:\n$$\np(\\lambda \\mid \\mathcal{D}, \\alpha_0, \\beta_0) \\propto p(\\mathcal{D} \\mid \\lambda) \\cdot p(\\lambda \\mid \\alpha_0, \\beta_0)\n$$\n$$\np(\\lambda \\mid \\mathcal{D}, \\alpha_0, \\beta_0) \\propto (\\lambda^K e^{-\\lambda T}) \\cdot (\\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}) = \\lambda^{K + \\alpha_0 - 1} e^{-(\\beta_0 + T)\\lambda}\n$$\nThis expression is the kernel of a Gamma distribution. This confirms that the Gamma distribution is a conjugate prior for the Poisson likelihood, meaning the posterior distribution is also a Gamma distribution. The parameters of this posterior Gamma distribution, which we denote $\\alpha_N$ and $\\beta_N$, are found by inspection:\n$$\n\\alpha_N = \\alpha_0 + K = \\alpha_0 + \\sum_{i=1}^{N} k_i\n$$\n$$\n\\beta_N = \\beta_0 + T = \\beta_0 + \\sum_{i=1}^{N} t_i\n$$\nThe task is to compute the posterior mean, $\\mathbb{E}[\\lambda \\mid \\mathcal{D}]$. The expected value of a random variable $X$ following a Gamma distribution with shape $\\alpha$ and rate $\\beta$ is $\\mathbb{E}[X] = \\alpha/\\beta$. Applying this to our posterior distribution, we obtain the posterior mean of $\\lambda$:\n$$\n\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{\\alpha_N}{\\beta_N} = \\frac{\\alpha_0 + \\sum k_i}{\\beta_0 + \\sum t_i}\n$$\nThis is the general formula for the solution. We now apply it to each specified test case.\n\nCase 1: $k = (1, 0, 2)$, $t = (1.0, 2.0, 3.0)\\,\\mathrm{s}$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n$\\sum k_i = 1+0+2=3$.\n$\\sum t_i = 1.0+2.0+3.0=6.0\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{1.0 + 3}{1.0 + 6.0} = \\frac{4.0}{7.0} \\approx 0.571429\\,\\mathrm{s^{-1}}$.\n\nCase 2: $k = (0, 0)$, $t = (10.0, 20.0)\\,\\mathrm{s}$, $\\alpha_0 = 2.0$, $\\beta_0 = 5.0\\,\\mathrm{s}$.\n$\\sum k_i = 0+0=0$.\n$\\sum t_i = 10.0+20.0=30.0\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{2.0 + 0}{5.0 + 30.0} = \\frac{2.0}{35.0} \\approx 0.057143\\,\\mathrm{s^{-1}}$.\n\nCase 3: $k = (130, 250, 410)$, $t = (100.0, 200.0, 300.0)\\,\\mathrm{s}$, $\\alpha_0 = 0.5$, $\\beta_0 = 0.1\\,\\mathrm{s}$.\n$\\sum k_i = 130+250+410=790$.\n$\\sum t_i = 100.0+200.0+300.0=600.0\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{0.5 + 790}{0.1 + 600.0} = \\frac{790.5}{600.1} \\approx 1.317280\\,\\mathrm{s^{-1}}$.\n\nCase 4: $k = (0, 1)$, $t = (0.001, 0.002)\\,\\mathrm{s}$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n$\\sum k_i = 0+1=1$.\n$\\sum t_i = 0.001+0.002=0.003\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{1.0 + 1}{1.0 + 0.003} = \\frac{2.0}{1.003} \\approx 1.994018\\,\\mathrm{s^{-1}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the posterior mean of a Poisson rate parameter for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: k=(1,0,2), t=(1.0,2.0,3.0), a0=1.0, b0=1.0\n        {'k': [1, 0, 2], 't': [1.0, 2.0, 3.0], 'a0': 1.0, 'b0': 1.0},\n        # Case 2: k=(0,0), t=(10.0,20.0), a0=2.0, b0=5.0\n        {'k': [0, 0], 't': [10.0, 20.0], 'a0': 2.0, 'b0': 5.0},\n        # Case 3: k=(130,250,410), t=(100.0,200.0,300.0), a0=0.5, b0=0.1\n        {'k': [130, 250, 410], 't': [100.0, 200.0, 300.0], 'a0': 0.5, 'b0': 0.1},\n        # Case 4: k=(0,1), t=(0.001,0.002), a0=1.0, b0=1.0\n        {'k': [0, 1], 't': [0.001, 0.002], 'a0': 1.0, 'b0': 1.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        # The posterior distribution for lambda is a Gamma distribution with\n        # updated parameters alpha_N and beta_N.\n        # alpha_N = alpha_0 + sum(k_i)\n        # beta_N = beta_0 + sum(t_i)\n        # The posterior mean E[lambda | data] is alpha_N / beta_N.\n\n        # Calculate total counts and total time\n        total_counts = np.sum(case['k'])\n        total_time = np.sum(case['t'])\n        \n        # Get prior parameters\n        alpha_0 = case['a0']\n        beta_0 = case['b0']\n        \n        # Calculate posterior parameters\n        alpha_n = alpha_0 + total_counts\n        beta_n = beta_0 + total_time\n        \n        # Calculate posterior mean\n        posterior_mean = alpha_n / beta_n\n        \n        # Format the result as a string rounded to six decimal places.\n        # The format specifier \"{:.6f}\" handles both rounding and formatting.\n        results.append(f\"{posterior_mean:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on single-parameter estimation, our next exercise tackles the inference of a vector quantity: the Earth's horizontal magnetic field. Using simulated compass readings, we will estimate the field's components, $\\mathbf{B} = [B_x, B_y]^T$, within a linear-Gaussian framework. This practice is crucial for understanding how Bayesian methods apply to multi-parameter linear models and demonstrates the important role of the prior as a regularizer, ensuring stable and physically meaningful results even with limited data .",
            "id": "2376023",
            "problem": "You are given a physical sensing scenario and must perform Bayesian inference to estimate the horizontal components of the Earth's magnetic field vector using only scalar compass-like readings taken at different instrument orientations. Consider an idealized single-axis magnetic sensor rigidly attached to a platform. At a known yaw orientation angle $\\theta_k$ (in radians), the sensor reports a scalar reading $m_k$ (in microtesla, $\\mu$T) equal to the projection of the local horizontal magnetic field onto its body-fixed sensing axis, corrupted by additive noise. The horizontal magnetic field vector is $\\mathbf{B} = [B_x, B_y]^\\top$ in the local East-North frame, with $B_x$ in the eastward direction and $B_y$ in the northward direction. The measurement model is\n$$\nm_k = B_x \\cos(\\theta_k) + B_y \\sin(\\theta_k) + \\varepsilon_k,\n$$\nwhere $\\varepsilon_k$ are independent and identically distributed Gaussian noise terms with zero mean and known standard deviation $\\sigma$. Assume a zero-mean isotropic Gaussian prior for the field components,\n$$\n[B_x, B_y]^\\top \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_0^2 \\mathbf{I}_2).\n$$\nYour task is to derive from first principles and then implement a program that computes the Bayesian posterior mean estimates of $B_x$ and $B_y$ for a set of given test cases. The program must not read input; it must contain and use the provided test suite as constants. The program should compute the posterior mean for each test case using the Gaussian prior and likelihood specified above, and output the pair $[\\hat{B}_x, \\hat{B}_y]$ for each case.\n\nAll angles must be treated in radians; all magnetic field values must be expressed in microtesla ($\\mu$T). The final reported estimates for each component must be rounded to three decimals. For numerical stability, your method must remain well-defined even when the orientation angles are closely clustered; in particular, the prior must regularize the inference so that the posterior is well-defined.\n\nTest suite to implement and solve:\n- Test case 1 (well-spread orientations, moderate noise): use angles (radians) $[0, \\pi/4, \\pi/2, 3\\pi/4, \\pi, 5\\pi/4, 3\\pi/2, 7\\pi/4]$, measurements ($\\mu$T) $[25.5, -11.4066, -38.8, -46.3619, -24.9, 10.6066, 38.5, 46.2619]$, noise standard deviation $\\sigma = 1.0$ ($\\mu$T), prior standard deviation $\\sigma_0 = 100.0$ ($\\mu$T).\n- Test case 2 (closely clustered orientations, moderate-to-low noise): use angles (radians) $[0.15, 0.20, 0.25, 0.30, 0.35]$, measurements ($\\mu$T) $[17.2596, 20.03415, 21.85932, 24.42936, 26.48858]$, noise standard deviation $\\sigma = 0.5$ ($\\mu$T), prior standard deviation $\\sigma_0 = 100.0$ ($\\mu$T).\n- Test case 3 (orthogonal orientations, very low noise, minimal data): use angles (radians) $[0.0, 1.5707963267948966]$, measurements ($\\mu$T) $[-29.98, 14.97]$, noise standard deviation $\\sigma = 0.1$ ($\\mu$T), prior standard deviation $\\sigma_0 = 100.0$ ($\\mu$T).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list of the rounded posterior means for $B_x$ and $B_y$ for the corresponding test case. For example, the overall format must be\n$$\n[[\\hat{B}_{x,1}, \\hat{B}_{y,1}], [\\hat{B}_{x,2}, \\hat{B}_{y,2}], [\\hat{B}_{x,3}, \\hat{B}_{y,3}]]\n$$\nwith each number rounded to three decimals and expressed in $\\mu$T, but without units printed. The line must contain no other text.",
            "solution": "The problem presented is a canonical application of Bayesian inference to a linear-Gaussian state estimation problem, which is a fundamental task in computational physics and engineering. The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We proceed with the derivation from first principles.\n\nThe state to be estimated is the horizontal magnetic field vector, denoted as $\\mathbf{B} = [B_x, B_y]^\\top$. The measurements are governed by a linear model. For a set of $N$ measurements, we have the system:\n$$\n\\mathbf{m} = \\mathbf{H}\\mathbf{B} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{m} = [m_1, m_2, \\dots, m_N]^\\top$ is the vector of scalar measurements, $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_N]^\\top$ is the vector of measurement noise, and $\\mathbf{H}$ is the observation matrix of size $N \\times 2$. The $k$-th row of $\\mathbf{H}$ corresponds to the $k$-th measurement and is given by $[\\cos(\\theta_k), \\sin(\\theta_k)]$.\n\nThe problem states a prior distribution for the state vector $\\mathbf{B}$ and a distribution for the noise $\\boldsymbol{\\varepsilon}$.\nThe prior on $\\mathbf{B}$ is a zero-mean isotropic Gaussian:\n$$\np(\\mathbf{B}) = \\mathcal{N}(\\mathbf{B} | \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)\n$$\nwith prior mean $\\boldsymbol{\\mu}_0 = \\mathbf{0} = [0, 0]^\\top$ and prior covariance $\\boldsymbol{\\Sigma}_0 = \\sigma_0^2 \\mathbf{I}_2$, where $\\mathbf{I}_2$ is the $2 \\times 2$ identity matrix.\n\nThe measurement noise terms $\\varepsilon_k$ are independent and identically distributed (i.i.d.) Gaussian with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$. Consequently, the noise vector $\\boldsymbol{\\varepsilon}$ follows a multivariate Gaussian distribution $\\mathcal{N}(\\boldsymbol{\\varepsilon} | \\mathbf{0}, \\mathbf{R})$, where the measurement noise covariance matrix is $\\mathbf{R} = \\sigma^2 \\mathbf{I}_N$.\n\nFrom the measurement model, the likelihood of observing $\\mathbf{m}$ given the state $\\mathbf{B}$ is also Gaussian:\n$$\np(\\mathbf{m}|\\mathbf{B}) = \\mathcal{N}(\\mathbf{m} | \\mathbf{H}\\mathbf{B}, \\mathbf{R})\n$$\nThis is because $\\mathbf{m}$ is a linear transformation of the Gaussian variable $\\boldsymbol{\\varepsilon}$ ($\\boldsymbol{\\varepsilon} = \\mathbf{m} - \\mathbf{H}\\mathbf{B}$).\n\nOur goal is to find the posterior distribution of the state, $p(\\mathbf{B}|\\mathbf{m})$, using Bayes' theorem:\n$$\np(\\mathbf{B}|\\mathbf{m}) \\propto p(\\mathbf{m}|\\mathbf{B}) p(\\mathbf{B})\n$$\nSince the prior and the likelihood are both Gaussian, the posterior distribution is also Gaussian. We denote it as $p(\\mathbf{B}|\\mathbf{m}) = \\mathcal{N}(\\mathbf{B} | \\boldsymbol{\\mu}_N, \\boldsymbol{\\Sigma}_N)$, where $\\boldsymbol{\\mu}_N$ is the posterior mean and $\\boldsymbol{\\Sigma}_N$ is the posterior covariance.\n\nThe standard formulas for the posterior parameters in a linear-Gaussian model are:\n$$\n\\boldsymbol{\\Sigma}_N^{-1} = \\boldsymbol{\\Sigma}_0^{-1} + \\mathbf{H}^\\top \\mathbf{R}^{-1} \\mathbf{H}\n$$\n$$\n\\boldsymbol{\\mu}_N = \\boldsymbol{\\Sigma}_N (\\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0 + \\mathbf{H}^\\top \\mathbf{R}^{-1} \\mathbf{m})\n$$\nSubstituting our specific prior and noise parameters: $\\boldsymbol{\\mu}_0 = \\mathbf{0}$, $\\boldsymbol{\\Sigma}_0^{-1} = (\\sigma_0^2 \\mathbf{I}_2)^{-1} = \\frac{1}{\\sigma_0^2}\\mathbf{I}_2$, and $\\mathbf{R}^{-1} = (\\sigma^2 \\mathbf{I}_N)^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}_N$.\n\nThe inverse posterior covariance (or posterior precision matrix) becomes:\n$$\n\\boldsymbol{\\Sigma}_N^{-1} = \\frac{1}{\\sigma_0^2}\\mathbf{I}_2 + \\frac{1}{\\sigma^2}\\mathbf{H}^\\top \\mathbf{H}\n$$\nThe posterior mean, which is the desired estimate $\\hat{\\mathbf{B}}$, is:\n$$\n\\hat{\\mathbf{B}} = \\boldsymbol{\\mu}_N = \\boldsymbol{\\Sigma}_N \\left(\\left(\\frac{1}{\\sigma_0^2}\\mathbf{I}_2\\right)\\mathbf{0} + \\mathbf{H}^\\top \\left(\\frac{1}{\\sigma^2}\\mathbf{I}_N\\right) \\mathbf{m}\\right) = \\boldsymbol{\\Sigma}_N \\left(\\frac{1}{\\sigma^2}\\mathbf{H}^\\top \\mathbf{m}\\right)\n$$\nSubstituting the expression for $\\boldsymbol{\\Sigma}_N^{-1}$:\n$$\n\\hat{\\mathbf{B}} = \\left( \\frac{1}{\\sigma_0^2}\\mathbf{I}_2 + \\frac{1}{\\sigma^2}\\mathbf{H}^\\top \\mathbf{H} \\right)^{-1} \\left(\\frac{1}{\\sigma^2}\\mathbf{H}^\\top \\mathbf{m}\\right)\n$$\nTo simplify, we can multiply the expression inside the inverse by $\\sigma^2$ and the term outside the inverse by $\\sigma^2$:\n$$\n\\hat{\\mathbf{B}} = \\left( \\sigma^2 \\left( \\frac{1}{\\sigma_0^2}\\mathbf{I}_2 + \\frac{1}{\\sigma^2}\\mathbf{H}^\\top \\mathbf{H} \\right) \\right)^{-1} \\left(\\sigma^2 \\frac{1}{\\sigma^2}\\mathbf{H}^\\top \\mathbf{m}\\right)\n$$\n$$\n\\hat{\\mathbf{B}} = \\left( \\frac{\\sigma^2}{\\sigma_0^2}\\mathbf{I}_2 + \\mathbf{H}^\\top \\mathbf{H} \\right)^{-1} \\mathbf{H}^\\top \\mathbf{m}\n$$\nThis is the final expression for the posterior mean estimate of $\\mathbf{B}$. The term $\\frac{\\sigma^2}{\\sigma_0^2}\\mathbf{I}_2$ acts as a regularization term. It adds a small positive value to the diagonal of the matrix $\\mathbf{H}^\\top \\mathbf{H}$, ensuring that the matrix to be inverted is always positive definite and thus invertible. This is crucial for numerical stability, particularly in cases where the columns of $\\mathbf{H}$ are nearly collinear (i.e., when the orientation angles $\\theta_k$ are closely clustered), which would make $\\mathbf{H}^\\top \\mathbf{H}$ ill-conditioned or singular.\n\nThe algorithm to be implemented is as follows:\n$1$. For each test case, define the data vectors $\\boldsymbol{\\theta}$ and $\\mathbf{m}$, and the parameters $\\sigma$ and $\\sigma_0$.\n$2$. Construct the $N \\times 2$ observation matrix $\\mathbf{H}$ where the $k$-th row is $[\\cos(\\theta_k), \\sin(\\theta_k)]$.\n$3$. Compute the regularization parameter $\\lambda = (\\sigma/\\sigma_0)^2$.\n$4$. Compute the $2 \\times 2$ matrix $\\mathbf{A} = \\mathbf{H}^\\top \\mathbf{H} + \\lambda \\mathbf{I}_2$.\n$5$. Compute the $2 \\times 1$ vector $\\mathbf{b} = \\mathbf{H}^\\top \\mathbf{m}$.\n$6$. Solve the linear system $\\mathbf{A} \\hat{\\mathbf{B}} = \\mathbf{b}$ for $\\hat{\\mathbf{B}}$, which is equivalent to computing $\\hat{\\mathbf{B}} = \\mathbf{A}^{-1}\\mathbf{b}$.\n$7$. The resulting vector $\\hat{\\mathbf{B}} = [\\hat{B}_x, \\hat{B}_y]^\\top$ is the posterior mean estimate. The components are then rounded to three decimal places for reporting.\nThis procedure will be applied to each of the three test cases specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Bayesian posterior mean estimates for the horizontal components\n    of the Earth's magnetic field vector for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"angles\": np.array([0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi, 5*np.pi/4, 3*np.pi/2, 7*np.pi/4]),\n            \"measurements\": np.array([25.5, -11.4066, -38.8, -46.3619, -24.9, 10.6066, 38.5, 46.2619]),\n            \"sigma\": 1.0,\n            \"sigma0\": 100.0,\n        },\n        {\n            \"angles\": np.array([0.15, 0.20, 0.25, 0.30, 0.35]),\n            \"measurements\": np.array([17.2596, 20.03415, 21.85932, 24.42936, 26.48858]),\n            \"sigma\": 0.5,\n            \"sigma0\": 100.0,\n        },\n        {\n            \"angles\": np.array([0.0, 1.5707963267948966]),\n            \"measurements\": np.array([-29.98, 14.97]),\n            \"sigma\": 0.1,\n            \"sigma0\": 100.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        angles = case[\"angles\"]\n        measurements = case[\"measurements\"]\n        sigma = case[\"sigma\"]\n        sigma0 = case[\"sigma0\"]\n\n        # 1. Construct the observation matrix H.\n        # H has shape (N, 2), where N is the number of measurements.\n        H = np.vstack((np.cos(angles), np.sin(angles))).T\n\n        # 2. Compute the regularization parameter lambda.\n        lambda_reg = (sigma / sigma0)**2\n        \n        # 3. Compute H_transpose * H and H_transpose * m.\n        H_T_H = H.T @ H\n        H_T_m = H.T @ measurements\n        \n        # 4. Form the matrix A = H_T_H + lambda * I.\n        # np.identity(2) creates a 2x2 identity matrix.\n        A = H_T_H + lambda_reg * np.identity(2)\n        \n        # 5. Solve for the posterior mean B_hat = A_inverse * H_T_m.\n        # np.linalg.inv computes the inverse of matrix A.\n        # The @ operator performs matrix multiplication.\n        B_hat = np.linalg.inv(A) @ H_T_m\n        \n        # 6. Store the result.\n        # The problem does not require rounding at this stage, but rounding\n        # is performed for the final output string formatting.\n        results.append(B_hat.tolist())\n\n    # Final print statement in the exact required format.\n    # The format is [[Bx1,By1],[Bx2,By2],[Bx3,By3]] with numbers\n    # rounded to three decimals and no spaces.\n    results_str = []\n    for res_pair in results:\n        bx_str = f\"{res_pair[0]:.3f}\"\n        by_str = f\"{res_pair[1]:.3f}\"\n        results_str.append(f\"[{bx_str},{by_str}]\")\n    \n    final_output = f\"[{','.join(results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice moves beyond parameter estimation to the more general task of model selection. We will determine the most probable quantum state, described by the integer $n$, of a particle in a box based on noisy position measurements. This advanced problem requires you to compute the Bayesian evidence for a set of discrete models, introducing the powerful technique of marginalizing over latent variablesâ€”a cornerstone of hierarchical Bayesian modeling .",
            "id": "2375965",
            "problem": "A nonrelativistic particle of mass confined in a one-dimensional infinite potential well of length $L$ has stationary states indexed by a positive integer quantum number $n$. The spatial wavefunction is normalized and the position probability density is given by the square modulus of the wavefunction. Consider an experimental procedure that measures the particle position with additive, independent, zero-mean Gaussian noise of known standard deviation $\\sigma$. The true position is supported only on the interval $[0,L]$, but the measured value can lie anywhere on the real line due to noise. You are asked to perform Bayesian inference on the integer quantum number $n$ from a finite set of such noisy position measurements.\n\nYour program must implement the following modeling assumptions and compute the requested outputs:\n\n- The infinite square well stationary-state wavefunction leads to a position probability density $p(x \\mid n)$ supported on $x \\in [0,L]$.\n- Each measurement $y$ is generated by first drawing $x$ from $p(x \\mid n)$ and then drawing $y$ from a normal distribution with mean $x$ and variance $\\sigma^2$, independently across measurements. That is, the conditional density $p(y \\mid x)$ is Gaussian with standard deviation $\\sigma$.\n- Measurements are conditionally independent given $n$.\n- A prior $p(n)$ over a finite candidate set of integers is provided for each test case.\n- The posterior $p(n \\mid y_1,\\dots,y_M)$ is proportional to the product of the prior and the likelihood across all measurements, with the likelihood for each measurement obtained by marginalizing over the latent true position $x$.\n- The maximum a posteriori estimate is the integer $n$ that maximizes the posterior. In case of ties within numerical precision, choose the smallest $n$.\n\nAll physical quantities must be treated in the International System of Units (SI). All positions, including $L$, $\\sigma$, and the measurements $y$, must be in meters. Angles, where they appear in trigonometric functions, are dimensionless arguments constructed from physical quantities so that the arguments are unitless. Your final answers are integers and thus unitless.\n\nYour program must solve the following test suite of four cases. For each case, compute the maximum a posteriori estimate of $n$ from the provided data and priors. Use only the provided data; do not generate any random numbers.\n\n- Case A (moderate noise, tri-modal structure): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.02\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with a uniform prior over these candidates, measurements $y$ given by the list $[0.151, 0.170, 0.497, 0.512, 0.835, 0.820, 0.505, 0.166, 0.842, 0.158]$ in meters.\n- Case B (near-noiseless, single-peaked structure): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.005\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with a uniform prior over these candidates, measurements $y$ given by the list $[0.480, 0.514, 0.492, 0.508, 0.501]$ in meters.\n- Case C (high noise, prior-dominated): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.15\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with prior $p(n) \\propto 1/n^2$ over these candidates, measurements $y$ given by the list $[0.10, 0.90, 0.25, 0.75]$ in meters.\n- Case D (measurements outside the well due to noise): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.03\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with a uniform prior over these candidates, measurements $y$ given by the list $[-0.03, 1.04, 0.24, 0.76, 0.26, 0.50]$ in meters.\n\nRequirements for the computation:\n\n- Use the standard infinite square well model: the position probability density for state index $n$ is proportional to $\\sin^2(n \\pi x / L)$ on $[0,L]$ and zero outside, normalized to integrate to one over $[0,L]$.\n- Use a Gaussian model for measurement noise as specified above.\n- Assume conditional independence of measurements given $n$ to form the likelihood of the dataset as a product over individual measurements.\n- Marginalize the latent true position $x$ out of each per-measurement likelihood by integrating over the physical interval $[0,L]$.\n- Compute posterior probabilities up to a multiplicative constant and identify the maximizing $n$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the same order as the cases A, B, C, D. For example, an output might look like \"[3,1,1,2]\" where each entry is the maximum a posteriori $n$ for the corresponding case. The exact format, including brackets and commas without spaces, is mandatory.",
            "solution": "The problem presented requires the application of Bayesian inference to determine the most probable quantum state of a particle in a one-dimensional infinite potential well, given a set of noisy position measurements. The problem is scientifically sound, well-posed, and all necessary information for its solution is provided.\n\nThe core of the problem is to compute the posterior probability distribution for the quantum number $n$ given a set of measurements $D = \\{y_1, y_2, \\dots, y_M\\}$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(n | D) \\propto p(D | n) p(n)\n$$\nThe measurements are assumed to be conditionally independent given the quantum number $n$. Therefore, the total likelihood for the dataset $D$ is the product of the likelihoods for each individual measurement:\n$$\np(D | n) = \\prod_{i=1}^{M} p(y_i | n)\n$$\nFor numerical stability, it is preferable to work with log-probabilities. The log-posterior is given by:\n$$\n\\log p(n | D) = \\log p(n) + \\sum_{i=1}^{M} \\log p(y_i | n) + C\n$$\nwhere $C$ is a normalization constant that does not depend on $n$. Our objective is to find the maximum a posteriori (MAP) estimate of $n$, which is the value of $n$ from the candidate set that maximizes this log-posterior:\n$$\nn_{\\text{MAP}} = \\underset{n}{\\arg\\max} \\left( \\log p(n) + \\sum_{i=1}^{M} \\log p(y_i | n) \\right)\n$$\nThe central task is to compute the likelihood for a single measurement, $p(y_i | n)$. A measurement $y_i$ is a noisy version of some true particle position $x$. The true position $x$ is not observed and is therefore a latent variable. We must marginalize (integrate out) this latent variable to find the likelihood. The marginalization is an application of the law of total probability:\n$$\np(y_i | n) = \\int p(y_i, x | n) dx = \\int p(y_i | x, n) p(x | n) dx\n$$\nThe problem states that the measurement $y_i$ depends only on the true position $x$, so $p(y_i | x, n) = p(y_i | x)$. The integral becomes:\n$$\np(y_i | n) = \\int p(y_i | x) p(x | n) dx\n$$\nWe must now define the two probability density functions in the integrand.\n\n$1$. The position probability density $p(x | n)$ for a particle in stationary state $n$ of an infinite square well of length $L$ is given by the squared modulus of the normalized wavefunction. The wavefunction is $\\psi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{n \\pi x}{L}\\right)$ for $x \\in [0, L]$ and zero otherwise. Thus, the probability density is:\n$$\np(x | n) = \\frac{2}{L} \\sin^2\\left(\\frac{n \\pi x}{L}\\right), \\quad x \\in [0, L]\n$$\n\n$2$. The measurement noise is modeled as a Gaussian (normal) distribution with mean $x$ (the true position) and standard deviation $\\sigma$. The conditional probability of observing $y_i$ given a true position $x$ is:\n$$\np(y_i | x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right)\n$$\nThe integration domain for the latent variable $x$ is the physical extent of the potential well, which is $[0, L]$. Combining these expressions, the per-measurement likelihood is:\n$$\np(y_i | n) = \\int_{0}^{L} \\left[ \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right) \\right] \\left[ \\frac{2}{L} \\sin^2\\left(\\frac{n \\pi x}{L}\\right) \\right] dx\n$$\nThis integral does not have a simple closed-form solution and must be evaluated numerically for each pair of $(y_i, n)$. We utilize numerical quadrature for this purpose. Since the factors $\\frac{2}{L\\sqrt{2\\pi}\\sigma}$ are constant with respect to $n$ for a given test case, they can be omitted when finding the argmax, simplifying the proportional log-posterior to:\n$$\nLP(n) \\propto \\log p_{\\text{unnorm}}(n) + \\sum_{i=1}^{M} \\log \\left( \\int_{0}^{L} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right) \\sin^2\\left(\\frac{n \\pi x}{L}\\right) dx \\right)\n$$\nwhere $p_{\\text{unnorm}}(n)$ is the unnormalized prior. For a uniform prior, $\\log p(n)$ is constant and can be ignored. For the prior $p(n) \\propto 1/n^2$, we use $\\log p_{\\text{unnorm}}(n) = -2 \\log n$.\n\nThe overall algorithm is as follows:\nFor each test case:\n$1$. For each candidate integer $n$ in the set $\\{1, 2, 3, 4, 5\\}$:\n    a. Calculate the log-prior term, $\\log p_{\\text{unnorm}}(n)$.\n    b. Initialize a total log-likelihood for this $n$ to $0$.\n    c. For each measurement $y_i$ in the provided list:\n        i. Numerically compute the integral $I(y_i, n) = \\int_{0}^{L} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right) \\sin^2\\left(\\frac{n \\pi x}{L}\\right) dx$.\n        ii. Add $\\log(I(y_i, n))$ to the total log-likelihood.\n    d. The unnormalized log-posterior for $n$ is the sum of the log-prior term and the total log-likelihood.\n$2$. After computing the log-posterior for all candidate $n$'s, identify the value of $n$ that yields the maximum log-posterior.\n$3$. In case of ties, the smallest value of $n$ is chosen as specified.\n\nThis procedure is implemented for each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inference problem for the four specified test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Format: (L, sigma, n_candidates, prior_spec, measurements)\n    test_cases = [\n        # Case A\n        (1.0, 0.02, np.array([1, 2, 3, 4, 5]), ('uniform',), \n         np.array([0.151, 0.170, 0.497, 0.512, 0.835, 0.820, 0.505, 0.166, 0.842, 0.158])),\n        # Case B\n        (1.0, 0.005, np.array([1, 2, 3, 4, 5]), ('uniform',), \n         np.array([0.480, 0.514, 0.492, 0.508, 0.501])),\n        # Case C\n        (1.0, 0.15, np.array([1, 2, 3, 4, 5]), ('inv_sq',), \n         np.array([0.10, 0.90, 0.25, 0.75])),\n        # Case D\n        (1.0, 0.03, np.array([1, 2, 3, 4, 5]), ('uniform',), \n         np.array([-0.03, 1.04, 0.24, 0.76, 0.26, 0.50]))\n    ]\n\n    results = []\n\n    for case in test_cases:\n        L, sigma, n_candidates, prior_spec, measurements = case\n        log_posteriors = np.zeros(len(n_candidates))\n\n        for i, n in enumerate(n_candidates):\n            # Calculate log prior\n            log_prior = 0.0\n            if prior_spec[0] == 'inv_sq':\n                # p(n) is proportional to 1/n^2, so log p(n) is proportional to -2*log(n).\n                log_prior = -2.0 * np.log(n)\n            # For uniform prior, log_prior is constant and can be taken as 0 for argmax.\n\n            # Calculate total log likelihood\n            total_log_likelihood = 0.0\n            for y in measurements:\n                # Define the integrand for the marginal likelihood.\n                # Wwe can omit constant factors that do not depend on n.\n                # The integrand is p(y|x) * p(x|n) up to constants.\n                def integrand(x):\n                    # p(x|n) term, proportional to sin^2(n*pi*x/L)\n                    psi_sq_part = np.sin(n * np.pi * x / L)**2\n                    \n                    # p(y|x) term, proportional to exp(-(y-x)^2/(2*sigma^2))\n                    gaussian_part = np.exp(-0.5 * ((y - x) / sigma)**2)\n                    \n                    return gaussian_part * psi_sq_part\n                \n                # Numerically integrate over the well [0, L]\n                integral_val, _ = quad(integrand, 0, L)\n                \n                # Add the log of the integral to the total log likelihood.\n                # If integral_val is 0, its log is -inf, which is correct.\n                if integral_val > 0:\n                    total_log_likelihood += np.log(integral_val)\n                else:\n                    total_log_likelihood += -np.inf\n\n            # Unnormalized log posterior\n            log_posteriors[i] = log_prior + total_log_likelihood\n\n        # Find the n that maximizes the posterior.\n        # Use np.isclose for floating point comparison to handle ties.\n        max_log_post = np.max(log_posteriors)\n        \n        # Check for -inf case (likelihood was zero for all n)\n        if np.isneginf(max_log_post):\n            # This is an edge case, unlikely here. If it happens, any n is equally bad.\n            # The problem asks for the smallest n in case of a tie.\n            best_n = n_candidates[0]\n        else:\n            # Find all indices that are close to the maximum value\n            best_indices = np.where(np.isclose(log_posteriors, max_log_post))[0]\n            # Choose the one corresponding to the smallest n as per the tie-breaking rule.\n            # Since n_candidates is sorted, the first index corresponds to the smallest n.\n            best_n = n_candidates[best_indices[0]]\n            \n        results.append(best_n)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}