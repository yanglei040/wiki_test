{
    "hands_on_practices": [
        {
            "introduction": "The best way to build confidence in a computational method is to test it in a controlled environment where the \"correct\" answer is known. This first practice provides a crucial sanity check for Principal Component Analysis. We will generate synthetic data from a multivariate Gaussian distribution with a known covariance matrix, then apply PCA to see if it can recover the original principal axes and their corresponding variances. This exercise  builds a foundational understanding of the deep connection between the sample covariance of a dataset and the underlying population statistics it represents.",
            "id": "2430049",
            "problem": "You will write a complete program that constructs synthetic data from a multivariate Gaussian distribution with a known covariance matrix, performs principal component analysis, and verifies that the empirical principal components match the population principal components within specified tolerances. Work entirely from first principles of probability and linear algebra. The final program must produce a single line of output as specified below.\n\nDefinitions and setup:\n- Let $d \\in \\mathbb{N}$ denote the dimension and let $N \\in \\mathbb{N}$ denote the number of independent samples.\n- Let $\\Sigma \\in \\mathbb{R}^{d \\times d}$ be a symmetric positive definite covariance matrix with eigen-decomposition\n$$\n\\Sigma = Q \\,\\Lambda\\, Q^\\top,\n$$\nwhere $Q \\in \\mathbb{R}^{d \\times d}$ is an orthogonal matrix whose columns are population eigenvectors $\\{q_i\\}_{i=1}^d$, and $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)$ with population eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\gt 0$.\n- Generate $N$ independent samples $x^{(1)},\\dots,x^{(N)} \\in \\mathbb{R}^d$ from the multivariate Gaussian distribution with zero mean and covariance $\\Sigma$, denoted $\\mathcal{N}(0,\\Sigma)$.\n- Form the empirical covariance matrix $S \\in \\mathbb{R}^{d \\times d}$ from the samples by subtracting the sample mean and using the unbiased estimator:\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x^{(n)}, \\quad\nS = \\frac{1}{N-1} \\sum_{n=1}^{N} \\big(x^{(n)} - \\bar{x}\\big)\\big(x^{(n)} - \\bar{x}\\big)^\\top.\n$$\n- Perform principal component analysis (PCA) on $S$ by computing its eigen-decomposition\n$$\nS = U \\, M \\, U^\\top,\n$$\nwhere $U \\in \\mathbb{R}^{d \\times d}$ has orthonormal columns $\\{u_i\\}_{i=1}^d$ (empirical eigenvectors) and $M = \\mathrm{diag}(\\mu_1,\\dots,\\mu_d)$ with empirical eigenvalues ordered $\\mu_1 \\ge \\mu_2 \\ge \\cdots \\ge \\mu_d \\ge 0$.\n\nVerification criteria:\n- Eigenvalues: define the maximum relative error\n$$\n\\varepsilon_\\lambda = \\max_{1 \\le i \\le d} \\frac{|\\mu_i - \\lambda_i|}{\\lambda_i}.\n$$\n- Eigenvectors for nondegenerate eigenvalues: for any index $i$ such that the corresponding population eigenvalue $\\lambda_i$ is distinct from all other population eigenvalues, define the alignment via\n$$\nc_i = |q_i^\\top u_i|, \\quad s_i = \\sqrt{1 - c_i^2}.\n$$\n- Eigen-subspaces for degenerate eigenvalues: for any index set $C \\subset \\{1,\\dots,d\\}$ of size $|C| \\ge 2$ in which all $\\{\\lambda_i\\}_{i \\in C}$ are equal, define the population projector $P_{\\mathrm{true}}(C) = Q_{[:,C]} Q_{[:,C]}^\\top$ and the empirical projector $P_{\\mathrm{est}}(C) = U_{[:,C]} U_{[:,C]}^\\top$, and measure\n$$\n\\Delta_P(C) = \\| P_{\\mathrm{est}}(C) - P_{\\mathrm{true}}(C) \\|_F,\n$$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm.\n\nFor each test case, declare success if and only if all of the following hold simultaneously:\n- $\\varepsilon_\\lambda \\le \\tau_\\lambda$,\n- for all singleton index sets $C = \\{i\\}$, $s_i \\le \\tau_v$,\n- for all degenerate index sets $C$ with $|C| \\ge 2$, $\\Delta_P(C) \\le \\tau_P$.\n\nUse the following global tolerances for all cases: $\\tau_\\lambda = 0.06$, $\\tau_v = 0.10$, $\\tau_P = 0.20$.\n\nConstruction details to ensure reproducibility:\n- For each test case, construct $Q$ as an orthogonal matrix by applying the $\\mathrm{QR}$ decomposition to a $d \\times d$ matrix with independent standard normal entries generated from a pseudo-random number generator initialized with the specified integer seed, and adjust column signs so that the diagonal of $R$ from $\\mathrm{QR}$ has nonnegative entries. Construct $\\Lambda$ from the specified population eigenvalues, and set $\\Sigma = Q \\Lambda Q^\\top$.\n- Generate the $N$ samples by drawing $Z \\in \\mathbb{R}^{N \\times d}$ with independent standard normal entries using the same random number generator (initialized with the same seed stated for the case) and setting $X = Z \\, B^\\top$ with $B = Q \\,\\Lambda^{1/2}$, where $\\Lambda^{1/2} = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_d})$.\n\nTest suite:\n- Case $1$ (distinct spectrum, $3$-dimensional):\n  - $d = 3$, $N = 80000$, population eigenvalues $[\\lambda_1,\\lambda_2,\\lambda_3] = [4.0, 1.0, 0.25]$, seed $= 11$, degenerate index sets $= \\{\\{1\\},\\{2\\},\\{3\\}\\}$.\n- Case $2$ (degenerate leading subspace, $5$-dimensional):\n  - $d = 5$, $N = 100000$, population eigenvalues $[\\lambda_1,\\dots,\\lambda_5] = [5.0, 5.0, 2.0, 1.0, 0.5]$, seed $= 22$, degenerate index sets $= \\{\\{1,2\\},\\{3\\},\\{4\\},\\{5\\}\\}$.\n- Case $3$ (high anisotropy, $2$-dimensional):\n  - $d = 2$, $N = 40000$, population eigenvalues $[\\lambda_1,\\lambda_2] = [9.0, 0.09]$, seed $= 33$, degenerate index sets $= \\{\\{1\\},\\{2\\}\\}$.\n- Case $4$ (fully isotropic, $4$-dimensional):\n  - $d = 4$, $N = 20000$, population eigenvalues $[\\lambda_1,\\dots,\\lambda_4] = [1.0, 1.0, 1.0, 1.0]$, seed $= 44$, degenerate index sets $= \\{\\{1,2,3,4\\}\\}$.\n\nIndexing in all formulas is one-based as written; when implementing in a language with zero-based indexing, adjust indices accordingly.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the test cases in the order stated above, where each entry is a boolean indicating success for that case (e.g., \"[True,False,True,True]\").",
            "solution": "The goal is to verify that principal component analysis on empirical data recovers the eigenstructure of the population covariance matrix of a multivariate Gaussian distribution within specified tolerances. The solution proceeds from the definitions of the multivariate Gaussian distribution, covariance, and eigen-decomposition.\n\n1. Population construction. For each case we are given a dimension $d$, a set of population eigenvalues $\\{\\lambda_i\\}_{i=1}^d$ ordered nonincreasingly, and a seed. We construct an orthogonal matrix $Q \\in \\mathbb{R}^{d \\times d}$ to define the eigenvectors, and set\n$$\n\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d), \\quad \\Sigma = Q \\Lambda Q^\\top.\n$$\nBy construction, $\\Sigma$ is symmetric positive definite, since $\\lambda_i \\gt 0$ for all $i$ and $Q$ is orthogonal.\n\nTo obtain $Q$, we sample a random matrix $A \\in \\mathbb{R}^{d \\times d}$ with independent standard normal entries and compute its $\\mathrm{QR}$ decomposition, $A = \\tilde{Q} R$, where $\\tilde{Q}$ is orthogonal and $R$ is upper triangular. To remove column-sign ambiguity and to make the mapping deterministic, we adjust signs so that the diagonal entries of $R$ are nonnegative: if $R_{ii} \\lt 0$, we multiply column $i$ of $\\tilde{Q}$ and row $i$ of $R$ by $-1$. The resulting $Q$ is orthogonal and has columns that are uniformly distributed on the Stiefel manifold.\n\n2. Data generation. For a multivariate Gaussian $\\mathcal{N}(0,\\Sigma)$, if $Z \\in \\mathbb{R}^{N \\times d}$ has independent standard normal entries and $B \\in \\mathbb{R}^{d \\times d}$ satisfies $B B^\\top = \\Sigma$, then $X = Z B^\\top$ consists of $N$ independent samples from $\\mathcal{N}(0,\\Sigma)$. Using the eigen-decomposition of $\\Sigma$, we set\n$$\nB = Q \\Lambda^{1/2}, \\quad \\Lambda^{1/2} = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_d}),\n$$\nwhich satisfies $B B^\\top = Q \\Lambda^{1/2} (Q \\Lambda^{1/2})^\\top = Q \\Lambda Q^\\top = \\Sigma$. We generate $Z$ using a pseudo-random number generator initialized with the specified seed for the case, ensuring reproducibility. Thus we obtain the data matrix $X \\in \\mathbb{R}^{N \\times d}$, whose rows are the sample vectors $x^{(n)}$.\n\n3. Empirical covariance. We compute the sample mean\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x^{(n)},\n$$\nand the unbiased empirical covariance matrix\n$$\nS = \\frac{1}{N-1} \\sum_{n=1}^{N} \\big(x^{(n)} - \\bar{x}\\big)\\big(x^{(n)} - \\bar{x}\\big)^\\top.\n$$\nBy the strong law of large numbers, $S \\to \\Sigma$ almost surely as $N \\to \\infty$.\n\n4. Principal component analysis. We compute the eigen-decomposition of $S$:\n$$\nS = U M U^\\top,\n$$\nwhere $U$ has orthonormal columns and $M$ is diagonal with nonnegative entries. We order the empirical eigenvalues $\\mu_1 \\ge \\mu_2 \\ge \\cdots \\ge \\mu_d \\ge 0$, and permute the corresponding eigenvectors consistently.\n\n5. Comparison metrics.\n- Eigenvalues: We compute the maximum relative error\n$$\n\\varepsilon_\\lambda = \\max_{1 \\le i \\le d} \\frac{|\\mu_i - \\lambda_i|}{\\lambda_i}.\n$$\nSince $S \\to \\Sigma$, we expect $\\mu_i \\to \\lambda_i$ and thus $\\varepsilon_\\lambda$ small for sufficiently large $N$.\n\n- Eigenvectors for nondegenerate eigenvalues: When $\\lambda_i$ is simple (distinct from all other population eigenvalues), classical matrix perturbation results imply that the associated empirical eigenvector $u_i$ converges to $q_i$ up to an arbitrary sign. We account for the sign ambiguity by computing\n$$\nc_i = |q_i^\\top u_i|, \\quad s_i = \\sqrt{1 - c_i^2}.\n$$\nHere $s_i$ is the sine of the principal angle between the one-dimensional subspaces spanned by $q_i$ and $u_i$. We require $s_i \\le \\tau_v$.\n\n- Eigen-subspaces for degenerate eigenvalues: When a set of population eigenvalues are equal, the corresponding invariant subspace is identifiable, but any orthonormal basis within that subspace is acceptable. Let $C$ be an index set with $|C| \\ge 2$ such that $\\lambda_i$ is equal for all $i \\in C$. We compare the empirical and population projectors onto these subspaces:\n$$\nP_{\\mathrm{true}}(C) = Q_{[:,C]} Q_{[:,C]}^\\top, \\quad P_{\\mathrm{est}}(C) = U_{[:,C]} U_{[:,C]}^\\top,\n$$\nand measure the discrepancy by the Frobenius norm\n$$\n\\Delta_P(C) = \\| P_{\\mathrm{est}}(C) - P_{\\mathrm{true}}(C) \\|_F.\n$$\nThis metric is invariant to the choice of orthonormal bases spanning the subspaces. As $N$ grows, $\\Delta_P(C) \\to 0$.\n\n6. Tolerances and pass criteria. We enforce three tolerances: $\\tau_\\lambda = 0.06$ for eigenvalue relative error, $\\tau_v = 0.10$ for one-dimensional subspace misalignment measured by $s_i$, and $\\tau_P = 0.20$ for degenerate subspace projector discrepancy. A test case passes if all three conditions are satisfied simultaneously for the appropriate indices and subspaces.\n\n7. Test suite. We evaluate four cases:\n- Case $1$: $d = 3$, $N = 80000$, distinct spectrum with $[\\lambda_1,\\lambda_2,\\lambda_3] = [4.0, 1.0, 0.25]$, seed $= 11$, degenerate index sets $\\{\\{1\\},\\{2\\},\\{3\\}\\}$.\n- Case $2$: $d = 5$, $N = 100000$, with a degenerate leading pair $[\\lambda_1,\\dots,\\lambda_5] = [5.0, 5.0, 2.0, 1.0, 0.5]$, seed $= 22$, degenerate index sets $\\{\\{1,2\\},\\{3\\},\\{4\\},\\{5\\}\\}$.\n- Case $3$: $d = 2$, $N = 40000$, highly anisotropic $[\\lambda_1,\\lambda_2] = [9.0, 0.09]$, seed $= 33$, degenerate index sets $\\{\\{1\\},\\{2\\}\\}$.\n- Case $4$: $d = 4$, $N = 20000$, isotropic $[\\lambda_1,\\dots,\\lambda_4] = [1.0, 1.0, 1.0, 1.0]$, seed $= 44$, degenerate index sets $\\{\\{1,2,3,4\\}\\}$. In this case, $P_{\\mathrm{true}}(C) = I$ and $P_{\\mathrm{est}}(C) = I$ for the full-space cluster, so $\\Delta_P(C) = 0$, reflecting the fact that the eigenvectors are unidentifiable but the subspace (the entire space) matches exactly.\n\n8. Output. For each case, compute the boolean indicating whether all criteria pass, and print the list of four booleans in order, formatted as a single line \"[b1,b2,b3,b4]\".\n\nThis approach is justified by the convergence of sample covariance to population covariance and the continuity of eigenvalues and invariant subspaces under symmetric matrix perturbations. The specified tolerances accommodate finite-sample fluctuations that scale on the order of $N^{-1/2}$, and the projector metric provides a basis-invariant comparison for degenerate eigenspaces.",
            "answer": "```python\nimport numpy as np\n\ndef orthogonal_from_seed(d: int, seed: int) -> np.ndarray:\n    \"\"\"\n    Construct a deterministic orthogonal matrix Q in R^{d x d} from a given seed\n    by QR decomposition of a standard normal matrix with sign correction to\n    ensure R has nonnegative diagonal entries.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((d, d))\n    Q, R = np.linalg.qr(A)\n    # Ensure deterministic sign: make diagonal of R nonnegative\n    signs = np.sign(np.diag(R))\n    signs[signs == 0.0] = 1.0\n    Q = Q * signs  # broadcast column scaling\n    return Q\n\ndef generate_samples(N: int, lambdas: np.ndarray, Q: np.ndarray, seed: int) -> np.ndarray:\n    \"\"\"\n    Generate N samples from N(0, Sigma) where Sigma = Q diag(lambdas) Q^T.\n    Uses X = Z B^T with B = Q diag(sqrt(lambdas)), Z ~ N(0, I).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d = len(lambdas)\n    Z = rng.standard_normal((N, d))\n    sqrt_lambdas = np.sqrt(lambdas)\n    B = Q * sqrt_lambdas  # each column i of Q scaled by sqrt(lambdas[i])\n    X = Z @ B.T\n    return X\n\ndef empirical_covariance(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute unbiased empirical covariance matrix S from data matrix X (N x d),\n    by centering and using factor 1/(N-1).\n    \"\"\"\n    Xc = X - X.mean(axis=0, keepdims=True)\n    N = X.shape[0]\n    S = (Xc.T @ Xc) / (N - 1)\n    return S\n\ndef pca_eigendecomposition(S: np.ndarray):\n    \"\"\"\n    Compute eigen-decomposition of symmetric matrix S, returning eigenvalues\n    and eigenvectors sorted in descending order of eigenvalues.\n    \"\"\"\n    w, V = np.linalg.eigh(S)\n    idx = np.argsort(w)[::-1]\n    w = w[idx]\n    V = V[:, idx]\n    return w, V\n\ndef max_relative_eigenvalue_error(mu: np.ndarray, lam: np.ndarray) -> float:\n    return float(np.max(np.abs(mu - lam) / lam))\n\ndef singleton_vector_misalignment(U: np.ndarray, Q: np.ndarray, singletons: list) -> float:\n    \"\"\"\n    Compute max s_i = sqrt(1 - |q_i^T u_i|^2) over singleton index sets {i}.\n    Indices are 0-based.\n    \"\"\"\n    if not singletons:\n        return 0.0\n    s_vals = []\n    for i in singletons:\n        ci = float(abs(Q[:, i].T @ U[:, i]))\n        ci = max(min(ci, 1.0), 0.0)\n        si = float(np.sqrt(max(0.0, 1.0 - ci * ci)))\n        s_vals.append(si)\n    return float(np.max(s_vals)) if s_vals else 0.0\n\ndef max_projector_discrepancy(U: np.ndarray, Q: np.ndarray, clusters: list) -> float:\n    \"\"\"\n    For each cluster C with |C| >= 2, compute Frobenius norm of projector difference.\n    Return the maximum over such clusters; return 0.0 if none.\n    \"\"\"\n    deltas = []\n    for C in clusters:\n        if len(C) >= 2:\n            Uc = U[:, C]\n            Qc = Q[:, C]\n            Pest = Uc @ Uc.T\n            Ptrue = Qc @ Qc.T\n            D = Pest - Ptrue\n            delta = float(np.linalg.norm(D, ord='fro'))\n            deltas.append(delta)\n    return float(np.max(deltas)) if deltas else 0.0\n\ndef run_case(d: int, N: int, lambdas_list: list, seed: int, clusters_1based: list,\n             tau_lambda: float, tau_v: float, tau_P: float) -> bool:\n    \"\"\"\n    Run a single test case with specified parameters and tolerances.\n    clusters_1based is a list of index lists using 1-based indices; convert to 0-based.\n    \"\"\"\n    lambdas = np.array(lambdas_list, dtype=float)\n    # Build Q and Sigma\n    Q = orthogonal_from_seed(d, seed)\n    # Generate samples\n    X = generate_samples(N, lambdas, Q, seed)\n    # Empirical covariance and PCA\n    S = empirical_covariance(X)\n    mu, U = pca_eigendecomposition(S)\n    # Ensure eigenvalues are in descending order corresponding to lambdas (which are given descending)\n    # Prepare index clusters\n    clusters0 = [[i - 1 for i in C] for C in clusters_1based]\n    # Split clusters into singletons and degenerate\n    singleton_indices = [C[0] for C in clusters0 if len(C) == 1]\n    # Metrics\n    eps_lambda = max_relative_eigenvalue_error(mu, lambdas)\n    s_max = singleton_vector_misalignment(U, Q, singleton_indices)\n    deltaP_max = max_projector_discrepancy(U, Q, clusters0)\n    # Pass criteria\n    return (eps_lambda <= tau_lambda) and (s_max <= tau_v) and (deltaP_max <= tau_P)\n\ndef solve():\n    # Global tolerances\n    tau_lambda = 0.06\n    tau_v = 0.10\n    tau_P = 0.20\n\n    # Define the test cases in the specified order\n    test_cases = [\n        # Case 1: d=3, N=80000, lambdas=[4.0,1.0,0.25], seed=11, clusters={{1},{2},{3}}\n        {\"d\": 3, \"N\": 80000, \"lambdas\": [4.0, 1.0, 0.25], \"seed\": 11, \"clusters\": [[1], [2], [3]]},\n        # Case 2: d=5, N=100000, lambdas=[5.0,5.0,2.0,1.0,0.5], seed=22, clusters={{1,2},{3},{4},{5}}\n        {\"d\": 5, \"N\": 100000, \"lambdas\": [5.0, 5.0, 2.0, 1.0, 0.5], \"seed\": 22, \"clusters\": [[1, 2], [3], [4], [5]]},\n        # Case 3: d=2, N=40000, lambdas=[9.0,0.09], seed=33, clusters={{1},{2}}\n        {\"d\": 2, \"N\": 40000, \"lambdas\": [9.0, 0.09], \"seed\": 33, \"clusters\": [[1], [2]]},\n        # Case 4: d=4, N=20000, lambdas=[1.0,1.0,1.0,1.0], seed=44, clusters={{1,2,3,4}}\n        {\"d\": 4, \"N\": 20000, \"lambdas\": [1.0, 1.0, 1.0, 1.0], \"seed\": 44, \"clusters\": [[1, 2, 3, 4]]},\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = run_case(\n            d=case[\"d\"],\n            N=case[\"N\"],\n            lambdas_list=case[\"lambdas\"],\n            seed=case[\"seed\"],\n            clusters_1based=case[\"clusters\"],\n            tau_lambda=tau_lambda,\n            tau_v=tau_v,\n            tau_P=tau_P\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "In real-world physical systems, variables are often measured in different units and can have vastly different scales—for instance, mixing positions in meters with temperatures in Kelvin. This practice explores a critical aspect of applying PCA: its sensitivity to feature scaling. By performing PCA on a dataset before and after standardization , you will discover how features with large numerical variance can dominate the analysis, and why rescaling the data to have unit variance is often a necessary first step to uncover the true correlational structure.",
            "id": "2430028",
            "problem": "You are given a family of data matrices with columns on vastly different numerical scales. For each case, consider a real-valued data matrix $X \\in \\mathbb{R}^{n \\times d}$ with $d=3$ features and $n$ samples. Define $X$ deterministically by explicit formulas for each test case below. The task is to compare Principal Component Analysis (PCA) performed on the mean-centered data and on the standardized data, and to quantify how the dominant principal component direction and its explained variance fraction change due to standardization. Principal Component Analysis (PCA) is defined here as the eigen-decomposition of the sample covariance matrix of the transformed data. Standardization of features is defined here as centering each column (subtracting its sample mean) and dividing by its sample standard deviation; if a column has zero standard deviation, leave that standardized column identically zero after centering.\n\nDefinitions to use:\n- Given $X \\in \\mathbb{R}^{n \\times d}$, let $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ be the column-wise sample mean and let $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ denote the centered data, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones. The sample covariance matrix is $S = \\frac{1}{n-1} X_c^\\top X_c$.\n- The PCA eigenvalues and eigenvectors are the eigenpairs $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$ of $S$, with $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$ and orthonormal eigenvectors $\\{\\mathbf{v}_k\\}$. The explained variance ratio of the first principal component is $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$.\n- For standardized data, compute the column-wise sample standard deviations $\\sigma_j$ of $X_c$. Form $Z$ by $Z_{:,j} = X_{c,:,j}/\\sigma_j$ for all $j$ with $\\sigma_j \\neq 0$, and $Z_{:,j} = \\mathbf{0}$ for any $j$ with $\\sigma_j = 0$. Then define $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ and its eigenpairs $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$ sorted $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$, with explained variance ratio $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$.\n- The alignment between two unit principal directions $\\mathbf{v}_1$ and $\\mathbf{u}_1$ is measured by $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$. The sign indeterminacy of eigenvectors is handled by the absolute value.\n- To quantify component dominance by original axes, define $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ and $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$, using zero-based indexing for feature indices.\n\nFor each test case, you must compute the following ordered list of quantities:\n- $r_1$ computed from $S$,\n- $r_1^{(z)}$ computed from $S^{(z)}$,\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$,\n- $i_{\\mathrm{before}}$,\n- $i_{\\mathrm{after}}$.\n\nTest suite (each case defines $n$, then $t_i$ for $i \\in \\{0,\\dots,n-1\\}$, and the three features as functions of $t_i$):\n- Case $1$: $n=200$. For each $i \\in \\{0,\\dots,199\\}$, let $t_i = \\frac{i}{199}$, and define\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$,\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$,\n  - $x_{i3} = 0.001\\, t_i$.\n  Assemble $X$ by stacking these three features as columns.\n- Case $2$: $n=100$. For each $i \\in \\{0,\\dots,99\\}$, let $t_i = \\frac{i}{99}$, and define\n  - $x_{i1} = 10^6\\, t_i$,\n  - $x_{i2} = 0$,\n  - $x_{i3} = 10\\,(t_i - 0.5)$.\n  Assemble $X$ by stacking these three features as columns.\n- Case $3$: $n=150$. For each $i \\in \\{0,\\dots,149\\}$, let $t_i = \\frac{i}{149}$, and define\n  - $x_{i1} = 1000\\,(2 t_i - 1)$,\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$,\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$.\n  Assemble $X$ by stacking these three features as columns.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the ordered list $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$. Aggregate the three cases into a single list of three lists, in the order of cases $1$, $2$, $3$. For example, the overall printed structure must be of the form $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$.\n\nAll answers are dimensionless real numbers or integers as specified. No physical units or angle units are required because all requested quantities are pure numbers.",
            "solution": "The user has provided a problem that is valid and requires a solution. The problem statement is scientifically grounded in the fields of linear algebra and statistics, specifically Principal Component Analysis (PCA). It is well-posed, providing deterministic instructions for constructing the data, defining all necessary mathematical objects and procedures, and requesting a set of specific, computable quantities. The language is objective and free of ambiguity. Therefore, a reasoned, step-by-step solution can be constructed.\n\nThe problem requires a comparison of PCA performed on mean-centered data versus standardized data for three distinct cases. The core of the problem lies in observing how scaling affects the outcome of PCA. PCA identifies the directions of maximum variance in a dataset. When features (columns of the data matrix) have vastly different scales, the feature with the largest variance will dominate the first principal component, regardless of the underlying data structure. Standardization, which rescales each feature to have a mean of $0$ and a standard deviation of $1$, prevents this by placing all features on an equal footing.\n\nThe overall procedure is as follows:\n$1$. For each test case, construct the $n \\times d$ data matrix $X$, where $d=3$.\n$2$. Perform PCA on the mean-centered data $X_c$.\n    a. Compute the column-wise sample mean vector $\\bar{\\mathbf{x}}$.\n    b. Center the data: $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$.\n    c. Compute the sample covariance matrix $S = \\frac{1}{n-1} X_c^\\top X_c$.\n    d. Find the eigenvalues $\\lambda_k$ and eigenvectors $\\mathbf{v}_k$ of $S$ by solving the eigenproblem $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$. The eigenvalues are sorted, $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$, and the eigenvectors $\\{\\mathbf{v}_k\\}$ are orthonormal. The first principal component direction is $\\mathbf{v}_1$.\n    e. Compute the explained variance ratio for the first component: $r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$.\n    f. Identify the original feature that dominates $\\mathbf{v}_1$: $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$.\n\n$3$. Perform PCA on the standardized data $Z$.\n    a. Compute the column-wise sample standard deviations, $\\sigma_j$, of $X$ using a divisor of $n-1$.\n    b. Construct the standardized data matrix $Z$. Each column $Z_{:,j}$ is obtained by scaling the corresponding centered column $X_{c,:,j}$ by $1/\\sigma_j$. If $\\sigma_j=0$, the column $Z_{:,j}$ is set to a zero vector.\n    c. Compute the sample covariance matrix of $Z$: $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$. This matrix is equivalent to the sample correlation matrix of $X$. Its diagonal entries are $1$ for any non-constant feature.\n    d. Find the eigenvalues $\\mu_k$ and eigenvectors $\\mathbf{u}_k$ of $S^{(z)}$, sorted such that $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$. The first principal component direction of the standardized data is $\\mathbf{u}_1$.\n    e. Compute the corresponding explained variance ratio: $r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$. The sum in the denominator, $\\text{Tr}(S^{(z)})$, equals the number of non-constant features.\n    f. Identify the original feature that dominates $\\mathbf{u}_1$: $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$.\n\n$4$. Compare the results from the two analyses by computing the alignment metric $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$, which measures the cosine of the angle between the two principal directions.\n\n$5$. For each case, the final output is the ordered list $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$.\n\nCase-specific analysis:\n- **Case 1**: The data consists of three features with scales $O(10^3)$, $O(1)$, and $O(10^{-3})$. The variance of the first feature, $x_1 = 1000 \\cos(2\\pi t_i)$, will be overwhelmingly larger than the others. Thus, the first principal component $\\mathbf{v}_1$ of the unstandardized data is expected to align almost perfectly with the first feature axis. This will yield $r_1 \\approx 1$ and $i_{\\mathrm{before}} = 0$. After standardization, all features have unit variance, and the structural relationship (an elliptical trajectory in the $(x_1, x_2)$ plane) will become apparent. The variance will be distributed more equitably, leading to a smaller $r_1^{(z)}$, and $\\mathbf{u}_1$ will be a combination of features $1$ and $2$.\n- **Case 2**: The first feature, $x_1 = 10^6 t_i$, has a massive scale. The second feature, $x_2=0$, is constant and has zero variance. The third feature, $x_3 = 10(t_i-0.5)$, has a much smaller scale than the first. For the unstandardized data, PCA will be dominated by feature $1$, giving $r_1 \\approx 1$ and $i_{\\mathrm{before}}=0$. After standardization, the constant feature $x_2$ remains a zero vector. Features $1$ and $3$ are both linear functions of $t_i$ and will become perfectly correlated after centering and scaling. Their standardized versions will be identical, $Z_{:,1} = Z_{:,3}$. The data will collapse onto a single direction in the $(Z_1, Z_3)$ plane. This will result in $r_1^{(z)}=1$ (as the effective rank is $1$ among the non-constant features) and an eigenvector $\\mathbf{u}_1$ of the form $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$.\n- **Case 3**: The first feature, $x_1 = 1000(2t_i-1)$, has a large scale. The second feature, $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$, is highly correlated with the first but has a much smaller scale. The third feature is of negligible scale. As with the other cases, unstandardized PCA will be dictated by the first feature's scale, so $r_1 \\approx 1$ and $i_{\\mathrm{before}}=0$. After standardization, the strong linear relationship between features $1$ and $2$ will be the most prominent characteristic. The first principal component $\\mathbf{u}_1$ will capture this shared variance, representing a direction along the major axis of the correlated cloud, roughly at a $45$-degree angle between the standardized axes of features $1$ and $2$.\n\nThe implementation will utilize `numpy` for all numerical computations, particularly `numpy.linalg.eigh` for the eigendecomposition of the symmetric covariance matrices. Care will be taken to handle the sorting of eigenvalues in descending order and the case of zero standard deviation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance > 0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds > 0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z > 0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While PCA is powerful, it is not without its limitations. The method's objective is to find directions that maximize variance, which makes it inherently sensitive to data points that lie far from the rest—outliers. This final practice demonstrates this vulnerability in a clear, two-dimensional setting . By adding a single, distant outlier to an otherwise well-behaved dataset, we can quantify exactly how much this errant point can \"pull\" the primary axis of variation, a vital lesson in data cleaning and interpreting PCA results with caution.",
            "id": "2430058",
            "problem": "You are to quantify how a single, distant outlier affects the direction of the first principal component in a $2$-dimensional dataset. Consider a deterministic base cloud of $M$ points lying on an ellipse centered at the origin, defined by the parametric set\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\tfrac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\tfrac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0,1,2,\\dots,M-1,\n$$\nwith $r_x \\gt 0$ and $r_y \\gt 0$. Form an augmented dataset by adding one additional point (the outlier) at coordinates\n$$\n\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}.\n$$\nLet $\\mathbf{S}_0$ be the sample covariance matrix of the base cloud and $\\mathbf{S}_1$ be the sample covariance matrix of the augmented dataset that contains the $M$ ellipse points and the outlier. The first principal component is defined as the unit eigenvector of the respective sample covariance matrix associated with its largest eigenvalue. Denote by $\\mathbf{u}_0 \\in \\mathbb{R}^2$ the unit eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}_0$, and by $\\mathbf{u}_1 \\in \\mathbb{R}^2$ the analogous eigenvector for $\\mathbf{S}_1$. Because eigenvectors are defined up to a sign, define the acute angular difference $\\Delta$ between the two principal directions as\n$$\n\\Delta = \\arccos\\!\\left(\\left|\\mathbf{u}_0^\\top \\mathbf{u}_1\\right|\\right).\n$$\nYou must compute $\\Delta$ in radians. All final numerical answers must be expressed in radians and rounded to $6$ decimal places.\n\nTest suite. For each parameter tuple $(r_x, r_y, M, o_x, o_y)$ below, construct the base cloud and augmented dataset as specified, compute $\\Delta$, and report the result:\n- Case $1$ (general case): $(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 10.0, 10.0)$.\n- Case $2$ (boundary, no effect): $(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 0.0, 0.0)$.\n- Case $3$ (edge, outlier dominates orthogonal direction): $(r_x, r_y, M, o_x, o_y) = (2.0, 1.0, 40, 0.0, 1000.0)$.\n- Case $4$ (near-isotropic base, moderate outlier): $(r_x, r_y, M, o_x, o_y) = (1.05, 1.0, 50, 5.0, 0.2)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above, with each value rounded to $6$ decimal places. For example, a valid output format is\n\"[x_1,x_2,x_3,x_4]\"\nwhere each $x_i$ is a float in radians with exactly $6$ digits after the decimal point.",
            "solution": "The problem is subjected to validation and is deemed valid. It is a well-posed problem in computational physics and linear algebra, resting on established scientific principles. All terms are defined with sufficient rigor, and the provided data are consistent and complete.\n\nThe task is to compute the angular deviation, $\\Delta$, of the first principal component of a $2$-dimensional dataset when a single outlier is introduced. The base dataset is a cloud of $M$ points uniformly distributed on an ellipse, and the augmented dataset includes one additional outlier point.\n\nThe first principal component of a dataset is the direction of maximum variance, which corresponds to the unit eigenvector associated with the largest eigenvalue of the sample covariance matrix. Let the set of $N$ data points be $\\{\\mathbf{p}_i\\}_{i=1}^N$, where each $\\mathbf{p}_i \\in \\mathbb{R}^2$. The sample mean is $\\bar{\\mathbf{p}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{p}_i$. The sample covariance matrix $\\mathbf{S}$ is given by:\n$$\n\\mathbf{S} = \\frac{1}{N-1}\\sum_{i=1}^N (\\mathbf{p}_i - \\bar{\\mathbf{p}})(\\mathbf{p}_i - \\bar{\\mathbf{p}})^\\top\n$$\nNote that the specific choice of denominator, whether $N$ or $N-1$, is inconsequential as it only scales the covariance matrix and does not alter its eigenvectors. The numerical implementation will use the conventional unbiased estimator with a denominator of $N-1$.\n\nFirst, we analyze the base cloud of $M$ points, $\\{\\mathbf{x}_k\\}_{k=0}^{M-1}$, defined by:\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\frac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\frac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0, 1, \\dots, M-1\n$$\nDue to the symmetry of the cosine and sine functions over a full cycle, the sample mean $\\bar{\\mathbf{x}}_0$ of the base cloud is the zero vector for $M > 1$:\n$$\n\\bar{\\mathbf{x}}_0 = \\frac{1}{M}\\sum_{k=0}^{M-1} \\mathbf{x}_k = \\begin{bmatrix} \\frac{r_x}{M}\\sum_{k=0}^{M-1}\\cos(\\frac{2\\pi k}{M}) \\\\ \\frac{r_y}{M}\\sum_{k=0}^{M-1}\\sin(\\frac{2\\pi k}{M}) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\nThe sample covariance matrix of the base cloud, $\\mathbf{S}_0$, is then:\n$$\n\\mathbf{S}_0 = \\frac{1}{M-1}\\sum_{k=0}^{M-1} \\mathbf{x}_k \\mathbf{x}_k^\\top = \\frac{1}{M-1} \\sum_{k=0}^{M-1} \\begin{bmatrix} r_x^2 \\cos^2(\\theta_k) & r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) \\\\ r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) & r_y^2 \\sin^2(\\theta_k) \\end{bmatrix}\n$$\nwhere $\\theta_k = \\frac{2\\pi k}{M}$. For $M > 2$, the sums of the off-diagonal terms evaluate to zero. The sums of the diagonal terms are $\\sum \\cos^2(\\theta_k) = M/2$ and $\\sum \\sin^2(\\theta_k) = M/2$. Thus, $\\mathbf{S}_0$ is a diagonal matrix:\n$$\n\\mathbf{S}_0 = \\frac{M}{2(M-1)} \\begin{bmatrix} r_x^2 & 0 \\\\ 0 & r_y^2 \\end{bmatrix}\n$$\nThe eigenvectors of a diagonal matrix are the standard basis vectors. The largest eigenvalue corresponds to the larger of $r_x^2$ and $r_y^2$. If $r_x > r_y$, the first principal component is $\\mathbf{u}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. If $r_y > r_x$, it is $\\mathbf{u}_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. This aligns with the semi-major axis of the ellipse.\n\nNext, we analyze the augmented dataset, which comprises the $M$ points of the base cloud plus the outlier $\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}$. The total number of points is $N = M+1$. The mean of the augmented dataset, $\\bar{\\mathbf{x}}_1$, is:\n$$\n\\bar{\\mathbf{x}}_1 = \\frac{1}{M+1}\\left(\\sum_{k=0}^{M-1}\\mathbf{x}_k + \\mathbf{z}\\right) = \\frac{1}{M+1}\\mathbf{z}\n$$\nThe covariance matrix $\\mathbf{S}_1$ is calculated over this set of $M+1$ points.\n$$\n\\mathbf{S}_1 = \\frac{1}{M} \\left( \\sum_{k=0}^{M-1}(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)^\\top + (\\mathbf{z} - \\bar{\\mathbf{x}}_1)(\\mathbf{z} - \\bar{\\mathbf{x}}_1)^\\top \\right)\n$$\nIn general, $\\mathbf{S}_1$ is not a diagonal matrix. The outlier, especially if its coordinates $(o_x, o_y)$ are large, will significantly shift the mean and contribute a large term to the covariance sum, thereby rotating the principal axes of the data distribution. The first principal component $\\mathbf{u}_1$ is the unit eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}_1$. This is found by performing an eigendecomposition of the numerically computed $\\mathbf{S}_1$.\n\nThe angular difference $\\Delta$ is computed as the acute angle between the two principal component vectors $\\mathbf{u}_0$ and $\\mathbf{u}_1$:\n$$\n\\Delta = \\arccos(|\\mathbf{u}_0^\\top \\mathbf{u}_1|)\n$$\nThe absolute value of the dot product ensures that the angle is acute, accounting for the fact that an eigenvector and its negative are equivalent.\n\nThe algorithmic procedure to solve for each test case is as follows:\n$1$. Construct the base dataset of $M$ points on the ellipse defined by $r_x$ and $r_y$.\n$2$. Compute the sample covariance matrix $\\mathbf{S}_0$ for the base dataset.\n$3$. Find the eigenvectors and eigenvalues of $\\mathbf{S}_0$. The eigenvector $\\mathbf{u}_0$ corresponding to the largest eigenvalue is the first principal component.\n$4$. Construct the augmented dataset by adding the outlier point $\\mathbf{z}=(o_x, o_y)$ to the base dataset.\n$5$. Compute the sample covariance matrix $\\mathbf{S}_1$ for the augmented dataset.\n$6$. Find the eigenvectors and eigenvalues of $\\mathbf{S}_1$. The eigenvector $\\mathbf{u}_1$ corresponding to the largest eigenvalue is the new first principal component.\n$7$. Calculate the angular difference $\\Delta = \\arccos(|\\mathbf{u}_0^\\top \\mathbf{u}_1|)$ in radians. The dot product argument should be clipped to the range $[-1, 1]$ to avoid numerical errors.\n$8$. Round the result to $6$ decimal places.\nThis procedure is implemented for each provided set of parameters.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the angular difference in the first principal component of a 2D dataset\n    due to the addition of an outlier.\n    \"\"\"\n    test_cases = [\n        # (r_x, r_y, M, o_x, o_y)\n        (3.0, 1.0, 60, 10.0, 10.0),\n        (3.0, 1.0, 60, 0.0, 0.0),\n        (2.0, 1.0, 40, 0.0, 1000.0),\n        (1.05, 1.0, 50, 5.0, 0.2),\n    ]\n\n    results = []\n    \n    for r_x, r_y, M, o_x, o_y in test_cases:\n        # Step 1: Construct the base dataset\n        theta = (2 * np.pi / M) * np.arange(M)\n        x_coords = r_x * np.cos(theta)\n        y_coords = r_y * np.sin(theta)\n        base_cloud = np.stack((x_coords, y_coords), axis=1)\n\n        # Step 2  3: PCA on the base cloud\n        # Covariance matrix for a centered ellipse is diagonal, so we can determine u0 analytically.\n        # This is more robust and faster than numerical computation for this specific geometry.\n        # Although numerical computation would yield the same result.\n        # Let's use numerical computation for generality and to match the full algorithm described.\n        # The choice of divisor (N or N-1) does not affect the eigenvectors.\n        # np.cov uses N-1 by default.\n        cov_0 = np.cov(base_cloud, rowvar=False)\n        eigvals_0, eigvecs_0 = np.linalg.eigh(cov_0)\n        # eigh sorts eigenvalues in ascending order, so the last eigenvector is the principal one.\n        u_0 = eigvecs_0[:, -1]\n\n        # Step 4: Construct the augmented dataset\n        outlier = np.array([[o_x, o_y]])\n        augmented_cloud = np.vstack((base_cloud, outlier))\n\n        # Step 5  6: PCA on the augmented dataset\n        cov_1 = np.cov(augmented_cloud, rowvar=False)\n        eigvals_1, eigvecs_1 = np.linalg.eigh(cov_1)\n        u_1 = eigvecs_1[:, -1]\n\n        # Step 7: Compute the angular difference\n        # Dot product between the two unit eigenvectors\n        dot_product = np.dot(u_0, u_1)\n        \n        # Take the absolute value to find the acute angle\n        abs_dot_product = np.abs(dot_product)\n        \n        # Clip to handle potential floating point inaccuracies > 1.0\n        clipped_dot = np.clip(abs_dot_product, -1.0, 1.0)\n        \n        delta = np.arccos(clipped_dot)\n\n        # Step 8: Append rounded result\n        results.append(round(delta, 6))\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}