{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的检验方式就是动手实践。第一个练习是一个计算物理学中的经典问题：利用直方图重加权技术计算伊辛模型的比热。通过这个练习，你将学习如何利用在少数几个不同温度下进行的蒙特卡洛模拟数据，来预测系统在连续温度范围内的热力学性质，这极大地节省了计算资源。这个练习将引导你从第一性原理出发，实现加权直方图分析方法（WHAM）的核心迭代逻辑。",
            "id": "2401585",
            "problem": "考虑一个二维铁磁 Ising 模型，建立在边长为 $L$ 的正方形晶格上，具有周期性边界条件，耦合常数 $J=1$，玻尔兹曼常数设为 $k_B=1$。其哈密顿量为 $H = -\\sum_{\\langle ij \\rangle} s_i s_j$，其中 $s_i \\in \\{-1,+1\\}$，求和遍及正方形晶格上的最近邻自旋。设 $N=L^2$ 为自旋总数，$N_b=2N$ 为键总数。对于 $L=4$（即 $N=16$），可能的总能量为九个值：$-32, -24, -16, -8, 0, 8, 16, 24, 32$（以 $J$ 为单位）。在逆温为 $\\beta=1/T$ 的正则系综中，观测到能量 $E$ 的概率为 $p_\\beta(E) \\propto g(E)\\,\\exp(-\\beta E)$，其中 $g(E)$ 是态密度（简并度）。单位自旋的比热定义为 $C(T)=\\beta^2\\,\\mathrm{Var}_\\beta(E)/N$。\n\n现为您提供 $L=4$ 系统在三个温度下分箱的蒙特卡洛能量直方图：一个低于临界温度，一个在其附近，一个高于临界温度。每个直方图都报告了与上述九个能量箱（按顺序）对应的计数。这三个温度及其直方图（每个总计 $100000$ 个样本）如下：\n- 温度 $T_1 = 2.0$，计数（按能量从 $-32$ 到 $32$ 的顺序列出）：$[36000, 42000, 18000, 3000, 800, 100, 60, 30, 10]$。\n- 温度 $T_2 = 2.269$，计数：$[6000, 14000, 24000, 23000, 17000, 9000, 4000, 2000, 1000]$。\n- 温度 $T_3 = 3.0$，计数：$[1000, 4000, 9000, 16000, 21000, 20000, 16000, 9000, 4000]$。\n\n从正则系综的第一性原理出发，且不假设任何关于态密度 $g(E)$ 的先验知识，请使用直方图重加权方法，结合这三个直方图来估计 $g(E)$，然后计算在一系列目标温度下的单位自旋比热 $C(T)$。您的方法必须仅依赖于所提供的直方图，不得使用任何外部数据或进行额外的模拟。您必须：\n- 通过强制与正则概率的一致性，论证并实现一种统计上可靠的方法，结合在不同温度下收集的多个直方图，以推断 $g(E)$（可相差一个乘法常数）。\n- 根据推断出的 $g(E)$，计算在任意目标温度 $T$ 下的配分函数（作为对九个能量的离散求和），并评估能量的平均值和方差以获得 $C(T)$。\n\n假设所有直方图均来自独立样本（您可以忽略自相关修正）。所有计算均在 $k_B=1$ 和 $J=1$ 的单位制下进行，要求的输出是单位自旋的比热，在此单位制下为无量纲量。\n\n测试组：计算以下目标温度（以 $J$ 为单位）下的 $C(T)$：$[1.8, 2.0, 2.269, 2.5, 3.0, 3.2]$。该集合包含一个略低于所提供最低直方图温度的温度，两个与所提供直方图温度重合的温度，一个接近临界温度的温度，以及两个高于所提供最高直方图温度的温度。\n\n您的程序必须生成单行输出，其中包含六个结果，以逗号分隔并用方括号括起，每个值四舍五入到六位小数（例如，“[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901]”）。所有输出都是在 $k_B=1$ 单位制下的单位自旋比热。",
            "solution": "所提出的问题在科学上是合理的、自洽的且定义明确。它要求应用多重直方图重加权方法——这是计算统计物理学中一种标准且强大的技术——来分析二维 Ising 模型的模拟数据。因此，我们将进行严谨的求解。\n\n此分析的基础是统计力学中的正则系综。对于一个处于固定逆温 $\\beta = 1/T$（玻尔兹曼常数 $k_B=1$）的系统，观测到能量为 $E$ 的微观状态的概率 $p_\\beta(E)$ 由下式给出：\n$$\np_\\beta(E) = \\frac{g(E) e^{-\\beta E}}{Z(\\beta)}\n$$\n这里，$g(E)$ 是态密度（或简并度），即对应于能量 $E$ 的不同微观状态的数量。配分函数 $Z(\\beta)$ 作为归一化常数，定义为对所有可能能级 $E_i$ 的求和：\n$$\nZ(\\beta) = \\sum_{i} g(E_i) e^{-\\beta E_i}\n$$\n该问题提供了来自 $R=3$ 次独立蒙特卡洛模拟的数据，这些模拟在逆温 $\\beta_j=1/T_j$（$j \\in \\{1, 2, 3\\}$）下进行。对于每次模拟 $j$，我们都得到了一个能量直方图，其中包含在总共 $M_j$ 个样本中观测到能量 $E_i$ 的次数 $n_{ij}$。因此，经验概率为 $n_{ij}/M_j$。这为真实的正则概率提供了一个估计：\n$$\n\\frac{n_{ij}}{M_j} \\approx p_{\\beta_j}(E_i) = \\frac{g(E_i) e^{-\\beta_j E_i}}{Z(\\beta_j)}\n$$\n根据这个关系，可以使用单次模拟 $j$ 的数据推导出态密度 $g(E_i)$ 的一个估计值：\n$$\ng(E_i) \\propto \\frac{n_{ij}}{M_j} e^{\\beta_j E_i}\n$$\n比例常数中包含了未知的配分函数 $Z(\\beta_j)$。通过结合所有 $R$ 次模拟的数据，可以获得对 $g(E_i)$ 更稳健的估计。在统计上，实现这一目标的最佳方法是通过多重直方图方法，也称为加权直方图分析方法（WHAM）。该方法产生了一个改进的 $g(E_i)$ 估计值，该估计值与所有数据集同时保持一致。态密度的组合估计由下式给出：\n$$\ng(E_i) = \\frac{\\sum_{j=1}^{R} n_{ij}}{\\sum_{j=1}^{R} M_j e^{f_j - \\beta_j E_i}}\n$$\n其中参数 $f_j$ 与模拟的无量纲自由能 $F_j = -T_j \\ln Z(\\beta_j)$ 相关，使得 $Z(\\beta_j) \\propto e^{-f_j}$。这些参数必须通过自洽方式确定。将 $g(E_i)$ 的表达式代回到配分函数 $Z(\\beta_k) \\propto e^{-f_k}$ 的定义中，得到一组自洽方程：\n$$\ne^{-f_k} = \\sum_{i} g(E_i) e^{-\\beta_k E_i} = \\sum_{i} \\frac{\\sum_{j=1}^{R} n_{ij}}{\\sum_{j=1}^{R} M_j e^{f_j - \\beta_j E_i}} e^{-\\beta_k E_i} \\quad \\text{for } k=1, \\dots, R\n$$\n这 $R$ 个关于 $R$ 个未知数 $\\{f_1, \\dots, f_R\\}$ 的耦合非线性方程组可以通过迭代求解。值得注意的是，如果 $\\{f_j\\}$ 是一个解，那么对于任意常数 $C$，$\\{f_j + C\\}$ 也是一个解。这种规范自由度可以通过将其中一个参数设为常数来固定，例如 $f_1=0$。为确保数值稳定性，最好使用对数来实现迭代求解，特别是使用 `log-sum-exp` 技巧来处理指数和。\n\n迭代过程如下：\n1. 初始化参数，例如，对所有 $j=1, \\dots, R$ 设 $f_j = 0$。\n2. 在每次迭代中，为所有能级 $i$ 计算态密度对数 $\\ln g(E_i)$ 的更新估计值：\n   $$\n   \\ln g(E_i) = \\ln\\left(\\sum_{j=1}^{R} n_{ij}\\right) - \\ln\\left(\\sum_{j=1}^{R} M_j e^{f_j - \\beta_j E_i}\\right)\n   $$\n3. 使用新的 $\\ln g(E_i)$ 重新计算自由能参数：\n   $$\n   f_k^{\\text{new}} = -\\ln\\left(\\sum_i e^{\\ln g(E_i) - \\beta_k E_i}\\right)\n   $$\n4. 归一化新的集合 $\\{f_k^{\\text{new}}\\}$，例如通过平移所有值使得第一个元素为零：$f_k \\leftarrow f_k^{\\text{new}} - f_1^{\\text{new}}$。\n5. 重复步骤 2-4，直到 $\\{f_j\\}$ 的值收敛到所需容差范围内。\n\n一旦获得收敛的 $f_j$ 值，我们就得到了 $\\ln g(E_i)$ 的最终估计值（可相差一个无关的加法常数）。这使得我们可以在任何目标逆温 $\\beta_{\\text{target}}$ 下计算任意热力学可观测量。量 $A(E)$ 的期望值为：\n$$\n\\langle A \\rangle_{\\beta_{\\text{target}}} = \\frac{\\sum_i A(E_i) g(E_i) e^{-\\beta_{\\text{target}} E_i}}{\\sum_i g(E_i) e^{-\\beta_{\\text{target}} E_i}} = \\frac{\\sum_i A(E_i) e^{\\ln g(E_i) - \\beta_{\\text{target}} E_i}}{\\sum_i e^{\\ln g(E_i) - \\beta_{\\text{target}} E_i}}\n$$\n该问题要求计算单位自旋比热 $C(T)$，其定义为：\n$$\nC(T) = \\frac{\\beta^2}{N} \\text{Var}_\\beta(E) = \\frac{\\beta^2}{N} \\left(\\langle E^2 \\rangle_\\beta - \\langle E \\rangle_\\beta^2\\right)\n$$\n其中 $N=16$ 是自旋数。我们将使用重加权后的态密度，为每个指定的目标温度计算期望值 $\\langle E \\rangle$ 和 $\\langle E^2 \\rangle$，然后确定 $C(T)$。所提供的数据包括在 $L=4$ 的晶格上，温度 $T \\in \\{2.0, 2.269, 3.0\\}$ 时的直方图，以及能级 $E_i \\in \\{-32, -24, -16, -8, 0, 8, 16, 24, 32\\}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating specific heat using the multiple histogram \n    reweighting method for the 2D Ising model.\n    \"\"\"\n    # Define problem parameters and givens\n    L = 4\n    N_spins = L * L\n    # The nine possible energy levels for the L=4 system\n    energies = np.array([-32, -24, -16, -8, 0, 8, 16, 24, 32], dtype=float)\n\n    # Simulation temperatures and their corresponding inverse temperatures (beta)\n    sim_temps = np.array([2.0, 2.269, 3.0])\n    sim_betas = 1.0 / sim_temps\n\n    # Histogram counts from the three simulations\n    counts = np.array([\n        [36000, 42000, 18000, 3000, 800, 100, 60, 30, 10],      # T=2.0\n        [6000, 14000, 24000, 23000, 17000, 9000, 4000, 2000, 1000], # T=2.269\n        [1000, 4000, 9000, 16000, 21000, 20000, 16000, 9000, 4000]  # T=3.0\n    ], dtype=float)\n\n    num_samples_per_sim = 100000.0\n    R, K = counts.shape  # R = number of simulations, K = number of energy bins\n\n    # Total counts for each energy level, summed across all simulations\n    total_counts_per_energy = np.sum(counts, axis=0)\n    # Exclude bins with zero counts from the WHAM equations to avoid log(0)\n    valid_bins_mask = total_counts_per_energy > 0\n    total_counts_per_energy = total_counts_per_energy[valid_bins_mask]\n    energies = energies[valid_bins_mask]\n    counts = counts[:, valid_bins_mask]\n    \n    # --- Multiple Histogram Reweighting (WHAM) ---\n    # Iteratively solve for the free-energy-related parameters f_j.\n    # The parameters f are related to the log of the partition functions.\n    f = np.zeros(R)\n\n    # Precompute terms for efficiency\n    beta_E_matrix = np.outer(sim_betas, energies)  # Shape (R, K')\n    log_num_samples = np.log(num_samples_per_sim)\n\n    max_iter = 1000\n    tolerance = 1e-12\n\n    for _ in range(max_iter):\n        f_old = f.copy()\n\n        # Step 1: Calculate the log of the density of states, g(E), using the current f_j.\n        # log g(E_i) = log(sum_j n_ij) - log(sum_j M_j exp(f_j - beta_j E_i))\n        f_broadcast = np.tile(f, (energies.size, 1)).T\n        log_denominators = logsumexp(log_num_samples + f_broadcast - beta_E_matrix, axis=0)\n        log_g = np.log(total_counts_per_energy) - log_denominators\n\n        # Step 2: Update f_k using the self-consistency equation.\n        # exp(-f_k) = sum_i g(E_i) exp(-beta_k E_i)\n        # We work with logs: f_k = -log(sum_i exp(log g_i - beta_k E_i))\n        log_g_broadcast = np.tile(log_g, (R, 1))\n        f_new_unnormalized = -logsumexp(log_g_broadcast - beta_E_matrix, axis=1)\n\n        # Normalize the new f_k to fix the gauge freedom (set f_1 = 0)\n        f = f_new_unnormalized - f_new_unnormalized[0]\n\n        # Check for convergence\n        if np.max(np.abs(f - f_old))  tolerance:\n            break\n            \n    # The last computed log_g is the final estimate from the converged f_j.\n\n    # --- Calculation of Specific Heat at Target Temperatures ---\n    target_temps = np.array([1.8, 2.0, 2.269, 2.5, 3.0, 3.2])\n    target_betas = 1.0 / target_temps\n\n    results = []\n    for beta in target_betas:\n        # Calculate log of terms in the partition function sum: log(g(E_i)) - beta * E_i\n        w = log_g - beta * energies\n\n        # Use logsumexp trick for numerical stability to compute expectation values\n        w_max = np.max(w)\n        exp_w_shifted = np.exp(w - w_max)\n        \n        # Denominator for expectation values is sum_i exp(w_i - w_max)\n        denominator = np.sum(exp_w_shifted)\n        \n        # Calculate expectation values of E and E^2\n        avg_E = np.sum(energies * exp_w_shifted) / denominator\n        avg_E2 = np.sum(energies**2 * exp_w_shifted) / denominator\n        \n        # Calculate variance and specific heat\n        variance = avg_E2 - avg_E**2\n        specific_heat = (beta**2 / N_spins) * variance\n        results.append(specific_heat)\n\n    # Final print statement\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了加权直方图分析方法（WHAM）的基本实现后，理解其应用的先决条件和局限性至关重要。这个练习是一个思想实验，它模拟了在实际研究中可能遇到的一个常见问题：数据丢失。通过分析这个情景，你将更深刻地理解为何相邻模拟窗口之间的数据重叠是WHAM方法能够成功的根本保证，这有助于你在未来的研究中做出正确的方法论选择。",
            "id": "2466530",
            "problem": "在一维反应坐标 $x$ 的伞形采样中，系统使用一系列以位置 $x_i$ 为中心的谐波偏置势 $w_i(x)$ 进行研究，以确保相邻偏置分布之间有足够的重叠。目标是使用加权直方图分析方法 (WHAM) 估算一个连续区间上的平均力势 (PMF) $W(x)$。在一个包含许多窗口的大规模计算中，其中一个中间窗口的轨迹和直方图数据被损坏且无法恢复，而所有其他窗口的采样似乎都按预期进行。对于 WHAM 分析，最佳的行动方案是什么？\n\nA. 排除损坏的窗口，对剩余的窗口运行 WHAM；只要存在一些重叠，WHAM 将从相邻窗口推断出缺失的区域。\n\nB. 重新运行缺失的伞形窗口（或扩展相邻窗口的采样，直到它们的直方图跨越间隙重叠），以在执行 WHAM 之前恢复连接的重叠网络；否则不应继续进行。\n\nC. 在后处理中将两个相邻窗口合并为单个伞形窗口，并在不进行任何额外模拟的情况下运行 WHAM 来填补间隙。\n\nD. 根据谐波偏置函数和假设的高斯分布为缺失的窗口构建一个合成直方图以保持连续性，然后将其包含在 WHAM 中。\n\nE. 在缺失窗口两侧的两个片段上分别运行 WHAM，并通过匹配它们在表观势垒顶部的数值来对齐得到的 PMF。",
            "solution": "问题陈述描述了在应用加权直方图分析方法 (WHAM) 进行平均力势 (PMF) 计算时一个常见但关键的问题。该方法的有效性取决于输入数据的特定要求。我们首先将验证问题陈述本身。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n-   系统涉及一个一维反应坐标 $x$。\n-   使用一系列以位置 $x_i$ 为中心的谐波偏置势 $w_i(x)$ 进行伞形采样。\n-   目标是使用 WHAM 计算 PMF, $W(x)$。\n-   一个关键条件是“相邻偏置分布之间有足够的重叠”。\n-   发生了一次数据损坏事件：1 个中间窗口的数据“被损坏且无法恢复”。\n-   所有其他窗口的数据被认为是有效的。\n-   问题要求“最佳的行动方案”。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题具有科学依据。使用谐波偏置的伞形采样及后续的 WHAM 分析是计算化学和物理学中用于自由能计算的经典技术。丢失中间窗口数据的情景是一个现实的实践挑战。该问题提法得当，因为它要求基于 WHAM 原则的正确方法论程序。它是客观的，不包含任何科学上不合理的假设或歧义。在此背景下，“最佳的行动方案”指的是最严谨和科学上最有效的方法。\n\n**步骤3：结论与行动**\n问题陈述是有效的。它描述了计算统计力学中一个清晰、可解决的问题。我们可以继续进行解答。\n\n### 推导与分析\n\n平均力势 $W(x)$ 与沿反应坐标 $x$ 的平衡概率分布 $P(x)$ 通过表达式 $W(x) = -k_B T \\ln P(x) + C$ 相关联，其中 $C$ 是一个任意常数。在伞形采样中，我们执行 $N_{win}$ 次模拟，每次都用一个偏置势 $w_i(x)$ 进行偏置，其中 $i=1, ..., N_{win}$。在窗口 $i$ 中得到的偏置概率分布是 $p_i(x)$。\n\nWHAM 方程提供了一个框架，用于组合所有 $N_{win}$ 个窗口的数据，以获得全局、无偏分布 $P(x)$ 的最佳估计。其核心方程是：\n$$ P(x) = \\frac{\\sum_{i=1}^{N_{win}} n_i(x)}{\\sum_{j=1}^{N_{win}} N_j \\exp(-\\beta[w_j(x) - F_j])} $$\n$$ \\exp(-\\beta F_i) = \\int P(x') \\exp(-\\beta w_i(x')) dx' $$\n这里，$n_i(x)$ 是窗口 $i$ 中构象的直方图，$N_j$ 是窗口 $j$ 中的总样本数，$\\beta = 1/(k_B T)$，$F_j$ 是与每个偏置势相关的自由能。这些方程必须针对 $P(x)$ 和集合 $\\{F_j\\}$ 进行自洽求解。\n\n关键的洞见在于如何确定自由能 $F_j$。它们只能在一个总体的加性常数内被确定。其相对值 $F_j - F_k$ 由采样分布的重叠区域决定。如果窗口集合可以被划分为两个或多个子集，使得一个子集中的任何窗口的采样分布都不与另一个子集的分布重叠，那么这些子集之间的相对自由能就是不确定的。\n\n在所描述的场景中，丢失一个中间窗口（比如窗口 $k$）会在窗口 $k-1$ 和窗口 $k+1$ 之间的采样中造成一个间隙。如果这两个窗口的直方图 $n_{k-1}(x)$ 和 $n_{k+1}(x)$ 在 $x$ 坐标上没有一个两者都非零的区域，那么整个窗口集合就被划分为两个不相连的组：$\\{1, ..., k-1\\}$ 和 $\\{k+1, ..., N_{win}\\}$。WHAM 算法将无法确定这两组之间的自由能偏移量，从而导致两个独立的、具有任意垂直位移的 PMF 片段。无法构建覆盖整个范围的连续 PMF。\n\n现在，我们基于这个原则来评估每个选项。\n\n**A. 排除损坏的窗口，对剩余的窗口运行 WHAM；只要存在一些重叠，WHAM 将从相邻窗口推断出缺失的区域。**\n这个说法是根本错误的。WHAM 是一种分析方法，而不是一个为没有数据的区域进行推断的引擎。如果窗口 $k$ 的丢失导致窗口 $k-1$ 和窗口 $k+1$ 之间没有重叠，WHAM 无法连接由此产生的两个数据片段。它将生成两条具有未知相对位移的 PMF 曲线。即使存在微小且统计上不可靠的重叠，两个片段之间的连接也将极其不可靠，导致全局 PMF 出现巨大误差。WHAM 不会跨越数据间隙进行“推断”或“插值”。\n**结论：错误。**\n\n**B. 重新运行缺失的伞形窗口（或扩展相邻窗口的采样，直到它们的直方图跨越间隙重叠），以在执行 WHAM 之前恢复连接的重叠网络；否则不应继续进行。**\n这是唯一科学上严谨的行动方案。一个有效的 WHAM 计算的先决条件是，存在一个跨越整个反应坐标的、由重叠的直方图构成的连通网络。一个窗口的丢失会打破这种连通性。重新建立它的唯一方法是生成缺失的数据。这可以通过重新运行丢失窗口 $k$ 的模拟，或者通过扩展窗口 $k-1$ 和 $k+1$ 的模拟，使其采样区域扩大并最终充分重叠来实现。只有拥有完整且连通的数据集，才能正确应用 WHAM，从而得出一个单一、连续且可靠的 PMF。这种方法维护了科学可重复性和方法论正确性的标准。\n**结论：正确。**\n\n**C. 在后处理中将两个相邻窗口合并为单个伞形窗口，并在不进行任何额外模拟的情况下运行 WHAM 来填补间隙。**\n这个提议是荒谬的。直方图 $n_{k-1}(x)$ 和 $n_{k+1}(x)$ 是在两个不同的偏置势 $w_{k-1}(x)$ 和 $w_{k+1}(x)$ 下生成的。简单地将它们相加（“合并”）是一个在数学上无效的操作，它忽略了其背后的物理原理。每个直方图都是一组有条件的观测结果。要组合它们，恰恰需要 WHAM 形式体系，而正如已经确定的，如果它们不重叠，WHAM 就会失效。这种“解决方案”是对数据的任意且无意义的操纵。\n**结论：错误。**\n\n**D. 根据谐波偏置函数和假设的高斯分布为缺失的窗口构建一个合成直方图以保持连续性，然后将其包含在 WHAM 中。**\n这是一种数据捏造行为。偏置概率分布是 $p_i(x) \\propto \\exp(-\\beta[W(x) + w_i(x)])$。假设 $p_i(x)$ 是一个简单的高斯分布，就等于对我们希望计算的量 $W(x)$ 做出了一个强烈的假设。具体来说，这相当于假设 $W(x)$ 在该区域本身就是一个抛物线。这在计算中引入了一个不可控、无法检验且几乎肯定是错误的假设，从根本上破坏了计算的目的，即从模拟数据中*发现* $W(x)$，而不是强加一个 $W(x)$。这种程序在科学上是不可接受的。\n**结论：错误。**\n\n**E. 在缺失窗口两侧的两个片段上分别运行 WHAM，并通过匹配它们在表观势垒顶部的数值来对齐得到的 PMF。**\n这个程序正确地指出了数据间隙的后果——产生了两个独立的 PMF 片段。然而，其提出的对齐方案是临时性的且不可靠。无法保证“势垒顶部”是正确的对齐点，甚至无法保证它被任一数据片段采样到。对齐需要找到两部分之间正确的自由能偏移量，而这个量在没有连接它们的数据的情况下是无法确定的。任何对齐方案，无论是在势垒、最小值处，还是通过其他一些标准，都是一种脱离 WHAM 方法严谨性的、不受控制的近似。虽然在不得已的情况下有时会使用，但这绝不是“最佳”或最正确的行动方案，最佳方案应该是生成缺失的数据。\n**结论：错误。**\n\n总而言之，WHAM 程序的完整性要求一个由重叠分布构成的连通集合。任何未能通过生成新的、有效的模拟数据来恢复这种连通性的行为，都构成严重的方法论缺陷。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "最后一个练习是一个更高级、更全面的综合性项目，它将带你从一维能量重加权走向更复杂的二维自由能面重构。在计算化学和生物物理学中，计算反应坐标描述下的自由能面是一个核心任务。这个练习不仅要求你实现一个完整的二维WHAM程序，还引入了非正交反应坐标这一真实世界中常见复杂情况，并要求你完成从生成模拟数据、进行数据分析到与解析解进行比较验证的完整科学计算流程。",
            "id": "2401621",
            "problem": "要求您实现一个完整的、可复现的程序，当反应坐标非正交时，使用加权直方图分析方法（WHAM）构建二维自由能面。您的实现必须基于统计力学和概率论的第一性原理，并且不得依赖任何专门的 WHAM 库。该程序必须从一个已知的无偏系统中生成合成的伞形采样数据，使用统计上一致的权重合并直方图，并根据解析参考对结果进行验证。所有能量都必须以 $k_\\mathrm{B} T$ 为单位表示，因此是无量纲的。不涉及角度。最终输出必须是单行文本，包含一个如下文指定的浮点数列表。\n\n您的推导和实现的基本依据必须是：\n- 玻尔兹曼分布 $p(\\mathbf{z}) \\propto \\exp\\{-\\beta U(\\mathbf{z})\\}$，其中 $\\beta = 1/(k_\\mathrm{B} T)$，$\\mathbf{z} = (x,y)^\\top$ 表示微观坐标。\n- 添加了偏置势的伞形采样原理，该原理通过移动采样来改善对感兴趣区域的覆盖。\n- 基于统计一致性和最大似然论的离散直方图重加权法。\n\n您必须构建一个模型，其中无偏势能是一个包含耦合项的二维二次型，\n$$\nU_0(\\mathbf{z}) = \\tfrac{1}{2}\\,\\mathbf{z}^\\top \\mathbf{K}\\,\\mathbf{z}, \\quad \\mathbf{K} = \\begin{pmatrix} k_x  k_{xy} \\\\ k_{xy}  k_y \\end{pmatrix},\n$$\n并且反应坐标是线性和非正交的，\n$$\n\\mathbf{s}(\\mathbf{z}) = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = \\mathbf{A}\\,\\mathbf{z}, \\quad \\mathbf{A} = \\begin{pmatrix} 1  0 \\\\ 1  \\alpha \\end{pmatrix},\n$$\n因此 $s_1 = x$ 且 $s_2 = x + \\alpha y$，其中 $\\alpha \\neq 0$。对于每个伞形窗 $i$，偏置在两个反应坐标上都是谐振的，\n$$\nU^{(i)}_\\mathrm{b}(\\mathbf{s}) = \\tfrac{1}{2}\\,k_1\\,\\big(s_1 - c^{(i)}_1\\big)^2 + \\tfrac{1}{2}\\,k_2\\,\\big(s_2 - c^{(i)}_2\\big)^2.\n$$\n窗 $i$ 中的总偏置能量为 $U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))$。由于 $U_0$ 和 $U^{(i)}_\\mathrm{b}$ 都是二次的，且 $\\mathbf{s}$ 是线性的，因此 $\\mathbf{z}$ 中的偏置分布是高斯分布，可以进行精确采样。\n\n您的程序必须：\n- 对于每个指定的测试用例，通过从与 $\\exp\\{-\\beta[U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))]\\}$ 成正比的高斯分布中采样，在每个窗 $i$ 中生成独立样本，并使用给定的精确随机种子。您必须从第一性原理推导出相关的高斯均值和协方差；除了使用您推导出的参数进行多元正态分布采样外，不要使用任何黑盒采样器。\n- 在具有给定边界和区间数量的均匀网格上离散化 $\\mathbf{s}$ 空间，为 $\\mathbf{s}$ 中的每个窗构建一个独立的二维直方图，然后使用统计上一致的 WHAM 不动点迭代合并直方图，以估计网格上的无偏概率 $P(\\mathbf{s})$。\n- 将 $P(\\mathbf{s})$ 转换为自由能 $F(\\mathbf{s}) = -\\ln P(\\mathbf{s})$（以 $k_\\mathrm{B} T$ 为单位），并平移 $F(\\mathbf{s})$，使其在已访问的网格区间上的最小值为零。\n- 基于在 $\\mathbf{s} = \\mathbf{A}\\mathbf{z}$ 下无偏分布的精确高斯变换，计算解析参考自由能 $F_\\mathrm{ref}(\\mathbf{s})$，并将其平移，使其在相同的已访问区间上的最小值为零。\n- 在所有窗的总计数至少达到指定阈值（使用阈值计数 $\\geq 5$）的网格区间上，计算 $F(\\mathbf{s})$ 和 $F_\\mathrm{ref}(\\mathbf{s})$ 之间的均方根误差（RMSE）。每个测试用例的 RMSE 是一个浮点数。\n\n测试套件。为以下三种情况实现上述要求。在所有情况下，使用 $\\beta = 1$（即以 $k_\\mathrm{B} T$ 作为能量单位）和相同的无偏刚度矩阵，\n- 无偏刚度：$\\mathbf{K} = \\begin{pmatrix} 1.5  0.3 \\\\ 0.3  1.0 \\end{pmatrix}$。\n\n案例 1（理想情况）：\n- 反应坐标非正交性：$\\alpha = 0.5$。\n- 伞形弹簧常数：$k_1 = 4.0$, $k_2 = 2.0$。\n- 伞形中心：$\\{c^{(i)}_1\\} \\in \\{-1.2, 0.0, 1.2\\}$ 和 $\\{c^{(i)}_2\\} \\in \\{-1.2, 0.0, 1.2\\}$，形成一个 $3 \\times 3$ 的窗格网格。\n- 每窗样本数：对于所有 $i$，$N_i = 4000$。\n- s中的直方图网格：$s_1 \\in [-2.0, 2.0]$，41 个区间；$s_2 \\in [-2.0, 2.0]$，41 个区间。\n- 随机种子：$123$。\n\n案例 2（更强的非正交性和更硬的伞形势）：\n- 反应坐标非正交性：$\\alpha = 0.9$。\n- 伞形弹簧常数：$k_1 = 6.0$, $k_2 = 6.0$。\n- 伞形中心：$\\{c^{(i)}_1\\} \\in \\{-1.8, 0.0, 1.8\\}$ 和 $\\{c^{(i)}_2\\} \\in \\{-1.8, 0.0, 1.8\\}$，形成一个 $3 \\times 3$ 的网格。\n- 每窗样本数：对于所有 $i$，$N_i = 1500$。\n- s中的直方图网格：$s_1 \\in [-3.0, 3.0]$，45 个区间；$s_2 \\in [-3.0, 3.0]$，45 个区间。\n- 随机种子：$456$。\n\n案例 3（窗口和样本数较少的边缘情况）：\n- 反应坐标非正交性：$\\alpha = 0.2$。\n- 伞形弹簧常数：$k_1 = 3.0$, $k_2 = 3.0$。\n- 伞形中心：$\\{c^{(i)}_1\\} \\in \\{-1.2, 1.2\\}$ 和 $\\{c^{(i)}_2\\} \\in \\{-1.2, 1.2\\}$，形成一个 $2 \\times 2$ 的网格。\n- 每窗样本数：对于所有 $i$，$N_i = 600$。\n- s中的直方图网格：$s_1 \\in [-2.0, 2.0]$，35 个区间；$s_2 \\in [-2.0, 2.0]$，35 个区间。\n- 随机种子：$789$。\n\n您必须遵守的算法细节：\n- 对于每个窗 $i$ 中的采样，推导由 $U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))$ 所隐含的、形式为 $\\mathcal{N}(\\boldsymbol{\\mu}^{(i)}, \\boldsymbol{\\Sigma}^{(i)})$ 的 $\\mathbf{z}$ 中偏置分布的高斯均值和协方差，并使用指定的种子精确抽取 $N_i$ 个样本。\n- 使用上文描述的精确区间边界，为每个窗在 $(s_1,s_2)$ 中构建独立的二维直方图。\n- 在离散网格上实现 WHAM 不动点迭代，求解一组偏移量 $\\{f_i\\}$ 和网格上的无偏概率 $P(\\mathbf{s})$，使用稳定的数值程序。将 $P(\\mathbf{s})$ 归一化，使其在所有已访问区间上的总和为 1。\n- 构建 $F(\\mathbf{s}) = -\\ln P(\\mathbf{s})$ 并将其平移，使得在已访问区间上的 $\\min F = 0$。\n- 构建由无偏高斯分布和线性变换 $\\mathbf{s} = \\mathbf{A}\\mathbf{z}$ 所隐含的解析参考 $F_\\mathrm{ref}(\\mathbf{s})$，并将其平移，使得在相同的已访问区间上的 $\\min F_\\mathrm{ref} = 0$。\n- 在所有窗的总计数至少为 5 的区间上，计算 $F(\\mathbf{s})$ 和 $F_\\mathrm{ref}(\\mathbf{s})$ 之间的 RMSE。\n\n最终输出格式：\n- 您的程序必须生成单行输出，其中包含案例 1、2 和 3 的三个 RMSE 值，四舍五入到六位小数，以逗号分隔的列表形式，并用方括号括起来，例如 [0.012345,0.067890,0.123456]。\n\n执行约束：\n- 程序必须是自包含的，不需要用户输入，也不读取外部文件。\n- 使用另行指定的编程语言和库约束。",
            "solution": "该问题要求实现加权直方图分析方法（WHAM），以从人工生成的伞形采样数据中重建二维自由能面。反应坐标被指定为底层微观坐标的非正交线性组合。解决方案必须从统计力学的第一性原理推导得出，实现时不得依赖专门的外部 WHAM 库，并需与精确的解析结果进行验证。所有能量都是无量纲的，以 $k_\\mathrm{B} T$ 为单位表示，这等效于设置 $\\beta = (k_\\mathrm{B} T)^{-1} = 1$。\n\n### 1. 理论框架\n\n**1.1. 系统定义**\n系统的无偏势能是笛卡尔坐标 $\\mathbf{z} = (x, y)^\\top$ 的二次型：\n$$\nU_0(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^\\top \\mathbf{K}\\,\\mathbf{z}\n$$\n其中 $\\mathbf{K}$ 是一个对称正定刚度矩阵。相应的无偏概率分布是一个均值为零、协方差矩阵为 $\\mathbf{\\Sigma_z} = \\mathbf{K}^{-1}$ 的多元正态（高斯）分布：\n$$\np_0(\\mathbf{z}) \\propto \\exp(-U_0(\\mathbf{z})) \\implies \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}^{-1})\n$$\n\n反应坐标 $\\mathbf{s} = (s_1, s_2)^\\top$ 是 $\\mathbf{z}$ 的一个线性变换：\n$$\n\\mathbf{s}(\\mathbf{z}) = \\mathbf{A}\\,\\mathbf{z}\n$$\n其中 $\\mathbf{A}$ 是一个可逆矩阵，表示反应坐标通常情况下是非正交的。\n\n伞形采样在一组 $M$ 个模拟窗中进行，索引为 $i = 1, \\dots, M$。在每个窗 $i$ 中，添加一个谐振偏置势 $U^{(i)}_\\mathrm{b}(\\mathbf{s})$ 来将系统约束在反应坐标空间中的一个中心 $\\mathbf{c}^{(i)} = (c^{(i)}_1, c^{(i)}_2)^\\top$ 附近：\n$$\nU^{(i)}_\\mathrm{b}(\\mathbf{s}) = \\frac{1}{2} (\\mathbf{s} - \\mathbf{c}^{(i)})^\\top \\mathbf{K}_\\mathrm{b} (\\mathbf{s} - \\mathbf{c}^{(i)})\n$$\n其中 $\\mathbf{K}_\\mathrm{b} = \\mathrm{diag}(k_1, k_2)$ 是谐振力常数的对角矩阵。\n\n**1.2. 用于数据生成的偏置分布**\n窗 $i$ 中的总势能为 $U^{(i)}(\\mathbf{z}) = U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))$。为了生成样本，我们必须确定所得概率分布 $p^{(i)}(\\mathbf{z}) \\propto \\exp(-U^{(i)}(\\mathbf{z}))$ 的参数。代入定义，我们得到：\n$$\nU^{(i)}(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^\\top \\mathbf{K}\\,\\mathbf{z} + \\frac{1}{2} (\\mathbf{A}\\mathbf{z} - \\mathbf{c}^{(i)})^\\top \\mathbf{K}_\\mathrm{b} (\\mathbf{A}\\mathbf{z} - \\mathbf{c}^{(i)})\n$$\n展开第二项得到：\n$$\nU^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z})) = \\frac{1}{2}\\mathbf{z}^\\top (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A}) \\mathbf{z} - \\mathbf{z}^\\top (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}) + \\frac{1}{2}(\\mathbf{c}^{(i)})^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}\n$$\n合并各项，总势能为：\n$$\nU^{(i)}(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^\\top (\\mathbf{K} + \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A}) \\mathbf{z} - \\mathbf{z}^\\top (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}) + \\mathrm{const.}\n$$\n这是高斯分布的指数部分。对于变量 $\\mathbf{z}$，一个均值为 $\\boldsymbol{\\mu}$、协方差为 $\\mathbf{\\Sigma}$ 的通用高斯密度与 $\\exp(-\\frac{1}{2}(\\mathbf{z}-\\boldsymbol{\\mu})^\\top\\mathbf{\\Sigma}^{-1}(\\mathbf{z}-\\boldsymbol{\\mu}))$ 成正比，展开后为 $\\exp(-\\frac{1}{2}\\mathbf{z}^\\top\\mathbf{\\Sigma}^{-1}\\mathbf{z} + \\mathbf{z}^\\top\\mathbf{\\Sigma}^{-1}\\boldsymbol{\\mu} + \\mathrm{const.})$。通过将此形式与 $\\exp(-U^{(i)}(\\mathbf{z}))$ 进行比较，我们确定窗 $i$ 中偏置分布的逆协方差和均值：\n$$\n\\mathbf{\\Sigma}^{(i)^{-1}} = \\mathbf{K} + \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A}\n$$\n$$\n\\mathbf{\\Sigma}^{(i)^{-1}} \\boldsymbol{\\mu}^{(i)} = \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}\n$$\n求解参数可得出窗 $i$ 中 $\\mathbf{z}$ 的采样分布：\n$$\n\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}^{(i)}, \\mathbf{\\Sigma}^{(i)})\n$$\n其协方差为 $\\mathbf{\\Sigma}^{(i)} = (\\mathbf{K} + \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A})^{-1}$，均值为 $\\boldsymbol{\\mu}^{(i)} = \\mathbf{\\Sigma}^{(i)} (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)})$。\n\n**1.3. 解析参考自由能**\n反应坐标 $\\mathbf{s}$ 的无偏分布可以从高斯变量线性变换的性质中得到。由于 $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}^{-1})$ 且 $\\mathbf{s} = \\mathbf{A}\\mathbf{z}$，$\\mathbf{s}$ 的分布也是高斯的，即 $\\mathbf{s} \\sim \\mathcal{N}(\\boldsymbol{\\mu_s}, \\mathbf{\\Sigma_s})$，其均值为 $\\boldsymbol{\\mu_s} = \\mathbf{A}\\boldsymbol{\\mu_z} = \\mathbf{0}$，协方差为 $\\mathbf{\\Sigma_s} = \\mathbf{A} \\mathbf{\\Sigma_z} \\mathbf{A}^\\top = \\mathbf{A} \\mathbf{K}^{-1} \\mathbf{A}^\\top$。\n\n概率密度为 $p(\\mathbf{s}) \\propto \\exp(-\\frac{1}{2}\\mathbf{s}^\\top \\mathbf{\\Sigma_s}^{-1} \\mathbf{s})$。解析自由能定义为 $F(\\mathbf{s}) = -\\ln p(\\mathbf{s})$（以 $k_\\mathrm{B} T$ 为单位）。因此，除去一个无关的加性常数：\n$$\nF_\\mathrm{ref}(\\mathbf{s}) = \\frac{1}{2}\\mathbf{s}^\\top (\\mathbf{A} \\mathbf{K}^{-1} \\mathbf{A}^\\top)^{-1} \\mathbf{s}\n$$\n这为 WHAM 重建提供了用于比较的理论基准。\n\n**1.4. WHAM 实现**\n来自所有 $M$ 个窗的数据使用 WHAM 进行合并。反应坐标空间 $\\mathbf{s}$ 被离散化为由 $j$ 索引的区间网格。设 $H_{ij}$ 是从窗 $i$ 落入区间 $j$ 的样本数，而 $N_i = \\sum_j H_{ij}$ 是来自窗 $i$ 的总样本数。WHAM 的目标是找到每个区间 $j$ 的无偏概率 $P_j$ 和每个窗 $i$ 的无量纲自由能偏移量 $f_i$。这些由自洽方程确定：\n$$\nP_j = \\frac{\\sum_{i=1}^M H_{ij}}{\\sum_{i=1}^M N_i \\exp(f_i - U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))}\n$$\n$$\n\\exp(-f_i) = \\sum_j P_j \\exp(-U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))\n$$\n其中 $\\mathbf{s}_j$ 是区间 $j$ 中心的坐标。这些方程通过迭代求解。为保证数值稳定性，计算在对数空间中进行。\n\n迭代过程如下：\n1.  初始化所有 $i=1, \\dots, M$ 的 $f_i = 0$。\n2.  重复更新 $\\{f_i\\}$ 和 $\\{P_j\\}$ 直至收敛：\n    a. 对于每个已访问的区间 $j$（其中 $\\sum_i H_{ij} > 0$），使用当前的 $\\{f_i\\}$ 计算未归一化的概率 $P'_j$：\n       $$\n       \\ln P'_j = \\ln\\left(\\sum_i H_{ij}\\right) - \\mathrm{logsumexp}_i(\\ln N_i + f_i - U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))\n       $$\n    b. 在所有已访问区间的集合上对概率进行归一化：\n       $$\n       \\ln P_j = \\ln P'_j - \\mathrm{logsumexp}_{k \\in \\text{visited}} (\\ln P'_k)\n       $$\n    c. 使用新的概率 $\\{P_j\\}$ 更新自由能偏移量 $\\{f_i\\}$：\n       $$\n       f_i^{\\text{new}} = -\\mathrm{logsumexp}_{j \\in \\text{visited}} (\\ln P_j - U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))\n       $$\n    d. 为防止数值漂移，锚定自由能，例如 $f_i \\leftarrow f_i^{\\text{new}} - f_1^{\\text{new}}$。\n    e. 检查收敛性，例如，要求任何 $f_i$ 的最大绝对变化量小于一个很小的容差（如 $10^{-9}$）。\n\n**1.5. 自由能面与误差计算**\n一旦获得收敛的概率 $P_j$，自由能面就计算为 $F_j = -\\ln P_j$。该曲面仅对已访问的区间有定义；否则 $F_j$ 为无穷大。为了便于比较，将曲面进行平移，使其在已访问区间上的最小值为零。\n\n解析参考自由能 $F_{\\mathrm{ref}, j}$ 在区间中心 $\\mathbf{s}_j$ 处进行评估。然后通过减去其在相同已访问区间集合上计算出的最小值来进行平移。这确保了误差计算有一个一致的参考基准。\n\n计算均方根误差（RMSE）以量化重建的准确性。比较范围仅限于总样本数 $\\sum_i H_{ij}$ 至少为 5 的区间，以排除统计上不可靠的区域。设 $\\mathcal{J}$ 为此类区间的集合。RMSE 为：\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{|\\mathcal{J}|} \\sum_{j \\in \\mathcal{J}} (F_j - F_{\\mathrm{ref}, j})^2}\n$$\n该程序将应用于问题陈述中指定的三个测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef run_analysis(K_unbiased, alpha, k_bias, c1_centers, c2_centers, n_samples_per_window, s_lims, n_bins, seed):\n    \"\"\"\n    Performs a complete WHAM analysis for a single test case.\n    \"\"\"\n    # 1. Setup system parameters\n    rng = np.random.default_rng(seed)\n    A = np.array([[1, 0], [1, alpha]])\n    K_b = np.diag(k_bias)\n    \n    # Generate the grid of umbrella window centers\n    window_centers = np.array(np.meshgrid(c1_centers, c2_centers)).T.reshape(-1, 2)\n    n_windows = len(window_centers)\n    \n    # Pre-calculate matrices for sampling distribution\n    # These are constant across all windows for this problem\n    # Sigma_inv = K + A.T @ K_b @ A\n    Sigma_inv_biased = K_unbiased + A.T @ K_b @ A\n    Sigma_biased = np.linalg.inv(Sigma_inv_biased)\n    \n    # Setup histogram grid\n    s1_edges = np.linspace(s_lims[0][0], s_lims[0][1], n_bins[0] + 1)\n    s2_edges = np.linspace(s_lims[1][0], s_lims[1][1], n_bins[1] + 1)\n    s1_centers = (s1_edges[:-1] + s1_edges[1:]) / 2.0\n    s2_centers = (s2_edges[:-1] + s2_edges[1:]) / 2.0\n    \n    hist_shape = (n_bins[0], n_bins[1])\n    n_bins_total = n_bins[0] * n_bins[1]\n\n    # All histograms and related data will be stored flattened for easier processing\n    all_histograms = np.zeros((n_windows, n_bins_total))\n    \n    # 2. Generate samples and build histograms\n    for i in range(n_windows):\n        c_i = window_centers[i]\n        \n        # Derive mean of the biased distribution for window i\n        mu_biased = Sigma_biased @ A.T @ K_b @ c_i\n        \n        # Sample from the biased distribution in z-space\n        z_samples = rng.multivariate_normal(mu_biased, Sigma_biased, size=n_samples_per_window)\n        \n        # Transform samples to reaction coordinate space s\n        s_samples = z_samples @ A.T\n        \n        # Build 2D histogram for this window\n        hist, _, _ = np.histogram2d(s_samples[:, 0], s_samples[:, 1], bins=[s1_edges, s2_edges])\n        all_histograms[i, :] = hist.flatten()\n\n    # 3. WHAM Calculation\n    S1_grid, S2_grid = np.meshgrid(s1_centers, s2_centers, indexing='ij')\n    s_coords = np.vstack([S1_grid.ravel(), S2_grid.ravel()]).T\n\n    # Pre-calculate bias energies for all bins and all windows\n    U_bias_all = np.zeros((n_windows, n_bins_total))\n    for i in range(n_windows):\n        c_i = window_centers[i]\n        ds1 = s_coords[:, 0] - c_i[0]\n        ds2 = s_coords[:, 1] - c_i[1]\n        U_bias_all[i, :] = 0.5 * k_bias[0] * ds1**2 + 0.5 * k_bias[1] * ds2**2\n        \n    total_counts = np.sum(all_histograms, axis=0)\n    visited_mask = total_counts > 0\n    \n    counts_visited = total_counts[visited_mask]\n    U_bias_visited = U_bias_all[:, visited_mask]\n    n_samples_per_window_arr = np.full(n_windows, n_samples_per_window)\n\n    f = np.zeros(n_windows)\n    \n    # WHAM fixed-point iteration\n    for iteration in range(2000): # More than enough for convergence\n        f_old = np.copy(f)\n        \n        # Calculate unnormalized probabilities P' in log space\n        log_numer = np.log(counts_visited)\n        \n        arg_logsumexp_i = np.log(n_samples_per_window_arr)[:, np.newaxis] + f[:, np.newaxis] - U_bias_visited\n        log_denom = logsumexp(arg_logsumexp_i, axis=0)\n        \n        log_P_unnorm = log_numer - log_denom\n        \n        # Normalize probabilities P over visited bins\n        log_Z_P = logsumexp(log_P_unnorm)\n        log_P = log_P_unnorm - log_Z_P\n        \n        # Update free energy offsets f\n        arg_logsumexp_j = log_P[np.newaxis, :] - U_bias_visited\n        f = -logsumexp(arg_logsumexp_j, axis=1)\n        f -= f[0] # Anchor the free energies\n        \n        # Check for convergence\n        if np.max(np.abs(f - f_old))  1e-9:\n            break\n            \n    # 4. Construct Free Energy Surface (FES)\n    F_wham = np.full(n_bins_total, np.inf)\n    F_wham[visited_mask] = -log_P\n    F_wham -= np.min(F_wham[visited_mask])\n\n    # 5. Construct Analytical Reference FES\n    K_inv_unbiased = np.linalg.inv(K_unbiased)\n    Sigma_s = A @ K_inv_unbiased @ A.T\n    K_s = np.linalg.inv(Sigma_s)\n    \n    F_ref = 0.5 * np.sum((s_coords @ K_s) * s_coords, axis=1)\n    F_ref -= np.min(F_ref[visited_mask])\n    \n    # 6. Calculate RMSE\n    rmse_mask = total_counts >= 5\n    if np.sum(rmse_mask) > 0:\n        diff_sq = (F_wham[rmse_mask] - F_ref[rmse_mask])**2\n        rmse = np.sqrt(np.mean(diff_sq))\n    else:\n        rmse = 0.0 # Or nan, depending on convention for no valid bins.\n        \n    return rmse\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the WHAM analysis.\n    \"\"\"\n    K_unbiased = np.array([[1.5, 0.3], [0.3, 1.0]])\n\n    test_cases = [\n        # Case 1\n        {\n            \"alpha\": 0.5, \"k_bias\": [4.0, 2.0],\n            \"c1_centers\": [-1.2, 0.0, 1.2], \"c2_centers\": [-1.2, 0.0, 1.2],\n            \"n_samples_per_window\": 4000,\n            \"s_lims\": [[-2.0, 2.0], [-2.0, 2.0]], \"n_bins\": [41, 41],\n            \"seed\": 123\n        },\n        # Case 2\n        {\n            \"alpha\": 0.9, \"k_bias\": [6.0, 6.0],\n            \"c1_centers\": [-1.8, 0.0, 1.8], \"c2_centers\": [-1.8, 0.0, 1.8],\n            \"n_samples_per_window\": 1500,\n            \"s_lims\": [[-3.0, 3.0], [-3.0, 3.0]], \"n_bins\": [45, 45],\n            \"seed\": 456\n        },\n        # Case 3\n        {\n            \"alpha\": 0.2, \"k_bias\": [3.0, 3.0],\n            \"c1_centers\": [-1.2, 1.2], \"c2_centers\": [-1.2, 1.2],\n            \"n_samples_per_window\": 600,\n            \"s_lims\": [[-2.0, 2.0], [-2.0, 2.0]], \"n_bins\": [35, 35],\n            \"seed\": 789\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        rmse = run_analysis(K_unbiased=K_unbiased, **case)\n        results.append(rmse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}