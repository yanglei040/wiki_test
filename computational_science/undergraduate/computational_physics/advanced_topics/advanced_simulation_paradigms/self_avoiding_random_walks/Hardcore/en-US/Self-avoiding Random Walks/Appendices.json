{
    "hands_on_practices": [
        {
            "introduction": "Understanding the macroscopic properties of self-avoiding walks often begins with direct, \"brute-force\" enumeration for small systems. This exercise challenges you to compute the average spatial extent, or \"span,\" of a walk by generating every possible configuration up to a small number of steps, $N$. By performing this exact calculation, you will observe firsthand the combinatorial explosion that makes this method impractical for large $N$, and you will uncover the first hints of the fundamental power-law scaling, $\\langle S_N \\rangle \\propto N^{\\nu}$, that governs the physics of long polymer chains .",
            "id": "2436367",
            "problem": "You are asked to study the span of a Self-Avoiding Walk (SAW) on a two-dimensional square lattice in a way that is precise, reproducible, and testable. A Self-Avoiding Walk is a sequence of lattice sites in which each step moves by one unit to one of the four nearest neighbors and no site is visited more than once. Consider walks that start at the origin $(0,0)$, have exactly $N$ unit steps, and lie on the infinite square lattice.\n\nFor a walk of length $N$, denote the visited positions by $\\{(x_i,y_i)\\}_{i=0}^{N}$ with $(x_0,y_0)=(0,0)$ and $\\|(x_{i+1},y_{i+1})-(x_i,y_i)\\|_1=1$ for all $i$, and $(x_i,y_i)\\neq(x_j,y_j)$ whenever $i\\neq j$. Define the span in the $x$-direction by\n$$\nS_N=\\max_{0\\le i\\le N} x_i - \\min_{0\\le i\\le N} x_i.\n$$\nLet $\\langle S_N\\rangle$ denote the arithmetic mean of $S_N$ taken over all self-avoiding walks of length $N$ that start at the origin, with each walk assigned equal weight.\n\nTasks:\n1. For each specified value of $N$, compute the exact value of $\\langle S_N\\rangle$.\n2. Using two specified values $N_1$ and $N_2$ with $N_2N_1\\ge 1$, compute the scaling exponent estimate $\\nu$ defined by the pairwise log-slope\n$$\n\\nu=\\frac{\\log\\big(\\langle S_{N_2}\\rangle/\\langle S_{N_1}\\rangle\\big)}{\\log\\big(N_2/N_1\\big)}.\n$$\n\nTest suite:\n- Case $1$: $N=0$.\n- Case $2$: $N=1$.\n- Case $3$: $N=2$.\n- Case $4$: $N=5$.\n- Case $5$: $N=8$.\n- Case $6$: $N=10$.\n- Case $7$: $(N_1,N_2)=(4,10)$, for which the program must compute $\\nu$ using the definition above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order and formatting:\n- $[\\langle S_0\\rangle,\\langle S_1\\rangle,\\langle S_2\\rangle,\\langle S_5\\rangle,\\langle S_8\\rangle,\\langle S_{10}\\rangle,\\nu]$,\n- each number must be rounded to exactly six digits after the decimal point,\n- no spaces are permitted in the output line.\n\nNo physical units are involved. Angles are not used. Percentages are not used. All quantities are pure numbers. The expected answers for each case are real numbers (floats) as defined above.",
            "solution": "The problem statement has been rigorously validated. It is scientifically sound, well-posed, and unambiguous, representing a standard computational exercise in the study of self-avoiding walks. A solution can therefore be derived. The task is to compute the mean span $\\langle S_N \\rangle$ for a Self-Avoiding Walk (SAW) of $N$ steps on a two-dimensional square lattice, and a related scaling exponent $\\nu$.\n\n**1. Theoretical Framework**\n\nA Self-Avoiding Walk of length $N$ is a sequence of $N+1$ distinct lattice sites $p_0, p_1, \\ldots, p_N$, where $p_0$ is the origin $(0,0)$ and each subsequent site $p_{i+1}$ is a nearest neighbor of $p_i$. The distance between adjacent sites satisfies $\\|p_{i+1} - p_i\\|_1 = 1$. The span of a walk in the $x$-direction is defined as $S_N = \\max_{i} x_i - \\min_{i} x_i$, where $p_i = (x_i, y_i)$.\n\nThe quantity of interest, $\\langle S_N \\rangle$, is the arithmetic mean of $S_N$ over the entire ensemble of unique SAWs of length $N$, where each walk is given equal probability.\n$$\n\\langle S_N \\rangle = \\frac{1}{C_N} \\sum_{w \\in \\mathcal{W}_N} S_N(w)\n$$\nHere, $\\mathcal{W}_N$ is the set of all SAWs of length $N$, and $C_N = |\\mathcal{W}_N|$ is the total number of such walks.\n\nFor small $N$, an exact enumeration of all possible walks is computationally feasible. This is the method we shall employ.\n\n**2. Algorithmic Approach: Recursive Backtracking**\n\nWe will generate all SAWs of a given length $N$ using a recursive backtracking (depth-first search) algorithm.\nThe state of the recursion is defined by the current position, the number of steps taken, and the set of visited sites.\n\nThe algorithm proceeds as follows:\n- Start at the origin $p_0 = (0,0)$ with a path containing only this point.\n- Recursively extend the walk by one step. From the current position $p_i$, attempt to move to each of the four nearest neighbors.\n- A move to a new site $p_{i+1}$ is valid only if $p_{i+1}$ has not been previously visited.\n- If a move is valid, add the new site to the path and recurse. After the recursive call returns, backtrack by removing the site from the path to explore other possibilities.\n- The recursion terminates when the walk reaches the desired length of $N$ steps (i.e., the path contains $N+1$ sites).\n\n**3. Optimization using Lattice Symmetry**\n\nThe computational effort can be reduced by a factor of approximately $4$ by exploiting the symmetries of the square lattice. The set of all SAWs is invariant under rotations by $90^\\circ$, $180^\\circ$, $270^\\circ$, and reflections.\n\nInstead of generating all walks, we only generate the subset of walks, denoted $\\mathcal{W}_R$, where the first step is fixed to a specific direction, for instance, to the right, to position $(1,0)$. For any walk $w \\in \\mathcal{W}_R$, we can generate three other distinct walks by rotating it by $90^\\circ$, $180^\\circ$, and $270^\\circ$. These correspond to walks starting with steps up, left, and down, respectively.\n\nLet $w_R$ be a walk in $\\mathcal{W}_R$. Its coordinates are $\\{(x_i, y_i)\\}$.\n- The reflection across the $y$-axis gives a walk $w_L \\in \\mathcal{W}_L$ with coordinates $\\{(-x_i, y_i)\\}$. Its x-span is $S_x(w_L) = \\max(-x_i) - \\min(-x_i) = \\max(x_i) - \\min(x_i) = S_x(w_R)$.\n- The $90^\\circ$ counter-clockwise rotation gives a walk $w_U \\in \\mathcal{W}_U$ with coordinates $\\{(-y_i, x_i)\\}$. Its x-span is $S_x(w_U) = \\max(-y_i) - \\min(-y_i) = \\max(y_i) - \\min(y_i) = S_y(w_R)$, the y-span of the original walk.\n- Similarly, for a walk $w_D \\in \\mathcal{W}_D$, $S_x(w_D) = S_y(w_R)$.\n\nThe total sum of x-spans over all walks is:\n$$\n\\sum_{w \\in \\mathcal{W}_N} S_x(w) = \\sum_{w \\in \\mathcal{W}_R} S_x(w) + \\sum_{w \\in \\mathcal{W}_L} S_x(w) + \\sum_{w \\in \\mathcal{W}_U} S_x(w) + \\sum_{w \\in \\mathcal{W}_D} S_x(w)\n$$\n$$\n\\sum_{w \\in \\mathcal{W}_N} S_x(w) = 2 \\sum_{w \\in \\mathcal{W}_R} S_x(w) + 2 \\sum_{w \\in \\mathcal{W}_R} S_y(w)\n$$\nThe total number of walks is $C_N = 4 |\\mathcal{W}_R|$ for $N \\ge 1$.\nTherefore, the average span is:\n$$\n\\langle S_N \\rangle = \\frac{2 \\sum_{w \\in \\mathcal{W}_R} (S_x(w) + S_y(w))}{4 |\\mathcal{W}_R|} = \\frac{1}{2 |\\mathcal{W}_R|} \\sum_{w \\in \\mathcal{W}_R} (S_x(w) + S_y(w))\n$$\nThis optimized formula requires enumerating only the walks in $\\mathcal{W}_R$, calculating both their x- and y-spans, and combining the results. This is the procedure implemented. Trivial cases for $N=0$ and $N=1$ are handled directly:\n- For $N=0$, there is one walk, $\\{(0,0)\\}$. $S_0=0-0=0$. So $\\langle S_0 \\rangle = 0$.\n- For $N=1$, there are four walks. Two have $S_1=1$ (steps along x-axis) and two have $S_1=0$ (steps along y-axis). $\\langle S_1 \\rangle = (1+1+0+0)/4 = 0.5$.\n\n**4. Calculation of the Scaling Exponent**\n\nThe problem requires computing an estimate for the scaling exponent $\\nu$ using two pairs of data $(N_1, \\langle S_{N_1} \\rangle)$ and $(N_2, \\langle S_{N_2} \\rangle)$. This relationship is based on the expected power-law scaling for large $N$, $\\langle S_N \\rangle \\sim N^\\nu$. The exponent $\\nu$ is estimated using the pairwise log-slope formula:\n$$\n\\nu=\\frac{\\log\\big(\\langle S_{N_2}\\rangle/\\langle S_{N_1}\\rangle\\big)}{\\log\\big(N_2/N_1\\big)}\n$$\nFor this problem, we use $(N_1, N_2) = (4, 10)$. This requires the computation of $\\langle S_4 \\rangle$ and $\\langle S_{10} \\rangle$.\n\n**5. Implementation Plan**\n\nA Python class will encapsulate the logic. A recursive method will perform the optimized walk generation. To ensure the self-avoiding condition is met efficiently, a `set` of tuples will store the visited coordinates for $O(1)$ average time complexity lookups. The program will calculate $\\langle S_N \\rangle$ for $N \\in \\{0, 1, 2, 4, 5, 8, 10\\}$, then compute $\\nu$ using the values for $N=4$ and $N=10$, and finally format the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass SawEnumerator:\n    \"\"\"\n    A class to enumerate Self-Avoiding Walks (SAWs) on a 2D square lattice\n    and calculate the average span. It uses a recursive backtracking algorithm\n    optimized with lattice symmetries.\n    \"\"\"\n    def __init__(self):\n        self.total_x_span_sum = 0.0\n        self.total_y_span_sum = 0.0\n        self.walk_count_one_dir = 0\n\n    def _walk_recursive(self, current_steps, max_steps, pos, path_set, path_list):\n        \"\"\"\n        Recursively generates SAWs from a given state.\n\n        Args:\n            current_steps (int): The number of steps already taken.\n            max_steps (int): The target number of steps for the walk.\n            pos (tuple): The current (x, y) position of the walk.\n            path_set (set): A set of visited coordinates for fast lookups.\n            path_list (list): The ordered list of coordinates in the path.\n        \"\"\"\n        if current_steps == max_steps:\n            self.walk_count_one_dir += 1\n            x_coords = [p[0] for p in path_list]\n            y_coords = [p[1] for p in path_list]\n            self.total_x_span_sum += max(x_coords) - min(x_coords)\n            self.total_y_span_sum += max(y_coords) - min(y_coords)\n            return\n\n        x, y = pos\n        # Possible moves to nearest neighbors\n        moves = [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)]\n\n        for next_pos in moves:\n            if next_pos not in path_set:\n                # Explore this valid move\n                path_set.add(next_pos)\n                path_list.append(next_pos)\n                self._walk_recursive(current_steps + 1, max_steps, next_pos, path_set, path_list)\n                # Backtrack to explore other branches\n                path_list.pop()\n                path_set.remove(next_pos)\n\n    def calculate_avg_span(self, N):\n        \"\"\"\n        Calculates the average x-span for all SAWs of length N.\n\n        Args:\n            N (int): The number of steps in the SAW.\n\n        Returns:\n            float: The average span S_N.\n        \"\"\"\n        if N == 0:\n            # The only walk is a single point at the origin. Span is 0.\n            return 0.0\n        \n        if N == 1:\n            # Four walks: (0,0)-(1,0), (0,0)-(-1,0), (0,0)-(0,1), (0,0)-(0,-1).\n            # Spans are 1, 1, 0, 0. Average is 2/4 = 0.5.\n            return 0.5\n\n        # Reset counters for a new calculation\n        self.total_x_span_sum = 0.0\n        self.total_y_span_sum = 0.0\n        self.walk_count_one_dir = 0\n\n        # Optimization: only generate walks starting with a step to the right (1,0).\n        start_pos = (0, 0)\n        first_step_pos = (1, 0)\n        \n        path_set = {start_pos, first_step_pos}\n        path_list = [start_pos, first_step_pos]\n        \n        # We have taken 1 step, so N-1 steps remain. The recursion explores these.\n        self._walk_recursive(1, N, first_step_pos, path_set, path_list)\n\n        # Using symmetry to find total span and total walks from the partial calculation.\n        # Walks starting right/left contribute proportionally to total_x_span_sum.\n        # Walks starting up/down contribute proportionally to total_y_span_sum.\n        total_span = 2 * self.total_x_span_sum + 2 * self.total_y_span_sum\n        total_walks = 4 * self.walk_count_one_dir\n\n        if total_walks == 0:\n            # This case should not be reached for N = 1\n            return 0.0\n\n        return total_span / total_walks\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as specified.\n    Calculates average spans and the scaling exponent nu for the given test cases.\n    \"\"\"\n    # The set of N values for which S_N must be calculated.\n    test_cases_N = [0, 1, 2, 5, 8, 10]\n    \n    # Parameters for the nu calculation\n    N1, N2 = 4, 10\n    \n    calculator = SawEnumerator()\n    \n    # Store calculated S_N values in a dictionary.\n    s_N_values = {}\n    \n    # Determine all unique N values needed for all tasks.\n    all_N_needed = sorted(list(set(test_cases_N + [N1, N2])))\n    \n    for N in all_N_needed:\n        s_N_values[N] = calculator.calculate_avg_span(N)\n        \n    s0 = s_N_values[0]\n    s1 = s_N_values[1]\n    s2 = s_N_values[2]\n    s4 = s_N_values[4]\n    s5 = s_N_values[5]\n    s8 = s_N_values[8]\n    s10 = s_N_values[10]\n\n    # Compute the scaling exponent nu using the specified formula.\n    nu = np.log(s10 / s4) / np.log(N2 / N1)\n    \n    # Assemble the final list of results in the required order.\n    results = [s0, s1, s2, s5, s8, s10, nu]\n    \n    # Format the output string to exactly match the problem specification:\n    # six decimal places, comma-separated, no spaces, enclosed in brackets.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While exact enumeration is insightful, its exponential complexity renders it useless for the long walks relevant to most physical systems. This practice introduces a powerful alternative: the Rosenbluth-Rosenbluth algorithm, a classic Monte Carlo importance sampling method designed to navigate the vast configuration space of SAWs. By implementing this technique, you will learn to generate statistically significant samples of long walks and use them to estimate the total number of configurations $c_n$, and from it, the fundamental connective constant $\\mu$ .",
            "id": "2436407",
            "problem": "Build a complete, runnable program that estimates the connective constant $\\mu$ for two planar lattices by statistically sampling self-avoiding walks and aggregating the results for a predetermined test suite. A self-avoiding walk of length $n$ on a lattice is a sequence of lattice sites $(\\mathbf{x}_0,\\mathbf{x}_1,\\dots,\\mathbf{x}_n)$ such that $\\mathbf{x}_0=(0,0)$, each step is to a nearest neighbor, and all sites are distinct. For a given lattice $\\mathcal{L}$, let $c_n(\\mathcal{L})$ denote the number of self-avoiding walks of length $n$ starting at the origin, and define the connective constant by\n$$\n\\mu(\\mathcal{L})=\\lim_{n\\to\\infty} \\big(c_n(\\mathcal{L})\\big)^{1/n}.\n$$\nTwo infinite lattices in two dimensions are to be used:\n- The square lattice $\\mathsf{SQ}$ with nearest-neighbor set from $\\mathbf{x}=(i,j)\\in\\mathbb{Z}^2$ given by $\\{(i\\pm 1,j),(i,j\\pm 1)\\}$.\n- The honeycomb lattice $\\mathsf{HC}$ represented on $\\mathbb{Z}^2$ as a $3$-regular graph defined by parity: from $\\mathbf{x}=(i,j)$, if $i+j$ is even then the neighbors are $\\{(i+1,j),(i-1,j),(i,j+1)\\}$; if $i+j$ is odd then the neighbors are $\\{(i+1,j),(i-1,j),(i,j-1)\\}$. This embedding yields the connectivity of the planar honeycomb lattice.\n\nFor a given lattice $\\mathcal{L}$ and walk length $n$, your program must produce an estimate $\\widehat{\\mu}_n(\\mathcal{L})$ constructed from a Monte Carlo estimate $\\widehat{c}_n(\\mathcal{L})$ of $c_n(\\mathcal{L})$ via\n$$\n\\widehat{\\mu}_n(\\mathcal{L})=\\big(\\widehat{c}_n(\\mathcal{L})\\big)^{1/n}.\n$$\nThe estimate $\\widehat{c}_n(\\mathcal{L})$ must be obtained from $M$ independent random trials driven by a pseudorandom number generator initialized by the specified integer seed $s$. There are no physical units in this problem. Angles are not used. All outputs must be real numbers.\n\nTest Suite:\nProvide results for the following ordered list of parameter tuples $(\\mathcal{L},n,M,s)$:\n- Test $1$: $(\\mathsf{HC},\\, n=20,\\, M=20000,\\, s=12345)$.\n- Test $2$: $(\\mathsf{SQ},\\, n=20,\\, M=20000,\\, s=12345)$.\n- Test $3$: $(\\mathsf{HC},\\, n=40,\\, M=10000,\\, s=2023)$.\n- Test $4$: $(\\mathsf{SQ},\\, n=40,\\, M=10000,\\, s=2023)$.\n- Test $5$: $(\\mathsf{HC},\\, n=1,\\, M=50000,\\, s=7)$.\n- Test $6$: $(\\mathsf{SQ},\\, n=2,\\, M=50000,\\, s=11)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the tests above. Each entry must be a real number representing $\\widehat{\\mu}_n(\\mathcal{L})$ for that test. For example, the output must look like\n$[x_1,x_2,x_3,x_4,x_5,x_6]$\nwhere each $x_k$ is the estimated value for test $k$. No additional text should be printed.",
            "solution": "The problem has been subjected to rigorous validation and is determined to be sound. It is scientifically grounded in the established theory of self-avoiding walks, a topic of significance in statistical mechanics and computational physics. The problem is well-posed, objective, and provides a complete specification of test parameters, though it omits naming the specific Monte Carlo algorithm to be used. This omission is not a critical flaw, as a standard, appropriate algorithm can be inferred from the context. The computational task is feasible with the given parameters. Therefore, a solution will be provided, based upon the standard Rosenbluth-Rosenbluth importance sampling method.\n\nThe central task is to estimate the connective constant $\\mu(\\mathcal{L})$ for two planar lattices, the square lattice $\\mathsf{SQ}$ and the honeycomb lattice $\\mathsf{HC}$. The connective constant is defined as the limit\n$$\n\\mu(\\mathcal{L}) = \\lim_{n\\to\\infty} \\big(c_n(\\mathcal{L})\\big)^{1/n}\n$$\nwhere $c_n(\\mathcal{L})$ is the total number of self-avoiding walks (SAWs) of length $n$ starting from the origin on a lattice $\\mathcal{L}$. The problem states that the estimation must be performed by first producing a Monte Carlo estimate $\\widehat{c}_n(\\mathcal{L})$ of $c_n(\\mathcal{L})$, and then computing the estimate for the connective constant as $\\widehat{\\mu}_n(\\mathcal{L}) = \\big(\\widehat{c}_n(\\mathcal{L})\\big)^{1/n}$.\n\nDirect enumeration of $c_n(\\mathcal{L})$ is computationally intractable for all but very small values of $n$, as the number of walks grows exponentially. A naive Monte Carlo method of generating simple random walks and counting the fraction that are self-avoiding is also profoundly inefficient due to the high rate of attrition; the probability of a random walk being self-avoiding vanishes exponentially with $n$. Consequently, a more sophisticated approach is required. The Rosenbluth-Rosenbluth algorithm is a classic importance sampling method designed specifically for this problem. It constructs a SAW step-by-step while calculating a corrective weight, providing an unbiased estimate of $c_n(\\mathcal{L})$.\n\nThe Rosenbluth-Rosenbluth algorithm proceeds as follows for a single trial:\n$1$. A walk begins at the origin, $\\mathbf{x}_0 = (0,0)$. The initial weight of the walk is $W=1$.\n$2$. For each step $k$ from $1$ to $n$, the algorithm identifies all valid, non-self-intersecting nearest-neighbor sites from the current position $\\mathbf{x}_{k-1}$. Let the number of such available sites be $m_k$.\n$3$. If at any step $m_k=0$, the walk is \"trapped\" and cannot continue without self-intersection. The trial is terminated, and its contribution to the total count is a weight of $0$.\n$4$. If $m_k  0$, the walk's weight is multiplied by $m_k$, so that $W \\to W \\times m_k$. One of the $m_k$ available sites is chosen uniformly at random to become the next site $\\mathbf{x}_k$ in the walk.\n$5$. If a walk of length $n$ is successfully generated without being trapped, it has a final weight $W_n = \\prod_{k=1}^n m_k$. This weight accounts for the fact that the walk was generated via a biased process (i.e., not all SAWs of length $n$ are equally likely to be generated).\n\nThis procedure is repeated for $M$ independent trials. The estimate for $c_n(\\mathcal{L})$ is the arithmetic mean of the weights from all trials:\n$$\n\\widehat{c}_n(\\mathcal{L}) = \\frac{1}{M} \\sum_{i=1}^{M} W_n^{(i)}\n$$\nThis provides an unbiased estimate, i.e., $\\mathbb{E}[\\widehat{c}_n(\\mathcal{L})] = c_n(\\mathcal{L})$. Finally, the estimate for the connective constant is calculated as $\\widehat{\\mu}_n(\\mathcal{L}) = (\\widehat{c}_n(\\mathcal{L}))^{1/n}$.\n\nThe implementation of this algorithm requires a clear definition of the lattice structures.\nThe square lattice, $\\mathsf{SQ}$, has nearest neighbors of a site $(i,j)$ given by $\\{(i\\pm 1, j), (i, j\\pm 1)\\}$, a coordination number of $z=4$.\nThe honeycomb lattice, $\\mathsf{HC}$, is represented on a square grid $\\mathbb{Z}^2$. For a site $(i,j)$, if $i+j$ is even, the neighbors are $\\{(i+1,j), (i-1,j), (i,j+1)\\}$; if $i+j$ is odd, the neighbors are $\\{(i+1,j), (i-1,j), (i,j-1)\\}$. This is a $3$-regular graph, so $z=3$.\n\nThe program is structured to handle the specified test cases. For each case $(\\mathcal{L}, n, M, s)$, a new pseudorandom number generator `numpy.random.default_rng` is initialized with the integer seed $s$ to ensure reproducibility. The simulation then performs $M$ trials, accumulating the weights to compute $\\widehat{c}_n(\\mathcal{L})$. To maintain computational efficiency, the set of visited sites in a walk is stored in a Python `set` data structure, which provides average $O(1)$ time complexity for lookups during the self-avoidance check. The final result for each test case is computed and collected into a list, which is then formatted into the required output string. For the test cases with very small $n$ (e.g., $n=1$ or $n=2$), the algorithm correctly computes the exact values of $c_n$, as there is no path variation that affects the weight calculation, thus serving as a validation of the implementation's correctness.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating the connective constant for specified lattices\n    using the Rosenbluth-Rosenbluth Monte Carlo method.\n    \"\"\"\n\n    def get_neighbors(pos, lattice_type):\n        \"\"\"\n        Returns the list of nearest neighbors for a given position on a lattice.\n\n        Args:\n            pos (tuple): The (i, j) coordinates of the site.\n            lattice_type (str): The type of lattice, 'SQ' or 'HC'.\n\n        Returns:\n            list: A list of neighbor coordinate tuples.\n        \"\"\"\n        i, j = pos\n        if lattice_type == 'SQ':\n            return [(i + 1, j), (i - 1, j), (i, j + 1), (i, j - 1)]\n        elif lattice_type == 'HC':\n            # Parity-based definition for honeycomb lattice on a square grid\n            if (i + j) % 2 == 0:\n                return [(i + 1, j), (i - 1, j), (i, j + 1)]\n            else: # (i + j) is odd\n                return [(i + 1, j), (i - 1, j), (i, j - 1)]\n        return []\n\n    def run_rosenbluth_trial(lattice_type, n, rng):\n        \"\"\"\n        Performs a single trial of the Rosenbluth-Rosenbluth algorithm.\n\n        Args:\n            lattice_type (str): The lattice type.\n            n (int): The desired length of the self-avoiding walk.\n            rng (numpy.random.Generator): The random number generator to use.\n\n        Returns:\n            float: The weight of the generated walk. Returns 0.0 if the walk is trapped.\n        \"\"\"\n        current_pos = (0, 0)\n        visited_sites = {current_pos}\n        weight = 1.0\n\n        for _ in range(n):\n            neighbors = get_neighbors(current_pos, lattice_type)\n            # Filter for neighbors not already in the path\n            valid_neighbors = [p for p in neighbors if p not in visited_sites]\n            \n            m_k = len(valid_neighbors)\n            \n            if m_k == 0:\n                # The walk is trapped, so its weight contribution is zero.\n                return 0.0\n            \n            # Update the weight of the walk\n            weight *= m_k\n            \n            # Choose the next position uniformly from the valid neighbors\n            next_pos_idx = rng.integers(0, m_k)\n            current_pos = valid_neighbors[next_pos_idx]\n            visited_sites.add(current_pos)\n            \n        return weight\n\n    def estimate_mu(lattice_type, n, M, seed):\n        \"\"\"\n        Estimates the connective constant mu for a given lattice.\n\n        Args:\n            lattice_type (str): The lattice type ('SQ' or 'HC').\n            n (int): The length of the self-avoiding walks.\n            M (int): The number of Monte Carlo trials.\n            seed (int): The seed for the random number generator.\n\n        Returns:\n            float: The estimated connective constant.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        total_weight = 0.0\n        for _ in range(M):\n            total_weight += run_rosenbluth_trial(lattice_type, n, rng)\n            \n        c_n_est = total_weight / M\n        \n        if c_n_est == 0.0:\n            # This can happen if all walks get trapped.\n            return 0.0\n        \n        mu_est = c_n_est**(1.0 / n)\n        return mu_est\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lattice_type, n, M, seed)\n        ('HC', 20, 20000, 12345),\n        ('SQ', 20, 20000, 12345),\n        ('HC', 40, 10000, 2023),\n        ('SQ', 40, 10000, 2023),\n        ('HC', 1, 50000, 7),\n        ('SQ', 2, 50000, 11),\n    ]\n\n    results = []\n    for case in test_cases:\n        lattice_type, n, M, seed = case\n        result = estimate_mu(lattice_type, n, M, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Extracting physical laws from numerical simulations requires careful data analysis, as raw results are always influenced by finite system sizes and statistical noise. This practice focuses on finite-size scaling, an essential technique for systematically accounting for these effects and extrapolating to the asymptotic limit ($N \\to \\infty$). By learning to analyze how \"effective exponents\" change with system size, you will be able to obtain highly accurate estimates of universal critical exponents like $\\nu$, transforming noisy data into a precise physical measurement .",
            "id": "2436413",
            "problem": "You are given the task of estimating the Flory exponent $\\nu$ that characterizes the asymptotic growth of the mean-squared end-to-end distance $\\langle R_N^2 \\rangle$ of an $N$-step Self-Avoiding Walk (SAW) on a two-dimensional square lattice. The Self-Avoiding Walk (SAW) is defined as a nearest-neighbor walk on $\\mathbb{Z}^2$ that never revisits a previously visited site. The end-to-end vector after $N$ steps is denoted by $R_N$, and the ensemble mean $\\langle R_N^2 \\rangle$ is taken over an ensemble of SAWs of length $N$ generated by a fixed, well-defined sampling procedure. The Flory exponent $\\nu$ is defined by the asymptotic relation that, for large $N$, the typical size of a SAW obeys a power law with multiplicative finite-size corrections. The goal is to use finite-size scaling to improve the accuracy of the estimate of $\\nu$ in the presence of those corrections and sampling noise.\n\nBase principles and definitions to be used:\n- Core definition: A Self-Avoiding Walk (SAW) of length $N$ on the square lattice $\\mathbb{Z}^2$ is a sequence of distinct lattice sites $\\{x_0, x_1, \\dots, x_N\\}$ with $x_{i+1}$ a nearest neighbor of $x_i$ and $x_0 = (0,0)$.\n- Observable: The end-to-end vector is $R_N = x_N - x_0$, and the mean-squared end-to-end distance is $\\langle R_N^2 \\rangle$.\n- Well-tested fact: For large $N$, $\\langle R_N^2 \\rangle$ grows like a power law multiplied by a slowly decaying correction, consistent with finite-size scaling. You may treat the presence of an algebraically decaying correction as a valid and broadly accepted hypothesis in the study of critical phenomena and polymer statistics.\n\nYour program must implement a finite-size scaling analysis that reduces the bias from subleading corrections. You must:\n- Construct an estimator for $\\nu$ based on a linear regression of $\\ln \\langle R_N^2 \\rangle$ versus $\\ln N$ (baseline), and a refinement that uses effective exponents and extrapolation against an algebraic correction in $N$ to estimate the $N \\to \\infty$ limit (finite-size scaling estimator).\n- The finite-size scaling estimator must be based on the following logic: define a local slope or \"effective exponent\" from neighboring $N$ values and extrapolate that slope to the $N \\to \\infty$ limit by regressing it against a negative power of $N$; this regression must include a tunable correction exponent.\n\nDo not assume or use any domain-specific numerical constants such as the expected value of $\\nu$ in two dimensions. Your algorithm must be derived from the definitions and the finite-size scaling hypothesis alone.\n\nSynthetic simulation data generation:\n- To ensure testability and reproducibility, you will generate synthetic \"simulation-like\" datasets for $\\langle R_N^2 \\rangle$ that emulate Monte Carlo outcomes by combining a leading power law with an algebraic correction and multiplicative log-normal noise. Specifically, for each given list of $N$ values, generate data according to\n$$\n\\langle R_N^2 \\rangle_{\\text{obs}} = A \\, N^{2 \\nu_{\\text{true}}} \\left(1 + B \\, N^{-\\Delta_{\\text{true}}}\\right) \\times \\exp(\\epsilon_N),\n$$\nwhere $\\epsilon_N$ are independent samples from a normal distribution with mean $0$ and standard deviation $\\sigma$; use a fixed pseudorandom number generator seed per case to guarantee reproducibility. Here $A$, $B$, $\\nu_{\\text{true}}$, $\\Delta_{\\text{true}}$, and $\\sigma$ are case-specific parameters given below. You must not use these ground-truth parameters in your estimator; they are only used to generate the synthetic dataset.\n\nEstimator design requirements:\n- Baseline estimator: Fit a straight line to $(x_i, y_i)$ with $x_i = \\ln N_i$ and $y_i = \\ln \\langle R_{N_i}^2 \\rangle_{\\text{obs}}$ by least squares, and use the slope divided by $2$ as a baseline estimate for $\\nu$. This baseline is not part of the required output but should be implemented to validate your approach internally if desired.\n- Finite-size scaling estimator:\n  - Construct effective slopes $s_i$ from adjacent points using\n  $$\n  s_i = \\frac{1}{2}\\,\\frac{\\ln \\langle R_{N_{i+1}}^2 \\rangle_{\\text{obs}} - \\ln \\langle R_{N_{i}}^2 \\rangle_{\\text{obs}}}{\\ln N_{i+1} - \\ln N_i},\n  $$\n  so that $s_i$ approximates $\\nu$ up to a finite-size correction.\n  - For a grid of candidate correction exponents $\\Delta \\in [\\delta_{\\min}, \\delta_{\\max}]$ sampled uniformly, perform a linear regression of $s_i$ against $N_{\\text{eff},i}^{-\\Delta}$, where $N_{\\text{eff},i}$ is a representative scale for the pair $(N_i, N_{i+1})$; you must use the geometric mean $N_{\\text{eff},i} = \\sqrt{N_i N_{i+1}}$.\n  - For each candidate $\\Delta$, regress $s_i = \\nu + K \\, N_{\\text{eff},i}^{-\\Delta}$ via least squares, record the residual sum of squares, and select the $\\Delta$ that minimizes this residual. The intercept of the corresponding fit is your finite-size scaling estimate $\\widehat{\\nu}_{\\text{fss}}$.\n  - Your implementation must not assume a known value of $\\Delta$; it must search over a range and select it from the data.\n\nTest suite:\nCreate four cases by generating synthetic datasets using the rule above. For each case, you must generate the data internally using the given parameters and the specified random seed. You must not use any external input. For each case, return only $\\widehat{\\nu}_{\\text{fss}}$.\n\n- Case $1$:\n  - $N$ values: $\\{12, 16, 20, 24, 28, 32\\}$.\n  - Parameters: $A = 1.10$, $B = 0.85$, $\\nu_{\\text{true}} = 0.75$, $\\Delta_{\\text{true}} = 0.50$, $\\sigma = 0.02$.\n  - Random seed: $12345$.\n- Case $2$:\n  - $N$ values: $\\{16, 20, 24, 28, 32, 40\\}$.\n  - Parameters: $A = 0.95$, $B = -0.60$, $\\nu_{\\text{true}} = 0.75$, $\\Delta_{\\text{true}} = 0.80$, $\\sigma = 0.05$.\n  - Random seed: $24680$.\n- Case $3$:\n  - $N$ values: $\\{10, 12, 14, 16, 18, 20, 24\\}$.\n  - Parameters: $A = 1.00$, $B = 1.50$, $\\nu_{\\text{true}} = 0.75$, $\\Delta_{\\text{true}} = 0.40$, $\\sigma = 0.03$.\n  - Random seed: $13579$.\n- Case $4$:\n  - $N$ values: $\\{20, 24, 28, 32, 36, 40, 48, 56\\}$.\n  - Parameters: $A = 1.25$, $B = 0.50$, $\\nu_{\\text{true}} = 0.75$, $\\Delta_{\\text{true}} = 1.00$, $\\sigma = 0.04$.\n  - Random seed: $112233$.\n\nImplementation details:\n- You must use a uniform grid for candidate $\\Delta$ values in the finite-size scaling estimator with bounds $\\delta_{\\min} = 0.20$ and $\\delta_{\\max} = 1.50$, and grid spacing $\\delta_{\\text{step}} = 0.01$.\n- Use natural logarithms.\n- All regression steps must be ordinary least squares in the Euclidean norm with no additional weights.\n- Angles are not involved; there is no unit requirement. Report final floats in plain numerical form.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases from $1$ to $4$, with each value rounded to exactly $6$ decimal places, for example, $[0.750000,0.749500,0.751200,0.748900]$.",
            "solution": "The problem statement has been validated and is determined to be a valid, well-posed problem in computational physics. It is scientifically grounded in the established theory of critical phenomena and polymer statistics, specifically the study of Self-Avoiding Walks (SAWs). All definitions, parameters, and algorithmic requirements are stated with sufficient clarity and precision to permit a unique and verifiable solution. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe objective is to estimate the Flory exponent $\\nu$ for a Self-Avoiding Walk on a two-dimensional square lattice, $\\mathbb{Z}^2$. This exponent governs the asymptotic scaling of the polymer's size with its length. The primary observable is the mean-squared end-to-end distance, $\\langle R_N^2 \\rangle$, for a walk of $N$ steps. For large $N$, this quantity is expected to follow a power law:\n$$\n\\langle R_N^2 \\rangle \\sim A N^{2\\nu}\n$$\nwhere $A$ is a non-universal amplitude and $\\nu$ is a universal critical exponent. In practice, for finite $N$, there are corrections to this leading behavior. The problem provides a model that incorporates the leading correction term, which is the standard approach in finite-size scaling analysis:\n$$\n\\langle R_N^2 \\rangle = A N^{2\\nu} \\left(1 + B N^{-\\Delta} + \\dots\\right)\n$$\nHere, $B$ is another non-universal amplitude and $\\Delta  0$ is the leading correction-to-scaling exponent. The synthetic data to be analyzed is generated from this model with additional multiplicative log-normal noise to simulate statistical errors from a Monte Carlo experiment:\n$$\n\\langle R_N^2 \\rangle_{\\text{obs}} = A N^{2\\nu_{\\text{true}}} \\left(1 + B N^{-\\Delta_{\\text{true}}}\\right) \\exp(\\epsilon_N)\n$$\nwhere $\\epsilon_N$ is a normally distributed random variable with mean $0$ and standard deviation $\\sigma$. Our task is to estimate $\\nu$ from such data without knowledge of the true parameters $A$, $B$, $\\nu_{\\text{true}}$, $\\Delta_{\\text{true}}$, or $\\sigma$.\n\nA naive approach would be to linearize the leading power law by taking the logarithm: $\\ln \\langle R_N^2 \\rangle \\approx \\ln A + 2\\nu \\ln N$. A linear fit of $\\ln \\langle R_N^2 \\rangle$ versus $\\ln N$ would then yield a slope of $2\\nu$. However, the correction term $B N^{-\\Delta}$ introduces a systematic deviation, biasing the estimate.\n\nThe specified finite-size scaling method is designed to systematically account for this correction. First, we take the natural logarithm of the full model (approximating $\\ln(1+x) \\approx x$ for small $x = B N^{-\\Delta}$ and ignoring the noise term for clarity):\n$$\n\\ln \\langle R_N^2 \\rangle \\approx \\ln A + 2\\nu \\ln N + B N^{-\\Delta}\n$$\nTo analyze how the estimate for $\\nu$ changes with $N$, we define an effective exponent, $s_i$, from pairs of adjacent data points $(N_i, \\langle R_{N_i}^2 \\rangle_{\\text{obs}})$ and $(N_{i+1}, \\langle R_{N_{i+1}}^2 \\rangle_{\\text{obs}})$:\n$$\ns_i = \\frac{1}{2} \\frac{\\ln \\langle R_{N_{i+1}}^2 \\rangle_{\\text{obs}} - \\ln \\langle R_{N_{i}}^2 \\rangle_{\\text{obs}}}{\\ln N_{i+1} - \\ln N_i}\n$$\nThis quantity is a discrete approximation of the local derivative $\\frac{1}{2} \\frac{d(\\ln \\langle R_N^2 \\rangle)}{d(\\ln N)}$. Differentiating our model for $\\ln \\langle R_N^2 \\rangle$ with respect to $\\ln N$ gives:\n$$\n\\frac{1}{2} \\frac{d(\\ln \\langle R_N^2 \\rangle)}{d(\\ln N)} = \\frac{1}{2} \\frac{d}{d(\\ln N)} \\left( \\ln A + 2\\nu \\ln N + B N^{-\\Delta} \\right) = \\nu - \\frac{1}{2} B \\Delta N^{-\\Delta}\n$$\nThis suggests that the effective exponent $s_i$, evaluated at an effective scale $N_{\\text{eff},i}$ between $N_i$ and $N_{i+1}$, should behave as:\n$$\ns_i \\approx \\nu + K (N_{\\text{eff},i})^{-\\Delta}\n$$\nwhere $K = -B\\Delta/2$ is a constant. We use the geometric mean $N_{\\text{eff},i} = \\sqrt{N_i N_{i+1}}$ as the effective scale.\n\nThis equation provides a powerful method for analysis. It is a linear relationship between $s_i$ and the variable $x_i = (N_{\\text{eff},i})^{-\\Delta}$. The intercept of this linear relationship is the asymptotic exponent $\\nu$, which is precisely the quantity we wish to estimate.\n\nThe correction exponent $\\Delta$ is, however, also unknown. The algorithm addresses this by performing a search. We define a grid of candidate values for $\\Delta$ in the range $[\\delta_{\\min}, \\delta_{\\max}] = [0.20, 1.50]$. For each candidate $\\Delta$, we perform an ordinary least-squares linear regression of $s_i$ on $(N_{\\text{eff},i})^{-\\Delta}$. The quality of each fit is measured by its residual sum of squares (RSS). The value of $\\Delta$ that yields the minimum RSS is considered the best estimate for the correction exponent. The final estimate for the Flory exponent, $\\widehat{\\nu}_{\\text{fss}}$, is the intercept of the linear fit corresponding to this optimal $\\Delta$. This procedure systematically removes the leading finite-size bias and provides an extrapolated estimate for the $N \\to \\infty$ limit.\n\nThe overall algorithmic procedure is as follows:\n$1$. For each test case, generate the synthetic dataset $\\{\\left(N_i, \\langle R_{N_i}^2 \\rangle_{\\text{obs}}\\right)\\}$ using the provided parameters and random seed.\n$2$. Compute the sequence of effective exponents $\\{s_i\\}$ and effective scales $\\{N_{\\text{eff},i}\\}$.\n$3$. Iterate through the grid of candidate correction exponents $\\Delta \\in [0.20, 0.21, \\dots, 1.50]$.\n$4$. For each $\\Delta$, perform a linear regression of $s_i$ against $(N_{\\text{eff},i})^{-\\Delta}$ to obtain an intercept $\\nu(\\Delta)$ and the corresponding RSS$(\\Delta)$.\n$5$. Find the optimal correction exponent $\\Delta^* = \\arg\\min_{\\Delta} \\text{RSS}(\\Delta)$.\n$6$. The final estimate is $\\widehat{\\nu}_{\\text{fss}} = \\nu(\\Delta^*)$.\nThis procedure is implemented for each of the four specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Estimates the Flory exponent nu for a 2D Self-Avoiding Walk using\n    a finite-size scaling analysis on synthetic data.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"N_values\": [12, 16, 20, 24, 28, 32],\n            \"A\": 1.10, \"B\": 0.85, \"nu_true\": 0.75, \"Delta_true\": 0.50, \"sigma\": 0.02,\n            \"seed\": 12345\n        },\n        {\n            \"N_values\": [16, 20, 24, 28, 32, 40],\n            \"A\": 0.95, \"B\": -0.60, \"nu_true\": 0.75, \"Delta_true\": 0.80, \"sigma\": 0.05,\n            \"seed\": 24680\n        },\n        {\n            \"N_values\": [10, 12, 14, 16, 18, 20, 24],\n            \"A\": 1.00, \"B\": 1.50, \"nu_true\": 0.75, \"Delta_true\": 0.40, \"sigma\": 0.03,\n            \"seed\": 13579\n        },\n        {\n            \"N_values\": [20, 24, 28, 32, 36, 40, 48, 56],\n            \"A\": 1.25, \"B\": 0.50, \"nu_true\": 0.75, \"Delta_true\": 1.00, \"sigma\": 0.04,\n            \"seed\": 112233\n        }\n    ]\n\n    results = []\n    \n    # Define parameters for the finite-size scaling estimator\n    delta_min = 0.20\n    delta_max = 1.50\n    delta_step = 0.01\n    \n    delta_grid = np.arange(delta_min, delta_max + delta_step, delta_step)\n\n    for case in test_cases:\n        # 1. Generate synthetic data\n        rng = np.random.default_rng(case[\"seed\"])\n        N = np.array(case[\"N_values\"], dtype=float)\n        \n        # Generate log-normal noise\n        epsilon_N = rng.normal(0, case[\"sigma\"], size=len(N))\n        \n        # Calculate observed mean-squared end-to-end distance using the provided model\n        r2_obs = (case[\"A\"] * N**(2 * case[\"nu_true\"]) *\n                  (1 + case[\"B\"] * N**(-case[\"Delta_true\"])) *\n                  np.exp(epsilon_N))\n\n        # 2. Construct effective slopes and effective lengths\n        log_r2 = np.log(r2_obs)\n        log_N = np.log(N)\n        \n        # Effective slopes s_i from adjacent points\n        s_i = 0.5 * (log_r2[1:] - log_r2[:-1]) / (log_N[1:] - log_N[:-1])\n        # Effective scales N_eff,i (geometric mean)\n        N_eff = np.sqrt(N[1:] * N[:-1])\n\n        # 3. Search for the best correction exponent Delta by minimizing RSS\n        min_rss = float('inf')\n        best_nu_fss = None\n\n        y_data = s_i\n        \n        for delta_candidate in delta_grid:\n            # Independent variable for regression: x = N_eff^(-Delta)\n            x_data = N_eff**(-delta_candidate)\n            \n            # Setup for Ordinary Least Squares: y = p[1] + p[0] * x\n            # where p[0] is the slope and p[1] is the intercept (our nu estimate)\n            A = np.vstack([x_data, np.ones(len(x_data))]).T\n            \n            p, residuals, _, _ = np.linalg.lstsq(A, y_data, rcond=None)\n            \n            # The residual sum of squares is returned by lstsq\n            rss = residuals[0] if residuals.size > 0 else 0.0\n            intercept_nu = p[1]\n\n            # Update if this Delta gives a better fit (lower RSS)\n            if rss  min_rss:\n                min_rss = rss\n                best_nu_fss = intercept_nu\n        \n        results.append(best_nu_fss)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}