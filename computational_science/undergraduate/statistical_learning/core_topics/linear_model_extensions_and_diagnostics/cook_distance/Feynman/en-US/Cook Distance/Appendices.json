{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Cook's distance, we must build it from its foundational principles. This first exercise challenges you to derive an influence measure directly from the change in estimated parameters when an observation is removed, a concept at the very heart of diagnostics . By implementing this from scratch and extending it to measure the influence on a specific interaction term, you will gain a robust, first-principles understanding of how influence is quantified.",
            "id": "3111539",
            "problem": "You are given a linear regression setting with an interaction term. Consider a response vector $y \\in \\mathbb{R}^{n}$ and a design matrix $X \\in \\mathbb{R}^{n \\times p}$ that includes an intercept column, two predictors $x_1$ and $x_2$, and their interaction $x_1 x_2$, so that $p = 4$. Assume the ordinary least squares (OLS) model $y = X \\beta + \\varepsilon$ with $\\mathbb{E}[\\varepsilon] = 0$ and $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I$. The OLS estimator is the minimizer of the sum of squared errors and can be obtained from the normal equations. For each observation index $i \\in \\{0,1,\\dots,n-1\\}$, define the leave-one-out estimator by refitting OLS after removing the $i$-th row of $(X,y)$. Let the difference between the full-sample estimate and the leave-one-out estimate be denoted by the vector $\\Delta_i \\in \\mathbb{R}^{p}$.\n\nYour tasks are:\n- Using only fundamental definitions of ordinary least squares (OLS) estimation, leave-one-out refitting, and the sample-based quadratic form induced by $X^\\top X$, derive a principled scalar measure of overall influence per observation that scales the parameter shift $\\Delta_i$ by the information in $X^\\top X$ and by an estimate of the noise variance, normalized by the model dimension $p$.\n- Further, define a partial analog that focuses only on a specified subset $S \\subset \\{0,1,\\dots,p-1\\}$ of coefficients by restricting the parameter shift to coordinates in $S$ and using the corresponding principal submatrix of $X^\\top X$, normalized by $|S|$ and the same estimate of the noise variance. For this problem, $S$ will always be the singleton set containing only the interaction coefficient index.\n- Implement these definitions algorithmically by direct refitting (full fit and $n$ leave-one-out fits) and the appropriate quadratic forms. Do not assume any shortcut identities for leave-one-out or for influence; compute what the definitions require from first principles.\n- Interpret indexing as $0$-based. When a ratio between the partial and overall measures is requested and the denominator is zero, define the ratio to be $0$ for that observation.\n\nUse the following test suite. In all cases, construct $X$ with columns $[1, x_1, x_2, x_1 x_2]$ in that order, and let $S = \\{3\\}$ (only the interaction coefficient). Use the same underlying data-generating coefficients $\\beta^\\star = (\\beta_0,\\beta_1,\\beta_2,\\beta_3) = (1.0, 2.0, -1.5, 0.8)$ for forming the response values together with the explicitly provided noise vectors. All numeric values below are to be used exactly as given.\n\nTest Case 1 (a point with an extreme interaction term and a large deviation in the response):\n- $n = 10$.\n- $x_1 = [-1.0, -0.5, 0.0, 0.5, 1.0, -0.8, 0.7, -0.2, 0.3, 5.0]$.\n- $x_2 = [0.9, -1.1, 0.2, -0.4, 0.6, -0.7, 0.8, 0.3, -0.5, 5.0]$.\n- Noise vector $\\eta = [0.05, -0.04, 0.01, -0.02, 0.03, 0.02, -0.01, 0.00, 0.01, 0.00]$.\n- Form $y$ by $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$ for $i = 0,\\dots,9$, then add an additional deviation of $8.0$ to the last response: $y_9 \\leftarrow y_9 + 8.0$.\n- Required outputs for this case:\n  1. The index $i^\\star$ (an integer) that maximizes the partial measure over $S = \\{3\\}$.\n  2. The overall measure (a float) at $i^\\star$, rounded to $6$ decimal places.\n  3. The partial measure (a float) at $i^\\star$, rounded to $6$ decimal places.\n\nTest Case 2 (no extreme values; moderate interactions):\n- $n = 12$.\n- $x_1 = [-1.2, -0.9, -0.6, -0.3, 0.0, 0.3, 0.6, 0.9, 1.2, -0.4, 0.4, 0.1]$.\n- $x_2 = [0.5, -0.4, 0.7, -0.8, 0.3, -0.2, 0.1, -0.1, 0.2, 0.9, -0.9, 0.0]$.\n- Noise vector $\\eta = [0.01, -0.02, 0.03, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, -0.02, 0.00]$.\n- Form $y$ by $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$ for $i = 0,\\dots,11$.\n- Let the threshold be $\\tau = 0.05$.\n- Required outputs for this case:\n  1. The maximum overall measure over all $i$ (a float), rounded to $6$ decimal places.\n  2. The count (an integer) of points with partial measure strictly greater than $\\tau$.\n\nTest Case 3 (a high-leverage interaction point but with small residual):\n- $n = 6$.\n- $x_1 = [-2.0, -1.0, 0.0, 1.0, 2.0, 4.0]$.\n- $x_2 = [0.5, -0.5, 0.2, -0.2, 0.5, 4.0]$.\n- Noise vector $\\eta = [0.00, 0.01, -0.01, 0.00, 0.02, 0.00]$.\n- Form $y$ by $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$ for $i = 0,\\dots,5$.\n- Required output for this case:\n  1. The mean of the ratios $\\rho_i$ across all $i$, where $\\rho_i$ is the ratio of the partial to the overall measure at $i$ when the overall measure is nonzero, and $\\rho_i = 0$ if the overall measure equals $0$. Report this mean as a float rounded to $6$ decimal places.\n\nImplementation notes and constraints:\n- Estimate the noise variance from the full fit via the unbiased residual variance $\\hat{\\sigma}^2 = \\text{RSS}/(n-p)$, where $\\text{RSS}$ is the residual sum of squares from the full fit and $p = 4$.\n- For each observation $i$, compute a leave-one-out refit to obtain the corresponding parameter shift.\n- The subset $S$ is $\\{3\\}$, that is, only the interaction coefficient.\n- All indices are $0$-based.\n- Your program should produce a single line of output containing the results for all three test cases in order, flattened into one list and enclosed in square brackets with comma-separated values and no spaces. Specifically, the output format is\n  $[i^\\star, \\text{overall at } i^\\star, \\text{partial at } i^\\star, \\max \\text{ overall}, \\text{count partial}>\\tau, \\text{mean ratio}]$\n  where the floats are rounded to $6$ decimal places as specified.",
            "solution": "### 1. Theoretical Framework\n\nThe problem is set within the context of ordinary least squares (OLS) linear regression.\n\n#### 1.1. Model and Estimation\nThe model is given by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is a vector of unobserved errors with $\\mathbb{E}[\\varepsilon] = 0$ and $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I$. The dimension of the parameter vector is $p=4$, and the columns of $X$ correspond to an intercept, two predictors $x_1$ and $x_2$, and their interaction term $x_1 x_2$.\n\nThe OLS estimator $\\hat{\\beta}$ minimizes the residual sum of squares, $\\|y - X\\beta\\|^2$. The solution is found via the normal equations:\n$$ (X^\\top X) \\hat{\\beta} = X^\\top y $$\nIf $X^\\top X$ is invertible, the unique solution is $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$.\n\nThe residuals from this full-data fit are $e = y - X\\hat{\\beta}$. The residual sum of squares is $\\text{RSS} = e^\\top e$. An unbiased estimator for the error variance $\\sigma^2$ is:\n$$ \\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n-p} $$\nThis estimator will be used for scaling the influence measures.\n\n#### 1.2. Leave-One-Out (LOO) Estimation\nFor each observation $i \\in \\{0, 1, \\dots, n-1\\}$, we define the leave-one-out (LOO) dataset by removing the $i$-th row from both $X$ and $y$. Let these be denoted by $X_{(i)} \\in \\mathbb{R}^{(n-1) \\times p}$ and $y_{(i)} \\in \\mathbb{R}^{n-1}$.\n\nThe LOO OLS estimate, $\\hat{\\beta}_{(i)}$, is obtained by fitting the model to this reduced dataset:\n$$ \\hat{\\beta}_{(i)} = (X_{(i)}^\\top X_{(i)})^{-1} X_{(i)}^\\top y_{(i)} $$\nThe change in the coefficient estimate upon removing observation $i$ is the parameter shift vector:\n$$ \\Delta_i = \\hat{\\beta} - \\hat{\\beta}_{(i)} $$\n\n### 2. Derivation of Influence Measures\n\nThe problem asks for two principled measures of influence, which we derive from the provided specifications.\n\n#### 2.1. Overall Influence Measure\nThe measure must scale the parameter shift $\\Delta_i$ using the information in $X^\\top X$ and the estimated noise variance $\\hat{\\sigma}^2$, normalized by the model dimension $p$.\n\nA natural way to measure the \"size\" of the shift vector $\\Delta_i$ in the geometric context of the design matrix $X$ is to use the quadratic form induced by the matrix $X^\\top X$. This quantity, $\\Delta_i^\\top (X^\\top X) \\Delta_i$, represents the sum of squared differences between predictions made by the full model and the LOO model:\n$$ \\|X\\hat{\\beta} - X\\hat{\\beta}_{(i)}\\|^2 = \\|X(\\hat{\\beta} - \\hat{\\beta}_{(i)})\\|^2 = \\|X\\Delta_i\\|^2 = (X\\Delta_i)^\\top(X\\Delta_i) = \\Delta_i^\\top X^\\top X \\Delta_i $$\nThis term quantifies the total impact of removing observation $i$ on the fitted values $\\hat{y}$.\n\nTo create a standardized, dimensionless measure, we scale this by the estimated error variance $\\hat{\\sigma}^2$. Further normalization by the number of parameters, $p$, yields an average influence per parameter. This formalizes the verbal description into the following overall influence measure, $D_i^{\\text{overall}}$:\n$$ D_i^{\\text{overall}} = \\frac{\\Delta_i^\\top (X^\\top X) \\Delta_i}{p \\hat{\\sigma}^2} $$\nThis formulation is equivalent to the standard definition of Cook's distance.\n\n#### 2.2. Partial Influence Measure\nA partial analogue is required, focusing on a subset of coefficients $S \\subset \\{0, 1, \\dots, p-1\\}$. This measure is constructed by restricting the parameter shift vector to its components in $S$, denoted $\\Delta_{i,S}$, and using the corresponding principal submatrix of $X^\\top X$, denoted $(X^\\top X)_S$. The result is normalized by the size of the subset, $|S|$, and the variance estimate $\\hat{\\sigma}^2$.\n\nFollowing the same logic, the quadratic form for the partial shift is $\\Delta_{i,S}^\\top (X^\\top X)_S \\Delta_{i,S}$. Applying the specified normalizations defines the partial influence measure, $D_{i,S}^{\\text{partial}}$:\n$$ D_{i,S}^{\\text{partial}} = \\frac{\\Delta_{i,S}^\\top (X^\\top X)_S \\Delta_{i,S}}{|S| \\hat{\\sigma}^2} $$\nFor this problem, the subset is $S=\\{3\\}$, corresponding to the coefficient of the interaction term $x_1 x_2$. Therefore, $|S|=1$. The vector $\\Delta_{i,S}$ becomes the scalar $\\Delta_{i,3} = \\hat{\\beta}_3 - \\hat{\\beta}_{(i),3}$, and the matrix $(X^\\top X)_S$ is the scalar element $(X^\\top X)_{3,3}$ (using $0$-based indexing). The partial measure simplifies to:\n$$ D_{i,\\{3\\}}^{\\text{partial}} = \\frac{(\\Delta_{i,3})^2 (X^\\top X)_{3,3}}{\\hat{\\sigma}^2} $$\n\n### 3. Algorithmic Implementation\n\nThe solution will be implemented by following these derived formulas directly, without resorting to any algebraic shortcuts for LOO estimation (such as those involving the hat matrix).\n\nFor each test case:\n1.  Construct the design matrix $X \\in \\mathbb{R}^{n \\times 4}$ with columns $[1, x_1, x_2, x_1 x_2]$ and the response vector $y \\in \\mathbb{R}^n$ from the given data-generating process and noise.\n2.  Compute the full-data OLS estimate $\\hat{\\beta}$ by solving the normal equations $(X^\\top X) \\hat{\\beta} = X^\\top y$.\n3.  Calculate the unbiased variance estimate $\\hat{\\sigma}^2 = \\frac{1}{n-p} \\|y - X\\hat{\\beta}\\|^2$.\n4.  Iterate through each observation $i=0, \\dots, n-1$:\n    a. Construct the LOO data $(X_{(i)}, y_{(i)})$ by deleting the $i$-th row.\n    b. Compute the LOO estimate $\\hat{\\beta}_{(i)}$ by solving $(X_{(i)}^\\top X_{(i)}) \\hat{\\beta}_{(i)} = X_{(i)}^\\top y_{(i)}$.\n    c. Calculate the parameter shift $\\Delta_i = \\hat{\\beta} - \\hat{\\beta}_{(i)}$.\n    d. Compute $D_i^{\\text{overall}}$ and $D_{i,\\{3\\}}^{\\text{partial}}$ using the formulas derived above.\n5.  Collect the computed measures and calculate the specific outputs required for each test case (e.g., maxima, counts, means of ratios).\n\nAll numerical results will be rounded to $6$ decimal places as specified. The ratio $\\rho_i = D_{i,\\{3\\}}^{\\text{partial}} / D_i^{\\text{overall}}$ is defined as $0$ if $D_i^{\\text{overall}}=0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases and print the final result.\n    \"\"\"\n    beta_star = np.array([1.0, 2.0, -1.5, 0.8])\n    p = 4\n    S_idx = 3\n\n    # Test Case 1\n    # Givens\n    n1 = 10\n    x1_1 = np.array([-1.0, -0.5, 0.0, 0.5, 1.0, -0.8, 0.7, -0.2, 0.3, 5.0])\n    x2_1 = np.array([0.9, -1.1, 0.2, -0.4, 0.6, -0.7, 0.8, 0.3, -0.5, 5.0])\n    eta1 = np.array([0.05, -0.04, 0.01, -0.02, 0.03, 0.02, -0.01, 0.00, 0.01, 0.00])\n    y1_mod = (9, 8.0)\n    \n    # Process Case 1\n    overall1, partial1 = process_case(n1, x1_1, x2_1, beta_star, eta1, y1_mod, p, S_idx)\n    i_star = np.argmax(partial1)\n    result1 = int(i_star)\n    result2 = round(overall1[i_star], 6)\n    result3 = round(partial1[i_star], 6)\n\n    # Test Case 2\n    # Givens\n    n2 = 12\n    x1_2 = np.array([-1.2, -0.9, -0.6, -0.3, 0.0, 0.3, 0.6, 0.9, 1.2, -0.4, 0.4, 0.1])\n    x2_2 = np.array([0.5, -0.4, 0.7, -0.8, 0.3, -0.2, 0.1, -0.1, 0.2, 0.9, -0.9, 0.0])\n    eta2 = np.array([0.01, -0.02, 0.03, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, -0.02, 0.00])\n    tau = 0.05\n    \n    # Process Case 2\n    overall2, partial2 = process_case(n2, x1_2, x2_2, beta_star, eta2, None, p, S_idx)\n    result4 = round(np.max(overall2), 6)\n    result5 = int(np.sum(partial2 > tau))\n\n    # Test Case 3\n    # Givens\n    n3 = 6\n    x1_3 = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 4.0])\n    x2_3 = np.array([0.5, -0.5, 0.2, -0.2, 0.5, 4.0])\n    eta3 = np.array([0.00, 0.01, -0.01, 0.00, 0.02, 0.00])\n\n    # Process Case 3\n    overall3, partial3 = process_case(n3, x1_3, x2_3, beta_star, eta3, None, p, S_idx)\n    ratios = np.zeros_like(overall3)\n    non_zero_mask = overall3 != 0\n    ratios[non_zero_mask] = partial3[non_zero_mask] / overall3[non_zero_mask]\n    result6 = round(np.mean(ratios), 6)\n\n    # Combine results and print\n    final_results = [result1, result2, result3, result4, result5, result6]\n    print(f\"[{','.join(map(str, final_results))}]\")\n\n\ndef process_case(n, x1, x2, beta_star, eta, y_mod, p, S_idx):\n    \"\"\"\n    Performs the full analysis for a single test case.\n    \"\"\"\n    # 1. Construct design matrix X and response vector y\n    X = np.column_stack([np.ones(n), x1, x2, x1*x2])\n    y = X @ beta_star + eta\n    if y_mod:\n        y[y_mod[0]] += y_mod[1]\n\n    # 2. Full data fit\n    XTX = X.T @ X\n    XTy = X.T @ y\n    beta_hat = np.linalg.solve(XTX, XTy)\n    \n    # 3. Estimate noise variance\n    residuals = y - X @ beta_hat\n    rss = residuals.T @ residuals\n    sigma2_hat = rss / (n - p)\n\n    overall_measures = np.zeros(n)\n    partial_measures = np.zeros(n)\n\n    # 4. Leave-one-out refitting loop\n    for i in range(n):\n        # Create LOO data\n        X_i = np.delete(X, i, axis=0)\n        y_i = np.delete(y, i, axis=0)\n        \n        # LOO fit\n        XTX_i = X_i.T @ X_i\n        XTy_i = X_i.T @ y_i\n        try:\n            beta_hat_i = np.linalg.solve(XTX_i, XTy_i)\n        except np.linalg.LinAlgError:\n            # Handle cases where LOO matrix is singular, not expected for these tests\n            beta_hat_i = np.full(p, np.nan)\n\n        # Parameter shift\n        delta_i = beta_hat - beta_hat_i\n        \n        # Avoid issues with division by zero or NaN propagation\n        if np.isnan(delta_i).any() or sigma2_hat == 0:\n            overall_measures[i] = 0.0\n            partial_measures[i] = 0.0\n            continue\n        \n        # Overall influence measure\n        numerator_overall = delta_i.T @ XTX @ delta_i\n        d_overall = numerator_overall / (p * sigma2_hat)\n        overall_measures[i] = d_overall\n\n        # Partial influence measure\n        delta_i_S = delta_i[S_idx] \n        XTX_S_val = XTX[S_idx, S_idx]\n        \n        numerator_partial = (delta_i_S**2) * XTX_S_val\n        # |S| = 1, so no division by |S|\n        d_partial = numerator_partial / sigma2_hat\n        partial_measures[i] = d_partial\n\n    return overall_measures, partial_measures\n\nsolve()\n\n```"
        },
        {
            "introduction": "Influence is not a monolithic concept, and the \"most influential\" point can change depending on your objective. This practice contrasts Cook's distance, a measure of overall parameter influence, with other diagnostics like the PRESS residual, which focuses on a point's self-prediction error . Through a carefully constructed example, you will see that different diagnostic tools can flag different data points, highlighting the importance of choosing the right tool for the analytical question at hand.",
            "id": "3111497",
            "problem": "Consider a univariate ordinary least squares (OLS) linear regression with an intercept, where the design matrix is $X \\in \\mathbb{R}^{n \\times p}$ with $p = 2$ columns corresponding to the intercept and a single predictor. Let the predictor values be $x_i \\in \\mathbb{R}$ for $i = 0, 1, \\dots, n-1$, and responses $y_i \\in \\mathbb{R}$. The model is $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, and the OLS estimator $\\hat{\\beta}$ is defined by the normal equations. Define the hat matrix $H$ as the orthogonal projector onto the column space of $X$, residuals $r = y - X\\hat{\\beta}$, and the mean squared error $\\mathrm{MSE}$ as the residual sum of squares divided by the degrees of freedom $n - p$. Cook's distance $D_i$ measures the overall impact of deleting observation $i$ on the estimated regression, while the Predicted Residual Error Sum of Squares (PRESS) residual for observation $i$, denoted $\\mathrm{PRESS}_i$, is the leave-one-out residual for observation $i$.\n\nYour task is to:\n- Start from the OLS normal equations and the definition of the hat matrix $H$, and derive expressions that let you compute, for each observation $i$, the Cook's distance $D_i$ and the magnitude of the leave-one-out residual $\\lvert \\mathrm{PRESS}_i \\rvert$ without refitting the regression $n$ times.\n- Define the predictive harm at a specific target point $x^\\ast$ for observation deletion $i$ as the absolute change in the predicted value at $x^\\ast$ between the full-data fit and the fit with observation $i$ deleted, that is $h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert$.\n- Using these definitions, construct a data example and demonstrate that the ranking of observations by Cook's distance $D_i$ differs from the ranking suggested by the magnitudes $\\lvert \\mathrm{PRESS}_i \\rvert$ when evaluated for predictive harm at a specific $x^\\ast$. In other words, show that the observation flagged as most harmful by $D_i$ is not the same as the one whose deletion most changes the prediction at $x^\\ast$, and that this can also differ from the observation with the largest $\\lvert \\mathrm{PRESS}_i \\rvert$.\n\nUse the following scientifically consistent, self-contained dataset (constructed so that the OLS fit equals a simple line and residuals are orthogonal to the column space of $X$):\n- Number of observations $n = 5$.\n- Predictor values $x = [-2, -1, 0, 1, 2]$.\n- Choose a target line $y = 1 + x$ and residuals $r = [1, -2.5, 3, -2.5, 1]$ that satisfy $X^\\top r = 0$; define $y_i = 1 + x_i + r_i$ for $i = 0, 1, 2, 3, 4$.\n\nTest suite (three target points):\n- Case $1$: $x^\\ast = 2.5$.\n- Case $2$: $x^\\ast = 0.0$.\n- Case $3$: $x^\\ast = 10.0$.\n\nImplementation requirements:\n- Work from first principles: use the OLS normal equations, the definition of the hat matrix, and linear algebra identities to obtain computational formulas; do not refit the model $n$ times.\n- For each test case, compute:\n    1. The index (using zero-based indexing) of the observation with the largest Cook's distance $D_i$.\n    2. The index (zero-based) of the observation with the largest $\\lvert \\mathrm{PRESS}_i \\rvert$.\n    3. The index (zero-based) of the observation with the largest predictive harm $h_i(x^\\ast)$.\n    4. A boolean indicating whether the largest Cook's distance corresponds to the largest predictive harm at $x^\\ast$.\n    5. A boolean indicating whether the largest $\\lvert \\mathrm{PRESS}_i \\rvert$ corresponds to the largest predictive harm at $x^\\ast$.\n- In the presence of ties, choose the smallest index.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[\\text{argmaxD}, \\text{argmaxPRESS}, \\text{argmaxHarm}, \\text{CookEqualsHarm}, \\text{PRESSEqualsHarm}]$, aggregated as $[[\\dots],[\\dots],[\\dots]]$ on a single line. All indices are zero-based integers and the booleans must be either $\\text{True}$ or $\\text{False}$.",
            "solution": "### **1. Theoretical Derivations**\n\nLet the OLS model be $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^p$, and $\\varepsilon$ is a vector of errors. The OLS estimate of $\\beta$ is given by the normal equations $X^\\top X \\hat{\\beta} = X^\\top y$, which yields $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$. The predicted values are $\\hat{y} = X\\hat{\\beta} = Hy$, where $H = X(X^\\top X)^{-1}X^\\top$ is the hat matrix. The diagonal elements of $H$, denoted $h_{ii}$, are the leverages of each observation. The residuals are $r = y - \\hat{y} = (I-H)y$.\n\nLet $\\hat{\\beta}^{(-i)}$ denote the OLS estimate of $\\beta$ when observation $i$ (with corresponding data row $x_i^\\top$ and response $y_i$) is removed from the dataset. Our goal is to compute diagnostics related to this leave-one-out (LOO) model without explicitly re-fitting the regression $n$ times.\n\nThe core result is an expression for the change in the coefficient vector, $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$. This can be efficiently derived using the Sherman-Morrison formula for the inverse of a rank-$1$ updated matrix. The matrix $(X^{(-i)})^\\top X^{(-i)}$ can be written as a rank-$1$ update of $X^\\top X$:\n$$ (X^{(-i)})^\\top X^{(-i)} = \\sum_{j \\neq i} x_j x_j^\\top = X^\\top X - x_i x_i^\\top $$\nApplying the Sherman-Morrison formula, $(A - uv^\\top)^{-1} = A^{-1} + \\frac{A^{-1}uv^\\top A^{-1}}{1 - v^\\top A^{-1}u}$, with $A=X^\\top X$ and $u=v=x_i$, gives:\n$$ ((X^{(-i)})^\\top X^{(-i)})^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - x_i^\\top (X^\\top X)^{-1} x_i} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - h_{ii}} $$\nThe LOO estimate is $\\hat{\\beta}^{(-i)} = ((X^{(-i)})^\\top X^{(-i)})^{-1} (X^{(-i)})^\\top y^{(-i)}$. Using the identity above and that $(X^{(-i)})^\\top y^{(-i)} = X^\\top y - x_i y_i$, after algebraic simplification, we arrive at the well-known result:\n$$ \\hat{\\beta} - \\hat{\\beta}^{(-i)} = \\frac{(X^\\top X)^{-1} x_i (y_i - x_i^\\top \\hat{\\beta})}{1 - h_{ii}} = \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\nwhere $r_i = y_i - \\hat{y}_i$ is the $i$-th ordinary residual. This expression is central to computing all required LOO diagnostics efficiently.\n\n#### **PRESS Residual ($\\mathrm{PRESS}_i$)**\nThe Predicted Residual Error Sum of Squares (PRESS) residual is the error in predicting $y_i$ using a model built without observation $i$.\n$$ \\mathrm{PRESS}_i = y_i - \\hat{y}_i^{(-i)} = y_i - x_i^\\top \\hat{\\beta}^{(-i)} $$\nSubstituting $\\hat{\\beta}^{(-i)} = \\hat{\\beta} - (\\hat{\\beta} - \\hat{\\beta}^{(-i)})$:\n$$ \\mathrm{PRESS}_i = y_i - x_i^\\top \\left( \\hat{\\beta} - \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = (y_i - x_i^\\top \\hat{\\beta}) + \\frac{x_i^\\top (X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n$$ \\mathrm{PRESS}_i = r_i + \\frac{h_{ii} r_i}{1 - h_{ii}} = r_i \\left( 1 + \\frac{h_{ii}}{1 - h_{ii}} \\right) = \\frac{r_i}{1 - h_{ii}} $$\nThis provides a simple formula for the PRESS residual using only the ordinary residual $r_i$ and the leverage $h_{ii}$. We are interested in its magnitude, $\\lvert \\mathrm{PRESS}_i \\rvert$.\n\n#### **Cook's Distance ($D_i$)**\nCook's distance measures the effect of deleting observation $i$ on the vector of fitted values. It is defined as:\n$$ D_i = \\frac{(\\hat{y} - \\hat{y}^{(-i)})^\\top (\\hat{y} - \\hat{y}^{(-i)})}{p \\cdot \\mathrm{MSE}} $$\nwhere $\\hat{y}^{(-i)} = X \\hat{\\beta}^{(-i)}$ and $\\mathrm{MSE} = \\frac{1}{n-p} \\sum_{j=1}^n r_j^2$. The numerator is the squared Euclidean distance between the fitted value vectors.\n$$ \\hat{y} - \\hat{y}^{(-i)} = X(\\hat{\\beta} - \\hat{\\beta}^{(-i)}) = X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\nThe numerator becomes:\n$$ \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right)^\\top \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = \\frac{r_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} x_i = \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nSubstituting this back into the definition of $D_i$:\n$$ D_i = \\frac{1}{p \\cdot \\mathrm{MSE}} \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nThis formula allows calculation of $D_i$ from the residual $r_i$, leverage $h_{ii}$, number of predictors $p$, and the mean squared error $\\mathrm{MSE}$.\n\n#### **Predictive Harm ($h_i(x^\\ast)$)**\nThe predictive harm at a target point $x^\\ast$ is the absolute change in the prediction at that point when observation $i$ is deleted. Let $x_{new}^\\top = [1, x^\\ast]$.\n$$ h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert = \\lvert x_{new}^\\top \\hat{\\beta} - x_{new}^\\top \\hat{\\beta}^{(-i)} \\rvert = \\lvert x_{new}^\\top (\\hat{\\beta} - \\hat{\\beta}^{(-i)}) \\rvert $$\nUsing our core result for $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$:\n$$ h_i(x^\\ast) = \\left\\lvert x_{new}^\\top \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right\\rvert = \\frac{\\lvert r_i \\rvert}{1 - h_{ii}} \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert \\mathrm{PRESS}_i \\rvert \\cdot \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert $$\nThis shows that predictive harm is the magnitude of the PRESS residual, re-weighted by a factor that depends on the target point $x^\\ast$ and the deleted point $x_i$.\n\n### **2. Application to the Dataset**\n\nWe are given $n=5$, $p=2$, predictors $x = [-2, -1, 0, 1, 2]^\\top$, and residuals $r = [1, -2.5, 3, -2.5, 1]^\\top$. The design matrix $X$ is:\n$$ X = \\begin{pmatrix} 1 & -2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\nThe problem is constructed such that the OLS fit for $y_i = (1+x_i) + r_i$ is exactly $\\hat{y}_i = 1+x_i$. The predictor values are centered, $\\sum_{i=0}^{4} x_i = 0$. This simplifies $X^\\top X$:\n$$ X^\\top X = \\begin{pmatrix} n & \\sum x_i \\\\ \\sum x_i & \\sum x_i^2 \\end{pmatrix} = \\begin{pmatrix} 5 & 0 \\\\ 0 & 10 \\end{pmatrix} \\implies (X^\\top X)^{-1} = \\begin{pmatrix} 1/5 & 0 \\\\ 0 & 1/10 \\end{pmatrix} $$\nThe leverage for observation $i$ is $h_{ii} = x_i^\\top (X^\\top X)^{-1} x_i = \\begin{pmatrix} 1 & x_i \\end{pmatrix} \\begin{pmatrix} 1/5 & 0 \\\\ 0 & 1/10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix} = \\frac{1}{5} + \\frac{x_i^2}{10}$.\nThe leverages for $i=0, \\dots, 4$ are $[0.6, 0.3, 0.2, 0.3, 0.6]$.\n\nThe Residual Sum of Squares is $\\mathrm{RSS} = \\sum r_i^2 = 1^2 + (-2.5)^2 + 3^2 + (-2.5)^2 + 1^2 = 23.5$.\nThe Mean Squared Error is $\\mathrm{MSE} = \\frac{\\mathrm{RSS}}{n-p} = \\frac{23.5}{5-2} = \\frac{23.5}{3}$.\n\nNow we can compute the diagnostics.\n- The factor for Cook's distance is $D_i \\propto r_i^2 \\frac{h_{ii}}{(1-h_{ii})^2}$. The values, proportional to $D_i$, are $[3.75, 3.8265, 2.8125, 3.8265, 3.75]$. The maximum occurs at indices $1$ and $3$. The smallest index is selected: $\\text{argmax}(D_i) = 1$.\n- The PRESS residual magnitudes are $|\\mathrm{PRESS}_i| = \\frac{|r_i|}{1-h_{ii}}$. The values are $[2.5, 3.5714, 3.75, 3.5714, 2.5]$. The maximum occurs at index $2$: $\\text{argmax}(|\\mathrm{PRESS}_i|) = 2$.\n\nThe predictive harm factor is $\\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert [1, x^\\ast] [1/5, x_i/10]^\\top \\rvert = \\lvert \\frac{1}{5} + \\frac{x^\\ast x_i}{10} \\rvert$.\n\n- **Case 1: $x^\\ast = 2.5$**\n$h_i(2.5) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0.25 x_i \\rvert$. The values are $[0.75, 0.1786, 0.75, 1.6071, 1.75]$.\n$\\text{argmax}(h_i(2.5)) = 4$.\nResults: $[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$.\n\n- **Case 2: $x^\\ast = 0.0$**\n$h_i(0.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0 \\rvert = 0.2 |\\mathrm{PRESS}_i|$. The ranking is the same as for $|\\mathrm{PRESS}_i|$.\n$\\text{argmax}(h_i(0.0)) = 2$.\nResults: $[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=2, \\text{False}, \\text{True}]$.\n\n- **Case 3: $x^\\ast = 10.0$**\n$h_i(10.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + x_i \\rvert$. The values are $[4.5, 2.8571, 0.75, 4.2857, 5.5]$.\n$\\text{argmax}(h_i(10.0)) = 4$.\nResults: $[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$.\n\nThis analysis demonstrates that the concept of \"most influential point\" is context-dependent. Cook's distance averages influence over the entire data space and identifies observation $1$ (and $3$) as most influential due to a combination of a moderately large residual and moderate leverage. The PRESS score, focusing only on self-prediction, identifies observation $2$ due to its maximal residual magnitude. Predictive harm, however, depends on the target point $x^\\ast$. For predictions far from the data's center (large $|x^\\ast|$), high-leverage points (like $0$ and $4$) become dominant, as their removal most affects the regression slope.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression influence diagnostics problem.\n\n    The solution proceeds in three main steps:\n    1.  Setup: Define the dataset (x, r), the number of observations (n), and\n        the number of parameters (p). The response y and coefficients are\n        implicitly defined by the problem statement.\n    2.  Pre-computation: Calculate quantities that are constant across all test\n        cases. This includes the (X^T X)^-1 matrix, leverages (h_ii),\n        mean squared error (MSE), Cook's distances (D_i), and PRESS residuals.\n        The indices of the observations with the largest D_i and |PRESS_i|\n        are determined here.\n    3.  Per-case computation: Loop through each target point x_star provided\n        in the test suite. For each case, calculate the predictive harm h_i(x_star)\n        for all observations, find the index of the observation with the\n        largest harm, and compare this index with the pre-computed indices for\n        D_i and |PRESS_i|.\n    \"\"\"\n    # 1. Setup based on the problem statement\n    n = 5  # Number of observations\n    p = 2  # Number of parameters (intercept + 1 predictor)\n    x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    r = np.array([1.0, -2.5, 3.0, -2.5, 1.0]) # Ordinary residuals\n\n    # 2. Pre-computation of constant quantities\n    \n    # Design matrix X would be np.c_[np.ones(n), x]\n    # Since sum(x) = 0, X^T*X is a diagonal matrix.\n    # X_T_X = [[n, sum(x)], [sum(x), sum(x^2)]] = [[5, 0], [0, 10]]\n    X_T_X_inv = np.array([[1/n, 0], [0, 1/np.sum(x**2)]])\n\n    # Calculate leverages (h_ii). h_ii = x_i^T * (X^T*X)^-1 * x_i\n    # where x_i is the i-th row of X, i.e., [1, x_i].\n    h_ii = np.zeros(n)\n    for i in range(n):\n        x_i_row = np.array([1, x[i]])\n        h_ii[i] = x_i_row @ X_T_X_inv @ x_i_row\n\n    # Calculate Mean Squared Error (MSE)\n    rss = np.sum(r**2)\n    mse = rss / (n - p)\n\n    # Calculate Cook's Distances (D_i)\n    # D_i = r_i^2 * h_ii / (p * MSE * (1 - h_ii)^2)\n    D_i = (r**2 / (p * mse)) * (h_ii / (1 - h_ii)**2)\n    argmax_D = np.argmax(D_i)\n\n    # Calculate magnitudes of PRESS residuals\n    # |PRESS_i| = |r_i / (1 - h_ii)|\n    press_i_mag = np.abs(r / (1 - h_ii))\n    argmax_press = np.argmax(press_i_mag)\n\n    # 3. Per-case computation for each target point x_star\n    test_cases = [2.5, 0.0, 10.0]\n    results = []\n\n    for x_star in test_cases:\n        # Calculate predictive harm h_i(x_star)\n        # h_i(x_star) = |PRESS_i| * |x_new^T * (X^T*X)^-1 * x_i|\n        # where x_new = [1, x_star] and x_i = [1, x[i]]\n        \n        harm_factor = np.zeros(n)\n        x_new_row = np.array([1, x_star])\n        for i in range(n):\n            x_i_row = np.array([1, x[i]])\n            harm_factor[i] = np.abs(x_new_row @ X_T_X_inv @ x_i_row)\n        \n        h_i = press_i_mag * harm_factor\n        argmax_harm = np.argmax(h_i)\n        \n        # Check if the argmax indices match\n        cook_equals_harm = (argmax_D == argmax_harm)\n        press_equals_harm = (argmax_press == argmax_harm)\n        \n        # Append the results for the current case\n        case_result = [\n            int(argmax_D),\n            int(argmax_press),\n            int(argmax_harm),\n            bool(cook_equals_harm),\n            bool(press_equals_harm)\n        ]\n        results.append(case_result)\n\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "After identifying influential points, a common but delicate next step is to consider removing them. This exercise simulates this data-cleaning strategy within a cross-validation framework to reveal the fundamental trade-off involved . You will quantitatively measure how removing points with high Cook's distance can reduce model overfitting, but also how it may lead to information loss and poorer performance on new, unseen data, providing a crucial lesson in the practical application of influence diagnostics.",
            "id": "3111535",
            "problem": "You are asked to design and implement a complete, runnable program that evaluates a cross-validation procedure for ordinary least squares regression which removes influential observations before fitting, where influence is quantified by Cook’s distance. Your program must adhere to the following mathematical setup and produce results for a fixed test suite.\n\nThe fundamental base is ordinary least squares (OLS) linear regression. Given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, an OLS estimator $\\hat{\\beta} \\in \\mathbb{R}^{p}$ minimizes the sum of squared residuals. The hat matrix is $H = X (X^{\\top} X)^{-1} X^{\\top}$, fitted values are $\\hat{y} = H y$, residuals are $e = y - \\hat{y}$, and the diagonal entries $h_{ii}$ of $H$ are called leverages. Cook’s distance for observation $i$ is an influence measure that compares the fitted model using all data to the fitted model when observation $i$ is left out. You must derive the computable expression for Cook’s distance starting from the OLS normal equations and the definition of the hat matrix, and then use it in your algorithm. Do not assume any pre-given formula for Cook’s distance in your design.\n\nYour task is to:\n- Construct a $k$-fold cross-validation loop on a training set. For each fold, within the training portion of the fold only, compute Cook’s distance $D_i$ for each training observation. Remove those points with $D_i$ strictly greater than a user-specified threshold $\\tau$ before fitting the model for that fold. Fit on the remaining training observations of the fold, compute the mean squared error (MSE) on the remaining training observations for that fold, and compute the MSE on the held-out validation portion for that fold. Average the validation-minus-training MSE gap across folds to obtain a cross-validation overfitting gap.\n- Separately, fit two models on the full training set (not per-fold): one model using all training observations and one model after removing points whose Cook’s distance exceeds the same threshold $\\tau$ as computed on the full training set. Evaluate both models on an external, disjoint test set, and take the difference in test MSEs (cleaned model minus full model) as a measure of information loss on the external test set.\n\nData generation protocol:\n- For each test case, generate the training features $X \\in \\mathbb{R}^{n \\times p}$ with entries independently drawn from a standard normal distribution $\\mathcal{N}(0,1)$.\n- Generate a coefficient vector $\\beta \\in \\mathbb{R}^{p}$ deterministically by setting the $j$-th component to $\\beta_j = (-1)^{j+1} \\cdot \\frac{j}{p}$ for $j \\in \\{1,\\dots,p\\}$.\n- Generate a clean noise vector $\\varepsilon \\in \\mathbb{R}^{n}$ with independent entries $\\varepsilon_i \\sim \\mathcal{N}(0,1)$.\n- To induce influential observations, choose a subset $S$ of the training indices of size $\\lfloor f \\cdot n \\rfloor$ (where $f \\in [0,1]$ is the specified outlier fraction) and modify those rows by multiplying their feature vectors by a leverage scale $a > 0$ and adding heavy-tailed noise by replacing $\\varepsilon_i$ with $\\varepsilon_i' \\sim \\mathcal{N}(0,b^2)$ for $i \\in S$, where $b > 0$ is the specified outlier noise scale. The response is $y = X \\beta + \\varepsilon$ for non-outlier rows and $y = X \\beta + \\varepsilon'$ for outlier rows after scaling $X$ by $a$.\n- Generate an external test set $(X_{\\text{test}}, y_{\\text{test}})$ of size $n_{\\text{test}}$ in the same way as the clean (non-outlier) data with $\\varepsilon_{\\text{test}} \\sim \\mathcal{N}(0,1)$ and without any outlier modifications. There are no physical units.\n\nAlgorithmic requirements:\n- In each fold, compute influence from first principles using OLS quantities from the training portion, then apply the threshold $\\tau$ to drop high-influence points, refit, and compute fold training MSE and fold validation MSE. If removal yields fewer than $p+1$ training observations in a fold, do not remove any observations in that fold.\n- Compute the cross-validation overfitting gap as the average across folds of $(\\text{validation MSE} - \\text{training MSE})$.\n- Compute the information loss on the external test set as the difference $(\\text{test MSE of cleaned full-training model} - \\text{test MSE of full-training model without removal})$.\n- Also report the fraction of training observations removed on the full training set.\n\nTest suite:\n- Use the following four test cases. Each case specifies a random seed $s$, training size $n$, test size $n_{\\text{test}}$, number of features $p$, number of folds $k$, Cook’s threshold $\\tau$, outlier fraction $f$, leverage scale $a$, and outlier noise scale $b$.\n    1. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (42, 120, 200, 5, 5, +\\infty, 0.1, 6, 8)$.\n    2. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (42, 120, 200, 5, 5, \\frac{4}{n}, 0.1, 6, 8)$.\n    3. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (42, 120, 200, 5, 5, \\frac{1}{n}, 0.1, 6, 8)$.\n    4. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (123, 80, 200, 6, 4, \\frac{1}{n}, 0, 6, 8)$.\n- For each test case, your program must output a list of three floats:\n    - The cross-validation overfitting reduction, defined as the baseline gap with no removal minus the gap with removal at threshold $\\tau$ for that case.\n    - The information loss on the external test set, defined as the test mean squared error of the cleaned full-training model minus the test mean squared error of the full-training model without removal.\n    - The fraction of training points removed when computing Cook’s distance on the full training set.\n- The baseline gap is always computed with $\\tau = \\infty$ (no removal); the comparison gap uses the case-specific $\\tau$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, one list per test case, each inner list ordered as specified above. Round every float to $6$ decimal places. For example, an output with two cases should look like $[[0.123456,0.000001,0.100000],[0.000000,0.000000,0.000000]]$.\n\nAngle units are not applicable. No physical units are involved. All numeric quantities in the output must be dimensionless real numbers rounded to $6$ decimal places.",
            "solution": "### 1. Derivation of Cook's Distance\n\nCook's distance, $D_i$, for an observation $i$ measures the change in the fitted regression coefficients when that observation is removed from the dataset. It is formally defined in terms of the change in the fitted response values.\n\nLet the linear model be $y = X\\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix of $n$ observations and $p$ features, $y \\in \\mathbb{R}^{n}$ is the response vector, and $\\beta \\in \\mathbb{R}^{p}$ is the coefficient vector. The OLS estimator, $\\hat{\\beta}$, which minimizes the sum of squared residuals, is given by the normal equations:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\nThe vector of fitted values is $\\hat{y} = X\\hat{\\beta} = X(X^\\top X)^{-1} X^\\top y = Hy$, where $H = X(X^\\top X)^{-1} X^\\top$ is the hat matrix. The diagonal elements of $H$, denoted $h_{ii}$, are the leverages of each observation.\n\nLet $\\hat{\\beta}_{(i)}$ be the OLS estimator computed with the $i$-th observation $(x_i^\\top, y_i)$ removed. Let $X_{(i)}$ and $y_{(i)}$ denote the data matrices with the $i$-th row excluded. Then $\\hat{\\beta}_{(i)} = (X_{(i)}^\\top X_{(i)})^{-1} X_{(i)}^\\top y_{(i)}$. Cook's distance is defined as the scaled squared Euclidean distance between the vectors of fitted values obtained with all data and with observation $i$ removed:\n$$ D_i = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p \\cdot s^2} = \\frac{\\| \\hat{y} - \\hat{y}_{(i)} \\|_2^2}{p \\cdot s^2} $$\nwhere $\\hat{y}_{(i)} = X \\hat{\\beta}_{(i)}$ is the vector of predictions for the original data points using the leave-one-out model, and $s^2 = \\frac{1}{n-p} e^\\top e$ is the mean squared error (unbiased variance estimate) of the full model, with $e = y - \\hat{y}$ being the vector of residuals.\n\nTo find a computable expression for $D_i$, we need to relate $\\hat{\\beta}$ to $\\hat{\\beta}_{(i)}$. A key identity, derived using the Sherman-Morrison-Woodbury formula, connects the two:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{(X^\\top X)^{-1} x_i (y_i - x_i^\\top \\hat{\\beta})}{1 - h_{ii}} = \\frac{(X^\\top X)^{-1} x_i e_i}{1 - h_{ii}} $$\nwhere $e_i$ is the residual for observation $i$ from the full model, and $h_{ii} = x_i^\\top (X^\\top X)^{-1} x_i$ is its leverage.\n\nThe numerator of $D_i$ is $\\| X(\\hat{\\beta} - \\hat{\\beta}_{(i)}) \\|_2^2 = (\\hat{\\beta} - \\hat{\\beta}_{(i)})^\\top X^\\top X (\\hat{\\beta} - \\hat{\\beta}_{(i)})$. Substituting the expression for $\\hat{\\beta} - \\hat{\\beta}_{(i)}$:\n$$ (\\hat{\\beta} - \\hat{\\beta}_{(i)})^\\top X^\\top X (\\hat{\\beta} - \\hat{\\beta}_{(i)}) = \\left( \\frac{(X^\\top X)^{-1} x_i e_i}{1 - h_{ii}} \\right)^\\top X^\\top X \\left( \\frac{(X^\\top X)^{-1} x_i e_i}{1 - h_{ii}} \\right) $$\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} (X^\\top X) (X^\\top X)^{-1} x_i $$\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} x_i = \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nSubstituting this result back into the definition of $D_i$, we obtain the final computable expression:\n$$ D_i = \\frac{e_i^2}{p \\cdot s^2} \\frac{h_{ii}}{(1 - h_{ii})^2} $$\nThis formula allows for the efficient calculation of all $D_i$ from the quantities of a single OLS fit on the full dataset: the residuals $e_i$, leverages $h_{ii}$, the number of features $p$, and the overall mean squared error $s^2$. The leverages $h_{ii}$ can be computed efficiently as the diagonal elements of $QQ^\\top$, where $Q$ is the orthogonal matrix from the QR decomposition of $X$.\n\n### 2. Algorithmic Design\n\nThe implementation follows the problem specification, encompassing data generation, a cross-validation procedure, and a final evaluation on a test set.\n\n**Data Generation:**\nFor each test case, data is generated according to the specified protocol.\n1.  A random seed $s$ ensures reproducibility.\n2.  The training feature matrix $X \\in \\mathbb{R}^{n \\times p}$ is drawn from $\\mathcal{N}(0,1)$.\n3.  The true coefficient vector $\\beta \\in \\mathbb{R}^{p}$ is created deterministically with $\\beta_j = (-1)^{j+1} \\cdot \\frac{j}{p}$.\n4.  A noise vector $\\varepsilon \\in \\mathbb{R}^{n}$ is drawn from $\\mathcal{N}(0,1)$.\n5.  A subset $S$ of $\\lfloor f \\cdot n \\rfloor$ indices is selected to be influential points. For each $i \\in S$, the feature vector $x_i^\\top$ is scaled by $a$ and the noise term $\\varepsilon_i$ is replaced by a draw from a heavy-tailed distribution $\\mathcal{N}(0,b^2)$.\n6.  The final response vector is $y = X\\beta + \\text{noise}$.\n7.  A test set $(X_{\\text{test}}, y_{\\text{test}})$ of size $n_{\\text{test}}$ is generated in the same manner as the \"clean\" (non-influential) training data.\n\n**Cross-Validation Overfitting Gap:**\nThis metric quantifies the degree of overfitting.\n1.  The training data indices are randomly shuffled and split into $k$ equal-sized folds.\n2.  For each fold, the data for that fold is held out as the validation set, and the remaining $k-1$ folds constitute the training set for that iteration.\n3.  Within each iteration, Cook's distances $D_i$ are computed for all observations in the current training set.\n4.  Observations with $D_i > \\tau$ are identified for removal. A crucial condition is checked: if removing these points would leave the training set with fewer than $p+1$ observations, no points are removed for that fold to ensure the OLS problem remains well-defined.\n5.  An OLS model is fitted on the (potentially cleaned) training data to obtain $\\hat{\\beta}_{\\text{fold}}$.\n6.  The mean squared error (MSE) is calculated on this same (cleaned) training set, yielding $\\text{MSE}_{\\text{train,fold}}$.\n7.  The model is then used to predict responses for the held-out validation set, yielding $\\text{MSE}_{\\text{val,fold}}$.\n8.  The overfitting gap for the fold is $\\text{MSE}_{\\text{val,fold}} - \\text{MSE}_{\\text{train,fold}}$.\n9.  This process is repeated for all $k$ folds, and the average of the $k$ gaps is the final cross-validation overfitting gap.\n\n**Output Metrics:**\nFor each test case, three specific metrics are computed:\n1.  **Cross-validation overfitting reduction:** This is the primary metric for evaluating the cross-validation procedure. It's defined as $(\\text{Gap}_{\\tau=\\infty} - \\text{Gap}_{\\tau=\\text{case}})$, where $\\text{Gap}_{\\tau=\\infty}$ is the overfitting gap calculated without removing any points (baseline), and $\\text{Gap}_{\\tau=\\text{case}}$ is the gap calculated using the specific threshold $\\tau$ for that test case. A positive value indicates that removing influential points reduces overfitting.\n2.  **Information loss on the external test set:** This metric assesses the impact of cleaning on generalization performance on unseen data.\n    -   A \"full model\" is trained using all $n$ training observations.\n    -   A \"cleaned model\" is trained after removing observations from the full training set for which $D_i > \\tau$ (subject to the same $p+1$ constraint).\n    -   Both models are evaluated on the external test set. The information loss is the difference in their test MSEs: $\\text{MSE}_{\\text{test,cleaned}} - \\text{MSE}_{\\text{test,full}}$. A positive value suggests that the cleaning process removed information valuable for prediction on clean external data.\n3.  **Fraction of training points removed:** This is simply the number of points removed during the training of the \"cleaned model\" on the full training set, divided by the total number of training points, $n$.\n\nThe implementation will systematically execute these steps for each test case in the provided suite, ensuring all calculations adhere to these principles and rounding the final numerical results to six decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef get_cooks_distances(X, y):\n    \"\"\"\n    Computes Cook's distance for each observation in a linear regression model.\n    \"\"\"\n    n, p = X.shape\n    if n <= p:\n        return np.zeros(n)\n\n    try:\n        beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n    except np.linalg.LinAlgError:\n        return np.full(n, np.inf)\n\n    residuals = y - X @ beta_hat\n    mse = (residuals @ residuals) / (n - p)\n\n    if mse < 1e-12:\n        return np.full(n, np.inf)\n\n    # Efficiently compute leverages h_ii from QR decomposition\n    Q, _ = np.linalg.qr(X)\n    leverages = np.sum(Q**2, axis=1)\n\n    # Clip for numerical stability if h_ii is close to 1\n    leverages = np.clip(leverages, 0, 1 - 1e-8)\n\n    cooks_d = (residuals**2 / (p * mse)) * (leverages / (1 - leverages)**2)\n    return cooks_d\n\ndef calculate_cv_gap(X_full, y_full, k, p_features, tau, rng):\n    \"\"\"\n    Performs k-fold cross-validation and computes the average overfitting gap.\n    \"\"\"\n    n = X_full.shape[0]\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    fold_indices = np.array_split(indices, k)\n    \n    gaps = []\n\n    for i in range(k):\n        val_idx = fold_indices[i]\n        train_idx_list = [fold_indices[j] for j in range(k) if j != i]\n        if not train_idx_list:\n            continue\n        train_idx = np.concatenate(train_idx_list)\n\n        X_train_fold, y_train_fold = X_full[train_idx], y_full[train_idx]\n        X_val_fold, y_val_fold = X_full[val_idx], y_full[val_idx]\n        \n        n_train_fold = X_train_fold.shape[0]\n        \n        X_train_final, y_train_final = X_train_fold, y_train_fold\n        \n        if tau != float('inf'):\n            cooks_d = get_cooks_distances(X_train_fold, y_train_fold)\n            to_remove = np.where(cooks_d > tau)[0]\n\n            if n_train_fold - len(to_remove) >= p_features + 1:\n                keep_idx = np.setdiff1d(np.arange(n_train_fold), to_remove)\n                X_train_final = X_train_fold[keep_idx]\n                y_train_final = y_train_fold[keep_idx]\n        \n        if X_train_final.shape[0] <= p_features:\n            # Not enough points to fit model, count as max error\n            gaps.append(np.var(y_val_fold))\n            continue\n\n        try:\n            beta_hat = np.linalg.lstsq(X_train_final, y_train_final, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            gaps.append(np.var(y_val_fold))\n            continue\n\n        train_mse = np.mean((y_train_final - X_train_final @ beta_hat)**2)\n        val_mse = np.mean((y_val_fold - X_val_fold @ beta_hat)**2)\n        gaps.append(val_mse - train_mse)\n\n    return np.mean(gaps) if gaps else 0.0\n\ndef analyze_full_set(X_train, y_train, X_test, y_test, p_features, tau):\n    \"\"\"\n    Computes information loss and removal fraction on the full training set.\n    \"\"\"\n    n_train = X_train.shape[0]\n\n    # Full model\n    beta_full = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n    mse_full_test = np.mean((y_test - X_test @ beta_full)**2)\n\n    # Cleaned model\n    cooks_d = get_cooks_distances(X_train, y_train)\n    to_remove = np.where(cooks_d > tau)[0]\n    \n    keep_idx = np.arange(n_train)\n    if n_train - len(to_remove) >= p_features + 1:\n        keep_idx = np.setdiff1d(np.arange(n_train), to_remove)\n    \n    fraction_removed = (n_train - len(keep_idx)) / n_train\n    \n    X_train_clean, y_train_clean = X_train[keep_idx], y_train[keep_idx]\n    \n    beta_clean = np.linalg.lstsq(X_train_clean, y_train_clean, rcond=None)[0]\n    mse_clean_test = np.mean((y_test - X_test @ beta_clean)**2)\n    \n    info_loss = mse_clean_test - mse_full_test\n    \n    return info_loss, fraction_removed\n    \ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (42, 120, 200, 5, 5, float('inf'), 0.1, 6, 8),\n        (42, 120, 200, 5, 5, 4/120, 0.1, 6, 8),\n        (42, 120, 200, 5, 5, 1/120, 0.1, 6, 8),\n        (123, 80, 200, 6, 4, 1/80, 0, 6, 8)\n    ]\n    \n    all_results = []\n    baseline_gap_cache = {}\n\n    for s, n, n_test, p, k, tau, f, a, b in test_cases:\n        rng = np.random.default_rng(s)\n\n        # Generate data\n        X_train_full = rng.normal(0, 1, size=(n, p))\n        beta = np.array([((-1)**(j+1)) * (j/p) for j in range(1, p + 1)])\n        \n        noise = rng.normal(0, 1, size=n)\n        if f > 0:\n            num_outliers = int(np.floor(f * n))\n            outlier_indices = rng.choice(n, size=num_outliers, replace=False)\n            X_train_full[outlier_indices, :] *= a\n            noise[outlier_indices] = rng.normal(0, b, size=num_outliers)\n            \n        y_train_full = X_train_full @ beta + noise\n        \n        X_test = rng.normal(0, 1, size=(n_test, p))\n        y_test = X_test @ beta + rng.normal(0, 1, size=n_test)\n\n        # Use caching for baseline gap on the same dataset\n        data_key = (s, n, p, f, a, b)\n        if data_key not in baseline_gap_cache:\n            # We need a new RNG for CV shuffling to be consistent across runs\n            cv_rng = np.random.default_rng(s) \n            baseline_gap_cache[data_key] = calculate_cv_gap(\n                X_train_full, y_train_full, k, p, float('inf'), cv_rng)\n\n        avg_baseline_gap = baseline_gap_cache[data_key]\n        \n        cv_rng = np.random.default_rng(s) # Reset RNG for fair comparison\n        avg_cleaned_gap = calculate_cv_gap(\n            X_train_full, y_train_full, k, p, tau, cv_rng)\n            \n        overfitting_reduction = avg_baseline_gap - avg_cleaned_gap\n\n        info_loss, removal_fraction = analyze_full_set(\n            X_train_full, y_train_full, X_test, y_test, p, tau)\n\n        all_results.append([overfitting_reduction, info_loss, removal_fraction])\n\n    # Format the final output string\n    formatted_results = []\n    for res_list in all_results:\n        formatted_list = [f\"{v:.6f}\" for v in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}