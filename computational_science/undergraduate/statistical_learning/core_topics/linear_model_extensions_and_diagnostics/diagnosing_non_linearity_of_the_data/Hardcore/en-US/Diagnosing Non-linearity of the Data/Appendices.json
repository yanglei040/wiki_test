{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of model diagnostics is the principle that a good model should leave no discernible pattern in its errors. This exercise  puts this principle into practice by exploring a common non-linear relationship: saturation. You will simulate data that follows this pattern and discover how the shortcomings of a simple linear fit are revealed both by a structured pattern in its residuals and by the superior performance of a correctly specified non-linear model. This practice builds foundational skills in fitting both linear and non-linear models and using concrete metrics like Mean Squared Error and residual correlation to make principled decisions about model adequacy.",
            "id": "3114995",
            "problem": "You are given a data-generating process that exhibits saturation. The response is generated according to the model $$y = \\alpha + \\beta\\left(1 - e^{-\\gamma x}\\right) + \\varepsilon,$$ where $x$ is a predictor, $\\alpha$ is an intercept, $\\beta$ is an amplitude parameter, $\\gamma$ is a rate parameter controlling the saturation, and $\\varepsilon$ is a mean-zero noise term. Your task is to implement a program that, for each specified test case, simulates data from this model, fits both a linear and a nonlinear model, computes diagnostics to assess non-linearity, and then outputs a boolean decision for each test case indicating whether non-linearity is detected by the diagnostics.\n\nFundamental base for the task:\n- Ordinary Least Squares (OLS) fits a linear model by minimizing the sum of squared residuals. Given observed pairs $(x_i, y_i)$ for $i=1,\\dots,n$, the linear model $$y_i = a + b x_i + \\text{residual}_i$$ is fit by minimizing $$\\sum_{i=1}^n \\left(y_i - a - b x_i\\right)^2.$$ Let the fitted values be $\\hat{y}_i^{\\text{lin}}$ and residuals be $r_i^{\\text{lin}} = y_i - \\hat{y}_i^{\\text{lin}}$.\n- Nonlinear Least Squares fits a parametric nonlinear model by minimizing the sum of squared residuals. For the saturating model $$y_i = \\alpha + \\beta\\left(1 - e^{-\\gamma x_i}\\right) + \\text{residual}_i,$$ we obtain fitted values $\\hat{y}_i^{\\text{nl}}$ and residuals $r_i^{\\text{nl}} = y_i - \\hat{y}_i^{\\text{nl}}$ by minimizing $$\\sum_{i=1}^n \\left(y_i - \\alpha - \\beta\\left(1 - e^{-\\gamma x_i}\\right)\\right)^2.$$\n- The mean squared error (MSE) for a set of residuals $\\{r_i\\}_{i=1}^n$ is $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n r_i^2.$$\n- The coefficient of determination is defined as $$R^2 = 1 - \\frac{\\sum_{i=1}^n r_i^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2},$$ where $\\bar{y}$ is the sample mean of $y_i$.\n- The Spearman rank correlation coefficient between two sequences $(u_i)$ and $(v_i)$ measures the monotonic association based on ranks. Denote it by $\\rho_s(u, v)$.\n\nDiagnostic criteria to detect non-linearity:\n- Compute the linear fit and the nonlinear saturating fit for each case. From each fit, compute $\\text{MSE}_{\\text{lin}}$, $\\text{MSE}_{\\text{nl}}$, $R^2_{\\text{lin}}$, $R^2_{\\text{nl}}$, and the Spearman rank correlation coefficient $\\rho_s(r^{\\text{lin}}, x)$ between the linear residuals $r^{\\text{lin}}$ and the predictor $x$.\n- Declare that non-linearity is detected for a test case if both conditions hold:\n  1. $$\\frac{\\text{MSE}_{\\text{lin}}}{\\text{MSE}_{\\text{nl}}} \\ge \\tau,$$ where $\\tau = 1.05$.\n  2. $$\\left|\\rho_s\\left(r^{\\text{lin}}, x\\right)\\right| \\ge t,$$ where $t = 0.25$.\nThese criteria operationalize two complementary diagnostics: substantial improvement in error by the nonlinear fit and monotonic structure remaining in the linear residuals.\n\nData simulation details:\n- For each test case, generate $n$ equally spaced points $x$ on the interval $[0, x_{\\max}]$ inclusive.\n- Generate $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ independently for each observation.\n- Use the parameters $(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n)$ specified for each test case.\n- Use a fixed random number generator seed of $12345$ for reproducibility.\n\nTest suite:\n- Case $1$: $(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.5, 0.02, 0.1, 5.0, 200)$\n- Case $2$: $(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.5, 1.0, 0.1, 5.0, 200)$\n- Case $3$: $(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 0.0, 0.5, 0.1, 5.0, 200)$\n- Case $4$: $(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.5, 0.5, 1.5, 5.0, 200)$\n- Case $5$: $(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.0, 0.5, 0.2, 3.0, 150)$\n\nYour program must:\n- Implement the data simulation and both model fits for each test case.\n- Compute $\\text{MSE}_{\\text{lin}}$, $\\text{MSE}_{\\text{nl}}$, $R^2_{\\text{lin}}$, $R^2_{\\text{nl}}$, and $\\rho_s\\left(r^{\\text{lin}}, x\\right)$.\n- Apply the decision rule with $\\tau = 1.05$ and $t = 0.25$ to produce a boolean per case indicating whether non-linearity is detected.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example `[result_1,result_2,...,result_5]`, where each `result_k` is a boolean corresponding to test case $k$ in order.",
            "solution": "We begin with the fundamental framework that Ordinary Least Squares (OLS) seeks parameters that minimize the sum of squared residuals. For the linear model $$y_i = a + b x_i + \\text{residual}_i,$$ the OLS estimates are obtained by minimizing $$\\sum_{i=1}^n \\left(y_i - a - b x_i\\right)^2.$$ This optimization can be solved via the normal equations or a least squares solver; its validity relies on the property that, under classical assumptions such as independent mean-zero errors with finite variance, OLS provides unbiased estimators of linear model parameters and is the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov theorem. Even without these assumptions, OLS yields the minimizer of the sum of squared residuals.\n\nFor the saturating nonlinear model $$y_i = \\alpha + \\beta\\left(1 - e^{-\\gamma x_i}\\right) + \\text{residual}_i,$$ Nonlinear Least Squares aims to minimize $$\\sum_{i=1}^n \\left(y_i - \\alpha - \\beta\\left(1 - e^{-\\gamma x_i}\\right)\\right)^2.$$ Although closed-form solutions are generally unavailable for nonlinear models, iterative numerical optimization, such as trust-region methods, can efficiently estimate $(\\alpha, \\beta, \\gamma)$ starting from informed initial guesses. Good initial values can be constructed using simple heuristics: for example, $\\alpha$ can be initialized near the minimum of $y$, $\\beta$ near the range of $y$, and $\\gamma$ near $1/x_{\\max}$ to reflect the scale of saturation.\n\nDiagnostics:\n- The mean squared error (MSE) $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n r_i^2$$ quantifies average squared residual magnitude. Comparing $\\text{MSE}_{\\text{lin}}$ and $\\text{MSE}_{\\text{nl}}$ indicates how much a nonlinear fit reduces error relative to a linear one.\n- The coefficient of determination $$R^2 = 1 - \\frac{\\sum_{i=1}^n r_i^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$ measures the proportion of variance explained by the model. While not used in the decision rule, it provides complementary context.\n- Spearman rank correlation $\\rho_s(r^{\\text{lin}}, x)$ captures monotonic patterns in the linear residuals with respect to $x$. If the true relationship is nonlinear (e.g., saturating), a linear fit often leaves structured residuals that exhibit monotonic trends with $x$. The Spearman coefficient is robust to non-Gaussian distributions and reflects monotonic association based on ranks, making it suitable for detecting order-based structure.\n\nDecision rule:\n- We declare non-linearity if two independently motivated diagnostics both indicate it:\n  1. Sufficient MSE improvement by the nonlinear fit: $$\\frac{\\text{MSE}_{\\text{lin}}}{\\text{MSE}_{\\text{nl}}} \\ge \\tau,$$ with $\\tau = 1.05$ set to require at least a modest improvement.\n  2. Notable monotonic structure in linear residuals: $$\\left|\\rho_s\\left(r^{\\text{lin}}, x\\right)\\right| \\ge t,$$ with $t = 0.25$ chosen to flag residual-ordering that is unlikely to occur under a well-specified linear model.\n\nAlgorithmic steps for each test case:\n1. Generate $n$ equally spaced $x$ on $[0, x_{\\max}]$. Draw $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ using a fixed seed for reproducibility. Form $$y_i = \\alpha + \\beta\\left(1 - e^{-\\gamma x_i}\\right) + \\varepsilon_i.$$\n2. Fit the linear model via least squares to obtain $(a, b)$, compute fitted values $\\hat{y}_i^{\\text{lin}}$ and residuals $r_i^{\\text{lin}} = y_i - \\hat{y}_i^{\\text{lin}}$.\n3. Fit the nonlinear saturating model via nonlinear least squares (trust-region method under bounds), initialize $(\\alpha, \\beta, \\gamma)$ as described, and compute fitted values $\\hat{y}_i^{\\text{nl}}$ and residuals $r_i^{\\text{nl}}$.\n4. Compute $\\text{MSE}_{\\text{lin}}$, $\\text{MSE}_{\\text{nl}}$, $R^2_{\\text{lin}}$, $R^2_{\\text{nl}}$, and $\\rho_s\\left(r^{\\text{lin}}, x\\right)$.\n5. Apply the decision rule with $\\tau = 1.05$ and $t = 0.25$ to produce a boolean classification for non-linearity.\n\nCoverage of the test suite:\n- Case $1$ uses small $\\gamma$ to approximate linear behavior ($e^{-\\gamma x} \\approx 1 - \\gamma x$ when $\\gamma x$ is small), so the nonlinear model should not markedly outperform the linear model, and residual monotonic structure should be weak.\n- Case $2$ uses large $\\gamma$ for rapid saturation, causing the linear fit to leave pronounced structure in residuals and the nonlinear fit to provide substantial error reduction.\n- Case $3$ sets $\\beta = 0$, making $y$ constant plus noise, hence no relationship with $x$ should be detected and neither model should be strongly preferred.\n- Case $4$ introduces high noise with moderate $\\gamma$ so that the signal is obscured; diagnostics should be conservative.\n- Case $5$ introduces moderate saturation with moderate noise on a shorter $x$ interval, yielding detectable non-linearity under the criteria.\n\nThe program aggregates the boolean results for all cases into a single bracketed, comma-separated list as specified, ensuring deterministic outputs via a fixed random seed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import spearmanr\n\ndef saturating_model(x, alpha, beta, gamma):\n    return alpha + beta * (1.0 - np.exp(-gamma * x))\n\ndef fit_linear(x, y):\n    # Design matrix for linear model: [1, x]\n    X = np.column_stack((np.ones_like(x), x))\n    # Solve least squares\n    coef, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    a, b = coef\n    y_hat = a + b * x\n    residuals = y - y_hat\n    return a, b, y_hat, residuals\n\ndef fit_nonlinear(x, y):\n    # Initial guesses: alpha near min(y), beta near (max(y)-min(y)),\n    # gamma near 1/x_max (positive).\n    y_min = np.min(y)\n    y_max = np.max(y)\n    alpha0 = y_min\n    beta0 = max(y_max - y_min, 1e-3)\n    x_max = np.max(x)\n    gamma0 = 1.0 / max(x_max, 1.0)\n    p0 = (alpha0, beta0, gamma0)\n\n    # Bounds: gamma > 0; alpha and beta unbounded.\n    bounds = ((-np.inf, -np.inf, 1e-9), (np.inf, np.inf, np.inf))\n    try:\n        popt, _ = curve_fit(saturating_model, x, y, p0=p0, bounds=bounds, maxfev=10000)\n        alpha, beta, gamma = popt\n        y_hat = saturating_model(x, alpha, beta, gamma)\n        residuals = y - y_hat\n        return alpha, beta, gamma, y_hat, residuals\n    except Exception:\n        # Fallback: if nonlinear fit fails, return linear-like results to avoid crash\n        # This will make diagnostics conservative (likely not detect nonlinearity).\n        y_hat = np.full_like(y, np.mean(y))\n        residuals = y - y_hat\n        return np.mean(y), 0.0, 1.0, y_hat, residuals\n\ndef mse(residuals):\n    return float(np.mean(residuals**2))\n\ndef r2(y, y_hat):\n    y_mean = np.mean(y)\n    sse = np.sum((y - y_hat)**2)\n    sst = np.sum((y - y_mean)**2)\n    if sst == 0.0:\n        return 0.0\n    return float(1.0 - sse / sst)\n\ndef detect_nonlinearity(mse_lin, mse_nl, spearman_r, tau=1.05, t=0.25):\n    improvement_ratio = mse_lin / mse_nl if mse_nl > 0 else np.inf\n    return (improvement_ratio >= tau) and (abs(spearman_r) >= t)\n\ndef simulate_case(alpha, beta, gamma, sigma, x_max, n, rng):\n    x = np.linspace(0.0, x_max, n)\n    eps = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = alpha + beta * (1.0 - np.exp(-gamma * x)) + eps\n    return x, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, beta, gamma, sigma, x_max, n)\n        (0.5, 1.5, 0.02, 0.1, 5.0, 200),  # Case 1: near-linear\n        (0.5, 1.5, 1.0, 0.1, 5.0, 200),   # Case 2: strong saturation\n        (0.5, 0.0, 0.5, 0.1, 5.0, 200),   # Case 3: no relationship\n        (0.5, 1.5, 0.5, 1.5, 5.0, 200),   # Case 4: high noise\n        (0.5, 1.0, 0.5, 0.2, 3.0, 150),   # Case 5: moderate saturation\n    ]\n\n    # Fixed seed for reproducibility\n    rng = np.random.default_rng(12345)\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, sigma, x_max, n = case\n        x, y = simulate_case(alpha, beta, gamma, sigma, x_max, n, rng)\n\n        # Fit linear model\n        a_lin, b_lin, yhat_lin, resid_lin = fit_linear(x, y)\n\n        # Fit nonlinear saturating model\n        a_nl, b_nl, g_nl, yhat_nl, resid_nl = fit_nonlinear(x, y)\n\n        # Compute diagnostics\n        mse_lin = mse(resid_lin)\n        mse_nl = mse(resid_nl)\n        r2_lin = r2(y, yhat_lin)\n        r2_nl = r2(y, yhat_nl)\n        # Spearman rank correlation between linear residuals and x\n        spearman_r, _ = spearmanr(resid_lin, x)\n\n        # Decision rule\n        flag = detect_nonlinearity(mse_lin, mse_nl, spearman_r, tau=1.05, t=0.25)\n        results.append(flag)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While comparing pre-defined models is powerful, what if we want to detect non-linearity without committing to a specific alternative functional form? This practice  guides you through the process of inventing a novel, non-parametric diagnostic based on a simple intuition: a non-linear function's \"local linearity\" will vary across its domain. You will implement a moving-window approach to measure the variability in local goodness-of-fit ($R^2$) and use the bootstrap to construct a hypothesis test, a powerful technique for assessing the significance of custom-built statistics. This exercise fosters deep statistical thinking by moving beyond off-the-shelf tests to creating and validating your own diagnostic tools.",
            "id": "3114947",
            "problem": "You are given a univariate predictor-response dataset $\\{(x_i,y_i)\\}_{i=1}^n$ with an unknown regression function $f(x)$ under the additive noise model $y_i = f(x_i) + \\varepsilon_i$, where $\\varepsilon_i$ are independent and identically distributed with zero mean and finite variance. Your goal is to diagnose non-linearity in $f(x)$ by computing the local coefficient of determination within moving windows over $x$ and evaluating whether the variability of these local coefficients departs from what would be expected under a globally linear relationship.\n\nUsing only fundamental definitions from ordinary least squares and resampling logic, design a program that does the following for each test case:\n\n1) Sort the data by the predictor so that $x_1 \\le x_2 \\le \\cdots \\le x_n$.\n\n2) For a fixed window size $m$ with $3 \\le m < n$, define overlapping contiguous windows of indices $W_j = \\{j, j+1, \\dots, j + m - 1\\}$ for $j \\in \\{1, 2, \\dots, n - m + 1\\}$. For each window $W_j$, fit a local ordinary least squares line $y \\approx a_j + b_j x$ using only $\\{(x_i,y_i): i \\in W_j\\}$, and compute the local coefficient of determination\n$$\nR^2_j \\;=\\; 1 \\;-\\; \\frac{\\sum_{i \\in W_j} (y_i - \\widehat{y}_i)^2}{\\sum_{i \\in W_j} (y_i - \\overline{y}_{W_j})^2},\n$$\nwhere $\\widehat{y}_i = a_j + b_j x_i$ for $i \\in W_j$, and $\\overline{y}_{W_j}$ is the sample mean of $\\{y_i : i \\in W_j\\}$. In the degenerate case where $\\sum_{i \\in W_j} (y_i - \\overline{y}_{W_j})^2 = 0$, set $R^2_j = 1$ if $\\sum_{i \\in W_j} (y_i - \\widehat{y}_i)^2 = 0$ and $R^2_j = 0$ otherwise.\n\n3) Define the variability statistic\n$$\nS_{\\text{obs}} \\;=\\; \\text{sd}\\big( \\{R^2_j\\}_{j=1}^{n-m+1} \\big),\n$$\nthe sample standard deviation of the local coefficients of determination across windows.\n\n4) Construct a null reference for $S_{\\text{obs}}$ under the global linear model $y_i = \\alpha + \\beta x_i + \\varepsilon_i$ by:\n   - Fitting the global ordinary least squares line $y \\approx \\alpha + \\beta x$ to all $n$ data points, obtaining $\\widehat{y}_i$ and residuals $e_i = y_i - \\widehat{y}_i$.\n   - For a positive integer $B$, and for each $b \\in \\{1,2,\\dots,B\\}$, generate a bootstrap dataset $y_i^{(b)} = \\widehat{y}_i + e^{*(b)}_i$ where $\\{e^{*(b)}_i\\}_{i=1}^n$ are sampled with replacement from $\\{e_i\\}_{i=1}^n$. For each bootstrap dataset, compute the corresponding windowed local coefficients of determination and their sample standard deviation $S_b$ in the same way as $S_{\\text{obs}}$.\n   - Compute the one-sided p-value\n$$\np \\;=\\; \\frac{1 + \\sum_{b=1}^B \\mathbb{I}\\{ S_b \\ge S_{\\text{obs}} \\}}{B + 1}.\n$$\n\n5) Decide non-linearity via the rule: declare non-linearity detected if $p < \\alpha$, where $\\alpha$ is a given significance level.\n\nRequirements:\n- Use the ordinary least squares solution for line fitting within each window and globally, grounded in the definition that ordinary least squares minimizes $\\sum_i (y_i - a - b x_i)^2$ over $(a,b)$.\n- Use the definition of the coefficient of determination $R^2$ within each window as specified above.\n- Implement the bootstrap by residual resampling as described.\n\nAngle units are not applicable. There are no physical units. All proportions must be expressed as decimals.\n\nTest Suite:\nYour program must evaluate the following four test cases. For each, generate data internally using a pseudorandom number generator seeded as specified. For all cases use window size $m = 31$, number of bootstrap replicates $B = 199$, and significance level $\\alpha = 0.05$.\n\n- Case $1$ (globally linear with low noise; expect no non-linearity):\n  - Seed $= 12345$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently for $i = 1, \\dots, n$.\n  - Response generation: $y_i = -0.5 + 2.0\\,x_i + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, 0.2^2)$ independently.\n\n- Case $2$ (quadratic curvature; expect non-linearity):\n  - Seed $= 67890$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently.\n  - Response generation: $y_i = 1.0\\,x_i^2 + 0.0\\,x_i + 0.0 + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, 0.3^2)$ independently.\n\n- Case $3$ (piecewise linear with a kink; expect non-linearity):\n  - Seed $= 4242$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently.\n  - Response generation: $y_i = \\begin{cases} -1.0\\,x_i + 0.0 + \\varepsilon_i, & x_i < 0, \\\\ 2.5\\,x_i + 0.0 + \\varepsilon_i, & x_i \\ge 0, \\end{cases}$ with $\\varepsilon_i \\sim \\mathcal{N}(0, 0.25^2)$ independently.\n\n- Case $4$ (globally linear with high noise; expect no non-linearity):\n  - Seed $= 999$.\n  - Sample size $n = 220$.\n  - Predictor generation: $x_i \\sim \\text{Uniform}([-2.5, 2.5])$ independently.\n  - Response generation: $y_i = 0.0 + 1.5\\,x_i + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, 2.0^2)$ independently.\n\nOutput specification:\n- For each case, output a boolean indicating whether non-linearity is detected by the above testing rule (true if $p < \\alpha$, false otherwise).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the cases $1$ through $4$ (for example, `[True,False,True,False]`).",
            "solution": "The problem asks for the design and implementation of a statistical procedure to detect non-linearity in a predictor-response relationship, $y_i = f(x_i) + \\varepsilon_i$. The method is based on analyzing the variability of the coefficient of determination, $R^2$, computed over moving windows of the data. The decision for or against non-linearity is made via a formal hypothesis test, with a null distribution generated using a bootstrap resampling technique.\n\nThe procedure is validated as scientifically sound, well-posed, and fully specified. All parameters, data generation models, and computational formulas are provided unambiguously.\n\n### Principle-Based Design\n\nThe fundamental principle of this test is that a globally linear function should appear locally linear everywhere. Conversely, a globally non-linear function will exhibit varying degrees of linearity across its domain.\n\n$1$. **The Test Statistic**: The core idea is to slide a window of fixed size $m$\nacross the data (which has been sorted by the predictor, $x$) and perform a simple linear regression within each window. For each window $W_j = \\{j, j+1, \\dots, j + m - 1\\}$, we compute the local coefficient of determination, $R^2_j$.\nIf the true function $f(x)$ is linear, $f(x) = \\alpha + \\beta x$, then the model in each window is also linear. The local $R^2_j$ values should be relatively high and stable, with fluctuations primarily due to the random noise $\\varepsilon_i$. However, if $f(x)$ is non-linear (e.g., quadratic), the quality of a linear fit will change depending on the window's position. In regions of high curvature, a straight line is a poor approximation, leading to a low $R^2_j$. In flatter regions, the fit will be better, yielding a higher $R^2_j$. This induces significant variability in the sequence of $\\{R^2_j\\}$ values.\nThe test statistic $S_{\\text{obs}} = \\text{sd}\\big( \\{R^2_j\\}_{j=1}^{n-m+1} \\big)$, the sample standard deviation of these local coefficients of determination, is designed to capture precisely this variability. A large value of $S_{\\text{obs}}$ suggests non-linearity.\n\n$2$. **Hypothesis Testing and the Bootstrap**: To objectively decide if $S_{\\text{obs}}$ is \"large enough\" to reject linearity, we frame the problem as a hypothesis test. The null hypothesis, $H_0$, is that the relationship is globally linear: $y_i = \\alpha + \\beta x_i + \\varepsilon_i$. We need to determine the distribution of the statistic $S$ under this null hypothesis.\nThe bootstrap provides a computational method for approximating this null distribution. The specified residual bootstrap procedure works as follows:\n- First, we fit the global linear model to the entire dataset $\\{(x_i,y_i)\\}_{i=1}^n$ using ordinary least squares (OLS), obtaining the estimated coefficients $\\widehat{\\alpha}$ and $\\widehat{\\beta}$, a set of fitted values $\\widehat{y}_i = \\widehat{\\alpha} + \\widehat{\\beta} x_i$, and the corresponding residuals $e_i = y_i - \\widehat{y}_i$. These residuals represent our best estimate of the underlying noise process.\n- We then generate $B$ bootstrap datasets. Each synthetic dataset $\\{(x_i, y_i^{(b)})\\}$ is created by holding the predictors $x_i$ and the null-model fit $\\widehat{y}_i$ fixed, and adding a new set of noise terms: $y_i^{(b)} = \\widehat{y}_i + e^{*(b)}_i$. The bootstrap residuals $\\{e^{*(b)}_i\\}_{i=1}^n$ are obtained by sampling with replacement from the original set of residuals $\\{e_i\\}_{i=1}^n$.\n- By construction, each bootstrap dataset conforms to the null hypothesis of linearity, while preserving the noise characteristics of the original data.\n- For each of the $B$ bootstrap datasets, we compute the statistic $S_b$ in the exact same manner as $S_{\\text{obs}}$. The collection $\\{S_b\\}_{b=1}^B$ forms an empirical reference distribution for our test statistic under $H_0$.\n\n$3$. **P-value and Decision**: The one-sided p-value is the probability of observing a test statistic as large or larger than $S_{\\text{obs}}$, assuming $H_0$ is true. This is estimated from the bootstrap samples using the formula:\n$$\np \\;=\\; \\frac{1 + \\sum_{b=1}^B \\mathbb{I}\\{ S_b \\ge S_{\\text{obs}} \\}}{B + 1}\n$$\nThe inclusion of $1$ in the numerator and denominator prevents a p-value of $0$ and provides a more stable estimate. If this p-value is smaller than a pre-defined significance level $\\alpha$ (e.g., $0.05$), we reject the null hypothesis and conclude that there is statistically significant evidence of non-linearity.\n\n### Algorithmic Implementation Steps\n\nThe implementation follows the described logic for each test case.\n\n$1$. **Data Generation and Sorting**: Data $\\{(x_i,y_i)\\}_{i=1}^n$ are generated according to the specified model and random seed. The data pairs are then sorted based on the predictor values $x_i$ to enable the use of contiguous moving windows.\n\n$2$. **Calculation of $S_{\\text{obs}}$**: A loop iterates from $j = 1$ to $n - m + 1$. In each iteration, the data for the window $W_j$ are extracted. An OLS line $y \\approx a_j + b_j x$ is fitted to this window's data. The OLS solution for the slope $b_j$ and intercept $a_j$ minimizes the sum of squared errors and is given by known formulas:\n$$\nb_j = \\frac{\\sum_{i \\in W_j} (x_i - \\overline{x}_{W_j})(y_i - \\overline{y}_{W_j})}{\\sum_{i \\in W_j} (x_i - \\overline{x}_{W_j})^2}, \\quad a_j = \\overline{y}_{W_j} - b_j \\overline{x}_{W_j}\n$$\nwhere $\\overline{x}_{W_j}$ and $\\overline{y}_{W_j}$ are the sample means in window $W_j$. From the fitted values $\\widehat{y}_i = a_j + b_j x_i$, the local coefficient of determination is calculated as:\n$$\nR^2_j \\;=\\; 1 \\;-\\; \\frac{\\text{SSE}_j}{\\text{SST}_j} \\;=\\; 1 \\;-\\; \\frac{\\sum_{i \\in W_j} (y_i - \\widehat{y}_i)^2}{\\sum_{i \\in W_j} (y_i - \\overline{y}_{W_j})^2}\n$$\nAfter collecting all $R^2_j$ values, their sample standard deviation is computed to yield $S_{\\text{obs}}$.\n\n$3$. **Bootstrap Simulation**: The global OLS fit is performed on the full sorted dataset to get residuals. Then, a loop runs $B$ times. In each iteration, a bootstrap dataset is created, the full windowing procedure is re-run on this new dataset to calculate a bootstrap statistic $S_b$.\n\n$4$. **Conclusion**: The computed p-value is compared against the significance level $\\alpha = 0.05$. The function returns `True` (non-linearity detected) if $p < \\alpha$, and `False` otherwise. This process is repeated for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_S(x_sorted, y, m):\n    \"\"\"\n    Computes the variability statistic S for a given dataset (x, y) and window size m.\n    Assumes x_sorted is already sorted, and y corresponds to x_sorted.\n    \n    Args:\n        x_sorted (np.ndarray): Sorted predictor values.\n        y (np.ndarray): Corresponding response values.\n        m (int): Window size.\n        \n    Returns:\n        float: The sample standard deviation of local R^2 values.\n    \"\"\"\n    n = len(x_sorted)\n    num_windows = n - m + 1\n    r2_values = np.zeros(num_windows)\n\n    for j in range(num_windows):\n        x_w = x_sorted[j:j+m]\n        y_w = y[j:j+m]\n        \n        y_mean_w = np.mean(y_w)\n        sst_w = np.sum((y_w - y_mean_w)**2)\n\n        # Handle the degenerate case where all y-values in the window are the same.\n        # Problem statement: \"In the degenerate case where sum(y_i - y_bar)^2 = 0, \n        # set R^2_j = 1 if sum(y_i - y_hat_i)^2 = 0 and R^2_j = 0 otherwise.\"\n        # For an OLS fit, if SST=0, then SSE must also be 0 because the best fit line\n        # is the horizontal line at the mean, which perfectly fits the constant data.\n        if sst_w == 0:\n            r2_values[j] = 1.0\n            continue # Move to the next window\n            \n        x_mean_w = np.mean(x_w)\n        ss_xx_w = np.sum((x_w - x_mean_w)**2)\n        \n        # Handle the degenerate case where all x-values in the window are the same.\n        if ss_xx_w == 0:\n            # The best linear fit is a horizontal line at the mean of y.\n            y_hat_w = np.full_like(y_w, y_mean_w)\n        else:\n            ss_xy_w = np.sum((x_w - x_mean_w) * (y_w - y_mean_w))\n            b_w = ss_xy_w / ss_xx_w\n            a_w = y_mean_w - b_w * x_mean_w\n            y_hat_w = a_w + b_w * x_w\n        \n        sse_w = np.sum((y_w - y_hat_w)**2)\n        \n        r2_values[j] = 1.0 - sse_w / sst_w\n            \n    # Per the problem, compute the sample standard deviation. This requires ddof=1.\n    return np.std(r2_values, ddof=1)\n\ndef _test_nonlinearity(x, y, m, B, alpha):\n    \"\"\"\n    Performs the full non-linearity test procedure as described in the problem.\n    \n    Args:\n        x (np.ndarray): Predictor values.\n        y (np.ndarray): Response values.\n        m (int): Window size.\n        B (int): Number of bootstrap replicates.\n        alpha (float): Significance level.\n        \n    Returns:\n        bool: True if non-linearity is detected, False otherwise.\n    \"\"\"\n    n = len(x)\n\n    # 1. Sort the data by the predictor\n    sort_indices = np.argsort(x)\n    x_sorted = x[sort_indices]\n    y_sorted = y[sort_indices]\n\n    # 2.  3. Compute the observed variability statistic S_obs\n    S_obs = _calculate_S(x_sorted, y_sorted, m)\n\n    # 4. Construct a null reference distribution using a bootstrap procedure\n    # Fit the global ordinary least squares line\n    x_mean_global = np.mean(x_sorted)\n    y_mean_global = np.mean(y_sorted)\n    ss_xx_global = np.sum((x_sorted - x_mean_global)**2)\n    ss_xy_global = np.sum((x_sorted - x_mean_global) * (y_sorted - y_mean_global))\n    \n    if ss_xx_global == 0:\n        b_global = 0.0\n    else:\n        b_global = ss_xy_global / ss_xx_global\n    \n    a_global = y_mean_global - b_global * x_sorted\n    \n    y_hat_global = a_global + b_global * x_sorted\n    residuals = y_sorted - y_hat_global\n\n    # Bootstrap loop\n    S_boot = np.zeros(B)\n    for b_idx in range(B):\n        # Generate a bootstrap dataset\n        boot_residuals = np.random.choice(residuals, size=n, replace=True)\n        y_boot = y_hat_global + boot_residuals\n        # Compute the statistic for the bootstrap sample\n        S_boot[b_idx] = _calculate_S(x_sorted, y_boot, m)\n\n    # Compute the one-sided p-value\n    numerator = 1.0 + np.sum(S_boot >= S_obs)\n    denominator = float(B + 1)\n    p_value = numerator / denominator\n\n    # 5. Decide non-linearity based on the significance level\n    return p_value  alpha\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        {'seed': 12345, 'n': 220, 'case_type': 'linear_low_noise'},\n        {'seed': 67890, 'n': 220, 'case_type': 'quadratic'},\n        {'seed': 4242, 'n': 220, 'case_type': 'piecewise'},\n        {'seed': 999, 'n': 220, 'case_type': 'linear_high_noise'}\n    ]\n    \n    m = 31\n    B = 199\n    alpha = 0.05\n    \n    results = []\n\n    for case in test_cases:\n        np.random.seed(case['seed'])\n        n = case['n']\n        \n        # Generate predictor variable\n        x = np.random.uniform(-2.5, 2.5, n)\n        \n        # Generate response variable based on the case type\n        if case['case_type'] == 'linear_low_noise':\n            eps = np.random.normal(loc=0.0, scale=0.2, size=n)\n            y = -0.5 + 2.0 * x + eps\n        elif case['case_type'] == 'quadratic':\n            eps = np.random.normal(loc=0.0, scale=0.3, size=n)\n            y = 1.0 * x**2 + eps\n        elif case['case_type'] == 'piecewise':\n            eps = np.random.normal(loc=0.0, scale=0.25, size=n)\n            y = np.where(x  0, -1.0 * x, 2.5 * x) + eps\n        elif case['case_type'] == 'linear_high_noise':\n            eps = np.random.normal(loc=0.0, scale=2.0, size=n)\n            y = 1.5 * x + eps\n\n        # Perform the test and store the boolean result\n        is_nonlinear = _test_nonlinearity(x, y, m, B, alpha)\n        results.append(is_nonlinear)\n\n    # Print final result in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once non-linearity is suspected, a crucial question arises: which modeling approach is best suited to capture it? This capstone exercise  simulates a controlled experiment to compare the power of three widely used methods—polynomial regression, splines, and kernel methods—in detecting a non-linear signal amidst noise. By systematically varying the signal strength, you will determine the \"detection threshold\" for each technique, revealing the trade-offs between model flexibility, signal-to-noise ratio, and statistical power. This practice provides practical insight into the relative strengths of different non-linear modeling strategies and solidifies advanced concepts like cross-validation and statistical power analysis in a tangible, comparative framework.",
            "id": "3114979",
            "problem": "You are asked to implement a complete, runnable program that empirically diagnoses non-linearity in data by constructing a nested curriculum of diagnostics: start with a residual-based augmentation, then move to splines, then to kernel methods. Your program will compare the empirical detection thresholds of each diagnostic method under varying noise levels, using a controlled simulation design.\n\nThe foundational base for this problem comprises the following well-tested definitions and procedures:\n- Ordinary Least Squares (OLS): Given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, the OLS estimator $\\hat{\\beta}$ minimizes the squared error $\\|y - X \\beta\\|_2^2$.\n- Residuals: For fitted values $\\hat{y} = X \\hat{\\beta}$, residuals are $r = y - \\hat{y}$.\n- $K$-fold cross-validation: Split indices $\\{1,\\ldots,n\\}$ into $K$ disjoint folds of approximately equal size; for each fold $k$, fit a model on $\\{1,\\ldots,n\\} \\setminus \\text{fold}_k$ and evaluate the mean squared error (MSE) on $\\text{fold}_k$; aggregate across folds.\n- Student's $t$ distribution: For paired fold-wise error differences with $K$ folds, a $t$-statistic $t = \\bar{d}/(s_d/\\sqrt{K})$ has, under standard assumptions, approximately a Student's $t$ distribution with $K-1$ degrees of freedom, where $\\bar{d}$ is the sample mean and $s_d$ is the sample standard deviation of the differences.\n\nYour task is to implement the following simulation-based detection pipeline and report detection thresholds.\n\nData generating process:\n- Draw inputs $x_i \\sim \\text{Uniform}(0,1)$ independently for $i = 1,\\ldots,n$.\n- Define a linear baseline $y_i^{\\text{lin}} = \\beta_0 + \\beta_1 x_i$ and a non-linear component $g(x_i) = \\sin(2\\pi x_i)$.\n- For a given non-linearity amplitude $a \\ge 0$ and noise standard deviation $\\sigma > 0$, generate\n$$\ny_i = y_i^{\\text{lin}} + a \\, g(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2) \\text{ independently}.\n$$\n\nDiagnostics to compare (in increasing flexibility; each compared against the same linear baseline):\n1. Residual-based augmentation (polynomial add-on): augment the linear model with $x^2$ (interpretable as detecting curvature visible in a residual plot). Concretely, compare baseline $[1, x]$ versus augmented $[1, x, x^2]$.\n2. Splines: fit a cubic regression spline using a truncated power basis with internal knots at fixed locations; design matrix $[1, x, (x - t_1)_+^3, \\ldots, (x - t_K)_+^3]$, where $(u)_+ = \\max(u,0)$.\n3. Kernel method via Random Fourier Features (RFF) approximation of the Gaussian kernel: map $x$ to a feature vector $\\phi(x) \\in \\mathbb{R}^{m}$ with entries $\\sqrt{2/m} \\cos(\\omega_j x + b_j)$ where $\\omega_j \\sim \\mathcal{N}(0, \\ell^{-2})$ and $b_j \\sim \\text{Uniform}(0,2\\pi)$, then fit linear ridge regression on the features (include an intercept).\n\nDetection logic (uniform across all diagnostics):\n- Use $K$-fold cross-validation to compute fold-wise mean squared errors for the baseline model, $\\{\\text{MSE}_{\\text{base},k}\\}_{k=1}^K$, and for a diagnostic model, $\\{\\text{MSE}_{\\text{diag},k}\\}_{k=1}^K$.\n- Compute paired differences $d_k = \\text{MSE}_{\\text{base},k} - \\text{MSE}_{\\text{diag},k}$ for $k=1,\\ldots,K$.\n- Compute the one-sided $t$ statistic\n$$\nt = \\frac{\\bar{d}}{s_d/\\sqrt{K}}, \\quad \\text{with} \\quad \\bar{d} = \\frac{1}{K}\\sum_{k=1}^K d_k,\\quad s_d^2 = \\frac{1}{K-1}\\sum_{k=1}^K (d_k - \\bar{d})^2.\n$$\n- Let $p$ be the one-sided $p$-value under a Student's $t$ distribution with $K-1$ degrees of freedom, for the alternative $\\bar{d} > 0$. Declare non-linearity detected if $\\bar{d} > 0$ and $p  \\alpha$.\n\nEmpirical detection probability and threshold:\n- For fixed $(a,\\sigma)$, repeat the data generation and detection decision independently $R$ times; estimate the detection probability as the fraction of replications that declared detection.\n- For a grid of amplitudes $\\mathcal{A} = \\{a_1  a_2  \\cdots  a_G\\}$, define the empirical detection threshold as the smallest $a_g \\in \\mathcal{A}$ with estimated detection probability at least a target power $\\pi^\\star$. If no $a_g$ reaches the target power, report a sentinel value.\n\nTest suite and fixed parameters:\n- Use $n = 120$, $\\beta_0 = 0$, $\\beta_1 = 1$, $K = 5$, $\\alpha = 0.05$, $R = 40$, and target power $\\pi^\\star = 0.8$.\n- Use spline internal knots at $t_j \\in \\{0.2, 0.4, 0.6, 0.8\\}$.\n- For Random Fourier Features use feature dimension $m = 40$, Gaussian kernel length scale $\\ell = 0.2$, and ridge penalty $\\lambda = 10^{-3}$.\n- Amplitude grid $\\mathcal{A} = \\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\}$.\n- Noise levels to test: $\\sigma \\in \\{0.1, 0.5, 1.0\\}$.\n\nOutput specification:\n- For each noise level $\\sigma$ in the given order, compute the empirical detection threshold for the residual-based augmentation, the spline, and the kernel method, respectively. If the target power is not reached for any amplitude in $\\mathcal{A}$, output the sentinel value $-1.0$ for that diagnostic under that noise.\n- Your program should produce a single line of output containing the nine thresholds as a comma-separated list enclosed in square brackets, in the order\n`[θ_poly(σ=0.1), θ_spline(σ=0.1), θ_kernel(σ=0.1), θ_poly(σ=0.5), θ_spline(σ=0.5), θ_kernel(σ=0.5), θ_poly(σ=1.0), θ_spline(σ=1.0), θ_kernel(σ=1.0)]`.\n- Express each threshold as a decimal rounded to two digits after the decimal point.\n\nAngle units are not applicable. No physical units are involved. Percentages must not be used; all probabilities should be treated as real numbers in $[0,1]$.\n\nYour program must be self-contained, must not require input, and must follow these exact specifications. Use a fixed random seed internally to ensure reproducibility of results.",
            "solution": "The problem posed is to conduct a simulation-based study to empirically evaluate and compare the detection thresholds of three distinct statistical methods for diagnosing non-linearity in a regression context. The problem is well-defined, scientifically grounded in established statistical learning principles, and provides a complete set of parameters and procedures for a reproducible computational experiment. Therefore, the problem is deemed valid. We proceed with a detailed description of the solution methodology.\n\nThe core of the task is to determine, for different noise levels, the minimum signal strength of a non-linear component required for each diagnostic method to reliably detect its presence.\n\n**1. Data Generating Process (DGP)**\n\nThe simulation is predicated on a controlled data generating process. For each of the $R$ replications, we generate a dataset of size $n = 120$.\nFirst, the predictor variable $x_i$ is drawn independently from a uniform distribution:\n$$x_i \\sim \\text{Uniform}(0, 1) \\quad \\text{for } i = 1, \\dots, n$$\nThe response variable $y_i$ is constructed as a sum of a linear component, a non-linear component, and a random noise term. The linear baseline is given by $y_i^{\\text{lin}} = \\beta_0 + \\beta_1 x_i$, with specified coefficients $\\beta_0 = 0$ and $\\beta_1 = 1$. The non-linear signal is a sinusoidal function $g(x_i) = \\sin(2\\pi x_i)$. The overall model for the response is:\n$$y_i = (\\beta_0 + \\beta_1 x_i) + a \\cdot g(x_i) + \\varepsilon_i$$\nwhere $a \\ge 0$ is the amplitude of the non-linear component, and $\\varepsilon_i$ are independent and identically distributed noise terms drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. This framework allows us to systematically vary the non-linearity strength $a$ and the noise level $\\sigma$.\n\n**2. Diagnostic Methods and Model Comparison**\n\nWe compare three diagnostic models of increasing flexibility against a common baseline model. The baseline model is always the simple linear regression model, which attempts to fit the data using only an intercept and a linear term for $x$. Its design matrix is $X_{\\text{base}} = [1, x]$.\n\nThe three diagnostic methods are:\n1.  **Residual-based Augmentation (Polynomial)**: This method augments the linear model with a quadratic term, $x^2$. The design matrix for this augmented model is $X_{\\text{poly}} = [1, x, x^2]$. This model is effective at capturing simple, symmetric curvature that might be apparent in a plot of residuals from the linear model against the predictor $x$.\n\n2.  **Cubic Splines**: This method offers greater flexibility by using a piecewise polynomial function. We employ a cubic regression spline with a truncated power basis. The design matrix is constructed as:\n    $$X_{\\text{spline}} = [1, x, (x - t_1)_+^3, \\dots, (x - t_4)_+^3]$$\n    where $(u)_+ = \\max(u, 0)$ and the internal knots are fixed at $t_j \\in \\{0.2, 0.4, 0.6, 0.8\\}$. The spline model can adapt to more complex, localized non-linear patterns than a simple global polynomial.\n\n3.  **Kernel Method (via Random Fourier Features)**: This is the most flexible method, designed to approximate a model based on a Gaussian kernel $k(x, x') = \\exp(-\\|x-x'\\|^2 / (2\\ell^2))$. Direct kernel methods can be computationally intensive. Here, we use Random Fourier Features (RFF) as a computationally efficient approximation. The input $x$ is mapped into a higher-dimensional feature space of dimension $m=40$ using a feature map $\\phi(x) \\in \\mathbb{R}^{m}$. The $j$-th component of $\\phi(x)$ is:\n    $$\\phi_j(x) = \\sqrt{\\frac{2}{m}} \\cos(\\omega_j x + b_j)$$\n    where the frequencies $\\omega_j$ are drawn from $\\mathcal{N}(0, \\ell^{-2})$ and the phase shifts $b_j$ are drawn from $\\text{Uniform}(0, 2\\pi)$. The kernel length scale is fixed at $\\ell=0.2$. A linear ridge regression model is then fitted to these features, with an added intercept term. The ridge penalty $\\lambda = 10^{-3}$ is used to regularize the model and prevent overfitting in the high-dimensional feature space.\n\n**3. Statistical Detection Logic**\n\nFor each diagnostic method, its performance is compared to the baseline linear model using $K$-fold cross-validation, with $K = 5$. The dataset is split into $K$ disjoint folds. For each fold $k \\in \\{1, \\dots, K\\}$, the baseline model and the diagnostic model are trained on the other $K-1$ folds. Their respective Mean Squared Errors (MSE), denoted $\\text{MSE}_{\\text{base},k}$ and $\\text{MSE}_{\\text{diag},k}$, are then calculated on the held-out data of fold $k$.\n\nThis procedure yields $K$ paired error measurements. We compute the paired differences $d_k = \\text{MSE}_{\\text{base},k} - \\text{MSE}_{\\text{diag},k}$. A positive value for $d_k$ indicates that the diagnostic model had lower prediction error on fold $k$, suggesting it provided a better fit.\n\nTo statistically assess whether the diagnostic model is significantly better, we perform a one-sided paired $t$-test. The null hypothesis is that the diagnostic model is not better than the baseline ($\\mathbb{E}[d] \\le 0$), while the alternative hypothesis is that it is better ($\\mathbb{E}[d] > 0$). The $t$-statistic is calculated as:\n$$t = \\frac{\\bar{d}}{s_d / \\sqrt{K}}$$\nwhere $\\bar{d} = \\frac{1}{K}\\sum_{k=1}^K d_k$ is the sample mean of the differences and $s_d^2 = \\frac{1}{K-1}\\sum_{k=1}^K (d_k - \\bar{d})^2$ is the sample variance.\n\nUnder the null hypothesis, this statistic approximately follows a Student's $t$ distribution with $K-1=4$ degrees of freedom. We calculate the one-sided $p$-value. Non-linearity is declared \"detected\" if two conditions are met: the diagnostic model has a lower average cross-validation error ($\\bar{d} > 0$) and the result is statistically significant ($p  \\alpha$, with significance level $\\alpha = 0.05$).\n\n**4. Empirical Detection Threshold**\n\nThe overall simulation proceeds by testing a grid of non-linearity amplitudes $\\mathcal{A} = \\{0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\}$ for each specified noise level $\\sigma \\in \\{0.1, 0.5, 1.0\\}$.\n\nFor each pair $(a, \\sigma)$ and for each diagnostic method, we repeat the entire process of data generation and detection $R = 40$ times. The detection probability is estimated as the fraction of these $R$ replications in which non-linearity was detected.\n\nThe empirical detection threshold, $\\theta$, for a given diagnostic method and noise level, is defined as the smallest amplitude $a_g \\in \\mathcal{A}$ for which the estimated detection probability meets or exceeds a target power of $\\pi^\\star = 0.8$. If no amplitude in the grid achieves this target power, the threshold is reported as the sentinel value $-1.0$.\n\nThe final output consists of the nine computed thresholds, rounded to two decimal places, for each combination of diagnostic method and noise level, ordered as specified in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as t_dist\n\ndef solve():\n    \"\"\"\n    Implements the full simulation pipeline to find detection thresholds for \n    non-linearity diagnostics.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    N_SAMPLES = 120\n    BETA_0 = 0.0\n    BETA_1 = 1.0\n    N_FOLDS = 5\n    ALPHA = 0.05\n    N_REPLICATIONS = 40\n    TARGET_POWER = 0.8\n    SPLINE_KNOTS = np.array([0.2, 0.4, 0.6, 0.8])\n    RFF_M = 40\n    RFF_L = 0.2\n    RIDGE_LAMBDA = 1e-3\n    AMPLITUDE_GRID = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n    SIGMA_LEVELS = [0.1, 0.5, 1.0]\n    SENTINEL_VALUE = -1.0\n    \n    # Set a fixed random seed for reproducibility\n    np.random.seed(42)\n\n    def generate_data(a, sigma):\n        \"\"\"Generates data from the specified model.\"\"\"\n        x = np.random.uniform(0, 1, N_SAMPLES)\n        y_lin = BETA_0 + BETA_1 * x\n        g_x = np.sin(2 * np.pi * x)\n        epsilon = np.random.normal(0, sigma, N_SAMPLES)\n        y = y_lin + a * g_x + epsilon\n        return x, y\n\n    def solve_ols(X, y):\n        \"\"\"Solves Ordinary Least Squares.\"\"\"\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        return beta\n\n    def solve_ridge(X, y, lam):\n        \"\"\"Solves Ridge Regression with a penalty on all but the intercept.\"\"\"\n        p = X.shape[1]\n        penalty_matrix = lam * np.eye(p)\n        penalty_matrix[0, 0] = 0.0  # Do not penalize the intercept\n        A = X.T @ X + penalty_matrix\n        b = X.T @ y\n        beta = np.linalg.solve(A, b)\n        return beta\n\n    def make_design_matrix(x, method, knots=None, omega=None, b=None):\n        \"\"\"Creates the design matrix for a given method.\"\"\"\n        x_reshaped = x.reshape(-1, 1)\n        if method == 'base':\n            return np.c_[np.ones(len(x)), x]\n        elif method == 'poly':\n            return np.c_[np.ones(len(x)), x, x**2]\n        elif method == 'spline':\n            base_matrix = np.c_[np.ones(len(x)), x]\n            # Truncated power basis: (x - t)_+^3\n            spline_features = np.maximum(0, x_reshaped - knots)**3\n            return np.c_[base_matrix, spline_features]\n        elif method == 'kernel':\n            # RFF features: sqrt(2/m) * cos(omega*x + b)\n            rff_features = np.sqrt(2.0 / RFF_M) * np.cos(x_reshaped * omega + b)\n            return np.c_[np.ones(len(x)), rff_features]\n        else:\n            raise ValueError(f\"Unknown method '{method}'\")\n\n    def run_cv_comparison(x, y, diag_method_name, knots, rff_params):\n        \"\"\"\n        Performs K-fold CV and returns paired MSE differences.\n        \"\"\"\n        indices = np.arange(N_SAMPLES)\n        np.random.shuffle(indices)\n        fold_indices = np.array_split(indices, N_FOLDS)\n        \n        mse_diffs = []\n        for k in range(N_FOLDS):\n            val_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[j] for j in range(N_FOLDS) if j != k])\n            \n            x_train, y_train = x[train_idx], y[train_idx]\n            x_val, y_val = x[val_idx], y[val_idx]\n            \n            # Base model\n            X_base_train = make_design_matrix(x_train, 'base')\n            beta_base = solve_ols(X_base_train, y_train)\n            X_base_val = make_design_matrix(x_val, 'base')\n            y_pred_base = X_base_val @ beta_base\n            mse_base = np.mean((y_val - y_pred_base)**2)\n            \n            # Diagnostic model\n            if diag_method_name == 'kernel':\n                omega, b = rff_params\n                X_diag_train = make_design_matrix(x_train, 'kernel', omega=omega, b=b)\n                beta_diag = solve_ridge(X_diag_train, y_train, RIDGE_LAMBDA)\n                X_diag_val = make_design_matrix(x_val, 'kernel', omega=omega, b=b)\n            else: # poly or spline\n                X_diag_train = make_design_matrix(x_train, diag_method_name, knots=knots)\n                beta_diag = solve_ols(X_diag_train, y_train)\n                X_diag_val = make_design_matrix(x_val, diag_method_name, knots=knots)\n            \n            y_pred_diag = X_diag_val @ beta_diag\n            mse_diag = np.mean((y_val - y_pred_diag)**2)\n            \n            mse_diffs.append(mse_base - mse_diag)\n            \n        return np.array(mse_diffs)\n\n    def test_detection(mse_diffs):\n        \"\"\"Performs a one-sided paired t-test.\"\"\"\n        d_bar = np.mean(mse_diffs)\n        if d_bar = 0:\n            return False\n        \n        s_d = np.std(mse_diffs, ddof=1)\n        if s_d == 0:\n            # If all differences are identical and positive, it's a perfect improvement.\n            # This can happen in noise-free or low-noise scenarios. Treat as significant.\n            return True if d_bar > 0 else False\n        \n        t_stat = d_bar / (s_d / np.sqrt(N_FOLDS))\n        p_val = t_dist.sf(t_stat, df=N_FOLDS - 1)\n        \n        return p_val  ALPHA\n\n    all_thresholds = []\n    \n    diagnostic_methods = ['poly', 'spline', 'kernel']\n    \n    for sigma in SIGMA_LEVELS:\n        for method_name in diagnostic_methods:\n            detection_probs = {}\n            for a in AMPLITUDE_GRID:\n                detection_count = 0\n                for _ in range(N_REPLICATIONS):\n                    x_data, y_data = generate_data(a, sigma)\n                    \n                    # RFF parameters must be fixed for a given dataset (i.e., per replication)\n                    rff_p = None\n                    if method_name == 'kernel':\n                        omega = np.random.normal(0, 1.0 / RFF_L, size=RFF_M)\n                        b = np.random.uniform(0, 2 * np.pi, size=RFF_M)\n                        rff_p = (omega, b)\n\n                    mse_differences = run_cv_comparison(x_data, y_data, method_name, SPLINE_KNOTS, rff_p)\n                    \n                    if test_detection(mse_differences):\n                        detection_count += 1\n                \n                detection_probs[a] = detection_count / N_REPLICATIONS\n            \n            # Find the detection threshold\n            threshold = SENTINEL_VALUE\n            for a_val in AMPLITUDE_GRID:\n                if detection_probs[a_val] >= TARGET_POWER:\n                    threshold = a_val\n                    break\n            \n            all_thresholds.append(threshold)\n            \n    # Format the final output\n    formatted_results = [f\"{t:.2f}\" for t in all_thresholds]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}