{
    "hands_on_practices": [
        {
            "introduction": "杠杆值并非总是能通过观察单个预测变量的边缘分布直观地看出来。本练习旨在揭示一个更微妙的情形：即使单个预测变量的取值并不极端，它们的交互作用也可能创造出高杠杆点。通过构建和比较包含与不包含交互项的两种模型，你将学会如何在完整的模型设定下评估杠杆率，从而更深刻地理解杠杆值的本质。",
            "id": "3154847",
            "problem": "考虑一个线性回归情景，其中响应被建模为特征的线性组合，可能包含交互项。普通最小二乘法 (OLS) 估计量通过最小化残差平方和，得到一个对响应而言是线性的拟合值算子。您的任务是实现一个完整的程序，该程序构建包含和不包含刻意构造的极端交互模式的数据集，然后通过包含交互项的增广设计矩阵来检测高杠杆点。\n\n推导和算法需基于以下基本定义和事实：\n- 在一个设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$ 的线性模型中，OLS 拟合值是通过将一个线性算子应用于响应向量得到的。该拟合值算子可以写成一个矩阵（投影矩阵或“帽子”矩阵）的形式，它将任何响应向量映射到其在 $X$ 的列空间中的预测值。\n- 帽子矩阵的对角线元素量化了杠杆值。观测值 $i$ 的杠杆值是帽子矩阵的第 $i$ 个对角线元素，它衡量了该观测值的预测变量向量在由 $X$ 导出的几何空间中与预测变量云主体的距离。\n- 帽子矩阵的迹等于设计矩阵的列数 $p$，这意味着平均杠杆值为 $p/n$。\n\n除了这些核心事实，您不得假定任何快捷公式。您应从这些原则中推导出所需的任何工作表达式和算法。\n\n程序要求：\n1. 构建三个合成数据集，每个数据集的响应都由一个带交互项的模型生成。对于每个数据集，拟合两个模型：一个不含交互项的基础模型和一个带交互项的增广模型。对于这两个模型，计算所有观测值的杠杆值，并使用准则 $h_{ii}  2 \\cdot \\bar{h}$ 标记高杠杆点，其中 $\\bar{h}$ 是相应模型下的平均杠杆值。\n2. 响应应由以下模型生成\n   $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 x_2) + \\varepsilon, $$\n   参数固定为 $ \\beta_0 = 0.5$, $ \\beta_1 = 1.0$, $ \\beta_2 = -0.7$, $ \\beta_3 = 0.8$，且独立噪声 $ \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，其中 $ \\sigma = 0.3$。每个数据集中的所有随机生成过程必须通过固定的种子来确定，以确保结果是可复现的。\n\n3. 数据集和测试套件。完全按照规定构建以下三个数据集：\n   - 测试用例 1（理想情况，相对于低方差主体而言，具有中等边际值的极端交互）：\n     - 样本量 $n = 60$。\n     - 随机种子 $s = 12345$。\n     - 独立生成 $x_1$ 和 $x_2$，其中 $x_j \\sim \\mathcal{N}(0, 0.6^2)$，对于 $j \\in \\{1,2\\}$。\n     - 强制将索引为 $i^\\star = 12$ 的单个观测值的 $x_1$ 设为 $1.8$，$x_2$ 设为 $2.2$。这产生了一个交互作用 $x_1 x_2 \\approx 3.96$，相对于低方差主体下典型的 $x_1 x_2$ 尺度而言，这是一个极端值。\n   - 测试用例 2（边界条件，边际离群值但无极端交互）：\n     - 样本量 $n = 60$。\n     - 随机种子 $s = 24680$。\n     - 独立生成 $x_1$ 和 $x_2$，其中 $x_j \\sim \\mathcal{N}(0, 0.6^2)$。\n     - 强制将索引为 $i^\\star = 7$ 的单个观测值的 $x_1$ 设为 $4.0$，$x_2$ 设为 $0.2$。这产生了一个交互作用 $x_1 x_2 = 0.8$，相对于主体而言并非极端值，但 $x_1$ 是一个强烈的边际离群值。\n   - 测试用例 3（边缘情况，近共线性放大了交互杠杆）：\n     - 样本量 $n = 25$。\n     - 随机种子 $s = 31415$。\n     - 生成 $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ 并独立设置 $x_2 = x_1 + \\delta$，其中 $\\delta \\sim \\mathcal{N}(0, 0.1^2)$，从而产生近共线性。\n     - 强制将索引为 $i^\\star = 5$ 的单个观测值的 $x_1$ 设为 $2.0$，$x_2$ 设为 $2.0$，在一个小样本、近共线性的背景下，这导致了一个较大的交互作用 $x_1 x_2 = 4.0$。\n\n4. 对于每个数据集：\n   - 构建基础设计矩阵，其列为 $[1, x_1, x_2]$（截距加主效应）。\n   - 构建增广设计矩阵，其列为 $[1, x_1, x_2, x_1 x_2]$（截距加主效应加交互作用）。\n   - 对于每个设计矩阵，计算帽子矩阵及其对角线以获得杠杆值。标记满足 $h_{ii}  2 \\cdot (p/n)$ 的索引，其中 $p$ 等于所用设计矩阵的列数。\n   - 为每个数据集返回两个列表：由基础模型标记的已排序索引和由增广模型标记的已排序索引。\n\n5. 最终输出格式：\n   - 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素本身是两个列表的列表：第一个列表是基础模型标记的索引，第二个列表是增广模型标记的索引。\n   - 具体来说，输出应如下所示：\n     $$ [ [ [i_{1,1}, i_{1,2}, \\dots], [j_{1,1}, j_{1,2}, \\dots] ], [ [i_{2,1}, \\dots], [j_{2,1}, \\dots] ], [ [i_{3,1}, \\dots], [j_{3,1}, \\dots] ] ] $$\n     其中索引是升序排列的整数。\n\n此任务不涉及物理单位，不涉及角度单位，并且在任何地方都不需要百分比。",
            "solution": "我们从线性回归的第一性原理开始。在一个设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$、响应向量为 $y \\in \\mathbb{R}^{n}$ 的线性模型中，普通最小二乘法 (OLS) 估计量寻求参数 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 来最小化残差平方和\n$$ S(\\beta) = \\| y - X \\beta \\|_2^2. $$\n将 $S(\\beta)$ 对 $\\beta$ 的梯度设为零，得到正规方程\n$$ X^\\top X \\hat{\\beta} = X^\\top y. $$\n当 $X^\\top X$ 可逆时，唯一解为\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y. $$\n如果 $X^\\top X$ 不可逆，可以使用 Moore–Penrose 伪逆来获得最小范数解\n$$ \\hat{\\beta} = (X^\\top X)^{+} X^\\top y, $$\n其中 $(\\cdot)^{+}$ 表示伪逆。\n\nOLS 拟合值为\n$$ \\hat{y} = X \\hat{\\beta} = X (X^\\top X)^{+} X^\\top y = H y, $$\n其中\n$$ H = X (X^\\top X)^{+} X^\\top $$\n是投影矩阵（“帽子”矩阵）。矩阵 $H$ 是对称且幂等的，即 $H^\\top = H$ 且 $H^2 = H$。一个直接且基本的性质是\n$$ \\mathrm{trace}(H) = p, $$\n即 $X$ 的列数。因此，平均杠杆值为\n$$ \\bar{h} = \\frac{1}{n} \\sum_{i=1}^{n} h_{ii} = \\frac{p}{n}, $$\n其中 $h_{ii}$ 表示 $H$ 的第 $i$ 个对角线元素，即观测值 $i$ 的杠杆值。\n\n杠杆值量化了在由 $X$ 导出的度量中，一个观测值的预测变量向量与预测变量空间中心的距离。特别地，向 $X$ 添加列（例如，一个交互特征）会扩展特征空间，创造出一些方向，观测值可能沿着这些方向远离主体，从而可能增加其杠杆值。一个观测值可能单独具有中等的主效应 $x_1$ 和 $x_2$，但却有一个极端的交互作用 $x_1 x_2$；在增广空间中，这个观测值可能沿着交互维度远离主云团，这会增加其 $h_{ii}$，即使仅靠主效应不会产生高杠杆。\n\n基于这些原理推导出的算法步骤：\n1. 对于每个数据集，构建两个设计矩阵：\n   - 基础设计矩阵 $X_{\\mathrm{base}}$，其列为 $[1, x_1, x_2]$（截距和主效应）。\n   - 增广设计矩阵 $X_{\\mathrm{aug}}$，其列为 $[1, x_1, x_2, x_1 x_2]$（截距、主效应和交互作用）。\n2. 对于每个 $X$，使用 $X^\\top X$ 的伪逆计算帽子矩阵 $H = X (X^\\top X)^{+} X^\\top$，以确保数值稳定性。\n3. 提取杠杆值 $h_{ii}$ 作为 $H$ 的对角线元素。\n4. 使用基本性质 $\\bar{h} = p/n$，标记满足 $h_{ii}  2 \\cdot \\bar{h} = 2p/n$ 的索引。乘法常数 $2$ 设定了一个保守的准则，用于突出那些杠杆值显著超过平均值的观测值，并且它完全由从 $X$ 导出的 $p$ 和 $n$ 表示。\n\n数据集的构建：\n- 测试用例 1（理想情况）：\n  - 使用种子 $s = 12345$，为 $n=60$ 独立生成 $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$。强制索引 $i^\\star = 12$ 的值为 $x_1 = 1.8$，$x_2 = 2.2$，产生交互作用 $x_1 x_2 \\approx 3.96$，相对于低方差下的主体 $x_1 x_2$ 尺度而言较大。\n  - 生成 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\varepsilon$，其中 $\\beta_0 = 0.5$，$\\beta_1 = 1.0$，$\\beta_2 = -0.7$，$\\beta_3 = 0.8$，且 $\\varepsilon \\sim \\mathcal{N}(0, 0.3^2)$（独立）。\n- 测试用例 2（边界条件，边际离群值，非极端交互）：\n  - 使用种子 $s = 24680$，为 $n = 60$ 独立生成 $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$。强制索引 $i^\\star = 7$ 的值为 $x_1 = 4.0$，$x_2 = 0.2$，产生交互作用 $x_1 x_2 = 0.8$，在主体尺度下并非极端值，而 $x_1$ 是一个大的边际离群值。\n  - 如上生成 $y$。\n- 测试用例 3（边缘情况，近共线性）：\n  - 使用种子 $s = 31415$，为 $n = 25$ 独立生成 $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ 和 $\\delta \\sim \\mathcal{N}(0, 0.1^2)$。设置 $x_2 = x_1 + \\delta$，产生近共线性。强制索引 $i^\\star = 5$ 的值为 $x_1 = 2.0$，$x_2 = 2.0$，因此交互作用为 $x_1 x_2 = 4.0$，这在一个小样本、近共线性的背景下是显著的。\n  - 如上生成 $y$。\n\n该检测方法为何有效：\n- 对于具有 $p = 3$ 的基础设计矩阵 $X_{\\mathrm{base}}$，平均杠杆值为 $\\bar{h} = 3/n$。一个边际离群值（例如，大的 $x_1$）往往会增加其在基础模型下的杠杆值。然而，一个具有中等边际值但大交互作用的观测值在基础模型中并不显得极端，因为交互作用并未在其中表示；因此其杠杆值可能保持在平均水平附近。\n- 对于具有 $p = 4$ 的增广设计矩阵 $X_{\\mathrm{aug}}$，平均杠杆值为 $\\bar{h} = 4/n$。交互特征 $x_1 x_2$ 提供了一个新的方向。一个相对于典型尺度具有较大 $x_1 x_2$ 的观测值将沿着这个方向远离主体，从而增加其在 $X_{\\mathrm{aug}}$ 中的杠杆值 $h_{ii}$。因此，即使其边际分量仅为中等，这样的观测值也可能被增广模型标记为高杠杆。\n\n实现与输出：\n- 对于每个数据集，如上所述计算基础模型和增广模型的标记索引。将标记的索引按升序排序。\n- 程序必须打印单行，其中包含三个测试用例的结果列表。每个结果是一对列表：第一个用于基础模型，第二个用于增广模型。格式为\n  $$ [ [ \\text{base}_1, \\text{aug}_1 ], [ \\text{base}_2, \\text{aug}_2 ], [ \\text{base}_3, \\text{aug}_3 ] ], $$\n  其中每个 $\\text{base}_k$ 和 $\\text{aug}_k$ 是一个整数列表，代表测试用例 $k$ 的被标记观测值的索引。\n\n此过程完全基于 OLS 正规方程的推导、投影（帽子）矩阵 $H$ 及其基本性质，没有引用超出这些核心定义的快捷公式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hat_leverages(X: np.ndarray) - np.ndarray:\n    \"\"\"\n    Compute leverage values (diagonal of hat matrix) for design matrix X.\n    Uses the pseudoinverse of X^T X for numerical stability.\n    \"\"\"\n    XTX = X.T @ X\n    XTX_pinv = np.linalg.pinv(XTX)\n    H = X @ XTX_pinv @ X.T\n    # Numerical safety: clip tiny negative due to rounding to zero\n    h = np.clip(np.diag(H), 0.0, 1.0)\n    return h\n\ndef flag_high_leverage(h: np.ndarray, p: int) - np.ndarray:\n    \"\"\"\n    Flag indices where leverage exceeds 2 * (p/n).\n    Returns sorted indices as a numpy array of ints.\n    \"\"\"\n    n = h.shape[0]\n    avg_h = p / n\n    threshold = 2.0 * avg_h\n    idx = np.where(h  threshold)[0]\n    return np.sort(idx)\n\ndef build_case_1():\n    # Test case 1: extreme interaction with moderate marginals relative to low-variance bulk\n    n = 60\n    rng = np.random.default_rng(12345)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    # Force special observation\n    i_star = 12\n    x1[i_star] = 1.8\n    x2[i_star] = 2.2\n\n    # True parameters\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_2():\n    # Test case 2: marginal outlier without extreme interaction\n    n = 60\n    rng = np.random.default_rng(24680)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    i_star = 7\n    x1[i_star] = 4.0\n    x2[i_star] = 0.2\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_3():\n    # Test case 3: near collinearity amplifies interaction leverage\n    n = 25\n    rng = np.random.default_rng(31415)\n    x1 = rng.normal(0.0, 0.8, size=n)\n    delta = rng.normal(0.0, 0.1, size=n)\n    x2 = x1 + delta\n    i_star = 5\n    x1[i_star] = 2.0\n    x2[i_star] = 2.0\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = [build_case_1, build_case_2, build_case_3]\n\n    results = []\n    for build in cases:\n        X_base, X_aug, y = build()\n\n        # Compute leverages\n        h_base = hat_leverages(X_base)\n        h_aug = hat_leverages(X_aug)\n\n        # Flag high leverage indices using threshold 2 * (p/n)\n        base_flags = flag_high_leverage(h_base, p=X_base.shape[1]).tolist()\n        aug_flags = flag_high_leverage(h_aug, p=X_aug.shape[1]).tolist()\n\n        results.append([base_flags, aug_flags])\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```"
        },
        {
            "introduction": "影响点的影响远不止于改变模型系数，它们甚至能从根本上改变我们对最佳模型的选择。这个实践练习将引导你通过一个思想实验，探索单个数据点如何扭曲赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 等模型选择标准。通过亲手实现这个过程，你将直观地看到一个影响点是如何引导我们选择一个完全不同的模型，从而揭示了数据质量对模型结论的决定性作用。",
            "id": "3154883",
            "problem": "给定一个综合回归场景，用于评估单个强影响观测点的存在如何影响使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 进行模型选择。您将在高斯线性模型框架下工作：响应变量由线性信号加噪声生成，候选模型是次数在 $\\{1,2,3\\}$ 内的多项式。您的程序必须仅使用基于基本定义的推导来实现以下内容。\n\n从带有普通最小二乘法 (OLS) 的高斯线性模型开始：数据由 $n$ 对 $(x_i,y_i)$ 组成，其中 $x_i \\in \\mathbb{R}$，模型为 $y = X\\beta + \\varepsilon$，其中 $X$ 是设计矩阵，$\\beta$ 是参数向量，$\\varepsilon$ 是噪声向量。假设 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵，$\\sigma^2$ 是未知方差。使用高斯模型的最大似然估计 (MLE) 以根据残差平方和和样本量推导出最大化对数似然，然后使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 的定义，以根据拟合的残差、参数数量和样本量获得可实现的表达式。不要假设任何预先提供的快捷公式。\n\n将候选模型定义为包含截距的次数为 $d \\in \\{1,2,3\\}$ 的多项式回归，因此每个设计矩阵都包含列 $1, x, x^2, \\dots, x^d$。对于下方的每个测试用例：\n- 生成 $n$ 个基线预测变量 $x_i$，它们独立同分布于 $[-2,2]$ 上的均匀分布。\n- 从线性信号 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ 生成响应，其中 $\\beta_0 = 1.0$，$\\beta_1 = 2.0$，且 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。\n- 为每个候选模型计算 AIC 和 BIC，并记录所选的阶数（准则值最小的那个）。\n- 在 $x_{\\text{out}}$ 处添加一个强影响点，其响应为 $y_{\\text{out}} = \\beta_0 + \\beta_1 x_{\\text{out}} + \\Delta$，其中 $\\Delta$ 是一个指定的偏差，然后重新计算 AIC 和 BIC，并记录新选择的阶数。\n\n您的程序必须通过为随机数生成器使用指定的种子来确定性地运行以下测试套件：\n- 测试用例 1：$n=50$, $\\sigma=1.0$, $x_{\\text{out}}=6.0$, $\\Delta=30.0$, 种子 $=0$。\n- 测试用例 2：$n=50$, $\\sigma=0.2$, $x_{\\text{out}}=8.0$, $\\Delta=-60.0$, 种子 $=1$。\n- 测试用例 3：$n=20$, $\\sigma=0.05$, $x_{\\text{out}}=3.0$, $\\Delta=2.0$, 种子 $=2$。\n\n对于每个测试用例，报告在添加离群点前后，AIC 和 BIC 所选择的阶数。设 $d_{\\text{AIC}}^{\\text{before}}$ 和 $d_{\\text{AIC}}^{\\text{after}}$ 分别为添加离群点前后 AIC 选择的阶数；类似地定义 $d_{\\text{BIC}}^{\\text{before}}$ 和 $d_{\\text{BIC}}^{\\text{after}}$。要求的输出格式是包含一个列表的列表的单行：\n$[[d_{\\text{AIC}}^{\\text{before}},d_{\\text{AIC}}^{\\text{after}},d_{\\text{BIC}}^{\\text{before}},d_{\\text{BIC}}^{\\text{after}}],\\dots]$，\n其中每个内部列表对应一个测试用例，所有条目都是整数。\n\n您的程序应生成包含结果的单行输出，格式为方括号内以逗号分隔的列表（例如， $[[1,2,1,1],[\\dots],\\dots]$）。此问题不涉及任何物理单位或角度；所有输出均为无量纲整数。",
            "solution": "该问题要求在存在强影响数据点的情况下，对模型选择准则，特别是赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 进行分析。我们将在高斯线性模型的框架内进行操作，候选模型是不同次数的多项式。关键的第一步是在此背景下，从 AIC 和 BIC 的基本定义中推导出它们的显式公式。\n\n### 高斯线性模型的 AIC 和 BIC 推导\n\n**1. 模型与似然函数**\n指定的模型是高斯线性模型 $\\mathbf{y} = X\\beta + \\varepsilon$，其中 $\\mathbf{y}$ 是响应变量的 $n \\times 1$ 向量， $X$ 是预测变量的 $n \\times p$ 设计矩阵，$\\beta$ 是模型系数的 $p \\times 1$ 向量，$\\varepsilon$ 是误差项的 $n \\times 1$ 向量。问题陈述中说明，误差是独立同分布 (i.i.d.) 的，遵循均值为 $0$、方差为未知的 $\\sigma^2$ 的正态分布，记作 $\\varepsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I_n)$。\n\n对于单个数据点 $(x_i, y_i)$，其概率密度函数为：\n$$\nf(y_i | \\mathbf{x}_i, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^T\\beta)^2}{2\\sigma^2} \\right)\n$$\n其中 $\\mathbf{x}_i^T$ 是设计矩阵 $X$ 的第 $i$ 行。\n假设独立性，整个包含 $n$ 个观测值的数据集的似然函数 $L$ 是各个密度的乘积：\n$$\nL(\\beta, \\sigma^2; \\mathbf{y}, X) = \\prod_{i=1}^n f(y_i | \\mathbf{x}_i, \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T\\beta)^2 \\right)\n$$\n用矩阵表示，这变为：\n$$\nL(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta) \\right)\n$$\n因此，对数似然函数 $\\ell(\\beta, \\sigma^2) = \\ln L(\\beta, \\sigma^2)$ 为：\n$$\n\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)\n$$\n\n**2. 最大似然估计 (MLE)**\n为了找到最大化对数似然，我们必须求出参数 $\\beta$ 和 $\\sigma^2$ 的最大似然估计 (MLEs)。\n\n首先，对于固定的 $\\sigma^2$，我们关于 $\\beta$ 最大化 $\\ell$。这等价于最小化项 $(\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)$，即残差平方和 (RSS)。解是标准的普通最小二乘法 (OLS) 估计量：\n$$\n\\hat{\\beta}_{\\text{MLE}} = (X^T X)^{-1} X^T \\mathbf{y}\n$$\n接下来，我们将 $\\hat{\\beta}_{\\text{MLE}}$ 代入对数似然函数，并关于 $\\sigma^2$ 进行最大化。令 $\\text{RSS} = (\\mathbf{y} - X\\hat{\\beta}_{\\text{MLE}})^T (\\mathbf{y} - X\\hat{\\beta}_{\\text{MLE}})$ 为最小化的残差平方和。对数似然函数变为：\n$$\n\\ell(\\hat{\\beta}_{\\text{MLE}}, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{\\text{RSS}}{2\\sigma^2}\n$$\n对 $\\sigma^2$ 求导并设为零，可得：\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\text{RSS}}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{\\text{RSS}}{n}\n$$\n\n**3. 最大化对数似然**\n将最大似然估计 $\\hat{\\beta}_{\\text{MLE}}$ 和 $\\hat{\\sigma}^2_{\\text{MLE}}$ 代回对数似然函数，得到最大化值 $\\hat{\\ell}$：\n$$\n\\hat{\\ell} = \\ell(\\hat{\\beta}_{\\text{MLE}}, \\hat{\\sigma}^2_{\\text{MLE}}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\text{RSS}}{n}\\right) - \\frac{\\text{RSS}}{2(\\text{RSS}/n)}\n$$\n$$\n\\hat{\\ell} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\n\n**4. 信息准则的构建**\nAIC 和 BIC 是使用 $\\hat{\\ell}$、观测数量 $n$ 和估计参数数量 $k$ 来定义的。对于一个次数为 $d$ 的多项式回归，模型有 $d+1$ 个系数（$\\beta_0, \\dots, \\beta_d$）和一个方差参数（$\\sigma^2$），所以 $k = (d+1) + 1 = d+2$。\n\n赤池信息准则 (AIC) 是：\n$$\n\\text{AIC} = 2k - 2\\hat{\\ell} = 2(d+2) + n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\n\n贝叶斯信息准则 (BIC) 是：\n$$\n\\text{BIC} = k\\ln(n) - 2\\hat{\\ell} = (d+2)\\ln(n) + n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\n在这两种情况下，较小的值表示模型拟合得更好，它平衡了拟合优度（通过 $\\hat{\\ell}$）和模型复杂度（通过 $k$）。对于固定的数据集，项 $n(\\ln(2\\pi) + 1)$ 在所有模型中是恒定的，可以在比较时忽略，但为了忠实于推导，实现将使用完整的表达式。\n\n### 算法流程\n\n程序将对每个测试用例执行以下步骤：\n1.  设置随机种子以实现确定性数据生成。\n2.  生成一个包含 $n$ 个点 $(x_i, y_i)$ 的基线数据集，其中 $x_i \\sim U[-2, 2]$ 且 $y_i = 1 + 2x_i + \\varepsilon_i$，$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n3.  对于每个候选多项式次数 $d \\in \\{1, 2, 3\\}$：\n    a. 构建 $n \\times (d+1)$ 的设计矩阵 $X$。\n    b. 使用 `numpy.linalg.lstsq` 通过 OLS 计算 $\\hat{\\beta}$。\n    c. 计算 RSS。\n    d. 使用推导出的公式计算 AIC 和 BIC。\n4.  确定分别产生最小 AIC 和 BIC 值的阶数 $d_{\\text{AIC}}^{\\text{before}}$ 和 $d_{\\text{BIC}}^{\\text{before}}$。\n5.  用指定的强影响点 $(x_{\\text{out}}, y_{\\text{out}})$ 扩充数据集，使样本量增加到 $n+1$。\n6.  对扩充后的数据集重复步骤 3。\n7.  确定新的最优阶数 $d_{\\text{AIC}}^{\\text{after}}$ 和 $d_{\\text{BIC}}^{\\text{after}}$。\n8.  收集并按规定格式化这四个选定的阶数，作为最终输出。\n此过程直接模拟了高杠杆、高残差点对模型选择的影响。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of an influential point on model selection using AIC and BIC.\n    \"\"\"\n\n    def _select_model(x_data, y_data):\n        \"\"\"\n        Fits polynomial models and selects the best degree using AIC and BIC.\n\n        Args:\n            x_data (np.ndarray): 1D array of predictor values.\n            y_data (np.ndarray): 1D array of response values.\n        \n        Returns:\n            tuple: A tuple containing the selected degree by AIC and BIC.\n        \"\"\"\n        n_samples = len(y_data)\n        d_candidates = [1, 2, 3]\n        \n        model_metrics = []\n        for d in d_candidates:\n            # Construct the design matrix for a polynomial of degree d.\n            # X has columns for x^0, x^1, ..., x^d.\n            X = np.vander(x_data, d + 1, increasing=True)\n            \n            # Solve for beta_hat using Ordinary Least Squares (OLS).\n            # np.linalg.lstsq solves the equation y = Xb.\n            beta_hat, rss_container, _, _ = np.linalg.lstsq(X, y_data, rcond=None)\n            \n            # The returned rss_container is the sum of squared residuals if n  p,\n            # otherwise it's an empty array.\n            if n_samples > (d + 1):\n                rss = rss_container[0]\n            else:\n                # If n = p, the fit might be exact, so RSS must be calculated manually.\n                residuals = y_data - X @ beta_hat\n                rss = np.sum(residuals**2)\n\n            # The number of estimated parameters k is (d+1) for beta coefficients\n            # plus 1 for the variance sigma^2.\n            k = float(d + 2)\n            \n            # Guard against log(0) in case of a perfect fit.\n            if rss  1e-15:\n                # A perfect fit suggests gross overfitting. Assign a very poor score.\n                log_lik_max = -np.inf\n            else:\n                # Maximized log-likelihood for a Gaussian model.\n                log_lik_max = -n_samples / 2.0 * (np.log(2.0 * np.pi) + np.log(rss / n_samples) + 1.0)\n            \n            # Calculate AIC and BIC using their formal definitions.\n            aic = 2.0 * k - 2.0 * log_lik_max\n            bic = k * np.log(n_samples) - 2.0 * log_lik_max\n            \n            model_metrics.append({'d': d, 'aic': aic, 'bic': bic})\n        \n        # Select the degree that minimizes each criterion.\n        best_aic_model = min(model_metrics, key=lambda x: x['aic'])\n        best_bic_model = min(model_metrics, key=lambda x: x['bic'])\n        \n        return best_aic_model['d'], best_bic_model['d']\n\n    test_cases = [\n        # (n, sigma, x_out, delta, seed)\n        (50, 1.0, 6.0, 30.0, 0),\n        (50, 0.2, 8.0, -60.0, 1),\n        (20, 0.05, 3.0, 2.0, 2),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, sigma, x_out, delta, seed = case\n        \n        # Set the seed for reproducible random number generation.\n        rng = np.random.default_rng(seed)\n        \n        beta_0, beta_1 = 1.0, 2.0\n        \n        # --- Analysis Before Adding Outlier ---\n        # Generate baseline data from a true linear model with Gaussian noise.\n        x_base = rng.uniform(-2.0, 2.0, size=n)\n        epsilon = rng.normal(0.0, sigma, size=n)\n        y_base = beta_0 + beta_1 * x_base + epsilon\n        \n        # Select best model degree for the baseline data.\n        d_aic_before, d_bic_before = _select_model(x_base, y_base)\n\n        # --- Analysis After Adding Outlier ---\n        # Create the influential point and add it to the dataset.\n        y_out = beta_0 + beta_1 * x_out + delta\n        x_after = np.append(x_base, x_out)\n        y_after = np.append(y_base, y_out)\n        \n        # Re-run model selection on the augmented data.\n        d_aic_after, d_bic_after = _select_model(x_after, y_after)\n        \n        all_results.append([d_aic_before, d_aic_after, d_bic_before, d_bic_after])\n    \n    # Format the output to match the precise specification: [[1,2,1,1],[...],...]\n    # without spaces inside the inner lists.\n    inner_list_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_list_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "真实世界的问题往往涉及多个相关的响应变量，此时，逐个分析影响点可能会产生误导。本练习将你的诊断技能提升到多响应回归的领域，要求你从第一性原理出发，实现多变量库克距离的计算。通过比较多变量诊断与单变量诊断的结果，你将体会到在复杂模型中采用整体视角的重要性。",
            "id": "3154904",
            "problem": "给定一个多响应线性回归设定，其中响应矩阵有多列，并且对于某些测试用例，某个观测值在所有响应中均为极端值。您的任务是实现一个程序，为每个测试用例构建一个合成但科学上合理的的数据集，使用普通最小二乘法 (OLS) 拟合模型，计算每个观测值的多元库克距离，计算每个响应维度的单变量库克距离，并比较这些诊断结果。\n\n推导的基本出发点必须是标准多响应线性模型、OLS 正规方程、投影（帽子）矩阵、残差和残差协方差估计量，而不是依赖于影响力的简化公式。令 $n$ 表示观测数量，$p$ 表示包括截距项在内的预测变量数量，$m$ 表示响应变量的数量。模型为 $Y \\in \\mathbb{R}^{n \\times m}$，其行向量为 $y_i^\\top$；预测变量矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其行向量为 $x_i^\\top$；系数矩阵为 $B \\in \\mathbb{R}^{p \\times m}$；以及加性噪声为 $E \\in \\mathbb{R}^{n \\times m}$。您必须使用从正规方程导出的 OLS 估计量来拟合模型，并使用残差协方差估计量来缩放多元影响度量。除了线性模型和最小二乘框架，不要假设任何特殊的分布捷径。\n\n对于每个测试用例，按以下方式生成数据：\n- 创建 $X$，使其包含一列全为 1 的截距项和 $p - 1$ 个标准正态特征。\n- 从标准正态分布中抽取 $B$ 的元素。\n- 从均值为 $0$ 且具有指定标准差的正态分布中抽取独立的噪声 $E$。\n- 构成 $Y = X B + E$。\n- 对指定的观测索引 $i^\\*$ 应用指定的极端配置：\n    - 高杠杆修改：将 $X$ 中第 $i^\\*$ 行的非截距特征乘以给定因子。\n    - 极端响应修改：为 $Y$ 中第 $i^\\*$ 行的所有 $m$ 个响应添加一个常数偏移量。\n- 在修改第 $i^\\*$ 行的 $X$ 之后，确保在应用任何响应偏移之前，将 $Y_{i^\\* \\cdot}$ 重新计算为 $x_{i^\\*}^\\top B + e_{i^\\*}$，这样在进行极端响应偏移之前，$Y$ 内部与线性模型加噪声的结构保持一致。\n\n您必须计算：\n1. 每个观测值的多元库克距离，其构建原理基于案例删除下 OLS 解的变化，并由残差协方差进行缩放。并通过一个布尔值总结每个测试用例，该值指示指定的极端观测值 $i^\\*$ 是否获得了最大的多元库克距离。\n2. 每个响应维度的每个观测值的单变量库克距离，使用标准 OLS 量计算。并通过一个整数总结每个测试用例，该整数计算 $m$ 个单变量响应中有多少个将 $i^\\*$ 识别为具有最大库克距离的观测值。\n\n您的程序必须从第一性原理实现计算，而不是依赖任何内置的统计学习例程。最终输出必须将所有测试用例的这些摘要聚合为单行，格式为方括号内以逗号分隔的列表。\n\n测试套件：\n- 案例 $1$（理想路径：极端残差但无高杠杆）：\n    - $n = 40$，$p = 3$（一个截距项加 $2$ 个特征），$m = 3$，噪声标准差 $1.0$，随机种子 $12345$，极端值索引 $i^\\* = 5$，高杠杆乘数 $1.0$（无杠杆变化），极端响应偏移量 $12.0$。\n- 案例 $2$（边界情况：高杠杆但无极端残差）：\n    - $n = 40$，$p = 3$，$m = 3$，噪声标准差 $1.0$，随机种子 $12346$，极端值索引 $i^\\* = 10$，高杠杆乘数 $12.0$，极端响应偏移量 $0.0$。\n- 案例 $3$（边缘情况：兼具高杠杆和极端残差）：\n    - $n = 40$，$p = 3$，$m = 3$，噪声标准差 $1.0$，随机种子 $12347$，极端值索引 $i^\\* = 20$，高杠杆乘数 $12.0$，极端响应偏移量 $12.0$。\n\n答案格式：\n- 对于每个案例，生成两个值：\n    - 一个布尔值，表示指定的 $i^\\*$ 是否为多元库克距离的 argmax。\n    - 一个整数，等于 $i^\\*$ 作为单变量库克距离 argmax 的响应维度数量。\n- 您的程序应生成单行输出，其中包含按测试用例顺序排列的六个结果，格式为方括号内以逗号分隔的列表：$[\\text{case1\\_multivariate\\_bool},\\text{case1\\_univariate\\_count},\\text{case2\\_multivariate\\_bool},\\text{case2\\_univariate\\_count},\\text{case3\\_multivariate\\_bool},\\text{case3\\_univariate\\_count}]$。\n\n此问题中不需要物理单位、角度单位或百分比，所有输出必须是布尔或整数类型。通过严格遵守线性模型假设和使用指定种子进行可复现的随机生成，确保科学真实性。",
            "solution": "我们从多响应线性模型开始，该模型具有 $n$ 个观测值、$p$ 个包括截距项的预测变量和 $m$ 个响应变量。令 $Y \\in \\mathbb{R}^{n \\times m}$ 为响应矩阵，$X \\in \\mathbb{R}^{n \\times p}$ 为预测变量矩阵，并假设模型为 $Y = X B + E$，其中 $B \\in \\mathbb{R}^{p \\times m}$ 是待估计的系数矩阵，$E$ 是噪声。\n\n估计的基本基础是普通最小二乘法 (OLS)。OLS 估计量是通过最小化残差的弗罗贝尼乌斯范数获得的，从而得出正规方程。经过充分检验的 OLS 解的公式为\n$$\n\\hat{B} = (X^\\top X)^{-1} X^\\top Y,\n$$\n假设 $X^\\top X$ 是可逆的。在实践中，必要时可以使用 Moore–Penrose 伪逆，但对于随机生成的特征且 $n \\gg p$ 的情况，可逆性是典型的。拟合值矩阵为 $\\hat{Y} = X \\hat{B}$，残差矩阵为 $\\hat{E} = Y - \\hat{Y}$。残差协方差估计量由下式给出\n$$\n\\hat{S} = \\frac{\\hat{E}^\\top \\hat{E}}{n - p},\n$$\n这是一个经过充分检验的噪声协方差估计量，假设在多元线性模型中存在同方差性。\n\n杠杆率是一个基本概念：投影或帽子矩阵 $H$ 为\n$$\nH = X (X^\\top X)^{-1} X^\\top,\n$$\n杠杆值为 $h_{ii} = H_{ii}$。高杠杆值表示某个观测值的预测变量值不寻常，这可能对拟合模型产生强烈影响。\n\n为了进行影响比较，我们考虑删除观测值 $i$ 的效果。定义留一法估计量 $\\hat{B}_{(i)}$，它是通过从 $X$ 和 $Y$ 中移除第 $i$ 行计算得出的。系数估计的变化量为 $\\Delta_i = \\hat{B} - \\hat{B}_{(i)}$。一个有原则的多元影响度量使用 $X^\\top X$ 几何结构和逆残差协方差，聚合了此变化在所有响应上的缩放二次型，\n$$ M_i = \\Delta_i^\\top (X^\\top X) \\Delta_i \\in \\mathbb{R}^{m \\times m}, \\quad \\text{以及} \\quad D_i^{\\text{multi}} = \\frac{1}{m p} \\operatorname{tr}\\!\\left(M_i \\hat{S}^{-1}\\right). $$\n这种构造推广了单变量情况下使用 $X^\\top X$ 加权和均方误差归一化的做法，通过适当地使用 $m$ 个响应维度上的逆残差协方差进行缩放，并除以 $m p$，以便在 $m = 1$ 的特殊情况下，它简化为熟悉的单变量情况，即除以 $p$ 乘以均方误差。\n\n对于单变量诊断，当考虑单个响应 $y \\in \\mathbb{R}^n$ 时，OLS 残差向量为 $\\hat{e} = y - X \\hat{\\beta}$，均方误差 (MSE) 为\n$$\n\\text{MSE} = \\frac{\\hat{e}^\\top \\hat{e}}{n - p},\n$$\n一个公认的影响度量是库克距离，\n$$ D_i^{\\text{uni}} = \\frac{\\hat{e}_i^2}{p \\cdot \\text{MSE}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}. $$\n该表达式源于通过分块逆或 Sherman–Morrison 恒等式推导出的、拟合值的留一法变化与残差 $ \\hat{e}_i $ 及杠杆值 $ h_{ii} $ 之间的精确关系。\n\n算法设计：\n- 对于每个测试用例，构建 $X$，抽取 $B$，抽取噪声 $E$，构成 $Y = X B + E$，对 $X$ 和/或 $Y$ 应用指定的极端配置，通过在应用极端响应偏移之前将 $Y_{i^\\* \\cdot}$ 重新计算为 $x_{i^\\*}^\\top B + e_{i^\\*}$ 来确保内部一致性。\n- 拟合多元 OLS 以获得 $\\hat{B}$、残差 $\\hat{E}$ 和 $\\hat{S}$，以及 $X^\\top X$。\n- 对于每个观测值 $i$，通过移除第 $i$ 行并重新拟合来计算留一法估计量 $\\hat{B}_{(i)}$，计算 $\\Delta_i = \\hat{B} - \\hat{B}_{(i)}$，构成 $M_i = \\Delta_i^\\top (X^\\top X) \\Delta_i$，并计算多元库克距离 $D_i^{\\text{multi}} = \\frac{1}{m p} \\operatorname{tr}(M_i \\hat{S}^{-1})$。\n- 为计算单变量库克距离，对每个响应列 $y^{(j)}$，计算 $\\hat{\\beta}^{(j)}$、残差 $\\hat{e}^{(j)}$、该响应的 MSE，以及来自公共帽子矩阵 $H$ 的杠杆值 $h_{ii}$，然后应用单变量公式以获得所有 $i$ 的 $D_i^{\\text{uni}, j}$。\n- 对于每个案例，确定指定的极端索引 $i^\\*$ 是否为多元库克距离的 argmax，并计算在多少个响应维度中 $i^\\*$ 是单变量库克距离的 argmax。\n\n测试套件中的解释预期：\n- 案例 1 呈现了一个在所有响应中具有极端残差但无高杠杆的观测值，多元和单变量库克距离都应能强烈地标记出它。\n- 案例 2 呈现了一个具有高杠杆但无极端残差的观测值；仅有杠杆而没有大残差通常不会产生高的库克距离，因此其影响可能不大，并非最大。\n- 案例 3 同时呈现了高杠杆和极端残差，这两者共同作用应在多元和单变量诊断中都产生巨大影响。\n\n程序以确切指定的格式输出单行，汇总了三个案例中对 $i^\\*$ 的布尔多元识别结果和整数单变量识别计数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_multivariate_ols(X, Y):\n    \"\"\"\n    Fit multiresponse OLS: returns B_hat, residual covariance S_hat, and XTX.\n    \"\"\"\n    # Use explicit normal equations with pinv for numerical stability\n    X_pinv = np.linalg.pinv(X)\n    B_hat = X_pinv @ Y\n    Y_hat = X @ B_hat\n    E_hat = Y - Y_hat\n    n, p = X.shape\n    # Residual covariance estimator\n    S_hat = (E_hat.T @ E_hat) / (n - p)\n    XTX = X.T @ X\n    return B_hat, S_hat, XTX\n\ndef multivariate_cooks_distance(X, Y):\n    \"\"\"\n    Compute multivariate Cook's distance per observation using case deletion.\n    D_i = (1/(m*p)) * tr( (Delta_i^T X^T X Delta_i) S_hat^{-1} ).\n    \"\"\"\n    n, p = X.shape\n    m = Y.shape[1]\n    B_hat, S_hat, XTX = fit_multivariate_ols(X, Y)\n    S_inv = np.linalg.inv(S_hat)\n    D = np.zeros(n)\n    for i in range(n):\n        # Delete observation i\n        X_minus_i = np.delete(X, i, axis=0)\n        Y_minus_i = np.delete(Y, i, axis=0)\n        # Refit\n        B_minus_i = np.linalg.pinv(X_minus_i) @ Y_minus_i\n        Delta = B_hat - B_minus_i  # p x m\n        M = Delta.T @ XTX @ Delta  # m x m\n        D[i] = (np.trace(M @ S_inv)) / (m * p)\n    return D\n\ndef univariate_cooks_distance(X, y):\n    \"\"\"\n    Compute univariate Cook's distance for a single response vector y.\n    Uses D_i = (e_i^2/(p*MSE)) * (h_ii/(1 - h_ii)^2).\n    \"\"\"\n    n, p = X.shape\n    # OLS fit\n    beta_hat = np.linalg.pinv(X) @ y\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    # Hat matrix and leverages\n    H = X @ np.linalg.pinv(X)\n    h = np.clip(np.diag(H), 0.0, 1.0)\n    # Mean Squared Error\n    MSE = float(e.T @ e) / (n - p)\n    # Cook's distance\n    denom = (1.0 - h) ** 2\n    # Avoid division by zero with safe handling\n    safe = denom > 1e-12\n    D = np.zeros(n)\n    if MSE > 1e-12:\n        D[safe] = ( (e[safe] ** 2) / (p * MSE) ) * ( h[safe] / denom[safe] )\n    # If denom is extremely small, set influence to a large number to reflect instability\n    D[~safe] = np.inf\n    return D\n\ndef generate_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std):\n    \"\"\"\n    Generate a synthetic multiresponse regression case.\n    X: intercept + (p-1) standard normal features.\n    B: standard normal coefficients.\n    E: normal noise with std noise_std.\n    Apply leverage and response modifications at extreme_index.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d_features = p - 1\n    X_features = rng.normal(0.0, 1.0, size=(n, d_features))\n    X = np.hstack([np.ones((n, 1)), X_features])\n    B = rng.normal(0.0, 1.0, size=(p, m))\n    # Noise\n    E = rng.normal(0.0, noise_std, size=(n, m))\n    # Apply leverage modification on X at extreme_index\n    if leverage_multiplier != 1.0:\n        X[extreme_index, 1:] *= leverage_multiplier\n    # Build Y consistent with X and noise\n    Y = X @ B + E\n    # Now apply response offset (extreme in all responses) at extreme_index\n    if response_offset != 0.0:\n        Y[extreme_index, :] += response_offset\n    return X, Y, B\n\ndef analyze_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std):\n    \"\"\"\n    For a given case, compute multivariate Cook's distances and univariate counts.\n    Returns:\n      bool indicating if extreme_index is argmax of multivariate Cook's distance,\n      int count of univariate responses where extreme_index is argmax of Cook's distance.\n    \"\"\"\n    X, Y, _ = generate_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std)\n    # Multivariate Cook's distances\n    D_multi = multivariate_cooks_distance(X, Y)\n    extreme_is_max_multi = int(np.argmax(D_multi)) == extreme_index\n    # Univariate Cook's distances per response\n    count_univariate_max = 0\n    for j in range(m):\n        D_uni = univariate_cooks_distance(X, Y[:, j])\n        if int(np.argmax(D_uni)) == extreme_index:\n            count_univariate_max += 1\n    return bool(extreme_is_max_multi), int(count_univariate_max)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std)\n    test_cases = [\n        (40, 3, 3, 12345, 5, 1.0, 12.0, 1.0),   # Case 1: extreme residual, no leverage\n        (40, 3, 3, 12346, 10, 12.0, 0.0, 1.0),  # Case 2: high leverage, no residual extreme\n        (40, 3, 3, 12347, 20, 12.0, 12.0, 1.0), # Case 3: high leverage and extreme residual\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, m, seed, extreme_index, lev_mult, resp_off, noise_std = case\n        is_max_multi, uni_count = analyze_case(n, p, m, seed, extreme_index, lev_mult, resp_off, noise_std)\n        results.append(str(is_max_multi).lower())\n        results.append(uni_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}