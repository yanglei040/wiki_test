{
    "hands_on_practices": [
        {
            "introduction": "Data preprocessing is a standard step in building statistical models, but how do these transformations interact with more complex model features like interaction terms? This exercise delves into this question by examining the effects of predictor standardization on a linear model that includes an interaction. By comparing the properties of models built with original and standardized predictors, you will develop a deeper intuition for what remains invariant, such as the model's predictive power, and what changes, such as the individual coefficient values and their interpretations .",
            "id": "3132263",
            "problem": "A data analyst is studying a response $y$ and two predictors $x_1$ and $x_2$ using ordinary least squares (OLS). They consider two linear models with an interaction term and an intercept. The first model uses the original predictors:\n$$\n\\text{Model-U:}\\quad y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_{1i} \\;+\\; \\beta_2 x_{2i} \\;+\\; \\beta_3 x_{1i} x_{2i} \\;+\\; \\varepsilon_i,\\quad i=1,\\dots,n,\n$$\nwhere $\\varepsilon_i$ are mean-zero errors and $n$ is the sample size. The second model uses standardized predictors $z_1$ and $z_2$ defined by\n$$\nz_{1i} \\;=\\; \\frac{x_{1i} - \\bar{x}_1}{s_1},\\qquad z_{2i} \\;=\\; \\frac{x_{2i} - \\bar{x}_2}{s_2},\n$$\nwhere $\\bar{x}_j$ is the sample mean and $s_j$ is the sample standard deviation of $x_j$ for $j\\in\\{1,2\\}$, and the interaction term is the product $z_{1i} z_{2i}$:\n$$\n\\text{Model-S:}\\quad y_i \\;=\\; \\alpha_0 \\;+\\; \\alpha_1 z_{1i} \\;+\\; \\alpha_2 z_{2i} \\;+\\; \\alpha_3 z_{1i} z_{2i} \\;+\\; \\varepsilon_i.\n$$\nBoth models include an intercept and are fit by minimizing the OLS sum of squared residuals. Assume $s_1>0$ and $s_2>0$.\n\nWhich of the following statements are true?\n\nA. The OLS fitted values $\\hat{y}_i$ from Model-U and Model-S are identical for all $i$.\n\nB. The interaction coefficient transforms by a scale factor: $\\alpha_3 = \\beta_3 \\, s_1 s_2$.\n\nC. In the presence of the interaction term, standardizing ensures $\\alpha_1 = \\beta_1 / s_1$ and $\\alpha_2 = \\beta_2 / s_2$.\n\nD. The null hypothesis of no interaction, $H_0:\\beta_3=0$, yields the same test decision and $p$-value as $H_0:\\alpha_3=0$ when using OLS.\n\nE. Standardizing the predictors reduces the variance inflation factor (VIF) of the main effects to $1$.",
            "solution": "The user has provided a problem concerning the comparison of two ordinary least squares (OLS) linear regression models. The first model uses original predictors, and the second uses standardized predictors. I will first validate the problem statement and then proceed to a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   Response variable: $y$\n*   Predictors: $x_1$, $x_2$\n*   Modeling technique: Ordinary Least Squares (OLS)\n*   Model-U (Unstandardized): $y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_{1i} \\;+\\; \\beta_2 x_{2i} \\;+\\; \\beta_3 x_{1i} x_{2i} \\;+\\; \\varepsilon_i$, for $i=1,\\dots,n$.\n*   Model-S (Standardized): $y_i \\;=\\; \\alpha_0 \\;+\\; \\alpha_1 z_{1i} \\;+\\; \\alpha_2 z_{2i} \\;+\\; \\alpha_3 z_{1i} z_{2i} \\;+\\; \\varepsilon_i$.\n*   Error terms $\\varepsilon_i$ have a mean of zero.\n*   $n$ is the sample size.\n*   The standardized predictors are defined as:\n    $$z_{1i} \\;=\\; \\frac{x_{1i} - \\bar{x}_1}{s_1},\\qquad z_{2i} \\;=\\; \\frac{x_{2i} - \\bar{x}_2}{s_2}$$\n    where $\\bar{x}_j$ is the sample mean of $x_j$ and $s_j$ is the sample standard deviation of $x_j$.\n*   Assumptions: $s_1 > 0$ and $s_2 > 0$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is well-grounded in the theory of statistical linear models. It addresses the effects of predictor transformations (specifically, standardization) on model parameters, fitted values, and hypothesis tests. This is a fundamental and standard topic in statistics and machine learning.\n2.  **Well-Posed**: The problem is well-posed. The a priori assumption is that the design matrices of the models have full column rank, allowing for a unique OLS solution. The transformations are explicitly defined. The questions asked are about the mathematical relationships between the two models, which can be unambiguously determined.\n3.  **Objective**: The problem is stated using precise mathematical and statistical terminology. It is free of ambiguity, subjectivity, or opinion.\n4.  **Flaw Checklist**: The problem does not violate any of the specified invalidity criteria. It is scientifically sound, formalizable, complete, realistic, and well-structured.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed with a full derivation and analysis.\n\n### Solution Derivation\n\nThe core of this problem lies in understanding the relationship between the set of predictors in Model-U, $\\{1, x_1, x_2, x_1 x_2\\}$, and the set of predictors in Model-S, $\\{1, z_1, z_2, z_1 z_2\\}$.\n\nThe transformation from $x_j$ to $z_j$ is an invertible affine transformation:\n$x_{1i} = s_1 z_{1i} + \\bar{x}_1$\n$x_{2i} = s_2 z_{2i} + \\bar{x}_2$\n\nLet's examine the column space of the design matrix for each model. Let $C(X_U)$ be the column space for Model-U and $C(X_S)$ be the column space for Model-S.\n\nThe predictors of Model-S can be expressed as linear combinations of the predictors of Model-U:\n*   $1$ is a predictor in both models.\n*   $z_{1i} = \\frac{1}{s_1} x_{1i} - \\frac{\\bar{x}_1}{s_1} (1)$. This is a linear combination of $x_1$ and $1$.\n*   $z_{2i} = \\frac{1}{s_2} x_{2i} - \\frac{\\bar{x}_2}{s_2} (1)$. This is a linear combination of $x_2$ and $1$.\n*   $z_{1i}z_{2i} = \\left(\\frac{x_{1i} - \\bar{x}_1}{s_1}\\right) \\left(\\frac{x_{2i} - \\bar{x}_2}{s_2}\\right) = \\frac{1}{s_1 s_2} (x_{1i}x_{2i} - \\bar{x}_2 x_{1i} - \\bar{x}_1 x_{2i} + \\bar{x}_1 \\bar{x}_2)$. This is a linear combination of $x_1 x_2$, $x_1$, $x_2$, and $1$.\n\nSince every predictor in Model-S is a linear combination of the predictors in Model-U, the column space of the Model-S design matrix is a subspace of the column space of the Model-U design matrix, i.e., $C(X_S) \\subseteq C(X_U)$.\n\nConversely, the predictors of Model-U can be expressed as linear combinations of the predictors of Model-S:\n*   $1$ is a predictor in both models.\n*   $x_{1i} = s_1 z_{1i} + \\bar{x}_1 (1)$. This is a linear combination of $z_1$ and $1$.\n*   $x_{2i} = s_2 z_{2i} + \\bar{x}_2 (1)$. This is a linear combination of $z_2$ and $1$.\n*   $x_{1i} x_{2i} = (s_1 z_{1i} + \\bar{x}_1)(s_2 z_{2i} + \\bar{x}_2) = s_1 s_2 z_{1i} z_{2i} + s_1 \\bar{x}_2 z_{1i} + s_2 \\bar{x}_1 z_{2i} + \\bar{x}_1 \\bar{x}_2$. This is a linear combination of $z_1 z_2$, $z_1$, $z_2$, and $1$.\n\nSince every predictor in Model-U is a linear combination of the predictors in Model-S, $C(X_U) \\subseteq C(X_S)$.\nCombining both results, we have $C(X_U) = C(X_S)$. The two models are reparameterizations of each other, spanning the exact same vector space of predictors.\n\n### Option-by-Option Analysis\n\n**A. The OLS fitted values $\\hat{y}_i$ from Model-U and Model-S are identical for all $i$.**\nIn OLS, the vector of fitted values, $\\hat{\\mathbf{y}}$, is the orthogonal projection of the response vector $\\mathbf{y}$ onto the column space of the design matrix. As established above, both models have identical predictor column spaces, $C(X_U) = C(X_S)$. The projection of $\\mathbf{y}$ onto this common space must be unique. Therefore, the vector of fitted values $\\hat{\\mathbf{y}}$ is identical for both models, which implies that $\\hat{y}_i$ is the same for each observation $i=1,\\dots,n$.\n**Verdict: Correct.**\n\n**B. The interaction coefficient transforms by a scale factor: $\\alpha_3 = \\beta_3 \\, s_1 s_2$.**\nTo find the relationship between the coefficients, we can substitute the definitions of $z_1$ and $z_2$ into Model-S and rearrange it into the form of Model-U.\n$$ y_i = \\alpha_0 + \\alpha_1 \\left(\\frac{x_{1i} - \\bar{x}_1}{s_1}\\right) + \\alpha_2 \\left(\\frac{x_{2i} - \\bar{x}_2}{s_2}\\right) + \\alpha_3 \\left(\\frac{x_{1i} - \\bar{x}_1}{s_1}\\right) \\left(\\frac{x_{2i} - \\bar{x}_2}{s_2}\\right) + \\varepsilon_i $$\nExpanding the terms:\n$$ y_i = \\dots + \\frac{\\alpha_3}{s_1 s_2} (x_{1i} x_{2i} - \\bar{x}_2 x_{1i} - \\bar{x}_1 x_{2i} + \\bar{x}_1 \\bar{x}_2) + \\dots + \\varepsilon_i $$\nThe coefficient of the $x_{1i}x_{2i}$ term in this expanded model is $\\frac{\\alpha_3}{s_1 s_2}$. By comparing this to Model-U, where the coefficient of $x_{1i}x_{2i}$ is $\\beta_3$, we must have:\n$$ \\beta_3 = \\frac{\\alpha_3}{s_1 s_2} $$\nRearranging for $\\alpha_3$ gives:\n$$ \\alpha_3 = \\beta_3 s_1 s_2 $$\n**Verdict: Correct.**\n\n**C. In the presence of the interaction term, standardizing ensures $\\alpha_1 = \\beta_1 / s_1$ and $\\alpha_2 = \\beta_2 / s_2$.**\nLet's continue the expansion from (B) and collect the coefficients for $x_{1i}$ and $x_{2i}$.\nCoefficient of $x_{1i}$: From the $\\alpha_1$ term, we have $\\frac{\\alpha_1}{s_1}$. From the $\\alpha_3$ term, we have $-\\frac{\\alpha_3 \\bar{x}_2}{s_1 s_2}$. Thus, the total coefficient for $x_{1i}$ is $\\frac{\\alpha_1}{s_1} - \\frac{\\alpha_3 \\bar{x}_2}{s_1 s_2}$. Comparing to Model-U, we get:\n$$ \\beta_1 = \\frac{\\alpha_1}{s_1} - \\frac{\\alpha_3 \\bar{x}_2}{s_1 s_2} $$\nSolving for $\\alpha_1$:\n$$ \\alpha_1 = s_1 \\beta_1 + \\frac{\\alpha_3 \\bar{x}_2}{s_2} $$\nUsing the relationship from (B), $\\alpha_3 = \\beta_3 s_1 s_2$:\n$$ \\alpha_1 = s_1 \\beta_1 + \\frac{(\\beta_3 s_1 s_2) \\bar{x}_2}{s_2} = s_1 \\beta_1 + s_1 \\bar{x}_2 \\beta_3 = s_1(\\beta_1 + \\bar{x}_2 \\beta_3) $$\nThe statement claims $\\alpha_1 = \\beta_1 / s_1$. This is clearly not true in general. The simple scaling relationship holds only if the second term is zero, which requires $\\beta_3=0$ (no interaction) or $\\bar{x}_2=0$ (predictor $x_2$ is centered). Since the problem includes an interaction and does not assume centered data, the statement is false. A similar derivation holds for $\\alpha_2$ and $\\beta_2$.\n**Verdict: Incorrect.**\n\n**D. The null hypothesis of no interaction, $H_0:\\beta_3=0$, yields the same test decision and $p$-value as $H_0:\\alpha_3=0$ when using OLS.**\nThe hypothesis test for a single coefficient is typically a t-test or an equivalent F-test. The F-test compares the residual sum of squares (RSS) of the full model to the RSS of a reduced model where the coefficient is constrained to be zero.\nThe F-statistic is given by $F = \\frac{(\\text{RSS}_{\\text{reduced}} - \\text{RSS}_{\\text{full}})/df_{\\text{dropped}}}{\\text{RSS}_{\\text{full}}/df_{\\text{full}}}$.\nFor testing $H_0: \\beta_3=0$, the full model is Model-U and the reduced model is $y_i = \\beta_0' + \\beta_1' x_{1i} + \\beta_2' x_{2i} + \\varepsilon_i'$.\nFor testing $H_0: \\alpha_3=0$, the full model is Model-S and the reduced model is $y_i = \\alpha_0' + \\alpha_1' z_{1i} + \\alpha_2' z_{2i} + \\varepsilon_i'$.\nFrom (A), we know that the RSS for the full Model-U and full Model-S are identical: $\\text{RSS}_{\\text{full,U}} = \\text{RSS}_{\\text{full,S}}$.\nNow consider the reduced models. The predictor space for the reduced U-model is $\\text{span}\\{1, x_1, x_2\\}$. The predictor space for the reduced S-model is $\\text{span}\\{1, z_1, z_2\\}$. Since $z_1$ and $z_2$ are just affine transformations of $x_1$ and $x_2$, these two spaces are identical. Therefore, the projections onto these spaces are the same, and their RSS values are identical: $\\text{RSS}_{\\text{reduced,U}} = \\text{RSS}_{\\text{reduced,S}}$.\nSince all the corresponding RSS values and degrees of freedom are identical for both hypothesis tests, the resulting F-statistics must be identical. An identical F-statistic implies an identical p-value and thus the same test decision at any given significance level.\nAlternatively, the t-statistic for $\\alpha_3$ is $t_{\\alpha_3} = \\hat{\\alpha}_3/SE(\\hat{\\alpha}_3)$. From (B), $\\hat{\\alpha}_3 = \\hat{\\beta}_3 s_1 s_2$. The standard errors also transform proportionally, $SE(\\hat{\\alpha}_3) = SE(\\hat{\\beta}_3) s_1 s_2$. Therefore, $t_{\\alpha_3} = \\frac{\\hat{\\beta}_3 s_1 s_2}{SE(\\hat{\\beta}_3) s_1 s_2} = t_{\\beta_3}$. The t-statistics are identical.\n**Verdict: Correct.**\n\n**E. Standardizing the predictors reduces the variance inflation factor (VIF) of the main effects to $1$.**\nThe VIF for a predictor $j$ is $VIF_j = 1/(1 - R_j^2)$, where $R_j^2$ is the coefficient of determination from regressing predictor $j$ on all other predictors in the model. A VIF of $1$ implies $R_j^2=0$, which means predictor $j$ is completely uncorrelated with all other predictors.\nLet's consider $VIF_{z_1}$ in Model-S. The predictors are $\\{1, z_1, z_2, z_1 z_2\\}$. To calculate $VIF_{z_1}$, we need the $R^2$ from the regression of $z_1$ on $\\{1, z_2, z_1 z_2\\}$.\nBy construction, a standardized variable $z_1$ has a mean of zero, so it is orthogonal to the intercept term (the vector of ones). However, $z_1$ is not generally orthogonal to the other predictors, $z_2$ and $z_1 z_2$.\n1. Correlation between $z_1$ and $z_2$: $\\text{corr}(z_1, z_2) = \\text{corr}(x_1, x_2)$. This is generally not zero.\n2. Correlation between $z_1$ and $z_1 z_2$: The sample covariance is proportional to $\\sum_{i=1}^n z_{1i} (z_{1i} z_{2i}) = \\sum_{i=1}^n z_{1i}^2 z_{2i}$. This sum is not guaranteed to be zero. For example, if $z_1$ and $z_2$ are positively correlated, points with large $z_{1i}^2$ might tend to have positive $z_{2i}$, leading to a positive sum.\nSince $z_1$ is generally correlated with $z_2$ and/or $z_1 z_2$, the $R^2$ from regressing $z_1$ on the other predictors will not be zero. Consequently, $VIF_{z_1}$ will not be $1$. While standardization can often reduce multicollinearity (especially between main effects and interactions like $x_1$ and $x_1x_2$), it does not eliminate it or guarantee VIFs of $1$.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "While interaction terms can capture important non-linear relationships, they can also introduce practical challenges like multicollinearity. This hands-on simulation is designed to give you a concrete feel for this problem by exploring how correlation between two main predictors affects the stability of the estimated interaction coefficient. By systematically varying the correlation and observing its impact on the confidence intervals for the interaction term, you will gain a practical understanding of why high collinearity can make it difficult to reliably interpret individual model coefficients .",
            "id": "3132247",
            "problem": "You are asked to examine, via controlled simulation, how correlation between predictors inflates uncertainty in the estimated coefficient of an interaction term in a normal linear model. Build a program that, for a set of specified test cases, estimates the empirical coverage and the average length of a two-sided confidence interval for the interaction coefficient under varying predictor correlation. The problem is framed in purely mathematical terms and requires using the fundamental base of the normal linear model, Ordinary Least Squares (OLS), and small-sample inference with the Student t-distribution.\n\nConsider the normal linear model\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independently of the predictors. For each simulation replicate, generate the predictors as a correlated bivariate normal vector\n$$\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n\\sim \\mathcal{N}\\!\\left(\n\\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix},\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}\n\\right)\n$$\nwith fixed means $\\mu_1 = 1$ and $\\mu_2 = 1$ and correlation $\\rho \\in (-1,1)$. Generate the response using the model with fixed parameters $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_{12} = 0.5$, and $\\sigma = 1$.\n\nYour program must, for each test case, perform Monte Carlo simulation with $R$ independent replicates. In each replicate: draw a sample of size $n$ from the specified distribution for $(x_1,x_2)$ and $\\varepsilon$, fit the OLS model including an intercept, main effects $x_1$, $x_2$, and the interaction $x_1 x_2$, and construct a two-sided confidence interval for $\\beta_{12}$ at nominal level $1-\\alpha = 0.95$ using the Student t-distribution with $n-p$ degrees of freedom, where $p=4$ is the number of regression coefficients including the intercept. Record whether the true $\\beta_{12}$ is contained in the interval (coverage indicator equal to $1$ if yes and $0$ otherwise) and the interval length (a nonnegative real number). After $R$ replicates, report for each test case:\n- the empirical coverage probability as a decimal (the average of the coverage indicators), and\n- the average interval length (the average of the replicate interval lengths).\n\nUse the following fixed values for all test cases:\n- $R = 500$,\n- $\\alpha = 0.05$,\n- $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_{12} = 0.5$,\n- $\\sigma = 1$,\n- $\\mu_1 = 1$, $\\mu_2 = 1$,\n- random seed fixed to $123456$ for reproducibility.\n\nTest suite:\n1. $(n,\\rho) = (200, 0.0)$,\n2. $(n,\\rho) = (200, 0.9)$,\n3. $(n,\\rho) = (200, -0.9)$,\n4. $(n,\\rho) = (60, 0.99)$.\n\nOutput specification:\n- For each test case in the listed order, output a two-element list $[c,\\ell]$ where $c$ is the empirical coverage probability and $\\ell$ is the average confidence-interval length. Both $c$ and $\\ell$ must be rounded to $3$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list in the same format, for example:\n$[[c_1,\\ell_1],[c_2,\\ell_2],[c_3,\\ell_3],[c_4,\\ell_4]]$.\n- There are no physical units or angles in this problem. Coverage must be expressed as a decimal, not with a percent sign.\n\nScientific realism and derivation base:\n- Use only the standard properties of the normal linear model, the OLS estimator, and small-sample t-based confidence intervals as the fundamental base. Do not assume or use any shortcut formulas beyond these fundamentals.\n- Ensure all random number generation is controlled by the fixed seed so that outputs are reproducible.\n\nYour program must be a complete, runnable script that requires no user input and no external files, and must adhere to the specified output format exactly.",
            "solution": "The problem is valid. It is a well-posed and scientifically grounded exercise in computational statistics, investigating the effect of multicollinearity on inference for an interaction term within the standard normal linear model framework. All required parameters, constants, and procedures are clearly specified.\n\nThe objective is to conduct a Monte Carlo simulation to quantify how the correlation $\\rho$ between two predictors, $x_1$ and $x_2$, affects the empirical coverage and average length of the confidence interval for the interaction coefficient $\\beta_{12}$.\n\nThe statistical foundation is the normal linear model, which in matrix form is expressed as:\n$$\ny = X\\beta + \\varepsilon\n$$\nwhere $y$ is an $n \\times 1$ vector of observations, $X$ is the $n \\times p$ design matrix, $\\beta$ is the $p \\times 1$ vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of unobserved errors. For this problem, the model is $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon$, so the number of parameters is $p=4$. The coefficient vector is $\\beta = [\\beta_0, \\beta_1, \\beta_2, \\beta_{12}]^T$. The $i$-th row of the design matrix $X$ is $[1, x_{i1}, x_{i2}, x_{i1}x_{i2}]$. The errors $\\varepsilon_i$ are assumed to be independent and identically distributed as $\\mathcal{N}(0, \\sigma^2)$.\n\nThe Ordinary Least Squares (OLS) estimator for the coefficient vector $\\beta$ is given by:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\nUnder the model assumptions, the conditional distribution of the OLS estimator given the design matrix $X$ is normal, with mean $\\beta$ and variance-covariance matrix:\n$$\n\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X^T X)^{-1}\n$$\nIn a practical setting, the error variance $\\sigma^2$ is unknown and must be estimated from the data. The unbiased estimator for $\\sigma^2$ is:\n$$\ns^2 = \\frac{e^T e}{n-p} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p}\n$$\nwhere $e = y - X\\hat{\\beta}$ is the vector of residuals and $n-p$ are the degrees of freedom. The estimated variance of the coefficient vector is then $\\hat{\\text{Var}}(\\hat{\\beta}) = s^2(X^T X)^{-1}$. The standard error of a single coefficient estimate $\\hat{\\beta}_j$ is the square root of the $j$-th diagonal element of this matrix:\n$$\nse(\\hat{\\beta}_j) = \\sqrt{s^2 \\left((X^T X)^{-1}\\right)_{jj}}\n$$\nwhere the indices $j$ range from $0$ to $p-1=3$. Our coefficient of interest, $\\beta_{12}$, corresponds to index $j=3$.\n\nFor small samples, inference is based on the Student's t-distribution. The pivotal quantity for the interaction coefficient $\\beta_{12}$ is:\n$$\n\\frac{\\hat{\\beta}_{12} - \\beta_{12}}{se(\\hat{\\beta}_{12})} \\sim t_{n-p}\n$$\nwhere $t_{n-p}$ is the Student's t-distribution with $n-p$ degrees of freedom. A two-sided confidence interval for $\\beta_{12}$ at a nominal level of $1-\\alpha$ is constructed as:\n$$\n\\hat{\\beta}_{12} \\pm t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})\n$$\nwhere $t_{1-\\alpha/2, n-p}$ is the upper $(1-\\alpha/2)$ critical value of the $t_{n-p}$ distribution. The length of this interval is $2 \\cdot t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})$. High correlation between predictors in the design matrix $X$ (multicollinearity) inflates the diagonal elements of $(X^T X)^{-1}$, leading to larger standard errors and, consequently, wider confidence intervals. This simulation is designed to demonstrate this effect quantitatively.\n\nThe simulation proceeds as follows for each test case $(n, \\rho)$:\n1.  Initialize a random number generator with the fixed seed $123456$.\n2.  Perform a loop for $R=500$ replicates. In each replicate:\n    a. Generate a sample of $n$ predictor pairs $(x_1, x_2)$ from a bivariate normal distribution with mean $\\mu = [1, 1]^T$ and covariance matrix $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$.\n    b. Construct the $n \\times 4$ design matrix $X$ with columns for the intercept, $x_1$, $x_2$, and the interaction term $x_1 x_2$.\n    c. Generate a sample of $n$ errors $\\varepsilon$ from $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=1$.\n    d. Generate the response vector $y$ using the true model $y = X\\beta_{true} + \\varepsilon$, where $\\beta_{true} = [0, 1, 1, 0.5]^T$.\n    e. Compute the OLS estimate $\\hat{\\beta} = (X^T X)^{-1} X^T y$.\n    f. Calculate the estimated error variance $s^2$.\n    g. Calculate the standard error of the interaction coefficient estimate, $se(\\hat{\\beta}_{12})$, using the formula above.\n    h. Determine the critical value $t_{crit} = t_{1-\\alpha/2, n-4}$ for $\\alpha=0.05$.\n    i. Construct the confidence interval for $\\beta_{12}$ and calculate its length.\n    j. Record a binary indicator ($1$ if the interval contains the true value $\\beta_{12}=0.5$, $0$ otherwise) and the interval length.\n3.  After all replicates, calculate the average of the coverage indicators to obtain the empirical coverage probability and the average of the interval lengths.\n4.  Store the final rounded results for the test case.\n\nThis procedure is repeated for all four test cases, and the results are compiled into the final specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Simulates the effect of predictor correlation on the confidence interval \n    of an interaction term in a linear model.\n    \"\"\"\n    # Define fixed parameters from the problem statement\n    R = 500\n    ALPHA = 0.05\n    BETA_TRUE = np.array([0.0, 1.0, 1.0, 0.5])  # [beta0, beta1, beta2, beta12]\n    SIGMA = 1.0\n    MU = np.array([1.0, 1.0])\n    RANDOM_SEED = 123456\n    \n    # Define test cases (n, rho)\n    test_cases = [\n        (200, 0.0),\n        (200, 0.9),\n        (200, -0.9),\n        (60, 0.99),\n    ]\n\n    all_results = []\n    \n    # Initialize a single random number generator for reproducibility across all cases\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    for n, rho in test_cases:\n        coverage_indicators = []\n        interval_lengths = []\n\n        # Number of model parameters (intercept, x1, x2, x1*x2)\n        p = 4\n        # Degrees of freedom for the t-distribution\n        df = n - p\n        # Critical t-value for a two-sided (1-alpha)% confidence interval\n        t_crit = t.ppf(1 - ALPHA / 2, df)\n\n        for _ in range(R):\n            # Step 1: Generate the data for one replicate\n            \n            # Define the covariance matrix for the predictors (x1, x2)\n            cov_matrix = np.array([[1.0, rho], [rho, 1.0]])\n            \n            # Generate n samples of [x1, x2]\n            predictors = rng.multivariate_normal(MU, cov_matrix, size=n)\n            x1 = predictors[:, 0]\n            x2 = predictors[:, 1]\n            \n            # Construct the design matrix X of size n x p\n            X = np.ones((n, p))\n            X[:, 1] = x1\n            X[:, 2] = x2\n            X[:, 3] = x1 * x2\n            \n            # Generate n error terms\n            epsilon = rng.normal(loc=0, scale=SIGMA, size=n)\n            \n            # Generate the response variable y\n            y = X @ BETA_TRUE + epsilon\n\n            # Step 2: Fit the OLS model and get coefficient estimates\n            \n            # Calculate (X'X)^-1 for standard errors\n            inv_xtx = np.linalg.inv(X.T @ X)\n            \n            # Calculate OLS coefficient estimates: beta_hat = (X'X)^-1 * X'y\n            beta_hat = inv_xtx @ X.T @ y\n            \n            # Step 3: Construct the confidence interval for the interaction term (beta_12)\n            \n            # Calculate residuals\n            residuals = y - X @ beta_hat\n            \n            # Calculate residual sum of squares (RSS)\n            rss = residuals.T @ residuals\n            \n            # Estimate the error variance (s^2)\n            s2 = rss / df\n            \n            # Calculate the standard error of the interaction coefficient estimate\n            # This is the 4th coefficient (index 3)\n            se_beta12 = np.sqrt(s2 * inv_xtx[3, 3])\n            \n            # Calculate the margin of error\n            margin_of_error = t_crit * se_beta12\n            \n            # Calculate the length of the confidence interval\n            length = 2 * margin_of_error\n            interval_lengths.append(length)\n            \n            # Get the point estimate for the interaction coefficient\n            beta12_hat = beta_hat[3]\n            \n            # Define the confidence interval bounds\n            lower_bound = beta12_hat - margin_of_error\n            upper_bound = beta12_hat + margin_of_error\n            \n            # Check if the true parameter value is covered by the interval\n            true_beta12 = BETA_TRUE[3]\n            covered = 1 if (lower_bound <= true_beta12 <= upper_bound) else 0\n            coverage_indicators.append(covered)\n\n        # Calculate empirical coverage and average length for the current test case\n        empirical_coverage = np.mean(coverage_indicators)\n        avg_length = np.mean(interval_lengths)\n        \n        # Format results as specified (rounded to 3 decimal places)\n        result = [round(empirical_coverage, 3), round(avg_length, 3)]\n        all_results.append(result)\n\n    # Final print statement in the exact required format: [[c1,l1],[c2,l2],...]\n    formatted_results = [f\"[{c},{l}]\" for c, l in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The standard linear interaction term, $x_1 x_2$, assumes the interaction effect is consistent across the entire feature space, but is this always a realistic assumption? This exercise challenges you to compare a global polynomial model with a more flexible, local model—a regression tree—on data where the interaction is active only in a specific region. This practical comparison will illuminate the critical trade-off between simple, interpretable global models and adaptive, local models, a central theme in modern statistical learning .",
            "id": "3132277",
            "problem": "You are asked to write a complete, runnable program that compares a global polynomial regression with interaction against a local piecewise-constant regression tree on data where the interaction between predictors exists only in a subset of the input space. Your implementation must follow definitions from empirical risk minimization using squared loss, ordinary least squares (OLS), and greedy sum-of-squares splitting.\n\nConsider predictors $x_1$ and $x_2$ sampled independently from the uniform distribution on $[0,1]$. The latent regression function is\n$$\nf(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1 > \\tau_1, x_2 > \\tau_2\\}\\, c\\, x_1 x_2,\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, $a_1 = 1.0$, $a_2 = -1.0$, $b_1 = 0.5$, $b_2 = -0.5$, and $(\\tau_1,\\tau_2)$ and $c$ are scenario-specific parameters. Observations are generated as\n$$\ny = f(x_1,x_2) + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent Gaussian noise.\n\nYour program must implement and compare the following two estimators:\n\n- Global polynomial with interaction: Fit a linear model using ordinary least squares (OLS) on the feature vector\n$$\n\\phi(x_1,x_2) = \\left[1,\\, x_1,\\, x_2,\\, x_1^2,\\, x_2^2,\\, x_1 x_2\\right].\n$$\nThis model imposes a single global interaction term $x_1 x_2$ across the entire input space.\n\n- Local regression tree: Fit a binary, axis-aligned regression tree that predicts a constant in each leaf. Build the tree by greedily minimizing the empirical sum of squared errors at each split. At a node with sample responses $\\{y_i\\}_{i=1}^n$, the node impurity is\n$$\n\\mathrm{SSE} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2,\\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n$$\nA candidate split on feature $j \\in \\{1,2\\}$ at threshold $t$ partitions the node into left and right children; choose the split that minimizes the sum of child SSE subject to a minimum leaf size constraint. Stop splitting when the maximum depth is reached, no valid split satisfies the minimum leaf size, or no split yields a reduction in SSE. Each leaf predicts the sample mean of $y$ within that leaf.\n\nFor evaluation, for each scenario below, generate a training set of size $n_{\\text{train}}$ and an independent test set of size $n_{\\text{test}} = 20000$. Train both models on the training data. On the test set, compute the out-of-sample mean squared error (MSE) against the noise-free target $f(x_1,x_2)$:\n$$\n\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} \\left(\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i})\\right)^2.\n$$\nThis MSE measures function approximation quality, not noise prediction.\n\nUse the following test suite of scenarios. For each scenario $k$, use the specified random seeds for training and test generation to ensure reproducibility. The tree uses the indicated maximum depth and minimum leaf size.\n\n- Scenario $1$: $n_{\\text{train}} = 400$, $c = 3.0$, $(\\tau_1,\\tau_2) = (0.5, 0.5)$, $\\sigma = 0.1$, tree maximum depth $= 2$, tree minimum leaf size $= 20$, training seed $= 7$, test seed $= 97$.\n- Scenario $2$: $n_{\\text{train}} = 400$, $c = 0.5$, $(\\tau_1,\\tau_2) = (0.6, 0.6)$, $\\sigma = 0.1$, tree maximum depth $= 2$, tree minimum leaf size $= 20$, training seed $= 8$, test seed $= 98$.\n- Scenario $3$: $n_{\\text{train}} = 400$, $c = 3.0$, $(\\tau_1,\\tau_2) = (0.5, 0.5)$, $\\sigma = 1.0$, tree maximum depth $= 2$, tree minimum leaf size $= 40$, training seed $= 9$, test seed $= 99$.\n- Scenario $4$: $n_{\\text{train}} = 800$, $c = 5.0$, $(\\tau_1,\\tau_2) = (0.85, 0.85)$, $\\sigma = 0.1$, tree maximum depth $= 3$, tree minimum leaf size $= 10$, training seed $= 10$, test seed $= 100$.\n\nYour program must, for each scenario in order, output two floating-point numbers: first the MSE of the global polynomial with interaction, then the MSE of the local regression tree. Concatenate all scenarios’ results into a single list.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers enclosed in square brackets, with each number printed to exactly $6$ decimal places, in the order\n$$\n[\\mathrm{MSE}^{\\text{global}}_1,\\mathrm{MSE}^{\\text{tree}}_1,\\mathrm{MSE}^{\\text{global}}_2,\\mathrm{MSE}^{\\text{tree}}_2,\\mathrm{MSE}^{\\text{global}}_3,\\mathrm{MSE}^{\\text{tree}}_3,\\mathrm{MSE}^{\\text{global}}_4,\\mathrm{MSE}^{\\text{tree}}_4].\n$$\nNo other text should be printed.",
            "solution": "The problem requires a comparison between two regression models—a global polynomial model and a local regression tree—on a simulated dataset where an interaction term is present only in a specific sub-region of the feature space. The comparison is based on the out-of-sample Mean Squared Error (MSE) against the true, noise-free data generating function.\n\n### Step 1: Problem Validation\n\nThe first step is to validate the problem statement.\n\n**Givens Extracted:**\n- **Predictors:** $x_1, x_2$ are sampled independently from a $\\text{Uniform}(0,1)$ distribution.\n- **Latent Function:** $f(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1 > \\tau_1, x_2 > \\tau_2\\}\\, c\\, x_1 x_2$.\n- **Constants:** $a_1 = 1.0$, $a_2 = -1.0$, $b_1 = 0.5$, $b_2 = -0.5$.\n- **Noise Model:** $y = f(x_1,x_2) + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n- **Estimator 1 (Global Polynomial):** Ordinary Least Squares (OLS) fit to the feature vector $\\phi(x_1,x_2) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]$.\n- **Estimator 2 (Local Regression Tree):** A binary, axis-aligned tree built by greedily minimizing the sum of squared errors (SSE) at each split, subject to a maximum depth and a minimum leaf size. Leaf nodes predict the sample mean of responses.\n- **Tree Impurity:** $\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\bar{y})^2$.\n- **Evaluation Metric:** Out-of-sample MSE on a test set of size $n_{\\text{test}} = 20000$, calculated as $\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i}))^2$.\n- **Scenarios:** Four scenarios are defined with specific parameters for $n_{\\text{train}}$, $c$, $(\\tau_1,\\tau_2)$, $\\sigma$, tree maximum depth, tree minimum leaf size, and random seeds for training and testing data generation.\n\n**Validation Against Criteria:**\n- **Scientific Grounding:** The problem is firmly rooted in standard statistical learning theory, employing well-established concepts like OLS, regression trees, empirical risk minimization with squared error loss, and simulation-based model comparison. All definitions are standard and mathematically sound.\n- **Well-Posedness:** The problem provides a complete set of instructions. The data generation process, model structures, fitting procedures, and evaluation metrics are all explicitly defined. The use of random seeds ensures that the numerical results are reproducible. A unique and meaningful solution exists.\n- **Objectivity:** The problem is stated in precise, formal language, free of ambiguity or subjective claims.\n- **Completeness and Consistency:** All necessary parameters for each of the four scenarios are provided, and there are no internal contradictions.\n- **Relevance:** The problem directly addresses the topic of modeling interactions and non-linearities, a central theme in statistical learning. It poses a meaningful question about the trade-offs between a global model that may be misspecified and a local, more flexible model that might overfit or be better at capturing local phenomena.\n\n**Verdict:** The problem is valid, scientifically sound, well-posed, and complete. We may proceed with the solution.\n\n### Step 2: Solution Derivation and Algorithmic Design\n\nThe core of the task is to implement the data generation process, the two specified models, and the evaluation procedure for each given scenario.\n\n**Data Generation**\nFor each scenario, we generate a training set $(X_{\\text{train}}, y_{\\text{train}})$ of size $n_{\\text{train}}$ and a test set $(X_{\\text{test}}, y_{\\text{test}})$ of size $n_{\\text{test}} = 20000$. The predictors $x_1, x_2$ are drawn from $\\text{Uniform}(0,1)$. The true function values $f(x_1, x_2)$ are computed, and the observed responses $y$ are generated by adding Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$. The random number generator is seeded as specified for reproducibility.\n\n**Model 1: Global Polynomial with Interaction**\nThis model assumes the relationship between the predictors and the response can be approximated by a single polynomial function over the entire feature space:\n$$\n\\hat{f}(x_1, x_2) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_1^2 + \\hat{\\beta}_4 x_2^2 + \\hat{\\beta}_5 x_1 x_2\n$$\nThe coefficients $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\dots, \\hat{\\beta}_5]^T$ are estimated using OLS. Given the training data $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$, we form the design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$:\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1 & x_{1,1} & x_{2,1} & x_{1,1}^2 & x_{2,1}^2 & x_{1,1}x_{2,1} \\\\\n1 & x_{1,2} & x_{2,2} & x_{1,2}^2 & x_{2,2}^2 & x_{1,2}x_{2,2} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{1,n_{\\text{train}}} & x_{2,n_{\\text{train}}} & x_{1,n_{\\text{train}}}^2 & x_{2,n_{\\text{train}}}^2 & x_{1,n_{\\text{train}}}x_{2,n_{\\text{train}}}\n\\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n_{\\text{train}}} \\end{pmatrix}\n$$\nThe OLS solution finds the $\\hat{\\boldsymbol{\\beta}}$ that minimizes the residual sum of squares $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$. This solution is given by $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$. For numerical stability, this linear system is preferably solved using methods like QR decomposition or SVD, which is handled by library functions like `scipy.linalg.lstsq`. Once $\\hat{\\boldsymbol{\\beta}}$ is found, predictions for new data points in the test set are made by $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$.\n\n**Model 2: Local Regression Tree**\nThis model partitions the feature space $[0,1] \\times [0,1]$ into disjoint rectangular regions and fits a simple constant model (the mean of the responses) in each region. The partitioning is done greedily and recursively.\n\n- **Node Representation:** A tree is composed of nodes. An internal node specifies a split (a feature index $j \\in \\{1,2\\}$ and a threshold $t$), and has two children (left and right). A leaf node has no children and stores a prediction value (the sample mean of $y$ for the training points that fall into its region).\n\n- **Splitting an Internal Node:** At a given node containing a subset of training data, we search for the best split. A split is defined by a feature $j$ and a value $t$. It partitions the data into a left set $\\{ \\mathbf{x}_i \\mid x_{i,j} \\le t \\}$ and a right set $\\{ \\mathbf{x}_i \\mid x_{i,j} > t \\}$. The quality of a split is measured by the reduction in the total sum of squared errors:\n$$\n\\text{Gain}(j, t) = \\text{SSE}_{\\text{parent}} - (\\text{SSE}_{\\text{left}} + \\text{SSE}_{\\text{right}})\n$$\nwhere $\\text{SSE}_{\\text{region}} = \\sum_{i \\in \\text{region}} (y_i - \\bar{y}_{\\text{region}})^2$. We search over all features $j \\in \\{1,2\\}$ and all valid split points $t$ to find the combination that maximizes this gain. A split is valid only if it respects the minimum leaf size constraint: both the left and right children must contain at least the minimum number of samples. The potential split points $t$ for a feature can be efficiently chosen as the midpoints between consecutive unique sorted values of that feature.\n\n- **Recursive Tree Construction:** The algorithm proceeds as follows:\n  1. Start with the root node containing all training data.\n  2. For a node, check stopping conditions:\n     a. The current depth equals the maximum allowed depth.\n     b. The number of samples in the node is less than twice the minimum leaf size (since any split would violate the constraint).\n     c. All response values $y_i$ in the node are identical (SSE is $0$, no further improvement is possible).\n  3. If a stopping condition is met, create a leaf node and store the mean of the responses as its prediction value.\n  4. Otherwise, find the best split $(j^*, t^*)$ by maximizing the SSE gain.\n  5. If no split provides a positive gain, or no valid split exists, create a leaf node.\n  6. Otherwise, create an internal node with the split $(j^*, t^*)$. Partition the data and recursively build the left and right subtrees.\n\n- **Prediction:** To predict for a new point $\\mathbf{x}_{\\text{new}}$, we traverse the tree from the root. At each internal node, we compare the relevant feature of $\\mathbf{x}_{\\text{new}}$ with the node's threshold to decide whether to go left or right, until we reach a leaf node. The prediction is the value stored in that leaf.\n\n**Evaluation**\nFor each scenario, after training both models on the training data, we generate predictions $\\hat{f}_{\\text{poly}}$ and $\\hat{f}_{\\text{tree}}$ for the features in the large test set. The performance is measured by the MSE against the true function values $f_{\\text{test}}$ (without noise):\n$$\n\\text{MSE}_{\\text{poly}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{poly}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\n$$\n\\text{MSE}_{\\text{tree}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{tree}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\nThese two values are computed for each of the four scenarios and reported.\n\nThis systematic approach ensures that all requirements of the problem are met, providing a rigorous comparison between a global parametric model and a local non-parametric model. The implementation will follow these principles precisely.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the data generation, model training,\n    evaluation, and printing of results for all specified scenarios.\n    \"\"\"\n\n    # --- Helper Classes and Functions ---\n\n    class Node:\n        \"\"\"Represents a node in the regression tree.\"\"\"\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n            self.feature_index = feature_index  # Feature to split on\n            self.threshold = threshold          # Threshold for the split\n            self.left = left                    # Left subtree (for values <= threshold)\n            self.right = right                  # Right subtree (for values > threshold)\n            self.value = value                  # Prediction value if it's a leaf node\n\n    class DecisionTree:\n        \"\"\"\n        A regression tree that uses greedy sum-of-squares splitting.\n        \"\"\"\n        def __init__(self, max_depth=2, min_samples_leaf=1):\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.root = None\n\n        def fit(self, X, y):\n            \"\"\"Build the regression tree from training data.\"\"\"\n            self.root = self._grow_tree(X, y)\n\n        def _grow_tree(self, X, y, depth=0):\n            \"\"\"Recursively grow the tree.\"\"\"\n            n_samples, n_features = X.shape\n            \n            # Check stopping criteria\n            is_pure = len(np.unique(y)) == 1\n            if (depth >= self.max_depth or\n                    n_samples < 2 * self.min_samples_leaf or\n                    is_pure):\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            best_split = self._find_best_split(X, y, n_samples, n_features)\n\n            if best_split['gain'] <= 0:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n            \n            left_indices = X[:, best_split['feature_index']] <= best_split['threshold']\n            right_indices = ~left_indices\n            \n            left_subtree = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n            right_subtree = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n            \n            return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree)\n\n        def _find_best_split(self, X, y, n_samples, n_features):\n            \"\"\"Find the best feature and threshold to split on.\"\"\"\n            best_split = {'gain': -1}\n            current_sse = self._calculate_sse(y)\n\n            for feat_idx in range(n_features):\n                thresholds = np.unique(X[:, feat_idx])\n                \n                # Use midpoints between unique sorted values as potential splits\n                if len(thresholds) > 1:\n                    test_thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n                else: \n                    continue\n\n                for threshold in test_thresholds:\n                    left_indices = X[:, feat_idx] <= threshold\n                    right_indices = ~left_indices\n                    \n                    if np.sum(left_indices) < self.min_samples_leaf or np.sum(right_indices) < self.min_samples_leaf:\n                        continue\n                        \n                    y_left, y_right = y[left_indices], y[right_indices]\n                    \n                    sse_left = self._calculate_sse(y_left)\n                    sse_right = self._calculate_sse(y_right)\n                    \n                    total_child_sse = sse_left + sse_right\n                    gain = current_sse - total_child_sse\n\n                    if gain > best_split['gain']:\n                        best_split = {\n                            'feature_index': feat_idx,\n                            'threshold': threshold,\n                            'gain': gain\n                        }\n            return best_split\n\n        @staticmethod\n        def _calculate_sse(y):\n            \"\"\"Calculate Sum of Squared Errors.\"\"\"\n            if len(y) == 0:\n                return 0\n            mean = np.mean(y)\n            return np.sum((y - mean) ** 2)\n\n        def predict(self, X):\n            \"\"\"Make predictions for a set of samples.\"\"\"\n            return np.array([self._predict_one(x, self.root) for x in X])\n\n        def _predict_one(self, x, node):\n            \"\"\"Traverse the tree to predict for a single sample.\"\"\"\n            if node.value is not None:\n                return node.value\n            if x[node.feature_index] <= node.threshold:\n                return self._predict_one(x, node.left)\n            else:\n                return self._predict_one(x, node.right)\n\n    def true_f(x1, x2, c, tau1, tau2):\n        \"\"\"Computes the latent noise-free function value.\"\"\"\n        a1, a2, b1, b2 = 1.0, -1.0, 0.5, -0.5\n        interaction_term = c * x1 * x2 * ((x1 > tau1) & (x2 > tau2))\n        return a1 * x1 + a2 * x2 + b1 * x1**2 + b2 * x2**2 + interaction_term\n\n    def generate_data(n, seed, c, tau1, tau2, sigma):\n        \"\"\"Generates training or test data.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.uniform(0, 1, size=(n, 2))\n        x1, x2 = X[:, 0], X[:, 1]\n        \n        f_vals = true_f(x1, x2, c, tau1, tau2)\n        noise = rng.normal(0, sigma, size=n)\n        y = f_vals + noise\n        \n        return X, y, f_vals\n\n    def fit_predict_polynomial(X_train, y_train, X_test):\n        \"\"\"Fits OLS polynomial model and predicts on test data.\"\"\"\n        # Construct design matrix for training\n        X_design_train = np.c_[\n            np.ones(X_train.shape[0]),\n            X_train[:, 0],\n            X_train[:, 1],\n            X_train[:, 0]**2,\n            X_train[:, 1]**2,\n            X_train[:, 0] * X_train[:, 1]\n        ]\n        \n        # Solve for coefficients using least squares\n        beta, _, _, _ = linalg.lstsq(X_design_train, y_train)\n        \n        # Construct design matrix for testing\n        X_design_test = np.c_[\n            np.ones(X_test.shape[0]),\n            X_test[:, 0],\n            X_test[:, 1],\n            X_test[:, 0]**2,\n            X_test[:, 1]**2,\n            X_test[:, 0] * X_test[:, 1]\n        ]\n        \n        # Make predictions\n        y_pred = X_design_test @ beta\n        return y_pred\n    \n    # --- Scenarios and Main Execution Logic ---\n\n    test_cases = [\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 7, 'test_seed': 97},\n        {'n_train': 400, 'c': 0.5, 'tau1': 0.6, 'tau2': 0.6, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 8, 'test_seed': 98},\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 1.0, 'depth': 2, 'min_leaf': 40, 'train_seed': 9, 'test_seed': 99},\n        {'n_train': 800, 'c': 5.0, 'tau1': 0.85, 'tau2': 0.85, 'sigma': 0.1, 'depth': 3, 'min_leaf': 10, 'train_seed': 10, 'test_seed': 100},\n    ]\n\n    all_results = []\n    n_test = 20000\n\n    for case in test_cases:\n        # Generate data\n        X_train, y_train, _ = generate_data(case['n_train'], case['train_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n        X_test, _, f_test = generate_data(n_test, case['test_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n\n        # Model 1: Global Polynomial\n        poly_preds = fit_predict_polynomial(X_train, y_train, X_test)\n        mse_poly = np.mean((poly_preds - f_test)**2)\n        \n        # Model 2: Local Regression Tree\n        tree = DecisionTree(max_depth=case['depth'], min_samples_leaf=case['min_leaf'])\n        tree.fit(X_train, y_train)\n        tree_preds = tree.predict(X_test)\n        mse_tree = np.mean((tree_preds - f_test)**2)\n\n        all_results.extend([mse_poly, mse_tree])\n    \n    # Format and print final results\n    print(f\"[{','.join([f'{r:.6f}' for r in all_results])}]\")\n\nsolve()\n\n```"
        }
    ]
}