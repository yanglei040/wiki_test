{
    "hands_on_practices": [
        {
            "introduction": "Once a linear model containing qualitative predictors is fitted, a primary task is to interpret what the model has learned. This practice moves beyond a simple inspection of coefficients to quantify the practical impact of categorical variables. You will implement a 'category-switch' experiment, a form of counterfactual analysis, to calculate precisely how the model's prediction changes when an observation's category is hypothetically altered, providing a clear and powerful method for understanding your model's behavior, especially in the presence of interactions. ",
            "id": "3164642",
            "problem": "You are given a linear regression model with qualitative predictors encoded using reference-cell (also called one-hot with intercept) dummy variables. Let the predicted outcome be denoted by $\\hat{y}(\\mathbf{x}, \\mathbf{z})$, where $\\mathbf{x} \\in \\mathbb{R}^p$ is a vector of quantitative features and $\\mathbf{z}$ is a vector of categorical levels, one level per categorical variable. For each categorical variable $c$ with level set $\\mathcal{L}_c$, a designated baseline level $\\text{baseline}_c \\in \\mathcal{L}_c$ is not explicitly encoded; only indicators for levels in $\\mathcal{L}_c \\setminus \\{\\text{baseline}_c\\}$ enter the model. Optionally, interactions between categorical levels and quantitative features may be included.\n\nFormally, the model has the form\n$$\n\\hat{y}(\\mathbf{x}, \\mathbf{z}) \\;=\\; \\beta_0 \\;+\\; \\sum_{j=1}^{p} \\beta_j x_j \\;+\\; \\sum_{c \\in \\mathcal{C}} \\sum_{l \\in \\mathcal{L}_c \\setminus \\{\\text{baseline}_c\\}} \\gamma_{c,l}\\,\\mathbf{1}\\{z_c = l\\}\n\\;+\\; \\sum_{(c,j)\\in \\mathcal{I}} \\sum_{l \\in \\mathcal{L}_c \\setminus \\{\\text{baseline}_c\\}} \\delta_{c,l,j}\\,\\mathbf{1}\\{z_c = l\\}\\,x_j,\n$$\nwhere $\\beta_0 \\in \\mathbb{R}$ is the intercept, $\\beta_j \\in \\mathbb{R}$ are coefficients for the quantitative features, $\\gamma_{c,l} \\in \\mathbb{R}$ are main-effect dummy coefficients for categorical variable $c$ at level $l$, and $\\delta_{c,l,j} \\in \\mathbb{R}$ are interaction coefficients between level $l$ of categorical variable $c$ and quantitative feature $x_j$. The set $\\mathcal{I}$ lists the categorical–quantitative interaction pairs present in the model.\n\nFor a given observation $(\\mathbf{x}, \\mathbf{z})$, consider a counterfactual in which a single categorical variable $c^\\star$ changes its level from $a$ to $b$, producing $\\mathbf{z}_{c^\\star \\leftarrow b}$. Define the raw category-switch effect as\n$$\n\\Delta_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z})\n\\;=\\;\n\\hat{y}(\\mathbf{x}, \\mathbf{z}_{c^\\star \\leftarrow b})\n-\n\\hat{y}(\\mathbf{x}, \\mathbf{z}),\n$$\nand the standardized effect size, given a positive scalar $\\sigma > 0$, as\n$$\nE^{\\mathrm{std}}_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z}) = \\frac{\\Delta_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z})}{\\sigma}.\n$$\n\nYour task is to write a program that, for each of the following test cases, computes $E^{\\mathrm{std}}_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z})$ according to the model specification supplied in the test case. All test cases use reference-cell coding with an intercept; the baseline level of each categorical variable is explicitly specified. If a categorical level equals its baseline, its main-effect contribution is $0$; likewise for interactions, absent or baseline levels contribute $0$.\n\nTest suite:\n\n- Test case $1$:\n  - Quantitative features: one feature named $x$ with coefficient $\\beta_x = 1.5$ and intercept $\\beta_0 = 2.0$.\n  - Categorical variables: one variable named color with levels $\\{\\text{red}, \\text{blue}, \\text{green}\\}$, baseline level red. Main-effect coefficients: $\\gamma_{\\text{color},\\text{blue}} = 0.8$, $\\gamma_{\\text{color},\\text{green}} = -0.5$. No interactions.\n  - Observation: $x = 3.0$, color $=$ red. Switch color to blue. Use $\\sigma = 0.4$.\n\n- Test case $2$:\n  - Quantitative features: one feature named $x$ with coefficient $\\beta_x = -1.0$ and intercept $\\beta_0 = 0.0$.\n  - Categorical variables: one variable named color with levels $\\{\\text{red}, \\text{blue}, \\text{green}\\}$, baseline level red. Main-effect coefficients: $\\gamma_{\\text{color},\\text{blue}} = 1.2$, $\\gamma_{\\text{color},\\text{green}} = -0.3$. No interactions.\n  - Observation: $x = -5.0$, color $=$ green. Switch color to green. Use $\\sigma = 1.0$.\n\n- Test case $3$:\n  - Quantitative features: one feature named $p$ with coefficient $\\beta_p = -2.0$ and intercept $\\beta_0 = 0.0$.\n  - Categorical variables:\n    - brand with levels $\\{\\text{A}, \\text{B}\\}$, baseline A; $\\gamma_{\\text{brand},\\text{B}} = 1.2$.\n    - season with levels $\\{\\text{winter}, \\text{spring}, \\text{summer}, \\text{autumn}\\}$, baseline winter; $\\gamma_{\\text{season},\\text{spring}} = 0.5$, $\\gamma_{\\text{season},\\text{summer}} = 1.0$, $\\gamma_{\\text{season},\\text{autumn}} = 0.2$.\n    - No interactions.\n  - Observation: $p = 0.75$, brand $=$ B, season $=$ autumn. Switch season to summer. Use $\\sigma = 0.5$.\n\n- Test case $4$:\n  - Quantitative features: one feature named $n\\_users$ with coefficient $\\beta_{n\\_users} = 0.2$ and intercept $\\beta_0 = 0.0$.\n  - Categorical variables: one variable named tier with levels $\\{\\text{basic}, \\text{pro}, \\text{enterprise}\\}$, baseline basic. Main-effect coefficients: $\\gamma_{\\text{tier},\\text{pro}} = 0.3$, $\\gamma_{\\text{tier},\\text{enterprise}} = 1.0$.\n  - Interactions present: between tier and $n\\_users$, with $\\delta_{\\text{tier},\\text{pro},\\,n\\_users} = 0.1$, $\\delta_{\\text{tier},\\text{enterprise},\\,n\\_users} = 0.05$.\n  - Observation: $n\\_users = 10$, tier $=$ pro. Switch tier to enterprise. Use $\\sigma = 0.2$.\n\n- Test case $5$:\n  - Quantitative features: two features $x_1$ and $x_2$ with coefficients $\\beta_{x_1} = 1.0$, $\\beta_{x_2} = -0.5$, and intercept $\\beta_0 = 5.0$.\n  - Categorical variables:\n    - brand with levels $\\{\\text{A}, \\text{B}\\}$, baseline A; $\\gamma_{\\text{brand},\\text{B}} = -0.7$.\n    - region with levels $\\{\\text{north}, \\text{south}, \\text{east}\\}$, baseline north; $\\gamma_{\\text{region},\\text{south}} = 0.4$, $\\gamma_{\\text{region},\\text{east}} = -0.2$.\n    - No interactions.\n  - Observation: $x_1 = 4.0$, $x_2 = -2.0$, brand $=$ B, region $=$ south. Switch brand to A. Use $\\sigma = 0.7$.\n\nYour program must compute the standardized effect size $E^{\\mathrm{std}}_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z})$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$). No physical units are involved. Angles are not involved. Percentages are not involved. The output entries must be real numbers (floating-point) in the order of the test cases $1$ through $5$.",
            "solution": "The problem asks for the computation of a standardized effect size, $E^{\\mathrm{std}}$, associated with a counterfactual change in a single categorical predictor within a linear regression model. The model incorporates quantitative features, qualitative (categorical) features, and their interactions.\n\nFirst, we must formalize the calculation of the raw category-switch effect, $\\Delta_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z})$. The model is given by:\n$$\n\\hat{y}(\\mathbf{x}, \\mathbf{z}) \\;=\\; \\beta_0 \\;+\\; \\sum_{j=1}^{p} \\beta_j x_j \\;+\\; \\sum_{c \\in \\mathcal{C}} \\sum_{l \\in \\mathcal{L}_c \\setminus \\{\\text{baseline}_c\\}} \\gamma_{c,l}\\,\\mathbf{1}\\{z_c = l\\}\n\\;+\\; \\sum_{(c,j)\\in \\mathcal{I}} \\sum_{l \\in \\mathcal{L}_c \\setminus \\{\\text{baseline}_c\\}} \\delta_{c,l,j}\\,\\mathbf{1}\\{z_c = l\\}\\,x_j\n$$\nThe effect is defined as the difference in predicted outcomes:\n$$\n\\Delta_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z}) \\;=\\; \\hat{y}(\\mathbf{x}, \\mathbf{z}_{c^\\star \\leftarrow b}) - \\hat{y}(\\mathbf{x}, \\mathbf{z})\n$$\nwhere $\\mathbf{z}_{c^\\star \\leftarrow b}$ is the feature vector identical to $\\mathbf{z}$ except that the categorical variable $c^\\star$ has its level changed from $a$ to $b$.\n\nWhen we compute this difference, all terms in the model expression for $\\hat{y}$ that do not depend on the level of $c^\\star$ will cancel out. This includes the intercept $\\beta_0$, the main effects of all quantitative predictors $\\sum_{j=1}^{p} \\beta_j x_j$, and the main and interaction effects of all other categorical variables $c \\in \\mathcal{C}$ where $c \\neq c^\\star$.\n\nThe only parts that do not cancel are those related to $c^\\star$. Let us define the total contribution of a level $l$ of a categorical variable $c$ to the prediction for a given quantitative feature vector $\\mathbf{x}$. Let's denote this contribution by $f(c, l, \\mathbf{x})$. From the model definition, this contribution is:\n$$\nf(c, l, \\mathbf{x}) = \\left( \\sum_{l' \\in \\mathcal{L}_c \\setminus \\{\\text{baseline}_c\\}} \\gamma_{c,l'}\\,\\mathbf{1}\\{l = l'\\} \\right) + \\left( \\sum_{(c,j)\\in \\mathcal{I}} \\sum_{l' \\in \\mathcal{L}_c \\setminus \\{\\text{baseline}_c\\}} \\delta_{c,l',j}\\,\\mathbf{1}\\{l = l'\\}\\,x_j \\right)\n$$\nThis expression can be simplified. If level $l$ is the baseline level, $\\text{baseline}_c$, then all indicator functions $\\mathbf{1}\\{l=l'\\}$ are zero, and thus $f(c, \\text{baseline}_c, \\mathbf{x}) = 0$. If $l$ is not a baseline level, then the expression simplifies to:\n$$\nf(c, l, \\mathbf{x}) = \\gamma_{c,l} + \\sum_{j \\text{ s.t. } (c,j) \\in \\mathcal{I}} \\delta_{c,l,j}\\,x_j \\quad \\text{for } l \\neq \\text{baseline}_c\n$$\nWe can unify these cases by adopting the convention that coefficients for baseline levels are zero: $\\gamma_{c,\\text{baseline}_c} = 0$ and $\\delta_{c,\\text{baseline}_c,j} = 0$ for all $c, j$.\n\nThe raw effect $\\Delta_{c^\\star:a \\to b}$ is the difference between the contributions of the new level $b$ and the original level $a$:\n$$\n\\Delta_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z}) = f(c^\\star, b, \\mathbf{x}) - f(c^\\star, a, \\mathbf{x})\n$$\n$$\n\\Delta_{c^\\star:a \\to b} = \\left( \\gamma_{c^\\star,b} + \\sum_{j \\text{ s.t. } (c^\\star,j) \\in \\mathcal{I}} \\delta_{c^\\star,b,j}\\,x_j \\right) - \\left( \\gamma_{c^\\star,a} + \\sum_{j \\text{ s.t. } (c^\\star,j) \\in \\mathcal{I}} \\delta_{c^\\star,a,j}\\,x_j \\right)\n$$\nFinally, the standardized effect size is obtained by dividing by $\\sigma$:\n$$\nE^{\\mathrm{std}}_{c^\\star:a \\to b}(\\mathbf{x}, \\mathbf{z}) = \\frac{\\Delta_{c^\\star:a \\to b}}{\\sigma}\n$$\nWe now apply this framework to each test case.\n\nTest case 1:\n- Switch $c^\\star=\\text{color}$ from $a=\\text{red}$ to $b=\\text{blue}$.\n- The level $\\text{red}$ is the baseline, so $\\gamma_{\\text{color,red}}=0$. The coefficient for $\\text{blue}$ is $\\gamma_{\\text{color,blue}} = 0.8$.\n- There are no interactions, so all $\\delta$ coefficients are $0$.\n- $\\Delta_{\\text{color}:\\text{red} \\to \\text{blue}} = (\\gamma_{\\text{color,blue}}) - (\\gamma_{\\text{color,red}}) = 0.8 - 0 = 0.8$.\n- Given $\\sigma = 0.4$, the standardized effect is $E^{\\mathrm{std}} = 0.8 / 0.4 = 2.0$.\n\nTest case 2:\n- Switch $c^\\star=\\text{color}$ from $a=\\text{green}$ to $b=\\text{green}$.\n- Since the starting level is the same as the ending level, the change in prediction must be zero.\n- $\\Delta_{\\text{color}:\\text{green} \\to \\text{green}} = \\hat{y}(\\dots, \\text{color}=\\text{green}) - \\hat{y}(\\dots, \\text{color}=\\text{green}) = 0$.\n- With $\\sigma = 1.0$, the standardized effect is $E^{\\mathrm{std}} = 0 / 1.0 = 0.0$.\n\nTest case 3:\n- Switch $c^\\star=\\text{season}$ from $a=\\text{autumn}$ to $b=\\text{summer}$.\n- The variable `brand` does not change, so its contribution cancels. There are no interactions.\n- The baseline for `season` is `winter`. Neither `autumn` nor `summer` is the baseline.\n- Coefficients are $\\gamma_{\\text{season,autumn}} = 0.2$ and $\\gamma_{\\text{season,summer}} = 1.0$.\n- $\\Delta_{\\text{season}:\\text{autumn} \\to \\text{summer}} = \\gamma_{\\text{season,summer}} - \\gamma_{\\text{season,autumn}} = 1.0 - 0.2 = 0.8$.\n- With $\\sigma = 0.5$, the standardized effect is $E^{\\mathrm{std}} = 0.8 / 0.5 = 1.6$.\n\nTest case 4:\n- Switch $c^\\star=\\text{tier}$ from $a=\\text{pro}$ to $b=\\text{enterprise}$.\n- This case includes interactions between `tier` and $n\\_users$.\n- The observation is $n\\_users = 10$. The baseline for `tier` is `basic`.\n- The change involves two non-baseline levels, `pro` and `enterprise`.\n- $\\Delta_{\\text{tier}:\\text{pro} \\to \\text{enterprise}} = f(\\text{tier}, \\text{enterprise}, \\{n\\_users: 10\\}) - f(\\text{tier}, \\text{pro}, \\{n\\_users: 10\\})$.\n- $f(\\text{tier}, \\text{enterprise}, \\{n\\_users: 10\\}) = \\gamma_{\\text{tier,enterprise}} + \\delta_{\\text{tier,enterprise},n\\_users} \\times n\\_users = 1.0 + 0.05 \\times 10 = 1.5$.\n- $f(\\text{tier}, \\text{pro}, \\{n\\_users: 10\\}) = \\gamma_{\\text{tier,pro}} + \\delta_{\\text{tier,pro},n\\_users} \\times n\\_users = 0.3 + 0.1 \\times 10 = 1.3$.\n- $\\Delta = 1.5 - 1.3 = 0.2$.\n- With $\\sigma = 0.2$, the standardized effect is $E^{\\mathrm{std}} = 0.2 / 0.2 = 1.0$.\n\nTest case 5:\n- Switch $c^\\star=\\text{brand}$ from $a=\\text{B}$ to $b=\\text{A}$.\n- The level `A` is the baseline for `brand`, so its effective coefficient is $0$. The level `B` has coefficient $\\gamma_{\\text{brand,B}} = -0.7$.\n- There are no interactions. The state of other variables (`region`, $x_1$, $x_2$) is irrelevant as their contributions cancel.\n- $\\Delta_{\\text{brand}:\\text{B} \\to \\text{A}} = \\gamma_{\\text{brand,A}} - \\gamma_{\\text{brand,B}} = 0 - (-0.7) = 0.7$.\n- With $\\sigma = 0.7$, the standardized effect is $E^{\\mathrm{std}} = 0.7 / 0.7 = 1.0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the standardized effect size for a counterfactual category switch\n    in a linear regression model for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        {\n            \"c_star\": \"color\",\n            \"from_level\": \"red\",\n            \"to_level\": \"blue\",\n            \"x_vals\": {\"x\": 3.0},\n            \"coeffs\": {\n                \"gamma\": {\"color\": {\"blue\": 0.8, \"green\": -0.5}}\n            },\n            \"baselines\": {\"color\": \"red\"},\n            \"sigma\": 0.4\n        },\n        # Test case 2\n        {\n            \"c_star\": \"color\",\n            \"from_level\": \"green\",\n            \"to_level\": \"green\",\n            \"x_vals\": {\"x\": -5.0},\n            \"coeffs\": {\n                \"gamma\": {\"color\": {\"blue\": 1.2, \"green\": -0.3}}\n            },\n            \"baselines\": {\"color\": \"red\"},\n            \"sigma\": 1.0\n        },\n        # Test case 3\n        {\n            \"c_star\": \"season\",\n            \"from_level\": \"autumn\",\n            \"to_level\": \"summer\",\n            \"x_vals\": {\"p\": 0.75},\n            \"coeffs\": {\n                \"gamma\": {\n                    \"brand\": {\"B\": 1.2},\n                    \"season\": {\"spring\": 0.5, \"summer\": 1.0, \"autumn\": 0.2}\n                }\n            },\n            \"baselines\": {\"brand\": \"A\", \"season\": \"winter\"},\n            \"sigma\": 0.5\n        },\n        # Test case 4\n        {\n            \"c_star\": \"tier\",\n            \"from_level\": \"pro\",\n            \"to_level\": \"enterprise\",\n            \"x_vals\": {\"n_users\": 10},\n            \"coeffs\": {\n                \"gamma\": {\"tier\": {\"pro\": 0.3, \"enterprise\": 1.0}},\n                \"delta\": {\n                    \"tier\": {\n                        \"pro\": {\"n_users\": 0.1},\n                        \"enterprise\": {\"n_users\": 0.05}\n                    }\n                }\n            },\n            \"baselines\": {\"tier\": \"basic\"},\n            \"sigma\": 0.2\n        },\n        # Test case 5\n        {\n            \"c_star\": \"brand\",\n            \"from_level\": \"B\",\n            \"to_level\": \"A\",\n            \"x_vals\": {\"x1\": 4.0, \"x2\": -2.0},\n            \"coeffs\": {\n                \"gamma\": {\n                    \"brand\": {\"B\": -0.7},\n                    \"region\": {\"south\": 0.4, \"east\": -0.2}\n                }\n            },\n            \"baselines\": {\"brand\": \"A\", \"region\": \"north\"},\n            \"sigma\": 0.7\n        }\n    ]\n\n    def get_level_contribution(c_var, level, x_vals, coeffs, baselines):\n        \"\"\"\n        Calculates the total contribution of a specific categorical level to the\n        prediction, including main and interaction effects.\n        \"\"\"\n        if level == baselines[c_var]:\n            return 0.0\n\n        # Main effect (gamma coefficient)\n        gamma = coeffs.get(\"gamma\", {}).get(c_var, {}).get(level, 0.0)\n\n        # Sum of interaction effects (delta coefficients)\n        delta_sum = 0.0\n        # Check if interactions are defined for the variable and level\n        delta_coeffs_for_level = coeffs.get(\"delta\", {}).get(c_var, {}).get(level)\n        if delta_coeffs_for_level:\n            for x_var, x_val in x_vals.items():\n                if x_var in delta_coeffs_for_level:\n                    delta_sum += delta_coeffs_for_level[x_var] * x_val\n        \n        return gamma + delta_sum\n\n    results = []\n    for case in test_cases:\n        c_star = case[\"c_star\"]\n        from_level = case[\"from_level\"]\n        to_level = case[\"to_level\"]\n        x_vals = case[\"x_vals\"]\n        coeffs = case[\"coeffs\"]\n        baselines = case[\"baselines\"]\n        sigma = case[\"sigma\"]\n\n        # Calculate contribution from the original level\n        contrib_from = get_level_contribution(\n            c_star, from_level, x_vals, coeffs, baselines\n        )\n        \n        # Calculate contribution from the new (counterfactual) level\n        contrib_to = get_level_contribution(\n            c_star, to_level, x_vals, coeffs, baselines\n        )\n\n        # Raw effect is the difference in contributions\n        raw_effect = contrib_to - contrib_from\n        \n        # Standardized effect\n        std_effect = raw_effect / sigma\n        results.append(std_effect)\n\n    # Format the output as specified\n    formatted_results = [f\"{r:.16g}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Choosing the best model from a set of candidates is a cornerstone of statistical learning, but standard methods can be misleading when qualitative predictors with many levels are involved. A predictor with $L$ categories adds $L-1$ parameters to your model, a complexity not captured by simply counting the number of predictors. This exercise challenges you to derive a more robust information criterion, rooted in information theory, that correctly penalizes a model for its true number of free parameters, enabling a more principled and accurate approach to model selection. ",
            "id": "3164632",
            "problem": "A data analyst is comparing candidate regression models in a statistical learning setting where multiple qualitative predictors appear with many levels. When qualitative predictors are coded using dummy variables with a single reference level, each qualitative predictor with $L$ observed levels contributes $L-1$ free coefficients to the linear predictor, whereas a quantitative predictor contributes one free slope coefficient, and the intercept contributes one free coefficient. Starting from the principle that model selection should minimize the expected Kullback–Leibler (KL) divergence between the true data-generating distribution and the candidate model distribution, and using the asymptotic justification that the training log-likelihood overestimates out-of-sample performance by an amount proportional to the number of free parameters, derive an information criterion that penalizes the total number of estimable level coefficients rather than merely the count of predictors. Then, apply your derived criterion to the following fitted model:\n- The maximized log-likelihood is $\\ln L(\\hat{\\theta}) = -920.35$.\n- There are $q = 4$ quantitative predictors.\n- There are three qualitative predictors with levels $L_{1} = 30$, $L_{2} = 7$, and $L_{3} = 4$.\nCompute the value of your level-penalized information criterion for this fitted model. Round your final answer to four significant figures.",
            "solution": "The problem requires the derivation of a model selection criterion based on minimizing the expected Kullback-Leibler (KL) divergence and then applying this criterion to a specific regression model.\n\n**Derivation of the Information Criterion**\n\nLet the true, unknown data-generating probability distribution be $f(x)$. Let $g_{\\theta}(x)$ be the probability distribution corresponding to a candidate model, parameterized by a vector $\\theta$. The Kullback-Leibler (KL) divergence from $g_{\\theta}$ to $f$ measures the information lost when $g_{\\theta}$ is used to approximate $f$:\n$$\nD_{\\text{KL}}(f || g_{\\theta}) = \\int f(x) \\ln \\left( \\frac{f(x)}{g_{\\theta}(x)} \\right) dx = E_{f}[\\ln f(X)] - E_{f}[\\ln g_{\\theta}(X)]\n$$\nTo select the best model, we seek to minimize this divergence. Since $E_{f}[\\ln f(X)]$ is constant across all candidate models, minimizing $D_{\\text{KL}}(f || g_{\\theta})$ is equivalent to maximizing the expected log-likelihood of the model, $E_{f}[\\ln g_{\\theta}(X)]$.\n\nThe true distribution $f$ is unknown, so this expectation cannot be computed directly. Instead, we use the maximized log-likelihood on the training data, $\\ln L(\\hat{\\theta}) = \\sum_{i=1}^{n} \\ln g_{\\hat{\\theta}}(x_i)$, where $\\hat{\\theta}$ is the maximum likelihood estimate of $\\theta$.\n\nAs stated in the problem, $\\ln L(\\hat{\\theta})$ is a biased estimator of the model's expected performance on new data. The problem provides the asymptotic result that this in-sample log-likelihood overestimates the true expected out-of-sample log-likelihood by an amount proportional to the number of free parameters, which we denote as $k$. Specifically, the bias is approximately $k$.\n$$\nE[\\text{expected log-likelihood for new data}] \\approx \\ln L(\\hat{\\theta}) - k\n$$\nTo correct for this optimistic bias, we penalize the maximized log-likelihood by the number of estimated parameters. A model is preferred if it has a higher value of this penalized log-likelihood, $\\ln L(\\hat{\\theta}) - k$.\n\nConventionally, information criteria are expressed on a deviance scale ($-2$ times the log-likelihood) and are minimized. Multiplying the penalized log-likelihood by $-2$ yields the information criterion, which we will call $IC$:\n$$\nIC = -2(\\ln L(\\hat{\\theta}) - k) = -2\\ln L(\\hat{\\theta}) + 2k\n$$\nThis is the well-known Akaike Information Criterion (AIC). The problem's \"level-penalized information criterion\" is precisely this criterion, where $k$ is calculated by correctly accounting for all free coefficients as specified.\n\n**Calculation of the Number of Free Parameters ($k$)**\n\nThe problem provides an explicit recipe for counting the number of free parameters, $k$, in the model's linear predictor.\n- The model includes an intercept, which contributes $1$ parameter.\n- There are $q = 4$ quantitative predictors, each contributing one slope coefficient. This gives $4$ parameters.\n- There are three qualitative predictors. For a predictor with $L$ levels coded with a reference level, it contributes $L-1$ parameters (for the dummy variables).\n    - Predictor 1 has $L_1 = 30$ levels, contributing $L_1 - 1 = 30 - 1 = 29$ parameters.\n    - Predictor 2 has $L_2 = 7$ levels, contributing $L_2 - 1 = 7 - 1 = 6$ parameters.\n    - Predictor 3 has $L_3 = 4$ levels, contributing $L_3 - 1 = 4 - 1 = 3$ parameters.\n\nThe total number of estimable coefficients, $k$, is the sum of these:\n$$\nk = 1 + q + (L_1 - 1) + (L_2 - 1) + (L_3 - 1)\n$$\nSubstituting the given values:\n$$\nk = 1 + 4 + (30 - 1) + (7 - 1) + (4 - 1) = 1 + 4 + 29 + 6 + 3 = 43\n$$\nNote that this count adheres to the problem's focus on coefficients in the linear predictor.\n\n**Application to the Fitted Model**\n\nWe now compute the value of the information criterion using the derived formula and the given model data.\n- Derived criterion: $IC = -2\\ln L(\\hat{\\theta}) + 2k$\n- Given maximized log-likelihood: $\\ln L(\\hat{\\theta}) = -920.35$\n- Calculated number of parameters: $k = 43$\n\nSubstituting these values into the formula:\n$$\nIC = -2(-920.35) + 2(43)\n$$\n$$\nIC = 1840.7 + 86\n$$\n$$\nIC = 1926.7\n$$\nThe problem requires the final answer to be rounded to four significant figures. Rounding $1926.7$ to four significant figures gives $1927$.",
            "answer": "$$\n\\boxed{1927}\n$$"
        },
        {
            "introduction": "A fundamental assumption of Ordinary Least Squares (OLS) is that the variance of the errors is constant, a property known as homoscedasticity. However, when our data is composed of distinct groups—as defined by a qualitative predictor—this assumption is often violated, leading to inefficient estimates and misleading inference. This hands-on exercise guides you through a complete workflow to diagnose such group-wise heteroscedasticity and apply Weighted Least Squares (WLS) as a corrective measure, an essential skill for building robust and reliable statistical models. ",
            "id": "3164625",
            "problem": "You are given a statistical learning task involving a linear model with a single quantitative predictor and a qualitative predictor with multiple levels encoded via dummy variables. The goal is to diagnose and model heteroscedasticity across category levels and to incorporate level-specific weights using Weighted Least Squares (WLS). Work within the following foundational framework: the classical linear model assumes a response $y$ relates linearly to predictors through a design matrix $X$, with additive errors $\\varepsilon$ such that $\\mathbb{E}[\\varepsilon] = 0$, uncorrelated errors, and potentially non-constant variance across groups. A qualitative predictor with levels is represented via dummy variables relative to a chosen reference level. For each test case, assume the qualitative predictor has levels $A$, $B$, and $C$ and the reference level is $A$.\n\nYour program must:\n- Construct the design matrix $X$ with an intercept, the quantitative predictor $x$, and dummy variables for the non-reference levels $B$ and $C$.\n- Compute the Ordinary Least Squares (OLS) estimate using only the given data and the design matrix.\n- Compute residuals $r = y - X\\hat{\\beta}_{\\text{OLS}}$ and, for each level $g \\in \\{A,B,C\\}$, estimate a level-specific variance $\\hat{\\sigma}^2_g$ as the mean of squared residuals within that level.\n- Diagnose heteroscedasticity by computing the ratio $R = \\max_g \\hat{\\sigma}^2_g \\,/\\, \\min_g \\hat{\\sigma}^2_g$ and comparing it to a threshold $\\tau$. Declare heteroscedasticity present if $R > \\tau$.\n- If heteroscedasticity is present, perform WLS using diagonal weights $w_i = 1 / \\hat{\\sigma}^2_{g(i)}$, where $g(i)$ denotes the level of observation $i$. If not present, set $w_i = 1$ for all $i$. Compute the WLS estimate.\n- Extract the following coefficients from both OLS and WLS: the slope on $x$, and the dummy coefficients for levels $B$ and $C$ (relative to $A$).\n\nFor numerical output, round all floating-point results to $6$ decimal places. There are no physical units and no angles in this problem. Percentages are not used.\n\nTest Suite:\nUse the following test cases (each test case is a tuple $(x, y, \\text{groups})$) and the common threshold $\\tau = 1.5$. In all cases, groups is a list of the same length as $x$ and $y$, consisting only of the strings \"A\", \"B\", or \"C\". The design matrix must use \"A\" as the reference level.\n\n- Test Case 1 (heteroscedastic across levels):\n    - $x = [1, 2, 3, 4, 1.5, 2.5, 3.5, 4.5, 0.5, 1.5, 2.5, 3.5]$\n    - $y = [3.7, 4.9, 6.6, 7.8, 5.75, 2.95, 7.95, 5.55, 4.25, 3.95, 7.0, 6.65]$\n    - groups $= [\\text{\"A\"}, \\text{\"A\"}, \\text{\"A\"}, \\text{\"A\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"C\"}, \\text{\"C\"}, \\text{\"C\"}, \\text{\"C\"}]$\n\n- Test Case 2 (approximately homoscedastic across levels):\n    - $x = [1, 2, 3, 4, 1.5, 2.5, 3.5, 4.5, 0.5, 1.5, 2.5, 3.5]$\n    - $y = [3.8, 4.6, 6.7, 7.9, 3.45, 4.45, 6.35, 7.55, 3.5, 4.55, 6.4, 7.65]$\n    - groups $= [\\text{\"A\"}, \\text{\"A\"}, \\text{\"A\"}, \\text{\"A\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"C\"}, \\text{\"C\"}, \\text{\"C\"}, \\text{\"C\"}]$\n\n- Test Case 3 (unbalanced group sizes, strong heteroscedasticity):\n    - $x = [1, 2, 3, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 1, 3, 5]$\n    - $y = [1.6, 1.8, 2.8, 3.3, 3.7, 4.29, 4.71, 5.28, 5.72, 2.5, -1.0, 4.0]$\n    - groups $= [\\text{\"A\"}, \\text{\"A\"}, \\text{\"A\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"B\"}, \\text{\"C\"}, \\text{\"C\"}, \\text{\"C\"}]$\n\nRequired Final Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of the form:\n$[\\text{hetero}, \\text{OLS\\_slope\\_x}, \\text{WLS\\_slope\\_x}, \\text{OLS\\_gamma\\_B}, \\text{WLS\\_gamma\\_B}, \\text{OLS\\_gamma\\_C}, \\text{WLS\\_gamma\\_C}]$.\nHere $\\text{hetero}$ is a boolean (True or False), and the remaining entries are floats rounded to $6$ decimal places. For the three test cases, the program must print a single line like:\n$[[\\ldots],[\\ldots],[\\ldots]]$.",
            "solution": "## Solution Methodology\n\nThis problem requires the implementation of a standard statistical workflow for handling heteroscedasticity in a linear regression model with both quantitative and qualitative predictors. The solution proceeds through a sequence of defined steps, from model specification and estimation to diagnosis and correction.\n\n### 1. Model Specification and Design Matrix\nThe linear model is specified as:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\gamma_B D_{i,B} + \\gamma_C D_{i,C} + \\varepsilon_i\n$$\nHere, $y_i$ and $x_i$ are the $i$-th observations of the response and quantitative predictor, respectively. $\\beta_0$ is the intercept, which represents the expected value of $y$ for an observation in the reference group ($A$) when $x=0$. $\\beta_1$ is the slope coefficient for the quantitative predictor $x$. The terms $D_{i,B}$ and $D_{i,C}$ are dummy variables for the qualitative predictor's levels.\n- $D_{i,B} = 1$ if observation $i$ is in group $B$, and $0$ otherwise.\n- $D_{i,C} = 1$ if observation $i$ is in group $C$, and $0$ otherwise.\nFor an observation in the reference group $A$, both $D_{i,B}$ and $D_{i,C}$ are $0$.\nThe coefficients $\\gamma_B$ and $\\gamma_C$ represent the differential effect on the intercept for an observation being in group $B$ or $C$ relative to group $A$.\n\nThis model can be expressed in matrix form as $y = X\\beta + \\varepsilon$, where $y$ is the vector of responses, $X$ is the design matrix, $\\beta$ is the vector of coefficients, and $\\varepsilon$ is the vector of errors. For $N$ observations, the design matrix $X$ of size $N \\times 4$ is constructed as:\n$$\nX = \\begin{bmatrix}\n1 & x_1 & D_{1,B} & D_{1,C} \\\\\n1 & x_2 & D_{2,B} & D_{2,C} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_N & D_{N,B} & D_{N,C}\n\\end{bmatrix}\n\\quad \\text{and} \\quad\n\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\gamma_B \\\\ \\gamma_C \\end{bmatrix}\n$$\n\n### 2. Ordinary Least Squares (OLS) Estimation\nThe OLS estimator for $\\beta$, denoted $\\hat{\\beta}_{\\text{OLS}}$, is found by minimizing the sum of squared residuals, $\\sum r_i^2 = (y-X\\beta)^T(y-X\\beta)$. This yields the closed-form solution:\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^T X)^{-1} X^T y\n$$\n\n### 3. Heteroscedasticity Diagnosis\nAfter obtaining $\\hat{\\beta}_{\\text{OLS}}$, we compute the vector of residuals $r = y - X\\hat{\\beta}_{\\text{OLS}}$. Heteroscedasticity implies that the variance of the errors, $\\text{Var}(\\varepsilon_i) = \\sigma_i^2$, is not constant across all observations. In this problem, we test if the variance is constant across the groups $\\{A, B, C\\}$. We estimate the variance for each group $g$ as the mean of the squared residuals for observations within that group:\n$$\n\\hat{\\sigma}^2_g = \\frac{1}{n_g} \\sum_{i \\in \\text{group } g} r_i^2\n$$\nwhere $n_g$ is the number of observations in group $g$.\n\nA simple diagnostic tool is the ratio of the largest estimated group variance to the smallest:\n$$\nR = \\frac{\\max(\\hat{\\sigma}^2_A, \\hat{\\sigma}^2_B, \\hat{\\sigma}^2_C)}{\\min(\\hat{\\sigma}^2_A, \\hat{\\sigma}^2_B, \\hat{\\sigma}^2_C)}\n$$\nIf this ratio $R$ exceeds a predefined threshold $\\tau$, we conclude that significant heteroscedasticity is present. For this problem, $\\tau = 1.5$.\n\n### 4. Weighted Least Squares (WLS) Estimation\nIf heteroscedasticity is detected, the OLS estimator is no longer the Best Linear Unbiased Estimator (BLUE), although it remains unbiased. WLS provides a more efficient estimator by giving less weight to observations with higher variance. The WLS estimator is:\n$$\n\\hat{\\beta}_{\\text{WLS}} = (X^T W X)^{-1} X^T W y\n$$\nwhere $W$ is a diagonal matrix of weights. The optimal weights are inversely proportional to the variance of the errors, $w_i = 1/\\sigma_i^2$. Since the true variances are unknown, we use their estimates. For an observation $i$ belonging to group $g(i)$, the weight is set to $w_i = 1/\\hat{\\sigma}^2_{g(i)}$. The weight matrix $W$ is thus:\n$$\nW = \\text{diag}(w_1, w_2, \\ldots, w_N)\n$$\nIf heteroscedasticity is not detected ($R \\le \\tau$), we are instructed to use unit weights, $w_i = 1$ for all $i$. This makes $W$ the identity matrix $I$, and the WLS estimator becomes identical to the OLS estimator: $\\hat{\\beta}_{\\text{WLS}} = (X^T I X)^{-1} X^T I y = \\hat{\\beta}_{\\text{OLS}}$.\n\n### 5. Extraction of Coefficients\nThe final step for each test case is to extract the relevant coefficients from the estimated vectors $\\hat{\\beta}_{\\text{OLS}}$ and $\\hat{\\beta}_{\\text{WLS}}$. The required coefficients are:\n- The slope on $x$: $\\hat{\\beta}_1$, which is the second element of the $\\hat{\\beta}$ vector.\n- The dummy coefficient for level $B$: $\\hat{\\gamma}_B$, the third element.\n- The dummy coefficient for level $C$: $\\hat{\\gamma}_C$, the fourth element.\nThese values, along with the boolean flag for heteroscedasticity, are compiled into the final output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of statistical learning problems involving OLS, heteroscedasticity\n    diagnosis, and WLS for a linear model with a qualitative predictor.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            [1, 2, 3, 4, 1.5, 2.5, 3.5, 4.5, 0.5, 1.5, 2.5, 3.5],\n            [3.7, 4.9, 6.6, 7.8, 5.75, 2.95, 7.95, 5.55, 4.25, 3.95, 7.0, 6.65],\n            [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\", \"C\"],\n        ),\n        (\n            [1, 2, 3, 4, 1.5, 2.5, 3.5, 4.5, 0.5, 1.5, 2.5, 3.5],\n            [3.8, 4.6, 6.7, 7.9, 3.45, 4.45, 6.35, 7.55, 3.5, 4.55, 6.4, 7.65],\n            [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\", \"C\"],\n        ),\n        (\n            [1, 2, 3, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 1, 3, 5],\n            [1.6, 1.8, 2.8, 3.3, 3.7, 4.29, 4.71, 5.28, 5.72, 2.5, -1.0, 4.0],\n            [\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\"],\n        )\n    ]\n    \n    tau = 1.5\n    all_results = []\n    \n    for case in test_cases:\n        x_data, y_data, groups_data = case\n        \n        # Convert to numpy arrays for vectorized operations\n        x = np.array(x_data)\n        y = np.array(y_data)\n        groups = np.array(groups_data)\n        n_obs = len(y)\n        \n        # 1. Construct the design matrix X\n        X = np.zeros((n_obs, 4))\n        X[:, 0] = 1  # Intercept\n        X[:, 1] = x  # Quantitative predictor\n        X[:, 2] = (groups == \"B\").astype(int)  # Dummy for level B\n        X[:, 3] = (groups == \"C\").astype(int)  # Dummy for level C\n        \n        # 2. Compute the Ordinary Least Squares (OLS) estimate\n        try:\n            XTX_inv = np.linalg.inv(X.T @ X)\n            beta_ols = XTX_inv @ (X.T @ y)\n        except np.linalg.LinAlgError:\n            # This case should not occur with the given test data\n            # but is good practice for numerical stability.\n            all_results.append(None)\n            continue\n            \n        # 3. Compute residuals and estimate level-specific variances\n        residuals = y - X @ beta_ols\n        \n        group_indices = {\n            'A': np.where(groups == 'A')[0],\n            'B': np.where(groups == 'B')[0],\n            'C': np.where(groups == 'C')[0]\n        }\n        \n        group_variances = {}\n        for level, indices in group_indices.items():\n            if len(indices) > 0:\n                group_variances[level] = np.mean(residuals[indices]**2)\n            else:\n                 group_variances[level] = np.nan # Handle cases where a group might be empty\n        \n        valid_variances = [v for v in group_variances.values() if not np.isnan(v)]\n        \n        # 4. Diagnose heteroscedasticity\n        hetero_present = False\n        if len(valid_variances) > 1:\n            R = max(valid_variances) / min(valid_variances) if min(valid_variances) > 0 else float('inf')\n            if R > tau:\n                hetero_present = True\n\n        # 5. Perform WLS if heteroscedasticity is present\n        if hetero_present:\n            weights = np.ones(n_obs)\n            for level, var in group_variances.items():\n                if not np.isnan(var) and var > 0:\n                    weights[group_indices[level]] = 1.0 / var\n\n            W = np.diag(weights)\n            \n            try:\n                XTWX_inv = np.linalg.inv(X.T @ W @ X)\n                beta_wls = XTWX_inv @ (X.T @ W @ y)\n            except np.linalg.LinAlgError:\n                all_results.append(None)\n                continue\n        else:\n            # If not heteroscedastic, WLS is equivalent to OLS (with unit weights)\n            beta_wls = beta_ols\n            \n        # 6. Extract coefficients and round to 6 decimal places\n        ols_slope_x = round(beta_ols[1], 6)\n        wls_slope_x = round(beta_wls[1], 6)\n        ols_gamma_B = round(beta_ols[2], 6)\n        wls_gamma_B = round(beta_wls[2], 6)\n        ols_gamma_C = round(beta_ols[3], 6)\n        wls_gamma_C = round(beta_wls[3], 6)\n        \n        case_result = [\n            hetero_present,\n            ols_slope_x, wls_slope_x,\n            ols_gamma_B, wls_gamma_B,\n            ols_gamma_C, wls_gamma_C\n        ]\n        all_results.append(case_result)\n        \n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation\n    results_str = [str(res) for res in all_results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}