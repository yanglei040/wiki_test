{
    "hands_on_practices": [
        {
            "introduction": "Weighted Least Squares is designed to improve estimation efficiency by down-weighting observations with high error variance. However, when the theoretically optimal weights become extremely large for certain data points, they can paradoxically destabilize the estimation process. This exercise explores a scenario where the error variance is proportional to the magnitude of the predictor, $|x_i|$, leading to weights that approach infinity as $x_i$ approaches zero. By analyzing the variance of the intercept estimator and the condition number of the normal equations, you will gain first-hand insight into why this creates numerical and statistical instability, and how a simple stabilization technique provides a practical remedy .",
            "id": "3128028",
            "problem": "Consider the linear regression model with heteroscedastic errors, where a response variable is modeled as $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, for $i = 1, \\dots, n$. Assume the errors satisfy $E[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2 |x_i|$, where $\\sigma^2 > 0$ is an unknown constant of proportionality. Weighted Least Squares (WLS) uses positive weights $w_i$ to minimize $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$. When $\\operatorname{Var}(\\varepsilon_i)$ is proportional to $|x_i|$, a natural choice is to take $w_i$ proportional to $1/|x_i|$, but this can excessively amplify observations with $x_i$ near $0$ and distort intercept estimation. To mitigate this, consider stabilized weights $w_i = 1 / (|x_i| + \\delta)$, where $\\delta \\ge 0$ is a stabilization parameter.\n\nStarting from the linear model definition, and the principle that the best linear unbiased estimator under heteroscedasticity arises from appropriately weighting by the inverse of the error variance, derive the variance of the intercept estimator under a general diagonal weight matrix and a diagonal error variance structure implied by $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$. Explain, from first principles and without shortcut formulas provided in the problem statement, why the choice $w_i = 1/|x_i|$ can cause instability for intercept estimation when some $|x_i|$ are very small, and how the stabilized choice $w_i = 1/(|x_i| + \\delta)$ bounds the influence of near-zero $x_i$.\n\nThen, implement a program that, for each test case specified below, computes the following two quantities:\n- The ratio $r$ of the WLS intercept estimator variance to the Ordinary Least Squares (OLS) intercept estimator variance, where OLS sets $w_i = 1$ for all $i$ but still respects $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$ when assessing the estimator variance.\n- The spectral condition number $\\kappa$ of the weighted normal matrix $X^\\top W X$, where $X$ is the design matrix with first column equal to ones and second column equal to the $x_i$, and $W = \\operatorname{diag}(w_1, \\dots, w_n)$.\n\nYour program must:\n- Use $X = \\begin{bmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$.\n- Use $\\Sigma = \\operatorname{diag}(\\sigma^2 |x_1|, \\dots, \\sigma^2 |x_n|)$ to represent the error variance structure.\n- Compute the intercept variance under WLS using only the model assumptions provided above.\n- Compute the intercept variance under OLS using only the model assumptions provided above.\n- Set $\\sigma^2 = 1$ for all test cases.\n- Round each reported float to six decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a comma-separated pair $[r,\\kappa]$ with no spaces.\n\nTest Suite (each test case specifies the vector of $x_i$ values and the stabilization parameter $\\delta$):\n1. Happy path (no zeros): $x = (-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.0$.\n2. Boundary sensitivity (near-zero values, no exact zeros): $x = (-2.0, -1.0, -0.5, -10^{-8}, 10^{-8}, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.0$.\n3. Stabilized with an exact zero: $x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.1$.\n4. Strong stabilization: $x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.5$.\n\nAnswer specification:\n- For each test case, output the pair $[r,\\kappa]$ as described above.\n- Your program should produce a single line of output containing all test case results as a comma-separated list enclosed in square brackets. There must be no spaces anywhere in the output string.",
            "solution": "The problem statement is a valid exercise in statistical learning, concerning the properties of Weighted Least Squares (WLS) estimators under heteroscedasticity. It requires both a theoretical derivation and explanation from first principles and a numerical implementation to illustrate the concepts. All components of the problem are scientifically grounded, well-defined, and internally consistent.\n\nThe linear model is given in matrix form as $y = X\\beta + \\varepsilon$, where $y$ is an $n \\times 1$ vector of observations, $X$ is the $n \\times 2$ design matrix, $\\beta = [\\beta_0, \\beta_1]^\\top$ is the vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of errors. The assumptions on the errors are $E[\\varepsilon] = 0$ and $\\operatorname{Var}(\\varepsilon) = E[\\varepsilon \\varepsilon^\\top] = \\Sigma$, where $\\Sigma$ is a diagonal matrix with entries $[\\Sigma]_{ii} = \\sigma^2|x_i|$.\n\nThe Weighted Least Squares (WLS) estimator, $\\hat{\\beta}_{WLS}$, is found by minimizing the weighted sum of squared residuals, $S(\\beta) = (y - X\\beta)^\\top W (y - X\\beta)$, where $W$ is a diagonal matrix of positive weights $w_i$. The solution to this minimization problem is given by the weighted normal equations:\n$$ (X^\\top W X) \\hat{\\beta}_{WLS} = X^\\top W y $$\nThis yields the estimator:\n$$ \\hat{\\beta}_{WLS} = (X^\\top W X)^{-1} X^\\top W y $$\n\nThe variance of this estimator is derived as follows. Since $\\hat{\\beta}_{WLS}$ is a linear function of $y$, its variance is:\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = \\operatorname{Var}((X^\\top W X)^{-1} X^\\top W y) $$\nUsing the fact that $\\beta$ is a constant vector and $\\operatorname{Var}(y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\operatorname{Var}(\\varepsilon) = \\Sigma$, we have:\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W) \\operatorname{Var}(y) (X^\\top W)^\\top ((X^\\top W X)^{-1})^\\top $$\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W^\\top X) (X^\\top W X)^{-1} $$\nSince $W$ and $X^\\top W X$ are symmetric, this simplifies to the \"sandwich\" formula:\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1} $$\nThe variance of the intercept estimator, $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$, is the first diagonal element, i.e., the $(1,1)$-entry, of this $2 \\times 2$ matrix. This general formula is valid for any choice of positive weights $W$ and is not restricted to the \"optimal\" case where $W \\propto \\Sigma^{-1}$.\n\nNow we analyze the choice of weights.\nThe optimal weights for WLS, which produce the Best Linear Unbiased Estimator (BLUE), are proportional to the inverse of the error variances, i.e., $w_i \\propto 1/\\operatorname{Var}(\\varepsilon_i) = 1/(\\sigma^2|x_i|)$. Let us choose $w_i = 1/|x_i|$ (implicitly setting $\\sigma^2=1$ in the weight definition, which is permissible as the estimator is invariant to a constant scaling of weights). In this special case, the weight matrix is $W = (1/\\sigma^2)\\Sigma^{-1}$ (if we were to include the $\\sigma^2$ term). Let's see how this affects the sandwich formula's middle term:\n$$ X^\\top W \\Sigma W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) \\Sigma \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\nAnd the outer terms:\n$$ X^\\top W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\nSubstituting these into the sandwich formula yields:\n$$ \\operatorname{Var}(\\hat{\\beta}_{BLUE}) = \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right) \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} = \\sigma^2 (X^\\top \\Sigma^{-1} X)^{-1} = (X^\\top W X)^{-1} \\sigma^2 $$\n(using $W = \\Sigma^{-1}/\\sigma^2$ would give $\\sigma^4(X^\\top \\Sigma^{-1} X)^{-1}$ etc. If we set $W = \\Sigma^{-1}$, $\\operatorname{Var} = (X^\\top\\Sigma^{-1}X)^{-1}$). With the choice $w_i=1/|x_i|$ and $\\sigma^2=1$, we get $W = \\Sigma^{-1}$ and $\\operatorname{Var}(\\hat{\\beta})=(X^\\top W X)^{-1}$.\n\nInstability arises when an observation $x_k$ is very close to $0$ and we use the theoretically optimal weights $w_i = 1/|x_i|$ (i.e., $\\delta=0$).\n1.  **Numerical Instability**: The weight $w_k = 1/|x_k|$ becomes extremely large. The normal matrix $X^\\top W X = \\begin{pmatrix} \\sum w_i & \\sum w_i x_i \\\\ \\sum w_i x_i & \\sum w_i x_i^2 \\end{pmatrix}$ becomes ill-conditioned. The term $\\sum w_i$ is dominated by $w_k$, making the top-left entry enormous, while the other entries are not scaled to the same degree. For example, $\\sum w_i x_i^2 = \\sum |x_i|$ is not dominated by the $k$-th term. This disparity in magnitudes makes the matrix nearly singular, which is quantified by a very large spectral condition number, $\\kappa$. Inverting such a matrix is numerically unstable and highly sensitive to small perturbations in the data.\n2.  **Estimation Distortion**: The WLS objective function $\\sum w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$ is dominated by the term for $i=k$. To minimize this sum, the regression line is forced to pass extremely close to the point $(x_k, y_k)$, i.e., $y_k \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1 x_k$. As $x_k \\to 0$, this implies $\\hat{\\beta}_0 \\approx y_k$. The estimate of the intercept becomes almost entirely determined by a single observation, $y_k$, whose own value is subject to random error. This extreme amplification of the influence of one data point distorts the intercept estimate, making it unreliable.\n\nThe stabilized weights $w_i = 1 / (|x_i| + \\delta)$ with $\\delta > 0$ mitigate this problem.\nAs $x_i \\to 0$, the stabilized weight $w_i \\to 1/\\delta$. The weight is thus bounded from above by $1/\\delta$. This prevents any single weight from becoming arbitrarily large. Consequently, no single observation can dominate the least squares fit. All entries in the matrix $X^\\top W X$ remain bounded and of comparable magnitude, leading to a much smaller (better) condition number and a more stable, robust numerical solution. The resulting intercept estimate, $\\hat{\\beta}_0$, will be a balanced compromise across all data points, as intended by the least squares principle. The trade-off is that these non-optimal weights lead to an estimator that is no longer the BLUE, potentially increasing its variance compared to the theoretical (but unstable) optimum.\n\nFor the implementation, we compute the following quantities:\n- The variance of the WLS intercept estimator, $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$, is the $(1,1)$-entry of $(X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1}$, where $W=\\operatorname{diag}(1/(|x_i|+\\delta))$ and $\\Sigma=\\operatorname{diag}(|x_i|)$ (with $\\sigma^2=1$).\n- The variance of the OLS intercept estimator, $\\operatorname{Var}(\\hat{\\beta}_{0, OLS})$, is computed using the same general sandwich formula but with weights $w_i=1$ for all $i$. So, we set $W_{OLS}=I$ (the identity matrix). The variance is the $(1,1)$-entry of $(X^\\top X)^{-1} (X^\\top \\Sigma X) (X^\\top X)^{-1}$. This correctly accounts for the true heteroscedastic error structure even though the OLS estimator itself does not use this information.\n- The ratio is $r = \\operatorname{Var}(\\hat{\\beta}_{0, WLS}) / \\operatorname{Var}(\\hat{\\beta}_{0, OLS})$.\n- The condition number is $\\kappa = \\kappa(X^\\top W X)$.\n\nThe program will execute these calculations for each specified test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of WLS to OLS intercept variance and the condition number\n    of the weighted normal matrix for several test cases of heteroscedastic linear regression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -1e-8, 1e-8, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.1},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.5},\n    ]\n\n    results = []\n    \n    # Set the unknown constant of proportionality to 1 as specified.\n    sigma_squared = 1.0\n\n    for case in test_cases:\n        x_vec = case[\"x\"]\n        delta = case[\"delta\"]\n        n = len(x_vec)\n\n        # Construct the design matrix X\n        X = np.vstack((np.ones(n), x_vec)).T\n\n        # Construct the true error covariance matrix Sigma\n        # Var(epsilon_i) = sigma^2 * |x_i|\n        Sigma = np.diag(sigma_squared * np.abs(x_vec))\n\n        # --- WLS Calculation ---\n        # Construct the WLS weight matrix W\n        # w_i = 1 / (|x_i| + delta)\n        w_vals = 1.0 / (np.abs(x_vec) + delta)\n        W = np.diag(w_vals)\n\n        # Calculate the weighted normal matrix for WLS\n        X_T_W_X = X.T @ W @ X\n        \n        # Calculate the \"sandwich\" middle part for WLS\n        X_T_W_Sigma_W_X = X.T @ W @ Sigma @ W @ X\n\n        # Calculate the variance-covariance matrix for the WLS estimator\n        # Var(beta_hat_WLS) = (X'WX)^-1 (X'W Sigma WX) (X'WX)^-1\n        X_T_W_X_inv = np.linalg.inv(X_T_W_X)\n        var_beta_wls = X_T_W_X_inv @ X_T_W_Sigma_W_X @ X_T_W_X_inv\n\n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_wls = var_beta_wls[0, 0]\n\n        # Calculate the spectral condition number of the weighted normal matrix\n        kappa = np.linalg.cond(X_T_W_X)\n\n        # --- OLS Calculation ---\n        # For OLS, the weights w_i are all 1.\n        # The estimator is (X'X)^-1 X'y, and its variance must be calculated\n        # using the true heteroscedastic covariance matrix Sigma.\n        X_T_X = X.T @ X\n        \n        # OLS \"sandwich\" middle part\n        X_T_Sigma_X = X.T @ Sigma @ X\n\n        # Calculate the variance-covariance matrix for the OLS estimator\n        # Var(beta_hat_OLS) = (X'X)^-1 (X' Sigma X) (X'X)^-1\n        X_T_X_inv = np.linalg.inv(X_T_X)\n        var_beta_ols = X_T_X_inv @ X_T_Sigma_X @ X_T_X_inv\n        \n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_ols = var_beta_ols[0, 0]\n\n        # --- Ratio Calculation ---\n        # Ratio of WLS intercept variance to OLS intercept variance\n        if var_b0_ols == 0:\n            # Handle potential division by zero, though unlikely in these cases\n            r = np.inf if var_b0_wls > 0 else 0.0\n        else:\n            r = var_b0_wls / var_b0_ols\n        \n        results.append((r, kappa))\n\n    # Format the final output string as specified, with 6 decimal places and no spaces\n    output_str = \",\".join([f\"[{r:.6f},{k:.6f}]\" for r, k in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In many real-world applications, we may suspect that heteroscedasticity is present, but we do not know the exact form of the variance function. This practice introduces Feasible Weighted Least Squares (FWLS), a powerful multi-stage procedure that addresses this challenge by using the data itself to estimate the variance structure. You will first perform an Ordinary Least Squares (OLS) regression to obtain residuals, and then use these residuals in an auxiliary regression to estimate the unknown parameter governing the heteroscedasticity. This hands-on problem demonstrates how to make WLS practical when the ideal weights are unknown and also investigates the procedure's sensitivity to high-leverage outliers .",
            "id": "3128024",
            "problem": "You are given a simple linear regression model with one predictor and a heteroscedastic error structure. For observations indexed by $i = 1, \\dots, n$, suppose the data are generated according to\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i,\n$$\nwith\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 \\lvert x_i \\rvert^\\alpha,\n$$\nwhere $\\sigma^2 > 0$ and $\\alpha \\in \\mathbb{R}$. Your task is to derive and implement a feasible Weighted Least Squares (WLS) estimator by estimating the unknown exponent $\\alpha$ from data using residuals from an initial Ordinary Least Squares (OLS) fit, and then examine the sensitivity of this procedure to outliers in the predictor $x$.\n\nFundamental base:\n- Ordinary Least Squares (OLS) minimizes $\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2$ and solves the normal equations arising from the first-order optimality conditions.\n- Weighted Least Squares (WLS) minimizes $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$, where the weights $\\{w_i\\}$ are prescribed, and solves the weighted normal equations. For heteroscedastic models with known variances, the optimal weights are inversely proportional to the conditional variances.\n- If the variance model is unknown but parametrically specified up to $\\alpha$, a feasible WLS procedure estimates $\\alpha$ from data and plugs the estimate into the weights.\n\nImplementation requirements:\n1. Generate data deterministically for each test case using the following protocol.\n   - Let $x_i$ be independently drawn from a standard normal distribution $\\mathcal{N}(0,1)$ using a specified pseudorandom number generator seed.\n   - Let $z_i$ be independently drawn from a standard normal distribution $\\mathcal{N}(0,1)$ using a different seed, independent of the draws of $x_i$.\n   - Define a stabilized magnitude function $m(x) = \\max(\\lvert x \\rvert, \\delta)$ with $\\delta = 10^{-8}$ for numerical stability. Use $m(x)$ wherever $\\lvert x \\rvert$ appears in variance-related computations (including data generation, weight construction, and the auxiliary regression) to avoid undefined operations such as $\\log 0$ and to handle negative $\\alpha$ without singularities.\n   - Generate errors as $\\varepsilon_i = \\sigma \\, m(x_i)^{\\alpha/2} z_i$ so that $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$ approximates the target form with stability.\n   - Generate responses as $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n2. For each dataset (without outliers):\n   - Fit OLS to obtain residuals $r_i = y_i - \\hat{\\beta}_0^{\\text{OLS}} - \\hat{\\beta}_1^{\\text{OLS}} x_i$.\n   - Estimate $\\alpha$ by an auxiliary OLS regression of $\\log(r_i^2)$ on an intercept and $\\log(m(x_i))$, i.e., regress $t_i = \\log(\\max(r_i^2,\\delta))$ on $1$ and $z_i = \\log(m(x_i))$. Denote the slope estimate by $\\hat{\\alpha}$.\n   - Construct WLS weights $w_i = m(x_i)^{-\\hat{\\alpha}}$ and compute the WLS estimate of $\\beta_1$, denoted $\\hat{\\beta}_1^{\\text{WLS}}$.\n3. Outlier sensitivity analysis in $x$:\n   - Create a copy of the original predictor vector and replace a single entry at index $j = \\lfloor n/2 \\rfloor$ with an outlier of magnitude $s \\cdot s_x$, where $s$ is a provided outlier scale and $s_x$ is the sample standard deviation of the original $x$.\n   - Using the same noise draws $\\{z_i\\}$ as above, regenerate $\\varepsilon_i$ from the updated $x$ via $\\varepsilon_i^{\\text{out}} = \\sigma \\, m(x_i^{\\text{out}})^{\\alpha/2} z_i$ and set $y_i^{\\text{out}} = \\beta_0 + \\beta_1 x_i^{\\text{out}} + \\varepsilon_i^{\\text{out}}$.\n   - Repeat the feasible WLS procedure to obtain $\\hat{\\alpha}^{\\text{out}}$ and $\\hat{\\beta}_1^{\\text{WLS,out}}$.\n   - Report sensitivity metrics defined as $\\Delta_\\alpha = \\lvert \\hat{\\alpha}^{\\text{out}} - \\hat{\\alpha} \\rvert$ and $\\Delta_{\\beta_1} = \\lvert \\hat{\\beta}_1^{\\text{WLS,out}} - \\hat{\\beta}_1^{\\text{WLS}} \\rvert$.\n4. Numerical reporting:\n   - For each test case, report a list of six floating-point numbers rounded to four decimal places:\n     - $\\hat{\\alpha}$,\n     - $\\hat{\\beta}_1^{\\text{WLS}}$,\n     - $\\hat{\\alpha}^{\\text{out}}$,\n     - $\\hat{\\beta}_1^{\\text{WLS,out}}$,\n     - $\\Delta_\\alpha$,\n     - $\\Delta_{\\beta_1}$.\n5. Randomness and reproducibility:\n   - Use the same base seed across test cases, but derive distinct seeds by adding fixed offsets as specified in the test suite below to guarantee deterministic and reproducible outputs.\n   - For test case index $k$ (starting from $0$), use seed $s_x = s_0 + k$ for generating $\\{x_i\\}$ and seed $s_z = s_0 + 100k$ for generating $\\{z_i\\}$, where $s_0 = 2757$.\n\nTest suite:\nProvide results for the following four parameter sets. For each, follow the full procedure both without and with the $x$ outlier injection described above.\n- Test 1 (happy path, increasing variance in $\\lvert x \\rvert$): $n = 200$, $\\beta_0 = 1.0$, $\\beta_1 = 2.0$, $\\sigma = 1.0$, $\\alpha = 1.0$, outlier scale $s = 25.0$.\n- Test 2 (homoscedastic boundary): $n = 200$, $\\beta_0 = 0.5$, $\\beta_1 = -1.5$, $\\sigma = 1.0$, $\\alpha = 0.0$, outlier scale $s = 25.0$.\n- Test 3 (variance decreasing in $\\lvert x \\rvert$): $n = 200$, $\\beta_0 = 0.0$, $\\beta_1 = 1.0$, $\\sigma = 1.0$, $\\alpha = -1.0$, outlier scale $s = 25.0$.\n- Test 4 (small sample, moderate heteroscedasticity): $n = 30$, $\\beta_0 = 1.0$, $\\beta_1 = 0.5$, $\\sigma = 0.5$, $\\alpha = 0.5$, outlier scale $s = 50.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself a list in the order\n$[\\hat{\\alpha}, \\hat{\\beta}_1^{\\text{WLS}}, \\hat{\\alpha}^{\\text{out}}, \\hat{\\beta}_1^{\\text{WLS,out}}, \\Delta_\\alpha, \\Delta_{\\beta_1}]$,\nwith all entries rounded to four decimal places. For example:\n[$[\\cdots]$, $[\\cdots]$, $[\\cdots]$, $[\\cdots]$].",
            "solution": "The problem presented requires the derivation and implementation of a feasible Weighted Least Squares (WLS) procedure to address heteroscedasticity in a simple linear regression model. The core of the task is to estimate the parameter governing the variance structure and then analyze the sensitivity of this estimation procedure to a high-leverage outlier in the predictor variable.\n\nThe data are generated from the model:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, \\dots, n\n$$\nwhere the errors $\\varepsilon_i$ are heteroscedastic with a conditional variance structure given by:\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 \\lvert x_i \\rvert^\\alpha\n$$\nHere, $\\beta_0$ and $\\beta_1$ are the regression coefficients, $\\sigma^2 > 0$ is a variance scale parameter, and $\\alpha \\in \\mathbb{R}$ is an exponent that dictates how the error variance changes with the magnitude of the predictor $x_i$. For numerical stability, especially when $x_i=0$ or for negative $\\alpha$, we employ a stabilized magnitude function $m(x) = \\max(\\lvert x \\rvert, \\delta)$ with a small positive constant $\\delta$, so the operational variance model is $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$.\n\nIn matrix notation, the model is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of responses, $\\mathbf{X}$ is the $n \\times 2$ design matrix with columns for the intercept and the predictor $x_i$, $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ is the vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of errors. The covariance matrix of the errors, conditional on $\\mathbf{X}$, is $\\operatorname{Cov}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}) = \\boldsymbol{\\Omega}$, which is a diagonal matrix with entries $(\\boldsymbol{\\Omega})_{ii} = \\sigma^2 m(x_i)^\\alpha$.\n\nIf the error variances were known, the optimal estimator for $\\boldsymbol{\\beta}$ would be the Weighted Least Squares (WLS) estimator, which minimizes the weighted sum of squared residuals $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$. The optimal weights $w_i$ are inversely proportional to the variances, i.e., $w_i \\propto (\\operatorname{Var}(\\varepsilon_i \\mid x_i))^{-1}$. We can set the weights $w_i = m(x_i)^{-\\alpha}$. The WLS estimator is given by:\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nwhere $\\mathbf{W} = \\operatorname{diag}(w_1, \\dots, w_n)$ is the diagonal matrix of weights.\n\nHowever, the parameter $\\alpha$ is unknown and must be estimated from the data. This leads to a multi-stage procedure known as Feasible WLS (or Feasible Generalized Least Squares, FGLS).\n\n**Step 1: Initial Ordinary Least Squares (OLS) Estimation**\nFirst, we ignore the heteroscedasticity and compute the OLS estimator of $\\boldsymbol{\\beta}$:\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nThe OLS estimator remains unbiased under heteroscedasticity but is no longer the Best Linear Unbiased Estimator (BLUE); its standard errors are also biased. We then compute the OLS residuals, $r_i = y_i - (\\hat{\\beta}_0^{\\text{OLS}} + \\hat{\\beta}_1^{\\text{OLS}} x_i)$, which serve as proxies for the unobservable true errors $\\varepsilon_i$.\n\n**Step 2: The Auxiliary Regression to Estimate $\\alpha$**\nThe core of the feasible procedure is to use the residuals to estimate the unknown variance parameter $\\alpha$. The variance model is $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$. Taking the natural logarithm gives a linear relationship:\n$$\n\\log(\\operatorname{Var}(\\varepsilon_i \\mid x_i)) = \\log(\\sigma^2) + \\alpha \\log(m(x_i))\n$$\nSince $E[r_i^2 \\mid x_i]$ approximates $\\operatorname{Var}(\\varepsilon_i \\mid x_i)$, we can establish an auxiliary regression model. Following the problem specification, we regress $t_i = \\log(\\max(r_i^2, \\delta))$ on an intercept and $z_i = \\log(m(x_i))$:\n$$\nt_i = \\gamma_0 + \\gamma_1 z_i + u_i\n$$\nwhere $\\gamma_0$ is an estimate for $\\log(\\sigma^2)$ plus a term related to the expectation of $\\log(\\chi^2_1)$ noise, and $\\gamma_1$ is an estimate for $\\alpha$. We fit this auxiliary model using OLS and take the slope estimate as our estimate for $\\alpha$, denoted $\\hat{\\alpha}$.\n\n**Step 3: WLS with Estimated Weights**\nWith the estimate $\\hat{\\alpha}$, we construct the feasible weights:\n$$\n\\hat{w}_i = m(x_i)^{-\\hat{\\alpha}}\n$$\nWe form the estimated weight matrix $\\hat{\\mathbf{W}} = \\operatorname{diag}(\\hat{w}_1, \\dots, \\hat{w}_n)$ and compute the feasible WLS estimator:\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{FWLS}} = (\\mathbf{X}^T \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\mathbf{X}^T \\hat{\\mathbf{W}} \\mathbf{y}\n$$\nWe extract the second component of this vector as our final estimate $\\hat{\\beta}_1^{\\text{WLS}}$.\n\n**Outlier Sensitivity Analysis**\nThe problem requires an analysis of this procedure's sensitivity to a high-leverage point in the predictor variable. An outlier is introduced by replacing a single value $x_j$ (at index $j = \\lfloor n/2 \\rfloor$) with a large value $x_j^{\\text{out}} = s \\cdot s_x$, where $s_x$ is the sample standard deviation of the original predictors and $s$ is a large scaling factor. This creates a new dataset $(x_i^{\\text{out}}, y_i^{\\text{out}})$.\n\nThe entire feasible WLS procedure is then repeated on this outlier-contaminated dataset. The outlier $x_j^{\\text{out}}$ is a high-leverage point, meaning it has a strong potential to influence the regression line. This influence propagates through the entire estimation chain:\n1.  The initial OLS fit $(\\hat{\\boldsymbol{\\beta}}^{\\text{OLS,out}})$ will be heavily skewed by the outlier.\n2.  This distorted fit leads to a distorted set of residuals $r_i^{\\text{out}}$, particularly a potentially very large residual at the outlier's index $j$.\n3.  The large outlier residual can dominate the auxiliary regression, leading to a poor estimate $\\hat{\\alpha}^{\\text{out}}$.\n4.  The corrupted estimate $\\hat{\\alpha}^{\\text{out}}$ results in inappropriate weights $\\hat{w}_i^{\\text{out}}$, which may fail to correctly up-weight or down-weight observations, leading to a biased final estimate $\\hat{\\beta}_1^{\\text{WLS,out}}$.\n\nThe sensitivity is quantified by the absolute differences $\\Delta_\\alpha = \\lvert \\hat{\\alpha}^{\\text{out}} - \\hat{\\alpha} \\rvert$ and $\\Delta_{\\beta_1} = \\lvert \\hat{\\beta}_1^{\\text{WLS,out}} - \\hat{\\beta}_1^{\\text{WLS}} \\rvert$. This analysis demonstrates a key vulnerability of automated multi-stage statistical procedures: a non-robust initial step can corrupt the entire analysis, even if subsequent steps are theoretically designed to handle the underlying problem (like WLS for heteroscedasticity).\n\nThe implementation will deterministically generate data for each test case using specified parameters and random seeds. It will then execute the described feasible WLS procedure for both the original dataset and the one with an injected outlier, calculate the required estimates, and compute the sensitivity metrics.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    test_cases = [\n        # (n, beta_0, beta_1, sigma, alpha, s)\n        (200, 1.0, 2.0, 1.0, 1.0, 25.0),\n        (200, 0.5, -1.5, 1.0, 0.0, 25.0),\n        (200, 0.0, 1.0, 1.0, -1.0, 25.0),\n        (30, 1.0, 0.5, 0.5, 0.5, 50.0),\n    ]\n\n    all_results = []\n    for i, params in enumerate(test_cases):\n        case_result = solve_case(*params, case_idx=i)\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists with no spaces.\n    output_str = \",\".join([str(res).replace(\" \", \"\") for res in all_results])\n    print(f\"[{output_str}]\")\n\ndef solve_case(n, beta0, beta1, sigma, alpha, s, case_idx):\n    \"\"\"\n    Solves a single test case for the Feasible WLS procedure and its\n    sensitivity to an outlier.\n    \"\"\"\n    s0 = 2757\n    delta = 1e-8\n\n    def m_func(x_vec):\n        \"\"\"Stabilized magnitude function.\"\"\"\n        return np.maximum(np.abs(x_vec), delta)\n\n    def run_feasible_wls(x, y):\n        \"\"\"\n        Performs the complete two-stage feasible WLS estimation.\n        \n        Args:\n            x (np.array): Predictor variable vector.\n            y (np.array): Response variable vector.\n\n        Returns:\n            tuple: A tuple containing (alpha_hat, beta1_wls_hat).\n        \"\"\"\n        # Stage 1: OLS to get residuals\n        X_mat = np.vstack([np.ones(len(x)), x]).T\n        try:\n            beta_ols = np.linalg.solve(X_mat.T @ X_mat, X_mat.T @ y)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular matrices, though unlikely here\n            beta_ols = np.linalg.pinv(X_mat.T @ X_mat) @ X_mat.T @ y\n        \n        residuals = y - X_mat @ beta_ols\n\n        # Stage 2: Auxiliary regression to estimate alpha\n        log_sq_residuals = np.log(np.maximum(residuals**2, delta))\n        log_m_x = np.log(m_func(x))\n        Z_mat = np.vstack([np.ones(len(log_m_x)), log_m_x]).T\n        \n        try:\n            gamma = np.linalg.solve(Z_mat.T @ Z_mat, Z_mat.T @ log_sq_residuals)\n        except np.linalg.LinAlgError:\n            gamma = np.linalg.pinv(Z_mat.T @ Z_mat) @ Z_mat.T @ log_sq_residuals\n            \n        alpha_hat = gamma[1]\n\n        # Stage 3: WLS with estimated weights\n        weights = m_func(x)**(-alpha_hat)\n        W_mat = np.diag(weights)\n        \n        try:\n            beta_wls = np.linalg.solve(X_mat.T @ W_mat @ X_mat, X_mat.T @ W_mat @ y)\n        except np.linalg.LinAlgError:\n            beta_wls = np.linalg.pinv(X_mat.T @ W_mat @ X_mat) @ (X_mat.T @ W_mat @ y)\n            \n        beta1_wls_hat = beta_wls[1]\n\n        return alpha_hat, beta1_wls_hat\n\n    # --- Data Generation (No Outlier) ---\n    rng_x = np.random.default_rng(seed=s0 + case_idx)\n    rng_z = np.random.default_rng(seed=s0 + 100 * case_idx)\n    \n    x_original = rng_x.normal(loc=0, scale=1, size=n)\n    z_noise = rng_z.normal(loc=0, scale=1, size=n)\n    \n    eps_original = sigma * m_func(x_original)**(alpha / 2) * z_noise\n    y_original = beta0 + beta1 * x_original + eps_original\n\n    alpha_hat, beta1_wls_hat = run_feasible_wls(x_original, y_original)\n\n    # --- Outlier Injection and Analysis ---\n    x_outlier = x_original.copy()\n    outlier_idx = n // 2\n    # Use ddof=1 for sample standard deviation\n    s_x = np.std(x_original, ddof=1)\n    x_outlier[outlier_idx] = s * s_x\n    \n    # Regenerate responses with the new x vector but same noise draws\n    eps_outlier = sigma * m_func(x_outlier)**(alpha / 2) * z_noise\n    y_outlier = beta0 + beta1 * x_outlier + eps_outlier\n\n    alpha_hat_out, beta1_wls_hat_out = run_feasible_wls(x_outlier, y_outlier)\n\n    # --- Calculate Sensitivity Metrics ---\n    delta_alpha = np.abs(alpha_hat_out - alpha_hat)\n    delta_beta1 = np.abs(beta1_wls_hat_out - beta1_wls_hat)\n\n    # --- Package results for printing ---\n    result_list = [\n        round(alpha_hat, 4),\n        round(beta1_wls_hat, 4),\n        round(alpha_hat_out, 4),\n        round(beta1_wls_hat_out, 4),\n        round(delta_alpha, 4),\n        round(delta_beta1, 4)\n    ]\n\n    return result_list\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While WLS is a primary tool for combating heteroscedasticity, it is not the only one. For certain types of data, particularly when error is multiplicative, transforming the response variable can be an effective alternative that both linearizes the relationship and stabilizes the variance. This exercise places you in the role of a data analyst comparing two competing strategies: a WLS model on the original scale versus an OLS model on the log-transformed response. By evaluating each approach on its residual diagnostics and out-of-sample predictive accuracy, you will learn to critically assess which modeling choice is more appropriate for the problem at hand .",
            "id": "3128007",
            "problem": "You are given a comparison task between two regression strategies in the presence of heteroscedasticity induced by multiplicative noise. The foundational setting is a univariate linear predictor with a strictly positive response generated by a multiplicative noise model. The base facts to use are the definitions of linear regression and its sum-of-squares objective, the notion of heteroscedasticity as non-constant error variance across observations, and the principle that Weighted Least Squares (WLS) minimizes a weighted sum of squared residuals to stabilize variance when it depends on the magnitude of the response. You must implement both strategies, evaluate residual diagnostics, and assess predictive accuracy.\n\nData-generating process for each test case:\n- For each observation $i \\in \\{1, \\dots, n\\}$, draw predictor $x_i$ independently and identically distributed from the uniform distribution on the interval $[0, L]$.\n- Draw noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ independently across $i$.\n- Generate the response as $y_i = \\exp(\\beta_0 + \\beta_1 x_i + \\varepsilon_i)$, which induces multiplicative noise on the original scale and additive noise on the log scale.\n\nModeling strategies to implement:\n1. Log-transform model (Ordinary Least Squares (OLS) on the log scale):\n   - Fit the linear model $\\log y_i = \\gamma_0 + \\gamma_1 x_i + e_i$ on the training split by minimizing the unweighted sum of squares on the log scale.\n   - For predictions on the original response scale, use Duan's smearing estimate: for training residuals $r_j = \\log y_j - (\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_j)$, define the smearing factor $S = \\frac{1}{n_{\\text{train}}}\\sum_{j=1}^{n_{\\text{train}}} \\exp(r_j)$. Predict on the original scale via $\\hat{y}_i^{\\text{log}} = \\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_i) \\cdot S$.\n\n2. Weighted Least Squares (WLS) on the original scale:\n   - Fit the linear model $y_i = \\beta_0 + \\beta_1 x_i + \\eta_i$ on the training split by minimizing a weighted sum of squares with weights $w_i = 1 / y_i^2$.\n\nResidual diagnostics:\n- For each fitted model, compute the training residuals and the fitted values on the modelâ€™s working scale.\n- Define the heteroscedasticity index $H$ as the absolute Pearson correlation between the absolute residuals and the fitted values on the same scale, i.e., $H = \\left|\\operatorname{corr}\\left(|r_i|, \\hat{m}_i\\right)\\right|$, where $r_i$ are residuals and $\\hat{m}_i$ are fitted values. A smaller $H$ indicates better residual diagnostics.\n\nPredictive accuracy:\n- Use the Root Mean Squared Error (RMSE) on the original response scale computed on the test split, i.e., $\\operatorname{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2}$.\n- For the log-transform model, use the smearing back-transformation defined above; for the WLS model, use its fitted predictions directly on the original scale.\n\nTrain/test split:\n- Randomly split the data into training and test sets with a training proportion $p_{\\text{train}}$ for each test case. Use the provided random seed to ensure reproducibility.\n\nComparison rule and final outputs:\n- For residual diagnostics, return an integer $a$ per test case where $a=0$ if the log-transform model has a strictly smaller $H$ than WLS, $a=1$ if WLS has a strictly smaller $H$ than the log-transform model, and $a=2$ if they are tied within a tolerance $\\tau$.\n- For predictive accuracy, return an integer $b$ per test case where $b=0$ if the log-transform model has a strictly smaller $\\operatorname{RMSE}$ than WLS, $b=1$ if WLS has a strictly smaller $\\operatorname{RMSE}$ than the log-transform model, and $b=2$ if they are tied within the same tolerance $\\tau$.\n- Use the tolerance $\\tau = 10^{-8}$.\n\nTest suite parameters:\n- Case $1$: $n=200$, $\\beta_0=1.0$, $\\beta_1=0.8$, $\\sigma=0.5$, $L=3.0$, $p_{\\text{train}}=0.7$, seed $=42$.\n- Case $2$: $n=200$, $\\beta_0=1.0$, $\\beta_1=0.8$, $\\sigma=0.05$, $L=3.0$, $p_{\\text{train}}=0.7$, seed $=202$.\n- Case $3$: $n=60$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\sigma=0.6$, $L=2.0$, $p_{\\text{train}}=0.7$, seed $=7$.\n- Case $4$: $n=400$, $\\beta_0=0.2$, $\\beta_1=0.9$, $\\sigma=0.9$, $L=4.0$, $p_{\\text{train}}=0.7$, seed $=99$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case contributing a two-integer list $[a,b]$ in the same order as the test suite. For example, a valid output format is $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$. No physical units or angles are involved in this problem, and there are no percentage outputs; all computed quantities are dimensionless.",
            "solution": "The problem is valid. It is scientifically grounded in established principles of statistical regression modeling, specifically addressing heteroscedasticity. The problem is well-posed, providing a complete and consistent set of instructions, data generation procedures, and evaluation metrics, ensuring a unique and verifiable solution for each test case. The language is objective and formal.\n\nThe solution methodology involves comparing two regression strategies for a dataset exhibiting heteroscedasticity arising from a multiplicative error structure. For each test case, we first generate the data, then split it into training and testing sets. We then apply two distinct modeling approaches: Ordinary Least Squares (OLS) on log-transformed data and Weighted Least Squares (WLS) on the original data. Finally, we evaluate and compare the models based on a residual diagnostic (heteroscedasticity index) and predictive accuracy (Root Mean Squared Error).\n\n**1. Data Generation and Splitting**\n\nFor each of the $n$ observations, the predictor variable $x_i$ is drawn from a uniform distribution:\n$$x_i \\sim \\mathcal{U}(0, L)$$\nThe response variable $y_i$ is generated from a log-normal model, where the noise is additive on the logarithmic scale:\n$$y_i = \\exp(\\beta_0 + \\beta_1 x_i + \\varepsilon_i), \\quad \\text{where } \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\nThis process implies that on the original scale, the error is multiplicative and the variance of $y_i$ is not constant across different values of $x_i$, a condition known as heteroscedasticity. Specifically, $\\operatorname{Var}(y_i|x_i)$ is proportional to $(\\mathbb{E}[y_i|x_i])^2$.\n\nThe generated dataset of size $n$ is then partitioned into a training set and a test set, according to the proportion $p_{\\text{train}}$. A fixed random seed ensures the reproducibility of this split. Let $n_{\\text{train}} = \\lfloor n \\cdot p_{\\text{train}} \\rfloor$ and $n_{\\text{test}} = n - n_{\\text{train}}$.\n\n**2. Strategy 1: Log-Transform Ordinary Least Squares (OLS)**\n\nThis approach first linearizes the relationship and stabilizes the variance by applying a logarithmic transformation to the response variable. The model fitted to the training data is:\n$$\\log y_i = \\gamma_0 + \\gamma_1 x_i + e_i$$\nThis is a standard linear model, whose parameters $\\boldsymbol{\\gamma} = [\\gamma_0, \\gamma_1]^T$ are estimated using OLS by minimizing the sum of squared errors, $\\sum e_i^2$. The solution is given by the normal equations:\n$$\\hat{\\boldsymbol{\\gamma}} = (\\mathbf{X}_{\\text{train}}^T \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^T \\mathbf{z}_{\\text{train}}$$\nwhere $\\mathbf{z}_{\\text{train}}$ is the vector of log-transformed training responses, $(\\log y_1, \\dots, \\log y_{n_{\\text{train}}})^T$, and $\\mathbf{X}_{\\text{train}}$ is the $n_{\\text{train}} \\times 2$ design matrix with a column of ones and a column of $x_i$ values.\n\nFor residual diagnostics, we use the training data. The fitted values are $\\hat{\\mathbf{z}}_{\\text{train}} = \\mathbf{X}_{\\text{train}} \\hat{\\boldsymbol{\\gamma}}$, and the residuals are $\\mathbf{r}_{\\text{log}} = \\mathbf{z}_{\\text{train}} - \\hat{\\mathbf{z}}_{\\text{train}}$. The heteroscedasticity index is calculated as:\n$$H_{\\text{log}} = \\left|\\operatorname{corr}\\left(|\\mathbf{r}_{\\text{log}}|, \\hat{\\mathbf{z}}_{\\text{train}}\\right)\\right|$$\n\nFor prediction, we must back-transform the predictions from the log scale to the original scale. A naive back-transformation $\\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_i)$ would be biased. We use Duan's smearing estimate to correct for this. The smearing factor $S$ is computed from the training residuals:\n$$S = \\frac{1}{n_{\\text{train}}}\\sum_{j=1}^{n_{\\text{train}}} \\exp(r_{\\text{log}, j})$$\nPredictions for the test set on the original scale are then:\n$$\\hat{y}_i^{\\text{log}} = \\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_{\\text{test}, i}) \\cdot S$$\nThe predictive accuracy is measured by the Root Mean Squared Error (RMSE) on the test set:\n$$\\operatorname{RMSE}_{\\text{log}} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_i^{\\text{log}})^2}$$\n\n**3. Strategy 2: Weighted Least Squares (WLS)**\n\nThis strategy models the linear relationship directly on the original scale, $y_i = \\beta_0 + \\beta_1 x_i + \\eta_i$, but accounts for heteroscedasticity by assigning weights to each observation that are inversely proportional to its variance. Given that $\\operatorname{Var}(y_i|x_i)$ is approximately proportional to $y_i^2$, we use the weights $w_i = 1/y_i^2$. WLS minimizes the weighted sum of squared residuals, $\\sum w_i \\eta_i^2$. The parameter estimate $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$ is found by solving:\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}_{\\text{train}}^T \\mathbf{W} \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^T \\mathbf{W} \\mathbf{y}_{\\text{train}}$$\nwhere $\\mathbf{W}$ is a diagonal matrix with the weights $w_i = 1/y_{\\text{train}, i}^2$ on its diagonal.\n\nThe residual diagnostics are performed on the original scale. The fitted values are $\\hat{\\mathbf{y}}_{\\text{wls, train}} = \\mathbf{X}_{\\text{train}} \\hat{\\boldsymbol{\\beta}}$, and the residuals are $\\mathbf{r}_{\\text{wls}} = \\mathbf{y}_{\\text{train}} - \\hat{\\mathbf{y}}_{\\text{wls, train}}$. The heteroscedasticity index is:\n$$H_{\\text{wls}} = \\left|\\operatorname{corr}\\left(|\\mathbf{r}_{\\text{wls}}|, \\hat{\\mathbf{y}}_{\\text{wls, train}}\\right)\\right|$$\n\nPredictions for the test set are made directly using the fitted model:\n$$\\hat{y}_i^{\\text{wls}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{\\text{test}, i}$$\nThe corresponding RMSE on the test set is:\n$$\\operatorname{RMSE}_{\\text{wls}} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_i^{\\text{wls}})^2}$$\n\n**4. Comparison**\n\nFor each test case, the two models are compared on two criteria using a tolerance $\\tau = 10^{-8}$.\n- **Residual Diagnostics (output $a$)**:\n  - If $H_{\\text{log}} < H_{\\text{wls}} - \\tau$, $a=0$.\n  - If $H_{\\text{wls}} < H_{\\text{log}} - \\tau$, $a=1$.\n  - Otherwise, $a=2$.\n- **Predictive Accuracy (output $b$)**:\n  - If $\\operatorname{RMSE}_{\\text{log}} < \\operatorname{RMSE}_{\\text{wls}} - \\tau$, $b=0$.\n  - If $\\operatorname{RMSE}_{\\text{wls}} < \\operatorname{RMSE}_{\\text{log}} - \\tau$, $b=1$.\n  - Otherwise, $b=2$.\n\nThe final output is a list of $[a, b]$ pairs for all specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: n=200, beta0=1.0, beta1=0.8, sigma=0.5, L=3.0, p_train=0.7, seed=42\n        (200, 1.0, 0.8, 0.5, 3.0, 0.7, 42),\n        # Case 2: n=200, beta0=1.0, beta1=0.8, sigma=0.05, L=3.0, p_train=0.7, seed=202\n        (200, 1.0, 0.8, 0.05, 3.0, 0.7, 202),\n        # Case 3: n=60, beta0=0.5, beta1=1.2, sigma=0.6, L=2.0, p_train=0.7, seed=7\n        (60, 0.5, 1.2, 0.6, 2.0, 0.7, 7),\n        # Case 4: n=400, beta0=0.2, beta1=0.9, sigma=0.9, L=4.0, p_train=0.7, seed=99\n        (400, 0.2, 0.9, 0.9, 4.0, 0.7, 99),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(*case)\n        results.append(result)\n\n    output_str = \",\".join([str(res).replace(\" \", \"\") for res in results])\n    print(f\"[{output_str}]\")\n\ndef _solve_case(n, beta_0, beta_1, sigma, L, p_train, seed, tau=1e-8):\n    \"\"\"\n    Solves a single test case for the heteroscedasticity problem.\n    \"\"\"\n    # 1. Generate Data\n    rng = np.random.default_rng(seed)\n    x = rng.uniform(0, L, n)\n    # Note: np.random.normal's scale parameter is standard deviation (sigma), not variance (sigma^2)\n    eps = rng.normal(0, sigma, n)\n    y = np.exp(beta_0 + beta_1 * x + eps)\n\n    # 2. Split Data\n    n_train = int(np.floor(n * p_train))\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    train_indices = indices[:n_train]\n    test_indices = indices[n_train:]\n\n    x_train, y_train = x[train_indices], y[train_indices]\n    x_test, y_test = x[test_indices], y[test_indices]\n    \n    # --- Strategy 1: Log-transform model (OLS on log scale) ---\n    log_y_train = np.log(y_train)\n    X_train = np.vstack([np.ones(n_train), x_train]).T\n    \n    # Fit using OLS normal equations\n    gamma_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ log_y_train\n    \n    # Residual diagnostics\n    log_y_fit_train = X_train @ gamma_hat\n    res_log = log_y_train - log_y_fit_train\n    \n    # Correlation for H_log. Handle constant array case for robustness.\n    if np.std(np.abs(res_log))  1e-12 or np.std(log_y_fit_train)  1e-12:\n        corr_log = 0.0\n    else:\n        corr_log = np.corrcoef(np.abs(res_log), log_y_fit_train)[0, 1]\n    H_log = np.abs(corr_log)\n    \n    # Prediction and RMSE with Duan's smearing\n    smearing_factor = np.mean(np.exp(res_log))\n    n_test = len(x_test)\n    X_test = np.vstack([np.ones(n_test), x_test]).T\n    log_y_pred_test = X_test @ gamma_hat\n    y_pred_log = np.exp(log_y_pred_test) * smearing_factor\n    RMSE_log = np.sqrt(np.mean((y_test - y_pred_log)**2))\n\n    # --- Strategy 2: Weighted Least Squares (WLS) on original scale ---\n    weights = 1 / y_train**2\n    W_diag = np.diag(weights)\n    \n    # Fit using WLS normal equations\n    beta_hat = np.linalg.inv(X_train.T @ W_diag @ X_train) @ X_train.T @ W_diag @ y_train\n\n    # Residual diagnostics\n    y_fit_wls_train = X_train @ beta_hat\n    res_wls = y_train - y_fit_wls_train\n\n    # Correlation for H_wls. Handle constant array case for robustness.\n    if np.std(np.abs(res_wls))  1e-12 or np.std(y_fit_wls_train)  1e-12:\n        corr_wls = 0.0\n    else:\n        corr_wls = np.corrcoef(np.abs(res_wls), y_fit_wls_train)[0, 1]\n    H_wls = np.abs(corr_wls)\n\n    # Prediction and RMSE\n    y_pred_wls = X_test @ beta_hat\n    RMSE_wls = np.sqrt(np.mean((y_test - y_pred_wls)**2))\n\n    # --- Comparison ---\n    # a: Residual diagnostics comparison\n    if H_log  H_wls - tau:\n        a = 0\n    elif H_wls  H_log - tau:\n        a = 1\n    else:\n        a = 2\n        \n    # b: Predictive accuracy comparison\n    if RMSE_log  RMSE_wls - tau:\n        b = 0\n    elif RMSE_wls  RMSE_log - tau:\n        b = 1\n    else:\n        b = 2\n\n    return [a, b]\n\nsolve()\n```"
        }
    ]
}