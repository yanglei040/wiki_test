{
    "hands_on_practices": [
        {
            "introduction": "The \"kernel trick\" can feel abstract, allowing us to compute in high dimensions without ever visiting them. This exercise pulls back the curtain by asking you to derive the explicit feature map $\\phi(\\mathbf{x})$ that corresponds to a given polynomial kernel. By making the implicit mapping explicit, you will gain a concrete understanding of how a simple function $K(\\mathbf{x}, \\mathbf{z})$ in input space corresponds to an inner product $\\langle\\phi(\\mathbf{x}), \\phi(\\mathbf{z})\\rangle$ in a much richer feature space .",
            "id": "90260",
            "problem": "In the field of computational materials science, kernel-based machine learning methods, such as Support Vector Machines (SVMs) and Kernel Ridge Regression (KRR), are powerful tools for predicting material properties from a set of descriptors. These methods rely on a kernel function, $K(\\mathbf{x}, \\mathbf{z})$, which computes a generalized similarity between two materials represented by their descriptor vectors, $\\mathbf{x}$ and $\\mathbf{z}$.\n\nA key concept is the \"kernel trick,\" which allows us to work in a high-dimensional feature space without explicitly computing the coordinates of the data in that space. The kernel function is equivalent to an inner product in a feature space $\\mathcal{F}$: $K(\\mathbf{x}, \\mathbf{z}) = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{z}) \\rangle_{\\mathcal{F}}$, where $\\phi$ is a mapping from the input space to the feature space $\\mathcal{F}$.\n\nConsider a model for a property of a 2D material, where each material is described by a two-component descriptor vector $\\mathbf{x} = [x_1, x_2]^T$. An anisotropic, inhomogeneous polynomial kernel is proposed to capture non-linear relationships and differing importance between the descriptors:\n$$\nK(\\mathbf{x}, \\mathbf{z}) = (\\alpha x_1 z_1 + \\beta x_2 z_2 + \\gamma)^2\n$$\nHere, $\\alpha$, $\\beta$, and $\\gamma$ are positive real-valued hyperparameters of the model.\n\nThis kernel corresponds to an implicit mapping $\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^N$ for some dimension $N$. Your task is to make this mapping explicit and use it to derive a property of the feature space.\n\n**Derive the squared L2-norm of the feature vector, $\\|\\phi(\\mathbf{x})\\|^2$, associated with this kernel. Your derivation must proceed by first finding an explicit, valid representation for the feature map vector $\\phi(\\mathbf{x})$.**",
            "solution": "The problem asks for the squared L2-norm of the feature vector $\\phi(\\mathbf{x})$ corresponding to the kernel $K(\\mathbf{x}, \\mathbf{z}) = (\\alpha x_1 z_1 + \\beta x_2 z_2 + \\gamma)^2$. The derivation must involve finding the explicit form of $\\phi(\\mathbf{x})$.\n\nThe defining property of the feature map $\\phi$ is that the kernel function is the inner product of the feature vectors:\n$$\nK(\\mathbf{x}, \\mathbf{z}) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{z})\n$$\nThe squared L2-norm of the feature vector is a special case of the kernel evaluation, where both arguments are the same:\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}) = K(\\mathbf{x}, \\mathbf{x})\n$$\nWhile this identity provides a shortcut to the final answer, the problem requires an explicit derivation through the feature map $\\phi(\\mathbf{x})$.\n\nFirst, let's find the feature map $\\phi(\\mathbf{x})$. We can rewrite the kernel to match the form of a standard inhomogeneous polynomial kernel. Let's introduce a transformed coordinate system:\n$$\nx'_1 = \\sqrt{\\alpha} x_1\n$$\n$$\nx'_2 = \\sqrt{\\beta} x_2\n$$\nLet $\\mathbf{x'} = [x'_1, x'_2]^T$. Similarly, $z'_1 = \\sqrt{\\alpha} z_1$ and $z'_2 = \\sqrt{\\beta} z_2$, so $\\mathbf{z'} = [z'_1, z'_2]^T$.\n\nSubstituting these into the kernel expression, we get:\n$$\nK(\\mathbf{x}, \\mathbf{z}) = ((\\sqrt{\\alpha}x_1)(\\sqrt{\\alpha}z_1) + (\\sqrt{\\beta}x_2)(\\sqrt{\\beta}z_2) + \\gamma)^2 = (x'_1 z'_1 + x'_2 z'_2 + \\gamma)^2\n$$\nThis is the standard inhomogeneous polynomial kernel of degree 2 in the 2D space of $\\mathbf{x'}$, with an offset $c=\\gamma$:\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (\\mathbf{x'}^T\\mathbf{z'} + \\gamma)^2\n$$\nTo find the feature map $\\phi(\\mathbf{x'})$, we expand the kernel expression:\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (\\mathbf{x'}^T\\mathbf{z'} + \\gamma)^2 = (\\mathbf{x'}^T\\mathbf{z'})^2 + 2\\gamma(\\mathbf{x'}^T\\mathbf{z'}) + \\gamma^2\n$$\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (x'_1 z'_1 + x'_2 z'_2)^2 + 2\\gamma(x'_1 z'_1 + x'_2 z'_2) + \\gamma^2\n$$\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (x'_1)^2(z'_1)^2 + (x'_2)^2(z'_2)^2 + 2x'_1 x'_2 z'_1 z'_2 + 2\\gamma x'_1 z'_1 + 2\\gamma x'_2 z'_2 + \\gamma^2\n$$\nWe need to write this as a sum of products $\\sum_i \\phi_i(\\mathbf{x'}) \\phi_i(\\mathbf{z'})$. We can identify the components by inspection:\n- Term $(x'_1)^2(z'_1)^2$: suggests a feature component $\\phi_i(\\mathbf{x'}) = (x'_1)^2$.\n- Term $(x'_2)^2(z'_2)^2$: suggests a feature component $\\phi_j(\\mathbf{x'}) = (x'_2)^2$.\n- Term $2x'_1 x'_2 z'_1 z'_2$: suggests $\\phi_k(\\mathbf{x'}) = \\sqrt{2}x'_1 x'_2$. Then $\\phi_k(\\mathbf{x'}) \\phi_k(\\mathbf{z'}) = 2x'_1 x'_2 z'_1 z'_2$.\n- Term $2\\gamma x'_1 z'_1$: suggests $\\phi_l(\\mathbf{x'}) = \\sqrt{2\\gamma}x'_1$. Then $\\phi_l(\\mathbf{x'}) \\phi_l(\\mathbf{z'}) = 2\\gamma x'_1 z'_1$.\n- Term $2\\gamma x'_2 z'_2$: suggests $\\phi_m(\\mathbf{x'}) = \\sqrt{2\\gamma}x'_2$. Then $\\phi_m(\\mathbf{x'}) \\phi_m(\\mathbf{z'}) = 2\\gamma x'_2 z'_2$.\n- Term $\\gamma^2$: suggests $\\phi_p(\\mathbf{x'}) = \\gamma$. Then $\\phi_p(\\mathbf{x'}) \\phi_p(\\mathbf{z'}) = \\gamma^2$.\n\nCombining these, a valid feature map for $\\mathbf{x'}$ is a 6-dimensional vector:\n$$\n\\phi(\\mathbf{x'}) = \\begin{pmatrix} (x'_1)^2 \\\\ (x'_2)^2 \\\\ \\sqrt{2}x'_1 x'_2 \\\\ \\sqrt{2\\gamma}x'_1 \\\\ \\sqrt{2\\gamma}x'_2 \\\\ \\gamma \\end{pmatrix}\n$$\nNow, we substitute back the original coordinates $x_1, x_2$:\n$$\nx'_1 = \\sqrt{\\alpha} x_1, \\quad x'_2 = \\sqrt{\\beta} x_2\n$$\nThis gives the explicit feature map $\\phi(\\mathbf{x})$:\n$$\n\\phi(\\mathbf{x}) = \\begin{pmatrix} (\\sqrt{\\alpha}x_1)^2 \\\\ (\\sqrt{\\beta}x_2)^2 \\\\ \\sqrt{2}(\\sqrt{\\alpha}x_1)(\\sqrt{\\beta}x_2) \\\\ \\sqrt{2\\gamma}(\\sqrt{\\alpha}x_1) \\\\ \\sqrt{2\\gamma}(\\sqrt{\\beta}x_2) \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} \\alpha x_1^2 \\\\ \\beta x_2^2 \\\\ \\sqrt{2\\alpha\\beta} x_1 x_2 \\\\ \\sqrt{2\\alpha\\gamma} x_1 \\\\ \\sqrt{2\\beta\\gamma} x_2 \\\\ \\gamma \\end{pmatrix}\n$$\nThe problem asks for the squared L2-norm of this vector, $\\|\\phi(\\mathbf{x})\\|^2 = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x})$. We compute this by summing the squares of its components:\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = (\\alpha x_1^2)^2 + (\\beta x_2^2)^2 + (\\sqrt{2\\alpha\\beta} x_1 x_2)^2 + (\\sqrt{2\\alpha\\gamma} x_1)^2 + (\\sqrt{2\\beta\\gamma} x_2)^2 + \\gamma^2\n$$\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = \\alpha^2 x_1^4 + \\beta^2 x_2^4 + 2\\alpha\\beta x_1^2 x_2^2 + 2\\alpha\\gamma x_1^2 + 2\\beta\\gamma x_2^2 + \\gamma^2\n$$\nThis expression can be recognized as the expansion of a squared trinomial. Let's group the terms to make this clear:\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = (\\alpha^2 x_1^4) + (\\beta^2 x_2^4) + \\gamma^2 + 2(\\alpha\\beta x_1^2 x_2^2) + 2(\\alpha\\gamma x_1^2) + 2(\\beta\\gamma x_2^2)\n$$\nThis is the expansion of $(A+B+C)^2 = A^2+B^2+C^2+2AB+2AC+2BC$, with $A = \\alpha x_1^2$, $B = \\beta x_2^2$, and $C = \\gamma$.\nTherefore, the expression simplifies to:\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = (\\alpha x_1^2 + \\beta x_2^2 + \\gamma)^2\n$$\nAs a check, we can use the identity $\\|\\phi(\\mathbf{x})\\|^2 = K(\\mathbf{x}, \\mathbf{x})$.\n$$\nK(\\mathbf{x}, \\mathbf{x}) = (\\alpha x_1 x_1 + \\beta x_2 x_2 + \\gamma)^2 = (\\alpha x_1^2 + \\beta x_2^2 + \\gamma)^2\n$$\nThe result matches, confirming the derivation.",
            "answer": "$$\n\\boxed{(\\alpha x_1^2 + \\beta x_2^2 + \\gamma)^2}\n$$"
        },
        {
            "introduction": "Now that we understand a kernel represents an inner product in a feature space, we can explore *why* this is so powerful. This practice challenges you to implement and compare a standard linear perceptron with a kernelized version on the classic XOR problem, a task impossible for linear classifiers . Through coding, you will witness firsthand how the kernel trick enables a linear algorithm to learn a fundamentally non-linear decision boundary.",
            "id": "3183909",
            "problem": "You will implement and compare two learning algorithms for binary classification: a standard perceptron in input space and a kernelized perceptron that uses a polynomial kernel to implicitly work in a higher-dimensional feature space. The goal is to demonstrate how the kernel trick enables the perceptron to solve the exclusive-or relation, and to quantify convergence behavior.\n\nStart from the following fundamental base:\n- A dataset consists of input vectors $\\mathbf{x} \\in \\mathbb{R}^d$ and labels $y \\in \\{-1,+1\\}$.\n- A linear classifier in input space uses a decision function $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$.\n- The perceptron learning algorithm performs updates of the form $\\mathbf{w} \\leftarrow \\mathbf{w} + y \\mathbf{x}$ and $b \\leftarrow b + y$ whenever an example $(\\mathbf{x},y)$ is misclassified, cycling deterministically through the training set until it converges or a maximum number of epochs is reached.\n- A kernel function $k(\\mathbf{x},\\mathbf{z})$ corresponds to an inner product in some feature space via a feature map $\\Phi$, that is $k(\\mathbf{x},\\mathbf{z}) = \\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{z}) \\rangle$.\n- The polynomial kernel of degree $d$ with offset $c$ is $k(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z} + c)^d$. You must use $c = 1$ throughout this problem.\n\nDefine the following tasks your program must perform:\n1. Implement a deterministic perceptron in input space that:\n   - Uses labels $y \\in \\{-1,+1\\}$.\n   - Augments each input by a constant feature to incorporate the bias term $b$ as part of the weight vector, so that updates become a single vector update.\n   - Initializes the weight vector to the zero vector.\n   - Scans the training samples in a fixed order $(i=0,1,\\dots,n-1)$ in each epoch and applies the update whenever $y_i (\\mathbf{w}^\\top \\mathbf{x}_i) \\le 0$.\n   - Uses the decision rule $\\operatorname{sign}(s)$ that returns $+1$ when $s \\ge 0$ and $-1$ otherwise.\n   - Returns the total number of updates performed up to and including the epoch in which it first achieves zero training error. If it fails to converge within the given maximum number of epochs, return $-1$.\n\n2. Derive and implement a kernelized perceptron that uses only kernel evaluations with the polynomial kernel $k(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z} + 1)^d$:\n   - Represent the classifier in terms of coefficients $\\alpha_i \\ge 0$ assigned to training examples, with prediction $f(\\mathbf{x}) = \\operatorname{sign}\\left(\\sum_{j=1}^n \\alpha_j y_j k(\\mathbf{x}_j,\\mathbf{x})\\right)$.\n   - Initialize all coefficients $\\alpha_i$ to zero.\n   - On a misclassification of $(\\mathbf{x}_i,y_i)$, update $\\alpha_i \\leftarrow \\alpha_i + 1$.\n   - Iterate deterministically in the fixed order $(i=0,1,\\dots,n-1)$ per epoch and use the same decision rule and stopping criterion as above.\n   - Return the total number of updates until convergence, or $-1$ if no convergence in the allotted epochs.\n\nConstruct and use the following datasets:\n- XOR dataset in $\\mathbb{R}^2$: inputs $\\mathbf{x} \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$ with labels $y=+1$ if exactly one coordinate equals $1$ and $y=-1$ otherwise. Explicitly, the label vector is $[-1,+1,+1,-1]$ in the order given.\n- AND dataset in $\\mathbb{R}^2$: same inputs but labels $y=+1$ only for $(1,1)$ and $y=-1$ otherwise. Explicitly, the label vector is $[-1,-1,-1,+1]$ in the order given.\n\nAngle units are not used in this problem. No physical units are involved.\n\nTest suite and required output:\n- Run the following four test cases, each defined by the dataset name, the polynomial degree $d$, and the maximum number of epochs:\n  1. XOR, $d=2$, $\\text{max\\_epochs}=50$.\n  2. XOR, $d=1$, $\\text{max\\_epochs}=50$.\n  3. AND, $d=2$, $\\text{max\\_epochs}=50$.\n  4. XOR, $d=3$, $\\text{max\\_epochs}=50$.\n- For each test case, compute two integers: the number of updates to convergence for the input-space perceptron and the number of updates to convergence for the kernelized perceptron with the specified degree. If a method fails to converge within the maximum number of epochs, report $-1$ for that method.\n- Your program should produce a single line of output containing a list of four elements, each element itself being a two-element list in the order [linear_updates, kernel_updates] for the corresponding test case, with no spaces. For example: \"[[a,b],[c,d],[e,f],[g,h]]\" where each letter stands for an integer as specified above.\n\nScientific realism and derivation expectations:\n- Justify in your solution why the XOR dataset is not linearly separable in input space and why it becomes linearly separable in an appropriate polynomial feature space, thereby explaining why the kernelized perceptron converges with degree $d \\ge 2$ while the linear perceptron does not.\n- Base your reasoning strictly on definitions of linear separability, the perceptron update rule, and the definition of a kernel as an inner product in a feature space.",
            "solution": "The problem requires the implementation and comparison of two binary classification algorithms: a standard perceptron operating in the input space and a kernelized perceptron using a polynomial kernel. The analysis will focus on their ability to solve linearly separable and non-linearly separable problems, exemplified by the AND and XOR datasets, respectively.\n\n### 1. The Standard Perceptron in Input Space\n\nThe standard perceptron algorithm learns a linear discriminant function to separate two classes. For an input vector $\\mathbf{x} \\in \\mathbb{R}^d$, the decision function is $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector and $b \\in \\mathbb{R}$ is the bias. The problem specifies that $\\operatorname{sign}(s)$ returns $+1$ for $s \\ge 0$ and $-1$ for $s < 0$.\n\nTo simplify the learning algorithm, the bias $b$ can be incorporated into the weight vector. This is achieved by augmenting each input vector $\\mathbf{x}$ with a constant feature, typically $1$, to form an augmented vector $\\mathbf{x}' = [\\mathbf{x}^\\top, 1]^\\top \\in \\mathbb{R}^{d+1}$. Similarly, the augmented weight vector is $\\mathbf{w}' = [\\mathbf{w}^\\top, b]^\\top \\in \\mathbb{R}^{d+1}$. The decision function then becomes $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}'^\\top \\mathbf{x}')$.\n\nThe learning process is iterative. The algorithm cycles through the training samples $(\\mathbf{x}_i, y_i)$ for $i \\in \\{0, \\dots, n-1\\}$, where $y_i \\in \\{-1, +1\\}$. The weight vector $\\mathbf{w}'$ is initialized to the zero vector. For each sample, a misclassification occurs if the sign of the model's output does not match the true label. The condition for misclassification, as specified, is $y_i(\\mathbf{w}'^\\top \\mathbf{x}'_i) \\le 0$. Upon misclassification, the weight vector is updated according to the rule:\n$$\n\\mathbf{w}' \\leftarrow \\mathbf{w}' + y_i \\mathbf{x}'_i\n$$\nThe algorithm proceeds epoch by epoch, where one epoch consists of a full pass through the training data. According to the Perceptron Convergence Theorem, if a training dataset is linearly separable, the algorithm is guaranteed to find a separating hyperplane in a finite number of updates. If the data is not linearly separable, the algorithm will not converge and will cycle indefinitely.\n\n### 2. The Kernelized Perceptron\n\nThe kernel trick allows linear classifiers like the perceptron to learn non-linear decision boundaries. The core idea is to implicitly map the input data $\\mathbf{x}$ into a higher-dimensional feature space $\\mathcal{F}$ via a map $\\Phi: \\mathbb{R}^d \\to \\mathcal{F}$, and then apply a linear classifier in $\\mathcal{F}$.\n\nThe weight vector $\\mathbf{W}$ in the feature space $\\mathcal{F}$ can be expressed in its \"dual form\" as a linear combination of the mapped training samples:\n$$\n\\mathbf{W} = \\sum_{j=1}^n \\alpha_j y_j \\Phi(\\mathbf{x}_j)\n$$\nwhere $\\alpha_j$ are coefficients, initialized to $0$. The decision function for a new point $\\mathbf{x}$ is $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{W}^\\top \\Phi(\\mathbf{x}))$. Substituting the dual form of $\\mathbf{W}$ gives:\n$$\nf(\\mathbf{x}) = \\operatorname{sign}\\left( \\left( \\sum_{j=1}^n \\alpha_j y_j \\Phi(\\mathbf{x}_j) \\right)^\\top \\Phi(\\mathbf{x}) \\right) = \\operatorname{sign}\\left( \\sum_{j=1}^n \\alpha_j y_j \\langle \\Phi(\\mathbf{x}_j), \\Phi(\\mathbf{x}) \\rangle \\right)\n$$\nThe key insight of the kernel trick is that the inner product in the feature space, $\\langle \\Phi(\\mathbf{x}_j), \\Phi(\\mathbf{x}) \\rangle$, can be computed efficiently by a kernel function $k(\\mathbf{x}_j, \\mathbf{x})$ in the original input space. The decision function is expressed entirely in terms of kernel evaluations:\n$$\nf(\\mathbf{x}) = \\operatorname{sign}\\left(\\sum_{j=1}^n \\alpha_j y_j k(\\mathbf{x}_j, \\mathbf{x})\\right)\n$$\nThe update rule is also derived in this dual representation. An update in the primal feature space, $\\mathbf{W} \\leftarrow \\mathbf{W} + y_i \\Phi(\\mathbf{x}_i)$, corresponds to incrementing the coefficient $\\alpha_i$ of the misclassified point $(\\mathbf{x}_i, y_i)$. The update rule is simply $\\alpha_i \\leftarrow \\alpha_i + 1$.\n\nThe problem specifies the polynomial kernel of degree $d$ with offset $c=1$:\n$$\nk(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z} + 1)^d\n$$\nThe offset $c=1$ is crucial, as it implicitly handles the bias term. For example, for $d=2$ and $\\mathbf{x} \\in \\mathbb{R}^2$, the kernel $k(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top\\mathbf{z}+1)^2$ corresponds to an inner product in a $6$-dimensional feature space with a basis like $(1, \\sqrt{2}x_1, \\sqrt{2}x_2, x_1^2, x_2^2, \\sqrt{2}x_1x_2)$. The constant feature $1$ in the feature map allows the learning of a bias term.\n\n### 3. Analysis of Datasets and Separability\n\n**AND Dataset**: The inputs are $\\{(0,0), (0,1), (1,0), (1,1)\\}$ with labels $\\{-1, -1, -1, +1\\}$. This dataset is linearly separable in $\\mathbb{R}^2$. For example, the line $x_1 + x_2 - 1.5 = 0$ perfectly separates the point $(1,1)$ from the other three. Since the data is linearly separable, the Perceptron Convergence Theorem guarantees that both the standard perceptron and the kernelized perceptron (for any degree $d \\ge 1$) will converge.\n\n**XOR Dataset**: The inputs are the same, but with labels $\\{-1, +1, +1, -1\\}$.\n- **In Input Space $\\mathbb{R}^2$**: This dataset is not linearly separable. A geometric proof is illuminating: the convex hull of the positive-class points, which is the line segment connecting $(0,1)$ and $(1,0)$, intersects the convex hull of the negative-class points, the line segment connecting $(0,0)$ and $(1,1)$. Since their convex hulls intersect, no line can separate the two sets. As a result, the standard perceptron is not guaranteed to converge and will fail to find a solution.\n- **In Polynomial Feature Space**: The kernel trick can overcome this limitation. Consider the polynomial kernel with degree $d=2$. This kernel implicitly maps the data to a higher-dimensional space that includes, among others, a feature proportional to the product $x_1x_2$. Let's examine the separability in a simplified feature space with coordinates $(x_1, x_2, x_1x_2)$:\n  - $(0,0) \\to (0,0,0)$, label $-1$\n  - $(0,1) \\to (0,1,0)$, label $+1$\n  - $(1,0) \\to (1,0,0)$, label $+1$\n  - $(1,1) \\to (1,1,1)$, label $-1$\nIn this $3$D space, the points are linearly separable. For example, a separating hyperplane is given by the equation $z_1+z_2 - 2z_3 - 1 = 0$. Let's verify this:\n  - $(0,0,0) \\to 0+0-0-1 = -1$. Label is $-1$. $y_i(\\text{score}) > 0$. Correct.\n  - $(0,1,0) \\to 0+1-0-1 = 0$. Label is $+1$. $y_i(\\text{score}) \\ge 0$. Correct.\n  - $(1,0,0) \\to 1+0-0-1 = 0$. Label is $+1$. $y_i(\\text{score}) \\ge 0$. Correct.\n  - $(1,1,1) \\to 1+1-2-1 = -1$. Label is $-1$. $y_i(\\text{score}) > 0$. Correct.\nSince the XOR dataset becomes linearly separable in the feature space induced by the polynomial kernel with $d \\ge 2$, the kernelized perceptron is guaranteed to converge. For $d=1$, the kernel is $k(\\mathbf{x},\\mathbf{z}) = \\mathbf{x}^\\top\\mathbf{z}+1$, which is equivalent to a linear classifier in the input space, so it will fail for the XOR problem. For $d=3$, the feature space is even richer and the data remains separable, so convergence is also guaranteed.\n\n### 4. Expected Results\n\nBased on this analysis, the expected outcomes for the test cases are:\n1.  **XOR, $d=2$**: Linear perceptron fails (returns $-1$), Kernel perceptron converges.\n2.  **XOR, $d=1$**: Both fail (return $-1$), as the $d=1$ kernel corresponds to a linear classifier.\n3.  **AND, $d=2$**: Both converge, as the AND dataset is linearly separable.\n4.  **XOR, $d=3$**: Linear perceptron fails (returns $-1$), Kernel perceptron converges.\n\nThe implementation will now execute these algorithms and report the number of updates until convergence.",
            "answer": "```python\nimport numpy as np\n\ndef standard_perceptron(X, y, max_epochs):\n    \"\"\"\n    Implements the standard perceptron algorithm in input space.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Augment X to handle bias term b\n    X_aug = np.c_[X, np.ones(n_samples)]\n    w = np.zeros(n_features + 1)\n    total_updates = 0\n\n    for _ in range(max_epochs):\n        updates_in_epoch = 0\n        for i in range(n_samples):\n            score = X_aug[i] @ w\n            # Update on misclassification\n            if y[i] * score <= 0:\n                w += y[i] * X_aug[i]\n                total_updates += 1\n                updates_in_epoch += 1\n        \n        # Check for convergence\n        if updates_in_epoch == 0:\n            return total_updates\n            \n    # Failed to converge\n    return -1\n\ndef kernel_perceptron(X, y, degree, max_epochs):\n    \"\"\"\n    Implements the kernelized perceptron algorithm.\n    \"\"\"\n    n_samples, _ = X.shape\n    alphas = np.zeros(n_samples)\n    \n    # Pre-compute the Gram matrix using the polynomial kernel\n    gram_matrix = (X @ X.T + 1) ** degree\n    total_updates = 0\n\n    for _ in range(max_epochs):\n        updates_in_epoch = 0\n        for i in range(n_samples):\n            # The decision function uses kernel evaluations\n            score = (alphas * y) @ gram_matrix[:, i]\n            # Update on misclassification\n            if y[i] * score <= 0:\n                alphas[i] += 1\n                total_updates += 1\n                updates_in_epoch += 1\n        \n        # Check for convergence\n        if updates_in_epoch == 0:\n            return total_updates\n            \n    # Failed to converge\n    return -1\n\ndef solve():\n    \"\"\"\n    Runs the defined test suite and prints the formatted results.\n    \"\"\"\n    # Define datasets\n    X_xor = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n    y_xor = np.array([-1., 1., 1.,-1.])\n    \n    X_and = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n    y_and = np.array([-1., -1., -1., 1.])\n    \n    datasets = {\n        'XOR': (X_xor, y_xor),\n        'AND': (X_and, y_and)\n    }\n    \n    # Define test cases\n    test_cases = [\n        # (dataset_name, polynomial_degree, max_epochs)\n        ('XOR', 2, 50),\n        ('XOR', 1, 50),\n        ('AND', 2, 50),\n        ('XOR', 3, 50),\n    ]\n\n    results = []\n    for d_name, degree, max_e in test_cases:\n        X, y = datasets[d_name]\n        \n        # Run standard perceptron\n        linear_updates = standard_perceptron(X, y, max_e)\n        \n        # Run kernelized perceptron\n        kernel_updates = kernel_perceptron(X, y, degree, max_e)\n\n        results.append([linear_updates, kernel_updates])\n        \n    # Format the output string to match the problem specification \"[[a,b],[c,d],...]\"\n    output_str = f\"[{','.join([str(r).replace(' ', '') for r in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The power of the kernel trick comes with a crucial choice: which kernel should one use? This final exercise moves into the practical art of model selection, demonstrating how a kernel's intrinsic properties can determine its success or failure . By comparing a \"global\" polynomial kernel with a \"local\" Radial Basis Function (RBF) kernel on carefully constructed data, you will discover why matching the kernel's nature to the data's structure is critical for generalization.",
            "id": "3183944",
            "problem": "You must write a complete program that compares Kernel Ridge Regression (KRR) with two different kernels—a Gaussian Radial Basis Function (RBF) kernel and a polynomial kernel—across three deterministic classification tasks. The goal is to demonstrate a case where the choice of kernel critically affects performance: specifically, to construct data with a global structure so that the polynomial kernel succeeds while the RBF kernel fails due to its locality, and to analyze coverage cases.\n\nUse the following fundamental base for your derivations and implementation:\n- KRR optimizes the Tikhonov-regularized empirical risk\n$$\n\\min_{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^{n} (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2,\n$$\nwhere $\\mathcal{H}$ is a Reproducing Kernel Hilbert Space (RKHS) associated with a positive definite kernel $k(\\cdot,\\cdot)$, $\\lambda \\gt 0$, and $\\{(x_i, y_i)\\}_{i=1}^{n}$ are training pairs. By the Representer Theorem, the optimizer has the form $f(\\cdot) = \\sum_{i=1}^{n} \\alpha_i k(x_i, \\cdot)$, and the coefficients $\\alpha \\in \\mathbb{R}^n$ satisfy\n$$\n\\alpha = (K + \\lambda I)^{-1} y,\n$$\nwith $K \\in \\mathbb{R}^{n \\times n}$, $K_{ij} = k(x_i, x_j)$, and $y \\in \\mathbb{R}^n$.\n- For classification with labels $y \\in \\{-1, +1\\}$, predict $\\hat{y} = \\operatorname{sign}(f(x))$ with the tie-breaking convention $\\operatorname{sign}(0) = +1$.\n\nKernels to use:\n- Gaussian RBF kernel: $k_{\\mathrm{RBF}}(x, z) = \\exp\\left(-\\gamma \\lVert x - z \\rVert_2^2 \\right)$ with bandwidth parameter $\\gamma \\gt 0$.\n- Polynomial kernel: $k_{\\mathrm{poly}}(x, z) = (x^\\top z + c)^d$ with degree $d \\in \\mathbb{N}$ and coefficient $c \\ge 0$.\n\nYour program must implement KRR in dual form using the above equations and apply it with both kernels on three test cases. For each test case, compute the classification accuracy for the polynomial kernel and for the RBF kernel, then compute their difference. Accuracy is the fraction of correctly classified test labels in $[0,1]$.\n\nIntuition to be demonstrated and explained by your code’s outputs:\n- In a high-dimensional setting with a global separating direction, the polynomial kernel of low degree can capture global structure because it encodes global algebraic interactions such as inner products and sums across coordinates. In contrast, the Gaussian RBF kernel with a very small length-scale (large $\\gamma$) is highly local, so predictions at test points far from the training set collapse toward $0$; combined with the tie-breaking rule $\\operatorname{sign}(0) = +1$, this yields poor accuracy on globally structured tasks if the training set does not cover the global structure.\n\nImplement the following deterministic test suite. There are no physical units involved. All angles (if any appear) must be in radians. Percentages must be expressed as decimals in $[0,1]$.\n\nTest Case A (global structure where RBF fails but polynomial succeeds):\n- Dimension: $d = 20$.\n- Training inputs: $n = 10$ points $x_i = t_i \\cdot \\mathbf{1}_d$ with $t_i = -1 + \\frac{2i}{9}$ for $i \\in \\{0, 1, \\dots, 9\\}$ and $\\mathbf{1}_d \\in \\mathbb{R}^d$ the vector of all ones. Training labels: $y_i = \\operatorname{sign}(t_i)$ with the tie-breaking rule $\\operatorname{sign}(0) = +1$ (not actually triggered here).\n- Test inputs: $m = 21$ points $x = s_j \\cdot \\mathbf{1}_d$ with $s_j = -2 + \\frac{4j}{20}$ for $j \\in \\{0, 1, \\dots, 20\\}$. Test labels: $y = \\operatorname{sign}(s_j)$ with $\\operatorname{sign}(0) = +1$.\n- Kernels and hyperparameters:\n  - Polynomial kernel: degree $d_{\\mathrm{poly}} = 1$, coefficient $c = 0$, ridge $\\lambda = 10^{-6}$.\n  - Gaussian RBF kernel: $\\gamma = 50$, ridge $\\lambda = 10^{-6}$.\n- Expectation: polynomial kernel captures the global linear rule across all coordinates; RBF kernel with large $\\gamma$ is too local and fails to generalize across the domain width.\n\nTest Case B (happy path where both succeed):\n- Dimension: $d = 2$.\n- Training inputs: six points forming two well-separated clusters,\n  - Class $-1$: $(-1.0, -1.0)$, $(-1.1, -0.9)$, $(-0.9, -1.1)$,\n  - Class $+1$: $(1.0, 1.0)$, $(1.1, 0.9)$, $(0.9, 1.1)$.\n- Test inputs: four points,\n  - Class $-1$: $(-1.2, -0.8)$, $(-0.8, -1.2)$,\n  - Class $+1$: $(1.2, 0.8)$, $(0.8, 1.2)$.\n- Kernels and hyperparameters:\n  - Polynomial kernel: degree $d_{\\mathrm{poly}} = 1$, coefficient $c = 0$, ridge $\\lambda = 10^{-6}$.\n  - Gaussian RBF kernel: $\\gamma = 1$, ridge $\\lambda = 10^{-6}$.\n- Expectation: both kernels achieve near-perfect accuracy.\n\nTest Case C (edge case: degree mismatch where polynomial can underfit and RBF succeeds):\n- Dimension: $d = 1$.\n- Training inputs: four points $x \\in \\{-1.0, -0.2, 0.2, 1.0\\}$ with labels $y = \\operatorname{sign}(x)$ and $\\operatorname{sign}(0) = +1$.\n- Test inputs: five points $x \\in \\{-0.8, -0.1, 0.0, 0.1, 0.8\\}$ with labels $y = \\operatorname{sign}(x)$ and $\\operatorname{sign}(0) = +1$.\n- Kernels and hyperparameters:\n  - Polynomial kernel: degree $d_{\\mathrm{poly}} = 2$, coefficient $c = 0$, ridge $\\lambda = 10^{-6}$.\n  - Gaussian RBF kernel: $\\gamma = 2$, ridge $\\lambda = 10^{-6}$.\n- Expectation: the even-degree polynomial kernel cannot represent the odd decision boundary well and may fail; the RBF kernel with moderate locality succeeds.\n\nProgram requirements:\n- Implement KRR as described. For each test case, train two separate models (polynomial and RBF), compute the predicted labels on the test inputs, and compute the accuracy for each kernel as a float in $[0,1]$. Also compute the difference $\\text{accuracy}_{\\mathrm{poly}} - \\text{accuracy}_{\\mathrm{RBF}}$.\n- Final Output Format: Your program should produce a single line of output containing $9$ results as a comma-separated list enclosed in square brackets: for Test Case A, output the polynomial accuracy, the RBF accuracy, and their difference; then repeat the same three numbers for Test Case B and for Test Case C, in that order. For example, the output must look like\n\"[a1,a2,a3,a4,a5,a6,a7,a8,a9]\"\nwith each $a_k$ a decimal number. Round each reported number to exactly $4$ decimal places.",
            "solution": "The problem requires a comparison of Kernel Ridge Regression (KRR) with a polynomial kernel and a Gaussian Radial Basis Function (RBF) kernel across three distinct classification tasks. The core of the problem lies in demonstrating how the intrinsic properties of a kernel—specifically, its global versus local nature—determine its suitability for a given data structure.\n\nFirst, we formalize the Kernel Ridge Regression algorithm. Given a training set of $n$ data pairs $\\{(x_i, y_i)\\}_{i=1}^{n}$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, +1\\}$, KRR seeks a function $f$ from a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$, defined by a kernel $k(\\cdot, \\cdot)$, that minimizes the regularized squared loss:\n$$\n\\min_{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^{n} (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2\n$$\nHere, $\\lambda > 0$ is the regularization parameter that controls the trade-off between fitting the data and maintaining a small norm in $\\mathcal{H}$, which helps prevent overfitting. The Representer Theorem states that the optimal solution $f^*$ has a finite expansion in terms of the training data:\n$$\nf^*(x) = \\sum_{i=1}^{n} \\alpha_i k(x_i, x)\n$$\nThe dual coefficients $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^\\top \\in \\mathbb{R}^n$ are obtained by solving the linear system:\n$$\n(K + \\lambda I) \\alpha = y\n$$\nwhere $K$ is the $n \\times n$ Gram matrix with entries $K_{ij} = k(x_i, x_j)$, $I$ is the $n \\times n$ identity matrix, and $y = (y_1, \\dots, y_n)^\\top$. We will adhere to this formulation as given in the problem statement.\n\nOnce the coefficients $\\alpha$ are determined, predictions for new test points $x_{\\text{test}}$ are made by evaluating $f^*(x_{\\text{test}})$. For classification, the predicted label $\\hat{y}$ is given by the sign of this evaluation:\n$$\n\\hat{y} = \\operatorname{sign}(f^*(x_{\\text{test}})) = \\operatorname{sign}\\left(\\sum_{i=1}^{n} \\alpha_i k(x_i, x_{\\text{test}})\\right)\n$$\nWe use the specified tie-breaking rule $\\operatorname{sign}(0) = +1$.\n\nThe two kernels to be compared are:\n1.  **Polynomial Kernel**: $k_{\\mathrm{poly}}(x, z) = (x^\\top z + c)^d$. This kernel computes similarity based on polynomial combinations of the original features. For degree $d=1$ and $c=0$, it reduces to the linear kernel $k(x,z) = x^\\top z$, which captures global linear relationships. Its feature space consists of global interactions between coordinates.\n2.  **Gaussian RBF Kernel**: $k_{\\mathrm{RBF}}(x, z) = \\exp(-\\gamma \\lVert x - z \\rVert_2^2)$. This kernel's value depends on the Euclidean distance between points. The parameter $\\gamma$ controls the \"width\" of the kernel; a large $\\gamma$ corresponds to a small length-scale, making the kernel highly *local*. The prediction at a point $x$ is primarily influenced by training points that are very close to $x$.\n\nThe implementation will consist of functions to compute the kernel matrices, a function to solve for the KRR coefficients $\\alpha$, and a function to make predictions. These will be applied to the three test cases.\n\n**Test Case A Analysis**: This case is designed to highlight the failure of a local kernel on a task with global structure. The data lies on a single line in $\\mathbb{R}^{20}$ defined by the vector of all ones, $\\mathbf{1}_{20}$. The label is determined simply by the sign of the scalar multiple. A polynomial kernel with degree $d=1$ (i.e., a linear kernel) is perfectly suited to learn this global linear rule and is expected to achieve perfect accuracy. The RBF kernel with a very large $\\gamma=50$ is highly localized. Test points far from the training data interval $[-1, 1]$ (in the scalar projection) will be far from all training points in Euclidean distance. The kernel evaluation $k(x_{\\text{test}}, x_i)$ will be close to zero for all $i$. Consequently, the predictive function $f(x_{\\text{test}})$ will approach $0$, and the prediction will default to $\\operatorname{sign}(0) = +1$. This will cause misclassification for all test points with a true label of $-1$ that lie outside the training data's span.\n\n**Test Case B Analysis**: This case presents a simple, linearly separable classification problem in $\\mathbb{R}^2$. Two distinct, well-separated clusters of points correspond to the two classes. Both the linear (polynomial degree $1$) kernel and the RBF kernel are expected to perform perfectly. The linear kernel can find the separating hyperplane, and the RBF kernel can easily separate the two \"bumps\" of data density.\n\n**Test Case C Analysis**: This case demonstrates a failure mode for the polynomial kernel due to a mismatch in functional form (parity). The task is to learn the function $\\operatorname{sign}(x)$ in one dimension, which is an *odd* function ($f(-x) = -f(x)$). The chosen polynomial kernel, $k(x, z) = (xz)^2 = x^2z^2$, generates an RKHS of *even* functions, as any function in its span has the form $f(x) = w x^2$ for some weight $w$. An even function is fundamentally unsuited to approximate an odd one, leading to poor performance. The best even-function approximation to $\\operatorname{sign}(x)$ on a symmetric domain is $f(x)=0$, which results in all predictions being $+1$. The RBF kernel, being a universal approximator, is not constrained by such symmetries and is flexible enough to model the decision boundary at $x=0$ correctly.\n\nThe program implements these steps, calculates the accuracy for each kernel on each test case, and computes their difference, providing quantitative evidence for the theoretical analysis.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Kernel Ridge Regression with Polynomial and RBF kernels\n    across three specified test cases.\n    \"\"\"\n\n    # --- Kernel Functions ---\n    def rbf_kernel(X1, X2, gamma):\n        \"\"\"Computes the Gaussian RBF kernel matrix.\"\"\"\n        # Using the identity ||x-z||^2 = ||x||^2 + ||z||^2 - 2x^Tz for vectorization\n        X1_norm_sq = np.sum(X1**2, axis=1, keepdims=True)\n        X2_norm_sq = np.sum(X2**2, axis=1, keepdims=True)\n        dot_product = X1 @ X2.T\n        distsq = X1_norm_sq + X2_norm_sq.T - 2 * dot_product\n        return np.exp(-gamma * distsq)\n\n    def polynomial_kernel(X1, X2, d, c):\n        \"\"\"Computes the Polynomial kernel matrix.\"\"\"\n        return (X1 @ X2.T + c)**d\n\n    # --- KRR Algorithm ---\n    def train_krr(X_train, y_train, kernel_func, lmbda, **kernel_params):\n        \"\"\"Trains the KRR model by solving for the dual coefficients alpha.\"\"\"\n        n = X_train.shape[0]\n        K = kernel_func(X_train, X_train, **kernel_params)\n        # alpha = (K + lambda*I)^-1 * y\n        alpha = np.linalg.solve(K + lmbda * np.identity(n), y_train)\n        return alpha\n\n    def predict_krr(X_test, X_train, alpha, kernel_func, **kernel_params):\n        \"\"\"Makes predictions using the trained KRR model.\"\"\"\n        K_test = kernel_func(X_test, X_train, **kernel_params)\n        f_test = K_test @ alpha\n        # Classification with sign(0) = +1 convention\n        return np.where(f_test >= 0, 1.0, -1.0)\n\n    def calculate_accuracy(y_pred, y_true):\n        \"\"\"Calculates classification accuracy.\"\"\"\n        return np.mean(y_pred == y_true)\n\n    results = []\n    \n    # === Test Case A ===\n    d_a = 20\n    n_a = 10\n    t_a = -1 + 2 * np.arange(n_a) / (n_a - 1)\n    X_train_a = t_a[:, np.newaxis] * np.ones((1, d_a))\n    y_train_a = np.sign(t_a)\n    y_train_a[y_train_a == 0] = 1 # Not triggered, but good practice\n    \n    m_a = 21\n    s_a = -2 + 4 * np.arange(m_a) / (m_a - 1)\n    X_test_a = s_a[:, np.newaxis] * np.ones((1, d_a))\n    y_test_a = np.sign(s_a)\n    y_test_a[y_test_a == 0] = 1  # Apply sign(0) = +1 rule\n\n    poly_params_a = {'d': 1, 'c': 0}\n    rbf_params_a = {'gamma': 50}\n    lmbda_a = 1e-6\n\n    # Polynomial kernel on Test Case A\n    alpha_poly_a = train_krr(X_train_a, y_train_a, polynomial_kernel, lmbda_a, **poly_params_a)\n    y_pred_poly_a = predict_krr(X_test_a, X_train_a, alpha_poly_a, polynomial_kernel, **poly_params_a)\n    acc_poly_a = calculate_accuracy(y_pred_poly_a, y_test_a)\n\n    # RBF kernel on Test Case A\n    alpha_rbf_a = train_krr(X_train_a, y_train_a, rbf_kernel, lmbda_a, **rbf_params_a)\n    y_pred_rbf_a = predict_krr(X_test_a, X_train_a, alpha_rbf_a, rbf_kernel, **rbf_params_a)\n    acc_rbf_a = calculate_accuracy(y_pred_rbf_a, y_test_a)\n    \n    results.extend([acc_poly_a, acc_rbf_a, acc_poly_a - acc_rbf_a])\n\n    # === Test Case B ===\n    X_train_b = np.array([[-1.0, -1.0], [-1.1, -0.9], [-0.9, -1.1], [1.0, 1.0], [1.1, 0.9], [0.9, 1.1]])\n    y_train_b = np.array([-1, -1, -1, 1, 1, 1])\n    X_test_b = np.array([[-1.2, -0.8], [-0.8, -1.2], [1.2, 0.8], [0.8, 1.2]])\n    y_test_b = np.array([-1, -1, 1, 1])\n    \n    poly_params_b = {'d': 1, 'c': 0}\n    rbf_params_b = {'gamma': 1}\n    lmbda_b = 1e-6\n\n    # Polynomial kernel on Test Case B\n    alpha_poly_b = train_krr(X_train_b, y_train_b, polynomial_kernel, lmbda_b, **poly_params_b)\n    y_pred_poly_b = predict_krr(X_test_b, X_train_b, alpha_poly_b, polynomial_kernel, **poly_params_b)\n    acc_poly_b = calculate_accuracy(y_pred_poly_b, y_test_b)\n\n    # RBF kernel on Test Case B\n    alpha_rbf_b = train_krr(X_train_b, y_train_b, rbf_kernel, lmbda_b, **rbf_params_b)\n    y_pred_rbf_b = predict_krr(X_test_b, X_train_b, alpha_rbf_b, rbf_kernel, **rbf_params_b)\n    acc_rbf_b = calculate_accuracy(y_pred_rbf_b, y_test_b)\n    \n    results.extend([acc_poly_b, acc_rbf_b, acc_poly_b - acc_rbf_b])\n\n    # === Test Case C ===\n    X_train_c = np.array([-1.0, -0.2, 0.2, 1.0])[:, np.newaxis]\n    y_train_c = np.sign(X_train_c.flatten())\n    \n    X_test_c = np.array([-0.8, -0.1, 0.0, 0.1, 0.8])[:, np.newaxis]\n    y_test_c = np.sign(X_test_c.flatten())\n    y_test_c[y_test_c == 0] = 1 # Apply sign(0) = +1 rule\n    \n    poly_params_c = {'d': 2, 'c': 0}\n    rbf_params_c = {'gamma': 2}\n    lmbda_c = 1e-6\n\n    # Polynomial kernel on Test Case C\n    alpha_poly_c = train_krr(X_train_c, y_train_c, polynomial_kernel, lmbda_c, **poly_params_c)\n    y_pred_poly_c = predict_krr(X_test_c, X_train_c, alpha_poly_c, polynomial_kernel, **poly_params_c)\n    acc_poly_c = calculate_accuracy(y_pred_poly_c, y_test_c)\n\n    # RBF kernel on Test Case C\n    alpha_rbf_c = train_krr(X_train_c, y_train_c, rbf_kernel, lmbda_c, **rbf_params_c)\n    y_pred_rbf_c = predict_krr(X_test_c, X_train_c, alpha_rbf_c, rbf_kernel, **rbf_params_c)\n    acc_rbf_c = calculate_accuracy(y_pred_rbf_c, y_test_c)\n\n    results.extend([acc_poly_c, acc_rbf_c, acc_poly_c - acc_rbf_c])\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{r:.4f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}