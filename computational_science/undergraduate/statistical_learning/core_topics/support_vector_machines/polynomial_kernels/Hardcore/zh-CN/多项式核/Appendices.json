{
    "hands_on_practices": [
        {
            "introduction": "多项式核的“次数”$d$是其最重要的超参数之一，它直接决定了模型能够学习的决策边界的复杂性。这个练习将挑战你构建一个简单的数据集，在该数据集上，只有三次核才能成功分类，而线性和二次核则会失败，从而直观地展示高次核在捕捉复杂特征交互时的必要性 。",
            "id": "3158522",
            "problem": "要求你构建一个二元分类数据集，以突显三阶多项式核何时能捕捉到一阶和二阶核无法捕捉的交互作用。考虑输入 $x \\in \\{-1,+1\\}^3$ 和由三重交互作用的类奇偶校验规则定义的标签。你在无噪声数据上使用 $1$、$2$ 和 $3$ 阶的非齐次多项式核来训练一个核方法。训练目标是经验风险最小化，并要求训练误差为零。对于任何返回所选核特征空间中的解的算法，哪个选项正确地指定了一个数据集和所需的最小样本量，使得三阶核可以精确拟合数据，而一阶和二阶核不能？\n\nA. 使用包含 $\\{-1,+1\\}^3$ 中所有 $8$ 个点的数据集，标签为 $y = x_1 x_2 x_3$。可靠地学习该三次结构所需的最小样本量是 $8$。\n\nB. 使用 $\\{-1,+1\\}^3$ 中 $6$ 个不同点的数据集，选择这些点使得恰好一半的点满足 $x_1 x_2 x_3 = +1$，另一半满足 $x_1 x_2 x_3 = -1$。最小样本量是 $6$。\n\nC. 使用包含 $\\{-1,+1\\}^3$ 中所有 $8$ 个点的数据集，标签为 $y = x_1 x_2 x_3$。可靠地学习该三次结构所需的最小样本量是 $7$。\n\nD. 使用 $\\{-1,+1\\}^3$ 中 $4$ 个点的数据集（例如，汉明权重等于 $1$ 或 $2$ 的点），标签为 $y = x_1 x_2 x_3$。最小样本量是 $4$。",
            "solution": "用户希望我仔细解决一个关于多项式核的机器学习问题。\n\n### 问题验证\n\n首先，我将根据指定的协议验证问题陈述。\n\n#### 第一步：提取已知条件\n\n-   输入空间：$x \\in \\{-1,+1\\}^3$。\n-   标签：由三重交互作用的类奇偶校验规则定义，在选项中指定为 $y = x_1 x_2 x_3$。\n-   模型：使用非齐次多项式核的核方法，阶数分别为 $d=1$、$d=2$ 和 $d=3$。通用形式为 $K_d(x, z) = (\\langle x, z \\rangle + c)^d$。我们假设标准选择 $c=1$，因此 $K_d(x, z) = (\\langle x, z \\rangle + 1)^d$。\n-   训练：经验风险最小化，并要求零训练误差。\n-   算法约束：根据表示定理（Representer Theorem），返回的解必须在所选核特征的生成空间内，这对于核方法是标准的。\n-   问题：存在一个数据集，使得三阶核可以实现零训练误差，而一阶和二阶核不能，所需的最小样本量 $N$ 是多少？\n\n#### 第二步：使用提取的已知条件进行验证\n\n-   **科学依据**：该问题是统计学习理论中的一个典型练习，具体涉及核方法的表达能力。多项式核、特征空间以及某些函数（如奇偶校验函数）的不可分性等概念是该领域的基础。该设置在科学上是合理的。\n-   **适定性**：该问题要求在特定的、可形式化的条件下求一个最小整数值（$N$）。这样的最小值的存在性和唯一性是明确定义的。\n-   **客观性**：语言精确，并使用了机器学习中标准的、无歧义的术语。没有主观因素。\n\n该问题不违反任何无效性标准。它是一个适定的、有科学依据的统计学习问题。\n\n#### 第三步：结论与行动\n\n问题是**有效的**。我将继续进行推导和求解。\n\n### 推导\n\n问题的核心在于理解在定义域 $x \\in \\{-1,+1\\}^3$ 上与多项式核相关联的函数空间。如果在一个相关的再生核希尔伯特空间 (RKHS) 中存在一个函数 $f$，使得对于所有 $i=1, \\dots, N$ 都有 $f(x_i) = y_i$，那么一个给定的核方法可以在训练集 $\\{(x_i, y_i)\\}_{i=1}^N$ 上以零误差学习。\n\n对于一个 $d$ 阶非齐次多项式核，其特征映射 $\\phi_d(x)$ 包括了所有阶数最高为 $d$ 的单项式对应的特征。对于输入 $x \\in \\{-1,+1\\}^3$，任何分量 $x_j$ 都满足 $x_j^2 = 1$。这简化了唯一单项式的集合。\n\n定义域 $\\{-1,+1\\}^3$ 上的所有可能函数的集合构成一个 $8$ 维向量空间，因为该定义域有 $2^3=8$ 个点。该空间的一个基由所有唯一的、非平凡的单项式给出：\n$$\n\\mathcal{B} = \\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3, x_1x_2x_3\\}\n$$\n在 $x \\in \\{-1,+1\\}^3$ 上的均匀分布下取期望，这些基函数是相互正交的。\n\n目标函数是 $g(x) = x_1 x_2 x_3$，它是这个基中最高阶的单项式。\n\n我们来分析不同核阶数的函数空间：\n\n1.  **一阶核 ($d=1$)**：特征空间 $\\mathcal{F}_1$ 由阶数最高为 $1$ 的单项式张成。\n    $$ \\mathcal{F}_1 = \\text{span}\\{1, x_1, x_2, x_3\\} $$\n    这是一个 $4$ 维空间。显然，$g(x) = x_1x_2x_3$ 不在 $\\mathcal{F}_1$ 中。\n\n2.  **二阶核 ($d=2$)**：特征空间 $\\mathcal{F}_2$ 由阶数最高为 $2$ 的单项式张成。\n    $$ \\mathcal{F}_2 = \\text{span}\\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3\\} $$\n    这是一个 $7$ 维空间。目标函数 $g(x) = x_1x_2x_3$ 与 $\\mathcal{F}_2$ 的所有基向量正交，因此不在 $\\mathcal{F}_2$ 中。因此，二阶核无法在其整个定义域上表示函数 $g(x)$。\n\n3.  **三阶核 ($d=3$)**：特征空间 $\\mathcal{F}_3$ 由阶数最高为 $3$ 的单项式张成。\n    $$ \\mathcal{F}_3 = \\text{span}\\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3, x_1x_2x_3\\} $$\n    这个空间是 $\\{-1,+1\\}^3$ 上所有函数的完整 $8$ 维空间。目标函数 $g(x) = x_1x_2x_3$ 是这个空间的一个基向量。因此，三阶核可以精确表示 $g(x)$。\n\n现在，我们必须找到最小的样本量 $N$，使得存在一个数据集 $\\{(x_i, y_i)\\}_{i=1}^N$（其中 $y_i=g(x_i)$），该数据集可以被三阶核拟合，但不能被二阶核拟合。\n\n如果存在系数 $c_j$，使得对于一个函数 $f(x) = \\sum_{j=1}^7 c_j h_j(x)$（其中 $h_j$ 是 $\\mathcal{F}_2$ 的基函数），我们有 $f(x_i) = y_i = g(x_i)$ 对所有 $i=1, \\dots, N$ 成立，那么一个来自 $\\mathcal{F}_2$ 的函数就可以拟合数据。这是一个包含 $N$ 个线性方程和 $7$ 个未知系数的方程组。\n\n我们来分析一个大小为 $N$ 的样本。这相当于检查标签向量 $(g(x_1), \\dots, g(x_N))$ 是否在基函数值向量 $\\{(h_j(x_1), \\dots, h_j(x_N))\\}_{j=1}^7$ 的生成空间内。\n\n考虑任何大小为 $N \\le 7$ 的数据集。设点为 $x_1, \\dots, x_N$。$\\mathcal{B}$ 中的 $8$ 个基函数在这些 $N$ 个点上求值后，成为 $\\mathbb{R}^N$ 中的 $8$ 个向量。由于 $N \\le 7$，这 $8$ 个向量必然是线性相关的。这意味着存在一组非零系数 $\\{a_j\\}_{j=1}^8$ 使得：\n$$ \\sum_{j=1}^8 a_j h_j(x_i) = 0, \\quad \\text{for } i=1, \\dots, N $$\n其中 $h_1, \\dots, h_7$ 是 $\\mathcal{F}_2$ 的基，而 $h_8(x) = g(x) = x_1x_2x_3$。\n\n这些基函数（它们构成一个 Walsh-Hadamard 系统）的一个关键性质是，它们中的任何 $7$ 个在任何 $7$ 个不同点上都是线性无关的。然而，完整的 $8$ 个函数集合变得线性相关。这种相关性的系数由在完整的 $8$ 个点集合中被遗漏的那个点上基函数的值给出。对于任何 $7$ 个点的集合，基函数之间存在的线性相关关系中，对于 $h_8=x_1x_2x_3$ 项的系数 $a_8$ 将是非零的。这是因为系数 $a_8$ 与 $h_8(x_k)$ 成正比，其中 $x_k$ 是被遗漏的点，而 $h_8(x_k)=x_{k1}x_{k2}x_{k3}$ 总是 $\\pm 1$。\n\n由于 $a_8 \\ne 0$，我们可以重新排列这个相关性：\n$$ h_8(x_i) = -\\frac{1}{a_8} \\sum_{j=1}^7 a_j h_j(x_i) $$\n这表明，在任何包含 $N \\le 7$ 个点的集合上，函数 $g(x)=x_1x_2x_3$ 都可以表示为 $\\mathcal{F}_2$ 的基函数的线性组合。因此，对于任何大小为 $N \\le 7$ 的数据集，二阶核（并引申到一阶核）*能够*实现零训练误差。\n\n因此，要找到一个二阶核*无法*拟合的数据集，我们需要一个大小为 $N > 7$ 的样本。\n\n现在考虑 $N=8$。让数据集包含 $\\{-1,+1\\}^3$ 中的所有 $8$ 个点。标签为 $y_i = g(x_i) = x_{i1}x_{i2}x_{i3}$。\n-   三阶核可以拟合此数据，因为 $g(x)$ 在其特征空间 $\\mathcal{F}_3$ 中。学习器可以简单地选择 $f(x)=g(x)$。\n-   二阶核无法拟合此数据。要实现拟合，需要找到一个函数 $f \\in \\mathcal{F}_2$，使得对于所有 $8$ 个点 $x_i$ 都有 $f(x_i) = g(x_i)$。这将意味着在整个定义域上 $f(x) = g(x)$，这是不可能的，因为 $g(x) \\notin \\mathcal{F}_2$。更正式地说，基函数值的 $8$ 个向量 $\\{(h_j(x_1), \\dots, h_j(x_8))\\}_{j=1}^8$ 是线性无关的。因此，对应于 $h_8=g$ 的向量不能写成对应于 $h_1, \\dots, h_7$ 的向量的线性组合。\n\n因此，所需的最小样本量是 $N=8$，而一个能够证明这一点的就是包含所有 $8$ 个点的数据集。\n\n### 逐项分析\n\n**A. 使用包含 $\\{-1,+1\\}^3$ 中所有 $8$ 个点的数据集，标签为 $y = x_1 x_2 x_3$。可靠地学习该三次结构所需的最小样本量是 $8$。**\n这个选项正确地指出了数据集和最小样本量。如上所推导，样本量 $N=8$ 是必要且充分的。有了所有 $8$ 个点，全套单项式的线性无关性确保了三次项 $x_1x_2x_3$ 不能由低阶项表示。\n**结论：正确。**\n\n**B. 使用 $\\{-1,+1\\}^3$ 中 $6$ 个不同点的数据集，选择这些点使得恰好一半的点满足 $x_1 x_2 x_3 = +1$ 和一半满足 $x_1 x_2 x_3 = -1$。最小样本量是 $6$。**\n这个选项声称最小样本量是 $6$。我的推导表明，对于任何大小为 $N \\le 7$ 的数据集，二阶核都能完美拟合数据。因此，大小为 $6$ 的样本不足以保证二阶核会失败。\n**结论：不正确。**\n\n**C. 使用包含 $\\{-1,+1\\}^3$ 中所有 $8$ 个点的数据集，标签为 $y = x_1 x_2 x_3$。可靠地学习该三次结构所需的最小样本量是 $7$。**\n这个选项声称最小样本量是 $7$。这是不正确的。如前所述，对于任何大小为 $7$ 的样本，基函数会变得线性相关，从而允许二阶核完美拟合数据。最小样本量必须大于 $7$。\n**结论：不正确。**\n\n**D. 使用 $\\{-1,+1\\}^3$ 中 $4$ 个点的数据集（例如，那些汉明权重等于 $1$ 或 $2$ 的点），标签为 $y = x_1 x_2 x_3$。最小样本量是 $4$。**\n这个选项声称最小样本量是 $4$。这与选项 B 和 C 的原因相同，都是不正确的。大小为 $4$ 的样本是不够的；二阶核总是可以拟合这种大小的数据集。\n**结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "将理论知识付诸实践是掌握任何概念的关键。本练习将指导你通过编程实现核岭回归，并探索“部分”多项式核的性能，即只包含特定次数项的核函数 。通过这种方式，你将深入理解核函数的设计如何具体影响模型选择和最终的预测效果。",
            "id": "3158464",
            "problem": "您将执行一个二元分类任务，并被要求比较仅包含选定齐次阶数的部分多项式核与通过完整二项式展开得到的完全非齐次多项式核的性能。您的程序必须通过核岭回归（KRR）实现分类，并针对每个指定的测试用例，报告部分核与具有相同总阶数和偏移量的相应完全二项式核在测试集上的误分类率之差。\n\n从以下核心定义和原则开始：\n- 对于向量 $x \\in \\mathbb{R}^p$ 和 $y \\in \\mathbb{R}^p$，齐次 $m$ 阶多项式核是由对称张量内积给出的，即 $k_m(x,y) = (x^\\top y)^m$，其中整数 $m \\ge 0$。\n- 给定整数总阶数 $d \\ge 0$ 和偏移量 $c \\ge 0$，总阶数为 $d$ 的完全非齐次多项式核是通过整合从 $0$ 到 $d$ 的所有齐次阶数构建的，其权重由二项式结构导出，为非负值。部分多项式核选择齐次阶数的一个子集。具体来说，对于子集 $S \\subseteq \\{0,1,\\dots,d\\}$，定义\n$$\nK_S(x,y) \\;=\\; \\sum_{m \\in S} \\binom{d}{m}\\, c^{\\,d-m}\\, (x^\\top y)^m.\n$$\n当取 $S = \\{0,1,\\dots,d\\}$ 时，得到完全核，记为 $K_{\\text{full}}(x,y)$。\n\n学习规则：\n- 使用正则化参数 $\\lambda > 0$ 进行核岭回归（KRR）分类。设训练数据为 $\\{(x_i, y_i)\\}_{i=1}^n$，标签为 $y_i \\in \\{-1,+1\\}$。设 $K \\in \\mathbb{R}^{n \\times n}$ 为训练格拉姆矩阵，其元素为 $K_{ij} = K(x_i,x_j)$，其中 $K$ 是所选的核函数。系数向量 $\\alpha \\in \\mathbb{R}^n$ 被定义为以下线性系统的唯一解\n$$\n\\big(K + n \\lambda I_n\\big)\\, \\alpha \\;=\\; y,\n$$\n其中 $I_n$ 是 $n \\times n$ 的单位矩阵， $y = (y_1,\\dots,y_n)^\\top$。对于一个测试点 $z$，预测值为\n$$\nf(z) \\;=\\; \\sum_{i=1}^n \\alpha_i\\, K(x_i, z),\n$$\n预测标签为 $\\mathrm{sign}(f(z))$，并定义 $\\mathrm{sign}(0)$ 为 $+1$。测试集上的误分类率是预测标签与真实标签不一致的测试点所占的比例，表示为 $[0,1]$ 内的一个实数。\n\n训练和测试数据：\n- 训练输入 $X_{\\text{train}}$ 属于 $\\mathbb{R}^{8 \\times 2}$（每行为一个输入）：\n  - $(\\,1.0,\\,1.0\\,)$\n  - $(\\,1.0,\\,-1.0\\,)$\n  - $(\\,-1.0,\\,1.0\\,)$\n  - $(\\,-1.0,\\,-1.0\\,)$\n  - $(\\,2.0,\\,0.5\\,)$\n  - $(\\,-2.0,\\,0.5\\,)$\n  - $(\\,0.5,\\,-2.0\\,)$\n  - $(\\,-0.5,\\,-2.0\\,)$\n- 训练标签 $y_{\\text{train}} \\in \\{-1,+1\\}^8$：\n  - $+1$, $-1$, $-1$, $+1$, $+1$, $-1$, $-1$, $+1$。\n- 测试输入 $X_{\\text{test}}$ 属于 $\\mathbb{R}^{8 \\times 2}$：\n  - $(\\,0.5,\\,0.5\\,)$\n  - $(\\,0.5,\\,-0.5\\,)$\n  - $(\\,-0.5,\\,0.5\\,)$\n  - $(\\,-0.5,\\,-0.5\\,)$\n  - $(\\,3.0,\\,0.1\\,)$\n  - $(\\,-3.0,\\,0.1\\,)$\n  - $(\\,0.1,\\,-3.0\\,)$\n  - $(\\,-0.1,\\,-3.0\\,)$\n- 测试标签 $y_{\\text{test}} \\in \\{-1,+1\\}^8$：\n  - $+1$, $-1$, $-1$, $+1$, $+1$, $-1$, $-1$, $+1$。\n\n任务：\n- 对于下方的每个测试用例，计算测试集上的两个误分类率：一个使用部分核 $K_S$，另一个使用具有相同 $d$ 和 $c$ 的完全核 $K_{\\text{full}}$。报告其差值\n$$\n\\Delta \\;=\\; \\text{error}(K_S) \\;-\\; \\text{error}(K_{\\text{full}}).\n$$\n负值 $\\Delta$ 表示部分核取得了更低的错误率（一种改进），而正值 $\\Delta$ 表示完全核的性能更好。\n\n测试套件（每个用例为 $(d, c, S, \\lambda)$，其中 $S$被明确列出）：\n1. $(\\,3,\\, 1.0,\\, \\{2,3\\},\\, 10^{-2}\\,)$\n2. $(\\,3,\\, 1.0,\\, \\{0,1,2,3\\},\\, 10^{-2}\\,)$\n3. $(\\,3,\\, 1.0,\\, \\{2\\},\\, 10^{-2}\\,)$\n4. $(\\,4,\\, 0.5,\\, \\{4\\},\\, 10^{-2}\\,)$\n5. $(\\,2,\\, 1.0,\\, \\{2\\},\\, 10^{-3}\\,)$\n6. $(\\,3,\\, 1.0,\\, \\{0\\},\\, 10^{-2}\\,)$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用逗号分隔的十进制数列表，四舍五入到六位小数，并用方括号括起来。例如，一个包含三个结果的输出应形如 $[\\text{r}_1,\\text{r}_2,\\text{r}_3]$，其中每个 $\\text{r}_i$ 是一个四舍五入到六位小数的实数。\n\n科学真实性和约束条件：\n- 所有计算都在实数上进行，不涉及物理单位。\n- 不使用角度。\n- 百分比必须表示为 $[0,1]$ 内的小数。\n- 通过为每个测试使用指定的 $\\lambda > 0$ 并直接求解线性系统而非使用迭代近似来确保数值稳定性。\n\n您的程序必须是一个完整的、可运行的程序，执行这些计算并打印所需的单行输出。不需要用户输入。",
            "solution": "用户提供了一个统计学习领域的问题，具体涉及使用多项式核的核岭回归（KRR）。任务是比较部分多项式核与完全非齐次多项式核的分类性能。\n\n问题陈述已经过验证，被认为是自洽的，科学上基于统计学习理论的原理，并且在数学上是适定的。多项式核、KRR学习规则和评估指标（误分类率）的定义都是标准且明确的。KRR 目标函数涉及求解线性系统 $(K + n \\lambda I_n) \\alpha = y$。由于格拉姆矩阵 $K$ 是半正定的，且正则化参数 $\\lambda$ 严格为正，矩阵 $(K + n \\lambda I_n)$ 是对称正定的，这保证了系数向量 $\\alpha$ 存在唯一且稳定的解。所有数据和参数都已明确提供。因此，该问题是有效的，并将构建一个解决方案。\n\n对于每个测试用例，解决方案系统地按以下步骤进行：\n1.  **定义核函数**：对于由 $(d, c, S, \\lambda)$ 指定的每个用例，我们定义两种核函数：使用给定阶数集合 $S$ 的部分核 $K_S(x,y)$，以及使用完整阶数集合 $S_{\\text{full}} = \\{0, 1, \\dots, d\\}$ 的完全非齐次核 $K_{\\text{full}}(x,y)$。核函数由下式给出：\n    $$\n    K(x,y) = \\sum_{m \\in \\text{degrees}} \\binom{d}{m} c^{d-m} (x^\\top y)^m\n    $$\n2.  **计算格拉姆矩阵**：对于每种核函数，我们必须计算训练格拉姆矩阵和测试格拉姆矩阵。\n    *   训练格拉姆矩阵 $G \\in \\mathbb{R}^{n \\times n}$ 是在训练数据 $X_{\\text{train}}$ 上计算的。其元素为 $G_{ij} = K(x_i, x_j)$，其中 $x_i, x_j$ 是训练样本。为提高效率，我们首先计算所有点积的矩阵 $D_{\\text{train}} = X_{\\text{train}} X_{\\text{train}}^\\top$，然后将定义核函数的总和逐元素应用于该矩阵。\n    *   测试格拉姆矩阵 $G_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times n}$ 用于预测。其元素为 $(G_{\\text{test}})_{ij} = K(z_i, x_j)$，其中 $z_i$ 是测试样本，$x_j$ 是训练样本。这可以通过点积矩阵 $D_{\\text{test}} = X_{\\text{test}} X_{\\text{train}}^\\top$ 高效计算。\n\n3.  **求解KRR系数**：KRR的核心是求解系数向量 $\\alpha \\in \\mathbb{R}^n$。对于给定的训练格拉姆矩阵 $G$、正则化参数 $\\lambda$、训练标签 $y_{\\text{train}}$ 以及训练样本数 $n$，我们求解线性系统：\n    $$\n    (G + n \\lambda I_n) \\alpha = y_{\\text{train}}\n    $$\n    求解此系统以得到 $\\alpha$。此步骤对部分核（得到 $\\alpha_S$）和完全核（得到 $\\alpha_{\\text{full}}$）分别执行。\n\n4.  **预测与评估**：使用计算出的系数和测试格拉姆矩阵为测试集 $X_{\\text{test}}$ 生成预测。预测得分向量为 $f_{\\text{test}} = G_{\\text{test}} \\alpha$。通过取这些得分的符号来获得预测标签 $\\hat{y}$：\n    $$\n    \\hat{y}_i = \\mathrm{sign}(f_{\\text{test}, i})\n    $$\n    问题规定 $\\mathrm{sign}(0)$ 应映射为 $+1$。误分类率是预测标签与真实标签 $y_{\\text{test}}$ 不匹配的测试样本所占的比例：\n    $$\n    \\text{error} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\mathbb{I}(\\hat{y}_i \\neq y_{\\text{test}, i})\n    $$\n    其中 $\\mathbb{I}(\\cdot)$ 是指示函数。此率对部分核 $\\text{error}(K_S)$ 和完全核 $\\text{error}(K_{\\text{full}})$ 分别计算。\n\n5.  **计算最终差异**：对于每个测试用例，最终结果是误分类率的差值：\n    $$\n    \\Delta = \\text{error}(K_S) - \\text{error}(K_{\\text{full}})\n    $$\n该值量化了因限制多项式核中齐次阶数集合而导致的性能变化。负值表示使用部分核带来了改进。请注意，对于测试用例2，集合 $S$ 为 $\\{0,1,2,3\\}$，这是 $d=3$ 时的完整集合。因此，$K_S = K_{\\text{full}}$，所得差值 $\\Delta$ 必须恰好为 $0$。这可以作为对实现的一个有用的健全性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Solves the kernel ridge regression problem for the specified test cases.\n    \"\"\"\n    # Define training and test data as specified in the problem statement.\n    X_train = np.array([\n        [1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0],\n        [2.0, 0.5], [-2.0, 0.5], [0.5, -2.0], [-0.5, -2.0]\n    ])\n    y_train = np.array([1, -1, -1, 1, 1, -1, -1, 1])\n\n    X_test = np.array([\n        [0.5, 0.5], [0.5, -0.5], [-0.5, 0.5], [-0.5, -0.5],\n        [3.0, 0.1], [-3.0, 0.1], [0.1, -3.0], [-0.1, -3.0]\n    ])\n    y_test = np.array([1, -1, -1, 1, 1, -1, -1, 1])\n    \n    n_train = X_train.shape[0]\n\n    # Pre-compute dot product matrices for efficiency.\n    dots_train = X_train @ X_train.T\n    dots_test = X_test @ X_train.T\n\n    # Define the test suite. S is represented as a set for efficient lookup.\n    test_cases = [\n        # (d, c, S, lambda)\n        (3, 1.0, {2, 3}, 1e-2),\n        (3, 1.0, {0, 1, 2, 3}, 1e-2),\n        (3, 1.0, {2}, 1e-2),\n        (4, 0.5, {4}, 1e-2),\n        (2, 1.0, {2}, 1e-3),\n        (3, 1.0, {0}, 1e-2)\n    ]\n\n    def gram_matrix_from_dots(dot_products_matrix, d, c, S):\n        \"\"\"\n        Computes the Gram matrix from a pre-computed matrix of dot products.\n        \"\"\"\n        gram = np.zeros_like(dot_products_matrix, dtype=np.float64)\n        for m in S:\n            if m < 0 or m > d:\n                continue\n            # Binomial coefficient can be zero if c=0 and d-m>0.\n            # dot_product can be zero.\n            # Use np.power to handle element-wise exponentiation safely.\n            term_coeff = comb(d, m) * np.power(c, d - m, dtype=np.float64)\n            gram += term_coeff * np.power(dot_products_matrix, m, dtype=np.float64)\n        return gram\n\n    def calculate_misclassification_rate(d, c, S, lam):\n        \"\"\"\n        Performs KRR and returns the misclassification rate on the test set.\n        \"\"\"\n        # 1. Compute training Gram matrix\n        K_train = gram_matrix_from_dots(dots_train, d, c, S)\n        \n        # 2. Solve for alpha coefficients\n        A = K_train + n_train * lam * np.identity(n_train)\n        alpha = np.linalg.solve(A, y_train)\n        \n        # 3. Compute test Gram matrix for predictions\n        K_test = gram_matrix_from_dots(dots_test, d, c, S)\n        \n        # 4. Make predictions\n        f_test = K_test @ alpha\n        y_pred = np.sign(f_test)\n        y_pred[y_pred == 0] = 1.0 # As per rule sign(0) = +1\n        \n        # 5. Calculate misclassification rate\n        error_rate = np.mean(y_pred != y_test)\n        return error_rate\n\n    results = []\n    # Memoization for full kernel errors to avoid redundant calculations\n    full_kernel_errors = {}\n\n    for d, c, S, lam in test_cases:\n        # Calculate error for the partial kernel K_S\n        error_S = calculate_misclassification_rate(d, c, S, lam)\n        \n        # Check if the full kernel error for this (d, c, lam) is already computed\n        full_params_key = (d, c, lam)\n        if full_params_key not in full_kernel_errors:\n            S_full = set(range(d + 1))\n            error_full = calculate_misclassification_rate(d, c, S_full, lam)\n            full_kernel_errors[full_params_key] = error_full\n        else:\n            error_full = full_kernel_errors[full_params_key]\n        \n        # Compute the difference and append to results\n        delta = error_S - error_full\n        results.append(delta)\n\n    # Format the final output as specified\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "核方法的性能并非孤立存在，它与数据预处理的方式紧密相关。这个思想实验将引导你分析特征中心化（一种常见的数据预处理技术）对多项式核及其显式偏置项$b$的影响 。这将揭示数据属性与模型参数之间微妙而重要的联系，加深你对模型在实际应用中如何工作的理解。",
            "id": "3158554",
            "problem": "考虑一个在再生核希尔伯特空间中的二元分类问题，该空间由一个次数为 $p$、偏移参数为 $c$ 的多项式核以及决策函数中的一个显式偏置项构建。设决策函数为 $f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x}) + b$，其中 $y_i \\in \\{-1, +1\\}$ 且 $b \\in \\mathbb{R}$，如同在支持向量机 (SVM) 中一样。假设特征通过中心化进行预处理，即用 $\\tilde{\\mathbf{x}} = \\mathbf{x} - \\bar{\\mathbf{x}}$ 替换每个输入 $\\mathbf{x}$，其中 $\\bar{\\mathbf{x}}$ 是训练集上的经验均值向量。分析成对相似度 $k((\\mathbf{x}-\\bar{\\mathbf{x}}), (\\mathbf{y}-\\bar{\\mathbf{y}}))$ 与非中心化的相似度 $k(\\mathbf{x}, \\mathbf{y})$ 相比有何变化，并预测在 $c>0$（非齐次多项式核）与 $c=0$（齐次多项式核）的情况下，中心化对学习到的偏置项 $b$ 的定性影响。哪个陈述最准确？\n\nA. 对于 $c>0$，中心化通过移除由均值引起的线性交叉项来改变成对相似度，同时保持由 $c$ 引入的常数特征不变；结果，与非中心化的情况相比，显式偏置 $b$ 的幅度通常会减小，因为部分截距由核的常数特征承载。对于 $c=0$，没有常数特征，因此 $b$ 仍然是必不可少的，并且其幅度未必减小。\n\nB. 中心化保持所有成对相似度不变，即对于所有 $\\mathbf{x}, \\mathbf{y}$ 都有 $k((\\mathbf{x}-\\bar{\\mathbf{x}}), (\\mathbf{y}-\\bar{\\mathbf{y}})) = k(\\mathbf{x}, \\mathbf{y})$，因此学习到的偏置 $b$ 不受影响。\n\nC. 中心化移除了由 $c>0$ 隐含的常数特征，消除了特征映射中的任何隐式截距；因此，学习到的偏置 $b$ 的幅度必须增加。\n\nD. 对于任何多项式核，特征中心化都迫使学习到的偏置 $b$ 精确地为 $0$。",
            "solution": "首先验证问题陈述，以确保其科学上合理、适定且客观。\n\n### 第1步：提取已知条件\n-   **问题领域**：使用类似支持向量机 (SVM) 的模型进行二元分类。\n-   **决策函数**：$f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x}) + b$，其中标签 $y_i \\in \\{-1, +1\\}$，并带有一个显式偏置项 $b \\in \\mathbb{R}$。\n-   **核函数**：次数为 $p$ 的多项式核，$k(\\mathbf{u}, \\mathbf{v}) = (\\mathbf{u}^\\top \\mathbf{v} + c)^p$，带有一个偏移参数 $c$。\n-   **数据预处理**：特征中心化，其中每个输入向量 $\\mathbf{x}$ 被替换为 $\\tilde{\\mathbf{x}} = \\mathbf{x} - \\bar{\\mathbf{x}}$。$\\bar{\\mathbf{x}}$ 是训练数据向量的经验均值。\n-   **任务**：分析这种中心化对成对相似度和学习到的偏置项 $b$ 的影响，比较非齐次情况 ($c>0$) 和齐次情况 ($c=0$)。\n\n### 第2步：使用提取的已知条件进行验证\n-   **科学基础**：该问题植根于统计学习理论中标准且完善的概念，特别是核方法和支持向量机。多项式核、特征中心化以及偏置项的作用都是基本主题。\n-   **适定性**：该问题要求对特定数据预处理步骤对模型参数的影响进行定性分析。这是机器学习中一个标准的分析性问题，可以从所涉及组件的数学性质中推导出逻辑上自洽的答案。\n-   **客观性**：该问题使用机器学习领域的精确技术语言进行陈述。它要求基于数学性质进行预测性分析，而非主观意见。\n\n### 第3步：结论与行动\n问题陈述是有效的。它提出了一个关于数据预处理与核机器中模型参数之间相互作用的明确定义的概念性问题。开始推导解答。\n\n### 解答推导\n\n问题的核心在于理解决策函数中的显式偏置项 $b$ 与核的特征映射所提供的任何“隐式偏置”之间的相互作用。决策边界 $f(\\mathbf{x})=0$ 对应于再生核希尔伯特空间 (RKHS) 或特征空间中的一个超平面。偏置 $b$ 决定了该超平面相对于特征空间原点的偏移量。\n\n多项式核定义为 $k(\\mathbf{u}, \\mathbf{v}) = (\\mathbf{u}^\\top \\mathbf{v} + c)^p$。通过核技巧，这等价于高维特征空间中的内积，$k(\\mathbf{u}, \\mathbf{v}) = \\phi(\\mathbf{u})^\\top \\phi(\\mathbf{v})$。决策函数为 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) + b$，其中 $\\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\phi(\\mathbf{x}_i)$。\n\n**隐式偏置与 $c$ 的作用**\n\n我们可以通过考虑核的二项式展开来分析特征映射 $\\phi(\\mathbf{x})$：\n$$k(\\mathbf{u}, \\mathbf{v}) = \\sum_{k=0}^{p} \\binom{p}{k} (\\mathbf{u}^\\top \\mathbf{v})^k c^{p-k}$$\n\n1.  **非齐次情况 ($c>0$)**：\n    当 $c > 0$ 时，求和中包含 $k=0$ 的项：$\\binom{p}{0}(\\mathbf{u}^\\top \\mathbf{v})^0 c^{p-0} = c^p$。这是一个与 $\\mathbf{u}$ 和 $\\mathbf{v}$ 无关的常数值。这意味着特征映射 $\\phi(\\mathbf{x})$ 包含一个对于所有输入 $\\mathbf{x}$ 其值都为常数的特征（例如，一个值为 $\\sqrt{c^p}$ 的特征）。我们将这个常数特征表示为 $\\phi_0(\\mathbf{x}) = C$。\n    决策函数可以写成 $f(\\mathbf{x}) = (\\sum_{j>0} w_j \\phi_j(\\mathbf{x})) + w_0 \\phi_0(\\mathbf{x}) + b$。由于 $\\phi_0(\\mathbf{x})=C$，总偏移量为 $(\\sum_{j>0} w_j \\phi_j(\\mathbf{x})) + (w_0 C + b)$。项 $w_0 C$ 充当一个通过权重 $w_0$ 优化的隐式偏置。SVM 优化器可以在显式偏置 $b$ 和这个隐式偏置之间进行权衡。\n\n2.  **齐次情况 ($c=0$)**：\n    当 $c=0$ 时，核为 $k(\\mathbf{u}, \\mathbf{v}) = (\\mathbf{u}^\\top \\mathbf{v})^p$。特征映射 $\\phi(\\mathbf{x})$ 完全由总次数为 $p$ 的单项式组成。例如，对于 $p=2$ 和 $\\mathbf{x} \\in \\mathbb{R}^2$，特征的形式为 $x_1^2$, $x_2^2$, $x_1 x_2$。没有常数特征；实际上，$\\phi(\\mathbf{0}) = \\mathbf{0}$。因此，没有隐式偏置机制。决策边界所需的任何偏移都必须完全由显式偏置项 $b$ 提供。\n\n**特征中心化的效果**\n\n中心化将输入 $\\mathbf{x}$ 替换为 $\\tilde{\\mathbf{x}} = \\mathbf{x} - \\bar{\\mathbf{x}}$。核矩阵现在使用这些中心化向量计算：$\\tilde{k}(\\mathbf{x}_i, \\mathbf{x}_j) = k(\\tilde{\\mathbf{x}}_i, \\tilde{\\mathbf{x}}_j)$。\n\n新的内积参数是 $(\\mathbf{x}-\\bar{\\mathbf{x}})^\\top(\\mathbf{y}-\\bar{\\mathbf{y}}) = \\mathbf{x}^\\top\\mathbf{y} - \\mathbf{x}^\\top\\bar{\\mathbf{y}} - \\bar{\\mathbf{x}}^\\top\\mathbf{y} + \\|\\bar{\\mathbf{x}}\\|^2$。显然，这与 $\\mathbf{x}^\\top\\mathbf{y}$ 不同，因此中心化改变了成对相似度。\n\n让我们分析其对偏置 $b$ 的影响。\n\n**对非齐次情况 ($c>0$) 的分析**：\n-   特征映射函数 $\\phi(\\cdot)$ 由核的形式决定。当我们对输入数据进行中心化时，我们计算的是 $\\phi(\\mathbf{x}-\\bar{\\mathbf{x}})$。由于函数 $\\phi$ 本身没有改变，如果 $c>0$，它仍然会产生一个常数特征。隐式偏置机制保持不变。\n-   在非中心化的情况下，非零均值 $\\bar{\\mathbf{x}} \\neq \\mathbf{0}$ 会导致数据云的全局平移。这种平移会传播到特征空间，意味着特征向量 $\\{\\phi(\\mathbf{x}_i)\\}_{i=1}^n$ 的均值很可能远离原点。为了最优地分离类别，决策超平面可能需要一个较大的偏移（总偏置）。这个偏移由 $b$ 和隐式偏置共同提供。\n-   通过对输入数据进行中心化，新的训练集 $\\{\\tilde{\\mathbf{x}}_i\\}$ 的均值为零。虽然这不能保证特征向量 $\\{\\phi(\\tilde{\\mathbf{x}}_i)\\}$ 的均值为零，但它确实使特征空间的线性分量中心化，并通常将特征空间数据云的质心移近原点。因此，最优超平面所需的总偏移的幅度通常会减小。\n-   由于所需的总偏移较小，且模型仍然具有灵活的隐式偏置机制（来自与 $c$ 相关的常数特征），优化器很可能会找到一个解，其中显式偏置项 $b$ 的幅度较小。隐式偏置可以“吸收”所需较小偏移的更大部分。\n\n**对齐次情况 ($c=0$) 的分析**：\n-   对于中心化数据，核为 $\\tilde{k}(\\mathbf{u}, \\mathbf{v}) = ((\\mathbf{u}-\\bar{\\mathbf{x}})^\\top(\\mathbf{v}-\\bar{\\mathbf{x}}))^p$。如前所述，相应的特征映射 $\\phi(\\cdot)$ 没有常数特征。显式偏置 $b$ 是决策边界偏移的唯一来源，因此是必不可少的。\n-   考虑非中心化的情况。核为 $k(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^\\top\\mathbf{y})^p$。如果我们重写 $\\mathbf{x} = \\tilde{\\mathbf{x}} + \\bar{\\mathbf{x}}$，内积变为 $\\mathbf{x}^\\top\\mathbf{y} = (\\tilde{\\mathbf{x}}+\\bar{\\mathbf{x}})^\\top(\\tilde{\\mathbf{y}}+\\bar{\\mathbf{x}}) = \\tilde{\\mathbf{x}}^\\top\\tilde{\\mathbf{y}} + \\tilde{\\mathbf{x}}^\\top\\bar{\\mathbf{x}} + \\bar{\\mathbf{x}}^\\top\\tilde{\\mathbf{y}} + \\|\\bar{\\mathbf{x}}\\|^2$。\n-   非中心化数据的核是 $k(\\mathbf{x}, \\mathbf{y}) = (\\tilde{\\mathbf{x}}^\\top\\tilde{\\mathbf{y}} + \\tilde{\\mathbf{x}}^\\top\\bar{\\mathbf{x}} + \\bar{\\mathbf{x}}^\\top\\tilde{\\mathbf{y}} + \\|\\bar{\\mathbf{x}}\\|^2)^p$。当展开时，$\\|\\bar{\\mathbf{x}}\\|^2$ 项（以及其他涉及 $\\bar{\\mathbf{x}}$ 的项）的存在就像一个数据依赖的偏移，产生了类似于非齐次核的效果。它在特征空间中生成了低次项和常数项。这提供了一种形式的隐式偏移。\n-   当我们对数据进行中心化时，我们转而使用 $\\tilde{k}(\\tilde{\\mathbf{x}}, \\tilde{\\mathbf{y}}) = (\\tilde{\\mathbf{x}}^\\top\\tilde{\\mathbf{y}})^p$。这移除了由数据均值 $\\bar{\\mathbf{x}}$ 引起的隐式偏移效应。提供偏移的全部责任现在都落在了显式偏置项 $b$ 上。因此，$b$ 不仅是必不可少的，而且其幅度不一定会减小；它甚至可能需要增加以补偿隐式偏移效应的损失。\n\n### 逐项分析选项\n\n**A. 对于 $c>0$，中心化通过移除由均值引起的线性交叉项来改变成对相似度，同时保持由 $c$ 引入的常数特征不变；结果，与非中心化的情况相比，显式偏置 $b$ 的幅度通常会减小，因为部分截距由核的常数特征承载。对于 $c=0$，没有常数特征，因此 $b$ 仍然是必不可少的，并且其幅度未必减小。**\n-   该陈述准确地反映了上述详细分析。\n    -   对于 $c>0$：它正确地指出常数特征保持不变，并且中心化输入使得数据云在特征空间中更加居中，从而减少了对较大显式偏置 $b$ 的需求。措辞“移除由均值引起的线性交叉项”正确地捕捉到中心化数据的核 $k(\\tilde{\\mathbf{x}}, \\tilde{\\mathbf{y}})$ 比用中心化分量表示的非中心化数据的核 $k(\\tilde{\\mathbf{x}}+\\bar{\\mathbf{x}}, \\tilde{\\mathbf{y}}+\\bar{\\mathbf{x}})$ 更简单。\n    -   对于 $c=0$：它正确地指出中心化核没有常数特征，使得 $b$ 必不可少。它还正确地推断出，由于中心化移除了非零均值产生的隐式偏移生成效应，因此不能保证 $b$ 会减小。\n-   **结论：正确。**\n\n**B. 中心化保持所有成对相似度不变，即对于所有 $\\mathbf{x}, \\mathbf{y}$ 都有 $k((\\mathbf{x}-\\bar{\\mathbf{x}}), (\\mathbf{y}-\\bar{\\mathbf{y}})) = k(\\mathbf{x}, \\mathbf{y})$，因此学习到的偏置 $b$ 不受影响。**\n-   前提是错误的。如前所示，通常情况下 $(\\mathbf{x}-\\bar{\\mathbf{x}})^\\top(\\mathbf{y}-\\bar{\\mathbf{y}}) \\neq \\mathbf{x}^\\top\\mathbf{y}$，所以核值会改变。因此，学习到的参数，包括 $b$，会受到影响。\n-   **结论：不正确。**\n\n**C. 中心化移除了由 $c>0$ 隐含的常数特征，消除了特征映射中的任何隐式截距；因此，学习到的偏置 $b$ 的幅度必须增加。**\n-   该陈述的前提是错误的。将输入向量 $\\mathbf{x}$ 中心化为 $\\mathbf{x}-\\bar{\\mathbf{x}}$ 会改变特征映射函数 $\\phi(\\cdot)$ 的参数，但不会改变函数 $\\phi$ 本身。如果 $c>0$，$\\phi(\\cdot)$ 的定义方式使其无论输入是什么，总会产生一个常数特征。因此，常数特征没有被移除。\n-   **结论：不正确。**\n\n**D. 对于任何多项式核，特征中心化都迫使学习到的偏置 $b$ 精确地为 $0$。**\n-   这是一个过于强烈且错误的断言。对输入数据进行中心化通常不会使数据云在高维特征空间中居中，即 $\\frac{1}{n} \\sum_i \\phi(\\mathbf{x}_i - \\bar{\\mathbf{x}})$ 不一定为 $\\mathbf{0}$。最优分离超平面可能仍然需要一个非零偏移来最好地分离类别，特别是如果类别分布在特征空间中是不对称的。\n-   **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}