{
    "hands_on_practices": [
        {
            "introduction": "This practice explores the fundamental connection between the hard-margin SVM for perfectly separable data and the more general soft-margin SVM. By implementing and observing the behavior of the soft-margin classifier as the regularization parameter $C$ increases, you will gain a concrete understanding of how $C$ controls the trade-off between margin width and classification errors . This exercise provides a hands-on feel for the role of this critical hyperparameter and how the two formulations converge for cleanly separated data.",
            "id": "3178313",
            "problem": "You will implement and analyze a binary linear Support Vector Machine (SVM) within the context of statistical learning. The program you produce must construct a linearly separable dataset, compute the hard-margin solution, and then verify that the soft-margin solution converges to the hard-margin solution for sufficiently large regularization parameter $C$, with a quantitative threshold derived from the Karush–Kuhn–Tucker (KKT) conditions using zero slack variables $\\xi_i=0$.\n\nFundamental base and core definitions to be used:\n- A Support Vector Machine (SVM) aims to find a linear classifier $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b$ such that the decision rule $\\operatorname{sign}(f(\\mathbf{x}))$ separates the classes.\n- For the hard-margin SVM (linearly separable case), the primal optimization problem is to minimize $\\frac{1}{2}\\|\\mathbf{w}\\|^2$ subject to the constraints $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1$ for all $i$.\n- For the soft-margin SVM, the primal optimization problem is to minimize $\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i$ subject to $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i$ and $\\xi_i \\ge 0$ for all $i$.\n- The dual optimization formulation uses Lagrange multipliers $\\alpha_i$ and the Gram matrix $Q$ defined by $Q_{ij} = y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j$. In the hard-margin dual, the constraints are $\\alpha_i \\ge 0$ and $\\sum_{i=1}^n y_i \\alpha_i = 0$. In the soft-margin dual, the constraints are $0 \\le \\alpha_i \\le C$ and $\\sum_{i=1}^n y_i \\alpha_i = 0$.\n- The Karush–Kuhn–Tucker (KKT) conditions impose complementary slackness and stationarity relationships between primal and dual variables.\n\nYour program must:\n1. Construct the following fixed, deterministic, linearly separable dataset in $\\mathbb{R}^2$:\n   - Positive class ($y=+1$): $\\mathbf{x}_1=(2,2)$, $\\mathbf{x}_2=(2,3)$, $\\mathbf{x}_3=(3,2)$, $\\mathbf{x}_4=(3,3)$.\n   - Negative class ($y=-1$): $\\mathbf{x}_5=(-2,-2)$, $\\mathbf{x}_6=(-3,-2)$, $\\mathbf{x}_7=(-2,-3)$, $\\mathbf{x}_8=(-3,-3)$.\n   Use the label vector $y_i \\in \\{-1,+1\\}$ in the natural order of listing.\n2. Solve the hard-margin dual problem to obtain the optimal multipliers $\\alpha_i^{\\mathrm{H}}$, from which compute the hard-margin parameters $\\mathbf{w}_{\\mathrm{H}} = \\sum_{i=1}^n \\alpha_i^{\\mathrm{H}} y_i \\mathbf{x}_i$ and $b_{\\mathrm{H}}$ using the support vectors (those with $\\alpha_i^{\\mathrm{H}} > 0$) via the margin condition $y_i(\\mathbf{w}_{\\mathrm{H}}^\\top \\mathbf{x}_i + b_{\\mathrm{H}}) = 1$ averaged over available support vectors.\n3. Quantify a threshold value $C_\\star$ such that, when $C \\ge C_\\star$, the soft-margin solution attains zero slacks $\\xi_i=0$ and coincides with the hard-margin solution. You must determine $C_\\star$ using the KKT stationarity condition with $\\xi_i=0$ for the soft-margin primal, and relate it to the hard-margin dual multipliers. Do not assume any shortcut formulas; derive $C_\\star$ from first principles of the KKT conditions and feasibility.\n4. For the soft-margin SVM, solve the dual problem for different values of $C$, compute the resulting parameters $\\mathbf{w}(C)$ and $b(C)$, and the slacks $\\xi_i(C) = \\max\\{0, 1 - y_i(\\mathbf{w}(C)^\\top \\mathbf{x}_i + b(C))\\}$.\n5. Verify convergence by comparing $\\mathbf{w}(C)$ and $b(C)$ to $\\mathbf{w}_{\\mathrm{H}}$ and $b_{\\mathrm{H}}$ with appropriate numerical tolerance, and by checking whether all $\\xi_i(C)$ are equal to zero within a tolerance.\n\nTest suite and coverage:\n- Let $C_\\star$ be the threshold you derive. Define the following five test cases that probe different regimes:\n  1. $C = 0.10$ (very small $C$, expected nonzero slacks).\n  2. $C = \\frac{1}{2} C_\\star$ (below threshold).\n  3. $C = C_\\star$ (at threshold).\n  4. $C = 1.10 \\, C_\\star$ (slightly above threshold).\n  5. $C = 1000 \\, C_\\star$ (well above threshold).\n- For each test case, your program must compute and return a result as a list with three entries:\n  - A boolean indicating whether all slacks satisfy $\\xi_i(C) = 0$ within a tolerance of $10^{-6}$.\n  - A boolean indicating whether $\\|\\mathbf{w}(C) - \\mathbf{w}_{\\mathrm{H}}\\|_2 \\le 10^{-5}$ and $|b(C) - b_{\\mathrm{H}}| \\le 10^{-5}$.\n  - The floating-point value of $C_\\star$ (the same per test case).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the three values as described. For example: \"[[true_or_false,true_or_false,C_star_value],[...],...]\".\n- No physical units, angles, or percentages are involved in this problem; all quantities are dimensionless real numbers or booleans.",
            "solution": "The user provided a valid problem.\nThe problem requires an analysis of the relationship between the hard-margin and soft-margin linear Support Vector Machines (SVMs). The primary objective is to solve for the hard-margin SVM, then derive a regularization threshold, $C_\\star$, from first principles, such that for any regularization parameter $C \\ge C_\\star$, the soft-margin SVM solution converges to the hard-margin solution. This convergence is to be verified numerically.\n\nThe analysis proceeds in three stages:\n1.  Solving the hard-margin dual problem for a given linearly separable dataset.\n2.  Deriving the threshold $C_\\star$ from the Karush-Kuhn-Tucker (KKT) conditions.\n3.  Implementing a numerical solution to solve the soft-margin dual problem and verifying the convergence for a set of test values of $C$.\n\n**1. Hard-Margin SVM Solution**\n\nThe dataset is specified as $n=8$ points in $\\mathbb{R}^2$.\nThe data points $\\mathbf{x}_i$ and their corresponding class labels $y_i \\in \\{+1, -1\\}$ are:\n$\\mathbf{X} = \\begin{bmatrix} (2, 2)^\\top \\\\ (2, 3)^\\top \\\\ (3, 2)^\\top \\\\ (3, 3)^\\top \\\\ (-2, -2)^\\top \\\\ (-3, -2)^\\top \\\\ (-2, -3)^\\top \\\\ (-3, -3)^\\top \\end{bmatrix}$, $\\mathbf{y} = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ +1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$.\n\nFor the hard-margin SVM, we seek to minimize $\\frac{1}{2}\\|\\mathbf{w}\\|^2$ subject to $y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b) \\ge 1$ for all $i$. The corresponding dual problem is a quadratic programming (QP) problem:\n$$\n\\underset{\\mathbf{\\alpha}}{\\text{maximize}} \\quad L_D(\\mathbf{\\alpha}) = \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i^\\top \\mathbf{x}_j)\n$$\nsubject to the constraints:\n$$\n\\sum_{i=1}^n \\alpha_i y_i = 0 \\quad \\text{and} \\quad \\alpha_i \\ge 0 \\quad \\text{for } i=1, \\dots, n.\n$$\nThis is equivalent to minimizing $\\frac{1}{2}\\mathbf{\\alpha}^\\top Q \\mathbf{\\alpha} - \\mathbf{1}^\\top \\mathbf{\\alpha}$ subject to the same constraints, where $Q$ is the Gram matrix with elements $Q_{ij} = y_i y_j (\\mathbf{x}_i^\\top \\mathbf{x}_j)$.\n\nSolving this QP problem for the given dataset yields the optimal Lagrange multipliers $\\mathbf{\\alpha}^{\\mathrm{H}}$. Due to the symmetry of the dataset, only the points closest to the opposite class, $\\mathbf{x}_1=(2,2)^\\top$ and $\\mathbf{x}_5=(-2,-2)^\\top$, are support vectors. The optimal multipliers are:\n$$\n\\alpha_1^{\\mathrm{H}} = \\frac{1}{16}, \\quad \\alpha_5^{\\mathrm{H}} = \\frac{1}{16}, \\quad \\text{and} \\quad \\alpha_i^{\\mathrm{H}} = 0 \\text{ for } i \\notin \\{1, 5\\}.\n$$\n\nWith the optimal multipliers, the weight vector $\\mathbf{w}_{\\mathrm{H}}$ is computed as:\n$$\n\\mathbf{w}_{\\mathrm{H}} = \\sum_{i=1}^n \\alpha_i^{\\mathrm{H}} y_i \\mathbf{x}_i = \\alpha_1^{\\mathrm{H}} y_1 \\mathbf{x}_1 + \\alpha_5^{\\mathrm{H}} y_5 \\mathbf{x}_5\n$$\n$$\n\\mathbf{w}_{\\mathrm{H}} = \\frac{1}{16}(+1)\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + \\frac{1}{16}(-1)\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix} = \\frac{1}{16}\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + \\frac{1}{16}\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{8}\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0.25 \\\\ 0.25 \\end{pmatrix}.\n$$\n\nThe bias term $b_{\\mathrm{H}}$ is found using a support vector, for which the margin condition $y_k(\\mathbf{w}_{\\mathrm{H}}^\\top \\mathbf{x}_k + b_{\\mathrm{H}}) = 1$ holds. Using support vector $\\mathbf{x}_1$:\n$$\n(+1)\\left(\\begin{pmatrix} 0.25 \\\\ 0.25 \\end{pmatrix}^\\top \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} + b_{\\mathrm{H}}\\right) = 1 \\implies (0.5 + 0.5) + b_{\\mathrm{H}} = 1 \\implies 1 + b_{\\mathrm{H}} = 1 \\implies b_{\\mathrm{H}} = 0.\n$$\nUsing $\\mathbf{x}_5$ yields the same result. The hard-margin solution is thus $\\mathbf{w}_{\\mathrm{H}} = (0.25, 0.25)^\\top$ and $b_{\\mathrm{H}}=0$.\n\n**2. Derivation of the Convergence Threshold $C_\\star$**\n\nThe primal problem for the soft-margin SVM is:\n$$\n\\underset{\\mathbf{w}, b, \\mathbf{\\xi}}{\\text{minimize}} \\quad \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n$$\nsubject to $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i$ and $\\xi_i \\ge 0$ for $i=1, \\dots, n$.\n\nThe Lagrangian for this problem is:\n$$\nL_P(\\mathbf{w}, b, \\mathbf{\\xi}, \\mathbf{\\alpha}, \\mathbf{\\mu}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i [y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) - 1 + \\xi_i] - \\sum_{i=1}^n \\mu_i \\xi_i,\n$$\nwhere $\\alpha_i \\ge 0$ and $\\mu_i \\ge 0$ are Lagrange multipliers. The KKT stationarity conditions are:\n1.  $\\nabla_{\\mathbf{w}} L_P = \\mathbf{w} - \\sum_i \\alpha_i y_i \\mathbf{x}_i = \\mathbf{0} \\implies \\mathbf{w} = \\sum_i \\alpha_i y_i \\mathbf{x}_i$\n2.  $\\frac{\\partial L_P}{\\partial b} = -\\sum_i \\alpha_i y_i = 0$\n3.  $\\frac{\\partial L_P}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0$\n\nThe soft-margin solution $(\\mathbf{w}(C), b(C))$ converges to the hard-margin solution $(\\mathbf{w}_{\\mathrm{H}}, b_{\\mathrm{H}})$ if and only if all slack variables $\\xi_i$ are zero. When $\\xi_i=0$ for all $i$, the soft-margin problem constraints effectively become the hard-margin constraints. The solution $(\\mathbf{w}_{\\mathrm{H}}, b_{\\mathrm{H}})$ and its dual variables $\\alpha_i^{\\mathrm{H}}$ must satisfy all KKT conditions of the soft-margin problem.\n\nConditions (1) and (2) are identical to the hard-margin formulation and are satisfied by construction. Condition (3), $C - \\alpha_i - \\mu_i = 0$, combined with the non-negativity constraint $\\mu_i \\ge 0$, imposes a new requirement on the multipliers $\\alpha_i$:\n$$\nC - \\alpha_i = \\mu_i \\ge 0 \\implies C \\ge \\alpha_i.\n$$\nThis inequality must hold for the hard-margin multipliers $\\alpha_i^{\\mathrm{H}}$ to be feasible dual variables for the soft-margin problem (which correspond to a zero-slack primal solution). Therefore, for the solutions to coincide, we must have $C \\ge \\alpha_i^{\\mathrm{H}}$ for all $i=1, \\dots, n$.\nThe minimal value of $C$ that guarantees this condition for all multipliers is the threshold $C_\\star$:\n$$\nC_\\star = \\max_{i} \\{\\alpha_i^{\\mathrm{H}}\\}.\n$$\nUsing the hard-margin solution, we find:\n$$\nC_\\star = \\max\\left\\{\\frac{1}{16}, 0, \\dots, \\frac{1}{16}, \\dots, 0\\right\\} = \\frac{1}{16} = 0.0625.\n$$\nFor any $C \\ge C_\\star$, the optimal soft-margin multipliers $\\alpha_i(C)$ will be identical to $\\alpha_i^{\\mathrm{H}}$, and consequently $\\mathbf{w}(C) = \\mathbf{w}_{\\mathrm{H}}$, $b(C) = b_{\\mathrm{H}}$, and all $\\xi_i(C) = 0$.\n\n**3. Algorithmic Implementation and Verification**\n\nAn algorithm is implemented to solve the dual QP problem for both the hard-margin case (by setting $C=\\infty$) and the soft-margin case for various finite $C$.\n1.  **Solve Hard-Margin Case**: The QP solver is used to find $\\mathbf{\\alpha}^{\\mathrm{H}}$, from which $\\mathbf{w}_{\\mathrm{H}}$, $b_{\\mathrm{H}}$, and $C_\\star = \\max(\\mathbf{\\alpha}^{\\mathrm{H}})$ are computed.\n2.  **Iterate Through Test Cases**: For each specified value of $C$ ($0.10$, $0.5 C_\\star$, $C_\\star$, $1.1 C_\\star$, $1000 C_\\star$), the soft-margin dual QP is solved with constraints $0 \\le \\alpha_i \\le C$ to find $\\mathbf{\\alpha}(C)$.\n3.  **Compute Soft-Margin Parameters**: From $\\mathbf{\\alpha}(C)$, the parameters $\\mathbf{w}(C)$ and $b(C)$ are calculated. The bias $b(C)$ is determined by averaging $y_i - \\mathbf{w}(C)^\\top\\mathbf{x}_i$ over all support vectors for which $0 < \\alpha_i(C) < C$. If no such points exist, $b(C)$ is taken as the midpoint of the feasible range determined by all support vectors ($\\alpha_i(C)>0$).\n4.  **Compute Slacks and Verify**: The slack variables are computed via $\\xi_i(C) = \\max\\{0, 1 - y_i(\\mathbf{w}(C)^\\top\\mathbf{x}_i + b(C))\\}$.\n5.  **Check Conditions**: Two conditions are checked: (a) if all $\\xi_i(C)$ are zero within a tolerance of $10^{-6}$, and (b) if $\\mathbf{w}(C)$ and $b(C)$ match $\\mathbf{w}_{\\mathrm{H}}$ and $b_{\\mathrm{H}}$ within a tolerance of $10^{-5}$.\n\nThe results for the specified test cases are expected to confirm that for $C \\ge C_\\star$, the soft-margin solution is identical to the hard-margin solution (zero slacks, matching parameters), while for $C<C_\\star$, the solutions differ and non-zero slacks appear.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    \"\"\"\n    Main function to perform SVM analysis as described in the problem.\n    \"\"\"\n    # 1. Construct the fixed, deterministic, linearly separable dataset.\n    X = np.array([\n        [2, 2], [2, 3], [3, 2], [3, 3],  # Positive class\n        [-2, -2], [-3, -2], [-2, -3], [-3, -3]  # Negative class\n    ])\n    y = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n    n_samples, n_features = X.shape\n\n    # Gram matrix Q_ij = y_i * y_j * x_i^T * x_j\n    Q = np.outer(y, y) * (X @ X.T)\n\n    def solve_svm_dual(C_val):\n        \"\"\"Solves the SVM dual QP problem for a given C.\"\"\"\n        \n        # Objective function to minimize: 0.5 * alpha^T * Q * alpha - 1^T * alpha\n        def objective(alpha):\n            return 0.5 * alpha @ Q @ alpha - np.sum(alpha)\n\n        # Equality constraint: sum(alpha_i * y_i) = 0\n        constraints = [{'type': 'eq', 'fun': lambda alpha: alpha @ y}]\n\n        # Bounds on alpha: 0 <= alpha_i <= C\n        # For hard-margin, C is effectively infinity.\n        bounds = Bounds(0, C_val)\n\n        # Initial guess for alpha\n        alpha0 = np.zeros(n_samples)\n\n        # Solve the QP problem\n        res = minimize(objective, alpha0, method='SLSQP', bounds=bounds, constraints=constraints)\n        \n        return res.x if res.success else None\n\n    def get_w_b(alpha, C):\n        \"\"\"Computes w and b from the optimal alpha values.\"\"\"\n        # Set small alpha values to zero\n        alpha[alpha < 1e-7] = 0\n        \n        # Compute w = sum(alpha_i * y_i * x_i)\n        w = np.sum((alpha * y)[:, np.newaxis] * X, axis=0)\n        \n        # Find support vectors\n        sv_indices = np.where(alpha > 1e-7)[0]\n        \n        # Robustly compute b\n        # Try to find support vectors on the margin (0 < alpha < C)\n        margin_sv_indices = np.where((alpha > 1e-7) & (alpha < C - 1e-7))[0]\n        \n        if len(margin_sv_indices) > 0:\n            b = np.mean([y[i] - X[i] @ w for i in margin_sv_indices])\n        elif len(sv_indices) > 0:\n            # Fallback for cases where no SVs are on the margin (e.g., small C)\n            # Use all SVs to find the feasible range for b\n            sv_pos_indices = [i for i in sv_indices if y[i] > 0]\n            sv_neg_indices = [i for i in sv_indices if y[i] < 0]\n            \n            b_highs = [1 - X[i] @ w for i in sv_pos_indices]\n            b_lows = [-1 - X[i] @ w for i in sv_neg_indices]\n            \n            b_high = min(b_highs) if b_highs else np.inf\n            b_low = max(b_lows) if b_lows else -np.inf\n            \n            b = (b_low + b_high) / 2.0\n        else: # No support vectors, should not happen for this problem\n            b = 0.0\n\n        return w, b\n\n    # 2. Solve the hard-margin dual problem (C -> infinity)\n    alpha_H = solve_svm_dual(np.inf)\n    if alpha_H is None:\n        raise RuntimeError(\"Hard-margin QP optimization failed.\")\n    \n    w_H, b_H = get_w_b(alpha_H, np.inf)\n\n    # 3. Quantify the threshold value C_star\n    C_star = np.max(alpha_H)\n\n    # 4. Define test cases\n    test_cases = [\n        0.10,\n        0.5 * C_star,\n        C_star,\n        1.10 * C_star,\n        1000 * C_star\n    ]\n\n    all_results = []\n    \n    # Tolerances\n    slack_tol = 1e-6\n    param_tol = 1e-5\n\n    # 5. Loop through test cases, solve soft-margin SVM and verify\n    for C in test_cases:\n        alpha_C = solve_svm_dual(C)\n        if alpha_C is None:\n            # Append a failure state if optimization fails\n            all_results.append([False, False, C_star])\n            continue\n\n        w_C, b_C = get_w_b(alpha_C.copy(), C)\n        \n        # Compute slacks\n        slacks = np.maximum(0, 1 - y * (X @ w_C + b_C))\n        \n        # Check conditions\n        all_slacks_zero = np.all(slacks < slack_tol)\n        params_converged = np.linalg.norm(w_C - w_H) <= param_tol and abs(b_C - b_H) <= param_tol\n        \n        result = [all_slacks_zero, params_converged, C_star]\n        all_results.append(result)\n\n    # Format output according to problem specification\n    def format_result(res_list):\n        bool1, bool2, c_val = res_list\n        # The example in the prompt uses lowercase booleans\n        return f\"[{str(bool1).lower()},{str(bool2).lower()},{c_val:.6f}]\"\n\n    formatted_results = [format_result(res) for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many real-world datasets are not linearly separable, which is where the power of the kernel trick comes into play. This exercise will guide you through the process of making a non-linear dataset separable by mapping it to a higher-dimensional feature space, a core concept in modern machine learning . You will compare the \"explicit\" method of building this feature space with the \"implicit\" method of using a kernel function, providing a clear and practical demonstration of why the kernel trick is so elegant and efficient.",
            "id": "3178252",
            "problem": "You are asked to write a complete, runnable program that demonstrates, from first principles, how an explicit polynomial feature augmentation realizes a quadratic kernel within a Support Vector Machine (SVM) classification framework, and to compare explicit and implicit realizations. Work strictly in binary classification with labels in $\\{-1,+1\\}$. Use the hard-margin Support Vector Machine (SVM) formulation as the mathematical foundation, and approximate it numerically through the soft-margin dual with a large penalty parameter $C$ to ensure feasibility and numerical stability.\n\nFundamental base and definitions to employ:\n- The hard-margin SVM primal problem seeks a separating hyperplane in a feature space $\\mathcal{H}$ by solving\n$$\n\\min_{\\mathbf{w}, b}\\ \\frac{1}{2}\\lVert \\mathbf{w} \\rVert^2 \\quad \\text{subject to}\\quad y_i\\left(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b\\right) \\ge 1 \\ \\text{for all } i,\n$$\nwhere $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$ is a feature map and $y_i \\in \\{-1,+1\\}$ are labels.\n- The soft-margin SVM dual problem (used for numerical computation) minimizes the convex objective\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha} \\quad \\text{subject to}\\quad \\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0,\\ \\ 0 \\le \\alpha_i \\le C,\n$$\nwith $\\mathbf{Q}_{ij} = y_i y_j K(\\mathbf{x}_i,\\mathbf{x}_j)$, where $K$ is a positive semidefinite kernel. For a linear kernel $K(\\mathbf{x},\\mathbf{z}) = \\mathbf{x}^\\top \\mathbf{z}$, this corresponds to an SVM in the original input space. For a nonlinear kernel, the decision function is $f(\\mathbf{x}) = \\sum_{i=1}^n \\alpha_i y_i K(\\mathbf{x}_i,\\mathbf{x}) + b$.\n- The geometric margin in the feature space is $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$, where $\\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\phi(\\mathbf{x}_i)$ when using the representer theorem.\n- The explicit quadratic feature map in $\\mathbb{R}^2$ is defined by\n$$\n\\phi(\\mathbf{x}) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2}\\,x_1 x_2 \\end{bmatrix}.\n$$\nYou will realize the homogeneous quadratic kernel implicitly as $K(\\mathbf{x},\\mathbf{z}) = \\left(\\mathbf{x}^\\top \\mathbf{z}\\right)^2$, and explicitly via the inner product $\\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$.\n\nTasks your program must perform:\n- Implement a soft-margin SVM solver in the dual with penalty parameter $C$ chosen as $C = 10^5$ to approximate the hard-margin solution. Use this to compute $\\boldsymbol{\\alpha}$, the bias $b$, and predictions.\n- For the homogeneous quadratic kernel, compare the implicit kernel $K(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z})^2$ to the explicit inner product $\\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$ by numerically checking the equality of the corresponding Gram matrices on the training points.\n- Using the learned dual variables for the quadratic kernel, explicitly reconstruct $\\mathbf{w}$ in the augmented feature space via $\\mathbf{w} = \\sum_i \\alpha_i y_i \\phi(\\mathbf{x}_i)$, compute the geometric margin $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$, and verify that decision values computed using the explicit $(\\mathbf{w}, b)$ and the implicit kernel decision function agree on the training points.\n- Assess linear separability in the original input space by training with the linear kernel $K(\\mathbf{x},\\mathbf{z})=\\mathbf{x}^\\top \\mathbf{z}$ and checking whether all training labels are correctly classified. Similarly, assess separability under the quadratic kernel.\n\nAngle units are not applicable. No physical units are involved. All reported numerical margins must be real numbers (floats). All booleans must be exact logical values. Do not output percentages.\n\nTest suite to cover multiple facets:\n- Dataset A (nonlinearly separable in input, linearly separable after quadratic mapping):\n  - Inputs $\\mathbf{X}_A = \\left\\{(1,1),\\ (1,-1),\\ (-1,1),\\ (-1,-1)\\right\\}$, labels $\\mathbf{y}_A = \\{+1,\\ -1,\\ -1,\\ +1\\}$.\n- Dataset B (already linearly separable in input):\n  - Inputs $\\mathbf{X}_B = \\left\\{(2,2),\\ (2,3),\\ (-2,-2),\\ (-2,-3)\\right\\}$, labels $\\mathbf{y}_B = \\{+1,\\ +1,\\ -1,\\ -1\\}$.\n- Dataset C (minimal two-point opposite-class set):\n  - Inputs $\\mathbf{X}_C = \\left\\{(1,0),\\ (-1,0)\\right\\}$, labels $\\mathbf{y}_C = \\{+1,\\ -1\\}$.\n\nFor each dataset in the order A, B, C, your program must compute and return the following five values:\n$1.$ A boolean indicating whether perfect classification is achieved by the linear kernel on the training data (interpreted as linear separability in input space).\n$2.$ A boolean indicating whether perfect classification is achieved by the quadratic kernel on the training data.\n$3.$ A boolean indicating whether the explicit Gram matrix $\\left[\\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)\\right]_{ij}$ equals the implicit Gram matrix $\\left[(\\mathbf{x}_i^\\top \\mathbf{x}_j)^2\\right]_{ij}$ within an absolute tolerance of $10^{-10}$.\n$4.$ A boolean indicating whether the decision values computed explicitly via $(\\mathbf{w},b)$ in the augmented feature space agree in sign with those from the implicit kernel decision function on the training data.\n$5.$ The geometric margin $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$ computed in the explicit feature space for the quadratic kernel, rounded to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the $15$ results for datasets A, B, and C in order, as a comma-separated list enclosed in square brackets (for example, \"[resultA1,resultA2,resultA3,resultA4,resultA5,resultB1,...,resultC5]\"). The booleans and floats must appear in this exact sequence and order, with the margin values rounded to six decimal places as specified. No other text must be printed.",
            "solution": "The problem requires a numerical demonstration and verification of the kernel trick, specifically for a homogeneous quadratic kernel in a Support Vector Machine (SVM) context. This involves implementing an SVM solver, comparing an implicit kernel function to an explicit feature map, and evaluating performance on several datasets.\n\nThe mathematical basis is the SVM formulation for binary classification. Given a training dataset $\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ with input vectors $\\mathbf{x}_i \\in \\mathbb{R}^d$ and class labels $y_i \\in \\{-1, +1\\}$, the hard-margin SVM aims to find a separating hyperplane $(\\mathbf{w}, b)$ in a feature space $\\mathcal{H}$. The mapping from the input space to the feature space is given by a function $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$. The objective is to maximize the geometric margin, $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$, which is equivalent to solving the following primal optimization problem:\n$$\n\\min_{\\mathbf{w}, b} \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 \\quad \\text{subject to} \\quad y_i (\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b) \\ge 1, \\quad \\forall i=1, \\dots, n.\n$$\nFor computational purposes, particularly when the feature space $\\mathcal{H}$ is high-dimensional, it is more practical to solve the dual problem. The program will approximate the hard-margin solution by using the soft-margin dual formulation with a very large penalty parameter, $C = 10^5$. The dual problem is a convex quadratic programming (QP) problem expressed as:\n$$\n\\min_{\\boldsymbol{\\alpha}} \\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha} \\quad \\text{subject to} \\quad \\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0, \\quad 0 \\le \\alpha_i \\le C,\n$$\nwhere $\\boldsymbol{\\alpha} = [\\alpha_1, \\dots, \\alpha_n]^\\top$ are the Lagrange multipliers, $\\mathbf{1}$ is an $n$-dimensional vector of ones, and $\\mathbf{Q}$ is an $n \\times n$ matrix with entries $\\mathbf{Q}_{ij} = y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$. The kernel function, $K(\\mathbf{x}, \\mathbf{z}) = \\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$, computes the inner product in the feature space without the need to explicitly compute the feature vectors $\\phi(\\mathbf{x})$. This is the essence of the kernel trick.\n\nUpon solving for the optimal Lagrange multipliers $\\boldsymbol{\\alpha}^*$, the weight vector $\\mathbf{w}$ in the feature space is given by the representer theorem:\n$$\n\\mathbf{w} = \\sum_{i=1}^n \\alpha_i^* y_i \\phi(\\mathbf{x}_i).\n$$\nThe bias term $b$ can be calculated from any support vector $\\mathbf{x}_k$ for which $0 < \\alpha_k^* < C$, using the Karush-Kuhn-Tucker (KKT) condition $y_k(\\mathbf{w}^\\top \\phi(\\mathbf{x}_k) + b) = 1$, which gives:\n$$\nb = y_k - \\mathbf{w}^\\top \\phi(\\mathbf{x}_k) = y_k - \\sum_{i=1}^n \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}_k).\n$$\nFor numerical robustness, $b$ is typically averaged over all such support vectors.\n\nThe problem specifies two kernels: a linear kernel $K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^\\top \\mathbf{z}$ (where $\\phi(\\mathbf{x}) = \\mathbf{x}$) and a homogeneous quadratic kernel $K(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z})^2$. The key task is to verify that this quadratic kernel is realized by the explicit feature map $\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^3$ defined as:\n$$\n\\phi(\\mathbf{x}) = \\phi\\left(\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\right) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2} x_1 x_2 \\end{bmatrix}.\n$$\nThe equivalence is confirmed by calculating the inner product in this feature space:\n$$\n\\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z}) = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ \\sqrt{2} x_1 x_2 \\end{bmatrix}^\\top \\begin{bmatrix} z_1^2 \\\\ z_2^2 \\\\ \\sqrt{2} z_1 z_2 \\end{bmatrix} = x_1^2 z_1^2 + x_2^2 z_2^2 + 2x_1 x_2 z_1 z_2 = (x_1 z_1 + x_2 z_2)^2 = (\\mathbf{x}^\\top \\mathbf{z})^2.\n$$\nThe program will implement a QP solver using `scipy.optimize.minimize` to find $\\boldsymbol{\\alpha}^*$. For each of the three datasets provided, it will perform the following five analyses:\n$1$. **Linear Separability**: Train an SVM with the linear kernel and check if it perfectly classifies the training data.\n$2$. **Quadratic Separability**: Train an SVM with the quadratic kernel and check for perfect classification.\n$3$. **Gram Matrix Equivalence**: Numerically verify that the Gram matrix from the implicit kernel, $[\\mathbf{G}_{\\text{implicit}}]_{ij} = (\\mathbf{x}_i^\\top \\mathbf{x}_j)^2$, is identical to the Gram matrix from the explicit feature map, $[\\mathbf{G}_{\\text{explicit}}]_{ij} = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)$.\n$4$. **Decision Function Equivalence**: For the quadratic kernel, compute the decision function values $f(\\mathbf{x})$ for all training points in two ways: implicitly via $f(\\mathbf{x}) = \\sum_i \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}) + b$, and explicitly via $f(\\mathbf{x}) = \\mathbf{w}^\\top \\phi(\\mathbf{x}) + b$. The signs of these values must agree.\n$5$. **Geometric Margin Calculation**: Using the explicitly constructed $\\mathbf{w}$ for the quadratic kernel, compute the geometric margin $\\gamma = 1/\\lVert \\mathbf{w} \\rVert$. If the feature-space data is not linearly separable (as in Dataset C), the separating hyperplane is undefined, leading to $\\lVert \\mathbf{w} \\rVert \\to 0$. In this case, the margin is considered to be $0.0$, signifying the absence of a separating margin.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_svm_dual(X, y, C, kernel):\n    \"\"\"\n    Solves the dual soft-margin SVM problem using a QP solver.\n    \"\"\"\n    n_samples, _ = X.shape\n\n    # Construct the Gram matrix\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(n_samples):\n            K[i, j] = kernel(X[i], X[j])\n\n    # Construct the Q matrix for the QP problem\n    Q = np.outer(y, y) * K\n\n    # Define the objective function (to be minimized)\n    def objective(alpha):\n        return 0.5 * np.dot(alpha, np.dot(Q, alpha)) - np.sum(alpha)\n\n    # Define the equality constraint: sum(alpha_i * y_i) = 0\n    constraints = ({'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)})\n\n    # Define the bounds for alpha_i: 0 <= alpha_i <= C\n    bounds = [(0, C) for _ in range(n_samples)]\n\n    # Initial guess for alpha\n    alpha_init = np.zeros(n_samples)\n\n    # Solve the QP problem\n    res = minimize(objective, alpha_init, method='SLSQP', bounds=bounds, constraints=constraints)\n    alphas = res.x\n    \n    # Clean up small alpha values\n    alphas[alphas < 1e-6] = 0\n\n    # Calculate the bias term 'b'\n    # It's robust to average 'b' over all support vectors.\n    sv_indices = np.where(alphas > 1e-6)[0]\n    \n    if len(sv_indices) > 0:\n        b_values = []\n        for sv_idx in sv_indices:\n            # Prediction for a support vector without bias\n            pred_sv = np.sum(alphas * y * K[:, sv_idx])\n            b_values.append(y[sv_idx] - pred_sv)\n        b = np.mean(b_values)\n    else:\n        # Fallback if no support vectors found (unlikely for these datasets)\n        b = 0.0\n\n    return alphas, b\n\ndef get_decision_values(X_eval, X_train, y_train, alphas, b, kernel):\n    \"\"\"\n    Computes decision values for given data points.\n    \"\"\"\n    n_eval = X_eval.shape[0]\n    n_train = X_train.shape[0]\n    decision_values = np.zeros(n_eval)\n    \n    for i in range(n_eval):\n        s = 0\n        for j in range(n_train):\n            if alphas[j] > 1e-6:\n                s += alphas[j] * y_train[j] * kernel(X_train[j], X_eval[i])\n        decision_values[i] = s + b\n\n    return decision_values\n\ndef solve_case(X, y, C):\n    \"\"\"\n    Performs all 5 required tasks for a single dataset.\n    \"\"\"\n    n_samples, _ = X.shape\n\n    # Kernel function definitions\n    linear_kernel = lambda x, z: np.dot(x, z)\n    quad_kernel = lambda x, z: np.dot(x, z)**2\n\n    # 1. Assess linear separability\n    alphas_lin, b_lin = solve_svm_dual(X, y, C, linear_kernel)\n    dec_vals_lin = get_decision_values(X, X, y, alphas_lin, b_lin, linear_kernel)\n    is_lin_separable = np.all(np.sign(dec_vals_lin) == y)\n\n    # 2. Assess quadratic separability\n    alphas_quad, b_quad = solve_svm_dual(X, y, C, quad_kernel)\n    dec_vals_quad_implicit = get_decision_values(X, X, y, alphas_quad, b_quad, quad_kernel)\n    is_quad_separable = np.all(np.sign(dec_vals_quad_implicit) == y)\n\n    # 3. Verify Gram matrix equality\n    G_implicit = np.array([[quad_kernel(xi, xj) for xj in X] for xi in X])\n    \n    phi_map = lambda x: np.array([x[0]**2, x[1]**2, np.sqrt(2) * x[0] * x[1]])\n    Phi = np.array([phi_map(x_i) for x_i in X])\n    G_explicit = Phi @ Phi.T\n    \n    gram_matrices_equal = np.allclose(G_implicit, G_explicit, atol=1e-10)\n\n    # 4. Verify decision function agreement\n    w_explicit = np.sum(alphas_quad[:, np.newaxis] * y[:, np.newaxis] * Phi, axis=0)\n    dec_vals_quad_explicit = Phi @ w_explicit + b_quad\n    decision_funcs_agree = np.all(np.sign(dec_vals_quad_implicit) == np.sign(dec_vals_quad_explicit))\n    \n    # 5. Compute geometric margin for the quadratic kernel\n    w_norm = np.linalg.norm(w_explicit)\n    # If not separable, the concept of margin is ill-defined. Report 0.\n    # For Dataset C (quadratic), points collapse, w is numerically zero.\n    if not is_quad_separable and w_norm < 1e-9:\n        geometric_margin = 0.0\n    else:\n        geometric_margin = 1.0 / w_norm if w_norm > 1e-9 else 0.0\n\n    return [\n        is_lin_separable,\n        is_quad_separable,\n        gram_matrices_equal,\n        decision_funcs_agree,\n        round(geometric_margin, 6)\n    ]\n\ndef solve():\n    \"\"\"\n    Main entry point to run all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        (np.array([[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]]),\n         np.array([1, -1, -1, 1])),\n        # Dataset B\n        (np.array([[2.0, 2.0], [2.0, 3.0], [-2.0, -2.0], [-2.0, -3.0]]),\n         np.array([1, 1, -1, -1])),\n        # Dataset C\n        (np.array([[1.0, 0.0], [-1.0, 0.0]]),\n         np.array([1, -1]))\n    ]\n    \n    C = 1.0e5\n    final_output_list = []\n    \n    for X, y in test_cases:\n        results = solve_case(X, y, C)\n        \n        # Append results to the final list, formatting as required\n        final_output_list.append(str(results[0]).lower())\n        final_output_list.append(str(results[1]).lower())\n        final_output_list.append(str(results[2]).lower())\n        final_output_list.append(str(results[3]).lower())\n        final_output_list.append(f\"{results[4]:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(final_output_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Understanding the theory of SVMs is one thing, but knowing how they are trained computationally reveals a deeper layer of insight. This advanced practice delves into the Sequential Minimal Optimization (SMO) algorithm, the standard method for efficiently solving the SVM dual problem . By deriving the update rule for a pair of dual variables and applying it to a numerical example, you will appreciate the mechanics of the optimization process that finds the optimal separating hyperplane.",
            "id": "3178321",
            "problem": "A binary soft-margin Support Vector Machine (SVM) trains by maximizing the dual objective subject to equality and box constraints. Let there be training pairs $\\{(x_{k}, y_{k})\\}_{k=1}^{n}$ with labels $y_{k} \\in \\{-1, +1\\}$ and a positive semidefinite kernel $K(\\cdot,\\cdot)$. The dual variables are $\\alpha_{k}$, the penalty parameter is $C > 0$, and the dual problem is to maximize a concave quadratic objective as a function of the vector $\\alpha$, subject to the constraints $0 \\le \\alpha_{k} \\le C$ for all $k$ and $\\sum_{k=1}^{n} \\alpha_{k} y_{k} = 0$. The Sequential Minimal Optimization (SMO) method updates a pair $(\\alpha_{i}, \\alpha_{j})$ while holding the other coordinates fixed, ensuring the equality constraint and box constraints remain satisfied.\n\nStarting only from the SVM dual objective and the constraints described above, and using a line-search argument restricted to the two-dimensional subspace affecting only $\\alpha_{i}$ and $\\alpha_{j}$ while preserving the equality constraint $\\sum_{k=1}^{n} \\alpha_{k} y_{k} = 0$, derive the closed-form pair-update for $(\\alpha_{i}, \\alpha_{j})$ that results from optimizing the dual objective along this feasible line and then applying the box constraints $0 \\le \\alpha_{i}, \\alpha_{j} \\le C$. Express your derivation in terms of the kernel entries $K(x_{i}, x_{i})$, $K(x_{j}, x_{j})$, $K(x_{i}, x_{j})$, the labels $y_{i}, y_{j}$, and the current prediction errors $E_{i}$ and $E_{j}$ defined by $E_{k} = f(x_{k}) - y_{k}$, where $f$ is the current decision function.\n\nThen, evaluate one SMO update step numerically for the following specific scenario (chosen to be ill-conditioned in the sense that the two-by-two kernel submatrix has small curvature):\n- Penalty: $C = 1$.\n- Current variables: $\\alpha_{i} = 0.4$, $\\alpha_{j} = 0.6$.\n- Labels: $y_{i} = +1$, $y_{j} = -1$.\n- Kernel entries: $K(x_{i}, x_{i}) = 1$, $K(x_{j}, x_{j}) = 1$, $K(x_{i}, x_{j}) = 0.9999$.\n- Errors: $E_{i} = 0.3$, $E_{j} = -0.4$.\n\nCompute the bounds for the feasible interval of $\\alpha_{j}$ implied by preserving the equality constraint and the box constraints, perform the pair update, and determine the updated values $\\alpha_{i}^{\\text{new}}$ and $\\alpha_{j}^{\\text{new}}$. Additionally, using your line-search derivation, provide the closed-form expression for the one-step improvement $\\Delta W$ in the dual objective (the amount by which the dual objective increases in this SMO step) in terms of the curvature parameter and the actual clipped step, and evaluate it numerically for the given data. Round all requested numerical outputs to four significant figures.\n\nYour final answer must be a single row matrix containing, in order, the updated values $\\alpha_{i}^{\\text{new}}$, $\\alpha_{j}^{\\text{new}}$, and the dual objective improvement $\\Delta W$, rounded to four significant figures and with no units.",
            "solution": "The problem is valid as it is a standard, well-posed problem in statistical learning concerning the derivation and application of the Sequential Minimal Optimization (SMO) algorithm for Support Vector Machines (SVMs). All provided information is scientifically sound, self-contained, and consistent.\n\n### Part 1: Derivation of the SMO Pair-Update\n\nThe dual problem for a soft-margin SVM is to maximize the objective function $W(\\alpha)$ with respect to the dual variables $\\alpha_k$:\n$$\nW(\\alpha) = \\sum_{k=1}^{n} \\alpha_k - \\frac{1}{2} \\sum_{k=1}^{n} \\sum_{l=1}^{n} \\alpha_k \\alpha_l y_k y_l K(x_k, x_l)\n$$\nsubject to the constraints:\n$$\n\\sum_{k=1}^{n} \\alpha_k y_k = 0\n$$\n$$\n0 \\le \\alpha_k \\le C, \\quad \\text{for } k = 1, \\dots, n\n$$\nwhere $K(x_k, x_l)$ is denoted as $K_{kl}$.\n\nSMO selects a pair of variables, say $\\alpha_i$ and $\\alpha_j$, to optimize while keeping all other variables $\\alpha_k$ (for $k \\ne i, j$) fixed. Let the current (old) values be $\\alpha_k^{\\text{old}}$ and the new values be $\\alpha_k^{\\text{new}}$.\n\nFrom the equality constraint, we have:\n$$\n\\sum_{k=1}^{n} \\alpha_k^{\\text{new}} y_k = \\alpha_i^{\\text{new}} y_i + \\alpha_j^{\\text{new}} y_j + \\sum_{k \\ne i,j} \\alpha_k^{\\text{old}} y_k = 0\n$$\nSince the old values also satisfy the constraint:\n$$\n\\alpha_i^{\\text{old}} y_i + \\alpha_j^{\\text{old}} y_j + \\sum_{k \\ne i,j} \\alpha_k^{\\text{old}} y_k = 0\n$$\nit follows that:\n$$\n\\alpha_i^{\\text{new}} y_i + \\alpha_j^{\\text{new}} y_j = \\alpha_i^{\\text{old}} y_i + \\alpha_j^{\\text{old}} y_j = \\gamma\n$$\nwhere $\\gamma$ is a constant during this update step. Multiplying by $y_i$ (and using $y_i^2 = 1$), we can express $\\alpha_i^{\\text{new}}$ in terms of $\\alpha_j^{\\text{new}}$:\n$$\n\\alpha_i^{\\text{new}} = \\gamma y_i - \\alpha_j^{\\text{new}} y_i y_j\n$$\nLet $s = y_i y_j$. Then $\\alpha_i^{\\text{new}} = \\gamma y_i - s \\alpha_j^{\\text{new}}$. This shows that the new values must lie on a line.\n\nWe now optimize the objective function $W$ along this line. We can express $W$ as a function of a single variable, $\\alpha_j^{\\text{new}}$. Let's drop the \"new\" superscript for simplicity.\nThe objective $W$ is a quadratic function of the $\\alpha_k$. Substituting $\\alpha_i = \\gamma y_i - s \\alpha_j$ into $W$, it becomes a quadratic function of $\\alpha_j$. We are interested in its second derivative to find the maximum. The terms in $W$ involving $\\alpha_i$ and $\\alpha_j$ are:\n$$\nW_{\\text{sub}} = \\alpha_i + \\alpha_j - \\frac{1}{2} (\\alpha_i^2 K_{ii} + \\alpha_j^2 K_{jj} + 2 \\alpha_i \\alpha_j y_i y_j K_{ij}) + \\dots\n$$\nwhere other terms are either constant or linear in $\\alpha_i, \\alpha_j$. The quadratic part in $(\\alpha_i, \\alpha_j)$ is $-\\frac{1}{2}(\\alpha_i^2 K_{ii} + \\alpha_j^2 K_{jj} + 2 s \\alpha_i \\alpha_j K_{ij})$. Substituting $\\alpha_i = \\gamma y_i - s \\alpha_j$:\n$$\n-\\frac{1}{2}((\\gamma y_i - s \\alpha_j)^2 K_{ii} + \\alpha_j^2 K_{jj} + 2s(\\gamma y_i - s \\alpha_j)\\alpha_j K_{ij})\n$$\nThe coefficient of $\\alpha_j^2$ is:\n$$\n-\\frac{1}{2}((-s)^2 K_{ii} + K_{jj} + 2s(-s)K_{ij}) = -\\frac{1}{2}(s^2 K_{ii} + K_{jj} - 2s^2 K_{ij}) = -\\frac{1}{2}(K_{ii} + K_{jj} - 2K_{ij})\n$$\nsince $s^2 = (y_i y_j)^2 = y_i^2 y_j^2 = 1$. Let $\\eta = K_{ii} + K_{jj} - 2K_{ij}$. The second derivative of $W$ with respect to $\\alpha_j$ along the feasible line is $\\frac{d^2 W}{d\\alpha_j^2} = -\\eta$. Since $K$ is positive semidefinite, the corresponding Gram matrix is positive semidefinite, which implies $\\eta \\ge 0$. For $\\eta > 0$, $W$ is a strictly concave quadratic function of $\\alpha_j$, and has a unique maximum.\n\nTo find the location of this maximum, we set the first derivative to zero. Let $u_k = \\sum_{l=1}^n \\alpha_l y_l K_{lk} = f(x_k) - b$. The gradient of $W$ is $\\frac{\\partial W}{\\partial \\alpha_k} = 1 - y_k u_k$. The derivative of $W$ along the line is:\n$$\n\\frac{dW}{d\\alpha_j} = \\frac{\\partial W}{\\partial \\alpha_j} + \\frac{\\partial W}{\\partial \\alpha_i} \\frac{d\\alpha_i}{d\\alpha_j} = (1 - y_j u_j) + (1 - y_i u_i)(-s)\n$$\nwhere $u_i$ and $u_j$ are functions of the new $\\alpha_i, \\alpha_j$. Let $\\alpha_k^{\\text{new}} = \\alpha_k^{\\text{old}} + \\Delta\\alpha_k$. Then $\\Delta\\alpha_i = -s \\Delta\\alpha_j$.\nThe new $u_k$ values are $u_k^{\\text{new}} = u_k^{\\text{old}} + \\Delta\\alpha_i y_i K_{ik} + \\Delta\\alpha_j y_j K_{jk}$.\n$u_i^{\\text{new}} = u_i^{\\text{old}} - s \\Delta\\alpha_j y_i K_{ii} + \\Delta\\alpha_j y_j K_{ij} = u_i^{\\text{old}} + \\Delta\\alpha_j y_j (K_{ij} - K_{ii})$.\n$u_j^{\\text{new}} = u_j^{\\text{old}} - s \\Delta\\alpha_j y_i K_{ji} + \\Delta\\alpha_j y_j K_{jj} = u_j^{\\text{old}} + \\Delta\\alpha_j y_j (K_{jj} - K_{ij})$.\nSetting $\\frac{dW}{d\\alpha_j} = 0$ at the old values for the linear part and new for the quadratic part:\n$0 = \\frac{dW}{d\\alpha_j}|_{\\alpha_j^{\\text{old}}} + \\frac{d^2 W}{d\\alpha_j^2}|_{\\alpha_j^{\\text{old}}} (\\alpha_j^{\\text{new,unc}} - \\alpha_j^{\\text{old}})$.\nThe derivative at the old values is $(1-y_j u_j^{\\text{old}}) - s(1-y_i u_i^{\\text{old}})$.\nUsing $E_k = u_k^{\\text{old}} + b^{\\text{old}} - y_k$, this is $y_j(b^{\\text{old}}-E_j^{\\text{old}}) - s y_i(b^{\\text{old}}-E_i^{\\text{old}}) = y_j(b-E_j) - y_j(b-E_i) = y_j(E_i-E_j)$.\nSo, $y_j(E_i-E_j) - \\eta(\\alpha_j^{\\text{new,unc}} - \\alpha_j^{\\text{old}}) = 0$.\nThis gives the unconstrained update for $\\alpha_j$:\n$$\n\\alpha_j^{\\text{new, unc}} = \\alpha_j^{\\text{old}} + \\frac{y_j(E_i - E_j)}{\\eta} = \\alpha_j^{\\text{old}} + \\frac{y_j(E_i - E_j)}{K_{ii} + K_{jj} - 2K_{ij}}\n$$\nThis value must be clipped to satisfy the box constraints $0 \\le \\alpha_i^{\\text{new}} \\le C$ and $0 \\le \\alpha_j^{\\text{new}} \\le C$. These constraints define a feasible interval $[L, H]$ for $\\alpha_j^{\\text{new}}$.\nIf $y_i \\ne y_j$ ($s=-1$), the constraint is $\\alpha_i - \\alpha_j = \\alpha_i^{\\text{old}} - \\alpha_j^{\\text{old}}$. The bounds are $L = \\max(0, \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}})$ and $H = \\min(C, C + \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}})$.\nIf $y_i = y_j$ ($s=1$), the constraint is $\\alpha_i + \\alpha_j = \\alpha_i^{\\text{old}} + \\alpha_j^{\\text{old}}$. The bounds are $L = \\max(0, \\alpha_i^{\\text{old}} + \\alpha_j^{\\text{old}} - C)$ and $H = \\min(C, \\alpha_i^{\\text{old}} + \\alpha_j^{\\text{old}})$.\nThe final updated value for $\\alpha_j$ is:\n$$\n\\alpha_j^{\\text{new}} = \\begin{cases} H & \\text{if } \\alpha_j^{\\text{new, unc}} > H \\\\ \\alpha_j^{\\text{new, unc}} & \\text{if } L \\le \\alpha_j^{\\text{new, unc}} \\le H \\\\ L & \\text{if } \\alpha_j^{\\text{new, unc}} < L \\end{cases}\n$$\nThe value for $\\alpha_i^{\\text{new}}$ is then found using the linear constraint: $\\alpha_i^{\\text{new}} = \\alpha_i^{\\text{old}} + s(\\alpha_j^{\\text{old}} - \\alpha_j^{\\text{new}})$.\n\n### Part 2: Derivation of the Objective Improvement $\\Delta W$\n\nThe change in the objective function, $\\Delta W = W(\\alpha^{\\text{new}}) - W(\\alpha^{\\text{old}})$, can be found by Taylor-expanding $W$ around $\\alpha^{\\text{old}}$ up to the second order. The change vector is $\\Delta\\vec{\\alpha}$ with non-zero components $\\Delta\\alpha_i$ and $\\Delta\\alpha_j$.\n$$\n\\Delta W \\approx \\nabla W(\\alpha^{\\text{old}})^T \\Delta\\vec{\\alpha} + \\frac{1}{2} \\Delta\\vec{\\alpha}^T \\mathbf{H} \\Delta\\vec{\\alpha}\n$$\nwhere $\\mathbf{H}$ is the Hessian matrix of $W$, with elements $H_{kl} = -y_k y_l K_{kl}$. Since $W$ is quadratic, this expansion is exact.\nThe gradient term is: $\\frac{\\partial W}{\\partial \\alpha_i} \\Delta\\alpha_i + \\frac{\\partial W}{\\partial \\alpha_j} \\Delta\\alpha_j$.\nWith $\\Delta\\alpha_i = -s \\Delta\\alpha_j$ and $\\frac{\\partial W}{\\partial \\alpha_k} = y_k(b-E_k)$ at the old values, this term is:\n$y_i(b-E_i)(-s\\Delta\\alpha_j) + y_j(b-E_j)\\Delta\\alpha_j = \\Delta\\alpha_j[-y_j(b-E_i) + y_j(b-E_j)] = \\Delta\\alpha_j y_j(E_i-E_j)$.\nThe Hessian term is $\\frac{1}{2}(\\Delta\\alpha_i^2 H_{ii} + \\Delta\\alpha_j^2 H_{jj} + 2\\Delta\\alpha_i\\Delta\\alpha_j H_{ij})$\n$= \\frac{1}{2}((-s\\Delta\\alpha_j)^2(-K_{ii}) + (\\Delta\\alpha_j)^2(-K_{jj}) + 2(-s\\Delta\\alpha_j)(\\Delta\\alpha_j)(-sK_{ij}))$\n$= \\frac{1}{2}(\\Delta\\alpha_j)^2 (-K_{ii} - K_{jj} + 2K_{ij}) = -\\frac{1}{2} \\eta (\\Delta\\alpha_j)^2$.\nThus, the objective improvement is:\n$$\n\\Delta W = \\Delta \\alpha_j y_j(E_i - E_j) - \\frac{1}{2} \\eta (\\Delta \\alpha_j)^2\n$$\nwhere $\\Delta\\alpha_j = \\alpha_j^{\\text{new}} - \\alpha_j^{\\text{old}}$ is the actual, clipped change in $\\alpha_j$.\n\n### Part 3: Numerical Evaluation\n\nGiven data:\n- $C = 1$.\n- $\\alpha_i^{\\text{old}} = 0.4$, $\\alpha_j^{\\text{old}} = 0.6$.\n- $y_i = +1$, $y_j = -1$.\n- $K_{ii} = 1$, $K_{jj} = 1$, $K_{ij} = 0.9999$.\n- $E_i = 0.3$, $E_j = -0.4$.\n\n1.  **Compute parameters**:\n    $s = y_i y_j = (+1)(-1) = -1$.\n    Since $y_i \\ne y_j$, we use the corresponding bounds for $L$ and $H$:\n    $L = \\max(0, \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}}) = \\max(0, 0.6 - 0.4) = \\max(0, 0.2) = 0.2$.\n    $H = \\min(C, C + \\alpha_j^{\\text{old}} - \\alpha_i^{\\text{old}}) = \\min(1, 1 + 0.6 - 0.4) = \\min(1, 1.2) = 1$.\n    The feasible interval for $\\alpha_j^{\\text{new}}$ is $[0.2, 1]$.\n    $\\eta = K_{ii} + K_{jj} - 2K_{ij} = 1 + 1 - 2(0.9999) = 2 - 1.9998 = 0.0002$.\n\n2.  **Compute unclipped update**:\n    $E_i - E_j = 0.3 - (-0.4) = 0.7$.\n    $\\alpha_j^{\\text{new, unc}} = \\alpha_j^{\\text{old}} + \\frac{y_j(E_i - E_j)}{\\eta} = 0.6 + \\frac{(-1)(0.7)}{0.0002} = 0.6 - \\frac{0.7}{0.0002} = 0.6 - 3500 = -3499.4$.\n\n3.  **Clip $\\alpha_j^{\\text{new}}$ and compute $\\alpha_i^{\\text{new}}$**:\n    Since $\\alpha_j^{\\text{new, unc}} = -3499.4 < L = 0.2$, we clip the value:\n    $\\alpha_j^{\\text{new}} = L = 0.2$.\n    Now we find the corresponding $\\alpha_i^{\\text{new}}$:\n    $\\alpha_i^{\\text{new}} = \\alpha_i^{\\text{old}} + s(\\alpha_j^{\\text{old}} - \\alpha_j^{\\text{new}}) = 0.4 + (-1)(0.6 - 0.2) = 0.4 - 0.4 = 0$.\n    Rounding to four significant figures, $\\alpha_i^{\\text{new}} = 0.0000$ and $\\alpha_j^{\\text{new}} = 0.2000$.\n\n4.  **Compute objective improvement $\\Delta W$**:\n    The actual change in $\\alpha_j$ is $\\Delta\\alpha_j = \\alpha_j^{\\text{new}} - \\alpha_j^{\\text{old}} = 0.2 - 0.6 = -0.4$.\n    Using the derived formula for $\\Delta W$:\n    $\\Delta W = \\Delta \\alpha_j y_j(E_i - E_j) - \\frac{1}{2} \\eta (\\Delta \\alpha_j)^2$\n    $\\Delta W = (-0.4) \\times (-1) \\times (0.7) - \\frac{1}{2} (0.0002) (-0.4)^2$\n    $\\Delta W = (0.4)(0.7) - (0.0001)(0.16)$\n    $\\Delta W = 0.28 - 0.000016 = 0.279984$.\n    Rounding to four significant figures, $\\Delta W = 0.2800$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.0000 & 0.2000 & 0.2800 \\end{pmatrix}}\n$$"
        }
    ]
}