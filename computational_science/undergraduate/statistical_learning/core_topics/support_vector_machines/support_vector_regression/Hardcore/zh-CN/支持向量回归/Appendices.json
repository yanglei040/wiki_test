{
    "hands_on_practices": [
        {
            "introduction": "要真正理解支持向量回归（SVR），最好的方法是亲自动手实践。在依赖复杂的软件包之前，手动求解一个简单的SVR问题是建立直观理解的宝贵一步。这个练习将带你回归第一性原理，通过一个仅有两个数据点的极简场景，揭示SVR的核心优化任务：即在保持模型“平坦”（最小化$w$）与将数据点拟合在$\\epsilon$-不敏感管道内之间取得平衡。通过这个过程，你将对SVR的原始目标函数和约束条件有更深刻的认识。",
            "id": "3178709",
            "problem": "考虑一个包含两个数据点的一维训练集：$(x_1, y_1) = (0, 0)$ 和 $(x_2, y_2) = (1, 2)$。拟合一个线性支持向量回归 (SVR) 模型，其预测器为 $f(x) = w x + b$，使用 $\\epsilon$-不敏感损失函数，其中 $\\epsilon = 0.5$，正则化参数 $C = 1$。从 SVR 的基本原始定义出发：在满足 $\\epsilon$-管道约束的条件下，最小化权重正则化项与 $\\epsilon$-不敏感松弛惩罚项之和，不要使用任何预先推导的快捷公式。将该优化问题明确地设置为一个二次规划 (QP) 问题，并根据第一性原理进行推理以求得精确解。确定 $w$ 和 $b$ 的精确最优值，并指出哪些训练点是支持向量，根据 Karush–Kuhn–Tucker (KKT) 互补松弛条件解释原因。将你最终拟合的参数 $(w,b)$ 以行矩阵的形式报告。无需四舍五入。",
            "solution": "题目要求我们为一个包含两个点 $(x_1, y_1) = (0, 0)$ 和 $(x_2, y_2) = (1, 2)$ 的训练集拟合一个线性支持向量回归 (SVR) 模型 $f(x) = w x + b$。给定的 SVR 参数为不敏感参数 $\\epsilon = 0.5$ 和正则化参数 $C=1$。我们需要从 SVR 的基本原始定义出发解决这个问题。\n\nSVR 的原始优化问题旨在最小化模型复杂度（正则化项）和训练误差（损失函数）的组合。模型复杂度由 $\\frac{1}{2} \\|w\\|^2$ 度量，对于一维输入，即为 $\\frac{1}{2} w^2$。误差由 $\\epsilon$-不敏感损失度量，该损失只惩罚大于 $\\epsilon$ 的残差。这是通过为每个数据点 $i$ 引入非负的松弛变量 $\\xi_i$ 和 $\\xi_i^*$ 来实现的。需要最小化的目标函数是：\n$$\n\\min_{w, b, \\xi, \\xi^*} \\frac{1}{2} w^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n$$\n该最小化问题受到一组定义了数据点周围的 $\\epsilon$-管道的约束条件的限制：\n$$\n\\begin{cases}\ny_i - (w x_i + b) \\le \\epsilon + \\xi_i,  \\text{for } i=1, \\dots, n \\\\\n(w x_i + b) - y_i \\le \\epsilon + \\xi_i^*,  \\text{for } i=1, \\dots, n \\\\\n\\xi_i \\ge 0, \\xi_i^* \\ge 0,  \\text{for } i=1, \\dots, n\n\\end{cases}\n$$\n这里，数据点的数量 $n=2$。我们代入给定的值：\n- 数据点 1: $(x_1, y_1) = (0, 0)$\n- 数据点 2: $(x_2, y_2) = (1, 2)$\n- 参数: $\\epsilon = 0.5$, $C = 1$\n\n目标函数变为：\n$$\n\\min_{w, b, \\xi_1, \\xi_1^*, \\xi_2, \\xi_2^*} \\frac{1}{2} w^2 + 1 \\cdot (\\xi_1 + \\xi_1^* + \\xi_2 + \\xi_2^*)\n$$\n对于点 1 $((x_1, y_1) = (0, 0))$ 的约束条件是：\n$$\n0 - (w \\cdot 0 + b) \\le 0.5 + \\xi_1 \\quad \\implies \\quad -b \\le 0.5 + \\xi_1\n$$\n$$\n(w \\cdot 0 + b) - 0 \\le 0.5 + \\xi_1^* \\quad \\implies \\quad b \\le 0.5 + \\xi_1^*\n$$\n对于点 2 $((x_2, y_2) = (1, 2))$ 的约束条件是：\n$$\n2 - (w \\cdot 1 + b) \\le 0.5 + \\xi_2 \\quad \\implies \\quad 1.5 \\le w + b + \\xi_2\n$$\n$$\n(w \\cdot 1 + b) - 2 \\le 0.5 + \\xi_2^* \\quad \\implies \\quad w + b \\le 2.5 + \\xi_2^*\n$$\n以及松弛变量的非负约束：\n$$\n\\xi_1, \\xi_1^*, \\xi_2, \\xi_2^* \\ge 0\n$$\n由于松弛变量在目标函数中受到惩罚，我们希望使它们尽可能小。这意味着我们应该首先研究所有松弛变量都为零的情况，即 $\\xi_1 = \\xi_1^* = \\xi_2 = \\xi_2^* = 0$。这对应于找到一个对所有数据点都完全位于 $\\epsilon$-管道内的函数 $f(x)$。约束条件简化为：\n$$\n-b \\le 0.5 \\quad \\implies \\quad b \\ge -0.5\n$$\n$$\nb \\le 0.5\n$$\n$$\n1.5 \\le w + b\n$$\n$$\nw + b \\le 2.5\n$$\n这些不等式定义了 $(w, b)$ 的一个可行域。在此区域内，目标函数简化为 $\\frac{1}{2} w^2$。为了找到解，我们必须在这个可行域中找到使 $\\frac{1}{2} w^2$ 最小化的点 $(w, b)$，这等价于最小化 $|w|$。\n\n可行域由 $-0.5 \\le b \\le 0.5$ 和 $1.5 \\le w+b \\le 2.5$ 定义。为了最小化 $|w|$，我们考察约束 $w+b \\ge 1.5$，它可以写成 $w \\ge 1.5 - b$。为了使 $w$ 尽可能小（同时假设 $w \\ge 0$，考虑到直线从 $(0,0)$ 经过 $(1,2)$，这个假设是合理的），我们必须使 $b$ 尽可能大。在可行域中 $b$ 的最大值是 $b=0.5$。将这个值代入关于 $w$ 的不等式，得到 $w \\ge 1.5 - 0.5 = 1$。因此，$w$ 的最小可能值为 $1$。\n\n我们来检查点 $(w, b) = (1, 0.5)$ 是否在可行域内。\n- $b=0.5$: 条件 $-0.5 \\le 0.5 \\le 0.5$ 得到满足。\n- $w+b = 1+0.5=1.5$: 条件 $1.5 \\le 1.5 \\le 2.5$ 得到满足。\n所以，点 $(w, b) = (1, 0.5)$ 在可行域内。在这一点上，目标函数的值是 $\\frac{1}{2} w^2 = \\frac{1}{2} (1)^2 = 0.5$。\n\n现在，我们必须考虑具有非零松弛变量的解是否能产生更低的目标函数值。任何非零的松弛都会给目标函数增加一个正的惩罚。只有当允许一些惩罚所带来的 $\\frac{1}{2}w^2$ 项的减少量大于所产生的惩罚时，才可能得到更好的解。\n让我们考虑对当前解进行一个小的偏离，例如，将 $w$ 减小到 $w' = 1-\\delta$，其中 $\\delta  0$ 是一个小数。为了最小化松弛惩罚，我们可以保持 $b=0.5$。那么 $w'+b' = 1.5-\\delta$。约束 $w+b \\ge 1.5$ 被违反了。为了满足它，我们需要 $\\xi_2 \\ge \\delta$。最小的惩罚是 $\\xi_2=\\delta$。新的目标函数值将是 $\\frac{1}{2}(1-\\delta)^2 + \\delta = \\frac{1}{2}(1-2\\delta+\\delta^2) + \\delta = 0.5 - \\delta + 0.5\\delta^2 + \\delta = 0.5 + 0.5\\delta^2$。这个值大于 $0.5$。这个推理表明，任何引入松弛惩罚的偏离都会增加目标函数值。因此，最优解确实是 $(w, b) = (1, 0.5)$，且所有松弛变量均为零。\n\n为了正式确认此解并识别支持向量，我们使用 Karush–Kuhn–Tucker (KKT) 条件。如果一个数据点 $i$ 对应的拉格朗日乘子 $\\alpha_i$ 或 $\\alpha_i^*$ 非零，那么它就是一个支持向量。KKT 互补松弛条件要求，对于一个有效约束，其乘子可以非零；而对于一个非零乘子，其约束必须是有效的。\n\n我们来检验我们的解 $(w,b)=(1,0.5)$ 和 $f(x)=x+0.5$：\n- 对于点 1 $(x_1, y_1) = (0, 0)$:\n  - 预测值为 $f(0) = 1(0) + 0.5 = 0.5$。\n  - 残差为 $y_1 - f(0) = 0 - 0.5 = -0.5$。\n  - 绝对残差 $|-0.5| = 0.5$，恰好等于 $\\epsilon$。\n  - 被激活的具体约束是 $(w x_1 + b) - y_1 = \\epsilon$。由于该约束被激活，其相关的拉格朗日乘子 $\\alpha_1^*$ 可以非零。因此，点 1 是一个支持向量。\n\n- 对于点 2 $(x_2, y_2) = (1, 2)$:\n  - 预测值为 $f(1) = 1(1) + 0.5 = 1.5$。\n  - 残差为 $y_2 - f(1) = 2 - 1.5 = 0.5$。\n  - 绝对残差 $|0.5| = 0.5$，也恰好等于 $\\epsilon$。\n  - 被激活的具体约束是 $y_2 - (w x_2 + b) = \\epsilon$。由于该约束被激活，其相关的拉格朗日乘子 $\\alpha_2$ 可以非零。因此，点 2 也是一个支持向量。\n\n两个数据点都恰好位于 $\\epsilon$-管道的边界上。KKT 的平稳性条件是 $\\sum(\\alpha_i - \\alpha_i^*) = 0$ 和 $w = \\sum(\\alpha_i - \\alpha_i^*)x_i$。对于我们的解，$\\alpha_1=0$ 和 $\\alpha_2^*=0$，因为每个点只触及管道边界的一侧。平稳性条件变为 $\\alpha_2 - \\alpha_1^* = 0$ 和 $w = \\alpha_2 x_2 = \\alpha_2$。由于 $w=1$，我们得到 $\\alpha_2=1$。这意味着 $\\alpha_1^*=1$。由于 $C=1$，两个乘子都达到了它们的上界，这与点位于边界上（或边界外，但此处并非如此，因为所有 $\\xi_i, \\xi_i^*$ 都为零）的情况是一致的。所有 KKT 条件都得到满足，确认了 $(w,b)=(1,0.5)$ 是精确的最优解。\n\n最优参数是 $w=1$ 和 $b=0.5$。两个训练点 $(0,0)$ 和 $(1,2)$ 都是支持向量，因为它们位于 $\\epsilon$-管道的边界上，这意味着它们的残差的绝对值恰好等于 $\\epsilon$。这一点通过它们在对偶问题中的非零拉格朗日乘子（$\\alpha_1^*=1$ 和 $\\alpha_2=1$）得到证实，根据 KKT 理论，这是成为支持向量的必要条件。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  0.5 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "从手动计算过渡到编程实现，我们现在来探讨SVR超参数的实际影响。像$\\epsilon$这样的参数并非抽象的理论值，它们对模型的最终表现有着深远的影响。这个练习将通过一个具体的编程任务，向你展示一个被错误设定的$\\epsilon$值如何导致模型学习到不佳的斜率，从而在外推（extrapolation）任务上表现糟糕。你将看到，通过精心选择$\\epsilon$，SVR能够更准确地捕捉数据的内在趋势，这对于构建稳健的预测模型至关重要。",
            "id": "3178721",
            "problem": "要求您通过显式构造和计算来演示，当 epsilon 不敏感管参数被错误指定时，使用线性模型的支持向量回归（SVR）会如何导致外推效果不佳，以及重新调整 epsilon 不敏感参数如何改善学习到的斜率和外推误差。请在线性支持向量回归（SVR）的设定下进行操作，其中预测器形式为 $f(x) = w x + b$，且经验风险基于 epsilon 不敏感损失。使用一维数据集，并通过一个符合原理的凸优化程序来求解原始（primal）公式。\n\n使用的基本原理：\n- 经验风险最小化（ERM）原则：选择参数 $(w, b)$ 以在训练集 $\\{(x_i, y_i)\\}_{i=1}^n$ 上最小化一个正则化的经验损失。\n- 支持向量回归（SVR）中使用的 epsilon 不敏感损失：对于残差 $r_i = y_i - f(x_i)$，损失为 $L_{\\epsilon}(r_i) = \\max(0, |r_i| - \\epsilon)$。\n- 原始形式下的凸正则化目标：最小化正则化项与经验 epsilon 不敏感损失之和。\n\n您的任务：\n1. 使用下面指定的包含 $n = 13$ 个点的固定训练数据集。输入位置为：\n   $x = [-1.5, -1.3, -1.0, -0.7, -0.4, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0, 1.3, 1.5]$。\n   相应的输出由一个真实的线性函数加上小的确定性噪声生成：\n   对于相同的 $x_i$，$y_i = 2 x_i + 1 + \\nu_i$，噪声值为固定的\n   $\\nu = [0.02, -0.03, 0.01, 0.05, -0.04, 0.03, 0.0, -0.02, 0.05, -0.01, 0.02, -0.04, 0.03]$，\n   因此 $y$ 是完全确定的。\n\n2. 将要最小化的 SVR 原始目标定义为\n   $$J(w, b) = \\frac{1}{2}\\lambda w^2 + C \\sum_{i=1}^{n} \\max\\big(0, |y_i - (w x_i + b)| - \\epsilon\\big),$$\n   其中 $\\lambda  0$ 是斜率 $w$ 的正则化权重，$C  0$ 是经验损失权重，$\\epsilon \\ge 0$ 是 epsilon 不敏感管的半径。您必须在目标中将 $b$ 视为无正则化的（即不对 $b$ 进行惩罚）。\n\n3. 实现一个正确的算法来最小化关于 $(w, b)$ 的 $J(w, b)$。您必须使用一个从上述基本原理出发的、符合原则的凸优化方法。一个有效的方法是使用带有递减步长的次梯度法来处理 epsilon 不敏感损失的不可微性。您的实现必须：\n   - 将 $w$ 和 $b$ 初始化为 $0$。\n   - 基于 $J(w, b)$ 关于 $w$ 和 $b$ 的次梯度进行迭代更新。\n   - 对于 $t = 0, 1, \\dots, T-1$，使用形式为 $\\eta_t = \\eta_0 / \\sqrt{t + 1}$ 的递减步长方案，其中 $\\eta_0  0$ 是固定的，迭代次数 $T$ 足够大以确保在给定数据集上收敛。\n   - 使用固定的超参数 $\\lambda = 1.0$，$C = 10.0$，$\\eta_0 = 0.01$ 和 $T = 10000$。\n\n4. 通过在训练域外的单个测试点 $x_{\\text{out}} = 3.0$ 上评估训练好的线性 SVR，并将其与真实的潜在线性函数 $f^{\\star}(x) = 2x + 1$ 进行比较，来构建一个外推的反例。将外推误差定义为绝对差值：\n   $$E(\\epsilon) = \\big| \\, (w(\\epsilon) \\cdot x_{\\text{out}} + b(\\epsilon)) - (2 \\cdot x_{\\text{out}} + 1) \\, \\big|.$$\n\n5. 通过为以下 epsilon 值的测试集计算 $(w(\\epsilon), E(\\epsilon))$，展示重新调整 $\\epsilon$ 如何改变学习到的斜率 $w(\\epsilon)$ 并减少外推误差 $E(\\epsilon)$：\n   - 一个错误指定的大 epsilon: $\\epsilon = 3.5$（旨在通过使模型趋向于 $w \\approx 0$ 来产生差的外推效果）。\n   - 一个精心选择的小 epsilon: $\\epsilon = 0.1$（旨在恢复接近真实值 $w^{\\star} = 2$ 的斜率并产生小的外推误差）。\n   - 一个边界情况: $\\epsilon = 0.0$（绝对偏差损失，测试鲁棒性）。\n\n6. 您的程序必须生成单行输出，其中包含六个结果，按上面列出的 epsilon 值的顺序排列。该行必须是方括号内的一个逗号分隔列表，顺序完全如下，每个值四舍五入到 $6$ 位小数：\n   $$[w(3.5), E(3.5), w(0.1), E(0.1), w(0.0), E(0.0)].$$\n\n注意：\n- 完全在纯数学术语下工作；不涉及物理单位。\n- 必须如上所述明确定义和使用支持向量回归（SVR）。\n- 除了基本原理和给定的目标之外，您不得使用任何快捷公式；相反，应从第一性原理推导并实现优化更新。\n- 通过遵循给定的数据集和优化设置，确保科学真实性和内部一致性。",
            "solution": "该问题要求演示当 epsilon 不敏感管参数 $\\epsilon$ 被错误指定时，支持向量回归（SVR）会如何导致外推效果不佳，以及当选择一个更合适的 $\\epsilon$ 时性能如何改善。这将通过使用次梯度下降法为给定数据集求解 SVR 原始目标来展示。\n\n线性 SVR 模型由 $f(x) = w x + b$ 给出，其中由于输入 $x$ 是一维的，所以 $w$ 和 $b$ 是标量参数。\n\n目标是找到参数 $(w, b)$，以最小化正则化的经验风险函数 $J(w, b)$：\n$$\nJ(w, b) = \\frac{1}{2}\\lambda w^2 + C \\sum_{i=1}^{n} \\max\\big(0, |y_i - (w x_i + b)| - \\epsilon\\big)\n$$\n这里，$\\lambda  0$ 是正则化参数，$C  0$ 是损失的惩罚参数，$\\epsilon \\ge 0$ 定义了 epsilon 不敏感管的半径。项 $\\frac{1}{2}\\lambda w^2$ 是一个 Tikhonov 正则化项，它惩罚大的斜率值以防止过拟合，求和项是训练集 $\\{(x_i, y_i)\\}_{i=1}^n$ 上的总经验损失。\n\n目标函数 $J(w,b)$ 是凸函数，因为它是两个凸函数之和：正则化项 $\\frac{1}{2}\\lambda w^2$（它在 $w$ 上是严格凸的）和经验损失项。损失函数 $L_{\\epsilon}(r) = \\max(0, |r| - \\epsilon)$ 是残差 $r$ 的一个凸函数，并且由于残差 $r_i = y_i - (w x_i + b)$ 是 $(w, b)$ 的一个仿射函数，所以复合函数 $L_{\\epsilon}(r_i(w, b))$ 在 $(w, b)$ 上也是凸的。\n\n由于存在绝对值和 $\\max$ 算子，目标函数在 $|y_i - (w x_i + b)| = \\epsilon$ 或 $y_i - (w x_i + b) = 0$ 的点上是不可微的。因此，我们将使用次梯度法，这是梯度下降法对不可微凸函数的扩展。\n\n在第 $t$ 次迭代时，次梯度下降的更新规则是：\n$$\nw_{t+1} = w_t - \\eta_t g_{w,t}\n$$\n$$\nb_{t+1} = b_t - \\eta_t g_{b,t}\n$$\n其中 $\\eta_t$ 是步长，$(g_{w,t}, g_{b,t})$ 是 $J$ 在 $(w_t, b_t)$ 处的一个次梯度。\n\n函数之和的次梯度是它们各自次梯度的和。因此，我们可以通过对正则化项的梯度和损失项的次梯度求和来找到 $J(w, b)$ 的次梯度。\n\n1.  **正则化项的梯度：**\n    正则化项 $J_{reg}(w) = \\frac{1}{2}\\lambda w^2$ 关于 $w$ 是可微的。\n    $$\n    \\frac{\\partial J_{reg}}{\\partial w} = \\lambda w\n    $$\n    该项不依赖于 $b$，因此其关于 $b$ 的导数为 $0$。\n\n2.  **损失项的次梯度：**\n    损失项是 $J_{loss}(w, b) = C \\sum_{i=1}^{n} L_{\\epsilon}(r_i)$，其中 $r_i = y_i - (w x_i + b)$。\n    让我们找到 $L_{\\epsilon}(r_i) = \\max(0, |r_i| - \\epsilon)$ 关于 $w$ 和 $b$ 的次梯度。我们使用次微分的链式法则。$L_{\\epsilon}(r_i)$ 关于 $w$ 的次梯度可以计算为 $(\\partial_{r_i} L_{\\epsilon}) \\cdot (\\frac{\\partial r_i}{\\partial w})$。\n\n    首先，让我们找到 $L_{\\epsilon}$ 关于残差 $r_i$ 的次梯度。令\n    $\\partial_{r_i} L_{\\epsilon}$ 表示次微分中的一个元素。\n    -   如果 $|r_i|  \\epsilon$，则 $|r_i| - \\epsilon  0$，所以 $L_{\\epsilon}(r_i) = 0$。导数为 $0$。\n    -   如果 $r_i  \\epsilon$，则 $|r_i| = r_i$，所以 $L_{\\epsilon}(r_i) = r_i - \\epsilon$。导数为 $1$。\n    -   如果 $r_i  -\\epsilon$，则 $|r_i| = -r_i$，所以 $L_{\\epsilon}(r_i) = -r_i - \\epsilon$。导数为 $-1$。\n    -   在不可微点 $|r_i| = \\epsilon$，我们可以从次微分区间中选择任何值。为了算法实现，我们可以选择 $0$。\n\n    $L_{\\epsilon}(r_i)$ 关于 $r_i$ 的一个有效次梯度选择，记为 $g_{r_i}$，是：\n    $$\n    g_{r_i} \\in \\partial_{r_i} L_{\\epsilon}(r_i) \\quad \\text{其中} \\quad g_{r_i} = \\begin{cases}\n    1  \\text{若 } r_i  \\epsilon \\\\\n    -1  \\text{若 } r_i  -\\epsilon \\\\\n    0  \\text{若 } |r_i| \\le \\epsilon\n    \\end{cases}\n    $$\n    残差 $r_i$ 关于 $w$ 和 $b$ 的偏导数是：\n    $$\n    \\frac{\\partial r_i}{\\partial w} = -x_i \\quad \\text{和} \\quad \\frac{\\partial r_i}{\\partial b} = -1\n    $$\n    使用链式法则，总损失项关于 $w$ 和 $b$ 的一个次梯度是：\n    $$\n    g_{J_{loss}, w} = C \\sum_{i=1}^n (g_{r_i} \\cdot \\frac{\\partial r_i}{\\partial w}) = C \\sum_{i=1}^n g_{r_i} (-x_i) = -C \\sum_{i=1}^n g_{r_i} x_i\n    $$\n    $$\n    g_{J_{loss}, b} = C \\sum_{i=1}^n (g_{r_i} \\cdot \\frac{\\partial r_i}{\\partial b}) = C \\sum_{i=1}^n g_{r_i} (-1) = -C \\sum_{i=1}^n g_{r_i}\n    $$\n\n3.  **J(w, b) 的完整次梯度：**\n    结合各个部分，完整目标函数 $J(w, b)$ 的一个次梯度 $(g_w, g_b)$ 是：\n    $$\n    g_w = \\lambda w - C \\sum_{i=1}^n g_{r_i} x_i\n    $$\n    $$\n    g_b = - C \\sum_{i=1}^n g_{r_i}\n    $$\n    其中 $g_{r_i}$ 是根据残差 $r_i = y_i - (w x_i + b)$ 和上面定义的 $\\epsilon$ 确定的。\n\n该算法通过初始化 $w_0=0, b_0=0$ 并迭代应用更新规则 $T$ 步来进行，使用递减步长方案 $\\eta_t = \\eta_0 / \\sqrt{t + 1}$。我们对每个指定的 $\\epsilon \\in \\{3.5, 0.1, 0.0\\}$ 值执行此过程。对于每个得到的模型 $(w(\\epsilon), b(\\epsilon))$，我们计算在 $x_{\\text{out}}=3.0$ 处的外推误差 $E(\\epsilon) = |(w(\\epsilon) x_{\\text{out}} + b(\\epsilon)) - (2 x_{\\text{out}} + 1)|$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the SVR problem using subgradient descent for different epsilon values\n    and computes the resulting learned slope and extrapolation error.\n    \"\"\"\n    # Step 1: Define the fixed training dataset and problem parameters.\n    x_train = np.array([-1.5, -1.3, -1.0, -0.7, -0.4, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0, 1.3, 1.5])\n    nu = np.array([0.02, -0.03, 0.01, 0.05, -0.04, 0.03, 0.0, -0.02, 0.05, -0.01, 0.02, -0.04, 0.03])\n    # The output values are generated by a true linear function plus noise.\n    y_train = 2 * x_train + 1 + nu\n\n    # Step 2: Define the SVR and optimization hyperparameters.\n    lambda_reg = 1.0\n    C = 10.0\n    eta0 = 0.01\n    T = 10000\n\n    # Step 3: Define the test case for extrapolation.\n    x_out = 3.0\n    # The true value at the extrapolation point.\n    y_true_out = 2 * x_out + 1\n\n    # Epsilon values to be tested, as specified in the problem.\n    epsilon_values = [3.5, 0.1, 0.0]\n\n    results = []\n\n    # Step 4: Iterate through each epsilon, train the SVR model, and evaluate it.\n    for epsilon in epsilon_values:\n        # Train the SVR model using subgradient descent.\n        w, b = train_svr(x_train, y_train, lambda_reg, C, epsilon, eta0, T)\n\n        # Calculate the model's prediction at the extrapolation point.\n        y_pred_out = w * x_out + b\n\n        # Calculate the extrapolation error.\n        extrapolation_error = abs(y_pred_out - y_true_out)\n\n        # Store the results with the required precision.\n        results.append(f\"{w:.6f}\")\n        results.append(f\"{extrapolation_error:.6f}\")\n\n    # Step 5: Print the final output in the specified format.\n    print(f\"[{','.join(results)}]\")\n\n\ndef train_svr(x, y, lambda_reg, C, epsilon, eta0, T):\n    \"\"\"\n    Implements subgradient descent to find the optimal w and b for SVR.\n\n    Args:\n        x (np.ndarray): Input feature vector.\n        y (np.ndarray): Target value vector.\n        lambda_reg (float): Regularization parameter.\n        C (float): Loss penalty parameter.\n        epsilon (float): Epsilon-insensitive tube radius.\n        eta0 (float): Initial learning rate for the step size schedule.\n        T (int): Number of iterations.\n\n    Returns:\n        tuple[float, float]: The learned parameters (w, b).\n    \"\"\"\n    # Initialize parameters.\n    w = 0.0\n    b = 0.0\n    n = len(x)\n\n    # Perform T iterations of subgradient descent.\n    for t in range(T):\n        # Calculate the diminishing step size for the current iteration.\n        eta_t = eta0 / np.sqrt(t + 1)\n\n        # Calculate the residuals for all data points.\n        residuals = y - (w * x + b)\n\n        # Determine the coefficients for the loss subgradient based on the residuals.\n        # This implements the subgradient g_ri from the theoretical derivation.\n        g_loss_coeffs = np.zeros(n)\n        g_loss_coeffs[residuals  epsilon] = 1.0\n        g_loss_coeffs[residuals  -epsilon] = -1.0\n\n        # Calculate the full subgradient of the objective function J(w, b).\n        # g_w = (gradient of regularization) + (subgradient of loss wrt w)\n        g_w = lambda_reg * w - C * np.sum(g_loss_coeffs * x)\n        # g_b = (subgradient of loss wrt b)\n        g_b = - C * np.sum(g_loss_coeffs)\n\n        # Update the parameters using the subgradient descent rule.\n        w = w - eta_t * g_w\n        b = b - eta_t * g_b\n\n    return w, b\n\nsolve()\n```"
        },
        {
            "introduction": "在SVR模型中，虽然权重$w$通常是关注的焦点，但偏置项（或截距）$b$对于获得精确预测同样至关重要。这个更深入的练习将引导你探索$b$是如何通过对偶规划和KKT（Karush–Kuhn–Tucker）条件来确定的。特别地，它将揭示当所有支持向量都落在$\\epsilon$-管道的同一侧时，模型可能会出现系统性偏差。通过完成这个练习，你将对SVR解的微妙之处有更精细的理解，并认识到支持向量的分布如何影响最终模型的偏置。",
            "id": "3178771",
            "problem": "考虑支持向量回归问题，其使用线性函数类，其中预测函数为 $f(x) = w^\\top x + b$。学习问题使用 $\\varepsilon$-不敏感损失和对 $w$ 的 $\\ell_2$ 正则化。训练数据由数据对 $(x_i, y_i)$ 组成，其中 $i = 1, \\dots, n$，$x_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\mathbb{R}$。正则化参数为 $C  0$，管道宽度为 $\\varepsilon  0$。基本基础是使用带有凸约束的 $\\varepsilon$-不敏感损失进行经验风险最小化，以及用于带线性约束的凸优化的拉格朗日对偶和 Karush–Kuhn–Tucker (KKT) 条件。目标是检验偏置项 $b$ 是如何从 KKT 条件中计算出来的，以及当所有支持向量都位于 $\\varepsilon$-管道的同一侧时，它会受到怎样的影响。\n\n从使用 $\\varepsilon$-不敏感损失的凸优化表述出发，推导线性核情况下的对偶优化问题，并描述如何从 KKT 条件计算偏置项 $b$。实现一个求解器，该求解器：\n- 使用数值优化器解决小规模实例的线性支持向量回归的对偶优化问题，\n- 计算 $w$，然后仅使用非边界支持向量（即满足 $0  \\alpha_i  C$ 或 $0  \\alpha_i^\\star  C$ 的向量）根据 KKT 关系计算 $b$；当管道只有一侧有非边界支持向量时，使用相应的一侧来计算 $b$ 并解释潜在的偏差，\n- 如果根本没有非边界支持向量（一种边界情况），则回退到基于数据和学习到的 $w$ 计算 $b$ 的一致估计量。\n\n最后，通过报告每个测试案例的学习到的 $b$ 和平均残差 $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))$，说明残差偏度对计算出的 $b$ 的影响。\n\n您的程序必须为以下三个具有 $d = 1$ 和 $n = 10$ 的确定性测试案例实现上述逻辑：\n\n- 测试案例 1（平衡的残差；支持向量在两侧）：\n  - $x_i$ 值：$[-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]$。\n  - 用于合成 $y_i$ 的真实线性模型：$y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$，其中 $w_{\\text{true}} = 1.5$，$b_{\\text{true}} = 0.2$，以及 $\\text{noise}_i$ 值为 $[0.12, -0.12, 0.08, -0.08, 0.15, -0.15, 0.05, -0.05, 0.11, -0.11]$。\n  - 使用 $C = 1.0$ 和 $\\varepsilon = 0.1$。\n\n- 测试案例 2（单侧残差；所有支持向量均在上方）：\n  - $x_i$ 值：与测试案例 1 相同。\n  - 真实线性模型：$y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$，其中 $w_{\\text{true}} = 1.0$，$b_{\\text{true}} = 0.0$，以及严格非负的 $\\text{noise}_i$ 值为 $[0.2, 0.15, 0.12, 0.18, 0.25, 0.05, 0.14, 0.11, 0.16, 0.22]$。\n  - 使用 $C = 1.0$ 和 $\\varepsilon = 0.1$。\n\n- 测试案例 3（$C$ 值非常小的边界情况；可能没有非边界支持向量）：\n  - $x_i$ 值：与测试案例 1 相同。\n  - 真实线性模型：$y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$，其中 $w_{\\text{true}} = 1.2$，$b_{\\text{true}} = -0.1$，以及 $\\text{noise}_i$ 值为 $[0.09, -0.09, 0.08, -0.08, 0.07, -0.07, 0.06, -0.06, 0.05, -0.05]$。\n  - 使用 $C = 0.05$ 和 $\\varepsilon = 0.1$。\n\n对于每个测试案例，计算并返回：\n- 学习到的偏置项 $b$，作为一个浮点数，\n- 平均残差 $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))$，作为一个浮点数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[b_1,m_1,b_2,m_2,b_3,m_3]$），其中 $b_k$ 是测试案例 $k$ 的偏置项，$m_k$ 是相应的平均残差。不涉及物理单位，所有报告的值必须是实数。",
            "solution": "支持向量回归（SVR）问题旨在寻找一个函数 $f(x)$，该函数在尽可能“平坦”的同时，逼近一组训练数据点 $\\{(x_i, y_i)\\}_{i=1}^n$。对于线性情况，函数为 $f(x) = w^\\top x + b$，其中 $x_i \\in \\mathbb{R}^d$，$y_i \\in \\mathbb{R}$，$w \\in \\mathbb{R}^d$，$b \\in \\mathbb{R}$。\n\n该问题被表述为一个凸优化问题，使用 $\\varepsilon$-不敏感损失函数，该函数不惩罚与真实值距离在某个 $\\varepsilon  0$ 以内的误差。超出此“管道”的偏差会受到线性惩罚，由正则化参数 $C  0$ 控制。函数的平坦度由权重向量的 $\\ell_2$-范数 $\\|w\\|^2$ 来衡量。\n\n**原始表述**\n\n为了处理误差，我们为每个数据点 $i$ 引入非负松弛变量 $\\xi_i$ 和 $\\xi_i^\\star$。原始问题是最小化正则化风险：\n$$\n\\min_{w, b, \\xi, \\xi^\\star} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^\\star)\n$$\n受以下约束：\n$$\n\\begin{align*}\ny_i - (w^\\top x_i + b) \\le \\varepsilon + \\xi_i, \\quad i=1, \\dots, n \\\\\n(w^\\top x_i + b) - y_i \\le \\varepsilon + \\xi_i^\\star, \\quad i=1, \\dots, n \\\\\n\\xi_i, \\xi_i^\\star \\ge 0, \\quad i=1, \\dots, n\n\\end{align*}\n$$\n前两个约束确保预测值 $f(x_i) = w^\\top x_i + b$ 在 $y_i$ 周围的 $\\varepsilon$-管道内，偏差不超过松弛变量。\n\n**拉格朗日函数与对偶表述**\n\n为了解决这个约束优化问题，我们通过引入非负拉格朗日乘子 $\\alpha_i, \\alpha_i^\\star$（用于主要约束）和 $\\mu_i, \\mu_i^\\star$（用于松弛变量的非负性约束）来构造拉格朗日函数：\n$$\nL = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^\\star) - \\sum_{i=1}^n \\alpha_i (\\varepsilon + \\xi_i - y_i + w^\\top x_i + b) - \\sum_{i=1}^n \\alpha_i^\\star (\\varepsilon + \\xi_i^\\star + y_i - w^\\top x_i - b) - \\sum_{i=1}^n \\mu_i \\xi_i - \\sum_{i=1}^n \\mu_i^\\star \\xi_i^\\star\n$$\n将 $L$ 对原始变量（$w, b, \\xi_i, \\xi_i^\\star$）的偏导数设为零，得到最优性的 Karush-Kuhn-Tucker (KKT) 条件：\n$$\n\\frac{\\partial L}{\\partial w} = w - \\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) x_i = 0 \\implies w = \\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) x_i\n$$\n$$\n\\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) = 0 \\implies \\sum_{i=1}^n \\alpha_i = \\sum_{i=1}^n \\alpha_i^\\star\n$$\n$$\n\\frac{\\partial L}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0\n$$\n$$\n\\frac{\\partial L}{\\partial \\xi_i^\\star} = C - \\alpha_i^\\star - \\mu_i^\\star = 0\n$$\n将这些代回拉格朗日函数，得到 Wolfe 对偶优化问题。目标是最大化对偶目标函数，这等价于最小化其负数：\n$$\n\\min_{\\alpha, \\alpha^\\star} \\quad \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha_i - \\alpha_i^\\star)(\\alpha_j - \\alpha_j^\\star) (x_i^\\top x_j) - \\sum_{i=1}^n y_i(\\alpha_i - \\alpha_i^\\star) + \\varepsilon \\sum_{i=1}^n (\\alpha_i + \\alpha_i^\\star)\n$$\n受以下从 KKT 条件推导出的约束：\n$$\n\\begin{align*}\n\\sum_{i=1}^n (\\alpha_i - \\alpha_i^\\star) = 0 \\\\\n0 \\le \\alpha_i \\le C, \\quad i=1, \\dots, n \\\\\n0 \\le \\alpha_i^\\star \\le C, \\quad i=1, \\dots, n\n\\end{align*}\n$$\n这是一个关于 $2n$ 个对偶变量 $\\alpha_i$ 和 $\\alpha_i^\\star$ 的二次规划（QP）问题，可以使用数值优化器求解。\n\n**偏置项 $b$ 的计算**\n\n一旦找到最优对偶变量 $\\alpha^\\ast, \\alpha^{\\star\\ast}$，权重向量 $w$ 计算为 $w = \\sum_{i=1}^n (\\alpha_i^\\ast - \\alpha_i^{\\star\\ast}) x_i$。偏置项 $b$ 使用 KKT 互补松弛条件确定：\n$$\n\\begin{align*}\n\\alpha_i (\\varepsilon + \\xi_i - y_i + w^\\top x_i + b) = 0 \\\\\n\\alpha_i^\\star (\\varepsilon + \\xi_i^\\star + y_i - w^\\top x_i - b) = 0 \\\\\n(C - \\alpha_i) \\xi_i = 0 \\\\\n(C - \\alpha_i^\\star) \\xi_i^\\star = 0\n\\end{align*}\n如果 $\\alpha_i  0$ 或 $\\alpha_i^\\star  0$，则数据点 $i$ 是一个**支持向量**。我们特别关注**非边界支持向量**，其对偶变量严格介于 $0$ 和 $C$ 之间。\n\n1.  如果 $0  \\alpha_i^\\ast  C$，那么 $\\xi_i = 0$，并且第一个 KKT 条件意味着 $\\varepsilon - y_i + w^\\top x_i + b = 0$。这给出 $b = y_i - w^\\top x_i - \\varepsilon$。这些点恰好位于 $\\varepsilon$-管道的上边界。\n2.  如果 $0  \\alpha_i^{\\star\\ast}  C$，那么 $\\xi_i^\\star = 0$，并且第二个 KKT 条件意味着 $\\varepsilon + y_i - w^\\top x_i - b = 0$。这给出 $b = y_i - w^\\top x_i + \\varepsilon$。这些点恰好位于 $\\varepsilon$-管道的下边界。\n\n为提高数值稳定性，计算 $b$ 的标准流程是对所有非边界支持向量得到的值取平均。\n$$\nb = \\text{mean}\\left( \\{y_i - w^\\top x_i - \\varepsilon \\mid 0  \\alpha_i^\\ast  C\\} \\cup \\{y_i - w^\\top x_i + \\varepsilon \\mid 0  \\alpha_i^{\\star\\ast}  C\\} \\right)\n$$\n\n如果所有非边界支持向量都位于管道的一侧（例如，只存在 $0  \\alpha_i^\\ast  C$ 的点），这表明残差存在系统性偏斜。例如，如果所有这些点都在上边界，这意味着模型倾向于低估（$y_i > f(x_i)$）。仅基于这些点计算 $b$ 可能会捕捉到这种系统性偏移，导致非零的平均残差 $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))$，这量化了模型的整体偏差。\n\n在不存在非边界支持向量的边界情况下（所有 $\\alpha_i^\\ast, \\alpha_i^{\\star\\ast}$ 都在 $0$ 或 $C$），上述方法失效。一个关于 $b$ 的一致估计量必须满足所有数据点的 KKT 条件。一个有效的 $b$ 必须位于区间 $[b_{\\text{low}}, b_{\\text{high}}]$ 内，其中：\n$$\nb_{\\text{low}} = \\max_{i: \\alpha_i^\\ast  C} (y_i - w^\\top x_i - \\varepsilon) \\quad \\text{和} \\quad b_{\\text{high}} = \\min_{i: \\alpha_i^{\\star\\ast}  C} (y_i - w^\\top x_i + \\varepsilon)\n$$\n当不存在非边界支持向量时，索引集分别为 $\\{i | \\alpha_i^\\ast=0\\}$ 和 $\\{i | \\alpha_i^{\\star\\ast}=0\\}$。一个稳健的选择是该可行区间的中点：$b = \\frac{1}{2}(b_{\\text{low}} + b_{\\text{high}})$。\n\n该实现将为每个测试案例解决对偶 QP 问题，计算 $w$，然后根据是否存在非边界支持向量，使用适当的方法计算 $b$。最后，它将报告学习到的 $b$ 和由此产生的平均残差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_svr(X, y, C, epsilon):\n    \"\"\"\n    Solves a linear Support Vector Regression problem.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, n_features).\n        y (np.ndarray): Target values of shape (n_samples,).\n        C (float): Regularization parameter.\n        epsilon (float): Epsilon-tube width.\n\n    Returns:\n        tuple: A tuple containing the bias `b` (float) and the mean residual (float).\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Construct the Gram matrix for the linear kernel.\n    K = X @ X.T\n    \n    # Formulate the dual QP problem for scipy.optimize.minimize.\n    # We want to minimize: 0.5 * beta.T @ Q @ beta + p.T @ beta\n    # where beta is the concatenation of alpha and alpha_star vectors.\n    \n    # Quadratic term matrix Q\n    Q = np.block([\n        [K, -K],\n        [-K, K]\n    ])\n    \n    # Linear term vector p\n    p = np.concatenate([-y + epsilon, y + epsilon])\n    \n    # Objective function and its Jacobian (gradient)\n    def objective(beta):\n        return 0.5 * beta.T @ Q @ beta + p.T @ beta\n        \n    def jacobian(beta):\n        return Q @ beta + p\n        \n    # Define constraints for the optimizer.\n    # Equality constraint: sum(alpha_i - alpha_i_star) = 0\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda beta: np.sum(beta[:n_samples]) - np.sum(beta[n_samples:]),\n        'jac': lambda beta: np.concatenate([np.ones(n_samples), -np.ones(n_samples)])\n    })\n    \n    # Box constraints (bounds) for dual variables: 0 = alpha_i, alpha_i_star = C\n    bounds = [(0, C)] * (2 * n_samples)\n    \n    # Initial guess for the dual variables\n    beta_0 = np.zeros(2 * n_samples)\n    \n    # Solve the QP problem using SLSQP\n    result = minimize(objective, beta_0, jac=jacobian, bounds=bounds, constraints=constraints, method='SLSQP')\n    beta = result.x\n    \n    alpha = beta[:n_samples]\n    alpha_star = beta[n_samples:]\n    \n    # A small tolerance to handle numerical precision issues when checking boundaries.\n    tol = 1e-6\n\n    # Compute the weight vector w. For d=1, w is a scalar.\n    w = np.sum((alpha - alpha_star).reshape(-1, 1) * X, axis=0)\n    \n    # Compute the bias term b using KKT conditions.\n    \n    # Find indices of non-bound support vectors\n    non_bound_sv_indices_upper = np.where((alpha  tol)  (alpha  C - tol))[0]\n    non_bound_sv_indices_lower = np.where((alpha_star  tol)  (alpha_star  C - tol))[0]\n\n    b_values = []\n    \n    # Check for non-bound SVs on the upper boundary\n    if len(non_bound_sv_indices_upper)  0:\n        b_upper_candidates = y[non_bound_sv_indices_upper] - X[non_bound_sv_indices_upper] @ w - epsilon\n        b_values.extend(b_upper_candidates.tolist())\n        \n    # Check for non-bound SVs on the lower boundary\n    if len(non_bound_sv_indices_lower)  0:\n        b_lower_candidates = y[non_bound_sv_indices_lower] - X[non_bound_sv_indices_lower] @ w + epsilon\n        b_values.extend(b_lower_candidates.tolist())\n\n    if len(b_values)  0:\n        # Standard method: average over all non-bound SVs\n        b = np.mean(b_values)\n    else:\n        # Fallback method: no non-bound SVs exist.\n        # Compute b from the interval defined by all points satisfying KKT conditions.\n        s1_indices = np.where(alpha  C - tol)[0]\n        s2_indices = np.where(alpha_star  C - tol)[0]\n        \n        b_low = np.max(y[s1_indices] - X[s1_indices] @ w - epsilon)\n        b_high = np.min(y[s2_indices] - X[s2_indices] @ w + epsilon)\n        \n        b = (b_low + b_high) / 2\n        \n    # Calculate final predictions and the mean residual.\n    f_x = X @ w + b\n    mean_residual = np.mean(y - f_x)\n    \n    return b, mean_residual\n\ndef solve():\n    # Define the test cases from the problem statement.\n    \n    # Test case 1 (balanced residuals)\n    x1 = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8])\n    w_true1, b_true1 = 1.5, 0.2\n    noise1 = np.array([0.12, -0.12, 0.08, -0.08, 0.15, -0.15, 0.05, -0.05, 0.11, -0.11])\n    y1 = w_true1 * x1 + b_true1 + noise1\n    params1 = (x1.reshape(-1, 1), y1, 1.0, 0.1)\n\n    # Test case 2 (one-sided residuals)\n    x2 = x1\n    w_true2, b_true2 = 1.0, 0.0\n    noise2 = np.array([0.2, 0.15, 0.12, 0.18, 0.25, 0.05, 0.14, 0.11, 0.16, 0.22])\n    y2 = w_true2 * x2 + b_true2 + noise2\n    params2 = (x2.reshape(-1, 1), y2, 1.0, 0.1)\n\n    # Test case 3 (boundary case, small C)\n    x3 = x1\n    w_true3, b_true3 = 1.2, -0.1\n    noise3 = np.array([0.09, -0.09, 0.08, -0.08, 0.07, -0.07, 0.06, -0.06, 0.05, -0.05])\n    y3 = w_true3 * x3 + b_true3 + noise3\n    params3 = (x3.reshape(-1, 1), y3, 0.05, 0.1)\n\n    test_cases = [params1, params2, params3]\n\n    results = []\n    for case in test_cases:\n        X, y, C, epsilon = case\n        b, mean_res = solve_svr(X, y, C, epsilon)\n        results.append(b)\n        results.append(mean_res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}