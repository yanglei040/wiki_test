{
    "hands_on_practices": [
        {
            "introduction": "径向基函数（RBF）核的核心是欧几里得距离，这使得它对特征的尺度（scale）异常敏感。如果不同特征的数值范围相差巨大，会发生什么？这个练习将通过一个思想实验，揭示在应用RBF核之前进行特征标准化的极端重要性，帮助你理解为什么忽略这一步会使模型“只见树木，不见森林”。",
            "id": "2433217",
            "problem": "您正在构建一个二元分类器，以使用每个样本具有 $p$ 个特征（表示为 $\\mathbf{x} \\in \\mathbb{R}^p$）的基因表达谱来区分肿瘤样本和正常样本。原始数据包含异构的尺度：一些基因的原始计数数量级在 $10^3$ 到 $10^4$ 之间，而其他基因的归一化值则接近 $0$ 或在 $[0,1]$ 范围内。您使用径向基函数 (RBF) 核（即高斯核）来训练一个支持向量机 (SVM)，其定义为\n$$\nk(\\mathbf{x}, \\mathbf{z}) \\;=\\; \\exp\\!\\big(-\\gamma \\,\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2\\big),\n$$\n其中核参数 $\\gamma > 0$ 和正则化参数 $C > 0$ 是固定的。在训练前，您忘记了对特征进行归一化或标准化。\n\n哪种说法最能描述这种情况对在原始输入空间中学到的决策边界的影响？\n\nA. 决策边界不变，因为核技巧使分类器对特征缩放具有不变性。\n\nB. 决策边界变得由高量级的基因主导：欧几里得距离由这些坐标决定，使得 $k(\\mathbf{x}, \\mathbf{z})$ 接近于 $0$，除非样本在这些基因上极其接近。分类器变得非常局部化，并沿着高尺度方向变得高度扭曲，同时有效忽略了低尺度的基因。\n\nC. 决策边界简化为单个线性超平面，因为大量级特征导致 RBF 核在样本间近似恒定。\n\nD. 正则化隐式地重新缩放了特征，因此决策边界实际上与所有基因都预先标准化的情况相同。",
            "solution": "首先，我们必须验证问题陈述的有效性。\n\n该问题描述了在基因表达数据上使用支持向量机 (SVM) 进行二元分类的任务。已知条件如下：\n- 输入数据：向量 $\\mathbf{x} \\in \\mathbb{R}^p$，其中 $p$ 是特征（基因）的数量。\n- 特征尺度：高度异构，一些特征的量级在 $10^3$ 到 $10^4$ 之间，而其他特征则在 $[0, 1]$ 范围内。\n- 模型：使用径向基函数 (RBF) 核的支持向量机。\n- 核定义：$k(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2)$，其中 $\\gamma > 0$ 是一个固定参数。\n- 正则化：一个固定的参数 $C > 0$。\n- 关键条件：在训练前未对特征进行归一化或标准化。\n- 问题：描述这种缺乏缩放对学到的决策边界的影响。\n\n该问题陈述具有科学依据，提法得当且客观。它描述了在机器学习算法（特别是像核SVM这样基于距离的方法）的实际应用中一个常见且关键的问题。问题设定是自洽的，没有矛盾或模糊之处。所用术语在计算生物学和机器学习领域是标准的。因此，该问题是有效的，我们可以进行求解。\n\n分析的核心在于 RBF 核的自变量，即两个数据点之间的平方欧几里得距离 $\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$。该距离计算为所有特征上的总和：\n$$\n\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2 = \\sum_{i=1}^{p} (x_i - z_i)^2\n$$\n让我们将特征索引集 $\\{1, 2, \\dots, p\\}$ 划分为两个不相交的集合：$I_{high}$ 表示高量级特征，而 $I_{low}$ 表示低量级特征。\n$I_{high}$ 中的特征值量级在 $10^3$ 到 $10^4$ 之间。这些特征上一个很小的相对差异可能导致一个非常大的绝对差异。例如，如果 $x_j, z_j \\in I_{high}$ 仅相差 $1\\%$，比如 $x_j=5000$ 和 $z_j=5050$，那么差的平方 $(x_j - z_j)^2 = 50^2 = 2500$。\n相比之下，对于 $I_{low}$ 中的特征，其值在 $[0, 1]$ 范围内，可能的最大平方差为 $(1-0)^2 = 1$。\n\n总的平方欧几里得距离可以写成：\n$$\n\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2 = \\sum_{j \\in I_{high}} (x_j - z_j)^2 + \\sum_{k \\in I_{low}} (x_k - z_k)^2\n$$\n很明显，这个和完全由高量级特征的项主导。相比之下，低量级特征的贡献在数值上可以忽略不计。对于任何两个在高尺度维度上不是极其接近的不同点 $\\mathbf{x}$ 和 $\\mathbf{z}$，项 $\\sum_{j \\in I_{high}} (x_j - z_j)^2$ 将是一个非常大的正数。因此，整个平方距离 $\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$ 将会很大。\n\n现在，考虑核函数 $k(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2)$。由于 $\\gamma > 0$ 且对于大多数点对 $(\\mathbf{x}, \\mathbf{z})$，$\\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$ 都很大，因此指数 $-\\gamma \\lVert \\mathbf{x} - \\mathbf{z} \\rVert^2$ 将是一个绝对值很大的负数。这导致：\n$$\nk(\\mathbf{x}, \\mathbf{z}) \\approx \\exp(-\\text{large positive number}) \\approx 0\n$$\n核函数衡量了一种“相似性”的概念，对于任何一对点，除非它们彼此异常接近（特别是在对应于高量级特征的维度上），否则其值将几乎为零。\n\n对于一个新点 $\\mathbf{u}$，决策函数由 $f(\\mathbf{u}) = \\sum_{i \\in \\text{SV}} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{u}) + b$ 给出，其中 $\\mathbf{x}_i$ 是支持向量，$y_i \\in \\{-1, 1\\}$ 是它们的标签，$\\alpha_i$ 是学到的权重。每个支持向量 $\\mathbf{x}_i$ 对点 $\\mathbf{u}$ 的预测影响是通过核值 $k(\\mathbf{x}_i, \\mathbf{u})$ 来调节的。由于这个值接近于零，除非 $\\mathbf{u}$ 位于 $\\mathbf{x}_i$ 的一个非常小的邻域内，因此每个支持向量都具有一个高度局部化的影响范围。这迫使决策边界成为这些小的影响范围球体的复杂拼接。分类器基本上会试图通过创建一个对高尺度特征的微小变化非常敏感的、高度扭曲的边界来“记忆”训练数据，同时忽略低尺度特征中包含的任何信息。\n\n现在我们评估给出的选项：\n\n**A. 决策边界不变，因为核技巧使分类器对特征缩放具有不变性。**\n这个说法根本上是错误的。RBF 核依赖于欧几里得距离，这使其对输入特征的尺度高度敏感。对缩放的不变性不是核技巧的普遍属性。某些核，如线性核，可能对统一缩放具有不变性，但 RBF 核是需要特征归一化才能得到有意义结果的典型函数例子。\n结论：**错误**。\n\n**B. 决策边界变得由高量级的基因主导：欧几里得距离由这些坐标决定，使得 $k(\\mathbf{x}, \\mathbf{z})$ 接近于 $0$，除非样本在这些基因上极其接近。分类器变得非常局部化，并沿着高尺度方向变得高度扭曲，同时有效忽略了低尺度的基因。**\n这个说法准确地总结了我们的推导。高量级特征主导了欧几里得距离。这使得 RBF 核值对于除了高维特征子空间中最近的邻居之外的所有点都趋近于 $0$。这种影响的局部化导致了一个高度复杂、扭曲和过拟合的决策边界，它对低量级特征实际上是“视而不见”的。\n结论：**正确**。\n\n**C. 决策边界简化为单个线性超平面，因为大量级特征导致 RBF 核在样本间近似恒定。**\n这与正确的效果恰恰相反。核值确实近似恒定，但其值是 $0$，而不是一个非零常数。一个非零常数核（或者更准确地说，一个所有非对角线元素都相似且接近对角线元素的核矩阵）只会在 $\\gamma$ 极小的情况下出现，这会使 SVM 表现得像一个线性分类器。而对于固定的 $\\gamma > 0$，大量级特征会产生相反的效果：它们使核变得高度非线性和局部化。\n结论：**错误**。\n\n**D. 正则化隐式地重新缩放了特征，因此决策边界实际上与所有基因都预先标准化的情况相同。**\n这个说法表明了对正则化参数 $C$ 作用的误解。在 SVM 中，$C$ 控制对错分类训练样本的惩罚。它在间隔大小和训练误差之间进行权衡。它不执行任何形式的特征缩放，无论是显式的还是隐式的。对偶形式中的权重 $\\alpha_i$ 与数据点（支持向量）相关联，而不是与特征相关联。因此，正则化不能弥补特征归一化的缺失。\n结论：**错误**。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "获得一个在训练集上表现完美但在测试集上完全失效的模型，是机器学习实践中令人沮丧却又常见的经历。这个练习模拟了一个典型的过拟合场景，挑战你诊断出问题的根源。通过分析模型表现，你将深入理解RBF核的关键超参数$\\gamma$是如何控制模型复杂度的，以及如何识别因$\\gamma$值不当而导致的模型“记忆”训练数据而非学习通用规律的现象。",
            "id": "2433181",
            "problem": "一个研究团队正在构建一个支持向量机（SVM）分类器，以根据序列衍生的特征（如 `$k$`-mer 频率、预测的二级结构分数和 Pfam 结构域计数）来预测蛋白质功能。数据集包含 $n=2000$ 个蛋白质，被划分为类别均衡的分层训练/测试集。一个使用径向基函数（RBF）核的支持向量机（SVM）在使用标准特征缩放后进行训练。该模型在训练集上达到了 $99\\%$ 的准确率，但在测试集上只有 $50\\%$ 的准确率。\n\n根据带有松弛变量的 SVM 最小化一个在最大化间隔和训练误差之间进行权衡的目标函数这一基本定义：\n$$\n\\begin{aligned}\n\\min_{\\mathbf{w},b,\\boldsymbol{\\xi}} \\quad & \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 + C \\sum_{i=1}^{n} \\xi_i \\\\\n\\text{s.t.} \\quad & y_i\\left(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b\\right) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0,\n\\end{aligned}\n$$\n并且核技巧用核函数 $k(\\mathbf{x},\\mathbf{x}')$ 替代了内积 $\\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle$，其中 RBF 核定义为\n$$\nk(\\mathbf{x},\\mathbf{x}') = \\exp\\!\\left(-\\gamma \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^2\\right),\n$$\n请就此生物信息学任务中的泛化能力与训练拟合进行推理。\n\n哪个超参数最可能是造成观察到的差距的原因？为什么？\n\nA. 正则化参数 $C$ 太大，因此模型严重惩罚训练误差，缩小了间隔，并对训练数据过拟合。\n\nB. 正则化参数 $C$ 太小，因此模型欠拟合；这解释了 $99\\%$ 的训练准确率但 $50\\%$ 的测试准确率。\n\nC. RBF 核宽度参数 $\\gamma$ 太大，因此 $k(\\mathbf{x},\\mathbf{x}')$ 变得高度局部化，决策函数变得过于复杂，实际上是记住了训练集。\n\nD. RBF 核宽度参数 $\\gamma$ 太小，因此核函数变得过于宽泛且接近线性；这解释了 $99\\%$ 的训练准确率但 $50\\%$ 的测试准确率。",
            "solution": "问题陈述需经过验证。\n\n**步骤1：提取已知信息**\n- 机器学习任务：用于蛋白质功能预测的支持向量机（SVM）分类器。\n- 特征：$k$-mer 频率、预测的二级结构分数、Pfam 结构域计数。\n- 数据集大小：$n=2000$ 个蛋白质。\n- 数据划分：类别均衡的分层训练/测试集划分。\n- 模型：使用径向基函数（RBF）核的 SVM，采用标准特征缩放。\n- 性能：训练集准确率 $99\\%$，测试集准确率 $50\\%$。\n- SVM 目标函数：$\\min_{\\mathbf{w},b,\\boldsymbol{\\xi}} \\ \\frac{1}{2}\\lVert \\mathbf{w}\\rVert^2 + C \\sum_{i=1}^{n} \\xi_i$。\n- SVM 约束条件：$y_i\\left(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i) + b\\right) \\ge 1 - \\xi_i$ 且 $\\xi_i \\ge 0$。\n- 核技巧：内积 $\\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle$ 被核函数 $k(\\mathbf{x},\\mathbf{x}')$ 替代。\n- RBF 核定义：$k(\\mathbf{x},\\mathbf{x}') = \\exp\\!\\left(-\\gamma \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^2\\right)$。\n- 问题：确定最可能导致观察到的性能差距的超参数，并解释原因。\n\n**步骤2：使用提取的已知信息进行验证**\n该问题具有科学依据，描述了计算生物学和机器学习中的一个标准且现实的场景。SVM 目标函数和 RBF 核的定义在数学上是正确的。观察到的现象——训练准确率（$99\\%$）和测试准确率（$50\\%$）之间的巨大差异——是严重过拟合的典型案例。对于一个类别均衡的二分类任务，$50\\%$ 的测试准确率相当于随机猜测，表明模型完全没有泛化能力。这个问题提得很好，要求根据超参数 $C$ 和 $\\gamma$ 的作用，对过拟合的原因进行合乎逻辑的推断。问题陈述是自洽的、客观的且内部一致的。\n\n**步骤3：结论与行动**\n问题有效。将推导解决方案。\n\n问题的核心是训练准确率（$99\\%$）和测试准确率（$50\\%$）之间的巨大差距。这是一个教科书式的严重过拟合的例子，模型完美地学习了训练数据（包括噪声），以至于无法泛化到未见过的数据上。在类别均衡的情况下，$50\\%$ 的测试准确率表明模型对新数据的预测能力不比随机猜测好。我们必须分析两个超参数 $C$ 和 $\\gamma$ 的作用，以确定最可能的原因。\n\n参数 $C$ 是正则化参数。它控制着在最大化间隔和最小化训练集分类误差之间的权衡。\n- 大的 $C$ 会对被错误分类的训练样本（即 $\\xi_i > 0$ 的样本）施加高额惩罚。这迫使优化器去寻找一个能够尽可能多地正确分类训练样本的决策边界，即使这意味着需要一个复杂的边界和一个小间隔。因此，大的 $C$ 会助长过拟合。\n- 小的 $C$ 施加的惩罚较小，允许更多的训练样本被错误分类，以换取更大的间隔。这会导致一个更简单、更“软”的决策边界，如果 $C$ 太小，可能会导致欠拟合。\n\nRBF 核 $k(\\mathbf{x},\\mathbf{x}') = \\exp(-\\gamma \\lVert \\mathbf{x} - \\mathbf{x}' \\rVert^2)$ 中的参数 $\\gamma$ 定义了单个训练样本的影响范围。\n- 小的 $\\gamma$ 会导致每个支持向量有较大的影响半径，因为核函数值随距离增加而下降缓慢。最终的决策边界会很平滑，其行为类似于线性分类器。非常小的 $\\gamma$ 可能导致欠拟合。\n- 大的 $\\gamma$ 会导致一个非常小的影响半径。即使对于离支持向量中等距离的点，核函数值也会迅速下降到接近零。这意味着决策函数只受支持向量紧邻区域内的点的影响。最终的决策边界变得高度复杂和非线性，实质上是围绕训练样本的一系列小的决策区域“孤岛”。这使得模型能够“记住”训练集，导致极端过拟合。\n\n鉴于性能指标——近乎完美的训练准确率和随机猜测水平的测试准确率——模型不仅是过拟合，而且是完全没有学到可泛化的模式。虽然大的 $C$ 通过惩罚训练误差来导致过拟合，但一个非常大的 $\\gamma$ 为观察到的极端“记忆”行为提供了机制。当 $\\gamma$ 很大时，每个训练点都可以成为自己的支持向量，在自身周围创建一个局部的决策区域。这完美地解释了模型如何在训练集上达到 $99\\%$ 的准确率，而对于那些不落在某个训练点极近位置的测试点却毫无预测能力。因此，对于这种特定的、灾难性的失败模式，一个过大的 $\\gamma$ 是最直接、最有说服力的解释。\n\n选项评估：\n\nA. 正则化参数 $C$ 太大，因此模型严重惩罚训练误差，缩小了间隔，并对训练数据过拟合。\n这种说法在事实上是正确的。大的 $C$ 确实会导致过拟合。然而，它不像 $\\gamma$ 的效应那样，能直接解释性能为何会崩溃到 $50\\%$（随机猜测）的极端情况。它是一个促成因素，但对于这个特定的结果来说，可能不是主要的或影响最大的因素。\n\nB. 正则化参数 $C$ 太小，因此模型欠拟合；这解释了 $99\\%$ 的训练准确率但 $50\\%$ 的测试准确率。\n这种说法是自相矛盾的。小的 $C$ 会导致欠拟合，表现为训练准确率低，而不是 $99\\%$。因此，该选项**不正确**。\n\nC. RBF 核宽度参数 $\\gamma$ 太大，因此 $k(\\mathbf{x},\\mathbf{x}')$ 变得高度局部化，决策函数变得过于复杂，实际上是记住了训练集。\n这种说法准确地描述了大 $\\gamma$ 的影响。高度的局部化导致模型能够完美拟合训练数据的特定排列，从而获得近乎完美的训练准确率。同样是这种复杂性导致了泛化能力的完全丧失，产生的测试准确率不比随机猜测好。这是对观察到的性能最精确的解释。该选项**正确**。\n\nD. RBF 核宽度参数 $\\gamma$ 太小，因此核函数变得过于宽泛且接近线性；这解释了 $99\\%$ 的训练准确率但 $50\\%$ 的测试准确率。\n这种说法是自相矛盾的。小的 $\\gamma$ 会导致一个更简单的、近乎线性的模型，如果真实边界是复杂的，这将导致欠拟合。它不可能在复杂的数据集上达到 $99\\%$ 的训练准确率。因此，该选项**不正确**。\n\n比较 A 和 C，选项 C 为观察到的极端过拟合现象提供了一个更强大、更具体的解释。由大 $\\gamma$ 引起的“记忆”效应是泛化能力完全崩溃至随机猜测水平的最可能原因。",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "理论上的优雅有时会在计算实践中遭遇数值不稳定的挑战，核矩阵正是这样一个例子。这个动手编程练习将带你深入RBF核的“底层”，通过实验探索当超参数$\\gamma$取极端值或数据存在重复点时，核矩阵$K$如何变得病态（ill-conditioned）。你将亲手验证这些失效模式，并测试诸如“抖动”（jitter）之类的正则化技巧如何有效地恢复数值稳定性，从而对核方法的计算基础建立更深刻的认识。",
            "id": "3165648",
            "problem": "您将分析在高斯径向基函数（RBF）核矩阵中，当带宽参数取极端值时的数值稳定性。对于数据点 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，高斯核定义为\n$$\nK_{ij} = \\exp\\!\\left(-\\gamma \\,\\lVert x_i - x_j \\rVert_2^2\\right),\n$$\n其中 $\\gamma > 0$ 是逆长度尺度。在某些配置下，核矩阵可能会变得数值病态，这会导致在核岭回归等统计学习算法中出现不稳定的线性代数运算。您的目标是识别失效模式，并测试两种正则化策略：添加一个小的对角抖动和通过先验区间来限制参数 $\\gamma$。\n\n将使用的基本原理和定义：\n- 核矩阵 $K$ 是对称正半定的。线性求解的数值稳定性由谱条件数 $\\kappa_2(K) = \\sigma_{\\max}(K) / \\sigma_{\\min}(K)$ 来表征，其中 $\\sigma_{\\max}$ 和 $\\sigma_{\\min}$ 分别是奇异值分解（SVD）得到的最大和最小奇异值。\n- 为避免在有限精度算术中除以小于机器精度的值，请使用截断条件数估计器\n$$\n\\kappa_\\delta(K) = \\frac{\\sigma_{\\max}(K)}{\\max\\{\\sigma_{\\min}(K),\\delta\\}},\n$$\n其中 $\\delta = 10^{-15}$。以浮点数形式报告 $\\log_{10} \\kappa_\\delta(K)$。\n- 通过最小奇异值定义一个近奇异性指标：如果一个矩阵 $K$ 的 $\\sigma_{\\min}(K)  \\tau$，则称其为近奇异的，阈值 $\\tau = 10^{-12}$。\n- 抖动正则化是加上一个单位矩阵的倍数：对于 $\\epsilon  0$，定义 $K_\\epsilon = K + \\epsilon I_n$，这将使所有特征值偏移 $\\epsilon$。\n- 有界 $\\gamma$ 先验通过 $\\gamma_{\\text{clip}} = \\min\\{\\max\\{\\gamma,\\gamma_{\\min}\\},\\gamma_{\\max}\\}$ 将提议的 $\\gamma$ 值裁剪到一个区间 $[\\gamma_{\\min},\\gamma_{\\max}]$ 内。\n\n数据：\n- 数据集 $\\mathcal{A}$ （包含完全重复的点）：$n=6$， $d=2$，点\n$$\nx_1=(0,0),\\; x_2=(1,0),\\; x_3=(0,1),\\; x_4=(1,1),\\; x_5=(0.1,0.1),\\; x_6=(0.1,0.1).\n$$\n- 数据集 $\\mathcal{B}$ （所有点都不同）：$n=6$， $d=2$，点\n$$\nx_1=(0,0),\\; x_2=(1,0),\\; x_3=(0,1),\\; x_4=(1,1),\\; x_5=(0.1,0.1),\\; x_6=(0.9,0.9).\n$$\n\n正则化参数：\n- 近奇异性阈值：$\\tau = 10^{-12}$。\n- 条件数裁剪：$\\delta = 10^{-15}$。\n- 抖动水平：$\\epsilon = 10^{-6}$。\n- 有界 $\\gamma$ 先验区间：$\\gamma_{\\min} = 10^{-3}$，$\\gamma_{\\max} = 10^{3}$。\n\n测试套件：\n对每一项，精确计算指定的量。\n1. 在数据集 $\\mathcal{A}$ 上，当 $\\gamma = 10^{-12}$ 且无抖动时，使用阈值 $\\tau$ 报告近奇异性布尔值。\n2. 在数据集 $\\mathcal{A}$ 上，当 $\\gamma = 1$ 且无抖动时，使用阈值 $\\tau$ 报告近奇异性布尔值。\n3. 在数据集 $\\mathcal{A}$ 上，当 $\\gamma = 1$ 且抖动 $\\epsilon = 10^{-6}$ 时，以浮点数形式报告 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$。\n4. 在数据集 $\\mathcal{B}$ 上，当 $\\gamma = 10^{-12}$ 且无抖动时，以浮点数形式报告 $\\log_{10}\\kappa_\\delta(K)$。\n5. 在数据集 $\\mathcal{B}$ 上，对于候选集 $\\{\\gamma\\} = \\{10^{-12}, 10^{-3}, 1, 10^{3}, 10^{12}\\}$，应用区间为 $[\\gamma_{\\min},\\gamma_{\\max}] = [10^{-3}, 10^{3}]$ 的有界 $\\gamma$ 先验，无抖动，并在裁剪后以浮点数形式报告最坏情况（集合中的最大值）的 $\\log_{10}\\kappa_\\delta(K)$。\n6. 在数据集 $\\mathcal{B}$ 上，对于相同的候选集 $\\{\\gamma\\}$，不应用有界 $\\gamma$ 先验，但为每个核矩阵添加抖动 $\\epsilon = 10^{-6}$；以浮点数形式报告最坏情况（集合中的最大值）的 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$。\n7. 在数据集 $\\mathcal{B}$ 上，当 $\\gamma = 10^{6}$ 且无抖动时，以浮点数形式报告 $\\log_{10}\\kappa_\\delta(K)$。\n\n您的程序必须生成单行输出，其中包含按此精确顺序排列的结果，结果为方括号括起来的逗号分隔列表：\n[result1,result2,result3,result4,result5,result6,result7]\n所有布尔值都不得加引号，所有浮点数必须以标准 Python 浮点格式打印。用户无需输入任何内容，也不得读取或写入任何文件。",
            "solution": "该问题要求分析高斯径向基函数（RBF）核矩阵 $K$ 在逆长度尺度参数 $\\gamma$ 的不同选择下的数值稳定性。稳定性由矩阵的谱条件数量化。我们将研究两种主要的病态来源以及两种相应的正则化策略。\n\n首先，我们建立基本原理。两个点 $x_i, x_j \\in \\mathbb{R}^d$ 之间的高斯 RBF 核由下式给出\n$$\nK_{ij} = \\exp(-\\gamma \\lVert x_i - x_j \\rVert_2^2)\n$$\n其中 $\\gamma  0$。得到的 $n \\times n$ 矩阵 $K$ 是对称且正半定的。求解涉及 $K$ 的线性系统（例如在支持向量机或核岭回归中）的数值稳定性，取决于其谱条件数 $\\kappa_2(K) = \\sigma_{\\max}(K) / \\sigma_{\\min}(K)$，其中 $\\sigma_{\\max}$ 和 $\\sigma_{\\min}$ 分别是 $K$ 的最大和最小奇异值。由于 $K$ 是对称正半定的，其奇异值就是其特征值。大的条件数表明矩阵接近奇异，并且涉及它的数值计算可能不稳定。问题定义了该度量的一个截断版本，$\\kappa_\\delta(K) = \\sigma_{\\max}(K) / \\max\\{\\sigma_{\\min}(K), \\delta\\}$，并设置了一个下限 $\\delta = 10^{-15}$ 来处理低于机器精度的值。\n\n高斯核矩阵的两种主要失效模式是：\n1.  **$\\gamma$ 的选择**：\n    - 当 $\\gamma \\to 0$ 时，指数 $-\\gamma \\lVert x_i - x_j \\rVert_2^2 \\to 0$。因此，每个元素 $K_{ij} \\to \\exp(0) = 1$。核矩阵 $K$ 趋近于全 1 矩阵 $J_n$。该矩阵的秩为 1，有一个等于 $n$ 的特征值和 $n-1$ 个等于 0 的特征值。因此，对于非常小的 $\\gamma$，$K$ 会变得近奇异，导致极大的条件数。\n    - 当 $\\gamma \\to \\infty$ 时，对于不同的点 $x_i \\neq x_j$，指数 $-\\gamma \\lVert x_i - x_j \\rVert_2^2 \\to -\\infty$，因此非对角元素 $K_{ij} \\to 0$。对角元素保持 $K_{ii} = \\exp(0)=1$。因此，$K$ 趋近于单位矩阵 $I_n$，这是完美条件化的（$\\kappa_2(I_n) = 1$）。但是，如果点非常接近，非对角项可能不会足够快地衰减到零，或者如果达到机器精度限制，它们可能在数值上变为零，这可能孤立点并影响谱。\n\n2.  **数据几何形状**：如果数据集包含重复的点，例如，对于 $i \\neq k$ 有 $x_i = x_k$，那么核矩阵的第 $i$ 行和第 $k$ 行（以及列）将变得相同。这种线性相关性使得矩阵 $K$ 对于任何 $\\gamma  0$ 都是奇异的，意味着其最小奇异值恰好为 0。\n\n为缓解这些问题，提出了两种正则化策略：\n1.  **抖动正则化**：将一个小的正项 $\\epsilon$ 加到 $K$ 的对角线上，得到 $K_\\epsilon = K + \\epsilon I_n$。这将 $K$ 的所有特征值移动了 $\\epsilon$。如果 $K$ 的特征值是 $\\lambda_1 \\ge \\dots \\ge \\lambda_n \\ge 0$，那么 $K_\\epsilon$ 的特征值是 $\\lambda_i + \\epsilon$。这保证了 $K_\\epsilon$ 的最小特征值至少为 $\\epsilon$，从而限制了条件数。\n2.  **有界 $\\gamma$ 先验**：这种方法将 $\\gamma$ 限制在一个预定义的区间 $[\\gamma_{\\min}, \\gamma_{\\max}]$ 内。通过裁剪候选的 $\\gamma$ 值，它防止了使用已知会导致病态的极端值。\n\n现在我们根据这些原理评估七个测试用例。\n\n**测试用例 1：在数据集 $\\mathcal{A}$ 上，当 $\\gamma = 10^{-12}$ 且无抖动时，报告近奇异性布尔值。**\n数据集 $\\mathcal{A}$ 包含重复点：$x_5 = x_6 = (0.1, 0.1)$。这意味着对于任何 $\\gamma  0$，核矩阵 $K$ 的第 5 行和第 6 行将是相同的。具有线性相关行的矩阵是奇异的，意味着其行列式为 0，并且至少有一个零特征值。因此，$\\sigma_{\\min}(K)$ 将为 0（或在有限精度算术中为机器 epsilon 的数量级）。近奇异性阈值为 $\\tau = 10^{-12}$。由于 $\\sigma_{\\min}(K) \\approx 0  10^{-12}$，该矩阵被声明为近奇异的。结果是 `True`。\n\n**测试用例 2：在数据集 $\\mathcal{A}$ 上，当 $\\gamma = 1$ 且无抖动时，报告近奇异性布尔值。**\n与前一情况一样，数据集 $\\mathcal{A}$ 中存在重复点 $x_5=x_6$ 确保了核矩阵 $K$ 是奇异的。此属性与 $\\gamma$ 的值无关（只要 $\\gamma  0$）。因此，$\\sigma_{\\min}(K) \\approx 0$，小于阈值 $\\tau=10^{-12}$。该矩阵是近奇异的。结果是 `True`。\n\n**测试用例 3：在数据集 $\\mathcal{A}$ 上，当 $\\gamma = 1$ 且抖动 $\\epsilon = 10^{-6}$ 时，报告 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$。**\n从测试用例 2 中我们知道，对于 $\\gamma=1$ 和数据集 $\\mathcal{A}$，核矩阵 $K$ 是奇异的，所以 $\\sigma_{\\min}(K) = 0$。应用抖动正则化构成 $K_\\epsilon = K + \\epsilon I_n$。这将 $K$ 的特征值移动了 $\\epsilon = 10^{-6}$。$K_\\epsilon$ 的最小奇异值（特征值）将约等于 $\\epsilon$。因此，$\\sigma_{\\min}(K_\\epsilon) \\approx 10^{-6}$。最大奇异值 $\\sigma_{\\max}(K_\\epsilon)$ 将是 $\\sigma_{\\max}(K) + \\epsilon$。那么条件数是 $\\kappa_\\delta(K_\\epsilon) \\approx (\\sigma_{\\max}(K)+\\epsilon)/\\epsilon$。我们用数值方法计算这个值。对数条件数将大约是 $\\log_{10}(\\sigma_{\\max}(K) / 10^{-6}) = \\log_{10}(\\sigma_{\\max}(K)) + 6$。\n\n**测试用例 4：在数据集 $\\mathcal{B}$ 上，当 $\\gamma = 10^{-12}$ 且无抖动时，报告 $\\log_{10}\\kappa_\\delta(K)$。**\n数据集 $\\mathcal{B}$ 包含不同的点，所以矩阵不保证是奇异的。但是，$\\gamma = 10^{-12}$ 非常小。如原理中所解释，当 $\\gamma \\to 0$ 时，核矩阵 $K$ 趋近于秩为 1 的全 1 矩阵 $J_6$。该矩阵将是近奇异的，有一个接近 $n=6$ 的大奇异值和 $n-1=5$ 个非常接近 0 的奇异值。最小奇异值 $\\sigma_{\\min}(K)$ 预计将远小于裁剪参数 $\\delta = 10^{-15}$。因此，截断条件数将是 $\\kappa_\\delta(K) = \\sigma_{\\max}(K) / \\delta$。结果将是 $\\log_{10}(\\sigma_{\\max}(K)) - \\log_{10}(10^{-15}) \\approx \\log_{10}(6) + 15 \\approx 15.778$。\n\n**测试用例 5：在数据集 $\\mathcal{B}$ 上，应用有界 $\\gamma$ 先验后，报告最坏情况的 $\\log_{10}\\kappa_\\delta(K)$。**\n$\\gamma$ 的候选集是 $\\{10^{-12}, 10^{-3}, 1, 10^{3}, 10^{12}\\}$。先验区间是 $[\\gamma_{\\min}, \\gamma_{\\max}] = [10^{-3}, 10^{3}]$。将裁剪规则 $\\gamma_{\\text{clip}} = \\min\\{\\max\\{\\gamma, \\gamma_{\\min}\\}, \\gamma_{\\max}\\}$ 应用于该集合得到：\n- $\\gamma=10^{-12} \\to 10^{-3}$\n- $\\gamma=10^{-3} \\to 10^{-3}$\n- $\\gamma=1 \\to 1$\n- $\\gamma=10^{3} \\to 10^{3}$\n- $\\gamma=10^{12} \\to 10^{3}$\n要测试的有效参数集是 $\\{10^{-3}, 1, 10^{3}\\}$。我们计算这三个 $\\gamma$ 值中每个值的 $\\log_{10}\\kappa_\\delta(K)$ 并报告最大值。最小的有效 gamma，$\\gamma=10^{-3}$，预计会产生最高的条件数，因为它最接近 $\\gamma \\to 0$ 的奇异状态。\n\n**测试用例 6：在数据集 $\\mathcal{B}$ 上，使用抖动报告最坏情况的 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$。**\n我们使用原始的 $\\gamma$ 候选集 $\\{10^{-12}, 10^{-3}, 1, 10^{3}, 10^{12}\\}$，但为每种情况应用抖动 $\\epsilon = 10^{-6}$。抖动可以防止条件数爆炸。\n- 对于 $\\gamma=10^{-12}$，$K \\approx J_6$。$K_\\epsilon$ 的特征值大约是 $\\{6+\\epsilon, \\epsilon, \\epsilon, \\epsilon, \\epsilon, \\epsilon\\}$。所以 $\\kappa(K_\\epsilon) \\approx (6+\\epsilon)/\\epsilon \\approx 6 \\times 10^6$。\n- 对于大的 $\\gamma=10^{12}$，$K \\approx I_6$，所以 $K_\\epsilon \\approx (1+\\epsilon)I_6$，并且 $\\kappa(K_\\epsilon) \\approx 1$。\n最坏情况（最大）的条件数预计出现在最小的 $\\gamma$ 时，即 $\\gamma=10^{-12}$。我们计算五个 $\\log_{10}\\kappa_\\delta(K_\\epsilon)$ 的值并取最大值。\n\n**测试用例 7：在数据集 $\\mathcal{B}$ 上，当 $\\gamma = 10^6$ 且无抖动时，报告 $\\log_{10}\\kappa_\\delta(K)$。**\n这里 $\\gamma=10^6$ 非常大。由于数据集 $\\mathcal{B}$ 中的所有点都是不同的，核矩阵 $K$ 将趋近于单位矩阵 $I_6$。非对角项 $K_{ij} = \\exp(-10^6 \\lVert x_i - x_j \\rVert_2^2)$ 将极其接近于零。矩阵将是强对角占优的，所有对角项都为 1。因此，其所有奇异值都将非常接近 1，其条件数将接近 1。得到的 $\\log_{10}\\kappa_\\delta(K)$ 将非常接近 0。\n\n该实现将通过数值计算这些量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not needed, numpy is sufficient\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing the 7 specified test cases on RBF kernel matrices.\n    \"\"\"\n\n    # --- Data and Parameters ---\n    \n    # Dataset A (with duplicate points)\n    X_A = np.array([\n        [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0],\n        [0.1, 0.1], [0.1, 0.1]\n    ])\n    \n    # Dataset B (all distinct points)\n    X_B = np.array([\n        [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0],\n        [0.1, 0.1], [0.9, 0.9]\n    ])\n    \n    # Regularization parameters\n    tau = 1e-12\n    delta = 1e-15\n    epsilon = 1e-6\n    gamma_min = 1e-3\n    gamma_max = 1e3\n\n    # --- Helper Functions ---\n    \n    def get_kernel_matrix(X, gamma):\n        \"\"\"Computes the Gaussian RBF kernel matrix.\"\"\"\n        # Pairwise squared Euclidean distances\n        # Using broadcasting for efficiency: (n, 1, d) - (1, n, d) -> (n, n, d)\n        diffs = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n        sq_dists = np.sum(diffs**2, axis=-1)\n        \n        # Gaussian kernel\n        K = np.exp(-gamma * sq_dists)\n        return K\n\n    def get_log_cond(K, d):\n        \"\"\"Computes the clipped log10 condition number.\"\"\"\n        # Singular values\n        s = np.linalg.svd(K, compute_uv=False)\n        sigma_max = np.max(s)\n        sigma_min = np.min(s)\n        \n        # Clipped condition number\n        kappa_delta = sigma_max / np.maximum(sigma_min, d)\n        \n        return np.log10(kappa_delta)\n        \n    def is_nearly_singular(K, t):\n        \"\"\"Checks if a matrix is nearly singular based on its smallest singular value.\"\"\"\n        s = np.linalg.svd(K, compute_uv=False)\n        sigma_min = np.min(s)\n        return sigma_min  t\n\n    results = []\n\n    # --- Test Suite Execution ---\n\n    # Test 1\n    gamma1 = 1e-12\n    K1 = get_kernel_matrix(X_A, gamma1)\n    results.append(is_nearly_singular(K1, tau))\n\n    # Test 2\n    gamma2 = 1.0\n    K2 = get_kernel_matrix(X_A, gamma2)\n    results.append(is_nearly_singular(K2, tau))\n\n    # Test 3\n    gamma3 = 1.0\n    K3 = get_kernel_matrix(X_A, gamma3)\n    K3_eps = K3 + epsilon * np.eye(K3.shape[0])\n    results.append(get_log_cond(K3_eps, delta))\n\n    # Test 4\n    gamma4 = 1e-12\n    K4 = get_kernel_matrix(X_B, gamma4)\n    results.append(get_log_cond(K4, delta))\n    \n    # Test 5\n    gamma_set = [1e-12, 1e-3, 1.0, 1e3, 1e12]\n    clipped_gammas = [np.clip(g, gamma_min, gamma_max) for g in gamma_set]\n    log_conds5 = []\n    for g in np.unique(clipped_gammas): # Use unique to avoid redundant calculations\n         K5 = get_kernel_matrix(X_B, g)\n         log_conds5.append(get_log_cond(K5, delta))\n    results.append(np.max(log_conds5))\n\n    # Test 6\n    log_conds6 = []\n    for g in gamma_set:\n        K6 = get_kernel_matrix(X_B, g)\n        K6_eps = K6 + epsilon * np.eye(K6.shape[0])\n        log_conds6.append(get_log_cond(K6_eps, delta))\n    results.append(np.max(log_conds6))\n\n    # Test 7\n    gamma7 = 1e6\n    K7 = get_kernel_matrix(X_B, gamma7)\n    results.append(get_log_cond(K7, delta))\n    \n    # --- Final Output ---\n    # Format: [result1,result2,result3,result4,result5,result6,result7]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}