{
    "hands_on_practices": [
        {
            "introduction": "在我们计算回归系数的置信区间之前，必须首先确保模型中的系数是唯一可识别的。这个练习通过一个经典的“虚拟变量陷阱”场景，展示了完美多重共线性如何导致普通最小二乘法（OLS）估计失效。通过这个实践，你将学会如何通过重新参数化模型来解决这个问题，并为有意义的参数（如组间均值差异）计算出明确的置信区间 。",
            "id": "3176621",
            "problem": "一位数据分析师使用线性回归模型对时薪进行建模，该模型包含一个截距项和用于表示组成员身份的虚拟变量。有两个组，$A$ 组和 $B$ 组，分析师使用虚拟变量 $D_A$ 和 $D_B$ 对成员身份进行编码，其中对于 $A$ 组成员，$D_A=1$，否则为 $0$；对于 $B$ 组成员，$D_B=1$，否则为 $0$。观测数据如下：\n- $A$ 组工资：$y=(10,12,11)$，\n- $B$ 组工资：$y=(7,9,8)$，\n因此总样本量为 $n=6$。分析师首先拟合了一个包含截距项和两个虚拟变量的模型，然后考虑重新参数化以解决任何潜在的可识别性问题。\n\n在此背景下，选择所有关于 $(X^{\\top}X)^{-1}$ 的存在性、回归系数的可识别性，以及重新参数化如何恢复回归系数的可识别标准误和置信区间的正确陈述。你可以假设一个同方差误差模型，并使用基于 $t$ 分布的常规 $95\\%$ 置信区间，其残差自由度与拟合模型相对应。\n\nA. 当模型中同时包含截距项以及 $D_A$ 和 $D_B$ 时，设计矩阵的列是完全共线的，因此 $X^{\\top}X$ 是奇异的，$(X^{\\top}X)^{-1}$ 不存在。\n\nB. 重新参数化，使用一个截距项和一个虚拟变量（例如，去掉 $D_B$ 并保留 $D_A$），会得到一个满列秩的设计矩阵，并且 $D_A$ 的系数（即 $A$ 组与 $B$ 组的均值差异）的 $95\\%$ 双侧置信区间大约为 $[0.73,\\,5.27]$。\n\nC. 另一种方法是，去掉截距项但保留 $D_A$ 和 $D_B$，这也会得到一个满列秩的设计矩阵，并且 $\\beta_A-\\beta_B$ 的 $95\\%$ 双侧置信区间与选项 B 中的置信区间相同。\n\nD. 即使不进行重新参数化，在包含截距项和两个虚拟变量的过参数化模型中，仍然可以为 $D_A$ 和 $D_B$ 的各自系数获得唯一的、与软件无关的标准误和 $95\\%$ 置信区间，因为普通最小二乘法会自动忽略冗余的列。",
            "solution": "问题陈述具有科学依据，表述清晰且客观。它提供了一个标准的教科书式场景，用于演示线性回归中的多重共线性、模型参数化和可估性等概念。分析所需的所有数据和条件都已给出且一致。我将对每个选项进行完整的推导和评估。\n\n响应向量 $y$ 结合了两组的工资：\n$$y = (10, 12, 11, 7, 9, 8)^{\\top}$$\n虚拟变量定义为：对于 $A$ 组的观测值（前三个），$D_A=1$；对于 $B$ 组的观测值（后三个），$D_A=0$。而对于 $B$ 组，$D_B=1$，对于 $A$ 组，$D_B=0$。\n\nA. 当模型中同时包含截距项以及 $D_A$ 和 $D_B$ 时，设计矩阵的列是完全共线的，因此 $X^{\\top}X$ 是奇异的，$(X^{\\top}X)^{-1}$ 不存在。\n\n模型为 $y_i = \\beta_0 + \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon_i$。设计矩阵 $X$ 有 $n=6$ 行和 $p=3$ 列（一列对应截距项 $\\beta_0$，一列对应 $\\beta_A$，一列对应 $\\beta_B$）。\n这些列分别是截距向量 $c_1$、$D_A$ 的向量 $c_2$ 和 $D_B$ 的向量 $c_3$：\n$$\nc_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nc_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nc_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n设计矩阵为 $X = [c_1, c_2, c_3]$。这些列之间存在线性相关性，因为对于每个观测值，个体要么属于 $A$ 组，要么属于 $B$ 组，这意味着 $D_A + D_B = 1$。因此，虚拟变量列的总和等于截距列：\n$$c_2 + c_3 = c_1 \\implies 1 \\cdot c_1 - 1 \\cdot c_2 - 1 \\cdot c_3 = \\mathbf{0}$$\n这种完全共线性意味着 $X$ 的列是线性相关的，矩阵 $X$ 不是满列秩的。$X$ 的秩为 $2$，而不是 $3$。\n因此，矩阵 $X^{\\top}X$ 是奇异的（不可逆）。我们可以计算它来验证：\n$$\nX^{\\top}X = \\begin{pmatrix}\n1  1  1  1  1  1 \\\\\n1  1  1  0  0  0 \\\\\n0  0  0  1  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1  0 \\\\\n1  1  0 \\\\\n1  1  0 \\\\\n1  0  1 \\\\\n1  0  1 \\\\\n1  0  1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6  3  3 \\\\\n3  3  0 \\\\\n3  0  3\n\\end{pmatrix}\n$$\n该矩阵的行列式为 $\\det(X^{\\top}X) = 6(3 \\cdot 3 - 0) - 3(3 \\cdot 3 - 0) + 3(0 - 3 \\cdot 3) = 54 - 27 - 27 = 0$。由于行列式为零，$X^{\\top}X$ 是奇异的，其逆矩阵 $(X^{\\top}X)^{-1}$ 不存在。这种情况被称为虚拟变量陷阱。系数 $\\beta_0, \\beta_A, \\beta_B$ 不是唯一可识别的。\n\n该陈述是**正确的**。\n\nB. 重新参数化，使用一个截距项和一个虚拟变量（例如，去掉 $D_B$ 并保留 $D_A$），会得到一个满列秩的设计矩阵，并且 $D_A$ 的系数（即 $A$ 组与 $B$ 组的均值差异）的 $95\\%$ 双侧置信区间大约为 $[0.73,\\,5.27]$。\n\n重新参数化的模型是 $y_i = \\beta'_0 + \\beta'_A D_{Ai} + \\epsilon'_i$。这是解决共线性问题的标准方法。设计矩阵 $X_1$ 有两列：\n$$\nX_1 = \\begin{pmatrix}\n1  1 \\\\\n1  1 \\\\\n1  1 \\\\\n1  0 \\\\\n1  0 \\\\\n1  0\n\\end{pmatrix}\n$$\n这两列是线性无关的，所以 $X_1$ 是满列秩（秩为 $2$），并且 $(X_1^{\\top}X_1)^{-1}$ 存在。\nOLS 估计量是 $\\hat{\\beta}' = (X_1^{\\top}X_1)^{-1}X_1^{\\top}y$。\n$$\nX_1^{\\top}X_1 = \\begin{pmatrix} 6  3 \\\\ 3  3 \\end{pmatrix} \\quad \\implies \\quad (X_1^{\\top}X_1)^{-1} = \\frac{1}{18-9} \\begin{pmatrix} 3  -3 \\\\ -3  6 \\end{pmatrix} = \\begin{pmatrix} 1/3  -1/3 \\\\ -1/3  2/3 \\end{pmatrix}\n$$\n$$\nX_1^{\\top}y = \\begin{pmatrix} \\sum y_i \\\\ \\sum_{i \\in A} y_i \\end{pmatrix} = \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}' = \\begin{pmatrix} \\hat{\\beta}'_0 \\\\ \\hat{\\beta}'_A \\end{pmatrix} = \\begin{pmatrix} 1/3  -1/3 \\\\ -1/3  2/3 \\end{pmatrix} \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix} = \\begin{pmatrix} (57-33)/3 \\\\ (-57+66)/3 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 3 \\end{pmatrix}\n$$\n在这个模型中，$\\beta'_0$ 代表参照组（$B$ 组，其中 $D_A=0$）的平均工资，而 $\\beta'_A$ 代表 $A$ 组和 $B$ 组之间的平均工资差异。$B$ 组的样本均值为 $(7+9+8)/3 = 8$，所以 $\\hat{\\beta}'_0=8$。$A$ 组的样本均值为 $(10+12+11)/3 = 11$。均值差异是 $11-8=3$，所以 $\\hat{\\beta}'_A=3$。陈述中对系数的解释是正确的。\n\n为了求出 $\\beta'_A$ 的置信区间，我们需要它的标准误。$SE(\\hat{\\beta}'_A) = \\sqrt{\\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22}}$。\n首先，我们计算残差平方和（$RSS$）来估计误差方差 $\\sigma^2$。对于 $A$ 组，拟合值为 $\\hat{y}_i = 8+3(1)=11$；对于 $B$ 组，拟合值为 $\\hat{y}_i = 8+3(0)=8$。\n$$RSS = \\sum(y_i-\\hat{y}_i)^2 = (10-11)^2+(12-11)^2+(11-11)^2 + (7-8)^2+(9-8)^2+(8-8)^2 = 1+1+0+1+1+0 = 4$$\n参数个数为 $p=2$。残差的自由度为 $df=n-p=6-2=4$。\n误差方差的无偏估计是 $\\hat{\\sigma}^2 = \\frac{RSS}{n-p} = \\frac{4}{4} = 1$。\n$\\hat{\\beta}'_A$ 的方差是 $\\text{Var}(\\hat{\\beta}'_A) = \\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22} = 1 \\cdot (2/3) = 2/3$。\n标准误是 $SE(\\hat{\\beta}'_A) = \\sqrt{2/3}$。\n对于自由度为 $df=4$ 的 $95\\%$ 置信区间，临界 t 值为 $t_{0.025, 4} = 2.776$。\n置信区间为 $\\hat{\\beta}'_A \\pm t_{0.025, 4} \\cdot SE(\\hat{\\beta}'_A)$:\n$$3 \\pm 2.776 \\cdot \\sqrt{2/3} \\approx 3 \\pm 2.776 \\cdot 0.8165 \\approx 3 \\pm 2.266$$\n这给出的区间是 $[0.734, 5.266]$，四舍五入后为 $[0.73, 5.27]$。\n\n该陈述是**正确的**。\n\nC. 另一种方法是，去掉截距项但保留 $D_A$ 和 $D_B$，这也会得到一个满列秩的设计矩阵，并且 $\\beta_A-\\beta_B$ 的 $95\\%$ 双侧置信区间与选项 B 中的置信区间相同。\n\n替代模型是 $y_i = \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon''_{i}$。设计矩阵 $X_2$ 是：\n$$\nX_2 = \\begin{pmatrix}\n1  0 \\\\\n1  0 \\\\\n1  0 \\\\\n0  1 \\\\\n0  1 \\\\\n0  1\n\\end{pmatrix}\n$$\n这两列是正交的，因此是线性无关的。$X_2$ 具有满列秩。\nOLS 估计量是 $\\hat{\\beta}'' = (X_2^{\\top}X_2)^{-1}X_2^{\\top}y$。\n$$\nX_2^{\\top}X_2 = \\begin{pmatrix} 3  0 \\\\ 0  3 \\end{pmatrix} \\quad \\implies \\quad (X_2^{\\top}X_2)^{-1} = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix}\n$$\n$$\nX_2^{\\top}y = \\begin{pmatrix} \\sum_{i \\in A} y_i \\\\ \\sum_{i \\in B} y_i \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}'' = \\begin{pmatrix} \\hat{\\beta}_A \\\\ \\hat{\\beta}_B \\end{pmatrix} = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 8 \\end{pmatrix}\n$$\n在这个模型中，$\\beta_A$ 是 $A$ 组的平均工资，$\\beta_B$ 是 $B$ 组的平均工资。它们的估计值分别是各自的样本均值。\n我们关心的是差异 $\\beta_A - \\beta_B$ 的置信区间。点估计为 $\\hat{\\beta}_A - \\hat{\\beta}_B = 11 - 8 = 3$。这与模型 B 中的点估计 $\\hat{\\beta}'_A$ 相同。\n这个差异的方差是 $\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\text{Var}(\\hat{\\beta}_A) + \\text{Var}(\\hat{\\beta}_B) - 2\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B)$。\n为了求出系数的方差-协方差矩阵，我们首先需要这个模型的 $\\hat{\\sigma}^2$。拟合值对于 $A$ 组是 $\\hat{y}_i = 11$，对于 $B$ 组是 $\\hat{y}_i = 8$。这些拟合值与模型 B 中的相同。因此，残差和 $RSS$ 是相同的：$RSS=4$。参数个数为 $p=2$，所以自由度 $df=n-p=4$，也与之前相同。因此，$\\hat{\\sigma}^2 = RSS/df = 4/4 = 1$。\n方差-协方差矩阵是 $\\text{Cov}(\\hat{\\beta}'') = \\hat{\\sigma}^2(X_2^{\\top}X_2)^{-1} = 1 \\cdot \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix}$。\n由此可得，$\\text{Var}(\\hat{\\beta}_A) = 1/3$，$\\text{Var}(\\hat{\\beta}_B) = 1/3$，且 $\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B) = 0$。\n所以，$\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = 1/3 + 1/3 - 0 = 2/3$。\n这个差异的标准误是 $SE(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\sqrt{2/3}$。\n点估计（$3$）、标准误（$\\sqrt{2/3}$）和自由度（$4$）都与选项 B 中 $\\hat{\\beta}'_A$ 的相应值相同。因此，$\\beta_A-\\beta_B$ 的 $95\\%$ 置信区间必然是相同的。模型 B 和 C 都是有效的重新参数化，它们张成了相同的拟合值向量空间，并且 $\\beta_A-\\beta_B$ 和 $\\beta'_A$ 都是可估函数，代表了相同的物理量（均值差异），所以它们的估计值和置信区间必须相同。\n\n该陈述是**正确的**。\n\nD. 即使不进行重新参数化，在包含截距项和两个虚拟变量的过参数化模型中，仍然可以为 $D_A$ 和 $D_B$ 的各自系数获得唯一的、与软件无关的标准误和 $95\\%$ 置信区间，因为普通最小二乘法会自动忽略冗余的列。\n\n正如在 A 中所证实的，过参数化模型的矩阵 $X^{\\top}X$ 是奇异的。这意味着系数向量 $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_A, \\hat{\\beta}_B)^{\\top}$ 没有唯一解。正规方程组 $X^{\\top}X\\hat{\\beta}=X^{\\top}y$ 有无穷多个解。\n具体来说，如果 $\\hat{\\beta}^*$ 是一个解，那么对于任意标量 $c$，向量 $\\hat{\\beta}^* + c(1, -1, -1)^{\\top}$ 也是一个解，因为 $(1, -1, -1)^{\\top}$ 位于 $X$ 的零空间中。\n单个系数 $\\beta_A$ 和 $\\beta_B$ 不是“可估”函数，这意味着它们的估计值依赖于为找到一个特定解而施加的任意约束。不同的统计软件包会施加不同的约束（例如，将一个系数设为零，或使用和为零的约束），这会导致 $\\hat{\\beta}_A$ 和 $\\hat{\\beta}_B$ 的值不同。例如，设置 $\\beta_B=0$ 会得到 $\\hat{\\beta}_A=3$，而设置 $\\beta_A=0$ 会得到 $\\hat{\\beta}_B=-3$。\n由于 $\\beta_A$ 和 $\\beta_B$ 的点估计不是唯一的，并且依赖于所使用的软件，因此它们的标准误和置信区间也不是唯一定义的或与软件无关的。$\\hat{\\beta}$ 的方差在形式上由 $\\sigma^2(X^{\\top}X)^-$ 给出，其中 $(X^{\\top}X)^-$ 是 $X^{\\top}X$ 的一个广义逆。广义逆不是唯一的，不同的选择会导致对于像 $\\beta_A$ 和 $\\beta_B$ 这样的不可估系数，其方差也不同（且没有意义）。\n关于 OLS“自动忽略冗余列”的说法，是一些软件实现方式的粗略描述，但是选择忽略哪一列是任意的，并且会影响各个系数的值。正是由于这种模糊性，才需要进行重新参数化，以获得对可解释参数的有意义、唯一的估计和推断。\n\n该陈述是**不正确的**。",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "当一个回归模型包含多个具有不同单位和量纲的预测变量时，直接比较它们的系数大小是没有意义的。这个练习将指导你如何通过标准化变量来解决这个问题，将所有系数转换到一个共同的、无单位的尺度上。你将推导出标准化系数（“贝塔系数”）与其标准误之间的关系，从而能够更有根据地比较不同预测变量的相对影响力 。",
            "id": "3176613",
            "problem": "一位研究人员使用普通最小二乘法 (OLS) 拟合了一个多元线性回归模型，该模型包含一个截距项，用于通过两个预测变量 $X_{1}$ 和 $X_{2}$ 来预测一个连续结果 $Y$。假设通常的线性模型条件成立：$Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$，其中 $\\mathbb{E}[\\varepsilon \\mid X_{1}, X_{2}] = 0$ 且 $\\operatorname{Var}(\\varepsilon \\mid X_{1}, X_{2}) = \\sigma^{2}$，$\\sigma^{2}$ 为常数。\n\n从一个大小为 $n$ 的样本中（将 $n$ 视为固定的），根据观测数据计算出以下经验性汇总统计量：\n- 样本标准差：$s_{X_{1}} = 8$，$s_{X_{2}} = 0.5$，$s_{Y} = 15$。\n- 估计的未标准化斜率及其报告的标准误：$\\hat{\\beta}_{1} = 2.50$，其 $\\operatorname{SE}(\\hat{\\beta}_{1}) = 0.60$；以及 $\\hat{\\beta}_{2} = -5.40$，其 $\\operatorname{SE}(\\hat{\\beta}_{2}) = 1.80$。\n\n定义标准化变量 $Z_{Y} = (Y - \\bar{Y})/s_{Y}$，$Z_{1} = (X_{1} - \\bar{X}_{1})/s_{X_{1}}$，以及 $Z_{2} = (X_{2} - \\bar{X}_{2})/s_{X_{2}}$。考虑将 $Z_{Y}$ 对 $Z_{1}$ 和 $Z_{2}$ 进行带截距项的回归：\n$$\nZ_{Y} = \\gamma_{0} + \\gamma_{1} Z_{1} + \\gamma_{2} Z_{2} + \\text{误差}.\n$$\n\n任务：\n1. 从 OLS 的定义以及回归变量和响应变量仿射重缩放的线性性质出发，用 $s_{X_{1}}, s_{X_{2}}, s_{Y}$ 表示，推导出标准化斜率系数向量 $(\\gamma_{1}, \\gamma_{2})^{\\top}$ 和未标准化斜率系数向量 $(\\beta_{1}, \\beta_{2})^{\\top}$ 之间的显式解析关系。你的推导应阐明为何标准化斜率是无单位的。\n2. 在所述假设下，使用 OLS 估计量的抽样分布，推导标准化斜率 $(\\hat{\\gamma}_{1}, \\hat{\\gamma}_{2})$ 的协方差矩阵，特别是其标准误，如何与 $(\\hat{\\beta}_{1}, \\hat{\\beta}_{2})$ 的相关联。推导时，将 $s_{X_{1}}, s_{X_{2}}, s_{Y}$ 视为从同一样本计算出的固定标量。\n3. 使用你的结果，根据上述汇总统计量，计算对应于 $X_{1}$ 的标准化系数 $\\hat{\\gamma}_{1}$ 的标准误的数值。\n\n给出 $\\hat{\\gamma}_{1}$ 标准误的最终数值答案，四舍五入到四位有效数字。最终答案中不要包含任何单位。",
            "solution": "在尝试解答之前，对问题陈述的有效性进行评估。\n\n### 步骤1：提取已知条件\n-   **模型：** 带截距项的多元线性回归，$Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$。\n-   **假设：** 普通最小二乘法 (OLS) 条件成立，包括 $\\mathbb{E}[\\varepsilon \\mid X_{1}, X_{2}] = 0$ 和 $\\operatorname{Var}(\\varepsilon \\mid X_{1}, X_{2}) = \\sigma^{2}$ (同方差性)。\n-   **样本大小：** $n$。\n-   **样本标准差：** $s_{X_{1}} = 8$，$s_{X_{2}} = 0.5$，$s_{Y} = 15$。\n-   **估计的未标准化系数：** $\\hat{\\beta}_{1} = 2.50$，$\\hat{\\beta}_{2} = -5.40$。\n-   **未标准化系数的标准误：** $\\operatorname{SE}(\\hat{\\beta}_{1}) = 0.60$，$\\operatorname{SE}(\\hat{\\beta}_{2}) = 1.80$。\n-   **标准化变量：** $Z_{Y} = (Y - \\bar{Y})/s_{Y}$，$Z_{1} = (X_{1} - \\bar{X}_{1})/s_{X_{1}}$，$Z_{2} = (X_{2} - \\bar{X}_{2})/s_{X_{2}}$。\n-   **标准化模型：** $Z_{Y} = \\gamma_{0} + \\gamma_{1} Z_{1} + \\gamma_{2} Z_{2} + \\text{误差}$。\n-   **任务：**\n    1.  推导标准化斜率 $(\\gamma_{1}, \\gamma_{2})$ 与未标准化斜率 $(\\beta_{1}, \\beta_{2})$ 之间的关系。\n    2.  推导标准化和未标准化斜率估计量标准误之间的关系。\n    3.  计算 $\\operatorname{SE}(\\hat{\\gamma}_{1})$ 的数值。\n\n### 步骤2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n-   **科学依据：** 该问题牢固地植根于线性回归理论，这是统计学和统计学习的核心课题。所有概念，如 OLS、标准化和标准误，都是标准的且定义明确。\n-   **适定性：** 该问题提供了推导所需表达式和计算最终值所需的所有数据和关系。任务规定明确，可得出唯一解。\n-   **客观性：** 该问题以精确的数学语言陈述，没有任何主观性或模糊性。\n\n未发现任何缺陷。该问题并非科学上不成立、不可形式化、不完整、不现实或不适定。这是一个标准的、非平凡的、旨在理解回归系数性质的练习。\n\n### 步骤3：结论和行动\n该问题被判定为**有效**。将提供完整的解答。\n\n解答分为三部分，对应问题陈述中的任务。\n\n**1. 推导标准化系数与未标准化系数之间的关系**\n\n我们从未标准化的总体参数线性回归模型开始：\n$$Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon$$\n标准化变量的定义为 $Z_{Y} = (Y - \\bar{Y})/s_{Y}$，$Z_{1} = (X_{1} - \\bar{X}_{1})/s_{X_{1}}$，以及 $Z_{2} = (X_{2} - \\bar{X}_{2})/s_{X_{2}}$。我们可以用标准化变量及其样本均值和标准差来表示原始变量：\n$$Y = s_{Y} Z_{Y} + \\bar{Y}$$\n$$X_{1} = s_{X_{1}} Z_{1} + \\bar{X}_{1}$$\n$$X_{2} = s_{X_{2}} Z_{2} + \\bar{X}_{2}$$\n将这些表达式代入未标准化的模型方程中，得到：\n$$s_{Y} Z_{Y} + \\bar{Y} = \\beta_{0} + \\beta_{1} (s_{X_{1}} Z_{1} + \\bar{X}_{1}) + \\beta_{2} (s_{X_{2}} Z_{2} + \\bar{X}_{2}) + \\varepsilon$$\n为了推导标准化模型，我们解出 $Z_{Y}$：\n$$s_{Y} Z_{Y} = (\\beta_{0} + \\beta_{1} \\bar{X}_{1} + \\beta_{2} \\bar{X}_{2} - \\bar{Y}) + (\\beta_{1} s_{X_{1}}) Z_{1} + (\\beta_{2} s_{X_{2}}) Z_{2} + \\varepsilon$$\n$$Z_{Y} = \\frac{\\beta_{0} + \\beta_{1} \\bar{X}_{1} + \\beta_{2} \\bar{X}_{2} - \\bar{Y}}{s_{Y}} + \\left(\\beta_{1} \\frac{s_{X_{1}}}{s_{Y}}\\right) Z_{1} + \\left(\\beta_{2} \\frac{s_{X_{2}}}{s_{Y}}\\right) Z_{2} + \\frac{\\varepsilon}{s_{Y}}$$\n该方程具有标准化回归模型 $Z_{Y} = \\gamma_{0} + \\gamma_{1} Z_{1} + \\gamma_{2} Z_{2} + \\text{误差'}$ 的形式，其中新的误差项是 $\\text{误差'} = \\varepsilon/s_{Y}$。通过比较系数，我们建立以下关系：\n$$\\gamma_{1} = \\beta_{1} \\frac{s_{X_{1}}}{s_{Y}}$$\n$$\\gamma_{2} = \\beta_{2} \\frac{s_{X_{2}}}{s_{Y}}$$\n标准化斜率系数向量 $(\\gamma_{1}, \\gamma_{2})^{\\top}$ 通过一个由标准差比率构成的对角缩放矩阵与未标准化向量 $(\\beta_{1}, \\beta_{2})^{\\top}$ 相关联。\n\n这些标准化系数（在某些软件包中常被称为“贝塔系数”）是无单位的。为了说明这一点，考虑 $\\gamma_{1}$ 中涉及的各项的单位。设 $[Q]$ 表示量 $Q$ 的单位。未标准化斜率 $\\beta_1$ 的单位是 $[Y]/[X_1]$。标准差的单位是 $[s_Y] = [Y]$ 和 $[s_{X_1}] = [X_1]$。因此，$\\gamma_1$ 的单位是：\n$$[\\gamma_{1}] = [\\beta_{1}] \\frac{[s_{X_{1}}]}{[s_{Y}]} = \\frac{[Y]}{[X_{1}]} \\frac{[X_{1}]}{[Y]} = 1$$\n得到的系数是一个无量纲的量。类似的论证也适用于 $\\gamma_{2}$。\n\n**2. 推导标准误之间的关系**\n\n由于估计过程的线性性，上述为总体参数推导出的关系也适用于其 OLS 估计量。因此，对于估计的系数：\n$$\\hat{\\gamma}_{1} = \\hat{\\beta}_{1} \\frac{s_{X_{1}}}{s_{Y}}$$\n$$\\hat{\\gamma}_{2} = \\hat{\\beta}_{2} \\frac{s_{X_{2}}}{s_{Y}}$$\n估计量的标准误是其抽样分布的标准差，即 $\\operatorname{SE}(\\cdot) = \\sqrt{\\operatorname{Var}(\\cdot)}$。问题指明，样本标准差 $s_{X_{1}}, s_{X_{2}}, s_{Y}$ 应被视为固定的、非随机的标量。这意味着我们是在这些观测到的样本统计量为条件的条件下，求解估计量的方差。\n\n使用方差的性质，即对于随机变量 $X$ 和常数 $c$，有 $\\operatorname{Var}(cX) = c^2 \\operatorname{Var}(X)$，我们可以求出 $\\hat{\\gamma}_{1}$ 的方差：\n$$\\operatorname{Var}(\\hat{\\gamma}_{1}) = \\operatorname{Var}\\left(\\hat{\\beta}_{1} \\frac{s_{X_{1}}}{s_{Y}}\\right) = \\left(\\frac{s_{X_{1}}}{s_{Y}}\\right)^2 \\operatorname{Var}(\\hat{\\beta}_{1})$$\n对两边取平方根，即可得到标准误之间的关系。由于标准差是非负的，比率 $s_{X_1}/s_Y$ 也是非负的。\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\sqrt{\\left(\\frac{s_{X_{1}}}{s_{Y}}\\right)^2 \\operatorname{Var}(\\hat{\\beta}_{1})} = \\frac{s_{X_{1}}}{s_{Y}} \\sqrt{\\operatorname{Var}(\\hat{\\beta}_{1})} = \\frac{s_{X_{1}}}{s_{Y}} \\operatorname{SE}(\\hat{\\beta}_{1})$$\n类似的关系也适用于第二个系数：\n$$\\operatorname{SE}(\\hat{\\gamma}_{2}) = \\frac{s_{X_{2}}}{s_{Y}} \\operatorname{SE}(\\hat{\\beta}_{2})$$\n更一般地，设 $\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2})^{\\top}$ 和 $\\hat{\\boldsymbol{\\gamma}} = (\\hat{\\gamma}_{1}, \\hat{\\gamma}_{2})^{\\top}$。该关系可以写成矩阵形式 $\\hat{\\boldsymbol{\\gamma}} = \\mathbf{S} \\hat{\\boldsymbol{\\beta}}$，其中 $\\mathbf{S}$ 是对角矩阵：\n$$\\mathbf{S} = \\frac{1}{s_Y} \\begin{pmatrix} s_{X_1}  0 \\\\ 0  s_{X_2} \\end{pmatrix}$$\n标准化系数的协方差矩阵 $\\operatorname{Cov}(\\hat{\\boldsymbol{\\gamma}})$ 与未标准化系数的协方差矩阵 $\\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}})$ 通过规则 $\\operatorname{Cov}(\\mathbf{A}\\mathbf{X}) = \\mathbf{A}\\operatorname{Cov}(\\mathbf{X})\\mathbf{A}^{\\top}$ 相关联：\n$$\\operatorname{Cov}(\\hat{\\boldsymbol{\\gamma}}) = \\mathbf{S} \\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{S}^{\\top} = \\mathbf{S} \\operatorname{Cov}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{S}$$\n$\\operatorname{Cov}(\\hat{\\boldsymbol{\\gamma}})$ 的对角元素是方差 $\\operatorname{Var}(\\hat{\\gamma}_{1})$ 和 $\\operatorname{Var}(\\hat{\\gamma}_{2})$，这证实了上述按分量推导的正确性。\n\n**3. $\\operatorname{SE}(\\hat{\\gamma}_{1})$ 的数值计算**\n\n使用第2部分推导的公式和问题陈述中提供的数据：\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\frac{s_{X_{1}}}{s_{Y}} \\operatorname{SE}(\\hat{\\beta}_{1})$$\n给定值为 $s_{X_{1}} = 8$，$s_{Y} = 15$，以及 $\\operatorname{SE}(\\hat{\\beta}_{1}) = 0.60$。代入这些值：\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\frac{8}{15} \\times 0.60$$\n计算如下：\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = \\frac{8}{15} \\times \\frac{60}{100} = \\frac{8 \\times 4}{100} = \\frac{32}{100} = 0.32$$\n问题要求答案四舍五入到四位有效数字。精确值为 $0.32$。为了用四位有效数字表示，我们在后面追加两个零。\n$$\\operatorname{SE}(\\hat{\\gamma}_{1}) = 0.3200$$",
            "answer": "$$\\boxed{0.3200}$$"
        },
        {
            "introduction": "回归分析的结果有时可能对少数几个数据点异常敏感，这些点被称为“强影响点”。本实践将通过编程练习，让你亲手实现一个诊断流程，使用库克距离（Cook's distance）来识别这些强影响点。通过移除影响最大的数据点并重新拟合模型，你将直观地看到杠杆值（leverage）和残差大小如何共同作用，从而显著改变斜率系数的置信区间的宽度和位置 。",
            "id": "3176663",
            "problem": "给定一系列独立的数据集，每个数据集包含适用于带截距项的简单线性回归的配对观测值 $(x_i, y_i)$。您的任务是编写一个完整的程序，对于每个数据集，定量分析移除单个最具影响力的观测值（以库克距离衡量）如何改变斜率系数 $ \\beta_1 $ 的双侧 $95\\%$ 置信区间。该分析必须基于标准的简单线性回归模型，并从第一性原理出发，即从普通最小二乘法和投影（帽子）矩阵的定义开始，并使用高斯-马尔可夫 (Gauss–Markov) 框架的经典假设。您只能使用以下基本事实：线性模型设定、最小二乘法正规方程、投影矩阵的性质、无偏残差方差估计量，以及用于有限样本推断的学生t分布 (Student’s $t$ distribution)。\n\n对于每个数据集，仅使用这些基本事实执行以下操作：\n- 拟合简单线性回归模型 $ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $，其中 $ \\varepsilon_i $ 独立同分布，均值为 $ 0 $，方差恒为 $ \\sigma^2 $。\n- 从帽子矩阵的对角线计算杠杆值 $ h_i $，计算残差 $ e_i $，并使用自由度为 $ n - p $ 的无偏估计量计算均方误差 (MSE)，其中 $ n $ 是样本量，$ p $ 是包括截距项在内的回归参数数量。\n- 根据这些量，为每个观测值计算库克距离 $ D_i $，将具有最大 $ D_i $ 的观测值索标识为最具影响力的观测值，并记录该观测值在完整拟合下的杠杆值 $ h_i $、绝对残差 $ |e_i| $ 和库克距离 $ D_i $。\n- 在完整数据下为 $ \\beta_1 $ 构建双侧 $95\\%$ 置信区间并计算其总宽度。然后，移除最具影响力的观测值，重新拟合模型，并计算新的 $ \\beta_1 $ 双侧 $95\\%$ 置信区间宽度。\n- 对于每个数据集，按此确切顺序返回一个包含 $ 6 $ 个值的列表：$[\\text{width\\_before}, \\text{width\\_after}, \\text{narrower}, h_i, |e_i|, D_i]$，其中如果移除后区间宽度减小，则 $\\text{narrower}$ 为 $1$，否则为 $0$。所有浮点数值必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 案例 $ 1 $（高杠杆值和巨大残差的离群点）：\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20] $\n  - $ y = [1.0, 3.0, 5.1, 7.0, 9.2, 11.1, 13.0, 15.1, 17.2, 19.0, 21.1, 10.0] $\n- 案例 $ 2 $（高杠杆值但残差小的点）：\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20] $\n  - $ y = [1.2, 3.1, 5.0, 7.1, 9.0, 11.2, 13.2, 15.1, 17.0, 19.1, 40.9] $\n- 案例 $ 3 $（中等杠杆值但残差大的点）：\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] $\n  - $ y = [1.0, 3.1, 5.0, 7.2, 9.0, 50.0, 13.1, 15.0, 17.2, 19.1, 21.0] $\n- 案例 $ 4 $（小样本含极端 $ x $ 值点）：\n  - $ x = [0, 1, 2, 10, 11] $\n  - $ y = [1.0, 3.0, 5.2, 15.0, 23.1] $\n\n实现要求：\n- 您的实现必须显式地构建所需的线性代数对象，并且必须使用学生t分布来确定 $ \\beta_1 $ 的双侧 $95\\%$ 置信区间。\n- 在所有拟合中使用 $ p = 2 $ 个参数（截距和斜率）。\n- 双侧区间水平使用 $ \\alpha = 0.05 $。\n- 将所有浮点输出四舍五入到 $ 6 $ 位小数。\n- 最终程序必须生成单行输出，其中包含所有案例的结果，格式为方括号括起来的逗号分隔的列表之列表，例如 $ [r_1, r_2, r_3, r_4] $，其中每个 $ r_k $ 是上文描述的按指定顺序排列的 $ 6 $ 元组列表。",
            "solution": "该问题要求在简单线性回归中分析影响点。该分析将从第一性原理出发，基于普通最小二乘 (OLS) 框架进行。我们将首先建立理论基础，然后概述计算步骤。\n\n简单线性回归模型设定如下：\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\quad \\text{for } i = 1, \\dots, n $$\n其中 $y_i$ 是响应变量，$x_i$ 是预测变量，$\\beta_0$ 是截距，$\\beta_1$ 是斜率，$\\varepsilon_i$ 是独立同分布的误差项，其均值 $E[\\varepsilon_i] = 0$，方差恒为 $\\text{Var}(\\varepsilon_i) = \\sigma^2$。\n\n用矩阵表示法，该模型可表达为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中：\n$$\n\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}, \\quad\n\\boldsymbol{\\varepsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}\n$$\n设计矩阵 $\\mathbf{X}$ 有 $n$ 行和 $p=2$ 列。\n\nOLS 估计量 $\\hat{\\boldsymbol{\\beta}}$ 是通过最小化残差平方和 (SSE) $\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ 得到的。对 $\\boldsymbol{\\beta}$ 求导并令其为零，可得到正规方程：\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y} $$\n假设 $\\mathbf{X}^T\\mathbf{X}$ 可逆，则 $\\boldsymbol{\\beta}$ 的 OLS 估计量为：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n\n由此，我们定义几个关键量。拟合值为 $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbfX^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$。矩阵 $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ 是一个投影矩阵，被称为帽子矩阵，因为它为 $\\mathbf{y}$“戴上了帽子”。\n第 $i$ 个观测值的杠杆值 $h_i$ 是帽子矩阵的第 $i$ 个对角元素 $H_{ii}$。它衡量了观测响应 $y_i$ 对其自身拟合值 $\\hat{y}_i$ 的影响，即 $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_i$。残差向量为 $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}$。\n\n误差方差 $\\sigma^2$ 的一个无偏估计量是均方误差 (MSE)：\n$$ s^2 = \\text{MSE} = \\frac{\\mathbf{e}^T\\mathbf{e}}{n-p} $$\n其中分母 $n-p$ 代表残差的自由度。对于此问题，$p=2$。\n\n为识别影响点，我们使用库克距离 $D_i$。它衡量了删除观测值 $i$ 对整个估计系数向量的影响。它可以使用来自完整模型拟合的量进行高效计算：\n$$ D_i = \\frac{e_i^2}{p \\cdot \\text{MSE}} \\left[ \\frac{h_i}{(1-h_i)^2} \\right] $$\n较大的 $D_i$ 值表示观测值 $i$ 的影响力更大。具有最大库克距离的观测值被视为“最具影响力的”观测值。\n\n对于斜率系数 $\\beta_1$ 的统计推断，我们使用其抽样分布。$\\hat{\\boldsymbol{\\beta}}$ 的方差-协方差矩阵为 $\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$。我们通过用 $s^2$ 替代 $\\sigma^2$ 来估计该矩阵。$\\hat{\\beta}_1$ 的估计方差是 $s^2(\\mathbf{X}^T\\mathbf{X})^{-1}$ 的第二行第二列的元素（使用基于1的索引），我们使用基于0的索引将其表示为 $s^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}$。$\\hat{\\beta}_1$ 的标准误是该值的平方根：\n$$ \\text{SE}(\\hat{\\beta}_1) = \\sqrt{s^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}} = s \\sqrt{[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}} $$\n枢轴量 $\\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SE}(\\hat{\\beta}_1)}$ 服从自由度为 $n-p$ 的学生t分布 (Student's $t$-distribution)。$\\beta_1$ 的双侧 $100(1-\\alpha)\\%$ 置信区间构造如下：\n$$ \\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-p} \\cdot \\text{SE}(\\hat{\\beta}_1) $$\n其中 $t_{1-\\alpha/2, n-p}$ 是来自 t 分布的上侧临界值。对于 $95\\%$ 置信区间，$\\alpha=0.05$。此区间的总宽度为 $W = 2 \\cdot t_{0.975, n-p} \\cdot \\text{SE}(\\hat{\\beta}_1)$。\n\n每个数据集的处理流程如下：\n1.  对于完整数据集 $(x_i, y_i)_{i=1}^n$，构建设计矩阵 $\\mathbf{X}$ 并为所有 $i$ 计算 $\\hat{\\boldsymbol{\\beta}}$、$\\mathbf{H}$、$\\mathbf{e}$、$s^2$ 和 $D_i$。\n2.  识别对应于最大库克距离 $\\max(D_i)$ 的索引 $i^*$。记录 $h_{i^*}$、 $|e_{i^*}|$ 和 $D_{i^*}$。\n3.  计算标准误 $\\text{SE}(\\hat{\\beta}_1)$ 和 t 临界值 $t_{0.975, n-2}$，以求出 $\\beta_1$ 的 $95\\%$ 置信区间的宽度，记为 `width_before`。\n4.  移除影响点 $(x_{i^*}, y_{i^*})$，形成一个大小为 $n-1$ 的新数据集。\n5.  在缩减后的数据集上重新拟合回归模型。这包括重新计算设计矩阵 $\\mathbf{X}'$、系数估计值 $\\hat{\\boldsymbol{\\beta}}'$、残差方差 $(s')^2$、标准误 $\\text{SE}(\\hat{\\beta}_1')$ 和新的 t 临界值 $t_{0.975, n-1-2}$。\n6.  计算新的 $95\\%$ 置信区间的宽度 `width_after`。\n7.  比较宽度，如果 `width_after` 小于 `width_before`，则将标志 `narrower` 设置为 $1$，否则设置为 $0$。\n8.  返回六个所需的值，四舍五入到六位小数。\n\n对所提供的四个测试案例中的每一个都执行此完整过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20], dtype=np.float64),\n         np.array([1.0, 3.0, 5.1, 7.0, 9.2, 11.1, 13.0, 15.1, 17.2, 19.0, 21.1, 10.0], dtype=np.float64)),\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20], dtype=np.float64),\n         np.array([1.2, 3.1, 5.0, 7.1, 9.0, 11.2, 13.2, 15.1, 17.0, 19.1, 40.9], dtype=np.float64)),\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64),\n         np.array([1.0, 3.1, 5.0, 7.2, 9.0, 50.0, 13.1, 15.0, 17.2, 19.1, 21.0], dtype=np.float64)),\n        (np.array([0, 1, 2, 10, 11], dtype=np.float64),\n         np.array([1.0, 3.0, 5.2, 15.0, 23.1], dtype=np.float64)),\n    ]\n\n    results = []\n    for x, y in test_cases:\n        result = analyze_influence(x, y)\n        results.append(result)\n    \n    # Format the final output string\n    output_str = \"[\" + ','.join([f\"[{','.join(map(str, r))}]\" for r in results]) + \"]\"\n    print(output_str)\n\ndef get_regression_stats(x, y, alpha=0.05):\n    \"\"\"\n    Performs simple linear regression and returns key statistics.\n    \"\"\"\n    n = len(x)\n    p = 2  # Number of parameters: intercept and slope\n\n    # Construct the design matrix X\n    X = np.c_[np.ones(n), x]\n\n    # Calculate beta_hat using normal equations: (X'X)b = X'y\n    # This is more numerically stable than computing the inverse directly for beta.\n    try:\n        XTX = X.T @ X\n        XTy = X.T @ y\n        beta_hat = np.linalg.solve(XTX, XTy)\n    except np.linalg.LinAlgError:\n        # Handle cases of singular matrix, though not expected with given data\n        return None\n\n    # Calculate hat matrix, residuals, and MSE\n    XTX_inv = np.linalg.inv(XTX)\n    H = X @ XTX_inv @ X.T\n    leverages = np.diag(H)\n    residuals = y - X @ beta_hat\n    mse = (residuals.T @ residuals) / (n - p)\n    \n    # Calculate Cook's distance\n    cooks_d = (residuals**2 / (p * mse)) * (leverages / (1 - leverages)**2)\n    \n    # Calculate confidence interval width for the slope (beta_1)\n    var_beta_hat = mse * XTX_inv\n    se_beta1 = np.sqrt(var_beta_hat[1, 1])\n    t_critical = t.ppf(1 - alpha/2, n - p)\n    ci_width = 2 * t_critical * se_beta1\n    \n    return {\n        'beta_hat': beta_hat,\n        'leverages': leverages,\n        'residuals': residuals,\n        'mse': mse,\n        'cooks_d': cooks_d,\n        'ci_width_beta1': ci_width\n    }\n\ndef analyze_influence(x, y, alpha=0.05):\n    \"\"\"\n    Analyzes the influence of the most influential point on B1's CI.\n    \"\"\"\n    # 1. Fit model on full data\n    full_fit_stats = get_regression_stats(x, y, alpha)\n    if full_fit_stats is None:\n        return [0.0] * 6 # Should not happen\n\n    width_before = full_fit_stats['ci_width_beta1']\n    \n    # 2. Identify most influential observation\n    cooks_d = full_fit_stats['cooks_d']\n    influential_idx = np.argmax(cooks_d)\n    \n    # 3. Record its stats\n    h_i = full_fit_stats['leverages'][influential_idx]\n    abs_e_i = np.abs(full_fit_stats['residuals'][influential_idx])\n    d_i = cooks_d[influential_idx]\n\n    # 4. Remove the influential observation and refit\n    x_reduced = np.delete(x, influential_idx)\n    y_reduced = np.delete(y, influential_idx)\n    \n    reduced_fit_stats = get_regression_stats(x_reduced, y_reduced, alpha)\n    if reduced_fit_stats is None:\n        width_after = 0.0 # Should not happen\n    else:\n        width_after = reduced_fit_stats['ci_width_beta1']\n    \n    # 5. Compare CI widths\n    narrower = 1 if width_after  width_before else 0\n\n    # 6. Format and return results\n    return [\n        round(width_before, 6),\n        round(width_after, 6),\n        narrower,\n        round(h_i, 6),\n        round(abs_e_i, 6),\n        round(d_i, 6)\n    ]\n\n# Execute the main function\nsolve()\n\n```"
        }
    ]
}