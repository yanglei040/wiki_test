{
    "hands_on_practices": [
        {
            "introduction": "One of the most critical lessons in statistical learning is understanding the limits of a model. While Ordinary Least Squares (OLS) can find the best linear fit for your training data, its predictive power can break down dramatically when you venture beyond the domain of that data—a practice known as extrapolation. This exercise  will guide you through scenarios that highlight the dangers of extrapolation, showing how a model with a low training error can produce wildly inaccurate predictions on new, unseen data.",
            "id": "3138831",
            "problem": "Consider a one-dimensional supervised learning setup where the input is a scalar $x$ and the response is a scalar $y$. You will estimate regression coefficients via Ordinary Least Squares (OLS), defined as the choice of coefficients that minimize the Residual Sum of Squares (RSS). The fundamental base for this problem is the definition of the squared-error loss and the concept of minimizing empirical risk: given training pairs $\\{(x_i, y_i)\\}_{i=1}^n$, define the linear predictor $f(x) = \\beta_0 + \\beta_1 x$ and choose coefficients that minimize the training RSS, which is the sum of squared residuals.\n\nYour program must implement the following tasks grounded in first principles:\n\n1. Generate training data $(x_i, y_i)$ according to specified data-generating processes, where $y_i = f_{\\text{true}}(x_i) + \\varepsilon_i$, with $\\varepsilon_i$ sampled independently from a normal distribution with zero mean and specified standard deviation. The training inputs must lie in a specified interval that does not overlap with the test input interval.\n\n2. Fit a linear model with intercept by minimizing the training RSS to obtain coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_1)$.\n\n3. Compute the training Mean Squared Error (MSE), defined as the mean of squared residuals on the training data, and the test Mean Squared Error (MSE) computed on a noise-free test set drawn from the specified test interval using the true data-generating function $f_{\\text{true}}(x)$ without noise.\n\n4. For each test case, output the ratio $r = \\text{MSE}_{\\text{test}} / \\text{MSE}_{\\text{train}}$ as a float. This ratio quantifies how extrapolation performance compares to in-sample fit. Express each ratio as a decimal rounded to $6$ places.\n\nYou must not use any pre-supplied regression formulas in the problem statement; instead, base your implementation on the principle of minimizing squared error. If the design matrix is rank-deficient, use a principled least-squares solution that attains the minimum training RSS.\n\nDefine the following acronyms on first use:\n- Ordinary Least Squares (OLS): the method that chooses coefficients to minimize the Residual Sum of Squares (RSS).\n- Residual Sum of Squares (RSS): the sum of squared residuals on training data.\n- Mean Squared Error (MSE): the average of squared residuals over a dataset.\n\nTest Suite Specification:\n- Case $1$ (happy path, linear truth and extrapolation is reliable):\n  - Training inputs: $n_{\\text{train}} = 50$ points uniformly spaced on $[-1, 1]$.\n  - Test inputs: $n_{\\text{test}} = 50$ points uniformly spaced on $[2, 3]$.\n  - True function: $f_{\\text{true}}(x) = 2x + 1$.\n  - Noise standard deviation: $\\sigma = 0.05$.\n  - Random seed for noise: seed $0$.\n\n- Case $2$ (nonlinear truth within small training support, extrapolation is poor despite small training RSS):\n  - Training inputs: $n_{\\text{train}} = 50$ points uniformly spaced on $[-0.5, 0.5]$.\n  - Test inputs: $n_{\\text{test}} = 50$ points uniformly spaced on $[2, 3]$.\n  - True function: $f_{\\text{true}}(x) = x^3$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n  - Random seed for noise: seed $1$.\n\n- Case $3$ (boundary case with rank-deficient design, extrapolation becomes effectively constant):\n  - Training inputs: $n_{\\text{train}} = 30$ points all at $x = 0$.\n  - Test inputs: $n_{\\text{test}} = 30$ points uniformly spaced on $[1, 2]$.\n  - True function: $f_{\\text{true}}(x) = 2x + 1$.\n  - Noise standard deviation: $\\sigma = 0.05$.\n  - Random seed for noise: seed $2$.\n\nAlgorithmic Requirements:\n- Construct the design matrix $X$ with a column of ones and a column of training inputs.\n- Minimize the training RSS to obtain $(\\hat{\\beta}_0, \\hat{\\beta}_1)$. If the matrix implied by the normal equations is not invertible, use a Moore-Penrose pseudoinverse solution that minimizes training RSS.\n- Compute $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{test}}$ as specified, where $\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$ for a dataset of size $m$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, specifically the list of the three ratios $[r_1, r_2, r_3]$ rounded to $6$ decimal places, where $r_k$ corresponds to Case $k$ in the order above.",
            "solution": "The problem requires the implementation and evaluation of a linear regression model based on the principle of Ordinary Least Squares (OLS), which is a method that selects regression coefficients to minimize the Residual Sum of Squares (RSS). The RSS is defined as the sum of squared differences between the observed responses and the predictions of the linear model. We will analyze the model's performance by comparing its in-sample fit to its extrapolation capability.\n\nThe model to be fitted is a simple linear regression model of the form $f(x) = \\beta_0 + \\beta_1 x$, where $\\beta_0$ is the intercept and $\\beta_1$ is the slope. Given a set of training data points $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$, the goal is to find the coefficient vector $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$ that minimizes the training RSS:\n$$\n\\text{RSS}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n_{\\text{train}}} (y_i - f(x_i))^2 = \\sum_{i=1}^{n_{\\text{train}}} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n$$\n\nThis problem is most elegantly formulated using linear algebra. We can represent the system of linear equations for all training points in matrix form. Let $\\mathbf{y}$ be the vector of observed responses, $X$ be the design matrix, $\\boldsymbol{\\beta}$ be the vector of coefficients, and $\\boldsymbol{\\varepsilon}$ be the vector of errors. The model is $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$.\n\nThe design matrix $X$ for a simple linear model is constructed by augmenting a column of ones (for the intercept $\\beta_0$) to the vector of input values $\\mathbf{x}_{\\text{train}}$:\n$$\nX = \\begin{pmatrix}\n1  x_1 \\\\\n1  x_2 \\\\\n\\vdots  \\vdots \\\\\n1  x_{n_{\\text{train}}}\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_{n_{\\text{train}}}\n\\end{pmatrix}\n$$\n\nThe RSS can then be written as the squared Euclidean norm of the residual vector $\\mathbf{r} = \\mathbf{y} - X\\boldsymbol{\\beta}$:\n$$\n\\text{RSS}(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - X\\boldsymbol{\\beta}\\|^2_2\n$$\nTo find the coefficients $\\hat{\\boldsymbol{\\beta}}$ that minimize this quantity, we take the gradient with respect to $\\boldsymbol{\\beta}$ and set it to zero, which yields the normal equations:\n$$\n(X^T X) \\hat{\\boldsymbol{\\beta}} = X^T \\mathbf{y}\n$$\nIf the matrix $X^T X$ is invertible, the unique OLS solution is:\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^T X)^{-1} X^T \\mathbf{y}\n$$\nHowever, if $X^T X$ is singular (i.e., not invertible), which occurs when the columns of $X$ are linearly dependent, there are infinitely many solutions for $\\hat{\\boldsymbol{\\beta}}$ that minimize the RSS. This happens in Case $3$ of the problem, where all training inputs $x_i$ are identical, making the second column of $X$ a multiple of the first. In such cases, a unique solution is chosen by finding the one with the minimum Euclidean norm $\\|\\boldsymbol{\\beta}\\|_2$. This solution is given by the Moore-Penrose pseudoinverse, denoted by the `$+$` superscript:\n$$\n\\hat{\\boldsymbol{\\beta}} = X^+ \\mathbf{y}\n$$\nwhere $X^+ = (X^T X)^+ X^T$. This formulation provides the least-squares solution for both invertible and singular cases, and is thus the general approach we will implement.\n\nOnce the coefficients $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$ are estimated, we can make predictions for any given input $x$ using $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$. We then evaluate the model's performance using the Mean Squared Error (MSE), defined as the average of the squared residuals over a dataset of size $m$: $\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$.\n\nWe will compute two MSE values:\n$1$. The training MSE, $\\text{MSE}_{\\text{train}}$, is calculated on the training data $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$.\n$2$. The test MSE, $\\text{MSE}_{\\text{test}}$, is calculated on a noise-free test set. This set consists of inputs $x_{\\text{test}, j}$ and true, noiseless responses $y_{\\text{test}, j} = f_{\\text{true}}(x_{\\text{test}, j})$.\n\nThe final step for each case is to compute the ratio $r = \\text{MSE}_{\\text{test}} / \\text{MSE}_{\\text{train}}$, which quantifies the degradation of performance when extrapolating outside the training data domain.\n\n**Case 1:** Training on $[-1, 1]$, testing on $[2, 3]$. The true function is linear: $f_{\\text{true}}(x) = 2x + 1$. The linear model is correctly specified. We expect the estimated coefficients to be close to $(\\beta_0, \\beta_1) = (1, 2)$. As the model is correct, it should extrapolate well, leading to a ratio $r_1$ close to $1$.\n\n**Case 2:** Training on $[-0.5, 0.5]$, testing on $[2, 3]$. The true function is cubic: $f_{\\text{true}}(x) = x^3$. The linear model is misspecified. While it might provide a reasonable fit within the narrow training interval, its linear projection will drastically diverge from the cubic curve in the extrapolation region. This will result in a very large $\\text{MSE}_{\\text{test}}$ and consequently a large ratio $r_2$.\n\n**Case 3:** Training on $x_i=0$ for all $i$. The design matrix $X$ has a column of ones and a column of zeros, making it rank-deficient. The pseudoinverse solution will correctly estimate $\\hat{\\beta}_0$ as the mean of the training responses and set $\\hat{\\beta}_1 = 0$ (the minimum-norm choice). The resulting model is a constant function, $\\hat{y} = \\hat{\\beta}_0$. When this constant model is used to predict values for the test set on $[1, 2]$ where the true function is $f_{\\text{true}}(x) = 2x + 1$, a large test error will occur. This will produce a large ratio $r_3$.\n\nThe implementation will follow these principles, using `numpy` for numerical computations, including the pseudoinverse for solving the least-squares problem robustly across all cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an Ordinary Least Squares (OLS) regression model\n    for three distinct test cases, computing the ratio of test MSE to training MSE.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Happy path, linear truth and reliable extrapolation\n        {\n            \"n_train\": 50,\n            \"train_x_spec\": {\"type\": \"uniform\", \"range\": [-1, 1]},\n            \"n_test\": 50,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [2, 3]},\n            \"f_true\": lambda x: 2 * x + 1,\n            \"noise_sigma\": 0.05,\n            \"seed\": 0,\n        },\n        # Case 2: Nonlinear truth, poor extrapolation\n        {\n            \"n_train\": 50,\n            \"train_x_spec\": {\"type\": \"uniform\", \"range\": [-0.5, 0.5]},\n            \"n_test\": 50,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [2, 3]},\n            \"f_true\": lambda x: x**3,\n            \"noise_sigma\": 0.02,\n            \"seed\": 1,\n        },\n        # Case 3: Rank-deficient design matrix\n        {\n            \"n_train\": 30,\n            \"train_x_spec\": {\"type\": \"constant\", \"value\": 0},\n            \"n_test\": 30,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [1, 2]},\n            \"f_true\": lambda x: 2 * x + 1,\n            \"noise_sigma\": 0.05,\n            \"seed\": 2,\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        #\n        # 1. Generate Training and Test Data\n        #\n        rng = np.random.default_rng(case[\"seed\"])\n\n        # Training data\n        n_train = case[\"n_train\"]\n        train_spec = case[\"train_x_spec\"]\n        if train_spec[\"type\"] == \"uniform\":\n            x_train = np.linspace(train_spec[\"range\"][0], train_spec[\"range\"][1], n_train)\n        elif train_spec[\"type\"] == \"constant\":\n            x_train = np.full(n_train, train_spec[\"value\"])\n        \n        y_true_train = case[\"f_true\"](x_train)\n        noise = rng.normal(0, case[\"noise_sigma\"], n_train)\n        y_train = y_true_train + noise\n\n        # Test data (noise-free)\n        n_test = case[\"n_test\"]\n        test_spec = case[\"test_x_spec\"]\n        if test_spec[\"type\"] == \"uniform\":\n            x_test = np.linspace(test_spec[\"range\"][0], test_spec[\"range\"][1], n_test)\n        \n        y_test = case[\"f_true\"](x_test)\n\n        #\n        # 2. Fit Linear Model using OLS from first principles\n        #\n        # Construct the design matrix X for training data\n        X_train = np.vstack([np.ones(n_train), x_train]).T\n        \n        # Solve for beta_hat using the Moore-Penrose pseudoinverse.\n        # This is equivalent to beta_hat = (X.T @ X)^+ @ X.T @ y\n        # and correctly handles rank-deficient cases.\n        beta_hat = np.linalg.pinv(X_train) @ y_train\n\n        #\n        # 3. Compute Training and Test MSE\n        #\n        \n        # Training MSE\n        # Predictions: y_hat = X @ beta_hat\n        y_hat_train = X_train @ beta_hat\n        \n        # Residuals: y - y_hat\n        residuals_train = y_train - y_hat_train\n        \n        # MSE = mean of squared residuals\n        mse_train = np.mean(residuals_train**2)\n\n        # Test MSE\n        # Construct the design matrix for test data\n        X_test = np.vstack([np.ones(n_test), x_test]).T\n        \n        # Predictions on test data using the fitted model\n        y_hat_test = X_test @ beta_hat\n        \n        # Residuals against the true (noise-free) test values\n        residuals_test = y_test - y_hat_test\n        \n        # MSE\n        mse_test = np.mean(residuals_test**2)\n\n        #\n        # 4. Compute the Ratio\n        #\n        ratio = mse_test / mse_train\n        results.append(round(ratio, 6))\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Linear regression is not limited to numeric predictors. To model the effect of categorical variables (e.g., group A vs. B vs. C), we often use indicator variables, but a naive approach can break the model. This hands-on problem  explores the issue of non-identifiability caused by multicollinearity when encoding categorical data, forcing you to derive a solution from first principles and understand how different coding schemes can lead to equivalent, interpretable models.",
            "id": "3138898",
            "problem": "A single categorical predictor with three levels is used to explain a real-valued response. Consider the following data with categorical levels $\\text{A}$, $\\text{B}$, and $\\text{C}$:\n- Group $\\text{A}$: responses $y$ equal to $2$ and $4$.\n- Group $\\text{B}$: responses $y$ equal to $3$, $5$, and $7$.\n- Group $\\text{C}$: response $y$ equal to $10$.\n\nWe attempt a linear model with an intercept and one-hot encoding that includes all three level indicators:\n$$\ny_{i} = \\beta_{0} + \\beta_{\\text{A}} \\,\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\beta_{\\text{B}} \\,\\mathbf{1}\\{g_{i}=\\text{B}\\} + \\beta_{\\text{C}} \\,\\mathbf{1}\\{g_{i}=\\text{C}\\} + \\varepsilon_{i},\n$$\nwhere $g_{i}$ denotes the group of observation $i$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nStarting from the definition of ordinary least squares (OLS), which chooses coefficients by minimizing the sum of squared residuals, explain why the coefficients in this fully one-hot encoded model are not uniquely identifiable. Then, impose the identifiability constraint\n$$\n\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0\n$$\nand, working from first principles, derive the constrained least squares estimates in closed form. Compute their numerical values for the given data.\n\nFinally, briefly explain how adopting reference coding by dropping level $\\text{C}$ leads to an equivalent fitted model and what the resulting coefficients represent, without using any shortcut formulas.\n\nReport your final answer as the vector of constrained estimates $(\\beta_{0}, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}})$, expressed exactly. No rounding is required.",
            "solution": "The problem requires an analysis of a linear model with a three-level categorical predictor, first by explaining the non-identifiability of a fully one-hot encoded model, and then by deriving and computing constrained least squares estimates. Finally, an equivalent reference-coded model must be explained.\n\nFirst, we perform the problem validation.\n\n### Step 1: Extract Givens\n- Data:\n    - Group $\\text{A}$: responses $y$ are $2$ and $4$. The number of observations is $n_{\\text{A}}=2$.\n    - Group $\\text{B}$: responses $y$ are $3$, $5$, and $7$. The number of observations is $n_{\\text{B}}=3$.\n    - Group $\\text{C}$: response $y$ is $10$. The number of observations is $n_{\\text{C}}=1$.\n- Model (Full One-Hot Encoding):\n$$\ny_{i} = \\beta_{0} + \\beta_{\\text{A}} \\,\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\beta_{\\text{B}} \\,\\mathbf{1}\\{g_{i}=\\text{B}\\} + \\beta_{\\text{C}} \\,\\mathbf{1}\\{g_{i}=\\text{C}\\} + \\varepsilon_{i}\n$$\n- Identifiability Constraint:\n$$\n\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0\n$$\n- Tasks:\n    1. Explain why the coefficients in the fully one-hot encoded model are not uniquely identifiable.\n    2. Derive the constrained least squares estimates from first principles.\n    3. Compute the numerical values of the constrained estimates.\n    4. Explain the equivalent reference-coded model where level $\\text{C}$ is dropped.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the theory of linear models and ordinary least squares (OLS), which are core concepts in statistics. The issue of non-identifiability due to multicollinearity in models with categorical predictors is a standard topic. The use of a sum-to-zero constraint to achieve identifiability is a valid and common technique. The problem is well-posed, providing all necessary data and a clear set of objectives. The language is precise and objective. There are no scientific or factual unsoundness, no missing information, and the setup is not contradictory or unrealistic. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\n#### Non-identifiability of the Full Model\n\nThe specified linear model can be expressed in matrix form as $Y = X\\beta + \\varepsilon$. The vector of parameters is $\\beta = [\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}}]^T$. The design matrix $X$ has columns corresponding to the intercept and the three indicator variables for groups $\\text{A}$, $\\text{B}$, and $\\text{C}$. Let these columns be $X_0, X_{\\text{A}}, X_{\\text{B}}, X_{\\text{C}}$.\n- $X_0$ is a column of all ones, representing the intercept $\\beta_0$.\n- $X_{\\text{A}}$ is a column with a $1$ for observations in group $\\text{A}$ and $0$ otherwise.\n- $X_{\\text{B}}$ is a column with a $1$ for observations in group $\\text{B}$ and $0$ otherwise.\n- $X_{\\text{C}}$ is a column with a $1$ for observations in group $\\text{C}$ and $0$ otherwise.\n\nFor any observation $i$, the corresponding indicator variables satisfy the property $\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\mathbf{1}\\{g_{i}=\\text{B}\\} + \\mathbf{1}\\{g_{i}=\\text{C}\\} = 1$. This means that the sum of the columns corresponding to the categorical levels is equal to the intercept column: $X_{\\text{A}} + X_{\\text{B}} + X_{\\text{C}} = X_0$. This indicates perfect linear dependency, or multicollinearity, among the columns of the design matrix $X$.\n\nThe OLS estimate for $\\beta$ is given by $\\hat{\\beta} = (X^T X)^{-1} X^T Y$. Because the columns of $X$ are linearly dependent, the matrix $X^T X$ is singular (not invertible). Consequently, its inverse $(X^T X)^{-1}$ does not exist, and there is no unique solution for $\\hat{\\beta}$.\n\nAlternatively, consider a set of coefficients $(\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}})$ that minimizes the sum of squared residuals. For any arbitrary constant $c$, consider a new set of coefficients $(\\beta_0 + c, \\beta_{\\text{A}} - c, \\beta_{\\text{B}} - c, \\beta_{\\text{C}} - c)$. The predicted value for an observation in group $\\text{A}$ is $(\\beta_0 + c) + (\\beta_{\\text{A}} - c) = \\beta_0 + \\beta_{\\text{A}}$, which is unchanged. Similarly, predictions for groups $\\text{B}$ and $\\text{C}$ are also unchanged. Since the predicted values are identical for any choice of $c$, the sum of squared residuals is also identical. This implies there are infinitely many solutions, and thus the coefficients are not uniquely identifiable.\n\n#### Constrained Least Squares Estimation\n\nTo find the unique estimates under the given constraint, we minimize the Residual Sum of Squares (RSS) subject to the constraint. The objective function is the RSS:\n$$\n\\text{RSS} = \\sum_{i \\in \\text{A}} (y_i - (\\beta_0 + \\beta_{\\text{A}}))^2 + \\sum_{i \\in \\text{B}} (y_i - (\\beta_0 + \\beta_{\\text{B}}))^2 + \\sum_{i \\in \\text{C}} (y_i - (\\beta_0 + \\beta_{\\text{C}}))^2\n$$\nThe constraint is $\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0$.\n\nWe use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}}, \\lambda) = \\text{RSS} - \\lambda(\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}})\n$$\nWe find the partial derivatives with respect to each parameter and set them to zero. Let $\\bar{y}_{\\text{A}}, \\bar{y}_{\\text{B}}, \\bar{y}_{\\text{C}}$ be the sample means of the responses in each group, and $n_{\\text{A}}, n_{\\text{B}}, n_{\\text{C}}$ be the sample sizes.\n\n1.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_0} = -2 \\sum_{i \\in \\text{A}}(y_i - \\beta_0 - \\beta_{\\text{A}}) - 2 \\sum_{i \\in \\text{B}}(y_i - \\beta_0 - \\beta_{\\text{B}}) - 2 \\sum_{i \\in \\text{C}}(y_i - \\beta_0 - \\beta_{\\text{C}}) = 0$\n    This simplifies to $n_{\\text{A}}(\\bar{y}_{\\text{A}} - \\beta_0 - \\beta_{\\text{A}}) + n_{\\text{B}}(\\bar{y}_{\\text{B}} - \\beta_0 - \\beta_{\\text{B}}) + n_{\\text{C}}(\\bar{y}_{\\text{C}} - \\beta_0 - \\beta_{\\text{C}}) = 0$.\n\n2.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{A}}} = -2 \\sum_{i \\in \\text{A}}(y_i - \\beta_0 - \\beta_{\\text{A}}) - \\lambda = 0 \\implies n_{\\text{A}}(\\bar{y}_{\\text{A}} - \\beta_0 - \\beta_{\\text{A}}) = \\lambda/2$.\n\n3.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{B}}} = -2 \\sum_{i \\in \\text{B}}(y_i - \\beta_0 - \\beta_{\\text{B}}) - \\lambda = 0 \\implies n_{\\text{B}}(\\bar{y}_{\\text{B}} - \\beta_0 - \\beta_{\\text{B}}) = \\lambda/2$.\n\n4.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{C}}} = -2 \\sum_{i \\in \\text{C}}(y_i - \\beta_0 - \\beta_{\\text{C}}) - \\lambda = 0 \\implies n_{\\text{C}}(\\bar{y}_{\\text{C}} - \\beta_0 - \\beta_{\\text{C}}) = \\lambda/2$.\n\n5.  $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}}) = 0$.\n\nSubstituting the results from (2), (3), and (4) into equation (1) gives:\n$\\lambda/2 + \\lambda/2 + \\lambda/2 = 0 \\implies 3\\lambda/2 = 0 \\implies \\lambda = 0$.\n\nWith $\\lambda=0$, equations (2)-(4) become (since $n_k  0$):\n- $\\bar{y}_{\\text{A}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{A}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}}$\n- $\\bar{y}_{\\text{B}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{B}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}}$\n- $\\bar{y}_{\\text{C}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{C}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}}$\n\nThese equations show that the fitted value for each group is simply its sample mean. We can express the $\\hat{\\beta}_k$ terms as a function of $\\hat{\\beta}_0$:\n- $\\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\beta}_0$\n- $\\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\beta}_0$\n- $\\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}} - \\hat{\\beta}_0$\n\nSubstituting these into the constraint equation (5):\n$(\\bar{y}_{\\text{A}} - \\hat{\\beta}_0) + (\\bar{y}_{\\text{B}} - \\hat{\\beta}_0) + (\\bar{y}_{\\text{C}} - \\hat{\\beta}_0) = 0$\n$\\bar{y}_{\\text{A}} + \\bar{y}_{\\text{B}} + \\bar{y}_{\\text{C}} - 3\\hat{\\beta}_0 = 0$\nSolving for $\\hat{\\beta}_0$:\n$$\n\\hat{\\beta}_0 = \\frac{\\bar{y}_{\\text{A}} + \\bar{y}_{\\text{B}} + \\bar{y}_{\\text{C}}}{3}\n$$\nThe remaining coefficients are then found as $\\hat{\\beta}_k = \\bar{y}_k - \\hat{\\beta}_0$. $\\hat{\\beta}_0$ is the unweighted average of the group means, and $\\hat{\\beta}_k$ is the deviation of the mean of group $k$ from this average.\n\n#### Numerical Computation\nFirst, we compute the sample means from the data:\n- $\\bar{y}_{\\text{A}} = \\frac{2+4}{2} = 3$\n- $\\bar{y}_{\\text{B}} = \\frac{3+5+7}{3} = \\frac{15}{3} = 5$\n- $\\bar{y}_{\\text{C}} = \\frac{10}{1} = 10$\n\nNow, we compute the coefficients:\n- $\\hat{\\beta}_0 = \\frac{3 + 5 + 10}{3} = \\frac{18}{3} = 6$\n- $\\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\beta}_0 = 3 - 6 = -3$\n- $\\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\beta}_0 = 5 - 6 = -1$\n- $\\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}} - \\hat{\\beta}_0 = 10 - 6 = 4$\n\nThe vector of constrained estimates is $(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{A}}, \\hat{\\beta}_{\\text{B}}, \\hat{\\beta}_{\\text{C}}) = (6, -3, -1, 4)$.\nWe can verify the constraint: $\\hat{\\beta}_{\\text{A}} + \\hat{\\beta}_{\\text{B}} + \\hat{\\beta}_{\\text{C}} = -3 + (-1) + 4 = 0$.\n\n#### Equivalent Reference-Coded Model\nIf we adopt reference coding by dropping level $\\text{C}$, the model becomes:\n$$\ny_i = \\gamma_0 + \\gamma_{\\text{A}} \\mathbf{1}\\{g_i=\\text{A}\\} + \\gamma_{\\text{B}} \\mathbf{1}\\{g_i=\\text{B}\\} + \\varepsilon_i\n$$\nHere, group $\\text{C}$ is the reference level. The design matrix for this model has linearly independent columns, so the OLS estimates are unique. The expected values for each group are:\n- $E[y|g=\\text{A}] = \\gamma_0 + \\gamma_{\\text{A}}$\n- $E[y|g=\\text{B}] = \\gamma_0 + \\gamma_{\\text{B}}$\n- $E[y|g=\\text{C}] = \\gamma_0$\n\nThe OLS estimates of these expected values are the respective group sample means. Thus, we have:\n- $\\hat{\\gamma}_0 = \\bar{y}_{\\text{C}} = 10$\n- $\\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{A}} = \\bar{y}_{\\text{A}} \\implies \\hat{\\gamma}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\gamma}_0 = \\bar{y}_{\\text{A}} - \\bar{y}_{\\text{C}} = 3 - 10 = -7$\n- $\\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{B}} = \\bar{y}_{\\text{B}} \\implies \\hat{\\gamma}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\gamma}_0 = \\bar{y}_{\\text{B}} - \\bar{y}_{\\text{C}} = 5 - 10 = -5$\n\nThe interpretation is that $\\hat{\\gamma}_0$ is the estimated mean of the reference group ($\\text{C}$), while $\\hat{\\gamma}_{\\text{A}}$ and $\\hat{\\gamma}_{\\text{B}}$ are the estimated differences in means between groups $\\text{A}$ and $\\text{B}$ and the reference group $\\text{C}$, respectively.\n\nThe fitted values for this model are:\n- For group $\\text{A}$: $\\hat{y}_{\\text{A}} = \\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{A}} = 10 + (-7) = 3 = \\bar{y}_{\\text{A}}$\n- For group $\\text{B}$: $\\hat{y}_{\\text{B}} = \\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{B}} = 10 + (-5) = 5 = \\bar{y}_{\\text{B}}$\n- For group $\\text{C}$: $\\hat{y}_{\\text{C}} = \\hat{\\gamma}_0 = 10 = \\bar{y}_{\\text{C}}$\n\nThe fitted values are identical to those from the constrained model (e.g., for Group A, $\\hat{\\beta}_0 + \\hat{\\beta}_{\\text{A}} = 6 - 3 = 3$). Since both models produce the same fitted values for all observations, they are equivalent in terms of predictive\npower and goodness of fit (e.g., they have the same RSS). The parameterizations are different, but they describe the same underlying fit to the data.",
            "answer": "$$\\boxed{\\begin{pmatrix} 6  -3  -1  4 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Not all data points are created equal; some have a much larger impact on the final regression model than others. To become a responsible data analyst, you must learn to identify these influential points. This practice  delves into two key diagnostic measures—studentized residuals and Cook's distance—to help you distinguish between outliers (points with large errors) and high-leverage points (points that disproportionately pull the regression line toward them).",
            "id": "3138894",
            "problem": "You are to write a complete program that, for a set of prescribed linear regression datasets, fits the ordinary least squares model with an intercept, computes externally studentized residuals and Cook’s distance, and then evaluates specific logical conditions that compare these two influence measures. The program must aggregate the results for all cases into a single output line as specified below.\n\nStart from the following fundamental bases:\n\n- The least squares estimator solves the optimization problem that minimizes the residual sum of squares: given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with full column rank and a response vector $y \\in \\mathbb{R}^{n}$, the least squares estimate $\\hat{\\beta} \\in \\mathbb{R}^{p}$ satisfies the normal equations $X^{\\top}X \\hat{\\beta} = X^{\\top} y$.\n- The fitted values are $\\hat{y} = X \\hat{\\beta}$ and the residuals are $e = y - \\hat{y}$.\n- The hat matrix is $H = X (X^{\\top} X)^{-1} X^{\\top}$ with diagonal entries $h_{ii}$ (the leverages).\n- The residual variance estimate is $s^{2} = \\frac{1}{n - p} \\sum_{i=1}^{n} e_{i}^{2}$ provided $n  p$.\n\nDefinitions to implement:\n\n- The externally studentized residual for observation $i$ is $t_{i} = \\dfrac{e_{i}}{s_{(i)} \\sqrt{1 - h_{ii}}}$, where $s_{(i)}^{2}$ is the residual variance estimate computed from the model refitted after deleting observation $i$. An equivalent computation avoiding explicit refits uses the identity $s_{(i)}^{2} = \\dfrac{(n - p) s^{2} - \\dfrac{e_{i}^{2}}{1 - h_{ii}}}{n - p - 1}$ when $n - p - 1  0$.\n- Cook’s distance for observation $i$ is defined as $D_{i} = \\dfrac{(\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)})}{p\\, s^{2}}$, where $\\hat{\\beta}_{(i)}$ is the least squares estimate computed from the dataset with observation $i$ removed. You must compute $D_{i}$ using this definition.\n\nImplementation details:\n\n- For each dataset, construct $X$ by concatenating a column of ones (the intercept) and the given one-dimensional feature vector $x$.\n- Compute $\\hat{\\beta}$ by solving $X^{\\top}X \\hat{\\beta} = X^{\\top} y$.\n- Compute $\\hat{y}$, $e$, $H$, $h_{ii}$, $s^{2}$, the externally studentized residuals $t_{i}$, and Cook’s distances $D_{i}$ as defined above.\n\nTest suite:\n\nYour program must use exactly the following three datasets and conditions. Indices are zero-based.\n\n- Case $1$ (single vertical outlier; expect both measures to flag the same point):\n  - $x = [\\,0,1,2,3,4,5,6,7,8,9\\,]$,\n  - $y = [\\,1.2,2.9,5.1,7.0,9.2,80.0,13.1,15.0,18.2,20.1\\,]$,\n  - designated index $i^{\\star} = 5$,\n  - thresholds: $T_{r} = 2.0$, $T_{D} = \\dfrac{4}{n}$ with $n$ equal to the dataset size,\n  - required boolean for this case: $\\big(|t_{i^{\\star}}|  T_{r}\\big) \\wedge \\big(D_{i^{\\star}}  T_{D}\\big)$.\n\n- Case $2$ (high leverage with small residual; show influence despite a small residual-based signal):\n  - $x = [\\,0,1,2,3,4,5,6,50\\,]$,\n  - $y = [\\,2.5,1.6,2.9,2.7,3.6,3.4,4.0,17.1\\,]$,\n  - designated index $i^{\\star} = 7$,\n  - thresholds: $T_{r} = 2.0$, $T_{D} = \\dfrac{4}{n}$ with $n$ equal to the dataset size,\n  - required boolean for this case: $\\big(|t_{i^{\\star}}| \\le T_{r}\\big) \\wedge \\big(D_{i^{\\star}}  T_{D}\\big)$.\n\n- Case $3$ (no outlier; all points behave regularly):\n  - $x = [\\,0,1,2,3,4,5,6,7,8,9\\,]$,\n  - $y = [\\,3.1,2.3,2.2,1.4,1.1,0.3,0.2,-0.5,-0.9,-1.6\\,]$,\n  - thresholds: $T_{r} = 2.0$, $T_{D} = \\dfrac{4}{n}$ with $n$ equal to the dataset size,\n  - required boolean for this case: all observations simultaneously satisfy $|t_{i}| \\le T_{r}$ and $D_{i} \\le T_{D}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, for example $[\\,\\text{True},\\text{False},\\text{True}\\,]$.\n- No other text should be printed.\n- There are no physical units or angles involved, so no unit conversion is required.",
            "solution": "The problem requires the implementation of ordinary least squares (OLS) regression and the computation of two key influence diagnostics—externally studentized residuals and Cook’s distance—for three specified datasets. The validity of certain logical conditions based on these diagnostics must then be evaluated. The process strictly adheres to the provided mathematical definitions.\n\nFirst, we establish the linear regression model. For a given dataset consisting of a one-dimensional feature vector $x \\in \\mathbb{R}^{n}$ and a response vector $y \\in \\mathbb{R}^{n}$, we fit the model $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$. In matrix form, this is $y = X \\beta + \\epsilon$, where $\\beta = [\\beta_0, \\beta_1]^{\\top}$ is the vector of parameters. The design matrix $X \\in \\mathbb{R}^{n \\times p}$ is constructed by prepending a column of ones to the feature vector $x$ to account for the intercept term $\\beta_0$. Thus, $X = [\\mathbf{1} \\mid x]$ and the number of parameters is $p=2$.\n\nThe OLS estimate $\\hat{\\beta}$ is found by minimizing the residual sum of squares, $RSS(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta)$. The solution to this minimization problem is given by the normal equations:\n$$\n(X^{\\top}X) \\hat{\\beta} = X^{\\top} y\n$$\nAssuming $X$ has full column rank (which is true for all given datasets), we can solve for $\\hat{\\beta}$ as $\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top}y$.\n\nFrom $\\hat{\\beta}$, we compute several fundamental quantities:\n- The vector of fitted values: $\\hat{y} = X \\hat{\\beta}$.\n- The vector of residuals: $e = y - \\hat{y}$.\n- The hat matrix, which maps responses to fitted values ($\\hat{y} = Hy$): $H = X(X^{\\top}X)^{-1}X^{\\top}$. The diagonal elements of $H$, denoted $h_{ii}$, are the leverages of each observation. They measure the potential for an observation to influence its own fitted value.\n- An unbiased estimate of the error variance $\\sigma^2$ is the residual mean square or residual variance estimate, $s^2$:\n$$\ns^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} e_i^2 = \\frac{e^{\\top}e}{n-p}\n$$\nThis is valid provided $n  p$.\n\nWith these quantities, we can compute the required influence diagnostics.\n\nThe externally studentized residual, $t_i$, for observation $i$ is defined as:\n$$\nt_{i} = \\frac{e_{i}}{s_{(i)} \\sqrt{1 - h_{ii}}}\n$$\nHere, $s_{(i)}$ is the standard error of the regression computed from a dataset where observation $i$ has been deleted. This diagnostic measures the extent to which the $i$-th observation is an outlier by comparing its residual to the variability of the model fitted without that observation. Explicitly refitting the model $n$ times would be computationally expensive. Instead, we use the provided identity, which is valid for $n-p-1  0$:\n$$\ns_{(i)}^{2} = \\frac{(n-p)s^2 - \\frac{e_i^2}{1-h_{ii}}}{n-p-1}\n$$\n\nCook's distance, $D_i$, measures the effect of deleting observation $i$ on the entire vector of estimated coefficients $\\hat{\\beta}$. The problem mandates using the definitional formula:\n$$\nD_{i} = \\frac{(\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)})}{p s^{2}}\n$$\nHere, $\\hat{\\beta}_{(i)}$ is the coefficient vector estimated from the dataset with the $i$-th observation removed. To comply with this requirement, the computational procedure involves a loop running from $i=1$ to $n$. In each iteration $i$, the $i$-th row from both $X$ and $y$ is removed to form $X_{(i)}$ and $y_{(i)}$, and the normal equations $(X_{(i)}^{\\top}X_{(i)}) \\hat{\\beta}_{(i)} = X_{(i)}^{\\top}y_{(i)}$ are solved to find $\\hat{\\beta}_{(i)}$. This value is then used in the formula for $D_i$.\n\nThe overall algorithm for each test case is as follows:\n1.  Receive the vectors $x$ and $y$. Set $n$ to the number of observations and $p=2$.\n2.  Construct the design matrix $X$.\n3.  Solve the normal equations for the full-data coefficient estimate $\\hat{\\beta}$.\n4.  Compute residuals $e$, hat matrix diagonals $h_{ii}$, and the residual variance $s^2$.\n5.  For all $i \\in \\{1, \\dots, n\\}$, calculate the externally studentized residuals $t_i$ using the formula for $s_{(i)}^2$.\n6.  For all $i \\in \\{1, \\dots, n\\}$, calculate Cook's distance $D_i$ by explicitly refitting the model on the leave-one-out dataset to find $\\hat{\\beta}_{(i)}$.\n7.  Evaluate the boolean condition specified for the case using the computed arrays of $t_i$ and $D_i$ and the given thresholds.\n\nThis procedure is applied to each of the three test cases, and the resulting boolean values are aggregated into a final list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the linear regression influence diagnostics problem for three test cases.\n    \"\"\"\n\n    def compute_diagnostics(x: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Computes regression diagnostics for a given dataset (x, y).\n\n        Args:\n            x: 1D numpy array of feature values.\n            y: 1D numpy array of response values.\n\n        Returns:\n            A tuple containing:\n            - t_residuals: Externally studentized residuals.\n            - cooks_distances: Cook's distances.\n        \"\"\"\n        n = len(x)\n        p = 2  # Number of parameters: intercept (beta_0) and slope (beta_1)\n\n        # Construct the design matrix X with an intercept column.\n        X = np.c_[np.ones(n), x]\n\n        # --- Full model fit ---\n        # Solve the normal equations: (X'X)beta_hat = X'y\n        XTX = X.T @ X\n        XTy = X.T @ y\n        beta_hat = np.linalg.solve(XTX, XTy)\n\n        # Compute residuals and residual variance\n        y_hat = X @ beta_hat\n        e = y - y_hat\n        \n        # Check for n  p\n        if not n  p:\n            raise ValueError(\"n must be greater than p for s^2 to be defined.\")\n        s2 = (e.T @ e) / (n - p)\n\n        # Compute hat matrix H and leverages h_ii\n        XTX_inv = np.linalg.inv(XTX)\n        H = X @ XTX_inv @ X.T\n        h_ii = np.diag(H)\n        \n        # --- Externally Studentized Residuals ---\n        # Check for n - p - 1  0\n        if not n - p - 1  0:\n            raise ValueError(\"n-p-1 must be positive for s_(i)^2 to be defined.\")\n        \n        # An identity for s_(i)^2 is used to avoid refitting n times.\n        # s_(i)^2 = [(n-p)s^2 - e_i^2/(1-h_ii)] / (n-p-1)\n        s2_i = ((n - p) * s2 - e**2 / (1 - h_ii)) / (n - p - 1)\n        t_residuals = e / (np.sqrt(s2_i) * np.sqrt(1 - h_ii))\n\n        # --- Cook's Distances ---\n        # Computed via the definition, requiring a loop and refitting.\n        cooks_distances = np.zeros(n)\n        for i in range(n):\n            # Create leave-one-out data\n            X_i = np.delete(X, i, axis=0)\n            y_i = np.delete(y, i)\n\n            # Solve for beta_hat_(i)\n            beta_hat_i = np.linalg.solve(X_i.T @ X_i, X_i.T @ y_i)\n\n            # Compute Cook's distance D_i\n            beta_diff = beta_hat - beta_hat_i\n            numerator = beta_diff.T @ XTX @ beta_diff\n            cooks_distances[i] = numerator / (p * s2)\n            \n        return t_residuals, cooks_distances\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float),\n            \"y\": np.array([1.2, 2.9, 5.1, 7.0, 9.2, 80.0, 13.1, 15.0, 18.2, 20.1], dtype=float),\n            \"i_star\": 5,\n            \"T_r\": 2.0,\n        },\n        {\n            \"x\": np.array([0, 1, 2, 3, 4, 5, 6, 50], dtype=float),\n            \"y\": np.array([2.5, 1.6, 2.9, 2.7, 3.6, 3.4, 4.0, 17.1], dtype=float),\n            \"i_star\": 7,\n            \"T_r\": 2.0,\n        },\n        {\n            \"x\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float),\n            \"y\": np.array([3.1, 2.3, 2.2, 1.4, 1.1, 0.3, 0.2, -0.5, -0.9, -1.6], dtype=float),\n            \"T_r\": 2.0,\n        }\n    ]\n\n    results = []\n\n    # Case 1\n    case1 = test_cases[0]\n    n1 = len(case1[\"x\"])\n    t1, D1 = compute_diagnostics(case1[\"x\"], case1[\"y\"])\n    T_r1 = case1[\"T_r\"]\n    T_D1 = 4.0 / n1\n    i_star1 = case1[\"i_star\"]\n    result1 = (np.abs(t1[i_star1])  T_r1) and (D1[i_star1]  T_D1)\n    results.append(result1)\n\n    # Case 2\n    case2 = test_cases[1]\n    n2 = len(case2[\"x\"])\n    t2, D2 = compute_diagnostics(case2[\"x\"], case2[\"y\"])\n    T_r2 = case2[\"T_r\"]\n    T_D2 = 4.0 / n2\n    i_star2 = case2[\"i_star\"]\n    result2 = (np.abs(t2[i_star2]) = T_r2) and (D2[i_star2]  T_D2)\n    results.append(result2)\n\n    # Case 3\n    case3 = test_cases[2]\n    n3 = len(case3[\"x\"])\n    t3, D3 = compute_diagnostics(case3[\"x\"], case3[\"y\"])\n    T_r3 = case3[\"T_r\"]\n    T_D3 = 4.0 / n3\n    result3 = np.all((np.abs(t3) = T_r3)  (D3 = T_D3))\n    results.append(result3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}