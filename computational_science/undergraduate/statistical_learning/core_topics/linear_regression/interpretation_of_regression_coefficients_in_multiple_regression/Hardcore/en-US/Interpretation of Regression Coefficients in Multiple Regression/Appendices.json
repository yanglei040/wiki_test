{
    "hands_on_practices": [
        {
            "introduction": "The numerical value of a regression coefficient is fundamentally tied to the units of its predictor. This exercise demonstrates this scaling principle by showing how changing a predictor's units—for example, from centimeters to meters—directly rescales its coefficient. Working through this practice  is a foundational step toward correctly interpreting an effect's magnitude and ensuring that model predictions remain consistent and physically meaningful regardless of the measurement scale.",
            "id": "3132949",
            "problem": "A researcher fits a multiple linear regression by ordinary least squares (OLS) to model systolic blood pressure as a function of height and age. The response is systolic blood pressure $y$ in $\\mathrm{mmHg}$, and the predictors are height $x_{1}$ in centimeters and age $x_{2}$ in years. The fitted model using centimeters is\n$$\ny \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\,x_{1} \\;+\\; \\beta_{2}\\,x_{2} \\;+\\; \\varepsilon,\n$$\nwith estimated coefficients\n$$\n\\hat{\\beta}_{0} \\;=\\; 50,\\quad \\hat{\\beta}_{1} \\;=\\; 0.12,\\quad \\hat{\\beta}_{2} \\;=\\; 0.7.\n$$\nThe researcher then decides to report results using meters for height, defining $z_{1} \\equiv x_{1}/100$, and to write the model as\n$$\ny \\;=\\; \\gamma_{0} \\;+\\; \\gamma_{1}\\,z_{1} \\;+\\; \\gamma_{2}\\,x_{2} \\;+\\; \\varepsilon.\n$$\nTasks:\n- Starting from the definition of the multiple linear regression model and the principle that fitted predictions for the same physical individual must be invariant to a change of measurement units, derive how the coefficients transform when replacing $x_{1}$ by $z_{1} \\equiv x_{1}/100$. Use this to compute the numerical value of $\\hat{\\gamma}_{1}$.\n- For a person with height $x_{1} = 180$ centimeters (that is, $z_{1} = 1.80$ meters) and age $x_{2} = 50$ years, compute the fitted value $\\hat{y}$ using both the centimeter and meter representations, and verify equality.\n\nReport as your final answer only the value of the rescaled slope coefficient $\\hat{\\gamma}_{1}$, with no units. No rounding is required.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A multiple linear regression model for systolic blood pressure `y` (in mmHg) as a function of height `x_1` (in centimeters) and age `x_2` (in years):\n$$y \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\,x_{1} \\;+\\; \\beta_{2}\\,x_{2} \\;+\\; \\varepsilon$$\n- The estimated coefficients for this model, obtained via ordinary least squares (OLS), are:\n$$\n\\hat{\\beta}_{0} \\;=\\; 50,\\quad \\hat{\\beta}_{1} \\;=\\; 0.12,\\quad \\hat{\\beta}_{2} \\;=\\; 0.7\n$$\n- A second model is proposed where height is measured in meters, `z_1`, defined by the transformation `z_{1} \\equiv x_{1}/100`. The model has the form:\n$$y \\;=\\; \\gamma_{0} \\;+\\; \\gamma_{1}\\,z_{1} \\;+\\; \\gamma_{2}\\,x_{2} \\;+\\; \\varepsilon$$\n- The problem requires the derivation of the transformation from coefficients `(\\beta_0, \\beta_1, \\beta_2)` to `(\\gamma_0, \\gamma_1, \\gamma_2)` based on the principle that fitted predictions `\\hat{y}` must be invariant to the change of units for height.\n- A numerical value for `\\hat{\\gamma}_{1}` must be computed.\n- The equality of fitted values must be verified for a specific subject with `x_1 = 180` cm and `x_2 = 50` years.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It deals with a standard, fundamental concept in linear regression: the effect of linear transformations of predictor variables on the regression coefficients. The premises are consistent, the language is precise, and the setup is well-posed, leading to a unique and meaningful solution. The values provided are plausible for a biostatistical context. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\nLet the first fitted model, using height in centimeters (`x_1`), be denoted `M_1`, and the second fitted model, using height in meters (`z_1`), be denoted `M_2`.\n\nThe equation for the predicted systolic blood pressure, `\\hat{y}`, from model `M_1` is:\n$$\n\\hat{y}_{M_1} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2}\n$$\nThe equation for the predicted value from model `M_2` is:\n$$\n\\hat{y}_{M_2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\nThe core principle stated in the problem is that the predicted value for any given individual must be independent of the units used for the predictors. This is a principle of physical invariance. Therefore, for any individual with height `x_1` (in cm) or `z_1` (in m) and age `x_2`, the predicted values must be equal:\n$$\n\\hat{y}_{M_1} = \\hat{y}_{M_2}\n$$\nThis implies:\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\nWe are given the relationship between `x_1` and `z_1`: `z_{1} = x_{1}/100`, which is equivalent to `x_{1} = 100z_{1}`. To relate the coefficients, we substitute the definition of `z_1` into the right-hand side of the equality:\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}\\left(\\frac{x_{1}}{100}\\right) + \\hat{\\gamma}_{2}x_{2}\n$$\nLet us rearrange the right-hand side to group terms by the original predictors `x_1` and `x_2`:\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\left(\\frac{\\hat{\\gamma}_{1}}{100}\\right)x_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\nFor this identity to hold true for all possible values of the predictors `x_1` and `x_2`, the coefficients of the corresponding terms on both sides of the equation must be equal. This is a direct application of the principle of equating coefficients of polynomials.\n\nBy comparing the constant terms (intercepts), we find:\n$$\n\\hat{\\beta}_{0} = \\hat{\\gamma}_{0}\n$$\nBy comparing the coefficients of the `x_1` term, we find:\n$$\n\\hat{\\beta}_{1} = \\frac{\\hat{\\gamma}_{1}}{100}\n$$\nBy comparing the coefficients of the `x_2` term, we find:\n$$\n\\hat{\\beta}_{2} = \\hat{\\gamma}_{2}\n$$\nFrom the second equality, we can solve for `\\hat{\\gamma}_{1}`:\n$$\n\\hat{\\gamma}_{1} = 100 \\cdot \\hat{\\beta}_{1}\n$$\nUsing the given numerical value `\\hat{\\beta}_{1} = 0.12`, we compute `\\hat{\\gamma}_{1}`:\n$$\n\\hat{\\gamma}_{1} = 100 \\cdot 0.12 = 12\n$$\nThe other coefficients for the model in meters are `\\hat{\\gamma}_{0} = \\hat{\\beta}_{0} = 50` and `\\hat{\\gamma}_{2} = \\hat{\\beta}_{2} = 0.7`.\n\n### Verification\nWe now verify that the fitted value `\\hat{y}` is the same for an individual with height `x_1 = 180` cm and age `x_2 = 50` years. The corresponding height in meters is `z_1 = 180 / 100 = 1.80` m.\n\nUsing the first model (centimeters):\n$$\n\\hat{y}_{M_1} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = 50 + (0.12)(180) + (0.7)(50)\n$$\n$$\n\\hat{y}_{M_1} = 50 + 21.6 + 35 = 106.6\n$$\nUsing the second model (meters):\n$$\n\\hat{y}_{M_2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2} = 50 + (12)(1.80) + (0.7)(50)\n$$\n$$\n\\hat{y}_{M_2} = 50 + 21.6 + 35 = 106.6\n$$\nThe predicted values are identical, `\\hat{y}_{M_1} = \\hat{y}_{M_2} = 106.6`, which verifies the correctness of our derived coefficient transformation. The question asks for the numerical value of `\\hat{\\gamma}_{1}`.",
            "answer": "$$\n\\boxed{12}\n$$"
        },
        {
            "introduction": "A core tenet of multiple regression is that a coefficient represents a *partial* effect, which can sometimes be deeply counter-intuitive. In a phenomenon known as suppression, the sign of a coefficient can be the opposite of the variable's simple correlation with the outcome, a statistical surprise often related to Simpson's paradox. This exercise challenges you to derive this sign reversal from the fundamental normal equations , offering a rich geometric and algebraic understanding of what it truly means to \"control for\" a variable.",
            "id": "3132937",
            "problem": "Consider a population in which the pair $\\{x_1, x_2\\}$ of predictors and the response $y$ are jointly distributed with finite second moments. The predictors are standardized so that $\\operatorname{E}[x_1] = \\operatorname{E}[x_2] = 0$ and $\\operatorname{Var}(x_1) = \\operatorname{Var}(x_2) = 1$. Suppose the predictors are positively associated, with $\\operatorname{Cov}(x_1, x_2) = \\rho$ for some $0 < \\rho < 1$, and the response has zero mean, $\\operatorname{E}[y] = 0$. Empirical investigation in this population yields the following covariances: $\\operatorname{Cov}(x_1, y) = 0.2$ and $\\operatorname{Cov}(x_2, y) = 0.8$. \n\nWe fit the multiple linear model $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$ by ordinary least squares (OLS), interpreted as the linear projection of $y$ onto the span of $\\{x_1, x_2\\}$ that minimizes the expected squared residual. Starting from the fundamental definitions of covariance, variance, and the least-squares optimality (normal equations) for linear projection, derive the coefficient $\\beta_1$ as a function of $\\rho$ and then evaluate it at $\\rho = 0.9$. In your derivation, make clear why the marginal association between $x_1$ and $y$ being positive does not preclude a negative partial coefficient $\\beta_1$ once $x_2$ is controlled (suppression), and explain this sign reversal in geometric terms using orthogonal projections.\n\nCompute the numerical value of $\\beta_1$ at $\\rho = 0.9$ and round your answer to four significant figures. No units are required for this coefficient.",
            "solution": "The problem asks for the derivation of a multiple regression coefficient, an explanation of the suppression effect, and a numerical calculation. We begin by validating the problem statement.\n\n### Problem Validation\nThe problem is well-defined within the framework of theoretical statistics and linear models.\n**Givens:**\n- Predictors $\\{x_1, x_2\\}$ and response $y$ are random variables with finite second moments.\n- $\\operatorname{E}[x_1] = 0$, $\\operatorname{E}[x_2] = 0$, $\\operatorname{E}[y] = 0$.\n- $\\operatorname{Var}(x_1) = 1$, $\\operatorname{Var}(x_2) = 1$.\n- $\\operatorname{Cov}(x_1, x_2) = \\rho$, with $0 < \\rho < 1$.\n- $\\operatorname{Cov}(x_1, y) = 0.2$.\n- $\\operatorname{Cov}(x_2, y) = 0.8$.\n- The model is $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$, fit by ordinary least squares (OLS).\n\n**Validation:**\n1.  **Scientifically Grounded:** The problem uses standard, well-established concepts of expectation, variance, covariance, and ordinary least squares regression. All premises are consistent with statistical theory.\n2.  **Well-Posed:** The problem provides all necessary information (covariances and variances) to determine the OLS coefficients $\\beta_1$ and $\\beta_2$. The condition $0 < \\rho < 1$ ensures that the predictors are not perfectly collinear, guaranteeing a unique solution.\n3.  **Objective:** The problem is stated in precise mathematical language and asks for a formal derivation and a numerical calculation.\n\nThe problem is deemed valid and self-contained.\n\n### Derivation of the Coefficient $\\beta_1$\nThe OLS procedure minimizes the expected squared residual, $S = \\operatorname{E}[\\varepsilon^2] = \\operatorname{E}[(y - \\beta_0 - \\beta_1 x_1 - \\beta_2 x_2)^2]$.\n\nFirst, we determine the intercept $\\beta_0$. Taking the expectation of the model equation:\n$$ \\operatorname{E}[y] = \\operatorname{E}[\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon] = \\beta_0 + \\beta_1 \\operatorname{E}[x_1] + \\beta_2 \\operatorname{E}[x_2] + \\operatorname{E}[\\varepsilon] $$\nBy construction, OLS ensures that the expected value of the residual is zero, $\\operatorname{E}[\\varepsilon] = 0$. Using the given zero-mean conditions, we have:\n$$ 0 = \\beta_0 + \\beta_1(0) + \\beta_2(0) + 0 $$\nThis implies $\\beta_0 = 0$. The model simplifies to $y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$.\n\nThe minimization of $S = \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)^2]$ leads to the normal equations, which state that the residual vector must be orthogonal to each predictor vector. In terms of expectation, this means $\\operatorname{E}[\\varepsilon x_1] = 0$ and $\\operatorname{E}[\\varepsilon x_2] = 0$.\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_1] = 0 \\implies \\operatorname{E}[yx_1] - \\beta_1 \\operatorname{E}[x_1^2] - \\beta_2 \\operatorname{E}[x_1x_2] = 0 $$\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_2] = 0 \\implies \\operatorname{E}[yx_2] - \\beta_1 \\operatorname{E}[x_1x_2] - \\beta_2 \\operatorname{E}[x_2^2] = 0 $$\nGiven that all variables have zero mean, the expectations of products are equal to covariances, and the expectation of squares are equal to variances:\n- $\\operatorname{E}[yx_1] = \\operatorname{Cov}(x_1, y) = 0.2$\n- $\\operatorname{E}[yx_2] = \\operatorname{Cov}(x_2, y) = 0.8$\n- $\\operatorname{E}[x_1^2] = \\operatorname{Var}(x_1) = 1$\n- $\\operatorname{E}[x_2^2] = \\operatorname{Var}(x_2) = 1$\n- $\\operatorname{E}[x_1x_2] = \\operatorname{Cov}(x_1, x_2) = \\rho$\n\nSubstituting these into the normal equations gives the following system of linear equations for $\\beta_1$ and $\\beta_2$:\n$$ \\begin{cases} (1) \\quad 1 \\cdot \\beta_1 + \\rho \\cdot \\beta_2 = 0.2 \\\\ (2) \\quad \\rho \\cdot \\beta_1 + 1 \\cdot \\beta_2 = 0.8 \\end{cases} $$\nWe can solve this system for $\\beta_1$. From equation (1), we express $\\beta_2$ in terms of $\\beta_1$:\n$$ \\beta_2 = \\frac{0.2 - \\beta_1}{\\rho} $$\nSubstituting this into equation (2):\n$$ \\rho \\beta_1 + \\left( \\frac{0.2 - \\beta_1}{\\rho} \\right) = 0.8 $$\nMultiplying by $\\rho$ (which is non-zero) to clear the denominator:\n$$ \\rho^2 \\beta_1 + 0.2 - \\beta_1 = 0.8 \\rho $$\n$$ \\beta_1(\\rho^2 - 1) = 0.8 \\rho - 0.2 $$\n$$ \\beta_1 = \\frac{0.8 \\rho - 0.2}{\\rho^2 - 1} = \\frac{-(0.2 - 0.8 \\rho)}{-(1 - \\rho^2)} $$\nThis yields the expression for $\\beta_1$ as a function of $\\rho$:\n$$ \\beta_1 = \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2} $$\n\n### Suppression Effect and Geometric Interpretation\nThe marginal association between $x_1$ and $y$ is given by $\\operatorname{Cov}(x_1, y) = 0.2$, which is positive. However, the partial coefficient $\\beta_1$ can be negative. This phenomenon is a form of suppression. The condition for $\\beta_1$ to be negative is:\n$$ \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2} < 0 $$\nSince $0 < \\rho < 1$, the denominator $1 - \\rho^2$ is positive. Thus, the sign of $\\beta_1$ is determined by the numerator:\n$$ 0.2 - 0.8 \\rho < 0 \\implies 0.2 < 0.8 \\rho \\implies \\rho > \\frac{0.2}{0.8} = 0.25 $$\nSo, if $\\rho > 0.25$, the partial effect of $x_1$ on $y$ is negative, despite their positive marginal covariance.\n\n**Conceptual Explanation:**\nThe coefficient $\\beta_1$ represents the expected change in $y$ for a one-unit change in $x_1$, while holding $x_2$ constant. The predictor $x_2$ has a strong positive association with $y$ ($\\operatorname{Cov}(x_2, y) = 0.8$), and $x_1$ is also positively correlated with $x_2$ ($\\rho > 0$). A portion of the observed positive association between $x_1$ and $y$ is mediated through $x_2$: an increase in $x_1$ is associated with an increase in $x_2$, which in turn is associated with a strong increase in $y$. Multiple regression \"partials out\" this indirect effect. If the indirect positive association via $x_2$ is sufficiently strong (i.e., $\\rho$ is large enough), it can mask an underlying negative direct association between $x_1$ and $y$.\n\n**Geometric Interpretation:**\nIn the vector space of zero-mean random variables, the OLS coefficient $\\beta_1$ can be understood through orthogonal projections, as specified by the Frisch-Waugh-Lovell theorem. The coefficient $\\beta_1$ is the regression coefficient of $y$ on the part of $x_1$ that is orthogonal to $x_2$. Let $x_{1|2}$ be the residual from regressing $x_1$ on $x_2$:\n$$ x_{1|2} = x_1 - \\frac{\\operatorname{Cov}(x_1, x_2)}{\\operatorname{Var}(x_2)} x_2 = x_1 - \\rho x_2 $$\nThen $\\beta_1$ is given by the simple regression of $y$ on $x_{1|2}$:\n$$ \\beta_1 = \\frac{\\operatorname{Cov}(y, x_{1|2})}{\\operatorname{Var}(x_{1|2})} $$\nThe numerator is:\n$$ \\operatorname{Cov}(y, x_1 - \\rho x_2) = \\operatorname{Cov}(y, x_1) - \\rho \\operatorname{Cov}(y, x_2) = 0.2 - \\rho(0.8) $$\nThe denominator is:\n$$ \\operatorname{Var}(x_1 - \\rho x_2) = \\operatorname{Var}(x_1) - 2\\rho \\operatorname{Cov}(x_1, x_2) + \\rho^2 \\operatorname{Var}(x_2) = 1 - 2\\rho(\\rho) + \\rho^2(1) = 1 - \\rho^2 $$\nThus, $\\beta_1 = \\frac{0.2 - 0.8\\rho}{1 - \\rho^2}$, confirming our derivation. Geometrically, $\\beta_1$ is not determined by the projection of the vector $y$ onto $x_1$ (which is positive), but by the projection of $y$ onto the vector $x_{1|2}$ (the part of $x_1$ orthogonal to $x_2$). If $\\rho$ is large and positive, the vectors $x_1$ and $x_2$ are close in direction. The vector $x_{1|2}$ represents the \"correction\" needed to go from $\\rho x_2$ to $x_1$. The projection of $y$ onto this \"correction\" vector can be negative even if the projection of $y$ onto $x_1$ is positive. This happens when the strong pull of $y$ towards $x_2$ (due to $\\operatorname{Cov}(y, x_2) = 0.8$) is so dominant that the vector component required for the residual part of $x_1$ becomes negative.\n\n### Numerical Calculation\nWe evaluate $\\beta_1$ at $\\rho = 0.9$:\n$$ \\beta_1 = \\frac{0.2 - 0.8(0.9)}{1 - (0.9)^2} = \\frac{0.2 - 0.72}{1 - 0.81} = \\frac{-0.52}{0.19} $$\n$$ \\beta_1 \\approx -2.736842... $$\nRounding to four significant figures, we get $\\beta_1 = -2.737$.\nThe result confirms the suppression effect: with a high correlation of $\\rho = 0.9$ (which is $> 0.25$), the partial coefficient $\\beta_1$ is indeed negative.",
            "answer": "$$\\boxed{-2.737}$$"
        },
        {
            "introduction": "Practitioners often \"control for\" variables to reduce confounding bias, but this intuition can sometimes be misleading. This practice explores a scenario involving a \"collider\"—a variable caused by two other predictors—where conditioning on it does not reduce bias but instead introduces a different problem. Through a carefully designed simulation , you will see how including a collider can dramatically inflate the variance of another coefficient's estimate, making it so unstable that its sign may flip, leading to incorrect scientific conclusions. This provides a crucial lesson on the importance of causal reasoning in model building.",
            "id": "3132979",
            "problem": "Consider a data-generating process in which two explanatory variables $x_1$ and $x_2$ are jointly Gaussian and independent, and a third variable $C$ is a collider influenced by both $x_1$ and $x_2$. The outcome $y$ depends linearly on $x_1$ only. Specifically, let $x_1 \\sim \\mathcal{N}(0,1)$, $x_2 \\sim \\mathcal{N}(0,1)$ independently, and define\n$$\nC = \\gamma_1 x_1 + \\gamma_2 x_2 + \\varepsilon_C,\n$$\n$$\ny = \\beta_1 x_1 + \\varepsilon_y,\n$$\nwith $\\varepsilon_C \\sim \\mathcal{N}(0,\\sigma_C^2)$ and $\\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2)$, independent of $(x_1, x_2)$. This encodes a collider structure $x_1 \\to C \\leftarrow x_2$ and an outcome $y$ that depends solely on $x_1$.\n\nYou will implement Ordinary Least Squares (OLS) regressions under two model specifications:\n- Model A: regress $y$ on $x_1$ only.\n- Model B: regress $y$ on $x_1$ and $C$ (i.e., “controlling for” the collider).\n\nFrom the foundational definitions of OLS as the solution to minimizing the sum of squared residuals and the Frisch–Waugh–Lovell theorem, reason about the coefficient on $x_1$ in Model B. Show how, in the population, including $C$ does not change the expected coefficient on $x_1$, but changes its sampling variability through the residual variation in $x_1$ after projecting out $C$. Explain why, in finite samples, including $C$ can inflate variance enough that the estimated coefficient on $x_1$ may flip sign even when the true $\\beta_1$ is positive, thereby illustrating why “controlling for” a collider can mislead interpretation of regression coefficients in multiple regression.\n\nYour program must:\n1. Generate synthetic data according to the above data-generating process for each test case below.\n2. Estimate the OLS coefficient on $x_1$ in Model A and Model B for each case.\n3. Report whether the sign of the estimated coefficient on $x_1$ flips between Model A and Model B for each case (boolean).\n4. For one case, search a range of random seeds until a sign flip arises to demonstrate the phenomenon in finite samples.\n5. For the boundary case where $\\gamma_2=0$, compute the variance inflation factor predicted by the design, which in the two-regressor case equals $1/(1-\\rho^2)$, where $\\rho$ is the sample correlation between $x_1$ and $C$. Report this as a float.\n\nImportant details:\n- All random draws must be reproducible given a specified integer seed.\n- No physical units apply.\n- All outputs must be numeric or boolean only.\n\nTest suite parameter values:\n- Case $1$ (happy path, large sample, positive effect): $n=10000$, $\\beta_1=0.8$, $\\gamma_1=1.0$, $\\gamma_2=1.0$, $\\sigma_C=1.0$, $\\sigma_y=0.5$, seed $=42$.\n- Case $2$ (finite-sample sign flip search under severe multicollinearity): $n=60$, $\\beta_1=0.5$, $\\gamma_1=10.0$, $\\gamma_2=0.05$, $\\sigma_C=0.05$, $\\sigma_y=8.0$, and search seeds in the integer range $[0,5000)$ until the sign of the coefficient on $x_1$ differs between Model A and Model B; return the first seed that exhibits sign flip. If no seed is found in the range, return $-1$ and the last computed results.\n- Case $3$ (boundary condition, no collider contribution from $x_2$): $n=10000$, $\\beta_1=1.0$, $\\gamma_1=1.0$, $\\gamma_2=0.0$, $\\sigma_C=0.5$, $\\sigma_y=1.0$, seed $=7$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$$\n[\\hat{\\beta}_{1,\\text{A,case1}}, \\hat{\\beta}_{1,\\text{B,case1}}, \\text{flip}_{\\text{case1}}, \\text{seed}_{\\text{case2}}, \\hat{\\beta}_{1,\\text{A,case2}}, \\hat{\\beta}_{1,\\text{B,case2}}, \\text{flip}_{\\text{case2}}, \\text{VIF}_{\\text{case3}}]\n$$\nwhere $\\hat{\\beta}_{1,\\text{A}}$ is the estimated coefficient on $x_1$ in Model A, $\\hat{\\beta}_{1,\\text{B}}$ is the corresponding coefficient in Model B, $\\text{flip}$ is a boolean indicating whether the sign flipped when including $C$, and $\\text{VIF}$ is the variance inflation factor computed as $1/(1-\\rho^2)$ in Case $3$, with $\\rho$ the sample correlation between $x_1$ and $C$.\n\nYour implementation must use the Ordinary Least Squares estimator exactly and aggregate results from the three test cases into the single-line output format described above.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in statistical theory, well-posed with a clear data-generating process and objectives, and free of ambiguity or contradiction. We may therefore proceed with a solution.\n\nThe problem asks us to investigate the consequences of including a collider variable in an Ordinary Least Squares (OLS) regression. The core of the analysis rests on the distinction between the expected value (or population value) of a coefficient and its sampling variance in finite samples. We will use the Frisch–Waugh–Lovell (FWL) theorem as a theoretical tool.\n\nLet the data-generating process be defined as:\n$x_1 \\sim \\mathcal{N}(0,1)$\n$x_2 \\sim \\mathcal{N}(0,1)$, independent of $x_1$\n$C = \\gamma_1 x_1 + \\gamma_2 x_2 + \\varepsilon_C$, with $\\varepsilon_C \\sim \\mathcal{N}(0,\\sigma_C^2)$\n$y = \\beta_1 x_1 + \\varepsilon_y$, with $\\varepsilon_y \\sim \\mathcal{N}(0,\\sigma_y^2)$\nAll error terms are mutually independent and independent of $x_1$ and $x_2$.\n\nThis setup describes a directed acyclic graph where $x_1$ and $x_2$ are independent causes of $C$, making $C$ a collider ($x_1 \\to C \\leftarrow x_2$). The outcome $y$ is caused only by $x_1$ ($x_1 \\to y$).\n\nModel A: Simple Regression\nThe first model is a simple regression of $y$ on $x_1$:\n$$y = \\alpha_A + \\beta_{1,A} x_1 + u_A$$\nThe population OLS coefficient for $x_1$ is given by $\\beta_{1,A} = \\frac{\\text{Cov}(x_1, y)}{\\text{Var}(x_1)}$.\nSubstituting the true model for $y$:\n$$\\text{Cov}(x_1, y) = \\text{Cov}(x_1, \\beta_1 x_1 + \\varepsilon_y) = \\beta_1 \\text{Cov}(x_1, x_1) + \\text{Cov}(x_1, \\varepsilon_y)$$\nSince $x_1$ and $\\varepsilon_y$ are independent, $\\text{Cov}(x_1, \\varepsilon_y) = 0$. Thus, $\\text{Cov}(x_1, y) = \\beta_1 \\text{Var}(x_1)$.\nThe population coefficient is therefore:\n$$\\beta_{1,A} = \\frac{\\beta_1 \\text{Var}(x_1)}{\\text{Var}(x_1)} = \\beta_1$$\nThis is an unbiased estimator of the true causal effect, as expected, since there are no omitted confounders.\n\nModel B: Multiple Regression with Collider\nThe second model includes the collider $C$ as a regressor:\n$$y = \\alpha_B + \\beta_{1,B} x_1 + \\beta_{C,B} C + u_B$$\nTo determine the population coefficient $\\beta_{1,B}$, we apply the Frisch–Waugh–Lovell theorem. It states that $\\beta_{1,B}$ is the coefficient from a simple regression of $\\tilde{y}$ on $\\tilde{x}_1$, where $\\tilde{y}$ are the residuals from regressing $y$ on $C$, and $\\tilde{x}_1$ are the residuals from regressing $x_1$ on $C$.\nThe population residual from regressing a variable $v$ on $w$ is $v - \\frac{\\text{Cov}(v,w)}{\\text{Var}(w)}w$.\n1.  Residualize $x_1$ with respect to $C$:\n    $\\text{Cov}(x_1, C) = \\text{Cov}(x_1, \\gamma_1 x_1 + \\gamma_2 x_2 + \\varepsilon_C) = \\gamma_1 \\text{Var}(x_1) = \\gamma_1$ (since $\\text{Var}(x_1)=1$).\n    $\\text{Var}(C) = \\text{Var}(\\gamma_1 x_1 + \\gamma_2 x_2 + \\varepsilon_C) = \\gamma_1^2 \\text{Var}(x_1) + \\gamma_2^2 \\text{Var}(x_2) + \\text{Var}(\\varepsilon_C) = \\gamma_1^2 + \\gamma_2^2 + \\sigma_C^2$.\n    Let's denote $\\text{Var}(C) = V_C$.\n    The residual is $\\tilde{x}_1 = x_1 - \\frac{\\gamma_1}{V_C}C$.\n2.  Residualize $y$ with respect to $C$:\n    $\\text{Cov}(y, C) = \\text{Cov}(\\beta_1 x_1 + \\varepsilon_y, \\gamma_1 x_1 + \\gamma_2 x_2 + \\varepsilon_C) = \\beta_1 \\gamma_1 \\text{Cov}(x_1, x_1) = \\beta_1 \\gamma_1$.\n    The residual is $\\tilde{y} = y - \\frac{\\beta_1 \\gamma_1}{V_C}C$.\n\n3.  Compute $\\beta_{1,B} = \\frac{\\text{Cov}(\\tilde{x}_1, \\tilde{y})}{\\text{Var}(\\tilde{x}_1)}$.\n    Numerator:\n    $\\text{Cov}(\\tilde{x}_1, \\tilde{y}) = \\text{Cov}(x_1 - \\frac{\\gamma_1}{V_C}C, y - \\frac{\\beta_1 \\gamma_1}{V_C}C)$\n    $= \\text{Cov}(x_1, y) - \\frac{\\beta_1 \\gamma_1}{V_C} \\text{Cov}(x_1, C) - \\frac{\\gamma_1}{V_C} \\text{Cov}(C, y) + \\frac{\\gamma_1}{V_C}\\frac{\\beta_1 \\gamma_1}{V_C} \\text{Var}(C)$\n    $= \\beta_1 - \\frac{\\beta_1 \\gamma_1}{V_C}(\\gamma_1) - \\frac{\\gamma_1}{V_C}(\\beta_1 \\gamma_1) + \\frac{\\beta_1 \\gamma_1^2}{V_C}$\n    $= \\beta_1 - \\frac{\\beta_1 \\gamma_1^2}{V_C} = \\beta_1 \\left(1 - \\frac{\\gamma_1^2}{V_C}\\right)$\n    Denominator: $\\text{Var}(\\tilde{x}_1)$ is the variance of the residuals from regressing $x_1$ on $C$, which is $\\text{Var}(x_1)(1 - \\rho_{x_1, C}^2)$.\n    $\\rho_{x_1, C}^2 = \\frac{\\text{Cov}(x_1, C)^2}{\\text{Var}(x_1)\\text{Var}(C)} = \\frac{\\gamma_1^2}{1 \\cdot V_C} = \\frac{\\gamma_1^2}{V_C}$.\n    $\\text{Var}(\\tilde{x}_1) = \\text{Var}(x_1) (1 - \\rho_{x_1, C}^2) = 1 \\cdot \\left(1 - \\frac{\\gamma_1^2}{V_C}\\right)$.\n\n    Combining these gives:\n    $$\\beta_{1,B} = \\frac{\\beta_1 (1 - \\gamma_1^2/V_C)}{(1 - \\gamma_1^2/V_C)} = \\beta_1$$\nThis demonstrates that, in the population, the expected coefficient on $x_1$ is unchanged at $\\beta_1$ even when controlling for the collider $C$. This seems counter-intuitive to the typical \"collider bias\" story. The bias arises in a different context, e.g., estimating the association between $x_1$ and $x_2$. Here, the issue is not bias but variance.\n\nVariance of the Estimators\nThe sampling variance of the OLS coefficient $\\hat{\\beta}_j$ in a multiple regression is given by:\n$$\\text{Var}(\\hat{\\beta}_j) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2 (1 - R_j^2)}$$\nwhere $\\sigma^2$ is the variance of the regression error, and $R_j^2$ is the R-squared from regressing $x_j$ on all other explanatory variables. The term $1/(1-R_j^2)$ is the Variance Inflation Factor (VIF).\n\nFor Model A, there are no other regressors, so $R_1^2=0$. The error variance is $\\sigma_{u_A}^2 = \\text{Var}(\\varepsilon_y) = \\sigma_y^2$.\n$$\\text{Var}(\\hat{\\beta}_{1,A}) \\approx \\frac{\\sigma_y^2}{n \\text{Var}(x_1)}$$\n\nFor Model B, the other regressor is $C$. $R_1^2$ is the R-squared from regressing $x_1$ on $C$, which in the sample will be approximately $\\rho_{x_1, C}^2$. The population error variance $\\sigma_{u_B}^2$ is the variance of $y$ after accounting for both $x_1$ and $C$. Since the population coefficient on $C$ is zero (as shown by a similar FWL argument), the error is still $\\varepsilon_y$, and $\\sigma_{u_B}^2 = \\sigma_y^2$.\n$$\\text{Var}(\\hat{\\beta}_{1,B}) \\approx \\frac{\\sigma_y^2}{n \\text{Var}(x_1) (1 - \\rho_{x_1, C}^2)} = \\text{Var}(\\hat{\\beta}_{1,A}) \\cdot \\frac{1}{1 - \\rho_{x_1, C}^2}$$\nSince $C = \\gamma_1 x_1 + \\gamma_2 x_2 + \\varepsilon_C$, $x_1$ and $C$ are correlated. The population correlation squared is $\\rho_{x_1, C}^2 = \\frac{\\gamma_1^2}{\\gamma_1^2 + \\gamma_2^2 + \\sigma_C^2}$. This is non-zero as long as $\\gamma_1 \\neq 0$.\n\nTherefore, $\\text{Var}(\\hat{\\beta}_{1,B}) > \\text{Var}(\\hat{\\beta}_{1,A})$. Controlling for the collider $C$ introduces multicollinearity, which inflates the sampling variance of the coefficient estimate for $x_1$.\n\nSignificance of Variance Inflation\nWhile the estimator $\\hat{\\beta}_{1,B}$ is unbiased (its sampling distribution is centered on the true $\\beta_1$), its variance can be enormous if the correlation between $x_1$ and $C$ is high. A high variance means that any single finite-sample estimate $\\hat{\\beta}_{1,B}$ is likely to be far from the true value $\\beta_1$. If the variance is large enough relative to the magnitude of $\\beta_1$, there is a substantial probability that $\\hat{\\beta}_{1,B}$ will have the opposite sign of $\\beta_1$. This is particularly likely with a small sample size ($n$), a large error variance in the outcome equation ($\\sigma_y^2$), and high collinearity (large $\\gamma_1$ relative to $\\gamma_2$ and $\\sigma_C$). A researcher observing such a sign flip might incorrectly conclude that $x_1$ has a negative effect, when in fact the true effect is positive. This demonstrates how \"controlling for\" a variable that is mechanistically subsequent to a cause ($x_1$) can corrupt statistical inference, not by introducing bias in the expectation, but by catastrophically inflating variance.\n\nThe specific test cases are designed to illustrate this. Case 2 uses parameters that create extreme multicollinearity ($\\gamma_1=10$ vs $\\gamma_2=0.05, \\sigma_C=0.05$), a small sample ($n=60$), and high outcome noise ($\\sigma_y=8$), making a sign flip highly probable. Case 3 explores the boundary where $C$ is perfectly explained by $x_1$ and noise (since $\\gamma_2=0$), calculating the VIF from the resulting collinearity.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the collider bias simulation problem by generating data,\n    running OLS regressions, and reporting the specified metrics.\n    \"\"\"\n\n    def run_ols(X, y):\n        \"\"\"\n        Performs Ordinary Least Squares regression.\n        \n        Args:\n            X (np.ndarray): Design matrix of shape (n_samples, n_features).\n                            Should include an intercept column if desired.\n            y (np.ndarray): Response vector of shape (n_samples,).\n\n        Returns:\n            np.ndarray: Estimated coefficients.\n        \"\"\"\n        coeffs, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        return coeffs\n\n    def generate_data(n, beta1, gamma1, gamma2, sigma_C, sigma_y, seed):\n        \"\"\"\n        Generates synthetic data based on the specified DGP.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        x1 = rng.normal(0, 1, n)\n        x2 = rng.normal(0, 1, n)\n        eps_C = rng.normal(0, sigma_C, n)\n        eps_y = rng.normal(0, sigma_y, n)\n        \n        C = gamma1 * x1 + gamma2 * x2 + eps_C\n        y = beta1 * x1 + eps_y\n        \n        return x1, C, y\n\n    # --- Case 1: Happy path, large sample ---\n    n1, b1_1, g1_1, g2_1, sC_1, sy_1, seed1 = 10000, 0.8, 1.0, 1.0, 1.0, 0.5, 42\n    x1_1, C_1, y_1 = generate_data(n1, b1_1, g1_1, g2_1, sC_1, sy_1, seed1)\n    \n    # Model A: y ~ x1\n    X_A1 = np.c_[np.ones(n1), x1_1]\n    beta_A1 = run_ols(X_A1, y_1)\n    beta_1_A_case1 = beta_A1[1]\n\n    # Model B: y ~ x1 + C\n    X_B1 = np.c_[np.ones(n1), x1_1, C_1]\n    beta_B1 = run_ols(X_B1, y_1)\n    beta_1_B_case1 = beta_B1[1]\n\n    flip_case1 = np.sign(beta_1_A_case1) != np.sign(beta_1_B_case1)\n\n    # --- Case 2: Finite-sample sign flip search ---\n    n2, b1_2, g1_2, g2_2, sC_2, sy_2 = 60, 0.5, 10.0, 0.05, 0.05, 8.0\n    found_seed = -1\n    beta_1_A_case2, beta_1_B_case2, flip_case2 = np.nan, np.nan, False\n\n    for seed in range(5000):\n        x1_2, C_2, y_2 = generate_data(n2, b1_2, g1_2, g2_2, sC_2, sy_2, seed)\n\n        # Model A\n        X_A2 = np.c_[np.ones(n2), x1_2]\n        beta_A2 = run_ols(X_A2, y_2)\n        current_beta_A = beta_A2[1]\n\n        # Model B\n        X_B2 = np.c_[np.ones(n2), x1_2, C_2]\n        beta_B2 = run_ols(X_B2, y_2)\n        current_beta_B = beta_B2[1]\n        \n        # Store last computed results in case no flip is found\n        beta_1_A_case2 = current_beta_A\n        beta_1_B_case2 = current_beta_B\n\n        # Check for sign flip (positive to non-positive or vice-versa)\n        if np.sign(current_beta_A) != np.sign(current_beta_B):\n            found_seed = seed\n            flip_case2 = True\n            break\n    \n    # If loop completes without finding a seed, flip_case2 remains False from initialization.\n    if found_seed == -1:\n        flip_case2 = False\n\n\n    # --- Case 3: Boundary condition, VIF calculation ---\n    n3, b1_3, g1_3, g2_3, sC_3, sy_3, seed3 = 10000, 1.0, 1.0, 0.0, 0.5, 1.0, 7\n    x1_3, C_3, _ = generate_data(n3, b1_3, g1_3, g2_3, sC_3, sy_3, seed3)\n\n    # Calculate sample correlation between x1 and C\n    # np.corrcoef returns a 2x2 matrix\n    rho_matrix = np.corrcoef(x1_3, C_3)\n    rho = rho_matrix[0, 1]\n    \n    # Calculate Variance Inflation Factor (VIF)\n    vif_case3 = 1 / (1 - rho**2)\n\n    # --- Aggregate and Format Output ---\n    results = [\n        beta_1_A_case1,\n        beta_1_B_case1,\n        flip_case1,\n        found_seed,\n        beta_1_A_case2,\n        beta_1_B_case2,\n        flip_case2,\n        vif_case3\n    ]\n\n    # The spec requires a comma-separated string list.\n    # Python's str() correctly converts booleans to 'True'/'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}