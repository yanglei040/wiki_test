{
    "hands_on_practices": [
        {
            "introduction": "The overall F-test tells us if our model, as a whole, is a significant improvement over a simple mean. However, a more powerful application of this framework is in comparing nested models to determine if a specific subset of predictors adds significant explanatory power. This practice  provides a hands-on calculation for quantifying the value of adding interaction terms to a main-effects model, a fundamental skill in model building and refinement.",
            "id": "3182401",
            "problem": "A manufacturing team models the output yield $y$ of $n=50$ batches as a linear function of three measured covariates: reactant concentration $x_{1}$, furnace temperature $x_{2}$, and catalyst indicator $x_{3}$ (coded $0$ or $1$). To capture synergistic effects, they include the product terms $x_{1}x_{2}$ and $x_{2}x_{3}$ in the model. Let the full model be\n$$\ny_{i} \\;=\\; \\beta_{0} \\;+\\; \\beta_{1} x_{1i} \\;+\\; \\beta_{2} x_{2i} \\;+\\; \\beta_{3} x_{3i} \\;+\\; \\beta_{4} (x_{1i}x_{2i}) \\;+\\; \\beta_{5} (x_{2i}x_{3i}) \\;+\\; \\varepsilon_{i}, \\quad i=1,\\dots,50,\n$$\nwith $\\varepsilon_{i}$ assumed independent and identically distributed as mean-zero Gaussian with constant variance. The reduced model with only main effects is\n$$\ny_{i} \\;=\\; \\beta_{0} \\;+\\; \\beta_{1} x_{1i} \\;+\\; \\beta_{2} x_{2i} \\;+\\; \\beta_{3} x_{3i} \\;+\\; \\varepsilon_{i}.\n$$\nFrom an Ordinary Least Squares (OLS) fit, the Analysis of Variance (ANOVA) quantities computed about the sample mean are:\n- total sum of squares $TSS = 1200$,\n- residual sum of squares for the full model $SSE_{\\text{full}} = 400$,\n- residual sum of squares for the reduced (main-effects-only) model $SSE_{\\text{main}} = 550$.\n\nStarting from the classical linear model assumptions and the core properties of quadratic forms under normal errors, derive the appropriate test statistics for:\n1. the overall regression significance of the full model (all five non-intercept coefficients jointly equal to zero), and\n2. the incremental gain from adding the two interaction terms $x_{1}x_{2}$ and $x_{2}x_{3}$ to the main-effects model, using the logic of nested models.\n\nCompute the numerical values of both test statistics. Express your final answer as a row matrix with the first entry equal to the overall significance statistic for the full model and the second entry equal to the partial statistic for the interactions. Round your two numbers to four significant figures. No units are required.",
            "solution": "The problem requires the derivation and computation of two distinct F-statistics related to a multiple linear regression analysis. The first tests the overall significance of the full model, while the second tests the significance of a subset of predictors (the interaction terms). The foundation for both tests is the comparison of nested linear models.\n\nLet a general linear model be specified in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is an $n \\times 1$ vector of observations, $\\mathbf{X}$ is an $n \\times p$ design matrix of rank $p$, $\\boldsymbol{\\beta}$ is a $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is an $n \\times 1$ vector of errors with $\\boldsymbol{\\varepsilon} \\sim N(0, \\sigma^2 \\mathbf{I})$. The residual sum of squares is given by $SSE = ||\\mathbf{y} - \\hat{\\mathbf{y}}||^2 = \\mathbf{y}^T(\\mathbf{I} - \\mathbf{H})\\mathbf{y}$, where $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ is the hat matrix. Under the normality assumption, $\\frac{SSE}{\\sigma^2} \\sim \\chi^2_{n-p}$.\n\nThe general F-test for a linear hypothesis $H_0: \\mathbf{L}\\boldsymbol{\\beta} = \\mathbf{0}$, where $\\mathbf{L}$ is a $q \\times p$ matrix of rank $q$, compares a full model (where the hypothesis is not imposed) with a reduced model (where the hypothesis is imposed). The test statistic is:\n$$\nF = \\frac{(SSE_R - SSE_F) / q}{SSE_F / (n-p_F)}\n$$\nwhere $SSE_R$ and $SSE_F$ are the residual sums of squares for the reduced and full models, respectively. The number of parameters in the full model is $p_F$, and $q$ is the number of linear constraints imposed by the null hypothesis, which also equals $p_F - p_R$, where $p_R$ is the number of parameters in the reduced model. Under $H_0$, this statistic follows an F-distribution with $q$ and $n-p_F$ degrees of freedom, denoted $F(q, n-p_F)$.\n\nThe provided data are:\n- Number of observations: $n=50$\n- Total Sum of Squares: $TSS = 1200$\n- Residual Sum of Squares (full model): $SSE_{\\text{full}} = 400$\n- Residual Sum of Squares (main-effects model): $SSE_{\\text{main}} = 550$\n\nThe full model has predictors $x_1$, $x_2$, $x_3$, $x_1x_2$, and $x_2x_3$. The number of non-intercept predictors is $k_{\\text{full}}=5$. The total number of parameters, including the intercept $\\beta_0$, is $p_{\\text{full}} = k_{\\text{full}} + 1 = 6$.\n\n### 1. Overall Regression Significance of the Full Model\n\nThis test assesses the null hypothesis that all non-intercept coefficients are zero: $H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0$.\n\n- **Full Model**: This is the model containing all $5$ predictors plus an intercept. The residual sum of squares is given as $SSE_F = SSE_{\\text{full}} = 400$. The number of parameters is $p_F = p_{\\text{full}} = 6$. The error degrees of freedom are $df_E = n - p_{\\text{full}} = 50 - 6 = 44$. The corresponding Mean Squared Error is $MSE_{\\text{full}} = \\frac{SSE_{\\text{full}}}{n - p_{\\text{full}}}$.\n\n- **Reduced Model**: Under $H_0$, the model becomes $y_i = \\beta_0 + \\varepsilon_i$. This is an intercept-only model. For an intercept-only model, the OLS estimate for $\\beta_0$ is the sample mean $\\bar{y}$, and the residual sum of squares is, by definition, the total sum of squares, $TSS$. Thus, $SSE_R = TSS = 1200$. The number of parameters is $p_R = 1$. The number of constraints is $q = p_{\\text{full}} - p_R = 6-1=5$, which corresponds to the $5$ coefficients set to zero.\n\nThe sum of squares explained by the regression is $SSR_{\\text{full}} = SSE_R - SSE_F = TSS - SSE_{\\text{full}}$. This quantity represents the reduction in error sum of squares achieved by using the full model over the intercept-only model.\n$SSR_{\\text{full}} = 1200 - 400 = 800$.\n\nThe F-statistic for overall significance is the ratio of the Mean Square for Regression ($MSR_{\\text{full}}$) to the Mean Square for Error ($MSE_{\\text{full}}$):\n$$\nF_{\\text{overall}} = \\frac{MSR_{\\text{full}}}{MSE_{\\text{full}}} = \\frac{SSR_{\\text{full}} / (p_{\\text{full}} - 1)}{SSE_{\\text{full}} / (n - p_{\\text{full}})}\n$$\nSubstituting the given values:\n$$\nF_{\\text{overall}} = \\frac{(1200 - 400) / 5}{400 / (50 - 6)} = \\frac{800 / 5}{400 / 44} = \\frac{160}{400/44} = 160 \\times \\frac{44}{400} = 0.4 \\times 44 = 17.6\n$$\nUnder $H_0$, this statistic follows an $F(5, 44)$ distribution.\n\n### 2. Incremental Gain from Interaction Terms\n\nThis test assesses the null hypothesis that the coefficients of the interaction terms are both zero: $H_0: \\beta_4 = \\beta_5 = 0$. This is a partial F-test comparing two nested models.\n\n- **Full Model**: This is again the model with all $5$ predictors: $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\beta_{3} x_{3i} + \\beta_{4} (x_{1i}x_{2i}) + \\beta_{5} (x_{2i}x_{3i}) + \\varepsilon_{i}$. Its SSE is $SSE_F = SSE_{\\text{full}} = 400$, and it has $p_F = p_{\\text{full}} = 6$ parameters. The denominator of the F-statistic is the MSE of this full model, $MSE_{\\text{full}} = \\frac{SSE_{\\text{full}}}{n - p_{\\text{full}}} = \\frac{400}{44}$.\n\n- **Reduced Model**: This is the model under $H_0$, which excludes the interaction terms. This is the main-effects model provided in the problem statement: $y_{i} = \\beta_{0} + \\beta_{1} x_{1i} + \\beta_{2} x_{2i} + \\beta_{3} x_{3i} + \\varepsilon_{i}$. Its SSE is given as $SSE_R = SSE_{\\text{main}} = 550$. This model has $k_{\\text{main}} = 3$ predictors, so the number of parameters is $p_R = p_{\\text{main}} = 3+1 = 4$.\n\nThe number of constraints, $q$, is the number of additional parameters in the full model compared to the reduced model, which is $q = p_F - p_R = 6 - 4 = 2$. This matches the number of coefficients set to zero in the null hypothesis.\n\nThe F-statistic for this partial test is:\n$$\nF_{\\text{partial}} = \\frac{(SSE_{\\text{main}} - SSE_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{main}})}{SSE_{\\text{full}} / (n - p_{\\text{full}})}\n$$\nThe numerator represents the marginal reduction in SSE from adding the two interaction terms to the main-effects model, normalized by the number of added terms.\nSubstituting the given values:\n$$\nF_{\\text{partial}} = \\frac{(550 - 400) / 2}{400 / (50 - 6)} = \\frac{150 / 2}{400 / 44} = \\frac{75}{400 / 44} = 75 \\times \\frac{44}{400} = \\frac{3 \\times 25 \\times 44}{16 \\times 25} = \\frac{3 \\times 44}{16} = \\frac{3 \\times 11}{4} = \\frac{33}{4} = 8.25\n$$\nUnder $H_0$, this statistic follows an $F(2, 44)$ distribution.\n\nThe problem requires the two values rounded to four significant figures.\n$F_{\\text{overall}} = 17.6$ becomes $17.60$.\n$F_{\\text{partial}} = 8.25$ becomes $8.250$.\nThe final answer is presented as a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n17.60  8.250\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "When building a regression model, it is tempting to believe that adding more predictors will always lead to a better, more significant model. This is not always the case, especially when new predictors are highly correlated with existing ones. This exercise  explores the important concepts of multicollinearity and model complexity, revealing how adding a seemingly useful but redundant predictor can actually dilute the model's overall significance and reduce the F-statistic.",
            "id": "3182478",
            "problem": "Consider a standard linear regression setting in statistical learning with a response variable $y$ and predictors collected in a design matrix $X$, fit by Ordinary Least Squares (OLS). The OLS model is $y = X\\beta + \\varepsilon$, where $\\varepsilon$ has mean $0$ and constant variance, and an intercept is included. The overall regression significance is assessed by comparing explained and unexplained variation using sums of squares. Assume the following scenario.\n\nYou have $n=120$ independent observations. First, fit a simple regression of $y$ on a single predictor $x$. Next, add a second predictor $z$ that is a noisy copy of $x$, meaning $z$ is highly correlated with $x$ but does not carry substantially new signal beyond what $x$ already provides. Suppose all variables $y$, $x$, and $z$ have been centered to mean $0$ and standardized to unit variance, and the sample correlations are:\n- $r_{yx} = 0.60$,\n- $r_{yz} = 0.58$,\n- $r_{xz} = 0.95$.\n\nStarting from the definitions of total sum of squares, residual sum of squares, and explained sum of squares, and using OLS with an intercept, derive the overall regression $F$-statistic for a model with $p$ predictors. Then, use correlation-based reasoning for standardized variables to quantify how the coefficient of determination $R^{2}$ changes when the redundant predictor $z$ is added to $x$.\n\nCompute the ratio of the overall regression $F$-statistics,\n$$\\frac{F_{\\text{with }x,z}}{F_{\\text{with }x}},$$\nfor the two models (the first with $p=1$ predictor $x$, the second with $p=2$ predictors $x$ and $z$) under the given correlations and sample size. Round your final ratio to four significant figures.\n\nIn addition to the calculation, briefly explain qualitatively how the degrees of freedom change between the two models and how the high correlation between $x$ and $z$ can induce ill-conditioning in the design, affecting the stability of coefficient estimates and the overall significance assessment. The final numeric ratio is the only quantity to be reported in the final answer box.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All necessary data are provided, and the framework is that of standard statistical linear regression. We may proceed with the solution.\n\nFirst, we derive the general formula for the overall regression $F$-statistic. In a linear regression model with $n$ observations and $p$ predictors (plus an intercept), the total variation in the response variable $y$ is partitioned into explained and unexplained components. The Total Sum of Squares ($SST$) is given by $SST = \\sum_{i=1}^n (y_i - \\bar{y})^2$. This is partitioned into the Regression Sum of Squares ($SSR$), $SSR = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2$, and the Error Sum of Squares ($SSE$), $SSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$. The fundamental identity is $SST = SSR + SSE$.\n\nThe coefficient of determination, $R^2$, is the fraction of total variance explained by the model:\n$$R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$$\nFrom this, we can express $SSR$ and $SSE$ in terms of $R^2$ and $SST$:\n$$SSR = R^2 \\cdot SST$$\n$$SSE = (1 - R^2) \\cdot SST$$\nThe overall $F$-statistic tests the null hypothesis that all predictor coefficients are zero ($H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$). It is the ratio of the Mean Square for Regression ($MSR$) to the Mean Square for Error ($MSE$). The degrees of freedom for the regression are $p$, and for the residuals are $n - p - 1$.\n$$MSR = \\frac{SSR}{p}$$\n$$MSE = \\frac{SSE}{n-p-1}$$\nThe $F$-statistic is therefore:\n$$F = \\frac{MSR}{MSE} = \\frac{SSR/p}{SSE/(n-p-1)}$$\nSubstituting the expressions in terms of $R^2$:\n$$F = \\frac{(R^2 \\cdot SST) / p}{((1 - R^2) \\cdot SST) / (n-p-1)} = \\frac{R^2/p}{(1-R^2)/(n-p-1)}$$\nThis is the required general formula.\n\nNext, we compute the coefficient of determination, $R^2$, for the two models. The variables $y$, $x$, and $z$ are standardized, which simplifies the calculations.\nFor the first model, a simple linear regression of $y$ on $x$, we have $p=1$. The coefficient of determination, which we denote $R^2_x$, is the square of the sample correlation between $y$ and $x$.\n$$R^2_x = (r_{yx})^2 = (0.60)^2 = 0.36$$\n\nFor the second model, a multiple linear regression of $y$ on $x$ and $z$, we have $p=2$. For standardized variables, the coefficient of multiple determination, denoted $R^2_{xz}$, is given by the formula:\n$$R^2_{xz} = \\frac{r_{yx}^2 + r_{yz}^2 - 2 r_{yx} r_{yz} r_{xz}}{1 - r_{xz}^2}$$\nSubstituting the given correlation values: $r_{yx} = 0.60$, $r_{yz} = 0.58$, and $r_{xz} = 0.95$.\n$$R^2_{xz} = \\frac{(0.60)^2 + (0.58)^2 - 2(0.60)(0.58)(0.95)}{1 - (0.95)^2}$$\n$$R^2_{xz} = \\frac{0.36 + 0.3364 - 0.6612}{1 - 0.9025} = \\frac{0.0352}{0.0975}$$\nNotice that $R^2_{xz} \\approx 0.3610256$, a very marginal increase from $R^2_x = 0.36$. This is expected because $z$ is highly correlated with $x$ and provides little new information for predicting $y$.\n\nNow we compute the $F$-statistic for each model using $n=120$.\nFor the model with only $x$ ($p_1=1$):\n$$F_{\\text{with }x} = \\frac{R^2_x/p_1}{(1-R^2_x)/(n-p_1-1)} = \\frac{0.36/1}{(1-0.36)/(120-1-1)} = \\frac{0.36}{0.64/118} = \\frac{0.36 \\times 118}{0.64} = 66.375$$\n\nFor the model with $x$ and $z$ ($p_2=2$):\n$$F_{\\text{with }x,z} = \\frac{R^2_{xz}/p_2}{(1-R^2_{xz})/(n-p_2-1)} = \\frac{(0.0352/0.0975)/2}{(1 - 0.0352/0.0975)/(120-2-1)}$$\n$$F_{\\text{with }x,z} = \\frac{0.0352 / (2 \\times 0.0975)}{( (0.0975 - 0.0352)/0.0975 ) / 117} = \\frac{0.0352 / 0.195}{(0.0623/0.0975)/117} = \\frac{0.0352 \\times 117}{2 \\times 0.0623} = \\frac{4.1184}{0.1246} \\approx 33.053...$$\n\nFinally, we compute the ratio of the two $F$-statistics:\n$$\\frac{F_{\\text{with }x,z}}{F_{\\text{with }x}} = \\frac{33.053...}{66.375} \\approx 0.49800...$$\nRounding to four significant figures, the ratio is $0.4980$.\n\nA qualitative explanation of the related phenomena is warranted.\nWhen adding the predictor $z$, the degrees of freedom for the regression model increase from $p_1=1$ to $p_2=2$. Correspondingly, the degrees of freedom for the residuals decrease from $n-p_1-1=118$ to $n-p_2-1=117$. The total degrees of freedom, $n-1=119$, remain unchanged.\n\nThe high correlation between $x$ and $z$ ($r_{xz}=0.95$) is known as multicollinearity. This induces ill-conditioning in the design matrix $X$. The matrix $(X^T X)$, which is inverted to find the OLS coefficient estimates $\\hat{\\beta}$, becomes nearly singular. Consequently, the inverse matrix has large diagonal elements, which inflates the variance of the coefficient estimates ($\\text{Var}(\\hat{\\beta}_j)$). This makes the individual coefficient estimates unstable and their standard errors large, compromising the assessment of each predictor's individual contribution.\n\nRegarding overall significance, the $F$-statistic decreases despite $R^2$ increasing. The formula $F = (R^2/p) / ((1-R^2)/(n-p-1))$ reveals why. The term $R^2/p$ represents the average explained variance per predictor. When adding the redundant predictor $z$, $R^2$ increases only marginally (from $0.36$ to $\\approx 0.361$), but the number of predictors $p$ doubles from $1$ to $2$. The numerator term $R^2/p$ thus drops sharply from $0.36/1=0.36$ to $\\approx 0.361/2 \\approx 0.18$. This reflects a penalty for model complexity; the small gain in explanatory power from adding $z$ is not sufficient to justify the \"cost\" of an additional parameter, leading to a less significant overall model as judged by the F-statistic.",
            "answer": "$$\\boxed{0.4980}$$"
        },
        {
            "introduction": "A common point of confusion in statistical learning is the distinction between a model's statistical significance and its predictive utility. Is a model with a non-significant F-statistic useless? This practice  tackles this question directly by presenting a scenario where a model shows clear predictive improvement but fails to achieve conventional statistical significance, helping to clarify the crucial role of statistical power and sample size in hypothesis testing.",
            "id": "3182416",
            "problem": "An analyst fits a multiple linear regression with an intercept and $p$ predictors to a response $Y$ under the standard normal linear model $Y = X\\beta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$. The goal is to assess the overall regression significance of the $p$ predictors while also noting predictive performance on new data. Consider the following designed case intended to highlight a tension between inference and prediction when the sample size $n$ is small.\n\nYou are told that for a fitted model with $n = 9$ observations and $p = 3$ predictors (excluding the intercept), the total sum of squares about the mean is $\\operatorname{SST} = 88$, and the residual sum of squares for the fitted model is $\\operatorname{SSE} = 25$. Independently, the analyst also evaluates predictive performance using Leave-One-Out Cross-Validation (LOOCV) and finds the LOOCV mean squared error for the fitted model to be $5.6$, whereas for the intercept-only (null) model it is $12.3$.\n\nTasks:\n- Starting from the normal linear model assumptions and the definitions of the total sum of squares and the residual sum of squares, derive the overall regression test statistic used to assess $H_{0}: \\beta_{1} = \\cdots = \\beta_{p} = 0$ against the alternative that at least one slope is nonzero, explicitly identifying the relevant degrees of freedom.\n- Use the supplied summaries to compute the observed value of this test statistic.\n- Briefly explain, in one or two sentences, how it can be that the model exhibits superior predictive performance by LOOCV while the overall regression test may fail to reach conventional significance when $n$ is small.\n\nReport only the numerical value of the observed test statistic as your final answer. Round your result to three significant figures.",
            "solution": "The problem is assessed to be valid. It is self-contained, scientifically grounded in the principles of linear regression analysis, and well-posed. All necessary data for the required calculations are provided.\n\nThe primary task is to derive the F-statistic for the overall significance of a multiple linear regression model and then compute its value. The model is given by $Y = X\\beta + \\varepsilon$, where $Y$ is the $n \\times 1$ vector of observations, $X$ is the $n \\times (p+1)$ design matrix (including a column of ones for the intercept), $\\beta$ is the $(p+1) \\times 1$ vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of errors with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$.\n\nThe test for overall regression significance evaluates the null hypothesis $H_{0}: \\beta_{1} = \\beta_{2} = \\cdots = \\beta_{p} = 0$ against the alternative hypothesis $H_{A}$: at least one $\\beta_{j} \\neq 0$ for $j \\in \\{1, \\dots, p\\}$. This is a comparison between two nested models:\n1. The full model, which includes all $p$ predictors and an intercept.\n2. The reduced (or null) model, which includes only the intercept. This model is specified by the constraints of the null hypothesis.\n\nThe test statistic is constructed from the partitioning of the total sum of squares, $\\operatorname{SST}$. The total sum of squares measures the total variability in the response variable $Y$ around its sample mean $\\bar{y}$. It is defined as $\\operatorname{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$. The degrees of freedom associated with $\\operatorname{SST}$ are $df_{T} = n-1$.\n\nThe residual sum of squares, $\\operatorname{SSE}$, measures the variability that remains unexplained after fitting the regression model. It is defined as $\\operatorname{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$, where $\\hat{y}_i$ are the fitted values from the full model. The degrees of freedom for error in the full model are $df_{E} = n - (\\text{number of parameters}) = n - (p+1)$.\n\nThe regression sum of squares, $\\operatorname{SSR}$, measures the variability in $Y$ that is explained by the regression model. It is the difference between the total variability and the unexplained variability: $\\operatorname{SSR} = \\operatorname{SST} - \\operatorname{SSE}$. The degrees of freedom for regression correspond to the number of predictor coefficients being tested, which is $p$. This can also be seen as the difference in the degrees of freedom for error between the reduced model ($n-1$) and the full model ($n-p-1$), so $df_{R} = (n-1) - (n-p-1) = p$.\n\nThe F-test statistic is the ratio of two mean squares. A mean square is a sum of squares divided by its degrees of freedom.\nThe Mean Square for Regression is $\\operatorname{MSR} = \\frac{\\operatorname{SSR}}{df_{R}} = \\frac{\\operatorname{SST} - \\operatorname{SSE}}{p}$.\nThe Mean Square for Error is $\\operatorname{MSE} = \\frac{\\operatorname{SSE}}{df_{E}} = \\frac{\\operatorname{SSE}}{n-p-1}$.\n\nThe F-statistic is defined as the ratio $F = \\frac{\\operatorname{MSR}}{\\operatorname{MSE}}$. Under the assumption that the null hypothesis $H_{0}$ is true, this statistic follows an F-distribution with $p$ and $n-p-1$ degrees of freedom.\nThe formula is:\n$$F = \\frac{(\\operatorname{SST} - \\operatorname{SSE}) / p}{\\operatorname{SSE} / (n-p-1)}$$\nThe relevant degrees of freedom are $p$ for the numerator and $n-p-1$ for the denominator.\n\nWe are given the following values:\n- Sample size: $n = 9$\n- Number of predictors: $p = 3$\n- Total sum of squares: $\\operatorname{SST} = 88$\n- Residual sum of squares: $\\operatorname{SSE} = 25$\n\nFirst, we calculate the degrees of freedom:\n- Numerator degrees of freedom: $df_{1} = p = 3$\n- Denominator degrees of freedom: $df_{2} = n - p - 1 = 9 - 3 - 1 = 5$\n\nNext, we calculate the mean squares:\n- $\\operatorname{SSR} = \\operatorname{SST} - \\operatorname{SSE} = 88 - 25 = 63$\n- $\\operatorname{MSR} = \\frac{\\operatorname{SSR}}{p} = \\frac{63}{3} = 21$\n- $\\operatorname{MSE} = \\frac{\\operatorname{SSE}}{n-p-1} = \\frac{25}{5} = 5$\n\nFinally, we compute the observed F-statistic:\n$$F_{\\text{obs}} = \\frac{\\operatorname{MSR}}{\\operatorname{MSE}} = \\frac{21}{5} = 4.2$$\nRounding to three significant figures, the value is $4.20$.\n\nThe final task is to explain the apparent contradiction between predictive performance and statistical significance. The LOOCV MSE for the fitted model ($5.6$) is less than half that of the null model ($12.3$), suggesting substantial predictive improvement. However, the F-statistic of $4.20$ with $(3, 5)$ degrees of freedom has a p-value of approximately $0.08$, which is not significant at a conventional $\\alpha = 0.05$ level (the critical value $F_{0.05, 3, 5}$ is $5.41$). This occurs because the F-test has low statistical power due to the very small sample size ($n=9$) and resulting small error degrees of freedom ($df_{E}=5$). Cross-validation assesses predictive utility, which can be evident even when the sample is too small for a formal hypothesis test to provide statistically significant evidence against the null.",
            "answer": "$$\\boxed{4.20}$$"
        }
    ]
}