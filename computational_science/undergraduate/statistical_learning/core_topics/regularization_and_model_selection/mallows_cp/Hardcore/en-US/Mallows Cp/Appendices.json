{
    "hands_on_practices": [
        {
            "introduction": "The penalty term is a critical component of any model selection criterion, but its definition is more subtle than it first appears. This exercise probes the true meaning of model complexity, especially when predictors are redundant. You will demonstrate that the penalty in Mallows' $C_p$ is determined by the model's effective degrees of freedom, captured by the trace of the hat matrix $\\mathrm{tr}(H)$, rather than a naive count of variables .",
            "id": "3143701",
            "problem": "Consider a linear regression model with homoscedastic Gaussian errors, given by $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$ collects an intercept and $p-1$ predictors, $\\beta \\in \\mathbb{R}^{p}$ is an unknown parameter vector, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Suppose you fit the model by Ordinary Least Squares (OLS), interpreting the estimator as the orthogonal projection onto the column space of $X$ via the Moore–Penrose pseudoinverse, so that the fitted values are $\\hat{y} = H y$ with $H = X X^{+}$, and $X^{+}$ denotes the Moore–Penrose pseudoinverse. The matrix $H$ is often called the hat matrix. Mallows' $C_{p}$ (Cp) is a model selection criterion designed to provide an unbiased estimate of the in-sample prediction error at the observed design under the assumptions stated.\n\nYou are given a design with $n = 25$ observations and $p = 5$ columns in $X$: an intercept and four predictor columns $(x_{1}, x_{2}, x_{3}, x_{4})$. The predictors satisfy a perfect linear redundancy $x_{4} = 2 x_{1} - x_{3}$, and you may assume that the intercept column is linearly independent of the other predictor columns. You fit the redundant model by OLS using the pseudoinverse and obtain a residual sum of squares $\\mathrm{RSS} = 180$. The noise variance is known to be $\\sigma^{2} = 9$.\n\nTasks:\n1. Starting from the fundamental properties of linear least squares projections and unbiased risk estimation for linear smoothers, explain why the quantity $\\mathrm{tr}(H)$ equals the rank of $X$ and not the raw number of columns, even under perfect redundancy. In particular, argue that the penalty entering Mallows' $C_{p}$ depends on $\\mathrm{tr}(H)$ and therefore reflects rank deficiency.\n2. Determine the value of $\\mathrm{tr}(H)$ for the given $X$.\n3. Using your results, compute the value of Mallows' $C_{p}$ for this fitted model.\n\nProvide the final numerical value of $C_{p}$ as a single number. No rounding is required; give the exact value.",
            "solution": "The problem asks for an explanation of the role of the hat matrix trace in Mallows' $C_p$ under multicollinearity, and then to compute the value of $C_p$ for a given rank-deficient linear model.\n\n### Part 1: Explanation of $\\mathrm{tr}(H)$ and its role in Mallows' $C_p$\n\nThe fitted values in an Ordinary Least Squares (OLS) regression are given by $\\hat{y} = Hy$, where $H$ is the hat matrix. The matrix $H$ is the orthogonal projection matrix onto the column space of the design matrix $X$, denoted $\\mathrm{col}(X)$. When $X$ is of full column rank, $H = X(X^T X)^{-1}X^T$. In the general case, including a rank-deficient $X$, the hat matrix is defined using the Moore-Penrose pseudoinverse, $X^{+}$, as $H = XX^{+}$. In both cases, $H$ is a symmetric and idempotent ($H^2 = H$) projection matrix.\n\nA fundamental property of any projection matrix $P$ is that its trace is equal to its rank: $\\mathrm{tr}(P) = \\mathrm{rank}(P)$. Since $H$ is the projection matrix onto $\\mathrm{col}(X)$, its rank is the dimension of this subspace. By definition, the dimension of the column space of a matrix is the rank of that matrix. Therefore, we have the chain of equalities:\n$$\n\\mathrm{tr}(H) = \\mathrm{rank}(H) = \\mathrm{dim}(\\mathrm{col}(X)) = \\mathrm{rank}(X)\n$$\nThis relationship holds irrespective of whether $X$ has full rank or is rank-deficient. If $X \\in \\mathbb{R}^{n \\times p}$ has $\\mathrm{rank}(X) = r \\le p$, then $\\mathrm{tr}(H) = r$. The quantity $d = \\mathrm{tr}(H)$ is often referred to as the effective degrees of freedom of the linear fit.\n\nMallows' $C_p$ is designed to be an unbiased estimate of the in-sample mean squared prediction error, scaled by the noise variance $\\sigma^2$. The true in-sample mean squared error is the expected squared distance between the fitted values $\\hat{y}$ and the true mean vector $\\mu = E[y] = X\\beta$. Let's denote this error as $J$:\n$$\nJ = \\frac{1}{\\sigma^2} E\\left[ \\| \\hat{y} - \\mu \\|_2^2 \\right]\n$$\nWe can express $\\hat{y} - \\mu$ as $Hy - \\mu = H(X\\beta + \\varepsilon) - X\\beta = H(X\\beta) + H\\varepsilon - X\\beta$. Since $\\mu = X\\beta$ is in the column space of $X$, the projection $H$ leaves it unchanged, i.e., $H(X\\beta) = X\\beta$. Thus, $\\hat{y} - \\mu = H\\varepsilon$.\nThe expectation becomes:\n$$\nE\\left[ \\| H\\varepsilon \\|_2^2 \\right] = E\\left[ \\varepsilon^T H^T H \\varepsilon \\right] = E\\left[ \\varepsilon^T H \\varepsilon \\right]\n$$\nsince $H$ is idempotent ($H^2 = H$) and symmetric ($H^T=H$). Using the trace identity for quadratic forms of random vectors, $E[\\varepsilon^T A \\varepsilon] = \\mathrm{tr}(A \\cdot \\mathrm{Cov}(\\varepsilon)) + E[\\varepsilon]^T A E[\\varepsilon]$, and given $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, we have $E[\\varepsilon]=0$ and $\\mathrm{Cov}(\\varepsilon) = \\sigma^2 I_n$.\nThis gives:\n$$\nE\\left[ \\| H\\varepsilon \\|_2^2 \\right] = \\mathrm{tr}(H \\cdot \\sigma^2 I_n) = \\sigma^2 \\mathrm{tr}(H)\n$$\nSo, the true scaled in-sample prediction error is $J = \\frac{1}{\\sigma^2} (\\sigma^2 \\mathrm{tr}(H)) = \\mathrm{tr}(H) = d$.\n\nThe canonical definition of Mallows' $C_p$ statistic is:\n$$\nC_p = \\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\n$$\nwhere $\\mathrm{RSS} = \\|y - \\hat{y}\\|_2^2$ is the residual sum of squares and $d = \\mathrm{tr}(H)$. This statistic is an unbiased estimator for $J=d$. To see this, we compute its expectation. The RSS can be written as $\\mathrm{RSS} = \\|(I-H)y\\|^2 = \\|(I-H)(X\\beta+\\varepsilon)\\|^2 = \\|(I-H)\\varepsilon\\|^2$, since $(I-H)X\\beta = 0$. The expectation is:\n$$\nE[\\mathrm{RSS}] = E[\\|(I-H)\\varepsilon\\|^2] = \\sigma^2 \\mathrm{tr}(I-H) = \\sigma^2(n - \\mathrm{tr}(H)) = \\sigma^2(n-d)\n$$\nNow, taking the expectation of the $C_p$ statistic:\n$$\nE[C_p] = E\\left[\\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\\right] = \\frac{E[\\mathrm{RSS}]}{\\sigma^2} - n + 2d = \\frac{\\sigma^2(n-d)}{\\sigma^2} - n + 2d = (n-d) - n + 2d = d\n$$\nSince $E[C_p] = d = J$, the $C_p$ statistic is indeed an unbiased estimator of the scaled in-sample prediction error. The penalty term in this expression, $2d$, is twice the effective degrees of freedom, which is $2 \\cdot \\mathrm{tr}(H)$. This explicitly shows that the penalty correctly adjusts for rank deficiency by depending on $\\mathrm{rank}(X)$ rather than the total number of columns $p$.\n\n### Part 2: Determine the value of $\\mathrm{tr}(H)$\n\nWe need to find the rank of the design matrix $X$. The matrix $X$ has $n=25$ rows and $p=5$ columns. The columns correspond to an intercept and four predictors $(x_1, x_2, x_3, x_4)$.\nLet the columns of $X$ be $c_0, c_1, c_2, c_3, c_4$.\nWe are given a perfect linear dependency among the predictor columns:\n$$\nx_4 = 2x_1 - x_3\n$$\nThis means the column vector $c_4$ is a linear combination of columns $c_1$ and $c_3$: $c_4 = 2c_1 - c_3$. This relation implies that the set of five columns is linearly dependent. The rank of $X$ must therefore be less than $5$. This dependency reduces the rank by at least one.\n\nThe problem implies that this is the only linear dependency among the columns. Specifically, the set of columns $\\{c_0, c_1, c_2, c_3\\}$ that spans the column space of $X$ is assumed to be linearly independent.\nThe column space of $X$ is $\\mathrm{col}(X) = \\mathrm{span}\\{c_0, c_1, c_2, c_3, c_4\\}$. Since $c_4$ is in the span of $\\{c_1, c_3\\}$, we can write $\\mathrm{col}(X) = \\mathrm{span}\\{c_0, c_1, c_2, c_3\\}$.\nThe dimension of this space is the number of vectors in a basis. If $\\{c_0, c_1, c_2, c_3\\}$ are linearly independent, they form a basis for $\\mathrm{col}(X)$. In this case, the dimension of the column space is $4$.\nThus, the rank of the matrix $X$ is $4$.\n\nAs established in Part 1, $\\mathrm{tr}(H) = \\mathrm{rank}(X)$.\nTherefore, the value of $\\mathrm{tr}(H)$ is $4$.\n\n### Part 3: Compute the value of Mallows' $C_p$\n\nThe formula for Mallows' $C_p$ when the noise variance $\\sigma^2$ is known is:\n$$\nC_p = \\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\n$$\nwhere $d = \\mathrm{tr}(H)$.\n\nFrom the problem statement and our analysis, we have the following values:\n-   Residual Sum of Squares, $\\mathrm{RSS} = 180$.\n-   Noise variance, $\\sigma^2 = 9$.\n-   Number of observations, $n = 25$.\n-   Effective degrees of freedom, $d = \\mathrm{tr}(H) = 4$.\n\nSubstituting these values into the formula:\n$$\nC_p = \\frac{180}{9} - 25 + 2(4)\n$$\n$$\nC_p = 20 - 25 + 8\n$$\n$$\nC_p = -5 + 8\n$$\n$$\nC_p = 3\n$$\nThe value of Mallows' $C_p$ for this fitted model is $3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "With a solid understanding of the penalty term, we can now use the full $C_p$ statistic for its primary purpose: model selection. This hands-on coding exercise places you in the role of a data analyst deciding whether to increase model complexity by adding nonlinear features. You will use the change in $C_p$, denoted $\\Delta C_p$, to quantitatively balance the improved fit against the cost of additional parameters, a core task in applied statistics .",
            "id": "3143724",
            "problem": "Consider the supervised learning setting with a linear regression model. Let the response vector be $y \\in \\mathbb{R}^n$ and the predictor matrix be $X \\in \\mathbb{R}^{n \\times p}$, with an intercept term included in the model as a separate parameter. Assume the classic homoskedastic independent noise model where the errors are independent and identically distributed (i.i.d.) with mean $0$ and variance $\\sigma^2$. You are given an external estimate of the error variance, denoted by $\\hat{\\sigma}^2$, and you will use it to evaluate the effect of adding nonlinear terms to the model. Specifically, you will explore the sensitivity of the selection criterion known as Mallows' $C_p$ statistic when augmenting a linear model with squared predictor terms, and you will compute the change in $C_p$ when moving from the linear model to the quadratic-augmented model (i.e., adding $x_j^2$ for each original predictor $x_j$).\n\nYour task is to write a program that:\n- Fits the linear model using Ordinary Least Squares (OLS) to compute the training residual sum of squares, called the Residual Sum of Squares (RSS).\n- Fits the quadratic-augmented model using Ordinary Least Squares (OLS), where each original predictor $x_j$ is accompanied by its squared term $x_j^2$, again computing the training RSS.\n- Counts model parameters as the total number of estimated coefficients including the intercept. For a linear model with $p$ predictors, this is $1 + p$. For the quadratic-augmented model with squared terms for each predictor (no interaction terms), this is $1 + p + p = 1 + 2p$.\n- From fundamental definitions and properties of linear regression and error variance, derives Mallows' $C_p$ for each model and then computes the change in $C_p$,\n$$\\Delta C_p = C_p^{\\text{quad}} - C_p^{\\text{lin}}.$$\n- Decides whether adding the quadratic terms is warranted based on the computed $\\Delta C_p$ and the given $\\hat{\\sigma}^2$, using a principle-based decision rule derived from first principles.\n\nYou must generate the synthetic data according to the following well-specified test suite. In each case, generate $X$ with independent standard normal entries, i.e., each $x_{ij} \\sim \\mathcal{N}(0,1)$, and generate $y$ from a model that may contain both linear and quadratic contributions plus i.i.d. Gaussian noise. Use the specified random seed for reproducibility. In all cases below, the intercept is included in the data-generating process.\n\nTest Suite (each case provides $(\\text{seed}, n, p, \\text{intercept}, \\beta, \\gamma, \\hat{\\sigma}^2)$):\n- Case $1$: seed $= 123$, $n = 50$, $p = 2$, intercept $= 0.5$, linear coefficients $\\beta = [1.0, -2.0]$, quadratic coefficients $\\gamma = [0.0, 0.0]$, and $\\hat{\\sigma}^2 = 1.0$. The data-generating process is\n$$y = 0.5 + 1.0 \\cdot x_1 - 2.0 \\cdot x_2 + \\varepsilon,$$\nwith $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$.\n- Case $2$: seed $= 456$, $n = 50$, $p = 2$, intercept $= -0.2$, linear coefficients $\\beta = [1.0, -2.0]$, quadratic coefficients $\\gamma = [0.8, 0.0]$, and $\\hat{\\sigma}^2 = 1.0$. The data-generating process is\n$$y = -0.2 + 1.0 \\cdot x_1 - 2.0 \\cdot x_2 + 0.8 \\cdot x_1^2 + \\varepsilon,$$\nwith $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$.\n- Case $3$: seed $= 789$, $n = 200$, $p = 3$, intercept $= 0.0$, linear coefficients $\\beta = [0.5, -1.0, 0.0]$, quadratic coefficients $\\gamma = [0.2, 0.2, 0.0]$, and $\\hat{\\sigma}^2 = 1.0$. The data-generating process is\n$$y = 0.0 + 0.5 \\cdot x_1 - 1.0 \\cdot x_2 + 0.2 \\cdot x_1^2 + 0.2 \\cdot x_2^2 + \\varepsilon,$$\nwith $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$.\n- Case $4$: seed $= 42$, $n = 80$, $p = 1$, intercept $= 0.0$, linear coefficients $\\beta = [0.0]$, quadratic coefficients $\\gamma = [3.0]$, and $\\hat{\\sigma}^2 = 1.0$. The data-generating process is\n$$y = 0.0 + 3.0 \\cdot x_1^2 + \\varepsilon,$$\nwith $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$.\n\nImplementation details:\n- Use Ordinary Least Squares (OLS), defined as minimizing the training Residual Sum of Squares (RSS), and compute $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ for each fitted model.\n- Use only squared terms in the quadratic-augmented model (no interaction terms).\n- Use $\\hat{\\sigma}^2$ exactly as given in each test case.\n- Decide that quadratic terms are warranted precisely when $\\Delta C_p < 0$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the decisions for the four test cases as a comma-separated list of boolean values enclosed in square brackets (for example, `[True, False, True, False]`), where `True` means the quadratic terms are warranted and `False` means they are not.\n\nYour program must be entirely self-contained and must not read any input. It must generate the synthetic data as specified above and follow the rules outlined to produce the final output. The only allowed libraries are Numerical Python (NumPy) and the Python standard library.",
            "solution": "The problem at hand requires a quantitative comparison between a simple linear regression model and an augmented model containing quadratic terms. The objective is to decide whether the inclusion of these higher-order terms is justified for a given dataset. The decision will be based on Mallows' $C_p$ statistic, a well-established criterion for model selection that balances goodness-of-fit against model complexity.\n\nFirst, we formalize the mathematical setup. We are given a dataset consisting of $n$ observations. For each observation $i \\in \\{1, 2, \\dots, n\\}$, we have a response $y_i$ and $p$ predictor variables $x_{i1}, x_{i2}, \\dots, x_{ip}$. We can represent the data using a response vector $y \\in \\mathbb{R}^n$ and a predictor matrix $X \\in \\mathbb{R}^{n \\times p}$.\n\nWe consider two nested models for the relationship between the predictors and the response.\n\n1.  The Linear Model ($M_{\\text{lin}}$): This model assumes a linear relationship and includes an intercept term $\\beta_0$. The model equation is:\n    $$ y_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\varepsilon_i $$\n    The parameters to be estimated are the coefficients $\\beta_0, \\beta_1, \\dots, \\beta_p$. The total number of parameters in this model is $d_{\\text{lin}} = p + 1$.\n\n2.  The Quadratic-Augmented Model ($M_{\\text{quad}}$): This model extends the linear model by adding the square of each predictor as a new feature. Interaction terms like $x_{ij}x_{ik}$ for $j \\neq k$ are explicitly excluded. The model equation is:\n    $$ y_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\sum_{j=1}^{p} \\gamma_j x_{ij}^2 + \\varepsilon_i $$\n    The parameters to be estimated are $\\beta_0$, the linear coefficients $\\beta_1, \\dots, \\beta_p$, and the quadratic coefficients $\\gamma_1, \\dots, \\gamma_p$. The total number of parameters is $d_{\\text{quad}} = 1 + p + p = 2p + 1$.\n\nFor both models, the term $\\varepsilon_i$ represents the error, which is assumed to be an independent and identically distributed (i.i.d.) random variable with mean $E[\\varepsilon_i] = 0$ and variance $Var(\\varepsilon_i) = \\sigma^2$.\n\nThe parameters of these models are estimated using the method of Ordinary Least Squares (OLS). OLS finds the parameter values that minimize the Residual Sum of Squares (RSS), which is the sum of the squared differences between the observed responses $y_i$ and the predicted responses $\\hat{y}_i$.\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\nIn matrix notation, let $Z$ be the design matrix for a given model (which includes a leading column of ones for the intercept). For $M_{\\text{lin}}$, $Z_{\\text{lin}} \\in \\mathbb{R}^{n \\times (p+1)}$. For $M_{\\text{quad}}$, $Z_{\\text{quad}} \\in \\mathbb{R}^{n \\times (2p+1)}$. The OLS coefficient vector $\\hat{\\theta}$ is the solution to the normal equations:\n$$ (Z^T Z) \\hat{\\theta} = Z^T y $$\nThe predicted values are then $\\hat{y} = Z \\hat{\\theta}$, and the RSS is given by $\\|y - Z\\hat{\\theta}\\|_2^2$.\n\nSince $M_{\\text{quad}}$ has more parameters and includes all terms from $M_{\\text{lin}}$, it can always achieve a fit that is at least as good as $M_{\\text{lin}}$ on the training data. This means that $\\text{RSS}_{\\text{quad}} \\le \\text{RSS}_{\\text{lin}}$. A smaller RSS alone is not sufficient to prefer a more complex model, as this can lead to overfitting.\n\nMallows' $C_p$ addresses this issue by penalizing model complexity. It is an estimate of the scaled expected prediction error on new data. For a model with $d$ parameters, the $C_p$ statistic is defined as:\n$$ C_p = \\frac{\\text{RSS}}{\\hat{\\sigma}^2} + 2d - n $$\nHere, $\\hat{\\sigma}^2$ is an estimate of the true error variance $\\sigma^2$. A good model will have a $C_p$ value close to its number of parameters $d$. When comparing models, we prefer the model with the lower $C_p$ value.\n\nThe problem requires us to compute the change in $C_p$ when moving from the linear to the quadratic model:\n$$ \\Delta C_p = C_p^{\\text{quad}} - C_p^{\\text{lin}} $$\nSubstituting the definition of $C_p$ for each model:\n$$ \\Delta C_p = \\left( \\frac{\\text{RSS}_{\\text{quad}}}{\\hat{\\sigma}^2} + 2d_{\\text{quad}} - n \\right) - \\left( \\frac{\\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2d_{\\text{lin}} - n \\right) $$\n$$ \\Delta C_p = \\frac{\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2(d_{\\text{quad}} - d_{\\text{lin}}) $$\nWe have $d_{\\text{lin}} = p+1$ and $d_{\\text{quad}} = 2p+1$, so the change in the number of parameters is $d_{\\text{quad}} - d_{\\text{lin}} = (2p+1) - (p+1) = p$.\nThe final expression for the change in $C_p$ is:\n$$ \\Delta C_p = \\frac{\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2p $$\nThe decision rule is to select the quadratic-augmented model if it yields a lower $C_p$ value, which corresponds to the condition $\\Delta C_p < 0$. This condition can be rewritten as:\n$$ \\frac{\\text{RSS}_{\\text{lin}} - \\text{RSS}_{\\text{quad}}}{\\hat{\\sigma}^2} > 2p $$\nThis inequality provides a clear principle: the reduction in the residual sum of squares, scaled by the noise variance estimate, must be greater than twice the number of added parameters. This reflects the trade-off between bias and variance. A large reduction in RSS suggests the added quadratic terms are capturing real structure in the data (reducing bias), and if this reduction is large enough to overcome the complexity penalty, the new model is preferred.\n\nThe computational procedure for each test case is as follows:\n1.  Set the random seed for reproducibility.\n2.  Generate the $n \\times p$ predictor matrix $X$ with entries drawn from $\\mathcal{N}(0,1)$.\n3.  Generate the response vector $y$ using the specified data-generating process, which includes an intercept, linear terms, quadratic terms (if any), and Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ where $\\sigma^2=1.0$ for all test cases.\n4.  Construct the design matrix for the linear model, $Z_{\\text{lin}}$, by prepending a column of ones to $X$.\n5.  Perform OLS regression of $y$ on $Z_{\\text{lin}}$ to obtain $\\text{RSS}_{\\text{lin}}$.\n6.  Construct the design matrix for the quadratic model, $Z_{\\text{quad}}$, by prepending a column of ones to the matrix formed by concatenating $X$ and $X^2$ (element-wise square).\n7.  Perform OLS regression of $y$ on $Z_{\\text{quad}}$ to obtain $\\text{RSS}_{\\text{quad}}$.\n8.  Using the given values for $p$ and $\\hat{\\sigma}^2$, calculate $\\Delta C_p = (\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}) / \\hat{\\sigma}^2 + 2p$.\n9.  Determine if the quadratic terms are warranted by checking if $\\Delta C_p < 0$. Record the boolean result.\nThis procedure will be repeated for all four test cases to generate the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of deciding whether to augment a linear model with quadratic terms\n    based on the change in Mallows' Cp statistic for four specified test cases.\n    \"\"\"\n    \n    # Each tuple contains: (seed, n, p, intercept, beta, gamma, sigma_hat_sq)\n    test_cases = [\n        (123, 50, 2, 0.5, np.array([1.0, -2.0]), np.array([0.0, 0.0]), 1.0),\n        (456, 50, 2, -0.2, np.array([1.0, -2.0]), np.array([0.8, 0.0]), 1.0),\n        (789, 200, 3, 0.0, np.array([0.5, -1.0, 0.0]), np.array([0.2, 0.2, 0.0]), 1.0),\n        (42, 80, 1, 0.0, np.array([0.0]), np.array([3.0]), 1.0)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, n, p, intercept, beta, gamma, sigma_hat_sq = case\n\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate synthetic data\n        # Generate predictor matrix X with standard normal entries\n        X = np.random.randn(n, p)\n        \n        # Generate response vector y based on the true model\n        # The true error variance is 1.0 for all cases\n        true_sigma = 1.0\n        epsilon = np.random.randn(n) * true_sigma\n        \n        # Calculate the true mean response\n        mu = intercept + X @ beta + (X**2) @ gamma\n        \n        y = mu + epsilon\n\n        # 2. Fit the linear model\n        # Construct the design matrix with an intercept term\n        Z_lin = np.c_[np.ones(n), X]\n        \n        # Perform OLS using np.linalg.lstsq\n        # The function returns coefficients, RSS, rank, and singular values.\n        # The second return value is an array containing the RSS.\n        _, rss_lin_array, _, _ = np.linalg.lstsq(Z_lin, y, rcond=None)\n        rss_lin = rss_lin_array[0]\n\n        # 3. Fit the quadratic-augmented model\n        # Construct the design matrix with intercept, linear, and squared terms\n        X_quad_features = np.c_[X, X**2]\n        Z_quad = np.c_[np.ones(n), X_quad_features]\n        \n        # Perform OLS for the quadratic model\n        _, rss_quad_array, _, _ = np.linalg.lstsq(Z_quad, y, rcond=None)\n        rss_quad = rss_quad_array[0]\n\n        # 4. Compute Delta Cp and make a decision\n        # The change in the number of parameters is p\n        # delta_d = (2*p + 1) - (p + 1) = p\n        \n        # Mallows' Cp formula leads to: Delta_Cp = (RSS_quad - RSS_lin)/sigma_hat^2 + 2*p\n        delta_cp = (rss_quad - rss_lin) / sigma_hat_sq + 2 * p\n        \n        # Decision: quadratic terms are warranted if Delta_Cp < 0\n        is_warranted = delta_cp < 0\n        results.append(is_warranted)\n\n    # Format and print the final output\n    print(f\"[{', '.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Beyond its use as a formula for model comparison, what is the fundamental mathematical principle behind $C_p$ minimization? This practice reveals an elegant connection by asking you to derive a key theoretical result. For the idealized case of an orthogonal design, minimizing $C_p$ is mathematically equivalent to applying a specific hard-thresholding rule to the model coefficients, linking this classical method to modern ideas in high-dimensional statistics .",
            "id": "3143696",
            "problem": "Consider the fixed-design linear model $y = X \\beta + \\varepsilon$ with $y \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Assume the following setup to eliminate intercept complications and to simplify the geometry:\n- The response $y$ and each column $x_{j}$ of $X$ are centered so that an intercept is not included in the model.\n- The columns of $X$ are orthogonal and scaled so that $X^{\\top} X = n I_{p}$, that is, $\\langle x_{j}, x_{k} \\rangle = 0$ for $j \\neq k$ and $\\|x_{j}\\|^{2} = n$ for all $j \\in \\{1, \\dots, p\\}$.\n\nFor any subset $S \\subseteq \\{1, \\dots, p\\}$ with cardinality $|S| = k$, denote by $X_{S}$ the submatrix of $X$ with columns indexed by $S$, and let $\\hat{\\beta}_{S}$ be the Ordinary Least Squares (OLS) estimate obtained by regressing $y$ on $X_{S}$. Let $\\text{RSS}(S)$ denote the residual sum of squares of this subset fit. Let $\\hat{\\sigma}^{2}$ be a fixed estimate of $\\sigma^{2}$ obtained from the full $p$-variable model and treated as constant across all subsets.\n\nMallows' $C_{p}$ for model $S$ is defined by\n$$\nC_{p}(S) = \\frac{\\text{RSS}(S)}{\\hat{\\sigma}^{2}} - \\left(n - 2k\\right).\n$$\n\nWorking from first principles and using only the definitions above plus linear algebra of orthogonal projections and the OLS normal equations, derive the exact decision rule that results from minimizing $C_{p}(S)$ over all subsets $S$ when $X$ is orthogonal as specified. In particular, show that the $C_{p}$-minimizing subset is obtained by hard-thresholding the individual OLS coefficients $\\{ \\hat{\\beta}_{j} \\}_{j=1}^{p}$ by comparing $|\\hat{\\beta}_{j}|$ to a single variance-based threshold that does not depend on $j$.\n\nWhat is the exact closed-form expression (in terms of $n$ and $\\hat{\\sigma}^{2}$) for this universal threshold $T$ such that the $C_{p}$-minimizing rule includes predictor $j$ if and only if $|\\hat{\\beta}_{j}| > T$? Provide your final answer as a single analytic expression. No numerical rounding is required.",
            "solution": "The objective of this problem is to derive the decision rule for subset selection that minimizes Mallows' $C_p$ statistic under the specific condition of an orthogonal design matrix $X$. We must show that this rule is equivalent to hard-thresholding the Ordinary Least Squares (OLS) coefficients and find the explicit expression for this threshold.\n\nThe Mallows' $C_p$ statistic for a model with a subset of predictors $S \\subseteq \\{1, \\dots, p\\}$ of size $|S|=k$ is given by:\n$$\nC_{p}(S) = \\frac{\\text{RSS}(S)}{\\hat{\\sigma}^{2}} - (n - 2k)\n$$\nwhere $\\text{RSS}(S)$ is the residual sum of squares for the model using predictors in $S$, $n$ is the number of observations, and $\\hat{\\sigma}^{2}$ is a fixed estimate of the error variance $\\sigma^2$, typically from the full model. To minimize $C_p(S)$, we must first find a more convenient expression for $\\text{RSS}(S)$.\n\nThe problem states that the columns of the design matrix $X \\in \\mathbb{R}^{n \\times p}$ are orthogonal and scaled such that $X^{\\top} X = n I_{p}$. This property significantly simplifies the OLS calculations.\n\nFirst, consider the OLS estimates for the full model with all $p$ predictors. The coefficient estimates $\\hat{\\beta} \\in \\mathbb{R}^p$ are given by the normal equations:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top}y\n$$\nSubstituting the orthogonality condition $X^{\\top}X = n I_{p}$, we get:\n$$\n\\hat{\\beta} = (n I_{p})^{-1} X^{\\top}y = \\frac{1}{n} X^{\\top}y\n$$\nThe $j$-th component of this vector is $\\hat{\\beta}_j = \\frac{1}{n} x_j^{\\top}y$, where $x_j$ is the $j$-th column of $X$.\n\nNext, consider a subset model using predictors indexed by a set $S$, with $|S|=k$. The design matrix for this model is $X_S$. The OLS estimates for this subset model, $\\hat{\\beta}_S$, are given by:\n$$\n\\hat{\\beta}_S = (X_S^{\\top}X_S)^{-1} X_S^{\\top}y\n$$\nDue to the column orthogonality of $X$, the submatrix $X_S$ also has orthogonal columns. Thus, $X_S^{\\top}X_S = n I_{k}$. This leads to:\n$$\n\\hat{\\beta}_S = (n I_{k})^{-1} X_S^{\\top}y = \\frac{1}{n} X_S^{\\top}y\n$$\nA crucial consequence is that for any predictor $j \\in S$, its coefficient estimate in the subset model is $\\frac{1}{n} x_j^{\\top}y$, which is identical to its coefficient estimate $\\hat{\\beta}_j$ from the full model. This means that under orthogonality, subset selection does not alter the coefficients of the selected variables.\n\nNow, we derive an expression for $\\text{RSS}(S)$. The fitted values for the subset model are $\\hat{y}_S = X_S \\hat{\\beta}_S$. The residual sum of squares is $\\text{RSS}(S) = \\|y - \\hat{y}_S\\|^2$. Since $\\hat{y}_S$ is the orthogonal projection of $y$ onto the column space of $X_S$, by the Pythagorean theorem, we have $\\|y\\|^2 = \\|\\hat{y}_S\\|^2 + \\|y - \\hat{y}_S\\|^2$. Thus, $\\text{RSS}(S) = \\|y\\|^2 - \\|\\hat{y}_S\\|^2$.\nLet's compute $\\|\\hat{y}_S\\|^2$:\n$$\n\\|\\hat{y}_S\\|^2 = \\hat{y}_S^{\\top}\\hat{y}_S = (X_S \\hat{\\beta}_S)^{\\top}(X_S \\hat{\\beta}_S) = \\hat{\\beta}_S^{\\top} X_S^{\\top}X_S \\hat{\\beta}_S\n$$\nSubstituting $X_S^{\\top}X_S = n I_{k}$:\n$$\n\\|\\hat{y}_S\\|^2 = \\hat{\\beta}_S^{\\top} (n I_{k}) \\hat{\\beta}_S = n \\|\\hat{\\beta}_S\\|^2 = n \\sum_{j \\in S} (\\hat{\\beta}_j)^2\n$$\nNote that we use $\\hat{\\beta}_j$ to denote the coefficient from the full model, which we've shown is the same for $j \\in S$.\nSo, the residual sum of squares for the subset model is:\n$$\n\\text{RSS}(S) = \\|y\\|^2 - n \\sum_{j \\in S} \\hat{\\beta}_j^2\n$$\nNow we substitute this into the expression for $C_p(S)$:\n$$\nC_{p}(S) = \\frac{\\|y\\|^2 - n \\sum_{j \\in S} \\hat{\\beta}_j^2}{\\hat{\\sigma}^{2}} - (n - 2|S|)\n$$\nTo minimize $C_p(S)$, we need to choose the set $S$. We can reformulate this minimization problem by making an independent decision for each predictor $j \\in \\{1, \\dots, p\\}$. Let's introduce an indicator variable $\\delta_j \\in \\{0, 1\\}$ for each predictor, where $\\delta_j=1$ if predictor $j$ is included in the model (i.e., $j \\in S$) and $\\delta_j=0$ otherwise.\nWith this notation, $|S| = k = \\sum_{j=1}^{p} \\delta_j$ and $\\sum_{j \\in S} \\hat{\\beta}_j^2 = \\sum_{j=1}^{p} \\delta_j \\hat{\\beta}_j^2$.\nThe $C_p$ expression becomes:\n$$\nC_{p}(S) = \\frac{\\|y\\|^2 - n \\sum_{j=1}^{p} \\delta_j \\hat{\\beta}_j^2}{\\hat{\\sigma}^{2}} - n + 2 \\sum_{j=1}^{p} \\delta_j\n$$\nLet's rearrange the terms to isolate the parts dependent on the choice of $\\delta_j$:\n$$\nC_{p}(S) = \\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right) + \\sum_{j=1}^{p} \\left( 2 \\delta_j - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\delta_j \\right)\n$$\n$$\nC_{p}(S) = \\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right) + \\sum_{j=1}^{p} \\delta_j \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right)\n$$\nThe term $\\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right)$ is constant with respect to the choice of subset $S$. Therefore, to minimize $C_p(S)$, we must minimize the summation term. Since the decision for each $\\delta_j$ is independent of the others, we can minimize the sum by minimizing each term individually.\n\nFor each predictor $j$, we have two choices for $\\delta_j$:\n1.  Exclude predictor $j$: Set $\\delta_j = 0$. The contribution to the sum for this predictor is $0 \\times \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right) = 0$.\n2.  Include predictor $j$: Set $\\delta_j = 1$. The contribution to the sum for this predictor is $1 \\times \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right) = 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}$.\n\nWe will choose to include predictor $j$ (i.e., set $\\delta_j=1$) if and only if doing so reduces the total sum. This occurs when the contribution from including it is less than the contribution from excluding it:\n$$\n2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} < 0\n$$\nRearranging this inequality, we get the condition for including predictor $j$:\n$$\n2 < \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}\n$$\n$$\n\\hat{\\beta}_j^2 > \\frac{2 \\hat{\\sigma}^2}{n}\n$$\nTaking the square root of both sides (both are non-negative), we obtain the final decision rule:\n$$\n|\\hat{\\beta}_j| > \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}\n$$\nThis is a hard-thresholding rule. The $C_p$-minimizing subset $S$ consists of all predictors $j$ whose full-model OLS coefficient estimate $\\hat{\\beta}_j$ has a magnitude greater than a universal threshold $T = \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}$. This threshold depends only on the sample size $n$ and the variance estimate $\\hat{\\sigma}^2$, not on the specific predictor index $j$.\n\nThe exact closed-form expression for this threshold is therefore $T = \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}$.",
            "answer": "$$\\boxed{\\sqrt{\\frac{2 \\hat{\\sigma}^{2}}{n}}}$$"
        }
    ]
}