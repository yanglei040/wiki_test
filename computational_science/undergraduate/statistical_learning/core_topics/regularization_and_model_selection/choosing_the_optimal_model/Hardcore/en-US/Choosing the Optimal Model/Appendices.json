{
    "hands_on_practices": [
        {
            "introduction": "In statistical learning, a common question is 'Which model is the best?' This exercise demonstrates a fundamental principle: the answer depends entirely on your definition of 'best.' By evaluating a set of classifiers using different metrics—top-$k$ accuracy, the $F_1$-score under a budget, and AUROC—you will see firsthand how the optimal model changes based on the specific goals of the task. This practice is crucial for developing the critical thinking needed to align model selection with real-world application requirements.",
            "id": "3107729",
            "problem": "Consider a multiclass classification setting with $3$ classes labeled as $0$, $1$, and $2$. There are $N=8$ samples with true class labels given by the vector $y=(0,1,2,1,0,2,1,0)$. We evaluate three candidate models $\\mathcal{M}_0$, $\\mathcal{M}_1$, and $\\mathcal{M}_2$ that output, for each sample, a vector of nonnegative scores across classes used for ranking. For each model $\\mathcal{M}_m$, let $S^{(m)} \\in \\mathbb{R}^{8 \\times 3}$ denote its score matrix, where $S^{(m)}_{i,c}$ is the score assigned by model $m$ to class $c \\in \\{0,1,2\\}$ for sample $i \\in \\{0,1,\\dots,7\\}$. The score matrices are:\n\n- For $\\mathcal{M}_0$: rows are $(0.45,0.40,0.15)$, $(0.10,0.80,0.10)$, $(0.30,0.20,0.50)$, $(0.25,0.60,0.15)$, $(0.30,0.65,0.05)$, $(0.25,0.25,0.50)$, $(0.20,0.75,0.05)$, $(0.55,0.25,0.20)$.\n- For $\\mathcal{M}_1$: rows are $(0.35,0.55,0.10)$, $(0.30,0.50,0.20)$, $(0.20,0.15,0.65)$, $(0.15,0.70,0.15)$, $(0.60,0.25,0.15)$, $(0.10,0.20,0.70)$, $(0.05,0.85,0.10)$, $(0.65,0.20,0.15)$.\n- For $\\mathcal{M}_2$: rows are $(0.30,0.35,0.35)$, $(0.05,0.92,0.03)$, $(0.25,0.40,0.35)$, $(0.10,0.85,0.05)$, $(0.20,0.55,0.25)$, $(0.15,0.35,0.50)$, $(0.02,0.95,0.03)$, $(0.40,0.45,0.15)$.\n\nWe define the following evaluation metrics, grounded in core definitions:\n\n1. Top-$k$ accuracy: For a given $k \\in \\{1,2,3\\}$, define the top-$k$ set for sample $i$ as the set of $k$ classes with highest scores according to a tie-breaking rule described below. The top-$k$ accuracy of a model is the proportion of samples for which the true class $y_i$ lies within this top-$k$ set. The tie-breaking rule for class ranking is: first sort classes by descending score, and for any ties in score, sort tied classes by ascending class index.\n\n2. F1-score (F1): For binary evaluation of class $1$ versus the rest, define the predicted positive set as those samples whose class-$1$ score is above a threshold. For any threshold, define True Positives ($\\mathrm{TP}$), False Positives ($\\mathrm{FP}$), and False Negatives ($\\mathrm{FN}$) in the one-versus-rest sense for class $1$. The precision is $\\mathrm{Precision} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FP})$, the recall is $\\mathrm{Recall} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$, and the F1-score is $F1 = 2 \\cdot \\mathrm{Precision} \\cdot \\mathrm{Recall} / (\\mathrm{Precision}+\\mathrm{Recall})$, equivalently $F1 = \\dfrac{2 \\mathrm{TP}}{2 \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}}$. A use-case constraint imposes a resource budget on predicted positives: let $b \\in (0,1]$ be the allowed fraction of samples that may be flagged positive, so the predicted positives must be at most $\\lfloor b \\cdot N \\rfloor$. Under this constraint, the optimal threshold for maximizing $F1$ over all allowable thresholds will select the top $m$ samples by class-$1$ score for some $m \\in \\{0,1,\\dots,\\lfloor bN \\rfloor\\}$, subject to the same tie-breaking rule on indices when scores are equal.\n\n3. Area Under the Receiver Operating Characteristic (AUROC): For binary evaluation of class $1$ versus the rest, the Receiver Operating Characteristic (ROC) curve plots the True Positive Rate ($\\mathrm{TPR} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$) versus the False Positive Rate ($\\mathrm{FPR} = \\mathrm{FP}/(\\mathrm{FP}+\\mathrm{TN})$) across all thresholds on the class-$1$ scores. The Area Under the Receiver Operating Characteristic (AUROC) is the area under this curve, which can be computed equivalently via the Mann–Whitney statistic as the probability that a randomly chosen positive sample has a higher class-$1$ score than a randomly chosen negative sample, with ties counted as one-half.\n\nModel selection rule: For a given metric, the chosen model index is the smallest $m \\in \\{0,1,2\\}$ that achieves the maximal value of that metric among $\\{\\mathcal{M}_0,\\mathcal{M}_1,\\mathcal{M}_2\\}$, with ties broken by selecting the smallest index.\n\nYour program must compute, for each test case in the suite below:\n- The index of the model that maximizes top-$k$ accuracy.\n- The index of the model that maximizes $F1$ for class $1$ under the budget constraint $b$.\n- The index of the model that maximizes AUROC for class $1$.\n\nTest Suite:\n- Test case $1$: $k=1$, $b=0.5$.\n- Test case $2$: $k=2$, $b=0.25$.\n- Test case $3$: $k=2$, $b=0.125$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list of three integers in square brackets representing $[$top-$k$ model index, F1-under-$b$ model index, AUROC model index$]$. For example, the output should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$, with no spaces anywhere in the line.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of statistical learning, well-posed with all necessary data and unambiguous definitions, and objective in its formulation. We will proceed with a full solution.\n\nThe problem requires us to evaluate three models, $\\mathcal{M}_0$, $\\mathcal{M}_1$, and $\\mathcal{M}_2$, on a multiclass classification task with $N=8$ samples and $3$ classes. The true labels are given by the vector $y=(0,1,2,1,0,2,1,0)$. For each model, we are provided an $8 \\times 3$ score matrix $S^{(m)}$. We will compute three metrics for each model: top-$k$ accuracy, a budgeted F1-score for class $1$, and the Area Under the Receiver Operating Characteristic (AUROC) for class $1$. For each metric, we select the model with the highest score, breaking ties by choosing the smallest model index $m \\in \\{0,1,2\\}$.\n\nLet's denote the set of models as $\\{\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2\\}$. The score matrices $S^{(0)}, S^{(1)}, S^{(2)}$ and the label vector $y$ are the primary inputs.\n\n**1. Top-$k$ Accuracy Calculation**\n\nThe top-$k$ accuracy is the fraction of samples for which the true class label $y_i$ is among the $k$ classes with the highest scores. The ranking of classes for a sample $i$ is determined by sorting scores in descending order, with ties broken by ascending class index.\n\nLet $S^{(m)}_{i, \\cdot}$ be the vector of scores for sample $i$ from model $\\mathcal{M}_m$. We form pairs $(S^{(m)}_{i,c}, c)$ for $c \\in \\{0,1,2\\}$. These pairs are sorted first by score (descending) and then by class index $c$ (ascending). The set of top-$k$ predicted classes, $\\hat{Y}_{i,k}^{(m)}$, is the set of the first $k$ class indices from this sorted list.\n\nThe top-$k$ accuracy for model $\\mathcal{M}_m$ is given by:\n$$ \\text{Accuracy}_k(\\mathcal{M}_m) = \\frac{1}{N} \\sum_{i=0}^{N-1} I(y_i \\in \\hat{Y}_{i,k}^{(m)}) $$\nwhere $I(\\cdot)$ is the indicator function.\n\n**2. Budgeted F1-Score for Class 1**\n\nThis metric evaluates the models' ability to identify samples of class $1$ (positives) versus classes $0$ and $2$ (negatives). The set of true positive samples is $P = \\{i | y_i = 1\\} = \\{1, 3, 6\\}$, so the total number of positives is $N_P = 3$. The set of true negative samples is $N_{set} = \\{i | y_i \\neq 1\\} = \\{0, 2, 4, 5, 7\\}$, with $N_N = 5$.\n\nA budget constraint limits the number of samples that can be predicted as positive to at most $m_{max} = \\lfloor b \\cdot N \\rfloor$. To maximize the F1-score, we must find the optimal number of predicted positives, $m^*$, where $0 \\le m^* \\le m_{max}$. For each possible number of predictions $m_{pred} \\in \\{0, 1, \\dots, m_{max}\\}$, we select the top $m_{pred}$ samples based on their class-$1$ scores, $S^{(m)}_{\\cdot, 1}$. The ranking of samples is determined by sorting their class-$1$ scores in descending order, with ties broken by ascending sample index $i$.\n\nFor each $m_{pred}$, we compute:\n- $\\mathrm{TP}(m_{pred})$: Number of true positives among the top $m_{pred}$ samples.\n- $\\mathrm{FP}(m_{pred})$: Number of true negatives among the top $m_{pred}$ samples.\n- $\\mathrm{FN}(m_{pred}) = N_P - \\mathrm{TP}(m_{pred})$.\n\nThe F1-score for a given $m_{pred}$ is:\n$$ F1(m_{pred}) = \\frac{2 \\cdot \\mathrm{TP}(m_{pred})}{2 \\cdot \\mathrm{TP}(m_{pred}) + \\mathrm{FP}(m_{pred}) + \\mathrm{FN}(m_{pred})} $$\nThe F1-score for model $\\mathcal{M}_m$ under budget $b$ is the maximum of these values:\n$$ F1_b(\\mathcal{M}_m) = \\max_{m_{pred}=0, \\dots, m_{max}} F1(m_{pred}) $$\n\n**3. AUROC for Class 1**\n\nThe AUROC for the binary task of class $1$ versus the rest can be computed using the Mann-Whitney U statistic. It measures the probability that a randomly chosen positive sample has a higher score for class $1$ than a randomly chosen negative sample.\n\nLet $S_P^{(m)} = \\{S_{i,1}^{(m)} | i \\in P\\}$ be the set of class-$1$ scores for the positive samples and $S_N^{(m)} = \\{S_{j,1}^{(m)} | j \\in N_{set}\\}$ be the scores for the negative samples. The AUROC for model $\\mathcal{M}_m$ is:\n$$ \\text{AUROC}(\\mathcal{M}_m) = \\frac{1}{N_P \\cdot N_N} \\sum_{s_p \\in S_P^{(m)}} \\sum_{s_n \\in S_N^{(m)}} \\left( I(s_p > s_n) + 0.5 \\cdot I(s_p = s_n) \\right) $$\n\n**Model Selection**\nFor each test case and each metric, we calculate the metric's value for all three models, $\\{\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2\\}$. Let the computed values be $\\{v_0, v_1, v_2\\}$. The maximum value is $v_{max} = \\max(v_0, v_1, v_2)$. The chosen model index is $\\arg \\min \\{m | v_m = v_{max}\\}$.\n\n**Calculations for Test Cases**\n\nLet's compute the results for each test case.\n\n**Test Case 1: $k=1, b=0.5$**\n- $N=8$, so for F1-score, $m_{max} = \\lfloor 0.5 \\times 8 \\rfloor = 4$.\n- **Top-1 Accuracy:**\n  - $\\text{Acc}_1(\\mathcal{M}_0) = 7/8 = 0.875$\n  - $\\text{Acc}_1(\\mathcal{M}_1) = 7/8 = 0.875$\n  - $\\text{Acc}_1(\\mathcal{M}_2) = 4/8 = 0.5$\n  - Max is $0.875$, tied between $\\mathcal{M}_0$ and $\\mathcal{M}_1$. We select index $0$.\n- **F1-Score ($b=0.5$):**\n  - For $\\mathcal{M}_0$, max F1 is $6/7 \\approx 0.857$ (at $m_{pred}=4$).\n  - For $\\mathcal{M}_1$, max F1 is $6/7 \\approx 0.857$ (at $m_{pred}=4$).\n  - For $\\mathcal{M}_2$, max F1 is $1.0$ (at $m_{pred}=3$, where $\\mathrm{TP}=3, \\mathrm{FP}=0$).\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **AUROC:**\n  - $\\text{AUROC}(\\mathcal{M}_0) = 14/15 \\approx 0.9333$\n  - $\\text{AUROC}(\\mathcal{M}_1) = 14/15 \\approx 0.9333$\n  - $\\text{AUROC}(\\mathcal{M}_2) = 15/15 = 1.0$\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **Result for Test Case 1:** $[0, 2, 2]$\n\n**Test Case 2: $k=2, b=0.25$**\n- For F1-score, $m_{max} = \\lfloor 0.25 \\times 8 \\rfloor = 2$.\n- **Top-2 Accuracy:**\n  - $\\text{Acc}_2(\\mathcal{M}_0) = 8/8 = 1.0$\n  - $\\text{Acc}_2(\\mathcal{M}_1) = 8/8 = 1.0$\n  - $\\text{Acc}_2(\\mathcal{M}_2) = 6/8 = 0.75$\n  - Max is $1.0$, tied between $\\mathcal{M}_0$ and $\\mathcal{M}_1$. We select index $0$.\n- **F1-Score ($b=0.25$):**\n  - For $\\mathcal{M}_0$, max F1 is $0.8$ (at $m_{pred}=2$, $\\mathrm{TP}=2, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_1$, max F1 is $0.8$ (at $m_{pred}=2$, $\\mathrm{TP}=2, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_2$, max F1 is $0.8$ (at $m_{pred}=2$, $\\mathrm{TP}=2, \\mathrm{FP}=0$).\n  - All models score $0.8$. We select the smallest index, $0$.\n- **AUROC:** (This is independent of $k$ and $b$)\n  - Values are identical to Test Case 1.\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **Result for Test Case 2:** $[0, 0, 2]$\n\n**Test Case 3: $k=2, b=0.125$**\n- For F1-score, $m_{max} = \\lfloor 0.125 \\times 8 \\rfloor = 1$.\n- **Top-2 Accuracy:** (This is independent of $b$)\n  - Values are identical to Test Case 2.\n  - Max is $1.0$, tied between $\\mathcal{M}_0$ and $\\mathcal{M}_1$. We select index $0$.\n- **F1-Score ($b=0.125$):**\n  - For $\\mathcal{M}_0$, max F1 is $0.5$ (at $m_{pred}=1$, $\\mathrm{TP}=1, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_1$, max F1 is $0.5$ (at $m_{pred}=1$, $\\mathrm{TP}=1, \\mathrm{FP}=0$).\n  - For $\\mathcal{M}_2$, max F1 is $0.5$ (at $m_{pred}=1$, $\\mathrm{TP}=1, \\mathrm{FP}=0$).\n  - All models score $0.5$. We select the smallest index, $0$.\n- **AUROC:** (This is independent of $k$ and $b$)\n  - Values are identical to Test Case 1.\n  - Max is $1.0$, achieved by $\\mathcal{M}_2$. We select index $2$.\n- **Result for Test Case 3:** $[0, 0, 2]$\n\nThe program will implement these computations systematically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model evaluation problem by calculating three metrics for three models\n    across three test cases and selecting the best model for each.\n    \"\"\"\n    # Define problem givens\n    y_true = np.array([0, 1, 2, 1, 0, 2, 1, 0])\n    num_samples = len(y_true)\n    num_classes = 3\n\n    scores = [\n        np.array([\n            [0.45, 0.40, 0.15], [0.10, 0.80, 0.10], [0.30, 0.20, 0.50],\n            [0.25, 0.60, 0.15], [0.30, 0.65, 0.05], [0.25, 0.25, 0.50],\n            [0.20, 0.75, 0.05], [0.55, 0.25, 0.20]\n        ]),\n        np.array([\n            [0.35, 0.55, 0.10], [0.30, 0.50, 0.20], [0.20, 0.15, 0.65],\n            [0.15, 0.70, 0.15], [0.60, 0.25, 0.15], [0.10, 0.20, 0.70],\n            [0.05, 0.85, 0.10], [0.65, 0.20, 0.15]\n        ]),\n        np.array([\n            [0.30, 0.35, 0.35], [0.05, 0.92, 0.03], [0.25, 0.40, 0.35],\n            [0.10, 0.85, 0.05], [0.20, 0.55, 0.25], [0.15, 0.35, 0.50],\n            [0.02, 0.95, 0.03], [0.40, 0.45, 0.15]\n        ])\n    ]\n\n    test_cases = [\n        {'k': 1, 'b': 0.5},\n        {'k': 2, 'b': 0.25},\n        {'k': 2, 'b': 0.125}\n    ]\n\n    def select_best_model(metric_scores):\n        \"\"\"Selects model index with max score, breaking ties by smallest index.\"\"\"\n        max_score = np.max(metric_scores)\n        best_model_idx = np.where(metric_scores == max_score)[0][0]\n        return best_model_idx\n\n    # --- Metric Calculation Functions ---\n\n    def calculate_top_k_accuracy(model_scores, y_true, k, num_classes):\n        correct_predictions = 0\n        class_indices = np.arange(num_classes)\n        for i in range(len(y_true)):\n            sample_scores = model_scores[i]\n            # Tie-breaking: sort by score desc, then class index asc\n            sorted_indices = np.lexsort((class_indices, -sample_scores))\n            top_k_classes = sorted_indices[:k]\n            if y_true[i] in top_k_classes:\n                correct_predictions += 1\n        return correct_predictions / len(y_true)\n\n    def calculate_budgeted_f1(model_scores, y_true, b, num_samples):\n        class1_scores = model_scores[:, 1]\n        is_positive = (y_true == 1)\n        \n        sample_indices = np.arange(num_samples)\n        # Tie-breaking: sort samples by class 1 score desc, then sample index asc\n        ranked_sample_indices = np.lexsort((sample_indices, -class1_scores))\n        \n        num_positives_total = np.sum(is_positive)\n        max_predictions = int(np.floor(b * num_samples))\n        \n        max_f1 = 0.0\n        \n        for m_pred in range(max_predictions + 1):\n            if m_pred == 0:\n                tp = 0\n                fp = 0\n            else:\n                predicted_pos_indices = ranked_sample_indices[:m_pred]\n                tp = np.sum(is_positive[predicted_pos_indices])\n                fp = m_pred - tp\n            \n            fn = num_positives_total - tp\n            \n            denominator = 2 * tp + fp + fn\n            if denominator > 0:\n                f1 = (2 * tp) / denominator\n                if f1 > max_f1:\n                    max_f1 = f1\n        return max_f1\n\n    def calculate_auroc(model_scores, y_true):\n        class1_scores = model_scores[:, 1]\n        is_positive = (y_true == 1)\n        \n        pos_scores = class1_scores[is_positive]\n        neg_scores = class1_scores[~is_positive]\n        \n        if len(pos_scores) == 0 or len(neg_scores) == 0:\n            return 0.5 \n\n        numerator = 0.0\n        for p_score in pos_scores:\n            for n_score in neg_scores:\n                if p_score > n_score:\n                    numerator += 1.0\n                elif p_score == n_score:\n                    numerator += 0.5\n        \n        denominator = len(pos_scores) * len(neg_scores)\n        return numerator / denominator\n\n    # --- Main Loop ---\n    \n    all_results = []\n    \n    # Pre-calculate AUROC as it's independent of test cases\n    auroc_scores = np.array([calculate_auroc(s, y_true) for s in scores])\n    best_auroc_model = select_best_model(auroc_scores)\n\n    for case in test_cases:\n        k = case['k']\n        b = case['b']\n        \n        # Top-k accuracy\n        top_k_scores = np.array([\n            calculate_top_k_accuracy(s, y_true, k, num_classes) for s in scores\n        ])\n        best_top_k_model = select_best_model(top_k_scores)\n        \n        # Budgeted F1\n        f1_scores = np.array([\n            calculate_budgeted_f1(s, y_true, b, num_samples) for s in scores\n        ])\n        best_f1_model = select_best_model(f1_scores)\n        \n        case_results = [best_top_k_model, best_f1_model, best_auroc_model]\n        all_results.append(case_results)\n\n    # Format and print the final output\n    output_str = f\"[{','.join([f'[{r[0]},{r[1]},{r[2]}]' for r in all_results])}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond standard metrics, this practice challenges you to select a model by directly optimizing for a measure of real-world value. You will work with a cost matrix to define a custom utility function, reflecting the fact that different types of errors have different consequences. A key part of this exercise is learning to jointly tune the model's hyperparameters and the decision threshold within a cross-validation framework, a vital skill for deploying classifiers that deliver maximum practical impact.",
            "id": "3107638",
            "problem": "You are given three independent binary classification tasks. For each task, your goal is to implement a principled procedure that jointly selects a model and a decision threshold by maximizing an estimate of expected utility induced by a specified cost matrix. The program you produce must be a complete, runnable program that carries out this procedure and outputs the final test-set utilities for all tasks in a single line.\n\nFundamental base. Use the following foundational definitions:\n- A binary classifier maps feature vectors $x \\in \\mathbb{R}^d$ to a real-valued score $s(x) \\in \\mathbb{R}$, which is then interpreted as a probability estimate $p(y=1 \\mid x) \\in [0,1]$ using the logistic function. Assume the probabilistic linear model known as logistic regression with $\\ell_2$ regularization is used to fit $p(y=1 \\mid x)$.\n- A decision rule with threshold $t \\in [0,1]$ predicts $\\hat{y}(x;t)=\\mathbf{1}\\{p(y=1 \\mid x) \\ge t\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- A cost matrix $C \\in \\mathbb{R}^{2 \\times 2}$ assigns a nonnegative cost $C_{ij}$ to predicting class $j \\in \\{0,1\\}$ when the true class is $i \\in \\{0,1\\}$. Define the utility matrix $U$ by $U_{ij}=-C_{ij}$. The expected utility of a classifier-threshold pair on a dataset $\\{(x_n,y_n)\\}_{n=1}^N$ is the empirical average $\\frac{1}{N}\\sum_{n=1}^N U_{y_n,\\hat{y}(x_n;t)}$.\n- $K$-fold Cross-Validation (Cross-Validation (CV)) partitions the training set into $K$ disjoint folds of approximately equal size, trains on $K-1$ folds, validates on the held-out fold, and averages the validation criterion across folds to estimate generalization performance.\n\nTask. For each case below:\n1. Use $K$-fold Cross-Validation with $K = 3$ to jointly select the regularization strength $\\lambda$ of logistic regression and the decision threshold $t$ by maximizing the average validation expected utility. The search sets are:\n   - Regularization candidates $\\Lambda=\\{\\lambda_1,\\lambda_2,\\lambda_3\\}=\\{\\,0.0,\\,0.1,\\,1.0\\,\\}$.\n   - Threshold grid $\\mathcal{T}=\\{\\,0.00,\\,0.05,\\,0.10,\\,\\dots,\\,0.95,\\,1.00\\,\\}$.\n   Use deterministic folds: assign example index $n$ (zero-based) to fold $n \\bmod K$.\n   Break ties by choosing the smallest $\\lambda$ among maximizers, and then the smallest $t$ among the remaining maximizers.\n2. Retrain logistic regression on the full training set using the selected $\\lambda$, then evaluate the expected utility on the provided test set using the selected threshold $t$.\n3. Report, for each case, the test-set expected utility as a float rounded to $6$ decimal places. Do not report intermediate values or parameters.\n\nDatasets and cost matrices.\n\nCase A:\n- Training features $X_{\\text{train}} \\in \\mathbb{R}^{6 \\times 2}$ and labels $y_{\\text{train}} \\in \\{0,1\\}^6$:\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  0.0  0.0\\\\\n  0.2  0.1\\\\\n  0.4  0.2\\\\\n  0.6  0.8\\\\\n  0.8  0.7\\\\\n  1.0  0.9\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Test features $X_{\\text{test}} \\in \\mathbb{R}^{2 \\times 2}$ and labels $y_{\\text{test}} \\in \\{0,1\\}^2$:\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  0.3  0.2\\\\\n  0.7  0.75\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  0\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Cost matrix $C=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$.\n\nCase B:\n- Training features and labels:\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  -1.0  -1.0\\\\\n  -0.8  -0.6\\\\\n  -0.6  -0.8\\\\\n  0.5  0.4\\\\\n  0.6  0.3\\\\\n  0.2  0.1\\\\\n  -0.2  0.0\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Test features and labels:\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  -0.7  -0.7\\\\\n  0.55  0.35\\\\\n  0.0  0.1\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  0\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Cost matrix $C=\\begin{bmatrix}0  1\\\\ 4  0\\end{bmatrix}$.\n\nCase C:\n- Training features and labels:\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  0.0  1.0\\\\\n  0.1  0.9\\\\\n  0.9  0.1\\\\\n  1.0  0.0\\\\\n  0.45  0.55\\\\\n  0.55  0.45\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  0\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- Test features and labels:\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  0.52  0.48\\\\\n  0.48  0.52\\\\\n  0.6  0.4\\\\\n  0.4  0.6\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  1\\\\\n  0\\\\\n  1\\\\\n  0\n  \\end{bmatrix}.\n  $$\n- Cost matrix $C=\\begin{bmatrix}0  3\\\\ 1  0\\end{bmatrix}$.\n\nImplementation requirements.\n- Use logistic regression with $\\ell_2$ regularization and a bias term, trained by gradient-based optimization on the regularized negative log-likelihood. You may assume labels in $\\{0,1\\}$ and use the logistic link.\n- For each threshold $t \\in \\mathcal{T}$ and $\\lambda \\in \\Lambda$, estimate the mean validation expected utility across the $K$ folds, then select the pair $(\\lambda^\\star,t^\\star)$ that maximizes this quantity under the tie-breaking rule given above.\n- After selection, retrain on the full training set with $\\lambda^\\star$, then evaluate the mean test-set expected utility using $t^\\star$.\n- Angle units are not applicable. There are no physical units. All reported expected utilities must be decimals, rounded to $6$ digits after the decimal point.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, for example, $[u_A,u_B,u_C]$, where each $u_\\cdot$ is the rounded test-set expected utility as specified above.",
            "solution": "We must design a principled joint model-and-threshold selection scheme based on expected utility derived from a cost matrix. The fundamental base consists of the probabilistic interpretation of a binary classifier, the construction of a decision rule via thresholding, cost-to-utility conversion, and Cross-Validation (CV) as an estimator of out-of-sample performance.\n\nFirst, consider a binary classification setting with inputs $x \\in \\mathbb{R}^d$ and labels $y \\in \\{0,1\\}$. A probabilistic linear classifier under the Bernoulli model uses the logistic link to produce $p_\\theta(y=1\\mid x)=\\sigma(w^\\top x + b)$, where $\\sigma(z)=\\frac{1}{1+e^{-z}}$, parameters $\\theta=(w,b)$ with $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$, and regularization strength $\\lambda \\ge 0$ controls the $\\ell_2$ penalty on $w$. Training proceeds by minimizing the regularized negative log-likelihood over the training data $\\{(x_n,y_n)\\}_{n=1}^N$, which is a convex objective in $(w,b)$ and has a unique minimizer for fixed $\\lambda$.\n\nSecond, decision-making requires a threshold $t \\in [0,1]$ to convert probabilities into class predictions: $\\hat{y}(x;t)=\\mathbf{1}\\{\\sigma(w^\\top x+b)\\ge t\\}$. The consequences of decisions are captured by a cost matrix $C \\in \\mathbb{R}^{2\\times 2}$, where $C_{ij}$ is the cost incurred when the true class is $i$ and the prediction is $j$. We define the utility matrix $U$ by $U=-C$, so maximizing expected utility is equivalent to minimizing expected cost. Given a dataset, the empirical expected utility of $(\\theta,t)$ is\n$$\n\\bar{u}(\\theta,t) \\;=\\; \\frac{1}{N}\\sum_{n=1}^N U_{y_n,\\,\\hat{y}(x_n;t)}.\n$$\nThis quantities rewards correct decisions according to the utility matrix and penalizes mistakes according to the negative costs.\n\nThird, to choose the optimal model and threshold, we require an unbiased estimate of out-of-sample expected utility. $K$-fold Cross-Validation (CV) supplies this by partitioning the training set indices into $K$ folds $\\{\\mathcal{I}_k\\}_{k=0}^{K-1}$, training on $\\bigcup_{j\\ne k}\\mathcal{I}_j$ and validating on $\\mathcal{I}_k$. For any candidate $\\lambda$ and $t$, define\n$$\n\\widehat{u}_{\\text{CV}}(\\lambda,t) \\;=\\; \\frac{1}{K}\\sum_{k=0}^{K-1} \\frac{1}{|\\mathcal{I}_k|}\\sum_{n\\in \\mathcal{I}_k} U_{y_n,\\,\\hat{y}_{\\lambda,k}(x_n;t)},\n$$\nwhere $\\hat{y}_{\\lambda,k}$ is the decision rule obtained by training logistic regression with regularization $\\lambda$ on the training folds $\\bigcup_{j\\ne k}\\mathcal{I}_j$ and applying the threshold $t$ to its probabilistic outputs on the validation fold $\\mathcal{I}_k$. The pair $(\\lambda^\\star,t^\\star)$ is then chosen to maximize $\\widehat{u}_{\\text{CV}}(\\lambda,t)$ over the search grids. Ties are broken by selecting the smallest $\\lambda$ and, among those, the smallest $t$. Selecting $t$ within CV is essential because the validation probabilities depend on the trained model, and choosing $t$ on the same validation data as used to evaluate utility ensures that the threshold choice is tuned to generalize, reducing optimistic bias that would occur if $t$ were tuned on the final test set.\n\nAlgorithmic steps:\n1. For each case, build a deterministic $K$-fold partition by placing index $n$ into fold $n \\bmod K$, ensuring reproducibility and coverage of edge indices.\n2. For each $\\lambda \\in \\{0.0,0.1,1.0\\}$:\n   - For each fold $k \\in \\{0,1,2\\}$:\n     - Train logistic regression with $\\ell_2$ penalty $\\lambda$ on the union of the other folds, minimizing the regularized negative log-likelihood using gradient-based optimization. The gradient for $w$ combines the average residual term with the regularization; the gradient for $b$ is the average residual. Iterative updates $w \\leftarrow w - \\eta \\nabla_w$, $b \\leftarrow b - \\eta \\nabla_b$ reduce the convex objective, where $\\eta0$ is a learning rate.\n     - Compute probabilistic predictions on the validation fold.\n   - For each threshold $t \\in \\{0.00,0.05,\\dots,1.00\\}$, convert the validation probabilities to predictions and compute the per-fold utilities, then average across folds to obtain $\\widehat{u}_{\\text{CV}}(\\lambda,t)$.\n3. Choose $(\\lambda^\\star,t^\\star)$ maximizing the Cross-Validation estimate, using the specified tie-breaking rule.\n4. Retrain logistic regression on the full training set using $\\lambda^\\star$, compute probabilistic predictions on the test set, apply $t^\\star$ to obtain decisions, and compute the mean test-set expected utility $\\bar{u}_{\\text{test}}$.\n5. Round each $\\bar{u}_{\\text{test}}$ to $6$ decimal places.\n\nDesign coverage rationale:\n- The threshold grid includes the boundary values $t=0$ and $t=1$, capturing edge cases where all examples are predicted as positive or negative, respectively, which is important under extreme cost asymmetries.\n- The regularization grid spans no regularization ($\\lambda=0.0$), mild ($\\lambda=0.1$), and strong ($\\lambda=1.0$) to test bias-variance trade-offs.\n- The datasets cover: balanced costs (Case A), asymmetric costs penalizing false negatives more (Case B), and asymmetric costs penalizing false positives more (Case C). This ensures that threshold shifts in opposite directions are exercised.\n\nThe program implements the above algorithm precisely, ensuring deterministic folds, joint selection of model complexity and threshold, retraining on the full training set after selection, and computing the final outputs $[u_A,u_B,u_C]$ in the required format. Each utility is a decimal (not a percentage), rounded to $6$ places, as specified.",
            "answer": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef train_logistic_regression(X, y, lambda_reg=0.0, lr=0.1, iters=3000):\n    \"\"\"\n    Train logistic regression with L2 regularization on weights (not bias).\n    X: (n_samples, n_features)\n    y: (n_samples,) in {0,1}\n    Returns weights w (n_features,) and bias b.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d, dtype=float)\n    b = 0.0\n    for _ in range(iters):\n        z = X @ w + b\n        p = sigmoid(z)\n        # gradient\n        residual = (p - y)  # shape (n,)\n        grad_w = (X.T @ residual) / n + lambda_reg * w\n        grad_b = np.sum(residual) / n\n        # update\n        w -= lr * grad_w\n        b -= lr * grad_b\n    return w, b\n\ndef predict_proba(X, w, b):\n    return sigmoid(X @ w + b)\n\ndef expected_utility_from_probs(y_true, y_prob, threshold, cost_matrix):\n    \"\"\"\n    y_true: (n,) in {0,1}\n    y_prob: (n,) in [0,1]\n    threshold: float in [0,1]\n    cost_matrix: 2x2 numpy array\n    Returns mean utility (negative cost).\n    \"\"\"\n    y_pred = (y_prob = threshold).astype(int)\n    # Utility matrix is negative of cost matrix\n    U = -cost_matrix\n    # Map each pair (y_true[i], y_pred[i]) to utility\n    utilities = U[y_true, y_pred]\n    return float(np.mean(utilities))\n\ndef kfold_indices(n, K):\n    \"\"\"\n    Deterministic K-fold split by index modulo K.\n    Returns list of folds, each is a numpy array of indices.\n    \"\"\"\n    folds = [[] for _ in range(K)]\n    for idx in range(n):\n        folds[idx % K].append(idx)\n    return [np.array(f, dtype=int) for f in folds]\n\ndef joint_cv_select_threshold_and_lambda(X, y, cost_matrix, lambdas, thresholds, K=3, lr=0.1, iters=3000):\n    \"\"\"\n    Perform K-fold CV to jointly select lambda and threshold maximizing expected utility.\n    Returns selected (lambda_star, threshold_star).\n    Tie-breaking: smallest lambda, then smallest threshold.\n    \"\"\"\n    n = X.shape[0]\n    folds = kfold_indices(n, K)\n    # Precompute folds' train/val splits\n    fold_train_val = []\n    all_indices = np.arange(n, dtype=int)\n    for val_idx in folds:\n        train_idx = np.setdiff1d(all_indices, val_idx, assume_unique=True)\n        fold_train_val.append((train_idx, val_idx))\n\n    best_u = -np.inf\n    best_lambda = None\n    best_t = None\n\n    # Iterate lambdas in ascending order for tie-breaking\n    for lam in sorted(lambdas):\n        # For each fold, train model and store validation probabilities\n        val_probs_per_fold = []\n        y_val_per_fold = []\n        for (tr_idx, va_idx) in fold_train_val:\n            w, b = train_logistic_regression(X[tr_idx], y[tr_idx], lambda_reg=lam, lr=lr, iters=iters)\n            probs = predict_proba(X[va_idx], w, b)\n            val_probs_per_fold.append(probs)\n            y_val_per_fold.append(y[va_idx])\n        # Concatenate for averaging across folds per threshold\n        # However, to respect equal weighting per example, we compute fold-wise means and average,\n        # as specified in the problem statement.\n        # We'll compute per-fold utilities for each threshold and average across folds.\n        for t in sorted(thresholds):\n            u_folds = []\n            for probs, yv in zip(val_probs_per_fold, y_val_per_fold):\n                u = expected_utility_from_probs(yv, probs, t, cost_matrix)\n                u_folds.append(u)\n            u_cv = float(np.mean(u_folds))\n            if (u_cv > best_u) or (np.isclose(u_cv, best_u) and (best_lambda is not None) and (lam  best_lambda)) or (np.isclose(u_cv, best_u) and lam == best_lambda and (best_t is not None) and (t  best_t)):\n                best_u = u_cv\n                best_lambda = lam\n                best_t = t\n\n    return best_lambda, best_t\n\ndef solve():\n    # Define threshold grid and lambda grid\n    thresholds = np.round(np.linspace(0.0, 1.0, 21), 2)  # 0.00, 0.05, ..., 1.00\n    lambdas = [0.0, 0.1, 1.0]\n    K = 3\n\n    # Case A\n    X_train_A = np.array([\n        [0.0, 0.0],\n        [0.2, 0.1],\n        [0.4, 0.2],\n        [0.6, 0.8],\n        [0.8, 0.7],\n        [1.0, 0.9],\n    ], dtype=float)\n    y_train_A = np.array([0, 0, 0, 1, 1, 1], dtype=int)\n    X_test_A = np.array([\n        [0.3, 0.2],\n        [0.7, 0.75],\n    ], dtype=float)\n    y_test_A = np.array([0, 1], dtype=int)\n    C_A = np.array([[0.0, 1.0],\n                    [1.0, 0.0]], dtype=float)\n\n    # Case B\n    X_train_B = np.array([\n        [-1.0, -1.0],\n        [-0.8, -0.6],\n        [-0.6, -0.8],\n        [0.5, 0.4],\n        [0.6, 0.3],\n        [0.2, 0.1],\n        [-0.2, 0.0],\n    ], dtype=float)\n    y_train_B = np.array([0, 0, 0, 1, 1, 1, 1], dtype=int)\n    X_test_B = np.array([\n        [-0.7, -0.7],\n        [0.55, 0.35],\n        [0.0, 0.1],\n    ], dtype=float)\n    y_test_B = np.array([0, 1, 1], dtype=int)\n    C_B = np.array([[0.0, 1.0],\n                    [4.0, 0.0]], dtype=float)\n\n    # Case C\n    X_train_C = np.array([\n        [0.0, 1.0],\n        [0.1, 0.9],\n        [0.9, 0.1],\n        [1.0, 0.0],\n        [0.45, 0.55],\n        [0.55, 0.45],\n    ], dtype=float)\n    y_train_C = np.array([0, 0, 1, 1, 0, 1], dtype=int)\n    X_test_C = np.array([\n        [0.52, 0.48],\n        [0.48, 0.52],\n        [0.6, 0.4],\n        [0.4, 0.6],\n    ], dtype=float)\n    y_test_C = np.array([1, 0, 1, 0], dtype=int)\n    C_C = np.array([[0.0, 3.0],\n                    [1.0, 0.0]], dtype=float)\n\n    cases = [\n        (X_train_A, y_train_A, X_test_A, y_test_A, C_A),\n        (X_train_B, y_train_B, X_test_B, y_test_B, C_B),\n        (X_train_C, y_train_C, X_test_C, y_test_C, C_C),\n    ]\n\n    results = []\n    # Training parameters (chosen to ensure convergence on small datasets)\n    lr = 0.2\n    iters = 4000\n\n    for X_tr, y_tr, X_te, y_te, C in cases:\n        lam_star, t_star = joint_cv_select_threshold_and_lambda(\n            X_tr, y_tr, C, lambdas, thresholds, K=K, lr=lr, iters=iters\n        )\n        w, b = train_logistic_regression(X_tr, y_tr, lambda_reg=lam_star, lr=lr, iters=iters)\n        probs_test = predict_proba(X_te, w, b)\n        u_test = expected_utility_from_probs(y_te, probs_test, t_star, C)\n        # Round to 6 decimal places\n        results.append(f\"{u_test:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Optimal model selection also involves choosing the right level of complexity and the right structural assumptions for your model. This regression exercise provides a hands-on exploration of the classic bias-variance trade-off. By comparing a model with a strong monotonicity constraint (isotonic regression) against a highly flexible one (a cubic spline), you will discover how a model's performance depends on the interplay between its inherent assumptions, the true nature of the data, and the level of noise.",
            "id": "3107704",
            "problem": "You are given a regression selection task grounded in empirical risk minimization within statistical learning. The underlying signal has a globally monotone trend with a localized non-monotonic region. Your objective is to implement two model families and choose the optimal model via cross-validation, comparing a monotone constraint model against a flexible spline model. The target is to output, for each test case, the index of the better-performing model as a single list.\n\nFoundational base:\n- The data are independent and identically distributed samples $\\{(x_i, y_i)\\}_{i=1}^n$ where $x_i \\in [0,1]$, $y_i = f(x_i) + \\varepsilon_i$, and the noise $\\varepsilon_i$ is Gaussian with mean $0$ and variance $\\sigma^2$.\n- The learning goal is to approximate the regression function $f(x)$ by minimizing expected squared prediction risk $R(f) = \\mathbb{E}[(Y - f(X))^2]$.\n- Since the distribution is unknown, use $K$-fold cross-validation to estimate generalization error and perform model selection by empirical risk minimization.\n\nModels to implement and compare:\n- Monotone constraint model: isotonic regression, which estimates a nondecreasing function $\\hat{f}_{\\mathrm{iso}}$ using the pool-adjacent-violators algorithm on the training data ordered by $x$. For prediction at new $x$, evaluate the learned step function as left-constant between observed training points.\n- Flexible model: cubic smoothing spline, which estimates a smooth function $\\hat{f}_{\\mathrm{spline}}$ with smoothing parameter $s$ controlling the trade-off between fidelity and smoothness.\n\nData generation for each test case:\n- Generate $x_i$ as evenly spaced points on $[0,1]$.\n- Define the underlying function as $f(x) = x + b(x)$ where the bump $b(x)$ introduces a local non-monotonicity:\n$$\nb(x) = \\begin{cases}\nA \\sin\\!\\left(\\dfrac{\\pi (x - x_0)}{w}\\right),  \\text{if } |x - x_0| \\le w,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\n- Generate $y_i = f(x_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nCross-validation and model selection protocol:\n- Use $K$-fold cross-validation with $K = 5$, partitioning the index set into $5$ contiguous, approximately equal folds in the $x$-sorted order.\n- For isotonic regression, compute the mean squared error (MSE) across folds directly.\n- For the cubic smoothing spline, evaluate a grid of smoothing parameters and choose the one that yields the lowest cross-validated MSE. Define the grid via a scale $S = \\operatorname{Var}(y) \\cdot n$ and consider $s \\in \\{0, 0.05 S, 0.2 S, 1 S, 5 S, 20 S\\}$.\n- Choose the model with the smallest cross-validated MSE among the isotonic regression and the best spline from the grid.\n\nAnswer representation:\n- For each test case, output the selected model as an integer: output $0$ if isotonic regression is selected, and $1$ if a spline is selected.\n\nTest suite:\n- Each test case is a tuple $(n, \\sigma, A, w, x_0, \\text{seed})$.\n- Use the following cases:\n    1. $(200, 0.05, 0.0, 0.12, 0.60, 42)$: pure monotone baseline ($A = 0$).\n    2. $(200, 0.05, 0.20, 0.12, 0.60, 43)$: small bump on predominantly monotone trend.\n    3. $(200, 0.05, 0.80, 0.12, 0.60, 44)$: pronounced local non-monotonicity.\n    4. $(200, 0.50, 0.80, 0.12, 0.60, 45)$: pronounced bump under high noise.\n    5. $(30, 0.10, 0.50, 0.15, 0.55, 46)$: small sample with a local bump.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i \\in \\{0,1\\}$ corresponds to the selected model for the $i$-th test case.",
            "solution": "The problem requires a comparative analysis of two regression models—isotonic regression and a cubic smoothing spline—on a specified set of regression tasks. The core of the problem lies in model selection via cross-validation, a fundamental technique in statistical learning for estimating the generalization error of a model and selecting the one that best balances bias and variance.\n\nThe problem is deemed valid as it is scientifically grounded in established statistical principles, well-posed with all necessary information provided for a reproducible computational experiment, and objective in its formulation and evaluation criteria.\n\nThe step-by-step methodology is as follows:\n\n**1. Data Generation**\n\nFor each test case, defined by a tuple of parameters $(n, \\sigma, A, w, x_0, \\text{seed})$, we generate a dataset of size $n$.\n- A `Numpy` random number generator is initialized with the given `seed` to ensure reproducibility.\n- The predictor variable $x$ consists of $n$ equally spaced points in the interval $[0, 1]$.\n- The true underlying regression function $f(x)$ is defined as the sum of a linear trend and a localized sinusoidal \"bump\": $f(x) = x + b(x)$, where\n$$\nb(x) = \\begin{cases}\nA \\sin\\!\\left(\\dfrac{\\pi (x - x_0)}{w}\\right),  \\text{if } |x - x_0| \\le w,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nThe parameter $A$ controls the amplitude of the bump, thereby controlling the degree of local non-monotonicity. When $A=0$, the function $f(x)=x$ is purely monotonic.\n- The observed response variable $y_i$ is generated by adding Gaussian noise to the true function value: $y_i = f(x_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\n**2. Cross-Validation Framework**\n\nTo estimate the generalization mean squared error (MSE) for each model, we employ $K$-fold cross-validation with $K = 5$.\n- The dataset, which is naturally ordered by the values of $x_i$, is partitioned into $K=5$ contiguous, non-overlapping folds.\n- For each fold $k \\in \\{1, \\dots, 5\\}$, the $k$-th fold serves as the validation set, and the remaining $K-1$ folds constitute the training set.\n- A model is trained on the training set and its performance is evaluated by computing the mean squared error on the validation set.\n- The final cross-validated MSE for the model is the average of the MSEs computed across all $K$ folds.\n\n**3. Model Implementation and Evaluation**\n\nWe implement and evaluate two distinct models.\n\n**Model 0: Isotonic Regression**\nIsotonic regression is a non-parametric method that fits a non-decreasing piecewise-constant function, $\\hat{f}_{\\mathrm{iso}}$, to the data. This model imposes a strong structural constraint (monotonicity), which can be highly effective if the underlying signal is indeed monotonic.\n- **Training**: For each training set, the isotonic fit is computed using the Pool-Adjacent-Violators Algorithm (PAVA). PAVA iteratively merges adjacent blocks of points that violate the non-decreasing constraint, replacing them with their weighted average until the entire sequence of fitted values is monotonic.\n- **Prediction**: For a new point $x_{\\text{new}}$ in the validation set, the prediction $\\hat{f}_{\\mathrm{iso}}(x_{\\text{new}})$ is determined by the fitted step function. As specified, we use a left-constant evaluation rule: the predicted value is the fitted value corresponding to the largest training point $x_{\\text{train}}$ such that $x_{\\text{train}} \\le x_{\\text{new}}$.\n- **Cross-Validated Error**: The mean squared error, $MSE_{\\text{iso}}$, is calculated by averaging the prediction errors over all $K$ folds.\n\n**Model 1: Cubic Smoothing Spline**\nA cubic smoothing spline, $\\hat{f}_{\\mathrm{spline}}$, is a more flexible non-parametric model that fits a smooth curve to the data. It is the solution to minimizing the penalized residual sum of squares:\n$$\n\\sum_{i=1}^{n_{\\text{train}}} (y_i - \\hat{f}(x_i))^2 + s \\int (\\hat{f}''(t))^2 dt\n$$\nThe smoothing parameter, $s \\ge 0$, controls the trade-off between the goodness of fit (the first term) and the smoothness of the function (the second term, a penalty on the integrated squared second derivative).\n- **Hyperparameter Tuning**: The performance of the spline is critically dependent on the choice of $s$. We perform a grid search to find the optimal smoothing parameter. The grid is defined relative to the data's scale: $S = n \\cdot \\operatorname{Var}(y)$, where $y$ is the full dataset's response vector. The grid of candidate values is $s \\in \\{0, 0.05 S, 0.2 S, 1 S, 5 S, 20 S\\}$. A value of $s=0$ results in an interpolating spline.\n- **Cross-Validated Error**: For each candidate value of $s$, we compute its $K$-fold cross-validated MSE. The optimal spline is the one corresponding to the parameter $s^*$ that yields the minimum CV-MSE. This minimum value, $MSE_{\\text{spline}} = \\min_{s} MSE_{\\text{spline}}(s)$, serves as the performance metric for the spline model family. This procedure is nested within the main model comparison; for each of the $K$ outer folds, a spline is fit for each $s$ value, and its error on the validation set is computed. The total error for a given $s$ is the average over the $K$ folds.\n\n**4. Model Selection**\n\nThe final step is to select the better-performing model for the given dataset. The decision is based on a direct comparison of the estimated generalization errors:\n- If $MSE_{\\text{iso}} \\le MSE_{\\text{spline}}$, the isotonic regression model (Model 0) is chosen. The simpler model is preferred in case of a tie.\n- Otherwise, if $MSE_{\\text{spline}}  MSE_{\\text{iso}}$, the cubic smoothing spline (Model 1) is chosen.\n\n**Anticipated Results and Bias-Variance Trade-off**\n\nThe outcome for each test case depends on the interplay between signal structure, sample size, and noise level, illustrating the bias-variance trade-off.\n- **Case 1 ($A=0.0$):** The true signal is monotonic. Isotonic regression is correctly specified (low bias) and constrained (low variance). The spline is unnecessarily flexible and may overfit (higher variance). Isotonic regression (Model 0) is expected to be selected.\n- **Case 2 ($A=0.20$):** A small non-monotonic bump is introduced. Isotonic regression is now misspecified and will have high bias in the region of the bump, as it will flatten it. However, its variance remains low. The spline has lower bias as it can capture the bump, but its flexibility leads to higher variance. The choice is ambiguous and depends on whether the reduction in bias from the spline outweighs its increase in variance.\n- **Case 3 ($A=0.80$):** The bump is pronounced. The bias of the isotonic model becomes very large, likely dominating its error. The spline's ability to fit the non-monotonic feature (low bias) should make it the superior model, so Model 1 is expected.\n- **Case 4 ($A=0.80$, $\\sigma=0.50$):** A pronounced bump but with high noise. The high noise level significantly increases the risk of overfitting for the flexible spline model, leading to very high variance. The simpler, more stable (though biased) isotonic model might yield a lower overall MSE. Model 0 could be selected.\n- **Case 5 ($n=30$):** Small sample size. With limited data, variance is a dominant component of prediction error. Flexible models like splines are highly susceptible to overfitting. The strong structural assumption of the isotonic model acts as a powerful regularizer, leading to lower variance and likely better generalization. Model 0 is expected.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import UnivariateSpline\nimport sys\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the given test cases.\n    \"\"\"\n    # The version check is for local testing and should be removed or commented out\n    # in a strict environment where versions are guaranteed.\n    # required_versions = {'numpy': '1.23.5', 'scipy': '1.11.4'}\n    # if sys.version_info.major != 3 or sys.version_info.minor != 12:\n    #     print(f\"Warning: Python version is {sys.version}, not 3.12\")\n    # if np.__version__ != required_versions['numpy']:\n    #     print(f\"Warning: Numpy version is {np.__version__}, not {required_versions['numpy']}\")\n    # try:\n    #     import scipy\n    #     if scipy.__version__ != required_versions['scipy']:\n    #         print(f\"Warning: Scipy version is {scipy.__version__}, not {required_versions['scipy']}\")\n    # except ImportError:\n    #     print(\"Scipy is not installed.\")\n\n    test_cases = [\n        (200, 0.05, 0.0, 0.12, 0.60, 42),\n        (200, 0.05, 0.20, 0.12, 0.60, 43),\n        (200, 0.05, 0.80, 0.12, 0.60, 44),\n        (200, 0.50, 0.80, 0.12, 0.60, 45),\n        (30, 0.10, 0.50, 0.15, 0.55, 46),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, sigma, A, w, x0, seed = case\n        \n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        x = np.linspace(0, 1, n)\n        \n        def bump_fn(x, A, w, x0):\n            b = np.zeros_like(x)\n            mask = np.abs(x - x0) = w\n            b[mask] = A * np.sin(np.pi * (x[mask] - x0) / w)\n            return b\n\n        f_x = x + bump_fn(x, A, w, x0)\n        y = f_x + rng.normal(0, sigma, size=n)\n\n        # 2. Cross-Validation Setup\n        K = 5\n        indices = np.arange(n)\n        folds = np.array_split(indices, K)\n\n        # 3. Model 0: Isotonic Regression Evaluation\n        def pava(y_in):\n            \"\"\"Pool-Adjacent-Violators Algorithm.\"\"\"\n            n_pava = len(y_in)\n            if n_pava == 0:\n                return np.array([])\n            \n            pools = [[val, 1] for val in y_in]\n            \n            i = 0\n            while i  len(pools) - 1:\n                if pools[i][0]  pools[i+1][0]:\n                    val1, w1 = pools[i]\n                    val2, w2 = pools[i+1]\n                    new_w = w1 + w2\n                    new_val = (val1 * w1 + val2 * w2) / new_w\n                    pools[i+1] = [new_val, new_w]\n                    pools.pop(i)\n                    i = max(0, i - 1)\n                else:\n                    i += 1\n            \n            y_iso = np.zeros(n_pava)\n            current_pos = 0\n            for val, weight in pools:\n                y_iso[current_pos : current_pos + weight] = val\n                current_pos += weight\n            return y_iso\n\n        iso_errors = []\n        for k in range(K):\n            val_indices = folds[k]\n            train_indices = np.concatenate([folds[j] for j in range(K) if j != k])\n            \n            x_train, y_train = x[train_indices], y[train_indices]\n            x_val, y_val = x[val_indices], y[val_indices]\n\n            y_iso_train = pava(y_train)\n            \n            # Predict on validation set (left-constant interpolation/extrapolation)\n            pred_indices = np.searchsorted(x_train, x_val, side='right') - 1\n            pred_indices = np.maximum(0, pred_indices)\n            y_iso_pred = y_iso_train[pred_indices]\n            \n            iso_errors.append(np.mean((y_val - y_iso_pred)**2))\n        \n        mse_iso = np.mean(iso_errors)\n\n        # 4. Model 1: Cubic Smoothing Spline Evaluation\n        S = np.var(y) * n if n  1 else 1.0\n        s_grid = [0, 0.05 * S, 0.2 * S, 1 * S, 5 * S, 20 * S]\n        \n        spline_cv_mses = []\n        for s_val in s_grid:\n            spline_s_errors = []\n            for k in range(K):\n                val_indices = folds[k]\n                train_indices = np.concatenate([folds[j] for j in range(K) if j != k])\n            \n                x_train, y_train = x[train_indices], y[train_indices]\n                x_val, y_val = x[val_indices], y[val_indices]\n\n                # UnivariateSpline requires at least k+1 points for degree k\n                if len(x_train) = 3: # k=3 is the default\n                    spline_s_errors.append(np.inf)\n                    continue\n\n                try:\n                    spline = UnivariateSpline(x_train, y_train, s=s_val, k=3)\n                    y_spline_pred = spline(x_val)\n                    spline_s_errors.append(np.mean((y_val - y_spline_pred)**2))\n                except:\n                     spline_s_errors.append(np.inf) # Handle potential fitting errors\n\n            spline_cv_mses.append(np.mean(spline_s_errors))\n            \n        mse_spline = np.min(spline_cv_mses)\n\n        # 5. Model Selection\n        # Select Model 0 (Isotonic) if its MSE is less than or equal to the spline's MSE\n        selected_model = 0 if mse_iso = mse_spline else 1\n        results.append(selected_model)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}