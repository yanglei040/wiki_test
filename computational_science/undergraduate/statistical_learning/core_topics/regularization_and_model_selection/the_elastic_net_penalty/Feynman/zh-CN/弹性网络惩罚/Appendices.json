{
    "hands_on_practices": [
        {
            "introduction": "许多用于统计学习的优化算法，例如近端梯度下降，依赖于一个被称为近端算子的关键子程序。这个练习将引导你推导出弹性网络惩罚项的近端算子的闭式解。理解这一推导过程是掌握这些强大算法如何高效求解复杂的非光滑优化问题的基础。",
            "id": "2164012",
            "problem": "在许多高级信号处理应用中，例如磁共振成像 (MRI) 中的压缩感知，一项关键任务是从测量值中恢复信号 $\\mathbf{x} \\in \\mathbb{R}^n$。弹性网络正则化是解决此类问题的一种强大技术，因为它能促进稀疏解（即解中包含许多零分量），同时也能处理变量之间的相关性。\n\n邻近梯度法是一种非常适合解决包含此类正则化项的优化问题的迭代算法。这些方法的一个基本构件是邻近算子。对于一个给定的函数 $f(\\mathbf{x})$，其带有标量参数 $\\gamma  0$ 的邻近算子定义为：\n$$ \\text{prox}_{\\gamma f}(\\mathbf{z}) = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\left( f(\\mathbf{x}) + \\frac{1}{2\\gamma} \\|\\mathbf{x} - \\mathbf{z}\\|_2^2 \\right) $$\n用于表示此类算子的一个标准函数是逐元素的软阈值算子。对于一个向量 $\\mathbf{a}$ 和一个阈值 $\\kappa  0$，它对每个分量 $i$ 定义为 $(S_\\kappa(\\mathbf{a}))_i = \\text{sign}(a_i) \\max(|a_i| - \\kappa, 0)$。\n\n考虑弹性网络惩罚函数 $f(\\mathbf{x}) = \\lambda_1 \\|\\mathbf{x}\\|_1 + \\frac{\\lambda_2}{2} \\|\\mathbf{x}\\|_2^2$，其中 $\\lambda_1  0$ 和 $\\lambda_2  0$ 是正的正则化参数，$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n |x_i|$ 是 $L_1$ 范数，以及 $\\|\\mathbf{x}\\|_2^2 = \\sum_{i=1}^n x_i^2$ 是 $L_2$ 范数的平方。\n\n你的任务是为邻近算子 $\\text{prox}_{\\gamma f}(\\mathbf{z})$ 推导一个闭式解析表达式。最终的表达式应以紧凑的向量形式书写。",
            "solution": "我们求解函数 $f(\\mathbf{x})=\\lambda_{1}\\|\\mathbf{x}\\|_{1}+\\frac{\\lambda_{2}}{2}\\|\\mathbf{x}\\|_{2}^{2}$（其中 $\\gamma0$, $\\lambda_{1}0$, $\\lambda_{2}0$）的 $\\operatorname{prox}_{\\gamma f}(\\mathbf{z})$。根据定义，\n$$\n\\operatorname{prox}_{\\gamma f}(\\mathbf{z})=\\arg\\min_{\\mathbf{x}\\in\\mathbb{R}^{n}}\\left(\\lambda_{1}\\|\\mathbf{x}\\|_{1}+\\frac{\\lambda_{2}}{2}\\|\\mathbf{x}\\|_{2}^{2}+\\frac{1}{2\\gamma}\\|\\mathbf{x}-\\mathbf{z}\\|_{2}^{2}\\right).\n$$\n目标函数在各个坐标上是可分的，因为每一项都是逐分量应用的函数之和。因此，对每个 $i\\in\\{1,\\dots,n\\}$，我们求解以下标量问题\n$$\n\\min_{x\\in\\mathbb{R}}\\left(\\lambda_{1}|x|+\\frac{\\lambda_{2}}{2}x^{2}+\\frac{1}{2\\gamma}(x-z)^{2}\\right),\n$$\n其中 $z$ 表示第 $i$ 个分量 $z_{i}$。展开并组合各项：\n$$\n\\lambda_{1}|x|+\\frac{1}{2}\\left(\\lambda_{2}+\\frac{1}{\\gamma}\\right)x^{2}-\\frac{z}{\\gamma}x+\\frac{z^{2}}{2\\gamma}.\n$$\n加性常数 $\\frac{z^{2}}{2\\gamma}$ 不影响最小值点。令 $a=\\lambda_{2}+\\frac{1}{\\gamma}$ 和 $b=\\frac{z}{\\gamma}$。那么我们等价地最小化\n$$\n\\frac{a}{2}x^{2}-bx+\\lambda_{1}|x|=\\frac{a}{2}\\left(x-\\frac{b}{a}\\right)^{2}+\\lambda_{1}|x|+\\text{constant}.\n$$\n由于 $a0$，乘以正常数 $a$ 不会改变最小值点，所以问题等价于\n$$\n\\min_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}\\left(x-\\frac{b}{a}\\right)^{2}+\\frac{\\lambda_{1}}{a}|x|\\right).\n$$\n根据已知的邻近恒等式 $\\operatorname{prox}_{\\kappa|\\cdot|}(u)=S_{\\kappa}(u)$，标量最小值点是\n$$\nx^{\\star}=S_{\\lambda_{1}/a}\\!\\left(\\frac{b}{a}\\right).\n$$\n代入 $a=\\lambda_{2}+\\frac{1}{\\gamma}$ 和 $b=\\frac{z}{\\gamma}$ 得到\n$$\nx^{\\star}=S_{\\frac{\\gamma\\lambda_{1}}{1+\\gamma\\lambda_{2}}}\\!\\left(\\frac{z}{1+\\gamma\\lambda_{2}}\\right).\n$$\n使用对所有 $\\alpha0$ 成立的缩放性质 $S_{\\alpha\\tau}(\\alpha u)=\\alpha S_{\\tau}(u)$，这可以写成\n$$\nx^{\\star}=\\frac{1}{1+\\gamma\\lambda_{2}}\\,S_{\\gamma\\lambda_{1}}(z).\n$$\n将此操作逐元素地应用于所有分量，得到紧凑的向量形式\n$$\n\\operatorname{prox}_{\\gamma f}(\\mathbf{z})=\\frac{1}{1+\\gamma\\lambda_{2}}\\,S_{\\gamma\\lambda_{1}}(\\mathbf{z}),\n$$\n其中 $S_{\\kappa}(\\cdot)$ 是逐元素的软阈值算子。",
            "answer": "$$\\boxed{\\frac{1}{1+\\gamma\\lambda_{2}}\\,S_{\\gamma\\lambda_{1}}(\\mathbf{z})}$$"
        },
        {
            "introduction": "在应用惩罚项之前，我们必须考虑数据的尺度。本练习通过一个思想实验，探讨了将弹性网络应用于未标准化特征会如何显著且不公平地改变变量的选择结果。通过这个场景的分析，你将深刻而直观地理解为什么特征标准化是惩罚回归中一个不可或缺的步骤。",
            "id": "3182165",
            "problem": "您正在使用设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，将带有弹性网络 (EN) 惩罚的线性模型拟合到响应向量 $y \\in \\mathbb{R}^{n}$ 上。考虑目标函数\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right),\n$$\n其中混合参数为 $\\alpha \\in (0,1]$，调整参数为 $\\lambda  0$。您将研究当 $\\lambda$ 从 $+\\infty$ 递减时，特征标准化如何影响哪些变量进入 EN 正则化路径。\n\n假设以下数据条件：\n- 有 $p=2$ 个特征，其列向量 $x_{1},x_{2} \\in \\mathbb{R}^{n}$ 是中心化的，样本内积为 $(1/n)\\,x_{1}^{\\top}x_{2}=0$。\n- 它们的样本方差为 $(1/n)\\,x_{1}^{\\top}x_{1}=s_{1}^{2}$ 和 $(1/n)\\,x_{2}^{\\top}x_{2}=s_{2}^{2}$，其中 $s_{1}=10$ 且 $s_{2}=1$。\n- 令 $z_{j}=x_{j}/s_{j}$ 表示标准化后的列，因此对于 $j\\in\\{1,2\\}$，有 $(1/n)\\,z_{j}^{\\top}z_{j}=1$。\n- 响应 $y$ 是中心化的，并且与两个特征具有相等的标准化相关性：$(1/n)\\,z_{1}^{\\top}y=(1/n)\\,z_{2}^{\\top}y=1$。\n\n假设您使用 $\\alpha=0.5$ 拟合 EN，并随着 $\\lambda$ 从一个非常大的值减小到 $0$ 来追踪其解路径。考虑两种方法：\n- 方法 U：在未标准化的列 $x_{1},x_{2}$ 上拟合 EN。\n- 方法 S：首先将列标准化为 $z_{1},z_{2}$（单位方差），然后拟合 EN。\n\n哪个陈述最能描述标准化对 $x_{1}$ 和 $x_{2}$ 进入 EN 路径顺序的影响，并解释为什么不进行标准化会扭曲相对惩罚并误导变量选择？\n\nA. 在方法 U 中，$x_{1}$ 在比 $x_{2}$ 更大的 $\\lambda$ 值处变为非零，因此首先进入模型；在方法 S 中，它们同时进入。不进行标准化时，惩罚作用于与特征方差成反比的系数上，因此对于相同的标准化信号，方差较大的特征 $x_{1}$ 受到的有效惩罚更少，从而使选择偏向于 $x_{1}$。\n\nB. 在方法 U 中，$x_{2}$ 首先变为非零，因为它的系数必须更大才能拟合相同的信号，因此更容易克服惩罚；在方法 S 中，$x_{1}$ 仍然首先进入。不进行标准化时，方差较小的特征受到的有效惩罚更少。\n\nC. 岭惩罚部分 $((1-\\alpha)\\lVert \\beta \\rVert_{2}^{2})$ 消除了所有尺度效应，因此对于任何 $\\alpha \\in (0,1)$，方法 U 和 S 都会产生相同的进入顺序；对于 EN 而言，标准化是不必要的。\n\nD. 因为 $x_{1}$ 和 $x_{2}$ 是正交的，只要它们与 $y$ 的标准化相关性相等，在方法 U 和 S 中，两个特征总是在任何 $\\lambda$ 值处同时进入模型；正交性确保了进入顺序的尺度不变性。",
            "solution": "用户希望我分析特征标准化对弹性网络正则化路径中变量进入顺序的影响。\n\n### 第 1 步：提取已知条件\n\n- **目标函数：** $\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right)$\n- **参数：** $\\lambda  0$，$\\alpha \\in (0,1]$。对于本问题，具体为 $\\alpha = 0.5$。\n- **响应：** $y \\in \\mathbb{R}^{n}$，中心化的。\n- **特征：** $p=2$ 个特征，$x_{1}, x_{2} \\in \\mathbb{R}^{n}$。\n- **数据条件：**\n    - 特征是中心化的。\n    - 样本正交性：$(1/n)\\,x_{1}^{\\top}x_{2}=0$。\n    - 样本方差：$(1/n)\\,x_{1}^{\\top}x_{1}=s_{1}^{2}$ 和 $(1/n)\\,x_{2}^{\\top}x_{2}=s_{2}^{2}$。\n    - 标准差：$s_{1}=10$ 和 $s_{2}=1$。\n    - 标准化特征：$z_{j}=x_{j}/s_{j}$，对于 $j \\in \\{1,2\\}$，因此 $(1/n)\\,z_{j}^{\\top}z_{j}=1$。\n    - 与响应的标准化相关性：$(1/n)\\,z_{1}^{\\top}y = 1$ 和 $(1/n)\\,z_{2}^{\\top}y = 1$。\n- **方法：**\n    - **方法 U：** 在未标准化的列 $x_{1}, x_{2}$ 上拟合 EN。\n    - **方法 S：** 在标准化的列 $z_{1}, z_{2}$ 上拟合 EN。\n\n### 第 2 步：使用提取的已知条件进行验证\n\n问题陈述具有科学依据、定义明确且客观。\n1.  **科学/事实合理性**：该问题在正则化线性模型（弹性网络）的标准框架内提出，这是统计学习和优化的一个核心课题。目标函数是弹性网络的标准形式。特征标准化、正则化路径和变量进入顺序等概念定义明确，是该方法的基础。\n2.  **定义明确**：该问题提供了一套完整且一致的条件。特征的数量、它们的属性（正交性、方差）以及它们与响应变量的关系都已指定，从而可以对变量进入顺序的问题得出一个唯一、可确定的解。\n3.  **客观性**：该问题使用精确的数学和统计语言，没有主观或模糊的术语。\n4.  **完整性和一致性**：所提供的数据（$s_1$, $s_2$, 标准化相关性）是充分且不矛盾的。\n5.  **未检测到其他缺陷**：该问题并非微不足道，因为它探讨了惩罚回归中一个关键且常被误解的方面。它是可形式化的，并且在数学上是可验证的。\n\n### 第 3 步：结论与行动\n\n问题陈述是**有效的**。开始求解。\n\n### 推导\n\n当调整参数 $\\lambda$ 从 $+\\infty$ 递减时，变量进入弹性网络正则化路径的顺序由 Karush-Kuhn-Tucker (KKT) 最优性条件决定。变量 $j$ 将在能满足 KKT 条件且其系数 $\\beta_j$ 非零的那个最大的 $\\lambda$ 值处进入模型。\n\n令目标函数为 $L(\\beta)$。\n$$\nL(\\beta) = \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\alpha \\sum_{j=1}^{p}|\\beta_j| + \\frac{1-\\alpha}{2}\\sum_{j=1}^{p}\\beta_j^2\\right)\n$$\nKKT 条件指出，在最优解处，$L(\\beta)$ 关于 $\\beta_j$ 的次梯度必须包含 $0$。该次梯度为：\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = -\\frac{1}{n}x_j^{\\top}(y - X\\beta) + \\lambda(\\alpha \\cdot \\text{sgn}(\\beta_j) + (1-\\alpha)\\beta_j)\n$$\n其中 $\\text{sgn}(\\beta_j)$ 是绝对值函数的次梯度，当 $\\beta_j \\neq 0$ 时为 $\\{-1, 1\\}$，当 $\\beta_j = 0$ 时为 $[-1, 1]$。\n\n我们关心的是第一个系数变为非零时的 $\\lambda$ 的阈值。在这一点上，所有其他系数仍然为零。因此我们可以在 KKT 条件中设置 $\\beta=0$。$\\beta_j$ 保持为零的条件是存在一个值 $s_j \\in [-1, 1]$ 使得：\n$$\n-\\frac{1}{n}x_j^{\\top}(y - X\\cdot \\mathbf{0}) + \\lambda(\\alpha \\cdot s_j + (1-\\alpha)\\cdot 0) = 0\n$$\n$$\n-\\frac{1}{n}x_j^{\\top}y + \\lambda \\alpha s_j = 0 \\quad \\implies \\quad \\frac{1}{n}x_j^{\\top}y = \\lambda \\alpha s_j\n$$\n要使 $\\beta_j=0$ 的解存在，必须有 $|\\frac{1}{n}x_j^{\\top}y| \\le \\lambda \\alpha$，因为 $|s_j| \\le 1$。\n一旦 $\\lambda$ 下降到无法再满足此条件的值以下，系数 $\\beta_j$ 就会变为非零。这个阈值 $\\lambda_{j,\\text{enter}}$ 是：\n$$\n\\lambda_{j,\\text{enter}} = \\frac{|\\frac{1}{n}x_j^{\\top}y|}{\\alpha}\n$$\n第一个进入模型的变量是具有最大 $\\lambda_{j,\\text{enter}}$ 的那个，这对应于使绝对内积 $|\\frac{1}{n}x_j^{\\top}y|$ 最大化的特征 $j$。与 $(1-\\alpha)$ 成正比的岭惩罚项在 $\\beta=0$ 处的梯度为 $0$，因此不影响进入顺序。\n\n现在，我们将此原则应用于这两种方法。\n\n**方法 U：在未标准化的数据（$x_{1}, x_{2}$）上拟合**\n我们需要找到内积 $|\\frac{1}{n}x_1^{\\top}y|$ 和 $|\\frac{1}{n}x_2^{\\top}y|$。\n给定条件是 $(1/n)z_j^{\\top}y = 1$ 和 $z_j = x_j/s_j$。\n对于特征 1：\n$$\n\\frac{1}{n}z_1^{\\top}y = \\frac{1}{n}\\left(\\frac{x_1}{s_1}\\right)^{\\top}y = \\frac{1}{s_1}\\left(\\frac{1}{n}x_1^{\\top}y\\right) = 1\n$$\n$$\n\\implies \\frac{1}{n}x_1^{\\top}y = s_1 = 10\n$$\n对于特征 2：\n$$\n\\frac{1}{n}z_2^{\\top}y = \\frac{1}{n}\\left(\\frac{x_2}{s_2}\\right)^{\\top}y = \\frac{1}{s_2}\\left(\\frac{1}{n}x_2^{\\top}y\\right) = 1\n$$\n$$\n\\implies \\frac{1}{n}x_2^{\\top}y = s_2 = 1\n$$\n进入阈值为：\n- 对于 $x_1$： $\\lambda_{1,\\text{enter}} = \\frac{|10|}{\\alpha} = \\frac{10}{0.5} = 20$。\n- 对于 $x_2$： $\\lambda_{2,\\text{enter}} = \\frac{|1|}{\\alpha} = \\frac{1}{0.5} = 2$。\n\n由于 $\\lambda_{1,\\text{enter}}  \\lambda_{2,\\text{enter}}$，特征 $x_1$ 首先进入模型（在更大的 $\\lambda$ 值处）。\n\n**方法 S：在标准化的数据（$z_{1}, z_{2}$）上拟合**\n现在特征是 $z_1$ 和 $z_2$。我们需要找到内积 $|\\frac{1}{n}z_1^{\\top}y|$ 和 $|\\frac{1}{n}z_2^{\\top}y|$。这些在问题陈述中已经直接给出。\n$$\n\\frac{1}{n}z_1^{\\top}y = 1\n$$\n$$\n\\frac{1}{n}z_2^{\\top}y = 1\n$$\n进入阈值为：\n- 对于 $z_1$： $\\lambda_{1,\\text{enter}} = \\frac{|1|}{\\alpha} = \\frac{1}{0.5} = 2$。\n- 对于 $z_2$： $\\lambda_{2,\\text{enter}} = \\frac{|1|}{\\alpha} = \\frac{1}{0.5} = 2$。\n\n由于 $\\lambda_{1,\\text{enter}} = \\lambda_{2,\\text{enter}}$，两个特征同时进入模型。\n\n**关于标准化效果的解释**\n弹性网络的 L1 惩罚部分 $\\lambda \\alpha \\sum |\\beta_j|$ 直接作用于系数 $\\beta_j$。这些系数不是尺度不变的。考虑未标准化模型的系数 ($\\beta$) 和一个假设的标准化模型（其中 $y \\approx \\sum \\gamma_j z_j$）的系数 ($\\gamma$) 之间的关系。代入 $z_j = x_j/s_j$，我们得到 $y \\approx \\sum \\gamma_j (x_j/s_j) = \\sum (\\gamma_j/s_j)x_j$。因此，$\\beta_j = \\gamma_j/s_j$。\n\n对未标准化系数的 L1 惩罚是 $\\lambda \\alpha \\sum|\\beta_j| = \\lambda \\alpha \\sum|\\gamma_j/s_j|$。对潜在“效应大小” $\\gamma_j$ 的惩罚与特征的标准差 $s_j$ 成反比。\n对于特征 $x_1$，其 $s_1=10$，对其效应的惩罚按 $1/10$ 缩放。\n对于特征 $x_2$，其 $s_2=1$，对其效应的惩罚按 $1/1$ 缩放。\n因此，方差较大的特征 ($x_1$) 受到的有效惩罚更少。在产生相同预测影响的情况下，其系数 $\\beta_1$ 可以比 $\\beta_2$ 更小，从而使其更容易“躲过”惩罚。这使得变量选择偏向于方差较高的特征，这正是我们在方法 U 中观察到的现象。标准化将所有特征在惩罚方面置于同等地位，这就是为什么它们在方法 S 中同时进入模型，反映了它们与响应具有相等的标准化相关性。\n\n### 逐项分析\n\n**A. 在方法 U 中，$x_{1}$ 在比 $x_{2}$ 更大的 $\\lambda$ 值处变为非零，因此首先进入模型；在方法 S 中，它们同时进入。不进行标准化时，惩罚作用于与特征方差成反比的系数上，因此对于相同的标准化信号，方差较大的特征 $x_{1}$ 受到的有效惩罚更少，从而使选择偏向于 $x_{1}$。**\n- **评估：** 该陈述与推导完全一致。\n    - 方法 U：$x_1$ 在 $\\lambda = 20$ 时首先进入，而 $x_2$ 在 $\\lambda = 2$ 时进入。正确。\n    - 方法 S：它们在 $\\lambda=2$ 时同时进入。正确。\n    - 解释：惩罚作用于与方差成反比的系数，导致高方差特征受到的较小的有效惩罚，这一推理正是正确的机制。\n- **结论：** **正确**。\n\n**B. 在方法 U 中，$x_{2}$ 首先变为非零，因为它的系数必须更大才能拟合相同的信号，因此更容易克服惩罚；在方法 S 中，$x_{1}$ 仍然首先进入。不进行标准化时，方差较小的特征受到的有效惩罚更少。**\n- **评估：** 这包含多个错误。\n    - 方法 U：它声称 $x_2$ 首先进入，这与推导出的结果相反。\n    - 方法 S：它声称 $x_1$ 首先进入，这是错误的；它们是同时进入的。\n    - 解释：它错误地声称小方差特征受到的惩罚更少。事实恰恰相反：小方差特征需要一个更大的系数，而这个更大的系数会受到 L1 范数更重的惩罚。\n- **结论：** **错误**。\n\n**C. 岭惩罚部分 $((1-\\alpha)\\lVert \\beta \\rVert_{2}^{2})$ 消除了所有尺度效应，因此对于任何 $\\alpha \\in (0,1)$，方法 U 和 S 都会产生相同的进入顺序；对于 EN 而言，标准化是不必要的。**\n- **评估：** 这从根本上是错误的。推导表明，进入顺序由 L1 惩罚项决定，并且与岭惩罚部分无关（因为其在 $\\beta=0$ 处的梯度为零）。岭惩罚对尺度敏感，但它不影响哪个变量*首先*变为非零。此外，推导明确表明方法 U 和 S 具有不同的进入顺序，从而证伪了其主要论点。\n- **结论：** **错误**。\n\n**D. 因为 $x_{1}$ 和 $x_{2}$ 是正交的，只要它们与 $y$ 的标准化相关性相等，在方法 U 和 S 中，两个特征总是在任何 $\\lambda$ 值处同时进入模型；正交性确保了进入顺序的尺度不变性。**\n- **评估：** 虽然正交性简化了分析，但它并不能保证进入顺序的尺度不变性。进入顺序由 $|\\frac{1}{n}x_j^{\\top}y|$ 决定，而这是与尺度相关的。我们在方法 U 中的结果（尽管存在正交性，$x_1$ 仍然在 $x_2$ 之前进入）就是一个直接的反例。\n- **结论：** **错误**。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "现在，让我们通过一个动手编程挑战将理论付诸实践。你将从零开始构建一个坐标下降求解器，以展示弹性网络最著名的特性之一：分组效应。这个练习将让你创造一个Lasso在高度相关性面前失效的场景，并亲眼见证弹性网络如何成功地识别出所有相关的预测变量。",
            "id": "3191319",
            "problem": "您被要求编写一个完整的、可运行的程序，该程序构建一个合成的回归场景。在该场景中，最小绝对收缩和选择算子（LASSO）由于一个弱但必要的变量与一个更强的变量高度相关而未能选择它，然后展示弹性网络（Elastic Net）如何通过其分组效应来恢复这个弱变量。您的程序必须从第一性原理出发，为这两种惩罚项实现各自的坐标下降求解器，并且不得依赖任何外部库中的模型拟合函数。\n\n用作起点的背景定义：\n- 考虑一个线性模型，其响应变量为 $y \\in \\mathbb{R}^{n}$，预测变量为 $X \\in \\mathbb{R}^{n \\times p}$；普通最小二乘法（OLS）的目标是最小化均方误差，即最小化 $\\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2}$，其中 $\\beta \\in \\mathbb{R}^{p}$。\n- LASSO 添加 $\\ell_{1}$ 惩罚项以鼓励稀疏性：最小化 $\\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{1}$，其中 $\\beta \\in \\mathbb{R}^{p}$ 且 $\\lambda \\ge 0$。\n- 弹性网络（EN）结合了 $\\ell_{1}$ 和 $\\ell_{2}$ 惩罚项，以同时鼓励稀疏性和分组：最小化\n$\\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right)$\n，其中 $\\beta \\in \\mathbb{R}^{p}$，$\\lambda \\ge 0$ 且混合参数 $\\alpha \\in [0,1]$。\n- 对于此类问题，一个标准且经过充分检验的做法是标准化预测变量（零均值，单位方差）并中心化响应变量，这样在拟合过程中使用零截距是合适的。\n\n您的任务：\n- 生成两个预测变量的数据 $X = [X_{1}, X_{2}]$，其列之间具有指定的相关性 $\\rho$。使用线性信号 $y = X\\beta + \\varepsilon$，其中 $\\beta = (\\beta_{1}, \\beta_{2})$，高斯噪声 $\\varepsilon$ 的均值为 $0$，方差为 $\\sigma^{2}$。\n- 为 LASSO 和弹性网络实现坐标下降求解器，以最小化上述目标函数，操作于标准化的预测变量和中心化的响应变量。使用收敛容差和固定的最大迭代次数。不要使用来自外部库的封闭形式“黑箱”求解器或训练例程。\n- 对于变量选择，如果一个系数的绝对值超过一个小阈值（为此您可以使用 $10^{-6}$），则将其视为“已选择”。\n\n测试套件：\n构建以下四个案例，以检验和验证选择行为。在每个案例中，使用指定的值和固定的种子，以确保结果是可复现的。下面所有数值都必须完全按照给定的使用。\n\n- 案例 A（LASSO下的遮蔽效应）：\n  - 样本量 $n = 400$。\n  - $X_{1}$ 和 $X_{2}$ 之间的相关性 $\\rho = 0.98$。\n  - 真实系数 $\\beta = (1.0, 0.15)$。\n  - 噪声标准差 $\\sigma = 0.05$。\n  - LASSO 的惩罚强度 $\\lambda = 0.12$。\n  - 随机种子 $202311$。\n  - 输出一个布尔值，指示 LASSO 是否选择了弱变量 $X_{2}$。\n\n- 案例 B（相同数据，弹性网络补救）：\n  - 使用与案例 A 中完全相同的 $X$ 和 $y$。\n  - 弹性网络，具有相同的惩罚强度 $\\lambda = 0.12$ 和混合参数 $\\alpha = 0.2$。\n  - 输出一个布尔值，指示弹性网络是否选择了弱变量 $X_{2}$。\n\n- 案例 C（LASSO下的低相关性控制）：\n  - 样本量 $n = 400$。\n  - $X_{1}$ 和 $X_{2}$ 之间的相关性 $\\rho = 0.10$。\n  - 真实系数 $\\beta = (1.0, 0.15)$。\n  - 噪声标准差 $\\sigma = 0.05$。\n  - LASSO 的惩罚强度 $\\lambda = 0.12$。\n  - 随机种子 $202312$。\n  - 输出一个布尔值，指示 LASSO 是否选择了弱变量 $X_{2}$。\n\n- 案例 D（弹性网络下的完美共线性和分组）：\n  - 样本量 $n = 400$。\n  - 完美共线性 $\\rho = 1.0$，因此 $X_{2} = X_{1}$ 是确定性的。\n  - 真实系数 $\\beta = (1.0, 0.15)$。\n  - 噪声标准差 $\\sigma = 0.01$。\n  - 弹性网络，惩罚强度 $\\lambda = 0.08$，混合参数 $\\alpha = 0.10$。\n  - 随机种子 $202313$。\n  - 输出一个布尔值，指示弹性网络是否同时选择了 $X_{1}$ 和 $X_{2}$。\n\n必需的最终输出格式：\n- 您的程序应生成单行输出，其中包含四个布尔值结果，按顺序对应案例 A、B、C 和 D，形式为用方括号括起来的逗号分隔列表，例如，“[True,False,True,True]”。\n- 无需报告物理单位、角度单位或百分比。\n\n科学真实性：\n- 所有数据生成和参数必须与高斯线性模型及上述定义一致。\n- 程序必须精确使用指定的种子和参数，无用户输入，无外部文件。",
            "solution": "该问题被评估为有效。这是一个在计算统计学领域中定义良好且具有科学依据的问题，为获得唯一且可复现的解提供了所有必要的参数和定义。它没有矛盾、歧义和事实错误。\n\n任务是展示最小绝对收缩和选择算子（LASSO）与弹性网络的变量选择特性，特别是在涉及高度相关预测变量的场景中。我们将从第一性原理出发实现坐标下降算法来解决所需的优化问题，并将其应用于四个特定的测试案例。\n\n首先，我们为求解器建立理论基础。一般问题是找到系数向量 $\\beta \\in \\mathbb{R}^p$，以最小化带惩罚的最小二乘目标函数。假设预测变量 $X \\in \\mathbb{R}^{n \\times p}$ 经过标准化，每列的均值为零，标准差为一；响应变量 $y \\in \\mathbb{R}^n$ 经过中心化，均值为零。在此标准化下，对于所有列 $j \\in \\{1, \\dots, p\\}$，我们有 $\\frac{1}{n} \\sum_{i=1}^{n} x_{ij}^2 = 1$。\n\n弹性网络的目标函数由下式给出：\n$$\nL(\\beta) = \\frac{1}{2n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda\\left(\\alpha \\lVert \\beta \\rVert_{1} + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_{2}^{2}\\right)\n$$\n其中 $\\lambda \\ge 0$ 是总体的惩罚强度，$\\alpha \\in [0,1]$ 是混合参数。LASSO 是弹性网络在 $\\alpha=1$ 时的特例。\n\n我们使用坐标下降算法。该算法针对每个系数 $\\beta_j$ 单独迭代优化目标函数，同时保持所有其他系数 $\\beta_k$（$k \\neq j$）固定。为了推导 $\\beta_j$ 的更新规则，我们考虑 $L(\\beta)$ 中依赖于 $\\beta_j$ 的项：\n$$\nL_j(\\beta_j) = \\frac{1}{2n}\\sum_{i=1}^{n} \\left(y_i - \\sum_{k \\neq j} x_{ik}\\beta_k - x_{ij}\\beta_j\\right)^2 + \\lambda\\alpha|\\beta_j| + \\frac{\\lambda(1-\\alpha)}{2}\\beta_j^2 + C\n$$\n其中 $C$ 包含不依赖于 $\\beta_j$ 的项。令 $r^{(j)} = y - \\sum_{k \\neq j} X_k\\beta_k$ 为排除第 $j$ 个预测变量时的偏残差向量。表达式变为：\n$$\nL_j(\\beta_j) = \\frac{1}{2n}\\sum_{i=1}^{n} (r_i^{(j)} - x_{ij}\\beta_j)^2 + \\lambda\\alpha|\\beta_j| + \\frac{\\lambda(1-\\alpha)}{2}\\beta_j^2 + C\n$$\n展开平方项并利用 $\\frac{1}{n}\\sum_{i=1}^n x_{ij}^2 = 1$ 这一事实，我们得到（忽略常数项）：\n$$\nL_j(\\beta_j) \\propto \\frac{1}{2}\\beta_j^2 - \\beta_j\\left(\\frac{1}{n}\\sum_{i=1}^n x_{ij}r_i^{(j)}\\right) + \\lambda\\alpha|\\beta_j| + \\frac{\\lambda(1-\\alpha)}{2}\\beta_j^2\n$$\n令 $z_j = \\frac{1}{n}X_j^T r^{(j)} = \\frac{1}{n}X_j^T(y - \\sum_{k \\neq j} X_k\\beta_k)$。我们需要最小化：\n$$\nf(\\beta_j) = \\frac{1}{2}(1 + \\lambda(1-\\alpha))\\beta_j^2 - z_j \\beta_j + \\lambda\\alpha|\\beta_j|\n$$\n$f(\\beta_j)$ 关于 $\\beta_j$ 的次梯度是：\n$$\n\\partial f(\\beta_j) = (1 + \\lambda(1-\\alpha))\\beta_j - z_j + \\lambda\\alpha \\cdot \\partial|\\beta_j|\n$$\n其中 $\\partial|\\beta_j|$ 是绝对值函数的次梯度，对于 $\\beta_j \\neq 0$ 是 $\\text{sgn}(\\beta_j)$，对于 $\\beta_j = 0$ 是区间 $[-1, 1]$。将次梯度设为 $0$ 得到驻点条件：\n$$\nz_j = (1 + \\lambda(1-\\alpha))\\beta_j + \\lambda\\alpha \\cdot \\text{sgn}(\\beta_j) \\quad (\\text{if } \\beta_j \\neq 0)\n$$\n解出 $\\beta_j$ 得：\n$$\n\\beta_j = \\frac{z_j - \\lambda\\alpha \\cdot \\text{sgn}(\\beta_j)}{1+\\lambda(1-\\alpha)}\n$$\n这个解可以使用软阈值算子紧凑地写出，该算子定义为 $S(\\gamma, \\tau) = \\text{sgn}(\\gamma)\\max(|\\gamma|-\\tau, 0)$。$\\beta_j$ 的更新规则是：\n$$\n\\beta_j^{\\text{new}} \\leftarrow \\frac{S(z_j, \\lambda\\alpha)}{1+\\lambda(1-\\alpha)}\n$$\n量 $z_j$ 可以在算法循环内高效计算。给定当前估计的 $\\beta$，我们可以写出 $z_j = \\frac{1}{n}X_j^T(y - X\\beta + X_j\\beta_j) = \\frac{1}{n}X_j^T(y-X\\beta) + \\beta_j$。\n\n对于 LASSO 的情况，我们设 $\\alpha=1$。更新规则简化为：\n$$\n\\beta_j^{\\text{new}} \\leftarrow \\frac{S(z_j, \\lambda)}{1+\\lambda(1-1)} = S(z_j, \\lambda)\n$$\n这是 LASSO 的标准软阈值更新。因此，一个为弹性网络设计的求解器可以处理这两个问题。\n\n算法流程如下：\n1. 初始化 $\\beta$ 为零向量。\n2. 迭代直至收敛：\n   a. 对每个系数 $j=1, \\dots, p$：\n      i. 计算 $z_j = \\frac{1}{n}X_j^T(y - X\\beta) + \\beta_j$。\n      ii. 使用上面推导的弹性网络更新规则更新 $\\beta_j$。\n   b. 检查 $\\beta$ 向量的变化是否低于容差。\n\n现在我们将此框架应用于四个指定的测试案例。\n\n- **案例 A（LASSO，高相关性）**：对于两个具有高正相关性（$\\rho=0.98$）的预测变量 $X_1$ 和 $X_2$，以及一个真实模型，其中 $X_1$ 的系数较大（$\\beta_1=1.0$）而 $X_2$ 的系数虽小但非零（$\\beta_2=0.15$），LASSO 惩罚项的行为是众所周知的不稳定。项 $\\frac{1}{n}X_1^T(y - X_2\\beta_2)$ 会很大，将 $\\beta_1$ 推入模型。一旦 $\\beta_1$ 被包含进来，它将解释 $X_1$ 和 $X_2$ 共享的大部分方差。残差与 $X_2$ 的相关性将变得非常小，导致其系数 $\\beta_2$ 被 $\\ell_1$ 惩罚项收缩至零。我们预期 LASSO 会无法选择弱变量 $X_2$。\n\n- **案例 B（弹性网络，高相关性）**：使用与案例 A 相同的数据，我们应用 $\\alpha=0.2$ 的弹性网络。非零的 $\\ell_2$ 惩罚项（$\\frac{1-\\alpha}{2} \\lVert\\beta\\rVert_2^2$）鼓励分组。对于高度相关的预测变量，它对一个系数大而另一个为零的解的惩罚，要大于对两个系数都非零并分担“负荷”的解的惩罚。这种分组效应预计会将 $\\beta_2$ 和 $\\beta_1$ 一同拉入模型，从而正确识别两个变量。\n\n- **案例 C（LASSO，低相关性）**：作为对照，我们将相关性降低到 $\\rho=0.10$。在低相关性下，预测变量几乎是正交的。LASSO 在这种情况下表现良好，预计能正确识别强（$\\beta_1$）和弱（$\\beta_2$）信号，因为它们不竞争解释响应变量中的相同方差。\n\n- **案例 D（弹性网络，完美共线性）**：这里 $\\rho=1.0$，所以 $X_1 = X_2$。对于 LASSO，$\\beta_1+\\beta_2$ 的解是唯一的，但各自的值不是；LASSO 会任意地将一个系数设为非零，另一个设为零。然而，弹性网络的 $\\ell_2$ 惩罚项是严格凸的，会强制产生唯一的解。对于固定的和 $\\beta_1+\\beta_2=c$，项 $\\beta_1^2 + \\beta_2^2$ 在 $\\beta_1=\\beta_2=c/2$ 时最小化。因此，我们预期弹性网络会选择两个变量，并赋予它们相似的非零系数。\n\n下面的程序将为每个案例生成数据并执行求解器，根据系数的绝对值是否超过阈值 $10^{-6}$ 来报告指定的布尔结果。",
            "answer": "```python\nimport numpy as np\n\ndef generate_data(n, beta_true, rho, sigma, seed):\n    \"\"\"\n    Generates synthetic data for a two-predictor linear model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate two independent standard normal vectors\n    Z1 = rng.standard_normal(n)\n    Z2 = rng.standard_normal(n)\n    \n    # Create correlated predictors X1 and X2\n    X1 = Z1\n    X2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n    \n    X = np.column_stack([X1, X2])\n    \n    # Generate response variable\n    noise = rng.normal(0, sigma, n)\n    y = X @ beta_true + noise\n    \n    # Standardize predictors and center response\n    X_mean = X.mean(axis=0)\n    # Using ddof=0 for population standard deviation as assumed in derivations\n    X_std = X.std(axis=0, ddof=0) \n    \n    # Handle the case of zero standard deviation (e.g., constant column)\n    # in the unlikely event rho=1 and numerical error makes std zero\n    X_std[X_std == 0] = 1.0  \n    \n    X_scaled = (X - X_mean) / X_std\n    y_centered = y - y.mean()\n    \n    return X_scaled, y_centered\n\ndef elastic_net_solver(X, y, lambda_, alpha, max_iter=1000, tol=1e-8):\n    \"\"\"\n    Coordinate descent solver for the Elastic Net problem.\n    LASSO is a special case with alpha=1.\n    \"\"\"\n    n_samples, n_features = X.shape\n    beta = np.zeros(n_features)\n    \n    # Since X is standardized, (1/n) * ||X_j||^2 = 1 for all j.\n    \n    for i in range(max_iter):\n        beta_old = beta.copy()\n        \n        for j in range(n_features):\n            # Calculate z_j = (1/n) * X_j^T * r^(j)\n            # Efficiently: z_j = (1/n) * X_j^T * (y - X*beta + X_j*beta_j)\n            # z_j = (1/n) * X_j^T * (y - X*beta) + beta_j\n            \n            # Note: X is standardized, so (1/n) * (X_j^T @ X_j) = 1\n            z_j = (X[:, j].T @ (y - X @ beta) / n_samples) + beta[j]\n\n            # Apply the soft-thresholding update rule for Elastic Net\n            l1_penalty = lambda_ * alpha\n            \n            if z_j > l1_penalty:\n                beta[j] = (z_j - l1_penalty) / (1 + lambda_ * (1 - alpha))\n            elif z_j  -l1_penalty:\n                beta[j] = (z_j + l1_penalty) / (1 + lambda_ * (1 - alpha))\n            else:\n                beta[j] = 0.0\n\n        # Check for convergence\n        if np.linalg.norm(beta - beta_old)  tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"\n    Sets up and solves the four test cases specified in the problem.\n    \"\"\"\n    selection_threshold = 1e-6\n    results = []\n\n    # --- Case A: LASSO, high correlation ---\n    n_A = 400\n    rho_A = 0.98\n    beta_true_A = np.array([1.0, 0.15])\n    sigma_A = 0.05\n    lambda_A = 0.12\n    alpha_A = 1.0  # LASSO\n    seed_A = 202311\n    \n    X_A, y_A = generate_data(n_A, beta_true_A, rho_A, sigma_A, seed_A)\n    beta_lasso_A = elastic_net_solver(X_A, y_A, lambda_A, alpha_A)\n    result_A = abs(beta_lasso_A[1]) > selection_threshold\n    results.append(result_A)\n\n    # --- Case B: Elastic Net, high correlation (same data as A) ---\n    lambda_B = 0.12\n    alpha_B = 0.2\n    \n    beta_en_B = elastic_net_solver(X_A, y_A, lambda_B, alpha_B)\n    result_B = abs(beta_en_B[1]) > selection_threshold\n    results.append(result_B)\n\n    # --- Case C: LASSO, low correlation ---\n    n_C = 400\n    rho_C = 0.10\n    beta_true_C = np.array([1.0, 0.15])\n    sigma_C = 0.05\n    lambda_C = 0.12\n    alpha_C = 1.0  # LASSO\n    seed_C = 202312\n    \n    X_C, y_C = generate_data(n_C, beta_true_C, rho_C, sigma_C, seed_C)\n    beta_lasso_C = elastic_net_solver(X_C, y_C, lambda_C, alpha_C)\n    result_C = abs(beta_lasso_C[1]) > selection_threshold\n    results.append(result_C)\n\n    # --- Case D: Elastic Net, perfect collinearity ---\n    n_D = 400\n    rho_D = 1.0\n    beta_true_D = np.array([1.0, 0.15])\n    sigma_D = 0.01\n    lambda_D = 0.08\n    alpha_D = 0.10\n    seed_D = 202313\n    \n    X_D, y_D = generate_data(n_D, beta_true_D, rho_D, sigma_D, seed_D)\n    beta_en_D = elastic_net_solver(X_D, y_D, lambda_D, alpha_D)\n    result_D = (abs(beta_en_D[0]) > selection_threshold) and (abs(beta_en_D[1]) > selection_threshold)\n    results.append(result_D)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}