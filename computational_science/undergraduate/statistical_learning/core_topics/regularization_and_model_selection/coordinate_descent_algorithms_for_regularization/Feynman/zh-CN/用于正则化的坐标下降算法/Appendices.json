{
    "hands_on_practices": [
        {
            "introduction": "要真正理解一个算法，最好的方法莫过于亲手实现它。本练习将指导您从第一性原理出发，为LASSO问题推导坐标下降的单坐标更新法则。通过自行推导和实施，您将对坐标下降法如何通过一系列简单的单变量优化步骤来求解复杂的LASSO目标函数，以及其中软阈值算子的核心作用，建立起坚实而直观的理解。",
            "id": "2861565",
            "problem": "你的任务是利用凸优化和信号处理的原理，为最小绝对收缩和选择算子（LASSO）问题推导并实现一个循环坐标下降算法。\n\n考虑一个设计矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和观测向量 $b \\in \\mathbb{R}^{m}$ 的 LASSO 目标函数：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是给定的正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。\n\n你的任务是：\n1. 从第一性原理出发，为 $f(x)$ 的循环坐标下降法推导单坐标最小化更新规则。从 $f(x)$ 的定义和 $\\ell_1$ 范数的次梯度最优性条件开始，在保持所有其他坐标固定的情况下，对 $f(x)$ 关于单个坐标 $x_i$ 的最小化进行推理。仅使用基础事实，包括凸函数的性质、绝对值的次梯度以及基础线性代数。不要先验地假设任何特定的闭式更新。\n2. 证明坐标级最小化器是通过将软阈值算子应用于当前迭代点和残差的仿射函数得到的。清晰地定义你推导过程中引入的所有量。\n3. 实现一个使用所推导更新规则的循环坐标下降算法。你的实现必须：\n   - 维持残差 $r \\triangleq b - A x$，并在每次坐标更新后增量式地更新它，以实现每次坐标更新 $\\mathcal{O}(m)$ 的成本。\n   - 使用由 $S_{\\tau}(z) \\triangleq \\mathrm{sign}(z)\\max(|z| - \\tau, 0)$ 定义的软阈值算子。\n   - 当任一坐标在整个一轮循环中的最大绝对变化量小于容差 $\\varepsilon$ 或达到最大轮数时终止。\n   - 返回最终迭代点 $x$，并在要求时返回每轮结束时的目标函数值序列，以评估单调性。\n\n你可以使用的基础知识：\n- $\\|\\cdot\\|_2^2$ 和 $\\|\\cdot\\|_1$ 的凸性及其次梯度的性质。\n- 次梯度最优性条件：对于凸函数 $f$ 的一个最优点 $x^\\star$，有 $0 \\in \\partial f(x^\\star)$。\n- 绝对值的次微分：对于 $t \\in \\mathbb{R}$，如果 $t \\ne 0$，则 $\\partial |t| = \\{\\mathrm{sign}(t)\\}$；如果 $t = 0$，则 $\\partial |t| = [-1,1]$。\n- 用于残差更新的线性代数恒等式。\n\n将目标函数值定义为：\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n\n测试套件：\n实现你的程序以运行以下五个测试用例，并将结果汇总到单行输出中。\n\n- 测试 1（正交标准列，解析检验）：设置 $A = I_4$，$b = [3,-1,0.2,-0.5]^\\top$，$\\lambda = 0.7$。运行你的坐标下降法得到 $x_{\\mathrm{cd}}$。对于正交标准列，已知的解析解是 $x^\\star = S_{\\lambda}(A^\\top b) = S_{\\lambda}(b)$。输出标量\n  $$\n  e_1 \\triangleq \\|x_{\\mathrm{cd}} - x^\\star\\|_{\\infty}.\n  $$\n\n- 测试 2（一般高矩阵系统，Karush–Kuhn–Tucker (KKT) 检验）：生成一个 $A \\in \\mathbb{R}^{60 \\times 30}$，其元素为独立的标准正态分布，然后将每列归一化为单位 $\\ell_2$ 范数。使用固定的伪随机种子 $0$ 以使实例确定。定义一个 $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$，其在索引 $0,5,10,15,20$ 处的非零元素值分别为 $[2.5,-1.7,1.2,-0.9,1.8]$，其余为零。设 $b = A x_{\\mathrm{true}} + \\eta$，其中 $\\eta \\in \\mathbb{R}^{60}$ 的元素为独立的、标准差为 $0.01$ 的正态分布，使用相同的种子 $0$ 生成。设 $\\lambda = 0.05$。运行坐标下降法得到 $x_{\\mathrm{cd}}$。验证 LASSO 的 KKT 条件：令 $g \\triangleq A^\\top(A x_{\\mathrm{cd}} - b)$，\n  - 如果 $x_{\\mathrm{cd},i} \\ne 0$，则 $g_i + \\lambda \\,\\mathrm{sign}(x_{\\mathrm{cd},i}) = 0$。\n  - 如果 $x_{\\mathrm{cd},i} = 0$，则 $|g_i| \\le \\lambda$。\n  由于数值误差，在这些检验中使用 $10^{-4}$ 的容差。输出布尔值 $b_2$，表示所有坐标是否在容差范围内满足 KKT 条件。\n\n- 测试 3（大正则化项将解驱动至零）：使用 $A = I_4$，$b = [3,-1,0.2,-0.5]^\\top$，以及 $\\lambda = 10^6$。输出布尔值 $b_3$，表示返回的解是否在 $10^{-12}$ 的绝对容差内为零向量。\n\n- 测试 4（零正则化项简化为最小二乘）：使用伪随机种子 $1$ 生成 $A \\in \\mathbb{R}^{40 \\times 10}$，其元素为独立的标准正态分布。使用种子 $2$ 生成 $b \\in \\mathbb{R}^{40}$，其元素为独立的标准正态分布。设 $\\lambda = 0$。设 $x_{\\mathrm{ls}}$ 表示最小化 $\\frac{1}{2}\\|A x - b\\|_2^2$ 的最小二乘解，通过标准线性最小二乘法计算。运行坐标下降法得到 $x_{\\mathrm{cd}}$。输出标量\n  $$\n  e_4 \\triangleq \\frac{\\|x_{\\mathrm{cd}} - x_{\\mathrm{ls}}\\|_2}{\\max(\\|x_{\\mathrm{ls}}\\|_2, 10^{-12})}.\n  $$\n\n- 测试 5（目标函数值在各轮次间单调下降）：使用伪随机种子 $3$ 生成 $A \\in \\mathbb{R}^{30 \\times 15}$ 和 $b \\in \\mathbb{R}^{30}$，其元素为独立的标准正态分布。设 $\\lambda = 0.1$。记录每次完整遍历所有坐标后的目标函数值，并验证该序列在数值容差 $10^{-10}$ 内是否非增。输出布尔值 $b_5$，表示单调性是否成立。\n\n你的程序应产生单行输出，其中包含按顺序 $[e_1, b_2, b_3, e_4, b_5]$ 的逗号分隔列表形式的结果，并用方括号括起来。此问题不涉及物理单位，也无角度单位。所有数值输出应为指定的实数或布尔值，不带百分号。你的实现必须对给定的实例具有鲁棒性，并且不应需要任何用户输入。",
            "solution": "我们从凸优化问题开始\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda \\ge 0$。函数 $f$ 是凸函数，因为它是凸函数 $\\frac{1}{2}\\|A x - b\\|_2^2$ 和凸函数 $\\lambda \\|x\\|_1$ 的和。\n\n循环坐标下降法一次最小化一个坐标上的 $f$，同时保持其他坐标固定。固定一个索引 $i \\in \\{1,\\dots,n\\}$，并用 $a_i \\in \\mathbb{R}^m$ 表示 $A$ 的第 $i$ 列。设 $x \\in \\mathbb{R}^n$ 是当前迭代点，并定义残差\n$$\nr \\triangleq b - A x.\n$$\n因为 $A x = \\sum_{j=1}^n a_j x_j$，只将 $x_i$ 更改为一个新值 $t \\in \\mathbb{R}$ 会得到一个新向量 $x^{(i \\leftarrow t)}$ 和新残差\n$$\nr^{(i \\leftarrow t)} = b - A x^{(i \\leftarrow t)} = b - \\left(A x + a_i (t - x_i)\\right) = r - a_i (t - x_i).\n$$\n目标函数作为 $t$ 的函数（其他坐标固定）变为\n\\begin{align*}\n\\phi_i(t) \\triangleq \\frac{1}{2}\\|A x^{(i \\leftarrow t)} - b\\|_2^2 + \\lambda \\left(\\sum_{j \\ne i} |x_j| + |t|\\right) \\\\\n= \\frac{1}{2}\\|r^{(i \\leftarrow t)}\\|_2^2 + \\lambda |t| + \\text{与 } t \\text{ 无关的常数} \\\\\n= \\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2 + \\lambda |t| + \\text{常数}.\n\\end{align*}\n使用 $\\|u - v\\|_2^2 = \\|u\\|_2^2 - 2 u^\\top v + \\|v\\|_2^2 展开平方范数，我们得到\n\\begin{align*}\n\\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2\n= \\frac{1}{2}\\|r\\|_2^2 - (t - x_i) a_i^\\top r + \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2.\n\\end{align*}\n舍去与 $t$ 无关的项，坐标级目标函数简化为单变量凸函数\n$$\n\\tilde{\\phi}_i(t) \\triangleq \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t|.\n$$\n通过配方法，定义 $d_i \\triangleq \\|a_i\\|_2^2$ 和\n$$\nc_i \\triangleq x_i + \\frac{a_i^\\top r}{d_i} \\quad \\text{当 } d_i > 0 \\text{ 时}.\n$$\n那么\n\\begin{align*}\n\\tilde{\\phi}_i(t)\n= \\frac{1}{2} d_i (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t| \\\\\n= \\frac{1}{2} d_i \\left(t - x_i - \\frac{a_i^\\top r}{d_i}\\right)^2 - \\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i} + \\lambda |t|.\n\\end{align*}\n忽略常数 $-\\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i}$，在 $t$ 上最小化 $\\tilde{\\phi}_i(t)$ 等价于最小化\n$$\n\\psi_i(t) \\triangleq \\frac{1}{2} d_i (t - c_i)^2 + \\lambda |t|.\n$$\n这个一维凸问题的次梯度最优性条件是\n$$\n0 \\in \\partial \\psi_i(t^\\star) = d_i (t^\\star - c_i) + \\lambda \\,\\partial |t^\\star|,\n$$\n其中绝对值的次微分是：如果 $t^\\star \\ne 0$，则 $\\partial |t^\\star| = \\{\\mathrm{sign}(t^\\star)\\}$；如果 $t^\\star = 0$，则 $\\partial |t^\\star| = [-1, 1]$。\n\n考虑两种情况。\n\n情况 1：$t^\\star \\ne 0$。那么次梯度条件是\n$$\nd_i (t^\\star - c_i) + \\lambda \\,\\mathrm{sign}(t^\\star) = 0 \\;\\;\\Longleftrightarrow\\;\\; t^\\star = c_i - \\frac{\\lambda}{d_i} \\,\\mathrm{sign}(t^\\star).\n$$\n这意味着 $|c_i| > \\lambda/d_i$，解通过将 $c_i$ 向零收缩 $\\lambda/d_i$ 同时保持符号来获得：\n$$\nt^\\star = \\mathrm{sign}(c_i)\\left(|c_i| - \\frac{\\lambda}{d_i}\\right).\n$$\n\n情况 2：$t^\\star = 0$。那么次梯度条件变为\n$$\n0 \\in - d_i c_i + \\lambda [-1,1] \\;\\;\\Longleftrightarrow\\;\\; |d_i c_i| \\le \\lambda \\;\\;\\Longleftrightarrow\\;\\; |c_i| \\le \\frac{\\lambda}{d_i}.\n$$\n结合两种情况，得到软阈值形式\n$$\nt^\\star = S_{\\lambda/d_i}(c_i) \\triangleq \\mathrm{sign}(c_i)\\max\\left(|c_i| - \\frac{\\lambda}{d_i}, \\, 0 \\right).\n$$\n等价地，使用残差定义 $r = b - A x$，我们有\n$$\nc_i = x_i + \\frac{a_i^\\top r}{d_i} = \\frac{a_i^\\top r + d_i x_i}{d_i},\n$$\n所以坐标级最小化器为\n$$\nx_i \\leftarrow S_{\\lambda/\\|a_i\\|_2^2}\\!\\left(\\frac{a_i^\\top r + \\|a_i\\|_2^2 x_i}{\\|a_i\\|_2^2}\\right).\n$$\n如果 $d_i = \\|a_i\\|_2^2 = 0$（一个零列），$x_i$ 的任何变化都不会影响二次项；对于 $\\lambda > 0$，$\\lambda |t|$ 的最小化器是 $t^\\star = 0$。在我们的实现中，如果 $d_i = 0$ 且 $\\lambda > 0$，我们设置 $x_i \\leftarrow 0$；如果 $\\lambda = 0$ 且 $d_i = 0$，该坐标是无关的，可以保持不变。\n\n高效的残差更新：如果 $\\Delta_i \\triangleq x_i^{\\text{new}} - x_i^{\\text{old}}$，那么\n$$\nr^{\\text{new}} = b - A x^{\\text{new}} = b - \\left(A x^{\\text{old}} + a_i \\Delta_i\\right) = r^{\\text{old}} - a_i \\Delta_i,\n$$\n这需要 $\\mathcal{O}(m)$ 次操作。\n\n收敛性和单调性：每次坐标更新都会在该坐标上精确地最小化 $f$，因此在每次坐标更新后，$f$ 是非递增的，因此在每轮（完整遍历所有坐标）后也是非递增的。当一轮中最大的坐标绝对变化量低于一个容差或达到最大轮数时，算法终止。\n\n通过 Karush–Kuhn–Tucker (KKT) 条件进行最优性验证：令 $g(x) \\triangleq A^\\top (A x - b)$ 为光滑部分的梯度。LASSO 中 $x^\\star$ 最优的 KKT 条件是\n$$\n0 \\in g(x^\\star) + \\lambda \\,\\partial \\|x^\\star\\|_1,\n$$\n这等价于分量级条件\n$$\n\\begin{cases}\ng_i(x^\\star) + \\lambda \\,\\mathrm{sign}(x_i^\\star) = 0,  \\text{若 } x_i^\\star \\ne 0, \\\\\n|g_i(x^\\star)| \\le \\lambda,  \\text{若 } x_i^\\star = 0.\n\\end{cases}\n$$\n在实践中，我们在一个小的数值容差范围内检查这些等式和不等式。\n\n测试用例和输出：我们实现指定的五个测试用例并计算\n- $e_1 = \\|x_{\\mathrm{cd}} - S_{\\lambda}(b)\\|_\\infty$ 用于正交标准列，\n- $b_2$ 表示高矩阵系统在容差范围内满足 KKT 条件，\n- $b_3$ 表示对于非常大的 $\\lambda$，解为零，\n- $e_4$ 当 $\\lambda = 0$ 时与最小二乘解的相对误差，\n- $b_5$ 表示目标函数值在各轮次间单调非增。\n\n最终程序将结果以单个列表 $[e_1, b_2, b_3, e_4, b_5]$ 的形式单行输出。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S_tau(z) = sign(z) * max(|z| - tau, 0).\"\"\"\n    if tau == 0:\n        return z\n    abs_z = abs(z)\n    if abs_z = tau:\n        return 0.0\n    return np.copysign(abs_z - tau, z)\n\ndef objective_value(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float) -> float:\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lambd * float(np.linalg.norm(x, 1))\n\ndef coordinate_descent_lasso(\n    A: np.ndarray,\n    b: np.ndarray,\n    lambd: float,\n    max_epochs: int = 2000,\n    tol: float = 1e-8,\n    record_objective: bool = False\n):\n    \"\"\"\n    Cyclic coordinate descent for LASSO:\n        minimize 0.5 * ||A x - b||_2^2 + lambd * ||x||_1.\n    Uses residual updates for O(m) per coordinate.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n, dtype=float)\n    r = b - A @ x  # Initialize residual r = b - A x\n\n    obj_hist = []\n\n    # Precompute squared column norms d_i = ||a_i||_2^2\n    col_sq_norms = np.sum(A * A, axis=0)\n\n    for epoch in range(max_epochs):\n        max_delta = 0.0\n        for i in range(n):\n            ai = A[:, i]\n            di = col_sq_norms[i]\n            xi_old = x[i]\n\n            if di == 0.0:\n                xi_new = 0.0 if lambd > 0 else xi_old\n            else:\n                # Update residual to compute partial residual effectively\n                # r_partial = r_full + a_i * x_i_old\n                # Then, the argument to soft-threshold is (a_i.T @ r_partial) / d_i\n                ci = (ai.T @ r + di * xi_old) / di\n                # Update via soft-thresholding with threshold lambd / d_i\n                xi_new = soft_threshold(ci, lambd / di)\n\n            delta = xi_new - xi_old\n            if delta != 0.0:\n                # Update full residual: r_new = r_old - a_i * delta\n                r -= ai * delta\n                x[i] = xi_new\n            \n            max_delta = max(max_delta, abs(delta))\n\n        if record_objective:\n            obj_hist.append(objective_value(A, b, x, lambd))\n\n        if max_delta  tol:\n            break\n\n    if record_objective:\n        return x, obj_hist\n    return x\n\ndef kkt_satisfied(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float, tol: float = 1e-4) -> bool:\n    \"\"\"\n    Check KKT conditions for LASSO:\n      g = A^T (A x - b)\n      If x_i != 0: g_i + lambd * sign(x_i) = 0\n      If x_i == 0: |g_i| = lambd\n    with numerical tolerance tol.\n    \"\"\"\n    g = A.T @ (A @ x - b)\n    for i in range(x.size):\n        xi = x[i]\n        gi = g[i]\n        if abs(xi) > 1e-12: # Check for non-zero with tolerance\n            if abs(gi + lambd * np.sign(xi)) > tol:\n                return False\n        else: # Coefficient is zero\n            if abs(gi) - lambd > tol:\n                return False\n    return True\n\ndef test_suite():\n    results = []\n\n    # Test 1: Orthonormal columns (A = I), analytical solution S_lambda(b)\n    A1 = np.eye(4)\n    b1 = np.array([3.0, -1.0, 0.2, -0.5])\n    lambd1 = 0.7\n    x1 = coordinate_descent_lasso(A1, b1, lambd1, max_epochs=100, tol=1e-12)\n    x1_star = np.array([soft_threshold(b1[i], lambd1) for i in range(4)])\n    e1 = float(np.max(np.abs(x1 - x1_star)))\n    results.append(e1)\n\n    # Test 2: General tall system, KKT check\n    rng = np.random.default_rng(0)\n    A2 = rng.standard_normal((60, 30))\n    col_norms = np.linalg.norm(A2, axis=0)\n    col_norms[col_norms == 0.0] = 1.0\n    A2 = A2 / col_norms\n    x_true = np.zeros(30)\n    nz_idx = [0, 5, 10, 15, 20]\n    nz_vals = [2.5, -1.7, 1.2, -0.9, 1.8]\n    x_true[nz_idx] = nz_vals\n    noise = rng.normal(0.0, 0.01, size=60)\n    b2 = A2 @ x_true + noise\n    lambd2 = 0.05\n    x2 = coordinate_descent_lasso(A2, b2, lambd2, max_epochs=2000, tol=1e-10)\n    b2_ok = kkt_satisfied(A2, b2, x2, lambd2, tol=1e-4)\n    results.append(bool(b2_ok))\n\n    # Test 3: Large lambda -> zero solution\n    lambd3 = 1e6\n    x3 = coordinate_descent_lasso(A1, b1, lambd3, max_epochs=50, tol=1e-14)\n    b3_ok = bool(np.allclose(x3, 0.0, atol=1e-12))\n    results.append(b3_ok)\n\n    # Test 4: Zero lambda -> least squares\n    rng1 = np.random.default_rng(1)\n    A4 = rng1.standard_normal((40, 10))\n    rng2 = np.random.default_rng(2)\n    b4 = rng2.standard_normal(40)\n    lambd4 = 0.0\n    x4_cd = coordinate_descent_lasso(A4, b4, lambd4, max_epochs=5000, tol=1e-12)\n    x4_ls, *_ = np.linalg.lstsq(A4, b4, rcond=None)\n    denom = max(np.linalg.norm(x4_ls), 1e-12)\n    e4 = float(np.linalg.norm(x4_cd - x4_ls) / denom)\n    results.append(e4)\n\n    # Test 5: Monotone objective decrease across epochs\n    rng3 = np.random.default_rng(3)\n    A5 = rng3.standard_normal((30, 15))\n    b5 = rng3.standard_normal(30)\n    lambd5 = 0.1\n    x5, obj_hist = coordinate_descent_lasso(A5, b5, lambd5, max_epochs=200, tol=1e-10, record_objective=True)\n    diffs = np.diff(obj_hist)\n    b5_ok = bool(np.all(diffs = 1e-10))\n    results.append(b5_ok)\n\n    return results\n\ndef solve():\n    results = test_suite()\n    # Format to string representation of a list\n    # e.g., '[0.0,True,True,2.83151523999908e-15,True]'\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在掌握了坐标下降法的基本实现后，我们来探讨一个至关重要的实际问题：特征标准化。坐标下降的更新规则表明，施加于每个系数的有效正则化强度，会受到对应特征列范数的影响。本练习通过一个精心设计的实验，生动地展示了特征尺度的差异如何导致算法错误地偏爱某些特征，从而强调了数据预处理在正则化模型中的关键作用。",
            "id": "3111928",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 回归问题，该问题旨在寻找系数 $\\beta \\in \\mathbb{R}^p$ 以最小化目标函数\n$$\n\\frac{1}{2}\\,\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta \\rVert_1,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个设计矩阵，其列为 $X_{\\cdot j}$，$y \\in \\mathbb{R}^n$ 是一个响应向量，$\\lambda \\ge 0$ 是一个正则化参数。在循环坐标下降法中，每次更新单个系数 $\\beta_j$，同时保持所有其他坐标固定，并重复循环所有坐标直至收敛。在实践中，列 $X_{\\cdot j}$ 的尺度会显著影响坐标级别的更新，当特征未标准化时，这可能会扭曲有效的收缩并使特征选择产生偏差。对于本任务，特征标准化意味着缩放每一列 $X_{\\cdot j}$，使其欧几里得范数的平方等于样本大小 $n$，即 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 从 $\\beta = 0$ 开始，实现循环坐标下降法以最小化 LASSO 目标函数，并迭代直到系数在 $\\ell_\\infty$ 范数下的变化小于固定的容差 $\\tau$，或达到最大迭代次数。\n- 对每个测试用例，以两种模式运行求解器：不进行特征标准化和进行特征标准化（将每列缩放至欧几里得范数的平方等于 $n$）。\n- 使用以下确定性的玩具数据集，这些数据集旨在揭示特征标准化缺失如何扭曲坐标更新并使特征选择产生偏差。在所有情况下，随机抽样均来自标准正态分布，并且必须按每个案例中的规定为伪随机数生成器设置种子以保证可复现性。\n\n测试套件定义：\n- 案例 A（尺度倾斜和共线性特征）：\n  - 参数：$n = 60$，$p = 2$，$c = 20$，$\\lambda = 200$，种子 $= 13$。\n  - 数据生成：抽取 $u \\in \\mathbb{R}^n$、$e_1 \\in \\mathbb{R}^n$、$e_2 \\in \\mathbb{R}^n$ 和 $e_y \\in \\mathbb{R}^n$，每个向量的元素均为独立同分布的标准正态分布。令 $x_1 = u + 0.01\\, e_1$，$x_2 = c\\, u + 0.5\\, e_2$，$y = u + 0.01\\, e_y$。构成 $X = [x_1, x_2]$。\n- 案例 B（基准，具有一个相关特征和一个无关特征的等尺度特征）：\n  - 参数：$n = 80$，$p = 2$，$\\lambda = 30$，种子 $= 7$。\n  - 数据生成：抽取 $u \\in \\mathbb{R}^n$、$v \\in \\mathbb{R}^n$、$e_1 \\in \\mathbb{R}^n$、$e_2 \\in \\mathbb{R}^n$ 和 $e_y \\in \\mathbb{R}^n$。令 $x_1 = u + 0.01\\, e_1$，$x_2 = v + 0.01\\, e_2$（其中 $u$ 与 $v$ 独立），$y = u + 0.01\\, e_y$。构成 $X = [x_1, x_2]$。\n- 案例 C（具有非常强正则化的边界情况）：\n  - 参数：$n = 80$，$p = 2$，$\\lambda = 2000$，种子 $= 11$。\n  - 数据生成：与案例 B 相同。\n\n算法要求：\n- 使用在每次迭代中跨坐标维护的残差 $r = y - X \\beta$ 来实现循环坐标下降更新，当单个 $\\beta_j$ 发生变化时，增量更新 $r$。\n- 使用收敛容差 $\\tau = 10^{-8}$ 和最多 $1000$ 次对所有坐标的完整循环。\n- 收敛后，将满足 $\\lvert \\beta_j \\rvert  10^{-8}$ 的索引 $j$ 识别为所选特征。\n\n最终输出规范：\n- 对于每个测试用例，生成一个包含以下内容的三元组：\n  - 未进行标准化时所选特征的索引列表。\n  - 进行标准化后所选特征的索引列表。\n  - 一个布尔值，指示这两个列表是否不同。\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个元素必须是上述的三元组，并以 Python 风格列表的字面量形式表示。例如，格式应类似于 $[\\,[\\![\\dots]\\! ,\\ [\\![\\dots]\\!] ,\\ \\text{True}]\\, ,\\ \\dots]$，但打印为标准的 Python 列表字面量，例如 `[[[1],[0],True],[[0],[0],False],[[],[],False]]`。不涉及物理单位、角度单位或百分比；所有输出均为纯数学列表和布尔值。",
            "solution": "该问题经评估有效。它在统计学习和数值优化领域是一个适定、有科学依据且客观的问题。所有必要的参数、数据生成过程和算法约束都已明确定义，从而允许一个唯一且可验证的解。\n\n任务是实现一个循环坐标下降算法来解决 LASSO（最小绝对收缩和选择算子）回归问题。需要最小化的目标函数是：\n$$\nL(\\beta) = \\frac{1}{2}\\,\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta \\rVert_1\n$$\n其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\lambda \\ge 0$ 是正则化参数。$\\ell_1$ 范数惩罚项 $\\lVert \\beta \\rVert_1 = \\sum_{j=1}^p |\\beta_j|$ 能在解中诱导稀疏性。\n\n坐标下降法一次仅针对单个系数 $\\beta_j$ 优化目标函数，同时保持所有其他系数 $\\beta_k$（对于 $k \\ne j$）固定。此过程在所有系数上循环进行，直至收敛。\n\n为了推导单个系数 $\\beta_j$ 的更新法则，我们将目标函数视为仅关于 $\\beta_j$ 的函数：\n$$\nL(\\beta_j) = \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - \\sum_{k \\ne j} X_{ik}\\beta_k - X_{ij}\\beta_j\\right)^2 + \\lambda \\sum_{k \\ne j} |\\beta_k| + \\lambda |\\beta_j|\n$$\n令 $r_{(-j)} = y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k$ 为偏残差向量，其中 $X_{\\cdot k}$ 是 $X$ 的第 $k$ 列。$L(\\beta_j)$ 中依赖于 $\\beta_j$ 的项是：\n$$\nL_j(\\beta_j) = \\frac{1}{2} \\lVert r_{(-j)} - X_{\\cdot j}\\beta_j \\rVert_2^2 + \\lambda |\\beta_j|\n$$\n这是一个一维的 LASSO 问题。为求最小值，我们计算关于 $\\beta_j$ 的次梯度并将其设为 $0$。次梯度 $\\partial L_j(\\beta_j)$ 是：\n$$\n\\partial L_j(\\beta_j) = -X_{\\cdot j}^T (r_{(-j)} - X_{\\cdot j}\\beta_j) + \\lambda \\partial |\\beta_j|\n$$\n其中 $\\partial |\\beta_j|$ 是绝对值函数的次梯度。将次梯度设为 $0$ 得到最优性条件：\n$$\nX_{\\cdot j}^T X_{\\cdot j} \\beta_j - X_{\\cdot j}^T r_{(-j)} \\in \\lambda \\partial |\\beta_j|\n$$\n令 $\\rho_j = X_{\\cdot j}^T r_{(-j)}$ 且 $z_j = X_{\\cdot j}^T X_{\\cdot j} = \\lVert X_{\\cdot j} \\rVert_2^2$。该条件变为 $z_j \\beta_j - \\rho_j \\in \\lambda \\partial |\\beta_j|$。此问题的解由软阈值算子 $S(\\cdot, \\cdot)$ 给出：\n$$\n\\hat{\\beta}_j = S\\left(\\frac{\\rho_j}{z_j}, \\frac{\\lambda}{z_j}\\right)\n$$\n其中 $S(a, \\nu) = \\text{sign}(a) \\max(|a| - \\nu, 0)$。这可以分段写为：\n$$\n\\hat{\\beta}_j =\n\\begin{cases}\n(\\rho_j + \\lambda) / z_j  \\text{若 } \\rho_j  -\\lambda \\\\\n(\\rho_j - \\lambda) / z_j  \\text{若 } \\rho_j > \\lambda \\\\\n0  \\text{若 } |\\rho_j| \\le \\lambda\n\\end{cases}\n$$\n为实现高效计算，我们维护完整的残差 $r = y - X\\beta$ 并进行增量更新。偏残差项 $\\rho_j$ 可以用完整残差表示。设 $\\beta^{\\text{old}}$ 是更新 $\\beta_j$ 之前的系数向量。当前的完整残差是 $r = y - \\sum_k X_{\\cdot k}\\beta_k^{\\text{old}}$。偏残差是 $r_{(-j)} = y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k^{\\text{old}} = r + X_{\\cdot j}\\beta_j^{\\text{old}}$。\n因此，$\\rho_j = X_{\\cdot j}^T (r + X_{\\cdot j}\\beta_j^{\\text{old}}) = X_{\\cdot j}^T r + z_j \\beta_j^{\\text{old}}$。\n\n循环坐标下降算法的步骤如下：\n1. 初始化 $\\beta = 0$，$r = y$。对所有 $j=1, \\dots, p$，预计算 $z_j = \\lVert X_{\\cdot j} \\rVert_2^2$。\n2. 对于每次迭代（循环）：\n3.   对于每个坐标 $j=1, \\dots, p$：\n    a. 令 $\\beta_j^{\\text{old}}$ 为第 $j$ 个系数的当前值。\n    b. 更新残差以有效计算偏残差：$r \\leftarrow r + X_{\\cdot j}\\beta_j^{\\text{old}}$。\n    c. 计算 $\\rho_j = X_{\\cdot j}^T r$。\n    d. 使用 $\\rho_j$ 和 $z_j$ 通过软阈值法则计算新系数 $\\beta_j^{\\text{new}}$。\n    e. 更新系数：$\\beta_j \\leftarrow \\beta_j^{\\text{new}}$。\n    f. 使用新系数更新残差：$r \\leftarrow r - X_{\\cdot j}\\beta_j^{\\text{new}}$。\n    g. 在循环中跟踪最大系数变化量 $\\max_j |\\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}|$。\n4.   完成一个完整循环后，如果最大系数变化量小于容差 $\\tau=10^{-8}$，或达到最大迭代次数，则停止。\n\n该问题要求在两种模式下运行此算法：一种使用原始数据 $X$，另一种使用标准化后的特征。标准化确保了有效的正则化不会因特征尺度的不同而产生偏差。指定的标准化规则是缩放每列 $X_{\\cdot j}$ 以形成新列 $X'_{\\cdot j}$，使其欧几里得范数的平方等于样本大小 $n$，即 $\\lVert X'_{\\cdot j} \\rVert_2^2 = n$。第 $j$ 列的缩放因子为 $s_j = \\sqrt{n / z_j}$。标准化后的矩阵为 $X'_{\\cdot j} = s_j X_{\\cdot j}$。当使用 $X'$ 时，更新法则中的所有 $z'_j$ 都等于 $n$，使得收缩项 $\\lambda/n$ 对所有系数都是统一的。\n\n解答程序为指定的测试案例实现了该算法，生成数据，在有和没有标准化的两种情况下运行求解器，并报告所选特征的索引（其中 $|\\beta_j| > 10^{-8}$）。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(rho, lambd):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(rho) * np.maximum(np.abs(rho) - lambd, 0)\n\ndef coordinate_descent(X, y, lambda_val, tol=1e-8, max_iter=1000):\n    \"\"\"Solves the LASSO problem using cyclic coordinate descent.\"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n    \n    # Precompute squared l2-norms of columns of X.\n    z = np.sum(X**2, axis=0)\n\n    # Initialize residual. Since beta starts at 0, r = y.\n    r = y.copy()\n    \n    for _ in range(max_iter):\n        max_change = 0.0\n\n        for j in range(p):\n            beta_old_j = beta[j]\n\n            if z[j] == 0:\n                continue\n\n            # Calculate rho_j using the full residual r and the old beta_j\n            # rho_j = X_j^T * (y - sum_{k!=j} X_k beta_k) = X_j^T * (r + X_j * beta_old_j)\n            rho_j = X[:, j].T @ (r + X[:, j] * beta_old_j)\n            \n            # Update beta_j using soft-thresholding\n            # Solution to min_{b} 0.5 ||rho_j/sqrt(z_j) - sqrt(z_j) b||^2 + lambda |b|\n            # is b = S(rho_j/z_j, lambda/z_j).\n            beta_new_j = soft_threshold(rho_j / z[j], lambda_val / z[j])\n            \n            delta = beta_new_j - beta_old_j\n            \n            if delta != 0:\n                beta[j] = beta_new_j\n                # Update residual incrementally\n                r -= X[:, j] * delta\n            \n            max_change = max(max_change, abs(delta))\n        \n        if max_change  tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    \n    def run_case(n, lambda_val, seed, case_id):\n        rng = np.random.default_rng(seed)\n        \n        if case_id == 'A':\n            c = 20\n            u = rng.standard_normal(n)\n            e1 = rng.standard_normal(n)\n            e2 = rng.standard_normal(n)\n            ey = rng.standard_normal(n)\n            x1 = u + 0.01 * e1\n            x2 = c * u + 0.5 * e2\n            y = u + 0.01 * ey\n            X = np.stack([x1, x2], axis=1)\n        elif case_id in ['B', 'C']:\n            u = rng.standard_normal(n)\n            v = rng.standard_normal(n)\n            e1 = rng.standard_normal(n)\n            e2 = rng.standard_normal(n)\n            ey = rng.standard_normal(n)\n            x1 = u + 0.01 * e1\n            x2 = v + 0.01 * e2\n            y = u + 0.01 * ey\n            X = np.stack([x1, x2], axis=1)\n\n        # Run without standardization\n        beta_unstd = coordinate_descent(X, y, lambda_val)\n        selected_unstd = [j for j, b in enumerate(beta_unstd) if abs(b) > 1e-8]\n\n        # Run with standardization\n        X_orig = X.copy()\n        col_norms_sq = np.sum(X_orig**2, axis=0)\n        # Handle potential zero-norm columns to avoid division by zero\n        scale_factors = np.ones_like(col_norms_sq)\n        non_zero_mask = col_norms_sq > 1e-12\n        scale_factors[non_zero_mask] = np.sqrt(n / col_norms_sq[non_zero_mask])\n        \n        X_std = X_orig * scale_factors\n        \n        # Solve for beta_std in the standardized space\n        beta_std_space = coordinate_descent(X_std, y, lambda_val)\n        \n        # The selected feature indices are the same, regardless of the space\n        selected_std = [j for j, b in enumerate(beta_std_space) if abs(b) > 1e-8]\n        \n        # Compare results\n        is_different = (set(selected_unstd) != set(selected_std))\n        \n        return [selected_unstd, selected_std, is_different]\n\n    # Test Suite\n    results = []\n    # Case A\n    results.append(run_case(n=60, lambda_val=200, seed=13, case_id='A'))\n    # Case B\n    results.append(run_case(n=80, lambda_val=30, seed=7, case_id='B'))\n    # Case C\n    results.append(run_case(n=80, lambda_val=2000, seed=11, case_id='C'))\n\n    print(results)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "当模型中的特征完全相同时（即存在完美共线性），LASSO的解便不再是唯一的。本练习旨在揭示坐标下降法在这种情形下的一个微妙特性：路径依赖性。您将通过一个实验发现，仅仅改变坐标更新的顺序，就会导致算法在相同的特征上分配不同的系数，尽管这些解在预测上是等价的。这个过程能够加深您对优化算法与问题几何形态之间相互作用的理解。",
            "id": "3111866",
            "problem": "您的任务是为最小绝对值收敛和选择算子 (LASSO) 实现一个循环坐标下降算法，该算法旨在最小化以下凸目标函数：\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1,\n$$\n其中 $n$ 是样本数，$p$ 是特征数，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{y} \\in \\mathbb{R}^{n}$ 是响应向量，$\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是正则化参数，$\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数，$\\left\\| \\cdot \\right\\|_1$ 表示 $L_1$ 范数。从基本的凸优化原理和次梯度最优性条件出发，推导并实现一个坐标更新法则，该法则在保持其他坐标固定的同时，沿单个坐标最小化目标函数。您的实现必须：\n- 在对所有坐标的遍历中，按照指定的坐标顺序执行循环更新。\n- 在每次坐标更新后，高效地维护和更新残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。\n- 当单次完整遍历中坐标的最大绝对变化量小于一个容差值（使用 $10^{-12}$）时，或者当达到最大遍历次数（使用 $100$）时，算法终止。\n\n需要研究的核心现象是当 $\\boldsymbol{X}$ 的两列完全相同时解的非唯一性。在这种情况下，目标函数仅取决于重复系数之和，对重复坐标进行不同的分配可以达到相同的目标函数值。您必须通过实验证明坐标下降的路径依赖性和对称性破缺：改变坐标更新的顺序会导致重复特征的系数分配不同，尽管预测结果相同。\n\n实现一个程序，该程序对每个测试用例运行两次坐标下降算法，使用两种不同的坐标顺序：首先是顺序 $[0,1]$，然后是顺序 $[1,0]$。计算并报告能够捕捉两次运行之间差异的量化指标。\n\n对所有测试用例使用以下固定数据：\n- 设 $n = 10$，并定义向量 $\\boldsymbol{x} = [1,2,3,4,5,6,7,8,9,10]^\\top$。\n- 构建 $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times 2}$，使其两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。\n- 定义三个测试用例，它们在响应向量 $\\boldsymbol{y}$ 和正则化参数 $\\lambda$ 上有所不同：\n  1. 测试用例 A (happy path): $\\boldsymbol{y} = 3 \\boldsymbol{x}$ 和 $\\lambda = 5$。\n  2. 测试用例 B (sign edge case): $\\boldsymbol{y} = -3 \\boldsymbol{x}$ 和 $\\lambda = 5$。\n  3. 测试用例 C (boundary case): $\\boldsymbol{y} = 3 \\boldsymbol{x}$ 和 $\\lambda = 200$。\n\n初始化和停止条件：\n- 在所有运行中，将 $\\boldsymbol{\\beta}$ 初始化为零向量。\n- 按规定使用容差值 $10^{-12}$ 和最大遍历次数 $100$。\n\n对于每个测试用例，运行算法两次（一次使用坐标更新顺序 $[0,1]$，一次使用 $[1,0]$），并计算以下四个指标：\n1. 两个系数向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{\\beta}^{(01)} - \\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，作为一个浮点数。\n2. 重复系数之和的绝对差，即 $ \\left| \\left( \\beta^{(01)}_0 + \\beta^{(01)}_1 \\right) - \\left( \\beta^{(10)}_0 + \\beta^{(10)}_1 \\right) \\right| $，作为一个浮点数。\n3. 两个预测向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{X}\\boldsymbol{\\beta}^{(01)} - \\boldsymbol{X}\\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，作为一个浮点数。\n4. 一个指示对称性破缺的布尔值，定义为在至少一次运行中重复系数是否不同，即 $ \\left| \\beta^{(01)}_0 - \\beta^{(01)}_1 \\right|  10^{-9} $ 或 $ \\left| \\beta^{(10)}_0 - \\beta^{(10)}_1 \\right|  10^{-9} $ 是否成立。\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，结果形式为用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是按上述顺序排列的四个指标的列表。例如，输出必须具有以下形式：\n$$\n[\\,[m_{A,1},m_{A,2},m_{A,3},b_A],\\,[m_{B,1},m_{B,2},m_{B,3},b_B],\\,[m_{C,1},m_{C,2},m_{C,3},b_C]\\,],\n$$\n其中 $m_{\\cdot,\\cdot}$ 是浮点数，$b_{\\cdot}$ 是布尔值。不涉及任何物理单位或角度；所有量都是无量纲的实数。请遵循目标函数的凸性和基于次梯度的坐标最小化定义，不依赖于特设的启发式方法，以确保科学真实性。",
            "solution": "用户提供的问题是有效的。这是一个在计算统计学领域中定义明确且具有科学依据的任务，具体聚焦于 LASSO 回归的坐标下降算法。所有必要的参数和条件都已提供，其目标是探索在存在共线性特征时解的路径依赖现象，这是优化中的一个标准课题。\n\n### 1. LASSO 目标函数\n\n该问题要求最小化 LASSO 目标函数，该函数由一个最小二乘数据保真项和一个 $L_1$ 范数正则化项组成。对于系数向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$，目标函数 $L(\\boldsymbol{\\beta})$ 由下式给出：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1\n$$\n在这里，$\\boldsymbol{y} \\in \\mathbb{R}^n$ 是响应向量，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$n$ 是样本数，$p$ 是特征数，$\\lambda \\ge 0$ 是正则化参数。$L_1$ 范数定义为 $\\left\\| \\boldsymbol{\\beta} \\right\\|_1 = \\sum_{j=1}^p |\\beta_j|$。此目标函数是凸的，这保证了全局最小值的存在。\n\n### 2. 坐标更新法则的推导\n\n坐标下降法一次只针对一个坐标优化目标函数，同时保持所有其他坐标固定。为了推导第 $k$ 个系数 $\\beta_k$ 的更新规则，我们视所有其他系数 $\\beta_j$（对于 $j \\neq k$）为常数。\n\n目标函数可以写成将涉及 $\\beta_k$ 的项分离出来的形式：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda \\sum_{j \\neq k} |\\beta_j| + \\lambda |\\beta_k|\n$$\n我们定义部分残差 $\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j$。这些是在模型中移除特征 $k$ 时的残差。不涉及 $\\beta_k$ 的项在对 $\\beta_k$ 进行最小化时是常数。因此，需要为 $\\beta_k$ 最小化的目标是：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left\\| \\boldsymbol{r}_k - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda |\\beta_k|\n$$\n展开二次项得到：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left( \\boldsymbol{r}_k^\\top \\boldsymbol{r}_k - 2\\beta_k \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k^2 \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k} \\right) + \\lambda |\\beta_k|\n$$\n为了找到这个凸函数的最小值，我们使用次梯度最优性条件，该条件表明在最小值点，$0$ 必须在 $L_k(\\beta_k)$ 的次梯度中。绝对值函数 $|\\cdot|$ 的次梯度是符号函数 `sgn`，它在 $0$ 处是一个集合值函数。\n$$\n\\frac{\\partial L_k(\\beta_k)}{\\partial \\beta_k} = \\frac{1}{n} \\left( -\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) \\right) + \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n将次梯度设置为包含 $0$：\n$$\n0 \\in \\frac{1}{n} \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k - \\beta_k \\frac{1}{n} \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n我们定义 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k$ 和 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}$。条件变为：\n$$\n\\beta_k z_k \\in \\rho_k - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n这导出了软阈值函数 $S(a, \\delta) = \\text{sgn}(a)\\max(|a|-\\delta, 0)$。$\\beta_k$ 的解是：\n$$\n\\beta_k = \\frac{S(\\rho_k, \\lambda)}{z_k}\n$$\n此更新法则沿第 $k$ 个坐标最小化目标函数。\n\n### 3. 坐标下降算法与高效更新\n\n循环坐标下降算法重复迭代所有坐标 $k=0, 1, \\dots, p-1$ 直至收敛。为了计算效率，我们避免在每一步重新计算部分残差 $\\boldsymbol{r}_k$。相反，我们维护完整的残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。部分残差可以用完整残差和更新前 $\\beta_k$ 的当前值表示：\n$$\n\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j = \\left(\\boldsymbol{y} - \\sum_{j=1}^p \\boldsymbol{X}_{:,j}\\beta_j \\right) + \\boldsymbol{X}_{:,k}\\beta_k = \\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k\n$$\n因此，项 $\\rho_k$ 可以计算为 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k)$，其中 $\\boldsymbol{r}$ 和 $\\beta_k$ 是对坐标 $k$ 进行更新前的值。\n\n将 $\\beta_k$ 从 $\\beta_k^{\\text{old}}$ 更新为 $\\beta_k^{\\text{new}}$ 后，完整残差 $\\boldsymbol{r}$ 也必须更新。预测值的变化是 $\\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。新的残差是：\n$$\n\\boldsymbol{r}^{\\text{new}} = \\boldsymbol{r}^{\\text{old}} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})\n$$\n这是一个高效的 $O(n)$ 更新。算法如下：\n\n1. 初始化 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 和 $\\boldsymbol{r} = \\boldsymbol{y}$。对所有 $k$ 预计算 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top\\boldsymbol{X}_{:,k}$。\n2. 对于每次遍历，直至达到最大遍历次数：\n    a. 初始化 `max_abs_change` 为 $0$。\n    b. 对于指定顺序中的每个坐标 $k$：\n        i. 存储 $\\beta_k^{\\text{old}} = \\beta_k$。\n        ii. 计算 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k^{\\text{old}})$。\n        iii. 计算 $\\beta_k^{\\text{new}} = S(\\rho_k, \\lambda) / z_k$。\n        iv. 更新残差：$\\boldsymbol{r} \\leftarrow \\boldsymbol{r} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。\n        v. 更新系数：$\\beta_k \\leftarrow \\beta_k^{\\text{new}}$。\n        vi. 用 $|\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}}|$ 更新 `max_abs_change`。\n    c. 如果 `max_abs_change` 小于一个容差值（例如 $10^{-12}$），则终止。\n3. 返回最终的系数向量 $\\boldsymbol{\\beta}$。\n\n### 4. 相同列导致的非唯一性\n\n该问题设置了一个情景，其中 $\\boldsymbol{X}$ 的两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。在这种情况下，模型预测为：\n$$\n\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X}_{:,0}\\beta_0 + \\boldsymbol{X}_{:,1}\\beta_1 = \\boldsymbol{x}\\beta_0 + \\boldsymbol{x}\\beta_1 = (\\beta_0 + \\beta_1)\\boldsymbol{x}\n$$\n目标函数的数据保真项仅取决于系数之和 $\\beta_{\\text{sum}} = \\beta_0 + \\beta_1$。$L_1$ 惩罚项是 $\\lambda(|\\beta_0| + |\\beta_1|)$。优化问题变为：\n$$\n\\min_{\\beta_0, \\beta_1} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - (\\beta_0 + \\beta_1)\\boldsymbol{x} \\right\\|_2^2 + \\lambda(|\\beta_0| + |\\beta_1|)\n$$\n虽然和 $\\beta_{\\text{sum}}$ 的最优值是唯一的，但可以有无穷多对 $(\\beta_0, \\beta_1)$ 产生这个和。对于任意给定的最优 $\\beta_{\\text{sum}}$，当 $\\beta_{\\text{sum}}$ 的全部量级被分配给单个系数时（例如，$\\beta_0 = \\beta_{\\text{sum}}, \\beta_1 = 0$ 或反之），$L_1$ 惩罚项 $\\lambda(|\\beta_0| + |\\beta_1|)$ 被最小化，假设 $\\beta_{\\text{sum}} \\neq 0$。\n\n循环坐标下降算法根据更新顺序打破了这种对称性。当首先更新坐标 $0$ 时，它将尽可能多地吸收信号，可能为坐标 $1$ 留下很小的残差。如果残差信号低于由 $\\lambda$ 设定的阈值，则 $\\beta_1$ 的更新将为零。将顺序颠倒为 $[1, 0]$ 将导致 $\\beta_1$ 首先吸收信号，从而得到一个不同的最终系数向量 $\\boldsymbol{\\beta}$，即使和 $\\beta_0+\\beta_1$ 以及预测值 $\\boldsymbol{X}\\boldsymbol{\\beta}$ 将是相同的（在数值精度范围内）。这种路径依赖性是坐标下降在非严格凸目标上运行的一个关键特征。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the coordinate descent for LASSO to demonstrate path dependence.\n    \"\"\"\n\n    def coordinate_descent(X, y, lambda_val, order, tol, max_passes):\n        \"\"\"\n        Performs cyclic coordinate descent for LASSO.\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n        residual = y.copy()\n\n        # Precompute column normalization factors\n        z = np.array([(1/n) * X[:, k].T @ X[:, k] for k in range(p)])\n\n        def soft_threshold(a, delta):\n            return np.sign(a) * np.maximum(np.abs(a) - delta, 0)\n\n        for _ in range(max_passes):\n            max_abs_change = 0.0\n            \n            for k in order:\n                beta_old_k = beta[k]\n                \n                # Calculate rho_k using the efficient residual update formula\n                # rho_k = (1/n) * X_k^T * (y - sum_{j!=k} X_j * beta_j)\n                # which is equivalent to (1/n) * X_k^T * (residual + X_k * beta_old_k)\n                rho_k = (1/n) * X[:, k].T @ (residual + X[:, k] * beta_old_k)\n\n                # Update beta_k using soft-thresholding\n                beta_new_k = soft_threshold(rho_k, lambda_val) / z[k] if z[k] != 0 else 0\n                \n                delta_beta_k = beta_new_k - beta_old_k\n                \n                if delta_beta_k != 0:\n                    beta[k] = beta_new_k\n                    # Update residual efficiently\n                    residual -= X[:, k] * delta_beta_k\n\n                max_abs_change = max(max_abs_change, np.abs(delta_beta_k))\n\n            if max_abs_change  tol:\n                break\n                \n        return beta\n\n    # --- Fixed Data ---\n    n = 10\n    x_vec = np.arange(1, 11, dtype=float)\n    X = np.stack([x_vec, x_vec], axis=1)\n    \n    # --- Algorithm Parameters ---\n    tol = 1e-12\n    max_passes = 100\n    sym_tol = 1e-9\n\n    # --- Test Cases ---\n    test_cases_params = [\n        # Case A: happy path\n        {'y': 3 * x_vec, 'lambda': 5.0},\n        # Case B: sign edge case\n        {'y': -3 * x_vec, 'lambda': 5.0},\n        # Case C: boundary case (high regularization)\n        {'y': 3 * x_vec, 'lambda': 200.0},\n    ]\n\n    results = []\n    \n    for params in test_cases_params:\n        y = params['y']\n        lambda_val = params['lambda']\n\n        # Run with order [0, 1]\n        beta_01 = coordinate_descent(X, y, lambda_val, order=[0, 1], tol=tol, max_passes=max_passes)\n        \n        # Run with order [1, 0]\n        beta_10 = coordinate_descent(X, y, lambda_val, order=[1, 0], tol=tol, max_passes=max_passes)\n\n        # --- Compute Metrics ---\n        # 1. Euclidean norm of the difference between the two coefficient vectors\n        metric1 = np.linalg.norm(beta_01 - beta_10)\n\n        # 2. Absolute difference in the sum of the duplicate coefficients\n        metric2 = np.abs(np.sum(beta_01) - np.sum(beta_10))\n\n        # 3. Euclidean norm of the difference between the two prediction vectors\n        pred_01 = X @ beta_01\n        pred_10 = X @ beta_10\n        metric3 = np.linalg.norm(pred_01 - pred_10)\n\n        # 4. Boolean indicating symmetry-breaking\n        m4_cond1 = np.abs(beta_01[0] - beta_01[1]) > sym_tol\n        m4_cond2 = np.abs(beta_10[0] - beta_10[1]) > sym_tol\n        metric4 = m4_cond1 or m4_cond2\n\n        results.append([metric1, metric2, metric3, metric4])\n\n    # Format output as specified: [[m_A1,m_A2,m_A3,b_A],[m_B1,m_B2,m_B3,b_B],...]\n    inner_results_str = []\n    for metrics in results:\n        m1, m2, m3, b = metrics\n        # Use str(b) to get 'True' or 'False'\n        inner_results_str.append(f\"[{m1},{m2},{m3},{str(b)}]\")\n    \n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}