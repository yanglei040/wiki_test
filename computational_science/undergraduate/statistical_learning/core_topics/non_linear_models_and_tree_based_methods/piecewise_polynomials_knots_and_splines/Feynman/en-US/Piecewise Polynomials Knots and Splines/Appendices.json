{
    "hands_on_practices": [
        {
            "introduction": "The truncated power basis is conceptually simple, but its polynomial and hinge-like terms are often highly correlated. This multicollinearity can inflate the variance of coefficient estimates, making them unstable and difficult to interpret. This exercise  provides a hands-on look at this numerical issue by comparing the coefficient variances from a raw truncated power basis to those from a numerically stable orthonormal basis, illustrating a crucial step in building robust spline models.",
            "id": "3157128",
            "problem": "You are given the task of analyzing coefficient variance in ordinary least squares (OLS) linear regression models that use cubic spline bases. The foundational setup is the linear model $y = X \\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $X$ is a design matrix whose columns are spline basis functions evaluated at $n$ inputs. The well-tested formula for the sampling variance of the OLS estimator is $\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$, under full column rank and homoscedasticity. You will construct a raw cubic spline truncated power basis, orthogonalize it via the Gram–Schmidt procedure to reduce collinearity, and compare the theoretical coefficient variances for the raw basis versus the orthonormalized basis.\n\nDefinitions and construction:\n- A cubic truncated power spline basis with internal knots $\\{k_1,\\dots,k_m\\}$ on $[0,1]$ consists of the functions $\\phi_0(x)=1$, $\\phi_1(x)=x$, $\\phi_2(x)=x^2$, $\\phi_3(x)=x^3$, and for each internal knot $k_j$, a function $\\phi_{3+j}(x) = \\left( x - k_j \\right)_+^3$, where $(u)_+ = \\max(u, 0)$.\n- For inputs $\\{x_i\\}_{i=1}^n$, the design matrix $X \\in \\mathbb{R}^{n \\times p}$ has entries $X_{i,j} = \\phi_j(x_i)$ for $j=0,\\dots,3+m$, so that $p = 4 + m$.\n- Gram–Schmidt orthogonalization constructs a matrix $Q \\in \\mathbb{R}^{n \\times p}$ whose columns are orthonormal vectors spanning the same column space as $X$, i.e., $Q^\\top Q = I_p$ and $\\operatorname{span}\\{Q_{\\cdot,1},\\dots,Q_{\\cdot,p}\\} = \\operatorname{span}\\{X_{\\cdot,1},\\dots,X_{\\cdot,p}\\}$.\n\nTasks to implement:\n1. For each test case, create a grid of $n$ inputs $x_i = \\frac{i - 0.5}{n}$ for $i = 1,\\dots,n$ on $[0,1]$.\n2. Construct the raw cubic truncated power basis design matrix $X$ using the specified internal knots for the case.\n3. Compute the theoretical OLS coefficient variance vector for the raw basis, defined as $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (X^\\top X)^{-1} \\right)$, where $\\operatorname{diag}(A)$ denotes the vector of diagonal entries of matrix $A$.\n4. Orthogonalize the columns of $X$ using the classical modified Gram–Schmidt procedure to obtain $Q$ with $Q^\\top Q \\approx I_p$.\n5. Compute the theoretical OLS coefficient variance vector for the orthonormal basis, defined as $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (Q^\\top Q)^{-1} \\right)$.\n6. Round each variance value to six decimal places.\n\nTest suite:\n- Case A (happy path): $n = 200$, internal knots $\\{0.3, 0.6\\}$, noise variance $\\sigma^2 = 0.5$.\n- Case B (edge case with closely spaced knots increasing collinearity): $n = 50$, internal knots $\\{0.49, 0.5, 0.51\\}$, noise variance $\\sigma^2 = 1.0$.\n- Case C (boundary case with no internal knots; pure cubic polynomial basis): $n = 150$, internal knots $\\{\\}$, noise variance $\\sigma^2 = 0.2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a pair of lists $[v_{\\text{raw}}, v_{\\text{orth}}]$, where $v_{\\text{raw}}$ is the list of raw-basis coefficient variances and $v_{\\text{orth}}$ is the list of orthonormal-basis coefficient variances, both rounded to six decimal places. For example, an output with two cases might look like $[[[0.1,0.2],[0.1,0.1]],[[0.3,0.4],[0.3,0.3]]]$. No additional text should be printed.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of statistical linear models and numerical linear algebra, is well-posed with all necessary information provided, and is entirely objective. The task is to compare the theoretical variance of ordinary least squares (OLS) coefficient estimators for models using two different basis representations for cubic splines: a raw truncated power basis and its orthonormal counterpart derived from the Modified Gram-Schmidt procedure.\n\n### Theoretical Framework\n\nThe problem is situated within the context of OLS linear regression. The model is given by:\n$$\ny = X \\beta + \\varepsilon\n$$\nwhere $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the error vector. We assume the errors are independent and identically distributed with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe OLS estimator for $\\beta$ is found by minimizing the residual sum of squares and is given by:\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\nThe sampling variance-covariance matrix of this estimator is a cornerstone of statistical inference and is given by:\n$$\n\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}\n$$\nThe variance of the $j$-th coefficient estimator, $\\hat{\\beta}_j$, is the $j$-th diagonal element of this matrix:\n$$\n\\mathrm{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left[ (X^\\top X)^{-1} \\right]_{jj}\n$$\nThe problem requires computing this vector of variances, $\\sigma^2 \\cdot \\operatorname{diag}((X^\\top X)^{-1})$, for two different choices of the design matrix, $X$.\n\n### Step-by-Step Solution Procedure\n\n1.  **Input Data Generation**: For each test case with $n$ data points, we first generate a uniform grid of inputs $\\{x_i\\}_{i=1}^n$ on the interval $[0,1]$ using the formula $x_i = \\frac{i - 0.5}{n}$ for $i=1, \\dots, n$.\n\n2.  **Raw Basis Design Matrix ($X$) Construction**:\n    A cubic truncated power spline basis with $m$ internal knots $\\{k_1, \\dots, k_m\\}$ consists of $p = 4 + m$ basis functions:\n    -   $\\phi_0(x) = 1$\n    -   $\\phi_1(x) = x$\n    -   $\\phi_2(x) = x^2$\n    -   $\\phi_3(x) = x^3$\n    -   $\\phi_{3+j}(x) = (x - k_j)_+^3 = \\max(0, x - k_j)^3$ for $j=1, \\dots, m$.\n\n    The design matrix $X$ is an $n \\times p$ matrix where each entry is $X_{ij} = \\phi_{j-1}(x_i)$ (using $0$-based indexing for columns, $j=0, \\dots, p-1$). The columns of $X$ are often highly correlated, a condition known as multicollinearity. This makes the matrix $X^\\top X$ ill-conditioned, meaning its inverse is sensitive to small perturbations, which typically manifests as large variances for the coefficient estimators.\n\n3.  **Variance Calculation for Raw Basis**:\n    Following the theoretical formula, the vector of coefficient variances, $v_{\\text{raw}}$, is calculated as:\n    $$\n    v_{\\text{raw}} = \\sigma^2 \\cdot \\operatorname{diag}\\left((X^\\top X)^{-1}\\right)\n    $$\n    This involves computing the Gram matrix $X^\\top X$, its inverse, extracting the diagonal elements, and scaling by the given noise variance $\\sigma^2$.\n\n4.  **Orthonormal Basis Design Matrix ($Q$) Construction**:\n    To mitigate the effects of multicollinearity, the basis vectors (columns of $X$) can be orthogonalized. The problem specifies using the Modified Gram-Schmidt (MGS) algorithm. MGS is a numerically more stable procedure than its classical counterpart for producing an orthonormal matrix $Q$ from an input matrix $X$. The columns of $Q$ form an orthonormal basis for the column space of $X$.\n    The MGS algorithm proceeds as follows, starting with a matrix $V^{(0)} = X$:\n    For $j=1, \\dots, p$:\n    1.  Normalize the $j$-th column vector: $q_j = V^{(j-1)}_{\\cdot, j} / \\|V^{(j-1)}_{\\cdot, j}\\|_2$.\n    2.  For all subsequent columns $k = j+1, \\dots, p$, remove the component parallel to $q_j$: $V^{(j)}_{\\cdot, k} = V^{(j-1)}_{\\cdot, k} - (q_j^\\top V^{(j-1)}_{\\cdot, k}) q_j$.\n    The resulting matrix $Q$ has columns $q_1, \\dots, q_p$ which are orthonormal by construction, so $Q^\\top Q = I_p$.\n\n5.  **Variance Calculation for Orthonormal Basis**:\n    When we use the orthonormal basis $Q$, the linear model is reparameterized as $y = Q \\gamma + \\varepsilon$. The OLS estimator for the new coefficients $\\gamma$ is $\\hat{\\gamma} = (Q^\\top Q)^{-1} Q^\\top y$. The corresponding variance-covariance matrix is:\n    $$\n    \\mathrm{Var}(\\hat{\\gamma}) = \\sigma^2 (Q^\\top Q)^{-1}\n    $$\n    Since $Q$ is orthonormal, $Q^\\top Q \\approx I_p$ (the identity matrix), subject to floating-point precision. Therefore, its inverse is also approximately the identity matrix, $(Q^\\top Q)^{-1} \\approx I_p$. The vector of coefficient variances, $v_{\\text{orth}}$, becomes:\n    $$\n    v_{\\text{orth}} = \\sigma^2 \\cdot \\operatorname{diag}(I_p) = \\sigma^2 \\cdot [1, 1, \\dots, 1]^\\top = [\\sigma^2, \\sigma^2, \\dots, \\sigma^2]^\\top\n    $$\n    This demonstrates a key advantage of using an orthogonal basis: the coefficient estimators are uncorrelated and their variances are all equal to the noise variance $\\sigma^2$, irrespective of the original basis structure. This theoretical result provides a valuable check for the numerical implementation.\n\nThe final step is to round all calculated variance values to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing OLS coefficient variances for raw and\n    orthonormalized cubic spline bases across several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path\n        {'n': 200, 'knots': [0.3, 0.6], 'sigma_sq': 0.5},\n        # Case B: edge case with closely spaced knots a.k.a. high collinearity\n        {'n': 50, 'knots': [0.49, 0.5, 0.51], 'sigma_sq': 1.0},\n        # Case C: boundary case with no internal knots (polynomial basis)\n        {'n': 150, 'knots': [], 'sigma_sq': 0.2}\n    ]\n\n    results = []\n    \n    # Helper functions\n    def format_list(lst):\n        # Format a list of numbers into a comma-separated string without spaces\n        return f\"[{','.join(f'{x:.6f}' for x in lst)}]\"\n\n    def format_case_result(case_result):\n        # case_result is [v_raw_list, v_orth_list]\n        return f\"[{format_list(case_result[0])},{format_list(case_result[1])}]\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Performs Modified Gram-Schmidt orthogonalization on the columns of matrix A.\n        Returns an orthonormal matrix Q.\n        \"\"\"\n        # Work on a copy with float64 for numerical precision\n        V = A.copy().astype(np.float64)\n        num_cols = V.shape[1]\n        Q = np.zeros_like(V, dtype=np.float64)\n        \n        for j in range(num_cols):\n            # Normalize the j-th column of V to get the j-th column of Q\n            norm_vj = np.linalg.norm(V[:, j])\n            \n            # Handle potential linear dependency (column is ~zero)\n            if norm_vj < 1e-12:\n                Q[:, j] = 0.0\n            else:\n                Q[:, j] = V[:, j] / norm_vj\n            \n            # Orthogonalize all subsequent columns of V against the new Q column\n            for k in range(j + 1, num_cols):\n                V[:, k] -= np.dot(Q[:, j], V[:, k]) * Q[:, j]\n                \n        return Q\n\n    for case in test_cases:\n        n = case['n']\n        knots = case['knots']\n        sigma_sq = case['sigma_sq']\n\n        # 1. Create a grid of n inputs\n        x = (np.arange(1, n + 1) - 0.5) / n\n\n        # 2. Construct the raw cubic truncated power basis design matrix X\n        p = 4 + len(knots)\n        X = np.zeros((n, p))\n        \n        # Polynomial part\n        X[:, 0] = 1\n        X[:, 1] = x\n        X[:, 2] = x**2\n        X[:, 3] = x**3\n        \n        # Truncated power part\n        if knots:\n            for j, knot in enumerate(knots):\n                X[:, 4 + j] = np.maximum(x - knot, 0)**3\n        \n        # 3. Compute the theoretical OLS coefficient variance vector for the raw basis\n        # Use float64 for higher precision, especially for inversion\n        X_64 = X.astype(np.float64)\n        gram_X = X_64.T @ X_64\n        try:\n            inv_gram_X = np.linalg.inv(gram_X)\n            v_raw_diag = np.diag(inv_gram_X)\n            v_raw = sigma_sq * v_raw_diag\n        except np.linalg.LinAlgError:\n            # A singular matrix would imply perfect collinearity\n            v_raw = np.full(p, np.inf)\n\n        # 4. Orthogonalize the columns of X to obtain Q\n        Q = modified_gram_schmidt(X)\n\n        # 5. Compute the theoretical OLS coefficient variance vector for the orthonormal basis\n        gram_Q = Q.T @ Q\n        try:\n            inv_gram_Q = np.linalg.inv(gram_Q)\n            v_orth_diag = np.diag(inv_gram_Q)\n            v_orth = sigma_sq * v_orth_diag\n        except np.linalg.LinAlgError:\n            v_orth = np.full(p, np.inf)\n            \n        # 6. Round and store results\n        # The problem statement implies rounding is the final output step,\n        # but for clean storage and formatting, we do it here.\n        v_raw_rounded = np.round(v_raw, 6)\n        v_orth_rounded = np.round(v_orth, 6)\n        \n        results.append([v_raw_rounded.tolist(), v_orth_rounded.tolist()])\n\n    # Final print statement in the exact required format.\n    formatted_results = [format_case_result(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While many regression techniques focus on the conditional mean, splines can be paired with other objective functions to model the entire conditional distribution. This practice  explores this flexibility by combining cubic splines with quantile regression, which uses the \"pinball loss\" function to estimate conditional quantiles like the median or the $90^{th}$ percentile. You will implement this powerful technique and investigate the sensitivity of the resulting nonlinear quantile curves to the placement of knots.",
            "id": "3157118",
            "problem": "You are given the task of constructing and analyzing quantile regression splines within the framework of piecewise polynomials, knots, and splines in statistical learning. The fundamental base you should start from is the definition of quantiles via the pinball loss and the representation of regression splines via truncated power bases. Rely only on these foundational definitions and well-tested facts; do not assume any pre-derived shortcut formulas.\n\nConstruct a cubic regression spline basis using the truncated power representation. For a set of inputs $x \\in \\mathbb{R}$, an internal knot set $\\mathcal{K} = \\{k_{1},\\dots,k_{m}\\}$ with $k_{j} \\in \\mathbb{R}$, and degree $p = 3$, define the basis functions as the union of polynomial terms up to degree $3$ and the truncated cubic terms:\n$$\n\\{1, x, x^{2}, x^{3}\\} \\cup \\{(x - k_{j})_{+}^{3} \\,:\\, j = 1,\\dots,m\\},\n$$\nwhere $(t)_{+} = \\max\\{t, 0\\}$.\n\nLet the pinball loss for a quantile level $\\tau \\in (0,1)$ be defined for a residual $u \\in \\mathbb{R}$ by\n$$\n\\rho_{\\tau}(u) = u \\cdot \\left(\\tau - \\mathbb{I}\\{u < 0\\}\\right).\n$$\nQuantile regression at level $\\tau$ estimates coefficients $\\beta \\in \\mathbb{R}^{d}$ by minimizing the empirical risk $\\sum_{i=1}^{n} \\rho_{\\tau}(y_{i} - f_{\\beta}(x_{i}))$, where $f_{\\beta}(x)$ is the spline model induced by the above basis. You must formulate and solve this problem starting from the above definitions in a numerically exact way appropriate for convex, piecewise-linear objectives.\n\nData. Use the following fixed dataset. Let $n = 41$ and define $x_{i} = i/40$ for $i \\in \\{0,1,\\dots,40\\}$, so $x_{i} \\in [0,1]$. Define the deterministic response\n$$\ny_{i} = \\sin(2\\pi x_{i}) + 0.3 \\cdot \\sin(5\\pi x_{i}) + 0.1 \\cdot x_{i}.\n$$\nNo randomness is permitted.\n\nTasks. You will study the sensitivity of quantile spline estimates to the placement of knots by comparing fitted functions under different knot configurations. For each task below, fit two spline models with the same degree $p = 3$ and the specified quantile level $\\tau$, but with different internal knot sets. Then compute a sensitivity metric defined in each case.\n\n- Test One (median, grid maximum difference). Quantile level $\\tau = 0.5$. Use internal knots $\\mathcal{K}_{A} = \\{0.25, 0.5, 0.75\\}$ and $\\mathcal{K}_{B} = \\{0.2, 0.5, 0.8\\}$. Fit both models on the training data. Define a uniform evaluation grid of $201$ points on $[0,1]$, and compute\n$$\nS_{1} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- Test Two (upper quantile, grid maximum difference). Quantile level $\\tau = 0.9$. Use the same knots as in Test One, i.e., $\\mathcal{K}_{A} = \\{0.25, 0.5, 0.75\\}$ and $\\mathcal{K}_{B} = \\{0.2, 0.5, 0.8\\}$. Define the same uniform evaluation grid of $201$ points on $[0,1]$, and compute\n$$\nS_{2} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- Test Three (boundary condition, minimal versus one knot). Quantile level $\\tau = 0.5$. Compare a global cubic polynomial with no internal knots $\\mathcal{K}_{A} = \\varnothing$ to a spline with one internal knot $\\mathcal{K}_{B} = \\{0.5\\}$. Using the same $201$-point grid on $[0,1]$, compute\n$$\nS_{3} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- Test Four (localized sensitivity on training points, upper quantile). Quantile level $\\tau = 0.9$. Use $\\mathcal{K}_{A} = \\{0.33, 0.66\\}$ and $\\mathcal{K}_{B} = \\{0.34, 0.66\\}$. Compute the root-mean-square deviation of the two fits evaluated at the training inputs $\\{x_{i}\\}_{i=1}^{n}$:\n$$\nS_{4} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{f}_{A,\\tau}(x_{i}) - \\hat{f}_{B,\\tau}(x_{i}) \\right)^{2}}.\n$$\n\nProgram requirements.\n\n- Implement a function to construct the cubic regression spline basis using the truncated power representation for any given set of internal knots.\n\n- Implement a solver for quantile regression at a given $\\tau$ that is numerically exact for convex, piecewise-linear objectives as induced by the pinball loss. You may introduce auxiliary variables consistent with the definitions to transform the problem into a form solvable by standard convex optimization tools.\n\n- Use the dataset and test suite exactly as specified above. No randomness is allowed.\n\n- Numerical evaluation grid: use exactly $201$ equispaced points on $[0,1]$ including both endpoints.\n\n- Output specification: Your program should produce a single line of output containing the results $[S_{1}, S_{2}, S_{3}, S_{4}]$ as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, in this exact order and format, e.g., $[0.123456,0.234567,0.345678,0.456789]$.\n\nNotes.\n\n- There are no physical units in this problem.\n\n- All angles are in radians, and all trigonometric functions are standard.\n\n- Each of $S_{1}, S_{2}, S_{3}, S_{4}$ must be a single real number (a float).",
            "solution": "The problem is well-defined, scientifically sound, and internally consistent. All necessary data, functions, and evaluation criteria are provided. The task is to perform quantile regression using cubic splines, a standard technique in statistical learning. The methodology for solving the quantile regression optimization problem is hinted at by the description of the objective function as convex and piecewise-linear, for which Linear Programming ($LP$) is the exact and appropriate numerical method.\n\n### Principle-Based Design\n\n1.  **Model Specification**: The model is a cubic spline function $f_{\\beta}(x)$ defined using a truncated power basis. For a given set of input points $x$, degree $p=3$, and a set of $m$ internal knots $\\mathcal{K} = \\{k_1, \\dots, k_m\\}$, the function is a linear combination of basis functions:\n    $$\n    f_{\\beta}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^{m} \\beta_{3+j} (x - k_j)_+^3\n    $$\n    This can be written compactly as $f_{\\beta}(x) = H_x \\beta$, where $H_x$ is the row vector of basis functions evaluated at $x$, and $\\beta$ is the vector of coefficients. The total number of coefficients is $d = p+1+m = 4+m$.\n\n2.  **Quantile Regression Objective**: The goal is to find the coefficient vector $\\beta$ that minimizes the empirical risk, defined as the sum of pinball losses over the $n$ data points:\n    $$\n    \\min_{\\beta \\in \\mathbb{R}^d} \\sum_{i=1}^{n} \\rho_{\\tau}(y_i - f_{\\beta}(x_i))\n    $$\n    where the pinball loss for a residual $u_i = y_i - f_{\\beta}(x_i)$ at quantile level $\\tau \\in (0,1)$ is given by $\\rho_{\\tau}(u_i) = u_i(\\tau - \\mathbb{I}\\{u_i < 0\\})$.\n\n3.  **Linear Programming Formulation**: The pinball loss is a convex, piecewise-linear function. The objective, being a sum of such functions, is also convex. This allows the optimization problem to be transformed into a standard Linear Program ($LP$). We introduce non-negative auxiliary variables $u_i^+$ and $u_i^-$ for each data point $i \\in \\{1, \\dots, n\\}$. We decompose the residual $u_i$ as $u_i = u_i^+ - u_i^-$, where $u_i^+ = \\max(u_i, 0)$ and $u_i^- = \\max(-u_i, 0)$. The pinball loss can then be re-expressed as a linear function of these new variables: $\\rho_{\\tau}(u_i) = \\tau u_i^+ + (1-\\tau)u_i^-$.\n\n    The optimization problem now becomes:\n    $$\n    \\min_{\\beta, u^+, u^-} \\sum_{i=1}^{n} \\left( \\tau u_i^+ + (1-\\tau)u_i^- \\right)\n    $$\n    subject to the constraints for $i=1, \\dots, n$:\n    $$\n    f_{\\beta}(x_i) + u_i^+ - u_i^- = y_i\n    $$\n    $$\n    u_i^+ \\ge 0, \\quad u_i^- \\ge 0\n    $$\n    The coefficients $\\beta_j$ are unconstrained in sign. Standard LP solvers require all variables to be non-negative. We therefore decompose each coefficient $\\beta_j$ into its positive and negative parts: $\\beta_j = \\beta_j^+ - \\beta_j^-$, with $\\beta_j^+ \\ge 0$ and $\\beta_j^- \\ge 0$.\n\n    Let $H$ be the $n \\times d$ design matrix where $H_{ij}$ is the $j$-th basis function evaluated at $x_i$. The final LP formulation is:\n    $$\n    \\min_{\\beta^+, \\beta^-, u^+, u^-} \\left( \\mathbf{0}^T\\beta^+ + \\mathbf{0}^T\\beta^- + \\tau \\mathbf{1}^T u^+ + (1-\\tau)\\mathbf{1}^T u^- \\right)\n    $$\n    subject to:\n    $$\n    H\\beta^+ - H\\beta^- + I u^+ - I u^- = \\mathbf{y}\n    $$\n    $$\n    \\beta^+ \\ge \\mathbf{0}, \\quad \\beta^- \\ge \\mathbf{0}, \\quad u^+ \\ge \\mathbf{0}, \\quad u^- \\ge \\mathbf{0}\n    $$\n    This is in the canonical form $\\min \\mathbf{c}^T\\mathbf{z}$ subject to $\\mathbf{A}_{eq}\\mathbf{z} = \\mathbf{b}_{eq}$ and $\\mathbf{z} \\ge \\mathbf{0}$, which can be solved efficiently.\n\n4.  **Implementation and Evaluation**:\n    *   A function is implemented to construct the design matrix $H$ for any given set of points $x$ and knots $\\mathcal{K}$, based on the cubic truncated power basis.\n    *   A solver function takes the training data $(x_i, y_i)$, knots $\\mathcal{K}$, and quantile level $\\tau$, assembles the components of the LP described above ($\\mathbf{c}, \\mathbf{A}_{eq}, \\mathbf{b}_{eq}$), and uses `scipy.optimize.linprog` to find the optimal solution vector $\\mathbf{z}$. From $\\mathbf{z}$, the coefficient vectors $\\beta^+$ and $\\beta^-$ are extracted, and the final spline coefficient vector is recovered as $\\beta = \\beta^+ - \\beta^-$.\n    *   For each of the four specified tests, two spline models ($\\hat{f}_{A,\\tau}$ and $\\hat{f}_{B,\\tau}$) are fitted using their respective knot sets, $\\mathcal{K}_A$ and $\\mathcal{K}_B$.\n    *   The fitted models are then used to predict values on the specified evaluation points (a fine grid of $201$ points for Tests $1-3$, and the training points for Test $4$).\n    *   The required sensitivity metrics ($S_1, S_2, S_3, S_4$) are calculated based on these predictions, measuring the discrepancy between the two fitted functions in each test. The results are collected and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes quantile regression splines by solving a series of\n    linear programming problems.\n    \"\"\"\n\n    def create_design_matrix(x, knots, p=3):\n        \"\"\"\n        Constructs the design matrix for a polynomial spline with a truncated power basis.\n        \n        Args:\n            x (np.ndarray): Input data points.\n            knots (list or np.ndarray): Internal knot locations.\n            p (int): Degree of the polynomial spline.\n\n        Returns:\n            np.ndarray: The design matrix H.\n        \"\"\"\n        x_flat = np.asarray(x).flatten()\n        \n        # Polynomial basis terms: 1, x, x^2, x^3\n        H_poly = np.vander(x_flat, N=p + 1, increasing=True)\n\n        if not knots:\n            return H_poly\n\n        knots_arr = np.asarray(knots)\n        # Truncated power basis terms: (x - k)_+^p\n        # The shape is (len(x), len(knots))\n        H_trunc = np.maximum(0, x_flat[:, np.newaxis] - knots_arr[np.newaxis, :])**p\n        \n        return np.hstack([H_poly, H_trunc])\n\n    def fit_quantile_spline(x_train, y_train, knots, tau):\n        \"\"\"\n        Fits a quantile regression spline by formulating and solving a linear program.\n\n        Args:\n            x_train (np.ndarray): Training input data.\n            y_train (np.ndarray): Training response data.\n            knots (list): Internal knot locations.\n            tau (float): The quantile level, in (0, 1).\n\n        Returns:\n            np.ndarray: The estimated spline coefficients beta.\n        \"\"\"\n        n = len(x_train)\n        H = create_design_matrix(x_train, knots, p=3)\n        d = H.shape[1] # Number of basis functions\n\n        # Formulate the Linear Program for quantile regression\n        # Variables z = [beta+, beta-, u+, u-]\n        # size = d + d + n + n = 2d + 2n\n        \n        # Objective vector c\n        c = np.concatenate([np.zeros(2 * d),\n                            tau * np.ones(n),\n                            (1 - tau) * np.ones(n)])\n        \n        # Equality constraints matrix A_eq\n        # H*beta+ - H*beta- + I*u+ - I*u- = y\n        A_eq = np.hstack([H, -H, np.eye(n), -np.eye(n)])\n        \n        # Equality constraints vector b_eq\n        b_eq = y_train\n\n        # All variables are non-negative, so bounds are (0, None). This is the default.\n        res = linprog(c, A_eq=A_eq, b_eq=b_eq, method='highs')\n\n        if not res.success:\n            raise RuntimeError(f\"LP solver failed for knots={knots}, tau={tau}: {res.message}\")\n\n        solution = res.x\n        beta_plus = solution[:d]\n        beta_minus = solution[d:2*d]\n        beta = beta_plus - beta_minus\n        \n        return beta\n\n    def predict(x_eval, knots, beta):\n        \"\"\"\n        Predicts response values using a fitted spline model.\n\n        Args:\n            x_eval (np.ndarray): Points to evaluate the model at.\n            knots (list): The knots used to fit the model.\n            beta (np.ndarray): The fitted spline coefficients.\n\n        Returns:\n            np.ndarray: The predicted values.\n        \"\"\"\n        H_eval = create_design_matrix(x_eval, knots, p=3)\n        return H_eval @ beta\n\n    # Define the fixed dataset\n    n = 41\n    x_train = np.linspace(0, 1, n)\n    y_train = np.sin(2 * np.pi * x_train) + 0.3 * np.sin(5 * np.pi * x_train) + 0.1 * x_train\n\n    # Define the evaluation grid\n    x_grid = np.linspace(0, 1, 201)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Test One\n        {'tau': 0.5, 'knots_A': [0.25, 0.5, 0.75], 'knots_B': [0.2, 0.5, 0.8], 'metric': 'max_abs_diff_grid'},\n        # Test Two\n        {'tau': 0.9, 'knots_A': [0.25, 0.5, 0.75], 'knots_B': [0.2, 0.5, 0.8], 'metric': 'max_abs_diff_grid'},\n        # Test Three\n        {'tau': 0.5, 'knots_A': [], 'knots_B': [0.5], 'metric': 'max_abs_diff_grid'},\n        # Test Four\n        {'tau': 0.9, 'knots_A': [0.33, 0.66], 'knots_B': [0.34, 0.66], 'metric': 'rmsd_train'}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        tau = case['tau']\n        knots_A = case['knots_A']\n        knots_B = case['knots_B']\n        \n        # Fit model A\n        beta_A = fit_quantile_spline(x_train, y_train, knots_A, tau)\n        \n        # Fit model B\n        beta_B = fit_quantile_spline(x_train, y_train, knots_B, tau)\n\n        if case['metric'] == 'max_abs_diff_grid':\n            pred_A = predict(x_grid, knots_A, beta_A)\n            pred_B = predict(x_grid, knots_B, beta_B)\n            metric_val = np.max(np.abs(pred_A - pred_B))\n        elif case['metric'] == 'rmsd_train':\n            pred_A = predict(x_train, knots_A, beta_A)\n            pred_B = predict(x_train, knots_B, beta_B)\n            metric_val = np.sqrt(np.mean((pred_A - pred_B)**2))\n        else:\n            raise ValueError(f\"Unknown metric: {case['metric']}\")\n            \n        results.append(metric_val)\n\n    # Format and print the final output\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Splines are not just for flexible curve fitting; they are powerful tools for formal statistical inference. A common scientific question is whether a relationship changes abruptly at a certain threshold. This practice  demonstrates how to use a simple piecewise-linear spline to model and test for such a changepoint. You will perform a statistical power analysis to quantify the ability of a $t$-test on the spline coefficient to correctly detect a true change in slope.",
            "id": "3157150",
            "problem": "Consider the following targeted-knot piecewise-linear spline regression model intended to detect a change in slope at a suspected biological threshold. For a set of fixed design points $x_1, x_2, \\dots, x_n$ in $[0,1]$, define the hinge function $(u)_+ = \\max(u, 0)$ and the working spline basis at a chosen knot location $\\tau_{\\text{knot}}$ by the columns $1$, $x$, and $(x - \\tau_{\\text{knot}})_+$. The regression model is\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i,\\quad i=1,\\dots,n,\n$$\nwhere $\\varepsilon_i$ are independent and identically distributed as a Gaussian (normal) random variable with mean $0$ and variance $\\sigma^2$, written $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nSuppose the true mean function exhibits a slope change of magnitude $\\gamma$ at the biological threshold $\\tau_{\\text{true}}$, given by\n$$\nf_{\\text{true}}(x) = \\gamma \\,(x - \\tau_{\\text{true}})_+.\n$$\nWe wish to quantify the statistical power of a two-sided hypothesis test at significance level $\\alpha$ for\n$$\nH_0:\\ \\beta_2 = 0 \\quad \\text{versus} \\quad H_1:\\ \\beta_2 \\neq 0,\n$$\nwhich operationalizes detection of a steep slope change at the threshold using a targeted knot placement $\\tau_{\\text{knot}}$ near the suspected threshold $\\tau_{\\text{true}}$.\n\nStarting only from fundamental definitions of Ordinary Least Squares (OLS), properties of Gaussian errors, and well-tested distributional facts for linear regression, derive how to compute the statistical power $P(\\text{reject } H_0 \\mid \\gamma,\\sigma,\\tau_{\\text{true}},\\tau_{\\text{knot}},n)$ for the two-sided $t$-test on $\\beta_2$. Your derivation must begin from:\n- The OLS estimator definition $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$, where $X$ is the $n \\times p$ design matrix formed by the columns $1$, $x$, and $(x - \\tau_{\\text{knot}})_+$ (thus $p=3$), and $\\boldsymbol{y}$ is the response vector.\n- The normal error model $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$.\n- The well-tested fact that, with unknown variance estimated from residuals, a $t$-statistic for a single linear regression coefficient follows a Noncentral $t$ distribution (NCT) under alternatives, with Degrees of Freedom (DoF) $n-p$ and a noncentrality parameter that depends on the true mean projected onto the working design.\n\nThen, implement the derived computation in a complete, runnable program that:\n- Constructs the fixed design points $x_i$ as $n$ evenly spaced points on $[0,1]$.\n- Forms the working design matrix $X$ at the chosen $\\tau_{\\text{knot}}$.\n- Projects $f_{\\text{true}}(x)$ onto the columns of $X$ to find the pseudo-true coefficient vector $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}$, and uses its third component $\\beta_2^\\star$ to parameterize the two-sided $t$-test for the hinge coefficient.\n- Uses the Noncentral $t$ distribution to compute the exact two-sided power at level $\\alpha$ for testing $H_0:\\beta_2=0$, with the Degrees of Freedom $df=n-p$ and noncentrality parameter $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{(X^\\top X)^{-1}_{3,3}})$.\n\nYour program must evaluate the power for the following test suite, which covers a general case, misspecified knot placement, the no-signal edge case, boundary-threshold placement, high-noise with a steep effect, and an offset-knot with few points beyond the threshold:\n- Case $1$: $n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=2.0$, $\\alpha=0.05$.\n- Case $2$: $n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.55$, $\\gamma=2.0$, $\\alpha=0.05$.\n- Case $3$: $n=30$, $\\sigma=2.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=0.0$, $\\alpha=0.05$.\n- Case $4$: $n=120$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.10$, $\\tau_{\\text{knot}}=0.10$, $\\gamma=1.5$, $\\alpha=0.05$.\n- Case $5$: $n=120$, $\\sigma=3.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=5.0$, $\\alpha=0.05$.\n- Case $6$: $n=60$, $\\sigma=0.5$, $\\tau_{\\text{true}}=0.80$, $\\tau_{\\text{knot}}=0.75$, $\\gamma=1.0$, $\\alpha=0.05$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases given above, for example, $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$. Each $\\text{result}_k$ must be a floating-point number in $[0,1]$ equal to the computed statistical power for case $k$.",
            "solution": "The task is to derive and compute the statistical power for a hypothesis test on a coefficient in a piecewise-linear spline regression model. The power is the probability of correctly rejecting the null hypothesis when a specific alternative is true. We will follow a principled derivation starting from the fundamentals of Ordinary Least Squares (OLS) regression and the distributional theory for its estimators.\n\n**1. Model Specification and Problem Setup**\n\nThe regression model under consideration is:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i, \\quad i=1,\\dots,n\n$$\nwhere $(u)_+ = \\max(u, 0)$ is the hinge function, $\\tau_{\\text{knot}}$ is a fixed knot, and the errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.) normal random variables, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nIn matrix notation, the model is $\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where:\n- $\\boldsymbol{y} = [y_1, \\dots, y_n]^\\top$ is the response vector.\n- $X$ is the $n \\times 3$ design matrix with columns corresponding to the basis functions:\n$$\nX = \\begin{bmatrix}\n1 & x_1 & (x_1 - \\tau_{\\text{knot}})_+ \\\\\n1 & x_2 & (x_2 - \\tau_{\\text{knot}})_+ \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_n & (x_n - \\tau_{\\text{knot}})_+\n\\end{bmatrix}\n$$\n- $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$ is the coefficient vector.\n- $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_n]^\\top$ is the error vector, with distribution $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe true data-generating process is assumed to have a mean function given by:\n$$\nf_{\\text{true}}(x) = \\gamma (x - \\tau_{\\text{true}})_+\n$$\nThis implies that the observed data $\\boldsymbol{y}$ are realizations from the model $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{f_{\\text{true}}}$ is the vector with elements $f_{\\text{true}}(x_i)$.\n\nWe are testing the null hypothesis $H_0: \\beta_2 = 0$ against the alternative $H_1: \\beta_2 \\neq 0$ at a significance level $\\alpha$.\n\n**2. Distribution of the OLS Estimator**\n\nThe OLS estimator for $\\boldsymbol{\\beta}$ is defined as $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$. To analyze its properties under the true data-generating process, we substitute $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$:\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top (\\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}) = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}\n$$\nThe expected value of $\\hat{\\boldsymbol{\\beta}}$ is:\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = E[(X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}] + E[(X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon}]\n$$\nSince $E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$, the expectation simplifies to:\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} \\equiv \\boldsymbol{\\beta}^\\star\n$$\nThis vector $\\boldsymbol{\\beta}^\\star$ is the \"pseudo-true\" parameter vector. It represents the best approximation of the true mean function $\\boldsymbol{f_{\\text{true}}}$ within the vector space spanned by the columns of the design matrix $X$. In general, $\\boldsymbol{\\beta}^\\star$ is not equal to $(0, 0, \\gamma)^\\top$ unless the basis functions and knots are perfectly aligned with the true function and have no intercept or linear term.\n\nThe covariance matrix of $\\hat{\\boldsymbol{\\beta}}$ is:\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = E\\left[ (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}]) (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}])^\\top \\right] = E\\left[ \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right) \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right)^\\top \\right]\n$$\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon}^\\top] X (X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top (\\sigma^2 I_n) X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1}\n$$\nSince $\\hat{\\boldsymbol{\\beta}}$ is a linear transformation of the Gaussian vector $\\boldsymbol{\\varepsilon}$, it is also normally distributed:\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}^\\star, \\sigma^2 (X^\\top X)^{-1})\n$$\n\n**3. The t-statistic and its Distribution**\n\nWe are interested in the coefficient $\\beta_2$. The estimator $\\hat{\\beta}_2$ is the third element of $\\hat{\\boldsymbol{\\beta}}$. Its distribution is univariate normal:\n$$\n\\hat{\\beta}_2 \\sim \\mathcal{N}(\\beta_2^\\star, \\sigma^2 C_{33})\n$$\nwhere $\\beta_2^\\star$ is the third element of $\\boldsymbol{\\beta}^\\star$ and $C_{33} = [(X^\\top X)^{-1}]_{3,3}$ is the third diagonal element of the inverse of the Gram matrix. The standard error of $\\hat{\\beta}_2$ is $\\text{se}(\\hat{\\beta}_2) = \\sigma \\sqrt{C_{33}}$.\n\nIn practice, $\\sigma^2$ is unknown and must be estimated from the data. The unbiased estimator is $\\hat{\\sigma}^2 = \\frac{1}{n-p} \\text{RSS}$, where $p=3$ is the number of parameters and $\\text{RSS} = \\|\\boldsymbol{y} - X\\hat{\\boldsymbol{\\beta}}\\|^2$ is the residual sum of squares. The test statistic replaces the true standard error with the estimated standard error, $\\hat{\\text{se}}(\\hat{\\beta}_2) = \\hat{\\sigma}\\sqrt{C_{33}}$:\n$$\nT = \\frac{\\hat{\\beta}_2 - 0}{\\hat{\\text{se}}(\\hat{\\beta}_2)} = \\frac{\\hat{\\beta}_2}{\\hat{\\sigma}\\sqrt{C_{33}}}\n$$\nUnder the general alternative hypothesis where $E[\\hat{\\beta}_2] = \\beta_2^\\star \\neq 0$, this statistic follows a Noncentral $t$-distribution. The degrees of freedom ($df$) are those associated with the variance estimate, $df = n-p = n-3$. The noncentrality parameter (NCP), $\\delta$, is the standardized mean of the numerator:\n$$\n\\delta = \\frac{E[\\hat{\\beta}_2]}{\\text{se}(\\hat{\\beta}_2)} = \\frac{\\beta_2^\\star}{\\sigma\\sqrt{C_{33}}}\n$$\nThus, under the alternative specified by $\\boldsymbol{f_{\\text{true}}}$, the test statistic has the distribution $T \\sim t(df, \\delta)$, a noncentral $t$-distribution with $df=n-3$ degrees of freedom and NCP $\\delta$.\n\n**4. Power Calculation**\n\nFor a two-sided test at significance level $\\alpha$, we reject the null hypothesis $H_0: \\beta_2=0$ if the observed value of the test statistic $|T|$ exceeds a critical value. This critical value, which we denote $t_{\\text{crit}}$, is determined from the null distribution. Under $H_0$, $\\gamma=0$, which implies $\\boldsymbol{f_{\\text{true}}} = \\boldsymbol{0}$, $\\boldsymbol{\\beta}^\\star = \\boldsymbol{0}$, and $\\delta = 0$. The null distribution is therefore a central $t$-distribution, $T \\sim t(df)$. The critical value $t_{\\text{crit}}$ is the upper $(1-\\alpha/2)$ quantile of this central $t$-distribution:\n$$\nt_{\\text{crit}} = t_{df, 1-\\alpha/2}\n$$\nThe statistical power is the probability of rejecting $H_0$ when the alternative is true, i.e., when $T \\sim t(df, \\delta)$.\n$$\n\\text{Power} = P(|T| > t_{\\text{crit}} \\mid T \\sim t(df, \\delta)) = P(T > t_{\\text{crit}}) + P(T < -t_{\\text{crit}})\n$$\nLet $F_{t(df, \\delta)}$ be the cumulative distribution function (CDF) of the noncentral $t$-distribution. The power is then calculated as:\n$$\n\\text{Power} = \\left( 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) \\right) + F_{t(df, \\delta)}(-t_{\\text{crit}})\n$$\n\n**5. Computational Steps**\n\nThe procedure to compute the power for each case is as follows:\n1.  Given $n, \\sigma, \\tau_{\\text{true}}, \\tau_{\\text{knot}}, \\gamma, \\alpha$.\n2.  Define the design points $x_i$ as $n$ equally spaced points from $0$ to $1$, inclusive.\n3.  Construct the $n \\times 3$ design matrix $X$ with columns $1$, $x_i$, and $(x_i - \\tau_{\\text{knot}})_+$.\n4.  Construct the $n \\times 1$ true mean vector $\\boldsymbol{f}_{\\text{true}}$ with elements $\\gamma (x_i - \\tau_{\\text{true}})_+$.\n5.  Compute the matrix product $X^\\top X$ and its inverse, $(X^\\top X)^{-1}$.\n6.  Compute the pseudo-true parameter vector $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f}_{\\text{true}}$.\n7.  Extract the third component, $\\beta_2^\\star = \\boldsymbol{\\beta}^\\star_2$.\n8.  Extract the third diagonal element of the inverse matrix, $C_{33} = [(X^\\top X)^{-1}]_{2,2}$ (using $0$-based indexing for the third row/column).\n9.  Calculate the noncentrality parameter $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{C_{33}})$.\n10. Set the degrees of freedom $df = n - 3$.\n11. Find the critical value $t_{\\text{crit}}$ from the inverse CDF (or percent-point function) of the central $t$-distribution: $t_{\\text{crit}} = F^{-1}_{t(df)}(1 - \\alpha/2)$.\n12. Compute the power using the CDF of the noncentral $t$-distribution with $df$ and $\\delta$: $\\text{Power} = 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) + F_{t(df, \\delta)}(-t_{\\text{crit}})$.\n\nThis completes the derivation and specification of the algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, nct\n\ndef solve():\n    \"\"\"\n    Computes the statistical power for a hypothesis test on a coefficient in a\n    piecewise-linear spline regression model for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.50, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 2: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.55, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.55, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 3: n=30, sigma=2.0, tau_true=0.50, tau_knot=0.50, gamma=0.0, alpha=0.05\n        {'n': 30, 'sigma': 2.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 0.0, 'alpha': 0.05},\n        # Case 4: n=120, sigma=1.0, tau_true=0.10, tau_knot=0.10, gamma=1.5, alpha=0.05\n        {'n': 120, 'sigma': 1.0, 'tau_true': 0.10, 'tau_knot': 0.10, 'gamma': 1.5, 'alpha': 0.05},\n        # Case 5: n=120, sigma=3.0, tau_true=0.50, tau_knot=0.50, gamma=5.0, alpha=0.05\n        {'n': 120, 'sigma': 3.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 5.0, 'alpha': 0.05},\n        # Case 6: n=60, sigma=0.5, tau_true=0.80, tau_knot=0.75, gamma=1.0, alpha=0.05\n        {'n': 60, 'sigma': 0.5, 'tau_true': 0.80, 'tau_knot': 0.75, 'gamma': 1.0, 'alpha': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        # Extract parameters for the current case\n        n = params['n']\n        sigma = params['sigma']\n        tau_true = params['tau_true']\n        tau_knot = params['tau_knot']\n        gamma = params['gamma']\n        alpha = params['alpha']\n        \n        # 1. Construct the fixed design points x_i as n evenly spaced points on [0,1]\n        x = np.linspace(0.0, 1.0, n)\n        \n        # 2. Form the working design matrix X\n        # Columns are 1, x, and (x - tau_knot)_+\n        p = 3\n        X = np.zeros((n, p))\n        X[:, 0] = 1.0\n        X[:, 1] = x\n        X[:, 2] = np.maximum(0, x - tau_knot)\n\n        # 3. Construct the true mean function vector f_true\n        f_true = gamma * np.maximum(0, x - tau_true)\n        \n        # 4. Compute the pseudo-true coefficient vector beta_star\n        # beta_star = (X^T X)^-1 X^T f_true\n        try:\n            XTX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # This case occurs if X has linearly dependent columns.\n            # For the given knots, X should have full rank.\n            # E.g., if tau_knot >= 1, the third column is all zeros.\n            results.append(np.nan) # Mark as not-a-number if problem occurs\n            continue\n\n        beta_star = XTX_inv @ X.T @ f_true\n        \n        # 5. Extract the third component, beta_2_star\n        beta_2_star = beta_star[2]\n        \n        # 6. Extract the variance term for beta_2_hat from (X^T X)^-1\n        C_33 = XTX_inv[2, 2]\n        \n        # 7. Calculate the noncentrality parameter (NCP), delta\n        delta = beta_2_star / (sigma * np.sqrt(C_33))\n        \n        # 8. Define degrees of freedom for the t-test\n        df = n - p\n        \n        # 9. Find the critical value for the two-sided test\n        t_crit = t.ppf(1 - alpha / 2, df)\n        \n        # 10. Compute the power using the Noncentral t (NCT) distribution\n        # Power = P(T > t_crit) + P(T  -t_crit) where T ~ NCT(df, delta)\n        power = 1.0 - nct.cdf(t_crit, df, delta) + nct.cdf(-t_crit, df, delta)\n        \n        results.append(power)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}