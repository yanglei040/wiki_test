{
    "hands_on_practices": [
        {
            "introduction": "When constructing splines using a simple truncated power basis, a practical issue often arises: the basis functions are highly correlated. This multicollinearity can inflate the variance of our coefficient estimates, making them unstable and difficult to interpret. This practice provides a hands-on look at this problem by comparing the coefficient variances from a raw truncated power basis to those from an orthonormal basis created using the Gram-Schmidt procedure . By doing so, you will gain a deeper appreciation for the numerical stability of different basis choices and why alternatives like B-splines are often preferred in practice.",
            "id": "3157128",
            "problem": "You are given the task of analyzing coefficient variance in ordinary least squares (OLS) linear regression models that use cubic spline bases. The foundational setup is the linear model $y = X \\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $X$ is a design matrix whose columns are spline basis functions evaluated at $n$ inputs. The well-tested formula for the sampling variance of the OLS estimator is $\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$, under full column rank and homoscedasticity. You will construct a raw cubic spline truncated power basis, orthogonalize it via the Gram–Schmidt procedure to reduce collinearity, and compare the theoretical coefficient variances for the raw basis versus the orthonormalized basis.\n\nDefinitions and construction:\n- A cubic truncated power spline basis with internal knots $\\{k_1,\\dots,k_m\\}$ on $[0,1]$ consists of the functions $\\phi_0(x)=1$, $\\phi_1(x)=x$, $\\phi_2(x)=x^2$, $\\phi_3(x)=x^3$, and for each internal knot $k_j$, a function $\\phi_{3+j}(x) = \\left( x - k_j \\right)_+^3$, where $(u)_+ = \\max(u, 0)$.\n- For inputs $\\{x_i\\}_{i=1}^n$, the design matrix $X \\in \\mathbb{R}^{n \\times p}$ has entries $X_{i,j} = \\phi_j(x_i)$ for $j=0,\\dots,3+m$, so that $p = 4 + m$.\n- Gram–Schmidt orthogonalization constructs a matrix $Q \\in \\mathbb{R}^{n \\times p}$ whose columns are orthonormal vectors spanning the same column space as $X$, i.e., $Q^\\top Q = I_p$ and $\\operatorname{span}\\{Q_{\\cdot,1},\\dots,Q_{\\cdot,p}\\} = \\operatorname{span}\\{X_{\\cdot,1},\\dots,X_{\\cdot,p}\\}$.\n\nTasks to implement:\n1. For each test case, create a grid of $n$ inputs $x_i = \\frac{i - 0.5}{n}$ for $i = 1,\\dots,n$ on $[0,1]$.\n2. Construct the raw cubic truncated power basis design matrix $X$ using the specified internal knots for the case.\n3. Compute the theoretical OLS coefficient variance vector for the raw basis, defined as $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (X^\\top X)^{-1} \\right)$, where $\\operatorname{diag}(A)$ denotes the vector of diagonal entries of matrix $A$.\n4. Orthogonalize the columns of $X$ using the classical modified Gram–Schmidt procedure to obtain $Q$ with $Q^\\top Q \\approx I_p$.\n5. Compute the theoretical OLS coefficient variance vector for the orthonormal basis, defined as $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (Q^\\top Q)^{-1} \\right)$.\n6. Round each variance value to six decimal places.\n\nTest suite:\n- Case A (happy path): $n = 200$, internal knots $\\{0.3, 0.6\\}$, noise variance $\\sigma^2 = 0.5$.\n- Case B (edge case with closely spaced knots increasing collinearity): $n = 50$, internal knots $\\{0.49, 0.5, 0.51\\}$, noise variance $\\sigma^2 = 1.0$.\n- Case C (boundary case with no internal knots; pure cubic polynomial basis): $n = 150$, internal knots $\\{\\}$, noise variance $\\sigma^2 = 0.2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a pair of lists $[v_{\\text{raw}}, v_{\\text{orth}}]$, where $v_{\\text{raw}}$ is the list of raw-basis coefficient variances and $v_{\\text{orth}}$ is the list of orthonormal-basis coefficient variances, both rounded to six decimal places. For example, an output with two cases might look like $[[[0.1,0.2],[0.1,0.1]],[[0.3,0.4],[0.3,0.3]]]$. No additional text should be printed.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of statistical linear models and numerical linear algebra, is well-posed with all necessary information provided, and is entirely objective. The task is to compare the theoretical variance of ordinary least squares (OLS) coefficient estimators for models using two different basis representations for cubic splines: a raw truncated power basis and its orthonormal counterpart derived from the Modified Gram-Schmidt procedure.\n\n### Theoretical Framework\n\nThe problem is situated within the context of OLS linear regression. The model is given by:\n$$\ny = X \\beta + \\varepsilon\n$$\nwhere $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the error vector. We assume the errors are independent and identically distributed with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe OLS estimator for $\\beta$ is found by minimizing the residual sum of squares and is given by:\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\nThe sampling variance-covariance matrix of this estimator is a cornerstone of statistical inference and is given by:\n$$\n\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}\n$$\nThe variance of the $j$-th coefficient estimator, $\\hat{\\beta}_j$, is the $j$-th diagonal element of this matrix:\n$$\n\\mathrm{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left[ (X^\\top X)^{-1} \\right]_{jj}\n$$\nThe problem requires computing this vector of variances, $\\sigma^2 \\cdot \\operatorname{diag}((X^\\top X)^{-1})$, for two different choices of the design matrix, $X$.\n\n### Step-by-Step Solution Procedure\n\n1.  **Input Data Generation**: For each test case with $n$ data points, we first generate a uniform grid of inputs $\\{x_i\\}_{i=1}^n$ on the interval $[0,1]$ using the formula $x_i = \\frac{i - 0.5}{n}$ for $i=1, \\dots, n$.\n\n2.  **Raw Basis Design Matrix ($X$) Construction**:\n    A cubic truncated power spline basis with $m$ internal knots $\\{k_1, \\dots, k_m\\}$ consists of $p = 4 + m$ basis functions:\n    -   $\\phi_0(x) = 1$\n    -   $\\phi_1(x) = x$\n    -   $\\phi_2(x) = x^2$\n    -   $\\phi_3(x) = x^3$\n    -   $\\phi_{3+j}(x) = (x - k_j)_+^3 = \\max(0, x - k_j)^3$ for $j=1, \\dots, m$.\n\n    The design matrix $X$ is an $n \\times p$ matrix where each entry is $X_{ij} = \\phi_{j-1}(x_i)$ (using $0$-based indexing for columns, $j=0, \\dots, p-1$). The columns of $X$ are often highly correlated, a condition known as multicollinearity. This makes the matrix $X^\\top X$ ill-conditioned, meaning its inverse is sensitive to small perturbations, which typically manifests as large variances for the coefficient estimators.\n\n3.  **Variance Calculation for Raw Basis**:\n    Following the theoretical formula, the vector of coefficient variances, $v_{\\text{raw}}$, is calculated as:\n    $$\n    v_{\\text{raw}} = \\sigma^2 \\cdot \\operatorname{diag}\\left((X^\\top X)^{-1}\\right)\n    $$\n    This involves computing the Gram matrix $X^\\top X$, its inverse, extracting the diagonal elements, and scaling by the given noise variance $\\sigma^2$.\n\n4.  **Orthonormal Basis Design Matrix ($Q$) Construction**:\n    To mitigate the effects of multicollinearity, the basis vectors (columns of $X$) can be orthogonalized. The problem specifies using the Modified Gram-Schmidt (MGS) algorithm. MGS is a numerically more stable procedure than its classical counterpart for producing an orthonormal matrix $Q$ from an input matrix $X$. The columns of $Q$ form an orthonormal basis for the column space of $X$.\n    The MGS algorithm proceeds as follows, starting with a matrix $V^{(0)} = X$:\n    For $j=1, \\dots, p$:\n    1.  Normalize the $j$-th column vector: $q_j = V^{(j-1)}_{\\cdot, j} / \\|V^{(j-1)}_{\\cdot, j}\\|_2$.\n    2.  For all subsequent columns $k = j+1, \\dots, p$, remove the component parallel to $q_j$: $V^{(j)}_{\\cdot, k} = V^{(j-1)}_{\\cdot, k} - (q_j^\\top V^{(j-1)}_{\\cdot, k}) q_j$.\n    The resulting matrix $Q$ has columns $q_1, \\dots, q_p$ which are orthonormal by construction, so $Q^\\top Q = I_p$.\n\n5.  **Variance Calculation for Orthonormal Basis**:\n    When we use the orthonormal basis $Q$, the linear model is reparameterized as $y = Q \\gamma + \\varepsilon$. The OLS estimator for the new coefficients $\\gamma$ is $\\hat{\\gamma} = (Q^\\top Q)^{-1} Q^\\top y$. The corresponding variance-covariance matrix is:\n    $$\n    \\mathrm{Var}(\\hat{\\gamma}) = \\sigma^2 (Q^\\top Q)^{-1}\n    $$\n    Since $Q$ is orthonormal, $Q^\\top Q \\approx I_p$ (the identity matrix), subject to floating-point precision. Therefore, its inverse is also approximately the identity matrix, $(Q^\\top Q)^{-1} \\approx I_p$. The vector of coefficient variances, $v_{\\text{orth}}$, becomes:\n    $$\n    v_{\\text{orth}} = \\sigma^2 \\cdot \\operatorname{diag}(I_p) = \\sigma^2 \\cdot [1, 1, \\dots, 1]^\\top = [\\sigma^2, \\sigma^2, \\dots, \\sigma^2]^\\top\n    $$\n    This demonstrates a key advantage of using an orthogonal basis: the coefficient estimators are uncorrelated and their variances are all equal to the noise variance $\\sigma^2$, irrespective of the original basis structure. This theoretical result provides a valuable check for the numerical implementation.\n\nThe final step is to round all calculated variance values to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing OLS coefficient variances for raw and\n    orthonormalized cubic spline bases across several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path\n        {'n': 200, 'knots': [0.3, 0.6], 'sigma_sq': 0.5},\n        # Case B: edge case with closely spaced knots a.k.a. high collinearity\n        {'n': 50, 'knots': [0.49, 0.5, 0.51], 'sigma_sq': 1.0},\n        # Case C: boundary case with no internal knots (polynomial basis)\n        {'n': 150, 'knots': [], 'sigma_sq': 0.2}\n    ]\n\n    results = []\n    \n    # Helper functions\n    def format_list(lst):\n        # Format a list of numbers into a comma-separated string without spaces\n        return f\"[{','.join(f'{x:.6f}' for x in lst)}]\"\n\n    def format_case_result(case_result):\n        # case_result is [v_raw_list, v_orth_list]\n        return f\"[{format_list(case_result[0])},{format_list(case_result[1])}]\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Performs Modified Gram-Schmidt orthogonalization on the columns of matrix A.\n        Returns an orthonormal matrix Q.\n        \"\"\"\n        # Work on a copy with float64 for numerical precision\n        V = A.copy().astype(np.float64)\n        num_cols = V.shape[1]\n        Q = np.zeros_like(V, dtype=np.float64)\n        \n        for j in range(num_cols):\n            # Normalize the j-th column of V to get the j-th column of Q\n            norm_vj = np.linalg.norm(V[:, j])\n            \n            # Handle potential linear dependency (column is ~zero)\n            if norm_vj  1e-12:\n                Q[:, j] = 0.0\n            else:\n                Q[:, j] = V[:, j] / norm_vj\n            \n            # Orthogonalize all subsequent columns of V against the new Q column\n            for k in range(j + 1, num_cols):\n                V[:, k] -= np.dot(Q[:, j], V[:, k]) * Q[:, j]\n                \n        return Q\n\n    for case in test_cases:\n        n = case['n']\n        knots = case['knots']\n        sigma_sq = case['sigma_sq']\n\n        # 1. Create a grid of n inputs\n        x = (np.arange(1, n + 1) - 0.5) / n\n\n        # 2. Construct the raw cubic truncated power basis design matrix X\n        p = 4 + len(knots)\n        X = np.zeros((n, p))\n        \n        # Polynomial part\n        X[:, 0] = 1\n        X[:, 1] = x\n        X[:, 2] = x**2\n        X[:, 3] = x**3\n        \n        # Truncated power part\n        if knots:\n            for j, knot in enumerate(knots):\n                X[:, 4 + j] = np.maximum(x - knot, 0)**3\n        \n        # 3. Compute the theoretical OLS coefficient variance vector for the raw basis\n        # Use float64 for higher precision, especially for inversion\n        X_64 = X.astype(np.float64)\n        gram_X = X_64.T @ X_64\n        try:\n            inv_gram_X = np.linalg.inv(gram_X)\n            v_raw_diag = np.diag(inv_gram_X)\n            v_raw = sigma_sq * v_raw_diag\n        except np.linalg.LinAlgError:\n            # A singular matrix would imply perfect collinearity\n            v_raw = np.full(p, np.inf)\n\n        # 4. Orthogonalize the columns of X to obtain Q\n        Q = modified_gram_schmidt(X)\n\n        # 5. Compute the theoretical OLS coefficient variance vector for the orthonormal basis\n        gram_Q = Q.T @ Q\n        try:\n            inv_gram_Q = np.linalg.inv(gram_Q)\n            v_orth_diag = np.diag(inv_gram_Q)\n            v_orth = sigma_sq * v_orth_diag\n        except np.linalg.LinAlgError:\n            v_orth = np.full(p, np.inf)\n            \n        # 6. Round and store results\n        # The problem statement implies rounding is the final output step,\n        # but for clean storage and formatting, we do it here.\n        v_raw_rounded = np.round(v_raw, 6)\n        v_orth_rounded = np.round(v_orth, 6)\n        \n        results.append([v_raw_rounded.tolist(), v_orth_rounded.tolist()])\n\n    # Final print statement in the exact required format.\n    formatted_results = [format_case_result(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Splines are not just for flexible curve fitting; they are powerful tools for scientific inquiry. A common task is to determine if a relationship between variables changes at a specific threshold, which can be modeled as a change in slope at a knot. This practice guides you through calculating the statistical power to detect such a change using a piecewise-linear spline model . You will connect the theoretical concepts of Ordinary Least Squares (OLS) and hypothesis testing to the practical question of how study design, such as sample size and noise level, affects your ability to discover a true effect.",
            "id": "3157150",
            "problem": "Consider the following targeted-knot piecewise-linear spline regression model intended to detect a change in slope at a suspected biological threshold. For a set of fixed design points $x_1, x_2, \\dots, x_n$ in $[0,1]$, define the hinge function $(u)_+ = \\max(u, 0)$ and the working spline basis at a chosen knot location $\\tau_{\\text{knot}}$ by the columns $1$, $x$, and $(x - \\tau_{\\text{knot}})_+$. The regression model is\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i,\\quad i=1,\\dots,n,\n$$\nwhere $\\varepsilon_i$ are independent and identically distributed as a Gaussian (normal) random variable with mean $0$ and variance $\\sigma^2$, written $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nSuppose the true mean function exhibits a slope change of magnitude $\\gamma$ at the biological threshold $\\tau_{\\text{true}}$, given by\n$$\nf_{\\text{true}}(x) = \\gamma \\,(x - \\tau_{\\text{true}})_+.\n$$\nWe wish to quantify the statistical power of a two-sided hypothesis test at significance level $\\alpha$ for\n$$\nH_0:\\ \\beta_2 = 0 \\quad \\text{versus} \\quad H_1:\\ \\beta_2 \\neq 0,\n$$\nwhich operationalizes detection of a steep slope change at the threshold using a targeted knot placement $\\tau_{\\text{knot}}$ near the suspected threshold $\\tau_{\\text{true}}$.\n\nStarting only from fundamental definitions of Ordinary Least Squares (OLS), properties of Gaussian errors, and well-tested distributional facts for linear regression, derive how to compute the statistical power $P(\\text{reject } H_0 \\mid \\gamma,\\sigma,\\tau_{\\text{true}},\\tau_{\\text{knot}},n)$ for the two-sided $t$-test on $\\beta_2$. Your derivation must begin from:\n- The OLS estimator definition $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$, where $X$ is the $n \\times p$ design matrix formed by the columns $1$, $x$, and $(x - \\tau_{\\text{knot}})_+$ (thus $p=3$), and $\\boldsymbol{y}$ is the response vector.\n- The normal error model $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$.\n- The well-tested fact that, with unknown variance estimated from residuals, a $t$-statistic for a single linear regression coefficient follows a Noncentral $t$ distribution (NCT) under alternatives, with Degrees of Freedom (DoF) $n-p$ and a noncentrality parameter that depends on the true mean projected onto the working design.\n\nThen, implement the derived computation in a complete, runnable program that:\n- Constructs the fixed design points $x_i$ as $n$ evenly spaced points on $[0,1]$.\n- Forms the working design matrix $X$ at the chosen $\\tau_{\\text{knot}}$.\n- Projects $f_{\\text{true}}(x)$ onto the columns of $X$ to find the pseudo-true coefficient vector $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}$, and uses its third component $\\beta_2^\\star$ to parameterize the two-sided $t$-test for the hinge coefficient.\n- Uses the Noncentral $t$ distribution to compute the exact two-sided power at level $\\alpha$ for testing $H_0:\\beta_2=0$, with the Degrees of Freedom $df=n-p$ and noncentrality parameter $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{(X^\\top X)^{-1}_{3,3}})$.\n\nYour program must evaluate the power for the following test suite, which covers a general case, misspecified knot placement, the no-signal edge case, boundary-threshold placement, high-noise with a steep effect, and an offset-knot with few points beyond the threshold:\n- Case $1$: $n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=2.0$, $\\alpha=0.05$.\n- Case $2$: $n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.55$, $\\gamma=2.0$, $\\alpha=0.05$.\n- Case $3$: $n=30$, $\\sigma=2.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=0.0$, $\\alpha=0.05$.\n- Case $4$: $n=120$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.10$, $\\tau_{\\text{knot}}=0.10$, $\\gamma=1.5$, $\\alpha=0.05$.\n- Case $5$: $n=120$, $\\sigma=3.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=5.0$, $\\alpha=0.05$.\n- Case $6$: $n=60$, $\\sigma=0.5$, $\\tau_{\\text{true}}=0.80$, $\\tau_{\\text{knot}}=0.75$, $\\gamma=1.0$, $\\alpha=0.05$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases given above, for example, $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$. Each $\\text{result}_k$ must be a floating-point number in $[0,1]$ equal to the computed statistical power for case $k$.",
            "solution": "The task is to derive and compute the statistical power for a hypothesis test on a coefficient in a piecewise-linear spline regression model. The power is the probability of correctly rejecting the null hypothesis when a specific alternative is true. We will follow a principled derivation starting from the fundamentals of Ordinary Least Squares (OLS) regression and the distributional theory for its estimators.\n\n**1. Model Specification and Problem Setup**\n\nThe regression model under consideration is:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i, \\quad i=1,\\dots,n\n$$\nwhere $(u)_+ = \\max(u, 0)$ is the hinge function, $\\tau_{\\text{knot}}$ is a fixed knot, and the errors $\\varepsilon_i$ are independent and identically distributed (i.i.d.) normal random variables, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nIn matrix notation, the model is $\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where:\n- $\\boldsymbol{y} = [y_1, \\dots, y_n]^\\top$ is the response vector.\n- $X$ is the $n \\times 3$ design matrix with columns corresponding to the basis functions:\n$$\nX = \\begin{bmatrix}\n1  x_1  (x_1 - \\tau_{\\text{knot}})_+ \\\\\n1  x_2  (x_2 - \\tau_{\\text{knot}})_+ \\\\\n\\vdots  \\vdots  \\vdots \\\\\n1  x_n  (x_n - \\tau_{\\text{knot}})_+\n\\end{bmatrix}\n$$\n- $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$ is the coefficient vector.\n- $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_n]^\\top$ is the error vector, with distribution $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe true data-generating process is assumed to have a mean function given by:\n$$\nf_{\\text{true}}(x) = \\gamma (x - \\tau_{\\text{true}})_+\n$$\nThis implies that the observed data $\\boldsymbol{y}$ are realizations from the model $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{f_{\\text{true}}}$ is the vector with elements $f_{\\text{true}}(x_i)$.\n\nWe are testing the null hypothesis $H_0: \\beta_2 = 0$ against the alternative $H_1: \\beta_2 \\neq 0$ at a significance level $\\alpha$.\n\n**2. Distribution of the OLS Estimator**\n\nThe OLS estimator for $\\boldsymbol{\\beta}$ is defined as $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$. To analyze its properties under the true data-generating process, we substitute $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$:\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top (\\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}) = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}\n$$\nThe expected value of $\\hat{\\boldsymbol{\\beta}}$ is:\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = E[(X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}] + E[(X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon}]\n$$\nSince $E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$, the expectation simplifies to:\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} \\equiv \\boldsymbol{\\beta}^\\star\n$$\nThis vector $\\boldsymbol{\\beta}^\\star$ is the \"pseudo-true\" parameter vector. It represents the best approximation of the true mean function $\\boldsymbol{f_{\\text{true}}}$ within the vector space spanned by the columns of the design matrix $X$. In general, $\\boldsymbol{\\beta}^\\star$ is not equal to $(0, 0, \\gamma)^\\top$ unless the basis functions and knots are perfectly aligned with the true function and have no intercept or linear term.\n\nThe covariance matrix of $\\hat{\\boldsymbol{\\beta}}$ is:\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = E\\left[ (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}]) (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}])^\\top \\right] = E\\left[ \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right) \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right)^\\top \\right]\n$$\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon}^\\top] X (X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top (\\sigma^2 I_n) X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1}\n$$\nSince $\\hat{\\boldsymbol{\\beta}}$ is a linear transformation of the Gaussian vector $\\boldsymbol{\\varepsilon}$, it is also normally distributed:\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}^\\star, \\sigma^2 (X^\\top X)^{-1})\n$$\n\n**3. The t-statistic and its Distribution**\n\nWe are interested in the coefficient $\\beta_2$. The estimator $\\hat{\\beta}_2$ is the third element of $\\hat{\\boldsymbol{\\beta}}$. Its distribution is univariate normal:\n$$\n\\hat{\\beta}_2 \\sim \\mathcal{N}(\\beta_2^\\star, \\sigma^2 C_{33})\n$$\nwhere $\\beta_2^\\star$ is the third element of $\\boldsymbol{\\beta}^\\star$ and $C_{33} = [(X^\\top X)^{-1}]_{3,3}$ is the third diagonal element of the inverse of the Gram matrix. The standard error of $\\hat{\\beta}_2$ is $\\text{se}(\\hat{\\beta}_2) = \\sigma \\sqrt{C_{33}}$.\n\nIn practice, $\\sigma^2$ is unknown and must be estimated from the data. The unbiased estimator is $\\hat{\\sigma}^2 = \\frac{1}{n-p} \\text{RSS}$, where $p=3$ is the number of parameters and $\\text{RSS} = \\|\\boldsymbol{y} - X\\hat{\\boldsymbol{\\beta}}\\|^2$ is the residual sum of squares. The test statistic replaces the true standard error with the estimated standard error, $\\hat{\\text{se}}(\\hat{\\beta}_2) = \\hat{\\sigma}\\sqrt{C_{33}}$:\n$$\nT = \\frac{\\hat{\\beta}_2 - 0}{\\hat{\\text{se}}(\\hat{\\beta}_2)} = \\frac{\\hat{\\beta}_2}{\\hat{\\sigma}\\sqrt{C_{33}}}\n$$\nUnder the general alternative hypothesis where $E[\\hat{\\beta}_2] = \\beta_2^\\star \\neq 0$, this statistic follows a Noncentral $t$-distribution. The degrees of freedom ($df$) are those associated with the variance estimate, $df = n-p = n-3$. The noncentrality parameter (NCP), $\\delta$, is the standardized mean of the numerator:\n$$\n\\delta = \\frac{E[\\hat{\\beta}_2]}{\\text{se}(\\hat{\\beta}_2)} = \\frac{\\beta_2^\\star}{\\sigma\\sqrt{C_{33}}}\n$$\nThus, under the alternative specified by $\\boldsymbol{f_{\\text{true}}}$, the test statistic has the distribution $T \\sim t(df, \\delta)$, a noncentral $t$-distribution with $df=n-3$ degrees of freedom and NCP $\\delta$.\n\n**4. Power Calculation**\n\nFor a two-sided test at significance level $\\alpha$, we reject the null hypothesis $H_0: \\beta_2=0$ if the observed value of the test statistic $|T|$ exceeds a critical value. This critical value, which we denote $t_{\\text{crit}}$, is determined from the null distribution. Under $H_0$, $\\gamma=0$, which implies $\\boldsymbol{f_{\\text{true}}} = \\boldsymbol{0}$, $\\boldsymbol{\\beta}^\\star = \\boldsymbol{0}$, and $\\delta = 0$. The null distribution is therefore a central $t$-distribution, $T \\sim t(df)$. The critical value $t_{\\text{crit}}$ is the upper $(1-\\alpha/2)$ quantile of this central $t$-distribution:\n$$\nt_{\\text{crit}} = t_{df, 1-\\alpha/2}\n$$\nThe statistical power is the probability of rejecting $H_0$ when the alternative is true, i.e., when $T \\sim t(df, \\delta)$.\n$$\n\\text{Power} = P(|T| > t_{\\text{crit}} \\mid T \\sim t(df, \\delta)) = P(T > t_{\\text{crit}}) + P(T  -t_{\\text{crit}})\n$$\nLet $F_{t(df, \\delta)}$ be the cumulative distribution function (CDF) of the noncentral $t$-distribution. The power is then calculated as:\n$$\n\\text{Power} = \\left( 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) \\right) + F_{t(df, \\delta)}(-t_{\\text{crit}})\n$$\n\n**5. Computational Steps**\n\nThe procedure to compute the power for each case is as follows:\n1.  Given $n, \\sigma, \\tau_{\\text{true}}, \\tau_{\\text{knot}}, \\gamma, \\alpha$.\n2.  Define the design points $x_i$ as $n$ equally spaced points from $0$ to $1$, inclusive.\n3.  Construct the $n \\times 3$ design matrix $X$ with columns $1$, $x_i$, and $(x_i - \\tau_{\\text{knot}})_+$.\n4.  Construct the $n \\times 1$ true mean vector $\\boldsymbol{f}_{\\text{true}}$ with elements $\\gamma (x_i - \\tau_{\\text{true}})_+$.\n5.  Compute the matrix product $X^\\top X$ and its inverse, $(X^\\top X)^{-1}$.\n6.  Compute the pseudo-true parameter vector $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f}_{\\text{true}}$.\n7.  Extract the third component, $\\beta_2^\\star = \\boldsymbol{\\beta}^\\star_2$.\n8.  Extract the third diagonal element of the inverse matrix, $C_{33} = [(X^\\top X)^{-1}]_{2,2}$ (using $0$-based indexing for the third row/column).\n9.  Calculate the noncentrality parameter $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{C_{33}})$.\n10. Set the degrees of freedom $df = n - 3$.\n11. Find the critical value $t_{\\text{crit}}$ from the inverse CDF (or percent-point function) of the central $t$-distribution: $t_{\\text{crit}} = F^{-1}_{t(df)}(1 - \\alpha/2)$.\n12. Compute the power using the CDF of the noncentral $t$-distribution with $df$ and $\\delta$: $\\text{Power} = 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) + F_{t(df, \\delta)}(-t_{\\text{crit}})$.\n\nThis completes the derivation and specification of the algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, nct\n\ndef solve():\n    \"\"\"\n    Computes the statistical power for a hypothesis test on a coefficient in a\n    piecewise-linear spline regression model for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.50, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 2: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.55, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.55, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 3: n=30, sigma=2.0, tau_true=0.50, tau_knot=0.50, gamma=0.0, alpha=0.05\n        {'n': 30, 'sigma': 2.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 0.0, 'alpha': 0.05},\n        # Case 4: n=120, sigma=1.0, tau_true=0.10, tau_knot=0.10, gamma=1.5, alpha=0.05\n        {'n': 120, 'sigma': 1.0, 'tau_true': 0.10, 'tau_knot': 0.10, 'gamma': 1.5, 'alpha': 0.05},\n        # Case 5: n=120, sigma=3.0, tau_true=0.50, tau_knot=0.50, gamma=5.0, alpha=0.05\n        {'n': 120, 'sigma': 3.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 5.0, 'alpha': 0.05},\n        # Case 6: n=60, sigma=0.5, tau_true=0.80, tau_knot=0.75, gamma=1.0, alpha=0.05\n        {'n': 60, 'sigma': 0.5, 'tau_true': 0.80, 'tau_knot': 0.75, 'gamma': 1.0, 'alpha': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        # Extract parameters for the current case\n        n = params['n']\n        sigma = params['sigma']\n        tau_true = params['tau_true']\n        tau_knot = params['tau_knot']\n        gamma = params['gamma']\n        alpha = params['alpha']\n        \n        # 1. Construct the fixed design points x_i as n evenly spaced points on [0,1]\n        x = np.linspace(0.0, 1.0, n)\n        \n        # 2. Form the working design matrix X\n        # Columns are 1, x, and (x - tau_knot)_+\n        p = 3\n        X = np.zeros((n, p))\n        X[:, 0] = 1.0\n        X[:, 1] = x\n        X[:, 2] = np.maximum(0, x - tau_knot)\n\n        # 3. Construct the true mean function vector f_true\n        f_true = gamma * np.maximum(0, x - tau_true)\n        \n        # 4. Compute the pseudo-true coefficient vector beta_star\n        # beta_star = (X^T X)^-1 X^T f_true\n        try:\n            XTX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # This case occurs if X has linearly dependent columns.\n            # For the given knots, X should have full rank.\n            # E.g., if tau_knot = 1, the third column is all zeros.\n            results.append(np.nan) # Mark as not-a-number if problem occurs\n            continue\n\n        beta_star = XTX_inv @ X.T @ f_true\n        \n        # 5. Extract the third component, beta_2_star\n        beta_2_star = beta_star[2]\n        \n        # 6. Extract the variance term for beta_2_hat from (X^T X)^-1\n        C_33 = XTX_inv[2, 2]\n        \n        # 7. Calculate the noncentrality parameter (NCP), delta\n        delta = beta_2_star / (sigma * np.sqrt(C_33))\n        \n        # 8. Define degrees of freedom for the t-test\n        df = n - p\n        \n        # 9. Find the critical value for the two-sided test\n        t_crit = t.ppf(1 - alpha / 2, df)\n        \n        # 10. Compute the power using the Noncentral t (NCT) distribution\n        # Power = P(T  t_crit) + P(T  -t_crit) where T ~ NCT(df, delta)\n        power = 1.0 - nct.cdf(t_crit, df, delta) + nct.cdf(-t_crit, df, delta)\n        \n        results.append(power)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central challenge in spline modeling is deciding where and how many knots to place. While manual placement or cross-validation are common strategies, a more sophisticated approach is to let the data inform the model's complexity in a probabilistic framework. In this exercise, you will implement a simplified Bayesian variable selection method using \"spike-and-slab\" priors to calculate the posterior probability that each potential knot is needed in the model . This provides a hands-on introduction to probabilistic modeling for automated knot selection, offering a powerful alternative to traditional methods.",
            "id": "3157114",
            "problem": "Consider the one-dimensional regression setting where observations are pairs $(x_i, y_i)$ for $i = 1, \\dots, n$, with $x_i \\in [0,1]$. We construct a spline using a truncated linear basis with candidate knots $\\{\\xi_k\\}_{k=1}^K$, so that the spline component is a linear combination of functions $h_k(x) = (x - \\xi_k)_+$, where $(\\cdot)_+$ denotes the positive part. Let $z_{ik} = h_k(x_i)$ denote the $i$th entry of the basis vector $\\mathbf{z}_k \\in \\mathbb{R}^n$ corresponding to knot $\\xi_k$.\n\nAssume the following generative model for the data:\n$$\ny_i = \\alpha + \\beta x_i + \\sum_{k=1}^K \\gamma_k \\theta_k (x_i - \\xi_k)_+ + \\varepsilon_i,\n$$\nwhere $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent and identically distributed Gaussian noise, $\\gamma_k \\in \\{0,1\\}$ indicates inclusion of knot $\\xi_k$, and $\\theta_k \\in \\mathbb{R}$ is the coefficient for the $k$th basis function. Impose a spike-and-slab prior on the knot coefficients: $\\mathbb{P}(\\gamma_k = 1) = \\pi$ independently across $k$, and conditional on $\\gamma_k = 1$, $\\theta_k \\sim \\mathcal{N}(0, \\tau^2)$ independently across $k$. Conditional on $\\gamma_k = 0$, $\\theta_k = 0$ deterministically.\n\nWe wish to perform Bayesian inference that quantifies posterior uncertainty over knot locations and constructs a Bayesian spline estimate. To make the computation tractable at the intermediate undergraduate level, proceed by first fitting the base linear trend $(\\alpha, \\beta)$ using ordinary least squares without any knot terms, and then work with the residual vector $\\mathbf{r} \\in \\mathbb{R}^n$ defined by $r_i = y_i - \\hat{\\alpha} - \\hat{\\beta} x_i$, where $(\\hat{\\alpha}, \\hat{\\beta})$ is the least-squares fit of $y$ on $[1, x]$. For each candidate knot $\\xi_k$, consider the one-at-a-time model for the residuals:\n$$\n\\mathbf{r} = \\gamma_k \\theta_k \\mathbf{z}_k + \\boldsymbol{\\varepsilon}, \\quad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n),\n$$\nwith the same spike-and-slab prior for $(\\gamma_k, \\theta_k)$ as above.\n\nUsing Bayes' theorem, the Normal likelihood, and standard linear algebra identities (the matrix determinant lemma and the Woodbury identity), derive the Bayes factor (BF) comparing the model with inclusion $\\gamma_k = 1$ to the model without inclusion $\\gamma_k = 0$, marginalizing over $\\theta_k$ under the slab prior. Show that the Bayes factor has the form\n$$\n\\text{BF}_k = \\left(1 + a \\,\\|\\mathbf{z}_k\\|^2\\right)^{-1/2} \\exp\\left( \\frac{1}{2} \\cdot \\frac{a}{1 + a \\,\\|\\mathbf{z}_k\\|^2} \\cdot \\frac{(\\mathbf{z}_k^\\top \\mathbf{r})^2}{\\sigma^2} \\right),\n$$\nwhere $a = \\tau^2/\\sigma^2$, $\\|\\mathbf{z}_k\\|^2 = \\sum_{i=1}^n z_{ik}^2$, and $\\mathbf{z}_k^\\top \\mathbf{r} = \\sum_{i=1}^n z_{ik} r_i$. Then, compute the posterior inclusion probability for knot $\\xi_k$ via\n$$\np_k = \\mathbb{P}(\\gamma_k = 1 \\mid \\mathbf{r}) = \\frac{\\pi \\cdot \\text{BF}_k}{\\pi \\cdot \\text{BF}_k + (1 - \\pi)}.\n$$\nAlso, conditional on inclusion, the posterior for $\\theta_k$ is Gaussian with mean and variance\n$$\n\\mu_k = \\left(\\frac{1}{\\tau^2} + \\frac{\\|\\mathbf{z}_k\\|^2}{\\sigma^2}\\right)^{-1} \\cdot \\frac{\\mathbf{z}_k^\\top \\mathbf{r}}{\\sigma^2}, \\quad\nv_k = \\left(\\frac{1}{\\tau^2} + \\frac{\\|\\mathbf{z}_k\\|^2}{\\sigma^2}\\right)^{-1}.\n$$\nUse this to construct the posterior mean coefficient $\\mathbb{E}[\\theta_k \\mid \\mathbf{r}] = p_k \\mu_k$ and the posterior mean spline estimate\n$$\n\\hat{f}(x_i) = \\hat{\\alpha} + \\hat{\\beta} x_i + \\sum_{k=1}^K \\left(p_k \\mu_k\\right) (x_i - \\xi_k)_+.\n$$\n\nImplement a program that, for each test case below, computes the posterior inclusion probabilities $\\{p_k\\}_{k=1}^K$ using the formulas above. You do not need to output $\\hat{f}(x_i)$, but you must compute and use $\\mathbf{r}$ and $\\mathbf{z}_k$ as specified.\n\nFor all test cases, use the same hyperparameters $\\pi = 0.2$ and $\\tau^2 = 1.0$. The observation noise variance $\\sigma^2$ is known and provided per test case. Use the seeds provided to ensure reproducibility when generating $x_i$ values and noise.\n\nTest Suite:\n- Case 1 (happy path, clear single knot): $n = 60$, true function $f(x) = 2.0 + 1.0 \\cdot (x - 0.5)_+$ with $\\sigma^2 = 0.01$, candidate knots $\\{\\xi_k\\} = \\{0.25, 0.50, 0.75\\}$, random seed $42$, with $x_i$ drawn uniformly from $[0,1]$ and $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n- Case 2 (boundary/none, no true knot): $n = 60$, true function $f(x) = 1.0 + 0.5 x$ with $\\sigma^2 = 0.01$, candidate knots $\\{\\xi_k\\} = \\{0.20, 0.40, 0.60, 0.80\\}$, random seed $123$, with $x_i$ drawn uniformly from $[0,1]$ and $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n- Case 3 (multiple signals, two knots with opposite signs and one near boundary): $n = 60$, true function $f(x) = 0.5 - 0.2 x + 0.8 \\cdot (x - 0.30)_+ - 0.6 \\cdot (x - 0.70)_+$ with $\\sigma^2 = 0.01$, candidate knots $\\{\\xi_k\\} = \\{0.30, 0.32, 0.70, 0.90\\}$, random seed $2023$, with $x_i$ drawn uniformly from $[0,1]$ and $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be the list of posterior inclusion probabilities for the knots in that case, in the same order as the candidate knots, so the overall output is of the form $[[p^{(1)}_1, \\dots, p^{(1)}_{K_1}], [p^{(2)}_1, \\dots, p^{(2)}_{K_2}], [p^{(3)}_1, \\dots, p^{(3)}_{K_3}]]$, where superscripts denote the case index. All numbers should be standard floating-point values without any physical unit or percentage sign.",
            "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in Bayesian statistical theory, specifically the application of spike-and-slab priors for variable selection in the context of spline regression. The problem is well-posed, providing all necessary data, parameters, and a clear, tractable computational procedure. The use of a simplified one-at-a-time knot analysis is explicitly noted as a pedagogical choice for tractability, which is acceptable. The required task is objective and mathematically formalizable. Therefore, a complete solution will be provided.\n\nThe core of the problem is to compute the posterior inclusion probability $p_k = \\mathbb{P}(\\gamma_k = 1 \\mid \\mathbf{r})$ for each candidate knot $\\xi_k$. This requires deriving and then applying the formula for the Bayes Factor, $\\text{BF}_k$.\n\n### Step 1: Derivation of the Bayes Factor\n\nThe Bayes factor $\\text{BF}_k$ compares two models for the residuals $\\mathbf{r}$:\n-   Model $M_1 (\\gamma_k=1)$: The knot $\\xi_k$ is included. The model is $\\mathbf{r} = \\theta_k \\mathbf{z}_k + \\boldsymbol{\\varepsilon}$, where $\\theta_k \\sim \\mathcal{N}(0, \\tau^2)$.\n-   Model $M_0 (\\gamma_k=0)$: The knot $\\xi_k$ is excluded. The model is $\\mathbf{r} = \\boldsymbol{\\varepsilon}$.\n\nIn both models, the noise term is $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$. The Bayes factor is the ratio of the marginal likelihoods of the data under each model:\n$$\n\\text{BF}_k = \\frac{p(\\mathbf{r} \\mid M_1)}{p(\\mathbf{r} \\mid M_0)}\n$$\n\nFirst, we compute the marginal likelihood for the null model, $M_0$. Under $M_0$, the residuals $\\mathbf{r}$ are drawn directly from the noise distribution, $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$. The probability density is:\n$$\np(\\mathbf{r} \\mid M_0) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\mathbf{r}^\\top \\mathbf{r}\\right)\n$$\n\nNext, we compute the marginal likelihood for the alternative model, $M_1$. This requires integrating over the unknown coefficient $\\theta_k$:\n$$\np(\\mathbf{r} \\mid M_1) = \\int p(\\mathbf{r} \\mid \\theta_k, M_1) p(\\theta_k \\mid M_1) d\\theta_k\n$$\nThe terms in the integral are:\n-   The likelihood $p(\\mathbf{r} \\mid \\theta_k, M_1)$: Given $\\theta_k$, the model is $\\mathbf{r} \\sim \\mathcal{N}(\\theta_k \\mathbf{z}_k, \\sigma^2 \\mathbf{I}_n)$. Its density is:\n    $$\n    p(\\mathbf{r} \\mid \\theta_k, M_1) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} (\\mathbf{r} - \\theta_k \\mathbf{z}_k)^\\top (\\mathbf{r} - \\theta_k \\mathbf{z}_k)\\right)\n    $$\n-   The prior $p(\\theta_k \\mid M_1)$: The slab prior for $\\theta_k$ is $\\mathcal{N}(0, \\tau^2)$. Its density is:\n    $$\n    p(\\theta_k \\mid M_1) = (2\\pi\\tau^2)^{-1/2} \\exp\\left(-\\frac{1}{2\\tau^2} \\theta_k^2\\right)\n    $$\n\nTo evaluate the integral, we combine the exponents from the likelihood and prior:\n$$\n\\begin{align*}\n -\\frac{1}{2\\sigma^2} (\\mathbf{r}^\\top\\mathbf{r} - 2\\theta_k \\mathbf{z}_k^\\top \\mathbf{r} + \\theta_k^2 \\|\\mathbf{z}_k\\|^2) - \\frac{1}{2\\tau^2} \\theta_k^2 \\\\\n=\\  -\\frac{1}{2} \\left[ \\theta_k^2 \\left(\\frac{\\|\\mathbf{z}_k\\|^2}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2\\theta_k \\frac{\\mathbf{z}_k^\\top \\mathbf{r}}{\\sigma^2} + \\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\sigma^2} \\right]\n\\end{align*}\n$$\nThis is a quadratic form in $\\theta_k$. We complete the square with respect to $\\theta_k$. The term in the square brackets has the form $A\\theta_k^2 - 2B\\theta_k + C$, which can be written as $A(\\theta_k - B/A)^2 + C - B^2/A$.\nHere, $A = \\frac{\\|\\mathbf{z}_k\\|^2}{\\sigma^2} + \\frac{1}{\\tau^2}$ and $B = \\frac{\\mathbf{z}_k^\\top \\mathbf{r}}{\\sigma^2}$.\nThe term $A$ is the inverse posterior variance of $\\theta_k$, $A = v_k^{-1}$, and the term $B/A$ is the posterior mean, $\\mu_k$. This confirms the formulas for $\\mu_k$ and $v_k$ given in the problem statement.\nThe exponent becomes:\n$$\n-\\frac{1}{2v_k}(\\theta_k - \\mu_k)^2 -\\frac{1}{2}\\left(\\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\sigma^2} - \\frac{\\mu_k^2}{v_k}\\right)\n$$\nThe marginal likelihood $p(\\mathbf{r} \\mid M_1)$ is obtained by integrating the product of the densities:\n$$\n\\begin{align*}\np(\\mathbf{r} \\mid M_1) = \\int (2\\pi\\sigma^2)^{-n/2} (2\\pi\\tau^2)^{-1/2} \\exp\\left[-\\frac{1}{2v_k}(\\theta_k - \\mu_k)^2\\right] \\exp\\left[-\\frac{1}{2}\\left(\\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\sigma^2} - \\frac{\\mu_k^2}{v_k}\\right)\\right] d\\theta_k \\\\\n= (2\\pi\\sigma^2)^{-n/2} (2\\pi\\tau^2)^{-1/2} (2\\pi v_k)^{1/2} \\exp\\left[-\\frac{1}{2}\\left(\\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\sigma^2} - \\frac{\\mu_k^2}{v_k}\\right)\\right]\n\\end{align*}\n$$\nNow we can form the Bayes factor ratio $\\text{BF}_k = p(\\mathbf{r} \\mid M_1) / p(\\mathbf{r} \\mid M_0)$:\n$$\n\\text{BF}_k = \\frac{(2\\pi\\sigma^2)^{-n/2} (2\\pi\\tau^2)^{-1/2} (2\\pi v_k)^{1/2} \\exp\\left(-\\frac{1}{2}\\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\sigma^2}\\right) \\exp\\left(\\frac{1}{2}\\frac{\\mu_k^2}{v_k}\\right)}{(2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2} \\frac{\\mathbf{r}^\\top \\mathbf{r}}{\\sigma^2}\\right)}\n$$\nThe terms involving $\\mathbf{r}^\\top\\mathbf{r}$ and $(2\\pi\\sigma^2)^{-n/2}$ cancel, leaving:\n$$\n\\text{BF}_k = (2\\pi\\tau^2)^{-1/2} (2\\pi v_k)^{1/2} \\exp\\left(\\frac{1}{2} \\frac{\\mu_k^2}{v_k}\\right) = \\left(\\frac{v_k}{\\tau^2}\\right)^{1/2} \\exp\\left(\\frac{1}{2} v_k \\left(\\frac{\\mathbf{z}_k^\\top \\mathbf{r}}{\\sigma^2}\\right)^2\\right)\n$$\nLet's substitute $v_k = \\left(\\frac{1}{\\tau^2} + \\frac{\\|\\mathbf{z}_k\\|^2}{\\sigma^2}\\right)^{-1} = \\frac{\\tau^2\\sigma^2}{\\sigma^2 + \\tau^2\\|\\mathbf{z}_k\\|^2}$.\nThe pre-factor is:\n$$\n\\left(\\frac{v_k}{\\tau^2}\\right)^{1/2} = \\left(\\frac{1}{\\tau^2} \\frac{\\tau^2\\sigma^2}{\\sigma^2 + \\tau^2\\|\\mathbf{z}_k\\|^2}\\right)^{1/2} = \\left(\\frac{\\sigma^2}{\\sigma^2 + \\tau^2\\|\\mathbf{z}_k\\|^2}\\right)^{1/2}\n$$\nDividing the numerator and denominator inside the parenthesis by $\\sigma^2$ and letting $a = \\tau^2/\\sigma^2$, we get:\n$$\n\\left(\\frac{1}{1 + (\\tau^2/\\sigma^2)\\|\\mathbf{z}_k\\|^2}\\right)^{1/2} = \\left(1 + a\\|\\mathbf{z}_k\\|^2\\right)^{-1/2}\n$$\nThe term in the exponent is:\n$$\n\\frac{1}{2} v_k \\left(\\frac{\\mathbf{z}_k^\\top \\mathbf{r}}{\\sigma^2}\\right)^2 = \\frac{1}{2} \\frac{\\tau^2\\sigma^2}{\\sigma^2 + \\tau^2\\|\\mathbf{z}_k\\|^2} \\frac{(\\mathbf{z}_k^\\top \\mathbf{r})^2}{\\sigma^4} = \\frac{1}{2} \\frac{\\tau^2}{\\sigma^2 + \\tau^2\\|\\mathbf{z}_k\\|^2} \\frac{(\\mathbf{z}_k^\\top \\mathbf{r})^2}{\\sigma^2}\n$$\nDividing numerator and denominator of the first fraction by $\\sigma^2$:\n$$\n\\frac{1}{2} \\frac{\\tau^2/\\sigma^2}{1 + (\\tau^2/\\sigma^2)\\|\\mathbf{z}_k\\|^2} \\frac{(\\mathbf{z}_k^\\top \\mathbf{r})^2}{\\sigma^2} = \\frac{1}{2} \\frac{a}{1 + a\\|\\mathbf{z}_k\\|^2} \\frac{(\\mathbf{z}_k^\\top \\mathbf{r})^2}{\\sigma^2}\n$$\nCombining these results yields the desired formula for the Bayes Factor:\n$$\n\\text{BF}_k = \\left(1 + a \\|\\mathbf{z}_k\\|^2\\right)^{-1/2} \\exp\\left( \\frac{1}{2} \\cdot \\frac{a}{1 + a \\|\\mathbf{z}_k\\|^2} \\cdot \\frac{(\\mathbf{z}_k^\\top \\mathbf{r})^2}{\\sigma^2} \\right)\n$$\nThis completes the derivation.\n\n### Step 2: Computational Procedure\n\nThe algorithm to compute the posterior inclusion probabilities $\\{p_k\\}$ for each test case is as follows:\n\n1.  **Initialize Parameters**: For each test case, define the sample size $n$, the true function $f(x)$, the noise variance $\\sigma^2$, the set of candidate knots $\\{\\xi_k\\}_{k=1}^K$, and the random seed. The hyperparameters are fixed at $\\pi = 0.2$ and $\\tau^2 = 1.0$.\n\n2.  **Generate Data**:\n    -   Set the random number generator seed for reproducibility.\n    -   Generate $n$ independent samples $x_i$ from a uniform distribution on $[0,1]$.\n    -   Compute the true function values $f(x_i)$ for each $x_i$.\n    -   Generate $n$ noise samples $\\varepsilon_i$ from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$.\n    -   Construct the observed data $y_i = f(x_i) + \\varepsilon_i$.\n\n3.  **Compute Residuals**:\n    -   Construct the design matrix $\\mathbf{X}$ for ordinary least squares (OLS) regression. This is an $n \\times 2$ matrix where the first column consists of all ones (for the intercept $\\alpha$) and the second column contains the values $x_i$ (for the slope $\\beta$).\n    -   Calculate the OLS estimates $(\\hat{\\alpha}, \\hat{\\beta})$ using the formula $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$.\n    -   Compute the residual vector $\\mathbf{r} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$, where $\\mathbf{y} = (y_1, \\dots, y_n)^\\top$.\n\n4.  **Calculate Posterior Inclusion Probabilities**:\n    -   Iterate through each candidate knot $\\xi_k$ for the current test case.\n    -   For each $\\xi_k$, construct the corresponding basis vector $\\mathbf{z}_k \\in \\mathbb{R}^n$ with elements $z_{ik} = (x_i - \\xi_k)_+ = \\max(0, x_i - \\xi_k)$.\n    -   Compute the required quantities: the squared norm $\\|\\mathbf{z}_k\\|^2 = \\mathbf{z}_k^\\top \\mathbf{z}_k$ and the dot product $\\mathbf{z}_k^\\top \\mathbf{r}$.\n    -   Calculate the ratio $a = \\tau^2 / \\sigma^2$.\n    -   Substitute these values into the derived formula for $\\text{BF}_k$.\n    -   Use the computed $\\text{BF}_k$ and the prior inclusion probability $\\pi$ to find the posterior inclusion probability $p_k$:\n        $$\n        p_k = \\frac{\\pi \\cdot \\text{BF}_k}{\\pi \\cdot \\text{BF}_k + (1 - \\pi)}\n        $$\n    -   Store the computed $p_k$ values.\n\n5.  **Format Output**: Collect the lists of probabilities for each test case and format them into the specified string `[[...],[...],...]` for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes posterior inclusion probabilities for spline knots in a Bayesian\n    linear regression model for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"n\": 60,\n            \"true_func\": lambda x: 2.0 + 1.0 * np.maximum(0, x - 0.5),\n            \"sigma_sq\": 0.01,\n            \"knots\": [0.25, 0.50, 0.75],\n            \"seed\": 42\n        },\n        {\n            \"case_id\": 2,\n            \"n\": 60,\n            \"true_func\": lambda x: 1.0 + 0.5 * x,\n            \"sigma_sq\": 0.01,\n            \"knots\": [0.20, 0.40, 0.60, 0.80],\n            \"seed\": 123\n        },\n        {\n            \"case_id\": 3,\n            \"n\": 60,\n            \"true_func\": lambda x: 0.5 - 0.2 * x + 0.8 * np.maximum(0, x - 0.30) - 0.6 * np.maximum(0, x - 0.70),\n            \"sigma_sq\": 0.01,\n            \"knots\": [0.30, 0.32, 0.70, 0.90],\n            \"seed\": 2023\n        }\n    ]\n\n    # Global hyperparameters\n    pi = 0.2\n    tau_sq = 1.0\n\n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        true_func = case[\"true_func\"]\n        sigma_sq = case[\"sigma_sq\"]\n        knots = case[\"knots\"]\n        seed = case[\"seed\"]\n\n        # 1. Generate data\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(0, 1, n)\n        epsilon = rng.normal(0, np.sqrt(sigma_sq), n)\n        y = true_func(x) + epsilon\n\n        # 2. Compute residuals from OLS fit\n        X_ols = np.vstack([np.ones(n), x]).T\n        try:\n            beta_hat = np.linalg.inv(X_ols.T @ X_ols) @ X_ols.T @ y\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for stability if X.T @ X is singular\n            beta_hat = np.linalg.pinv(X_ols.T @ X_ols) @ X_ols.T @ y\n        \n        residuals = y - X_ols @ beta_hat\n\n        # 3. Calculate posterior inclusion probabilities for each knot\n        case_probabilities = []\n        a = tau_sq / sigma_sq\n\n        for knot in knots:\n            # Construct the basis vector z_k\n            z_k = np.maximum(0, x - knot)\n\n            # Compute necessary summaries\n            z_k_norm_sq = z_k.T @ z_k\n            z_k_dot_r = z_k.T @ residuals\n\n            # Calculate Bayes Factor (BF_k)\n            term1 = 1.0 + a * z_k_norm_sq\n            \n            # Handle potential underflow/overflow or invalid values in term1\n            if term1 = 0:\n                # This case is unlikely with positive a and norm_sq, but for safety\n                bf_k = 0.0\n            else:\n                bf_k_prefactor = 1.0 / np.sqrt(term1)\n                exp_numerator = a * (z_k_dot_r ** 2)\n                exp_denominator = 2.0 * sigma_sq * term1\n                exp_term = np.exp(exp_numerator / exp_denominator)\n                bf_k = bf_k_prefactor * exp_term\n\n            # Calculate posterior inclusion probability (p_k)\n            p_k_numerator = pi * bf_k\n            p_k_denominator = p_k_numerator + (1.0 - pi)\n\n            if p_k_denominator == 0:\n                p_k = 0.0 if p_k_numerator == 0.0 else 1.0\n            else:\n                p_k = p_k_numerator / p_k_denominator\n            \n            case_probabilities.append(p_k)\n            \n        all_results.append(case_probabilities)\n\n    # Format the final output string without extra spaces\n    inner_parts = [f\"[{','.join(map(str, p_list))}]\" for p_list in all_results]\n    final_output_string = f\"[{','.join(inner_parts)}]\"\n\n    print(final_output_string)\n```"
        }
    ]
}