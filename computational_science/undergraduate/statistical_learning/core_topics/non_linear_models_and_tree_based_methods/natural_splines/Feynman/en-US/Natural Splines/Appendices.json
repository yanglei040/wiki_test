{
    "hands_on_practices": [
        {
            "introduction": "Natural splines offer a powerful method for approximating complex, non-linear functions from a set of points. The core idea is to fit a series of smooth, piecewise cubic polynomials that are constrained to behave linearly at the boundaries. This practice asks you to build a natural spline to approximate the exponential function, allowing you to explore a fundamental trade-off: how does increasing the number of knots improve the accuracy of the fit? By calculating the $L^2$ approximation error, you will gain a quantitative understanding of the spline's performance and its convergence properties .",
            "id": "3153014",
            "problem": "You are asked to implement a program that constructs natural cubic spline approximations to the function $f(x) = e^x$ on the interval $[0,1]$ for several specified total knot counts and computes the $L^2$ approximation error of each spline. In the context of statistical learning, a natural spline is a piecewise cubic polynomial function that is twice continuously differentiable and satisfies linear boundary conditions at the endpoints, which constrain the function to be less flexible near the boundaries and often reduce variance. For cubic natural splines, the natural boundary conditions set the second derivatives at the interval endpoints to zero.\n\nStarting from the following foundational definitions:\n- A natural cubic spline $s_K(x)$ with total knot count $K$ on $[0,1]$ is a function that is a piecewise cubic polynomial on subintervals determined by a non-decreasing sequence of knots $0 = \\tau_0 < \\tau_1 < \\cdots < \\tau_{K-1} = 1$, satisfies interpolation constraints $s_K(\\tau_i) = f(\\tau_i)$ for all $i \\in \\{0,1,\\dots,K-1\\}$, has continuous first and second derivatives on $[0,1]$, and satisfies the boundary conditions $s_K''(0) = 0$ and $s_K''(1) = 0$.\n- The $L^2$ approximation error over $[0,1]$ is defined as\n$$\nE_K \\;=\\; \\left( \\int_{0}^{1} \\big( s_K(x) - f(x) \\big)^2 \\, dx \\right)^{1/2}.\n$$\n\nYour task is to:\n1. For each specified total knot count $K$, construct a uniform knot sequence on $[0,1]$ given by $\\tau_i = \\frac{i}{K-1}$ for $i \\in \\{0,1,\\dots,K-1\\}$.\n2. Using these knots, form the natural cubic spline $s_K(x)$ that interpolates $f(x) = e^x$ at the knots and satisfies $s_K''(0) = 0$ and $s_K''(1) = 0$.\n3. Compute the $L^2$ approximation error $E_K$ on $[0,1]$ for each spline by numerically evaluating the integral.\n\nImplementation requirements:\n- Use a numerically stable and accurate method to evaluate the integral defining $E_K$. The final error values must be computed to high accuracy.\n- There are no physical units and no angles involved in this problem.\n- Your program must be a complete, runnable program that does not require any external input.\n\nTest suite:\n- Use the following total knot counts $K$: $\\{2, 3, 5, 11\\}$.\n  - $K = 2$ exercises the boundary case where only the endpoints are knots.\n  - $K = 3$ includes a single interior knot at $x = \\frac{1}{2}$.\n  - $K = 5$ introduces a moderate number of interior knots.\n  - $K = 11$ provides a finer knot grid on $[0,1]$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the provided test cases. Each $E_K$ must be rounded to exactly eight digits after the decimal point. For example: \"[a,b,c,d]\" where $a$, $b$, $c$, and $d$ are the rounded values for $K \\in \\{2,3,5,11\\}$ respectively.",
            "solution": "The problem requires the construction of natural cubic splines to approximate the function $f(x) = e^x$ on the interval $[0, 1]$, and the subsequent calculation of the $L^2$ approximation error for a given set of total knot counts $K$. The solution involves two primary stages: first, constructing the spline, and second, evaluating the error integral.\n\n**1. Natural Cubic Spline Construction**\n\nA cubic spline $s_K(x)$ with a set of $K$ knots $\\tau_0 < \\tau_1 < \\dots < \\tau_{K-1}$ is a function that is a piecewise cubic polynomial. On each subinterval $[\\tau_i, \\tau_{i+1}]$ for $i \\in \\{0, 1, \\dots, K-2\\}$, the spline $s_K(x)$ is represented by a unique cubic polynomial, which we denote as $s_i(x)$. The total number of unknown coefficients for these $K-1$ polynomials is $4(K-1)$. These coefficients are determined by enforcing a set of conditions.\n\nThe standard conditions for an interpolating cubic spline are:\n-   **Interpolation**: The spline must pass through the given data points. For each knot $\\tau_i$, we require $s_K(\\tau_i) = f(\\tau_i)$. Let $y_i = f(\\tau_i)$. This provides $K$ conditions, specifically $s_i(\\tau_i) = y_i$ and $s_i(\\tau_{i+1}) = y_{i+1}$ for all $i \\in \\{0, \\dots, K-2\\}$. These constitute $2(K-1)$ constraints.\n-   **Continuity of the First Derivative**: At each interior knot $\\tau_i$ for $i \\in \\{1, \\dots, K-2\\}$, the first derivative of the spline must be continuous: $s'_{i-1}(\\tau_i) = s'_i(\\tau_i)$. This adds $K-2$ constraints.\n-   **Continuity of the Second Derivative**: Similarly, at each interior knot $\\tau_i$ for $i \\in \\{1, \\dots, K-2\\}$, the second derivative must be continuous: $s''_{i-1}(\\tau_i) = s''_i(\\tau_i)$. This adds another $K-2$ constraints.\n\nThe total number of constraints from interpolation and continuity is $2(K-1) + (K-2) + (K-2) = 4K-6$. Since there are $4(K-1)$ coefficients to determine, we need two additional constraints. These are supplied by the boundary conditions. For a **natural cubic spline**, the boundary conditions are:\n-   **Natural Boundary Conditions**: The second derivative of the spline is zero at the endpoints of the interval. Given the interval $[0, 1]$ with $\\tau_0=0$ and $\\tau_{K-1}=1$, these conditions are $s_K''(0) = 0$ and $s_K''(1) = 0$.\n\nA standard and computationally stable method to determine the spline coefficients is to first solve for the second derivatives at the knots, denoted by $M_i = s_K''(\\tau_i)$. For a uniform knot sequence, where the spacing between any two consecutive knots is constant, $h = \\tau_{i+1} - \\tau_i = \\frac{1}{K-1}$, the continuity conditions lead to a system of linear equations for the unknown values $M_1, \\dots, M_{K-2}$. The governing equations for $i \\in \\{1, \\dots, K-2\\}$ are:\n$$\nM_{i-1} + 4M_i + M_{i+1} = \\frac{6}{h^2}(y_{i+1} - 2y_i + y_{i-1})\n$$\nThe natural boundary conditions provide $M_0 = 0$ and $M_{K-1} = 0$. Substituting these into the system results in a $(K-2) \\times (K-2)$ tridiagonal system of linear equations for the interior second derivatives $M_1, \\dots, M_{K-2}$. This system is strictly diagonally dominant and thus has a unique solution.\n\nOnce all the values of $M_i$ are known, the cubic polynomial $s_i(x)$ on any subinterval $[\\tau_i, \\tau_{i+1}]$ is uniquely determined by:\n$$\ns_i(x) = M_i \\frac{(\\tau_{i+1}-x)^3}{6h} + M_{i+1} \\frac{(x-\\tau_i)^3}{6h} + \\left(\\frac{y_{i+1}-y_i}{h} - \\frac{h}{6}(M_{i+1}-M_i)\\right)(x-\\tau_i) + \\left(y_i - \\frac{M_i h^2}{6}\\right)\n$$\nThis manual construction can be conveniently and robustly performed using established numerical libraries. The `scipy.interpolate.CubicSpline` class in Python's SciPy library, with the `bc_type='natural'` argument, automates this entire procedure. It takes the knot locations $\\tau_i$ and corresponding function values $y_i=e^{\\tau_i}$ as input and constructs the required spline function.\n\n**2. $L^2$ Approximation Error Calculation**\n\nThe quality of the spline approximation is measured by the $L^2$ error, defined as:\n$$\nE_K = \\left( \\int_{0}^{1} \\big( s_K(x) - f(x) \\big)^2 \\, dx \\right)^{1/2}\n$$\nIn this problem, $f(x) = e^x$. The integrand is $(s_K(x) - e^x)^2$. Since $s_K(x)$ is a piecewise polynomial and $e^x$ is an infinitely differentiable function, the integrand is continuous and well-behaved, making it suitable for high-accuracy numerical integration.\n\nThe integral is evaluated over the entire interval $[0, 1]$. An efficient way to compute this is to use an adaptive quadrature routine, which adjusts its step size to achieve a desired accuracy, concentrating effort in regions where the integrand varies most. The `scipy.integrate.quad` function is an excellent choice for this task, as it provides a robust and accurate implementation of the QUADPACK library's adaptive integration algorithms.\n\n**3. Algorithmic Implementation**\n\nThe overall algorithm for each specified total knot count $K$ is as follows:\n1.  **Generate Knots and Data**: For a given $K$, create the uniform knot sequence $\\tau_i = \\frac{i}{K-1}$ for $i \\in \\{0, 1, \\dots, K-1\\}$ on the interval $[0, 1]$. Compute the corresponding function values $y_i = e^{\\tau_i}$.\n2.  **Construct Spline**: Use `scipy.interpolate.CubicSpline` with the knots $\\tau$, values $y$, and boundary condition type `bc_type='natural'` to create the spline function $s_K(x)$.\n3.  **Define Integrand**: Define a Python function that computes the squared error $(s_K(x) - e^x)^2$ for any given $x \\in [0, 1]$.\n4.  **Numerical Integration**: Use `scipy.integrate.quad` to compute the definite integral of the squared error function from $x=0$ to $x=1$.\n5.  **Compute Error**: Calculate the final $L^2$ error $E_K$ by taking the square root of the integral's result.\n6.  **Format Output**: Format the computed error $E_K$ to eight decimal places as required.\n\nThis procedure is repeated for each value of $K$ in the test suite $\\{2, 3, 5, 11\\}$. The special case $K=2$ results in a linear function, which is correctly handled by the `CubicSpline` constructor with natural boundary conditions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import CubicSpline\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Constructs natural cubic splines for f(x) = e^x and computes their L2 error.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [2, 3, 5, 11]\n\n    results = []\n    for K in test_cases:\n        # Step 1: Generate a uniform knot sequence on [0,1] and evaluate f(x) at these knots.\n        # The number of knots is K. The knots are tau_i = i/(K-1) for i in {0, ..., K-1}.\n        knots = np.linspace(0.0, 1.0, K)\n        \n        # The function to approximate is f(x) = e^x.\n        y_values = np.exp(knots)\n        \n        # Step 2: Construct the natural cubic spline.\n        # The 'bc_type=\"natural\"' argument sets the second derivatives at the endpoints\n        # (x=0 and x=1) to zero, which is the definition of a natural spline.\n        # This function handles the underlying linear algebra (solving the tridiagonal system) internally.\n        natural_spline = CubicSpline(knots, y_values, bc_type='natural')\n        \n        # Step 3: Define the integrand for the L^2 error calculation.\n        # The integrand is the squared difference between the spline and the true function.\n        def squared_error_integrand(x):\n            return (natural_spline(x) - np.exp(x))**2\n            \n        # Step 4: Numerically evaluate the integral of the squared error over [0,1].\n        # scipy.integrate.quad uses adaptive quadrature for high accuracy.\n        # It returns a tuple: (result, estimated_absolute_error). We only need the result.\n        integral_value, _ = quad(squared_error_integrand, 0.0, 1.0)\n        \n        # Step 5: Compute the L^2 error, which is the square root of the integral.\n        l2_error = np.sqrt(integral_value)\n        \n        # Append the result, formatted to exactly eight decimal places.\n        results.append(f\"{l2_error:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While natural splines are celebrated for their stable boundary behavior, this property comes from a specific assumption: the function being modeled should have zero curvature (i.e., a second derivative of zero) at its endpoints. What happens when this assumption is violated? This practice provides a sharp, analytical look at this exact scenario by asking you to approximate a function, $f(x) = x^4$, whose curvature is decidedly non-zero at the boundaries. By calculating the error in the second derivative, you will gain a deeper appreciation for the limitations of the natural boundary condition and the importance of critically evaluating a model's underlying assumptions .",
            "id": "2189207",
            "problem": "Consider the function $f(x) = x^4$. A natural cubic spline, $S(x)$, is constructed to interpolate this function at the nodes $x_0 = 0$, $x_1 = 1$, and $x_2 = 2$. A cubic spline is defined as a piecewise cubic polynomial that is twice continuously differentiable across its nodes. A natural cubic spline additionally satisfies the boundary conditions $S''(x_0) = 0$ and $S''(x_n) = 0$, where $x_0$ and $x_n$ are the first and last nodes, respectively.\n\nThis choice of boundary conditions, while mathematically convenient, can lead to large approximation errors if the true function's second derivative is not zero at the boundaries.\n\nCalculate the absolute error in the approximation of the second derivative at the point $x = 1.8$, defined as $|S''(1.8) - f''(1.8)|$.\n\nRound your final answer to four significant figures.",
            "solution": "The goal is to compute the absolute error $|S''(1.8) - f''(1.8)|$. The process involves three main parts:\n1.  Calculate the true second derivative of the function $f(x)$ at $x=1.8$.\n2.  Determine the second derivative of the natural cubic spline $S(x)$ at $x=1.8$.\n3.  Compute the absolute difference.\n\n**Step 1: Calculate the true second derivative $f''(1.8)$**\n\nThe given function is $f(x) = x^4$. We find its first and second derivatives:\n$$f'(x) = \\frac{d}{dx}(x^4) = 4x^3$$\n$$f''(x) = \\frac{d}{dx}(4x^3) = 12x^2$$\nNow, we evaluate the second derivative at $x = 1.8$:\n$$f''(1.8) = 12(1.8)^2 = 12(3.24) = 38.88$$\n\n**Step 2: Determine the spline's second derivative $S''(1.8)$**\n\nTo find $S''(x)$, we first need to find the values of the second derivative of the spline at the nodes, denoted as $M_i = S''(x_i)$. The nodes are $x_0=0, x_1=1, x_2=2$. The corresponding function values are:\n$$y_0 = f(x_0) = 0^4 = 0$$\n$$y_1 = f(x_1) = 1^4 = 1$$\n$$y_2 = f(x_2) = 2^4 = 16$$\nThe step sizes are $h_i = x_{i+1} - x_i$:\n$$h_0 = x_1 - x_0 = 1 - 0 = 1$$\n$$h_1 = x_2 - x_1 = 2 - 1 = 1$$\n\nThe moments $M_i$ for a cubic spline are related by the following system of equations for the interior nodes (in this case, only $i=1$):\n$$h_{i-1}M_{i-1} + 2(h_{i-1} + h_i)M_i + h_i M_{i+1} = 6 \\left( \\frac{y_{i+1}-y_i}{h_i} - \\frac{y_i-y_{i-1}}{h_{i-1}} \\right)$$\nFor $i=1$:\n$$h_0 M_0 + 2(h_0 + h_1)M_1 + h_1 M_2 = 6 \\left( \\frac{y_2 - y_1}{h_1} - \\frac{y_1 - y_0}{h_0} \\right)$$\nWe are given that the spline is a *natural* cubic spline, which means the second derivatives at the endpoints are zero:\n$$M_0 = S''(x_0) = S''(0) = 0$$\n$$M_2 = S''(x_2) = S''(2) = 0$$\nSubstituting all known values into the equation for $M_1$:\n$$(1)(0) + 2(1 + 1)M_1 + (1)(0) = 6 \\left( \\frac{16 - 1}{1} - \\frac{1 - 0}{1} \\right)$$\n$$4M_1 = 6 (15 - 1)$$\n$$4M_1 = 6 (14)$$\n$$4M_1 = 84$$\n$$M_1 = 21$$\nSo, the moments of the spline are $M_0=0, M_1=21, M_2=0$.\n\nThe second derivative of a cubic spline on the interval $[x_i, x_{i+1}]$ is a linear function that interpolates the moments $M_i$ and $M_{i+1}$:\n$$S_i''(x) = M_i \\frac{x_{i+1} - x}{h_i} + M_{i+1} \\frac{x - x_i}{h_i}$$\nThe point $x=1.8$ lies in the second interval, $[x_1, x_2] = [1, 2]$, so we set $i=1$:\n$$S_1''(x) = M_1 \\frac{x_2 - x}{h_1} + M_2 \\frac{x - x_1}{h_1}$$\nSubstituting the known values $M_1=21, M_2=0, x_1=1, x_2=2, h_1=1$:\n$$S''(x) = 21 \\frac{2 - x}{1} + 0 \\frac{x - 1}{1} = 21(2-x) \\quad \\text{for } x \\in [1, 2]$$\nNow we can evaluate the spline's second derivative at $x=1.8$:\n$$S''(1.8) = 21(2 - 1.8) = 21(0.2) = 4.2$$\n\n**Step 3: Compute the absolute error**\n\nFinally, we calculate the absolute error as defined in the problem statement:\n$$\\text{Error} = |S''(1.8) - f''(1.8)|$$\n$$\\text{Error} = |4.2 - 38.88| = |-34.68| = 34.68$$\nThe problem asks to round the answer to four significant figures. The number $34.68$ already has four significant figures.",
            "answer": "$$\\boxed{34.68}$$"
        },
        {
            "introduction": "The true power of natural splines in statistical learning is revealed when we move from simple interpolation to noisy regression. Global polynomials, while flexible, often exhibit wild oscillations near the boundaries of the data, leading to high variance and poor predictive performance. Natural splines solve this problem by enforcing linear behavior in the tails, which acts as a powerful regularizer. This final practice challenges you to implement and compare both models, demonstrating empirically why the constrained nature of natural splines makes them a superior choice for robust function estimation in the presence of noise .",
            "id": "3153008",
            "problem": "You are given a synthetic regression setting in which the goal is to compare the behavior of a natural cubic spline and a global cubic polynomial fit when approximating an unknown target function. Starting from the basic principles of least squares regression and the definition of cubic splines with natural boundary conditions, implement two regression estimators on the same training data: (i) a global cubic polynomial and (ii) a natural cubic spline with equally spaced knots over the training input range. Then, using numerical integration, compute the integrated squared error for each estimator over a specified evaluation interval that includes the training range and extends into the tails.\n\nUse the following fundamental base:\n- Ordinary least squares regression: given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and responses $y \\in \\mathbb{R}^n$, the least squares estimator $\\hat{\\beta}$ minimizes $\\sum_{i=1}^n (y_i - (X \\hat{\\beta})_i)^2$ and yields $\\hat{f}(x) = x^\\top \\hat{\\beta}$ for a feature vector $x$.\n- Cubic spline regression: a cubic spline is a piecewise cubic function with continuity of first and second derivatives at knot points. Natural cubic splines impose natural boundary conditions, requiring the second derivative to be zero at the boundary knots, which implies linear behavior outside the boundary knot interval.\n- Integrated squared error: for an estimator $\\hat{f}$ and the true function $f$, the integrated squared error over an interval $[A,B]$ is $\\int_A^B \\left(\\hat{f}(x) - f(x)\\right)^2 \\, dx$, which may be approximated numerically by a Riemann sum or trapezoidal rule over a fine grid.\n\nImplement the following, without providing or relying on shortcut formulas in the problem text:\n\n1. Data generation. For each test case, generate training inputs $x_i$ uniformly spaced in $[a,b]$ and outputs $y_i = f(x_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are independent and identically distributed Gaussian noise. Use angles in radians for any trigonometric function.\n2. Estimators.\n   - Global cubic polynomial: use the feature map $x \\mapsto [1, x, x^2, x^3]$ for regression.\n   - Natural cubic spline: use a standard natural cubic spline basis with boundary knots at $a$ and $b$ and $M-2$ internal knots equally spaced in $[a,b]$, producing a $M$-dimensional design with the first two columns representing the linear part and the remaining $M-2$ columns representing the adjusted truncated cubic components that enforce natural boundary conditions.\n3. Evaluation. For each fitted model, approximate the integrated squared error over $[A,B]$ by evaluating the true function $f$ and the fitted function $\\hat{f}$ on a uniform grid of $N_{\\text{eval}}$ points and applying the trapezoidal rule.\n\nTest suite. Implement and run the following four test cases, each specified by $(f, a, b, A, B, n_{\\text{train}}, \\sigma, M, N_{\\text{eval}}, \\text{seed})$:\n- Case $1$ (happy path with moderate tails): $f(x) = \\sin(2x) + 0.3 x$, $a=-3$, $b=3$, $A=-5$, $B=5$, $n_{\\text{train}}=61$, $\\sigma=0.1$, $M=7$, $N_{\\text{eval}}=10001$, $\\text{seed}=1$.\n- Case $2$ (noise-free and linear-dominant tails): $f(x) = x + 0.5 \\sin(x)$, $a=-1$, $b=1$, $A=-4$, $B=4$, $n_{\\text{train}}=41$, $\\sigma=0.0$, $M=5$, $N_{\\text{eval}}=10001$, $\\text{seed}=2$.\n- Case $3$ (curvature plus linear trend, moderate noise): $f(x) = e^{-x^2} + x$, $a=-2$, $b=2$, $A=-3$, $B=3$, $n_{\\text{train}}=81$, $\\sigma=0.05$, $M=6$, $N_{\\text{eval}}=10001$, $\\text{seed}=3$.\n- Case $4$ (edge case with minimal knots and wide tails): $f(x) = \\sin(x)$, $a=-2$, $b=2$, $A=-6$, $B=6$, $n_{\\text{train}}=51$, $\\sigma=0.0$, $M=3$, $N_{\\text{eval}}=10001$, $\\text{seed}=4$.\n\nFor each case, compute three quantities: the integrated squared error of the natural spline $\\text{ISE}_{\\text{ns}}$, the integrated squared error of the cubic polynomial $\\text{ISE}_{\\text{cp}}$, and the difference $\\text{ISE}_{\\text{ns}} - \\text{ISE}_{\\text{cp}}$. Round all reported floats to $6$ decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results as a list of lists, with one sublist per test case in the sequence of cases $1$ through $4$. Each sublist must be of the form $[\\text{ISE}_{\\text{ns}}, \\text{ISE}_{\\text{cp}}, \\text{ISE}_{\\text{ns}} - \\text{ISE}_{\\text{cp}}]$, and all numbers rounded to $6$ decimal places, for example: $[[0.123456,0.234567,-0.111111],[...],...]$.",
            "solution": "The problem requires a comparison of two regression models—a global cubic polynomial and a natural cubic spline—by evaluating their performance in fitting a target function over a specified interval. The performance metric is the Integrated Squared Error (ISE). The analysis proceeds from first principles of ordinary least squares (OLS) regression and the mathematical definition of the basis functions for each model.\n\nThe foundation for both models is OLS regression. Given a set of $n_{\\text{train}}$ training pairs $(x_i, y_i)$, we aim to find a function $\\hat{f}$ that minimizes the residual sum of squares, $\\text{RSS}(\\beta) = \\sum_{i=1}^{n_{\\text{train}}} (y_i - \\hat{f}(x_i))^2$. For linear models of the form $\\hat{f}(x) = \\sum_{j=1}^{p} \\beta_j h_j(x)$, where $\\{h_j(x)\\}_{j=1}^p$ is a set of basis functions, this is equivalent to solving for the coefficient vector $\\hat{\\beta} \\in \\mathbb{R}^p$ that minimizes $\\|y - X\\beta\\|^2$. Here, $y = [y_1, \\dots, y_{n_{\\text{train}}}]^\\top$ is the vector of responses, and $X$ is the $n_{\\text{train}} \\times p$ design matrix with entries $X_{ij} = h_j(x_i)$. The well-known solution is given by $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$, which is numerically best computed using methods like Singular Value Decomposition, as implemented in standard linear algebra libraries (`lstsq`).\n\nThe first model is a **global cubic polynomial**, where the function is assumed to have the form $\\hat{f}_{\\text{cp}}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$. The basis functions are $\\{h_1(x), h_2(x), h_3(x), h_4(x)\\} = \\{1, x, x^2, x^3\\}$. The corresponding design matrix, $X_{\\text{cp}}$, is a $n_{\\text{train}} \\times 4$ matrix where the $i$-th row is $[1, x_i, x_i^2, x_i^3]$.\n\nThe second model is a **natural cubic spline**. A cubic spline is a piecewise cubic polynomial that is continuous and has continuous first and second derivatives at a series of points called knots. A natural cubic spline imposes an additional constraint: the function must be linear beyond the boundary knots. This is enforced by requiring the second derivative to be zero at the minimum and maximum knots. For a set of $M$ knots $\\xi_1 < \\xi_2 < \\dots < \\xi_M$, where $\\xi_1$ and $\\xi_M$ are the boundary knots, a standard $M$-dimensional basis for the space of natural cubic splines is given by:\n$N_1(x) = 1$\n$N_2(x) = x$\n$N_{j+2}(x) = d_j(x) - d_{M-1}(x)$ for $j = 1, \\dots, M-2$.\nThe function $d_j(x)$ is defined as:\n$$d_j(x) = \\frac{(x - \\xi_j)_+^3 - (x - \\xi_M)_+^3}{\\xi_M - \\xi_j}$$\nwhere $(u)_+ = u$ if $u > 0$ and $0$ otherwise. This choice of basis functions explicitly ensures that any linear combination of them is linear for $x < \\xi_1$ and $x > \\xi_M$. For each test case, we construct a set of $M$ knots equally spaced over the training interval $[a,b]$, such that $\\xi_1=a$ and $\\xi_M=b$. The corresponding $n_{\\text{train}} \\times M$ design matrix, $X_{\\text{ns}}$, is constructed with the $i$-th row being $[N_1(x_i), N_2(x_i), \\dots, N_M(x_i)]$.\n\nOnce both models are fitted to the training data $(x_i, y_i = f(x_i) + \\mathcal{N}(0, \\sigma^2))$ to obtain the estimators $\\hat{f}_{\\text{cp}}(x)$ and $\\hat{f}_{\\text{ns}}(x)$, we evaluate their performance. The metric is the Integrated Squared Error (ISE) over an evaluation interval $[A,B]$:\n$$\\text{ISE}(\\hat{f}) = \\int_A^B (\\hat{f}(x) - f(x))^2 \\, dx$$\nThis integral is approximated numerically using the trapezoidal rule. A fine, uniform grid of $N_{\\text{eval}}$ points is created spanning the interval $[A,B]$. Let these points be $z_k$ for $k=1, \\dots, N_{\\text{eval}}$, with a constant spacing of $\\Delta z = (B-A)/(N_{\\text{eval}}-1)$. The ISE is then approximated as:\n$$\\text{ISE}(\\hat{f}) \\approx \\sum_{k=1}^{N_{\\text{eval}}-1} \\frac{(\\hat{f}(z_k) - f(z_k))^2 + (\\hat{f}(z_{k+1}) - f(z_{k+1}))^2}{2} \\Delta z$$\nThis procedure is applied to calculate $\\text{ISE}_{\\text{cp}}$ for the cubic polynomial and $\\text{ISE}_{\\text{ns}}$ for the natural spline, providing a quantitative basis for comparing their behavior, particularly in the extrapolation regions outside the training interval $[a,b]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares global cubic polynomial and natural cubic spline\n    regression models based on their integrated squared error.\n    \"\"\"\n\n    # Define the true functions for the test cases.\n    # Angles are in radians.\n    functions = {\n        1: lambda x: np.sin(2 * x) + 0.3 * x,\n        2: lambda x: x + 0.5 * np.sin(x),\n        3: lambda x: np.exp(-x**2) + x,\n        4: lambda x: np.sin(x),\n    }\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, -3, 3, -5, 5, 61, 0.1, 7, 10001, 1),\n        (2, -1, 1, -4, 4, 41, 0.0, 5, 10001, 2),\n        (3, -2, 2, -3, 3, 81, 0.05, 6, 10001, 3),\n        (4, -2, 2, -6, 6, 51, 0.0, 3, 10001, 4),\n    ]\n\n    def build_natural_spline_basis(x, knots):\n        \"\"\"\n        Constructs the design matrix for a natural cubic spline.\n        The basis is derived from \"The Elements of Statistical Learning\".\n        \"\"\"\n        M = len(knots)\n        x = np.asarray(x)\n        basis = np.zeros((len(x), M))\n        \n        basis[:, 0] = 1.0\n        if M > 1:\n            basis[:, 1] = x\n\n        if M > 2:\n            def d_func(x_vec, knot_k, knot_M):\n                # Implements the d_k(x) function for the spline basis\n                term1 = np.maximum(0, x_vec - knot_k)**3\n                term2 = np.maximum(0, x_vec - knot_M)**3\n                return (term1 - term2) / (knot_M - knot_k)\n\n            knot_M_val = knots[-1]\n            d_M_minus_1_vec = d_func(x, knots[M-2], knot_M_val)\n            \n            for j in range(1, M - 1):\n                knot_j = knots[j-1]\n                d_j_vec = d_func(x, knot_j, knot_M_val)\n                basis[:, j + 1] = d_j_vec - d_M_minus_1_vec\n                \n        return basis\n\n    results = []\n    for case in test_cases:\n        f_id, a, b, A, B, n_train, sigma, M, N_eval, seed = case\n        f = functions[f_id]\n\n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        x_train = np.linspace(a, b, n_train)\n        noise = rng.normal(0, sigma, size=n_train)\n        y_train = f(x_train) + noise\n\n        # 2. Estimators\n        # Global Cubic Polynomial\n        X_cp_train = np.vander(x_train, 4, increasing=True)\n        beta_cp = np.linalg.lstsq(X_cp_train, y_train, rcond=None)[0]\n\n        def f_hat_cp(x_eval):\n            X_cp_eval = np.vander(x_eval, 4, increasing=True)\n            return X_cp_eval @ beta_cp\n\n        # Natural Cubic Spline\n        knots = np.linspace(a, b, M)\n        X_ns_train = build_natural_spline_basis(x_train, knots)\n        beta_ns = np.linalg.lstsq(X_ns_train, y_train, rcond=None)[0]\n        \n        def f_hat_ns(x_eval):\n            X_ns_eval = build_natural_spline_basis(x_eval, knots)\n            return X_ns_eval @ beta_ns\n\n        # 3. Evaluation\n        x_eval = np.linspace(A, B, N_eval)\n        y_true_eval = f(x_eval)\n\n        # ISE for Cubic Polynomial\n        y_hat_cp_eval = f_hat_cp(x_eval)\n        squared_error_cp = (y_hat_cp_eval - y_true_eval)**2\n        ise_cp = np.trapz(squared_error_cp, x_eval)\n\n        # ISE for Natural Spline\n        y_hat_ns_eval = f_hat_ns(x_eval)\n        squared_error_ns = (y_hat_ns_eval - y_true_eval)**2\n        ise_ns = np.trapz(squared_error_ns, x_eval)\n\n        # Store results rounded to 6 decimal places\n        diff = ise_ns - ise_cp\n        results.append([round(ise_ns, 6), round(ise_cp, 6), round(diff, 6)])\n\n    # Final print statement in the exact required format.\n    case_strs = []\n    for res in results:\n        num_strs = [f\"{val:.6f}\" for val in res]\n        case_strs.append(f\"[{','.join(num_strs)}]\")\n    print(f\"[{','.join(case_strs)}]\")\n\nsolve()\n```"
        }
    ]
}