{
    "hands_on_practices": [
        {
            "introduction": "多元局部回归是局部加权最小二乘法的一个直接扩展，它在更高维度的数据探索中非常有用。然而，与所有基于最小二乘法的方法一样，当局部邻域中的预测变量高度相关（即共线性）时，其性能可能会严重下降。本实践将通过解析计算的方式，让您亲手量化这种共線性对估计器方差的影响，并探索如何利用Tikhonov正则化技术来稳定模型 。",
            "id": "3141257",
            "problem": "您需要实现一个完全独立的程序，比较多元预测变量共线性下的局部估计散点图平滑（LOESS），分析病态局部设计矩阵如何影响拟合值的方差，并评估 Tikhonov 正则化的稳定效应。该程序的基础必须构建于加权最小二乘法原理和标准线性模型方差传播之上，从局部线性回归的公式化以及使用紧支集核函数以关注邻近数据的作用开始。\n\n考虑一个数据集，包含 $n$ 个二元预测变量 $\\{(x_{1i}, x_{2i})\\}_{i=1}^n$ 和响应 $\\{y_i\\}_{i=1}^n$，这些数据由一个平滑的潜在函数加上方差为 $\\sigma^2$ 的加性同方差噪声生成。局部线性 LOESS 估计量在目标点 $(t_1, t_2)$ 进行评估。在局部线性 LOESS 中，局部模型使用一个截距和在目标点中心化的预测变量，估计过程通过求解局部加权最小二乘法正规方程进行。当预测变量在目标点的邻域内高度共线性时，局部设计矩阵会变得病态，从而放大了拟合值的方差。Tikhonov 正则化通过向局部正规矩阵添加一个单位矩阵的倍数来减轻病态性并控制方差膨胀。\n\n您的程序必须：\n- 使用一个紧支集、单调递减的核权重函数，以矩阵形式构建局部线性 LOESS 估计量。该函数为邻域半径外的点分配零权重，并为靠近 $(t_1, t_2)$ 的点分配更大的权重；请使用标准的三次立方核函数。\n- 将预测变量在 $(t_1, t_2)$ 处中心化，使得在 $(t_1, t_2)$ 处的拟合值等于估计的截距。\n- 在方差为 $\\sigma^2$ 的同方差噪声下，计算在 $(t_1, t_2)$ 处拟合值的解析方差，使用从线性模型和指定的核权重获得的加权最小二乘估计量的协方差。\n- 通过向局部正规矩阵添加一个惩罚参数 $\\lambda \\ge 0$ 来引入 Tikhonov 正则化，并在此修改下重新计算拟合值的方差。\n\n对于测试用例，数据集的生成必须通过固定的随机种子来确保确定性。使用 $n$ 等于 $200$。对于共线性场景，令 $x_{1i}$ 为从 $[0,1]$ 上的均匀分布中独立抽取的值，并设置 $x_{2i} = x_{1i} + \\epsilon_i$，其中 $\\epsilon_i$ 从均值为 $0$、标准差指定的正态分布中抽取；对于独立场景，从 $[0,1]$ 上的均匀分布中独立抽取 $x_{2i}$。潜在信号 $g(x_1, x_2)$ 可以是任何平滑函数；在所述假设下，拟合值的解析方差取决于设计和权重，而不取决于响应。使用三次立方核函数根据到 $(t_1, t_2)$ 的欧几里得距离和一个跨度分数 $f \\in (0,1)$ 来计算局部权重，该分数表示用于定义邻域半径的最近邻的比例。所有计算必须以纯数学术语表示，不带任何物理单位。\n\n计算在 $(t_1, t_2)$ 处拟合值的方差，针对以下测试套件，每个套件由一个参数元组 $(\\text{scenario}, \\epsilon\\text{ std}, f, \\lambda, n, \\text{seed}, (t_1,t_2), \\sigma^2)$ 指定：\n-   **案例 1** (一般共线性，非正则化): $(\\text{collinear}, 0.02, 0.25, 0, 200, 0, (0.5, 0.5), 0.0025)$\n-   **案例 2** (一般共线性，轻度正则化): $(\\text{collinear}, 0.02, 0.25, 10^{-3}, 200, 0, (0.5, 0.5), 0.0025)$\n-   **案例 3** (近乎完全共线性，非正则化，更小跨度): $(\\text{collinear}, 10^{-4}, 0.15, 0, 200, 1, (0.5, 0.5), 0.0025)$\n-   **案例 4** (近乎完全共线性，更强正则化): $(\\text{collinear}, 10^{-4}, 0.15, 10^{-2}, 200, 1, (0.5, 0.5), 0.0025)$\n-   **案例 5** (独立预测变量，基线): $(\\text{independent}, 0, 0.25, 0, 200, 2, (0.5, 0.5), 0.0025)$\n\n您的程序应生成单行输出，其中包含按上述案例顺序排列的结果，形式为用方括号括起来的逗号分隔列表，每个条目是该案例在 $(t_1, t_2)$ 处计算出的拟合值方差（一个浮点数），例如 `\"[v_1,v_2,v_3,v_4,v_5]\"`。",
            "solution": "该问题要求实现和分析一个局部估计散点图平滑（LOESS）估计量，重点关注预测变量共线性和 Tikhonov 正则化如何影响拟合值的方差。该分析将通过在标准线性模型假设下推导 LOESS 估计量的解析方差来进行。\n\n### **理论基础：局部加权最小二乘法**\n\nLOESS 是一种非参数回归方法，它将一个简单模型拟合到数据的局部子集。对于给定的目标点 $t = (t_1, t_2)$，我们用一个局部线性模型（一个切平面）来近似潜在函数 $g(x_1, x_2)$。我们使用在 $t$ 点中心化的预测变量，以确保估计的截距直接对应于在 $t$ 点的拟合值。对于数据点 $(x_{1i}, x_{2i})$，局部线性模型为：\n$$\ng(x_{1i}, x_{2i}) \\approx \\beta_0 + \\beta_1 (x_{1i} - t_1) + \\beta_2 (x_{2i} - t_2)\n$$\n在目标点 $t$ 的 LOESS 估计定义为 $\\hat{g}(t) = \\hat{\\beta}_0$。系数 $\\beta = [\\beta_0, \\beta_1, \\beta_2]^T$ 通过求解一个加权最小二乘（WLS）问题来估计。目标是最小化所有 $n$ 个数据点的加权残差平方和：\n$$\n\\min_{\\beta} \\sum_{i=1}^{n} w_i(t) \\left( y_i - \\left( \\beta_0 + \\beta_1 (x_{1i} - t_1) + \\beta_2 (x_{2i} - t_2) \\right) \\right)^2\n$$\n权重 $w_i(t)$ 由一个核函数确定，该函数为更靠近 $t$ 的点赋予更大的权重，并为定义邻域之外的点赋予零权重。\n\n### **矩阵表示法和估计量方差**\n\nWLS 问题可以用矩阵形式表示。令 $Y$ 为观测响应 $\\{y_i\\}$ 的 $n \\times 1$ 向量，$X_t$ 为 $n \\times 3$ 的局部设计矩阵，$W_t$ 为 $n \\times n$ 的权重对角矩阵。$X_t$ 的第 $i$ 行是 $[1, x_{1i} - t_1, x_{2i} - t_2]$，$W_t$ 的第 $i$ 个对角元素是 $w_i(t)$。最小化问题等价于求解正规方程：\n$$\n(X_t^T W_t X_t) \\hat{\\beta} = X_t^T W_t Y\n$$\n$\\beta$ 的 WLS 估计量为：\n$$\n\\hat{\\beta} = (X_t^T W_t X_t)^{-1} X_t^T W_t Y\n$$\n在同方差噪声的假设下，其中响应 $Y$ 的真实均值为 $g(X)$，协方差为 $\\text{Cov}(Y) = \\sigma^2 I_n$，估计量 $\\hat{\\beta}$ 的协方差矩阵由下式给出：\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X_t^T W_t X_t)^{-1} (X_t^T W_t^2 X_t) (X_t^T W_t X_t)^{-1}\n$$\n其中 $W_t^2$ 是一个对角矩阵，其元素为 $w_i(t)^2$。在 $t$ 点的拟合值为 $\\hat{g}(t) = \\hat{\\beta}_0 = e_1^T \\hat{\\beta}$，其中 $e_1 = [1, 0, 0]^T$。因此，拟合值的方差为：\n$$\n\\text{Var}(\\hat{g}(t)) = e_1^T \\text{Cov}(\\hat{\\beta}) e_1\n$$\n这对应于 $\\text{Cov}(\\hat{\\beta})$ 矩阵的左上角元素 $(1,1)$。在 $t$ 的邻域内，预测变量 $(x_{1i} - t_1)$ 和 $(x_{2i} - t_2)$ 之间的高度共线性使得“局部正规矩阵” $X_t^T W_t X_t$ 近乎奇异，这会夸大其逆矩阵的元素，从而灾难性地增加了估计量的方差。\n\n### **Tikhonov 正则化**\n\n为了在存在共线性的情况下稳定解，我们引入 Tikhonov 正则化。目标函数被修改为包含一个关于系数大小的惩罚项：\n$$\n\\min_{\\beta} \\left\\{ (Y - X_t \\beta)^T W_t (Y - X_t \\beta) + \\lambda \\beta^T \\beta \\right\\}\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数。正则化的正规方程为：\n$$\n(X_t^T W_t X_t + \\lambda I_3) \\hat{\\beta}_{\\lambda} = X_t^T W_t Y\n$$\n其中 $I_3$ 是 $3 \\times 3$ 的单位矩阵。正则化估计量 $\\hat{\\beta}_{\\lambda}$ 为：\n$$\n\\hat{\\beta}_{\\lambda} = (X_t^T W_t X_t + \\lambda I_3)^{-1} X_t^T W_t Y\n$$\n添加项 $\\lambda I_3$ 确保了待求逆的矩阵是良态的，即使 $X_t^T W_t X_t$ 不是。遵循与之前相同的推导，正则化估计量的协方差矩阵为：\n$$\n\\text{Cov}(\\hat{\\beta}_{\\lambda}) = \\sigma^2 (X_t^T W_t X_t + \\lambda I_3)^{-1} (X_t^T W_t^2 X_t) (X_t^T W_t X_t + \\lambda I_3)^{-1}\n$$\n正则化拟合值 $\\hat{g}_{\\lambda}(t)$ 的方差是该矩阵的 $(1,1)$ 元素。注意，非正则化情况是 $\\lambda=0$ 的一个特例。\n\n### **核权重与计算过程**\n\n权重 $w_i(t)$ 使用三次立方核函数计算。首先，我们在 $t$ 周围定义一个邻域，该邻域包含总数据点中比例为 $f$ 的点。\n1.  对所有点 $i=1, \\dots, n$，计算欧几里得距离 $d_i = \\|(x_{1i}, x_{2i}) - t\\|_2$。\n2.  邻域大小为 $q = \\lceil f \\cdot n \\rceil$。邻域半径 $h$ 是到 $t$ 的第 $q$ 个最近邻的距离。\n3.  对所有点的距离进行缩放：$u_i = d_i / h$。\n4.  对于邻域内的点（$u_i  1$），三次立方权重被赋值为 $w_i(t) = (1 - u_i^3)^3$，对于邻域外的点（$u_i \\ge 1$），权重为 $w_i(t) = 0$。\n\n每个测试用例的计算过程包括：生成预测变量数据 $(x_{1i}, x_{2i})$，使用指定的跨度 $f$ 和目标点 $t$ 计算权重向量，构建矩阵 $X_t^T W_t X_t$ 和 $X_t^T W_t^2 X_t$，最后使用给定的正则化参数 $\\lambda$ 和噪声方差 $\\sigma^2$ 的相应公式计算方差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_loess_variance(scenario, eps_std, f, lamb, n, seed, t, sigma2):\n    \"\"\"\n    Computes the analytic variance of the LOESS fitted value at a target point t.\n\n    Args:\n        scenario (str): 'collinear' or 'independent' for data generation.\n        eps_std (float): Standard deviation for collinearity noise.\n        f (float): Span fraction for the LOESS kernel.\n        lamb (float): Tikhonov regularization parameter.\n        n (int): Number of data points.\n        seed (int): Seed for the random number generator.\n        t (tuple): The target point (t1, t2) for the LOESS fit.\n        sigma2 (float): Variance of the homoscedastic noise in the response.\n\n    Returns:\n        float: The computed variance of the fitted value at t.\n    \"\"\"\n    # 1. Generate predictor data deterministically.\n    rng = np.random.default_rng(seed)\n    x1 = rng.uniform(0, 1, n)\n    if scenario == 'collinear':\n        epsilon = rng.normal(0, eps_std, n)\n        x2 = x1 + epsilon\n    elif scenario == 'independent':\n        x2 = rng.uniform(0, 1, n)\n    else:\n        raise ValueError(f\"Unknown scenario: {scenario}\")\n    \n    X_data = np.stack([x1, x2], axis=1)\n    \n    # 2. Calculate tri-cube kernel weights.\n    t_vec = np.array(t)\n    distances = np.linalg.norm(X_data - t_vec, axis=1)\n    \n    # Determine the neighborhood radius h based on the span f.\n    # The neighborhood includes q = ceil(f*n) points.\n    q = int(np.ceil(f * n))\n    if q = 0 or q > n:\n        # Invalid span, this case should not happen with problem constraints f in (0,1).\n        return np.nan\n\n    # h is the distance to the q-th nearest neighbor (using 0-based index q-1).\n    h = np.sort(distances)[q - 1]\n\n    # Handle the case where the radius is zero to avoid division by zero.\n    if h == 0.0:\n        u = np.full_like(distances, 2.0)  # Weights will be 0\n        u[distances == 0.0] = 0.0        # Points at t get full weight\n    else:\n        u = distances / h\n    \n    # Apply the tri-cube kernel function.\n    weights = np.zeros(n)\n    mask = u  1.0\n    weights[mask] = (1 - u[mask]**3)**3\n\n    # 3. Construct local design matrix and other required matrices.\n    X_centered = X_data - t_vec\n    Xt = np.hstack([np.ones((n, 1)), X_centered])\n    p = Xt.shape[1] # Number of parameters (intercept, slope1, slope2) is 3\n\n    # Efficiently compute Xt.T @ W @ Xt and Xt.T @ W^2 @ Xt\n    # where W is a diagonal matrix of weights.\n    M_normal = (Xt.T * weights) @ Xt\n    M_squared_weights = (Xt.T * (weights**2)) @ Xt\n\n    # 4. Apply Tikhonov regularization.\n    M_lambda = M_normal + lamb * np.eye(p)\n\n    # 5. Compute the variance of the fitted value.\n    try:\n        inv_M_lambda = np.linalg.inv(M_lambda)\n    except np.linalg.LinAlgError:\n        # If the matrix is singular even after regularization (e.g., lambda=0)\n        return np.inf\n\n    # Variance matrix for the coefficient estimator beta_hat\n    cov_beta_hat_unscaled = inv_M_lambda @ M_squared_weights @ inv_M_lambda\n    \n    # The variance of the fitted value (the intercept) is the top-left element.\n    var_beta0_unscaled = cov_beta_hat_unscaled[0, 0]\n    \n    # Scale by the noise variance sigma^2.\n    final_variance = sigma2 * var_beta0_unscaled\n    \n    return final_variance\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (scenario, eps_std, f, lambda, n, seed, t, sigma2)\n        ('collinear', 0.02, 0.25, 0.0, 200, 0, (0.5, 0.5), 0.0025),\n        ('collinear', 0.02, 0.25, 1e-3, 200, 0, (0.5, 0.5), 0.0025),\n        ('collinear', 1e-4, 0.15, 0.0, 200, 1, (0.5, 0.5), 0.0025),\n        ('collinear', 1e-4, 0.15, 1e-2, 200, 1, (0.5, 0.5), 0.0025),\n        ('independent', 0.0, 0.25, 0.0, 200, 2, (0.5, 0.5), 0.0025),\n    ]\n\n    results = []\n    for case in test_cases:\n        scenario, eps_std, f, lamb, n, seed, t, sigma2 = case\n        variance = calculate_loess_variance(scenario, eps_std, f, lamb, n, seed, t, sigma2)\n        results.append(variance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{v:.8f}' for v in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "局部回归的一个关键优势是其稳健性，即抵抗数据中少数异常值（离群点）干扰的能力，这使其成为探索性数据分析的强大工具。为了实现这一目标，标准LOESS算法采用了一种迭代重加权方案，自动降低具有大残差的数据点的影响。在本练习中，您将实现这种稳健的LOESS估计器，并系统地分析其在面对高杠杆率离群点时的“崩溃点” 。",
            "id": "3141335",
            "problem": "本任务要求实现一个完整的程序，该程序构建一个迭代重加权局部回归估计量，称为局部估计散点图平滑（LOcally Estimated Scatterplot Smoothing, LOESS），并分析其在预测变量中存在高杠杆离群值污染下的稳健性。该程序必须是自包含的，且不应需要任何外部输入。算法应基于公认的原理构建：加权最小二乘法和稳健尺度估计，问题陈述中不会提供快捷公式。\n\n核心任务是在评估点上执行阶数为 $d$ 的局部多项式回归，其邻域由一个范围在 $(0,1]$ 内的跨度参数 $\\alpha$ 决定。邻域权重必须平滑地依赖于预测变量中的距离，稳健性迭代必须通过乘性重加权残差来减小响应变量中极端偏差的影响。稳健性重加权必须使用一个降权函数，该函数与从残差分布中导出的一个尺度相关联。该算法必须执行指定的迭代次数，并从均匀的残差权重开始。\n\n您将根据以下规则构建一个数据集：\n- 设 $n$ 为观测总数，其中未受污染的预测变量值 $x$ 从 $[0,1]$ 上的均匀分布中独立抽样，并对应于由真实函数 $f(x)$ 加上加性噪声生成的响应 $y$。\n- 真实函数为 $f(x) = \\sin(2\\pi x)$，其中 $x$ 在 $[0,1]$ 区间内。\n- 噪声是独立同分布的，每个观测值都受到一个均值为零、标准差为 $\\sigma$ 的高斯噪声的扰动。\n- 污染比例 $p$（在 $[0,1]$ 内）表示被高杠杆离群值替换的观测值比例：这些离群值的预测变量值位于定义域的两端（在两端均衡分布），其响应值则向相反方向偏移一个大的固定振幅 $A$，以构成对抗性样本。\n- 污染过程会替换干净的数据点，以保持总样本量 $n$ 不变。\n\n稳健性分析通过在一个位于 $[0,1]$ 内的密集评估网格上，比较 LOESS 估计量与真实函数 $f(x)$ 的预测质量来进行。对于每个污染比例 $p$，计算 LOESS 在该网格上的预测相对于 $f(x)$ 的均方根误差 (RMSE)。如果该 RMSE 与无污染情况下的 RMSE 之比超过阈值 $\\tau$，则定义为在 $p$ 处发生崩溃。\n\n您的程序必须实现该算法，并对以下固定参数值（即测试套件）产生结果：\n- 总样本量 $n = 200$。\n- 多项式阶数 $d = 1$（局部线性）。\n- 跨度参数 $\\alpha = 0.8$。\n- 噪声标准差 $\\sigma = 0.1$。\n- 离群值振幅 $A = 8.0$。\n- 初始拟合后，稳健性迭代次数为 $2$。\n- 阈值 $\\tau = 3.0$。\n- 评估网格：$[0,1]$ 内 $200$ 个等距点。\n- 污染比例 $p \\in \\{0.0, 0.1, 0.3, 0.5\\}$。\n\n程序必须使用固定的随机种子以确保确定性行为。您必须为每个 $p$ 值计算一个布尔值，以指示在该 $p$ 值下是否发生崩溃。此外，还需计算给定集合中导致崩溃的最小污染比例，如果没有任何比例导致崩溃，则该值为 $1.0$。\n\n最终输出格式规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。\n- 该列表应包含一系列布尔值（每个污染比例对应一个，顺序相同），其后是一个浮点数，给出发生崩溃的最小污染比例，四舍五入到小数点后 $3$ 位。\n- 例如，如果最后两个污染比例导致崩溃，且其中最小的比例是 $0.3$，则输出应为形如 `\"[False,True,True,True,0.300]\"` 的一行。",
            "solution": "该问题要求实现并分析一种稳健的局部回归算法，具体是 LOESS (局部估计散点图平滑)，也称为局部加权回归。分析内容涉及评估该算法在数据被高杠杆离群值污染时的崩溃特性。\n\n该问题具有充分的科学依据，算法上具体明确，并且是定量定义的。它基于非参数统计和稳健估计的既定原则。所有参数和过程都已指定，使得该问题是适定的，并允许在给定随机源的情况下得到唯一、可验证的解。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n解决方案的结构如下：\n1.  数据生成过程的详细描述，包括离群值的引入。\n2.  迭代重加权 LOESS 算法的逐步阐述。\n3.  性能评估和崩溃分析的方法论。\n\n**1. 数据生成**\n\n数据集包含 $n = 200$ 个观测值。其中比例为 $p$ 的部分是离群值，而其余的 $n(1-p)$ 个是“干净”的数据点。\n\n干净的数据由以下模型生成：\n$$ y_i = f(x_i) + \\epsilon_i $$\n其中，真实的基础函数是 $f(x) = \\sin(2\\pi x)$，对于 $x \\in [0, 1]$。预测变量 $x_i$ 从均匀分布 $x_i \\sim U[0, 1]$ 中抽取。噪声项 $\\epsilon_i$ 是来自均值为零、标准差为 $\\sigma = 0.1$ 的正态分布的独立同分布样本，即 $\\epsilon_i \\sim N(0, \\sigma^2)$。\n\n一定数量的点，即 $n_{out} = \\text{round}(n \\times p)$，被指定为离群值。这些离群值被构造成高杠杆和对抗性的。离群值的预测变量值被放置在定义域的两端，$x=0$ 和 $x=1$，以最大化它们对拟合的影响（杠杆作用）。这些点的放置是均衡的，有 $\\lfloor n_{out}/2 \\rfloor$ 个离群值在 $x=0$ 处，$\\lceil n_{out}/2 \\rceil$ 个在 $x=1$ 处。它们对应的响应值从真实函数值偏移了一个很大的振幅 $A=8.0$。由于 $f(0)=f(1)=0$，位于 $x=0$ 的离群值响应设为 $y=+A$，位于 $x=1$ 的设为 $y=-A$。这些离群值会替换掉等量随机选择的干净数据点，以保持总样本量 $n$ 不变。\n\n**2. 迭代重加权 LOESS 算法**\n\nLOESS 是一种非参数方法，它通过执行一系列局部加权回归来对数据拟合一条平滑曲线。其核心思想是，对于任何评估点 $x_0$，拟合结果由 $x_0$ 周围一小部分数据点的邻域决定。离 $x_0$ 越近的点获得越大的权重。其稳健性是通过迭代地降低那些与前一次拟合相比具有较大残差的点的权重来实现的。\n\n算法按以下步骤进行，共进行 $1+k_{robust}$ 次迭代，其中 $k_{robust}=2$ 是稳健性重加权步骤的次数。\n\n**步骤 2.1：初始化**\n最初，所有点都被认为是同等可靠的。所有数据点 $(x_i, y_i)$ 的稳健性权重 $\\delta_i$ 初始化为 1：\n$$ \\delta_i^{(0)} = 1 \\quad \\text{for } i = 1, \\dots, n $$\n\n**步骤 2.2：局部回归拟合（每次迭代）**\n对于评估点集中的每个点 $x_0$，都会对数据进行局部多项式拟合。在本问题中，多项式阶数为 $d=1$（局部线性拟合）。\n\n**2.2.1. 邻域选择：**\n定义一个邻域 $\\mathcal{N}(x_0)$，它由 $q$ 个训练点 $(x_i, y_i)$ 组成，这些点的 $x_i$ 值最接近 $x_0$。邻域的大小 $q$ 由跨度参数 $\\alpha = 0.8$ 决定：\n$$ q = \\lceil \\alpha n \\rceil = \\lceil 0.8 \\times 200 \\rceil = 160 $$\n设 $\\Delta(x_0)$ 为 $x_0$ 到其在所有 $x_i$ 中第 $q$ 个最近邻的距离。这个距离定义了邻域的半宽度。\n\n**2.2.2. 邻域加权：**\n邻域 $\\mathcal{N}(x_0)$ 中的每个点 $x_i$ 根据其到 $x_0$ 的距离被赋予一个邻域权重 $w_i(x_0)$。使用标准的三次立方权重函数：\n$$ w_i(x_0) = W\\left(\\frac{|x_i - x_0|}{\\Delta(x_0)}\\right) $$\n其中当 $|u|  1$ 时，$W(u) = (1 - |u|^3)^3$，否则 $W(u) = 0$。该函数给予位于 $x_0$ 处的点最高权重，并向邻域边缘平滑地将权重减小到零。\n\n**2.2.3. 加权最小二乘法 (WLS)：**\n在每个评估点 $x_0$ 处，我们对邻域 $\\mathcal{N}(x_0)$ 内的数据拟合一个局部线性模型 $g(x) = \\beta_0 + \\beta_1 (x - x_0)$。通过最小化加权残差平方和来求解系数 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)^T$：\n$$ \\min_{\\beta_0, \\beta_1} \\sum_{i \\in \\mathcal{N}(x_0)} v_i(x_0) [y_i - (\\beta_0 + \\beta_1(x_i - x_0))]^2 $$\n在当前迭代中，点 $i$ 的总权重 $v_i(x_0)$ 是其邻域权重 $w_i(x_0)$ 和其当前稳健性权重 $\\delta_i$ 的乘积：\n$$ v_i(x_0) = w_i(x_0) \\delta_i $$\n这个 WLS 问题可以使用正规方程组求解：$(\\mathbf{X}^T \\mathbf{V} \\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{V} \\mathbf{y}$，其中 $\\mathbf{X}$ 是设计矩阵，其行向量为 $(1, x_i - x_0)$，$\\mathbf{y}$ 是响应向量，$\\mathbf{V}$ 是由总权重 $v_i(x_0)$ 构成的对角矩阵。\n那么，在 $x_0$ 处的 LOESS 拟合值为 $\\hat{y}(x_0) = \\hat{\\beta}_0$。\n\n**步骤 2.3：稳健性重加权**\n在初始拟合（迭代 0）之后，以及在随后的每次稳健性迭代中，都会更新稳健性权重 $\\delta_i$。正是这一步骤使得算法对响应变量 $y$ 中的离群值具有稳健性。\n\n**2.3.1. 残差计算：**\n首先，我们计算所有训练点的残差 $r_i = y_i - \\hat{y}(x_i)$，其中 $\\hat{y}(x_i)$ 是使用当前迭代的拟合结果，在训练点 $x_i$ 本身处的 LOESS 预测值。\n\n**2.3.2. 稳健尺度估计：**\n使用中位数绝对偏差 (MAD) 对残差的离散程度进行稳健估计：\n$$ s = \\frac{1}{C} \\cdot \\text{median}_{i} |r_i| $$\n常数 $C = \\Phi^{-1}(0.75) \\approx 0.6745$，其中 $\\Phi^{-1}$ 是标准正态分布的分位数函数。如果残差服从正态分布，该常数使得 $s$ 成为标准差 $\\sigma$ 的一个近似无偏估计量。\n\n**2.3.3. 稳健性权重更新：**\n下一次迭代的新的稳健性权重 $\\delta_i$ 使用双平方（或双权重）函数计算，这是一种降权的 M 估计量：\n$$ \\delta_i \\leftarrow B\\left(\\frac{r_i}{6s}\\right) $$\n其中当 $|u|  1$ 时，$B(u) = (1 - u^2)^2$，否则 $B(u)=0$。因子 $6$ 是一个平衡效率和稳健性的调节常数。残差大于 $6s$ 的点将被赋予零权重，从而有效地将它们从下一次拟合迭代中移除。\n\n此过程（步骤 2.2 和 2.3）重复指定的稳健性迭代次数（$k_{robust}=2$）。最终的 LOESS 拟合在最后一次迭代中获得。\n\n**3. 性能评估和崩溃分析**\n\nLOESS 估计量的稳健性是通过在一个由 $[0, 1]$ 内 $N_{grid}=200$ 个等距点组成的精细评估网格上，计算其相对于真实函数 $f(x)$ 的均方根误差 (RMSE) 来评估的。\n对于给定的污染比例 $p$，其 RMSE 为：\n$$ \\text{RMSE}_p = \\sqrt{\\frac{1}{N_{grid}} \\sum_{j=1}^{N_{grid}} (\\hat{y}_{p}(x_{eval, j}) - f(x_{eval, j}))^2} $$\n其中 $\\hat{y}_{p}$ 是在污染比例为 $p$ 下的最终 LOESS 估计。\n\n如果在某个污染水平 $p$ 下的 RMSE 显著高于无污染（$p=0.0$）时的基线 RMSE，则定义为发生崩溃。其条件是：\n$$ \\frac{\\text{RMSE}_p}{\\text{RMSE}_{0.0}} > \\tau $$\n其中阈值 $\\tau=3.0$。\n\n程序会为每个指定的污染比例 $p \\in \\{0.0, 0.1, 0.3, 0.5\\}$ 计算此值。最终输出包含：针对每个 $p$ 的一个布尔值，指示是否发生崩溃；以及在该集合中观测到崩溃的最小 $p$ 值。如果对于所有测试的比例都没有发生崩溃，则报告该值为 $1.0$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the robustness of an iteratively reweighted LOESS estimator.\n    \"\"\"\n    \n    # --- Fixed Parameters from Problem Statement ---\n    N = 200\n    DEGREE = 1\n    ALPHA = 0.8\n    SIGMA = 0.1\n    OUTLIER_AMPLITUDE = 8.0\n    ROBUST_ITERS = 2\n    BREAKDOWN_THRESHOLD = 3.0\n    N_GRID = 200\n    P_VALUES = [0.0, 0.1, 0.3, 0.5]\n    SEED = 42\n\n    # --- Helper Functions for LOESS ---\n\n    def tricube_weight(u):\n        \"\"\"Tricube weight function for neighborhood weights.\"\"\"\n        u = np.abs(u)\n        return np.where(u  1, (1 - u**3)**3, 0)\n\n    def bisquare_weight(u):\n        \"\"\"Bisquare weight function for robustness weights.\"\"\"\n        u = np.abs(u)\n        return np.where(u  1, (1 - u**2)**2, 0)\n\n    def loess_fit(eval_points, x_data, y_data, robust_weights, alpha, degree):\n        \"\"\"\n        Performs the LOESS fit for a given set of evaluation points.\n        \"\"\"\n        n_data = len(x_data)\n        q = int(np.ceil(alpha * n_data))\n        \n        y_pred = np.zeros(len(eval_points))\n\n        for i, x0 in enumerate(eval_points):\n            dists = np.abs(x_data - x0)\n            \n            # Find neighborhood\n            sorted_indices = np.argsort(dists)\n            neighborhood_indices = sorted_indices[:q]\n            \n            # Neighborhood radius\n            h = dists[sorted_indices[q-1]]\n            \n            if h == 0.0:\n                # All points in neighborhood are at x0. Take weighted average.\n                mask = dists == 0\n                r_w = robust_weights[mask]\n                y_at_x0 = y_data[mask]\n                if np.sum(r_w) > 0:\n                    y_pred[i] = np.sum(r_w * y_at_x0) / np.sum(r_w)\n                else:\n                    y_pred[i] = np.mean(y_at_x0) # Fallback if all weights are zero\n                continue\n\n            # Data for local regression\n            x_hood = x_data[neighborhood_indices]\n            y_hood = y_data[neighborhood_indices]\n            \n            # Neighborhood weights (tricube)\n            scaled_dists = dists[neighborhood_indices] / h\n            neighborhood_w = tricube_weight(scaled_dists)\n            \n            # Total weights (neighborhood * robustness)\n            robust_w_hood = robust_weights[neighborhood_indices]\n            total_w = neighborhood_w * robust_w_hood\n            \n            # Weighted Least Squares\n            X = np.ones((q, degree + 1))\n            for d in range(1, degree + 1):\n                X[:, d] = (x_hood - x0)**d\n            \n            W = np.diag(total_w)\n            \n            A_matrix = X.T @ W @ X\n            b_vector = X.T @ W @ y_hood\n            \n            try:\n                beta = np.linalg.solve(A_matrix, b_vector)\n                y_pred[i] = beta[0]\n            except np.linalg.LinAlgError:\n                # Fallback to weighted average if matrix is singular\n                if np.sum(total_w) > 0:\n                    y_pred[i] = np.sum(total_w * y_hood) / np.sum(total_w)\n                else: \n                    # If all weights are 0, use unweighted neighborhood mean\n                    y_pred[i] = np.mean(y_hood)\n                    \n        return y_pred\n\n    def robust_loess_pipeline(x_data, y_data, eval_points, alpha, degree, n_robust_iters):\n        \"\"\"\n        Runs the full iteratively reweighted LOESS pipeline.\n        \"\"\"\n        n_data = len(x_data)\n        robust_weights = np.ones(n_data)\n        \n        # Mad constant for normal distribution\n        mad_c = norm.ppf(0.75) # Approximately 0.6745\n\n        # Total iterations = 1 (initial) + n_robust_iters\n        for k in range(n_robust_iters + 1):\n            if k > 0: # Update robust weights for iterations 1, 2, ...\n                # Predict at training points to get residuals\n                y_hat_train = loess_fit(x_data, x_data, y_data, robust_weights, alpha, degree)\n                residuals = y_data - y_hat_train\n                \n                # Robust scale estimation (MAD)\n                s = np.median(np.abs(residuals)) / mad_c\n                if s == 0:\n                    # If MAD is 0, no re-weighting unless there are non-zero residuals\n                    if np.any(residuals != 0):\n                        # Use a small non-zero scale to avoid division by zero\n                        s = np.mean(np.abs(residuals)) / mad_c\n                    else: # All residuals are zero, no need to re-weight\n                        break\n\n                # Bisquare robustness weights\n                scaled_residuals = residuals / (6 * s)\n                robust_weights = bisquare_weight(scaled_residuals)\n        \n        # Final fit on the evaluation grid using final weights\n        y_hat_eval = loess_fit(eval_points, x_data, y_data, robust_weights, alpha, degree)\n        return y_hat_eval\n\n    # --- Main Analysis Logic ---\n    rng = np.random.default_rng(SEED)\n\n    def f_true(x):\n        return np.sin(2 * np.pi * x)\n\n    def generate_data(p):\n        n_clean = int(N * (1-p))\n        n_outliers = N - n_clean\n\n        # Generate base clean data\n        x_clean_full = rng.uniform(0, 1, N)\n        y_clean_full = f_true(x_clean_full) + rng.normal(0, SIGMA, N)\n        \n        # Select a subset of clean points\n        clean_indices = rng.choice(N, size=n_clean, replace=False)\n        x_final = x_clean_full[clean_indices]\n        y_final = y_clean_full[clean_indices]\n\n        if n_outliers > 0:\n            # Generate outliers\n            n_out_x0 = n_outliers // 2\n            n_out_x1 = n_outliers - n_out_x0\n            \n            x_outliers = np.concatenate([np.zeros(n_out_x0), np.ones(n_out_x1)])\n            y_outliers = np.concatenate([\n                np.full(n_out_x0, OUTLIER_AMPLITUDE),\n                np.full(n_out_x1, -OUTLIER_AMPLITUDE)\n            ])\n            \n            # Combine clean and outlier data\n            x_final = np.concatenate([x_final, x_outliers])\n            y_final = np.concatenate([y_final, y_outliers])\n            \n        return x_final, y_final\n\n    eval_grid = np.linspace(0, 1, N_GRID)\n    y_true_grid = f_true(eval_grid)\n    \n    rmses = []\n    for p in P_VALUES:\n        x, y = generate_data(p)\n        y_hat = robust_loess_pipeline(x, y, eval_grid, ALPHA, DEGREE, ROBUST_ITERS)\n        \n        rmse = np.sqrt(np.mean((y_hat - y_true_grid)**2))\n        rmses.append(rmse)\n\n    # --- Breakdown Analysis ---\n    rmse0 = rmses[0]\n    breakdowns = [r / rmse0 > BREAKDOWN_THRESHOLD for r in rmses]\n    \n    breakdown_p = 1.0\n    for i, p in enumerate(P_VALUES):\n        if breakdowns[i]:\n            breakdown_p = p\n            break\n            \n    # Format and print the final result\n    result_list = [str(b) for b in breakdowns]\n    result_list.append(f\"{breakdown_p:.3f}\")\n    \n    print(f\"[{','.join(result_list)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "除了作为一种平滑技术，LOESS的跨度（span）参数使其能够充当一个可调的“低通滤波器”，让我们能够从不同尺度上审视数据。通过在一系列递增的跨度上迭代应用LOESS，我们可以将一个复杂的信号分解为不同频率的成分，类似于傅里叶分析或小波分解的思想。这个练习将引导您实现LOESS的多尺度分解，从而对跨度参数的角色建立起深刻的直觉 。",
            "id": "3141280",
            "problem": "实现一种多尺度局部估计散点图平滑（LOESS）分解，该分解基于加权最小二乘和局部多项式近似的基本原理。目标是在逐渐增大的跨度上构建一系列局部加权线性拟合，并解释每个尺度下的残差。从定义开始：在位置 $x_0$ 处的局部回归估计是通过最小化加权平方误差和得到的，其中权重随与 $x_0$ 的距离而减小。使用一个紧支撑、单调递减的核函数来定义局部性。满足这些性质的一个标准选择是三立方核函数。对于每个跨度参数 $ \\alpha \\in (0,1] $，邻域大小为 $k = \\lceil \\alpha n \\rceil$，其中 $n$ 是观测点的数量，权重在超出第 $k$ 个最近邻的距离后为零。\n\n设数据为 $ \\{(x_i, y_i)\\}_{i=1}^n $，其中 $x_i$ 严格递增，$y_i$ 为实数值。对于给定的跨度 $ \\alpha $，定义 LOESS 算子 $L_{\\alpha}$ 如下：对于每个位置 $x_0$，根据绝对距离 $|x_i - x_0|$ 选择 $k = \\lceil \\alpha n \\rceil$ 个最近的点。令 $d_{\\max}$ 为这 $k$ 个邻点中的最大距离。对每个 $i$，定义权重\n$$\nw_i(x_0) = \\begin{cases}\n\\left(1 - \\left(\\frac{|x_i - x_0|}{d_{\\max}}\\right)^3\\right)^3,  \\text{if } |x_i - x_0| \\le d_{\\max}, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n通过最小化以下表达式来获得在 $x_0$ 处的局部线性拟合\n$$\n\\sum_{i=1}^n w_i(x_0) \\left( y_i - \\beta_0(x_0) - \\beta_1(x_0)\\,(x_i - x_0) \\right)^2\n$$\n关于 $ \\beta_0(x_0) $ 和 $ \\beta_1(x_0) $。在 $x_0$ 处的 LOESS 估计是 $ \\hat{y}(x_0) = \\beta_0(x_0) $。\n\n使用一个严格递增的跨度序列 $ \\alpha_1  \\alpha_2  \\cdots  \\alpha_m $ 来构建多尺度分解。通过迭代定义残差和分量\n$$\nr_0 = y, \\quad c_s = L_{\\alpha_s}(r_{s-1}), \\quad r_s = r_{s-1} - c_s, \\quad s = 1,2,\\dots,m.\n$$\n这将得到分解\n$$\ny = \\sum_{s=1}^m c_s + r_m,\n$$\n其中 $c_s$ 表示在尺度 $s$ 捕获的分量，$r_m$ 是最终的余项。\n\n所有三角函数的参数都必须解释为弧度。在所有涉及随机性的计算中，使用固定的伪随机种子 $42$ 以确保可复现性。\n\n**测试套件与任务：**\n- 对于每个测试用例，根据指定的参数生成合成数据，并应用上述多尺度 LOESS 分解。然后计算所需的度量指标，并根据下面定义的阈值返回布尔结果。\n\n- **测试用例 1 (一般情况):**\n  - 参数: $n = 201$, $x_i$ 在 $[0,1]$ 上均匀分布，跨度 $(\\alpha_1,\\alpha_2,\\alpha_3) = (0.1,0.3,0.6)$。\n  - 信号分量:\n    $$\n    g_3(x) = 0.8 \\sin(2\\pi x), \\quad g_2(x) = 0.4 \\sin(10\\pi x), \\quad g_1(x) = 0.2 \\sin(30\\pi x)\n    $$\n    $$\n    y(x) = g_3(x) + g_2(x) + g_1(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2), \\quad \\sigma = 0.05\n    $$\n  - 分解: 计算 $c_1, c_2, c_3$ 和 $r_3$。\n  - 度量指标: 计算 $c_1$ 和 $g_1$ 之间、$c_2$ 和 $g_2$ 之间以及 $c_3$ 和 $g_3$ 之间的绝对皮尔逊相关系数。\n  - 阈值与布尔值:\n    $$\n    b_{11} = (|\\mathrm{corr}(c_1,g_1)| \\ge 0.8), \\quad\n    b_{12} = (|\\mathrm{corr}(c_2,g_2)| \\ge 0.9), \\quad\n    b_{13} = (|\\mathrm{corr}(c_3,g_3)| \\ge 0.95)\n    $$\n\n- **测试用例 2 (边界条件: 较小的 $n$ 和较高的噪声):**\n  - 参数: $n = 61$, $x_i$ 在 $[0,1]$ 上均匀分布，跨度 $(\\alpha_1,\\alpha_2,\\alpha_3) = (0.15,0.4,0.7)$。\n  - 信号分量:\n    $$\n    g_3(x) = 0.7 \\sin(2\\pi x), \\quad g_2(x) = 0.5 \\sin(8\\pi x), \\quad g_1(x) = 0.3 \\sin(24\\pi x)\n    $$\n    $$\n    y(x) = g_3(x) + g_2(x) + g_1(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2), \\quad \\sigma = 0.10\n    $$\n  - 分解: 计算 $c_1, c_2, c_3$ 和 $r_3$。\n  - 度量指标: 计算 $c_1$ 和 $g_1$ 之间、$c_2$ 和 $g_2$ 之间以及 $c_3$ 和 $g_3$ 之间的绝对皮尔逊相关系数。\n  - 阈值与布尔值:\n    $$\n    b_{21} = (|\\mathrm{corr}(c_1,g_1)| \\ge 0.6), \\quad\n    b_{22} = (|\\mathrm{corr}(c_2,g_2)| \\ge 0.8), \\quad\n    b_{23} = (|\\mathrm{corr}(c_3,g_3)| \\ge 0.9)\n    $$\n\n- **测试用例 3 (边缘情况: 主导趋势，无中、细尺度分量):**\n  - 参数: $n = 121$, $x_i$ 在 $[0,1]$ 上均匀分布，跨度 $(\\alpha_1,\\alpha_2,\\alpha_3) = (0.1,0.3,0.6)$。\n  - 信号分量:\n    $$\n    g_3(x) = 0.9 \\cos(1.5\\pi x), \\quad g_2(x) = 0, \\quad g_1(x) = 0\n    $$\n    $$\n    y(x) = g_3(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2), \\quad \\sigma = 0.05\n    $$\n  - 分解: 计算 $c_1, c_2, c_3$ 和 $r_3$。\n  - 度量指标: 令 $\\mathrm{Var}(z)$ 表示样本的经验方差。计算 $v_y = \\mathrm{Var}(y)$, $v_{c_3} = \\mathrm{Var}(c_3)$ 和 $v_{12} = \\mathrm{Var}(c_1) + \\mathrm{Var}(c_2)$。同时计算 $|\\mathrm{corr}(c_3,g_3)|$。\n  - 阈值与布尔值:\n    $$\n    b_{31} = \\left(\\frac{v_{c_3}}{v_y} \\ge 0.7\\right), \\quad\n    b_{32} = \\left(\\frac{v_{12}}{v_y} \\le 0.2\\right), \\quad\n    b_{33} = (|\\mathrm{corr}(c_3,g_3)| \\ge 0.9)\n    $$\n\n**最终输出格式:**\n您的程序应生成单行输出，其中包含所有测试用例的布尔结果，聚合为一个用方括号括起来的逗号分隔列表。顺序必须是\n$$\n[b_{11}, b_{12}, b_{13}, b_{21}, b_{22}, b_{23}, b_{31}, b_{32}, b_{33}].\n$$\n所有布尔值必须是编程语言原生的布尔字面量格式。",
            "solution": "用户要求实现一个多尺度局部估计散点图平滑（LOESS）分解。我们将验证此过程，然后遵循指定的约束条件开发一个完整的解决方案。\n\n### 步骤 1：提取已知条件\n\n-   **数据**：一组 $n$ 个观测值 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $x_i$ 严格递增，$y_i$ 为实数值。\n-   **LOESS 算子定义**：\n    -   **跨度参数**：$\\alpha \\in (0,1]$。\n    -   **邻域大小**：$k = \\lceil \\alpha n \\rceil$。\n    -   **邻域**：对于每个点 $x_0$，具有最小距离 $|x_i - x_0|$ 的 $k$ 个点。\n    -   **最大距离**：$d_{\\max}$ 是邻域内最大的距离 $|x_i - x_0|$。\n    -   **权重函数 (三立方核函数)**：对于每个点 $x_i$ 相对于目标点 $x_0$：\n        $$\n        w_i(x_0) = \\begin{cases}\n        \\left(1 - \\left(\\frac{|x_i - x_0|}{d_{\\max}}\\right)^3\\right)^3,  \\text{if } |x_i - x_0| \\le d_{\\max}, \\\\\n        0,  \\text{otherwise}.\n        \\end{cases}\n        $$\n    -   **局部多项式拟合**：在每个 $x_0$ 处，找到系数 $\\beta_0(x_0)$ 和 $\\beta_1(x_0)$，以最小化加权平方误差和：\n        $$\n        \\sum_{i=1}^n w_i(x_0) \\left( y_i - \\beta_0(x_0) - \\beta_1(x_0)\\,(x_i - x_0) \\right)^2\n        $$\n    -   **LOESS 估计**：在 $x_0$ 处的估计值为 $\\hat{y}(x_0) = \\beta_0(x_0)$。该算子表示为 $L_{\\alpha}$。\n-   **多尺度分解**：\n    -   **跨度**：一个严格递增的序列 $\\alpha_1  \\alpha_2  \\cdots  \\alpha_m$。\n    -   **迭代过程**：对于 $s = 1, 2, \\dots, m$：\n        -   **初始残差**：$r_0 = y$。\n        -   **分量**：$c_s = L_{\\alpha_s}(r_{s-1})$。\n        -   **下一个残差**：$r_s = r_{s-1} - c_s$。\n    -   **分解公式**：$y = \\sum_{s=1}^m c_s + r_m$。\n-   **可复现性**：使用固定的伪随机种子 $42$。\n-   **测试用例**：提供了三个具体的测试用例，包含 $n$ 的参数、$x_i$ 的分布、$\\alpha$ 值、信号分量 $g_s(x)$ 和噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。每个用例都定义了需要评估的度量指标（皮尔逊相关系数、方差比）和布尔阈值 ($b_{ij}$)。\n-   **最终输出**：一个单行字符串，表示布尔结果的列表：$[b_{11}, b_{12}, b_{13}, b_{21}, b_{22}, b_{23}, b_{31}, b_{32}, b_{33}]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n1.  **科学性**：该问题描述了 LOESS，这是一种由 Cleveland (1979) 开发的、成熟的非参数回归方法。局部多项式回归、加权最小二乘法和三立方核函数的使用都是该方法的标准组成部分。多尺度分解是一种有效的统计技术，用于在不同分辨率下分析时间序列或其他序列数据，概念上类似于小波分解或季节性-趋势分解（STL）。\n2.  **适定性**：问题的核心是为局部线性拟合求解一个加权最小二乘（WLS）问题。对于一个给定的点 $x_0$，只要设计矩阵是满秩的，WLS 问题就有唯一解。在 $x_0$ 处的局部线性拟合的设计矩阵包含常数项（$1$）和线性项（$x_i - x_0$）的列。由于问题规定 $x_i$ 是严格递增的，任何大小为 $k \\ge 2$ 的邻域都将包含具有不同 $x$ 值的点。这确保了设计矩阵不会秩亏，并且 $(\\beta_0, \\beta_1)$ 的唯一解存在。所有测试用例使用的参数都会导致 $k \\ge 10$，因此问题是适定的。\n3.  **客观性**：所有的定义、参数和任务都以数学和算法的精度进行了规定。信号生成、噪声模型和评估指标（相关性、方差）都是客观且可量化的。\n4.  **完整性**：问题提供了所有必要的信息：LOESS 平滑器的数学公式、迭代分解的结构以及带有所有必需参数和评估标准的完整测试用例。没有缺少必要信息。\n5.  **一致性**：已知条件中没有矛盾。三立方核函数的定义与其在 LOESS 中的使用是一致的。分解过程在逻辑上是合理的。测试用例旨在有意义地探究分解的行为（即，较小的跨度捕获较高频率的分量）。\n\n### 步骤 3：结论与行动\n\n该问题具有科学性、适定性、客观性、完整性和内部一致性。它是计算统计学中一个标准的数值实现任务。该问题是**有效的**。将提供一个解决方案。\n\n### 求解推导\n\n解决方案需要实现 LOESS 算子 $L_{\\alpha}$，然后迭代地应用它。\n\n#### LOESS 平滑器: $L_{\\alpha}$\n对于给定的数据集 $(x, y)$ 和跨度 $\\alpha$，平滑后的向量 $\\hat{y} = L_{\\alpha}(y)$ 是通过对数据集中的每个点 $x_j$ (其中 $j=1, \\dots, n$) 应用局部回归过程来计算的。\n\n1.  **邻域选择**：在每个目标点 $x_j$ 处，我们必须从集合 $\\{x_i\\}_{i=1}^n$ 中确定其 $k = \\lceil \\alpha n \\rceil$ 个最近邻。这通过计算所有 $i$ 的距离 $|x_i - x_j|$ 并选择与最小距离相对应的 $k$ 个点来完成。\n\n2.  **权重计算**：为邻域中的点计算三立方权重。设邻域索引集为 $\\mathcal{N}_j$。此邻域中的最大距离为 $d_{\\max,j} = \\max_{i \\in \\mathcal{N}_j} |x_i - x_j|$。然后为 $i \\in \\mathcal{N}_j$ 计算权重 $w_i(x_j) = (1 - (|x_i-x_j|/d_{\\max,j})^3)^3$。所有其他权重为零。\n\n3.  **加权最小二乘 (WLS)**：我们求解最小化目标函数的系数 $\\beta_0(x_j)$ 和 $\\beta_1(x_j)$。这是一个 WLS 问题。让上划线表示限制在邻域 $\\mathcal{N}_j$ 内的向量/矩阵。模型是 $\\bar{y} \\approx \\beta_0 + \\beta_1(\\bar{x} - x_j)$。设计矩阵是 $\\mathbf{X}_j = \\begin{pmatrix} 1  \\bar{x}_1 - x_j \\\\ \\vdots  \\vdots \\\\ 1  \\bar{x}_k - x_j \\end{pmatrix}$。令 $\\mathbf{W}_j$ 为一个对角矩阵，其对角线上是权重 $w_i(x_j)$。$\\boldsymbol{\\beta}_j = [\\beta_0(x_j), \\beta_1(x_j)]^T$ 的 WLS 解由下式给出：\n    $$\n    \\boldsymbol{\\beta}_j = (\\mathbf{X}_j^T \\mathbf{W}_j \\mathbf{X}_j)^{-1} \\mathbf{X}_j^T \\mathbf{W}_j \\bar{y}\n    $$\n    在 $x_j$ 处的 LOESS 估计是这个局部拟合的截距，$\\hat{y}_j = \\beta_0(x_j)$。\n\n#### 多尺度分解\n这是一个对 LOESS 平滑器的直接迭代应用。从初始数据作为第一个残差开始，$r_0 = y$：\n-   第一个分量，捕捉最细尺度，是 $c_1 = L_{\\alpha_1}(r_0)$。\n-   更新残差：$r_1 = r_0 - c_1$。\n-   通过平滑新的残差找到第二个分量：$c_2 = L_{\\alpha_2}(r_1)$。\n-   再次更新残差：$r_2 = r_1 - c_2$。\n-   这个过程对所有 $m$ 个跨度继续进行。\n\n最终的分解是 $y = c_1 + c_2 + \\dots + c_m + r_m$，其中 $r_m$ 是最终的残差。\n\n#### 测试用例与度量指标\n实现将遵循此逻辑为每个测试用例生成数据，应用分解，并计算所需的度量指标。对于相关性，将使用皮尔逊相关系数。对于方差，将根据“经验方差”的含义使用样本方差（$ddof=1$）。计算出的度量指标将与给定的阈值进行比较，以生成最终的布尔值。",
            "answer": "```python\nimport numpy as np\n\ndef loess_smoother(x, y, alpha):\n    \"\"\"\n    Computes the LOESS smoothed values for a given 1D data set.\n\n    Args:\n        x (np.ndarray): The independent variable values, must be sorted.\n        y (np.ndarray): The dependent variable values.\n        alpha (float): The span parameter, determining the neighborhood size.\n\n    Returns:\n        np.ndarray: The smoothed y-values.\n    \"\"\"\n    n = len(x)\n    k = int(np.ceil(alpha * n))\n    \n    # In all test cases, k >= 10, so this check is not strictly necessary but good practice.\n    if k  2:\n        raise ValueError(\"Span alpha is too small, resulting in k  2 neighbors.\")\n\n    y_hat = np.empty(n)\n\n    for j in range(n):\n        x0 = x[j]\n\n        # 1. Find k-nearest neighbors and max distance\n        distances = np.abs(x - x0)\n        neighbor_indices = np.argsort(distances)[:k]\n        d_max = distances[neighbor_indices[-1]]\n\n        # 2. Calculate tri-cube weights. Handle d_max=0 as a safeguard.\n        if d_max == 0:\n            y_hat[j] = np.mean(y[neighbor_indices])\n            continue\n\n        u = distances[neighbor_indices] / d_max\n        weights = (1 - u**3)**3\n\n        # 3. Perform weighted least squares\n        x_neigh = x[neighbor_indices]\n        y_neigh = y[neighbor_indices]\n        \n        # Design matrix is centered at x0\n        X_neigh = np.vstack([np.ones(k), x_neigh - x0]).T\n\n        # Efficiently compute (X.T * W * X) and (X.T * W * y)\n        X_w = X_neigh * weights[:, np.newaxis]\n        A = X_w.T @ X_neigh\n        b = X_w.T @ y_neigh\n        \n        try:\n            # Solve for beta = [beta0, beta1]\n            beta = np.linalg.solve(A, b)\n            # The fitted value at x0 is the intercept term\n            y_hat[j] = beta[0]\n        except np.linalg.LinAlgError:\n            # Fallback to local weighted average if matrix is singular (numerically)\n            y_hat[j] = np.sum(weights * y_neigh) / np.sum(weights)\n\n    return y_hat\n\ndef multi_scale_loess(x, y, alphas):\n    \"\"\"\n    Performs a multi-scale LOESS decomposition.\n\n    Args:\n        x (np.ndarray): The independent variable values.\n        y (np.ndarray): The dependent variable values.\n        alphas (tuple or list): A sequence of increasing span parameters.\n\n    Returns:\n        tuple: A tuple containing the list of components and the final residual.\n    \"\"\"\n    residuals = y.copy()\n    components = []\n    for alpha in alphas:\n        c = loess_smoother(x, residuals, alpha)\n        components.append(c)\n        residuals -= c\n    return components, residuals\n\ndef pearson_corr(u, v):\n    \"\"\"\n    Computes the Pearson correlation coefficient between two vectors.\n    Handles constant vectors by returning 0.0.\n    \"\"\"\n    if np.std(u) == 0 or np.std(v) == 0:\n        return 0.0\n    return np.corrcoef(u, v)[0, 1]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    np.random.seed(42)\n    final_results = []\n\n    # --- Test Case 1 ---\n    n1 = 201\n    x1 = np.linspace(0, 1, n1)\n    alphas1 = (0.1, 0.3, 0.6)\n    sigma1 = 0.05\n    \n    g1_high_freq = 0.2 * np.sin(30 * np.pi * x1)\n    g1_med_freq = 0.4 * np.sin(10 * np.pi * x1)\n    g1_low_freq = 0.8 * np.sin(2 * np.pi * x1)\n\n    eps1 = np.random.normal(0, sigma1, n1)\n    y1 = g1_high_freq + g1_med_freq + g1_low_freq + eps1\n    \n    components1, _ = multi_scale_loess(x1, y1, alphas1)\n    c1, c2, c3 = components1\n    \n    corr1 = np.abs(pearson_corr(c1, g1_high_freq))\n    corr2 = np.abs(pearson_corr(c2, g1_med_freq))\n    corr3 = np.abs(pearson_corr(c3, g1_low_freq))\n\n    b11 = corr1 >= 0.8\n    b12 = corr2 >= 0.9\n    b13 = corr3 >= 0.95\n    final_results.extend([b11, b12, b13])\n\n    # --- Test Case 2 ---\n    n2 = 61\n    x2 = np.linspace(0, 1, n2)\n    alphas2 = (0.15, 0.4, 0.7)\n    sigma2 = 0.10\n    \n    g2_high_freq = 0.3 * np.sin(24 * np.pi * x2)\n    g2_med_freq = 0.5 * np.sin(8 * np.pi * x2)\n    g2_low_freq = 0.7 * np.sin(2 * np.pi * x2)\n\n    eps2 = np.random.normal(0, sigma2, n2)\n    y2 = g2_high_freq + g2_med_freq + g2_low_freq + eps2\n    \n    components2, _ = multi_scale_loess(x2, y2, alphas2)\n    c1, c2, c3 = components2\n    \n    corr1 = np.abs(pearson_corr(c1, g2_high_freq))\n    corr2 = np.abs(pearson_corr(c2, g2_med_freq))\n    corr3 = np.abs(pearson_corr(c3, g2_low_freq))\n    \n    b21 = corr1 >= 0.6\n    b22 = corr2 >= 0.8\n    b23 = corr3 >= 0.9\n    final_results.extend([b21, b22, b23])\n\n    # --- Test Case 3 ---\n    n3 = 121\n    x3 = np.linspace(0, 1, n3)\n    alphas3 = (0.1, 0.3, 0.6)\n    sigma3 = 0.05\n    \n    g3_low_freq = 0.9 * np.cos(1.5 * np.pi * x3)\n    \n    eps3 = np.random.normal(0, sigma3, n3)\n    y3 = g3_low_freq + eps3\n\n    components3, _ = multi_scale_loess(x3, y3, alphas3)\n    c1, c2, c3 = components3\n    \n    # Use ddof=1 for sample variance as per \"empirical variance\"\n    v_y = np.var(y3, ddof=1)\n    v_c3 = np.var(c3, ddof=1)\n    v_12 = np.var(c1, ddof=1) + np.var(c2, ddof=1)\n    corr3 = np.abs(pearson_corr(c3, g3_low_freq))\n    \n    # Avoid division by zero if variance is zero\n    if v_y > 0:\n        b31 = (v_c3 / v_y) >= 0.7\n        b32 = (v_12 / v_y) = 0.2\n    else: # Should not happen with the given data generation\n        b31, b32 = False, False\n\n    b33 = corr3 >= 0.9\n    final_results.extend([b31, b32, b33])\n    \n    # Format the final output string\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}