{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our theoretical discussion in a practical text classification scenario. You will calculate the Information Gain ($IG$) for two different features—one common, one rare—to see firsthand how this measure quantifies a feature's predictive power. This exercise is designed to not only build your computational skills but also to introduce a critical concept in machine learning: how a high $IG$ on training data, especially from a rare feature, can be a sign of overfitting. ",
            "id": "3131370",
            "problem": "A binary text classification task has two classes, labeled $y \\in \\{+1,-1\\}$. The training set contains $N=20$ documents, with $N_{+}=10$ positive and $N_{-}=10$ negative. Consider two candidate binary features corresponding to the presence of two words in a document: the rare word “zephyr” and the more common word “team.” The observed document-level co-occurrence counts in the training set are as follows:\n- For “zephyr”: among the $n_{z}=4$ documents containing the word, $4$ are positive and $0$ are negative; among the $N-n_{z}=16$ documents not containing the word, $6$ are positive and $10$ are negative.\n- For “team”: among the $n_{t}=10$ documents containing the word, $6$ are positive and $4$ are negative; among the $N-n_{t}=10$ documents not containing the word, $4$ are positive and $6$ are negative.\n\nUsing only standard definitions from information theory and decision tree splitting in statistical learning, first compute the information gain (IG) for splitting on the presence of “zephyr,” then compute the information gain for splitting on the presence of “team,” both measured in bits (that is, base $2$ logarithms). Argue, based on these calculations, how the rare word can appear to be a strong splitter on the training data and why this may reflect overfitting in a text setting.\n\nTo mitigate overfitting, impose a minimum support constraint requiring any chosen split word to appear in at least $s=5$ training documents. Under this constraint, determine the best available split among the two candidates and compute its information gain in bits. Report as your final answer the information gain of the best available split under the minimum support constraint $s=5$, rounded to $4$ significant figures.",
            "solution": "The problem requires us to compute and compare the information gain (IG) for two features, \"zephyr\" and \"team,\" and then re-evaluate the choice based on a minimum support constraint. The information gain is defined as $IG(Y; X) = H(Y) - H(Y|X)$, where $H$ is the Shannon entropy measured in bits (using $\\log_2$).\n\n**Step 1: Calculate the entropy of the parent node.**\nThe entire dataset has $N=20$ documents, with $N_+=10$ positive and $N_-=10$ negative. The class proportions are $p_+ = p_- = 0.5$. The entropy of the parent node is therefore maximal:\n$$ H(\\text{Parent}) = -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 1 \\text{ bit} $$\n\n**Step 2: Calculate the Information Gain for the \"zephyr\" feature.**\nThis feature splits the data into two children: those containing \"zephyr\" and those not.\n- **Child 1 (has \"zephyr\"):** Contains $n_z=4$ documents, all of which are positive (4+, 0-). This node is pure, so its entropy is $H(\\text{has 'zephyr'}) = 0$.\n- **Child 2 (no \"zephyr\"):** Contains $16$ documents (6+, 10-). The proportions are $p_+ = 6/16 = 3/8$ and $p_- = 10/16 = 5/8$. Its entropy is:\n$$ H(\\text{no 'zephyr'}) = -\\left(\\frac{3}{8}\\log_2\\frac{3}{8} + \\frac{5}{8}\\log_2\\frac{5}{8}\\right) \\approx 0.9544 \\text{ bits} $$\nThe weighted average entropy of the children (the conditional entropy) is:\n$$ H(Y|\\text{'zephyr'}) = \\frac{4}{20} H(\\text{has 'zephyr'}) + \\frac{16}{20} H(\\text{no 'zephyr'}) = 0.2 \\times 0 + 0.8 \\times 0.9544 \\approx 0.7635 \\text{ bits} $$\nThe information gain is the reduction in entropy:\n$$ IG(\\text{'zephyr'}) = H(\\text{Parent}) - H(Y|\\text{'zephyr'}) \\approx 1 - 0.7635 = 0.2365 \\text{ bits} $$\n\n**Step 3: Calculate the Information Gain for the \"team\" feature.**\n- **Child 1 (has \"team\"):** Contains $n_t=10$ documents (6+, 4-). The proportions are $p_+=0.6, p_-=0.4$. Its entropy is:\n$$ H(\\text{has 'team'}) = - (0.6 \\log_2(0.6) + 0.4 \\log_2(0.4)) \\approx 0.9710 \\text{ bits} $$\n- **Child 2 (no \"team\"):** Contains $10$ documents (4+, 6-). This is symmetric to the first child, so its entropy is also $H(\\text{no 'team'}) \\approx 0.9710 \\text{ bits}$.\nThe weighted average entropy of the children is:\n$$ H(Y|\\text{'team'}) = \\frac{10}{20} (0.9710) + \\frac{10}{20} (0.9710) \\approx 0.9710 \\text{ bits} $$\nThe information gain is:\n$$ IG(\\text{'team'}) = H(\\text{Parent}) - H(Y|\\text{'team'}) \\approx 1 - 0.9710 = 0.0290 \\text{ bits} $$\n\n**Step 4: Analyze results and overfitting.**\nWithout constraints, $IG(\\text{'zephyr'}) \\approx 0.2365$ is much higher than $IG(\\text{'team'}) \\approx 0.0290$. The \"zephyr\" feature creates a perfectly pure node, making it appear to be a very powerful predictor. However, this perfect split is based on only 4 instances. This is a classic indicator of potential overfitting, where the model learns a spurious correlation present in the small training sample that is unlikely to generalize to new data.\n\n**Step 5: Apply the minimum support constraint and determine the final answer.**\nThe constraint requires a feature to be present in at least $s=5$ documents to be considered for a split.\n- \"zephyr\" is present in $n_z=4$ documents. Since $4  5$, this split is **disallowed**.\n- \"team\" is present in $n_t=10$ documents. Since $10 \\ge 5$, this split is **allowed**.\nUnder the constraint, the only valid split is on the feature \"team\". Therefore, it is the best available split. The information gain for this split is approximately $0.0290494$. Rounded to 4 significant figures, the final answer is $0.02905$.",
            "answer": "$$\n\\boxed{0.02905}\n$$"
        },
        {
            "introduction": "Decision tree algorithms are typically 'greedy,' meaning they make the best possible split at each step without looking ahead. This exercise uses a cleverly designed dataset to expose the potential downfall of this greedy strategy. By analyzing a situation involving feature interactions (an XOR relationship), you will calculate why a feature with zero initial Information Gain is ultimately part of a much better model, revealing the limitations of a purely local optimization approach. ",
            "id": "3131390",
            "problem": "A binary classification task is constructed to highlight the limits of greedy feature selection in decision trees by exhibiting an interaction that only becomes exploitable after a modest early split. Consider a dataset of $16$ examples with binary features $A,B,C \\in \\{0,1\\}$ and binary label $Y \\in \\{0,1\\}$. The data are defined as follows.\n\n1. Across the entire dataset, the joint values $(A,C)$ are uniformly distributed: each of $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$ occurs exactly $4$ times. The label is deterministically given by the exclusive OR (XOR), $Y = A \\oplus C$.\n\n2. The feature $B$ is correlated with $Y$ such that exactly $12$ examples satisfy $B=Y$ and exactly $4$ examples satisfy $B \\neq Y$. Moreover, this correlation is arranged symmetrically across $(A,C)$ as follows:\n   - Within the subset $B=1$ (which has $8$ examples), there are $6$ with $Y=1$ and $2$ with $Y=0$, with $(A,C)$ composition: $(0,1)$ occurs $3$ times, $(1,0)$ occurs $3$ times, $(0,0)$ occurs $1$ time, and $(1,1)$ occurs $1$ time.\n   - Within the subset $B=0$ (which has $8$ examples), there are $6$ with $Y=0$ and $2$ with $Y=1$, with $(A,C)$ composition: $(0,0)$ occurs $3$ times, $(1,1)$ occurs $3$ times, $(0,1)$ occurs $1$ time, and $(1,0)$ occurs $1$ time.\n\nUse the standard definition of entropy with logarithm base $2$, and define information gain (IG) as the reduction in entropy. Consider decision trees limited to depth $2$ (a root split and one additional split per branch). Let $G_A$ denote the maximum total information gain achievable by any depth-$2$ tree whose root split is constrained to be on feature $A$, and let $G_B$ denote the maximum total information gain achievable by any depth-$2$ tree whose root split is constrained to be on feature $B$.\n\nCompute the quantity $\\Delta = G_A - G_B$ in bits. Round your answer to four significant figures.",
            "solution": "We begin from the core definitions. For a binary label $Y$ with empirical class probabilities $p$ and $1-p$, the entropy is\n$$\nH(Y) \\;=\\; -\\,p\\,\\log_{2}(p)\\;-\\;(1-p)\\,\\log_{2}(1-p).\n$$\nFor a feature $X$, the information gain of splitting on $X$ is the reduction in entropy,\n$$\n\\mathrm{IG}(Y;X) \\;=\\; H(Y) \\;-\\; H(Y\\mid X),\n$$\nwhere\n$$\nH(Y\\mid X)\\;=\\;\\sum_{x} P(X=x)\\,H(Y\\mid X=x).\n$$\nA depth-$2$ tree with root $X$ and one further split per child has total information gain equal to the root entropy $H(Y)$ minus the final weighted conditional entropy after the second splits.\n\nStep $1$: Compute the root entropy $H(Y)$. The construction has $(A,C)$ uniformly distributed and $Y = A \\oplus C$, so exactly half the examples have $Y=1$ and half have $Y=0$. Thus $P(Y=1)=P(Y=0)=\\frac{1}{2}$, and\n$$\nH(Y) \\;=\\; -\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\;-\\;\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\;=\\;1.\n$$\n\nStep $2$: Evaluate $G_A$, the best depth-$2$ total information gain with root split on $A$.\n\n- First, compute $H(Y\\mid A)$. Because $(A,C)$ is uniform and $Y=A\\oplus C$, conditioning on $A=0$ yields $Y=C$ with $P(Y=1\\mid A=0)=\\frac{1}{2}$, and conditioning on $A=1$ yields $Y=1-C$ with $P(Y=1\\mid A=1)=\\frac{1}{2}$. Hence $H(Y\\mid A=0)=1$ and $H(Y\\mid A=1)=1$, so $H(Y\\mid A)=1$ and the root information gain on $A$ is $0$.\n- However, after splitting on $A$, an additional split on $C$ in each branch yields perfect purity because $Y$ is a deterministic function of $(A,C)$. Specifically, for $A=0$, $Y=C$, and for $A=1$, $Y=1-C$. Thus $H(Y\\mid A,C)=0$ in both branches. Therefore, the final weighted conditional entropy after the second split is $0$.\n- Consequently, the total information gain with this depth-$2$ tree is\n$$\nG_A \\;=\\; H(Y) - 0 \\;=\\; 1.\n$$\n\nStep $3$: Evaluate $G_B$, the best depth-$2$ total information gain with root split on $B$.\n\nFrom the construction, the branch $B=1$ contains $8$ examples with $6$ of class $Y=1$ and $2$ of class $Y=0$. Therefore\n$$\nH(Y\\mid B=1) \\;=\\; -\\tfrac{6}{8}\\log_{2}\\tfrac{6}{8}\\;-\\;\\tfrac{2}{8}\\log_{2}\\tfrac{2}{8}\n\\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\nSimilarly, the branch $B=0$ contains $8$ examples with $6$ of class $Y=0$ and $2$ of class $Y=1$, which yields the same entropy,\n$$\nH(Y\\mid B=0) \\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\nThus\n$$\nH(Y\\mid B) \\;=\\; \\tfrac{1}{2}\\,H(Y\\mid B=1) + \\tfrac{1}{2}\\,H(Y\\mid B=0)\n\\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\nThe root information gain on $B$ is\n$$\n\\mathrm{IG}(Y;B) \\;=\\; 1 \\;-\\; \\Big(-\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\Big).\n$$\n\nNext, assess the best possible additional split within each $B$ branch. By the specified $(A,C)$ compositions, within $B=1$ the counts by $(A,C)$ are $(0,1)$: $3$, $(1,0)$: $3$ (all with $Y=1$), and $(0,0)$: $1$, $(1,1)$: $1$ (both with $Y=0$). Consider splitting on $A$ in the $B=1$ branch:\n- For $A=0$ within $B=1$, there are $3$ with $Y=1$ and $1$ with $Y=0$, so $H(Y\\mid B=1,A=0)= -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}-\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}$.\n- For $A=1$ within $B=1$, there are $3$ with $Y=1$ and $1$ with $Y=0$, yielding the same entropy.\n\nHence\n$$\nH(Y\\mid B=1, A) \\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\n\\;=\\; H(Y\\mid B=1).\n$$\nSo splitting on $A$ within $B=1$ yields no further reduction. By symmetry of the counts, splitting on $C$ within $B=1$ also yields no reduction. An analogous computation holds in the $B=0$ branch because its $(A,C)$ composition is the complement with the same $3:1$ ratios by $A$ and by $C$. Therefore, no single additional split on $A$ or $C$ within either $B$ branch reduces entropy:\n$$\nH(Y\\mid B, \\text{one more split on } A \\text{ or } C) \\;=\\; H(Y\\mid B).\n$$\nTherefore the best achievable total information gain under the depth-$2$ constraint with root $B$ is just the root gain:\n$$\nG_B \\;=\\; 1 \\;-\\; \\Big(-\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\Big)\n\\;=\\; 1 - h\\!\\left(\\tfrac{1}{4}\\right),\n$$\nwhere $h(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$ is the binary entropy function.\n\nStep $4$: Compute the requested difference\n$$\n\\Delta \\;=\\; G_A - G_B \\;=\\; 1 - \\Big(1 - h\\!\\left(\\tfrac{1}{4}\\right)\\Big) \\;=\\; h\\!\\left(\\tfrac{1}{4}\\right)\n\\;=\\; -\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\;-\\;\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}.\n$$\nNumerically,\n$$\n\\log_{2}\\tfrac{1}{4}=-2,\\quad \\log_{2}\\tfrac{3}{4}=\\log_{2}3 - 2 \\approx 1.5849625 - 2 = -0.4150375,\n$$\nso\n$$\n\\Delta \\;=\\; -\\tfrac{1}{4}(-2)\\;-\\;\\tfrac{3}{4}(-0.4150375)\n\\;=\\; 0.5 + 0.3112781 \\;=\\; 0.8112781\\ldots\n$$\nRounding to four significant figures gives $0.8113$.",
            "answer": "$$\\boxed{0.8113}$$"
        },
        {
            "introduction": "Now, we move from manual calculation to implementation, tackling the nuances of different impurity measures in code. This comprehensive exercise challenges you to compare splits based on Shannon entropy and Gini impurity, especially in the presence of high-cardinality categorical features with rare levels. You will implement the splitting logic and evaluate not just the impurity reduction but also the model's generalization accuracy, providing a deeper insight into the practical trade-offs between these two popular criteria. ",
            "id": "3112936",
            "problem": "You are asked to implement and compare binary splits for a categorical feature in a decision tree classifier using two splitting criteria: Shannon entropy (information gain) and Gini impurity (Gini decrease). The task focuses on the behavior of these criteria in the presence of many rare categories and the trade-off between capturing information and overfitting small counts. Your program must, for each provided test case, determine the optimal binary partition of categories under each criterion, compute the training split quality metrics, and evaluate the expected generalization accuracy under known true category probabilities.\n\nDefinitions and foundational base:\n- Let there be a single categorical feature with $K$ categories. For category $k \\in \\{1,\\dots,K\\}$, you are given the training sample count $n_k$, the observed count $c_k$ of the positive class ($y=1$), and the true underlying probability $p_k$ for $y=1$.\n- The empirical positive rate of category $k$ is $\\hat{p}_k = c_k / n_k$.\n- The parent node has total $N = \\sum_{k=1}^{K} n_k$ and empirical parent rate $\\hat{p}_{\\mathrm{parent}} = \\left(\\sum_{k=1}^{K} c_k\\right)/N$.\n- The Shannon entropy of a binary distribution with probability $p$ is $H(p) = -p\\log(p) - (1-p)\\log(1-p)$, and the Gini impurity is $G(p) = 2p(1-p)$. Use the natural logarithm for entropy; note that the logarithm base scales $H(p)$ by a constant, which does not change the maximizing split.\n- For any binary split of the $K$ categories into a left subset $\\mathcal{L}$ and a right subset $\\mathcal{R}$, with counts $N_{\\mathcal{L}}$ and $N_{\\mathcal{R}}$ and empirical rates $\\hat{p}_{\\mathcal{L}}$ and $\\hat{p}_{\\mathcal{R}}$, the conditional entropy is\n$$\nH_{\\mathrm{cond}} = \\frac{N_{\\mathcal{L}}}{N} H(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} H(\\hat{p}_{\\mathcal{R}}),\n$$\nand the information gain is\n$$\n\\mathrm{IG} = H(\\hat{p}_{\\mathrm{parent}}) - H_{\\mathrm{cond}}.\n$$\nSimilarly, the conditional Gini impurity is\n$$\nG_{\\mathrm{cond}} = \\frac{N_{\\mathcal{L}}}{N} G(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} G(\\hat{p}_{\\mathcal{R}}),\n$$\nand the Gini decrease is\n$$\n\\mathrm{GD} = G(\\hat{p}_{\\mathrm{parent}}) - G_{\\mathrm{cond}}.\n$$\n\nBinary split search protocol:\n- Order categories by ascending empirical rate $\\hat{p}_k$ to obtain a sequence of categories. Restrict attention to prefix splits at indices $t \\in \\{1,\\dots,K-1\\}$: the left child contains the first $t$ categories in the sorted order, and the right child contains the remaining $K-t$ categories. This protocol is admissible for binary classification when the impurity is a separable, Schur-concave function, and it avoids exponential search over set partitions while preserving the optimal split for a broad class of impurity measures including Shannon entropy and Gini impurity.\n- For each $t$, compute the information gain $\\mathrm{IG}(t)$ and the Gini decrease $\\mathrm{GD}(t)$. Let $t^{\\star}_{H}$ be the index maximizing $\\mathrm{IG}(t)$, and $t^{\\star}_{G}$ be the index maximizing $\\mathrm{GD}(t)$. In case of ties, choose the smallest $t$.\n- Define the one-split classifier using training data: for each child, predict the majority class based on $\\hat{p}_{\\mathcal{L}}$ and $\\hat{p}_{\\mathcal{R}}$ (predict $1$ when the rate is at least $0.5$, else predict $0$; break ties by predicting $1$). The expected generalization accuracy under the known true probabilities $p_k$ is computed as\n$$\n\\mathrm{Acc} = \\frac{1}{N}\\left(\\sum_{k \\in \\mathcal{L}} n_k \\cdot \\bigl(\\mathbf{1}[\\hat{p}_{\\mathcal{L}} \\ge 0.5] \\cdot p_k + \\mathbf{1}[\\hat{p}_{\\mathcal{L}}  0.5] \\cdot (1-p_k)\\bigr) + \\sum_{k \\in \\mathcal{R}} n_k \\cdot \\bigl(\\mathbf{1}[\\hat{p}_{\\mathcal{R}} \\ge 0.5] \\cdot p_k + \\mathbf{1}[\\hat{p}_{\\mathcal{R}}  0.5] \\cdot (1-p_k)\\bigr)\\right),\n$$\nwhere $\\mathbf{1}[\\cdot]$ denotes the indicator function. This evaluates expected correctness per category under its true probability and weights by $n_k$.\n\nYour program must implement the above protocol and produce, for each test case, the following outputs:\n- The index $t^{\\star}_{H}$ chosen by Shannon entropy.\n- The index $t^{\\star}_{G}$ chosen by Gini impurity.\n- The maximal information gain $\\mathrm{IG}(t^{\\star}_{H})$ as a float.\n- The maximal Gini decrease $\\mathrm{GD}(t^{\\star}_{G})$ as a float.\n- The expected generalization accuracies $\\mathrm{Acc}_{H}$ and $\\mathrm{Acc}_{G}$ of the respective one-split classifiers, as floats in $[0,1]$.\n\nTest suite:\nFor each test case, arrays are presented in the order of categories before sorting. Your program must sort by empirical rate $\\hat{p}_k$ before evaluating splits.\n\n- Test Case 1 (many rare categories, training suggests purity but true rates are moderate):\n    - $n = [200,200,200,200,5,5,5,5,5,5,5,5]$\n    - $c = [130,70,120,80,5,5,5,5,5,5,5,5]$\n    - $p_{\\mathrm{true}} = [0.6,0.4,0.6,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]$\n\n- Test Case 2 (rare categories genuinely informative with small counts; compare criteria sensitivity):\n    - $n = [100,100,100,100,10,10,10,10,10,10,10,10,10,10]$\n    - $c = [55,45,60,40,9,9,9,9,9,9,9,9,9,9]$\n    - $p_{\\mathrm{true}} = [0.55,0.45,0.60,0.40,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95]$\n\n- Test Case 3 (boundary case, all categories with empirical and true rate at $0.5$):\n    - $n = [50,50,50,50,50,50]$\n    - $c = [25,25,25,25,25,25]$\n    - $p_{\\mathrm{true}} = [0.5,0.5,0.5,0.5,0.5,0.5]$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form\n$[t^{\\star}_{H}, t^{\\star}_{G}, \\mathrm{IG}(t^{\\star}_{H}), \\mathrm{GD}(t^{\\star}_{G}), \\mathrm{Acc}_{H}, \\mathrm{Acc}_{G}]$.\nFor example, the printed output should look like\n[[tH1,tG1,IG1,GD1,AccH1,AccG1],[tH2,tG2,IG2,GD2,AccH2,AccG2],[tH3,tG3,IG3,GD3,AccH3,AccG3]].\nAll outputs must be decimals or integers; do not use percentages.",
            "solution": "The problem requires the implementation of a decision tree splitting algorithm for a single categorical feature, comparing two common splitting criteria: Information Gain (derived from Shannon entropy) and Gini Decrease (derived from Gini impurity). The process involves finding the optimal binary partition of the feature's categories that maximizes these criteria and then evaluating the generalization performance of the resulting one-split classifiers.\n\nThe solution is structured as follows:\nFirst, for each test case, we must prepare the data. The categories are defined by their sample counts ($n_k$), positive class counts ($c_k$), and true positive class probabilities ($p_k$). The primary variable for splitting is the empirical positive rate $\\hat{p}_k = c_k / n_k$. The core of the specified splitting protocol is to order the categories by this empirical rate $\\hat{p}_k$ in ascending order. This reduces the search space for the optimal binary partition from an exponential number of set partitions to a linear number ($K-1$) of prefix splits, where $K$ is the number of categories. This simplification is guaranteed to preserve the optimal split for impurity functions like Shannon entropy and Gini impurity in the context of a binary response.\n\nSecond, we calculate the impurity of the parent node (containing all data) before any split. Let $N = \\sum_k n_k$ be the total sample size and $C = \\sum_k c_k$ be the total positive class count. The parent empirical rate is $\\hat{p}_{\\mathrm{parent}} = C/N$. The parent node's impurity is calculated using the provided formulas for Shannon entropy $H(p) = -p\\log(p) - (1-p)\\log(1-p)$ and Gini impurity $G(p) = 2p(1-p)$. When $p=0$ or $p=1$, the entropy is $0$.\n\nThird, we iterate through all possible prefix splits based on the sorted categories. A split at index $t \\in \\{1, \\dots, K-1\\}$ partitions the categories into a left child node $\\mathcal{L}$ containing the first $t$ categories and a right child node $\\mathcal{R}$ containing the remaining $K-t$ categories. For each potential split, we calculate the properties of the child nodes:\n- Left child $\\mathcal{L}$: Total count $N_{\\mathcal{L}} = \\sum_{k=1}^{t} n_{s,k}$ and positive count $C_{\\mathcal{L}} = \\sum_{k=1}^{t} c_{s,k}$, where the subscript $s$ denotes the sorted order. The empirical rate is $\\hat{p}_{\\mathcal{L}} = C_{\\mathcal{L}} / N_{\\mathcal{L}}$.\n- Right child $\\mathcal{R}$: Total count $N_{\\mathcal{R}} = N - N_{\\mathcal{L}}$ and positive count $C_{\\mathcal{R}} = C - C_{\\mathcal{L}}$. The empirical rate is $\\hat{p}_{\\mathcal{R}} = C_{\\mathcal{R}} / N_{\\mathcal{R}}$.\n\nUsing these, we compute the conditional impurity for the split, which is a weighted average of the child node impurities:\n$$\nH_{\\mathrm{cond}}(t) = \\frac{N_{\\mathcal{L}}}{N} H(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} H(\\hat{p}_{\\mathcal{R}})\n$$\n$$\nG_{\\mathrm{cond}}(t) = \\frac{N_{\\mathcal{L}}}{N} G(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} G(\\hat{p}_{\\mathcal{R}})\n$$\nThe quality of the split is then quantified by the reduction in impurity from the parent to the children:\n- Information Gain: $\\mathrm{IG}(t) = H(\\hat{p}_{\\mathrm{parent}}) - H_{\\mathrm{cond}}(t)$\n- Gini Decrease: $\\mathrm{GD}(t) = G(\\hat{p}_{\\mathrm{parent}}) - G_{\\mathrm{cond}}(t)$\n\nWe find the split index $t^{\\star}_{H}$ that maximizes $\\mathrm{IG}(t)$ and $t^{\\star}_{G}$ that maximizes $\\mathrm{GD}(t)$. In case of ties for the maximum value, the problem specifies selecting the smallest index $t$. This is handled by iterating from $t=1$ to $K-1$ and updating the optimal split only when a strictly greater impurity reduction is found.\n\nFourth, once the optimal splits are identified by both criteria, we evaluate the expected generalization accuracy of the corresponding one-split classifiers. The prediction for each child node ($\\mathcal{L}$ or $\\mathcal{R}$) is determined by its empirical rate: predict class $1$ if the rate is $\\ge 0.5$ and class $0$ otherwise. The accuracy is then calculated using the known true probabilities $p_k$. The formula for accuracy is the weighted average of the probabilities of a correct prediction over all categories:\n$$\n\\mathrm{Acc} = \\frac{1}{N}\\left(\\sum_{k \\in \\mathcal{L}} n_k \\cdot P(\\text{correct}|k) + \\sum_{k \\in \\mathcal{R}} n_k \\cdot P(\\text{correct}|k)\\right)\n$$\nwhere $P(\\text{correct}|k)$ is $p_k$ if the prediction for the node containing category $k$ is $1$, and $1-p_k$ if the prediction is $0$. This procedure is carried out for the split found by information gain ($\\mathrm{Acc}_H$) and for the split found by Gini decrease ($\\mathrm{Acc}_G$).\n\nThe entire procedure is encapsulated in a function that processes each test case and returns the six specified results: $t^{\\star}_{H}$, $t^{\\star}_{G}$, $\\mathrm{IG}(t^{\\star}_{H})$, $\\mathrm{GD}(t^{\\star}_{G})$, $\\mathrm{Acc}_{H}$, and $\\mathrm{Acc}_{G}$. The implementation uses `numpy` for efficient array operations, particularly for calculating cumulative sums to speed up the iteration over splits.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def entropy(p):\n        \"\"\"Calculates Shannon entropy for a binary distribution.\"\"\"\n        if p == 0 or p == 1:\n            return 0.0\n        return -p * np.log(p) - (1 - p) * np.log(1 - p)\n\n    def gini(p):\n        \"\"\"Calculates Gini impurity for a binary distribution.\"\"\"\n        return 2 * p * (1 - p)\n\n    def solve_case(n_orig, c_orig, p_true_orig):\n        \"\"\"\n        Solves the problem for a single test case.\n        \"\"\"\n        n_orig = np.array(n_orig, dtype=float)\n        c_orig = np.array(c_orig, dtype=float)\n        p_true_orig = np.array(p_true_orig, dtype=float)\n        \n        # 1. Calculate p_hat and combine data\n        p_hat_orig = np.divide(c_orig, n_orig, out=np.zeros_like(c_orig, dtype=float), where=n_orig != 0)\n        \n        K = len(n_orig)\n\n        # Combine into a list of tuples to sort: (p_hat, n, c, p_true)\n        # Using a secondary sort key (original index) for stability, though not required by a problem\n        # but good practice.\n        indexed_data = list(zip(p_hat_orig, n_orig, c_orig, p_true_orig, range(K)))\n        sorted_indexed_data = sorted(indexed_data)\n        \n        if not sorted_indexed_data:\n            # Handle empty input case, though not in test suite\n            return [1, 1, 0.0, 0.0, 0.5, 0.5]\n\n        # Unzip into sorted arrays\n        p_hat_s, n_s, c_s, p_true_s, _ = map(np.array, zip(*sorted_indexed_data))\n\n        # 2. Parent node stats\n        N_total = n_s.sum()\n        C_total = c_s.sum()\n        \n        if N_total == 0:\n            return [1, 1, 0.0, 0.0, 0.5, 0.5]\n\n        p_hat_parent = C_total / N_total\n        H_parent = entropy(p_hat_parent)\n        G_parent = gini(p_hat_parent)\n\n        # Handle case with only one category where no split is possible\n        if K = 1:\n            pred = 1 if p_hat_parent >= 0.5 else 0\n            correct_mass = 0\n            if pred == 1:\n                correct_mass = (n_s * p_true_s).sum()\n            else:\n                correct_mass = (n_s * (1 - p_true_s)).sum()\n            acc = correct_mass / N_total if N_total > 0 else 0.5\n            return [1, 1, 0.0, 0.0, acc, acc]\n            \n        # 3. Iterate through splits\n        n_cumsum = np.cumsum(n_s)\n        c_cumsum = np.cumsum(c_s)\n\n        max_ig = -np.inf\n        t_star_h = 1\n        max_gd = -np.inf\n        t_star_g = 1\n\n        for i in range(K - 1):\n            t = i + 1\n\n            N_L = n_cumsum[i]\n            C_L = c_cumsum[i]\n            p_hat_L = C_L / N_L if N_L > 0 else 0.0\n\n            N_R = N_total - N_L\n            C_R = C_total - C_L\n            p_hat_R = C_R / N_R if N_R > 0 else 0.0\n            \n            H_cond = (N_L / N_total) * entropy(p_hat_L) + (N_R / N_total) * entropy(p_hat_R)\n            G_cond = (N_L / N_total) * gini(p_hat_L) + (N_R / N_total) * gini(p_hat_R)\n            \n            ig = H_parent - H_cond\n            gd = G_parent - G_cond\n\n            # First value found becomes the max\n            if i == 0:\n                max_ig = ig\n                max_gd = gd\n            \n            if ig > max_ig:\n                max_ig = ig\n                t_star_h = t\n            \n            if gd > max_gd:\n                max_gd = gd\n                t_star_g = t\n\n        # 4. Calculate accuracies\n        def calculate_accuracy(t_star):\n            # Left child\n            N_L = n_cumsum[t_star - 1]\n            C_L = c_cumsum[t_star - 1]\n            p_hat_L = C_L / N_L if N_L > 0 else 0.0\n            pred_L = 1 if p_hat_L >= 0.5 else 0\n\n            # Right child\n            N_R = N_total - N_L\n            C_R = C_total - C_L\n            p_hat_R = C_R / N_R if N_R > 0 else 0.0\n            pred_R = 1 if p_hat_R >= 0.5 else 0\n\n            total_correct_mass = 0.0\n            \n            # Left node contribution\n            n_L_cats = n_s[:t_star]\n            p_true_L_cats = p_true_s[:t_star]\n            if pred_L == 1:\n                total_correct_mass += np.sum(n_L_cats * p_true_L_cats)\n            else:\n                total_correct_mass += np.sum(n_L_cats * (1 - p_true_L_cats))\n            \n            # Right node contribution\n            n_R_cats = n_s[t_star:]\n            p_true_R_cats = p_true_s[t_star:]\n            if pred_R == 1:\n                total_correct_mass += np.sum(n_R_cats * p_true_R_cats)\n            else:\n                total_correct_mass += np.sum(n_R_cats * (1 - p_true_R_cats))\n            \n            return total_correct_mass / N_total\n\n        Acc_H = calculate_accuracy(t_star_h)\n        Acc_G = calculate_accuracy(t_star_g)\n\n        return [t_star_h, t_star_g, max_ig, max_gd, Acc_H, Acc_G]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": [200, 200, 200, 200, 5, 5, 5, 5, 5, 5, 5, 5],\n            \"c\": [130, 70, 120, 80, 5, 5, 5, 5, 5, 5, 5, 5],\n            \"p_true\": [0.6, 0.4, 0.6, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n        },\n        {\n            \"n\": [100, 100, 100, 100, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n            \"c\": [55, 45, 60, 40, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n            \"p_true\": [0.55, 0.45, 0.60, 0.40, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n        },\n        {\n            \"n\": [50, 50, 50, 50, 50, 50],\n            \"c\": [25, 25, 25, 25, 25, 25],\n            \"p_true\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case[\"n\"], case[\"c\"], case[\"p_true\"])\n        results.append(f\"[{','.join(f'{x:.7f}' for x in result)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}