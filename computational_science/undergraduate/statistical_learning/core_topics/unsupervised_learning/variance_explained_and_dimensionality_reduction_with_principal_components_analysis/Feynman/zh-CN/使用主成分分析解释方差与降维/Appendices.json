{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在巩固解释方差比（Explained Variance Ratio, EVR）的核心概念。通过一个假设的几何衰减特征值模型，你将直接观察到特征值衰减速率如何决定需要多少个主成分才能捕获所需的方差比例。这项实践有助于建立关于数据内在结构与降维有效性之间关系的直观理解 。",
            "id": "3191932",
            "problem": "一个由 $p=50$ 个中心化和标准化预测变量组成的数据集，使用主成分分析（PCA; Principal Components Analysis）进行建模。设样本协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$，其谱分解为 $\\Sigma = U \\Lambda U^{\\top}$，其中 $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\lambda_{2},\\dots,\\lambda_{p})$，$U$ 包含标准正交的特征向量。假设特征值按非递增顺序排列，即 $\\lambda_{1} \\ge \\lambda_{2} \\ge \\dots \\ge \\lambda_{p} \\ge 0$。前 $k$ 个主成分的方差解释率（EVR; Explained Variance Ratio）定义为由前 $k$ 个特征向量张成的子空间所捕获的总方差的比例。\n\n考虑两种可能的特征谱，每种都由一个几何衰减率 $r \\in (0,1)$ 参数化，该衰减率控制特征间隙（即 $\\lambda_{i+1}/\\lambda_{i} = r$）：\n- 谱 $\\mathcal{A}$：$\\lambda_{i} = r_{\\mathcal{A}}^{\\,i-1}$，其中 $r_{\\mathcal{A}} = 0.80$，对于 $i=1,2,\\dots,50$。\n- 谱 $\\mathcal{B}$：$\\lambda_{i} = r_{\\mathcal{B}}^{\\,i-1}$，其中 $r_{\\mathcal{B}} = 0.95$，对于 $i=1,2,\\dots,50$。\n\n从 PCA 和 EVR 的定义出发，推导每种谱的累积 EVR 作为 $k$ 的函数。然后，对每种谱，确定最小整数 $k$，使得累积 EVR 至少达到目标水平 $\\tau \\in \\{0.90, 0.95, 0.99\\}$。\n\n按以下顺序报告六个最小整数作为您的最终答案：$\\big(k_{\\mathcal{A}}(0.90),\\,k_{\\mathcal{A}}(0.95),\\,k_{\\mathcal{A}}(0.99),\\,k_{\\mathcal{B}}(0.90),\\,k_{\\mathcal{B}}(0.95),\\,k_{\\mathcal{B}}(0.99)\\big)$。由于 $k$ 根据定义是整数，因此无需舍入说明。请将目标表示为小数（例如，$0.90$ 而不是百分比）。",
            "solution": "用户希望找到所需的最少主成分数 $k$，以便为两种不同的特征值谱解释特定比例的总方差。\n\n### 步骤1：问题验证\n\n**提取的已知条件：**\n*   预测变量数量：$p=50$。\n*   数据是中心化和标准化的。\n*   样本协方差矩阵：$\\Sigma \\in \\mathbb{R}^{p \\times p}$。\n*   谱分解：$\\Sigma = U \\Lambda U^{\\top}$，其中 $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\lambda_{2},\\dots,\\lambda_{p})$。\n*   特征值排序：$\\lambda_{1} \\ge \\lambda_{2} \\ge \\dots \\ge \\lambda_{p} \\ge 0$。\n*   方差解释率（EVR）定义：前 $k$ 个主成分捕获的总方差的比例。\n*   谱 $\\mathcal{A}$：特征值为 $\\lambda_{i} = r_{\\mathcal{A}}^{\\,i-1}$，其中 $r_{\\mathcal{A}} = 0.80$，对于 $i=1,2,\\dots,50$。\n*   谱 $\\mathcal{B}$：特征值为 $\\lambda_{i} = r_{\\mathcal{B}}^{\\,i-1}$，其中 $r_{\\mathcal{B}} = 0.95$，对于 $i=1,2,\\dots,50$。\n*   目标 EVR 水平：$\\tau \\in \\{0.90, 0.95, 0.99\\}$。\n*   目标：对每种谱和每个目标水平 $\\tau$，找到最小整数 $k$，使得累积 EVR 至少为 $\\tau$。\n*   报告格式：一个包含六个整数的向量 $\\big(k_{\\mathcal{A}}(0.90),\\,k_{\\mathcal{A}}(0.95),\\,k_{\\mathcal{A}}(0.99),\\,k_{\\mathcal{B}}(0.90),\\,k_{\\mathcal{B}}(0.95),\\,k_{\\mathcal{B}}(0.99)\\big)$。\n\n**验证结论：**\n该问题在科学上基于统计学习的原理，特别是主成分分析（PCA）。它在数学上是适定的，提供了所有必要信息（$p$、谱模型、目标阈值）以得出唯一且有意义的解。语言客观而精确。该问题不违反任何无效性标准。这是一个应用 PCA 和方差解释定义的标准、可形式化的练习。\n\n**结论：**问题有效。\n\n### 步骤2：求解推导\n\n分析从 PCA 的基本定义开始。数据集中的总方差由样本协方差矩阵 $\\Sigma$ 的迹给出，它等于其特征值的总和。\n$$ V_{\\text{total}} = \\operatorname{tr}(\\Sigma) = \\sum_{i=1}^{p} \\lambda_i $$\n前 $k$ 个主成分捕获的方差是前 $k$ 个最大特征值的和。\n$$ V_k = \\sum_{i=1}^{k} \\lambda_i $$\n前 $k$ 个成分的累积方差解释率（EVR），记为 $\\text{EVR}(k)$，是前 $k$ 个成分捕获的方差与总方差的比率。\n$$ \\text{EVR}(k) = \\frac{V_k}{V_{\\text{total}}} = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{p} \\lambda_i} $$\n问题指定了一个遵循几何级数的特征值谱：$\\lambda_i = r^{i-1}$，对于 $i=1, 2, \\dots, p$。EVR 表达式中的和是有限几何级数。首项为 $a=1$、公比为 $r$ 的几何级数前 $n$ 项的和为 $\\sum_{j=0}^{n-1} ar^j = a\\frac{1-r^n}{1-r}$。在我们的例子中，$a=1$。\n\n分子是前 $k$ 个特征值的和：\n$$ \\sum_{i=1}^{k} \\lambda_i = \\sum_{j=0}^{k-1} r^j = \\frac{1 - r^k}{1 - r} $$\n分母是所有 $p$ 个特征值的和：\n$$ \\sum_{i=1}^{p} \\lambda_i = \\sum_{j=0}^{p-1} r^j = \\frac{1 - r^p}{1 - r} $$\n将这些代入 EVR 公式，$(1-r)$ 项相互抵消：\n$$ \\text{EVR}(k) = \\frac{\\frac{1 - r^k}{1 - r}}{\\frac{1 - r^p}{1 - r}} = \\frac{1 - r^k}{1 - r^p} $$\n问题要求找到最小整数 $k$，使得对于给定的 $\\tau$ 值，$\\text{EVR}(k) \\ge \\tau$。我们必须解这个不等式：\n$$ \\frac{1 - r^k}{1 - r^p} \\ge \\tau $$\n整理各项以分离出 $k$：\n$$ 1 - r^k \\ge \\tau (1 - r^p) $$\n$$ -r^k \\ge \\tau(1 - r^p) - 1 $$\n$$ r^k \\le 1 - \\tau(1 - r^p) $$\n为了求解 $k$，我们对两边取自然对数。由于 $r \\in (0,1)$，其对数 $\\ln(r)$ 是负数。因此，除以 $\\ln(r)$ 会反转不等号。\n$$ k \\ln(r) \\le \\ln\\left(1 - \\tau(1 - r^p)\\right) $$\n$$ k \\ge \\frac{\\ln\\left(1 - \\tau(1 - r^p)\\right)}{\\ln(r)} $$\n由于 $k$ 必须是整数，满足此条件的最小整数 $k$ 是大于或等于右侧值的最小整数，这由向上取整函数给出。\n$$ k = \\left\\lceil \\frac{\\ln\\left(1 - \\tau(1 - r^p)\\right)}{\\ln(r)} \\right\\rceil $$\n现在，我们将此公式应用于每个指定的谱和目标水平 $\\tau$，其中 $p=50$。\n\n**谱 $\\mathcal{A}$ 的计算（$r_{\\mathcal{A}} = 0.80$）：**\n此谱的常数项是 $1 - r_{\\mathcal{A}}^p = 1 - (0.80)^{50}$。\n1.  对于 $\\tau=0.90$：\n    $$ k_{\\mathcal{A}}(0.90) = \\left\\lceil \\frac{\\ln\\left(1 - 0.90(1 - (0.80)^{50})\\right)}{\\ln(0.80)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.1000128...)}{\\ln(0.80)} \\right\\rceil = \\lceil 10.318... \\rceil = 11 $$\n2.  对于 $\\tau=0.95$：\n    $$ k_{\\mathcal{A}}(0.95) = \\left\\lceil \\frac{\\ln\\left(1 - 0.95(1 - (0.80)^{50})\\right)}{\\ln(0.80)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.0500135...)}{\\ln(0.80)} \\right\\rceil = \\lceil 13.423... \\rceil = 14 $$\n3.  对于 $\\tau=0.99$：\n    $$ k_{\\mathcal{A}}(0.99) = \\left\\lceil \\frac{\\ln\\left(1 - 0.99(1 - (0.80)^{50})\\right)}{\\ln(0.80)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.0100141...)}{\\ln(0.80)} \\right\\rceil = \\lceil 20.630... \\rceil = 21 $$\n\n**谱 $\\mathcal{B}$ 的计算（$r_{\\mathcal{B}} = 0.95$）：**\n此谱的常数项是 $1 - r_{\\mathcal{B}}^p = 1 - (0.95)^{50}$。\n1.  对于 $\\tau=0.90$：\n    $$ k_{\\mathcal{B}}(0.90) = \\left\\lceil \\frac{\\ln\\left(1 - 0.90(1 - (0.95)^{50})\\right)}{\\ln(0.95)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.169250...)}{\\ln(0.95)} \\right\\rceil = \\lceil 34.628... \\rceil = 35 $$\n2.  对于 $\\tau=0.95$：\n    $$ k_{\\mathcal{B}}(0.95) = \\left\\lceil \\frac{\\ln\\left(1 - 0.95(1 - (0.95)^{50})\\right)}{\\ln(0.95)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.123097...)}{\\ln(0.95)} \\right\\rceil = \\lceil 40.838... \\rceil = 41 $$\n3.  对于 $\\tau=0.99$：\n    $$ k_{\\mathcal{B}}(0.99) = \\left\\lceil \\frac{\\ln\\left(1 - 0.99(1 - (0.95)^{50})\\right)}{\\ln(0.95)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.086175...)}{\\ln(0.95)} \\right\\rceil = \\lceil 47.788... \\rceil = 48 $$\n\n按所要求的顺序，六个最小整数为 $(11, 14, 21, 35, 41, 48)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11  14  21  35  41  48\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "现实世界中的数据集往往很复杂，主成分分析（PCA）的应用也需要更深入的考量。本练习探讨了一个常见情景：数据中包含分类特征。通过分析哑变量编码（dummy variable encoding）如何影响PCA的结果，你将学会批判性地评估PCA的输出，并理解特征的表示方式会如何影响模型识别出的“主要”模式 。",
            "id": "3191947",
            "problem": "一位数据分析师正在使用主成分分析（PCA）研究一个混合特征集的降维问题，该特征集包含一个连续特征和一个编码为哑变量的分类特征。数据包含 $n = 6$ 个观测值，均匀分为三组 $\\{A, B, C\\}$，通过两个哑变量 $z_1 = \\mathbb{1}\\{G = B\\}$ 和 $z_2 = \\mathbb{1}\\{G = C\\}$ 进行编码，其中 $G$ 表示组成员身份。连续特征 $x$ 与组成员身份无关，且变异性很小。观测值如下（每个观测值是一个行向量 $(x, z_1, z_2)$）：\n- A组：$(x, z_1, z_2) = (-0.1, 0, 0)$，$(0.1, 0, 0)$\n- B组：$(x, z_1, z_2) = (-0.1, 1, 0)$，$(0.1, 1, 0)$\n- C组：$(x, z_1, z_2) = (-0.1, 0, 1)$，$(0.1, 0, 1)$\n\n分析师根据统计学习中 PCA 的标准惯例执行 PCA：对列进行中心化（减去经验均值），主成分是最大化投影方差的正交归一化方向，等价于中心化数据协方差（二阶矩）矩阵的特征向量。一个主成分的解释方差是其相关的特征值，解释方差比是该特征值除以所有特征值之和。\n\n分析师接着考虑两种情景：\n- 情景1：对具有列 $(x, z_1, z_2)$ 的中心化矩阵应用 PCA。\n- 情景2：连续特征 $x$ 在组内进行去均值处理（即用 $x - \\bar{x}_{G}$ 替换 $x$，其中 $\\bar{x}_{G}$ 是观测值所在组内 $x$ 的均值），然后对残差 $x$ 单独应用 PCA（删除哑变量列），或者对残差 $x$ 与中心化的哑变量列一起应用 PCA。\n\n下列哪个陈述是正确的？\n\nA. 在情景1中，第一个主成分与 B 组和 C 组之间的对比对齐（近似与 $z_1 - z_2$ 成比例），其解释方差比大于 $0.7$。\n\nB. 如果在 PCA 之前不对列进行中心化，哑变量将不会贡献任何方差，因为它们是二元的，所以它们不会主导任何主成分。\n\nC. 在情景2中，如果只对残差 $x$ 应用 PCA（删除 $z_1$ 和 $z_2$），第一个主成分解释了大约 $100\\%$ 的剩余方差。\n\nD. 在情景2中，如果 $x$ 在组内去均值但保留中心化的哑变量列，最大特征值仍然由哑变量子空间主导（约为 $1/3$），因此第一个主成分仍然与分类变量对齐。\n\nE. 如果分类特征不是用两个哑变量编码，而是用一个单一的对比编码列 $w$ 来编码，其值为：B组为 $+1$，C组为 $-1$，A组为 $0$，那么中心化后 $w$ 的方差将是 $2/9$，因此相对于双哑变量编码，最大特征值会减小。",
            "solution": "用户希望我解决一个关于主成分分析（PCA）的统计学习问题。\n\n### 步骤1：提取给定信息\n-   观测值数量：$n = 6$。\n-   组数：$3$ (标记为 $\\{A, B, C\\}$)，每组 $2$ 个观测值。\n-   特征：一个连续特征 $x$，以及一个由两个哑变量 $z_1 = \\mathbb{1}\\{G = B\\}$ 和 $z_2 = \\mathbb{1}\\{G = C\\}$ 编码的分类特征 $G$。\n-   数据矩阵，行为 $(x, z_1, z_2)$：\n    -   A组 ($2$ 个观测值): $(-0.1, 0, 0)$, $(0.1, 0, 0)$\n    -   B组 ($2$ 个观测值): $(-0.1, 1, 0)$, $(0.1, 1, 0)$\n    -   C组 ($2$ 个观测值): $(-0.1, 0, 1)$, $(0.1, 0, 1)$\n-   PCA 过程：通过减去经验均值对列进行中心化。主成分是中心化数据协方差矩阵的特征向量。\n-   协方差矩阵定义：“中心化数据的二阶矩矩阵”，这意味着除数为 $n$。\n-   情景1：对具有列 $(x, z_1, z_2)$ 的中心化矩阵应用 PCA。\n-   情景2：特征 $x$ 被其组内去均值版本 $x_{resid} = x - \\bar{x}_{G}$ 替换。然后，PCA 应用于 (a) 单独的 $x_{resid}$，或 (b) $x_{resid}$ 与中心化的哑变量列。\n\n### 步骤2：使用提取的给定信息进行验证\n-   **科学依据**：该问题使用 PCA，这是统计学和机器学习中一个标准且成熟的技术。分析混合连续和分类数据的背景是一个常见且有效的应用领域。\n-   **良态问题**：数据已明确提供，并且程序（中心化、PCA、去均值）已定义。问题是自包含的，并提供了足够的信息来计算唯一解。\n-   **客观性**：问题以精确、客观的语言陈述。数据是数值型的，任务是在标准框架内的计算和解释。\n-   **未发现缺陷**：该问题不违反科学原理，不是不完整或矛盾的，不基于不切实际的前提，也不是病态的。将 $x$ 描述为具有“小变异性”且“与组成员身份无关”与所提供的数据一致，其中 $x$ 的分布在各组之间是相同的，并且其方差相对于哑变量贡献的方差而言很小。\n\n### 步骤3：结论和行动\n问题陈述是**有效**的。我将继续进行解题推导。\n\n### 情景1分析\n首先，我们构建完整的包含列 $x$、$z_1$ 和 $z_2$ 的数据矩阵 $X$。\n$$\nX = \\begin{pmatrix}\n-0.1  0  0 \\\\\n0.1  0  0 \\\\\n-0.1  1  0 \\\\\n0.1  1  0 \\\\\n-0.1  0  1 \\\\\n0.1  0  1\n\\end{pmatrix}\n$$\nPCA 过程从对列进行中心化开始。我们计算每列的均值：\n-   $\\bar{x} = \\frac{1}{6}(-0.1+0.1-0.1+0.1-0.1+0.1) = 0$\n-   $\\bar{z}_1 = \\frac{1}{6}(0+0+1+1+0+0) = \\frac{2}{6} = \\frac{1}{3}$\n-   $\\bar{z}_2 = \\frac{1}{6}(0+0+0+0+1+1) = \\frac{2}{6} = \\frac{1}{3}$\n\n中心化的数据矩阵 $X_c$ 具有列 $x_c = x-\\bar{x}$、$z_{1c} = z_1-\\bar{z}_1$ 和 $z_{2c} = z_2-\\bar{z}_2$。\n$$\nX_c = \\begin{pmatrix}\n-0.1  -1/3  -1/3 \\\\\n0.1  -1/3  -1/3 \\\\\n-0.1  2/3  -1/3 \\\\\n0.1  2/3  -1/3 \\\\\n-0.1  -1/3  2/3 \\\\\n0.1  -1/3  2/3\n\\end{pmatrix}\n$$\n接下来，我们计算协方差矩阵 $S = \\frac{1}{n} X_c^T X_c$。首先，我们计算 $X_c^T X_c$ 的点积：\n-   $\\sum x_{ci}^2 = 6 \\times (0.1)^2 = 0.06$\n-   $\\sum z_{1ci}^2 = 4 \\times (-1/3)^2 + 2 \\times (2/3)^2 = 4/9 + 8/9 = 12/9 = 4/3$\n-   $\\sum z_{2ci}^2 = 4 \\times (-1/3)^2 + 2 \\times (2/3)^2 = 12/9 = 4/3$\n-   $\\sum x_{ci} z_{1ci} = 0$ (由于 $x$ 值在每个组内的对称性)\n-   $\\sum x_{ci} z_{2ci} = 0$ (原因同上)\n-   $\\sum z_{1ci} z_{2ci} = 2 \\times (-1/3)(-1/3) + 2 \\times (2/3)(-1/3) + 2 \\times (-1/3)(2/3) = 2/9 - 4/9 - 4/9 = -6/9 = -2/3$\n\n所以，$X_c^T X_c = \\begin{pmatrix} 0.06  0  0 \\\\ 0  4/3  -2/3 \\\\ 0  -2/3  4/3 \\end{pmatrix}$。\n协方差矩阵为 $S = \\frac{1}{6} X_c^T X_c$：\n$$\nS = \\begin{pmatrix} 0.01  0  0 \\\\ 0  \\frac{4}{18}  \\frac{-2}{18} \\\\ 0  \\frac{-2}{18}  \\frac{4}{18} \\end{pmatrix} = \\begin{pmatrix} 0.01  0  0 \\\\ 0  2/9  -1/9 \\\\ 0  -1/9  2/9 \\end{pmatrix}\n$$\n$S$ 的特征值和特征向量决定了主成分。由于 $S$ 是块对角矩阵，一个特征向量是 $(1, 0, 0)^T$，对应的特征值为 $\\lambda_x = 0.01$。另外两个由哑变量的子矩阵 $S_z = \\begin{pmatrix} 2/9  -1/9 \\\\ -1/9  2/9 \\end{pmatrix}$ 决定。\n$S_z$ 的特征方程是 $(2/9 - \\lambda)^2 - (-1/9)^2 = 0$，这得到 $2/9 - \\lambda = \\pm 1/9$。因此特征值为 $\\lambda = 2/9 \\mp 1/9$。\n-   最大特征值：$\\lambda_1 = 2/9 + 1/9 = 3/9 = 1/3$。\n-   第二大特征值：$\\lambda_2 = 2/9 - 1/9 = 1/9$。\n$S$ 的特征值按降序排列为 $\\lambda_1 = 1/3$、$\\lambda_2 = 1/9$ 和 $\\lambda_3 = 0.01$。\n\n$\\lambda_1 = 1/3$ 对应的特征向量可以通过 $(S_z - (1/3)I)v = 0$ 求得，这给出 $\\begin{pmatrix} -1/9  -1/9 \\\\ -1/9  -1/9 \\end{pmatrix}v = 0$。这意味着特征向量与 $(1, -1)^T$ 成比例。完整、归一化的主成分载荷向量是 $(0, 1/\\sqrt{2}, -1/\\sqrt{2})^T$。\n\n总方差是 $S$ 的迹，$\\operatorname{Tr}(S) = \\sum \\lambda_i = 1/3 + 1/9 + 0.01 = 4/9 + 1/100 = \\frac{400+9}{900} = \\frac{409}{900}$。\n第一个主成分的解释方差比是 $\\frac{\\lambda_1}{\\operatorname{Tr}(S)} = \\frac{1/3}{409/900} = \\frac{1}{3} \\times \\frac{900}{409} = \\frac{300}{409} \\approx 0.7335$。\n\n### 情景2分析\n在此情景中，$x$ 被替换为 $x_{resid} = x - \\bar{x}_{G}$。我们必须计算 $x$ 的组内均值：\n-   A组：$\\bar{x}_A = \\frac{-0.1 + 0.1}{2} = 0$。\n-   B组：$\\bar{x}_B = \\frac{-0.1 + 0.1}{2} = 0$。\n-   C组：$\\bar{x}_C = \\frac{-0.1 + 0.1}{2} = 0$。\n由于所有组内均值都为 $0$，残差向量与原始 $x$ 向量相同：$x_{resid} = x$。\n\n**情景2a**：PCA 单独应用于 $x_{resid}$。\n这意味着对单列向量 $x = (-0.1, 0.1, -0.1, 0.1, -0.1, 0.1)^T$ 执行 PCA。这是一个一维数据集。在一维中，只有一个主成分，根据定义，它必须捕获数据的所有（$100\\%$）方差。此数据的方差为 $\\lambda_3 = 0.01$，非零。\n\n**情景2b**：PCA 应用于 $x_{resid}$ 与中心化的哑变量列。\n问题陈述我们对由“残差 $x$ 与中心化的哑变量列”构成的数据应用 PCA。假设标准的 PCA 过程应用于这些指定的列。这些列是 $(x_{resid}, z_1, z_2)$。由于 $x_{resid} = x$，这组列是 $(x, z_1, z_2)$。PCA 过程要求对这些列进行中心化，这与情景1的过程完全相同。因此，情景2b的 PCA 结果（协方差矩阵、特征值、特征向量）与情景1的结果相同。\n\n### 逐项分析\n\n**A. 在情景1中，第一个主成分与 B 组和 C 组之间的对比对齐（近似与 $z_1 - z_2$ 成比例），其解释方差比大于 $0.7$。**\n第一个主成分的载荷向量与 $(0, 1, -1)^T$ 成比例，这对应于得分 $z_1 - z_2$（应用于未中心化的哑变量以进行解释）。此得分对于 B 组是 $1$，对于 C 组是 $-1$，对于 A 组是 $0$，代表了 B 和 C 之间的对比。计算出的解释方差比为 $300/409 \\approx 0.7335$，大于 $0.7$。\n**结论：正确。**\n\n**B. 如果在 PCA 之前不对列进行中心化，哑变量将不会贡献任何方差，因为它们是二元的，所以它们不会主导任何主成分。**\n这个陈述包含多个错误。首先，一个非恒定的二元变量具有非零方差。例如，$z_1$ 的经验方差是 $\\bar{z}_1(1-\\bar{z}_1) = (1/3)(2/3) = 2/9 \\neq 0$。其次，对未中心化数据进行 PCA 考虑的是关于原点的二阶矩，而不是方差（二阶中心矩）。$z_1$ 列的平方和是 $1^2+1^2=2$，而 $z_2$ 的是 $1^2+1^2=2$。对于 $x$ 列，它是 $6 \\times (0.1)^2 = 0.06$。$z$ 列的二阶矩远大于 $x$ 列，因此会主导第一个主成分。\n**结论：不正确。**\n\n**C. 在情景2中，如果只对残差 $x$ 应用 PCA（删除 $z_1$ 和 $z_2$），第一个主成分解释了大约 $100\\%$ 的剩余方差。**\n正如情景2a的分析所确定的，对单列数据（一维数据集）进行 PCA 会产生一个单一的主成分。这个成分精确地解释了该数据中 $100\\%$ 的方差。“剩余方差”一词指的是被分析的残差向量 $x_{resid}$ 的方差。说它解释“大约 $100\\%$”是一个真实的陈述。\n**结论：正确。**\n\n**D. 在情景2中，如果 $x$ 在组内去均值但保留中心化的哑变量列，最大特征值仍然由哑变量子空间主导（约为 $1/3$），因此第一个主成分仍然与分类变量对齐。**\n如情景2b的分析所示，此情况下的 PCA 过程与情景1中的完全相同，因为 $x_{resid} = x$。因此，所有结果都相同。最大特征值恰好是 $1/3$，即“大约 $1/3$”。其对应的特征向量 $(0, 1/\\sqrt{2}, -1/\\sqrt{2})^T$ 完全位于由中心化哑变量张成的子空间中，因此“由哑变量子空间主导”和“与分类变量对齐”的陈述是正确的。“保持”和“仍然”这些性质与情景1中相同。\n**结论：正确。**\n\n**E. 如果分类特征不是用两个哑变量编码，而是用一个单一的对比编码列 $w$ 来编码，其值为：B组为 $+1$，C组为 $-1$，A组为 $0$，那么中心化后 $w$ 的方差将是 $2/9$，因此相对于双哑变量编码，最大特征值会减小。**\n新的列 $w$ 是 $(0, 0, 1, 1, -1, -1)^T$。其均值为 $\\bar{w}=0$，所以它已经中心化了。平方和为 $\\sum w_i^2 = 0+0+1+1+1+1=4$。方差（使用 $1/n$ 公式）是 $\\text{Var}(w) = 4/6 = 2/3$。该陈述声称方差是 $2/9$，这是错误的。对于第二部分，PCA 将应用于列 $(x, w)$。由于 $\\sum x_i w_i = 0$，协方差矩阵将是对角矩阵，其对角线元素为 $\\text{Var}(x)=0.01$ 和 $\\text{Var}(w)=2/3$。新的最大特征值将是 $2/3 \\approx 0.667$。原始双哑变量编码中的最大特征值是 $1/3 \\approx 0.333$。因此，最大特征值会*增加*，而不是减少。\n**结论：不正确。**\n\n正确的陈述是 A、C 和 D。",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "在应用PCA时，一个关键问题是“应该保留多少个主成分？”。本练习将理论与一种强大的、基于模拟的方法——平行分析（Parallel Analysis）——相结合。通过亲手实现该技术，你将学会如何创建一个统计基线，用以区分有意义的信号和随机噪声，从而为选择降维后的维度数量提供一个稳健的、数据驱动的答案 。",
            "id": "3191949",
            "problem": "您需要实现一个完整、可运行的程序，该程序执行主成分分析 (PCA) 的平行分析，并用其对超出独立高斯噪声的低维结构的存在性进行逐分量的假设检验。推导和算法必须基于以下基本定义和事实：对一个中心化的数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 进行主成分分析 (PCA)，其过程是先构建无偏样本协方差 $S = \\frac{1}{n-1} X^{\\top} X$，然后计算其特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$，其中 $\\mathrm{trace}(S) = \\sum_{i=1}^p \\lambda_i$ 等于总样本方差。在一个零模型下，其中特征是独立同分布的标准正态分布，即 $Z \\in \\mathbb{R}^{n \\times p}$ 的每个条目都独立地从 $\\mathcal{N}(0,1)$ 中抽取，此时样本协方差的特征值是 Wishart 矩阵的特征值，并且由于抽样变异性而在 1 附近波动。平行分析是一种基于重抽样的方法，它为每个分量索引 $i$ 估计一个阈值 $t_i$ 作为第 $i$ 大特征值零分布的经验百分位数，然后将观测到的 $\\lambda_i$ 与 $t_i$ 进行比较，以决定是否保留分量 $i$。您必须将任何比例或百分位数表示为 $[0,1]$ 区间内的小数。\n\n为每个测试用例实现以下假设检验情景。对于每个 $i \\in \\{1,\\dots,p\\}$，检验零假设 $H_{0,i}$（即分量 $i$ 中没有超出独立标准正态噪声的信号）与备择假设 $H_{1,i}$（即存在信号）。使用以下决策规则：通过 $m$ 次零模拟的蒙特卡洛方法，将 $t_i$ 计算为第 $i$ 大特征值零分布的经验 $q$ 分位数（其中 $q \\in (0,1)$）。当且仅当 $\\lambda_i > t_i$ 时，拒绝 $H_{0,i}$。因此，保留的分量数量是该严格不等式成立的索引 $i$ 的计数。\n\n您的程序必须：\n- 对于观测数据：首先使用指定的伪随机种子，从 $\\mathcal{N}(0,1)$ 中独立抽样条目来构建一个基础矩阵 $W$，然后对前 $r$ 个特征应用对角“尖峰”以编码信号，从而构造 $X \\in \\mathbb{R}^{n \\times p}$。也就是说，对于给定的非负整数 $r \\le p$ 和非负实数 $s$，将 $W$ 的前 $r$ 列乘以 $\\sqrt{1+s}$ 以获得 $X$。在计算 $S = \\frac{1}{n-1} X^{\\top} X$ 及其特征值 $\\lambda_1 \\ge \\cdots \\ge \\lambda_p$ 之前，通过减去列均值来中心化 $X$ 的列。\n- 对于零模拟：对于 $m$ 次重复中的每一次，使用指定的伪随机种子，从 $\\mathcal{N}(0,1)$ 中独立抽样条目来生成 $Z \\in \\mathbb{R}^{n \\times p}$，通过减去列均值来中心化 $Z$ 的列，构建 $S_0 = \\frac{1}{n-1} Z^{\\top} Z$，计算 $S_0$ 的特征值，将它们按降序排序，并记录所得向量。在 $m$ 次重复之后，为每个分量索引 $i$，计算该索引处 $m$ 个记录值的经验 $q$ 分位数，以获得 $t_i$。\n- 统计满足 $\\lambda_i > t_i$ 的索引 $i$ 的数量，并为该测试用例输出该计数值（一个整数）。\n\n您必须遵循的设计选择：\n- 对观测数据和零数据使用相同的中心化和协方差公式，以便阈值是针对观测到的估计量进行校准的。\n- 使用严格不等式 $\\lambda_i > t_i$ 进行拒绝，因此 $\\lambda_i = t_i$ 不会导致拒绝。\n- 为了可复现性，使用指定的种子和允许显式设定种子的 NumPy 伪随机数生成器接口。\n\n测试套件。在以下三个测试用例上实现并运行您的程序。每个用例指定了 $(n,p,r,s,q,m,\\text{seed}_{\\text{obs}},\\text{seed}_{\\text{null}})$：\n- 用例 A (纯噪声，高百分位数): $(n=\\,$$300$$,\\,p=\\,$$10$$,\\,r=\\,$$0$$,\\,s=\\,$$0.0$$,\\,q=\\,$$0.99$$,\\,m=\\,$$2000$$,\\,\\text{seed}_{\\text{obs}}=\\,$$20240501$$,\\,\\text{seed}_{\\text{null}}=\\,$$424242$$)$.\n- 用例 B (清晰的低秩信号): $(n=\\,$$300$$,\\,p=\\,$$10$$,\\,r=\\,$$3$$,\\,s=\\,$$4.0$$,\\,q=\\,$$0.95$$,\\,m=\\,$$1000$$,\\,\\text{seed}_{\\text{obs}}=\\,$$20240502$$,\\,\\text{seed}_{\\text{null}}=\\,$$424242$$)$.\n- 用例 C (边界尺度的尖峰): $(n=\\,$$80$$,\\,p=\\,$$20$$,\\,r=\\,$$1$$,\\,s=\\,$$1.0$$,\\,q=\\,$$0.95$$,\\,m=\\,$$2000$$,\\,\\text{seed}_{\\text{obs}}=\\,$$20240503$$,\\,\\text{seed}_{\\text{null}}=\\,$$777$$)$.\n\n输出规范。您的程序必须在标准输出上产生仅一行内容，由一个左方括号、A、B、C 三个测试用例的整数结果（按顺序排列，用逗号分隔且无空格）、以及一个右方括号组成。例如，如果用例 A、B、C 的结果分别为 $k_A$、$k_B$ 和 $k_C$，那么该单行必须是字符“ [ ”，后跟 $k_A$，然后是“,”，然后是 $k_B$，然后是“,”，然后是 $k_C$，最后是“ ] ”，不得有任何额外的字符、空格或换行。",
            "solution": "该问题要求实现一种称为平行分析的统计方法，以确定数据集中显著主成分的数量。此过程被构建为一种逐分量的假设检验。解决方案涉及生成一个特定的“加尖”数据集，计算其主成分特征值，然后将这些特征值与在纯噪声零模型下通过蒙特卡洛模拟得出的阈值进行比较。\n\n**1. 科学原理与背景**\n\n主成分分析（PCA）是统计学和机器学习中降维的基石。给定一个具有 $n$ 个观测和 $p$ 个特征的数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$，假设该矩阵已中心化（即列均值为零），PCA 通过寻找一组新的正交轴（称为主成分）来寻求一个低维表示。这些分量按顺序排列，第一个分量捕获数据中可能的最大方差，第二个分量捕获最大的剩余方差，依此类推。\n\n在数学上，主成分是样本协方差矩阵 $S = \\frac{1}{n-1} X^{\\top} X$ 的特征向量。每个分量捕获的方差由相应的特征值给出。设 $S$ 的特征值按降序排列为：$\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$。数据中的总方差为 $\\mathrm{trace}(S) = \\sum_{i=1}^p \\lambda_i$。\n\n应用 PCA 的一个关键问题是要保留多少个分量。保留太少可能会丢弃重要信号，而保留太多则可能引入噪声，违背了降维的目的。平行分析通过为纯粹由随机抽样变异性所预期的特征值大小建立一个基线来解决这个问题。\n\n对于每个分量 $i$ 的零假设 $H_{0,i}$ 是，在第 $i$ 个分量中，不存在超出由独立同分布（IID）高斯噪声构成的数据集中所能观察到的结构或信号。具体来说，零模型假定数据矩阵为 $Z \\in \\mathbb{R}^{n \\times p}$，其中每个条目 $Z_{jk} \\sim \\mathcal{N}(0,1)$。这种噪声矩阵的协方差矩阵的特征值遵循一个已知的理论分布（在渐近极限下为 Marchenko-Pastur 定律，对于有限样本为 Wishart 分布），但在实际应用中通常通过经验模拟来获得。\n\n平行分析将来自实际数据的观测特征值 $\\lambda_i$ 与来自零模型的特征值分布进行比较。如果一个分量的特征值在统计上大于从噪声中预期的值，则该分量被认为是显著的。该问题将这种比较形式化为假设检验：对于每个分量 $i$，如果 $\\lambda_i > t_i$，我们就拒绝 $H_{0,i}$，其中 $t_i$ 是第 $i$ 个特征值的模拟零分布的一个高百分位数（例如，$q=0.95$ 分位数）。被拒绝的假设的数量即为要保留的分量数量。\n\n**2. 算法流程**\n\n对于每个测试用例，实现过程遵循一系列定义明确的步骤，这些步骤由参数 $(n, p, r, s, q, m, \\text{seed}_{\\text{obs}}, \\text{seed}_{\\text{null}})$ 指定。\n\n**步骤 2.1：观测数据分析**\n\n1.  **生成数据矩阵：** 生成一个初始矩阵 $W \\in \\mathbb{R}^{n \\times p}$，其每个条目都独立地从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取。此过程使用 $\\text{seed}_{\\text{obs}}$ 作为种子以保证可复现性。\n2.  **引入信号（尖峰模型）：** 为模拟低秩信号结构，将 $W$ 的前 $r$ 列乘以因子 $\\sqrt{1+s}$。这样就得到了观测数据矩阵 $X$。如果 $r=0$ 或 $s=0$，此步骤不引入任何信号，此时 $X=W$。\n3.  **数据中心化：** 通过减去各自的均值来中心化 $X$ 的列。将此中心化矩阵记为 $X_c$。对于任意列 $j$，有 $X_{c, \\cdot j} = X_{\\cdot j} - \\bar{X}_{\\cdot j}$，其中 $\\bar{X}_{\\cdot j} = \\frac{1}{n}\\sum_{i=1}^n X_{ij}$。\n4.  **计算特征值：** 计算无偏样本协方差矩阵 $S = \\frac{1}{n-1} X_c^{\\top} X_c$。然后计算 $S$ 的特征值并按降序排序，以生成观测特征值向量 $(\\lambda_1, \\lambda_2, \\dots, \\lambda_p)$。\n\n**步骤 2.2：零分布的模拟**\n\n1.  **初始化蒙特卡洛模拟：** 准备一个数组来存储来自 $m$ 次模拟的特征值，形成一个大小为 $m \\times p$ 的矩阵。使用 $\\text{seed}_{\\text{null}}$ 初始化一个单独的随机数生成器。\n2.  **重复循环：** 以下过程重复 $m$ 次：\n    a. 生成一个零数据矩阵 $Z \\in \\mathbb{R}^{n \\times p}$，其条目从 $\\mathcal{N}(0,1)$ 中独立同分布地抽取。\n    b. 将矩阵 $Z$ 中心化以生成 $Z_c$。\n    c. 构建样本协方差矩阵 $S_0 = \\frac{1}{n-1} Z_c^{\\top} Z_c$。\n    d. 计算 $S_0$ 的特征值，按降序排序，并作为一行存储在结果数组中。\n\n**步骤 2.3：假设检验与最终计数**\n\n1.  **计算阈值：** 对于每个分量 $i \\in \\{1,\\dots,p\\}$，从该分量的 $m$ 个模拟特征值集合中计算出第 $q$ 个经验分位数 $t_i$。这将产生一个阈值向量 $(t_1, t_2, \\dots, t_p)$。\n2.  **比较和计数：** 将每个观测特征值 $\\lambda_i$ 与其对应的阈值 $t_i$ 进行比较。保留的分量数是满足严格不等式 $\\lambda_i > t_i$ 的索引 $i$ 的总数。这个整数计数是该测试用例的最终结果。\n\n此过程将应用于所提供的三个测试用例中的每一个。\n-   **用例 A** ($n=300, p=10, r=0, s=0.0, q=0.99, m=2000, \\text{seed}_{\\text{obs}}=20240501, \\text{seed}_{\\text{null}}=424242$): 一个纯噪声的用例。当 $q=0.99$ 时，我们预计偶然保留的分量会非常少（如果有的话）（大约是 $p=10$ 个分量中的 $1\\%$，所以可能是 $0$ 或 $1$）。\n-   **用例 B** ($n=300, p=10, r=3, s=4.0, q=0.95, m=1000, \\text{seed}_{\\text{obs}}=20240502, \\text{seed}_{\\text{null}}=424242$): 一个具有 $r=3$ 个加尖分量的强信号用例。尖峰强度 $s=4.0$ 相当大，因此我们预计分析将正确识别出这 $3$ 个分量。\n-   **用例 C** ($n=80, p=20, r=1, s=1.0, q=0.95, m=2000, \\text{seed}_{\\text{obs}}=20240503, \\text{seed}_{\\text{null}}=777$): 一个边界尺度的用例，具有单个较弱的尖峰（$r=1, s=1.0$）以及相对于特征数 $p=20$ 而言较小的样本量 $n=80$。这在一个更具挑战性的情景中测试了该方法的检测能力。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_parallel_analysis(n, p, r, s, q, m, seed_obs, seed_null):\n    \"\"\"\n    Performs a parallel analysis for a single test case.\n\n    Args:\n        n (int): Number of observations.\n        p (int): Number of features.\n        r (int): Number of 'spiked' features with signal.\n        s (float): Signal strength parameter for the spike.\n        q (float): Quantile to use for the threshold.\n        m (int): Number of Monte Carlo simulations for the null distribution.\n        seed_obs (int): Pseudorandom seed for generating observed data.\n        seed_null (int): Pseudorandom seed for generating null data.\n\n    Returns:\n        int: The number of components retained based on the parallel analysis.\n    \"\"\"\n    # Initialize the random number generator for observed data\n    rng_obs = np.random.default_rng(seed_obs)\n\n    # Step 1: Analyze observed data\n    # Generate base matrix W from N(0,1)\n    W = rng_obs.standard_normal((n, p))\n    \n    # Create the observed data matrix X by spiking the first r columns\n    X = W.copy()\n    if r > 0:\n        signal_factor = np.sqrt(1 + s)\n        X[:, :r] *= signal_factor\n        \n    # Center the data\n    X_centered = X - X.mean(axis=0)\n    \n    # Compute the sample covariance matrix\n    # S = (X_centered.T @ X_centered) / (n - 1)\n    # Using np.cov is robust and standard. It expects variables in rows,\n    # so we transpose X_centered. `ddof=1` for unbiased estimator.\n    S = np.cov(X_centered, rowvar=False, ddof=1)\n    \n    # Compute eigenvalues of S. eigvalsh is for Hermitian (real-symmetric) matrices.\n    # It returns eigenvalues in ascending order, so we reverse them.\n    observed_eigenvalues = np.linalg.eigvalsh(S)[::-1]\n\n    # Step 2: Simulate the null distribution\n    # Initialize the random number generator for null simulations\n    rng_null = np.random.default_rng(seed_null)\n    \n    null_eigenvalues_matrix = np.zeros((m, p))\n    \n    for i in range(m):\n        # Generate null data matrix Z from N(0,1)\n        Z = rng_null.standard_normal((n, p))\n        \n        # Center the null data\n        Z_centered = Z - Z.mean(axis=0)\n        \n        # Compute the null sample covariance matrix\n        S_null = np.cov(Z_centered, rowvar=False, ddof=1)\n        \n        # Compute and store sorted eigenvalues\n        null_eigenvalues_matrix[i, :] = np.linalg.eigvalsh(S_null)[::-1]\n\n    # Step 3: Calculate thresholds and make decisions\n    # Compute the q-th quantile for each component across all simulations\n    thresholds = np.quantile(null_eigenvalues_matrix, q, axis=0)\n    \n    # Count the number of observed eigenvalues that exceed the threshold\n    retained_components_count = np.sum(observed_eigenvalues > thresholds)\n    \n    return retained_components_count\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite: (n, p, r, s, q, m, seed_obs, seed_null)\n    test_cases = [\n        # Case A: Noise-only, high percentile\n        (300, 10, 0, 0.0, 0.99, 2000, 20240501, 424242),\n        # Case B: Clear low-rank signal\n        (300, 10, 3, 4.0, 0.95, 1000, 20240502, 424242),\n        # Case C: Boundary-scale spike\n        (80, 20, 1, 1.0, 0.95, 2000, 20240503, 777),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_parallel_analysis(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format \"[k_A,k_B,k_C]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}