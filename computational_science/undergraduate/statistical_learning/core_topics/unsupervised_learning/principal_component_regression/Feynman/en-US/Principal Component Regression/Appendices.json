{
    "hands_on_practices": [
        {
            "introduction": "Before we dive into complex algorithms for model selection, it is crucial to understand the core principle—and potential pitfall—of Principal Component Regression. This exercise uses the simplifying assumption of orthogonal predictors to reveal that PCR is fundamentally an unsupervised method; it selects components based on the variance within the predictors ($X$), without considering their relationship to the response ($y$). Understanding this is the first step toward appreciating why more sophisticated, supervised selection methods are necessary .",
            "id": "3160754",
            "problem": "Consider a centered predictor matrix $X \\in \\mathbb{R}^{n \\times p}$ with columns that are pairwise orthogonal so that $X^\\top X$ is diagonal, and a centered response vector $Y \\in \\mathbb{R}^{n}$. Principal Component Analysis (PCA) is performed on $X$ and Principal Component Regression (PCR) is defined as regressing $Y$ on the first $k$ principal component scores corresponding to the $k$ largest eigenvalues of the empirical covariance of $X$. Assume ordinary least squares is used in the regression stage. Based on the fundamental definitions of PCA and PCR, select all statements that are correct.\n\nA. When the columns of $X$ are orthogonal, PCR with $k$ components is equivalent to ordinary least squares on the $k$ columns of $X$ having the largest sample variance.\n\nB. In the orthogonal-columns setting, selecting $k$ by choosing the $k$ largest-variance principal components is desirable for prediction because high-variance directions typically have higher signal about $Y$.\n\nC. There exist data-generating processes where the most predictive directions lie in low-variance columns of $X$, so variance-based selection can harm out-of-sample prediction performance.\n\nD. If the noise variance in $Y$ is high and the columns of $X$ are orthogonal, using all $p$ columns always yields lower out-of-sample prediction error than using any $k < p$ components.",
            "solution": "The user wants me to validate the problem statement and, if valid, solve the problem by evaluating each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Predictor matrix $X \\in \\mathbb{R}^{n \\times p}$ is centered.\n- The columns of $X$ are pairwise orthogonal.\n- $X^\\top X$ is a diagonal matrix.\n- Response vector $Y \\in \\mathbb{R}^{n}$ is centered.\n- Principal Component Analysis (PCA) is performed on $X$.\n- Principal Component Regression (PCR) is defined as regressing $Y$ on the first $k$ principal component (PC) scores.\n- The first $k$ PC scores are those corresponding to the $k$ largest eigenvalues of the empirical covariance of $X$.\n- The regression is an ordinary least squares (OLS) regression.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in the theory of statistical learning. PCA, PCR, OLS, and the properties of orthogonal matrices are standard mathematical and statistical concepts. The setup is a theoretical case designed to test conceptual understanding. It is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. The definitions are clear, and the task is to evaluate a set of statements based on these definitions, which can be done unambiguously through mathematical derivation.\n- **Objectivity**: The problem statement is expressed in objective, technical language, free from subjective or biased phrasing.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness**: None.\n2.  **Non-Formalizable or Irrelevant**: The problem is directly relevant to principal component regression and is formalizable.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and consistent. The statement that columns are orthogonal implies $X^\\top X$ is diagonal.\n4.  **Unrealistic or Infeasible**: The assumption of orthogonal predictors is a simplification for theoretical analysis but is not scientifically impossible or invalid.\n5.  **Ill-Posed or Poorly Structured**: None.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires understanding the specific consequences of orthogonality on the PCA procedure, which is a non-trivial insight.\n7.  **Outside Scientific Verifiability**: All statements can be mathematically verified.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed to the solution.\n\n### Derivation and Option Analysis\n\nLet the columns of the predictor matrix be $X = [x_1 | x_2 | \\dots | x_p]$, where each $x_j \\in \\mathbb{R}^n$.\nThe problem states that the columns are centered, meaning the mean of each column is $0$. The columns are also pairwise orthogonal, which means $x_i^\\top x_j = 0$ for all $i \\neq j$.\n\nThe empirical covariance matrix of the predictors is given by $S = \\frac{1}{n-1} X^\\top X$.\nLet's compute the matrix $X^\\top X$:\n$$\n(X^\\top X)_{ij} = x_i^\\top x_j\n$$\nDue to orthogonality, this matrix is diagonal:\n$$\nX^\\top X = \\begin{pmatrix}\nx_1^\\top x_1 & 0 & \\dots & 0 \\\\\n0 & x_2^\\top x_2 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & x_p^\\top x_p\n\\end{pmatrix} = \\text{diag}(\\|x_1\\|^2, \\|x_2\\|^2, \\dots, \\|x_p\\|^2)\n$$\nThe covariance matrix is therefore also diagonal:\n$$\nS = \\frac{1}{n-1} \\text{diag}(\\|x_1\\|^2, \\|x_2\\|^2, \\dots, \\|x_p\\|^2)\n$$\nThe sample variance of the $j$-th predictor column $x_j$ is $\\text{Var}(x_j) = \\frac{1}{n-1} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2$. Since the columns are centered, $\\bar{x}_j=0$, so $\\text{Var}(x_j) = \\frac{1}{n-1} \\sum_{i=1}^n x_{ij}^2 = \\frac{1}{n-1} \\|x_j\\|^2$.\nThus, the diagonal entries of $S$ are the sample variances of the columns of $X$:\n$$\nS = \\text{diag}(\\text{Var}(x_1), \\text{Var}(x_2), \\dots, \\text{Var}(x_p))\n$$\nIn PCA, the principal component directions (or loadings) are the eigenvectors of the covariance matrix $S$. Since $S$ is a diagonal matrix, its eigenvectors are the standard basis vectors $v_j = e_j = (0, \\dots, 1, \\dots, 0)^\\top$ for $j=1, \\dots, p$. The corresponding eigenvalues are the diagonal entries themselves, $\\lambda_j = \\text{Var}(x_j)$.\n\nThe principal component (PC) scores are computed as $Z = XV$, where $V$ is the matrix whose columns are the eigenvectors $v_j$. In this case, $V = I_p$, the $p \\times p$ identity matrix.\nTherefore, the PC scores are $Z = XI_p = X$.\nThis means that when the predictor columns are orthogonal, the PC scores are simply the original predictor columns, $z_j = x_j$. The variance of the $j$-th principal component is the variance of the $j$-th original column.\n\nPCR proceeds by regressing $Y$ on the first $k$ PC scores corresponding to the $k$ largest eigenvalues. The eigenvalues are the variances of the columns. So, PCR selects the $k$ original columns of $X$ that have the largest sample variances and performs OLS of $Y$ on these selected columns.\n\nNow, we evaluate each statement.\n\n**A. When the columns of $X$ are orthogonal, PCR with $k$ components is equivalent to ordinary least squares on the $k$ columns of $X$ having the largest sample variance.**\nAs derived above, in the case of orthogonal columns, the principal components are the original columns of $X$. The ordering of the principal components by their corresponding eigenvalues is an ordering by the sample variance of the original columns. Therefore, selecting the first $k$ principal components is identical to selecting the $k$ columns of $X$ with the largest sample variances. The regression step is then OLS on these selected columns. The statement is a direct and accurate description of this process.\n**Verdict: Correct.**\n\n**B. In the orthogonal-columns setting, selecting $k$ by choosing the $k$ largest-variance principal components is desirable for prediction because high-variance directions typically have higher signal about $Y$.**\nThis statement presents the rationale behind using variance as a selection criterion. The core claim is that \"high-variance directions typically have higher signal about $Y$\". \"Signal\" in this context refers to the predictive power, i.e., the strength of the relationship between a predictor and the response $Y$. This relationship is measured by quantities like correlation or the magnitude of the true regression coefficient, not by the predictor's own variance. PCR, by its definition, is an unsupervised dimension reduction technique; it selects components based solely on the structure within $X$, without any regard to $Y$. The assumption that high variance in $X$ implies high relevance for predicting $Y$ is a heuristic that does not hold in general. As will be shown in the analysis of option C, it is easy to construct counterexamples. Since the justification provided (\"because...\") is not a scientifically guaranteed principle but a fallible heuristic, the statement is flawed from a rigorous perspective.\n**Verdict: Incorrect.**\n\n**C. There exist data-generating processes where the most predictive directions lie in low-variance columns of $X$, so variance-based selection can harm out-of-sample prediction performance.**\nThis statement is the formal counterpoint to the flawed heuristic in statement B. Let's construct a simple example.\nLet $p=2$ and $X=[x_1 | x_2]$ have centered and orthogonal columns. Let the true data-generating process be $Y = \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$.\nSuppose $\\text{Var}(x_1) = 100$ and $\\text{Var}(x_2) = 0.1$. This means $\\|x_1\\|^2$ is much larger than $\\|x_2\\|^2$.\nNow, suppose the true coefficients are $\\beta_1 = 0$ and $\\beta_2 = 10$. In this scenario, the entire predictive signal for $Y$ comes from $x_2$, the low-variance column. The high-variance column $x_1$ is pure noise with respect to $Y$.\nIf we perform PCR with $k=1$, the method will select the component with the highest variance, which is $x_1$. The resulting model will regress $Y$ on $x_1$. Since the true relationship is $Y = 10 x_2 + \\epsilon$, this model will have very poor predictive performance. An ideal feature selection method would have chosen $x_2$. By selecting based on variance, PCR has discarded the only useful predictor. This demonstrates that variance-based selection can be harmful to prediction.\n**Verdict: Correct.**\n\n**D. If the noise variance in $Y$ is high and the columns of $X$ are orthogonal, using all $p$ columns always yields lower out-of-sample prediction error than using any $k < p$ components.**\nThis statement concerns the bias-variance tradeoff. Using all $p$ columns constitutes a full OLS model. Since the true data-generating process is unknown, the full model is unbiased for the parameters it estimates, but its predictions suffer from variance due to the estimation error in the coefficients. The prediction error is a sum of squared bias and variance.\nFull OLS model ($k=p$): Low (or zero) bias, high variance.\nPCR model ($k<p$): This is a reduced model, which is equivalent to OLS on a subset of predictors. It introduces bias by setting coefficients of dropped predictors to zero. However, it reduces the overall variance of the prediction because fewer parameters are estimated from the noisy data.\nThe variance of the OLS coefficient estimates is proportional to the noise variance of $Y$, say $\\sigma^2$. Specifically, for orthogonal predictors, $\\text{Var}(\\hat{\\beta}_j) = \\sigma^2/\\|x_j\\|^2$. When $\\sigma^2$ is high, the variance term becomes large and can dominate the prediction error. In such cases, methods that reduce model variance (like PCR, ridge regression, or LASSO) often outperform OLS in terms of out-of-sample prediction error, even though they introduce bias. A reduced model ($k<p$) can have a lower prediction error than the full model if the reduction in variance is greater than the increase in squared bias. The statement that the full model is \"always\" better is a strong claim that contradicts this fundamental principle. High noise variance is precisely the scenario where a reduced model is often preferred.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Now that we recognize the need for a selection criterion based on predictive performance, we turn to a powerful tool: cross-validation. While re-fitting a model numerous times can be computationally expensive, statistical theory often provides elegant shortcuts. This exercise guides you through the derivation of the Predicted Residual Sum of Squares (PRESS) statistic, an efficient analytical formula for Leave-One-Out Cross-Validation (LOOCV) that avoids repeated model fitting, making it a practical tool for selecting the optimal number of components .",
            "id": "1951651",
            "problem": "A materials engineering team is investigating the properties of a new type of high-entropy alloy. Their goal is to build a predictive model for the alloy's fracture toughness, denoted by $y$, based on a set of $p$ quantitative predictor variables. These predictors, represented by a vector $\\mathbf{x} \\in \\mathbb{R}^p$, include the concentrations of constituent elements and various thermomechanical processing parameters.\n\nThe team has collected a dataset of $n$ alloy samples. Let the $n \\times p$ matrix $\\mathbf{X}$ contain the predictor values for all samples and the $n \\times 1$ vector $\\mathbf{y}$ contain the corresponding fracture toughness measurements. For simplicity, assume that the columns of $\\mathbf{X}$ and the vector $\\mathbf{y}$ have been centered to have a mean of zero.\n\nDue to the nature of alloy design, many of the predictors are expected to be highly correlated. To address this multicollinearity, the team decides to use Principal Component Regression (PCR), a two-stage procedure. First, Principal Component Analysis (PCA) is performed on the predictor matrix $\\mathbf{X}$ to obtain a new set of orthogonal variables called principal components. Second, the response $\\mathbf{y}$ is regressed on a subset of these principal components.\n\nThe PCR model using the first $k$ principal components is defined as follows:\n1. The principal component scores are computed as $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$, where $\\mathbf{V}_k$ is the $p \\times k$ matrix whose columns are the first $k$ principal component loading vectors (i.e., the eigenvectors of the sample covariance matrix of $\\mathbf{X}$).\n2. A linear regression of $\\mathbf{y}$ on $\\mathbf{Z}_k$ is performed to get the fitted values $\\hat{\\mathbf{y}}^{(k)}$.\n\nA critical step in PCR is selecting the optimal number of components, $k$. A common method is to choose the $k$ that minimizes the prediction error estimated via cross-validation. The team plans to use Leave-One-Out Cross-Validation (LOOCV). The performance metric is the Predicted Residual Sum of Squares (PRESS), defined as $\\text{PRESS}(k) = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)}^{(k)})^2$, where $\\hat{y}_{(-i)}^{(k)}$ is the prediction for the $i$-th observation from a PCR model with $k$ components that was trained on all data except observation $i$.\n\nWhile this procedure is well-defined, re-fitting the PCR model $n$ times (once for each left-out observation) is computationally inefficient. Your task is to derive an efficient, analytical expression for $\\text{PRESS}(k)$ that avoids this repeated fitting.\n\nExpress the $\\text{PRESS}(k)$ statistic in a form that can be calculated using only a single PCR model fit on the full dataset of $n$ observations. Your final expression should involve the ordinary residuals, $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$, and the diagonal elements, $h_{ii}^{(k)}$, of the appropriate projection matrix (or \"hat matrix\") associated with the PCR model.",
            "solution": "The goal is to find a computationally efficient formula for the Predicted Residual Sum of Squares (PRESS) statistic for a Principal Component Regression (PCR) model with $k$ components.\n\nFirst, we recall a general result for any linear model of the form $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$, where $\\mathbf{H}$ is the projection or \"hat\" matrix. The prediction for the $i$-th observation from a model fitted without that observation, $\\hat{y}_{(-i)}$, can be related to the results from the model fitted on all data. The Leave-One-Out Cross-Validation (LOOCV) residual for observation $i$ is given by the well-known formula:\n$$ y_i - \\hat{y}_{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} $$\nwhere $\\hat{y}_i$ is the fitted value for observation $i$ from the full model, $e_i = y_i - \\hat{y}_i$ is the ordinary residual, and $h_{ii}$ is the $i$-th diagonal element of the hat matrix $\\mathbf{H}$.\n\nThe PRESS statistic is the sum of the squares of these LOOCV residuals:\n$$ \\text{PRESS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)})^2 = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 $$\nTo apply this to our PCR problem, we need to identify the specific hat matrix $\\mathbf{H}^{(k)}$ for a PCR model with $k$ components and find its diagonal elements $h_{ii}^{(k)}$.\n\nIn PCR, the model is built using the principal component scores, $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$, as the predictors. The linear regression of the centered response $\\mathbf{y}$ on $\\mathbf{Z}_k$ yields the estimated coefficients:\n$$ \\hat{\\boldsymbol{\\theta}}_k = (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\nThe fitted values $\\hat{\\mathbf{y}}^{(k)}$ are then given by:\n$$ \\hat{\\mathbf{y}}^{(k)} = \\mathbf{Z}_k \\hat{\\boldsymbol{\\theta}}_k = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\nBy comparing this to the general form $\\hat{\\mathbf{y}}^{(k)} = \\mathbf{H}^{(k)} \\mathbf{y}$, we can identify the hat matrix for PCR with $k$ components as:\n$$ \\mathbf{H}^{(k)} = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T $$\nThe columns of the scores matrix $\\mathbf{Z}_k$ are the score vectors $\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_k$. A key property of principal components is that these score vectors are orthogonal to each other: $\\mathbf{z}_j^T \\mathbf{z}_m = 0$ for $j \\neq m$. This means the matrix $\\mathbf{Z}_k^T \\mathbf{Z}_k$ is a $k \\times k$ diagonal matrix:\n$$ \\mathbf{Z}_k^T \\mathbf{Z}_k = \\text{diag}(\\mathbf{z}_1^T\\mathbf{z}_1, \\mathbf{z}_2^T\\mathbf{z}_2, \\dots, \\mathbf{z}_k^T\\mathbf{z}_k) = \\text{diag}(\\|\\mathbf{z}_1\\|^2, \\|\\mathbf{z}_2\\|^2, \\dots, \\|\\mathbf{z}_k\\|^2) $$\nwhere $\\|\\mathbf{z}_j\\|^2 = \\sum_{i=1}^{n} z_{ij}^2$ is the squared Euclidean norm of the $j$-th score vector.\n\nThe inverse matrix is also diagonal:\n$$ (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} = \\text{diag}(\\|\\mathbf{z}_1\\|^{-2}, \\|\\mathbf{z}_2\\|^{-2}, \\dots, \\|\\mathbf{z}_k\\|^{-2}) $$\nSubstituting this back into the expression for $\\mathbf{H}^{(k)}$, we see that the hat matrix is a sum of rank-one projection matrices:\n$$ \\mathbf{H}^{(k)} = \\begin{pmatrix} \\mathbf{z}_1 & \\dots & \\mathbf{z}_k \\end{pmatrix} \\begin{pmatrix} \\|\\mathbf{z}_1\\|^{-2} & & \\\\ & \\ddots & \\\\ & & \\|\\mathbf{z}_k\\|^{-2} \\end{pmatrix} \\begin{pmatrix} \\mathbf{z}_1^T \\\\ \\vdots \\\\ \\mathbf{z}_k^T \\end{pmatrix} = \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} $$\nWe are interested in the diagonal elements of this matrix, $h_{ii}^{(k)}$. The $(i,i)$-th element of the outer product matrix $\\mathbf{z}_j \\mathbf{z}_j^T$ is simply $z_{ij}^2$, where $z_{ij}$ is the $i$-th element of the vector $\\mathbf{z}_j$. Therefore, the diagonal elements of $\\mathbf{H}^{(k)}$ are:\n$$ h_{ii}^{(k)} = \\left( \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} \\right)_{ii} = \\sum_{j=1}^{k} \\frac{(\\mathbf{z}_j \\mathbf{z}_j^T)_{ii}}{\\|\\mathbf{z}_j\\|^2} = \\sum_{j=1}^{k} \\frac{z_{ij}^2}{\\sum_{l=1}^{n} z_{lj}^2} $$\nNow we have all the pieces. The PRESS statistic for a $k$-component PCR model can be computed by first fitting the model to the full dataset to obtain the ordinary residuals $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$ and the principal component scores $z_{ij}$. Then, these values are combined using the derived formula.\n\nThe final expression for $\\text{PRESS}(k)$ is:\n$$ \\text{PRESS}(k) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i^{(k)}}{1 - h_{ii}^{(k)}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^2 $$\nThis formula allows for the efficient calculation of the PRESS statistic for any $k$ after performing the PCA and regression just once on the full dataset.",
            "answer": "$$\\boxed{\\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^{2}}$$"
        },
        {
            "introduction": "This final practice brings our conceptual and theoretical understanding into a practical coding environment. You will compare two common strategies for choosing the number of components, $k$: the simple heuristic of selecting components that explain a high percentage of variance in the predictors, versus the more robust method of minimizing cross-validation error. By simulating scenarios where the most predictive components have low variance, this exercise provides concrete, hands-on proof of the principles explored in the previous practices and solidifies your ability to apply PCR effectively .",
            "id": "3160814",
            "problem": "You are given the task of implementing Principal Component Regression (PCR) and comparing two criteria for selecting the number of components $k$: selecting $k$ by a target cumulative variance explained threshold (e.g., $\\tau = 0.95$) versus selecting $k$ by minimizing $K$-fold Cross-Validation (CV) Mean Squared Error (MSE). Your implementation must be grounded in core definitions and well-tested formulas: Principal Component Analysis (PCA), Singular Value Decomposition (SVD), linear regression by least squares, and $K$-fold Cross-Validation.\n\nFundamental base:\n- Principal Component Analysis (PCA) transforms a centered data matrix $X \\in \\mathbb{R}^{n \\times p}$ into orthogonal principal components ordered by variance. If $X$ has thin Singular Value Decomposition (SVD) $X = U S V^\\top$, then the principal axes are the columns of $V$, the singular values are the diagonal entries of $S$, and the sample covariance eigenvalues are proportional to $S^2$. The explained variance ratio of the first $k$ components is the cumulative sum of the leading $k$ entries of $S^2$ divided by the sum of all entries of $S^2$.\n- Principal Component Regression (PCR) fits a linear model of a response $y \\in \\mathbb{R}^n$ using the scores of the first $k$ principal components. To include an intercept, center $X$ and $y$, regress $y$ on the first $k$ principal component score columns by least squares, and then add back the training mean of $y$ to predictions.\n- $K$-fold Cross-Validation (CV) partitions indices $\\{1,\\dots,n\\}$ into $K$ folds, trains on $K-1$ folds and validates on the held-out fold, and averages the validation Mean Squared Error (MSE) across folds. The MSE of predictions $\\hat{y}$ against $y$ is $\\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$ on a validation set of size $m$.\n\nDefinitions and tasks:\n1. Implement PCR as follows:\n   - Center $X$ by subtracting the column means and center $y$ by subtracting its mean on the training set.\n   - Compute the thin SVD $X_{\\text{train}} = U S V^\\top$ on the centered training predictors.\n   - Form principal component scores $Z_{\\text{train}} = X_{\\text{train}} V_{[:,1:k]}$ using the first $k$ columns of $V$.\n   - Fit the coefficients on $Z_{\\text{train}}$ by least squares. Predict on a test set by centering the test $X$ with the training means, projecting onto the same $V_{[:,1:k]}$, and adding back the training mean of $y$ to the centered predictions.\n2. Select $k$ by variance explained: compute the singular values $S$ of the centered full $X$, form $S^2$, and choose the smallest $k$ such that the cumulative ratio of the first $k$ entries of $S^2$ to the sum of all entries of $S^2$ is at least $\\tau$.\n3. Select $k$ by CV MSE: for each $k \\in \\{1,2,\\dots,p\\}$, compute the average $K$-fold CV MSE of PCR with $k$ components, and choose the $k$ that minimizes it. In case of ties, select the smallest $k$.\n4. For each scenario, report the tuple $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$, where $k_{\\text{var}}$ is the selection by variance explained and $k_{\\text{cv}}$ is the selection by CV MSE. For consistency, round the MSE values to $4$ decimal places.\n\nData generation model:\n- Let $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$ be a vector of target eigenvalues. Generate samples $x_i \\in \\mathbb{R}^p$ as $x_i = g_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$, where $g_i \\sim \\mathcal{N}(\\mathbf{0}, I_p)$ independently. This yields data with covariance $\\operatorname{diag}(\\boldsymbol{\\lambda})$ and principal axes equal to the coordinate axes (identity rotation).\n- Define a coefficient vector $\\beta \\in \\mathbb{R}^p$ in the principal component basis by a vector $\\beta_{\\text{rot}} \\in \\mathbb{R}^p$ specifying the contribution of each principal component to the response. Use $\\beta = \\beta_{\\text{rot}}$ directly because the rotation is the identity.\n- Generate the response as $y_i = x_i^\\top \\beta + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ independently.\n\nTest suite:\nImplement the above with $K$-fold Cross-Validation using $K = 5$. Use the identity rotation for principal axes and the following four scenarios. For each scenario, set the random seed for data generation and fold assignment to the specified seed.\n\n- Scenario $1$ (signal aligned with top-variance component):\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (1, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 42$\n- Scenario $2$ (signal aligned with lowest-variance component):\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (0, 0, 0, 0, 0, 0, 0, 1)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 7$\n- Scenario $3$ (flat variance spectrum, high noise, multi-component signal):\n  - $n = 300$, $p = 12$\n  - $\\boldsymbol{\\lambda} = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)$\n  - $\\beta_{\\text{rot}} = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 1.5$\n  - $\\tau = 0.95$\n  - seed $= 123$\n- Scenario $4$ (threshold at one, low noise, signal on first two components):\n  - $n = 120$, $p = 10$\n  - $\\boldsymbol{\\lambda} = (5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.2)$\n  - $\\beta_{\\text{rot}} = (1, 1, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.1$\n  - $\\tau = 1.0$\n  - seed $= 99$\n\nAngle units are not involved. No physical units are involved. The final outputs are numerical and must be reported as lists of integers and floats as follows.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each scenario contributes one list $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$, where the MSE entries are rounded to $4$ decimal places. For example, the output should look like $[[1,4,0.1234,0.0567],[\\dots],\\dots]$.",
            "solution": "The problem requires the implementation of Principal Component Regression (PCR) and a comparison of two distinct methods for selecting the optimal number of principal components, $k$. The first method selects $k$ based on a cumulative explained variance threshold, $\\tau$. The second method selects $k$ by minimizing the Mean Squared Error (MSE) estimated via $K$-fold Cross-Validation (CV). The comparison is performed across four simulated scenarios with specified data generation parameters.\n\n### 1. Data Generation Model\n\nThe data are generated from a linear model. For a specified number of samples $n$ and predictors $p$, the predictor matrix $X \\in \\mathbb{R}^{n \\times p}$ is constructed such that its population covariance matrix is diagonal with entries given by the vector $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$. Specifically, each row $x_i^\\top$ of $X$ is generated as:\n$$ x_i = G_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}}) $$\nwhere $G_i$ is a row vector of $p$ independent samples from the standard normal distribution, $\\mathcal{N}(0, 1)$. This is equivalent to generating a matrix $G \\in \\mathbb{R}^{n \\times p}$ with i.i.d. $\\mathcal{N}(0, 1)$ entries and setting $X = G \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$. The population principal axes of this data distribution are the standard basis vectors (an identity rotation).\n\nThe response vector $y \\in \\mathbb{R}^n$ is generated according to the linear model:\n$$ y_i = x_i^\\top \\beta + \\epsilon_i $$\nwhere $\\beta \\in \\mathbb{R}^p$ is the vector of true regression coefficients and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ are i.i.d. error terms. The coefficient vector $\\beta$ is given directly as $\\beta = \\beta_{\\text{rot}}$, where $\\beta_{\\text{rot}}$ specifies the signal strength along each principal component axis.\n\n### 2. Principal Component Regression (PCR) Algorithm\n\nPCR is a two-stage procedure that first reduces the dimensionality of the predictors using Principal Component Analysis (PCA) and then performs linear regression on the resulting components. The procedure, as specified for a training set $(X_{\\text{train}}, y_{\\text{train}})$ and a test set $X_{\\text{test}}$, is as follows:\n\n1.  **Data Centering**: The training data are centered to have zero mean. The test data are centered using the means from the training data to prevent information leakage.\n    $$ \\bar{x}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} x_{\\text{train},i}, \\quad \\bar{y}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} y_{\\text{train},i} $$\n    $$ X_{\\text{c,train}} = X_{\\text{train}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top, \\quad y_{\\text{c,train}} = y_{\\text{train}} - \\bar{y}_{\\text{train}} $$\n    $$ X_{\\text{c,test}} = X_{\\text{test}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top $$\n\n2.  **Principal Component Extraction**: The principal axes are found by computing the Singular Value Decomposition (SVD) of the centered training predictor matrix:\n    $$ X_{\\text{c,train}} = U S V^\\top $$\n    The columns of $V \\in \\mathbb{R}^{p \\times p}$ are the principal axes (or loading vectors).\n\n3.  **Score Projection**: The training data are projected onto the first $k$ principal axes to obtain the score matrix $Z_{\\text{train},k} \\in \\mathbb{R}^{n_{\\text{train}} \\times k}$:\n    $$ Z_{\\text{train},k} = X_{\\text{c,train}} V_k $$\n    where $V_k \\in \\mathbb{R}^{p \\times k}$ contains the first $k$ columns of $V$.\n\n4.  **Regression on Scores**: A linear model is fit using ordinary least squares to regress the centered response $y_{\\text{c,train}}$ onto the scores $Z_{\\text{train},k}$, yielding coefficients $\\hat{\\theta}_k \\in \\mathbb{R}^k$:\n    $$ \\hat{\\theta}_k = (Z_{\\text{train},k}^\\top Z_{\\text{train},k})^{-1} Z_{\\text{train},k}^\\top y_{\\text{c,train}} $$\n\n5.  **Prediction**: Predictions for the test set are made by first projecting the centered test data $X_{\\text{c,test}}$ onto the same $k$ principal axes to get test scores $Z_{\\text{test},k}$, then applying the fitted coefficients $\\hat{\\theta}_k$, and finally adding back the mean of the training response:\n    $$ Z_{\\text{test},k} = X_{\\text{c,test}} V_k $$\n    $$ \\hat{y}_{\\text{test}} = Z_{\\text{test},k} \\hat{\\theta}_k + \\bar{y}_{\\text{train}} $$\n\n### 3. Component Selection I: Cumulative Variance Explained\n\nThis method selects the number of components $k$ based on the proportion of variance in the predictor variables that is captured. It is applied to the full dataset $X$.\n\n1.  Center the full data matrix: $X_c = X - \\mathbf{1}\\bar{x}^\\top$.\n2.  Compute its SVD: $X_c = U S V^\\top$. The diagonal entries of $S$ are the singular values, $s_1 \\ge s_2 \\ge \\dots \\ge s_p \\ge 0$.\n3.  The variance explained by the $j$-th principal component is proportional to $s_j^2$. The cumulative explained variance ratio (CEVR) for the first $k$ components is:\n    $$ \\text{CEVR}(k) = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^p s_j^2} $$\n4.  Given a threshold $\\tau$, the number of components $k_{\\text{var}}$ is chosen as the minimum $k$ that satisfies the threshold:\n    $$ k_{\\text{var}} = \\min \\{ k \\in \\{1, 2, \\dots, p\\} \\mid \\text{CEVR}(k) \\ge \\tau \\} $$\n\n### 4. Component Selection II: $K$-Fold Cross-Validation\n\nThis method selects $k$ by directly estimating the prediction error of the PCR model for each possible value of $k$.\n\n1.  The dataset of $n$ samples is partitioned into $K$ disjoint folds of approximately equal size. For this problem, $K=5$.\n2.  For each candidate number of components $k \\in \\{1, 2, \\dots, p\\}$:\n    a. An outer loop iterates through each fold $j=1, \\dots, K$.\n    b. In each iteration, the $j$-th fold is designated as the validation set $(X_{\\text{val}}, y_{\\text{val}})$, and the remaining $K-1$ folds constitute the training set $(X_{\\text{train}}, y_{\\text{train}})$.\n    c. A PCR model with $k$ components is trained on $(X_{\\text{train}}, y_{\\text{train}})$.\n    d. The trained model is used to make predictions $\\hat{y}_{\\text{val}}$ on the validation set $X_{\\text{val}}$.\n    e. The Mean Squared Error (MSE) is computed for that fold: $\\text{MSE}_j(k) = \\frac{1}{n_{\\text{val}}} \\sum_{i \\in \\text{fold }j} (y_{\\text{val},i} - \\hat{y}_{\\text{val},i})^2$.\n3.  After iterating through all folds, the cross-validation MSE for $k$ components is the average of the fold-specific MSEs:\n    $$ \\text{CV-MSE}(k) = \\frac{1}{K} \\sum_{j=1}^K \\text{MSE}_j(k) $$\n4.  The optimal number of components $k_{\\text{cv}}$ is the one that minimizes this average error. Ties are broken by choosing the smallest $k$.\n    $$ k_{\\text{cv}} = \\operatorname{argmin}_{k \\in \\{1, \\dots, p\\}} \\text{CV-MSE}(k) $$\n\n### 5. Implementation and Evaluation\n\nFor each scenario, we first generate the data $(X, y)$. We then apply both selection criteria. The variance-based method yields $k_{\\text{var}}$. The cross-validation procedure is run for all $k \\in \\{1, \\dots, p\\}$, producing a list of CV-MSE values. From this list, we identify $k_{\\text{cv}}$ as the minimizer. The final reported values are $k_{\\text{var}}$, $k_{\\text{cv}}$, the pre-computed CV-MSE for $k_{\\text{var}}$ (i.e., $\\text{CV-MSE}(k_{\\text{var}})$), and the minimum CV-MSE (i.e., $\\text{CV-MSE}(k_{\\text{cv}})$). This provides a direct comparison of the predictive performance of the two selected models. The random seed is used to ensure reproducibility of both the data generation and the fold assignments in cross-validation.",
            "answer": "```python\nimport numpy as np\n\ndef generate_data(n, p, lambdas, beta_rot, sigma_eps, seed):\n    \"\"\"Generates data for a given scenario.\"\"\"\n    rng = np.random.default_rng(seed)\n    # Generate standard normal data\n    G = rng.normal(size=(n, p))\n    # Scale columns to achieve target covariance\n    sqrt_lambdas = np.sqrt(lambdas)\n    X = G * sqrt_lambdas\n    \n    # Generate response\n    # Since principal axes are standard basis, beta = beta_rot\n    beta = beta_rot\n    epsilon = rng.normal(scale=sigma_eps, size=n)\n    y = X @ beta + epsilon\n    \n    return X, y\n\ndef pcr_predict(X_train, y_train, X_test, k):\n    \"\"\"\n    Fits a PCR model on training data and predicts on test data.\n    \"\"\"\n    # 1. Centering\n    x_mean = np.mean(X_train, axis=0)\n    y_mean = np.mean(y_train)\n    \n    X_train_c = X_train - x_mean\n    y_train_c = y_train - y_mean\n    X_test_c = X_test - x_mean\n    \n    # 2. PCA on training data\n    # The problem specifies thin SVD X = U S V^T.\n    # np.linalg.svd returns U, s, Vt where s is a 1D array of singular values.\n    try:\n        _, s, Vt = np.linalg.svd(X_train_c, full_matrices=False)\n    except np.linalg.LinAlgError:\n         # In rare cases with highly collinear data, SVD can fail.\n         # This shouldn't happen with the specified data generation, but as a fallback:\n         return np.full(X_test.shape[0], y_mean) # Predict the mean\n         \n    V = Vt.T\n    \n    # 3. Project onto first k components\n    V_k = V[:, :k]\n    Z_train_k = X_train_c @ V_k\n    \n    # 4. Fit linear model on scores\n    # Using lstsq for numerical stability\n    theta_k, _, _, _ = np.linalg.lstsq(Z_train_k, y_train_c, rcond=None)\n    \n    # 5. Predict on test data\n    Z_test_k = X_test_c @ V_k\n    y_pred_c = Z_test_k @ theta_k\n    y_pred = y_pred_c + y_mean\n    \n    return y_pred\n\ndef select_k_variance(X, tau):\n    \"\"\"\n    Selects k based on cumulative explained variance.\n    \"\"\"\n    # Center the data\n    X_c = X - np.mean(X, axis=0)\n    \n    # SVD\n    _, s, _ = np.linalg.svd(X_c, full_matrices=False)\n    \n    # Explained variance ratios\n    s_squared = s**2\n    total_variance = np.sum(s_squared)\n    \n    if total_variance == 0:\n        return 1\n\n    cumulative_variance_ratio = np.cumsum(s_squared) / total_variance\n    \n    # Find smallest k such that ratio >= tau\n    k_var_candidates = np.where(cumulative_variance_ratio >= tau)[0]\n    if len(k_var_candidates) == 0:\n        # This occurs if tau=1.0 and there's numerical imprecision\n        k_var = X.shape[1]\n    else:\n        k_var = k_var_candidates[0] + 1 # +1 for 1-based indexing\n        \n    return k_var\n\ndef select_k_cv(X, y, p, K, seed):\n    \"\"\"\n    Selects k by K-fold Cross-Validation and returns CV MSE for all k.\n    \"\"\"\n    n = X.shape[0]\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    \n    folds = np.array_split(indices, K)\n    \n    cv_mses = []\n    \n    for k in range(1, p + 1):\n        fold_mses = []\n        for j in range(K):\n            val_indices = folds[j]\n            train_indices = np.concatenate([folds[i] for i in range(K) if i != j])\n            \n            X_train, y_train = X[train_indices], y[train_indices]\n            X_val, y_val = X[val_indices], y[val_indices]\n            \n            y_pred = pcr_predict(X_train, y_train, X_val, k)\n            \n            mse = np.mean((y_val - y_pred)**2)\n            fold_mses.append(mse)\n            \n        avg_mse = np.mean(fold_mses)\n        cv_mses.append(avg_mse)\n        \n    cv_mses = np.array(cv_mses)\n    # Get k that minimizes CV MSE, breaking ties by choosing smallest k.\n    # np.argmin() naturally does this.\n    k_cv = np.argmin(cv_mses) + 1 # +1 for 1-based indexing\n    \n    return k_cv, cv_mses\n\ndef solve():\n    \"\"\"\n    Main function to run the scenarios and print results.\n    \"\"\"\n    scenarios = [\n        {'n': 200, 'p': 8, 'lambdas': np.array([9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05]), \n         'beta_rot': np.array([1, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 0.2, 'tau': 0.95, 'seed': 42},\n        {'n': 200, 'p': 8, 'lambdas': np.array([9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05]), \n         'beta_rot': np.array([0, 0, 0, 0, 0, 0, 0, 1]), 'sigma_eps': 0.2, 'tau': 0.95, 'seed': 7},\n        {'n': 300, 'p': 12, 'lambdas': np.ones(12), \n         'beta_rot': np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 1.5, 'tau': 0.95, 'seed': 123},\n        {'n': 120, 'p': 10, 'lambdas': np.array([5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.2]), \n         'beta_rot': np.array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 0.1, 'tau': 1.0, 'seed': 99}\n    ]\n    \n    K = 5\n    results = []\n\n    for params in scenarios:\n        X, y = generate_data(params['n'], params['p'], params['lambdas'], \n                             params['beta_rot'], params['sigma_eps'], params['seed'])\n        \n        # Method 1: Variance Explained Threshold\n        k_var = select_k_variance(X, params['tau'])\n        \n        # Method 2: Cross-Validation\n        k_cv, cv_mses_all_k = select_k_cv(X, y, params['p'], K, params['seed'])\n        \n        # Get MSEs for the chosen k values\n        mse_for_k_var = cv_mses_all_k[k_var - 1]\n        mse_for_k_cv = cv_mses_all_k[k_cv - 1]\n        \n        results.append([k_var, k_cv, mse_for_k_var, mse_for_k_cv])\n        \n    # Format output as specified\n    formatted_results = []\n    for res in results:\n        # Rounding is handled by the format specifier\n        s = f\"[{res[0]},{res[1]},{res[2]:.4f},{res[3]:.4f}]\"\n        formatted_results.append(s)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}