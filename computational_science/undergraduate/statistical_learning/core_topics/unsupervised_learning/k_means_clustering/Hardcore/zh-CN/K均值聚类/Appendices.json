{
    "hands_on_practices": [
        {
            "introduction": "这个练习是掌握 K-means 算法的基石。通过从头开始实现经典的 Lloyd 算法，你将不仅仅是理解理论，更是要亲手构建它，并分析其在不同数据集上的性能表现。此练习将引导你推导算法的计算复杂度，并通过具体的“最优”和“最差”场景测试，直观地感受算法收敛速度的差异，从而深刻理解其运行效率和局限性。",
            "id": "3134960",
            "problem": "考虑用于 $k$-均值聚类的 Lloyd 算法。该算法迭代执行两个步骤：一个分配步骤，使用 $\\mathbb{R}^d$ 中的欧几里得距离将每个数据点映射到最近的质心；以及一个更新步骤，将每个质心重新计算为其簇中当前分配点的算术平均值。令 $n$ 表示点的数量，$k$ 表示簇的数量，$d$ 表示维度，$t$ 表示直到收敛（定义为分配不再改变）所需的迭代次数。分析的基本依据应为以下经过充分检验的事实：在每个分配步骤中，对于 $n$ 个点中的每一个，都会使用欧几里得范数计算其到 $\\mathbb{R}^d$ 中 $k$ 个质心中每一个的距离；在每个更新步骤中，每个质心都通过计算分配给它的点的均值来进行更新。\n\n任务：\n- 从这些事实以及欧几里得距离和算术平均值的定义出发，推导 Lloyd 算法单次迭代的计算复杂度，并用此推导来证明整个算法直到收敛的总渐近复杂度为 $O(nkdt)$。\n- 按照上述描述实现 $k$-均值聚类的 Lloyd 算法，使用 $\\mathbb{R}^d$ 中的欧几里得距离，并基于分配不变来判断收敛。如果在更新过程中有任何簇变为空，则确定性地将该质心重新初始化为当前距其所分配质心欧几里得距离最远的数据点（这能保持算法进展并避免未定义的均值）。\n- 在以下确定性测试套件上构建并运行。下文提到的所有角度都必须以弧度为单位进行解释。\n\n测试套件：\n1. $\\mathbb{R}^2$ 中的对抗性圆形数据集，旨在展示缓慢的收敛行为。其中 $k=2$，质心初始化为单位圆上两个相邻的点，以创建一个狭窄的初始决策边界，该边界会随着质心的移动而迁移。对于每个 $n \\in \\{16,32,64\\}$，在单位圆上以角度 $\\theta_i = \\frac{2\\pi i}{n}$（对于 $i = 0,1,\\dots,n-1$）均匀间隔地构造 $n$ 个点，用笛卡尔坐标表示为 $(\\cos\\theta_i,\\sin\\theta_i)$。将质心初始化为角度为 $\\theta_0 = 0$ 和 $\\theta_1 = \\frac{2\\pi}{n}$ 的点。\n2. $\\mathbb{R}^2$ 中一个分离良好的“理想路径”数据集，旨在展示快速收敛。其中 $n=200, k=2$，由两个各向同性的高斯斑点组成：通过设定伪随机数生成器的种子并从中心在 $(0,0)$、每个坐标方差为 $1$ 的正态分布中取一个固定序列，确定性地采样 $100$ 个点；以及中心在 $(10,10)$、方差相同的 $100$ 个点。为保持确定性，将质心初始化为数据集顺序中的前两个点。\n3. $\\mathbb{R}^1$ 中一个包含重复相同值的边缘案例数据集，其中 $n=9, k=3$，所有点均等于 $0$。将质心初始化为 $-1$, $0$ 和 $1$。\n\n对于每个测试用例，运行 Lloyd 算法直至收敛，并记录：\n- 迭代次数 $t$。\n- 乘积 $n \\cdot k \\cdot d \\cdot t$（作为一个整数），这是一个标量，反映了由复杂度推导所隐含的渐近操作计数因子。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个双元素列表 $[t, n \\cdot k \\cdot d \\cdot t]$，顺序与测试套件中的顺序相同，例如：$[[t_1, n_1 k_1 d_1 t_1],[t_2, n_2 k_2 d_2 t_2],[t_3, n_3 k_3 d_3 t_3],[t_4, n_4 k_4 d_4 t_4]]$。\n\n注意：不涉及物理单位。所有角度均以弧度为单位指定。要求的输出是整数或整数列表，最终输出格式必须是上文描述的精确的单行方括号列表。",
            "solution": "### 第一部分：计算复杂度推导\n\nLloyd 算法的总计算复杂度是迭代次数 $t$ 与单次迭代复杂度的乘积。一次迭代包括两个连续的步骤：分配步骤和更新步骤。我们基于所提供的先验事实来推导每一步的复杂度。\n\n令 $n$ 为数据点数量，$k$ 为簇的数量，$d$ 为数据空间 $\\mathbb{R}^d$ 的维度。\n\n**分配步骤复杂度**\n\n问题陈述中提到，在分配步骤中，“对于 $n$ 个点中的每一个，都会使用欧几里得范数计算其到 $\\mathbb{R}^d$ 中 $k$ 个质心中每一个的距离”。\n\n1.  **距离计算**：一个数据点 $\\mathbf{x} = (x_1, \\dots, x_d)$ 和一个质心 $\\mathbf{c} = (c_1, \\dots, c_d)$ 之间的欧几里得距离由 $D(\\mathbf{x}, \\mathbf{c}) = \\sqrt{\\sum_{i=1}^{d} (x_i - c_i)^2}$ 给出。为计算此值，我们执行：\n    *   $d$ 次减法 ($x_i - c_i$)。\n    *   $d$ 次乘法 (平方)。\n    *   $d-1$ 次加法 (求和)。\n    *   $1$ 次平方根运算。\n    算术运算的总数为 $d+d+(d-1)+1 = 3d$。因此，单次距离计算的复杂度与维度 $d$ 成正比，即 $O(d)$。请注意，为了找到*最近*的质心，我们可以使用欧几里得距离的平方，$D^2(\\mathbf{x}, \\mathbf{c}) = \\sum_{i=1}^{d} (x_i - c_i)^2$，这样可以避免计算成本更高的平方根运算，但其复杂度仍为 $O(d)$。\n\n2.  **总分配成本**：对于 $n$ 个数据点中的每一个，我们计算它到所有 $k$ 个质心的距离。这需要进行 $n \\times k$ 次距离计算。因此，所有距离计算的总成本为 $n \\times k \\times O(d) = O(nkd)$。\n\n3.  **寻找最小值**：对于每个数据点，在计算其到 $k$ 个质心的距离后，我们必须找到这 $k$ 个值中的最小值来确定分配。这需要 $k-1$ 次比较，其复杂度为 $O(k)$。由于必须对所有 $n$ 个点都执行此操作，因此寻找所有最小值的总复杂度为 $n \\times O(k) = O(nk)$。\n\n4.  **分配步骤总体复杂度**：分配步骤的总复杂度是距离计算和寻找最小值成本的总和：$O(nkd) + O(nk)$。由于 $d \\ge 1$，$O(nkd)$ 项占主导地位。因此，分配步骤的复杂度为 $O(nkd)$。\n\n**更新步骤复杂度**\n\n问题陈述中提到，在更新步骤中，“每个质心都通过计算分配给它的点的均值来进行更新”。\n\n1.  **均值计算**：假设簇 $j$ 包含 $n_j$ 个点，记为 $\\{\\mathbf{x}_{j,1}, \\dots, \\mathbf{x}_{j, n_j}\\}$。新的质心 $\\mathbf{c}_j'$ 是它们的算术平均值：$\\mathbf{c}_j' = \\frac{1}{n_j} \\sum_{i=1}^{n_j} \\mathbf{x}_{j,i}$。\n    *   为了计算总和 $\\sum_{i=1}^{n_j} \\mathbf{x}_{j,i}$，我们需要对 $n_j$ 个维度为 $d$ 的向量求和。这需要 $(n_j - 1) \\times d$ 次加法。其复杂度为 $O(n_j d)$。\n    *   为了计算最终的均值，我们将得到的和向量除以标量 $n_j$。这涉及 $d$ 次除法，其复杂度为 $O(d)$。\n    *   更新簇 $j$ 的一个质心的总复杂度为 $O(n_j d) + O(d) = O(n_j d)$。\n\n2.  **总更新成本**：我们必须对所有 $k$ 个簇执行此更新。总复杂度是每个簇成本的总和：$\\sum_{j=1}^{k} O(n_j d)$。这可以写成 $O(d \\sum_{j=1}^{k} n_j)$。\n    由于 $n$ 个数据点中的每一个都被精确地分配到一个簇中，所以所有簇中点的数量之和等于总点数：$\\sum_{j=1}^{k} n_j = n$。\n    因此，更新步骤的复杂度为 $O(nd)$。\n\n处理空簇的特殊规则要求找到与其分配的质心距离最远的数据点。这涉及对 $n$ 个预先计算好的距离进行扫描，这是一个 $O(n)$ 的操作。这个成本仅在簇为空时产生，并且被标准更新过程的 $O(nd)$ 成本所主导（因为 $d \\ge 1$），因此它不改变更新步骤的渐近复杂度。\n\n**总复杂度**\n\nLloyd 算法单次迭代的复杂度是分配步骤和更新步骤复杂度的总和：\n$$ C_{\\text{iteration}} = C_{\\text{assignment}} + C_{\\text{update}} = O(nkd) + O(nd) $$\n由于 $k \\ge 1$，$O(nkd)$ 项主导了 $O(nd)$。因此，单次迭代的复杂度为 $O(nkd)$。\n\n如果算法运行 $t$ 次迭代后收敛，则总计算复杂度为：\n$$ C_{\\text{total}} = t \\times C_{\\text{iteration}} = t \\times O(nkd) = O(nkdt) $$\n这证明了整个算法的渐近复杂度为 $O(nkdt)$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef lloyds_algorithm(data, k, initial_centroids):\n    \"\"\"\n    Performs k-means clustering using Lloyd's algorithm.\n\n    Args:\n        data (np.ndarray): The dataset of shape (n, d).\n        k (int): The number of clusters.\n        initial_centroids (np.ndarray): The initial centroids of shape (k, d).\n\n    Returns:\n        int: The number of iterations 't' until convergence.\n    \"\"\"\n    n, d = data.shape\n    centroids = np.copy(initial_centroids)\n    # Initialize assignments with a sentinel value that won't match any valid assignment\n    assignments = np.full(n, -1, dtype=int)\n    \n    t = 0\n    while True:\n        # --- Assignment Step ---\n        # Compute squared Euclidean distances from each point to each centroid\n        # cdist is efficient for this. 'sqeuclidean' avoids sqrt calculation.\n        dist_sq = cdist(data, centroids, 'sqeuclidean')\n        \n        # Assign each point to the closest centroid\n        # np.argmin handles ties by picking the first occurrence.\n        new_assignments = np.argmin(dist_sq, axis=1)\n        \n        # --- Convergence Check ---\n        # If assignments have not changed, the algorithm has converged.\n        if np.array_equal(new_assignments, assignments):\n            break\n            \n        assignments = new_assignments\n        \n        # --- Update Step ---\n        \n        # Per problem spec, find the point to use for reinitializing empty clusters.\n        # This is the point with the largest distance to ITS OWN assigned centroid.\n        # We use the distances and assignments from the current step to find this point.\n        dists_to_assigned_centroids = dist_sq[np.arange(n), assignments]\n        farthest_point_idx = np.argmax(dists_to_assigned_centroids)\n        reinit_point = data[farthest_point_idx]\n        \n        new_centroids = np.zeros_like(centroids)\n        for j in range(k):\n            # Select all points assigned to the current cluster\n            cluster_points = data[assignments == j]\n            \n            # If the cluster is not empty, compute the mean\n            if len(cluster_points) > 0:\n                new_centroids[j] = np.mean(cluster_points, axis=0)\n            # If the cluster is empty, reinitialize its centroid\n            else:\n                new_centroids[j] = reinit_point\n\n        centroids = new_centroids\n        t += 1 # Increment iteration count after a full (assign + update) cycle\n\n    return t\n\ndef solve():\n    \"\"\"\n    Generates test cases, runs Lloyd's algorithm, and prints the results.\n    \"\"\"\n    test_cases = []\n    \n    # Test Suite Part 1: Adversarial circle datasets\n    k1 = 2\n    d1 = 2\n    for n1 in [16, 32, 64]:\n        thetas = 2 * np.pi * np.arange(n1) / n1\n        points = np.column_stack([np.cos(thetas), np.sin(thetas)])\n        \n        # Initial centroids at angles 0 and 2*pi/n\n        c_theta_0 = 0.0\n        c_theta_1 = 2 * np.pi / n1\n        initial_centroids = np.array([\n            [np.cos(c_theta_0), np.sin(c_theta_0)],\n            [np.cos(c_theta_1), np.sin(c_theta_1)]\n        ])\n        test_cases.append({\n            \"name\": f\"Circle n={n1}\",\n            \"data\": points, \"k\": k1, \"d\": d1, \"n\": n1,\n            \"initial_centroids\": initial_centroids\n        })\n\n    # Test Suite Part 2: Well-separated Gaussian blobs\n    n2 = 200\n    k2 = 2\n    d2 = 2\n    np.random.seed(0) # For deterministic results\n    blob1 = np.random.normal(loc=0.0, scale=1.0, size=(100, d2))\n    blob2 = np.random.normal(loc=10.0, scale=1.0, size=(100, d2))\n    points2 = np.vstack([blob1, blob2])\n    # Initialize centroids to the first two points of the dataset\n    initial_centroids2 = points2[:k2]\n    test_cases.append({\n        \"name\": \"Gaussians\",\n        \"data\": points2, \"k\": k2, \"d\": d2, \"n\": n2,\n        \"initial_centroids\": initial_centroids2\n    })\n    \n    # Test Suite Part 3: Edge-case with identical points\n    n3 = 9\n    k3 = 3\n    d3 = 1\n    points3 = np.zeros((n3, d3))\n    initial_centroids3 = np.array([[-1.0], [0.0], [1.0]])\n    test_cases.append({\n        \"name\": \"Edge-case\",\n        \"data\": points3, \"k\": k3, \"d\": d3, \"n\": n3,\n        \"initial_centroids\": initial_centroids3\n    })\n    \n    results = []\n    for case in test_cases:\n        t = lloyds_algorithm(case[\"data\"], case[\"k\"], case[\"initial_centroids\"])\n        n, k, d = case[\"n\"], case[\"k\"], case[\"d\"]\n        product = n * k * d * t\n        results.append([t, product])\n    \n    # The final print statement must produce the exact required format.\n    # str() on a list of lists creates a string like '[[...], [...]]'\n    # .replace(\" \", \"\") removes spaces to match the required dense format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了基础 K-means 算法后，我们来探讨一个重要的扩展：如何处理数据点重要性不同的情况。这个练习将指导你从 K-means 的目标函数出发，通过微积分推导出带权重的质心更新规则[@problem_id:3_134938]。你将通过理论证明和编程实践，验证“为数据点賦予权重”与“复制数据点”这两种直观想法之间的深刻等价性，从而加深对算法核心优化目标的理解。",
            "id": "3134938",
            "problem": "设计并实现一个程序，在 $k$-means 聚类的背景下，形式化并测试点多重性的影响。从核心定义出发，推导如何将赋予数据点的多重性表示为这些点的整数复制或非负权重。然后实现相应的算法，以验证等价性并测量质心随着权重的变化而发生的漂移。\n\n基本基础：\n- 在一个有限数据集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ 上使用标准的 $k$-means 目标函数，其定义为最小化簇内平方距离之和，采用硬分配 $r_{ik} \\in \\{0,1\\}$，使得对于每个 $i$ 都有 $\\sum_{k=1}^K r_{ik} = 1$。\n- 使用平方欧几里得范数和多元微积分的标准原理，包括将梯度置零以找到最小值点。\n- 使用整数多重性对应于将一个数据点重复整数次的解释。\n\n任务：\n- 从带有非负权重 $\\{w_i\\}_{i=1}^n$ 的目标函数出发，推导在固定分配下最小化该目标的质心更新规则。使用上述基本基础来证明每一步，不要使用任何预先给出的最终公式或捷径。解释为什么当距离为平方欧几里得距离时，硬聚类分配不依赖于 $\\{w_i\\}$。\n- 仅使用定义和求和的线性性质，证明带有整数权重 $\\{w_i\\}$ 的加权目标函数与一个新数据集上的无权目标函数完全等价，其中新数据集里的每个 $x_i$ 被复制了 $w_i$ 次。并由此得出结论：在这种情况下，质心更新是相同的。\n- 实现确定性过程，给定一个数据集、整数权重和初始质心，在加权和复制-无权两种形式下，执行单个 $k$-means 分配步骤，然后进行一次质心更新。使用平方欧几里得距离，且不要随机化任何方面。\n\n测试套件和要求的输出：\n实现以下四个测试用例。对每个用例，计算指定的结果。程序必须将四个结果汇总到单行输出中，该输出包含一个用方括号括起来的逗号分隔列表，没有多余的空格。\n\n- 测试用例 1（$\\mathbb{R}^1$ 中的单簇等价性）：数据 $x_1 = 0$, $x_2 = 2$，整数权重为 $w_1 = 3$, $w_2 = 1$。分别通过加权更新和将 $x_1$ 复制三次、$x_2$ 复制一次后取无权平均值这两种方式计算质心。返回一个布尔值，指示两个质心之间的绝对差是否小于 $\\varepsilon = 10^{-12}$。\n\n- 测试用例 2（$\\mathbb{R}^2$ 中带有零权重边缘情况的单簇等价性）：数据 $x_1 = (0,0)$, $x_2 = (2,2)$, $x_3 = (10,10)$，整数权重为 $w_1 = 1$, $w_2 = 1$, $w_3 = 0$。分别通过加权更新和根据权重进行复制（注意权重为零的点被复制零次）这两种方式计算质心。返回一个布尔值，指示两个质心之差的欧几里得范数是否小于 $\\varepsilon = 10^{-12}$。\n\n- 测试用例 3（在 $\\mathbb{R}^2$ 中，$K = 2$ 的单次 $k$-means 迭代等价性）：数据点\n$x_1 = (0,0)$, $x_2 = (1,0)$, $x_3 = (0,1)$, $x_4 = (5,5)$, $x_5 = (6,5)$, $x_6 = (5,6)$\n，整数权重为\n$w_1 = 1$, $w_2 = 2$, $w_3 = 1$, $w_4 = 1$, $w_5 = 1$, $w_6 = 3$。\n使用固定的初始质心 $\\mu_1^{(0)} = (0,0)$ 和 $\\mu_2^{(0)} = (5,5)$。通过最小化到当前质心的平方欧几里得距离来执行一次分配步骤（注意此步骤不依赖于 $\\{w_i\\}$），然后进行一次质心更新。分别使用原始数据的加权更新方法，以及在每个 $x_i$ 被复制 $w_i$ 次的数据集上使用无权更新方法，各执行一次。返回一个布尔值，指示对应质心之差的欧几里得范数是否均小于 $\\varepsilon = 10^{-12}$。\n\n- 测试用例 4（当单个权重变化时，$K = 1$ 的质心漂移）：考虑 $n=5$ 个点 $(0,0)$ 的副本，每个副本的单位权重为 $1$，以及一个位于 $(3,0)$ 的离群点，其权重 $w$ 在集合 $\\{0,1,2,5,10\\}$ 中变化。对于每个指定的 $w$，计算 $K = 1$ 的更新后质心（这简化为所有点的加权平均值），然后计算该质心到原点的欧几里得距离。返回这五个距离的列表，每个距离四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 程序必须生成单行输出，其中包含一个用方括号括起来的、无空格的逗号分隔列表。列表中的元素必须按顺序为：测试用例 1 的布尔值，测试用例 2 的布尔值，测试用例 3 的布尔值，以及测试用例 4 的列表（该列表本身用方括号括起来，其中的条目用逗号分隔，无空格）。例如，结构必须是 $[b_1,b_2,b_3,[d_1,d_2,d_3,d_4,d_5]]$，其中 $b_j$ 是布尔值，$d_j$ 是精确格式化为 $6$ 位小数的浮点数。\n\n实现约束：\n- 全程使用平方欧几里得距离。\n- 所有整数权重均为非负。\n- 使用 $\\varepsilon = 10^{-12}$ 的严格容差进行相等性检查。\n- 不允许随机性、外部输入、文件或网络访问。",
            "solution": "### **理论推导**\n\n本节提供问题陈述所要求的数学推导。\n\n**1. 加权质心更新规则的推导**\n\n加权 $k$-means 的目标是找到质心集合 $\\{\\mu_k\\}_{k=1}^K$ 和分配 $\\{r_{ik}\\}$，以最小化簇内平方距离之和，其中每个点的贡献由权重 $w_i$ 进行缩放。目标函数 $J$ 为：\n$$\nJ(\\{\\mu_k\\}, \\{r_{ik}\\}) = \\sum_{i=1}^n \\sum_{k=1}^K r_{ik} w_i \\|x_i - \\mu_k\\|^2\n$$\n$k$-means 算法通过迭代方式最小化此函数。在质心更新步骤（M步）中，分配 $\\{r_{ik}\\}$ 是固定的，我们寻求找到最小化 $J$ 的质心 $\\{\\mu_k\\}$。由于不同簇的项是独立的，我们可以分别为每个簇最小化目标函数。\n\n我们专注于更新单个质心 $\\mu_k$。我们只需要考虑目标函数中依赖于 $\\mu_k$ 的部分，我们将其表示为 $J_k$：\n$$\nJ_k = \\sum_{i=1}^n r_{ik} w_i \\|x_i - \\mu_k\\|^2\n$$\n这里，$x_i \\in \\mathbb{R}^d$ 且 $\\mu_k \\in \\mathbb{R}^d$。平方欧几里得范数为 $\\|x_i - \\mu_k\\|^2 = (x_i - \\mu_k)^T(x_i - \\mu_k)$。为了找到最小值，我们计算 $J_k$ 关于 $\\mu_k$ 的梯度并将其置零。\n$$\n\\nabla_{\\mu_k} J_k = \\nabla_{\\mu_k} \\sum_{i=1}^n r_{ik} w_i (x_i - \\mu_k)^T(x_i - \\mu_k)\n$$\n利用梯度算子的线性和恒等式 $\\nabla_v (u-v)^T(u-v) = -2(u-v)$，我们得到：\n$$\n\\nabla_{\\mu_k} J_k = \\sum_{i=1}^n r_{ik} w_i \\nabla_{\\mu_k} (\\|x_i - \\mu_k\\|^2) = \\sum_{i=1}^n r_{ik} w_i [-2(x_i - \\mu_k)]\n$$\n将梯度置零以找到最优的 $\\mu_k$：\n$$\n-2 \\sum_{i=1}^n r_{ik} w_i (x_i - \\mu_k) = 0\n$$\n假设分配给簇 $k$ 的点并非都具有零权重，我们可以除以 $-2$：\n$$\n\\sum_{i=1}^n r_{ik} w_i x_i - \\sum_{i=1}^n r_{ik} w_i \\mu_k = 0\n$$\n$$\n\\left(\\sum_{i=1}^n r_{ik} w_i\\right) \\mu_k = \\sum_{i=1}^n r_{ik} w_i x_i\n$$\n求解 $\\mu_k$，我们得到质心更新规则：\n$$\n\\mu_k = \\frac{\\sum_{i=1}^n r_{ik} w_i x_i}{\\sum_{i=1}^n r_{ik} w_i}\n$$\n这表明一个簇的最优质心是分配给该簇的数据点的加权平均值。如果分母为零（即，没有具有正权重的点被分配到该簇），则质心未定义，通常保持不变或重新初始化。\n\n**分配步骤中权重无关性的解释：**\n在分配步骤（E步）中，质心 $\\{\\mu_k\\}$ 是固定的。对于每个数据点 $x_i$，我们必须将其分配给能最小化其对总目标 $J$ 贡献的簇。点 $x_i$ 的贡献是 $\\sum_{k=1}^K r_{ik} w_i \\|x_i - \\mu_k\\|^2$。由于只有一个 $r_{ik}$ 为 $1$（其他为 $0$），此贡献变为 $w_i \\|x_i - \\mu_c\\|^2$，其中 $c$ 是所分配簇的索引。\n\n为了最小化给定点 $x_i$ 的这一项，我们必须选择能够最小化 $w_i \\|x_i - \\mu_c\\|^2$ 的簇索引 $c$。\n- 如果 $w_i > 0$，权重 $w_i$ 是一个正的常数因子。最小化 $w_i \\|x_i - \\mu_c\\|^2$ 等价于最小化 $\\|x_i - \\mu_c\\|^2$（或其平方根，即欧几里得距离）。\n- 如果 $w_i = 0$，点 $x_i$ 对目标的贡献始终为零，无论其如何分配。因此，分配是任意的，对目标函数或后续的质心更新没有影响。\n\n在任何一种情况下，将点分配到簇的决策规则仅取决于找到欧几里得距离上最近的质心，而与权重 $w_i$ 的具体值无关。\n\n**2. 整数权重等价性的证明**\n\n我们现在将证明，对于一组非负整数权重 $\\{w_i\\}_{i=1}^n$，加权 $k$-means 目标与一个标准（无权）$k$-means 目标完全相同，后者的作用数据集是每个点 $x_i$ 被复制了 $w_i$ 次构成的新数据集。\n\n设原始数据集为 $X = \\{x_i\\}_{i=1}^n$。设整数权重为 $W = \\{w_i\\}_{i=1}^n$，其中 $w_i \\in \\{0, 1, 2, \\dots\\}$。构建一个新的、复制的数据集 $X' = \\{x'_j\\}_{j=1}^{N'}$，其中 $N' = \\sum_{i=1}^n w_i$。这个新数据集包含每个点 $x_i$ 的 $w_i$ 个副本。我们可以定义一个映射 $\\phi: \\{1, \\dots, N'\\} \\to \\{1, \\dots, n\\}$，使得 $x'_j = x_{\\phi(j)}$。映射到给定 $i$ 的索引 $j$ 的集合（即原像 $\\phi^{-1}(i)$）的大小为 $|\\phi^{-1}(i)| = w_i$。\n\n复制数据集 $X'$ 的无权 $k$-means 目标为：\n$$\nJ'_{unweighted} = \\sum_{j=1}^{N'} \\sum_{k=1}^K r'_{jk} \\|x'_j - \\mu_k\\|^2\n$$\n其中 $r'_{jk} \\in \\{0,1\\}$ 是 $X'$ 中点的硬分配。\n\n对于复制数据集中的任何点 $x'_j$，其位置为 $x_{\\phi(j)}$。$x'_j$ 的聚类分配是通过找到欧几里得距离最近的质心 $\\mu_k$ 来确定的。由于给定点 $x_i$ 的所有副本都在同一位置，它们都将被分配到同一个簇。也就是说，对于任何满足 $\\phi(j_1) = \\phi(j_2) = i$ 的 $j_1, j_2$，我们都会有 $r'_{j_1,k} = r'_{j_2,k} = r_{ik}$，对所有 $k=1, \\dots, K$ 均成立。\n\n我们现在可以通过根据原始点 $x_i$ 对关于 $j$ 的求和进行分组，来重写目标 $J'_{unweighted}$：\n$$\nJ'_{unweighted} = \\sum_{i=1}^n \\sum_{j \\in \\phi^{-1}(i)} \\left( \\sum_{k=1}^K r'_{jk} \\|x'_j - \\mu_k\\|^2 \\right)\n$$\n在关于 $j \\in \\phi^{-1}(i)$ 的内层求和中，我们知道 $x'_j = x_i$ 且 $r'_{jk} = r_{ik}$。将其代入：\n$$\nJ'_{unweighted} = \\sum_{i=1}^n \\sum_{j \\in \\phi^{-1}(i)} \\left( \\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2 \\right)\n$$\n项 $\\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2$ 对于集合 $\\phi^{-1}(i)$ 中的所有 $j$ 都是常数。该集合的大小为 $w_i$。因此，对 $j \\in \\phi^{-1}(i)$ 的求和简化为乘以该集合的大小：\n$$\n\\sum_{j \\in \\phi^{-1}(i)} \\left( \\dots \\right) = w_i \\left( \\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2 \\right)\n$$\n将此代回 $J'_{unweighted}$ 的表达式中：\n$$\nJ'_{unweighted} = \\sum_{i=1}^n w_i \\left( \\sum_{k=1}^K r_{ik} \\|x_i - \\mu_k\\|^2 \\right) = \\sum_{i=1}^n \\sum_{k=1}^K r_{ik} w_i \\|x_i - \\mu_k\\|^2\n$$\n这正是加权目标函数 $J_{weighted}$。由于目标函数完全相同，它们关于 $\\{\\mu_k\\}$ 的最小化必须产生相同的解。因此，从复制数据集上的无权目标推导出的质心更新规则等价于原始数据集上的加权更新规则，这证实了质心更新在两种情况下是相同的结论。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes, implements, and tests weighted k-means clustering.\n    - Derives weighted centroid updates.\n    - Proves equivalence with duplicated-unweighted data for integer weights.\n    - Runs four test cases to validate theory and measure centroid drift.\n    \"\"\"\n    epsilon = 1e-12\n\n    # Helper function to format results for the final print statement.\n    def format_result(item):\n        if isinstance(item, list):\n            # Format the list of floats to 6 decimal places, no spaces.\n            return f\"[{','.join(f'{x:.6f}' for x in item)}]\"\n        else:\n            # For booleans, the default str() is used.\n            return str(item).lower()\n\n    # --- Test Case 1 ---\n    # Single-cluster equivalence in R^1\n    x1_data = np.array([0., 2.])\n    w1_data = np.array([3, 1])\n    \n    # Weighted approach\n    centroid_weighted_1 = np.sum(x1_data * w1_data) / np.sum(w1_data)\n    \n    # Duplicated-unweighted approach\n    x1_dup = np.repeat(x1_data, w1_data)\n    centroid_dup_1 = np.mean(x1_dup)\n    \n    result_1 = np.abs(centroid_weighted_1 - centroid_dup_1)  epsilon\n\n    # --- Test Case 2 ---\n    # Single-cluster equivalence in R^2 with a zero-weight edge case\n    x2_data = np.array([[0., 0.], [2., 2.], [10., 10.]])\n    w2_data = np.array([1, 1, 0])\n    \n    # Weighted approach\n    # Note: Handles sum of weights being zero if all weights are zero\n    sum_weights_2 = np.sum(w2_data)\n    if sum_weights_2 > 0:\n        centroid_weighted_2 = np.sum(x2_data * w2_data[:, np.newaxis], axis=0) / sum_weights_2\n    else: # Should not happen in this test case\n        centroid_weighted_2 = np.zeros(x2_data.shape[1])\n        \n    # Duplicated-unweighted approach\n    w2_data_int = w2_data.astype(int)\n    x2_dup = np.repeat(x2_data, w2_data_int, axis=0)\n    \n    if x2_dup.shape[0] > 0:\n        centroid_dup_2 = np.mean(x2_dup, axis=0)\n    else:\n        centroid_dup_2 = np.zeros(x2_data.shape[1])\n    \n    result_2 = np.linalg.norm(centroid_weighted_2 - centroid_dup_2)  epsilon\n\n    # --- Test Case 3 ---\n    # Single k-means iteration equivalence in R^2 with K=2\n    x3_data = np.array([[0.,0.], [1.,0.], [0.,1.], [5.,5.], [6.,5.], [5.,6.]])\n    w3_data = np.array([1, 2, 1, 1, 1, 3])\n    mu3_initial = np.array([[0.,0.], [5.,5.]])\n\n    # Assignment step (common for both methods, independent of weights)\n    # Using numpy broadcasting to compute all squared distances at once\n    dist_sq = np.sum((x3_data[:, np.newaxis, :] - mu3_initial[np.newaxis, :, :])**2, axis=2)\n    assignments = np.argmin(dist_sq, axis=1)\n\n    # Weighted update\n    mu3_weighted_new = np.zeros_like(mu3_initial)\n    for k in range(2):\n        mask = (assignments == k)\n        cluster_weights = w3_data[mask]\n        sum_cluster_weights = np.sum(cluster_weights)\n        if sum_cluster_weights > 0:\n            mu3_weighted_new[k] = np.sum(x3_data[mask] * cluster_weights[:, np.newaxis], axis=0) / sum_cluster_weights\n        else: # If cluster is empty\n            mu3_weighted_new[k] = mu3_initial[k]\n    \n    # Duplicated-unweighted update\n    w3_data_int = w3_data.astype(int)\n    x3_dup = np.repeat(x3_data, w3_data_int, axis=0)\n    assignments_dup = np.repeat(assignments, w3_data_int, axis=0)\n    \n    mu3_dup_new = np.zeros_like(mu3_initial)\n    for k in range(2):\n        mask = (assignments_dup == k)\n        if np.any(mask):\n            mu3_dup_new[k] = np.mean(x3_dup[mask], axis=0)\n        else:\n            mu3_dup_new[k] = mu3_initial[k]\n\n    diff1 = np.linalg.norm(mu3_weighted_new[0] - mu3_dup_new[0])\n    diff2 = np.linalg.norm(mu3_weighted_new[1] - mu3_dup_new[1])\n    result_3 = (diff1  epsilon) and (diff2  epsilon)\n\n    # --- Test Case 4 ---\n    # Centroid drift as a single weight varies\n    base_points = np.array([[0., 0.]] * 5)\n    base_weights = np.array([1] * 5)\n    outlier_point = np.array([[3., 0.]])\n    outlier_weights = [0, 1, 2, 5, 10]\n    \n    distances = []\n    for w in outlier_weights:\n        # Full dataset and weights\n        all_points = np.vstack((base_points, outlier_point))\n        all_weights = np.append(base_weights, w)\n        \n        # Compute centroid (weighted mean)\n        sum_weights_4 = np.sum(all_weights)\n        if sum_weights_4 > 0:\n            centroid = np.sum(all_points * all_weights[:, np.newaxis], axis=0) / sum_weights_4\n        else:\n            centroid = np.zeros(2)\n\n        # Compute Euclidean distance from the origin\n        distance = np.linalg.norm(centroid)\n        distances.append(distance)\n        \n    result_4 = distances\n    \n    # -- Aggregate and Print Results --\n    final_results = [result_1, result_2, result_3, result_4]\n    \n    # The print statement must produce a single line in the specified format.\n    print(f\"[{','.join(map(format_result, final_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "现实世界的数据往往包含多种类型，而不仅仅是数值。这个练习将带你走近 K-means 的实际应用，解决包含数值和分类特征的混合数据聚类问题。你将学习如何使用独热编码（one-hot encoding）技术来量化分类变量，并探索缩放因子 $\\alpha$ 的变化如何影响最终的聚类结果，这对于理解数据预处理在聚类分析中的关键作用至关重要。",
            "id": "3134973",
            "problem": "考虑一个数据集，其中包含具有一个实值特征和一个分类特征的点。该分类特征可取三个值之一：A、B 或 C。您将使用独热编码来表示分类特征，并对每个虚拟变量应用一个缩放因子 $\\,\\alpha\\,$。也就是说，对于类别 A，使用独热向量 $[1,0,0]$；对于类别 B，使用 $[0,1,0]$；对于类别 C，使用 $[0,0,1]$，然后将此向量乘以 $\\,\\alpha\\,$。实值特征保持不缩放。一个点的完整编码特征向量是实值特征（第一个分量）和经 $\\,\\alpha\\,$ 缩放的独热向量（其余分量）的串联。\n\n使用的基本定义：\n- 在 $\\mathbb{R}^d$ 中，两个向量 $\\,\\mathbf{x}\\,$ 和 $\\,\\mathbf{y}\\,$ 之间的欧几里得距离由 $\\,\\|\\mathbf{x}-\\mathbf{y}\\|_2\\,$ 给出，而欧几里得距离的平方是 $\\,\\|\\mathbf{x}-\\mathbf{y}\\|_2^2\\,$。\n- 对于具有质心 $\\,\\{\\boldsymbol{\\mu}_j\\}_{j=1}^k\\,$ 和将每个点索引 $\\,i\\,$ 映射到其聚类的分配函数 $\\,c(i)\\in\\{1,\\dots,k\\}\\,$ 的 $\\,k\\,$ 个聚类，$\\,k$-均值目标 $\\,J\\,$ 由下式给出\n$$\nJ \\;=\\; \\sum_{i} \\big\\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{c(i)} \\big\\|_2^2.\n$$\n一个聚类的质心定义为分配给该聚类的特征向量的算术平均值。\n\n您的任务是分析在单次 Lloyd 迭代（分配步骤后跟质心重新计算，但不重复）下，分类独热虚拟变量的缩放因子 $\\,\\alpha\\,$ 如何影响欧几里得距离和 $\\,k$-均值目标 $\\,J\\,$。\n\n使用以下包含 $\\,6\\,$ 个点的数据集。每个点都有一个实值分量和一个分类标签：\n- 点 $\\,0\\,$：实值 $\\,0.0\\,$, 类别 A。\n- 点 $\\,1\\,$：实值 $\\,0.3\\,$, 类别 A。\n- 点 $\\,2\\,$：实值 $\\,0.9\\,$, 类别 A。\n- 点 $\\,3\\,$：实值 $\\,1.2\\,$, 类别 B。\n- 点 $\\,4\\,$：实值 $\\,0.6\\,$, 类别 C。\n- 点 $\\,5\\,$：实值 $\\,1.0\\,$, 类别 C。\n\n使用独热顺序 $[A,B,C]$ 对类别进行编码。\n\n对测试套件中每个指定的 $\\,\\alpha\\,$ 值执行以下操作：\n1. 使用上述方案（先是实值特征，然后是经 $\\,\\alpha\\,$ 缩放的独热向量）构建编码数据集。\n2. 将 $\\,k=2\\,$ 个质心分别初始化为点 $\\,0\\,$ 和点 $\\,5\\,$ 的编码向量。\n3. 使用欧几里得距离的平方将每个点分配给最近的质心，以避免不必要的平方根运算；如果距离完全相等，则选择索引较小的质心来打破平局。\n4. 在编码特征空间中，将质心重新计算为分配给每个聚类的点的平均值。\n5. 计算 $\\,k$-均值目标 $\\,J\\,$，即每个点到其重新计算的聚类质心的欧几里得距离平方的总和。\n6. 计算点 $\\,1\\,$ 和点 $\\,4\\,$ 的编码向量之间的欧几里得距离 $\\,\\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2\\,$。\n\n$\\,\\alpha\\,$ 的测试套件：\n- $\\,\\alpha = 0.0\\,$,\n- $\\,\\alpha = 0.5\\,$,\n- $\\,\\alpha = 1.0\\,$,\n- $\\,\\alpha = 1.5\\,$.\n\n您的程序应输出单行，其中包含测试套件的结果，形式为一对数值的列表，每对数值包含两个浮点数 $[d, J]$，其中 $\\,d\\,$ 是给定 $\\,\\alpha\\,$ 下的欧几里得距离 $\\,\\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2\\,$，$\\,J\\,$ 是该 $\\,\\alpha\\,$ 经过单次 Lloyd 迭代后对应的 $\\,k$-均值目标。最终输出格式必须是以下形式的单行\n$$\n[[d_0,J_0],[d_1,J_1],[d_2,J_2],[d_3,J_3]]\n$$\n对应于给定顺序的 $\\,\\alpha\\,$ 值。将每个浮点数表示为四舍五入到六位小数。此问题不应读取任何外部输入，也不涉及任何物理单位或角度。",
            "solution": "#### 2.1 特征向量编码\n一个具有实值 $r$ 和一个类别的点表示为一个 4 维向量 $\\mathbf{x} = [r, \\alpha v_1, \\alpha v_2, \\alpha v_3]$，其中 $[v_1, v_2, v_3]$ 是按 [A, B, C] 顺序排列的该类别的独热向量。\n\n数据集编码如下：\n- $\\mathbf{x}_0 = [0.0, \\alpha, 0, 0]$\n- $\\mathbf{x}_1 = [0.3, \\alpha, 0, 0]$\n- $\\mathbf{x}_2 = [0.9, \\alpha, 0, 0]$\n- $\\mathbf{x}_3 = [1.2, 0, \\alpha, 0]$\n- $\\mathbf{x}_4 = [0.6, 0, 0, \\alpha]$\n- $\\mathbf{x}_5 = [1.0, 0, 0, \\alpha]$\n\n#### 2.2 单次 Lloyd 迭代\n\n**初始质心**\n两个初始质心 $\\boldsymbol{\\mu}_1^{(0)}$ 和 $\\boldsymbol{\\mu}_2^{(0)}$ 初始化为点 0 和点 5 的编码向量。\n- $\\boldsymbol{\\mu}_1^{(0)} = \\mathbf{x}_0 = [0.0, \\alpha, 0, 0]$\n- $\\boldsymbol{\\mu}_2^{(0)} = \\mathbf{x}_5 = [1.0, 0, 0, \\alpha]$\n\n**分配步骤**\n每个点 $\\mathbf{x}_i$ 使用欧几里得距离的平方，被分配到对应于较近质心的聚类。\n\n- 对于 $\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5$，分配与 $\\alpha$ 无关：\n    - $\\mathbf{x}_0$: $\\|\\mathbf{x}_0 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = 0$。分配到聚类 1。\n    - $\\mathbf{x}_1$: $\\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (0.3)^2 = 0.09$。$\\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (-0.7)^2 + \\alpha^2 + (-\\alpha)^2 = 0.49 + 2\\alpha^2$。分配到聚类 1。\n    - $\\mathbf{x}_3$: $\\|\\mathbf{x}_3 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (1.2)^2 + (-\\alpha)^2 + \\alpha^2 = 1.44 + 2\\alpha^2$。$\\|\\mathbf{x}_3 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (0.2)^2 + \\alpha^2 + (-\\alpha)^2 = 0.04 + 2\\alpha^2$。分配到聚类 2。\n    - $\\mathbf{x}_4$: $\\|\\mathbf{x}_4 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (0.6)^2 + (-\\alpha)^2 + \\alpha^2 = 0.36 + 2\\alpha^2$。$\\|\\mathbf{x}_4 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (-0.4)^2 = 0.16$。分配到聚类 2。\n    - $\\mathbf{x}_5$: $\\|\\mathbf{x}_5 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = 0$。分配到聚类 2。\n\n- 对于 $\\mathbf{x}_2 = [0.9, \\alpha, 0, 0]$，分配取决于 $\\alpha$：\n    - $\\|\\mathbf{x}_2 - \\boldsymbol{\\mu}_1^{(0)}\\|_2^2 = (0.9)^2 = 0.81$。\n    - $\\|\\mathbf{x}_2 - \\boldsymbol{\\mu}_2^{(0)}\\|_2^2 = (0.9-1.0)^2 + \\alpha^2 + (-\\alpha)^2 = (-0.1)^2 + 2\\alpha^2 = 0.01 + 2\\alpha^2$。\n    - 我们比较 $0.81$ 和 $0.01 + 2\\alpha^2$。如果 $0.81 \\le 0.01 + 2\\alpha^2$，则分配到聚类 1，否则分配到聚类 2。\n    - $0.81 \\le 0.01 + 2\\alpha^2 \\implies 0.80 \\le 2\\alpha^2 \\implies 0.4 \\le \\alpha^2$。\n    - 因此，如果 $\\alpha^2 \\ge 0.4$，则 $\\mathbf{x}_2$ 被分配到聚类 1，如果 $\\alpha^2  0.4$，则被分配到聚类 2。\n\n测试值为 $\\alpha \\in \\{0.0, 0.5, 1.0, 1.5\\}$。\n- 对于 $\\alpha=0.0$, $\\alpha^2 = 0.0  0.4$。\n- 对于 $\\alpha=0.5$, $\\alpha^2 = 0.25  0.4$。\n- 对于 $\\alpha=1.0$, $\\alpha^2 = 1.0 \\ge 0.4$。\n- 对于 $\\alpha=1.5$, $\\alpha^2 = 2.25 \\ge 0.4$。\n\n这为聚类分配定义了两种情况。\n\n**更新步骤（质心重新计算）**\n\n- **情况 1：$\\alpha^2  0.4$（对于 $\\alpha=0.0, 0.5$）**\n    - 聚类 1：$\\{\\mathbf{x}_0, \\mathbf{x}_1\\}$\n    - 聚类 2：$\\{\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5\\}$\n    - 新质心：\n        - $\\boldsymbol{\\mu}_1^{(1)} = \\frac{1}{2}(\\mathbf{x}_0 + \\mathbf{x}_1) = \\frac{1}{2}[0.3, 2\\alpha, 0, 0] = [0.15, \\alpha, 0, 0]$\n        - $\\boldsymbol{\\mu}_2^{(1)} = \\frac{1}{4}(\\mathbf{x}_2+\\mathbf{x}_3+\\mathbf{x}_4+\\mathbf{x}_5) = \\frac{1}{4}[3.7, \\alpha, \\alpha, 2\\alpha] = [0.925, 0.25\\alpha, 0.25\\alpha, 0.5\\alpha]$\n\n- **情况 2：$\\alpha^2 \\ge 0.4$（对于 $\\alpha=1.0, 1.5$）**\n    - 聚类 1：$\\{\\mathbf{x}_0, \\mathbf{x}_1, \\mathbf{x}_2\\}$\n    - 聚类 2：$\\{\\mathbf{x}_3, \\mathbf{x}_4, \\mathbf{x}_5\\}$\n    - 新质心：\n        - $\\boldsymbol{\\mu}_1^{(1)} = \\frac{1}{3}(\\mathbf{x}_0+\\mathbf{x}_1+\\mathbf{x}_2) = \\frac{1}{3}[1.2, 3\\alpha, 0, 0] = [0.4, \\alpha, 0, 0]$\n        - $\\boldsymbol{\\mu}_2^{(1)} = \\frac{1}{3}(\\mathbf{x}_3+\\mathbf{x}_4+\\mathbf{x}_5) = \\frac{1}{3}[2.8, 0, \\alpha, 2\\alpha] = [\\frac{2.8}{3}, 0, \\frac{\\alpha}{3}, \\frac{2\\alpha}{3}]$\n\n#### 2.3 所需输出的计算\n\n**欧几里得距离 $d = \\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2$**\n该距离仅取决于编码，与聚类结果无关。\n- $\\mathbf{x}_1 - \\mathbf{x}_4 = [0.3-0.6, \\alpha-0, 0-0, 0-\\alpha] = [-0.3, \\alpha, 0, -\\alpha]$\n- $d^2 = \\|\\mathbf{x}_1 - \\mathbf{x}_4\\|_2^2 = (-0.3)^2 + \\alpha^2 + 0^2 + (-\\alpha)^2 = 0.09 + 2\\alpha^2$\n- $d = \\sqrt{0.09 + 2\\alpha^2}$\n\n**k-均值目标 $J$**\n目标 $J$ 是从每个点到其新质心 $\\boldsymbol{\\mu}_{c(i)}^{(1)}$ 的距离平方和。\n\n- **情况 1（对于 $\\alpha^2  0.4$ 的 $J_1$）：**\n    - 聚类 1 的距离平方和：$\\|\\mathbf{x}_0 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 + \\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 = (0.0-0.15)^2 + (0.3-0.15)^2 = 0.0225 + 0.0225 = 0.045$。\n    - 聚类 2 的距离平方和：这是各分量方差的总和。更直接的计算得出 $\\sum_{i \\in C_2} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_2^{(1)}\\|_2^2 = 0.1875 + 2.5\\alpha^2$。\n    - $J_1 = 0.045 + 0.1875 + 2.5\\alpha^2 = 0.2325 + 2.5\\alpha^2$。\n\n- **情况 2（对于 $\\alpha^2 \\ge 0.4$ 的 $J_2$）：**\n    - 聚类 1 的距离平方和：$\\|\\mathbf{x}_0 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 + \\|\\mathbf{x}_1 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 + \\|\\mathbf{x}_2 - \\boldsymbol{\\mu}_1^{(1)}\\|_2^2 = (0.0-0.4)^2 + (0.3-0.4)^2 + (0.9-0.4)^2 = 0.16 + 0.01 + 0.25 = 0.42$。\n    - 聚类 2 的距离平方和：$\\sum_{i \\in C_2} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_2^{(1)}\\|_2^2 = \\frac{1}{3}(0.56 + 4\\alpha^2)$。\n    - $J_2 = 0.42 + \\frac{0.56 + 4\\alpha^2}{3}$。\n\n#### 2.4 测试套件的最终计算\n\n- **对于 $\\alpha = 0.0$：**（情况 1）\n    - $d = \\sqrt{0.09 + 2(0.0)^2} = \\sqrt{0.09} = 0.3$。\n    - $J = 0.2325 + 2.5(0.0)^2 = 0.2325$。\n    - 结果：$[0.300000, 0.232500]$\n\n- **对于 $\\alpha = 0.5$：**（情况 1）\n    - $d = \\sqrt{0.09 + 2(0.5)^2} = \\sqrt{0.09 + 0.5} = \\sqrt{0.59} \\approx 0.768115$。\n    - $J = 0.2325 + 2.5(0.5)^2 = 0.2325 + 0.625 = 0.8575$。\n    - 结果：$[0.768115, 0.857500]$\n\n- **对于 $\\alpha = 1.0$：**（情况 2）\n    - $d = \\sqrt{0.09 + 2(1.0)^2} = \\sqrt{2.09} \\approx 1.445683$。\n    - $J = 0.42 + \\frac{0.56 + 4(1.0)^2}{3} = 0.42 + \\frac{4.56}{3} = 0.42 + 1.52 = 1.94$。\n    - 结果：$[1.445683, 1.940000]$\n\n- **对于 $\\alpha = 1.5$：**（情况 2）\n    - $d = \\sqrt{0.09 + 2(1.5)^2} = \\sqrt{0.09 + 4.5} = \\sqrt{4.59} \\approx 2.142429$。\n    - $J = 0.42 + \\frac{0.56 + 4(1.5)^2}{3} = 0.42 + \\frac{0.56 + 9}{3} = 0.42 + \\frac{9.56}{3} \\approx 0.42 + 3.186667 = 3.606667$。\n    - 结果：$[2.142429, 3.606667]$",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a single Lloyd iteration for k-means clustering on a mixed-type dataset\n    for various scaling factors of the categorical features.\n    \"\"\"\n    # Define the dataset of points with a real value and a categorical label.\n    raw_data = [\n        (0.0, 'A'), (0.3, 'A'), (0.9, 'A'),\n        (1.2, 'B'), (0.6, 'C'), (1.0, 'C')\n    ]\n\n    # Map categories to one-hot vectors based on the order [A, B, C].\n    categories = {'A': np.array([1, 0, 0]), 'B': np.array([0, 1, 0]), 'C': np.array([0, 0, 1])}\n    \n    # Test suite of scaling factors for the one-hot encoded features.\n    alphas = [0.0, 0.5, 1.0, 1.5]\n\n    final_results = []\n\n    for alpha in alphas:\n        # Step 1: Build the encoded dataset.\n        # The feature vector is [real_value, alpha * one_hot_vector].\n        encoded_data = []\n        for r_val, cat in raw_data:\n            scaled_one_hot = alpha * categories[cat]\n            point = np.concatenate(([r_val], scaled_one_hot))\n            encoded_data.append(point)\n        X = np.array(encoded_data)\n\n        # Step 2: Initialize k=2 centroids using point 0 and point 5.\n        initial_centroids = np.array([X[0], X[5]])\n\n        # Step 3: Assign each point to the nearest centroid.\n        # Tie-breaking: assign to the centroid with the smaller index (0).\n        assignments = []\n        for point in X:\n            dist_sq_0 = np.sum((point - initial_centroids[0])**2)\n            dist_sq_1 = np.sum((point - initial_centroids[1])**2)\n            if dist_sq_0 = dist_sq_1:\n                assignments.append(0)\n            else:\n                assignments.append(1)\n        assignments = np.array(assignments)\n\n        # Step 4: Recompute the centroids as the mean of assigned points.\n        cluster_0_points = X[assignments == 0]\n        cluster_1_points = X[assignments == 1]\n        \n        new_centroids = np.zeros_like(initial_centroids)\n        # Ensure clusters are not empty before computing mean.\n        if len(cluster_0_points) > 0:\n            new_centroids[0] = np.mean(cluster_0_points, axis=0)\n        if len(cluster_1_points) > 0:\n            new_centroids[1] = np.mean(cluster_1_points, axis=0)\n\n        # Step 5: Compute the k-means objective J.\n        # J is the sum of squared Euclidean distances to the new centroids.\n        J = 0.0\n        if len(cluster_0_points) > 0:\n            J += np.sum((cluster_0_points - new_centroids[0])**2)\n        if len(cluster_1_points) > 0:\n            J += np.sum((cluster_1_points - new_centroids[1])**2)\n\n        # Step 6: Compute the Euclidean distance between encoded points 1 and 4.\n        d = np.linalg.norm(X[1] - X[4])\n\n        final_results.append([d, J])\n\n    # Format the final output string as specified.\n    # [[d_0,J_0],[d_1,J_1],...] with values rounded to six decimal places.\n    result_strings = []\n    for d, J in final_results:\n        result_strings.append(f\"[{d:.6f},{J:.6f}]\")\n    \n    output_string = f\"[{','.join(result_strings)}]\"\n    print(output_string)\n\nsolve()\n```"
        }
    ]
}