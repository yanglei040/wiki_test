{
    "hands_on_practices": [
        {
            "introduction": "理论知识通往实践技能的桥梁在于亲手解决问题。本章提供了一系列精心设计的实践练习，旨在加深您对聚类分析中相异性度量选择的理解。这些练习将挑战您应用核心概念，并揭示不同度量在不同场景下的优势与劣经。",
            "id": "3109574",
            "problem": "给定一个二维有限数据集族，要求您量化对其中一个特征进行单调非线性变换后，欧几里得相异性以及通过 $k$-均值方法所得到的划分会发生怎样的变化。请基于以下基本要素进行操作：\n\n- $\\mathbb{R}^{d}$ 中的欧几里得距离：对于点 $x,y \\in \\mathbb{R}^{d}$，$d(x,y) = \\lVert x - y \\rVert_{2} = \\sqrt{\\sum_{j=1}^{d} (x_{j}-y_{j})^{2}}$。\n- $k$-均值目标函数：对于数据集 $X = \\{x_{1},\\dots,x_{n}\\} \\subset \\mathbb{R}^{d}$ 以及一个划分为 $k$ 个簇（质心为 $\\{\\mu_{1},\\dots,\\mu_{k}\\}$）的情况，簇内平方和 (within-cluster sum of squares) 为 $J = \\sum_{c=1}^{k} \\sum_{i : x_{i} \\in C_{c}} \\lVert x_{i} - \\mu_{c} \\rVert_{2}^{2}$，其中 $\\mu_{c}$ 是簇 $C_{c}$ 中所有点的平均值。\n- 对于数据集 $X = \\{x_{1},\\dots,x_{n}\\}$，定义平均成对欧几里得距离为 $\\overline{D}(X) = \\dfrac{2}{n(n-1)} \\sum_{1 \\le i  j \\le n} d(x_{i}, x_{j})$。\n- 考虑一个单调非线性变换 $g : \\mathbb{R}_{0} \\rightarrow \\mathbb{R}$，其定义为 $g(t) = \\ln(t)$，且仅应用于第一个特征。对于一个点 $x = (x_{1}, x_{2})$ 且 $x_{1}  0$，定义 $T(x) = (g(x_{1}), x_{2}) = (\\ln(x_{1}), x_{2})$，并将其逐点扩展到数据集上，即 $T(X) = \\{T(x_{i})\\}_{i=1}^{n}$。\n- 必须使用确定性的 $k$-均值算法，其初始化和更新步骤如下：\n  1. 通过最远优先遍历进行初始化：选择 $X$ 中字典序坐标最小的点作为第一个中心点；然后，迭代地选择下一个中心点，该中心点应为数据集中与已选中心点集具有最大最小欧几里得距离的点，直到选出 $k$ 个中心点。\n  2. 交替最小化：将每个点分配给欧几里得距离最近的中心点；将每个中心点更新为其所分配点的算术平均值；重复此过程，直到标签不再改变或达到最大100次迭代（以先到者为准）。\n  3. 如果在分配步骤后任何簇变为空，则立即且确定性地使用当前距其已分配中心点具有最大平方距离的数据点来重新播种该空簇，并在继续之前将该点重新分配给这个被清空的簇。\n\n您必须实现一个程序，对下面测试套件中的每个数据集执行以下操作：\n\n1. 在原始数据集 $X$ 上计算 $\\overline{D}(X)$。\n2. 在仅对第一个特征应用变换 $T$ 后，计算 $\\overline{D}(T(X))$。\n3. 在 $X$ 上运行（如上所述的）确定性 $k$-均值算法，以获得标签 $\\ell^{\\text{orig}} \\in \\{0,1,\\dots,k-1\\}^{n}$ 和相应的簇内平方和 $J_{\\text{orig}}$。\n4. 在 $T(X)$ 上运行相同的算法，以获得标签 $\\ell^{\\text{trans}}$ 和 $J_{\\text{trans}}$。\n5. 计算比率 $r = \\dfrac{\\overline{D}(T(X))}{\\overline{D}(X)}$。\n6. 使用基于列联表的标准对计数定义，计算 $\\ell^{\\text{orig}}$ 和 $\\ell^{\\text{trans}}$ 之间的调整兰德指数 (ARI)。如果 $n_{ij}$ 是被 $\\ell^{\\text{orig}}$ 划分到簇 $i$ 且被 $\\ell^{\\text{trans}}$ 划分到簇 $j$ 的点的数量，$a_{i} = \\sum_{j} n_{ij}$，$b_{j} = \\sum_{i} n_{ij}$，并且 $N = \\sum_{i,j} n_{ij}$，定义 $C_{2}(m) = \\dfrac{m(m-1)}{2}$。那么\n$$\n\\text{ARI} = \\frac{\\sum_{i,j} C_{2}(n_{ij}) - \\frac{\\left(\\sum_{i} C_{2}(a_{i})\\right) \\left(\\sum_{j} C_{2}(b_{j})\\right)}{C_{2}(N)}}\n{\\frac{1}{2}\\left(\\sum_{i} C_{2}(a_{i}) + \\sum_{j} C_{2}(b_{j})\\right) - \\frac{\\left(\\sum_{i} C_{2}(a_{i})\\right) \\left(\\sum_{j} C_{2}(b_{j})\\right)}{C_{2}(N)}}.\n$$\n7. 计算经过最优重标记后，簇成员关系发生改变的最少点数，其定义如下。令 $S_{k}$ 为 $\\{0,1,\\dots,k-1\\}$ 的所有排列的集合。对于 $\\pi \\in S_{k}$，将 $\\ell^{\\text{trans}}$ 重标记为 $\\pi(\\ell^{\\text{trans}})$ 并计算与 $\\ell^{\\text{orig}}$ 的匹配数。令 $M = \\max_{\\pi \\in S_{k}} \\left|\\{i : \\ell^{\\text{orig}}_{i} = \\pi(\\ell^{\\text{trans}}_{i})\\}\\right|$。输出改变的点数，即 $n - M$。\n\n测试套件。对于每种情况，$k$ 是所要求的簇数，点以 $(x_{1}, x_{2})$ 的形式给出，其中 $x_{1}  0$：\n\n- 情况 A（理想情况，两个分隔良好的 $x$ 坐标簇，预期具有稳定性）：$k = 2$，$X =$ \n  $\\{(9.0, 0.0), (10.0, 0.5), (11.0, -0.5), (19.0, 0.2), (20.0, -0.2), (21.0, 0.1)\\}$.\n- 情况 B（带状结构：变换压缩了 $x$ 坐标，可能将聚类结果翻转为水平带状）：$k = 2$，$X = \\{(2.0, 10.0), (4.0, 10.0), (8.0, 10.0), (16.0, 10.0), (32.0, 10.0), (2.5, 0.0), (5.0, 0.0), (9.0, 0.0), (18.0, 0.0), (36.0, 0.0)\\}$.\n- 情况 C（边界情况，第一个特征近乎恒定，按 $y$ 坐标可自然形成三个簇；预期具有不变性）：$k = 3$，$X = \\{(1.10, 0.0), (1.20, 0.1), (1.15, -0.1), (1.30, 3.0), (1.40, 3.1), (1.35, 2.9), (1.25, -3.0), (1.22, -3.1), (1.28, -2.9)\\}$.\n- 情况 D（第一个特征存在极端离群值；变换会减弱其杠杆作用，并可能改变簇成员关系）：$k = 2$，$X = \\{(5.0, 5.0), (6.0, 6.0), (5.5, 5.5), (7.0, 5.0), (7.5, 5.5), (1000.0, 5.2)\\}$.\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含一个结果列表，每个情况一个结果，并按 A、B、C、D 的顺序排列。每个情况的结果必须是 $[r, J_{\\text{orig}}, J_{\\text{trans}}, \\text{ARI}, \\text{changed}]$ 形式的列表，其中 $r$、$J_{\\text{orig}}$、$J_{\\text{trans}}$ 和 $\\text{ARI}$ 是小数点后精确到 $6$ 位的小数，而 $\\text{changed}$ 是一个整数。总输出必须是无空格的单行，例如，`[[0.123456,1.234567,1.111111,0.987654,2],[\\dots],\\dots]`。",
            "solution": "用户提供了一个统计学习领域中定义明确的计算问题，该问题特别关注非线性特征变换对相异性度量和聚类结果的影响。该问题具有科学依据、形式化规范并且可通过计算验证。我将提供完整的解决方案。\n\n问题的核心是分析单调非线性变换 $T(x_1, x_2) = (\\ln(x_1), x_2)$ 如何影响数据集的几何结构，并进而影响 $k$-均值聚类的结果。该变换会压缩第一个特征 $x_1$ 的较大值并拉伸其较小值，从而改变成对距离的结构。问题要求计算几个量来衡量这些变化：平均成对欧几里得距离的比率、原始数据和变换后数据的簇内平方和、用于比较所得簇划分的调整兰德指数 (ARI)，以及改变簇成员关系的最少点数。\n\n解决方案要求对每个测试用例按顺序实现以下步骤：\n\n**1. 数据准备**\n对于每个输入数据集 $X = \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}^2$ 其中 $x_i=(x_{i1}, x_{i2})$，我们首先构建变换后的数据集 $T(X)$。变换定义为 $T(x) = (\\ln(x_1), x_2)$，并逐元素应用于 $X$ 中的每个点。此步骤产生两个数据集：原始的 $X$ 和变换后的 $T(X)$。所有输入数据点都满足 $x_1  0$，以确保对数函数有定义。\n\n**2. 平均成对距离计算**\n问题将平均成对欧几里得距离定义为 $\\overline{D}(X) = \\frac{2}{n(n-1)} \\sum_{1 \\le i  j \\le n} d(x_i, x_j)$。需要对 $X$ 和 $T(X)$ 分别计算此值。在编程上，可以通过先计算所有 $\\frac{n(n-1)}{2}$ 个唯一的成对距离，然后求其算术平均值来高效实现。接着计算比率 $r = \\frac{\\overline{D}(T(X))}{\\overline{D}(X)}$。\n\n**3. 确定性 $k$-均值聚类**\n问题指定了一个确定性的 $k$-均值算法，必须精确实现。该算法将在 $X$ 和 $T(X)$ 上运行，并使用给定的簇数 $k$。算法包括三个主要部分：\n- **初始化（最远优先遍历）：** 这是一个用于选择初始质心的贪心过程。第一个质心选择为字典序最小的数据点。随后的质心从数据点中迭代选取，每一步选择与所有已选质心具有最大最小欧几里得距离的点。\n- **迭代优化：** 这是 Lloyd 算法的一个变体。它在两个步骤之间交替进行，最多迭代100次或直到收敛（即簇分配不再改变）：\n    - **分配步骤：** 根据欧几里得距离，将每个数据点分配给最近的质心所对应的簇。\n    - **更新步骤：** 将每个质心重新计算为其簇内所有数据点的算术平均值。\n- **空簇处理：** 问题提供了一个特定的、确定性的规则来处理分配步骤后出现空簇的情况。如果一个或多个簇变为空，则立即重新播种。对于每个空簇，选择当前与其已分配质心具有最大平方欧几里得距离的数据点作为该空簇的新种子。然后在算法进入质心更新步骤之前，将该点重新分配给这个新簇。如果有多个空簇，则重复此过程，从剩余具有最大距离的数据点中进行选择。\n\n算法收敛或终止后，记录最终的簇标签（$\\ell^{\\text{orig}}$ 和 $\\ell^{\\text{trans}}$）和最终的簇内平方和（$J_{\\text{orig}}$ 和 $J_{\\text{trans}}$）。簇内平方和由目标函数 $J = \\sum_{c=1}^{k} \\sum_{i : x_{i} \\in C_{c}} \\lVert x_{i} - \\mu_{c} \\rVert_{2}^{2}$ 定义。\n\n**4. 簇划分比较**\n使用两种度量来量化两个所得划分 $\\ell^{\\text{orig}}$ 和 $\\ell^{\\text{trans}}$ 之间的相似性：\n- **调整兰德指数 (ARI)：** ARI 是一种衡量两个数据聚类之间相似度的指标，并已针对偶然性进行了校正。它使用基于列联表 $n_{ij}$（第一个划分中簇 $i$ 和第二个划分中簇 $j$ 的公共点数）的给定公式计算：\n$$\n\\text{ARI} = \\frac{\\sum_{i,j} C_{2}(n_{ij}) - \\frac{\\left(\\sum_{i} C_{2}(a_{i})\\right) \\left(\\sum_{j} C_{2}(b_{j})\\right)}{C_{2}(N)}}\n{\\frac{1}{2}\\left(\\sum_{i} C_{2}(a_{i}) + \\sum_{j} C_{2}(b_{j})\\right) - \\frac{\\left(\\sum_{i} C_{2}(a_{i})\\right) \\left(\\sum_{j} C_{2}(b_{j})\\right)}{C_{2}(N)}}\n$$\n其中 $C_{2}(m) = \\binom{m}{2}$，$a_i$ 和 $b_j$ 分别是列联表的行和与列和，$N$ 是总点数。\n- **最小改变计数：** 该指标在找到簇标签的最优映射后，计算两个划分之间簇分配不同的点数。这是通过找到 $\\ell^{\\text{trans}}$ 标签 $\\{0, 1, \\dots, k-1\\}$ 的一个排列 $\\pi$ 来计算的，该排列使得与 $\\ell^{\\text{orig}}$ 的一致性数量最大化。对于给定的排列 $\\pi$，一致性数量是置换后列联表对角线元素的总和。通过遍历所有 $k!$ 种排列（对于小的 $k$ 是可行的）找到最大匹配数 $M$。最终计数为 $n - M$。\n\n通过对每个测试用例执行这整个流程，我们可以生成所需的 $[r, J_{\\text{orig}}, J_{\\text{trans}}, \\text{ARI}, \\text{changed}]$ 元组列表，然后将其格式化为最终的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom itertools import permutations\nfrom scipy.special import comb\n\ndef custom_kmeans(data, k):\n    \"\"\"\n    Implements the deterministic k-means algorithm specified in the problem.\n    \"\"\"\n    n, _ = data.shape\n    \n    # 1. Initialization: Farthest-first traversal\n    # Find lexicographically smallest point\n    sorted_indices = np.lexsort((data[:, 1], data[:, 0]))\n    first_center_idx = sorted_indices[0]\n    \n    centroid_indices = [first_center_idx]\n    \n    for _ in range(k - 1):\n        min_dists = np.full(n, np.inf)\n        \n        # for each point, find minimum dist to any existing centroid\n        current_centroids = data[centroid_indices]\n        for i in range(n):\n            point = data[i]\n            dists_to_centroids = np.linalg.norm(current_centroids - point, axis=1)\n            min_dists[i] = np.min(dists_to_centroids)\n\n        # To avoid re-selecting a centroid, set their distances to a non-maximal value\n        min_dists[centroid_indices] = -1.0\n        \n        next_center_idx = np.argmax(min_dists)\n        centroid_indices.append(next_center_idx)\n        \n    centroids = data[centroid_indices].copy()\n    labels = np.zeros(n, dtype=int)\n\n    # 2. Alternating minimization\n    for _ in range(100):\n        old_labels = np.copy(labels)\n        \n        # Assignment step\n        dist_sq = np.sum((data[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2, axis=2)\n        labels = np.argmin(dist_sq, axis=1)\n\n        # 3. Empty cluster handling\n        unique_labels, _ = np.unique(labels, return_counts=True)\n        if len(unique_labels)  k:\n            empty_clusters = [c for c in range(k) if c not in unique_labels]\n            \n            d_sq_to_assigned = np.sum((data - centroids[labels])**2, axis=1)\n            \n            farthest_pts_indices = np.argsort(d_sq_to_assigned)[::-1]\n            \n            used_pts_for_reseeding = set()\n            reseed_idx_ptr = 0\n            \n            for empty_c in empty_clusters:\n                pt_idx_to_reseed = -1\n                # Find next available farthest point\n                while pt_idx_to_reseed == -1:\n                    candidate_idx = farthest_pts_indices[reseed_idx_ptr]\n                    if candidate_idx not in used_pts_for_reseeding:\n                        pt_idx_to_reseed = candidate_idx\n                    reseed_idx_ptr += 1\n\n                used_pts_for_reseeding.add(pt_idx_to_reseed)\n                \n                centroids[empty_c] = data[pt_idx_to_reseed]\n                labels[pt_idx_to_reseed] = empty_c\n\n        # Update step\n        for j in range(k):\n            cluster_points = data[labels == j]\n            if len(cluster_points) > 0:\n                centroids[j] = np.mean(cluster_points, axis=0)\n\n        # Convergence check\n        if np.array_equal(labels, old_labels):\n            break\n\n    # Calculate final WCSS (J)\n    final_wcss = np.sum((data - centroids[labels])**2)\n            \n    return labels, final_wcss\n\ndef calculate_ari(labels_true, labels_pred, k):\n    \"\"\"\n    Computes the Adjusted Rand Index.\n    \"\"\"\n    n = len(labels_true)\n    contingency_table = np.zeros((k, k), dtype=int)\n    for i in range(n):\n        contingency_table[labels_true[i], labels_pred[i]] += 1\n\n    def C2(m):\n        if m  2:\n            return 0\n        return comb(m, 2, exact=True)\n\n    sum_nij_c2 = np.sum([C2(n_ij) for n_ij in contingency_table.flat])\n    sum_a_c2 = np.sum([C2(np.sum(contingency_table[i, :])) for i in range(k)])\n    sum_b_c2 = np.sum([C2(np.sum(contingency_table[:, j])) for j in range(k)])\n    \n    c2_n = C2(n)\n    if c2_n == 0:\n        return 1.0\n\n    expected_index = (sum_a_c2 * sum_b_c2) / c2_n\n    max_index = 0.5 * (sum_a_c2 + sum_b_c2)\n    \n    numerator = sum_nij_c2 - expected_index\n    denominator = max_index - expected_index\n    \n    if np.isclose(denominator, 0):\n        # Handle cases of perfect clustering or trivial partitions\n        return 1.0 if np.isclose(numerator, 0) else 0.0\n    \n    return numerator / denominator\n\ndef calculate_changed_count(labels_orig, labels_trans, k, n):\n    \"\"\"\n    Computes the minimal number of points with changed cluster membership.\n    \"\"\"\n    contingency_table = np.zeros((k, k), dtype=int)\n    for i in range(n):\n        contingency_table[labels_orig[i], labels_trans[i]] += 1\n\n    perms = list(permutations(range(k)))\n    max_matches = 0\n    \n    for p in perms:\n        current_matches = np.sum(contingency_table[range(k), p])\n        if current_matches > max_matches:\n            max_matches = current_matches\n            \n    return n - int(max_matches)\n\ndef process_case(X, k):\n    \"\"\"\n    Runs the full analysis pipeline for a single test case.\n    \"\"\"\n    X = np.array(X, dtype=float)\n    n = X.shape[0]\n\n    T_X = np.copy(X)\n    T_X[:, 0] = np.log(T_X[:, 0])\n    \n    d_orig_avg = np.mean(pdist(X, 'euclidean')) if n >= 2 else 0.0\n    d_trans_avg = np.mean(pdist(T_X, 'euclidean')) if n >= 2 else 0.0\n    \n    labels_orig, J_orig = custom_kmeans(X, k)\n    labels_trans, J_trans = custom_kmeans(T_X, k)\n    \n    r = d_trans_avg / d_orig_avg if d_orig_avg != 0 else 0.0\n    \n    ari = calculate_ari(labels_orig, labels_trans, k)\n    \n    changed = calculate_changed_count(labels_orig, labels_trans, k, n)\n    \n    return [\n        f\"{r:.6f}\",\n        f\"{J_orig:.6f}\",\n        f\"{J_trans:.6f}\",\n        f\"{ari:.6f}\",\n        changed\n    ]\n\ndef solve():\n    \"\"\"\n    Defines test cases and orchestrates the solution process.\n    \"\"\"\n    test_cases = [\n        (2, [(9.0, 0.0), (10.0, 0.5), (11.0, -0.5), (19.0, 0.2), (20.0, -0.2), (21.0, 0.1)]),\n        (2, [(2.0, 10.0), (4.0, 10.0), (8.0, 10.0), (16.0, 10.0), (32.0, 10.0), (2.5, 0.0), (5.0, 0.0), (9.0, 0.0), (18.0, 0.0), (36.0, 0.0)]),\n        (3, [(1.10, 0.0), (1.20, 0.1), (1.15, -0.1), (1.30, 3.0), (1.40, 3.1), (1.35, 2.9), (1.25, -3.0), (1.22, -3.1), (1.28, -2.9)]),\n        (2, [(5.0, 5.0), (6.0, 6.0), (5.5, 5.5), (7.0, 5.0), (7.5, 5.5), (1000.0, 5.2)])\n    ]\n\n    all_results = []\n    for k, X_data in test_cases:\n        result = process_case(X_data, k)\n        all_results.append(f\"[{','.join(map(str, result))}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在认识到欧几里得距离对非线性变换的敏感性之后 ()，我们转向另一个关键的实践环节：数据预处理。此练习将探讨特征归一化如何影响曼哈顿距离（$L_1$ 范数）的聚类结果。通过这个练习，您将量化归一化如何降低距离度量对特征尺度的敏感性，并比较其与欧几里得距离在处理多尺度数据时的表现差异。",
            "id": "3109629",
            "problem": "给定三个特征空间中的点的小型数据集，以及每个数据集的固定簇数。您的任务是编写一个完整的、可运行的程序，对于每个数据集，分析按特征归一化为单位方差如何影响使用曼哈顿距离的聚类，将聚类结果与使用欧几里得距离的结果进行比较，并量化归一化前后曼哈顿距离的轴对齐敏感度。\n\n将使用以下基本定义。设在 $\\mathbb{R}^d$ 中有 $n$ 个点 $\\{\\mathbf{x}_i\\}_{i=1}^n$，其中 $\\mathbf{x}_i = (x_{i1}, x_{i2}, \\dots, x_{id})$。两点 $\\mathbf{x}$ 和 $\\mathbf{y}$ 之间的曼哈顿距离（也称为 $\\ell_1$ 距离）是\n$$\nD_{\\text{Manhattan}}(\\mathbf{x}, \\mathbf{y}) = \\sum_{j=1}^d \\left| x_j - y_j \\right|.\n$$\n两点 $\\mathbf{x}$ 和 $\\mathbf{y}$ 之间的欧几里得距离（也称为 $\\ell_2$ 距离）是\n$$\nD_{\\text{Euclidean}}(\\mathbf{x}, \\mathbf{y}) = \\sqrt{ \\sum_{j=1}^d \\left( x_j - y_j \\right)^2 }.\n$$\n按特征归一化为单位方差是通过将每个特征 $j$ 除以其标准差 $\\sigma_j$ 来实现的，该标准差使用总体方差定义\n$$\n\\sigma_j = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{ij} - \\bar{x}_j \\right)^2 }, \\quad \\text{其中} \\quad \\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}.\n$$\n归一化数据 $\\tilde{\\mathbf{x}}_i$ 通过不进行均值中心化的逐分量相除得到：\n$$\n\\tilde{x}_{ij} = \\begin{cases}\n\\dfrac{x_{ij}}{\\sigma_j}  \\text{if } \\sigma_j  0, \\\\\nx_{ij}  \\text{if } \\sigma_j = 0.\n\\end{cases}\n$$\n使用具有完全链接的层次聚类（HC）进行聚类，其中两个簇之间的距离是它们点之间的最大成对距离，并通过最大簇数准则切割树状图以产生恰好 $k$ 个簇。\n\n为了比较两种聚类结果，为每种聚类形成共簇的无序索引对集合。对于由标签 $\\{c_i\\}_{i=1}^n$ 表示的聚类，定义集合\n$$\nS = \\{ (i, l) \\mid 1 \\le i  l \\le n, \\; c_i = c_l \\}.\n$$\n给定两个这样的集合 $S_1$ 和 $S_2$，使用 Jaccard 指数量化它们的相似性\n$$\nJ(S_1, S_2) = \\frac{ \\left| S_1 \\cap S_2 \\right| }{ \\left| S_1 \\cup S_2 \\right| }.\n$$\n最后，如下量化曼哈顿距离的轴对齐敏感度。对于数据集 $X \\in \\mathbb{R}^{n \\times d}$，定义每个特征的平均绝对差\n$$\n\\mu_j = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i  l \\le n} \\left| x_{ij} - x_{lj} \\right|,\n$$\n以及所有无序对的平均曼哈顿距离\n$$\nM = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i  l \\le n} \\sum_{j=1}^d \\left| x_{ij} - x_{lj} \\right|.\n$$\n定义轴贡献分数 $f_j = \\mu_j / M$ 和敏感度指数\n$$\nS_{\\text{axis}} = \\max_{1 \\le j \\le d} f_j.\n$$\n计算归一化前后的 $S_{\\text{axis}}$ 并报告其减少量 $S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$。\n\n为每个数据集实现以下操作：\n- 在原始数据上使用完全链接和曼哈顿距离计算层次聚类，得到标签 $\\{c_i^{\\text{L1,raw}}\\}$。\n- 按特征归一化为单位方差，得到 $\\tilde{X}$。\n- 在 $\\tilde{X}$ 上使用完全链接和曼哈顿距离计算层次聚类，得到标签 $\\{c_i^{\\text{L1,norm}}\\}$。\n- 在 $\\tilde{X}$ 上使用完全链接和欧几里得距离计算层次聚类，得到标签 $\\{c_i^{\\text{L2,norm}}\\}$。\n- 计算 Jaccard 指数 $J\\!\\left(S\\!\\left(c^{\\text{L1,raw}}\\right), S\\!\\left(c^{\\text{L1,norm}}\\right)\\right)$。\n- 计算 Jaccard 指数 $J\\!\\left(S\\!\\left(c^{\\text{L1,norm}}\\right), S\\!\\left(c^{\\text{L2,norm}}\\right)\\right)$。\n- 计算轴敏感度的减少量 $S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$。\n\n测试套件：\n- 测试用例 1 ($k = 2$, $d = 2$)：点\n  $\\left(-8, 0\\right)$, $\\left(-7, 1\\right)$, $\\left(-6, -1\\right)$, $\\left(-9, 0.5\\right)$, $\\left(-7, -0.5\\right)$, $\\left(3, 4\\right)$, $\\left(4, 4.5\\right)$, $\\left(2.5, 5\\right)$, $\\left(3.5, 3.5\\right)$, $\\left(4.5, 5.2\\right)$。\n- 测试用例 2 ($k = 3$, $d = 3$)：点\n  $\\left(0, 0, 100\\right)$, $\\left(1, 0, 100\\right)$, $\\left(0, 1, 100\\right)$, $\\left(5, 5, 100\\right)$, $\\left(6, 5, 100\\right)$, $\\left(5, 6, 100\\right)$, $\\left(-4, 5, 100\\right)$, $\\left(-5, 5, 100\\right)$, $\\left(-4, 6, 100\\right)$。\n- 测试用例 3 ($k = 2$, $d = 2$)：点\n  $\\left(0, 0\\right)$, $\\left(10, 0\\right)$, $\\left(0, 50\\right)$, $\\left(10, 50\\right)$, $\\left(100, 5\\right)$, $\\left(110, 5\\right)$。\n\n输出规范：\n- 对于每个测试用例，计算一个浮点数三元组 $\\left[J_1, J_2, R\\right]$，其中 $J_1 = J\\!\\left(S\\!\\left(c^{\\text{L1,raw}}\\right), S\\!\\left(c^{\\text{L1,norm}}\\right)\\right)$，$J_2 = J\\!\\left(S\\!\\left(c^{\\text{L1,norm}}\\right), S\\!\\left(c^{\\text{L2,norm}}\\right)\\right)$，以及 $R = S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$。\n- 将所有浮点数表示为四舍五入到三位小数的十进制数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素是对应一个测试用例的三元组。所需格式为\n$$\n\\left[ [J_{1,1}, J_{2,1}, R_1], [J_{1,2}, J_{2,2}, R_2], [J_{1,3}, J_{2,3}, R_3] \\right].\n$$\n通过严格遵守上述定义和指定的测试套件来确保科学真实性；不要引入任何外部随机性或数据。",
            "solution": "该问题要求分析特征归一化对使用曼哈顿距离的聚类结果的影响。此分析涉及比较聚类结果、量化它们之间的相似性，并测量距离度量的轴对齐敏感度的变化。该问题是适定的，其科学基础是统计学习的原理，并为确定性解提供了所有必要的定义、数据和参数。\n\n该解决方案通过对每个提供的数据集遵循一个结构化的多步计算过程来实现。\n\n**步骤 1：数据表示与归一化**\n\n对于每个测试用例，输入的点首先表示为一个 $n \\times d$ 矩阵 $X$，其中 $n$ 是点的数量，$d$ 是特征（维度）的数量。\n\n对此矩阵执行归一化为单位方差的操作。对于每个特征（列）$j \\in \\{1, \\dots, d\\}$，计算其总体标准差 $\\sigma_j$：\n$$\n\\sigma_j = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left( x_{ij} - \\bar{x}_j \\right)^2 }\n$$\n其中 $\\bar{x}_j$ 是第 $j$ 个特征的均值。然后通过逐分量相除得到归一化数据矩阵 $\\tilde{X}$。原始矩阵 $X$ 的每个元素 $x_{ij}$ 被转换为 $\\tilde{x}_{ij}$：\n$$\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sigma_j}\n$$\n处理了一个特殊情况，即某个特征的方差为零（$\\sigma_j = 0$），在这种情况下，保留原始值（$\\tilde{x}_{ij} = x_{ij}$）。此归一化过程缩放每个特征，使其方差变为 1，而不改变其均值（与原始数据除以 $\\sigma_j$ 所隐含的均值相比）。这与标准缩放不同，因为没有执行均值中心化。\n\n**步骤 2：层次聚类**\n\n应用具有完全链接的层次聚类 (HC)。算法过程如下：\n1.  使用指定的距离度量计算所有点的成对距离矩阵。问题要求使用曼哈顿距离 ($D_{\\text{Manhattan}}$) 和欧几里得距离 ($D_{\\text{Euclidean}}$)。\n2.  使用`complete`链接准则，其中两个簇之间的距离定义为第一个簇中的任意点与第二个簇中的任意点之间的最大距离。\n3.  聚类过程生成一个树状图，它是一个表示合并层次结构的树。\n4.  根据每个测试用例的指定，切割此树状图以形成恰好 $k$ 个簇。这是通过使用`maxclust`准则实现的。\n\n对每个数据集执行此过程三次，以获得三个不同的聚类结果：\n-   $c^{\\text{L1,raw}}$：在原始数据 $X$ 上使用曼哈顿距离。\n-   $c^{\\text{L1,norm}}$：在归一化数据 $\\tilde{X}$ 上使用曼哈顿距离。\n-   $c^{\\text{L2,norm}}$：在归一化数据 $\\tilde{X}$ 上使用欧几里得距离。\n\n**步骤 3：通过 Jaccard 指数比较聚类结果**\n\n为了比较两种不同聚类（例如，归一化前后）的结果，我们首先构建所有被分在同一个簇中的数据点的无序对集合。对于由标签数组 $\\{c_i\\}_{i=1}^n$ 表示的给定聚类，此集合为：\n$$\nS = \\{ (i, l) \\mid 1 \\le i  l \\le n, \\; c_i = c_l \\}\n$$\n给定来自两种不同聚类的两个这样的集合 $S_1$ 和 $S_2$，它们的相似性由 Jaccard 指数量化：\n$$\nJ(S_1, S_2) = \\frac{ \\left| S_1 \\cap S_2 \\right| }{ \\left| S_1 \\cup S_2 \\right| }\n$$\n该指数衡量两种聚类共有的共簇对数量与两种聚类中所有唯一共簇对总数的比率。值为 1 表示聚类结果完全相同，而值为 0 表示没有共享的共簇对。我们计算两个这样的指数：\n-   $J_1 = J\\!\\left(S\\!\\left(c^{\\text{L1,raw}}\\right), S\\!\\left(c^{\\text{L1,norm}}\\right)\\right)$，比较归一化前后的曼哈顿聚类。\n-   $J_2 = J\\!\\left(S\\!\\left(c^{\\text{L1,norm}}\\right), S\\!\\left(c^{\\text{L2,norm}}\\right)\\right)$，比较在归一化数据上的曼哈顿聚类和欧几里得聚类。\n\n**步骤 4：轴对齐敏感度分析**\n\n曼哈顿距离对特征的尺度敏感，因为它是沿每个轴的绝对差的简单总和。为了量化这一点，我们定义了一个轴敏感度指数 $S_{\\text{axis}}$。这需要计算每个特征的平均绝对差 $\\mu_j$ 和所有点对的总平均曼哈顿距离 $M$：\n$$\n\\mu_j = \\frac{1}{\\binom{n}{2}} \\sum_{1 \\le i  l \\le n} \\left| x_{ij} - x_{lj} \\right| \\quad \\text{和} \\quad M = \\sum_{j=1}^d \\mu_j\n$$\n每个特征对总平均曼哈顿距离的贡献由分数 $f_j = \\mu_j / M$ 给出。敏感度指数是这些分数中的最大值，捕捉了最主导特征的贡献：\n$$\nS_{\\text{axis}} = \\max_{1 \\le j \\le d} f_j\n$$\n通过将数据归一化为单位方差，特征的尺度被均等化，这有望减少任何单个特征的主导地位。我们计算原始数据（$S_{\\text{axis}}^{\\text{before}}$）和归一化数据（$S_{\\text{axis}}^{\\text{after}}$）的 $S_{\\text{axis}}$。敏感度的减少量则为 $R = S_{\\text{axis}}^{\\text{before}} - S_{\\text{axis}}^{\\text{after}}$。\n\n**步骤 5：最终计算与输出**\n\n对提供的三个测试用例中的每一个执行步骤 1-4 中概述的过程。对于每个用例，我们计算值三元组 $[J_1, J_2, R]$，其中每个浮点数四舍五入到三位小数。然后，根据输出格式的规定，将所有测试用例的结果汇总到一个列表中。",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndef compute_sensitivity(X: np.ndarray) - float:\n    \"\"\"Computes the axis-aligned sensitivity index for a dataset.\"\"\"\n    n, d = X.shape\n    if n  2:\n        return 0.0\n\n    num_pairs = n * (n - 1) / 2\n    \n    # Calculate mu_j for each feature j\n    # mu_j is the mean absolute difference for feature j over all pairs of points\n    mu_j = np.zeros(d)\n    for j in range(d):\n        # pdist on a single column gives the condensed vector of absolute differences\n        # between all pairs, for which we take the sum.\n        col_distances = pdist(X[:, j:j+1], metric='cityblock')\n        mu_j[j] = np.sum(col_distances) / num_pairs\n\n    # M is the mean Manhattan distance over all pairs of points, which is sum of mu_j\n    M = np.sum(mu_j)\n    \n    # If all points are identical, M=0. f_j would be 0/0.\n    # In this scenario, no axis dominates, so sensitivity is arguably 0 or 1/d.\n    # We return 0 as no single axis contributes more than any other.\n    if M == 0:\n        return 0.0\n\n    # f_j are the axis contribution fractions\n    f_j = mu_j / M\n    \n    # S_axis is the maximum contribution fraction\n    S_axis = np.max(f_j)\n    \n    return S_axis\n\ndef get_co_clustered_pairs(labels: np.ndarray) - set:\n    \"\"\"Generates the set of co-clustered unordered pairs.\"\"\"\n    n = len(labels)\n    pairs = set()\n    for i in range(n):\n        for l in range(i + 1, n):\n            if labels[i] == labels[l]:\n                pairs.add((i, l))\n    return pairs\n\ndef jaccard_index(set1: set, set2: set) - float:\n    \"\"\"Computes the Jaccard index between two sets.\"\"\"\n    intersection_size = len(set1.intersection(set2))\n    union_size = len(set1.union(set2))\n    \n    if union_size == 0:\n        return 1.0  # By convention, Jaccard index of two empty sets is 1\n        \n    return intersection_size / union_size\n\ndef analyze_dataset(data: list, k: int) - list:\n    \"\"\"\n    Performs the full analysis for a single dataset: clustering, comparison,\n    and sensitivity measurement.\n    \"\"\"\n    X_raw = np.array(data, dtype=float)\n    \n    # 1. Compute axis sensitivity on raw data\n    S_axis_before = compute_sensitivity(X_raw)\n\n    # 2. Perform hierarchical clustering on raw data with Manhattan distance\n    dist_l1_raw = pdist(X_raw, metric='cityblock')\n    Z_l1_raw = linkage(dist_l1_raw, method='complete')\n    c_l1_raw = fcluster(Z_l1_raw, t=k, criterion='maxclust')\n\n    # 3. Normalize data to unit variance (population std dev)\n    # The problem specifies using population std dev (ddof=0 in numpy.std)\n    sigmas = np.std(X_raw, axis=0, ddof=0)\n    # Handle columns with zero variance by not scaling them (division by 1)\n    non_zero_sigmas = np.where(sigmas > 0, sigmas, 1.0)\n    X_norm = X_raw / non_zero_sigmas\n\n    # 4. Compute axis sensitivity on normalized data\n    S_axis_after = compute_sensitivity(X_norm)\n\n    # 5. Perform clustering on normalized data\n    # Manhattan distance\n    dist_l1_norm = pdist(X_norm, metric='cityblock')\n    Z_l1_norm = linkage(dist_l1_norm, method='complete')\n    c_l1_norm = fcluster(Z_l1_norm, t=k, criterion='maxclust')\n\n    # Euclidean distance\n    dist_l2_norm = pdist(X_norm, metric='euclidean')\n    Z_l2_norm = linkage(dist_l2_norm, method='complete')\n    c_l2_norm = fcluster(Z_l2_norm, t=k, criterion='maxclust')\n\n    # 6. Compute Jaccard indices by comparing co-clustering sets\n    S_l1_raw = get_co_clustered_pairs(c_l1_raw)\n    S_l1_norm = get_co_clustered_pairs(c_l1_norm)\n    S_l2_norm = get_co_clustered_pairs(c_l2_norm)\n\n    J1 = jaccard_index(S_l1_raw, S_l1_norm)\n    J2 = jaccard_index(S_l1_norm, S_l2_norm)\n\n    # 7. Compute reduction in axis sensitivity\n    R = S_axis_before - S_axis_after\n\n    return [round(J1, 3), round(J2, 3), round(R, 3)]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run analysis, and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"k\": 2, \"d\": 2, \"points\": [\n                (-8, 0), (-7, 1), (-6, -1), (-9, 0.5), (-7, -0.5),\n                (3, 4), (4, 4.5), (2.5, 5), (3.5, 3.5), (4.5, 5.2)\n            ]\n        },\n        {\n            \"k\": 3, \"d\": 3, \"points\": [\n                (0, 0, 100), (1, 0, 100), (0, 1, 100),\n                (5, 5, 100), (6, 5, 100), (5, 6, 100),\n                (-4, 5, 100), (-5, 5, 100), (-4, 6, 100)\n            ]\n        },\n        {\n            \"k\": 2, \"d\": 2, \"points\": [\n                (0, 0), (10, 0), (0, 50), (10, 50),\n                (100, 5), (110, 5)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result_triple = analyze_dataset(case[\"points\"], case[\"k\"])\n        results.append(result_triple)\n        \n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{res[0]:.3f},{res[1]:.3f},{res[2]:.3f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在之前的练习中，我们已经探讨了特征变换和归一化的重要性。现在，我们将进入一个更高级的场景：当数据本身具有复杂的几何结构（例如，非球形簇）时，我们该如何选择度量？此练习将通过一个精心构建的例子，展示欧几里得距离在处理细长形簇时的局限性，并引入能够适应数据协方差的马氏距离（Mahalanobis distance）作为解决方案 ()。这个实践将深刻揭示，选择一个能反映数据内在几何特性的度量对于成功聚类至关重要。",
            "id": "3109620",
            "problem": "您的任务是实现一个完整的程序，该程序构建包含两个细长高斯簇的合成数据，应用基于密度的带噪声应用空间聚类 (DBSCAN) 进行聚类，并评估将相异性度量从欧几里得距离切换到适应样本协方差的 Mahalanobis 距离的效果。该程序应完全自包含，并为一个小测试套件生成一行聚合结果的输出。\n\n基本基础包括以下经过充分测试的定义和事实：\n- 均值向量为 $\\mu \\in \\mathbb{R}^2$、协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ 的二元正态分布由概率密度函数 $p(x) = \\frac{1}{2\\pi \\sqrt{\\det(\\Sigma)}} \\exp\\left( -\\frac{1}{2}(x - \\mu)^\\top \\Sigma^{-1} (x - \\mu) \\right)$ 指定，其中 $x \\in \\mathbb{R}^2$。\n- 两点 $x, y \\in \\mathbb{R}^2$ 之间的欧几里得距离为 $d_E(x,y) = \\|x - y\\|_2 = \\sqrt{(x - y)^\\top (x - y)}$。\n- 给定一个对称正定协方差矩阵 $\\Sigma$，两点 $x, y \\in \\mathbb{R}^2$ 之间的 Mahalanobis 距离为 $d_M(x,y) = \\sqrt{(x - y)^\\top \\Sigma^{-1} (x - y)}$。如果通过 Cholesky 分解得到 $\\Sigma = L L^\\top$，$L$ 为下三角矩阵，则 $d_M(x,y) = \\|L^{-1}(x - y)\\|_2$，这意味着 Mahalanobis 距离等于经过线性白化变换 $z = L^{-1} x$ 后的欧几里得距离。\n- DBSCAN 由参数 $\\varepsilon  0$（邻域半径）和 $m \\in \\mathbb{N}$（最小点数）定义。对于数据集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$ 和相异性度量 $d(\\cdot,\\cdot)$，将 $x_i$ 的 $\\varepsilon$-邻域定义为 $N_\\varepsilon(x_i) = \\{x_j : d(x_i, x_j) \\le \\varepsilon\\}$。如果 $|N_\\varepsilon(x_i)| \\ge m$，则点 $x_i$ 是一个核心点。簇是通过从核心点使用密度可达性进行迭代扩展而形成的，将核心点和边界点（某个核心点邻域内的点）都分配给同一个簇，而未分配给任何簇的点则被标记为噪声。\n\n问题要求：\n1. 在 $\\mathbb{R}^2$ 中构建两个细长的高斯簇，其共享的协方差矩阵通过特征分解设计。使用标准正交特征向量 $v_1 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$ 和 $v_2 = \\frac{1}{\\sqrt{2}}(-1,1)^\\top$，以及特征值 $\\lambda_1$（大，沿 $v_1$ 方向产生伸长）和 $\\lambda_2$（小，沿 $v_2$ 方向产生窄分布）。设 $Q = [v_1 \\ v_2] \\in \\mathbb{R}^{2 \\times 2}$，并设置 $\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\lambda_2) Q^\\top$。将两个簇的均值沿低方差方向 $v_2$ 对称放置在 $\\mu_1 = -d \\, v_2$ 和 $\\mu_2 = +d \\, v_2$ 处，其中 $d  0$ 控制半分离幅度。\n2. 根据给定的定义从基本原理出发实现 DBSCAN，不依赖外部聚类库。同时使用 $d_E$ 和 $d_M$，在 Mahalanobis 情况下，您必须使用无偏样本协方差估计量从整个数据集中估计协方差：$$\\hat{\\Sigma} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^\\top,$$ 其中 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$。为确保数值稳定性，在进行 Cholesky 分解前，向 $\\hat{\\Sigma}$ 添加一个小的岭 $\\lambda I$（其中 $\\lambda = 10^{-8}$）。\n3. 对于每个测试用例，运行两次 DBSCAN：一次使用欧几里得距离（基准），另一次使用通过白化变换适应于 $\\hat{\\Sigma}$ 的 Mahalanobis 距离。在每个测试用例的两次运行中使用相同的 $\\varepsilon$ 和 $m$。对于每次运行，报告发现的簇的整数数量（不包括标记为 $-1$ 的噪声）。通过一个布尔值来评估分离度的改善，该布尔值指示在该测试用例中，Mahalanobis 运行是否找到了比欧几里得运行严格更多的簇。\n\n测试套件：\n- 测试用例 1（理想情况）：每个簇 $n_{\\text{per}} = 400$ 个点（总共 $n = 800$），$\\lambda_1 = 100$，$\\lambda_2 = 0.25$， $d = 3$，$\\varepsilon = 7.0$，$m = 10$，随机种子 $= 123$。\n- 测试用例 2（边界条件）：$n_{\\text{per}} = 400$，$\\lambda_1 = 100$，$\\lambda_2 = 0.25$， $d = 3$，$\\varepsilon = 5.5$，$m = 10$，随机种子 $= 321$。\n- 测试用例 3（分离度较小的边缘情况）：$n_{\\text{per}} = 300$，$\\lambda_1 = 100$，$\\lambda_2 = 0.25$，$d = 2$，$\\varepsilon = 5.0$，$m = 10$，随机种子 $= 42$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，每个测试用例贡献一个形式为 $[\\text{clusters\\_euclidean}, \\text{clusters\\_mahalanobis}, \\text{improved}]$ 的子列表。例如，输出格式为 $$[[c_{E,1}, c_{M,1}, b_1],[c_{E,2}, c_{M,2}, b_2],[c_{E,3}, c_{M,3}, b_3]],$$ 其中 $c_{E,i}$ 和 $c_{M,i}$ 是整数，$b_i$ 是布尔值。本问题不涉及任何物理单位、角度单位或百分比表示。",
            "solution": "对用户提供的问题进行了分析，确定该问题是**有效的**。该问题在科学上合理、定义明确、客观，并包含了获得唯一、可验证解所需的所有信息。这是统计学习中的一个标准练习，旨在展示像 Mahalanobis 距离这样的自适应距离度量在对非球形数据分布进行聚类算法中的效用。\n\n### 基于原理的解决方案设计\n\n该解决方案主要分三个阶段进行：合成数据生成，DBSCAN 聚类算法（使用两种不同的距离度量）的实现与应用，以及结果评估。\n\n#### 1. 合成数据生成\n\n问题要求创建一个包含两个独立、细长的簇的数据集。这通过从两个二元高斯分布 $\\mathcal{N}(\\mu_1, \\Sigma)$ 和 $\\mathcal{N}(\\mu_2, \\Sigma)$ 中采样来实现。\n\n1.  **协方差矩阵 $\\Sigma$**：这些簇被设计为细长且旋转的。这通过共享协方差矩阵的特征分解来定义，即 $\\Sigma = Q \\Lambda Q^\\top$。\n    -   特征向量 $v_1 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$ 和 $v_2 = \\frac{1}{\\sqrt{2}}(-1,1)^\\top$ 构成了正交旋转矩阵 $Q = [v_1 \\ v_2]$ 的列。这些向量定义了簇的主轴。\n    -   特征值 $\\lambda_1$（大）和 $\\lambda_2$（小）是沿这些主轴的方差。$\\lambda_1$ 的大值导致沿 $v_1$ 方向的伸长，而小的 $\\lambda_2$ 确保簇沿 $v_2$ 方向是窄的。特征值矩阵为 $\\Lambda = \\operatorname{diag}(\\lambda_1, \\lambda_2)$。\n\n2.  **簇均值 $\\mu_1, \\mu_2$**：两个簇沿其窄轴 $v_2$ 分开。均值向量对称地放置在原点两侧，分别为 $\\mu_1 = -d \\cdot v_2$ 和 $\\mu_2 = d \\cdot v_2$，其中 $d$ 是给定的分离参数。簇中心之间的总欧几里得距离为 $2d$。\n\n对于每个测试用例，我们从两个分布中各生成 $n_{\\text{per}}$ 个数据点，得到一个总共包含 $n = 2 n_{\\text{per}}$ 个点的数据集。指定的随机种子确保了生成数据的可复现性。\n\n#### 2. DBSCAN 的实现与应用\n\n我们从基本原理出发实现基于密度的带噪声应用空间聚类 (DBSCAN) 算法。DBSCAN 根据点的密度识别簇。\n\n-   **核心概念**：该算法由两个参数控制：邻域半径 $\\varepsilon$ 和最小点数 $m$。如果一个点的 $\\varepsilon$-邻域内至少包含 $m$ 个点（包括其自身），则该点是一个**核心点**。**边界点**不是核心点，但位于某个核心点的邻域内。**噪声点**既不是核心点也不是边界点。簇的形成是通过连接彼此邻域内的核心点，并将所有边界点分配给附近核心点的簇。\n\n-   **实现**：我们的实现遍历所有点。如果一个未访问的点被发现是核心点，则启动一个新簇，并通过基于队列的扩展过程找到所有密度可达的点。为了提高效率，我们在开始主循环之前预先计算所有点对之间的距离矩阵。未被分配到任何簇的点被标记为噪声（-1）。\n\n我们对每个测试用例在两种场景下应用此 DBSCAN 实现：\n\n**a. 基准：欧几里得距离 ($d_E$)**\n第一次运行使用标准的欧几里得距离 $d_E(x,y) = \\|x-y\\|_2$。对于细长的簇，这个度量可能会产生误导。同一簇中两端两个点之间的距离可能大于不同但相邻簇中两个点之间的距离。这通常导致 DBSCAN 错误地将不同的细长簇合并，特别是当选择的 $\\varepsilon$ 与簇间分离度相当或更大时。\n\n**b. 自适应：Mahalanobis 距离 ($d_M$)**\n第二次运行通过使用 Mahalanobis 距离来解决欧几里得度量的缺点，该距离考虑了数据的协方差。给定一个协方差矩阵 $S$，距离为 $d_M(x,y) = \\sqrt{(x-y)^\\top S^{-1}(x-y)}$。这个度量有效地在一个“白化”空间中测量距离，在该空间中数据具有单位协方-差矩阵（即是球形的）。\n\n该解决方案利用一种高效的方法来应用此度量，正如问题陈述所建议的：\n1.  **估计协方差**：使用无偏估计量从整个数据集中估计样本协方差矩阵 $\\hat{\\Sigma}$：$\\hat{\\Sigma} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^\\top$。\n2.  **正则化与分解**：为保证数值稳定性，向 $\\hat{\\Sigma}$ 添加一个小的岭 $\\lambda I$（$\\lambda=10^{-8}$）。然后对得到的矩阵使用 Cholesky 分解：$\\hat{\\Sigma}_{\\text{reg}} = L L^\\top$，其中 $L$ 是一个下三角矩阵。\n3.  **白化变换**：将整个数据集 $X$ 变换到一个新的数据集 $Z$ 中，使得 $X$ 上的 Mahalanobis 距离对应于 $Z$ 上的欧几里得距离。变换为 $z_i^\\top = x_i^\\top (L^{-1})^\\top$。\n4.  **聚类**：然后对白化后的数据 $Z$ 使用标准欧几里得距离运行 DBSCAN。变换后点 $z_i$ 和 $z_j$ 之间的距离是 $\\|z_i - z_j\\|_2 = \\|L^{-1}(x_i - x_j)\\|_2$，这恰好是原始点 $x_i$ 和 $x_j$ 之间的 Mahalanobis 距离。\n\n经过此变换后，原始空间中的细长簇在白化空间中变为球形，只要 $\\varepsilon$ 选择得当，就很容易被像 DBSCAN 这样的基于密度的算法分离开。\n\n#### 3. 评估\n对于每个测试用例，找到的簇的数量是通过计算 DBSCAN 分配的唯一正簇标签的数量来确定的（标记为 -1 的噪声被排除）。总输出为：\n-   $c_E$：使用欧几里得距离找到的簇数。\n-   $c_M$：使用 Mahalanobis 距离找到的簇数。\n-   `improved`：一个布尔标志，如果 $c_M > c_E$ 则为 `True`，否则为 `False`。\n\n这个评估直接量化了在给定的聚类问题中使用数据自适应距离度量的优势。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, inv\n\n# Global constants for DBSCAN labels\nUNVISITED = 0\nNOISE = -1\n\ndef _range_query(n_points, dist_matrix, p_idx, eps):\n    \"\"\"Finds all points within eps distance of point p_idx using a precomputed distance matrix.\"\"\"\n    return np.where(dist_matrix[p_idx] = eps)[0]\n\ndef _expand_cluster(n_points, dist_matrix, labels, p_idx, neighbor_indices, cluster_id, eps, m):\n    \"\"\"Expands a cluster from a core point.\"\"\"\n    labels[p_idx] = cluster_id\n    \n    # Use a list as a queue, processing with an index to avoid pop(0)\n    queue = list(neighbor_indices)\n    head_idx = 0\n    while head_idx  len(queue):\n        current_p_idx = queue[head_idx]\n        head_idx += 1\n\n        if labels[current_p_idx] == NOISE:\n            labels[current_p_idx] = cluster_id  # Change noise to border point\n        \n        if labels[current_p_idx] == UNVISITED:\n            labels[current_p_idx] = cluster_id\n            \n            # Find neighbors of the current point\n            current_neighbor_indices = _range_query(n_points, dist_matrix, current_p_idx, eps)\n            \n            if len(current_neighbor_indices) >= m:\n                # This is a core point, add its neighbors to the queue\n                for neighbor_idx in current_neighbor_indices:\n                    # Add only points that are unvisited or noise\n                    if labels[neighbor_idx] in [UNVISITED, NOISE]:\n                        queue.append(neighbor_idx)\n\ndef dbscan(X, eps, m):\n    \"\"\"\n    Performs DBSCAN clustering from first principles.\n\n    Args:\n        X (np.ndarray): Data points, shape (n_samples, n_features).\n        eps (float): The radius of a neighborhood.\n        m (int): The minimum number of points required to form a dense region.\n\n    Returns:\n        np.ndarray: Array of cluster labels for each point. Noise is labeled -1.\n    \"\"\"\n    n_points = X.shape[0]\n    \n    # Pre-compute pairwise Euclidean distance matrix for efficiency\n    dist_matrix = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1))\n    \n    labels = np.full(n_points, UNVISITED, dtype=int)\n    cluster_id = 0\n    \n    for i in range(n_points):\n        if labels[i] != UNVISITED:\n            continue\n            \n        neighbor_indices = _range_query(n_points, dist_matrix, i, eps)\n        \n        if len(neighbor_indices)  m:\n            labels[i] = NOISE\n        else:\n            cluster_id += 1\n            _expand_cluster(n_points, dist_matrix, labels, i, neighbor_indices, cluster_id, eps, m)\n            \n    return labels\n\ndef generate_data(n_per, lambda1, lambda2, d, seed):\n    \"\"\"Generates two elongated Gaussian clusters.\"\"\"\n    np.random.seed(seed)\n    \n    v1 = np.array([1, 1]) / np.sqrt(2)\n    v2 = np.array([-1, 1]) / np.sqrt(2)\n    \n    Q = np.column_stack([v1, v2])\n    Lambda = np.diag([lambda1, lambda2])\n    Sigma = Q @ Lambda @ Q.T\n    \n    mu1 = -d * v2\n    mu2 = d * v2\n    \n    cluster1 = np.random.multivariate_normal(mu1, Sigma, n_per)\n    cluster2 = np.random.multivariate_normal(mu2, Sigma, n_per)\n    \n    return np.vstack([cluster1, cluster2])\n\ndef run_test_case(n_per, lambda1, lambda2, d, eps, m, seed):\n    \"\"\"Runs a single test case for the problem.\"\"\"\n    X = generate_data(n_per, lambda1, lambda2, d, seed)\n    n = X.shape[0]\n\n    # 1. Euclidean DBSCAN\n    labels_euclidean = dbscan(X, eps, m)\n    # Number of clusters is the max label, excluding noise (-1)\n    clusters_euclidean = len(np.unique(labels_euclidean[labels_euclidean > 0]))\n\n    # 2. Mahalanobis DBSCAN via whitening transform\n    # Estimate sample covariance\n    mu_hat = np.mean(X, axis=0)\n    Sigma_hat = np.cov(X, rowvar=False)\n    \n    # Regularize for numerical stability\n    ridge = 1e-8\n    Sigma_hat_reg = Sigma_hat + ridge * np.eye(X.shape[1])\n    \n    # Cholesky decomposition and whitening\n    L = cholesky(Sigma_hat_reg, lower=True)\n    L_inv = inv(L)\n    \n    # Transform data into whitened space\n    # z_i^T = (L^{-1} x_i^T)^T = x_i (L^{-1})^T\n    Z = X @ L_inv.T\n    \n    # Run DBSCAN on whitened data (equivalent to Mahalanobis on original data)\n    labels_mahalanobis = dbscan(Z, eps, m)\n    clusters_mahalanobis = len(np.unique(labels_mahalanobis[labels_mahalanobis > 0]))\n    \n    # 3. Evaluate improvement\n    improved = clusters_mahalanobis > clusters_euclidean\n    \n    return [clusters_euclidean, clusters_mahalanobis, improved]\n\ndef solve():\n    \"\"\"Main function to run test suite and print results.\"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_per, lambda1, lambda2, d, eps, m, seed)\n        (400, 100, 0.25, 3, 7.0, 10, 123),\n        (400, 100, 0.25, 3, 5.5, 10, 321),\n        (300, 100, 0.25, 2, 5.0, 10, 42),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_per, lambda1, lambda2, d, eps, m, seed = case\n        result = run_test_case(n_per, lambda1, lambda2, d, eps, m, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    case_strings = [f\"[{c_e},{c_m},{str(b)}]\" for c_e, c_m, b in results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n\n```"
        }
    ]
}