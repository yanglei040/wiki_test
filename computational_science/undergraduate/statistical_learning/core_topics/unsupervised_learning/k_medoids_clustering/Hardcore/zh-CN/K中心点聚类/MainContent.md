## 引言
K-medoids 是一种基础而强大的[划分聚类](@entry_id:166920)方法，在数据科学工具箱中占有重要地位。与广为人知的 K-means 算法不同，K-medoids 通过选择数据集中的实际观测点作为簇的代表（即“[中心点](@entry_id:636820)”），赋予了其独特的稳健性和灵活性。这一特性解决了数据科学中的一个核心挑战：当数据包含异常值，或者数据对象（如基因序列或客户资料）无法用简单的向量均值来表示时，我们应如何进行有效的[聚类](@entry_id:266727)？本文旨在填补这一知识空白，系统地阐述 K-medoids 的内在逻辑和实践价值。在接下来的章节中，我们将首先深入剖析其 **原则与机制**，从目标函数到核心的 PAM 算法，揭示其对异常值的稳健性以及处理任意[距离度量](@entry_id:636073)的能力。接着，我们将跨越学科界限，在 **应用与跨学科连接** 中展示 K-medoids 如何在[计算生物学](@entry_id:146988)、市场营销乃至[算法公平性](@entry_id:143652)等不同领域解决实际问题。最后，通过一系列 **动手实践** 练习，您将有机会将理论应用于具体情境，巩固对这一强大工具的理解。

## 原则与机制

本章旨在深入探讨 K-Medoids [聚类算法](@entry_id:146720)的核心原则与工作机制。我们将从其目标函数的定义出发，剖析其作为[聚类](@entry_id:266727)[中心点](@entry_id:636820)的独特性质，阐明其处理任意差异性度量的强大能力，并详细介绍实现该算法的关键技术，包括经典的 PAM 算法、其[计算效率](@entry_id:270255)的挑战与优化，以及其在[优化理论](@entry_id:144639)中的深刻根基。

### K-Medoids 的[目标函数](@entry_id:267263)

K-Medoids 聚类的核心思想是将一个包含 $n$ 个数据点的数据集 $X = \{x_1, x_2, \dots, x_n\}$ 划分为 $k$ 个簇。其独特之处在于，它要求每个簇的代表点（即 **中心点**, **medoid**）必须是数据集中的一个实际数据点。

给定一个用于衡量数据点之间差异的 **差异性函数 (dissimilarity function)** $d(x_i, x_j)$，以及一个包含 $k$ 个中心点的集合 $M \subset X$（其中 $|M|=k$），K-Medoids 算法的目标是最小化所有数据点到其最近中心点的差异性之和。这个[目标函数](@entry_id:267263) $J(M)$ 可以表示为：

$$
J(M) = \sum_{i=1}^{n} \min_{m \in M} d(x_i, m)
$$

此[目标函数](@entry_id:267263)包含两个关键要素：差异性函数 $d(\cdot, \cdot)$ 的选择，以及[中心点](@entry_id:636820)必须从原始数据集中选取的约束 $M \subset X$。正是这两个要素共同赋予了 K-Medoids 算法独特的优势和适用场景。

### 中心点：Medoid 的定义与性质

为了深刻理解 K-Medoids，我们必须首先理解其核心概念——**medoid**——作为数据中心的本质。

#### Medoid 的概念：与均值和几何中位数的对比

在最简单的情形下（$k=1$），整个数据集只有一个中心点。这个 **1-medoid** 被定义为数据集中使得与所有其他点的差异性之和最小的那个点。

这个定义将 medoid 与其他常见的[中心性度量](@entry_id:144795)区分开来。例如，我们所熟知的 **[算术平均值](@entry_id:165355) (arithmetic mean)** 是最小化点到中心*平方欧氏距离*之和的中心，即 $\arg\min_z \sum_i \|x_i - z\|_2^2$。而 **几何中位数 (geometric median)** 则是最小化点到中心*欧氏距离*之和的中心，即 $\arg\min_{z \in \mathbb{R}^d} \sum_i \|x_i - z\|_2$。

一个至关重要的区别是，算术平均值和几何[中位数](@entry_id:264877)都可以是空间中的任意点，不一定存在于原始数据集中。而 medoid 根据其定义，必须是数据集中的一员。

考虑一个简单的一维数据集 $\mathcal{S}=\{0, 2, 3, 10\}$。
要找到它的 1-medoid，我们需要在集合 $\mathcal{S}$ 中寻找一个点 $m$，使其与集合中所有点的距离（[绝对值](@entry_id:147688)）之和最小。
- 若选择 $m=0$，总距离为 $|0-0| + |2-0| + |3-0| + |10-0| = 15$。
- 若选择 $m=2$，总距离为 $|0-2| + |2-2| + |3-2| + |10-2| = 11$。
- 若选择 $m=3$，总距离为 $|0-3| + |2-3| + |3-3| + |10-3| = 11$。
- 若选择 $m=10$，总距离为 $|0-10| + |2-10| + |3-10| + |10-10| = 25$。
最小值为 $11$，因此点 $2$ 和 $3$ 都是这个数据集的 1-medoid。

然而，该数据集的几何[中位数](@entry_id:264877)是使函数 $f(z) = |z-0| + |z-2| + |z-3| + |z-10|$ 最小化的任意实数 $z$。可以证明，对于包含偶数个点的一维数据集，所有位于中间两个点之间的闭区间内的点都是几何中位数。在此例中，几何[中位数](@entry_id:264877)的集合是区间 $[2, 3]$。这意味着像 $2.5$ 这样的点也是几何[中位数](@entry_id:264877)，但它显然不在原始数据集 $\mathcal{S}$ 中 。

medoid 必须是数据点的这一约束并非人为的限制，而是处理非向量数据时的必然要求。对于某些[度量空间](@entry_id:138860)，最小化总差异性的“理论[中心点](@entry_id:636820)”（有时称为 **弗雷歇均值 (Fréchet mean)**）可能根本不存在于原始数据点之中。例如，对于一个由字符串 `{"011", "101", "110"}` 组成的集合，在 **[编辑距离](@entry_id:152711) (edit distance)** 下，可以证明字符串 `"111"` 是一个比任何原始字符串都更好的中心，因为它到三个原始字符串的距离之和更小。然而，我们无法凭空“创造”出这个 `"111"`。K-Medoids 通过将搜索空间限制在已有的数据点上，巧妙地回避了这个问题 。

#### 对异常值的稳健性

Medoid 的另一个关键特性是其对 **异常值 (outliers)** 的 **稳健性 (robustness)**。算术平均值对异常值极为敏感，单个极端值就能将其“拉”向很远的位置。相比之下，medoid（以及更广泛意义上的中位数）则稳健得多。

我们可以通过一个思想实验来量化这种稳健性。假设一个核心数据集 $S$ 被 $m$ 个位于遥远位置 $y$ 的相同异常值所污染。我们想知道，需要多少个异常值才能将一个非最优的候选点 $x_b$ “提升”为新的唯一 medoid。这需要解出一系列关于 $m$ 的不等式。对于每个竞争者 $p \neq x_b$，我们要求：

$$
\sum_{s_i \in S} d(x_b, s_i) + m \cdot d(x_b, y)  \sum_{s_i \in S} d(p, s_i) + m \cdot d(p, y)
$$

通过整理，我们得到关于 $m$ 的条件。分析表明，如果目标点 $x_b$ 比竞争者 $p$ 离异常值更近，那么增加 $m$ 将有助于 $x_b$ 获胜。反之，如果 $x_b$ 比 $p$ 离异常值更远，那么 $m$ 的增加将对 $x_b$ 不利，甚至可能存在一个 $m$ 的上限，超过该上限 $x_b$ 将永无胜算 。这种分析也揭示了，在使用 $L_1$ 范数（[曼哈顿距离](@entry_id:141126)）时，medoid 通常比使用 $L_2$ 范数（欧氏距离）时更加稳健，因为它对大偏差的惩罚是线性的，而非平方的。

这种稳健性也体现在高维度的 **噪声特征 (noise features)** 中。在许多现实场景中，数据可能包含少数提供清晰聚类信号的维度和大量无关的噪声维度。在一个模拟实验中，如果数据点的中心更新规则采用[算术平均值](@entry_id:165355)（如 K-Means），噪声维度中的极端值会严重扭曲中心的计算。而 K-Medoids 的[中心点](@entry_id:636820)由于必须是真实数据点，它被“锚定”在数据的内在[流形](@entry_id:153038)上，受噪声维度的影响要小得多 。

### 任意差异性度量的威力

K-Medoids 算法相较于 K-Means 等其他原型[聚类方法](@entry_id:747401)，其最显著的优势在于能够使用 **任意的差异性度量**，只要该度量能计算出数据集中任意两点之间的差异值即可。

K-Means 算法在更新聚类中心时，需要计算簇内所有点的算术平均值。这一操作内在地要求数据存在于一个[向量空间](@entry_id:151108)中，其中加法和[标量乘法](@entry_id:155971)是有定义的。然而，在许多应用领域，数据对象并非向量，例如文本字符串、蛋白质序列、图结构或时间序列。对于这些数据，计算“平均字符串”或“平均图”是没有意义的。

K-Medoids 完美地解决了这一问题。回顾其[目标函数](@entry_id:267263)和算法流程，我们可以发现它仅需要一个预先计算好的、包含所有点对之间差异值的 $n \times n$ 矩阵。算法的所有步骤——将点分配给最近的[中心点](@entry_id:636820)，以及评估交换[中心点](@entry_id:636820)带来的成本变化——都只依赖于查阅这个矩阵。由于中心点本身就是数据点，算法从不需要在抽象空间中创造新的代表点。

让我们用一个具体的例子来说明。考虑一个由字符串组成的微型数据集：$\{\text{cat}, \text{cut}, \text{cot}, \text{cute}, \text{dog}\}$。我们可以使用 **[莱文斯坦距离](@entry_id:152711) (Levenshtein distance)** 作为差异性度量，它定义了将一个字符串转换为另一个所需的最少单字符编辑（插入、删除、替换）次数。例如，$d(\text{cat}, \text{cut})=1$，$d(\text{cat}, \text{dog})=3$。

假设我们希望用 $k=2$ 个[中心点](@entry_id:636820)来[聚类](@entry_id:266727)这个数据集。K-Medoids 算法会在 5 个字符串中选择 2 个作为中心点。通过穷举所有 $\binom{5}{2}=10$ 种可能的[中心点](@entry_id:636820)组合，并为每种组合计算总的目标函数值（即每个字符串到其最近[中心点](@entry_id:636820)的距离之和），我们可以发现选择 $\{\text{cut}, \text{dog}\}$作为中心点时，总差异性最小，其值为 $3$。此时，形成的两个簇为 $\{\text{cat}, \text{cut}, \text{cot}, \text{cute}\}$ 和 $\{\text{dog}\}$。这个过程完全不依赖于任何[向量空间](@entry_id:151108)运算，展示了 K-Medoids 在处理复杂数据类型上的强大灵活性 。

### 寻找[中心点](@entry_id:636820)的算法机制

最小化 K-Medoids [目标函数](@entry_id:267263)是一个组合优化问题。对于中等或大规模的数据集，穷举所有可能的 $k$ 个[中心点](@entry_id:636820)组合（共 $\binom{n}{k}$ 种）是不可行的，因为这个问题是 **NP-难 (NP-hard)** 的。因此，我们通常依赖启发式算法来寻找一个高质量的近似解。

#### 围绕中心点划分 (PAM) 算法

**PAM (Partitioning Around Medoids)** 算法是解决 K-Medoids 问题的经典方法。它是一种贪心[局部搜索](@entry_id:636449)算法，分为两个主要阶段：
1.  **BUILD 阶段**：选择一个初始的中心点集合 $M$。
2.  **SWAP 阶段**：迭代地尝试通过交换中心点与非[中心点](@entry_id:636820)来改进当前的[聚类](@entry_id:266727)方案，直到无法进一步降低[目标函数](@entry_id:267263)值。

SWAP 阶段是 PAM 算法的核心。在每一轮迭代中，算法会系统地考虑所有可能的单个交换操作。一个交换操作是指，用一个当前非[中心点](@entry_id:636820)的数据点 $h$ 替换掉一个当前的[中心点](@entry_id:636820) $i$。总共有 $k(n-k)$ 种这样的交换可能。对于每一种可能的交换，算法会计算其对总目标函数 $J(M)$ 的影响。然后，它会执行那个能带来最大成本降低的交换。如果没有任何[交换能](@entry_id:137069)降低成本，算法就停止，因为它已经到达了一个 **局部最优解 (local minimum)**。在实践中，我们可以设置一个阈值 $\epsilon \ge 0$，当最大的成本降低量不超过 $\epsilon$ 时，算法就终止 。

#### 初始化的重要性与局部最优问题

像所有[局部搜索](@entry_id:636449)算法一样，PAM 的最终结果高度依赖于初始[中心点](@entry_id:636820)的选择。一个糟糕的起点可能导致算法陷入一个质量很差的局部最优解。

我们可以构建一个特定的数据集来清晰地展示这个问题。设想一个数据集由两部分构成：一条线上密布的大量点，以及一个远离该线的、独立的紧凑点簇。如果采用简单的 **朴素初始化**（例如，选择数据集中的前 $k$ 个点），初始中心点可能全部落在线上。PAM 算法在尝试将一个线上[中心点](@entry_id:636820)交换到远处的点簇时，可能会发现这样做虽然极大地降低了远处点簇的成本，但由于失去了一个线上[中心点](@entry_id:636820)，线上部分的总成本增加得更多，导致整体成本上升。因此，算法会拒绝这次交换，被“困”在一个所有[中心点](@entry_id:636820)都在线上的次优解中。

相比之下，一种更智能的初始化策略，如 **K-Medoids++**，可以有效避免这种陷阱。K-Medoids++ 的思想是选择彼此之间相距较远的初始中心点。一种确定性的实现方式是：首先选择离整个数据集质心最远的点作为第一个中心点，然后迭代地选择下一个中心点，使其与已选[中心点](@entry_id:636820)集合的[最近距离](@entry_id:164459)最大化。在这种策略下，那个遥远的点簇由于其距离的显著性，几乎肯定会在早期被选为中心点。有了这个好的起点，PAM 算法就能在此基础上进行微调，最终达到一个质量高得多的解 。

#### 计算复杂性与[可扩展性](@entry_id:636611)

PAM 算法虽然概念简单且有效，但其计算成本很高。在每次 SWAP 迭代中，朴素的实现需要评估 $k(n-k)$ 次交换。每次评估都需要遍历所有 $n$ 个数据点来重新计算目标函数，导致每次迭代的复杂度高达 $O(k(n-k)n)$。对于大型数据集，这是无法接受的。

**CLARA (Clustering Large Applications)** 算法是为解决 PAM 的[可扩展性](@entry_id:636611)问题而提出的。CLARA 的策略是基于采样的：它不是在整个数据集上运行 PAM，而是在数据集的多个随机小[子集](@entry_id:261956)上运行 PAM。它会执行若干次这样的过程，每次都从数据集中抽取一个样本，用 PAM 找到这个样本的最优中心点，然后用这些[中心点](@entry_id:636820)来评估在整个数据集上的聚类质量（即目标函数值）。最后，CLARA 返回所有采样运行中找到的最佳[中心点](@entry_id:636820)集合。

CLARA 的性能取决于样本能否代表整体[数据结构](@entry_id:262134)，特别是能否包含至少一个“真正”的最优[中心点](@entry_id:636820)。我们可以从概率上分析这个问题。如果总共有 $N$ 个点，其中有 $k$ 个是（未知的）全局最优中心点，那么在一个大小为 $s$ 的随机样本中，包含至少一个最优中心点的概率有一个简单的下界：

$$
P(\text{至少一个最优中心点在样本中}) \ge 1 - \left(1 - \frac{k}{N}\right)^s
$$

这个公式允许我们根据期望的成功概率来确定所需的最小样本量 $s$。CLARA 通过用多次小规模的 PAM 计算代替一次大规模的计算，显著降低了总成本，使其能够应用于大型数据集 。

此外，我们也可以直接对 PAM 算法本身进行优化。如果差异性度量满足 **[三角不等式](@entry_id:143750) (triangle inequality)**，我们可以在 SWAP 步骤中 **剪枝 (prune)**掉大量不必要的计算。例如，在评估是否要将点 $x$ 的归属从其当前[中心点](@entry_id:636820) $m(x)$ 切换到一个新的候选中心点 $h$ 时，我们不必总是计算 $d(x,h)$。利用[三角不等式](@entry_id:143750) $d(x,h) \ge |d(x,c) - d(h,c)|$（其中 $c$ 是任意点），我们可以推导出 $d(x,h)$ 的一个下界。如果这个下界已经大于 $x$ 到其当前[中心点](@entry_id:636820)的距离，那么 $h$ 就不可能成为 $x$ 的新中心点，我们就可以安全地跳过对 $d(x,h)$ 的精确计算。这种优化可以显著加快 PAM 算法的运行速度 。

### 理论基础：与[设施选址问题](@entry_id:172318)的联系

K-Medoids 问题不仅是一个实用的[聚类](@entry_id:266727)工具，它在理论上也与运筹学中的一个经典问题——**离散 k-中位数[设施选址问题](@entry_id:172318) (discrete k-median facility location problem)**——有着深刻的联系。

我们可以将 K-Medoids 问题重新表述为：
-   **客户 (clients)**：数据集中的所有 $n$ 个点。
-   **潜在设施位置 (potential facility locations)**：同样是数据集中的所有 $n$ 个点。
-   **目标**：从 $n$ 个潜在位置中选择 $k$ 个位置开设“设施”（即成为中心点），并为每个“客户”分配到最近的设施，使得所有客户到其服务设施的总“[运输成本](@entry_id:274604)”（即差异性）最小。

这种等价性使我们能够运用[组合优化](@entry_id:264983)中的强大工具来分析和求解 K-Medoids 问题。具体来说，我们可以构建一个 **[混合整数线性规划](@entry_id:636618) (Mixed Integer Linear Program, MILP)** 模型。设[二元变量](@entry_id:162761) $y_j \in \{0, 1\}$ 表示是否选择数据点 $x_j$ 作为中心点（$y_j=1$ 表示是），[二元变量](@entry_id:162761) $x_{ij} \in \{0, 1\}$ 表示是否将点 $x_i$ 分配给中心点 $x_j$（$x_{ij}=1$ 表示是）。

最小化目标 $\sum_{i,j} d(x_i, x_j) x_{ij}$，需满足以下约束：
1.  **中心点数量约束**：必须恰好选择 $k$ 个中心点，即 $\sum_{j=1}^n y_j = k$。
2.  **分配约束**：每个点必须被分配给且仅被分配给一个[中心点](@entry_id:636820)，即对每个 $i$，$\sum_{j=1}^n x_{ij} = 1$。
3.  **耦合约束**：一个点 $x_i$ 只能被分配给一个“开放”的[中心点](@entry_id:636820) $x_j$，即对所有 $i, j$，$x_{ij} \le y_j$。

虽然直接求解这个 MILP 仍然是 NP-难的，但我们可以通过 **[线性规划松弛](@entry_id:267116) (LP relaxation)** 来寻找近似解。松弛方法允许变量 $y_j$ 和 $x_{ij}$ 取 $[0, 1]$ 区间内的连续值。求解这个松弛后的 LP 问题虽然得到的是分数解（例如，$y_j=0.5$），但它为原问题的最优解提供了一个下界，并且有多种 **舍入 (rounding)** 技术可以将这些分数解转换回一个高质量的可行整数解。这个理论视角将 K-Medoids 置于一个更广阔的近似算法框架内，为设计更高级的算法提供了理论指导 。