{
    "hands_on_practices": [
        {
            "introduction": "In many real-world classification problems, such as fraud detection or disease screening, one class is far more common than the other. This exercise  demonstrates why accuracy is a poor and often misleading metric in these imbalanced scenarios. By working through a hypothetical case from foundational principles, you will gain a deeper intuition for why metrics like the $F_1$-score and Balanced Accuracy provide a more faithful assessment of a classifier's performance.",
            "id": "3118882",
            "problem": "You are given a binary classification setting with severe class imbalance. The true label is denoted by $y \\in \\{0,1\\}$, with a fixed base rate $\\mathbb{P}(y=1)=p=0.01$. A classifier maps each instance into a class label $\\hat{y} \\in \\{0,1\\}$. A probabilistic model maps each instance into a score $s \\in [0,1]$ interpreted as a calibrated probability, meaning for any score value $v$ in the range of $s$, the conditional probability satisfies $\\mathbb{P}(y=1 \\mid s=v) = v$. To convert scores into hard labels, one uses a threshold rule of the form \"predict $\\hat{y}=1$ if and only if $s \\ge \\theta$,\" with a specified tie convention that includes equality in the positive class. Your tasks are to derive and compute evaluation metrics for several cases using only core definitions and the law of total probability.\n\nStart from the fundamental base: the confusion outcomes defined by True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN) events. Work with population fractions rather than finite-sample counts. Use the law of total probability to obtain each confusion component from calibrated probability strata and thresholding decisions. Then, using the standard definitions of Accuracy, Balanced Accuracy (mean of True Positive Rate and True Negative Rate), and the F-score of order one (F1), compute the requested quantities below. If a ratio requires division by zero in its denominator, adopt the convention that the ratio is zero. In particular, if no predictions are made for the positive class, define precision to be zero and hence the F1 value to be zero. All values must be expressed as decimals, not as percentages.\n\nSynthetic calibrated two-bin model. Consider a score model that outputs only two values: a \"high\" score $p_h$ on a subset $S_h$ of instances with mass $w \\in (0,1)$, and a \"low\" score $p_\\ell$ on the complement $S_\\ell$ with mass $1-w$. The model is calibrated, so the conditional positive rate on $S_h$ is $p_h$ and on $S_\\ell$ is $p_\\ell$. The overall positive base rate must equal $p$, so $p = w \\, p_h + (1-w)\\, p_\\ell$. You must use this identity to set $p_\\ell$ from the specified parameters when it is not explicitly given. For a threshold $\\theta$, the decision regions are: predict positive on any stratum whose score is at least $\\theta$ and negative otherwise.\n\nTest suite. Compute, for each case below, the Accuracy, Balanced Accuracy, and F1 according to the above definitions. Output each metric rounded to exactly six digits after the decimal point.\n\n- Case A (always-negative classifier): A trivial classifier that predicts $\\hat{y}=0$ for all instances, irrespective of inputs.\n- Case B (calibrated, threshold at high score): Two-bin calibrated model with $w=0.02$, $p_h=0.50$, and $p_\\ell$ determined by the base rate $p=0.01$. Use threshold $\\theta=0.50$.\n- Case C (calibrated, threshold too high): Two-bin calibrated model with $w=0.05$, $p_h=0.20$, and $p_\\ell$ determined by $p=0.01$. Use threshold $\\theta=0.50$.\n- Case D (calibrated, F1-optimal among discrete policies): Same parameters as Case C. Choose the prediction policy that maximizes F1 among the three distinct hard-labeling regimes induced by thresholds: predict none positive (equivalently, $\\thetap_h$), predict only the high-score bin as positive (equivalently, $p_\\ell\\theta\\le p_h$), or predict both bins as positive (equivalently, $\\theta\\le p_\\ell$). In case of ties in F1, select the regime with the fewest predicted positives.\n\nFinal output format. Your program should produce a single line of output containing $12$ values, in this exact order:\n$[\\text{Acc}_A,\\text{BalAcc}_A,\\text{F1}_A,\\text{Acc}_B,\\text{BalAcc}_B,\\text{F1}_B,\\text{Acc}_C,\\text{BalAcc}_C,\\text{F1}_C,\\text{Acc}_D,\\text{BalAcc}_D,\\text{F1}_D]$,\nrounded to six digits after the decimal point, and printed as a comma-separated list enclosed in square brackets (e.g., $[0.000000,0.000000,\\dots]$). No other output is permitted.",
            "solution": "The problem requires the calculation of three evaluation metrics—Accuracy, Balanced Accuracy, and the F1-score—for a binary classifier under several scenarios. The setting is a population-level analysis, not a finite sample, characterized by a base rate of positive instances $\\mathbb{P}(y=1) = p = 0.01$.\n\nFirst, we establish the definitions of the metrics based on the population fractions of the four confusion matrix outcomes: True Positives ($TP$), False Positives ($FP$), True Negatives ($TN$), and False Negatives ($FN$). These fractions sum to $1$: $TP+FP+TN+FN=1$.\n\nThe total positive and negative populations are given by:\n$$TP + FN = \\mathbb{P}(y=1) = p$$\n$$TN + FP = \\mathbb{P}(y=0) = 1-p$$\n\nThe metrics are defined as:\n1.  **Accuracy ($Acc$)**: The fraction of correct predictions.\n    $$Acc = TP + TN$$\n2.  **True Positive Rate ($TPR$)** or **Recall ($Rec$)**: The fraction of actual positives correctly identified.\n    $$TPR = Rec = \\frac{TP}{TP+FN} = \\frac{TP}{p}$$\n3.  **True Negative Rate ($TNR$)**: The fraction of actual negatives correctly identified.\n    $$TNR = \\frac{TN}{TN+FP} = \\frac{TN}{1-p}$$\n4.  **Balanced Accuracy ($BalAcc$)**: The arithmetic mean of $TPR$ and $TNR$.\n    $$BalAcc = \\frac{1}{2}(TPR + TNR)$$\n5.  **Precision ($Prec$)**: The fraction of positive predictions that are correct.\n    $$Prec = \\frac{TP}{TP+FP}$$\n    The denominator $TP+FP = \\mathbb{P}(\\hat{y}=1)$ is the total fraction of instances predicted as positive. If $\\mathbb{P}(\\hat{y}=1)=0$, then $Prec=0$ by convention.\n6.  **F1-Score ($F1$)**: The harmonic mean of Precision and Recall.\n    $$F1 = 2 \\cdot \\frac{Prec \\cdot Rec}{Prec + Rec}$$\n    If $Prec+Rec=0$, which occurs if $TP=0$, then $F1=0$.\n\nThe problem introduces a calibrated two-bin score model. The model outputs a \"high\" score $s=p_h$ on a fraction $w$ of the data, and a \"low\" score $s=p_\\ell$ on the remaining fraction $1-w$. Calibration implies $\\mathbb{P}(y=1|s=p_h)=p_h$ and $\\mathbb{P}(y=1|s=p_\\ell)=p_\\ell$. The overall base rate $p$ is constrained by the law of total probability: $p = w \\cdot p_h + (1-w) \\cdot p_\\ell$. From this, we can determine $p_\\ell$ as $p_\\ell = \\frac{p - w p_h}{1-w}$.\n\nThe confusion matrix components can be derived by considering the classifier's decision rule, \"predict $\\hat{y}=1$ if $s \\ge \\theta$\", and the joint probabilities of score and true label.\n$$TP = \\mathbb{P}(\\hat{y}=1, y=1) \\qquad FP = \\mathbb{P}(\\hat{y}=1, y=0)$$\n$$TN = \\mathbb{P}(\\hat{y}=0, y=0) \\qquad FN = \\mathbb{P}(\\hat{y}=0, y=1)$$\n\nLet's analyze the three distinct prediction policies possible with a two-bin model, assuming $p_h  p_\\ell$.\n\n**Policy 1: Always predict negative ($\\hat{y}=0$)**. This occurs for a threshold $\\theta  p_h$.\n- $\\mathbb{P}(\\hat{y}=1)=0$, so $TP=0, FP=0$.\n- All instances are predicted negative, so $FN = \\mathbb{P}(y=1) = p$ and $TN = \\mathbb{P}(y=0) = 1-p$.\n- Metrics:\n    - $Acc = TP+TN = 1-p$.\n    - $TPR = 0/p = 0$. $TNR = (1-p)/(1-p) = 1$.\n    - $BalAcc = \\frac{1}{2}(0+1) = 0.5$.\n    - $Prec=0$ (by convention), $Rec=0$.\n    - $F1 = 0$.\n\n**Policy 2: Predict high-score bin positive, low-score bin negative**. This occurs for $p_\\ell  \\theta \\le p_h$.\n- $\\hat{y}=1$ only for the $S_h$ stratum (mass $w$).\n- $TP = \\mathbb{P}(s=p_h, y=1) = \\mathbb{P}(y=1|s=p_h)\\mathbb{P}(s=p_h) = p_h w$.\n- $FP = \\mathbb{P}(s=p_h, y=0) = (1-p_h)w$.\n- $TN = \\mathbb{P}(s=p_\\ell, y=0) = (1-p_\\ell)(1-w)$.\n- $FN = \\mathbb{P}(s=p_\\ell, y=1) = p_\\ell(1-w)$.\n- Metrics:\n    - $Acc = p_h w + (1-p_\\ell)(1-w)$.\n    - $TPR = \\frac{p_h w}{p}$. $TNR = \\frac{(1-p_\\ell)(1-w)}{1-p}$.\n    - $BalAcc = \\frac{1}{2} \\left( \\frac{p_h w}{p} + \\frac{(1-p_\\ell)(1-w)}{1-p} \\right)$.\n    - $Prec = \\frac{p_h w}{p_h w + (1-p_h)w} = \\frac{p_h w}{w} = p_h$.\n    - $Rec = TPR = \\frac{p_h w}{p}$.\n    - $F1 = 2 \\frac{p_h \\cdot (p_h w/p)}{p_h + p_h w/p} = \\frac{2 p_h^2 w}{p_h(p+w)} = \\frac{2 p_h w}{p+w}$.\n\n**Policy 3: Always predict positive ($\\hat{y}=1$)**. This occurs for $\\theta \\le p_\\ell$.\n- $\\mathbb{P}(\\hat{y}=0)=0$, so $TN=0, FN=0$.\n- All instances are predicted positive, so $TP = \\mathbb{P}(y=1) = p$ and $FP = \\mathbb{P}(y=0) = 1-p$.\n- Metrics:\n    - $Acc = TP+TN = p$.\n    - $TPR = p/p = 1$. $TNR = 0/(1-p) = 0$.\n    - $BalAcc = \\frac{1}{2}(1+0) = 0.5$.\n    - $Prec = \\frac{p}{p+(1-p)} = p$. $Rec=1$.\n    - $F1 = 2 \\frac{p \\cdot 1}{p+1} = \\frac{2p}{p+1}$.\n\nWith these general formulas, we can compute the values for each case. The base rate is $p=0.01$.\n\n**Case A: Always-negative classifier**\nThis is Policy 1.\n- $Acc_A = 1-p = 1-0.01 = 0.99$.\n- $BalAcc_A = 0.5$.\n- $F1_A = 0$.\nValues: $0.990000, 0.500000, 0.000000$.\n\n**Case B: Calibrated, threshold at high score**\nParameters: $w=0.02$, $p_h=0.50$, threshold $\\theta=0.50$.\nFirst, we find $p_\\ell$: $p_\\ell = \\frac{0.01 - 0.02 \\cdot 0.50}{1-0.02} = \\frac{0.01 - 0.01}{0.98} = 0$.\nThe scores are $p_h=0.50$ and $p_\\ell=0$. Since $0  \\theta=0.50 \\le 0.50$, this is Policy 2.\n- $Acc_B = p_h w + (1-p_\\ell)(1-w) = 0.50 \\cdot 0.02 + (1-0)(1-0.02) = 0.01 + 0.98 = 0.99$.\n- $TPR_B = \\frac{p_h w}{p} = \\frac{0.50 \\cdot 0.02}{0.01} = 1$.\n- $TNR_B = \\frac{(1-p_\\ell)(1-w)}{1-p} = \\frac{(1-0)(1-0.02)}{1-0.01} = \\frac{0.98}{0.99}$.\n- $BalAcc_B = \\frac{1}{2}(1 + \\frac{0.98}{0.99}) = \\frac{1.97}{1.98} \\approx 0.994949$.\n- $F1_B = \\frac{2 p_h w}{p+w} = \\frac{2 \\cdot 0.50 \\cdot 0.02}{0.01+0.02} = \\frac{0.02}{0.03} = \\frac{2}{3} \\approx 0.666667$.\nValues: $0.990000, 0.994949, 0.666667$.\n\n**Case C: Calibrated, threshold too high**\nParameters: $w=0.05$, $p_h=0.20$, threshold $\\theta=0.50$.\nFirst, we find $p_\\ell$: $p_\\ell = \\frac{0.01 - 0.05 \\cdot 0.20}{1-0.05} = \\frac{0.01 - 0.01}{0.95} = 0$.\nThe scores are $p_h=0.20$ and $p_\\ell=0$. Since $\\theta=0.50  p_h=0.20$, this is Policy 1.\nThe results are identical to Case A.\n- $Acc_C = 0.99$.\n- $BalAcc_C = 0.5$.\n- $F1_C = 0$.\nValues: $0.990000, 0.500000, 0.000000$.\n\n**Case D: Calibrated, F1-optimal policy**\nParameters are the same as Case C: $w=0.05, p_h=0.20, p_\\ell=0$. We must choose the policy that maximizes $F1$.\n- **Policy 1 ($F1_1$)**: $F1_1=0$.\n- **Policy 2 ($F1_2$)**: $F1_2 = \\frac{2 p_h w}{p+w} = \\frac{2 \\cdot 0.20 \\cdot 0.05}{0.01+0.05} = \\frac{0.02}{0.06} = \\frac{1}{3} \\approx 0.333333$.\n- **Policy 3 ($F1_3$)**: $F1_3 = \\frac{2p}{p+1} = \\frac{2 \\cdot 0.01}{1.01} = \\frac{0.02}{1.01} \\approx 0.019802$.\nComparing the F1 scores, $F1_2$ is the maximum. Thus, the optimal policy is Policy 2. We now compute its metrics.\n- $Acc_D = p_h w + (1-p_\\ell)(1-w) = 0.20 \\cdot 0.05 + (1-0)(1-0.05) = 0.01 + 0.95 = 0.96$.\n- $TPR_D = \\frac{p_h w}{p} = \\frac{0.20 \\cdot 0.05}{0.01} = 1$.\n- $TNR_D = \\frac{(1-p_\\ell)(1-w)}{1-p} = \\frac{(1-0)(1-0.05)}{1-0.01} = \\frac{0.95}{0.99}$.\n- $BalAcc_D = \\frac{1}{2}(1 + \\frac{0.95}{0.99}) = \\frac{1.94}{1.98} \\approx 0.979798$.\n- $F1_D = 1/3 \\approx 0.333333$.\nValues: $0.960000, 0.979798, 0.333333$.\n\nCombining all results for the final output:\n- Acc_A, BalAcc_A, F1_A: $0.990000, 0.500000, 0.000000$\n- Acc_B, BalAcc_B, F1_B: $0.990000, 0.994949, 0.666667$\n- Acc_C, BalAcc_C, F1_C: $0.990000, 0.500000, 0.000000$\n- Acc_D, BalAcc_D, F1_D: $0.960000, 0.979798, 0.333333$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Accuracy, Balanced Accuracy, and F1-score for four\n    binary classification scenarios as defined in the problem.\n    \"\"\"\n\n    def calculate_metrics(p_rate, TP, FP, TN, FN):\n        \"\"\"\n        Calculates evaluation metrics from confusion matrix components.\n        \n        Args:\n            p_rate (float): The base rate P(y=1).\n            TP, FP, TN, FN (float): Population fractions for confusion matrix.\n\n        Returns:\n            tuple: A tuple containing (Accuracy, Balanced Accuracy, F1-score).\n        \"\"\"\n        # Denominators for rates\n        # real_pos = p_rate\n        # real_neg = 1 - p_rate\n        real_pos = TP + FN\n        real_neg = TN + FP\n        pred_pos = TP + FP\n        \n        # Accuracy\n        acc = TP + TN\n\n        # True Positive Rate (Recall) and True Negative Rate\n        tpr = TP / real_pos if real_pos > 0 else 0.0\n        tnr = TN / real_neg if real_neg > 0 else 0.0\n\n        # Balanced Accuracy\n        bal_acc = 0.5 * (tpr + tnr)\n\n        # Precision and Recall\n        prec = TP / pred_pos if pred_pos > 0 else 0.0\n        recall = tpr # Recall is the same as TPR\n\n        # F1 Score\n        f1 = 2 * (prec * recall) / (prec + recall) if (prec + recall) > 0 else 0.0\n\n        return acc, bal_acc, f1\n\n    results = []\n    p = 0.01\n\n    # Case A: Always-negative classifier (Policy 1)\n    TP_A, FP_A, TN_A, FN_A = 0.0, 0.0, 1 - p, p\n    acc_A, bal_acc_A, f1_A = calculate_metrics(p, TP_A, FP_A, TN_A, FN_A)\n    results.extend([acc_A, bal_acc_A, f1_A])\n\n    # Case B: Calibrated, threshold at high score\n    w_B, p_h_B = 0.02, 0.50\n    p_l_B = (p - w_B * p_h_B) / (1 - w_B)\n    # Threshold theta = 0.50, which is = p_h_B. This is Policy 2.\n    TP_B = p_h_B * w_B\n    FP_B = (1 - p_h_B) * w_B\n    TN_B = (1 - p_l_B) * (1 - w_B)\n    FN_B = p_l_B * (1 - w_B)\n    acc_B, bal_acc_B, f1_B = calculate_metrics(p, TP_B, FP_B, TN_B, FN_B)\n    results.extend([acc_B, bal_acc_B, f1_B])\n\n    # Case C: Calibrated, threshold too high\n    w_C, p_h_C = 0.05, 0.20\n    p_l_C = (p - w_C * p_h_C) / (1 - w_C)\n    # Threshold theta = 0.50, which is > p_h_C. This is Policy 1.\n    TP_C, FP_C, TN_C, FN_C = 0.0, 0.0, 1 - p, p\n    acc_C, bal_acc_C, f1_C = calculate_metrics(p, TP_C, FP_C, TN_C, FN_C)\n    results.extend([acc_C, bal_acc_C, f1_C])\n    \n    # Case D: Calibrated, F1-optimal among discrete policies\n    w_D, p_h_D = w_C, p_h_C\n    p_l_D = p_l_C\n\n    # Evaluate F1 for the three possible policies\n    \n    # Policy 1: Predict none positive\n    # F1 is 0, as calculated in Case A.\n    f1_1 = f1_A\n    metrics_1 = (acc_A, bal_acc_A, f1_1)\n    pred_pos_1 = 0.0\n\n    # Policy 2: Predict high-score bin as positive\n    TP_2 = p_h_D * w_D\n    FP_2 = (1 - p_h_D) * w_D\n    TN_2 = (1 - p_l_D) * (1 - w_D)\n    FN_2 = p_l_D * (1 - w_D)\n    metrics_2 = calculate_metrics(p, TP_2, FP_2, TN_2, FN_2)\n    f1_2 = metrics_2[2]\n    pred_pos_2 = w_D\n\n    # Policy 3: Predict all positive\n    TP_3, FP_3, TN_3, FN_3 = p, 1 - p, 0.0, 0.0\n    metrics_3 = calculate_metrics(p, TP_3, FP_3, TN_3, FN_3)\n    f1_3 = metrics_3[2]\n    pred_pos_3 = 1.0\n\n    # Find the optimal policy: maximize F1, with tie-breaking by fewest predicted positives\n    policies = [\n        (f1_1, pred_pos_1, metrics_1),\n        (f1_2, pred_pos_2, metrics_2),\n        (f1_3, pred_pos_3, metrics_3)\n    ]\n    \n    # Sort by F1 descending (-f1), then by predicted positives ascending (pred_pos)\n    # The best policy is the first element after sorting.\n    best_policy = sorted(policies, key=lambda x: (-x[0], x[1]))[0]\n    acc_D, bal_acc_D, f1_D = best_policy[2]\n    results.extend([acc_D, bal_acc_D, f1_D])\n\n    # Format the final output string with rounding to 6 decimal places\n    output_str = f\"[{','.join(f'{x:.6f}' for x in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Evaluating a classifier often requires understanding its performance across a range of decision thresholds, not just a single one. This practice  explores the two most common tools for this task: the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR) curve. By constructing these curves and their corresponding Area Under the Curve (AUC) in a scenario of extreme class imbalance, you will discover why ROC-AUC can be optimistically misleading and why PR-AUC is often the more informative metric when positive examples are rare.",
            "id": "3118905",
            "problem": "You are given the task of comparing two evaluation metrics for binary classification under extreme class imbalance using only foundational definitions. Consider a binary dataset of size $N$ with labels $y_i \\in \\{0,1\\}$ and predicted scores $s_i \\in \\mathbb{R}$ for $i \\in \\{1,\\dots,N\\}$. For any threshold $\\tau \\in \\mathbb{R}$, define the predicted label $\\hat{y}_i(\\tau) = \\mathbf{1}\\{s_i \\ge \\tau\\}$. Let $\\mathrm{TP}(\\tau)$, $\\mathrm{FP}(\\tau)$, $\\mathrm{TN}(\\tau)$, and $\\mathrm{FN}(\\tau)$ denote the threshold-dependent counts of true positives, false positives, true negatives, and false negatives, respectively. From these, define the following standard rates at threshold $\\tau$:\n- True Positive Rate (TPR): $\\mathrm{TPR}(\\tau) = \\dfrac{\\mathrm{TP}(\\tau)}{\\mathrm{TP}(\\tau) + \\mathrm{FN}(\\tau)}$.\n- False Positive Rate (FPR): $\\mathrm{FPR}(\\tau) = \\dfrac{\\mathrm{FP}(\\tau)}{\\mathrm{FP}(\\tau) + \\mathrm{TN}(\\tau)}$.\n- Precision: $\\mathrm{Prec}(\\tau) = \\dfrac{\\mathrm{TP}(\\tau)}{\\mathrm{TP}(\\tau) + \\mathrm{FP}(\\tau)}$.\n- Recall: $\\mathrm{Rec}(\\tau) = \\dfrac{\\mathrm{TP}(\\tau)}{\\mathrm{TP}(\\tau) + \\mathrm{FN}(\\tau)}$.\n\nThe Receiver Operating Characteristic (ROC) curve is the set of points $\\{(\\mathrm{FPR}(\\tau), \\mathrm{TPR}(\\tau))\\}$ induced by sweeping $\\tau$ from $+\\infty$ down to $-\\infty$. The Precision-Recall (PR) curve is the set of points $\\{(\\mathrm{Rec}(\\tau), \\mathrm{Prec}(\\tau))\\}$ produced by the same sweep. Construct both curves by sorting examples by score in descending order, sweeping once from highest to lowest score, and, in the presence of ties, aggregating all examples with equal score into a single step before computing the next point.\n\nDefine the Area Under the ROC Curve (ROC-AUC) as the integral of $\\mathrm{TPR}$ with respect to $\\mathrm{FPR}$ over $[0,1]$, computed via the trapezoidal rule on the sequence of $(\\mathrm{FPR}, \\mathrm{TPR})$ points obtained as described above, with the anchors $(0,0)$ and $(1,1)$ included. Define the area under the Precision-Recall curve (PR-AUC) as the Average Precision (AP), namely the Riemann–Stieltjes integral of $\\mathrm{Prec}$ with respect to $\\mathrm{Rec}$ over $[0,1]$ using the right-continuous step function approximation:\n$$\n\\mathrm{AP} \\;=\\; \\sum_{k=1}^{K} \\mathrm{Prec}_k \\,\\big(\\mathrm{Rec}_k - \\mathrm{Rec}_{k-1}\\big),\n$$\nwhere $(\\mathrm{Rec}_k,\\mathrm{Prec}_k)$ are the points after each distinct-score step in the descending sweep and $\\mathrm{Rec}_0 = 0$. No smoothing or interpolation beyond these definitions is allowed.\n\nYour program must evaluate these metrics for an extreme class imbalance regime with $p(y=1)=0.001$. To make this fully deterministic and testable, construct datasets via the following parameterization. For a given pair $(N,r)$:\n- Use $N = 1000$ so that $p(y=1) = 1/N = 0.001$.\n- Define scores $s_k = 1 - \\dfrac{k-1}{N-1}$ for $k \\in \\{1,\\dots,N\\}$ so that $s_1  s_2  \\dots  s_N$, ensuring a strictly descending list of scores with no ties.\n- Define labels $y_k$ as $y_r = 1$ and $y_k = 0$ for all $k \\neq r$, i.e., exactly one positive at rank $r$ in the descending score order.\n\nUsing only the above definitions, compute for each test case the following three quantities:\n- ROC-AUC as defined by trapezoidal integration on $(\\mathrm{FPR},\\mathrm{TPR})$.\n- PR-AUC using Average Precision as defined above.\n- The difference $\\mathrm{ROC\\mbox{-}AUC} - \\mathrm{PR\\mbox{-}AUC}$.\n\nTest suite:\n- Case A (general discriminative case): $(N,r) = (1000,10)$.\n- Case B (perfect ranking boundary): $(N,r) = (1000,1)$.\n- Case C (worst ranking edge): $(N,r) = (1000,1000)$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results, one per test case, where each result is a list of three real numbers $[\\mathrm{ROC\\mbox{-}AUC}, \\mathrm{PR\\mbox{-}AUC}, \\mathrm{ROC\\mbox{-}AUC} - \\mathrm{PR\\mbox{-}AUC}]$, each rounded to six decimal places. For example, the output should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$ with no additional text.",
            "solution": "The task is to compute the Receiver Operating Characteristic Area Under the Curve (ROC-AUC) and the Precision-Recall Area Under the Curve (PR-AUC) for a uniquely constructed dataset and compare them. The dataset has $N=1000$ examples, with exactly one positive instance ($P=1$) and $N-1=999$ negative instances ($N_{neg}=999$). The scores are strictly decreasing, and the single positive example is at rank $r$ in this ordering.\n\nWe will derive analytical expressions for ROC-AUC and PR-AUC based on the provided definitions. The procedure involves processing examples one by one in descending order of their scores, from rank $k=1$ to $k=N$. At each step $k$, we update the counts of true positives (TP) and false positives (FP).\n\nLet $\\mathrm{TP}_k$ and $\\mathrm{FP}_k$ be the counts after processing the $k$-th example. The total number of positive examples is $P = 1$ and negative examples is $N_{neg} = N-1$.\nThe rates at step $k$ are:\n- $\\mathrm{TPR}_k = \\mathrm{TP}_k / P = \\mathrm{TP}_k$\n- $\\mathrm{FPR}_k = \\mathrm{FP}_k / N_{neg} = \\mathrm{FP}_k / (N-1)$\n- $\\mathrm{Rec}_k = \\mathrm{TPR}_k = \\mathrm{TP}_k$\n- $\\mathrm{Prec}_k = \\mathrm{TP}_k / (\\mathrm{TP}_k + \\mathrm{FP}_k)$\n\nThe sequence of points for the curves is generated by varying the classification threshold, which is equivalent to processing examples in descending order of score. Let $(\\mathrm{FPR}_k, \\mathrm{TPR}_k)$ be the point on the ROC curve after processing the $k$-th ranked example. The initial state at $k=0$ (threshold at $+\\infty$) corresponds to $\\mathrm{TP}_0=0, \\mathrm{FP}_0=0$, giving the ROC point $(0,0)$.\n\n**Derivation of ROC-AUC**\nThe ROC-AUC is calculated using the trapezoidal rule on the sequence of points $(\\mathrm{FPR}_k, \\mathrm{TPR}_k)$ for $k=0, \\dots, N$. The area is $\\sum_{k=1}^{N} \\frac{1}{2} (\\mathrm{FPR}_k - \\mathrm{FPR}_{k-1}) (\\mathrm{TPR}_k + \\mathrm{TPR}_{k-1})$.\n\n1.  For steps $k=1, \\dots, r-1$:\n    - The examples are all negative. At each step $k$, $\\mathrm{TP}_k=0$ and $\\mathrm{FP}_k=k$.\n    - $\\mathrm{TPR}_k=0$, $\\mathrm{FPR}_k = k/(N-1)$.\n    - The points move from $(\\mathrm{FPR}_{k-1}, \\mathrm{TPR}_{k-1}) = ((k-1)/(N-1), 0)$ to $(\\mathrm{FPR}_k, \\mathrm{TPR}_k) = (k/(N-1), 0)$.\n    - The term in the sum is $\\frac{1}{2} (k/(N-1) - (k-1)/(N-1)) (0+0) = 0$. The area contribution is zero.\n\n2.  At step $k=r$:\n    - The $r$-th example is positive. $\\mathrm{TP}_r=1$ and $\\mathrm{FP}_r=r-1$ (the negatives from ranks $1$ to $r-1$).\n    - $\\mathrm{TPR}_r=1$, $\\mathrm{FPR}_r = (r-1)/(N-1)$.\n    - The previous point was $(\\mathrm{FPR}_{r-1}, \\mathrm{TPR}_{r-1}) = ((r-1)/(N-1), 0)$.\n    - The change in FPR is $\\mathrm{FPR}_r - \\mathrm{FPR}_{r-1} = (r-1)/(N-1) - (r-1)/(N-1) = 0$. The area contribution is zero. This step corresponds to a vertical line on the ROC plot.\n\n3.  For steps $k=r+1, \\dots, N$:\n    - The examples are negative. At step $k$, $\\mathrm{TP}_k=1$ and $\\mathrm{FP}_k = k-1$ (since the positive at rank $r$ and $k-r$ other negatives have been seen).\n    - $\\mathrm{TPR}_k=1$, $\\mathrm{FPR}_k = (k-1)/(N-1)$.\n    - The previous point was $(\\mathrm{FPR}_{k-1}, \\mathrm{TPR}_{k-1})$. For $k>r$, $\\mathrm{TPR}_{k-1}=1$ and $\\mathrm{FPR}_{k-1} = ((k-1)-1)/(N-1) = (k-2)/(N-1)$.\n    - The area of the trapezoid for step $k$ is:\n    $$ \\frac{1}{2} \\left( \\frac{k-1}{N-1} - \\frac{k-2}{N-1} \\right) (1+1) = \\frac{1}{2} \\left( \\frac{1}{N-1} \\right) (2) = \\frac{1}{N-1} $$\n    - This contribution of $1/(N-1)$ occurs for each step from $k=r+1$ to $k=N$. The number of such steps is $N-(r+1)+1 = N-r$.\n\nThe total ROC-AUC is the sum of these contributions:\n$$ \\mathrm{ROC\\mbox{-}AUC} = \\sum_{k=r+1}^{N} \\frac{1}{N-1} = (N-r) \\times \\frac{1}{N-1} = \\frac{N-r}{N-1} $$\n\n**Derivation of PR-AUC (Average Precision)**\nThe PR-AUC is defined as $\\mathrm{AP} = \\sum_{k=1}^{N} \\mathrm{Prec}_k (\\mathrm{Rec}_k - \\mathrm{Rec}_{k-1})$, with $\\mathrm{Rec}_0 = 0$. The term $(\\mathrm{Rec}_k - \\mathrm{Rec}_{k-1})$ represents the increase in recall at step $k$.\n\n1.  For steps $k=1, \\dots, r-1$:\n    - The examples are negative, so $\\mathrm{TP}_k=0$. This means $\\mathrm{Rec}_k = \\mathrm{TP}_k / P = 0$.\n    - For these steps, $\\mathrm{Rec}_k - \\mathrm{Rec}_{k-1} = 0 - 0 = 0$. The contribution to the sum is zero.\n\n2.  At step $k=r$:\n    - The positive example is encountered. $\\mathrm{TP}_r=1$, so $\\mathrm{Rec}_r=1$.\n    - The previous recall was $\\mathrm{Rec}_{r-1}=0$. The change is $\\mathrm{Rec}_r - \\mathrm{Rec}_{r-1} = 1-0=1$.\n    - The precision at this step is $\\mathrm{Prec}_r = \\frac{\\mathrm{TP}_r}{\\mathrm{TP}_r + \\mathrm{FP}_r} = \\frac{1}{1 + (r-1)} = \\frac{1}{r}$.\n    - The contribution to the sum is $\\mathrm{Prec}_r (\\mathrm{Rec}_r - \\mathrm{Rec}_{r-1}) = \\frac{1}{r} \\times 1 = \\frac{1}{r}$.\n\n3.  For steps $k=r+1, \\dots, N$:\n    - $\\mathrm{TP}_k$ remains $1$, so $\\mathrm{Rec}_k=1$.\n    - The change in recall is $\\mathrm{Rec}_k - \\mathrm{Rec}_{k-1} = 1 - 1 = 0$. The contribution is zero.\n\nThe sum for AP collapses to a single non-zero term at $k=r$.\n$$ \\mathrm{PR\\mbox{-}AUC} = \\mathrm{AP} = \\frac{1}{r} $$\n\n**Computation for Test Cases**\nWe apply these derived formulas to the three cases with $N=1000$.\n\n- **Case A: $(N,r) = (1000,10)$**\n    - $\\mathrm{ROC\\mbox{-}AUC} = \\frac{1000-10}{1000-1} = \\frac{990}{999} \\approx 0.990991$\n    - $\\mathrm{PR\\mbox{-}AUC} = \\frac{1}{10} = 0.1$\n    - $\\text{Difference} = \\frac{990}{999} - 0.1 \\approx 0.890991$\n\n- **Case B: $(N,r) = (1000,1)$** (Perfect ranking)\n    - $\\mathrm{ROC\\mbox{-}AUC} = \\frac{1000-1}{1000-1} = \\frac{999}{999} = 1.0$\n    - $\\mathrm{PR\\mbox{-}AUC} = \\frac{1}{1} = 1.0$\n    - $\\text{Difference} = 1.0 - 1.0 = 0.0$\n\n- **Case C: $(N,r) = (1000,1000)$** (Worst-case ranking)\n    - $\\mathrm{ROC\\mbox{-}AUC} = \\frac{1000-1000}{1000-1} = \\frac{0}{999} = 0.0$\n    - $\\mathrm{PR\\mbox{-}AUC} = \\frac{1}{1000} = 0.001$\n    - $\\text{Difference} = 0.0 - 0.001 = -0.001$\n\nThese calculations demonstrate the different sensitivities of ROC-AUC and PR-AUC. Under extreme imbalance, a high ROC-AUC can be achieved even if the precision is very low, as seen in Case A. PR-AUC, conversely, is more sensitive to the ranking of the few positive examples.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics_programmatically(N, r):\n    \"\"\"\n    Calculates ROC-AUC and PR-AUC by simulating the step-by-step process.\n    This serves as a verification of the analytically derived formulas.\n    \"\"\"\n    tp, fp = 0, 0\n    P, N_neg = 1, N - 1\n\n    roc_auc = 0.0\n    pr_auc = 0.0\n\n    tpr_prev, fpr_prev = 0.0, 0.0\n    rec_prev = 0.0\n    \n    # Loop through examples in descending order of score\n    for k in range(1, N + 1):\n        # Update TP/FP counts based on the label at rank k\n        if k == r:\n            tp += 1\n        else:\n            fp += 1\n\n        # Calculate current rates\n        tpr_curr = tp / P if P > 0 else 0.0\n        fpr_curr = fp / N_neg if N_neg > 0 else 0.0\n        rec_curr = tpr_curr\n        # Precision is defined only for tp+fp > 0\n        prec_curr = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n\n        # Update ROC-AUC using the trapezoidal rule\n        # Area of trapezoid = width * average height\n        roc_auc += (fpr_curr - fpr_prev) * (tpr_curr + tpr_prev) / 2.0\n\n        # Update PR-AUC (Average Precision)\n        # Add Prec * delta_Rec for steps where recall increases\n        delta_rec = rec_curr - rec_prev\n        if delta_rec > 0:\n            pr_auc += prec_curr * delta_rec\n\n        # Update \"previous\" state for the next iteration\n        tpr_prev, fpr_prev = tpr_curr, fpr_curr\n        rec_prev = rec_curr\n\n    return [roc_auc, pr_auc, roc_auc - pr_auc]\n\ndef calculate_metrics_analytically(N, r):\n    \"\"\"\n    Calculates ROC-AUC and PR-AUC using the derived closed-form formulas.\n    \"\"\"\n    # Derived formula for ROC-AUC\n    # This is equivalent to the probabilistic interpretation: the fraction of\n    # negative examples ranked lower than the positive example.\n    # There are N-1 negative examples. The positive is at rank r.\n    # It is ranked higher than (N-1) - (r-1) = N-r negative examples.\n    # So AUC = (N-r) / (N-1)\n    if N == 1:\n      # Edge case, though problem states N=1000\n      return [1.0, 1.0, 0.0] if r==1 else [0.0, 0.0, 0.0]\n      \n    roc_auc = (N - r) / (N - 1)\n    \n    # Derived formula for PR-AUC (Average Precision)\n    # The change in recall is 1 at rank r, and 0 elsewhere.\n    # Precision at rank r is 1/r.\n    pr_auc = 1 / r\n    \n    diff = roc_auc - pr_auc\n    \n    return [roc_auc, pr_auc, diff]\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        (1000, 10),    # Case A\n        (1000, 1),     # Case B\n        (1000, 1000),  # Case C\n    ]\n\n    results = []\n    for N, r in test_cases:\n        # The analytical solution is exact and derived from the problem's definitions.\n        # It is more efficient and robust than a programmatic simulation.\n        case_values = calculate_metrics_analytically(N, r)\n        \n        # Format each value to six decimal places and create the string representation\n        # of the list for this case, e.g., \"[0.990991,0.100000,0.890991]\"\n        formatted_case_values = [f\"{val:.6f}\" for val in case_values]\n        results.append(f\"[{','.join(formatted_case_values)}]\")\n\n    # Join the results for all cases into the final output string format:\n    # \"[[...],[...],[...]]\"\n    final_output = f\"[{','.join(results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While metrics like the $F_1$-score are useful, they implicitly assume that all errors are equally costly, which is rarely true in practice. This hands-on problem  bridges the gap between statistical performance and real-world utility by introducing asymmetric misclassification costs. You will learn that the \"best\" classifier is the one that minimizes tangible costs, a decision which may not align with maximizing a standard metric like the $F_1$-score.",
            "id": "3118850",
            "problem": "A binary classifier produces, for each input instance, a calibrated score $s \\in [0,1]$ interpreted as the conditional probability $s = \\mathbb{P}(Y=1 \\mid X)$ of the positive class. The decision rule classifies an instance as positive if $s \\geq t$ for some threshold $t$. Consider the following dataset of $12$ instances with pairs $(s_i, y_i)$, where $y_i \\in \\{0,1\\}$ is the true label ($1$ for positive, $0$ for negative):\n$(0.95, 1)$; $(0.85, 1)$; $(0.70, 0)$; $(0.65, 1)$; $(0.60, 0)$; $(0.55, 1)$; $(0.50, 0)$; $(0.40, 1)$; $(0.35, 0)$; $(0.20, 0)$; $(0.10, 0)$; $(0.05, 1)$.\n\nSuppose the misclassification costs are asymmetric with false negative cost $C_{\\mathrm{FN}} = 20$ and false positive cost $C_{\\mathrm{FP}} = 1$. Define the expected cost of a deterministic decision for an instance with calibrated score $s$ as follows: if it is classified positive, the expected cost is $C_{\\mathrm{FP}} \\cdot (1 - s)$; if it is classified negative, the expected cost is $C_{\\mathrm{FN}} \\cdot s$.\n\nYou are to:\n- Use risk minimization from first principles to determine the threshold $t_{\\mathrm{cost}}$ that minimizes expected misclassification cost, and compute the total expected cost on the dataset under this threshold by summing the expected costs over all instances.\n- Using only the foundational definitions of the confusion matrix counts (True Positive, False Positive, True Negative, False Negative), precision, recall, and the harmonic mean, determine the threshold $t_{F1}$ (chosen from the set of unique score values in the dataset) that maximizes the $F_1$ score on this dataset. Then, compute the total expected cost on the dataset under $t_{F1}$ by summing the expected costs over all instances.\n- Quantify the regret of optimizing $F_1$ instead of expected cost as the difference between the total expected cost under $t_{F1}$ and the total expected cost under $t_{\\mathrm{cost}}$.\n\nRound your final numerical answer for the regret to four significant figures. Express your final answer as a single real number with no units.",
            "solution": "### Part 1: Risk Minimization and Optimal Cost Threshold\nWe seek the threshold $t_{\\mathrm{cost}}$ that minimizes the expected misclassification cost. For a single instance with score $s = \\mathbb{P}(Y=1 \\mid X)$, the probability of it being negative is $\\mathbb{P}(Y=0 \\mid X) = 1 - s$.\n\nThe expected cost of classifying the instance as positive (action $a=1$) is:\n$$E[\\text{Cost} \\mid s, a=1] = C_{\\mathrm{FP}} \\cdot \\mathbb{P}(Y=0 \\mid X) = C_{\\mathrm{FP}}(1-s)$$\nThe expected cost of classifying the instance as negative (action $a=0$) is:\n$$E[\\text{Cost} \\mid s, a=0] = C_{\\mathrm{FN}} \\cdot \\mathbb{P}(Y=1 \\mid X) = C_{\\mathrm{FN}} s$$\n\nThe principle of risk minimization dictates that we should choose the action with the lower expected cost. We should classify as positive if:\n$$E[\\text{Cost} \\mid s, a=1] \\leq E[\\text{Cost} \\mid s, a=0]$$\n$$C_{\\mathrm{FP}}(1-s) \\leq C_{\\mathrm{FN}} s$$\nSubstituting the given cost values, $C_{\\mathrm{FP}} = 1$ and $C_{\\mathrm{FN}} = 20$:\n$$1(1-s) \\leq 20 s$$\n$$1 - s \\leq 20 s$$\n$$1 \\leq 21 s$$\n$$s \\geq \\frac{1}{21}$$\nThe decision rule is to classify as positive if the score $s$ is greater than or equal to the threshold $t_{\\mathrm{cost}} = \\frac{1}{21}$.\nNumerically, $t_{\\mathrm{cost}} \\approx 0.0476$.\nThe smallest score in the dataset is $s=0.05$. Since $0.05 = \\frac{1}{20}$ and $\\frac{1}{20}  \\frac{1}{21}$, every instance in the dataset has a score $s_i  t_{\\mathrm{cost}}$.\nTherefore, under this optimal threshold, every instance is classified as positive.\n\nThe total expected cost on the dataset, $C_{\\mathrm{total}}(t_{\\mathrm{cost}})$, is the sum of the expected costs for each instance, given the decision rule. Since all instances are classified as positive, the cost for each is $C_{\\mathrm{FP}}(1-s_i) = 1(1-s_i)$.\n$$C_{\\mathrm{total}}(t_{\\mathrm{cost}}) = \\sum_{i=1}^{12} (1-s_i) = 12 - \\sum_{i=1}^{12} s_i$$\nLet's sum the scores:\n$$\\sum s_i = 0.95+0.85+0.70+0.65+0.60+0.55+0.50+0.40+0.35+0.20+0.10+0.05 = 5.90$$\nThe total expected cost is:\n$$C_{\\mathrm{total}}(t_{\\mathrm{cost}}) = 12 - 5.90 = 6.10$$\n\n### Part 2: $F_1$ Score Maximization and its Associated Cost\nThe $F_1$ score is the harmonic mean of precision ($P$) and recall ($R$):\n$$F_1 = 2 \\cdot \\frac{P \\cdot R}{P+R}$$\nwhere $P = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$ and $R = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$. Here, $\\mathrm{TP}$, $\\mathrm{FP}$, and $\\mathrm{FN}$ are the counts of true positives, false positives, and false negatives, respectively. The total number of positive instances in the dataset is $P_{true} = 6$, and the total number of negative instances is $N_{true} = 6$.\n\nWe evaluate the $F_1$ score for each unique score value in the dataset as a potential threshold $t$. The decision rule is: predict positive if $s_i \\geq t$.\n\n- $t = 0.95$: $\\mathrm{TP}=1, \\mathrm{FP}=0$. $P=\\frac{1}{1}=1, R=\\frac{1}{6}$. $F_1 = \\frac{2 \\cdot 1 \\cdot \\frac{1}{6}}{1+\\frac{1}{6}} = \\frac{1/3}{7/6} = \\frac{2}{7}$.\n- $t = 0.85$: $\\mathrm{TP}=2, \\mathrm{FP}=0$. $P=\\frac{2}{2}=1, R=\\frac{2}{6}=\\frac{1}{3}$. $F_1 = \\frac{2 \\cdot 1 \\cdot \\frac{1}{3}}{1+\\frac{1}{3}} = \\frac{2/3}{4/3} = \\frac{1}{2}$.\n- $t = 0.70$: $\\mathrm{TP}=2, \\mathrm{FP}=1$. $P=\\frac{2}{3}, R=\\frac{2}{6}=\\frac{1}{3}$. $F_1 = \\frac{2 \\cdot \\frac{2}{3} \\cdot \\frac{1}{3}}{\\frac{2}{3}+\\frac{1}{3}} = \\frac{4/9}{1} = \\frac{4}{9}$.\n- $t = 0.65$: $\\mathrm{TP}=3, \\mathrm{FP}=1$. $P=\\frac{3}{4}, R=\\frac{3}{6}=\\frac{1}{2}$. $F_1 = \\frac{2 \\cdot \\frac{3}{4} \\cdot \\frac{1}{2}}{\\frac{3}{4}+\\frac{1}{2}} = \\frac{3/4}{5/4} = \\frac{3}{5}$.\n- $t = 0.55$: $\\mathrm{TP}=4, \\mathrm{FP}=2$. $P=\\frac{4}{6}=\\frac{2}{3}, R=\\frac{4}{6}=\\frac{2}{3}$. $F_1 = \\frac{2 \\cdot \\frac{2}{3} \\cdot \\frac{2}{3}}{\\frac{2}{3}+\\frac{2}{3}} = \\frac{8/9}{4/3} = \\frac{2}{3}$.\n- $t = 0.50$: $\\mathrm{TP}=4, \\mathrm{FP}=3$. $P=\\frac{4}{7}, R=\\frac{4}{6}=\\frac{2}{3}$. $F_1 = \\frac{2 \\cdot \\frac{4}{7} \\cdot \\frac{2}{3}}{\\frac{4}{7}+\\frac{2}{3}} = \\frac{16/21}{26/21} = \\frac{8}{13}$.\n- $t = 0.40$: $\\mathrm{TP}=5, \\mathrm{FP}=3$. $P=\\frac{5}{8}, R=\\frac{5}{6}$. $F_1 = \\frac{2 \\cdot \\frac{5}{8} \\cdot \\frac{5}{6}}{\\frac{5}{8}+\\frac{5}{6}} = \\frac{50/48}{35/24} = \\frac{25/24}{35/24} = \\frac{25}{35} = \\frac{5}{7}$.\n- $t = 0.35$: $\\mathrm{TP}=5, \\mathrm{FP}=4$. $P=\\frac{5}{9}, R=\\frac{5}{6}$. $F_1 = \\frac{2 \\cdot \\frac{5}{9} \\cdot \\frac{5}{6}}{\\frac{5}{9}+\\frac{5}{6}} = \\frac{50/54}{25/18} = \\frac{25/27}{25/18} = \\frac{18}{27} = \\frac{2}{3}$.\n- $t = 0.20$: $\\mathrm{TP}=5, \\mathrm{FP}=5$. $P=\\frac{5}{10}=\\frac{1}{2}, R=\\frac{5}{6}$. $F_1 = \\frac{2 \\cdot \\frac{1}{2} \\cdot \\frac{5}{6}}{\\frac{1}{2}+\\frac{5}{6}} = \\frac{5/6}{8/6} = \\frac{5}{8}$.\n- $t = 0.10$: $\\mathrm{TP}=5, \\mathrm{FP}=6$. $P=\\frac{5}{11}, R=\\frac{5}{6}$. $F_1 = \\frac{2 \\cdot \\frac{5}{11} \\cdot \\frac{5}{6}}{\\frac{5}{11}+\\frac{5}{6}} = \\frac{50/66}{85/66} = \\frac{50}{85} = \\frac{10}{17}$.\n- $t = 0.05$: $\\mathrm{TP}=6, \\mathrm{FP}=6$. $P=\\frac{6}{12}=\\frac{1}{2}, R=\\frac{6}{6}=1$. $F_1 = \\frac{2 \\cdot \\frac{1}{2} \\cdot 1}{\\frac{1}{2}+1} = \\frac{1}{3/2} = \\frac{2}{3}$.\n\nComparing the $F_1$ scores: $\\frac{2}{7} \\approx 0.286$, $\\frac{1}{2}=0.5$, $\\frac{4}{9}\\approx 0.444$, $\\frac{3}{5}=0.6$, $\\frac{2}{3}\\approx 0.667$, $\\frac{8}{13}\\approx 0.615$, $\\frac{5}{7}\\approx 0.714$, $\\frac{5}{8}=0.625$, $\\frac{10}{17}\\approx 0.588$.\nThe maximum $F_1$ score is $\\frac{5}{7}$, which occurs at the threshold $t_{F1} = 0.40$.\n\nNow, we calculate the total expected cost for $t_{F1} = 0.40$.\n- For instances with $s_i \\geq 0.40$, we classify as positive, and the cost is $1(1-s_i)$. These are the scores: $0.95, 0.85, 0.70, 0.65, 0.60, 0.55, 0.50, 0.40$.\n- For instances with $s_i  0.40$, we classify as negative, and the cost is $20 s_i$. These are the scores: $0.35, 0.20, 0.10, 0.05$.\n\nThe total expected cost $C_{\\mathrm{total}}(t_{F1})$ is:\n$$C_{\\mathrm{total}}(t_{F1}) = \\sum_{s_i \\geq 0.40} (1-s_i) + \\sum_{s_i  0.40} 20 s_i$$\nSum for the first group:\n$$ (1-0.95) + (1-0.85) + (1-0.70) + (1-0.65) + (1-0.60) + (1-0.55) + (1-0.50) + (1-0.40) $$\n$$ = 0.05 + 0.15 + 0.30 + 0.35 + 0.40 + 0.45 + 0.50 + 0.60 = 2.80 $$\nSum for the second group:\n$$ 20(0.35) + 20(0.20) + 20(0.10) + 20(0.05) $$\n$$ = 7 + 4 + 2 + 1 = 14.0 $$\nTotal expected cost:\n$$ C_{\\mathrm{total}}(t_{F1}) = 2.80 + 14.0 = 16.80 $$\n\n### Part 3: Regret Calculation\nThe regret of optimizing for $F_1$ score instead of expected cost is the difference between the total cost under $t_{F1}$ and the minimum possible total cost, which is achieved with $t_{\\mathrm{cost}}$.\n$$ \\text{Regret} = C_{\\mathrm{total}}(t_{F1}) - C_{\\mathrm{total}}(t_{\\mathrm{cost}}) $$\n$$ \\text{Regret} = 16.80 - 6.10 = 10.70 $$\nThe problem requires the answer to be rounded to four significant figures. The calculated value $10.70$ is already in this form.",
            "answer": "$$\n\\boxed{10.70}\n$$"
        }
    ]
}