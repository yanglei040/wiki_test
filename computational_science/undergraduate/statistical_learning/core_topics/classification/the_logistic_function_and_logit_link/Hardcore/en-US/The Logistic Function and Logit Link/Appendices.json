{
    "hands_on_practices": [
        {
            "introduction": "A key step in any modeling process is evaluating how well the model fits the observed data. For Generalized Linear Models like logistic regression, deviance serves as a fundamental measure of goodness-of-fit, analogous to the sum of squared errors in ordinary least squares. This practice exercise provides a concrete, step-by-step walkthrough of calculating deviance, starting from a fitted model and a small dataset. By working through this problem , you will solidify your understanding of how the linear predictor, the logistic function, and the log-likelihood come together to quantify a model's performance.",
            "id": "1930939",
            "problem": "In a study on the effectiveness of a new pesticide, the outcome for an insect is binary: survival (denoted by $y=0$) or non-survival (denoted by $y=1$). The probability of non-survival, $p$, is modeled using a Generalized Linear Model (GLM) with a logistic link function, an approach commonly known as logistic regression. This model relates the probability $p$ to the concentration $x$ of the pesticide (in parts per million, ppm) via the equation for the log-odds:\n$$ \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x $$\nBased on extensive preliminary experiments, the fitted model parameters are determined to be $\\beta_0 = -1.50$ and $\\beta_1 = 0.80$.\n\nA new experiment is conducted on a small group of four insects, yielding the following data:\n- Insect 1: Concentration $x_1 = 1.0$ ppm, Outcome $y_1 = 0$ (survived)\n- Insect 2: Concentration $x_2 = 2.0$ ppm, Outcome $y_2 = 1$ (did not survive)\n- Insect 3: Concentration $x_3 = 3.0$ ppm, Outcome $y_3 = 1$ (did not survive)\n- Insect 4: Concentration $x_4 = 4.0$ ppm, Outcome $y_4 = 0$ (survived)\n\nThe total deviance of a model is a measure of how well the model fits the data. For a set of binary outcomes $y_i$ ($i=1, \\dots, n$) and their corresponding model-fitted probabilities $\\hat{p}_i$, the total deviance is given by the formula:\n$$ D = -2 \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i)\\ln(1-\\hat{p}_i) \\right] $$\n\nCalculate the total deviance $D$ of the logistic regression model for this group of four insects. Round your final answer to four significant figures.",
            "solution": "The model uses the logistic link, so for each insect with concentration $x_{i}$, the linear predictor is $\\eta_{i}=\\beta_{0}+\\beta_{1}x_{i}$ and the fitted probability is $\\hat{p}_{i}=\\frac{1}{1+\\exp(-\\eta_{i})}$.\n\nGiven $\\beta_{0}=-1.50$ and $\\beta_{1}=0.80$, compute $\\eta_{i}$ and $\\hat{p}_{i}$:\n- Insect 1: $x_{1}=1.0$, $\\eta_{1}=-1.5+0.8(1)=-0.7$, so $\\hat{p}_{1}=\\frac{1}{1+\\exp(0.7)}\\approx 0.331812227$.\n- Insect 2: $x_{2}=2.0$, $\\eta_{2}=-1.5+0.8(2)=0.1$, so $\\hat{p}_{2}=\\frac{1}{1+\\exp(-0.1)}\\approx 0.524979188$.\n- Insect 3: $x_{3}=3.0$, $\\eta_{3}=-1.5+0.8(3)=0.9$, so $\\hat{p}_{3}=\\frac{1}{1+\\exp(-0.9)}\\approx 0.710949501$.\n- Insect 4: $x_{4}=4.0$, $\\eta_{4}=-1.5+0.8(4)=1.7$, so $\\hat{p}_{4}=\\frac{1}{1+\\exp(-1.7)}\\approx 0.845534734$.\n\nThe total deviance is\n$$\nD=-2\\sum_{i=1}^{4}\\left[y_{i}\\ln(\\hat{p}_{i})+(1-y_{i})\\ln(1-\\hat{p}_{i})\\right].\n$$\nCompute each term using the observed $y_{i}$:\n- Insect 1: $y_{1}=0$, contribution $-2\\ln(1-\\hat{p}_{1})=-2\\ln(0.668187773)\\approx 0.80637182$.\n- Insect 2: $y_{2}=1$, contribution $-2\\ln(\\hat{p}_{2})=-2\\ln(0.524979188)\\approx 1.28879255$.\n- Insect 3: $y_{3}=1$, contribution $-2\\ln(\\hat{p}_{3})=-2\\ln(0.710949501)\\approx 0.68230800$.\n- Insect 4: $y_{4}=0$, contribution $-2\\ln(1-\\hat{p}_{4})=-2\\ln(0.154465266)\\approx 3.73557206$.\n\nSum the contributions:\n$$\nD \\approx 0.80637182+1.28879255+0.68230800+3.73557206=6.51304443.\n$$\nRounded to four significant figures, the total deviance is $6.513$.",
            "answer": "$$\\boxed{6.513}$$"
        },
        {
            "introduction": "The coefficients in a logistic regression model hold the key to interpreting the relationship between predictors and outcomes, but their values are not absolute. They are inherently tied to the scale and units of the features themselves. This exercise  explores this crucial concept by asking you to determine how coefficients must adapt to a linear rescaling of a feature to keep the model's predictions unchanged. Working through this derivation will provide deep insight into why feature standardization is a common practice and how to properly interpret coefficients in the context of their measurement units.",
            "id": "3185465",
            "problem": "Consider a binary classification model using the logistic function with the logit link: for a feature vector $x = (x_{1}, \\dots, x_{d})$, the conditional probability of class $1$ is $p(x) = \\frac{1}{1 + \\exp(-\\eta(x))}$, where the linear predictor is $\\eta(x) = \\beta_{0} + \\sum_{k=1}^{d} \\beta_{k} x_{k}$. You decide to change the measurement units of a single feature $x_{j}$ by defining a rescaled feature $x_{j}^{\\text{new}} = \\frac{x_{j} - u}{v}$, where $u$ is a fixed real constant and $v$ is a fixed positive real constant. All other features $x_{k}$ for $k \\neq j$ are left unchanged.\n\nYou want the predicted probabilities $p(x)$ to remain exactly the same for every input after this rescaling, while expressing the model in terms of the transformed feature $x_{j}^{\\text{new}}$. That is, you seek new coefficients $\\tilde{\\beta}_{0}$, $\\tilde{\\beta}_{j}$, and $\\tilde{\\beta}_{k}$ for $k \\neq j$ such that the transformed linear predictor $\\tilde{\\eta}(x) = \\tilde{\\beta}_{0} + \\tilde{\\beta}_{j} x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\tilde{\\beta}_{k} x_{k}$ satisfies $p(x) = \\frac{1}{1 + \\exp(-\\tilde{\\eta}(x))}$ for all $x$. Express the pair $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$ in closed form in terms of $\\beta_{0}$, $\\beta_{j}$, $u$, and $v$. Provide your final answer as a row matrix using the LaTeX pmatrix environment with entries in the order $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$.",
            "solution": "The problem requires finding the new coefficients $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$ for a logistic regression model after a linear transformation of a single feature, $x_{j}$, such that the predicted probabilities remain unchanged for all input vectors $x$.\n\nThe conditional probability of class $1$ is given by the logistic function, $p(x) = \\sigma(\\eta(x))$, where $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ and $\\eta(x)$ is the linear predictor. The logistic function $\\sigma(z)$ is a strictly monotonically increasing function of its argument $z$. Therefore, for the predicted probabilities to remain identical for any given input vector $x$ after transforming the feature $x_{j}$, the value of the linear predictor must also remain unchanged. This means the original linear predictor, $\\eta(x)$, must be equal to the new linear predictor, $\\tilde{\\eta}(x)$, for all $x$.\n\nThe original linear predictor is given by:\n$$ \\eta(x) = \\beta_{0} + \\sum_{k=1}^{d} \\beta_{k} x_{k} = \\beta_{0} + \\beta_{j} x_{j} + \\sum_{k \\neq j, k=1}^{d} \\beta_{k} x_{k} $$\n\nThe new feature $x_{j}^{\\text{new}}$ is defined by the transformation:\n$$ x_{j}^{\\text{new}} = \\frac{x_{j} - u}{v} $$\nwhere $u$ is a real constant and $v$ is a positive real constant. To express the original predictor in terms of the new feature, we must first express $x_j$ as a function of $x_{j}^{\\text{new}}$. Rearranging the transformation equation gives:\n$$ v x_{j}^{\\text{new}} = x_{j} - u $$\n$$ x_{j} = v x_{j}^{\\text{new}} + u $$\n\nNow, we substitute this expression for $x_{j}$ back into the equation for the original linear predictor $\\eta(x)$:\n$$ \\eta(x) = \\beta_{0} + \\beta_{j} (v x_{j}^{\\text{new}} + u) + \\sum_{k \\neq j} \\beta_{k} x_{k} $$\n\nDistributing the term $\\beta_{j}$ and regrouping the terms to match the structure of the new linear predictor gives:\n$$ \\eta(x) = \\beta_{0} + \\beta_{j} v x_{j}^{\\text{new}} + \\beta_{j} u + \\sum_{k \\neq j} \\beta_{k} x_{k} $$\n$$ \\eta(x) = (\\beta_{0} + \\beta_{j} u) + (\\beta_{j} v) x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\beta_{k} x_{k} $$\n\nThe new linear predictor is defined in terms of the new coefficients $\\tilde{\\beta}_{0}$, $\\tilde{\\beta}_{j}$, and $\\tilde{\\beta}_{k}$ for $k \\neq j$:\n$$ \\tilde{\\eta}(x) = \\tilde{\\beta}_{0} + \\tilde{\\beta}_{j} x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\tilde{\\beta}_{k} x_{k} $$\n\nFor the condition $\\eta(x) = \\tilde{\\eta}(x)$ to hold for all possible values of the feature vector $x$ (and thus for all values of $x_{j}^{\\text{new}}$ and $x_k$ for $k \\neq j$), the coefficients of the corresponding terms in the two polynomial expressions for the linear predictors must be equal. We can equate the coefficients term by term:\n\nComparing the expression for $\\eta(x)$ in terms of $x_{j}^{\\text{new}}$ with the expression for $\\tilde{\\eta}(x)$:\n$$ (\\beta_{0} + \\beta_{j} u) + (\\beta_{j} v) x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\beta_{k} x_{k} = \\tilde{\\beta}_{0} + \\tilde{\\beta}_{j} x_{j}^{\\text{new}} + \\sum_{k \\neq j} \\tilde{\\beta}_{k} x_{k} $$\n\n1.  Equating the constant terms (the intercepts):\n    $$ \\tilde{\\beta}_{0} = \\beta_{0} + \\beta_{j} u $$\n\n2.  Equating the coefficients of the transformed feature $x_{j}^{\\text{new}}$:\n    $$ \\tilde{\\beta}_{j} = \\beta_{j} v $$\n\n3.  Equating the coefficients of the other features $x_{k}$ for $k \\neq j$:\n    $$ \\tilde{\\beta}_{k} = \\beta_{k} $$\n\nThe problem asks for the expressions for the pair $(\\tilde{\\beta}_{0}, \\tilde{\\beta}_{j})$. Based on the derivation above, these are:\n$$ \\tilde{\\beta}_{0} = \\beta_{0} + \\beta_{j} u $$\n$$ \\tilde{\\beta}_{j} = \\beta_{j} v $$\n\nThese expressions give the new intercept and the new coefficient for the rescaled feature, ensuring that the model's predictions are invariant under the specified linear transformation of the feature $x_j$. The final answer will be presented as a row matrix as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\beta_{0} + \\beta_{j} u  \\beta_{j} v\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A logistic regression model provides a probability, but in the real world, we often need to make a binary decision. The final step of turning a probabilistic score into a classification requires setting a decision threshold, a step that has significant practical consequences. This problem  moves beyond the default 0.5 threshold to a more sophisticated decision-theoretic framework, guiding you to derive the optimal threshold by explicitly considering the costs of different types of errors and the underlying prevalence of the condition in the population. This practice is essential for deploying classifiers in applications like medical diagnosis or fraud detection, where the consequences of false positives and false negatives are unequal.",
            "id": "3185480",
            "problem": "A binary classifier is trained to predict whether an individual has a condition ($Y=1$) or not ($Y=0$). The prevalence of the condition in the target population is $\\pi = \\mathbb{P}(Y=1)$ with $0  \\pi  1$. The classifier is a logistic regression model with linear predictor $\\eta(x)$ and output score $q(x)$ given by the logistic function $\\sigma$ and the logit link, namely $q(x) = \\sigma(\\eta(x))$ where $\\sigma(z) = \\left(1 + \\exp(-z)\\right)^{-1}$ and $\\operatorname{logit}(q) = \\ln\\!\\left(\\frac{q}{1-q}\\right) = \\eta(x)$. The model was trained on a balanced dataset so that its score $q(x)$ is calibrated to equal class priors, meaning $q(x) = \\frac{f_{1}(x)}{f_{0}(x) + f_{1}(x)}$, where $f_{y}(x)$ denotes the class-conditional density of features $x$ given $Y=y$.\n\nYou must choose a single probability threshold $t$ to apply to $q(x)$: predict $Y=1$ when $q(x) \\ge t$ and predict $Y=0$ otherwise. Misclassification incurs asymmetric costs: a False Negative (FN) incurs cost $C_{FN}  0$, and a False Positive (FP) incurs cost $C_{FP}  0$. Starting from core definitions of conditional risk minimization and Bayes decision theory, and treating $\\pi$, $C_{FN}$, and $C_{FP}$ as fixed and known, derive the closed-form expression for the cost-optimal threshold $t^{*}$ as a function of $\\pi$, $C_{FN}$, and $C_{FP}$ to be applied directly to $q(x)$.\n\nYour final answer must be a single analytical expression for $t^{*}$.",
            "solution": "The optimal decision threshold $t^*$ is derived by minimizing the expected cost of a decision, a principle known as conditional risk minimization. For any given feature vector $x$, we should predict $Y=1$ if the expected cost of doing so is less than or equal to the expected cost of predicting $Y=0$.\n\nThe conditional risk (expected cost) of predicting $Y=1$ is the cost of a False Positive ($C_{FP}$) multiplied by the probability of the true class being $Y=0$ given $x$:\n$$R(\\text{predict } 1 | x) = C_{FP} \\cdot \\mathbb{P}(Y=0|x)$$\n\nThe conditional risk of predicting $Y=0$ is the cost of a False Negative ($C_{FN}$) multiplied by the probability of the true class being $Y=1$ given $x$:\n$$R(\\text{predict } 0 | x) = C_{FN} \\cdot \\mathbb{P}(Y=1|x)$$\n\nThe optimal decision rule is to predict $Y=1$ if $R(\\text{predict } 1 | x) \\le R(\\text{predict } 0 | x)$. Let $p(x) = \\mathbb{P}(Y=1|x)$ be the true posterior probability for the target population. The rule is:\n$$C_{FP} \\cdot (1 - p(x)) \\le C_{FN} \\cdot p(x)$$\nRearranging this inequality gives a condition on the true posterior odds:\n$$\\frac{p(x)}{1-p(x)} \\ge \\frac{C_{FP}}{C_{FN}}$$\n\nNext, we relate the model score $q(x)$ to the true posterior $p(x)$. The problem states $q(x)$ is calibrated on a balanced dataset (priors of 0.5), so $q(x) = \\frac{f_1(x)}{f_0(x) + f_1(x)}$, where $f_y(x) = p(x|Y=y)$. From this, we find the likelihood ratio $\\frac{f_1(x)}{f_0(x)}$ in terms of $q(x)$:\n$$\\frac{f_1(x)}{f_0(x)} = \\frac{q(x)}{1-q(x)}$$\n\nThe true posterior probability $p(x)$ uses the true population prevalence $\\pi$. By Bayes' theorem, the true posterior odds are the likelihood ratio times the prior odds:\n$$\\frac{p(x)}{1-p(x)} = \\frac{f_1(x)}{f_0(x)} \\cdot \\frac{\\pi}{1-\\pi}$$\nSubstituting the expression for the likelihood ratio:\n$$\\frac{p(x)}{1-p(x)} = \\frac{q(x)}{1-q(x)} \\cdot \\frac{\\pi}{1-\\pi}$$\n\nNow we substitute this into our decision rule:\n$$\\frac{q(x)}{1-q(x)} \\cdot \\frac{\\pi}{1-\\pi} \\ge \\frac{C_{FP}}{C_{FN}}$$\nWe solve this for $q(x)$ to find the threshold $t^*$.\n$$\\frac{q(x)}{1-q(x)} \\ge \\frac{C_{FP}}{C_{FN}} \\cdot \\frac{1-\\pi}{\\pi}$$\nLet the right-hand side be $K = \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}$. The inequality becomes $\\frac{q(x)}{1-q(x)} \\ge K$. Since $q(x)$ is a probability, we can rearrange:\n$$q(x) \\ge K(1-q(x)) \\implies q(x)(1+K) \\ge K \\implies q(x) \\ge \\frac{K}{1+K}$$\n\nThe optimal threshold $t^*$ is the boundary of this inequality:\n$$t^* = \\frac{K}{1+K} = \\frac{\\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}}{1 + \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi}}$$\nMultiplying the numerator and denominator by $C_{FN}\\pi$ simplifies the expression:\n$$t^* = \\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi + C_{FP}(1-\\pi)}$$\nThis is the final expression for the cost-optimal threshold to be applied to the score $q(x)$.",
            "answer": "$$\\boxed{\\frac{C_{FP}(1-\\pi)}{C_{FN}\\pi + C_{FP}(1-\\pi)}}$$"
        }
    ]
}