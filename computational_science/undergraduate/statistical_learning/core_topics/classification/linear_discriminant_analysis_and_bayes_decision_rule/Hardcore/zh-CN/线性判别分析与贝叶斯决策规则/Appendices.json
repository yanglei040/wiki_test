{
    "hands_on_practices": [
        {
            "introduction": "理论的核心是理解其能力的边界。对于任何分类问题，我们能达到的最佳性能是什么？这个练习将指导你动手计算贝叶斯决策规则下的理论最小误差率，即贝叶斯误差。通过为一个经典的LDA场景推导这个理论极限 ，你将把抽象的理论概念与具体的公式联系起来，并理解它与我们用有限数据构建的“插件式”分类器性能的关系。",
            "id": "3139739",
            "problem": "考虑一个二元分类问题，其类别标签为 $Y \\in \\{0,1\\}$，类别先验概率相等，即 $\\pi_{0}=\\pi_{1}=\\tfrac{1}{2}$，特征向量为 $X \\in \\mathbb{R}^{2}$。假设类条件分布是具有共同协方差矩阵的多元高斯分布，\n$$\nX \\mid Y=k \\sim \\mathcal{N}(\\mu_{k},\\Sigma), \\quad k \\in \\{0,1\\},\n$$\n其中\n$$\n\\mu_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad \\mu_{1}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\quad \\Sigma=\\begin{pmatrix}1   0.5 \\\\ 0.5  2\\end{pmatrix}.\n$$\n从等错误分类代价的贝叶斯决策规则的定义出发，推导此设置下的贝叶斯分类器以及该贝叶斯分类器的精确错分概率。请使用标准正态累积分布函数 $\\Phi(\\cdot)$ 和给定参数，以精确的闭式形式表示您的最终答案。您的最终答案必须是单一的闭式解析表达式。\n\n此外，假设您通过对每类 $n$ 个带标签的样本进行最大似然估计来估计 $\\mu_{0}$、$\\mu_{1}$ 和 $\\Sigma$，然后使用得到的线性规则进行分类，从而构建一个插入式线性判别分析 (LDA) 分类器。请在您的解答中，从第一性原理出发，解释当 $n \\to \\infty$ 时，该插入式分类器的错分概率是否收敛，如果收敛，其收敛值是多少，并说明原因。\n\n无需进行数值近似或四舍五入；请提供贝叶斯错分概率的精确表达式。",
            "solution": "该问题要求推导一个具有高斯类条件密度的二元分类问题的贝叶斯分类器及其错分概率，并分析插入式线性判别分析 (LDA) 分类器的渐近行为。\n\n首先，我们推导贝叶斯分类器。贝叶斯决策规则旨在最小化错分概率。对于一个类别标签为 $Y \\in \\{0,1\\}$ 且错误分类代价相等的二元分类问题，该规则将特征向量 $x$ 分配给具有最高后验概率的类别。也就是说，如果 $P(Y=1|X=x)  P(Y=0|X=x)$，我们预测为类别 1，否则预测为类别 0。\n\n根据贝叶斯定理，后验概率为 $P(Y=k|X=x) = \\frac{p(x|Y=k)\\pi_k}{p(x)}$，其中 $p(x|Y=k) = f_k(x)$ 是类条件密度，$\\pi_k = P(Y=k)$ 是类先验概率。决策规则变为：如果 $f_1(x)\\pi_1  f_0(x)\\pi_0$，则预测为类别 1。\n\n问题陈述先验概率相等，$\\pi_0 = \\pi_1 = \\frac{1}{2}$。因此，规则简化为比较类条件密度：如果 $f_1(x)  f_0(x)$，则预测为类别 1。这等价于比较它们的对数，即 $\\ln f_1(x)  \\ln f_0(x)$，因为对数函数是严格递增函数。\n\n类条件密度被给定为多元高斯分布：$X | Y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma)$。类别 $k$ 的对数密度为：\n$$\n\\ln f_k(x) = -\\frac{p}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma| - \\frac{1}{2}(x-\\mu_k)^T \\Sigma^{-1} (x-\\mu_k)\n$$\n其中 $p=2$ 是 $x$ 的维度。决策规则 $\\ln f_1(x)  \\ln f_0(x)$ 变为：\n$$\n-\\frac{1}{2}(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1)  -\\frac{1}{2}(x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0)\n$$\n两边乘以 $-2$ 会反转不等号：\n$$\n(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1)  (x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0)\n$$\n展开二次型得到：\n$$\nx^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu_1 + \\mu_1^T\\Sigma^{-1}\\mu_1  x^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu_0 + \\mu_0^T\\Sigma^{-1}\\mu_0\n$$\n$x^T\\Sigma^{-1}x$ 项被消掉。重新排列剩余项以分离出 $x$，得到线性决策边界：\n$$\n2x^T\\Sigma^{-1}(\\mu_1 - \\mu_0)  \\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0\n$$\n贝叶斯分类器由该规则定义。如果不等式成立，它就预测为类别 1。这就是 LDA 的决策规则。左侧是 $x$ 的一个线性函数。\n\n接下来，我们推导该贝叶斯分类器的错分概率。我们定义一个线性得分 $S(x) = (\\mu_1-\\mu_0)^T \\Sigma^{-1} x$。决策规则可以写成：如果 $S(x)  \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0)$，则预测为类别 1。\n随机变量 $S(X)$ 是高斯随机向量 $X$ 的线性投影。因此，其分布也是高斯分布。我们对每个类别求其条件分布。\n\n如果 $X \\sim \\mathcal{N}(\\mu_k, \\Sigma)$，那么线性变换 $A X$ 的分布为 $\\mathcal{N}(A\\mu_k, A\\Sigma A^T)$。在这里，变换为 $A = (\\mu_1-\\mu_0)^T \\Sigma^{-1}$。\n给定 $Y=k$ 时 $S(X)$ 的条件均值为：\n$$\nm_k = E[S(X)|Y=k] = (\\mu_1-\\mu_0)^T \\Sigma^{-1} E[X|Y=k] = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_k\n$$\n$S(X)$ 的条件方差对两个类别是相同的，因为 $\\Sigma$ 是公共的：\n$$\n\\sigma_S^2 = \\text{Var}(S(X)|Y=k) = ((\\mu_1-\\mu_0)^T \\Sigma^{-1}) \\Sigma ((\\mu_1-\\mu_0)^T \\Sigma^{-1})^T = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0)\n$$\n这个量是类别均值之间 Mahalanobis 距离的平方，记为 $\\Delta^2$。所以，$\\sigma_S^2 = \\Delta^2$。\n得分的均值为 $m_0 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_0$ 和 $m_1 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_1$。注意 $m_1 - m_0 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0) = \\Delta^2$。\n\n$S(X)$ 的决策阈值为 $T = \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0) = \\frac{1}{2}(m_1+m_0)$。这个阈值恰好位于两个条件得分均值的中点。\n\n总错分概率，即贝叶斯错误率，为：\n$P(\\text{error}) = \\pi_0 P(\\text{predict } 1 | Y=0) + \\pi_1 P(\\text{predict } 0 | Y=1)$。\n当 $\\pi_0=\\pi_1=\\frac{1}{2}$ 时：\n$P(\\text{error}) = \\frac{1}{2} P(S(X)  T | Y=0) + \\frac{1}{2} P(S(X) \\le T | Y=1)$。\n给定 $Y=0$， $S(X) \\sim \\mathcal{N}(m_0, \\Delta^2)$。 $P(S(X)  T | Y=0) = P\\left(\\frac{S(X)-m_0}{\\Delta}  \\frac{T-m_0}{\\Delta}\\right) = P\\left(Z  \\frac{(m_0+m_1)/2 - m_0}{\\Delta}\\right) = P\\left(Z  \\frac{m_1-m_0}{2\\Delta}\\right) = P\\left(Z  \\frac{\\Delta^2}{2\\Delta}\\right) = P(Z  \\frac{\\Delta}{2}) = 1 - \\Phi(\\frac{\\Delta}{2}) = \\Phi(-\\frac{\\Delta}{2})$，其中 $Z \\sim \\mathcal{N}(0,1)$。\n\n给定 $Y=1$， $S(X) \\sim \\mathcal{N}(m_1, \\Delta^2)$。 $P(S(X) \\le T | Y=1) = P\\left(\\frac{S(X)-m_1}{\\Delta} \\le \\frac{T-m_1}{\\Delta}\\right) = P\\left(Z \\le \\frac{(m_0+m_1)/2 - m_1}{\\Delta}\\right) = P\\left(Z \\le \\frac{m_0-m_1}{2\\Delta}\\right) = P\\left(Z \\le -\\frac{\\Delta^2}{2\\Delta}\\right) = \\Phi(-\\frac{\\Delta}{2})$。\n\n两个条件错误概率相等。总错分概率为：\n$P(\\text{error}) = \\frac{1}{2}\\Phi(-\\frac{\\Delta}{2}) + \\frac{1}{2}\\Phi(-\\frac{\\Delta}{2}) = \\Phi(-\\frac{\\Delta}{2})$。\n\n现在，我们用给定的参数计算 $\\Delta^2$：\n$$\n\\mu_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad \\mu_{1}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\quad \\Sigma=\\begin{pmatrix}1   0.5 \\\\ 0.5  2\\end{pmatrix}.\n$$\n$\\Sigma$ 的行列式是 $\\det(\\Sigma) = (1)(2) - (0.5)(0.5) = 2 - 0.25 = 1.75 = \\frac{7}{4}$。\n$\\Sigma$ 的逆矩阵是：\n$$\n\\Sigma^{-1} = \\frac{1}{7/4} \\begin{pmatrix} 2   -0.5 \\\\ -0.5  1 \\end{pmatrix} = \\frac{4}{7} \\begin{pmatrix} 2   -0.5 \\\\ -0.5  1 \\end{pmatrix} = \\begin{pmatrix} 8/7   -2/7 \\\\ -2/7  4/7 \\end{pmatrix}.\n$$\n均值之差为 $\\mu_1 - \\mu_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\nMahalanobis 距离的平方为：\n$$\n\\Delta^2 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0) = \\begin{pmatrix} 1   2 \\end{pmatrix} \\begin{pmatrix} 8/7   -2/7 \\\\ -2/7  4/7 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\n\\Delta^2 = \\begin{pmatrix} 1   2 \\end{pmatrix} \\begin{pmatrix} (8/7)(1) + (-2/7)(2) \\\\ (-2/7)(1) + (4/7)(2) \\end{pmatrix} = \\begin{pmatrix} 1   2 \\end{pmatrix} \\begin{pmatrix} 4/7 \\\\ 6/7 \\end{pmatrix} = (1)(\\frac{4}{7}) + (2)(\\frac{6}{7}) = \\frac{4+12}{7} = \\frac{16}{7}.\n$$\nMahalanobis 距离为 $\\Delta = \\sqrt{\\frac{16}{7}} = \\frac{4}{\\sqrt{7}}$。\n因此，贝叶斯错分概率为：\n$$\nP(\\text{error}) = \\Phi\\left(-\\frac{\\Delta}{2}\\right) = \\Phi\\left(-\\frac{1}{2} \\cdot \\frac{4}{\\sqrt{7}}\\right) = \\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right).\n$$\n\n最后，关于插入式 LDA 分类器：该分类器是通过首先从每类 $n$ 个样本的训练集中估计未知参数 $\\mu_0$、$\\mu_1$ 和 $\\Sigma$，然后将这些估计值“插入”到上面推导的 LDA 决策规则中来构建的。这些参数的标准估计量是样本均值和合并样本协方差矩阵。\n$$\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i: y_i=k} x_i, \\quad \\hat{\\Sigma} = \\frac{1}{2n-2} \\sum_{k=0}^{1} \\sum_{i: y_i=k} (x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T\n$$\n根据大数定律，这些估计量是一致的。当样本数量 $n$ 趋于无穷大时，估计的参数依概率收敛于它们的真实值：\n$$\n\\hat{\\mu}_k \\xrightarrow{p} \\mu_k \\quad \\text{和} \\quad \\hat{\\Sigma} \\xrightarrow{p} \\Sigma \\quad \\text{当 } n \\to \\infty \\text{ 时}.\n$$\nLDA 决策边界是这些参数的连续函数。根据连续映射定理，插入式分类器的决策函数 $\\hat{\\delta}(x)$ 依概率收敛于贝叶斯最优决策函数 $\\delta(x)$。\n$$\n\\hat{\\delta}(x) = x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_1 - \\hat{\\mu}_0) - \\frac{1}{2}(\\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 - \\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0) \\xrightarrow{p} \\delta(x) \\quad \\text{当 } n \\to \\infty \\text{ 时}\n$$\n因此，插入式 LDA 分类器的错分概率（这是对真实数据分布的期望）收敛于最优贝叶斯分类器的错分概率。这个最小可能错误率就是贝叶斯错误率。\n因此，当 $n \\to \\infty$ 时，插入式 LDA 分类器的错分概率收敛到我们计算出的精确贝叶斯错分概率，即 $\\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right)$。",
            "answer": "$$\\boxed{\\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right)}$$"
        },
        {
            "introduction": "理论在完美世界中成立，但真实数据往往充满噪声。这个练习将带你从理想模型走向混乱的现实，探讨数据异常值如何破坏LDA分类器的性能。你将通过一个具体的编码任务 ，亲手构建标准LDA和其稳健版本，直观地看到单个异常点如何误导模型，以及稳健的估计方法如何成为重要的防线。",
            "id": "3139742",
            "problem": "考虑一个二维二元分类问题，其类别由 $0$ 和 $1$ 索引。对于零一损失函数，贝叶斯决策规则选择能使后验概率 $P(Y=y \\mid \\mathbf{x})$ 最大化的类别标签 $y \\in \\{0,1\\}$。假设类别条件密度是具有相同协方差矩阵和相等类别先验的多变量高斯分布。在此基础上，推导比较两个判别函数的线性决策规则，并为标准估计器和鲁棒估计器实现相应的线性判别分析 (LDA) 分类器。\n\n您的程序必须仅使用此处提供的信息完成以下任务。\n\n1) 建模假设和估计器：\n- 假设对于每个类别 $k \\in \\{0,1\\}$，训练数据 $\\mathbf{X}_k \\in \\mathbb{R}^{n_k \\times 2}$ 独立地从一个高斯分布中抽取，该分布的类别均值为 $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^2$，共享协方差矩阵为 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$，并且类别先验相等，即 $P(Y=0) = P(Y=1) = 1/2$。\n- 标准 LDA 估计器必须对每个类别 $k$ 使用样本均值 $\\hat{\\boldsymbol{\\mu}}_k$，以及通过对由 $(n_k - 1)$ 加权的类内无偏样本协方差进行平均得到的合并协方差估计器 $\\hat{\\boldsymbol{\\Sigma}}$。\n- 鲁棒 LDA 估计器必须使用 $\\alpha$-截尾策略以实现鲁棒性：对于每个类别 $k$，计算坐标中位数 $\\tilde{\\boldsymbol{m}}_k$，计算每个类别样本到 $\\tilde{\\boldsymbol{m}}_k$ 的欧几里得距离，舍去最远的 $\\lceil \\alpha n_k \\rceil$ 个点，然后从截尾后的集合中计算截尾类别均值 $\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_k$ 和合并协方差 $\\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})}$。使用 $\\alpha = 0.2$。如果由于有限样本效应导致协方差矩阵不可逆，则添加一个小的正岭项 $\\lambda \\mathbf{I}$，其中 $\\lambda = 10^{-6}$。\n\n2) 训练数据集：\n- 干净训练集（无离群值）：\n  - 类别 $0$：$\\mathbf{X}_0^{(\\mathrm{clean})} = \\{(0,0), (0.2,0.1), (-0.1,-0.2), (0.1,0)\\}$，因此 $n_0 = 4$。\n  - 类别 $1$：$\\mathbf{X}_1^{(\\mathrm{clean})} = \\{(2,0), (2.1,0.2), (1.9,-0.1), (2.2,0.05)\\}$，因此 $n_1 = 4$。\n- 类别 $0$ 的含离群值污染的数据集：\n  - 含一个离群值的类别 $0$：$\\mathbf{X}_0^{(\\mathrm{out})} = \\mathbf{X}_0^{(\\mathrm{clean})} \\cup \\{(10,0)\\}$，因此 $n_0 = 5$。\n  - 类别 $1$ 保持为 $\\mathbf{X}_1^{(\\mathrm{clean})}$。\n\n3) 分类器和预测规则：\n- 使用从指定训练集估计的 $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$ 实现标准 LDA 分类器。\n- 使用按规定通过 $\\alpha=0.2$ 的 $\\alpha$-截尾法估计的 $(\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_0, \\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_1, \\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})})$ 实现鲁棒 LDA 分类器。\n- 使用从高斯共享协方差模型和相等先验推导出的线性判别规则，为任何输入 $\\mathbf{x} \\in \\mathbb{R}^2$ 生成 $\\{0,1\\}$ 中的预测标签。\n\n4) 测试套件和必需输出：\n- 定义查询点 $\\mathbf{q} = (1.1, 0.0)$。\n- 定义查询集 $\\mathcal{S} = \\{(0.96,0.0), (1.03,0.0), (1.10,0.0), (1.17,0.0), (1.24,0.0)\\}$。\n- 训练三个分类器：\n  - 在干净数据 $(\\mathbf{X}_0^{(\\mathrm{clean})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n  - 在含离群值的数据 $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n  - 在含离群值的数据上训练的鲁棒 LDA，在估计均值和合并协方差之前，对每个类别分别应用 $\\alpha = 0.2$ 截尾处理。\n- 计算以下六个布尔值输出：\n  - $b_1$：在干净数据上训练的标准 LDA 为 $\\mathbf{q}$ 预测的标签等于 $1$。\n  - $b_2$：在含离群值的数据上训练的标准 LDA 为 $\\mathbf{q}$ 预测的标签等于 $0$。\n  - $b_3$：在含离群值的数据上训练的鲁棒 LDA 为 $\\mathbf{q}$ 预测的标签等于在干净数据上训练的标准 LDA 的预测标签。\n  - $b_4$：在含离群值的数据上训练的标准 LDA 和鲁棒 LDA 为 $\\mathbf{q}$ 预测的标签不同。\n  - $b_5$：在 $\\mathcal{S}$ 中至少存在一个点，其在干净数据上训练的标准 LDA 下的预测标签与在含离群值数据上训练的标准 LDA 下的预测标签不同。\n  - $b_6$：对于 $\\mathcal{S}$ 中的每个点，其在干净数据上训练的标准 LDA 下的预测标签等于在含离群值数据上训练的鲁棒 LDA 下的预测标签。\n\n5) 最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的六个布尔值列表，顺序为 $[b_1, b_2, b_3, b_4, b_5, b_6]$。例如，一个语法上有效的示例是 $[True,False,True,True,False,True]$。\n\n不涉及物理单位。不使用角度。所有计算均按规定在 $\\mathbb{R}^2$ 中纯数值进行。程序必须是自包含的，并且不得读取任何输入。",
            "solution": "该问题被评估为有效。这是一个在统计学习领域内定义明确、有科学依据且客观的问题，特别关注线性判别分析 (LDA) 以及离群值对参数估计的影响。它要求实现标准估计器和鲁棒估计器，并将它们应用于指定的数据集。\n\n### 原理与推导\n\n此问题的核心在于用于分类的贝叶斯决策理论。对于类别为 $k \\in \\{0, 1\\}$ 的二元分类任务和零一损失函数，贝叶斯决策规则将特征向量 $\\mathbf{x}$ 分配给具有最高后验概率的类别。\n$$ \\hat{y} = \\arg\\max_{k \\in \\{0,1\\}} P(Y=k \\mid \\mathbf{x}) $$\n使用贝叶斯定理，$P(Y=k \\mid \\mathbf{x}) \\propto P(\\mathbf{x} \\mid Y=k) P(Y=k) = f_k(\\mathbf{x}) \\pi_k$，其中 $f_k(\\mathbf{x})$ 是类别条件概率密度，$\\pi_k$ 是类别先验。\n\n问题陈述，类别条件密度是具有特定类别均值 $\\boldsymbol{\\mu}_k$ 和一个共同协方差矩阵 $\\boldsymbol{\\Sigma}$ 的多变量高斯分布：\n$$ f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\\right) $$\n其中 $d=2$。给定的先验是相等的，即 $\\pi_0 = \\pi_1 = 1/2$。\n\n决策规则等同于最大化后验概率的对数，由此我们推导出判别函数 $\\delta_k(\\mathbf{x})$：\n$$ \\delta_k(\\mathbf{x}) = \\ln(f_k(\\mathbf{x}) \\pi_k) = \\ln f_k(\\mathbf{x}) + \\ln \\pi_k $$\n代入高斯密度并舍去对于所有类别 $k$ 均为常数的项（即 $-\\frac{d}{2}\\ln(2\\pi)$、$-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}|$ 以及由于先验相等而为常数的 $\\ln \\pi_k$），最大化问题简化为：\n$$ \\hat{y} = \\arg\\max_{k \\in \\{0,1\\}} \\left[ -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right] $$\n这等同于最小化从 $\\mathbf{x}$ 到类别均值 $\\boldsymbol{\\mu}_k$ 的马氏距离（Mahalanobis distance）的平方：\n$$ \\hat{y} = \\arg\\min_{k \\in \\{0,1\\}} \\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right] $$\n决策边界是到两个类别均值的马氏距离相等的点的集合。如果输入 $\\mathbf{x}$ 到 $\\boldsymbol{\\mu}_1$ 的马氏距离小于到 $\\boldsymbol{\\mu}_0$ 的马氏距离，则它被分类为类别 1，否则为类别 0。\n\n### 参数估计\n\n在实践中，真实参数 $\\boldsymbol{\\mu}_k$ 和 $\\boldsymbol{\\Sigma}$ 是未知的，必须从训练数据中估计。设类别 $k$ 的训练数据为 $\\mathbf{X}_k \\in \\mathbb{R}^{n_k \\times d}$。\n\n1.  **标准估计器**：\n    -   类别均值 $\\boldsymbol{\\mu}_k$ 通过样本均值估计：$\\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\mathbf{x}_{i}$。\n    -   共享协方差矩阵 $\\boldsymbol{\\Sigma}$ 通过合并协方差矩阵 $\\hat{\\boldsymbol{\\Sigma}}$ 估计，该矩阵是类内散布矩阵的平均值：\n        $$ \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n_0 + n_1 - 2} \\sum_{k=0}^{1} \\sum_{i=1}^{n_k} (\\mathbf{x}_{i,k} - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_{i,k} - \\hat{\\boldsymbol{\\mu}}_k)^T $$\n        这对应于指定的过程，即对由 $(n_k-1)$ 加权的无偏样本协方差进行平均。\n\n2.  **鲁棒估计器**：\n    为了减轻离群值的影响，使用 $\\alpha=0.2$ 的 $\\alpha$-截尾估计策略。\n    -   对于每个类别 $k$，计算其坐标中位数 $\\tilde{\\boldsymbol{m}}_k$。\n    -   计算类中每个样本 $\\mathbf{x}_i$ 的欧几里得距离 $||\\mathbf{x}_i - \\tilde{\\boldsymbol{m}}_k||_2$。\n    -   将具有最大距离的 $\\lceil \\alpha n_k \\rceil$ 个样本识别为离群值并从训练集中移除。\n    -   然后使用这些截尾后的数据集计算均值 $\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_k$ 和合并协方差 $\\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})}$ 的标准估计器。\n\n### 实现与逻辑\n\n解决方案涉及按规定训练三个不同的 LDA 分类器：\n1.  **分类器 1 (C1)**：在干净数据集 $(\\mathbf{X}_0^{(\\mathrm{clean})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n2.  **分类器 2 (C2)**：在含离群值的数据集 $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n3.  **分类器 3 (C3)**：在含离群值的数据集 $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的鲁棒 LDA（使用 $\\alpha=0.2$ 截尾）。\n\n对于每个分类器，计算估计参数 $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$。在预测之前，检查 $\\hat{\\boldsymbol{\\Sigma}}$ 的可逆性。如果它是奇异的（行列式接近于零），则通过加上 $\\lambda\\mathbf{I}$ 进行正则化，其中 $\\lambda = 10^{-6}$。\n\n使用推导出的马氏距离规则为三个分类器中的每一个对查询点 $\\mathbf{q}$ 和查询集 $\\mathcal{S}$ 进行预测。最后，基于这些预测评估六个布尔条件（$b_1$ 到 $b_6$）。\n-   $b_1$: `$C1(\\mathbf{q}) = 1$`\n-   $b_2$: `$C2(\\mathbf{q}) = 0$`\n-   $b_3$: `$C3(\\mathbf{q}) = C1(\\mathbf{q})$`\n-   $b_4$: `$C2(\\mathbf{q}) \\neq C3(\\mathbf{q})$`\n-   $b_5$: $\\exists \\mathbf{p} \\in \\mathcal{S} \\text{ s.t. } C1(\\mathbf{p}) \\neq C2(\\mathbf{p})$\n-   $b_6$: $\\forall \\mathbf{p} \\in \\mathcal{S}, C1(\\mathbf{p}) = C3(\\mathbf{p})$\n\n最终输出是这些布尔值的列表。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates standard and robust LDA classifiers based on the problem specification.\n    \"\"\"\n\n    # 1. Data Definitions\n    X0_clean = np.array([[0.0, 0.0], [0.2, 0.1], [-0.1, -0.2], [0.1, 0.0]])\n    X1_clean = np.array([[2.0, 0.0], [2.1, 0.2], [1.9, -0.1], [2.2, 0.05]])\n    X0_out = np.vstack([X0_clean, [10.0, 0.0]])\n\n    q = np.array([1.1, 0.0])\n    S_set = np.array([\n        [0.96, 0.0], [1.03, 0.0], [1.10, 0.0], [1.17, 0.0], [1.24, 0.0]\n    ])\n    \n    alpha = 0.2\n    regularization_lambda = 1e-6\n\n    # 2. Estimator and Classifier Implementation\n    def train_standard_lda(X0, X1):\n        \"\"\"\n        Estimates means and pooled covariance for standard LDA.\n        \"\"\"\n        n0, d = X0.shape\n        n1, _ = X1.shape\n        \n        mu0 = np.mean(X0, axis=0)\n        mu1 = np.mean(X1, axis=0)\n\n        # Sum of squares (scatter matrices)\n        S0 = (X0 - mu0).T @ (X0 - mu0)\n        S1 = (X1 - mu1).T @ (X1 - mu1)\n        \n        # Pooled covariance matrix\n        Sigma_pooled = (S0 + S1) / (n0 + n1 - 2)\n        \n        return mu0, mu1, Sigma_pooled\n\n    def train_robust_lda(X0, X1, alpha_val):\n        \"\"\"\n        Implements the alpha-trimmed robust estimation procedure.\n        \"\"\"\n        trimmed_datasets = []\n        for X_class in [X0, X1]:\n            n_class, _ = X_class.shape\n            m_class = np.median(X_class, axis=0)\n            dists = np.linalg.norm(X_class - m_class, axis=1)\n            n_trim = math.ceil(alpha_val * n_class)\n            \n            if n_trim  0 and n_trim  n_class:\n                trim_indices = np.argsort(dists)[-n_trim:]\n                X_trimmed = np.delete(X_class, trim_indices, axis=0)\n                trimmed_datasets.append(X_trimmed)\n            else:\n                trimmed_datasets.append(X_class)\n        \n        X0_trimmed, X1_trimmed = trimmed_datasets\n        \n        return train_standard_lda(X0_trimmed, X1_trimmed)\n\n    def classify(x, params):\n        \"\"\"\n        Predicts class label for a point x using estimated LDA parameters.\n        \"\"\"\n        mu0, mu1, Sigma = params\n        \n        # Regularize if singular\n        if np.isclose(np.linalg.det(Sigma), 0):\n            Sigma = Sigma + regularization_lambda * np.identity(Sigma.shape[0])\n            \n        Sigma_inv = np.linalg.inv(Sigma)\n        \n        # Compare Mahalanobis distances\n        dist0 = (x - mu0).T @ Sigma_inv @ (x - mu0)\n        dist1 = (x - mu1).T @ Sigma_inv @ (x - mu1)\n        \n        return 0 if dist0 = dist1 else 1\n\n    # 3. Training the Three Classifiers\n    # C1: Standard LDA on clean data\n    params_c1 = train_standard_lda(X0_clean, X1_clean)\n    \n    # C2: Standard LDA on outlier-contaminated data\n    params_c2 = train_standard_lda(X0_out, X1_clean)\n    \n    # C3: Robust LDA on outlier-contaminated data\n    params_c3 = train_robust_lda(X0_out, X1_clean, alpha)\n\n    # 4. Evaluating the Six Boolean Conditions\n    # Predictions for the query point q\n    pred_q_c1 = classify(q, params_c1)\n    pred_q_c2 = classify(q, params_c2)\n    pred_q_c3 = classify(q, params_c3)\n\n    # b1: label(q) by C1 is 1\n    b1 = (pred_q_c1 == 1)\n    \n    # b2: label(q) by C2 is 0\n    b2 = (pred_q_c2 == 0)\n\n    # b3: label(q) by C3 equals label by C1\n    b3 = (pred_q_c3 == pred_q_c1)\n    \n    # b4: labels for q by C2 and C3 are different\n    b4 = (pred_q_c2 != pred_q_c3)\n\n    # Predictions for the query set S\n    preds_S_c1 = np.array([classify(p, params_c1) for p in S_set])\n    preds_S_c2 = np.array([classify(p, params_c2) for p in S_set])\n    preds_S_c3 = np.array([classify(p, params_c3) for p in S_set])\n\n    # b5: Exists p in S with different labels from C1 and C2\n    b5 = np.any(preds_S_c1 != preds_S_c2)\n    \n    # b6: For every p in S, label from C1 equals label from C3\n    b6 = np.all(preds_S_c1 == preds_S_c3)\n\n    # 5. Final Output\n    results = [b1, b2, b3, b4, b5, b6]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "构建模型后，我们如何确信它在未来也能表现良好？这个练习聚焦于一个关键的实践问题：过拟合。你将实现一个原则性的检验方法 ，通过比较模型在训练数据和验证数据上的费雪比（Fisher ratio）来检测模型是否过度拟合了训练数据中的噪声，从而将统计概念与分类误差的“乐观度”联系起来。",
            "id": "3139695",
            "problem": "给定一个共享协方差高斯假设下的两类分类问题，在这种情况下，线性判别分析 (LDA) 是贝叶斯决策规则。设类条件分布为具有公共协方差矩阵的多元正态分布。从第一性原理可知，该模型的贝叶斯决策规则是线性的，而 Fisher 准则源于将数据投影到一个判别方向上，通过最大化类间方差与类内方差之比来分离类别。\n\n您的任务是，通过比较在训练数据与验证数据上的 Fisher 比率，为在训练数据上学到的判别子空间推导并实现一个有原则的过拟合检测测试，并将观测到的差异与重代入误差的乐观度联系起来。仅从以下基础出发：\n- 两类问题的 Fisher 准则定义：沿着方向 $w$，类间散度为 $w^{\\top} S_{b} w$，类内散度为 $w^{\\top} S_{w} w$，其中 $S_{b}$ 和 $S_{w}$ 分别是根据样本矩计算出的类间和类内散度矩阵。\n- 在共享协方差的高斯模型和相等的类先验概率下，贝叶斯决策规则是线性的，并且与 LDA 一致，LDA 使用从训练数据中估计的合并类内协方差和类均值。\n\n您必须：\n- 使用训练数据估计类均值 $\\mu_{0}$ 和 $\\mu_{1}$以及合并的类内散度矩阵 $S_{w}^{(\\mathrm{train})}$。将 Fisher 方向 $w$ 定义为 Fisher 准则中隐含的广义特征值问题的解，在两类情况下，该解与 $(S_{w}^{(\\mathrm{train})})^{+}(\\mu_{1}-\\mu_{0})$ 成正比，其中 $(\\cdot)^{+}$ 表示 Moore–Penrose 伪逆。\n- 使用学习到的方向 $w$，计算训练数据上的 Fisher 比率\n$$\nJ_{\\mathrm{train}}(w) \\equiv \\frac{w^{\\top} S_{b}^{(\\mathrm{train})} w}{w^{\\top} S_{w}^{(\\mathrm{train})} w},\n$$\n和验证数据上的 Fisher 比率\n$$\nJ_{\\mathrm{val}}(w) \\equiv \\frac{w^{\\top} S_{b}^{(\\mathrm{val})} w}{w^{\\top} S_{w}^{(\\mathrm{val})} w},\n$$\n其中 $S_{b}^{(\\cdot)}$ 定义为相应数据划分上样本均值之差的外积，$S_{w}^{(\\cdot)}$ 为该划分上的合并类内散度。\n- 定义相对 Fisher 下降统计量\n$$\ng \\equiv 1 - \\frac{J_{\\mathrm{val}}(w)}{J_{\\mathrm{train}}(w)},\n$$\n约定如果 $J_{\\mathrm{train}}(w) \\le 0$，则设 $g \\equiv 1$。\n- 使用以下考虑维度和样本量的容忍度来判断是否过拟合：\n$$\n\\tau(p,n) \\equiv \\min\\!\\left(\\frac{1}{2}, \\sqrt{\\frac{p}{\\max(1,\\,n - p)}}\\right),\n$$\n其中 $p$ 是特征维度，$n$ 是训练样本总数（两类之和）。如果 $g > \\tau(p,n)$，则声明“检测到过拟合”。\n- 通过估计合并协方差和类均值，在训练集上训练一个 LDA 分类器，并评估重代入误差（训练误差）和验证误差。报告乐观度，其定义为验证误差与重代入误差之差，表示为 $[0,1]$ 范围内的小数。\n\n每个测试用例的数据生成协议：\n- 从两个 $p$ 维高斯类生成训练和验证数据，类别 $0$ 的均值为 $\\mu_{0}$，类别 $1$ 的均值为 $\\mu_{1}$，两者都使用单位协方差矩阵。使用指定的固定随机种子以确保可复现性。为训练每个类别抽取 $n_{\\mathrm{tr}}$ 个样本，为验证每个类别抽取 $n_{\\mathrm{va}}$ 个样本。\n\n测试套件：\n- 情况 A（良好分离，样本量充足）：\n  - 维度 $p = 2$\n  - 每类训练样本数 $n_{\\mathrm{tr}} = 50$\n  - 每类验证样本数 $n_{\\mathrm{va}} = 400$\n  - 均值 $\\mu_{0} = (0, 0)$, $\\mu_{1} = (2.5, 0)$\n  - 协方差 $I_{p}$\n  - 随机种子 $13$\n- 情况 B（无信号，高维，小样本；在判别方向上容易过拟合）：\n  - 维度 $p = 10$\n  - 每类训练样本数 $n_{\\mathrm{tr}} = 6$\n  - 每类验证样本数 $n_{\\mathrm{va}} = 400$\n  - 均值 $\\mu_{0} = (0,\\ldots,0)$, $\\mu_{1} = (0,\\ldots,0)$\n  - 协方差 $I_{p}$\n  - 随机种子 $7$\n- 情况 C（中等分离度，中等样本量）：\n  - 维度 $p = 5$\n  - 每类训练样本数 $n_{\\mathrm{tr}} = 20$\n  - 每类验证样本数 $n_{\\mathrm{va}} = 400$\n  - 均值 $\\mu_{0} = (0,0,0,0,0)$, $\\mu_{1} = (1,0,0,0,0)$\n  - 协方差 $I_{p}$\n  - 随机种子 $101$\n\n程序要求：\n- 对每种情况，根据上述参数生成数据，从训练数据中估计 Fisher 方向，计算 $J_{\\mathrm{train}}(w)$ 和 $J_{\\mathrm{val}}(w)$、相对下降值 $g$、阈值 $\\tau(p,n)$（其中 $n = 2 n_{\\mathrm{tr}}$），确定布尔类型的过拟合标志，并计算在训练集上训练的 LDA 分类器下的乐观度（定义为验证误差减去重代入误差）。\n- 您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果。对于每种情况，按顺序输出一个包含三个元素的列表：布尔类型的过拟合决策、作为浮点数的相对下降值 $g$、以及作为浮点数的乐观度。因此，最终输出应类似于“[ [flag_A,g_A,opt_A], [flag_B,g_B,opt_B], [flag_C,g_C,opt_C] ]”，其中布尔值和数字采用常规的机器可读形式。",
            "solution": "该问题要求开发并实现一个用于检测线性判别分析 (LDA) 中过拟合的测试。这是通过量化一个学到的判别方向在独立验证集上的性能相较于其在训练集上性能的下降程度来完成的。此次比较的核心指标是 Fisher 比率，它衡量类别的可分性。该比率从训练集到验证集的显著下降表明，判别方向被过度调优以适应训练数据的特定噪声和采样偏差。这种统计上的过拟合与分类错误率的乐观度相关，乐观度定义为验证误差与重代入（训练）误差之差。\n\n理论基础是两类分类设置下的 LDA。我们假设每个类别 $k \\in \\{0, 1\\}$ 的数据都从一个 $p$ 维高斯分布 $\\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma})$ 中抽取。LDA 的一个关键假设是协方差矩阵在各类别间共享，即 $\\boldsymbol{\\Sigma}_0 = \\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}$。在该模型下，并且在类先验概率相等的情况下，贝叶斯最优决策边界是线性的。\n\nFisher 的 LDA 旨在寻找一个方向向量 $\\boldsymbol{w} \\in \\mathbb{R}^p$，当数据投影到这个方向上时，能够最大化类间散度与类内散度之比。这个比率就是 Fisher 准则 $J(\\boldsymbol{w})$。投影后的类间散度与投影后类均值之间距离的平方 $(\\boldsymbol{w}^\\top \\hat{\\boldsymbol{\\mu}}_1 - \\boldsymbol{w}^\\top \\hat{\\boldsymbol{\\mu}}_0)^2$ 相关，而投影后的类内散度是每个投影后类别内方差的总和，即 $\\sum_{k=0}^{1} \\sum_{\\boldsymbol{x}_i \\in C_k} (\\boldsymbol{w}^\\top \\boldsymbol{x}_i - \\boldsymbol{w}^\\top \\hat{\\boldsymbol{\\mu}}_k)^2$，其中 $\\hat{\\boldsymbol{\\mu}}_k$ 是样本均值。这可以用矩阵形式表示为：\n$$\nJ(\\boldsymbol{w}) = \\frac{\\boldsymbol{w}^\\top \\boldsymbol{S}_b \\boldsymbol{w}}{\\boldsymbol{w}^\\top \\boldsymbol{S}_w \\boldsymbol{w}}\n$$\n此处，$\\boldsymbol{S}_b$ 是类间散度矩阵，$\\boldsymbol{S}_w$ 是类内散度矩阵。对于一个两类问题，它们可以从样本数据中计算得出：\n$$\n\\boldsymbol{S}_b = (\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)(\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)^\\top\n$$\n$$\n\\boldsymbol{S}_w = \\sum_{k=0}^{1} \\boldsymbol{S}_k = \\sum_{k=0}^{1} \\sum_{\\boldsymbol{x}_i \\in C_k} (\\boldsymbol{x}_i - \\hat{\\boldsymbol{\\mu}}_k)(\\boldsymbol{x}_i - \\hat{\\boldsymbol{\\mu}}_k)^\\top\n$$\n其中 $C_k$ 表示属于类别 $k$ 的样本集合。最大化 $J(\\boldsymbol{w})$ 的向量 $\\boldsymbol{w}$ 是广义特征值问题 $\\boldsymbol{S}_b\\boldsymbol{w} = \\lambda \\boldsymbol{S}_w\\boldsymbol{w}$ 的解。在两类情况下，$\\boldsymbol{S}_b$ 是一个秩为1的矩阵，其解简化为：\n$$\n\\boldsymbol{w} \\propto \\boldsymbol{S}_w^{-1} (\\hat{\\boldsymbol{\\mu}}_1 - \\hat{\\boldsymbol{\\mu}}_0)\n$$\n在实践中，尤其是在样本数量 $n$ 不远大于维度 $p$ 的高维设置中，矩阵 $\\boldsymbol{S}_w$ 可能是病态的或奇异的。为确保解的稳定性，我们使用 Moore-Penrose 伪逆，记为 $(\\cdot)^{+}$。因此，判别方向 $\\boldsymbol{w}$ 从训练数据中估计为：\n$$\n\\boldsymbol{w} = (\\boldsymbol{S}_{w}^{(\\mathrm{train})})^{+} (\\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})} - \\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})})\n$$\n向量 $\\boldsymbol{w}$ 为之优化的、在训练数据上达到的 Fisher 准则值为：\n$$\nJ_{\\mathrm{train}}(\\boldsymbol{w}) = \\frac{\\boldsymbol{w}^\\top \\boldsymbol{S}_{b}^{(\\mathrm{train})} \\boldsymbol{w}}{\\boldsymbol{w}^\\top \\boldsymbol{S}_{w}^{(\\mathrm{train})} \\boldsymbol{w}}\n$$\n其中 $\\boldsymbol{S}_{b}^{(\\mathrm{train})}$ 和 $\\boldsymbol{S}_{w}^{(\\mathrm{train})}$ 是从训练数据中计算的。\n\n为了检测过拟合，我们评估方向 $\\boldsymbol{w}$ 对未见过的验证数据的泛化能力。我们使用相同的 $\\boldsymbol{w}$，但使用从验证集导出的散度矩阵来计算 Fisher 比率：\n$$\nJ_{\\mathrm{val}}(\\boldsymbol{w}) = \\frac{\\boldsymbol{w}^\\top \\boldsymbol{S}_{b}^{(\\mathrm{val})} \\boldsymbol{w}}{\\boldsymbol{w}^\\top \\boldsymbol{S}_{w}^{(\\mathrm{val})} \\boldsymbol{w}}\n$$\n一个过拟合的模型学到的方向 $\\boldsymbol{w}$ 是高度特定于训练数据的，这导致了较高的 $J_{\\mathrm{train}}(\\boldsymbol{w})$。这个方向对于验证集来说不太可能是最优的，从而导致显著较低的 $J_{\\mathrm{val}}(\\boldsymbol{w})$。我们使用相对 Fisher 下降统计量 $g$ 来量化这种性能下降：\n$$\ng \\equiv 1 - \\frac{J_{\\mathrm{val}}(\\boldsymbol{w})}{J_{\\mathrm{train}}(\\boldsymbol{w})}\n$$\n$g$ 值接近 $0$ 意味着在训练集和验证集上性能相似（泛化能力好），而一个大的正值表示性能大幅下降（过拟合）。根据问题要求，如果 $J_{\\mathrm{train}}(\\boldsymbol{w}) \\le 0$（一种退化情况），我们设 $g=1$。\n\n下降值 $g$ 的显著性取决于问题的统计复杂性，即特征维度 $p$ 和总训练样本量 $n$。由于有限样本效应，总会预期出现一些性能下降。过拟合的决策是通过将 $g$ 与一个考虑维度和样本量的容忍度 $\\tau(p,n)$ 进行比较来形式化的：\n$$\n\\tau(p,n) \\equiv \\min\\!\\left(\\frac{1}{2}, \\sqrt{\\frac{p}{\\max(1,\\,n - p)}}\\right)\n$$\n该阈值随着比率 $p/n$ 的增加而增加，体现了过拟合在高维、小样本情况下更可能发生的原则。如果 $g > \\tau(p,n)$，我们判定为过拟合。\n\n最后，我们将此测试与分类性能联系起来。LDA 分类器在训练数据上进行训练。假设类别先验概率相等，其决策规则基于 $\\boldsymbol{w}^\\top \\boldsymbol{x} - c$ 的符号对新点 $\\boldsymbol{x}$ 进行分类，其中阈值 $c$ 设在投影后类均值的中点：$c = \\frac{1}{2}\\boldsymbol{w}^\\top(\\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})} + \\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})})$。重代入误差是此分类器在训练数据上的错误率。验证误差是其在验证数据上的错误率。乐观度是两者之差：\n$$\n\\text{乐观度} = (\\text{验证误差}) - (\\text{重代入误差})\n$$\n一个大的正向乐观度是过拟合的直接症状。所提出的基于 $g$ 的测试预计会将具有高乐观度的情况标记为过拟合。\n\n对每个测试用例，实现将按以下步骤进行：\n$1$。根据指定的参数（$p, n_{\\mathrm{tr}}, n_{\\mathrm{va}}, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma} = \\boldsymbol{I}_p$）和随机种子，生成训练和验证数据集。\n$2$。从训练数据中，计算样本均值 $\\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})}, \\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})}$ 和散度矩阵 $\\boldsymbol{S}_{b}^{(\\mathrm{train})}, \\boldsymbol{S}_{w}^{(\\mathrm{train})}$。\n$3$。计算 Fisher 判别方向 $\\boldsymbol{w} = (\\boldsymbol{S}_{w}^{(\\mathrm{train})})^{+} (\\hat{\\boldsymbol{\\mu}}_{1}^{(\\mathrm{train})} - \\hat{\\boldsymbol{\\mu}}_{0}^{(\\mathrm{train})})$。\n$4$。从验证数据中计算散度矩阵 $\\boldsymbol{S}_{b}^{(\\mathrm{val})}$ 和 $\\boldsymbol{S}_{w}^{(\\mathrm{val})}$。\n$5$。计算 $J_{\\mathrm{train}}(\\boldsymbol{w})$ 和 $J_{\\mathrm{val}}(\\boldsymbol{w})$，并由此计算统计量 $g$。\n$6$。计算阈值 $\\tau(p, n)$（其中 $n = 2 n_{\\mathrm{tr}}$），并确定布尔类型的过拟合标志。\n$7$。使用从训练数据导出的 $\\boldsymbol{w}$ 和阈值 $c$ 构建 LDA 分类器。\n$8$。评估重代入误差率和验证误差率以计算乐观度。\n$9$。将所有测试用例的结果整理成所需的输出格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, performing LDA,\n    and calculating the overfitting statistics and error optimism.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: well-separated, adequate sample size\n        {\n            \"p\": 2, \"n_tr\": 50, \"n_va\": 400,\n            \"mu0\": np.array([0.0, 0.0]), \"mu1\": np.array([2.5, 0.0]),\n            \"seed\": 13\n        },\n        # Case B: no signal, high-dimensional, small sample\n        {\n            \"p\": 10, \"n_tr\": 6, \"n_va\": 400,\n            \"mu0\": np.zeros(10), \"mu1\": np.zeros(10),\n            \"seed\": 7\n        },\n        # Case C: moderate separation, moderate sample size\n        {\n            \"p\": 5, \"n_tr\": 20, \"n_va\": 400,\n            \"mu0\": np.zeros(5), \"mu1\": np.array([1.0, 0.0, 0.0, 0.0, 0.0]),\n            \"seed\": 101\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p, n_tr, n_va = case[\"p\"], case[\"n_tr\"], case[\"n_va\"]\n        mu0, mu1 = case[\"mu0\"], case[\"mu1\"]\n        seed = case[\"seed\"]\n\n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        cov_matrix = np.identity(p)\n        \n        X_tr_0 = rng.multivariate_normal(mu0, cov_matrix, size=n_tr)\n        X_tr_1 = rng.multivariate_normal(mu1, cov_matrix, size=n_tr)\n        X_va_0 = rng.multivariate_normal(mu0, cov_matrix, size=n_va)\n        X_va_1 = rng.multivariate_normal(mu1, cov_matrix, size=n_va)\n\n        X_train = np.vstack((X_tr_0, X_tr_1))\n        y_train = np.hstack((np.zeros(n_tr, dtype=int), np.ones(n_tr, dtype=int)))\n        X_val = np.vstack((X_va_0, X_va_1))\n        y_val = np.hstack((np.zeros(n_va, dtype=int), np.ones(n_va, dtype=int)))\n\n        # Helper function to compute sample means and scatter matrices\n        def get_scatter_matrices(X0, X1):\n            mu0_hat = np.mean(X0, axis=0)\n            mu1_hat = np.mean(X1, axis=0)\n            \n            # Within-class scatter\n            S0 = (X0 - mu0_hat).T @ (X0 - mu0_hat)\n            S1 = (X1 - mu1_hat).T @ (X1 - mu1_hat)\n            Sw = S0 + S1\n            \n            # Between-class scatter\n            mu_diff = mu1_hat - mu0_hat\n            Sb = np.outer(mu_diff, mu_diff)\n            \n            return mu0_hat, mu1_hat, Sw, Sb\n\n        # 2. Training: Estimate Fisher direction w\n        mu0_tr, mu1_tr, Sw_tr, Sb_tr = get_scatter_matrices(X_tr_0, X_tr_1)\n        w = np.linalg.pinv(Sw_tr) @ (mu1_tr - mu0_tr)\n        \n        # 3. Compute Fisher Ratios\n        J_train_num = w.T @ Sb_tr @ w\n        J_train_den = w.T @ Sw_tr @ w\n        J_train = J_train_num / J_train_den if J_train_den  1e-15 else 0.0\n\n        _, _, Sw_val, Sb_val = get_scatter_matrices(X_va_0, X_va_1)\n        J_val_num = w.T @ Sb_val @ w\n        J_val_den = w.T @ Sw_val @ w\n        J_val = J_val_num / J_val_den if J_val_den  1e-15 else 0.0\n            \n        # 4. Compute Relative Fisher Drop (g) and Overfitting Decision\n        g = 1.0 - (J_val / J_train) if J_train  0.0 else 1.0\n        \n        n_total_train = 2 * n_tr\n        tau_denom = max(1.0, n_total_train - p)\n        tau = min(0.5, np.sqrt(p / tau_denom))\n        \n        is_overfitting = g  tau\n        \n        # 5. LDA Classifier Errors and Optimism\n        threshold = w.T @ (mu0_tr + mu1_tr) / 2.0\n        \n        # Resubstitution error\n        predictions_train = (X_train @ w  threshold).astype(int)\n        resub_error = np.mean(predictions_train != y_train)\n        \n        # Validation error\n        predictions_val = (X_val @ w  threshold).astype(int)\n        val_error = np.mean(predictions_val != y_val)\n        \n        optimism = val_error - resub_error\n        \n        all_results.append([is_overfitting, g, optimism])\n\n    # Final print statement in the exact required format.\n    formatted_cases = []\n    for r in all_results:\n        # The required format is a list of lists, comma-separated, with no spaces.\n        # str() is used for booleans and numbers for standard machine-readable form.\n        case_str = f\"[{str(r[0])},{str(r[1])},{str(r[2])}]\"\n        formatted_cases.append(case_str)\n        \n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```"
        }
    ]
}