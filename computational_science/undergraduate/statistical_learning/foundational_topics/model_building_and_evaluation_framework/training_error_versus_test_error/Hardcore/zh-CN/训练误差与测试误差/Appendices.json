{
    "hands_on_practices": [
        {
            "introduction": "第一个练习将通过岭回归 (ridge regression) 让您亲身体验经典的偏差-方差权衡。您将通过计算来追踪训练误差和测试误差在正则化强度 $\\lambda$ 变化时的轨迹，从而观察到一个基本概念：通过增加训练误差来有意识地接受少量偏差，可以显著降低模型的方差，并最终获得由测试误差衡量的更优泛化性能。这个练习对于理解正则化如何在线性模型中防止过拟合至关重要 。",
            "id": "3188165",
            "problem": "您将执行一个明确定义的比较任务，即比较岭回归在正则化路径上的训练误差与测试误差。该任务完全基于平方损失的经验风险最小化来构建。对于每个测试用例，您需要使用三角函数（角度以弧度为单位）构建确定性的设计矩阵和响应，沿着预设的正则化强度网格计算岭回归估计量，并报告使测试误差最小的正则化值，同时检查相应的训练误差是否并非网格上的最小训练误差。\n\n基本定义：\n- 令 $X \\in \\mathbb{R}^{n \\times d}$ 为设计矩阵，$y \\in \\mathbb{R}^{n}$ 为响应向量。带平方损失的岭回归定义为以下优化问题的解 $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{d}$\n$$\n\\hat{\\beta}_{\\lambda} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{d}} \\left\\{\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\\right\\}.\n$$\n- 当 $\\lambda = 0$ 时，该问题简化为普通最小二乘经验风险最小化。如果正规方程存在多个解（例如，当 $n  d$ 时），则使用 Moore–Penrose 伪逆来获得最小化经验风险的最小欧几里得范数解。\n- 对于任何估计量 $\\hat{\\beta}$，训练均方误差（Mean Squared Error (MSE)）为\n$$\nR_{\\text{train}}(\\hat{\\beta}) = \\frac{1}{n}\\|y - X\\hat{\\beta}\\|_{2}^{2},\n$$\n对于一个包含 $m$ 个样本的测试集 $(X_{\\text{test}}, y_{\\text{test}})$，测试均方误差为\n$$\nR_{\\text{test}}(\\hat{\\beta}) = \\frac{1}{m}\\|y_{\\text{test}} - X_{\\text{test}}\\hat{\\beta}\\|_{2}^{2}.\n$$\n\n任务：\n- 对于每个给定的测试用例，计算正则化网格中每个 $\\lambda$ 对应的 $\\hat{\\beta}_{\\lambda}$\n$$\n\\Lambda = [\\,0,\\,10^{-8},\\,10^{-6},\\,10^{-4},\\,10^{-3},\\,10^{-2},\\,10^{-1},\\,1,\\,10,\\,100\\,].\n$$\n- 对于每个 $\\lambda \\in \\Lambda$，计算 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda})$ 和 $R_{\\text{test}}(\\hat{\\beta}_{\\lambda})$。\n- 找出使 $R_{\\text{test}}(\\hat{\\beta}_{\\lambda})$ 最小化的 $\\lambda^{\\star} \\in \\Lambda$ 的索引 $k^{\\star}$。如果存在多个相同的值，则选择最小的索引（即首次出现的位置）。\n- 确定在 $\\lambda^{\\star}$ 处的训练误差是否严格大于网格上的最小训练误差，即是否满足\n$$\nR_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}}) > \\min_{\\lambda \\in \\Lambda} R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) + \\varepsilon,\n$$\n数值容差为 $\\varepsilon = 10^{-12}$。\n\n角度单位：所有三角函数均使用弧度作为输入。\n\n最终输出格式：\n- 为每个测试用例输出一个列表 $[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$，其中：\n  - $k^{\\star}$ 是 $\\Lambda$ 中的一个整数索引，\n  - $\\lambda^{\\star}$ 是一个四舍五入到 $6$ 位小数的浮点数，\n  - $R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}})$ 是一个四舍五入到 $6$ 位小数的浮点数，\n  - $\\text{train\\_nonmin}$ 是一个布尔值，指示 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}})$ 是否严格比网格上的最小训练 MSE 大至少 $\\varepsilon$。\n- 将所有测试用例的结果汇总到一行中，以方括号括起来的逗号分隔列表形式打印（例如，$[\\text{result\\_1},\\text{result\\_2},\\text{result\\_3}]$），其中每个 $\\text{result\\_i}$ 本身都是如上所述的列表。\n\n测试套件：\n根据以下规则构建三个确定性测试用例。所有角度都以弧度为单位。\n\n- 测试用例 1（欠定，$n  d$，噪声较大）：\n  - 训练集大小：$n = 8$, $d = 12$。测试集大小：$m = 200$。\n  - 对于 $i \\in \\{0,1,\\dots,n-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    X_{ij} = \\sin\\big((i+1)(j+1)\\big) + 0.1 \\cos(i - 2j).\n    $$\n  - 对于 $i \\in \\{0,1,\\dots,m-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (X_{\\text{test}})_{ij} = \\cos\\left(\\frac{(i+1)(j+1)}{2}\\right) + 0.1 \\sin(i + j).\n    $$\n  - 此用例的真实参数：对于 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (\\beta^{\\star})_{j} = (-1)^{j} \\cdot \\frac{j+1}{d}.\n    $$\n  - 噪声：\n    $$\n    \\varepsilon_{i} = 0.3 \\cdot \\sin(3i + 1) \\quad \\text{for } i \\in \\{0,\\dots,n-1\\}, \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.3 \\cdot \\cos(5i + 2) \\quad \\text{for } i \\in \\{0,\\dots,m-1\\}.\n    $$\n  - 响应：$y = X \\beta^{\\star} + \\varepsilon$ 和 $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$。\n\n- 测试用例 2（过定，具有强共线性，中等噪声）：\n  - 训练集大小：$n = 40$, $d = 10$。测试集大小：$m = 200$。\n  - 对于 $i \\in \\{0,1,\\dots,n-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    X_{ij} = \\sin\\big(0.2(i+1)\\big) + 0.01 \\cos\\big(0.3(i+1) + 0.1(j+1)\\big).\n    $$\n  - 对于 $i \\in \\{0,1,\\dots,m-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (X_{\\text{test}})_{ij} = \\sin\\big(0.21(i+1)\\big) + 0.01 \\cos\\big(0.31(i+1) + 0.11(j+1)\\big).\n    $$\n  - 此用例的真实参数：对于 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (\\beta^{\\star})_{j} = \\frac{1}{j+1}.\n    $$\n  - 噪声：\n    $$\n    \\varepsilon_{i} = 0.2 \\cdot \\cos\\big(0.5(i+1)\\big), \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.2 \\cdot \\sin\\big(0.4(i+1)\\big).\n    $$\n  - 响应：$y = X \\beta^{\\star} + \\varepsilon$ 和 $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$。\n\n- 测试用例 3（良定，低噪声）：\n  - 训练集大小：$n = 200$, $d = 5$。测试集大小：$m = 200$。\n  - 对于 $i \\in \\{0,1,\\dots,n-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    X_{ij} = \\sin\\left(\\frac{(i+1)(j+2)}{7}\\right) + 0.2 \\cos\\big((i+1) + 0.3(j+1)\\big).\n    $$\n  - 对于 $i \\in \\{0,1,\\dots,m-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (X_{\\text{test}})_{ij} = \\sin\\left(\\frac{(i+1)(j+2)}{7.3}\\right) + 0.2 \\cos\\big((i+1) + 0.33(j+1)\\big).\n    $$\n  - 此用例的真实参数：对于 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (\\beta^{\\star})_{j} = (-0.7)^{j}.\n    $$\n  - 噪声：\n    $$\n    \\varepsilon_{i} = 0.02 \\cdot \\sin\\big(0.1(i+1) + 0.3\\big), \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.02 \\cdot \\cos\\big(0.12(i+1) - 0.1\\big).\n    $$\n  - 响应：$y = X \\beta^{\\star} + \\varepsilon$ 和 $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$。\n\n实现要求：\n- 对于 $\\lambda  0$，通过求解岭目标函数的一阶最优性条件来获得 $\\hat{\\beta}_{\\lambda}$。\n- 对于 $\\lambda = 0$，使用 Moore–Penrose 伪逆获得最小化经验风险的最小欧几里得范数解。\n- 在检查训练误差是否不是最小值时，比较实数是否严格不等时使用数值容差 $\\varepsilon = 10^{-12}$。\n- 将输出中的所有浮点数四舍五入到 $6$ 位小数。\n\n您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表形式的结果，每个元素是针对一个测试用例的列表 $[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$，按用例 $1, 2, 3$ 的顺序排列。",
            "solution": "该问题要求对岭回归在指定的正则化参数网格 $\\Lambda$ 上的训练误差和测试误差进行比较分析。这是统计学习中一个经典的练习，用以展示偏差-方差权衡。问题提供了一个确定性过程来为三种不同场景构建训练和测试数据。\n\n问题的核心在于求解岭回归优化问题以获得估计量 $\\hat{\\beta}_{\\lambda}$：\n$$\n\\hat{\\beta}_{\\lambda} = \\arg\\min_{\\beta \\in \\mathbb{R}^{d}} \\left\\{\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\\right\\}\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n为了找到解 $\\hat{\\beta}_{\\lambda}$，我们通过计算目标函数关于 $\\beta$ 的梯度并将其设为零，来推导一阶最优性条件：\n$$\n\\nabla_{\\beta} \\left( \\frac{1}{n}(y - X\\beta)^T(y - X\\beta) + \\lambda \\beta^T\\beta \\right) = 0\n$$\n$$\n\\frac{1}{n} \\nabla_{\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) + \\lambda (2\\beta) = 0\n$$\n$$\n\\frac{1}{n} (-2X^T y + 2X^T X \\beta) + 2\\lambda\\beta = 0\n$$\n两边乘以 $n/2$，我们得到：\n$$\n-X^T y + X^T X \\beta + n\\lambda\\beta = 0\n$$\n$$\n(X^T X + n\\lambda I) \\beta = X^T y\n$$\n对于 $\\lambda  0$，矩阵 $(X^T X + n\\lambda I)$ 保证是可逆的，因为 $X^T X$ 是半正定的，加上单位矩阵 $I$ 的一个正常数倍使其变为正定的。因此，唯一解是：\n$$\n\\hat{\\beta}_{\\lambda} = (X^T X + n\\lambda I)^{-1} X^T y\n$$\n对于 $\\lambda = 0$ 的特殊情况，问题简化为普通最小二乘法 (OLS)。如果矩阵 $X^T X$ 是奇异的（例如，在 $n  d$ 的欠定情况下），则存在无穷多个解可以最小化训练误差。问题指定使用最小欧几里得范数解，该解由 Moore-Penrose 伪逆 $X^{+}$ 唯一给出：\n$$\n\\hat{\\beta}_{0} = X^{+} y\n$$\n\n计算过程如下：\n1.  对于每个测试用例，根据指定的确定性公式生成训练数据 $(X, y)$ 和测试数据 $(X_{\\text{test}}, y_{\\text{test}})$。参数 $n, d, m$ 以及用于生成矩阵、真实参数向量 $\\beta^{\\star}$ 和噪声项的函数均已提供。为了效率，数据生成使用向量化的 `numpy` 操作实现。\n2.  遍历给定网格 $\\Lambda = [\\,0,\\,10^{-8},\\,10^{-6},\\,10^{-4},\\,10^{-3},\\,10^{-2},\\,10^{-1},\\,1,\\,10,\\,100\\,]$ 中的每个正则化强度 $\\lambda$。\n3.  在每次迭代中，使用适当的公式计算系数向量 $\\hat{\\beta}_{\\lambda}$：对于 $\\lambda=0$ 使用 `np.linalg.pinv(X) @ y`，对于 $\\lambda>0$ 使用 `np.linalg.inv(XTX + n*lambda*I) @ XTy`。\n4.  利用计算出的 $\\hat{\\beta}_{\\lambda}$，计算训练和测试均方误差 (MSE)：\n    $$\n    R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) = \\frac{1}{n}\\|y - X\\hat{\\beta}_{\\lambda}\\|_{2}^{2}\n    $$\n    $$\n    R_{\\text{test}}(\\hat{\\beta}_{\\lambda}) = \\frac{1}{m}\\|y_{\\text{test}} - X_{\\text{test}}\\hat{\\beta}_{\\lambda}\\|_{2}^{2}\n    $$\n5.  在计算完所有 $\\lambda \\in \\Lambda$ 的误差后，找出使测试误差 $R_{\\text{test}}$ 最小化的 $\\lambda^{\\star}$ 对应的索引 $k^{\\star}$。如果出现平局，选择最小的索引。\n6.  最后，执行检查，看在这个最优 $\\lambda^{\\star}$ 处的训练误差 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}})$ 是否严格大于在整个网格上观察到的最小训练误差。岭回归的训练误差是关于 $\\lambda$ 的单调非递减函数。因此，最小训练误差出现在 $\\lambda=0$ 处。检查 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}})  \\min_{\\lambda \\in \\Lambda} R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) + \\varepsilon$ (其中 $\\varepsilon=10^{-12}$) 验证了从 OLS 解（具有最佳训练性能）转移到正则化解是否能带来测试性能的提升，这是有效正则化的一个标志。\n7.  每个测试用例的结果——$[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$——被收集起来，最终输出被格式化为这些结果列表的字符串表示形式。",
            "answer": "```python\nimport numpy as np\n\ndef _solve_one_case(n, d, m, X_gen_rule, X_test_gen_rule, beta_star_gen_rule, eps_gen_rule, eps_test_gen_rule):\n    \"\"\"\n    Solves a single test case for the ridge regression problem.\n    \"\"\"\n    LAMBDA_GRID = [0.0, 1e-8, 1e-6, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n    EPSILON = 1e-12\n\n    # --- Data Generation ---\n    # Use 0-based indices for generation, consistent with problem statement's math notation.\n    \n    # Training set\n    i_train_idx = np.arange(n)[:, np.newaxis]\n    j_dim_idx = np.arange(d)[np.newaxis, :]\n    X = X_gen_rule(i_train_idx, j_dim_idx)\n\n    beta_star = beta_star_gen_rule(np.arange(d))\n    \n    eps_train = eps_gen_rule(np.arange(n))\n    \n    y = X @ beta_star + eps_train\n\n    # Test set\n    i_test_idx = np.arange(m)[:, np.newaxis]\n    X_test = X_test_gen_rule(i_test_idx, j_dim_idx)\n    \n    eps_test = eps_test_gen_rule(np.arange(m))\n    \n    y_test = X_test @ beta_star + eps_test\n    \n    # --- Ridge Path Calculation ---\n    train_errors = []\n    test_errors = []\n    \n    XTX = X.T @ X\n    XTy = X.T @ y\n    I_d = np.identity(d)\n    \n    for lambda_val in LAMBDA_GRID:\n        if lambda_val == 0.0:\n            # For lambda=0, use Moore-Penrose pseudoinverse for min-norm OLS solution\n            beta_hat = np.linalg.pinv(X) @ y\n        else:\n            # For lambda  0, solve (X'X + n*lambda*I)beta = X'y\n            A = XTX + n * lambda_val * I_d\n            beta_hat = np.linalg.inv(A) @ XTy\n\n        # --- Error Calculation ---\n        # Training MSE: R_train = (1/n) * ||y - X*beta||^2\n        train_mse = np.mean(np.square(y - X @ beta_hat))\n        train_errors.append(train_mse)\n        \n        # Test MSE: R_test = (1/m) * ||y_test - X_test*beta||^2\n        test_mse = np.mean(np.square(y_test - X_test @ beta_hat))\n        test_errors.append(test_mse)\n        \n    # --- Analysis ---\n    # Find the index of lambda that minimizes test error\n    k_star = int(np.argmin(test_errors))\n    lambda_star = LAMBDA_GRID[k_star]\n    min_test_error = test_errors[k_star]\n\n    # Check if the training error at lambda_star is strictly greater than the minimum\n    min_train_error = np.min(train_errors)\n    train_error_at_best_lambda = train_errors[k_star]\n    train_nonmin = bool(train_error_at_best_lambda > min_train_error + EPSILON)\n\n    # --- Formatting ---\n    return [\n        k_star, \n        round(lambda_star, 6), \n        round(min_test_error, 6), \n        train_nonmin\n    ]\n\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (underdetermined, n  d, noisier)\n        {\n            \"n\": 8, \"d\": 12, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin((i+1)*(j+1)) + 0.1 * np.cos(i - 2*j),\n            \"X_test_gen_rule\": lambda i, j: np.cos((i+1)*(j+1)/2.0) + 0.1 * np.sin(i + j),\n            \"beta_star_gen_rule\": lambda j: ((-1)**j) * (j+1)/12.0,\n            \"eps_gen_rule\": lambda i: 0.3 * np.sin(3*i + 1),\n            \"eps_test_gen_rule\": lambda i: 0.3 * np.cos(5*i + 2)\n        },\n        # Test case 2 (overdetermined with strong collinearity, moderate noise)\n        {\n            \"n\": 40, \"d\": 10, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin(0.2*(i+1)) + 0.01 * np.cos(0.3*(i+1) + 0.1*(j+1)),\n            \"X_test_gen_rule\": lambda i, j: np.sin(0.21*(i+1)) + 0.01 * np.cos(0.31*(i+1) + 0.11*(j+1)),\n            \"beta_star_gen_rule\": lambda j: 1.0/(j+1),\n            \"eps_gen_rule\": lambda i: 0.2 * np.cos(0.5*(i+1)),\n            \"eps_test_gen_rule\": lambda i: 0.2 * np.sin(0.4*(i+1))\n        },\n        # Test case 3 (well-determined, low noise)\n        {\n            \"n\": 200, \"d\": 5, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin((i+1)*(j+2)/7.0) + 0.2 * np.cos((i+1) + 0.3*(j+1)),\n            \"X_test_gen_rule\": lambda i, j: np.sin((i+1)*(j+2)/7.3) + 0.2 * np.cos((i+1) + 0.33*(j+1)),\n            \"beta_star_gen_rule\": lambda j: (-0.7)**j,\n            \"eps_gen_rule\": lambda i: 0.02 * np.sin(0.1*(i+1) + 0.3),\n            \"eps_test_gen_rule\": lambda i: 0.02 * np.cos(0.12*(i+1) - 0.1)\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = _solve_one_case(**case_params)\n        results.append(result)\n\n    # The final output must be a string representation of a list of lists, with no spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "在正则化概念的基础上，本练习将探讨另一种模型复杂度控制的情境：决策树。您将首先生成一棵完全生长的决策树，它能达到零训练误差 ($\\hat R_{\\text{train}}=0$) 但很可能导致过拟合，然后您将应用成本复杂度剪枝技术。这个练习  旨在证明，降低模型的结构复杂度——在此即修剪树的分支——是另一种提升测试性能的有效方法，即使这意味着牺牲对训练集的完美拟合。",
            "id": "3188147",
            "problem": "您必须编写一个完整且可运行的程序，该程序在一个合成数据集上构建一个生长至纯的二元分类决策树，然后应用成本复杂度剪枝来展示经验训练误差和样本外测试误差之间的对比。您的代码必须实现所有逻辑，不依赖外部机器学习库；只允许使用基础的数值计算库。本问题中的所有符号、变量和数字都用 LaTeX 书写。\n\n您必须使用的基本定义和原则如下。\n\n- 使用0-1损失的经验风险：给定数据 $\\{(x_i,y_i)\\}_{i=1}^{n}$，其中 $x_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{0,1\\}$，分类器 $h$ 的经验误分类计数为 $R(h) = \\sum_{i=1}^{n} \\mathbf{1}\\{h(x_i) \\neq y_i\\}$。经验误分类率为 $\\hat r(h) = R(h)/n$。\n- 样本外测试误分类率在一个独立的测试集上类似地定义，该测试集与训练集源于相同的数据生成机制；将其表示为 $r_{\\text{test}}(h)$。\n- 决策树分类器通过轴对齐的阈值分裂递归地划分 $\\mathbb{R}^d$，并为每个终端区域（叶节点）分配一个类别标签。叶节点的预测是落入该叶节点的训练样本中的多数类别。\n- 生长至纯：持续分裂，直到每个叶节点只包含单一类别的样本，或者直到每个叶节点最多只包含一个训练样本。在特征向量各不相同的情况下，这将得到 $\\hat r_{\\text{train}} = 0$。\n- 成本复杂度剪枝：给定一棵树 $T$，定义惩罚经验目标\n$$\nR_{\\alpha}(T) = R(T) + \\alpha \\cdot |T|,\n$$\n其中 $R(T)$ 是由 $T$ 在训练集上引起的经验误分类计数，而 $|T|$ 是 $T$ 中的叶节点数量。对于任意 $\\alpha \\ge 0$，剪枝后的子树 $T_{\\alpha}$ 在完全生长树的所有子树中，使 $R_{\\alpha}(T)$ 最小化。\n\n您的程序必须实现：\n\n1. 使用轴对齐分裂在训练集上训练一个二元决策树。每当可以减少不纯度时，使用基尼不纯度来选择分裂。如果没有分裂可以减少不纯度，但一个节点中包含至少两个不同标签的样本，您仍然必须进行分裂，以实现完全生长至纯（例如，沿着某个特征将相邻的样本分开）。仅当每个叶节点都是纯的或只包含一个样本时才停止。\n2. 对给定的树计算经验训练误分类率 $\\hat r_{\\text{train}}$ 和测试误分类率 $r_{\\text{test}}$。\n3. 为完全生长的树计算标准的“最弱连接”成本复杂度剪枝路径。在每个内部节点 $t$ 处，令 $R(t)$ 为如果将 $t$ 替换为一个预测该节点多数类别的叶节点时的经验误分类计数，并令 $R(T_t)$ 为 $t$ 下当前子树的误分类计数。令 $L_t$ 为节点 $t$ 下的叶节点数量。特定于节点的剪枝值为\n$$\ng(t) = \\frac{R(t) - R(T_t)}{L_t - 1}.\n$$\n同时剪掉所有达到最小 $g(t)$ 值的内部节点以获得路径上的下一个子树，并重复此过程，直到只剩下一个单叶节点的树。这会产生一个非递减序列 $\\alpha_0 \\le \\alpha_1 \\le \\dots$ 其中每个 $\\alpha_k$ 是步骤 $k$ 中的最小 $g(t)$ 值。\n4. 按如下方式为给定的 $\\alpha \\ge 0$ 选择子树：选择剪枝路径上其关联阈值 $\\alpha_k \\le \\alpha$ 的最大子树。当 $\\alpha = 0$ 时，必须选择完全生长的树。\n\n数据生成必须是确定性的并且科学上是现实的：\n\n- 输入空间维度为 $d=2$。对于给定的样本大小 $n$，生成特征 $X \\in \\mathbb{R}^{n \\times 2}$，其坐标独立且均匀分布在 $[0,1]$ 上。使用由指定整数作为种子的确定性伪随机数生成器以确保可复现性。\n- 通过以下方式定义底层的无噪声标签：\n$$\ny^{\\star} = \\mathbf{1}\\{x_1  \\tau(x_2)\\}, \\quad \\text{其中 } \\tau(u) = \\tfrac{1}{2} + \\tfrac{1}{4}\\sin(4\\pi u).\n$$\n- 独立地注入标签噪声：以概率 $\\eta \\in [0,1)$ 将 $y^{\\star}$ 翻转为 $1 - y^{\\star}$。对翻转操作使用一个独立的、同样是确定性种子的伪随机数生成器流。\n- 测试集通过相同的机制独立生成，并有其自己指定的种子。将所有误分类率报告为 $[0,1]$ 范围内的实数。\n\n测试套件和要求的输出：\n\n实现以下三个测试用例。对每个用例，您必须构建完全生长的树 $T_{\\text{full}}$，计算其训练误分类率 $\\hat r_{\\text{train}}(T_{\\text{full}})$ 和测试误分类率 $r_{\\text{test}}(T_{\\text{full}})$，然后为指定的 $\\alpha$ 生成剪枝后的树 $T_{\\alpha}$ 并计算相应的比率。您的程序必须返回一个包含三个布尔值的单一列表，每个测试用例一个，并以指定格式聚合成单行字符串。\n\n- 测试用例 1（“happy path”：演示过拟合以及剪枝带来的改善）：\n  - 参数：$n_{\\text{train}} = 120$, $n_{\\text{test}} = 8000$, $\\eta = 0.25$, 训练集种子 $= 1337$, 测试集种子 $= 2027$。\n  - $\\alpha$ 的选择：扫描剪枝路径，并选择使测试误分类率 $r_{\\text{test}}(T_{\\alpha})$ 最小化的子树 $T_{\\alpha}$。通过选择叶节点较少的子树来打破平局，如果仍然平局，则选择在剪枝路径上出现较晚的那个（对应于较大的 $\\alpha$）。此用例必须输出的布尔值为\n    $$\n    b_1 = \\big(\\hat r_{\\text{train}}(T_{\\text{full}}) = 0\\big) \\wedge \\big(r_{\\text{test}}(T_{\\alpha})  r_{\\text{test}}(T_{\\text{full}})\\big) \\wedge \\big(\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})\\big).\n    $$\n- 测试用例 2（边界情况：$\\alpha = 0$ 恢复为不剪枝）：\n  - 参数：$n_{\\text{train}} = 120$, $n_{\\text{test}} = 8000$, $\\eta = 0.25$, 训练集种子 $= 1337$, 测试集种子 $= 2027$，以及 $\\alpha = 0$。\n  - 此用例必须输出的布尔值为\n    $$\n    b_2 = \\big(\\hat r_{\\text{train}}(T_{\\alpha}) = \\hat r_{\\text{train}}(T_{\\text{full}})\\big) \\wedge \\big(r_{\\text{test}}(T_{\\alpha}) = r_{\\text{test}}(T_{\\text{full}})\\big).\n    $$\n- 测试用例 3（边缘情况：非常大的 $\\alpha$ 会使树坍缩为树桩）：\n  - 参数：$n_{\\text{train}} = 120$, $n_{\\text{test}} = 8000$, $\\eta = 0.25$, 训练集种子 $= 1337$, 测试集种子 $= 2027$，以及 $\\alpha = 10.0$。\n  - 此用例必须输出的布尔值为\n    $$\n    b_3 = \\big(|T_{\\alpha}| = 1\\big) \\wedge \\big(\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})\\big).\n    $$\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序为 $[b_1,b_2,b_3]$。每个 $b_i$ 必须是布尔字面量。例如，一个有效的输出行格式为 $[{\\tt True},{\\tt False},{\\tt True}]$。\n\n本任务中不需要物理单位或角度单位。所有概率和比率必须作为 $[0,1]$ 范围内的实数返回，但最终只需打印布尔值 $b_1$、$b_2$ 和 $b_3$ 作为最终输出。",
            "solution": "用户的请求是开发一个 Python 程序，从头开始实现一个二元分类决策树，包括成本复杂度剪枝算法。然后，该程序必须用于在指定的合成数据集上分析训练误差和测试误差之间的权衡。这个问题定义明确，在统计学习理论中有科学依据，并为确定性和可验证的解决方案提供了所有必要的参数和定义。因此，该问题是有效的。\n\n该解决方案被设计为一组相互作用的组件，旨在忠实地实现指定的算法。\n\n**1. 数据生成**\n\n一个函数根据问题描述的数据生成过程来生成训练和测试数据集。这个过程是确定性的，依赖于使用指定整数作为种子的伪随机数生成器来确保可复现性。\n- 特征空间为 $\\mathbb{R}^2$，其坐标 $x_1$ 和 $x_2$ 从 $[0, 1]$ 上的均匀分布中独立抽取。\n- 基于一个非线性决策边界分配一个真实的、无噪声的类别标签 $y^{\\star}$：\n$$\ny^{\\star} = \\mathbf{1}\\{x_1  \\tau(x_2)\\}, \\quad \\text{其中 } \\tau(u) = \\frac{1}{2} + \\frac{1}{4}\\sin(4\\pi u)\n$$\n这个边界是一条正弦波，从而创建了一个复杂的分离任务。\n- 通过以给定的概率 $\\eta$ 翻转真实标签 $y^{\\star}$ 来引入标签噪声。此过程使用一个独立的、带种子的随机数流，以确保噪声独立于特征的生成。\n\n**2. 决策树生长**\n\n分类器的核心是一个递归生长的决策树。使用一个 `Node` 类来表示树结构。每个节点存储了划分给它的数据信息、分裂标准（如果是内部节点）或预测值（如果是叶节点）。\n\n生长算法按以下步骤进行：\n- **递归划分**：从根节点的整个训练集开始，算法寻找最佳的轴对齐分裂来划分数据。\n- **分裂标准**：使用基尼不纯度来评估一次分裂的质量。对于一组样本 $S$，基尼不纯度为 $G(S) = 1 - \\sum_{c \\in \\{0,1\\}} p_c^2$，其中 $p_c$ 是类别为 $c$ 的样本比例。算法搜索所有特征和所有可能的分裂阈值，以找到能使子节点的加权平均基尼不纯度最低的分裂。可能的分裂阈值取为连续唯一特征值之间的中点。\n- **停止与纯度规则**：如果一个节点变得“纯”（只包含一个类别的样本）或只包含一个样本，则该分支的递归终止。关键的是，为了达到指定的训练误差为零，如果一个节点不纯但没有分裂能减少基尼不纯度，则仍然强制进行分裂。这通过找到第一个可用特征并将第一个数据点与其余数据点分开来实现，从而保证树会一直生长直到每个叶节点都变纯，实际上是记住了包括噪声在内的训练数据。这会得到一棵完全生长的树 $T_{\\text{full}}$，其经验训练误分类率为 $\\hat r_{\\text{train}}(T_{\\text{full}}) = 0$。\n\n**3. 成本复杂度剪枝**\n\n完全生长的树 $T_{\\text{full}}$ 预计会对训练数据过拟合。实现成本复杂度剪枝是为了找到一系列更小、可能泛化能力更好的子树。\n- **最弱连接剪枝**：该算法通过迭代剪枝分支来生成一系列子树。在每一步，它会识别出“最弱连接”——即移除该内部节点会导致每个叶节点的误分类成本增加最小的那个节点。\n- **剪枝参数 $g(t)$**：对于当前树中的每个内部节点 $t$，计算一个参数 $g(t)$：\n$$\ng(t) = \\frac{R(t) - R(T_t)}{L_t - 1}\n$$\n这里，$R(t)$ 是如果节点 $t$ 被转换为叶节点时在训练数据上的误分类计数，$R(T_t)$ 是当前以 $t$ 为根的整个子树的总误分类计数，而 $L_t$ 是该子树中的叶节点数量。\n- **路径生成**：算法从 $T_0 = T_{\\text{full}}$ 开始。在步骤 $k$ 中，它为当前树 $T_{k-1}$ 中的所有内部节点计算 $g(t)$。找到的最小值 $\\alpha_k = \\min_t g(t)$ 成为剪枝序列中的下一个阈值。通过取 $T_{k-1}$ 并将所有 $g(t) = \\alpha_k$ 的节点 $t$ 转换成叶节点，形成一棵新树 $T_k$。这个过程重复进行，直到只剩下根节点，从而产生一个树的序列 $[T_0, T_1, \\dots, T_m]$ 以及相应的非递减复杂度参数序列 $[\\alpha_0, \\alpha_1, \\dots, \\alpha_m]$，其中 $\\alpha_0=0$。使用树结构的深拷贝来维护序列中不同的树。\n\n**4. 评估和测试用例**\n\n程序接着使用生成的数据、完整的树和剪枝路径来评估三个指定的测试用例。\n- 对于给定的复杂度参数 $\\alpha$ 值，从生成的路径中选择最优子树 $T_\\alpha$。选择规则是从路径中选择树 $T_k$，使其对应的参数 $\\alpha_k$ 是小于或等于给定 $\\alpha$ 的最大值。\n- 为相关树（$T_{\\text{full}}$ 和 $T_\\alpha$）计算训练集和测试集的误分类率。\n- **测试用例 1**：这个用例要求在剪枝路径上找到使测试误差最小化的子树。搜索遍历路径中的所有树，计算它们的测试误差，并根据指定的平局决胜规则（偏好更小的树，然后是更大的 $\\alpha$）选择最佳的一个。布尔条件 $b_1$ 验证了完整树过拟合（$\\hat r_{\\text{train}}=0$），剪枝后的树泛化能力更好（$r_{\\text{test}}(T_{\\alpha})  r_{\\text{test}}(T_{\\text{full}})$），并且这种改善是以牺牲训练性能为代价的（$\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})$）。\n- **测试用例 2**：当 $\\alpha=0$ 时，选择规则必须选择路径中的第一棵树，即 $T_{\\text{full}}$ 本身。布尔条件 $b_2$ 验证了所选树的误差与完整树的误差相同。\n- **测试用例 3**：当 $\\alpha=10.0$ 这样一个非常大的值时，选择规则预计会选择剪枝最严重的树，即单叶根树桩。布尔条件 $b_3$ 验证了结果树只有一个叶节点（$|T_{\\alpha}| = 1$）并且其训练误差高于完整树。\n\n最终输出将这三个测试的布尔结果聚合成一个格式化的字符串。",
            "answer": "```python\nimport numpy as np\nimport copy\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main solver function to implement and test the decision tree and pruning algorithm.\n    \"\"\"\n\n    class Node:\n        \"\"\"\n        Represents a node in the decision tree.\n        \"\"\"\n        _id_counter = 0\n\n        def __init__(self, indices):\n            self.id = Node._id_counter\n            Node._id_counter += 1\n            \n            self.indices = np.array(indices, dtype=int)\n            \n            self.is_leaf = False\n            self.feature_idx = None\n            self.threshold = None\n            self.left = None\n            self.right = None\n            \n            self.value = None\n\n            self.n_samples = len(self.indices)\n            self.n0 = 0\n            self.n1 = 0\n            self.misclass_as_leaf = 0.0 # R(t)\n            self.gini = 0.0\n\n    class DecisionTree:\n        \"\"\"\n        Encapsulates the decision tree logic: growing, predicting, and helper methods.\n        \"\"\"\n        def __init__(self):\n            self.root = None\n\n        def _calculate_gini(self, y_node):\n            if len(y_node) == 0:\n                return 0.0\n            p1 = np.sum(y_node == 1) / len(y_node)\n            p0 = 1.0 - p1\n            return 1.0 - (p0**2 + p1**2)\n\n        def _find_split(self, X, y, indices):\n            if len(indices) = 1:\n                return None\n\n            parent_gini = self._calculate_gini(y[indices])\n            best_gini = parent_gini\n            best_split = None\n            n_features = X.shape[1]\n\n            for feature_idx in range(n_features):\n                unique_vals = np.unique(X[indices, feature_idx])\n                if len(unique_vals) = 1:\n                    continue\n\n                for i in range(len(unique_vals) - 1):\n                    threshold = (unique_vals[i] + unique_vals[i+1]) / 2.0\n                    \n                    left_indices = indices[X[indices, feature_idx] = threshold]\n                    right_indices = indices[X[indices, feature_idx] > threshold]\n                    \n                    if len(left_indices) == 0 or len(right_indices) == 0:\n                        continue\n\n                    gini_left = self._calculate_gini(y[left_indices])\n                    gini_right = self._calculate_gini(y[right_indices])\n                    \n                    w_left = len(left_indices) / len(indices)\n                    w_right = len(right_indices) / len(indices)\n                    \n                    weighted_gini = w_left * gini_left + w_right * gini_right\n                    \n                    if weighted_gini  best_gini:\n                        best_gini = weighted_gini\n                        best_split = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'left_indices': left_indices,\n                            'right_indices': right_indices\n                        }\n            \n            # Force-split rule for purity if no Gini improvement\n            if best_split is None and parent_gini > 0:\n                for feature_idx in range(n_features):\n                    sorted_indices = indices[np.argsort(X[indices, feature_idx])]\n                    if X[sorted_indices[0], feature_idx]  X[sorted_indices[-1], feature_idx]:\n                        threshold = (X[sorted_indices[0], feature_idx] + X[sorted_indices[1], feature_idx]) / 2.0\n                        left_indices = sorted_indices[X[sorted_indices, feature_idx] = threshold]\n                        right_indices = sorted_indices[X[sorted_indices, feature_idx] > threshold]\n                        best_split = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'left_indices': left_indices,\n                            'right_indices': right_indices\n                        }\n                        break\n            \n            return best_split\n\n        def _grow(self, X, y, indices):\n            node = Node(indices)\n            y_node = y[node.indices]\n            \n            node.n0 = np.sum(y_node == 0)\n            node.n1 = np.sum(y_node == 1)\n            node.value = 1 if node.n1 > node.n0 else 0\n            node.misclass_as_leaf = min(node.n0, node.n1)\n            node.gini = self._calculate_gini(y_node)\n\n            if node.gini == 0.0 or node.n_samples = 1:\n                node.is_leaf = True\n                return node\n            \n            split = self._find_split(X, y, node.indices)\n\n            if split is None:\n                node.is_leaf = True\n                return node\n\n            node.feature_idx = split['feature_idx']\n            node.threshold = split['threshold']\n            node.left = self._grow(X, y, split['left_indices'])\n            node.right = self._grow(X, y, split['right_indices'])\n            \n            return node\n\n        def fit(self, X, y):\n            Node._id_counter = 0\n            self.root = self._grow(X, y, np.arange(len(y)))\n\n        def _predict_single(self, x, node):\n            if node.is_leaf:\n                return node.value\n            if x[node.feature_idx] = node.threshold:\n                return self._predict_single(x, node.left)\n            return self._predict_single(x, node.right)\n\n        def predict(self, X):\n            return np.array([self._predict_single(x, self.root) for x in X])\n\n    def generate_data(n, feature_seed, noise_seed, eta):\n        rng_features = np.random.default_rng(feature_seed)\n        X = rng_features.uniform(0, 1, size=(n, 2))\n        \n        tau = 0.5 + 0.25 * np.sin(4 * np.pi * X[:, 1])\n        y_star = (X[:, 0] > tau).astype(int)\n        \n        rng_noise = np.random.default_rng(noise_seed)\n        flips = rng_noise.random(n)  eta\n        y = y_star.copy()\n        y[flips] = 1 - y[flips]\n        \n        return X, y\n\n    def get_misclassification_rate(y_true, y_pred):\n        return np.mean(y_true != y_pred)\n\n    def get_subtree_info(node):\n        if node.is_leaf:\n            return 1, node.misclass_as_leaf\n        \n        left_leaves, left_misclass = get_subtree_info(node.left)\n        right_leaves, right_misclass = get_subtree_info(node.right)\n        \n        return left_leaves + right_leaves, left_misclass + right_misclass\n\n    def get_leaf_count(node):\n        if node is None:\n            return 0\n        if node.is_leaf:\n            return 1\n        return get_leaf_count(node.left) + get_leaf_count(node.right)\n\n    def get_pruning_path(tree, X_train, y_train):\n        path = [(0.0, copy.deepcopy(tree))]\n        current_tree = tree\n\n        while True:\n            internal_nodes = []\n            q = deque([current_tree.root])\n            node_map = {current_tree.root.id: current_tree.root}\n            while q:\n                node = q.popleft()\n                if not node.is_leaf:\n                    internal_nodes.append(node)\n                    if node.left: q.append(node.left); node_map[node.left.id] = node.left\n                    if node.right: q.append(node.right); node_map[node.right.id] = node.right\n            \n            if not internal_nodes:\n                break\n            \n            g_values = []\n            for node in internal_nodes:\n                L_t, R_Tt = get_subtree_info(node)\n                R_t = node.misclass_as_leaf\n                if L_t > 1:\n                    g = (R_t - R_Tt) / (L_t - 1)\n                    g_values.append((g, node.id))\n            \n            if not g_values:\n                break\n            \n            min_g = min(g for g, _ in g_values)\n            \n            next_tree_obj = copy.deepcopy(current_tree)\n            \n            nodes_to_prune_ids = {nid for g, nid in g_values if np.isclose(g, min_g)}\n            \n            q = deque([next_tree_obj.root])\n            next_node_map = {next_tree_obj.root.id: next_tree_obj.root}\n            while q:\n                node = q.popleft()\n                if not node.is_leaf:\n                    if node.left: q.append(node.left); next_node_map[node.left.id] = node.left\n                    if node.right: q.append(node.right); next_node_map[node.right.id] = node.right\n\n            for node_id in nodes_to_prune_ids:\n                node_to_prune = next_node_map[node_id]\n                node_to_prune.is_leaf = True\n                node_to_prune.left = None\n                node_to_prune.right = None\n\n            current_tree = next_tree_obj\n            path.append((min_g, current_tree))\n        \n        return path\n\n    def select_tree_for_alpha(path, alpha):\n        best_tree = None\n        best_alpha_k = -1.0\n        for alpha_k, tree in path:\n            if alpha_k = alpha and alpha_k >= best_alpha_k:\n                best_alpha_k = alpha_k\n                best_tree = tree\n        return best_tree\n\n    # --- Main Execution Logic ---\n    \n    # Common parameters for all test cases\n    n_train = 120\n    n_test = 8000\n    eta = 0.25\n    train_seed = 1337\n    test_seed = 2027\n\n    X_train, y_train = generate_data(n_train, train_seed, train_seed + 1, eta)\n    X_test, y_test = generate_data(n_test, test_seed, test_seed + 1, eta)\n\n    full_tree = DecisionTree()\n    full_tree.fit(X_train, y_train)\n\n    y_pred_train_full = full_tree.predict(X_train)\n    r_train_full = get_misclassification_rate(y_train, y_pred_train_full)\n\n    y_pred_test_full = full_tree.predict(X_test)\n    r_test_full = get_misclassification_rate(y_test, y_pred_test_full)\n\n    pruning_path = get_pruning_path(full_tree, X_train, y_train)\n    \n    results = []\n\n    # Test Case 1: Optimal alpha by test error\n    best_test_err = float('inf')\n    best_tree_alpha1 = None\n    best_leaf_count = float('inf')\n    best_alpha_val = -1.0\n\n    for alpha_k, tree_k in pruning_path:\n        y_pred_test_k = tree_k.predict(X_test)\n        test_err_k = get_misclassification_rate(y_test, y_pred_test_k)\n        leaf_count_k = get_leaf_count(tree_k.root)\n\n        if test_err_k  best_test_err:\n            best_test_err = test_err_k\n            best_tree_alpha1 = tree_k\n            best_leaf_count = leaf_count_k\n            best_alpha_val = alpha_k\n        elif np.isclose(test_err_k, best_test_err):\n            if leaf_count_k  best_leaf_count:\n                best_tree_alpha1 = tree_k\n                best_leaf_count = leaf_count_k\n                best_alpha_val = alpha_k\n            elif leaf_count_k == best_leaf_count and alpha_k > best_alpha_val:\n                best_tree_alpha1 = tree_k\n                best_alpha_val = alpha_k\n    \n    T_alpha1 = best_tree_alpha1\n    r_test_alpha1 = best_test_err\n    r_train_alpha1 = get_misclassification_rate(y_train, T_alpha1.predict(X_train))\n    \n    b1 = (np.isclose(r_train_full, 0.0)) and (r_test_alpha1  r_test_full) and (r_train_alpha1 > r_train_full)\n    results.append(b1)\n    \n    # Test Case 2: alpha = 0\n    alpha2 = 0.0\n    T_alpha2 = select_tree_for_alpha(pruning_path, alpha2)\n    r_train_alpha2 = get_misclassification_rate(y_train, T_alpha2.predict(X_train))\n    r_test_alpha2 = get_misclassification_rate(y_test, T_alpha2.predict(X_test))\n    \n    b2 = np.isclose(r_train_alpha2, r_train_full) and np.isclose(r_test_alpha2, r_test_full)\n    results.append(b2)\n\n    # Test Case 3: alpha = 10.0\n    alpha3 = 10.0\n    T_alpha3 = select_tree_for_alpha(pruning_path, alpha3)\n    r_train_alpha3 = get_misclassification_rate(y_train, T_alpha3.predict(X_train))\n    num_leaves_alpha3 = get_leaf_count(T_alpha3.root)\n    \n    b3 = (num_leaves_alpha3 == 1) and (r_train_alpha3 > r_train_full)\n    results.append(b3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在探索了经典的防过拟合方法之后，这个练习将介绍一个出人意料的现代现象——“良性过拟合”(benign overfitting)。您将构建一个基于核方法的分类器，该分类器能完美地插值含噪声的训练数据（即训练误差为零），但同时仍能表现出接近理论最优值（贝叶斯误差率）的优异泛化性能。这个高级练习  挑战了“完美拟合总是有害”的传统观念，让您得以一窥复杂模型在高维空间中的微妙行为。",
            "id": "3188112",
            "problem": "考虑统计学习中的一个二元分类场景，其目标是比较一种插值核方法的训练误差和测试误差。设输入空间为 $\\mathbb{R}^d$，并考虑以下数据生成过程：根据以下规则抽取 $n$ 个独立的训练样本 $\\{(X_i, Y_i)\\}_{i=1}^n$ 和 $m$ 个独立的测试样本 $\\{(X'_j, Y'_j)\\}_{j=1}^m$：\n- $X_i \\sim \\mathcal{N}(0, I_d)$ 和 $X'_j \\sim \\mathcal{N}(0, I_d)$ 独立地，其中 $I_d$ 是 $d \\times d$ 的单位矩阵；\n- 抽取一个固定但未知的方向 $w \\sim \\mathcal{N}(0, I_d)$，其独立于所有的 $X_i$ 和 $X'_j$；\n- 定义贝叶斯标签 $Y_i^{\\star} = \\mathrm{sign}(w^\\top X_i)$ 和 $Y'_{j}{}^{\\star} = \\mathrm{sign}(w^\\top X'_j)$，其中如果 $t \\ge 0$，则 $\\mathrm{sign}(t) = 1$，否则为 $-1$；\n- 通过独立翻转引入标签噪声：对于一个固定的噪声水平 $\\eta \\in (0, 0.5)$，以 $1 - \\eta$ 的概率设置 $Y_i = Y_i^{\\star}$，以 $\\eta$ 的概率设置 $Y_i = -Y_i^{\\star}$，类似地，$Y'_j$ 相对于 $Y'_{j}{}^{\\star}$ 也进行同样处理。\n\n根据贝叶斯分类器的定义，贝叶斯法则是 $x \\mapsto \\mathrm{sign}(w^\\top x)$，并且贝叶斯0-1风险（贝叶斯误差）等于噪声率 $\\eta$，因为给定 $X$ 的条件下 $Y$ 的条件分布以概率 $\\eta$ 翻转。\n\n通过在与高斯（径向基函数）核 $k(x, x') = \\exp\\left(-\\|x - x'\\|_2^2 / (2 \\ell^2)\\right)$ 相关联的再生核希尔伯特空间 (RKHS) 中进行无岭核回归来定义插值核分类器，其中 $\\ell  0$ 是长度尺度。设 $K \\in \\mathbb{R}^{n \\times n}$ 是核矩阵，其元素为 $K_{ij} = k(X_i, X_j)$。无岭插值器是函数 $\\hat f(x) = \\sum_{i=1}^n \\alpha_i k(x, X_i)$，其系数 $\\alpha \\in \\mathbb{R}^n$ 被选择以满足 $K \\alpha = y$，其中 $y \\in \\mathbb{R}^n$ 包含观测到的标签 $Y_i \\in \\{-1, 1\\}$。由于高斯核是严格正定的，并且 $X_i$ 几乎必然是不同的，因此 $K$ 几乎必然是可逆的，并且当通过 $\\mathrm{sign}(\\hat f(X_i))$ 进行分类时，插值条件 $K \\alpha = y$ 确保了在训练集上的经验误差为零。\n\n训练0-1误差定义为\n$$\n\\hat R_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\mathrm{sign}(\\hat f(X_i)) \\ne Y_i\\},\n$$\n测试0-1误差定义为\n$$\nR_{\\text{test}} = \\frac{1}{m} \\sum_{j=1}^m \\mathbf{1}\\{\\mathrm{sign}(\\hat f(X'_j)) \\ne Y'_j\\}.\n$$\n良性过拟合现象指的是 $\\hat R_{\\text{train}} = 0$ 但 $R_{\\text{test}}$ 接近贝叶斯误差 $\\eta$ 的情况，这是由于数据分布下核积分算子的特征值快速衰减，从而强调了与贝叶斯决策边界对齐的低复杂度分量。\n\n您的任务是实现一个程序，对于指定的参数设置测试套件 $(n, m, d, \\ell, \\eta, \\text{seed})$，构建上述无岭高斯核插值器，计算 $\\hat R_{\\text{train}}$ 和差值 $R_{\\text{test}} - \\eta$，并在整个测试套件中汇总这些量。\n\n要使用的测试套件是：\n- 情况1：$(n, m, d, \\ell, \\eta, \\text{seed}) = (400, 10000, 10, 2.0, 0.1, 0)$，\n- 情况2：$(n, m, d, \\ell, \\eta, \\text{seed}) = (400, 10000, 10, 0.2, 0.1, 1)$，\n- 情况3：$(n, m, d, \\ell, \\eta, \\text{seed}) = (400, 10000, 10, 2.0, 0.3, 2)$，\n- 情况4：$(n, m, d, \\ell, \\eta, \\text{seed}) = (200, 10000, 50, 2.0, 0.1, 3)$。\n\n对于每种情况：\n- 根据上述过程生成数据，使用给定的 $\\text{seed}$ 初始化所有随机抽取；\n- 构建核矩阵 $K$，求解 $K \\alpha = y$，并定义 $\\hat f(x) = \\sum_{i=1}^n \\alpha_i k(x, X_i)$；\n- 使用预测值 $\\mathrm{sign}(\\hat f(\\cdot))$ 和0-1损失计算 $\\hat R_{\\text{train}}$ 和 $R_{\\text{test}}$；\n- 计算差值 $R_{\\text{test}} - \\eta$。\n\n所有误差必须表示为小数（浮点数），而不是百分比符号。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序如下：\n$$\n[\\hat R_{\\text{train}}^{(1)},\\, (R_{\\text{test}}^{(1)} - \\eta^{(1)}),\\, \\hat R_{\\text{train}}^{(2)},\\, (R_{\\text{test}}^{(2)} - \\eta^{(2)}),\\, \\hat R_{\\text{train}}^{(3)},\\, (R_{\\text{test}}^{(3)} - \\eta^{(3)}),\\, \\hat R_{\\text{train}}^{(4)},\\, (R_{\\text{test}}^{(4)} - \\eta^{(4)})].\n$$\n此问题不涉及任何物理单位或角度。程序必须是自包含的，并且不得读取任何输入。由于使用了固定的随机种子，评估应该是确定性的。测试套件的设计涵盖：\n- 一个具有中等长度尺度和低噪声的通用情况，预计会表现出良性过拟合（情况1），\n- 一个长度尺度非常小的情况，预计会因引出高度局部化的拟合而损害泛化能力（情况2），\n- 一个噪声较高的情况，用于探究良性过拟合下对贝叶斯误差的接近程度（情况3），\n- 一个样本较少的高维情况，用于检验对维度的敏感性（情况4）。",
            "solution": "该问题是有效的。它在统计学习理论（特别是在核方法和良性过拟合现象方面）的既定框架内，提出了一个明确定义的计算任务。所有参数、数据生成过程和目标都以足够的精度进行了规定，并且在科学上是合理的。\n\n解决方案通过为每个参数情况实现指定的模拟来进行。该过程可分为四个主要阶段：数据生成、核分类器构建、误差计算和结果汇总。\n\n### 步骤1：数据生成\n对于每个测试用例，我们都给定了一组参数 $(n, m, d, \\ell, \\eta, \\text{seed})$。数据生成过程如下进行，使用以给定 `seed` 初始化的随机数生成器以保证可复现性。\n\n1.  **定义贝叶斯决策边界**：从标准多元正态分布 $w \\sim \\mathcal{N}(0, I_d)$ 中抽取一个固定但未知的方向向量 $w \\in \\mathbb{R}^d$。该向量定义了真实的潜在线性分类器。\n\n2.  **生成特征向量**：我们生成 $n$ 个训练特征向量 $\\{X_i\\}_{i=1}^n$ 和 $m$ 个测试特征向量 $\\{X'_j\\}_{j=1}^m$。每个向量都独立地从 $\\mathbb{R}^d$ 中的标准多元正态分布中抽取，即 $X_i, X'_j \\sim \\mathcal{N}(0, I_d)$。\n\n3.  **确定贝叶斯标签**：真实、无噪声的标签（贝叶斯标签）由特征向量在方向 $w$ 上的投影的符号决定。对于训练集，$Y_i^{\\star} = \\mathrm{sign}(w^\\top X_i)$；对于测试集，$Y'_{j}{}^{\\star} = \\mathrm{sign}(w^\\top X'_j)$。函数 $\\mathrm{sign}(t)$ 定义为：如果 $t \\ge 0$ 则为 $1$，否则为 $-1$。由于 $w^\\top X$ 是一个连续随机变量，事件 $w^\\top X = 0$ 的概率为零，因此 $\\mathrm{sign}(0)$ 的选择在实践中无关紧要。\n\n4.  **引入标签噪声**：观测到的标签 $Y_i$ 和 $Y'_j$是通过以固定的概率 $\\eta \\in (0, 0.5)$ 翻转贝叶斯标签生成的。也就是说，对于每个样本，观测标签 $Y$ 等于贝叶斯标签 $Y^{\\star}$ 的概率为 $1 - \\eta$，而被翻转为 $-Y^{\\star}$ 的概率为 $\\eta$。此过程独立应用于所有训练和测试样本。此数据生成过程的贝叶斯误差率恰好是 $\\eta$。\n\n### 步骤2：核分类器构建\n该分类器是一个基于高斯（RBF）核 $k(x, x') = \\exp\\left(-\\|x - x'\\|_2^2 / (2 \\ell^2)\\right)$ 的插值核预测器，其中 $\\ell$ 是长度尺度参数。\n\n1.  **构建核矩阵**：计算 $n \\times n$ 的训练核矩阵 $K$，其元素为 $K_{ij} = k(X_i, X_j)$。所有配对 $(i, j)$ 的平方欧氏距离 $\\|X_i - X_j\\|_2^2$ 可以被高效计算。\n\n2.  **求解系数**：预测函数的形式为 $\\hat f(x) = \\sum_{i=1}^n \\alpha_i k(x, X_i)$。系数 $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^\\top$ 通过求解线性系统 $K \\alpha = y$ 来确定，其中 $y = (Y_1, \\dots, Y_n)^\\top$ 是观测到的训练标签向量。这是一种“无岭”回归，因为没有正则化项 $\\lambda I$。由于高斯核是严格正定的，并且点 $X_i$ 以概率1是不同的，所以矩阵 $K$ 几乎必然是可逆的，从而保证了 $\\alpha$ 有唯一解。\n\n### 步骤3：预测和误差计算\n确定系数 $\\alpha$ 后，我们就可以进行预测并评估分类器的性能。\n\n1.  **训练误差 $\\hat R_{\\text{train}}$**：模型在训练数据上的预测由向量 $K \\alpha$ 给出。根据构造，$K \\alpha = y$。因此，对第 $i$ 个训练样本的预测是 $\\hat f(X_i) = y_i$。分类标签是 $\\mathrm{sign}(\\hat f(X_i)) = \\mathrm{sign}(y_i)$。由于 $y_i \\in \\{-1, 1\\}$，我们有 $\\mathrm{sign}(y_i) = y_i = Y_i$。因此，对于每个训练样本，预测标签总是与观测标签相同。这意味着训练误差 $\\hat R_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\mathrm{sign}(\\hat f(X_i)) \\ne Y_i\\}$ 恒为零。这是插值分类器的一个定义性特征。\n\n2.  **测试误差 $R_{\\text{test}}$**：为了评估泛化性能，我们使用 $m$ 个测试样本。\n    *   首先，我们计算测试点 $\\{X'_j\\}$ 和训练点 $\\{X_i\\}$ 之间的 $m \\times n$ 核矩阵 $K_{\\text{test}}$，其中 $(K_{\\text{test}})_{ji} = k(X'_j, X_i)$。\n    *   然后，测试集上的函数值以向量形式计算：$\\hat{y}' = K_{\\text{test}} \\alpha$。\n    *   测试集的预测标签是 $\\mathrm{sign}(\\hat{y}')$。\n    *   测试误差计算为被错误分类的测试样本的比例：$R_{\\text{test}} = \\frac{1}{m} \\sum_{j=1}^m \\mathbf{1}\\{\\mathrm{sign}(\\hat{y}'_j) \\ne Y'_j\\}$。\n\n### 步骤4：结果汇总\n对四个指定的测试用例中的每一个执行该过程。对于每个用例 $k \\in \\{1, 2, 3, 4\\}$，我们计算训练误差 $\\hat R_{\\text{train}}^{(k)}$（将为 $0$）以及测试误差与贝叶斯误差之差 $R_{\\text{test}}^{(k)} - \\eta^{(k)}$。最终输出是一个列表，按指定顺序包含这八个值。\n\n实现中使用了 `numpy` 进行数值运算，并使用 `scipy.spatial.distance.cdist` 高效计算核矩阵所需的成对平方欧氏距离矩阵。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Solves the statistical learning problem for a suite of test cases.\n    For each case, it constructs a ridgeless Gaussian kernel interpolant,\n    computes its training and test errors, and calculates the difference\n    between the test error and the Bayes risk.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, m, d, l, eta, seed)\n        (400, 10000, 10, 2.0, 0.1, 0),\n        (400, 10000, 10, 0.2, 0.1, 1),\n        (400, 10000, 10, 2.0, 0.3, 2),\n        (200, 10000, 50, 2.0, 0.1, 3),\n    ]\n\n    results = []\n    \n    for n, m, d, ell, eta, seed in test_cases:\n        # 1. Data Generation\n        \n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate fixed unknown direction w\n        w = rng.standard_normal(size=d)\n\n        # Generate training and test data\n        X_train = rng.standard_normal(size=(n, d))\n        X_test = rng.standard_normal(size=(m, d))\n\n        # Compute Bayes (noise-free) labels\n        y_star_train = np.sign(X_train @ w)\n        y_star_test = np.sign(X_test @ w)\n        # Handle sign(0) = 1 as per problem spec\n        y_star_train[y_star_train == 0] = 1\n        y_star_test[y_star_test == 0] = 1\n\n        # Introduce label noise to create observed labels\n        flip_train = rng.choice([-1, 1], size=n, p=[eta, 1 - eta])\n        y_train = y_star_train * flip_train\n        \n        flip_test = rng.choice([-1, 1], size=m, p=[eta, 1 - eta])\n        y_test = y_star_test * flip_test\n\n        # 2. Kernel Classifier Construction\n        \n        # Compute the training kernel matrix K\n        # K_ij = exp(-||X_i - X_j||^2 / (2 * l^2))\n        sq_dists_train = cdist(X_train, X_train, 'sqeuclidean')\n        K = np.exp(-sq_dists_train / (2 * ell**2))\n\n        # Solve for coefficients alpha in K * alpha = y\n        try:\n            alpha = np.linalg.solve(K, y_train)\n        except np.linalg.LinAlgError:\n            # Add a small ridge for numerical stability if K is singular\n            # This is not expected for distinct points with an RBF kernel\n            # but is good practice for robustness.\n            alpha = np.linalg.solve(K + 1e-10 * np.eye(n), y_train)\n\n        # 3. Prediction and Error Calculation\n        \n        # Compute training error\n        # By construction, K @ alpha = y_train, so predictions on training points\n        # are y_train. sign(y_train) = y_train since y_train is {-1, 1}.\n        # Thus, training error is always 0 for an interpolator.\n        r_train = 0.0\n\n        # Compute test error\n        # Compute kernel matrix between test and training points\n        sq_dists_test = cdist(X_test, X_train, 'sqeuclidean')\n        K_test = np.exp(-sq_dists_test / (2 * ell**2))\n\n        # Get function values on test points\n        f_hat_test = K_test @ alpha\n\n        # Get predicted labels\n        y_pred_test = np.sign(f_hat_test)\n        y_pred_test[y_pred_test == 0] = 1 # Handle sign(0) = 1\n\n        # Calculate test error (0-1 loss)\n        r_test = np.mean(y_pred_test != y_test)\n\n        # 4. Aggregation\n        \n        # Compute the difference between test error and Bayes error\n        diff = r_test - eta\n        \n        results.extend([r_train, diff])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}