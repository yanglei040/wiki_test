{
    "hands_on_practices": [
        {
            "introduction": "This exercise explores the subtle but important relationship between how we represent our features and the stability of our model. You will compare two different bases for the same class of quadratic functions—raw monomials and orthogonal Legendre polynomials—to see how feature orthogonality affects the variance of the model's parameters. This practice demonstrates a key principle: while the final prediction might be the same, a well-chosen basis can lead to more stable and interpretable parameter estimates, providing a foundational insight into model conditioning and variance .",
            "id": "3180640",
            "problem": "You are given a supervised learning problem with scalar input and output. Inputs $x$ are independently and identically distributed according to the uniform distribution on the interval $[-1,1]$, and outputs are generated by a noise-corrupted quadratic signal,\n$$\ny \\;=\\; f(x) + \\varepsilon, \\quad f(x) \\;=\\; \\beta_{0} + \\beta_{1} x + \\beta_{2} x^{2}, \\quad \\varepsilon \\sim \\text{zero-mean i.i.d. with } \\mathrm{Var}(\\varepsilon) = \\sigma^{2}.\n$$\nYou fit $f$ by linear regression using Ordinary Least Squares (OLS) in two different feature bases spanning the same quadratic function class:\n\n- Raw monomials: $m(x) = \\begin{pmatrix} 1 \\\\ x \\\\ x^{2} \\end{pmatrix}$.\n- An orthogonal polynomial basis: Legendre polynomials up to degree $2$, scaled to be orthonormal under the uniform probability measure on $[-1,1]$. Let $P_{0}(x) = 1$, $P_{1}(x) = x$, and $P_{2}(x) = \\frac{3x^{2} - 1}{2}$ denote the Legendre polynomials. Define the scaled feature vector\n$$\n\\ell(x) \\;=\\; \\begin{pmatrix} \\sqrt{1}\\,P_{0}(x) \\\\ \\sqrt{3}\\,P_{1}(x) \\\\ \\sqrt{5}\\,P_{2}(x) \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 \\\\\n\\sqrt{3}\\,x \\\\\n\\sqrt{5}\\,\\dfrac{3x^{2}-1}{2}\n\\end{pmatrix}.\n$$\nThe Legendre polynomials satisfy the well-tested orthogonality identity $\\int_{-1}^{1} P_{m}(x) P_{n}(x)\\,dx = \\dfrac{2}{2n+1}\\,\\delta_{mn}$, where $\\delta_{mn}$ is the Kronecker delta.\n\nYou collect $n$ samples $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ and use OLS in each basis separately to obtain an estimator $\\hat{f}$. Starting only from the definitions above, the normal equations for OLS, and basic probability limit laws, derive the large-sample prediction variance at an arbitrary test input $x_{0}\\in[-1,1]$ for each basis and compare them. Your derivation must explicitly express any expectations over $x$ as integrals with respect to the uniform probability density on $[-1,1]$, and must not assume any pre-stated formula for estimator covariance.\n\nFinally, provide the single closed-form analytic expression, in terms of $x_{0}$, $n$, and $\\sigma^{2}$, for the common asymptotic prediction variance $\\mathrm{Var}(\\hat{f}(x_{0}))$ that both bases yield. Your final answer must be a single expression and must not include any units or inequalities. No rounding is required.",
            "solution": "The problem asks for the derivation of the large-sample prediction variance for an Ordinary Least Squares (OLS) estimator, $\\hat{f}(x_0)$, at a test point $x_0$. The derivation is to be performed for two different bases of the quadratic polynomial space, and the results compared. The derivation must start from the normal equations.\n\nLet the general feature vector be $\\phi(x) \\in \\mathbb{R}^p$, where $p=3$ in this problem. The model is $\\hat{f}(x) = \\phi(x)^T \\hat{\\theta}$. Given $n$ data points $\\{(x_i, y_i)\\}_{i=1}^n$, the OLS estimator $\\hat{\\theta}$ minimizes the sum of squared residuals. The solution is given by the normal equations:\n$$ (\\Phi^T \\Phi) \\hat{\\theta} = \\Phi^T Y $$\nwhere $\\Phi$ is the $n \\times p$ design matrix with rows $\\phi(x_i)^T$, and $Y$ is the $n \\times 1$ vector of observed outputs $y_i$. This gives the estimator:\n$$ \\hat{\\theta} = (\\Phi^T \\Phi)^{-1} \\Phi^T Y $$\nThe true data generating process is $y_i = f(x_i) + \\varepsilon_i$, where $f(x)$ is in the function space spanned by the basis functions $\\phi(x)$. This means there exists a true parameter vector $\\theta_{\\text{true}}$ such that $f(x) = \\phi(x)^T \\theta_{\\text{true}}$. In matrix form, $Y = \\Phi \\theta_{\\text{true}} + \\vec{\\varepsilon}$, where $\\vec{\\varepsilon}$ is the vector of noise terms.\n\nSubstituting this into the expression for $\\hat{\\theta}$:\n$$ \\hat{\\theta} = (\\Phi^T \\Phi)^{-1} \\Phi^T (\\Phi \\theta_{\\text{true}} + \\vec{\\varepsilon}) = (\\Phi^T \\Phi)^{-1} (\\Phi^T \\Phi) \\theta_{\\text{true}} + (\\Phi^T \\Phi)^{-1} \\Phi^T \\vec{\\varepsilon} = \\theta_{\\text{true}} + (\\Phi^T \\Phi)^{-1} \\Phi^T \\vec{\\varepsilon} $$\nThe model is correctly specified, so the OLS estimator is unbiased, as $E[\\hat{\\theta} | \\Phi] = \\theta_{\\text{true}} + (\\Phi^T \\Phi)^{-1} \\Phi^T E[\\vec{\\varepsilon}] = \\theta_{\\text{true}}$, given that $E[\\vec{\\varepsilon}] = \\vec{0}$.\n\nThe prediction at a test point $x_0$ is $\\hat{f}(x_0) = \\phi(x_0)^T \\hat{\\theta}$. The prediction variance, conditional on the training inputs $\\{x_i\\}$, is:\n$$ \\mathrm{Var}(\\hat{f}(x_0) | \\Phi) = \\mathrm{Var}(\\phi(x_0)^T \\hat{\\theta} | \\Phi) = \\phi(x_0)^T \\mathrm{Cov}(\\hat{\\theta} | \\Phi) \\phi(x_0) $$\nThe covariance of the estimator $\\hat{\\theta}$ is:\n$$ \\mathrm{Cov}(\\hat{\\theta} | \\Phi) = E[(\\hat{\\theta} - \\theta_{\\text{true}})(\\hat{\\theta} - \\theta_{\\text{true}})^T | \\Phi] = E[ ((\\Phi^T \\Phi)^{-1} \\Phi^T \\vec{\\varepsilon}) ((\\Phi^T \\Phi)^{-1} \\Phi^T \\vec{\\varepsilon})^T | \\Phi] $$\n$$ \\mathrm{Cov}(\\hat{\\theta} | \\Phi) = (\\Phi^T \\Phi)^{-1} \\Phi^T E[\\vec{\\varepsilon}\\vec{\\varepsilon}^T | \\Phi] \\Phi (\\Phi^T \\Phi)^{-1} $$\nSince the noise terms $\\varepsilon_i$ are i.i.d. with variance $\\sigma^2$, we have $E[\\vec{\\varepsilon}\\vec{\\varepsilon}^T] = \\sigma^2 I_n$, where $I_n$ is the $n \\times n$ identity matrix.\n$$ \\mathrm{Cov}(\\hat{\\theta} | \\Phi) = (\\Phi^T \\Phi)^{-1} \\Phi^T (\\sigma^2 I_n) \\Phi (\\Phi^T \\Phi)^{-1} = \\sigma^2 (\\Phi^T \\Phi)^{-1} (\\Phi^T \\Phi) (\\Phi^T \\Phi)^{-1} = \\sigma^2 (\\Phi^T \\Phi)^{-1} $$\nSubstituting this back into the variance expression for the prediction:\n$$ \\mathrm{Var}(\\hat{f}(x_0) | \\Phi) = \\sigma^2 \\phi(x_0)^T (\\Phi^T \\Phi)^{-1} \\phi(x_0) $$\nFor large $n$, we use the law of large numbers on the matrix $\\frac{1}{n} \\Phi^T \\Phi$:\n$$ \\frac{1}{n} \\Phi^T \\Phi = \\frac{1}{n} \\sum_{i=1}^n \\phi(x_i) \\phi(x_i)^T \\xrightarrow{p} E[\\phi(x) \\phi(x)^T] $$\nLet $G = E[\\phi(x) \\phi(x)^T]$. The expectation is over the distribution of $x$, which is uniform on $[-1, 1]$. The probability density function is $p(x) = 1/2$ for $x \\in [-1, 1]$. For any function $g(x)$, $E[g(x)] = \\int_{-1}^{1} g(x) \\frac{1}{2} dx$.\nThe large-sample approximation for the inverse is $(\\Phi^T \\Phi)^{-1} \\approx \\frac{1}{n} G^{-1}$. The asymptotic prediction variance is therefore:\n$$ \\mathrm{Var}(\\hat{f}(x_0)) = \\frac{\\sigma^2}{n} \\phi(x_0)^T G^{-1} \\phi(x_0) $$\nWe now apply this general result to the two specified bases.\n\n**Basis 1: Raw Monomials**\nThe feature vector is $m(x) = \\begin{pmatrix} 1  x  x^2 \\end{pmatrix}^T$. We compute the matrix $G_m = E[m(x) m(x)^T]$. The entries are $G_{m,jk} = E[x^{j+k-2}]$ for $j, k \\in \\{1,2,3\\}$.\nWe require the moments $E[x^k] = \\int_{-1}^1 x^k \\frac{1}{2} dx = \\frac{1}{2} \\left[ \\frac{x^{k+1}}{k+1} \\right]_{-1}^1$. This is $0$ for odd $k$ and $\\frac{1}{k+1}$ for even $k$.\n$E[x^0]=1$, $E[x^1]=0$, $E[x^2]=1/3$, $E[x^3]=0$, $E[x^4]=1/5$.\n$$ G_m = E \\begin{pmatrix} 1  x  x^2 \\\\ x  x^2  x^3 \\\\ x^2  x^3  x^4 \\end{pmatrix} = \\begin{pmatrix} E[1]  E[x]  E[x^2] \\\\ E[x]  E[x^2]  E[x^3] \\\\ E[x^2]  E[x^3]  E[x^4] \\end{pmatrix} = \\begin{pmatrix} 1  0  \\frac{1}{3} \\\\ 0  \\frac{1}{3}  0 \\\\ \\frac{1}{3}  0  \\frac{1}{5} \\end{pmatrix} $$\nTo find the inverse $G_m^{-1}$, we compute its determinant: $\\det(G_m) = 1(\\frac{1}{3}\\frac{1}{5} - 0) - 0 + \\frac{1}{3}(0 - \\frac{1}{9}) = \\frac{1}{15} - \\frac{1}{27} = \\frac{9-5}{135} = \\frac{4}{135}$.\nThe inverse is $G_m^{-1} = \\frac{1}{\\det(G_m)} \\text{adj}(G_m)$. The cofactor matrix is symmetric, so it equals its adjugate.\n$$ \\text{adj}(G_m) = \\begin{pmatrix} \\frac{1}{15}  0  -\\frac{1}{9} \\\\ 0  \\frac{1}{5}-\\frac{1}{9}  0 \\\\ -\\frac{1}{9}  0  \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{15}  0  -\\frac{1}{9} \\\\ 0  \\frac{4}{45}  0 \\\\ -\\frac{1}{9}  0  \\frac{1}{3} \\end{pmatrix} $$\n$$ G_m^{-1} = \\frac{135}{4} \\begin{pmatrix} \\frac{1}{15}  0  -\\frac{1}{9} \\\\ 0  \\frac{4}{45}  0 \\\\ -\\frac{1}{9}  0  \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{4}  0  -\\frac{15}{4} \\\\ 0  3  0 \\\\ -\\frac{15}{4}  0  \\frac{45}{4} \\end{pmatrix} $$\nThe prediction variance is:\n$$ \\mathrm{Var}(\\hat{f}_m(x_0)) = \\frac{\\sigma^2}{n} m(x_0)^T G_m^{-1} m(x_0) = \\frac{\\sigma^2}{n} \\begin{pmatrix} 1  x_0  x_0^2 \\end{pmatrix} \\begin{pmatrix} \\frac{9}{4}  0  -\\frac{15}{4} \\\\ 0  3  0 \\\\ -\\frac{15}{4}  0  \\frac{45}{4} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_0 \\\\ x_0^2 \\end{pmatrix} $$\n$$ = \\frac{\\sigma^2}{n} \\left( (\\frac{9}{4} \\cdot 1) + (3 \\cdot x_0^2) + (\\frac{45}{4} \\cdot x_0^4) + 2(-\\frac{15}{4} \\cdot 1 \\cdot x_0^2) \\right) $$\n$$ = \\frac{\\sigma^2}{n} \\left( \\frac{9}{4} + 3x_0^2 - \\frac{15}{2} x_0^2 + \\frac{45}{4} x_0^4 \\right) $$\n$$ = \\frac{\\sigma^2}{n} \\left( \\frac{9}{4} - \\frac{9}{2} x_0^2 + \\frac{45}{4} x_0^4 \\right) = \\frac{\\sigma^2}{4n} (9 - 18x_0^2 + 45x_0^4) $$\n\n**Basis 2: Orthonormal Legendre Polynomials**\nThe feature vector is $\\ell(x) = \\begin{pmatrix} \\sqrt{1}P_0(x)  \\sqrt{3}P_1(x)  \\sqrt{5}P_2(x) \\end{pmatrix}^T$. Let's denote its components as $\\ell_j(x) = \\sqrt{2j+1}P_j(x)$ for $j=0,1,2$.\nWe compute $G_\\ell = E[\\ell(x) \\ell(x)^T]$. The entries are $G_{\\ell,jk} = E[\\ell_j(x) \\ell_k(x)]$.\nUsing the given orthogonality property $\\int_{-1}^1 P_j(x)P_k(x)dx = \\frac{2}{2k+1}\\delta_{jk}$, we have:\n$$ E[P_j(x)P_k(x)] = \\int_{-1}^1 P_j(x)P_k(x) \\frac{1}{2} dx = \\frac{1}{2k+1}\\delta_{jk} $$\nSo the entries of $G_\\ell$ are:\n$$ G_{\\ell,jk} = E[\\sqrt{2j+1} P_j(x) \\sqrt{2k+1} P_k(x)] = \\sqrt{(2j+1)(2k+1)} E[P_j(x)P_k(x)] = \\sqrt{(2j+1)(2k+1)} \\frac{1}{2k+1} \\delta_{jk} = \\delta_{jk} $$\nThus, $G_\\ell$ is the $3 \\times 3$ identity matrix, $I_3$. This is why the basis is called orthonormal.\nThe inverse is trivial: $G_\\ell^{-1} = I_3$.\nThe prediction variance is:\n$$ \\mathrm{Var}(\\hat{f}_\\ell(x_0)) = \\frac{\\sigma^2}{n} \\ell(x_0)^T G_\\ell^{-1} \\ell(x_0) = \\frac{\\sigma^2}{n} \\ell(x_0)^T \\ell(x_0) = \\frac{\\sigma^2}{n} \\sum_{j=0}^2 (\\ell_j(x_0))^2 $$\nUsing the definitions $\\ell_0(x)=1$, $\\ell_1(x)=\\sqrt{3}x$, and $\\ell_2(x) = \\sqrt{5}\\frac{3x^2-1}{2}$:\n$$ \\mathrm{Var}(\\hat{f}_\\ell(x_0)) = \\frac{\\sigma^2}{n} \\left( 1^2 + (\\sqrt{3}x_0)^2 + \\left(\\sqrt{5}\\frac{3x_0^2-1}{2}\\right)^2 \\right) $$\n$$ = \\frac{\\sigma^2}{n} \\left( 1 + 3x_0^2 + 5 \\frac{(3x_0^2-1)^2}{4} \\right) = \\frac{\\sigma^2}{n} \\left( 1 + 3x_0^2 + \\frac{5}{4}(9x_0^4 - 6x_0^2 + 1) \\right) $$\n$$ = \\frac{\\sigma^2}{n} \\left( 1 + 3x_0^2 + \\frac{45}{4}x_0^4 - \\frac{30}{4}x_0^2 + \\frac{5}{4} \\right) $$\n$$ = \\frac{\\sigma^2}{n} \\left( (1+\\frac{5}{4}) + (3-\\frac{30}{4})x_0^2 + \\frac{45}{4}x_0^4 \\right) = \\frac{\\sigma^2}{n} \\left( \\frac{9}{4} - \\frac{18}{4}x_0^2 + \\frac{45}{4}x_0^4 \\right) $$\n$$ = \\frac{\\sigma^2}{4n} (9 - 18x_0^2 + 45x_0^4) $$\n\n**Comparison**\nThe asymptotic prediction variance derived for both bases is identical:\n$$ \\mathrm{Var}(\\hat{f}(x_0)) = \\frac{\\sigma^2}{4n} (9 - 18x_0^2 + 45x_0^4) $$\nThis result is expected. The OLS estimator $\\hat{f}$ is the orthogonal projection of the data vector $Y$ onto the subspace spanned by the columns of the design matrix. Both bases, monomials $\\{1, x, x^2\\}$ and Legendre polynomials $\\{P_0, P_1, P_2\\}$, span the exact same function space of quadratic polynomials. Therefore, the subspace onto which the data is projected is identical, irrespective of the basis chosen to represent it. Consequently, the OLS predictor $\\hat{f}(x)$ is unique and independent of the basis. It follows that any property of this predictor, such as its variance, must also be the same. The calculation for the orthonormal basis is significantly simpler due to the diagonality (in this case, identity) of the matrix $G$.\nThe final expression for the common asymptotic prediction variance is thus confirmed by two separate derivations.",
            "answer": "$$\n\\boxed{\\frac{\\sigma^2}{4n} (9 - 18x_0^2 + 45x_0^4)}\n$$"
        },
        {
            "introduction": "Here, we tackle the classic problem of multicollinearity and its impact on prediction variance using one of the most fundamental tools for regularization: ridge regression. You will analytically derive the bias and variance of a ridge estimator under two distinct scenarios: one with perfectly uncorrelated (orthogonal) features and another with highly correlated features. This exercise provides a concrete demonstration of how the regularization parameter $\\lambda$ introduces a small, controlled bias to dramatically reduce the variance caused by collinear predictors, illustrating the core of the bias-variance trade-off .",
            "id": "3180600",
            "problem": "Consider the standard linear model $Y = X \\beta + \\varepsilon$ with $X \\in \\mathbb{R}^{n \\times p}$, true parameter vector $\\beta \\in \\mathbb{R}^{p}$, and noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. For a fixed regularization strength $\\lambda > 0$, the ridge estimator is $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y$. For a new input $x_{0} \\in \\mathbb{R}^{p}$, define the prediction $\\hat{f}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$. The mean-squared prediction error for the conditional mean at $x_{0}$ is $\\mathrm{MSE}(x_{0}) = \\mathbb{E}\\big[(\\hat{f}_{\\lambda}(x_{0}) - x_{0}^{\\top} \\beta)^{2}\\big]$, where the expectation is over the training noise $\\varepsilon$ only.\n\nYou will compare two designs with $p = 2$ and the same sample size $n$ and noise level $\\sigma^{2}$:\n- Design A (orthogonal): $X^{\\top} X = n I_{2}$.\n- Design B (highly correlated): $X^{\\top} X = n \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ with correlation $\\rho \\in (0, 1)$.\n\nUsing only the model definition, the ridge estimator definition, and the definitions of expectation and variance, first derive closed-form expressions for the prediction bias $\\mathrm{Bias}(x_{0}) = \\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] - x_{0}^{\\top} \\beta$ and the prediction variance $\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0}))$ in each design. Then, for the specific values $n = 100$, $\\sigma^{2} = 1$, $\\lambda = 1$, $\\rho = 0.9$, $\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, compute the ratio\n$$\nR \\;=\\; \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})},\n$$\nwhere $\\mathrm{MSE}_{A}(x_{0})$ and $\\mathrm{MSE}_{B}(x_{0})$ denote the mean-squared prediction errors under Design A and Design B, respectively. Here $\\mathrm{MSE}(x_{0})$ refers to the error for predicting the conditional mean $x_{0}^{\\top} \\beta$ and therefore equals the sum of squared bias and variance.\n\nProvide your final numerical value of $R$ rounded to four significant figures.",
            "solution": "### Derivation of General Bias and Variance\nFirst, we derive general expressions for the bias and variance of the prediction $\\hat{f}_{\\lambda}(x_{0})$. The ridge estimator is $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y$.\n\nThe expectation of the predictor is:\n$$\n\\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] = \\mathbb{E}[x_{0}^{\\top} \\hat{\\beta}_{\\lambda}] = x_{0}^{\\top} \\mathbb{E}[\\hat{\\beta}_{\\lambda}]\n$$\nSince $\\mathbb{E}[Y] = \\mathbb{E}[X \\beta + \\varepsilon] = X \\beta$, the expectation of the estimator is:\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda}] = \\mathbb{E}[(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} (X \\beta)\n$$\nThe bias of the prediction is then:\n$$\n\\mathrm{Bias}(x_{0}) = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta - x_{0}^{\\top} \\beta = x_{0}^{\\top} \\left[ (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - I_p \\right] \\beta\n$$\nUsing the identity $I_p = (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p)$, we simplify the term in the brackets:\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p) = -\\lambda (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\nSo, the bias is:\n$$\n\\mathrm{Bias}(x_{0}) = -\\lambda x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} \\beta\n$$\nThe variance of the prediction is:\n$$\n\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0})) = \\mathrm{Var}(x_{0}^{\\top} \\hat{\\beta}_{\\lambda}) = x_{0}^{\\top} \\mathrm{Var}(\\hat{\\beta}_{\\lambda}) x_{0}\n$$\nThe variance of the estimator is:\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\mathrm{Var}((X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y) = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\mathrm{Var}(Y) (X^{\\top})^{\\top} ((X^{\\top} X + \\lambda I_{p})^{-1})^{\\top}\n$$\nUsing $\\mathrm{Var}(Y) = \\sigma^2 I_n$ and the fact that $(X^{\\top} X + \\lambda I_{p})^{-1}$ is symmetric:\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\sigma^{2} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\nSo, the prediction variance is:\n$$\n\\mathrm{Var}(x_{0}) = \\sigma^{2} x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1} x_{0}\n$$\n\n### Analysis of Design A (Orthogonal)\nFor Design A, $X^{\\top} X = n I_{2}$. The term $(X^{\\top} X + \\lambda I_{2})$ becomes $n I_{2} + \\lambda I_{2} = (n+\\lambda)I_2$. Its inverse is $\\frac{1}{n+\\lambda}I_2$.\n\n**Squared Bias for Design A**:\n$$\n(\\mathrm{Bias}_{A}(x_{0}))^2 = \\left( -\\lambda x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) \\beta \\right)^2 = \\frac{\\lambda^2}{(n+\\lambda)^2} (x_{0}^{\\top} \\beta)^2\n$$\nWith the given values, $x_{0}^{\\top} \\beta = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + (-1) \\cdot 1 = 0$.\nTherefore, $(\\mathrm{Bias}_{A}(x_{0}))^2 = 0$.\n\n**Variance for Design A**:\n$$\n\\mathrm{Var}_{A}(x_{0}) = \\sigma^{2} x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) (n I_2) \\left(\\frac{1}{n+\\lambda}I_2\\right) x_{0} = \\frac{n \\sigma^2}{(n+\\lambda)^2} x_{0}^{\\top} x_{0}\n$$\nWith the given values, $x_{0}^{\\top} x_{0} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1^2 + (-1)^2 = 2$.\nSo, $\\mathrm{Var}_{A}(x_{0}) = \\frac{100 \\cdot 1}{(100+1)^2} \\cdot 2 = \\frac{200}{101^2} = \\frac{200}{10201}$.\n\n**MSE for Design A**:\n$\\mathrm{MSE}_{A}(x_{0}) = (\\mathrm{Bias}_{A}(x_{0}))^2 + \\mathrm{Var}_{A}(x_{0}) = 0 + \\frac{200}{10201} = \\frac{200}{10201}$.\n\n### Analysis of Design B (Correlated)\nFor Design B, $X^{\\top} X = n \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$. The eigenvalues of $\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ are $1+\\rho$ and $1-\\rho$, with corresponding eigenvectors $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe eigenvalues of $X^{\\top} X$ are $\\lambda_1 = n(1+\\rho)$ and $\\lambda_2 = n(1-\\rho)$.\nThe normalized eigenvectors are $v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nThe given vectors are $\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\sqrt{2} v_1$ and $x_0 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\sqrt{2} v_2$. They are aligned with the eigenvectors.\n\nLet $U = \\begin{pmatrix} v_1  v_2 \\end{pmatrix}$ be the matrix of eigenvectors. Then $X^{\\top}X = U D U^{\\top}$ where $D = \\mathrm{diag}(\\lambda_1, \\lambda_2)$.\nThen $X^{\\top}X + \\lambda I_2 = U D U^{\\top} + \\lambda U U^{\\top} = U(D+\\lambda I_2)U^{\\top}$.\nThe inverse is $(X^{\\top}X + \\lambda I_2)^{-1} = U(D+\\lambda I_2)^{-1}U^{\\top}$.\n\n**Squared Bias for Design B**:\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} \\beta\n$$\nWe have $U^{\\top}x_0 = \\sqrt{2} U^{\\top}v_2 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and $U^{\\top}\\beta = \\sqrt{2} U^{\\top}v_1 = \\sqrt{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda (\\sqrt{2}\\begin{pmatrix} 0  1 \\end{pmatrix}) (D+\\lambda I_2)^{-1} (\\sqrt{2}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}) = -2\\lambda \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\lambda_1+\\lambda}  0 \\\\ 0  \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -2\\lambda \\begin{pmatrix} 0  \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 0\n$$\nTherefore, $(\\mathrm{Bias}_{B}(x_{0}))^2 = 0$.\n\n**Variance for Design B**:\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} U D U^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} x_{0}\n$$\nThis simplifies to:\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U D(D+\\lambda I_2)^{-2} U^{\\top} x_{0}\n$$\nUsing $U^{\\top}x_0 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 (\\sqrt{2}\\begin{pmatrix} 0  1 \\end{pmatrix}) \\begin{pmatrix} \\frac{\\lambda_1}{(\\lambda_1+\\lambda)^2}  0 \\\\ 0  \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2} \\end{pmatrix} (\\sqrt{2}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}) = 2\\sigma^2 \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2}\n$$\nNow, substitute the numerical values: $n = 100$, $\\sigma^{2} = 1$, $\\lambda = 1$, $\\rho = 0.9$.\n$\\lambda_2 = n(1-\\rho) = 100(1-0.9) = 100(0.1) = 10$.\n$$\n\\mathrm{Var}_{B}(x_{0}) = 2(1) \\frac{10}{(10+1)^2} = \\frac{20}{11^2} = \\frac{20}{121}\n$$\n\n**MSE for Design B**:\n$\\mathrm{MSE}_{B}(x_{0}) = (\\mathrm{Bias}_{B}(x_{0}))^2 + \\mathrm{Var}_{B}(x_{0}) = 0 + \\frac{20}{121} = \\frac{20}{121}$.\n\n### Calculation of the Ratio R\nThe ratio $R$ is the quotient of the two MSE values.\n$$\nR = \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})} = \\frac{20/121}{200/10201} = \\frac{20}{121} \\times \\frac{10201}{200} = \\frac{10201}{10 \\times 121} = \\frac{10201}{1210}\n$$\nFor the numerical value rounded to four significant figures:\n$R \\approx 8.4305785...$\nRounding to four significant figures gives $8.431$.",
            "answer": "$$\n\\boxed{8.431}\n$$"
        },
        {
            "introduction": "This practice moves beyond simple parameter shrinkage to a more general form of regularization by penalizing a model's \"roughness\" or complexity. Using a simplified smoothing spline model, you will derive explicit expressions for the bias and variance as a function of the regularization strength $\\lambda$, which penalizes the curvature of the fitted function. By finding the optimal $\\lambda$ that minimizes the mean squared error, you will gain hands-on experience with how nonparametric methods manage the trade-off between fitting the data closely (low bias) and avoiding overly complex functions (low variance) .",
            "id": "3180624",
            "problem": "Consider the nonparametric regression model with independent and identically distributed noise: $y_i = f_0(x_i) + \\varepsilon_i$ for $i = 1,\\dots,n$, where $x_i = i/n$ are equally spaced in $[0,1]$ and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. Let the estimator be obtained by minimizing a penalized empirical risk with a roughness penalty on the second derivative: among functions of the form $f(x) = \\theta \\,\\varphi(x)$, choose $\\hat{f}$ to minimize\n$$\n\\frac{1}{n} \\sum_{i=1}^n \\big(y_i - f(x_i)\\big)^2 \\;+\\; \\lambda \\int_0^1 \\big(f''(x)\\big)^2 \\, dx,\n$$\nwhere $\\lambda \\ge 0$ is a tuning parameter and $\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)$ for a fixed positive integer $k$. Assume $n$ is an even integer multiple of $k$ so that $(1/n)\\sum_{i=1}^n \\varphi(x_i)^2 = 1$ holds exactly.\n\n1) Starting from the definitions of pointwise bias, variance, and mean squared error, derive explicit formulas for the pointwise bias and variance of $\\hat{f}(x_0)$ as functions of $\\lambda$ at an arbitrary $x_0 \\in [0,1]$. Your derivation should proceed from first principles by solving the penalized least squares problem for $\\hat{\\theta}$, expressing $\\hat{f}(x_0)$ as a linear estimator of the data, and then taking expectations and variances.\n\n2) Specialize to the known signal $f_0(x) = \\beta \\sin(2\\pi k x)$ with $\\beta \\in \\mathbb{R}$, and take $x_0$ to be a location of maximal curvature for $f_0$ (for example, any $x_0$ such that $\\sin(2\\pi k x_0) = 1$). Compute the curvature weight $r := \\int_0^1 \\big(\\varphi''(x)\\big)^2\\,dx$ explicitly, and use your formulas from part (1) to write the pointwise mean squared error $\\mathbb{E}\\big[(\\hat{f}(x_0) - f_0(x_0))^2\\big]$ as a function of $\\lambda$.\n\n3) Determine the value of $\\lambda$ that minimizes this pointwise mean squared error at $x_0$ from part (2). Express your final answer as a closed-form expression in terms of $n$, $\\sigma^2$, $\\beta$, and $k$. No numerical evaluation is required. Provide only the final expression. Do not include any units.\n\nAnswer format requirement: Your final answer must be a single closed-form analytic expression. If you perform any approximation, state none; provide the exact expression. Do not round your answer.",
            "solution": "The problem asks for the optimal tuning parameter $\\lambda$ that minimizes the pointwise mean squared error (MSE) of a penalized least squares estimator. We will solve this in three parts as requested. First, we derive general expressions for the pointwise bias and variance of the estimator. Second, we specialize these expressions to the given signal and evaluation point, and compute the MSE. Third, we minimize this MSE with respect to $\\lambda$.\n\nThe estimator $\\hat{f}(x)$ has the form $f(x) = \\theta \\varphi(x)$, where $\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)$. The parameter $\\hat{\\theta}$ is found by minimizing the objective function:\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\big(y_i - \\theta \\varphi(x_i)\\big)^2 \\;+\\; \\lambda \\int_0^1 \\big((\\theta\\varphi(x))''\\big)^2 \\, dx\n$$\nWe can rewrite the penalty term. Let $r := \\int_0^1 \\big(\\varphi''(x)\\big)^2 \\, dx$. The objective function becomes:\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\big(y_i - \\theta \\varphi(x_i)\\big)^2 \\;+\\; \\lambda \\theta^2 r\n$$\nThis is a quadratic function of $\\theta$. To find the minimum, we differentiate with respect to $\\theta$ and set the result to zero:\n$$\n\\frac{dL}{d\\theta} = \\frac{1}{n} \\sum_{i=1}^n 2\\big(y_i - \\theta \\varphi(x_i)\\big)(-\\varphi(x_i)) \\;+\\; 2 \\lambda \\theta r = 0\n$$\n$$\n-\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) + \\frac{\\theta}{n} \\sum_{i=1}^n \\varphi(x_i)^2 + \\lambda \\theta r = 0\n$$\nWe are given the condition $\\frac{1}{n} \\sum_{i=1}^n \\varphi(x_i)^2 = 1$. Substituting this into the equation gives:\n$$\n-\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) + \\theta + \\lambda \\theta r = 0\n$$\nSolving for $\\hat{\\theta}$, the estimator for $\\theta$:\n$$\n\\hat{\\theta}(1 + \\lambda r) = \\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) \\implies \\hat{\\theta} = \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r}\n$$\nThe estimator for the function at a point $x_0$ is $\\hat{f}(x_0) = \\hat{\\theta} \\varphi(x_0)$.\n\n**1) Pointwise Bias and Variance**\n\nThe pointwise bias of the estimator $\\hat{f}(x_0)$ is defined as $\\text{Bias}(\\hat{f}(x_0)) = \\mathbb{E}[\\hat{f}(x_0)] - f_0(x_0)$. The expectation is taken with respect to the noise distribution. We use $\\mathbb{E}[y_i] = \\mathbb{E}[f_0(x_i) + \\varepsilon_i] = f_0(x_i)$ since $\\mathbb{E}[\\varepsilon_i] = 0$.\n$$\n\\mathbb{E}[\\hat{f}(x_0)] = \\mathbb{E}\\left[ \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r} \\varphi(x_0) \\right] = \\frac{\\varphi(x_0)}{1 + \\lambda r} \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}[y_i] \\varphi(x_i) = \\frac{\\varphi(x_0)}{1 + \\lambda r} \\frac{1}{n} \\sum_{i=1}^n f_0(x_i) \\varphi(x_i)\n$$\nLet's define the discrete inner product $\\langle g, h \\rangle_n = \\frac{1}{n} \\sum_{i=1}^n g(x_i)h(x_i)$. Then $\\mathbb{E}[\\hat{f}(x_0)] = \\frac{\\langle f_0, \\varphi \\rangle_n}{1 + \\lambda r} \\varphi(x_0)$.\nThe bias is:\n$$\n\\text{Bias}(\\hat{f}(x_0)) = \\frac{\\langle f_0, \\varphi \\rangle_n}{1 + \\lambda r} \\varphi(x_0) - f_0(x_0)\n$$\nThe pointwise variance is $\\text{Var}(\\hat{f}(x_0)) = \\text{Var}(\\hat{\\theta} \\varphi(x_0)) = \\varphi(x_0)^2 \\text{Var}(\\hat{\\theta})$. The noise variables $\\varepsilon_i$ are independent, so the $y_i$ are independent with $\\text{Var}(y_i) = \\sigma^2$.\n$$\n\\text{Var}(\\hat{\\theta}) = \\text{Var}\\left( \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r} \\right) = \\frac{1}{(1 + \\lambda r)^2} \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)\\right)\n$$\n$$\n\\text{Var}(\\hat{\\theta}) = \\frac{1}{n^2 (1 + \\lambda r)^2} \\sum_{i=1}^n \\varphi(x_i)^2 \\text{Var}(y_i) = \\frac{\\sigma^2}{n^2 (1 + \\lambda r)^2} \\sum_{i=1}^n \\varphi(x_i)^2\n$$\nUsing the given condition $\\sum_{i=1}^n \\varphi(x_i)^2 = n$, we get:\n$$\n\\text{Var}(\\hat{\\theta}) = \\frac{n \\sigma^2}{n^2 (1 + \\lambda r)^2} = \\frac{\\sigma^2}{n (1 + \\lambda r)^2}\n$$\nSo, the pointwise variance of the estimator is:\n$$\n\\text{Var}(\\hat{f}(x_0)) = \\varphi(x_0)^2 \\text{Var}(\\hat{\\theta}) = \\frac{\\sigma^2 \\varphi(x_0)^2}{n (1 + \\lambda r)^2}\n$$\n\n**2) Mean Squared Error for the Specific Case**\n\nFirst, we compute the curvature weight $r$:\n$$\n\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)\n$$\n$$\n\\varphi'(x) = \\sqrt{2}\\,(2\\pi k)\\,\\cos(2\\pi k x)\n$$\n$$\n\\varphi''(x) = -\\sqrt{2}\\,(2\\pi k)^2\\,\\sin(2\\pi k x)\n$$\n$$\nr = \\int_0^1 \\big(\\varphi''(x)\\big)^2 \\, dx = \\int_0^1 \\left(-\\sqrt{2}\\,(2\\pi k)^2\\,\\sin(2\\pi k x)\\right)^2 \\, dx = 2(2\\pi k)^4 \\int_0^1 \\sin^2(2\\pi k x) \\, dx\n$$\nUsing the identity $\\sin^2(\\alpha) = \\frac{1 - \\cos(2\\alpha)}{2}$ and that $\\int_0^1 \\cos(4\\pi kx)dx=0$:\n$$\nr = 2(16\\pi^4 k^4) \\int_0^1 \\frac{1}{2} \\, dx = 16\\pi^4 k^4\n$$\nNow, we specialize to the signal $f_0(x) = \\beta \\sin(2\\pi k x) = \\frac{\\beta}{\\sqrt{2}}\\varphi(x)$. The discrete inner product is:\n$$\n\\langle f_0, \\varphi \\rangle_n = \\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{\\beta}{\\sqrt{2}}\\varphi(x_i)\\right) \\varphi(x_i) = \\frac{\\beta}{\\sqrt{2}} \\left( \\frac{1}{n} \\sum_{i=1}^n \\varphi(x_i)^2 \\right) = \\frac{\\beta}{\\sqrt{2}} \\cdot 1 = \\frac{\\beta}{\\sqrt{2}}\n$$\nWe evaluate at $x_0$ such that $\\sin(2\\pi k x_0) = 1$. At this point, $\\varphi(x_0) = \\sqrt{2}\\sin(2\\pi k x_0) = \\sqrt{2}$ and $f_0(x_0) = \\beta\\sin(2\\pi k x_0) = \\beta$.\nSubstituting these into the bias and variance formulas:\n$$\n\\text{Bias}(\\hat{f}(x_0)) = \\frac{\\beta/\\sqrt{2}}{1 + \\lambda r} (\\sqrt{2}) - \\beta = \\frac{\\beta}{1 + \\lambda r} - \\beta = \\frac{\\beta - \\beta(1+\\lambda r)}{1 + \\lambda r} = -\\frac{\\beta \\lambda r}{1 + \\lambda r}\n$$\n$$\n\\text{Var}(\\hat{f}(x_0)) = \\frac{\\sigma^2 (\\sqrt{2})^2}{n (1 + \\lambda r)^2} = \\frac{2\\sigma^2}{n (1 + \\lambda r)^2}\n$$\nThe pointwise mean squared error is $\\text{MSE}(\\hat{f}(x_0)) = (\\text{Bias}(\\hat{f}(x_0)))^2 + \\text{Var}(\\hat{f}(x_0))$:\n$$\n\\text{MSE}(\\lambda) = \\left(-\\frac{\\beta \\lambda r}{1 + \\lambda r}\\right)^2 + \\frac{2\\sigma^2}{n (1 + \\lambda r)^2} = \\frac{\\beta^2 \\lambda^2 r^2}{(1 + \\lambda r)^2} + \\frac{2\\sigma^2}{n (1 + \\lambda r)^2} = \\frac{n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2}{n(1 + \\lambda r)^2}\n$$\n\n**3) Optimal Value of $\\lambda$**\n\nTo find the value of $\\lambda$ that minimizes the MSE, we differentiate $\\text{MSE}(\\lambda)$ with respect to $\\lambda$ and set it to zero.\n$$\n\\frac{d}{d\\lambda}\\text{MSE}(\\lambda) = \\frac{d}{d\\lambda} \\left[ \\frac{n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2}{n(1 + \\lambda r)^2} \\right]\n$$\nUsing the quotient rule, and noting that the factor $1/n$ is a constant:\n$$\n\\frac{d}{d\\lambda}\\text{MSE}(\\lambda) = \\frac{1}{n} \\frac{(2n\\beta^2 \\lambda r^2)(1+\\lambda r)^2 - (n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2)(2(1+\\lambda r)r)}{(1+\\lambda r)^4} = 0\n$$\nFor $\\lambda \\ge 0$, we can divide by $2(1+\\lambda r) \\neq 0$:\n$$\n(n\\beta^2 \\lambda r^2)(1+\\lambda r) - r(n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2) = 0\n$$\n$$\nn\\beta^2 \\lambda r^2 + n\\beta^2 \\lambda^2 r^3 - n\\beta^2 \\lambda^2 r^3 - 2\\sigma^2 r = 0\n$$\n$$\nn\\beta^2 \\lambda r^2 - 2\\sigma^2 r = 0\n$$\nAssuming $r \\neq 0$ (which is true as $k$ is a positive integer) and $\\beta \\neq 0$:\n$$\nn\\beta^2 \\lambda r = 2\\sigma^2 \\implies \\lambda = \\frac{2\\sigma^2}{n\\beta^2 r}\n$$\nThe second derivative test confirms this is a minimum. Substituting the value of $r = 16\\pi^4 k^4$:\n$$\n\\lambda = \\frac{2\\sigma^2}{n\\beta^2 (16\\pi^4 k^4)} = \\frac{\\sigma^2}{8 n \\beta^2 \\pi^4 k^4}\n$$\nThis is the optimal value of the tuning parameter $\\lambda$.",
            "answer": "$$\\boxed{\\frac{\\sigma^2}{8 n \\beta^2 \\pi^4 k^4}}$$"
        }
    ]
}