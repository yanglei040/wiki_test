{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of exploratory data analysis is to characterize the relationship between variables. While a simple Pearson correlation coefficient is a go-to metric, it only captures the strength of a *linear* association. This practice challenges you to think more deeply by comparing the Pearson correlation to the Spearman rank correlation, a powerful technique for diagnosing monotonic, but nonlinear, relationships and making more informed modeling choices .",
            "id": "3120045",
            "problem": "A data analyst is performing Exploratory Data Analysis (EDA) for a regression task with a continuously measured predictor $X$ and response $Y$. The dataset contains $n=320$ observations $\\{(x_i,y_i)\\}_{i=1}^{n}$. The analyst notes that $X$ is right-skewed (sample skewness approximately $1.1$) and $Y$ is nonnegative. The sample Pearson correlation between $X$ and $Y$ is $0.43$, while the sample Spearman rank correlation is $0.87$. A boxplot-based Interquartile Range (IQR) check flags $5$ mild high-end $x_i$ outliers. After trimming the largest $1\\%$ of $x_i$ values, the correlations remain approximately the same: Pearson $0.42$ and Spearman $0.86$. No other data quality issues are detected.\n\nWhich of the following is the most plausible diagnostic conclusion about the relationship between $X$ and $Y$, and which modeling step is most appropriate as a next action for predicting $Y$ from $X$?\n\nA. The relationship is approximately linear with homoscedastic noise; the Pearson–Spearman discrepancy is due to sampling variability. Proceed with ordinary least squares (OLS) without transformation.\n\nB. The relationship is monotonic but nonlinear (for example, concave increasing). The high rank correlation with moderate Pearson suggests using a monotonic transformation of $X$ (such as $\\log$) or a monotonic model (for example, isotonic regression or a shape-constrained spline).\n\nC. $X$ and $Y$ are independent; the high Spearman rank correlation is an artifact of skewness. Avoid using $X$ in the model.\n\nD. The association is driven by a few extreme outliers; winsorize $X$ and fit a linear OLS model on the winsorized data.\n\nE. The relationship is non-monotonic (for example, U-shaped), so a high-degree polynomial (for example, degree $5$) is preferred; the Spearman value being high is consistent with such non-monotonicity under skewness.",
            "solution": "### Derivation and Option Analysis\nThe core of the problem lies in interpreting the collection of statistical evidence, primarily the stark difference between the Pearson correlation coefficient ($r_{XY}=0.43$) and the Spearman rank correlation coefficient ($\\rho_{XY}=0.87$).\n\n**Principle-Based Derivation**\n\n1.  **Interpreting Spearman Correlation:** The Spearman correlation coefficient, $\\rho_s$, measures the strength and direction of a monotonic relationship. It is calculated as the Pearson correlation coefficient of the ranked variables. A value of $\\rho_{XY} = 0.87$ is very close to $1$, indicating a strong positive monotonic relationship. This means that as $X$ increases, $Y$ consistently tends to increase (or at least not decrease).\n\n2.  **Interpreting Pearson Correlation:** The Pearson correlation coefficient, $r$, measures the strength and direction of a *linear* relationship. A value of $r_{XY} = 0.43$ indicates a moderate positive linear relationship.\n\n3.  **Reconciling the Discrepancy:** The significant discrepancy ($\\rho_{XY} \\gg r_{XY}$) is the most informative clue. It implies that the relationship between $X$ and $Y$ is strong and monotonic, but not linear. If the relationship were linear, the Pearson and Spearman correlations would be very close in value. A classic example of a relationship producing this signature is a curve, for instance, a logarithmic or power-law relationship, which is monotonic but not linear. A scatterplot of such data would show a clear trend following a curve, for which a straight line is a poor fit, resulting in a modest $r$ value. However, the ranks would still be highly correlated, resulting in a high $\\rho_s$ value.\n\n4.  **Analyzing the Role of Outliers:** Outliers can have a strong influence on the Pearson correlation while leaving the Spearman correlation relatively unaffected. This could be a potential explanation for the discrepancy. However, the problem provides a direct test of this hypothesis: trimming the largest $1\\%$ of $x_i$ values (which includes the most extreme values in the right-skewed distribution) has a negligible effect on the correlations ($r$ changes from $0.43$ to $0.42$, $\\rho_s$ from $0.87$ to $0.86$). This result robustly demonstrates that the low Pearson correlation is *not* an artifact of a few influential outliers. The cause must be the intrinsic non-linear structure of the relationship across the bulk of the data.\n\n5.  **Synthesizing a Diagnosis and Next Step:** The evidence overwhelmingly points to a strong, monotonic, non-linear increasing relationship between $X$ and $Y$. The right-skewness of $X$ and non-negativity of $Y$ are consistent with a concave relationship (e.g., $Y \\propto \\log(X)$) or a convex one (e.g., $Y \\propto \\exp(X)$). Given this diagnosis, a simple linear regression of $Y$ on $X$ is inappropriate as it would fail to capture the curvature, leading to biased predictions and incorrect inferences. The appropriate next step is to either:\n    a. Apply a monotonic transformation to one or both variables to linearize the relationship. Given that $X$ is right-skewed, a transformation such as $\\log(X)$ or $\\sqrt{X}$ is a standard and promising approach. One would then fit a linear model to the transformed data.\n    b. Use a model that can intrinsically capture a monotonic, non-linear relationship without transformation. Examples include isotonic regression or a Generalized Additive Model (GAM) with a shape-constrained spline (specifically, a monotonically increasing spline).\n\n**Option-by-Option Analysis**\n\n**A. The relationship is approximately linear with homoscedastic noise; the Pearson–Spearman discrepancy is due to sampling variability. Proceed with ordinary least squares (OLS) without transformation.**\n*   **Analysis:** This assertion is incorrect. The discrepancy between $r_{XY}=0.43$ and $\\rho_{XY}=0.87$ is far too large to be attributed to sampling variability, especially with a sample size of $n=320$. The high Spearman correlation directly contradicts the claim that the relationship is \"approximately linear.\" Proceeding with OLS would be a modeling error.\n*   **Verdict:** Incorrect.\n\n**B. The relationship is monotonic but nonlinear (for example, concave increasing). The high rank correlation with moderate Pearson suggests using a monotonic transformation of $X$ (such as $\\log$) or a monotonic model (for example, isotonic regression or a shape-constrained spline).**\n*   **Analysis:** This option aligns perfectly with our derivation. It correctly diagnoses the relationship as \"monotonic but nonlinear\" based on the correlation discrepancy. The example of a \"concave increasing\" relationship is a plausible scenario. Most importantly, it proposes the two most appropriate classes of modeling strategies: applying a linearizing monotonic transformation (like $\\log$) or using a flexible monotonic model (like isotonic regression or splines). This is the best course of action.\n*   **Verdict:** Correct.\n\n**C. $X$ and $Y$ are independent; the high Spearman rank correlation is an artifact of skewness. Avoid using $X$ in the model.**\n*   **Analysis:** This statement is fundamentally wrong. A Spearman correlation of $0.87$ provides extremely strong evidence *against* independence. Skewness itself does not create a strong monotonic relationship where none exists. Discarding $X$ as a predictor would mean ignoring a very strong, albeit non-linear, signal.\n*   **Verdict:** Incorrect.\n\n**D. The association is driven by a few extreme outliers; winsorize $X$ and fit a linear OLS model on the winsorized data.**\n*   **Analysis:** This hypothesis is directly refuted by the evidence in the problem statement. The experiment of trimming the top $1\\%$ of $x_i$ values showed that the correlations were stable. This proves the association is not driven by these few extreme values. Therefore, the premise for this action is false.\n*   **Verdict:** Incorrect.\n\n**E. The relationship is non-monotonic (for example, U-shaped), so a high-degree polynomial (for example, degree $5$) is preferred; the Spearman value being high is consistent with such non-monotonicity under skewness.**\n*   **Analysis:** This is incorrect. A high Spearman correlation is the primary indicator of a *monotonic* relationship. A non-monotonic relationship like a U-shape would produce a Spearman correlation close to $0$, because the ranks of $Y$ would first decrease and then increase as the ranks of $X$ increase, breaking any monotonic trend. The claim that high $\\rho_s$ is consistent with non-monotonicity is false. Consequently, modeling with a complex polynomial is not indicated by the data.\n*   **Verdict:** Incorrect.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "When building models with multiple features, the scale and distribution of each predictor can have a profound impact, especially for regularized methods like LASSO and Ridge regression. This exercise demonstrates how performing EDA on your predictors is a critical and non-negotiable step before modeling. By examining the vast differences in scale across variables, you will learn to justify a proper preprocessing plan to ensure your model treats each feature fairly and performs optimally .",
            "id": "3120036",
            "problem": "A company has collected a dataset with $n=500$ customers, a continuous response variable $Y$ (annual spending), and four predictors $X_1, X_2, X_3, X_4$. An exploratory data analysis (EDA) was performed by plotting histograms, kernel density estimates, and box plots for each $X_j$ and computing their empirical ranges and spread. The findings are summarized as follows.\n\n$X_1$ (annual income in dollars): approximately uniform on $[2\\times 10^4, 2\\times 10^5]$, sample mean $\\approx 8.0\\times 10^4$, sample standard deviation $\\approx 3.0\\times 10^4$.\n\n$X_2$ (click-through rate as a proportion): concentrated near $0$ with a long right tail on $[0,1]$, sample mean $\\approx 8\\times 10^{-2}$, sample standard deviation $\\approx 5\\times 10^{-2}$.\n\n$X_3$ (number of purchases per year): highly skewed right with several outliers near $200$, empirical range $[0, 200]$, sample mean $\\approx 12$, sample standard deviation $\\approx 20$.\n\n$X_4$ (survey satisfaction score on a $10$-point scale): approximately symmetric on $[0,10]$, sample mean $\\approx 6.5$, sample standard deviation $\\approx 1.5$.\n\nYou plan to fit linear models with regularization to predict $Y$ from $(X_1, X_2, X_3, X_4)$ using ridge regression and the least absolute shrinkage and selection operator (LASSO). Your colleague suggests “skip standardization because regularization will handle everything.” Based solely on the EDA, reason from the definitions of these methods and the observed scales and distributions to predict how the lack of standardization would differentially affect ridge versus LASSO, and propose a preprocessing plan that is justified by the EDA.\n\nWhich option most accurately states the expected impact of omitting standardization and gives an EDA-informed preprocessing plan?\n\nA. Without standardization, LASSO will tend to favor predictors measured on larger scales (for example $X_1$) because their coefficients can be smaller in magnitude for the same predictive contribution, reducing the $\\ell_1$ penalty relative to small-scale features (for example $X_2$). Ridge will also be sensitive to scale but will shrink coefficients continuously rather than setting them exactly to zero, producing uneven shrinkage across features. A justified preprocessing plan is to center and scale all predictors to zero mean and unit variance, and additionally apply a robust transformation or scaling to handle skewness and outliers (for example, log or a median–interquartile-range scaling for $X_3$), while inspecting $X_2$ for bounded-proportion considerations.\n\nB. Ridge regression is invariant to feature scaling because it penalizes squared residuals, but LASSO is harmed only by multicollinearity. Therefore, do not standardize; instead, drop outliers from $X_3$ and leave the rest unchanged.\n\nC. Both ridge and LASSO are invariant to the measurement units of the predictors, so omitting standardization has no effect on either method. The best preprocessing is to convert $X_1$ from dollars to thousands of dollars and to leave $X_2, X_3, X_4$ as-is.\n\nD. Without standardization, LASSO will more often zero-out large-scale predictors (for example $X_1$) because their coefficients are larger and incur more penalty, whereas small-scale predictors (for example $X_2$) are protected. Ridge is unaffected by scale. The best preprocessing is min–max normalization of only $X_2$ to $[0,1]$ since it is already bounded.",
            "solution": "### Principle-Based Derivation\nThe problem asks to evaluate the effect of omitting feature standardization when using regularized linear models like Ridge and LASSO regression.\n\n1.  **Definitions of Ridge and LASSO:** Both methods penalize the size of the model coefficients $\\beta_j$.\n    *   Ridge regression adds a penalty proportional to $\\sum \\beta_j^2$ (the $\\ell_2$ norm).\n    *   LASSO adds a penalty proportional to $\\sum |\\beta_j|$ (the $\\ell_1$ norm).\n    The goal is to prevent overfitting by shrinking coefficients towards zero.\n\n2.  **Impact of Feature Scale:** The magnitude of a coefficient $\\beta_j$ is not independent of the scale of its corresponding predictor $X_j$. If a predictor $X_j$ is measured on a large scale (e.g., annual income, $X_1$, with $\\sigma_1 \\approx 30,000$), its coefficient $\\beta_1$ will naturally be small to produce a given effect on the response $Y$. Conversely, if a predictor $X_k$ is on a small scale (e.g., click-through rate, $X_2$, with $\\sigma_2 \\approx 0.05$), its coefficient $\\beta_2$ must be large to produce the same effect.\n\n3.  **Interaction with Regularization:** Without standardization, the penalty is applied unfairly. The large coefficient $\\beta_2$ required for the small-scale predictor $X_2$ will incur a very large penalty, causing it to be shrunk aggressively (and potentially set to zero by LASSO). The small coefficient $\\beta_1$ for the large-scale predictor $X_1$ will incur a small penalty and be shrunk much less. This means the model's behavior—which variables are selected and how much they are shrunk—becomes dependent on the arbitrary choice of units, which is statistically unsound. The colleague's suggestion is therefore incorrect.\n\n4.  **EDA-Informed Preprocessing Plan:** The EDA reveals vast differences in scale: $\\sigma(X_1) \\approx 3 \\times 10^4$, $\\sigma(X_2) \\approx 5 \\times 10^{-2}$, $\\sigma(X_3) \\approx 20$, $\\sigma(X_4) \\approx 1.5$. A proper plan must address this:\n    *   **Standardization:** All predictors must be brought to a common scale. Standardizing to zero mean and unit variance (Z-score scaling) is the standard procedure.\n    *   **Skewness and Outliers:** $X_3$ is highly right-skewed with outliers. Standard scaling is sensitive to this. A more robust approach is justified, such as applying a log transformation first, or using robust scaling based on the median and Interquartile Range (IQR).\n\n### Option-by-Option Analysis\n\n**A. Without standardization, LASSO will tend to favor predictors measured on larger scales (for example $X_1$) because their coefficients can be smaller in magnitude for the same predictive contribution, reducing the $\\ell_1$ penalty relative to small-scale features (for example $X_2$). Ridge will also be sensitive to scale but will shrink coefficients continuously rather than setting them exactly to zero, producing uneven shrinkage across features. A justified preprocessing plan is to center and scale all predictors to zero mean and unit variance, and additionally apply a robust transformation or scaling to handle skewness and outliers (for example, log or a median–interquartile-range scaling for $X_3$), while inspecting $X_2$ for bounded-proportion considerations.**\n*   **Analysis:** This option correctly describes that large-scale predictors are favored (less penalized) and that both Ridge and LASSO are sensitive to scale. The proposed preprocessing plan is comprehensive and well-justified by the EDA, including standard scaling for all and robust handling for the skewed predictor $X_3$.\n*   **Verdict:** Correct.\n\n**B. Ridge regression is invariant to feature scaling because it penalizes squared residuals, but LASSO is harmed only by multicollinearity. Therefore, do not standardize; instead, drop outliers from $X_3$ and leave the rest unchanged.**\n*   **Analysis:** This is fundamentally incorrect. Ridge's $\\ell_2$ penalty is highly sensitive to coefficient magnitudes and thus to feature scaling.\n*   **Verdict:** Incorrect.\n\n**C. Both ridge and LASSO are invariant to the measurement units of the predictors, so omitting standardization has no effect on either method. The best preprocessing is to convert $X_1$ from dollars to thousands of dollars and to leave $X_2, X_3, X_4$ as-is.**\n*   **Analysis:** This is fundamentally incorrect. Both methods are scale-dependent.\n*   **Verdict:** Incorrect.\n\n**D. Without standardization, LASSO will more often zero-out large-scale predictors (for example $X_1$) because their coefficients are larger and incur more penalty, whereas small-scale predictors (for example $X_2$) are protected. Ridge is unaffected by scale. The best preprocessing is min–max normalization of only $X_2$ to $[0,1]$ since it is already bounded.**\n*   **Analysis:** This has the logic reversed. Large-scale predictors have smaller coefficients and are penalized *less*, not more. The claim that Ridge is unaffected by scale is also false.\n*   **Verdict:** Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While much of EDA focuses on predictors, the characteristics of the response variable, $Y$, are equally important for building reliable models, particularly for inference. Skewed response variables can violate the assumptions of common models like ordinary least squares regression. This practice guides you through the process of selecting and evaluating transformations (like the logarithm or Box-Cox) to make the response distribution more symmetric, which in turn can help stabilize variance and normalize model residuals .",
            "id": "3120037",
            "problem": "A data analyst is performing exploratory data analysis on a strictly positive response variable $Y$ (for example, service times in seconds) with sample size $n=80$. The analyst suspects right-skewness and considers monotone transformations to stabilize variance and move the marginal distribution closer to Gaussian for downstream modeling. Three transformations are evaluated: the natural logarithm $\\log(Y)$, the square-root $\\sqrt{Y}$, and the Box–Cox family $T_{\\lambda}(Y)=(Y^{\\lambda}-1)/\\lambda$ for $\\lambda \\neq 0$ and $T_{0}(Y)=\\log(Y)$, where $\\lambda$ is estimated by maximum likelihood under the working assumption that $T_{\\lambda}(Y)$ is approximately Gaussian.\n\nTo compare how well each transformed variable aligns with a Gaussian reference, the analyst uses the following empirical summaries against the standard normal quantile–quantile plot: the Pearson correlation between ordered transformed values and corresponding theoretical normal quantiles (denoted $r_{\\mathrm{QQ}}$), the Shapiro–Wilk $p$-value for normality, and the sample skewness. The results are:\n\n- Raw $Y$: $r_{\\mathrm{QQ}}=0.955$, Shapiro–Wilk $p0.001$, skewness $=2.4$.\n- $\\log(Y)$: $r_{\\mathrm{QQ}}=0.994$, Shapiro–Wilk $p=0.12$, skewness $=0.15$.\n- $\\sqrt{Y}$: $r_{\\mathrm{QQ}}=0.987$, Shapiro–Wilk $p=0.03$, skewness $=0.50$.\n- Box–Cox with $\\hat{\\lambda}=0.15$: $r_{\\mathrm{QQ}}=0.996$, Shapiro–Wilk $p=0.21$, skewness $=0.05$.\n\nThe intended learning algorithm is ordinary least squares linear regression to predict $Y$ from a set of predictors $X$, with goals that include accurate point predictions and valid small-sample interval estimates. The analyst is also considering alternatives such as tree-based regression and regression methods with robust losses.\n\nWhich of the following statements are most justified by the evidence and by the principles underlying the methods?\n\nA. Among the considered transformations, the Box–Cox transform with $\\hat{\\lambda}=0.15$ best achieves approximate marginal normality of $Y$ by the given diagnostics; for ordinary least squares linear regression, exact normality of $Y$ is not required for unbiased point estimates of coefficients, but normality and homoscedasticity of residuals matter for small-sample inference, so transforming $Y$ can be useful depending on the inferential goal.\n\nB. The logarithm is preferable because it yields the largest Shapiro–Wilk $p$-value, and normality of $Y$ is mandatory for ordinary least squares to be consistent.\n\nC. Because random forest regression is invariant to monotone transformations of the response, transforming $Y$ cannot change predictive performance, so no transformation should be applied.\n\nD. Given right-skewness, the square-root is the only transformation that reduces skewness without changing the mean–variance relationship; Box–Cox is unsuitable when $\\hat{\\lambda}$ is not an integer.\n\nE. If the goal includes well-calibrated small-sample prediction or confidence intervals with linear regression, achieving approximately normal and homoscedastic residuals is valuable; starting with the Box–Cox transform that best normalizes the marginal distribution is reasonable, but one must still check residual diagnostics post-fit, and if violations persist, consider heteroskedasticity-robust standard errors, Huber loss regression, or quantile regression.\n\nF. For $k$-nearest neighbors regression, normality of $Y$ is crucial because distance computations depend on $Y$, so transforming $Y$ is necessary before fitting $k$-nearest neighbors.",
            "solution": "### Analysis of Diagnostic Results\nThe problem provides diagnostic metrics for different transformations of a right-skewed response variable $Y$. The goal is to make the distribution more Gaussian.\n- **Raw Y**: Highly skewed (2.4), fails normality test ($p0.001$).\n- **$\\sqrt{Y}$**: Improves skewness (0.50), but still fails normality test ($p=0.03$).\n- **$\\log(Y)$**: Greatly improves skewness (0.15) and passes normality test ($p=0.12$).\n- **Box-Cox ($\\hat{\\lambda}=0.15$)**: Performs best on all metrics: lowest skewness (0.05), highest Q-Q plot correlation (0.996), and highest Shapiro-Wilk p-value (0.21).\n\nThe Box-Cox transformation is the most successful at normalizing the *marginal* distribution of $Y$.\n\n### Evaluation of Statements\n\n**A. Among the considered transformations, the Box–Cox transform with $\\hat{\\lambda}=0.15$ best achieves approximate marginal normality of $Y$ by the given diagnostics; for ordinary least squares linear regression, exact normality of $Y$ is not required for unbiased point estimates of coefficients, but normality and homoscedasticity of residuals matter for small-sample inference, so transforming $Y$ can be useful depending on the inferential goal.**\n*   **Analysis:** The first clause is supported by the data. The second and third clauses are fundamental and correct principles of OLS regression. The entire statement is accurate and well-reasoned.\n*   **Verdict:** Correct.\n\n**B. The logarithm is preferable because it yields the largest Shapiro–Wilk $p$-value, and normality of $Y$ is mandatory for ordinary least squares to be consistent.**\n*   **Analysis:** This contains a factual error (Box-Cox has a larger p-value) and a theoretical error (normality is not required for OLS consistency).\n*   **Verdict:** Incorrect.\n\n**C. Because random forest regression is invariant to monotone transformations of the response, transforming $Y$ cannot change predictive performance, so no transformation should be applied.**\n*   **Analysis:** While RF is largely invariant to monotonic response transformations, the conclusion is a *non sequitur*. The primary model is OLS, and even for RF, performance metrics on the original scale can be affected by back-transformation bias.\n*   **Verdict:** Incorrect.\n\n**D. Given right-skewness, the square-root is the only transformation that reduces skewness without changing the mean–variance relationship; Box–Cox is unsuitable when $\\hat{\\lambda}$ is not an integer.**\n*   **Analysis:** This contains multiple theoretical fallacies. Other transforms also reduce skewness, variance stabilization is about *changing* the mean-variance relationship, and the Box-Cox parameter $\\lambda$ is a real number, not restricted to integers.\n*   **Verdict:** Incorrect.\n\n**E. If the goal includes well-calibrated small-sample prediction or confidence intervals with linear regression, achieving approximately normal and homoscedastic residuals is valuable; starting with the Box–Cox transform that best normalizes the marginal distribution is reasonable, but one must still check residual diagnostics post-fit, and if violations persist, consider heteroskedasticity-robust standard errors, Huber loss regression, or quantile regression.**\n*   **Analysis:** This statement describes a complete, professionally sound statistical workflow. It correctly identifies the importance of residuals, frames the marginal transformation as a reasonable starting heuristic, emphasizes the non-negotiable step of checking residuals post-fit, and lists the correct alternative methods if assumptions are still violated. This is a comprehensive and accurate statement.\n*   **Verdict:** Correct.\n\n**F. For $k$-nearest neighbors regression, normality of $Y$ is crucial because distance computations depend on $Y$, so transforming $Y$ is necessary before fitting $k$-nearest neighbors.**\n*   **Analysis:** This fundamentally misunderstands k-NN regression. Distances are computed in the predictor space ($X$), not using the response variable ($Y$). k-NN makes no distributional assumptions on $Y$.\n*   **Verdict:** Incorrect.",
            "answer": "$$\\boxed{AE}$$"
        }
    ]
}