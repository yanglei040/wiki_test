{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering the bias-variance trade-off is to develop a concrete intuition for how model complexity affects performance. This exercise provides a foundational, hands-on experience in generating and identifying underfitting, good fits, and overfitting. By implementing polynomial regression on synthetic data, you will not only observe the classic patterns in training and validation error but also learn to quantify the \"wiggliness\" of an overfit model by analyzing its residuals in the frequency domain .",
            "id": "3135788",
            "problem": "You are given a supervised learning setting for one-dimensional regression modeled in terms of Empirical Risk Minimization (ERM). Let the input be $x \\in [-1,1]$ and the ground-truth target function be a polynomial of known degree $d^\\star$, contaminated by zero-mean Gaussian noise with variance $\\sigma^2$. Specifically, let the data be generated as $y = f^\\star(x) + \\varepsilon$, where $f^\\star(x) = \\sum_{k=0}^{d^\\star} a_k x^k$, $d^\\star = 4$, the coefficients are $a_0 = 0.3$, $a_1 = -0.8$, $a_2 = 0.5$, $a_3 = 0.0$, $a_4 = 0.7$, and the noise is $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 0.1$ (so $\\sigma^2 = 0.01$). The dataset size is $N = 200$ points with inputs $x$ drawn uniformly from $[-1,1]$. Use $N_{\\text{train}} = 120$ samples for training and $N_{\\text{valid}} = 80$ samples for validation. Use a fixed random seed, $42$, to ensure reproducibility.\n\nYour task is to implement polynomial regression with ridge regularization (also known as $\\ell_2$ regularization). For a chosen model degree $d$ and a regularization strength $\\lambda \\ge 0$, construct the design matrix $\\Phi \\in \\mathbb{R}^{m \\times (d+1)}$ with entries $\\Phi_{i,k} = x_i^k$. Given training data $(\\Phi_{\\text{train}}, y_{\\text{train}})$, compute the ridge estimator coefficients $w \\in \\mathbb{R}^{d+1}$ using the closed-form solution\n$$\nw = \\left(\\Phi_{\\text{train}}^\\top \\Phi_{\\text{train}} + \\lambda I\\right)^{-1} \\Phi_{\\text{train}}^\\top y_{\\text{train}},\n$$\nwhere $I$ is the $(d+1) \\times (d+1)$ identity matrix. Use this estimator to obtain predictions on training and validation sets, and compute the residuals $r_{\\text{train}} = y_{\\text{train}} - \\hat{y}_{\\text{train}}$ and $r_{\\text{valid}} = y_{\\text{valid}} - \\hat{y}_{\\text{valid}}$.\n\nFrom first principles, identify underfitting and overfitting using the following definitions and measurements:\n\n- Mean Squared Error (MSE) is defined as $ \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2 $. Let $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{valid}}$ denote the training and validation mean squared errors, respectively.\n- Oscillation in residuals is quantified in the frequency domain. Compute the Discrete Fourier Transform (DFT) of the validation residual sequence sorted by its corresponding inputs $x$ in ascending order. Use the real-valued DFT $R = \\text{rfft}(r_{\\text{valid-sorted}})$ and define the high-frequency energy ratio as\n$$\n\\rho_{\\text{HF}} = \\frac{\\sum_{k \\in \\mathcal{H}} |R_k|^2}{\\sum_{k \\in \\mathcal{P}} |R_k|^2}\n$$,\nwhere $\\mathcal{P}$ indexes all positive-frequency bins excluding the zero-frequency bin and $\\mathcal{H}$ indexes the top quartile of positive-frequency bins (the highest-frequency $25\\%$ of $\\mathcal{P}$). If the denominator is zero, define $\\rho_{\\text{HF}} = 0$.\n\nUse the following classification rules with fixed thresholds to decide whether a model is underfitting, well-fit, or overfitting. Denote the known noise variance by $\\sigma^2 = 0.01$, and let the thresholds be $t_u = 1.3$, $t_o = 0.9$, $t_o' = 1.2$, $h_u = 0.35$, and $h_o = 0.45$.\n\n- Underfitting (code $0$): declare underfitting if either $d < d^\\star$ or if $\\text{MSE}_{\\text{train}} \\ge t_u \\sigma^2$ and $\\text{MSE}_{\\text{valid}} \\ge t_u \\sigma^2$ and $\\rho_{\\text{HF}} \\le h_u$.\n- Overfitting (code $2$): declare overfitting if $d > d^\\star$ and $\\text{MSE}_{\\text{train}} \\le t_o \\sigma^2$ and $\\text{MSE}_{\\text{valid}} \\ge t_o' \\sigma^2$ and $\\rho_{\\text{HF}} \\ge h_o$.\n- Well-fit (code $1$): if neither of the above conditions hold, declare well-fit.\n\nImplement the above and evaluate the following test suite that varies $d$ and $\\lambda$:\n\n- Case $1$: $d = 2$, $\\lambda = 0.001$.\n- Case $2$: $d = 4$, $\\lambda = 0.05$.\n- Case $3$: $d = 12$, $\\lambda = 0.0$.\n- Case $4$: $d = 12$, $\\lambda = 10.0$.\n- Case $5$: $d = 4$, $\\lambda = 0.0$.\n\nYour program must generate the dataset as specified, fit the model for each case, compute the metrics, and output the classification codes for the cases in the given order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0,1,2,0,1]$). No physical units, angle units, or percentages are required for this problem. The final output values are integers specified as above. The program must be complete and runnable as is, with no external inputs or files. Use a deterministic seed as specified so results are reproducible for anyone running the program.",
            "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically sound, self-contained, and well-posed, providing a clear and formalizable task in the domain of computational statistics and machine learning.\n\nThe task is to classify polynomial regression models as underfitting, well-fit, or overfitting based on a set of precise, quantitative criteria. The solution involves data generation, model fitting, metric computation, and classification for several test cases. The entire process is deterministic due to a specified random seed.\n\n### Step 1: Data Generation and Preparation\n\nThe foundation of this regression problem is a synthetic dataset. The ground-truth relationship between the input $x$ and the output $y$ is defined by a known polynomial function $f^\\star(x)$ of degree $d^\\star=4$:\n$$\nf^\\star(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4\n$$\nwith coefficients $a_0 = 0.3$, $a_1 = -0.8$, $a_2 = 0.5$, $a_3 = 0.0$, and $a_4 = 0.7$.\n\nThe observed data are corrupted by additive white Gaussian noise, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, where the noise variance is $\\sigma^2 = 0.01$. Thus, each data point $(x_i, y_i)$ is generated according to the model:\n$$\ny_i = f^\\star(x_i) + \\varepsilon_i\n$$\nA total of $N=200$ data points are created. The input values $x_i$ are drawn from a uniform distribution over the interval $[-1, 1]$. To ensure reproducibility, the random number generator is initialized with a fixed seed of $42$.\n\nThe generated dataset of $N=200$ points is then deterministically shuffled and split into a training set of size $N_{\\text{train}} = 120$ and a validation set of size $N_{\\text{valid}} = 80$. This partitioning allows us to train the model and independently evaluate its generalization performance.\n\n### Step 2: Polynomial Regression with Ridge Regularization\n\nFor each test case, we fit a polynomial model of a specified degree $d$ to the training data. The model hypothesis is of the form:\n$$\n\\hat{y}(x) = \\sum_{k=0}^d w_k x^k = \\mathbf{w}^\\top \\phi(x)\n$$\nwhere $\\mathbf{w} \\in \\mathbb{R}^{d+1}$ is the vector of model coefficients to be learned and $\\phi(x) = [1, x, x^2, \\dots, x^d]^\\top$ is the feature vector.\n\nFor a set of $m$ training samples, we construct the design matrix $\\Phi_{\\text{train}} \\in \\mathbb{R}^{m \\times (d+1)}$, where each entry is $(\\Phi_{\\text{train}})_{i,k} = x_i^k$ for $i \\in \\{1, \\dots, m\\}$ and $k \\in \\{0, \\dots, d\\}$.\n\nThe coefficients $\\mathbf{w}$ are estimated using ridge regression, which minimizes the regularized sum of squared errors:\n$$\n\\mathcal{L}(\\mathbf{w}) = \\|\\mathbf{y}_{\\text{train}} - \\Phi_{\\text{train}}\\mathbf{w}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2\n$$\nHere, $\\lambda \\ge 0$ is the regularization parameter that controls the penalty on the magnitude of the coefficients. The closed-form solution for the optimal weight vector $\\mathbf{w}$ is given by the normal equations:\n$$\n\\mathbf{w} = \\left(\\Phi_{\\text{train}}^\\top \\Phi_{\\text{train}} + \\lambda I\\right)^{-1} \\Phi_{\\text{train}}^\\top \\mathbf{y}_{\\text{train}}\n$$\nwhere $I$ is the $(d+1) \\times (d+1)$ identity matrix. For numerical stability, this linear system is solved using `numpy.linalg.solve` rather than by computing the matrix inverse explicitly.\n\n### Step 3: Model Evaluation Metrics\n\nOnce the model is trained (i.e., $\\mathbf{w}$ is computed), its performance is evaluated using two key metrics.\n\n**Mean Squared Error (MSE):** The MSE measures the average squared difference between the predicted values $\\hat{y}_i$ and the actual values $y_i$. It is computed for both the training and validation sets:\n$$\n\\text{MSE}_{\\text{train}} = \\frac{1}{N_{\\text{train}}} \\sum_{i=1}^{N_{\\text{train}}} (y_{\\text{train},i} - \\hat{y}_{\\text{train},i})^2\n$$\n$$\n\\text{MSE}_{\\text{valid}} = \\frac{1}{N_{\\text{valid}}} \\sum_{i=1}^{N_{\\text{valid}}} (y_{\\text{valid},i} - \\hat{y}_{\\text{valid},i})^2\n$$\n\n**High-Frequency Energy Ratio ($\\rho_{\\text{HF}}$):** This metric quantifies the oscillatory nature of the model's errors on the validation set, which is a common symptom of overfitting. The procedure is as follows:\n1.  Compute the validation residuals: $\\mathbf{r}_{\\text{valid}} = \\mathbf{y}_{\\text{valid}} - \\hat{\\mathbf{y}}_{\\text{valid}}$.\n2.  Sort these residuals based on the ascending order of their corresponding input values $x_{\\text{valid}}$. Let this sorted sequence be $\\mathbf{r}_{\\text{valid-sorted}}$.\n3.  Compute the real-valued Discrete Fourier Transform (DFT) of the sorted residuals: $R = \\text{rfft}(\\mathbf{r}_{\\text{valid-sorted}})$. For $N_{\\text{valid}} = 80$, the output $R$ is a complex-valued array of length $41$.\n4.  The set of positive-frequency bins, $\\mathcal{P}$, consists of all bins except the zero-frequency (DC) component. For the RFFT output $R$, these correspond to indices $k \\in \\{1, 2, \\dots, 40\\}$.\n5.  The set of high-frequency bins, $\\mathcal{H}$, is defined as the top quartile (highest $25\\%$) of the frequencies in $\\mathcal{P}$. This corresponds to the last $40 \\times 0.25 = 10$ bins, which have indices $k \\in \\{31, 32, \\dots, 40\\}$.\n6.  The high-frequency energy ratio is then the ratio of the energy in $\\mathcal{H}$ to the total energy in $\\mathcal{P}$:\n    $$\n    \\rho_{\\text{HF}} = \\frac{\\sum_{k \\in \\mathcal{H}} |R_k|^2}{\\sum_{k \\in \\mathcal{P}} |R_k|^2}\n    $$\n    If the denominator is zero, $\\rho_{\\text{HF}}$ is defined as $0$.\n\n### Step 4: Classification Logic\n\nThe computed metrics are used to classify each model as underfitting, well-fit, or overfitting according to a fixed set of rules. The ground truth noise variance is $\\sigma^2 = 0.01$, and the thresholds are $t_u = 1.3$, $t_o = 0.9$, $t_o' = 1.2$, $h_u = 0.35$, and $h_o = 0.45$.\n\n- **Underfitting (Code $0$):** A model is declared to be underfitting if its degree $d$ is less than the true degree $d^\\star$, *or* if it exhibits high error on both training and validation sets coupled with low residual oscillation. Formally:\n  $$\n  (d < d^\\star) \\lor (\\text{MSE}_{\\text{train}} \\ge t_u \\sigma^2 \\land \\text{MSE}_{\\text{valid}} \\ge t_u \\sigma^2 \\land \\rho_{\\text{HF}} \\le h_u)\n  $$\n\n- **Overfitting (Code $2$):** A model is declared to be overfitting if its degree $d$ is greater than $d^\\star$ *and* it shows a low training error, a significantly higher validation error, and high-frequency oscillations in its residuals. Formally:\n  $$\n  (d > d^\\star) \\land (\\text{MSE}_{\\text{train}} \\le t_o \\sigma^2 \\land \\text{MSE}_{\\text{valid}} \\ge t_o' \\sigma^2 \\land \\rho_{\\text{HF}} \\ge h_o)\n  $$\n\n- **Well-fit (Code $1$):** If a model meets neither the underfitting nor the overfitting criteria, it is classified as well-fit.\n\nThese rules provide a concrete, algorithmic definition of the bias-variance trade-off concepts. The program implements this logic for each specified test case, generating a final list of classification codes.",
            "answer": "```python\nimport numpy as np\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Main function to execute the polynomial regression analysis and classification.\n    \"\"\"\n    #\n    # Step 0: Define constants and problem parameters\n    #\n    RANDOM_SEED = 42\n    D_STAR = 4\n    A_COEFFS = np.array([0.3, -0.8, 0.5, 0.0, 0.7])\n    SIGMA = 0.1\n    SIGMA_SQUARED = SIGMA**2\n    N_TOTAL = 200\n    N_TRAIN = 120\n    N_VALID = 80\n\n    # Classification thresholds\n    T_U = 1.3\n    T_O = 0.9\n    T_O_PRIME = 1.2\n    H_U = 0.35\n    H_O = 0.45\n\n    # Test cases to evaluate\n    test_cases = [\n        {'d': 2, 'lambda': 0.001},  # Case 1\n        {'d': 4, 'lambda': 0.05},   # Case 2\n        {'d': 12, 'lambda': 0.0},    # Case 3\n        {'d': 12, 'lambda': 10.0},   # Case 4\n        {'d': 4, 'lambda': 0.0},    # Case 5\n    ]\n    \n    #\n    # Step 1: Generate dataset\n    #\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate x values\n    x = rng.uniform(-1, 1, size=N_TOTAL)\n\n    # Generate true function values y_star\n    def f_star(x_in):\n        return A_COEFFS[0] + A_COEFFS[1] * x_in + A_COEFFS[2] * x_in**2 + \\\n               A_COEFFS[3] * x_in**3 + A_COEFFS[4] * x_in**4\n\n    y_star = f_star(x)\n\n    # Add Gaussian noise\n    noise = rng.normal(0, SIGMA, size=N_TOTAL)\n    y = y_star + noise\n\n    # Split into training and validation sets\n    indices = np.arange(N_TOTAL)\n    rng.shuffle(indices)\n    \n    train_indices = indices[:N_TRAIN]\n    valid_indices = indices[N_TRAIN:]\n\n    x_train, y_train = x[train_indices], y[train_indices]\n    x_valid, y_valid = x[valid_indices], y[valid_indices]\n\n    #\n    # Helper functions\n    #\n    def construct_design_matrix(x_data, degree):\n        \"\"\"Constructs the polynomial design matrix Phi.\"\"\"\n        return np.vander(x_data, degree + 1, increasing=True)\n\n    results = []\n\n    #\n    # Step 2-4: Process each test case\n    #\n    for case in test_cases:\n        d = case['d']\n        lambda_reg = case['lambda']\n\n        # Construct design matrices\n        phi_train = construct_design_matrix(x_train, d)\n        phi_valid = construct_design_matrix(x_valid, d)\n\n        # Fit the model using ridge regression (numerically stable)\n        d_plus_1 = d + 1\n        A = phi_train.T @ phi_train + lambda_reg * np.eye(d_plus_1)\n        b = phi_train.T @ y_train\n        w = np.linalg.solve(A, b)\n\n        # Make predictions\n        y_hat_train = phi_train @ w\n        y_hat_valid = phi_valid @ w\n\n        # Calculate metrics\n        # a) MSE\n        mse_train = np.mean((y_train - y_hat_train)**2)\n        mse_valid = np.mean((y_valid - y_hat_valid)**2)\n        \n        # b) High-frequency energy ratio rho_HF\n        residuals_valid = y_valid - y_hat_valid\n        \n        # Sort residuals according to x_valid\n        sort_indices = np.argsort(x_valid)\n        residuals_valid_sorted = residuals_valid[sort_indices]\n        \n        # Compute RFFT\n        R = scipy.fft.rfft(residuals_valid_sorted)\n        \n        # Calculate energies\n        # P: positive frequencies (indices 1 to end)\n        # H: top quartile of P (last 10 for N_valid=80)\n        # N_valid = 80 -> rfft length = 41. P_indices = 1..40. H_indices = 31..40.\n        num_positive_freqs = len(R) - 1\n        top_quartile_size = int(np.ceil(0.25 * num_positive_freqs))\n        \n        energy_P = np.sum(np.abs(R[1:])**2)\n        energy_H = np.sum(np.abs(R[-top_quartile_size:])**2)\n        \n        rho_hf = energy_H / energy_P if energy_P > 0 else 0.0\n        \n        # Apply classification rules\n        code = 1 # Default to well-fit\n\n        # Underfitting rule\n        is_underfit_by_degree = (d  D_STAR)\n        is_underfit_by_metrics = (mse_train >= T_U * SIGMA_SQUARED and \\\n                                  mse_valid >= T_U * SIGMA_SQUARED and \\\n                                  rho_hf = H_U)\n        if is_underfit_by_degree or is_underfit_by_metrics:\n            code = 0\n\n        # Overfitting rule\n        is_overfit_by_metrics = (d > D_STAR and \\\n                                 mse_train = T_O * SIGMA_SQUARED and \\\n                                 mse_valid >= T_O_PRIME * SIGMA_SQUARED and \\\n                                 rho_hf >= H_O)\n        if is_overfit_by_metrics:\n            code = 2\n            \n        results.append(code)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "After learning to identify overfitting, the next logical step is to learn how to mitigate it. This practice explores regularization, a cornerstone technique for controlling model complexity and improving generalization. By implementing a regularized linear regression estimator, you will directly manipulate the bias-variance trade-off through a \"robustness\" parameter $\\lambda$ and observe its effect on training error, test error, and the generalization gap, ultimately finding the optimal model that balances fitting the data with avoiding memorization .",
            "id": "3189697",
            "problem": "You will investigate how a robustness parameter in a simple distributionally robust optimization setting trades off overfitting and underfitting in linear regression. Your program must generate synthetic data, train a family of estimators indexed by a robustness parameter, and compute diagnostic quantities that verify that moderate robustness can reduce overfitting while excessive robustness induces underfitting.\n\nFundamental base. Consider linear regression with squared loss. Let $x \\in \\mathbb{R}^d$, $y \\in \\mathbb{R}$, and a linear predictor $f_w(x) = w^\\top x$ with parameter $w \\in \\mathbb{R}^d$. The empirical risk minimization objective on training data $\\{(x_i,y_i)\\}_{i=1}^n$ is the average squared loss $n^{-1} \\sum_{i=1}^n (y_i - w^\\top x_i)^2$. A distributionally robust optimization formulation replaces the nominal expectation with a worst-case expectation over an ambiguity set around the empirical distribution. For a broad class of ambiguity sets induced by $\\ell_2$-bounded perturbations of features, the robust counterpart is known to be equivalent to empirical risk plus an $\\ell_2$ penalty on $w$, with penalty weight that increases with the robustness radius. This yields a family of estimators that interpolate between empirical risk minimization (low robustness) and heavily regularized solutions (high robustness).\n\nData generation. For each test case below, generate a training set and an independent test set from the same distribution as follows. For a given dimension $d$ and sample size $n$, let $G \\in \\mathbb{R}^{d \\times d}$ have independent standard normal entries. Compute the thin orthogonal factor $Q \\in \\mathbb{R}^{d \\times d}$ from the $QR$ decomposition of $G$. Let the eigenvalues be $e_k = 1 - 0.95 (k-1)/(d-1)$ for $k \\in \\{1,\\dots,d\\}$ and define the covariance matrix $\\Sigma = Q \\,\\mathrm{diag}(e_1,\\dots,e_d)\\, Q^\\top$. Draw the rows of $X \\in \\mathbb{R}^{n \\times d}$ independently from the multivariate normal distribution with mean zero and covariance $\\Sigma$, and draw the rows of $X_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times d}$ the same way, where $n_{\\mathrm{test}} = 2000$. Define the ground-truth parameter $w^\\star \\in \\mathbb{R}^d$ by $w^\\star_j = 1/j$ for $j \\in \\{1,\\dots,10\\}$ and $w^\\star_j = 0$ for $j \\in \\{11,\\dots,d\\}$. Generate $y = X w^\\star + \\sigma \\,\\varepsilon$ and $y_{\\mathrm{test}} = X_{\\mathrm{test}} w^\\star + \\sigma \\,\\varepsilon_{\\mathrm{test}}$, where the entries of $\\varepsilon \\in \\mathbb{R}^n$ and $\\varepsilon_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}}}$ are independent standard normal random variables. Use the specified random seed for each test case to initialize the pseudorandom number generator before generating $G$, $X$, $X_{\\mathrm{test}}$, $\\varepsilon$, and $\\varepsilon_{\\mathrm{test}}$.\n\nRobust estimator. For each nonnegative robustness parameter $\\lambda$, train the estimator that minimizes the empirical squared loss plus $\\lambda$ times the squared $\\ell_2$ norm of $w$, that is, minimize $n^{-1} \\sum_{i=1}^n (y_i - w^\\top x_i)^2 + \\lambda \\|w\\|_2^2$. Compute the minimizer $w_\\lambda$ exactly, and then compute:\n- The training mean squared error $\\mathrm{MSE}_{\\mathrm{train}}(\\lambda) = n^{-1} \\sum_{i=1}^n (y_i - x_i^\\top w_\\lambda)^2$.\n- The test mean squared error $\\mathrm{MSE}_{\\mathrm{test}}(\\lambda) = n_{\\mathrm{test}}^{-1} \\sum_{i=1}^{n_{\\mathrm{test}}} (y_{\\mathrm{test},i} - x_{\\mathrm{test},i}^\\top w_\\lambda)^2$.\n- The generalization gap $g(\\lambda) = \\mathrm{MSE}_{\\mathrm{test}}(\\lambda) - \\mathrm{MSE}_{\\mathrm{train}}(\\lambda)$.\n\nYour program must process the following three test cases. Each test case is a tuple $(n,d,\\sigma,\\text{seed},\\Lambda)$ where $\\Lambda$ is the list of robustness parameters to evaluate:\n- Test case $1$: $(n=80, d=60, \\sigma=1.5, \\text{seed}=7, \\Lambda = [0, 0.1, 1, 10, 100])$.\n- Test case $2$: $(n=100, d=30, \\sigma=2.0, \\text{seed}=11, \\Lambda = [0, 0.01, 0.1, 1, 5])$.\n- Test case $3$: $(n=60, d=50, \\sigma=0.5, \\text{seed}=19, \\Lambda = [0, 0.01, 0.1, 1, 20])$.\n\nFor each test case, compute and report three integers:\n1. The index $i^\\star$ (zero-based) of the $\\lambda \\in \\Lambda$ that attains the smallest test mean squared error $\\mathrm{MSE}_{\\mathrm{test}}(\\lambda)$, breaking ties by choosing the smallest index.\n2. An indicator $b_1 \\in \\{0,1\\}$ that equals $1$ if the sequence $\\mathrm{MSE}_{\\mathrm{train}}(\\lambda)$ over $\\Lambda$ is strictly increasing with $\\lambda$ up to numerical tolerance (interpret this as nondecreasing with at least one strict increase, using a tolerance of $10^{-10}$), and equals $0$ otherwise.\n3. An indicator $b_2 \\in \\{0,1\\}$ that equals $1$ if the generalization gap strictly decreases from $\\lambda=0$ to $\\lambda = \\Lambda[i^\\star]$ up to numerical tolerance (i.e., $g(0) > g(\\Lambda[i^\\star]) + 10^{-10}$), and equals $0$ otherwise.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, formed by concatenating the triples for the test cases in order. For example, the output must look like $[i^\\star_1,b_{1,1},b_{2,1},i^\\star_2,b_{1,2},b_{2,2},i^\\star_3,b_{1,3},b_{2,3}]$.\n\nNo physical units or angle units are involved; all quantities are dimensionless real numbers. Ensure that all computations are deterministic by using the specified seeds for the pseudorandom number generator within each test case.",
            "solution": "The problem requires an analysis of the trade-off between underfitting and overfitting in a linear regression setting, using a mechanism derived from distributionally robust optimization. This framework introduces a robustness parameter $\\lambda$, which, as described, is equivalent to the regularization parameter in Ridge Regression. We will first establish the analytical solution for the estimator, then detail the data generation and evaluation process.\n\n### Principle-Based Design\n\n#### 1. The Robust Estimator as Ridge Regression\n\nThe problem defines an estimator $w_\\lambda$ that minimizes the objective function for a given robustness parameter $\\lambda \\ge 0$:\n$$ J(w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - w^\\top x_i)^2 + \\lambda \\|w\\|_2^2 $$\nThis objective consists of two terms: the empirical risk (average squared error on the training data) and a regularization term ($\\ell_2$ penalty on the parameter vector $w$). This is the well-known objective function for Ridge Regression.\n\nTo find the optimal parameter vector $w_\\lambda$ that minimizes $J(w)$, we compute the gradient of $J(w)$ with respect to $w$ and set it to zero. In matrix notation, let $X \\in \\mathbb{R}^{n \\times d}$ be the matrix of feature vectors $x_i^\\top$ and $y \\in \\mathbb{R}^n$ be the vector of target values $y_i$. The objective can be written as:\n$$ J(w) = \\frac{1}{n} \\|y - Xw\\|_2^2 + \\lambda w^\\top w $$\nThe gradient $\\nabla_w J(w)$ is:\n$$ \\nabla_w J(w) = \\nabla_w \\left( \\frac{1}{n}(y - Xw)^\\top(y - Xw) + \\lambda w^\\top w \\right) $$\n$$ \\nabla_w J(w) = \\frac{1}{n} \\nabla_w (y^\\top y - 2y^\\top Xw + w^\\top X^\\top Xw) + 2\\lambda w $$\n$$ \\nabla_w J(w) = \\frac{1}{n} (-2X^\\top y + 2X^\\top Xw) + 2\\lambda w $$\nSetting the gradient to zero to find the minimum:\n$$ \\frac{1}{n} (2X^\\top Xw - 2X^\\top y) + 2\\lambda w = 0 $$\n$$ X^\\top Xw - X^\\top y + n\\lambda w = 0 $$\n$$ (X^\\top X + n\\lambda I)w = X^\\top y $$\nwhere $I$ is the $d \\times d$ identity matrix.\n\nThis yields the normal equations for Ridge Regression. The solution $w_\\lambda$ is found by solving this linear system:\n$$ w_\\lambda = (X^\\top X + n\\lambda I)^{-1} X^\\top y $$\nFor any $\\lambda > 0$, the matrix $(X^\\top X + n\\lambda I)$ is positive definite and thus invertible, guaranteeing a unique solution. For $\\lambda=0$, this reduces to Ordinary Least Squares (OLS). We will use a numerically stable linear system solver to find $w_\\lambda$ for each value in the provided list $\\Lambda$.\n\n#### 2. Data Generation Process\n\nThe problem specifies a detailed procedure for generating synthetic data, ensuring reproducibility through a random seed.\n1.  A random orthogonal matrix $Q \\in \\mathbb{R}^{d \\times d}$ is generated from the QR decomposition of a matrix $G$ with standard normal entries. The columns of $Q$ will serve as the eigenvectors of the data covariance matrix.\n2.  A set of eigenvalues $e_k = 1 - 0.95 \\frac{k-1}{d-1}$ for $k=1, \\dots, d$ is defined. These eigenvalues decay linearly from $e_1=1$ to $e_d=0.05$, creating a structured covariance.\n3.  The covariance matrix is constructed as $\\Sigma = Q \\mathrm{diag}(e_1, \\dots, e_d) Q^\\top$.\n4.  Feature vectors for training ($X \\in \\mathbb{R}^{n \\times d}$) and testing ($X_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times d}$) are drawn from a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$.\n5.  A ground-truth parameter vector $w^\\star \\in \\mathbb{R}^d$ is defined with sparse non-zero elements, simulating a scenario where only a few features are relevant. Specifically, $w^\\star_j = 1/j$ for $j \\in \\{1,\\dots,10\\}$ and $w^\\star_j = 0$ for $j > 10$.\n6.  The target values are generated via the linear model $y = Xw^\\star + \\sigma \\varepsilon$ and $y_{\\mathrm{test}} = X_{\\mathrm{test}}w^\\star + \\sigma \\varepsilon_{\\mathrm{test}}$, where $\\varepsilon$ and $\\varepsilon_{\\mathrm{test}}$ are vectors of i.i.d. standard normal noise.\n\n#### 3. Evaluation and Interpretation\n\nFor each $\\lambda \\in \\Lambda$, we compute $w_\\lambda$ and evaluate its performance using three key metrics:\n-   **Training Mean Squared Error ($\\mathrm{MSE}_{\\mathrm{train}}(\\lambda)$)**: Measures how well the model fits the data it was trained on. A model that overfits will have a very low training MSE.\n-   **Test Mean Squared Error ($\\mathrm{MSE}_{\\mathrm{test}}(\\lambda)$)**: Measures how well the model generalizes to new, unseen data from the same distribution. This is the primary metric for model performance.\n-   **Generalization Gap ($g(\\lambda)$)**: The difference $\\mathrm{MSE}_{\\mathrm{test}}(\\lambda) - \\mathrm{MSE}_{\\mathrm{train}}(\\lambda)$. A large gap is a hallmark of overfitting.\n\nThe required outputs for each test case are designed to diagnose overfitting and underfitting:\n1.  **$i^\\star$**: This is the index of the $\\lambda$ that minimizes the test error. This $\\lambda_ {\\mathrm{opt}} = \\Lambda[i^\\star]$ represents the optimal amount of regularization. If $\\lambda_{\\mathrm{opt}} > 0$, it indicates that the unregularized OLS model ($\\lambda=0$) is suboptimal, likely due to overfitting.\n2.  **$b_1$**: This indicator verifies that $\\mathrm{MSE}_{\\mathrm{train}}(\\lambda)$ is a non-decreasing function of $\\lambda$. This is an expected theoretical property: as regularization strength $\\lambda$ increases, the constraint on $w$ becomes tighter, which cannot decrease the minimized loss on the training set.\n3.  **$b_2$**: This indicator checks if the generalization gap is smaller at the optimal $\\lambda_{\\mathrm{opt}}$ compared to $\\lambda=0$. A significant reduction in the gap ($g(0) > g(\\lambda_{\\mathrm{opt}})$) demonstrates that regularization has successfully mitigated overfitting.\n\nThe parameter $\\lambda$ controls the bias-variance trade-off. Low $\\lambda$ (low robustness) leads to low-bias, high-variance estimators that can overfit the training data. High $\\lambda$ (high robustness) leads to high-bias, low-variance estimators that can underfit the data. The optimal $\\lambda$ strikes a balance, minimizing the test error.\n\nThe solution is implemented by following these steps for each test case, calculating the required quantities, and formatting the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates the effect of a robustness parameter on overfitting and underfitting\n    in linear regression by solving a series of test cases.\n\n    The problem is framed as a distributionally robust optimization problem which is\n    equivalent to Ridge Regression. For each test case, the function performs:\n    1. Synthetic data generation based on specified parameters (n, d, sigma, seed).\n    2. Training a series of Ridge regression models for a list of robustness\n       parameters (lambda).\n    3. Computing training MSE, test MSE, and the generalization gap for each model.\n    4. Determining diagnostic quantities (i_star, b1, b2) that characterize the\n       model's behavior in terms of the bias-variance trade-off.\n    \"\"\"\n    test_cases = [\n        (80, 60, 1.5, 7, [0, 0.1, 1, 10, 100]),\n        (100, 30, 2.0, 11, [0, 0.01, 0.1, 1, 5]),\n        (60, 50, 0.5, 19, [0, 0.01, 0.1, 1, 20]),\n    ]\n\n    final_results = []\n    n_test = 2000\n    tol = 1e-10\n\n    for n, d, sigma, seed, Lambda in test_cases:\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        # Generate covariance matrix Sigma\n        G = np.random.randn(d, d)\n        Q, _ = np.linalg.qr(G)\n        \n        # Eigenvalues e_k\n        if d == 1:\n            eigenvalues = np.array([1.0])\n        else:\n            k = np.arange(1, d + 1)\n            # The formula is e_k = 1 - 0.95 * (k-1)/(d-1)\n            eigenvalues = 1 - 0.95 * (k - 1) / (d - 1)\n        \n        Sigma = Q @ np.diag(eigenvalues) @ Q.T\n        \n        # Ground-truth parameter w_star\n        w_star = np.zeros(d)\n        num_nonzero = min(10, d)\n        w_star[:num_nonzero] = 1.0 / np.arange(1, num_nonzero + 1)\n        \n        # Generate feature matrices X and X_test\n        mean_vec = np.zeros(d)\n        X = np.random.multivariate_normal(mean_vec, Sigma, n)\n        X_test = np.random.multivariate_normal(mean_vec, Sigma, n_test)\n        \n        # Generate noise and target variables y and y_test\n        epsilon = np.random.randn(n)\n        epsilon_test = np.random.randn(n_test)\n        y = X @ w_star + sigma * epsilon\n        y_test = X_test @ w_star + sigma * epsilon_test\n\n        # 2. Model Training and Evaluation\n        train_mses = []\n        test_mses = []\n        gaps = []\n        \n        # Pre-compute parts of the normal equation for efficiency\n        XTX = X.T @ X\n        XTy = X.T @ y\n        \n        for lam in Lambda:\n            # Solve (X'X + n*lambda*I)w = X'y for w\n            A = XTX + n * lam * np.identity(d)\n            w_lambda = np.linalg.solve(A, XTy)\n            \n            # Compute training MSE\n            mse_train = np.mean((y - X @ w_lambda)**2)\n            train_mses.append(mse_train)\n            \n            # Compute test MSE\n            mse_test = np.mean((y_test - X_test @ w_lambda)**2)\n            test_mses.append(mse_test)\n            \n            # Compute generalization gap\n            gaps.append(mse_test - mse_train)\n            \n        # 3. Compute Required Outputs\n        \n        # Output 1: i_star (index of minimum test MSE)\n        i_star = np.argmin(test_mses)\n        \n        # Output 2: b1 (indicator for non-decreasing train MSE with strict increase)\n        train_mses_arr = np.array(train_mses)\n        if len(Lambda) > 1:\n            diffs = np.diff(train_mses_arr)\n            is_nondecreasing = np.all(diffs >= -tol)\n            has_strict_increase = np.any(diffs > tol)\n            b1 = 1 if is_nondecreasing and has_strict_increase else 0\n        else:\n            b1 = 0\n            \n        # Output 3: b2 (indicator for generalization gap reduction)\n        # Assumes Lambda[0] = 0, which is true for all test cases\n        g_zero = gaps[0]\n        g_istar = gaps[i_star]\n        b2 = 1 if g_zero > g_istar + tol else 0\n        \n        final_results.extend([i_star, b1, b2])\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, observing a gap between training and validation error is not always sufficient; due to randomness in data splits, this gap might occur by chance. This exercise elevates our analysis by introducing statistical rigor, framing the detection of overfitting as a formal hypothesis testing problem. You will use paired $t$-tests on cross-validation results to determine if a model's validation error is significantly higher than its training error, learning to make statistically sound conclusions while managing the risk of false discoveries across multiple model comparisons .",
            "id": "3189600",
            "problem": "You are given cross-validation (CV) fold-wise training and validation error arrays for multiple hyperparameter settings of the same learning algorithm. The goal is to determine whether increased model capacity leads to statistically significant overfitting by formally testing whether validation errors exceed training errors on average. Frame this as a set of paired hypothesis tests, one per hyperparameter setting, and adjust for multiple comparisons.\n\nBase definitions: For each hyperparameter setting indexed by $i \\in \\{1,2,3,4\\}$ and each fold indexed by $k \\in \\{1,\\dots,n\\}$ with $n=10$, you have training errors $e_{\\text{train},i,k}$ and validation errors $e_{\\text{valid},i,k}$. Define the paired differences $d_{i,k} = e_{\\text{valid},i,k} - e_{\\text{train},i,k}$. Overfitting is operationalized as the alternative hypothesis $H_{1,i}: \\mathbb{E}[d_{i,k}]  0$, against the null $H_{0,i}: \\mathbb{E}[d_{i,k}] = 0$, tested with a paired Student’s $t$-test under the standard assumptions for paired samples (independent folds, approximately normal differences). Use the one-sided direction corresponding to $H_{1,i}$.\n\nYou must control the overall error rate across the $m=4$ simultaneous tests using two procedures:\n- Bonferroni correction for Family-Wise Error Rate (FWER) control.\n- Benjamini-Hochberg (BH) procedure for False Discovery Rate (FDR) control.\n\nAdopt a significance level $\\alpha = 0.05$ for both corrections. Report an overfitting decision per hyperparameter setting as a boolean: return $\\,\\text{True}\\,$ if the hypothesis test for that setting is significant after the specified multiple-comparison correction and the sample mean of $d_{i,k}$ is positive, otherwise return $\\,\\text{False}\\,$.\n\nTest suite:\n- There are $m=4$ hyperparameter settings with $n=10$ folds each. The arrays below are fold-wise errors (unitless, expressed as decimals).\n- Hyperparameter $i=1$ (high-capacity model, expected strong overfitting):\n  - $e_{\\text{train},1,\\cdot} = [0.06, 0.05, 0.07, 0.08, 0.06, 0.05, 0.07, 0.06, 0.08, 0.05]$\n  - $e_{\\text{valid},1,\\cdot} = [0.16, 0.14, 0.18, 0.20, 0.16, 0.14, 0.17, 0.16, 0.20, 0.14]$\n- Hyperparameter $i=2$ (well-tuned model, expected no significant difference):\n  - $e_{\\text{train},2,\\cdot} = [0.12, 0.13, 0.11, 0.12, 0.13, 0.11, 0.12, 0.12, 0.13, 0.11]$\n  - $e_{\\text{valid},2,\\cdot} = [0.121, 0.130, 0.112, 0.123, 0.131, 0.109, 0.118, 0.122, 0.131, 0.110]$\n- Hyperparameter $i=3$ (low-capacity model, expected underfitting; validation errors not larger than training):\n  - $e_{\\text{train},3,\\cdot} = [0.20, 0.19, 0.21, 0.22, 0.20, 0.19, 0.21, 0.22, 0.20, 0.19]$\n  - $e_{\\text{valid},3,\\cdot} = [0.18, 0.17, 0.19, 0.20, 0.18, 0.17, 0.19, 0.20, 0.18, 0.17]$\n- Hyperparameter $i=4$ (borderline case, small positive differences with substantial variability):\n  - $e_{\\text{train},4,\\cdot} = [0.095, 0.105, 0.100, 0.110, 0.090, 0.103, 0.098, 0.107, 0.096, 0.104]$\n  - $e_{\\text{valid},4,\\cdot} = [0.090, 0.115, 0.140, 0.110, 0.105, 0.133, 0.088, 0.127, 0.101, 0.129]$\n\nAlgorithmic requirements:\n- For each $i$, compute the one-sided paired $t$-test $p$-value for $H_{1,i}: \\mathbb{E}[d_{i,k}]  0$.\n- Apply Bonferroni correction: adjusted $p$-values $p^{\\text{Bonf}}_{i} = \\min(1, m \\cdot p_{i})$, and decide $\\,\\text{True}\\,$ if $p^{\\text{Bonf}}_{i}  \\alpha$ and the sample mean of $d_{i,k}$ is positive.\n- Apply Benjamini-Hochberg (BH) step-up procedure at level $\\alpha$: sort the $p$-values, find the largest index $k$ such that $p_{(k)} \\leq \\frac{k}{m}\\alpha$, reject all hypotheses with $p_{i} \\leq p_{(k)}$, and decide $\\,\\text{True}\\,$ for those rejected hypotheses having a positive sample mean of $d_{i,k}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, consisting of two lists with no spaces: the first list gives the Bonferroni-based decisions for $i=1,2,3,4$ in order, and the second list gives the Benjamini-Hochberg-based decisions for $i=1,2,3,4$ in order. For example, a valid output has the form $[[\\text{True},\\text{False},\\text{False},\\text{True}],[\\text{True},\\text{False},\\text{False},\\text{True}]]$.",
            "solution": "The problem requires a formal statistical procedure to determine if increased model capacity leads to significant overfitting. This is accomplished by performing a series of paired hypothesis tests on the training and validation errors from a $k$-fold cross-validation procedure for $m=4$ different hyperparameter settings. We will control the overall error rate from these multiple comparisons using both the Bonferroni and Benjamini-Hochberg (BH) methods.\n\nThe fundamental principle is to operationalize overfitting as the condition where the expected validation error is greater than the expected training error. For a given hyperparameter setting $i$, the data consists of paired observations of training error, $e_{\\text{train},i,k}$, and validation error, $e_{\\text{valid},i,k}$, for each of the $n=10$ folds. The pairing arises because both errors are computed on the same data partition (fold $k$).\n\nFirst, we define the difference for each fold $k$ of hyperparameter setting $i$ as $d_{i,k} = e_{\\text{valid},i,k} - e_{\\text{train},i,k}$. A positive mean difference, $\\mathbb{E}[d_{i,k}]  0$, indicates overfitting. We formalize this as a one-sided hypothesis test:\n- Null hypothesis $H_{0,i}: \\mu_{d_i} = \\mathbb{E}[d_{i,k}] = 0$ (no difference between validation and training error).\n- Alternative hypothesis $H_{1,i}: \\mu_{d_i}  0$ (validation error is greater than training error, i.e., overfitting).\n\nWe use a paired Student’s $t$-test for this purpose. The test statistic for each hypothesis $i$ is calculated as:\n$$ t_i = \\frac{\\bar{d}_i}{s_{d_i} / \\sqrt{n}} $$\nwhere $\\bar{d}_i$ is the sample mean of the differences $d_{i,k}$, $s_{d_i}$ is the sample standard deviation of these differences, and $n=10$ is the number of folds (samples). Under the null hypothesis, this statistic follows a $t$-distribution with $n-1=9$ degrees of freedom. From this, we compute a one-sided $p$-value, $p_i$, for each of the $m=4$ tests.\n\nSince we are performing $m=4$ tests simultaneously, we must correct for multiple comparisons to control the overall error rate at a significance level $\\alpha=0.05$.\n\n1.  **Bonferroni Correction (Family-Wise Error Rate Control)**: This method controls the probability of making at least one Type I error (a false positive) across all tests. It is a conservative method that adjusts the significance threshold for each individual test to $\\alpha' = \\alpha/m$. A null hypothesis $H_{0,i}$ is rejected if its corresponding $p$-value $p_i  \\alpha/m$. The final decision for overfitting is $\\text{True}$ if $H_{0,i}$ is rejected and the sample mean difference $\\bar{d}_i  0$.\n\n2.  **Benjamini-Hochberg (BH) Procedure (False Discovery Rate Control)**: This method controls the expected proportion of false positives among all rejected hypotheses. It is generally more powerful (less conservative) than Bonferroni. The procedure is as follows:\n    a. Sort the raw $p$-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n    b. Find the largest rank $k$ such that $p_{(k)} \\le \\frac{k}{m}\\alpha$. Let this be $k_{\\text{max}}$.\n    c. Reject all null hypotheses $H_{0,i}$ for which $p_i \\le p_{(k_{\\text{max}})}$. If no such $k$ exists, no hypotheses are rejected.\n    d. The final decision for overfitting is $\\text{True}$ if $H_{0,i}$ is rejected and $\\bar{d}_i  0$.\n\n**Step 1: Calculate Differences and Raw p-values**\n\nWe are given $n=10$, $m=4$, and $\\alpha=0.05$. For each hyperparameter setting $i$, we compute the differences $d_{i,k}$ and perform a one-sided paired $t$-test.\n\n-   **Hyperparameter $i=1$**:\n    $e_{\\text{train},1} = [0.06, 0.05, 0.07, 0.08, 0.06, 0.05, 0.07, 0.06, 0.08, 0.05]$\n    $e_{\\text{valid},1} = [0.16, 0.14, 0.18, 0.20, 0.16, 0.14, 0.17, 0.16, 0.20, 0.14]$\n    $d_1 = [0.10, 0.09, 0.11, 0.12, 0.10, 0.09, 0.10, 0.10, 0.12, 0.09]$\n    $\\bar{d}_1 = 0.102$, $s_{d_1} \\approx 0.01135$. The $t$-statistic is $t_1 \\approx 28.38$. With $df=9$, the $p$-value is $p_1 \\approx 1.5 \\times 10^{-10}$.\n\n-   **Hyperparameter $i=2$**:\n    $e_{\\text{train},2} = [0.12, 0.13, 0.11, 0.12, 0.13, 0.11, 0.12, 0.12, 0.13, 0.11]$\n    $e_{\\text{valid},2} = [0.121, 0.130, 0.112, 0.123, 0.131, 0.109, 0.118, 0.122, 0.131, 0.110]$\n    $d_2 = [0.001, 0.0, 0.002, 0.003, 0.001, -0.001, -0.002, 0.002, 0.001, 0.0]$\n    $\\bar{d}_2 = 0.0007$, $s_{d_2} \\approx 0.00164$. The $t$-statistic is $t_2 \\approx 1.35$. With $df=9$, the $p$-value is $p_2 \\approx 0.105$.\n\n-   **Hyperparameter $i=3$**:\n    $e_{\\text{train},3} = [0.20, 0.19, 0.21, 0.22, 0.20, 0.19, 0.21, 0.22, 0.20, 0.19]$\n    $e_{\\text{valid},3} = [0.18, 0.17, 0.19, 0.20, 0.18, 0.17, 0.19, 0.20, 0.18, 0.17]$\n    $d_3 = [-0.02, -0.02, -0.02, -0.02, -0.02, -0.02, -0.02, -0.02, -0.02, -0.02]$\n    $\\bar{d}_3 = -0.02$. Since the sample mean difference is negative, the data points away from the alternative hypothesis $H_{1,3}: \\mu_{d_3}  0$. The decision is $\\text{False}$ irrespective of the $p$-value. For completeness, $p_3 \\approx 1.0$.\n\n-   **Hyperparameter $i=4$**:\n    $e_{\\text{train},4} = [0.095, 0.105, 0.100, 0.110, 0.090, 0.103, 0.098, 0.107, 0.096, 0.104]$\n    $e_{\\text{valid},4} = [0.090, 0.115, 0.140, 0.110, 0.105, 0.133, 0.088, 0.127, 0.101, 0.129]$\n    $d_4 = [-0.005, 0.01, 0.04, 0.0, 0.015, 0.03, -0.01, 0.02, 0.005, 0.025]$\n    $\\bar{d}_4 = 0.013$, $s_{d_4} \\approx 0.0173$. The $t$-statistic is $t_4 \\approx 2.38$. With $df=9$, the $p$-value is $p_4 \\approx 0.0205$.\n\nSummary of raw $p$-values and mean differences:\n- $i=1$: $\\bar{d}_1 = 0.102  0$, $p_1 \\approx 1.5 \\times 10^{-10}$\n- $i=2$: $\\bar{d}_2 = 0.0007  0$, $p_2 \\approx 0.105$\n- $i=3$: $\\bar{d}_3 = -0.02  0$, $p_3 \\approx 1.0$\n- $i=4$: $\\bar{d}_4 = 0.013  0$, $p_4 \\approx 0.0205$\n\n**Step 2: Apply Bonferroni Correction**\n\nThe adjusted significance level is $\\alpha' = \\alpha / m = 0.05 / 4 = 0.0125$.\n- $i=1$: $\\bar{d}_1  0$ and $p_1 \\approx 1.5 \\times 10^{-10}  0.0125$. Result: $\\text{True}$.\n- $i=2$: $\\bar{d}_2  0$ but $p_2 \\approx 0.105 \\not 0.0125$. Result: $\\text{False}$.\n- $i=3$: $\\bar{d}_3  0$. Result: $\\text{False}$.\n- $i=4$: $\\bar{d}_4  0$ but $p_4 \\approx 0.0205 \\not 0.0125$. Result: $\\text{False}$.\n\nThe Bonferroni decision list is $[\\text{True}, \\text{False}, \\text{False}, \\text{False}]$.\n\n**Step 3: Apply Benjamini-Hochberg Procedure**\n\n1.  Sorted $p$-values:\n    - $p_{(1)} = p_1 \\approx 1.5 \\times 10^{-10}$ (original index 1)\n    - $p_{(2)} = p_4 \\approx 0.0205$ (original index 4)\n    - $p_{(3)} = p_2 \\approx 0.105$ (original index 2)\n    - $p_{(4)} = p_3 \\approx 1.0$ (original index 3)\n\n2.  Critical values $\\frac{k}{m}\\alpha = \\frac{k}{4}(0.05) = k \\cdot 0.0125$:\n    - $k=1: 0.0125$\n    - $k=2: 0.0250$\n    - $k=3: 0.0375$\n    - $k=4: 0.0500$\n\n3.  Find largest $k$ such that $p_{(k)} \\le \\frac{k}{m}\\alpha$:\n    - For $k=4$: $p_{(4)} \\approx 1.0 \\not\\le 0.0500$.\n    - For $k=3$: $p_{(3)} \\approx 0.105 \\not\\le 0.0375$.\n    - For $k=2$: $p_{(2)} \\approx 0.0205 \\le 0.0250$. This is true.\n    Thus, $k_{\\text{max}}=2$. The significance threshold is $p_{(2)} \\approx 0.0205$.\n\n4.  Reject all $H_{0,i}$ where $p_i \\le 0.0205$. This applies to $p_1$ and $p_4$.\n    - $i=1$: $H_0$ is rejected. $\\bar{d}_1  0$. Result: $\\text{True}$.\n    - $i=2$: $H_0$ is not rejected ($p_2  0.0205$). Result: $\\text{False}$.\n    - $i=3$: $\\bar{d}_3  0$. Result: $\\text{False}$.\n    - $i=4$: $H_0$ is rejected. $\\bar{d}_4  0$. Result: $\\text{True}$.\n\nThe Benjamini-Hochberg decision list is $[\\text{True}, \\text{False}, \\text{False}, \\text{True}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Performs paired t-tests and multiple comparison corrections to detect overfitting.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0.06, 0.05, 0.07, 0.08, 0.06, 0.05, 0.07, 0.06, 0.08, 0.05]),\n            np.array([0.16, 0.14, 0.18, 0.20, 0.16, 0.14, 0.17, 0.16, 0.20, 0.14])\n        ),\n        (\n            np.array([0.12, 0.13, 0.11, 0.12, 0.13, 0.11, 0.12, 0.12, 0.13, 0.11]),\n            np.array([0.121, 0.130, 0.112, 0.123, 0.131, 0.109, 0.118, 0.122, 0.131, 0.110])\n        ),\n        (\n            np.array([0.20, 0.19, 0.21, 0.22, 0.20, 0.19, 0.21, 0.22, 0.20, 0.19]),\n            np.array([0.18, 0.17, 0.19, 0.20, 0.18, 0.17, 0.19, 0.20, 0.18, 0.17])\n        ),\n        (\n            np.array([0.095, 0.105, 0.100, 0.110, 0.090, 0.103, 0.098, 0.107, 0.096, 0.104]),\n            np.array([0.090, 0.115, 0.140, 0.110, 0.105, 0.133, 0.088, 0.127, 0.101, 0.129])\n        )\n    ]\n\n    alpha = 0.05\n    m = len(test_cases)\n    \n    # --- Step 1: Calculate raw p-values and mean differences ---\n    p_values = []\n    mean_diffs = []\n    for e_train, e_valid in test_cases:\n        diffs = e_valid - e_train\n        mean_diffs.append(np.mean(diffs))\n        \n        # We only care about positive differences for overfitting\n        if np.mean(diffs) = 0:\n            p_values.append(1.0)\n        else:\n            # Perform one-sided paired t-test for valid > train\n            t_stat, p_val = stats.ttest_rel(e_valid, e_train, alternative='greater')\n            p_values.append(p_val)\n\n    # --- Step 2: Bonferroni Correction ---\n    bonferroni_threshold = alpha / m\n    bonferroni_decisions = []\n    for i in range(m):\n        is_significant = p_values[i]  bonferroni_threshold\n        is_positive_diff = mean_diffs[i] > 0\n        bonferroni_decisions.append(is_significant and is_positive_diff)\n\n    # --- Step 3: Benjamini-Hochberg (BH) Procedure ---\n    bh_decisions = [False] * m\n    \n    # Sort p-values while keeping track of original indices\n    # We use a stable sort to handle ties, although not strictly necessary here\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = np.array(p_values)[sorted_indices]\n\n    # Find the largest k such that p_(k) = (k/m)*alpha\n    k_max = 0\n    for k in range(m, 0, -1):\n        if sorted_p_values[k-1] = (k / m) * alpha:\n            k_max = k\n            break\n    \n    # If a threshold is found, reject corresponding hypotheses\n    if k_max > 0:\n        p_threshold = sorted_p_values[k_max - 1]\n        for i in range(m):\n            if p_values[i] = p_threshold and mean_diffs[i] > 0:\n                bh_decisions[i] = True\n\n    # --- Final Output Formatting ---\n    # Convert Python boolean to string 'True' or 'False'\n    bonf_str = ','.join(map(str, bonferroni_decisions))\n    bh_str = ','.join(map(str, bh_decisions))\n    \n    print(f\"[[{bonf_str}],[{bh_str}]]\")\n\nsolve()\n\n```"
        }
    ]
}