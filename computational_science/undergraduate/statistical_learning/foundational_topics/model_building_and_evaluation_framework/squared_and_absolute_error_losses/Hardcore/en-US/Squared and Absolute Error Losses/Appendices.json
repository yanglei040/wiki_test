{
    "hands_on_practices": [
        {
            "introduction": "To build a strong intuition for the practical differences between squared ($L_2$) and absolute ($L_1$) error, we begin with a direct comparison. This exercise presents a hypothetical scenario where two models are evaluated on a small dataset containing a significant outlier. By calculating the Mean Squared Error ($MSE$) and Mean Absolute Error ($MAE$) for each, you will see firsthand how the squaring operation in $MSE$ amplifies the impact of large errors, leading to a different conclusion than the more robust $MAE$. This practice is designed to make the abstract concepts of sensitivity and robustness concrete and memorable.",
            "id": "3168840",
            "problem": "Consider a deep learning regression task where two models, labeled $X$ and $Y$, are evaluated on $n=10$ hold-out samples. The residuals (predicted minus true) are observed as follows. For model $X$, the residuals are $-0.5$ for $9$ samples and $10$ for $1$ sample. For model $Y$, the residuals are $-1.8$ for $5$ samples and $+1.8$ for $5$ samples. Without using any pre-given formulas, determine which model is ranked better by Mean Absolute Error (MAE) and which is ranked better by Mean Squared Error (MSE), and select the option that correctly explains why the rankings reverse by referencing appropriate residual distribution moments. Recall that Mean Absolute Error (MAE) and Mean Squared Error (MSE) are standard regression performance metrics.\n\nWhich option is correct?\n\nA. $MAE$ prefers model $X$ while $MSE$ prefers model $Y$. This reversal is explained because $MSE$ is driven by the second moment (variance) and becomes large under heavy-tailed residuals with high kurtosis due to the single extreme outlier in $X$, whereas $MAE$ depends on the first absolute moment and is comparatively robust to isolated outliers. The positive skewness in $X$ indicates asymmetry but does not by itself overturn $MAE$’s robustness.\n\nB. Both $MAE$ and $MSE$ prefer model $Y$ because its mean residual is $0$, and $MSE$ mainly depends on the mean; the outlier in $X$ does not change the ranking once the mean is accounted for.\n\nC. $MAE$ prefers model $Y$ while $MSE$ prefers model $X$ because $MAE$ amplifies large outliers more strongly than $MSE$, so the extreme residual in $X$ hurts $MAE$ but not $MSE$.\n\nD. The rankings cannot reverse when there is a single positive outlier; such reversal requires negative skewness. Therefore, with the given residuals, both $MAE$ and $MSE$ prefer the same model.\n\nE. $MAE$ prefers model $X$ and $MSE$ prefers model $Y$, but the reversal is primarily because $MSE$ responds to skewness rather than variance; skewness alone directly increases $MSE$ even if variance is similar.",
            "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- A deep learning regression task involves two models, $X$ and $Y$.\n- The number of hold-out samples is $n=10$.\n- The residuals for model $X$, denoted as the set $R_X$, consist of $9$ values of $-0.5$ and $1$ value of $10$.\n- The residuals for model $Y$, denoted as the set $R_Y$, consist of $5$ values of $-1.8$ and $5$ values of $1.8$.\n- The task is to determine which model is ranked better by Mean Absolute Error (MAE) and Mean Squared Error (MSE) and to explain the potential reversal of rankings by referencing residual distribution moments.\n- The constraint is to solve \"without using any pre-given formulas,\" which is interpreted as deriving the results from the fundamental definitions of the metrics.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on fundamental and standard concepts in statistics and machine learning evaluation: Mean Absolute Error (MAE), Mean Squared Error (MSE), and the analysis of residual distributions using statistical moments (mean, variance, skewness, kurtosis). These are well-established and core to the field.\n- **Well-Posed:** The problem provides all necessary numerical data to calculate the required metrics. The sets of residuals are fully specified, and the number of samples is consistent. The question is unambiguous, leading to a unique, calculable solution.\n- **Objective:** The problem is stated using precise, objective language and numerical data. It is free of subjective or opinion-based claims.\n\nThe problem does not violate any of the invalidity criteria. It is a well-posed, scientifically sound, and objective question that tests the understanding of regression metrics.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of Solution\nThe metrics are defined from first principles as follows. For a set of $n$ residuals $\\{r_i\\}_{i=1}^n$:\n- **Mean Absolute Error (MAE)** is the average of the absolute values of the residuals: $MAE = \\frac{1}{n} \\sum_{i=1}^n |r_i|$.\n- **Mean Squared Error (MSE)** is the average of the squared values of the residuals: $MSE = \\frac{1}{n} \\sum_{i=1}^n r_i^2$.\nA lower value for either metric indicates a better-performing model.\n\n**Calculation for Model X:**\nThe residuals are $R_X = \\{ \\underbrace{-0.5, \\dots, -0.5}_{9 \\text{ times}}, 10 \\}$. The number of samples is $n=10$.\n\n- **MAE for Model X ($MAE_X$):**\n$$MAE_X = \\frac{1}{10} \\left( 9 \\times |-0.5| + 1 \\times |10| \\right)$$\n$$MAE_X = \\frac{1}{10} \\left( 9 \\times 0.5 + 10 \\right)$$\n$$MAE_X = \\frac{1}{10} \\left( 4.5 + 10 \\right) = \\frac{14.5}{10} = 1.45$$\n\n- **MSE for Model X ($MSE_X$):**\n$$MSE_X = \\frac{1}{10} \\left( 9 \\times (-0.5)^2 + 1 \\times (10)^2 \\right)$$\n$$MSE_X = \\frac{1}{10} \\left( 9 \\times 0.25 + 100 \\right)$$\n$$MSE_X = \\frac{1}{10} \\left( 2.25 + 100 \\right) = \\frac{102.25}{10} = 10.225$$\n\n**Calculation for Model Y:**\nThe residuals are $R_Y = \\{ \\underbrace{-1.8, \\dots, -1.8}_{5 \\text{ times}}, \\underbrace{1.8, \\dots, 1.8}_{5 \\text{ times}} \\}$. The number of samples is $n=10$.\n\n- **MAE for Model Y ($MAE_Y$):**\n$$MAE_Y = \\frac{1}{10} \\left( 5 \\times |-1.8| + 5 \\times |1.8| \\right)$$\n$$MAE_Y = \\frac{1}{10} \\left( 5 \\times 1.8 + 5 \\times 1.8 \\right)$$\n$$MAE_Y = \\frac{1}{10} \\left( 10 \\times 1.8 \\right) = 1.8$$\n\n- **MSE for Model Y ($MSE_Y$):**\n$$MSE_Y = \\frac{1}{10} \\left( 5 \\times (-1.8)^2 + 5 \\times (1.8)^2 \\right)$$\n$$MSE_Y = \\frac{1}{10} \\left( 10 \\times (1.8)^2 \\right)$$\n$$MSE_Y = (1.8)^2 = 3.24$$\n\n**Comparison of Rankings:**\n- **MAE:** $MAE_X = 1.45$ and $MAE_Y = 1.8$. Since $1.45  1.8$, model $X$ is ranked better by MAE.\n- **MSE:** $MSE_X = 10.225$ and $MSE_Y = 3.24$. Since $10.225 > 3.24$, model $Y$ is ranked better by MSE.\n\nThe rankings are indeed reversed.\n\n**Analysis using Residual Distribution Moments:**\n- **MAE** is the first absolute moment about the origin ($E[|R|]$). It treats all errors linearly (in absolute value). For model $X$, the $9$ small errors result in a small contribution to the sum, and the single large outlier contributes linearly ($10$). The overall average remains relatively low.\n- **MSE** is the second moment about the origin ($E[R^2]$). It is related to the variance (the second central moment, $\\sigma^2 = E[(R-\\mu)^2]$) and the mean (the first moment, $\\mu = E[R]$) by the identity $MSE = \\sigma^2 + \\mu^2$. Due to the squaring operation, MSE penalizes large errors disproportionately. For model $X$, the single residual of $10$ contributes $10^2 = 100$ to the sum of squares, dominating the total and resulting in a very high MSE. This sensitivity to extreme outliers is characteristic of MSE.\n- The presence of an extreme outlier, as in the residuals of model $X$, creates a \"heavy-tailed\" distribution. This is quantified by **kurtosis** (the scaled fourth central moment), which for model $X$ would be very high (a leptokurtic distribution). High kurtosis indicates that outliers are more extreme than in a normal distribution, and MSE is highly sensitive to this.\n- The distribution for model $X$ is also asymmetric, as shown by its **skewness** (the scaled third central moment). The single large positive residual creates a strong positive skew. While skewness describes asymmetry, it is the kurtosis and the underlying variance that are most directly connected to the magnitude of the MSE.\n- Model $Y$'s residuals are symmetric around $0$, so its mean and skewness are both $0$. It has no extreme outliers, resulting in a moderate variance and thus a moderate MSE.\n\n### Option-by-Option Analysis\n\n**A. $MAE$ prefers model $X$ while $MSE$ prefers model $Y$. This reversal is explained because $MSE$ is driven by the second moment (variance) and becomes large under heavy-tailed residuals with high kurtosis due to the single extreme outlier in $X$, whereas $MAE$ depends on the first absolute moment and is comparatively robust to isolated outliers. The positive skewness in $X$ indicates asymmetry but does not by itself overturn $MAE$’s robustness.**\n- The rankings are correct: our calculations show $MAE$ prefers $X$ and $MSE$ prefers $Y$.\n- The explanation is sound. $MSE$ is indeed driven by the second moment ($E[R^2]$) and is thus closely related to variance. It is extremely sensitive to heavy tails and high kurtosis, which are caused by the outlier in $X$'s residuals.\n- Conversely, $MAE$ ($E[|R|]$) is the first absolute moment and is known to be more robust to such outliers.\n- The statement about skewness is also correct; it indicates asymmetry, but the primary cause of the MSE's large value is the magnitude of the squared error, which is better described in terms of variance and kurtosis.\n- **Verdict:** Correct.\n\n**B. Both $MAE$ and $MSE$ prefer model $Y$ because its mean residual is $0$, and $MSE$ mainly depends on the mean; the outlier in $X$ does not change the ranking once the mean is accounted for.**\n- The first clause, \"Both $MAE$ and $MSE$ prefer model $Y$,\" is false. Our calculations show $MAE$ prefers model $X$.\n- The reasoning, \"$MSE$ mainly depends on the mean,\" is false. $MSE = \\sigma^2 + \\mu^2$. For model $X$, the mean is $\\mu_X = \\frac{1}{10}(9 \\times -0.5 + 10) = 0.55$, so $\\mu_X^2 = 0.3025$. The variance is $\\sigma_X^2 = MSE_X - \\mu_X^2 = 10.225 - 0.3025 = 9.9225$. The MSE is clearly dominated by the variance, not the mean.\n- **Verdict:** Incorrect.\n\n**C. $MAE$ prefers model $Y$ while $MSE$ prefers model $X$ because $MAE$ amplifies large outliers more strongly than $MSE$, so the extreme residual in $X$ hurts $MAE$ but not $MSE$.**\n- The ranking, \"$MAE$ prefers model $Y$ while $MSE$ prefers model $X$,\" is the exact opposite of what was calculated.\n- The reasoning, \"$MAE$ amplifies large outliers more strongly than $MSE$,\" is fundamentally false. The squaring operation in MSE ($r^2$) causes much stronger amplification of large errors than the absolute value operation in MAE ($|r|$).\n- **Verdict:** Incorrect.\n\n**D. The rankings cannot reverse when there is a single positive outlier; such reversal requires negative skewness. Therefore, with the given residuals, both $MAE$ and $MSE$ prefer the same model.**\n- The premise, \"The rankings cannot reverse when there is a single positive outlier,\" is demonstrably false, as the calculations have shown a reversal.\n- The claim that reversal \"requires negative skewness\" is a baseless and incorrect assertion. The reversal depends on the interplay between the magnitude of the outlier and the distribution of the other residuals, not on the sign of the skew.\n- **Verdict:** Incorrect.\n\n**E. $MAE$ prefers model $X$ and $MSE$ prefers model $Y$, but the reversal is primarily because $MSE$ responds to skewness rather than variance; skewness alone directly increases $MSE$ even if variance is similar.**\n- The initial ranking is correct.\n- The reasoning, \"$MSE$ responds to skewness rather than variance,\" is false. $MSE$ is the second moment about the origin. Variance is the second central moment. Skewness is related to the third central moment. They are distinct concepts. MSE is directly a function of variance and mean, not skewness. High skewness often co-occurs with high variance in outlier-driven distributions, but MSE's value is determined by the variance and mean, not the skewness directly.\n- **Verdict:** Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving from a concrete example to a more general theoretical framework, this practice explores the concept of oracle risk in the presence of contaminated data. We will model a regression scenario where the noise is not simple but rather a mixture of a standard normal distribution and a heavy-tailed distribution, simulating occasional large errors. Your task is to derive the optimal \"oracle\" predictors that minimize the expected squared ($L_2$) and absolute ($L_1$) loss, and then to compute their respective risks, proving from first principles why $L_1$ loss is preferable when dealing with heavy-tailed contamination.",
            "id": "3175076",
            "problem": "Consider a regression setting with an unknown deterministic target function $f$ and a regressor $X$. The observed response is modeled as $Y = f(X) + Z$, where the noise $Z$ is independent of $X$ and has a symmetric two-component Gaussian mixture distribution\n$$\nZ \\sim 0.9\\,\\mathcal{N}(0,1) + 0.1\\,\\mathcal{N}(0,25).\n$$\nDefine the squared loss $\\ell_{2}(y,t) = (y-t)^{2}$ and the absolute loss $\\ell_{1}(y,t) = |y-t|$. For a measurable predictor $g$, the expected risk under a loss $\\ell$ is $R_{\\ell}(g) = \\mathbb{E}[\\ell(Y,g(X))]$. The oracle risk $R_{\\ell}^{\\star}$ is the infimum of $R_{\\ell}(g)$ over all measurable functions $g$.\n\nStarting from these definitions, derive from first principles the oracle predictors that minimize the respective risks under $\\ell_{2}$ and $\\ell_{1}$. Then compute the exact oracle risks $R_{2}^{\\star}$ and $R_{1}^{\\star}$ for the given model. Finally, justify, using your derived expressions, why the absolute loss $\\ell_{1}$ is preferable to the squared loss $\\ell_{2}$ under heavy-tailed contamination in the noise distribution.\n\nReport your final answers as a two-entry row matrix $\\begin{pmatrix} R_{2}^{\\star}  R_{1}^{\\star} \\end{pmatrix}$. No rounding is required; provide exact expressions.",
            "solution": "The problem is validated as being scientifically sound, well-posed, and objective. It presents a standard scenario in statistical decision theory to compare the robustness of oracle predictors under different loss functions.\n\n**1. Derivation of Oracle Predictors**\nThe expected risk for a predictor $g$ is $R_{\\ell}(g) = \\mathbb{E}[\\ell(Y,g(X))]$. By the law of total expectation, this can be written as $R_{\\ell}(g) = \\mathbb{E}_{X}\\left[\\mathbb{E}_{Y|X}[\\ell(Y, g(X))|X=x]\\right]$. To minimize the overall risk, we must minimize the inner expectation for each value of $x$. The optimal predictor at $X=x$, let's call it $g^*(x)$, is the value that minimizes the conditional risk $\\mathbb{E}[\\ell(Y, a)|X=x]$ with respect to $a$.\nGiven $Y = f(X) + Z$, the conditional distribution of $Y$ given $X=x$ is simply the distribution of $f(x)+Z$, since $Z$ is independent of $X$.\n\n-   **Squared Loss ($\\ell_2$):** The optimal predictor $g_{2}^{\\star}(x)$ minimizes $\\mathbb{E}[(Y-a)^2|X=x]$. It is a standard result that the minimizer is the conditional mean: $g_{2}^{\\star}(x) = \\mathbb{E}[Y|X=x] = \\mathbb{E}[f(x)+Z] = f(x) + \\mathbb{E}[Z]$.\n-   **Absolute Loss ($\\ell_1$):** The optimal predictor $g_{1}^{\\star}(x)$ minimizes $\\mathbb{E}[|Y-a||X=x]$. The minimizer is the conditional median: $g_{1}^{\\star}(x) = \\text{median}(Y|X=x) = \\text{median}(f(x)+Z) = f(x) + \\text{median}(Z)$.\n\n**2. Calculation of Noise Properties and Oracle Predictors**\nThe noise is given by the mixture distribution $Z \\sim 0.9\\,\\mathcal{N}(0,1) + 0.1\\,\\mathcal{N}(0,25)$. Let $Z_1 \\sim \\mathcal{N}(0,1)$ and $Z_2 \\sim \\mathcal{N}(0,25)$.\n-   **Mean of Z:** The mean is $\\mathbb{E}[Z] = 0.9\\,\\mathbb{E}[Z_1] + 0.1\\,\\mathbb{E}[Z_2] = 0.9 \\cdot 0 + 0.1 \\cdot 0 = 0$.\n-   **Median of Z:** The probability density function of $Z$ is a weighted sum of two symmetric PDFs centered at zero. The resulting PDF is also symmetric about zero. Therefore, $\\text{median}(Z) = 0$.\nWith these results, both oracle predictors simplify to the same function:\n$g_{2}^{\\star}(x) = f(x) + 0 = f(x)$\n$g_{1}^{\\star}(x) = f(x) + 0 = f(x)$\n\n**3. Computation of Oracle Risks**\nThe oracle risk is the risk of the optimal predictor.\n-   **Squared Risk ($R_{2}^{\\star}$):**\n$R_{2}^{\\star} = \\mathbb{E}[\\ell_{2}(Y, g_{2}^{\\star}(X))] = \\mathbb{E}[(Y-f(X))^2] = \\mathbb{E}[Z^2]$.\n$\\mathbb{E}[Z^2]$ is the variance of $Z$ since $\\mathbb{E}[Z]=0$. We compute it using the law of total expectation:\n$R_{2}^{\\star} = \\mathbb{E}[Z^2] = 0.9 \\cdot \\mathbb{E}[Z_1^2] + 0.1 \\cdot \\mathbb{E}[Z_2^2] = 0.9 \\cdot (\\text{Var}(Z_1) + \\mathbb{E}[Z_1]^2) + 0.1 \\cdot (\\text{Var}(Z_2) + \\mathbb{E}[Z_2]^2)$\n$R_{2}^{\\star} = 0.9 \\cdot (1 + 0^2) + 0.1 \\cdot (25 + 0^2) = 0.9 + 2.5 = 3.4 = \\frac{17}{5}$.\n\n-   **Absolute Risk ($R_{1}^{\\star}$):**\n$R_{1}^{\\star} = \\mathbb{E}[\\ell_{1}(Y, g_{1}^{\\star}(X))] = \\mathbb{E}[|Y-f(X)|] = \\mathbb{E}[|Z|]$.\nTo compute $\\mathbb{E}[|Z|]$, we use the law of total expectation again. For a normal distribution $\\mathcal{N}(0, \\sigma^2)$, the mean absolute deviation is $\\mathbb{E}[|X|] = \\sigma\\sqrt{2/\\pi}$.\nFor $Z_1 \\sim \\mathcal{N}(0,1)$, $\\sigma_1=1$, so $\\mathbb{E}[|Z_1|] = \\sqrt{2/\\pi}$.\nFor $Z_2 \\sim \\mathcal{N}(0,25)$, $\\sigma_2=5$, so $\\mathbb{E}[|Z_2|] = 5\\sqrt{2/\\pi}$.\n$R_{1}^{\\star} = \\mathbb{E}[|Z|] = 0.9 \\cdot \\mathbb{E}[|Z_1|] + 0.1 \\cdot \\mathbb{E}[|Z_2|] = 0.9 \\cdot \\sqrt{\\frac{2}{\\pi}} + 0.1 \\cdot 5\\sqrt{\\frac{2}{\\pi}}$\n$R_{1}^{\\star} = (0.9 + 0.5) \\sqrt{\\frac{2}{\\pi}} = 1.4 \\sqrt{\\frac{2}{\\pi}} = \\frac{7}{5} \\sqrt{\\frac{2}{\\pi}}$.\n\n**4. Justification**\nThe oracle risk under squared loss is $R_{2}^{\\star} = 3.4$, while the oracle risk under absolute loss is $R_{1}^{\\star} = 1.4\\sqrt{2/\\pi} \\approx 1.4 \\cdot 0.798 \\approx 1.117$. The absolute loss leads to a significantly lower risk.\n\nThe reason is the differential impact of the heavy-tailed component, $\\mathcal{N}(0,25)$.\n-   For the squared risk, the contribution from the heavy-tailed component is $0.1 \\times (\\text{variance}) = 0.1 \\times 25 = 2.5$. This single term accounts for over $73\\%$ of the total risk ($2.5/3.4$). The quadratic nature of the loss function heavily penalizes the large errors drawn from this component.\n-   For the absolute risk, the contribution is $0.1 \\times (\\text{std. dev.}) \\times \\sqrt{2/\\pi} = 0.1 \\times 5 \\times \\sqrt{2/\\pi} = 0.5 \\sqrt{2/\\pi}$. This accounts for only about $36\\%$ of the total risk ($0.5/1.4$). The linear penalty of the absolute loss is less sensitive to the large magnitude of errors.\n\nThis demonstrates that for distributions with heavy-tailed contamination (i.e., prone to outliers), the absolute loss is preferable as it yields a more robust predictor with lower expected error.\n\n**Final Answer**\nThe oracle risks are $R_{2}^{\\star} = \\frac{17}{5}$ and $R_{1}^{\\star} = \\frac{7}{5} \\sqrt{\\frac{2}{\\pi}}$. The final answer is the row matrix:",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{17}{5}  \\frac{7}{5} \\sqrt{\\frac{2}{\\pi}} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Our final practice ventures beyond the typical outlier scenario to explore the behavior of loss functions on more complex data structures. Here, we will analyze a bimodal distribution, where the data is concentrated in two separate regions with a gap of zero probability in between. By deriving the optimal predictors for both $L_2$ and $L_1$ loss, you will uncover the intriguing result that both the mean and the median may fall into this empty \"trough.\" This exercise challenges our assumptions about what a \"good\" prediction looks like and deepens our understanding of how the geometry of the data distribution interacts with our choice of loss function.",
            "id": "3175104",
            "problem": "Consider a fixed covariate value $x_{0}$. You are given the following conditional distribution that is bimodal with a trough of zero density between the modes: with probability $\\frac{1}{2}$, $Y|X=x_{0}$ is uniformly distributed on the interval $[-1,-\\frac{1}{2}]$, and with probability $\\frac{1}{2}$, $Y|X=x_{0}$ is uniformly distributed on the interval $[\\frac{1}{2},1]$. Work within the statistical learning framework in which a predictor $a \\in \\mathbb{R}$ is chosen to minimize a conditional prediction risk at $x_{0}$.\n\n1. Starting from the definition of conditional prediction risk under squared error loss, $R_{2}(a \\,|\\, x_{0}) = \\mathbb{E}\\!\\left[(Y-a)^{2} \\,\\middle|\\, X=x_{0}\\right]$, derive from first principles the optimal predictor $a_{2}^{\\star}(x_{0})$ as the unique minimizer of $R_{2}(a \\,|\\, x_{0})$. Relate the location of $a_{2}^{\\star}(x_{0})$ to the two modes of the conditional distribution $Y|X=x_{0}$.\n\n2. Starting from the definition of conditional prediction risk under absolute error loss, $R_{1}(a \\,|\\, x_{0}) = \\mathbb{E}\\!\\left[|Y-a| \\,\\middle|\\, X=x_{0}\\right]$, derive from first principles the set of minimizers and identify the optimal predictor $a_{1}^{\\star}(x_{0})$ using the tie-breaking rule “choose the smallest median” when the minimizer is not unique. Explain why $a_{1}^{\\star}(x_{0})$ lies in the trough.\n\n3. Compute the exact minimized conditional risks $R_{2}\\!\\left(a_{2}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)$ and $R_{1}\\!\\left(a_{1}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)$ by evaluating the necessary expectations under the given conditional distribution.\n\n4. Provide, in simplest exact form, the ratio\n$$\\frac{R_{1}\\!\\left(a_{1}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)}{R_{2}\\!\\left(a_{2}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)}.$$\n\nFinally, based on your computations, briefly comment (qualitatively) on the predictive risk implications of using squared error loss versus absolute error loss for this bimodal conditional distribution. Your final numerical answer must be the exact ratio as a single simplified fraction with no rounding.",
            "solution": "The problem asks for an analysis of optimal predictors under squared and absolute error loss for a specific bimodal conditional distribution.\n\nFirst, we must validate the problem statement.\n**Step 1: Extract Givens**\n- A fixed covariate value $x_{0}$.\n- The conditional distribution of the random variable $Y$ given $X=x_{0}$ is a mixture distribution:\n  - With probability $\\frac{1}{2}$, $Y|X=x_{0}$ is uniformly distributed on $[-1, -\\frac{1}{2}]$.\n  - With probability $\\frac{1}{2}$, $Y|X=x_{0}$ is uniformly distributed on $[\\frac{1}{2}, 1]$.\n- The task is to find an optimal predictor $a \\in \\mathbb{R}$ that minimizes a conditional prediction risk at $x_{0}$.\n- Part 1 requires deriving the optimal predictor $a_{2}^{\\star}(x_{0})$ for the squared error loss risk, $R_{2}(a \\,|\\, x_{0}) = \\mathbb{E}\\!\\left[(Y-a)^{2} \\,\\middle|\\, X=x_{0}\\right]$.\n- Part 2 requires deriving the optimal predictor $a_{1}^{\\star}(x_{0})$ for the absolute error loss risk, $R_{1}(a \\,|\\, x_{0}) = \\mathbb{E}\\!\\left[|Y-a| \\,\\middle|\\, X=x_{0}\\right]$, using the tie-breaking rule \"choose the smallest median\" if the minimizer is not unique.\n- Part 3 requires computing the minimized risks $R_{2}\\!\\left(a_{2}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)$ and $R_{1}\\!\\left(a_{1}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)$.\n- Part 4 requires computing the ratio of these minimized risks.\n- A final qualitative comment on the implications is requested.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is set within the standard framework of statistical learning theory. The concepts of squared and absolute error loss, conditional expectation, and risk minimization are fundamental. The specified conditional distribution is a valid probability distribution (a mixture of two uniform distributions). The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem provides all necessary definitions, data, and conditions. The objectives for each part are explicit and unambiguous. A tie-breaking rule is provided for the median, ensuring a unique answer for $a_{1}^{\\star}(x_{0})$. The problem is well-posed.\n- **Objective:** The language is formal, precise, and free of any subjective or opinion-based claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nLet $f(y|x_0)$ denote the conditional probability density function (PDF) of $Y$ given $X=x_0$. The distribution is a mixture of two uniform distributions. The PDF of $U[c,d]$ is $\\frac{1}{d-c}$.\nFor the interval $[-1, -\\frac{1}{2}]$, the length is $-\\frac{1}{2}-(-1) = \\frac{1}{2}$, so the PDF is $\\frac{1}{1/2}=2$.\nFor the interval $[\\frac{1}{2}, 1]$, the length is $1-\\frac{1}{2} = \\frac{1}{2}$, so the PDF is $\\frac{1}{1/2}=2$.\nThe mixture PDF is given by the weighted sum of the component PDFs:\n$$f(y|x_0) = \\frac{1}{2} \\cdot 2 \\cdot \\mathbf{1}_{y \\in [-1, -1/2]} + \\frac{1}{2} \\cdot 2 \\cdot \\mathbf{1}_{y \\in [1/2, 1]}$$\nwhere $\\mathbf{1}$ is the indicator function. Thus, the conditional PDF is:\n$$f(y|x_0) = \\begin{cases} 1  \\text{if } y \\in [-1, -1/2] \\\\ 1  \\text{if } y \\in [1/2, 1] \\\\ 0  \\text{otherwise} \\end{cases}$$\n\n**1. Optimal Predictor under Squared Error Loss**\nThe conditional risk is $R_{2}(a \\,|\\, x_{0}) = \\mathbb{E}\\!\\left[(Y-a)^{2} \\,\\middle|\\, X=x_{0}\\right]$. To find the minimizer $a_{2}^{\\star}(x_{0})$, we differentiate $R_{2}$ with respect to $a$ and set the result to zero.\n$$ \\frac{d}{da} R_{2}(a \\,|\\, x_{0}) = \\frac{d}{da} \\mathbb{E}\\!\\left[Y^2 - 2aY + a^2 \\,\\middle|\\, X=x_{0}\\right] $$\nBy linearity of expectation and differentiating under the expectation sign:\n$$ \\frac{d}{da} R_{2}(a \\,|\\, x_{0}) = \\mathbb{E}\\!\\left[\\frac{d}{da}(Y^2 - 2aY + a^2) \\,\\middle|\\, X=x_{0}\\right] = \\mathbb{E}\\!\\left[-2Y + 2a \\,\\middle|\\, X=x_{0}\\right] = 2a - 2\\mathbb{E}[Y|X=x_{0}] $$\nSetting the derivative to zero yields $2a - 2\\mathbb{E}[Y|X=x_{0}] = 0$, which implies $a = \\mathbb{E}[Y|X=x_{0}]$. The second derivative is $\\frac{d^2}{da^2}R_2(a|x_0) = 2 > 0$, confirming this is a unique minimum.\nThus, the optimal predictor is the conditional mean, $a_{2}^{\\star}(x_{0}) = \\mathbb{E}[Y|X=x_{0}]$. We compute this expectation:\n$$ \\mathbb{E}[Y|X=x_{0}] = \\int_{-\\infty}^{\\infty} y f(y|x_0) dy = \\int_{-1}^{-1/2} y \\cdot 1 \\, dy + \\int_{1/2}^{1} y \\cdot 1 \\, dy $$\n$$ \\mathbb{E}[Y|X=x_{0}] = \\left[\\frac{y^2}{2}\\right]_{-1}^{-1/2} + \\left[\\frac{y^2}{2}\\right]_{1/2}^{1} = \\frac{1}{2}\\left(\\left(-\\frac{1}{2}\\right)^2 - (-1)^2\\right) + \\frac{1}{2}\\left(1^2 - \\left(\\frac{1}{2}\\right)^2\\right) $$\n$$ \\mathbb{E}[Y|X=x_{0}] = \\frac{1}{2}\\left(\\frac{1}{4} - 1\\right) + \\frac{1}{2}\\left(1 - \\frac{1}{4}\\right) = \\frac{1}{2}\\left(-\\frac{3}{4}\\right) + \\frac{1}{2}\\left(\\frac{3}{4}\\right) = -\\frac{3}{8} + \\frac{3}{8} = 0 $$\nSo, $a_{2}^{\\star}(x_{0}) = 0$. The two modes of the distribution are the intervals $[-1, -\\frac{1}{2}]$ and $[\\frac{1}{2}, 1]$. The optimal predictor $a_{2}^{\\star}(x_{0})=0$ is the center of symmetry of the distribution and lies exactly in the middle of the trough of zero density that separates the two modes.\n\n**2. Optimal Predictor under Absolute Error Loss**\nThe conditional risk is $R_{1}(a \\,|\\, x_{0}) = \\mathbb{E}\\!\\left[|Y-a| \\,\\middle|\\, X=x_{0}\\right]$. The value(s) of $a$ that minimize this risk are the median(s) of the conditional distribution $Y|X=x_{0}$. A median $m$ is any value such that $P(Y \\le m|X=x_0) \\ge \\frac{1}{2}$ and $P(Y \\ge m|X=x_0) \\ge \\frac{1}{2}$.\nTo find the medians, we first find the conditional cumulative distribution function (CDF), $F(y|x_0) = P(Y \\le y|X=x_0) = \\int_{-\\infty}^{y} f(t|x_0) dt$.\n- For $y  -1$, $F(y|x_0) = 0$.\n- For $-1 \\le y \\le -\\frac{1}{2}$, $F(y|x_0) = \\int_{-1}^{y} 1 \\, dt = y+1$.\n- For $-\\frac{1}{2}  y  \\frac{1}{2}$, $F(y|x_0) = F(-1/2|x_0) + \\int_{-1/2}^{y} 0 \\, dt = (-\\frac{1}{2}+1) + 0 = \\frac{1}{2}$.\n- For $\\frac{1}{2} \\le y \\le 1$, $F(y|x_0) = F(-1/2|x_0) + \\int_{1/2}^{y} 1 \\, dt = \\frac{1}{2} + (y - \\frac{1}{2}) = y$.\n- For $y > 1$, $F(y|x_0) = 1$.\nThe set of medians consists of all values $m$ for which $F(m|x_0) = \\frac{1}{2}$. From the CDF, we see this holds for any $m$ in the interval $[-\\frac{1}{2}, \\frac{1}{2}]$.\nThe minimizer of $R_1(a|x_0)$ is therefore not unique; the set of minimizers is $[-\\frac{1}{2}, \\frac{1}{2}]$. The problem specifies the tie-breaking rule \"choose the smallest median\", so we must select the smallest value in this set.\nThus, the optimal predictor is $a_{1}^{\\star}(x_{0}) = -\\frac{1}{2}$.\nThe trough of zero density is the open interval $(-\\frac{1}{2}, \\frac{1}{2})$. The entire set of medians, $[-\\frac{1}{2}, \\frac{1}{2}]$, is the closure of this trough. Any such median splits the probability mass equally; half the mass is in $[-1, -\\frac{1}{2}]$ and the other half is in $[\\frac{1}{2}, 1]$. Any point between these two regions of support serves as a valid median. Our specific predictor $a_{1}^{\\star}(x_{0})=-\\frac{1}{2}$ lies at the boundary of the trough.\n\n**3. Computation of Minimized Risks**\nFor the squared error loss, with $a_{2}^{\\star}(x_{0}) = 0$:\n$$ R_{2}\\!\\left(a_{2}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right) = \\mathbb{E}\\!\\left[(Y-0)^2 \\,\\middle|\\, X=x_{0}\\right] = \\mathbb{E}[Y^2|X=x_{0}] $$\n$$ \\mathbb{E}[Y^2|X=x_{0}] = \\int_{-1}^{-1/2} y^2 \\cdot 1 \\, dy + \\int_{1/2}^{1} y^2 \\cdot 1 \\, dy $$\nDue to the symmetry of the integrand $y^2$ and the integration domains about $0$, the two integrals are equal:\n$$ \\mathbb{E}[Y^2|X=x_{0}] = 2 \\int_{1/2}^{1} y^2 dy = 2 \\left[\\frac{y^3}{3}\\right]_{1/2}^{1} = 2 \\left(\\frac{1^3}{3} - \\frac{(1/2)^3}{3}\\right) = 2 \\left(\\frac{1}{3} - \\frac{1}{24}\\right) = 2\\left(\\frac{8-1}{24}\\right) = 2\\left(\\frac{7}{24}\\right) = \\frac{14}{24} = \\frac{7}{12} $$\nSo, $R_{2}\\!\\left(a_{2}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right) = \\frac{7}{12}$.\n\nFor the absolute error loss, with $a_{1}^{\\star}(x_{0}) = -\\frac{1}{2}$:\n$$ R_{1}\\!\\left(a_{1}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right) = \\mathbb{E}\\!\\left[\\left|Y - \\left(-\\frac{1}{2}\\right)\\right| \\,\\middle|\\, X=x_{0}\\right] = \\mathbb{E}\\!\\left[\\left|Y + \\frac{1}{2}\\right| \\,\\middle|\\, X=x_{0}\\right] $$\n$$ R_{1} = \\int_{-1}^{-1/2} \\left|y + \\frac{1}{2}\\right| dy + \\int_{1/2}^{1} \\left|y + \\frac{1}{2}\\right| dy $$\nFor $y \\in [-1, -\\frac{1}{2}]$, $y+\\frac{1}{2} \\le 0$, so $|y+\\frac{1}{2}| = -(y+\\frac{1}{2})$. For $y \\in [\\frac{1}{2}, 1]$, $y+\\frac{1}{2} > 0$, so $|y+\\frac{1}{2}| = y+\\frac{1}{2}$.\n$$ R_{1} = \\int_{-1}^{-1/2} -\\left(y+\\frac{1}{2}\\right) dy + \\int_{1/2}^{1} \\left(y+\\frac{1}{2}\\right) dy $$\n$$ R_{1} = \\left[-\\frac{y^2}{2} - \\frac{y}{2}\\right]_{-1}^{-1/2} + \\left[\\frac{y^2}{2} + \\frac{y}{2}\\right]_{1/2}^{1} $$\n$$ R_{1} = \\left(-\\frac{1}{8}+\\frac{1}{4}\\right) - \\left(-\\frac{1}{2}+\\frac{1}{2}\\right) + \\left(\\frac{1}{2}+\\frac{1}{2}\\right) - \\left(\\frac{1}{8}+\\frac{1}{4}\\right) = \\frac{1}{8} - 0 + 1 - \\frac{3}{8} = 1 - \\frac{2}{8} = 1 - \\frac{1}{4} = \\frac{3}{4} $$\nSo, $R_{1}\\!\\left(a_{1}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right) = \\frac{3}{4}$.\n\n**4. Ratio of Minimized Risks**\n$$ \\frac{R_{1}\\!\\left(a_{1}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)}{R_{2}\\!\\left(a_{2}^{\\star}(x_{0}) \\,\\middle|\\, x_{0}\\right)} = \\frac{3/4}{7/12} = \\frac{3}{4} \\cdot \\frac{12}{7} = \\frac{3 \\cdot 3}{7} = \\frac{9}{7} $$\n\n**Qualitative Comment**\nFor this bimodal conditional distribution with a wide separation between modes, the squared error loss and absolute error loss yield optimal predictors that fall in the central trough of zero probability density. The squared error predictor, $a_{2}^{\\star}(x_0)=0$, is the conditional mean. It is a \"compromise\" predictor located at the center of mass of the distribution, equally far from both modes, a consequence of the squared penalty on large errors. The absolute error predictor, $a_{1}^{\\star}(x_0)$, is a conditional median. The set of medians is the entire interval $[-\\frac{1}{2}, \\frac{1}{2}]$, demonstrating that any point that splits the probability mass in half is an optimal solution, regardless of its distance to the modes.\nThe critical implication is that for such multimodal distributions, both loss functions can result in predictors that are not representative of any actual data cluster. The mean is forced into a \"no man's land\" by the pull from both modes, while the median is indeterminate within this region, highlighting the different sensitivities of these loss functions to the distribution's geometry.",
            "answer": "$$\\boxed{\\frac{9}{7}}$$"
        }
    ]
}