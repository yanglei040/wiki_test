{
    "hands_on_practices": [
        {
            "introduction": "Semi-supervised learning often relies on a key heuristic known as the cluster assumption, which posits that decision boundaries should not pass through high-density regions of the input space. This exercise challenges you to think critically about this foundational principle by exploring a scenario where it is violated. By analyzing how an entropy minimization regularizer behaves in this context, you will develop a deeper intuition for the failure modes of common semi-supervised methods and the importance of aligning algorithmic assumptions with data characteristics .",
            "id": "3162680",
            "problem": "You are given a semi-supervised classification setting with a small labeled set drawn from a binary distribution and a large unlabeled set drawn from the same marginal distribution of inputs. Consider methods that add an unlabeled-data regularization encouraging low prediction uncertainty, commonly operationalized by minimizing the expected entropy of the predictive distribution on unlabeled inputs. Let entropy be defined for a categorical distribution $q(y \\mid x)$ over $K$ classes as $H(q(y \\mid x)) = - \\sum_{k=1}^{K} q_k(y \\mid x) \\log q_k(y \\mid x)$, and recall the cluster assumption: decision boundaries should lie in regions where the marginal input density $p(x)$ is low. Also recall the Bayes decision rule: a classifier that predicts $\\hat{y}(x) = \\arg\\max_{y} p(y \\mid x)$ minimizes the expected $0$-$1$ loss.\n\nWhich option correctly constructs a scientifically consistent dataset and explains why entropy minimization on unlabeled data can push the learned decision boundary away from the Bayes decision rule, specifically due to a violation of the cluster assumption where the posterior $p(y \\mid x)$ changes across a region of high marginal density?\n\nA. Inputs $x \\in \\mathbb{R}^{2}$ are drawn from a single isotropic Gaussian $p(x) = \\mathcal{N}(x; 0, \\sigma^{2} I_{2})$ with $\\sigma^{2} > 0$. Labels are deterministic per-half-space: $p(y=1 \\mid x) = 1$ if $x_1 \\ge 0$ and $p(y=1 \\mid x) = 0$ if $x_1  0$ (so $p(y=0 \\mid x) = 1 - p(y=1 \\mid x)$). The Bayes decision boundary is the vertical line $x_1 = 0$, which passes through the highest-density region near $x = 0$. Entropy minimization encourages uniformly confident predictions on the high-density unlabeled inputs and thus may push the boundary far into the low-density tails to avoid uncertain predictions near $x_1 = 0$, yielding a boundary that is inconsistent with the Bayes rule and causing large classification error on one half-space.\n\nB. Inputs $x \\in \\mathbb{R}^{2}$ are drawn from a mixture of two well-separated Gaussians on the $x_{1}$ axis: $p(x) = \\tfrac{1}{2}\\,\\mathcal{N}(x; (-m, 0), \\sigma^{2} I_{2}) + \\tfrac{1}{2}\\,\\mathcal{N}(x; (m, 0), \\sigma^{2} I_{2})$ with $m \\gg \\sigma  0$. Labels follow cluster membership: $p(y=0 \\mid x)$ is high on the left cluster and $p(y=1 \\mid x)$ is high on the right cluster. The Bayes boundary lies between clusters (near $x_1 = 0$), a low-density region. Entropy minimization aligns the boundary with this density valley, improving classification.\n\nC. Inputs $x \\in \\mathbb{R}^{2}$ have a radial bimodal marginal: the distribution of $r = \\lVert x \\rVert_{2}$ has two modes at $r = r_{1}$ and $r = r_{2}$ with $r_{1}  r_{2}$, and density is low in the annulus between these radii. Labels are radial-thresholded: $p(y=0 \\mid x)$ is high for $r \\approx r_{1}$ and $p(y=1 \\mid x)$ is high for $r \\approx r_{2}$. The Bayes decision boundary is the annulus of low density between rings, so entropy minimization encourages a boundary in the low-density gap, supporting correct decisions.\n\nD. Inputs $x \\in \\mathbb{R}^{2}$ are drawn from a heavy-tailed but unimodal distribution centered at $0$, for example a product Laplace $p(x) \\propto \\exp(-\\lvert x_1 \\rvert / b - \\lvert x_2 \\rvert / b)$ with $b  0$. Labels are radial: $p(y=1 \\mid x) = 1$ if $\\lVert x \\rVert_{2} \\le R$ and $p(y=1 \\mid x) = 0$ otherwise, where $R  0$. The Bayes boundary is the circle $\\lVert x \\rVert_{2} = R$, which lies in a moderate-density region but not near the peak at $x = 0$. Entropy minimization does not typically push the boundary away from the Bayes solution here, because the posterior flip does not occur at the highest-density point; there is no strong violation of the cluster assumption.\n\nChoose the best option and justify your choice by reasoning from the definitions of the Bayes decision rule, entropy, and the cluster assumption.",
            "solution": "The user requires an analysis of a problem in statistical learning concerning semi-supervised learning, specifically the potential failure of entropy minimization regularization when the cluster assumption is violated. I will first validate the problem statement and then proceed to a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Setting**: Semi-supervised classification.\n-   **Data**: A small labeled set and a large unlabeled set, both drawn from the same marginal distribution of inputs $p(x)$. The labeled set is from a binary distribution.\n-   **Method**: An unlabeled-data regularizer that encourages low prediction uncertainty, operationalized by minimizing the expected entropy of the predictive distribution $q(y \\mid x)$ on unlabeled inputs.\n-   **Entropy Definition**: For a categorical distribution $q(y \\mid x)$ over $K$ classes, the entropy is $H(q(y \\mid x)) = - \\sum_{k=1}^{K} q_k(y \\mid x) \\log q_k(y \\mid x)$.\n-   **Cluster Assumption**: Decision boundaries should lie in regions where the marginal input density $p(x)$ is low.\n-   **Bayes Decision Rule**: The classifier that predicts $\\hat{y}(x) = \\arg\\max_{y} p(y \\mid x)$ minimizes the expected $0-1$ loss.\n-   **Question**: Identify which option correctly constructs a dataset and explains why entropy minimization on unlabeled data can push the learned decision boundary away from the Bayes decision rule, due to a violation of the cluster assumption (i.e., the posterior $p(y \\mid x)$ changes across a region of high marginal density $p(x)$).\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem statement is firmly rooted in established concepts of statistical learning theory. Semi-supervised learning, entropy minimization (a form of uncertainty regularization), the cluster assumption, and the Bayes decision rule are all standard and well-defined topics. The premise that a mismatch between the data distribution and the cluster assumption can cause semi-supervised methods to fail is a known and important issue in the field.\n2.  **Well-Posed**: The problem is well-posed. It asks to identify the single best option among four scenarios that illustrates a specific, well-described phenomenon. The existence of a correct answer can be determined through logical deduction based on the provided definitions.\n3.  **Objective**: The language used is precise, technical, and objective. It avoids any subjective or ambiguous terminology.\n4.  **Completeness and Consistency**: The problem is self-contained. It provides all necessary definitions ($H(q(y \\mid x))$, Bayes rule, cluster assumption) and context to evaluate the options. There are no internal contradictions.\n5.  **Other Flaws**: The problem does not exhibit any other flaws. It is not trivial, as it requires understanding the interplay between the marginal data density $p(x)$, the posterior probability $p(y \\mid x)$, and the behavior of the entropy regularizer.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed with deriving the solution and evaluating each option.\n\n### Solution Derivation\n\nThe core of the problem lies in understanding the conflict between the objective of the supervised loss and the objective of the entropy regularization term in a specific setting.\n\n-   The supervised loss term (e.g., cross-entropy on labeled data) encourages the learned predictive distribution $q(y \\mid x)$ to match the true posterior $p(y \\mid x)$. The decision boundary for such a model would ideally converge to the Bayes decision boundary, which is the locus of points where $p(y=1 \\mid x) = p(y=0 \\mid x) = 0.5$ (for a binary problem).\n-   The entropy regularization term, $\\mathbb{E}_{x \\sim p_{unlabeled}(x)}[H(q(y \\mid x))]$, encourages the model to be confident (low entropy) in its predictions for the unlabeled data. A prediction is low-entropy if $q(y \\mid x)$ is close to a one-hot vector (e.g., $q(y=1 \\mid x)$ is close to $0$ or $1$). The entropy is maximized where the model is least certain, i.e., $q(y=1 \\mid x) = 0.5$.\n-   The **cluster assumption** posits that the decision boundary (where uncertainty should be highest) lies in a low-density region of $p(x)$. If this holds, there are few unlabeled points near the boundary, so the entropy regularizer does not exert a strong influence there, and the two objectives align well.\n-   A **violation of the cluster assumption** occurs when the decision boundary lies in a high-density region of $p(x)$. In this case, many unlabeled points are near the boundary. The entropy regularizer will strongly penalize the model for being uncertain on these points, pushing the model's decision boundary away from this high-density region and into a low-density one, even if this contradicts the true Bayes decision boundary indicated by the (scarce) labeled data.\n\nWe need to find the option that describes this failure mode.\n\n**Option-by-Option Analysis**\n\n**A. Inputs $x \\in \\mathbb{R}^{2}$ are drawn from a single isotropic Gaussian $p(x) = \\mathcal{N}(x; 0, \\sigma^{2} I_{2})$ with $\\sigma^{2}  0$. Labels are deterministic per-half-space: $p(y=1 \\mid x) = 1$ if $x_{1} \\ge 0$ and $p(y=1 \\mid x) = 0$ if $x_{1}  0$. The Bayes decision boundary is the vertical line $x_{1} = 0$, which passes through the highest-density region near $x = 0$. Entropy minimization encourages uniformly confident predictions on the high-density unlabeled inputs and thus may push the boundary far into the low-density tails to avoid uncertain predictions near $x_{1} = 0$, yielding a boundary that is inconsistent with the Bayes rule and causing large classification error on one half-space.**\n\n-   **Data Distribution**: The marginal density $p(x)$ is a single Gaussian centered at the origin, $x=(0,0)$. The highest density is at the center.\n-   **Bayes Boundary**: The true posterior $p(y \\mid x)$ jumps from $0$ to $1$ at the line $x_1 = 0$. The Bayes-optimal decision boundary is therefore $x_1 = 0$.\n-   **Cluster Assumption**: This boundary, $x_1=0$, passes directly through the mode of the data distribution at $x=(0,0)$. This is a severe violation of the cluster assumption, as the decision boundary crosses the region of highest marginal density.\n-   **Effect of Entropy Minimization**: A large number of unlabeled points will be concentrated around the origin, and thus near the true boundary $x_1 = 0$. A classifier attempting to model this boundary will necessarily have high uncertainty (predictive probability close to $0.5$) for points near $x_1=0$. The entropy regularizer, acting on the dense unlabeled data in this region, will create a strong incentive for the model to \"move\" its decision boundary to a place where there are fewer data points, i.e., into the low-density tails of the Gaussian. For instance, the model might learn a boundary $x_1=c$ for some large $|c|$, or a curved boundary far from the origin. This modification achieves low entropy for the vast majority of unlabeled points but places the boundary far from the true one, leading to poor generalization performance.\n-   **Verdict**: This option correctly describes a dataset that violates the cluster assumption and provides a scientifically sound explanation for why entropy minimization would lead to a suboptimal classifier, pushing the learned boundary away from the Bayes-optimal one. **Correct**.\n\n**B. Inputs $x \\in \\mathbb{R}^{2}$ are drawn from a mixture of two well-separated Gaussians on the $x_{1}$ axis: $p(x) = \\tfrac{1}{2}\\,\\mathcal{N}(x; (-m, 0), \\sigma^{2} I_{2}) + \\tfrac{1}{2}\\,\\mathcal{N}(x; (m, 0), \\sigma^{2} I_{2})$ with $m \\gg \\sigma  0$. Labels follow cluster membership: $p(y=0 \\mid x)$ is high on the left cluster and $p(y=1 \\mid x)$ is high on the right cluster. The Bayes boundary lies between clusters (near $x_{1} = 0$), a low-density region. Entropy minimization aligns the boundary with this density valley, improving classification.**\n\n-   **Data Distribution**: The marginal density $p(x)$ is high in two clusters and low in the region between them (a density \"valley\").\n-   **Bayes Boundary**: The boundary separating the two classes lies in the valley near $x_1=0$.\n-   **Cluster Assumption**: The decision boundary lies in a low-density region. The cluster assumption is *satisfied*.\n-   **Effect of Entropy Minimization**: Entropy minimization helps the model by encouraging it to place the decision boundary in the low-density valley, which is consistent with the Bayes-optimal solution. This is the canonical example where semi-supervised learning *works well*.\n-   **Verdict**: This option describes a situation where entropy minimization is beneficial, which is contrary to the phenomenon the question asks to illustrate. **Incorrect**.\n\n**C. Inputs $x \\in \\mathbb{R}^{2}$ have a radial bimodal marginal: the distribution of $r = \\lVert x \\rVert_{2}$ has two modes at $r = r_{1}$ and $r = r_{2}$ with $r_{1}  r_{2}$, and density is low in the annulus between these radii. Labels are radial-thresholded: $p(y=0 \\mid x)$ is high for $r \\approx r_{1}$ and $p(y=1 \\mid x)$ is high for $r \\approx r_{2}$. The Bayes decision boundary is the annulus of low density between rings, so entropy minimization encourages a boundary in the low-density gap, supporting correct decisions.**\n\n-   **Data Distribution**: The data forms two concentric high-density rings with a low-density gap between them.\n-   **Bayes Boundary**: The decision boundary will be a circle in the low-density annulus separating the two rings.\n-   **Cluster Assumption**: The decision boundary lies in a low-density region. The cluster assumption is *satisfied*.\n-   **Effect of Entropy Minimization**: Similar to option B, entropy minimization would push the decision boundary into the low-density gap, which aligns with the Bayes-optimal solution. This is another example where semi-supervised learning is expected to succeed.\n-   **Verdict**: This option describes a situation where entropy minimization is beneficial, not harmful. **Incorrect**.\n\n**D. Inputs $x \\in \\mathbb{R}^{2}$ are drawn from a heavy-tailed but unimodal distribution centered at $0$, for example a product Laplace $p(x) \\propto \\exp(-\\lvert x_{1} \\rvert / b - \\lvert x_{2} \\rvert / b)$ with $b  0$. Labels are radial: $p(y=1 \\mid x) = 1$ if $\\lVert x \\rVert_{2} \\le R$ and $p(y=1 \\mid x) = 0$ otherwise, where $R  0$. The Bayes boundary is the circle $\\lVert x \\rVert_{2} = R$, which lies in a moderate-density region but not near the peak at $x = 0$. Entropy minimization does not typically push the boundary away from the Bayes solution here, because the posterior flip does not occur at the highest-density point; there is no strong violation of the cluster assumption.**\n\n-   **Data Distribution**: Unimodal distribution, with density peaking at the origin.\n-   **Bayes Boundary**: The boundary is a circle of radius $R$.\n-   **Cluster Assumption**: The boundary $\\|x\\|_2 = R$ (for $R0$) does not pass through the point of maximum density ($x=0$). However, it does pass through a region that can have a substantial density, so the cluster assumption is still violated, though perhaps not as severely as in Option A where the boundary crosses the mode.\n-   **Effect of Entropy Minimization**: The option itself *argues against* this being a good example of the failure mode. It claims that entropy minimization does not \"typically push the boundary away\". While the reasoning provided is brief, the option's conclusion is that this scenario does not illustrate the desired failure mechanism. The question asks for an option that *constructs a dataset and explains why* the failure occurs. This option constructs a dataset and then argues the failure is not prominent.\n-   **Verdict**: This option does not fulfill the core task of explaining why the boundary is pushed away. It explicitly states the contrary. **Incorrect**.\n\n**Conclusion**\n\nOption A provides the clearest and most accurate example of the scenario described in the prompt. It sets up a data distribution and a classification task that result in a strong violation of the cluster assumption, where the Bayes-optimal decision boundary passes through the region of highest data density. It then correctly explains that the entropy minimization regularizer, in this case, will conflict with the supervised objective and push the learned decision boundary into a low-density region far from the optimal one, thus degrading performance.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After exploring the conceptual underpinnings, let's dissect the mechanics of a classic semi-supervised algorithm: self-training. This practice places you in a simplified, yet powerful, theoretical setting of a Gaussian mixture model to analyze the core feedback loop where a model iteratively refines its parameters using its own predictions on unlabeled data. Deriving the update equations and analyzing their fixed points provides a rigorous, quantitative understanding of the dynamics that drive this form of learning, moving from a high-level idea to a concrete mathematical formulation .",
            "id": "3162593",
            "problem": "Consider a semi-supervised self-training procedure in statistical learning that combines ideas from supervised learning (labeled data), unsupervised learning (unlabeled data), and semi-supervised learning (jointly leveraging labeled and unlabeled data). A one-dimensional feature $x \\in \\mathbb{R}$ is generated by one of two classes, $C_{1}$ or $C_{2}$, with class-conditional distributions $x \\mid C_{k} \\sim \\mathcal{N}(\\mu_{k}, \\sigma^{2})$ for $k \\in \\{1,2\\}$, equal and known variance $\\sigma^{2}$, and equal class priors $P(C_{1}) = P(C_{2}) = \\frac{1}{2}$. You are given small labeled sets from each class producing initial sample mean estimates $m_{1}^{(0)}$ and $m_{2}^{(0)}$. You are also given a large unlabeled sample drawn independently from the mixture with the above specification.\n\nAt iteration $t$, the self-training procedure is defined as follows:\n- A decision threshold $\\tau^{(t)}$ is set to the midpoint of the current class mean estimates: $\\tau^{(t)} = \\frac{1}{2}\\big(m_{1}^{(t)} + m_{2}^{(t)}\\big)$.\n- Every unlabeled point $x$ is pseudo-labeled by a plug-in classifier: assign $C_1$ if $x \\le \\tau^{(t)}$ and $C_2$ if $x > \\tau^{(t)}$.\n- Class mean estimates are updated by a convex combination of the previous estimates and the means of the pseudo-labeled partitions:\n$$\nm_{1}^{(t+1)} = \\lambda\\, m_{1}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X \\le \\tau^{(t)}\\right], \\quad\nm_{2}^{(t+1)} = \\lambda\\, m_{2}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X  \\tau^{(t)}\\right],\n$$\nwhere $X$ denotes a random variable with the unlabeled mixture distribution and $\\lambda \\in (0,1)$ is a fixed weight reflecting the relative influence of labeled data.\n\nStarting from the definitions of conditional expectation and the properties of the normal distribution, derive the following in terms of the normal Probability Density Function (PDF) $\\phi$ and Cumulative Distribution Function (CDF) $\\Phi$:\n1. Closed-form expressions for $\\mathbb{E}\\!\\left[X \\,\\middle|\\, X \\le \\tau\\right]$ and $\\mathbb{E}\\!\\left[X \\,\\middle|\\, X  \\tau\\right]$ under the given mixture model with equal priors.\n2. The corresponding iterative update equations for $m_{1}^{(t+1)}$, $m_{2}^{(t+1)}$, and $\\tau^{(t+1)}$.\n\nFinally, specialize to the symmetric case with $\\mu_{1} = -\\mu$, $\\mu_{2} = \\mu$, where $\\mu  0$. Determine the fixed-point threshold $\\tau^{\\star}$ satisfying $\\tau^{\\star} = \\frac{1}{2}(m_{1}^{\\star} + m_{2}^{\\star})$ and $m_{k}^{\\star} = \\lambda m_{k}^{\\star} + (1-\\lambda) \\mathbb{E}[X \\mid \\text{pseudo-label indicates } C_k \\text{ at } \\tau^{\\star}]$ for $k \\in \\{1,2\\}$, and provide its exact value. No rounding is required. The final answer must be a single real number.",
            "solution": "The user wants me to solve a problem about a semi-supervised self-training algorithm.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Feature Space**: a one-dimensional feature $x \\in \\mathbb{R}$.\n-   **Classes**: $C_{1}$ and $C_{2}$.\n-   **Class-Conditional Distributions**: $x \\mid C_{k} \\sim \\mathcal{N}(\\mu_{k}, \\sigma^{2})$ for $k \\in \\{1,2\\}$.\n-   **Variance**: $\\sigma^{2}$ is equal for both classes and known.\n-   **Class Priors**: $P(C_{1}) = P(C_{2}) = \\frac{1}{2}$.\n-   **Initial Data**: Small labeled sets provide initial mean estimates $m_{1}^{(0)}$ and $m_{2}^{(0)}$. A large unlabeled sample is drawn from the specified mixture distribution.\n-   **Self-Training Procedure at Iteration $t$**:\n    -   Decision threshold: $\\tau^{(t)} = \\frac{1}{2}\\big(m_{1}^{(t)} + m_{2}^{(t)}\\big)$.\n    -   Pseudo-labeling rule: Assign class $C_{1}$ if $x \\le \\tau^{(t)}$ and $C_{2}$ if $x  \\tau^{(t)}$.\n    -   Mean update equations:\n        $$\n        m_{1}^{(t+1)} = \\lambda\\, m_{1}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X \\le \\tau^{(t)}\\right]\n        $$\n        $$\n        m_{2}^{(t+1)} = \\lambda\\, m_{2}^{(t)} + (1-\\lambda)\\, \\mathbb{E}\\!\\left[X \\,\\middle|\\, X  \\tau^{(t)}\\right]\n        $$\n    -   Weight parameter: $\\lambda \\in (0,1)$.\n    -   Random variable $X$: drawn from the unlabeled mixture distribution.\n-   **Tasks**:\n    1.  Derive closed-form expressions for $\\mathbbE[X \\mid X \\le \\tau]$ and $\\mathbb{E}[X \\mid X > \\tau]$ in terms of the standard normal PDF $\\phi$ and CDF $\\Phi$.\n    2.  Derive the iterative update equations for $m_{1}^{(t+1)}$, $m_{2}^{(t+1)}$, and $\\tau^{(t+1)}$.\n    3.  For the symmetric case $\\mu_{1} = -\\mu$ and $\\mu_{2} = \\mu$ ($\\mu  0$), determine the exact value of the fixed-point threshold $\\tau^{\\star}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is well-established within the field of statistical learning, dealing with a variant of self-training for semi-supervised learning. The concepts (Gaussian mixture models, conditional expectation, iterative estimation) are standard and scientifically sound.\n-   **Well-Posed**: The problem is clearly defined with all necessary components to derive the requested quantities and analyze the fixed point. A unique solution for the fixed-point threshold is plausible under the specified symmetry.\n-   **Objective**: The problem is stated using formal mathematical language, free from any subjective or ambiguous terminology.\n-   **Conclusion**: The problem does not violate any of the invalidity criteria. It is a valid, solvable problem in statistical learning theory.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the solution.\n\n### Solution\n\nThe random variable $X$ is drawn from a mixture of two normal distributions. Its probability density function (PDF), $f_X(x)$, is given by the law of total probability:\n$$f_X(x) = P(C_1) f_{X|C_1}(x) + P(C_2) f_{X|C_2}(x)$$\nLet $\\phi_{\\mu, \\sigma^2}(x)$ be the PDF of a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$. We have $\\phi_{\\mu, \\sigma^2}(x) = \\frac{1}{\\sigma} \\phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$, where $\\phi(z)$ is the standard normal PDF. Given $P(C_1)=P(C_2)=\\frac{1}{2}$, the mixture PDF is:\n$$f_X(x) = \\frac{1}{2} \\phi_{\\mu_1, \\sigma^2}(x) + \\frac{1}{2} \\phi_{\\mu_2, \\sigma^2}(x)$$\nThe cumulative distribution function (CDF) of $X$ is $F_X(\\tau) = P(X \\le \\tau) = \\int_{-\\infty}^{\\tau} f_X(x) dx$. Let $\\Phi(z)$ be the standard normal CDF.\n$$F_X(\\tau) = \\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) + \\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)$$\n\n**1. Closed-form expressions for conditional expectations**\n\nWe first derive the expression for the partial expectation of a normal variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n$$ \\int_{-\\infty}^{a} y \\, \\phi_{\\mu, \\sigma^2}(y) dy = \\int_{-\\infty}^{a} y \\frac{1}{\\sigma}\\phi\\left(\\frac{y-\\mu}{\\sigma}\\right) dy $$\nLet $z = \\frac{y-\\mu}{\\sigma}$, so $y = \\mu + \\sigma z$ and $dy=\\sigma dz$. The upper integration limit becomes $z_a = \\frac{a-\\mu}{\\sigma}$.\n$$ \\int_{-\\infty}^{z_a} (\\mu + \\sigma z) \\phi(z) dz = \\mu \\int_{-\\infty}^{z_a} \\phi(z) dz + \\sigma \\int_{-\\infty}^{z_a} z \\phi(z) dz $$\nThe first integral is $\\mu \\Phi(z_a)$. For the second, since $\\frac{d}{dz}\\phi(z) = -z\\phi(z)$, we have $\\int z\\phi(z)dz = -\\phi(z)$.\n$$ \\mu \\Phi(z_a) + \\sigma [-\\phi(z)]_{-\\infty}^{z_a} = \\mu \\Phi(z_a) - \\sigma \\phi(z_a) $$\nSo, $\\int_{-\\infty}^{a} y \\, \\phi_{\\mu, \\sigma^2}(y) dy = \\mu \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{a-\\mu}{\\sigma}\\right)$.\n\nThe conditional expectation $\\mathbb{E}[X \\mid X \\le \\tau]$ is given by:\n$$ \\mathbb{E}[X \\mid X \\le \\tau] = \\frac{\\int_{-\\infty}^{\\tau} x f_X(x) dx}{P(X \\le \\tau)} = \\frac{\\frac{1}{2} \\int_{-\\infty}^{\\tau} x \\phi_{\\mu_1, \\sigma^2}(x) dx + \\frac{1}{2} \\int_{-\\infty}^{\\tau} x \\phi_{\\mu_2, \\sigma^2}(x) dx}{\\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) + \\frac{1}{2} \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)} $$\nUsing the partial expectation formula and simplifying, we get:\n$$ \\mathbb{E}[X \\mid X \\le \\tau] = \\frac{\\left[\\mu_1 \\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right] + \\left[\\mu_2 \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right]}{\\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right) + \\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)} $$\n\nFor the second conditional expectation, $\\mathbb{E}[X \\mid X  \\tau]$, we use the fact that $\\mathbb{E}[Y] = \\int_{-\\infty}^{\\infty} y \\phi_{\\mu,\\sigma^2}(y)dy = \\mu$.\n$$ \\int_{\\tau}^{\\infty} y \\phi_{\\mu, \\sigma^2}(y) dy = \\mathbb{E}[Y] - \\int_{-\\infty}^{\\tau} y \\phi_{\\mu, \\sigma^2}(y) dy = \\mu - \\left[\\mu \\Phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right) - \\sigma \\phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right)\\right] $$\n$$ = \\mu \\left(1 - \\Phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right)\\right) + \\sigma \\phi\\left(\\frac{\\tau-\\mu}{\\sigma}\\right) $$\nThe conditional expectation is:\n$$ \\mathbb{E}[X \\mid X  \\tau] = \\frac{\\int_{\\tau}^{\\infty} x f_X(x) dx}{P(X  \\tau)} = \\frac{\\frac{1}{2} \\int_{\\tau}^{\\infty} x \\phi_{\\mu_1, \\sigma^2}(x) dx + \\frac{1}{2} \\int_{\\tau}^{\\infty} x \\phi_{\\mu_2, \\sigma^2}(x) dx}{\\frac{1}{2} P(X  \\tau|C_1) + \\frac{1}{2} P(X  \\tau|C_2)} $$\n$$ \\mathbb{E}[X \\mid X  \\tau] = \\frac{\\left[\\mu_1\\left(1-\\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right) + \\sigma\\phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right] + \\left[\\mu_2\\left(1-\\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right) + \\sigma\\phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right]}{\\left(1-\\Phi\\left(\\frac{\\tau-\\mu_1}{\\sigma}\\right)\\right) + \\left(1-\\Phi\\left(\\frac{\\tau-\\mu_2}{\\sigma}\\right)\\right)} $$\n\nTo simplify notation, let $z_k = \\frac{\\tau-\\mu_k}{\\sigma}$, $\\Phi_k = \\Phi(z_k)$, and $\\phi_k = \\phi(z_k)$.\n$$ \\mathbb{E}[X \\mid X \\le \\tau] = \\frac{\\mu_1 \\Phi_1 + \\mu_2 \\Phi_2 - \\sigma(\\phi_1 + \\phi_2)}{\\Phi_1 + \\Phi_2} $$\n$$ \\mathbb{E}[X \\mid X  \\tau] = \\frac{\\mu_1(1-\\Phi_1) + \\mu_2(1-\\Phi_2) + \\sigma(\\phi_1 + \\phi_2)}{2 - \\Phi_1 - \\Phi_2} $$\n\n**2. Iterative update equations**\n\nThe iterative update equations are obtained by substituting the expressions from Part 1 into the given recurrence relations. The threshold at iteration $t$ is $\\tau^{(t)} = \\frac{1}{2}(m_{1}^{(t)} + m_{2}^{(t)})$. The expectation is taken with respect to the true mixture distribution, so we use the true parameters $\\mu_k$ and $\\sigma$.\nLet $z_k^{(t)} = \\frac{\\tau^{(t)}-\\mu_k}{\\sigma}$, $\\Phi_k^{(t)} = \\Phi(z_k^{(t)})$, and $\\phi_k^{(t)} = \\phi(z_k^{(t)})$.\n\nThe update for $m_1$ is:\n$$ m_{1}^{(t+1)} = \\lambda\\, m_{1}^{(t)} + (1-\\lambda)\\,\\left[\\frac{\\mu_1 \\Phi_1^{(t)} + \\mu_2 \\Phi_2^{(t)} - \\sigma(\\phi_1^{(t)} + \\phi_2^{(t)})}{\\Phi_1^{(t)} + \\Phi_2^{(t)}}\\right] $$\nThe update for $m_2$ is:\n$$ m_{2}^{(t+1)} = \\lambda\\, m_{2}^{(t)} + (1-\\lambda)\\,\\left[\\frac{\\mu_1(1-\\Phi_1^{(t)}) + \\mu_2(1-\\Phi_2^{(t)}) + \\sigma(\\phi_1^{(t)} + \\phi_2^{(t)})}{2 - \\Phi_1^{(t)} - \\Phi_2^{(t)}}\\right] $$\nThe update for the threshold is:\n$$ \\tau^{(t+1)} = \\frac{1}{2} \\left( m_{1}^{(t+1)} + m_{2}^{(t+1)} \\right) $$\n\n**3. Fixed-point threshold in the symmetric case**\n\nWe consider the symmetric case where $\\mu_1 = -\\mu$ and $\\mu_2 = \\mu$ for some $\\mu  0$.\nA fixed point $(m_1^{\\star}, m_2^{\\star}, \\tau^{\\star})$ satisfies the update equations with all time superscripts removed. The condition for the means, since $\\lambda \\in (0,1)$, simplifies to:\n$$ m_{1}^{\\star} = \\mathbb{E}[X \\mid X \\le \\tau^{\\star}] $$\n$$ m_{2}^{\\star} = \\mathbb{E}[X \\mid X  \\tau^{\\star}] $$\nAnd the threshold is given by:\n$$ \\tau^{\\star} = \\frac{1}{2}\\left(m_{1}^{\\star} + m_{2}^{\\star}\\right) $$\nSubstituting the mean equations into the threshold equation gives a self-consistency condition for $\\tau^{\\star}$:\n$$ \\tau^{\\star} = \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le \\tau^{\\star}] + \\mathbb{E}[X \\mid X  \\tau^{\\star}] \\right) $$\nLet's define the function $H(\\tau) = \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le \\tau] + \\mathbb{E}[X \\mid X  \\tau] \\right)$. We seek fixed points $\\tau^{\\star}$ such that $\\tau^{\\star} = H(\\tau^{\\star})$.\n\nIn the symmetric case, the mixture PDF $f_X(x)$ is an even function:\n$$ f_X(-x) = \\frac{1}{2} \\phi_{-\\mu, \\sigma^2}(-x) + \\frac{1}{2} \\phi_{\\mu, \\sigma^2}(-x) = \\frac{1}{2\\sigma}\\phi\\left(\\frac{-x-(-\\mu)}{\\sigma}\\right) + \\frac{1}{2\\sigma}\\phi\\left(\\frac{-x-\\mu}{\\sigma}\\right) $$\n$$ = \\frac{1}{2\\sigma}\\phi\\left(\\frac{-(x-\\mu)}{\\sigma}\\right) + \\frac{1}{2\\sigma}\\phi\\left(\\frac{-(x+\\mu)}{\\sigma}\\right) = \\frac{1}{2\\sigma}\\phi\\left(\\frac{x-\\mu}{\\sigma}\\right) + \\frac{1}{2\\sigma}\\phi\\left(\\frac{x+\\mu}{\\sigma}\\right) $$\n$$ = \\frac{1}{2} \\phi_{\\mu, \\sigma^2}(x) + \\frac{1}{2} \\phi_{-\\mu, \\sigma^2}(x) = f_X(x) $$\nNow, let's analyze the function $H(\\tau)$. Consider $H(-\\tau)$:\n$$ H(-\\tau) = \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le -\\tau] + \\mathbb{E}[X \\mid X  -\\tau] \\right) $$\nLet's evaluate the components. Let $Y = -X$. The PDF of $Y$ is $f_Y(y) = f_X(-y) = f_X(y)$, so $Y$ has the same distribution as $X$.\n$$ \\mathbb{E}[X \\mid X \\le -\\tau] = \\mathbb{E}[-Y \\mid -Y \\le -\\tau] = \\mathbb{E}[-Y \\mid Y \\ge \\tau] = -\\mathbb{E}[Y \\mid Y \\ge \\tau] = -\\mathbb{E}[X \\mid X \\ge \\tau] $$\n$$ \\mathbb{E}[X \\mid X  -\\tau] = \\mathbb{E}[-Y \\mid -Y  -\\tau] = \\mathbb{E}[-Y \\mid Y  \\tau] = -\\mathbb{E}[Y \\mid Y  \\tau] = -\\mathbb{E}[X \\mid X  \\tau] $$\nNote that for continuous variables, $\\mathbb{E}[X \\mid X  \\tau] = \\mathbb{E}[X \\mid X \\le \\tau]$ and $\\mathbb{E}[X \\mid X \\ge \\tau] = \\mathbb{E}[X \\mid X  \\tau]$.\nSo, we have shown $\\mathbb{E}[X \\mid X \\le -\\tau] = -\\mathbb{E}[X \\mid X  \\tau]$ and $\\mathbb{E}[X \\mid X  -\\tau] = -\\mathbb{E}[X \\mid X \\le \\tau]$.\nSubstituting these into the expression for $H(-\\tau)$:\n$$ H(-\\tau) = \\frac{1}{2}\\left( -\\mathbb{E}[X \\mid X  \\tau] - \\mathbb{E}[X \\mid X \\le \\tau] \\right) = - \\frac{1}{2}\\left( \\mathbb{E}[X \\mid X \\le \\tau] + \\mathbb{E}[X \\mid X  \\tau] \\right) = -H(\\tau) $$\nThus, $H(\\tau)$ is an odd function.\nThe fixed-point equation is $\\tau = H(\\tau)$. For $\\tau=0$, we have $H(0) = -H(-0) = -H(0)$, which implies $2H(0)=0$, so $H(0)=0$.\nTherefore, $\\tau^{\\star} = 0$ is a solution to the fixed-point equation.\n\nThe problem asks for \"the\" fixed-point threshold, implying uniqueness. Let $g(\\tau) = H(\\tau) - \\tau$. The roots of $g(\\tau)$ are the fixed points. Since $H(\\tau)$ is odd, $g(\\tau)$ is also an odd function: $g(-\\tau) = H(-\\tau) - (-\\tau) = -H(\\tau) + \\tau = -g(\\tau)$.\nIf there were a non-zero solution $\\tau_0 \\ne 0$ such that $g(\\tau_0)=0$, then it would immediately follow that $g(-\\tau_0) = -g(\\tau_0) = 0$, meaning $-\\tau_0$ is also a solution. The existence of a non-zero solution would imply non-uniqueness. For the solution to be unique as suggested by the problem's phrasing, the only possibility is that there are no non-zero solutions. This leaves $\\tau^{\\star}=0$ as the unique fixed point.\n\nThe fixed-point threshold is a single value, and given the problem's symmetry, the only unique candidate is the center of symmetry.\n\nFinal Answer: The fixed-point threshold $\\tau^{\\star}$ is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "To bridge theory and practice, we now turn to a modern and powerful semi-supervised framework: the teacher-student model, a cornerstone of knowledge distillation. This hands-on coding exercise focuses on the critical role of temperature scaling in generating \"soft\" pseudo-labels from a teacher model and how these labels guide the student's training. By implementing and calculating key metrics like Expected Calibration Error ($ECE$) and cross-entropy, you will directly observe how tuning the temperature $T$ impacts error propagation and model calibration, providing tangible insights into a vital hyperparameter in contemporary semi-supervised learning pipelines .",
            "id": "3162622",
            "problem": "You are given a semi-supervised learning scenario with a teacher-student model. The teacher is trained on labeled data and produces soft labels for unlabeled inputs via temperature scaling. The student is trained to imitate these soft labels on the unlabeled inputs. Your task is to analyze how the temperature affects calibration and error propagation using fundamental definitions, and to implement a program that computes the specified metrics for a fixed test suite of temperatures on a concrete dataset.\n\nFundamental base:\n- Supervised learning uses pairs $\\{(x_i, y_i)\\}_{i=1}^n$ with $y_i$ as target labels. Unsupervised learning uses only inputs $\\{x_i\\}_{i=1}^m$. Semi-supervised learning uses both labeled and unlabeled inputs, often with pseudo-labels inferred for the unlabeled inputs.\n- A teacher model with logits vector $\\mathbf{z} \\in \\mathbb{R}^K$ for $K$ classes, under temperature $T \\in \\mathbb{R}_{0}$, produces a probability distribution via the softmax transformation $p_T(k \\mid \\mathbf{z}) = \\dfrac{\\exp(z_k / T)}{\\sum_{j=1}^K \\exp(z_j / T)}$.\n- A student trained to minimize the cross-entropy to the teacher’s soft labels has the objective on unlabeled inputs $L(\\theta) = \\mathbb{E}\\left[-\\sum_{k=1}^K q_T(k \\mid \\mathbf{z}) \\log p_\\theta(k \\mid x)\\right]$, where $q_T$ denotes the teacher distribution at temperature $T$ and $p_\\theta$ denotes the student distribution. Assuming sufficient capacity and optimization, the minimizer satisfies $p_\\theta(\\cdot \\mid x) = q_T(\\cdot \\mid \\mathbf{z})$.\n- Calibration is assessed by Expected Calibration Error (ECE). Given predicted confidence $c_i = \\max_k p(k \\mid x_i)$ and correctness indicator $a_i \\in \\{0,1\\}$, partition the interval $[0,1]$ into $B$ equal-width bins. For bin $b$ with indices $\\mathcal{I}_b$, define $n_b = |\\mathcal{I}_b|$, empirical accuracy $\\text{acc}_b = \\dfrac{1}{n_b} \\sum_{i \\in \\mathcal{I}_b} a_i$, and mean confidence $\\text{conf}_b = \\dfrac{1}{n_b} \\sum_{i \\in \\mathcal{I}_b} c_i$. The Expected Calibration Error is $\\text{ECE} = \\sum_{b=1}^B \\dfrac{n_b}{N} \\left| \\text{acc}_b - \\text{conf}_b \\right|$, where $N$ is the total number of samples.\n- Error propagation from teacher to student under perfect imitation is characterized by the classification error rate and the average cross-entropy with respect to the true labels on the unlabeled set: $\\text{Err} = \\dfrac{1}{N} \\sum_{i=1}^N \\mathbb{I}\\left[\\arg\\max_k q_T(k \\mid \\mathbf{z}_i) \\neq y_i\\right]$ and $\\text{CE} = -\\dfrac{1}{N} \\sum_{i=1}^N \\log q_T(y_i \\mid \\mathbf{z}_i)$.\n\nDataset and parameters:\n- Number of classes is $K = 3$.\n- Unlabeled inputs (used for evaluation with ground-truth labels known but not used to train the teacher here) have teacher logits\n$$\n\\mathbf{Z} = \\begin{bmatrix}\n5.0  0.2  -1.0 \\\\\n2.0  2.5  0.1 \\\\\n0.5  0.4  0.3 \\\\\n10.0  -2.0  -3.0 \\\\\n-1.0  4.0  2.0 \\\\\n1.0  1.0  1.0 \\\\\n0.0  0.7  -0.2 \\\\\n0.3  -0.1  1.0\n\\end{bmatrix},\n\\quad\n\\mathbf{y} = \\begin{bmatrix}\n0 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 2 \\\\ 1 \\\\ 1 \\\\ 1\n\\end{bmatrix}.\n$$\n- The test suite of temperatures is $T \\in \\{0.1, 0.5, 1.0, 2.0, 5.0, 20.0\\}$.\n- Use $B = 5$ equal-width bins for the Expected Calibration Error, with bin edges at $\\{0, 0.2, 0.4, 0.6, 0.8, 1.0\\}$.\n\nTasks:\n1. For each temperature $T$ in the test suite, compute the teacher’s soft labels $q_T(\\cdot \\mid \\mathbf{z}_i)$ for each sample using the temperature-scaled softmax.\n2. For each temperature $T$, compute:\n   - The Expected Calibration Error $\\text{ECE}$ (with $B = 5$ bins) using the predicted confidences $c_i = \\max_k q_T(k \\mid \\mathbf{z}_i)$ and correctness indicators $a_i = \\mathbb{I}[\\arg\\max_k q_T(k \\mid \\mathbf{z}_i) = y_i]$.\n   - The average cross-entropy $\\text{CE} = -\\dfrac{1}{N} \\sum_{i=1}^N \\log q_T(y_i \\mid \\mathbf{z}_i)$, expressed as a decimal.\n   - The classification error rate $\\text{Err}$, expressed as a decimal.\n3. Aggregate the results across all temperatures into a single line of output as a comma-separated Python-style list. Each element should be a list of three floats $[\\text{ECE}, \\text{CE}, \\text{Err}]$ corresponding to one temperature, in the same order as the temperatures appear in the test suite.\n\nDesign for coverage:\n- $T = 0.1$ tests extreme sharpening (near one-hot behavior).\n- $T = 20.0$ tests extreme smoothing (near uniform behavior).\n- Intermediate values $T = 0.5, 1.0, 2.0, 5.0$ cover typical calibration regimes.\n- The logits include high-confidence correct, high-confidence incorrect, ties, and near-ties to exercise binning and error propagation.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is a list $[\\text{ECE}, \\text{CE}, \\text{Err}]$ for the corresponding temperature. For example: \"[[ece_T1,ce_T1,err_T1],[ece_T2,ce_T2,err_T2],...]\".\n- All reported quantities must be decimals (no percentage signs).",
            "solution": "The problem statement has been meticulously reviewed and is determined to be valid. It is scientifically grounded in the principles of statistical learning, specifically concerning teacher-student models, temperature scaling for knowledge distillation, and standard evaluation metrics like calibration error, cross-entropy, and classification error. The problem is well-posed, providing all necessary data, parameters, and explicit mathematical definitions for a unique and verifiable solution.\n\nThe task is to compute three performance metrics—Expected Calibration Error ($\\text{ECE}$), average cross-entropy ($\\text{CE}$), and classification error rate ($\\text{Err}$)—for a given set of teacher-model logits under various temperature scaling settings. We are provided with a logits matrix $\\mathbf{Z} \\in \\mathbb{R}^{N \\times K}$, corresponding true labels $\\mathbf{y} \\in \\{0, \\dots, K-1\\}^N$, a set of temperatures $T$, and parameters for computing $\\text{ECE}$. Here, the number of samples is $N=8$, the number of classes is $K=3$, and the number of bins for $\\text{ECE}$ is $B=5$.\n\nThe overall procedure involves iterating through each specified temperature $T$ in the set $\\{0.1, 0.5, 1.0, 2.0, 5.0, 20.0\\}$ and executing the following sequence of steps.\n\nFirst, for each sample $i$ with logits vector $\\mathbf{z}_i$, we compute the soft-label probability distribution $\\mathbf{q}_i = q_T(\\cdot \\mid \\mathbf{z}_i)$ using the temperature-scaled softmax function. The probability for class $k$ is given by:\n$$\nq_T(k \\mid \\mathbf{z}_i) = \\frac{\\exp(z_{ik} / T)}{\\sum_{j=1}^{K} \\exp(z_{ij} / T)}\n$$\nwhere $z_{ik}$ is the logit for the $i$-th sample and $k$-th class. This calculation is performed for all $N=8$ samples, yielding an $8 \\times 3$ probability matrix $\\mathbf{Q}_T$.\n\nSecond, using the computed probability distributions $\\mathbf{Q}_T$ and the ground-truth labels $\\mathbf{y}$, we calculate the three required metrics for the current temperature $T$.\n\n1.  **Classification Error Rate ($\\text{Err}$)**: The predicted class for each sample is the one with the highest probability: $\\hat{y}_i = \\arg\\max_k q_T(k \\mid \\mathbf{z}_i)$. The application of a positive temperature $T  0$ is a monotonic transformation of the logits, so the predicted class $\\hat{y}_i$ is independent of $T$. It is solely determined by $\\arg\\max_k z_{ik}$. The error rate is the fraction of samples where the prediction does not match the true label:\n    $$\n    \\text{Err} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}[\\hat{y}_i \\neq y_i]\n    $$\n    where $\\mathbb{I}[\\cdot]$ is the indicator function.\n\n2.  **Average Cross-Entropy ($\\text{CE}$)**: This metric measures the average negative log-probability of the true classes. It quantifies the cost incurred by the student model if it perfectly imitates the teacher's soft labels.\n    $$\n    \\text{CE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log q_T(y_i \\mid \\mathbf{z}_i)\n    $$\n    The natural logarithm is used as is standard for this definition.\n\n3.  **Expected Calibration Error ($\\text{ECE}$)**: This metric assesses the model's calibration by comparing its confidence to its empirical accuracy. The confidence for sample $i$ is defined as the probability of the predicted class, $c_i = \\max_k q_T(k \\mid \\mathbf{z}_i)$. The samples are then partitioned into $B=5$ equal-width bins based on their confidence values. The bins are defined by the edges $\\{0, 0.2, 0.4, 0.6, 0.8, 1.0\\}$. For each bin $b$, we calculate its average accuracy, $\\text{acc}_b$, and average confidence, $\\text{conf}_b$. The $\\text{ECE}$ is the weighted average of the absolute difference between these two quantities:\n    $$\n    \\text{ECE} = \\sum_{b=1}^{B} \\frac{n_b}{N} \\left| \\text{acc}_b - \\text{conf}_b \\right|\n    $$\n    where $n_b$ is the number of samples in bin $b$. If a bin is empty ($n_b=0$), its contribution to the sum is $0$. The correctness of a prediction for sample $i$ is $a_i = \\mathbb{I}[\\hat{y}_i = y_i]$. The average accuracy and confidence for bin $b$ (containing samples with indices in $\\mathcal{I}_b$) are:\n    $$\n    \\text{acc}_b = \\frac{1}{n_b} \\sum_{i \\in \\mathcal{I}_b} a_i \\quad \\text{and} \\quad \\text{conf}_b = \\frac{1}{n_b} \\sum_{i \\in \\mathcal{I}_b} c_i\n    $$\n\nBy systematically applying these calculations for each temperature in the test suite, we can analyze the effect of $T$ on the student model's inherited properties. The results are then aggregated into the specified list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes ECE, CE, and Err for a teacher-student model scenario\n    at different temperatures.\n    \"\"\"\n    # Define the dataset and parameters from the problem statement.\n    Z = np.array([\n        [5.0, 0.2, -1.0],\n        [2.0, 2.5, 0.1],\n        [0.5, 0.4, 0.3],\n        [10.0, -2.0, -3.0],\n        [-1.0, 4.0, 2.0],\n        [1.0, 1.0, 1.0],\n        [0.0, 0.7, -0.2],\n        [0.3, -0.1, 1.0]\n    ])\n    y = np.array([0, 0, 2, 0, 2, 1, 1, 1])\n    \n    test_cases = [\n        0.1, 0.5, 1.0, 2.0, 5.0, 20.0\n    ]\n    \n    N = Z.shape[0]\n    K = Z.shape[1]\n    B = 5\n\n    # Predictions are independent of temperature T  0\n    predictions = np.argmax(Z, axis=1)\n    correctness = (predictions == y).astype(float)\n    error_rate = 1.0 - np.mean(correctness)\n\n    results = []\n\n    def softmax_temp(logits, T):\n        \"\"\"\n        Computes softmax probabilities with temperature scaling.\n        Uses a numerically stable implementation.\n        \"\"\"\n        scaled_logits = logits / T\n        # Subtract max for numerical stability\n        max_logits_per_row = np.max(scaled_logits, axis=1, keepdims=True)\n        exps = np.exp(scaled_logits - max_logits_per_row)\n        sum_of_exps = np.sum(exps, axis=1, keepdims=True)\n        return exps / sum_of_exps\n\n    for T in test_cases:\n        # 1. Compute soft labels (probability distributions)\n        q_T = softmax_temp(Z, T)\n        \n        # 2. Compute metrics for the current temperature\n        \n        # Confidences for the current temperature\n        confidences = np.max(q_T, axis=1)\n\n        # Average Cross-Entropy (CE)\n        # Probabilities assigned to the true class\n        true_class_probs = q_T[np.arange(N), y]\n        # Adding a small epsilon to prevent log(0) in extreme cases\n        cross_entropy = -np.mean(np.log(true_class_probs + 1e-15))\n        \n        # Expected Calibration Error (ECE)\n        ece = 0.0\n        bin_edges = np.linspace(0.0, 1.0, B + 1)\n        \n        for b in range(B):\n            # Define bin range (lower, upper]\n            lower_bound = bin_edges[b]\n            upper_bound = bin_edges[b+1]\n            \n            # Identify samples whose confidence falls into the current bin\n            in_bin = (confidences  lower_bound)  (confidences = upper_bound)\n            \n            # For the first bin, include confidence == 0, although softmax is always  0\n            if b == 0:\n                in_bin = (confidences = lower_bound)  (confidences = upper_bound)\n                \n            n_b = np.sum(in_bin)\n            \n            if n_b  0:\n                # Calculate average accuracy and confidence for the bin\n                acc_b = np.mean(correctness[in_bin])\n                conf_b = np.mean(confidences[in_bin])\n                \n                # Add weighted difference to ECE\n                ece += (n_b / N) * np.abs(acc_b - conf_b)\n        \n        # Store results for this temperature\n        results.append([ece, cross_entropy, error_rate])\n\n    # Final print statement in the exact required format.\n    # The str() on a list gives the desired '[...]' format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}