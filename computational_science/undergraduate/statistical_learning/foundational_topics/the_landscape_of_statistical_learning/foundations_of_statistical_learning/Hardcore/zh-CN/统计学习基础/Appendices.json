{
    "hands_on_practices": [
        {
            "introduction": "统计学习的核心挑战之一是在模型复杂度和泛化能力之间取得平衡。这个练习通过一个多项式回归的场景，引导我们从第一性原理出发，推导期望预测误差的偏置-方差分解。通过亲手计算，你将清晰地看到模型复杂度（由多项式阶数 $k$ 控制）如何直接影响偏置和方差，从而深刻理解欠拟合与过拟合的根源。",
            "id": "3121938",
            "problem": "考虑一个在区间 $[-1,1]$ 上的一维回归问题。设 $x$ 从 $[-1,1]$ 上的均匀分布中抽取，响应由 $y = f^{\\star}(x) + \\varepsilon$ 生成，其中 $\\varepsilon$ 是独立噪声，满足 $\\mathbb{E}[\\varepsilon \\mid x] = 0$ 和 $\\operatorname{Var}(\\varepsilon \\mid x) = \\sigma^{2}$。假设未知的回归函数 $f^{\\star}$ 在均匀测度下，对于 $[-1,1]$ 上的一个标准正交多项式基 $\\{\\phi_{j}\\}_{j \\ge 0}$ 存在一个 $L^{2}$ 展开，即 $f^{\\star}(x) = \\sum_{j=0}^{\\infty} \\theta_{j} \\phi_{j}(x)$，其中 $\\sum_{j=0}^{\\infty} \\theta_{j}^{2}  \\infty$，且 $\\int_{-1}^{1} \\phi_{j}(x)\\phi_{\\ell}(x)\\,\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$。\n\n你收集了一个大小为 $n$ 的训练样本 $\\{(x_{i},y_{i})\\}_{i=1}^{n}$，其中 $x_{i}$ 是确定性选择的，使得前 $k+1$ 个基函数相对于设计点上的离散均匀测度是经验性标准正交的，并且与所有更高阶的基函数正交：\n\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{j}(x_{i})\\,\\phi_{\\ell}(x_{i}) = \n\\begin{cases}\n1,  j=\\ell \\le k,\\\\\n0,  j \\ne \\ell,\\; j,\\ell \\le k,\\\\\n0,  \\ell \\le k  j.\n\\end{cases}\n$$\n\n你通过对特征 $\\{\\phi_{0},\\dots,\\phi_{k}\\}$ 应用普通最小二乘法来拟合 $k$ 次多项式最小二乘预测器 $\\widehat{f}_{k}(x) = \\sum_{j=0}^{k} \\widehat{\\beta}_{j}\\,\\phi_{j}(x)$。\n\n仅使用第一性原理（即条件期望和方差的定义、上述的正交性关系以及最小二乘法的正规方程），推导对于从相同数据生成过程中独立抽取的一个新测试点 $(x,y)$ 的期望积分平方预测误差的偏差-方差分解：\n\n$$\n\\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big],\n$$\n\n其中，期望是针对训练噪声、训练输入、测试输入 $x$ 和测试噪声计算的。将你的最终结果表示为关于 $k$、$n$、$\\sigma^{2}$ 和系数 $\\{\\theta_{j}\\}_{j \\ge 0}$ 的封闭形式。你的最终答案必须是一个单一的封闭形式解析表达式。无需四舍五入。",
            "solution": "该问题被评估为有效，因为它是科学上合理的、适定的和客观的。它代表了统计学习中的一个标准理论练习。因此，我们可以继续进行推导。\n\n需要分析的量是期望积分平方预测误差，由下式给出\n$$ \\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big] $$\n总期望 $\\mathbb{E}$ 是对所有随机性来源计算的：测试点的输入 $x$ 及其相关噪声 $\\varepsilon$，以及训练数据中的噪声，我们记为 $\\{\\varepsilon_{i}\\}_{i=1}^{n}$。训练输入 $\\{x_i\\}_{i=1}^n$ 是确定性的。拟合模型 $\\widehat{f}_k$ 依赖于训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$，因此也依赖于训练噪声 $\\{\\varepsilon_i\\}_{i=1}^n$。\n\n我们首先应用全期望定律，以训练数据 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ 和测试输入 $x$ 为条件。响应 $y$ 由 $y = f^{\\star}(x)+\\varepsilon$ 给出。\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ (f^{\\star}(x) + \\varepsilon - \\widehat{f}_{k}(x))^2 \\mid \\mathcal{D}, x \\right] \\right] $$\n展开平方，我们得到：\n$$ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) + \\varepsilon^2 $$\n交叉项的条件期望为零，因为测试噪声 $\\varepsilon$ 独立于 $\\mathcal{D}$ 和 $x$，并且 $\\mathbb{E}[\\varepsilon | x] = 0$：\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) \\mid \\mathcal{D}, x \\right] = 2(f^{\\star}(x) - \\widehat{f}_{k}(x))\\mathbb{E}_{\\varepsilon | x}[\\varepsilon] = 0 $$\n$\\varepsilon^2$ 项的条件期望是噪声的条件方差，因为其均值为零：\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} [\\varepsilon^2 \\mid \\mathcal{D}, x] = \\mathbb{E}_{\\varepsilon|x}[\\varepsilon^2 | x] = \\operatorname{Var}(\\varepsilon|x) + (\\mathbb{E}[\\varepsilon|x])^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\n将这些代回，风险变为：\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + \\sigma^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] + \\sigma^2 $$\n项 $\\sigma^2$ 是不可约误差。剩余项是平均积分平方误差 (MISE)。我们通过引入平均预测器函数 $\\bar{f}_k(x) = \\mathbb{E}_{\\mathcal{D}}[\\widehat{f}_k(x)]$ 将其分解为偏差和方差分量。\n$$ \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\bar{f}_k(x) + \\bar{f}_k(x) - \\widehat{f}_{k}(x))^2 \\right] $$\n展开平方并注意交叉项 $\\mathbb{E}_{\\mathcal{D}}[(f^{\\star}(x) - \\bar{f}_k(x))(\\bar{f}_k(x) - \\widehat{f}_{k}(x))]$ 因 $\\mathbb{E}_{\\mathcal{D}}[\\bar{f}_k(x) - \\widehat{f}_{k}(x)] = 0$ 而消失，我们得到标准分解：\n$$ \\mathcal{R}_{k,n} = \\underbrace{\\mathbb{E}_{x}\\left[(f^{\\star}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{积分平方偏差}} + \\underbrace{\\mathbb{E}_{\\mathcal{D},x}\\left[(\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{积分方差}} + \\sigma^2 $$\n我们现在推导每一项。这需要找到普通最小二乘 (OLS) 系数 $\\widehat{\\beta}_j$。OLS 估计器最小化 $\\sum_{i=1}^n (y_i - \\sum_{j=0}^k \\beta_j \\phi_j(x_i))^2$。正规方程为：\n$$ \\sum_{j=0}^{k} \\widehat{\\beta}_j \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_j(x_i) \\phi_{\\ell}(x_i)\\right) = \\frac{1}{n}\\sum_{i=1}^{n} y_i \\phi_{\\ell}(x_i), \\quad \\text{for } \\ell=0, \\dots, k $$\n利用给定的关于设计点 $\\{x_i\\}$ 的经验性标准正交条件，左侧简化为 $\\widehat{\\beta}_{\\ell}$。因此，对于 $j=0, \\dots, k$：\n$$ \\widehat{\\beta}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\phi_j(x_i) $$\n接下来，我们求解平均预测器 $\\bar{f}_k(x) = \\sum_{j=0}^k \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] \\phi_j(x)$。期望是针对训练噪声计算的。由于 $y_i = f^{\\star}(x_i) + \\varepsilon_i$ 且 $\\mathbb{E}[\\varepsilon_i|x_i]=0$，我们有 $\\mathbb{E}_{\\mathcal{D}}[y_i] = f^{\\star}(x_i)$。\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}_{\\mathcal{D}}[y_i] \\phi_j(x_i) = \\frac{1}{n} \\sum_{i=1}^{n} f^{\\star}(x_i) \\phi_j(x_i) $$\n代入展开式 $f^{\\star}(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)$：\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)\\right) \\phi_j(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{\\ell}(x_i) \\phi_j(x_i)\\right) $$\n对于 $j \\le k$，使用所提供的经验性标准正交条件，括号中的项在 $\\ell \\le k$ 时为 $\\mathbf{1}\\{j=\\ell\\}$，在 $\\ell  k$ 时为 $0$。因此，对于 $j \\le k$：\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j $$\n平均预测器是 $f^{\\star}$ 在所选函数空间上的投影：\n$$ \\bar{f}_k(x) = \\sum_{j=0}^{k} \\theta_j \\phi_j(x) $$\n现在我们可以计算偏差项。期望 $\\mathbb{E}_x$ 对应于在 $[-1,1]$ 上对 $\\frac{dx}{2}$ 的积分。\n$$ \\text{Bias}^2 = \\mathbb{E}_{x}\\left[ \\left( f^{\\star}(x) - \\bar{f}_k(x) \\right)^2 \\right] = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=0}^{\\infty} \\theta_j \\phi_j(x) - \\sum_{j=0}^{k} \\theta_j \\phi_j(x) \\right)^2 \\right] $$\n$$ = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=k+1}^{\\infty} \\theta_j \\phi_j(x) \\right)^2 \\right] = \\sum_{j=k+1}^{\\infty} \\sum_{\\ell=k+1}^{\\infty} \\theta_j \\theta_{\\ell} \\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] $$\n使用总体的标准正交性，$\\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] = \\int_{-1}^1 \\phi_j(x)\\phi_{\\ell}(x)\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$，偏差项为：\n$$ \\text{积分平方偏差} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 $$\n接下来，我们计算方差项。\n$$ \\text{方差} = \\mathbb{E}_{\\mathcal{D},x}\\left[ (\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbb{E}_{x} \\left[ \\left( \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)\\phi_j(x) \\right)^2 \\right] \\right] $$\n由于总体的标准正交性，关于 $x$ 的内部期望得以简化：\n$$ \\mathbb{E}_{x} \\left[ \\sum_{j=0}^{k} \\sum_{\\ell=0}^{k} (\\widehat{\\beta}_j - \\theta_j)(\\widehat{\\beta}_{\\ell} - \\theta_{\\ell}) \\phi_j(x)\\phi_{\\ell}(x) \\right] = \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 $$\n那么积分方差就是对训练数据的期望：\n$$ \\text{积分方差} = \\mathbb{E}_{\\mathcal{D}}\\left[ \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\mathbb{E}_{\\mathcal{D}}\\left[ (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) $$\n因为 $\\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j$。我们计算系数的方差。\n$$ \\widehat{\\beta}_j - \\theta_j = \\left( \\frac{1}{n}\\sum_{i=1}^n y_i \\phi_j(x_i) \\right) - \\left( \\frac{1}{n}\\sum_{i=1}^n f^{\\star}(x_i) \\phi_j(x_i) \\right) = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) $$\n噪声 $\\{\\varepsilon_i\\}$ 是独立的，方差为 $\\sigma^2$，而 $\\phi_j(x_i)$ 是确定性常数。\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\operatorname{Var}\\left( \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}(\\varepsilon_i \\phi_j(x_i)) = \\frac{1}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 \\operatorname{Var}(\\varepsilon_i) $$\n$$ = \\frac{\\sigma^2}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 = \\frac{\\sigma^2}{n} \\left(\\frac{1}{n}\\sum_{i=1}^n \\phi_j(x_i)\\phi_j(x_i)\\right) $$\n对于 $j \\le k$，使用经验性标准正交性，括号中的项为 $1$。\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\frac{\\sigma^2}{n} $$\n积分方差是对 $j=0, \\dots, k$ 的求和：\n$$ \\text{积分方差} = \\sum_{j=0}^{k} \\frac{\\sigma^2}{n} = \\frac{(k+1)\\sigma^2}{n} $$\n最后，我们将这三个分量组合起来，得到总的期望误差 $\\mathcal{R}_{k,n}$。\n$$ \\mathcal{R}_{k,n} = \\text{积分平方偏差} + \\text{积分方差} + \\text{不可约误差} $$\n$$ \\mathcal{R}_{k,n} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 + \\frac{(k+1)\\sigma^2}{n} + \\sigma^2 $$\n该表达式代表了期望预测误差的完整偏差-方差分解。",
            "answer": "$$\\boxed{\\sigma^2 + \\frac{\\sigma^2(k+1)}{n} + \\sum_{j=k+1}^{\\infty} \\theta_j^2}$$"
        },
        {
            "introduction": "在理解了偏置-方差的权衡之后，我们转向分类问题，并引入一个更形式化的工具来衡量模型的“容量”或“复杂度”——VC维。这个练习将通过一个最简单的阈值分类器例子，让你计算其成长函数和VC维，并进一步推导样本复杂度界。这有助于你理解为什么简单的模型类别（低VC维）能够从更少的数据中进行可靠的学习和泛化。",
            "id": "3122009",
            "problem": "考虑在实数轴上的二元分类问题，其假设类为阈值分类器。对于每个阈值 $t \\in \\mathbb{R}$，定义分类器 $h_t : \\mathbb{R} \\to \\{0,1\\}$ 为 $h_t(x) = \\mathbb{I}\\{x \\ge t\\}$，其中 $\\mathbb{I}\\{\\cdot\\}$ 是指示函数。假设数据从 $\\mathbb{R} \\times \\{0,1\\}$ 上的一个未知分布中独立同分布地抽取，且该分布可被此假设类实现：即存在一个未知的 $t_{\\star} \\in \\mathbb{R}$，使得 $Y = \\mathbb{I}\\{X \\ge t_{\\star}\\}$ 几乎必然成立。\n\n从增长函数和 Vapnik–Chervonenkis (VC) 维度的定义出发，并仅使用基本的概率工具和独立性，完成以下任务：\n\n1. 计算在 $\\mathbb{R}$ 上的阈值分类器类的增长函数 $\\Pi_{\\mathcal{H}}(n)$，其中 $\\Pi_{\\mathcal{H}}(n)$ 定义为，在 $\\mathbb{R}$ 中所有包含 $n$ 个不同点的集合上，假设类 $\\mathcal{H}$ 中的分类器能够在这些点上产生的不同 $\\{0,1\\}$-标签组合的最大数量。\n\n2. 确定 $\\mathcal{H}$ 的 Vapnik–Chervonenkis 维度 $d_{\\mathrm{VC}}$。\n\n3. 推导一个非渐近的可实现情况下的样本量条件，该条件需确保经验风险最小化具有以下泛化性质：以至少 $1 - \\delta$ 的概率，任何在 $m$ 个独立同分布样本上达到零经验误差的 $\\mathcal{H}$ 中的分类器，其真实误差至多为 $\\epsilon$。将此条件用 $\\epsilon$、$\\delta$ 和增长函数明确表示，然后使用您计算出的 $\\Pi_{\\mathcal{H}}(\\cdot)$ 将其具体化。\n\n将您的最终答案表示为 $\\Pi_{\\mathcal{H}}(n)$ 和 $d_{\\mathrm{VC}}$ 这两个量，按此顺序排成一行。不需要进行数值四舍五入，也不涉及物理单位。",
            "solution": "此问题将首先被验证，若有效，则按要求分三部分解决：计算增长函数、确定 Vapnik-Chervonenkis (VC) 维度，以及推导可实现情况下的样本量条件。\n\n### 问题验证\n\n**第1步：提取已知条件**\n- **假设类**：$\\mathcal{H} = \\{h_t : \\mathbb{R} \\to \\{0,1\\} \\mid h_t(x) = \\mathbb{I}\\{x \\ge t\\}, t \\in \\mathbb{R}\\}$。这些分类器是实数轴上的阈值分类器。\n- **数据分布**：数据点 $(X, Y)$ 从 $\\mathbb{R} \\times \\{0,1\\}$ 上的一个未知分布中独立同分布 (i.i.d.) 地抽取。\n- **可实现性假设**：存在一个真实阈值 $t_{\\star} \\in \\mathbb{R}$，使得标签由规则 $Y = \\mathbb{I}\\{X \\ge t_{\\star}\\}$ 几乎必然地生成。\n- **任务1**：计算增长函数 $\\Pi_{\\mathcal{H}}(n) = \\max_{x_1, \\dots, x_n \\in \\mathbb{R}} |\\mathcal{H}|_{\\{x_1, \\dots, x_n\\}}|$，其中 $|\\mathcal{H}|_{\\{x_1, \\dots, x_n\\}}|$ 是假设类 $\\mathcal{H}$ 中的分类器在集合 $\\{x_1, \\dots, x_n\\}$ 上能产生的不同标签组合的数量。\n- **任务2**：确定 VC 维度，$d_{\\mathrm{VC}} = \\max\\{n \\in \\mathbb{N} \\mid \\Pi_{\\mathcal{H}}(n) = 2^n\\}$。\n- **任务3**：推导关于样本量 $m$ 的一个条件，以保证以至少 $1 - \\delta$ 的概率，任何经验误差为零的分类器 $\\hat{h} \\in \\mathcal{H}$ 的真实误差至多为 $\\epsilon$。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学或事实不健全**：该问题是统计学习理论中一个标准的、基础的练习。阈值分类器、增长函数和 VC 维度的概念在数学上和科学上都是健全的。不存在缺陷。\n- **无法形式化或不相关**：该问题是形式化陈述的，并且与统计学习的基础直接相关。不存在缺陷。\n- **不完整或矛盾的设置**：该问题是自洽的。所有必要的定义（假设类、增长函数、VC维度）和假设（可实现性、独立同分布数据）都已提供，并且是相互一致的。不存在缺陷。\n- **不切实际或不可行**：该设置是学习理论中用于教学目的的一个简化但经典的模范。它在物理上或科学上并非不合理。不存在缺陷。\n- **不适定或结构不良**：该问题是适定的。待计算的量（$\\Pi_{\\mathcal{H}}(n)$ 和 $d_{\\mathrm{VC}}$）由问题定义唯一确定。样本复杂度界的推导是一个标准程序。不存在缺陷。\n- **伪深刻、琐碎或同义反复**：虽然这是一个经典的例子，但解决它需要对基本定义有清晰的理解和正确的应用。它并非琐碎或人为设计的。不存在缺陷。\n- **超出科学可验证范围**：这些主张在数学上是可推导和可验证的。不存在缺陷。\n\n**第3步：结论与行动**\n该问题有效。将提供完整的解决方案。\n\n### 解答\n\n**第1部分：计算增长函数 $\\Pi_{\\mathcal{H}}(n)$**\n\n增长函数 $\\Pi_{\\mathcal{H}}(n)$ 是假设类 $\\mathcal{H}$ 能够标记一个包含 $n$ 个点的集合的最大方式数。设 $S = \\{x_1, x_2, \\dots, x_n\\}$ 是 $\\mathbb{R}$ 中任意一个包含 $n$ 个不同点的集合。为了分析这些对分（dichotomies），我们可以不失一般性地对这些点进行排序：$x_{(1)}  x_{(2)}  \\dots  x_{(n)}$。\n\n分类器 $h_t(x) = \\mathbb{I}\\{x \\ge t\\}$ 将大于或等于阈值 $t$ 的点标记为 $1$，小于 $t$ 的点标记为 $0$。对于这些有序点，其标签向量为 $(h_t(x_{(1)}), h_t(x_{(2)}), \\dots, h_t(x_{(n)}))$。具体的标签组合取决于阈值 $t$ 相对于点 $x_{(i)}$ 的位置。\n\n$n$ 个不同的点将实数轴划分为 $n+1$ 个开区间。我们分析将 $t$ 放置在这些区域中每一个所产生的标签组合。\n1.  如果 $t > x_{(n)}$，那么对于所有 $i \\in \\{1, \\dots, n\\}$，都有 $x_{(i)}  t$。因此，对所有 $i$，$h_t(x_{(i)}) = 0$。标签向量为 $(0, 0, \\dots, 0)$。\n2.  如果 $x_{(n-1)}  t \\le x_{(n)}$，那么 $h_t(x_{(n)}) = 1$，并且对于所有 $i  n$，都有 $x_{(i)}  t$，所以 $h_t(x_{(i)}) = 0$。标签向量为 $(0, 0, \\dots, 0, 1)$。\n3.  如果 $x_{(n-2)}  t \\le x_{(n-1)}$，那么 $h_t(x_{(n-1)}) = 1$ 且 $h_t(x_{(n)}) = 1$。对于所有 $i  n-1$，都有 $x_{(i)}  t$，所以 $h_t(x_{(i)}) = 0$。标签向量为 $(0, 0, \\dots, 1, 1)$。\n...\nk. 一般地，对于 $k \\in \\{1, \\dots, n\\}$，如果我们放置阈值使得 $x_{(k-1)}  t \\le x_{(k)}$（约定 $x_{(0)} = -\\infty$），那么对于所有 $i \\ge k$，$h_t(x_{(i)}) = 1$，而对于所有 $i  k$，$h_t(x_{(i)}) = 0$。\n...\nn+1. 如果 $t \\le x_{(1)}$，那么对于所有 $i \\in \\{1, \\dots, n\\}$，都有 $x_{(i)} \\ge t$。因此，对所有 $i$，$h_t(x_{(i)}) = 1$。标签向量为 $(1, 1, \\dots, 1)$。\n\n让我们列出为有序点 $(x_{(1)}, \\dots, x_{(n)})$ 生成的不同标签组合：\n- $(0, 0, \\dots, 0, 0)$\n- $(0, 0, \\dots, 0, 1)$\n- $(0, 0, \\dots, 1, 1)$\n- ...\n- $(0, 1, \\dots, 1, 1)$\n- $(1, 1, \\dots, 1, 1)$\n\n有 $n$ 种标签组合至少包含一个 $0$ 和一个 $1$，再加上全零向量和全一向量。这总共给出了 $n+1$ 种不同的标签组合。例如，对于任何 $t \\in (x_{(n-j)}, x_{(n-j+1)}]$，都会生成恰好有 $j$ 个 $1$ 的标签组合 $(0, \\dots, 0, 1, \\dots, 1)$。由于所有的对分都是“某个点右侧的所有点都标记为1”的形式，我们无法在有序点上生成例如 $(1, 0, \\dots)$ 这样的标签组合。\n\n无论这 $n$ 个不同点的具体位置如何，不同对分的数量都是 $n+1$。因此，在所有大小为 $n$ 的集合上的最大数量也是 $n+1$。\n$$ \\Pi_{\\mathcal{H}}(n) = n+1 $$\n\n**第2部分：确定 Vapnik-Chervonenkis 维度 $d_{\\mathrm{VC}}$**\n\nVC 维度 $d_{\\mathrm{VC}}$ 定义为假设类 $\\mathcal{H}$ 能够打散一个包含 $n$ 个点的集合的最大整数 $n$，这意味着它可以为该集合生成所有 $2^n$ 种可能的标签组合。这等价于找到满足 $\\Pi_{\\mathcal{H}}(n) = 2^n$ 的最大整数 $n$。\n\n使用我们刚刚计算出的增长函数，我们需要找到满足以下等式的最大整数 $n$：\n$$ n+1 = 2^n $$\n让我们测试一些小的整数 $n$：\n- 对于 $n=1$：$1+1 = 2$ 且 $2^1 = 2$。等式成立。因此，$\\mathcal{H}$ 可以打散1个点。确实，对于任意点 $x_1$，标签 $\\{1\\}$ 可由 $h_{x_1}$ 产生，而标签 $\\{0\\}$ 可由 $h_{x_1+\\delta}$（对于任意 $\\delta0$）产生。\n- 对于 $n=2$：$2+1 = 3  2^2=4$。等式不成立。如第1部分所示，对于任意两点 $x_1  x_2$，标签组合 $(1,0)$ 无法生成，因为如果 $h_t(x_1)=1$，那么 $t \\le x_1$，这意味着 $t  x_2$，所以 $h_t(x_2)$ 也必须是 $1$。\n- 对于 $n > 2$：可以通过归纳法证明，当 $n > 1$ 时，$n+1  2^n$。\n\n使 $\\Pi_{\\mathcal{H}}(n) = 2^n$ 成立的最大整数 $n$ 是 $n=1$。\n$$ d_{\\mathrm{VC}} = 1 $$\n\n**第3部分：推导样本量条件**\n\n我们处于可实现设定中，并寻求一个样本量 $m$，使得以至少 $1-\\delta$ 的概率，任何经验风险为零（$R_S(\\hat{h}) = 0$）的假设 $\\hat{h} \\in \\mathcal{H}$ 的真实风险至多为 $\\epsilon$（$R(\\hat{h}) \\le \\epsilon$）。这是一个标准的可能近似正确（PAC）学习保证。\n\n该条件可以表述为对“坏”事件的概率进行限定，即存在一个与样本一致但真实误差很高的假设：\n$$ P(\\exists h \\in \\mathcal{H} \\text{ such that } R_S(h)=0 \\text{ and } R(h)  \\epsilon) \\le \\delta $$\n令 $H_{bad} = \\{h \\in \\mathcal{H} \\mid R(h)  \\epsilon\\}$。我们关心的事件是，我们的 $m$ 个样本组成的集合 $S$ 被 $H_{bad}$ 中至少一个假设完美标记。\n\n限定此概率的标准方法是应用联合界（union bound）。对（可能无限的）集合 $H_{bad}$ 进行朴素的联合界是不可行的。取而代之的是，对假设类 $\\mathcal{H}$ 能够标记特定样本点 $x_1, \\dots, x_m$ 的有限种不同方式应用联合界。这种标签组合的数量最多为 $\\Pi_{\\mathcal{H}}(m)$。对于任何固定的“坏”假设 $h \\in H_{bad}$，它与一个大小为 $m$ 的随机样本一致的概率是 $(1-R(h))^m  (1-\\epsilon)^m$。一个结合了这些思想的简化论证，得出了对坏事件概率的以下界限：\n$$ P(\\text{bad event}) \\le \\Pi_{\\mathcal{H}}(m)(1-\\epsilon)^m $$\n虽然一个完全严谨的证明需要更高级的技术，如对称化，但这个界是一个常见的起点，并抓住了本质的权衡。我们将此上界设为小于或等于 $\\delta$：\n$$ \\Pi_{\\mathcal{H}}(m)(1-\\epsilon)^m \\le \\delta $$\n为了找到关于 $m$ 的条件，我们可以对两边取自然对数：\n$$ \\ln(\\Pi_{\\mathcal{H}}(m)) + m \\ln(1-\\epsilon) \\le \\ln(\\delta) $$\n使用著名不等式 $\\ln(1-x) \\le -x$（对于 $x \\in [0,1)$），我们有 $\\ln(1-\\epsilon) \\le -\\epsilon$。代入这个不等式可以得到一个更简单（尽管稍微宽松一些）的条件：\n$$ \\ln(\\Pi_{\\mathcal{H}}(m)) - m\\epsilon \\le \\ln(\\delta) $$\n重新整理这个不等式以表达关于 $m$ 的条件，得到：\n$$ m\\epsilon \\ge \\ln(\\Pi_{\\mathcal{H}}(m)) + \\ln\\left(\\frac{1}{\\delta}\\right) $$\n$$ m \\ge \\frac{1}{\\epsilon} \\left( \\ln(\\Pi_{\\mathcal{H}}(m)) + \\ln\\left(\\frac{1}{\\delta}\\right) \\right) $$\n这是一个用增长函数 $\\Pi_{\\mathcal{H}}(\\cdot)$、$\\epsilon$ 和 $\\delta$ 表示的通用样本量条件。\n\n现在，我们使用阈值分类器类的增长函数 $\\Pi_{\\mathcal{H}}(m) = m+1$ 来具体化这个结果：\n$$ m \\ge \\frac{1}{\\epsilon} \\left( \\ln(m+1) + \\ln\\left(\\frac{1}{\\delta}\\right) \\right) $$\n这个隐式不等式就是所求的样本量条件。它表明，对于固定的 $\\epsilon$ 和 $\\delta$，所需的样本量 $m$ 随其自身对数增长，导致总样本复杂度大致与 $\\frac{1}{\\epsilon}\\ln(\\frac{1}{\\epsilon\\delta})$ 成正比。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} n+1  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "理论告诉我们必须控制模型复杂度，而正则化是实现这一目标的最重要技术之一。这个练习将理论与实践相结合，通过计算Rademacher复杂度，让你比较两种常用正则化方法（$\\ell_1$范数和$\\ell_2$范数）对模型泛化能力的影响。你将推导出在哪种稀疏性条件下$\\ell_1$正则化更优，从而理解Lasso等方法为何在特征选择和高维问题中如此有效。",
            "id": "3121905",
            "problem": "考虑一个在 $\\mathbb{R}^{d}$ 上的二元分类问题，其线性预测器为 $f_{\\mathbf{w}}(\\mathbf{x}) = \\mathbf{w}^{\\top} \\mathbf{x}$，通过在范数约束下的经验风险最小化（ERM）进行学习。设损失函数 $\\ell(y, f_{\\mathbf{w}}(\\mathbf{x}))$ 对其第二个参数是 $1$-Lipschitz 的，并且其值有界，范围在 $[0,1]$ 内。假设输入 $\\mathbf{x}$ 几乎必然地同时满足 $\\|\\mathbf{x}\\|_{\\infty} \\leq r$ 和 $\\|\\mathbf{x}\\|_{2} \\leq R$，其中 $r  0$, $R  0$ 且 $d \\geq 2$。\n\n定义两个假设类别：\n$$\n\\mathcal{F}_{1} = \\left\\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{1} \\leq B_{1} \\right\\}\n\\quad\\text{和}\\quad\n\\mathcal{F}_{2} = \\left\\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{2} \\leq B_{2} \\right\\},\n$$\n其中 $B_{1}, B_{2}  0$。假设未知的目标参数 $\\mathbf{w}^{\\star}$ 是 $s$-稀疏的，即其最多有 $s$ 个非零坐标，并满足 $\\|\\mathbf{w}^{\\star}\\|_{2} \\leq B_{2}$。在此稀疏性假设下，使用不等式 $\\|\\mathbf{w}^{\\star}\\|_{1} \\leq \\sqrt{s}\\,\\|\\mathbf{w}^{\\star}\\|_{2}$ 来设定 $B_{1} = \\sqrt{s}\\,B_{2}$，从而确保 $\\mathcal{F}_{1}$ 和 $\\mathcal{F}_{2}$ 都包含 $\\mathbf{w}^{\\star}$。\n\n设 $S = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ 是一个大小为 $n \\geq 2$ 的样本。从经验 Rademacher 复杂度的定义出发，\n$$\n\\mathfrak{R}_{n}(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}, S} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} f(\\mathbf{x}_{i}) \\right],\n$$\n其中 $\\sigma_{1},\\dots,\\sigma_{n}$ 是取值于 $\\{-1,+1\\}$ 的独立 Rademacher 随机变量，请仅利用对偶范数关系和 Rademacher 随机变量和的基本集中不等式，推导出 $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ 和 $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$ 关于 $B_{1}$、$B_{2}$、$r$、$R$、$d$ 和 $n$ 的上界。然后，利用一个标准的泛化论证（即对于 $1$-Lipschitz 有界损失，较小的经验 Rademacher 复杂度会导致较小的 ERM 泛化差距），计算使得 $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ 的上界小于或等于 $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$ 的上界的最大稀疏度 $s_{\\star}$（以一个依赖于 $d$、$r$ 和 $R$ 的闭式符号表达式形式）。请以 $s_{\\star} = \\cdots$ 的表达式形式给出你的最终答案。",
            "solution": "该问题要求推导两个假设类别 $\\mathcal{F}_{1}$ 和 $\\mathcal{F}_{2}$ 的经验 Rademacher 复杂度的上界，然后找出使得 $\\mathcal{F}_{1}$ 的界小于或等于 $\\mathcal{F}_{2}$ 的界的最大稀疏度 $s_{\\star}$。\n\n设 $S = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ 是一个固定的样本。经验 Rademacher 复杂度定义为：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} f(\\mathbf{x}_{i}) \\right]\n$$\n总 Rademacher 复杂度为 $\\mathfrak{R}_{n}(\\mathcal{F}) = \\mathbb{E}_{S}[\\hat{\\mathfrak{R}}_{S}(\\mathcal{F})]$。我们将推导 $\\hat{\\mathfrak{R}}_{S}$ 的不依赖于特定样本 $S$ 的上界，这些上界将直接作为 $\\mathfrak{R}_{n}(\\mathcal{F})$ 的上界。\n\n**第1步：推导 $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ 的上界**\n\n假设类别 $\\mathcal{F}_{1}$ 定义为 $\\mathcal{F}_{1} = \\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{1} \\leq B_{1} \\}$。$\\mathcal{F}_{1}$ 的经验 Rademacher 复杂度为：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{w}^{\\top}\\mathbf{x}_{i} \\right]\n$$\n根据内积的线性性质，我们可以重排求和项：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\mathbf{w}^{\\top} \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) \\right]\n$$\n期望内的表达式具有 $\\sup_{\\|\\mathbf{w}\\|_{p} \\leq B} \\mathbf{w}^{\\top}\\mathbf{v}$ 的形式。根据对偶范数的定义，该上确界等于 $B \\|\\mathbf{v}\\|_{q}$，其中 $\\|\\cdot\\|_{q}$ 是 $\\|\\cdot\\|_{p}$ 的对偶范数。$\\ell_1$-范数的对偶范数是 $\\ell_{\\infty}$-范数。因此，我们有：\n$$\n\\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\mathbf{w}^{\\top} \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) = B_{1} \\left\\| \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty}\n$$\n将此代回 $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1})$ 的表达式中：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\frac{B_{1}}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty} \\right]\n$$\n为了给定期望上界，我们使用一个关于独立随机向量和的标准最大值不等式。对于 $\\mathbb{R}^d$ 中的一组向量 $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}$，一个从 Hoeffding 不等式和联合界推导出的界是：\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{v}_{i} \\right\\|_{\\infty} \\right] \\leq \\sqrt{2 \\ln(2d)} \\max_{j \\in \\{1,\\dots,d\\}} \\sqrt{\\sum_{i=1}^{n} v_{ij}^{2}}\n$$\n在我们的例子中，$\\mathbf{v}_i = \\mathbf{x}_i$。给定对所有 $i$ 都有 $\\|\\mathbf{x}_{i}\\|_{\\infty} \\leq r$，这意味着对所有 $i,j$ 都有 $|x_{ij}| \\leq r$。我们可以对平方和进行放缩：\n$$\n\\sum_{i=1}^{n} x_{ij}^{2} \\leq \\sum_{i=1}^{n} r^{2} = n r^{2}\n$$\n这对任意坐标 $j$ 都成立。因此，$\\max_{j} \\sqrt{\\sum_{i=1}^{n} x_{ij}^{2}} \\leq \\sqrt{n r^{2}} = r\\sqrt{n}$。\n将此代入最大值不等式：\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty} \\right] \\leq r \\sqrt{n} \\sqrt{2 \\ln(2d)}\n$$\n最后，我们将此代回 $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1})$ 的表达式：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) \\leq \\frac{B_{1}}{n} \\left( r \\sqrt{n} \\sqrt{2 \\ln(2d)} \\right) = \\frac{B_{1} r \\sqrt{2 \\ln(2d)}}{\\sqrt{n}}\n$$\n由于这个上界与样本 $S$ 无关，它也是总 Rademacher 复杂度 $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ 的一个上界。\n\n**第2步：推导 $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$ 的上界**\n\n假设类别 $\\mathcal{F}_{2}$ 定义为 $\\mathcal{F}_{2} = \\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{2} \\leq B_{2} \\}$。$\\mathcal{F}_{2}$ 的经验 Rademacher 复杂度为：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{w}^{\\top}\\mathbf{x}_{i} \\right] = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\mathbf{w}^{\\top} \\left( \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) \\right]\n$$\n$\\ell_2$-范数是自对偶的。因此：\n$$\n\\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\mathbf{w}^{\\top} \\left( \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) = B_{2} \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}\n$$\n将此代回，我们得到：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) = \\frac{B_{2}}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right]\n$$\n我们可以使用琴生不等式来给定期望的上界，因为平方根函数是凹函数：$\\mathbb{E}[\\sqrt{X}] \\leq \\sqrt{\\mathbb{E}[X]}$。\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sqrt{\\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2}} \\right] \\leq \\sqrt{\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2} \\right]}\n$$\n让我们计算平方根内的项：\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2} \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left(\\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i}\\right)^{\\top} \\left(\\sum_{j=1}^{n} \\sigma_{j} \\mathbf{x}_{j}\\right) \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\sigma_{i}\\sigma_{j} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{j}) \\right]\n$$\n根据期望的线性性质，我们将期望移到求和符号内。由于 $\\sigma_i$ 是独立的 Rademacher 变量，$\\mathbb{E}[\\sigma_i]=0$ 且 $\\mathbb{E}[\\sigma_i^2]=1$。因此，$\\mathbb{E}[\\sigma_i \\sigma_j] = \\delta_{ij}$（克罗内克 δ）。\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{j}) \\mathbb{E}_{\\boldsymbol{\\sigma}}[\\sigma_{i}\\sigma_{j}] = \\sum_{i=1}^{n} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}) \\mathbb{E}[\\sigma_{i}^{2}] = \\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2}\n$$\n给定对所有 $i$ 都有 $\\|\\mathbf{x}_{i}\\|_{2} \\leq R$。所以，$\\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2} \\leq \\sum_{i=1}^{n} R^{2} = nR^{2}$。\n综上所述：\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right] \\leq \\sqrt{\\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2}} \\leq \\sqrt{nR^{2}} = R\\sqrt{n}\n$$\n将此代入 $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2})$ 的表达式中：\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) \\leq \\frac{B_{2}}{n} (R\\sqrt{n}) = \\frac{B_{2}R}{\\sqrt{n}}\n$$\n这个上界也与 $S$ 无关，因此它也是 $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$ 的一个上界。\n\n**第3步：比较两个上界**\n\n我们想要找到最大的稀疏度 $s_{\\star}$，使得 $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ 的上界小于或等于 $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$ 的上界：\n$$\n\\frac{B_{1} r \\sqrt{2 \\ln(2d)}}{\\sqrt{n}} \\leq \\frac{B_{2}R}{\\sqrt{n}}\n$$\n由于 $n \\geq 2$，我们可以两边同乘以 $\\sqrt{n}$：\n$$\nB_{1} r \\sqrt{2 \\ln(2d)} \\leq B_{2}R\n$$\n题目规定我们应设置 $B_{1} = \\sqrt{s}B_{2}$ 来确保 $s$-稀疏向量 $\\mathbf{w}^{\\star}$ 属于 $\\mathcal{F}_{1}$。将此代入不等式：\n$$\n(\\sqrt{s}B_{2}) r \\sqrt{2 \\ln(2d)} \\leq B_{2}R\n$$\n由于 $B_2  0$，我们可以两边同除以 $B_{2}$：\n$$\n\\sqrt{s} \\, r \\sqrt{2 \\ln(2d)} \\leq R\n$$\n现在，我们求解 $s$。由于 $r, R  0$ 且 $d \\ge 2$，所有项都为正。\n$$\n\\sqrt{s} \\leq \\frac{R}{r \\sqrt{2 \\ln(2d)}}\n$$\n两边平方得到关于 $s$ 的条件：\n$$\ns \\leq \\frac{R^2}{r^2 (2 \\ln(2d))} = \\frac{R^2}{2r^2 \\ln(2d)}\n$$\n满足此不等式的 $s$ 的最大值是右侧的表达式。这就是所求的稀疏度 $s_{\\star}$。",
            "answer": "$$\\boxed{\\frac{R^2}{2 r^2 \\ln(2d)}}$$"
        }
    ]
}