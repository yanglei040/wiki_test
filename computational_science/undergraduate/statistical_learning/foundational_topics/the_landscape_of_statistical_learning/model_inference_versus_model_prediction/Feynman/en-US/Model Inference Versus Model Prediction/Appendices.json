{
    "hands_on_practices": [
        {
            "introduction": "When adding a new predictor to a linear model, how should its value be judged? We can use an inferential lens, asking if it explains a statistically significant portion of the variance in our existing data. Alternatively, we can use a predictive lens, asking if it improves our model's performance on new, unseen data. This exercise  challenges you to contrast these two perspectives by comparing the in-sample partial $R^2$ with the out-of-sample predictive $R^2$, revealing the pitfalls of overfitting and the crucial distinction between statistical significance and predictive power.",
            "id": "3149009",
            "problem": "A data analyst is studying a linear response $Y$ and a set of $p$ baseline predictors $X_1,\\dots,X_p$ using ordinary least squares (OLS). The analyst considers adding one more predictor $Z$ and wants to reason carefully about two distinct quantities:\n\n- The partial $R^2$ of $Z$ given $X_1,\\dots,X_p$ computed on the training sample, intended for inference about the additional explained variance attributable to $Z$ in the population, conditional on $X_1,\\dots,X_p$.\n- The holdout predictive $R^2$ computed on an independent test set, intended to measure out-of-sample predictive performance after adding $Z$.\n\nAssume the classical linear model with independent and identically distributed (i.i.d.) errors holds for the training data, and that the test set is drawn from the same data-generating mechanism as the training set. The analyst is also concerned about overfitting when $p$ is large relative to the training sample size $n_{\\text{train}}$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. Under the classical linear model with correct specification, the sample partial $R^2$ of $Z$ given $X_1,\\dots,X_p$ is an unbiased estimator of the population fraction of variance in $Y$ explained uniquely by $Z$ conditional on $X_1,\\dots,X_p$.\n\nB. If $Z$ carries no additional signal beyond $X_1,\\dots,X_p$ (its population coefficient is zero), then in expectation over the training sample the training $R^2$ must weakly increase upon adding $Z$, while the holdout predictive $R^2$ strictly decreases on average.\n\nC. If the partial $F$-test for $Z$ is statistically significant at level $0.05$ on the training data, then the holdout predictive $R^2$ is guaranteed to increase (relative to the reduced model without $Z$).\n\nD. Partial $R^2$ is invariant to any one-to-one rescaling of $Y$, but it can change under rescaling of the predictors (for example, measuring $Z$ in meters versus centimeters).\n\nE. In an extreme overfitting regime where $p \\approx n_{\\text{train}} - 1$, it is possible for the training-sample partial $R^2$ of an actually irrelevant $Z$ to be arbitrarily close to $1$, while the holdout predictive $R^2$ becomes very negative.",
            "solution": "The user has requested a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Response variable: Linear response $Y$.\n- Baseline predictors: A set of $p$ predictors $X_1, \\dots, X_p$.\n- Additional predictor: $Z$.\n- Statistical method: Ordinary Least Squares (OLS).\n- Quantities of interest:\n    1. Partial $R^2$ of $Z$ given $X_1, \\dots, X_p$, computed on a training sample for inference.\n    2. Holdout predictive $R^2$, computed on an independent test set for out-of-sample performance.\n- Assumptions:\n    1. The classical linear model with independent and identically distributed (i.i.d.) errors holds for the training data.\n    2. The test set is drawn from the same data-generating mechanism as the training set.\n- A specific concern is mentioned: overfitting when $p$ is large relative to the training sample size $n_{\\text{train}}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound, well-posed, and objective.\n- **Scientific Grounding**: The problem deals with fundamental and well-established concepts in statistical learning and linear regression theory, namely OLS, partial $R^2$, predictive $R^2$, overfitting, and the classical linear model assumptions. These are standard topics in any advanced statistics or econometrics curriculum.\n- **Well-Posed**: The problem sets up a clear comparison between two distinct statistical measures (in-sample partial $R^2$ for inference vs. out-of-sample predictive $R^2$ for performance) under a standard set of assumptions. The question asks to evaluate the correctness of several statements, which is a mathematically and logically addressable task.\n- **Objective**: The language is technical and precise, using standard terminology from statistics. It is free of subjective or ambiguous phrasing that would prevent a rigorous analysis.\n- **Completeness**: The problem provides all necessary information and assumptions required to evaluate the statements. It defines the context (OLS, classical linear model), the data structure (training and test sets), and the quantities to be compared.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. The analysis can proceed.\n\n### Solution Derivation\n\nLet us define the models and quantities formally. Let the training sample have size $n_{\\text{train}}$.\nThe reduced model is:\n$$ Y = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j + \\epsilon $$\nThe full model is:\n$$ Y = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j + \\theta Z + \\epsilon $$\nLet $SSR_{\\text{red}}$ be the sum of squared residuals from fitting the reduced model on the training data.\nLet $SSR_{\\text{full}}$ be the sum of squared residuals from fitting the full model on the training data.\n\nThe **training-sample partial $R^2$** of $Z$ given $X = \\{X_1, \\dots, X_p\\}$ is defined as:\n$$ R^2_{Z|X} = \\frac{SSR_{\\text{red}} - SSR_{\\text{full}}}{SSR_{\\text{red}}} $$\nThis measures the proportion of variance in $Y$ unexplained by $X$ that is subsequently explained by the addition of $Z$.\n\nThe **holdout predictive $R^2$** is computed on an independent test set of size $n_{\\text{test}}$. Let $Y_{\\text{test},i}$ be the observed values and $\\hat{Y}_{\\text{test},i}$ be the predicted values from a model fit on the training data. Let $\\bar{Y}_{\\text{test}}$ be the mean of the response in the test set. The predictive $R^2$ is:\n$$ R^2_{\\text{pred}} = 1 - \\frac{\\sum_{i=1}^{n_{\\text{test}}} (Y_{\\text{test},i} - \\hat{Y}_{\\text{test},i})^2}{\\sum_{i=1}^{n_{\\text{test}}} (Y_{\\text{test},i} - \\bar{Y}_{\\text{test}})^2} = 1 - \\frac{SSE_{\\text{test}}}{TSS_{\\text{test}}} $$\nThis quantity can be negative if the model's predictions are worse than simply using the test set mean (i.e., $SSE_{\\text{test}} > TSS_{\\text{test}}$).\n\nNow, we evaluate each statement.\n\n**A. Under the classical linear model with correct specification, the sample partial $R^2$ of $Z$ given $X_1,\\dots,X_p$ is an unbiased estimator of the population fraction of variance in $Y$ explained uniquely by $Z$ conditional on $X_1,\\dots,X_p$.**\n\nThe sample $R^2$ is a known biased estimator of the population $R^2$, as it tends to be optimistically high. The same principle applies to partial $R^2$. By construction, adding any predictor $Z$ (that is not a perfect linear combination of the $X$ predictors) to a model will result in $SSR_{\\text{full}} < SSR_{\\text{red}}$ for a given sample, thus yielding a strictly positive partial $R^2$ value. This is true even if the true population coefficient $\\theta$ of $Z$ is zero. In that case, the population partial $R^2$ is zero, but the expected value of the sample partial $R^2$, $E[R^2_{Z|X}]$, will be greater than zero due to chance correlation in the sample. Therefore, the sample partial $R^2$ is a biased estimator of its population counterpart, with the bias being positive. The adjusted $R^2$ statistic is designed to correct for this very bias.\n\nVerdict: **Incorrect**.\n\n**B. If $Z$ carries no additional signal beyond $X_1,\\dots,X_p$ (its population coefficient is zero), then in expectation over the training sample the training $R^2$ must weakly increase upon adding $Z$, while the holdout predictive $R^2$ strictly decreases on average.**\n\nThis statement has two parts.\n1.  **\"the training $R^2$ must weakly increase upon adding $Z$\"**: When adding a predictor to an OLS model, the sum of squared residuals of the new model can never be greater than that of the old model ($SSR_{\\text{full}} \\le SSR_{\\text{red}}$). Since the total sum of squares ($TSS$) is constant for the training data, the coefficient of determination $R^2 = 1 - SSR/TSS$ must be non-decreasing (weakly increasing). This holds for any single training sample, so it certainly holds in expectation.\n2.  **\"the holdout predictive $R^2$ strictly decreases on average\"**: If $Z$'s population coefficient is zero, adding it to the model means we are estimating an extra, unnecessary parameter. The model estimation process on the training data will likely assign a non-zero coefficient $\\hat{\\theta}$ to $Z$ due to sampling noise. This leads to overfitting. When this overfitted model is used for prediction on a holdout set, it incorporates this spurious relationship. The expected out-of-sample prediction error consists of three components: irreducible error, bias squared, and prediction variance. While OLS gives unbiased coefficient estimates (so model bias is zero), adding the irrelevant predictor $Z$ increases the variance of the predictions $\\hat{Y}$. A higher variance in prediction leads to a higher expected mean squared error on the test set. Since $R^2_{\\text{pred}} = 1 - MSE_{\\text{test}} / Var(Y_{\\text{test}})$, a higher expected $MSE_{\\text{test}}$ leads to a lower expected $R^2_{\\text{pred}}$. The decrease is strict (on average) as long as $Z$ is not perfectly orthogonal to the other predictors in the population, which would cause the prediction variance to strictly increase.\n\nVerdict: **Correct**.\n\n**C. If the partial $F$-test for $Z$ is statistically significant at level $0.05$ on the training data, then the holdout predictive $R^2$ is guaranteed to increase (relative to the reduced model without $Z$).**\n\nStatistical significance is a probabilistic concept. A significant partial $F$-test at level $\\alpha=0.05$ for the coefficient of $Z$ means that *if* the null hypothesis ($H_0: \\theta=0$) were true, the probability of observing a test statistic as or more extreme than the one computed from the sample is less than $0.05$. This does not guarantee that the null hypothesis is false. It is possible that the null hypothesis is true and our sample is one of the $5\\%$ that yields a significant result by chance (a Type I error). In such a case, we would be adding an irrelevant variable to our model. As established in the analysis of option B, adding an irrelevant variable increases model complexity and is expected to decrease, not increase, predictive performance on a holdout set. The word \"guaranteed\" makes this statement definitively false. There is no deterministic link between in-sample statistical significance and out-of-sample predictive improvement.\n\nVerdict: **Incorrect**.\n\n**D. Partial $R^2$ is invariant to any one-to-one rescaling of $Y$, but it can change under rescaling of the predictors (for example, measuring $Z$ in meters versus centimeters).**\n\nThis statement has two parts.\n1.  **\"invariant to any one-to-one rescaling of $Y$\"**: The term \"rescaling\" in the context of linear models typically implies a linear transformation, $Y' = aY + b$ where $a \\ne 0$. If $Y$ is transformed to $Y' = aY+b$, the residuals are scaled by $a$, so $e' = ae$. Consequently, the sum of squared residuals is scaled by $a^2$, so $SSR' = a^2 SSR$. Both $SSR'_{\\text{red}}$ and $SSR'_{\\text{full}}$ are scaled by the same factor $a^2$. The partial $R^2$ is a ratio of such quantities:\n    $$ R'^2_{Z|X} = \\frac{SSR'_{\\text{red}} - SSR'_{\\text{full}}}{SSR'_{\\text{red}}} = \\frac{a^2 SSR_{\\text{red}} - a^2 SSR_{\\text{full}}}{a^2 SSR_{\\text{red}}} = \\frac{a^2(SSR_{\\text{red}} - SSR_{\\text{full}})}{a^2 SSR_{\\text{red}}} = R^2_{Z|X} $$\n    So, it is invariant to linear rescaling. However, the term \"one-to-one\" could include non-linear transformations (e.g., $Y' = \\log(Y)$), which would not preserve the partial $R^2$. Ambiguity aside, the second part of the statement is definitively false.\n2.  **\"it can change under rescaling of the predictors\"**: Let's consider a linear rescaling of a predictor, e.g., $Z' = cZ$ for a constant $c \\ne 0$. The column space spanned by the predictors $[X_1, \\dots, X_p, Z]$ is identical to the column space spanned by $[X_1, \\dots, X_p, Z']$. Since the OLS fit is determined by the projection of $Y$ onto this column space, the projection matrix, the fitted values $\\hat{Y}$, and the residuals are all unchanged. Hence, $SSR_{\\text{full}}$ is unchanged. Similarly, rescaling any of the $X_j$ predictors does not change the column space of the reduced or full model. Therefore, both $SSR_{\\text{red}}$ and $SSR_{\\text{full}}$ are invariant to linear rescaling of any of the predictors. As a result, the partial $R^2$ is also invariant. The statement claims it can change, which is false for linear rescaling.\n\nSince the second part of the statement is incorrect, the entire statement is incorrect.\n\nVerdict: **Incorrect**.\n\n**E. In an extreme overfitting regime where $p \\approx n_{\\text{train}} - 1$, it is possible for the training-sample partial $R^2$ of an actually irrelevant $Z$ to be arbitrarily close to $1$, while the holdout predictive $R^2$ becomes very negative.**\n\nThis statement describes a classic overfitting scenario.\n1.  **\"training-sample partial $R^2$ ... arbitrarily close to $1$\"**: Let's consider the number of parameters to be estimated. The reduced model estimates $p+1$ parameters (including an intercept). The full model estimates $p+2$ parameters. If $p+1 = n_{\\text{train}}-1$ (i.e., $p = n_{\\text{train}}-2$), the full model has $p+2=n_{\\text{train}}$ parameters to estimate from $n_{\\text{train}}$ data points. Such a model will fit the training data perfectly, resulting in $SSR_{\\text{full}} = 0$. The reduced model has $n_{\\text{train}}-1$ parameters and will have a non-zero (though likely small) $SSR_{\\text{red}}$. In this case, the partial $R^2$ is:\n    $$ R^2_{Z|X} = \\frac{SSR_{\\text{red}} - 0}{SSR_{\\text{red}}} = 1 $$\n    So, it is not only possible for it to be arbitrarily close to $1$, but it can be exactly $1$. This holds even if $Z$ is \"actually irrelevant\" (its population coefficient is zero).\n2.  **\"holdout predictive $R^2$ becomes very negative\"**: A model that has been forced to fit the noise in the training data by using up all its degrees of freedom will have extremely large and unstable coefficient estimates. When applied to a new, independent test set, its predictions $\\hat{Y}_{\\text{test}}$ will be very poor and can deviate wildly from the true values $Y_{\\text{test}}$. The sum of squared errors on the test set, $SSE_{\\text{test}} = \\sum (Y_{\\text{test}} - \\hat{Y}_{\\text{test}})^2$, can therefore become much larger than the total sum of squares, $TSS_{\\text{test}} = \\sum (Y_{\\text{test}} - \\bar{Y}_{\\text{test}})^2$. In this situation, the predictive $R^2 = 1 - SSE_{\\text{test}}/TSS_{\\text{test}}$ will become a large negative number.\n\nThis statement correctly captures the dual phenomena of perfect in-sample fit and disastrous out-of-sample performance characteristic of extreme overfitting.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{BE}$$"
        },
        {
            "introduction": "When we say a feature is \"important,\" what do we mean? This question often has two different answers depending on our goal. An inferential approach, using tools like the Wald test, asks if a feature has a statistically significant relationship with the response, conditional on other features. A predictive approach, using methods like permutation importance, asks how much the model's accuracy suffers if we lose access to that feature's information. This practice problem  presents scenarios with feature interactions and high correlation to demonstrate why these two notions of importance can dramatically diverge.",
            "id": "3148898",
            "problem": "A supervised learning analyst is comparing prediction-oriented feature importance with inference-oriented coefficient significance in a linear regression setting. Consider a training dataset of size $n$ with two standardized features $X_1$ and $X_2$ drawn from a mean-zero bivariate normal distribution with unit variances and correlation $\\rho = 0.95$, and a response\n$$\nY \\;=\\; \\beta_1 X_1 \\;+\\; \\beta_2 X_2 \\;+\\; \\beta_3 X_1 X_2 \\;+\\; \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ and $\\sigma^2$ is finite and fixed. The analyst fits the correctly specified linear model with regressors $X_1$, $X_2$, and $X_1 X_2$ by Ordinary Least Squares (OLS), and evaluates two quantities for each feature:\n- Permutation feature importance for $X_j$ defined as the expected increase in prediction error (mean squared error) when the observed $X_j$ values are randomly permuted in the test set, holding the fitted model parameters fixed.\n- The Wald test for the null hypothesis $H_0:\\beta_j = 0$, based on the OLS estimator’s asymptotic normality, i.e., $Z_j = \\widehat{\\beta}_j/\\mathrm{SE}(\\widehat{\\beta}_j)$, which under $H_0$ is approximately standard normal.\n\nThe analyst examines two scenarios:\n- Scenario $1$: $\\beta_1 = 0$, $\\beta_2 = 0$, $\\beta_3 = 1$.\n- Scenario $2$: $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_3 = 0$.\n\nAssume $n$ is large enough that asymptotic approximations for OLS and the Wald tests are meaningful, and that permutation importance is evaluated on a fresh test set from the same distribution.\n\nWhich of the following statements are correct in light of the fundamental definitions of permutation importance and the Wald test, and the effects of collinearity and interactions?\n\nA. In Scenario $1$, the permutation importance for $X_1$ can be large even though the Wald test for $H_0:\\beta_1=0$ is non-significant, because permuting $X_1$ destroys the interaction signal $X_1 X_2$ that drives prediction, while the Wald test for $\\beta_1$ only probes the main-effect coefficient.\n\nB. In Scenario $2$, severe collinearity inflates the variance of $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$, so individual Wald tests for $H_0:\\beta_1=0$ and $H_0:\\beta_2=0$ may be non-significant; at the same time, the permutation importance for either $X_1$ or $X_2$ can be small because permuting one redundant feature leaves the other to carry most of the predictive signal.\n\nC. In Scenario $1$, the Wald test for $H_0:\\beta_3=0$ will be non-significant because collinearity between $X_1$, $X_2$, and $X_1 X_2$ hides the interaction effect in finite samples.\n\nD. When the model is correctly specified and sample size $n$ is large, permutation importance and Wald test results necessarily agree on feature importance rankings.\n\nE. If the analyst refits the model in Scenario $1$ without the interaction term $X_1 X_2$, the permutation importance for $X_1$ must become larger than it was in the correctly specified model, because the model will rely more on the main effect to compensate for the missing interaction.",
            "solution": "The user wants me to act as a meticulous STEM professor and solve a statistical learning problem.\n\n### PROBLEM VALIDATION\n\n#### Step 1: Extract Givens\n- **Setting**: A supervised learning analysis of a linear regression model.\n- **Data**: Training set of size $n$.\n- **Features**: Two standardized features, $X_1$ and $X_2$.\n- **Feature Distribution**: $(X_1, X_2) \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$, where the mean vector is $\\boldsymbol{0}$, and the covariance matrix is $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with $\\rho = 0.95$.\n- **True Response Model**: $Y = \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\varepsilon$.\n- **Error Term**: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, with $\\sigma^2$ finite and fixed.\n- **Fitted Model**: $\\widehat{Y} = \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 X_2 + \\widehat{\\beta}_3 (X_1 X_2)$, fitted by Ordinary Least Squares (OLS). This is a correctly specified model form.\n- **Metrics**:\n    1.  **Permutation Feature Importance**: For a feature $X_j$, defined as the expected increase in mean squared error (MSE) when the values of $X_j$ are randomly permuted in a test set, holding fitted parameters fixed.\n    2.  **Wald Test**: For the null hypothesis $H_0: \\beta_j=0$, using the test statistic $Z_j = \\widehat{\\beta}_j / \\mathrm{SE}(\\widehat{\\beta}_j)$.\n- **Assumptions**: $n$ is large (asymptotic OLS theory applies). Permutation importance is evaluated on a fresh test set from the same data-generating distribution.\n- **Scenarios**:\n    - **Scenario 1**: True coefficients are $\\beta_1 = 0$, $\\beta_2 = 0$, $\\beta_3 = 1$. The true model is $Y = X_1 X_2 + \\varepsilon$.\n    - **Scenario 2**: True coefficients are $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_3 = 0$. The true model is $Y = X_1 + X_2 + \\varepsilon$.\n\n#### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is entirely within the established framework of statistical learning and econometrics. It uses standard concepts like OLS regression, multicollinearity, interaction effects, Wald tests, and permutation importance. These are fundamental topics in the field. The setup is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem is well-posed. It provides two distinct, fully specified scenarios and asks for an evaluation of statements concerning the behavior of two well-defined statistical metrics. The assumptions (large $n$, fresh test set) are clear and sufficient to allow for a rigorous analysis.\n3.  **Objective**: The problem is stated using precise, objective, and formal mathematical and statistical language. There are no subjective or ambiguous terms.\n4.  **Flaw Checklist**: The problem does not violate any of the listed invalidity criteria. It is a standard, albeit conceptually challenging, problem exploring the difference between statistical inference and prediction.\n\n#### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed to the solution.\n\n### SOLUTION\n\nThis problem requires a careful analysis of how two different measures of feature importance—one inferential (Wald test) and one predictive (permutation importance)—behave under conditions of high collinearity and interaction effects.\n\nFirst, let's analyze the collinearity among the regressors used in the model: $X_1$, $X_2$, and the interaction term $X_{12} = X_1 X_2$.\n- The features $X_1$ and $X_2$ are highly collinear by definition, with $\\mathrm{Corr}(X_1, X_2) = \\rho = 0.95$.\n- Let's examine the correlation between the main effects and the interaction term. Since $X_1$ and $X_2$ are drawn from a mean-zero bivariate normal distribution, we can calculate their covariance with the product term $X_1 X_2$.\n$$ \\mathrm{Cov}(X_1, X_1 X_2) = E[X_1 (X_1 X_2)] - E[X_1]E[X_1 X_2] $$\nSince $E[X_1]=0$, this simplifies to $E[X_1^2 X_2]$. For jointly Gaussian random variables with zero mean, the expectation of the product of an odd number of variables is zero. Thus, $E[X_1^2 X_2] = 0$.\nBy symmetry, $\\mathrm{Cov}(X_2, X_1 X_2) = E[X_1 X_2^2] = 0$.\nThis means that the interaction term $X_1 X_2$ is uncorrelated with the main effect terms $X_1$ and $X_2$. Consequently, in the OLS regression on $\\{X_1, X_2, X_1 X_2\\}$, the variance of $\\widehat{\\beta_3}$ will not be inflated by collinearity with the main effects, whereas the variances of $\\widehat{\\beta_1}$ and $\\widehat{\\beta_2}$ will be heavily inflated due to the high correlation between $X_1$ and $X_2$. The variance inflation factor (VIF) for $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$ will be approximately $1/(1-\\rho^2) = 1/(1-0.95^2) \\approx 10.26$, while the VIF for $\\widehat{\\beta}_3$ will be close to $1$.\n\nNow, let's analyze the two scenarios.\n\n#### Scenario 1: $Y = X_1 X_2 + \\varepsilon$ (True coefficients: $\\beta_1=0, \\beta_2=0, \\beta_3=1$)\n- **Wald Tests**: The fitted model is $\\widehat{Y} = \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 X_2 + \\widehat{\\beta}_3 X_1 X_2$.\n    - For $H_0: \\beta_1 = 0$: The null hypothesis is true. Therefore, the Wald test will be non-significant with high probability (specifically, with probability $1-\\alpha$ at significance level $\\alpha$).\n    - For $H_0: \\beta_2 = 0$: Similarly, the null hypothesis is true, and the test will be non-significant.\n    - For $H_0: \\beta_3 = 0$: The null hypothesis is false, as the true $\\beta_3=1$. Since $n$ is large and the standard error of $\\widehat{\\beta}_3$ is not inflated by collinearity, the OLS estimator $\\widehat{\\beta}_3$ will be a consistent estimator of $1$, and the test statistic $Z_3 = \\widehat{\\beta}_3 / \\mathrm{SE}(\\widehat{\\beta}_3)$ will be large. The test will be highly significant.\n- **Permutation Importance**: The predictive power of the model comes exclusively from the term $\\widehat{\\beta}_3 X_1 X_2$, where $\\widehat{\\beta}_3 \\approx 1$. To calculate the permutation importance of $X_1$, we permute its values in the test set. This changes the predictions from $\\widehat{Y}_i \\approx X_{i1} X_{i2}$ to $\\widehat{Y}_i^{\\text{perm}} \\approx X_{\\pi(i),1} X_{i2}$, where $\\pi$ is a random permutation. Since $X_{\\pi(i),1}$ is now decorrelated from $X_{i1}$ and $Y_i$, the term $X_{\\pi(i),1} X_{i2}$ no longer tracks the signal component of $Y_i$. This breaks the model's predictive ability, causing a large increase in the mean squared error. Therefore, the permutation importance of $X_1$ will be large. By symmetry, the permutation importance of $X_2$ will also be large.\n\n#### Scenario 2: $Y = X_1 + X_2 + \\varepsilon$ (True coefficients: $\\beta_1=1, \\beta_2=1, \\beta_3=0$)\n- **Wald Tests**:\n    - For $H_0: \\beta_1 = 0$ and $H_0: \\beta_2 = 0$: These null hypotheses are false. However, due to the severe collinearity between $X_1$ and $X_2$ ($\\rho = 0.95$), the standard errors $\\mathrm{SE}(\\widehat{\\beta}_1)$ and $\\mathrm{SE}(\\widehat{\\beta}_2)$ will be very large. The model has difficulty attributing the shared predictive power uniquely to either $X_1$ or $X_2$. This large variance reduces the magnitude of the Wald statistics $Z_1$ and $Z_2$, significantly lowering the power of the tests. It is therefore quite possible, and indeed typical in such cases, for both individual tests to be non-significant, failing to reject the null hypotheses even though the true coefficients are non-zero.\n    - For $H_0: \\beta_3 = 0$: This null hypothesis is true. The test will be non-significant with high probability.\n- **Permutation Importance**: The model's predictive power comes from approximating the true signal $X_1 + X_2$. The fitted model is $\\widehat{Y} \\approx \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 X_2$. To find the permutation importance of $X_1$, we permute its values. The new prediction is $\\widehat{Y}^{\\text{perm}} \\approx \\widehat{\\beta}_1 X_{\\pi(i),1} + \\widehat{\\beta}_2 X_{i2}$. Because $X_1$ and $X_2$ are highly correlated, $X_2$ is a very good proxy for $X_1$. After permuting $X_1$ and effectively removing its direct contribution, the model can still make good predictions using just the $X_2$ term. The information in $X_1$ is largely redundant given $X_2$. Consequently, the increase in prediction error will be small. The permutation importance for $X_1$ will be small. By symmetry, the same holds for $X_2$.\n\nWith this foundation, let us evaluate the given options.\n\n---\n**Option-by-Option Analysis**\n\n**A. In Scenario 1, the permutation importance for $X_1$ can be large even though the Wald test for $H_0:\\beta_1=0$ is non-significant, because permuting $X_1$ destroys the interaction signal $X_1 X_2$ that drives prediction, while the Wald test for $\\beta_1$ only probes the main-effect coefficient.**\n- Our analysis of Scenario 1 showed that the permutation importance for $X_1$ is indeed large, as permuting it breaks the model's sole predictive component.\n- Our analysis also showed that the Wald test for $H_0:\\beta_1=0$ will be non-significant because the true coefficient $\\beta_1$ is zero.\n- The reasoning provided is accurate: permutation importance captures the predictive value of $X_1$ in any form, including interactions, while the Wald test is specific to the coefficient of the $X_1$ main effect term.\n- This statement correctly contrasts the two metrics and their behavior in the presence of interactions.\n- **Verdict: Correct.**\n\n**B. In Scenario 2, severe collinearity inflates the variance of $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$, so individual Wald tests for $H_0:\\beta_1=0$ and $H_0:\\beta_2=0$ may be non-significant; at the same time, the permutation importance for either $X_1$ or $X_2$ can be small because permuting one redundant feature leaves the other to carry most of the predictive signal.**\n- Our analysis of Scenario 2 showed that high collinearity inflates the coefficient variances, which can lead to non-significant Wald tests despite the true coefficients being non-zero.\n- Our analysis also showed that permutation importance for either feature can be small due to their redundancy; the presence of one suffices for good prediction.\n- The reasoning provided for both phenomena is precisely as derived.\n- This statement correctly describes the effects of multicollinearity on both inferential and predictive importance measures.\n- **Verdict: Correct.**\n\n**C. In Scenario 1, the Wald test for $H_0:\\beta_3=0$ will be non-significant because collinearity between $X_1$, $X_2$, and $X_1 X_2$ hides the interaction effect in finite samples.**\n- Our analysis of Scenario 1 concluded that the Wald test for $H_0:\\beta_3=0$ will be *significant*, as the true $\\beta_3=1$, $n$ is large, and the standard error of $\\widehat{\\beta}_3$ is not inflated by collinearity.\n- The premise that there is problematic collinearity involving the interaction term $X_1 X_2$ is false for this data distribution.\n- **Verdict: Incorrect.**\n\n**D. When the model is correctly specified and sample size $n$ is large, permutation importance and Wald test results necessarily agree on feature importance rankings.**\n- This statement claims a necessary agreement. Scenarios 1 and 2 serve as direct counterexamples.\n- In Scenario 1, permutation importance ranks $X_1$ and $X_2$ as highly important, while their main effect Wald tests show they are not significant. This is a disagreement.\n- In Scenario 2, permutation importance ranks $X_1$ and $X_2$ as having low importance, while the true underlying model requires them both (i.e., their true coefficients are non-zero). If one could overcome the collinearity issue (e.g., via a joint F-test), one would find them significant.\n- The fundamental difference in what these two metrics measure (marginal predictive contribution vs. conditional parametric significance) means they do not have to agree, even in ideal asymptotic conditions.\n- **Verdict: Incorrect.**\n\n**E. If the analyst refits the model in Scenario 1 without the interaction term $X_1 X_2$, the permutation importance for $X_1$ must become larger than it was in the correctly specified model, because the model will rely more on the main effect to compensate for the missing interaction.**\n- In Scenario 1, the true model is $Y = X_1 X_2 + \\varepsilon$. We refit a misspecified model $\\widehat{Y} = \\widehat{\\gamma}_1 X_1 + \\widehat{\\gamma}_2 X_2$.\n- The OLS coefficients $\\widehat{\\gamma}_1, \\widehat{\\gamma}_2$ converge to the coefficients of the best linear projection of $Y$ onto the span of $\\{X_1, X_2\\}$. The population coefficients are $\\gamma_1 = \\mathrm{Cov}(Y, X_1)/\\mathrm{Var}(X_1)$ and $\\gamma_2=\\mathrm{Cov}(Y,X_2)/\\mathrm{Var}(X_2)$ in an orthogonal basis, but here we require a multiple regression projection. The true coefficients of the projection are given by $(E[X^T X])^{-1} E[X^T Y]$. We found $E[X^T Y] = (E[X_1 Y], E[X_2 Y])^T = (E[X_1^2 X_2], E[X_1 X_2^2])^T = (0, 0)^T$.\n- Thus, the coefficients for the misspecified model, $\\widehat{\\gamma}_1$ and $\\widehat{\\gamma}_2$, will converge to $0$. The model $\\widehat{Y}_{\\text{miss}} \\approx 0$ has no predictive power.\n- The permutation importance of $X_1$ in a model that always predicts zero is zero, as permuting the feature does not change the prediction or the error.\n- The permutation importance of $X_1$ was large in the correctly specified model. It becomes approximately zero in the misspecified model. Therefore, it becomes smaller, not larger.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "In machine learning, we often deal with two kinds of uncertainty: uncertainty about our model's parameters (an inferential concern) and uncertainty in our model's predictions (a predictive concern). It is tempting to think these two are always aligned, but this is not the case. This hands-on coding exercise  guides you through implementing a regularized logistic regression model from scratch to explore this distinction. By tuning the model's regularization and querying it at different points, you will see firsthand how inferential variance can decrease while predictive uncertainty simultaneously increases, a key trade-off in building robust models.",
            "id": "3149003",
            "problem": "You are given a binary classification setting in statistical learning and asked to carefully compare model prediction and model inference. Consider the Bernoulli conditional model with a logistic link: the observable label $y \\in \\{0,1\\}$ is modeled given a feature vector $x \\in \\mathbb{R}^d$ and a parameter vector $\\theta \\in \\mathbb{R}^d$ via the conditional probability $p(y=1 \\mid x,\\theta) = \\sigma(\\theta^\\top x)$, where the logistic sigmoid $\\sigma(z)$ is defined by the fundamental relation $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$. The predictive entropy at a point $x$ for a fitted parameter $\\hat{\\theta}$ is given by the Shannon entropy in natural units (nats), $H(x) = -\\hat{p}(y=1 \\mid x)\\ln \\hat{p}(y=1 \\mid x) - \\hat{p}(y=0 \\mid x)\\ln \\hat{p}(y=0 \\mid x)$.\n\nTo estimate the parameter vector, use Maximum a Posteriori (MAP) under a zero-mean isotropic Gaussian prior with precision $\\lambda > 0$, which implies a prior density proportional to $\\exp\\!\\left(-\\frac{\\lambda}{2}\\|\\theta\\|_2^2\\right)$. The training data are pairs $(x_i,y_i)$ for $i=1,\\dots,n$ with $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}^d$. The posterior over $\\theta$ is proportional to the product of the likelihood and the prior. Starting only from the core definitions of the Bernoulli likelihood for independent samples, the logistic link, and the Gaussian prior, derive the gradient and the Hessian of the log-posterior. Then, implement an iterative second-order method to find a stationary point $\\hat{\\theta}$ of the log-posterior (the MAP estimate). Use a local quadratic approximation of the log-posterior at $\\hat{\\theta}$ to define an inferential covariance matrix $\\widehat{\\Sigma}$ as the inverse of the negative Hessian at $\\hat{\\theta}$. The inferential variance measure you must report is the trace of $\\widehat{\\Sigma}$, denoted $\\operatorname{tr}(\\widehat{\\Sigma})$, which aggregates parameter-wise uncertainties. For prediction at a query point $x^\\star$, compute $\\hat{p}(y=1 \\mid x^\\star) = \\sigma(\\hat{\\theta}^\\top x^\\star)$ and its entropy $H(x^\\star)$ using natural logarithms. In summary, for each test case you must output the pair consisting of the predictive entropy and the inferential variance trace, i.e., [$H(x^\\star)$, $\\operatorname{tr}(\\widehat{\\Sigma})$].\n\nYour program must implement the following without any external input:\n\n- Use the fixed training set with $n = 10$ and $d = 3$ (including an intercept term). The feature vectors include an intercept as their first component equal to $1$. The positive class ($y_i = 1$) has five points with features $(x_{i,1}, x_{i,2}, x_{i,3})$ equal to $(1, 2.0, 2.0)$, $(1, 2.5, 1.5)$, $(1, 1.7, 2.3)$, $(1, 2.2, 1.8)$, $(1, 1.8, 2.1)$. The negative class ($y_i = 0$) has five points with features $(x_{i,1}, x_{i,2}, x_{i,3})$ equal to $(1, -2.0, -2.0)$, $(1, -2.5, -1.5)$, $(1, -1.7, -2.3)$, $(1, -2.2, -1.8)$, $(1, -1.8, -2.1)$.\n\n- Starting only from the definitions, correctly derive and implement the MAP estimation via an iterative second-order ascent procedure that uses the gradient and the Hessian of the log-posterior. Ensure convergence to a stationary point by checking the norm of the parameter update at each iteration and stop when it is below a small tolerance.\n\n- At the MAP estimate $\\hat{\\theta}$, use the local quadratic approximation to form the inferential covariance $\\widehat{\\Sigma}$ as the inverse of the negative Hessian at $\\hat{\\theta}$, and compute $\\operatorname{tr}(\\widehat{\\Sigma})$.\n\n- For each query, compute the predictive probability $\\hat{p}(y=1 \\mid x^\\star)$ and the predictive entropy $H(x^\\star)$ using natural logarithms (nats).\n\nDesign and analyze conditions where predictive entropy increases while inferential variance decreases by appropriately choosing the prior precision $\\lambda$ and the query $x^\\star$. The following four test cases are prescribed to probe different facets:\n\n- Test case A (happy path: low regularization, confident prediction): prior precision $\\lambda = 0.1$, query vector $x^\\star$ components equal to $1$, $2.5$, $2.0$.\n\n- Test case B (increasing regularization to shrink parameters): prior precision $\\lambda = 20.0$, query vector $x^\\star$ components equal to $1$, $2.5$, $2.0$.\n\n- Test case C (edge case: zero non-intercept features): prior precision $\\lambda = 0.1$, query vector $x^\\star$ components equal to $1$, $0.0$, $0.0$.\n\n- Test case D (boundary stress: strong regularization, large query magnitude): prior precision $\\lambda = 50.0$, query vector $x^\\star$ components equal to $1$, $6.0$, $6.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a two-element list of floats [$H(x^\\star), \\operatorname{tr}(\\widehat{\\Sigma})$] in that order. For example, the printed output must look like $[[h_1,v_1],[h_2,v_2],[h_3,v_3],[h_4,v_4]]$ with actual numeric values for $h_j$ and $v_j$ computed by your program.",
            "solution": "The problem requires a comparison of model prediction and model inference within a Bayesian logistic regression framework. This involves deriving and implementing a Maximum a Posteriori (MAP) estimation for the model parameters, and then computing two distinct measures: a predictive uncertainty metric (entropy) and an inferential uncertainty metric (trace of the parameter covariance matrix).\n\nThe solution proceeds in four stages:\n1.  Formulation of the log-posterior function from the specified likelihood and prior.\n2.  Derivation of the gradient and Hessian of the log-posterior, necessary for optimization.\n3.  Specification of the second-order optimization algorithm (Newton-Raphson) to find the MAP estimate $\\hat{\\theta}$.\n4.  Definition of the predictive and inferential uncertainty measures to be computed.\n\n**1. Log-Posterior Formulation**\n\nThe model for a binary label $y_i \\in \\{0, 1\\}$ given a feature vector $x_i \\in \\mathbb{R}^d$ is the Bernoulli distribution with probability $p_i = p(y_i=1 \\mid x_i, \\theta) = \\sigma(\\theta^\\top x_i)$, where $\\sigma(z) = (1+e^{-z})^{-1}$ is the logistic sigmoid function.\n\nThe likelihood for a single observation $(x_i, y_i)$ is $p(y_i \\mid x_i, \\theta) = p_i^{y_i} (1-p_i)^{1-y_i}$. The log-likelihood is $\\ln p(y_i \\mid x_i, \\theta) = y_i \\ln p_i + (1-y_i) \\ln(1-p_i)$. Using the identities $\\ln p_i = \\ln \\sigma(\\theta^\\top x_i) = -\\ln(1+e^{-\\theta^\\top x_i})$ and $\\ln(1-p_i) = \\ln(1-\\sigma(\\theta^\\top x_i)) = -\\theta^\\top x_i - \\ln(1+e^{-\\theta^\\top x_i})$, we can rewrite the log-likelihood as:\n$$\n\\ln p(y_i \\mid x_i, \\theta) = y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i})\n$$\nFor a dataset of $n$ independent samples $D = \\{(x_i, y_i)\\}_{i=1}^n$, the total log-likelihood is the sum over all samples:\n$$\nLL(\\theta) = \\sum_{i=1}^n \\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right)\n$$\nThe problem specifies a zero-mean isotropic Gaussian prior on the parameter vector $\\theta$, with precision $\\lambda > 0$. The prior density is $p(\\theta) \\propto \\exp\\left(-\\frac{\\lambda}{2}\\|\\theta\\|_2^2\\right)$. The log-prior, up to an additive constant, is:\n$$\n\\ln p(\\theta) = -\\frac{\\lambda}{2} \\theta^\\top \\theta\n$$\nThe log-posterior function $\\mathcal{L}(\\theta)$, which we aim to maximize for MAP estimation, is the sum of the log-likelihood and the log-prior:\n$$\n\\mathcal{L}(\\theta) = \\ln p(\\theta \\mid D) \\propto LL(\\theta) + \\ln p(\\theta) = \\left( \\sum_{i=1}^n \\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right) \\right) - \\frac{\\lambda}{2} \\theta^\\top \\theta\n$$\n\n**2. Gradient and Hessian of the Log-Posterior**\n\nTo optimize $\\mathcal{L}(\\theta)$, we compute its gradient and Hessian with respect to $\\theta$.\n\nThe gradient of the log-posterior is:\n$$\n\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\sum_{i=1}^n \\nabla_{\\theta}\\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right) - \\nabla_{\\theta}\\left(\\frac{\\lambda}{2} \\theta^\\top \\theta\\right)\n$$\nUsing the chain rule, $\\nabla_{\\theta}\\ln(1+e^{\\theta^\\top x_i}) = \\frac{e^{\\theta^\\top x_i}}{1+e^{\\theta^\\top x_i}} x_i = \\sigma(\\theta^\\top x_i)x_i = p_i x_i$. The gradient of the log-likelihood for one sample is $(y_i - p_i)x_i$. The gradient of the log-prior is $-\\lambda\\theta$.\nSumming over all samples, the full gradient is:\n$$\n\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\sum_{i=1}^n (y_i - \\sigma(\\theta^\\top x_i)) x_i - \\lambda\\theta\n$$\nIn matrix notation, with $X$ being the $n \\times d$ design matrix, $y$ the vector of labels, and $p$ the vector of probabilities, this is $g(\\theta) = X^\\top(y-p) - \\lambda\\theta$.\n\nThe Hessian is the matrix of second derivatives, $\\nabla^2_{\\theta} \\mathcal{L}(\\theta)$. The second derivative of the log-likelihood term for one sample is:\n$$\n\\nabla^2_{\\theta} \\left((y_i - p_i)x_i \\right) = -x_i (\\nabla_{\\theta} p_i)^\\top = -x_i \\left( \\sigma'(\\theta^\\top x_i) x_i \\right)^\\top = - \\sigma(\\theta^\\top x_i)(1-\\sigma(\\theta^\\top x_i)) x_i x_i^\\top\n$$\nThe Hessian of the log-prior term is $-\\lambda I$, where $I$ is the $d \\times d$ identity matrix.\nThe full Hessian of the log-posterior is:\n$$\nH(\\theta) = \\nabla^2_{\\theta} \\mathcal{L}(\\theta) = - \\sum_{i=1}^n \\sigma(\\theta^\\top x_i)(1-\\sigma(\\theta^\\top x_i)) x_i x_i^\\top - \\lambda I\n$$\nIn matrix notation, this is $H(\\theta) = -X^\\top W X - \\lambda I$, where $W$ is an $n \\times n$ diagonal matrix with elements $W_{ii} = p_i(1-p_i)$. Since $p_i(1-p_i) \\ge 0$, the matrix $X^\\top W X$ is positive semi-definite. For $\\lambda > 0$, the Hessian $H(\\theta)$ is strictly negative definite, which implies that the log-posterior $\\mathcal{L}(\\theta)$ is a strictly concave function. This guarantees the existence of a unique global maximum, which can be found efficiently.\n\n**3. Optimization via Newton-Raphson Method**\n\nWe use an iterative second-order method, specifically the Newton-Raphson algorithm, to find the MAP estimate $\\hat{\\theta}$ that maximizes $\\mathcal{L}(\\theta)$. Starting with an initial guess $\\theta_0$ (e.g., the zero vector), the parameters are updated iteratively:\n$$\n\\theta_{k+1} = \\theta_k - [H(\\theta_k)]^{-1} g(\\theta_k)\n$$\nwhere $g(\\theta_k)$ and $H(\\theta_k)$ are the gradient and Hessian evaluated at the current estimate $\\theta_k$. The update step $\\Delta\\theta_k = -[H(\\theta_k)]^{-1} g(\\theta_k)$ is found by solving the linear system $H(\\theta_k)\\Delta\\theta_k = -g(\\theta_k)$. The iteration continues until the norm of the update step, $\\|\\Delta\\theta_k\\|_2$, falls below a small tolerance. The resulting parameter vector is the MAP estimate $\\hat{\\theta}$.\n\n**4. Inferential and Predictive Measures**\n\nUpon convergence to $\\hat{\\theta}$, we compute the two specified quantities:\n\n*   **Inferential Variance:** The uncertainty in the parameter estimates is captured by the posterior covariance matrix. A local Gaussian approximation to the posterior distribution at its mode $\\hat{\\theta}$ is formed using the Hessian. The covariance matrix of this approximation is the inverse of the negative Hessian evaluated at the mode:\n    $$\n    \\widehat{\\Sigma} = [-H(\\hat{\\theta})]^{-1} = [X^\\top \\widehat{W} X + \\lambda I]^{-1}\n    $$\n    where $\\widehat{W}$ is the weight matrix evaluated at $\\hat{\\theta}$. This matrix is the observed Fisher information matrix regularized by the prior. The measure of total parameter uncertainty is the trace of this covariance matrix, $\\operatorname{tr}(\\widehat{\\Sigma})$.\n\n*   **Predictive Entropy:** For a new query point $x^\\star$, the model's prediction is the probability $\\hat{p}(y=1 \\mid x^\\star) = \\sigma(\\hat{\\theta}^\\top x^\\star)$. The uncertainty of this single prediction is quantified by the Shannon entropy of the corresponding Bernoulli distribution, measured in nats:\n    $$\n    H(x^\\star) = - \\hat{p}^\\star \\ln(\\hat{p}^\\star) - (1-\\hat{p}^\\star)\\ln(1-\\hat{p}^\\star)\n    $$\n    where $\\hat{p}^\\star = \\hat{p}(y=1 \\mid x^\\star)$. The entropy is maximized at $H=\\ln(2) \\approx 0.693$ when the prediction is most uncertain ($\\hat{p}^\\star = 0.5$) and is $0$ when the prediction is fully confident ($\\hat{p}^\\star = 0$ or $\\hat{p}^\\star = 1$).\n\nThe following program implements this entire procedure for the specified dataset and test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_entropy(p):\n    \"\"\"Calculates the Shannon entropy for a Bernoulli probability p.\"\"\"\n    if p <= 1e-12 or p >= 1 - 1e-12:\n        return 0.0\n    return -(p * np.log(p) + (1 - p) * np.log(1 - p))\n\ndef compute_map_estimate(X, y, lam, tol=1e-9, max_iter=100):\n    \"\"\"\n    Computes the MAP estimate for logistic regression using Newton's method.\n    \n    Args:\n        X (np.ndarray): Design matrix of shape (n, d).\n        y (np.ndarray): Target vector of shape (n,).\n        lam (float): Precision of the Gaussian prior.\n        tol (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n        \n    Returns:\n        tuple: A tuple containing:\n            - theta_hat (np.ndarray): The MAP parameter estimate of shape (d,).\n            - hessian_at_map (np.ndarray): The Hessian of the log-posterior at theta_hat.\n    \"\"\"\n    n, d = X.shape\n    theta = np.zeros(d)\n\n    for i in range(max_iter):\n        # Calculate linear predictors and probabilities\n        z = X @ theta\n        p = 1 / (1 + np.exp(-z))\n\n        # Calculate gradient of the log-posterior\n        grad = X.T @ (y - p) - lam * theta\n\n        # Calculate Hessian of the log-posterior\n        W_diag = p * (1 - p)\n        # Efficiently compute X.T @ W @ X without forming a dense W\n        Hess = -X.T @ (W_diag[:, np.newaxis] * X) - lam * np.identity(d)\n\n        # Solve the Newton step: H * delta = -g\n        delta_theta = np.linalg.solve(Hess, -grad)\n\n        # Update parameters\n        theta += delta_theta\n        \n        # Check for convergence\n        if np.linalg.norm(delta_theta) < tol:\n            break\n    \n    # After convergence, one final calculation of the Hessian at the final theta\n    z_final = X @ theta\n    p_final = 1 / (1 + np.exp(-z_final))\n    W_diag_final = p_final * (1 - p_final)\n    hessian_at_map = -X.T @ (W_diag_final[:, np.newaxis] * X) - lam * np.identity(d)\n    \n    return theta, hessian_at_map\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, running all test cases.\n    \"\"\"\n    # Define the fixed training set\n    X_train = np.array([\n        # Positive class (y=1)\n        [1.0, 2.0, 2.0],\n        [1.0, 2.5, 1.5],\n        [1.0, 1.7, 2.3],\n        [1.0, 2.2, 1.8],\n        [1.0, 1.8, 2.1],\n        # Negative class (y=0)\n        [1.0, -2.0, -2.0],\n        [1.0, -2.5, -1.5],\n        [1.0, -1.7, -2.3],\n        [1.0, -2.2, -1.8],\n        [1.0, -1.8, -2.1]\n    ])\n    y_train = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # Define the prescribed test cases\n    test_cases = [\n        # (lambda, x_star)\n        (0.1, np.array([1.0, 2.5, 2.0])), # Case A\n        (20.0, np.array([1.0, 2.5, 2.0])), # Case B\n        (0.1, np.array([1.0, 0.0, 0.0])), # Case C\n        (50.0, np.array([1.0, 6.0, 6.0]))  # Case D\n    ]\n\n    results = []\n    for lam, x_star in test_cases:\n        # 1. Compute MAP estimate and Hessian at MAP\n        theta_hat, hessian_at_map = compute_map_estimate(X_train, y_train, lam)\n        \n        # 2. Compute inferential variance trace\n        # Inferential covariance is the inverse of the negative Hessian\n        neg_hessian = -hessian_at_map\n        try:\n            sigma_hat = np.linalg.inv(neg_hessian)\n            tr_sigma = np.trace(sigma_hat)\n        except np.linalg.LinAlgError:\n            tr_sigma = float('inf') # Should not happen given problem structure\n            \n        # 3. Compute predictive entropy\n        z_star = x_star @ theta_hat\n        p_star = 1 / (1 + np.exp(-z_star))\n        entropy = calculate_entropy(p_star)\n\n        # Store the pair of results\n        results.append([entropy, tr_sigma])\n    \n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{h:.6f},{v:.6f}]\" for h, v in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}