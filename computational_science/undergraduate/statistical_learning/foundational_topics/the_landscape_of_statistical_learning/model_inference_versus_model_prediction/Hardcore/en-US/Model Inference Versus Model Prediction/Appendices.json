{
    "hands_on_practices": [
        {
            "introduction": "This first exercise establishes the conceptual cornerstone of our topic using the familiar setting of Ordinary Least Squares (OLS) regression. We will dissect the crucial difference between an inferential measure, the partial $R^2$, and a predictive measure, the holdout $R^2$. By analyzing how these metrics behave, particularly when a model begins to overfit, you will build a foundational understanding of why explaining data you already have is fundamentally different from predicting data you have not yet seen .",
            "id": "3149009",
            "problem": "A data analyst is studying a linear response $Y$ and a set of $p$ baseline predictors $X_1,\\dots,X_p$ using ordinary least squares (OLS). The analyst considers adding one more predictor $Z$ and wants to reason carefully about two distinct quantities:\n\n- The partial $R^2$ of $Z$ given $X_1,\\dots,X_p$ computed on the training sample, intended for inference about the additional explained variance attributable to $Z$ in the population, conditional on $X_1,\\dots,X_p$.\n- The holdout predictive $R^2$ computed on an independent test set, intended to measure out-of-sample predictive performance after adding $Z$.\n\nAssume the classical linear model with independent and identically distributed (i.i.d.) errors holds for the training data, and that the test set is drawn from the same data-generating mechanism as the training set. The analyst is also concerned about overfitting when $p$ is large relative to the training sample size $n_{\\text{train}}$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. Under the classical linear model with correct specification, the sample partial $R^2$ of $Z$ given $X_1,\\dots,X_p$ is an unbiased estimator of the population fraction of variance in $Y$ explained uniquely by $Z$ conditional on $X_1,\\dots,X_p$.\n\nB. If $Z$ carries no additional signal beyond $X_1,\\dots,X_p$ (its population coefficient is zero), then in expectation over the training sample the training $R^2$ must weakly increase upon adding $Z$, while the holdout predictive $R^2$ strictly decreases on average.\n\nC. If the partial $F$-test for $Z$ is statistically significant at level $0.05$ on the training data, then the holdout predictive $R^2$ is guaranteed to increase (relative to the reduced model without $Z$).\n\nD. Partial $R^2$ is invariant to any one-to-one rescaling of $Y$, but it can change under rescaling of the predictors (for example, measuring $Z$ in meters versus centimeters).\n\nE. In an extreme overfitting regime where $p \\approx n_{\\text{train}} - 1$, it is possible for the training-sample partial $R^2$ of an actually irrelevant $Z$ to be arbitrarily close to $1$, while the holdout predictive $R^2$ becomes very negative.",
            "solution": "The user has requested a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Response variable: Linear response $Y$.\n- Baseline predictors: A set of $p$ predictors $X_1, \\dots, X_p$.\n- Additional predictor: $Z$.\n- Statistical method: Ordinary Least Squares (OLS).\n- Quantities of interest:\n    1. Partial $R^2$ of $Z$ given $X_1, \\dots, X_p$, computed on a training sample for inference.\n    2. Holdout predictive $R^2$, computed on an independent test set for out-of-sample performance.\n- Assumptions:\n    1. The classical linear model with independent and identically distributed (i.i.d.) errors holds for the training data.\n    2. The test set is drawn from the same data-generating mechanism as the training set.\n- A specific concern is mentioned: overfitting when $p$ is large relative to the training sample size $n_{\\text{train}}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound, well-posed, and objective.\n- **Scientific Grounding**: The problem deals with fundamental and well-established concepts in statistical learning and linear regression theory, namely OLS, partial $R^2$, predictive $R^2$, overfitting, and the classical linear model assumptions. These are standard topics in any advanced statistics or econometrics curriculum.\n- **Well-Posed**: The problem sets up a clear comparison between two distinct statistical measures (in-sample partial $R^2$ for inference vs. out-of-sample predictive $R^2$ for performance) under a standard set of assumptions. The question asks to evaluate the correctness of several statements, which is a mathematically and logically addressable task.\n- **Objective**: The language is technical and precise, using standard terminology from statistics. It is free of subjective or ambiguous phrasing that would prevent a rigorous analysis.\n- **Completeness**: The problem provides all necessary information and assumptions required to evaluate the statements. It defines the context (OLS, classical linear model), the data structure (training and test sets), and the quantities to be compared.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. The analysis can proceed.\n\n### Solution Derivation\n\nLet us define the models and quantities formally. Let the training sample have size $n_{\\text{train}}$.\nThe reduced model is:\n$$ Y = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j + \\epsilon $$\nThe full model is:\n$$ Y = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j + \\theta Z + \\epsilon $$\nLet $SSR_{\\text{red}}$ be the sum of squared residuals from fitting the reduced model on the training data.\nLet $SSR_{\\text{full}}$ be the sum of squared residuals from fitting the full model on the training data.\n\nThe **training-sample partial $R^2$** of $Z$ given $X = \\{X_1, \\dots, X_p\\}$ is defined as:\n$$ R^2_{Z|X} = \\frac{SSR_{\\text{red}} - SSR_{\\text{full}}}{SSR_{\\text{red}}} $$\nThis measures the proportion of variance in $Y$ unexplained by $X$ that is subsequently explained by the addition of $Z$.\n\nThe **holdout predictive $R^2$** is computed on an independent test set of size $n_{\\text{test}}$. Let $Y_{\\text{test},i}$ be the observed values and $\\hat{Y}_{\\text{test},i}$ be the predicted values from a model fit on the training data. Let $\\bar{Y}_{\\text{test}}$ be the mean of the response in the test set. The predictive $R^2$ is:\n$$ R^2_{\\text{pred}} = 1 - \\frac{\\sum_{i=1}^{n_{\\text{test}}} (Y_{\\text{test},i} - \\hat{Y}_{\\text{test},i})^2}{\\sum_{i=1}^{n_{\\text{test}}} (Y_{\\text{test},i} - \\bar{Y}_{\\text{test}})^2} = 1 - \\frac{SSE_{\\text{test}}}{TSS_{\\text{test}}} $$\nThis quantity can be negative if the model's predictions are worse than simply using the test set mean (i.e., $SSE_{\\text{test}} > TSS_{\\text{test}}$).\n\nNow, we evaluate each statement.\n\n**A. Under the classical linear model with correct specification, the sample partial $R^2$ of $Z$ given $X_1,\\dots,X_p$ is an unbiased estimator of the population fraction of variance in $Y$ explained uniquely by $Z$ conditional on $X_1,\\dots,X_p$.**\n\nThe sample $R^2$ is a known biased estimator of the population $R^2$, as it tends to be optimistically high. The same principle applies to partial $R^2$. By construction, adding any predictor $Z$ (that is not a perfect linear combination of the $X$ predictors) to a model will result in $SSR_{\\text{full}} < SSR_{\\text{red}}$ for a given sample, thus yielding a strictly positive partial $R^2$ value. This is true even if the true population coefficient $\\theta$ of $Z$ is zero. In that case, the population partial $R^2$ is zero, but the expected value of the sample partial $R^2$, $E[R^2_{Z|X}]$, will be greater than zero due to chance correlation in the sample. Therefore, the sample partial $R^2$ is a biased estimator of its population counterpart, with the bias being positive. The adjusted $R^2$ statistic is designed to correct for this very bias.\n\nVerdict: **Incorrect**.\n\n**B. If $Z$ carries no additional signal beyond $X_1,\\dots,X_p$ (its population coefficient is zero), then in expectation over the training sample the training $R^2$ must weakly increase upon adding $Z$, while the holdout predictive $R^2$ strictly decreases on average.**\n\nThis statement has two parts.\n1.  **\"the training $R^2$ must weakly increase upon adding $Z$\"**: When adding a predictor to an OLS model, the sum of squared residuals of the new model can never be greater than that of the old model ($SSR_{\\text{full}} \\le SSR_{\\text{red}}$). Since the total sum of squares ($TSS$) is constant for the training data, the coefficient of determination $R^2 = 1 - SSR/TSS$ must be non-decreasing (weakly increasing). This holds for any single training sample, so it certainly holds in expectation.\n2.  **\"the holdout predictive $R^2$ strictly decreases on average\"**: If $Z$'s population coefficient is zero, adding it to the model means we are estimating an extra, unnecessary parameter. The model estimation process on the training data will likely assign a non-zero coefficient $\\hat{\\theta}$ to $Z$ due to sampling noise. This leads to overfitting. When this overfitted model is used for prediction on a holdout set, it incorporates this spurious relationship. The expected out-of-sample prediction error consists of three components: irreducible error, bias squared, and prediction variance. While OLS gives unbiased coefficient estimates (so model bias is zero), adding the irrelevant predictor $Z$ increases the variance of the predictions $\\hat{Y}$. A higher variance in prediction leads to a higher expected mean squared error on the test set. Since $R^2_{\\text{pred}} = 1 - MSE_{\\text{test}} / Var(Y_{\\text{test}})$, a higher expected $MSE_{\\text{test}}$ leads to a lower expected $R^2_{\\text{pred}}$. The decrease is strict (on average) as long as $Z$ is not perfectly orthogonal to the other predictors in the population, which would cause the prediction variance to strictly increase.\n\nVerdict: **Correct**.\n\n**C. If the partial $F$-test for $Z$ is statistically significant at level $0.05$ on the training data, then the holdout predictive $R^2$ is guaranteed to increase (relative to the reduced model without $Z$).**\n\nStatistical significance is a probabilistic concept. A significant partial $F$-test at level $\\alpha=0.05$ for the coefficient of $Z$ means that *if* the null hypothesis ($H_0: \\theta=0$) were true, the probability of observing a test statistic as or more extreme than the one computed from the sample is less than $0.05$. This does not guarantee that the null hypothesis is false. It is possible that the null hypothesis is true and our sample is one of the $5\\%$ that yields a significant result by chance (a Type I error). In such a case, we would be adding an irrelevant variable to our model. As established in the analysis of option B, adding an irrelevant variable increases model complexity and is expected to decrease, not increase, predictive performance on a holdout set. The word \"guaranteed\" makes this statement definitively false. There is no deterministic link between in-sample statistical significance and out-of-sample predictive improvement.\n\nVerdict: **Incorrect**.\n\n**D. Partial $R^2$ is invariant to any one-to-one rescaling of $Y$, but it can change under rescaling of the predictors (for example, measuring $Z$ in meters versus centimeters).**\n\nThis statement has two parts.\n1.  **\"invariant to any one-to-one rescaling of $Y$\"**: The term \"rescaling\" in the context of linear models typically implies a linear transformation, $Y' = aY + b$ where $a \\ne 0$. If $Y$ is transformed to $Y' = aY+b$, the residuals are scaled by $a$, so $e' = ae$. Consequently, the sum of squared residuals is scaled by $a^2$, so $SSR' = a^2 SSR$. Both $SSR'_{\\text{red}}$ and $SSR'_{\\text{full}}$ are scaled by the same factor $a^2$. The partial $R^2$ is a ratio of such quantities:\n    $$ R'^2_{Z|X} = \\frac{SSR'_{\\text{red}} - SSR'_{\\text{full}}}{SSR'_{\\text{red}}} = \\frac{a^2 SSR_{\\text{red}} - a^2 SSR_{\\text{full}}}{a^2 SSR_{\\text{red}}} = \\frac{a^2(SSR_{\\text{red}} - SSR_{\\text{full}})}{a^2 SSR_{\\text{red}}} = R^2_{Z|X} $$\n    So, it is invariant to linear rescaling. However, the term \"one-to-one\" could include non-linear transformations (e.g., $Y' = \\log(Y)$), which would not preserve the partial $R^2$. Ambiguity aside, the second part of the statement is definitively false.\n2.  **\"it can change under rescaling of the predictors\"**: Let's consider a linear rescaling of a predictor, e.g., $Z' = cZ$ for a constant $c \\ne 0$. The column space spanned by the predictors $[X_1, \\dots, X_p, Z]$ is identical to the column space spanned by $[X_1, \\dots, X_p, Z']$. Since the OLS fit is determined by the projection of $Y$ onto this column space, the projection matrix, the fitted values $\\hat{Y}$, and the residuals are all unchanged. Hence, $SSR_{\\text{full}}$ is unchanged. Similarly, rescaling any of the $X_j$ predictors does not change the column space of the reduced or full model. Therefore, both $SSR_{\\text{red}}$ and $SSR_{\\text{full}}$ are invariant to linear rescaling of any of the predictors. As a result, the partial $R^2$ is also invariant. The statement claims it can change, which is false for linear rescaling.\n\nSince the second part of the statement is incorrect, the entire statement is incorrect.\n\nVerdict: **Incorrect**.\n\n**E. In an extreme overfitting regime where $p \\approx n_{\\text{train}} - 1$, it is possible for the training-sample partial $R^2$ of an actually irrelevant $Z$ to be arbitrarily close to $1$, while the holdout predictive $R^2$ becomes very negative.**\n\nThis statement describes a classic overfitting scenario.\n1.  **\"training-sample partial $R^2$ ... arbitrarily close to $1$\"**: Let's consider the number of parameters to be estimated. The reduced model estimates $p+1$ parameters (including an intercept). The full model estimates $p+2$ parameters. If $p+1 = n_{\\text{train}}-1$ (i.e., $p = n_{\\text{train}}-2$), the full model has $p+2=n_{\\text{train}}$ parameters to estimate from $n_{\\text{train}}$ data points. Such a model will fit the training data perfectly, resulting in $SSR_{\\text{full}} = 0$. The reduced model has $n_{\\text{train}}-1$ parameters and will have a non-zero (though likely small) $SSR_{\\text{red}}$. In this case, the partial $R^2$ is:\n    $$ R^2_{Z|X} = \\frac{SSR_{\\text{red}} - 0}{SSR_{\\text{red}}} = 1 $$\n    So, it is not only possible for it to be arbitrarily close to $1$, but it can be exactly $1$. This holds even if $Z$ is \"actually irrelevant\" (its population coefficient is zero).\n2.  **\"holdout predictive $R^2$ becomes very negative\"**: A model that has been forced to fit the noise in the training data by using up all its degrees of freedom will have extremely large and unstable coefficient estimates. When applied to a new, independent test set, its predictions $\\hat{Y}_{\\text{test}}$ will be very poor and can deviate wildly from the true values $Y_{\\text{test}}$. The sum of squared errors on the test set, $SSE_{\\text{test}} = \\sum (Y_{\\text{test}} - \\hat{Y}_{\\text{test}})^2$, can therefore become much larger than the total sum of squares, $TSS_{\\text{test}} = \\sum (Y_{\\text{test}} - \\bar{Y}_{\\text{test}})^2$. In this situation, the predictive $R^2 = 1 - SSE_{\\text{test}}/TSS_{\\text{test}}$ will become a large negative number.\n\nThis statement correctly captures the dual phenomena of perfect in-sample fit and disastrous out-of-sample performance characteristic of extreme overfitting.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{BE}$$"
        },
        {
            "introduction": "Building on the foundational concepts, this practice moves from general model fit to the vital task of assessing individual feature importance. You will investigate scenarios where a classic inferential tool, the Wald test for a coefficient's significance, tells a different story from a modern predictive metric, permutation importance. This exercise uses hypothetical data with high collinearity and interaction effects to reveal precisely why a feature that is statistically significant is not always predictively useful, and vice-versa .",
            "id": "3148898",
            "problem": "A supervised learning analyst is comparing prediction-oriented feature importance with inference-oriented coefficient significance in a linear regression setting. Consider a training dataset of size $n$ with two standardized features $X_1$ and $X_2$ drawn from a mean-zero bivariate normal distribution with unit variances and correlation $\\rho = 0.95$, and a response\n$$\nY \\;=\\; \\beta_1 X_1 \\;+\\; \\beta_2 X_2 \\;+\\; \\beta_3 X_1 X_2 \\;+\\; \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ and $\\sigma^2$ is finite and fixed. The analyst fits the correctly specified linear model with regressors $X_1$, $X_2$, and $X_1 X_2$ by Ordinary Least Squares (OLS), and evaluates two quantities for each feature:\n- Permutation feature importance for $X_j$ defined as the expected increase in prediction error (mean squared error) when the observed $X_j$ values are randomly permuted in the test set, holding the fitted model parameters fixed.\n- The Wald test for the null hypothesis $H_0:\\beta_j = 0$, based on the OLS estimator’s asymptotic normality, i.e., $Z_j = \\widehat{\\beta}_j/\\mathrm{SE}(\\widehat{\\beta}_j)$, which under $H_0$ is approximately standard normal.\n\nThe analyst examines two scenarios:\n- Scenario $1$: $\\beta_1 = 0$, $\\beta_2 = 0$, $\\beta_3 = 1$.\n- Scenario $2$: $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_3 = 0$.\n\nAssume $n$ is large enough that asymptotic approximations for OLS and the Wald tests are meaningful, and that permutation importance is evaluated on a fresh test set from the same distribution.\n\nWhich of the following statements are correct in light of the fundamental definitions of permutation importance and the Wald test, and the effects of collinearity and interactions?\n\nA. In Scenario $1$, the permutation importance for $X_1$ can be large even though the Wald test for $H_0:\\beta_1=0$ is non-significant, because permuting $X_1$ destroys the interaction signal $X_1 X_2$ that drives prediction, while the Wald test for $\\beta_1$ only probes the main-effect coefficient.\n\nB. In Scenario $2$, severe collinearity inflates the variance of $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$, so individual Wald tests for $H_0:\\beta_1=0$ and $H_0:\\beta_2=0$ may be non-significant; at the same time, the permutation importance for either $X_1$ or $X_2$ can be small because permuting one redundant feature leaves the other to carry most of the predictive signal.\n\nC. In Scenario $1$, the Wald test for $H_0:\\beta_3=0$ will be non-significant because collinearity between $X_1$, $X_2$, and $X_1 X_2$ hides the interaction effect in finite samples.\n\nD. When the model is correctly specified and sample size $n$ is large, permutation importance and Wald test results necessarily agree on feature importance rankings.\n\nE. If the analyst refits the model in Scenario $1$ without the interaction term $X_1 X_2$, the permutation importance for $X_1$ must become larger than it was in the correctly specified model, because the model will rely more on the main effect to compensate for the missing interaction.",
            "solution": "The user wants me to act as a meticulous STEM professor and solve a statistical learning problem.\n\n### PROBLEM VALIDATION\n\n#### Step 1: Extract Givens\n- **Setting**: A supervised learning analysis of a linear regression model.\n- **Data**: Training set of size $n$.\n- **Features**: Two standardized features, $X_1$ and $X_2$.\n- **Feature Distribution**: $(X_1, X_2) \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$, where the mean vector is $\\boldsymbol{0}$, and the covariance matrix is $\\boldsymbol{\\Sigma} = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with $\\rho = 0.95$.\n- **True Response Model**: $Y = \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\varepsilon$.\n- **Error Term**: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, with $\\sigma^2$ finite and fixed.\n- **Fitted Model**: $\\widehat{Y} = \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 X_2 + \\widehat{\\beta}_3 (X_1 X_2)$, fitted by Ordinary Least Squares (OLS). This is a correctly specified model form.\n- **Metrics**:\n    1.  **Permutation Feature Importance**: For a feature $X_j$, defined as the expected increase in mean squared error (MSE) when the values of $X_j$ are randomly permuted in a test set, holding fitted parameters fixed.\n    2.  **Wald Test**: For the null hypothesis $H_0: \\beta_j=0$, using the test statistic $Z_j = \\widehat{\\beta}_j / \\mathrm{SE}(\\widehat{\\beta}_j)$.\n- **Assumptions**: $n$ is large (asymptotic OLS theory applies). Permutation importance is evaluated on a fresh test set from the same data-generating distribution.\n- **Scenarios**:\n    - **Scenario 1**: True coefficients are $\\beta_1 = 0$, $\\beta_2 = 0$, $\\beta_3 = 1$. The true model is $Y = X_1 X_2 + \\varepsilon$.\n    - **Scenario 2**: True coefficients are $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_3 = 0$. The true model is $Y = X_1 + X_2 + \\varepsilon$.\n\n#### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is entirely within the established framework of statistical learning and econometrics. It uses standard concepts like OLS regression, multicollinearity, interaction effects, Wald tests, and permutation importance. These are fundamental topics in the field. The setup is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem is well-posed. It provides two distinct, fully specified scenarios and asks for an evaluation of statements concerning the behavior of two well-defined statistical metrics. The assumptions (large $n$, fresh test set) are clear and sufficient to allow for a rigorous analysis.\n3.  **Objective**: The problem is stated using precise, objective, and formal mathematical and statistical language. There are no subjective or ambiguous terms.\n4.  **Flaw Checklist**: The problem does not violate any of the listed invalidity criteria. It is a standard, albeit conceptually challenging, problem exploring the difference between statistical inference and prediction.\n\n#### Step 3: Verdict and Action\nThe problem statement is **valid**. I will proceed to the solution.\n\n### SOLUTION\n\nThis problem requires a careful analysis of how two different measures of feature importance—one inferential (Wald test) and one predictive (permutation importance)—behave under conditions of high collinearity and interaction effects.\n\nFirst, let's analyze the collinearity among the regressors used in the model: $X_1$, $X_2$, and the interaction term $X_{12} = X_1 X_2$.\n- The features $X_1$ and $X_2$ are highly collinear by definition, with $\\mathrm{Corr}(X_1, X_2) = \\rho = 0.95$.\n- Let's examine the correlation between the main effects and the interaction term. Since $X_1$ and $X_2$ are drawn from a mean-zero bivariate normal distribution, we can calculate their covariance with the product term $X_1 X_2$.\n$$ \\mathrm{Cov}(X_1, X_1 X_2) = E[X_1 (X_1 X_2)] - E[X_1]E[X_1 X_2] $$\nSince $E[X_1]=0$, this simplifies to $E[X_1^2 X_2]$. For jointly Gaussian random variables with zero mean, the expectation of the product of an odd number of variables is zero. Thus, $E[X_1^2 X_2] = 0$.\nBy symmetry, $\\mathrm{Cov}(X_2, X_1 X_2) = E[X_1 X_2^2] = 0$.\nThis means that the interaction term $X_1 X_2$ is uncorrelated with the main effect terms $X_1$ and $X_2$. Consequently, in the OLS regression on $\\{X_1, X_2, X_1 X_2\\}$, the variance of $\\widehat{\\beta_3}$ will not be inflated by collinearity with the main effects, whereas the variances of $\\widehat{\\beta_1}$ and $\\widehat{\\beta_2}$ will be heavily inflated due to the high correlation between $X_1$ and $X_2$. The variance inflation factor (VIF) for $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$ will be approximately $1/(1-\\rho^2) = 1/(1-0.95^2) \\approx 10.26$, while the VIF for $\\widehat{\\beta}_3$ will be close to $1$.\n\nNow, let's analyze the two scenarios.\n\n#### Scenario 1: $Y = X_1 X_2 + \\varepsilon$ (True coefficients: $\\beta_1=0, \\beta_2=0, \\beta_3=1$)\n- **Wald Tests**: The fitted model is $\\widehat{Y} = \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 X_2 + \\widehat{\\beta}_3 X_1 X_2$.\n    - For $H_0: \\beta_1 = 0$: The null hypothesis is true. Therefore, the Wald test will be non-significant with high probability (specifically, with probability $1-\\alpha$ at significance level $\\alpha$).\n    - For $H_0: \\beta_2 = 0$: Similarly, the null hypothesis is true, and the test will be non-significant.\n    - For $H_0: \\beta_3 = 0$: The null hypothesis is false, as the true $\\beta_3=1$. Since $n$ is large and the standard error of $\\widehat{\\beta}_3$ is not inflated by collinearity, the OLS estimator $\\widehat{\\beta}_3$ will be a consistent estimator of $1$, and the test statistic $Z_3 = \\widehat{\\beta}_3 / \\mathrm{SE}(\\widehat{\\beta}_3)$ will be large. The test will be highly significant.\n- **Permutation Importance**: The predictive power of the model comes exclusively from the term $\\widehat{\\beta}_3 X_1 X_2$, where $\\widehat{\\beta}_3 \\approx 1$. To calculate the permutation importance of $X_1$, we permute its values in the test set. This changes the predictions from $\\widehat{Y}_i \\approx X_{i1} X_{i2}$ to $\\widehat{Y}_i^{\\text{perm}} \\approx X_{\\pi(i),1} X_{i2}$, where $\\pi$ is a random permutation. Since $X_{\\pi(i),1}$ is now decorrelated from $X_{i1}$ and $Y_i$, the term $X_{\\pi(i),1} X_{i2}$ no longer tracks the signal component of $Y_i$. This breaks the model's predictive ability, causing a large increase in the mean squared error. Therefore, the permutation importance of $X_1$ will be large. By symmetry, the permutation importance of $X_2$ will also be large.\n\n#### Scenario 2: $Y = X_1 + X_2 + \\varepsilon$ (True coefficients: $\\beta_1=1, \\beta_2=1, \\beta_3=0$)\n- **Wald Tests**:\n    - For $H_0: \\beta_1 = 0$ and $H_0: \\beta_2 = 0$: These null hypotheses are false. However, due to the severe collinearity between $X_1$ and $X_2$ ($\\rho = 0.95$), the standard errors $\\mathrm{SE}(\\widehat{\\beta}_1)$ and $\\mathrm{SE}(\\widehat{\\beta}_2)$ will be very large. The model has difficulty attributing the shared predictive power uniquely to either $X_1$ or $X_2$. This large variance reduces the magnitude of the Wald statistics $Z_1$ and $Z_2$, significantly lowering the power of the tests. It is therefore quite possible, and indeed typical in such cases, for both individual tests to be non-significant, failing to reject the null hypotheses even though the true coefficients are non-zero.\n    - For $H_0: \\beta_3 = 0$: This null hypothesis is true. The test will be non-significant with high probability.\n- **Permutation Importance**: The model's predictive power comes from approximating the true signal $X_1 + X_2$. The fitted model is $\\widehat{Y} \\approx \\widehat{\\beta}_1 X_1 + \\widehat{\\beta}_2 X_2$. To find the permutation importance of $X_1$, we permute its values. The new prediction is $\\widehat{Y}^{\\text{perm}} \\approx \\widehat{\\beta}_1 X_{\\pi(i),1} + \\widehat{\\beta}_2 X_{i2}$. Because $X_1$ and $X_2$ are highly correlated, $X_2$ is a very good proxy for $X_1$. After permuting $X_1$ and effectively removing its direct contribution, the model can still make good predictions using just the $X_2$ term. The information in $X_1$ is largely redundant given $X_2$. Consequently, the increase in prediction error will be small. The permutation importance for $X_1$ will be small. By symmetry, the same holds for $X_2$.\n\nWith this foundation, let us evaluate the given options.\n\n---\n**Option-by-Option Analysis**\n\n**A. In Scenario 1, the permutation importance for $X_1$ can be large even though the Wald test for $H_0:\\beta_1=0$ is non-significant, because permuting $X_1$ destroys the interaction signal $X_1 X_2$ that drives prediction, while the Wald test for $\\beta_1$ only probes the main-effect coefficient.**\n- Our analysis of Scenario 1 showed that the permutation importance for $X_1$ is indeed large, as permuting it breaks the model's sole predictive component.\n- Our analysis also showed that the Wald test for $H_0:\\beta_1=0$ will be non-significant because the true coefficient $\\beta_1$ is zero.\n- The reasoning provided is accurate: permutation importance captures the predictive value of $X_1$ in any form, including interactions, while the Wald test is specific to the coefficient of the $X_1$ main effect term.\n- This statement correctly contrasts the two metrics and their behavior in the presence of interactions.\n- **Verdict: Correct.**\n\n**B. In Scenario 2, severe collinearity inflates the variance of $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_2$, so individual Wald tests for $H_0:\\beta_1=0$ and $H_0:\\beta_2=0$ may be non-significant; at the same time, the permutation importance for either $X_1$ or $X_2$ can be small because permuting one redundant feature leaves the other to carry most of the predictive signal.**\n- Our analysis of Scenario 2 showed that high collinearity inflates the coefficient variances, which can lead to non-significant Wald tests despite the true coefficients being non-zero.\n- Our analysis also showed that permutation importance for either feature can be small due to their redundancy; the presence of one suffices for good prediction.\n- The reasoning provided for both phenomena is precisely as derived.\n- This statement correctly describes the effects of multicollinearity on both inferential and predictive importance measures.\n- **Verdict: Correct.**\n\n**C. In Scenario 1, the Wald test for $H_0:\\beta_3=0$ will be non-significant because collinearity between $X_1$, $X_2$, and $X_1 X_2$ hides the interaction effect in finite samples.**\n- Our analysis of Scenario 1 concluded that the Wald test for $H_0:\\beta_3=0$ will be *significant*, as the true $\\beta_3=1$, $n$ is large, and the standard error of $\\widehat{\\beta}_3$ is not inflated by collinearity.\n- The premise that there is problematic collinearity involving the interaction term $X_1 X_2$ is false for this data distribution.\n- **Verdict: Incorrect.**\n\n**D. When the model is correctly specified and sample size $n$ is large, permutation importance and Wald test results necessarily agree on feature importance rankings.**\n- This statement claims a necessary agreement. Scenarios 1 and 2 serve as direct counterexamples.\n- In Scenario 1, permutation importance ranks $X_1$ and $X_2$ as highly important, while their main effect Wald tests show they are not significant. This is a disagreement.\n- In Scenario 2, permutation importance ranks $X_1$ and $X_2$ as having low importance, while the true underlying model requires them both (i.e., their true coefficients are non-zero). If one could overcome the collinearity issue (e.g., via a joint F-test), one would find them significant.\n- The fundamental difference in what these two metrics measure (marginal predictive contribution vs. conditional parametric significance) means they do not have to agree, even in ideal asymptotic conditions.\n- **Verdict: Incorrect.**\n\n**E. If the analyst refits the model in Scenario 1 without the interaction term $X_1 X_2$, the permutation importance for $X_1$ must become larger than it was in the correctly specified model, because the model will rely more on the main effect to compensate for the missing interaction.**\n- In Scenario 1, the true model is $Y = X_1 X_2 + \\varepsilon$. We refit a misspecified model $\\widehat{Y} = \\widehat{\\gamma}_1 X_1 + \\widehat{\\gamma}_2 X_2$.\n- The OLS coefficients $\\widehat{\\gamma}_1, \\widehat{\\gamma}_2$ converge to the coefficients of the best linear projection of $Y$ onto the span of $\\{X_1, X_2\\}$. The population coefficients are $\\gamma_1 = \\mathrm{Cov}(Y, X_1)/\\mathrm{Var}(X_1)$ and $\\gamma_2=\\mathrm{Cov}(Y,X_2)/\\mathrm{Var}(X_2)$ in an orthogonal basis, but here we require a multiple regression projection. The true coefficients of the projection are given by $(E[X^T X])^{-1} E[X^T Y]$. We found $E[X^T Y] = (E[X_1 Y], E[X_2 Y])^T = (E[X_1^2 X_2], E[X_1 X_2^2])^T = (0, 0)^T$.\n- Thus, the coefficients for the misspecified model, $\\widehat{\\gamma}_1$ and $\\widehat{\\gamma}_2$, will converge to $0$. The model $\\widehat{Y}_{\\text{miss}} \\approx 0$ has no predictive power.\n- The permutation importance of $X_1$ in a model that always predicts zero is zero, as permuting the feature does not change the prediction or the error.\n- The permutation importance of $X_1$ was large in the correctly specified model. It becomes approximately zero in the misspecified model. Therefore, it becomes smaller, not larger.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "Here, we transition from conceptual thought experiments to a practical coding challenge that embodies the inference-prediction trade-off. You will explore the consequences of applying a Box-Cox transformation to a response variable—a common technique to stabilize variance and improve a model's predictive power. This hands-on practice requires you to quantify the 'cost' of this predictive gain by deriving and implementing the correction for back-transformation bias, which is essential for valid inference on the data's original scale .",
            "id": "3148926",
            "problem": "You are given a comparison between model prediction and model inference when a response transformation is used to stabilize variance. Consider the Box–Cox transformation family applied to a strictly positive response. Let the transformation be defined for any real parameter $\\lambda$ by\n$$\nT_{\\lambda}(y) =\n\\begin{cases}\n\\dfrac{y^{\\lambda}-1}{\\lambda}, & \\lambda \\neq 0 \\\\\n\\log(y), & \\lambda = 0\n\\end{cases}\n$$\nwith inverse mapping $g_{\\lambda}(z) = T_{\\lambda}^{-1}(z)$ given by\n$$\ng_{\\lambda}(z) =\n\\begin{cases}\n(\\lambda z + 1)^{1/\\lambda}, & \\lambda \\neq 0 \\\\\n\\exp(z), & \\lambda = 0\n\\end{cases}\n$$\nAssume a linear regression model on the transformed scale:\n$$\nZ \\equiv T_{\\lambda}(Y) \\mid X \\sim \\text{Normal}(\\mu, \\sigma^2),\n$$\nwhere $\\mu = X^{\\top}\\beta$ and $\\sigma^2 > 0$ is the conditional variance on the transformed scale. Your task is to operationalize, via a program, the following quantities that contrast prediction and inference on the original scale $Y$:\n- For prediction: compute the naive back-transformation prediction $g_{\\lambda}(\\mu)$ and the bias-corrected prediction that accounts for back-transformation bias via a second-order expansion.\n- For inference: compute the marginal effect of a single regressor $x_j$ on the original-scale conditional mean, first naively via the chain rule on $g_{\\lambda}$, and then with a variance-aware correction that uses a second-order expansion.\n\nUse the following fundamental bases:\n- The Box–Cox definitions above.\n- The second-order Taylor expansion of a twice-differentiable function $g$ around $m$:\n$$\ng(Z) \\approx g(m) + g'(m)(Z-m) + \\tfrac{1}{2}g''(m)(Z-m)^2,\n$$\nand, for a random variable $Z$ with $\\mathbb{E}[Z]=m$ and $\\operatorname{Var}(Z)=s^2$, the implication\n$$\n\\mathbb{E}[g(Z)] \\approx g(m) + \\tfrac{1}{2} g''(m) s^2.\n$$\n- The chain rule for differentiation.\n\nDerive from these bases, without introducing additional unproven formulas, the following program outputs for each test case with parameters $(\\lambda,\\mu,\\sigma^2,\\beta_j)$:\n1. The naive back-transformation prediction on the original scale, which is the value of $\\mathbb{E}[Y \\mid X]$ when variance is ignored: a single float equal to $g_{\\lambda}(\\mu)$.\n2. The bias-corrected prediction on the original scale using a second-order expansion: a single float equal to $g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\,\\sigma^2$.\n3. The naive marginal effect of $x_j$ on the original-scale conditional mean, using only the chain rule and the coefficient $\\beta_j$: a single float equal to $\\beta_j\\, g_{\\lambda}'(\\mu)$.\n4. The variance-aware corrected marginal effect of $x_j$ on the original-scale conditional mean using a second-order expansion: a single float equal to $\\beta_j \\big(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\big)$.\n\nImplementation details and constraints:\n- Treat the case $\\lambda = 0$ by its definition $g_{0}(z)=\\exp(z)$. For derivatives of $g_{\\lambda}$ at $\\lambda=0$, use the corresponding derivatives of the exponential function.\n- For all other real $\\lambda$, the inverse $g_{\\lambda}(z)$ is defined only when $\\lambda z + 1 > 0$; all provided test cases satisfy this condition at the evaluation point $z=\\mu$.\n- No physical units are involved. All outputs are real-valued floats.\n- Angles are not used.\n- Fractions and decimals are both acceptable; do not use a percentage sign.\n\nTest suite:\nProvide results for the following four parameter sets, each written as $(\\lambda,\\mu,\\sigma^2,\\beta_j)$:\n- Case $1$: $(0,\\, 0.0,\\, 0.04,\\, 0.2)$.\n- Case $2$: $(0.5,\\, 1.5,\\, 0.36,\\, 1.0)$.\n- Case $3$: $(1.0,\\, 3.0,\\, 2.0,\\, 2.0)$.\n- Case $4$: $(-0.5,\\, 1.0,\\, 0.25,\\, 0.5)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the four cases as a list of lists, where each inner list corresponds to one case and is ordered as\n$[\\text{naive\\_prediction}, \\text{bias\\_corrected\\_prediction}, \\text{naive\\_marginal\\_effect}, \\text{bias\\_corrected\\_marginal\\_effect}]$.\nFor example, the program must print a string of the form\n$[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],[a_{31},a_{32},a_{33},a_{34}],[a_{41},a_{42},a_{43},a_{44}]]$\nwith numeric entries filled in for the test suite above, and no additional text.",
            "solution": "We start from the definitions and principles specified. The Box–Cox inverse $g_{\\lambda}(z)$ is\n$$\ng_{\\lambda}(z) =\n\\begin{cases}\n(\\lambda z + 1)^{1/\\lambda}, & \\lambda \\neq 0 \\\\\n\\exp(z), & \\lambda = 0\n\\end{cases}\n$$\nWe assume the transformed response $Z = T_{\\lambda}(Y)$ satisfies $Z \\mid X \\sim \\text{Normal}(\\mu, \\sigma^2)$, where $\\mu = X^{\\top}\\beta$ and $\\sigma^2 > 0$.\n\nGoal 1 (prediction): We aim to compute $\\mathbb{E}[Y \\mid X] = \\mathbb{E}[g_{\\lambda}(Z) \\mid X]$. The naive back-transformation ignores the randomness of $Z$ and sets\n$$\n\\text{Naive prediction} = g_{\\lambda}(\\mu).\n$$\nTo account for the back-transformation bias induced by nonlinearity, we use the second-order Taylor expansion of $g_{\\lambda}(Z)$ around $m=\\mu$:\n$$\ng_{\\lambda}(Z) \\approx g_{\\lambda}(\\mu) + g_{\\lambda}'(\\mu)(Z-\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu) (Z-\\mu)^2.\n$$\nTaking conditional expectation given $X$ and using $\\mathbb{E}[Z-\\mu \\mid X]=0$ and $\\mathbb{E}[(Z-\\mu)^2 \\mid X] = \\sigma^2$ yields the second-order delta-method approximation\n$$\n\\mathbb{E}[Y \\mid X] = \\mathbb{E}[g_{\\lambda}(Z) \\mid X] \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2.\n$$\nThus, the bias-corrected prediction is\n$$\n\\text{Bias-corrected prediction} \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2.\n$$\n\nGoal 2 (inference): The marginal effect of a single regressor $x_j$ on the original-scale conditional mean is the partial derivative $\\dfrac{\\partial}{\\partial x_j} \\mathbb{E}[Y \\mid X]$. First, naively treating $\\mathbb{E}[Y \\mid X] \\approx g_{\\lambda}(\\mu)$, by the chain rule and $\\mu = X^{\\top}\\beta$, we obtain\n$$\n\\text{Naive marginal effect} = \\dfrac{\\partial}{\\partial x_j} g_{\\lambda}(\\mu) = g_{\\lambda}'(\\mu) \\cdot \\dfrac{\\partial \\mu}{\\partial x_j} = g_{\\lambda}'(\\mu) \\cdot \\beta_j.\n$$\nNext, incorporating variance via the same second-order approximation for the conditional mean,\n$$\n\\mathbb{E}[Y \\mid X] \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2,\n$$\nwe differentiate with respect to $x_j$ and use the chain rule again. Since $\\sigma^2$ is treated as constant with respect to $x_j$ in this setup, we get\n$$\n\\dfrac{\\partial}{\\partial x_j} \\mathbb{E}[Y \\mid X] \\approx g_{\\lambda}'(\\mu)\\, \\beta_j + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\, \\beta_j = \\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right).\n$$\nTherefore,\n$$\n\\text{Bias-corrected marginal effect} \\approx \\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right).\n$$\n\nWe now compute the derivatives of $g_{\\lambda}$ needed above. For $\\lambda \\neq 0$, $g_{\\lambda}$ can be written as $g_{\\lambda}(z) = (\\lambda z + 1)^{K}$ with $K = 1/\\lambda$. Using repeated differentiation of a power of an affine function, for any real $k$,\n$$\n\\dfrac{d}{dz} (a z + b)^k = k a (a z + b)^{k-1}, \\quad\n\\dfrac{d^2}{dz^2} (a z + b)^k = k (k-1) a^2 (a z + b)^{k-2}, \\quad\n\\dfrac{d^3}{dz^3} (a z + b)^k = k (k-1) (k-2) a^3 (a z + b)^{k-3}.\n$$\nSetting $a=\\lambda$, $b=1$, and $k=K=1/\\lambda$, we obtain for $\\lambda \\neq 0$:\n$$\ng_{\\lambda}'(z) = (\\lambda z + 1)^{\\frac{1}{\\lambda}-1},\n$$\n$$\ng_{\\lambda}''(z) = \\left(\\frac{1}{\\lambda}-1\\right)\\lambda\\, (\\lambda z + 1)^{\\frac{1}{\\lambda}-2},\n$$\n$$\ng_{\\lambda}'''(z) = \\left(\\frac{1}{\\lambda}-1\\right)\\left(\\frac{1}{\\lambda}-2\\right)\\lambda^2\\, (\\lambda z + 1)^{\\frac{1}{\\lambda}-3}.\n$$\nFor $\\lambda = 0$, we have $g_{0}(z) = \\exp(z)$ and hence\n$$\ng_{0}'(z) = \\exp(z), \\quad g_{0}''(z) = \\exp(z), \\quad g_{0}'''(z) = \\exp(z).\n$$\n\nAlgorithm design for the program:\n- For each case $(\\lambda,\\mu,\\sigma^2,\\beta_j)$, compute $g_{\\lambda}(\\mu)$, $g_{\\lambda}''(\\mu)$, and $g_{\\lambda}'(\\mu)$, $g_{\\lambda}'''(\\mu)$ using the appropriate branch ($\\lambda=0$ or $\\lambda \\neq 0$).\n- Compute the four required outputs per case:\n  - Naive prediction: $g_{\\lambda}(\\mu)$.\n  - Bias-corrected prediction: $g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2$.\n  - Naive marginal effect: $\\beta_j\\, g_{\\lambda}'(\\mu)$.\n  - Bias-corrected marginal effect: $\\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right)$.\n- Assemble the outputs for all test cases in the prescribed order into a list of lists and print them on a single line, with no extra text.\n\nCoverage of the test suite:\n- Case $1$ uses $\\lambda = 0$ (log transform), exercising the special branch and illustrating that back-transform bias depends on $\\sigma^2$ via the exponential function.\n- Case $2$ uses $\\lambda = 0.5$ with moderate $\\mu$ and $\\sigma^2$, testing general Box–Cox derivatives in a typical stabilized-variance setting.\n- Case $3$ uses $\\lambda = 1.0$ (identity up to a shift), where $g_{\\lambda}''(\\mu)=0$ and $g_{\\lambda}'''(\\mu)=0$, yielding zero bias correction and marginal effects equal to $\\beta_j$, a useful boundary check.\n- Case $4$ uses $\\lambda = -0.5$ with $\\lambda \\mu + 1 > 0$, testing behavior for a negative parameter and confirming proper domain handling at the evaluation point.\n\nInterpretation relative to model inference versus model prediction:\n- Prediction on the original scale benefits from variance stabilization on the transformed scale but requires bias correction upon back-transformation to approximate $\\mathbb{E}[Y \\mid X]$; the naive $g_{\\lambda}(\\mu)$ underestimates the original mean when $g_{\\lambda}$ is convex at $\\mu$ (for example, the exponential case).\n- Inference on the original scale is complicated because the marginal effect depends on $\\mu$ and $\\sigma^2$ through $g_{\\lambda}'(\\mu)$ and $g_{\\lambda}'''(\\mu)$; thus, a single coefficient $\\beta_j$ on the transformed scale does not translate to a constant effect on the original scale, and variance-aware inference introduces additional curvature terms.\n\nThe program directly implements these derivations to produce the required numeric outputs for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef g_lambda(z: float, lam: float) -> float:\n    \"\"\"Inverse Box-Cox transformation g_lambda(z).\"\"\"\n    if abs(lam) < 1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    # Domain check is assumed valid at evaluation point by problem statement.\n    return math.pow(base, 1.0 / lam)\n\ndef g1_lambda(z: float, lam: float) -> float:\n    \"\"\"First derivative g'(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam) < 1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return math.pow(base, (1.0 / lam) - 1.0)\n\ndef g2_lambda(z: float, lam: float) -> float:\n    \"\"\"Second derivative g''(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam) < 1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return ((1.0 / lam) - 1.0) * lam * math.pow(base, (1.0 / lam) - 2.0)\n\ndef g3_lambda(z: float, lam: float) -> float:\n    \"\"\"Third derivative g'''(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam) < 1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return ((1.0 / lam) - 1.0) * ((1.0 / lam) - 2.0) * (lam ** 2) * math.pow(base, (1.0 / lam) - 3.0)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (lambda, mu, sigma2, beta_j)\n    test_cases = [\n        (0.0, 0.0, 0.04, 0.2),    # Case 1\n        (0.5, 1.5, 0.36, 1.0),    # Case 2\n        (1.0, 3.0, 2.0, 2.0),     # Case 3\n        (-0.5, 1.0, 0.25, 0.5),   # Case 4\n    ]\n\n    results = []\n    for lam, mu, sigma2, beta_j in test_cases:\n        g = g_lambda(mu, lam)\n        g1 = g1_lambda(mu, lam)\n        g2 = g2_lambda(mu, lam)\n        g3 = g3_lambda(mu, lam)\n\n        naive_pred = g\n        bias_corr_pred = g + 0.5 * g2 * sigma2\n\n        naive_effect = beta_j * g1\n        bias_corr_effect = beta_j * (g1 + 0.5 * g3 * sigma2)\n\n        results.append([naive_pred, bias_corr_pred, naive_effect, bias_corr_effect])\n\n    # Final print statement in the exact required format.\n    # Print a single line JSON-like list of lists with full-precision floats.\n    def fmt(x):\n        # Use repr-like formatting for better precision consistency\n        return format(x, '.15g')\n    line = \"[\" + \",\".join(\"[\" + \",\".join(fmt(v) for v in row) + \"]\" for row in results) + \"]\"\n    print(line)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}