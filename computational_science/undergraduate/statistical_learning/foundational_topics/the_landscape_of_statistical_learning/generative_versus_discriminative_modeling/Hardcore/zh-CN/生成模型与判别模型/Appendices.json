{
    "hands_on_practices": [
        {
            "introduction": "现实世界的数据往往是不完整的。生成模型和判别模型如何处理缺失的特征？本练习将探讨一种情景，其中缺失模式本身就包含了关于类别标签的信息。通过对联合分布 $p(x, y, m)$ 进行建模，生成模型可以利用这些信息，而一个只处理完整数据的朴素判别模型则可能产生偏差。通过推导和计算，您将量化这种偏差，并理解生成模型的一个关键优势：它们能够表示完整的数据生成过程。",
            "id": "3124923",
            "problem": "给定一个二元分类场景，该场景具有单个实值特征和显式缺失机制。设类别标签为伯努利随机变量 $Y \\in \\{0,1\\}$，其先验概率为 $P(Y=1)=\\pi$ 和 $P(Y=0)=1-\\pi$。设 $X \\in \\mathbb{R}$ 是一个可能缺失的特征，并设 $M \\in \\{0,1\\}$ 为缺失指示符，其中 $M=1$ 表示 $X$ 缺失，$M=0$ 表示 $X$ 被观测到。假设以下数据生成过程：\n- 联合分布可分解为 $p(y,x,m) = p(y)\\,p(x \\mid y)\\,p(m \\mid y)$，即缺失仅取决于类别，并且在给定类别的情况下与特征条件独立（相对于 $X$ 是随机缺失 (MAR)）。\n- 缺失机制按类别定义为 $P(M=1 \\mid Y=1)=\\alpha_1$ 和 $P(M=1 \\mid Y=0)=\\alpha_0$，其中 $0 \\lt \\alpha_0 \\lt 1$ 和 $0 \\lt \\alpha_1 \\lt 1$。\n\n一个忽略 $M$ 的判别模型试图直接从观测到的数据对中学习 $p(y \\mid x)$。当预测时 $X$ 缺失且 $M$ 被忽略时，由于没有可作为条件的观测特征值，此类模型退化为预测先验概率 $P(Y=1)=\\pi$。生成模型使用完整的分解式 $p(y)\\,p(x \\mid y)\\,p(m \\mid y)$，即使在 $X$ 未被观测到的情况下，也允许通过 $p(m \\mid y)$ 从缺失指示符 $M$ 更新对 $Y$ 的信念。\n\n任务 A（推导）。仅从贝叶斯法则和全概率定律出发：\n- 推导 $P(Y=1 \\mid M=1)$ 和 $P(Y=1 \\mid M=0)$，用 $\\pi$、$\\alpha_0$ 和 $\\alpha_1$ 表示。\n- 将忽略 $M$（当 $X$ 缺失时）的判别预测定义为 $\\widehat{P}_{\\text{disc}}(Y=1 \\mid M=1)=\\pi$。将 $M=1$ 时的偏差定义为 $b_1 = P(Y=1 \\mid M=1) - \\pi$，类似地，将 $M=0$ 时的偏差定义为 $b_0 = P(Y=1 \\mid M=0) - \\pi$。用 $\\pi$、$\\alpha_0$ 和 $\\alpha_1$ 表示 $b_1$ 和 $b_0$。\n- 考虑一种训练方法，该方法丢弃特征缺失的案例（即，限制于 $M=0$ 的情况），然后在这个子集上拟合一个用于 $p(y \\mid x)$ 的判别模型，同时忽略 $M$。在上述 MAR 假设下，推导 $P(Y=1 \\mid X=x, M=0)$ 的对数几率与 $P(Y=1 \\mid X=x)$ 的对数几率之间的关系。证明它们之间相差一个仅依赖于 $\\alpha_0$ 和 $\\alpha_1$ 的加性常数偏移。将此偏移表示为 $s$，并用 $\\alpha_0$ 和 $\\alpha_1$ 来表示它。\n\n任务 B（计算）。实现一个程序，给定 $(\\pi,\\alpha_0,\\alpha_1)$，计算以下三个量：\n- $b_1$，即上面定义的 $M=1$ 时的偏差。\n- $b_0$，即上面定义的 $M=0$ 时的偏差。\n- $s$，即上面定义的常数对数几率偏移。\n\n使用以下参数三元组 $(\\pi,\\alpha_0,\\alpha_1)$ 的测试套件：\n- 测试用例 1：$(\\pi,\\alpha_0,\\alpha_1) = (0.5, 0.2, 0.8)$。\n- 测试用例 2：$(\\pi,\\alpha_0,\\alpha_1) = (0.4, 0.3, 0.3)$。\n- 测试用例 3：$(\\pi,\\alpha_0,\\alpha_1) = (0.3, 0.01, 0.99)$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[b_1^{(1)}, b_0^{(1)}, s^{(1)}, b_1^{(2)}, b_0^{(2)}, s^{(2)}, b_1^{(3)}, b_0^{(3)}, s^{(3)}]$，其中上标表示测试用例索引。在打印输出中，每个数字必须精确到 $6$ 位小数。不应打印任何其他文本。",
            "solution": "在尝试任何解决方案之前，需对问题陈述进行验证。\n\n给定条件如下：\n- 一个二元类别标签 $Y \\in \\{0, 1\\}$，其先验概率为 $P(Y=1) = \\pi$。\n- 一个单一实值特征 $X \\in \\mathbb{R}$。\n- 一个缺失指示符 $M \\in \\{0, 1\\}$，其中 $M=1$ 表示 $X$ 缺失。\n- 联合分布分解：$p(y,x,m) = p(y) \\, p(x \\mid y) \\, p(m \\mid y)$。这意味着随机缺失 (MAR) 假设，即在给定 $Y$ 的情况下，$X$ 与 $M$ 条件独立。\n- 按类别划分的条件缺失概率：$P(M=1 \\mid Y=1) = \\alpha_1$ 和 $P(M=1 \\mid Y=0) = \\alpha_0$。\n- 参数约束：$0 \\lt \\alpha_0 \\lt 1$ 和 $0 \\lt \\alpha_1 \\lt 1$。\n- 当 $X$ 缺失时判别预测的定义：$\\widehat{P}_{\\text{disc}}(Y=1 \\mid M=1) = \\pi$。\n- 偏差的定义：$b_1 = P(Y=1 \\mid M=1) - \\pi$ 和 $b_0 = P(Y=1 \\mid M=0) - \\pi$。\n- 对数几率偏移 $s$ 的定义：$P(Y=1 \\mid X=x, M=0)$ 的对数几率与 $P(Y=1 \\mid X=x)$ 的对数几率之间的加性常数差。\n\n评估问题的有效性。\n- **科学依据**：该问题在缺失数据和概率建模的统计理论中有坚实的基础。MAR、生成模型、判别模型、贝叶斯法则和对数几率等概念都是标准的，并有严格的定义。\n- **适定性**：该问题是适定的。它提供了所有必要的定义和参数，以便为指定的任务推导出唯一且有意义的量。\n- **客观性**：该问题以精确、形式化和客观的数学语言陈述。\n\n问题陈述被认为是有效的，因为它在科学上是合理的、适定的、客观的，并且没有可辨别的缺陷。我们可以开始解决问题。\n\n任务 A：推导\n\n首先，我们推导在给定缺失状态 $M$ 的情况下，类别为 $Y=1$ 的后验概率。我们将使用贝叶斯法则 $P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$ 和全概率定律 $P(B) = \\sum_i P(B \\mid A_i)P(A_i)$。\n\n$P(Y=1 \\mid M=1)$ 的推导：\n后验概率 $P(Y=1 \\mid M=1)$ 由贝叶斯法则给出：\n$$P(Y=1 \\mid M=1) = \\frac{P(M=1 \\mid Y=1) P(Y=1)}{P(M=1)}$$\n分子中的项是给定的 $P(M=1 \\mid Y=1) = \\alpha_1$ 和 $P(Y=1) = \\pi$。分母 $P(M=1)$ 是缺失的边际概率，我们使用全概率定律在类别 $Y=0$ 和 $Y=1$ 上展开它：\n$$P(M=1) = P(M=1 \\mid Y=1)P(Y=1) + P(M=1 \\mid Y=0)P(Y=0)$$\n代入已知量，$P(Y=0)=1-\\pi$ 和 $P(M=1 \\mid Y=0)=\\alpha_0$：\n$$P(M=1) = \\alpha_1 \\pi + \\alpha_0 (1-\\pi)$$\n将这些结果结合起来，得到后验概率的表达式：\n$$P(Y=1 \\mid M=1) = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\n\n$P(Y=1 \\mid M=0)$ 的推导：\n这个推导遵循相同的逻辑。我们首先确定观测到特征（$M=0$）的概率：\n$P(M=0 \\mid Y=1) = 1 - P(M=1 \\mid Y=1) = 1 - \\alpha_1$\n$P(M=0 \\mid Y=0) = 1 - P(M=1 \\mid Y=0) = 1 - \\alpha_0$\n使用贝叶斯法则：\n$$P(Y=1 \\mid M=0) = \\frac{P(M=0 \\mid Y=1) P(Y=1)}{P(M=0)}$$\n分子是 $(1 - \\alpha_1) \\pi$。分母是特征被观测到的边际概率：\n$$P(M=0) = P(M=0 \\mid Y=1)P(Y=1) + P(M=0 \\mid Y=0)P(Y=0) = (1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)$$\n因此，后验概率是：\n$$P(Y=1 \\mid M=0) = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\n偏差 $b_1$ 和 $b_0$ 的推导：\n偏差 $b_1$ 定义为 $b_1 = P(Y=1 \\mid M=1) - \\pi$。代入推导出的 $P(Y=1 \\mid M=1)$ 表达式：\n$$b_1 = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - \\pi = \\pi \\left( \\frac{\\alpha_1}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - 1 \\right)$$\n$$b_1 = \\pi \\left( \\frac{\\alpha_1 - (\\alpha_1 \\pi + \\alpha_0 (1-\\pi))}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right) = \\pi \\left( \\frac{\\alpha_1(1-\\pi) - \\alpha_0(1-\\pi)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right)$$\n$$b_1 = \\frac{\\pi (1-\\pi) (\\alpha_1 - \\alpha_0)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\n这个表达式表明，偏差 $b_1$ 为零当且仅当 $\\alpha_1 = \\alpha_0$，即当缺失与类别完全独立时。\n\n偏差 $b_0$ 定义为 $b_0 = P(Y=1 \\mid M=0) - \\pi$。代入推导出的 $P(Y=1 \\mid M=0)$ 表达式：\n$$b_0 = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - \\pi = \\pi \\left( \\frac{1 - \\alpha_1}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - 1 \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1) - ((1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi))}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1)(1-\\pi) - (1 - \\alpha_0)(1-\\pi)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\frac{\\pi (1-\\pi) (\\alpha_0 - \\alpha_1)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\n对数几率偏移 $s$ 的推导：\n概率为 $p$ 的事件的对数几率是 $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$。我们需要比较 $P(Y=1 \\mid X=x)$ 的对数几率与 $P(Y=1 \\mid X=x, M=0)$ 的对数几率。\n$P(Y=1 \\mid X=x)$ 的几率是：\n$$\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)} = \\frac{p(x \\mid Y=1)P(Y=1)}{p(x \\mid Y=0)P(Y=0)} = \\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}$$\n取自然对数得到对数几率：\n$$\\text{log-odds}(P(Y=1 \\mid X=x)) = \\log\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) + \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$$\n$P(Y=1 \\mid X=x, M=0)$ 的几率是：\n$$\\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\frac{p(Y=1, x, M=0)}{p(Y=0, x, M=0)}$$\n使用给定的分解式 $p(y,x,m) = p(y)p(x \\mid y)p(m \\mid y)$：\n$$ \\frac{p(Y=1)p(x \\mid Y=1)p(M=0 \\mid Y=1)}{p(Y=0)p(x \\mid Y=0)p(M=0 \\mid Y=0)} = \\frac{\\pi \\, p(x \\mid Y=1) \\, (1-\\alpha_1)}{(1-\\pi) \\, p(x \\mid Y=0) \\, (1-\\alpha_0)} $$\n$$ \\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) \\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\n对观测数据的子集取自然对数得到对数几率：\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\log\\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\text{log-odds}(P(Y=1 \\mid X=x)) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\n问题将偏移量 $s$ 定义为这个加性常数。因此：\n$$s = \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\n这个偏移量表示在指定的 MAR 机制下，当模型仅在具有观测特征的数据子集上训练时，在对数几率尺度上引入的偏差。该偏差仅取决于两个类别之间观测率的差异。\n\n这些推导完成了任务 A。这些公式现在将用于任务 B 的计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes biases and log-odds shift for given test cases\n    based on a model of missing data in binary classification.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (pi, alpha_0, alpha_1).\n    test_cases = [\n        (0.5, 0.2, 0.8),\n        (0.4, 0.3, 0.3),\n        (0.3, 0.01, 0.99),\n    ]\n\n    # List to store the results for all test cases.\n    results = []\n\n    for case in test_cases:\n        pi, alpha_0, alpha_1 = case\n\n        # --- Calculate b_1, the bias for M=1 ---\n        # Formula: b_1 = (pi * (1-pi) * (alpha_1 - alpha_0)) / (alpha_1 * pi + alpha_0 * (1-pi))\n        # This formula is well-defined as the denominator is P(M=1), which is > 0.\n        b1_numerator = pi * (1.0 - pi) * (alpha_1 - alpha_0)\n        b1_denominator = alpha_1 * pi + alpha_0 * (1.0 - pi)\n        b1 = b1_numerator / b1_denominator\n\n        # --- Calculate b_0, the bias for M=0 ---\n        # Formula: b_0 = (pi * (1-pi) * (alpha_0 - alpha_1)) / ((1-alpha_1) * pi + (1-alpha_0) * (1-pi))\n        # This formula is well-defined as the denominator is P(M=0), which is > 0.\n        b0_numerator = pi * (1.0 - pi) * (alpha_0 - alpha_1)\n        b0_denominator = (1.0 - alpha_1) * pi + (1.0 - alpha_0) * (1.0 - pi)\n        b0 = b0_numerator / b0_denominator\n\n        # --- Calculate s, the log-odds shift ---\n        # Formula: s = log((1-alpha_1) / (1-alpha_0))\n        # np.log is the natural logarithm.\n        # The argument is well-defined and positive because 0  alpha_0, alpha_1  1.\n        s = np.log((1.0 - alpha_1) / (1.0 - alpha_0))\n\n        results.extend([b1, b0, s])\n\n    # Format the final output string exactly as required.\n    # Each number is rounded to 6 decimal places.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\n# Execute the main function.\nsolve()\n```"
        },
        {
            "introduction": "在高维数据集中，并非所有特征都有用。我们如何选择信息最丰富的特征呢？本练习对比了两种特征选择的哲学。生成方法根据特征对类别条件数据模型的拟合优度来评估特征，而判别方法则根据特征对分类边界的直接影响来对它们进行排序。通过在具有不同特征（例如相关特征）的数据集上实现和比较这两种方法，您将深入了解它们各自的优缺点，以及在何种情况下更适合选择哪种方法。",
            "id": "3124940",
            "problem": "给定一个二元分类场景，其中包含随机变量 $X \\in \\mathbb{R}^d$ 和 $Y \\in \\{0,1\\}$。我们考虑两类模型：一个指定类条件分布 $p(x \\mid y)$ 的生成模型，以及一个指定条件类别概率 $p(y \\mid x)$ 的判别模型。您的任务是实现两种特征选择过程，并在几个合成测试案例上对它们进行比较。\n\n此任务的基础是条件概率和贝叶斯法则的定义、用于参数估计的最大似然原理，以及经过充分检验的逻辑回归模型对 $p(y \\mid x)$ 的表述。生成过程将通过特征对类条件对数似然的经验期望的贡献来为特征评分，而判别过程将通过对数条件概率关于每个特征的梯度大小的经验平均值来为特征评分。\n\n使用的定义：\n- 一个函数 $f(X,Y)$ 在数据集 $\\{(x_i,y_i)\\}_{i=1}^n$ 上的经验期望是 $\\frac{1}{n}\\sum_{i=1}^n f(x_i,y_i)$。\n- 在特征条件独立的朴素贝叶斯假设下，一个类条件高斯模型形式为 $p(x \\mid y=k) = \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{k,j}, \\sigma_{k,j}^2)$，其中 $\\mu_{k,j}$ 和 $\\sigma_{k,j}^2$ 分别是类别 $k \\in \\{0,1\\}$ 中特征 $j$ 的特定类别均值和方差，通过最大似然估计（MLE）进行估计。\n- 逻辑回归将 $p(y=1 \\mid x)$ 建模为 $\\sigma(w^\\top x + b)$，其中 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 是逻辑函数，$w \\in \\mathbb{R}^d$ 是权重向量，$b \\in \\mathbb{R}$ 是偏置项，通过最大似然估计（MLE）进行估计，可选择使用$\\ell_2$正则化（也称为岭惩罚）以提高数值稳定性。\n\n您的程序必须实现：\n- 特征 $j$ 的生成特征分数：$s^{\\text{gen}}_j = \\mathbb{E}\\left[\\log p(x_j \\mid y)\\right]$，其中期望是在经验数据集上计算的，而 $p(x_j \\mid y)$ 是一维高斯分布 $\\mathcal{N}(x_j \\mid \\mu_{y,j}, \\sigma_{y,j}^2)$，其参数是通过对仅限于类别 $y$ 的训练数据进行 MLE 学习得到的。为了数值稳定性，您可以将方差的下限设置为一个小的正值 $10^{-6}$。\n- 特征 $j$ 的判别特征分数：$s^{\\text{disc}}_j = \\mathbb{E}\\left[\\left|\\frac{\\partial}{\\partial x_j}\\log p(y \\mid x)\\right|\\right]$，该值在通过 MLE（对 $w$ 施加大小为 $\\lambda = 0.1$ 的 $\\ell_2$ 惩罚以确保优化问题良定）找到的拟合逻辑回归参数 $(w,b)$ 处进行评估。梯度是关于输入特征 $x_j$ 计算的。\n\n对于这两种过程，按得分降序选择前 $k$ 个特征，若得分相同则优先选择较小的特征索引。报告每个测试案例所选的特征索引列表。\n\n数据生成：\n所有测试案例都使用固定的伪随机种子 $42$ 以确保可复现性。每个案例都是一个具有 $d=6$ 个特征的二元分类问题。除非另有说明，否则使用独立的高斯特征。对于类别 $y=0$ 和类别 $y=1$，每个特征的均值和方差如下指定。对于每个案例，从类别 $y=0$ 的分布中抽取 $n_0$ 个样本，从类别 $y=1$ 的分布中抽取 $n_1$ 个样本，将它们连接起来形成数据集 $\\{(x_i,y_i)\\}_{i=1}^{n}$，其中 $n=n_0+n_1$，并按指定设置 $k$。本问题不涉及角度和物理单位。\n\n- 案例 1（均衡，两个信息特征）：\n  - $n_0 = 200$, $n_1 = 200$, $d = 6$, $k=2$。\n  - 类别 0：特征 0 的均值为 -2，方差为 1；特征 1 的均值为 2，方差为 1；特征 2,3,4,5 的均值为 0，方差为 3。\n  - 类别 1：特征 0 的均值为 2，方差为 1；特征 1 的均值为 -2，方差为 1；特征 2,3,4,5 的均值为 0，方差为 3。\n\n- 案例 2（均衡，一个低方差高信息量特征）：\n  - $n_0 = 300$, $n_1 = 300$, $d = 6$, $k=2$。\n  - 类别 0：特征 0 的均值为 -1，方差为 1；特征 2 的均值为 -1，方差为 0.2；特征 1,3,4,5 的均值为 0，方差为 2。\n  - 类别 1：特征 0 的均值为 1，方差为 1；特征 2 的均值为 1，方差为 0.2；特征 1,3,4,5 的均值为 0，方差为 2。\n\n- 案例 3（类别不均衡，两个信息特征）：\n  - $n_0 = 60$, $n_1 = 240$, $d = 6$, $k=2$。\n  - 类别 0：特征 0 的均值为 -1，方差为 1；特征 3 的均值为 -1，方差为 1；特征 1,2,4,5 的均值为 0，方差为 5。\n  - 类别 1：特征 0 的均值为 1，方差为 1；特征 3 的均值为 1，方差为 1；特征 1,2,4,5 的均值为 0，方差为 5。\n\n- 案例 4（均衡，两个冗余相关的信​​息特征）：\n  - $n_0 = 250$, $n_1 = 250$, $d = 6$, $k=2$。\n  - 为每个样本构建一个潜标量 $z$，其中 $z \\sim \\mathcal{N}(\\mu_y, 1)$，$\\mu_0 = -2$ 且 $\\mu_1 = 2$。\n  - 将特征 0 设置为 $x_0 = z + \\epsilon_0$，特征 1 设置为 $x_1 = z + \\epsilon_1$，其中 $\\epsilon_0 \\sim \\mathcal{N}(0, 0.1^2)$ 且 $\\epsilon_1 \\sim \\mathcal{N}(0, 0.1^2)$。\n  - 特征 2,3,4,5 是独立噪声，对于两个类别，均值为 0，方差为 9。\n\n实现约束：\n- 使用最大似然估计（MLE）为生成模型拟合 $\\mu_{k,j}$ 和 $\\sigma_{k,j}^2$。\n- 通过最小化带有大小为 $\\lambda = 0.1$ 的 $\\ell_2$ 惩罚的正则化负对数似然，使用数值稳定的优化器来拟合逻辑回归参数 $(w,b)$。在进行判别模型拟合时，通过减去每个特征在所有训练样本（非特定类别）上的全局均值并除以其全局标准差来对特征进行标准化。\n- 对于生成分数，将 $s^{\\text{gen}}_j$ 精确计算为 $\\log \\mathcal{N}(x_{i,j} \\mid \\mu_{y_i,j}, \\sigma_{y_i,j}^2)$ 的经验均值。\n- 对于判别分数，将 $s^{\\text{disc}}_j$ 计算为在拟合参数 $(w,b)$ 处 $\\left|\\frac{\\partial}{\\partial x_j}\\log p(y_i \\mid x_i)\\right|$ 的经验均值。\n\n测试套件和答案规范：\n- 为指定的四个案例实现以上内容，使用固定的种子 $42$。\n- 对于每个案例，输出在生成评分和判别评分下选出的前 $k$ 个特征索引。\n- 最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中每个元素对应一个案例，并且本身是一个双元素列表 $[G,D]$。这里 $G$ 是生成选择的 $k$ 个整数的列表，$D$ 是判别选择的 $k$ 个整数的列表。例如，一个包含两个案例的输出行将看起来像 $[[[0,1],[0,1]],[[2,0],[2,0]]]$。打印的输出中不能有任何空白字符。\n\n您的程序必须是一个完整的、可运行的程序，不需要用户输入，并且只使用指定的 Python 标准库、NumPy 和 SciPy。",
            "solution": "该问题要求实现并比较两种截然不同的特征选择方法：一种是生成式方法，另一种是判别式方法。解决方案涉及为多个测试案例生成合成数据，应用这两种选择算法，并报告排名最高的特征。\n\n### 1. 生成式特征选择\n\n生成式方法对特征的类条件概率分布 $p(x \\mid y)$ 进行建模。由此，可以使用贝叶斯法则推导出后验概率 $p(y \\mid x)$，这也需要对类先验概率 $p(y)$ 进行建模。指定的生成模型是具有高斯类条件分布的朴素贝叶斯分类器。该模型基于一个强假设：给定类别标签，特征之间条件独立。\n\n对于给定的类别 $k \\in \\{0, 1\\}$，其分布为：\n$$p(x \\mid y=k) = \\prod_{j=1}^d p(x_j \\mid y=k) = \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{k,j}, \\sigma_{k,j}^2)$$\n其中 $x_j$ 是第 $j$ 个特征，$\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ 是高斯分布的概率密度函数。\n\n**参数估计**：每个类别 $k$ 和特征 $j$ 的参数 $(\\mu_{k,j}, \\sigma_{k,j}^2)$ 都是使用最大似然估计（MLE）从训练数据中估计的。对于属于类别 $k$ 的一组样本 $\\{x_{i,j}\\}_{i \\mid y_i=k}$，MLE 估计值是样本均值和样本方差：\n$$\\hat{\\mu}_{k,j} = \\frac{1}{n_k} \\sum_{i: y_i=k} x_{i,j}$$\n$$\\hat{\\sigma}_{k,j}^2 = \\frac{1}{n_k} \\sum_{i: y_i=k} (x_{i,j} - \\hat{\\mu}_{k,j})^2$$\n其中 $n_k$ 是类别 $k$ 中的样本数。为了数值稳定性，估计的方差被限制在一个小的正值下限 $\\epsilon = 10^{-6}$。\n\n**特征评分**：特征 $j$ 的分数，表示为 $s^{\\text{gen}}_j$，定义为该特征的类条件对数似然的经验期望：\n$$s^{\\text{gen}}_j = \\mathbb{E}\\left[\\log p(x_j \\mid y)\\right] = \\frac{1}{n} \\sum_{i=1}^n \\log p(x_{i,j} \\mid y_i)$$\n这个分数是使用每个样本 $(x_i, y_i)$ 的估计参数 $(\\hat{\\mu}_{y_i,j}, \\hat{\\sigma}_{y_i,j}^2)$ 来计算的。单个观测值 $x_{i,j}$ 在给定其类别 $y_i$ 的情况下的对数似然为：\n$$\\log \\mathcal{N}(x_{i,j} \\mid \\hat{\\mu}_{y_i,j}, \\hat{\\sigma}_{y_i,j}^2) = -\\frac{1}{2} \\log(2\\pi\\hat{\\sigma}_{y_i,j}^2) - \\frac{(x_{i,j} - \\hat{\\mu}_{y_i,j})^2}{2\\hat{\\sigma}_{y_i,j}^2}$$\n该分数衡量了特征 $j$ 的数据在多大程度上符合学习到的类条件高斯模型。产生更高平均对数似然的特征被认为更具信息量。\n\n### 2. 判别式特征选择\n\n判别式方法直接对后验概率 $p(y \\mid x)$ 进行建模，绕过了对特征分布 $p(x \\mid y)$ 进行建模的需要。指定的模型是逻辑回归。\n\n逻辑回归模型将类别 1 的概率定义为：\n$$p(y=1 \\mid x) = \\sigma(w^\\top x' + b) = \\frac{1}{1 + \\exp(-(w^\\top x' + b))}$$\n其中 $w \\in \\mathbb{R}^d$ 是权重向量，$b \\in \\mathbb{R}$ 是偏置项，$\\sigma(\\cdot)$ 是逻辑 S 型函数。输入向量 $x'$ 表示经过标准化（减去全局均值并除以每个特征在所有数据上的全局标准差）后的特征。\n\n**参数估计**：参数 $(w, b)$ 是通过最小化正则化负对数似然（也称为带 $\\ell_2$ 惩罚的交叉熵损失）来找到的。要最小化的目标函数是：\n$$L(w,b) = -\\sum_{i=1}^n \\left[ y_i \\log p(y_i=1 \\mid x_i) + (1-y_i) \\log p(y_i=0 \\mid x_i) \\right] + \\frac{\\lambda}{2} \\|w\\|_2^2$$\n其中 $\\lambda=0.1$ 是正则化强度。这是一个凸优化问题，可以使用诸如 L-BFGS 之类的数值方法高效解决。\n\n**特征评分**：特征 $j$ 的分数 $s^{\\text{disc}}_j$ 是对数条件概率关于原始（未标准化）特征 $x_j$ 的梯度幅值的经验平均值：\n$$s^{\\text{disc}}_j = \\mathbb{E}\\left[\\left|\\frac{\\partial}{\\partial x_j}\\log p(y \\mid x)\\right|\\right] = \\frac{1}{n} \\sum_{i=1}^n \\left|\\frac{\\partial}{\\partial x_j}\\log p(y_i \\mid x_i)\\right|$$\n我们使用链式法则推导梯度。令 $z_i = w^\\top x'_i + b$。单个样本的对数似然为 $l_i = \\log p(y_i | x_i)$。关于线性激活 $z_i$ 的梯度是 $\\frac{\\partial l_i}{\\partial z_i} = y_i - \\sigma(z_i)$。由于 $x'_{i,j} = (x_{i,j} - \\mu^{\\text{glob}}_j) / \\sigma^{\\text{glob}}_j$，我们有 $\\frac{\\partial z_i}{\\partial x_j} = \\frac{w_j}{\\sigma^{\\text{glob}}_j}$。因此，关于原始特征的梯度是：\n$$\\frac{\\partial l_i}{\\partial x_j} = \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial x_j} = (y_i - \\sigma(z_i)) \\frac{w_j}{\\sigma^{\\text{glob}}_j}$$\n特征 $j$ 的分数变为：\n$$s^{\\text{disc}}_j = \\frac{1}{n} \\sum_{i=1}^n \\left| (y_i - \\sigma(z_i)) \\frac{w_j}{\\sigma^{\\text{glob}}_j} \\right| = \\left|\\frac{w_j}{\\sigma^{\\text{glob}}_j}\\right| \\left(\\frac{1}{n} \\sum_{i=1}^n |y_i - \\sigma(z_i)|\\right)$$\n由于项 $\\frac{1}{n} \\sum_i |y_i - \\sigma(z_i)|$ 对所有特征 $j$ 都是常数，因此特征的排名仅由学习到的标准化特征的权重 $w_j$ 的大小除以原始特征的标准差 $\\sigma^{\\text{glob}}_j$ 来决定。该度量量化了每个特征对模型预测的影响。\n\n### 3. 特征排序与选择\n\n对于这两种方法，在计算完所有特征 $j=1, \\dots, d$ 的分数 $\\{s_j\\}$ 后，特征按其分数的降序进行排序。为确保排序的确定性，通过优先选择索引较小的特征来打破平局。然后从这个排序列表中选出前 $k$ 个特征。在计算上，这是通过对键 $(\\text{-score}, \\text{index})$ 进行字典序排序来实现的。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit, xlogy\n\ndef generate_data(case_params, rng):\n    \"\"\"\n    Generates synthetic data for a given test case.\n    \"\"\"\n    d = case_params['d']\n    n0, n1 = case_params['n0'], case_params['n1']\n    n = n0 + n1\n    X = np.zeros((n, d))\n    y = np.concatenate([np.zeros(n0), np.ones(n1)])\n\n    if case_params['id'] in [1, 2, 3]:\n        means0, var0 = case_params['class0_params']\n        means1, var1 = case_params['class1_params']\n        std0, std1 = np.sqrt(var0), np.sqrt(var1)\n        \n        X[:n0, :] = rng.normal(loc=means0, scale=std0, size=(n0, d))\n        X[n0:, :] = rng.normal(loc=means1, scale=std1, size=(n1, d))\n    \n    elif case_params['id'] == 4:\n        mu0, mu1 = -2, 2\n        \n        # Generate latent variable z\n        z0 = rng.normal(loc=mu0, scale=1, size=n0)\n        z1 = rng.normal(loc=mu1, scale=1, size=n1)\n        z = np.concatenate([z0, z1])\n        \n        # Generate correlated features x0, x1 from z\n        epsilon0 = rng.normal(loc=0, scale=0.1, size=n)\n        epsilon1 = rng.normal(loc=0, scale=0.1, size=n)\n        X[:, 0] = z + epsilon0\n        X[:, 1] = z + epsilon1\n        \n        # Generate independent noise features\n        X[:, 2:] = rng.normal(loc=0, scale=3, size=(n, d - 2))\n        \n    return X, y\n\ndef get_top_k_indices(scores, k):\n    \"\"\"\n    Selects top-k feature indices based on scores, with tie-breaking.\n    \"\"\"\n    indices = np.arange(len(scores))\n    # Sort by score (descending) and then index (ascending) to break ties\n    sorted_indices = np.lexsort((indices, -scores))\n    return sorted_indices[:k]\n    \ndef generative_selector(X, y, k):\n    \"\"\"\n    Computes generative feature scores and selects top k.\n    \"\"\"\n    n, d = X.shape\n    \n    X0 = X[y == 0]\n    X1 = X[y == 1]\n    n0, n1 = len(X0), len(X1)\n\n    # MLE for parameters per class\n    mu0 = np.mean(X0, axis=0) if n0 > 0 else np.zeros(d)\n    var0 = np.var(X0, axis=0) if n0 > 1 else np.ones(d)\n    mu1 = np.mean(X1, axis=0) if n1 > 0 else np.zeros(d)\n    var1 = np.var(X1, axis=0) if n1 > 1 else np.ones(d)\n    \n    # Apply variance floor\n    var0 = np.maximum(var0, 1e-6)\n    var1 = np.maximum(var1, 1e-6)\n    \n    mus = np.array([mu0, mu1])\n    variances = np.array([var0, var1])\n    \n    # Compute empirical expectation of log-likelihood\n    log_likelihoods = np.zeros(d)\n    for i in range(n):\n        yi = int(y[i])\n        xi = X[i, :]\n        mu_yi = mus[yi, :]\n        var_yi = variances[yi, :]\n        \n        log_pdf_i = -0.5 * np.log(2 * np.pi * var_yi) - ((xi - mu_yi)**2) / (2 * var_yi)\n        log_likelihoods += log_pdf_i\n        \n    scores = log_likelihoods / n\n\n    return get_top_k_indices(scores, k)\n\ndef discriminative_selector(X, y, k, lambda_reg):\n    \"\"\"\n    Computes discriminative feature scores and selects top k.\n    \"\"\"\n    n, d = X.shape\n    \n    # Standardize features (globally)\n    mu_glob = np.mean(X, axis=0)\n    std_glob = np.std(X, axis=0)\n    std_glob[std_glob == 0] = 1.0  # Avoid division by zero\n    \n    X_std = (X - mu_glob) / std_glob\n    \n    # Objective function for logistic regression\n    def objective_function(params, X_s, y_l, lambda_r):\n        w = params[:-1]\n        b = params[-1]\n        z = X_s @ w + b\n        y_hat = expit(z)\n        \n        # Regularized negative log-likelihood (cost)\n        nll = -np.sum(xlogy(y_l, y_hat) + xlogy(1 - y_l, 1 - y_hat))\n        l2_penalty = 0.5 * lambda_r * np.sum(w**2)\n        cost = nll + l2_penalty\n        \n        # Gradient\n        error = y_hat - y_l\n        grad_w = X_s.T @ error + lambda_r * w\n        grad_b = np.sum(error)\n        grad = np.concatenate((grad_w, [grad_b]))\n        return cost, grad\n\n    initial_params = np.zeros(d + 1)\n    res = minimize(\n        objective_function,\n        initial_params,\n        args=(X_std, y, lambda_reg),\n        jac=True,\n        method='L-BFGS-B'\n    )\n    \n    w_opt = res.x[:-1]\n    \n    # Feature score is proportional to |w_j / std_j|\n    scores = np.abs(w_opt / std_glob)\n    \n    return get_top_k_indices(scores, k)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            'id': 1, 'n0': 200, 'n1': 200, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-2, 2, 0, 0, 0, 0]), np.array([1, 1, 3, 3, 3, 3])),\n            'class1_params': (np.array([2, -2, 0, 0, 0, 0]), np.array([1, 1, 3, 3, 3, 3]))\n        },\n        {\n            'id': 2, 'n0': 300, 'n1': 300, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-1, 0, -1, 0, 0, 0]), np.array([1, 2, 0.2, 2, 2, 2])),\n            'class1_params': (np.array([1, 0, 1, 0, 0, 0]), np.array([1, 2, 0.2, 2, 2, 2]))\n        },\n        {\n            'id': 3, 'n0': 60, 'n1': 240, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-1, 0, 0, -1, 0, 0]), np.array([1, 5, 5, 1, 5, 5])),\n            'class1_params': (np.array([1, 0, 0, 1, 0, 0]), np.array([1, 5, 5, 1, 5, 5]))\n        },\n        {\n            'id': 4, 'n0': 250, 'n1': 250, 'd': 6, 'k': 2\n            # Special generation logic is handled inside generate_data\n        }\n    ]\n    \n    rng = np.random.default_rng(42)\n    lambda_reg = 0.1\n    \n    results_str_list = []\n    \n    for case in test_cases:\n        X, y = generate_data(case, rng)\n        k = case['k']\n        \n        gen_indices = generative_selector(X, y, k)\n        disc_indices = discriminative_selector(X, y, k, lambda_reg)\n        \n        gen_indices_str = f\"[{','.join(map(str, gen_indices))}]\"\n        disc_indices_str = f\"[{','.join(map(str, disc_indices))}]\"\n        \n        results_str_list.append(f\"[{gen_indices_str},{disc_indices_str}]\")\n        \n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们是否必须在纯粹的生成模型或判别模型之间做出选择？我们能否将两者结合起来？这最后一个练习介绍了一种混合目标函数，它将判别损失与生成正则化项相结合。这种方法旨在利用判别模型强大的预测能力，同时受益于生成模型提供的结构化假设和正则化效果。您将通过实现一个基于梯度的优化算法来训练这样一个混合模型，并研究权衡参数 $\\lambda$ 如何影响模型的泛化能力。这个练习展示了一种现代方法，它将生成与判别的区别视为一个连续的光谱，使您能够为获得最佳性能而调整模型。",
            "id": "3124935",
            "problem": "给定一个二元分类场景，其输入为 $x \\in \\mathbb{R}$，标签为 $y \\in \\{0,1\\}$。考虑一个结合了条件负对数损失和边缘密度正则化项的混合经验风险。该模型使用一个共享参数 $ \\mu \\in \\mathbb{R} $ 来耦合条件部分和边缘部分，以及一个斜率参数 $ a \\in \\mathbb{R} $ 用于条件链接。条件模型为 $p(y \\mid x) = \\mathrm{Bernoulli}(\\sigma(z))$，其中 $z = a(x - \\mu)$，而 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ 是逻辑S型函数。$x$ 的边缘密度模型是一个已知方差为 $s^2$、未知均值为 $\\mu$ 的高斯分布，即 $p(x) = \\mathcal{N}(\\mu, s^2)$。对于一个数据集 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$，定义混合经验风险\n$$\nR_n(\\mu, a; \\lambda) \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma\\!\\left(a(x_i - \\mu)\\right) - (1-y_i) \\log \\left(1 - \\sigma\\!\\left(a(x_i - \\mu)\\right)\\right) \\right] \\;+\\; \\lambda \\cdot \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right],\n$$\n其中 $\\lambda \\ge 0$ 是一个权衡参数。第一项是经验条件负对数损失，第二项是带有惩罚权重 $\\lambda$ 的经验负对数边缘密度。你的任务是通过从第一性原理推导学习规则，然后评估测试条件损失，来分析 $\\lambda$ 如何影响泛化能力。\n\n推导的基本依据：\n- 使用逻辑S型函数 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ 的定义和伯努利负对数似然。\n- 使用一维高斯分布 $\\mathcal{N}(\\mu, s^2)$ 的负对数密度。\n- 使用经验风险最小化的概念将训练损失与泛化能力联系起来。\n\n要求的推导和计算：\n1) 从上述定义出发，推导 $R_n(\\mu, a; \\lambda)$ 关于 $\\mu$ 和 $a$ 的梯度。仅使用基本微积分和链式法则，将梯度表示为 $\\{x_i, y_i\\}_{i=1}^n$、$a$、$\\mu$、$s^2$ 和 $\\lambda$ 的函数。除了此处陈述的基本定义外，不要假设任何已知的梯度闭式解。\n2) 对于固定的 $\\lambda$，实现全批量梯度下降以在 $(\\mu, a)$ 上最小化 $R_n(\\mu, a; \\lambda)$，并使用以下固定设置以确保确定性：\n   - 初始化：$a_0 = 1.0$ 和 $\\mu_0 = \\frac{1}{n}\\sum_{i=1}^n x_i$。\n   - 学习率调度：$\\eta_k = \\frac{\\eta_0}{\\sqrt{k+1}}$，其中 $\\eta_0 = 0.3$，$k$ 是从 $k=0$ 开始的第 $k$ 次迭代索引。\n   - 迭代次数：$T = 2000$。\n   - 已知方差：$s^2 = 1.0$。\n   - 每次更新后进行参数裁剪：将 $a$ 裁剪到区间 $[-10, 10]$，将 $\\mu$ 裁剪到区间 $[-5, 5]$。\n3) 对于每个训练好的 $(\\widehat{\\mu}, \\widehat{a})$，评估测试条件负对数损失\n$$\n\\widehat{R}_{\\text{test}}(\\widehat{\\mu}, \\widehat{a}) \\;=\\; \\frac{1}{m} \\sum_{j=1}^m \\left[ - y^{\\text{test}}_j \\log \\sigma\\!\\left(\\widehat{a}(x^{\\text{test}}_j - \\widehat{\\mu})\\right) - \\left(1-y^{\\text{test}}_j\\right) \\log \\left(1 - \\sigma\\!\\left(\\widehat{a}(x^{\\text{test}}_j - \\widehat{\\mu})\\right)\\right) \\right].\n$$\n4) 对于下面的每个数据集，以及指定网格中的每个 $\\lambda$，使用第 2) 项中的规定进行训练，按第 3) 项计算测试条件损失，并选择使测试条件损失最小化的 $\\lambda$。如果出现差值在绝对容差 $10^{-6}$ 以内的平局，则在所有最小化器中选择最小的 $\\lambda$。\n\n测试套件包括三个数据集，每个数据集都有一个训练集和一个测试集，以及相同的 $\\lambda$ 网格和方差 $s^2$：\n- 数据集A（平衡簇）：\n  - 训练集：$x^{\\text{train}} = [-1.3, -1.0, -0.7, 0.6, 0.9, 1.2]$，$y^{\\text{train}} = [0, 0, 0, 1, 1, 1]$。\n  - 测试集：$x^{\\text{test}} = [-1.1, -0.9, 0.8, 1.1]$，$y^{\\text{test}} = [0, 0, 1, 1]$。\n- 数据集B（无信号）：\n  - 训练集：$x^{\\text{train}} = [-0.3, -0.1, 0.1, 0.3, -0.2, 0.2]$，$y^{\\text{train}} = [0, 1, 0, 1, 1, 0]$。\n  - 测试集：$x^{\\text{test}} = [-0.4, 0.0, 0.4]$，$y^{\\text{test}} = [0, 1, 0]$。\n- 数据集C（类别不平衡）：\n  - 训练集：$x^{\\text{train}} = [-1.2, -0.8, 0.4, 0.8, 1.0, 1.2, 1.4]$，$y^{\\text{train}} = [0, 0, 1, 1, 1, 1, 1]$。\n  - 测试集：$x^{\\text{test}} = [-0.9, -0.7, 0.9, 1.1]$，$y^{\\text{test}} = [0, 0, 1, 1]$。\n- 方差：所有数据集的 $s^2 = 1.0$。\n- 权衡网格：$\\Lambda = [0.0, 0.1, 1.0, 10.0]$。\n\n本问题不涉及角度和物理单位。所有输出必须是实值浮点数。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中第 $k$ 个条目是按 A、B、C 顺序为第 $k$ 个数据集选择的 $\\lambda$（例如，输出如 $[0.0,0.1,1.0]$）。确保实现仅使用提供的规范，不引入任何随机性。\n\n你的最终程序必须实现这些推导和计算，并以上述单行格式为每个数据集输出所选的 $\\lambda$。目标是将混合目标在判别式建模和生成式建模之间进行插值的理论与促进更好测试条件损失的 $\\lambda$ 的经验选择联系起来，从而从泛化能力的角度分析 $\\lambda$。",
            "solution": "该问题要求我们通过推导其梯度并实现一个基于梯度下降的学习算法来分析一个混合经验风险函数。目标是找到最优的权衡参数 $\\lambda$，以最小化三个不同数据集上的测试条件损失。\n\n混合经验风险由下式给出：\n$$\nR_n(\\mu, a; \\lambda) \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma\\!\\left(a(x_i - \\mu)\\right) - (1-y_i) \\log \\left(1 - \\sigma\\!\\left(a(x_i - \\mu)\\right)\\right) \\right] \\;+\\; \\lambda \\cdot \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right]\n$$\n这可以写成一个条件损失分量 $L_C$ 和一个由 $\\lambda$ 加权的边缘损失分量 $L_M$ 的和：\n$$\nR_n(\\mu, a; \\lambda) = L_C(\\mu, a) + \\lambda L_M(\\mu)\n$$\n其中\n$$\nL_C(\\mu, a) = \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma(z_i) - (1-y_i) \\log(1 - \\sigma(z_i)) \\right] \\text{, with } z_i = a(x_i - \\mu)\n$$\n$$\nL_M(\\mu) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right]\n$$\n\n为了使用梯度下降最小化此风险函数，我们必须计算它关于参数 $\\mu$ 和 $a$ 的偏导数。\n\n**1. 梯度推导**\n\n首先，我们确定逻辑S型函数 $\\sigma(t) = \\frac{1}{1 + e^{-t}} = (1 + e^{-t})^{-1}$ 的导数。使用链式法则：\n$$\n\\frac{d\\sigma(t)}{dt} = -1 \\cdot (1 + e^{-t})^{-2} \\cdot (-e^{-t}) = \\frac{e^{-t}}{(1+e^{-t})^2}\n$$\n这可以重写为：\n$$\n\\frac{d\\sigma(t)}{dt} = \\frac{1}{1+e^{-t}} \\cdot \\frac{e^{-t}}{1+e^{-t}} = \\sigma(t) \\cdot \\frac{(1+e^{-t})-1}{1+e^{-t}} = \\sigma(t) \\cdot \\left(1 - \\frac{1}{1+e^{-t}}\\right) = \\sigma(t)(1-\\sigma(t))\n$$\n\n现在，我们来求单样本条件损失项 $l_i = - y_i \\log \\sigma(z_i) - (1-y_i) \\log(1 - \\sigma(z_i))$ 关于 $z_i$ 的梯度：\n$$\n\\frac{\\partial l_i}{\\partial z_i} = \\frac{\\partial l_i}{\\partial \\sigma(z_i)} \\frac{d\\sigma(z_i)}{dz_i}\n$$\n第一部分是：\n$$\n\\frac{\\partial l_i}{\\partial \\sigma(z_i)} = -\\frac{y_i}{\\sigma(z_i)} - \\frac{1-y_i}{1-\\sigma(z_i)}(-1) = \\frac{-y_i(1-\\sigma(z_i)) + (1-y_i)\\sigma(z_i)}{\\sigma(z_i)(1-\\sigma(z_i))} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))}\n$$\n将其与S型函数的导数结合：\n$$\n\\frac{\\partial l_i}{\\partial z_i} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))} \\cdot \\sigma(z_i)(1-\\sigma(z_i)) = \\sigma(z_i) - y_i\n$$\n\n有了这个结果，我们就可以求出总经验风险 $R_n$ 的梯度。\n\n**关于 $\\mu$ 的梯度：**\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = \\frac{\\partial L_C}{\\partial \\mu} + \\lambda \\frac{\\partial L_M}{\\partial \\mu}\n$$\n条件部分 $L_C$ 的梯度使用链式法则求得：\n$$\n\\frac{\\partial L_C}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\mu}\n$$\n由于 $z_i = a(x_i - \\mu)$，我们有 $\\frac{\\partial z_i}{\\partial \\mu} = -a$。\n$$\n\\frac{\\partial L_C}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i) \\cdot (-a) = -\\frac{a}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i)\n$$\n边缘部分 $L_M$ 的梯度为：\n$$\n\\frac{\\partial L_M}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right) = \\frac{1}{n} \\sum_{i=1}^n \\left( 0 + \\frac{2(x_i - \\mu) \\cdot (-1)}{2s^2} \\right) = -\\frac{1}{ns^2} \\sum_{i=1}^n (x_i - \\mu)\n$$\n结合两个部分，关于 $\\mu$ 的完整梯度为：\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = -\\frac{a}{n} \\sum_{i=1}^n (\\sigma(a(x_i-\\mu)) - y_i) - \\frac{\\lambda}{ns^2} \\sum_{i=1}^n (x_i - \\mu)\n$$\n这可以写成样本的平均值形式：\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\left[ -a(\\sigma(a(x_i-\\mu)) - y_i) - \\frac{\\lambda}{s^2}(x_i - \\mu) \\right]\n$$\n\n**关于 $a$ 的梯度：**\n$$\n\\frac{\\partial R_n}{\\partial a} = \\frac{\\partial L_C}{\\partial a} + \\lambda \\frac{\\partial L_M}{\\partial a}\n$$\n条件部分 $L_C$ 关于 $a$ 的梯度为：\n$$\n\\frac{\\partial L_C}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial a}\n$$\n由于 $z_i = a(x_i - \\mu)$，我们有 $\\frac{\\partial z_i}{\\partial a} = x_i - \\mu$。\n$$\n\\frac{\\partial L_C}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i) (x_i - \\mu)\n$$\n边缘部分 $L_M$ 不依赖于 $a$，所以 $\\frac{\\partial L_M}{\\partial a} = 0$。\n因此，关于 $a$ 的完整梯度为：\n$$\n\\frac{\\partial R_n}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)(\\sigma(a(x_i - \\mu)) - y_i)\n$$\n\n这些推导出的梯度被用于全批量梯度下降算法，以找到对于每个给定的 $\\lambda$ 值能最小化混合经验风险的参数 $(\\widehat{\\mu}, \\widehat{a})$。该算法通过初始化参数，然后在梯度的相反方向上迭代更新它们来进行。训练后，性能通过一个单独的测试集上的条件负对数损失来衡量。对每个数据集和每个 $\\lambda$ 值重复此过程，以找到能够产生最佳泛化性能（通过最低的测试损失来衡量）的配置。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the best lambda for a hybrid generative-discriminative model.\n    \"\"\"\n\n    def sigmoid(t):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        # Clip the input to avoid overflow in np.exp\n        t = np.clip(t, -500, 500)\n        return 1 / (1 + np.exp(-t))\n\n    def softplus(x):\n        \"\"\"\n        Numerically stable computation of log(1 + exp(x)).\n        \"\"\"\n        return np.where(x > 30, x, np.log(1 + np.exp(x)))\n\n    test_cases = [\n        (\n            np.array([-1.3, -1.0, -0.7, 0.6, 0.9, 1.2]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            np.array([-1.1, -0.9, 0.8, 1.1]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([-0.3, -0.1, 0.1, 0.3, -0.2, 0.2]),\n            np.array([0, 1, 0, 1, 1, 0]),\n            np.array([-0.4, 0.0, 0.4]),\n            np.array([0, 1, 0])\n        ),\n        (\n            np.array([-1.2, -0.8, 0.4, 0.8, 1.0, 1.2, 1.4]),\n            np.array([0, 0, 1, 1, 1, 1, 1]),\n            np.array([-0.9, -0.7, 0.9, 1.1]),\n            np.array([0, 0, 1, 1])\n        )\n    ]\n    \n    s2 = 1.0\n    lambda_grid = [0.0, 0.1, 1.0, 10.0]\n    eta0 = 0.3\n    T = 2000\n    \n    final_lambdas = []\n\n    for train_x, train_y, test_x, test_y in test_cases:\n        min_test_loss = float('inf')\n        best_lambda_for_case = -1.0\n        \n        for lambda_val in lambda_grid:\n            # Initialization\n            mu = np.mean(train_x)\n            a = 1.0\n            \n            # Gradient Descent\n            for k in range(T):\n                eta_k = eta0 / np.sqrt(k + 1)\n                \n                # Calculate gradients\n                z = a * (train_x - mu)\n                sigma_z = sigmoid(z)\n                \n                # Gradient w.r.t. mu\n                grad_mu_term1 = -a * (sigma_z - train_y)\n                grad_mu_term2 = lambda_val * (-(1 / s2) * (train_x - mu))\n                grad_mu = np.mean(grad_mu_term1 + grad_mu_term2)\n\n                # Gradient w.r.t. a\n                grad_a = np.mean((train_x - mu) * (sigma_z - train_y))\n                \n                # Update parameters\n                mu -= eta_k * grad_mu\n                a -= eta_k * grad_a\n                \n                # Clipping\n                mu = np.clip(mu, -5.0, 5.0)\n                a = np.clip(a, -10.0, 10.0)\n            \n            mu_hat, a_hat = mu, a\n            \n            # Calculate test conditional negative log-loss\n            z_test = a_hat * (test_x - mu_hat)\n            \n            # Use numerically stable loss formulation: y*softplus(-z) + (1-y)*softplus(z)\n            loss_per_item = test_y * softplus(-z_test) + (1 - test_y) * softplus(z_test)\n            current_test_loss = np.mean(loss_per_item)\n\n            # Update best lambda based on minimizing test loss\n            # Tie-breaking rule: choose smallest lambda for ties within 1e-6 tolerance.\n            if current_test_loss  min_test_loss - 1e-6:\n                min_test_loss = current_test_loss\n                best_lambda_for_case = lambda_val\n            # If best_lambda_for_case is unassigned, assign it\n            elif best_lambda_for_case == -1.0:\n                 best_lambda_for_case = lambda_val\n\n        final_lambdas.append(best_lambda_for_case)\n        \n    print(f\"[{','.join(map(str, final_lambdas))}]\")\n\nsolve()\n```"
        }
    ]
}