{
    "hands_on_practices": [
        {
            "introduction": "The Central Limit Theorem is a cornerstone of statistics, but the best way to build intuition for its power is to see it in action. This coding exercise  makes the abstract theory concrete by applying it to a modern machine learning problem: evaluating model robustness. By simulating the effect of random adversarial noise, you will empirically calculate an average loss and an accuracy metric, and then use the CLT to construct confidence intervals around your estimates, bridging the gap between statistical theory and computational practice.",
            "id": "3171846",
            "problem": "Consider a fixed vector $w \\in \\mathbb{R}^d$, a scalar margin $m \\in \\mathbb{R}$, and random perturbations $\\delta \\sim \\mathcal{N}(0, \\sigma^2 I_d)$ where $I_d$ is the $d \\times d$ identity matrix. Define the adversarial hinge loss under random perturbations as\n$$\nL_{\\text{adv}}(\\delta) = \\max\\left(0,\\, m - w^\\top \\delta\\right).\n$$\nLet $\\{ \\delta_i \\}_{i=1}^n$ be independent and identically distributed (i.i.d.) samples from $\\mathcal{N}(0, \\sigma^2 I_d)$, and define the averaged adversarial loss as the sample mean\n$$\n\\overline{L}_{\\text{adv}} = \\frac{1}{n} \\sum_{i=1}^{n} L_{\\text{adv}}(\\delta_i).\n$$\nDefine the robust accuracy under random perturbations as the probability that the hinge loss is exactly zero under a random draw of $\\delta$, which is equivalently the probability that the margin is not violated under perturbation:\n$$\np = \\mathbb{P}\\left( L_{\\text{adv}}(\\delta) = 0 \\right) = \\mathbb{P}\\left( w^\\top \\delta \\ge m \\right).\n$$\nLet $Y_i = \\mathbb{I}\\{ L_{\\text{adv}}(\\delta_i) = 0 \\}$ be the indicator of a non-violation for the $i$-th perturbation, and let $\\widehat{p} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ be the sample proportion of non-violations.\n\nYour task is to use the foundational definitions of sample means, variances, and independence, together with the Central Limit Theorem (CLT) for i.i.d. random variables, to construct a program that:\n- Generates the specified test suites of perturbations,\n- Computes $\\overline{L}_{\\text{adv}}$ and a two-sided $95\\%$ confidence interval for $\\overline{L}_{\\text{adv}}$ using the CLT,\n- Computes $\\widehat{p}$ and a two-sided $95\\%$ confidence interval for $p$ using the CLT applied to Bernoulli random variables,\n- Aggregates the results for each test case into a list of six floating-point numbers:\n$$\n\\left[\\overline{L}_{\\text{adv}},\\ \\text{CI}_{\\text{low}}(\\overline{L}_{\\text{adv}}),\\ \\text{CI}_{\\text{high}}(\\overline{L}_{\\text{adv}}),\\ \\widehat{p},\\ \\text{CI}_{\\text{low}}(p),\\ \\text{CI}_{\\text{high}}(p)\\right],\n$$\nwhere $\\text{CI}_{\\text{low}}$ and $\\text{CI}_{\\text{high}}$ are the lower and upper bounds of the confidence interval, respectively.\n\nUse the following foundational bases only:\n- The definition of the sample mean and sample variance,\n- The definition of independence and identical distribution,\n- The statement of the Central Limit Theorem (CLT) for i.i.d. random variables with finite variance.\n\nYou must not use any closed-form probability expressions for normal distributions or any pre-derived formulas beyond what is implied by the CLT and basic sample statistics.\n\nFor numerical reproducibility, use the pseudorandom generator with the specified seeds for each test case.\n\nTest Suite:\n- Case $1$ (general happy path): $d = 5$, $w = [1.0,\\ -0.5,\\ 0.3,\\ 0.8,\\ -0.2]$, $m = 0.4$, $\\sigma = 0.5$, $n = 5000$, seed $= 12345$.\n- Case $2$ (boundary, low robust accuracy with small sample): $d = 10$, $w = [0.3,\\ -0.1,\\ 0.2,\\ -0.4,\\ 0.5,\\ -0.7,\\ 0.9,\\ -0.2,\\ 0.1,\\ -0.3]$, $m = 0.6$, $\\sigma = 0.1$, $n = 30$, seed $= 2025$.\n- Case $3$ (boundary, high robust accuracy with small sample): $d = 10$, $w = [0.3,\\ -0.1,\\ 0.2,\\ -0.4,\\ 0.5,\\ -0.7,\\ 0.9,\\ -0.2,\\ 0.1,\\ -0.3]$, $m = -0.6$, $\\sigma = 0.1$, $n = 30$, seed $= 2026$.\n- Case $4$ (high dimension, large sample): $d = 50$, $w_j = (-1)^j \\left(0.1 + 0.02 j\\right)$ for $j = 1,2,\\dots,50$, $m = 0.5$, $\\sigma = 0.05$, $n = 10000$, seed $= 9876$.\n\nComputational requirements and output specification:\n- For each case, compute the sample mean $\\overline{L}_{\\text{adv}}$, the unbiased sample variance $S^2$ of $\\{L_{\\text{adv}}(\\delta_i)\\}$, and use the CLT to construct a two-sided $95\\%$ confidence interval:\n$$\n\\overline{L}_{\\text{adv}} \\pm z_{0.975}\\ \\sqrt{\\frac{S^2}{n}},\n$$\nwhere $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution. Use $z_{0.975} \\approx 1.959964$.\n- For robust accuracy, compute the sample proportion $\\widehat{p}$ and its two-sided $95\\%$ CLT interval:\n$$\n\\widehat{p} \\pm z_{0.975}\\ \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}},\n$$\nand then clip the bounds to lie within $[0,1]$.\n- Your program should produce a single line of output containing the results for all four test cases as a comma-separated list of lists enclosed in square brackets. Each floating-point number must be rounded to six decimal places. For example, the overall format should be\n$$\n\\left[\\left[r_{1,1}, r_{1,2}, \\dots, r_{1,6}\\right], \\left[r_{2,1}, \\dots, r_{2,6}\\right], \\left[r_{3,1}, \\dots, r_{3,6}\\right], \\left[r_{4,1}, \\dots, r_{4,6}\\right]\\right],\n$$\nwhere each $r_{i,j}$ is a float rounded to six decimal places.\n\nNo physical units or angles are involved. All numerical quantities must be expressed as decimals. The program must be complete and runnable without any user input, external files, or network access.",
            "solution": "The problem requires the construction of a computational procedure to estimate two quantities and their confidence intervals based on a Monte Carlo simulation. The foundation of this procedure rests on the principles of sampling distributions and the Central Limit Theorem (CLT).\n\nFirst, we formalize the random variables of interest. The perturbation vector $\\delta$ is a $d$-dimensional random variable drawn from a multivariate normal distribution, $\\delta \\sim \\mathcal{N}(0, \\sigma^2 I_d)$, where $I_d$ is the $d \\times d$ identity matrix. The projection of this perturbation onto a fixed vector $w \\in \\mathbb{R}^d$ defines a new one-dimensional random variable, $Z = w^\\top \\delta$. Since $Z$ is a linear combination of independent Gaussian random variables, it is itself a Gaussian random variable. Its mean is $\\mathbb{E}[Z] = \\mathbb{E}[w^\\top \\delta] = w^\\top \\mathbb{E}[\\delta] = w^\\top 0 = 0$. Its variance is $\\text{Var}(Z) = \\text{Var}(w^\\top \\delta) = w^\\top \\text{Cov}(\\delta) w = w^\\top (\\sigma^2 I_d) w = \\sigma^2 w^\\top w = \\sigma^2 \\|w\\|_2^2$. Thus, $Z \\sim \\mathcal{N}(0, \\sigma^2 \\|w\\|_2^2)$.\n\nThe problem defines two key quantities derived from $Z$. First, the adversarial hinge loss, $L_{\\text{adv}}(\\delta) = \\max(0, m - w^\\top \\delta) = \\max(0, m - Z)$. This is a transformation of the random variable $Z$. Second, the robust accuracy, $p = \\mathbb{P}( L_{\\text{adv}}(\\delta) = 0 ) = \\mathbb{P}(m - w^\\top \\delta \\le 0) = \\mathbb{P}(Z \\ge m)$.\n\nThe analytic calculation of the expected loss $\\mathbb{E}[L_{\\text{adv}}]$ or the probability $p$ would require integrating the probability density function of $Z$, an approach prohibited by the problem statement. Instead, we must rely on a Monte Carlo simulation and the Central Limit Theorem. The simulation process involves generating $n$ independent and identically distributed (i.i.d.) samples, $\\{\\delta_i\\}_{i=1}^n$, from the distribution $\\mathcal{N}(0, \\sigma^2 I_d)$. For each sample $\\delta_i$, we compute the corresponding scalar projection $z_i = w^\\top \\delta_i$. This gives us a set of i.i.d. samples $\\{z_i\\}_{i=1}^n$ from the distribution of $Z$. From these samples, we generate two sets of i.i.d. random variables: a set of hinge losses, $\\{L_i\\}_{i=1}^n$, where $L_i = \\max(0, m - z_i)$, and a set of indicator variables, $\\{Y_i\\}_{i=1}^n$, where $Y_i = \\mathbb{I}\\{z_i \\ge m\\}$. Each $Y_i$ is a Bernoulli random variable with success probability $p$. The core of the task is to estimate the population means of $L_{\\text{adv}}$ and $Y$ and to construct confidence intervals for these means using the CLT.\n\nTo estimate the mean adversarial loss and its confidence interval, let $\\mu_L = \\mathbb{E}[L_{\\text{adv}}]$ be the true mean adversarial loss. The sample mean, $\\overline{L}_{\\text{adv}} = \\frac{1}{n} \\sum_{i=1}^{n} L_i$, is an unbiased estimator of $\\mu_L$. The Central Limit Theorem states that for a sufficiently large sample size $n$, the distribution of the sample mean is approximately normal:\n$$\n\\overline{L}_{\\text{adv}} \\approx \\mathcal{N}\\left(\\mu_L, \\frac{\\sigma_L^2}{n}\\right)\n$$\nwhere $\\sigma_L^2 = \\text{Var}(L_{\\text{adv}})$ is the true variance of the loss. Since $\\sigma_L^2$ is unknown, we estimate it using the unbiased sample variance, $S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (L_i - \\overline{L}_{\\text{adv}})^2$. The quantity $\\frac{\\overline{L}_{\\text{adv}} - \\mu_L}{S/\\sqrt{n}}$ approximately follows a standard normal distribution, $\\mathcal{N}(0,1)$. To construct a $95\\%$ confidence interval for $\\mu_L$, we find the value $z_{0.975}$ such that $\\mathbb{P}(-z_{0.975} \\le \\mathcal{N}(0,1) \\le z_{0.975}) = 0.95$. This gives the interval $\\overline{L}_{\\text{adv}} \\pm z_{0.975} \\frac{S}{\\sqrt{n}}$. Using the given value $z_{0.975} \\approx 1.959964$, the confidence interval bounds are $\\text{CI}_{\\text{low}}(\\overline{L}_{\\text{adv}}) = \\overline{L}_{\\text{adv}} - 1.959964 \\cdot \\sqrt{\\frac{S^2}{n}}$ and $\\text{CI}_{\\text{high}}(\\overline{L}_{\\text{adv}}) = \\overline{L}_{\\text{adv}} + 1.959964 \\cdot \\sqrt{\\frac{S^2}{n}}$.\n\nTo estimate the robust accuracy and its confidence interval, we note that $p = \\mathbb{P}(Z \\ge m)$. The variable $Y_i = \\mathbb{I}\\{z_i \\ge m\\}$ is a Bernoulli trial with success probability $p$. The sample proportion, $\\widehat{p} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$, is the sample mean of these Bernoulli variables and serves as an unbiased estimator of $p$. The CLT, in the form of the De Moivre-Laplace Theorem, states that for large $n$, $\\widehat{p} \\approx \\mathcal{N}\\left(p, \\frac{p(1-p)}{n}\\right)$. The standard error of $\\widehat{p}$ is $\\sqrt{\\frac{p(1-p)}{n}}$. Since $p$ is unknown, we substitute its estimate $\\widehat{p}$ to get the estimated standard error, $\\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}$. The standardized quantity $\\frac{\\widehat{p} - p}{\\sqrt{\\widehat{p}(1-\\widehat{p})/n}}$ is approximately $\\mathcal{N}(0,1)$. This leads to the Wald-type $95\\%$ confidence interval for $p$, given by $\\widehat{p} \\pm z_{0.975} \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}$. The initial bounds are $\\text{CI}_{\\text{low}}'(p) = \\widehat{p} - 1.959964 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}$ and $\\text{CI}_{\\text{high}}'(p) = \\widehat{p} + 1.959964 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}$. Since $p$ is a probability, it must be in the range $[0, 1]$. We clip the bounds to ensure they lie within this range: $\\text{CI}_{\\text{low}}(p) = \\max(0, \\text{CI}_{\\text{low}}'(p))$ and $\\text{CI}_{\\text{high}}(p) = \\min(1, \\text{CI}_{\\text{high}}'(p))$. This completes the theoretical framework. The implementation will follow these steps for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not used as per the problem description.\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, performing Monte Carlo\n    simulations to compute adversarial hinge loss and robust accuracy, and\n    calculating their 95% confidence intervals using the Central Limit Theorem.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 5,\n            \"w\": np.array([1.0, -0.5, 0.3, 0.8, -0.2]),\n            \"m\": 0.4,\n            \"sigma\": 0.5,\n            \"n\": 5000,\n            \"seed\": 12345,\n        },\n        {\n            \"d\": 10,\n            \"w\": np.array([0.3, -0.1, 0.2, -0.4, 0.5, -0.7, 0.9, -0.2, 0.1, -0.3]),\n            \"m\": 0.6,\n            \"sigma\": 0.1,\n            \"n\": 30,\n            \"seed\": 2025,\n        },\n        {\n            \"d\": 10,\n            \"w\": np.array([0.3, -0.1, 0.2, -0.4, 0.5, -0.7, 0.9, -0.2, 0.1, -0.3]),\n            \"m\": -0.6,\n            \"sigma\": 0.1,\n            \"n\": 30,\n            \"seed\": 2026,\n        },\n        {\n            \"d\": 50,\n            \"w\": np.array([((-1)**(j + 1)) * (0.1 + 0.02 * (j + 1)) for j in range(50)]),\n            \"m\": 0.5,\n            \"sigma\": 0.05,\n            \"n\": 10000,\n            \"seed\": 9876,\n        },\n    ]\n\n    all_results = []\n    z_crit = 1.959964\n\n    for case in test_cases:\n        d, w, m, sigma, n, seed = (\n            case[\"d\"],\n            case[\"w\"],\n            case[\"m\"],\n            case[\"sigma\"],\n            case[\"n\"],\n            case[\"seed\"],\n        )\n\n        # Initialize the random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate n i.i.d. samples of perturbations\n        # delta_i ~ N(0, sigma^2 * I_d)\n        perturbations = rng.standard_normal(size=(n, d)) * sigma\n\n        # Compute the projections w^T * delta_i for all samples\n        projections = perturbations @ w\n\n        # --- Adversarial Loss Calculation ---\n        \n        # Compute the adversarial hinge loss for each sample\n        # L_adv(delta_i) = max(0, m - w^T * delta_i)\n        losses = np.maximum(0, m - projections)\n\n        # Compute the sample mean of the adversarial loss\n        mean_loss = np.mean(losses)\n\n        # Compute the unbiased sample variance of the loss (ddof=1 for n-1 denominator)\n        if n  1:\n            var_loss = np.var(losses, ddof=1)\n        else:\n            var_loss = 0.0 # Variance is zero for a single sample\n\n        # Compute the standard error of the mean loss\n        se_loss = np.sqrt(var_loss / n) if n > 0 else 0.0\n\n        # Compute the 95% confidence interval for the mean loss\n        half_width_loss = z_crit * se_loss\n        ci_low_loss = mean_loss - half_width_loss\n        ci_high_loss = mean_loss + half_width_loss\n\n        # --- Robust Accuracy Calculation ---\n\n        # Compute the indicator for robust accuracy for each sample\n        # Y_i = I{w^T * delta_i >= m}\n        indicators = (projections >= m).astype(float)\n\n        # Compute the sample proportion (estimator for p)\n        p_hat = np.mean(indicators)\n\n        # Compute the standard error for the proportion\n        # The variance of the estimator p_hat is p(1-p)/n, estimated by p_hat(1-p_hat)/n\n        se_p = np.sqrt(p_hat * (1 - p_hat) / n) if n > 0 else 0.0\n\n        # Compute the 95% confidence interval for p\n        half_width_p = z_crit * se_p\n        ci_low_p = p_hat - half_width_p\n        ci_high_p = p_hat + half_width_p\n\n        # Clip the confidence interval for p to the valid range [0, 1]\n        ci_low_p_clipped = np.clip(ci_low_p, 0.0, 1.0)\n        ci_high_p_clipped = np.clip(ci_high_p, 0.0, 1.0)\n        \n        # Aggregate and round results to six decimal places\n        case_results = [\n            round(mean_loss, 6),\n            round(ci_low_loss, 6),\n            round(ci_high_loss, 6),\n            round(p_hat, 6),\n            round(ci_low_p_clipped, 6),\n            round(ci_high_p_clipped, 6),\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists without spaces\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Having applied the Central Limit Theorem in a simulation, we now turn to its use in analytical derivation. This practice  explores Batch Normalization, a key technique that stabilizes and accelerates the training of deep neural networks. You will use the CLT to formally derive the asymptotic sampling distributions of the running statistics used in a BN layer, providing a theoretical justification for how these components work and how their uncertainty can be precisely quantified.",
            "id": "3171886",
            "problem": "A single hidden layer in a deep network uses Batch Normalization (BN). Let $X$ denote a scalar pre-activation at a fixed channel, modeled as independent and identically distributed draws from a normal distribution $\\mathcal{N}(\\mu, \\sigma^{2})$. BN observes $B$ independent mini-batches, each of size $n$, and for each batch $b \\in \\{1, \\dots, B\\}$ computes the per-batch statistics: the sample mean $\\hat{\\mu}_{b}$ and the unbiased sample variance $\\hat{\\sigma}_{b}^{2}$. Define the averaged BN statistics across batches by\n$$\\bar{\\mu}_{B} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\mu}_{b}, \\qquad \\bar{\\sigma}_{B}^{2} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\sigma}_{b}^{2}.$$\nStarting from the fundamental definitions of the sample mean and sample variance, and invoking only the Central Limit Theorem (CLT), derive the asymptotic sampling distributions of $\\bar{\\mu}_{B}$ and $\\bar{\\sigma}_{B}^{2}$ as $B$ grows, and use these distributions to justify normality-based uncertainty bounds for the running averages. Then, for a BN layer with $n = 128$, $B = 50$, $\\mu = 0.2$, and $\\sigma = 0.9$, suppose we set robust running averages at the expected values of $\\bar{\\mu}_{B}$ and $\\bar{\\sigma}_{B}^{2}$ while attaching symmetric uncertainty bounds at confidence level $0.95$ to each. Define the bound radius $R$ to be the larger half-width among the two $0.95$-coverage normal-based intervals for $\\bar{\\mu}_{B}$ and $\\bar{\\sigma}_{B}^{2}$. Compute $R$ and round your answer to four significant figures. Express the final value as a pure number with no units.",
            "solution": "The problem asks for the derivation of the asymptotic sampling distributions for the averaged Batch Normalization (BN) statistics, $\\bar{\\mu}_{B}$ and $\\bar{\\sigma}_{B}^{2}$, and then to compute a specific uncertainty bound radius, $R$. The validation of the problem statement is the mandatory first step.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The pre-activation $X$ is modeled as independent and identically distributed (i.i.d.) draws from a normal distribution, $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$.\n- There are $B$ independent mini-batches, each of size $n$.\n- For each batch $b \\in \\{1, \\dots, B\\}$, the per-batch statistics are the sample mean $\\hat{\\mu}_{b}$ and the unbiased sample variance $\\hat{\\sigma}_{b}^{2}$.\n- The averaged BN statistics are defined as $\\bar{\\mu}_{B} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\mu}_{b}$ and $\\bar{\\sigma}_{B}^{2} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\sigma}_{b}^{2}$.\n- The derivation of asymptotic sampling distributions must be done by invoking the Central Limit Theorem (CLT).\n- Numerical values for computation: $n = 128$, $B = 50$, $\\mu = 0.2$, $\\sigma = 0.9$.\n- Confidence level for uncertainty bounds is $0.95$.\n- The bound radius $R$ is the larger half-width of the two $0.95$-coverage normal-based intervals for $\\bar{\\mu}_{B}$ and $\\bar{\\sigma}_{B}^{2}$.\n- The final answer must be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It addresses the statistical properties of estimators used in Batch Normalization, a standard technique in deep learning. The problem is based on fundamental principles of mathematical statistics, namely sampling distributions and the Central Limit Theorem. All definitions and parameters are provided, leading to a unique and verifiable solution. The constraint to use the CLT is a standard directive in statistical problems to justify the use of a normal approximation for a sample mean, which is appropriate here. The problem is mathematically and scientifically sound.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n### Solution\n\nThe solution is divided into three parts: deriving the asymptotic sampling distributions, justifying the normality-based bounds, and computing the numerical value of the bound radius $R$.\n\n**1. Asymptotic Sampling Distribution of $\\bar{\\mu}_{B}$**\n\nLet $X_{i,b}$ be the $i$-th pre-activation in the $b$-th mini-batch, for $i \\in \\{1, \\dots, n\\}$ and $b \\in \\{1, \\dots, B\\}$. By assumption, $X_{i,b} \\sim \\mathcal{N}(\\mu, \\sigma^2)$ are i.i.d.\n\nFor a single mini-batch $b$, the sample mean is $\\hat{\\mu}_{b} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i,b}$. Since the sum of independent normal random variables is also normal, $\\hat{\\mu}_{b}$ is exactly normally distributed. Its expectation and variance are:\n$$E[\\hat{\\mu}_{b}] = E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i,b}\\right] = \\frac{1}{n} \\sum_{i=1}^{n} E[X_{i,b}] = \\frac{1}{n} (n\\mu) = \\mu$$\n$$Var(\\hat{\\mu}_{b}) = Var\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i,b}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} Var(X_{i,b}) = \\frac{1}{n^2} (n\\sigma^2) = \\frac{\\sigma^2}{n}$$\nSo, for each batch $b$, the sample mean is a random variable $\\hat{\\mu}_{b} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)$.\n\nThe averaged statistic $\\bar{\\mu}_{B}$ is the sample mean of $B$ i.i.d. random variables $\\hat{\\mu}_{1}, \\dots, \\hat{\\mu}_{B}$. The Central Limit Theorem (CLT) states that for a large number of i.i.d. random variables, their sample mean is approximately normally distributed. Applying the CLT to the sequence $\\{\\hat{\\mu}_b\\}_{b=1}^B$, the asymptotic distribution of $\\bar{\\mu}_{B}$ as $B \\to \\infty$ is:\n$$\\bar{\\mu}_{B} \\approx \\mathcal{N}\\left(E[\\hat{\\mu}_{b}], \\frac{Var(\\hat{\\mu}_{b})}{B}\\right)$$\nSubstituting the moments of $\\hat{\\mu}_{b}$:\n$$\\bar{\\mu}_{B} \\approx \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2/n}{B}\\right) = \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{nB}\\right)$$\n(Note: Since each $\\hat{\\mu}_b$ is already normal, their mean $\\bar{\\mu}_B$ is exactly normal, but the problem requires justification via the CLT, which we have provided.)\n\n**2. Asymptotic Sampling Distribution of $\\bar{\\sigma}_{B}^{2}$**\n\nFor a single mini-batch $b$, the unbiased sample variance is $\\hat{\\sigma}_{b}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_{i,b} - \\hat{\\mu}_{b})^2$.\nFor data from a normal distribution, the quantity $\\frac{(n-1)\\hat{\\sigma}_{b}^{2}}{\\sigma^2}$ follows a chi-squared distribution with $n-1$ degrees of freedom, i.e., $\\frac{(n-1)\\hat{\\sigma}_{b}^{2}}{\\sigma^2} \\sim \\chi_{n-1}^2$.\n\nFrom this, we can find the expectation and variance of $\\hat{\\sigma}_{b}^{2}$. The mean and variance of a $\\chi_k^2$ random variable are $k$ and $2k$, respectively.\n$$E[\\hat{\\sigma}_{b}^{2}] = E\\left[\\frac{\\sigma^2}{n-1} \\chi_{n-1}^2\\right] = \\frac{\\sigma^2}{n-1} E[\\chi_{n-1}^2] = \\frac{\\sigma^2}{n-1} (n-1) = \\sigma^2$$\nThis confirms that $\\hat{\\sigma}_{b}^{2}$ is an unbiased estimator of $\\sigma^2$.\n$$Var(\\hat{\\sigma}_{b}^{2}) = Var\\left(\\frac{\\sigma^2}{n-1} \\chi_{n-1}^2\\right) = \\left(\\frac{\\sigma^2}{n-1}\\right)^2 Var(\\chi_{n-1}^2) = \\frac{\\sigma^4}{(n-1)^2} [2(n-1)] = \\frac{2\\sigma^4}{n-1}$$\nThe averaged statistic $\\bar{\\sigma}_{B}^{2}$ is the sample mean of $B$ i.i.d. random variables $\\hat{\\sigma}_{1}^{2}, \\dots, \\hat{\\sigma}_{B}^{2}$. These variables are not normally distributed; they follow a scaled chi-squared distribution. Therefore, we must invoke the CLT to find the asymptotic distribution of their mean, $\\bar{\\sigma}_{B}^{2}$. As $B \\to \\infty$, the CLT gives:\n$$\\bar{\\sigma}_{B}^{2} \\approx \\mathcal{N}\\left(E[\\hat{\\sigma}_{b}^{2}], \\frac{Var(\\hat{\\sigma}_{b}^{2})}{B}\\right)$$\nSubstituting the moments of $\\hat{\\sigma}_{b}^{2}$:\n$$\\bar{\\sigma}_{B}^{2} \\approx \\mathcal{N}\\left(\\sigma^2, \\frac{2\\sigma^4/(n-1)}{B}\\right) = \\mathcal{N}\\left(\\sigma^2, \\frac{2\\sigma^4}{B(n-1)}\\right)$$\n\n**3. Justification for Normality-Based Uncertainty Bounds**\n\nThe asymptotic normality of both $\\bar{\\mu}_{B}$ and $\\bar{\\sigma}_{B}^{2}$, as established by the CLT, is the justification for using normality-based uncertainty bounds. For a large number of batches $B$, the sampling distributions of these running averages can be well-approximated by normal distributions. A symmetric $(1-\\alpha)$ confidence interval for a normally distributed estimator $\\hat{\\theta}$ with mean $\\theta$ and variance $V$ is given by $\\hat{\\theta} \\pm z_{1-\\alpha/2} \\sqrt{V}$. The problem asks for uncertainty bounds centered at the expected values of the estimators. The half-width of such a bound is $z_{1-\\alpha/2} \\sqrt{\\text{Var}(\\text{estimator})}$.\n\n**4. Computation of the Bound Radius $R$**\n\nWe are given $n = 128$, $B = 50$, $\\mu = 0.2$, and $\\sigma = 0.9$. The confidence level is $0.95$, so $\\alpha = 0.05$. The critical value from the standard normal distribution is $z_{1-\\alpha/2} = z_{0.975} \\approx 1.95996$. We will use the standard approximation $z_{0.975} = 1.96$.\n\nFirst, we compute the half-width of the uncertainty bound for $\\bar{\\mu}_{B}$, which we denote $R_{\\mu}$.\n$$R_{\\mu} = z_{0.975} \\sqrt{Var(\\bar{\\mu}_{B})} = z_{0.975} \\sqrt{\\frac{\\sigma^2}{nB}}$$\nSubstituting the values: $\\sigma^2 = (0.9)^2 = 0.81$.\n$$R_{\\mu} = 1.96 \\sqrt{\\frac{0.81}{128 \\times 50}} = 1.96 \\sqrt{\\frac{0.81}{6400}} = 1.96 \\times \\frac{0.9}{80} = 1.96 \\times 0.01125 = 0.02205$$\n\nNext, we compute the half-width of the uncertainty bound for $\\bar{\\sigma}_{B}^{2}$, which we denote $R_{\\sigma^2}$.\n$$R_{\\sigma^2} = z_{0.975} \\sqrt{Var(\\bar{\\sigma}_{B}^{2})} = z_{0.975} \\sqrt{\\frac{2\\sigma^4}{B(n-1)}}$$\nSubstituting the values: $\\sigma^4 = (0.81)^2 = 0.6561$ and $n-1 = 127$.\n$$R_{\\sigma^2} = 1.96 \\sqrt{\\frac{2 \\times 0.6561}{50 \\times 127}} = 1.96 \\sqrt{\\frac{1.3122}{6350}} \\approx 1.96 \\sqrt{0.00020664567}$$\n$$R_{\\sigma^2} \\approx 1.96 \\times 0.0143751754 \\approx 0.0281753439$$\n\nThe bound radius $R$ is defined as the larger of these two half-widths.\n$$R = \\max(R_{\\mu}, R_{\\sigma^2}) = \\max(0.02205, 0.0281753439) = 0.0281753439$$\nRounding the result to four significant figures gives:\n$$R \\approx 0.02818$$",
            "answer": "$$\\boxed{0.02818}$$"
        },
        {
            "introduction": "This final practice  tackles a critical challenge in the era of big data: computational scalability. When a dataset is too massive to analyze in its entirety, we often resort to subsampling. In this exercise, you will theoretically investigate this practical strategy, deriving the asymptotic distribution of the subsample mean and quantifying the statistical trade-offs, such as bias, that are introduced. This problem demonstrates the rigorous thinking required to adapt classical theorems like the CLT to the constraints of modern large-scale data analysis.",
            "id": "3171767",
            "problem": "A data scientist analyzes a massive independent and identically distributed sample $\\{X_{1},\\dots,X_{n}\\}$ from a real-valued distribution with mean $\\mu$ and variance $\\sigma^{2}\\in(0,\\infty)$. Assume $\\mathbb{E}|X_{1}|^{3}\\infty$. For computational scalability, instead of resampling $n$ observations with replacement (bootstrap), the scientist repeatedly draws subsamples of size $b=b_{n}$ uniformly without replacement from the $n$ observed values, where $b\\to\\infty$ and $b/n\\to 0$ as $n\\to\\infty$. Let $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ be the full-sample mean and let $\\bar{X}_{b}$ denote the mean of a subsample of size $b$ drawn uniformly without replacement from the $n$ observed values.\n\n1) Using only fundamental definitions of expectation and variance together with the classical Central Limit Theorem (CLT), derive the asymptotic distribution of $Z_{b}=\\sqrt{b}\\,(\\bar{X}_{b}-\\mu)$ under the regime $b\\to\\infty$, $b/n\\to 0$. Your derivation must start from the decomposition of $Z_{b}$ into a term measuring without-replacement sampling fluctuation around $\\bar{X}_{n}$ and a term involving $\\bar{X}_{n}-\\mu$, and must justify each limit by appealing to standard limit theorems and moment conditions.\n\n2) Define the finite-population variance across the observed values as $S_{n}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X}_{n})^{2}$. Derive, from first principles (i.e., by computing indicator covariances for simple random sampling without replacement and applying the law of total variance), the conditional mean and conditional variance of the subsampling pivot $T_{b}=\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n})$ given $X_{1},\\dots,X_{n}$. Then compute the exact expected bias of this conditional variance as an estimator of $\\sigma^{2}$, namely\n$$\n\\mathbb{E}\\!\\left[\\operatorname{Var}\\!\\left(T_{b}\\mid X_{1},\\dots,X_{n}\\right)\\right]-\\sigma^{2},\n$$\nexpressed in closed form in terms of $n$, $b$, and $\\sigma^{2}$.\n\nProvide this exact bias as your final answer in a single closed-form expression. No numerical approximation or rounding is required.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. All conditions and definitions are standard in the field of statistical learning and asymptotic theory. The problem is a formal exercise in probability and statistics, specifically regarding the properties of subsampling estimators. I will proceed with the required derivations.\n\nThe problem is divided into two parts. First, we derive the asymptotic distribution of $Z_{b}=\\sqrt{b}\\,(\\bar{X}_{b}-\\mu)$. Second, we compute the exact expected bias of the conditional variance of the subsampling pivot $T_{b}=\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n})$ as an estimator of the population variance $\\sigma^{2}$.\n\n**Part 1: Asymptotic Distribution of $Z_{b}$**\n\nWe are asked to derive the asymptotic distribution of $Z_{b}=\\sqrt{b}\\,(\\bar{X}_{b}-\\mu)$ by decomposing it as follows:\n$$Z_{b} = \\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n}) + \\sqrt{b}\\,(\\bar{X}_{n}-\\mu)$$\nWe will analyze the asymptotic behavior of each term under the regime where $n\\to\\infty$, $b\\to\\infty$, and $b/n\\to 0$.\n\nLet's analyze the second term, $\\sqrt{b}\\,(\\bar{X}_{n}-\\mu)$. We can rewrite this term as:\n$$\\sqrt{b}\\,(\\bar{X}_{n}-\\mu) = \\sqrt{\\frac{b}{n}} \\left( \\sqrt{n}\\,(\\bar{X}_{n}-\\mu) \\right)$$\nThe sample $\\{X_{1},\\dots,X_{n}\\}$ consists of independent and identically distributed (i.i.d.) random variables with mean $\\mathbb{E}[X_{1}]=\\mu$ and variance $\\operatorname{Var}(X_{1})=\\sigma^{2} \\in (0,\\infty)$. The condition $\\mathbb{E}|X_{1}|^{3}\\infty$ implies that the second moment is finite. By the classical Central Limit Theorem (CLT), the term $\\sqrt{n}\\,(\\bar{X}_{n}-\\mu)$ converges in distribution to a normal distribution with mean $0$ and variance $\\sigma^{2}$:\n$$\\sqrt{n}\\,(\\bar{X}_{n}-\\mu) \\xrightarrow{d} N(0, \\sigma^{2}) \\text{ as } n\\to\\infty$$\nThe notation $\\xrightarrow{d}$ denotes convergence in distribution. A sequence of random variables that converges in distribution is stochastically bounded (or tight).\nThe problem specifies that $b/n \\to 0$ as $n \\to \\infty$. Consequently, the pre-factor $\\sqrt{b/n}$ converges to $0$:\n$$\\sqrt{\\frac{b}{n}} \\to 0 \\text{ as } n\\to\\infty$$\nBy Slutsky's theorem, the product of a sequence of random variables converging in distribution and a sequence of constants converging to $0$ converges in probability to $0$. Let $A_{n} = \\sqrt{n}\\,(\\bar{X}_{n}-\\mu)$ and $c_{n} = \\sqrt{b/n}$. Since $A_{n} \\xrightarrow{d} A \\sim N(0, \\sigma^{2})$ and $c_{n} \\to 0$, we have $c_{n}A_{n} \\xrightarrow{d} 0 \\cdot A = 0$. Convergence in distribution to a constant is equivalent to convergence in probability. Thus,\n$$\\sqrt{b}\\,(\\bar{X}_{n}-\\mu) \\xrightarrow{p} 0 \\text{ as } n\\to\\infty$$\nwhere $\\xrightarrow{p}$ denotes convergence in probability.\n\nNow, let's analyze the first term, $\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n})$. This term represents the fluctuation of the subsample mean around the full-sample mean, arising from the without-replacement sampling process. We analyze its behavior conditional on the observed sample $\\mathcal{X}_{n} = \\{X_{1},\\dots,X_{n}\\}$. Conditionally, we are sampling from a fixed finite population of size $n$ with values $\\{X_{1},\\dots,X_{n}\\}$. The mean of this population is $\\bar{X}_{n}$ and its variance is $S_{n}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X}_{n})^{2}$.\n\nThe subsample mean $\\bar{X}_{b}$ has a conditional variance given by the standard formula for simple random sampling without replacement:\n$$\\operatorname{Var}(\\bar{X}_{b} \\mid \\mathcal{X}_{n}) = \\frac{\\sigma_{pop}^{2}}{b} \\left(1 - \\frac{b}{n}\\right)$$\nwhere $\\sigma_{pop}^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X}_{n})^{2}$ is the (unbiased) population variance. In terms of the given $S_{n}^{2}$, we have $\\sigma_{pop}^{2} = \\frac{n}{n-1}S_{n}^{2}$. Substituting this gives:\n$$\\operatorname{Var}(\\bar{X}_{b} \\mid \\mathcal{X}_{n}) = \\frac{1}{b}\\frac{n}{n-1}S_{n}^{2} \\left(\\frac{n-b}{n}\\right) = \\frac{S_{n}^{2}}{b}\\frac{n-b}{n-1}$$\nThe variance of the term $\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n})$ conditional on $\\mathcal{X}_{n}$ is:\n$$\\operatorname{Var}\\left(\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n}) \\mid \\mathcal{X}_{n}\\right) = b\\,\\operatorname{Var}(\\bar{X}_{b} \\mid \\mathcal{X}_{n}) = S_{n}^{2}\\frac{n-b}{n-1}$$\nBy the Law of Large Numbers, $\\bar{X}_{n} \\xrightarrow{p} \\mu$ and $\\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2} \\xrightarrow{p} \\mathbb{E}[X_{1}^{2}] = \\sigma^{2}+\\mu^{2}$. By the continuous mapping theorem, $S_{n}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2} - \\bar{X}_{n}^{2} \\xrightarrow{p} (\\sigma^{2}+\\mu^{2}) - \\mu^{2} = \\sigma^{2}$. The factor $\\frac{n-b}{n-1} = \\frac{1-b/n}{1-1/n} \\to 1$ since $b/n \\to 0$. Therefore, the conditional variance converges in probability to $\\sigma^{2}$:\n$$\\operatorname{Var}\\left(\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n}) \\mid \\mathcal{X}_{n}\\right) \\xrightarrow{p} \\sigma^{2}$$\nA central limit theorem for sampling without replacement from a sequence of finite populations (e.g., by HÃ¡jek) states that under the conditions $b\\to\\infty$, $n-b\\to\\infty$ (which is implied by $b/n\\to 0$), and a Lindeberg-type condition on the finite population values, the normalized sample mean is asymptotically normal. The condition $\\mathbb{E}|X_{1}|^{3}\\infty$ ensures the sample behaves sufficiently well for this CLT to hold in probability. Thus, conditional on $\\mathcal{X}_{n}$, we have $\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n}) \\xrightarrow{d} N(0, \\sigma^{2})$. Since the limiting distribution does not depend on the specific realization of $\\mathcal{X}_{n}$, this convergence also holds unconditionally:\n$$\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n}) \\xrightarrow{d} N(0, \\sigma^{2})$$\nUsing Slutsky's theorem on the decomposition of $Z_{b}$, we combine the two results. Let $A_{n} = \\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n})$ and $B_{n} = \\sqrt{b}\\,(\\bar{X}_{n}-\\mu)$. We have $A_{n} \\xrightarrow{d} N(0, \\sigma^{2})$ and $B_{n} \\xrightarrow{p} 0$. Therefore, their sum converges in distribution to $N(0, \\sigma^{2})$:\n$$Z_{b} = A_{n} + B_{n} \\xrightarrow{d} N(0, \\sigma^{2})$$\n\n**Part 2: Conditional Moments and Expected Bias**\n\nWe are asked to derive the conditional mean and variance of $T_{b}=\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n})$ given $\\mathcal{X}_{n}=\\{X_{1},\\dots,X_{n}\\}$, and then compute an expected bias.\n\nConditional on $\\mathcal{X}_{n}$, the values $\\{X_{1},\\dots,X_{n}\\}$ are fixed constants. Let $I_{i}$ be an indicator variable such that $I_{i}=1$ if $X_{i}$ is in the subsample of size $b$, and $I_{i}=0$ otherwise. The subsample mean is $\\bar{X}_{b}=\\frac{1}{b}\\sum_{i=1}^{n}I_{i}X_{i}$.\n\nThe conditional mean of $T_{b}$ is:\n$$\\mathbb{E}[T_{b} \\mid \\mathcal{X}_{n}] = \\mathbb{E}[\\sqrt{b}\\,(\\bar{X}_{b}-\\bar{X}_{n}) \\mid \\mathcal{X}_{n}] = \\sqrt{b}\\,(\\mathbb{E}[\\bar{X}_{b} \\mid \\mathcal{X}_{n}]-\\bar{X}_{n})$$\nFor uniform sampling without replacement, $\\mathbb{P}(I_{i}=1) = b/n$. The conditional expectation of $\\bar{X}_{b}$ is:\n$$\\mathbb{E}[\\bar{X}_{b} \\mid \\mathcal{X}_{n}] = \\mathbb{E}\\left[\\frac{1}{b}\\sum_{i=1}^{n}I_{i}X_{i} \\mid \\mathcal{X}_{n}\\right] = \\frac{1}{b}\\sum_{i=1}^{n}X_{i}\\mathbb{E}[I_{i}] = \\frac{1}{b}\\sum_{i=1}^{n}X_{i}\\left(\\frac{b}{n}\\right) = \\frac{1}{n}\\sum_{i=1}^{n}X_{i} = \\bar{X}_{n}$$\nThus, the conditional mean is $\\mathbb{E}[T_{b} \\mid \\mathcal{X}_{n}] = \\sqrt{b}\\,(\\bar{X}_{n}-\\bar{X}_{n})=0$.\n\nThe conditional variance of $T_{b}$ is $\\operatorname{Var}(T_{b} \\mid \\mathcal{X}_{n}) = b\\,\\operatorname{Var}(\\bar{X}_{b} \\mid \\mathcal{X}_{n})$. We compute this from first principles using indicator covariances.\n$$\\operatorname{Var}(\\bar{X}_{b} \\mid \\mathcal{X}_{n}) = \\operatorname{Var}\\left(\\frac{1}{b}\\sum_{i=1}^{n}I_{i}X_{i}\\right) = \\frac{1}{b^{2}}\\operatorname{Var}\\left(\\sum_{i=1}^{n}I_{i}X_{i}\\right) = \\frac{1}{b^{2}}\\left(\\sum_{i=1}^{n}X_{i}^{2}\\operatorname{Var}(I_{i}) + \\sum_{i\\neq j}X_{i}X_{j}\\operatorname{Cov}(I_{i},I_{j})\\right)$$\nThe variance of the indicator $I_{i}$ is $\\operatorname{Var}(I_{i}) = \\frac{b}{n}(1-\\frac{b}{n})=\\frac{b(n-b)}{n^{2}}$.\nFor $i\\neq j$, the covariance is $\\operatorname{Cov}(I_{i},I_{j}) = \\mathbb{E}[I_{i}I_{j}] - \\mathbb{E}[I_{i}]\\mathbb{E}[I_{j}]$.\n$\\mathbb{E}[I_{i}I_{j}] = \\mathbb{P}(I_{i}=1, I_{j}=1) = \\mathbb{P}(I_{j}=1|I_{i}=1)\\mathbb{P}(I_{i}=1) = \\frac{b-1}{n-1}\\frac{b}{n}$.\n$$\\operatorname{Cov}(I_{i},I_{j}) = \\frac{b(b-1)}{n(n-1)} - \\left(\\frac{b}{n}\\right)^{2} = \\frac{b}{n}\\left(\\frac{b-1}{n-1}-\\frac{b}{n}\\right) = \\frac{b}{n}\\frac{n(b-1)-b(n-1)}{n(n-1)} = -\\frac{b(n-b)}{n^{2}(n-1)}$$\nSubstituting these into the variance expression:\n$$\\operatorname{Var}\\left(\\sum I_{i}X_{i}\\right) = \\frac{b(n-b)}{n^{2}}\\sum_{i=1}^{n}X_{i}^{2} - \\frac{b(n-b)}{n^{2}(n-1)}\\sum_{i\\neq j}X_{i}X_{j}$$\nUsing $\\sum_{i\\neq j}X_{i}X_{j} = (\\sum X_{i})^{2} - \\sum X_{i}^{2} = n^{2}\\bar{X}_{n}^{2} - \\sum X_{i}^{2}$:\n$$\\operatorname{Var}\\left(\\sum I_{i}X_{i}\\right) = \\frac{b(n-b)}{n^{2}(n-1)}\\left((n-1)\\sum X_{i}^{2} - (n^{2}\\bar{X}_{n}^{2} - \\sum X_{i}^{2})\\right)$$\n$$= \\frac{b(n-b)}{n^{2}(n-1)}\\left(n\\sum X_{i}^{2} - n^{2}\\bar{X}_{n}^{2}\\right) = \\frac{b n(n-b)}{n^{2}(n-1)}\\left(\\sum X_{i}^{2} - n\\bar{X}_{n}^{2}\\right)$$\nSince $\\sum(X_{i}-\\bar{X}_{n})^{2} = \\sum X_{i}^{2} - n\\bar{X}_{n}^{2} = n S_{n}^{2}$:\n$$\\operatorname{Var}\\left(\\sum I_{i}X_{i}\\right) = \\frac{b(n-b)}{n(n-1)} (n S_{n}^{2}) = \\frac{b(n-b)}{n-1} S_{n}^{2}$$\nThen, $\\operatorname{Var}(\\bar{X}_{b} \\mid \\mathcal{X}_{n}) = \\frac{1}{b^{2}}\\frac{b(n-b)}{n-1}S_{n}^{2} = \\frac{n-b}{b(n-1)}S_{n}^{2}$.\nFinally, the conditional variance of $T_{b}$ is:\n$$\\operatorname{Var}(T_{b} \\mid \\mathcal{X}_{n}) = b\\,\\operatorname{Var}(\\bar{X}_{b} \\mid \\mathcal{X}_{n}) = \\frac{n-b}{n-1}S_{n}^{2}$$\nNow, we compute the expected bias of this quantity as an estimator for $\\sigma^{2}$:\n$$\\text{Bias} = \\mathbb{E}\\left[\\operatorname{Var}(T_{b} \\mid \\mathcal{X}_{n})\\right] - \\sigma^{2} = \\mathbb{E}\\left[\\frac{n-b}{n-1}S_{n}^{2}\\right] - \\sigma^{2} = \\frac{n-b}{n-1}\\mathbb{E}[S_{n}^{2}] - \\sigma^{2}$$\nWe need to find the expectation of $S_{n}^{2}$.\n$$S_{n}^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X}_{n})^{2} = \\frac{1}{n}\\sum_{i=1}^{n}X_{i}^{2} - \\bar{X}_{n}^{2}$$\nTaking the expectation:\n$$\\mathbb{E}[S_{n}^{2}] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}^{2}] - \\mathbb{E}[\\bar{X}_{n}^{2}]$$\nFor each $i$, $\\mathbb{E}[X_{i}^{2}] = \\operatorname{Var}(X_{i}) + (\\mathbb{E}[X_{i}])^{2} = \\sigma^{2}+\\mu^{2}$.\nFor the sample mean $\\bar{X}_{n}$, $\\mathbb{E}[\\bar{X}_{n}]=\\mu$ and $\\operatorname{Var}(\\bar{X}_{n})=\\sigma^{2}/n$. So $\\mathbb{E}[\\bar{X}_{n}^{2}] = \\operatorname{Var}(\\bar{X}_{n}) + (\\mathbb{E}[\\bar{X}_{n}])^{2} = \\frac{\\sigma^{2}}{n}+\\mu^{2}$.\nSubstituting these in:\n$$\\mathbb{E}[S_{n}^{2}] = \\frac{1}{n}\\sum_{i=1}^{n}(\\sigma^{2}+\\mu^{2}) - \\left(\\frac{\\sigma^{2}}{n}+\\mu^{2}\\right) = (\\sigma^{2}+\\mu^{2}) - \\frac{\\sigma^{2}}{n} - \\mu^{2} = \\sigma^{2} - \\frac{\\sigma^{2}}{n} = \\frac{n-1}{n}\\sigma^{2}$$\nNow we substitute this into the bias formula:\n$$\\text{Bias} = \\frac{n-b}{n-1}\\left(\\frac{n-1}{n}\\sigma^{2}\\right) - \\sigma^{2} = \\frac{n-b}{n}\\sigma^{2} - \\sigma^{2}$$\n$$= \\left(\\frac{n-b}{n}-1\\right)\\sigma^{2} = \\left(\\frac{n-b-n}{n}\\right)\\sigma^{2} = -\\frac{b}{n}\\sigma^{2}$$\nThis is the exact expected bias of the conditional variance as an estimator of $\\sigma^{2}$.",
            "answer": "$$\\boxed{-\\frac{b}{n}\\sigma^{2}}$$"
        }
    ]
}