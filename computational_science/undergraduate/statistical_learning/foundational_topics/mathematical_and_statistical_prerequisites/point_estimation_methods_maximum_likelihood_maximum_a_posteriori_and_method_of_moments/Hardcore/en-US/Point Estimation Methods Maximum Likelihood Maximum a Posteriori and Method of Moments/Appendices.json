{
    "hands_on_practices": [
        {
            "introduction": "This first exercise serves as a foundational warm-up, focusing on the estimation of a single parameter, the variance $\\sigma^2$, in the familiar Normal distribution. By deriving the Maximum Likelihood (MLE), Method of Moments (MoM), and Maximum a Posteriori (MAP) estimators, you will gain hands-on experience with each technique in a controlled setting. This practice highlights the direct application of core principles and allows for a clear comparison of the resulting estimators .",
            "id": "3157673",
            "problem": "Consider independent and identically distributed observations $X_{1},\\dots,X_{n}$ from a Normal (Gaussian) distribution with known mean $\\mu$ and unknown variance $\\sigma^{2}$, that is $X_{i}\\sim \\mathcal{N}(\\mu,\\sigma^{2})$ for $i=1,\\dots,n$. Define the centered sum of squares $Q=\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$ and the empirical second central moment $S^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}=\\frac{Q}{n}$.\n\nUsing only fundamental principles of likelihood and moments, do the following:\n\n1) Using the definition of the likelihood for the Normal model and the rule for maximizing a differentiable function via its derivative, derive the maximum likelihood estimator for $\\sigma^{2}$. Using the definition of the method of moments, equate the sample second central moment to its population counterpart and derive the method-of-moments estimator for $\\sigma^{2}$ based on $S^{2}$. State the relationship between these two estimators.\n\n2) Place a prior on $\\sigma^{2}$ given by an Inverse-Gamma distribution with shape $\\alpha0$ and scale $\\beta0$, whose density is $p(\\sigma^{2})=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}(\\sigma^{2})^{-(\\alpha+1)}\\exp(-\\beta/\\sigma^{2})$ for $\\sigma^{2}0$, where $\\Gamma(\\cdot)$ is the Gamma function. Using Bayes’ rule and calculus on the log-posterior, derive the maximum a posteriori estimator (posterior mode) of $\\sigma^{2}$ as a function of $Q$, $\\alpha$, $\\beta$, and $n$.\n\n3) Treating the data as random under the true variance $\\sigma^{2}$, use the distributional property of $Q$ for Normal data with known mean to compute the bias of the maximum a posteriori estimator relative to the true parameter, defined as $\\mathbb{E}_{\\sigma^{2}}[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}]-\\sigma^{2}$, in closed form as a symbolic expression in $\\alpha$, $\\beta$, $n$, and $\\sigma^{2}$.\n\nProvide only the final symbolic expression for the bias as your final answer. No numerical rounding is required. The final answer must be a single analytic expression.",
            "solution": "The problem statement is scientifically sound, self-contained, and well-posed, setting forth a standard problem in statistical inference that can be solved using established principles. We proceed with the derivation.\n\nLet the observations be $X_{1}, \\dots, X_{n}$, which are independent and identically distributed (i.i.d.) from a Normal distribution $\\mathcal{N}(\\mu, \\sigma^{2})$ where the mean $\\mu$ is known and the variance $\\sigma^{2}$ is unknown.\n\n**1) Maximum Likelihood and Method-of-Moments Estimators**\n\nFirst, we derive the Maximum Likelihood Estimator (MLE) for $\\sigma^{2}$. The probability density function for a single observation $X_{i}$ is:\n$$f(x_{i} | \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right)$$\nSince the observations are i.i.d., the likelihood function for the entire sample $\\mathbf{x} = (x_{1}, \\dots, x_{n})$ is the product of the individual densities:\n$$L(\\sigma^{2} | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_{i} | \\sigma^{2}) = \\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}\\right)$$\nUsing the definition $Q = \\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$, the likelihood can be written as:\n$$L(\\sigma^{2} | \\mathbf{x}) = (2\\pi\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{Q}{2\\sigma^{2}}\\right)$$\nTo find the maximum, we work with the log-likelihood function, $\\ell(\\sigma^{2} | \\mathbf{x}) = \\ln L(\\sigma^{2} | \\mathbf{x})$:\n$$\\ell(\\sigma^{2} | \\mathbf{x}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{Q}{2\\sigma^{2}}$$\nWe differentiate $\\ell$ with respect to $\\sigma^{2}$ and set the result to zero. Let $\\theta = \\sigma^{2}$ for notational convenience.\n$$\\frac{\\partial\\ell}{\\partial\\theta} = -\\frac{n}{2\\theta} + \\frac{Q}{2\\theta^{2}}$$\nSetting the derivative to zero to find the critical point:\n$$-\\frac{n}{2\\hat{\\theta}} + \\frac{Q}{2\\hat{\\theta}^{2}} = 0 \\implies \\frac{Q}{2\\hat{\\theta}^{2}} = \\frac{n}{2\\hat{\\theta}}$$\nAssuming $\\hat{\\theta} \\neq 0$, we can multiply by $2\\hat{\\theta}^{2}$ to get $Q = n\\hat{\\theta}$. Solving for $\\hat{\\theta}$:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MLE}} = \\hat{\\theta} = \\frac{Q}{n} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$$\nThis is the sample second central moment, $S^{2}$.\n\nNext, we derive the Method-of-Moments (MoM) estimator. The method equates population moments to sample moments. The first relevant population moment is the second central moment, $\\mathbb{E}[(X-\\mu)^{2}]$. By definition, for a random variable $X$, this is its variance.\n$$\\mathbb{E}[(X-\\mu)^{2}] = \\mathrm{Var}(X) = \\sigma^{2}$$\nThe corresponding sample second central moment is given in the problem as $S^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$.\nEquating the population moment to the sample moment gives the MoM estimator for $\\sigma^{2}$:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MoM}} = S^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$$\nComparing the two results, we find that the Maximum Likelihood Estimator and the Method-of-Moments estimator for $\\sigma^{2}$ are identical for this model:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MLE}} = \\widehat{\\sigma^{2}}_{\\mathrm{MoM}} = S^{2}$$\n\n**2) Maximum a Posteriori (MAP) Estimator**\n\nThe MAP estimator maximizes the posterior distribution. By Bayes' rule, the posterior density $p(\\sigma^{2} | \\mathbf{x})$ is proportional to the product of the likelihood and the prior:\n$$p(\\sigma^{2} | \\mathbf{x}) \\propto L(\\sigma^{2} | \\mathbf{x}) p(\\sigma^{2})$$\nThe prior on $\\sigma^{2}$ is an Inverse-Gamma$(\\alpha, \\beta)$ distribution:\n$$p(\\sigma^{2}) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}(\\sigma^{2})^{-(\\alpha+1)}\\exp\\left(-\\frac{\\beta}{\\sigma^{2}}\\right)$$\nWe work with the log-posterior, which is proportional to the sum of the log-likelihood and the log-prior:\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = \\ln L(\\sigma^{2} | \\mathbf{x}) + \\ln p(\\sigma^{2}) + C$$\nwhere $C$ is a constant that does not depend on $\\sigma^{2}$.\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = \\left(-\\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{Q}{2\\sigma^{2}}\\right) + \\left(-(\\alpha+1)\\ln(\\sigma^{2}) - \\frac{\\beta}{\\sigma^{2}}\\right) + C'$$\nCombining terms:\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = -\\left(\\frac{n}{2} + \\alpha + 1\\right)\\ln(\\sigma^{2}) - \\frac{1}{\\sigma^{2}}\\left(\\frac{Q}{2} + \\beta\\right) + C'$$\nTo find the mode of the posterior (the MAP estimator), we differentiate with respect to $\\sigma^{2}$ and set the derivative to zero. Let $\\theta = \\sigma^{2}$:\n$$\\frac{\\partial}{\\partial\\theta}\\ln p(\\theta | \\mathbf{x}) = -\\left(\\frac{n}{2} + \\alpha + 1\\right)\\frac{1}{\\theta} + \\left(\\frac{Q}{2} + \\beta\\right)\\frac{1}{\\theta^{2}}$$\nSetting to zero:\n$$\\left(\\frac{Q}{2} + \\beta\\right)\\frac{1}{\\hat{\\theta}^{2}} = \\left(\\frac{n}{2} + \\alpha + 1\\right)\\frac{1}{\\hat{\\theta}}$$\n$$\\frac{Q + 2\\beta}{2\\hat{\\theta}^{2}} = \\frac{n + 2\\alpha + 2}{2\\hat{\\theta}}$$\nSolving for $\\hat{\\theta}$ gives the MAP estimator:\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MAP}} = \\hat{\\theta} = \\frac{Q + 2\\beta}{n + 2\\alpha + 2}$$\n\n**3) Bias of the MAP Estimator**\n\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$. Here, we need to compute the bias of $\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}$ relative to the true variance $\\sigma^{2}$. The expectation is taken over the sampling distribution of the data, where $Q$ is a random variable.\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] - \\sigma^{2}$$\nFirst, we find the expectation of the estimator:\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\mathbb{E}_{\\sigma^{2}}\\left[\\frac{Q + 2\\beta}{n + 2\\alpha + 2}\\right]$$\nUsing the linearity of expectation, and noting that $n, \\alpha, \\beta$ are constants:\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\frac{\\mathbb{E}_{\\sigma^{2}}[Q] + 2\\beta}{n + 2\\alpha + 2}$$\nTo find $\\mathbb{E}_{\\sigma^{2}}[Q]$, we use the distributional property of $Q$. For $X_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, the standardized variable $Z_{i} = \\frac{X_{i}-\\mu}{\\sigma}$ follows a standard normal distribution, $\\mathcal{N}(0, 1)$. The square of a standard normal variable, $Z_{i}^{2}$, follows a chi-squared distribution with $1$ degree of freedom, $\\chi^{2}_{1}$.\nThe quantity $Q$ is defined as $Q = \\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$. We can write this as:\n$$Q = \\sum_{i=1}^{n} \\left(\\sigma \\frac{X_{i}-\\mu}{\\sigma}\\right)^{2} = \\sigma^{2} \\sum_{i=1}^{n} Z_{i}^{2}$$\nSince the $X_{i}$ are independent, the $Z_{i}$ are also independent. The sum of $n$ independent $\\chi^{2}_{1}$ random variables is a $\\chi^{2}$ random variable with $n$ degrees of freedom. Thus:\n$$\\frac{Q}{\\sigma^{2}} = \\sum_{i=1}^{n} Z_{i}^{2} \\sim \\chi^{2}_{n}$$\nThe expected value of a $\\chi^{2}_{n}$ random variable is its degrees of freedom, $n$.\n$$\\mathbb{E}\\left[\\frac{Q}{\\sigma^{2}}\\right] = n$$\nBy linearity of expectation, $\\frac{1}{\\sigma^{2}}\\mathbb{E}_{\\sigma^{2}}[Q] = n$, which implies:\n$$\\mathbb{E}_{\\sigma^{2}}[Q] = n\\sigma^{2}$$\nSubstituting this back into the expression for the expected value of the MAP estimator:\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\frac{n\\sigma^{2} + 2\\beta}{n + 2\\alpha + 2}$$\nFinally, we compute the bias:\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta}{n + 2\\alpha + 2} - \\sigma^{2}$$\nTo simplify, we put everything over a common denominator:\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta - \\sigma^{2}(n + 2\\alpha + 2)}{n + 2\\alpha + 2}$$\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta - n\\sigma^{2} - (2\\alpha + 2)\\sigma^{2}}{n + 2\\alpha + 2}$$\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{2\\beta - (2\\alpha + 2)\\sigma^{2}}{n + 2\\alpha + 2} = \\frac{2\\beta - 2(\\alpha + 1)\\sigma^{2}}{n + 2(\\alpha + 1)}$$\nThis is the final symbolic expression for the bias of the MAP estimator.",
            "answer": "$$\n\\boxed{\\frac{2\\beta - 2(\\alpha + 1)\\sigma^{2}}{n + 2\\alpha + 2}}\n$$"
        },
        {
            "introduction": "Building on the fundamentals, our second practice tackles a more realistic scenario common in fields like epidemiology: modeling event counts where observation periods, or 'exposures', vary. Using the Poisson distribution, you will not only derive different estimators but also analytically compare their variances . This exercise demonstrates the crucial concept of statistical efficiency and clarifies why one estimation method might be preferred over another in practice.",
            "id": "3157635",
            "problem": "Suppose that for $i=1,\\dots,n$ we observe independent counts $X_i$ with $X_i \\sim \\operatorname{Poisson}(e_i \\mu)$, where the exposures $e_i0$ are known constants and $\\mu0$ is an unknown rate parameter per unit exposure. Work from first principles (definitions of likelihood, Bayes' Rule, and moment-based estimation) to answer the following.\n\n- Using the definition of the joint likelihood for independent Poisson observations, derive the maximum likelihood estimate (MLE) of $\\mu$ expressed in terms of $\\{X_i\\}_{i=1}^{n}$ and $\\{e_i\\}_{i=1}^{n}$.\n- Assume a Gamma prior on $\\mu$ with shape-rate parameterization $\\mu \\sim \\operatorname{Gamma}(a,b)$, with density $p(\\mu)=\\dfrac{b^{a}}{\\Gamma(a)} \\mu^{a-1} \\exp(-b\\mu)$ for $a0$ and $b0$. Using Bayes' Rule and the definition of a mode, derive the maximum a posteriori (MAP) estimate of $\\mu$ in terms of $a$, $b$, $\\{X_i\\}_{i=1}^{n}$, and $\\{e_i\\}_{i=1}^{n}$.\n- Using the method of moments (MoM) and the identifying moment $E[X_i/e_i] = \\mu$, derive a MoM estimator for $\\mu$ based on $\\{X_i\\}_{i=1}^{n}$ and $\\{e_i\\}_{i=1}^{n}$.\n- Analyze and compare the variances of the MLE and the MoM estimator under variable exposures. Derive closed-form expressions for $\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MLE}})$ and $\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MoM}})$ as functions of $\\mu$ and $\\{e_i\\}_{i=1}^{n}$, starting from the variance property of the Poisson distribution and independence of the $X_i$.\n- Now consider a concrete dataset with $n=5$, exposures $(e_i)_{i=1}^{5}=(2.0,\\,0.5,\\,1.5,\\,3.0,\\,0.8)$ and observations $(X_i)_{i=1}^{5}=(3,\\,0,\\,2,\\,7,\\,1)$. For prior parameters $a=2$ and $b=1$, compute the numerical values of the MLE, the MoM, and the MAP estimates of $\\mu$.\n- Finally, using only the exposures in the previous bullet, compute the ratio $R=\\dfrac{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MoM}})}{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MLE}})}$. Report $R$ as a dimensionless real number and round your answer to four significant figures.",
            "solution": "We begin with the fundamental definitions for the Poisson model, likelihood for independent observations, the Gamma prior, and the method of moments.\n\nFor $i=1,\\dots,n$, the model is $X_i \\sim \\operatorname{Poisson}(e_i \\mu)$ with probability mass function\n$$\n\\Pr(X_i=x_i \\mid \\mu) \\;=\\; \\frac{(e_i \\mu)^{x_i} \\exp(-e_i \\mu)}{x_i!}, \\quad x_i \\in \\{0,1,2,\\dots\\}.\n$$\nBy independence, the joint likelihood is\n$$\nL(\\mu; x_1,\\dots,x_n) \\;=\\; \\prod_{i=1}^{n} \\frac{(e_i \\mu)^{x_i} \\exp(-e_i \\mu)}{x_i!}.\n$$\n\nMaximum likelihood estimate (MLE). Consider the log-likelihood,\n$$\n\\ell(\\mu) \\;=\\; \\sum_{i=1}^{n} \\bigl[ x_i \\ln(e_i \\mu) - e_i \\mu - \\ln(x_i!) \\bigr]\n\\;=\\; \\sum_{i=1}^{n} x_i \\ln \\mu + \\sum_{i=1}^{n} x_i \\ln e_i - \\mu \\sum_{i=1}^{n} e_i - \\sum_{i=1}^{n} \\ln(x_i!).\n$$\nDifferentiate with respect to $\\mu$:\n$$\n\\frac{\\partial \\ell(\\mu)}{\\partial \\mu} \\;=\\; \\frac{1}{\\mu} \\sum_{i=1}^{n} x_i \\;-\\; \\sum_{i=1}^{n} e_i.\n$$\nSet the derivative to zero to obtain the critical point:\n$$\n\\frac{1}{\\mu} \\sum_{i=1}^{n} x_i \\;-\\; \\sum_{i=1}^{n} e_i \\;=\\; 0\n\\;\\;\\Longrightarrow\\;\\;\n\\hat{\\mu}_{\\mathrm{MLE}} \\;=\\; \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} e_i}.\n$$\nThe second derivative is $\\frac{\\partial^2 \\ell(\\mu)}{\\partial \\mu^2} = -\\frac{1}{\\mu^2} \\sum_{i=1}^{n} x_i \\le 0$ for $\\mu0$ when at least one $x_i0$, confirming a maximum.\n\nMaximum a posteriori (MAP) estimate. Assume the Gamma prior $\\mu \\sim \\operatorname{Gamma}(a,b)$ with density $p(\\mu)=\\frac{b^{a}}{\\Gamma(a)} \\mu^{a-1} \\exp(-b \\mu)$, for $a0$ and $b0$. The posterior density is proportional to prior times likelihood:\n$$\np(\\mu \\mid x_{1:n}) \\;\\propto\\; \\mu^{a-1} \\exp(-b\\mu) \\prod_{i=1}^{n} (e_i \\mu)^{x_i} \\exp(-e_i \\mu)\n\\;\\propto\\; \\mu^{a-1+\\sum_{i=1}^{n} x_i} \\exp\\!\\bigl(-(b+\\sum_{i=1}^{n} e_i)\\mu\\bigr).\n$$\nThus $\\mu \\mid x_{1:n} \\sim \\operatorname{Gamma}\\!\\bigl(a+\\sum_{i=1}^{n} x_i,\\, b+\\sum_{i=1}^{n} e_i\\bigr)$ in the same shape-rate parameterization. For a Gamma distribution with shape $k1$ and rate $\\theta0$, the mode is $(k-1)/\\theta$. Therefore, provided $a+\\sum_{i=1}^{n} x_i1$, the MAP is\n$$\n\\hat{\\mu}_{\\mathrm{MAP}} \\;=\\; \\frac{a-1+\\sum_{i=1}^{n} x_i}{b+\\sum_{i=1}^{n} e_i}.\n$$\n\nMethod of moments (MoM). From the model, $E[X_i]=e_i \\mu$, so\n$$\nE\\!\\left[\\frac{X_i}{e_i}\\right] \\;=\\; \\mu.\n$$\nUsing the sample analogue of this moment condition, a natural MoM estimator is the sample mean of $\\frac{X_i}{e_i}$:\n$$\n\\hat{\\mu}_{\\mathrm{MoM}} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{e_i}.\n$$\n\nVariance analysis under variable exposures. We use $\\operatorname{Var}(X_i)=e_i \\mu$ for a Poisson random variable with mean $e_i \\mu$, and independence of the $X_i$.\n\n- For the MLE,\n$$\n\\hat{\\mu}_{\\mathrm{MLE}} \\;=\\; \\frac{\\sum_{i=1}^{n} X_i}{\\sum_{i=1}^{n} e_i}.\n$$\nBecause $\\sum_{i=1}^{n} X_i \\sim \\operatorname{Poisson}\\!\\bigl(\\mu \\sum_{i=1}^{n} e_i\\bigr)$ by additivity of independent Poisson variables, we have\n$$\n\\operatorname{Var}\\bigl(\\hat{\\mu}_{\\mathrm{MLE}}\\bigr)\n\\;=\\; \\frac{\\operatorname{Var}(\\sum_{i=1}^{n} X_i)}{\\bigl(\\sum_{i=1}^{n} e_i\\bigr)^2}\n\\;=\\; \\frac{\\mu \\sum_{i=1}^{n} e_i}{\\bigl(\\sum_{i=1}^{n} e_i\\bigr)^2}\n\\;=\\; \\frac{\\mu}{\\sum_{i=1}^{n} e_i}.\n$$\n\n- For the MoM estimator,\n$$\n\\hat{\\mu}_{\\mathrm{MoM}} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\frac{X_i}{e_i}.\n$$\nIndependence yields\n$$\n\\operatorname{Var}\\bigl(\\hat{\\mu}_{\\mathrm{MoM}}\\bigr)\n\\;=\\; \\frac{1}{n^2} \\sum_{i=1}^{n} \\operatorname{Var}\\!\\left(\\frac{X_i}{e_i}\\right)\n\\;=\\; \\frac{1}{n^2} \\sum_{i=1}^{n} \\frac{\\operatorname{Var}(X_i)}{e_i^2}\n\\;=\\; \\frac{1}{n^2} \\sum_{i=1}^{n} \\frac{e_i \\mu}{e_i^2}\n\\;=\\; \\frac{\\mu}{n^2} \\sum_{i=1}^{n} \\frac{1}{e_i}.\n$$\nConsequently, the variance ratio is\n$$\nR \\;\\equiv\\; \\frac{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MoM}})}{\\operatorname{Var}(\\hat{\\mu}_{\\mathrm{MLE}})}\n\\;=\\; \\frac{\\mu \\, \\frac{1}{n^2} \\sum_{i=1}^{n} \\frac{1}{e_i}}{\\mu \\, \\frac{1}{\\sum_{i=1}^{n} e_i}}\n\\;=\\; \\frac{\\bigl(\\sum_{i=1}^{n} e_i\\bigr)\\bigl(\\sum_{i=1}^{n} \\frac{1}{e_i}\\bigr)}{n^2}.\n$$\nBy the Cauchy–Schwarz inequality, $\\bigl(\\sum_{i=1}^{n} e_i\\bigr)\\bigl(\\sum_{i=1}^{n} \\frac{1}{e_i}\\bigr) \\ge n^2$ with equality if and only if all $e_i$ are equal, so $R \\ge 1$ and the MLE is at least as efficient in this sense, with strict inequality when the $e_i$ vary.\n\nNumerical computations for the given data. Let $n=5$, exposures $(e_i)=(2.0,\\,0.5,\\,1.5,\\,3.0,\\,0.8)$, and observations $(X_i)=(3,\\,0,\\,2,\\,7,\\,1)$. Compute the following:\n\n- MLE:\n$$\n\\sum_{i=1}^{5} X_i \\;=\\; 3+0+2+7+1 \\;=\\; 13, \n\\quad \\sum_{i=1}^{5} e_i \\;=\\; 2.0+0.5+1.5+3.0+0.8 \\;=\\; 7.8,\n$$\nso\n$$\n\\hat{\\mu}_{\\mathrm{MLE}} \\;=\\; \\frac{13}{7.8} \\;=\\; \\frac{65}{39} \\;\\approx\\; 1.666\\overline{6}.\n$$\n\n- MoM:\n$$\n\\frac{1}{5}\\sum_{i=1}^{5} \\frac{X_i}{e_i} \\;=\\; \\frac{1}{5}\\left(\\frac{3}{2.0} + \\frac{0}{0.5} + \\frac{2}{1.5} + \\frac{7}{3.0} + \\frac{1}{0.8}\\right)\n\\;=\\; \\frac{1}{5}\\left(\\frac{3}{2} + 0 + \\frac{4}{3} + \\frac{7}{3} + \\frac{5}{4}\\right)\n\\;=\\; \\frac{77}{60}\n\\;\\approx\\; 1.283\\overline{3}.\n$$\n\n- MAP with $a=2$ and $b=1$:\n$$\n\\hat{\\mu}_{\\mathrm{MAP}} \\;=\\; \\frac{a-1+\\sum X_i}{b+\\sum e_i}\n\\;=\\; \\frac{2-1+13}{1+7.8}\n\\;=\\; \\frac{14}{8.8}\n\\;=\\; \\frac{35}{22}\n\\;\\approx\\; 1.59\\overline{09}.\n$$\n\nVariance ratio using only exposures. Compute\n$$\n\\sum_{i=1}^{5} e_i \\;=\\; 7.8, \n\\quad \\sum_{i=1}^{5} \\frac{1}{e_i} \\;=\\; \\frac{1}{2.0} + \\frac{1}{0.5} + \\frac{1}{1.5} + \\frac{1}{3.0} + \\frac{1}{0.8}\n\\;=\\; \\frac{1}{2} + 2 + \\frac{2}{3} + \\frac{1}{3} + \\frac{5}{4}\n\\;=\\; \\frac{19}{4}\n\\;=\\; 4.75.\n$$\nHence\n$$\nR \\;=\\; \\frac{\\bigl(\\sum e_i\\bigr)\\bigl(\\sum \\frac{1}{e_i}\\bigr)}{n^2}\n\\;=\\; \\frac{(7.8)(4.75)}{5^2}\n\\;=\\; \\frac{1482}{1000}\n\\;=\\; 1.482.\n$$\nRounded to four significant figures, $R=1.482$.",
            "answer": "$$\\boxed{1.482}$$"
        },
        {
            "introduction": "Our final practice explores the subtleties of working with transformed data, using the lognormal distribution as an important case study. You will contrast estimators derived on the log-scale with those on the original scale and analyze the non-trivial issue of bias that arises from non-linear functions . This advanced problem sharpens your intuition about the consequences of modeling choices and the properties of 'plug-in' estimators.",
            "id": "3157621",
            "problem": "A positive-valued random sample $\\{X_{1},\\dots,X_{n}\\}$ is drawn independently and identically distributed from a lognormal model with parameters $(\\mu,\\sigma^{2})$, meaning that the logarithms $Y_{i}=\\ln X_{i}$ are independently and identically distributed as normal with mean $\\mu$ and variance $\\sigma^{2}$. Work on the log-scale by defining $Y_{i}=\\ln X_{i}$ for $i=1,\\dots,n$, and denote the sample mean $\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$ and the empirical second central moment on the log-scale $S^{2}_{Y}=\\frac{1}{n}\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}$. On the original scale, denote the sample mean $\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ and the empirical second central moment $S^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$.\n\nUsing only foundational facts and definitions from statistical learning and probability theory, complete the following tasks.\n\n1) Derive the maximum likelihood estimators for $(\\mu,\\sigma^{2})$ on the log-scale by maximizing the likelihood of the normal model for $\\{Y_{i}\\}_{i=1}^{n}$.\n\n2) Adopt conjugate priors for the normal model with unknown mean and variance on the log-scale: conditionally, $\\mu\\mid\\sigma^{2}\\sim\\mathcal{N}(\\mu_{0},\\sigma^{2}/\\kappa_{0})$ with known hyperparameters $\\mu_{0}\\in\\mathbb{R}$ and $\\kappa_{0}0$, and marginally $\\sigma^{2}\\sim\\text{Inverse-Gamma}(\\alpha_{0},\\beta_{0})$ with density proportional to $(\\sigma^{2})^{-(\\alpha_{0}+1)}\\exp(-\\beta_{0}/\\sigma^{2})$ for known $\\alpha_{0}0$ and $\\beta_{0}0$. Derive the maximum a posteriori estimators on the log-scale, and express the posterior mode for $\\mu$ in closed form using $(n,\\bar{Y},\\mu_{0},\\kappa_{0})$.\n\n3) Using the method of moments on the original scale, employ the population identities $E[X]=\\exp(\\mu+\\sigma^{2}/2)$ and $\\operatorname{Var}(X)=(\\exp(\\sigma^{2})-1)\\exp(2\\mu+\\sigma^{2})$ and equate them to the empirical moments $(\\bar{X},S^{2})$ to obtain method-of-moments estimators $\\widehat{\\mu}_{\\text{MoM}}$ and $\\widehat{\\sigma}^{2}_{\\text{MoM}}$ as explicit functions of $(\\bar{X},S^{2})$.\n\n4) Briefly discuss (analytically, not numerically) the finite-sample bias properties of the estimators on the log-scale obtained in part $1)$, and comment on the direction of bias one should expect when estimating $E[X]$ by a plug-in transformation of log-scale estimators.\n\nProvide complete derivations. For your final boxed answer, report only the closed-form expression for the maximum a posteriori estimator of $\\mu$ from part $2)$ as a function of $(n,\\bar{Y},\\mu_{0},\\kappa_{0})$. No numerical rounding is required, and no units are involved.",
            "solution": "The problem statement has been validated and found to be self-contained, scientifically grounded in statistical theory, and well-posed. All components—the lognormal model, definitions of estimators (MLE, MAP, MoM), and properties like bias—are standard concepts in statistics. Sufficient information is provided to complete all tasks. Therefore, a complete solution is warranted.\n\nThe solution is presented in four parts as requested by the problem statement.\n\n**1) Maximum Likelihood Estimators (MLE) on the Log-Scale**\n\nLet the sample be $\\{Y_1, \\dots, Y_n\\}$, where $Y_i = \\ln X_i$. The model assumes $Y_i$ are independent and identically distributed (i.i.d.) draws from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (PDF) for a single observation $Y_i$ is\n$$\nf(y_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\nThe likelihood function for the entire sample $\\{Y_1, \\dots, Y_n\\}$ is the product of the individual densities, given their independence:\n$$\nL(\\mu, \\sigma^2 \\mid \\{Y_i\\}) = \\prod_{i=1}^{n} f(Y_i \\mid \\mu, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\mu)^2 \\right)\n$$\nIt is more convenient to work with the log-likelihood function, $\\ell(\\mu, \\sigma^2) = \\ln L(\\mu, \\sigma^2)$:\n$$\n\\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\mu)^2\n$$\nTo find the maximum likelihood estimators (MLEs), we take the partial derivatives of $\\ell$ with respect to $\\mu$ and $\\sigma^2$ and set them to zero.\n\nFirst, with respect to $\\mu$:\n$$\n\\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(Y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\mu)\n$$\nSetting this derivative to zero (assuming $\\sigma^2  0$):\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (Y_i - \\widehat{\\mu}_{\\text{MLE}}) = 0 \\implies \\sum_{i=1}^{n} Y_i - n\\widehat{\\mu}_{\\text{MLE}} = 0 \\implies \\widehat{\\mu}_{\\text{MLE}} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i = \\bar{Y}\n$$\nNext, with respect to $\\sigma^2$:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (Y_i - \\mu)^2\n$$\nSetting this derivative to zero and substituting $\\mu = \\widehat{\\mu}_{\\text{MLE}} = \\bar{Y}$:\n$$\n-\\frac{n}{2\\widehat{\\sigma^2}_{\\text{MLE}}} + \\frac{1}{2(\\widehat{\\sigma^2}_{\\text{MLE}})^2} \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 = 0\n$$\nMultiplying by $2(\\widehat{\\sigma^2}_{\\text{MLE}})^2$ yields:\n$$\n-n\\widehat{\\sigma^2}_{\\text{MLE}} + \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 = 0 \\implies \\widehat{\\sigma^2}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 = S^2_Y\n$$\nThus, the maximum likelihood estimators for $(\\mu, \\sigma^2)$ are the sample mean and the empirical second central moment of the log-transformed data: $(\\widehat{\\mu}_{\\text{MLE}}, \\widehat{\\sigma^2}_{\\text{MLE}}) = (\\bar{Y}, S^2_Y)$.\n\n**2) Maximum a Posteriori (MAP) Estimators on the Log-Scale**\n\nThe posterior distribution is proportional to the product of the likelihood and the prior distribution: $p(\\mu, \\sigma^2 \\mid \\{Y_i\\}) \\propto L(\\{Y_i\\} \\mid \\mu, \\sigma^2) p(\\mu, \\sigma^2)$. The prior is specified as $p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2)$, with:\n$$\np(\\mu \\mid \\sigma^2) \\propto (\\sigma^2)^{-1/2} \\exp\\left( -\\frac{\\kappa_0(\\mu-\\mu_0)^2}{2\\sigma^2} \\right)\n$$\n$$\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_0+1)} \\exp\\left( -\\frac{\\beta_0}{\\sigma^2} \\right)\n$$\nThe log-posterior, up to an additive constant, is $\\ln p(\\mu, \\sigma^2 \\mid \\{Y_i\\}) \\propto \\ell(\\mu, \\sigma^2) + \\ln p(\\mu, \\sigma^2)$.\n$$\n\\ln p(\\mu, \\sigma^2 \\mid \\{Y_i\\}) \\propto -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(Y_i-\\mu)^2 - \\frac{1}{2}\\ln(\\sigma^2) - \\frac{\\kappa_0}{2\\sigma^2}(\\mu-\\mu_0)^2 - (\\alpha_0+1)\\ln(\\sigma^2) - \\frac{\\beta_0}{\\sigma^2}\n$$\nTo find the MAP estimator for $\\mu$, we differentiate the log-posterior with respect to $\\mu$ and set the result to zero. Only terms involving $\\mu$ are relevant:\n$$\n\\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(Y_i-\\mu)^2 - \\frac{\\kappa_0}{2\\sigma^2}(\\mu-\\mu_0)^2 \\right) = 0\n$$\n$$\n\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(Y_i - \\mu) - \\frac{\\kappa_0}{\\sigma^2}(\\mu-\\mu_0) = 0\n$$\nAssuming $\\sigma^2  0$, we can multiply by $\\sigma^2$:\n$$\n\\sum_{i=1}^{n} Y_i - n\\mu - \\kappa_0\\mu + \\kappa_0\\mu_0 = 0\n$$\n$$\nn\\bar{Y} + \\kappa_0\\mu_0 = (n+\\kappa_0)\\mu\n$$\nSolving for $\\mu$ gives the MAP estimator:\n$$\n\\widehat{\\mu}_{\\text{MAP}} = \\frac{n\\bar{Y} + \\kappa_0\\mu_0}{n + \\kappa_0}\n$$\nTo find the MAP estimator for $\\sigma^2$, we differentiate the log-posterior with respect to $\\sigma^2$. Let's collect all terms involving $\\sigma^2$:\n$$\n\\ln p(\\dots \\mid \\sigma^2) \\propto -\\left(\\frac{n+1}{2} + \\alpha_0+1\\right)\\ln(\\sigma^2) - \\frac{1}{\\sigma^2}\\left(\\frac{1}{2}\\sum(Y_i-\\mu)^2 + \\frac{\\kappa_0}{2}(\\mu-\\mu_0)^2 + \\beta_0\\right)\n$$\nLet $\\tau=\\sigma^2$. Differentiating with respect to $\\tau$ and setting to zero:\n$$\n-\\frac{1}{\\tau}\\left(\\frac{n}{2} + \\frac{1}{2} + \\alpha_0+1\\right) + \\frac{1}{\\tau^2}\\left(\\frac{1}{2}\\sum(Y_i-\\mu)^2 + \\frac{\\kappa_0}{2}(\\mu-\\mu_0)^2 + \\beta_0\\right) = 0\n$$\nSubstituting $\\mu = \\widehat{\\mu}_{\\text{MAP}}$ and solving for $\\tau = \\widehat{\\sigma^2}_{\\text{MAP}}$:\n$$\n\\widehat{\\sigma^2}_{\\text{MAP}} = \\frac{\\sum(Y_i-\\widehat{\\mu}_{\\text{MAP}})^2 + \\kappa_0(\\widehat{\\mu}_{\\text{MAP}}-\\mu_0)^2 + 2\\beta_0}{n+2\\alpha_0+3}\n$$\n\n**3) Method of Moments (MoM) Estimators on the Original Scale**\n\nThe method of moments equates population moments to their corresponding sample moments. We are given the population mean and variance of a lognormal random variable $X$:\n$$\nE[X] = \\exp(\\mu + \\sigma^2/2)\n$$\n$$\n\\operatorname{Var}(X) = (\\exp(\\sigma^2) - 1)\\exp(2\\mu + \\sigma^2)\n$$\nThe corresponding empirical moments are the sample mean $\\bar{X} = \\frac{1}{n}\\sum X_i$ and the empirical second central moment $S^2 = \\frac{1}{n}\\sum (X_i - \\bar{X})^2$. Note that we equate $\\operatorname{Var}(X)$ to $S^2$, not the unbiased sample variance.\nThe system of equations is:\n$$\n(1) \\quad \\bar{X} = \\exp(\\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}}/2)\n$$\n$$\n(2) \\quad S^2 = (\\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) - 1)\\exp(2\\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}})\n$$\nFrom equation (1), we can square both sides to get $\\bar{X}^2 = \\exp(2\\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}})$. Substituting this into equation (2):\n$$\nS^2 = (\\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) - 1)\\bar{X}^2\n$$\nSolving for $\\widehat{\\sigma}^2_{\\text{MoM}}$:\n$$\n\\frac{S^2}{\\bar{X}^2} = \\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) - 1\n$$\n$$\n\\exp(\\widehat{\\sigma}^2_{\\text{MoM}}) = 1 + \\frac{S^2}{\\bar{X}^2}\n$$\n$$\n\\widehat{\\sigma}^2_{\\text{MoM}} = \\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right)\n$$\nNow, from equation (1), take the natural logarithm:\n$$\n\\ln(\\bar{X}) = \\widehat{\\mu}_{\\text{MoM}} + \\widehat{\\sigma}^2_{\\text{MoM}}/2\n$$\nSolving for $\\widehat{\\mu}_{\\text{MoM}}$ and substituting the expression for $\\widehat{\\sigma}^2_{\\text{MoM}}$:\n$$\n\\widehat{\\mu}_{\\text{MoM}} = \\ln(\\bar{X}) - \\frac{1}{2}\\widehat{\\sigma}^2_{\\text{MoM}} = \\ln(\\bar{X}) - \\frac{1}{2}\\ln\\left(1 + \\frac{S^2}{\\bar{X}^2}\\right)\n$$\n\n**4) Finite-Sample Bias Properties**\n\nWe analyze the bias of the estimators found in part 1.\nFor $\\widehat{\\mu}_{\\text{MLE}} = \\bar{Y}$:\n$$\nE[\\widehat{\\mu}_{\\text{MLE}}] = E[\\bar{Y}] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} E[Y_i] = \\frac{1}{n}(n\\mu) = \\mu\n$$\nThe bias is $E[\\widehat{\\mu}_{\\text{MLE}}] - \\mu = 0$. Thus, $\\widehat{\\mu}_{\\text{MLE}}$ is an unbiased estimator of $\\mu$.\n\nFor $\\widehat{\\sigma^2}_{\\text{MLE}} = S^2_Y = \\frac{1}{n}\\sum(Y_i - \\bar{Y})^2$:\nIt is a standard result that for a normal sample, the quantity $\\frac{n S^2_Y}{\\sigma^2} = \\frac{\\sum(Y_i - \\bar{Y})^2}{\\sigma^2}$ follows a chi-squared distribution with $n-1$ degrees of freedom, i.e., $\\chi^2_{n-1}$. The expected value of a $\\chi^2_{n-1}$ random variable is $n-1$.\n$$\nE\\left[\\frac{n S^2_Y}{\\sigma^2}\\right] = n-1 \\implies E[S^2_Y] = \\frac{n-1}{n}\\sigma^2\n$$\nThe bias is $E[\\widehat{\\sigma^2}_{\\text{MLE}}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = -\\frac{1}{n}\\sigma^2$. Since $\\sigma^2  0$, the bias is negative. $\\widehat{\\sigma^2}_{\\text{MLE}}$ systematically underestimates the true variance $\\sigma^2$.\n\nNow, consider the plug-in estimator for the mean on the original scale, $\\widehat{E[X]} = \\exp(\\widehat{\\mu}_{\\text{MLE}} + \\widehat{\\sigma^2}_{\\text{MLE}}/2) = \\exp(\\bar{Y} + S_Y^2/2)$.\nFor a sample from a normal distribution, the sample mean $\\bar{Y}$ and sample variance $S^2_Y$ are independent (a consequence of Basu's theorem). Therefore, the expectation of their product of functions is the product of their expectations:\n$$\nE[\\widehat{E[X]}] = E[\\exp(\\bar{Y} + S_Y^2/2)] = E[\\exp(\\bar{Y})] E[\\exp(S_Y^2/2)]\n$$\nThe term $E[\\exp(\\bar{Y})]$ is the moment-generating function (MGF) of $\\bar{Y} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)$ evaluated at $t=1$. The MGF of a $\\mathcal{N}(m, s^2)$ distribution is $M(t)=\\exp(mt + s^2t^2/2)$.\n$$\nE[\\exp(\\bar{Y})] = \\exp\\left(\\mu \\cdot 1 + \\frac{(\\sigma^2/n) \\cdot 1^2}{2}\\right) = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right)\n$$\nThe term $E[\\exp(S_Y^2/2)]$ involves the MGF of a chi-squared distribution. Let $V = \\frac{n S_Y^2}{\\sigma^2} \\sim \\chi^2_{n-1}$. Then $S_Y^2 = \\frac{\\sigma^2}{n}V$.\n$$\nE[\\exp(S_Y^2/2)] = E\\left[\\exp\\left(\\frac{\\sigma^2}{2n}V\\right)\\right]\n$$\nThis is the MGF of $V \\sim \\chi^2_{n-1}$ evaluated at $t = \\frac{\\sigma^2}{2n}$. The MGF of a $\\chi^2_k$ distribution is $M_V(t) = (1-2t)^{-k/2}$.\n$$\nE[\\exp(S_Y^2/2)] = \\left(1 - 2\\frac{\\sigma^2}{2n}\\right)^{-(n-1)/2} = \\left(1 - \\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}\n$$\nCombining these results:\n$$\nE[\\widehat{E[X]}] = \\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right) \\left(1 - \\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}\n$$\nThis expression is not equal to the true mean $E[X] = \\exp(\\mu + \\sigma^2/2)$. The estimator is biased. To determine the direction of bias, we can analyze the ratio $\\frac{E[\\widehat{E[X]}]}{E[X]}$:\n$$\n\\frac{E[\\widehat{E[X]}]}{E[X]} = \\frac{\\exp\\left(\\mu + \\frac{\\sigma^2}{2n}\\right) \\left(1 - \\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}}{\\exp(\\mu + \\sigma^2/2)} = \\exp\\left(-\\frac{\\sigma^2}{2} + \\frac{\\sigma^2}{2n}\\right) \\left(1-\\frac{\\sigma^2}{n}\\right)^{-(n-1)/2}\n$$\n$$\n= \\exp\\left(-\\frac{(n-1)\\sigma^2}{2n}\\right) \\left(1-\\frac{\\sigma^2}{n}\\right)^{-(n-1)/2} = \\left[\\exp\\left(-\\frac{\\sigma^2}{n}\\right)\\left(1-\\frac{\\sigma^2}{n}\\right)^{-1}\\right]^{(n-1)/2}\n$$\nLet $u = \\sigma^2/n$. We analyze the term $h(u) = e^{-u}(1-u)^{-1}$. The Taylor series expansion of $\\ln(h(u))$ around $u=0$ is $\\ln(h(u)) = -u - \\ln(1-u) = -u - \\left(-u - \\frac{u^2}{2} - \\frac{u^3}{3} - \\dots\\right) = \\frac{u^2}{2} + \\frac{u^3}{3} + \\dots$.\nFor $u  0$, we have $\\ln(h(u))  0$, which implies $h(u)  1$.\nSince the base is greater than $1$ and the exponent $(n-1)/2$ is positive (for $n1$), the ratio is greater than $1$.\n$$\nE[\\widehat{E[X]}]  E[X]\n$$\nTherefore, the plug-in estimator for $E[X]$ has a positive bias; it tends to overestimate the true mean. This is a common consequence of applying a convex function (the exponential) to unbiased or nearly unbiased estimators of its arguments, an effect related to Jensen's inequality.",
            "answer": "$$\\boxed{\\frac{n\\bar{Y} + \\kappa_{0}\\mu_{0}}{n + \\kappa_{0}}}$$"
        }
    ]
}