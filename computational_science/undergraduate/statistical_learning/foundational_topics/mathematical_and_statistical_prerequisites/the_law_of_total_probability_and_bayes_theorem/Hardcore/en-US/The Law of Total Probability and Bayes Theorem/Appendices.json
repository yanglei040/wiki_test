{
    "hands_on_practices": [
        {
            "introduction": "Let's begin with a practical scenario involving sequential decision-making, a common problem in fields from medical diagnostics to industrial quality control. This exercise will guide you through using Bayes' theorem to update your belief about a hidden state as new evidence arrives. By incorporating costs and benefits, you will see how to formulate an optimal strategy that minimizes expected costs, a clear demonstration of Bayesian decision theory in action .",
            "id": "3184709",
            "problem": "A binary condition $Y \\in \\{\\text{present}, \\text{absent}\\}$ is to be predicted in a sequential testing protocol that uses two binary tests $T_1$ and $T_2$, each returning either $\\text{pos}$ or $\\text{neg}$. The prior probability of the condition is $P(Y=\\text{present}) = 0.03$, so $P(\\neg Y) = 0.97$. The tests have known operating characteristics:\n- $P(T_1=\\text{pos} \\mid Y=\\text{present}) = 0.93$ and $P(T_1=\\text{pos} \\mid \\neg Y) = 0.08$,\n- $P(T_2=\\text{pos} \\mid Y=\\text{present}) = 0.88$ and $P(T_2=\\text{pos} \\mid \\neg Y) = 0.04$.\n\nAssume that $T_1$ and $T_2$ are conditionally independent given $Y$ and also conditionally independent given $\\neg Y$. The sequential protocol is:\n- administer a first test; if it is $\\text{pos}$, administer the second test; if it is $\\text{neg}$, stop testing,\n- predict $Y=\\text{present}$ if and only if both tests are $\\text{pos}$; otherwise predict $Y=\\text{absent}$.\n\nUtilities are additive and specified as follows:\n- correctly predicting $Y=\\text{present}$ (true positive) yields $+50$ utility units,\n- falsely predicting $Y=\\text{present}$ (false positive) yields $-8$ utility units,\n- failing to predict $Y=\\text{present}$ (false negative) yields $-20$ utility units,\n- correctly predicting $Y=\\text{absent}$ (true negative) yields $0$ utility units,\n- administering $T_1$ costs $7$ utility units and administering $T_2$ costs $9$ utility units.\n\nUsing only the definitions of conditional probability and the law of total probability, compute the posterior probability $P(Y=\\text{present} \\mid T_1=\\text{pos}, T_2=\\text{pos})$. Then, under the stated protocol and utility model, determine which order of testing ($T_1$ first, then $T_2$; or $T_2$ first, then $T_1$) maximizes the expected utility. Encode the optimal order as an integer code: output $1$ if $T_1$ should be administered first, or $2$ if $T_2$ should be administered first.\n\nProvide your final answer as a row matrix containing two entries: the posterior probability $P(Y=\\text{present} \\mid T_1=\\text{pos}, T_2=\\text{pos})$ expressed as an exact fraction, and the integer code for the optimal order. Do not include units in the final answer.",
            "solution": "The problem asks for two quantities: the posterior probability of a condition being present given two positive tests, and the optimal order of administering these tests to maximize expected utility.\n\nFirst, we establish the notation and formalize the given information.\nLet $Y$ be the event that the condition is present, so $\\neg Y$ is the event that the condition is absent.\nLet $T_{1,p}$ and $T_{1,n}$ denote the events that test $T_1$ is positive and negative, respectively.\nLet $T_{2,p}$ and $T_{2,n}$ denote the events that test $T_2$ is positive and negative, respectively.\n\nThe givens are:\n- Prior probability: $P(Y) = 0.03$. This implies $P(\\neg Y) = 1 - 0.03 = 0.97$.\n- Test $T_1$ characteristics: $P(T_{1,p} | Y) = 0.93$ and $P(T_{1,p} | \\neg Y) = 0.08$.\n- Test $T_2$ characteristics: $P(T_{2,p} | Y) = 0.88$ and $P(T_{2,p} | \\neg Y) = 0.04$.\n- The tests $T_1$ and $T_2$ are conditionally independent given the state of the condition ($Y$ or $\\neg Y$).\n- Prediction rule: Predict $Y=\\text{present}$ if and only if both tests are positive. Otherwise, predict $Y=\\text{absent}$.\n- Utilities for prediction outcomes:\n  - True Positive (TP, predict 'present', actual 'present'): $U_{TP} = +50$.\n  - False Positive (FP, predict 'present', actual 'absent'): $U_{FP} = -8$.\n  - False Negative (FN, predict 'absent', actual 'present'): $U_{FN} = -20$.\n  - True Negative (TN, predict 'absent', actual 'absent'): $U_{TN} = 0$.\n- Costs of tests: $C_1 = 7$ for $T_1$, $C_2 = 9$ for $T_2$.\n\n**Part 1: Posterior Probability $P(Y \\mid T_{1,p}, T_{2,p})$**\n\nWe use Bayes' theorem to find the posterior probability:\n$$P(Y \\mid T_{1,p}, T_{2,p}) = \\frac{P(T_{1,p}, T_{2,p} \\mid Y) P(Y)}{P(T_{1,p}, T_{2,p})}$$\nThe denominator is the total probability of observing two positive tests, which can be expanded using the law of total probability:\n$$P(T_{1,p}, T_{2,p}) = P(T_{1,p}, T_{2,p} \\mid Y) P(Y) + P(T_{1,p}, T_{2,p} \\mid \\neg Y) P(\\neg Y)$$\nDue to the conditional independence of $T_1$ and $T_2$, we have:\n$P(T_{1,p}, T_{2,p} \\mid Y) = P(T_{1,p} \\mid Y) P(T_{2,p} \\mid Y)$\n$P(T_{1,p}, T_{2,p} \\mid \\neg Y) = P(T_{1,p} \\mid \\neg Y) P(T_{2,p} \\mid \\neg Y)$\n\nLet's calculate the components of the formula. It is best practice to work with fractions or decimals with sufficient precision to obtain an exact fractional answer.\n$P(Y) = 0.03 = \\frac{3}{100}$\n$P(\\neg Y) = 0.97 = \\frac{97}{100}$\n$P(T_{1,p} \\mid Y) = 0.93 = \\frac{93}{100}$\n$P(T_{1,p} \\mid \\neg Y) = 0.08 = \\frac{8}{100}$\n$P(T_{2,p} \\mid Y) = 0.88 = \\frac{88}{100}$\n$P(T_{2,p} \\mid \\neg Y) = 0.04 = \\frac{4}{100}$\n\nNumerator of Bayes' theorem:\n$$P(T_{1,p}, T_{2,p}, Y) = P(T_{1,p} \\mid Y) P(T_{2,p} \\mid Y) P(Y) = \\left(\\frac{93}{100}\\right) \\left(\\frac{88}{100}\\right) \\left(\\frac{3}{100}\\right) = \\frac{24552}{1000000} = 0.024552$$\nThis term is the joint probability of a true positive prediction.\n\nDenominator contribution from $\\neg Y$:\n$$P(T_{1,p}, T_{2,p}, \\neg Y) = P(T_{1,p} \\mid \\neg Y) P(T_{2,p} \\mid \\neg Y) P(\\neg Y) = \\left(\\frac{8}{100}\\right) \\left(\\frac{4}{100}\\right) \\left(\\frac{97}{100}\\right) = \\frac{3104}{1000000} = 0.003104$$\nThis term is the joint probability of a false positive prediction.\n\nTotal probability in the denominator:\n$$P(T_{1,p}, T_{2,p}) = P(T_{1,p}, T_{2,p}, Y) + P(T_{1,p}, T_{2,p}, \\neg Y) = 0.024552 + 0.003104 = 0.027656 = \\frac{27656}{1000000}$$\nNow we compute the posterior probability:\n$$P(Y \\mid T_{1,p}, T_{2,p}) = \\frac{0.024552}{0.027656} = \\frac{24552}{27656}$$\nTo simplify this fraction, we can find the greatest common divisor. Both numbers are divisible by $8$:\n$$P(Y \\mid T_{1,p}, T_{2,p}) = \\frac{24552 \\div 8}{27656 \\div 8} = \\frac{3069}{3457}$$\nThe prime factorization of the numerator is $3069 = 3^2 \\times 11 \\times 31$. The denominator $3457$ is not divisible by $3$, $11$, or $31$. Thus, the fraction is in its simplest form.\n\n**Part 2: Optimal Testing Order**\n\nThe total expected utility is the expected utility from the prediction outcome minus the expected cost of the tests:\n$$E[U] = E[U_{\\text{prediction}}] - E[C_{\\text{test}}]$$\nThe prediction rule is to predict $Y=\\text{present}$ if and only if both tests are positive, regardless of the order in which they are performed. Let's analyze the components of $E[U_{\\text{prediction}}]$:\n$$E[U_{\\text{prediction}}] = P(\\text{TP})U_{TP} + P(\\text{FP})U_{FP} + P(\\text{FN})U_{FN} + P(\\text{TN})U_{TN}$$\nThe probabilities of these four outcomes depend only on the final joint test results, not the sequence.\n- $P(\\text{TP}) = P(T_{1,p}, T_{2,p}, Y) = 0.024552$\n- $P(\\text{FP}) = P(T_{1,p}, T_{2,p}, \\neg Y) = 0.003104$\n- $P(\\text{FN}) = P(\\text{predict 'absent'}, Y) = P(Y) - P(\\text{TP}) = 0.03 - 0.024552 = 0.005448$\n- $P(\\text{TN}) = P(\\text{predict 'absent'}, \\neg Y) = P(\\neg Y) - P(\\text{FP}) = 0.97 - 0.003104 = 0.966896$\n\nSince these probabilities and the associated utilities ($U_{TP}, U_{FP}, U_{FN}, U_{TN}$) are independent of the testing order, $E[U_{\\text{prediction}}]$ is the same for both protocols. To maximize the total expected utility $E[U]$, we must therefore minimize the expected cost $E[C_{\\text{test}}]$.\n\nLet's calculate the expected cost for the two possible sequences.\n\n**Scenario 1: Administer $T_1$ first.**\nThe cost of $T_1$ is always incurred. The cost of $T_2$ is incurred only if $T_1$ is positive.\n$$E[C | S_1] = C_1 + C_2 \\times P(T_{1,p})$$\nWe need to calculate the marginal probability $P(T_{1,p})$:\n$$P(T_{1,p}) = P(T_{1,p} \\mid Y)P(Y) + P(T_{1,p} \\mid \\neg Y)P(\\neg Y)$$\n$$P(T_{1,p}) = (0.93)(0.03) + (0.08)(0.97) = 0.0279 + 0.0776 = 0.1055$$\nSo, the expected cost for Scenario 1 is:\n$$E[C | S_1] = 7 + 9 \\times 0.1055 = 7 + 0.9495 = 7.9495$$\n\n**Scenario 2: Administer $T_2$ first.**\nThe cost of $T_2$ is always incurred. The cost of $T_1$ is incurred only if $T_2$ is positive.\n$$E[C | S_2] = C_2 + C_1 \\times P(T_{2,p})$$\nWe need to calculate the marginal probability $P(T_{2,p})$:\n$$P(T_{2,p}) = P(T_{2,p} \\mid Y)P(Y) + P(T_{2,p} \\mid \\neg Y)P(\\neg Y)$$\n$$P(T_{2,p}) = (0.88)(0.03) + (0.04)(0.97) = 0.0264 + 0.0388 = 0.0652$$\nSo, the expected cost for Scenario 2 is:\n$$E[C | S_2] = 9 + 7 \\times 0.0652 = 9 + 0.4564 = 9.4564$$\n\nComparing the expected costs:\n$$E[C | S_1] = 7.9495$$\n$$E[C | S_2] = 9.4564$$\nSince $E[C | S_1]  E[C | S_2]$, the expected cost is minimized by administering test $T_1$ first. This sequential protocol maximizes the total expected utility. The integer code for this optimal order is $1$.\n\nThe final answer consists of the posterior probability and the optimal order code.\nPosterior Probability: $\\frac{3069}{3457}$\nOptimal Order Code: $1$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3069}{3457}  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Data can sometimes be deceiving, and aggregated statistics can hide crucial underlying trends. This next problem explores Simpson's paradox, a phenomenon where an association observed in separate groups reverses when the groups are combined. You will use the Law of Total Probability to algebraically construct this paradox, giving you a first-hand look at how a hidden confounding variable can lead to misleading conclusions and why careful, stratified analysis is so important .",
            "id": "3184667",
            "problem": "A binary classification task involves a binary feature $X \\in \\{0,1\\}$, a binary outcome $Y \\in \\{0,1\\}$, and a binary subgroup indicator $Z \\in \\{0,1\\}$. Within each subgroup $Z=z$, the association between $X$ and $Y$ favors $X=1$ in the sense that $P(Y=1 \\mid X=1, Z=z) > P(Y=1 \\mid X=0, Z=z)$ for both $z=0$ and $z=1$. Concretely, the subgroup-conditional success rates are:\n- For $Z=1$: $P(Y=1 \\mid X=1, Z=1) = 0.62$ and $P(Y=1 \\mid X=0, Z=1) = 0.60$.\n- For $Z=0$: $P(Y=1 \\mid X=1, Z=0) = 0.52$ and $P(Y=1 \\mid X=0, Z=0) = 0.50$.\n\nThe subgroup prevalence is $P(Z=1) = 0.50$. The feature propensity within subgroups is $P(X=1 \\mid Z=1) = 0.20$ and $P(X=1 \\mid Z=0) = b$, where $b \\in [0,1]$ is an unknown design parameter under your control.\n\nUsing only the definition of conditional probability, the law of total probability, and Bayes’ theorem, derive $P(Y=1 \\mid X=1)$ and $P(Y=1 \\mid X=0)$ as functions of $b$ by appropriately marginalizing over $Z$. Then, determine the exact threshold value $b^{\\star} \\in [0,1]$ at which the aggregated association between $X$ and $Y$ reverses in the sense that $P(Y=1 \\mid X=1) = P(Y=1 \\mid X=0)$; for $b > b^{\\star}$ the aggregated association exhibits a Simpson’s paradox reversal, i.e., $P(Y=1 \\mid X=1)  P(Y=1 \\mid X=0)$, even though within each subgroup $X=1$ has the higher success rate.\n\nProvide your final answer as the exact closed-form value of $b^{\\star}$. Do not round.",
            "solution": "The problem is first validated to ensure it is self-contained, consistent, and scientifically sound.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions for a binary classification task with a feature $X \\in \\{0,1\\}$, an outcome $Y \\in \\{0,1\\}$, and a subgroup indicator $Z \\in \\{0,1\\}$:\n- Subgroup-conditional success rates for $Z=1$:\n  - $P(Y=1 \\mid X=1, Z=1) = 0.62$\n  - $P(Y=1 \\mid X=0, Z=1) = 0.60$\n- Subgroup-conditional success rates for $Z=0$:\n  - $P(Y=1 \\mid X=1, Z=0) = 0.52$\n  - $P(Y=1 \\mid X=0, Z=0) = 0.50$\n- Subgroup prevalence:\n  - $P(Z=1) = 0.50$, which implies $P(Z=0) = 1 - 0.50 = 0.50$.\n- Feature propensity within subgroups:\n  - $P(X=1 \\mid Z=1) = 0.20$\n  - $P(X=1 \\mid Z=0) = b$, where $b \\in [0,1]$.\n- Objective: Find the threshold value $b^{\\star}$ where $P(Y=1 \\mid X=1) = P(Y=1 \\mid X=0)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in probability theory, specifically concerning the law of total probability, Bayes' theorem, and the statistical phenomenon known as Simpson's paradox.\n- **Scientifically Grounded:** The problem is based on the standard axioms and theorems of probability theory. Simpson's paradox is a well-documented and fundamental concept in statistics.\n- **Well-Posed:** All necessary probabilities are provided to define the system. The objective is to solve for a single parameter $b$ based on a single equation, which is expected to yield a unique solution within the specified domain $b \\in [0,1]$.\n- **Objective:** The problem is stated using precise mathematical notation and objective language, free of any subjective or ambiguous claims.\n- **Consistency:** The provided conditional probabilities $P(Y=1 \\mid X=x, Z=z)$ are all valid probabilities in $[0,1]$. For both subgroups ($z=0$ and $z=1$), the association is positive: $P(Y=1 \\mid X=1, Z=z) > P(Y=1 \\mid X=0, Z=z)$.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be derived.\n\nThe goal is to find the value of $b$, denoted $b^{\\star}$, for which the aggregated success rates are equal: $P(Y=1 \\mid X=1) = P(Y=1 \\mid X=0)$. To do this, we must first express these two conditional probabilities as functions of $b$ by marginalizing over the subgroup variable $Z$.\n\nWe can find $P(Y=1 \\mid X=x)$ using the definition of conditional probability, $P(A \\mid B) = \\frac{P(A, B)}{P(B)}$. Thus, we need to calculate the joint probabilities $P(Y=1, X=x)$ and the marginal probabilities $P(X=x)$.\n\nFirst, we calculate the marginal probability of $X=1$ using the law of total probability, conditioning on $Z$:\n$$P(X=1) = P(X=1 \\mid Z=1)P(Z=1) + P(X=1 \\mid Z=0)P(Z=0)$$\nSubstituting the given values:\n$$P(X=1) = (0.20)(0.50) + (b)(0.50) = 0.10 + 0.50b$$\nThe probability of $X=0$ is the complement:\n$$P(X=0) = 1 - P(X=1) = 1 - (0.10 + 0.50b) = 0.90 - 0.50b$$\n\nNext, we calculate the joint probability $P(Y=1, X=1)$ by marginalizing over $Z$:\n$$P(Y=1, X=1) = P(Y=1, X=1 \\mid Z=1)P(Z=1) + P(Y=1, X=1 \\mid Z=0)P(Z=0)$$\nUsing the chain rule for conditional probability, $P(A,B \\mid C) = P(A \\mid B,C)P(B \\mid C)$:\n$$P(Y=1, X=1) = P(Y=1 \\mid X=1, Z=1)P(X=1 \\mid Z=1)P(Z=1) + P(Y=1 \\mid X=1, Z=0)P(X=1 \\mid Z=0)P(Z=0)$$\nSubstituting the given values:\n$$P(Y=1, X=1) = (0.62)(0.20)(0.50) + (0.52)(b)(0.50) = 0.062 + 0.26b$$\n\nSimilarly, we calculate the joint probability $P(Y=1, X=0)$:\n$$P(Y=1, X=0) = P(Y=1 \\mid X=0, Z=1)P(X=0 \\mid Z=1)P(Z=1) + P(Y=1 \\mid X=0, Z=0)P(X=0 \\mid Z=0)P(Z=0)$$\nFirst, we find the necessary conditional probabilities for $X=0$:\n$P(X=0 \\mid Z=1) = 1 - P(X=1 \\mid Z=1) = 1 - 0.20 = 0.80$\n$P(X=0 \\mid Z=0) = 1 - P(X=1 \\mid Z=0) = 1 - b$\nSubstituting these and the other given values:\n$$P(Y=1, X=0) = (0.60)(0.80)(0.50) + (0.50)(1-b)(0.50)$$\n$$P(Y=1, X=0) = 0.24 + 0.25(1-b) = 0.24 + 0.25 - 0.25b = 0.49 - 0.25b$$\n\nNow we can formulate the aggregated conditional probabilities:\n$$P(Y=1 \\mid X=1) = \\frac{P(Y=1, X=1)}{P(X=1)} = \\frac{0.062 + 0.26b}{0.10 + 0.50b}$$\n$$P(Y=1 \\mid X=0) = \\frac{P(Y=1, X=0)}{P(X=0)} = \\frac{0.49 - 0.25b}{0.90 - 0.50b}$$\n\nWe are looking for the threshold $b^{\\star}$ where these two expressions are equal:\n$$\\frac{0.062 + 0.26b}{0.10 + 0.50b} = \\frac{0.49 - 0.25b}{0.90 - 0.50b}$$\nTo solve for $b$, we cross-multiply:\n$$(0.062 + 0.26b)(0.90 - 0.50b) = (0.49 - 0.25b)(0.10 + 0.50b)$$\nExpanding both sides of the equation:\nLHS: $(0.062)(0.90) - (0.062)(0.50b) + (0.26b)(0.90) - (0.26b)(0.50b) = 0.0558 - 0.031b + 0.234b - 0.13b^2$\n$$0.0558 + 0.203b - 0.13b^2$$\nRHS: $(0.49)(0.10) + (0.49)(0.50b) - (0.25b)(0.10) - (0.25b)(0.50b) = 0.049 + 0.245b - 0.025b - 0.125b^2$\n$$0.049 + 0.22b - 0.125b^2$$\nEquating the two expressions:\n$$0.0558 + 0.203b - 0.13b^2 = 0.049 + 0.22b - 0.125b^2$$\nRearranging the terms to form a standard quadratic equation of the form $ax^2+bx+c=0$:\n$$(-0.13 + 0.125)b^2 + (0.203 - 0.22)b + (0.0558 - 0.049) = 0$$\n$$-0.005b^2 - 0.017b + 0.0068 = 0$$\nTo simplify, we can multiply the entire equation by $-2000$:\n$$10b^2 + 34b - 13.6 = 0$$\nOr, to work with integers, we can multiply the original quadratic by $-10000$:\n$$50b^2 + 170b - 68 = 0$$\nDividing by $2$:\n$$25b^2 + 85b - 34 = 0$$\nWe solve this quadratic equation for $b$ using the quadratic formula $b = \\frac{-B \\pm \\sqrt{B^2 - 4AC}}{2A}$, with $A=25$, $B=85$, and $C=-34$.\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{85^2 - 4(25)(-34)}}{2(25)}$$\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{7225 - 100(-34)}}{50}$$\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{7225 + 3400}}{50}$$\n$$b^{\\star} = \\frac{-85 \\pm \\sqrt{10625}}{50}$$\nTo simplify the square root, we find the prime factors of $10625$: $10625 = 5^4 \\times 17 = 625 \\times 17$.\nTherefore, $\\sqrt{10625} = \\sqrt{625 \\times 17} = 25\\sqrt{17}$.\nSubstituting this back into the solution for $b^{\\star}$:\n$$b^{\\star} = \\frac{-85 \\pm 25\\sqrt{17}}{50}$$\nWe can divide the numerator and the denominator by their greatest common divisor, which is $5$:\n$$b^{\\star} = \\frac{-17 \\pm 5\\sqrt{17}}{10}$$\nThis gives two possible solutions:\n$$b_1 = \\frac{-17 - 5\\sqrt{17}}{10} \\quad \\text{and} \\quad b_2 = \\frac{-17 + 5\\sqrt{17}}{10}$$\nThe problem states that $b \\in [0,1]$.\nThe first solution, $b_1$, is clearly negative, as both terms in the numerator are negative. So, $b_1$ is not a valid solution.\nFor the second solution, $b_2$, we note that $\\sqrt{16}  \\sqrt{17}  \\sqrt{25}$, so $4  \\sqrt{17}  5$.\nThe numerator is approximately $-17 + 5(4.123) = -17 + 20.615 = 3.615$.\nSo, $b_2 \\approx \\frac{3.615}{10} = 0.3615$, which is within the valid range $[0,1]$.\nThus, the exact threshold value is:\n$$b^{\\star} = \\frac{-17 + 5\\sqrt{17}}{10}$$\nThis can be written as:\n$$b^{\\star} = \\frac{5\\sqrt{17} - 17}{10}$$\nFor $b > b^{\\star}$, the aggregated association reverses, yielding Simpson's paradox.",
            "answer": "$$\\boxed{\\frac{5\\sqrt{17} - 17}{10}}$$"
        },
        {
            "introduction": "Many powerful models in statistical learning rely on simplifying assumptions, with the Naive Bayes classifier's conditional independence assumption being a prime example. This exercise demonstrates what happens when that assumption is violated by a hidden, or latent, variable that creates dependencies between features. By calculating the posterior probability both correctly (by marginalizing over the latent variable) and incorrectly (using the naive assumption), you will quantify the precise bias introduced and gain a deeper appreciation for the mechanics of generative models .",
            "id": "3184732",
            "problem": "A binary classification problem is modeled generatively with a binary label $Y \\in \\{0,1\\}$, a binary latent variable $Z \\in \\{0,1\\}$, and two binary observed features $X_1, X_2 \\in \\{0,1\\}$. The joint data-generating process is as follows. The prior probabilities are $P(Y=1)=0.4$ and $P(Y=0)=0.6$. The latent variable depends on the label via $P(Z=1 \\mid Y=1)=0.7$, $P(Z=0 \\mid Y=1)=0.3$, $P(Z=1 \\mid Y=0)=0.2$, and $P(Z=0 \\mid Y=0)=0.8$. The observed features are conditionally independent given the pair $(Y,Z)$, that is, $X_1 \\perp X_2 \\mid (Y,Z)$, with conditional probabilities specified by\n$$P(X_1=1 \\mid Y=1, Z=1)=0.95,\\quad P(X_1=1 \\mid Y=1, Z=0)=0.30,$$\n$$P(X_1=1 \\mid Y=0, Z=1)=0.85,\\quad P(X_1=1 \\mid Y=0, Z=0)=0.10,$$\n$$P(X_2=1 \\mid Y=1, Z=1)=0.90,\\quad P(X_2=1 \\mid Y=1, Z=0)=0.25,$$\n$$P(X_2=1 \\mid Y=0, Z=1)=0.80,\\quad P(X_2=1 \\mid Y=0, Z=0)=0.05.$$\n\nYou observe $X_1=1$ and $X_2=1$. Using only the axioms of probability, the law of total probability, and Bayes' theorem, and starting from these generative assumptions, perform the following:\n\n1. Derive, from first principles, the expression for the posterior $P(Y=1 \\mid X_1=1, X_2=1)$ that correctly accounts for the latent variable $Z$, and compute its exact numerical value.\n\n2. Under the Naive Bayes (NB) assumption that $X_1$ and $X_2$ are conditionally independent given $Y$ alone (ignoring $Z$), derive the corresponding posterior $P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)$ and compute its exact numerical value.\n\n3. Report the multiplicative bias factor defined as $\\dfrac{P(Y=1 \\mid X_1=1, X_2=1)}{P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)}$ as a single exact value.\n\nExpress the final bias factor exactly; do not round.",
            "solution": "The user wants to compute a multiplicative bias factor comparing a posterior probability from a full generative model with one from a simplified Naive Bayes model.\n\nThe problem asks for three quantities:\n1. The posterior probability $P(Y=1 \\mid X_1=1, X_2=1)$ from the full generative model.\n2. The posterior probability $P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)$ under a Naive Bayes assumption.\n3. The multiplicative bias factor, defined as the ratio of the first quantity to the second.\n\nLet the event of interest be $A = \\{X_1=1, X_2=1\\}$.\n\n### Step 1: Extract Givens\nThe verbatim givens are:\n-   Binary label $Y \\in \\{0,1\\}$, latent variable $Z \\in \\{0,1\\}$, features $X_1, X_2 \\in \\{0,1\\}$.\n-   Priors on $Y$: $P(Y=1)=0.4$, $P(Y=0)=0.6$.\n-   Conditional probabilities for $Z$ given $Y$:\n    -   $P(Z=1 \\mid Y=1)=0.7$, $P(Z=0 \\mid Y=1)=0.3$.\n    -   $P(Z=1 \\mid Y=0)=0.2$, $P(Z=0 \\mid Y=0)=0.8$.\n-   Conditional probabilities for $X_1$ given $(Y,Z)$:\n    -   $P(X_1=1 \\mid Y=1, Z=1)=0.95$.\n    -   $P(X_1=1 \\mid Y=1, Z=0)=0.30$.\n    -   $P(X_1=1 \\mid Y=0, Z=1)=0.85$.\n    -   $P(X_1=1 \\mid Y=0, Z=0)=0.10$.\n-   Conditional probabilities for $X_2$ given $(Y,Z)$:\n    -   $P(X_2=1 \\mid Y=1, Z=1)=0.90$.\n    -   $P(X_2=1 \\mid Y=1, Z=0)=0.25$.\n    -   $P(X_2=1 \\mid Y=0, Z=1)=0.80$.\n    -   $P(X_2=1 \\mid Y=0, Z=0)=0.05$.\n-   Conditional Independence: $X_1 \\perp X_2 \\mid (Y,Z)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in probability theory and statistical learning. It describes a standard generative model with a latent variable. All probabilities provided are valid (i.e., in the range $[0, 1]$) and consistent (e.g., $P(Y=1)+P(Y=0)=1$). The problem is well-posed, providing all necessary information to answer the questions posed. The language is objective and precise. The problem is a valid exercise in applying the law of total probability and Bayes' theorem.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\n### Part 1: Derivation of the Correct Posterior $P(Y=1 \\mid X_1=1, X_2=1)$\n\nWe wish to compute $P(Y=1 \\mid A)$. Using Bayes' theorem:\n$$P(Y=1 \\mid A) = \\frac{P(A \\mid Y=1) P(Y=1)}{P(A)}$$\nThe denominator, $P(A)$, is the marginal probability of the evidence, which can be expanded using the law of total probability:\n$$P(A) = P(A \\mid Y=1)P(Y=1) + P(A \\mid Y=0)P(Y=0)$$\nThe core task is to compute the class-conditional likelihoods $P(A \\mid Y=y) = P(X_1=1, X_2=1 \\mid Y=y)$ for $y \\in \\{0, 1\\}$. Since the model includes the latent variable $Z$, we must marginalize it out using the law of total probability:\n$$P(A \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_1=1, X_2=1, Z=z \\mid Y=y)$$\nUsing the chain rule of probability, $P(B,C|D) = P(B|C,D)P(C|D)$, we get:\n$$P(A \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_1=1, X_2=1 \\mid Y=y, Z=z) P(Z=z \\mid Y=y)$$\nThe problem states that $X_1 \\perp X_2 \\mid (Y,Z)$, so $P(X_1=1, X_2=1 \\mid Y=y, Z=z) = P(X_1=1 \\mid Y=y, Z=z) P(X_2=1 \\mid Y=y, Z=z)$.\nSubstituting this into the summation gives the full expression for the likelihood:\n$$P(A \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_1=1 \\mid Y=y, Z=z) P(X_2=1 \\mid Y=y, Z=z) P(Z=z \\mid Y=y)$$\n\nNow, we compute the numerical values.\nFor $Y=1$:\n$$P(A \\mid Y=1) = P(X_1=1|Y=1,Z=1)P(X_2=1|Y=1,Z=1)P(Z=1|Y=1) + P(X_1=1|Y=1,Z=0)P(X_2=1|Y=1,Z=0)P(Z=0|Y=1)$$\n$$P(A \\mid Y=1) = (0.95)(0.90)(0.7) + (0.30)(0.25)(0.3) = 0.5985 + 0.0225 = 0.621$$\n\nFor $Y=0$:\n$$P(A \\mid Y=0) = P(X_1=1|Y=0,Z=1)P(X_2=1|Y=0,Z=1)P(Z=1|Y=0) + P(X_1=1|Y=0,Z=0)P(X_2=1|Y=0,Z=0)P(Z=0|Y=0)$$\n$$P(A \\mid Y=0) = (0.85)(0.80)(0.2) + (0.10)(0.05)(0.8) = 0.136 + 0.004 = 0.140$$\n\nNow we can compute the terms for Bayes' theorem.\nThe joint probability $P(A, Y=1) = P(A \\mid Y=1)P(Y=1) = 0.621 \\times 0.4 = 0.2484$.\nThe joint probability $P(A, Y=0) = P(A \\mid Y=0)P(Y=0) = 0.140 \\times 0.6 = 0.084$.\nThe marginal evidence is $P(A) = P(A, Y=1) + P(A, Y=0) = 0.2484 + 0.084 = 0.3324$.\n\nThe posterior is:\n$$P(Y=1 \\mid A) = \\frac{0.2484}{0.3324} = \\frac{2484}{3324} = \\frac{621}{831} = \\frac{207}{277}$$\n\n### Part 2: Derivation of the Naive Bayes Posterior $P_{\\mathrm{NB}}(Y=1 \\mid X_1=1, X_2=1)$\n\nThe Naive Bayes (NB) model assumes that the features $X_1$ and $X_2$ are conditionally independent given the label $Y$ alone: $X_1 \\perp X_2 \\mid Y$. The posterior is given by:\n$$P_{\\mathrm{NB}}(Y=1 \\mid A) = \\frac{P_{\\mathrm{NB}}(A \\mid Y=1) P(Y=1)}{P_{\\mathrm{NB}}(A)}$$\nUnder the NB assumption, the likelihood is $P_{\\mathrm{NB}}(A \\mid Y=y) = P(X_1=1 \\mid Y=y) P(X_2=1 \\mid Y=y)$.\n\nThe required marginal conditional probabilities $P(X_i=1 \\mid Y=y)$ are not directly given. They must be derived from the full generative model by marginalizing out the latent variable $Z$:\n$$P(X_i=1 \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_i=1 \\mid Y=y, Z=z) P(Z=z \\mid Y=y)$$\n\nWe compute these marginals:\n$$P(X_1=1 \\mid Y=1) = (0.95)(0.7) + (0.30)(0.3) = 0.665 + 0.09 = 0.755$$\n$$P(X_2=1 \\mid Y=1) = (0.90)(0.7) + (0.25)(0.3) = 0.63 + 0.075 = 0.705$$\n$$P(X_1=1 \\mid Y=0) = (0.85)(0.2) + (0.10)(0.8) = 0.17 + 0.08 = 0.25$$\n$$P(X_2=1 \\mid Y=0) = (0.80)(0.2) + (0.05)(0.8) = 0.16 + 0.04 = 0.20$$\n\nNow, we compute the NB likelihoods:\n$$P_{\\mathrm{NB}}(A \\mid Y=1) = P(X_1=1 \\mid Y=1)P(X_2=1 \\mid Y=1) = 0.755 \\times 0.705 = 0.532275$$\n$$P_{\\mathrm{NB}}(A \\mid Y=0) = P(X_1=1 \\mid Y=0)P(X_2=1 \\mid Y=0) = 0.25 \\times 0.20 = 0.05$$\n\nNext, we compute the terms for the NB version of Bayes' theorem.\nThe joint probability $P_{\\mathrm{NB}}(A, Y=1) = P_{\\mathrm{NB}}(A \\mid Y=1)P(Y=1) = 0.532275 \\times 0.4 = 0.21291$.\nThe joint probability $P_{\\mathrm{NB}}(A, Y=0) = P_{\\mathrm{NB}}(A \\mid Y=0)P(Y=0) = 0.05 \\times 0.6 = 0.03$.\nThe NB marginal evidence is $P_{\\mathrm{NB}}(A) = P_{\\mathrm{NB}}(A, Y=1) + P_{\\mathrm{NB}}(A, Y=0) = 0.21291 + 0.03 = 0.24291$.\n\nThe NB posterior is:\n$$P_{\\mathrm{NB}}(Y=1 \\mid A) = \\frac{0.21291}{0.24291} = \\frac{21291}{24291}$$\nSimplifying this fraction by dividing the numerator and denominator by their greatest common divisor, which is $3$:\n$$P_{\\mathrm{NB}}(Y=1 \\mid A) = \\frac{7097}{8097}$$\n\n### Part 3: Calculation of the Multiplicative Bias Factor\n\nThe multiplicative bias factor is the ratio of the true posterior to the NB posterior:\n$$\\text{Bias Factor} = \\frac{P(Y=1 \\mid A)}{P_{\\mathrm{NB}}(Y=1 \\mid A)} = \\frac{207/277}{21291/24291}$$\n$$\\text{Bias Factor} = \\frac{207}{277} \\times \\frac{24291}{21291}$$\nWe can work with the un-simplified fraction for the NB posterior to see if cancellations occur:\n$$\\text{Bias Factor} = \\frac{P(A, Y=1) / P(A)}{P_{\\mathrm{NB}}(A, Y=1) / P_{\\mathrm{NB}}(A)} = \\frac{0.2484 / 0.3324}{0.21291 / 0.24291}$$\n$$\\text{Bias Factor} = \\frac{2484/10000}{3324/10000} \\times \\frac{24291/100000}{21291/100000} = \\frac{2484}{3324} \\times \\frac{24291}{21291}$$\nLet's express the components as fractions:\n$P(A \\mid Y=1) = \\frac{621}{1000}$\n$P_{\\mathrm{NB}}(A \\mid Y=1) = \\frac{755}{1000} \\times \\frac{705}{1000} = \\frac{532275}{1000000} = \\frac{21291}{40000}$\n$P(A) = \\frac{3324}{10000}$\n$P_{\\mathrm{NB}}(A) = \\frac{24291}{100000}$\n\nThe bias factor can be written as $\\frac{P(A|Y=1)}{P_{\\mathrm{NB}}(A|Y=1)} \\times \\frac{P_{\\mathrm{NB}}(A)}{P(A)}$.\n$$\\text{Bias Factor} = \\frac{621/1000}{21291/40000} \\times \\frac{24291/100000}{3324/10000}$$\n$$\\text{Bias Factor} = \\left(\\frac{621}{1000} \\times \\frac{40000}{21291}\\right) \\times \\left(\\frac{24291}{100000} \\times \\frac{10000}{3324}\\right)$$\n$$\\text{Bias Factor} = \\left(\\frac{621 \\times 40}{21291}\\right) \\times \\left(\\frac{24291}{10 \\times 3324}\\right)$$\nWe know $621 = 3 \\times 207$ and $21291 = 3 \\times 7097$. Also $24291 = 9 \\times 2699$ and $3324 = 12 \\times 277$.\n$$\\text{Bias Factor} = \\left(\\frac{207 \\times 40}{7097}\\right) \\times \\left(\\frac{9 \\times 2699}{10 \\times 12 \\times 277}\\right)$$\n$$\\text{Bias Factor} = \\left(\\frac{207 \\times 40}{7097}\\right) \\times \\left(\\frac{3 \\times 2699}{40 \\times 277}\\right)$$\nThe factor of $40$ cancels:\n$$\\text{Bias Factor} = \\frac{207 \\times 3 \\times 2699}{277 \\times 7097} = \\frac{621 \\times 2699}{1965869} = \\frac{1676079}{1965869}$$",
            "answer": "$$\\boxed{\\frac{1676079}{1965869}}$$"
        }
    ]
}