{
    "hands_on_practices": [
        {
            "introduction": "LIME的力量在于其使用简单、可解释的代理模型来局部地解释复杂模型。但我们如何知道线性代理模型是否“足够好”？本实践将深入探讨这一问题的理论核心。通过分析一个具有曲率的模型，你将推导出一个精确的数学关系，它连接了黑盒模型的复杂度、局部邻域的大小以及解释的保真度，从而揭示何时我们必须考虑使用更复杂的代理模型。",
            "id": "3140905",
            "problem": "一个黑箱预测器 $f:\\mathbb{R}\\to\\mathbb{R}$ 需要在点 $x_{0}$ 附近使用“局部可解释模型无关解释”(LIME) 方法进行解释。在 $x_{0}$ 的局部邻域内，输入变量被重新参数化为 $t = x - x_{0}$，并通过高斯核赋予局部性权重，使得 $t \\sim \\mathcal{N}(0, r^{2})$，其中 $r > 0$ 是某个局部性尺度。用于拟合代理模型 $g$ 的目标是总体加权均方误差\n$$\\mathrm{WMSE}(g) \\equiv \\mathbb{E}\\left[(f(x_{0}+t) - g(x_{0}+t))^{2}\\right],$$\n其中期望是关于 $t$ 上的局部高斯权重计算的。\n\n假设 $f$ 在 $x_{0}$ 附近具有曲率，并且可以被其二阶泰勒展开很好地近似，\n$$f(x_{0}+t) = \\beta_{0} + \\beta_{1} t + \\beta_{2} t^{2},$$\n其中 $\\beta_{0}, \\beta_{1}, \\beta_{2} \\in \\mathbb{R}$，并假设观测值带有加性噪声 $\\varepsilon$，其满足 $\\mathbb{E}[\\varepsilon] = 0$ 和 $\\mathrm{Var}(\\varepsilon) = \\sigma^{2}$，且独立于 $t$。考虑两个局部代理模型：一个形式为 $g_{1}(x_{0}+t) = a + b t$ 的线性代理模型和一个形式为 $g_{2}(x_{0}+t) = a + b t + c t^{2}$ 的二次代理模型，每个模型都通过最小化 $\\mathrm{WMSE}(g)$ 来拟合。\n\n将保真度差距定义为线性代理模型相对于二次代理模型的超额加权均方误差，\n$$\\Delta(r) \\equiv \\mathrm{WMSE}(g_{1}) - \\mathrm{WMSE}(g_{2}).$$\n设 $\\delta > 0$ 是一个固定的保真度容差，它指定了当使用线性代理模型而非二次代理模型时，所允许的最大超额加权均方误差。推导最小局部性尺度 $r^{\\star}$ 的闭式表达式，使得只要 $r \\ge r^{\\star}$，二次项对于将超额误差保持在容差以下就变得必要，也就是说，$\\Delta(r) \\ge \\delta$。你的最终答案必须是仅用 $\\delta$ 和 $\\beta_{2}$ 表示的 $r^{\\star}$ 的单个闭式表达式。不需要进行数值近似。",
            "solution": "问题要求解最小局部性尺度 $r^{\\star}$，使得线性和二次代理模型之间的保真度差距 $\\Delta(r)$ 达到或超过容差 $\\delta$。保真度差距定义为 $\\Delta(r) \\equiv \\mathrm{WMSE}(g_{1}) - \\mathrm{WMSE}(g_{2})$。为此，我们必须首先通过最小化总体加权均方误差 $\\mathrm{WMSE}(g)$ 来确定每个代理模型 $g_1$ 和 $g_2$ 的最优参数。\n\n输入变量 $t = x - x_0$ 服从高斯分布 $t \\sim \\mathcal{N}(0, r^2)$。期望 $\\mathbb{E}[\\cdot]$ 是根据此分布计算的。我们将需要 $t$ 的各阶矩：\n$\\mathbb{E}[t] = 0$\n$\\mathbb{E}[t^2] = r^2$\n$\\mathbb{E}[t^3] = 0$\n$\\mathbb{E}[t^4] = 3(\\mathbb{E}[t^2])^2 = 3r^4$\n\n真实函数 $f$ 由一个二次函数近似：\n$f(x_0+t) = \\beta_0 + \\beta_1 t + \\beta_2 t^2$。\n\n问题陈述观测值包含加性噪声 $\\varepsilon$，其满足 $\\mathbb{E}[\\varepsilon]=0$ 和 $\\mathrm{Var}(\\varepsilon) = \\sigma^2$。如果目标是最小化关于带噪观测值的误差，我们将最小化 $\\mathbb{E}_{t, \\varepsilon}[(f(x_0+t)+\\varepsilon - g(x_0+t))^2]$。由于 $\\varepsilon$ 和 $t$ 的独立性以及 $\\mathbb{E}[\\varepsilon]=0$，该期望可展开为 $\\mathbb{E}_{t}[(f(x_0+t) - g(x_0+t))^2] + \\mathbb{E}_{\\varepsilon}[\\varepsilon^2] = \\mathrm{WMSE}(g) + \\sigma^2$。因为 $\\sigma^2$ 是一个常数，最小化此式等价于最小化所陈述的 $\\mathrm{WMSE}(g)$。此外，保真度差距 $\\Delta(r)$ 将是 $(\\mathrm{WMSE}(g_1)+\\sigma^2) - (\\mathrm{WMSE}(g_2)+\\sigma^2) = \\mathrm{WMSE}(g_1) - \\mathrm{WMSE}(g_2)$，所以噪声项不影响最终结果。\n\n首先，我们求最优线性代理模型 $g_1(x_0+t) = a+bt$。目标是最小化：\n$$\\mathrm{WMSE}(g_1) = \\mathbb{E}\\left[(f(x_0+t) - (a+bt))^2\\right]$$\n这是一个加权最小二乘问题。最优系数 $a$ 和 $b$ 可以通过将 $\\mathrm{WMSE}(g_1)$ 对 $a$ 和 $b$ 的偏导数设为零来求得。\n$$\\frac{\\partial}{\\partial a} \\mathrm{WMSE}(g_1) = \\mathbb{E}\\left[ -2(f(x_0+t) - a - bt) \\right] = -2(\\mathbb{E}[f] - a - b\\mathbb{E}[t]) = 0$$\n$$\\frac{\\partial}{\\partial b} \\mathrm{WMSE}(g_1) = \\mathbb{E}\\left[ -2t(f(x_0+t) - a - bt) \\right] = -2(\\mathbb{E}[tf] - a\\mathbb{E}[t] - b\\mathbb{E}[t^2]) = 0$$\n使用 $\\mathbb{E}[t]=0$ 和 $\\mathbb{E}[t^2]=r^2$，该方程组简化为：\n$a = \\mathbb{E}[f]$\n$b = \\frac{\\mathbb{E}[tf]}{r^2}$\n\n我们计算所需的期望：\n$\\mathbb{E}[f] = \\mathbb{E}[\\beta_0 + \\beta_1 t + \\beta_2 t^2] = \\beta_0 + \\beta_1\\mathbb{E}[t] + \\beta_2\\mathbb{E}[t^2] = \\beta_0 + \\beta_2 r^2$。\n$\\mathbb{E}[tf] = \\mathbb{E}[t(\\beta_0 + \\beta_1 t + \\beta_2 t^2)] = \\mathbb{E}[\\beta_0 t + \\beta_1 t^2 + \\beta_2 t^3] = \\beta_0\\mathbb{E}[t] + \\beta_1\\mathbb{E}[t^2] + \\beta_2\\mathbb{E}[t^3] = \\beta_1 r^2$。\n\n$g_1$ 的最优系数为：\n$a^{\\star} = \\beta_0 + \\beta_2 r^2$\n$b^{\\star} = \\frac{\\beta_1 r^2}{r^2} = \\beta_1$\n最优线性代理模型是 $g_1^{\\star}(x_0+t) = (\\beta_0 + \\beta_2 r^2) + \\beta_1 t$。\n\n现在，我们计算最小化后的误差 $\\mathrm{WMSE}(g_1^{\\star})$：\n$$f(x_0+t) - g_1^{\\star}(x_0+t) = (\\beta_0 + \\beta_1 t + \\beta_2 t^2) - ((\\beta_0 + \\beta_2 r^2) + \\beta_1 t) = \\beta_2(t^2 - r^2)$$\n$$\\mathrm{WMSE}(g_1^{\\star}) = \\mathbb{E}[(\\beta_2(t^2 - r^2))^2] = \\beta_2^2 \\mathbb{E}[(t^2-r^2)^2] = \\beta_2^2 \\mathbb{E}[t^4 - 2r^2t^2 + r^4]$$\n$$\\mathrm{WMSE}(g_1^{\\star}) = \\beta_2^2 (\\mathbb{E}[t^4] - 2r^2\\mathbb{E}[t^2] + r^4) = \\beta_2^2 (3r^4 - 2r^2(r^2) + r^4) = \\beta_2^2 (2r^4) = 2\\beta_2^2 r^4$$\n\n接下来，我们求最优二次代理模型 $g_2(x_0+t) = a+bt+ct^2$。目标是最小化：\n$$\\mathrm{WMSE}(g_2) = \\mathbb{E}\\left[(f(x_0+t) - (a+bt+ct^2))^2\\right]$$\n$$f(x_0+t) - g_2(x_0+t) = (\\beta_0 - a) + (\\beta_1 - b)t + (\\beta_2 - c)t^2$$\n误差 $\\mathrm{WMSE}(g_2)$ 是一个非负量。如果我们能选择 $a,b,c$ 使得对所有 $t$ 都有差值 $f-g_2$ 为零，那么误差就可以为零。这可以通过设置以下参数实现：\n$a = \\beta_0$\n$b = \\beta_1$\n$c = \\beta_2$\n使用这些系数，$g_2^{\\star}$ 与 $f$ 的局部近似完全相同。\n因此，最小化后的误差是：\n$$\\mathrm{WMSE}(g_2^{\\star}) = \\mathbb{E}[(f(x_0+t) - f(x_0+t))^2] = \\mathbb{E}[0] = 0$$\n\n现在我们可以计算保真度差距 $\\Delta(r)$：\n$$\\Delta(r) = \\mathrm{WMSE}(g_1^{\\star}) - \\mathrm{WMSE}(g_2^{\\star}) = 2\\beta_2^2 r^4 - 0 = 2\\beta_2^2 r^4$$\n\n最后，我们需要找到最小尺度 $r^{\\star}$，使得对所有 $r \\ge r^{\\star}$，都有 $\\Delta(r) \\ge \\delta$。函数 $\\Delta(r) = 2\\beta_2^2 r^4$ 对于 $r > 0$ 是单调递增的（假设 $\\beta_2 \\neq 0$，这由问题中提到的曲率所暗示）。因此，如果我们通过求解等式来找到 $r^{\\star}$，那么该条件将对所有 $r \\ge r^{\\star}$ 成立：\n$$\\Delta(r^{\\star}) = \\delta$$\n$$2\\beta_2^2 (r^{\\star})^4 = \\delta$$\n$$(r^{\\star})^4 = \\frac{\\delta}{2\\beta_2^2}$$\n$$r^{\\star} = \\left(\\frac{\\delta}{2\\beta_2^2}\\right)^{1/4}$$\n这就给出了最小局部性尺度 $r^{\\star}$ 的闭式表达式。",
            "answer": "$$\\boxed{\\left(\\frac{\\delta}{2\\beta_{2}^{2}}\\right)^{1/4}}$$"
        },
        {
            "introduction": "在探索了平滑函数下的理论之后，我们现在用一种极端的非线性情况来挑战LIME：一个不连续的函数。这种阈值行为在决策树或基于规则的系统等现实世界模型中很常见。通过这个动手编程实践，你将能够凭经验测试局部线性模型在多大程度上能够近似一个急剧的跳变，以及定义“局部”的核宽度参数$ \\sigma $如何关键地影响解释检测和表示这种不连续性的能力。",
            "id": "3140899",
            "problem": "考虑一个在第一个坐标上表现出急剧阈值变化的黑盒实值函数 $f:\\mathbb{R}^d \\to \\mathbb{R}$。该函数定义为\n$$\nf(\\mathbf{x}) \\;=\\; J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\},\n$$\n其中 $\\mathbf{x} = (x_1,\\dots,x_d)$，$J \\in \\mathbb{R}$ 是一个固定的跳跃幅度，$\\tau \\in \\mathbb{R}$ 是阈值，而 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n使用局部可解释模型无关解释 (Local Interpretable Model-Agnostic Explanations, LIME) 的原理构建一个局部代理模型。该代理模型是在目标点 $\\mathbf{x}_0 \\in \\mathbb{R}^d$ 处拟合的局部加权线性回归。样本 $\\mathbf{x}$ 的权重由欧几里得范数中的指数核函数给出\n$$\nw(\\mathbf{x}) \\;=\\; \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right),\n$$\n核宽度为 $\\sigma > 0$。局部线性代理模型具有以下形式\n$$\n\\hat{f}(\\mathbf{x}) \\;=\\; \\beta_0 + \\sum_{j=1}^d \\beta_j x_j,\n$$\n其中系数通过最小化加权最小二乘目标函数获得\n$$\n\\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2,\n$$\n给定一组从以 $\\mathbf{x}_0$ 为中心、各坐标上各向同性标准差为 $s$ 的高斯分布中独立抽取的 $N$ 个扰动样本 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$：\n$$\n\\mathbf{x}^{(i)} \\sim \\mathcal{N}\\!\\left(\\mathbf{x}_0, s^2 \\mathbf{I}_d\\right).\n$$\n\n定义以下可量化的概念，以判断局部代理模型是否捕捉到 $\\mathbf{x}_0$ 附近的阈值效应：\n- 第一个坐标上跨越核尺度的真实局部对比度为\n$$\nC_{\\text{true}} \\;=\\; \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|,\n$$\n其中 $\\mathbf{e}_1$ 是 $\\mathbb{R}^d$ 中的第一个规范基向量。对于给定的函数 $f$，如果 $x_{0,1} - \\sigma  \\tau \\le x_{0,1} + \\sigma$，则该式简化为 $C_{\\text{true}} = J$，否则 $C_{\\text{true}} = 0$。\n- 在相同尺度上，由代理模型预测的局部对比度为\n$$\nC_{\\text{pred}} \\;=\\; \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| \\;=\\; \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|.\n$$\n\n如果绝对误差满足以下条件，则声明局部代理模型捕捉到了该阈值\n$$\n\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\;\\le\\; \\varepsilon \\,\\max\\{1, |J|\\},\n$$\n对于一个固定的容差 $\\varepsilon  0$。\n\n任务。编写一个完整的程序，该程序：\n1. 实现指定的函数 $f$。\n2. 对于下面的每个测试用例，从 $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ 中生成 $N$ 个样本 $\\mathbf{x}^{(i)}$，并使用固定的伪随机种子 $12345$ 以确保可复现性。\n3. 使用测试用例的核宽度 $\\sigma$ 计算权重 $w(\\mathbf{x}^{(i)})$。\n4. 使用带截距项的加权最小二乘法拟合局部加权线性回归，以获得 $(\\beta_0,\\dots,\\beta_d)$。任何数值稳定的方法都是可接受的，但它必须精确地实现所述的加权目标函数。\n5. 按照定义计算 $C_{\\text{true}}$ 和 $C_{\\text{pred}}$，并返回一个布尔值，指示在 $\\varepsilon = 0.35$ 的情况下捕获准则是否成立。\n6. 将所有测试用例的布尔结果汇总到单行输出中，该输出包含用方括号括起来的逗号分隔列表，格式完全如下所示：例如，\"[True,False,True]\"。\n\n测试套件。使用以下四个测试用例，每个用例由元组 $(d,\\tau,J,\\mathbf{x}_0,s,\\sigma,N)$ 定义：\n- 用例 A (接近阈值，小核): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.02,\\;0.0),\\;s=\\;0.2,\\;\\sigma=\\;0.08,\\;N=\\;12000)$。\n- 用例 B (远离阈值): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.5,\\;0.0),\\;s=\\;0.2,\\;\\sigma=\\;0.08,\\;N=\\;8000)$。\n- 用例 C (恰在阈值上，非常宽的核): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.0,\\;0.0),\\;s=\\;0.05,\\;\\sigma=\\;0.5,\\;N=\\;12000)$。\n- 用例 D (恰在阈值上，非常窄的核，更高维度): $(d=\\;3,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.0,\\;0.1,\\;-0.1),\\;s=\\;0.2,\\;\\sigma=\\;0.02,\\;N=\\;20000)$。\n\n角度单位不适用。此问题中不出现物理单位。您的程序应生成单行输出，其中包含布尔结果，格式为方括号括起来的逗号分隔列表，顺序与上述测试套件用例的顺序完全一致，例如：\"[True,False,True,True]\"。不应打印任何其他文本。",
            "solution": "### 第 1 步：提取已知信息\n\n问题提供了以下定义和数据：\n\n- **黑盒函数**：$f:\\mathbb{R}^d \\to \\mathbb{R}$ 定义为 $f(\\mathbf{x}) = J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\}$，其中 $\\mathbf{x} = (x_1,\\dots,x_d)$，$J \\in \\mathbb{R}$ 是跳跃幅度，$\\tau \\in \\mathbb{R}$ 是阈值，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- **LIME 权重核函数**：$w(\\mathbf{x}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$，对于目标点 $\\mathbf{x}_0$ 和核宽度 $\\sigma  0$。\n- **局部代理模型**：一个线性模型 $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$。\n- **目标函数**：系数 $(\\beta_0, \\beta_1, \\dots, \\beta_d)$ 通过最小化加权最小二乘误差来确定：\n$$ \\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2 $$\n- **扰动样本**：一组从 $\\mathbf{x}^{(i)} \\sim \\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ 中抽取的 $N$ 个样本 $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$。\n- **真实局部对比度**：$C_{\\text{true}} = \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|$。\n- **预测局部对比度**：$C_{\\text{pred}} = \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| = \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|$。\n- **捕获准则**：$\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\le \\varepsilon \\,\\max\\{1, |J|\\}$，固定容差 $\\varepsilon = 0.35$。\n- **可复现性**：必须使用固定的伪随机种子 $12345$ 来生成样本。\n- **测试用例**：\n    - 用例 A: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.02, 0.0), s=0.2, \\sigma=0.08, N=12000)$\n    - 用例 B: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.5, 0.0), s=0.2, \\sigma=0.08, N=8000)$\n    - 用例 C: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.0), s=0.05, \\sigma=0.5, N=12000)$\n    - 用例 D: $(d=3, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.1, -0.1), s=0.2, \\sigma=0.02, N=20000)$\n\n### 第 2 步：使用提取的已知信息进行验证\n\n- **科学依据**：该问题在统计学习领域有充分的科学依据，特别是关于局部可解释模型无关解释 (LIME)。所描述的方法——局部加权线性回归、高斯核加权和高斯扰动——都是标准技术。函数 $f(\\mathbf{x})$ 是一个简单且定义明确的数学函数（阶跃函数）。\n- **适定性**：该问题是适定的。目标是基于一系列确定性计算，给定一组固定的参数和一个固定的随机种子，来计算一个布尔值。在指定条件下（从连续分布中抽样），加权最小二乘问题具有唯一、稳定的解。\n- **客观性**：问题以精确、客观的数学语言陈述。所有定义、参数和标准都是定量的、无歧义的。\n- **完整性与一致性**：问题是自洽的。为每个测试用例提供了所有必要的数据和定义。没有内部矛盾。\n\n### 第 3 步：结论与行动\n\n该问题是有效的。这是一个定义明确的计算任务，基于机器学习中已确立的原则。将提供完整的解决方案。\n\n### 解决方案\n\n任务是针对几个测试用例，确定一个 LIME 风格的局部线性代理模型是否能捕捉到给定函数 $f(\\mathbf{x})$ 的急剧阈值行为。这通过比较 $f(\\mathbf{x})$ 在一个局部区间内的真实变化与代理模型预测的变化来评估。为每个测试用例实现以下算法。\n\n首先，对于给定的参数为 $(d, \\tau, J, \\mathbf{x}_0, s, \\sigma, N)$ 的测试用例，我们生成 $N$ 个扰动样本。为保证可复现性，伪随机数生成器使用种子值 $12345$。每个样本 $\\mathbf{x}^{(i)}$ 从多元正态分布 $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ 中抽取，其中 $\\mathbf{I}_d$ 是 $d \\times d$ 的单位矩阵。\n\n接下来，我们为每个样本 $\\mathbf{x}^{(i)}$ 计算黑盒函数 $f(\\mathbf{x}^{(i)}) = J \\cdot \\mathbf{1}\\{x_1^{(i)} \\ge \\tau\\}$。这将产生一个响应向量 $\\mathbf{y} \\in \\mathbb{R}^N$，其中每个元素 $y_i = f(\\mathbf{x}^{(i)})$。\n\nLIME 方法的核心是通过解决一个加权最小二乘问题来拟合局部代理模型。每个样本 $\\mathbf{x}^{(i)}$ 的权重使用指数核函数 $w(\\mathbf{x}^{(i)}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}^{(i)} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$ 计算。这些权重 $w_i = w(\\mathbf{x}^{(i)})$ 构成权重矩阵 $\\mathbf{W}$ 的对角线元素。\n\n线性代理模型为 $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$。为了找到系数向量 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_d)^T$，我们求解加权最小二乘法的正规方程：\n$$ \\boldsymbol{\\beta} = (\\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{X}_{\\text{aug}})^{-1} \\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{y} $$\n这里，$\\mathbf{y}$ 是函数值 $f(\\mathbf{x}^{(i)})$ 的 $N \\times 1$ 向量。$\\mathbf{X}_{\\text{aug}}$ 是 $N \\times (d+1)$ 的增广设计矩阵，通过在 $N \\times d$ 的样本矩阵前添加一列全为 1 的列来构造：$\\mathbf{X}_{\\text{aug}} = [\\mathbf{1}_N, \\mathbf{X}]$。该方程使用稳定的线性代数求解器对 $\\boldsymbol{\\beta}$ 进行数值求解，这比直接计算矩阵的逆更稳健。\n\n确定系数向量 $\\boldsymbol{\\beta}$ 后，我们可以计算预测的局部对比度。我们感兴趣的系数是 $\\beta_1$，它对应于特征 $x_1$。预测的对比度由 $C_{\\text{pred}} = |2\\sigma \\beta_1|$ 给出。\n\n然后我们计算真实的局部对比度 $C_{\\text{true}}$，它被定义为函数 $f(\\mathbf{x})$ 在第一个坐标上跨越区间 $[x_{0,1} - \\sigma, x_{0,1} + \\sigma]$ 的实际变化：\n$$ C_{\\text{true}} = \\left| f(\\mathbf{x}_0 + \\sigma\\mathbf{e}_1) - f(\\mathbf{x}_0 - \\sigma\\mathbf{e}_1) \\right| $$\n其中 $\\mathbf{e}_1$ 是第一个标准基向量。这通过在两个点 $\\mathbf{x}_{\\text{plus}} = \\mathbf{x}_0 + \\sigma\\mathbf{e}_1$ 和 $\\mathbf{x}_{\\text{minus}} = \\mathbf{x}_0 - \\sigma\\mathbf{e}_1$ 上计算 $f$ 来得到。\n\n最后，我们应用捕获准则。如果预测对比度和真实对比度之间的绝对误差在指定的容差范围内，则认为局部代理模型已捕捉到阈值效应：\n$$ |C_{\\text{pred}} - C_{\\text{true}}| \\le \\varepsilon \\max\\{1, |J|\\} $$\n使用给定的值 $\\varepsilon = 0.35$ 对此不等式进行评估。为该测试用例记录所得的布尔值（True 或 False）。对所有四个测试用例重复这整个过程，并将布尔结果序列格式化为最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LIME surrogate model validation problem for a series of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, tau, J, x_0, s, sigma, N)\n    test_cases = [\n        # Case A: near-threshold, small kernel\n        (2, 0.0, 4.0, (0.02, 0.0), 0.2, 0.08, 12000),\n        # Case B: far from threshold\n        (2, 0.0, 4.0, (0.5, 0.0), 0.2, 0.08, 8000),\n        # Case C: exactly at threshold, very wide kernel\n        (2, 0.0, 4.0, (0.0, 0.0), 0.05, 0.5, 12000),\n        # Case D: exactly at threshold, very narrow kernel, higher dimension\n        (3, 0.0, 4.0, (0.0, 0.1, -0.1), 0.2, 0.02, 20000),\n    ]\n\n    # Fixed tolerance for the capture criterion\n    epsilon = 0.35\n    \n    # Store boolean results for each case\n    results = []\n\n    for case in test_cases:\n        d, tau, J, x_0_tuple, s, sigma, N = case\n        x_0 = np.array(x_0_tuple)\n\n        # Set the pseudo-random seed for reproducibility for each case\n        np.random.seed(12345)\n        \n        # 1. Generate N samples from N(x_0, s^2 * I_d)\n        samples = x_0 + s * np.random.standard_normal(size=(N, d))\n        \n        # 2. Evaluate the black-box function f(x) for all samples\n        # f(x) = J * 1{x_1 = tau}\n        f_values = J * (samples[:, 0] = tau).astype(float)\n        \n        # 3. Compute weights w(x) for all samples\n        # w(x) = exp(-||x - x_0||^2 / (2 * sigma^2))\n        sq_dists = np.sum((samples - x_0)**2, axis=1)\n        weights = np.exp(-sq_dists / (2 * sigma**2))\n        \n        # 4. Fit the locally weighted linear regression\n        # Create the augmented design matrix X_aug with an intercept column\n        X_aug = np.hstack([np.ones((N, 1)), samples])\n        Y = f_values\n        \n        # Construct the matrices for the normal equation: (X^T W X) beta = X^T W Y\n        # To do this efficiently, we use broadcasting with the weights vector\n        # instead of creating a large diagonal matrix W.\n        # Let A = X^T W X and b = X^T W Y\n        \n        # A = X_aug.T @ (weights[:, np.newaxis] * X_aug)\n        A = X_aug.T @ (weights.reshape(-1, 1) * X_aug) \n        # b = X_aug.T @ (weights * Y)\n        b = X_aug.T @ (weights * Y)\n        \n        # Solve the linear system A * beta = b for beta\n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse as a fallback for singular or ill-conditioned matrices\n            beta = np.linalg.pinv(A) @ b\n        \n        # The coefficient for the first coordinate, x_1, is beta[1]\n        beta_1 = beta[1]\n        \n        # 5. Compute C_true and C_pred\n        \n        # C_true = | f(x_0 + sigma*e_1) - f(x_0 - sigma*e_1) |\n        f_plus = J if (x_0[0] + sigma) = tau else 0.0\n        f_minus = J if (x_0[0] - sigma) = tau else 0.0\n        C_true = np.abs(f_plus - f_minus)\n        \n        # C_pred = | 2 * sigma * beta_1 |\n        C_pred = np.abs(2 * sigma * beta_1)\n        \n        # 6. Apply the capture criterion\n        abs_error = np.abs(C_pred - C_true)\n        tolerance_threshold = epsilon * np.max([1.0, np.abs(J)])\n        \n        is_captured = abs_error = tolerance_threshold\n        results.append(is_captured)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "LIME解释的保真度不仅取决于黑盒模型的结构，还取决于用于训练代理模型的数据样本质量。如果这些局部数据包含异常值，会发生什么？本实践旨在探讨鲁棒性这一关键方面。你将实现并比较一个使用$L_2$损失的标准LIME代理模型和一个采用Huber损失的鲁棒版本，亲身体验损失函数的一个简单改变如何能在存在噪声或异常数据点的情况下，使解释变得更加可靠。",
            "id": "3140869",
            "problem": "您需要实现并比较两种基于局部可解释模型无关解释 (LIME) 框架的局部线性代理模型。其中一个模型使用平方误差损失（记为 $L_2$ 损失）进行训练，另一个使用 Huber 损失进行训练，旨在检验模型对于远离目标点 $x_0$ 采样的离群值扰动的鲁棒性。\n\n这项任务的核心是拟合一个局部加权的线性代理模型，用于在固定点 $x_0 \\in \\mathbb{R}^p$ 附近近似一个平滑的黑箱回归函数 $f:\\mathbb{R}^p \\to \\mathbb{R}$。局部代理模型使用原始特征作为可解释特征，并通过一个核函数来强制实现局部性，该核函数会降低远离 $x_0$ 的点的权重。您必须实现基于 $L_2$ 的代理模型（加权最小二乘法）和基于 Huber 損失的代理模型（加权稳健回归）。然后，您将把估计出的局部系数与 $f$ 在 $x_0$ 处的真实梯度进行定量比较。\n\n基本原理：\n- 局部可解释模型无关解释 (LIME) 中的局部代理模型是通过最小化一个线性模型的局部性加权经验风险来定义的。对于一个带有局部性权重 $\\{w_i\\}_{i=1}^n$ 的数据集 $\\{(x_i,y_i)\\}_{i=1}^n$，$L_2$ 代理模型求解\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\left(y_i - \\beta_0 - \\beta^\\top x_i\\right)^2,\n$$\n而 Huber 损失代理模型求解\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\,\\rho_\\delta\\!\\left(y_i - \\beta_0 - \\beta^\\top x_i\\right),\n$$\n其中，带有阈值 $\\delta0$ 的 Huber 损失 $\\rho_\\delta$ 定义为\n$$\n\\rho_\\delta(r)=\\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r|\\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2,  \\text{if } |r|\\delta.\n\\end{cases}\n$$\n- 高斯核是编码局部性的标准选择。对于带宽 $\\sigma0$，定义\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right).\n$$\n\n您将使用一个固定的、平滑的黑箱模型 $f$，其维度为 $p=5$，定义如下\n$$\nf(x) \\;=\\; \\tanh\\!\\left(a^\\top x\\right) \\;+\\; \\frac{1}{2}\\left(b^\\top x\\right)^2 \\;+\\; c^\\top x \\;+\\; 0.2\\,\\sin\\!\\left(d^\\top x\\right),\n$$\n其中 $a,b,c,d \\in \\mathbb{R}^5$ 是固定向量，$x\\in\\mathbb{R}^5$。必须计算精确的解析梯度 $\\nabla f(x)$，并用它来评估代理模型在 $x_0$ 处的斜率系数的质量。\n\n邻域数据必须通过以下方式扰动 $x_0$ 来生成。给定一个污染比例 $\\gamma \\in [0,1]$、一个远距离离群值尺度 $R0$、局部噪声标准差 $s_{\\text{local}}0$ 以及远距离离群值噪声标准差 $s_{\\text{far}}0$，构造 $n$ 个扰动点，其中包括 $n_{\\text{out}}=\\lfloor \\gamma n \\rfloor$ 个“远”点和 $n_{\\text{in}}=n-n_{\\text{out}}$ 个“近”点：\n- 近点：$x_i = x_0 + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$，对于 $i=1,\\dots,n_{\\text{in}}$。\n- 远点：通过对标准高斯向量进行归一化，从单位球 $\\mathbb{S}^{p-1}$ 上均匀采样单位方向 $u_j$，然后设置 $x_j = x_0 + R\\,u_j + \\eta_j$，其中 $\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$，对于 $j=1,\\dots,n_{\\text{out}}$。\n\n对于每个合成数据集，计算 $y_i=f(x_i)$ 和使用带宽为 $\\sigma0$ 的高斯核计算局部性权重 $w_i$。在 $x_0$ 处拟合两个局部代理模型：\n- $L_2$ 代理模型：通过最小化加权平方误差来拟合。\n- Huber 损失代理模型：通过最小化带有固定阈值 $\\delta0$ 的加权 Huber 目标函数来拟合。\n\n评估：令 $\\widehat{\\beta}^{(2)}$ 表示从 $L_2$ 代理模型得到的斜率向量，$\\widehat{\\beta}^{(H)}$ 表示从 Huber 代理模型得到的斜率向量。令 $g_0=\\nabla f(x_0)$ 为在 $x_0$ 处的真实梯度。计算每个代理模型的斜率误差，即欧几里得范数\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2.\n$$\n将 Huber 代理模型相对于 $L_2$ 代理模型的改进报告为单一浮点数\n$$\n\\Delta \\;=\\; E_2 - E_H.\n$$\n正的 $\\Delta$ 值表明 Huber 代理模型在 $x_0$ 处对真实局部斜率的估计更为准确。\n\n实现要求：\n- 使用 $p=5$，$n=400$，$s_{\\text{local}}=0.4$，$s_{\\text{far}}=0.1$，$\\delta=1.0$。使用固定的随机种子以确保结果是确定性的。\n- 从独立标准正态分布中抽取 $x_0$、$a$、$b$、$c$ 和 $d$一次，并在所有测试案例中保持不变。\n- 使用给定 $\\sigma$ 的高斯核计算权重。\n- 通过加权最小二乘法求解 $L_2$ 问题。通过一种有原则的方法（如迭代重加权最小二乘法 (IRLS)）求解 Huber 问题，并将局部性权重 $\\{w_i\\}$ 正确地整合到目标函数中。收敛容差和迭代上限可以合理选择，但解必须是数值稳定的。\n\n测试套件：\n为以下五个参数集 $(\\gamma, R, \\sigma)$ 运行您的程序：\n1. $(0.0, 5.0, 1.0)$\n2. $(0.2, 5.0, 1.0)$\n3. $(0.6, 10.0, 1.0)$\n4. $(0.2, 5.0, 0.2)$\n5. $(0.2, 5.0, 5.0)$\n\n对于每种情况，按照规定构建邻域数据集，拟合两个代理模型，计算 $E_2$、$E_H$，并返回 $\\Delta=E_2-E_H$ 作为浮点数。为确保可比性，在一次运行中，除了测试案例参数外，底层的随机性必须由单一的全局种子固定。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含五个 $\\Delta$ 值的结果，以逗号分隔的列表形式，保留六位小数，并用方括号括起来，例如，“[0.123456,0.000001,-0.010203,0.500000,0.250000]”。",
            "solution": "目标是实现并比较两种用于可解释机器学习的局部线性代理模型，特别是在局部可解释模型无关解释 (LIME) 的框架内。一个代理模型基于标准的平方误差 ($L_2$) 损失，而另一个则采用稳健的 Huber 损失。比较的重点在于，估计出的局部线性系数（斜率）相对于黑箱函数真实梯度的准确性，尤其是在局部邻域数据中存在离群值的情况下。\n\n### 1. 问题构建\n\n给定一个平滑的非线性黑箱函数 $f:\\mathbb{R}^p \\to \\mathbb{R}$，其中维度固定为 $p=5$。该函数定义为：\n$$\nf(x) = \\tanh(a^\\top x) + \\frac{1}{2}\\left(b^\\top x\\right)^2 + c^\\top x + 0.2\\,\\sin(d^\\top x)\n$$\n其中 $a, b, c, d \\in \\mathbb{R}^p$ 是固定的参数向量。我们需要用一个线性模型来近似该函数在目标点 $x_0 \\in \\mathbb{R}^p$ 处的局部行为。局部线性近似斜率的真实值为 $f$ 在 $x_0$ 处求得的梯度，我们记为 $g_0 = \\nabla f(x_0)$。使用链式法则进行逐分量微分，得到解析梯度：\n$$\n\\nabla f(x) = \\left(1 - \\tanh^2(a^\\top x)\\right)a + \\left(b^\\top x\\right)b + c + 0.2\\cos(d^\\top x)d\n$$\n\n为训练局部代理模型，我们在 $x_0$ 周圍生成一个合成数据集 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $y_i=f(x_i)$。这个大小为 $n=400$ 的数据集被污染了比例为 $\\gamma$ 的离群值。数据集中有 $n_{\\text{in}} = n - \\lfloor \\gamma n \\rfloor$ 个“近”点和 $n_{\\text{out}} = \\lfloor \\gamma n \\rfloor$ 个“远”点。\n- 近点（内点）：$x_i = x_0 + \\epsilon_i$，噪声 $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$。\n- 远点（离群点）：$x_j = x_0 + R\\,u_j + \\eta_j$，其中 $u_j$是从单位球面 $\\mathbb{S}^{p-1}$ 上均匀采样的， $R$ 是一个大的距离尺度，$\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$ 是一个小扰动。\n\n为强制实现局部性，每个数据点 $(x_i, y_i)$ 被赋予一个权重 $w_i$，该权重基于其与 $x_0$ 的接近程度，由带宽为 $\\sigma$ 的高斯核决定：\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right)\n$$\n\n### 2. 局部代理模型\n\n代理模型是一个线性函数 $g(x) = \\beta_0 + \\beta^\\top x$，其中 $\\beta_0 \\in \\mathbb{R}$ 是截距，$\\beta \\in \\mathbb{R}^p$ 是斜率向量。参数通过最小化一个加权损失函数来找到。令 $\\tilde{x}_i = [1, x_i^\\top]^\\top$ 为增广特征向量，$\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$ 为完整的系数向量。\n\n#### 2.1. $L_2$ 损失代理模型（加权最小二乘法）\n标准方法是最小化加权平方误差之和：\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)^2\n$$\n这是一个标准的加权最小二乘 (WLS) 问题。以矩阵形式表示，令 $\\tilde{X}$ 为 $n \\times (p+1)$ 的设计矩阵，其行向量为 $\\tilde{x}_i^\\top$；令 $\\mathbf{y}$ 为响应向量；令 $W$ 为 $n \\times n$ 的对角矩阵，其对角线元素为局部性权重 $w_i$。目标是最小化 $(\\mathbf{y} - \\tilde{X}\\tilde{\\beta})^\\top W (\\mathbf{y} - \\tilde{X}\\tilde{\\beta})$。估计系数 $\\widehat{\\tilde{\\beta}}^{(2)}$ 的闭式解为：\n$$\n\\widehat{\\tilde{\\beta}}^{(2)} = (\\tilde{X}^\\top W \\tilde{X})^{-1} \\tilde{X}^\\top W \\mathbf{y}\n$$\n用于解释的斜率向量 $\\widehat{\\beta}^{(2)}$ 是 $\\widehat{\\tilde{\\beta}}^{(2)}$ 的最后 $p$ 个元素。\n\n#### 2.2. Huber 损失代理模型（稳健回归）\n为提高对离群值的鲁棒性，我们用阈值为 $\\delta  0$ 的 Huber 损失 $\\rho_\\delta(\\cdot)$ 替换平方误差：\n$$\n\\rho_\\delta(r) = \\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r|\\le \\delta \\\\\n\\delta |r| - \\frac{1}{2}\\delta^2,  \\text{if } |r|  \\delta\n\\end{cases}\n$$\nHuber 代理模型的系数 $\\widehat{\\tilde{\\beta}}^{(H)}$ 通过求解以下凸优化问题得到：\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\rho_\\delta\\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)\n$$\n这个问题没有闭式解，通常使用迭代重加权最小二乘法 (IRLS)进行数值求解。一阶最优性条件（估计方程）为 $\\sum_{i=1}^n w_i \\psi_\\delta(r_i) \\tilde{x}_i = \\mathbf{0}$，其中 $r_i = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}$，$\\psi_\\delta(r) = \\rho'_\\delta(r)$ 是 Huber 损失的导数。这些方程可以重写为 $\\sum_{i=1}^n w_i \\omega_i(r_i) r_i \\tilde{x}_i = \\mathbf{0}$，其中 $\\omega_i(r_i) = \\psi_\\delta(r_i)/r_i$ 是依赖于残差的权重。\nIRLS 算法流程如下：\n1.  初始化系数 $\\tilde{\\beta}^{(0)}$，例如，使用 $L_2$ 解 $\\widehat{\\tilde{\\beta}}^{(2)}$。\n2.  对于迭代 $k=0, 1, 2, \\dots$ 直至收敛：\n    a.  计算残差：$r_i^{(k)} = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}^{(k)}$。\n    b.  计算 IRLS 权重：如果 $|r_i^{(k)}| \\le \\delta$，则 $\\omega_i^{(k)} = 1$；如果 $|r_i^{(k)}|  \\delta$，则 $\\omega_i^{(k)} = \\delta / |r_i^{(k)}|$。这些权重会降低大残差数据点的影响。\n    c.  构建一个总权重对角矩阵 $W_{\\text{total}}^{(k)}$，其对角线元素为 $w_i \\cdot \\omega_i^{(k)}$，结合了局部性权重和残差权重。\n    d.  通过求解使用这些总权重的 WLS 问题来更新系数：\n        $$\n        \\tilde{\\beta}^{(k+1)} = (\\tilde{X}^\\top W_{\\text{total}}^{(k)} \\tilde{X})^{-1} \\tilde{X}^\\top W_{\\text{total}}^{(k)} \\mathbf{y}\n        $$\n3.  当系数的变化量 $\\|\\tilde{\\beta}^{(k+1)} - \\tilde{\\beta}^{(k)}\\|_2$ 小于一个小的容差时，算法终止。最终的斜率向量 $\\widehat{\\beta}^{(H)}$ 是收敛后 $\\tilde{\\beta}$ 的最后 $p$ 个元素。\n\n### 3. 评估\n每个代理模型的质量通过其估计的斜率向量与真实梯度 $g_0 = \\nabla f(x_0)$ 之间的欧几里得距离来衡量：\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2\n$$\n最终报告的指标是 Huber 代理模型相对于 $L_2$ 代理模型的改进程度，定义为其误差之差：\n$$\n\\Delta = E_2 - E_H\n$$\n$\\Delta$ 的正值表明 Huber 损失代理模型提供了对 $f$ 在 $x_0$ 处的局部行为更准确的估计，证明了其对离群值污染的鲁棒性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares L2-loss and Huber-loss local surrogate models.\n    \"\"\"\n    # --- Problem Constants and Fixed Parameters ---\n    P = 5\n    N = 400\n    S_LOCAL = 0.4\n    S_FAR = 0.1\n    DELTA = 1.0\n    RANDOM_SEED = 42\n\n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate and fix the model parameters and the point of interest\n    x0 = rng.standard_normal(size=P)\n    a = rng.standard_normal(size=P)\n    b = rng.standard_normal(size=P)\n    c = rng.standard_normal(size=P)\n    d = rng.standard_normal(size=P)\n\n    test_cases = [\n        (0.0, 5.0, 1.0),\n        (0.2, 5.0, 1.0),\n        (0.6, 10.0, 1.0),\n        (0.2, 5.0, 0.2),\n        (0.2, 5.0, 5.0),\n    ]\n\n    # --- Helper Functions ---\n\n    def f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The black-box function f(x).\"\"\"\n        return (\n            np.tanh(a_p @ x_vec)\n            + 0.5 * (b_p @ x_vec) ** 2\n            + c_p @ x_vec\n            + 0.2 * np.sin(d_p @ x_vec)\n        )\n\n    def grad_f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The analytical gradient of f(x).\"\"\"\n        grad = (\n            (1 - np.tanh(a_p @ x_vec) ** 2) * a_p\n            + (b_p @ x_vec) * b_p\n            + c_p\n            + 0.2 * np.cos(d_p @ x_vec) * d_p\n        )\n        return grad\n\n    def solve_wls(X_tilde, y, weights):\n        \"\"\"Solves a weighted least squares problem.\"\"\"\n        W = np.diag(weights)\n        # Using np.linalg.solve for stability: (X.T @ W @ X) @ beta = X.T @ W @ y\n        lhs = X_tilde.T @ W @ X_tilde\n        rhs = X_tilde.T @ W @ y\n        try:\n            beta_tilde = np.linalg.solve(lhs, rhs)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if singular\n            beta_tilde = np.linalg.pinv(lhs) @ rhs\n        return beta_tilde\n\n    def solve_huber_irls(X_tilde, y, locality_weights, delta, tol=1e-7, max_iter=100):\n        \"\"\"Solves a Huber regression problem using IRLS.\"\"\"\n        # Initialize with the standard WLS solution\n        beta_tilde = solve_wls(X_tilde, y, locality_weights)\n\n        for _ in range(max_iter):\n            residuals = y - X_tilde @ beta_tilde\n            abs_residuals = np.abs(residuals)\n            \n            # IRLS weights: handles r=0 case correctly via np.where\n            irls_weights = np.where(abs_residuals = delta, 1.0, delta / abs_residuals)\n            \n            total_weights = locality_weights * irls_weights\n            \n            beta_tilde_new = solve_wls(X_tilde, y, total_weights)\n            \n            # Check for convergence\n            if np.linalg.norm(beta_tilde_new - beta_tilde)  tol:\n                beta_tilde = beta_tilde_new\n                break\n            \n            beta_tilde = beta_tilde_new\n            \n        return beta_tilde\n\n    results = []\n\n    # --- Main Loop over Test Cases ---\n    for gamma, R, sigma in test_cases:\n        # 1. Generate neighborhood data\n        n_out = int(np.floor(gamma * N))\n        n_in = N - n_out\n\n        # Near points\n        epsilons = rng.normal(scale=S_LOCAL, size=(n_in, P))\n        X_in = x0 + epsilons\n\n        # Far points\n        if n_out > 0:\n            Z = rng.normal(size=(n_out, P))\n            directions = Z / np.linalg.norm(Z, axis=1, keepdims=True)\n            etas = rng.normal(scale=S_FAR, size=(n_out, P))\n            X_out = x0 + R * directions + etas\n            X = np.vstack((X_in, X_out))\n        else:\n            X = X_in\n\n        # Compute responses y = f(x)\n        y = np.array([f_model(x_i, a, b, c, d) for x_i in X])\n\n        # 2. Compute locality weights\n        distances_sq = np.sum((X - x0) ** 2, axis=1)\n        locality_weights = np.exp(-distances_sq / (2 * sigma**2))\n\n        # 3. Prepare matrices for regression\n        X_tilde = np.c_[np.ones(N), X]\n\n        # 4. Solve for L2 surrogate\n        beta_tilde_2 = solve_wls(X_tilde, y, locality_weights)\n        beta_hat_2 = beta_tilde_2[1:]\n\n        # 5. Solve for Huber surrogate\n        beta_tilde_H = solve_huber_irls(X_tilde, y, locality_weights, DELTA)\n        beta_hat_H = beta_tilde_H[1:]\n\n        # 6. Evaluate against the true gradient\n        g0 = grad_f_model(x0, a, b, c, d)\n        \n        e2 = np.linalg.norm(beta_hat_2 - g0)\n        eH = np.linalg.norm(beta_hat_H - g0)\n        \n        delta_error = e2 - eH\n        results.append(delta_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{v:.6f}' for v in results)}]\")\n\nsolve()\n```"
        }
    ]
}