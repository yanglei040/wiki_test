{
    "hands_on_practices": [
        {
            "introduction": "训练数据的偏差是导致算法不公平的常见根源，例如不同群体间的结果流行度存在差异。重采样是一种常见的预处理技术，旨在通过调整数据分布来强制实现公平性定义，例如人口均等。本练习将引导你分析一种通过二次抽样实现人口均等的方法，并使用霍维茨-汤普森（Horvitz-Thompson）估计量来严格量化这种干预对统计估计准确性的影响，从而揭示公平性与估计方差之间的权衡。",
            "id": "3098386",
            "problem": "考虑一个二元分类场景，其中有两个人口群体 $g \\in \\{0,1\\}$ 和一个二元结果 $Y \\in \\{0,1\\}$。总体大小分别为 $N_{0}$ 和 $N_{1}$，真实阳性率分别为 $p_{0} = \\mathbb{P}(Y=1 \\mid g=0)$ 和 $p_{1} = \\mathbb{P}(Y=1 \\mid g=1)$。您通过在每个组内独立地进行子抽样来构建一个训练数据集，采用以下依赖于标签的纳入方案：对于组 $g$，以概率 $a_{g} \\in (0,1]$ 独立地纳入每个正例 ($Y=1$)，并以概率 $b_{g} \\in (0,1]$ 独立地纳入每个负例 ($Y=0$)。从组 $g$ 中纳入的数据点的期望数量为 $N_{g}\\big(a_{g} p_{g} + b_{g} (1-p_{g})\\big)$。\n\n您希望在训练分布中实现人口均等，即均等化两个组的期望阳性率。也就是说，您要求构建的样本中每个组的期望阳性率等于一个共同的目标水平 $\\tau \\in (0,1)$：\n$$\n\\frac{a_{g} p_{g}}{a_{g} p_{g} + b_{g} (1-p_{g})} \\;=\\; \\tau \\quad \\text{for } g \\in \\{0,1\\}.\n$$\n此外，假设您有一个固定的期望抽样预算 $B>0$，它约束了总的期望样本大小：\n$$\nN_{0}\\big(a_{0} p_{0} + b_{0} (1-p_{0})\\big) + N_{1}\\big(a_{1} p_{1} + b_{1} (1-p_{1})\\big) \\;=\\; B.\n$$\n\n为了量化子抽样的准确性成本，您将使用 Horvitz–Thompson (HT) 估计量来估计真实的组阳性率 $p_{g}$。在这种情况下，该估计量对纳入的正例使用逆概率权重：\n$$\n\\widehat{p}_{g} \\;=\\; \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{\\mathbf{1}\\{\\text{unit } i \\text{ is included}\\} \\cdot \\mathbf{1}\\{Y_{i}=1\\}}{a_{g}}.\n$$\n您将通过目标均等阳性率约束来衡量公平性，并通过阳性率差异 $\\delta = p_{1} - p_{0}$ 的估计量的均方误差 (MSE) 来衡量准确性：\n$$\n\\widehat{\\delta} \\;=\\; \\widehat{p}_{1} - \\widehat{p}_{0}.\n$$\n\n从独立伯努利抽样的期望和方差的基本定义以及 Horvitz–Thompson 估计量的无偏性出发，推导出最优的纳入概率 $a_{0}^{\\star}, a_{1}^{\\star}, b_{0}^{\\star}, b_{1}^{\\star}$，以在上述人口均等约束和预算约束下，最小化均方误差 $\\mathbb{E}\\big[(\\widehat{\\delta} - \\delta)^{2}\\big]$。假设 $B$ 和 $\\tau$ 的值使得最优纳入概率位于区间 $(0,1]$ 内。\n\n将您的最终答案 $a_{0}^{\\star}, a_{1}^{\\star}, b_{0}^{\\star}, b_{1}^{\\star}$ 表示为 $N_{0}, N_{1}, p_{0}, p_{1}, \\tau, B$ 的闭式函数。最终答案必须是单个解析表达式，如果您选择呈现多个量，请使用 LaTeX 的 $\\mathrm{pmatrix}$ 环境将它们写成单行矩阵。不需要进行数值舍入。",
            "solution": "## 问题验证\n\n### 步骤 1：提取已知条件\n- **设置**：二元分类，具有二元结果 $Y \\in \\{0, 1\\}$ 和两个人口群体 $g \\in \\{0, 1\\}$。\n- **总体参数**：\n    - 组大小：$N_{0}$，$N_{1}$。\n    - 真实阳性率：$p_{0} = \\mathbb{P}(Y=1 \\mid g=0)$ 和 $p_{1} = \\mathbb{P}(Y=1 \\mid g=1)$。\n- **抽样方案**：在每个组内独立进行子抽样。\n    - 组 $g$ 中正例 ($Y=1$) 的纳入概率：$a_{g} \\in (0, 1]$。\n    - 组 $g$ 中负例 ($Y=0$) 的纳入概率：$b_{g} \\in (0, 1]$。\n- **组 g 的期望样本大小**：$N_{g}\\big(a_{g} p_{g} + b_{g} (1-p_{g})\\big)$。\n- **约束条件**：\n    1.  **人口均等约束**：每个组的抽样数据中的期望阳性率必须等于一个目标水平 $\\tau \\in (0,1)$：\n        $$ \\frac{a_{g}p_{g}}{a_{g} p_{g} + b_{g} (1-p_{g})} = \\tau \\quad \\text{for } g \\in \\{0,1\\}. $$\n    2.  **预算约束**：总期望样本大小固定为 $B > 0$：\n        $$ N_{0}\\big(a_{0} p_{0} + b_{0} (1-p_{0})\\big) + N_{1}\\big(a_{1} p_{1} + b_{1} (1-p_{1})\\big) = B. $$\n- **估计量与目标函数**：\n    - **$p_g$ 的 Horvitz–Thompson (HT) 估计量**：\n      $$ \\widehat{p}_{g} = \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{\\mathbf{1}\\{\\text{unit } i \\text{ is included}\\} \\cdot \\mathbf{1}\\{Y_{i}=1\\}}{a_{g}}. $$\n    - **阳性率差异 $\\delta = p_1 - p_0$ 的估计量**：$\\widehat{\\delta} = \\widehat{p}_{1} - \\widehat{p}_{0}$。\n    - **目标**：最小化 $\\widehat{\\delta}$ 的均方误差 (MSE)：$\\mathbb{E}\\big[(\\widehat{\\delta} - \\delta)^{2}\\big]$。\n- **假设**：参数 $B$ 和 $\\tau$ 使得最优纳入概率 $a_{g}^{\\star}, b_{g}^{\\star}$ 位于区间 $(0, 1]$ 内。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准评估问题陈述：\n\n-   **科学依据**：该问题在统计学习和算法公平性领域有充分的理论基础。它使用了诸如用于调查抽样的 Horvitz–Thompson 估计、作为公平性指标的人口均等以及作为估计量准确性标准度量的均方误差等既有概念。该设置在科学和数学上是合理的。\n-   **适定性**：该问题是一个适定的约束优化问题。它要求在满足一组等式约束的条件下，最小化一个明确定义的目标函数（MSE）。我们将证明目标函数是优化变量的凸函数，并且约束条件对这些变量是线性的，这确保了在给定条件下存在唯一的最小值。\n-   **客观性**：该问题使用精确、客观的数学语言进行陈述。所有术语都有正式定义，没有主观或基于意见的陈述。\n-   **无缺陷**：该问题没有表现出任何列出的无效性缺陷。它不是科学上不合理、不可形式化、不完整、矛盾、不现实或不适定的。假设解位于 $(0, 1]$ 内简化了分析，但并未使问题变得微不足道；它绕过了对不等式约束进行显式 Karush-Kuhn-Tucker (KKT) 分析的需要，将挑战集中在核心优化上。\n\n### 步骤 3：结论与行动\n问题有效。将提供一个完整、合理的解答。\n\n## 解答\n\n问题是找到最优的纳入概率 $a_{0}^{\\star}, a_{1}^{\\star}, b_{0}^{\\star}, b_{1}^{\\star}$，以在人口均等和预算约束下，最小化估计量 $\\widehat{\\delta} = \\widehat{p}_1 - \\widehat{p}_0$ 的均方误差 (MSE)。\n\n### 1. 推导目标函数 (MSE)\n估计量 $\\widehat{\\theta}$ 对参数 $\\theta$ 的均方误差为 $\\mathrm{MSE}(\\widehat{\\theta}) = \\mathrm{Var}(\\widehat{\\theta}) + (\\mathbb{E}[\\widehat{\\theta}] - \\theta)^2$。\n首先，我们证明估计量 $\\widehat{p}_{g}$ 的无偏性。设 $Z_{gi}$ 为指示随机变量，表示来自组 $g$ 的单元 $i$ 是否被纳入样本。其期望值取决于单元的（固定）结果 $Y_{gi}$：$\\mathbb{E}[Z_{gi} | Y_{gi}=1] = a_g$ 和 $\\mathbb{E}[Z_{gi} | Y_{gi}=0] = b_g$。\n$p_g$ 的估计量为 $\\widehat{p}_{g} = \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{Z_{gi} \\cdot \\mathbf{1}\\{Y_{gi}=1\\}}{a_{g}}$。\n$\\widehat{p}_{g}$ 的期望值为：\n$$ \\mathbb{E}[\\widehat{p}_{g}] = \\frac{1}{N_{g} a_{g}} \\sum_{i=1}^{N_{g}} \\mathbb{E}[Z_{gi} \\cdot \\mathbf{1}\\{Y_{gi}=1\\}] $$\n对于 $Y_{gi}=0$ 的单元 $i$，该项为 0。对于 $Y_{gi}=1$ 的单元 $i$，该项为 $Z_{gi}$，且 $\\mathbb{E}[Z_{gi}]=a_g$。\n$$ \\mathbb{E}[\\widehat{p}_{g}] = \\frac{1}{N_{g} a_{g}} \\sum_{i=1}^{N_{g}} \\mathbb{E}[Z_{gi}] \\mathbf{1}\\{Y_{gi}=1\\} = \\frac{1}{N_{g} a_{g}} \\sum_{i=1}^{N_{g}} a_g \\mathbf{1}\\{Y_{gi}=1\\} = \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\mathbf{1}\\{Y_{gi}=1\\} $$\n和 $\\sum_{i=1}^{N_{g}} \\mathbf{1}\\{Y_{gi}=1\\}$ 是组 $g$ 中正例的总数，即 $N_g p_g$。因此，\n$$ \\mathbb{E}[\\widehat{p}_{g}] = \\frac{1}{N_{g}} (N_g p_g) = p_g $$\n估计量 $\\widehat{p}_{g}$ 是无偏的。因此，差异的估计量 $\\widehat{\\delta} = \\widehat{p}_1 - \\widehat{p}_0$ 也是无偏的，因为 $\\mathbb{E}[\\widehat{\\delta}] = \\mathbb{E}[\\widehat{p}_1] - \\mathbb{E}[\\widehat{p}_0] = p_1 - p_0 = \\delta$。\n因此，MSE 等于方差：$\\mathrm{MSE}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{\\delta})$。由于两个组的抽样是独立的，$\\mathrm{Var}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{p}_1) + \\mathrm{Var}(\\widehat{p}_0)$。\n\n接下来，我们计算 $\\mathrm{Var}(\\widehat{p}_{g})$。由于单元是独立抽样的，和的方差等于方差的和：\n$$ \\mathrm{Var}(\\widehat{p}_{g}) = \\mathrm{Var}\\left(\\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}}{a_{g}}\\right) = \\frac{1}{N_{g}^2 a_{g}^2} \\sum_{i=1}^{N_{g}} \\mathrm{Var}(Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}) $$\n对于 $Y_{gi}=0$ 的单元 $i$，项 $Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}$ 始终为 0，因此其方差为 0。\n对于 $Y_{gi}=1$ 的单元 $i$，该项为 $Z_{gi}$。由于 $Y_{gi}=1$，$Z_{gi}$ 是一个成功概率为 $a_g$ 的伯努利随机变量。其方差为 $\\mathrm{Var}(Z_{gi}) = a_g(1-a_g)$。\n所以，$\\mathrm{Var}(Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}) = a_g(1-a_g) \\mathbf{1}\\{Y_{gi}=1\\}$。\n对组 $g$ 中的所有单元求和：\n$$ \\sum_{i=1}^{N_{g}} \\mathrm{Var}(Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}) = \\sum_{i=1}^{N_{g}} a_g(1-a_g) \\mathbf{1}\\{Y_{gi}=1\\} = a_g(1-a_g) (N_g p_g) $$\n将此代回 $\\widehat{p}_g$ 的方差表达式中：\n$$ \\mathrm{Var}(\\widehat{p}_{g}) = \\frac{1}{N_{g}^2 a_{g}^2} [a_g(1-a_g) N_g p_g] = \\frac{p_g(1-a_g)}{N_g a_g} = \\frac{p_g}{N_g a_g} - \\frac{p_g}{N_g} $$\n要最小化的目标函数是总 MSE：\n$$ \\mathrm{MSE}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{p}_{0}) + \\mathrm{Var}(\\widehat{p}_{1}) = \\frac{p_0(1-a_0)}{N_0 a_0} + \\frac{p_1(1-a_1)}{N_1 a_1} $$\n最小化这个表达式等价于最小化 $\\frac{p_0}{N_0 a_0} + \\frac{p_1}{N_1 a_1}$，因为其他项是常数。\n\n### 2. 简化约束条件\n优化变量是 $a_0, a_1, b_0, b_1$。目标函数仅依赖于 $a_0$ 和 $a_1$。我们使用约束来消去 $b_0$ 和 $b_1$ 并约束 $a_0$ 和 $a_1$。\n\n从组 $g$ 的人口均等约束：\n$$ \\frac{a_{g} p_{g}}{a_{g} p_{g} + b_{g} (1-p_{g})} = \\tau $$\n$$ a_g p_g = \\tau \\big(a_g p_g + b_g(1-p_g)\\big) \\implies a_g p_g (1-\\tau) = \\tau b_g (1-p_g) $$\n这使我们能够用 $a_g$ 表示 $b_g$：\n$$ b_g = a_g \\frac{p_g(1-\\tau)}{\\tau(1-p_g)} $$\n现在，我们将此代入预算约束。首先，我们简化组 $g$ 的期望样本数项：\n$$ N_{g}\\big(a_{g} p_{g} + b_{g} (1-p_{g})\\big) = N_g \\frac{a_g p_g}{\\tau} $$\n预算约束变为：\n$$ N_{0} \\frac{a_0 p_0}{\\tau} + N_{1} \\frac{a_1 p_1}{\\tau} = B $$\n乘以 $\\tau$ 得到关于 $a_0$ 和 $a_1$ 的线性约束：\n$$ N_0 p_0 a_0 + N_1 p_1 a_1 = B \\tau $$\n\n### 3. 求解约束优化问题\n问题现在是在约束 $N_0 p_0 a_0 + N_1 p_1 a_1 = B \\tau$ 下最小化 $f(a_0, a_1) = \\frac{p_0}{N_0 a_0} + \\frac{p_1}{N_1 a_1}$。\n我们使用拉格朗日乘子法。拉格朗日函数为：\n$$ \\mathcal{L}(a_0, a_1, \\lambda) = \\frac{p_0}{N_0 a_0} + \\frac{p_1}{N_1 a_1} + \\lambda(N_0 p_0 a_0 + N_1 p_1 a_1 - B \\tau) $$\n对 $a_0$ 和 $a_1$ 求偏导数并令其为零：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial a_0} = -\\frac{p_0}{N_0 a_0^2} + \\lambda N_0 p_0 = 0 \\implies \\lambda = \\frac{1}{N_0^2 a_0^2} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial a_1} = -\\frac{p_1}{N_1 a_1^2} + \\lambda N_1 p_1 = 0 \\implies \\lambda = \\frac{1}{N_1^2 a_1^2} $$\n从这些方程中，我们得到 $\\frac{1}{N_0^2 a_0^2} = \\frac{1}{N_1^2 a_1^2}$，这意味着 $N_0 a_0 = N_1 a_1$（因为 $N_g, a_g > 0$）。\n令 $N_0 a_0 = N_1 a_1 = K$ 为某个常数 $K$。则 $a_0 = K/N_0$ 且 $a_1 = K/N_1$。\n将这些代入预算约束：\n$$ N_0 p_0 \\left(\\frac{K}{N_0}\\right) + N_1 p_1 \\left(\\frac{K}{N_1}\\right) = B \\tau $$\n$$ p_0 K + p_1 K = B \\tau \\implies K(p_0 + p_1) = B \\tau \\implies K = \\frac{B \\tau}{p_0 + p_1} $$\n现在我们求出最优值 $a_0^{\\star}$ 和 $a_1^{\\star}$：\n$$ a_0^{\\star} = \\frac{K}{N_0} = \\frac{B \\tau}{N_0 (p_0 + p_1)} $$\n$$ a_1^{\\star} = \\frac{K}{N_1} = \\frac{B \\tau}{N_1 (p_0 + p_1)} $$\n问题陈述保证这些值在 $(0, 1]$ 内。\n\n### 4. 确定最优的 $b_0^{\\star}$ 和 $b_1^{\\star}$\n使用关系式 $b_g = a_g \\frac{p_g(1-\\tau)}{\\tau(1-p_g)}$，我们求出 $b_0$ 和 $b_1$ 的最优值：\n$$ b_0^{\\star} = a_0^{\\star} \\frac{p_0(1-\\tau)}{\\tau(1-p_0)} = \\frac{B \\tau}{N_0 (p_0 + p_1)} \\cdot \\frac{p_0(1-\\tau)}{\\tau(1-p_0)} = \\frac{B p_0 (1-\\tau)}{N_0 (1-p_0) (p_0 + p_1)} $$\n$$ b_1^{\\star} = a_1^{\\star} \\frac{p_1(1-\\tau)}{\\tau(1-p_1)} = \\frac{B \\tau}{N_1 (p_0 + p_1)} \\cdot \\frac{p_1(1-\\tau)}{\\tau(1-p_1)} = \\frac{B p_1 (1-\\tau)}{N_1 (1-p_1) (p_0 + p_1)} $$\n这些是在公平性和预算约束下，最小化阳性率差异估计量的 MSE 的最优纳入概率。这四个参数的最终表达式是：\n- $a_{0}^{\\star} = \\frac{B \\tau}{N_{0} (p_{0} + p_{1})}$\n- $a_{1}^{\\star} = \\frac{B \\tau}{N_{1} (p_{0} + p_{1})}$\n- $b_{0}^{\\star} = \\frac{B p_{0} (1 - \\tau)}{N_{0} (1 - p_{0}) (p_{0} + p_{1})}$\n- $b_{1}^{\\star} = \\frac{B p_{1} (1 - \\tau)}{N_{1} (1 - p_{1}) (p_{0} + p_{1})}$",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{B \\tau}{N_{0} (p_{0} + p_{1})} & \\frac{B \\tau}{N_{1} (p_{0} + p_{1})} & \\frac{B p_{0} (1 - \\tau)}{N_{0} (1 - p_{0}) (p_{0} + p_{1})} & \\frac{B p_{1} (1 - \\tau)}{N_{1} (1 - p_{1}) (p_{0} + p_{1})} \\end{pmatrix}} $$"
        },
        {
            "introduction": "除了在数据层面进行干预，我们还可以将公平性直接融入模型的学习目标中，这被称为“处理中”方法。本练习在一个回归场景下，将公平性定义为不同群体的均方误差（Mean Squared Error, MSE）相等，并探讨了当不同群体的数据特性（如噪声水平）存在差异时，单一模型如何可能产生不公平的结果。你将通过推导加权最小二乘法中的最优权重，来学习如何精确地调整学习过程，以在模型的整体预测精度和群体间的公平性之间做出有原则的权衡。",
            "id": "3098302",
            "problem": "一个二元敏感属性 $A \\in \\{0,1\\}$ 将一个总体划分为两个群体，其概率为 $\\mathbb{P}(A=a)=p_{a}$，其中 $p_{0}, p_{1} \\in (0,1)$ 且 $p_{0}+p_{1}=1$。单个实值特征 $X \\in \\mathbb{R}$ 的分布满足 $\\mathbb{E}[X \\mid A=a]=0$ 和 $\\operatorname{Var}(X \\mid A=a)=1$（对于每个 $a \\in \\{0,1\\}$），而结果 $Y \\in \\mathbb{R}$ 服从线性结构模型 $Y=\\beta_{A} X+\\varepsilon_{A}$，其中 $\\beta_{0}, \\beta_{1} \\in \\mathbb{R}$ 是特定于群体的斜率，$\\mathbb{E}[\\varepsilon_{A} \\mid X, A]=0$，且 $\\operatorname{Var}(\\varepsilon_{A} \\mid X, A=a)=\\sigma_{a}^{2}$，其中 $\\sigma_{0}^{2}, \\sigma_{1}^{2} > 0$。我们通过在 $\\theta \\in \\mathbb{R}$ 上最小化加权平方损失 $\\mathbb{E}[w(A)\\,(Y-\\theta X)^{2}]$ 来拟合一个单一的共享线性预测器 $\\hat{Y}_{\\theta}=\\theta X$，其中 $w(0)=w_{0}>0$ 和 $w(1)=w_{1}>0$ 是固定的正权重。为消除权重的尺度不确定性，假设归一化约束为 $p_{0} w_{0}+p_{1} w_{1}=1$。\n\n将回归中的公平性定义为群体条件均方误差的相等性：$\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=0]=\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=1]$。从条件期望、均方误差和加权最小二乘法的一阶最优性条件的核心定义出发，推导出唯一的归一化权重对 $(w_{0}, w_{1})$，该权重对使加权最小二乘解 $\\theta$ 满足公平性准则。您的推导应明确说明异方差噪声（即 $\\sigma_{0}^{2} \\neq \\sigma_{1}^{2}$）在使用单一共享预测器时，如何在预测准确性和公平性约束之间造成不可避免的权衡，然后展示适当的权重选择如何通过引导拟合的 $\\theta$ 来均衡条件均方误差，从而弥补这一问题。\n\n假设 $\\beta_{0} \\neq \\beta_{1}$，且参数使得公平性最优的共享斜率严格位于两个群体斜率之间，这在所述的归一化条件下保证了权重的正性。将您的最终答案表示为 $w_{0}$ 和 $w_{1}$ 关于 $\\beta_{0}$、$\\beta_{1}$、$\\sigma_{0}^{2}$、$\\sigma_{1}^{2}$、$p_{0}$ 和 $p_{1}$ 的闭式解析表达式。不需要数值近似，也不涉及单位。请使用 LaTeX 的 $\\operatorname{pmatrix}$ 环境将您的最终答案呈现为单行矩阵。",
            "solution": "该问题要求推导出一对唯一的归一化权重 $(w_{0}, w_{1})$，以确保加权最小二乘线性预测器满足特定的公平性准则。该公平性准则被定义为两个群体（$A=0$ 和 $A=1$）的群体条件均方误差相等。\n\n推导过程分为四个主要步骤：\n1.  通过最小化加权平方损失来确定最优预测器斜率 $\\theta$。\n2.  为每个群体 $a \\in \\{0,1\\}$ 构建群体条件均方误差 (MSE)。\n3.  应用公平性准则（群体条件 MSE 相等）来找到满足此约束的特定 $\\theta$ 值，记为 $\\theta_{F}$。\n4.  将步骤1中 $\\theta$ 的表达式与步骤3中的值 $\\theta_{F}$ 相等，并在给定的归一化约束下，解出由此产生的关于权重 $(w_{0}, w_{1})$ 的方程组。\n\n**步骤 1：加权最小二乘解**\n目标是找到使加权平方损失函数 $L(\\theta) = \\mathbb{E}[w(A)\\,(Y-\\theta X)^{2}]$ 最小化的斜率 $\\theta$。我们可以利用关于敏感属性 $A$ 的全期望定律来展开这个期望：\n$$L(\\theta) = \\sum_{a \\in \\{0,1\\}} \\mathbb{P}(A=a) \\mathbb{E}[w(A)\\,(Y-\\theta X)^{2} \\mid A=a]$$\n代入给定的概率 $p_{a}$ 和权重 $w_{a}$：\n$$L(\\theta) = p_{0} w_{0} \\mathbb{E}[(Y-\\theta X)^{2} \\mid A=0] + p_{1} w_{1} \\mathbb{E}[(Y-\\theta X)^{2} \\mid A=1]$$\n对于给定的群体 $A=a$，其结构模型为 $Y = \\beta_{a} X + \\varepsilon_{a}$。期望内的误差项变为：\n$$Y - \\theta X = (\\beta_{a} X + \\varepsilon_{a}) - \\theta X = (\\beta_{a} - \\theta)X + \\varepsilon_{a}$$\n平方误差为 $((\\beta_{a} - \\theta)X + \\varepsilon_{a})^{2} = (\\beta_{a}-\\theta)^{2}X^{2} + 2(\\beta_{a}-\\theta)X\\varepsilon_{a} + \\varepsilon_{a}^{2}$。我们计算其条件期望：\n$$\\mathbb{E}[((\\beta_{a} - \\theta)X + \\varepsilon_{a})^{2} \\mid A=a] = (\\beta_{a}-\\theta)^{2}\\mathbb{E}[X^{2} \\mid A=a] + 2(\\beta_{a}-\\theta)\\mathbb{E}[X\\varepsilon_{a} \\mid A=a] + \\mathbb{E}[\\varepsilon_{a}^{2} \\mid A=a]$$\n使用给定的条件：\n- $\\mathbb{E}[X \\mid A=a]=0$ 和 $\\operatorname{Var}(X \\mid A=a)=1$，这意味着 $\\mathbb{E}[X^{2} \\mid A=a] = \\operatorname{Var}(X \\mid A=a) + (\\mathbb{E}[X \\mid A=a])^{2} = 1 + 0^{2} = 1$。\n- $\\mathbb{E}[\\varepsilon_{A} \\mid X, A]=0$。根据迭代期望定律，$\\mathbb{E}[X\\varepsilon_{a} \\mid A=a] = \\mathbb{E}[\\mathbb{E}[X\\varepsilon_{a} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[X \\mathbb{E}[\\varepsilon_{a} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[X \\cdot 0 \\mid A=a] = 0$。\n- $\\operatorname{Var}(\\varepsilon_{A} \\mid X, A=a)=\\sigma_{a}^{2}$ 且 $\\mathbb{E}[\\varepsilon_{A} \\mid X, A=a] = 0$。这意味着 $\\mathbb{E}[\\varepsilon_{a}^{2} \\mid X, A=a] = \\sigma_{a}^{2}$。根据迭代期望定律，$\\mathbb{E}[\\varepsilon_{a}^{2} \\mid A=a] = \\mathbb{E}[\\mathbb{E}[\\varepsilon_{a}^{2} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[\\sigma_{a}^{2} \\mid A=a] = \\sigma_{a}^{2}$。\n\n将这些结果代回，群体 $a$ 的平方误差的条件期望是 $(\\beta_{a} - \\theta)^{2} + \\sigma_{a}^{2}$。损失函数变为：\n$$L(\\theta) = p_{0} w_{0} [(\\beta_{0} - \\theta)^{2} + \\sigma_{0}^{2}] + p_{1} w_{1} [(\\beta_{1} - \\theta)^{2} + \\sigma_{1}^{2}]$$\n为了找到最小值，我们计算关于 $\\theta$ 的导数并将其设为零（一阶最优性条件）：\n$$\\frac{dL}{d\\theta} = p_{0} w_{0} [2(\\beta_{0} - \\theta)(-1)] + p_{1} w_{1} [2(\\beta_{1} - \\theta)(-1)] = 0$$\n$$-2 [p_{0} w_{0} (\\beta_{0} - \\theta) + p_{1} w_{1} (\\beta_{1} - \\theta)] = 0$$\n$$(p_{0} w_{0} + p_{1} w_{1})\\theta = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$$\n应用归一化约束 $p_{0} w_{0} + p_{1} w_{1} = 1$，我们得到 $\\theta$ 的加权最小二乘解：\n$$\\theta = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$$\n\n**步骤 2：群体条件均方误差**\n群体 $a$ 的均方误差，记为 $\\text{MSE}_{a}$，定义为 $\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=a]$。在 $\\hat{Y}_{\\theta}=\\theta X$ 的情况下，这恰好是我们在步骤1中计算的条件期望：\n$$\\text{MSE}_{a} = \\mathbb{E}[(Y - \\theta X)^{2} \\mid A=a] = (\\beta_{a} - \\theta)^{2} + \\sigma_{a}^{2}$$\n这个表达式表明，一个群体的 MSE 有两个组成部分：一个偏差项 $(\\beta_{a}-\\theta)^{2}$，它衡量了群体最优斜率 $\\beta_a$ 和共享斜率 $\\theta$ 之间的平方差；以及一个来自不可约减噪声的方差项 $\\sigma_{a}^{2}$。\n\n**步骤 3：公平性约束**\n公平性准则要求 $\\text{MSE}_{0} = \\text{MSE}_{1}$。令 $\\theta_{F}$ 为满足此准则的 $\\theta$ 值。\n$$(\\beta_{0} - \\theta_{F})^{2} + \\sigma_{0}^{2} = (\\beta_{1} - \\theta_{F})^{2} + \\sigma_{1}^{2}$$\n展开平方项：\n$$\\beta_{0}^{2} - 2\\beta_{0}\\theta_{F} + \\theta_{F}^{2} + \\sigma_{0}^{2} = \\beta_{1}^{2} - 2\\beta_{1}\\theta_{F} + \\theta_{F}^{2} + \\sigma_{1}^{2}$$\n$$2\\beta_{1}\\theta_{F} - 2\\beta_{0}\\theta_{F} = \\beta_{1}^{2} - \\beta_{0}^{2} + \\sigma_{1}^{2} - \\sigma_{0}^{2}$$\n$$2\\theta_{F}(\\beta_{1} - \\beta_{0}) = (\\beta_{1} - \\beta_{0})(\\beta_{1} + \\beta_{0}) + (\\sigma_{1}^{2} - \\sigma_{0}^{2})$$\n由于 $\\beta_{0} \\neq \\beta_{1}$，我们可以除以 $2(\\beta_{1} - \\beta_{0})$：\n$$\\theta_{F} = \\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}$$\n这是使两组之间 MSE 相等的唯一斜率。如果噪声是同方差的（$\\sigma_{0}^{2} = \\sigma_{1}^{2}$），那么 $\\theta_F$ 就是两个特定群体斜率的平均值。然而，在异方差噪声（$\\sigma_{0}^{2} \\neq \\sigma_{1}^{2}$）的情况下，$\\theta_F$ 会偏离这个平均值以进行补偿。例如，如果群体1的噪声更大（$\\sigma_{1}^{2} > \\sigma_{0}^{2}$）且斜率更高（$\\beta_{1} > \\beta_{0}$），$\\theta_F$ 必须大于平均值，以减小群体1的偏差项（通过使 $\\theta_F$ 更接近 $\\beta_1$）并增加群体0的偏差项，从而平衡整体的 MSE。这说明了其中的权衡：实现公平性需要选择一个斜率 $\\theta_F$，这个斜率对于任何一个群体本身或者对于整个总体（通常是像 $p_0\\beta_0+p_1\\beta_1$ 这样的加权平均值）而言都不一定是最优的，其目的是为了平衡各群体之间的误差。权重 $(w_0, w_1)$ 是将加权最小二乘优化引向这个由公平性驱动的特定目标 $\\theta_F$ 的机制。\n\n**步骤 4：求解权重**\n我们现在有了一个关于 $w_{0}$ 和 $w_{1}$ 的二元线性方程组：\n1.  $\\theta_{F} = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$\n2.  $1 = p_{0} w_{0} + p_{1} w_{1}$\n\n从方程 (2) 中，我们可以写出 $p_{1} w_{1} = 1 - p_{0} w_{0}$。将其代入方程 (1)：\n$$\\theta_{F} = p_{0} w_{0} \\beta_{0} + (1 - p_{0} w_{0})\\beta_{1} = p_{0} w_{0} \\beta_{0} + \\beta_{1} - p_{0} w_{0} \\beta_{1}$$\n$$\\theta_{F} - \\beta_{1} = p_{0} w_{0} (\\beta_{0} - \\beta_{1})$$\n$$w_{0} = \\frac{\\theta_{F} - \\beta_{1}}{p_{0}(\\beta_{0} - \\beta_{1})} = \\frac{\\beta_{1} - \\theta_{F}}{p_{0}(\\beta_{1} - \\beta_{0})}$$\n代入 $\\theta_{F}$ 的表达式：\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\beta_{1} - \\left(\\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}\\right) \\right]$$\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{2\\beta_{1} - (\\beta_{0} + \\beta_{1})}{2} - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{\\beta_{1} - \\beta_{0}}{2} - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{0} = \\frac{1}{2p_{0}} \\left( 1 - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)$$\n\n类似地，我们可以通过将 $p_{0}w_{0} = 1 - p_{1}w_{1}$ 代入方程 (1) 来求解 $w_{1}$：\n$$\\theta_{F} = (1 - p_{1} w_{1})\\beta_{0} + p_{1} w_{1} \\beta_{1} = \\beta_{0} + p_{1} w_{1}(\\beta_{1} - \\beta_{0})$$\n$$w_{1} = \\frac{\\theta_{F} - \\beta_{0}}{p_{1}(\\beta_{1} - \\beta_{0})}$$\n代入 $\\theta_{F}$ 的表达式：\n$$w_{1} = \\frac{1}{p_{1}(\\beta_{1} - \\beta_{0})} \\left[ \\left(\\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}\\right) - \\beta_{0} \\right]$$\n$$w_{1} = \\frac{1}{p_{1}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{\\beta_{1} - \\beta_{0}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{1} = \\frac{1}{2p_{1}} \\left( 1 + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)$$\n\n这些就是所求的唯一归一化权重。假设 $\\theta_F$ 严格位于 $\\beta_0$ 和 $\\beta_1$ 之间，这确保了 $1 \\pm \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}}$ 项为正，因此 $w_0, w_1 > 0$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2p_{0}} \\left( 1 - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right) & \\frac{1}{2p_{1}} \\left( 1 + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "一个在训练数据上表现“公平”的模型，在实际部署后是否依然公平？本练习探讨了协变量漂移（covariate shift）这一关键挑战，即特征在群体内的分布在训练和测试环境之间发生变化。你将通过一个具体的例子发现，这种分布变化会破坏在训练时满足的公平性保证（如均等化赔率），并学习如何使用重要性加权（importance weighting）这一强大的技术，来稳健地审计和评估模型在目标真实世界分布上的公平性能。",
            "id": "3098292",
            "problem": "考虑经验风险最小化（ERM）下的二元分类问题，其中有一个二元敏感属性 $A \\in \\{0,1\\}$，一个二元特征 $X \\in \\{0,1\\}$，以及一个二元标签 $Y \\in \\{0,1\\}$。分类器定义为 $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$，其训练目标是最小化经验 $0$-$1$ 损失。设训练分布在不同组之间不平衡，其中 $P_{\\text{train}}(A=0) = 0.9$ 且 $P_{\\text{train}}(A=1) = 0.1$。假设标签条件分布在不同域和组之间是稳定的（仅存在协变量偏移），具体由 $P(Y=1 \\mid X=1) = \\frac{3}{4}$ 和 $P(Y=1 \\mid X=0) = \\frac{1}{4}$ 给出，因此对于训练和测试，$P(Y=0 \\mid X=1) = \\frac{1}{4}$ 和 $P(Y=0 \\mid X=0) = \\frac{3}{4}$。在训练分布上，协变量条件分布是组间平衡的：$P_{\\text{train}}(X=1 \\mid A=0) = P_{\\text{train}}(X=1 \\mid A=1) = \\frac{1}{2}$。在测试分布上，存在特定于组的协变量偏移：$P_{\\text{test}}(X=1 \\mid A=0) = \\frac{3}{4}$ 且 $P_{\\text{test}}(X=1 \\mid A=1) = \\frac{1}{4}$。\n\n我们审查关于假正率（FPR）的均等化赔率这一公平性概念，其中对于组 $a \\in \\{0,1\\}$，假正率定义为 $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$。首先，从定义出发进行推理，验证训练集上的均等化赔率（即 $\\text{FPR}_{0}$ 和 $\\text{FPR}_{1}$ 相等）成立，而测试集上的均等化赔率不成立。然后，通过定义权重 $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$，提出在协变量偏移下的重要性加权审查方法，并使用这些权重将测试域的FPR重写为训练域上的加权条件期望。\n\n根据给定的数值，显式地计算通过在训练分布上使用权重 $w_{a}(x)$ 进行审查所得到的重要性加权假正率差距 $\\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}}$。将您的最终答案表示为小数或分数（不带百分号）。无需四舍五入。",
            "solution": "首先将验证问题的正确性和可解性。\n\n### 步骤 1：提取已知条件\n- **问题类型**: 二元分类。\n- **敏感属性**: $A \\in \\{0,1\\}$。\n- **特征**: $X \\in \\{0,1\\}$。\n- **标签**: $Y \\in \\{0,1\\}$。\n- **分类器**: $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$。\n- **训练组分布**: $P_{\\text{train}}(A=0) = 0.9$，$P_{\\text{train}}(A=1) = 0.1$。\n- **稳定的标签条件分布**: $P(Y=1 \\mid X=1) = \\frac{3}{4}$ 且 $P(Y=1 \\mid X=0) = \\frac{1}{4}$。这意味着 $P(Y=0 \\mid X=1) = 1 - \\frac{3}{4} = \\frac{1}{4}$ 且 $P(Y=0 \\mid X=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$。该条件分布在不同域和组之间是恒定的。\n- **训练协变量条件分布**: $P_{\\text{train}}(X=1 \\mid A=0) = \\frac{1}{2}$ 且 $P_{\\text{train}}(X=1 \\mid A=1) = \\frac{1}{2}$。这意味着 $P_{\\text{train}}(X=0 \\mid A=0) = \\frac{1}{2}$ 且 $P_{\\text{train}}(X=0 \\mid A=1) = \\frac{1}{2}$。\n- **测试协变量条件分布**: $P_{\\text{test}}(X=1 \\mid A=0) = \\frac{3}{4}$ 且 $P_{\\text{test}}(X=1 \\mid A=1) = \\frac{1}{4}$。这意味着 $P_{\\text{test}}(X=0 \\mid A=0) = \\frac{1}{4}$ 且 $P_{\\text{test}}(X=0 \\mid A=1) = \\frac{3}{4}$。\n- **公平性度量**: 关于假正率（FPR）的均等化赔率。\n- **FPR 定义**: $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$。\n- **重要性权重定义**: $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$。\n- **目标**: 验证训练/测试集上的均等化赔率，然后计算重要性加权FPR差距 $\\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在统计学习和算法公平性领域具有科学依据。它使用了诸如经验风险最小化（ERM）、协变量偏移和均等化赔率等标准定义和概念。问题的设置是完全自洽的，提供了所有必要的概率分布。语言精确客观。没有矛盾、歧义或不切实际的假设。该问题是良定的，并导向一个唯一、稳定的解。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解答。\n\n### 解题推导\n该问题要求分析一个分类器在以协变量偏移为特征的训练和测试分布下的假正率（FPR）。分类器由 $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$ 给出，这意味着当且仅当特征 $X$ 为 1 时，它预测为正向结果（$\\hat{Y}=1$）。\n\n对于组 $a \\in \\{0, 1\\}$，假正率定义为 $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$。根据分类器的定义，这等价于 $\\text{FPR}_{a} = P(X=1 \\mid Y=0, A=a)$。我们将针对训练和测试分布计算这个量。\n\n使用贝叶斯定理，我们可以将FPR表示为：\n$$ \\text{FPR}_{a} = \\frac{P(Y=0 \\mid X=1, A=a) P(X=1 \\mid A=a)}{P(Y=0 \\mid A=a)} $$\n问题陈述标签条件分布是稳定的，即 $P(Y \\mid X, A) = P(Y \\mid X)$。因此，公式简化为：\n$$ \\text{FPR}_{a} = \\frac{P(Y=0 \\mid X=1) P(X=1 \\mid A=a)}{P(Y=0 \\mid A=a)} $$\n分母 $P(Y=0 \\mid A=a)$ 必须使用全概率定律为每个分布（训练和测试）计算：\n$$ P(Y=0 \\mid A=a) = \\sum_{x \\in \\{0,1\\}} P(Y=0 \\mid X=x, A=a) P(X=x \\mid A=a) $$\n$$ P_{\\text{dist}}(Y=0 \\mid A=a) = P(Y=0 \\mid X=0)P_{\\text{dist}}(X=0 \\mid A=a) + P(Y=0 \\mid X=1)P_{\\text{dist}}(X=1 \\mid A=a) $$\n其中 $P(Y=0 \\mid X=1) = \\frac{1}{4}$ 且 $P(Y=0 \\mid X=0) = \\frac{3}{4}$。\n\n**第一部分：验证训练和测试的均等化赔率**\n\n**训练分布：**\n首先，我们计算 $P_{\\text{train}}(Y=0 \\mid A=a)$：\n对于 $A=0$：$P_{\\text{train}}(Y=0 \\mid A=0) = (\\frac{3}{4})P_{\\text{train}}(X=0 \\mid A=0) + (\\frac{1}{4})P_{\\text{train}}(X=1 \\mid A=0) = (\\frac{3}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{2}) = \\frac{3}{8} + \\frac{1}{8} = \\frac{4}{8} = \\frac{1}{2}$。\n对于 $A=1$：$P_{\\text{train}}(Y=0 \\mid A=1) = (\\frac{3}{4})P_{\\text{train}}(X=0 \\mid A=1) + (\\frac{1}{4})P_{\\text{train}}(X=1 \\mid A=1) = (\\frac{3}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{2}) = \\frac{1}{2}$。\n\n现在，我们计算训练FPR：\n$\\text{FPR}_{0}^{\\text{train}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=0)}{P_{\\text{train}}(Y=0 \\mid A=0)} = \\frac{(\\frac{1}{4})(\\frac{1}{2})}{\\frac{1}{2}} = \\frac{1}{4}$。\n$\\text{FPR}_{1}^{\\text{train}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=1)}{P_{\\text{train}}(Y=0 \\mid A=1)} = \\frac{(\\frac{1}{4})(\\frac{1}{2})}{\\frac{1}{2}} = \\frac{1}{4}$。\n由于 $\\text{FPR}_{0}^{\\text{train}} = \\text{FPR}_{1}^{\\text{train}}$，因此在训练分布上，关于FPR的均等化赔率成立。\n\n**测试分布：**\n首先，我们计算 $P_{\\text{test}}(Y=0 \\mid A=a)$：\n对于 $A=0$：$P_{\\text{test}}(Y=0 \\mid A=0) = (\\frac{3}{4})P_{\\text{test}}(X=0 \\mid A=0) + (\\frac{1}{4})P_{\\text{test}}(X=1 \\mid A=0) = (\\frac{3}{4})(\\frac{1}{4}) + (\\frac{1}{4})(\\frac{3}{4}) = \\frac{3}{16} + \\frac{3}{16} = \\frac{6}{16} = \\frac{3}{8}$。\n对于 $A=1$：$P_{\\text{test}}(Y=0 \\mid A=1) = (\\frac{3}{4})P_{\\text{test}}(X=0 \\mid A=1) + (\\frac{1}{4})P_{\\text{test}}(X=1 \\mid A=1) = (\\frac{3}{4})(\\frac{3}{4}) + (\\frac{1}{4})(\\frac{1}{4}) = \\frac{9}{16} + \\frac{1}{16} = \\frac{10}{16} = \\frac{5}{8}$。\n\n现在，我们计算测试FPR：\n$\\text{FPR}_{0}^{\\text{test}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{test}}(X=1 \\mid A=0)}{P_{\\text{test}}(Y=0 \\mid A=0)} = \\frac{(\\frac{1}{4})(\\frac{3}{4})}{\\frac{3}{8}} = \\frac{\\frac{3}{16}}{\\frac{3}{8}} = \\frac{1}{2}$。\n$\\text{FPR}_{1}^{\\text{test}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{test}}(X=1 \\mid A=1)}{P_{\\text{test}}(Y=0 \\mid A=1)} = \\frac{(\\frac{1}{4})(\\frac{1}{4})}{\\frac{5}{8}} = \\frac{\\frac{1}{16}}{\\frac{5}{8}} = \\frac{1}{10}$。\n由于 $\\text{FPR}_{0}^{\\text{test}} \\neq \\text{FPR}_{1}^{\\text{test}}$，因此在测试分布上，关于FPR的均等化赔率不成立。\n\n**第二部分：重要性加权审查**\n\n重要性加权审查的目标是使用训练域的分布来估计测试域的量。重要性加权FPR，即 $\\text{FPR}_{a}^{\\text{iw}}$，是测试FPR（$\\text{FPR}_{a}^{\\text{test}}$）的一个估计量。在完全了解分布的情况下，这个理论量精确地等于 $\\text{FPR}_{a}^{\\text{test}}$。我们将通过显式计算来证明这一点。\n\n测试FPR可以写成期望的比率：\n$$ \\text{FPR}_{a}^{\\text{test}} = P_{\\text{test}}(\\hat{Y}=1 \\mid Y=0, A=a) = \\frac{P_{\\text{test}}(\\hat{Y}=1, Y=0 \\mid A=a)}{P_{\\text{test}}(Y=0 \\mid A=a)} = \\frac{E_{\\text{test}}[\\hat{Y}(X) \\mathbf{1}\\{Y=0\\} \\mid A=a]}{E_{\\text{test}}[\\mathbf{1}\\{Y=0\\} \\mid A=a]} $$\n在协变量偏移下，我们可以使用重要性权重 $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$ 将这些测试域期望重新表示为加权的训练域期望。这就给出了重要性加权FPR的公式：\n$$ \\text{FPR}_{a}^{\\text{iw}} = \\frac{E_{\\text{train}}[\\hat{Y}(X) \\mathbf{1}\\{Y=0\\} w_a(X) \\mid A=a]}{E_{\\text{train}}[\\mathbf{1}\\{Y=0\\} w_a(X) \\mid A=a]} $$\n首先，我们计算权重：\n对于 $A=0$：$w_0(1) = \\frac{P_{\\text{test}}(X=1 \\mid A=0)}{P_{\\text{train}}(X=1 \\mid A=0)} = \\frac{3/4}{1/2} = \\frac{3}{2}$。$w_0(0) = \\frac{P_{\\text{test}}(X=0 \\mid A=0)}{P_{\\text{train}}(X=0 \\mid A=0)} = \\frac{1/4}{1/2} = \\frac{1}{2}$。\n对于 $A=1$：$w_1(1) = \\frac{P_{\\text{test}}(X=1 \\mid A=1)}{P_{\\text{train}}(X=1 \\mid A=1)} = \\frac{1/4}{1/2} = \\frac{1}{2}$。$w_1(0) = \\frac{P_{\\text{test}}(X=0 \\mid A=1)}{P_{\\text{train}}(X=0 \\mid A=1)} = \\frac{3/4}{1/2} = \\frac{3}{2}$。\n\n现在我们为每个组 $a$ 计算分子和分母。期望 $E_{\\text{train}}[\\cdot \\mid A=a]$ 是对联合分布 $P_{\\text{train}}(X, Y \\mid A=a) = P(Y \\mid X) P_{\\text{train}}(X \\mid A=a)$ 求的。\n\n**对于组 $A=0$**：\n分子是 $N_0 = E_{\\text{train}}[\\mathbf{1}\\{X=1\\} \\mathbf{1}\\{Y=0\\} w_0(X) \\mid A=0]$。唯一的非零项是当 $X=1, Y=0$ 时。\n$N_0 = w_0(1) P_{\\text{train}}(X=1, Y=0 \\mid A=0) = w_0(1) P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=0) = (\\frac{3}{2}) (\\frac{1}{4}) (\\frac{1}{2}) = \\frac{3}{16}$。\n分母是 $D_0 = E_{\\text{train}}[\\mathbf{1}\\{Y=0\\} w_0(X) \\mid A=0]$。\n$D_0 = w_0(0) P_{\\text{train}}(X=0, Y=0 \\mid A=0) + w_0(1) P_{\\text{train}}(X=1, Y=0 \\mid A=0)$\n$D_0 = w_0(0) P(Y=0 \\mid X=0) P_{\\text{train}}(X=0 \\mid A=0) + N_0 = (\\frac{1}{2}) (\\frac{3}{4}) (\\frac{1}{2}) + \\frac{3}{16} = \\frac{3}{16} + \\frac{3}{16} = \\frac{6}{16} = \\frac{3}{8}$。\n所以，$\\text{FPR}_{0}^{\\text{iw}} = \\frac{N_0}{D_0} = \\frac{3/16}{3/8} = \\frac{1}{2}$。\n\n**对于组 $A=1$**：\n分子是 $N_1 = E_{\\text{train}}[\\mathbf{1}\\{X=1\\} \\mathbf{1}\\{Y=0\\} w_1(X) \\mid A=1]$。\n$N_1 = w_1(1) P_{\\text{train}}(X=1, Y=0 \\mid A=1) = w_1(1) P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=1) = (\\frac{1}{2}) (\\frac{1}{4}) (\\frac{1}{2}) = \\frac{1}{16}$。\n分母是 $D_1 = E_{\\text{train}}[\\mathbf{1}\\{Y=0\\} w_1(X) \\mid A=1]$。\n$D_1 = w_1(0) P_{\\text{train}}(X=0, Y=0 \\mid A=1) + w_1(1) P_{\\text{train}}(X=1, Y=0 \\mid A=1)$\n$D_1 = w_1(0) P(Y=0 \\mid X=0) P_{\\text{train}}(X=0 \\mid A=1) + N_1 = (\\frac{3}{2}) (\\frac{3}{4}) (\\frac{1}{2}) + \\frac{1}{16} = \\frac{9}{16} + \\frac{1}{16} = \\frac{10}{16} = \\frac{5}{8}$。\n所以，$\\text{FPR}_{1}^{\\text{iw}} = \\frac{N_1}{D_1} = \\frac{1/16}{5/8} = \\frac{1}{10}$。\n\n正如预期的，对于两个组，$\\text{FPR}_{a}^{\\text{iw}} = \\text{FPR}_{a}^{\\text{test}}$。重要性加权假正率差距为：\n$$ \\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}} = \\frac{1}{2} - \\frac{1}{10} = \\frac{5}{10} - \\frac{1}{10} = \\frac{4}{10} = \\frac{2}{5} $$\n作为小数，该值为 $0.4$。",
            "answer": "$$ \\boxed{\\frac{2}{5}} $$"
        }
    ]
}