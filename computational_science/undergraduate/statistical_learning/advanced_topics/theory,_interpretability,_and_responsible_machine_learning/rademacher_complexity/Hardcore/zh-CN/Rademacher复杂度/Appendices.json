{
    "hands_on_practices": [
        {
            "introduction": "本次练习旨在通过一个高度简化的假设类别，巩固对经验 Rademacher 复杂度的基本计算。通过直接处理一个仅包含两个函数（其在样本点上的投影向量分别为 $\\mathbf{v}$ 和 $-\\mathbf{v}$）的函数类，您可以专注于核心定义的应用，并理解 Rademacher 随机变量的期望如何度量函数类拟合随机噪声的能力。这个练习是掌握 Rademacher 复杂度计算机制的绝佳起点 。",
            "id": "694900",
            "problem": "在统计学习理论中，经验 Rademacher 复杂度是衡量一个函数类相对于给定数据集丰富程度的指标。它在推导依赖于数据的泛化界中扮演着至关重要的角色。\n\n设 $S = \\{z_1, z_2, \\ldots, z_n\\}$ 是 $\\mathbb{R}$ 中一个由 $n$ 个不同点组成的固定集合。设 $\\boldsymbol{\\sigma} = (\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)$ 是一个由独立 Rademacher 随机变量组成的向量，其中每个 $\\sigma_i$ 以相等的概率取值于 $\\{-1, 1\\}$：$P(\\sigma_i = 1) = P(\\sigma_i = -1) = 1/2$。\n\n对于一个函数类 $\\mathcal{F}$（其中每个函数 $f \\in \\mathcal{F}$ 从 $\\mathbb{R}$ 映射到 $\\mathbb{R}$），其相对于集合 $S$ 的经验 Rademacher 复杂度定义为：\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f(z_i) \\right]\n$$\n\n考虑一个从 $\\mathbb{R}$ 映射到 $\\{-1, 1\\}$ 的假设类 $\\mathcal{H}$。当这些函数在数据集 $S$ 中的数据点上求值时，它们会生成 $\\mathbb{R}^n$ 中的一个向量集。假设这个向量集，记为 $\\mathcal{H}|_S = \\{ (h(z_1), \\ldots, h(z_n)) \\mid h \\in \\mathcal{H} \\}$，恰好由两个向量组成：$\\mathbf{v}$ 和 $-\\mathbf{v}$，其中 $\\mathbf{v} \\in \\{-1, 1\\}^n$ 是某个向量。\n\n你的任务是计算该函数类的经验 Rademacher 复杂度 $\\hat{\\mathfrak{R}}_S(\\mathcal{H})$。最终表达式应为一个关于 $n$ 的闭式公式。",
            "solution": "1.  根据定义：\n    $$\n    \\hat{\\mathfrak{R}}_S(\\mathcal{H}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{h \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i h(z_i) \\right]\n    $$\n\n2.  因为在样本 $S$ 上的投影 $\\mathcal{H}|_S = \\{\\mathbf{v}, -\\mathbf{v}\\}$，所以上确界可以简化为：\n    $$\n    \\sup_{h \\in \\mathcal{H}} \\sum_{i=1}^n \\sigma_i h(z_i) = \\max\\left\\{ \\sum_{i=1}^n \\sigma_i v_i, -\\sum_{i=1}^n \\sigma_i v_i \\right\\} = \\left| \\sum_{i=1}^n \\sigma_i v_i \\right|\n    $$\n\n3.  令 $\\tau_i = \\sigma_i v_i$。由于 $v_i \\in \\{-1, 1\\}$ 且 $\\sigma_i$ 是 Rademacher 随机变量，$\\tau_i$ 也是独立的 Rademacher 随机变量。因此：\n    $$\n    \\hat{\\mathfrak{R}}_S(\\mathcal{H}) = \\frac{1}{n} \\mathbb{E} \\left[ \\left| \\sum_{i=1}^n \\tau_i \\right| \\right] = \\frac{1}{n} \\mathbb{E}[|S_n|], \\quad \\text{其中 } S_n = \\sum_{i=1}^n \\tau_i\n    $$\n\n4.  一个已知的组合学公式给出了 $n$ 个独立 Rademacher 随机变量之和的绝对值的期望：\n    $$\n    \\mathbb{E}[|S_n|] = \\sum_{k=0}^n |2k-n| \\binom{n}{k} 2^{-n} = n \\frac{\\binom{n-1}{\\lfloor\\frac{n-1}{2}\\rfloor}}{2^{n-1}}\n    $$\n\n5.  因此，经验 Rademacher 复杂度为：\n    $$\n    \\hat{\\mathfrak{R}}_S(\\mathcal{H}) = \\frac{1}{n} \\mathbb{E}[|S_n|] = \\frac{\\binom{n-1}{\\lfloor\\frac{n-1}{2}\\rfloor}}{2^{n-1}}\n    $$",
            "answer": "$$\\boxed{\\frac{\\binom{n-1}{\\lfloor\\frac{n-1}2\\rfloor}}{2^{\\,n-1}}}$$"
        },
        {
            "introduction": "这个练习将 Rademacher 复杂度的概念扩展到更高级的机器学习模型中，特别是再生核希尔伯特空间（RKHS）的背景。您将为一个由周期核生成的函数类（具体为其 RKHS 中的单位球）计算经验 Rademacher 复杂度。这个过程不仅展示了如何处理无限函数类，还突出了“表示定理技巧”(representer trick) 的威力，它将一个看似困难的函数上确界问题转化为一个关于核矩阵的计算问题 。",
            "id": "759130",
            "problem": "设 $\\mathcal{F}$ 是一个实值函数类。$\\mathcal{F}$ 关于有限点集 $S = \\{x_1, \\dots, x_n\\} \\subset \\mathbb{R}$ 的经验 Rademacher 复杂度定义为\n$$ \\hat{\\mathcal{R}}_S(\\mathcal{F}) = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\sum_{i=1}^n \\sigma_i f(x_i) \\right] $$\n其中 $\\boldsymbol{\\sigma} = (\\sigma_1, \\dots, \\sigma_n)$ 是一个独立 Rademacher 随机变量组成的向量，对于每个 $i$，都有 $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = 1/2$。\n\n考虑一个函数类，它是再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS) 的单位球。设 RKHS $\\mathcal{H}_k$ 由周期核 $k(x, x') = \\cos(\\omega(x-x'))$ 生成，其中 $\\omega = 2\\pi/P$，周期 $P > 0$ 为给定值。空间 $\\mathcal{H}_k$ 由形如 $f(x) = a \\cos(\\omega x) + b \\sin(\\omega x)$ 的函数组成，其中 $a, b \\in \\mathbb{R}$，其 RKHS 范数的平方为 $\\|f\\|_{\\mathcal{H}_k}^2 = a^2+b^2$。\n\n我们感兴趣的函数类是此 RKHS 中的单位球：\n$$ \\mathcal{F} = \\left\\{ f \\in \\mathcal{H}_k : \\|f\\|_{\\mathcal{H}_k} \\le 1 \\right\\} $$\n\n计算此函数类 $\\mathcal{F}$ 在 $n=3$ 个点的特定集合 $S = \\{x_1, x_2, x_3\\} = \\{0, P/4, P/2\\}$ 上的经验 Rademacher 复杂度 $\\hat{\\mathcal{R}}_S(\\mathcal{F})$。",
            "solution": "1.  我们使用表示定理技巧 (representer trick)。对于任何由核 $k$ 生成的 RKHS $\\mathcal{H}_k$ 和任意 Rademacher 向量 $\\boldsymbol{\\sigma}$，我们有：\n    $$\n    \\sup_{\\|f\\|_{\\mathcal{H}_k} \\le 1} \\sum_{i=1}^n \\sigma_i f(x_i) = \\left\\| \\sum_{i=1}^n \\sigma_i k(x_i, \\cdot) \\right\\|_{\\mathcal{H}_k}\n    $$\n\n2.  因此，经验 Rademacher 复杂度可以表示为：\n    $$\n    \\hat{\\mathcal{R}}_S(\\mathcal{F}) = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^3 \\sigma_i k(x_i, \\cdot) \\right\\|_{\\mathcal{H}_k} \\right] = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sqrt{\\sum_{i,j=1}^3 \\sigma_i \\sigma_j k(x_i, x_j)} \\right]\n    $$\n\n3.  对于给定的点 $x_1=0, x_2=P/4, x_3=P/2$ 和 $\\omega=2\\pi/P$，我们计算核函数 $k(x_i,x_j)=\\cos(\\omega(x_i-x_j))$ 的值，得到 Gram 矩阵 $K$：\n    $$\n    K = \\begin{pmatrix}\n    \\cos(0) & \\cos(-\\pi/2) & \\cos(-\\pi) \\\\\n    \\cos(\\pi/2) & \\cos(0) & \\cos(-\\pi/2) \\\\\n    \\cos(\\pi) & \\cos(\\pi/2) & \\cos(0)\n    \\end{pmatrix} = \\begin{pmatrix}\n     1 & 0 & -1 \\\\\n     0 & 1 & 0 \\\\\n    -1 & 0 & 1\n   \\end{pmatrix}\n   $$\n   因此，根号内的项为：\n   $$\n   \\sum_{i,j=1}^3 \\sigma_i \\sigma_j k(x_i,x_j) = \\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2 - 2\\sigma_1\\sigma_3 = 3 - 2\\sigma_1\\sigma_3\n   $$\n\n4.  由于 $\\sigma_1$ 和 $\\sigma_3$ 是独立的 Rademacher 变量，乘积 $\\sigma_1\\sigma_3$ 以等概率取值为 $1$ 和 $-1$。我们计算期望：\n    $$\n    \\mathbb{E} \\left[ \\sqrt{3 - 2\\sigma_1\\sigma_3} \\right] = \\frac{1}{2} \\sqrt{3 - 2(1)} + \\frac{1}{2} \\sqrt{3 - 2(-1)} = \\frac{1}{2}(1 + \\sqrt{5})\n    $$\n\n5.  最后，将结果代入复杂度公式中 ($n=3$)：\n    $$\n    \\hat{\\mathcal{R}}_S(\\mathcal{F}) = \\frac{1}{3} \\cdot \\frac{1+\\sqrt{5}}{2} = \\frac{1+\\sqrt{5}}{6}\n    $$",
            "answer": "$$\\boxed{\\frac{1+\\sqrt{5}}{6}}$$"
        },
        {
            "introduction": "理论是灰色的，而实践之树常青。本次练习将带您从纸笔计算走向实际的编程实现，在一个精心设计的实验中比较三种经典的泛化界：VC 界、Rademacher 复杂度界和 PAC-Bayes 界。通过为一类一维阈值分类器编写代码来计算并比较这些界，您将能直观地看到它们在不同样本量下的表现，并理解为何 Rademacher 复杂度这类依赖于数据的度量通常能提供比 VC 维这类与数据无关的度量更紧致的泛化保证 。",
            "id": "3161842",
            "problem": "你必须编写一个完整、可运行的程序，该程序实现一个实验，在相同的数据生成过程和相同的假设类上比较三个泛化界。主题是统计学习，具体课题为可能近似正确学习。对于每个指定的训练样本大小，程序应计算哪个界在数值上最紧。数值上最紧定义为：将数据集上测得的经验量代入界的表达式后，得到的真实风险的最小数值上界。\n\n你必须使用的基础是一组核心定义，包括经验风险、真实风险、经验风险最小化 (ERM)、Vapnik–Chervonenkis 维度 (VC)、Rademacher 复杂度和可能近似正确-贝叶斯 (PAC-Bayes)，以及关于一致收敛和对称化的标准集中原理。在设计方法时，不要依赖任何快捷公式；从此处陈述的、适用于二元分类和有限假设类的标准定义和事实推导出每个界。\n\n实验设计：\n- 假设类：考虑实轴上具有两种极性的阈值分类器。令 $H$ 为在切点 $(k)$（其中 $k \\in \\{0,1,\\dots,m\\}$，$m$ 是当前测试用例的样本大小）处分割 $\\mathbb{R}$ 的假设组成的类。对于任何阈值索引 $k$，定义两个假设：$h_{k}^{+}$ 对前 $k$ 个排序后的点预测为 $-1$，对其余 $m-k$ 个点预测为 $+1$；$h_{k}^{-}$ 对前 $k$ 个点预测为 $+1$，对其余点预测为 $-1$。这样就产生了一个大小为 $|H| = 2(m+1)$ 的有限类。实轴上阈值分类器的 Vapnik–Chervonenkis 维度 (VC) 为 $d = 1$。\n\n- 数据分布和数据集构建：对于给定的样本大小 $m$，构建一个确定性数据集 $S_{m} = \\{(x_{i}, y_{i})\\}_{i=1}^{m}$，其中对于每个 $i \\in \\{1,\\dots,m\\}$，有 $x_{i} = \\frac{i}{m+1}$，标签 $y_{i} \\in \\{-1,+1\\}$ 由 $y_{i} = \\text{sign}(x_{i} - \\theta)$ 定义，其中 $\\theta = 0.6$。为了避免平凡可分性并使某些样本大小下的经验误差不为零，通过在 $i$ 可被 4 整除时翻转 $y_{i}$（即，如果 4 整除 $i$，则用 $-y_{i}$ 替换 $y_{i}$）来确定性地引入标签噪声。这种构建方式确保了在给定的 $m$ 下，所有界都在相同的数据集上对相同的假设类 $H$ 进行评估。\n\n- 经验风险最小化 (ERM)：对于数据集 $S_{m}$，在 $0$-$1$ 损失下，找到使经验风险 $L_{\\hat{S}_{m}}(h)$ 最小化的假设 $h^{*} \\in H$。经验风险 $L_{\\hat{S}_{m}}(h)$ 定义为在 $S_{m}$ 上的平均分类误差。\n\n- 待比较的界：\n  1. 经典 Vapnik–Chervonenkis (VC) 一致收敛界：使用 VC 维 $d$，根据经验风险 $L_{\\hat{S}_{m}}(h)$、样本大小 $m$ 和置信度参数 $\\delta \\in (0,1)$，推导真实风险 $L(h)$ 的泛化上界。\n  2. Rademacher 复杂度界：使用取值于 $\\{-1,+1\\}$ 内的有限类 $H$，根据 $L_{\\hat{S}_{m}}(h)$、样本大小 $m$、置信度参数 $\\delta$ 以及基于其基数 $|H|$ 的（对于值域有界于 $[-1,1]$ 的函数）$H$ 的经验 Rademacher 复杂度的上界，推导真实风险 $L(h)$ 的一个界。\n  3. 可能近似正确-贝叶斯 (PAC-Bayes) 界：使用 $H$ 上的均匀先验 $P$ 和一个在 ERM 假设 $h^{*}$ 上的点质量后验 $Q$，根据 $L_{\\hat{S}_{m}}(Q)$、Kullback–Leibler (KL) 散度 $\\mathrm{KL}(Q \\| P)$、样本大小 $m$ 和置信度参数 $\\delta$，推导吉布斯分类器真实风险 $L(Q)$ 的一个上界。\n\n- 置信度参数：所有测试用例均使用 $\\delta = 0.05$。\n\n每个测试用例 $(m)$ 所需的计算步骤：\n1. 按照规定构建 $S_{m}$，并将 $H$ 构建为在 $m+1$ 个切点位置上的所有两种极性阈值的集合。\n2. 计算在 $H$ 上使 $L_{\\hat{S}_{m}}(h)$ 最小化的 $h^{*}$，并记录 $L_{\\hat{S}_{m}}(h^{*})$。\n3. 使用你推导的表达式，并代入 $d = 1$、 $|H| = 2(m+1)$ 和指定的 $\\delta$，计算真实风险的三个数值上界。\n4. 通过从三个数值上界中选择最小的一个，来确定哪个界最紧。\n\n测试套件：\n- 使用以下样本大小：$m \\in \\{10, 50, 200, 1\\}$。\n- 对于集合中的每个 $m$，执行上述步骤。\n\n最终输出规范：\n- 你的程序应产生单行输出，其中包含一个整数列表，每个测试用例对应一个整数，顺序与测试套件相同。每个整数指示哪个界最紧：使用 $0$ 表示经典 Vapnik–Chervonenkis 界， $1$ 表示 Rademacher 复杂度界， $2$ 表示可能近似正确-贝叶斯界。\n- 该行必须是用方括号括起来的逗号分隔列表，例如 $[0,1,2,1]$。\n\n所有角度都是抽象的，不涉及物理单位。所有数值输出必须是纯十进制数。任何地方都不得使用百分比；任何分数都必须表示为小数。",
            "solution": "用户提供的问题已被验证，并被确定为统计学习理论领域中一个适定、有科学依据且客观的问题。该问题要求对三个标准的泛化界进行数值比较。我们现在将着手解决，这包括推导每个界的数学表达式，然后实施一个计算实验来评估它们的数值。\n\n设 $\\mathcal{D}$ 是在 $\\mathcal{X} \\times \\mathcal{Y}$ 上的一个未知 underlying 数据分布，其中 $\\mathcal{X} = \\mathbb{R}$ 且 $\\mathcal{Y} = \\{-1, +1\\}$。一个假设是一个函数 $h: \\mathcal{X} \\to \\mathcal{Y}$。假设 $h$ 的性能由 $0-1$ 损失来衡量，$\\ell(h(x), y) = \\mathbf{1}_{h(x) \\neq y}$，其中 $\\mathbf{1}$ 是指示函数。 $h$ 的真实风险（或泛化误差）是其在分布 $\\mathcal{D}$ 上的期望损失，由 $L(h) = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[\\ell(h(x), y)]$ 给出。给定一个从 $\\mathcal{D}$ 中独立同分布抽取的、大小为 $m$ 的训练样本 $S_m = \\{(x_i, y_i)\\}_{i=1}^m$，经验风险是样本上的平均损失：$L_{\\hat{S}_m}(h) = \\frac{1}{m} \\sum_{i=1}^m \\ell(h(x_i), y_i)$。\n\n泛化界的目标是根据假设的经验风险 $L_{\\hat{S}_m}(h)$、样本大小 $m$、假设类 $H$ 的属性以及置信度参数 $\\delta \\in (0,1)$，为真实风险 $L(h)$ 提供一个高概率的上界。该问题要求我们找到经验风险最小化 (ERM) 假设 $h^{*} = \\arg\\min_{h \\in H} L_{\\hat{S}_m}(h)$，然后比较其真实风险 $L(h^{*})$ 的三个不同的上界。\n\n假设类 $H$ 是一维双极性阈值分类器的集合。对于一个大小为 $m$ 的数据集，阈值由 $m+1$ 个可能的切点定义。对于每个切点 $k \\in \\{0, 1, \\dots, m\\}$，有两个假设 $h_k^+$ 和 $h_k^-$。这构成了一个大小为 $|H| = 2(m+1)$ 的有限假设类。该类的 Vapnik-Chervonenkis (VC) 维度为 $d=1$。置信度参数为 $\\delta = 0.05$。\n\n三个界的推导如下。\n\n**1. 经典 Vapnik-Chervonenkis (VC) 界**\n\nVapnik 和 Chervonenkis 的理论提供了基于假设类 VC 维度的泛化界，VC 维度衡量了假设类的复杂性。对于任何具有有限 VC 维度 $d$ 的假设类 $H$，一个标准的一致收敛结果表明，对于样本 $S_m$ 的选择，至少以 $1-\\delta$ 的概率，以下不等式对所有 $h \\in H$ 成立：\n$$\nL(h) \\leq L_{\\hat{S}_m}(h) + \\sqrt{\\frac{8}{m} \\left( d \\log\\frac{2em}{d} + \\log\\frac{4}{\\delta} \\right)}\n$$\n该界是使用对称化论证（用“影子样本”上的第二个经验风险代替真实风险）和应用于增长函数 $\\tau_H(m)$ 的集中不等式（如 Hoeffding 不等式或 Bernstein 不等式）推导出来的。增长函数由 Sauer-Shelah 引理使用 VC 维度进行界定：$\\tau_H(m) \\leq (\\frac{em}{d})^d$。\n\n由于此不等式对所有 $h \\in H$ 成立，它也必然对 ERM 假设 $h^*$ 成立。给定 $d=1$。代入 $h=h^*$ 和 $d=1$，我们得到第一个界：\n$$\nL(h^*) \\leq L_{\\hat{S}_m}(h^*) + \\sqrt{\\frac{8}{m} \\left( \\log(2em) + \\log\\frac{4}{\\delta} \\right)}\n$$\n这是 VC 界的表达式，我们将其表示为界 $0$。\n\n**2. Rademacher 复杂度界**\n\nRademacher 复杂度提供了一种更依赖于数据的复杂性度量。对于一个样本 $S_m$，函数类 $F$ 的经验 Rademacher 复杂度是 $\\hat{\\mathfrak{R}}_S(F) = \\mathbb{E}_{\\sigma} [\\sup_{f \\in F} \\frac{1}{m} \\sum_{i=1}^m \\sigma_i f(x_i)]$，其中 $\\sigma_i$ 是独立的 Rademacher 随机变量（以 $1/2$ 的概率取值 $\\pm 1$）。（期望）Rademacher 复杂度是 $\\mathfrak{R}_m(F) = \\mathbb{E}_{S \\sim \\mathcal{D}^m}[\\hat{\\mathfrak{R}}_S(F)]$。\n\n对于使用 $0-1$ 损失的二元分类，一个标准定理通过假设类 $H$（其中假设映射到 $\\{-1, +1\\}$）的 Rademacher 复杂度将真实风险与经验风险联系起来。至少以 $1-\\delta$ 的概率，对于所有 $h \\in H$：\n$$\nL(h) \\leq L_{\\hat{S}_m}(h) + 2\\mathfrak{R}_m(H) + \\sqrt{\\frac{\\log(1/\\delta)}{2m}}\n$$\n问题指明我们应使用基于 $H$ 基数的 Rademacher 复杂度的上界。对于一个有限类 $H$，Massart 引理提供了经验 Rademacher 复杂度的一个界。对于任意固定的样本 $S_m$，令 $A = \\{ (h(x_1), \\dots, h(x_m)) \\in \\mathbb{R}^m : h \\in H \\}$。由于 $h(x_i) \\in \\{-1, +1\\}$， $A$ 中任何向量的欧几里得范数是 $\\|v\\|_2 = \\sqrt{m}$。Massart 引理给出：\n$$\n\\hat{\\mathfrak{R}}_S(H) \\leq \\frac{\\max_{v \\in A} \\|v\\|_2 \\sqrt{2\\log|A|}}{m} = \\frac{\\sqrt{m}\\sqrt{2\\log|A|}}{m} = \\sqrt{\\frac{2\\log|A|}{m}}\n$$\n由于 $|A| \\leq |H|$，我们有 $\\hat{\\mathfrak{R}}_S(H) \\leq \\sqrt{\\frac{2\\log|H|}{m}}$。因为这个上界不依赖于样本 $S$，它也为期望 Rademacher 复杂度提供了上界：$\\mathfrak{R}_m(H) \\leq \\sqrt{\\frac{2\\log|H|}{m}}$。\n\n将此代入主要的 Rademacher 不等式，并应用于 $h^*$（其中 $|H|=2(m+1)$），我们得到第二个界：\n$$\nL(h^*) \\leq L_{\\hat{S}_m}(h^*) + 2\\sqrt{\\frac{2\\log(2(m+1))}{m}} + \\sqrt{\\frac{\\log(1/\\delta)}{2m}}\n$$\n这是 Rademacher 界的表达式，表示为界 $1$。\n\n**3. 可能近似正确-贝叶斯 (PAC-Bayes) 界**\n\nPAC-Bayes 框架提供了对假设类 $H$ 上的“后验”分布 $Q$ 的风险的界。PAC-Bayes 定理的一种常见形式指出，对于 $H$ 上的任何先验分布 $P$，对于样本 $S_m$，至少以 $1-\\delta$ 的概率，以下不等式对 $H$ 上的所有后验分布 $Q$ 成立：\n$$\nL(Q) \\leq L_{\\hat{S}_m}(Q) + \\sqrt{\\frac{\\mathrm{KL}(Q \\| P) + \\log\\frac{2m}{\\delta}}{2m}}\n$$\n这里，$L(Q) = \\mathbb{E}_{h \\sim Q}[L(h)]$ 和 $L_{\\hat{S}_m}(Q) = \\mathbb{E}_{h \\sim Q}[L_{\\hat{S}_m}(h)]$ 是由 $Q$ 定义的吉布斯分类器的真实风险和经验风险。$\\mathrm{KL}(Q \\| P)$ 是 $Q$ 和 $P$ 之间的 Kullback-Leibler 散度。\n\n问题指定了均匀先验 $P$，因此对于所有 $h \\in H$，$P(h) = 1/|H|$。后验 $Q$ 是在 ERM 假设 $h^*$ 上的一个点质量，即 $Q(h^*)=1$ 且对于 $h \\ne h^*$ 有 $Q(h)=0$。对于这个 $Q$ 的选择：\n- 吉布斯分类器的风险是 $L(Q) = \\sum_h Q(h)L(h) = L(h^*)$。\n- 吉布斯分类器的经验风险是 $L_{\\hat{S}_m}(Q) = \\sum_h Q(h)L_{\\hat{S}_m}(h) = L_{\\hat{S}_m}(h^*)$。\n- KL 散度是 $\\mathrm{KL}(Q \\| P) = \\sum_h Q(h) \\log\\frac{Q(h)}{P(h)} = 1 \\cdot \\log\\frac{1}{1/|H|} = \\log|H|$。\n\n将这些代入 PAC-Bayes 不等式，并设 $|H|=2(m+1)$，我们得到第三个界：\n$$\nL(h^*) \\leq L_{\\hat{S}_m}(h^*) + \\sqrt{\\frac{\\log(2(m+1)) + \\log\\frac{2m}{\\delta}}{2m}}\n$$\n这是 PAC-Bayes 界的表达式，表示为界 $2$。\n\n计算任务是对于每个给定的样本大小 $m$，使用经验确定的 $L_{\\hat{S}_m}(h^*)$，计算这三个上界的数值，并确定哪个界产生的值最小。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements an experiment to compare three generalization bounds (VC, Rademacher, PAC-Bayes)\n    for threshold classifiers on a deterministically generated dataset.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [10, 50, 200, 1]\n\n    results = []\n    for m in test_cases:\n        # Define constants for the current test case\n        delta = 0.05\n        \n        if m == 0: # Handle m=0, not in test suite but avoids division by zero.\n            results.append(0) # Default choice for trivial case\n            continue\n\n        d = 1  # VC dimension of 1D thresholds\n\n        # Step 1: Build the dataset S_m as specified.\n        # x_i = i/(m+1) for i in {1, ..., m} using 1-based indexing for definition.\n        x = np.array([(i_one_based) / (m + 1) for i_one_based in range(1, m + 1)])\n        theta = 0.6\n        y = np.sign(x - theta)\n        \n        # Introduce label noise deterministically.\n        # y_i is flipped if i is divisible by 4 (1-based index).\n        for i_one_based in range(1, m + 1):\n            if i_one_based % 4 == 0:\n                y[i_one_based - 1] *= -1\n\n        # Step 2: Find the ERM hypothesis h* and its empirical risk L_S_hat(h*).\n        min_risk = float('inf')\n        \n        # The hypothesis class H consists of 2*(m+1) hypotheses.\n        # A hypothesis is defined by a cut k in {0, ..., m} and a polarity.\n        # k=0 means the cut is before the first point, k=m means after the last.\n        for k in range(m + 1):\n            # Hypothesis h_k^+: predicts -1 up to k, then +1.\n            pred_plus = np.array([-1.0] * k + [1.0] * (m - k))\n            errors_plus = np.sum(pred_plus != y)\n            risk_plus = errors_plus / m\n            if risk_plus  min_risk:\n                min_risk = risk_plus\n\n            # Hypothesis h_k^-: predicts +1 up to k, then -1.\n            pred_minus = np.array([1.0] * k + [-1.0] * (m - k))\n            errors_minus = np.sum(pred_minus != y)\n            risk_minus = errors_minus / m\n            if risk_minus  min_risk:\n                min_risk = risk_minus\n        \n        R_emp = min_risk\n        \n        # Step 3: Compute the three bounds' numerical upper values.\n        H_card = 2 * (m + 1)\n        \n        # Bound 0: Classical Vapnik-Chervonenkis (VC)\n        # L(h) = L_emp(h) + sqrt( (8/m) * (d*log(2*e*m/d) + log(4/delta)) )\n        bound_vc_complexity = np.sqrt((8 / m) * (d * np.log(2 * np.e * m / d) + np.log(4 / delta)))\n        bound_vc = R_emp + bound_vc_complexity\n\n        # Bound 1: Rademacher complexity bound\n        # L(h) = L_emp(h) + 2*R_m(H) + sqrt(log(1/delta)/(2*m))\n        # with R_m(H) = sqrt(2*log(|H|)/m)\n        log_H_card = np.log(H_card)\n        rademacher_upper_bound = np.sqrt(2 * log_H_card / m)\n        bound_rad_complexity = 2 * rademacher_upper_bound + np.sqrt(np.log(1 / delta) / (2 * m))\n        bound_rad = R_emp + bound_rad_complexity\n        \n        # Bound 2: Probably Approximately Correct-Bayesian (PAC-Bayes)\n        # L(Q) = L_emp(Q) + sqrt((KL(Q||P) + log(2*m/delta)) / (2*m))\n        # For our specific P and Q, L(Q)=L(h*), L_emp(Q)=L_emp(h*), KL(Q||P)=log(|H|)\n        bound_pac_complexity = np.sqrt((log_H_card + np.log(2 * m / delta)) / (2 * m))\n        bound_pac = R_emp + bound_pac_complexity\n\n        # Step 4: Determine which bound is tightest (smallest numerical value).\n        bounds = [bound_vc, bound_rad, bound_pac]\n        tightest_index = np.argmin(bounds)\n        results.append(tightest_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}