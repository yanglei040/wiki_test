{
    "hands_on_practices": [
        {
            "introduction": "第一个练习将带你直接动手应用贝叶斯风险的计算公式。在这个问题中，后验概率被设定为一个逻辑斯谛函数 (logistic function) 的形式，这是许多现代分类方法的核心模型。你的任务是通过对整个特征空间上的条件错误概率进行积分，来计算模型所能达到的最小错误率，从而巩固你对如何为一个给定的概率模型计算其理论性能极限的理解。",
            "id": "3180161",
            "problem": "考虑一个二元分类场景，其中特征 $X \\in \\mathbb{R}$ 是一个服从标准正态分布的随机变量 $X \\sim \\mathcal{N}(0,1)$，类别 $Y=1$ 的后验概率函数由 $\\eta(x) = \\sigma(a x + b)$ 给出，其中 $\\sigma(z)$ 是逻辑S型函数，定义为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，$a \\in \\mathbb{R}$ 和 $b \\in \\mathbb{R}$ 是参数。在 $0$-$1$ 损失下，贝叶斯分类器在点 $x$ 处预测后验概率较高的类别，而贝叶斯风险是使用此分类器时的期望错分概率。\n\n仅从 $0$-$1$ 损失下的贝叶斯分类器和贝叶斯风险的核心定义出发，推导贝叶斯风险作为关于 $X$ 的期望的适当表达式，并实现一个数值计算程序，以针对给定的 $\\eta(x)$ 和 $X \\sim \\mathcal{N}(0,1)$ 进行计算。您的程序必须：\n- 对标准正态密度在 $x \\in \\mathbb{R}$ 上使用精确的数值积分，来为每个参数对 $(a,b)$ 评估贝叶斯风险。\n- 生成浮点数输出，以小数形式（而非百分比）表示贝叶斯风险值。\n- 将每个计算出的贝叶斯风险四舍五入到小数点后 $10$ 位，作为最终报告值。\n\n测试套件：\n计算以下参数对 $(a,b)$ 的贝叶斯风险：\n- 情况 $1$：$(a,b) = (1.0, 0.0)$。\n- 情况 $2$：$(a,b) = (0.0, 0.0)$。\n- 情况 $3$：$(a,b) = (0.0, 2.0)$。\n- 情况 $4$：$(a,b) = (5.0, 0.0)$。\n- 情况 $5$：$(a,b) = (1.5, 2.0)$。\n- 情况 $6$：$(a,b) = (-1.5, -2.0)$。\n- 情况 $7$：$(a,b) = (-2.0, 1.0)$。\n\n最终输出格式：\n您的程序应生成单行输出，按顺序包含上述情况的结果，形式为方括号内以逗号分隔的列表（例如 $[r_1,r_2,r_3,r_4,r_5,r_6,r_7]$），其中每个 $r_i$ 是情况 $i$ 的贝叶斯风险，四舍五入到小数点后 $10$ 位。",
            "solution": "用户需要一个细致、基于原理的解决方案，从问题验证开始。\n\n### 步骤 1：提取已知条件\n- **场景**：二元分类。\n- **特征**：$X \\in \\mathbb{R}$。\n- **特征分布**：$X \\sim \\mathcal{N}(0,1)$（标准正态分布）。\n- **类别 $Y=1$ 的后验概率**：$\\eta(x) = P(Y=1|X=x) = \\sigma(ax+b)$。\n- **逻辑S型函数**：$\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- **参数**：$a \\in \\mathbb{R}$，$b \\in \\mathbb{R}$。\n- **损失函数**：$0-1$ 损失。\n- **贝叶斯分类器定义**：预测后验概率较高的类别。\n- **贝叶斯风险定义**：贝叶斯分类器的期望错分概率。\n- **任务**：推导贝叶斯风险的表达式，并实现一个数值程序来计算它。\n- **数值方法**：对标准正态密度在 $x \\in \\mathbb{R}$ 上进行精确的数值积分。\n- **测试用例**：给定以下参数对 $(a,b)$ 用于计算：\n    1. $(1.0, 0.0)$\n    2. $(0.0, 0.0)$\n    3. $(0.0, 2.0)$\n    4. $(5.0, 0.0)$\n    5. $(1.5, 2.0)$\n    6. $(-1.5, -2.0)$\n    7. $(-2.0, 1.0)$\n- **输出要求**：浮点数值，四舍五入到小数点后 $10$ 位，格式化为方括号内的逗号分隔列表。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学或事实的合理性**：该问题使用统计学习理论中标准的、成熟的概念进行表述，包括贝叶斯分类器、贝叶斯风险、后验概率、逻辑S型函数和正态分布。该设置代表了一个经典的逻辑回归模型。该问题在科学上是合理的。\n2.  **非形式化或不相关**：该问题是一项与贝叶斯分类器及其相关风险这一特定主题直接相关的形式化数学和计算任务。它不是隐喻性的或不相关的。\n3.  **不完整或矛盾的设置**：所有必要的组成部分都已提供：特征分布、后验概率的形式、损失函数以及待计算量的定义。该问题是自洽且一致的。\n4.  **不切实际或不可行**：该模型是统计学和机器学习中的标准模型。所需的计算涉及对一个表现良好的函数进行数值积分，这是一项标准且可行的数值任务。\n5.  **不适定或结构不良**：对于任何给定的参数 $(a,b)$，贝叶斯风险是一个唯一定义的量。该问题是适定的且结构清晰。\n6.  **伪深刻、琐碎或同义反复**：该问题需要从第一性原理进行正确推导和非平凡的数值计算。虽然某些测试用例（例如 $a=0$）会产生更简单的解析结果，但一般情况并非如此，需要真正应用数值方法。\n7.  **超出科学可验证性范围**：推导过程可进行数学证明，数值结果可通过独立计算进行验证。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解决方案。\n\n### 推导与方法\n\n目标是计算贝叶斯风险，它是在二元分类问题中期望 $0-1$ 损失的最小可能值。\n\n1.  **贝叶斯分类器规则**：$0-1$ 损失函数定义为 $L(y, \\hat{y}) = I(y \\neq \\hat{y})$，其中 $I(\\cdot)$ 是指示函数。目标是找到一个分类器 $g(x)$，以最小化风险 $R(g) = E[L(Y, g(X))]$。这通过为每个 $x$ 最小化条件风险 $E[L(Y, g(X))|X=x]$ 来实现。\n\n    在 $x$ 点，对于一个预测 $\\hat{y}$，其条件风险是：\n    $$ E[L(Y, \\hat{y})|X=x] = P(Y \\neq \\hat{y} | X=x) $$\n    如果我们预测类别 $1$（即 $\\hat{y}=1$），条件风险是 $P(Y=0|X=x) = 1 - P(Y=1|X=x) = 1 - \\eta(x)$。\n    如果我们预测类别 $0$（即 $\\hat{y}=0$），条件风险是 $P(Y=1|X=x) = \\eta(x)$。\n\n    贝叶斯分类器 $g^*(x)$ 通过选择后验概率较高的类别来最小化此条件风险：\n    $$ g^*(x) = \\begin{cases} 1  \\text{if } \\eta(x) \\geq 1 - \\eta(x) \\\\ 0  \\text{if } \\eta(x)  1 - \\eta(x) \\end{cases} $$\n    条件 $\\eta(x) \\geq 1 - \\eta(x)$ 可简化为 $\\eta(x) \\geq 0.5$。\n\n2.  **贝叶斯风险计算**：与贝叶斯分类器相关的风险是贝叶斯风险，记为 $R^*$。在使用贝叶斯分类器时，在点 $x$ 的条件风险是较小可能类别的概率，即 $\\min\\{\\eta(x), 1-\\eta(x)\\}$。\n\n    贝叶斯风险是此条件风险在 $X$ 分布上的期望：\n    $$ R^* = E_X[\\min\\{\\eta(X), 1-\\eta(X)\\}] $$\n    已知 $X$ 服从一个概率密度函数（PDF）为 $p(x)$ 的分布，该期望由以下积分给出：\n    $$ R^* = \\int_{-\\infty}^{\\infty} \\min\\{\\eta(x), 1-\\eta(x)\\} p(x) dx $$\n\n3.  **应用于特定问题**：\n    - 特征 $X$ 服从标准正态分布，$X \\sim \\mathcal{N}(0,1)$，因此其概率密度函数为 $p(x) = \\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}$。\n    - 后验概率为 $\\eta(x) = \\sigma(ax+b) = \\frac{1}{1 + e^{-(ax+b)}}$。\n\n    我们需要简化项 $\\min\\{\\eta(x), 1-\\eta(x)\\}$。注意，逻辑函数 $\\sigma(z)$ 是单调递增的，当 $z0$ 时 $\\sigma(z)  0.5$，$\\sigma(0)=0.5$，当 $z0$ 时 $\\sigma(z)  0.5$。\n    因此，$\\eta(x) = \\sigma(ax+b) \\geq 0.5$ 当且仅当其参数 $ax+b \\geq 0$。\n\n    - 如果 $ax+b \\geq 0$，则 $\\eta(x) \\geq 0.5$，因此 $\\min\\{\\eta(x), 1-\\eta(x)\\} = 1-\\eta(x)$。\n    - 如果 $ax+b  0$，则 $\\eta(x)  0.5$，因此 $\\min\\{\\eta(x), 1-\\eta(x)\\} = \\eta(x)$。\n\n    我们可以使用恒等式 $1 - \\sigma(z) = 1 - \\frac{1}{1+e^{-z}} = \\frac{e^{-z}}{1+e^{-z}} = \\frac{1}{e^z+1} = \\sigma(-z)$。\n    所以，$1-\\eta(x) = 1-\\sigma(ax+b) = \\sigma(-(ax+b))$。\n    因此，条件风险可以紧凑地写为：\n    $$ \\min\\{\\eta(x), 1-\\eta(x)\\} = \\begin{cases} \\sigma(-(ax+b))  \\text{if } ax+b \\geq 0 \\\\ \\sigma(ax+b)  \\text{if } ax+b  0 \\end{cases} $$\n    这等价于 $\\sigma(-|ax+b|)$。贝叶斯风险的表达式变为：\n    $$ R^* = \\int_{-\\infty}^{\\infty} \\sigma(-|ax+b|) \\phi(x) dx $$\n    代入 $\\sigma$ 和 $\\phi$ 的函数形式：\n    $$ R^* = \\int_{-\\infty}^{\\infty} \\frac{1}{1 + e^{|ax+b|}} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx $$\n\n4.  **数值评估**：该积分没有通用的闭式解析解，必须进行数值评估。我们将使用数值求积法来计算在域 $(-\\infty, \\infty)$ 上的定积分。Python 中的 `scipy.integrate.quad` 函数非常适合此目的。对于测试套件中提供的每一对参数 $(a,b)$，我们将定义被积函数 $f(x;a,b) = \\frac{\\phi(x)}{1 + e^{|ax+b|}}$ 并计算积分 $\\int_{-\\infty}^{\\infty} f(x;a,b) dx$。结果将按要求四舍五入到小数点后 10 位。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the Bayes risk for a binary classification problem with a normally\n    distributed feature and a logistic posterior probability function for\n    several sets of parameters.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.0),   # Case 1\n        (0.0, 0.0),   # Case 2\n        (0.0, 2.0),   # Case 3\n        (5.0, 0.0),   # Case 4\n        (1.5, 2.0),   # Case 5\n        (-1.5, -2.0), # Case 6\n        (-2.0, 1.0)   # Case 7\n    ]\n\n    results = []\n    \n    for a, b in test_cases:\n        # The Bayes risk R* is the expectation of the minimum of the posterior\n        # probabilities for the two classes, E[min(eta(X), 1-eta(X))], where\n        # X ~ N(0,1). This expectation is calculated by integrating the\n        # conditional risk over the distribution of X.\n        #\n        # Conditional risk at x: min(eta(x), 1-eta(x))\n        #   eta(x) = sigma(ax+b) = 1 / (1 + exp(-(ax+b)))\n        #   1-eta(x) = sigma(-(ax+b))\n        #   min(eta(x), 1-eta(x)) = sigma(-|ax+b|) = 1 / (1 + exp(|ax+b|))\n        #\n        # Distribution of X is standard normal, with PDF phi(x).\n        # Integrand for Bayes risk: (1 / (1 + exp(|ax+b|))) * phi(x)\n\n        # Define the integrand function for numerical integration.\n        # It takes x, a, and b as arguments.\n        def integrand(x, a_val, b_val):\n            # Calculate the conditional error probability\n            # sigma(-|ax+b|) = 1 / (1 + exp(|ax+b|))\n            conditional_risk = 1.0 / (1.0 + np.exp(np.abs(a_val * x + b_val)))\n            \n            # Multiply by the standard normal PDF\n            # norm.pdf(x, 0, 1) provides phi(x)\n            return conditional_risk * norm.pdf(x, loc=0, scale=1)\n\n        # Perform numerical integration over (-inf, +inf)\n        # quad returns a tuple (integral_value, error_estimate)\n        bayes_risk, _ = quad(integrand, -np.inf, np.inf, args=(a, b))\n        \n        # Round the result to 10 decimal places as specified\n        rounded_risk = round(bayes_risk, 10)\n        results.append(rounded_risk)\n\n    # Format the final output string as [r1,r2,...,r7]\n    # Using .10f format specifier to avoid scientific notation for small numbers\n    # and ensure trailing zeros if necessary.\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了基本计算之后，这个问题将引导你探索一个更复杂且富有洞察力的场景，其中类条件概率密度会导致非平凡的决策边界。你将会发现，当其中一个类别的分布是混合模型时，最优的贝叶斯决策区域可能会变成一个非凸集（由多个不相交的区间构成）。这个练习表明，最优分类器并不总是一个简单的阈值，并强调了使用数值方法来分析其结构和性能的必要性。",
            "id": "3180228",
            "problem": "考虑一个统计学习中的二元分类问题，其特征为 $X \\in \\mathbb{R}$，标签为 $Y \\in \\{0,1\\}$。类别先验概率为 $P(Y=0)=\\pi_0$ 和 $P(Y=1)=\\pi_1$，其中 $\\pi_0,\\pi_1 \\in (0,1)$ 且 $\\pi_0+\\pi_1=1$。类条件分布指定如下：\n- 对于类别 $Y=1$，条件分布 $X \\mid Y=1$ 是两个正态分量的双峰混合分布：其概率密度函数 (PDF) 为\n$$\nf_1(x) \\equiv w \\,\\phi(x;\\mu_{1a},\\sigma_{1a}) + (1-w)\\,\\phi(x;\\mu_{1b},\\sigma_{1b}),\n$$\n其中 $w \\in (0,1)$ 是混合权重，$\\phi(x;\\mu,\\sigma)$ 表示均值为 $\\mu$、标准差为 $\\sigma$ 的正态 PDF，即\n$$\n\\phi(x;\\mu,\\sigma) \\equiv \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n$$\n- 对于类别 $Y=0$，条件分布 $X \\mid Y=0$ 是单峰正态分布，其 PDF 为\n$$\nf_0(x) \\equiv \\phi(x;\\mu_0,\\sigma_0).\n$$\n\n分类决策 $\\hat{Y}(x) \\in \\{0,1\\}$ 会根据以下 $2\\times 2$ 损失设定产生损失：预测正确类别的损失为零，当真实类别为 $0$ 时预测为 $1$ 的损失为 $\\lambda_{10}  0$（假阳性），当真实类别为 $1$ 时预测为 $0$ 的损失为 $\\lambda_{01}  0$（假阴性）。贝叶斯分类器被定义为在每个 $x$ 处最小化条件期望损失的决策规则，贝叶斯风险是贝叶斯分类器在 $(X,Y)$ 联合分布下的期望损失。\n\n任务：\n1. 从核心定义（后验概率的贝叶斯定理和条件风险最小化）出发，推导此设置下的贝叶斯决策规则。解释为什么当 $f_1$ 是双峰而 $f_0$ 是单峰时，用于预测类别 $1$ 的贝叶斯决策区域在 $\\mathbb{R}$ 中可以是一个非凸集（不相交区间的并集），并根据原始量，确定在对称 $0$-$1$ 损失和非对称损失两种情况下刻画类别 $1$ 决策区域的不等式。\n2. 实现一个数值方法来：\n   - 在对称 $0$-$1$ 损失（即 $\\lambda_{10}=\\lambda_{01}=1$）下，确定预测类别 $1$ 的贝叶斯决策区域中不相交区间的数量。\n   - 确定该决策区域是否为非凸（当且仅当区间数量超过 $1$ 时，返回布尔值 `true`）。\n   - 计算对称 $0$-$1$ 损失下的贝叶斯错误率。将此错误率表示为十进制形式的实数（无百分号）。\n   - 计算指定非对称损失 $(\\lambda_{10},\\lambda_{01})$ 下的贝叶斯风险。将此风险表示为十进制形式的实数。\n\n使用纯粹的数学和算法推理。不要引用未从所述定义中推导出的公式。您的程序必须以数值方式执行计算，并且不得需要任何外部输入。\n\n测试套件：\n对于每个测试用例，参数以元组形式给出\n$$\n(\\pi_0,\\pi_1,w,\\mu_{1a},\\sigma_{1a},\\mu_{1b},\\sigma_{1b},\\mu_0,\\sigma_0,\\lambda_{10},\\lambda_{01}),\n$$\n所有量均为实值，且 $\\pi_0+\\pi_1=1$，$w \\in (0,1)$，所有标准差均为严格正数。测试套件如下：\n\n- 情况 A（具有清晰双峰 $f_1$ 和较宽单峰 $f_0$ 的一般正常路径，对称损失）：\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- 情况 B（$Y=0$ 具有占优先验的边界趋势，对称损失）：\n$$\n(0.8,\\,0.2,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- 情况 C（具有较窄 $f_0$ 和偏移混合分量的替代形状，对称损失）：\n$$\n(0.5,\\,0.5,\\,0.6,\\,-2.5,\\,0.5,\\,1.8,\\,0.5,\\,0.0,\\,0.7,\\,1.0,\\,1.0).\n$$\n- 情况 D（强调假阳性惩罚的非对称损失，形状与情况 A 相同）：\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,3.0).\n$$\n\n算法输出规范：\n- 对于每个测试用例，生成一个形式为 $[N,B,E,R]$ 的结果列表，其中：\n  - $N$ 是一个整数，表示在对称 $0$-$1$ 损失下，类别 $1$ 的贝叶斯决策区域中不相交区间的数量。\n  - $B$ 是一个布尔值，当且仅当在对称 $0$-$1$ 损失下决策区域是非凸时（即 $N1$）为 `true`。\n  - $E$ 是一个浮点数，表示在对称 $0$-$1$ 损失下的贝叶斯错误率。\n  - $R$ 是一个浮点数，表示在该测试用例指定 $(\\lambda_{10},\\lambda_{01})$ 下的贝叶斯风险。\n- 您的程序应生成单行输出，其中包含四个用例的结果，形式为逗号分隔的列表，并用方括号括起来，例如：\n$$\n[\\,[N_1,B_1,E_1,R_1],\\,[N_2,B_2,E_2,R_2],\\,[N_3,B_3,E_3,R_3],\\,[N_4,B_4,E_4,R_4]\\,].\n$$\n不涉及物理单位或角度。所有概率和风险必须以十进制（而非百分比）表示。",
            "solution": "该问题要求对一个特征维度为一的二元分类问题的贝叶斯分类器进行理论和数值分析。\n\n### **第一部分：理论推导与分析**\n\n**1. 基于条件风险最小化的贝叶斯决策规则**\n\n设 $Y \\in \\{0, 1\\}$ 为真实类别标签，$\\hat{Y}(x) \\in \\{0, 1\\}$ 为观测到 $X=x$ 时的预测类别。损失函数由矩阵 $L$ 给出，$L(i, j)$ 是当真实类别为 $j$ 时预测类别为 $i$ 的损失。根据问题陈述，我们有：\n- $L(0, 0) = 0$ (正确预测类别 $0$)\n- $L(1, 1) = 0$ (正确预测类别 $1$)\n- $L(1, 0) = \\lambda_{10}  0$ (假阳性，第一类错误)\n- $L(0, 1) = \\lambda_{01}  0$ (假阴性，第二类错误)\n\n贝叶斯分类器旨在最小化每个 $x$ 的条件期望损失（或条件风险）。给定 $X=x$，预测类别 $k$ 的条件风险是：\n$$\nR(k | x) = \\mathbb{E}[L(k, Y) | X=x] = \\sum_{j=0}^{1} L(k, j) P(Y=j | X=x)\n$$\n让我们为两个可能的决策 $\\hat{Y}(x)=0$ 和 $\\hat{Y}(x)=1$ 计算这个值。\n\n预测类别 $0$ 的条件风险是：\n$$\nR(\\hat{Y}=0 | x) = L(0, 0)P(Y=0 | x) + L(0, 1)P(Y=1 | x) = 0 \\cdot P(Y=0 | x) + \\lambda_{01} P(Y=1 | x) = \\lambda_{01} P(Y=1 | x)\n$$\n预测类别 $1$ 的条件风险是：\n$$\nR(\\hat{Y}=1 | x) = L(1, 0)P(Y=0 | x) + L(1, 1)P(Y=1 | x) = \\lambda_{10} P(Y=0 | x) + 0 \\cdot P(Y=1 | x) = \\lambda_{10} P(Y=0 | x)\n$$\n贝叶斯决策规则是选择使该条件风险最小化的类别。因此，我们当且仅当以下条件成立时预测类别 $1$：\n$$\nR(\\hat{Y}=1 | x)  R(\\hat{Y}=0 | x)\n$$\n$$\n\\lambda_{10} P(Y=0 | x)  \\lambda_{01} P(Y=1 | x)\n$$\n这个不等式定义了决策准则。为了使其可操作，我们使用贝叶斯定理来表示后验概率 $P(Y=j|x)$。后验概率正比于似然乘以先验：\n$$\nP(Y=j | x) = \\frac{f(x|Y=j) P(Y=j)}{f(x)} = \\frac{f_j(x) \\pi_j}{f(x)}\n$$\n其中 $f_j(x)$ 是类别 $j$ 的类条件 PDF，$\\pi_j$ 是类别 $j$ 的先验概率，$f(x) = \\sum_{j=0}^{1} f_j(x) \\pi_j$ 是 $X$ 的边际 PDF。\n\n将这些代入决策规则不等式：\n$$\n\\lambda_{10} \\frac{f_0(x) \\pi_0}{f(x)}  \\lambda_{01} \\frac{f_1(x) \\pi_1}{f(x)}\n$$\n由于对所有 $x$ 都有 $f(x)  0$，我们可以将不等式两边乘以 $f(x)$ 而不改变不等号方向。这得到了预测类别 $1$ 的决策规则的最终形式：\n$$\n\\lambda_{10} f_0(x) \\pi_0  \\lambda_{01} f_1(x) \\pi_1\n$$\n这可以表示为似然比检验。如果满足以下条件，我们预测类别 $1$：\n$$\n\\frac{f_1(x)}{f_0(x)}  \\frac{\\lambda_{10} \\pi_0}{\\lambda_{01} \\pi_1}\n$$\n类别 $1$ 的决策区域，记为 $\\mathcal{R}_1$，是集合 $\\{x \\in \\mathbb{R} \\mid \\lambda_{01} \\pi_1 f_1(x)  \\lambda_{10} \\pi_0 f_0(x)\\}$。\n\n对于对称 $0-1$ 损失的特殊情况，我们有 $\\lambda_{10} = \\lambda_{01} = 1$。规则简化为：\n$$\nf_1(x) \\pi_1  f_0(x) \\pi_0\n$$\n这是最大后验 (MAP) 规则，它表明我们应该选择具有更高后验概率的类别。\n\n**2. 决策区域的非凸性**\n\n决策边界是使不等式两边相等的点 $x$：\n$$\n\\lambda_{01} \\pi_1 f_1(x) = \\lambda_{10} \\pi_0 f_0(x)\n$$\n让我们定义一个函数 $g(x) = \\lambda_{01} \\pi_1 f_1(x) - \\lambda_{10} \\pi_0 f_0(x)$。类别 $1$ 的决策区域是 $g(x)0$ 的地方。边界是 $g(x)=0$ 的根。\n\n在这个问题中，$f_0(x)$ 是一个单峰正态 PDF，$\\phi(x;\\mu_0,\\sigma_0)$。函数 $f_1(x)$ 是两个正态 PDF 的双峰混合，$w \\,\\phi(x;\\mu_{1a},\\sigma_{1a}) + (1-w)\\,\\phi(x;\\mu_{1b},\\sigma_{1b})$。因此，$g(x)$ 是三个高斯函数的加权和与差。\n\n单个高斯函数有一个峰值。高斯混合可以是多峰的。函数 $\\lambda_{01} \\pi_1 f_1(x)$ 可以有两个不同的峰（如果分量分离得足够远），而函数 $\\lambda_{10} \\pi_0 f_0(x)$ 只有一个峰。一个有两个峰的曲线可以与一个只有一个峰的曲线多次相交。例如，如果双峰分布的两个峰相对于位于它们之间的单峰分布足够高，双峰曲线可能会升到单峰曲线上方，在中间骤降到其下方，然后再次升到其上方。这种情况会产生四个交点（$g(x)=0$ 的根）。\n\n设根为 $r_1  r_2  r_3  r_4$。$g(x)$ 的符号可能会在这些根之间交替。例如，如果当 $|x|$ 很大时 $g(x)$ 为负，那么类别 $1$ 的决策区域（其中 $g(x)0$）可能是两个不相交区间的并集，例如 $\\mathcal{R}_1 = (r_1, r_2) \\cup (r_3, r_4)$。不相交区间的并集是 $\\mathbb{R}$ 中的一个非凸集。因此，$f_1(x)$ 的双峰性是允许非凸决策区域的关键特征。\n\n**3. 贝叶斯错误率与贝叶斯风险**\n\n**贝叶斯风险** $R$ 是分类器可能达到的最小期望损失，由贝叶斯决策规则 $\\hat{Y}_{Bayes}(x)$ 实现。\n$$\nR = \\mathbb{E}[L(\\hat{Y}_{Bayes}(X), Y)] = \\int_{-\\infty}^{\\infty} \\mathbb{E}[L(\\hat{Y}_{Bayes}(x), Y) | X=x] f(x) dx\n$$\n内部的期望是在 $x$ 处最优决策的条件风险，即 $\\min\\{R(\\hat{Y}=0 | x), R(\\hat{Y}=1 | x)\\}$。\n$$\nR = \\int_{-\\infty}^{\\infty} \\min\\{\\lambda_{01} P(Y=1|x), \\lambda_{10} P(Y=0|x)\\} f(x) dx\n$$\n使用关系 $P(Y=j|x)f(x) = \\pi_j f_j(x)$，这简化为：\n$$\nR = \\int_{-\\infty}^{\\infty} \\min\\{\\lambda_{01} \\pi_1 f_1(x), \\lambda_{10} \\pi_0 f_0(x)\\} dx\n$$\n**贝叶斯错误率** $E$ 是对称 $0-1$ 损失（$\\lambda_{10} = \\lambda_{01} = 1$）下贝叶斯风险的特例。\n$$\nE = \\int_{-\\infty}^{\\infty} \\min\\{\\pi_1 f_1(x), \\pi_0 f_0(x)\\} dx\n$$\n这些积分可以通过数值计算。该过程涉及找到相应 $g(x)$ 函数的根以确定决策区域。设 $\\mathcal{R}_0$ 和 $\\mathcal{R}_1$ 分别是我们预测类别 $0$ 和 $1$ 的区域。积分可以重写为：\n- 对于错误率 $E$：$E = \\int_{\\mathcal{R}_1} \\pi_0 f_0(x) dx + \\int_{\\mathcal{R}_0} \\pi_1 f_1(x) dx$。\n- 对于风险 $R$：$R = \\int_{\\mathcal{R}'_1} \\lambda_{10} \\pi_0 f_0(x) dx + \\int_{\\mathcal{R}'_0} \\lambda_{01} \\pi_1 f_1(x) dx$，其中 $\\mathcal{R}'_0, \\mathcal{R}'_1$ 是非对称损失的区域。\n\n### **第二部分：数值实现策略**\n\n每个测试用例的数值解决方案包括以下步骤：\n1.  **定义函数**：实现 Python 函数，用于 PDF $f_0(x)$ 和 $f_1(x)$，以及决策函数 $g_{sym}(x) = \\pi_1 f_1(x) - \\pi_0 f_0(x)$ 和 $g_{asym}(x) = \\lambda_{01} \\pi_1 f_1(x) - \\lambda_{10} \\pi_0 f_0(x)$。\n2.  **求根**：为了找到对称损失下不相交区间的数量，确定 $g_{sym}(x)=0$ 的所有实根。一个稳健的方法是扫描一个宽范围的 $x$ 值以寻找符号变化，从而框定根的位置。然后，在每个框定的区间内使用像 `scipy.optimize.brentq` 这样的数值求解器来高精度地找到根。\n3.  **确定区间和 N, B**：排序后的根 $\\{r_1, r_2, \\dots, r_k\\}$ 将 $\\mathbb{R}$ 划分为 $k+1$ 个区间。通过在每个区间内的一个样本点上测试 $g_{sym}(x)$ 的符号，我们可以识别出构成决策区域 $\\mathcal{R}_1$ 的不相交区间集合。这些区间的数量给出 $N$，如果 $N1$，则 $B$ 为 `true`。\n4.  **计算贝叶斯错误率 E**：利用对称情况下的根，将实线 $(-\\infty, \\infty)$ 划分为多个区间。对于每个区间，确定它属于 $\\mathcal{R}_0$ 还是 $\\mathcal{R}_1$。使用 `scipy.integrate.quad` 对适当的误差项（在 $\\mathcal{R}_1$ 上是 $\\pi_0 f_0(x)$，在 $\\mathcal{R}_0$ 上是 $\\pi_1 f_1(x)$）进行数值积分，并将结果相加。\n5.  **计算贝叶斯风险 R**：对非对称情况重复此过程。找到 $g_{asym}(x)=0$ 的根。使用这些新根来定义非对称决策区域 $\\mathcal{R}'_0$ 和 $\\mathcal{R}'_1$。对适当的风险项（在 $\\mathcal{R}'_1$ 上是 $\\lambda_{10} \\pi_0 f_0(x)$，在 $\\mathcal{R}'_0$ 上是 $\\lambda_{01} \\pi_1 f_1(x)$）进行数值积分，并将结果相加，以找到总贝叶斯风险 $R$。\n该过程将为提供的测试套件实现。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case B\n        (0.8, 0.2, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case C\n        (0.5, 0.5, 0.6, -2.5, 0.5, 1.8, 0.5, 0.0, 0.7, 1.0, 1.0),\n        # Case D\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(case))\n\n    # Format the final output string as a compact list of lists\n    # e.g., [[N1,B1,E1,R1],[N2,B2,E2,R2],...]\n    output_string = str(results).replace(\" \", \"\").replace(\"True\", \"true\").replace(\"False\", \"false\")\n    print(output_string)\n\ndef process_case(params):\n    \"\"\"\n    Computes [N, B, E, R] for a single test case.\n    \"\"\"\n    pi0, pi1, w, mu1a, sigma1a, mu1b, sigma1b, mu0, sigma0, lam10, lam01 = params\n\n    # 1. Define PDFs\n    def f1(x):\n        return w * norm.pdf(x, loc=mu1a, scale=sigma1a) + (1-w) * norm.pdf(x, loc=mu1b, scale=sigma1b)\n\n    def f0(x):\n        return norm.pdf(x, loc=mu0, scale=sigma0)\n\n    # 2. Analyze symmetric loss case for N, B, E\n    def g_sym(x):\n        return pi1 * f1(x) - pi0 * f0(x)\n\n    def find_roots(func):\n        # Determine a sufficiently wide range to find all relevant roots\n        all_mus = [mu1a, mu1b, mu0]\n        all_sigmas = [sigma1a, sigma1b, sigma0]\n        search_min = min(all_mus) - 15 * max(all_sigmas)\n        search_max = max(all_mus) + 15 * max(all_sigmas)\n        \n        roots = []\n        grid = np.linspace(search_min, search_max, 20001)\n        y_vals = func(grid)\n        \n        for i in range(len(grid) - 1):\n            if np.sign(y_vals[i]) != np.sign(y_vals[i+1]):\n                try:\n                    root = brentq(func, grid[i], grid[i+1])\n                    roots.append(root)\n                except (ValueError, RuntimeError):\n                    # In case of issues, we skip, as sign change is the main trigger.\n                    pass\n        return sorted(list(set(roots)))\n\n    roots_sym = find_roots(g_sym)\n    \n    # 3. Determine N and B\n    R1_sym_intervals = []\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n        if g_sym(test_point) > 0:\n            R1_sym_intervals.append((a, b))\n    \n    N = len(R1_sym_intervals)\n    B = N > 1\n\n    # 4. Compute Bayes Error Rate E (symmetric loss)\n    # E = integral over R of min(pi0*f0, pi1*f1)\n    # This is equivalent to integrating pi1*f1 over R0 and pi0*f0 over R1\n    error = 0.0\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_sym(test_point) > 0: # Predict 1, error is if Y=0\n            error += quad(lambda x: pi0 * f0(x), a, b)[0]\n        else: # Predict 0, error is if Y=1\n            error += quad(lambda x: pi1 * f1(x), a, b)[0]\n    E = error\n    \n    # 5. Compute Bayes Risk R (asymmetric loss)\n    def g_asym(x):\n        return lam01 * pi1 * f1(x) - lam10 * pi0 * f0(x)\n\n    roots_asym = find_roots(g_asym)\n    \n    risk = 0.0\n    all_asym_points = [-np.inf] + roots_asym + [np.inf]\n    for i in range(len(all_asym_points) - 1):\n        a, b = all_asym_points[i], all_asym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_asym(test_point) > 0: # Predict 1, risk is lambda_10 * P(Y=0)\n            risk += quad(lambda x: lam10 * pi0 * f0(x), a, b)[0]\n        else: # Predict 0, risk is lambda_01 * P(Y=1)\n            risk += quad(lambda x: lam01 * pi1 * f1(x), a, b)[0]\n    R = risk\n\n    return [N, B, E, R]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "最后一个练习将我们的焦点从分类器本身的属性转移到它所处理的数据的性质上。我们将研究如何对连续特征进行量化——这是数字信号处理和数据存储中的常见步骤——以及这个过程如何不可避免地为任何分类器的性能设定一个上限。此处的挑战在于，你需要确定最优的量化边界以最小化贝叶斯误差，这将为你提供关于信息表示与分类精度之间基本权衡的深刻见解。",
            "id": "3180184",
            "problem": "一个两类分类问题通过一个有损测量设备进行观测，该设备输出一个有限分辨率的离散符号。设类别为 $\\mathcal{C}_{0}$ 和 $\\mathcal{C}_{1}$，其先验概率相等，为 $\\pi_{0} = \\pi_{1} = \\frac{1}{2}$。一个实值特征 $X \\in [0,1]$ 从类条件概率密度函数 $f_{0}(x)$ 和 $f_{1}(x)$ 中抽取，这些函数在以下五个连续子区间上是分段常数：\n$$\nS_{1} = [0,\\tfrac{1}{6}], \\quad\nS_{2} = [\\tfrac{1}{6},\\tfrac{1}{2}], \\quad\nS_{3} = [\\tfrac{1}{2},\\tfrac{2}{3}], \\quad\nS_{4} = [\\tfrac{2}{3},\\tfrac{11}{12}], \\quad\nS_{5} = [\\tfrac{11}{12},1].\n$$\n在这些子区间上，密度函数为：\n$$\nf_{1}(x) =\n\\begin{cases}\n1  x \\in S_{1}, \\\\\n\\frac{1}{2}  x \\in S_{2}, \\\\\n2  x \\in S_{3}, \\\\\n\\frac{1}{2}  x \\in S_{4}, \\\\\n\\frac{5}{2}  x \\in S_{5},\n\\end{cases}\n\\qquad\nf_{0}(x) =\n\\begin{cases}\n\\frac{1}{2}  x \\in S_{1}, \\\\\n\\frac{3}{2}  x \\in S_{2}, \\\\\n1  x \\in S_{3}, \\\\\n\\frac{2}{3}  x \\in S_{4}, \\\\\n1  x \\in S_{5},\n\\end{cases}\n$$\n且每个密度函数在 $[0,1]$ 上的积分为 $1$。该设备通过在 $[0,1]$ 中选择 $K-1$ 个内部边界，将 $X$ 量化为 $K = 2^{b}$ 个连续的区间（或箱），并输出包含 $X$ 的区间的索引。分类器仅观测这个 $b$ 比特的符号，并且必须根据区间索引，通过一个确定性函数输出一个类别标签。使用0-1损失下的贝叶斯风险定义，在 $K-1$ 个量化边界的所有可能放置方式和所有仅依赖于区间索引的决策规则中，确定当 $b = 2$ 时可实现的最小贝叶斯错误率。请以精确值的形式给出最终答案；无需四舍五入。",
            "solution": "本题旨在求一个两类分类问题中的最小可实现贝叶斯错误率，其中实值特征 $X$ 首先被量化到固定数量的区间中。\n\n### 步骤1：问题的形式化\n\n设两个类别为 $\\mathcal{C}_{0}$ 和 $\\mathcal{C}_{1}$。先验概率给定为相等：$\\pi_{0} = P(\\mathcal{C}_{0}) = \\frac{1}{2}$ 和 $\\pi_{1} = P(\\mathcal{C}_{1}) = \\frac{1}{2}$。特征 $X$ 具有类条件概率密度函数 (PDF) $f_{0}(x)$ 和 $f_{1}(x)$。\n\n分类器不直接观测 $X$。相反，区间 $[0,1]$ 被划分为 $K$ 个连续的区间，记为 $B_{j}$（$j=1, \\dots, K$）。分类器仅观测到 $X$ 落入的区间 $B_j$ 所对应的索引 $j$。对于本问题，比特数 $b=2$，因此区间数量为 $K=2^{2}=4$。该划分由 $K-1=3$ 个边界 $t_1, t_2, t_3$ 定义，满足 $0  t_1  t_2  t_3  1$。这些区间是 $B_1 = [0, t_1]$，$B_2 = (t_1, t_2]$，$B_3 = (t_2, t_3]$ 和 $B_4 = (t_3, 1]$。区间的边界是点，其概率测度为零，因此使用开区间还是闭区间不影响积分结果。\n\n决策规则是一个确定性函数 $g: \\{1,2,3,4\\} \\to \\{\\mathcal{C}_{0}, \\mathcal{C}_{1}\\}$，它将一个区间索引映射到一个类别标签。我们寻求找到最优划分 $\\{B_j\\}_{j=1}^4$ 和最优决策规则 $g$，以共同最小化贝叶斯错误率。\n\n对于0-1损失函数，贝叶斯风险即为错分概率 $P_e$。\n$P_e = P(g(\\text{bin}(X)) \\neq \\text{真实类别})$。\n这可以表示为对所有区间的求和：\n$$P_e = \\sum_{j=1}^{4} P(\\text{错误且 } X \\in B_j)$$\n$$P_e = \\sum_{j=1}^{4} P(\\text{错误} | X \\in B_j) P(X \\in B_j)$$\n对于一个给定的划分 $\\{B_j\\}$，对每个区间 $j$ 的最优决策规则 $g^*$ 是选择具有较高后验概率的类别：\n$$g^*(j) = \\underset{k \\in \\{0,1\\}}{\\operatorname{argmax}} P(\\mathcal{C}_k | X \\in B_j)$$\n使用贝叶斯定理，$P(\\mathcal{C}_k | X \\in B_j) = \\frac{P(X \\in B_j | \\mathcal{C}_k) P(\\mathcal{C}_k)}{P(X \\in B_j)}$。由于先验概率 $\\pi_0 = \\pi_1 = \\frac{1}{2}$ 相等，这简化为选择使似然 $P(X \\in B_j | \\mathcal{C}_k) = \\int_{B_j} f_k(x) dx$ 最大化的类别。\n令 $p_{k,j} = \\int_{B_j} f_k(x) dx$。决策规则是：如果 $p_{1,j}  p_{0,j}$，则 $g^*(j) = \\mathcal{C}_1$；如果 $p_{0,j}  p_{1,j}$，则 $g^*(j) = \\mathcal{C}_0$。如果 $p_{1,j} = p_{0,j}$，则选择是任意的，不影响最小错误率。\n\n在此最优规则下，对于区间 $j$ 的错误概率是另一类别的后验概率乘以落入该区间的概率：\n$P(\\text{错误, } X \\in B_j) = \\min \\{ P(\\mathcal{C}_0, X \\in B_j), P(\\mathcal{C}_1, X \\in B_j) \\}$。\n使用联合概率的定义，$P(\\mathcal{C}_k, X \\in B_j) = P(X \\in B_j | \\mathcal{C}_k) P(\\mathcal{C}_k) = p_{k,j} \\pi_k$。\n因此，对于一个固定的划分，总错误率为：\n$$P_e = \\sum_{j=1}^{4} \\min \\{ \\pi_0 p_{0,j}, \\pi_1 p_{1,j} \\} = \\frac{1}{2} \\sum_{j=1}^{4} \\min \\{ p_{0,j}, p_{1,j} \\}$$\n使用恒等式 $\\min(a,b) = \\frac{1}{2}(a+b - |a-b|)$：\n$$P_e = \\frac{1}{2} \\sum_{j=1}^{4} \\frac{1}{2} (p_{0,j} + p_{1,j} - |p_{1,j} - p_{0,j}|) = \\frac{1}{4} \\left( \\sum_{j=1}^{4} (p_{0,j} + p_{1,j}) - \\sum_{j=1}^{4} |p_{1,j} - p_{0,j}| \\right)$$\n由于对于 $k=0,1$，有 $\\sum_{j=1}^{4} p_{k,j} = \\sum_{j=1}^{4} \\int_{B_j} f_k(x) dx = \\int_0^1 f_k(x) dx = 1$：\n$$P_e = \\frac{1}{4} \\left( 1 + 1 - \\sum_{j=1}^{4} \\left| \\int_{B_j} (f_1(x) - f_0(x)) dx \\right| \\right) = \\frac{1}{2} - \\frac{1}{4} \\sum_{j=1}^{4} \\left| \\int_{B_j} (f_1(x) - f_0(x)) dx \\right|$$\n为了最小化 $P_e$，我们必须找到使总和 $S = \\sum_{j=1}^{4} \\left| \\int_{B_j} (f_1(x) - f_0(x)) dx \\right|$ 最大化的划分 $\\{B_j\\}_{j=1}^4$（即边界 $t_1, t_2, t_3$）。\n\n### 步骤2：最大化差值积分绝对值之和\n\n令 $h(x) = f_1(x) - f_0(x)$。函数 $h(x)$ 与 $f_0(x)$ 和 $f_1(x)$ 一样，在相同的子区间 $S_i$ 上是分段常数。这些子区间的长度为：$|S_1|=\\frac{1}{6}$， $|S_2|=\\frac{1}{3}$， $|S_3|=\\frac{1}{6}$， $|S_4|=\\frac{1}{4}$， $|S_5|=\\frac{1}{12}$。\n在这些子区间上 $h(x)$ 的值为：\n- $x \\in S_1$: $h(x) = 1 - \\frac{1}{2} = \\frac{1}{2}$\n- $x \\in S_2$: $h(x) = \\frac{1}{2} - \\frac{3}{2} = -1$\n- $x \\in S_3$: $h(x) = 2 - 1 = 1$\n- $x \\in S_4$: $h(x) = \\frac{1}{2} - \\frac{2}{3} = -\\frac{1}{6}$\n- $x \\in S_5$: $h(x) = \\frac{5}{2} - 1 = \\frac{3}{2}$\n\n当每个区间 $B_j$ 内的积分不涉及 $h(x)$ 的符号变化时，总和 $S$ 达到最大值。这表明最优边界 $t_1, t_2, t_3$ 应从 $h(x)$ 改变符号的点集中选择。这些点是区间 $S_i$ 的边界：$\\frac{1}{6}, \\frac{1}{2}, \\frac{2}{3}, \\frac{11}{12}$。\n我们有 $4$ 个候选位置来放置 $3$ 个边界。这意味着我们必须省略其中一个候选点。省略一个边界意味着合并两个相邻的 $S_i$ 区间。\n\n让我们计算 $h(x)$ 在每个子区间 $S_i$ 上的积分：\n- $I_1 = \\int_{S_1} h(x) dx = \\frac{1}{2} \\cdot |S_1| = \\frac{1}{2} \\cdot \\frac{1}{6} = \\frac{1}{12}$\n- $I_2 = \\int_{S_2} h(x) dx = -1 \\cdot |S_2| = -1 \\cdot \\frac{1}{3} = -\\frac{1}{3}$\n- $I_3 = \\int_{S_3} h(x) dx = 1 \\cdot |S_3| = 1 \\cdot \\frac{1}{6} = \\frac{1}{6}$\n- $I_4 = \\int_{S_4} h(x) dx = -\\frac{1}{6} \\cdot |S_4| = -\\frac{1}{6} \\cdot \\frac{1}{4} = -\\frac{1}{24}$\n- $I_5 = \\int_{S_5} h(x) dx = \\frac{3}{2} \\cdot |S_5| = \\frac{3}{2} \\cdot \\frac{1}{12} = \\frac{1}{8}$\n\n如果我们能使用全部 $4$ 个边界（形成 $5$ 个区间），则最大化的总和将是 $h(x)$ 积分的全变分：\n$S_5 = \\sum_{i=1}^5 |I_i| = \\left|\\frac{1}{12}\\right| + \\left|-\\frac{1}{3}\\right| + \\left|\\frac{1}{6}\\right| + \\left|-\\frac{1}{24}\\right| + \\left|\\frac{1}{8}\\right| = \\frac{1}{12} + \\frac{1}{3} + \\frac{1}{6} + \\frac{1}{24} + \\frac{1}{8} = \\frac{2+8+4+1+3}{24} = \\frac{18}{24} = \\frac{3}{4}$。\n\n由于我们只有 $3$ 个边界用于划分 $4$ 个区间，我们必须合并一对相邻的区域 $S_i, S_{i+1}$。这对应于移除它们之间的边界。当我们合并时，贡献项 $|I_i| + |I_{i+1}|$ 被替换为 $|I_i + I_{i+1}|$。根据三角不等式，有 $|I_i + I_{i+1}| \\le |I_i| + |I_{i+1}|$，因此总和 $S$ 将会减少或保持不变。为了最大化 $S$，我们必须最小化这个减少量（损失）。损失为 $(|I_i| + |I_{i+1}|) - |I_i + I_{i+1}|$。\n由于 $h(x)$ 在指定的边界处符号交替变化，因此 $I_i$ 和 $I_{i+1}$ 总是有相反的符号。损失简化为 $2 \\min(|I_i|, |I_{i+1}|)$。\n我们计算移除四个内部边界中每一个所带来的损失：\n1.  移除位于 $\\frac{1}{6}$ 的边界（合并 $S_1, S_2$）：损失 = $2 \\min(|I_1|, |I_2|) = 2 \\min(\\frac{1}{12}, \\frac{1}{3}) = 2 \\cdot \\frac{1}{12} = \\frac{1}{6}$。\n2.  移除位于 $\\frac{1}{2}$ 的边界（合并 $S_2, S_3$）：损失 = $2 \\min(|I_2|, |I_3|) = 2 \\min(\\frac{1}{3}, \\frac{1}{6}) = 2 \\cdot \\frac{1}{6} = \\frac{1}{3}$。\n3.  移除位于 $\\frac{2}{3}$ 的边界（合并 $S_3, S_4$）：损失 = $2 \\min(|I_3|, |I_4|) = 2 \\min(\\frac{1}{6}, \\frac{1}{24}) = 2 \\cdot \\frac{1}{24} = \\frac{1}{12}$。\n4.  移除位于 $\\frac{11}{12}$ 的边界（合并 $S_4, S_5$）：损失 = $2 \\min(|I_4|, |I_5|) = 2 \\min(\\frac{1}{24}, \\frac{1}{8}) = 2 \\cdot \\frac{1}{24} = \\frac{1}{12}$。\n\n最小损失为 $\\frac{1}{12}$，这发生在我们移除位于 $\\frac{2}{3}$ 或 $\\frac{11}{12}$ 的边界时。任一选择都会得到总和 $S$ 的可能最大值。\n最大化的总和为 $S_{\\max} = S_5 - \\text{min(Loss)} = \\frac{3}{4} - \\frac{1}{12} = \\frac{9-1}{12} = \\frac{8}{12} = \\frac{2}{3}$。\n\n### 步骤3：计算最小贝叶斯错误率\n\n我们将最大化的总和 $S_{\\max} = \\frac{2}{3}$ 代回到最小错误率 $P_{e, \\min}$ 的表达式中：\n$$P_{e, \\min} = \\frac{1}{2} - \\frac{1}{4} S_{\\max} = \\frac{1}{2} - \\frac{1}{4} \\left(\\frac{2}{3}\\right) = \\frac{1}{2} - \\frac{2}{12} = \\frac{1}{2} - \\frac{1}{6}$$\n$$P_{e, \\min} = \\frac{3-1}{6} = \\frac{2}{6} = \\frac{1}{3}$$\n最小可实现的贝叶斯错误率是 $\\frac{1}{3}$。例如，通过将量化边界设置在 $\\{\\frac{1}{6}, \\frac{1}{2}, \\frac{2}{3}\\}$ 或 $\\{\\frac{1}{6}, \\frac{1}{2}, \\frac{11}{12}\\}$ 处可以达到这个错误率。",
            "answer": "$$\n\\boxed{\\frac{1}{3}}\n$$"
        }
    ]
}