{
    "hands_on_practices": [
        {
            "introduction": "To truly master the concept of the Bayes classifier, it is essential to build its risk calculation from the ground up. This first exercise  challenges you to do just that in the context of discrete data, such as word counts in a document. By deriving the Bayes risk for classifying between a Poisson and a Negative Binomial distribution, you will directly apply the fundamental definition: summing the minimum achievable error probability at each possible outcome.",
            "id": "3180205",
            "problem": "Consider a document classification setting where the feature is a single nonnegative integer count $X \\in \\{0,1,2,\\ldots\\}$ representing the number of occurrences of a particular word in a message. The class label $Y \\in \\{1,2\\}$ denotes two categories (for example, two types of messages), with equal class priors $\\mathbb{P}(Y=1) = \\mathbb{P}(Y=2) = \\frac{1}{2}$.\n\nConditioned on the class label, the word count is modeled by two distinct class-conditional distributions:\n- If $Y=1$ then $X \\mid Y=1 \\sim \\operatorname{Poisson}(\\lambda)$ with parameter $\\lambda  0$, that is, $\\mathbb{P}(X=x \\mid Y=1) = \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}$ for $x=0,1,2,\\ldots$.\n- If $Y=2$ then $X \\mid Y=2 \\sim \\operatorname{Negative\\ Binomial}(r,q)$ with parameters $r \\in \\mathbb{N}$ and $q \\in (0,1)$, representing the number of failures $x$ before $r$ successes with success probability $q$. Its probability mass function is $\\mathbb{P}(X=x \\mid Y=2) = \\binom{r+x-1}{x} q^{r}(1-q)^{x}$ for $x=0,1,2,\\ldots$.\n\nThe decision-maker uses the Bayes classifier under the zero-one loss (0-1 loss), which classifies to the label that minimizes the probability of error at each observation. Starting from first principles, derive the Bayes risk (the minimum expected misclassification probability under the given generative model and zero-one loss) as an exact analytic expression in terms of $\\lambda$, $r$, and $q$. Your final answer must be a single closed-form expression; it may involve an infinite series. Do not approximate or round; provide the exact result.",
            "solution": "The Bayes risk, denoted $R^*$, is the minimum achievable expected loss under a given generative model. For the zero-one loss function, the Bayes risk corresponds to the minimum probability of misclassification. It is calculated by averaging the conditional error probability over all possible values of the feature $X$.\n\nFor a discrete feature $X$, the Bayes risk is given by:\n$$R^* = \\sum_{x=0}^{\\infty} \\mathbb{P}(X=x) \\mathbb{P}(\\text{error} \\mid X=x)$$\nThe conditional error probability, $\\mathbb{P}(\\text{error} \\mid X=x)$, is the probability of the less likely class given the observation $x$. This can be written as $\\min_{k \\in \\{1,2\\}} \\mathbb{P}(Y=k \\mid X=x)$. Using the definition of posterior probability $\\mathbb{P}(Y=k \\mid X=x) = \\frac{\\mathbb{P}(X=x \\mid Y=k) \\mathbb{P}(Y=k)}{\\mathbb{P}(X=x)} = \\frac{p_k(x) \\pi_k}{\\mathbb{P}(X=x)}$, the risk becomes:\n$$R^* = \\sum_{x=0}^{\\infty} \\mathbb{P}(X=x) \\frac{\\min(p_1(x) \\pi_1, p_2(x) \\pi_2)}{\\mathbb{P}(X=x)} = \\sum_{x=0}^{\\infty} \\min(p_1(x) \\pi_1, p_2(x) \\pi_2)$$\nHere, $p_k(x)$ is the class-conditional probability mass function (PMF) for class $k$, and $\\pi_k$ is the prior probability for class $k$.\n\nIn this problem, the priors are equal, $\\pi_1 = \\pi_2 = \\frac{1}{2}$. The PMFs are given as:\n- For class $Y=1$ (Poisson): $p_1(x) = \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}$\n- For class $Y=2$ (Negative Binomial): $p_2(x) = \\binom{r+x-1}{x} q^{r}(1-q)^{x}$\n\nSubstituting these into the expression for the Bayes risk, we can factor out the common prior probability $\\frac{1}{2}$:\n$$R^* = \\sum_{x=0}^{\\infty} \\min\\left(p_1(x) \\frac{1}{2}, p_2(x) \\frac{1}{2}\\right) = \\frac{1}{2} \\sum_{x=0}^{\\infty} \\min(p_1(x), p_2(x))$$\nThis leads to the final exact analytic expression for the Bayes risk as an infinite series:\n$$R^* = \\frac{1}{2} \\sum_{x=0}^{\\infty} \\min\\left( \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}, \\binom{r+x-1}{x} q^{r}(1-q)^{x} \\right)$$\nThis expression is the final answer, as it is an exact analytic form in terms of the given parameters, and the problem allows for the answer to be an infinite series.",
            "answer": "$$\n\\boxed{\\frac{1}{2} \\sum_{x=0}^{\\infty} \\min\\left( \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}, \\binom{r+x-1}{x} q^{r}(1-q)^{x} \\right)}\n$$"
        },
        {
            "introduction": "Analytical derivations are foundational, but much of modern statistical learning involves computation. This practice  bridges theory and implementation by asking you to numerically compute the Bayes risk when the posterior probability $\\eta(x) = P(Y=1|X=x)$ is given in the form of a logistic function, a cornerstone of many classification models. You will translate the theoretical expectation $E_X[\\min\\{\\eta(X), 1-\\eta(X)\\}]$ into a concrete numerical integral, a vital skill for evaluating the theoretical performance limits of real-world classifiers.",
            "id": "3180161",
            "problem": "Consider a binary classification setting with feature $X \\in \\mathbb{R}$ distributed as a standard normal random variable $X \\sim \\mathcal{N}(0,1)$ and posterior probability function for class $Y=1$ given by $\\eta(x) = \\sigma(a x + b)$, where $\\sigma(z)$ is the logistic sigmoid function defined by $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$, and $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$ are parameters. Under $0$-$1$ loss, the Bayes classifier at a point $x$ predicts the class with the higher posterior probability, and the Bayes risk is the expected misclassification probability when using this classifier.\n\nStarting only from the core definitions of the Bayes classifier and Bayes risk under $0$-$1$ loss, derive the appropriate expression for the Bayes risk as an expectation over $X$ and implement a numerical procedure to compute it for the given $\\eta(x)$ and $X \\sim \\mathcal{N}(0,1)$. Your program must:\n- Use accurate numerical integration over $x \\in \\mathbb{R}$ with respect to the standard normal density to evaluate the Bayes risk for each parameter pair $(a,b)$.\n- Produce floating-point outputs representing the Bayes risk values as decimals (not percentages).\n- Round each computed Bayes risk to $10$ decimal places for the final reported values.\n\nTest Suite:\nCompute the Bayes risk for the following parameter pairs $(a,b)$:\n- Case $1$: $(a,b) = (1.0, 0.0)$.\n- Case $2$: $(a,b) = (0.0, 0.0)$.\n- Case $3$: $(a,b) = (0.0, 2.0)$.\n- Case $4$: $(a,b) = (5.0, 0.0)$.\n- Case $5$: $(a,b) = (1.5, 2.0)$.\n- Case $6$: $(a,b) = (-1.5, -2.0)$.\n- Case $7$: $(a,b) = (-2.0, 1.0)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the above cases, in order, as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5,r_6,r_7]$), where each $r_i$ is the Bayes risk for Case $i$ rounded to $10$ decimal places.",
            "solution": "The Bayes risk, $R^*$, for a binary classification problem under the 0-1 loss is the expected probability of misclassification when using the optimal Bayes classifier. This is the expectation of the conditional error probability, taken over the distribution of the feature $X$.\nThe conditional error probability at a point $x$ is the probability of the less likely class, given by $\\min\\{P(Y=1|X=x), P(Y=0|X=x)\\}$. Using the given posterior probability $\\eta(x) = P(Y=1|X=x)$, this is $\\min\\{\\eta(x), 1-\\eta(x)\\}$.\n\nThe Bayes risk is therefore:\n$$ R^* = \\mathbb{E}_X[\\min\\{\\eta(X), 1-\\eta(X)\\}] $$\nSince the feature $X$ is distributed as a standard normal variable, $X \\sim \\mathcal{N}(0,1)$, with probability density function (PDF) $\\phi(x)$, the expectation is expressed as an integral:\n$$ R^* = \\int_{-\\infty}^{\\infty} \\min\\{\\eta(x), 1-\\eta(x)\\} \\phi(x) dx $$\nThe posterior is given by the logistic function $\\eta(x) = \\sigma(ax+b) = \\frac{1}{1 + e^{-(ax+b)}}$. A key property of the logistic function is that $1-\\sigma(z) = \\sigma(-z)$. This allows us to write the conditional error probability as:\n$$ \\min\\{\\eta(x), 1-\\eta(x)\\} = \\min\\{\\sigma(ax+b), \\sigma(-(ax+b))\\} = \\sigma(-|ax+b|) = \\frac{1}{1 + e^{|ax+b|}} $$\nThe final expression for the Bayes risk is the integral:\n$$ R^* = \\int_{-\\infty}^{\\infty} \\frac{1}{1 + e^{|ax+b|}} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx $$\nThis integral does not have a general closed-form solution and must be evaluated numerically using quadrature methods. The provided code implements this numerical integration for each given parameter pair $(a,b)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the Bayes risk for a binary classification problem with a normally\n    distributed feature and a logistic posterior probability function for\n    several sets of parameters.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.0),   # Case 1\n        (0.0, 0.0),   # Case 2\n        (0.0, 2.0),   # Case 3\n        (5.0, 0.0),   # Case 4\n        (1.5, 2.0),   # Case 5\n        (-1.5, -2.0), # Case 6\n        (-2.0, 1.0)   # Case 7\n    ]\n\n    results = []\n    \n    for a, b in test_cases:\n        # The Bayes risk R* is the expectation of the minimum of the posterior\n        # probabilities for the two classes, E[min(eta(X), 1-eta(X))], where\n        # X ~ N(0,1). This expectation is calculated by integrating the\n        # conditional risk over the distribution of X.\n        #\n        # Conditional risk at x: min(eta(x), 1-eta(x))\n        #   eta(x) = sigma(ax+b) = 1 / (1 + exp(-(ax+b)))\n        #   1-eta(x) = sigma(-(ax+b))\n        #   min(eta(x), 1-eta(x)) = sigma(-|ax+b|) = 1 / (1 + exp(|ax+b|))\n        #\n        # Distribution of X is standard normal, with PDF phi(x).\n        # Integrand for Bayes risk: (1 / (1 + exp(|ax+b|))) * phi(x)\n\n        # Define the integrand function for numerical integration.\n        # It takes x, a, and b as arguments.\n        def integrand(x, a_val, b_val):\n            # Calculate the conditional error probability\n            # sigma(-|ax+b|) = 1 / (1 + exp(|ax+b|))\n            conditional_risk = 1.0 / (1.0 + np.exp(np.abs(a_val * x + b_val)))\n            \n            # Multiply by the standard normal PDF\n            # norm.pdf(x, 0, 1) provides phi(x)\n            return conditional_risk * norm.pdf(x, loc=0, scale=1)\n\n        # Perform numerical integration over (-inf, +inf)\n        # quad returns a tuple (integral_value, error_estimate)\n        bayes_risk, _ = quad(integrand, -np.inf, np.inf, args=(a, b))\n        \n        # Round the result to 10 decimal places as specified\n        rounded_risk = round(bayes_risk, 10)\n        results.append(rounded_risk)\n\n    # Format the final output string as [r1,r2,...,r7]\n    # Using .10f format specifier to avoid scientific notation for small numbers\n    # and ensure trailing zeros if necessary.\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data is often more complex than simple, single-peaked distributions allow. This advanced exercise  delves into such a scenario, where one class is described by a bimodal mixture of normal distributions, leading to potentially non-convex decision regions. You will not only implement a numerical solution to find these complex boundaries but also extend your analysis to incorporate asymmetric loss, where the cost of different types of errors is unequal—a critical consideration in applications from medical diagnosis to financial fraud detection.",
            "id": "3180228",
            "problem": "Consider a binary classification problem in statistical learning with feature $X \\in \\mathbb{R}$ and label $Y \\in \\{0,1\\}$. The class prior probabilities are $P(Y=0)=\\pi_0$ and $P(Y=1)=\\pi_1$, where $\\pi_0,\\pi_1 \\in (0,1)$ and $\\pi_0+\\pi_1=1$. The class-conditional distributions are specified as follows:\n- For class $Y=1$, the conditional distribution $X \\mid Y=1$ is a bimodal mixture of two normal components: the Probability Density Function (PDF) is\n$$\nf_1(x) \\equiv w \\,\\phi(x;\\mu_{1a},\\sigma_{1a}) + (1-w)\\,\\phi(x;\\mu_{1b},\\sigma_{1b}),\n$$\nwhere $w \\in (0,1)$ is the mixture weight and $\\phi(x;\\mu,\\sigma)$ denotes the normal PDF with mean $\\mu$ and standard deviation $\\sigma$, namely\n$$\n\\phi(x;\\mu,\\sigma) \\equiv \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n$$\n- For class $Y=0$, the conditional distribution $X \\mid Y=0$ is unimodal normal with PDF\n$$\nf_0(x) \\equiv \\phi(x;\\mu_0,\\sigma_0).\n$$\n\nA classification decision $\\hat{Y}(x) \\in \\{0,1\\}$ incurs a loss according to the following $2\\times 2$ loss specification: predicting the correct class has zero loss, predicting $1$ when the true class is $0$ has loss $\\lambda_{10}  0$ (false positive), and predicting $0$ when the true class is $1$ has loss $\\lambda_{01}  0$ (false negative). The Bayes classifier is defined as the decision rule that minimizes the conditional expected loss at each $x$, and the Bayes risk is the expected loss of the Bayes classifier under the joint distribution of $(X,Y)$.\n\nTask:\n1. Starting from core definitions (Bayes’ theorem for posterior probabilities and conditional risk minimization), reason about the Bayes decision rule for this setting. Explain why, when $f_1$ is bimodal and $f_0$ is unimodal, the Bayes decision region for predicting class $1$ can be a non-convex set in $\\mathbb{R}$ (a union of disjoint intervals), and identify, in terms of primitive quantities, the inequality that characterizes the decision region for class $1$ under both symmetric $0$-$1$ loss and asymmetric loss.\n2. Implement a numerical method to:\n   - Determine the number of disjoint intervals in the Bayes decision region for predicting class $1$ under symmetric $0$-$1$ loss (i.e., $\\lambda_{10}=\\lambda_{01}=1$).\n   - Determine whether that decision region is non-convex (return a boolean that is $true$ if and only if the number of intervals exceeds $1$).\n   - Compute the Bayes error rate under symmetric $0$-$1$ loss. Express this error rate as a real number in decimal form (no percentage sign).\n   - Compute the Bayes risk under the specified asymmetric loss $(\\lambda_{10},\\lambda_{01})$. Express this risk as a real number in decimal form.\n\nUse purely mathematical and algorithmic reasoning. Do not appeal to formulas that are not derivable from the stated definitions. Your program must carry out the computations numerically and must not require any external input.\n\nTest Suite:\nFor each test case, parameters are given as a tuple\n$$\n(\\pi_0,\\pi_1,w,\\mu_{1a},\\sigma_{1a},\\mu_{1b},\\sigma_{1b},\\mu_0,\\sigma_0,\\lambda_{10},\\lambda_{01}),\n$$\nwith all quantities real-valued and $\\pi_0+\\pi_1=1$, $w \\in (0,1)$, and all standard deviations strictly positive. The test suite is:\n\n- Case A (general happy path with clearly bimodal $f_1$ and a broader unimodal $f_0$, symmetric loss):\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- Case B (boundary tendency with dominant prior for $Y=0$, symmetric loss):\n$$\n(0.8,\\,0.2,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- Case C (alternative shapes with narrower $f_0$ and shifted mixture components, symmetric loss):\n$$\n(0.5,\\,0.5,\\,0.6,\\,-2.5,\\,0.5,\\,1.8,\\,0.5,\\,0.0,\\,0.7,\\,1.0,\\,1.0).\n$$\n- Case D (asymmetric loss emphasizing false positive penalty, same shapes as Case A):\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,3.0).\n$$\n\nAlgorithmic Output Specification:\n- For each test case, produce a result list of the form $[N,B,E,R]$, where:\n  - $N$ is an integer giving the number of disjoint intervals in the Bayes decision region for class $1$ under symmetric $0$-$1$ loss.\n  - $B$ is a boolean that is $true$ if and only if the decision region is non-convex under symmetric $0$-$1$ loss (that is, $N1$).\n  - $E$ is a float giving the Bayes error rate under symmetric $0$-$1$ loss.\n  - $R$ is a float giving the Bayes risk under the specified $(\\lambda_{10},\\lambda_{01})$ for that test case.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, for example:\n$$\n[\\,[N_1,B_1,E_1,R_1],\\,[N_2,B_2,E_2,R_2],\\,[N_3,B_3,E_3,R_3],\\,[N_4,B_4,E_4,R_4]\\,].\n$$\nNo physical units or angles are involved. All probabilities and risks must be expressed as decimals (not percentages).",
            "solution": "**1. Bayes Decision Rule and Non-Convexity**\n\nThe Bayes classifier minimizes the conditional expected loss (conditional risk) at each point $x$. The conditional risk of predicting class $0$ is $R(\\hat{Y}=0|x) = \\lambda_{01} P(Y=1|x)$, and for predicting class $1$ is $R(\\hat{Y}=1|x) = \\lambda_{10} P(Y=0|x)$. The classifier predicts class $1$ if $R(\\hat{Y}=1|x)  R(\\hat{Y}=0|x)$, which leads to the inequality:\n$$ \\lambda_{10} P(Y=0|x)  \\lambda_{01} P(Y=1|x) $$\nUsing Bayes' theorem, $P(Y=j|x) \\propto \\pi_j f_j(x)$, we can express the decision rule in terms of the given prior probabilities ($\\pi_j$) and class-conditional densities ($f_j(x)$). The decision region for class $1$ is the set of $x$ where:\n$$ \\lambda_{10} \\pi_0 f_0(x)  \\lambda_{01} \\pi_1 f_1(x) $$\nFor the symmetric 0-1 loss case ($\\lambda_{10}=\\lambda_{01}=1$), this simplifies to predicting class 1 when $\\pi_1 f_1(x) > \\pi_0 f_0(x)$.\n\nThe decision boundaries are the roots of the equation $\\lambda_{01} \\pi_1 f_1(x) - \\lambda_{10} \\pi_0 f_0(x) = 0$. Since $f_1(x)$ is a bimodal mixture of two Gaussians and $f_0(x)$ is a unimodal Gaussian, their weighted difference can be a complex function with multiple roots. If this function crosses the zero axis multiple times, the decision region for class 1 (where the function is positive) can be a union of disjoint intervals (e.g., $(r_1, r_2) \\cup (r_3, r_4)$), which is a non-convex set.\n\n**2. Bayes Error Rate and Risk Calculation**\n\nThe **Bayes risk** ($R$) is the expected loss of the Bayes classifier. It is calculated by integrating the minimum conditional risk over the distribution of $X$:\n$$ R = \\int_{-\\infty}^{\\infty} \\min\\{\\lambda_{01} \\pi_1 f_1(x), \\lambda_{10} \\pi_0 f_0(x)\\} dx $$\nThe **Bayes error rate** ($E$) is the Bayes risk for the special case of symmetric 0-1 loss:\n$$ E = \\int_{-\\infty}^{\\infty} \\min\\{\\pi_1 f_1(x), \\pi_0 f_0(x)\\} dx $$\nNumerically, these integrals are computed by first finding the roots of the corresponding decision function to define the decision regions. For example, to compute $E$, we find the roots of $\\pi_1 f_1(x) - \\pi_0 f_0(x) = 0$. These roots define the regions $\\mathcal{R}_0$ and $\\mathcal{R}_1$. The error is then the sum of integrals: $\\int_{\\mathcal{R}_1} \\pi_0 f_0(x) dx$ (errors in region 1) and $\\int_{\\mathcal{R}_0} \\pi_1 f_1(x) dx$ (errors in region 0). A similar procedure is used for the asymmetric risk $R$. The number of disjoint intervals ($N$) constituting the symmetric decision region $\\mathcal{R}_1$ is found by counting, and the boolean for non-convexity ($B$) is simply $N > 1$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case B\n        (0.8, 0.2, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case C\n        (0.5, 0.5, 0.6, -2.5, 0.5, 1.8, 0.5, 0.0, 0.7, 1.0, 1.0),\n        # Case D\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(case))\n\n    # Format the final output string as a comma-separated list of lists,\n    # with booleans represented as lowercase 'true' or 'false'.\n    case_strings = []\n    for res_list in results:\n        N, B, E, R = res_list\n        case_strings.append(f\"[{N},{str(B).lower()},{E},{R}]\")\n    \n    output_string = f\"[{','.join(case_strings)}]\"\n    print(output_string)\n\ndef process_case(params):\n    \"\"\"\n    Computes [N, B, E, R] for a single test case.\n    \"\"\"\n    pi0, pi1, w, mu1a, sigma1a, mu1b, sigma1b, mu0, sigma0, lam10, lam01 = params\n\n    # 1. Define PDFs\n    def f1(x):\n        return w * norm.pdf(x, loc=mu1a, scale=sigma1a) + (1-w) * norm.pdf(x, loc=mu1b, scale=sigma1b)\n\n    def f0(x):\n        return norm.pdf(x, loc=mu0, scale=sigma0)\n\n    # 2. Analyze symmetric loss case for N, B, E\n    def g_sym(x):\n        return pi1 * f1(x) - pi0 * f0(x)\n\n    def find_roots(func):\n        # Determine a sufficiently wide range to find all relevant roots\n        all_mus = [mu1a, mu1b, mu0]\n        all_sigmas = [sigma1a, sigma1b, sigma0]\n        search_min = min(all_mus) - 15 * max(all_sigmas)\n        search_max = max(all_mus) + 15 * max(all_sigmas)\n        \n        roots = []\n        grid = np.linspace(search_min, search_max, 20001)\n        y_vals = func(grid)\n        \n        for i in range(len(grid) - 1):\n            if np.sign(y_vals[i]) != np.sign(y_vals[i+1]):\n                try:\n                    root = brentq(func, grid[i], grid[i+1])\n                    roots.append(root)\n                except (ValueError, RuntimeError):\n                    # In case of issues, we skip, as sign change is the main trigger.\n                    pass\n        return sorted(list(set(roots)))\n\n    roots_sym = find_roots(g_sym)\n    \n    # 3. Determine N and B\n    R1_sym_intervals = []\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n        if g_sym(test_point) > 0:\n            R1_sym_intervals.append((a, b))\n    \n    N = len(R1_sym_intervals)\n    B = N > 1\n\n    # 4. Compute Bayes Error Rate E (symmetric loss)\n    # E = integral over R of min(pi0*f0, pi1*f1)\n    # This is equivalent to integrating pi1*f1 over R0 and pi0*f0 over R1\n    error = 0.0\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_sym(test_point) > 0: # Predict 1, error is if Y=0\n            error += quad(lambda x: pi0 * f0(x), a, b)[0]\n        else: # Predict 0, error is if Y=1\n            error += quad(lambda x: pi1 * f1(x), a, b)[0]\n    E = error\n    \n    # 5. Compute Bayes Risk R (asymmetric loss)\n    def g_asym(x):\n        return lam01 * pi1 * f1(x) - lam10 * pi0 * f0(x)\n\n    roots_asym = find_roots(g_asym)\n    \n    risk = 0.0\n    all_asym_points = [-np.inf] + roots_asym + [np.inf]\n    for i in range(len(all_asym_points) - 1):\n        a, b = all_asym_points[i], all_asym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_asym(test_point) > 0: # Predict 1, risk is lambda_10 * P(Y=0)\n            risk += quad(lambda x: lam10 * pi0 * f0(x), a, b)[0]\n        else: # Predict 0, risk is lambda_01 * P(Y=1)\n            risk += quad(lambda x: lam01 * pi1 * f1(x), a, b)[0]\n    R = risk\n\n    return [N, B, E, R]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}