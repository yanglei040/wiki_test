{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp how feature importance measures work, it is essential to move beyond abstract definitions and compute them by hand. This first exercise provides a small, well-defined dataset and a simple decision tree, allowing you to calculate both impurity-based (Gini) and permutation-based importance from first principles. By working through the numbers , you will gain a concrete understanding of the mechanics behind these methods and see how they can sometimes lead to different conclusions about which features are most valuable.",
            "id": "3121084",
            "problem": "Consider a binary classification task with features $X_1$, $X_2$ and label $Y \\in \\{0,1\\}$. A hand-crafted decision tree of depth $2$ is defined as follows: at the root, split on $X_1 \\leq 0$; the left child (where $X_1=0$) splits on $X_2 \\leq 0$; the right child (where $X_1=1$) is a leaf. Each leaf predicts by majority class among the training samples that reach it.\n\nYou are given a synthetic dataset of $n=12$ observations with the following counts:\n- Three observations with $(X_1,X_2,Y)=(0,0,1)$,\n- One observation with $(X_1,X_2,Y)=(0,1,0)$,\n- Three observations with $(X_1,X_2,Y)=(1,0,1)$,\n- Five observations with $(X_1,X_2,Y)=(1,1,1)$.\n\nTasks:\n1. From first principles, compute the Gini impurity reduction at the root split on $X_1$ and at the left-child split on $X_2$, and form the impurity-based feature importances by weighting each split’s impurity reduction by the proportion of samples that reach the node at which the split occurs. Repeat the same derivation using Shannon entropy (natural logarithm).\n2. Define permutation importance for a feature as the expected decrease in classification accuracy (on this same dataset) when that feature’s values are uniformly randomly permuted across the $n$ samples while keeping the trained tree fixed. Compute the permutation importance for $X_1$ and for $X_2$ exactly (do not simulate; derive the expectations analytically).\n3. Finally, compute the difference between the permutation importance of $X_1$ and that of $X_2$.\n\nExpress your final answer as a single simplified fraction. No rounding is required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of statistical learning, is well-posed with a clear and complete set of data and definitions, and is expressed in objective, formal language. There are no contradictions, ambiguities, or unsound premises. We may proceed with the solution.\n\nThe decision tree structure and dataset are as follows:\n- The total number of samples is $n=12$.\n- Data counts:\n  - $3$ samples: $(X_1=0, X_2=0, Y=1)$\n  - $1$ sample: $(X_1=0, X_2=1, Y=0)$\n  - $3$ samples: $(X_1=1, X_2=0, Y=1)$\n  - $5$ samples: $(X_1=1, X_2=1, Y=1)$\n- Tree structure:\n  - Root node: Splits on $X_1 \\leq 0$. Since $X_1$ only takes values $0$ and $1$, this separates samples based on $X_1=0$ versus $X_1=1$.\n  - Left child (for $X_1=0$): Splits on $X_2 \\leq 0$, separating samples based on $X_2=0$ versus $X_2=1$.\n  - Right child (for $X_1=1$): This is a leaf node.\n- Leaf node predictions are by majority class of the samples in that leaf.\n\nLet's populate the tree with the training data to determine the leaf predictions.\n- **Root Node (Node 0):** Contains all $n=12$ samples.\n  - Class counts: $N_0 = 1$ (for $Y=0$), $N_1 = 11$ (for $Y=1$).\n- **Split at Root on $X_1$:**\n  - **Left Child (Node 1, $X_1=0$):** Contains $3+1=4$ samples.\n    - Class counts: $N_0 = 1$, $N_1 = 3$.\n  - **Right Child (Node 2, $X_1=1$):** Contains $3+5=8$ samples. This is a leaf.\n    - Class counts: $N_0 = 0$, $N_1 = 8$. The majority class is $1$. So, this leaf predicts $\\hat{Y}=1$.\n- **Split at Node 1 on $X_2$:**\n  - **Left-Left Child (Node 3, $X_1=0, X_2=0$):** Contains $3$ samples. This is a leaf.\n    - Class counts: $N_0 = 0$, $N_1 = 3$. The majority class is $1$. So, this leaf predicts $\\hat{Y}=1$.\n  - **Left-Right Child (Node 4, $X_1=0, X_2=1$):** Contains $1$ sample. This is a leaf.\n    - Class counts: $N_0 = 1$, $N_1 = 0$. The majority class is $0$. So, this leaf predicts $\\hat{Y}=0$.\n\nThe fixed tree's prediction rule is:\n- If $X_1=1$, predict $\\hat{Y}=1$.\n- If $X_1=0$ and $X_2=0$, predict $\\hat{Y}=1$.\n- If $X_1=0$ and $X_2=1$, predict $\\hat{Y}=0$.\n\n**1. Impurity-Based Feature Importance**\n\nThis part is computed as requested, but is not used for the final answer which concerns permutation importance.\n\n**Gini Impurity:** The Gini impurity of a node with class proportions $p_k$ is $I_G = 1 - \\sum_k p_k^2$. The impurity reduction is $\\Delta I_G = I_{G,parent} - \\sum_{c \\in children} \\frac{N_c}{N_{parent}} I_{G,c}$. The feature importance is the sum of weighted impurity reductions for that feature: $Imp(F) = \\sum_{s \\in \\text{splits on } F} \\frac{N_s}{N} \\Delta I_G(s)$.\n\n- **Root split on $X_1$:**\n  - $I_G(\\text{Root}): p_0=\\frac{1}{12}, p_1=\\frac{11}{12} \\implies I_G(\\text{Root}) = 1 - ((\\frac{1}{12})^2 + (\\frac{11}{12})^2) = 1 - \\frac{1+121}{144} = \\frac{22}{144} = \\frac{11}{72}$.\n  - $I_G(\\text{Node 1, } X_1=0): N=4, p_0=\\frac{1}{4}, p_1=\\frac{3}{4} \\implies I_G(\\text{Node 1}) = 1 - ((\\frac{1}{4})^2 + (\\frac{3}{4})^2) = 1 - \\frac{1+9}{16} = \\frac{6}{16} = \\frac{3}{8}$.\n  - $I_G(\\text{Node 2, } X_1=1): N=8, p_0=0, p_1=1 \\implies I_G(\\text{Node 2}) = 1 - (0^2+1^2) = 0$.\n  - Impurity reduction: $\\Delta I_G(\\text{root}) = \\frac{11}{72} - (\\frac{4}{12} \\cdot \\frac{3}{8} + \\frac{8}{12} \\cdot 0) = \\frac{11}{72} - \\frac{1}{8} = \\frac{11-9}{72} = \\frac{2}{72} = \\frac{1}{36}$.\n  - Importance of $X_1$: $Imp_G(X_1) = \\frac{12}{12} \\cdot \\Delta I_G(\\text{root}) = \\frac{1}{36}$.\n\n- **Node 1 split on $X_2$:**\n  - $I_G(\\text{Node 1}) = \\frac{3}{8}$.\n  - $I_G(\\text{Node 3, } X_1=0, X_2=0): N=3, p_1=1 \\implies I_G(\\text{Node 3}) = 0$.\n  - $I_G(\\text{Node 4, } X_1=0, X_2=1): N=1, p_0=1 \\implies I_G(\\text{Node 4}) = 0$.\n  - Impurity reduction: $\\Delta I_G(\\text{Node 1}) = \\frac{3}{8} - (\\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 0) = \\frac{3}{8}$.\n  - Importance of $X_2$: $Imp_G(X_2) = \\frac{4}{12} \\cdot \\Delta I_G(\\text{Node 1}) = \\frac{1}{3} \\cdot \\frac{3}{8} = \\frac{1}{8}$.\n\n**Shannon Entropy:** The entropy is $I_H = -\\sum_k p_k \\ln(p_k)$.\n- **Root split on $X_1$:**\n  - $I_H(\\text{Root}) = -(\\frac{1}{12}\\ln\\frac{1}{12} + \\frac{11}{12}\\ln\\frac{11}{12}) = \\frac{\\ln(12) + 11 \\ln(12/11)}{12}$.\n  - $I_H(\\text{Node 1}) = -(\\frac{1}{4}\\ln\\frac{1}{4} + \\frac{3}{4}\\ln\\frac{3}{4}) = \\frac{\\ln(4) + 3\\ln(4/3)}{4}$.\n  - $I_H(\\text{Node 2}) = 0$.\n  - Information gain: $\\Delta I_H(\\text{root}) = I_H(\\text{Root}) - \\frac{4}{12}I_H(\\text{Node 1}) = \\frac{16\\ln(2)+15\\ln(3) - 11\\ln(11)}{12}$.\n  - Importance of $X_1$: $Imp_H(X_1) = \\frac{12}{12} \\Delta I_H(\\text{root}) = \\frac{16\\ln(2)+15\\ln(3) - 11\\ln(11)}{12}$.\n\n- **Node 1 split on $X_2$:**\n  - $I_H(\\text{Node 1}) = \\frac{\\ln(4) + 3\\ln(4/3)}{4}$.\n  - $I_H(\\text{Node 3}) = 0$ and $I_H(\\text{Node 4}) = 0$.\n  - Information gain: $\\Delta I_H(\\text{Node 1}) = I_H(\\text{Node 1}) - 0 = \\frac{8\\ln(2)-3\\ln(3)}{4}$.\n  - Importance of $X_2$: $Imp_H(X_2) = \\frac{4}{12} \\Delta I_H(\\text{Node 1}) = \\frac{1}{3} \\frac{8\\ln(2)-3\\ln(3)}{4} = \\frac{8\\ln(2)-3\\ln(3)}{12}$.\n\n**2. Permutation Importance**\n\nPermutation importance for a feature $X_j$ is defined as $PI(X_j) = \\text{accuracy}_{\\text{original}} - E[\\text{accuracy}_{\\text{permuted}}]$.\n\nFirst, we compute the baseline accuracy on the original dataset.\n- For $(X_1=0, X_2=0, Y=1)$ ($3$ samples): prediction is $\\hat{Y}=1$. Correct.\n- For $(X_1=0, X_2=1, Y=0)$ ($1$ sample): prediction is $\\hat{Y}=0$. Correct.\n- For $(X_1=1, X_2=0, Y=1)$ ($3$ samples): prediction is $\\hat{Y}=1$. Correct.\n- For $(X_1=1, X_2=1, Y=1)$ ($5$ samples): prediction is $\\hat{Y}=1$. Correct.\nAll $12$ samples are classified correctly. Thus, $\\text{accuracy}_{\\text{original}} = \\frac{12}{12} = 1$.\n\nThe expected accuracy under permutation is $E[\\text{accuracy}] = \\frac{1}{n} \\sum_{i=1}^{n} P(\\hat{y}_i = y_i)$.\n\n**Permutation Importance of $X_1$**\nThe vector of $X_1$ values has $4$ zeros and $8$ ones. When permuted, for any sample $i$, the new value $X'_{1,i}$ has a probability $P(X'_{1,i}=0) = \\frac{4}{12} = \\frac{1}{3}$ and $P(X'_{1,i}=1) = \\frac{8}{12} = \\frac{2}{3}$. The values of $X_2$ and $Y$ for each sample remain fixed.\nWe calculate the probability of a correct prediction for each sample.\n- **$6$ samples with $X_2=0, Y=1$** (from groups $(0,0,1)$ and $(1,0,1)$):\n  - The prediction depends on $X'_{1,i}$.\n  - If $X'_{1,i}=0$ (prob $\\frac{1}{3}$), predict $\\hat{Y}=1$. Correct.\n  - If $X'_{1,i}=1$ (prob $\\frac{2}{3}$), predict $\\hat{Y}=1$. Correct.\n  - The prediction is always correct. $P(\\text{correct}) = 1$.\n- **$1$ sample with $X_2=1, Y=0$**:\n  - If $X'_{1,i}=0$ (prob $\\frac{1}{3}$), predict $\\hat{Y}=0$. Correct.\n  - If $X'_{1,i}=1$ (prob $\\frac{2}{3}$), predict $\\hat{Y}=1$. Incorrect.\n  - $P(\\text{correct}) = \\frac{1}{3}$.\n- **$5$ samples with $X_2=1, Y=1$**:\n  - If $X'_{1,i}=0$ (prob $\\frac{1}{3}$), predict $\\hat{Y}=0$. Incorrect.\n  - If $X'_{1,i}=1$ (prob $\\frac{2}{3}$), predict $\\hat{Y}=1$. Correct.\n  - $P(\\text{correct}) = \\frac{2}{3}$.\n\nThe expected number of correct predictions is $6 \\cdot 1 + 1 \\cdot \\frac{1}{3} + 5 \\cdot \\frac{2}{3} = 6 + \\frac{1}{3} + \\frac{10}{3} = 6 + \\frac{11}{3} = \\frac{18+11}{3} = \\frac{29}{3}$.\nThe expected accuracy is $E[\\text{accuracy}_{\\text{permuted } X_1}] = \\frac{29/3}{12} = \\frac{29}{36}$.\nThe permutation importance of $X_1$ is $PI(X_1) = 1 - \\frac{29}{36} = \\frac{7}{36}$.\n\n**Permutation Importance of $X_2$**\nThe vector of $X_2$ values has $3+3=6$ zeros and $1+5=6$ ones. When permuted, for any sample $i$, the new value $X'_{2,i}$ has probability $P(X'_{2,i}=0) = \\frac{6}{12} = \\frac{1}{2}$ and $P(X'_{2,i}=1) = \\frac{6}{12} = \\frac{1}{2}$. The values of $X_1$ and $Y$ remain fixed.\n- **$4$ samples with $X_1=0$**:\n  - $3$ samples have $Y=1$. Prediction is $\\hat{Y}=1$ if $X'_{2,i}=0$ (prob $\\frac{1}{2}$), and $\\hat{Y}=0$ if $X'_{2,i}=1$ (prob $\\frac{1}{2}$). $P(\\text{correct}) = \\frac{1}{2}$.\n  - $1$ sample has $Y=0$. Prediction is $\\hat{Y}=1$ if $X'_{2,i}=0$ (prob $\\frac{1}{2}$), and $\\hat{Y}=0$ if $X'_{2,i}=1$ (prob $\\frac{1}{2}$). $P(\\text{correct}) = \\frac{1}{2}$.\n- **$8$ samples with $X_1=1$**:\n  - All $8$ samples have $Y=1$. The tree predicts $\\hat{Y}=1$ for any sample with $X_1=1$, regardless of $X_2$. The prediction is always correct. $P(\\text{correct}) = 1$.\n\nThe expected number of correct predictions is $3 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} + 8 \\cdot 1 = \\frac{4}{2} + 8 = 2+8=10$.\nThe expected accuracy is $E[\\text{accuracy}_{\\text{permuted } X_2}] = \\frac{10}{12} = \\frac{5}{6}$.\nThe permutation importance of $X_2$ is $PI(X_2) = 1 - \\frac{5}{6} = \\frac{1}{6}$.\n\n**3. Final Calculation**\n\nThe final task is to compute the difference between the permutation importance of $X_1$ and that of $X_2$.\n$PI(X_1) - PI(X_2) = \\frac{7}{36} - \\frac{1}{6} = \\frac{7}{36} - \\frac{6}{36} = \\frac{1}{36}$.",
            "answer": "$$\\boxed{\\frac{1}{36}}$$"
        },
        {
            "introduction": "Feature importance scores can be misleading if interpreted without caution, especially when dealing with correlated predictors—a common occurrence in real-world datasets. This problem presents a classic thought experiment: what happens to the importance of two causal, yet perfectly correlated, features? By analyzing this scenario , you will uncover a key limitation of standard importance metrics and learn to critically assess the feature rankings produced by your models.",
            "id": "2384494",
            "problem": "You are given a gene expression data set with $n$ samples and $p$ genes, and the task is to classify a binary phenotype $Y \\in \\{0,1\\}$ using a Random Forest classifier that uses bootstrap aggregation and feature subsampling at each split. Suppose there are $2$ truly causal genes, indexed $a$ and $b$, whose expression features are $X_a$ and $X_b$. Across all $n$ samples, these two gene expression features are perfectly correlated in the sense that $\\rho(X_a, X_b) = 1$ and, up to a positive scaling constant, $X_a = X_b$. You compute feature importance in two standard ways: mean decrease in impurity (Gini) and permutation importance computed on Out-Of-Bag (OOB) samples. Which option best describes what happens to the measured importance scores of $X_a$ and $X_b$ in such a Random Forest and proposes a valid way to identify this situation from model behavior and the data?\n\nA. Because $X_a$ and $X_b$ are redundant, splits will be distributed between them across trees, so their impurity-based importances are divided, and the permutation importance of each feature individually can be small (possibly near $0$) if the other remains unpermuted; a practical diagnosis is to examine the pairwise correlation matrix to detect $\\rho(X_a, X_b) \\approx 1$ and to compare joint permutation (permute $X_a$ and $X_b$ together) against marginal permutation of each alone, or to use conditional permutation importance that conditions on the other feature.\n\nB. Perfect correlation causes both $X_a$ and $X_b$ to be selected in nearly every tree, so their importances approximately double relative to the single-feature case; this can be diagnosed by observing unusually large single-feature permutation importance for both genes.\n\nC. The Random Forest learning algorithm detects perfect correlation and discards one feature globally, forcing its importance to exactly $0$ for both impurity and permutation metrics; this can be diagnosed because the discarded feature has zero empirical variance across bootstrap samples.\n\nD. Perfect correlation makes the model predictions unstable and drives the Out-Of-Bag (OOB) error to the level of random guessing; diagnosis is based on observing identical partial dependence plots for $X_a$ and $X_b$ as evidence of perfect correlation.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   **Dataset**: A gene expression dataset with $n$ samples and $p$ genes.\n-   **Task**: Classification of a binary phenotype $Y \\in \\{0,1\\}$.\n-   **Model**: A Random Forest classifier.\n-   **Model Parameters**: The model uses bootstrap aggregation and feature subsampling at each split.\n-   **Features of Interest**: Two truly causal genes with expression features denoted as $X_a$ and $X_b$.\n-   **Key Condition**: The features $X_a$ and $X_b$ are perfectly correlated, with correlation coefficient $\\rho(X_a, X_b) = 1$. It is also stated that \"up to a positive scaling constant, $X_a = X_b$\". This implies $X_a = k X_b$ for some constant $k > 0$.\n-   **Metrics**: Feature importance is computed in two ways:\n    1.  Mean decrease in impurity (Gini importance).\n    2.  Permutation importance computed on Out-Of-Bag (OOB) samples.\n-   **Question**: Describe the effect on the importance scores of $X_a$ and $X_b$ and propose a valid method for identifying this situation.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in machine learning theory and its application in computational biology. Random Forests, feature importance metrics (Gini, permutation), and the issue of multicollinearity (correlated features) are standard topics. The scenario of highly correlated gene expression data is common in bioinformatics.\n2.  **Well-Posed**: The problem is well-posed. It describes a specific scenario and asks for the expected behavior of well-defined metrics, for which a unique and correct conceptual answer exists based on the mechanics of the Random Forest algorithm.\n3.  **Objective**: The problem statement is objective, using precise technical language without ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n### Derivation and Option Analysis\n\nThe problem concerns the behavior of feature importance measures in a Random Forest a_gorithm when two predictive features, $X_a$ and $X_b$, are perfectly correlated. Let us analyze the situation based on first principles. The condition $\\rho(X_a, X_b) = 1$ and $X_a = k X_b$ for $k > 0$ means that both features carry identical information regarding the phenotype $Y$. A split on $X_a$ at a threshold $t$ is equivalent to a split on $X_b$ at a threshold $k^{-1}t$.\n\nA Random Forest builds an ensemble of decision trees. For each tree, two levels of randomization are introduced:\n1.  **Bootstrap Aggregation**: Each tree is trained on a random subsample of the original $n$ samples, drawn with replacement.\n2.  **Feature Subsampling**: At each node of a tree, a split is determined by searching for the best feature and split-point only within a small, random subset of the $p$ total features. The size of this subset is a hyperparameter, typically denoted as `mtry`.\n\nLet's analyze the two specified importance metrics under these conditions.\n\n**1. Mean Decrease in Impurity (MDI or Gini Importance)**\nThe MDI for a feature is the total reduction in the Gini impurity criterion summed over all splits in the forest where that feature was used, averaged over all trees. Since $X_a$ and $X_b$ are causal, they are strong predictors. When a split is to be made in a tree, and the random feature subset selected at that node contains either $X_a$ or $X_b$ (but not both), that feature will likely be chosen if it is the best predictor in the subset. If the random subset contains *both* $X_a$ and $X_b$, they are equally good candidates for the split. The algorithm will have no preferential reason to choose one over the other. Typically, the choice might be arbitrary, for instance, based on which feature appears first in the data matrix.\nOver the ensemble of many trees, the opportunities to be chosen for a split are thus stochastically distributed between $X_a$ and $X_b$. Neither feature will be selected in all cases where such a predictive split is needed. Consequently, the total importance attributable to the information they both carry is *divided* between their individual MDI scores. Each feature will receive a score that is substantially less than the score a single such feature would have received if the other were absent. Their importance is diluted, not amplified.\n\n**2. Permutation Importance (on OOB samples)**\nThis metric is calculated as follows for a feature $X_j$:\ni. The prediction accuracy (or another score) of the trained forest is computed on the OOB samples.\nii. The values of feature $X_j$ in the OOB data are randomly permuted, breaking any association between $X_j$ and the true outcome $Y$.\niii. The prediction accuracy is re-computed on this permuted data.\niv. The permutation importance of $X_j$ is the drop in accuracy from step (i) to (iii).\n\nNow consider the permutation importance of $X_a$. When we permute the values of $X_a$, the values of $X_b$ remain in their original, unpermuted order. Since $X_b$ is perfectly correlated with the *original* $X_a$, the model can still access the complete predictive information from $X_b$. The trees in the forest that rely on this information for prediction will simply use $X_b$. Therefore, permuting $X_a$ while leaving $X_b$ intact will cause little to no drop in model performance. The measured permutation importance for $X_a$ will be very small, potentially close to $0$. By symmetry, the same is true when permuting $X_b$ while leaving $X_a$ intact. This demonstrates a critical weakness of standard permutation importance: its inability to properly assign importance to individual features within a group of correlated predictors. Both features will appear to be unimportant, which is misleading.\n\n**3. Diagnosis of the Situation**\n-   **Data-level Diagnosis**: The most direct and simplest method is to compute the pairwise correlation matrix for all features. A high correlation coefficient, $\\rho(X_a, X_b) \\approx 1$, immediately reveals the redundancy. This should be a standard step in exploratory data analysis.\n-   **Model-based Diagnosis**: The observation that two biologically plausible features receive low permutation importance is a strong signal. To confirm this is due to correlation, one can use more sophisticated importance measures.\n    -   **Joint Permutation Importance**: Permute $X_a$ and $X_b$ *simultaneously*. This destroys the predictive information from both, and the model performance should decrease substantially, revealing their high joint importance. Comparing a large joint importance to small individual (marginal) importances is a clear diagnostic.\n    -   **Conditional Permutation Importance**: This family of methods aims to measure the importance of a feature conditional on another. For example, the importance of $X_a$ conditional on $X_b$ would measure the additional predictive value of $X_a$ once the effect of $X_b$ is accounted for. In this case of perfect correlation, the conditional importance of $X_a$ given $X_b$ would be $0$, confirming its redundancy.\n\nNow let us evaluate the given options.\n\n**Option A**: \"Because $X_a$ and $X_b$ are redundant, splits will be distributed between them across trees, so their impurity-based importances are divided, and the permutation importance of each feature individually can be small (possibly near $0$) if the other remains unpermuted; a practical diagnosis is to examine the pairwise correlation matrix to detect $\\rho(X_a, X_b) \\approx 1$ and to compare joint permutation (permute $X_a$ and $X_b$ together) against marginal permutation of each alone, or to use conditional permutation importance that conditions on the other feature.\"\n-   **Analysis**: This statement accurately describes the division of impurity-based importance. It correctly identifies that the marginal permutation importance of each feature will be small due to the masking effect of the other. It also proposes valid and standard diagnostic procedures: checking the correlation matrix and using advanced permutation schemes like joint or conditional importance.\n-   **Verdict**: **Correct**.\n\n**Option B**: \"Perfect correlation causes both $X_a$ and $X_b$ to be selected in nearly every tree, so their importances approximately double relative to the single-feature case; this can be diagnosed by observing unusually large single-feature permutation importance for both genes.\"\n-   **Analysis**: This is fundamentally incorrect. First, due to feature subsampling, it is not even guaranteed that both features are available for selection at a given node. Second, even if both are available, a tree will select only one for the split. They are competitors, not collaborators. Their importance is divided, not doubled. Finally, the permutation importance will be unusually *small*, not large, due to masking.\n-   **Verdict**: **Incorrect**.\n\n**Option C**: \"The Random Forest learning algorithm detects perfect correlation and discards one feature globally, forcing its importance to exactly $0$ for both impurity and permutation metrics; this can be diagnosed because the discarded feature has zero empirical variance across bootstrap samples.\"\n-   **Analysis**: The standard Random Forest algorithm does not perform global feature selection. Feature selection occurs locally at each node of each tree. Therefore, no feature is \"discarded globally\". A feature's importance might be low, but it is unlikely to be exactly $0$ unless it is never chosen for a split in any tree, which is improbable for a causal feature. The diagnostic method is also nonsensical; bootstrap sampling acts on samples (rows), not features (columns), and would not alter a feature's empirical variance to zero.\n-   **Verdict**: **Incorrect**.\n\n**Option D**: \"Perfect correlation makes the model predictions unstable and drives the Out-Of-Bag (OOB) error to the level of random guessing; diagnosis is based on observing identical partial dependence plots for $X_a$ and $X_b$ as evidence of perfect correlation.\"\n-   **Analysis**: The premise is flawed. Multicollinearity leads to unstable *coefficient estimates* in linear models, but tree-based ensembles like Random Forests are robust in terms of *predictive accuracy*. The model will perform well because the necessary information is available from either $X_a$ or $X_b$. The OOB error will be low, not high. While it is true that the partial dependence plots (PDPs) for $X_a$ and $X_b$ would be identical (up to axis scaling), this is a consequence, not a primary problem. The main claim that predictions are unstable and OOB error is high is fundamentally wrong.\n-   **Verdict**: **Incorrect**.\n\nBased on this rigorous analysis, Option A is the only one that correctly describes the behavior of the feature importance metrics and proposes valid diagnostic methods.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Building on our conceptual understanding, this final practice challenges you to observe these principles in action by implementing a Random Forest classifier. You will explore how impurity-based importance, a cornerstone of tree ensembles, assigns relevance in two important edge cases: a completely uninformative (constant) feature and a perfectly redundant (duplicated) feature. This coding exercise  provides a powerful, practical demonstration of how importance can be diluted among correlated predictors and why it's crucial to be aware of your data's structure.",
            "id": "3166180",
            "problem": "You are asked to implement, from first principles, a binary-classification Random Forest based on Classification and Regression Trees (CART), using the Gini impurity as the split criterion, and to compute normalized feature importance scores defined as the mean decrease in impurity. Your implementation must handle edge cases where some features are constant (zero variance). You will then run the implementation on three specified test cases and report a compact numerical summary.\n\nDefinitions and requirements:\n- A decision tree is grown recursively. At each internal node with data indices $\\mathcal{I}$, you must:\n  1) Randomly select a subset of features of size $m$ (without replacement) from the $d$ available features, independently at each node.\n  2) For each selected feature $j$, consider all candidate thresholds formed as midpoints between sorted unique values of that feature among the samples in $\\mathcal{I}$. If a feature has only a single unique value, it cannot be split at this node.\n  3) For a candidate threshold $\\tau$, define the left child as the set of indices with $x_{ij} \\le \\tau$ and the right child as the set of indices with $x_{ij} > \\tau$. Compute the weighted Gini impurity of this split and choose the feature and threshold that maximizes the impurity decrease. If the best impurity decrease is non-positive, make the node a leaf.\n- For any node with index set $\\mathcal{I}$ and class labels $y_i \\in \\{0,1\\}$, the Gini impurity is $G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2$, where $p_k$ is the class proportion in $\\mathcal{I}$.\n- For a split of $\\mathcal{I}$ into left $\\mathcal{L}$ and right $\\mathcal{R}$ using feature $j$ and threshold $\\tau$, the impurity decrease contributed by that split is\n$$\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R}).$$\n- A Random Forest with $T$ trees is constructed by training each tree on a bootstrap sample of size $N$ (sampling with replacement) from the original training set, and applying the node-wise random feature subspace selection of size $m$.\n- The feature importance for feature $j$ over one tree is the sum, over all internal nodes where the best split uses feature $j$, of $\\Delta(\\mathcal{I}, j, \\tau)$ weighted by the number of samples $|\\mathcal{I}|$ at that node. Aggregate this sum over all trees. Let $S$ denote the total across all features of these aggregated, sample-weighted decreases. If $S > 0$, define the normalized importance of feature $j$ as $I_j = \\frac{\\text{aggregated decrease for } j}{S}$ so that $\\sum_{j=1}^{d} I_j = 1$. If $S = 0$, define $I_j = 0$ for all $j$.\n- There is no restriction on tree depth other than the implicit stopping condition when no valid split yields positive impurity decrease.\n\nImplementation constraints:\n- You must implement the above procedure exactly as stated, using pure numerical operations. No external machine learning libraries are allowed.\n- All randomness must be made reproducible using the specified seeds.\n\nDatasets and test suite:\nYou will run your implementation on three cases. In each case, construct the feature matrix $X \\in \\mathbb{R}^{N \\times d}$ and labels $y \\in \\{0,1\\}^N$ as specified, then train a Random Forest with the specified parameters, and compute the normalized feature importances $\\{I_j\\}_{j=1}^d$.\n\nCase $1$ (constant and noise features with a single informative feature):\n- Data generation seed: $42$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = 0$ for all $i$ (a constant feature).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently for all $i$.\n- Define labels $y_i = 1$ if $x_{i1} > 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 2$, forest randomness seed $2024$.\n- Compute normalized importances $(I_1, I_2, I_3)$.\n\nCase $2$ (perfectly duplicated informative feature and one noise feature):\n- Data generation seed: $123$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = x_{i1}$ for all $i$ (a perfect duplicate).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently.\n- Define labels $y_i = 1$ if $x_{i1} > 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 1$, forest randomness seed $2025$.\n- Compute normalized importances $(I_1, I_2, I_3)$, and also compute the duplicate-share statistic $S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}$.\n\nCase $3$ (all features constant):\n- Data generation seed: $7$.\n- $N = 100$, $d = 2$.\n- Set $x_{i1} = 0$ and $x_{i2} = 3$ for all $i$ (both features constant).\n- Generate labels $y_i$ as independent Bernoulli with $P(y_i = 1) = 0.5$ using the given seed.\n- Random Forest parameters: number of trees $T = 10$, node-wise feature subset size $m = 2$, forest randomness seed $99$.\n- Compute normalized importances $(I_1, I_2)$ and their sum $Q = I_1 + I_2$.\n\nNumerical reporting:\n- For Case $1$, report the three floats $I_1$, $I_2$, $I_3$.\n- For Case $2$, report the four floats $S_{\\text{dup}}$, $I_1$, $I_2$, $I_3$.\n- For Case $3$, report the three floats $Q$, $I_1$, $I_2$.\n- Round each reported float to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing all the reported results as a comma-separated list enclosed in square brackets, in the following order:\n$[I_1^{(1)}, I_2^{(1)}, I_3^{(1)}, S_{\\text{dup}}^{(2)}, I_1^{(2)}, I_2^{(2)}, I_3^{(2)}, Q^{(3)}, I_1^{(3)}, I_2^{(3)}]$,\nwhere the superscripts indicate the case index. There are no physical units. Angles are not used. All proportions must be reported as decimals, not as percentages.",
            "solution": "The user has specified the task of implementing a Random Forest classifier for binary classification from first principles. The implementation must adhere to a precise set of definitions for Classification and Regression Trees (CART), Gini impurity, node splitting, and a specific method for calculating feature importance based on the mean decrease in impurity. The implementation will be validated against three distinct test cases, and a set of numerical results must be reported in a specific format.\n\n### Problem Validation\n\nFirst, I will validate the problem statement against the required criteria.\n\n**Step 1: Extract Givens**\n\n- **Algorithm:** A Random Forest composed of `T` Classification and Regression Trees (CART) for binary classification (`y \\in \\{0,1\\}`).\n- **Tree Construction (Recursion):**\n    - At each node `\\mathcal{I}`, randomly select `m` features (from `d` total) without replacement.\n    - For each selected feature `j`, find the optimal split.\n    - **Thresholds (`\\tau`):** Midpoints between sorted unique values of feature `j` among samples in `\\mathcal{I}`.\n    - **Split Rule:** Left child `\\mathcal{L} = \\{i \\in \\mathcal{I} | x_{ij} \\le \\tau\\}`, Right child `\\mathcal{R} = \\{i \\in \\mathcal{I} | x_{ij} > \\tau\\}`.\n    - **Stopping Conditions:** A node becomes a leaf if the best impurity decrease is non-positive, or if no valid splits can be made (e.g., all features at the node are constant).\n- **Split Criterion (Gini Impurity):**\n    - **Impurity of a node `\\mathcal{I}`:** `G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2`, where `p_k` is the proportion of class `k`.\n    - **Impurity Decrease (Gini Gain):** `\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R})`. The split with maximum `\\Delta` is chosen.\n- **Ensemble Method (Random Forest):**\n    - Each of the `T` trees is trained on a bootstrap sample of size `N` (sampling with replacement) from the training data of size `N`.\n- **Feature Importance:**\n    - For a single tree and feature `j`, importance is the sum of `|\\mathcal{I}| \\cdot \\Delta(\\mathcal{I}, j, \\tau)` over all nodes where `j` was the best split feature.\n    - Total importance for feature `j` is the sum aggregated over all `T` trees.\n    - **Normalization:** Let `S` be the sum of aggregated importances over all features. Normalized importance `I_j` is `\\frac{\\text{aggregated decrease for } j}{S}` if `S > 0`, and `I_j = 0` if `S = 0`.\n- **Test Cases & Parameters:**\n    - **Case 1:** Data seed=`42`, `N=200`, `d=3`. `x_1 \\sim \\mathcal{N}(0,1)`, `x_2=0`, `x_3 \\sim \\mathcal{N}(0,1)`. `y=1` if `x_1 > 0`, else `0`. RF params: `T=50`, `m=2`, forest seed=`2024`. Report: `(I_1, I_2, I_3)`.\n    - **Case 2:** Data seed=`123`, `N=200`, `d=3`. `x_1 \\sim \\mathcal{N}(0,1)`, `x_2=x_1`, `x_3 \\sim \\mathcal{N}(0,1)`. `y=1` if `x_1 > 0`, else `0`. RF params: `T=50`, `m=1`, forest seed=`2025`. Report: `(S_{\\text{dup}}, I_1, I_2, I_3)` where `S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}`.\n    - **Case 3:** Data seed=`7`, `N=100`, `d=2`. `x_1=0`, `x_2=3`. `y \\sim \\text{Bernoulli}(0.5)`. RF params: `T=10`, `m=2`, forest seed=`99`. Report: `(Q, I_1, I_2)` where `Q = I_1 + I_2`.\n- **Output:** A single comma-separated list of `10` floating-point numbers rounded to six decimal places, enclosed in brackets.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem describes the standard Random Forest algorithm. The CART methodology, Gini impurity criterion, and mean decrease in impurity (MDI) for feature importance are all well-established concepts in statistical learning. The definitions provided are mathematically and algorithmically correct.\n- **Well-Posed:** The problem is meticulously specified. All parameters (`N`, `d`, `T`, `m`), data generation procedures, and random seeds are provided, which ensures that the result is unique, deterministic, and computable. The edge cases (constant features, no positive gain) are explicitly defined.\n- **Objective:** The language is formal, precise, and free of any subjective or ambiguous terminology.\n\nThe problem does not violate any of the invalidity criteria. It is a well-defined, scientifically sound computational task.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with the implementation and solution.\n\n### Algorithmic Implementation\n\nThe solution will be structured as a set of functions that collectively implement the specified Random Forest algorithm.\n\n1.  **`gini_impurity(y)`**: A helper function to compute the Gini impurity for a given array of labels `y`.\n2.  **`find_best_split(X, y, idxs, feature_subset)`**: This function iterates through a given subset of features. For each feature, it evaluates all valid thresholds (midpoints of unique values) to find the split that maximizes the Gini gain. It returns a dictionary containing the details of the best split found (feature index, threshold, gain, and child indices) or `None` if no valid split is found.\n3.  **`grow_tree(X, y, idxs, m, d, rng, tree_importances)`**: A recursive function that builds a single decision tree. At each step (node), it checks for stopping conditions (e.g., pure node). If not a leaf, it randomly selects `m` features, calls `find_best_split` to determine the optimal split, records the contribution to feature importance (`|\\mathcal{I}| \\cdot \\Delta`), and then recursively calls itself for the left and right children.\n4.  **`run_rf_case(X, y, T, m, forest_seed)`**: This function orchestrates the training of the entire Random Forest for a single test case. It initializes a random number generator with the forest seed. It then iterates `T` times, each time creating a bootstrap sample of the data and growing a tree using `grow_tree`. It aggregates the feature importances from all trees and returns the final normalized importance scores.\n5.  **`solve()`**: The main function that prepares the data for each of the three test cases as specified, calls `run_rf_case` with the appropriate parameters, computes the required summary statistics (`S_{\\text{dup}}`, `Q`), and prints the final combined results in the specified format.\n\nRandomness is managed carefully using `numpy.random.default_rng` to ensure reproducibility for data generation and all stochastic aspects of the algorithm (bootstrapping and feature selection). Special attention is given to the edge cases, such as handling constant features (which cannot be split) and normalizing importances only if the total impurity decrease is positive.",
            "answer": "```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Computes the Gini impurity of a set of labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    proportions = counts / n_samples\n    return 1.0 - np.sum(proportions**2)\n\ndef find_best_split(X, y, idxs, feature_subset):\n    \"\"\"Finds the best split for a node by iterating through features and thresholds.\"\"\"\n    X_node, y_node = X[idxs], y[idxs]\n    n_node = len(y_node)\n    \n    if n_node = 1:\n        return None\n\n    g_parent = gini_impurity(y_node)\n    best_gain = -1.0\n    best_split_info = None\n\n    for j in feature_subset:\n        feature_values = X_node[:, j]\n        unique_vals = np.unique(feature_values)\n\n        if len(unique_vals) = 1:\n            continue\n\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n        for tau in thresholds:\n            left_mask = feature_values = tau\n            right_mask = ~left_mask\n\n            y_left, y_right = y_node[left_mask], y_node[right_mask]\n\n            if len(y_left) == 0 or len(y_right) == 0:\n                continue\n\n            g_left = gini_impurity(y_left)\n            g_right = gini_impurity(y_right)\n            n_left, n_right = len(y_left), len(y_right)\n            \n            w_left = n_left / n_node\n            w_right = n_right / n_node\n            \n            gain = g_parent - (w_left * g_left + w_right * g_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_split_info = {\n                    'feature': j, \n                    'threshold': tau, \n                    'gain': gain,\n                    'left_idxs': idxs[left_mask],\n                    'right_idxs': idxs[right_mask]\n                }\n                \n    return best_split_info\n\ndef grow_tree(X, y, idxs, m, d, rng, tree_importances):\n    \"\"\"Recursively grows a single decision tree and accumulates feature importances.\"\"\"\n    if len(np.unique(y[idxs])) == 1:\n        return\n\n    m_node = min(m, d)\n    feature_subset = rng.choice(d, size=m_node, replace=False)\n    \n    best_split = find_best_split(X, y, idxs, feature_subset)\n    \n    if best_split is None or best_split['gain'] = 0:\n        return\n\n    j = best_split['feature']\n    gain = best_split['gain']\n    n_samples = len(idxs)\n    tree_importances[j] += n_samples * gain\n    \n    left_idxs = best_split['left_idxs']\n    right_idxs = best_split['right_idxs']\n    \n    grow_tree(X, y, left_idxs, m, d, rng, tree_importances)\n    grow_tree(X, y, right_idxs, m, d, rng, tree_importances)\n\ndef run_rf_case(X, y, T, m, forest_seed):\n    \"\"\"Runs the Random Forest algorithm for a given case and computes feature importances.\"\"\"\n    N, d = X.shape\n    forest_rng = np.random.default_rng(forest_seed)\n    total_importances = np.zeros(d)\n\n    for _ in range(T):\n        bootstrap_idxs = forest_rng.choice(N, size=N, replace=True)\n        X_sample, y_sample = X[bootstrap_idxs], y[bootstrap_idxs]\n        \n        tree_importances = np.zeros(d)\n        \n        # Grow a tree on the bootstrap sample\n        initial_idxs = np.arange(len(y_sample))\n        grow_tree(X_sample, y_sample, initial_idxs, m, d, forest_rng, tree_importances)\n        \n        total_importances += tree_importances\n\n    S = np.sum(total_importances)\n    if S > 0:\n        normalized_importances = total_importances / S\n    else:\n        normalized_importances = np.zeros(d)\n        \n    return normalized_importances\n\ndef solve():\n    \"\"\"Generates data for three cases, runs the RF, and reports the results.\"\"\"\n    all_results = []\n\n    # Case 1\n    data_seed, N, d = 42, 200, 3\n    T, m, forest_seed = 50, 2, 2024\n    rng_case1 = np.random.default_rng(data_seed)\n    X1 = np.zeros((N, d))\n    X1[:, 0] = rng_case1.normal(size=N)\n    X1[:, 1] = 0.0\n    X1[:, 2] = rng_case1.normal(size=N)\n    y1 = (X1[:, 0] > 0).astype(int)\n    importances1 = run_rf_case(X1, y1, T, m, forest_seed)\n    all_results.extend(importances1)\n\n    # Case 2\n    data_seed, N, d = 123, 200, 3\n    T, m, forest_seed = 50, 1, 2025\n    rng_case2 = np.random.default_rng(data_seed)\n    X2 = np.zeros((N, d))\n    X2[:, 0] = rng_case2.normal(size=N)\n    X2[:, 1] = X2[:, 0]\n    X2[:, 2] = rng_case2.normal(size=N)\n    y2 = (X2[:, 0] > 0).astype(int)\n    importances2 = run_rf_case(X2, y2, T, m, forest_seed)\n    I1_2, I2_2, I3_2 = importances2\n    denom = I1_2 + I2_2 + I3_2\n    S_dup = (I1_2 + I2_2) / denom if denom > 0 else 0.0\n    all_results.extend([S_dup, I1_2, I2_2, I3_2])\n\n    # Case 3\n    data_seed, N, d = 7, 100, 2\n    T, m, forest_seed = 10, 2, 99\n    rng_case3 = np.random.default_rng(data_seed)\n    X3 = np.zeros((N, d))\n    X3[:, 0] = 0.0\n    X3[:, 1] = 3.0\n    y3 = rng_case3.integers(0, 2, size=N)\n    importances3 = run_rf_case(X3, y3, T, m, forest_seed)\n    I1_3, I2_3 = importances3\n    Q = I1_3 + I2_3\n    all_results.extend([Q, I1_3, I2_3])\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}