{
    "hands_on_practices": [
        {
            "introduction": "Many machine learning workflows begin with data preprocessing, such as feature scaling. While often treated as a mere technical step, scaling is a powerful form of inductive bias, especially for distance-based algorithms like k-Nearest Neighbors. This practice  will guide you to computationally explore how different normalization methods change the underlying distance metric, thus altering the hypothesis space and leading to different classification boundaries. You will see firsthand that how you measure distance fundamentally changes what your model learns.",
            "id": "3129970",
            "problem": "You must write a complete program that compares how different feature normalizations induce different hypothesis spaces for a distance-based learner and, in doing so, quantify how normalization acts as inductive bias. The learner is the one-nearest neighbor classifier, formally defined as follows. Given a training set $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ with $\\mathbf{x}_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{0,1,\\dotsc,C-1\\}$, and a metric $d(\\cdot,\\cdot)$, the hypothesis $h$ in the hypothesis space $\\mathcal{H}$ returns, for any query $\\mathbf{q} \\in \\mathbb{R}^d$, the label $y_j$ of the training point $\\mathbf{x}_j$ that minimizes $d(\\mathbf{q}, \\mathbf{x}_j)$ (if there is a tie, pick the smallest index $j$). The hypothesis space is thus parameterized by the choice of metric, which here is induced by normalization.\n\nYou must implement two normalizations computed from the training data only:\n- Z-score normalization: for feature index $j \\in \\{1,\\dotsc,d\\}$, compute the population mean $\\mu_j$ and the population standard deviation $\\sigma_j$, and define the induced distance by\n$$\nd_{\\text{z}}(\\mathbf{q}, \\mathbf{x}) \\triangleq \\sqrt{\\sum_{j=1}^d \\left(\\frac{q_j - x_j}{\\sigma_j}\\right)^2},\n$$\nwith the convention that if $\\sigma_j = 0$ then that feature contributes $0$ to the sum (that is, the feature is ignored in the distance because it has zero variance in the training data).\n- Min-max normalization: for feature index $j$, compute the minimum $m_j$ and the maximum $M_j$, define the range $r_j \\triangleq M_j - m_j$, and define the induced distance by\n$$\nd_{\\text{mm}}(\\mathbf{q}, \\mathbf{x}) \\triangleq \\sqrt{\\sum_{j=1}^d \\left(\\frac{q_j - x_j}{r_j}\\right)^2},\n$$\nwith the convention that if $r_j = 0$ then that feature contributes $0$ to the sum.\n\nThese definitions encode inductive bias through per-feature weights in the metric: features with larger training variability are down-weighted, and the choice between $\\sigma_j$ and $r_j$ yields different weights, thus different hypothesis spaces and, potentially, different predictions.\n\nYou must implement the one-nearest neighbor classifier for each induced distance, returning the label of the single nearest training point for each query point, with a deterministic tie-breaking rule picking the smallest index among equally nearest training points.\n\nStarting from the fundamental definitions above, build a program that, for each test case described below, does the following:\n- Computes the training statistics for each normalization.\n- For each query, computes the predicted label under $d_{\\text{z}}$ and under $d_{\\text{mm}}$.\n- Computes the fraction of queries whose predicted labels agree under both normalizations, expressed as a decimal number rounded to $3$ decimals.\n- Aggregates the results for all test cases into a single line according to the specified output format.\n\nThere are no physical units involved in this problem. Angles do not appear. All outputs must be integers, booleans, floats, or lists of these types.\n\nTest Suite:\n- Test Case $1$ (general case with differing inductive biases and a zero-variance feature present):\n    - Training inputs $\\mathbf{X}$ with $n=6$ and $d=3$:\n      $\\mathbf{x}_1 = (1000, 0, 0)$, $\\mathbf{x}_2 = (1000, 1, 0)$, $\\mathbf{x}_3 = (0, 5, 0)$, $\\mathbf{x}_4 = (0, 6, 0)$, $\\mathbf{x}_5 = (0, 7, 0)$, $\\mathbf{x}_6 = (0, 8, 0)$.\n    - Labels: $y = [0, 0, 1, 1, 1, 1]$.\n    - Queries: $\\mathbf{q}_1 = (700, 6, 0)$, $\\mathbf{q}_2 = (650, 5.5, 0)$, $\\mathbf{q}_3 = (600, 7.5, 0)$.\n- Test Case $2$ (boundary condition where the per-feature scales are proportional between the two normalizations, implying a proportional metric and therefore identical nearest-neighbor rankings for any query):\n    - Training inputs with $n=4$ and $d=2$:\n      $\\mathbf{x}_1 = (0, 0)$, $\\mathbf{x}_2 = (1, 0)$, $\\mathbf{x}_3 = (0, 100)$, $\\mathbf{x}_4 = (1, 100)$.\n    - Labels: $y = [0, 0, 1, 1]$.\n    - Queries: $\\mathbf{q}_1 = (0.9, 10)$, $\\mathbf{q}_2 = (0.5, 90)$, $\\mathbf{q}_3 = (0.2, 30)$.\n- Test Case $3$ (edge case with a zero-variance feature where the queries differ substantially on that feature; the zero-variance feature must be ignored by both normalizations):\n    - Training inputs with $n=3$ and $d=2$:\n      $\\mathbf{x}_1 = (1, 5)$, $\\mathbf{x}_2 = (1, 5)$, $\\mathbf{x}_3 = (1, 10)$.\n    - Labels: $y = [0, 1, 1]$.\n    - Queries: $\\mathbf{q}_1 = (100, 7)$, $\\mathbf{q}_2 = (100, 5)$, $\\mathbf{q}_3 = (100, 9)$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a list of three elements:\n    1. The fraction of queries (rounded to $3$ decimals) for which the predicted labels under $d_{\\text{z}}$ and $d_{\\text{mm}}$ are the same, as a float.\n    2. The list of predicted labels under $d_{\\text{z}}$ for the queries, as integers.\n    3. The list of predicted labels under $d_{\\text{mm}}$ for the queries, as integers.\n- Concretely, the output must look like\n$[\\,[f_1,[\\ell^{\\text{z}}_{1},\\dotsc],[\\ell^{\\text{mm}}_{1},\\dotsc]],\\,[f_2,[\\dotsc],[\\dotsc]],\\,[f_3,[\\dotsc],[\\dotsc]]\\,]$\nwhere $f_k$ is the fraction for test case $k$, $\\ell^{\\text{z}}$ are labels under $d_{\\text{z}}$ and $\\ell^{\\text{mm}}$ are labels under $d_{\\text{mm}}$.\n\nYour program must be self-contained and must not read any input. It should implement the computations exactly as specified, with deterministic tie-breaking by smallest index in case of equal distances.",
            "solution": "The user has provided a well-defined computational problem in the domain of statistical learning theory. The task is to implement a one-nearest neighbor (1-NN) classifier under two different feature normalization schemes and to compare their predictions on a set of query points. The comparison quantifies how the choice of normalization, which acts as a form of inductive bias, affects the hypothesis space.\n\nThe problem is valid. It is scientifically sound, well-posed, and objective. All required definitions, data, and procedures are specified without ambiguity. The tie-breaking rule (smallest index) ensures a unique solution. The conventions for handling zero-variance features are clearly stated, preventing ill-defined computations.\n\nThe core of the problem lies in understanding how different normalization statistics—population standard deviation for Z-score normalization and range for min-max normalization—induce different distance metrics. The distance for a query point $\\mathbf{q}$ and a training point $\\mathbf{x}$ is defined as a scaled Euclidean distance:\n$$\nd(\\mathbf{q}, \\mathbf{x}) = \\sqrt{\\sum_{j=1}^d \\left(w_j (q_j - x_j)\\right)^2}\n$$\nwhere $d$ is the number of features, and $w_j$ is the weight for the $j$-th feature. For Z-score normalization, the weight is the inverse of the population standard deviation, $w_j = 1/\\sigma_j$. For min-max normalization, the weight is the inverse of the feature's range, $w_j = 1/r_j$. A feature with zero variability in the training set ($\\sigma_j=0$ or $r_j=0$) contributes $0$ to the total distance, effectively being ignored.\n\nThe 1-NN hypothesis $h(\\mathbf{q})$ maps a query point $\\mathbf{q}$ to the label $y_i$ of the training point $\\mathbf{x}_i$ that is closest to $\\mathbf{q}$ according to the chosen metric.\n$$\nh(\\mathbf{q}) = y_k \\quad \\text{where} \\quad k = \\underset{i \\in \\{1,\\dots,n\\}}{\\arg\\min} \\, d(\\mathbf{q}, \\mathbf{x}_i)\n$$\nSince the square root function is monotonic, minimizing $d(\\mathbf{q}, \\mathbf{x}_i)$ is equivalent to minimizing the squared distance $d^2(\\mathbf{q}, \\mathbf{x}_i)$, which is computationally more efficient. The tie-breaking rule of selecting the smallest index resolves any ambiguities.\n\nThe procedure for each test case is as follows:\n1.  **Compute Normalization Statistics**: For each feature $j$ of the training data $\\mathbf{X}$, calculate the population standard deviation $\\sigma_j$ and the range $r_j$.\n2.  **Define Metric Weights**: The weights for the Z-score induced metric are $\\{1/\\sigma_j\\}_{j=1}^d$ and for the min-max induced metric are $\\{1/r_j\\}_{j=1}^d$. If $\\sigma_j=0$ or $r_j=0$, the corresponding weight is effectively $0$ as that feature's contribution to the distance sum is defined as $0$.\n3.  **Classify Queries**: For each query point $\\mathbf{q}_k$, compute its squared distance to every training point $\\mathbf{x}_i$ under both $d_{\\text{z}}^2$ and $d_{\\text{mm}}^2$. Identify the index of the nearest neighbor for each metric, applying the tie-breaking rule. The predicted labels are the labels of these nearest neighbors.\n4.  **Calculate Agreement**: Compare the list of predictions from both methods. The agreement fraction is the number of queries for which both methods predict the same label, divided by the total number of queries.\n\nWe apply this procedure to each test case.\n\n**Test Case 1**:\n- Training Data $\\mathbf{X}$: $n=6, d=3$.\n- Statistics for feature $j=1$: $\\sigma_1 = 1000\\sqrt{2}/3 \\approx 471.4$, $r_1 = 1000$.\n- Statistics for feature $j=2$: $\\sigma_2 = \\sqrt{107/12} \\approx 2.986$, $r_2 = 8$.\n- Statistics for feature $j=3$: $\\sigma_3 = 0, r_3 = 0$. This feature is ignored.\nThe weights for the two metrics are different, leading to different hypothesis-space geometries. However, for the three specific query points provided, the determined nearest neighbor is the same under both metrics.\n- For $\\mathbf{q}_1=(700, 6, 0)$, the nearest neighbor is $\\mathbf{x}_4=(0, 6, 0)$ for both metrics. Label: $1$.\n- For $\\mathbf{q}_2=(650, 5.5, 0)$, there is a tie for nearest neighbor between $\\mathbf{x}_3=(0, 5, 0)$ and $\\mathbf{x}_4=(0, 6, 0)$. By the tie-breaking rule (smallest index), $\\mathbf{x}_3$ is chosen for both. Label: $1$.\n- For $\\mathbf{q}_3=(600, 7.5, 0)$, there is a tie between $\\mathbf{x}_5=(0, 7, 0)$ and $\\mathbf{x}_6=(0, 8, 0)$. $\\mathbf{x}_5$ is chosen for both. Label: $1$.\nThe predicted labels are $[1, 1, 1]$ for both methods. The agreement fraction is $3/3 = 1.0$.\n\n**Test Case 2**:\n- Training Data $\\mathbf{X}$: $n=4, d=2$.\n- Statistics for feature $j=1$: $\\sigma_1=0.5, r_1=1$.\n- Statistics for feature $j=2$: $\\sigma_2=50, r_2=100$.\nThe scales (weights) for the Z-score metric are $(1/\\sigma_1, 1/\\sigma_2)=(2, 0.02)$. The scales for the min-max metric are $(1/r_1, 1/r_2)=(1, 0.01)$. The Z-score scales are exactly twice the min-max scales. Therefore, $d_{\\text{z}}(\\mathbf{q}, \\mathbf{x}) = 2 \\cdot d_{\\text{mm}}(\\mathbf{q}, \\mathbf{x})$. Since one metric is a constant multiple of the other, they will always rank neighbors in the same order. The predictions must be identical.\n- Predictions for queries $\\mathbf{q}_1=(0.9, 10), \\mathbf{q}_2=(0.5, 90), \\mathbf{q}_3=(0.2, 30)$ are $[0, 1, 0]$ for both methods. The agreement fraction is $3/3 = 1.0$.\n\n**Test Case 3**:\n- Training Data $\\mathbf{X}$: $n=3, d=2$.\n- Statistics for feature $j=1$: $\\sigma_1=0, r_1=0$. This feature is ignored by both metrics.\n- Statistics for feature $j=2$: $\\sigma_2 = 5\\sqrt{2}/3, r_2 = 5$.\nSince the distance calculation for both metrics depends only on the second feature, for any given query, both will seek to minimize $(q_2 - x_{i2})^2$. The scaling factors $1/\\sigma_2^2$ and $1/r_2^2$ are constant across all training points for a fixed query and do not affect the ranking of neighbors. Thus, the predictions must be identical.\n- Training point feature $2$ values are $[5, 5, 10]$ for labels $[0, 1, 1]$.\n- For $\\mathbf{q}_1=(100, 7)$, $q_2=7$. The nearest $x_{i2}$ is $5$, corresponding to indices $1$ and $2$. The smallest index is $1$, so label is $0$.\n- For $\\mathbf{q}_2=(100, 5)$, $q_2=5$. The nearest $x_{i2}$ is $5$, corresponding to indices $1$ and $2$. The smallest index is $1$, so label is $0$.\n- For $\\mathbf{q}_3=(100, 9)$, $q_2=9$. The nearest $x_{i2}$ is $10$, corresponding to index $3$. Label is $1$.\nThe predicted labels are $[0, 0, 1]$ for both methods. The agreement fraction is $3/3 = 1.0$.",
            "answer": "```python\nimport numpy as np\n\ndef one_nn_predict(queries, train_X, train_y, scales):\n    \"\"\"\n    Predicts labels for queries using the 1-NN rule with a scaled Euclidean distance.\n    The 'scales' vector contains the per-feature weights (e.g., 1/sigma or 1/range).\n    \"\"\"\n    n_train = train_X.shape[0]\n    predictions = []\n\n    for q in queries:\n        # We use the squared Euclidean distance to find the nearest neighbor.\n        # This avoids the computationally expensive sqrt operation and does not\n        # change the result of argmin, as sqrt is a monotonic function.\n        \n        # Broadcasted, vectorized calculation of all squared distances from q\n        # to each point in train_X.\n        # diff shape: (n_train, n_features)\n        diff = q - train_X\n        # scales shape: (n_features,). Broadcasting applies it to each row of diff.\n        scaled_diff = diff * scales\n        # Sum of squares along the feature axis (axis=1)\n        sq_distances = np.sum(scaled_diff**2, axis=1)\n\n        # np.argmin finds the index of the minimum value. In case of ties,\n        # it returns the index of the first occurrence, which matches the\n        # problem's tie-breaking rule (pick the smallest index).\n        nearest_neighbor_idx = np.argmin(sq_distances)\n        predictions.append(train_y[nearest_neighbor_idx])\n        \n    return predictions\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1000., 0., 0.], [1000., 1., 0.], [0., 5., 0.], \n                [0., 6., 0.], [0., 7., 0.], [0., 8., 0.]\n            ]),\n            \"y\": np.array([0, 0, 1, 1, 1, 1]),\n            \"Q\": np.array([\n                [700., 6., 0.], [650., 5.5, 0.], [600., 7.5, 0.]\n            ])\n        },\n        {\n            \"X\": np.array([\n                [0., 0.], [1., 0.], [0., 100.], [1., 100.]\n            ]),\n            \"y\": np.array([0, 0, 1, 1]),\n            \"Q\": np.array([\n                [0.9, 10.], [0.5, 90.], [0.2, 30.]\n            ])\n        },\n        {\n            \"X\": np.array([\n                [1., 5.], [1., 5.], [1., 10.]\n            ]),\n            \"y\": np.array([0, 1, 1]),\n            \"Q\": np.array([\n                [100., 7.], [100., 5.], [100., 9.]\n            ])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X, y, Q = case[\"X\"], case[\"y\"], case[\"Q\"]\n        \n        # Compute stats for Z-score normalization.\n        # np.std with default ddof=0 computes the population standard deviation.\n        sigma = np.std(X, axis=0)\n        \n        # Compute stats for Min-Max normalization.\n        m = np.min(X, axis=0)\n        M = np.max(X, axis=0)\n        r = M - m\n        \n        # Create scales (weights) for distances, handling zero variance/range.\n        # The scale is 1/sigma or 1/r. If the divisor is 0, the scale is 0.\n        # A small epsilon is used for robust floating point comparison.\n        scales_z = np.zeros_like(sigma)\n        valid_sigma_mask = sigma > 1e-12\n        scales_z[valid_sigma_mask] = 1.0 / sigma[valid_sigma_mask]\n        \n        scales_mm = np.zeros_like(r)\n        valid_r_mask = r > 1e-12\n        scales_mm[valid_r_mask] = 1.0 / r[valid_r_mask]\n        \n        # Get predictions for both normalizations.\n        preds_z = one_nn_predict(Q, X, y, scales_z)\n        preds_mm = one_nn_predict(Q, X, y, scales_mm)\n        \n        # Calculate the fraction of queries with agreeing predictions.\n        agreement_count = np.sum(np.array(preds_z) == np.array(preds_mm))\n        agreement_fraction = agreement_count / len(Q)\n        \n        # Format the result for this test case as specified.\n        case_result = [\n            round(agreement_fraction, 3),\n            preds_z,\n            preds_mm\n        ]\n        results.append(case_result)\n\n    # The problem provides a skeleton print statement, which implies that\n    # Python's default string representation of lists is the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Choosing a hypothesis space involves a fundamental dilemma known as the bias-variance tradeoff. A simple model class (e.g., additive functions) has a strong inductive bias which, if incorrect, leads to high systematic error (bias), but its predictions are stable across different training sets (low variance). In this hands-on simulation , you will generate learning curves to compare a simple model to a more complex one, empirically demonstrating how the optimal choice of hypothesis space depends on the signal strength, noise level, and, most critically, the available sample size.",
            "id": "3129988",
            "problem": "You are tasked with designing and implementing a principled simulation to compare two hypothesis spaces in a supervised learning setting under limited sample sizes, and to analyze the bias-variance tradeoff via learning curves. The setting is as follows. Let the input be $\\mathbf{x} = (x_1, x_2)$ with $x_1$ and $x_2$ drawn independently and identically distributed (i.i.d.) from the uniform distribution on $[-1,1]$. The ground-truth regression function is\n$$\nf^\\star(\\mathbf{x}) \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_1 \\;+\\; \\beta_2 x_2 \\;+\\; \\beta_{12} x_1 x_2,\n$$\nand the observed output is\n$$\ny \\;=\\; f^\\star(\\mathbf{x}) \\;+\\; \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent Gaussian noise with variance $\\sigma^2$. Consider the following two hypothesis spaces (sets of functions) for linear least squares:\n- The additive hypothesis space without interaction terms,\n$$\n\\mathcal{H}_{\\text{add}} \\;=\\; \\left\\{ f(\\mathbf{x}) \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_1 \\;+\\; \\theta_2 x_2 \\right\\},\n$$\n- The interaction hypothesis space with an interaction term,\n$$\n\\mathcal{H}_{\\text{int}} \\;=\\; \\left\\{ f(\\mathbf{x}) \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_1 \\;+\\; \\theta_2 x_2 \\;+\\; \\theta_{12} x_1 x_2 \\right\\}.\n$$\nYour program must simulate learning curves and decompose error into bias and variance as functions of the training sample size $n \\in \\{8,32,128\\}$. For each fixed $n$, repeat training across $R$ independent replicates with $R=200$, each replicate using $n$ i.i.d. samples of $\\mathbf{x}$ and $y$ from the above data-generating process. For evaluation, use a fixed test set of size $M=512$ consisting of i.i.d. $\\mathbf{x}$ from the same distribution, and evaluate expected generalization error via the bias-variance decomposition.\n\nStart from the core definitions: for a learned predictor $\\hat{f}$, the expected squared prediction error at a fixed $\\mathbf{x}$ under squared loss is\n$$\n\\mathbb{E}\\!\\left[(\\hat{f}(\\mathbf{x}) - y)^2 \\mid \\mathbf{x}\\right] \\;=\\; \\left(\\mathbb{E}[\\hat{f}(\\mathbf{x})] - f^\\star(\\mathbf{x})\\right)^2 \\;+\\; \\mathrm{Var}(\\hat{f}(\\mathbf{x})) \\;+\\; \\sigma^2,\n$$\nwhere the expectation and variance are over the random training sample (and any algorithmic randomness). The learning curve at sample size $n$ is the average of this quantity over the test distribution of $\\mathbf{x}$. In your simulation, approximate these expectations by Monte Carlo averages across the $R$ replicates and across the $M$ test inputs.\n\nFor each test case, compute the learning curves for both $\\mathcal{H}_{\\text{add}}$ and $\\mathcal{H}_{\\text{int}}$, and then report the smallest training size $n$ (from the given set) at which the interaction hypothesis space’s generalization error is less than or equal to the additive hypothesis space’s generalization error. If no such crossing occurs for the given $n$ values, report $0$ for that test case.\n\nUse the following test suite of parameter values, where all parameters not explicitly listed are fixed to $\\beta_0 = 0$, $\\beta_1 = 1$, and $\\beta_2 = 1$:\n- Test case $1$: $\\beta_{12} = 0.8$, $\\sigma^2 = 0.05$,\n- Test case $2$: $\\beta_{12} = 0.8$, $\\sigma^2 = 1.0$,\n- Test case $3$: $\\beta_{12} = 0.0$, $\\sigma^2 = 0.05$.\n\nYour program must:\n- For each test case, simulate and estimate the learning curves at $n \\in \\{8,32,128\\}$ for both hypothesis spaces,\n- Use $R=200$ training replicates and a fixed test set size $M=512$ for the Monte Carlo approximations,\n- For each test case, return a single integer: the smallest $n$ at which the interaction space’s estimated generalization error is less than or equal to the additive space’s estimated generalization error, or $0$ if no crossing occurs.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly one integer per test case, in the order listed above.",
            "solution": "The problem requires a numerical simulation to compare the generalization performance of two nested linear models: an additive model, $\\mathcal{H}_{\\text{add}}$, and a model with an interaction term, $\\mathcal{H}_{\\text{int}}$. The comparison is framed within the context of the bias-variance tradeoff, and performance is evaluated as a function of the training sample size, $n$. The goal is to determine, for different data-generating parameters, the minimum sample size at which the more complex model, $\\mathcal{H}_{\\text{int}}$, achieves a generalization error less than or equal to that of the simpler model, $\\mathcal{H}_{\\text{add}}$.\n\nThe core of the solution is a Monte Carlo simulation designed to estimate the expected generalization error. The expected squared prediction error for a learned model $\\hat{f}$ at a specific input $\\mathbf{x}$ is decomposed into three components:\n$$\n\\mathbb{E}_{\\mathcal{D}, \\varepsilon}\\left[(\\hat{f}(\\mathbf{x}) - y)^2 \\mid \\mathbf{x}\\right] \\;=\\; \\underbrace{\\left(\\mathbb{E}_{\\mathcal{D}}[\\hat{f}(\\mathbf{x})] - f^\\star(\\mathbf{x})\\right)^2}_{\\text{Squared Bias}} \\;+\\; \\underbrace{\\mathbb{E}_{\\mathcal{D}}\\left[\\left(\\hat{f}(\\mathbf{x}) - \\mathbb{E}_{\\mathcal{D}}[\\hat{f}(\\mathbf{x})]\\right)^2\\right]}_{\\text{Variance}} \\;+\\; \\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n$$\nHere, the expectation $\\mathbb{E}_{\\mathcal{D}}$ is taken over the distribution of training datasets $\\mathcal{D}$ of a fixed size $n$. The total generalization error is the expectation of this quantity over the distribution of test inputs $\\mathbf{x}$. Our simulation approximates these expectations by averaging over a large number of draws.\n\nThe simulation procedure is as follows:\n\nFirst, we establish a fixed test set to ensure a consistent evaluation benchmark. A set of $M=512$ input vectors $\\{\\mathbf{x}^{(i)}\\}_{i=1}^M$ is generated, where each $\\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)})$ has components drawn independently from $\\mathcal{U}[-1, 1]$. For these test points, we compute the true, noise-free function values $f^\\star(\\mathbf{x}^{(i)})$ using the ground-truth function $f^\\star(\\mathbf{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2$ with the parameters specified for each test case.\n\nNext, for each training size $n \\in \\{8, 32, 128\\}$, we conduct $R=200$ independent training replicates. In each replicate $r \\in \\{1, \\dots, R\\}$:\n1.  A new training dataset $\\mathcal{D}_r$ of size $n$ is generated. The inputs $\\{\\mathbf{x}_j\\}_{j=1}^n$ are drawn i.i.d. from $\\mathcal{U}[-1, 1] \\times \\mathcal{U}[-1, 1]$. The corresponding outputs $\\{y_j\\}_{j=1}^n$ are computed as $y_j = f^\\star(\\mathbf{x}_j) + \\varepsilon_j$, where $\\varepsilon_j \\sim \\mathcal{N}(0, \\sigma^2)$.\n2.  Two models are fit to the training data $\\mathcal{D}_r$: one from $\\mathcal{H}_{\\text{add}}$ and one from $\\mathcal{H}_{\\text{int}}$. This is done using ordinary least squares, which finds the parameter vector $\\hat{\\theta}$ that minimizes the sum of squared residuals. For a model with design matrix $X$ and target vector $y$, the solution is $\\hat{\\theta} = (X^T X)^{-1} X^T y$. Numerically, this is best computed using a stable method such as Singular Value Decomposition, as implemented in `numpy.linalg.lstsq`.\n    -   For $\\mathcal{H}_{\\text{add}}$, the design matrix has columns corresponding to an intercept, $x_1$, and $x_2$.\n    -   For $\\mathcal{H}_{\\text{int}}$, the design matrix has an additional column for the interaction term $x_1 x_2$.\n3.  The two fitted models, let's call them $\\hat{f}_{\\text{add}, r}$ and $\\hat{f}_{\\text{int}, r}$, are then used to make predictions on the $M$ fixed test points. This yields two prediction vectors, $\\hat{y}_{\\text{add}, r}$ and $\\hat{y}_{\\text{int}, r}$, which are stored.\n\nAfter completing all $R=200$ replicates for a given $n$, we have $R$ predictions for each of the $M$ test points for both hypothesis spaces. We can now approximate the bias and variance terms. For each test point $\\mathbf{x}^{(i)}$ and for each model class (e.g., 'add'), we perform the following calculations:\n-   The average prediction across replicates, $\\bar{f}_{\\text{add}}(\\mathbf{x}^{(i)}) = \\frac{1}{R} \\sum_{r=1}^R \\hat{f}_{\\text{add}, r}(\\mathbf{x}^{(i)})$, approximates $\\mathbb{E}_{\\mathcal{D}}[\\hat{f}_{\\text{add}}(\\mathbf{x}^{(i)})]$.\n-   The squared bias at $\\mathbf{x}^{(i)}$ is estimated as $(\\bar{f}_{\\text{add}}(\\mathbf{x}^{(i)}) - f^\\star(\\mathbf{x}^{(i)}))^2$.\n-   The variance at $\\mathbf{x}^{(i)}$ is estimated as $\\frac{1}{R} \\sum_{r=1}^R (\\hat{f}_{\\text{add}, r}(\\mathbf{x}^{(i)}) - \\bar{f}_{\\text{add}}(\\mathbf{x}^{(i)}))^2$.\n\nThe total generalization error for the model is then the sum of three components: the average of the estimated squared biases over the $M$ test points, the average of the estimated variances over the $M$ test points, and the irreducible error $\\sigma^2$.\n\nThis entire process is repeated for each training size $n$. Finally, by comparing the computed generalization errors for $\\mathcal{H}_{\\text{add}}$ and $\\mathcal{H}_{\\text{int}}$ at each $n$, we identify the smallest $n$ for which $\\text{Error}(\\mathcal{H}_{\\text{int}}) \\le \\text{Error}(\\mathcal{H}_{\\text{add}})$. If this condition is not met for any of the specified values of $n$, the result for that test case is $0$. The procedure is applied to each of the three test cases, which vary the ground-truth parameter $\\beta_{12}$ and the noise variance $\\sigma^2$ to illustrate different scenarios of the bias-variance tradeoff.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (beta_12, sigma_sq)\n        (0.8, 0.05),\n        (0.8, 1.0),\n        (0.0, 0.05),\n    ]\n\n    # Fixed parameters from problem statement\n    base_params = {'beta0': 0.0, 'beta1': 1.0, 'beta2': 1.0}\n    n_values = [8, 32, 128]\n    R = 200\n    M = 512\n\n    results = []\n    for beta_12, sigma_sq in test_cases:\n        params = base_params.copy()\n        params['beta_12'] = beta_12\n        params['sigma_sq'] = sigma_sq\n        \n        crossover_n = simulate_learning_curves(params, n_values, R, M)\n        results.append(crossover_n)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_learning_curves(params, n_values, R, M):\n    \"\"\"\n    Simulates learning curves for one set of parameters.\n\n    Args:\n        params (dict): Dictionary of parameters (beta0, beta1, beta2, beta_12, sigma_sq).\n        n_values (list): List of training sample sizes to test.\n        R (int): Number of training replicates.\n        M (int): Size of the fixed test set.\n\n    Returns:\n        int: The smallest n at which the interaction model's error is less than\n             or equal to the additive model's error, or 0 if no crossover occurs.\n    \"\"\"\n    # Use a fixed seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    beta_star = np.array([params['beta0'], params['beta1'], params['beta2'], params['beta_12']])\n    sigma_sq = params['sigma_sq']\n    sigma = np.sqrt(sigma_sq)\n\n    # 1. Generate a fixed test set\n    x_test_raw = np.random.uniform(-1, 1, size=(M, 2))\n    x1_test, x2_test = x_test_raw[:, 0], x_test_raw[:, 1]\n    \n    # Design matrices for the test set\n    X_test_add = np.c_[np.ones(M), x1_test, x2_test]\n    X_test_int = np.c_[X_test_add, x1_test * x2_test]\n    \n    # True function values on the test set\n    f_star_test = X_test_int @ beta_star\n\n    errors = {n: {} for n in n_values}\n\n    # 2. Iterate through training sizes\n    for n in n_values:\n        # Storage for predictions from all R replicates for the current n\n        preds_add = np.zeros((M, R))\n        preds_int = np.zeros((M, R))\n\n        # 3. Loop over R replicates\n        for r in range(R):\n            # a. Generate training data\n            x_train_raw = np.random.uniform(-1, 1, size=(n, 2))\n            x1_train, x2_train = x_train_raw[:, 0], x_train_raw[:, 1]\n            \n            X_train_add = np.c_[np.ones(n), x1_train, x2_train]\n            X_train_int = np.c_[X_train_add, x1_train * x2_train]\n\n            f_star_train = X_train_int @ beta_star\n            noise = np.random.normal(0, sigma, size=n)\n            y_train = f_star_train + noise\n            \n            # b. Fit models using linear least squares\n            theta_add, _, _, _ = np.linalg.lstsq(X_train_add, y_train, rcond=None)\n            theta_int, _, _, _ = np.linalg.lstsq(X_train_int, y_train, rcond=None)\n\n            # c. Make and store predictions on the test set\n            preds_add[:, r] = X_test_add @ theta_add\n            preds_int[:, r] = X_test_int @ theta_int\n\n        # 4. Calculate bias-squared, variance, and total generalization error\n        # For the additive model\n        E_f_hat_add = np.mean(preds_add, axis=1)\n        bias_sq_add = (E_f_hat_add - f_star_test)**2\n        var_add = np.var(preds_add, axis=1) # ddof=0 is default, correct for population var over replicates\n        total_err_add = np.mean(bias_sq_add) + np.mean(var_add) + sigma_sq\n        errors[n]['add'] = total_err_add\n        \n        # For the interaction model\n        E_f_hat_int = np.mean(preds_int, axis=1)\n        bias_sq_int = (E_f_hat_int - f_star_test)**2\n        var_int = np.var(preds_int, axis=1)\n        total_err_int = np.mean(bias_sq_int) + np.mean(var_int) + sigma_sq\n        errors[n]['int'] = total_err_int\n\n    # 5. Find the crossover point\n    for n in sorted(n_values):\n        if errors[n]['int'] = errors[n]['add']:\n            return n\n    \n    return 0\n\nsolve()\n```"
        },
        {
            "introduction": "The inductive biases created by feature scaling are not just intuitive; they can be characterized with mathematical rigor. For linear models, scaling the input data is equivalent to imposing a geometric constraint on the learned weight vector, effectively creating an ellipsoidal \"search space\" for the optimal parameters. This advanced practice  challenges you to derive this relationship analytically and use it to discover how different scaling strategies—such as whitening—implicitly prioritize certain directions in the feature space, thereby shaping the model's final behavior.",
            "id": "3129964",
            "problem": "Consider a statistical learning setup with a linear hypothesis space $\\mathcal{H} = \\{x \\mapsto w^{\\top} x : w \\in \\mathbb{R}^{d}\\}$ and an input random vector $x \\in \\mathbb{R}^{d}$ that is drawn from a zero-mean multivariate Gaussian distribution with covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. A feature scaling matrix $S \\in \\mathbb{R}^{d \\times d}$ is applied to inputs before learning, sending $x$ to $z = S x$, and the learning algorithm penalizes the squared Euclidean norm of the weight vector in the scaled space, so that feasible weights satisfy $\\|w_{z}\\|_{2} \\leq 1$ in the scaled coordinates. The corresponding weight in the original coordinates is $w = S^{\\top} w_{z}$.\n\nThe geometric margin on an input $x$ for a given $w$ is the scalar $m = w^{\\top} x$, and when $x$ is random with distribution $\\mathcal{N}(0, \\Sigma)$, the margin is a univariate Gaussian with variance $\\operatorname{Var}(m) = \\operatorname{Var}(w^{\\top} x)$. The inductive bias produced by feature scaling is reflected in which directions in $\\mathbb{R}^{d}$ are favored subject to the unit-norm constraint in the scaled space.\n\nYou will examine two concrete scalings in dimension $d = 2$:\n- Raw scaling $S_{\\mathrm{raw}} = I$.\n- Whitening scaling $S_{\\mathrm{white}} = \\Sigma^{-1/2}$.\n\nLet the covariance be\n$$\n\\Sigma = \\begin{pmatrix}\n4  \\frac{6}{5} \\\\\n\\frac{6}{5}  1\n\\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from the definitions above, express the feasible set for $w$ in original coordinates under a general scaling $S$ as a quadratic constraint of the form $w^{\\top} M w \\leq 1$, and identify $M$ in terms of $S$.\n2. Using only fundamental facts about variance of linear forms and constrained optimization, derive the expression for the maximum possible margin variance $\\max\\{\\operatorname{Var}(w^{\\top} x) : w^{\\top} M w = 1\\}$ in terms of $\\Sigma$ and $M$, and explain why this maximization characterizes how scaling implicitly weights directions.\n3. Specialize your result to $S_{\\mathrm{raw}}$ and $S_{\\mathrm{white}}$ for the given $\\Sigma$ and compute the ratio of the maximal achievable margin variances under raw scaling to whitening. Provide the final ratio as a single exact, closed-form analytic expression. Do not round and do not include any units in your final answer.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It is a standard problem in statistical learning theory requiring an application of linear algebra and constrained optimization. We proceed with the solution.\n\nThe problem asks for a three-part solution. We will address each part in sequence.\n\n### Part 1: Feasible Set for $w$ in Original Coordinates\n\nWe are given the constraint on the weight vector $w_z$ in the scaled space:\n$$\n\\|w_z\\|_2 \\leq 1\n$$\nThis is equivalent to the quadratic constraint:\n$$\nw_z^{\\top} w_z \\leq 1\n$$\nThe relationship between the weight vector $w$ in the original space and $w_z$ in the scaled space is given by:\n$$\nw = S^{\\top} w_z\n$$\nwhere $S$ is the feature scaling matrix. To express the constraint in terms of $w$, we must first express $w_z$ in terms of $w$. Assuming the scaling matrix $S$ is invertible (which is a necessary condition for a one-to-one change of coordinates), we can take the inverse of its transpose:\n$$\nw_z = (S^{\\top})^{-1} w = S^{-T} w\n$$\nSubstituting this expression for $w_z$ into the quadratic constraint yields:\n$$\n(S^{-T} w)^{\\top} (S^{-T} w) \\leq 1\n$$\nUsing the property of transposes $(AB)^{\\top} = B^{\\top}A^{\\top}$, we get:\n$$\nw^{\\top} (S^{-T})^{\\top} (S^{-T}) w \\leq 1\n$$\nSince the transpose of an inverse is the inverse of the transpose, $(A^{-T})^{\\top} = (A^{\\top})^{-T} = (A^{-1})^{T \\top} = A^{-1}$. Thus, $(S^{-T})^{\\top} = S^{-1}$. The inequality becomes:\n$$\nw^{\\top} S^{-1} S^{-T} w \\leq 1\n$$\nThis expression can be rewritten as:\n$$\nw^{\\top} (S^{\\top} S)^{-1} w \\leq 1\n$$\nThis is a quadratic constraint of the form $w^{\\top} M w \\leq 1$, where the matrix $M$ is defined as:\n$$\nM = (S^{\\top} S)^{-1}\n$$\nThe matrix $M$ is symmetric and positive definite, as it is the inverse of a Gram matrix $S^{\\top}S$ (for invertible $S$). The feasible set for $w$ is an ellipsoid defined by this inequality.\n\n### Part 2: Maximum Margin Variance\n\nThe margin is given by $m = w^{\\top}x$. The input vector $x$ is a random variable drawn from a zero-mean multivariate Gaussian distribution, $x \\sim \\mathcal{N}(0, \\Sigma)$. The variance of the margin $m$ is:\n$$\n\\operatorname{Var}(m) = \\operatorname{Var}(w^{\\top}x) = E[(w^{\\top}x - E[w^{\\top}x])^2]\n$$\nSince $E[x]=0$, we have $E[w^{\\top}x] = w^{\\top}E[x] = 0$. The variance simplifies to:\n$$\n\\operatorname{Var}(w^{\\top}x) = E[(w^{\\top}x)^2] = E[w^{\\top}x x^{\\top}w] = w^{\\top}E[x x^{\\top}]w\n$$\nBy the definition of the covariance matrix for a zero-mean random vector, $\\Sigma = E[xx^{\\top}]$. Therefore, the margin variance is:\n$$\n\\operatorname{Var}(w^{\\top}x) = w^{\\top}\\Sigma w\n$$\nWe want to find the maximum possible margin variance subject to the constraint on $w$. The maximum will occur on the boundary of the feasible set, i.e., where $w^{\\top} M w = 1$. The optimization problem is:\n$$\n\\max_{w \\in \\mathbb{R}^d} w^{\\top}\\Sigma w \\quad \\text{subject to} \\quad w^{\\top} M w = 1\n$$\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(w, \\lambda) = w^{\\top}\\Sigma w - \\lambda(w^{\\top} M w - 1)\n$$\nTo find the extrema, we set the gradient with respect to $w$ to zero. Noting that $\\Sigma$ and $M$ are symmetric matrices, we use the identity $\\nabla_v (v^{\\top} A v) = 2 A v$:\n$$\n\\nabla_w \\mathcal{L} = 2\\Sigma w - 2\\lambda M w = 0\n$$\n$$\n\\Sigma w = \\lambda M w\n$$\nThis is a generalized eigenvalue problem. To find the value of the objective function at an extremum, we left-multiply by $w^{\\top}$:\n$$\nw^{\\top}\\Sigma w = \\lambda w^{\\top} M w\n$$\nUsing the constraint $w^{\\top} M w = 1$, we find that the objective function value is equal to the generalized eigenvalue $\\lambda$:\n$$\nw^{\\top}\\Sigma w = \\lambda\n$$\nTo maximize the variance, we must choose the largest generalized eigenvalue, $\\lambda_{\\max}$, of the pair $(\\Sigma, M)$.\n$$\n\\max\\{\\operatorname{Var}(w^{\\top} x) : w^{\\top} M w = 1\\} = \\lambda_{\\max}(\\Sigma, M)\n$$\nThis maximization characterizes the inductive bias because the matrix $M$, determined by the scaling $S$, shapes the search space for the weight vector $w$. The algorithm implicitly favors directions $w$ (the generalized eigenvectors) where the data variance ($w^{\\top}\\Sigma w$) is large relative to the penalty ($w^{\\top} M w$). The maximum achievable variance corresponds to the direction most amplified by this trade-off.\n\n### Part 3: Ratio of Maximal Variances\n\nWe are given the covariance matrix:\n$$\n\\Sigma = \\begin{pmatrix} 4  \\frac{6}{5} \\\\ \\frac{6}{5}  1 \\end{pmatrix}\n$$\n**Case 1: Raw scaling ($S_{\\mathrm{raw}} = I$)**\nFor $S = S_{\\mathrm{raw}} = I$, the identity matrix, the constraint matrix $M$ is:\n$$\nM_{\\mathrm{raw}} = (I^{\\top}I)^{-1} = I^{-1} = I\n$$\nThe generalized eigenvalue problem $\\Sigma w = \\lambda M_{\\mathrm{raw}} w$ becomes the standard eigenvalue problem $\\Sigma w = \\lambda w$. The maximum variance is the largest eigenvalue of $\\Sigma$, $\\lambda_{\\max}(\\Sigma)$. The eigenvalues are roots of the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} 4-\\lambda  \\frac{6}{5} \\\\ \\frac{6}{5}  1-\\lambda \\end{pmatrix} = (4-\\lambda)(1-\\lambda) - \\left(\\frac{6}{5}\\right)^2 = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + 4 - \\frac{36}{25} = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + \\frac{100 - 36}{25} = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + \\frac{64}{25} = 0\n$$\nUsing the quadratic formula:\n$$\n\\lambda = \\frac{5 \\pm \\sqrt{(-5)^2 - 4(1)(\\frac{64}{25})}}{2} = \\frac{5 \\pm \\sqrt{25 - \\frac{256}{25}}}{2} = \\frac{5 \\pm \\sqrt{\\frac{625-256}{25}}}{2}\n$$\n$$\n\\lambda = \\frac{5 \\pm \\sqrt{\\frac{369}{25}}}{2} = \\frac{5 \\pm \\frac{\\sqrt{369}}{5}}{2}\n$$\nSince $369 = 9 \\times 41$, $\\sqrt{369} = 3\\sqrt{41}$.\n$$\n\\lambda = \\frac{5 \\pm \\frac{3\\sqrt{41}}{5}}{2} = \\frac{25 \\pm 3\\sqrt{41}}{10}\n$$\nThe maximum eigenvalue is the larger root:\n$$\n\\operatorname{Var}_{\\mathrm{raw}}^* = \\lambda_{\\max}(\\Sigma) = \\frac{25 + 3\\sqrt{41}}{10}\n$$\n\n**Case 2: Whitening scaling ($S_{\\mathrm{white}} = \\Sigma^{-1/2}$)**\nFor $S = S_{\\mathrm{white}} = \\Sigma^{-1/2}$, where $\\Sigma^{-1/2}$ is the symmetric positive definite square root of $\\Sigma^{-1}$, the constraint matrix $M$ is:\n$$\nM_{\\mathrm{white}} = (S_{\\mathrm{white}}^{\\top} S_{\\mathrm{white}})^{-1} = ((\\Sigma^{-1/2})^{\\top}\\Sigma^{-1/2})^{-1}\n$$\nSince $\\Sigma^{-1/2}$ is symmetric, $(\\Sigma^{-1/2})^{\\top} = \\Sigma^{-1/2}$.\n$$\nM_{\\mathrm{white}} = (\\Sigma^{-1/2}\\Sigma^{-1/2})^{-1} = (\\Sigma^{-1})^{-1} = \\Sigma\n$$\nThe generalized eigenvalue problem is $\\Sigma w = \\lambda M_{\\mathrm{white}} w$, which becomes:\n$$\n\\Sigma w = \\lambda \\Sigma w\n$$\nSince $\\Sigma$ is positive definite, it is invertible. We can left-multiply by $\\Sigma^{-1}$:\n$$\nw = \\lambda w\n$$\nThis must hold for a non-zero eigenvector $w$, which implies that $\\lambda = 1$. The only generalized eigenvalue is $1$. Therefore, the maximum variance is:\n$$\n\\operatorname{Var}_{\\mathrm{white}}^* = 1\n$$\nThis result is intuitive: the optimization problem is $\\max w^{\\top}\\Sigma w$ subject to $w^{\\top}\\Sigma w=1$, which trivially has a maximum value of $1$.\n\n**Ratio Calculation**\nThe ratio of the maximal achievable margin variances is:\n$$\n\\frac{\\operatorname{Var}_{\\mathrm{raw}}^*}{\\operatorname{Var}_{\\mathrm{white}}^*} = \\frac{\\frac{25 + 3\\sqrt{41}}{10}}{1} = \\frac{25 + 3\\sqrt{41}}{10}\n$$",
            "answer": "$$\\boxed{\\frac{25 + 3\\sqrt{41}}{10}}$$"
        }
    ]
}