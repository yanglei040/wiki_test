{
    "hands_on_practices": [
        {
            "introduction": "Building a statistical model from the ground up is an essential skill for any practitioner. This first practice session guides you through the complete implementation of an adaptive LASSO solver . You will derive the coordinate descent update rule from first principles and construct a nested algorithm to handle the adaptive weighting scheme, solidifying your understanding of the interplay between the loss function, the weighted penalty, and the optimization procedure.",
            "id": "3111876",
            "problem": "You are asked to implement an adaptive least absolute shrinkage and selection operator (adaptive LASSO) solver using a nested algorithmic structure: an outer loop that updates coordinate-wise weights based on the current coefficient estimate, and an inner loop that runs cyclic coordinate descent to convergence for the corresponding weighted LASSO subproblem. Your program must implement the method, apply it to specified synthetic linear regression test cases, and output quantitative metrics characterizing variable selection and coefficient accuracy.\n\nStart from the core definitions of empirical risk minimization in linear regression with squared loss and the concept of regularization:\n\n1. The data consist of a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^{n}$. The linear model posits $y \\approx X \\beta$, where $\\beta \\in \\mathbb{R}^{p}$ is an unknown parameter vector to be estimated.\n\n2. The loss to be minimized is the empirical squared error, defined by\n$$\nL(\\beta) \\equiv \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2.\n$$\n\n3. The weighted least absolute shrinkage and selection operator (weighted LASSO) estimator with fixed nonnegative weights $w_j$ uses the penalized objective\n$$\nQ(\\beta; \\lambda, w) \\equiv \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_j \\right|,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter.\n\n4. The adaptive LASSO updates the weights using the current estimate $\\hat{\\beta}$ via\n$$\nw_j = \\frac{1}{\\left( \\left| \\hat{\\beta}_j \\right| + \\varepsilon \\right)^\\gamma},\n$$\nwith $\\gamma \\ge 0$ and a small $\\varepsilon > 0$ to ensure numerical stability if $\\hat{\\beta}_j = 0$.\n\nFrom these foundational definitions and the subgradient optimality conditions for the weighted $\\ell_1$ penalty, derive the coordinate-wise update rule for the inner loop coordinate descent solver that minimizes $Q(\\beta; \\lambda, w)$ with respect to a single coordinate $\\beta_j$ at a time, holding all other coordinates fixed. Your derivation must start from the above definitions and proceed through the stationarity (subgradient) condition for convex optimization with nonsmooth regularization. Do not assume any pre-known update formulas.\n\nImplementation requirements:\n\n- Standardize each feature column of $X$ by centering to zero mean and scaling to have squared Euclidean norm equal to $n$, and center $y$ to zero mean. Treat the intercept separately and do not penalize it.\n\n- Implement the inner cyclic coordinate descent that repeatedly updates each coordinate $\\beta_j$ until convergence (measured by the maximum absolute change in coordinates below a given tolerance). Use the derived coordinate-wise minimizer implied by the subgradient optimality condition.\n\n- Implement the outer adaptive loop that, starting from an initial estimate $\\hat{\\beta}^{(0)}$, updates the weights $w_j$ according to the adaptive rule with exponent $\\gamma$ and solves the weighted LASSO subproblem by inner coordinate descent to obtain $\\hat{\\beta}^{(1)}$, and so on, for a fixed number of outer iterations. Use $\\varepsilon$ strictly greater than zero in the weight formula to avoid division by zero.\n\n- For the initial estimate $\\hat{\\beta}^{(0)}$, use the ordinary least squares solution on the standardized predictors (with centering) computed via a numerically stable linear least squares routine. If the problem is underdetermined, use the minimum-norm least squares solution returned by the routine.\n\n- After estimating the coefficients on standardized data, map the coefficients back to the original feature scale to evaluate accuracy and support recovery. Define the estimated intercept explicitly from the centered model and the original column means.\n\n- For variable selection assessment, declare a coefficient to be selected (nonzero) if its absolute value in the original scale exceeds the threshold $10^{-3}$.\n\nTest suite:\n\nImplement your solver and run it on the following four synthetic test cases. In each case, generate $X$ with correlated Gaussian columns using a common-factor construction, generate a sparse ground-truth $\\beta^\\star$ with the first $k$ entries nonzero, and then generate $y = X \\beta^\\star + \\varepsilon$ with independent Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. For each test case, use the given random seed to ensure reproducibility. If a list of amplitudes is provided, use it for the first $k$ nonzero entries of $\\beta^\\star$ in order, otherwise use the default amplitudes $[2.0, -1.8, 1.5, -1.2, 1.0, -0.8]$ truncated to length $k$.\n\n- Test case 1 (happy path, moderate correlation):\n  - $(n, p, k) = (\\,80,\\, 30,\\, 5\\,)$,\n  - $\\rho = 0.3$,\n  - $\\sigma = 0.4$,\n  - $\\lambda = 0.12$,\n  - $\\gamma = 1.0$,\n  - outer iterations $= 3$,\n  - random seed $= 42$,\n  - amplitudes: default.\n\n- Test case 2 (boundary condition, $\\gamma = 0$ reduces to ordinary LASSO on standardized scale):\n  - Same data as test case 1 (use identical $X$, $y$ by the same seed),\n  - $\\lambda = 0.12$,\n  - $\\gamma = 0.0$,\n  - outer iterations $= 3$,\n  - amplitudes: default.\n\n- Test case 3 (high correlation, $p > n$):\n  - $(n, p, k) = (\\,50,\\, 100,\\, 6\\,)$,\n  - $\\rho = 0.8$,\n  - $\\sigma = 0.5$,\n  - $\\lambda = 0.18$,\n  - $\\gamma = 1.5$,\n  - outer iterations $= 5$,\n  - random seed $= 123$,\n  - amplitudes: default.\n\n- Test case 4 (edge case with small nonzero amplitudes):\n  - $(n, p, k) = (\\,100,\\, 40,\\, 6\\,)$,\n  - $\\rho = 0.4$,\n  - $\\sigma = 0.6$,\n  - $\\lambda = 0.10$,\n  - $\\gamma = 2.0$,\n  - outer iterations $= 5$,\n  - random seed $= 777$,\n  - amplitudes: $[\\,0.25,\\,-0.22,\\,0.20,\\,-0.18,\\,0.15,\\,-0.12\\,]$.\n\nYour program must, for each test case, output the list $[\\,\\mathrm{TP}, \\mathrm{TN}, \\mathrm{FP}, \\mathrm{FN}, \\mathrm{err}\\,]$, where:\n- $\\mathrm{TP}$ is the number of true positives (indices where $\\beta^\\star_j \\ne 0$ and the estimated coefficient exceeds the threshold in absolute value),\n- $\\mathrm{TN}$ is the number of true negatives (indices where $\\beta^\\star_j = 0$ and the estimated coefficient is below or equal to the threshold in absolute value),\n- $\\mathrm{FP}$ is the number of false positives,\n- $\\mathrm{FN}$ is the number of false negatives,\n- $\\mathrm{err}$ is the Euclidean norm $\\left\\| \\hat{\\beta} - \\beta^\\star \\right\\|_2$ in the original feature scale, rounded to $4$ decimal places.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list of the four per-case result lists, enclosed in square brackets. For example, a syntactically valid output looks like\n$$\n\\left[ [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,], [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,], [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,], [\\,\\cdot, \\cdot, \\cdot, \\cdot, \\cdot\\,] \\right]\n$$\nwith the numerical entries in place of the dots. There are no physical units in this problem. Angles are not involved. All proportions must be reported as decimals rather than percentages if they appear, but in this problem you must report integers and a float as specified above.",
            "solution": "The user wants to implement an adaptive LASSO solver using a nested coordinate descent algorithm. The first step is to validate the problem statement.\n\n### Step 1: Extract Givens\n- **Model**: Linear model $y \\approx X \\beta$, with $X \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^{n}$, $\\beta \\in \\mathbb{R}^{p}$.\n- **Loss Function**: Empirical squared error $L(\\beta) = \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2$.\n- **Weighted LASSO Objective**: $Q(\\beta; \\lambda, w) = \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_j \\right|$, with regularization parameter $\\lambda \\ge 0$ and weights $w_j \\ge 0$.\n- **Adaptive Weights**: The weights are updated via $w_j = \\frac{1}{\\left( \\left| \\hat{\\beta}_j \\right| + \\varepsilon \\right)^\\gamma}$, where $\\hat{\\beta}$ is the current coefficient estimate, $\\gamma \\ge 0$, and $\\varepsilon > 0$.\n- **Data Standardization**:\n    - Center columns of $X$ to have zero mean.\n    - Scale centered columns of $X$ to have a squared Euclidean norm of $n$.\n    - Center the response vector $y$ to have zero mean.\n- **Algorithm Structure**:\n    - **Outer Loop**: Iterates a fixed number of times. In each iteration, it updates the weights $w_j$ based on the current estimate $\\hat{\\beta}$ and then solves the resulting weighted LASSO subproblem.\n    - **Inner Loop**: A cyclic coordinate descent algorithm to solve the weighted LASSO subproblem for a fixed set of weights. It converges when the maximum absolute change in any coordinate is below a tolerance.\n- **Initialization**: The initial coefficient estimate $\\hat{\\beta}^{(0)}$ is the ordinary least squares (OLS) solution on standardized data. For underdetermined systems ($p > n$), the minimum-norm least squares solution is used.\n- **Coefficient Un-standardization**: After finding the final estimate $\\hat{\\beta}_s$ on standardized data, map it back to the original feature scale to get $\\hat{\\beta}$.\n- **Variable Selection Criterion**: A coefficient $\\hat{\\beta}_j$ is considered nonzero if $|\\hat{\\beta}_j| > 10^{-3}$.\n- **Test Cases**: Four synthetic test cases are defined with parameters $(n, p, k, \\rho, \\sigma, \\lambda, \\gamma, \\text{outer iterations, random seed, amplitudes})$, where $k$ is the number of nonzero true coefficients and $\\rho$ is the correlation between features.\n- **Data Generation**: Features $X$ are generated using a common-factor model $X_{ij} = \\sqrt{\\rho} f_i + \\sqrt{1-\\rho} Z_{ij}$, where $f_i, Z_{ij}$ are i.i.d. standard normal. The response is $y = X \\beta^\\star + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n- **Output Metrics**: For each test case, the output must be a list $[\\mathrm{TP}, \\mathrm{TN}, \\mathrm{FP}, \\mathrm{FN}, \\mathrm{err}]$, where TP, TN, FP, FN are counts for variable selection, and $\\mathrm{err} = \\| \\hat{\\beta} - \\beta^\\star \\|_2$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n\n- **Scientifically Grounded**: The problem is firmly rooted in the theory of statistical learning and convex optimization. The adaptive LASSO, coordinate descent, and the concepts of regularization and cross-validation are standard, well-established topics. The data generation model is a standard method for creating synthetic datasets with controlled correlation structures. The entire problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The objective function is convex, guaranteeing that a global minimum exists. With the $\\ell_1$ penalty, a unique solution is not always guaranteed, but the coordinate descent algorithm will converge to a minimum. The parameters for the algorithm and data generation are all explicitly provided, making the problem self-contained and admitting a deterministic solution path.\n- **Objective**: The problem is stated using precise mathematical definitions and objective language. There are no subjective or opinion-based statements. The evaluation criteria are quantitative and clearly defined.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  **Scientific/Factual Unsoundness**: No violations found.\n2.  **Non-Formalizable/Irrelevant**: The problem is highly formalizable and directly relevant to its stated topic.\n3.  **Incomplete/Contradictory Setup**: All necessary parameters and definitions are provided. There are no contradictions.\n4.  **Unrealistic/Infeasible**: The conditions and data are synthetic but well within the bounds of realistic scenarios for high-dimensional statistics.\n5.  **Ill-Posed/Poorly Structured**: The problem is well-structured and leads to a meaningful solution.\n6.  **Pseudo-Profound/Trivial**: The problem requires a non-trivial implementation of a modern statistical algorithm, including a derivation from first principles. It is not trivial or tautological.\n7.  **Outside Scientific Verifiability**: The results are fully verifiable by implementing the specified algorithm and running it on the specified data.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n---\n\n### Derivation of the Coordinate Descent Update Rule\n\nThe core of the inner loop is the coordinate-wise minimization of the weighted LASSO objective function:\n$$\nQ(\\beta; \\lambda, w) = \\frac{1}{2n} \\left\\| y - X \\beta \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_j \\right|\n$$\nThe algorithm operates on standardized data. Let $X_s$ be the standardized design matrix (columns centered and scaled to have squared norm $n$) and $y_c$ be the centered response vector. The objective for the coefficients $\\beta_s$ in this standardized space is:\n$$\nQ(\\beta_s) = \\frac{1}{2n} \\left\\| y_c - X_s \\beta_s \\right\\|_2^2 + \\lambda \\sum_{j=1}^p w_j \\left| \\beta_{s,j} \\right|\n$$\nCoordinate descent optimizes this function with respect to a single coefficient $\\beta_{s,j}$ at a time, holding all other coefficients $\\beta_{s,k}$ (for $k \\neq j$) fixed at their current values. Let's isolate the terms in $Q(\\beta_s)$ that depend on $\\beta_{s,j}$:\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2n} \\left\\| y_c - \\sum_{k \\neq j} X_{s,k} \\beta_{s,k} - X_{s,j} \\beta_{s,j} \\right\\|_2^2 + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\nLet $r_s^{(j)} = y_c - \\sum_{k \\neq j} X_{s,k} \\beta_{s,k}$ be the partial residual vector when the contribution of predictor $j$ is removed. The expression becomes:\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2n} \\left\\| r_s^{(j)} - X_{s,j} \\beta_{s,j} \\right\\|_2^2 + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\nExpanding the squared norm term:\n$$\n\\left\\| r_s^{(j)} - X_{s,j} \\beta_{s,j} \\right\\|_2^2 = (r_s^{(j)})^T r_s^{(j)} - 2 \\beta_{s,j} (X_{s,j})^T r_s^{(j)} + \\beta_{s,j}^2 (X_{s,j})^T X_{s,j}\n$$\nBy the specified standardization, $(X_{s,j})^T X_{s,j} = \\|X_{s,j}\\|_2^2 = n$. Substituting this, the objective simplifies to a 1D function of $\\beta_{s,j}$:\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2n} \\left( n \\beta_{s,j}^2 - 2 \\beta_{s,j} (X_{s,j})^T r_s^{(j)} \\right) + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\n$$\nQ(\\beta_{s,j}) = \\frac{1}{2} \\beta_{s,j}^2 - \\beta_{s,j} \\frac{(X_{s,j})^T r_s^{(j)}}{n} + \\lambda w_j |\\beta_{s,j}| + \\text{const}\n$$\nThis is a convex function. To find its minimum, we use the subgradient optimality condition, which states that $0$ must be in the subgradient of $Q$ at the minimum. The subgradient $\\partial_{\\beta_{s,j}} Q(\\beta_{s,j})$ is:\n$$\n\\partial_{\\beta_{s,j}} Q(\\beta_{s,j}) = \\beta_{s,j} - \\frac{(X_{s,j})^T r_s^{(j)}}{n} + \\lambda w_j \\partial|\\beta_{s,j}|\n$$\nwhere $\\partial|\\beta_{s,j}|$ is the subgradient of the absolute value function: $\\mathrm{sgn}(\\beta_{s,j})$ if $\\beta_{s,j} \\neq 0$, and the interval $[-1, 1]$ if $\\beta_{s,j} = 0$. Let $\\rho_j = \\frac{1}{n} (X_{s,j})^T r_s^{(j)}$. The optimality condition is $0 \\in \\beta_{s,j} - \\rho_j + \\lambda w_j \\partial|\\beta_{s,j}|$, or $\\rho_j - \\beta_{s,j} \\in \\lambda w_j \\partial|\\beta_{s,j}|$.\n\nWe analyze this condition in three cases:\n1.  If $\\beta_{s,j} > 0$, then $\\partial|\\beta_{s,j}| = \\{1\\}$. The condition is $\\rho_j - \\beta_{s,j} = \\lambda w_j$, which gives $\\beta_{s,j} = \\rho_j - \\lambda w_j$. This is consistent only if $\\rho_j - \\lambda w_j > 0$, i.e., $\\rho_j > \\lambda w_j$.\n2.  If $\\beta_{s,j} < 0$, then $\\partial|\\beta_{s,j}| = \\{-1\\}$. The condition is $\\rho_j - \\beta_{s,j} = -\\lambda w_j$, which gives $\\beta_{s,j} = \\rho_j + \\lambda w_j$. This is consistent only if $\\rho_j + \\lambda w_j < 0$, i.e., $\\rho_j < -\\lambda w_j$.\n3.  If $\\beta_{s,j} = 0$, then $\\partial|\\beta_{s,j}| = [-1, 1]$. The condition is $\\rho_j \\in [-\\lambda w_j, \\lambda w_j]$, or $|\\rho_j| \\le \\lambda w_j$.\n\nCombining these three cases yields the solution for $\\beta_{s,j}$ as a single soft-thresholding operation:\n$$\n\\hat{\\beta}_{s,j} = \\mathrm{sgn}(\\rho_j) \\max(0, |\\rho_j| - \\lambda w_j) =: S(\\rho_j, \\lambda w_j)\n$$\nwhere $\\rho_j = \\frac{1}{n} (X_{s,j})^T (y_c - \\sum_{k \\neq j} X_{s,k} \\beta_{s,k})$. This is the coordinate-wise update rule used in the inner loop of the algorithm.\n\n### Algorithmic Implementation\n\nThe overall procedure involves data generation, standardization, and a nested loop structure.\n\n1.  **Data Generation and Standardization**: For each test case, we generate data $(X, y, \\beta^\\star)$ as specified. We then compute the means $\\bar{X}_j$ and $\\bar{y}$ and standard deviations (related to the norm) $s_j = \\|X_j - \\bar{X}_j\\|_2$ for each column. The data is transformed to $(X_s, y_c)$ where $y_c = y - \\bar{y}$ and the columns of $X_s$ are $(X_j - \\bar{X}_j)\\sqrt{n}/s_j$.\n\n2.  **Initialization**: The initial coefficients for the adaptive scheme, $\\hat{\\beta}_s^{(0)}$, are obtained by solving the ordinary least squares problem $y_c = X_s \\beta_s$. We use a numerically stable routine like `numpy.linalg.lstsq`.\n\n3.  **Outer Loop**: This loop runs for a specified number of `outer_iterations`. In iteration $t = 1, 2, \\ldots$:\n    a.  **Weight Update**: The adaptive weights are computed based on the previous coefficient estimate $\\hat{\\beta}_s^{(t-1)}$: $w_j = (|\\hat{\\beta}_{s,j}^{(t-1)}| + \\varepsilon)^{-\\gamma}$.\n    b.  **Inner Loop (Weighted LASSO Solver)**: The coordinate descent algorithm is invoked to solve for $\\hat{\\beta}_s^{(t)}$ by minimizing $Q(\\beta_s; \\lambda, w)$. This loop proceeds as follows:\n        i.  Initialize with $\\beta_s = \\hat{\\beta}_s^{(t-1)}$.\n        ii. Repeatedly cycle through coordinates $j=1, \\ldots, p$, updating each $\\beta_{s,j}$ using the derived soft-thresholding rule. For efficiency, we maintain the full residual $r_s = y_c - X_s \\beta_s$ and update it after each coordinate step, which is an $O(n)$ operation.\n        iii. The loop terminates when the maximum absolute change over all coefficients in one full cycle falls below a small tolerance (e.g., $10^{-8}$).\n\n4.  **Un-standardization and Evaluation**: Once the final standardized coefficients $\\hat{\\beta}_s$ are obtained, they are transformed back to the original scale: $\\hat{\\beta}_j = \\hat{\\beta}_{s,j} \\sqrt{n} / s_j$. These are then compared to the true coefficients $\\beta^\\star$ to compute the required metrics: True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN), and the Euclidean error $\\|\\hat{\\beta} - \\beta^\\star\\|_2$.",
            "answer": "```python\nimport numpy as np\n\ndef generate_data(n, p, k, rho, sigma, seed, amplitudes):\n    \"\"\"\n    Generates synthetic data for a linear model with correlated features.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    f = rng.normal(size=(n, 1))\n    Z = rng.normal(size=(n, p))\n    X = np.sqrt(rho) * f + np.sqrt(1 - rho) * Z\n\n    beta_star = np.zeros(p)\n    beta_star[:k] = amplitudes[:k]\n\n    epsilon_noise = rng.normal(scale=sigma, size=n)\n    y = X @ beta_star + epsilon_noise\n\n    return X, y, beta_star\n\ndef soft_threshold(rho, C):\n    \"\"\"\n    Soft-thresholding operator.\n    \"\"\"\n    if rho > C:\n        return rho - C\n    elif rho  -C:\n        return rho + C\n    else:\n        return 0.0\n\ndef solve():\n    \"\"\"\n    Main function to run the adaptive LASSO solver on the test suite.\n    \"\"\"\n    default_amplitudes = [2.0, -1.8, 1.5, -1.2, 1.0, -0.8]\n    test_cases = [\n        {'n': 80, 'p': 30, 'k': 5, 'rho': 0.3, 'sigma': 0.4, 'lambda_': 0.12, 'gamma': 1.0, 'outer_iter': 3, 'seed': 42, 'amplitudes': default_amplitudes},\n        {'n': 80, 'p': 30, 'k': 5, 'rho': 0.3, 'sigma': 0.4, 'lambda_': 0.12, 'gamma': 0.0, 'outer_iter': 3, 'seed': 42, 'amplitudes': default_amplitudes},\n        {'n': 50, 'p': 100, 'k': 6, 'rho': 0.8, 'sigma': 0.5, 'lambda_': 0.18, 'gamma': 1.5, 'outer_iter': 5, 'seed': 123, 'amplitudes': default_amplitudes},\n        {'n': 100, 'p': 40, 'k': 6, 'rho': 0.4, 'sigma': 0.6, 'lambda_': 0.10, 'gamma': 2.0, 'outer_iter': 5, 'seed': 777, 'amplitudes': [0.25, -0.22, 0.20, -0.18, 0.15, -0.12]},\n    ]\n    \n    results = []\n    \n    # Algorithm parameters\n    epsilon = 1e-8\n    inner_tol = 1e-8\n    max_inner_iter = 1000\n    selection_threshold = 1e-3\n\n    for case in test_cases:\n        n, p = case['n'], case['p']\n        X, y, beta_star = generate_data(n, p, case['k'], case['rho'], case['sigma'], case['seed'], case['amplitudes'])\n\n        # 1. Standardize data\n        y_mean = y.mean()\n        X_mean = X.mean(axis=0)\n        y_c = y - y_mean\n        X_c = X - X_mean\n\n        X_c_norms = np.linalg.norm(X_c, axis=0)\n        non_const_cols = X_c_norms > 1e-12\n        \n        X_s = np.zeros_like(X_c)\n        # Only scale non-constant columns\n        scales = np.ones(p)\n        scales[non_const_cols] = X_c_norms[non_const_cols] / np.sqrt(n)\n        X_s[:, non_const_cols] = X_c[:, non_const_cols] / scales[non_const_cols, np.newaxis].T\n\n\n        # 2. Initial OLS estimate\n        beta_s, _, _, _ = np.linalg.lstsq(X_s, y_c, rcond=None)\n\n        # 3. Adaptive LASSO outer loop\n        for _ in range(case['outer_iter']):\n            # a. Update weights\n            weights = (np.abs(beta_s) + epsilon)**(-case['gamma'])\n\n            # b. Inner coordinate descent loop\n            # Initialize with solution from previous step\n            beta_s_cd = np.copy(beta_s)\n            \n            for _ in range(max_inner_iter):\n                max_change = 0.0\n                for j in range(p):\n                    if not non_const_cols[j]: continue\n                    \n                    beta_sj_old = beta_s_cd[j]\n                    \n                    # Calculate rho_j efficiently\n                    # rho_j = (X_s[:, j].T @ (y_c - np.dot(X_s, beta_s_cd) + X_s[:, j] * beta_sj_old)) / n\n                    dot_product = np.dot(X_s[:, j], y_c - np.dot(X_s, beta_s_cd) + X_s[:, j] * beta_sj_old)\n                    rho_j = dot_product / n\n                    \n                    # Update beta_sj\n                    beta_s_cd[j] = soft_threshold(rho_j, case['lambda_'] * weights[j])\n                    \n                    change = abs(beta_s_cd[j] - beta_sj_old)\n                    if change > max_change:\n                        max_change = change\n                \n                if max_change  inner_tol:\n                    break\n            beta_s = beta_s_cd\n        \n        # 4. Un-standardize coefficients\n        beta_final = np.zeros(p)\n        beta_final[non_const_cols] = beta_s[non_const_cols] / scales[non_const_cols]\n\n        # 5. Evaluate metrics\n        support_star = np.abs(beta_star) > 0\n        support_hat = np.abs(beta_final) > selection_threshold\n        \n        TP = np.sum(support_hat  support_star)\n        TN = np.sum(~support_hat  ~support_star)\n        FP = np.sum(support_hat  ~support_star)\n        FN = np.sum(~support_hat  support_star)\n        \n        err = np.linalg.norm(beta_final - beta_star)\n        \n        results.append([TP, TN, FP, FN, round(err, 4)])\n\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Now that you have implemented the adaptive LASSO, let's explore the theoretical underpinnings of its advantages over the standard LASSO. This exercise  presents a simplified yet insightful scenario with highly correlated predictors, allowing you to use the Karush-Kuhn-Tucker (KKT) optimality conditions as a lens. By calculating how the adaptive weights change the conditions for variable selection, you will uncover the precise mathematical mechanism that enables the adaptive LASSO to more effectively distinguish between true signals and spurious correlations.",
            "id": "3095616",
            "problem": "Consider a linear regression model with $n=2$ observations and $p=2$ predictors, where the design matrix $X \\in \\mathbb{R}^{2 \\times 2}$ has standardized columns such that the sample Gram matrix satisfies\n$$\n\\frac{1}{n} X^{\\top} X = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix},\n$$\nwith correlation level $\\rho = 0.95$. One explicit construction is\n$$\nX_1 = \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix}, \\quad X_2 = \\begin{pmatrix} \\sqrt{2} \\rho \\\\ \\sqrt{2} \\sqrt{1-\\rho^2} \\end{pmatrix},\n$$\nso that $\\frac{1}{n} X_1^{\\top} X_1 = 1$, $\\frac{1}{n} X_2^{\\top} X_2 = 1$, and $\\frac{1}{n} X_1^{\\top} X_2 = \\rho$. Let the response vector be $y = X \\beta^{\\star}$ with the true coefficient vector\n$$\n\\beta^{\\star} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}.\n$$\nDefine the standard $\\ell_1$ lasso estimator as the optimizer of the convex problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^2} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 |\\beta_j| \\right\\},\n$$\nand define the adaptive lasso estimator as\n$$\n\\min_{\\beta \\in \\mathbb{R}^2} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 w_j |\\beta_j| \\right\\},\n$$\nwhere the weights are $w_j = \\frac{1}{|\\tilde{\\beta}_j|^{\\gamma}}$ for a fixed $\\gamma = 1$, and $\\tilde{\\beta}$ is the ordinary least squares estimator computed on $(X,y)$.\n\nStarting from the definition of convex optimality via Karush–Kuhn–Tucker (KKT) conditions, derive the conditions under which the optimal solution has feature $1$ active with positive coefficient and feature $2$ excluded (i.e., $\\beta_1  0$ and $\\beta_2 = 0$) for both the standard lasso and the adaptive lasso. Use those conditions to compute the minimal regularization parameter values $\\lambda_{\\mathrm{L}}$ (standard lasso) and $\\lambda_{\\mathrm{AL}}$ (adaptive lasso) that guarantee feature $2$ is excluded while feature $1$ remains active. Finally, compute the ratio\n$$\nR = \\frac{\\lambda_{\\mathrm{AL}}}{\\lambda_{\\mathrm{L}}}.\n$$\nRound your final ratio $R$ to four significant figures.",
            "solution": "The user wants to solve a statistical learning problem by comparing the standard and adaptive lasso methods.\n\n### Step 1: Extract Givens\n-   Number of observations, $n=2$.\n-   Number of predictors, $p=2$.\n-   Design matrix $X \\in \\mathbb{R}^{2 \\times 2}$ with standardized columns.\n-   Sample Gram matrix: $\\frac{1}{n} X^{\\top} X = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$.\n-   Correlation: $\\rho = 0.95$.\n-   Response vector: $y = X \\beta^{\\star}$.\n-   True coefficient vector: $\\beta^{\\star} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}$.\n-   Standard lasso objective function: $L_{\\mathrm{L}}(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 |\\beta_j|$.\n-   Adaptive lasso objective function: $L_{\\mathrm{AL}}(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\sum_{j=1}^2 w_j |\\beta_j|$.\n-   Adaptive lasso weights: $w_j = \\frac{1}{|\\tilde{\\beta}_j|^{\\gamma}}$ with $\\gamma = 1$, where $\\tilde{\\beta}$ is the ordinary least squares (OLS) estimator.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in convex optimization applied to statistical learning.\n-   **Scientifically Grounded:** The problem uses standard definitions of lasso and adaptive lasso, which are fundamental techniques in high-dimensional statistics. The setup is based on a standard linear model.\n-   **Well-Posed:** All necessary data and definitions are provided to derive a unique solution for the requested quantities. The problem is self-contained.\n-   **Objective:** The problem statement is composed of precise, mathematical definitions and asks for a quantitative result. There is no ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe Karush-Kuhn-Tucker (KKT) conditions provide the necessary and sufficient conditions for optimality for a convex problem. For a general objective function of the form $F(\\beta) = f(\\beta) + g(\\beta)$, where $f(\\beta)$ is differentiable and $g(\\beta)$ is convex, the optimality condition for a solution $\\hat{\\beta}$ is $0 \\in \\nabla f(\\hat{\\beta}) + \\partial g(\\hat{\\beta})$, where $\\partial g(\\hat{\\beta})$ is the subgradient of $g$ at $\\hat{\\beta}$.\n\nIn our setting, the differentiable part is the loss function $f(\\beta) = \\frac{1}{2n} \\|y - X\\beta\\|_2^2$, whose gradient is $\\nabla f(\\beta) = -\\frac{1}{n}X^{\\top}(y - X\\beta)$. The non-differentiable part is the penalty term.\n\nWe need to analyze the case where the solution vector $\\hat{\\beta}$ has the form $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$ with $\\hat{\\beta}_1  0$.\n\nFirst, we compute the term $\\frac{1}{n}X^{\\top}y$. Since the model is noiseless, $y = X\\beta^{\\star}$, we have:\n$$\n\\frac{1}{n} X^{\\top}y = \\frac{1}{n} X^{\\top}X\\beta^{\\star} = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} \\begin{pmatrix} \\beta^{\\star}_1 \\\\ \\beta^{\\star}_2 \\end{pmatrix}\n$$\nSubstituting $\\beta^{\\star}_1=1$, $\\beta^{\\star}_2=0.2$, and $\\rho=0.95$:\n$$\n\\frac{1}{n} X^{\\top}y = \\begin{pmatrix} 1  0.95 \\\\ 0.95  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0.95 \\cdot 0.2 \\\\ 0.95 \\cdot 1 + 1 \\cdot 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 + 0.19 \\\\ 0.95 + 0.2 \\end{pmatrix} = \\begin{pmatrix} 1.19 \\\\ 1.15 \\end{pmatrix}\n$$\nSo, the correlations of the predictors with the response are $\\frac{1}{n}X_1^{\\top}y = 1.19$ and $\\frac{1}{n}X_2^{\\top}y = 1.15$.\n\n**Standard Lasso ($\\ell_1$ lasso)**\n\nThe penalty term is $g(\\beta) = \\lambda \\sum_{j=1}^2 |\\beta_j|$. For a solution $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$ with $\\hat{\\beta}_1  0$, the subgradient of the penalty is $(\\lambda \\cdot \\text{sign}(\\hat{\\beta}_1), \\lambda s_2)^{\\top} = (\\lambda, \\lambda s_2)^{\\top}$, where $s_2 \\in [-1, 1]$.\n\nThe KKT conditions are:\n1.  For feature $j=1$: $-\\frac{1}{n}X_1^{\\top}(y - X\\hat{\\beta}) + \\lambda = 0$.\n2.  For feature $j=2$: $|-\\frac{1}{n}X_2^{\\top}(y - X\\hat{\\beta})| \\le \\lambda$.\n\nLet's expand these conditions for $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$.\n1.  From the first condition:\n    $$\n    -\\frac{1}{n}X_1^{\\top}y + \\frac{1}{n}X_1^{\\top}X_1\\hat{\\beta}_1 + \\lambda = 0\n    $$\n    Using $\\frac{1}{n}X_1^{\\top}X_1 = 1$ and $\\frac{1}{n}X_1^{\\top}y = 1.19$, we get:\n    $$\n    -1.19 + \\hat{\\beta}_1 + \\lambda = 0 \\implies \\hat{\\beta}_1 = 1.19 - \\lambda\n    $$\n    The assumption $\\hat{\\beta}_1  0$ requires $1.19 - \\lambda  0$, which means $\\lambda  1.19$.\n\n2.  From the second condition:\n    $$\n    |-\\frac{1}{n}X_2^{\\top}y + \\frac{1}{n}X_2^{\\top}X_1\\hat{\\beta}_1| \\le \\lambda\n    $$\n    Using $\\frac{1}{n}X_2^{\\top}y = 1.15$, $\\frac{1}{n}X_2^{\\top}X_1 = \\rho = 0.95$, and the expression for $\\hat{\\beta}_1$:\n    $$\n    |-1.15 + 0.95(1.19 - \\lambda)| \\le \\lambda\n    $$\n    $$\n    |-1.15 + 1.1305 - 0.95\\lambda| \\le \\lambda\n    $$\n    $$\n    |-0.0195 - 0.95\\lambda| \\le \\lambda\n    $$\n    For $\\lambda  0$, the expression inside the absolute value is negative. So we can write:\n    $$\n    0.0195 + 0.95\\lambda \\le \\lambda\n    $$\n    $$\n    0.0195 \\le \\lambda - 0.95\\lambda = 0.05\\lambda\n    $$\n    $$\n    \\lambda \\ge \\frac{0.0195}{0.05} = 0.39\n    $$\n    The minimal value of the regularization parameter that ensures feature $2$ is excluded is $\\lambda_{\\mathrm{L}} = 0.39$. This value satisfies the condition $\\lambda  1.19$.\n\n**Adaptive Lasso**\n\nFirst, we must compute the OLS estimator $\\tilde{\\beta}$ to find the weights $w_j$.\n$$\n\\tilde{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y = \\left(\\frac{1}{n}X^{\\top}X\\right)^{-1}\\left(\\frac{1}{n}X^{\\top}y\\right)\n$$\nSince $y = X\\beta^{\\star}$, the OLS estimator in this noiseless setting is unbiased and exact:\n$$\n\\tilde{\\beta} = \\left(\\frac{1}{n}X^{\\top}X\\right)^{-1}\\left(\\frac{1}{n}X^{\\top}X\\beta^{\\star}\\right) = \\beta^{\\star} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\n$$\nThe weights are $w_j = 1/|\\tilde{\\beta}_j|^{\\gamma}$ with $\\gamma=1$:\n$$\nw_1 = \\frac{1}{|\\tilde{\\beta}_1|} = \\frac{1}{|1|} = 1\n$$\n$$\nw_2 = \\frac{1}{|\\tilde{\\beta}_2|} = \\frac{1}{|0.2|} = 5\n$$\nThe adaptive lasso is a weighted lasso problem with penalty $g(\\beta) = \\lambda \\sum_{j=1}^2 w_j |\\beta_j|$. For $\\hat{\\beta} = (\\hat{\\beta}_1, 0)^{\\top}$ with $\\hat{\\beta}_1  0$, the KKT conditions are:\n1.  For feature $j=1$: $-\\frac{1}{n}X_1^{\\top}(y - X\\hat{\\beta}) + \\lambda w_1 = 0$.\n2.  For feature $j=2$: $|-\\frac{1}{n}X_2^{\\top}(y - X\\hat{\\beta})| \\le \\lambda w_2$.\n\nLet's expand these conditions:\n1.  From the first condition, with $w_1 = 1$:\n    $$\n    -\\frac{1}{n}X_1^{\\top}y + \\frac{1}{n}X_1^{\\top}X_1\\hat{\\beta}_1 + \\lambda \\cdot 1 = 0\n    $$\n    $$\n    -1.19 + \\hat{\\beta}_1 + \\lambda = 0 \\implies \\hat{\\beta}_1 = 1.19 - \\lambda\n    $$\n    This is identical to the standard lasso case because $w_1=1$. The condition $\\hat{\\beta}_1  0$ implies $\\lambda  1.19$.\n\n2.  From the second condition, with $w_2 = 5$:\n    $$\n    |-\\frac{1}{n}X_2^{\\top}y + \\frac{1}{n}X_2^{\\top}X_1\\hat{\\beta}_1| \\le \\lambda w_2\n    $$\n    $$\n    |-1.15 + 0.95(1.19 - \\lambda)| \\le 5\\lambda\n    $$\n    $$\n    |-0.0195 - 0.95\\lambda| \\le 5\\lambda\n    $$\n    For $\\lambda  0$, this becomes:\n    $$\n    0.0195 + 0.95\\lambda \\le 5\\lambda\n    $$\n    $$\n    0.0195 \\le (5 - 0.95)\\lambda = 4.05\\lambda\n    $$\n    $$\n    \\lambda \\ge \\frac{0.0195}{4.05}\n    $$\n    The minimal value of the regularization parameter for adaptive lasso is $\\lambda_{\\mathrm{AL}} = \\frac{0.0195}{4.05}$.\n\n**Ratio Calculation**\n\nFinally, we compute the ratio $R = \\lambda_{\\mathrm{AL}} / \\lambda_{\\mathrm{L}}$:\n$$\nR = \\frac{\\lambda_{\\mathrm{AL}}}{\\lambda_{\\mathrm{L}}} = \\frac{0.0195 / 4.05}{0.39}\n$$\nWe can simplify this by noting that $\\lambda_{\\mathrm{L}} = 0.39 = \\frac{0.0195}{0.05}$:\n$$\nR = \\frac{0.0195 / 4.05}{0.0195 / 0.05} = \\frac{0.05}{4.05} = \\frac{5}{405} = \\frac{1}{81}\n$$\nAs a decimal, $R = \\frac{1}{81} \\approx 0.012345679...$. Rounding to four significant figures gives $0.01235$.",
            "answer": "$$\n\\boxed{0.01235}\n$$"
        },
        {
            "introduction": "The theoretical \"oracle\" properties of the adaptive LASSO are a powerful motivation for its use, but these are asymptotic guarantees that may not hold in finite samples. This final practice challenges you to construct a principled counterexample where the method fails to recover the true set of important variables, even with cross-validation used to select the tuning parameter . By exploring a scenario where high collinearity and a small sample size lead to an unstable pilot estimate for the weights, you will gain a critical perspective on the practical limitations of the adaptive LASSO and the importance of understanding the conditions under which theoretical properties apply.",
            "id": "3095617",
            "problem": "You are given the task of constructing a mathematically principled counterexample in the context of adaptive Least Absolute Shrinkage and Selection Operator (LASSO), where Cross-Validation (CV) for the regularization parameter selects a model that fails to recover the oracle support. The setup is the standard linear regression model with $n$ samples and $p$ predictors: $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^{p}$, and $\\varepsilon \\in \\mathbb{R}^{n}$ is a noise vector. Assume $\\varepsilon$ is centered and exhibits finite variance. The adaptive LASSO uses predictor-specific weights obtained from a pilot estimator (for example, ordinary least squares) to reweight the LASSO penalty. The oracle model is defined as the model that selects exactly the true support $S = \\{ j \\in \\{1,\\dots,p\\}: \\beta_j \\neq 0 \\}$.\n\nFundamental base: Use the empirical risk minimization principle with mean squared error as the loss and a weighted $\\ell_1$ penalty to formalize the adaptive LASSO. Cross-Validation (CV) is used to select the regularization parameter by approximating out-of-sample prediction error through data partitioning.\n\nYour program must implement the following, starting from first principles:\n\n- Construct synthetic data $X$ from a zero-mean multivariate normal distribution with a specified covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$, and construct $y$ with a specified true coefficient vector $\\beta$ and additive Gaussian noise $\\varepsilon$.\n- Compute pilot weights from a least squares pilot estimate on training partitions. To cause high variance in the weight estimation, optionally perturb the pilot estimate by adding centered Gaussian noise with a specified standard deviation; this simulates instability due to high collinearity and small sample size without violating the empirical risk minimization framework.\n- Fit the adaptive LASSO for a grid of regularization values using coordinate descent that minimizes the empirical squared error with a weighted $\\ell_1$ penalty. Standardize predictors so that each coordinate has unit empirical quadratic norm to ensure stable and interpretable coordinate updates.\n- Use $K$-fold Cross-Validation (CV) on the training procedure to select the regularization parameter. For each fold and each regularization value, compute the mean squared validation error, and select the parameter that minimizes the average validation error across folds.\n- After selecting the regularization parameter, refit the adaptive LASSO on the full dataset with weights recomputed from the full data pilot estimator under the same pilot noise setting, and return whether the selected support equals the oracle support $S$.\n\nTest suite: Implement and evaluate the following three cases. All random number generation must be seeded as specified for reproducibility. In each case, $p=6$ and the true coefficient vector is $\\beta = [1.5, 1.5, 0.3, 0, 0, 0]$, so the oracle support is $S = \\{0,1,2\\}$. The adaptive LASSO weight exponent is $\\gamma = 2.0$, and the number of CV folds is $K=5$.\n\n- Case $1$ (happy path): $n=400$, $\\Sigma = I_{6}$ (identity covariance), noise standard deviation $\\sigma_\\varepsilon = 0.2$, pilot noise standard deviation $\\sigma_{\\text{pilot}} = 0.0$, random seed $12345$.\n- Case $2$ (counterexample due to collinearity and pilot variance): $n=60$, $\\Sigma$ has entries $\\Sigma_{00} = \\Sigma_{11} = 1$, $\\Sigma_{01} = \\Sigma_{10} = 0.999$, and all other off-diagonals are $0$; $\\sigma_\\varepsilon = 0.4$, $\\sigma_{\\text{pilot}} = 1.2$, random seed $54321$.\n- Case $3$ (edge case with small $n$ and high pilot noise): $n=40$, $\\Sigma$ has entries $\\Sigma_{00} = \\Sigma_{11} = 1$, $\\Sigma_{01} = \\Sigma_{10} = 0.95$, and all other off-diagonals are $0$; $\\sigma_\\varepsilon = 0.8$, $\\sigma_{\\text{pilot}} = 1.5$, random seed $67890$.\n\nAlgorithmic details required:\n\n- Standardize $X$ by subtracting the training mean and scaling each column so that for each predictor $j$, the empirical quantity $(1/n)\\sum_{i=1}^{n} X_{ij}^2$ equals $1$. Center $y$ by subtracting the training mean; when evaluating validation mean squared error, add back the training mean to predictions.\n- Compute pilot coefficients on the training partitions using the Moore-Penrose pseudoinverse estimator to ensure well-defined behavior under near collinearity. Then perturb the pilot coefficients by adding independent centered Gaussian noise with standard deviation $\\sigma_{\\text{pilot}}$. Define the weight for coordinate $j$ as $w_j = 1/\\left(|\\hat{\\beta}^{\\text{pilot}}_j|^{\\gamma} + \\epsilon\\right)$ with $\\epsilon$ a small positive constant to avoid division by zero.\n- Use a geometric grid of $\\lambda$ values from a computed $\\lambda_{\\max}$ down to a fraction of $\\lambda_{\\max}$, where $\\lambda_{\\max}$ is a scale chosen large enough to set all coefficients to zero under the weighted penalty. Use $\\epsilon = 10^{-8}$ for numerical stability and choose a grid of $40$ values from $\\lambda_{\\max}$ down to $0.05 \\lambda_{\\max}$.\n- Implement coordinate descent with soft-thresholding updates to solve the weighted $\\ell_1$ penalized least squares on each training fold. Use warm starts along the $\\lambda$ grid to improve stability.\n\nOutput specification:\n\n- For each test case, compute a boolean indicating whether the final selected support equals the oracle support $S$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$), where each $resultk$ is a boolean. No physical units, angles, or percentages are involved.\n\nYour program must be complete and runnable, require no user input, use only the Python standard library together with the specified versions of Numerical Python (NumPy) and Scientific Python (SciPy), and adhere to the exact output format.",
            "solution": "The problem requires the construction of a counterexample to the oracle property of the adaptive LASSO. The oracle property, in this context, refers to the estimator's ability to correctly identify the set of true non-zero coefficients (the support) with probability tending to one as the sample size increases. This property, however, is asymptotic and can fail in finite samples, especially under challenging data conditions. This solution implements a simulation study to demonstrate such a failure.\n\nThe standard linear model is given by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the matrix of predictors, $\\beta \\in \\mathbb{R}^p$ is the coefficient vector, and $\\varepsilon \\in \\mathbb{R}^n$ is a vector of i.i.d. noise terms with mean $0$ and variance $\\sigma_\\varepsilon^2$. The true support is the set of indices of non-zero coefficients, $S = \\{j \\in \\{1, \\dots, p\\} : \\beta_j \\neq 0\\}$.\n\nThe adaptive LASSO estimator, $\\hat{\\beta}_{\\text{AL}}$, is the solution to the following optimization problem:\n$$\n\\hat{\\beta}_{\\text{AL}} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left( \\frac{1}{2} \\| y - X\\beta \\|_2^2 + \\lambda \\sum_{j=1}^{p} w_j |\\beta_j| \\right)\n$$\nThe key feature is the use of predictor-specific weights, $w_j$, which are typically data-driven. A standard choice for the weights is $w_j = 1 / |\\hat{\\beta}^{\\text{pilot}}_j|^\\gamma$, where $\\hat{\\beta}^{\\text{pilot}}$ is a consistent pilot estimate of $\\beta$ (e.g., from ordinary least squares or ridge regression), $\\gamma  0$ is a tuning parameter, and a small constant $\\epsilon$ is often added to the denominator for stability. The intuition is to apply a smaller penalty to coefficients with large pilot estimates (likely true signals) and a larger penalty to those with small pilot estimates (likely noise).\n\nThe oracle property of the adaptive LASSO relies on the quality of the pilot estimator. If $\\hat{\\beta}^{\\text{pilot}}$ is $\\sqrt{n}$-consistent, the property holds. However, in finite samples with high collinearity among predictors or a small sample size $n$ relative to the number of predictors $p$, the pilot estimate can have very high variance. This instability can lead to erroneous weights: a true, small coefficient might receive a large penalty and be incorrectly shrunk to zero, while a true zero coefficient might receive a small penalty and be incorrectly included in the model. The problem cleverly formalizes this instability by optionally adding Gaussian noise with standard deviation $\\sigma_{\\text{pilot}}$ to the pilot estimates, providing a direct mechanism to study the impact of pilot estimator variance.\n\nThe implementation proceeds as follows:\n\n1.  **Data Generation and Preprocessing**: For each test case, synthetic data $(X, y)$ is generated. The predictor matrix $X$ is drawn from a multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$, and the response $y$ is computed as $X\\beta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2 I)$. Before fitting, the predictors are standardized. First, we center each column of $X$ by subtracting its mean. Then, each column $X_j$ is scaled such that its empirical quadratic norm is unity, i.e., $\\frac{1}{n}\\sum_{i=1}^n X_{ij}^2=1$. The response vector $y$ is also centered. This standardization ensures that the penalty is applied fairly across all predictors.\n\n2.  **Cross-Validation for $\\lambda$ Selection**: The regularization parameter $\\lambda$ is crucial and is selected using $K$-fold cross-validation ($K=5$) to minimize out-of-sample prediction error. The data is partitioned into $K$ folds. For each fold $k \\in \\{1, \\dots, K\\}$:\n    - The model is trained on the remaining $K-1$ folds. This involves computing fold-specific pilot estimates $\\hat{\\beta}^{\\text{pilot}}$ (using the Moore-Penrose pseudoinverse `pinv` for stability under collinearity), perturbing them as specified, and calculating the weights $w_j = 1/(|\\hat{\\beta}^{\\text{pilot}}_j|^\\gamma + \\epsilon)$.\n    - The adaptive LASSO model is solved for a grid of $40$ logarithmically spaced $\\lambda$ values. The grid ranges from $\\lambda_{\\max}$ down to $0.05 \\lambda_{\\max}$, where $\\lambda_{\\max} = \\max_j |X_j^T y_{\\text{centered}}|/w_j$ is the smallest value of $\\lambda$ that shrinks all coefficients to zero.\n    - The mean squared error (MSE) is computed on the held-out validation fold for each fitted model along the $\\lambda$ path.\n    The total MSE for each $\\lambda$ value is summed across all folds. The optimal $\\lambda_{\\text{CV}}$ is the one corresponding to the minimum total MSE.\n\n3.  **Optimization via Coordinate Descent**: The adaptive LASSO objective function is solved using an iterative coordinate descent algorithm. For each predictor $j$, the coefficient $\\beta_j$ is updated while holding all other coefficients $\\beta_{k \\neq j}$ fixed. The update rule for a standardized design where $\\|X_j\\|_2^2 = n_{\\text{train}}$ is:\n    $$\n    \\hat{\\beta}_j \\leftarrow \\frac{1}{n_{\\text{train}}} S\\left( X_j^T \\left(y - \\sum_{k \\neq j} X_k \\hat{\\beta}_k\\right), \\lambda w_j \\right)\n    $$\n    where $S(z, \\tau) = \\text{sign}(z)\\max(|z|-\\tau, 0)$ is the soft-thresholding function. The algorithm cycles through all predictors until the coefficient estimates converge. To accelerate convergence, warm starts are used, where the solution for the previous (larger) $\\lambda$ value serves as the initial guess for the current $\\lambda$.\n\n4.  **Final Model Evaluation**: After identifying $\\lambda_{\\text{CV}}$, a final adaptive LASSO model is fitted on the entire dataset using this $\\lambda$. The weights for this fit are re-computed using a pilot estimate derived from the full dataset, again with the specified perturbation. The support of the resulting coefficient vector, $\\text{supp}(\\hat{\\beta}_{\\text{final}}) = \\{j : |\\hat{\\beta}_{\\text{final},j}|  \\text{tol}\\}$, is compared to the known oracle support $S$. A boolean value indicates whether the two sets are identical.\n\nThe three test cases are designed to illustrate the performance under different conditions:\n- **Case 1**: A \"happy path\" with a large sample size ($n=400$), no collinearity ($\\Sigma=I_6$), and a stable pilot estimator ($\\sigma_{\\text{pilot}}=0.0$). The oracle property is expected to hold.\n- **Case 2**: A challenging scenario with a small sample size ($n=60$), extreme collinearity between the first two predictors ($\\rho=0.999$), and high variance in the pilot estimator ($\\sigma_{\\text{pilot}}=1.2$). These conditions are engineered to cause the adaptive LASSO to fail in recovering the true support.\n- **Case 3**: An even more extreme case with very small $n=40$, high data noise, and very high pilot estimate variance ($\\sigma_{\\text{pilot}}=1.5$). This further highlights the breakdown of the asymptotic theory in difficult finite-sample settings.\n\nThe implementation correctly demonstrates that while the adaptive LASSO is a powerful tool with desirable asymptotic properties, its finite-sample performance is not guaranteed, and cross-validation, while effective for prediction, may not lead to the selection of the oracle model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import pinv\n\ndef solve():\n    \"\"\"\n    Main function to run the adaptive LASSO simulations for the specified test cases.\n    \"\"\"\n    \n    # Common parameters for all test cases\n    p_common = 6\n    beta_true_common = np.array([1.5, 1.5, 0.3, 0.0, 0.0, 0.0])\n    gamma_common = 2.0\n    K_common = 5\n    \n    # Test case definitions\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"n\": 400, \"p\": p_common, \"beta_true\": beta_true_common,\n            \"Sigma\": np.identity(p_common), \"sigma_eps\": 0.2, \"sigma_pilot\": 0.0,\n            \"K\": K_common, \"gamma\": gamma_common, \"seed\": 12345\n        },\n        # Case 2 (counterexample due to collinearity and pilot variance)\n        {\n            \"n\": 60, \"p\": p_common, \"beta_true\": beta_true_common,\n            \"Sigma\": np.identity(p_common), \"sigma_eps\": 0.4, \"sigma_pilot\": 1.2,\n            \"K\": K_common, \"gamma\": gamma_common, \"seed\": 54321, \"rho\": 0.999\n        },\n        # Case 3 (edge case with small n and high pilot noise)\n        {\n            \"n\": 40, \"p\": p_common, \"beta_true\": beta_true_common,\n            \"Sigma\": np.identity(p_common), \"sigma_eps\": 0.8, \"sigma_pilot\": 1.5,\n            \"K\": K_common, \"gamma\": gamma_common, \"seed\": 67890, \"rho\": 0.95\n        }\n    ]\n    # Update Sigma for cases 2 and 3\n    test_cases[1][\"Sigma\"][0, 1] = test_cases[1][\"Sigma\"][1, 0] = test_cases[1].pop(\"rho\")\n    test_cases[2][\"Sigma\"][0, 1] = test_cases[2][\"Sigma\"][1, 0] = test_cases[2].pop(\"rho\")\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef soft_threshold(rho, lam):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(rho) * np.maximum(np.abs(rho) - lam, 0)\n\n\ndef adaptive_lasso_cd(X, y, weights, lambda_val, beta_init, max_iter=1000, tol=1e-7):\n    \"\"\"\n    Solves the adaptive LASSO problem using coordinate descent.\n    Objective: 1/2 * ||y - X*beta||^2 + lambda * sum(w_j * |beta_j|)\n    \"\"\"\n    n, p = X.shape\n    beta = beta_init.copy()\n    \n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        for j in range(p):\n            rho_j = X[:, j].T @ (y - (X @ beta - X[:, j] * beta[j]))\n            beta[j] = soft_threshold(rho_j, lambda_val * weights[j]) / n\n        \n        if np.linalg.norm(beta - beta_old, ord=np.inf)  tol:\n            break\n            \n    return beta\n\n\ndef run_simulation(n, p, beta_true, Sigma, sigma_eps, sigma_pilot, K, gamma, seed):\n    \"\"\"\n    Runs a single simulation for a given set of parameters.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # 1. Generate data\n    X = np.random.multivariate_normal(np.zeros(p), Sigma, size=n)\n    epsilon_noise = np.random.normal(0, sigma_eps, size=n)\n    y = X @ beta_true + epsilon_noise\n    \n    oracle_support = {j for j, v in enumerate(beta_true) if v != 0}\n    \n    # 2. K-fold Cross-Validation\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    folds = np.array_split(indices, K)\n    \n    # Determine a single lambda grid for all folds based on the full data for consistency\n    X_mean_full = X.mean(axis=0)\n    y_mean_full = y.mean()\n    X_full_centered = X - X_mean_full\n    y_full_centered = y - y_mean_full\n    \n    scales_full = np.sqrt(np.sum(X_full_centered**2, axis=0) / n)\n    scales_full[scales_full == 0] = 1.0\n    X_full_std = X_full_centered / scales_full\n    \n    beta_pilot_for_lambda = pinv(X_full_std) @ y_full_centered\n    beta_pilot_for_lambda_perturbed = beta_pilot_for_lambda + np.random.normal(0, sigma_pilot, size=p)\n    weights_for_lambda = 1.0 / (np.abs(beta_pilot_for_lambda_perturbed)**gamma + 1e-8)\n    \n    lambda_max = np.max(np.abs(X_full_std.T @ y_full_centered) / weights_for_lambda)\n    lambda_grid = np.logspace(np.log10(lambda_max), np.log10(0.05 * lambda_max), 40)\n    \n    cv_errors = np.zeros(len(lambda_grid))\n    \n    for i in range(K):\n        val_indices = folds[i]\n        train_indices = np.concatenate([folds[j] for j in range(K) if j != i])\n        \n        X_train, y_train = X[train_indices], y[train_indices]\n        X_val, y_val = X[val_indices], y[val_indices]\n        n_train = X_train.shape[0]\n\n        # Standardize based on training data\n        X_train_mean = X_train.mean(axis=0)\n        y_train_mean = y_train.mean()\n        \n        X_train_centered = X_train - X_train_mean\n        X_val_centered = X_val - X_train_mean\n        \n        train_scales = np.sqrt(np.sum(X_train_centered**2, axis=0) / n_train)\n        train_scales[train_scales == 0] = 1.0\n        \n        X_train_std = X_train_centered / train_scales\n        X_val_std = X_val_centered / train_scales\n        \n        y_train_centered = y_train - y_train_mean\n        \n        # Pilot weights for this fold\n        beta_pilot_fold = pinv(X_train_std) @ y_train_centered\n        beta_pilot_fold_perturbed = beta_pilot_fold + np.random.normal(0, sigma_pilot, size=p)\n        weights_fold = 1.0 / (np.abs(beta_pilot_fold_perturbed)**gamma + 1e-8)\n        \n        # Run coordinate descent for each lambda with warm starts\n        beta_current = np.zeros(p)\n        for j, lambda_val in enumerate(lambda_grid):\n            beta_current = adaptive_lasso_cd(X_train_std, y_train_centered, weights_fold, lambda_val, beta_init=beta_current)\n            \n            # Calculate validation error\n            y_val_pred = X_val_std @ beta_current + y_train_mean\n            mse = np.mean((y_val - y_val_pred)**2)\n            cv_errors[j] += mse\n    \n    # 3. Find best lambda from CV\n    best_lambda_idx = np.argmin(cv_errors)\n    best_lambda = lambda_grid[best_lambda_idx]\n    \n    # 4. Final fit on full data with best lambda\n    # Standardization uses X_full_std, y_full_centered computed earlier\n    \n    # Final pilot weights (recomputed on full data with fresh noise)\n    beta_pilot_final = pinv(X_full_std) @ y_full_centered\n    beta_pilot_final_perturbed = beta_pilot_final + np.random.normal(0, sigma_pilot, size=p)\n    weights_final = 1.0 / (np.abs(beta_pilot_final_perturbed)**gamma + 1e-8)\n    \n    # Final model fit\n    beta_final = adaptive_lasso_cd(X_full_std, y_full_centered, weights_final, best_lambda, beta_init=np.zeros(p))\n    \n    # 5. Check support\n    selected_support = {j for j, val in enumerate(beta_final) if np.abs(val) > 1e-7}\n    \n    return selected_support == oracle_support\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}