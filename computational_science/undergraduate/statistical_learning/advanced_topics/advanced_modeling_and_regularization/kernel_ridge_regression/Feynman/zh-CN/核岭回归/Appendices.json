{
    "hands_on_practices": [
        {
            "introduction": "理论知识是基石，但只有通过亲手计算，我们才能真正内化复杂的概念。本练习旨在弥合再生核希尔伯特空间（$RKHS$）的抽象理论与具体求解过程之间的鸿沟。通过从头推导一个简单的核岭回归（$KRR$）问题 ，你将巩固对表示定理的应用以及如何确定系数向量 $\\alpha$ 并构建最终预测函数的理解。",
            "id": "3136849",
            "problem": "考虑对一维输入进行核岭回归，训练数据为 $\\{(x_1, y_1), (x_2, y_2)\\}$，其中 $x_1 = 0$，$x_2 = 1$，$y_1 = 1$，$y_2 = -1$。设假设空间是与高斯核 $k(x, x') = \\exp\\!\\big(-(x - x')^{2}\\big)$ 和正则化参数 $\\lambda = \\frac{1}{2}$ 相关联的再生核希尔伯特空间 (RKHS)。核岭回归估计量定义为在 RKHS 中所有函数 $f$ 中最小化以下目标的函数：\n$$\n\\sum_{i=1}^{2} \\big(y_i - f(x_i)\\big)^{2} + \\lambda \\|f\\|_{\\mathcal{H}}^{2}.\n$$\n使用 RKHS 的表示定理的基本陈述和一阶最优性条件，推导估计量的函数形式以及用核矩阵表示的系数的线性系统。然后，通过对核函数应用标准微积分，推导核岭回归预测器关于输入 $x$ 的导数 $\\frac{d}{dx} f(x)$。最后，对于给定的数据和参数，在点 $x^{\\star} = \\frac{1}{2}$ 处对此导数求值。将您的最终答案表示为仅包含有理数和指数的单一精确封闭形式解析表达式。不要进行数值近似。",
            "solution": "首先对问题进行验证，以确保其具有科学依据、适定且客观。\n\n### 步骤 1: 提取已知条件\n- 训练数据点：$(x_1, y_1) = (0, 1)$ 和 $(x_2, y_2) = (1, -1)$。\n- 输入向量：$\\mathbf{x} \\in \\mathbb{R}^1$。\n- 核函数：$k(x, x') = \\exp(-(x - x')^2)$。\n- 正则化参数：$\\lambda = \\frac{1}{2}$。\n- 要最小化的目标函数：$J(f) = \\sum_{i=1}^{2} (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}}^2$。\n- 任务：推导估计量 $f(x)$ 的函数形式、系数的线性系统、导数 $\\frac{d}{dx}f(x)$，并在点 $x^{\\star} = \\frac{1}{2}$ 处对该导数求值。\n\n### 步骤 2: 使用提取的已知条件进行验证\n- **科学依据：** 该问题基于核岭回归 (KRR)，这是机器学习和统计学中的一个标准和基础技术。使用高斯核和再生核希尔伯特空间 (RKHS) 框架是公认的方法。\n- **适定性：** 目标函数是平方误差项和范数平方惩罚项的和。对于 $\\lambda > 0$，此函数是严格凸的，因为对于不同点的核矩阵 $K$ 是正定的，这使得 $K + \\lambda I$ 也是正定且可逆的。这保证了存在唯一、稳定且有意义的解。\n- **客观性：** 问题以精确的数学语言陈述，没有任何主观或模棱两可的术语。\n\n### 步骤 3: 结论与行动\n该问题是有效的，因为它是自洽的、科学上合理的且适定的。将提供完整的解答。\n\n### 解答推导\n核岭回归 (KRR) 估计量 $f(x)$ 是再生核希尔伯特空间 (RKHS) $\\mathcal{H}$ 中的函数，它最小化正则化最小二乘目标函数：\n$$\nJ(f) = \\sum_{i=1}^{2} (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}}^2\n$$\n根据表示定理，最小化子 $f$ 可以表示为以训练数据点为中心的核函数的线性组合：\n$$\nf(x) = \\sum_{i=1}^{2} \\alpha_i k(x_i, x)\n$$\n其中 $\\alpha_i$ 是待定系数。\n\n$f$ 在 RKHS 中的范数由下式给出：\n$$\n\\|f\\|_{\\mathcal{H}}^2 = \\left\\langle \\sum_{i=1}^{2} \\alpha_i k(x_i, \\cdot), \\sum_{j=1}^{2} \\alpha_j k(x_j, \\cdot) \\right\\rangle_{\\mathcal{H}} = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\alpha_i \\alpha_j k(x_i, x_j) = \\boldsymbol{\\alpha}^T K \\boldsymbol{\\alpha}\n$$\n其中 $K$ 是一个 $2 \\times 2$ 的格拉姆矩阵（或核矩阵），其元素为 $K_{ij} = k(x_i, x_j)$，且 $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2)^T$。\n\n在训练点上的预测值为 $f(x_i) = \\sum_{j=1}^{2} \\alpha_j k(x_j, x_i) = (K\\boldsymbol{\\alpha})_i$。令 $\\mathbf{y} = (y_1, y_2)^T$。目标函数可以用矩阵形式重写为：\n$$\nJ(\\boldsymbol{\\alpha}) = (\\mathbf{y} - K\\boldsymbol{\\alpha})^T(\\mathbf{y} - K\\boldsymbol{\\alpha}) + \\lambda \\boldsymbol{\\alpha}^T K \\boldsymbol{\\alpha}\n$$\n为了找到最小化 $J(\\boldsymbol{\\alpha})$ 的最优系数 $\\boldsymbol{\\alpha}$，我们计算 $J$ 相对于 $\\boldsymbol{\\alpha}$ 的梯度并将其设为零。\n$$\n\\nabla_{\\boldsymbol{\\alpha}} J(\\boldsymbol{\\alpha}) = -2 K^T (\\mathbf{y} - K\\boldsymbol{\\alpha}) + 2 \\lambda K \\boldsymbol{\\alpha}\n$$\n由于核矩阵 $K$ 是对称的（$K^T=K$），上式可简化为：\n$$\n\\nabla_{\\boldsymbol{\\alpha}} J(\\boldsymbol{\\alpha}) = -2 K (\\mathbf{y} - K\\boldsymbol{\\alpha}) + 2 \\lambda K \\boldsymbol{\\alpha} = -2 K \\mathbf{y} + 2 K^2 \\boldsymbol{\\alpha} + 2 \\lambda K \\boldsymbol{\\alpha}\n$$\n将梯度设为零：\n$$\n-2 K \\mathbf{y} + 2 K^2 \\boldsymbol{\\alpha} + 2 \\lambda K \\boldsymbol{\\alpha} = \\mathbf{0} \\implies K( (K + \\lambda I)\\boldsymbol{\\alpha} - \\mathbf{y} ) = \\mathbf{0}\n$$\n由于高斯核对于不同的点生成正定核矩阵，因此 $K$ 是可逆的。所以，我们可以左乘 $K^{-1}$ 来获得系数的线性系统：\n$$\n(K + \\lambda I)\\boldsymbol{\\alpha} = \\mathbf{y}\n$$\n现在，我们代入给定的数据。训练点是 $x_1 = 0$ 和 $x_2 = 1$。核函数是 $k(x, x') = \\exp(-(x-x')^2)$。核矩阵 $K$ 是：\n$$\nK = \\begin{pmatrix} k(x_1, x_1)  k(x_1, x_2) \\\\ k(x_2, x_1)  k(x_2, x_2) \\end{pmatrix} = \\begin{pmatrix} k(0, 0)  k(0, 1) \\\\ k(1, 0)  k(1, 1) \\end{pmatrix}\n$$\n$K_{11} = k(0, 0) = \\exp(-(0-0)^2) = \\exp(0) = 1$。\n$K_{12} = k(0, 1) = \\exp(-(0-1)^2) = \\exp(-1)$。\n$K_{21} = k(1, 0) = \\exp(-(1-0)^2) = \\exp(-1)$。\n$K_{22} = k(1, 1) = \\exp(-(1-1)^2) = \\exp(0) = 1$。\n所以，$K = \\begin{pmatrix} 1  \\exp(-1) \\\\ \\exp(-1)  1 \\end{pmatrix}$。\n\n响应向量是 $\\mathbf{y} = (1, -1)^T$，正则化参数是 $\\lambda = \\frac{1}{2}$。线性系统变为：\n$$\n\\left( \\begin{pmatrix} 1  \\exp(-1) \\\\ \\exp(-1)  1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} \\frac{3}{2}  \\exp(-1) \\\\ \\exp(-1)  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n为了求解 $\\boldsymbol{\\alpha}$，我们对矩阵 $A = K + \\lambda I$ 求逆：\n$$\n\\det(A) = \\left(\\frac{3}{2}\\right)^2 - (\\exp(-1))^2 = \\frac{9}{4} - \\exp(-2)\n$$\n$$\nA^{-1} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2}  -\\exp(-1) \\\\ -\\exp(-1)  \\frac{3}{2} \\end{pmatrix}\n$$\n现在，我们求 $\\boldsymbol{\\alpha} = A^{-1}\\mathbf{y}$：\n$$\n\\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2}  -\\exp(-1) \\\\ -\\exp(-1)  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2} + \\exp(-1) \\\\ -\\exp(-1) - \\frac{3}{2} \\end{pmatrix}\n$$\n我们可以简化 $\\alpha_1$：\n$$\n\\alpha_1 = \\frac{\\frac{3}{2} + \\exp(-1)}{\\frac{9}{4} - \\exp(-2)} = \\frac{\\frac{3}{2} + \\exp(-1)}{(\\frac{3}{2} - \\exp(-1))(\\frac{3}{2} + \\exp(-1))} = \\frac{1}{\\frac{3}{2} - \\exp(-1)} = \\frac{1}{\\frac{3\\exp(1) - 2}{2\\exp(1)}} = \\frac{2\\exp(1)}{3\\exp(1) - 2}\n$$\n且 $\\alpha_2 = -\\alpha_1 = -\\frac{2\\exp(1)}{3\\exp(1) - 2}$。\n\nKRR 预测器是 $f(x) = \\alpha_1 k(0, x) + \\alpha_2 k(1, x) = \\alpha_1 \\exp(-x^2) + \\alpha_2 \\exp(-(1-x)^2)$。\n我们需要求它关于 $x$ 的导数：\n$$\n\\frac{d}{dx}f(x) = \\alpha_1 \\frac{d}{dx}\\exp(-x^2) + \\alpha_2 \\frac{d}{dx}\\exp(-(1-x)^2)\n$$\n使用链式法则：\n$$\n\\frac{d}{dx}f(x) = \\alpha_1 (-2x \\exp(-x^2)) + \\alpha_2 (-2(1-x)(-1) \\exp(-(1-x)^2))\n$$\n$$\n\\frac{d}{dx}f(x) = -2x \\alpha_1 \\exp(-x^2) + 2(1-x) \\alpha_2 \\exp(-(1-x)^2)\n$$\n最后，我们在点 $x^{\\star} = \\frac{1}{2}$ 处对该导数求值：\n$$\n\\frac{d}{dx}f(x)\\bigg|_{x=1/2} = -2\\left(\\frac{1}{2}\\right)\\alpha_1 \\exp\\left(-\\left(\\frac{1}{2}\\right)^2\\right) + 2\\left(1 - \\frac{1}{2}\\right)\\alpha_2 \\exp\\left(-\\left(1 - \\frac{1}{2}\\right)^2\\right)\n$$\n$$\n= -\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right) + 2\\left(\\frac{1}{2}\\right)\\alpha_2 \\exp\\left(-\\left(\\frac{1}{2}\\right)^2\\right)\n$$\n$$\n= -\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right) + \\alpha_2 \\exp\\left(-\\frac{1}{4}\\right) = (\\alpha_2 - \\alpha_1)\\exp\\left(-\\frac{1}{4}\\right)\n$$\n代入 $\\alpha_2 = -\\alpha_1$：\n$$\n(-\\alpha_1 - \\alpha_1)\\exp\\left(-\\frac{1}{4}\\right) = -2\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right)\n$$\n现在，代入 $\\alpha_1$ 的值：\n$$\n-2 \\left( \\frac{2\\exp(1)}{3\\exp(1) - 2} \\right) \\exp\\left(-\\frac{1}{4}\\right) = -\\frac{4\\exp(1)\\exp(-1/4)}{3\\exp(1) - 2}\n$$\n$$\n= -\\frac{4\\exp(1 - 1/4)}{3\\exp(1) - 2} = -\\frac{4\\exp(3/4)}{3\\exp(1) - 2}\n$$\n这就是最终的精确封闭解析表达式。",
            "answer": "$$\\boxed{-\\frac{4\\exp(3/4)}{3\\exp(1) - 2}}$$"
        },
        {
            "introduction": "理论公式在形式上可能简洁明了，但其在实际计算中可能隐藏着复杂性。本练习将引导你从纸笔推导转向计算机编程，揭示核岭回归中常见的数值稳定性问题，例如由病态核矩阵引起的挑战。你将学习如何诊断并使用“抖动”（jitter）这一关键技术来缓解这些问题，这是开发稳健机器学习模型的必备技能 。",
            "id": "3136815",
            "problem": "要求您编写一个完整、可运行的程序，研究通过 Cholesky 分解求解核岭回归法方程的数值条件和稳定性，并提出能够在不引入过度偏差的情况下稳定求解器的最小对角抖动量级。该问题必须完全用纯数学和数值线性代数的术语来解决，最终输出必须是单行的浮点抖动值列表，每个测试用例对应一个值。\n\n基本定义与事实：\n- 一个对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 的，当且仅当其所有特征值都严格为正。对于一个 SPD 矩阵 $A$，其 Cholesky 分解 $A = L L^{\\top}$ 存在且唯一，其中 $L$ 是一个具有正对角元的下三角矩阵。\n- 一个 SPD 矩阵 $A$ 的 2-范数条件数是 $\\kappa_2(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是 $A$ 的最大和最小特征值。\n- 在核岭回归中，求解对偶系数归结为求解形如 $(K + \\lambda I)\\alpha = y$ 的线性系统，其中 $K$ 是核 Gram 矩阵，$\\lambda \\ge 0$ 是一个正则化参数，$I$ 是单位矩阵，$y$ 是目标向量。\n- 添加一个单位矩阵的小倍数 $j I$（称为抖动），其中 $j \\ge 0$，可以通过增加 $\\lambda_{\\min}$ 来改善数值稳定性，但过大的 $j$ 会改变所求解的系统，从而使解产生偏差。\n\n您的任务：\n- 对于下方的每个测试用例，使用指定的带宽 $\\sigma > 0$ 和高斯径向基函数核 $k(\\mathbf{x}, \\mathbf{z}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|_2^2}{2\\sigma^2}\\right)$ 构建核矩阵 $K$。\n- 对于给定的基础正则化 $\\lambda \\ge 0$，构造 $M(\\lambda) = K + \\lambda I$。\n- 从一个可复现的正态分布中生成真实解向量 $\\alpha_{\\mathrm{true}} \\in \\mathbb{R}^n$，计算 $y = M(\\lambda)\\,\\alpha_{\\mathrm{true}}$，然后对于一个抖动值 $j$ 的网格，考虑受扰动的系统 $A(j) = M(\\lambda) + j I$。\n- 对于网格中的每个 $j$，执行以下所有操作：\n  1. 如果 $A(j)$ 不是 SPD（即，其最小特征值不严格为正），则跳过此 $j$。\n  2. 计算基于特征值的条件数 $\\kappa_2(A(j))$。\n  3. 使用基于特征分解的求解器计算 $A(j)\\,\\alpha = y$ 的参考解 $\\alpha_{\\mathrm{eig}}(j)$，该求解器对于 SPD 矩阵是良态的。\n  4. 使用 Cholesky 分解和三角求解计算 $A(j)\\,\\alpha = y$ 的基于 Cholesky 的解 $\\alpha_{\\mathrm{chol}}(j)$。\n  5. 将 Cholesky 求解的相对数值误差量化为 $e_{\\mathrm{num}}(j) = \\|\\alpha_{\\mathrm{chol}}(j) - \\alpha_{\\mathrm{eig}}(j)\\|_2 / \\|\\alpha_{\\mathrm{eig}}(j)\\|_2$。\n  6. 将添加抖动所引入的相对偏差量化为 $e_{\\mathrm{bias}}(j) = \\|\\alpha_{\\mathrm{eig}}(j) - \\alpha_{\\mathrm{true}}\\|_2 / \\|\\alpha_{\\mathrm{true}}\\|_2$。\n- 从网格中选择最小的抖动 $j^\\star$，使其同时满足数值稳定性和偏差标准\n  $$e_{\\mathrm{num}}(j^\\star) \\le \\tau_{\\mathrm{num}} \\quad \\text{和} \\quad e_{\\mathrm{bias}}(j^\\star) \\le \\tau_{\\mathrm{bias}},$$\n  容差为 $\\tau_{\\mathrm{num}} = 10^{-8}$ 和 $\\tau_{\\mathrm{bias}} = 10^{-3}$。\n- 如果网格中没有 $j$ 同时满足这两个标准，则在满足 $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$ 的那些 $j$ 中，选择使 $e_{\\mathrm{num}}(j)$ 最小的一个；如果也没有一个满足偏差标准，则选择 Cholesky 分解成功且 $A(j)$ 是 SPD 的、并使 $e_{\\mathrm{bias}}(j)$ 最小的 $j$。\n\n测试套件：\n- 通用设置：\n  - 使用高斯核 $k(\\mathbf{x}, \\mathbf{z}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|_2^2}{2\\sigma^2}\\right)$。\n  - 对所有描述的范数均使用欧几里得 2-范数。\n  - 抖动网格为 $\\{0\\} \\cup \\{10^k : k \\in \\{-16,-15,\\dots,-2\\}\\}$。\n  - 对于特征值检查，仅当数值计算出的最小特征值严格为正时，才将 $A(j)$ 视为 SPD。\n- 情况 1（病态但 SPD，旨在需要非零抖动）：\n  - 维度 $n = 25$，输入为 $\\mathbf{x}_i = i/(n-1)$，其中 $i \\in \\{0,1,\\dots,n-1\\}$，并视为一维列向量。\n  - 带宽 $\\sigma = 10.0$。\n  - 正则化 $\\lambda = 0$。\n  - 真实值：使用固定种子 $42$ 从 $\\mathcal{N}(0,1)^n$ 中抽取 $\\alpha_{\\mathrm{true}}$。\n- 情况 2（含重复项的半正定矩阵，Cholesky 在零抖动时失败；需要正抖动）：\n  - 从 $i \\in \\{0,1,\\dots,24\\}$ 的 $25$ 个输入 $\\mathbf{x}_i = i/24$ 开始，然后追加索引为 10 到 14 的输入的 5 个副本，以获得总共 $n = 30$ 个点，所有点都作为一维列向量。\n  - 带宽 $\\sigma = 0.2$。\n  - 正则化 $\\lambda = 0$。\n  - 真实值：使用固定种子 $7$ 从 $\\mathcal{N}(0,1)^n$ 中抽取 $\\alpha_{\\mathrm{true}}$。\n- 情况 3（良态，零抖动应已足够）：\n  - 维度 $n = 60$，输入使用固定种子 $123$ 从 $\\mathrm{Uniform}[0,1]$ 中独立抽取，并作为一维列向量。\n  - 带宽 $\\sigma = 2.0$。\n  - 正则化 $\\lambda = 10^{-12}$。\n  - 真实值：使用固定种子 $99$ 从 $\\mathcal{N}(0,1)^n$ 中抽取 $\\alpha_{\\mathrm{true}}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个浮点数对应一个测试用例所选的抖动 $j^\\star$，并按情况 1、2、3 的顺序排列。例如，输出应类似于 $[j_1,j_2,j_3]$。\n\n不涉及任何物理单位或角度。所有答案都是无量纲实数。在给定指定种子和输入的情况下，输出值必须是确定性的。不要打印任何额外文本。",
            "solution": "用户寻求为三个不同的测试用例确定添加到核岭回归矩阵 $M(\\lambda) = K + \\lambda I$ 中的最优对角抖动 $j^\\star$。最优抖动 $j^\\star$ 是从指定网格中找到的最小值，它能确保基于 Cholesky 的线性求解器的数值稳定性，同时将引起的解偏差保持在给定的容差范围内。该问题需要在线性代数和数值分析原理的指导下进行系统性的数值研究。\n\n每个测试用例的总体流程如下：\n1.  **系统设置**：构建数据点 $\\mathbf{x}_i$、高斯 RBF 核矩阵 $K$、基础回归矩阵 $M(\\lambda) = K + \\lambda I$ 和真实解向量 $\\alpha_{\\mathrm{true}}$。然后将目标向量合成为 $y = M(\\lambda) \\alpha_{\\mathrm{true}}$。\n2.  **迭代分析**：对于网格 $\\{0\\} \\cup \\{10^k : k \\in \\{-16, \\dots, -2\\}\\}$ 中的每个抖动值 $j$，构造受扰动矩阵 $A(j) = M(\\lambda) + j I$。\n3.  **验证与求解**：\n    a. 我们首先通过计算其特征值并确保最小特征值 $\\lambda_{\\min}(A(j)) > 0$ 来验证 $A(j)$ 是对称正定 (SPD) 的。如果 $A(j)$ 不是 SPD，则它不适合进行 Cholesky 分解，我们便丢弃该 $j$ 值。\n    b. 对于一个有效的 SPD 矩阵 $A(j)$，我们计算系统 $A(j)\\alpha = y$ 的两种解：\n        i. **参考解 $\\alpha_{\\mathrm{eig}}(j)$**：这是使用特征分解计算的。如果 $A(j) = VDV^\\top$，其中 $V$ 是特征向量的正交矩阵，$D$ 是特征值的对角矩阵，则解为 $\\alpha_{\\mathrm{eig}}(j) = V D^{-1} V^\\top y$。该方法对于 SPD 矩阵是数值稳定的，并作为我们准确性的基准。\n        ii. **Cholesky 解 $\\alpha_{\\mathrm{chol}}(j)$**：该解的计算方法是，首先找到 Cholesky 分解 $A(j) = LL^\\top$，然后求解两个三角系统：$Lz = y$（前向替换），然后是 $L^\\top \\alpha = z$（反向替换）。该方法计算效率高，但如果 $A(j)$ 是病态的（即具有大条件数 $\\kappa_2(A) = \\lambda_{\\max}/\\lambda_{\\min}$），则可能数值不稳定。如果对于一个理论上是 SPD 的矩阵，Cholesky 分解失败，这表明存在严重的病态问题，该 $j$ 值将被丢弃。\n4.  **误差量化**：为每个有效的 $j$ 计算两个误差度量：\n    a. **数值误差**：$e_{\\mathrm{num}}(j) = \\frac{\\|\\alpha_{\\mathrm{chol}}(j) - \\alpha_{\\mathrm{eig}}(j)\\|_2}{\\|\\alpha_{\\mathrm{eig}}(j)\\|_2}$。这衡量了基于 Cholesky 的求解器相对于稳定的基于特征分解的求解器的不准确性。高数值表明病态正在降低 Cholesky 解的质量。\n    b. **偏差误差**：$e_{\\mathrm{bias}}(j) = \\frac{\\|\\alpha_{\\mathrm{eig}}(j) - \\alpha_{\\mathrm{true}}\\|_2}{\\|\\alpha_{\\mathrm{true}}\\|_2}$。这衡量了抖动扰动系统 $A(j)\\alpha = y$ 的解与原始、未扰动问题的真实解 $\\alpha_{\\mathrm{true}}$ 的偏离程度。添加抖动 $j>0$ 内在地改变了我们正在求解的问题，从而引入了偏差。\n5.  **最优抖动选择**：最优抖动 $j^\\star$ 是基于一个分层标准选择的，该标准旨在找到能实现数值稳定性而不过度引入偏差的最小可能扰动。\n    a. **主要标准**：找到满足 $e_{\\mathrm{num}}(j) \\le \\tau_{\\mathrm{num}}$ 和 $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$ 的最小 $j$，其中容差给定为 $\\tau_{\\mathrm{num}} = 10^{-8}$ 和 $\\tau_{\\mathrm{bias}} = 10^{-3}$。\n    b. **次要标准（如果主要标准失败）**：如果没有 $j$ 同时满足这两个条件，我们放宽数值稳定性的要求。在所有满足偏差约束 $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$ 的 $j$ 中，我们选择使数值误差 $e_{\\mathrm{num}}(j)$ 最小的那个。\n    c. **第三标准（如果次要标准也失败）**：如果没有 $j$ 满足偏差约束，我们优先寻找一个解，无论其偏差多大。在所有使矩阵 $A(j)$ 为 SPD 且 Cholesky 求解器成功的 $j$ 中，我们选择使偏差误差 $e_{\\mathrm{bias}}(j)$ 最小的那个。\n\n此过程将应用于三个测试用例中的每一个，这些用例旨在探究不同场景：一个病态但理论上是 SPD 的矩阵，一个由重复数据产生的奇异矩阵，以及一个良态矩阵。\n\n**情况 1**：使用大带宽 $\\sigma=10.0$ 和 $\\lambda=0$ 生成 $K$。这会创建一个病态的核矩阵，其最小特征值非常接近于零，使得 $A(0)=K$ 对于 Cholesky 分解在数值上具有挑战性。预计需要一个小的非零抖动来“提升”这些小特征值并稳定计算。\n\n**情况 2**：输入数据包含重复点，且 $\\lambda=0$。这使得核矩阵 $K$ 在数学上是奇异的（即，它至少有一个特征值恰好为零），因而是半正定而非正定的。对 $A(0)=K$ 进行 Cholesky 分解保证会失败。严格需要一个正的抖动 $j>0$ 才能使矩阵 $A(j) = K + jI$ 成为正定矩阵。\n\n**情况 3**：矩阵由一个小的非零正则化 $\\lambda = 10^{-12}$ 和唯一的数据点构成。所得矩阵 $A(0) = K + \\lambda I$ 预计将是良态且严格正定的。因此，我们预期不需要额外的抖动，并且 $j^\\star=0$ 将满足主要标准。\n\n实现将使用 `numpy` 进行线性代数运算，并使用 `scipy.linalg` 中专门的、鲁棒的求解器，如 `cholesky` 和 `solve_triangular`。通过对所有随机数生成使用固定的种子来确保可复现性。最终输出将是一个列表，包含为每个情况确定的 $j^\\star$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular, eigh\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            'case_id': 1,\n            'n': 25,\n            'data_gen': lambda n, rng: np.linspace(0, 1, n).reshape(-1, 1),\n            'sigma': 10.0,\n            'lambda': 0.0,\n            'seed_alpha': 42,\n            'seed_data': None,\n        },\n        {\n            'case_id': 2,\n            'n_base': 25,\n            'data_gen': lambda n_base, rng: np.concatenate([\n                np.linspace(0, 1, n_base).reshape(-1, 1),\n                np.linspace(0, 1, n_base).reshape(-1, 1)[10:15]\n            ]),\n            'sigma': 0.2,\n            'lambda': 0.0,\n            'seed_alpha': 7,\n            'seed_data': None,\n        },\n        {\n            'case_id': 3,\n            'n': 60,\n            'data_gen': lambda n, rng: rng.uniform(0, 1, n).reshape(-1, 1),\n            'sigma': 2.0,\n            'lambda': 1e-12,\n            'seed_alpha': 99,\n            'seed_data': 123,\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        j_star = process_case(case_params)\n        results.append(j_star)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef rbf_kernel(X, sigma):\n    \"\"\"\n    Computes the Gaussian RBF kernel matrix.\n    k(x, z) = exp(-||x-z||_2^2 / (2*sigma^2))\n    \n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n        sigma (float): Bandwidth of the RBF kernel.\n\n    Returns:\n        np.ndarray: The kernel matrix K of shape (n, n).\n    \"\"\"\n    sq_dists = cdist(X, X, 'sqeuclidean')\n    return np.exp(-sq_dists / (2 * sigma**2))\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case to find the optimal jitter.\n    \"\"\"\n    # Tolerances\n    tau_num = 1e-8\n    tau_bias = 1e-3\n\n    # Generate data\n    data_rng = np.random.default_rng(params['seed_data']) if params['seed_data'] is not None else None\n    if params['case_id'] == 2:\n        X = params['data_gen'](params['n_base'], data_rng)\n        n = X.shape[0]\n    else:\n        n = params['n']\n        X = params['data_gen'](n, data_rng)\n\n    # Generate ground truth alpha\n    alpha_rng = np.random.default_rng(params['seed_alpha'])\n    alpha_true = alpha_rng.normal(0, 1, n)\n\n    # System setup\n    K = rbf_kernel(X, params['sigma'])\n    M_lambda = K + params['lambda'] * np.identity(n)\n    y = M_lambda @ alpha_true\n\n    # Jitter analysis\n    jitter_grid = [0.0] + [10.0**k for k in range(-16, -1)]\n    j_results = []\n\n    for j in jitter_grid:\n        A_j = M_lambda + j * np.identity(n)\n\n        # 1. Check if SPD via eigenvalues\n        try:\n            eigenvalues, V = eigh(A_j)\n            if np.min(eigenvalues) = 0:\n                continue\n        except np.linalg.LinAlgError:\n            continue\n\n        # 3. Reference solution (eigen-decomposition)\n        D_inv = np.diag(1.0 / eigenvalues)\n        alpha_eig = V @ (D_inv @ (V.T @ y))\n        \n        # 4. Cholesky solution\n        try:\n            L = cholesky(A_j, lower=True)\n            z = solve_triangular(L, y, lower=True, check_finite=False)\n            alpha_chol = solve_triangular(L.T, z, lower=False, check_finite=False)\n        except np.linalg.LinAlgError:\n            continue\n\n        # 5. Numerical error\n        norm_alpha_eig = np.linalg.norm(alpha_eig)\n        e_num = np.linalg.norm(alpha_chol - alpha_eig) / norm_alpha_eig if norm_alpha_eig > 0 else 0.0\n\n        # 6. Bias error\n        norm_alpha_true = np.linalg.norm(alpha_true)\n        e_bias = np.linalg.norm(alpha_eig - alpha_true) / norm_alpha_true if norm_alpha_true > 0 else 0.0\n\n        j_results.append({'j': j, 'e_num': e_num, 'e_bias': e_bias})\n    \n    # Selection logic\n    # Rule 1: Smallest j satisfying both criteria\n    s1_candidates = [r for r in j_results if r['e_num'] = tau_num and r['e_bias'] = tau_bias]\n    if s1_candidates:\n        return min(r['j'] for r in s1_candidates)\n\n    # Rule 2: Among those satisfying bias, find j that minimizes num_error\n    s2_candidates = [r for r in j_results if r['e_bias'] = tau_bias]\n    if s2_candidates:\n        # min will pick the first one in case of a tie in e_num, which corresponds to smaller j\n        best_choice = min(s2_candidates, key=lambda r: r['e_num'])\n        return best_choice['j']\n\n    # Rule 3: Among all successful, find j that minimizes bias_error\n    if j_results:\n        # min will pick the first one in case of a tie in e_bias, which corresponds to smaller j\n        best_choice = min(j_results, key=lambda r: r['e_bias'])\n        return best_choice['j']\n        \n    # Should not be reached in this problem\n    return float('nan')\n\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "模型评估是机器学习流程的核心环节，但像留一法交叉验证（$LOOCV$）这样的暴力方法在计算上可能成本高昂。本练习揭示了一个优美且极为重要的理论结果，它为计算 $LOOCV$ 预测提供了一条巨大的捷径，而无需重复训练模型。这个推导  不仅展示了数学洞察力在设计高效算法中的威力，还通过“帽子矩阵”的概念将核岭回归（$KRR$）与更广泛的线性平滑器家族联系起来。",
            "id": "3136836",
            "problem": "考虑一个训练样本 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，一个正半定核函数 $k(\\cdot, \\cdot)$，其 Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 定义为 $K_{ij} = k(x_{i}, x_{j})$，以及一个正则化参数 $\\lambda  0$。核岭回归 (Kernel Ridge Regression, KRR) 定义为惩罚经验风险\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{i} - f(x_{i})\\big)^{2} + \\lambda \\,\\|f\\|_{\\mathcal{H}}^{2},\n$$\n的最小化器，其中 $\\mathcal{H}$ 表示由 $k(\\cdot, \\cdot)$ 导出的再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS)。一个经过充分验证的事实是，在训练输入处的拟合值是响应向量的线性函数，因此存在一个称为帽子矩阵的矩阵 $S_{\\lambda} \\in \\mathbb{R}^{n \\times n}$，使得拟合值向量为 $\\hat{\\boldsymbol{y}} = S_{\\lambda} \\boldsymbol{y}$。令 $s_{ii}$ 表示 $S_{\\lambda}$ 的第 $i$ 个对角线元素，并令 $\\hat{y}_{i}$ 表示 $\\hat{\\boldsymbol{y}}$ 的第 $i$ 个元素。\n\n对于每个索引 $i \\in \\{1, \\dots, n\\}$，将留一法拟合值 $\\hat{y}_{-i}$ 定义为在除去数据对 $(x_{i}, y_{i})$ 的所有数据上训练 KRR 后，在 $x_{i}$ 处得到的预测值。仅从上述基本定义以及估计器相对于 $\\boldsymbol{y}$ 的线性性质出发，推导出 $\\hat{y}_{-i}$ 关于 $\\hat{y}_{i}$、$y_{i}$ 和 $s_{ii}$ 的闭式解析表达式。你的最终答案必须是单个闭式表达式。",
            "solution": "我们从给定的基础出发：核岭回归 (KRR) 在训练输入处产生的拟合值是响应向量 $\\boldsymbol{y} \\in \\mathbb{R}^{n}$ 的线性函数。因此，存在一个仅依赖于输入 $\\{x_{i}\\}_{i=1}^{n}$ 和 $\\lambda$ 的矩阵 $S_{\\lambda} \\in \\mathbb{R}^{n \\times n}$，使得\n$$\n\\hat{\\boldsymbol{y}} = S_{\\lambda} \\boldsymbol{y}.\n$$\n令 $s_{ij}$ 表示 $S_{\\lambda}$ 的 $(i,j)$ 元素，$s_{ii}$ 表示其在索引 $i$ 处的对角线元素。根据线性性质，第 $i$ 个拟合值可以写为\n$$\n\\hat{y}_{i} = \\sum_{j=1}^{n} s_{ij} y_{j} = s_{ii} y_{i} + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\n\n现在考虑留一法设置，其中数据对 $(x_{i}, y_{i})$ 被移除，KRR 在剩余的 $n-1$ 对数据上进行训练。用 $\\hat{y}_{-i}$ 表示这次留一法拟合在 $x_{i}$ 处的预测值。因为估计器相对于 $\\boldsymbol{y}$ 是线性的，且目标函数是二次的，所以增加一个残差恰好为零的观测值不会改变最小化器：如果在给定的拟合函数 $f$ 下，一个观测值满足 $y_{i} = f(x_{i})$，那么它对经验损失和梯度的贡献对于该拟合函数都为零，因此包含或排除该观测值不会改变最小化器。\n\n利用这一点，我们考虑在全部 $n$ 个点上进行训练，但将第 $i$ 个响应替换为变量 $t \\in \\mathbb{R}$。根据线性性质，\n$$\n\\hat{y}_{i}(t) = s_{ii} t + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\n选择 $t$ 等于留一法预测值 $\\hat{y}_{-i}$。在此选择下，观测值 $(x_{i}, t)$ 在由留一法拟合产生的拟合函数下将具有零残差，因此包含它不会改变最小化器。因此，在使用变量 $t$ 的全数据训练下，$x_{i}$ 处的拟合值必须等于 $t$ 本身：\n$$\nt = \\hat{y}_{i}(t) = s_{ii} t + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\n求解这个关于 $t$ 的标量线性方程，得到\n$$\nt = \\frac{\\sum_{j \\neq i} s_{ij} y_{j}}{1 - s_{ii}}.\n$$\n最后，用 $\\hat{y}_{i}$ 和 $y_{i}$ 来表示关于 $j \\neq i$ 的和：\n$$\n\\sum_{j \\neq i} s_{ij} y_{j} = \\hat{y}_{i} - s_{ii} y_{i}.\n$$\n将此代入 $t$ 的表达式中，即可得到留一法预测值的所需闭式公式：\n$$\n\\hat{y}_{-i} = \\frac{\\hat{y}_{i} - s_{ii} y_{i}}{1 - s_{ii}}.\n$$\n该表达式仅依赖于索引 $i$ 处的全数据拟合值、索引 $i$ 处的观测响应以及帽子矩阵 $S_{\\lambda}$ 的第 $i$ 个对角元素。",
            "answer": "$$\\boxed{\\frac{\\hat{y}_{i} - s_{ii} y_{i}}{1 - s_{ii}}}$$"
        }
    ]
}