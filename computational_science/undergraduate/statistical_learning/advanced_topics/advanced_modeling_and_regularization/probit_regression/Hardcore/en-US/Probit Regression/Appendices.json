{
    "hands_on_practices": [
        {
            "introduction": "Statistical software makes fitting models seem effortless, but under the hood, powerful optimization algorithms are at work. This practice pulls back the curtain on probit regression by guiding you through the implementation of one step of its primary fitting algorithm, the Newton-Raphson method. By deriving the score vector and Hessian matrix and coding the update step, you will gain a fundamental understanding of how model parameters are learned and why numerical stability is a critical consideration in practice. ",
            "id": "3162253",
            "problem": "Consider a binary response model with independent and identically distributed (IID) observations $\\{(x_i,y_i)\\}_{i=1}^n$, where $y_i \\in \\{0,1\\}$ and $x_i \\in \\mathbb{R}^p$ are fixed predictors. In the probit regression model, the conditional success probability is modeled as $p_i = \\Pr(y_i=1 \\mid x_i) = \\Phi(\\eta_i)$ with linear predictor $\\eta_i = x_i^\\top \\beta$ and $\\Phi(\\cdot)$ the standard normal cumulative distribution function. The log-likelihood for a parameter vector $\\beta \\in \\mathbb{R}^p$ is $\\ell(\\beta) = \\sum_{i=1}^n \\{ y_i \\log \\Phi(\\eta_i) + (1-y_i)\\log(1-\\Phi(\\eta_i)) \\}$. Starting from the fundamental definitions of the Bernoulli likelihood and the chain rule for differentiation, perform the following tasks:\n\n- Derive the score vector $s(\\beta)$ (the gradient of $\\ell(\\beta)$) and the observed Hessian matrix $H(\\beta)$ (the Jacobian of $s(\\beta)$), expressed in terms of $x_i$, $\\eta_i$, $\\Phi(\\eta_i)$, and the standard normal probability density function $\\varphi(\\eta_i)$.\n- Based on your expressions, implement exactly one Newton–Raphson iteration to produce an updated parameter vector $\\beta^{\\text{new}}$ from a current iterate $\\beta^{\\text{old}}$. Your implementation must be numerically robust for extreme values of $\\eta_i$, specifically for large positive or negative $\\eta_i$, where direct evaluation of ratios like $\\varphi(\\eta_i)/\\Phi(\\eta_i)$ or $\\varphi(\\eta_i)/(1-\\Phi(\\eta_i))$ can be unstable. Use log-domain evaluations and stable transformations that avoid catastrophic cancellation. If the observed Hessian is singular or ill-conditioned, apply a small diagonal ridge regularization that is negligible at ordinary scales but ensures a well-defined linear solve.\n- Analyze, in your implementation comments and in your solution derivation, why numerical instability arises near extreme $\\eta_i$ and how your chosen stabilization mitigates it.\n\nYour program must implement a function that, given a design matrix $X \\in \\mathbb{R}^{n \\times p}$, a binary response vector $y \\in \\{0,1\\}^n$, and a current iterate $\\beta^{\\text{old}} \\in \\mathbb{R}^p$, returns one Newton–Raphson update $\\beta^{\\text{new}}$. Use this function to process the following test suite. For each test case, compute and return the updated coefficient vector $\\beta^{\\text{new}}$ rounded to six decimal places.\n\nTest suite (each case specifies $(X, y, \\beta^{\\text{old}})$):\n\n- Case A (baseline, well-behaved): $X = \\begin{bmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$, $y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 0.5 \\end{bmatrix}$.\n- Case B (extreme tails mixed): $X = \\begin{bmatrix} 1  -10 \\\\ 1  0 \\\\ 1  10 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n- Case C (nearly separated): $X = \\begin{bmatrix} 1  -6 \\\\ 1  -3 \\\\ 1  0 \\\\ 1  3 \\\\ 1  6 \\end{bmatrix}$, $y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n- Case D (boundary extreme): $X = \\begin{bmatrix} 1  50 \\\\ 1  -50 \\end{bmatrix}$, $y = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. Each inner list corresponds to one case and must contain the updated coefficients in order, rounded to six decimal places. For example, an output with two cases and two coefficients per case must look like $[[a_{11},a_{12}],[a_{21},a_{22}]]$ with each $a_{ij}$ a decimal numeral. No physical units, angle units, or percentages are involved in this problem. The output must match the specified format exactly and contain the results for all four cases in the order A, B, C, D.",
            "solution": "The user wants to solve a statistical learning problem on probit regression.\n\n**1. Problem Validation**\n\nThe user has specified a standard problem in computational statistics: the implementation of the Newton-Raphson algorithm for fitting a probit regression model. All necessary components are provided: the model specification, the log-likelihood function, the task (derivation and implementation), and a set of test cases with well-defined inputs ($X$, $y$, $\\beta^{\\text{old}}$) and a required output format. The problem requires a careful handling of numerical stability, which is a standard and important topic in scientific computing. The problem is scientifically grounded, well-posed, and objective. It contains no contradictions, ambiguities, or pseudo-scientific claims.\n\n**Verdict: The problem is valid.**\n\n**2. Solution Derivation and Implementation Strategy**\n\nThe core of the problem is to find the maximum likelihood estimate (MLE) for the parameters $\\beta$ of a probit model. The Newton-Raphson algorithm is an iterative method for finding the roots of the first derivative of a function (the score vector, in this case), which corresponds to the extrema of the function itself (the log-likelihood). A single iteration updates the current parameter estimate $\\beta^{\\text{old}}$ to a new estimate $\\beta^{\\text{new}}$.\n\nThe probit model for a binary outcome $y_i \\in \\{0, 1\\}$ given predictors $x_i \\in \\mathbb{R}^p$ is defined by the success probability:\n$$p_i = \\Pr(y_i=1 \\mid x_i) = \\Phi(x_i^\\top \\beta) = \\Phi(\\eta_i)$$\nwhere $\\eta_i = x_i^\\top \\beta$ is the linear predictor and $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution. The log-likelihood function for $n$ independent observations is:\n$$\\ell(\\beta) = \\sum_{i=1}^n \\{ y_i \\log \\Phi(\\eta_i) + (1-y_i)\\log(1-\\Phi(\\eta_i)) \\}$$\n\n**2.1. Score Vector Derivation**\n\nThe score vector, $s(\\beta)$, is the gradient of the log-likelihood function with respect to $\\beta$. We find it by applying the chain rule. Let $\\varphi(\\cdot)$ be the probability density function (PDF) of the standard normal distribution, where $\\frac{d}{dz}\\Phi(z) = \\varphi(z)$.\n\nThe derivative of the log-likelihood for a single observation $\\ell_i$ with respect to a parameter component $\\beta_j$ is:\n$$\\frac{\\partial \\ell_i}{\\partial \\beta_j} = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$$\nThe first term is:\n$$\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i \\frac{1}{\\Phi(\\eta_i)} \\frac{\\partial \\Phi(\\eta_i)}{\\partial \\eta_i} + (1-y_i) \\frac{1}{1-\\Phi(\\eta_i)} \\frac{\\partial (1-\\Phi(\\eta_i))}{\\partial \\eta_i} = y_i \\frac{\\varphi(\\eta_i)}{\\Phi(\\eta_i)} - (1-y_i) \\frac{\\varphi(\\eta_i)}{1-\\Phi(\\eta_i)}$$\nThe second term is simply $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} (x_i^\\top \\beta) = x_{ij}$.\nCombining these, the $j$-th component of the score vector is:\n$$s_j(\\beta) = \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n x_{ij} \\left( y_i \\frac{\\varphi(\\eta_i)}{\\Phi(\\eta_i)} - (1-y_i) \\frac{\\varphi(\\eta_i)}{1-\\Phi(\\eta_i)} \\right)$$\nIn vector form, let's define two terms for notational clarity and numerical stability management: $\\lambda(\\eta) = \\frac{\\varphi(\\eta)}{\\Phi(\\eta)}$ and $\\lambda'(\\eta) = -\\frac{\\varphi(\\eta)}{1-\\Phi(\\eta)}$. The score vector can then be expressed as:\n$$s(\\beta) = \\sum_{i=1}^n x_i \\cdot \\left( y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i) \\right) = X^\\top c$$\nwhere $c$ is a vector with components $c_i = y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i)$.\n\n**2.2. Observed Hessian Matrix Derivation**\n\nThe observed Hessian matrix, $H(\\beta)$, is the matrix of second partial derivatives of the log-likelihood. Its element $H_{jk}$ is $\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}$. We find it by differentiating the score vector component $s_j(\\beta)$ with respect to $\\beta_k$:\n$$H_{jk}(\\beta) = \\frac{\\partial s_j(\\beta)}{\\partial \\beta_k} = \\sum_{i=1}^n x_{ij} \\frac{\\partial}{\\partial \\beta_k} \\left( \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\right) = \\sum_{i=1}^n x_{ij} \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = \\sum_{i=1}^n x_{ij} x_{ik} \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2}$$\nWe need the second derivative of $\\ell_i$ with respect to $\\eta_i$. Let's differentiate $\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i)$. Using the quotient rule and the fact that $\\frac{d}{d\\eta}\\varphi(\\eta) = -\\eta\\varphi(\\eta)$, we find:\n$$\\frac{d}{d\\eta}\\lambda(\\eta) = \\frac{d}{d\\eta}\\left(\\frac{\\varphi(\\eta)}{\\Phi(\\eta)}\\right) = \\frac{-\\eta\\varphi(\\eta)\\Phi(\\eta) - \\varphi(\\eta)^2}{\\Phi(\\eta)^2} = -\\eta\\lambda(\\eta) - \\lambda(\\eta)^2$$\n$$\\frac{d}{d\\eta}\\lambda'(\\eta) = \\frac{d}{d\\eta}\\left(-\\frac{\\varphi(\\eta)}{1-\\Phi(\\eta)}\\right) = -\\eta\\lambda'(\\eta) - \\lambda'(\\eta)^2$$\nTherefore, the second derivative is:\n$$\\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2} = y_i \\left(-\\eta_i \\lambda(\\eta_i) - \\lambda(\\eta_i)^2\\right) + (1-y_i) \\left(-\\eta_i \\lambda'(\\eta_i) - \\lambda'(\\eta_i)^2\\right)$$\nThe Hessian matrix can be written compactly as $H(\\beta) = X^\\top D X$, where $D$ is a diagonal matrix with diagonal entries $d_{ii} = \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2}$. For a strictly concave log-likelihood, $H(\\beta)$ is negative definite.\n\n**2.3. Newton-Raphson Update Step**\n\nThe Newton-Raphson method approximates the log-likelihood surface with a quadratic function and steps to its maximum. The update rule is:\n$$\\beta^{\\text{new}} = \\beta^{\\text{old}} - [H(\\beta^{\\text{old}})]^{-1} s(\\beta^{\\text{old}})$$\nComputationally, this is implemented by solving the linear system $H(\\beta^{\\text{old}}) \\Delta\\beta = s(\\beta^{\\text{old}})$ for the step $\\Delta\\beta$, and then updating $\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\Delta\\beta$.\n\n**2.4. Numerical Stability and Regularization**\n\nDirect computation of $\\lambda(\\eta_i)$ and $\\lambda'(\\eta_i)$ is numerically unstable for extreme values of $\\eta_i$.\n- For large positive $\\eta_i$, $1-\\Phi(\\eta_i)$ and $\\varphi(\\eta_i)$ both underflow to $0$, making $\\lambda'(\\eta_i)$ an unstable $0/0$ form.\n- For large negative $\\eta_i$, $\\Phi(\\eta_i)$ and $\\varphi(\\eta_i)$ both underflow to $0$, making $\\lambda(\\eta_i)$ an unstable $0/0$ form.\n\nTo resolve this, we compute these quantities in the log domain, which preserves numerical precision:\n$$\\lambda(\\eta) = \\exp(\\log(\\varphi(\\eta)) - \\log(\\Phi(\\eta)))$$\n$$\\lambda'(\\eta) = -\\exp(\\log(\\varphi(\\eta)) - \\log(1-\\Phi(\\eta)))$$\nSpecialized functions, such as `scipy.stats.norm.logpdf`, `logcdf`, and `logsf` (log survival function), are designed to compute these logarithmic quantities accurately even for arguments in the extreme tails of the distribution.\n\nFurthermore, if the data are perfectly or nearly separable, the MLE for $\\beta$ lies at infinity, and the Hessian matrix becomes singular or ill-conditioned. This makes the linear system for $\\Delta\\beta$ unsolvable. To prevent this, we apply a small amount of ridge regularization to the Hessian matrix. We solve $(H - \\epsilon I) \\Delta\\beta = s$, where $I$ is the identity matrix and $\\epsilon$ is a small positive constant (e.g., $10^{-8}$). This ensures the matrix is invertible while having a negligible effect on the solution in well-conditioned cases. This procedure is a form of a Levenberg-Marquardt-type adjustment, stabilizing the numerical inversion.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes a single Newton-Raphson update for probit regression coefficients\n    for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (baseline, well-behaved)\n        (\n            [[1, -1], [1, 0], [1, 1], [1, 2]],\n            [0, 0, 1, 1],\n            [0, 0.5]\n        ),\n        # Case B (extreme tails mixed)\n        (\n            [[1, -10], [1, 0], [1, 10]],\n            [1, 0, 0],\n            [0, 1]\n        ),\n        # Case C (nearly separated)\n        (\n            [[1, -6], [1, -3], [1, 0], [1, 3], [1, 6]],\n            [0, 0, 0, 1, 1],\n            [0, 1]\n        ),\n        # Case D (boundary extreme)\n        (\n            [[1, 50], [1, -50]],\n            [0, 1],\n            [0, 1]\n        )\n    ]\n\n    def probit_newton_step(X, y, beta_old):\n        \"\"\"\n        Performs one numerically robust Newton-Raphson iteration for probit regression.\n\n        - Analyzes instability and mitigation:\n          Direct computation of ratios like phi(eta)/Phi(eta) is unstable for extreme\n          eta where both numerator and denominator underflow to zero. This function\n          mitigates this by operating in the log domain using scipy's specialized\n          functions (logpdf, logcdf, logsf), which retain precision in the tails.\n          The expression exp(log(phi) - log(Phi)) is numerically stable.\n          \n          For (quasi-)separated data, the Hessian can be singular. This is handled\n          by adding a small diagonal ridge term (epsilon * I), which ensures the\n          matrix is invertible without significantly affecting the step in\n          well-conditioned cases.\n        \"\"\"\n        p = X.shape[1]\n        epsilon = 1e-8  # Regularization parameter\n\n        # 1. Compute linear predictor\n        eta = X @ beta_old\n\n        # 2. Compute components for score and Hessian stably in the log domain\n        log_phi = norm.logpdf(eta)\n        log_Phi = norm.logcdf(eta)\n        log_sf = norm.logsf(eta)  # log(1 - Phi(eta))\n\n        # lambda_i = phi(eta_i) / Phi(eta_i)\n        lambda_i = np.exp(log_phi - log_Phi)\n        # lambda_prime_i = -phi(eta_i) / (1 - Phi(eta_i))\n        lambda_prime_i = -np.exp(log_phi - log_sf)\n\n        # 3. Compute score vector s(beta)\n        s_components = np.where(y == 1, lambda_i, lambda_prime_i)\n        score = X.T @ s_components\n\n        # 4. Compute observed Hessian matrix H(beta)\n        d_i_if_y1 = -eta * lambda_i - lambda_i**2\n        d_i_if_y0 = -eta * lambda_prime_i - lambda_prime_i**2\n        d = np.where(y == 1, d_i_if_y1, d_i_if_y0)\n        \n        # H = X^T * diag(d) * X\n        hessian = (X.T * d) @ X\n\n        # 5. Regularize Hessian and solve for the update step delta_beta\n        hessian_reg = hessian - epsilon * np.eye(p)\n        \n        # The Newton-Raphson update is beta_new = beta_old - inv(H) * s.\n        # We solve H * delta_beta = s, then update beta_new = beta_old - delta_beta.\n        delta_beta = np.linalg.solve(hessian_reg, score)\n\n        # 6. Update beta\n        beta_new = beta_old - delta_beta\n        return beta_new\n\n    results = []\n    for X_list, y_list, beta_old_list in test_cases:\n        X = np.array(X_list, dtype=float)\n        y = np.array(y_list, dtype=float)\n        beta_old = np.array(beta_old_list, dtype=float)\n        beta_new = probit_newton_step(X, y, beta_old)\n        results.append(beta_new)\n    \n    # Format the output exactly as specified.\n    rounded_results = [np.round(res, 6) for res in results]\n    str_results = []\n    for res in rounded_results:\n        # Format each sublist as \"[v1,v2,...]\" without spaces after commas\n        str_results.append(f\"[{','.join(map(str, res))}]\")\n    \n    final_output = f\"[{','.join(str_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "After fitting a model, interpreting its coefficients is a crucial next step, but their values are dependent on the scale of the features. This exercise explores how a common preprocessing step—feature standardization—affects the coefficients of a probit model and clarifies their interpretation. You will verify the deterministic mapping between coefficients from unstandardized and standardized models and see how marginal effects provide a stable, interpretable measure of a feature's impact. ",
            "id": "3162301",
            "problem": "Consider a binary-response model under the probit link. Let $Y_i \\in \\{0,1\\}$ be observed outcomes, covariates $X_i \\in \\mathbb{R}^{p}$, and let the latent variable be $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ independent across $i$. The probability of $Y_i = 1$ is $P(Y_i=1 \\mid X_i) = \\Phi(X_i^\\top \\beta)$, where $\\Phi(\\cdot)$ denotes the standard normal cumulative distribution function (CDF). The parameters $\\beta$ are estimated by maximum likelihood estimation (MLE) by maximizing the log-likelihood $\\sum_{i=1}^{n} \\left[ Y_i \\log \\Phi(X_i^\\top \\beta) + (1-Y_i) \\log \\left(1 - \\Phi(X_i^\\top \\beta)\\right) \\right]$. Given a dataset with an intercept term, define feature standardization for each non-intercept feature $j$ as $Z_{ij} = (X_{ij} - \\mu_j)/\\sigma_j$, where $\\mu_j$ is the sample mean and $\\sigma_j$ is the sample standard deviation of feature $j$. The intercept is not standardized.\n\nYour task is to implement a program that:\n- Generates synthetic datasets from the latent-variable model $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,1)$, and sets $Y_i = \\mathbf{1}\\{Y_i^*  0\\}$. The program must use a fixed random seed $42$ for reproducibility.\n- For each dataset, fits a probit regression by MLE on the original features (including an intercept) to obtain $\\hat{\\beta}$, then standardizes the non-intercept features to zero mean and unit variance and refits the probit regression to obtain $\\hat{\\beta}^{\\text{std}}$ on standardized features.\n- Derives and quantitatively verifies the linear reparameterization induced by standardization. Specifically, for non-intercept slopes, the deterministic mapping from $\\hat{\\beta}$ to standardized slopes is $\\hat{\\beta}^{\\text{map}}_j = \\hat{\\beta}_j \\sigma_j$ for each feature $j$, and the standardized intercept is $\\hat{\\beta}^{\\text{map}}_0 = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$. Compute the maximum absolute difference across all coefficients between $\\hat{\\beta}^{\\text{std}}$ and $\\hat{\\beta}^{\\text{map}}$ as a single float.\n- Computes marginal effects at the mean (MEM). The marginal effect for feature $k$ at a point $x$ is $\\frac{\\partial}{\\partial x_k} \\Phi(X^\\top \\beta) = \\phi(X^\\top \\beta)\\,\\beta_k$, where $\\phi(\\cdot)$ denotes the standard normal probability density function (PDF). At the sample means of the original features, let $\\eta_{\\text{mean}} = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$ and compute $\\phi(\\eta_{\\text{mean}})$. For the original (unstandardized) features, compute the MEM per one-unit change: $\\text{MEM}^{\\text{orig}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}_j$, and the MEM per one-standard-deviation change: $\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}_j \\sigma_j$. For standardized features, the MEM per one-unit change equals $\\text{MEM}^{\\text{std}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}^{\\text{std}}_j$. Compute the maximum absolute difference between the vectors $\\text{MEM}^{\\text{std}}$ and $\\text{MEM}^{\\text{orig,1sd}}$ as a single float.\n\nImplement numerical optimization robustly with a smooth objective. Use only the standard normal CDF $\\Phi(\\cdot)$ and PDF $\\phi(\\cdot)$ and a gradient-based optimizer.\n\nTest suite:\n- Case A (single-feature): $n=1000$, one predictor $x_1 \\sim \\mathcal{N}(2,3^2)$, coefficients $(\\beta_0,\\beta_1)=(-0.5,1.5)$.\n- Case B (two features, heterogeneous scales): $n=2000$, $x_1 \\sim \\text{Uniform}[0,10]$, $x_2 \\sim \\mathcal{N}(-1,0.5^2)$, coefficients $(\\beta_0,\\beta_1,\\beta_2)=(0.5,0.2,-2.0)$.\n- Case C (two features, near-saturation regime): $n=1500$, $x_1 \\sim \\mathcal{N}(0,1^2)$, $x_2 \\sim \\mathcal{N}(0,1^2)$, coefficients $(\\beta_0,\\beta_1,\\beta_2)=(3.0,-0.5,0.5)$.\n\nFor each case, output two floats:\n- The maximum absolute coefficient mapping error: $\\max_{j \\in \\{0,1,\\dots,p\\}} \\left| \\hat{\\beta}^{\\text{std}}_j - \\hat{\\beta}^{\\text{map}}_j \\right|$.\n- The maximum absolute MEM equivalence error: $\\max_{j \\in \\{1,\\dots,p\\}} \\left| \\text{MEM}^{\\text{std}}_j - \\text{MEM}^{\\text{orig,1sd}}_j \\right|$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of lists for the three cases, with each inner list $[\\text{mapping\\_error},\\text{mem\\_error}]$ rounded to six decimal places, in the exact format\n$[[v_{A1},v_{A2}],[v_{B1},v_{B2}],[v_{C1},v_{C2}]]$.",
            "solution": "The problem requires the implementation and verification of key properties of the probit regression model, specifically concerning the effect of feature standardization on estimated coefficients and marginal effects. The solution involves data generation, model estimation via maximum likelihood, and quantitative comparison of theoretically equivalent quantities.\n\nFirst, we define the probit model. The probability of a positive outcome $Y_i=1$ for an observation $i$ with a vector of covariates $X_i \\in \\mathbb{R}^{p+1}$ (including an intercept term $X_{i0}=1$) is given by the standard normal cumulative distribution function (CDF), $\\Phi(\\cdot)$, applied to a linear combination of the covariates:\n$$\nP(Y_i=1 \\mid X_i) = \\Phi(X_i^\\top \\beta)\n$$\nHere, $\\beta \\in \\mathbb{R}^{p+1}$ is the vector of coefficients to be estimated. This model arises from a latent variable formulation $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ are independent and identically distributed standard normal errors, and the observed outcome is $Y_i = \\mathbf{1}\\{Y_i^*  0\\}$.\n\nThe coefficients $\\beta$ are estimated by maximizing the log-likelihood function. For a dataset of $n$ observations $\\{(X_i, Y_i)\\}_{i=1}^n$, the log-likelihood is:\n$$\n\\mathcal{L}(\\beta) = \\sum_{i=1}^{n} \\left[ Y_i \\log \\Phi(X_i^\\top \\beta) + (1-Y_i) \\log \\left(1 - \\Phi(X_i^\\top \\beta)\\right) \\right]\n$$\nTo perform this maximization using a gradient-based numerical optimizer, we need the gradient of $\\mathcal{L}(\\beta)$ with respect to $\\beta$. Let $\\eta_i = X_i^\\top \\beta$. The gradient for a single observation $i$ is:\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\beta} = \\left( \\frac{Y_i}{\\Phi(\\eta_i)} - \\frac{1-Y_i}{1 - \\Phi(\\eta_i)} \\right) \\phi(\\eta_i) X_i\n$$\nwhere $\\phi(\\cdot)$ is the standard normal probability density function (PDF), the derivative of $\\Phi(\\cdot)$. The total gradient is the sum over all observations: $\\nabla_{\\beta} \\mathcal{L} = \\sum_{i=1}^{n} \\nabla_{\\beta} \\ell_i(\\beta)$. We will minimize the negative of the log-likelihood function, $-\\mathcal{L}(\\beta)$, using the negative of this gradient.\n\nNext, we analyze the effect of feature standardization. For each non-intercept feature $j \\in \\{1, \\dots, p\\}$, we define its standardized version $Z_{ij} = (X_{ij} - \\mu_j) / \\sigma_j$, where $\\mu_j$ and $\\sigma_j$ are the sample mean and standard deviation of feature $j$, respectively. The intercept term remains $Z_{i0} = X_{i0} = 1$. The original linear predictor $X_i^\\top \\beta$ can be re-expressed in terms of the standardized features. By substituting $X_{ij} = Z_{ij}\\sigma_j + \\mu_j$, we get:\n$$\nX_i^\\top \\beta = \\beta_0 + \\sum_{j=1}^{p} X_{ij}\\beta_j = \\beta_0 + \\sum_{j=1}^{p} (Z_{ij}\\sigma_j + \\mu_j)\\beta_j\n$$\n$$\nX_i^\\top \\beta = \\left(\\beta_0 + \\sum_{j=1}^{p} \\beta_j\\mu_j\\right) + \\sum_{j=1}^{p} Z_{ij}(\\beta_j\\sigma_j)\n$$\nThis expression is in the form of a linear predictor for the standardized features, $Z_i^\\top \\beta^{\\text{std}}$, where the new coefficients $\\beta^{\\text{std}}$ are deterministically related to the original coefficients $\\beta$:\n$$\n\\beta^{\\text{std}}_0 = \\beta_0 + \\sum_{j=1}^{p} \\beta_j\\mu_j\n$$\n$$\n\\beta^{\\text{std}}_j = \\beta_j\\sigma_j \\quad \\text{for } j \\in \\{1, \\dots, p\\}\n$$\nThe problem requires us to verify this reparameterization. We first estimate $\\hat{\\beta}$ by fitting the model on the original features $X$. Then, we calculate the theoretically mapped standardized coefficients, $\\hat{\\beta}^{\\text{map}}$, using the formulas above with $\\hat{\\beta}$, $\\mu_j$, and $\\sigma_j$. We also estimate $\\hat{\\beta}^{\\text{std}}$ by fitting the model directly on the standardized features $Z$. A small difference between $\\hat{\\beta}^{\\text{map}}$ and $\\hat{\\beta}^{\\text{std}}$ will numerically validate the reparameterization theory, with any discrepancy attributable to the optimizer's tolerance. The mapping error is computed as $\\max_j |\\hat{\\beta}^{\\text{std}}_j - \\hat{\\beta}^{\\text{map}}_j|$.\n\nFinally, we examine the marginal effects, which measure the change in the probability of success for a change in a feature. The marginal effect of feature $k$ is:\n$$\n\\text{ME}_k(X) = \\frac{\\partial P(Y=1 \\mid X)}{\\partial X_k} = \\frac{\\partial}{\\partial X_k} \\Phi(X^\\top \\beta) = \\phi(X^\\top \\beta) \\beta_k\n$$\nThis effect depends on the value of all covariates $X$. A common practice is to evaluate it at the sample means of the features, known as the marginal effect at the mean (MEM). Let $\\eta_{\\text{mean}} = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$. Note that this is precisely the formula for the mapped intercept, $\\hat{\\beta}^{\\text{map}}_0$.\nFor the original model, the MEM for a one-unit change in $X_j$ is $\\text{MEM}^{\\text{orig}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j$. The MEM for a one-standard-deviation change in $X_j$ is:\n$$\n\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j\\sigma_j\n$$\nFor the standardized model, a one-unit change in the standardized feature $Z_j$ is equivalent to a one-standard-deviation change in the original feature $X_j$. The MEM for the standardized model, evaluated at the means (where $\\bar{Z}_j=0$ for $j \\ge 1$), is:\n$$\n\\text{MEM}^{\\text{std}}_j = \\phi(\\hat{\\beta}^{\\text{std}}_0)\\hat{\\beta}^{\\text{std}}_j\n$$\nSince $\\eta_{\\text{mean}} = \\hat{\\beta}^{\\text{map}}_0 \\approx \\hat{\\beta}^{\\text{std}}_0$, we can consistently use $\\phi(\\eta_{\\text{mean}})$ for both calculations as instructed. The task is to compare $\\text{MEM}^{\\text{orig,1sd}}_j$ with $\\text{MEM}^{\\text{std}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{std}}_j$. The equivalence follows directly from the coefficient mapping:\n$$\n\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j\\sigma_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{map}}_j \\approx \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{std}}_j = \\text{MEM}^{\\text{std}}_j\n$$\nThe MEM equivalence error, $\\max_{j \\ge 1} | \\text{MEM}^{\\text{std}}_j - \\text{MEM}^{\\text{orig,1sd}}_j |$, should therefore be very small, reflecting the accuracy of the coefficient mapping.\n\nThe computational procedure involves:\n1. Generating synthetic data for each test case from the latent variable model using a fixed random seed for reproducibility.\n2. Fitting the probit model on the original data to obtain $\\hat{\\beta}$.\n3. Computing feature means $\\mu_j$ and standard deviations $\\sigma_j$, and creating the standardized feature matrix $Z$.\n4. Fitting the probit model on the standardized data to obtain $\\hat{\\beta}^{\\text{std}}$.\n5. Calculating $\\hat{\\beta}^{\\text{map}}$ from $\\hat{\\beta}$ and computing the coefficient mapping error.\n6. Calculating the marginal effects $\\text{MEM}^{\\text{orig,1sd}}$ and $\\text{MEM}^{\\text{std}}$ and computing the MEM equivalence error.\nThis provides a complete verification of the theoretical relationships.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n\n    def fit_probit(X, y):\n        \"\"\"Fits a probit model using MLE.\"\"\"\n        \n        def neg_log_likelihood(beta, X, y):\n            \"\"\"Computes the negative log-likelihood for probit regression.\"\"\"\n            eta = X @ beta\n            # Use log-CDF and log-SF for numerical stability\n            log_phi = norm.logcdf(eta)\n            log_one_minus_phi = norm.logsf(eta)\n            \n            # log_phi can be -inf if eta is very small; log_one_minus_phi can be -inf if eta is very large.\n            # Replace -inf with a large negative number to prevent nan in sum.\n            log_phi[np.isneginf(log_phi)] = -1e9\n            log_one_minus_phi[np.isneginf(log_one_minus_phi)] = -1e9\n\n            log_L = np.sum(y * log_phi + (1 - y) * log_one_minus_phi)\n            return -log_L\n\n        def grad_neg_log_likelihood(beta, X, y):\n            \"\"\"Computes the gradient of the negative log-likelihood.\"\"\"\n            eta = X @ beta\n            pdf_val = norm.pdf(eta)\n            cdf_val = norm.cdf(eta)\n            \n            # Handle potential division by zero\n            # cdf_val can be 0 or 1 at the extremes of eta\n            # Hazard rate: lambda(eta) = pdf(eta) / cdf(eta)\n            # Reverse hazard rate: pdf(eta) / (1-cdf(eta))\n            # The limits are well-behaved, but numerically can be tricky\n            # Use a small epsilon to avoid 0 in denominator\n            epsilon = 1e-12\n            \n            term1 = y * (pdf_val / (cdf_val + epsilon))\n            term2 = (1 - y) * (pdf_val / (1 - cdf_val + epsilon))\n            \n            # The term inside the sum is (term1 - term2)\n            # The gradient for coeff beta_j is sum_i(X_ij * (term1_i - term2_i))\n            # which can be computed with a matrix-vector product\n            grad = X.T @ (term1 - term2)\n            return -grad\n\n        initial_guess = np.zeros(X.shape[1])\n        result = minimize(\n            fun=neg_log_likelihood,\n            x0=initial_guess,\n            args=(X, y),\n            method='BFGS',\n            jac=grad_neg_log_likelihood,\n            options={'gtol': 1e-7}\n        )\n        return result.x\n\n    def analyze_case(n, feature_specs, true_beta, rng):\n        \"\"\"Generates data, fits models, and computes errors for one case.\"\"\"\n        # 1. Generate data\n        p_features = len(feature_specs)\n        X_no_intercept = np.zeros((n, p_features))\n\n        for j, spec in enumerate(feature_specs):\n            dist_type, p1, p2 = spec\n            if dist_type == 'norm':\n                X_no_intercept[:, j] = rng.normal(loc=p1, scale=p2, size=n)\n            elif dist_type == 'uniform':\n                X_no_intercept[:, j] = rng.uniform(low=p1, high=p2, size=n)\n        \n        X_orig = np.c_[np.ones(n), X_no_intercept]\n        \n        epsilon = rng.normal(0, 1, n)\n        y_star = X_orig @ np.array(true_beta) + epsilon\n        y = (y_star  0).astype(int)\n\n        # 2. Fit model on original features\n        beta_hat_orig = fit_probit(X_orig, y)\n\n        # 3. Standardize features\n        mu = np.mean(X_no_intercept, axis=0)\n        sigma = np.std(X_no_intercept, axis=0)\n        X_std_no_intercept = (X_no_intercept - mu) / sigma\n        X_std = np.c_[np.ones(n), X_std_no_intercept]\n\n        # 4. Fit model on standardized features\n        beta_hat_std = fit_probit(X_std, y)\n\n        # 5. Compute reparameterization error\n        beta_hat_map = np.zeros_like(beta_hat_orig)\n        beta_hat_map[0] = beta_hat_orig[0] + np.dot(beta_hat_orig[1:], mu)\n        beta_hat_map[1:] = beta_hat_orig[1:] * sigma\n        mapping_error = np.max(np.abs(beta_hat_std - beta_hat_map))\n\n        # 6. Compute marginal effects error\n        eta_mean = beta_hat_orig[0] + np.dot(beta_hat_orig[1:], mu)\n        phi_eta_mean = norm.pdf(eta_mean)\n        \n        mem_orig_1sd = phi_eta_mean * beta_hat_orig[1:] * sigma\n        mem_std = phi_eta_mean * beta_hat_std[1:]\n        mem_error = np.max(np.abs(mem_std - mem_orig_1sd))\n        \n        return [round(mapping_error, 6), round(mem_error, 6)]\n\n    # Fixed random seed for reproducibility of the entire process\n    rng = np.random.default_rng(42)\n\n    test_cases = [\n        # Case A: n=1000, 1 feature N(2, 3^2), beta=(-0.5, 1.5)\n        (1000, [('norm', 2, 3)], [-0.5, 1.5]),\n        # Case B: n=2000, U[0,10], N(-1, 0.5^2), beta=(0.5, 0.2, -2.0)\n        (2000, [('uniform', 0, 10), ('norm', -1, 0.5)], [0.5, 0.2, -2.0]),\n        # Case C: n=1500, N(0,1), N(0,1), beta=(3.0, -0.5, 0.5)\n        (1500, [('norm', 0, 1), ('norm', 0, 1)], [3.0, -0.5, 0.5]),\n    ]\n\n    all_results = []\n    for n, specs, beta in test_cases:\n        case_results = analyze_case(n, specs, beta, rng)\n        all_results.append(case_results)\n\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Probit regression is one of several choices for binary classification, with logit regression being a popular alternative, each using a different link function. This raises an important question: how does the choice of link function impact a model's discriminative power? This practice demonstrates a profound and fundamental principle of model evaluation: for a given linear predictor, the rank-ordering of scores—and thus the Receiver Operating Characteristic (ROC) curve and its Area Under the Curve (AUC)—is invariant under any strictly increasing link function. ",
            "id": "3162345",
            "problem": "You are given a binary response setting with a linear score function $ \\eta = x^\\top \\beta + b_0 $, where $ x \\in \\mathbb{R}^d $ is an input vector, $ \\beta \\in \\mathbb{R}^d $ is a fixed coefficient vector, and $ b_0 \\in \\mathbb{R} $ is an intercept. For a binary class label $ Y \\in \\{0,1\\} $, one computes class scores by applying a strictly increasing link function $ g^{-1} $ pointwise to $ \\eta $. Consider the following three strictly increasing link functions: the probit inverse link $ g^{-1}_{\\text{probit}}(\\eta) = \\Phi(\\eta) $ where $ \\Phi $ is the cumulative distribution function of the standard normal distribution, the logit inverse link $ g^{-1}_{\\text{logit}}(\\eta) = \\Lambda(\\eta) $ where $ \\Lambda(\\eta) = \\frac{1}{1 + e^{-\\eta}} $, and the complementary log-log inverse link $ g^{-1}_{\\text{cloglog}}(\\eta) = 1 - \\exp(-\\exp(\\eta)) $. The Receiver Operating Characteristic (ROC) curve is defined from the family of indicators obtained by thresholding a real-valued score and plotting the true positive rate versus the false positive rate across all possible thresholds. The Area Under the ROC Curve (AUC) is the area under this ROC curve. By fundamental definitions, for any scores $ s \\in \\mathbb{R}^n $ and labels $ y \\in \\{0,1\\}^n $, the ROC curve depends only on the ordering of the scores, and the AUC equals the probability that a uniformly drawn positive instance has a strictly larger score than a uniformly drawn negative instance, plus one-half of the probability of a tie. Starting from these definitions, you must demonstrate computationally that applying any of the strictly increasing link functions $ \\Phi $, $ \\Lambda $, and $ 1 - \\exp(-\\exp(\\cdot)) $ to the same linear scores $ \\eta $ yields identical ROC curves and hence identical AUC values.\n\nYour program must implement the following steps for each test case:\n1. Generate a design matrix $ X \\in \\mathbb{R}^{n \\times d} $ according to the prescribed rule and a label vector $ y \\in \\{0,1\\}^n $ according to the prescribed rule.\n2. Compute the linear score $ \\eta = X \\beta + b_0 \\mathbf{1} $ where $ \\mathbf{1} $ is the vector of ones of length $ n $.\n3. Compute three score vectors by applying, pointwise to $ \\eta $, the probit inverse link $ \\Phi(\\eta) $, the logit inverse link $ \\Lambda(\\eta) $, and the complementary log-log inverse link $ 1 - \\exp(-\\exp(\\eta)) $.\n4. Compute the AUC of each of the three score vectors with respect to $ y $ using the equivalence between AUC and the Wilcoxon–Mann–Whitney statistic, or any other implementation that is derived from the fundamental ROC and AUC definitions and correctly accounts for ties.\n5. For each test case, output a boolean indicating whether the three AUC values are all equal within an absolute tolerance of $ 10^{-15} $.\n\nUse the following five test cases. All random number generation must be reproducible using the specified seeds. For integer-valued matrices, draw each entry independently and uniformly from integers in the closed interval $ [-2,2] $.\n\n- Test case $ 1 $ (general case):\n  - Seed $ 0 $, $ n = 200 $, $ d = 3 $, $ \\beta = (1.0, -0.5, 0.8) $, $ b_0 = 0.2 $.\n  - Generate $ X $ with independent standard normal entries.\n  - Generate labels using a Bernoulli mechanism with success probability $ \\Phi(\\eta) $ applied pointwise.\n\n- Test case $ 2 $ (class imbalance via intercept):\n  - Seed $ 1 $, $ n = 300 $, $ d = 4 $, $ \\beta = (1.0, 0.5, -1.2, 0.3) $, $ b_0 = -2.0 $.\n  - Generate $ X $ with independent standard normal entries.\n  - Generate labels using a Bernoulli mechanism with success probability $ \\Phi(\\eta) $ applied pointwise.\n\n- Test case $ 3 $ (near-separable deterministic labels):\n  - Seed $ 2 $, $ n = 250 $, $ d = 2 $, $ \\beta = (3.0, -3.0) $, $ b_0 = 0.0 $.\n  - Generate $ X $ with independent standard normal entries.\n  - Generate labels deterministically as $ y_i = \\mathbf{1}\\{\\eta_i  0\\} $ for $ i = 1,\\dots,n $.\n\n- Test case $ 4 $ (all scores equal, maximal ties):\n  - No randomness, $ n = 100 $, $ d = 1 $, $ \\beta = (0.0) $, $ b_0 = 0.0 $.\n  - Set $ X $ to be the all-zeros matrix, so $ \\eta $ is identically $ 0 $.\n  - Set labels deterministically to alternate: $ y_i = 1 $ if $ i $ is odd and $ y_i = 0 $ if $ i $ is even, for $ i = 1,\\dots,n $.\n\n- Test case $ 5 $ (discrete features inducing ties):\n  - Seed $ 3 $, $ n = 120 $, $ d = 2 $, $ \\beta = (1.0, 2.0) $, $ b_0 = -1.0 $.\n  - Generate $ X $ with independent integer entries uniformly in $ [-2,2] $.\n  - Generate labels using a Bernoulli mechanism with success probability $ \\Phi(\\eta) $ applied pointwise.\n\nYour program must produce a single line of output containing a list of five booleans corresponding to the five test cases, in order. For each test case, the boolean is $ \\text{True} $ if and only if the three AUC values computed from the probit, logit, and complementary log-log transformed scores are all equal within absolute tolerance $ 10^{-15} $, and $ \\text{False} $ otherwise. The exact output format must be a single line with the results as a comma-separated list enclosed in square brackets, for example $ [\\text{True},\\text{True},\\text{True},\\text{True},\\text{True}] $.",
            "solution": "The problem requires a computational demonstration of a fundamental property of the Receiver Operating Characteristic (ROC) curve and its corresponding Area Under the Curve (AUC). Specifically, it asserts that applying any strictly increasing transformation to a set of scores does not change the resulting ROC curve or the AUC. The solution will verify this principle for three specific, strictly increasing inverse link functions used in statistical modeling: the probit, logit, and complementary log-log functions.\n\nThe core principle is rooted in the definition of the ROC curve. An ROC curve is a two-dimensional plot of the True Positive Rate (TPR) against the False Positive Rate (FPR) as a classification threshold is varied. For a given score vector $s$ and a threshold $\\tau$, the rates are defined as:\n$$ \\text{TPR}(\\tau) = P(s  \\tau | Y=1) $$\n$$ \\text{FPR}(\\tau) = P(s  \\tau | Y=0) $$\nwhere $Y$ is the true binary label. The ROC curve is the set of all\n$(\\text{FPR}(\\tau), \\text{TPR}(\\tau))$ pairs over all possible values of $\\tau$.\n\nLet $f: \\mathbb{R} \\to \\mathbb{R}$ be any strictly increasing function. If we transform the scores $s$ into a new set of scores $s' = f(s)$, the ordering of the scores is preserved. That is, for any two scores $s_i$ and $s_j$, $s_i  s_j$ if and only if $f(s_i)  f(s_j)$.\nLet's consider the classification rule for the transformed scores, $s'  \\tau'$. This is equivalent to $f(s)  \\tau'$. Since $f$ is strictly increasing, it has a strictly increasing inverse $f^{-1}$, and the inequality is equivalent to $s  f^{-1}(\\tau')$. If we set $\\tau = f^{-1}(\\tau')$, the set of instances classified as positive is identical for the score-threshold pair $(s, \\tau)$ and the pair $(s', \\tau')$. As $\\tau'$ sweeps over the range of $s'$, the corresponding $\\tau$ sweeps over the range of $s$. Consequently, the set of $(\\text{FPR}, \\text{TPR})$ pairs generated is identical for both $s$ and $s'$. This means the ROC curves are identical, and therefore their areas (the AUC values) must also be identical.\n\nThe three link functions provided, $g^{-1}_{\\text{probit}}(\\eta) = \\Phi(\\eta)$, $g^{-1}_{\\text{logit}}(\\eta) = \\Lambda(\\eta)$, and $g^{-1}_{\\text{cloglog}}(\\eta) = 1 - \\exp(-\\exp(\\eta))$, are all strictly increasing functions of the linear score $\\eta$. Thus, the AUC computed from the scores generated by each of these functions should be identical.\n\nTo compute the AUC, we use its equivalence with the Wilcoxon-Mann-Whitney U statistic. The AUC is the probability that a score drawn uniformly at random from the positive class is strictly greater than a score drawn uniformly at random from the negative class, with a correction for ties. For a sample with $n_1$ positive instances (with scores $s_{1,i}$) and $n_0$ negative instances (with scores $s_{0,j}$), the AUC is estimated as:\n$$ \\text{AUC} = \\frac{1}{n_1 n_0} \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_0} \\left( \\mathbb{I}(s_{1,i}  s_{0,j}) + \\frac{1}{2} \\mathbb{I}(s_{1,i} = s_{0,j}) \\right) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. This formula correctly handles ties, which is crucial for test cases with discrete features or constant scores. If either the positive or negative class is empty ($n_1=0$ or $n_0=0$), the AUC is conventionally defined as $0.5$.\n\nThe computational procedure for each of the $5$ test cases is as follows:\n$1$. Set the random seed (if specified) for reproducibility. Using the given parameters ($n, d, \\beta, b_0$), generate the design matrix $X$ and the label vector $y$ according to the specified rules.\n$2$. Compute the linear score vector $\\eta = X \\beta + b_0 \\mathbf{1}$.\n$3$. Apply the three inverse link functions pointwise to $\\eta$ to obtain three score vectors: $s_{\\text{probit}} = \\Phi(\\eta)$, $s_{\\text{logit}} = \\Lambda(\\eta)$, and $s_{\\text{cloglog}} = 1 - \\exp(-\\exp(\\eta))$.\n$4$. For each of the three score vectors, calculate the AUC with respect to the label vector $y$ using the Wilcoxon-Mann-Whitney statistic formulation described above. This is implemented efficiently using vectorized matrix operations.\n$5$. Compare the three computed AUC values. A test case passes if all three values are equal within an absolute tolerance of $10^{-15}$, i.e., if $\\max(\\text{AUC}_{\\text{probit}}, \\text{AUC}_{\\text{logit}}, \\text{AUC}_{\\text{cloglog}}) - \\min(\\text{AUC}_{\\text{probit}}, \\text{AUC}_{\\text{logit}}, \\text{AUC}_{\\text{cloglog}})  10^{-15}$. The result is stored as a boolean value.\n\nThis procedure will be repeated for all $5$ test cases, and the final output will be a list of these boolean results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import expit\n\ndef calculate_auc(scores, labels):\n    \"\"\"\n    Calculates the Area Under the ROC Curve (AUC) using the \n    Wilcoxon-Mann-Whitney U statistic formulation. This handles ties correctly.\n    \"\"\"\n    pos_indices = np.where(labels == 1)[0]\n    neg_indices = np.where(labels == 0)[0]\n    \n    n_pos = len(pos_indices)\n    n_neg = len(neg_indices)\n    \n    # If there are no positive or no negative samples, AUC is undefined.\n    # A common convention is to return 0.5.\n    if n_pos == 0 or n_neg == 0:\n        return 0.5\n        \n    pos_scores = scores[pos_indices]\n    neg_scores = scores[neg_indices]\n    \n    # Use broadcasting to efficiently compare all pos vs. neg scores.\n    # Reshape pos_scores to a column vector for broadcasting.\n    pos_scores_reshaped = pos_scores.reshape(-1, 1)\n    \n    # Comparisons result in boolean matrices of shape (n_pos, n_neg).\n    gt_matrix = pos_scores_reshaped  neg_scores\n    eq_matrix = pos_scores_reshaped == neg_scores\n    \n    # The statistic is the sum of pairs where pos  neg + 0.5 * sum of ties.\n    mann_whitney_stat = np.sum(gt_matrix) + 0.5 * np.sum(eq_matrix)\n    \n    return mann_whitney_stat / (n_pos * n_neg)\n\ndef solve():\n    \"\"\"\n    Runs all test cases to verify the AUC invariance property.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general case)\n        {'seed': 0, 'n': 200, 'd': 3, 'beta': np.array([1.0, -0.5, 0.8]), 'b0': 0.2, 'X_type': 'normal', 'y_type': 'bernoulli'},\n        # Test case 2 (class imbalance via intercept)\n        {'seed': 1, 'n': 300, 'd': 4, 'beta': np.array([1.0, 0.5, -1.2, 0.3]), 'b0': -2.0, 'X_type': 'normal', 'y_type': 'bernoulli'},\n        # Test case 3 (near-separable deterministic labels)\n        {'seed': 2, 'n': 250, 'd': 2, 'beta': np.array([3.0, -3.0]), 'b0': 0.0, 'X_type': 'normal', 'y_type': 'deterministic'},\n        # Test case 4 (all scores equal, maximal ties)\n        {'seed': None, 'n': 100, 'd': 1, 'beta': np.array([0.0]), 'b0': 0.0, 'X_type': 'zeros', 'y_type': 'alternating'},\n        # Test case 5 (discrete features inducing ties)\n        {'seed': 3, 'n': 120, 'd': 2, 'beta': np.array([1.0, 2.0]), 'b0': -1.0, 'X_type': 'integer', 'y_type': 'bernoulli'}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Generate data\n        if case['seed'] is not None:\n            np.random.seed(case['seed'])\n        \n        n, d = case['n'], case['d']\n        beta, b0 = case['beta'], case['b0']\n        \n        if case['X_type'] == 'normal':\n            X = np.random.randn(n, d)\n        elif case['X_type'] == 'integer':\n            X = np.random.randint(-2, 3, size=(n, d))\n        elif case['X_type'] == 'zeros':\n            X = np.zeros((n, d))\n\n        # 2. Compute linear scores\n        eta = X @ beta + b0\n\n        # Generate labels\n        if case['y_type'] == 'bernoulli':\n            p = norm.cdf(eta) # Data generating process uses probit\n            y = np.random.binomial(1, p)\n        elif case['y_type'] == 'deterministic':\n            y = (eta  0).astype(int)\n        elif case['y_type'] == 'alternating':\n            # y_i = 1 if i is odd, 0 if even, for i=1..n\n            y = np.arange(1, n + 1) % 2\n\n        # 3. Compute scores using different link functions\n        scores_probit = norm.cdf(eta)\n        scores_logit = expit(eta) # scipy.special.expit is the logistic sigmoid\n        scores_cloglog = 1.0 - np.exp(-np.exp(eta))\n\n        # 4. Compute AUC for each score vector\n        auc_probit = calculate_auc(scores_probit, y)\n        auc_logit = calculate_auc(scores_logit, y)\n        auc_cloglog = calculate_auc(scores_cloglog, y)\n        \n        # 5. Check for equality within tolerance\n        aucs = np.array([auc_probit, auc_logit, auc_cloglog])\n        tolerance = 1e-15\n        is_equal = (np.max(aucs) - np.min(aucs))  tolerance\n        results.append(is_equal)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}