{
    "hands_on_practices": [
        {
            "introduction": "The art of time series modeling begins with careful diagnosis. Before fitting a complex model, we must 'listen' to the data by examining its correlation structures. This exercise  challenges you to interpret the classic signatures of seasonality and non-stationarity in the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF), guiding you to make a crucial decision in building a Seasonal Autoregressive Integrated Moving Average (SARIMA) model.",
            "id": "3187708",
            "problem": "A monthly time series $\\{x_t\\}_{t=1}^{120}$ for a retail product exhibits regular seasonality with period $s=12$. To assess the appropriate seasonal treatment before fitting a Seasonal Autoregressive Integrated Moving Average (SARIMA) model, a practitioner computes the sample autocorrelation function $\\hat{\\rho}(k)$ and the sample partial autocorrelation function $\\widehat{\\phi}(k)$ for the undifferenced series $\\{x_t\\}$. The following features are observed:\n\n- At seasonal lags, the autocorrelation is large and decays slowly: $\\hat{\\rho}(12) \\approx 0.82$, $\\hat{\\rho}(24) \\approx 0.65$, $\\hat{\\rho}(36) \\approx 0.51$; the sequence of peaks at $k \\in \\{12,24,36,\\dots\\}$ persists above typical significance thresholds and decreases only gradually.\n- At nonseasonal lags $k \\in \\{1,2,3,4,5\\}$, autocorrelations are also substantial but display gradual decay rather than a sharp cutoff.\n- The partial autocorrelation function shows notable spikes at seasonal lags: $\\widehat{\\phi}(12) \\approx 0.80$, $\\widehat{\\phi}(24) \\approx 0.58$, $\\widehat{\\phi}(36) \\approx 0.43$, while intermediate lags between multiples of $s$ are comparatively small; there is no single sharp cutoff at $k=12$.\n\nUsing the foundational definitions of weak stationarity (constant mean, finite variance, and autocovariance depending only on lag), the population autocorrelation function, and the partial autocorrelation function, infer the most appropriate next modeling step regarding the seasonal component. Which option below is most consistent with these diagnostics and most likely to yield a seasonally stationary series suitable for subsequent nonseasonal identification?\n\nA. Apply seasonal differencing of order $D=1$ with period $s=12$, that is, transform to $\\nabla_{12} x_t = x_t - x_{t-12}$ before further modeling.\n\nB. Keep $\\{x_t\\}$ undifferenced seasonally and introduce a seasonal autoregressive term of order $P=1$ at period $s=12$ within a SARIMA structure.\n\nC. Apply a nonseasonal difference of order $d=1$ (no seasonal differencing), and include a seasonal autoregressive term with $P=1$ at period $s=12$.\n\nD. Keep $\\{x_t\\}$ undifferenced seasonally and add a seasonal moving average term of order $Q=1$ at period $s=12$ within a SARIMA structure.",
            "solution": "The problem statement will be validated by first extracting the given information and then assessing its scientific validity and structural integrity.\n\n### Step 1: Extract Givens\n\n-   The time series is denoted as $\\{x_t\\}_{t=1}^{120}$.\n-   The data is monthly, implying a seasonal period of $s=12$.\n-   The goal is to determine the appropriate seasonal treatment for fitting a Seasonal Autoregressive Integrated Moving Average (SARIMA) model.\n-   The analysis is performed on the undifferenced series $\\{x_t\\}$.\n-   Diagnostics from the sample autocorrelation function (ACF), $\\hat{\\rho}(k)$:\n    -   At seasonal lags $k=12, 24, 36, \\dots$, the ACF is large and decays slowly.\n    -   Specific values are provided: $\\hat{\\rho}(12) \\approx 0.82$, $\\hat{\\rho}(24) \\approx 0.65$, $\\hat{\\rho}(36) \\approx 0.51$.\n    -   At nonseasonal lags $k \\in \\{1,2,3,4,5\\}$, the ACF is substantial and shows gradual decay.\n-   Diagnostics from the sample partial autocorrelation function (PACF), $\\widehat{\\phi}(k)$:\n    -   At seasonal lags, there are notable spikes: $\\widehat{\\phi}(12) \\approx 0.80$, $\\widehat{\\phi}(24) \\approx 0.58$, $\\widehat{\\phi}(36) \\approx 0.43$.\n    -   Intermediate lags between multiples of $s=12$ are comparatively small.\n    -   There is no single sharp cutoff at $k=12$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem is set within the well-established framework of Box-Jenkins methodology for time series analysis. The concepts of ACF, PACF, stationarity, differencing, and SARIMA models are fundamental to modern statistics and econometrics. The numerical values given for $\\hat{\\rho}(k)$ and $\\widehat{\\phi}(k)$ are plausible for a series with strong seasonality and non-stationarity. The problem is scientifically sound.\n-   **Well-Posed:** The problem provides specific, standard diagnostic information (the behavior of the ACF and PACF) and asks for the corresponding standard modeling conclusion. This is a classic pattern-recognition task in applied time series analysis, which is designed to lead to a specific, well-defined action.\n-   **Objective:** The description of the ACF and PACF uses standard terminology (\"decays slowly\", \"notable spikes\", \"sharp cutoff\") and supports these qualitative statements with quantitative values. The question is objective and free of subjective claims.\n-   **Completeness and Consistency:** The information is sufficient to make a reasoned decision about the seasonal component. The key diagnostic for seasonal non-stationarity—a slowly decaying ACF at seasonal lags—is explicitly stated and quantified. This observation is consistent with the report of a large spike in the PACF at the first seasonal lag, which together form the classic signature of a seasonal unit root.\n-   **No other flaws:** The problem is neither trivial nor ill-posed. It represents a realistic scenario encountered in practice.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is a standard and well-posed question in time series analysis. I will proceed to derive the solution.\n\n### Derivation and Option Analysis\n\nThe primary goal is to identify the appropriate transformation to make the series seasonally stationary. A time series is weakly stationary if its mean, variance, and autocovariance are independent of time. Non-stationarity in a time series often manifests as a trend or a seasonal pattern that does not have a constant mean. In the context of SARIMA models, non-stationarity is typically handled by differencing the series.\n\nWe are asked to identify the appropriate step for the seasonal component, so we must focus on the behavior of the ACF and PACF at seasonal lags, which are multiples of the seasonal period $s=12$ (i.e., lags $k=12, 24, 36, \\dots$).\n\n1.  **Analysis of the Autocorrelation Function (ACF):**\n    The problem states that the sample ACF, $\\hat{\\rho}(k)$, at seasonal lags is \"large and decays slowly\". Numerically, $\\hat{\\rho}(12) \\approx 0.82$, $\\hat{\\rho}(24) \\approx 0.65$, and $\\hat{\\rho}(36) \\approx 0.51$. This slow decay of the ACF is the canonical indicator of non-stationarity. A stationary autoregressive (AR) process also has a decaying ACF, but the decay is typically exponential and faster. The very slow, almost linear decay seen here is a strong sign of a unit root in the process. Since this behavior is observed at the seasonal lags, it indicates a **seasonal unit root**. A process with a seasonal unit root is seasonally non-stationary. The standard treatment for a seasonal unit root is to apply a seasonal difference of order one, which is the transformation $y_t = x_t - x_{t-s}$. For this problem, this corresponds to $\\nabla_{12} x_t = x_t - x_{t-12}$.\n\n2.  **Analysis of the Partial Autocorrelation Function (PACF):**\n    The problem states that the sample PACF, $\\widehat{\\phi}(k)$, shows \"notable spikes at seasonal lags,\" with $\\widehat{\\phi}(12) \\approx 0.80$. A large, significant spike at the first seasonal lag ($k=s$) in the PACF, combined with the slowly decaying seasonal ACF, reinforces the diagnosis of a seasonal unit root. For a non-seasonal random walk, $x_t = x_{t-1} + \\epsilon_t$, the ACF decays slowly from a value near $1$, and the PACF has a single spike at lag $1$. By analogy, for a purely seasonal process with a unit root, $(1-B^{12})x_t = \\epsilon_t$, the ACF at lags $12, 24, \\dots$ would decay slowly, and the PACF would have a significant spike at lag $12$. The given diagnostics align with this pattern. The fact that spikes at lags $24$ and $36$ are also present and decay suggests that after one seasonal differencing, a seasonal AR process might remain. However, the most urgent and necessary step, dictated by the strong evidence of non-stationarity, is to perform the seasonal differencing.\n\n3.  **Conclusion on Modeling Strategy:**\n    The combination of a slowly decaying ACF at seasonal lags and a large PACF spike at the first seasonal lag strongly indicates the presence of a seasonal unit root. The appropriate modeling action is to apply a seasonal difference of order $D=1$ to the series to induce seasonal stationarity. After this transformation, one would re-examine the ACF and PACF of the differenced series, $\\nabla_{12} x_t$, to identify the orders of the non-seasonal ($p,d,q$) and any remaining seasonal ($P,Q$) components. The observation about the nonseasonal lags (gradual decay in ACF) suggests that a regular difference ($d=1$) may also be needed, but the question specifically asks about the seasonal treatment based on the given diagnostics. The most immediate and critical step for the seasonal part is differencing.\n\n### Option-by-Option Analysis\n\n**A. Apply seasonal differencing of order $D=1$ with period $s=12$, that is, transform to $\\nabla_{12} x_t = x_t - x_{t-12}$ before further modeling.**\nThis option proposes exactly the action indicated by the diagnostic evidence. The slow decay of the ACF at seasonal lags is the classic signature of seasonal non-stationarity that requires seasonal differencing. This is the correct first step to address the seasonal component.\n**Verdict: Correct.**\n\n**B. Keep $\\{x_t\\}$ undifferenced seasonally and introduce a seasonal autoregressive term of order $P=1$ at period $s=12$ within a SARIMA structure.**\nThis is incorrect. A stationary seasonal $AR(1)$ process, $(1 - \\Phi_1 B^{12})x_t = \\epsilon_t$ with $|\\Phi_1|  1$, would have a seasonal ACF that decays exponentially and a seasonal PACF with a sharp cutoff at lag $12$. The observed slow decay in the ACF points to a unit root ($\\Phi_1=1$), which means the process is non-stationary. Fitting a stationary AR model to a non-stationary series is a misspecification. The series must be differenced first.\n**Verdict: Incorrect.**\n\n**C. Apply a nonseasonal difference of order $d=1$ (no seasonal differencing), and include a seasonal autoregressive term with $P=1$ at period $s=12$.**\nThis option fails to address the primary problem of seasonal non-stationarity. Applying only a nonseasonal difference, $\\nabla x_t = x_t - x_{t-1}$, will not remove the strong, slowly decaying correlations at seasonal lags. The resulting series would still be seasonally non-stationary. Furthermore, for the same reason as in option B, including a seasonal AR term without first applying the necessary seasonal difference is inappropriate.\n**Verdict: Incorrect.**\n\n**D. Keep $\\{x_t\\}$ undifferenced seasonally and add a seasonal moving average term of order $Q=1$ at period $s=12$ within a SARIMA structure.**\nThis is incorrect. A seasonal $MA(1)$ process would be characterized by a single significant spike in the ACF at lag $s=12$ and a PACF that tails off (decays) at seasonal lags ($k=12, 24, \\dots$). The observed diagnostics are the opposite of this pattern: the ACF tails off, and the PACF shows spikes. Therefore, a seasonal MA term is not indicated by the data.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A forecast is only as useful as its measure of uncertainty, a principle that becomes critical for 'persistent' time series where shocks have long-lasting effects. In this coding practice , you will move from theory to implementation by calculating how forecast error accumulates over time for a near-unit-root process. This provides a concrete understanding of why long-horizon predictions for such series, which are common in economics and finance, are inherently difficult.",
            "id": "3187694",
            "problem": "Consider a first-order autoregressive process (AR) as a special case of the Autoregressive Integrated Moving Average (ARIMA) family, defined by the recursion $X_t = \\phi X_{t-1} + \\varepsilon_t$, where $\\{\\varepsilon_t\\}$ is independent and identically distributed (i.i.d.) zero-mean noise with variance $\\sigma_\\varepsilon^2$, and $|\\phi|  1$ ensures weak stationarity. In near-unit-root settings, such as $\\phi$ very close to $1$, the accumulation of shocks over time can produce substantial forecast uncertainty at long horizons even though the process is technically stationary.\n\nYour task is to implement a program that, for a set of test cases, quantifies long-horizon forecast uncertainty for the near-unit-root case and related comparators. For each test case, you are given numerical values of the autoregressive parameter $\\phi$, the noise variance $\\sigma_\\varepsilon^2$, the current observed value $x_t$, and a forecast horizon $h \\in \\mathbb{N}$. Starting only from the model definition and the properties of i.i.d. noise, compute the following for each test case:\n\n1. The $h$-step-ahead forecast error standard deviation $\\sqrt{\\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t])}$.\n2. The dimensionless ratio $r$ defined as the $0.95$-level forecast half-width divided by the magnitude of the forecast mean, that is $r = z_{0.975} \\cdot \\sqrt{\\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t])} \\big/ \\left|\\mathbb{E}[X_{t+h} \\mid X_t = x_t]\\right|$, where $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution. Express $r$ as a decimal number without a percentage sign.\n\nUse only the foundational definitions above; do not use any pre-specified forecasting formulas beyond the model and i.i.d. properties. Assume $\\varepsilon_t$ is Gaussian to justify the use of $z_{0.975}$.\n\nTest suite:\n- Case A (near-unit-root, moderate horizon): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 10$.\n- Case B (near-unit-root, long horizon): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 100$.\n- Case C (closer to unit root, long horizon): $\\phi = 0.999$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 100$.\n- Case D (near-unit-root, reduced noise, long horizon): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 0.1$, $x_t = 100.0$, $h = 100$.\n- Case E (more strongly stationary comparator): $\\phi = 0.9$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 100.0$, $h = 100$.\n- Case F (near-unit-root, small current level): $\\phi = 0.99$, $\\sigma_\\varepsilon^2 = 1.0$, $x_t = 1.0$, $h = 100$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element comma-separated list of floats $[\\text{sd}, r]$ printed with six decimal places, for example: $[[\\text{sd}_A,\\ r_A],[\\text{sd}_B,\\ r_B],\\dots]$. No additional text should be printed. There are no physical units involved, and all angles are irrelevant to this problem.",
            "solution": "The problem is valid as it is scientifically grounded in the established theory of autoregressive time series models, is well-posed, and is expressed in objective, formal language. We will proceed to derive the necessary quantities from first principles as stipulated.\n\nThe model is a first-order autoregressive process, AR($1$), given by the recursion $X_t = \\phi X_{t-1} + \\varepsilon_t$, where $\\{\\varepsilon_t\\}$ is a sequence of independent and identically distributed (i.i.d.) random variables with mean $\\mathbb{E}[\\varepsilon_t] = 0$ and variance $\\operatorname{Var}(\\varepsilon_t) = \\sigma_\\varepsilon^2$. We are given the condition $|\\phi|  1$, which ensures weak stationarity. Our objective is to compute the $h$-step-ahead forecast mean and the variance of the forecast error, conditional on the observation $X_t = x_t$.\n\nFirst, we derive the $h$-step-ahead forecast mean, which is the conditional expectation $\\mathbb{E}[X_{t+h} \\mid X_t = x_t]$. We apply the law of total expectation iteratively.\nFor a forecast horizon of $h=1$:\n$$ \\mathbb{E}[X_{t+1} \\mid X_t = x_t] = \\mathbb{E}[\\phi X_t + \\varepsilon_{t+1} \\mid X_t = x_t] $$\nBy the linearity of expectation and noting that $X_t$ is the known value $x_t$ in the conditional information set:\n$$ \\mathbb{E}[X_{t+1} \\mid X_t = x_t] = \\phi x_t + \\mathbb{E}[\\varepsilon_{t+1} \\mid X_t = x_t] $$\nSince the noise term $\\varepsilon_{t+1}$ is independent of $X_t$, its conditional expectation equals its unconditional expectation, which is $0$.\n$$ \\mathbb{E}[X_{t+1} \\mid X_t = x_t] = \\phi x_t $$\nFor a forecast horizon of $h=2$:\n$$ \\mathbb{E}[X_{t+2} \\mid X_t = x_t] = \\mathbb{E}[\\phi X_{t+1} + \\varepsilon_{t+2} \\mid X_t = x_t] = \\phi \\mathbb{E}[X_{t+1} \\mid X_t = x_t] + \\mathbb{E}[\\varepsilon_{t+2} \\mid X_t = x_t] $$\nSubstituting the result for $h=1$ and using the independence of $\\varepsilon_{t+2}$ yields:\n$$ \\mathbb{E}[X_{t+2} \\mid X_t = x_t] = \\phi(\\phi x_t) + 0 = \\phi^2 x_t $$\nBy induction, the general formula for the $h$-step-ahead forecast mean is:\n$$ \\mathbb{E}[X_{t+h} \\mid X_t = x_t] = \\phi^h x_t $$\n\nNext, we derive the variance of the $h$-step-ahead forecast error, defined as $\\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t])$. This quantity is equivalent to the conditional variance $\\operatorname{Var}(X_{t+h} \\mid X_t = x_t)$. To find this, we first express $X_{t+h}$ in terms of $X_t$ and the subsequent noise terms by repeated substitution:\n$$ X_{t+1} = \\phi X_t + \\varepsilon_{t+1} $$\n$$ X_{t+2} = \\phi X_{t+1} + \\varepsilon_{t+2} = \\phi(\\phi X_t + \\varepsilon_{t+1}) + \\varepsilon_{t+2} = \\phi^2 X_t + \\phi\\varepsilon_{t+1} + \\varepsilon_{t+2} $$\nIn general, this recursive substitution expands to:\n$$ X_{t+h} = \\phi^h X_t + \\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k} $$\nThe forecast error, which we denote as $e_{t,h}$, is the difference between the actual future value $X_{t+h}$ and its forecast conditional on information at time $t$:\n$$ e_{t,h} = X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t] = \\left(\\phi^h x_t + \\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k}\\right) - \\phi^h x_t = \\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k} $$\nThe variance of this error is calculated using the properties of the i.i.d. noise terms. Since the terms $\\varepsilon_{t+1}, \\dots, \\varepsilon_{t+h}$ are mutually independent:\n$$ \\operatorname{Var}(e_{t,h}) = \\operatorname{Var}\\left(\\sum_{k=0}^{h-1} \\phi^k \\varepsilon_{t+h-k}\\right) = \\sum_{k=0}^{h-1} \\operatorname{Var}(\\phi^k \\varepsilon_{t+h-k}) $$\nUsing the variance property $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ and the given fact $\\operatorname{Var}(\\varepsilon_j) = \\sigma_\\varepsilon^2$:\n$$ \\operatorname{Var}(e_{t,h}) = \\sum_{k=0}^{h-1} (\\phi^k)^2 \\sigma_\\varepsilon^2 = \\sigma_\\varepsilon^2 \\sum_{k=0}^{h-1} (\\phi^2)^k $$\nThe summation is a finite geometric series with $h$ terms, a first term of $1$, and a common ratio of $\\phi^2$. The sum is given by the formula $\\frac{1 - (\\text{ratio})^{\\text{num_terms}}}{1 - \\text{ratio}}$, which in our case is $\\frac{1 - (\\phi^2)^h}{1 - \\phi^2}$.\nTherefore, the $h$-step-ahead forecast error variance, which we denote as $V_h$, is:\n$$ V_h = \\operatorname{Var}(X_{t+h} - \\mathbb{E}[X_{t+h} \\mid X_t = x_t]) = \\sigma_\\varepsilon^2 \\left(\\frac{1 - \\phi^{2h}}{1-\\phi^2}\\right) $$\nWith these derivations, the two quantities to be computed for each test case are:\n1. The forecast error standard deviation, $\\text{sd} = \\sqrt{V_h}$.\n2. The dimensionless ratio $r = z_{0.975} \\cdot \\text{sd} / \\left|\\mathbb{E}[X_{t+h} \\mid X_t = x_t]\\right|$, where $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution, whose use is justified by the problem's assumption that $\\varepsilon_t$ is Gaussian.\n\nThese derived formulas are implemented in the following program to compute the results for each specified test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes long-horizon forecast uncertainty metrics for an AR(1) process\n    for a given suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (phi, sigma_eps_sq, x_t, h)\n    test_cases = [\n        (0.99, 1.0, 100.0, 10),   # Case A\n        (0.99, 1.0, 100.0, 100),  # Case B\n        (0.999, 1.0, 100.0, 100), # Case C\n        (0.99, 0.1, 100.0, 100),  # Case D\n        (0.9, 1.0, 100.0, 100),   # Case E\n        (0.99, 1.0, 1.0, 100),    # Case F\n    ]\n\n    # The 0.975 quantile of the standard normal distribution, for a 95% interval.\n    z_0975 = norm.ppf(0.975)\n\n    results = []\n    for phi, sigma_eps_sq, x_t, h in test_cases:\n        # 1. Calculate the h-step-ahead forecast mean.\n        # E[X_{t+h} | X_t = x_t] = phi^h * x_t\n        forecast_mean = (phi**h) * x_t\n\n        # 2. Calculate the h-step-ahead forecast error variance.\n        # Var(error) = sigma_eps^2 * (1 - phi^(2h)) / (1 - phi^2)\n        # The condition |phi|  1 ensures the denominator is non-zero.\n        forecast_error_variance = (sigma_eps_sq * (1.0 - phi**(2 * h))) / (1.0 - phi**2)\n\n        # 3. Calculate the forecast error standard deviation (sd).\n        forecast_error_std_dev = np.sqrt(forecast_error_variance)\n\n        # 4. Calculate the dimensionless ratio r.\n        # r = z * sd / |mean|\n        # Use np.abs to handle the magnitude of the mean.\n        # The mean will not be exactly zero for the given test cases,\n        # so division by zero is not a concern.\n        if np.abs(forecast_mean)  1e-12: # Handle computationally near-zero means\n             # In this theoretical case, the ratio would be effectively infinite.\n             # We can represent it with a very large number or nan for robustness,\n             # although not needed for the given test suite.\n             r_ratio = float('inf')\n        else:\n             r_ratio = z_0975 * forecast_error_std_dev / np.abs(forecast_mean)\n\n        results.append([forecast_error_std_dev, r_ratio])\n\n    # Format the results into the exact required output string.\n    # e.g., [[sd_A, r_A],[sd_B, r_B],...] with 6 decimal places.\n    output_str = '[' + ','.join([f\"[{sd:.6f},{r:.6f}]\" for sd, r in results]) + ']'\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data analysis is a multi-stage pipeline, requiring more than just fitting a model to perfectly clean data. This comprehensive practice  simulates an end-to-end workflow, from wrangling raw count data to identifying unusual events. You will implement variance stabilization, differencing, model fitting, and residual analysis to build a system that can detect 'shocks' in a time series, such as a sudden spike in social media activity.",
            "id": "3187631",
            "problem": "Consider a discrete-time count process $\\{x_t\\}_{t=0}^{T-1}$ representing hashtag usage frequency per unit time. In many practical cases, count data can be reasonably approximated as following a Poisson-like mechanism with $\\operatorname{Var}(x_t)$ growing with $\\operatorname{E}(x_t)$, which motivates variance stabilization before modeling. Let $z_t$ denote a variance-stabilized transformation of $x_t$, and let $w_t$ denote the differenced series of order $d$, used to induce stationarity for integrated models.\n\nBase definitions and facts:\n- A time series $\\{y_t\\}$ is wide-sense stationary if $\\operatorname{E}(y_t) = \\mu$ for all $t$ and $\\operatorname{Cov}(y_t, y_{t+h})$ depends only on $h$, not on $t$.\n- The autocorrelation function (ACF) at lag $h$ is $\\rho(h) = \\frac{\\operatorname{Cov}(y_t,y_{t+h})}{\\operatorname{Var}(y_t)}$.\n- An Autoregressive Integrated Moving Average (ARIMA) model of order $(p,d,q)$ for $\\{x_t\\}$ means that the $d$-times differenced, possibly transformed series satisfies an Autoregressive Moving Average $ARMA(p,q)$ dynamics: $w_t = \\mu + \\sum_{i=1}^{p}\\phi_i w_{t-i} + \\epsilon_t + \\sum_{j=1}^{q}\\theta_j \\epsilon_{t-j}$, where $\\{\\epsilon_t\\}$ is white noise with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- For Poisson-like counts, the square-root transform $z_t = \\sqrt{x_t + c}$ with a small constant $c$ yields approximately constant variance; with the Delta method, for large $\\lambda$, $\\operatorname{Var}(\\sqrt{X+c}) \\approx \\frac{1}{4}$ when $X \\sim \\operatorname{Poisson}(\\lambda)$.\n- Differencing of order $d=1$ is defined by $w_t = z_t - z_{t-1}$ for $t \\ge 1$ (and produces a series indexed by $t=1,\\dots,T-1$). A shock in the original series at index $t^\\star$ manifests as a large $|w_{t^\\star}|$ or $|w_{t^\\star+1}|$, depending on the direction and persistence of the change.\n\nTask:\nImplement a program that, for each provided test case, treats hashtag frequency as a count time series, stabilizes variance, fits an $ARIMA(1,1,1)$ model via conditional Gaussian estimation on the differenced series (equivalently, fit an $ARMA(1,1)$ to $w_t$), and assesses shocks as large standardized residuals. Your implementation must adhere to the following modeling steps derived from first principles:\n\n1. Variance stabilization: Given counts $x_t$, produce $z_t$ using either the square-root transform $z_t = \\sqrt{x_t + 0.5}$ or the logarithmic transform $z_t = \\log(x_t + 1)$, as specified per test case.\n2. Differencing: For differencing order $d=1$, compute $w_t = z_t - z_{t-1}$ for $t=1,\\dots,T-1$.\n3. Centering: Compute $y_t = w_t - \\bar{w}$, where $\\bar{w}$ is the sample mean of $\\{w_t\\}$, to align with the $ARMA(1,1)$ form around zero mean.\n4. $ARMA(1,1)$ fitting by conditional sum-of-squares under Gaussian noise: Minimize the average squared innovations\n   $$J(\\phi,\\theta) = \\frac{1}{N}\\sum_{t=0}^{N-1}\\epsilon_t^2,$$\n   with the recursion\n   $$\\epsilon_t = y_t - \\phi y_{t-1} - \\theta \\epsilon_{t-1},$$\n   using initial conditions $y_{-1} = 0$ and $\\epsilon_{-1} = 0$. Constrain $\\phi,\\theta \\in (-1,1)$ to respect stationarity and invertibility in the conditional sense.\n5. Residual standardization: Estimate $\\sigma^2$ by $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{t=0}^{N-1}\\epsilon_t^2$ and compute standardized residuals $r_t = \\epsilon_t / \\hat{\\sigma}$.\n6. Shock identification: Using a specified threshold $T$ (dimensionless), declare a shock at original index $t = \\tau + d$ whenever $|r_\\tau| \\ge T$ for residual index $\\tau$ in the differenced series. Use $0$-based indexing for reported shock positions.\n\nTest suite:\nFor each case, generate the count time series $\\{x_t\\}$ exactly as specified, using $0$-based indexing and integer counts via flooring. Let $\\lfloor\\cdot\\rfloor$ denote the floor function.\n\n- Case A (happy path, moderate counts with two shocks):\n  - Length $T = 120$.\n  - Baseline: $b_t = 20 + 5\\sin\\left(\\frac{2\\pi t}{20}\\right)$ for $t=0,\\dots,119$.\n  - Shocks: $s_t = 40$ if $t \\in \\{30,75\\}$ and $s_t = 0$ otherwise.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor + s_t$.\n  - Variance stabilization: square-root $z_t = \\sqrt{x_t + 0.5}$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\n- Case B (edge case, no shocks):\n  - Length $T = 120$.\n  - Baseline: $b_t = 25 + 3\\sin\\left(\\frac{2\\pi t}{30}\\right)$ for $t=0,\\dots,119$.\n  - Shocks: $s_t = 0$ for all $t$.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor$.\n  - Variance stabilization: square-root $z_t = \\sqrt{x_t + 0.5}$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\n- Case C (boundary condition, short series with one shock):\n  - Length $T = 20$.\n  - Baseline: $b_t = 15 + 2\\sin\\left(\\frac{2\\pi t}{15}\\right)$ for $t=0,\\dots,19$.\n  - Shock: $s_t = 30$ if $t=10$ and $s_t = 0$ otherwise.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor + s_t$.\n  - Variance stabilization: square-root $z_t = \\sqrt{x_t + 0.5}$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\n- Case D (high counts, logarithmic stabilization, one shock):\n  - Length $T = 100$.\n  - Baseline: $b_t = 200 + 10\\sin\\left(\\frac{2\\pi t}{25}\\right)$ for $t=0,\\dots,99$.\n  - Shock: $s_t = 300$ if $t=60$ and $s_t = 0$ otherwise.\n  - Counts: $x_t = \\left\\lfloor b_t \\right\\rfloor + s_t$.\n  - Variance stabilization: logarithmic $z_t = \\log(x_t + 1)$.\n  - Differencing order: $d=1$.\n  - Shock threshold: $T = 3$.\n\nRequired output:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the list of detected shock indices (using $0$-based indexing). For example, the final output must be of the form\n$[\\,[\\text{indices for Case A}],\\,[\\text{indices for Case B}],\\,[\\text{indices for Case C}],\\,[\\text{indices for Case D}]\\,]$,\nwith each inner list consisting solely of integers in ascending order.\n\nNo physical units are involved; the threshold is dimensionless. Angles in trigonometric functions are in radians by construction via the $\\sin$ function’s argument. All reported quantities must be integers or lists of integers. The computation must be deterministic and require no random number generation.",
            "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in established principles of time series analysis, is well-posed with a clear and deterministic set of instructions, and is objective in its formulation. The task is to implement a specific algorithm for detecting shocks in a count time series using an ARIMA(1,1,1) model, which involves variance stabilization, differencing, parameter estimation, and residual analysis.\n\nThe solution will be constructed by systematically following the sequence of modeling steps as specified.\n\n### 1. Time Series Generation and Transformation\n\nFor each test case, a discrete-time count process $\\{x_t\\}_{t=0}^{T-1}$ of length $T$ is generated. The process is defined by a baseline component $b_t$ and a shock component $s_t$, such that the final count at time $t$ is $x_t = \\lfloor b_t \\rfloor + s_t$.\n\nCount data often exhibit heteroscedasticity, where the variance is a function of the mean. For a Poisson process, $\\operatorname{Var}(x_t) = \\operatorname{E}(x_t) = \\lambda_t$. To satisfy the constant variance assumption of many time series models, a variance-stabilizing transformation is applied. The problem specifies two such transformations:\n1.  **Square-root transform:** $z_t = \\sqrt{x_t + c}$, with $c=0.5$. This transform is motivated by the Delta method, which shows that for a Poisson-distributed random variable $X$ with mean $\\lambda$, the variance of $\\sqrt{X}$ is approximately constant ($\\approx 1/4$) for large $\\lambda$.\n2.  **Logarithmic transform:** $z_t = \\log(x_t + 1)$. This transform is effective when the standard deviation of the data is proportional to the mean.\n\nThe resulting series is $\\{z_t\\}_{t=0}^{T-1}$.\n\n### 2. Stationarity Induction and Centering\n\nThe \"I\" in ARIMA stands for \"Integrated\" and implies that the raw time series is non-stationary but its differences are stationary. Stationarity is induced by differencing the transformed series $\\{z_t\\}$. For a differencing order of $d=1$, the new series $\\{w_t\\}$ is defined as:\n$$w_t = z_t - z_{t-1}, \\quad \\text{for } t = 1, \\dots, T-1$$\nThis operation produces a series of length $N = T-1$. A shock at an original index $t^\\star$ introduces large-magnitude values in $w_{t^\\star}$ and $w_{t^\\star+1}$.\n\nThe ARMA model is to be fitted to a zero-mean series. Therefore, the series $\\{w_t\\}$ is centered by subtracting its sample mean, $\\bar{w}$:\n$$y_\\tau = w_{\\tau+1} - \\bar{w}, \\quad \\text{for } \\tau = 0, \\dots, N-1$$\nwhere $\\bar{w} = \\frac{1}{N}\\sum_{t=1}^{T-1} w_t$. The new series $\\{y_\\tau\\}_{\\tau=0}^{N-1}$ has a sample mean of zero. Note the re-indexing from $t$ to $\\tau$ for implementation convenience, where $\\tau = t-1$.\n\n### 3. ARMA(1,1) Parameter Estimation\n\nAn Autoregressive Moving Average $ARMA(1,1)$ model is specified for the stationary, centered series $\\{y_\\tau\\}$. The model is given by:\n$$y_\\tau = \\phi y_{\\tau-1} + \\epsilon_\\tau + \\theta \\epsilon_{\\tau-1}$$\nwhere $\\{\\epsilon_\\tau\\}$ is a white noise process with $\\epsilon_\\tau \\sim \\mathcal{N}(0, \\sigma^2)$, and $\\phi$ and $\\theta$ are the autoregressive and moving average parameters, respectively.\n\nTo estimate $(\\phi, \\theta)$, we use the method of conditional sum of squares (CSS). This involves minimizing an objective function $J(\\phi, \\theta)$ representing the mean squared innovations (residuals). The innovations are defined by rearranging the model equation:\n$$\\epsilon_\\tau = y_\\tau - \\phi y_{\\tau-1} - \\theta \\epsilon_{\\tau-1}$$\nThe objective function to minimize is:\n$$J(\\phi, \\theta) = \\frac{1}{N}\\sum_{\\tau=0}^{N-1}\\epsilon_\\tau^2$$\nThe recursion requires initial conditions. As specified, we use $y_{-1} = 0$ and $\\epsilon_{-1} = 0$. The first innovation is then $\\epsilon_0 = y_0$.\n\nThis is a numerical optimization problem. We seek $(\\hat{\\phi}, \\hat{\\theta})$ such that:\n$$(\\hat{\\phi}, \\hat{\\theta}) = \\underset{\\phi, \\theta}{\\operatorname{argmin}} \\, J(\\phi, \\theta)$$\nThe optimization is performed subject to the constraints $\\phi \\in (-1, 1)$ and $\\theta \\in (-1, 1)$, which are necessary conditions for the stationarity and invertibility of the ARMA process. A numerical solver, such as a quasi-Newton method with box constraints (e.g., L-BFGS-B), is suitable for this task.\n\n### 4. Shock Detection via Standardized Residuals\n\nAfter obtaining the optimal parameters $(\\hat{\\phi}, \\hat{\\theta})$, the final residual series $\\{\\hat{\\epsilon}_\\tau\\}_{\\tau=0}^{N-1}$ is calculated using the recursive formula with these estimated parameters. These residuals represent the portion of the data not explained by the fitted ARMA model. Shocks or anomalies are expected to manifest as large-magnitude residuals.\n\nThe variance of the white noise process, $\\sigma^2$, is estimated using the mean of the squared residuals:\n$$\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{\\tau=0}^{N-1}\\hat{\\epsilon}_\\tau^2$$\nNote that $\\hat{\\sigma}^2$ is simply the minimized value of the objective function $J(\\hat{\\phi}, \\hat{\\theta})$. The estimated standard deviation is $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$.\n\nThe residuals are then standardized to have unit variance:\n$$r_\\tau = \\frac{\\hat{\\epsilon}_\\tau}{\\hat{\\sigma}}$$\nStandardized residuals with large absolute values indicate points that are outliers with respect to the model.\n\nThe final step is to identify shock locations. A shock is declared whenever the absolute value of a standardized residual exceeds a specified dimensionless threshold, $T_{\\text{thresh}}$. That is, a shock is detected for each residual index $\\tau$ satisfying:\n$$|r_\\tau| \\ge T_{\\text{thresh}}$$\nThe problem specifies a rule to map the residual index $\\tau$ back to the original time series index $t$. For each $\\tau$ that meets the shock condition, the corresponding original index is reported as $t = \\tau + d$. With the differencing order $d=1$, this becomes:\n$$t = \\tau + 1$$\nThe final output for each test case is a sorted list of these detected shock indices $t$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef _process_case(case_params):\n    \"\"\"\n    Processes a single test case for shock detection in a time series.\n    \"\"\"\n    # Unpack parameters for the case\n    T_len, b_func, s_dict, transform_type, d_order, T_thresh = case_params\n\n    # 1. Generate the count time series {x_t}\n    t_series = np.arange(T_len)\n    baseline = b_func(t_series)\n    shocks = np.zeros(T_len)\n    for t_idx, val in s_dict.items():\n        shocks[t_idx] = val\n    x = np.floor(baseline) + shocks\n\n    # 2. Variance stabilization to get {z_t}\n    if transform_type == 'sqrt':\n        z = np.sqrt(x + 0.5)\n    elif transform_type == 'log':\n        z = np.log(x + 1)\n    else:\n        raise ValueError(\"Invalid transform type\")\n\n    # 3. Differencing to get {w_t}\n    if d_order != 1:\n        raise NotImplementedError(\"Only d=1 is implemented.\")\n    w = z[1:] - z[:-1]\n\n    # 4. Centering to get {y_t}\n    y = w - np.mean(w)\n    N = len(y)\n\n    # 5. ARMA(1,1) fitting by conditional sum-of-squares\n    def cost_function(params, y_data):\n        phi, theta = params\n        n_obs = len(y_data)\n        epsilons = np.zeros(n_obs)\n        \n        # Calculate residuals recursively\n        # Initial conditions y_{-1}=0, epsilon_{-1}=0 are implicit\n        \n        # for tau = 0\n        epsilons[0] = y_data[0] # from eps_0 = y_0 - phi*y_{-1} - theta*eps_{-1}\n        \n        # for tau = 1 to N-1\n        for tau in range(1, n_obs):\n            epsilons[tau] = y_data[tau] - phi * y_data[tau-1] - theta * epsilons[tau-1]\n            \n        return np.mean(epsilons**2)\n\n    # Use L-BFGS-B for optimization with bounds\n    initial_guess = [0.0, 0.0]\n    # Use bounds slightly inside (-1, 1) for stability\n    bnds = ((-0.999999, 0.999999), (-0.999999, 0.999999))\n    opt_result = minimize(cost_function, initial_guess, args=(y,), method='L-BFGS-B', bounds=bnds)\n    phi_hat, theta_hat = opt_result.x\n\n    # 6. Residual standardization\n    # Recalculate final residuals with optimal parameters\n    eps_hat = np.zeros(N)\n    eps_hat[0] = y[0]\n    for tau in range(1, N):\n        eps_hat[tau] = y[tau] - phi_hat * y[tau-1] - theta_hat * eps_hat[tau-1]\n\n    # Estimate sigma\n    sigma_hat_sq = np.mean(eps_hat**2)\n    sigma_hat = np.sqrt(sigma_hat_sq)\n\n    # Compute standardized residuals {r_t}\n    # Avoid division by zero if sigma_hat is very small\n    if sigma_hat  1e-9:\n        r = np.zeros(N)\n    else:\n        r = eps_hat / sigma_hat\n\n    # 7. Shock identification\n    # Find residual indices tau where |r_tau| = T_thresh\n    shock_residual_indices = np.where(np.abs(r) = T_thresh)[0]\n\n    # Convert residual indices back to original time series indices t = tau + d\n    shock_original_indices = [int(tau + d_order) for tau in shock_residual_indices]\n    \n    return sorted(shock_original_indices)\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A\n        (120, \n         lambda t: 20 + 5 * np.sin(2 * np.pi * t / 20), \n         {30: 40, 75: 40}, \n         'sqrt', 1, 3),\n        # Case B\n        (120, \n         lambda t: 25 + 3 * np.sin(2 * np.pi * t / 30), \n         {}, \n         'sqrt', 1, 3),\n        # Case C\n        (20, \n         lambda t: 15 + 2 * np.sin(2 * np.pi * t / 15), \n         {10: 30}, \n         'sqrt', 1, 3),\n        # Case D\n        (100, \n         lambda t: 200 + 10 * np.sin(2 * np.pi * t / 25), \n         {60: 300}, \n         'log', 1, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        shock_indices = _process_case(case)\n        results.append(shock_indices)\n\n    # Final print statement in the exact required format.\n    # The str() of a list produces the required '[...]' format for inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}