{
    "hands_on_practices": [
        {
            "introduction": "自训练的成功取决于其生成的伪标签的质量。这个练习提供了一个框架，用于量化添加到训练集中的伪标签的“噪声率”，并将其直接与模型的性能和你选择的置信度阈值联系起来。通过这个练习，你将对伪标签数据的数量与质量之间的基本权衡有更深刻的直觉。",
            "id": "3172749",
            "problem": "考虑标签为 $y \\in \\{0,1\\}$ 的二元分类问题，其类别先验概率为 $\\pi = \\Pr(y=1) \\in (0,1)$。一个固定的分类器产生预测值 $\\hat{y} \\in \\{0,1\\}$，其混淆率为 $\\alpha = \\Pr(\\hat{y}=1 \\mid y=0)$ 和 $\\beta = \\Pr(\\hat{y}=0 \\mid y=1)$，并且配备了校准的置信度分数，因此对于任何输入 $x$，预测的最高类别概率被用作置信度 $c(x) \\in [0,1]$。在一个自训练过程中，未标记的输入由该分类器进行伪标记，并且只有当其置信度超过一个固定的阈值 $\\tau \\in (0.5,1)$ 时，才会被添加到训练集中。\n\n定义选择率 $s_{0}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=0)$ 和 $s_{1}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=1)$，并假设在每个真实类别内部，超过阈值 $\\tau$ 与分类器是否预测正确是独立的，因此混淆率 $\\alpha$ 和 $\\beta$ 统一适用于被选择的子集。令 $n_{L}$ 表示标记数据集的大小，$n_{U}$ 表示未标记样本池的大小。假设所有被选择的未标记输入都被添加，产生预期数量为 $m = n_{U}\\big((1-\\pi)s_{0}(\\tau) + \\pi s_{1}(\\tau)\\big)$ 的伪标记样本。\n\n使用全概率法则和经验 $0$-$1$ 损失的定义，推导在阈值 $\\tau$ 下添加的伪标签中的预期噪声率的表达式，其定义为\n$$\n\\eta(\\tau) = \\Pr\\big(\\hat{y} \\neq y \\,\\big|\\, c(X) \\ge \\tau\\big),\n$$\n用 $\\alpha$、$\\beta$、$\\pi$、$s_{0}(\\tau)$ 和 $s_{1}(\\tau)$ 来表示。然后，分析在重新训练时，这个预期噪声率如何影响在增广数据集上通过经验风险最小化（ERM）得到的经验 $0$-$1$ 风险。这里有一个简化假设，即重新训练的分类器完美拟合添加的伪标签。给出增广数据集上的预期经验 $0$-$1$ 损失，作为 $n_{L}$、标记数据集上的经验损失 $R_{L}$、$m$ 和 $\\eta(\\tau)$ 的函数。\n\n请以 $\\eta(\\tau)$ 的闭式解析表达式的形式给出你的最终答案。不需要四舍五入。",
            "solution": "该问题被评估为有效，因为它是自洽的，在统计学习理论中有科学依据，并且问题陈述清晰。我们可以开始推导。\n\n问题要求进行两项推导。第一，添加的伪标签中的预期噪声率 $\\eta(\\tau)$ 的表达式。第二，分析增广数据集上的预期经验 $0$-$1$ 损失。\n\n**第一部分：预期噪声率 $\\eta(\\tau)$ 的推导**\n\n添加的伪标签中的预期噪声率定义为，在样本被选中进行伪标记的条件下，伪标签不正确的概率。选择标准是置信度 $c(X)$ 超过阈值 $\\tau$。形式上，这表示为：\n$$\n\\eta(\\tau) = \\Pr\\big(\\hat{y} \\neq y \\,\\big|\\, c(X) \\ge \\tau\\big)\n$$\n我们使用条件概率的定义：\n$$\n\\eta(\\tau) = \\frac{\\Pr(\\hat{y} \\neq y, c(X) \\ge \\tau)}{\\Pr(c(X) \\ge \\tau)}\n$$\n让我们将选择事件表示为 $S = \\{c(X) \\ge \\tau\\}$。表达式变为：\n$$\n\\eta(\\tau) = \\frac{\\Pr(\\hat{y} \\neq y, S)}{\\Pr(S)}\n$$\n我们将使用全概率法则，以真实类别标签 $y \\in \\{0, 1\\}$ 为条件，分别计算分子和分母。\n\n首先，我们计算分母，$\\Pr(S) = \\Pr(c(X) \\ge \\tau)$：\n$$\n\\Pr(S) = \\Pr(S \\mid y=0)\\Pr(y=0) + \\Pr(S \\mid y=1)\\Pr(y=1)\n$$\n根据问题陈述，我们已知：\n- 类别先验概率 $\\pi = \\Pr(y=1)$，这意味着 $\\Pr(y=0) = 1-\\pi$。\n- 类条件选择率 $s_{0}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=0)$ 和 $s_{1}(\\tau) = \\Pr(c(X) \\ge \\tau \\mid y=1)$。\n\n将这些已知条件代入 $\\Pr(S)$ 的表达式中：\n$$\n\\Pr(S) = s_{0}(\\tau)(1-\\pi) + s_{1}(\\tau)\\pi\n$$\n\n接下来，我们计算分子，$\\Pr(\\hat{y} \\neq y, S)$。同样，我们使用全概率法则，以真实类别 $y$ 为条件：\n$$\n\\Pr(\\hat{y} \\neq y, S) = \\Pr(\\hat{y} \\neq y, S \\mid y=0)\\Pr(y=0) + \\Pr(\\hat{y} \\neq y, S \\mid y=1)\\Pr(y=1)\n$$\n我们来分析和中的每一项：\n1. 对于 $y=0$ 的情况，不正确的预测意味着 $\\hat{y}=1$。所以，第一项是 $\\Pr(\\hat{y} = 1, S \\mid y=0)\\Pr(y=0)$。\n2. 对于 $y=1$ 的情况，不正确的预测意味着 $\\hat{y}=0$。所以，第二项是 $\\Pr(\\hat{y} = 0, S \\mid y=1)\\Pr(y=1)$。\n\n问题陈述了一个关键的独立性假设：“在每个真实类别内部，超过阈值 $\\tau$ 与分类器是否预测正确是独立的”。\n这可以形式化为：\n- 给定 $y=0$，不正确预测的事件，$\\{\\hat{y}=1\\}$，与选择事件 $S$ 是独立的。\n  $$ \\Pr(\\hat{y}=1, S \\mid y=0) = \\Pr(\\hat{y}=1 \\mid y=0) \\Pr(S \\mid y=0) $$\n- 给定 $y=1$，不正确预测的事件，$\\{\\hat{y}=0\\}$，与选择事件 $S$ 是独立的。\n  $$ \\Pr(\\hat{y}=0, S \\mid y=1) = \\Pr(\\hat{y}=0 \\mid y=1) \\Pr(S \\mid y=1) $$\n\n我们已知混淆率 $\\alpha = \\Pr(\\hat{y}=1 \\mid y=0)$ 和 $\\beta = \\Pr(\\hat{y}=0 \\mid y=1)$。使用这些和选择率，条件概率变为：\n- $\\Pr(\\hat{y}=1, S \\mid y=0) = \\alpha \\cdot s_{0}(\\tau)$\n- $\\Pr(\\hat{y}=0, S \\mid y=1) = \\beta \\cdot s_{1}(\\tau)$\n\n现在，我们可以写出分子的完整表达式：\n$$\n\\Pr(\\hat{y} \\neq y, S) = \\big( \\alpha \\cdot s_{0}(\\tau) \\big) (1-\\pi) + \\big( \\beta \\cdot s_{1}(\\tau) \\big) \\pi\n$$\n\n最后，我们将分子和分母结合起来，得到 $\\eta(\\tau)$ 的表达式：\n$$\n\\eta(\\tau) = \\frac{\\alpha s_{0}(\\tau)(1-\\pi) + \\beta s_{1}(\\tau)\\pi}{s_{0}(\\tau)(1-\\pi) + s_{1}(\\tau)\\pi}\n$$\n\n**第二部分：预期经验风险分析**\n\n问题的第二部分要求计算增广数据集上的预期经验 $0$-$1$ 损失。增广数据集 $D_{aug}$ 由大小为 $n_L$ 的原始标记集 $D_L$ 和大小为 $m$ 的伪标记样本集 $D_{pseudo}$ 组成。总大小为 $n_L + m$。\n\n对于一个重新训练的分类器 $h_{new}$，在这个增广数据集上的经验 $0$-$1$ 损失必须相对于*真实*标签进行评估，以量化噪声的影响。令 $y_i$ 为样本 $x_i$ 的真实标签。真实的经验损失为：\n$$\nR_{D_{aug}}^{true}(h_{new}) = \\frac{1}{n_L + m} \\left( \\sum_{(x_i, y_i) \\in D_L} \\mathbb{I}(h_{new}(x_i) \\neq y_i) + \\sum_{(x_j, y_j) \\in D_{pseudo}} \\mathbb{I}(h_{new}(x_j) \\neq y_j) \\right)\n$$\n问题指明，重新训练的分类器“完美拟合添加的伪标签”。这意味着对于任何伪标记样本 $(x_j, \\hat{y}_j) \\in D_{pseudo}$，我们有 $h_{new}(x_j) = \\hat{y}_j$。将此代入第二个求和中：\n$$\nR_{D_{aug}}^{true}(h_{new}) = \\frac{1}{n_L + m} \\left( \\sum_{(x_i, y_i) \\in D_L} \\mathbb{I}(h_{new}(x_i) \\neq y_i) + \\sum_{(x_j, y_j) \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j) \\right)\n$$\n第一个求和是原始标记集上的总损失，即 $n_L R_L$，其中 $R_L$ 是 $h_{new}$ 在标记数据集上的经验损失。表达式变为：\n$$\nR_{D_{aug}}^{true}(h_{new}) = \\frac{n_L R_L + \\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j)}{n_L + m}\n$$\n我们需要找到*预期*经验损失。期望是针对未标记数据的随机选择而计算的。\n$$\nE[R_{D_{aug}}^{true}(h_{new})] = \\frac{1}{n_L+m} E\\left[ n_L R_L + \\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j) \\right]\n$$\n假设 $R_L$ 是该过程的一个固定结果，并利用期望的线性性质：\n$$\nE[R_{D_{aug}}^{true}(h_{new})] = \\frac{n_L R_L + E\\left[\\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j)\\right]}{n_L+m}\n$$\n求和是针对 $m$ 个被选中的伪标记样本。对于每个这样的样本（索引为 $j$），项 $\\mathbb{I}(\\hat{y}_j \\neq y_j)$ 是一个伯努利随机变量。其期望是其伪标签不正确的概率。根据定义，这就是在样本被选中的条件下，预测不正确的概率，这正是 $\\eta(\\tau)$。\n$$\nE[\\mathbb{I}(\\hat{y}_j \\neq y_j)] = \\Pr(\\hat{y} \\neq y \\mid c(X) \\ge \\tau) = \\eta(\\tau)\n$$\n由于这对 $m$ 个独立同分布（i.i.d.）的被选样本中的每一个都成立，所以该和的期望为：\n$$\nE\\left[\\sum_{j \\in D_{pseudo}} \\mathbb{I}(\\hat{y}_j \\neq y_j)\\right] = \\sum_{j=1}^{m} E[\\mathbb{I}(\\hat{y}_j \\neq y_j)] = \\sum_{j=1}^{m} \\eta(\\tau) = m \\eta(\\tau)\n$$\n将其代回，增广数据集上的预期经验 $0$-$1$ 损失为：\n$$\nE[R_{D_{aug}}^{true}(h_{new})] = \\frac{n_L R_L + m \\eta(\\tau)}{n_L + m}\n$$\n这个表达式表明，最终的有效损失是原始标记数据上的损失 $R_L$ 和伪标签的噪声率 $\\eta(\\tau)$ 的加权平均。\n\n然而，问题只要求在最终答案中给出 $\\eta(\\tau)$ 的表达式。\n$$\n\\eta(\\tau) = \\frac{\\alpha s_0(\\tau)(1-\\pi) + \\beta s_1(\\tau)\\pi}{(1-\\pi)s_0(\\tau) + \\pi s_1(\\tau)}\n$$\n这也可以写成更紧凑的形式，但当前形式清晰且是直接推导出来的。\n对最终答案的表达式进行最后检查：\n$\\eta(\\tau) = \\frac{\\Pr(\\mathrm{error}, \\mathrm{selected})}{\\Pr(\\mathrm{selected})} = \\frac{\\Pr(\\mathrm{error}|y=0)\\Pr(y=0)\\Pr(\\mathrm{selected}|y=0) + \\Pr(\\mathrm{error}|y=1)\\Pr(y=1)\\Pr(\\mathrm{selected}|y=1)}{\\Pr(\\mathrm{selected}|y=0)\\Pr(y=0) + \\Pr(\\mathrm{selected}|y=1)\\Pr(y=1)}$。\n不，这是错误的。独立性是基于 $y$ 的条件。所以 $\\Pr(\\mathrm{error}, \\mathrm{selected} | y=0) = \\Pr(\\mathrm{error}|y=0)\\Pr(\\mathrm{selected}|y=0)$。\n$\\Pr(\\mathrm{error}, \\mathrm{selected}) = \\Pr(\\mathrm{error}, \\mathrm{selected} | y=0) \\Pr(y=0) + \\Pr(\\mathrm{error}, \\mathrm{selected} | y=1) \\Pr(y=1) = \\Pr(\\mathrm{error} | y=0) \\Pr(\\mathrm{selected} | y=0) \\Pr(y=0) + \\Pr(\\mathrm{error} | y=1) \\Pr(\\mathrm{selected} | y=1) \\Pr(y=1)$。\n$\\Pr(\\mathrm{error}|y=0) = \\Pr(\\hat{y}=1|y=0) = \\alpha$。\n$\\Pr(\\mathrm{error}|y=1) = \\Pr(\\hat{y}=0|y=1) = \\beta$。\n所以分子是 $\\alpha s_0(\\tau) (1-\\pi) + \\beta s_1(\\tau) \\pi$。\n分母是 $s_0(\\tau)(1-\\pi) + s_1(\\tau)\\pi$。\n推导是正确的。\n最终答案是 $\\eta(\\tau)$ 的表达式。",
            "answer": "$$\n\\boxed{\\frac{\\alpha s_{0}(\\tau)(1-\\pi) + \\beta s_{1}(\\tau)\\pi}{s_{0}(\\tau)(1-\\pi) + s_{1}(\\tau)\\pi}}\n$$"
        },
        {
            "introduction": "除了利用未标记数据，当初始标记数据集被损坏时，自训练还能出人意料地充当一种强大的去噪机制。这个问题挑战你证明在何种条件下，伪标签能够产生比初始信号更干净的训练信号。这揭示了自训练在那些干净标记数据是奢侈品的实际、非理想场景中的一个关键优势。",
            "id": "3172790",
            "problem": "考虑一个二元分类问题，其特征为 $X \\in \\mathcal{X}$，干净标签为 $Y \\in \\{0,1\\}$。你观察到一个有标签样本，其中每个干净标签都以对称标签噪声率 $\\rho \\in (0, \\tfrac{1}{2})$ 独立地被翻转：对每个样本，其观测标签有 $\\rho$ 的概率等于 $1-Y$，有 $1-\\rho$ 的概率等于 $Y$。你还有一个从 $X$ 的相同边缘分布中抽取的无标签样本。\n\n一个概率分类器在含噪有标签样本上被训练至最优，即对于任何 $x$，它都返回受污染的后验概率 $g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid X=x)$，其中 $\\tilde{Y}$ 表示观测到的含噪标签。定义干净后验概率为 $\\eta(x) = \\mathbb{P}(Y=1 \\mid X=x)$。\n\n进行一轮自训练 (Self-Training, ST) 的过程如下。在无标签样本上，根据 $g(x)$ 的贝叶斯决策，分配一个伪标签 $\\hat{Y}(x) \\in \\{0,1\\}$，即 $\\hat{Y}(x) = \\mathbf{1}\\{g(x) \\geq \\tfrac{1}{2}\\}$。只接受那些置信度边距超过固定阈值 $\\gamma \\in [0, \\tfrac{1}{2})$ 的伪标签，这意味着接受集为 $\\{x: |g(x) - \\tfrac{1}{2}| \\geq \\gamma\\}$。将原始的含噪有标签集与被接受的伪标签样本合并，并将合并后的集合的有效标签噪声率视为相对于干净标签 $Y$ 的错误标签比例。\n\n从对称标签噪声和条件概率的定义出发，推导 $g(x)$ 和 $\\eta(x)$ 之间的精确仿射关系。然后，仅使用此关系和接受规则 $|g(x) - \\tfrac{1}{2}| \\geq \\gamma$，计算被接受样本中伪标签错误率的一个分布无关的最坏情况上界。利用此上界，确定最小置信度边距 $\\gamma_{\\star}(\\rho)$，使得对于任何满足 $\\gamma  \\tfrac{1}{2} - \\rho$ 的 $\\gamma \\geq \\gamma_{\\star}(\\rho)$，一轮 ST 后的有效标签噪声率严格低于原始噪声率 $\\rho$，且此结论与 $X$ 的分布无关。\n\n你的最终答案应为 $\\gamma_{\\star}(\\rho)$ 关于 $\\rho$ 的单个闭式表达式。最终答案中不要包含任何不等式。",
            "solution": "首先验证问题，以确保其具有科学依据、问题定义良好且客观。\n\n### 步骤 1：提取已知条件\n-   **问题领域**：二元分类。\n-   **特征和标签**：$X \\in \\mathcal{X}$，干净标签 $Y \\in \\{0, 1\\}$。\n-   **噪声模型**：对称标签噪声，噪声率为 $\\rho \\in (0, \\frac{1}{2})$。观测到的含噪标签为 $\\tilde{Y}$。对每个样本，$\\mathbb{P}(\\tilde{Y} = 1-Y) = \\rho$ 且 $\\mathbb{P}(\\tilde{Y} = Y) = 1-\\rho$，其中噪声独立于 $X$。\n-   **数据**：一个带有含噪标签 $\\tilde{Y}$ 的有标签样本，以及一个来自 $X$ 的相同边缘分布的无标签样本。\n-   **分类器**：一个在含噪数据上训练至最优的概率分类器，得到受污染的后验概率 $g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid X=x)$。\n-   **干净后验概率**：$\\eta(x) = \\mathbb{P}(Y=1 \\mid X=x)$。\n-   **自训练 (ST) 规则**：\n    -   伪标签分配：$\\hat{Y}(x) = \\mathbf{1}\\{g(x) \\geq \\frac{1}{2}\\}$。\n    -   接受标准：如果 $|g(x) - \\frac{1}{2}| \\geq \\gamma$，则接受样本 $x$，其中 $\\gamma \\in [0, \\frac{1}{2})$ 是一个固定阈值。\n-   **目标**：\n    1.  推导 $g(x)$ 和 $\\eta(x)$ 之间的关系。\n    2.  为被接受的样本计算伪标签错误率的分布无关的最坏情况上界。\n    3.  找到最小置信度边距 $\\gamma_{\\star}(\\rho)$，使得对于任何满足 $\\gamma  \\frac{1}{2} - \\rho$ 的 $\\gamma \\geq \\gamma_{\\star}(\\rho)$，合并数据集（原始含噪集 + 被接受的伪标签集）的有效标签噪声率严格低于 $\\rho$。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述是科学合理的，基于统计学习的既定原则，特别是带标签噪声的学习和半监督学习。后验概率、对称噪声和自训练的概念在该领域是标准的。问题是适定的 (well-posed)，所有术语和条件都已精确定义，从而要求得出一个具体的、可推导的量。语言是客观和正式的。问题设定是自洽的，没有矛盾之处。约束 $\\rho \\in (0, \\frac{1}{2})$ 是标准的，确保标签不是完全随机的，而 $\\gamma  \\frac{1}{2} - \\rho$ 是一个一致的约束，其意义将在推导过程中变得清晰。该问题并非无足轻重，需要严谨的数学推导。\n\n### 步骤 3：结论与行动\n问题是**有效的**。将提供完整解答。\n\n### 推导过程\n解答按照问题陈述的要求分三个阶段进行。\n\n**1. $g(x)$ 和 $\\eta(x)$ 之间的关系**\n\n受污染的后验概率 $g(x)$ 定义为 $g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid X=x)$。使用全概率定律，我们可以通过对干净标签 $Y$ 取条件来展开此式：\n$$g(x) = \\mathbb{P}(\\tilde{Y}=1 \\mid Y=1, X=x)\\mathbb{P}(Y=1 \\mid X=x) + \\mathbb{P}(\\tilde{Y}=1 \\mid Y=0, X=x)\\mathbb{P}(Y=0 \\mid X=x)$$\n对称噪声模型指出，标签翻转概率独立于特征 $X$。因此，$\\mathbb{P}(\\tilde{Y}=1 \\mid Y=1, X=x) = \\mathbb{P}(\\tilde{Y}=1 \\mid Y=1) = 1-\\rho$ 且 $\\mathbb{P}(\\tilde{Y}=1 \\mid Y=0, X=x) = \\mathbb{P}(\\tilde{Y}=1 \\mid Y=0) = \\rho$。干净的后验概率由 $\\eta(x) = \\mathbb{P}(Y=1 \\mid X=x)$ 和 $1-\\eta(x) = \\mathbb{P}(Y=0 \\mid X=x)$ 给出。\n将这些代入 $g(x)$ 的方程中：\n$$g(x) = (1-\\rho)\\eta(x) + \\rho(1-\\eta(x))$$\n$$g(x) = (1-\\rho)\\eta(x) + \\rho - \\rho\\eta(x)$$\n$$g(x) = (1-2\\rho)\\eta(x) + \\rho$$\n这就是受污染的后验概率 $g(x)$ 和干净的后验概率 $\\eta(x)$ 之间的仿射关系。由于 $\\rho \\in (0, \\frac{1}{2})$，项 $1-2\\rho$ 非零，我们可以反转此关系，用 $g(x)$ 表示 $\\eta(x)$：\n$$\\eta(x) = \\frac{g(x) - \\rho}{1-2\\rho}$$\n\n**2. 最坏情况下的伪标签错误率**\n\n伪标签 $\\hat{Y}(x)$ 是基于受污染后验概率的贝叶斯决策规则分配的：$\\hat{Y}(x) = \\mathbf{1}\\{g(x) \\geq \\frac{1}{2}\\}$。如果一个样本的置信度边距 $|g(x) - \\frac{1}{2}|$ 至少为 $\\gamma$，它就会被接受用于自训练。这个接受条件将接受集划分为两个不相交的子集：\n-   集合 1：$\\{x \\mid g(x) \\geq \\frac{1}{2} + \\gamma\\}$。对于这些样本，伪标签为 $\\hat{Y}(x) = 1$。\n-   集合 2：$\\{x \\mid g(x) \\leq \\frac{1}{2} - \\gamma\\}$。对于这些样本，伪标签为 $\\hat{Y}(x) = 0$。\n\n对于给定的 $x$，伪标签错误率是伪标签 $\\hat{Y}(x)$ 与真实干净标签 $Y$ 不匹配的概率，即 $\\mathbb{P}(\\hat{Y}(x) \\neq Y \\mid X=x)$。让我们为任何被接受的样本找到此错误率的上界。\n\n对于集合 1 中的样本（$g(x) \\geq \\frac{1}{2} + \\gamma$ 且 $\\hat{Y}(x)=1$），如果真实标签是 $Y=0$，则发生错误。其概率为：\n$$\\mathbb{P}(Y=0 \\mid X=x) = 1 - \\eta(x) = 1 - \\frac{g(x) - \\rho}{1-2\\rho} = \\frac{(1-2\\rho) - (g(x) - \\rho)}{1-2\\rho} = \\frac{1 - \\rho - g(x)}{1-2\\rho}$$\n由于 $1-2\\rho  0$，这个错误概率是 $g(x)$ 的一个递减函数。为了找到最坏情况（最大）的上界，我们必须使用该集合中 $g(x)$ 的最小值，即 $g(x) = \\frac{1}{2} + \\gamma$。\n$$\\text{错误率 (集合 1)} \\leq \\frac{1 - \\rho - (\\frac{1}{2} + \\gamma)}{1-2\\rho} = \\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}$$\n\n对于集合 2 中的样本（$g(x) \\leq \\frac{1}{2} - \\gamma$ 且 $\\hat{Y}(x)=0$），如果真实标签是 $Y=1$，则发生错误。其概率为：\n$$\\mathbb{P}(Y=1 \\mid X=x) = \\eta(x) = \\frac{g(x) - \\rho}{1-2\\rho}$$\n由于 $1-2\\rho  0$，这个错误概率是 $g(x)$ 的一个递增函数。为了找到最坏情况的上界，我们必须使用该集合中 $g(x)$ 的最大值，即 $g(x) = \\frac{1}{2} - \\gamma$。\n$$\\text{错误率 (集合 2)} \\leq \\frac{(\\frac{1}{2} - \\gamma) - \\rho}{1-2\\rho} = \\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}$$\n两种情况都得出了相同的最坏情况错误上界。只要样本满足接受标准，这个上界就与 $x$ 和底层数据分布无关。令 $\\rho_{\\text{ST}}$ 表示接受集上的伪标签错误率。我们已经建立了一个分布无关的上界：\n$$\\rho_{\\text{ST}} \\leq \\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}$$\n\n**3. 最小置信度边距 $\\gamma_{\\star}(\\rho)$**\n\n问题要求找到一个条件，以确保合并后数据集的有效标签噪声率严格低于原始噪声率 $\\rho$。设原始的含噪有标签集为 $S_L$，其大小为 $n_L$；被接受的伪标签样本集为 $S_A$，其大小为 $n_A$。$S_L$ 中的错误数量约为 $n_L \\rho$。$S_A$ 中的错误数量为 $n_A \\rho_{\\text{ST}}$。\n有效噪声率 $\\rho_{\\text{eff}}$ 是总错误数除以合并数据集的总大小：\n$$\\rho_{\\text{eff}} = \\frac{n_L \\rho + n_A \\rho_{\\text{ST}}}{n_L + n_A}$$\n我们需要 $\\rho_{\\text{eff}}  \\rho$：\n$$\\frac{n_L \\rho + n_A \\rho_{\\text{ST}}}{n_L + n_A}  \\rho$$\n$$n_L \\rho + n_A \\rho_{\\text{ST}}  n_L \\rho + n_A \\rho$$\n假设 $n_A  0$（即自训练添加了一些样本），这个不等式简化为：\n$$\\rho_{\\text{ST}}  \\rho$$\n为保证此条件在与数据分布无关的情况下成立，我们必须强制要求 $\\rho_{\\text{ST}}$ 的最坏情况上界严格小于 $\\rho$：\n$$\\frac{\\frac{1}{2} - \\rho - \\gamma}{1-2\\rho}  \\rho$$\n由于 $\\rho \\in (0, \\frac{1}{2})$，我们有 $1-2\\rho > 0$。我们可以在不等式两边同乘以 $1-2\\rho$ 而不改变不等号的方向：\n$$\\frac{1}{2} - \\rho - \\gamma  \\rho(1-2\\rho)$$\n$$\\frac{1}{2} - \\rho - \\gamma  \\rho - 2\\rho^2$$\n现在，我们求解 $\\gamma$：\n$$\\frac{1}{2} - 2\\rho + 2\\rho^2  \\gamma$$\n这个不等式明确了 $\\gamma$ 必须满足的条件，以保证噪声率的降低。问题要求找到最小置信度边距 $\\gamma_{\\star}(\\rho)$，使得对于任何 $\\gamma \\geq \\gamma_{\\star}(\\rho)$，该条件都成立。这个最小值就是推导出的 $\\gamma$ 区间的下界。\n因此，最小置信度边距是：\n$$\\gamma_{\\star}(\\rho) = \\frac{1}{2} - 2\\rho + 2\\rho^2$$\n这也可以写成 $\\gamma_{\\star}(\\rho) = 2(\\rho - \\frac{1}{2})^2$。问题中的约束 $\\gamma  \\frac{1}{2} - \\rho$ 确保了 $\\gamma_{\\star}(\\rho)$ 是一个有效的选择，因为对于 $\\rho \\in (0, \\frac{1}{2})$，有 $2(\\rho - \\frac{1}{2})^2  \\frac{1}{2} - \\rho$。",
            "answer": "$$\\boxed{\\frac{1}{2} - 2\\rho + 2\\rho^2}$$"
        },
        {
            "introduction": "自训练的一个主要风险是，模型会自信地错误标记来自其从未见过的类别（即分布外数据）的数据，从而导致灾难性的误差累积。这个练习模拟了一种真实世界的防御机制，利用模型自身的不确定性（通过熵来衡量）来检测和拒绝这些异常值。你将应用贝叶斯决策理论来找到最优阈值，以平衡拒绝有用数据和接受有害数据之间的成本，这是构建稳健半监督系统的关键技能。",
            "id": "3172796",
            "problem": "一个带有参数 $\\theta$ 的分类器，通过对其 logits 应用 softmax 变换，为每个输入 $x$ 生成一个在 $K$ 个已见类别上的分类分布 $p_{\\theta}(y \\mid x)$。在采用自训练的半监督学习中，当模型足够自信时，未标记的输入 $\\{x_{i}\\}$ 会被赋予伪标签，否则将被搁置。然而，未标记样本池中也包含来自未见类别（分布外，OOD）的输入，必须防止这些输入被赋予伪标签，以避免误差累积。\n\n考虑一个异常检测器，它对香农熵 $H(p_{\\theta}(x)) = -\\sum_{j=1}^{K} p_{\\theta,j}(x) \\ln p_{\\theta,j}(x)$ 进行阈值化处理：如果 $H(p_{\\theta}(x)) \\leq \\tau$，则声明输入 $x$ 为分布内（有资格进行伪标记），否则声明其为异常（拒绝）。当 softmax logits $z_{j}(x)$ 经过良好校准时，该检测器在原理上等同于对能量分数 $E(x) = -\\ln\\left(\\sum_{j=1}^{K} \\exp(z_{j}(x))\\right)$ 进行阈值化，因为更高的不确定性（更平坦的 softmax）会同时增加熵和能量。\n\n假设在两种情况下，熵的科学合理模型如下：\n- 对于来自已见类别的输入，$H \\mid \\text{seen} \\sim \\mathcal{N}(\\mu_{s}, \\sigma^{2})$，其中 $\\mu_{s} = 0.4$，$\\sigma = 0.25$ (奈特)。\n- 对于来自未见类别的输入，$H \\mid \\text{unseen} \\sim \\mathcal{N}(\\mu_{u}, \\sigma^{2})$，其中 $\\mu_{u} = 1.3$，且 $\\sigma$ 相同，为 $0.25$ (奈特)。\n\n设未标记样本池中的先验概率为：已见类别输入的先验概率 $\\pi_{s} = 0.85$，未见类别输入的先验概率 $\\pi_{u} = 0.15$。假设误接受（将未见类别声明为已见类别，导致错误标记）的成本为 $C_{\\text{FA}} = 4$，而误拒绝（将已见类别声明为未见类别，导致放弃一个有用的伪标签）的成本为 $C_{\\text{FR}} = 1$。\n\n从贝叶斯决策理论的原理和上述定义出发，推导在此熵阈值规则下，最小化检测权衡的期望成本的贝叶斯最优阈值 $\\tau^{\\star}$。然后使用给定的参数计算 $\\tau^{\\star}$ 的数值。将最终数值答案四舍五入到四位有效数字，并以奈特为单位表示。",
            "solution": "问题要求我们根据模型预测的香农熵 $H$，为将输入分类为属于‘已见’类还是‘未见’类，求出贝叶斯最优决策阈值 $\\tau^{\\star}$。最优阈值是使与分类错误相关的总期望成本（贝叶斯风险）最小化的阈值。\n\n决策规则定义如下：\n- 如果观测到的熵 $h \\leq \\tau$，则声明为‘已见’。\n- 如果观测到的熵 $h  \\tau$，则声明为‘未见’。\n\n两种类型的错误及其相关成本是：\n1.  **误接受 (FA)**：将‘未见’输入声明为‘已见’。当真实类别为‘未见’但 $h \\leq \\tau$ 时发生。成本为 $C_{\\text{FA}} = 4$。\n2.  **误拒绝 (FR)**：将‘已见’输入声明为‘未见’。当真实类别为‘已见’但 $h  \\tau$ 时发生。成本为 $C_{\\text{FR}} = 1$。\n\n类别的先验概率给定为 $\\pi_{s} = P(\\text{seen}) = 0.85$ 和 $\\pi_{u} = P(\\text{unseen}) = 0.15$。\n\n每个类别的熵 $H$ 的条件分布给定为正态分布：\n- 对于‘已见’输入：$p(h|\\text{seen}) = p_s(h) = \\mathcal{N}(h; \\mu_s, \\sigma^2)$，其中 $\\mu_s = 0.4$，$\\sigma = 0.25$。\n- 对于‘未见’输入：$p(h|\\text{unseen}) = p_u(h) = \\mathcal{N}(h; \\mu_u, \\sigma^2)$，其中 $\\mu_u = 1.3$，$\\sigma = 0.25$。\n\n根据贝叶斯决策理论，我们必须最小化期望成本，即风险 $R(\\tau)$。风险是每种错误类型的成本乘以其发生概率的总和。\n\n误接受的概率是输入为‘未见’且被声明为‘已见’的联合概率：\n$$ P(\\text{FA}) = P(h \\leq \\tau, \\text{unseen}) = P(h \\leq \\tau | \\text{unseen}) P(\\text{unseen}) = \\pi_u \\int_{-\\infty}^{\\tau} p_u(h) \\, dh $$\n误拒绝的概率是输入为‘已见’且被声明为‘未见’的联合概率：\n$$ P(\\text{FR}) = P(h  \\tau, \\text{seen}) = P(h  \\tau | \\text{seen}) P(\\text{seen}) = \\pi_s \\int_{\\tau}^{\\infty} p_s(h) \\, dh $$\n\n总期望成本 $R(\\tau)$ 是成本与其概率乘积的总和：\n$$ R(\\tau) = C_{\\text{FA}} P(\\text{FA}) + C_{\\text{FR}} P(\\text{FR}) $$\n$$ R(\\tau) = C_{\\text{FA}} \\pi_u \\int_{-\\infty}^{\\tau} p_u(h) \\, dh + C_{\\text{FR}} \\pi_s \\int_{\\tau}^{\\infty} p_s(h) \\, dh $$\n\n为了找到最小化 $R(\\tau)$ 的最优阈值 $\\tau^{\\star}$，我们将 $R(\\tau)$ 对 $\\tau$ 求导，并令导数等于零。使用微积分基本定理（特别是莱布尼茨积分法则）：\n$$ \\frac{d}{d\\tau} \\int_{-\\infty}^{\\tau} f(x) \\, dx = f(\\tau) $$\n$$ \\frac{d}{d\\tau} \\int_{\\tau}^{\\infty} f(x) \\, dx = -f(\\tau) $$\n\n将此应用于风险函数 $R(\\tau)$：\n$$ \\frac{dR(\\tau)}{d\\tau} = C_{\\text{FA}} \\pi_u p_u(\\tau) + C_{\\text{FR}} \\pi_s (-p_s(\\tau)) $$\n令导数为零以找到最小值：\n$$ C_{\\text{FA}} \\pi_u p_u(\\tau^{\\star}) - C_{\\text{FR}} \\pi_s p_s(\\tau^{\\star}) = 0 $$\n$$ C_{\\text{FA}} \\pi_u p_u(\\tau^{\\star}) = C_{\\text{FR}} \\pi_s p_s(\\tau^{\\star}) $$\n这个方程定义了贝叶斯最优阈值 $\\tau^{\\star}$。它表明，在决策边界上，两个类别的加权似然相等，其中权重是成本和先验概率的乘积。\n\n现在，我们代入正态分布的概率密度函数 (PDF)：\n$$ p_s(h) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(h - \\mu_s)^2}{2\\sigma^2}\\right) $$\n$$ p_u(h) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(h - \\mu_u)^2}{2\\sigma^2}\\right) $$\n将这些代入 $\\tau = \\tau^{\\star}$ 的最优性条件中：\n$$ C_{\\text{FA}} \\pi_u \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\tau - \\mu_u)^2}{2\\sigma^2}\\right) = C_{\\text{FR}} \\pi_s \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\tau - \\mu_s)^2}{2\\sigma^2}\\right) $$\n归一化常数 $\\frac{1}{\\sqrt{2\\pi}\\sigma}$ 从两边消去：\n$$ C_{\\text{FA}} \\pi_u \\exp\\left(-\\frac{(\\tau - \\mu_u)^2}{2\\sigma^2}\\right) = C_{\\text{FR}} \\pi_s \\exp\\left(-\\frac{(\\tau - \\mu_s)^2}{2\\sigma^2}\\right) $$\n为了解出 $\\tau$，我们对两边取自然对数：\n$$ \\ln(C_{\\text{FA}} \\pi_u) - \\frac{(\\tau - \\mu_u)^2}{2\\sigma^2} = \\ln(C_{\\text{FR}} \\pi_s) - \\frac{(\\tau - \\mu_s)^2}{2\\sigma^2} $$\n重新整理各项以分离 $\\tau$：\n$$ \\frac{(\\tau - \\mu_s)^2}{2\\sigma^2} - \\frac{(\\tau - \\mu_u)^2}{2\\sigma^2} = \\ln(C_{\\text{FR}} \\pi_s) - \\ln(C_{\\text{FA}} \\pi_u) $$\n两边乘以 $2\\sigma^2$ 并展开平方项：\n$$ (\\tau^2 - 2\\tau\\mu_s + \\mu_s^2) - (\\tau^2 - 2\\tau\\mu_u + \\mu_u^2) = 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n$\\tau^2$ 项消掉：\n$$ 2\\tau\\mu_u - 2\\tau\\mu_s + \\mu_s^2 - \\mu_u^2 = 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n$$ 2\\tau(\\mu_u - \\mu_s) = \\mu_u^2 - \\mu_s^2 + 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n因式分解 $\\mu_u^2 - \\mu_s^2 = (\\mu_u - \\mu_s)(\\mu_u + \\mu_s)$:\n$$ 2\\tau(\\mu_u - \\mu_s) = (\\mu_u - \\mu_s)(\\mu_u + \\mu_s) + 2\\sigma^2 \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n由于 $\\mu_u \\neq \\mu_s$，我们可以两边除以 $2(\\mu_u - \\mu_s)$:\n$$ \\tau = \\frac{\\mu_u + \\mu_s}{2} + \\frac{\\sigma^2}{\\mu_u - \\mu_s} \\ln\\left(\\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u}\\right) $$\n这就是贝叶斯最优阈值 $\\tau^{\\star}$ 的解析表达式。\n\n现在，我们代入给定的数值：\n$\\mu_{s} = 0.4$, $\\mu_{u} = 1.3$, $\\sigma = 0.25$, $\\pi_{s} = 0.85$, $\\pi_{u} = 0.15$, $C_{\\text{FR}} = 1$, $C_{\\text{FA}} = 4$。\n\n第一项是均值的中点：\n$$ \\frac{\\mu_u + \\mu_s}{2} = \\frac{1.3 + 0.4}{2} = \\frac{1.7}{2} = 0.85 $$\n对数的参数是：\n$$ \\frac{C_{\\text{FR}} \\pi_s}{C_{\\text{FA}} \\pi_u} = \\frac{1 \\times 0.85}{4 \\times 0.15} = \\frac{0.85}{0.60} = \\frac{17}{12} $$\n其余项是：\n$$ \\mu_u - \\mu_s = 1.3 - 0.4 = 0.9 $$\n$$ \\sigma^2 = (0.25)^2 = 0.0625 $$\n将这些代入 $\\tau^{\\star}$ 的表达式中：\n$$ \\tau^{\\star} = 0.85 + \\frac{0.0625}{0.9} \\ln\\left(\\frac{17}{12}\\right) $$\n现在我们计算数值：\n$$ \\ln\\left(\\frac{17}{12}\\right) \\approx \\ln(1.41666...) \\approx 0.3483203 $$\n$$ \\frac{0.0625}{0.9} = \\frac{625 \\times 10^{-4}}{9 \\times 10^{-1}} = \\frac{625}{9000} = \\frac{5}{72} \\approx 0.0694444 $$\n将这些结合起来：\n$$ \\tau^{\\star} \\approx 0.85 + (0.0694444) \\times (0.3483203) $$\n$$ \\tau^{\\star} \\approx 0.85 + 0.0241889 $$\n$$ \\tau^{\\star} \\approx 0.8741889 $$\n将结果四舍五入到四位有效数字，得到 $0.8742$。\n单位是奈特，因为熵是使用自然对数计算的。",
            "answer": "$$\n\\boxed{0.8742}\n$$"
        }
    ]
}