{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of the Regression Discontinuity Design is that the treatment effect is identified by a discontinuity in the outcome variable at a precisely known cutoff. But what happens if the policy cutoff and the actual start of treatment are not perfectly aligned? This exercise  explores this critical pitfall, demonstrating how even a small misalignment can lead to a complete failure to identify the treatment effect, resulting in significant estimation bias.",
            "id": "3168533",
            "problem": "Consider a Regression Discontinuity Design (RDD), defined as the difference in the right-hand and left-hand limits of the conditional expectation of the observed outcome at a cutoff. Let the running variable be $X \\in \\mathbb{R}$ with a continuously differentiable density near a cutoff $c \\in \\mathbb{R}$. Assume the observed outcome $Y$ is generated by\n$$\nY = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon,\n$$\nwhere $f:\\mathbb{R}\\to\\mathbb{R}$ is continuous at $c$, $\\tau \\in \\mathbb{R}$ is a constant treatment effect, $\\Delta  0$ is a nonzero shift between the policy cutoff $c$ and the actual treatment start $c+\\Delta$, $\\varepsilon$ is a mean-zero disturbance satisfying $\\mathbb{E}[\\varepsilon \\mid X] = 0$, and $\\mathbb{1}\\{\\cdot\\}$ is the indicator function. There is no manipulation at the cutoff $c$ (the density of $X$ is continuous at $c$).\n\nYour tasks are:\n\n1) Using the definition of the RDD estimand as\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x],\n$$\nderive $\\theta_{\\text{RDD}}(c)$ under the given data-generating process, and express its bias relative to the treatment effect parameter $\\tau$.\n\n2) Consider a local-constant RDD estimator with a uniform kernel and symmetric bandwidth $h0$. That is, the estimator computes the difference between the sample average of $Y$ in the right-hand window $[c, c+h]$ and the sample average of $Y$ in the left-hand window $[c-h, c)$. Under large samples, a continuous density of $X$ at $c$, and the assumption that $f(x)$ is approximately constant in the neighborhood $[c-h, c+h]$ (i.e., $f(x) \\approx f(c)$ for $x \\in [c-h, c+h]$), derive the probability limit of this estimator and its bias relative to $\\tau$ as a function of $\\tau$, $\\Delta$, and $h$.\n\n3) Implement a program that computes the bias function derived in task 2 for the following test suite of parameter values. For each case, take $c=0$ (this choice does not affect the bias expression you derive) and return the bias as a real number (float):\n- Case A (happy path): $(\\tau, \\Delta, h) = (2.0, 0.3, 1.0)$.\n- Case B (boundary): $(\\tau, \\Delta, h) = (1.0, 1.0, 1.0)$.\n- Case C (edge with no treated in right window): $(\\tau, \\Delta, h) = (1.5, 1.2, 1.0)$.\n- Case D (near alignment): $(\\tau, \\Delta, h) = (0.75, 10^{-6}, 0.5)$.\n- Case E (negative treatment effect): $(\\tau, \\Delta, h) = (-1.5, 0.25, 0.5)$.\n\nThere are no physical units in this problem. All outputs must be expressed as real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where $r_i$ is the computed bias for the $i$-th case in the above order.",
            "solution": "The problem requires the analysis of a Regression Discontinuity Design (RDD) where the treatment assignment is shifted from the policy cutoff. We must first derive the theoretical RDD estimand and its bias, then derive the probability limit and bias of a practical local-constant estimator, and finally implement a program to compute this estimator's bias for specific parameter values.\n\n**Task 1: Derivation of the RDD Estimand and its Bias**\n\nThe RDD estimand is defined as the jump in the conditional expectation of the outcome $Y$ at the cutoff $c$:\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x]\n$$\nThe data-generating process is given by $Y = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon$. We first find the conditional expectation of $Y$ given $X=x$. Using the linearity of expectation and the assumption that $\\mathbb{E}[\\varepsilon \\mid X] = 0$, we have:\n$$\n\\mathbb{E}[Y \\mid X = x] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\} + \\mathbb{E}[\\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}\n$$\nNow, we evaluate the right-hand and left-hand limits at $x=c$.\n\nFor the right-hand limit, $x$ approaches $c$ from above, so $x  c$. Since the problem states $\\Delta  0$, for values of $x$ sufficiently close to $c$ (specifically, in the interval $(c, c+\\Delta)$), the condition $x \\ge c+\\Delta$ is false. Thus, the indicator function $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ is $0$. Given that $f(x)$ is continuous at $c$, the limit is:\n$$\n\\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\downarrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\nFor the left-hand limit, $x$ approaches $c$ from below, so $x  c$. Since $\\Delta  0$, it is always true that $x  c  c+\\Delta$. Therefore, the condition $x \\ge c+\\Delta$ is false, and the indicator function $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ is $0$. The limit is:\n$$\n\\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\uparrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\nSubstituting these limits into the definition of the RDD estimand gives:\n$$\n\\theta_{\\text{RDD}}(c) = f(c) - f(c) = 0\n$$\nThe bias of this estimand relative to the true treatment effect $\\tau$ is defined as $\\text{Bias} = \\theta_{\\text{RDD}}(c) - \\tau$. Therefore, the bias is:\n$$\n\\text{Bias} = 0 - \\tau = -\\tau\n$$\nThis result is intuitive: the RDD estimand looks for a discontinuity at $c$, but the actual discontinuity in the conditional mean function occurs at $c+\\Delta$. Due to the continuity of $f(x)$ at $c$, no jump is detected, leading to an estimate of $0$ and a bias of $-\\tau$.\n\n**Task 2: Derivation of the Estimator's Probability Limit and Bias**\n\nWe consider a local-constant RDD estimator, which is the difference in sample averages of $Y$ in the windows $[c, c+h]$ and $[c-h, c)$. In large samples (as $n \\to \\infty$), the probability limit (plim) of this estimator, denoted $\\hat{\\theta}_h$, is the difference in the true conditional expectations over these windows:\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h = \\mathbb{E}[Y \\mid c \\le X  c+h] - \\mathbb{E}[Y \\mid c-h \\le X  c]\n$$\nLet's analyze each term separately. For the left window, $[c-h, c)$, any value of $X$ satisfies $X  c  c+\\Delta$ (since $\\Delta  0$). Thus, $\\mathbb{1}\\{X \\ge c + \\Delta\\} = 0$ for all $X$ in this window.\n$$\n\\mathbb{E}[Y \\mid c-h \\le X  c] = \\mathbb{E}[f(X) + \\tau \\cdot 0 + \\varepsilon \\mid c-h \\le X  c] = \\mathbb{E}[f(X) \\mid c-h \\le X  c]\n$$\nUsing the assumption that $f(x) \\approx f(c)$ for $x \\in [c-h, c+h]$, this simplifies to:\n$$\n\\mathbb{E}[Y \\mid c-h \\le X  c] \\approx f(c)\n$$\nFor the right window, $[c, c+h]$, the indicator function is not always zero.\n$$\n\\mathbb{E}[Y \\mid c \\le X  c+h] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid c \\le X  c+h]\n$$\nUsing the approximation $f(x) \\approx f(c)$ and the law of iterated expectations, this becomes:\n$$\n\\mathbb{E}[Y \\mid c \\le X  c+h] \\approx f(c) + \\tau \\cdot \\mathbb{E}[\\mathbb{1}\\{X \\ge c + \\Delta\\} \\mid c \\le X  c+h]\n$$\nThe expectation of the indicator function is the conditional probability $P(X \\ge c+\\Delta \\mid c \\le X  c+h)$. This can be written as:\n$$\nP(X \\ge c+\\Delta \\mid c \\le X  c+h) = \\frac{P((X \\ge c+\\Delta) \\cap (c \\le X  c+h))}{P(c \\le X  c+h)}\n$$\nThe numerator is the probability of the intersection of two intervals, which is $P(\\max(c, c+\\Delta) \\le X  c+h) = P(c+\\Delta \\le X  c+h)$. This interval is non-empty only if $c+\\Delta  c+h$, which means $\\Delta  h$. If $\\Delta \\ge h$, the probability is $0$.\nLet $g(x)$ be the probability density function (PDF) of $X$. Since $g(x)$ is continuous at $c$, for a small bandwidth $h$, we can approximate the density as constant, $g(x) \\approx g(c)$, over the interval $[c, c+h]$.\nThe probability in the numerator is $\\int_{c+\\Delta}^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-(c+\\Delta)) = g(c) \\cdot (h-\\Delta)$, for $\\Delta  h$. If $\\Delta \\ge h$, it is $0$. This can be written as $g(c) \\cdot \\max(0, h-\\Delta)$.\nThe probability in the denominator is $\\int_c^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-c) = g(c) \\cdot h$.\nSo, the conditional probability is:\n$$\nP(X \\ge c+\\Delta \\mid c \\le X  c+h) \\approx \\frac{g(c) \\cdot \\max(0, h-\\Delta)}{g(c) \\cdot h} = \\frac{\\max(0, h-\\Delta)}{h}\n$$\nThe probability limit of the estimator is then:\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h \\approx \\left( f(c) + \\tau \\frac{\\max(0, h-\\Delta)}{h} \\right) - f(c) = \\tau \\frac{\\max(0, h-\\Delta)}{h}\n$$\nThe asymptotic bias is the difference between this probability limit and the true parameter $\\tau$:\n$$\n\\text{Bias} = \\text{plim}_{n \\to \\infty} \\hat{\\theta}_h - \\tau \\approx \\tau \\frac{\\max(0, h-\\Delta)}{h} - \\tau = \\tau \\left( \\frac{\\max(0, h-\\Delta)}{h} - 1 \\right)\n$$\nWe can simplify this expression. Note that $\\max(0, h-\\Delta) = h - \\min(h, \\Delta)$.\n$$\n\\text{Bias} \\approx \\tau \\left( \\frac{h-\\min(h, \\Delta)}{h} - 1 \\right) = \\tau \\left( 1 - \\frac{\\min(h, \\Delta)}{h} - 1 \\right) = -\\tau \\frac{\\min(h, \\Delta)}{h}\n$$\nThis is the final expression for the bias. It depends on the true effect $\\tau$, the policy misalignment $\\Delta$, and the estimator's bandwidth $h$. This formula is valid for both cases:\n1. If $\\Delta  h$, $\\min(h, \\Delta) = \\Delta$, so Bias $\\approx -\\tau \\frac{\\Delta}{h}$.\n2. If $\\Delta \\ge h$, $\\min(h, \\Delta) = h$, so Bias $\\approx -\\tau \\frac{h}{h} = -\\tau$.\n\n**Task 3: Numerical Computation of the Bias**\n\nWe will now implement a program to calculate the bias using the derived formula $\\text{Bias} = -\\tau \\frac{\\min(h, \\Delta)}{h}$ for the provided test cases, with $c=0$. The value of $c$ does not affect the bias formula.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local-constant RDD estimator with a misaligned cutoff.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (tau, Delta, h)\n    test_cases = [\n        # Case A (happy path)\n        (2.0, 0.3, 1.0),\n        # Case B (boundary)\n        (1.0, 1.0, 1.0),\n        # Case C (edge with no treated in right window)\n        (1.5, 1.2, 1.0),\n        # Case D (near alignment)\n        (0.75, 1e-6, 0.5),\n        # Case E (negative treatment effect)\n        (-1.5, 0.25, 0.5),\n    ]\n\n    def calculate_bias(tau: float, delta: float, h: float) - float:\n        \"\"\"\n        Calculates the asymptotic bias of the local-constant RDD estimator.\n\n        The formula for the bias is derived as:\n        Bias = -tau * min(h, delta) / h\n\n        Args:\n            tau: The true treatment effect.\n            delta: The shift between the policy cutoff and the treatment start.\n            h: The symmetric bandwidth of the estimator.\n\n        Returns:\n            The calculated bias as a float.\n        \"\"\"\n        # The derivation shows that the bandwidth h must be positive.\n        # The problem statement specifies h  0, so no division by zero error.\n        bias = -tau * min(h, delta) / h\n        return bias\n\n    results = []\n    for case in test_cases:\n        tau_val, delta_val, h_val = case\n        result = calculate_bias(tau_val, delta_val, h_val)\n        results.append(result)\n\n    # Format the final output string as a comma-separated list in brackets.\n    # The map(str, ...) converts each float result to its string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Even when the cutoff is correctly specified, the performance of an RDD estimator depends on the data available in its vicinity. Local polynomial regression, the workhorse method for RDD, relies on having sufficient data on both sides of the cutoff to make its estimates. This practice  uses a deterministic simulation to provide a hands-on look at how estimator bias can arise when the cutoff is located near the boundary of the data's support, leading to an asymmetric estimation window.",
            "id": "3168448",
            "problem": "You are given a task to algorithmically evaluate how truncation of the running variable near a boundary affects the bias of a local linear Regression Discontinuity (RD) estimator. Start from the foundational definition that, under continuity of the conditional outcome functions, the RD estimand at a cutoff is the jump in the limiting conditional mean of the observed outcome at the cutoff. Then design and implement an algorithm that constructs the estimator from first principles using only one-sided local linear regressions.\n\nData-generating setup:\n- The running variable is supported on a compact interval, specifically $X \\in [0,100]$. You must treat the support as truncated at the bounds, meaning no observations exist for $x \\lt 0$ or $x \\gt 100$.\n- Generate a deterministic, dense grid of running variable values: $M$ equally spaced points on $[0,100]$, with $M = 10001$, so that the grid spacing is $0.01$ and the grid is $\\{0,0.01,0.02,\\dots,100\\}$.\n- Define potential outcome mean functions $m_0(x)$ and $m_1(x)$ by\n  $$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2,\\quad m_1(x) = m_0(x) + \\tau,$$\n  where the constant treatment effect is $\\tau = 3$.\n- For a given cutoff $c$, define the observed outcome deterministically as\n  $$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\},$$\n  with no stochastic noise added.\n\nEstimator to implement:\n- For a specified bandwidth $h \\gt 0$, construct a one-sided local linear estimator on each side of the cutoff $c$, using only observations within the window $|X - c| \\le h$ and on the respective side of $c$.\n- Use the triangular kernel\n  $$K(u) = \\max\\{0, 1 - |u|\\},\\quad u = \\frac{X - c}{h}.$$\n- For the left side, use all points with $X \\lt c$ and $|X - c| \\le h$; for the right side, use all points with $X \\ge c$ and $|X - c| \\le h$.\n- On each side, fit a weighted least squares linear regression of the form\n  $$Y = \\alpha + \\beta\\,(X - c),$$\n  where the weights are $K\\big((X - c)/h\\big)$. Extract the fitted intercept $\\widehat{\\alpha}_-$ from the left-side fit and $\\widehat{\\alpha}_+$ from the right-side fit.\n- Define the RD estimate as\n  $$\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-.$$\n- Define the estimator bias for given $(c,h)$ as\n  $$\\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau.$$\n\nScientific goal:\n- Show algorithmically how proximity of the cutoff $c$ to a boundary (e.g., near $100$) increases the absolute bias due to asymmetric support on one side and curvature in $m_0(x)$ and $m_1(x)$.\n- Because there is no noise and the grid is dense and deterministic, the resulting biases are deterministic functions of $(c,h)$ and the data-generating process.\n\nTest suite:\n- Use the following five test cases for $(c,h)$:\n  1. $(c,h) = (50,10)$, a within-support interior case intended as a baseline.\n  2. $(c,h) = (95,10)$, near the upper boundary to illustrate increased bias.\n  3. $(c,h) = (98,10)$, even closer to the upper boundary to stress boundary effects.\n  4. $(c,h) = (95,5)$, same cutoff as case $2$ but a smaller bandwidth to assess bias reduction from reduced curvature exposure.\n  5. $(c,h) = (5,10)$, near the lower boundary to illustrate boundary effects on the opposite side.\n\nAnswer specification:\n- For each test case, compute the bias $\\mathrm{bias}(c,h)$ as a real number.\n- Your program must produce a single line of output containing the five results as a comma-separated list enclosed in square brackets, e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$, with each number printed as a decimal floating-point value. No units are involved. Angles are not used. If you choose to round, round to a fixed number of decimal places consistently across all outputs.\n\nYour program must be a complete, runnable implementation that performs the above computations exactly as specified and outputs only the single required line. No user input is permitted, and no external files may be used. The implementation language is specified separately and must be followed exactly.",
            "solution": "The problem requires the design and implementation of an algorithm to compute the bias of a local linear Regression Discontinuity (RD) estimator. The analysis focuses on how the bias is affected when the cutoff point is near the boundary of the data's support. The entire process is deterministic, using a dense grid of points and noise-free outcome functions.\n\nFirst, we formalize the problem setup based on the provided specifications. The running variable, denoted by $X$, has a compact support on the interval $[0, 100]$. We operate on a deterministic grid of $M = 10001$ equally spaced points within this interval, starting at $X=0$ and ending at $X=100$. The potential outcome mean functions are given by\n$$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2$$\n$$m_1(x) = m_0(x) + \\tau$$\nwhere the true treatment effect $\\tau$ is a constant equal to $3$. The observed outcome $Y$ for a given cutoff $c$ is deterministically defined as:\n$$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\}$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The RD estimand is the jump in the conditional expectation function at the cutoff, which in this noise-free setting is exactly $\\tau = m_1(c) - m_0(c) = 3$.\n\nThe task is to implement a one-sided local linear RD estimator. This involves performing a separate weighted least squares (WLS) regression on each side of the cutoff $c$. For a given bandwidth $h  0$, the data used for the estimation are restricted to the window $|X - c| \\le h$. The regression model to be fitted is:\n$$Y = \\alpha + \\beta\\,(X - c)$$\nThe weights are provided by the triangular kernel, $K(u) = \\max\\{0, 1 - |u|\\}$, where $u = (X - c)/h$.\n\nFor the right side of the cutoff, we minimize the weighted sum of squared residuals for all data points $X_i$ such that $c \\le X_i \\le \\min(c+h, 100)$:\n$$ \\min_{\\alpha_+, \\beta_+} \\sum_{i: c \\le X_i \\le c+h} K\\left(\\frac{X_i - c}{h}\\right) \\left[ Y_i - \\left(\\alpha_+ + \\beta_+ (X_i - c)\\right) \\right]^2 $$\nThe solution to this WLS problem yields the estimated intercept $\\widehat{\\alpha}_+$. Similarly, for the left side, we use data points $X_i$ such that $\\max(c-h, 0) \\le X_i  c$ to find the intercept $\\widehat{\\alpha}_-$. Note the strict inequality $X_i  c$ for the left side and the interaction with the support boundaries at $0$ and $100$.\n\nThe WLS estimator for the coefficient vector $\\mathbf{b} = [\\alpha, \\beta]^T$ can be expressed in matrix form as:\n$$ \\widehat{\\mathbf{b}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} (\\mathbf{X}^T \\mathbf{W} \\mathbf{y}) $$\nHere, $\\mathbf{y}$ is the vector of outcome values in the local window, $\\mathbf{X}$ is the design matrix with the first column being ones and the second column being the recentered variable $(X_i - c)$, and $\\mathbf{W}$ is a diagonal matrix containing the kernel weights $K((X_i-c)/h)$. The estimated intercept, $\\widehat{\\alpha}$, is the first element of the vector $\\widehat{\\mathbf{b}}$.\n\nOnce we have computed the intercepts from the left and right regressions, $\\widehat{\\alpha}_-$ and $\\widehat{\\alpha}_+$, the RD estimate of the treatment effect is:\n$$ \\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_- $$\nThe bias of this estimator for a given pair of $(c,h)$ is then calculated as:\n$$ \\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau $$\n\nThe scientific goal is to demonstrate how this bias changes as $c$ approaches a boundary of the support. The bias of a local linear estimator is principally due to the mismatch between the linear model and the true underlying function, which in this case has non-zero curvature ($m_0''(x) = 0.004$). For an interior point $c$ with a symmetric window $[c-h, c+h]$, the biases in $\\widehat{\\alpha}_+$ and $\\widehat{\\alpha}_-$ tend to have a similar structure and partially cancel when their difference is taken. However, when $c$ is near a boundary (e.g., $c=95$ with $h=10$), one of the estimation windows is truncated (e.g., the right-side window becomes $[95, 100]$ instead of $[95, 105]$). This truncation creates an asymmetric effective kernel, which breaks the symmetry of the bias terms. The cancellation is no longer effective, resulting in a larger overall bias in $\\widehat{\\tau}_{\\mathrm{RD}}$. Reducing the bandwidth $h$ shortens the estimation window, making the linear approximation more accurate over the smaller range and thus reducing the magnitude of the curvature-induced bias, even in boundary cases.\n\nThe algorithm proceeds by iterating through each test case $(c,h)$:\n1.  Generate the full set of outcome values $Y_i$ across the grid of $X_i \\in [0, 100]$.\n2.  For the given $(c,h)$, perform the WLS procedure for the left side, using data from $[\\max(0, c-h), c)$, to obtain $\\widehat{\\alpha}_-$.\n3.  Perform the WLS procedure for the right side, using data from $[c, \\min(100, c+h)]$, to obtain $\\widehat{\\alpha}_+$.\n4.  Calculate $\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-$.\n5.  Calculate and store the bias $\\widehat{\\tau}_{\\mathrm{RD}} - 3$.\nThis procedure is repeated for all five test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local linear Regression Discontinuity (RD) estimator\n    for several test cases, demonstrating boundary effects.\n    \"\"\"\n\n    # Define constants from the problem statement\n    M = 10001\n    TAU = 3.0\n\n    # Generate the deterministic grid for the running variable X\n    X_grid = np.linspace(0, 100, M)\n\n    def m0(x):\n        \"\"\"\n        Defines the potential outcome mean function for the control group.\n        \"\"\"\n        return 2.0 + 0.04 * x + 0.002 * x**2\n\n    def triangular_kernel(u):\n        \"\"\"\n        Defines the triangular kernel function.\n        \"\"\"\n        return np.maximum(0, 1 - np.abs(u))\n\n    def local_linear_fit(X_grid, Y_values, c, h, side):\n        \"\"\"\n        Performs a one-sided local linear regression and returns the\n        estimated intercept.\n\n        Args:\n            X_grid (np.ndarray): The grid of running variable values.\n            Y_values (np.ndarray): The corresponding observed outcome values.\n            c (float): The cutoff point.\n            h (float): The bandwidth.\n            side (str): 'left' or 'right' of the cutoff.\n\n        Returns:\n            float: The estimated intercept (alpha_hat).\n        \"\"\"\n        # Select data points based on side, bandwidth, and support [0, 100]\n        if side == 'right':\n            mask = (X_grid = c)  (X_grid = c + h)\n        elif side == 'left':\n            mask = (X_grid  c)  (X_grid = c - h)\n        else:\n            raise ValueError(\"Side must be 'left' or 'right'\")\n        \n        X_local = X_grid[mask]\n        Y_local = Y_values[mask]\n\n        if len(X_local)  2:\n            # Not enough points for a linear fit. This case is not expected\n            # with the problem's dense grid.\n            return np.nan\n\n        # Recenter the running variable\n        X_centered = X_local - c\n        \n        # Calculate kernel weights based on u = (X - c) / h\n        u = X_centered / h\n        weights = triangular_kernel(u)\n        \n        # Set up the weighted least squares problem: Y = alpha + beta * (X - c)\n        # Construct the design matrix.\n        design_matrix = np.vstack([np.ones(len(X_centered)), X_centered]).T\n\n        # Solve the normal equations: (X'WX)b = X'Wy\n        # This is more efficient than creating a diagonal weight matrix.\n        XT_W_X = design_matrix.T @ (weights[:, np.newaxis] * design_matrix)\n        XT_W_y = design_matrix.T @ (weights * Y_local)\n\n        try:\n            # Solve for the coefficient vector [alpha, beta]\n            coeffs = np.linalg.solve(XT_W_X, XT_W_y)\n            alpha_hat = coeffs[0]\n        except np.linalg.LinAlgError:\n            # This would occur if the matrix is singular.\n            alpha_hat = np.nan\n            \n        return alpha_hat\n\n    def compute_bias(c, h, X_grid, m0_func, tau_val):\n        \"\"\"\n        Computes the RD estimator bias for a given (c, h) pair.\n        \"\"\"\n        # Generate the observed outcome Y based on the cutoff c\n        Y_values = m0_func(X_grid) + tau_val * (X_grid = c)\n        \n        # Get intercept estimates from the left and right sides\n        alpha_hat_plus = local_linear_fit(X_grid, Y_values, c, h, side='right')\n        alpha_hat_minus = local_linear_fit(X_grid, Y_values, c, h, side='left')\n        \n        # Compute the RD estimate of the treatment effect\n        tau_hat_rd = alpha_hat_plus - alpha_hat_minus\n        \n        # Compute the final bias\n        bias = tau_hat_rd - tau_val\n        return bias\n\n    # Define the test suite of (c, h) pairs\n    test_cases = [\n        (50, 10),\n        (95, 10),\n        (98, 10),\n        (95, 5),\n        (5, 10),\n    ]\n\n    results = []\n    for c, h in test_cases:\n        bias_result = compute_bias(c, h, X_grid, m0, TAU)\n        results.append(bias_result)\n\n    # Format the output as a comma-separated list in brackets,\n    # with consistent decimal formatting.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond identifying potential pitfalls, a key task in statistical learning is to optimize our estimators for the best possible performance. In RDD, this often involves minimizing the Mean Squared Error (MSE), which balances a trade-off between estimation bias and variance. This advanced practice  guides you through the process of comparing two popular kernel functions, showing how their mathematical properties translate into different MSE performance, particularly when the underlying functions have curvature.",
            "id": "3168499",
            "problem": "You are asked to derive and implement a principled comparison of two kernels in a one-sided local polynomial regression discontinuity design (RD). Consider a sharp Regression Discontinuity Design (RD) at a cutoff $c$, where the treatment effect is the jump $\\tau = m_{+}(c) - m_{-}(c)$ in the conditional expectation functions $m_{+}(x)$ and $m_{-}(x)$ on the right and left of $c$, respectively. Assume one-sided local linear regression (polynomial order $1$) is used on each side of $c$ with a common bandwidth $h$, and let the normalized running-variable coordinates be $u_{+} = (x - c)/h$ for the right and $u_{-} = (c - x)/h$ for the left, so that one-sided kernels have support on $[0,1]$.\n\nStart from the following fundamental base:\n- Weighted Least Squares (WLS) defines the local polynomial estimator as the minimizer over intercept and slope that minimizes the weighted residual sum of squares, where weights are given by a kernel $K(u)$ applied to the normalized distance from the cutoff.\n- Smoothness of $m_{+}(x)$ and $m_{-}(x)$ implies a second-order Taylor expansion around $x=c$ is valid, yielding leading-order terms that govern the asymptotic bias when using local linear regression at a boundary.\n- Under standard sampling, independence, and regularity conditions in statistical learning for local polynomial regression, the law of large numbers and the central limit theorem imply that the design matrices built from kernel-weighted monomials converge to their population moment matrices. These moments are defined through integrals of $K(u)$ and $K(u)^2$ against powers of $u$ over the one-sided support.\n\nYour task is to:\n1. Define the one-sided kernel moment matrices for any kernel $K(u)$ supported on $[0,1]$:\n   - Let $p(u) = [1, u]^{\\top}$.\n   - Define the moment matrix $S = \\int_{0}^{1} K(u) p(u) p(u)^{\\top} \\, du$ with entries $S_{11} = \\int_{0}^{1} K(u) \\, du$, $S_{12}=S_{21} = \\int_{0}^{1} K(u) u \\, du$, $S_{22} = \\int_{0}^{1} K(u) u^{2} \\, du$.\n   - Define the bias-loading vector $R = \\int_{0}^{1} K(u) u^{2} p(u) \\, du$, i.e., $R = \\left[\\int_{0}^{1} K(u) u^{2} \\, du, \\int_{0}^{1} K(u) u^{3} \\, du \\right]^{\\top}$.\n   - Define the variance-loading matrix $\\Xi = \\int_{0}^{1} K(u)^{2} p(u) p(u)^{\\top} \\, du$ with entries $\\Xi_{11} = \\int_{0}^{1} K(u)^{2} \\, du$, $\\Xi_{12}=\\Xi_{21} = \\int_{0}^{1} K(u)^{2} u \\, du$, $\\Xi_{22} = \\int_{0}^{1} K(u)^{2} u^{2} \\, du$.\n2. Using the above, derive the leading-order asymptotic bias and variance of the one-sided local linear intercept on each side of $c$ in terms of $S$, $R$, and $\\Xi$, and then derive the corresponding quantities for the RD estimator $\\hat{\\tau}$ as the difference of right and left intercepts. Explicitly show how the curvature at the cutoff enters through the second derivatives $m_{+}''(c)$ and $m_{-}''(c)$.\n3. Specialize the derivation to two kernels:\n   - The triangular kernel: $K(u) = 1 - u$ for $u \\in [0,1]$ and $K(u)=0$ otherwise.\n   - The Epanechnikov kernel: $K(u) = \\frac{3}{4}(1 - u^{2})$ for $u \\in [0,1]$ and $K(u)=0$ otherwise.\n   Compute the required kernel integrals exactly, assemble $S$, $R$, and $\\Xi$, and evaluate the scalar bias and variance constants that apply to the RD estimator when both sides use the same kernel and bandwidth.\n4. Quantify and compare the mean squared error (MSE) of $\\hat{\\tau}$ for the two kernels under nonzero curvature at the cutoff, expressed via the curvature difference $\\Delta_{2} = m_{+}''(c) - m_{-}''(c) \\neq 0$, common bandwidth $h$, sample size $n$, right and left densities $g_{+}(c)$ and $g_{-}(c)$ of the running variable at $c$, and right and left noise variances $\\sigma_{+}^{2}$ and $\\sigma_{-}^{2}$. Your program must implement the derived formulas to compute $MSE_{\\text{tri}}$ and $MSE_{\\text{epa}}$, and then output $MSE_{\\text{tri}} - MSE_{\\text{epa}}$ for each test case listed below.\n5. Use the following test suite. For each test case, parameters are provided as a tuple $(n, h, g_{+}(c), g_{-}(c), \\sigma_{+}, \\sigma_{-}, \\Delta_{2})$. No physical units apply. Angles are not involved. The final answers are real numbers.\n   - Case 1 (balanced, moderate bandwidth, moderate curvature): $(10000, 0.2, 1.0, 1.0, 1.0, 1.0, 1.5)$.\n   - Case 2 (balanced, small bandwidth, stronger curvature): $(10000, 0.05, 1.0, 1.0, 1.0, 1.0, 2.0)$.\n   - Case 3 (balanced density, heteroskedastic sides): $(5000, 0.15, 0.8, 0.8, 1.0, 2.0, 1.0)$.\n   - Case 4 (imbalanced density, equal noise, negative curvature difference): $(8000, 0.12, 1.0, 0.5, 1.2, 1.2, -1.2)$.\n   - Case 5 (balanced density, large curvature, moderate bandwidth): $(5000, 0.25, 0.9, 0.9, 1.0, 1.0, 5.0)$.\n   - Case 6 (balanced density, very large bandwidth, large curvature): $(3000, 0.5, 1.0, 1.0, 1.0, 1.0, 5.0)$.\n6. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a float rounded to six decimal places, corresponding in order to the above test cases. For example, an output could look like $[0.001234,0.000567,-0.000890,0.000012,0.000345,-0.001234]$.",
            "solution": "We present the derivation and algorithm beginning from the fundamental definitions of local polynomial regression at a boundary within a sharp Regression Discontinuity Design (RD).\n\nSetup and definitions. A sharp RD at cutoff $c$ has conditional expectations $m_{+}(x)$ for $x \\ge c$ and $m_{-}(x)$ for $x  c$. The parameter of interest is $\\tau = m_{+}(c) - m_{-}(c)$. We estimate $\\tau$ using the difference of two one-sided local linear regressions. For the right side, define $u = (x - c)/h$ and restrict to $u \\in [0,1]$ using a one-sided kernel $K(u)$; for the left side, define $u = (c - x)/h$, again with $u \\in [0,1]$. On each side, we fit a local linear regression (polynomial order $1$). Let $p(u) = [1, u]^{\\top}$. Denote the local design moment matrices\n$$\nS = \\int_{0}^{1} K(u) p(u) p(u)^{\\top} \\, du\n= \\begin{bmatrix}\n\\mu_{0}  \\mu_{1} \\\\\n\\mu_{1}  \\mu_{2}\n\\end{bmatrix},\n$$\nwhere $\\mu_{j} = \\int_{0}^{1} K(u) u^{j} \\, du$. The bias-loading vector\n$$\nR = \\int_{0}^{1} K(u) u^{2} p(u) \\, du\n= \\begin{bmatrix}\n\\mu_{2} \\\\ \\mu_{3}\n\\end{bmatrix},\n$$\nwith $\\mu_{3} = \\int_{0}^{1} K(u) u^{3} \\, du$. The variance-loading matrix\n$$\n\\Xi = \\int_{0}^{1} K(u)^{2} p(u) p(u)^{\\top} \\, du\n= \\begin{bmatrix}\n\\kappa_{0}  \\kappa_{1} \\\\\n\\kappa_{1}  \\kappa_{2}\n\\end{bmatrix},\n$$\nwhere $\\kappa_{j} = \\int_{0}^{1} K(u)^{2} u^{j} \\, du$. These integrals arise from the law of large numbers and central limit theorem applied to the weighted regressors and errors.\n\nOne-sided local linear intercept and its asymptotics. Let the one-sided local linear estimator of $m(c)$ on a given side be the intercept $\\hat{\\alpha}$ from the solution to weighted least squares with weights $K(u)$ and regressors $p(u)$. Under smoothness, we use the second-order Taylor expansion\n$$\nm(c + h u) = m(c) + m'(c) h u + \\tfrac{1}{2} m''(c) h^{2} u^{2} + o(h^{2})\n$$\nfor the right side, and similarly\n$$\nm(c - h u) = m(c) - m'(c) h u + \\tfrac{1}{2} m''(c) h^{2} u^{2} + o(h^{2})\n$$\nfor the left side. The leading asymptotic bias of the one-sided local linear intercept is driven by the curvature term $\\tfrac{1}{2} m''(c) h^{2}$ interacting with the design. The standard local polynomial theory implies\n$$\n\\text{Bias}(\\hat{\\alpha}) = \\tfrac{1}{2} h^{2} m''(c) \\cdot B_{K} + o(h^{2}), \\quad \\text{where} \\quad B_{K} = e_{1}^{\\top} S^{-1} R,\n$$\nwith $e_{1} = [1, 0]^{\\top}$. The leading asymptotic variance is\n$$\n\\text{Var}(\\hat{\\alpha}) = \\frac{\\sigma^{2}}{n h g(c)} \\cdot V_{K} + o\\!\\left(\\frac{1}{n h}\\right), \\quad \\text{where} \\quad V_{K} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1}.\n$$\nHere $n$ is the sample size, $h$ is the bandwidth, $g(c)$ is the density of the running variable at $c$ on the relevant side, and $\\sigma^{2}$ is the error variance on that side. These expressions follow from the WLS normal equations, moment convergence to $S$, $R$, and $\\Xi$, and standard variance propagation in local polynomial regression.\n\nRD estimator as difference of one-sided intercepts. The RD estimator is the difference of the right and left intercepts, $\\hat{\\tau} = \\hat{\\alpha}_{+} - \\hat{\\alpha}_{-}$. Its leading asymptotic bias is the difference of the two one-sided biases:\n$$\n\\text{Bias}(\\hat{\\tau}) = \\tfrac{1}{2} h^{2} \\left( m_{+}''(c) - m_{-}''(c) \\right) B_{K} + o(h^{2})\n= \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} + o(h^{2}),\n$$\nwhere $\\Delta_{2} = m_{+}''(c) - m_{-}''(c)$ is the curvature difference at the cutoff. The two one-sided intercept estimators use disjoint samples (on either side of $c$), so the leading variance adds:\n$$\n\\text{Var}(\\hat{\\tau}) = \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} V_{K} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} V_{K} + o\\!\\left(\\frac{1}{n h}\\right)\n= V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right) + o\\!\\left(\\frac{1}{n h}\\right).\n$$\nTherefore, the leading mean squared error (MSE) of $\\hat{\\tau}$ is\n$$\n\\text{MSE}(\\hat{\\tau}) \\approx \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2} + V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right).\n$$\n\nKernel-specific integrals. We now compute $S$, $R$, and $\\Xi$ for the two kernels.\n\n1. Triangular kernel $K(u) = 1 - u$ on $[0,1]$:\n   - Moments $\\mu_{j} = \\int_{0}^{1} (1 - u) u^{j} \\, du = \\frac{1}{(j+1)(j+2)}$, hence\n     $$\n     \\mu_{0} = \\tfrac{1}{2}, \\quad \\mu_{1} = \\tfrac{1}{6}, \\quad \\mu_{2} = \\tfrac{1}{12}, \\quad \\mu_{3} = \\tfrac{1}{20}.\n     $$\n   - Squared-kernel moments $\\kappa_{j} = \\int_{0}^{1} (1 - u)^{2} u^{j} \\, du\n     = \\frac{1}{j+1} - \\frac{2}{j+2} + \\frac{1}{j+3}$, hence\n     $$\n     \\kappa_{0} = \\tfrac{1}{3}, \\quad \\kappa_{1} = \\tfrac{1}{12}, \\quad \\kappa_{2} = \\tfrac{1}{30}.\n     $$\n   - Assemble $S = \\begin{bmatrix} \\tfrac{1}{2}  \\tfrac{1}{6} \\\\ \\tfrac{1}{6}  \\tfrac{1}{12} \\end{bmatrix}$, $R = \\begin{bmatrix} \\tfrac{1}{12} \\\\ \\tfrac{1}{20} \\end{bmatrix}$, and $\\Xi = \\begin{bmatrix} \\tfrac{1}{3}  \\tfrac{1}{12} \\\\ \\tfrac{1}{12}  \\tfrac{1}{30} \\end{bmatrix}$.\n   - Invert $S$ and evaluate the constants:\n     $$\n     S^{-1} = \\begin{bmatrix} 6  -12 \\\\ -12  36 \\end{bmatrix}, \\quad\n     B_{\\text{tri}} = e_{1}^{\\top} S^{-1} R = 6 \\cdot \\tfrac{1}{12} - 12 \\cdot \\tfrac{1}{20} = -\\tfrac{1}{10},\n     $$\n     $$\n     V_{\\text{tri}} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1} = \\tfrac{24}{5}.\n     $$\n\n2. Epanechnikov kernel $K(u) = \\frac{3}{4}(1 - u^{2})$ on $[0,1]$:\n   - Moments $\\mu_{j} = \\frac{3}{4}\\left( \\frac{1}{j+1} - \\frac{1}{j+3} \\right)$, hence\n     $$\n     \\mu_{0} = \\tfrac{1}{2}, \\quad \\mu_{1} = \\tfrac{3}{16}, \\quad \\mu_{2} = \\tfrac{1}{10}, \\quad \\mu_{3} = \\tfrac{1}{16}.\n     $$\n   - Squared-kernel moments use $K(u)^{2} = \\frac{9}{16}(1 - 2u^{2} + u^{4})$, so\n     $$\n     \\kappa_{j} = \\frac{9}{16}\\left( \\frac{1}{j+1} - \\frac{2}{j+3} + \\frac{1}{j+5} \\right),\n     $$\n     hence\n     $$\n     \\kappa_{0} = \\tfrac{3}{10}, \\quad \\kappa_{1} = \\tfrac{3}{32}, \\quad \\kappa_{2} = \\tfrac{3}{70}.\n     $$\n   - Assemble $S = \\begin{bmatrix} \\tfrac{1}{2}  \\tfrac{3}{16} \\\\ \\tfrac{3}{16}  \\tfrac{1}{10} \\end{bmatrix}$, $R = \\begin{bmatrix} \\tfrac{1}{10} \\\\ \\tfrac{1}{16} \\end{bmatrix}$, and $\\Xi = \\begin{bmatrix} \\tfrac{3}{10}  \\tfrac{3}{32} \\\\ \\tfrac{3}{32}  \\tfrac{3}{70} \\end{bmatrix}$.\n   - Invert $S$ and evaluate the constants:\n     $$\n     S^{-1} = \\begin{bmatrix} \\tfrac{128}{19}  -\\tfrac{240}{19} \\\\ -\\tfrac{240}{19}  \\tfrac{640}{19} \\end{bmatrix}, \\quad\n     B_{\\text{epa}} = e_{1}^{\\top} S^{-1} R = \\tfrac{128}{19} \\cdot \\tfrac{1}{10} - \\tfrac{240}{19} \\cdot \\tfrac{1}{16} = -\\tfrac{11}{95},\n     $$\n     $$\n     V_{\\text{epa}} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1} = \\left(\\tfrac{128}{19}\\right)^{2} \\cdot \\tfrac{3}{10}\n     + 2 \\cdot \\left(\\tfrac{128}{19}\\right)\\left(-\\tfrac{240}{19}\\right) \\cdot \\tfrac{3}{32}\n     + \\left(-\\tfrac{240}{19}\\right)^{2} \\cdot \\tfrac{3}{70},\n     $$\n     which numerically evaluates to approximately $V_{\\text{epa}} \\approx 4.498$.\n\nMean squared error (MSE) of $\\hat{\\tau}$. Combining bias and variance,\n$$\n\\text{MSE}_{K}(\\hat{\\tau}) \\approx \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2}\n+ V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right),\n$$\nfor kernel $K \\in \\{\\text{triangular}, \\text{Epanechnikov}\\}$. Because $B_{\\text{tri}}$ and $B_{\\text{epa}}$ are both negative, the bias sign depends on $\\Delta_{2}$, but the MSE depends on the square of the bias and the sum of variances. Typically, the Epanechnikov kernel has a lower variance constant while the triangular kernel has a smaller magnitude bias constant.\n\nAlgorithm design. The program:\n- Computes $\\mu_{j}$ and $\\kappa_{j}$ for $j \\in \\{0,1,2,3\\}$ or $\\{0,1,2\\}$ using closed-form integrals for triangular and Epanechnikov kernels.\n- Forms $S$, $R$, and $\\Xi$, inverts $S$, and evaluates $B_{K}$ and $V_{K}$.\n- For each test case $(n, h, g_{+}, g_{-}, \\sigma_{+}, \\sigma_{-}, \\Delta_{2})$, computes\n  $$\n  \\text{MSE}_{K} = \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2}\n  + V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}} + \\frac{\\sigma_{-}^{2}}{n h g_{-}} \\right),\n  $$\n  and then outputs $\\text{MSE}_{\\text{tri}} - \\text{MSE}_{\\text{epa}}$ rounded to six decimals.\nThis approach integrates local polynomial RD theory with explicit kernel moment calculations to quantify differences in mean squared error attributable to kernel choice under nonzero curvature at the cutoff.",
            "answer": "```python\n# language: Python 3.12\n# libraries: numpy==1.23.5, scipy==1.11.4 (not used)\nimport numpy as np\n\ndef kernel_moments_triangular():\n    # Triangular kernel K(u) = 1 - u on [0,1]\n    # mu_j = ∫_0^1 (1 - u) u^j du = 1/((j+1)(j+2))\n    mu0 = 1.0 / (1 * 2)\n    mu1 = 1.0 / (2 * 3)\n    mu2 = 1.0 / (3 * 4)\n    mu3 = 1.0 / (4 * 5)\n    # kappa_j = ∫_0^1 (1 - u)^2 u^j du = 1/(j+1) - 2/(j+2) + 1/(j+3)\n    def kappa(j):\n        return 1.0 / (j + 1) - 2.0 / (j + 2) + 1.0 / (j + 3)\n    k0 = kappa(0)\n    k1 = kappa(1)\n    k2 = kappa(2)\n    return (mu0, mu1, mu2, mu3), (k0, k1, k2)\n\ndef kernel_moments_epanechnikov():\n    # Epanechnikov kernel K(u) = (3/4)(1 - u^2) on [0,1]\n    # mu_j = (3/4)[1/(j+1) - 1/(j+3)]\n    def mu(j):\n        return (3.0 / 4.0) * (1.0 / (j + 1) - 1.0 / (j + 3))\n    mu0 = mu(0)\n    mu1 = mu(1)\n    mu2 = mu(2)\n    mu3 = mu(3)\n    # kappa_j = ∫_0^1 K(u)^2 u^j du = (9/16)[1/(j+1) - 2/(j+3) + 1/(j+5)]\n    def kappa(j):\n        return (9.0 / 16.0) * (1.0 / (j + 1) - 2.0 / (j + 3) + 1.0 / (j + 5))\n    k0 = kappa(0)\n    k1 = kappa(1)\n    k2 = kappa(2)\n    return (mu0, mu1, mu2, mu3), (k0, k1, k2)\n\ndef bias_variance_constants(mus, kappas):\n    # Build S, R, Xi and compute B_K and V_K\n    mu0, mu1, mu2, mu3 = mus\n    k0, k1, k2 = kappas\n    S = np.array([[mu0, mu1],\n                  [mu1, mu2]], dtype=float)\n    R = np.array([mu2, mu3], dtype=float)\n    Xi = np.array([[k0, k1],\n                   [k1, k2]], dtype=float)\n    S_inv = np.linalg.inv(S)\n    e1 = np.array([1.0, 0.0], dtype=float)\n    B = float(e1 @ S_inv @ R)\n    V = float(e1 @ S_inv @ Xi @ S_inv @ e1)\n    return B, V\n\ndef mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, kernel):\n    # Compute MSE for RD estimator using given kernel\n    if kernel == 'triangular':\n        mus, kappas = kernel_moments_triangular()\n    elif kernel == 'epanechnikov':\n        mus, kappas = kernel_moments_epanechnikov()\n    else:\n        raise ValueError(\"Unsupported kernel\")\n    B, V = bias_variance_constants(mus, kappas)\n    bias = 0.5 * (h ** 2) * delta2 * B\n    var_part = V * ((sigma_plus ** 2) / (n * h * g_plus) + (sigma_minus ** 2) / (n * h * g_minus))\n    mse = bias ** 2 + var_part\n    return mse\n\ndef solve():\n    # Define the test cases as specified in the problem statement:\n    # Each case: (n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2)\n    test_cases = [\n        (10000, 0.2, 1.0, 1.0, 1.0, 1.0, 1.5),      # Case 1\n        (10000, 0.05, 1.0, 1.0, 1.0, 1.0, 2.0),     # Case 2\n        (5000, 0.15, 0.8, 0.8, 1.0, 2.0, 1.0),      # Case 3\n        (8000, 0.12, 1.0, 0.5, 1.2, 1.2, -1.2),     # Case 4\n        (5000, 0.25, 0.9, 0.9, 1.0, 1.0, 5.0),      # Case 5\n        (3000, 0.5, 1.0, 1.0, 1.0, 1.0, 5.0),       # Case 6\n    ]\n\n    results = []\n    for n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2 in test_cases:\n        mse_tri = mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, 'triangular')\n        mse_epa = mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, 'epanechnikov')\n        diff = mse_tri - mse_epa\n        results.append(diff)\n\n    # Round to six decimals and print in the exact required format\n    formatted = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```"
        }
    ]
}