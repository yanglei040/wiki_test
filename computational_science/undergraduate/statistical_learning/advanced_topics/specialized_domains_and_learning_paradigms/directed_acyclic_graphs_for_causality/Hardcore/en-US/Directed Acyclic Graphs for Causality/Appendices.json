{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will tackle one of the most famous statistical puzzles: Simpson's Paradox. This paradox demonstrates how a statistical association observed in a total population can be reversed within the subgroups that make up that population. By computationally constructing a dataset that exhibits this phenomenon , you will gain a concrete understanding of how a confounding variable, represented by $Z$ in the corresponding Directed Acyclic Graph, can distort the relationship between a cause $X$ and an effect $Y$.",
            "id": "3115837",
            "problem": "You are given a Directed Acyclic Graph (DAG) that encodes a causal structure for Simpson’s paradox. The graph has vertices $X$, $Y$, $Z$ with directed edges $Z \\to X$, $Z \\to Y$, and $X \\to Y$. This causal structure implies the Markov factorization of the joint distribution $p(Z,X,Y)$ as $p(Z) p(X \\mid Z) p(Y \\mid X,Z)$. The paradox to demonstrate is that $X$ and $Y$ can be negatively associated marginally but positively associated when conditioned on $Z$.\n\nYour task is to write a program that, for several parameterized cases consistent with the DAG factorization, constructs an explicit $2 \\times 2 \\times 2$ count table of $(X,Y,Z)$ with $X \\in \\{0,1\\}$, $Y \\in \\{0,1\\}$, $Z \\in \\{0,1\\}$ by multiplying a total sample size by exact rational probabilities. For each case, the program must determine whether the constructed dataset exhibits Simpson’s paradox, defined precisely as follows:\n\n- Define the marginal association difference $\\Delta = \\hat{p}(Y=1 \\mid X=1) - \\hat{p}(Y=1 \\mid X=0)$ computed from the constructed counts. The marginal association is negative if $\\Delta < 0$.\n- For each stratum $z \\in \\{0,1\\}$, define the conditional association difference $\\Delta_z = \\hat{p}(Y=1 \\mid X=1, Z=z) - \\hat{p}(Y=1 \\mid X=0, Z=z)$. The conditional association is positive in stratum $z$ if $\\Delta_z > 0$.\n\nThe dataset exhibits Simpson’s paradox if and only if $\\Delta < 0$ and both $\\Delta_0 > 0$ and $\\Delta_1 > 0$.\n\nBase your derivation and computation on the following core definitions and well-tested facts:\n- The Markov factorization induced by a Directed Acyclic Graph (DAG) $Z \\to X$, $Z \\to Y$, $X \\to Y$ is $p(Z,X,Y) = p(Z) p(X \\mid Z) p(Y \\mid X,Z)$.\n- Conditional probabilities combine by the law of total probability: for any event $A$, $p(A) = \\sum_z p(A \\mid Z=z) p(Z=z)$.\n- Sample proportions computed from exact integer counts equal the corresponding conditional probabilities in the constructed finite population.\n\nYour program must not use randomness. For each test case, all probabilities are provided as exact rational numbers that make the relevant counts integers given the total sample size. You must compute counts exactly using integer arithmetic implied by these rationals and evaluate the inequalities that define Simpson’s paradox using exact integer comparisons (for example, by cross multiplication rather than floating-point approximation).\n\nTest suite:\n- Case $1$ (happy path, strong paradox):\n  - Total sample size $N = 1000$.\n  - $p(Z=1) = \\frac{1}{2}$.\n  - $p(X=1 \\mid Z=0) = \\frac{1}{10}$, $p(X=1 \\mid Z=1) = \\frac{9}{10}$.\n  - $p(Y=1 \\mid X=1,Z=0) = \\frac{4}{5}$, $p(Y=1 \\mid X=0,Z=0) = \\frac{7}{10}$.\n  - $p(Y=1 \\mid X=1,Z=1) = \\frac{3}{10}$, $p(Y=1 \\mid X=0,Z=1) = \\frac{1}{5}$.\n- Case $2$ (edge case, mild paradox with small within-stratum effects):\n  - Total sample size $N = 200$.\n  - $p(Z=1) = \\frac{1}{2}$.\n  - $p(X=1 \\mid Z=0) = \\frac{1}{5}$, $p(X=1 \\mid Z=1) = \\frac{4}{5}$.\n  - $p(Y=1 \\mid X=1,Z=0) = \\frac{11}{20}$, $p(Y=1 \\mid X=0,Z=0) = \\frac{21}{40}$.\n  - $p(Y=1 \\mid X=1,Z=1) = \\frac{21}{80}$, $p(Y=1 \\mid X=0,Z=1) = \\frac{1}{4}$.\n- Case $3$ (control case, no paradox because there is no confounding from $Z$ into $X$):\n  - Total sample size $N = 1000$.\n  - $p(Z=1) = \\frac{1}{2}$.\n  - $p(X=1 \\mid Z=0) = \\frac{1}{2}$, $p(X=1 \\mid Z=1) = \\frac{1}{2}$.\n  - $p(Y=1 \\mid X=1,Z=0) = \\frac{3}{5}$, $p(Y=1 \\mid X=0,Z=0) = \\frac{2}{5}$.\n  - $p(Y=1 \\mid X=1,Z=1) = \\frac{3}{5}$, $p(Y=1 \\mid X=0,Z=1) = \\frac{2}{5}$.\n\nAlgorithmic requirements:\n- For each case, compute integer counts by multiplying $N$ by the rational probabilities in the DAG factorization order $p(Z)$, $p(X \\mid Z)$, $p(Y \\mid X,Z)$; for example, $n_{Z=1} = N \\cdot \\frac{a}{b}$, $n_{X=1,Z=z} = n_{Z=z} \\cdot \\frac{c_z}{d_z}$, $n_{Y=1,X=x,Z=z} = n_{X=x,Z=z} \\cdot \\frac{e_{x,z}}{f_{x,z}}$. All intermediate results must be integers.\n- Using the resulting counts, compute $\\Delta$ and each $\\Delta_z$ as differences in sample proportions and decide whether Simpson’s paradox holds using exact integer comparisons (no floating-point rounding).\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, in the same order as the cases, where each result is a boolean that is $True$ if and only if the dataset for that case exhibits Simpson’s paradox and is $False$ otherwise. For example, an output like $[True,True,False]$ is acceptable.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of causal inference and probability theory, self-contained, well-posed, and objective. All provided numerical values and constraints are consistent, allowing for a unique and meaningful solution.\n\nThe problem requires us to determine whether Simpson's paradox is exhibited in three different scenarios, each defined by a specific set of parameters for a causal model. The causal structure is given by the Directed Acyclic Graph (DAG) $Z \\to X$, $Z \\to Y$, and $X \\to Y$. The variables $X, Y, Z$ are all binary, taking values in $\\{0, 1\\}$. This DAG implies the Markov factorization of the joint probability distribution $p(X, Y, Z)$ as:\n$$p(X, Y, Z) = p(Z) \\, p(X \\mid Z) \\, p(Y \\mid X, Z)$$\nSimpson's paradox, in this context, is defined by a specific set of conditions:\n1.  The marginal association between $X$ and $Y$ is negative. This is quantified by the difference $\\Delta = \\hat{p}(Y=1 \\mid X=1) - \\hat{p}(Y=1 \\mid X=0)$, where $\\Delta < 0$. The notation $\\hat{p}$ refers to a proportion calculated from a finite sample (count data).\n2.  The conditional association between $X$ and $Y$, given $Z$, is positive for both strata of $Z$. This is quantified by the differences $\\Delta_z = \\hat{p}(Y=1 \\mid X=1, Z=z) - \\hat{p}(Y=1 \\mid X=0, Z=z)$ for $z \\in \\{0, 1\\}$. The conditions are $\\Delta_0 > 0$ and $\\Delta_1 > 0$.\n\nOur task is to construct a contingency table of counts for $(X, Y, Z)$ for each case and then verify these three inequalities. The problem requires using exact integer arithmetic to avoid floating-point precision errors.\n\nThe overall algorithmic procedure is as follows:\n\nFirst, we compute the counts for each of the $2 \\times 2 \\times 2 = 8$ possible combinations of $(X=x, Y=y, Z=z)$. The problem states that counts are to be derived from a total sample size $N$ and the provided exact rational probabilities. Let $n(A)$ denote the count of event $A$. We can compute the joint counts $n(X=x, Y=y, Z=z)$ following the causal order dictated by the DAG factorization.\n\nLet the provided probabilities be represented as rational numbers. For instance, $p(Z=1) = n_{Z=1}/d_{Z=1}$.\n\n1.  **Calculate counts of $Z$ strata**:\n    The number of individuals in each stratum of $Z$ is:\n    $$n(Z=1) = N \\cdot p(Z=1)$$\n    $$n(Z=0) = N \\cdot p(Z=0) = N \\cdot (1 - p(Z=1))$$\n\n2.  **Calculate joint counts of $(X, Z)$ strata**:\n    Within each stratum of $Z$, we calculate the counts for sub-strata of $X$:\n    $$n(X=1, Z=0) = n(Z=0) \\cdot p(X=1 \\mid Z=0)$$\n    $$n(X=0, Z=0) = n(Z=0) - n(X=1, Z=0)$$\n    $$n(X=1, Z=1) = n(Z=1) \\cdot p(X=1 \\mid Z=1)$$\n    $$n(X=0, Z=1) = n(Z=1) - n(X=1, Z=1)$$\n    The problem guarantees that these intermediate counts will be integers.\n\n3.  **Calculate the full joint counts $n(X, Y, Z)$**:\n    Finally, we calculate the number of individuals for whom $Y=1$ in each $(X, Z)$ stratum.\n    $$n(Y=1, X=1, Z=0) = n(X=1, Z=0) \\cdot p(Y=1 \\mid X=1, Z=0)$$\n    $$n(Y=1, X=0, Z=0) = n(X=0, Z=0) \\cdot p(Y=1 \\mid X=0, Z=0)$$\n    $$n(Y=1, X=1, Z=1) = n(X=1, Z=1) \\cdot p(Y=1 \\mid X=1, Z=1)$$\n    $$n(Y=1, X=0, Z=1) = n(X=0, Z=1) \\cdot p(Y=1 \\mid X=0, Z=1)$$\n    The counts for $Y=0$ can be found by subtraction, e.g., $n(Y=0, X=x, Z=z) = n(X=x, Z=z) - n(Y=1, X=x, Z=z)$.\n\nSecond, with the complete $2 \\times 2 \\times 2$ count table, we evaluate the conditions for Simpson's paradox.\n\n1.  **Verify Conditional Associations ($\\Delta_0 > 0, \\Delta_1 > 0$)**:\n    The problem states that sample proportions from the constructed counts will equal the underlying probabilities. Thus, $\\hat{p}(Y=1 \\mid X=x, Z=z) = p(Y=1 \\mid X=x, Z=z)$. The conditions $\\Delta_z > 0$ simplify to:\n    $$p(Y=1 \\mid X=1, Z=0) > p(Y=1 \\mid X=0, Z=0)$$\n    $$p(Y=1 \\mid X=1, Z=1) > p(Y=1 \\mid X=0, Z=1)$$\n    Given probabilities as fractions, e.g., $p_1 = a/b$ and $p_2 = c/d$, the inequality $p_1 > p_2$ is equivalent to $a/b > c/d$. To maintain precision, we check this using integer cross-multiplication: $a \\cdot d > c \\cdot b$, assuming positive denominators $b,d$. Both conditions must be met.\n\n2.  **Verify Marginal Association ($\\Delta < 0$)**:\n    The marginal association requires summing counts over the variable $Z$.\n    The total count of individuals with $X=1$ is $n(X=1) = n(X=1, Z=0) + n(X=1, Z=1)$.\n    The count of individuals with $Y=1$ and $X=1$ is $n(Y=1, X=1) = n(Y=1, X=1, Z=0) + n(Y=1, X=1, Z=1)$.\n    The marginal probability estimate is $\\hat{p}(Y=1 \\mid X=1) = \\frac{n(Y=1, X=1)}{n(X=1)}$.\n    Similarly, for $X=0$:\n    $n(X=0) = n(X=0, Z=0) + n(X=0, Z=1)$.\n    $n(Y=1, X=0) = n(Y=1, X=0, Z=0) + n(Y=1, X=0, Z=1)$.\n    $\\hat{p}(Y=1 \\mid X=0) = \\frac{n(Y=1, X=0)}{n(X=0)}$.\n\n    The condition $\\Delta < 0$ becomes $\\hat{p}(Y=1 \\mid X=1) < \\hat{p}(Y=1 \\mid X=0)$, or:\n    $$\\frac{n(Y=1, X=1)}{n(X=1)} < \\frac{n(Y=1, X=0)}{n(X=0)}$$\n    This inequality is verified using integer cross-multiplication to avoid floating-point errors:\n    $$n(Y=1, X=1) \\cdot n(X=0) < n(Y=1, X=0) \\cdot n(X=1)$$\n\nSimpson's paradox is present if and only if both conditional associations are positive and the marginal association is negative. This procedure will be applied to each test case to generate the final boolean result.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Simpson's paradox problem for a set of predefined test cases.\n    \"\"\"\n    \n    # Probabilities are represented as (numerator, denominator) tuples for exact rational arithmetic.\n    test_cases = [\n        # Case 1 (happy path, strong paradox)\n        {\n            \"N\": 1000,\n            \"p_z1\": (1, 2),\n            \"p_x1_z0\": (1, 10), \"p_x1_z1\": (9, 10),\n            \"p_y1_x1_z0\": (4, 5), \"p_y1_x0_z0\": (7, 10),\n            \"p_y1_x1_z1\": (3, 10), \"p_y1_x0_z1\": (1, 5)\n        },\n        # Case 2 (edge case, mild paradox with small within-stratum effects)\n        {\n            \"N\": 200,\n            \"p_z1\": (1, 2),\n            \"p_x1_z0\": (1, 5), \"p_x1_z1\": (4, 5),\n            \"p_y1_x1_z0\": (11, 20), \"p_y1_x0_z0\": (21, 40),\n            \"p_y1_x1_z1\": (21, 80), \"p_y1_x0_z1\": (1, 4)\n        },\n        # Case 3 (control case, no paradox because no confounding)\n        {\n            \"N\": 1000,\n            \"p_z1\": (1, 2),\n            \"p_x1_z0\": (1, 2), \"p_x1_z1\": (1, 2),\n            \"p_y1_x1_z0\": (3, 5), \"p_y1_x0_z0\": (2, 5),\n            \"p_y1_x1_z1\": (3, 5), \"p_y1_x0_z1\": (2, 5)\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        \n        # Unpack probabilities for convenience\n        p_z1 = case[\"p_z1\"]\n        p_x1_z0, p_x1_z1 = case[\"p_x1_z0\"], case[\"p_x1_z1\"]\n        p_y1_x1_z0, p_y1_x0_z0 = case[\"p_y1_x1_z0\"], case[\"p_y1_x0_z0\"]\n        p_y1_x1_z1, p_y1_x0_z1 = case[\"p_y1_x1_z1\"], case[\"p_y1_x0_z1\"]\n\n        # 1. Calculate counts of Z strata using exact integer division\n        n_z1 = N * p_z1[0] // p_z1[1]\n        n_z0 = N - n_z1\n\n        # 2. Calculate joint counts of (X, Z) strata\n        n_x1_z0 = n_z0 * p_x1_z0[0] // p_x1_z0[1]\n        n_x0_z0 = n_z0 - n_x1_z0\n        \n        n_x1_z1 = n_z1 * p_x1_z1[0] // p_x1_z1[1]\n        n_x0_z1 = n_z1 - n_x1_z1\n\n        # 3. Calculate full joint counts n(Y=1, X, Z)\n        n_y1_x1_z0 = n_x1_z0 * p_y1_x1_z0[0] // p_y1_x1_z0[1]\n        n_y1_x0_z0 = n_x0_z0 * p_y1_x0_z0[0] // p_y1_x0_z0[1]\n        \n        n_y1_x1_z1 = n_x1_z1 * p_y1_x1_z1[0] // p_y1_x1_z1[1]\n        n_y1_x0_z1 = n_x0_z1 * p_y1_x0_z1[0] // p_y1_x0_z1[1]\n\n        # 4. Check for positive conditional associations (Delta_z > 0)\n        # Using cross-multiplication for exact comparison of fractions a/b > c/d as a*d > c*b\n        cond_assoc_z0_pos = (p_y1_x1_z0[0] * p_y1_x0_z0[1]) > (p_y1_x0_z0[0] * p_y1_x1_z0[1])\n        cond_assoc_z1_pos = (p_y1_x1_z1[0] * p_y1_x0_z1[1]) > (p_y1_x0_z1[0] * p_y1_x1_z1[1])\n\n        all_cond_assoc_pos = cond_assoc_z0_pos and cond_assoc_z1_pos\n        \n        # 5. Check for negative marginal association (Delta < 0)\n        # Aggregate counts to get marginals\n        n_y1_x1 = n_y1_x1_z0 + n_y1_x1_z1\n        n_x1 = n_x1_z0 + n_x1_z1\n        \n        n_y1_x0 = n_y1_x0_z0 + n_y1_x0_z1\n        n_x0 = n_x0_z0 + n_x0_z1\n\n        # Check for marginal_association < 0 if denominators are non-zero\n        marg_assoc_neg = False\n        if n_x1 > 0 and n_x0 > 0:\n            # Check p(Y=1|X=1) < p(Y=1|X=0) via cross-multiplication\n            # n(Y=1,X=1)/n(X=1) < n(Y=1,X=0)/n(X=0) => n(Y=1,X=1)*n(X=0) < n(Y=1,X=0)*n(X=1)\n            marg_assoc_neg = (n_y1_x1 * n_x0) < (n_y1_x0 * n_x1)\n\n        # Simpson's Paradox holds if conditional associations are positive AND marginal association is negative.\n        paradox_exhibited = all_cond_assoc_pos and marg_assoc_neg\n        results.append(paradox_exhibited)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen how unobserved confounding can mislead us, we now turn to a powerful identification strategy for when we cannot directly measure the confounder. The front-door criterion provides a way to estimate a causal effect by observing a mediating variable that lies on the causal pathway from treatment to outcome. This practice  challenges you to apply the front-door adjustment formula to a public health scenario, translating an abstract causal principle into a tangible numerical estimate of vaccine effectiveness.",
            "id": "3115795",
            "problem": "A public health team is studying the causal effect of vaccination on infection using a Directed Acyclic Graph (DAG). Let $V$ denote vaccination status, $A$ denote post-vaccination antibody response, $I$ denote subsequent infection, and $C$ denote age. The DAG is $V \\to A \\to I$ with $C \\to V$ and $C \\to I$. There is no direct edge from $V$ to $I$. The variable $C$ is unobserved in the available data. The team suspects that $C$ confounds $V$ and $I$ but does not affect $A$ directly or indirectly except through $V$.\n\nAll variables $V$, $A$, and $I$ are binary with state space $\\{0,1\\}$. The observed data provide the following probabilities:\n- $P(V=1) = 0.6$ and $P(V=0) = 0.4$.\n- $P(A=1 \\mid V=1) = 0.8$ and $P(A=1 \\mid V=0) = 0.2$; hence $P(A=0 \\mid V=1) = 0.2$ and $P(A=0 \\mid V=0) = 0.8$.\n- $P(I=1 \\mid A=1, V=1) = 0.05$, $P(I=1 \\mid A=1, V=0) = 0.10$, $P(I=1 \\mid A=0, V=1) = 0.20$, and $P(I=1 \\mid A=0, V=0) = 0.30$.\n\nUsing only the graphical assumptions encoded by the DAG and standard rules that connect interventions to observational distributions, compute the average causal effect of vaccination on infection defined as\n$$\\tau \\equiv P(I=1 \\mid \\operatorname{do}(V=1)) - P(I=1 \\mid \\operatorname{do}(V=0)).$$\nExpress your final answer as a single decimal number and round to $4$ significant figures. Do not use a percentage sign.",
            "solution": "The problem is valid. It is a well-posed question in the domain of causal inference based on a Directed Acyclic Graph (DAG). The average causal effect (ACE) is identifiable from the provided observational data, despite the presence of an unobserved confounder $C$, because the structure of the DAG satisfies the front-door criterion.\n\nThe causal relationship is given by the DAG: $C \\to V \\to A \\to I$ and $C \\to I$. The variable $C$ is an unobserved confounder for the relationship between vaccination ($V$) and infection ($I$), as it creates a backdoor path $V \\leftarrow C \\to I$. The effect of $V$ on $I$ is mediated entirely by antibody response ($A$), as the only directed path from $V$ to $I$ is $V \\to A \\to I$.\n\nThe front-door criterion allows for the identification of the causal effect of $V$ on $I$ using the mediator $A$ if three conditions are met:\n1.  $A$ intercepts all directed paths from $V$ to $I$. This is true, as the only such path is $V \\to A \\to I$.\n2.  There is no unblocked backdoor path from $V$ to $A$. The only path from an ancestor of $V$ to $A$ is $C \\to V \\to A$, which is not a backdoor path. Thus, this condition is met.\n3.  All backdoor paths from $A$ to $I$ are blocked by $V$. The only backdoor path from $A$ to $I$ is $A \\leftarrow V \\leftarrow C \\to I$. This path is blocked by conditioning on $V$.\n\nSince all conditions are satisfied, the causal effect of $V$ on $I$, $P(I=1 \\mid \\operatorname{do}(V=v))$, can be computed using the front-door adjustment formula:\n$$ P(I=1 \\mid \\operatorname{do}(V=v)) = \\sum_{a \\in \\{0, 1\\}} P(A=a \\mid V=v) \\sum_{v' \\in \\{0, 1\\}} P(I=1 \\mid A=a, V=v') P(V=v') $$\nAll probabilities on the right-hand side are provided in the problem statement.\n\nLet's first calculate the inner summation term, which is independent of the intervention variable $v$. Let's denote this term by $S_a$:\n$$ S_a \\equiv \\sum_{v' \\in \\{0, 1\\}} P(I=1 \\mid A=a, V=v') P(V=v') $$\n\nFor $a=1$:\n$$ S_1 = P(I=1 \\mid A=1, V=0)P(V=0) + P(I=1 \\mid A=1, V=1)P(V=1) $$\nUsing the given values:\n$$ S_1 = (0.10)(0.4) + (0.05)(0.6) = 0.04 + 0.03 = 0.07 $$\n\nFor $a=0$:\n$$ S_0 = P(I=1 \\mid A=0, V=0)P(V=0) + P(I=1 \\mid A=0, V=1)P(V=1) $$\nUsing the given values:\n$$ S_0 = (0.30)(0.4) + (0.20)(0.6) = 0.12 + 0.12 = 0.24 $$\n\nNow we can compute the interventional probabilities for $V=1$ and $V=0$.\n\nFor intervention $\\operatorname{do}(V=1)$:\n$$ P(I=1 \\mid \\operatorname{do}(V=1)) = \\sum_{a \\in \\{0, 1\\}} P(A=a \\mid V=1) S_a $$\n$$ P(I=1 \\mid \\operatorname{do}(V=1)) = P(A=0 \\mid V=1)S_0 + P(A=1 \\mid V=1)S_1 $$\nUsing the given values:\n$$ P(I=1 \\mid \\operatorname{do}(V=1)) = (0.2)(0.24) + (0.8)(0.07) = 0.048 + 0.056 = 0.104 $$\n\nFor intervention $\\operatorname{do}(V=0)$:\n$$ P(I=1 \\mid \\operatorname{do}(V=0)) = \\sum_{a \\in \\{0, 1\\}} P(A=a \\mid V=0) S_a $$\n$$ P(I=1 \\mid \\operatorname{do}(V=0)) = P(A=0 \\mid V=0)S_0 + P(A=1 \\mid V=0)S_1 $$\nUsing the given values:\n$$ P(I=1 \\mid \\operatorname{do}(V=0)) = (0.8)(0.24) + (0.2)(0.07) = 0.192 + 0.014 = 0.206 $$\n\nFinally, the average causal effect $\\tau$ is the difference between these two quantities:\n$$ \\tau = P(I=1 \\mid \\operatorname{do}(V=1)) - P(I=1 \\mid \\operatorname{do}(V=0)) $$\n$$ \\tau = 0.104 - 0.206 = -0.102 $$\n\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$ \\tau = -0.1020 $$\nThis indicates that vaccination causally reduces the probability of infection by $0.1020$.",
            "answer": "$$\\boxed{-0.1020}$$"
        },
        {
            "introduction": "Our final practice serves as a crucial cautionary tale against the naive strategy of \"controlling for everything.\" Adjusting for variables is not always beneficial and can, in fact, introduce bias where none existed before. This exercise focuses on overcontrol bias, which arises when we condition on a variable that is a consequence of the outcome itself . By deriving the precise mathematical form of this bias in a linear model, you will develop a rigorous understanding of why the choice of control variables must be guided by the causal structure of the DAG, not statistical association alone.",
            "id": "3115802",
            "problem": "A researcher is studying a system represented by a directed acyclic graph (DAG) with arrows $C \\to X$, $C \\to Y$, $X \\to Y$, and $Y \\to O$. The data are generated by a linear structural causal model with mutually independent, zero-mean exogenous noises of finite, strictly positive variance:\n- $C \\sim \\mathcal{N}(0, \\sigma_{C}^{2})$,\n- $X = aC + \\varepsilon_{X}$ with $\\varepsilon_{X} \\sim \\mathcal{N}(0, \\sigma_{X}^{2})$,\n- $Y = bX + cC + \\varepsilon_{Y}$ with $\\varepsilon_{Y} \\sim \\mathcal{N}(0, \\sigma_{Y}^{2})$,\n- $O = dY + \\varepsilon_{O}$ with $\\varepsilon_{O} \\sim \\mathcal{N}(0, \\sigma_{O}^{2})$.\n\nThe coefficient $b$ is the causal effect of $X$ on $Y$. To estimate the effect of $X$ on $Y$, the researcher mistakenly runs an ordinary least squares (OLS) regression of $Y$ on $X$, $C$, and $O$ (thus adjusting for the descendant $O$ of $Y$). Under the stated assumptions, consider the large-sample (population) OLS coefficient on $X$ from this regression.\n\nUsing only foundational facts from linear regression and probability (for example, that the OLS estimand equals the solution to the population normal equations, and basic properties of covariance), derive an exact, closed-form analytic expression for the overcontrol bias, defined as the difference between the large-sample OLS coefficient on $X$ when regressing $Y$ on $X$, $C$, and $O$, and the true causal effect $b$.\n\nExpress your final answer as a simplified algebraic expression in terms of $b$, $d$, $\\sigma_{Y}^{2}$, and $\\sigma_{O}^{2}$ only. No numerical rounding is required.",
            "solution": "The problem requires the derivation of the overcontrol bias for the causal effect of $X$ on $Y$ when an ordinary least squares (OLS) regression of $Y$ is performed on $X$, the confounder $C$, and the descendant of $Y$, $O$. The bias is the difference between the population OLS coefficient for $X$ in this regression, which we denote $\\beta_X$, and the true causal effect, $b$.\n\nThe structural causal model is specified by the following equations:\n$$C \\sim \\mathcal{N}(0, \\sigma_{C}^{2})$$\n$$X = aC + \\varepsilon_{X}, \\quad \\varepsilon_{X} \\sim \\mathcal{N}(0, \\sigma_{X}^{2})$$\n$$Y = bX + cC + \\varepsilon_{Y}, \\quad \\varepsilon_{Y} \\sim \\mathcal{N}(0, \\sigma_{Y}^{2})$$\n$$O = dY + \\varepsilon_{O}, \\quad \\varepsilon_{O} \\sim \\mathcal{N}(0, \\sigma_{O}^{2})$$\nThe exogenous noise terms $C$, $\\varepsilon_X$, $\\varepsilon_Y$, and $\\varepsilon_O$ are mutually independent, have zero mean, and have finite, strictly positive variances denoted by $\\sigma_C^2$, $\\sigma_X^2$, $\\sigma_Y^2$, and $\\sigma_O^2$, respectively.\n\nThe researcher performs the regression $Y \\sim X + C + O$. Let the population OLS coefficients be $\\beta_X$, $\\beta_C$, and $\\beta_O$. The regression model is:\n$$Y = \\beta_X X + \\beta_C C + \\beta_O O + u$$\nwhere $u$ is the regression residual. A fundamental property of population OLS is that the residual is uncorrelated with the regressors. As all variables in the model have zero mean, this implies the covariance is zero:\n$$\\text{Cov}(u, X) = 0$$\n$$\\text{Cov}(u, C) = 0$$\n$$\\text{Cov}(u, O) = 0$$\n\nWe can express the residual $u$ by substituting the true structural equation for $Y$:\n$$u = Y - (\\beta_X X + \\beta_C C + \\beta_O O)$$\n$$u = (bX + cC + \\varepsilon_Y) - (\\beta_X X + \\beta_C C + \\beta_O O)$$\n$$u = (b - \\beta_X)X + (c - \\beta_C)C - \\beta_O O + \\varepsilon_Y$$\nLet $\\Delta_X = b - \\beta_X$ and $\\Delta_C = c - \\beta_C$. The quantity we need to find is the bias, which is $\\beta_X - b = -\\Delta_X$. The residual is now $u = \\Delta_X X + \\Delta_C C - \\beta_O O + \\varepsilon_Y$.\n\nApplying the first two orthogonality conditions:\n1. $\\text{Cov}(u, X) = \\text{Cov}(\\Delta_X X + \\Delta_C C - \\beta_O O + \\varepsilon_Y, X) = 0$. Using the bilinearity of covariance and noting that $\\text{Cov}(\\varepsilon_Y, X)=0$ because $\\varepsilon_Y$ is independent of $C$ and $\\varepsilon_X$, we get:\n$$\\Delta_X \\text{Var}(X) + \\Delta_C \\text{Cov}(X, C) - \\beta_O \\text{Cov}(O, X) = 0$$\n$$ \\implies \\Delta_X \\text{Var}(X) + \\Delta_C \\text{Cov}(X, C) = \\beta_O \\text{Cov}(O, X) \\quad (1)$$\n\n2. $\\text{Cov}(u, C) = \\text{Cov}(\\Delta_X X + \\Delta_C C - \\beta_O O + \\varepsilon_Y, C) = 0$. Since $\\text{Cov}(\\varepsilon_Y, C)=0$:\n$$\\Delta_X \\text{Cov}(X, C) + \\Delta_C \\text{Var}(C) - \\beta_O \\text{Cov}(O, C) = 0$$\n$$ \\implies \\Delta_X \\text{Cov}(X, C) + \\Delta_C \\text{Var}(C) = \\beta_O \\text{Cov}(O, C) \\quad (2)$$\n\nEquations $(1)$ and $(2)$ form a linear system for $(\\Delta_X, \\Delta_C)$. This system is identical to the normal equations for an OLS regression of a variable $\\beta_O O$ on $X$ and $C$. If we denote the coefficients of the population regression of $O$ on $X$ and $C$ as $\\gamma_X$ and $\\gamma_C$, then the solution to this system must be $\\Delta_X = \\beta_O \\gamma_X$ and $\\Delta_C = \\beta_O \\gamma_C$.\nThe bias is therefore $\\beta_X - b = -\\Delta_X = -\\beta_O \\gamma_X$.\n\nOur task now is to find $\\gamma_X$ and $\\beta_O$.\n\nFirst, we find $\\gamma_X$, the coefficient of $X$ in the regression of $O$ on $X$ and $C$.\nWe express $O$ in terms of $X$, $C$, and exogenous errors:\n$$O = dY + \\varepsilon_O = d(bX + cC + \\varepsilon_Y) + \\varepsilon_O = dbX + dcC + (d\\varepsilon_Y + \\varepsilon_O)$$\nThe population regression of $O$ on $X$ and $C$ has coefficients $db$ and $dc$ if the error term, $(d\\varepsilon_Y + \\varepsilon_O)$, is uncorrelated with the regressors $X$ and $C$.\n$\\text{Cov}(d\\varepsilon_Y + \\varepsilon_O, X) = d\\text{Cov}(\\varepsilon_Y, X) + \\text{Cov}(\\varepsilon_O, X) = 0$\n$\\text{Cov}(d\\varepsilon_Y + \\varepsilon_O, C) = d\\text{Cov}(\\varepsilon_Y, C) + \\text{Cov}(\\varepsilon_O, C) = 0$\nThese covariances are zero because $\\varepsilon_Y$ and $\\varepsilon_O$ are independent of $C$ and $\\varepsilon_X$ (and thus of $X=aC+\\varepsilon_X$).\nTherefore, the coefficient $\\gamma_X$ from regressing $O$ on $X$ and $C$ is exactly $db$.\nThe bias is now $\\beta_X - b = -(db)\\beta_O$.\n\nNext, we find $\\beta_O$. By the Frisch-Waugh-Lovell theorem, the coefficient $\\beta_O$ in a multiple regression of $Y$ on $(X, C, O)$ is equal to the coefficient of a simple regression of the residual of $Y$ on the residual of $O$, after both have been regressed on the other variables, $(X, C)$.\nLet $Y_{\\perp(X,C)}$ be the residual from regressing $Y$ on $X$ and $C$.\nLet $O_{\\perp(X,C)}$ be the residual from regressing $O$ on $X$ and $C$.\nThen, $\\beta_O = \\frac{\\text{Cov}(Y_{\\perp(X,C)}, O_{\\perp(X,C)})}{\\text{Var}(O_{\\perp(X,C)})}$.\n\nFrom the true model $Y = bX + cC + \\varepsilon_Y$, since $\\varepsilon_Y$ is uncorrelated with $X$ and $C$, the residual from regressing $Y$ on $X$ and $C$ is simply $Y_{\\perp(X,C)} = \\varepsilon_Y$.\nFrom the derived model $O = dbX+dcC+(d\\varepsilon_Y+\\varepsilon_O)$, since the term $(d\\varepsilon_Y+\\varepsilon_O)$ is uncorrelated with $X$ and $C$, the residual from regressing $O$ on $X$ and $C$ is $O_{\\perp(X,C)} = d\\varepsilon_Y+\\varepsilon_O$.\n\nWe can now compute the components of $\\beta_O$:\nThe covariance in the numerator is:\n$$\\text{Cov}(Y_{\\perp(X,C)}, O_{\\perp(X,C)}) = \\text{Cov}(\\varepsilon_Y, d\\varepsilon_Y + \\varepsilon_O) = d\\text{Cov}(\\varepsilon_Y, \\varepsilon_Y) + \\text{Cov}(\\varepsilon_Y, \\varepsilon_O)$$\nSince $\\varepsilon_Y$ and $\\varepsilon_O$ are independent, $\\text{Cov}(\\varepsilon_Y, \\varepsilon_O) = 0$. Thus, the numerator is $d\\text{Var}(\\varepsilon_Y) = d\\sigma_Y^2$.\n\nThe variance in the denominator is:\n$$\\text{Var}(O_{\\perp(X,C)}) = \\text{Var}(d\\varepsilon_Y + \\varepsilon_O) = d^2\\text{Var}(\\varepsilon_Y) + \\text{Var}(\\varepsilon_O) + 2d\\text{Cov}(\\varepsilon_Y, \\varepsilon_O)$$\nAgain, due to independence, this simplifies to $d^2\\text{Var}(\\varepsilon_Y) + \\text{Var}(\\varepsilon_O) = d^2\\sigma_Y^2 + \\sigma_O^2$.\n\nCombining these, we find the coefficient $\\beta_O$:\n$$\\beta_O = \\frac{d\\sigma_Y^2}{d^2\\sigma_Y^2 + \\sigma_O^2}$$\n\nFinally, we substitute this expression for $\\beta_O$ into our equation for the bias:\n$$\\text{Bias} = \\beta_X - b = -(db)\\beta_O = -db \\left( \\frac{d\\sigma_Y^2}{d^2\\sigma_Y^2 + \\sigma_O^2} \\right)$$\nSimplifying this expression gives the final answer:\n$$\\text{Bias} = -\\frac{b d^2 \\sigma_Y^2}{d^2\\sigma_Y^2 + \\sigma_O^2}$$\nThis is the overcontrol bias, expressed in terms of the required parameters.",
            "answer": "$$\n\\boxed{-\\frac{b d^{2} \\sigma_{Y}^{2}}{d^{2}\\sigma_{Y}^{2} + \\sigma_{O}^{2}}}\n$$"
        }
    ]
}