## 引言

在[高维数据](@entry_id:138874)无处不在的今天，从基因表达谱到金融市场波动，再到用户在线行为记录，我们面临着一个共同的挑战：如何在看似杂乱无章的海量变量中发现其背后简洁的驱动规律？潜[因子模型](@entry_id:141879)（Latent Factor Models）为此提供了一个强大而优雅的理论框架。其核心假设是，许多我们能观测到的变量，实际上是由少数几个我们无法直接观测到的、共享的“潜在因子”所驱动的。理解这些[潜因子](@entry_id:182794)，就如同找到了解读复杂现象的“罗塞塔石碑”，能够帮助我们降维、[去噪](@entry_id:165626)、预测并获得深刻的洞察。

然而，从数据中准确地估计这些看不见的因子并赋予其有意义的解释，并非易事。这引出了统计学和机器学习中的一系列核心问题：我们如何将数据的变异分解为共享部分和独立部分？如何处理模型固有的模糊性以确保结果的稳定性和可解释性？这些理论模型又如何在生命科学、[推荐系统](@entry_id:172804)和经济学等不同领域中转化为解决实际问题的有效工具？

本文将系统性地引导您深入潜[因子模型](@entry_id:141879)的世界。在“原理与机制”一章，我们将剖析模型背后的数学基础，探讨其核心的[方差分解](@entry_id:272134)思想、主要的估计算法（如[主成分分析](@entry_id:145395)和[因子分析](@entry_id:165399)），并直面旋转不确定性这一关键挑战及其解决方案。接下来，在“应用与[交叉](@entry_id:147634)学科联系”一章，我们将跨越学科边界，展示潜[因子模型](@entry_id:141879)如何在基因[组学数据整合](@entry_id:268201)、个性化推荐、[金融风险管理](@entry_id:138248)等前沿领域中发挥关键作用。最后，通过“动手实践”部分的一系列编程练习，您将有机会亲手实现和应用这些模型，将理论知识转化为解决真实世界问题的实践技能。

## 原理与机制

在介绍性章节之后，我们现在深入探讨潜在[因子模型](@entry_id:141879)的数学原理和核心机制。本章的目标是建立一个坚实的理论基础，阐明这些模型如何解释高维数据中的协[方差](@entry_id:200758)结构，并探讨其在估计、解释和应用中所面临的关键挑战和解决方案。

### 核心思想：[方差分解](@entry_id:272134)

潜在[因子模型](@entry_id:141879)的核心思想是将观测数据的变异性分解为两个部分：由少数共享的、不可观测的**潜在因子（latent factors）**驱动的**共同[方差](@entry_id:200758)（common variance）**，以及每个观测变量独有的**异质[方差](@entry_id:200758)（idiosyncratic variance）**或噪声。

形式上，对于一个 $p$ 维的观测数据向量 $x \in \mathbb{R}^p$（假设已中心化，即均值为零），一个线性潜在[因子模型](@entry_id:141879)可以表示为：

$$
x = \Lambda f + \varepsilon
$$

其中：
- $f \in \mathbb{R}^k$ 是一个包含 $k$ 个潜在因子的向量，其中 $k \ll p$。这些因子是不可观测的[随机变量](@entry_id:195330)。
- $\Lambda \in \mathbb{R}^{p \times k}$ 是**载荷矩阵（loading matrix）**，其元素 $\Lambda_{ij}$ 表示第 $j$ 个因子对第 $i$ 个观测变量的线性影响或“载荷”。
- $\varepsilon \in \mathbb{R}^p$ 是**异质噪声（idiosyncratic noise）**向量，代表了不能被共同因子解释的、每个观测变量独有的变异。

我们通常做如下基本假设：
1. 潜在因子 $f$ 的均值为零，$\mathbb{E}[f] = 0$。为了简化模型，通常假设因子是不相关的，且具有单位[方差](@entry_id:200758)，即其协方差矩阵为[单位矩阵](@entry_id:156724) $I_k$，$\mathbb{E}[ff^\top] = I_k$。
2. 噪声项 $\varepsilon$ 的均值为零，$\mathbb{E}[\varepsilon] = 0$。
3. 噪声项之间通常假设是不相关的，因此其协方差矩阵 $\Psi = \mathbb{E}[\varepsilon \varepsilon^\top]$ 是一个[对角矩阵](@entry_id:637782)，$\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_p)$。对角线上的元素 $\psi_i$ 代表了第 $i$ 个观测变量的异质[方差](@entry_id:200758)。
4. 关键的是，潜在因子与噪声项是不相关的，即 $\mathbb{E}[f \varepsilon^\top] = 0$。

基于这些假设，我们可以推导出观测数据 $x$ 的[协方差矩阵](@entry_id:139155) $\Sigma_x$ 的结构：

$$
\Sigma_x = \mathbb{E}[xx^\top] = \mathbb{E}[(\Lambda f + \varepsilon)(\Lambda f + \varepsilon)^\top] = \mathbb{E}[\Lambda f f^\top \Lambda^\top + \Lambda f \varepsilon^\top + \varepsilon f^\top \Lambda^\top + \varepsilon \varepsilon^\top]
$$

由于因子与噪声不相关，交叉项的期望为零。因此，我们得到潜在[因子模型](@entry_id:141879)的核心方程：

$$
\Sigma_x = \Lambda \mathbb{E}[ff^\top] \Lambda^\top + \mathbb{E}[\varepsilon \varepsilon^\top] = \Lambda \Lambda^\top + \Psi
$$

这个方程优美地体现了[方差分解](@entry_id:272134)的思想。观测变量之间的协[方差](@entry_id:200758)完全由共同因子贡献，体现在低秩矩阵 $\Lambda \Lambda^\top$ 中。而每个变量独有的变异则被隔离在[对角矩阵](@entry_id:637782) $\Psi$ 中 。潜在[因子模型](@entry_id:141879)的本质目标就是从观测到的样本协方差矩阵 $S$ 中估计出载荷矩阵 $\Lambda$ 和异质[方差](@entry_id:200758) $\Psi$。

### 估计方法与识别性挑战

#### 基于[主成分分析](@entry_id:145395)的估计

估计潜在[因子模型](@entry_id:141879)的一个直观方法是**主成分分析（Principal Component Analysis, PCA）**。PCA旨在找到数据中[方差](@entry_id:200758)最大的方向。根据[Eckart-Young-Mirsky定理](@entry_id:149772)，样本协方差矩阵 $S$ 的最佳低秩（秩为 $k$）逼近是通过其谱分解得到的。具体而言，如果 $S$ 的谱分解为 $S = U D U^\top$，其中 $U$ 是[特征向量](@entry_id:151813)矩阵，$D$ 是[特征值](@entry_id:154894)对角矩阵，那么最佳的秩-$k$ 逼近是 $S_k = U_k D_k U_k^\top$，其中 $U_k$ 包含了对应于 $k$ 个最大[特征值](@entry_id:154894)的[特征向量](@entry_id:151813)，$D_k$ 包含了这 $k$ 个最大的[特征值](@entry_id:154894)。

在[因子分析](@entry_id:165399)的框架下，我们可以将这个最佳逼[近视](@entry_id:178989)为对共同协[方差](@entry_id:200758)部分 $\Lambda\Lambda^\top$ 的估计，即 $\widehat{\Lambda\Lambda^\top} = S_k$。由此，我们可以构造一个 $\Lambda$ 的PCA估计量：

$$
\widehat{\Lambda}_{\text{PCA}} = U_k D_k^{1/2}
$$

这个估计量满足 $\widehat{\Lambda}_{\text{PCA}} \widehat{\Lambda}_{\text{PCA}}^\top = U_k D_k U_k^\top = S_k$，从而提供了一种从数据中估计载荷矩阵的方法 。

#### 旋转不确定性

然而，这种估计方法以及更广泛的[因子模型](@entry_id:141879)，都面临一个根本性的**识别性（identifiability）**问题，即**旋转不确定性（rotational indeterminacy）**。考虑任何一个 $k \times k$ 的**正交矩阵** $Q$（满足 $QQ^\top = Q^\top Q = I_k$）。我们可以定义一个新的载荷矩阵 $\Lambda^* = \Lambda Q$ 和一组新的因子 $f^* = Q^\top f$。

新的因子 $f^*$ 仍然满足我们的基本假设：
- $\mathbb{E}[f^*] = Q^\top \mathbb{E}[f] = 0$
- $\mathbb{E}[f^* (f^*)^\top] = \mathbb{E}[Q^\top f f^\top Q] = Q^\top \mathbb{E}[ff^\top] Q = Q^\top I_k Q = I_k$

更重要的是，模型所隐含的协[方差](@entry_id:200758)结构保持不变：

$$
\Lambda^* (\Lambda^*)^\top = (\Lambda Q)(\Lambda Q)^\top = \Lambda Q Q^\top \Lambda^\top = \Lambda I_k \Lambda^\top = \Lambda \Lambda^\top
$$

这意味着，对于任意一个正交变换（旋转或反射），我们都可以找到一组新的载荷和因子，它们在统计上与原始模型无法区分，因为它们生成了完全相同的观测[协方差矩阵](@entry_id:139155) 。这引出了一个深刻的结论：**在没有额外约束的情况下，[因子载荷](@entry_id:166383)和因子本身并不是唯一可识别的**。我们只能唯一地识别由 $\Lambda\Lambda^\top$ 定义的共同因子[子空间](@entry_id:150286)。

这个问题的实际影响是巨大的。如果我们从数据中得到了一个载荷矩阵 $\widehat{\Lambda}$，那么任何旋转后的版本 $\widehat{\Lambda}Q$ 都是一个同样有效的解。由于存在无限多个正交矩阵 $Q$，因此也存在无限多组同样“好”的载荷矩阵。这使得对单个因子（即 $\Lambda$ 的某一列）的解释变得任意和不稳定。例如，从一次采样中计算出的“因子1”可能与从另一次采样中计算出的“因子1”毫无关系，它们可能只是同一因子[子空间](@entry_id:150286)的不同[基向量](@entry_id:199546)。

#### 解决不确定性：实现可解释的因子

为了获得有意义且可重复的解释，我们必须引入额外的约束来消除旋转不确定性。这通常通过以下几种方式实现：

1.  **后验旋转以获得简单结构**：在探索性[因子分析](@entry_id:165399)（Exploratory Factor Analysis, EFA）中，一种常见的做法是先获得一个初始的载荷矩阵估计（如PCA或最大似然法），然后通过一个正交旋转使其达到**简单结构（simple structure）**。简单结构的目标是让每个因子只在少数几个观测变量上有高载荷，而在其他变量上载荷接近于零。这使得因子的解释变得更容易。**Varimax旋转**就是这样一种流行的方法，它通过最大化载荷平方值的列内[方差](@entry_id:200758)来实现这一目标 。

2.  **目标旋转（Procrustes Rotation）**：在验证性[因子分析](@entry_id:165399)（Confirmatory Factor Analysis, CFA）中，研究者可能对因子结构有先验的理论假设。例如，我们可能假设因子1代表“认知能力”，它应该在与数学和逻辑相关的测试上有高载荷；因子2代表“语言能力”，它应该在与词汇和阅读相关的测试上有高载荷。我们可以将这个假设编码为一个目标载荷矩阵 $T$。然后，我们可以寻找一个正交矩阵 $Q$，使得我们估计的载荷矩阵 $\widehat{\Lambda}$ 经过旋转后，与目标矩阵 $T$ 尽可能接近。这可以通过求解**正交Procrustes问题**来实现，即最小化[弗罗贝尼乌斯范数](@entry_id:143384) $\| \widehat{\Lambda}Q - T \|_F^2$。通过将每个样本的估计旋转到同一个固定的目标，我们可以在不同研究之间强制实现因子解释的一致性 。

3.  **在模型中施加约束**：一种更现代的方法是在模型估计过程中直接施加约束，而不是在估计后进行旋转。
    *   **正交性约束**：一些[因子分析](@entry_id:165399)的变体要求[因子载荷](@entry_id:166383)在某种度量下是正交的，例如，强制要求 $W^\top \Psi^{-1} W$ 为[对角矩阵](@entry_id:637782)。然而，这种约束本身并不总能完全解决不确定性。特别是，当模型拟合出的主成分空间中存在重复的[特征值](@entry_id:154894)时，对应的[特征向量](@entry_id:151813)（即因子方向）仍然可以在其[子空间](@entry_id:150286)内任意旋转，导致识别性问题 。
    *   **稀疏性约束**：一个非常强大的现代约束是**[稀疏性](@entry_id:136793)（sparsity）**。通过向模型的优化目标中加入 $\ell_1$ 惩罚项（如[LASSO](@entry_id:751223)），我们可以鼓励载荷矩阵 $W$ 中的许多元素为零。这种[稀疏性](@entry_id:136793)约束通常会破坏旋转对称性，因为旋转一个稀疏的载荷矩阵通常会得到一个非稀疏的结果。因此，稀疏性惩罚可以引导算法收敛到一个唯一的、具有简单结构的解，其中每个因子只与一小部分变量相关。这不仅使因子更易于解释，而且从根本上解决了旋转不确定性问题 。

### 概率模型视角：[因子分析](@entry_id:165399)及其变体

经典[因子分析](@entry_id:165399)（Factor Analysis, FA）通常是在一个完全概率化的框架内定义的。该模型假设：
- 潜在因子 $f$ 服从[标准正态分布](@entry_id:184509)：$f \sim \mathcal{N}(0, I_k)$。
- 给定因子 $f$ 的情况下，观测数据 $x$ 服从正态分布：$x \mid f \sim \mathcal{N}(\Lambda f, \Psi)$。

#### 使用[EM算法](@entry_id:274778)进行估计

由于潜在因子 $f$ 是未知的，直接最大化观测数据的似然函数是困难的。**期望-最大化（Expectation-Maximization, EM）算法**是解决此类含有[隐变量](@entry_id:150146)的[参数估计](@entry_id:139349)问题的标准工具 。[EM算法](@entry_id:274778)通过迭代执行以下两个步骤来寻找[最大似然估计](@entry_id:142509)：

1.  **E-步（Expectation Step）**：在给定当前参数估计 $\theta^{(t)} = \{\Lambda^{(t)}, \Psi^{(t)}\}$ 和观测数据 $x_n$ 的条件下，计算潜在因子 $f_n$ 的[后验分布](@entry_id:145605)的充分统计量。由于整个模型是高斯的，[后验分布](@entry_id:145605) $p(f_n | x_n, \theta^{(t)})$ 也是高斯分布。我们需要计算其[后验均值](@entry_id:173826) $\mathbb{E}[f_n \mid x_n]$ 和后验二阶矩 $\mathbb{E}[f_n f_n^\top \mid x_n]$。这一步可以被解释为：利用当前的模型，推断出最有可能生成我们所观测到的数据的那些看不见的因子。

2.  **M-步（Maximization Step）**：在给定从E-步计算出的潜在因子后验期望的条件下，最大化[完全数](@entry_id:636981)据的期望[对数似然函数](@entry_id:168593)，以更新模型参数 $\Lambda$ 和 $\Psi$。
    -   $\Lambda$ 的更新可以看作是一个[多元回归](@entry_id:144007)问题，其中我们将观测数据 $x_n$ 回归到其推断出的潜在因子[期望值](@entry_id:153208) $\mathbb{E}[f_n \mid x_n]$ 上。这[实质](@entry_id:149406)上是更新模型以最好地捕捉数据中的**共享协[方差](@entry_id:200758)**。
    -   $\Psi$ 的更新则计算了在用新估计的 $\Lambda$ 解释了数据[方差](@entry_id:200758)之后，每个维度上还剩下多少**残差[方差](@entry_id:200758)**。这个残差[方差](@entry_id:200758)就是对异质[方差](@entry_id:200758) $\Psi$ 的新估计。

[EM算法](@entry_id:274778)通过在这两个步骤之间迭代，不断完善对数据[方差](@entry_id:200758)的分解，直到参数收敛 。

#### [因子分析](@entry_id:165399)（FA）与概率PCA（PPCA）的比较

**概率主成分分析（Probabilistic PCA, PPCA）**可以看作是[因子分析](@entry_id:165399)的一个特例。F[A模型](@entry_id:158323)允许每个观测变量有其自身的异质[方差](@entry_id:200758)（$\Psi$ 是一个对角矩阵，对角线元素可以不同），而PPCA则施加了一个更强的约束，即所有维度的噪声[方差](@entry_id:200758)都是相同的，即 $\Psi = \sigma^2 I_p$（各向同性噪声）。

这个约束的差异导致了两者在建模能力上的重要区别 ：
- **灵活性**：FA更灵活，能够适应**异[方差](@entry_id:200758)噪声**，即不同变量具有不同水平的测量误差或独有变异。
- **简约性**：PPCA更简约，参数更少。当噪声确实接近各向同性时，它可以提供一个更稳定、更不易过拟合的模型。

然而，如果数据中的噪声确实是异[方差](@entry_id:200758)的，使用PPC[A模型](@entry_id:158323)将会导致模型错配。例如，假设真实的噪声[方差](@entry_id:200758)在不同维度间差异很大，PPCA为了拟[合数](@entry_id:263553)据，只能选择一个“平均”的噪声水平 $\sigma^2$。这将导致它高估某些维度的噪声[方差](@entry_id:200758)，同时低估另一些维度的噪声[方差](@entry_id:200758)。相比之下，FA能够准确地为每个维度捕捉其独特的噪声水平，从而提供对[数据协方差](@entry_id:748192)结构更精确的描述。通过比较两种模型在真实数据上的预期[对数似然](@entry_id:273783)，可以量化FA在处理异[方差](@entry_id:200758)噪声时的优势 。

### 潜在因子在监督学习与因果推断中的应用

到目前为止，我们主要将潜在[因子模型](@entry_id:141879)视为一种[无监督学习](@entry_id:160566)方法，用于理解数据 $x$ 自身的结构。然而，其在监督学习和因果推断中也扮演着至关重要的角色。

#### 监督降维：PCA vs. PLS

在监督学习任务中，我们的目标是预测一个响应变量 $y$。当预测变量 $x$ 的维度很高时，我们可能希望先将其[降维](@entry_id:142982)到一个或多个潜在因子上，然后用这些因子来预测 $y$。

- **PCA** 作为一种无监督方法，它提取的因子（主成分）是那些能够最大化解释 $x$ 本身[方差](@entry_id:200758)的方向。然而，这些[方差](@entry_id:200758)最大的方向可能与预测 $y$ 毫无关系。例如，数据中可能存在一个[方差](@entry_id:200758)极大的主成分，但它恰好与 $y$ 正交（不相关），因此对预测毫无帮助 。

- **偏最小二乘（Partial Least Squares, PLS）**是一种监督降维方法。与PCA不同，PLS寻找的潜在因子方向是那些能够最大化其与响应变量 $y$ 的**协[方差](@entry_id:200758)**的方向。这确保了提取出的因子是与预测任务最相关的。在一个情境中，如果 $x$ 中[方差](@entry_id:200758)最大的方向与 $y$ 无关，而一个[方差](@entry_id:200758)较小的方向却能完美预测 $y$，那么PCA会选择前者而失败，PLS则会选择后者而成功 。这揭示了一个重要原则：在监督学习中，[特征提取](@entry_id:164394)的目标应与预测目标对齐。

#### 建模未观测的[混淆变量](@entry_id:199777)

潜在[因子模型](@entry_id:141879)最强大的应用之一是处理**未观测的混淆变量（unobserved confounders）**。在[回归分析](@entry_id:165476)中，如果一个未被包含在模型中的变量同时影响了[自变量](@entry_id:267118) $x$ 和因变量 $y$，它就会导致对 $x$ 和 $y$ 之间关系的估计产生偏误，即**遗漏变量偏误（omitted variable bias）**。

考虑一个简单的数据生成过程，其中潜在一元因子 $z_i$ 同时影响 $x_i$ 和 $y_i$：
- $x_i = w z_i + \varepsilon_i$
- $y_i = \beta x_i + \theta z_i + \xi_i$

在这里，$\beta$ 是我们关心的 $x_i$ 对 $y_i$ 的真实因果效应。然而，由于 $z_i$ 是一个[共同原因](@entry_id:266381)，它诱导了 $x_i$ 和回归误差项（其中包含 $z_i$）之间的相关性。如果分析师不知道 $z_i$ 的存在，而仅仅对 $y_i$ 和 $x_i$ 进行普通最小二乘（OLS）回归，那么得到的 $\beta$ 的估计量 $\hat{\beta}_{\mathrm{OLS}}$ 将是有偏的。其概率极限为：

$$
\text{plim}_{n \to \infty} \hat{\beta}_{\mathrm{OLS}} = \beta + \frac{w \theta \sigma_{z}^{2}}{w^{2} \sigma_{z}^{2} + \sigma_{\varepsilon}^{2}}
$$

这个等式中的第二项就是由未观测的混淆因子 $z_i$ 引起的**渐进偏误** 。只有当 $z_i$ 与 $x_i$ 无关（$w=0$）或与 $y_i$ 无关（$\theta=0$）时，偏误才会消失。如果我们可以观测到（或准确估计出）$z_i$，并将其作为[协变](@entry_id:634097)量加入[回归模型](@entry_id:163386)中，那么对 $\beta$ 的估计将是无偏的。

一个经典的真实世界案例是**全基因组关联研究（GWAS）**中的**[群体分层](@entry_id:175542)（population stratification）**问题 。在一个混合了不同祖先群体的样本中，某个基因型（如一个SNP）的频率可能在不同群体间存在差异。同时，由于环境、生活方式等因素的不同，某个表型（如身高或疾病风险）的均值也可能在这些群体间存在差异。在这种情况下，“祖先”就是一个潜在的混淆因子。它同时与基因型和表型相关。如果在分析中忽略群体结构，可能会观察到[基因型与表型](@entry_id:142682)之间的**伪关联（spurious association）**。

PCA为此问题提供了一个优雅的解决方案。通过对大规[模的基](@entry_id:156416)因型数据矩阵进行PCA，研究者可以提取出反映个体间主要遗传变异模式的主成分。这些主成分通常能够很好地捕捉个体的祖先背景。因此，将这些主成分作为协变量加入到GWAS的回归模型中，就相当于控制了“祖先”这个混淆因子，从而可以大大减少伪关联，得到对基因型真实效应更准确的估计 。

### 超越二阶结构：[独立成分分析](@entry_id:261857)

[因子分析](@entry_id:165399)及其变体主要关注数据的二阶统计量（协[方差](@entry_id:200758)）。它们假设潜在因子是不相关的。**[独立成分分析](@entry_id:261857)（Independent Component Analysis, ICA）**则提出了一个更强的假设：潜在因子是**统计独立的（statistically independent）**。

统计独立是一个比不相关强得多的条件。不相关只意味着二阶[交叉](@entry_id:147634)矩为零，而独立意味着关于这些变量的任何函数都无法提供关于其他变量的信息。这个强假设的代价和回报是什么？

- **回报：解决旋转不确定性**。只要潜在因子中至多只有一个是[高斯分布](@entry_id:154414)的，ICA就能够唯一地识别出混合矩阵 $A$（在 $x=As$ 模型中），仅存在排序和尺度的模糊性。它利用了数据的[高阶统计量](@entry_id:193349)（如[峰度](@entry_id:269963)），这些统计量在旋转下会发生变化。因此，ICA能够恢复出潜在的独立源信号，而不仅仅是它们所在的[子空间](@entry_id:150286) 。
- **代价：对高斯性的敏感**。如果潜在因子本身是高斯分布的，那么它们的任何正交混合也都是[高斯分布](@entry_id:154414)的，并且分量之间仍然不相关。在这种情况下，[高阶统计量](@entry_id:193349)为零，ICA的准则失效，模型变得不可识别。

因此，FA和ICA适用于不同的场景 ：
- 当你相信潜在的源信号是**非高斯**的，并且你想恢复这些**独立的源信号**时（如“鸡尾酒会问题”中分离不同人的声音），ICA是正确的工具。
- 当你假设潜在的因子是**高斯**的，或者你只关心解释数据的**协[方差](@entry_id:200758)结构**，而不关心恢复特定的独立信号时，FA是合适的模型。

### 理论与实践中的高级考量

最后，我们简要提及两个高级主题。

首先，关于因子[子空间](@entry_id:150286)的可识别性的理论基础。我们如何确定数据中确实存在一个稳定的、可识别的秩-$k$ 结构？从[奇异值分解](@entry_id:138057)（SVD）的角度来看，如果数据矩阵 $X$ 的[奇异值](@entry_id:152907)谱在第 $k$ 个和第 $k+1$ 个[奇异值](@entry_id:152907)之间存在一个明显的“间隙”（即 $\sigma_k > \sigma_{k+1}$），那么与前 $k$ 个奇异值相关联的主导[子空间](@entry_id:150286)就是唯一且稳定可识别的。如果没有这样的间隙（$\sigma_k \approx \sigma_{k+1}$），那么从数据中估计出的[子空间](@entry_id:150286)可能不稳定，从而导致因子解释的困难 。

其次，是实际应用中的算法选择。一旦确定了模型，我们必须选择一个算法来拟合它。对于复杂的概率模型，我们通常有几种选择 ：
- **[EM算法](@entry_id:274778)**：对于许多模型（如[高斯混合模型](@entry_id:634640)、标准FA），[EM算法](@entry_id:274778)能够高效地找到参数的[点估计](@entry_id:174544)（最大似然或最大后验估计）。它适合于目标是获得快速、鲁棒的参数值的任务。
- **[马尔可夫链蒙特卡洛](@entry_id:138779)（MCMC）**：当我们需要对参数的不确定性进行精确量化时（例如，计算[可信区间](@entry_id:176433)来检验科学假设），MCMC是“黄金标准”。它通过从[后验分布](@entry_id:145605)中采样来近似整个后验分布，但计算成本通常很高。
- **[变分推断](@entry_id:634275)（VI）**：VI在速度和精度之间提供了一种折衷。它将推断问题转化为一个[优化问题](@entry_id:266749)，寻找一个简单的[分布](@entry_id:182848)来近似真实的后验分布。VI通常比MCMC快得多，并且可以通过[随机优化](@entry_id:178938)（如[随机变分推断](@entry_id:635911), SVI）扩展到海量数据集，但它可能会低估后验不确定性。

对这些算法的选择取决于具体应用中的数据规模、模型复杂性以及最终的推断目标。