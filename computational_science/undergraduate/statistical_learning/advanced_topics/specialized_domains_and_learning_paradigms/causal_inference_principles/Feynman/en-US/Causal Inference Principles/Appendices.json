{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of drawing causal conclusions from observational data is addressing confounding. Before we employ sophisticated estimators, it's crucial to understand when a standard regression adjustment is sufficient and when it will lead to biased results. This practice guides you through several thought experiments to build a robust intuition for the fundamental principle of conditional ignorability, also known as unconfoundedness . By analyzing different causal structures, you will learn to identify scenarios where conditioning on covariates successfully closes \"backdoor paths\" and where it fails due to unobserved confounders or imperfect proxy variables.",
            "id": "3106696",
            "problem": "Consider a binary treatment variable $T \\in \\{0,1\\}$, an observed covariate $X$, a possibly unobserved variable $U$, and an outcome $Y$. Assume the Stable Unit Treatment Value Assumption (SUTVA) and consistency, so that the observed outcome satisfies $Y = Y(T)$ for each unit, and define the average treatment effect (ATE) as $E\\big[Y(1) - Y(0)\\big]$. You will investigate whether regression adjustment using the conditional mean $E[Y \\mid T, X]$ suffices to identify the ATE, starting from the definitions of conditional independence and the law of total expectation. In all scenarios below, assume the error term $\\varepsilon$ is independent of $(T,X,U)$ with $E[\\varepsilon] = 0$ and finite variance.\n\nFor each scenario, determine whether using the regression function $E[Y \\mid T, X]$ is sufficient to identify the ATE, that is, whether the conditions under which $E[Y \\mid T, X]$ can be used to recover $E\\big[Y(1) - Y(0)\\big]$ hold. Select all scenarios where this is true.\n\nOption A:\n- $X \\sim \\text{Bernoulli}(0.5)$, $T \\sim \\text{Bernoulli}(0.5)$, and $T$ is independent of $X$.\n- The outcome is generated by $Y = 2T + X + \\varepsilon$.\n- There is no $U$ in this scenario.\n\nOption B:\n- $U \\sim \\text{Bernoulli}(0.5)$, $X \\sim \\text{Bernoulli}(0.5)$, and $X$ is independent of $U$.\n- The treatment is deterministically $T = U$.\n- The outcome is generated by $Y = 2T + U + \\varepsilon$.\n\nOption C:\n- $U \\sim \\text{Bernoulli}(0.5)$.\n- The observed covariate $X$ is a noisy proxy for $U$: $P(X = U) = 0.8$ and $P(X \\neq U) = 0.2$.\n- The treatment is deterministically $T = U$.\n- The outcome is generated by $Y = 2T + U + \\varepsilon$.\n\nOption D:\n- $U \\sim \\text{Bernoulli}(0.5)$, and $X = U$ (that is, $X$ equals $U$ without error).\n- The treatment is randomized conditional on $U$ so that $P(T = 1 \\mid U = 1) = 0.8$ and $P(T = 1 \\mid U = 0) = 0.2$.\n- The outcome is generated by $Y = 2T + U + \\varepsilon$.",
            "solution": "The problem asks to determine in which of the four scenarios the regression adjustment formula, based on the conditional expectation $E[Y \\mid T, X]$, is sufficient to identify the Average Treatment Effect (ATE), defined as $E\\big[Y(1) - Y(0)\\big]$.\n\nFirst, let's establish the theoretical foundation. The problem states we can assume SUTVA and consistency, which means $Y = Y(T)$. The ATE is $E[Y(1) - Y(0)]$. By the law of total expectation, we can write the ATE as:\n$$ E\\big[Y(1) - Y(0)\\big] = E_X\\Big[E\\big[Y(1) \\mid X\\big] - E\\big[Y(0) \\mid X\\big]\\Big] $$\nThe regression adjustment estimand is defined as:\n$$ \\tau_{adj} = E_X\\Big[E[Y \\mid T=1, X] - E[Y \\mid T=0, X]\\Big] $$\nBy consistency, $E[Y \\mid T=1, X] = E[Y(1) \\mid T=1, X]$ and $E[Y \\mid T=0, X] = E[Y(0) \\mid T=0, X]$.\nFor $\\tau_{adj}$ to be equal to the ATE, we require the conditional ignorability assumption (also known as unconfoundedness or selection on observables) to hold:\n$$ \\big(Y(1), Y(0)\\big) \\perp T \\mid X $$\nThis means that, conditional on the covariates $X$, the treatment assignment $T$ is independent of the potential outcomes. In the language of causal graphs, this is equivalent to stating that the set of covariates $X$ blocks all backdoor paths from the treatment $T$ to the outcome $Y$. A backdoor path is a non-causal path between $T$ and $Y$ that has an arrow into $T$.\n\nThe structural equation for the outcome in all scenarios implies that the true causal effect for any unit is $Y(1) - Y(0) = 2$. Therefore, the true Average Treatment Effect (ATE) is $E[2] = 2$. The problem reduces to checking in which scenarios the regression adjustment estimand $\\tau_{adj}$ is equal to $2$.\n\n### Option A\n- **Setup**: $T$ is fully randomized and independent of $X$. There is no unobserved confounder $U$.\n- **Analysis**: The conditional ignorability assumption $(Y(1), Y(0)) \\perp T \\mid X$ holds because treatment is independent of potential outcomes (which are functions of $X$ and $\\varepsilon$) even without conditioning on $X$. This is a randomized controlled trial. There are no backdoor paths to block. The regression adjustment estimand will recover the true ATE.\n- **Verdict**: Correct.\n\n### Option B\n- **Setup**: The treatment is determined by an unobserved variable $U$, so $T=U$. $U$ also directly affects the outcome $Y$. The observed covariate $X$ is independent of $U$.\n- **Analysis**: $U$ is a confounder, creating a backdoor path $T \\leftarrow U \\rightarrow Y$. Since $U$ is unobserved and the observed covariate $X$ is unrelated to $U$, conditioning on $X$ does not block this path. Conditional ignorability fails. The regression adjustment estimand will be biased. The observed outcome is $Y = 2T + T + \\varepsilon = 3T + \\varepsilon$, leading to an estimated effect of 3, not 2.\n- **Verdict**: Incorrect.\n\n### Option C\n- **Setup**: Similar to Option B, $U$ is an unobserved confounder ($T=U$). We observe a noisy proxy $X$ for $U$.\n- **Analysis**: The backdoor path $T \\leftarrow U \\rightarrow Y$ is still present. Conditioning on a proxy for the confounder, $X$, only partially blocks the backdoor path. This leads to residual confounding. Conditional ignorability fails. The regression adjustment estimand will be biased (it will still be 3, as conditioning on $X$ does not change the expectation of $Y$ given $T=U$).\n- **Verdict**: Incorrect.\n\n### Option D\n- **Setup**: $U$ is a common cause of $T$ and $Y$. However, $U$ is perfectly observed, as $X=U$.\n- **Analysis**: The backdoor path $T \\leftarrow U \\rightarrow Y$ exists. By conditioning on $X$, we are conditioning on the confounder $U$ itself. This successfully blocks the backdoor path. The conditional ignorability assumption $(Y(1), Y(0)) \\perp T \\mid X$ holds because once we condition on $X=U$, treatment assignment $T$ is independent of the potential outcomes (which are also functions of $U$ and the independent error $\\varepsilon$). The regression adjustment estimand will recover the true ATE.\n- **Verdict**: Correct.\n\nThe scenarios where using the regression function $E[Y \\mid T, X]$ is sufficient to identify the ATE are A and D.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Having established the necessity of adjusting for confounding, we now turn to a powerful method for doing so: Inverse Propensity Weighting (IPW). While theoretically elegant, applying IPW in practice reveals a common challenge related to the bias-variance trade-off. This exercise provides hands-on experience with the practical issue of extreme propensity scores, which can lead to estimators with very high variance . You will implement a standard IPW estimator and then explore propensity score trimming—a technique used to stabilize the estimates—providing a tangible understanding of how to balance bias and variance in real-world causal analyses.",
            "id": "3106651",
            "problem": "You are given a binary treatment variable $A \\in \\{0,1\\}$, a real-valued covariate $X \\in \\mathbb{R}$, and an outcome $Y \\in \\mathbb{R}$. Assume the Stable Unit Treatment Value Assumption (SUTVA), unconfoundedness $Y(a) \\perp A \\mid X$ for $a \\in \\{0,1\\}$, and positivity $0 < p(X) < 1$ almost surely, where $p(X)$ is the propensity score $p(X) = \\mathbb{P}(A=1 \\mid X)$. The target is the Average Treatment Effect (ATE) $\\tau = \\mathbb{E}[Y(1) - Y(0)]$. Consider the Inverse Probability Weighting (IPW) estimator of $\\tau$, which in its basic Horvitz–Thompson form uses weights $1/p(X)$ for treated and $1/(1-p(X))$ for control.\n\nTo assess robustness to extreme weights and study the bias–variance trade-off, define a trimming rule based on clipping the propensity score at a threshold $t \\in [0, 1/2)$. Let $p_t(X) = \\min(\\max(p(X), t), 1 - t)$. The clipped IPW estimator replaces $p(X)$ by $p_t(X)$ in the weights. This reduces variance but can introduce bias. Your task is to design and implement a robustness check that:\n- computes the untrimmed IPW estimator and its variance estimate,\n- computes the clipped IPW estimator across a grid of thresholds and quantifies the empirical bias–variance trade-off,\n- proposes and selects an adaptive trimming threshold based on the propensity score that balances a proxy of mean squared error (MSE),\n- and compares this adaptive choice to an oracle that minimizes true MSE (available in this simulation setting).\n\nBase your derivations on the core definitions and the law of iterated expectations. Do not assume any formula that directly gives the bias induced by clipping; instead, derive a workable plug-in approximation using conditional outcome models $\\mathbb{E}[Y \\mid A=a, X]$ estimated from data.\n\nImplementation details to follow exactly:\n1. Data-generating process. For each test case, simulate $n$ observations as follows. Draw $X \\sim \\mathcal{N}(0,1)$. Define the logit $\\ell(X) = \\beta_0 + \\beta_1 X$ and the propensity score $p(X) = \\sigma(\\ell(X)) = 1/(1+\\exp(-\\ell(X)))$. Draw $A \\sim \\mathrm{Bernoulli}(p(X))$. Let $Y = \\tau A + \\gamma X + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $(X,A)$. The true ATE is the constant $\\tau$ in all test cases.\n2. Untrimmed IPW estimator. Using the known $p(X)$, compute the untrimmed Horvitz–Thompson IPW estimate\n   $$ \\widehat{\\tau}_0 = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{A_i Y_i}{p(X_i)} - \\frac{(1-A_i)Y_i}{1-p(X_i)}\\right). $$\n   Estimate its variance by the sample variance of the summands divided by $n$:\n   $$ \\widehat{\\mathbb{V}}_0 = \\frac{1}{n}\\widehat{\\mathrm{Var}}\\left(\\frac{A Y}{p(X)} - \\frac{(1-A)Y}{1-p(X)}\\right). $$\n3. Clipped IPW estimator. For a grid $\\mathcal{T} = \\{0.00, 0.01, 0.02, \\ldots, 0.20\\}$, define $p_t(X)$ for each $t \\in \\mathcal{T}$ and compute\n   $$ \\widehat{\\tau}_t = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{A_i Y_i}{p_t(X_i)} - \\frac{(1-A_i)Y_i}{1-p_t(X_i)}\\right), \\quad \\widehat{\\mathbb{V}}_t = \\frac{1}{n}\\widehat{\\mathrm{Var}}\\left(\\frac{A Y}{p_t(X)} - \\frac{(1-A)Y}{1-p(X)}\\right). $$\n4. Bias–variance analysis. Derive an approximation to the clipping-induced bias using the law of iterated expectations and conditional outcome regression functions $m_a(x) = \\mathbb{E}[Y \\mid A=a, X=x]$. Use ordinary least squares to fit a single linear model with interaction,\n   $$ Y = \\alpha_0 + \\alpha_A A + \\alpha_X X + \\alpha_{AX} (A X) + \\text{noise}, $$\n   and then set $\\widehat{m}_0(x) = \\alpha_0 + \\alpha_X x$ and $\\widehat{m}_1(x) = \\alpha_0 + \\alpha_A + (\\alpha_X + \\alpha_{AX}) x.$\n   Show that the expected bias from clipping admits the approximation\n   $$ \\mathrm{Bias}(t) \\approx \\mathbb{E}\\left[\\widehat{m}_1(X)\\left(\\frac{p(X)}{p_t(X)} - 1\\right) - \\widehat{m}_0(X)\\left(\\frac{1-p(X)}{1-p_t(X)} - 1\\right)\\right], $$\n   and implement its sample analogue by averaging over the observed sample. Define the proxy mean squared error\n   $$ \\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t) = \\widehat{\\mathbb{V}}_t + \\left(\\widehat{\\mathrm{Bias}}(t)\\right)^2. $$\n5. Oracle and adaptive trimming. For each $t \\in \\mathcal{T}$, define the oracle mean squared error using the true $\\tau$:\n   $$ \\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t) = \\widehat{\\mathbb{V}}_t + \\left(\\widehat{\\tau}_t - \\tau\\right)^2. $$\n   Define $t_{\\mathrm{oracle}}$ as any minimizer of $\\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t)$ over $\\mathcal{T}$, choosing the smallest $t$ in case of ties. Define $t_{\\mathrm{adapt}}$ as any minimizer of $\\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t)$ over $\\mathcal{T}$, choosing the smallest $t$ in case of ties. Report the adaptive trimmed estimate $\\widehat{\\tau}_{t_{\\mathrm{adapt}}}$ and its true mean squared error $\\widehat{\\mathbb{V}}_{t_{\\mathrm{adapt}}} + (\\widehat{\\tau}_{t_{\\mathrm{adapt}}} - \\tau)^2$.\n6. Test suite. Implement the above for the following four simulation scenarios, each with the stated random seed, which must be used to initialize a pseudo-random number generator so that results are reproducible:\n   - Case $1$: $n = 4000$, $\\beta_0 = 0.0$, $\\beta_1 = 1.2$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 1$.\n   - Case $2$: $n = 4000$, $\\beta_0 = -2.5$, $\\beta_1 = 3.5$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 2$.\n   - Case $3$: $n = 300$, $\\beta_0 = -3.0$, $\\beta_1 = 4.0$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.5$, $\\text{seed} = 3$.\n   - Case $4$: $n = 4000$, $\\beta_0 = 0.0$, $\\beta_1 = 0.0$ (constant propensity score $p(X)=0.5$), $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 4$.\n7. Program output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a list of six floating-point numbers in the following order:\n   - $\\widehat{\\tau}_0$,\n   - $t_{\\mathrm{oracle}}$,\n   - $t_{\\mathrm{adapt}}$,\n   - $\\widehat{\\tau}_{t_{\\mathrm{adapt}}}$,\n   - $\\min_{t \\in \\mathcal{T}} \\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t)$,\n   - $\\widehat{\\mathbb{V}}_{t_{\\mathrm{adapt}}} + (\\widehat{\\tau}_{t_{\\mathrm{adapt}}} - \\tau)^2$.\n   All floating-point numbers should be rounded to $6$ decimal places. The final output should thus be a single line representing a list of four inner lists (one per case), for example, $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$.",
            "solution": "This problem requires a simulation study to explore the bias-variance trade-off of the Inverse Probability Weighting (IPW) estimator when propensity scores are trimmed.\n\n**1. IPW Estimator and Trimming**\nThe untrimmed Horvitz-Thompson IPW estimator for the Average Treatment Effect (ATE) is unbiased under the stated assumptions. Its expectation is:\n$$ \\mathbb{E}[\\widehat{\\tau}_0] = \\mathbb{E}\\left[\\frac{A Y}{p(X)}\\right] - \\mathbb{E}\\left[\\frac{(1-A)Y}{1-p(X)}\\right] = \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] = \\tau $$\nHowever, its variance can be large if propensity scores $p(X)$ are close to 0 or 1. Clipping the propensity scores at a threshold $t$ replaces $p(X)$ with $p_t(X) = \\min(\\max(p(X), t), 1-t)$. This bounds the inverse probability weights, reducing variance but introducing bias.\n\n**2. Bias Derivation**\nThe expectation of the clipped estimator $\\widehat{\\tau}_t$ is:\n$$ \\mathbb{E}[\\widehat{\\tau}_t] = \\mathbb{E}\\left[\\frac{p(X)}{p_t(X)}m_1(X) - \\frac{1-p(X)}{1-p_t(X)}m_0(X)\\right] $$\nwhere $m_a(X) = \\mathbb{E}[Y \\mid A=a, X]$. The bias is the difference between this expectation and the true ATE, $\\tau = \\mathbb{E}[m_1(X) - m_0(X)]$.\n$$ \\mathrm{Bias}(t) = \\mathbb{E}[\\widehat{\\tau}_t] - \\tau = \\mathbb{E}\\left[m_1(X)\\left(\\frac{p(X)}{p_t(X)} - 1\\right) - m_0(X)\\left(\\frac{1-p(X)}{1-p_t(X)} - 1\\right)\\right] $$\nThis formula shows that bias is introduced only in the regions where clipping occurs (i.e., where $p(X) \\neq p_t(X)$).\n\n**3. Adaptive Threshold Selection**\nIn practice, the bias formula is not directly usable as the true outcome models $m_a(X)$ are unknown. We can estimate them from data, for instance, by fitting an OLS model as specified in the problem. By plugging in the estimated models $\\widehat{m}_a(X)$, we get a sample-based estimate of the bias, $\\widehat{\\mathrm{Bias}}(t)$.\nThis allows us to construct a proxy for the Mean Squared Error (MSE):\n$$ \\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t) = \\widehat{\\mathbb{V}}_t + (\\widehat{\\mathrm{Bias}}(t))^2 $$\nwhere $\\widehat{\\mathbb{V}}_t$ is the estimated variance of the clipped estimator. The adaptive threshold $t_{\\mathrm{adapt}}$ is chosen to minimize this proxy MSE, providing a data-driven way to balance the bias-variance trade-off. This is contrasted with an oracle threshold $t_{\\mathrm{oracle}}$ that minimizes the true sample MSE, which is only available in a simulation context.\n\nThe simulation proceeds by generating data for each case, calculating the untrimmed and trimmed estimates, estimating the bias, finding the adaptive and oracle thresholds, and reporting the required performance metrics. The results demonstrate how trimming can be beneficial, especially in cases with extreme propensity scores (Case 2 and 3), and how the adaptive procedure can approximate the optimal level of trimming.",
            "answer": "[[1.011884,0.020000,0.030000,1.011311,0.001646,0.001672],[1.085731,0.060000,0.060000,1.085731,0.031572,0.031572],[1.704285,0.100000,0.090000,1.139634,0.169134,0.177001],[1.003185,0.000000,0.000000,1.003185,0.001633,0.001633]]"
        },
        {
            "introduction": "We have seen that both outcome regression and inverse propensity weighting rely on correctly specified models to produce unbiased causal estimates. What if we could build an estimator that is more forgiving? This practice introduces the Augmented Inverse Probability Weighted (AIPW) estimator, a method that cleverly combines both an outcome model and a propensity score model to achieve a desirable property known as double robustness . By simulating scenarios where one of the two models is misspecified, you will empirically verify that the AIPW estimator can still yield a consistent estimate of the treatment effect, demonstrating firsthand why it is a preferred and powerful tool in modern causal inference.",
            "id": "3106777",
            "problem": "Consider a binary treatment setting with potential outcomes $Y(1)$ and $Y(0)$, an observed covariate $X \\in \\mathbb{R}$, a treatment indicator $T \\in \\{0,1\\}$, and an observed outcome $Y \\in \\mathbb{R}$. Assume the following foundational principles: consistency $Y = T Y(1) + (1 - T) Y(0)$, conditional ignorability $(Y(1), Y(0)) \\perp T \\mid X$, and positivity $0 < \\mathbb{P}(T = 1 \\mid X = x) < 1$ for all realizations $x$. The target parameter is the average treatment effect (ATE), defined as $\\Delta = \\mathbb{E}[Y(1) - Y(0)]$.\n\nYou will study the Augmented Inverse Probability Weighted (AIPW) estimator under two statistical learning models:\n- An outcome regression model $m_t(x) \\approx \\mathbb{E}[Y \\mid T = t, X = x]$ for $t \\in \\{0,1\\}$.\n- A propensity score model $e(x) \\approx \\mathbb{P}(T = 1 \\mid X = x)$.\n\nDevelop an example where the outcome regression is correctly specified while the propensity score is misspecified, and empirically demonstrate the double robustness property of the Augmented Inverse Probability Weighted (AIPW) estimator, which states that the estimator is consistent if either the outcome regression model is correctly specified or the propensity score model is correctly specified (not necessarily both).\n\nData generating process for simulation:\n- Covariate: $X \\sim \\mathcal{N}(0,1)$.\n- Treatment assignment: $T \\mid X \\sim \\text{Bernoulli}(e^\\star(X))$, where $e^\\star(x) = \\frac{1}{1 + \\exp(-(\\alpha x + c))}$ is the logistic function with parameters $\\alpha$ and $c$.\n- Outcome: $Y = \\beta_0 + \\beta_1 X + \\tau T + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X$ and $T$ given $X$ and $T$. The true average treatment effect is constant and equal to $\\tau$.\n\nModel specifications to be used in estimation:\n- Correct outcome regression: regress $Y$ on an intercept, $T$, and $X$; then set $m_1(x)$ and $m_0(x)$ to the fitted conditional means under $T = 1$ and $T = 0$, respectively, using this linear model.\n- Misspecified outcome regression: regress $Y$ on an intercept and $T$ only (omit $X$).\n- Correct propensity score: use the true function $e^\\star(x) = \\frac{1}{1 + \\exp(-(\\alpha x + c))}$ with the known parameters $\\alpha$ and $c$ from the data generating process.\n- Misspecified propensity score: use the constant function $e(x) = 0.5$.\n\nFrom the foundational principles and the definitions above, derive the AIPW estimator for $\\Delta$ and implement it. For a sample $\\{(X_i,T_i,Y_i)\\}_{i=1}^n$, with estimated $m_1(X_i)$, $m_0(X_i)$, and $e(X_i)$, compute the AIPW estimate of $\\Delta$ and report its bias relative to the true $\\tau$.\n\nTest suite:\nFor each test, simulate data according to the data generating process with the specified parameters, fit the models as specified, compute the AIPW estimate $\\widehat{\\Delta}_{\\text{AIPW}}$, and report the bias $\\widehat{\\Delta}_{\\text{AIPW}} - \\tau$ as a float rounded to $6$ decimal places.\n\n- Test $1$ (double robustness with correct outcome and misspecified propensity, happy path): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression correct, propensity score misspecified, random seed $123$.\n- Test $2$ (double robustness with correct propensity and misspecified outcome): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression misspecified, propensity score correct, random seed $456$.\n- Test $3$ (both models misspecified): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression misspecified, propensity score misspecified, random seed $789$.\n- Test $4$ (both models correct): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression correct, propensity score correct, random seed $321$.\n- Test $5$ (edge case with near-positivity boundary but valid, double robustness with correct outcome and misspecified propensity): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 4.0$, $c = 0.0$, outcome regression correct, propensity score misspecified, random seed $654$.\n\nFinal output format:\nYour program should produce a single line of output containing the five biases as a comma-separated list enclosed in square brackets (e.g., $[b_1,b_2,b_3,b_4,b_5]$), where each $b_j$ is the bias for Test $j$ rounded to $6$ decimal places. No other text should be printed.",
            "solution": "The Augmented Inverse Probability Weighted (AIPW) estimator for the Average Treatment Effect (ATE), $\\Delta = \\mathbb{E}[Y(1) - Y(0)]$, combines a model for the outcome with a model for the treatment assignment (propensity score). Its key advantage is **double robustness**: it provides a consistent estimate if at least one of these two models is correctly specified.\n\nThe AIPW estimator for the ATE can be written as the sample average of a per-observation score:\n$$ \\widehat{\\Delta}_{\\text{AIPW}} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\left(\\frac{T_i(Y_i - \\hat{m}_1(X_i))}{\\hat{e}(X_i)} + \\hat{m}_1(X_i)\\right) - \\left(\\frac{(1-T_i)(Y_i - \\hat{m}_0(X_i))}{1-\\hat{e}(X_i)} + \\hat{m}_0(X_i)\\right) \\right] $$\nHere, $\\hat{m}_t(X_i)$ are the predicted outcomes for treatment level $t \\in \\{0,1\\}$ from an outcome regression model, and $\\hat{e}(X_i)$ is the predicted probability of treatment from a propensity score model.\n\nThe term $\\frac{T_i(Y_i - \\hat{m}_1(X_i))}{\\hat{e}(X_i)}$ serves as a correction to the simple regression-based estimate $\\hat{m}_1(X_i)$.\n- If the outcome model is correct ($\\hat{m}_1(x) \\approx \\mathbb{E}[Y|T=1, X=x]$), then the numerator's expectation is approximately zero, making the estimator robust to errors in the propensity score $\\hat{e}(X_i)$.\n- If the propensity score model is correct ($\\hat{e}(x) \\approx \\mathbb{P}(T=1|X=x)$), the inverse weighting ensures the whole term correctly re-weights the observed data to represent the full population, making the estimator robust to errors in the outcome model $\\hat{m}_1(X_i)$.\n\nThe simulation implements this estimator under different scenarios:\n1.  **Correct Outcome, Misspecified Propensity**: The estimator is protected by the correct outcome model.\n2.  **Misspecified Outcome, Correct Propensity**: The estimator is protected by the correct propensity score model.\n3.  **Both Misspecified**: Both protections fail, and the estimator is expected to be biased.\n4.  **Both Correct**: The estimator has the best statistical properties (lowest variance among unbiased estimators of this class).\n5.  **Edge Case**: A correct outcome model should protect the estimate even when the misspecified propensity model is particularly poor due to near-positivity violations.\n\nThe results of the simulation empirically verify this property by showing near-zero bias in cases 1, 2, 4, and 5, where at least one model is correct, and significant bias in case 3, where both models are misspecified.",
            "answer": "[-0.003923,0.003554,0.444976,-0.003362,-0.000624]"
        }
    ]
}