{
    "hands_on_practices": [
        {
            "introduction": "Gradient descent (GD) is the cornerstone of deep learning optimization, but its success critically depends on the choice of learning rate, $\\eta$. Too small, and convergence is slow; too large, and the process can become unstable. This practice lets you build a concrete intuition for this behavior by exploring how GD performs on a simple quadratic objective function, which approximates the local landscape of more complex loss functions. By simulating different scenarios, you will observe firsthand how the interaction between the learning rate and the curvature of the loss surface governs whether the optimization converges, oscillates, or diverges .",
            "id": "3154406",
            "problem": "Consider the quadratic objective function $f(x) = \\tfrac{1}{2} x^\\top H x$, where $H \\in \\mathbb{R}^{d \\times d}$ is a real symmetric matrix (the Hessian), and $x \\in \\mathbb{R}^d$. Gradient descent with a constant learning rate $\\eta  0$ applies the update $x_{t+1} = x_t - \\eta \\nabla f(x_t)$ for iterates indexed by $t \\in \\{0,1,2,\\dots\\}$. In the quadratic case, this yields a linear iteration $x_{t+1} = (I - \\eta H) x_t$. The behavior of the iterates $x_t$ (convergence, oscillation, or divergence) depends on the interaction between the learning rate $\\eta$ and the eigenvalues of $H$.\n\nYour task is to write a program that, for a given small test suite of matrices $H$ and learning rates $\\eta$, will:\n- Simulate gradient descent for $T$ iterations from a specified initial point $x_0$, using the update $x_{t+1} = x_t - \\eta H x_t$.\n- Compute the spectral radius of the iteration operator $I - \\eta H$, which is defined as the maximum absolute value of the eigenvalues of $I - \\eta H$.\n- Quantify the empirical change in norm by computing the ratio $\\|x_T\\|_2 / \\|x_0\\|_2$.\n- Classify the behavior using an integer code informed by the spectral radius:\n  - $0$ if the spectral radius is strictly less than $1$,\n  - $1$ if the spectral radius is equal to $1$ within a small numerical tolerance,\n  - $2$ if the spectral radius is greater than $1$.\n\nUse the following test suite. For all cases, use $x_0 = \\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$ and $T = 60$.\n- Case A (stable, happy path): $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$, $\\eta = 0.3$.\n- Case B (boundary condition): $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$, $\\eta = \\tfrac{2}{3}$.\n- Case C (over the bound): $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$, $\\eta = 0.9$.\n- Case D (nonconvex edge case with a negative eigenvalue): $H = \\begin{bmatrix}-1  0 \\\\ 0  4\\end{bmatrix}$, $\\eta = 0.3$.\n- Case E (ill-conditioned but under the bound): $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$, $\\eta = 0.19$.\n- Case F (ill-conditioned and over the bound): $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$, $\\eta = 0.21$.\n\nImplementation details and outputs:\n- For each case, compute the eigenvalues $\\{\\lambda_i\\}$ of $H$ and then the spectral radius of $I - \\eta H$ as $\\max_i |1 - \\eta \\lambda_i|$.\n- Simulate the iterates $x_{t+1} = x_t - \\eta H x_t$ for $t = 0,1,\\dots,T-1$ and compute the ratio $r_{\\text{emp}} = \\|x_T\\|_2 / \\|x_0\\|_2$.\n- Classify the behavior using the integer code described above, determined by the spectral radius of $I - \\eta H$.\n- The final output for each case is a list containing the spectral radius (a float), the empirical ratio $r_{\\text{emp}}$ (a float), and the classification code (an integer), in that order.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all six cases as a comma-separated list enclosed in square brackets, where each case contributes a list of the form $[\\text{spectral\\_radius}, \\text{empirical\\_ratio}, \\text{classification}]$. For example, the format is $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots,[a_6,b_6,c_6]]$.",
            "solution": "The problem requires an analysis of the behavior of the gradient descent algorithm applied to a quadratic objective function $f(x) = \\frac{1}{2} x^\\top H x$. The analysis involves both theoretical calculation based on the properties of the Hessian matrix $H$ and the learning rate $\\eta$, and empirical simulation of the iterative process.\n\nThe core of the problem lies in understanding the linear dynamical system that results from applying gradient descent to this specific objective function. The gradient of the function is $\\nabla f(x) = Hx$. The gradient descent update rule is given by:\n$$\nx_{t+1} = x_t - \\eta \\nabla f(x_t)\n$$\nSubstituting the gradient, we obtain a linear recurrence relation:\n$$\nx_{t+1} = x_t - \\eta H x_t = (I - \\eta H) x_t\n$$\nwhere $I$ is the identity matrix and $G = I - \\eta H$ is the iteration operator. The solution to this recurrence is $x_t = G^t x_0$. The long-term behavior of the iterates $x_t$ is determined by the properties of the matrix $G$.\n\nFor the sequence of iterates $x_t$ to converge to the zero vector (which is the unique minimum if $H$ is positive definite), it is necessary and sufficient that the spectral radius of the iteration matrix $G$ is strictly less than $1$. The spectral radius, denoted $\\rho(G)$, is defined as the maximum absolute value of the eigenvalues of $G$.\n$$\n\\rho(G) = \\max_{i} |\\lambda_i(G)|\n$$\nwhere $\\lambda_i(G)$ are the eigenvalues of $G$.\n\nIf $\\lambda_i(H)$ are the eigenvalues of the Hessian matrix $H$, then the eigenvalues of $G = I - \\eta H$ are given by $1 - \\eta \\lambda_i(H)$. Therefore, the spectral radius can be computed directly from the eigenvalues of $H$ and the learning rate $\\eta$:\n$$\n\\rho(I - \\eta H) = \\max_{i} |1 - \\eta \\lambda_i(H)|\n$$\nThe behavior of the gradient descent algorithm can be classified based on the value of $\\rho(I - \\eta H)$:\n- If $\\rho(I - \\eta H)  1$, the norm of the iterates $\\|x_t\\|_2$ will decay to $0$. This is the condition for convergence.\n- If $\\rho(I - \\eta H) = 1$, the norm of the iterates may remain bounded (oscillate) or grow polynomially if the eigenvalue with magnitude $1$ has a Jordan block of size greater than $1$. In this problem, since $H$ is symmetric, $G$ is also symmetric, and all Jordan blocks are of size $1$, so the norm will not diverge.\n- If $\\rho(I - \\eta H)  1$, the norm of the iterates $\\|x_t\\|_2$ will grow exponentially, and the method diverges.\n\nFor a convex problem where $H$ is positive definite (all $\\lambda_i(H)  0$), the convergence condition $\\rho(I - \\eta H)  1$ simplifies to finding an $\\eta$ such that $|1 - \\eta \\lambda_i(H)|  1$ for all $i$. This inequality is equivalent to $-1  1 - \\eta \\lambda_i(H)  1$, which implies $0  \\eta \\lambda_i(H)  2$. This must hold for all eigenvalues $\\lambda_i(H)$, which leads to the well-known condition for the learning rate: $0  \\eta  \\frac{2}{\\lambda_{\\max}(H)}$, where $\\lambda_{\\max}(H)$ is the largest eigenvalue of $H$.\n\nThe empirical ratio of norms, $\\|x_T\\|_2 / \\|x_0\\|_2$, is expected to correlate with the spectral radius. For a symmetric $G$, the induced $2$-norm is equal to the spectral radius, $\\|G\\|_2 = \\rho(G)$. Thus, we have the bound $\\|x_t\\|_2 \\le \\|G\\|_2^t \\|x_0\\|_2 = \\rho(G)^t \\|x_0\\|_2$. The ratio $\\|x_t\\|_2 / \\|x_0\\|_2$ is therefore upper-bounded by $\\rho(G)^t$. The actual value depends on the projection of the initial vector $x_0$ onto the eigenspaces of $G$.\n\nWe now analyze each test case with initial vector $x_0 = \\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$ and $T=60$ iterations.\n\n**Case A:** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$, $\\eta = 0.3$.\n- Eigenvalues of $H$: $\\lambda_1 = 3$, $\\lambda_2 = 1$.\n- Eigenvalues of $I - \\eta H$: $1 - 0.3 \\cdot 3 = 0.1$, and $1 - 0.3 \\cdot 1 = 0.7$.\n- Spectral radius: $\\rho = \\max(|0.1|, |0.7|) = 0.7$.\n- Classification: Since $\\rho=0.7  1$, the classification is $0$ (convergence).\n- The empirical ratio is expected to be small.\n\n**Case B:** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$, $\\eta = \\frac{2}{3}$.\n- Eigenvalues of $H$: $\\lambda_1 = 3$, $\\lambda_2 = 1$.\n- Eigenvalues of $I - \\eta H$: $1 - \\frac{2}{3} \\cdot 3 = -1$, and $1 - \\frac{2}{3} \\cdot 1 = \\frac{1}{3}$.\n- Spectral radius: $\\rho = \\max(|-1|, |\\frac{1}{3}|) = 1.0$.\n- Classification: Since $\\rho=1.0$, the classification is $1$ (boundary). One component of the error vector will oscillate, while the other decays.\n\n**Case C:** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$, $\\eta = 0.9$.\n- Eigenvalues of $H$: $\\lambda_1 = 3$, $\\lambda_2 = 1$.\n- Eigenvalues of $I - \\eta H$: $1 - 0.9 \\cdot 3 = -1.7$, and $1 - 0.9 \\cdot 1 = 0.1$.\n- Spectral radius: $\\rho = \\max(|-1.7|, |0.1|) = 1.7$.\n- Classification: Since $\\rho=1.7  1$, the classification is $2$ (divergence). The iterates will grow exponentially.\n\n**Case D:** $H = \\begin{bmatrix}-1  0 \\\\ 0  4\\end{bmatrix}$, $\\eta = 0.3$.\n- This objective function is non-convex as $H$ has a negative eigenvalue.\n- Eigenvalues of $H$: $\\lambda_1 = -1$, $\\lambda_2 = 4$.\n- Eigenvalues of $I - \\eta H$: $1 - 0.3 \\cdot (-1) = 1.3$, and $1 - 0.3 \\cdot 4 = -0.2$.\n- Spectral radius: $\\rho = \\max(|1.3|, |-0.2|) = 1.3$.\n- Classification: Since $\\rho=1.3  1$, the classification is $2$ (divergence). Gradient descent will diverge, moving away from the saddle point at the origin.\n\n**Case E:** $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$, $\\eta = 0.19$.\n- This matrix is ill-conditioned, with condition number $\\kappa = \\frac{10}{0.1} = 100$. The theoretical learning rate limit is $\\frac{2}{\\lambda_{\\max}} = \\frac{2}{10} = 0.2$.\n- Eigenvalues of $H$: $\\lambda_1 = 10$, $\\lambda_2 = 0.1$.\n- Eigenvalues of $I - \\eta H$: $1 - 0.19 \\cdot 10 = -0.9$, and $1 - 0.19 \\cdot 0.1 = 0.981$.\n- Spectral radius: $\\rho = \\max(|-0.9|, |0.981|) = 0.981$.\n- Classification: Since $\\rho=0.981  1$, the classification is $0$ (convergence). However, convergence will be slow as the spectral radius is very close to $1$.\n\n**Case F:** $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$, $\\eta = 0.21$.\n- The same ill-conditioned matrix as Case E, but the learning rate $\\eta=0.21$ is slightly above the stability threshold of $0.2$.\n- Eigenvalues of $H$: $\\lambda_1 = 10$, $\\lambda_2 = 0.1$.\n- Eigenvalues of $I - \\eta H$: $1 - 0.21 \\cdot 10 = -1.1$, and $1 - 0.21 \\cdot 0.1 = 0.979$.\n- Spectral radius: $\\rho = \\max(|-1.1|, |0.979|) = 1.1$.\n- Classification: Since $\\rho=1.1  1$, the classification is $2$ (divergence). This demonstrates the sensitivity of gradient descent to the learning rate, especially for ill-conditioned problems.\n\nThe following program will implement these calculations and simulations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the behavior of gradient descent on a quadratic objective function\n    for a given suite of test cases.\n\n    For each case, it computes:\n    1. The spectral radius of the iteration operator.\n    2. The empirical ratio of norms after T iterations.\n    3. A classification code based on the spectral radius.\n    \"\"\"\n    \n    # Define the six test cases from the problem statement.\n    test_cases = [\n        # Case A: stable, happy path\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 0.3},\n        # Case B: boundary condition\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 2.0 / 3.0},\n        # Case C: over the bound\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 0.9},\n        # Case D: nonconvex edge case\n        {'H': np.array([[-1.0, 0.0], [0.0, 4.0]]), 'eta': 0.3},\n        # Case E: ill-conditioned but under the bound\n        {'H': np.array([[10.0, 0.0], [0.0, 0.1]]), 'eta': 0.19},\n        # Case F: ill-conditioned and over the bound\n        {'H': np.array([[10.0, 0.0], [0.0, 0.1]]), 'eta': 0.21}\n    ]\n\n    # Shared parameters for all simulations\n    x0 = np.array([1.0, -1.0])\n    T = 60\n    norm_x0 = np.linalg.norm(x0)\n    \n    all_results = []\n    \n    for case in test_cases:\n        H = case['H']\n        eta = case['eta']\n        \n        # 1. Compute the spectral radius of the iteration operator G = I - eta*H\n        # Since H is symmetric, its eigenvalues are real. We use eigvalsh for stability.\n        # The eigenvalues of G are 1 - eta * lambda_i(H).\n        eigenvalues_H = np.linalg.eigvalsh(H)\n        eigenvalues_G = 1.0 - eta * eigenvalues_H\n        spectral_radius = np.max(np.abs(eigenvalues_G))\n        \n        # 2. Simulate gradient descent for T iterations\n        x_t = x0.copy()\n        for _ in range(T):\n            # Update rule: x_{t+1} = x_t - eta * H * x_t\n            x_t = x_t - eta * (H @ x_t)\n            \n        # 3. Compute the empirical change in norm\n        norm_xT = np.linalg.norm(x_t)\n        empirical_ratio = norm_xT / norm_x0\n        \n        # 4. Classify the behavior based on the spectral radius\n        classification = 0  # Default to convergence\n        if np.isclose(spectral_radius, 1.0):\n            classification = 1  # Boundary condition\n        elif spectral_radius  1.0:\n            classification = 2  # Divergence\n            \n        all_results.append([spectral_radius, empirical_ratio, classification])\n\n    # Format the final output string as a list of lists.\n    # e.g., [[valA1,valA2,valA3],[valB1,valB2,valB3],...]\n    result_strings = []\n    for res in all_results:\n        # Format each sublist to '[val1,val2,val3]' without extra spaces\n        formatted_res = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_strings.append(formatted_res)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In real-world deep learning, gradients are often noisy, which can challenge simple optimizers. Adaptive methods like Adam address this by maintaining moving averages of past gradients to inform the update step. This exercise dives into the mechanics of these methods by comparing the popular Adam optimizer to a close variant, AdaBelief, on a deliberately constructed noisy and \"spiky\" loss landscape. Through implementation and comparison, you will see how a subtle change in the algorithm—specifically, how it estimates the variance of the gradients—can lead to more stable training steps and better performance in challenging optimization environments .",
            "id": "3154379",
            "problem": "You are to implement and compare two adaptive optimization methods on a deliberately constructed one-dimensional objective with spiky gradients, and quantitatively evaluate training stability under stochastic noise. The two methods are Adaptive Moment Estimation (Adam) and AdaBelief, an Adam variant in which the second-order accumulator is based on the deviation of the current gradient from its first moment estimate (the optimizer’s \"belief\").\n\nConstruct the objective function\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,x^2 \\;+\\; s\\bigl(1 - \\cos(\\omega x)\\bigr) \\;+\\; \\tfrac{1}{2}\\,s\\bigl(1 - \\cos(3\\,\\omega x)\\bigr),\n$$\nwhere $x \\in \\mathbb{R}$, $s  0$, and $\\omega  0$. This choice ensures a unique minimizer near $x = 0$ for moderate $s$, while the gradient\n$$\n\\nabla f(x) \\;=\\; x \\;+\\; s\\,\\omega\\,\\sin(\\omega x) \\;+\\; \\tfrac{3}{2}\\,s\\,\\omega\\,\\sin(3\\,\\omega x)\n$$\nexhibits high-frequency spikes controlled by $\\omega$. The training dynamics are stochastic: use a noisy gradient $g_t = \\nabla f(x_t) + \\xi_t$, where $\\xi_t$ are independent samples from a zero-mean Gaussian with variance $\\sigma^2$, i.e., $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nStarting from a specified initial point $x_0$, run each optimizer for $T$ updates on the same realized noise sequence $\\{\\xi_t\\}_{t=1}^T$ to ensure a fair comparison. Implement both optimizers using their standard bias-corrected exponential moving averages for first-order and second-order terms; do not hard-code any closed-form or shortcut expressions beyond the standard definitions of the methods. Use fixed hyperparameters $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, and $\\varepsilon = 10^{-8}$.\n\nDefine and compute the following metrics for each optimizer on each test case:\n- The final objective value $f(x_T)$.\n- The average squared update magnitude\n$$\n\\overline{\\Delta^2} \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} (x_{t} - x_{t-1})^2.\n$$\n- The oscillation count, defined as the number of index pairs $(t-1,t)$, $t \\in \\{1,\\dots,T\\}$, such that $x_{t-1}$ and $x_t$ have opposite signs (i.e., the product $x_{t-1}\\,x_t  0$). This measures crossings of the line $x=0$.\n\nFor each test case, produce a boolean result indicating whether AdaBelief stabilizes training better than Adam under noisy gradients. Declare AdaBelief to be \"more stable\" if all of the following hold simultaneously:\n- Its average squared update magnitude is strictly smaller than Adam’s, i.e., $\\overline{\\Delta^2}_{\\text{AdaBelief}}  \\overline{\\Delta^2}_{\\text{Adam}}$ (allow a numerical tolerance of $10^{-12}$).\n- Its oscillation count is less than or equal to Adam’s.\n- Its final objective value is not worse than Adam’s by more than a tolerance of $10^{-3}$, i.e., $f(x_T)_{\\text{AdaBelief}} \\le f(x_T)_{\\text{Adam}} + 10^{-3}$.\n\nUse the following test suite; each case specifies $(\\omega, s, \\sigma, \\alpha, T, \\text{seed}, x_0)$:\n- Case $1$: $\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.1$, $\\alpha = 0.01$, $T = 300$, $\\text{seed} = 42$, $x_0 = 2.0$.\n- Case $2$: $\\omega = 60.0$, $s = 0.6$, $\\sigma = 0.3$, $\\alpha = 0.005$, $T = 500$, $\\text{seed} = 123$, $x_0 = -3.0$.\n- Case $3$ (edge case, no noise): $\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.0$, $\\alpha = 0.005$, $T = 300$, $\\text{seed} = 7$, $x_0 = 1.0$.\n- Case $4$ (high noise): $\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.5$, $\\alpha = 0.01$, $T = 500$, $\\text{seed} = 99$, $x_0 = 2.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true_case1,true_case2,true_case3,true_case4]\"), where each entry is a boolean indicating whether AdaBelief is more stable than Adam for the corresponding test case. No additional text should be printed.",
            "solution": "The problem requires a comparative analysis of two adaptive optimization algorithms, Adam and AdaBelief, on a specified one-dimensional, non-convex objective function under stochastic noise. The goal is to determine which optimizer exhibits greater stability based on a set of quantitative metrics. The solution proceeds by first defining the mathematical components of the problem, then detailing the implementation of the optimizers and the evaluation metrics, and finally executing this procedure for each test case.\n\n### 1. Problem Formulation\n\nThe objective function to be minimized is given by:\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,x^2 \\;+\\; s\\bigl(1 - \\cos(\\omega x)\\bigr) \\;+\\; \\tfrac{1}{2}\\,s\\bigl(1 - \\cos(3\\,\\omega x)\\bigr)\n$$\nwhere $x \\in \\mathbb{R}$ is the parameter to be optimized, and $s  0$ and $\\omega  0$ are constants that control the shape of the function. The first term, $\\tfrac{1}{2}x^2$, provides a convex base, while the cosine terms introduce high-frequency, non-convex oscillations.\n\nThe gradient of this function with respect to $x$ is:\n$$\n\\nabla f(x) \\;=\\; x \\;+\\; s\\,\\omega\\,\\sin(\\omega x) \\;+\\; \\tfrac{3}{2}\\,s\\,\\omega\\,\\sin(3\\,\\omega x)\n$$\nThe optimization is performed in a stochastic setting. At each time step $t$, the optimizer does not have access to the true gradient $\\nabla f(x_t)$ but rather to a noisy version:\n$$\ng_t = \\nabla f(x_t) + \\xi_t\n$$\nwhere $x_t$ is the parameter value at step $t$, and $\\xi_t$ is a noise term sampled independently from a zero-mean Gaussian distribution, $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$.\n\n### 2. Optimizer Algorithms\n\nBoth Adam and AdaBelief are adaptive learning rate methods that use exponential moving averages of the gradients (first moment) and squared gradients (second moment) to adapt the learning rate for each parameter. The optimization is initialized at $x_0$ and runs for $T$ steps. The fixed hyperparameters are $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, and $\\varepsilon = 10^{-8}$. The learning rate is denoted by $\\alpha$.\n\nAt each step $t$ from $1$ to $T$:\n1.  Compute the noisy gradient $g_t = \\nabla f(x_{t-1}) + \\xi_t$.\n2.  Update the first moment estimate, $m_t$:\n    $$\n    m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n    $$\n3.  Update the second moment estimate, $v_t$. This step is where the two algorithms differ.\n    -   **Adam**: The second moment is an exponential moving average of the squared gradients.\n        $$\n        v_t^{\\text{Adam}} = \\beta_2 v_{t-1}^{\\text{Adam}} + (1 - \\beta_2) g_t^2\n        $$\n    -   **AdaBelief**: The second moment is an exponential moving average of the squared difference between the current gradient $g_t$ and its first moment estimate $m_t$. This is the \"belief\" in the current gradient.\n        $$\n        v_t^{\\text{AdaBelief}} = \\beta_2 v_{t-1}^{\\text{AdaBelief}} + (1 - \\beta_2) (g_t - m_t)^2\n        $$\n4.  Compute bias-corrected moment estimates. Since $m_0$ and $v_0$ are initialized to $0$, the estimates are biased towards zero, especially during the initial steps. This bias is corrected as follows:\n    $$\n    \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n    $$\n    $$\n    \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n    $$\n5.  Update the parameter $x_t$:\n    $$\n    x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n    $$\n    The term $\\varepsilon$ is a small constant to prevent division by zero.\n\n### 3. Evaluation Metrics and Stability Criterion\n\nTo quantitatively compare the stability of Adam and AdaBelief, three metrics are computed for each optimizer's trajectory $\\{x_t\\}_{t=0}^T$:\n\n-   **Final objective value $f(x_T)$**: The value of the objective function at the final parameter $x_T$.\n-   **Average squared update magnitude $\\overline{\\Delta^2}$**: This measures the average step size during optimization, calculated as:\n    $$\n    \\overline{\\Delta^2} \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} (x_{t} - x_{t-1})^2\n    $$\n    Smaller values suggest smoother, more stable convergence.\n-   **Oscillation count**: The number of times the parameter trajectory crosses the $x=0$ line. This is counted as the number of indices $t \\in \\{1, \\dots, T\\}$ for which $x_{t-1} \\cdot x_t  0$. Fewer oscillations indicate a more direct path to the minimum.\n\nAdaBelief is declared \"more stable\" than Adam if and only if all three of the following conditions are met:\n1.  $\\overline{\\Delta^2}_{\\text{AdaBelief}}  \\overline{\\Delta^2}_{\\text{Adam}}$ (with a numerical tolerance of $10^{-12}$ for strict inequality).\n2.  $\\text{oscillation\\_count}_{\\text{AdaBelief}} \\le \\text{oscillation\\_count}_{\\text{Adam}}$.\n3.  $f(x_T)_{\\text{AdaBelief}} \\le f(x_T)_{\\text{Adam}} + 10^{-3}$, meaning its final objective value is not significantly worse than Adam's.\n\n### 4. Implementation Strategy\n\nFor each test case defined by the parameters $(\\omega, s, \\sigma, \\alpha, T, \\text{seed}, x_0)$, the following procedure is executed:\n\n1.  A single sequence of Gaussian noise samples, $\\{\\xi_t\\}_{t=1}^T$, is generated using the specified `seed` and standard deviation $\\sigma$. Using the same noise sequence for both optimizers is crucial for a fair comparison of their dynamics.\n2.  Two independent optimization runs are performed, one for Adam and one for AdaBelief, both starting from $x_0$ and using the same learning rate $\\alpha$ and noise sequence $\\{\\xi_t\\}_{t=1}^T$.\n3.  During each run, the entire trajectory of the parameter, $\\{x_0, x_1, \\dots, x_T\\}$, is stored.\n4.  After both runs complete, the three evaluation metrics ($f(x_T)$, $\\overline{\\Delta^2}$, and oscillation count) are calculated for each optimizer based on their respective trajectories.\n5.  The metrics are compared against the three stability conditions to produce a single boolean result for the test case.\n6.  This process is repeated for all four test cases, and the final results are compiled into a list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_optimization_and_compare(w, s, sig, alpha, T, seed, x0):\n    \"\"\"\n    Runs Adam and AdaBelief optimizers for a given test case and compares them\n    based on the specified stability criteria.\n    \"\"\"\n    BETA1 = 0.9\n    BETA2 = 0.999\n    EPSILON = 1e-8\n\n    # Define objective function and its gradient\n    f = lambda x: 0.5 * x**2 + s * (1 - np.cos(w * x)) + 0.5 * s * (1 - np.cos(3 * w * x))\n    grad_f = lambda x: x + s * w * np.sin(w * x) + 1.5 * s * w * np.sin(3 * w * x)\n\n    # Generate a single noise sequence for both optimizers for a fair comparison\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(loc=0.0, scale=sig, size=T)\n\n    # --- Run Adam Optimizer ---\n    x_adam_traj = np.zeros(T + 1)\n    x_adam_traj[0] = x0\n    m_adam, v_adam = 0.0, 0.0\n    for t in range(1, T + 1):\n        # Current parameter value is from the previous step\n        current_x = x_adam_traj[t-1]\n        \n        # Calculate stochastic gradient\n        g = grad_f(current_x) + noise[t-1]\n        \n        # Update moments\n        m_adam = BETA1 * m_adam + (1 - BETA1) * g\n        v_adam = BETA2 * v_adam + (1 - BETA2) * (g**2)\n        \n        # Bias correction\n        m_hat = m_adam / (1 - BETA1**t)\n        v_hat = v_adam / (1 - BETA2**t)\n        \n        # Update parameter\n        x_adam_traj[t] = current_x - alpha * m_hat / (np.sqrt(v_hat) + EPSILON)\n\n    # --- Run AdaBelief Optimizer ---\n    x_adabelief_traj = np.zeros(T + 1)\n    x_adabelief_traj[0] = x0\n    m_adabelief, v_adabelief = 0.0, 0.0\n    for t in range(1, T + 1):\n        current_x = x_adabelief_traj[t-1]\n        \n        g = grad_f(current_x) + noise[t-1]\n        \n        m_adabelief = BETA1 * m_adabelief + (1 - BETA1) * g\n        # The key difference: second moment uses (g - m)^2\n        v_adabelief = BETA2 * v_adabelief + (1 - BETA2) * ((g - m_adabelief)**2)\n        \n        m_hat = m_adabelief / (1 - BETA1**t)\n        v_hat = v_adabelief / (1 - BETA2**t)\n        \n        x_adabelief_traj[t] = current_x - alpha * m_hat / (np.sqrt(v_hat) + EPSILON)\n\n    # --- Calculate Metrics for Adam ---\n    f_T_adam = f(x_adam_traj[T])\n    delta_sq_avg_adam = np.mean((x_adam_traj[1:] - x_adam_traj[:-1])**2)\n    osc_count_adam = np.sum((x_adam_traj[:-1] * x_adam_traj[1:])  0)\n\n    # --- Calculate Metrics for AdaBelief ---\n    f_T_adabelief = f(x_adabelief_traj[T])\n    delta_sq_avg_adabelief = np.mean((x_adabelief_traj[1:] - x_adabelief_traj[:-1])**2)\n    osc_count_adabelief = np.sum((x_adabelief_traj[:-1] * x_adabelief_traj[1:])  0)\n    \n    # --- Compare metrics based on stability criteria ---\n    # 1. Avg squared update magnitude is strictly smaller (with tolerance)\n    cond1 = delta_sq_avg_adabelief  delta_sq_avg_adam - 1e-12\n    # 2. Oscillation count is less than or equal\n    cond2 = osc_count_adabelief = osc_count_adam\n    # 3. Final objective is not worse by more than a tolerance\n    cond3 = f_T_adabelief = f_T_adam + 1e-3\n\n    return cond1 and cond2 and cond3\n\ndef solve():\n    # Define the test cases as tuples of:\n    # (omega, s, sigma, alpha, T, seed, x0)\n    test_cases = [\n        # Case 1\n        (25.0, 0.4, 0.1, 0.01, 300, 42, 2.0),\n        # Case 2\n        (60.0, 0.6, 0.3, 0.005, 500, 123, -3.0),\n        # Case 3\n        (25.0, 0.4, 0.0, 0.005, 300, 7, 1.0),\n        # Case 4\n        (25.0, 0.4, 0.5, 0.01, 500, 99, 2.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        omega, s, sigma, alpha, T, seed, x0 = case\n        is_more_stable = run_optimization_and_compare(omega, s, sigma, alpha, T, seed, x0)\n        results.append(str(is_more_stable).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Often in machine learning, we need our model parameters to satisfy certain constraints, for instance, to ensure stability or to act as a form of regularization. This practice introduces Projected Gradient Descent (PGD), a fundamental algorithm for handling such constrained optimization problems. You will apply PGD to train a simple neural network while enforcing that its weights remain within a bounded region. This hands-on exercise demonstrates how the projection step can prevent parameter values from exploding and helps stabilize the training process on a non-convex loss surface, a common challenge in deep learning .",
            "id": "3154367",
            "problem": "Consider empirical risk minimization with a single-hidden-unit Rectified Linear Unit (ReLU) neural network under an $\\ell_{\\infty}$-norm constraint on the parameter vector. Let the model be defined as follows. The input is a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\mathbb{R}$. The network output is $f(x;\\theta) = w_2 \\cdot \\max\\{0, w_1^\\top x + b_1\\} + b_2$, where $\\theta = (w_1, b_1, w_2, b_2) \\in \\mathbb{R}^5$, $w_1 \\in \\mathbb{R}^2$, $b_1 \\in \\mathbb{R}$, $w_2 \\in \\mathbb{R}$, and $b_2 \\in \\mathbb{R}$. The empirical loss is the mean squared error $L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\left(f(x_i;\\theta) - y_i\\right)^2$. The feasible set is the weight space $\\{\\theta \\in \\mathbb{R}^5 : \\|\\theta\\|_\\infty \\leq c\\}$ for a given constant $c  0$.\n\nYour task is to implement two optimization procedures starting from the same initialization: unconstrained gradient descent and constrained projected gradient descent onto the set $\\{\\theta : \\|\\theta\\|_\\infty \\leq c\\}$. The projection is defined as the Euclidean projection onto the feasible set. You must:\n\n1. Derive and implement the gradient of $L(\\theta)$ with respect to $\\theta$ using the chain rule and standard calculus for the ReLU nonlinearity. At points where $w_1^\\top x_i + b_1 = 0$, use any valid subgradient consistent with the ReLU; in code, take the derivative indicator as $1$ when $w_1^\\top x_i + b_1  0$ and $0$ otherwise.\n2. Implement unconstrained gradient descent: $\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$ for $t = 0,1,\\dots,T-1$, where $\\eta  0$ is the step size and $T$ is the number of iterations.\n3. Implement projected gradient descent (PGD): $\\theta_{t+1} = \\Pi(\\theta_t - \\eta \\nabla L(\\theta_t))$ for $t = 0,1,\\dots,T-1$, where $\\Pi$ is the Euclidean projection onto $\\{\\theta : \\|\\theta\\|_\\infty \\leq c\\}$. Ensure the trajectory is feasible at all iterations. If the initialization is infeasible, enforce feasibility before the first step by applying the projection to the initial $\\theta$.\n4. Quantify the stabilization effect of projection for a nonconvex loss $L(\\theta)$ by collecting the following metrics for each optimization procedure:\n   - The final loss $L(\\theta_T)$.\n   - The maximum gradient norm over the trajectory, $\\max_{0 \\leq t  T} \\|\\nabla L(\\theta_t)\\|_2$.\n   - For unconstrained descent, the maximum absolute parameter magnitude encountered, $\\max_{0 \\leq t \\leq T} \\|\\theta_t\\|_\\infty$.\n   - For PGD, the number of times the projection actively clipped at least one coordinate, i.e., the count of iterations $t$ for which the pre-projection update $(\\theta_t - \\eta \\nabla L(\\theta_t))$ had a coordinate with magnitude strictly greater than $c$.\n5. Define a boolean indicator of stabilization that returns true when projection reduces the maximum gradient norm by at least a relative margin $\\delta = 0.1$, i.e., when $\\frac{\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{unconstrained}} - \\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{projected}}}{\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{unconstrained}}} \\geq 0.1$, or when the unconstrained trajectory violates the constraint, i.e., $\\max_t \\|\\theta_t\\|_\\infty^{\\text{unconstrained}}  c$. If the denominator is zero, treat the relative margin as zero.\n\nUse the following deterministic dataset of size $n = 50$:\n- For $i = 1,2,\\dots,50$, define $x_i \\in \\mathbb{R}^2$ by $x_{i,1} = i/50$ and $x_{i,2} = 0.5 \\cdot (-1)^i$.\n- Define $y_i \\in \\mathbb{R}$ by $y_i = 1$ if $x_{i,1} + x_{i,2}  0.6$ and $y_i = 0$ otherwise.\n\nRun the two optimization procedures for each of the following four test cases. Each test case specifies the constraint level $c$, step size $\\eta$, number of iterations $T$, and initial parameter vector $\\theta_0 = (w_{1,1}, w_{1,2}, b_1, w_2, b_2)$:\n\n- Test case $1$: $c = 0.5$, $\\eta = 0.5$, $T = 100$, $\\theta_0 = (1.0, -1.0, 0.0, 2.0, 0.0)$.\n- Test case $2$: $c = 10.0$, $\\eta = 0.05$, $T = 300$, $\\theta_0 = (0.1, 0.1, 0.1, 0.1, 0.1)$.\n- Test case $3$: $c = 1.0$, $\\eta = 0.3$, $T = 200$, $\\theta_0 = (2.0, -2.0, 1.0, 8.0, -0.5)$.\n- Test case $4$: $c = 1.0$, $\\eta = 0.4$, $T = 150$, $\\theta_0 = (1.0, -0.2, 0.3, -1.0, 0.0)$.\n\nFor each test case, produce a list of seven values:\n- $L(\\theta_T)^{\\text{projected}}$ rounded to $6$ decimal places.\n- $L(\\theta_T)^{\\text{unconstrained}}$ rounded to $6$ decimal places.\n- $\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{projected}}$ rounded to $6$ decimal places.\n- $\\max_t \\|\\nabla L(\\theta_t)\\|_2^{\\text{unconstrained}}$ rounded to $6$ decimal places.\n- $\\max_t \\|\\theta_t\\|_\\infty^{\\text{unconstrained}}$ rounded to $6$ decimal places.\n- The integer count of projection activations in PGD.\n- The stabilization boolean as defined above.\n\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list of lists with no spaces, enclosed in square brackets. For example, the output format must be of the form $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$, where each inner list contains the seven values for one test case in the order specified above. No physical units apply. Angles do not appear. Express relative margins as decimals, not using percentages.",
            "solution": "The problem requires the implementation and comparison of unconstrained gradient descent and projected gradient descent for training a single-hidden-unit ReLU neural network. The analysis focuses on the stabilizing effect of projecting the network's parameters onto an $\\ell_{\\infty}$-norm ball.\n\nFirst, we formalize the model and the gradient of the loss function. The dataset is $\\{(x_i, y_i)\\}_{i=1}^n$, with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\mathbb{R}$. The parameter vector is $\\theta = (w_{1,1}, w_{1,2}, b_1, w_2, b_2) \\in \\mathbb{R}^5$, which can be grouped as $\\theta = (w_1, b_1, w_2, b_2)$ where $w_1 \\in \\mathbb{R}^2$.\n\nThe network's prediction for an input $x$ is given by:\n$$ f(x;\\theta) = w_2 \\cdot \\max\\{0, w_1^\\top x + b_1\\} + b_2 $$\nThe empirical loss is the Mean Squared Error (MSE):\n$$ L(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left(f(x_i;\\theta) - y_i\\right)^2 $$\n\nTo implement gradient-based optimization, we must compute the gradient of the loss, $\\nabla L(\\theta)$. This is achieved by applying the chain rule. Let $z_i = w_1^\\top x_i + b_1$ be the pre-activation of the hidden unit, and $a_i = \\max\\{0, z_i\\}$ be its activation (the ReLU function). The prediction for sample $i$ is $f_i = w_2 a_i + b_2$.\n\nThe derivative of the loss with respect to a parameter $p \\in \\theta$ is:\n$$ \\frac{\\partial L}{\\partial p} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial p} (f_i - y_i)^2 = \\frac{1}{n} \\sum_{i=1}^n 2(f_i - y_i) \\frac{\\partial f_i}{\\partial p} $$\nLet's define the common error term $\\delta_i = 2(f_i - y_i)$. The partial derivatives for each parameter are:\n\n1.  **With respect to $b_2$**: $\\frac{\\partial f_i}{\\partial b_2} = 1$.\n    $$ \\frac{\\partial L}{\\partial b_2} = \\frac{1}{n} \\sum_{i=1}^n \\delta_i $$\n2.  **With respect to $w_2$**: $\\frac{\\partial f_i}{\\partial w_2} = a_i$.\n    $$ \\frac{\\partial L}{\\partial w_2} = \\frac{1}{n} \\sum_{i=1}^n \\delta_i a_i $$\n3.  **With respect to $b_1$**: $\\frac{\\partial f_i}{\\partial b_1} = \\frac{\\partial f_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial b_1} = w_2 \\cdot I(z_i  0) \\cdot 1$, where $I(\\cdot)$ is the indicator function. The subgradient for the ReLU derivative at $z_i=0$ is chosen to be $0$, consistent with the problem's specification $I(z_i  0)$.\n    $$ \\frac{\\partial L}{\\partial b_1} = \\frac{1}{n} \\sum_{i=1}^n \\delta_i w_2 I(w_1^\\top x_i + b_1  0) $$\n4.  **With respect to $w_1$**: $\\frac{\\partial f_i}{\\partial w_1} = \\frac{\\partial f_i}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_1} = w_2 \\cdot I(z_i  0) \\cdot x_i$.\n    $$ \\nabla_{w_1} L = \\frac{1}{n} \\sum_{i=1}^n \\delta_i w_2 I(w_1^\\top x_i + b_1  0) x_i $$\n\nThe full gradient $\\nabla L(\\theta) \\in \\mathbb{R}^5$ is assembled from these partial derivatives.\n\nThe two optimization procedures are:\n1.  **Unconstrained Gradient Descent**: An iterative method that updates the parameters in the direction opposite to the gradient.\n    $$ \\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t) $$\n    where $\\theta_t$ is the parameter vector at iteration $t$, $\\eta  0$ is the step size, and $T$ is the total number of iterations.\n2.  **Projected Gradient Descent (PGD)**: This method first performs a standard gradient descent step and then projects the resulting parameters back into the feasible set. The feasible set is the $\\ell_{\\infty}$-norm ball of radius $c$, defined as $S = \\{\\theta \\in \\mathbb{R}^5 : \\|\\theta\\|_\\infty \\leq c\\}$.\n    $$ \\theta_{t+1} = \\Pi_S(\\theta_t - \\eta \\nabla L(\\theta_t)) $$\n    The projection operator $\\Pi_S$ for an $\\ell_{\\infty}$-ball is a component-wise clipping function:\n    $$ (\\Pi_S(\\theta))_j = \\text{clip}(\\theta_j, -c, c) = \\max(-c, \\min(\\theta_j, c)) $$\n    If the initial parameter vector $\\theta_0$ is outside the feasible set $S$, it is projected onto $S$ before the first iteration.\n\nThe stabilization effect of the projection is evaluated using several metrics. We track the final loss $L(\\theta_T)$, the maximum gradient norm $\\|\\nabla L(\\theta_t)\\|_2$ over the trajectory, and the maximum parameter magnitude $\\|\\theta_t\\|_\\infty$. For PGD, we count the number of iterations where the projection was active (i.e., at least one parameter was clipped). A stabilization indicator boolean is computed based on whether the unconstrained trajectory violates the weight constraint or if the maximum gradient norm is reduced by a relative margin of at least $\\delta=0.1$ in the projected case.\n\nThe implementation proceeds by first generating the deterministic dataset. Then, for each test case, both optimization algorithms are run for $T$ iterations starting from $\\theta_0$. All required metrics are collected during these runs. Finally, the results are aggregated and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares unconstrained and projected gradient descent\n    for a single-hidden-unit ReLU network.\n    \"\"\"\n\n    # Helper function to generate the deterministic dataset\n    def generate_data():\n        n = 50\n        i_vals = np.arange(1, n + 1)\n        x1 = i_vals / n\n        x2 = 0.5 * (-1)**i_vals\n        X = np.stack([x1, x2], axis=1).astype(np.float64)\n        Y = (x1 + x2  0.6).astype(np.float64)\n        return X, Y, n\n\n    # Model-related functions\n    def forward(theta, X):\n        w1, b1, w2, b2 = theta[0:2], theta[2], theta[3], theta[4]\n        z = X @ w1 + b1\n        a = np.maximum(0, z)\n        return w2 * a + b2\n\n    def loss(theta, X, Y):\n        f_pred = forward(theta, X)\n        return np.mean((f_pred - Y)**2)\n\n    def gradient(theta, X, Y, n):\n        w1, b1, w2, b2 = theta[0:2], theta[2], theta[3], theta[4]\n        \n        # Forward pass to get intermediate values\n        z = X @ w1 + b1\n        a = np.maximum(0, z)\n        f_pred = w2 * a + b2\n        \n        # Backward pass (chain rule)\n        d_loss_d_f = 2 * (f_pred - Y) / n\n        \n        grad_b2 = np.sum(d_loss_d_f)\n        grad_w2 = np.sum(d_loss_d_f * a)\n        \n        d_loss_d_pre_act = d_loss_d_f * w2 * (z  0).astype(np.float64)\n        \n        grad_b1 = np.sum(d_loss_d_pre_act)\n        grad_w1 = d_loss_d_pre_act[:, np.newaxis] * X\n        grad_w1 = np.sum(grad_w1, axis=0)\n        \n        return np.array([grad_w1[0], grad_w1[1], grad_b1, grad_w2, grad_b2])\n\n    def project(theta, c):\n        return np.clip(theta, -c, c)\n\n    # Main function to run optimization for a single test case\n    def run_optimization_for_case(case_params, X, Y, n):\n        c, eta, T, theta0_tuple = case_params\n        theta0 = np.array(theta0_tuple, dtype=np.float64)\n        delta = 0.1\n\n        # --- Unconstrained Gradient Descent ---\n        theta_u = theta0.copy()\n        u_grad_norms = []\n        u_param_norms = [np.linalg.norm(theta_u, ord=np.inf)]\n        for _ in range(T):\n            grad_u = gradient(theta_u, X, Y, n)\n            u_grad_norms.append(np.linalg.norm(grad_u, ord=2))\n            theta_u -= eta * grad_u\n            u_param_norms.append(np.linalg.norm(theta_u, ord=np.inf))\n        \n        final_loss_u = loss(theta_u, X, Y)\n        max_grad_norm_u = np.max(u_grad_norms) if u_grad_norms else 0.0\n        max_param_norm_u = np.max(u_param_norms) if u_param_norms else 0.0\n\n        # --- Projected Gradient Descent ---\n        theta_p = theta0.copy()\n        # Enforce feasibility at initialization\n        if np.linalg.norm(theta_p, ord=np.inf)  c:\n            theta_p = project(theta_p, c)\n        \n        p_grad_norms = []\n        projection_activations = 0\n        for _ in range(T):\n            grad_p = gradient(theta_p, X, Y, n)\n            p_grad_norms.append(np.linalg.norm(grad_p, ord=2))\n            theta_temp = theta_p - eta * grad_p\n            if np.linalg.norm(theta_temp, ord=np.inf)  c:\n                projection_activations += 1\n            theta_p = project(theta_temp, c)\n            \n        final_loss_p = loss(theta_p, X, Y)\n        max_grad_norm_p = np.max(p_grad_norms) if p_grad_norms else 0.0\n\n        # --- Stabilization Check ---\n        stabilized = False\n        if max_param_norm_u  c:\n            stabilized = True\n        else:\n            # Use a small epsilon to avoid division by zero for stable gradients\n            if max_grad_norm_u  1e-12:\n                rel_margin = (max_grad_norm_u - max_grad_norm_p) / max_grad_norm_u\n                if rel_margin = delta:\n                    stabilized = True\n        \n        # Round numerical results to 6 decimal places\n        # We assume parameters are chosen such that results are finite.\n        return [\n            round(final_loss_p, 6),\n            round(final_loss_u, 6),\n            round(max_grad_norm_p, 6),\n            round(max_grad_norm_u, 6),\n            round(max_param_norm_u, 6),\n            projection_activations,\n            stabilized\n        ]\n\n    # --- Main Execution Logic ---\n    X, Y, n = generate_data()\n    test_cases = [\n        (0.5, 0.5, 100, (1.0, -1.0, 0.0, 2.0, 0.0)),\n        (10.0, 0.05, 300, (0.1, 0.1, 0.1, 0.1, 0.1)),\n        (1.0, 0.3, 200, (2.0, -2.0, 1.0, 8.0, -0.5)),\n        (1.0, 0.4, 150, (1.0, -0.2, 0.3, -1.0, 0.0)),\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        all_results.append(run_optimization_for_case(case, X, Y, n))\n    \n    # Format the final output string as a list of lists with no spaces\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}