## 引言
正则化是现代[统计学习](@entry_id:269475)与机器学习中不可或缺的核心概念，它通过在模型学习过程中引入约束或惩罚，有效解决了过拟合问题，并提升了模型的泛化能力。然而，机器学习的应用领域日益广泛和复杂，从高维基因组学数据到具有[复杂网络](@entry_id:261695)结构的社交数据，标准而单一的 $L_1$（Lasso）和 $L_2$（[岭回归](@entry_id:140984)）正则化已不足以捕捉数据中蕴含的丰富先验知识和结构信息。当前面临的挑战是如何设计和运用更精巧的正则化策略，以构建出更准确、更可解释、且更符合特定问题背景的模型。

本文旨在系统性地介绍一系列超越传统[范式](@entry_id:161181)的高级正则化策略。通过深入剖析这些方法的理论基础与应用场景，读者将掌握如何将复杂的结构化先验、鲁棒性要求乃至社会公平性考量融入学习框架中。
- 在“原理与机制”一章，我们将揭示多种高级[正则化技术](@entry_id:261393)背后的数学原理，包括通过数据扰动（如Dropout）实现的[隐式正则化](@entry_id:187599)、贝叶斯框架下的自适应惩罚（如[自动相关性确定](@entry_id:746592)），以及能够捕捉数据内在几何与分组结构的特殊惩[罚函数](@entry_id:638029)。
- 随后的“应用与跨学科联系”一章将展示这些策略如何在[基因组学](@entry_id:138123)、空间统计、网络科学、物理反问题等前沿领域中发挥关键作用，从而连接理论与实践。
- 最后，在“动手实践”部分，我们提供了一系列精心设计的编程练习，引导读者亲手实现和探索这些高级[正则化方法](@entry_id:150559)，加深对其工作机制和实际效果的理解。

让我们从探索这些精妙策略的内在原理开始，踏上构建更强大、更智能的[机器学习模型](@entry_id:262335)之旅。

## 原理与机制

继前一章对正则化基本概念的介绍之后，本章将深入探讨几种高级正则化策略的原理与机制。我们将超越标准的 $L_1$ 和 $L_2$ 惩罚，探索一系列旨在将更复杂的先验知识编码到学习算法中的技术。这些策略包括通过数据扰动进行[隐式正则化](@entry_id:187599)、利用贝叶斯框架进行自适应惩罚、以及设计能够捕捉数据内在结构（如[流形](@entry_id:153038)或特征分组）的特定惩罚函数。本章的目标是提供一个系统性的视角，揭示这些高级技术背后的数学原理，并阐明它们在实践中的应用与考量。

### 通过扰动实现正则化

正则化最直观的理解之一是将其视为一种向学习过程中引入随机性的机制。通过在训练数据或模型参数中注入噪声，我们可以有效地防止模型对训练样本的过度拟合。从数学上可以证明，这种随机扰动过程在期望意义上等价于向原始[损失函数](@entry_id:634569)中添加一个显式的正则化项。

#### 输入与参数噪声注入

让我们从一个简单的[线性回归](@entry_id:142318)模型开始，其预测函数为 $f_{\mathbf{w}}(\mathbf{x}) = \mathbf{w}^{\top}\mathbf{x}$。给定一个包含 $n$ 个样本的数据集 $\{(\mathbf{x}_{i}, y_{i})\}_{i=1}^{n}$，其经验平方[损失函数](@entry_id:634569)定义为：
$$
J(\mathbf{w}) = \frac{1}{n}\sum_{i=1}^{n}\big(y_{i} - \mathbf{w}^{\top}\mathbf{x}_{i}\big)^{2}
$$

一种正则化策略是在模型的输入端注入噪声。假设我们在每个训练样本 $\mathbf{x}_{i}$ 上添加一个[独立同分布](@entry_id:169067)的高斯噪声 $\boldsymbol{\epsilon}_{i} \sim \mathcal{N}(\mathbf{0}, \sigma_{x}^{2}\mathbf{I}_{d})$，从而得到带噪声的输入 $\tilde{\mathbf{x}}_{i} = \mathbf{x}_{i} + \boldsymbol{\epsilon}_{i}$。如果我们使用这些带噪声的数据进行训练，那么优化的目标就变成了带噪声的[损失函数](@entry_id:634569) $J_{\text{in}}(\mathbf{w}) = \frac{1}{n}\sum_{i=1}^{n}\big(y_{i} - \mathbf{w}^{\top}\tilde{\mathbf{x}}_{i}\big)^{2}$。为了理解这种策略的平均效果，我们可以计算该损失函数在噪声[分布](@entry_id:182848)上的[期望值](@entry_id:153208) $\mathbb{E}_{\boldsymbol{\epsilon}}[J_{\text{in}}(\mathbf{w})]$。

通过展开并利用[高斯噪声](@entry_id:260752)的性质（$\mathbb{E}[\boldsymbol{\epsilon}_{i}] = \mathbf{0}$ 和 $\text{Cov}(\boldsymbol{\epsilon}_{i}) = \sigma_{x}^{2}\mathbf{I}_{d}$），可以证明：
$$
\mathbb{E}_{\boldsymbol{\epsilon}}[J_{\text{in}}(\mathbf{w})] = J(\mathbf{w}) + \sigma_{x}^{2}\|\mathbf{w}\|^{2}
$$
这个结果揭示了一个深刻的联系：**在输入中注入均值为零、[方差](@entry_id:200758)为 $\sigma_{x}^{2}$ 的高斯噪声，在期望上等价于对原始损失函数添加一个 $L_2$ 正则化项，其正则化系数恰好是噪声的[方差](@entry_id:200758) $\sigma_{x}^{2}$** 。因此，通过调整注入噪声的强度，我们能够直接控制正则化的强度。

与此相对，我们也可以考虑在模型参数端注入噪声。例如，使用带噪声的权重 $\tilde{\mathbf{w}} = \mathbf{w} + \mathbf{z}$ 进行预测，其中 $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \sigma_{w}^{2}\mathbf{I}_{d})$。在这种情况下，对[损失函数](@entry_id:634569)取期望会得到一个不同的结果：
$$
\mathbb{E}_{\mathbf{z}}[J_{\text{param}}(\mathbf{w})] = J(\mathbf{w}) + \frac{\sigma_{w}^{2}}{n}\sum_{i=1}^{n}\|\mathbf{x}_{i}\|^{2}
$$
有趣的是，这里的附加项是一个与权重 $\mathbf{w}$ 无关的常数（只与数据集和噪声[方差](@entry_id:200758)有关）。因此，它不会在优化过程中对 $\mathbf{w}$ 产生正则化效应 。这两种噪声注入方式的对比，清晰地表明了扰动在模型中的作用位置对最终产生的正则化效果至关重要。

#### Dropout：[模型平均](@entry_id:635177)与正则化

**Dropout** 是一种更为复杂和强大的扰动技术，尤其在[深度学习](@entry_id:142022)领域取得了巨大成功。其核心思想是在训练过程的每一步中，以一定的概率随机地“丢弃”（即置为零）网络中的一部分神经元（或特征）。为了在一个更简单的线性模型中理解其机制，我们可以考虑 **特征丢弃 (feature dropout)**。

在训练每个样本 $\mathbf{x}_i$ 时，我们为其每个特征 $x_{ij}$ 独立地生成一个伯努利[随机变量](@entry_id:195330)掩码 $r_{ij} \in \{0,1\}$，其取值为1的概率为 $q$（称为“保留概率”）。然后，使用经过掩码和缩放的特征 $\tilde{x}_{ij} = \frac{r_{ij}}{q} x_{ij}$ 进行训练。这种被称为 **反向缩放 (inverted scaling)** 的技术，其精妙之处在于保证了带噪特征在期望上等于原始特征，即 $\mathbb{E}[\tilde{x}_{ij}] = x_{ij}$。

从概念上看，特征丢弃可以将训练过程视为对一个巨大模型集合的 **[集成学习](@entry_id:637726) (ensemble learning)**。每一次随机生成的掩码都定义了一个仅使用部分特征的“子模型”。整个训练过程则是在这些指数级数量的子模型上进行[参数共享](@entry_id:634285)和平均。在测试时，我们使用完整的[特征向量](@entry_id:151813)，而无需进行任何[掩码操作](@entry_id:751694)。由于反向缩放的巧妙设计，可以证明，在测试时使用完整权重得到的预测值，恰好是所有可能子模型预测值的期望（或平均）。对于线性模型 $f(\mathbf{x}; \mathbf{w}, b) = \mathbf{x}^\top\mathbf{w} + b$，对权重应用dropout（即 $\hat{y}_{\mathbf{m}}(\mathbf{x}) = \mathbf{x}^\top(\mathbf{m} \odot \mathbf{w}) + b$），在测试时使用缩放后的权重 $p\mathbf{w}$ 进行预测，得到的 $\hat{y}_{\mathrm{scale}}(\mathbf{x}) = \mathbf{x}^\top(p\mathbf{w}) + b$ 并不是一个近似，而是精确地等于所有[子模](@entry_id:148922)型预测的[期望值](@entry_id:153208) $\mathbb{E}_{\mathbf{m}}[\hat{y}_{\mathbf{m}}(\mathbf{x})]$ 。

与输入噪声注入类似，特征丢弃的训练过程也可以等效为一个显式的正则化项。对于线性回归模型，最小化带特征丢弃的平方损失的期望，等价于最小化如下目标函数：
$$
\mathcal{L}(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i^\top \mathbf{w})^2 + \frac{1-q}{q} \sum_{j=1}^d \left( \frac{1}{n} \sum_{i=1}^n x_{ij}^2 \right) w_j^2
$$
这个表达式清晰地表明，特征丢弃在[线性模型](@entry_id:178302)中引入了一个加权的 $L_2$ 惩罚。每个权重 $w_j$ 的惩罚系数不仅与保留概率 $q$ 有关，还与其对应特征的经验二阶矩（或[方差](@entry_id:200758)，如果特征已中心化）成正比。这是一种自适应的正则化形式：[方差](@entry_id:200758)较大的特征会受到更强的惩罚 。

当保留概率 $q \to 1$ 时，正则化系数 $\frac{1-q}{q} \to 0$，模型退化为标准的[最小二乘法](@entry_id:137100)。反之，当 $q \to 0$ 时，正则化系数趋于无穷，迫使所有权重趋向于零。这与我们对正则化强度的直观理解相符 。

### 正则化的贝叶斯视角

正则化的另一个深刻理解来自于[概率建模](@entry_id:168598)和[贝叶斯推断](@entry_id:146958)。在此框架下，正则化项可以被解释为模型参数的 **先验分布 (prior distribution)** 的负对数。通过为参数设定一个[先验分布](@entry_id:141376)，我们将关于参数可能取值的信念融入到模型中。**最大后验估计 (Maximum A Posteriori, MAP)** 寻找的是在给定数据下具有[最大后验概率](@entry_id:268939)的参数值，这自然地导出了一个包含损失项（似然）和正则化项（先验）的[目标函数](@entry_id:267263)。

例如，众所周知，对参数赋予[高斯先验](@entry_id:749752)[分布](@entry_id:182848)等价于 $L_2$ 正则化，而赋予拉普拉斯先验分布则等价于 $L_1$ 正则化。接下来，我们将探讨一种更复杂的层次化贝叶斯模型——**[自动相关性确定](@entry_id:746592) (Automatic Relevance Determination, ARD)**。

#### [自动相关性确定 (ARD)](@entry_id:746593)

在线性回归模型 $y = X w + \varepsilon$ 中，ARD 为每个权重系数 $w_j$ 引入一个独立的精度超参数 $\alpha_j$，并建立如下的层次化贝叶斯模型：
1.  **似然 (Likelihood)**: $p(y \mid w, \sigma^2) = \mathcal{N}(y \mid Xw, \sigma^2 I_n)$
2.  **参数先验 (Prior)**: $p(w \mid \boldsymbol{\alpha}) = \prod_{j=1}^p \mathcal{N}(w_j \mid 0, \alpha_j^{-1})$
3.  **[超先验](@entry_id:750480) (Hyperprior)**: $p(\boldsymbol{\alpha}) = \prod_{j=1}^p \text{Gamma}(\alpha_j \mid a_0, b_0)$

这里的关键思想是，每个权重 $w_j$ 都有其自身的[方差](@entry_id:200758) $\alpha_j^{-1}$，而这些[方差](@entry_id:200758)（或精度 $\alpha_j$）本身又是从一个共同的Gamma[分布](@entry_id:182848)中抽取的。通过在[贝叶斯推断](@entry_id:146958)中将这些超参数 $\alpha_j$ 边缘化（积分掉），我们可以得到 $w_j$ 的边缘[先验分布](@entry_id:141376) $p(w_j)$。这个边缘先验是一个 **学生t分布 ([Student's t-distribution](@entry_id:142096))**，其概率密度函数在零点处有一个尖峰，并且具有比[高斯分布](@entry_id:154414)更重的尾部。这种形状的[先验分布](@entry_id:141376)会强烈地将不重要的权重（即数据支持度不高的权重）推向零，从而实现 **稀疏性 (sparsity)** 。

为了求解ARD的[MAP估计](@entry_id:751667)，我们需要最小化负对数后验。直接对边缘化的后验进行优化是困难的，但我们可以采用一种迭代算法，例如 **[期望最大化](@entry_id:273892) (Expectation-Maximization, EM)** 算法，将精度 $\alpha_j$ 视为[隐变量](@entry_id:150146)。这最终导出了一个称为 **迭代重加权最小二乘 (Iteratively Reweighted Least Squares, IRLS)** 的优美算法 ：

1.  **E-步 (E-step)**: 在给定当前权重估计 $w^{(t-1)}$ 的情况下，计算每个精度超参数的[期望值](@entry_id:153208)。这个[期望值](@entry_id:153208)为：
    $$
    d_j^{(t-1)} \equiv \mathbb{E}[\alpha_j \mid w_j^{(t-1)}] = \frac{2a_0 + 1}{2b_0 + (w_j^{(t-1)})^2}
    $$
    这个值可以看作是第 $j$ 个特征的“相关性”或“重要性”的度量。如果 $w_j^{(t-1)}$ 很小，则 $d_j^{(t-1)}$ 很大，反之亦然。

2.  **M-步 (M-step)**: 更新权重 $w$，通过求解一个加权的[岭回归](@entry_id:140984)问题：
    $$
    w^{(t)} = \arg\min_w \left( \frac{1}{2\sigma^2} \|y - Xw\|_2^2 + \frac{1}{2} \sum_{j=1}^p d_j^{(t-1)} w_j^2 \right)
    $$
    其[闭式](@entry_id:271343)解为：
    $$
    w^{(t)} = (X^T X + \sigma^2 D^{(t-1)})^{-1} X^T y
    $$
    其中 $D^{(t-1)}$ 是一个[对角矩阵](@entry_id:637782)，对角[线元](@entry_id:196833)素为 $d_j^{(t-1)}$。

ARD通过这种迭代方式，为每个系数动态地调整其正则化强度。对于那些在拟[合数](@entry_id:263553)据时作用不大的系数，其值会变小，从而导致其对应的权重 $d_j$ 增大，在下一次迭代中对其施加更强的惩罚，进一步将其推向零。这种自适应机制使得ARD能够自动“确定”哪些特征是相关的，并将不相关的特征权重剪枝掉，从而成为一种强大的稀疏学习方法。

### 高级惩罚函数与结构

除了标准的范数惩罚外，我们还可以设计更复杂的惩罚项，以将关于解的期望结构（例如平滑性、分组性）直接编码到[优化问题](@entry_id:266749)中。

#### [标签平滑](@entry_id:635060)：一种[熵正则化](@entry_id:749012)

在[分类问题](@entry_id:637153)中，模型有时会变得 **过分自信 (overconfident)**，即对某些样本预测出接近0或1的概率。这种过分自信可能导致[模型泛化](@entry_id:174365)能力下降，并且对[标签噪声](@entry_id:636605)变得敏感。**[标签平滑](@entry_id:635060) (Label Smoothing)** 是一种简单而有效的技术，用于缓解此问题。

对于一个二[分类问题](@entry_id:637153)，[标签平滑](@entry_id:635060)将硬标签 $y \in \{0, 1\}$ 替换为一个软标签 $y^{(\mathrm{ls})} = (1 - \alpha)y + \alpha \cdot \frac{1}{2}$，其中 $\alpha \in [0, 1)$ 是一个小的平滑参数。例如，当 $\alpha=0.1$ 时，标签1变为0.95，标签0变为0.05。

虽然这看起来像是简单地“污染”了标签，但其背后有更深刻的正则化解释。在逻辑回归中，标准的训练目标是最小化模型[预测分布](@entry_id:165741) $p$ 与独热（one-hot）标签[分布](@entry_id:182848) $q$ 之间的 **[交叉熵](@entry_id:269529) (cross-entropy)**。[标签平滑](@entry_id:635060)可以被看作是最小化模型[预测分布](@entry_id:165741) $p$ 与一个平滑后的[目标分布](@entry_id:634522) $q'=(1-\alpha)q + \alpha u$ 之间的[交叉熵](@entry_id:269529)，其中 $u$ 是类别上的[均匀分布](@entry_id:194597)。

将使用平滑标签的[交叉熵损失](@entry_id:141524)函数展开，我们可以发现它等价于 ：
$$
\mathcal{L}_{\mathrm{LS}} = (1-\alpha) \cdot H(q, p) - \alpha \cdot \mathbb{E}_{k \sim u}[\log p(k)]
$$
其中 $H(q, p)$ 是原始的[交叉熵损失](@entry_id:141524)。第二项 $-\mathbb{E}_{k \sim u}[\log p(k)]$ 正好是模型[预测分布](@entry_id:165741) $p$ 的[负熵](@entry_id:194102)（乘以一个常数）。因此，最小化[标签平滑](@entry_id:635060)损失不仅是在最小化原始的[交叉熵](@entry_id:269529)，同时也是在 **最大化模型预测的熵**。熵最大的[概率分布](@entry_id:146404)是最不确定的[分布](@entry_id:182848)（即[均匀分布](@entry_id:194597)）。因此，[标签平滑](@entry_id:635060)作为一种 **[熵正则化](@entry_id:749012)**，它惩罚那些过于自信的预测，鼓励模型输出更“软”的概率，从而提高模型的泛化能力和校准性。

#### [流形正则化](@entry_id:637825)

在许多现实世界的应用中，特别是在 **[半监督学习](@entry_id:636420) (semi-supervised learning)** 场景下，我们拥有大量的未标记数据和少量的已标记数据。一个核心的假设是 **[流形假设](@entry_id:275135) (manifold assumption)**，即[高维数据](@entry_id:138874)点实际上[分布](@entry_id:182848)在一个或多个低维[流形](@entry_id:153038)上。基于此，我们期望一个好的预测函数 $f$ 在数据密集的区域（即沿着[流形](@entry_id:153038)）是平滑的。

**[流形正则化](@entry_id:637825) (Manifold Regularization)** 将这一思想形式化。首先，我们根据所有数据点（包括未标记的）构建一个图，其中节点代表数据点，边的权重 $W_{ij}$ 反映了数据点 $i$ 和 $j$ 之间的相似度。然后，我们可以定义图的 **[拉普拉斯矩阵](@entry_id:152110) (Graph Laplacian)** $L = D - W$，其中 $D$ 是度矩阵。二次型 $f^\top L f$ 可以度量函数 $f$ 在图上的平滑度：
$$
f^\top L f = \frac{1}{2} \sum_{i,j=1}^n W_{ij} (f_i - f_j)^2
$$
这个值越小，意味着函数 $f$ 在连接紧密的节点上的取值越相似，即函数越平滑。

[流形正则化](@entry_id:637825)的目标函数通常结合了三部分：在已标记数据上的拟合误差、一个[控制函数](@entry_id:183140)复杂度的环境正则化项（如 $L_2$ 范数），以及一个鼓励函数在[数据流形](@entry_id:636422)上平滑的内在正则化项（拉普拉斯惩罚）：
$$
J(f) = \sum_{i \in \mathcal{L}} (f_i - y_i)^2 + \gamma_{\mathrm{A}} \|f\|_2^2 + \gamma_{\mathrm{I}} f^\top L f
$$
其中 $\mathcal{L}$ 是已标记节点的集合。这是一个关于 $f$ 的二次函数，其最优解可以通过求解一个[线性方程组](@entry_id:148943)得到。这个框架优雅地将数据的几何结构融入到学习过程中，使得模型能够利用未标记数据来改善其泛化性能。

#### 排序[L1惩罚](@entry_id:144210) (SLOPE) 与特征分组

LASSO ($L_1$ 正则化) 以其产生[稀疏解](@entry_id:187463)的能力而闻名，但它在处理相关特征时存在一些不足，例如它倾向于在强相关特征组中随机选择一个而非全部。**排序[L1惩罚](@entry_id:144210)估计 (Sorted L-One Penalized Estimation, SLOPE)** 是LASSO的一种推广，它通过一种新颖的惩罚结构来改善这一行为，并能自适应地实现特征分组。

SLOPE的惩罚项定义为：
$$
J_{\text{SLOPE}}(\beta) = \sum_{j=1}^{p} \lambda_j |\beta|_{(j)}
$$
其中 $|\beta|_{(1)} \ge |\beta|_{(2)} \ge \cdots \ge |\beta|_{(p)}$ 是系数[绝对值](@entry_id:147688)的排序统计量，而 $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p \ge 0$ 是一组非递增的惩罚权重。与LASSO不同，SLOPE的惩罚不依赖于系数的原始索引，而是依赖于其在排序后的[绝对值](@entry_id:147688)中的秩。最大的系数[绝对值](@entry_id:147688)受到最大的惩罚 $\lambda_1$，第二大的受到次大的惩罚 $\lambda_2$，以此类推。

这种排序依赖的惩罚结构导致了系数之间的耦合，并产生了一个显著的 **分组效应 (grouping effect)**。当多个系数的真实效应大小相近时，SLOPE倾向于将它们的估计值“捆绑”在一起，使它们具有完全相同的[绝对值](@entry_id:147688)。

这个机制在正交设计的情况下尤为清晰。对于正交[设计矩阵](@entry_id:165826)（$X^\top X = I$），SLOPE的解可以通过对 $z = X^\top y$ 应用SLOPE惩罚的 **[近端算子](@entry_id:635396) (proximal operator)** 得到。这个计算过程涉及一个关键步骤：将一个初始收缩后的[向量投影](@entry_id:147046)到非负、非递增向量构成的锥上。这个投影通常通过 **池邻违规算法 (Pool Adjacent Violators Algorithm, PAVA)** 实现。当初始收缩后的向量序列不满足非递增约束时，PAVA会识别出“违规”的块，并用块内元素的平均值替换它们。正是这个平均化步骤，强制使得对应块内的[系数估计](@entry_id:175952)值具有相同的[绝对值](@entry_id:147688)，从而实现了分组 。

由于SLOPE惩罚只与系数[绝对值](@entry_id:147688)的集合有关，而与具体哪个系数对应哪个值无关，因此它是 **[置换](@entry_id:136432)不变 (permutation invariant)** 的。即，对特征的顺序进行任意重排，最终得到的[系数估计](@entry_id:175952)向量只是相应地重排，其值的集合保持不变 。

### 实践考量与关联

除了理解各种正则化策略的内在机制外，在实践中有效地应用它们也需要考虑一些重要问题，并理解它们与其他领域（如[数值优化](@entry_id:138060)和[核方法](@entry_id:276706)）的联系。

#### 选择正则化强度

正则化效果的好坏极度依赖于正则化参数（如 $\lambda$）的选择。对于高维[稀疏回归](@entry_id:276495)中的[LASSO](@entry_id:751223)，理论研究给出了一个“通用”的建议选择：$\lambda \propto \sqrt{\frac{\log p}{n}}$。这个选择旨在以高概率控制住噪声，确保能够正确地进行[变量选择](@entry_id:177971)和[参数估计](@entry_id:139349)。

然而，这个理论选择有一个隐藏的假设，即噪声的标准差 $\sigma$ 为1或已知。在实践中，$\sigma$ 通常是未知的。如果真实噪声水平很高（$\sigma \gg 1$），固定的 $\lambda$ 可能会导致正则化不足；反之，如果噪声水平很低（$\sigma \ll 1$），则可能导致过度正则化。

一个更稳健的策略是采用 **数据依赖的惩罚 (data-dependent penalty)**。具体做法是，首先通过一个“引导”估计（pilot estimation）得到噪声水平 $\widehat{\sigma}$ 的一个估计值，然后用它来调整 $\lambda$ 的尺度 ：
$$
\lambda_{\text{data}} = c \cdot \widehat{\sigma} \cdot \sqrt{\frac{\log p}{n}}
$$
噪声水平 $\widehat{\sigma}$ 可以通过多种方式估计，一个常用的方法是利用[岭回归](@entry_id:140984)。由于岭回归是[收缩估计](@entry_id:636807)，其残差通常能提供一个比[普通最小二乘法](@entry_id:137121)更稳定的噪声[方差估计](@entry_id:268607)，尤其是在 $p > n$ 的情况下。这种自适应地根据数据中的噪声水平来校准正则化强度的方法，通常能在更广泛的场景下取得更好的预测性能。

#### [核方法](@entry_id:276706)中的谱正则化

正则化的思想也深刻地体现在 **[核方法](@entry_id:276706) (kernel methods)** 中。标准的 **[核岭回归](@entry_id:636718) (Kernel Ridge Regression)** 最小化[目标函数](@entry_id:267263) $\|y - K\alpha\|_2^2 + \gamma \alpha^\top K \alpha$，这可以被看作是在由[核函数](@entry_id:145324)定义的[再生核希尔伯特空间](@entry_id:633928)（RKHS）中对[函数范数](@entry_id:165870)进行 $L_2$ 惩罚。

我们可以将其推广为一种更通用的 **谱正则化 (spectral regularization)** 形式，其惩罚项为 $\gamma \alpha^\top K^p \alpha$，其中 $p$ 是一个可调的实数指数 。通过对核矩阵 $K$ 进行谱分解（$K = U\Lambda U^\top$），我们可以分析这种正则化器如何作用于数据的不同“频率”成分（由[特征向量](@entry_id:151813)定义）。

可以证明，这种正则化器对响应向量 $y$ 在每个[特征向量](@entry_id:151813)方向上的投影进行收缩，收缩因子为：
$$
s(\mu) = \frac{1}{1 + \gamma \mu^{p-2}}
$$
其中 $\mu$ 是对应的[特征值](@entry_id:154894)。这个收缩因子的行为完全由指数 $p$ 决定：
- **当 $p  2$ 时** (例如，标准[核岭回归](@entry_id:636718) $p=1$): $s(\mu)$ 是 $\mu$ 的增函数。这意味着模型会强烈抑制与小[特征值](@entry_id:154894)（低频）相关的成分，而保留与大[特征值](@entry_id:154894)（高频）相关的成分。
- **当 $p = 2$ 时**: $s(\mu)$ 是一个与 $\mu$ 无关的常数 $\frac{1}{1+\gamma}$。所有频率成分受到同等程度的收缩。
- **当 $p > 2$ 时**: $s(\mu)$ 是 $\mu$ 的减函数。这是一种反直觉的正则化，它保留低频成分，而抑制高频成分。

这种谱视角对于理解正则化与 **Nyström [核近似](@entry_id:166372)** 等数值方法的相互作用至关重要。Nyström方法通过保留与最大[特征值](@entry_id:154894)对应的[子空间](@entry_id:150286)来低秩近似核矩阵。当 $p2$ 时，正则化器本身就会抑制小[特征值](@entry_id:154894)对应的成分，这与Nyström近似的行为是一致的，因此两者结合得很好。然而，当 $p>2$ 时，正则化器试图保留的恰恰是Nyström方法丢弃的成分，这会导致巨大的近似偏差 。

#### 正则化与[数值稳定性](@entry_id:146550)

最后，值得强调的是，正则化不仅是一种[防止过拟合](@entry_id:635166)的统计工具，也是一种改善[优化问题](@entry_id:266749) **数值稳定性 (numerical stability)** 的数学工具。

在没有正则化的[线性回归](@entry_id:142318)中，我们需要求解法方程 $(X^\top X)\beta = X^\top y$。如果特征之间存在高度相关性（[多重共线性](@entry_id:141597)），矩阵 $X^\top X$ 会变得病态（ill-conditioned），即其[条件数](@entry_id:145150)很大。这使得求解过程对输入中的微小扰动非常敏感，导致解的[方差](@entry_id:200758)极大。

岭回归通过向 $X^\top X$ 添加一个正定矩阵 $\lambda I$ 来解决这个问题。得到的矩阵 $A = X^\top X + \lambda I$ 的最小特征值至少为 $\lambda$，从而保证了其[可逆性](@entry_id:143146)并控制了其条件数。

更进一步，我们可以使用 **[预处理](@entry_id:141204) (preconditioning)** 技术来改善问题的几何结构。通过选择一个合适的预处理器 $P$ 并变换特征为 $\tilde{X} = XP^{-1}$，我们的目标是使新的[Gram矩阵](@entry_id:148915) $\tilde{X}^\top \tilde{X}$ 更接近单位矩阵（即更“各向同性”）。一个精巧的选择是 $P = (X^\top X + \lambda I)^{1/2}$。在这种[预处理](@entry_id:141204)下，岭回归问题被转化为一个条件数显著减小的等价问题，从而提高了数值求解的稳定性和精度 。这揭示了正则化在[统计建模](@entry_id:272466)和数值计算之间扮演的双重角色。