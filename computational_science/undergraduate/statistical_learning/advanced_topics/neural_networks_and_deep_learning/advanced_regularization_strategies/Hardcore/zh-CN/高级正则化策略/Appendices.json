{
    "hands_on_practices": [
        {
            "introduction": "标准的 $L_2$ 正则化（岭回归）通过一个各项同性的惩罚项来约束模型参数，平等地压缩所有系数。然而，在许多实际应用中，我们可能拥有关于参数之间特定关系的先验知识，例如某些系数应该相等或满足特定的线性约束。本练习将指导你如何将这些结构性先验知识编码为一个“零空间惩罚项”，从而设计出更符合问题背景的定制化正则化策略，并探索其对模型泛化能力的影响。",
            "id": "3096585",
            "problem": "给定一个线性预测模型，其参数向量为 $w \\in \\mathbb{R}^d$，输入矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，响应为 $y \\in \\mathbb{R}^n$。考虑一个二次目标函数，它结合了经验风险最小化与两个正则化项：一个各向同性 $\\ell_2$ 惩罚项和一个通过矩阵 $A \\in \\mathbb{R}^{k \\times d}$ 强制实现线性不变性的零空间惩罚项。该目标函数为\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 和 $\\mu \\ge 0$ 是超参数。零空间惩罚项鼓励 $w$ 接近 $A$ 的零空间，从而惩罚对指定线性不变性的偏离。\n\n你的任务是：\n- 仅从最小二乘最小化和矩阵微积分的基本定义出发，推导出 $J(w)$ 的最小化子 $w^\\star$ 的正规方程，并以线性系统解的形式获得 $w^\\star$ 的闭式解。然后推导相关的帽子矩阵 $H$（它将训练响应 $y$ 映射到其拟合值 $\\hat{y} = X w^\\star$）以及有效自由度 $\\mathrm{df} = \\mathrm{trace}(H)$。\n- 实现一个程序，用于求解 $w^\\star$，并针对若干指定的超参数和约束矩阵情况，计算所提供测试集上的有效自由度和样本外均方误差。\n\n使用以下固定的训练和测试数据：\n- 训练设计矩阵 $X_{\\mathrm{train}} \\in \\mathbb{R}^{6 \\times 3}$，\n$$\nX_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n2  1  0 \\\\\n1  -1  1 \\\\\n3  0  -2 \\\\\n0  2  1\n\\end{pmatrix}.\n$$\n- 训练响应 $y_{\\mathrm{train}} \\in \\mathbb{R}^{6}$，\n$$\ny_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n0.1 \\\\\n2.8 \\\\\n6.2 \\\\\n-1.0 \\\\\n7.7 \\\\\n3.1\n\\end{pmatrix}.\n$$\n- 测试设计矩阵 $X_{\\mathrm{test}} \\in \\mathbb{R}^{4 \\times 3}$，\n$$\nX_{\\mathrm{test}} \\;=\\;\n\\begin{pmatrix}\n1  1  0 \\\\\n0  1  2 \\\\\n2  -1  1 \\\\\n1  0  -1\n\\end{pmatrix}.\n$$\n- 无噪声测试响应 $y_{\\mathrm{test}} \\in \\mathbb{R}^{4}$，由真实参数 $w_{\\mathrm{true}} \\in \\mathbb{R}^3$（分量为 $w_{\\mathrm{true}} = (2,\\,2,\\,-1)$）生成：\n$$\ny_{\\mathrm{test}} \\;=\\; X_{\\mathrm{test}}\\, w_{\\mathrm{true}} \\;=\\;\n\\begin{pmatrix}\n4 \\\\\n0 \\\\\n1 \\\\\n3\n\\end{pmatrix}.\n$$\n\n对于所有测试用例，使用 $\\lambda = 0.1$。定义以下约束设置的测试套件，每个设置由一对 $(\\mu, A)$ 指定：\n- 情况 1：$\\mu = 0$, $A = \\begin{pmatrix} 0  0  0 \\end{pmatrix}$。\n- 情况 2：$\\mu = 50$, $A = \\begin{pmatrix} 1  -1  0 \\end{pmatrix}$，这会通过惩罚对由 $Aw = 0$ 定义的零空间的偏离来鼓励不变性 $w_1 = w_2$。\n- 情况 3：$\\mu = 50$, $A = \\begin{pmatrix} 0  1  1 \\end{pmatrix}$，这会通过惩罚对由 $Aw = 0$ 定义的零空间的偏离来鼓励不变性 $w_2 = -w_3$。\n\n对于每种情况：\n- 令 $n = 6$ 和 $d = 3$。构建对称正定矩阵\n$$\nM \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}} \\;+\\; \\lambda I_d \\;+\\; \\mu\\, A^\\top A,\n$$\n和右端项\n$$\nb \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}.\n$$\n- 通过求解线性系统 $M\\,w^\\star = b$ 来计算 $w^\\star$。\n- 计算帽子矩阵\n$$\nH \\;=\\; \\frac{1}{n}\\, X_{\\mathrm{train}}\\, M^{-1}\\, X_{\\mathrm{train}}^\\top,\n$$\n和有效自由度 $\\mathrm{df} = \\mathrm{trace}(H)$。\n- 计算测试均方误差\n$$\n\\mathrm{MSE}_{\\mathrm{test}} \\;=\\; \\frac{1}{m}\\, \\lVert X_{\\mathrm{test}}\\, w^\\star - y_{\\mathrm{test}} \\rVert_2^2,\n$$\n其中 $m = 4$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表应按 $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$ 的顺序排列，其中索引 $i$ 对应于上面的情况 $i$。\n- 在打印行中将所有浮点输出四舍五入到 $6$ 位小数。\n- 要求的最终输出是单行，例如 $[r_1,r_2,r_3,r_4,r_5,r_6]$，并按指定顺序排列。\n\n注意：不涉及角度，也不需要物理单位。所有数值量都应使用标准浮点运算作为实数计算。每个测试用例的答案必须按上述规定以浮点数形式报告。",
            "solution": "我们从线性最小二乘法和矩阵微积分的基本设置开始。设 $X \\in \\mathbb{R}^{n \\times d}$，$y \\in \\mathbb{R}^n$，以及 $w \\in \\mathbb{R}^d$。考虑目标函数\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$，$\\mu \\ge 0$，且 $A \\in \\mathbb{R}^{k \\times d}$。该目标函数等于一个严格凸二次型（前提是正则化确保了正定性）和非负惩罚项之和。通过将梯度设为零，可以得到唯一的最小化子。\n\n使用经过充分检验的二次型梯度矩阵微积分恒等式，即 $\\nabla_w \\lVert y - Xw \\rVert_2^2 = -2 X^\\top (y - Xw)$，$\\nabla_w \\lVert w \\rVert_2^2 = 2 w$，以及 $\\nabla_w \\lVert A w \\rVert_2^2 = 2 A^\\top A w$，我们计算 $J(w)$ 的梯度：\n$$\n\\nabla_w J(w) \\;=\\; -\\frac{2}{n} X^\\top (y - Xw) \\;+\\; 2 \\lambda w \\;+\\; 2 \\mu A^\\top A w.\n$$\n将 $\\nabla_w J(w) = 0$ 并除以 $2$ 得到正规方程\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A\\right) w \\;=\\; \\frac{1}{n} X^\\top y.\n$$\n定义对称正定矩阵\n$$\nM \\;=\\; \\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A,\n$$\n和右端项\n$$\nb \\;=\\; \\frac{1}{n} X^\\top y.\n$$\n然后，通过求解线性系统可以获得唯一的最小化子\n$$\nM w^\\star \\;=\\; b.\n$$\n除了线性系统求解之外，没有必要使用任何快捷公式；一种数值稳定的方法是使用线性求解器，而不是显式地求矩阵的逆。\n\n接下来，对于拟合值 $\\hat{y} = X w^\\star$，代入 $w^\\star = M^{-1} b$ 得到\n$$\n\\hat{y} \\;=\\; X M^{-1} \\left( \\frac{1}{n} X^\\top y \\right) \\;=\\; \\left( \\frac{1}{n} X M^{-1} X^\\top \\right) y.\n$$\n因此，帽子矩阵是\n$$\nH \\;=\\; \\frac{1}{n} X M^{-1} X^\\top.\n$$\n有效自由度，作为衡量线性平滑器模型灵活性的标准度量，由下式给出\n$$\n\\mathrm{df} \\;=\\; \\mathrm{trace}(H).\n$$\n零空间惩罚项通过添加半正定矩阵 $\\mu A^\\top A$ 来修改 $M$。这会收缩 $w$ 在与 $A$ 的零空间正交方向上的分量，从而有效减少这些方向上的方差。当真实参数 $w_{\\mathrm{true}}$ 接近 $A$ 的零空间时，该惩罚项通过减少方差而不引入太多额外偏差来改善泛化能力。相反，如果 $w_{\\mathrm{true}}$ 严重违反约束 $A w = 0$，该惩罚项会引入偏差，从而可能损害样本外性能。\n\n计算每个测试用例所需数量的算法步骤：\n- 输入 $X_{\\mathrm{train}} \\in \\mathbb{R}^{n \\times d}$，$y_{\\mathrm{train}} \\in \\mathbb{R}^n$，$X_{\\mathrm{test}} \\in \\mathbb{R}^{m \\times d}$，$y_{\\mathrm{test}} \\in \\mathbb{R}^{m}$，超参数 $\\lambda$ 和 $\\mu$，以及约束矩阵 $A \\in \\mathbb{R}^{k \\times d}$。\n- 构建 $S = \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}}$，$M = S + \\lambda I_d + \\mu A^\\top A$，以及 $b = \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}$。\n- 使用线性求解器求解 $M w^\\star = b$ 以获得 $w^\\star$。\n- 通过求解 $M C = X_{\\mathrm{train}}^\\top$ 来计算 $C = M^{-1} X_{\\mathrm{train}}^\\top$ 而无需显式求逆；等价地，$C$ 是一个矩阵右端项的解。然后 $H = \\frac{1}{n} X_{\\mathrm{train}} C$ 且 $\\mathrm{df} = \\mathrm{trace}(H)$。\n- 计算 $\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{m} \\lVert X_{\\mathrm{test}} w^\\star - y_{\\mathrm{test}} \\rVert_2^2$。\n- 将浮点输出四舍五入到 $6$ 位小数，并按指定顺序 $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$ 打印。\n\n对于所提供的测试套件：\n- 情况 1 使用 $\\mu = 0$ 和 $A = (0, 0, 0)$，这简化为岭回归，其中 $\\lambda = 0.1$。这给出了一个小于 $d = 3$ 的基准 $\\mathrm{df}$ 和某个样本外 $\\mathrm{MSE}_{\\mathrm{test}}$。\n- 情况 2 使用 $\\mu = 50$ 和 $A = (1, -1, 0)$，强烈鼓励 $w_1 \\approx w_2$。由于真实值满足 $w_1 = w_2$，这个惩罚项应该会减少估计量在 $w_1 - w_2$ 方向上的方差，并且通常会改善 $\\mathrm{MSE}_{\\mathrm{test}}$，同时进一步减少 $\\mathrm{df}$。\n- 情况 3 使用 $\\mu = 50$ 和 $A = (0, 1, 1)$，强烈鼓励 $w_2 \\approx - w_3$，这与 $w_{\\mathrm{true}}$ 中 $w_2 + w_3 = 1$ 的情况相冲突。这会引入偏差，可能导致 $\\mathrm{MSE}_{\\mathrm{test}}$ 膨胀，同时由于强惩罚，$\\mathrm{df}$ 也会有类似的减少。\n\n指定的程序使用线性系统求解，以数值稳定的方式精确实现了这些计算，确保结果以要求的格式聚合，并按要求四舍五入到 $6$ 位小数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    # Fixed training data (n=6, d=3)\n    X_train = np.array([\n        [1.0, 0.0, 2.0],\n        [0.0, 1.0, -1.0],\n        [2.0, 1.0, 0.0],\n        [1.0, -1.0, 1.0],\n        [3.0, 0.0, -2.0],\n        [0.0, 2.0, 1.0]\n    ], dtype=float)\n    y_train = np.array([0.1, 2.8, 6.2, -1.0, 7.7, 3.1], dtype=float)\n\n    # Fixed test data (m=4, d=3)\n    X_test = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 2.0],\n        [2.0, -1.0, 1.0],\n        [1.0, 0.0, -1.0]\n    ], dtype=float)\n    # Ground-truth w_true = (2, 2, -1), so y_test is noise-free\n    y_test = np.array([4.0, 0.0, 1.0, 3.0], dtype=float)\n\n    n, d = X_train.shape\n    m = X_test.shape[0]\n    lam = 0.1  # lambda\n\n    # Test suite of cases: (mu, A)\n    test_cases = [\n        (0.0, np.array([[0.0, 0.0, 0.0]], dtype=float)),     # Case 1\n        (50.0, np.array([[1.0, -1.0, 0.0]], dtype=float)),   # Case 2\n        (50.0, np.array([[0.0, 1.0, 1.0]], dtype=float)),    # Case 3\n    ]\n\n    # Precompute S and b components\n    Xt = X_train.T\n    S = (Xt @ X_train) / n\n    b = (Xt @ y_train) / n\n    I = np.eye(d)\n\n    results = []\n\n    for mu, A in test_cases:\n        # Form M = S + lam*I + mu * A^T A\n        AT_A = A.T @ A\n        M = S + lam * I + mu * AT_A\n\n        # Solve M w = b\n        w_star = np.linalg.solve(M, b)\n\n        # Compute H = (1/n) * X * M^{-1} * X^T without explicit inverse:\n        # Solve M * C = X^T - C = M^{-1} X^T\n        C = np.linalg.solve(M, Xt)  # shape (d, n)\n        H = (X_train @ C) / n\n        df = np.trace(H)\n\n        # Test MSE\n        y_pred_test = X_test @ w_star\n        mse_test = np.mean((y_pred_test - y_test) ** 2)\n\n        # Append rounded results in the specified order\n        results.extend([mse_test, df])\n\n    # Format results to 6 decimal places, no spaces\n    formatted = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "正则化策略的成功在很大程度上取决于正则化参数（如 $\\lambda$）的恰当选择，这通常需要通过耗时的交叉验证来完成。本练习将介绍一种更具物理意义和计算效率的方法——莫罗佐夫差异原则 (Morozov's discrepancy principle)。你将学习如何在一个被高斯噪声污染的线性逆问题中，利用已知的噪声水平来直接确定最优的吉洪诺夫（Tikhonov）正则化参数 $\\lambda$，从而加深对正则化与数据保真度之间权衡的理解。",
            "id": "3096680",
            "problem": "给定一个带有高斯测量噪声的线性逆问题和一个Tikhonov正则化估计量。设 $A \\in \\mathbb{R}^{m \\times n}$，$x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$，且 $y = A x_{\\mathrm{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{true}}^{2} I_{m})$ 具有独立同分布 (i.i.d.) 的分量。对于任意正则化参数 $\\lambda \\ge 0$，将Tikhonov估计量 $x_{\\lambda}$ 定义为严格凸目标 $\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ 的唯一最小化子。Morozov差异原则通过强制目标残差范数 $\\|A x_{\\lambda} - y\\|_{2}$ 等于一个预设的差异水平 $\\delta$ 来选择 $\\lambda$，其中 $\\delta$ 是通过一个假定的噪声标准差 $\\sigma_{\\mathrm{assumed}}$ 设置的，即 $\\delta = \\sqrt{m}\\,\\sigma_{\\mathrm{assumed}}$。\n\n您的任务是：\n\n- 从线性最小二乘和Tikhonov正则化的第一性原理出发，将残差 $\\|A x_{\\lambda} - y\\|_{2}$ 形式化为 $\\lambda$ 的函数，以揭示其对 $\\lambda$ 的单调依赖性。\n- 利用该单调性，设计一个鲁棒的算法来选择 $\\lambda$。当解存在时，通过求解 $\\|A x_{\\lambda} - y\\|_{2} = \\delta$ 来选择 $\\lambda$；否则返回一个边界值。具体而言：\n  - 令 $r_{\\min} = \\|A x_{0} - y\\|_{2}$ 和 $r_{\\max} = \\|y\\|_{2}$。\n  - 如果 $\\delta \\le r_{\\min}$，则设 $\\lambda^{\\star} = 0$。\n  - 如果 $\\delta \\ge r_{\\max}$，则设 $\\lambda^{\\star} = \\lambda_{\\max}$，其中 $\\lambda_{\\max}  0$ 是一个预设的上限。\n  - 否则，找到 $\\lambda^{\\star} \\in (0, \\lambda_{\\max})$ 使得 $\\|A x_{\\lambda^{\\star}} - y\\|_{2} = \\delta$ 在一个小的数值公差范围内成立。\n- 将上述算法实现为一个完整的程序，为固定的合成实例和四种假定的噪声水平（包括两种故意的错误设定）生成结果。\n\n使用以下固定的实例和测试套件：\n\n- 维度：$m = 60$，$n = 40$。\n- 随机种子：$7$。\n- 数据生成：\n  - $A$ 的元素是独立同分布的标准正态分布。\n  - $x_{\\mathrm{true}}$ 的元素是独立同分布的标准正态分布。\n  - $\\sigma_{\\mathrm{true}} = 0.1$。\n  - 噪声 $\\varepsilon$ 的元素是独立同分布的，服从 $\\mathcal{N}(0, \\sigma_{\\mathrm{true}}^{2})$ 分布。\n  - 观测值 $y = A x_{\\mathrm{true}} + \\varepsilon$。\n- 差异水平由 $\\delta = \\sqrt{m}\\,\\sigma_{\\mathrm{assumed}}$ 定义，使用以下四种假定的噪声水平：\n  1. $\\sigma_{\\mathrm{assumed}} = 0.1$（正确设定；理想情况）。\n  2. $\\sigma_{\\mathrm{assumed}} = 10^{-6}$（严重低估设定；预期会触及边界 $\\delta \\le r_{\\min}$）。\n  3. $\\sigma_{\\mathrm{assumed}} = 0.2$（中度高估设定；预期会得到一个内部解）。\n  4. $\\sigma_{\\mathrm{assumed}} = 10.0$（极端高估设定；预期会触及边界 $\\delta \\ge r_{\\max}$）。\n- 使用 $\\lambda_{\\max} = 10^{6}$ 作为搜索区间的上限。\n- 使用一种数值上稳定且高效的方法来计算残差范数 $\\|A x_{\\lambda} - y\\|_{2}$ 作为 $\\lambda$ 的函数，该方法不需从头重复求解线性系统。\n\n您的程序必须：\n\n- 完全按照描述构建固定实例。\n- 对于 $\\sigma_{\\mathrm{assumed}}$ 的四个值中的每一个，根据上述规则计算 $\\lambda^{\\star}$，并计算有符号差距 $\\|A x_{\\lambda^{\\star}} - y\\|_{2} - \\delta$。\n- 将结果汇总到一个列表中，按顺序为每个测试用例连接对 $(\\lambda^{\\star}, \\|A x_{\\lambda^{\\star}} - y\\|_{2} - \\delta)$，作为两个连续的浮点数。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按以下顺序排列四个测试用例的结果：$[\\lambda^{\\star}_{1}, \\mathrm{gap}_{1}, \\lambda^{\\star}_{2}, \\mathrm{gap}_{2}, \\lambda^{\\star}_{3}, \\mathrm{gap}_{3}, \\lambda^{\\star}_{4}, \\mathrm{gap}_{4}]$。",
            "solution": "该问题要求设计并实现一个算法，使用Morozov差异原则来寻找Tikhonov正则化参数 $\\lambda$。这首先需要推导残差范数作为 $\\lambda$ 的函数的解析表达式，证明其单调性，然后利用此性质构建一个鲁棒的求根程序。\n\n设线性模型为 $y = A x_{\\mathrm{true}} + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$，$y \\in \\mathbb{R}^{m}$，且 $\\varepsilon$ 为高斯噪声。对于给定的 $\\lambda \\ge 0$，Tikhonov正则化估计量 $x_{\\lambda}$ 是目标函数的唯一最小化子：\n$$J(x) = \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$$\n通过将梯度 $\\nabla_x J(x)$ 设置为零来找到最小化子：\n$$\\nabla_x J(x) = 2 A^T (A x - y) + 2 \\lambda x = 0$$\n这得到了Tikhonov问题的正规方程：\n$$(A^T A + \\lambda I_n) x = A^T y$$\n对于任意 $\\lambda  0$，矩阵 $(A^T A + \\lambda I_n)$ 是正定的，因此是可逆的。对于 $\\lambda=0$，如果 $A$ 具有满列秩，则该矩阵可逆。解为：\n$$x_{\\lambda} = (A^T A + \\lambda I_n)^{-1} A^T y$$\n残差向量为 $r(\\lambda) = A x_{\\lambda} - y$。我们的目标是将其范数 $\\|r(\\lambda)\\|_2$ 形式化为 $\\lambda$ 的函数。一种计算上高效的方法是使用 $A$ 的奇异值分解 (SVD)。对于矩阵 $A$（其中 $m \\ge n$），我们使用经济SVD：\n$$A = U \\Sigma V^T$$\n其中 $U \\in \\mathbb{R}^{m \\times n}$ 具有标准正交列 ($U^T U = I_n$)，$\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是一个对角矩阵，其对角线上为非负奇异值 $s_i$，而 $V \\in \\mathbb{R}^{n \\times n}$ 是一个正交矩阵 ($V^T V = V V^T = I_n$) 。\n\n将SVD代入 $x_{\\lambda}$ 的表达式中：\n$$x_{\\lambda} = ( (V \\Sigma^T U^T) (U \\Sigma V^T) + \\lambda I_n )^{-1} (V \\Sigma^T U^T) y$$\n$$x_{\\lambda} = ( V \\Sigma^2 V^T + \\lambda V V^T )^{-1} V \\Sigma^T U^T y$$\n$$x_{\\lambda} = ( V (\\Sigma^2 + \\lambda I_n) V^T )^{-1} V \\Sigma^T U^T y$$\n$$x_{\\lambda} = V (\\Sigma^2 + \\lambda I_n)^{-1} V^T V \\Sigma^T U^T y = V (\\Sigma^2 + \\lambda I_n)^{-1} \\Sigma U^T y$$\n现在，我们计算乘积 $A x_{\\lambda}$：\n$$A x_{\\lambda} = (U \\Sigma V^T) \\left( V (\\Sigma^2 + \\lambda I_n)^{-1} \\Sigma U^T y \\right) = U \\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} U^T y$$\n残差向量为 $r(\\lambda) = A x_{\\lambda} - y$。为了分析其范数，我们将 $y$ 分解为两个正交分量：其在 $A$ 的列空间（由 $U$ 的列张成）上的投影 $U U^T y$，以及其在正交补上的投影 $(I_m - U U^T) y$。\n$$r(\\lambda) = U \\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} U^T y - (U U^T y + (I_m - U U^T) y)$$\n$$r(\\lambda) = U \\left( \\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} - I_n \\right) U^T y - (I_m - U U^T) y$$\n括号中的矩阵项是对角的：\n$$\\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} - I_n = \\mathrm{diag}\\left(\\frac{s_i^2}{s_i^2 + \\lambda} - 1\\right) = \\mathrm{diag}\\left(\\frac{-\\lambda}{s_i^2 + \\lambda}\\right) = -\\lambda (\\Sigma^2 + \\lambda I_n)^{-1}$$\n因此，残差向量简化为：\n$$r(\\lambda) = -U \\left( \\lambda (\\Sigma^2 + \\lambda I_n)^{-1} \\right) U^T y - (I_m - U U^T) y$$\n第一项位于 $U$ 的列空间中，而第二项位于其正交补中。根据勾股定理，$r(\\lambda)$ 的范数平方是这两个正交向量范数平方的和：\n$$\\|r(\\lambda)\\|_2^2 = \\| -U \\lambda (\\Sigma^2 + \\lambda I_n)^{-1} U^T y \\|_2^2 + \\| -(I_m - U U^T) y \\|_2^2$$\n由于 $U$ 具有标准正交列，$\\|Uz\\|_2^2 = \\|z\\|_2^2$。令 $\\hat{y} = U^T y$，其中 $\\hat{y}_i = u_i^T y$。\n$$\\|r(\\lambda)\\|_2^2 = \\| \\lambda (\\Sigma^2 + \\lambda I_n)^{-1} \\hat{y} \\|_2^2 + \\|(I_m - U U^T) y\\|_2^2$$\n$$\\|r(\\lambda)\\|_2^2 = \\sum_{i=1}^{n} \\left( \\frac{\\lambda \\hat{y}_i}{s_i^2 + \\lambda} \\right)^2 + \\|(I_m - U U^T) y\\|_2^2$$\n此表达式提供了一种计算残差范数的高效方法，因为SVD和投影 $U^T y$ 只需计算一次。令 $\\phi(\\lambda) = \\|r(\\lambda)\\|_2^2$。\n\n为证明单调性，我们对 $\\lambda  0$ 的情况求 $\\phi(\\lambda)$ 关于 $\\lambda$ 的导数：\n$$\\frac{d\\phi}{d\\lambda} = \\sum_{i=1}^{n} \\hat{y}_i^2 \\frac{d}{d\\lambda} \\left( \\frac{\\lambda}{s_i^2 + \\lambda} \\right)^2 = \\sum_{i=1}^{n} \\hat{y}_i^2 \\cdot 2 \\left( \\frac{\\lambda}{s_i^2 + \\lambda} \\right) \\frac{(s_i^2 + \\lambda) \\cdot 1 - \\lambda \\cdot 1}{(s_i^2 + \\lambda)^2}$$\n$$\\frac{d\\phi}{d\\lambda} = \\sum_{i=1}^{n} \\frac{2 \\lambda s_i^2 \\hat{y}_i^2}{(s_i^2 + \\lambda)^3}$$\n由于 $\\lambda  0$ 且 $s_i \\ge 0$，和中的每一项都是非负的。因此，$\\frac{d\\phi}{d\\lambda} \\ge 0$，这意味着 $\\|r(\\lambda)\\|_2$ 是 $\\lambda$ 的单调非递减函数。这种单调性对于Morozov差异原则至关重要，该原则旨在寻找一个唯一的 $\\lambda^\\star$ 来求解 $\\|A x_{\\lambda^\\star} - y\\|_2 = \\delta$ 以满足目标差异 $\\delta$。\n\n可能的残差范数范围是有界的。\n最小残差范数 $r_{\\min}$ 出现在 $\\lambda = 0$ 时：\n$$r_{\\min} = \\|r(0)\\|_2 = \\sqrt{0 + \\|(I_m - U U^T) y\\|_2^2} = \\|(I_m - A A^+) y\\|_2$$\n这是非正则化最小二乘问题的残差范数，表示 $y$ 在 $A$ 的列空间之外的部分。\n最大残差范数 $r_{\\max}$ 是 $\\lambda \\to \\infty$ 时的极限：\n$$r_{\\max} = \\lim_{\\lambda \\to \\infty} \\|r(\\lambda)\\|_2 = \\sqrt{ \\sum_{i=1}^{n} \\left( \\lim_{\\lambda \\to \\infty} \\frac{\\lambda}{\\lambda} \\hat{y}_i \\right)^2 + \\|...\\|^2 } = \\sqrt{ \\sum_{i=1}^{n} \\hat{y}_i^2 + \\|(I_m - U U^T) y\\|_2^2 } = \\|y\\|_2$$\n这对应于解 $x_\\lambda \\to 0$。\n\n对于给定的差异 $\\delta = \\sqrt{m}\\,\\sigma_{\\mathrm{assumed}}$，寻找 $\\lambda^\\star$ 的算法如下：\n1.  计算 $r_{\\min}$ 和 $r_{\\max}$。\n2.  如果 $\\delta \\le r_{\\min}$：目标 $\\delta$ 小于或等于可能的最小残差。我们能做的最好情况是达到 $r_{\\min}$，这发生在 $\\lambda = 0$ 时。我们设置 $\\lambda^\\star = 0$。\n3.  如果 $\\delta \\ge r_{\\max}$：目标 $\\delta$ 大于或等于任何可实现的残差。根据问题规范，我们受限于 $\\lambda \\in [0, \\lambda_{\\max}]$，因此我们选择边界值 $\\lambda^\\star = \\lambda_{\\max}$。\n4.  如果 $r_{\\min}  \\delta  r_{\\max}$：由于 $\\|r(\\lambda)\\|_2$ 的连续性和单调性，存在一个唯一的 $\\lambda^\\star_0  0$ 使得 $\\|r(\\lambda^\\star_0)\\|_2 = \\delta$。我们通过数值求解方程 $f(\\lambda) = \\|r(\\lambda)\\|_2 - \\delta = 0$ 来找到这个根。由于我们必须在 $(0, \\lambda_{\\max})$ 中找到 $\\lambda^\\star$，我们在此区间内搜索。鉴于 $f(0)  0$ 且 $f(\\lambda)$ 是递增的，如果 $f(\\lambda_{\\max})  0$，则在 $(0, \\lambda_{\\max})$ 中存在一个根。像Brent方法这样的鲁棒求根方法适合此任务。最终解为 $\\lambda^\\star = \\lambda^\\star_0$。\n\n这个过程定义了一个鲁棒的算法来实现Morozov差异原则，处理了选择 $\\lambda$ 时的内部和边界情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves for the Tikhonov regularization parameter lambda using the Morozov discrepancy principle\n    for a fixed synthetic problem instance and four test cases of assumed noise levels.\n    \"\"\"\n    # Define problem parameters and test suite\n    m, n = 60, 40\n    random_seed = 7\n    sigma_true = 0.1\n    sigmas_assumed = [0.1, 1e-6, 0.2, 10.0]\n    lambda_max = 1.0e6\n\n    # Generate the fixed problem instance\n    rng = np.random.default_rng(random_seed)\n    A = rng.standard_normal((m, n))\n    x_true = rng.standard_normal(n)\n    noise = sigma_true * rng.standard_normal(m)\n    y = A @ x_true + noise\n\n    # Pre-computation using SVD for efficient residual norm calculation\n    # Use economy SVD since m >= n\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    \n    # y_hat contains the projection of y onto the basis of U's columns\n    y_hat = U.T @ y\n    \n    # The squared norm of the component of y orthogonal to the column space of A\n    # This is the irreducible part of the residual\n    # Using np.linalg.norm(y)**2 - np.linalg.norm(y_hat)**2 is numerically stable\n    # due to the Pythagorean theorem and avoids forming the large projection matrix.\n    norm_y_perp_sq = np.linalg.norm(y)**2 - np.linalg.norm(y_hat)**2\n\n    def get_residual_norm(lam: float) -> float:\n        \"\"\"\n        Computes ||A*x_lambda - y||_2 efficiently using SVD components.\n        \"\"\"\n        if lam == 0:\n            return np.sqrt(norm_y_perp_sq)\n        \n        # This term corresponds to the projection of the residual onto the column space of A\n        filter_factors = (lam * y_hat) / (s**2 + lam)\n        norm_proj_res_sq = np.sum(filter_factors**2)\n        \n        # The total squared residual norm is the sum of the squared norms of its\n        # orthogonal components.\n        residual_norm_sq = norm_proj_res_sq + norm_y_perp_sq\n        return np.sqrt(residual_norm_sq)\n\n    # Determine boundary residual norms\n    r_min = get_residual_norm(0.)\n    r_max = np.linalg.norm(y) # As lambda -> infinity, x_lambda -> 0, residual -> y\n    \n    results = []\n    \n    for sigma_assumed in sigmas_assumed:\n        # Calculate the discrepancy target\n        delta = np.sqrt(m) * sigma_assumed\n        \n        lambda_star = 0.0\n        \n        # Case 1: Target discrepancy is too small (unachievable)\n        if delta = r_min:\n            lambda_star = 0.0\n        # Case 2: Target discrepancy is too large (approached at large lambda)\n        elif delta >= r_max:\n            lambda_star = lambda_max\n        # Case 3: Interior case, find the unique lambda that matches the discrepancy\n        else:\n            # Objective function for the root-finder\n            root_func = lambda lam: get_residual_norm(lam) - delta\n            \n            # The function root_func is monotonic. Brent's method is robust.\n            # The interval [0, lambda_max] is guaranteed to bracket the root\n            # because root_func(0)  0 and root_func(lambda_max) will be > 0.\n            # An extremely large lambda_max might push get_residual_norm(lambda_max)\n            # beyond r_max due to floating point, but scipy handles it.\n            lambda_star = brentq(root_func, 0, lambda_max)\n            \n        # Calculate the final signed gap for verification\n        gap = get_residual_norm(lambda_star) - delta\n        \n        results.append(lambda_star)\n        results.append(gap)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在神经网络中，Dropout 作为一种强大的随机正则化技术被广泛使用，其工作机制似乎与在损失函数中添加显式惩罚项的传统正则化方法（如权重衰减）截然不同。本练习旨在揭示两者之间的深刻联系，挑战你通过数学推导来证明，在某些假设下，对隐藏层激活应用 Dropout 的期望效果等价于对权重施加一种数据依赖的自适应 $L_2$ 惩罚。这个过程将帮助你理解随机正则化如何隐式地引导优化过程，并从统一的视角看待不同的正则化策略。",
            "id": "3096661",
            "problem": "考虑一个监督学习问题，其输入向量为 $\\mathbf{x} \\in \\mathbb{R}^d$，标量目标为 $y \\in \\mathbb{R}$。设带有平方损失的经验风险为 $\\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(\\mathbf{x}_i)\\right)^2$，其中 $f$ 是一个模型。我们考虑一个具有 $h$ 个隐藏单元的单隐藏层网络，其隐藏激活为 $\\mathbf{a}(\\mathbf{x}) = \\phi(W\\mathbf{x}) \\in \\mathbb{R}^h$，输出为 $f(\\mathbf{x}) = \\mathbf{v}^\\top \\mathbf{a}(\\mathbf{x})$，其中 $\\phi$ 是一个逐点非线性函数，$W \\in \\mathbb{R}^{h \\times d}$ 是输入到隐藏层的权重，$\\mathbf{v} \\in \\mathbb{R}^h$ 是隐藏层到输出层的权重。权重衰减（Weight decay）是指向经验风险中添加一个显式的欧几里得范数（$\\ell_2$）惩罚项 $\\lambda \\|\\mathbf{v}\\|_2^2$，其中 $\\lambda  0$。Dropout 是指在训练期间应用于激活值的乘法伯努利（Bernoulli）掩码。具体来说，假设 dropout 以保留概率 $p \\in (0,1]$ 应用于隐藏激活，并使用反向缩放（inverted scaling）：$\\tilde{\\mathbf{a}}(\\mathbf{x}) = \\left(\\mathbf{m}/p\\right) \\odot \\mathbf{a}(\\mathbf{x})$，其中 $\\mathbf{m} \\in \\{0,1\\}^h$ 具有独立分量 $m_j \\sim \\mathrm{Bernoulli}(p)$，$\\odot$ 表示逐元素乘法。在 dropout 下的输出为 $\\tilde{f}(\\mathbf{x}) = \\mathbf{v}^\\top \\tilde{\\mathbf{a}}(\\mathbf{x})$。假设 dropout 掩码在单元和样本之间是标准独立的，并考虑经验风险在 dropout 随机性上的期望。此外，为了解释浅层网络中正则化效应的可解释性，我们对隐藏层在当前参数周围进行局部线性化，从而在样本 $\\{\\mathbf{x}_i\\}_{i=1}^n$ 上，每个隐藏激活 $a_j(\\mathbf{x}_i)$ 相对于 dropout 噪声可以被视为一个固定值，并且 $\\phi(W\\mathbf{x})$ 关于 $\\mathbf{x}$ 的雅可比矩阵（Jacobian）是有界的，使得 dropout 噪声中的二阶项主导正则化效应。\n\n基于这些假设，并从经验风险、平方损失以及使用反向缩放的 dropout 的构造出发，分析带有隐藏单元 dropout 的期望经验风险与无 dropout 的风险相比如何，以及这种比较与显式欧几里得范数（$\\ell_2$）权重衰减有何关系。然后确定以下哪些陈述是正确的：\n\nA. 在隐藏单元上使用保留概率为 $p$ 的反向 dropout 的情况下，带有平方损失的期望经验风险等于无 dropout 的经验风险加上一个形式为 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2$ 的附加项，前提是 dropout 掩码是独立的，并且隐藏激活相对于 dropout 被视为固定的。\n\nB. 如果 $\\lambda = \\frac{1-p}{p}$，对于任何数据集和架构，系数为 $\\lambda$ 的显式权重衰减都与隐藏单元 dropout 完全等效，从而产生相同的优化轨迹和解。\n\nC. 由使用反向缩放的隐藏单元 dropout 引起的正则化取决于隐藏激活的经验二阶矩，因此是数据依赖的，实际上充当了一种自适应的欧几里得范数（$\\ell_2$）惩罚，它更强烈地收缩连接到高方差隐藏单元的权重。\n\nD. 在相同假设下，隐藏单元 dropout 在 $\\mathbf{v}$ 上引起的是欧几里得范数（$\\ell_1$）惩罚，而不是欧几里得范数（$\\ell_2$）惩罚。\n\nE. 如果在具有平方损失的线性回归模型 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ 中，将保留概率为 $p$ 且使用反向缩放的 dropout 应用于输入，那么期望经验风险等于无 dropout 的风险加上 $\\frac{1-p}{p} \\sum_{j=1}^d w_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n x_{ij}^2$，其中 $x_{ij}$ 是 $\\mathbf{x}_i$ 的第 $j$ 个特征。",
            "solution": "用户想要对一个关于神经网络中 dropout 正则化属性的问题进行严格的验证和求解。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n*   **输入/输出：** 输入向量 $\\mathbf{x} \\in \\mathbb{R}^d$，标量目标 $y \\in \\mathbb{R}$。\n*   **数据集：** $n$ 个独立同分布（i.i.d.）样本 $(\\mathbf{x}_i, y_i)$。\n*   **经验风险：** $L_{emp} = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(\\mathbf{x}_i)\\right)^2$（平方损失）。\n*   **模型：** 单隐藏层网络。\n    *   隐藏层激活：$\\mathbf{a}(\\mathbf{x}) = \\phi(W\\mathbf{x}) \\in \\mathbb{R}^h$。\n    *   输出：$f(\\mathbf{x}) = \\mathbf{v}^\\top \\mathbf{a}(\\mathbf{x})$。\n    *   参数：$W \\in \\mathbb{R}^{h \\times d}$，$\\mathbf{v} \\in \\mathbb{R}^h$。\n*   **权重衰减：** 向经验风险添加的惩罚项 $\\lambda \\|\\mathbf{v}\\|_2^2$，其中 $\\lambda  0$。\n*   **Dropout：**\n    *   应用于隐藏激活 $\\mathbf{a}(\\mathbf{x})$。\n    *   Dropout 掩码：$\\mathbf{m} \\in \\{0,1\\}^h$，其独立分量 $m_j \\sim \\mathrm{Bernoulli}(p)$。\n    *   保留概率：$p \\in (0,1]$。\n    *   反向缩放：$\\tilde{\\mathbf{a}}(\\mathbf{x}) = \\left(\\mathbf{m}/p\\right) \\odot \\mathbf{a}(\\mathbf{x})$。\n    *   带 dropout 的输出：$\\tilde{f}(\\mathbf{x}) = \\mathbf{v}^\\top \\tilde{\\mathbf{a}}(\\mathbf{x})$。\n*   **假设：**\n    1.  目标是分析经验风险在 dropout 随机性上的期望，即 $\\mathbb{E}_{\\mathbf{m}}[L_{emp}(\\tilde{f})]$。\n    2.  Dropout 掩码 $\\mathbf{m}$ 在单元和样本之间是独立的。\n    3.  在分析中，隐藏激活 $a_j(\\mathbf{x}_i)$ 相对于 dropout 噪声被视为固定值。\n    4.  问题背景中提到了局部线性化和二阶项的主导作用，这为假设3提供了理由。\n\n**步骤2：使用提取的已知条件进行验证**\n\n*   **科学依据：** 该问题牢固地植根于统计学习和深度学习理论。Dropout 和权重衰减是标准的正则化技术。分析 dropout 下的期望损失以理解其正则化效应是该领域一个经典且成熟的程序。\n*   **问题定义明确：** 该问题要求进行具体的推导，并基于该推导评估一系列陈述。所提供的假设（例如，将激活相对于 dropout 噪声视为固定值）是此类分析的标准假设，并使问题可解。\n*   **客观性：** 问题使用精确的数学定义进行陈述，避免了任何主观或模糊的语言。\n\n**步骤3：结论与行动**\n\n问题陈述在科学上是合理的、定义明确且客观的。它提供了一套完整的定义和假设，足以进行严格的数学分析。该问题是 **有效的**。我们继续进行求解。\n\n### 推导\n\n目标是计算 dropout 下的期望经验风险 $\\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}]$，并将其与带有权重衰减的标准经验风险进行比较。带有 dropout 的经验风险是 $\\tilde{L}_{emp} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\tilde{f}(\\mathbf{x}_i))^2$。\n\n根据期望的线性性质，我们可以先分析单个样本 $i$ 的期望损失，然后求平均。\n样本 $i$ 的期望损失为 $\\mathbb{E}_{\\mathbf{m}}[(y_i - \\tilde{f}(\\mathbf{x}_i))^2]$。\n令 $Z = \\tilde{f}(\\mathbf{x}_i)$。由于 $y_i$ 相对于 dropout 随机性 $\\mathbf{m}$ 是一个常数，我们可以使用性质 $\\mathbb{E}[(c - Z)^2] = (c - \\mathbb{E}[Z])^2 + \\mathrm{Var}(Z)$。\n这里，$c = y_i$，期望和方差是关于 $\\mathbf{m}$ 计算的。\n\n1.  **计算 $\\mathbb{E}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)]$：**\n    带 dropout 的输出为 $\\tilde{f}(\\mathbf{x}_i) = \\mathbf{v}^\\top \\tilde{\\mathbf{a}}(\\mathbf{x}_i) = \\sum_{j=1}^h v_j \\tilde{a}_j(\\mathbf{x}_i)$。\n    受 dropout 影响的激活为 $\\tilde{a}_j(\\mathbf{x}_i) = \\frac{m_j}{p} a_j(\\mathbf{x}_i)$，其中 $m_j \\sim \\mathrm{Bernoulli}(p)$。\n    $m_j$ 的期望是 $\\mathbb{E}[m_j] = p$。\n    将 $v_j$ 和 $a_j(\\mathbf{x}_i)$ 视为相对于 $\\mathbf{m}$ 的常数：\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\mathbb{E}_{\\mathbf{m}} \\left[ \\sum_{j=1}^h v_j \\frac{m_j}{p} a_j(\\mathbf{x}_i) \\right] = \\sum_{j=1}^h v_j \\frac{\\mathbb{E}[m_j]}{p} a_j(\\mathbf{x}_i) = \\sum_{j=1}^h v_j \\frac{p}{p} a_j(\\mathbf{x}_i) = \\sum_{j=1}^h v_j a_j(\\mathbf{x}_i) = f(\\mathbf{x}_i) $$\n    这证实了由于使用了反向缩放，dropout 下的期望输出就是原始的、无 dropout 的输出。\n\n2.  **计算 $\\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)]$：**\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\mathrm{Var}_{\\mathbf{m}} \\left[ \\sum_{j=1}^h v_j \\tilde{a}_j(\\mathbf{x}_i) \\right] $$\n    对于不同的单元 $j$，dropout 掩码 $m_j$ 是独立的。因此，随机变量 $\\tilde{a}_j(\\mathbf{x}_i)$ 是独立的。对于独立随机变量的和，其方差等于各方差之和：\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\sum_{j=1}^h \\mathrm{Var}_{\\mathbf{m}}[v_j \\tilde{a}_j(\\mathbf{x}_i)] = \\sum_{j=1}^h v_j^2 \\mathrm{Var}_{\\mathbf{m}}[\\tilde{a}_j(\\mathbf{x}_i)] $$\n    现在我们计算单个受 dropout 影响的激活的方差：\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{a}_j(\\mathbf{x}_i)] = \\mathrm{Var}_{\\mathbf{m}}\\left[\\frac{m_j}{p} a_j(\\mathbf{x}_i)\\right] = \\left(\\frac{a_j(\\mathbf{x}_i)}{p}\\right)^2 \\mathrm{Var}(m_j) $$\n    一个伯努利（Bernoulli）变量的方差是 $\\mathrm{Var}(m_j) = p(1-p)$。\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{a}_j(\\mathbf{x}_i)] = \\frac{a_j(\\mathbf{x}_i)^2}{p^2} p(1-p) = \\frac{1-p}{p} a_j(\\mathbf{x}_i)^2 $$\n    将此结果代回到 $\\tilde{f}(\\mathbf{x}_i)$ 的方差表达式中：\n    $$ \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = \\sum_{j=1}^h v_j^2 \\left( \\frac{1-p}{p} a_j(\\mathbf{x}_i)^2 \\right) = \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 $$\n\n3.  **组合期望损失：**\n    将各部分组合起来，样本 $i$ 的期望损失为：\n    $$ \\mathbb{E}_{\\mathbf{m}}[(y_i - \\tilde{f}(\\mathbf{x}_i))^2] = (y_i - \\mathbb{E}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)])^2 + \\mathrm{Var}_{\\mathbf{m}}[\\tilde{f}(\\mathbf{x}_i)] = (y_i - f(\\mathbf{x}_i))^2 + \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 $$\n\n4.  **计算期望经验风险：**\n    最后，我们对所有 $n$ 个样本求平均：\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{m}}[(y_i - \\tilde{f}(\\mathbf{x}_i))^2] $$\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\frac{1}{n} \\sum_{i=1}^n \\left( (y_i - f(\\mathbf{x}_i))^2 + \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 \\right) $$\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2 \\right) + \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{1-p}{p} \\sum_{j=1}^h v_j^2 a_j(\\mathbf{x}_i)^2 \\right) $$\n    重新排列第二项中的求和顺序：\n    $$ \\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\underbrace{\\frac{1}{n} \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2}_{\\text{无 dropout 的经验风险}} + \\underbrace{\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\left( \\frac{1}{n} \\sum_{i=1}^n a_j(\\mathbf{x}_i)^2 \\right)}_{\\text{正则化项}} $$\n\n### 逐项分析\n\n**A. 在隐藏单元上使用保留概率为 $p$ 的反向 dropout 的情况下，带有平方损失的期望经验风险等于无 dropout 的经验风险加上一个形式为 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2$ 的附加项，前提是 dropout 掩码是独立的，并且隐藏激活相对于 dropout 被视为固定的。**\n\n我们推导出的结果是，带 dropout 的期望经验风险是无 dropout 的经验风险与一个正则化项 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\left(\\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2\\right)$ 的和。这与该选项中陈述的形式完全匹配。\n**结论：正确。**\n\n**B. 如果 $\\lambda = \\frac{1-p}{p}$，对于任何数据集和架构，系数为 $\\lambda$ 的显式权重衰减都与隐藏单元 dropout 完全等效，从而产生相同的优化轨迹和解。**\n\n权重衰减的总目标函数是：$L_{WD} = \\left(\\frac{1}{n}\\sum_i (y_i - f(\\mathbf{x}_i))^2\\right) + \\lambda \\sum_{j=1}^h v_j^2$。\ndropout 的期望目标函数是：$\\mathbb{E}_{\\mathbf{m}}[\\tilde{L}_{emp}] = \\left(\\frac{1}{n}\\sum_i (y_i - f(\\mathbf{x}_i))^2\\right) + \\sum_{j=1}^h \\left(\\frac{1-p}{p} \\left(\\frac{1}{n}\\sum_i a_j(\\mathbf{x}_i)^2\\right)\\right) v_j^2$。\n要使这两者等效，对于每个 $j$ 我们需要 $\\lambda = \\frac{1-p}{p} \\left(\\frac{1}{n}\\sum_i a_j(\\mathbf{x}_i)^2\\right)$。这不是一个单一的标量 $\\lambda$。每个权重 $v_j$ 的缩放因子取决于相应激活 $a_j$ 的经验二阶矩。这个因子是数据依赖的，并且在训练过程中随着权重 $W$ 的演变而变化。因此，dropout 不等同于标准权重衰减。此外，使用 dropout 进行训练在每一步优化的是一个随机目标，而不是期望目标，这会导致不同的梯度更新和优化轨迹。关于完全等效的说法是错误的。\n**结论：错误。**\n\n**C. 由使用反向缩放的隐藏单元 dropout 引起的正则化取决于隐藏激活的经验二阶矩，因此是数据依赖的，实际上充当了一种自适应的欧几里得范数（$\\ell_2$）惩罚，它更强烈地收缩连接到高方差隐藏单元的权重。**\n\n根据我们的推导，权重 $v_j$ 的正则化项与 $v_j^2$ 成正比，并按因子 $\\frac{1-p}{p} \\left(\\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2\\right)$ 进行缩放。项 $\\frac{1}{n}\\sum_{i=1}^n a_j(\\mathbf{x}_i)^2$ 是第 $j$ 个隐藏单元激活在数据集上的经验二阶矩。由于 $a_j(\\mathbf{x}_i)$ 依赖于输入数据 $\\mathbf{x}_i$，因此正则化是数据依赖的。它对那些相应隐藏单元 $a_j$ 具有较大平均平方激活的权重 $v_j$ 施加更强的惩罚（即更大的有效 $\\lambda_j$）。大的二阶矩通常与高方差相关（因为 $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\geq 0$），因此定性描述是准确的。这就是自适应 $\\ell_2$ 惩罚的定义。\n**结论：正确。**\n\n**D. 在相同假设下，隐藏单元 dropout 在 $\\mathbf{v}$ 上引起的是欧几里得范数（$\\ell_1$）惩罚，而不是欧几里得范数（$\\ell_2$）惩罚。**\n\n正则化项是 $\\frac{1-p}{p} \\sum_{j=1}^h v_j^2 \\left( \\frac{1}{n} \\sum_{i=1}^n a_j(\\mathbf{x}_i)^2 \\right)$。这是权重平方 $v_j^2$ 的加权和。基于平方值的惩罚是 $\\ell_2$ 惩罚（例如 $\\|\\mathbf{v}\\|_2^2 = \\sum_j v_j^2$）。$\\ell_1$ 惩罚将基于绝对值的和，即 $\\sum_j c_j |v_j|$。所引起的惩罚显然是 $\\ell_2$ 正则化的一种形式，而不是 $\\ell_1$。\n**结论：错误。**\n\n**E. 如果在具有平方损失的线性回归模型 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$ 中，将保留概率为 $p$ 且使用反向缩放的 dropout 应用于输入，那么期望经验风险等于无 dropout 的风险加上 $\\frac{1-p}{p} \\sum_{j=1}^d w_j^2 \\cdot \\frac{1}{n}\\sum_{i=1}^n x_{ij}^2$，其中 $x_{ij}$ 是 $\\mathbf{x}_i$ 的第 $j$ 个特征。**\n\n这种情况是该一般问题的一个特例。我们可以将线性回归模型映射到我们的单隐藏层网络，方法是：将隐藏单元数 $h$ 设置为输入维度 $d$，将隐藏层到输出层的权重 $\\mathbf{v}$ 设置为线性权重 $\\mathbf{w}$，将隐藏激活 $\\mathbf{a}(\\mathbf{x})$ 设置为输入特征 $\\mathbf{x}$（即，假设 $h=d$，$\\phi$ 是恒等函数，$W$ 是单位矩阵）。Dropout 应用于输入 $\\mathbf{x}$，这在这个类比中对应于将其应用于“激活”。\n通过直接代入我们的一般结果：\n$$ \\mathbb{E}_{\\mathbf{m}}[L_{emp}] = (\\text{无 dropout 的风险}) + \\frac{1-p}{p} \\sum_{j=1}^d w_j^2 \\left( \\frac{1}{n} \\sum_{i=1}^n x_{ij}^2 \\right) $$\n该表达式与选项中的陈述完全匹配。\n**结论：正确。**",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}