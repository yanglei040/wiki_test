{
    "hands_on_practices": [
        {
            "introduction": "为了真正理解权重衰减，我们首先需要分析它对模型参数的基本影响。本练习剥离了真实训练过程的复杂性，以分离出其核心机制，要求你找到一个平衡点，在该点上，来自损失梯度的拉力与权重衰减的收缩效应完美平衡。通过求解这个不动点 ，你将对权重衰减如何引导学习过程获得清晰的定量理解。",
            "id": "3169493",
            "problem": "您正在使用梯度下降（GD）和权重衰减来训练一个单层神经网络，其参数向量为 $w \\in \\mathbb{R}^{d}$，惩罚目标函数为 $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$，其中 $\\lambda > 0$ 是权重衰减系数。假设在整个迭代过程中，未正则化损失的梯度是一个常数，等于一个固定向量 $g \\in \\mathbb{R}^{d}$，并且学习率是一个常数 $\\eta > 0$。仅从应用于 $J(w)$ 的梯度下降更新的定义出发，并利用平方欧几里得范数的梯度是其系数缩放的恒等变换这一事实，推导在所述假设下的不动点 $w^{\\star}$（即如果迭代收敛，其收敛到的值）。您可以假设 $\\eta$ 和 $\\lambda$ 的取值能保证迭代对于任何初始化都收敛。请将您的最终答案表示为仅包含 $g$ 和 $\\lambda$ 的封闭形式解析表达式。不要引入任何数值。",
            "solution": "用户要求在特定假设下推导带权重衰减的梯度下降更新的不动点。首先对问题的正确性和可解性进行验证。\n\n### 第1步：提取已知条件\n- 参数向量为 $w \\in \\mathbb{R}^{d}$。\n- 优化算法是梯度下降（GD）。\n- 惩罚目标函数为 $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$。\n- 权重衰减系数为 $\\lambda > 0$。\n- 未正则化损失的梯度是一个常数向量：$\\nabla L(w) = g \\in \\mathbb{R}^{d}$。\n- 学习率是一个常数 $\\eta > 0$。\n- 正则化项的梯度由“平方欧几里得范数的梯度是其系数缩放的恒等变换”这一陈述给出，这意味着 $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\lambda w$。\n- 假设对于任何初始化，GD迭代都收敛到一个不動点 $w^{\\star}$。\n\n### 第2步：使用已知条件进行验证\n该问题描述了机器学习中L2正则化（权重衰减）的标准理论分析。假设损失函数梯度 $\\nabla L(w)$ 是一个常数向量 $g$，这是一种简化，旨在使分析易于处理。这是优化理论中常见的建模选择，代表了算法在简单二次目标上或在梯度近似恒定的局部区域中的行为。所有给定的项（$\\lambda$, $\\eta$）都有明确定义。目标是找到迭代过程的不动点，这是一个数学上适定（well-posed）的问题。该问题是自洽的，在其理论背景下是科学合理的，并且没有矛盾或含糊之处。\n\n### 第3步：结论与行动\n问题有效。将提供完整解答。\n\n### 不动点的推导\n梯度下降过程在每次迭代 $t$ 时根据以下规则更新参数向量 $w$：\n$$\nw^{(t+1)} = w^{(t)} - \\eta \\nabla J(w^{(t)})\n$$\n其中 $w^{(t)}$ 是在迭代 $t$ 时的参数向量，$\\eta$ 是学习率。\n\n要最小化的目标函数为：\n$$\nJ(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}\n$$\n为了应用更新规则，我们首先需要计算 $J(w)$ 相对于 $w$ 的梯度。利用梯度算子的线性性质，我们得到：\n$$\n\\nabla J(w) = \\nabla L(w) + \\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right)\n$$\n问题陈述为此梯度计算提供了两个关键信息：\n1. 未正则化损失的梯度是一个常数向量 $g$：\n$$\n\\nabla L(w) = g\n$$\n2. 正则化项 $\\frac{\\lambda}{2}\\|w\\|^{2}$ 的梯度是 $\\lambda w$。这与标准向量微积分一致，因为平方欧几里得范数 $\\|w\\|^{2} = w^T w$ 的梯度是 $2w$，所以 $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\frac{\\lambda}{2}(2w) = \\lambda w$。\n\n将这些代入 $\\nabla J(w)$ 的表达式中，我们得到：\n$$\n\\nabla J(w) = g + \\lambda w\n$$\n请注意，总目标函数的梯度不是恒定的，因为它取决于当前的参数向量 $w$。\n\n现在，我们可以通过将此梯度代入通用的更新方程来写出针对此问题的具体梯度下降更新规则：\n$$\nw^{(t+1)} = w^{(t)} - \\eta (g + \\lambda w^{(t)})\n$$\n此迭代的一个不动点，记为 $w^{\\star}$，是迭代不再改变的值。也就是说，如果 $w^{(t)} = w^{\\star}$，那么 $w^{(t+1)}$ 也将等于 $w^{\\star}$。我们通过在更新方程中设置 $w^{(t+1)} = w^{(t)} = w^{\\star}$ 来找到这个点：\n$$\nw^{\\star} = w^{\\star} - \\eta (g + \\lambda w^{\\star})\n$$\n我们现在可以解这个方程求出 $w^{\\star}$。从两边减去 $w^{\\star}$ 得到：\n$$\n0 = - \\eta (g + \\lambda w^{\\star})\n$$\n由于学习率 $\\eta$ 是一个给定的正常数（$\\eta > 0$），我们可以将两边同时除以 $-\\eta$ 而不影响等式：\n$$\n0 = g + \\lambda w^{\\star}\n$$\n这个方程表明，不动点是目标函数梯度为零向量的点，这也是 $J(w)$ 的临界点的定义。\n\n为了分离出 $w^{\\star}$，我们重新整理方程：\n$$\n\\lambda w^{\\star} = -g\n$$\n最后，由于权重衰减系数 $\\lambda$ 也是一个给定的正常数（$\\lambda > 0$），我们可以除以 $\\lambda$：\n$$\nw^{\\star} = -\\frac{1}{\\lambda} g\n$$\n这就是在所述假设下梯度下降迭代的不动点的封闭形式表达式。根据要求，结果仅取决于损失的恒定梯度 $g$ 和权重衰减系数 $\\lambda$。学习率 $\\eta$ 影响收敛到此不动点的速率，但不影响其值。迭代收敛的假设确保了 $w^{\\star}$ 是序列 $\\{w^{(t)}\\}$ 的极限。",
            "answer": "$$\n\\boxed{-\\frac{g}{\\lambda}}\n$$"
        },
        {
            "introduction": "在实践中，机器学习模型的构建并非凭空进行；关于数据预处理的选择可能对训练产生深远影响。本练习探讨了输入特征缩放与权重衰减之间的关键关系。你将推导当输入被重新缩放时，权重衰减系数 $\\lambda$ 必须如何调整 ，从而揭示正则化的效果并非独立于数据准备工作。",
            "id": "3169486",
            "problem": "考虑一个单层线性神经元（一个单层神经网络），其参数矢量为 $w \\in \\mathbb{R}^{d}$，将输入 $x \\in \\mathbb{R}^{d}$ 映射到预测值 $f_{w}(x) = w^{\\top} x$。该网络在数据集 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ 上进行训练，通过最小化使用均方误差（MSE）和 $L_2$ 权重衰减的经验风险。形式上，目标函数为\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是行向量为 $x_{i}^{\\top}$ 的设计矩阵，$y \\in \\mathbb{R}^{n}$ 是目标向量，$\\lambda > 0$ 是权重衰减系数，$\\|\\cdot\\|$ 表示欧几里得范数。假设每个输入都通过一个正常数 $\\alpha > 0$ 进行缩放，使得对于所有的 $i$ 都有 $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$，缩放后的设计矩阵变为 $X' = \\alpha X$。为了对原始输入 $x$ 保持相同的预测结果，我们同时将权重重新参数化为 $w' = \\frac{1}{\\alpha} w$，从而对于所有的 $x$ 都有 $f_{w'}(x') = f_{w}(x)$。\n\n定义缩放后的目标函数\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2},\n$$\n其中 $\\lambda' > 0$ 是输入缩放后应用的新权重衰减系数。从经验风险最小化与 MSE 和 $L_2$ 正则化的基本定义出发，确定 $\\lambda'$ 关于 $\\alpha$ 和 $\\lambda$ 的表达式，使得在联合变换 $(x_{i}, w, \\lambda) \\mapsto (x_{i}', w', \\lambda')$ 下，总目标函数值保持不变，即\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)\n$$\n对于所有的 $w$、$X$ 和 $y$。\n\n你的最终答案必须是关于 $\\alpha$ 和 $\\lambda$ 的单个闭式解析表达式。无需四舍五入。",
            "solution": "首先将根据既定标准对问题进行验证。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   模型：一个单层线性神经元，参数为 $w \\in \\mathbb{R}^{d}$，输入为 $x \\in \\mathbb{R}^{d}$。\n-   预测函数：$f_{w}(x) = w^{\\top} x$。\n-   数据集：$\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，目标向量为 $y \\in \\mathbb{R}^{n}$。\n-   原始目标函数：$J(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}$，其中 $\\lambda > 0$ 且 $\\|\\cdot\\|$ 是欧几里得范数。\n-   输入缩放：对于正常数 $\\alpha > 0$，有 $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$。缩放后的设计矩阵为 $X' = \\alpha X$。\n-   权重重新参数化：$w \\mapsto w' = \\frac{1}{\\alpha} w$。问题陈述这确保了 $f_{w'}(x') = f_{w}(x)$。\n-   缩放后的目标函数：$J(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2}$，其中 $\\lambda' > 0$。\n-   不变性条件：找到 $\\lambda'$ 的表达式，使得 $J(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)$ 对所有的 $w$、$X$ 和 $y$ 都成立。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学性：** 该问题是统计学习中的一个标准练习，具体分析了岭回归（带 $L_2$ 正则化的线性回归）的性质。所有概念都是机器学习理论的基础。\n-   **良态性（Well-Posed）：** 该问题提供了一个明确的目标和一个特定的不变性条件。它要求参数之间存在一个唯一确定的关系，这可以通过直接的代数运算找到。\n-   **客观性：** 问题使用形式化的数学语言和定义进行陈述，没有主观或含糊不清的术语。\n-   **完备性与一致性：** 所有必需的变量、函数和变换都已明确定义。设置中没有矛盾。条件 $f_{w'}(x') = f_{w}(x)$ 与 $x'$ 和 $w'$ 的定义是一致的，因为 ${w'}^{\\top} x' = (\\frac{1}{\\alpha}w)^{\\top}(\\alpha x) = \\frac{1}{\\alpha} w^{\\top} \\alpha x = w^{\\top} x$。\n\n**步骤 3：结论与行动**\n该问题被认为是 **有效的**，因为它具有科学性、良态性、客观性和内部一致性。可以继续求解过程。\n\n### 解答\n\n目标是确定 $\\lambda'$ 关于 $\\alpha$ 和 $\\lambda$ 的表达式，以确保目标函数的值在指定的变换下保持不变。不变性条件为：\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(w'; X', y, \\lambda'\\right)\n$$\n其中 $w' = \\frac{1}{\\alpha} w$，且 $X'$ 是行向量为 $x_{i}'^{\\top} = (\\alpha x_{i})^{\\top}$ 的矩阵。\n\n我们通过代入 $x_{i}'$ 和 $w'$ 的表达式来分析缩放后的目标函数 $J(w'; X', y, \\lambda')$。\n缩放后的目标函数由两部分组成：均方误差（MSE）项和正则化项。\n\n首先，考虑 MSE 项：\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2}\n$$\n代入 $w' = \\frac{1}{\\alpha} w$ 和 $x_{i}' = \\alpha x_{i}$：\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w\\right)^{\\top} (\\alpha x_{i})\n$$\n使用标量 $c$ 和向量 $a$ 的转置性质 $(c a)^{\\top} = c a^{\\top}$：\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w^{\\top}\\right) (\\alpha x_{i})\n$$\n由于标量乘法满足交换律和结合律：\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} \\cdot \\alpha\\right) (w^{\\top} x_{i}) = 1 \\cdot (w^{\\top} x_{i}) = w^{\\top} x_{i}\n$$\n这证实了如问题所述，在变换之后，给定数据点的预测值保持不变。\n因此，缩放后目标函数中的 MSE 项与原始 MSE 项相同：\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} = \\text{MSE}\n$$\n\n接下来，考虑缩放后目标函数中的正则化项：\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\|w'\\|^{2}\n$$\n代入 $w' = \\frac{1}{\\alpha} w$：\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left\\|\\frac{1}{\\alpha} w\\right\\|^{2}\n$$\n使用欧几里得范数的性质 $\\|c v\\| = |c| \\|v\\|$，其中 $c$ 是标量，$v$ 是向量：\n$$\n\\left\\|\\frac{1}{\\alpha} w\\right\\|^{2} = \\left(\\left|\\frac{1}{\\alpha}\\right| \\|w\\|\\right)^{2} = \\left(\\frac{1}{\\alpha}\\right)^{2} \\|w\\|^{2} = \\frac{1}{\\alpha^2} \\|w\\|^{2}\n$$\n问题陈述 $\\alpha > 0$，所以 $|\\frac{1}{\\alpha}| = \\frac{1}{\\alpha}$。\n将此代回 $\\text{Reg}'$ 的表达式中：\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left(\\frac{1}{\\alpha^2} \\|w\\|^{2}\\right) = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\n现在，我们可以用原始变量写出完整的缩放后目标函数：\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\n不变性条件要求该表达式对于所有的 $w$、$X$ 和 $y$ 都等于原始目标函数：\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}\n$$\n令两个总目标表达式相等：\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nMSE 项相同，因此可以消去，剩下正则化项的等式：\n$$\n\\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\n这个等式必须对任何参数矢量 $w \\in \\mathbb{R}^d$ 都成立。对于 $w$ 不是零向量（即 $\\|w\\|^2 \\neq 0$）的任何非平凡情况，我们可以将两边同时除以 $\\frac{1}{2}\\|w\\|^2$：\n$$\n\\lambda = \\frac{\\lambda'}{\\alpha^2}\n$$\n解出 $\\lambda'$ 得到所需的关系：\n$$\n\\lambda' = \\alpha^2 \\lambda\n$$\n这个结果表明，为了在输入特征缩放 $\\alpha$ 和相应的权重缩放 $1/\\alpha$ 的情况下保持目标函数等价，权重衰减系数 $\\lambda$ 必须缩放 $\\alpha^2$ 倍。",
            "answer": "$$\\boxed{\\alpha^{2} \\lambda}$$"
        },
        {
            "introduction": "选择正确的正则化方法是模型开发的关键一步，而这通常取决于数据的底层结构。本练习通过一个精心设计的思想实验，对比了权重衰减（$L_2$ 正则化）与 $L_1$ 正则化在处理相关特征时的不同表现。通过分析在何种条件下，权重衰减虽然不能产生稀疏解，却能在预测性能上超越 $L_1$ 正则化 ，你将深入理解这两种正则化策略在稳定性和特征选择方面的根本差异。",
            "id": "3169449",
            "problem": "考虑一个单输出神经网络，它有一个全连接层和恒等激活函数，因此预测由 $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$ 给出，其中权重向量为 $\\mathbf{w} \\in \\mathbb{R}^{2}$，输入为 $\\mathbf{x} \\in \\mathbb{R}^{2}$。数据根据带有高斯协变量的线性模型生成：$\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$，其中 $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$，相关性参数为 $\\rho \\in [-1, 1]$，标签满足 $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$，其中 $\\boldsymbol{\\theta} = (t, t)^{\\top}$ (对于某个 $t \\neq 0$)，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ 是独立噪声。在训练过程中应用了两种正则化方案：\n- $L_1$ 正则化（也称为Lasso），其惩罚参数产生一个稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ (对于某个 $a > 0$) 。\n- $L_2$ 正则化（权重衰减），其惩罚参数产生一个非稀疏解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ (对于相同的 $a > 0$) 。\n注意，$\\|\\mathbf{w}^{(1)}\\|_{1} = \\|\\mathbf{w}^{(2)}\\|_{1} = 2a$，但 $\\mathbf{w}^{(1)}$ 比 $\\mathbf{w}^{(2)}$ 更稀疏。假设测试输入与训练输入遵循相同的分布，性能通过期望均方误差（MSE）来评估，定义为 $\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$。\n\n哪个选项正确地指出了这样一种情况：$L_1$ 正则化在稀疏性上优于 $L_2$，但 $L_2$（权重衰减）产生了严格更优的预测性能，并根据特征相关性和估计器稳定性来解释这一结果？\n\nA. 当特征正相关（$0 \\le \\rho < 1$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$，其中 $t \\neq 0$）时，对于任何 $a > 0$，权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 达到的期望均方误差严格低于稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$，而 $\\mathbf{w}^{(1)}$ 仍然更稀疏。发生这种情况是因为 $L_2$ 将权重分布在相关的方向上，从而降低方差并稳定估计器，而 $L_1$ 则任意地丢弃一个相关输入，增加了偏差和不稳定性。\n\nB. 对于任何相关水平 $0 \\le \\rho < 1$，$L_1$ 正则化总是产生比 $L_2$ 更低的期望均方误差，因为它移除了冗余特征；相关性不影响估计器的稳定性，并且稀疏性直接转化为更好的预测准确性。\n\nC. 仅在特征完全相关（$\\rho = 1$）的情况下，权重衰减的预测性能才优于 $L_1$；否则，两种方法的期望均方误差相等，因为它们正则化的总幅度 $\\|\\mathbf{w}\\|_{1} = 2a$ 相同。\n\nD. 当特征强负相关（例如 $\\rho \\approx -0.9$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$）时，稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ 比权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 更稳定，产生更低的期望均方误差，因为丢弃一个负相关特征可以减少有害的方差；$L_2$ 保留了两个输入，因此会过拟合。",
            "solution": "用户希望我解决一个关于简单线性模型中正则化的问题。\n\n我的计划是：\n1.  **解构请求：** 我将分析角色、任务、格式规则和验证过程。\n2.  **执行计划：**\n    *   **验证问题：** 我将逐字提取所有给定信息，并根据科学原则进行核对，确保问题是适定且客观的。\n    *   **推导解法：** 如果问题有效，我将计算两种正则化方案的期望均方误差（MSE）。\n    *   **比较方案：** 我将比较 MSE，以确定在哪种条件下一种方案优于另一种。\n    *   **评估选项：** 我将根据我的推导和已建立的统计理论分析每个选项（A、B、C、D）。\n3.  **构建最终输出：** 我将以指定的 XML 风格和 LaTeX 格式编写解法和最终答案。\n\n---\n\n### 步骤 1：问题验证\n\n我将首先根据指示验证问题陈述。\n\n#### 步骤 1.1：提取已知条件\n\n-   **模型：** 单输出神经网络，一个全连接层，恒等激活函数。\n-   **预测：** $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$。\n-   **权重向量：** $\\mathbf{w} \\in \\mathbb{R}^{2}$。\n-   **输入向量：** $\\mathbf{x} \\in \\mathbb{R}^{2}$。\n-   **输入分布：** $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$。\n-   **协方差矩阵：** $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$。\n-   **相关性参数：** $\\rho \\in [-1, 1]$。\n-   **标签生成过程：** $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$。\n-   **真实参数向量：** $\\boldsymbol{\\theta} = (t, t)^{\\top}$ (对于某个 $t \\neq 0$)。\n-   **噪声：** $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$，与 $\\mathbf{x}$ 独立。\n-   **$L_1$ 正则化解 (Lasso)：** $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ (对于某个 $a > 0$)。\n-   **$L_2$ 正则化解 (权重衰减)：** $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ (对于相同的 $a > 0$)。\n-   **稀疏性：** $\\mathbf{w}^{(1)}$ 比 $\\mathbf{w}^{(2)}$ 更稀疏。\n-   **测试数据分布：** 与训练分布相同。\n-   **性能指标：** 期望均方误差 (MSE)，$\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$。\n\n#### 步骤 1.2：使用提取的已知条件进行验证\n\n-   **科学依据：** 该问题是统计学习理论中一个标准的、成熟的问题，涉及在线性模型中使用相关高斯特征时，对 Ridge ($L_2$) 和 Lasso ($L_1$) 回归进行比较。所有组成部分都基于统计学和机器学习的基本原理。\n-   **适定性：** 该问题是适定的。所有变量、分布和参数都已明确定义，其目标——比较两种特定估计器的期望 MSE——是明确的。可以进行唯一的比较。\n-   **客观性：** 问题陈述使用精确的数学语言表达，没有任何主观或基于意见的论断。\n\n该问题不存在任何无效性缺陷：\n1.  **科学/事实不健全：** 无。该设置是用于说明 $L_1$ 和 $L_2$ 正则化性质的典型示例。\n2.  **不可形式化/不相关：** 无。该问题是形式化的，并与指定主题直接相关。\n3.  **不完整/矛盾：** 无。该问题提供了计算和比较两种估计器期望 MSE 所需的所有信息。假设这些特定的解 $\\mathbf{w}^{(1)}$ 和 $\\mathbf{w}^{(2)}$ 是由它们各自的正则化方案针对某些惩罚参数产生的，这是一个用于比较分析的有效设置。\n4.  **不切实际/不可行：** 该设置是一个理论模型，但它完全可行且在数学上是一致的。\n5.  **不适定：** 该问题定义明确，并导致一个确定的结果。\n6.  **伪深刻/琐碎：** 该问题探讨了正则化中的一个核心且不琐碎的概念：在存在特征相关性的情况下，稀疏性与稳定性之间的权衡。\n7.  **超出科学可验证性范围：** 无。这些论断可以通过数学推导直接验证。\n\n#### 步骤 1.3：结论和行动\n\n问题陈述是 **有效的**。我将继续进行解答。\n\n### 步骤 2：推导与分析\n\n性能指标是期望均方误差 (MSE)，$\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$。期望是针对 $\\mathbf{x}$ 和 $\\varepsilon$ 的联合分布计算的。\n\n我们来分解 MSE。我们有 $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$。\n$$\ny - \\mathbf{w}^{\\top}\\mathbf{x} = (\\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon) - \\mathbf{w}^{\\top}\\mathbf{x} = (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x} + \\varepsilon\n$$\n对该表达式求平方得：\n$$\n(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2} = \\left((\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right)^2 + 2\\varepsilon (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x} + \\varepsilon^2\n$$\n取期望，我们注意到 $\\varepsilon$ 与 $\\mathbf{x}$ 独立且 $\\mathbb{E}[\\varepsilon] = 0$。因此，交叉项消失：$\\mathbb{E}\\left[2\\varepsilon (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right] = 2\\mathbb{E}[\\varepsilon] \\mathbb{E}\\left[(\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right] = 0$。我们还有 $\\mathbb{E}[\\varepsilon^2] = \\text{Var}(\\varepsilon) = \\sigma^2$。\n\n因此，期望 MSE 为：\n$$\n\\text{MSE}(\\mathbf{w}) = \\mathbb{E}\\left[\\left((\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right)^2\\right] + \\sigma^2\n$$\n第一项是二次型的期望。对于随机向量 $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$ 和固定向量 $\\mathbf{v} = \\boldsymbol{\\theta} - \\mathbf{w}$，随机变量 $Z = \\mathbf{v}^{\\top}\\mathbf{x}$ 服从正态分布，其均值为 $\\mathbb{E}[Z] = \\mathbf{v}^{\\top}\\mathbb{E}[\\mathbf{x}] = 0$，方差为 $\\text{Var}(Z) = \\mathbf{v}^{\\top}\\Sigma\\mathbf{v}$。因此，$\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2 = \\mathbf{v}^{\\top}\\Sigma\\mathbf{v}$。\n\n所以，期望 MSE 简化为：\n$$\n\\text{MSE}(\\mathbf{w}) = (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\Sigma(\\boldsymbol{\\theta} - \\mathbf{w}) + \\sigma^2\n$$\n项 $\\sigma^2$ 是两种估计器共有的不可约误差。为了比较它们，我们只需要比较风险项 $R(\\mathbf{w}) = (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\Sigma(\\boldsymbol{\\theta} - \\mathbf{w})$。\n\n给定 $\\boldsymbol{\\theta} = (t, t)^{\\top}$，$\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$，$\\mathbf{w}^{(2)} = (a, a)^{\\top}$，以及 $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$。\n\n**对 $L_1$ 解 $\\mathbf{w}^{(1)}$ 的分析：**\n差分向量为 $\\boldsymbol{\\theta} - \\mathbf{w}^{(1)} = (t, t)^{\\top} - (2a, 0)^{\\top} = (t - 2a, t)^{\\top}$。\n风险为：\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a, t) \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} \\begin{pmatrix} t - 2a \\\\ t \\end{pmatrix}\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a, t) \\begin{pmatrix} (t - 2a) + \\rho t \\\\ \\rho(t - 2a) + t \\end{pmatrix}\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a)[(t - 2a) + \\rho t] + t[\\rho(t - 2a) + t]\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t - 2a)^2 + 2\\rho t(t-2a) + t^2\n$$\n$$\nR(\\mathbf{w}^{(1)}) = (t^2 - 4at + 4a^2) + (2\\rho t^2 - 4a\\rho t) + t^2\n$$\n$$\nR(\\mathbf{w}^{(1)}) = 4a^2 - 4at(1+\\rho) + 2t^2(1+\\rho)\n$$\n\n**对 $L_2$ 解 $\\mathbf{w}^{(2)}$ 的分析：**\n差分向量为 $\\boldsymbol{\\theta} - \\mathbf{w}^{(2)} = (t, t)^{\\top} - (a, a)^{\\top} = (t - a, t - a)^{\\top}$。\n风险为：\n$$\nR(\\mathbf{w}^{(2)}) = (t - a, t - a) \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} \\begin{pmatrix} t - a \\\\ t - a \\end{pmatrix}\n$$\n令 $c = t - a$。表达式为 $(c, c) \\Sigma (c, c)^{\\top}$。\n$$\nR(\\mathbf{w}^{(2)}) = (c, c) \\begin{pmatrix} c + \\rho c \\\\ \\rho c + c \\end{pmatrix} = c(c + \\rho c) + c(\\rho c + c) = 2c^2 + 2\\rho c^2 = 2c^2(1+\\rho)\n$$\n将 $c = t - a$ 代回：\n$$\nR(\\mathbf{w}^{(2)}) = 2(t - a)^2(1 + \\rho)\n$$\n$$\nR(\\mathbf{w}^{(2)}) = 2(t^2 - 2at + a^2)(1 + \\rho)\n$$\n$$\nR(\\mathbf{w}^{(2)}) = 2t^2(1+\\rho) - 4at(1+\\rho) + 2a^2(1+\\rho)\n$$\n\n**比较：**\n我们想要找到 $L_2$ 解性能严格更优的条件，即 $\\text{MSE}(\\mathbf{w}^{(2)})  \\text{MSE}(\\mathbf{w}^{(1)})$，这等价于 $R(\\mathbf{w}^{(2)})  R(\\mathbf{w}^{(1)})$。\n\n$$\nR(\\mathbf{w}^{(1)}) = 4a^2 - 4at(1+\\rho) + 2t^2(1+\\rho)\n$$\n$$\nR(\\mathbf{w}^{(2)}) = 2a^2(1+\\rho) - 4at(1+\\rho) + 2t^2(1+\\rho)\n$$\n\n比较 $R(\\mathbf{w}^{(2)})  R(\\mathbf{w}^{(1)})$ 时，可以通过消去公共项 $-4at(1+\\rho) + 2t^2(1+\\rho)$ 来简化：\n$$\n2a^2(1+\\rho)  4a^2\n$$\n由于 $a  0$，我们可以除以 $2a^2$：\n$$\n1 + \\rho  2\n$$\n$$\n\\rho  1\n$$\n问题指定 $\\rho \\in [-1, 1]$。因此，对于任何严格小于 1 的 $\\rho$ 值，$\\ell_2$ 解 $\\mathbf{w}^{(2)}$ 产生的期望 MSE 严格低于 $\\ell_1$ 解 $\\mathbf{w}^{(1)}$。如果 $\\rho = 1$，它们的 MSE 相等。\n\n这个结果在直觉上是合理的。真实模型是 $\\boldsymbol{\\theta} = (t,t)^{\\top}$，意味着两个特征同等重要。$L_2$ 解 $\\mathbf{w}^{(2)} = (a,a)^{\\top}$ 尊重了这一结构，而稀疏的 $L_1$ 解 $\\mathbf{w}^{(1)}=(2a,0)^{\\top}$ 错误地丢弃了其中一个相关特征。当特征相关（$\\rho  0$）时，它们携带冗余信息。Lasso ($L_1$) 会任意选择其中一个，这可能是一个不稳定的选择，并通过将一个真实的非零系数设为零而引入偏差。Ridge ($L_2$) 通过其“分组效应”，将相关特征的系数一起收缩，从而得到一个更稳定的估计器，该估计器将权重分布在相关组中，这在本例中更好地表示了真实的底层模型。\n\n### 步骤 3：逐项分析选项\n\n**A. 当特征正相关（$0 \\le \\rho  1$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$，其中 $t \\neq 0$）时，对于任何 $a  0$，权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 达到的期望均方误差严格低于稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$，而 $\\mathbf{w}^{(1)}$ 仍然更稀疏。发生这种情况是因为 $L_2$ 将权重分布在相关的方向上，从而降低方差并稳定估计器，而 $L_1$ 则任意地丢弃一个相关输入，增加了偏差和不稳定性。**\n\n-   **性能论断：** 正确。条件 $0 \\le \\rho  1$ 是我们推导出的条件 $\\rho  1$ 的一个子集，在该条件下 $\\text{MSE}(\\mathbf{w}^{(2)})  \\text{MSE}(\\mathbf{w}^{(1)})$。\n-   **稀疏性论断：** 正确。$\\mathbf{w}^{(1)}$ 有一个零分量，而 $\\mathbf{w}^{(2)}$ 没有。\n-   **解释：** 正确。该推理准确地描述了 $L_2$ 正则化的“分组效应”，即通过在相关特征之间分配权重来提高稳定性和降低误差，这与 $L_1$ 任意选择一个特征的倾向形成对比。这是对这一现象的标准、公认的解释。\n\n结论：**正确**。\n\n**B. 对于任何相关水平 $0 \\le \\rho  1$，$L_1$ 正则化总是产生比 $L_2$ 更低的期望均方误差，因为它移除了冗余特征；相关性不影响估计器的稳定性，并且稀疏性直接转化为更好的预测准确性。**\n\n-   **性能论断：** 错误。我们的推导显示了相反的情况：对于 $0 \\le \\rho  1$，$L_2$ 产生严格更低的期望 MSE。\n-   **解释：** 错误。该解释包含多个谬误。移除特征并非总是有益的，特别是当真实系数非零时。相关性深刻影响估计器的稳定性。稀疏性不会自动转化为更好的准确性，尤其是在真实模型不稀疏的情况下。\n\n结论：**错误**。\n\n**C. 仅在特征完全相关（$\\rho = 1$）的情况下，权重衰减的预测性能才优于 $L_1$；否则，两种方法的期望均方误差相等，因为它们正则化的总幅度 $\\|\\mathbf{w}\\|_{1} = 2a$ 相同。**\n\n-   **性能论断：** 错误。我们的推导表明，当 $\\rho=1$ 时，MSE 相等。当 $\\rho  1$ 时，$L_2$ 优于 $L_1$。该选项颠倒了这一关系。\n-   **解释：** 错误。$\\|\\mathbf{w}^{(1)}\\|_{1} = \\|\\mathbf{w}^{(2)}\\|_{1}$ 这个事实只是问题设置中的一个附带条件，并非性能差异的原因。性能是由 MSE 计算决定的，而不是由某个范数的相等性决定的。\n\n结论：**错误**。\n\n**D. 当特征强负相关（例如 $\\rho \\approx -0.9$）且真实信号分布在两个输入上（$\\boldsymbol{\\theta} = (t, t)^{\\top}$）时，稀疏解 $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ 比权重衰减解 $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ 更稳定，产生更低的期望均方误差，因为丢弃一个负相关特征可以减少有害的方差；$L_2$ 保留了两个输入，因此会过拟合。**\n\n-   **性能论断：** 错误。条件 $\\rho \\approx -0.9$ 属于 $\\rho  1$ 的情况，我们的计算证明了在这种情况下 $L_2$ 的 MSE 更低，而不是 $L_1$。\n-   **解释：** 错误。其推理存在缺陷。当特征负相关时，使用它们的和，如在 $L_2$ 预测 $\\hat{y} = a(x_1+x_2)$ 中那样，实际上更稳定。和的方差是 $\\text{Var}(x_1+x_2) = \\text{Var}(x_1) + \\text{Var}(x_2) + 2\\text{Cov}(x_1,x_2) = 1+1+2\\rho = 2(1+\\rho)$。对于 $\\rho = -0.9$，该方差为 $2(0.1)=0.2$。仅基于 $x_1$ 的 $L_1$ 预测的方差与 $\\text{Var}(x_1)=1$ 成正比。因此，保留两个负相关特征可以降低方差；在这种情况下，它不会导致过拟合。\n\n结论：**错误**。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}