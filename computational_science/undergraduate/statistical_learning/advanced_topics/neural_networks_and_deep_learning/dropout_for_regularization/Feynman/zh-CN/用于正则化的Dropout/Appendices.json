{
    "hands_on_practices": [
        {
            "introduction": "在训练期间应用dropout后，我们需要一个确定性的模型用于测试。一种看似直观的方法是在测试时用其期望值替换随机的dropout掩码，即“均值缩放”。本练习将通过一个简单的非线性模型，揭示为何这种方法存在缺陷，并阐明为何现代的“倒置dropout”（inverted dropout）是更优越的选择。通过这个练习，你将亲手量化这种偏差，从而深刻理解dropout机制的精髓。",
            "id": "3117336",
            "problem": "考虑一个标量回归模型，该模型包含一个神经元，对线性变换后的输入施加非线性激活。设输入为 $x \\in \\mathbb{R}$，权重为 $w \\in \\mathbb{R}$，激活函数为二次映射 $\\phi(z) = z^{2}$。在训练期间，模型使用 dropout：预激活值与一个随机保留变量 $r \\in \\{0,1\\}$ 相乘，其中对于每个样本，$r \\sim \\mathrm{Bernoulli}(q)$ 独立同分布，保留概率为 $q \\in (0,1)$。因此，训练时的预测值为\n$$\n\\hat{y}_{\\text{train}}(r) = \\phi(w\\,x\\,r) .\n$$\n在测试时，实践者采用均值缩放（mean scaling）：随机保留变量被其均值替换，从而得到确定性预测\n$$\n\\hat{y}_{\\text{ms}} = \\phi(q\\,w\\,x) .\n$$\n仅使用期望的定义、伯努利分布和给定模型，推导并计算由均值缩放引起的偏差（bias），\n$$\n\\text{bias} \\;=\\; \\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] \\;-\\; \\hat{y}_{\\text{ms}} ,\n$$\n具体数值为 $x=-4$，$w=1.5$ 和 $q=0.7$。请将最终答案以一个实数形式给出。无需四舍五入。",
            "solution": "该问题要求计算一个带有 dropout 的简单神经网络模型，在测试时使用均值缩放所引起的偏差。偏差定义为训练时预测的期望值与测试时确定性预测值之间的差。\n$$\n\\text{bias} \\;=\\; \\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] \\;-\\; \\hat{y}_{\\text{ms}}\n$$\n首先，我们写出右侧两项的显式形式。训练时的预测由 $\\hat{y}_{\\text{train}}(r) = \\phi(w\\,x\\,r)$ 给出。使用指定的二次激活函数 $\\phi(z) = z^2$，上式变为：\n$$\n\\hat{y}_{\\text{train}}(r) = (w\\,x\\,r)^2 = w^2 x^2 r^2\n$$\n使用均值缩放的测试时预测，其中随机变量 $r$ 被其均值 $\\mathbb{E}[r]=q$ 替换，由 $\\hat{y}_{\\text{ms}} = \\phi(q\\,w\\,x)$ 给出。其计算结果为：\n$$\n\\hat{y}_{\\text{ms}} = (q\\,w\\,x)^2 = q^2 w^2 x^2\n$$\n接下来，我们必须计算训练时预测的期望值 $\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right]$。随机变量 $r$ 服从伯努利分布 $r \\sim \\mathrm{Bernoulli}(q)$，这意味着 $r$ 以概率 $q$ 取值 1，以概率 $1-q$ 取值 0。\n期望为：\n$$\n\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] = \\mathbb{E}_{r}\\!\\left[w^2 x^2 r^2\\right]\n$$\n由于 $w$ 和 $x$ 相对于 $r$ 的期望是常数，它们可以被提出来：\n$$\n\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] = w^2 x^2 \\mathbb{E}_{r}\\!\\left[r^2\\right]\n$$\n为了求 $\\mathbb{E}_{r}[r^2]$，我们使用离散随机变量的期望定义：\n$$\n\\mathbb{E}_{r}\\!\\left[r^2\\right] = \\sum_{k \\in \\{0,1\\}} k^2 P(r=k) = (0^2) \\cdot P(r=0) + (1^2) \\cdot P(r=1)\n$$\n代入伯努利分布的概率：\n$$\n\\mathbb{E}_{r}\\!\\left[r^2\\right] = (0) \\cdot (1-q) + (1) \\cdot q = q\n$$\n服从伯努利分布的随机变量 $r$ 有一个重要性质：对于任意整数次幂 $n \\ge 1$，我们有 $r^n=r$，因为 $0^n=0$ 且 $1^n=1$。因此，$\\mathbb{E}_{r}[r^2] = \\mathbb{E}_{r}[r] = q$。\n\n将此结果 $q$ 代回训练预测期望的表达式中：\n$$\n\\mathbb{E}_{r}\\!\\left[\\hat{y}_{\\text{train}}(r)\\right] = w^2 x^2 q\n$$\n现在我们可以通过从训练时预测的期望值中减去均值缩放的测试时预测值，来组合出偏差的完整表达式：\n$$\n\\text{bias} = (w^2 x^2 q) - (q^2 w^2 x^2)\n$$\n提出公因式 $w^2 x^2 q$ 得到该模型中偏差的一般符号表达式：\n$$\n\\text{bias} = w^2 x^2 q(1-q)\n$$\n最后，我们代入问题陈述中给出的具体数值：$x=-4$，$w=1.5$ 和 $q=0.7$。\n$$\n\\text{bias} = (1.5)^2 (-4)^2 (0.7)(1-0.7)\n$$\n$$\n\\text{bias} = (2.25) (16) (0.7)(0.3)\n$$\n我们分开计算乘积。首先是平方项的乘积：\n$$\n(2.25) \\cdot (16) = \\left(\\frac{9}{4}\\right) \\cdot (16) = 9 \\cdot 4 = 36\n$$\n接下来是涉及保留概率的乘积：\n$$\n(0.7) \\cdot (0.3) = 0.21\n$$\n将这两个结果相乘，得到偏差的最终值：\n$$\n\\text{bias} = 36 \\cdot 0.21 = 7.56\n$$",
            "answer": "$$\n\\boxed{7.56}\n$$"
        },
        {
            "introduction": "在现代神经网络中，各个层很少孤立使用。本练习旨在探讨两种常用技术——dropout和批归一化（Batch Normalization）——之间的关键相互作用。我们将分析它们在网络块中的相对顺序如何影响训练和推理阶段的数据分布，从而可能导致影响模型性能的“分布偏移”。理解这种相互作用对于设计稳健有效的网络架构至关重要。",
            "id": "3118023",
            "problem": "给定两个深度神经网络块，它们在其他方面完全相同，唯一的区别在于一个 dropout 层相对于一个 Batch Normalization (BN) 层的位置。Batch Normalization (BN) 指的是一种标准变换，它在训练期间使用批次均值和批次方差对每个通道进行归一化，在推理期间则使用训练过程中累积的运行（指数平均）均值和方差进行归一化，之后再进行一次学习到的仿射变换。Dropout 在训练期间使用常见的“倒置”实现：每个单元以概率 $p$ 被独立保留并按 $1/p$ 进行缩放，而在推理期间 dropout 被禁用。考虑一个通用的预激活值 $x$ 进入每个块中的 BN 层，并假设 dropout 掩码与 $x$ 无关。你将在留出数据上，分别在训练模式和推理模式下，为每个块和每个相关张量位置测量经验性的逐层统计量 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$，以诊断分布偏移。这两个块是：\n- A 块（“dropout 在 BN 之前”）：上一层的激活值 $\\to$ 保留概率为 $p$ 并进行倒置缩放的 dropout $\\to$ BN $\\to$ 剩余层。\n- B 块（“dropout 在 BN 之后”）：上一层的激活值 $\\to$ BN $\\to$ 保留概率为 $p$ 并进行倒置缩放的 dropout $\\to$ 剩余层。\n假设这些块的上游输入具有有限的一阶矩和二阶矩，并且训练使用足够大的批次，以至于 BN 的运行统计量收敛到相应的训练时批次统计量。在这些假设下，当比较 A 块与 B 块在训练与推理模式下，于 BN 输入和 BN 输出处测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 时，哪个陈述最能预测你将观察到的模式？\n\nA. 在 A 块中，BN 的运行方差被训练期间遇到的掩码噪声所放大，因此在推理时，BN 输入的方差小于运行方差，并且 BN 输出的方差收缩到大约 $p$（小于 $1$），而均值保持在大约 $0$。在 B 块中，BN 的统计量在 BN 输入和 BN 输出处，训练和推理之间都非常匹配，因此它们的均值和方差一致；任何训练-推理方差差异仅出现在 dropout 层之后，在该处，训练方差大约大 $1/p$ 倍。\n\nB. 在 A 块中，BN 消除了任何训练-推理不匹配，因此 BN 输入和 BN 输出在训练和推理之间的均值和方差完全匹配。在 B 块中，将 dropout 放在 BN 之后，在推理时，会在 BN 输出处引起一个与 $(1-p)$ 成比例的均值偏移。\n\nC. 在 A 块中，BN 之前的掩码操作会使 BN 的运行均值偏向 $0$，因此在推理时，BN 输出存在非零的均值不匹配，而其方差匹配。在 B 块中，BN 之后的掩码操作会使 BN 的运行方差向上偏置，因此在推理时，BN 输出的方差会膨胀 $1/p$ 倍。\n\nD. 使用倒置 dropout 时，位置无关紧要：对于任何 $p$，每一层的期望和方差在训练和推理之间都匹配，因此在两个块中，所有在 BN 输入和输出处测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 都一致。",
            "solution": "用户希望我分析两个涉及 Batch Normalization (BN) 和 Dropout 的神经网络结构块的统计特性，特别关注训练模式和推理模式之间的分布偏移。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   **A 块（“dropout 在 BN 之前”）：** 上一层的激活值 $\\to$ dropout $\\to$ BN $\\to$ 剩余层。\n-   **B 块（“dropout 在 BN 之后”）：** 上一层的激活值 $\\to$ BN $\\to$ dropout $\\to$ 剩余层。\n-   **Batch Normalization (BN)：**\n    -   训练：使用批次均值/方差进行归一化。之后进行学习到的仿射变换 $z = \\gamma \\hat{x} + \\beta$。\n    -   推理：使用训练期间累积的运行均值/方差进行归一化。之后进行相同的仿射变换。\n-   **Dropout：**\n    -   “倒置”实现。\n    -   训练：每个单元以概率 $p$ 保留，并按 $1/p$ 缩放。否则置零。\n    -   推理：Dropout 是一个恒等函数（被禁用）。\n-   **输入：** 进入该块的通用预激活值，我们称之为 $y$。dropout 掩码与 $y$ 无关。\n-   **任务：** 比较两个块在训练模式与推理模式下，在 BN 输入和 BN 输出处的经验性逐层统计量 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$。\n-   **假设：**\n    1.  上游输入 $y$ 具有有限的一阶矩和二阶矩。设 $\\operatorname{E}[y] = \\mu_y$ 和 $\\operatorname{Var}[y] = \\sigma_y^2$。\n    2.  训练使用足够大的批次，以至于 BN 的运行统计量收敛到相应的训练时批次统计量。这意味着 BN 存储的运行均值 $\\mu_{\\text{run}}$ 和运行方差 $\\sigma^2_{\\text{run}}$ 等于训练期间进入 BN 层的张量的总体均值和方差。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学性：** 该问题在深度学习和统计学原理上有充分的依据。对 Batch Normalization 和倒置 Dropout 的描述是标准的。这些层之间的相互作用在设计神经网络架构中是一个已知且重要的实践和理论考虑因素。\n-   **良构性：** 问题定义清晰。两个块、操作和分析目标都得到了明确的规定。关于大批次和收敛统计量的假设使得问题在分析上是可解的，并导向一个唯一的结论。\n-   **客观性：** 该问题要求对统计矩进行严谨的定量比较，这是一项客观的任务。\n\n**步骤 3：结论和行动**\n问题陈述是有效的。这是深度学习理论领域一个良构的问题。我将进行形式化的推导。\n\n### 推导\n\n设 $y$ 是块的输入，其 $\\operatorname{E}[y] = \\mu_y$ 且 $\\operatorname{Var}[y] = \\sigma_y^2$。\n设 $m$ 是一个用于 dropout 的伯努利随机变量，$P(m=1) = p$，且与 $y$ 无关。\nBN 层有可学习的参数 $\\gamma$ 和 $\\beta$。为简单起见且不失一般性，我们通常会先分析仿射变换之前的归一化输出（其均值为 $0$，方差为 $1$），然后再考虑 $\\gamma$ 和 $\\beta$ 的影响。因此，训练期间 BN 之后的目标均值和方差分别为 $\\beta$ 和 $\\gamma^2$。\n\n#### A 块分析 (Dropout $\\to$ BN)\n\n设该块为 $y \\xrightarrow{\\text{Dropout}} x \\xrightarrow{\\text{BN}} z$。张量 $x$ 是 BN 层的输入。\n\n**1. 训练模式：**\n-   BN 层的输入是 $x_{\\text{train}} = y \\cdot \\frac{m}{p}$。\n-   我们计算其统计量，根据假设，这些统计量将成为 BN 的运行统计量。\n-   **均值：** $\\operatorname{E}[x_{\\text{train}}] = \\operatorname{E}[y \\cdot \\frac{m}{p}] = \\operatorname{E}[y]\\operatorname{E}[\\frac{m}{p}] = \\mu_y \\cdot \\frac{\\operatorname{E}[m]}{p} = \\mu_y \\cdot \\frac{p}{p} = \\mu_y$。\n    运行均值为 $\\mu_{\\text{run}} = \\mu_y$。\n-   **方差：** 为计算 $\\operatorname{Var}[x_{\\text{train}}]$，我们首先求 $\\operatorname{E}[x_{\\text{train}}^2]$。\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[(y \\frac{m}{p})^2] = \\operatorname{E}[y^2 \\frac{m^2}{p^2}] = \\operatorname{E}[y^2]\\operatorname{E}[\\frac{m^2}{p^2}]$。\n    由于 $m \\in \\{0, 1\\}$，因此 $m^2=m$，所以 $\\operatorname{E}[m^2]=\\operatorname{E}[m]=p$。\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[y^2] \\frac{p}{p^2} = \\frac{1}{p}\\operatorname{E}[y^2] = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2)$。\n    $\\operatorname{Var}[x_{\\text{train}}] = \\operatorname{E}[x_{\\text{train}}^2] - (\\operatorname{E}[x_{\\text{train}}])^2 = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2) - \\mu_y^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1}{p} - 1) = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$。\n    运行方差为 $\\sigma_{\\text{run}}^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$。由于 $1/p > 1$，这个方差被 dropout 噪声“放大”了。\n-   **BN 输出：** 训练期间的 BN 输出 $z_{\\text{train}}$ 将有 $\\operatorname{E}[z_{\\text{train}}] \\approx \\beta$ 和 $\\operatorname{Var}[z_{\\text{train}}] \\approx \\gamma^2$。\n\n**2. 推理模式：**\n-   Dropout 被禁用。BN 层的输入是 $x_{\\text{inf}} = y$。\n-   **BN 输入处的统计量：** $\\operatorname{E}[x_{\\text{inf}}] = \\mu_y$ 和 $\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2$。\n-   **BN 输入处训练与推理的对比：**\n    -   均值匹配：$\\operatorname{E}[x_{\\text{inf}}] = \\operatorname{E}[x_{\\text{train}}] = \\mu_y$。\n    -   方差不匹配：$\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2  \\sigma_{\\text{run}}^2 = \\operatorname{Var}[x_{\\text{train}}]$。BN 存储的运行方差高估了真实的推理时输入方差。\n-   **BN 输出：** BN 层使用存储的运行统计量对 $x_{\\text{inf}}$ 进行归一化：\n    $z_{\\text{inf}} = \\gamma \\frac{x_{\\text{inf}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta$。\n-   **BN 输出的统计量：**\n    -   均值：$\\operatorname{E}[z_{\\text{inf}}] = \\frac{\\gamma}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\\operatorname{E}[y - \\mu_y] + \\beta = \\beta$。输出的均值与训练时一致。\n    -   方差：$\\operatorname{Var}[z_{\\text{inf}}] = \\operatorname{Var}[\\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}] = \\frac{\\gamma^2}{\\sigma_{\\text{run}}^2 + \\epsilon}\\operatorname{Var}[y] = \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_{\\text{run}}^2 + \\epsilon}$。\n    -   由于 $\\sigma_{\\text{run}}^2 > \\sigma_y^2$，我们有 $\\operatorname{Var}[z_{\\text{inf}}]  \\gamma^2 = \\operatorname{Var}[z_{\\text{train}}]$。推理时的输出方差小于训练时。\n    -   为简单起见，如果我们假设输入是中心化的（$\\mu_y=0$），那么 $\\sigma_{\\text{run}}^2 = \\sigma_y^2/p$。输出方差变为 $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_y^2/p} = \\gamma^2 p$。方差收缩了因子 $p$。\n\n**A 块总结：** 发生了训练-推理分布偏移。BN 层使用了来自训练的被放大的方差估计，这导致它错误地归一化推理数据，从而在其输出处产生收缩的方差。\n\n#### B 块分析 (BN $\\to$ Dropout)\n\n设该块为 $y \\xrightarrow{\\text{BN}} x \\xrightarrow{\\text{Dropout}} z$。张量 $x$ 现在是 BN 层的输出。\n\n**1. 训练模式：**\n-   BN 层的输入是 $y$。\n-   它计算并存储来自 $y$ 的统计量。\n-   运行均值：$\\mu_{\\text{run}} = \\operatorname{E}[y] = \\mu_y$。\n-   运行方差：$\\sigma_{\\text{run}}^2 = \\operatorname{Var}[y] = \\sigma_y^2$。\n-   **BN 输出 / Dropout 输入：** $x_{\\text{train}}$ 将有 $\\operatorname{E}[x_{\\text{train}}] \\approx \\beta$ 和 $\\operatorname{Var}[x_{\\text{train}}] \\approx \\gamma^2$。\n-   **Dropout 输出：** 最终输出是 $z_{\\text{train}} = x_{\\text{train}} \\cdot \\frac{m}{p}$。其方差将被 dropout 噪声放大，$\\operatorname{Var}[z_{\\text{train}}] \\approx \\frac{\\gamma^2}{p} + \\beta^2(\\frac{1-p}{p})$。\n\n**2. 推理模式：**\n-   Dropout 被禁用。\n-   **BN 的输入：** 输入仍然是 $y$。其统计量与训练时相比没有变化。因此，在 BN 输入处，没有训练-推理分布偏移。\n-   **BN 输出 / Dropout 输入：** BN 层使用 $\\mu_{\\text{run}} = \\mu_y$ 和 $\\sigma_{\\text{run}}^2 = \\sigma_y^2$ 对 $y$ 进行归一化。\n    $x_{\\text{inf}} = \\gamma \\frac{y - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_y^2 + \\epsilon}} + \\beta$。\n-   **BN 输出的统计量：**\n    -   均值：$\\operatorname{E}[x_{\\text{inf}}] \\approx \\beta$。\n    -   方差：$\\operatorname{Var}[x_{\\text{inf}}] \\approx \\gamma^2$。\n-   **BN 输出处训练与推理的对比：** $x$（BN 输出）的统计量在训练和推理之间匹配：$\\operatorname{E}[x_{\\text{train}}] \\approx \\operatorname{E}[x_{\\text{inf}}]$ 且 $\\operatorname{Var}[x_{\\text{train}}] \\approx \\operatorname{Var}[x_{\\text{inf}}]$。\n-   **Dropout 输出：** Dropout 是一个恒等映射，因此 $z_{\\text{inf}} = x_{\\text{inf}}$。最终输出的 $\\operatorname{E}[z_{\\text{inf}}] \\approx \\beta$ 且 $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\gamma^2$。\n-   训练-推理的差异仅出现在 dropout 层*之后*，其中 $\\operatorname{Var}[z_{\\text{train}}] > \\operatorname{Var}[z_{\\text{inf}}]$。\n\n**B 块总结：** 在 BN 层的输入或输出处没有分布偏移。BN 层在训练和测试中看到相同的数据分布，其存储的统计量是适当的。训练-推理的差异被隔离在 Dropout 层自身的输出处。\n\n### 逐项分析\n\n**A. 在 A 块中，BN 的运行方差被训练期间遇到的掩码噪声所放大，因此在推理时，BN 输入的方差小于运行方差，并且 BN 输出的方差收缩到大约 $p$（小于 $1$），而均值保持在大约 $0$。在 B 块中，BN 的统计量在 BN 输入和 BN 输出处，训练和推理之间都非常匹配，因此它们的均值和方差一致；任何训练-推理方差差异仅出现在 dropout 层之后，在该处，训练方差大约大 $1/p$ 倍。**\n-   对 A 块的描述与我们的分析完全一致。运行方差被噪声放大。这导致 BN 输出方差在推理时收缩大约 $p$ 的因子。均值保持稳定。\n-   对 B 块的描述也完全一致。BN 层的输入和输出统计量在训练和推理之间是稳定的。方差不匹配被隔离在 dropout 层的输出处，其中训练方差被放大了大约 $1/p$ 倍。\n-   **结论：正确。**\n\n**B. 在 A 块中，BN 消除了任何训练-推理不匹配，因此 BN 输入和 BN 输出在训练和推理之间的均值和方差完全匹配。在 B 块中，将 dropout 放在 BN 之后，在推理时，会在 BN 输出处引起一个与 $(1-p)$ 成比例的均值偏移。**\n-   关于 A 块的陈述不正确。BN 没有消除不匹配；它正是由于使用不正确的运行统计量而在其输出处造成不匹配的根源。在 BN 输出处存在方差不匹配。\n-   关于 B 块的陈述不正确。倒置 dropout 保持了均值。没有均值偏移。\n-   **结论：不正确。**\n\n**C. 在 A 块中，BN 之前的掩码操作会使 BN 的运行均值偏向 $0$，因此在推理时，BN 输出存在非零的均值不匹配，而其方差匹配。在 B 块中，BN 之后的掩码操作会使 BN 的运行方差向上偏置，因此在推理时，BN 输出的方差会膨胀 $1/p$ 倍。**\n-   关于 A 块的陈述不正确。倒置 dropout 确保运行均值是无偏估计（$\\operatorname{E}[x_{\\text{train}}] = \\mu_y$）。它还错误地声称方差匹配，而实际上方差是收缩的。\n-   关于 B 块的陈述不正确。掩码操作发生在 BN *之后*，所以它不能使 BN 的运行方差产生偏置。BN 在推理时的输出方差与训练时相匹配，并不会膨胀。\n-   **结论：不正确。**\n\n**D. 使用倒置 dropout 时，位置无关紧要：对于任何 $p$，每一层的期望和方差在训练和推理之间都匹配，因此在两个块中，所有在 BN 输入和输出处测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 都一致。**\n-   这个陈述明显是错误的。我们的分析表明，位置非常重要。A 块在 BN 输出处存在方差不匹配，而 B 块则没有。倒置 dropout 保留了一阶矩（均值）但不保留二阶矩（方差），这是观察到的现象的根本原因。\n-   **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Dropout应该放在网络的早期层还是后期层？这是一个常见的架构设计问题。本练习将通过量化分析dropout引入的噪声如何在多层网络中传播，来深入探讨这个问题。通过推导输出方差与dropout位置的关系，我们将揭示后续层如何放大或衰减这种噪声，从而为在网络中合理放置dropout层提供更具原则性的指导。",
            "id": "3118042",
            "problem": "考虑一个具有两个隐藏层的前馈线性网络。设输入向量为 $x \\in \\mathbb{R}^{d}$，定义第一个隐藏层的预激活为 $h^{(1)} = W_{1} x$，其中 $W_{1} \\in \\mathbb{R}^{h_{1} \\times d}$。定义第二个隐藏层的预激活为 $h^{(2)} = W_{2} h^{(1)}$，其中 $W_{2} \\in \\mathbb{R}^{h_{2} \\times h_{1}}$。标量输出为 $\\hat{y} = v^{\\top} h^{(2)}$，其中 $v \\in \\mathbb{R}^{h_{2}}$。假设输入是独立同分布的，服从零均值、单位协方差的多元高斯分布，即 $x \\sim \\mathcal{N}(0, I_{d})$，并关注对于一个固定的教师函数的均方误差（MSE）预测 $\\mathbb{E}[(\\hat{y} - y)^{2}]$，但仅分析 dropout 噪声对学生网络输出方差的影响。\n\n在训练期间，应用反向 dropout (inverted dropout)，其保留概率为 $q \\in (0,1]$（丢弃概率为 $p = 1 - q$），它将保留的单元按 $1/q$ 进行缩放，以保持预激活的期望值不变。考虑应用 dropout 的两个位置：\n- 早期层 dropout：将 dropout 掩码逐元素应用于 $h^{(1)}$，即 $\\tilde{h}^{(1)} = (m^{(1)}/q) \\odot h^{(1)}$，其中 $m^{(1)} \\in \\{0,1\\}^{h_{1}}$ 具有独立的伯努利$(q)$ 分布的条目。网络的其余部分保持不变，得到 $\\hat{y}_{\\mathrm{early}} = v^{\\top} W_{2} \\tilde{h}^{(1)}$。\n- 最终层 dropout：将 dropout 掩码逐元素应用于 $h^{(2)}$，即 $\\tilde{h}^{(2)} = (m^{(2)}/q) \\odot h^{(2)}$，其中 $m^{(2)} \\in \\{0,1\\}^{h_{2}}$ 具有独立的伯努利$(q)$ 分布的条目。输出为 $\\hat{y}_{\\mathrm{final}} = v^{\\top} \\tilde{h}^{(2)}$。\n\n仅从上述核心定义和关于高斯随机向量的公认事实出发，完成以下任务：\n1. 推导在早期层由 dropout 噪声引起的期望输出方差的表达式，该期望对 dropout 掩码和输入分布进行平均。将此量表示为 $\\mathrm{Var}_{\\mathrm{early}} = \\mathbb{E}_{x}[\\mathrm{Var}_{m^{(1)}}(\\hat{y}_{\\mathrm{early}} \\mid x)]$。\n2. 推导在最终层由 dropout 噪声引起的期望输出方差的表达式，该期望对 dropout 掩码和输入分布进行平均。将此量表示为 $\\mathrm{Var}_{\\mathrm{final}} = \\mathbb{E}_{x}[\\mathrm{Var}_{m^{(2)}}(\\hat{y}_{\\mathrm{final}} \\mid x)]$。\n3. 使用这些表达式定义一个对 dropout 位置的敏感度度量 $S = \\mathrm{Var}_{\\mathrm{early}} / \\mathrm{Var}_{\\mathrm{final}}$，它量化了早期层的 dropout 相对于最终层的 dropout 对输出的扰动程度。如果 $\\mathrm{Var}_{\\mathrm{final}} = 0$，当 $\\mathrm{Var}_{\\mathrm{early}} = 0$ 时定义 $S$ 为 $0$，当 $\\mathrm{Var}_{\\mathrm{early}}  0$ 时定义 $S$ 为 $+\\infty$。\n\n通过信号放大的角度解释敏感度：解释网络权重如何影响 $\\mathrm{Var}_{\\mathrm{early}}$ 和 $\\mathrm{Var}_{\\mathrm{final}}$，进而影响 $S$，阐明为什么某些 dropout 位置会导致更大的输出扰动。\n\n实现一个程序，对于一组给定的参数值测试套件，计算并输出每个案例的三元组 $[\\mathrm{Var}_{\\mathrm{early}}, \\mathrm{Var}_{\\mathrm{final}}, S]$。所有概率都应以小数形式提供（例如，$p = 0.5$），并且本问题中没有物理单位。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试案例，并且本身是一个包含三个浮点数的列表 $[\\mathrm{Var}_{\\mathrm{early}}, \\mathrm{Var}_{\\mathrm{final}}, S]$。\n\n使用以下测试套件：\n- 案例 1 (一般情况): $d = 3$, $h_{1} = 2$, $h_{2} = 2$, $p = 0.2$, $W_{1} = \\begin{bmatrix} 0.8  -0.4  0.3 \\\\ 0.1  0.5  -0.6 \\end{bmatrix}$, $W_{2} = \\begin{bmatrix} 1.2  -0.7 \\\\ 0.5  0.9 \\end{bmatrix}$, $v = \\begin{bmatrix} 0.9 \\\\ -0.3 \\end{bmatrix}$。\n- 案例 2 (边界情况，无 dropout): $W_{1}$、$W_{2}$ 和 $v$ 与案例 1 相同，但 $p = 0.0$。\n- 案例 3 (后续权重中的放大效应): $d = 3$, $h_{1} = 2$, $h_{2} = 2$, $p = 0.5$, $W_{1} = \\begin{bmatrix} 0.4  0.4  0.4 \\\\ 0.3  -0.2  0.1 \\end{bmatrix}$, $W_{2} = \\begin{bmatrix} 3.0  -2.5 \\\\ 2.0  1.5 \\end{bmatrix}$, $v = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。\n- 案例 4 (近乎最大的 dropout 和结构稀疏性): $d = 3$, $h_{1} = 2$, $h_{2} = 2$, $p = 0.9$, $W_{1} = \\begin{bmatrix} 0.9  -0.1  0.2 \\\\ 0.0  0.3  -0.4 \\end{bmatrix}$, $W_{2} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.5  0.0 \\end{bmatrix}$, $v = \\begin{bmatrix} 1.0 \\\\ 0.7 \\end{bmatrix}$。\n\n你的程序必须使用推导出的公式计算 $\\mathrm{Var}_{\\mathrm{early}}$、$\\mathrm{Var}_{\\mathrm{final}}$ 和 $S$，并输出一行包含 4 个结果三元组的列表，格式完全如下：$[[\\mathrm{Var}_{\\mathrm{early}}^{(1)},\\mathrm{Var}_{\\mathrm{final}}^{(1)},S^{(1)}],[\\mathrm{Var}_{\\mathrm{early}}^{(2)},\\mathrm{Var}_{\\mathrm{final}}^{(2)},S^{(2)}],[\\mathrm{Var}_{\\mathrm{early}}^{(3)},\\mathrm{Var}_{\\mathrm{final}}^{(3)},S^{(3)}],[\\mathrm{Var}_{\\mathrm{early}}^{(4)},\\mathrm{Var}_{\\mathrm{final}}^{(4)},S^{(4)}]]$。",
            "solution": "我们从网络定义开始。各层是线性的：$h^{(1)} = W_{1} x$，$h^{(2)} = W_{2} h^{(1)}$，输出为 $\\hat{y} = v^{\\top} h^{(2)}$。倒置 dropout 掩码被逐元素应用，保留概率为 $q = 1 - p$，并对保留的单元进行 $1/q$ 的缩放。\n\n对于早期层 dropout，掩码 $m^{(1)} \\in \\{0,1\\}^{h_{1}}$ 应用于 $h^{(1)}$ 以产生 $\\tilde{h}^{(1)} = (m^{(1)}/q) \\odot h^{(1)}$。输出变为\n$$\n\\hat{y}_{\\mathrm{early}} = v^{\\top} W_{2} \\tilde{h}^{(1)} = v^{\\top} W_{2}\\left(\\frac{m^{(1)}}{q} \\odot h^{(1)}\\right).\n$$\n定义 $a = W_{2}^{\\top} v \\in \\mathbb{R}^{h_{1}}$，因此\n$$\n\\hat{y}_{\\mathrm{early}} = a^{\\top}\\left(\\frac{m^{(1)}}{q} \\odot h^{(1)}\\right) = \\sum_{i=1}^{h_{1}} a_{i}\\left(\\frac{m_{i}^{(1)}}{q}\\right) h^{(1)}_{i}.\n$$\n在以 $x$ 为条件时，随机变量 $\\{m_{i}^{(1)}\\}$ 是独立的，且 $\\mathbb{E}[m_{i}^{(1)}] = q$，$\\mathrm{Var}(m_{i}^{(1)}/q) = \\frac{q(1-q)}{q^{2}} = \\frac{1-q}{q} = \\frac{p}{q}$。因此，在以 $x$ 为条件时，\n$$\n\\mathrm{Var}\\left(\\hat{y}_{\\mathrm{early}} \\mid x\\right) = \\sum_{i=1}^{h_{1}} a_{i}^{2} \\left(h^{(1)}_{i}\\right)^{2} \\cdot \\frac{p}{q}.\n$$\n我们对高斯输入分布 $x \\sim \\mathcal{N}(0, I_{d})$ 取期望。对于线性映射 $h^{(1)} = W_{1} x$，第 $i$ 个分量满足 $h^{(1)}_{i} = w_{1,i}^{\\top} x$，其中 $w_{1,i}^{\\top}$ 是 $W_{1}$ 的第 $i$ 行。对于零均值高斯向量，一个标准的事实是\n$$\n\\mathbb{E}\\left[\\left(w^{\\top} x\\right)^{2}\\right] = \\|w\\|_{2}^{2}.\n$$\n因此，\n$$\n\\mathbb{E}_{x}\\left[\\left(h^{(1)}_{i}\\right)^{2}\\right] = \\|w_{1,i}\\|_{2}^{2},\n$$\n由此可得\n$$\n\\mathrm{Var}_{\\mathrm{early}} = \\mathbb{E}_{x}\\left[\\mathrm{Var}\\left(\\hat{y}_{\\mathrm{early}} \\mid x\\right)\\right] = \\frac{p}{q} \\sum_{i=1}^{h_{1}} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}.\n$$\n这可以紧凑地写成 Frobenius 范数的平方，\n$$\n\\mathrm{Var}_{\\mathrm{early}} = \\frac{p}{q} \\left\\| \\mathrm{diag}(a) W_{1} \\right\\|_{F}^{2},\n$$\n因为 $\\left\\| \\mathrm{diag}(a) W_{1} \\right\\|_{F}^{2} = \\sum_{i=1}^{h_{1}} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}$。\n\n对于最终层 dropout，掩码 $m^{(2)} \\in \\{0,1\\}^{h_{2}}$ 应用于 $h^{(2)}$ 以产生 $\\tilde{h}^{(2)} = (m^{(2)}/q) \\odot h^{(2)}$，得到\n$$\n\\hat{y}_{\\mathrm{final}} = v^{\\top} \\tilde{h}^{(2)} = \\sum_{j=1}^{h_{2}} v_{j}\\left(\\frac{m_{j}^{(2)}}{q}\\right) h^{(2)}_{j}.\n$$\n在以 $x$ 为条件时，方差在 $h_{2}$ 个分量上独立累积：\n$$\n\\mathrm{Var}\\left(\\hat{y}_{\\mathrm{final}} \\mid x\\right) = \\sum_{j=1}^{h_{2}} v_{j}^{2} \\left(h^{(2)}_{j}\\right)^{2} \\cdot \\frac{p}{q}.\n$$\n我们现在需要 $\\mathbb{E}_{x}\\left[\\left(h^{(2)}_{j}\\right)^{2}\\right]$。注意 $h^{(2)} = W_{2} h^{(1)}$ 且 $h^{(1)} = W_{1} x$。将第 $j$ 个分量写为 $h^{(2)}_{j} = b_{j}^{\\top} h^{(1)}$，其中 $b_{j}^{\\top}$ 是 $W_{2}$ 的第 $j$ 行。随机向量 $h^{(1)} = W_{1} x$ 是零均值高斯分布，其协方差为 $W_{1} W_{1}^{\\top}$。对于一个协方差为 $\\Sigma$ 的零均值高斯向量 $\\xi$，$b^{\\top} \\xi$ 的方差是 $b^{\\top} \\Sigma b$。因此，\n$$\n\\mathbb{E}_{x}\\left[\\left(h^{(2)}_{j}\\right)^{2}\\right] = b_{j}^{\\top} W_{1} W_{1}^{\\top} b_{j} = \\left\\| W_{1}^{\\top} b_{j} \\right\\|_{2}^{2}.\n$$\n于是，\n$$\n\\mathrm{Var}_{\\mathrm{final}} = \\frac{p}{q} \\sum_{j=1}^{h_{2}} v_{j}^{2} \\left\\| W_{1}^{\\top} b_{j} \\right\\|_{2}^{2}.\n$$\n这也可以表示为紧凑的 Frobenius 范数形式。令 $D_{v} = \\mathrm{diag}(v)$。观察到\n$$\n\\sum_{j=1}^{h_{2}} v_{j}^{2} \\left\\| W_{1}^{\\top} b_{j} \\right\\|_{2}^{2} = \\left\\| W_{1}^{\\top} W_{2}^{\\top} D_{v} \\right\\|_{F}^{2} = \\left\\| D_{v} W_{2} W_{1} \\right\\|_{F}^{2}.\n$$\n因此，\n$$\n\\mathrm{Var}_{\\mathrm{final}} = \\frac{p}{q} \\left\\| \\mathrm{diag}(v) \\, W_{2} \\, W_{1} \\right\\|_{F}^{2}.\n$$\n\n敏感度度量定义为\n$$\nS = \\frac{\\mathrm{Var}_{\\mathrm{early}}}{\\mathrm{Var}_{\\mathrm{final}}}.\n$$\n按照惯例，如果 $\\mathrm{Var}_{\\mathrm{final}} = 0$ 且 $\\mathrm{Var}_{\\mathrm{early}} = 0$，则 $S = 0$；如果 $\\mathrm{Var}_{\\mathrm{final}} = 0$ 且 $\\mathrm{Var}_{\\mathrm{early}} > 0$，则 $S = +\\infty$。\n\n通过信号放大的角度进行解释：$\\mathrm{Var}_{\\mathrm{early}}$ 和 $\\mathrm{Var}_{\\mathrm{final}}$ 都与因子 $\\frac{p}{q}$ 成比例，这反映了更高的丢弃概率 $p$ 会使输出方差与掩码方差成比例增加，而较小的 $q$ 则由于倒置缩放而产生放大效应。通过权重的放大效应因位置而异：\n- 早期层 dropout 噪声在贡献到输出之前被乘以 $a = W_{2}^{\\top} v$。这得到 $\\mathrm{Var}_{\\mathrm{early}} \\propto \\sum_{i} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}$，表明 $h^{(1)}$ 上的噪声被下游进入第二层和读出层的“扇出”权重所放大。$W_{2}^{\\top} v$ 中的较大条目和 $W_{1}$ 的较大行范数会增加此方差。\n- 最终层 dropout 噪声直接应用于 $h^{(2)}$ 并由 $v$ 加权，得到 $\\mathrm{Var}_{\\mathrm{final}} \\propto \\left\\| \\mathrm{diag}(v) W_{2} W_{1} \\right\\|_{F}^{2}$。这汇集了从 $x$到 $h^{(2)}$ 每个分量（由 $v$ 加权）的整个前向路径上的放大效应。$W_{2}$ 和复合矩阵 $W_{2} W_{1}$ 的较大行范数，以及 $v$ 中的较大条目，会增加此方差。\n因此，$S$ 由 $W_{2}^{\\top} v$ 与 $W_{1}$ 行范数的相对对齐和大小，与 $\\mathrm{diag}(v) W_{2} W_{1}$ 中组合路径范数的对比所驱动。如果 $W_{2}$ 有与 $v$ 对齐的大列（使得 $a$ 很大），则早期层 dropout 噪声被强烈放大，从而增加 $S$。如果由 $v$ 加权的复合矩阵 $W_{2} W_{1}$ 具有大的 Frobenius 范数，则最终层 dropout 会导致更大的方差，从而减小 $S$。\n\n算法计算：\n1. 计算 $q = 1 - p$ 和因子 $r = \\frac{p}{q}$。\n2. 计算 $a = W_{2}^{\\top} v$。\n3. 计算 $W_{1}$ 的行范数的平方，即对每个 $i$ 计算 $\\|w_{1,i}\\|_{2}^{2}$。\n4. 计算 $\\mathrm{Var}_{\\mathrm{early}} = r \\sum_{i} a_{i}^{2} \\|w_{1,i}\\|_{2}^{2}$。\n5. 计算 $M = \\mathrm{diag}(v) W_{2} W_{1}$ 和 $\\mathrm{Var}_{\\mathrm{final}} = r \\| M \\|_{F}^{2}$。\n6. 使用上述惯例计算 $S$。\n7. 按照规定为每个测试案例输出 $[\\mathrm{Var}_{\\mathrm{early}}, \\mathrm{Var}_{\\mathrm{final}}, S]$。\n该测试套件涵盖了一个具有中等权重的一般情况，一个 $p=0$ 产生零方差的边界情况，一个具有较大后层权重以显示放大效应的情况，以及一个 $W_{2}$ 中具有结构稀疏性的近乎最大 dropout 的情况，突出了某些路径如何根据位置不同地关闭放大效应。",
            "answer": "```python\nimport numpy as np\n\ndef var_early(W1: np.ndarray, W2: np.ndarray, v: np.ndarray, p: float) - float:\n    \"\"\"\n    Compute Var_early = (p/q) * sum_i (a_i^2 * ||w1_i||^2), where a = W2^T v.\n    \"\"\"\n    if p == 0:\n        return 0.0\n    q = 1.0 - p\n    if q == 0:\n        return float('inf')\n    a = W2.T @ v  # shape (h1,)\n    row_norm_sq = np.sum(W1**2, axis=1)  # shape (h1,)\n    return (p / q) * float(np.sum((a**2) * row_norm_sq))\n\ndef var_final(W1: np.ndarray, W2: np.ndarray, v: np.ndarray, p: float) - float:\n    \"\"\"\n    Compute Var_final = (p/q) * ||diag(v) @ W2 @ W1||_F^2.\n    \"\"\"\n    if p == 0:\n        return 0.0\n    q = 1.0 - p\n    if q == 0:\n        return float('inf')\n    # Use broadcasting for efficiency instead of creating full diagonal matrix\n    # Dv_W2 = v[:, np.newaxis] * W2\n    # M = Dv_W2 @ W1\n    Dv = np.diag(v)\n    M = Dv @ W2 @ W1\n    frob_sq = float(np.sum(M**2))\n    return (p / q) * frob_sq\n\ndef sensitivity(W1: np.ndarray, W2: np.ndarray, v: np.ndarray, p: float):\n    ve = var_early(W1, W2, v, p)\n    vf = var_final(W1, W2, v, p)\n    # Sensitivity S = ve / vf with conventions.\n    if not np.isfinite(vf):\n        S = 0.0\n    elif vf == 0.0:\n        if ve == 0.0:\n            S = 0.0\n        else:\n            S = float('inf')\n    else:\n        S = ve / vf\n    return [ve, vf, S]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"W1\": np.array([[0.8, -0.4, 0.3],\n                            [0.1,  0.5, -0.6]], dtype=float),\n            \"W2\": np.array([[1.2, -0.7],\n                            [0.5,  0.9]], dtype=float),\n            \"v\": np.array([0.9, -0.3], dtype=float),\n            \"p\": 0.2\n        },\n        # Case 2 (boundary p=0.0)\n        {\n            \"W1\": np.array([[0.8, -0.4, 0.3],\n                            [0.1,  0.5, -0.6]], dtype=float),\n            \"W2\": np.array([[1.2, -0.7],\n                            [0.5,  0.9]], dtype=float),\n            \"v\": np.array([0.9, -0.3], dtype=float),\n            \"p\": 0.0\n        },\n        # Case 3 (amplification in later weights)\n        {\n            \"W1\": np.array([[0.4, 0.4, 0.4],\n                            [0.3, -0.2, 0.1]], dtype=float),\n            \"W2\": np.array([[3.0, -2.5],\n                            [2.0,  1.5]], dtype=float),\n            \"v\": np.array([1.0, 1.0], dtype=float),\n            \"p\": 0.5\n        },\n        # Case 4 (near-maximal dropout and structural sparsity)\n        {\n            \"W1\": np.array([[0.9, -0.1, 0.2],\n                            [0.0,  0.3, -0.4]], dtype=float),\n            \"W2\": np.array([[1.0, 0.0],\n                            [0.5, 0.0]], dtype=float),\n            \"v\": np.array([1.0, 0.7], dtype=float),\n            \"p\": 0.9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        W1 = case[\"W1\"]\n        W2 = case[\"W2\"]\n        v = case[\"v\"]\n        p = case[\"p\"]\n        result = sensitivity(W1, W2, v, p)\n        # Round to a reasonable number of decimal places for consistent output\n        results.append([round(r, 6) for r in result])\n\n    # Final print statement in the exact required format: a single line list of 4 triples.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}