{
    "hands_on_practices": [
        {
            "introduction": "Moving from theory to practice, our first exercise grounds the concept of Stochastic Gradient Descent in a concrete calculation. We will perform a single update step for a simple quadratic cost function. This practice is designed to demystify the core mechanism of SGD: using the gradient from just one data point to efficiently navigate towards a minimum, forming the fundamental building block of the entire optimization process .",
            "id": "2206637",
            "problem": "An iterative optimization algorithm is used to find a parameter $x$ that minimizes a cost function. The total cost function is an average of several component functions: $F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$. In this specific case, the component functions are quadratic and given by $f_i(x) = (x - c_i)^2$, where the constants are $c_i = i$ for $i = 1, 2, \\dots, 10$, and thus $N=10$.\n\nThe optimization process starts with an initial guess for the parameter, $x_0$. At each step, a new estimate, $x_{k+1}$, is calculated from the current estimate, $x_k$, by using only a single, randomly chosen component function, $f_j(x)$. The update rule is defined as:\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\nwhere $\\eta$ is a constant known as the learning rate.\n\nGiven an initial parameter value of $x_0 = 10.0$ and a learning rate of $\\eta = 0.1$, compute the value of the parameter $x_1$ after one update step. For this first step, the component function used is $f_j(x)$ with the index $j=5$.",
            "solution": "We are given component functions of the form $f_{i}(x) = (x - c_{i})^{2}$ with $c_{i} = i$. For the first update, the chosen index is $j=5$, so $f_{5}(x) = (x - 5)^{2}$.\n\nThe update rule is\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\nUsing the power rule and chain rule, the derivative of the chosen component is\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\nEvaluating at the current iterate $x_{0} = 10$ gives\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\nWith learning rate $\\eta = 0.1$, the update becomes\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\nThus, after one update step using $f_{5}$, the parameter value is $x_{1} = 9$.",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "Having mastered the single update step, we now turn to a crucial hyperparameter: the learning rate, $\\eta$. This value dictates the size of each step we take and is paramount for successful convergence. This exercise provides a hands-on demonstration of what can go wrong when the learning rate is too large, causing the algorithm to overshoot the minimum and diverge, a common pitfall in training machine learning models .",
            "id": "2206673",
            "problem": "In a machine learning context, we often optimize a model's parameters by minimizing a loss function. Consider a simplified model with a single scalar parameter, $w$. The loss function associated with a single data point is given by $L(w) = \\frac{1}{2} c w^2$, where the minimum loss occurs at $w=0$. The parameter is updated using the Stochastic Gradient Descent (SGD) algorithm. The update rule for the parameter at step $k$ is given by $w_{k+1} = w_k - \\eta \\nabla L(w_k)$, where $\\nabla L(w_k)$ is the gradient of the loss function evaluated at $w_k$, and $\\eta$ is the learning rate.\n\nSuppose the initial value of the parameter is $w_0 = 4.0$. The model parameters are set as $c = 0.75$ and the learning rate is $\\eta = 3.2$. Calculate the value of the parameter $w$ after 3 update steps (i.e., find $w_3$).\n\nRound your final answer to three significant figures.",
            "solution": "The loss is $L(w)=\\frac{1}{2} c w^{2}$. Its gradient is obtained by differentiation:\n$$\n\\nabla L(w)=\\frac{\\mathrm{d}}{\\mathrm{d}w}\\left(\\frac{1}{2} c w^{2}\\right)=c w.\n$$\nThe SGD update rule is\n$$\nw_{k+1}=w_{k}-\\eta \\nabla L(w_{k})=w_{k}-\\eta c w_{k}=(1-\\eta c)\\,w_{k}.\n$$\nThis linear recurrence solves to\n$$\nw_{k}=(1-\\eta c)^{k} w_{0}.\n$$\nSubstituting $c=0.75$, $\\eta=3.2$, and $w_{0}=4.0$,\n$$\n1-\\eta c=1-(3.2)(0.75)=1-2.4=-1.4,\n$$\nso\n$$\nw_{3}=(-1.4)^{3}\\cdot 4.0=-10.976.\n$$\nRounding to three significant figures gives $-11.0$.",
            "answer": "$$\\boxed{-11.0}$$"
        },
        {
            "introduction": "Gradient-based methods are powerful, but they are not universally applicable. This final practice shifts our focus from the mechanics of SGD to the properties of the problem itself, specifically the loss function. Through a thought experiment involving a discontinuous step function, we will explore a scenario where SGD fails because the gradient is zero almost everywhere, providing no useful direction for updates. This highlights the critical importance of choosing a loss function that is suitable for gradient-based optimization .",
            "id": "2206644",
            "problem": "An engineer is developing a real-time monitoring system for a critical process in a factory. The system uses a single sensor reading, denoted by $x$, to predict a key performance indicator, $y$. For this task, the engineer selects a simple linear model without a bias term: $y_{\\text{pred}} = w \\cdot x$, where $w$ is the single model parameter to be learned.\n\nFor this specific application, an error is only considered significant if its magnitude, $|y_{\\text{true}} - y_{\\text{pred}}|$, exceeds a predefined tolerance threshold, $\\epsilon > 0$. Any prediction error within this tolerance is deemed acceptable. To formalize this, the engineer designs a custom loss function, $L(w)$, for a single data point $(x, y_{\\text{true}})$ as follows:\n$$\nL(w) = \\begin{cases} 0  \\text{if } |y_{\\text{true}} - w \\cdot x| \\le \\epsilon \\\\ 1  \\text{if } |y_{\\text{true}} - w \\cdot x| > \\epsilon \\end{cases}\n$$\nThe engineer attempts to find the optimal value of $w$ by training the model on a large dataset of $(x, y_{\\text{true}})$ pairs using the Stochastic Gradient Descent (SGD) algorithm. After running the training process, they observe that the parameter $w$ barely changes from its initial random value, regardless of the learning rate used.\n\nWhich of the following statements provides the most accurate and fundamental explanation for this failure of the training process?\n\nA. The step-function loss is non-convex, meaning that Stochastic Gradient Descent (SGD) is likely to get trapped in a local minimum that is not the global minimum.\n\nB. For almost any given value of the parameter $w$, the gradient of the loss function with respect to $w$ is zero, meaning the SGD update step does not change the parameter's value.\n\nC. The loss function is discontinuous, and the infinite gradients at the points of discontinuity cause numerical overflow and unstable updates in the SGD algorithm.\n\nD. The stochasticity of SGD introduces too much noise when using a binary (0/1) loss, preventing the parameter $w$ from converging to a stable value.",
            "solution": "We consider the per-sample loss as a function of the single parameter $w$:\n$$\nL(w)=\\begin{cases}\n0  \\text{if } |y_{\\text{true}}-wx|\\le \\epsilon \\\\\n1  \\text{if } |y_{\\text{true}}-wx|\\epsilon\n\\end{cases}.\n$$\nDefine the error $e(w)=y_{\\text{true}}-wx$. The loss is a step function of $w$ that switches value at the points where $|e(w)|=\\epsilon$. For $x\\neq 0$, the condition $|y_{\\text{true}}-wx|\\le \\epsilon$ defines a closed interval in $w$:\n$$\n|y_{\\text{true}}-wx|\\le \\epsilon \\iff -\\epsilon \\le y_{\\text{true}}-wx \\le \\epsilon.\n$$\nIf $x0$, this gives\n$$\n\\frac{y_{\\text{true}}-\\epsilon}{x} \\le w \\le \\frac{y_{\\text{true}}+\\epsilon}{x},\n$$\nand if $x0$, the inequalities reverse but the feasible set is still the closed interval with endpoints $\\frac{y_{\\text{true}}-\\epsilon}{x}$ and $\\frac{y_{\\text{true}}+\\epsilon}{x}$. Outside this interval, $L(w)=1$; inside, $L(w)=0$. Therefore $L(w)$ is piecewise constant with jumps only at the two boundary points\n$$\nw=\\frac{y_{\\text{true}}-\\epsilon}{x} \\quad \\text{and} \\quad w=\\frac{y_{\\text{true}}+\\epsilon}{x}.\n$$\nFor $x=0$, we have $e(w)=y_{\\text{true}}$ independent of $w$, so $L(w)$ is constant for all $w$ and thus flat everywhere.\n\nDifferentiability: On any open interval that does not include a boundary point, $L(w)$ is constant, so its derivative is\n$$\n\\frac{dL}{dw}=0 \\quad \\text{for all } w \\notin S,\n$$\nwhere $S=\\{w:\\,|y_{\\text{true}}-wx|=\\epsilon\\}$. At $w\\in S$, $L(w)$ is discontinuous and not differentiable; there is no finite derivative, nor a well-defined subgradient for this $0/1$ step loss. Since $S$ is a finite set for a single sample (and a measure-zero set in $\\mathbb{R}$), the gradient with respect to $w$ is zero almost everywhere.\n\nIn stochastic gradient descent using any minibatch or single sample, the update is\n$$\nw_{t+1}=w_{t}-\\eta \\frac{\\partial L}{\\partial w}(w_{t}),\n$$\nbut $\\frac{\\partial L}{\\partial w}(w_{t})=0$ for all $w_{t}\\notin S$. Because hitting exactly $w_{t}\\in S$ has probability zero under continuous updates and floating-point arithmetic, the algorithm almost never encounters a non-differentiable point and thus almost always computes a zero gradient. Consequently, $w_{t+1}=w_{t}$ for essentially all steps, so the parameter barely changes regardless of learning rate.\n\nThis establishes that the fundamental reason training fails is that the gradient is zero almost everywhere. Option A mentions non-convexity, which is true, but non-convexity alone does not force zero updates; the decisive issue here is the vanishing gradient. Option C incorrectly attributes the issue to infinite gradients at discontinuities; the derivative is undefined at the jump points and these points are rarely, if ever, encountered in practice. Option D blames stochastic noise, but the lack of parameter change is due to zero gradients rather than noise. Therefore, the most accurate and fundamental explanation is that the gradient is zero for almost all $w$, preventing SGD from updating the parameter, which corresponds to option B.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}