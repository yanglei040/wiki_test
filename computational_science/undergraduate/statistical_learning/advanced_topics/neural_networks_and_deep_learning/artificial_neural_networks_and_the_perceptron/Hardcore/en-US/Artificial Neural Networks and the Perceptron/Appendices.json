{
    "hands_on_practices": [
        {
            "introduction": "While the standard perceptron algorithm guarantees convergence for linearly separable data, its path to a solution can be inefficient, especially on datasets with a small margin between classes. This exercise introduces the concept of momentum, a technique that helps accelerate learning by accumulating a velocity in consistent directions of the error surface. By implementing a perceptron with momentum, you will gain practical experience with a foundational optimization method used in modern deep learning and directly observe its effects on convergence speed and stability .",
            "id": "3099401",
            "problem": "Consider binary classification in a Euclidean space where inputs are vectors $x \\in \\mathbb{R}^d$ and labels are $y \\in \\{-1,+1\\}$. A linear classifier with bias uses an augmented weight vector $w \\in \\mathbb{R}^{d+1}$ and augmented inputs $\\tilde{x} \\in \\mathbb{R}^{d+1}$ defined by $\\tilde{x} = [x; 1]$, and predicts $f(\\tilde{x}) = \\mathrm{sign}(w^\\top \\tilde{x})$. The perceptron update is a mistake-driven rule that modifies $w$ whenever a data point is misclassified. In this task, you will implement a perceptron with momentum (a heavy-ball type update) on borderline-separable datasets to explore convergence speed and overshooting.\n\nFundamental base for this task:\n- Linear separability: A dataset $\\{(x_i,y_i)\\}_{i=1}^n$ is linearly separable if there exists $w^\\star$ such that $y_i (w^{\\star\\top} \\tilde{x}_i) > 0$ for all $i$. The perceptron learning principle updates $w$ when $y_i (w^\\top \\tilde{x}_i) \\le 0$.\n- Mistake-driven updates: On a misclassified example $(x_t,y_t)$, the classical perceptron performs $w_{t+1} = w_t + \\eta y_t \\tilde{x}_t$ for learning rate $\\eta > 0$.\n- Momentum (heavy-ball): A velocity vector $v_t$ is maintained with $v_{t+1} = \\beta v_t + \\eta y_t \\tilde{x}_t$ for momentum coefficient $\\beta \\in [0,1)$ on misclassification, and $v_{t+1} = \\beta v_t$ otherwise; the weight update is $w_{t+1} = w_t + v_{t+1}$.\n\nYour program must:\n1. Implement a single-pass, cyclic, mistake-driven perceptron with momentum. At iteration $t$, let $i = t \\bmod n$ index the next data point $(x_i,y_i)$. Compute the margin $m_t = y_i (w_t^\\top \\tilde{x}_i)$. If $m_t \\le 0$, perform the momentum update $v_{t+1} = \\beta v_t + \\eta y_i \\tilde{x}_i$, increment the mistake counter by $1$, and set $w_{t+1} = w_t + v_{t+1}$. If $m_t > 0$, perform $v_{t+1} = \\beta v_t$ and $w_{t+1} = w_t + v_{t+1}$. Initialize $w_0 = 0$ and $v_0 = 0$. Use an iteration cap $T_{\\max}$ to stop if convergence is not achieved.\n2. Convergence criterion: declare convergence as soon as $y_j (w^\\top \\tilde{x}_j) > 0$ for all $j \\in \\{1,\\dots,n\\}$.\n3. Overshooting quantification: let $M(w)$ be the total number of misclassified points under $w$, namely $M(w) = \\sum_{i=1}^n \\mathbf{1}\\{y_i (w^\\top \\tilde{x}_i) \\le 0\\}$. For each mistake-driven update (that is, each iteration with $m_t \\le 0$), compute $M(w_t)$ immediately before updating, and $M(w_{t+1})$ immediately after the update. Count an overshoot event if $M(w_{t+1}) > M(w_t)$. Define the overshoot ratio as the total overshoot events divided by the total number of mistake-driven updates. If there are zero mistake-driven updates, define the overshoot ratio as $0$.\n4. Convergence speed measure: report the total number of mistake-driven updates performed until convergence (or until the iteration cap, if no convergence).\n\nTest suite. Use the following fixed datasets and hyperparameters:\n- Dataset $\\mathcal{D}_1$ (borderline-separable in $\\mathbb{R}^2$): positives at $(1.0, 1.05)$, $(2.0, 2.05)$, $(3.0, 3.05)$ and negatives at $(1.0, 0.95)$, $(2.0, 1.95)$, $(3.0, 2.95)$. Labels are $+1$ for positives and $-1$ for negatives.\n- Dataset $\\mathcal{D}_2$ (extremely small margin): positives at $(0.0, 0.001)$, $(1.0, 1.001)$, $(2.0, 2.001)$ and negatives at $(0.0, -0.001)$, $(1.0, 0.999)$, $(2.0, 1.999)$, with labels $+1$ and $-1$ respectively.\n- Dataset $\\mathcal{D}_3$ (non-separable edge case): positives at $(0.0, 0.0)$, $(1.0, 1.0)$ and negatives at $(0.0, 0.0)$, $(1.0, 1.0)$, with labels $+1$ and $-1$ respectively.\n\nConstruct the following four test cases:\n- Test case $1$: dataset $\\mathcal{D}_1$, learning rate $\\eta = 0.1$, momentum $\\beta = 0.0$, iteration cap $T_{\\max} = 5000$.\n- Test case $2$: dataset $\\mathcal{D}_1$, learning rate $\\eta = 0.1$, momentum $\\beta = 0.9$, iteration cap $T_{\\max} = 5000$.\n- Test case $3$: dataset $\\mathcal{D}_2$, learning rate $\\eta = 0.05$, momentum $\\beta = 0.95$, iteration cap $T_{\\max} = 8000$.\n- Test case $4$: dataset $\\mathcal{D}_3$, learning rate $\\eta = 0.1$, momentum $\\beta = 0.9$, iteration cap $T_{\\max} = 2000$.\n\nRequired outputs for each test case, in order:\n- The integer number of mistake-driven updates until convergence (or until the iteration cap if not converged).\n- The overshoot ratio as a float rounded to $6$ decimals.\n- A boolean indicating whether convergence was achieved.\n\nFinal output format:\nYour program should produce a single line of output containing the aggregated results for the four test cases as a comma-separated list enclosed in square brackets, where each element is the list $[\\text{steps}, \\text{overshoot\\_ratio}, \\text{converged}]$ for one test case. For example, an output of the form $[[s_1, r_1, c_1],[s_2, r_2, c_2],[s_3, r_3, c_3],[s_4, r_4, c_4]]$, with the overshoot ratios rounded to $6$ decimals.",
            "solution": "We begin with the perceptron classification model in augmented form. For an input $x \\in \\mathbb{R}^d$, we append a constant bias coordinate to obtain $\\tilde{x} = [x; 1] \\in \\mathbb{R}^{d+1}$. A linear classifier parameterized by $w \\in \\mathbb{R}^{d+1}$ predicts $f(\\tilde{x}) = \\mathrm{sign}(w^\\top \\tilde{x})$. The margin of $(\\tilde{x},y)$ under $w$ is $m = y (w^\\top \\tilde{x})$. Correct classification corresponds to $m > 0$.\n\nThe perceptron learning principle is mistake-driven: whenever a point $(\\tilde{x}_i, y_i)$ is misclassified or lies on the decision boundary, $y_i (w^\\top \\tilde{x}_i) \\le 0$, the parameters are updated in the direction of $y_i \\tilde{x}_i$. The classical update is $w_{t+1} = w_t + \\eta y_t \\tilde{x}_t$ for learning rate $\\eta > 0$. To incorporate momentum, we maintain a velocity vector $v_t \\in \\mathbb{R}^{d+1}$ that exponentially averages past updates. The heavy-ball style update is defined by\n$$\nv_{t+1} =\n\\begin{cases}\n\\beta v_t + \\eta y_t \\tilde{x}_t & \\text{if } y_t (w_t^\\top \\tilde{x}_t) \\le 0,\\\\\n\\beta v_t & \\text{if } y_t (w_t^\\top \\tilde{x}_t) > 0,\n\\end{cases}\n\\qquad\nw_{t+1} = w_t + v_{t+1},\n$$\nwith $\\beta \\in [0,1)$ the momentum coefficient. We initialize $w_0 = 0$ and $v_0 = 0$. Iterations cycle deterministically through the dataset: at iteration $t$, the index is $i = t \\bmod n$ over $n$ points. This is a deterministic cyclic schedule aligned with mistake-driven logic to test convergence behavior.\n\nConvergence is detected when the current parameters $w$ correctly classify all training points, that is, when $y_j (w^\\top \\tilde{x}_j) > 0$ for all $j \\in \\{1,\\dots,n\\}$. We quantify two aspects:\n1. Convergence speed: the total number of mistake-driven updates taken until convergence. Formally, count the iterations where $y_t (w_t^\\top \\tilde{x}_t) \\le 0$ and an update uses $\\eta y_t \\tilde{x}_t$; denote this count by an integer $S$.\n2. Overshooting: momentum can cause updates that deteriorate the immediate classification performance, especially on borderline-separable datasets with small margins. We define $M(w) = \\sum_{i=1}^n \\mathbf{1}\\{y_i (w^\\top \\tilde{x}_i) \\le 0\\}$ as the misclassification count. On each mistake-driven update, compute $M(w_t)$ before updating and $M(w_{t+1})$ after. If $M(w_{t+1}) > M(w_t)$, count an overshoot event. The overshoot ratio is the number of overshoot events divided by $S$, and is defined to be $0$ if $S = 0$.\n\nThe algorithm proceeds as follows:\n- Initialize $w_0 = 0$, $v_0 = 0$, mistake count $S = 0$, overshoot count $O = 0$.\n- For $t = 0,1,2,\\dots$ until an iteration cap $T_{\\max}$:\n  - Set $i = t \\bmod n$ and compute $m_t = y_i (w_t^\\top \\tilde{x}_i)$.\n  - Compute the current misclassification count $M(w_t)$.\n  - If $m_t \\le 0$, update $v_{t+1} = \\beta v_t + \\eta y_i \\tilde{x}_i$, then $w_{t+1} = w_t + v_{t+1}$, increment $S$. Compute $M(w_{t+1})$; if $M(w_{t+1}) > M(w_t)$, increment $O$.\n  - If $m_t > 0$, update $v_{t+1} = \\beta v_t$ and $w_{t+1} = w_t + v_{t+1}$.\n  - After the update, check convergence: if $M(w_{t+1}) = 0$, stop.\n- The outputs are $S$, overshoot ratio $O/S$ rounded to $6$ decimals (or $0$ if $S = 0$), and a boolean indicating convergence.\n\nWhy these definitions are grounded:\n- The margin $y (w^\\top \\tilde{x})$ directly encodes correct classification and is the foundational quantity in linear classification under the perceptron principle.\n- The mistake-driven rule follows the well-tested perceptron algorithm, a cornerstone in statistical learning for linearly separable data.\n- Momentum with coefficient $\\beta$ averages past updates to accelerate movement along consistent directions, potentially reducing the number of updates to reach a separating hyperplane on borderline-separable data, but it can overshoot due to inertia, temporarily increasing $M(w)$.\n\nThe test suite explores:\n- A typical borderline-separable dataset $\\mathcal{D}_1$ with small offset between classes, comparing no momentum ($\\beta = 0$) and strong momentum ($\\beta = 0.9$).\n- An extremely small margin dataset $\\mathcal{D}_2$ with strong momentum ($\\beta = 0.95$) to highlight overshooting tendencies.\n- A non-separable dataset $\\mathcal{D}_3$ as a boundary condition to verify termination by iteration cap with no convergence.\n\nThe final program implements exactly this algorithm, computes the required metrics for each test case, rounds overshoot ratios to $6$ decimals, and prints the aggregated results in the specified single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef misclassification_count(w, Xb, y):\n    # y * (Xb @ w) <= 0 counts as misclassification\n    margins = y * (Xb @ w)\n    return int(np.sum(margins <= 0.0))\n\ndef perceptron_with_momentum(X, y, eta, beta, max_steps):\n    \"\"\"\n    Implements cyclic mistake-driven perceptron with momentum.\n\n    Parameters:\n        X: np.ndarray of shape (n_samples, n_features)\n        y: np.ndarray of shape (n_samples,), labels in {-1, +1}\n        eta: float, learning rate\n        beta: float, momentum coefficient in [0,1)\n        max_steps: int, iteration cap (each iteration visits one example)\n\n    Returns:\n        steps: int, number of mistake-driven updates performed\n        overshoot_ratio: float, overshoot events / steps (rounded to 6 decimals)\n        converged: bool, True if converged before cap\n    \"\"\"\n    n, d = X.shape\n    # Augment with bias term\n    Xb = np.hstack([X, np.ones((n, 1))])\n    w = np.zeros(d + 1, dtype=float)\n    v = np.zeros(d + 1, dtype=float)\n\n    steps = 0  # mistake-driven updates\n    overshoots = 0  # overshoot events only counted on mistake-driven updates\n\n    converged = False\n    for t in range(max_steps):\n        i = t % n\n        margin = y[i] * (np.dot(w, Xb[i]))\n        before_mis = misclassification_count(w, Xb, y)\n\n        if margin <= 0.0:\n            # mistake-driven update\n            v = beta * v + eta * y[i] * Xb[i]\n            w_new = w + v\n            after_mis = misclassification_count(w_new, Xb, y)\n            steps += 1\n            if after_mis > before_mis:\n                overshoots += 1\n            w = w_new\n        else:\n            # decay-only update\n            v = beta * v\n            w = w + v\n\n        # check convergence\n        if misclassification_count(w, Xb, y) == 0:\n            converged = True\n            break\n\n    if steps == 0:\n        overshoot_ratio = 0.0\n    else:\n        overshoot_ratio = round(overshoots / steps, 6)\n\n    return steps, overshoot_ratio, converged\n\ndef solve():\n    # Define the datasets as per the problem statement.\n    # Dataset D1: borderline-separable\n    X1 = np.array([\n        [1.0, 1.05], [2.0, 2.05], [3.0, 3.05],  # positives\n        [1.0, 0.95], [2.0, 1.95], [3.0, 2.95]   # negatives\n    ], dtype=float)\n    y1 = np.array([+1, +1, +1, -1, -1, -1], dtype=int)\n\n    # Dataset D2: extremely small margin\n    X2 = np.array([\n        [0.0, 0.001], [1.0, 1.001], [2.0, 2.001],   # positives\n        [0.0, -0.001], [1.0, 0.999], [2.0, 1.999]   # negatives\n    ], dtype=float)\n    y2 = np.array([+1, +1, +1, -1, -1, -1], dtype=int)\n\n    # Dataset D3: non-separable edge case\n    X3 = np.array([\n        [0.0, 0.0], [1.0, 1.0],   # positives\n        [0.0, 0.0], [1.0, 1.0]    # negatives (duplicate locations)\n    ], dtype=float)\n    y3 = np.array([+1, +1, -1, -1], dtype=int)\n\n    # Test cases: (X, y, eta, beta, max_steps)\n    test_cases = [\n        (X1, y1, 0.1, 0.0, 5000),   # Case 1: baseline no momentum\n        (X1, y1, 0.1, 0.9, 5000),   # Case 2: strong momentum on D1\n        (X2, y2, 0.05, 0.95, 8000), # Case 3: tiny margin with strong momentum\n        (X3, y3, 0.1, 0.9, 2000)    # Case 4: non-separable edge case\n    ]\n\n    results = []\n    for X, y, eta, beta, max_steps in test_cases:\n        steps, overshoot_ratio, converged = perceptron_with_momentum(X, y, eta, beta, max_steps)\n        # Ensure overshoot ratio is a float with up to 6 decimals already\n        results.append([steps, overshoot_ratio, converged])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The performance of many machine learning algorithms, including the perceptron, is highly dependent on the geometric properties of the input data. This practice explores how high correlation between features can create a poorly conditioned problem, slowing down the convergence of the learning algorithm. You will apply the Gram-Schmidt process, a fundamental tool from linear algebra, to transform the feature space into an orthonormal one and empirically verify the dramatic improvement in convergence rate . This exercise provides a concrete demonstration of how data pre-processing is a critical step in building effective machine learning models.",
            "id": "3099389",
            "problem": "You are asked to write a complete, runnable program that empirically analyzes how feature correlation affects the convergence rate of the perceptron learning algorithm, and how decorrelating features via the Gram–Schmidt process alters this behavior. Your program must implement the perceptron from first principles and perform a feature-space transformation based on Gram–Schmidt orthonormalization to obtain a decorrelated representation for comparison.\n\nFundamental bases to use:\n- Definition of a linear classifier with the perceptron update: for data points $\\{(x_i,y_i)\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{-1,+1\\}$, the perceptron maintains a weight vector $w \\in \\mathbb{R}^d$ and updates it upon a mistake on $(x_i,y_i)$ using the rule $w \\leftarrow w + y_i x_i$.\n- Gram–Schmidt orthonormalization: for a matrix $X \\in \\mathbb{R}^{n \\times d}$, the Gram–Schmidt process yields a factorization $X = QR$ where $Q \\in \\mathbb{R}^{n \\times d}$ has orthonormal columns and $R \\in \\mathbb{R}^{d \\times d}$ is upper triangular and invertible when $X$ has full column rank. The corresponding linear feature transformation $A = R^{-1}$ maps each $x_i$ to $z_i = A x_i$, producing $Z = X A = Q$ whose columns are orthonormal across the dataset.\n\nYour program must:\n- Generate synthetic, linearly separable datasets with controlled feature correlation. For each test case, draw $n$ samples in dimension $d$ from a zero-mean multivariate normal distribution with equicorrelation covariance $\\Sigma = (1-\\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top$, where $\\rho \\in (-1,1)$ is the correlation parameter and $\\mathbf{1}$ is the vector of ones in $\\mathbb{R}^d$. Draw a ground-truth weight $w_\\star \\in \\mathbb{R}^d$ from a standard normal distribution, and assign labels $y_i = \\operatorname{sign}(w_\\star^\\top x_i) \\in \\{-1,+1\\}$. To ensure a positive margin, modify the features by $x_i \\leftarrow x_i + m\\, y_i\\, u$ where $u = \\frac{w_\\star}{\\|w_\\star\\|_2}$ and $m > 0$ is a fixed margin-injection constant common to all test cases.\n- Implement a deterministic perceptron procedure that:\n  - Initializes $w = 0 \\in \\mathbb{R}^d$.\n  - Scans the $n$ samples in fixed index order $i = 1,2,\\dots,n$.\n  - Applies the update $w \\leftarrow w + y_i x_i$ on each mistake where $y_i (w^\\top x_i) \\le 0$.\n  - Continues making full passes over the dataset until a complete pass incurs zero mistakes, and returns the total number of updates made.\n- Compute the Gram–Schmidt transformation on the original feature matrix $X \\in \\mathbb{R}^{n \\times d}$ via $X = Q R$ and form decorrelated features $Z = X R^{-1} = Q$. Run the exact same perceptron procedure on $Z$ with the same labels $y$, and record the total number of updates to convergence.\n- Use a fixed random seed per test case for reproducibility. All random draws must use the stated seed for that test case.\n- For all linear algebra and norms, use the standard Euclidean inner product.\n\nTest suite to implement:\n- Use margin injection $m = 1.0$.\n- For each test case, generate data as specified above using the corresponding tuple $(d,n,\\rho,\\text{seed})$:\n  - Case $1$: $(d,n,\\rho,\\text{seed}) = (\\,2,\\,100,\\,0.0,\\,42\\,)$.\n  - Case $2$: $(d,n,\\rho,\\text{seed}) = (\\,2,\\,100,\\,0.95,\\,43\\,)$.\n  - Case $3$: $(d,n,\\rho,\\text{seed}) = (\\,2,\\,100,\\,{-0.95},\\,44\\,)$.\n  - Case $4$: $(d,n,\\rho,\\text{seed}) = (\\,3,\\,120,\\,0.8,\\,45\\,)$.\n\nRequired outputs per test case:\n- Produce a pair of integers $[u_{\\text{orig}}, u_{\\text{ortho}}]$, where $u_{\\text{orig}}$ is the total number of perceptron updates to convergence on the original correlated features $X$, and $u_{\\text{ortho}}$ is the total number of perceptron updates to convergence on the Gram–Schmidt–decorrelated features $Z$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the four pairs, enclosed in square brackets, with no spaces. For example: $[[u_1,v_1],[u_2,v_2],[u_3,v_3],[u_4,v_4]]$, where each $u_k$ and $v_k$ are integers corresponding to the $k$-th test case in the order listed above.",
            "solution": "The user requires an empirical analysis of the perceptron algorithm's convergence rate as a function of feature correlation. This is to be achieved by generating synthetic datasets with controlled inter-feature correlation, running a deterministic perceptron algorithm on this original data, and then comparing its performance against the same algorithm run on a decorrelated version of the data. The decorrelation is performed via a feature-space transformation derived from the Gram-Schmidt orthonormalization process.\n\nThe solution is structured as follows:\n1.  **Data Generation**: A precise method for creating linearly separable datasets with a specified correlation structure.\n2.  **Perceptron Algorithm**: The implementation of the perceptron learning rule.\n3.  **Feature Decorrelation**: The application of Gram-Schmidt orthonormalization to transform the feature space.\n4.  **Experimental Procedure**: The complete pipeline for a single test case, combining the above components to produce the required output.\n\n**1. Data Generation**\n\nA synthetic dataset consists of $n$ instances, each being a pair $(x_i, y_i)$ where $x_i \\in \\mathbb{R}^d$ is a feature vector and $y_i \\in \\{-1, +1\\}$ is a class label.\n\n- **Initial Feature Generation**: The feature vectors are initially drawn from a $d$-dimensional multivariate normal distribution with zero mean and a specified covariance matrix $\\Sigma$:\n$$\nx_i \\sim \\mathcal{N}(0, \\Sigma)\n$$\nThe covariance matrix $\\Sigma$ is constructed to have an equicorrelation structure, controlled by a parameter $\\rho \\in (-1/(d-1), 1)$:\n$$\n\\Sigma = (1-\\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\nHere, $I_d$ is the $d \\times d$ identity matrix and $\\mathbf{1}$ is a $d \\times 1$ vector of ones. This structure ensures that the variance of each feature is $1$ and the covariance between any two distinct features $j$ and $k$ is $\\text{Cov}(X_j, X_k) = \\rho$.\n\n- **Label Assignment**: A ground-truth separating hyperplane is defined by a weight vector $w_\\star \\in \\mathbb{R}^d$, whose components are drawn from a standard normal distribution, $w_{\\star,j} \\sim \\mathcal{N}(0,1)$. The labels are then assigned based on which side of this hyperplane each point $x_i$ falls:\n$$\ny_i = \\operatorname{sgn}(w_\\star^\\top x_i)\n$$\nTo ensure that $y_i \\in \\{-1, +1\\}$, any case where $w_\\star^\\top x_i = 0$ (a rare event with continuous distributions) is resolved by assigning $y_i = +1$.\n\n- **Margin Injection**: To ensure that the data is linearly separable with a non-zero margin (a condition for the perceptron a_lgorithm's guaranteed convergence), the feature vectors are adjusted. This process shifts each point $x_i$ further away from the separating hyperplane, in the correct direction. The modified feature vector $x'_i$ is given by:\n$$\nx'_i = x_i + m y_i u\n$$\nwhere $m > 0$ is a fixed margin constant ($m=1.0$ in this problem) and $u$ is the unit vector normal to the true hyperplane, $u = \\frac{w_\\star}{\\|w_\\star\\|_2}$. This operation guarantees that each point has a geometric margin of at least $m$ with respect to the hyperplane defined by $w_\\star$. The collection of these modified vectors $\\{x'_i\\}$ forms the final feature matrix $X$.\n\n**2. Perceptron Learning Algorithm**\n\nThe perceptron algorithm is an iterative method for finding a separating hyperplane for a linearly separable dataset. The algorithm implemented here is deterministic.\n\n- **Initialization**: The weight vector is initialized to the zero vector, $w = 0 \\in \\mathbb{R}^d$. A counter for the total number of updates is initialized to $0$.\n\n- **Iteration**: The algorithm proceeds in passes. In each pass, it iterates through all data points $(x_i, y_i)$ for $i=1, \\dots, n$ in a fixed order. For each point, it checks the classification condition:\n$$\ny_i (w^\\top x_i) \\le 0\n$$\nIf this condition is met, the point is misclassified (or lies on the boundary), and the weight vector is updated according to the perceptron rule:\n$$\nw \\leftarrow w + y_i x_i\n$$\nThe total update counter is incremented.\n\n- **Termination**: The algorithm terminates when it completes a full pass over the dataset without making any updates. The final value of the update counter is the result of interest, denoted $u$.\n\n**3. Feature Decorrelation via Gram-Schmidt**\n\nThe problem requires a comparison with a decorrelated feature set. This is achieved by applying a linear transformation to the original feature matrix $X \\in \\mathbb{R}^{n \\times d}$.\n\n- **QR Decomposition**: The Gram-Schmidt process is applied to the *columns* of the feature matrix $X$. The columns of $X$, $\\{c_1, \\dots, c_d\\}$, can be viewed as vectors in $\\mathbb{R}^n$, where each vector represents all observations of a single feature. The process is computationally realized via the reduced QR decomposition of $X$:\n$$\nX = QR\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times d}$ is a matrix with orthonormal columns (i.e., $Q^\\top Q = I_d$), and $R \\in \\mathbb{R}^{d \\times d}$ is an invertible upper triangular matrix (assuming $X$ has full column rank, which is highly probable for the given data generation process).\n\n- **Feature Transformation**: The problem defines the transformation via a matrix $A = R^{-1}$. The new feature matrix, $Z \\in \\mathbb{R}^{n \\times d}$, is obtained by applying this transformation to $X$:\n$$\nZ = XA = XR^{-1}\n$$\nBy substituting $X=QR$, we find the new feature matrix is simply $Q$:\n$$\nZ = (QR)R^{-1} = Q(RR^{-1}) = QI_d = Q\n$$\nThe new feature vectors, which are the rows of $Z=Q$, are then used with the original labels $y$ to train a second perceptron. The resulting feature space has the property that its feature-defining column vectors are orthonormal, meaning they are uncorrelated across the samples of the dataset.\n\n**4. Experimental Procedure**\n\nFor each test case specified by a tuple $(d, n, \\rho, \\text{seed})$, the following steps are executed:\n1.  Set the random number generator's seed for reproducibility.\n2.  Generate the original feature matrix $X$ and labels $y$ using the procedure described in section 1.\n3.  Run the perceptron algorithm (section 2) on the dataset $(X,y)$ and record the total number of updates to convergence, $u_{\\text{orig}}$.\n4.  Compute the reduced QR decomposition of $X$ to obtain $Q$ and $R$.\n5.  Set the transformed feature matrix $Z=Q$.\n6.  Run the perceptron algorithm on the transformed dataset $(Z,y)$ and record the total updates, $u_{\\text{ortho}}$.\n7.  The final output for the test case is the pair $[u_{\\text{orig}}, u_{\\text{ortho}}]$.\n\nThis comparative analysis is expected to show that for high values of correlation $|\\rho|$, the number of updates $u_{\\text{orig}}$ is significantly larger than $u_{\\text{ortho}}$. The orthonormalization process regularizes the geometry of the data, typically leading to faster and more stable convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_perceptron(features, labels):\n    \"\"\"\n    Runs the deterministic perceptron algorithm.\n\n    Args:\n        features (np.ndarray): The feature matrix (n_samples, n_features).\n        labels (np.ndarray): The label vector (n_samples,).\n\n    Returns:\n        int: The total number of updates until convergence.\n    \"\"\"\n    n_samples, n_features = features.shape\n    w = np.zeros(n_features)\n    total_updates = 0\n    \n    while True:\n        updates_in_pass = 0\n        for i in range(n_samples):\n            # Perceptron mistake condition\n            if labels[i] * np.dot(w, features[i]) <= 0:\n                # Perceptron update rule\n                w += labels[i] * features[i]\n                total_updates += 1\n                updates_in_pass += 1\n        \n        # Termination condition\n        if updates_in_pass == 0:\n            break\n            \n    return total_updates\n\ndef run_single_case(d, n, rho, seed, m):\n    \"\"\"\n    Performs the full analysis for a single test case.\n\n    Args:\n        d (int): Number of features (dimensions).\n        n (int): Number of samples.\n        rho (float): Correlation coefficient.\n        seed (int): Random seed for reproducibility.\n        m (float): Margin injection constant.\n\n    Returns:\n        list[int, int]: A pair of integers [u_orig, u_ortho].\n    \"\"\"\n    # 1. Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    # 2. Generate synthetic data\n    # Create the equicorrelation covariance matrix\n    cov_matrix = (1 - rho) * np.eye(d) + rho * np.ones((d, d))\n    \n    # Draw samples from a multivariate normal distribution\n    X_initial = rng.multivariate_normal(mean=np.zeros(d), cov=cov_matrix, size=n)\n    \n    # Draw a ground-truth weight vector\n    w_star = rng.standard_normal(size=d)\n    \n    # Assign labels\n    y = np.sign(X_initial @ w_star)\n    # Ensure labels are in {-1, +1}\n    y[y == 0] = 1\n    \n    # 3. Perform margin injection\n    u = w_star / np.linalg.norm(w_star)\n    # Use broadcasting to add the margin term to each row of X\n    X_orig = X_initial + m * y[:, np.newaxis] * u\n    \n    # 4. Run perceptron on original features\n    u_orig = run_perceptron(X_orig, y)\n    \n    # 5. Decorrelate features using Gram-Schmidt (QR decomposition)\n    # 'reduced' mode is essential for non-square matrices\n    Q, R = np.linalg.qr(X_orig, mode='reduced')\n    Z_ortho = Q\n    \n    # 6. Run perceptron on decorrelated features\n    u_ortho = run_perceptron(Z_ortho, y)\n    \n    return [u_orig, u_ortho]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Margin injection constant common to all tests\n    m = 1.0\n\n    # Test suite: (d, n, rho, seed)\n    test_cases = [\n        (2, 100, 0.0, 42),\n        (2, 100, 0.95, 43),\n        (2, 100, -0.95, 44),\n        (3, 120, 0.8, 45),\n    ]\n\n    results = []\n    for d, n, rho, seed in test_cases:\n        result = run_single_case(d, n, rho, seed, m)\n        results.append(result)\n\n    # Format the output string as specified, e.g., [[u1,v1],[u2,v2],...]\n    result_str_parts = [f\"[{u},{v}]\" for u, v in results]\n    final_output = f\"[{','.join(result_str_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world datasets are rarely perfect and often contain errors, such as incorrect labels. This exercise investigates how to build a more robust linear classifier by moving from the classic perceptron rule to a model trained with gradient descent on a squared error loss. You will implement and test label smoothing, a regularization technique that prevents the model from becoming overconfident by softening the \"hard\" target labels of $-1$ and $+1$ . This practice provides a direct bridge from the perceptron to the gradient-based optimization and regularization strategies that are central to modern artificial neural networks.",
            "id": "3099440",
            "problem": "You are asked to implement and analyze a single-layer artificial neuron (a perceptron-like model with identity activation) trained by Empirical Risk Minimization (ERM) under squared loss, comparing \"hard\" labels and \"soft\" labels produced by label smoothing. The goal is to test robustness to label noise by quantifying how label smoothing affects classification accuracy on clean data.\n\nFoundational base and definitions:\n- The model is a linear neuron with parameters $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$, producing an output $a_i = w^\\top x_i + b$ for input $x_i \\in \\mathbb{R}^d$.\n- The classification decision is $\\hat{y}_i = \\mathrm{sign}(a_i)$, where $\\mathrm{sign}(z) = +1$ if $z \\ge 0$ and $-1$ otherwise.\n- Under Empirical Risk Minimization (ERM), one minimizes the average loss over the training set. Use the squared loss between the model output and the target: for targets $t_i$, define the empirical risk $L(w,b) = \\frac{1}{n} \\sum_{i=1}^n (a_i - t_i)^2$.\n- Label smoothing transforms hard labels $y_i \\in \\{-1,+1\\}$ into soft targets $t_i = (1 - \\alpha) y_i$ with smoothing parameter $\\alpha \\in [0,1)$. Here, $\\alpha$ is a scalar, and $t_i \\in [-1,1]$.\n\nData generation and label noise:\n- Fix a ground-truth linear separator with parameters $w^\\star \\in \\mathbb{R}^d$ and $b^\\star \\in \\mathbb{R}$. For any input $x$, the clean label is $y = \\mathrm{sign}\\!\\left(w^{\\star \\top} x + b^\\star\\right)$.\n- Generate training and test inputs independently from a standard normal distribution: $x \\sim \\mathcal{N}(0, I_d)$.\n- Introduce label noise in training labels by flipping each clean label independently with probability $\\eta \\in [0,0.5)$, given as a decimal fraction (not a percentage). That is, the observed training label is $y^{\\text{noisy}} = y \\cdot z$ where $z = -1$ with probability $\\eta$ and $z = +1$ otherwise.\n- The test labels must remain clean (no label noise in the test set).\n\nTraining algorithm requirement:\n- Minimize $L(w,b)$ by gradient descent with respect to $(w,b)$ using full-batch gradients. Start from $w = 0$ and $b = 0$.\n- For given targets $t_i$, compute the gradient with respect to $w$ and $b$ and update $(w,b)$ iteratively with a constant step size. The learning rate is a scalar parameter denoted by $\\gamma$.\n- Train two models on the same noisy training data:\n  1. A baseline model trained with hard targets $t_i = y^{\\text{noisy}}$ (equivalently $\\alpha = 0$).\n  2. A smoothed model trained with soft targets $t_i = (1-\\alpha)\\,y^{\\text{noisy}}$ for the specified $\\alpha$.\n\nEvaluation:\n- After training, evaluate both models on a clean test set by computing the fraction of correctly classified test examples. For any test input $x$, the predicted label is $\\hat{y} = \\mathrm{sign}(w^\\top x + b)$ and the clean label is $y = \\mathrm{sign}(w^{\\star \\top} x + b^\\star)$; accuracy is the fraction of test inputs for which $\\hat{y} = y$.\n- For each test case, compute the difference $\\Delta = \\text{Acc}_{\\alpha} - \\text{Acc}_{0}$, where $\\text{Acc}_{\\alpha}$ is the accuracy of the smoothed model and $\\text{Acc}_{0}$ is the accuracy of the baseline model. This scalar $\\Delta$ quantifies the change in accuracy due to label smoothing under the specified label noise level.\n\nDeterminism:\n- To ensure deterministic outputs, set a fixed random seed for all random number generation. Use the same fixed $w^\\star$ and $b^\\star$ across all test cases. Each test case should use its own independently generated training and test sets, but all randomness must be controlled by the fixed seed.\n\nHyperparameters and sizes:\n- Use input dimension $d = 2$, number of training examples $n = 200$, number of test examples $m = 1000$, learning rate $\\gamma = 0.1$, and number of gradient descent iterations $T = 1000$. All these quantities must be treated as scalars with the specified values.\n\nTest suite:\n- Use the following test cases, each specified by the pair $(\\eta,\\alpha)$ with $\\eta$ and $\\alpha$ as decimal fractions:\n  1. $(\\eta = 0, \\alpha = 0.2)$\n  2. $(\\eta = 0.2, \\alpha = 0.2)$\n  3. $(\\eta = 0.4, \\alpha = 0.4)$\n  4. $(\\eta = 0.3, \\alpha = 0)$\n  5. $(\\eta = 0.4, \\alpha = 0.9)$\n\nRequired outputs:\n- For each test case, compute $\\Delta$ as defined above and round it to $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the rounded $\\Delta$ for the $i$-th test case.\n\nAngle units and physical units do not apply. Express all rates (such as $\\eta$) and parameters (such as $\\alpha$) as decimal fractions, not percentages.",
            "solution": "The problem requires an analysis of label smoothing as a regularization technique for a single linear neuron facing label noise. We will implement a simulation to compare the classification accuracy of a model trained with hard labels against one trained with smoothed labels. The framework for this analysis is Empirical Risk Minimization (ERM) with a squared loss function, and the optimization is performed via gradient descent.\n\nFirst, we formalize the components of the model and the learning problem. The model is a single-layer artificial neuron that computes a linear function of its input $x_i \\in \\mathbb{R}^d$. The neuron's output, or activation, is given by $a_i = w^\\top x_i + b$, where $w \\in \\mathbb{R}^d$ is the weight vector and $b \\in \\mathbb{R}$ is the bias term. For binary classification, a decision rule is applied to this activation. The predicted label $\\hat{y}_i$ is determined by the sign of the activation: $\\hat{y}_i = \\mathrm{sign}(a_i)$, where the sign function is defined as $\\mathrm{sign}(z) = +1$ if $z \\ge 0$ and $\\mathrm{sign}(z) = -1$ if $z < 0$.\n\nThe training process is guided by the principle of Empirical Risk Minimization. We aim to find the parameters $(w,b)$ that minimize the average loss over a training dataset of $n$ examples $\\{(x_i, t_i)\\}_{i=1}^n$, where $x_i$ are the input features and $t_i$ are the target values. The specified loss function is the squared error, leading to the empirical risk:\n$$\nL(w,b) = \\frac{1}{n} \\sum_{i=1}^n (a_i - t_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (w^\\top x_i + b - t_i)^2\n$$\nThis loss function is convex in $(w,b)$, guaranteeing that gradient descent can find a global minimum. To perform the optimization, we compute the gradients of $L(w,b)$ with respect to $w$ and $b$.\n\nThe gradient with respect to the weight vector $w$ is:\n$$\n\\nabla_w L(w,b) = \\frac{\\partial}{\\partial w} \\left( \\frac{1}{n} \\sum_{i=1}^n (w^\\top x_i + b - t_i)^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n 2(w^\\top x_i + b - t_i) \\frac{\\partial}{\\partial w}(w^\\top x_i) = \\frac{2}{n} \\sum_{i=1}^n (w^\\top x_i + b - t_i) x_i\n$$\nThe gradient with respect to the scalar bias $b$ is:\n$$\n\\nabla_b L(w,b) = \\frac{\\partial}{\\partial b} \\left( \\frac{1}{n} \\sum_{i=1}^n (w^\\top x_i + b - t_i)^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n 2(w^\\top x_i + b - t_i) \\frac{\\partial}{\\partial b}(b) = \\frac{2}{n} \\sum_{i=1}^n (w^\\top x_i + b - t_i)\n$$\nThese gradients are used in the full-batch gradient descent algorithm. Starting from initial parameters $(w_0, b_0)$, typically set to zero vectors and scalars, the parameters are updated iteratively for $T$ steps using a constant learning rate $\\gamma > 0$:\n$$\nw_{k+1} = w_k - \\gamma \\nabla_w L(w_k, b_k)\n$$\n$$\nb_{k+1} = b_k - \\gamma \\nabla_b L(w_k, b_k)\n$$\n\nThe data for this problem is generated synthetically to provide a controlled experimental setup. A ground-truth linear separator is defined by a fixed vector $w^\\star \\in \\mathbb{R}^d$ and bias $b^\\star \\in \\mathbb{R}$. For any input $x$, the true, clean label is $y = \\mathrm{sign}(w^{\\star \\top} x + b^\\star)$. Both training and test inputs $x$ are drawn from a standard $d$-dimensional normal distribution, $x \\sim \\mathcal{N}(0, I_d)$.\n\nA critical aspect of the problem is the introduction of label noise into the training set. The clean training labels $y_i$ are flipped with a probability $\\eta \\in [0, 0.5)$. This is modeled by multiplying the clean label by a random variable $z_i$ which takes the value $-1$ with probability $\\eta$ and $+1$ with probability $1-\\eta$. The resulting noisy label is $y^{\\text{noisy}}_i = y_i \\cdot z_i$. The test set labels remain clean.\n\nWe train two models on the same noisy training data. The distinction between them lies in the target values $t_i$ used in the loss function.\n1.  The baseline model uses \"hard\" targets, which are the noisy labels themselves: $t_i = y^{\\text{noisy}}_i$. This corresponds to the label smoothing formulation with a smoothing parameter $\\alpha = 0$.\n2.  The smoothed model uses \"soft\" targets, which are the noisy labels scaled by a factor $(1-\\alpha)$: $t_i = (1 - \\alpha) y^{\\text{noisy}}_i$, for a given $\\alpha \\in [0,1)$. This technique shrinks the target values towards $0$, which can prevent the model from becoming overconfident, especially in the presence of noisy labels.\n\nAfter training for $T=1000$ iterations with a learning rate of $\\gamma=0.1$, both models are evaluated on a large, clean test set of size $m=1000$. The performance metric is accuracy, defined as the fraction of test examples for which the predicted label $\\hat{y} = \\mathrm{sign}(w^\\top x + b)$ matches the true clean label $y = \\mathrm{sign}(w^{\\star \\top} x + b^\\star)$.\nThe final quantity of interest is the difference in accuracy between the smoothed model and the baseline model: $\\Delta = \\text{Acc}_{\\alpha} - \\text{Acc}_{0}$. This value quantifies the benefit (if $\\Delta > 0$) or detriment (if $\\Delta < 0$) of applying label smoothing at a specific level $\\alpha$ for a given noise level $\\eta$.\n\nThe entire simulation is deterministic. A fixed random seed ensures that the generation of data ($x_i$), the ground-truth parameters ($w^\\star, b^\\star$), and the label noise are reproducible. For each specified test case $(\\eta, \\alpha)$, the simulation is run independently, starting from the same initial random seed to ensure that the generated data and noise patterns are identical for test cases that happen to share the same $\\eta$, providing a controlled comparison of the effect of $\\alpha$. The specific parameters are: dimension $d=2$, training size $n=200$, test size $m=1000$.\n\nThe algorithm for each test case $(\\eta, \\alpha)$ is as follows:\n1.  Initialize the random number generator with a fixed seed.\n2.  Define fixed ground-truth parameters $w^\\star$ and $b^\\star$.\n3.  Generate $n$ training inputs $X_{\\text{train}}$ and $m$ test inputs $X_{\\text{test}}$ from $\\mathcal{N}(0, I_d)$.\n4.  Compute clean labels for both sets: $y_{\\text{train}} = \\mathrm{sign}(X_{\\text{train}} w^\\star + b^\\star)$ and $y_{\\text{test}} = \\mathrm{sign}(X_{\\text{test}} w^\\star + b^\\star)$.\n5.  Create noisy training labels $y_{\\text{noisy}}$ by flipping each label in $y_{\\text{train}}$ with probability $\\eta$.\n6.  Train the baseline model:\n    a. Set targets $t_{\\text{base}} = y_{\\text{noisy}}$.\n    b. Initialize $(w, b) = (0, 0)$.\n    c. Perform $T$ steps of gradient descent to obtain $(w_{\\text{base}}, b_{\\text{base}})$.\n7.  Train the smoothed model:\n    a. Set targets $t_{\\text{smooth}} = (1-\\alpha) y_{\\text{noisy}}$.\n    b. Initialize $(w, b) = (0, 0)$.\n    c. Perform $T$ steps of gradient descent to obtain $(w_{\\text{smooth}}, b_{\\text{smooth}})$. Note: if $\\alpha=0$, this model is identical to the baseline.\n8.  Evaluate both models on the clean test set $(X_{\\text{test}}, y_{\\text{test}})$ to get accuracies $\\text{Acc}_{0}$ and $\\text{Acc}_{\\alpha}$.\n9.  Compute and store $\\Delta = \\text{Acc}_{\\alpha} - \\text{Acc}_{0}$, rounded to $4$ decimal places.\n\nThis procedure will be repeated for all test cases specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a single-layer artificial neuron trained with squared loss\n    to compare the effects of hard vs. soft (smoothed) labels on robustness to label noise.\n    \"\"\"\n\n    # --- Hyperparameters and Fixed Settings ---\n    D = 2         # Input dimension\n    N = 200       # Number of training examples\n    M = 1000      # Number of test examples\n    GAMMA = 0.1   # Learning rate\n    T = 1000      # Number of gradient descent iterations\n    FIXED_SEED = 42 # Seed for reproducibility\n\n    # Fixed ground-truth separator parameters (w_star, b_star)\n    # Using a normalized w_star for stability.\n    w_star = np.array([0.6, -0.8])\n    b_star = 0.1\n\n    # --- Test Suite ---\n    test_cases = [\n        (0.0, 0.2),\n        (0.2, 0.2),\n        (0.4, 0.4),\n        (0.3, 0.0),\n        (0.4, 0.9)\n    ]\n\n    # --- Helper Functions ---\n    def sign(z):\n        \"\"\"Custom sign function: +1 if z >= 0, -1 otherwise.\"\"\"\n        return np.where(z >= 0, 1, -1)\n\n    def train(X, t, gamma, T):\n        \"\"\"\n        Trains a linear neuron using full-batch gradient descent.\n        \n        Args:\n            X (np.ndarray): Training inputs, shape (n, d).\n            t (np.ndarray): Target values, shape (n,).\n            gamma (float): Learning rate.\n            T (int): Number of iterations.\n\n        Returns:\n            tuple[np.ndarray, float]: Trained weight vector w and bias b.\n        \"\"\"\n        n_samples, d_features = X.shape\n        w = np.zeros(d_features)\n        b = 0.0\n\n        for _ in range(T):\n            # Model output/activation\n            a = X @ w + b\n            \n            # Error\n            error = a - t\n            \n            # Gradients of the squared loss L = (1/n) * sum((a_i - t_i)^2)\n            grad_w = (2 / n_samples) * (X.T @ error)\n            grad_b = (2 / n_samples) * np.sum(error)\n            \n            # Update parameters\n            w -= gamma * grad_w\n            b -= gamma * grad_b\n            \n        return w, b\n\n    def calculate_accuracy(X, y_true, w, b):\n        \"\"\"\n        Calculates classification accuracy.\n        \n        Args:\n            X (np.ndarray): Test inputs.\n            y_true (np.ndarray): True labels.\n            w (np.ndarray): Model weights.\n            b (float): Model bias.\n\n        Returns:\n            float: Accuracy score.\n        \"\"\"\n        y_pred = sign(X @ w + b)\n        return np.mean(y_pred == y_true)\n\n    # --- Main Simulation Loop ---\n    results = []\n    \n    for eta, alpha in test_cases:\n        # For each test case, re-seed to ensure controlled comparisons.\n        # This makes the generated data and noise pattern depend only on the seed,\n        # allowing for a fair comparison across different (eta, alpha) pairs.\n        rng = np.random.default_rng(FIXED_SEED)\n\n        # --- Data Generation ---\n        # Generate training data\n        X_train = rng.standard_normal(size=(N, D))\n        y_train_clean = sign(X_train @ w_star + b_star)\n        \n        # Introduce label noise\n        noise_mask = rng.random(size=N) < eta\n        y_train_noisy = y_train_clean.copy()\n        y_train_noisy[noise_mask] *= -1\n        \n        # Generate clean test data\n        X_test = rng.standard_normal(size=(M, D))\n        y_test_clean = sign(X_test @ w_star + b_star)\n\n        # --- Model Training ---\n        # 1. Baseline model (alpha = 0)\n        t_base = y_train_noisy\n        w_base, b_base = train(X_train, t_base, GAMMA, T)\n        \n        # 2. Smoothed model (with specified alpha)\n        # Note: If alpha is 0, this is identical to the baseline model.\n        t_smooth = (1 - alpha) * y_train_noisy\n        w_smooth, b_smooth = train(X_train, t_smooth, GAMMA, T)\n\n        # --- Evaluation ---\n        # Calculate accuracy for both models on the clean test set\n        acc_base = calculate_accuracy(X_test, y_test_clean, w_base, b_base)\n        acc_smooth = calculate_accuracy(X_test, y_test_clean, w_smooth, b_smooth)\n        \n        # Compute the difference in accuracy\n        delta = acc_smooth - acc_base\n        \n        # Round and store the result\n        results.append(round(delta, 4))\n        \n    # --- Final Output ---\n    # The format must be exactly a comma-separated list in brackets.\n    output_str = \",\".join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}