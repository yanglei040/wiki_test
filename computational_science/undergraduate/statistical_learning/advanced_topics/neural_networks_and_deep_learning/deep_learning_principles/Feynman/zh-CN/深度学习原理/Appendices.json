{
    "hands_on_practices": [
        {
            "introduction": "理解神经网络如何响应输入的微小变化，对于构建鲁棒的模型至关重要。本实践将探讨网络的 Lipschitz 常数（一个衡量其敏感度的数学指标）与其对扰动的鲁棒性之间的联系。通过系统地对权重的谱范数施加惩罚，你将经验性地研究正则化如何控制模型的平滑度，并影响其泛化能力和对抗攻击的抵抗力。",
            "id": "3113400",
            "problem": "考虑一个全连接双层前馈网络，该网络使用修正线性单元（ReLU）激活函数，将输入 $\\mathbf{x} \\in \\mathbb{R}^2$ 映射到一个标量输出。该模型为\n$$\n\\mathbf{h}(\\mathbf{x}) = \\operatorname{ReLU}\\!\\left(W_1 \\mathbf{x} + \\mathbf{b}_1\\right), \\qquad\nf(\\mathbf{x}) = W_2 \\mathbf{h}(\\mathbf{x}) + b_2,\n$$\n其中 $W_1 \\in \\mathbb{R}^{3 \\times 2}$，$\\mathbf{b}_1 \\in \\mathbb{R}^3$，$W_2 \\in \\mathbb{R}^{1 \\times 3}$，以及 $b_2 \\in \\mathbb{R}$。设训练集为固定集合 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $n = 8$，$\\mathbf{x}_i \\in \\mathbb{R}^2$，以及 $y_i \\in \\{-1, +1\\}$。\n\n参数的数值如下：\n- $W_1 = \\begin{bmatrix} 1.0  0.5 \\\\ -0.3  1.2 \\\\ 0.7  -0.8 \\end{bmatrix}$，$\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$，\n- $W_2 = \\begin{bmatrix} 0.9  0.7  -0.6 \\end{bmatrix}$，$b_2 = -0.1$。\n\n数据集为：\n- 正类 ($y=+1$)：$\\mathbf{x}_1 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$，$\\mathbf{x}_2 = \\begin{bmatrix} 1.2 \\\\ 0.9 \\end{bmatrix}$，$\\mathbf{x}_3 = \\begin{bmatrix} 0.8 \\\\ 1.1 \\end{bmatrix}$，$\\mathbf{x}_4 = \\begin{bmatrix} 1.1 \\\\ 1.2 \\end{bmatrix}$。\n- 负类 ($y=-1$)：$\\mathbf{x}_5 = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}$，$\\mathbf{x}_6 = \\begin{bmatrix} -1.2 \\\\ -0.9 \\end{bmatrix}$，$\\mathbf{x}_7 = \\begin{bmatrix} -0.8 \\\\ -1.1 \\end{bmatrix}$，$\\mathbf{x}_8 = \\begin{bmatrix} -1.1 \\\\ -1.2 \\end{bmatrix}$。\n\n您将通过一个由非负惩罚强度 $a \\ge 0$ 参数化的确定性重缩放规则，对线性层施加谱范数惩罚。对于给定的 $a$，惩罚后的权重定义为\n$$\n\\widetilde{W}_1(a) = \\frac{1}{1+a}\\, W_1,\\qquad \\widetilde{W}_2(a) = \\frac{1}{1+a}\\, W_2,\n$$\n同时保持偏置不变，即 $\\widetilde{\\mathbf{b}}_1(a) = \\mathbf{b}_1$ 和 $\\widetilde{b}_2(a) = b_2$。随着 $a$ 的增加，该规则单调地减小了线性映射的谱范数，反映了惩罚项 $\\sum_\\ell \\|W_\\ell\\|_2$ 的效果。\n\n您的任务是：\n1. 仅从分析和线性代数的核心定义出发，推导网络 $f$ 的全局利普希茨常数的一个可计算上界，该上界是线性映射的函数，并利用了修正线性单元（ReLU）是 $1$-利普希茨这一事实。您必须使用算子范数的定义、激活函数的 $1$-利普希茨性质以及利普希茨函数的复合规则来证明该界。\n2. 使用利普希茨连续性和分类间隔的定义，推导翻转 $y_i f(\\mathbf{x}_i)$ 符号所需的最小 $\\ell_2$-范数扰动的样本级下界，该下界仅用任务1中的利普希茨界和有符号间隔 $m_i = y_i f(\\mathbf{x}_i)$ 表示。解释为什么非正间隔的样本贡献的下界为零。\n3. 将经验 $0$-$1$ 风险（训练分类误差）定义为 $\\widehat{R}(f) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{y_i f(\\mathbf{x}_i) \\le 0\\}$。对通过上述重缩放规则得到的每个惩罚模型 $f_a$ 计算该风险。\n\n实现一个程序，对于以下测试集中的每个惩罚参数 $a$，\n- $a \\in \\{\\, 0,\\, 0.5,\\, 3,\\, 50 \\,\\}$，\n执行施加惩罚的步骤以获得 $\\widetilde{W}_1(a)$ 和 $\\widetilde{W}_2(a)$，然后计算：\n- 由您的推导证明的 $f_a$ 的一个有效利普希茨上界 $L(a)$，\n- 平均样本级鲁棒性下界 $\\frac{1}{n}\\sum_{i=1}^n r_i(a)$，其中 $r_i(a)$ 是在任务2中推导出的下界，\n- 经验风险 $\\widehat{R}(f_a)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素本身是对应于 $a$ 的列表 $[L(a), \\overline{r}(a), \\widehat{R}(f_a)]$，按此顺序排列，每个值都表示为十进制数。例如，一个包含四个测试用例的输出必须是 $[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]$ 的单行形式。不涉及物理单位或角度单位；所有值均为实数。",
            "solution": "该问题被评估为有效，因为它在科学上基于统计学习的原理，问题设定良好并提供了所有必要的数据，并且使用标准的数学定义进行了客观的表述。\n\n这里我们介绍推导和计算策略。对于给定的惩罚强度 $a \\ge 0$，网络函数 $f_a: \\mathbb{R}^2 \\to \\mathbb{R}$ 由复合函数定义\n$$\nf_a(\\mathbf{x}) = g_3(g_2(g_1(\\mathbf{x})))\n$$\n其中\n- $g_1(\\mathbf{x}) = \\widetilde{W}_1(a) \\mathbf{x} + \\widetilde{\\mathbf{b}}_1(a) = \\frac{1}{1+a}W_1 \\mathbf{x} + \\mathbf{b}_1$\n- $g_2(\\mathbf{h}) = \\operatorname{ReLU}(\\mathbf{h})$\n- $g_3(\\mathbf{h}) = \\widetilde{W}_2(a) \\mathbf{h} + \\widetilde{b}_2(a) = \\frac{1}{1+a}W_2 \\mathbf{h} + b_2$\n\n所有向量范数 $\\|\\cdot\\|$ 都假定为欧几里得 $\\ell_2$-范数，矩阵范数 $\\|\\cdot\\|_2$ 是相应的诱导算子范数（谱范数）。\n\n### 任务1：全局利普希茨常数上界的推导\n\n一个函数 $g: \\mathbb{R}^m \\to \\mathbb{R}^k$ 是 $L$-利普希茨连续的，如果对所有 $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^m$，不等式 $\\|g(\\mathbf{u}) - g(\\mathbf{v})\\| \\le L \\|\\mathbf{u} - \\mathbf{v}\\|$ 成立。满足此条件的最小 $L$ 便是 $g$ 的利普希茨常数。一个关键性质是，对于利普希茨函数的复合 $g = g_N \\circ \\dots \\circ g_1$，复合函数也是利普希茨的，其常数 $L_g \\le L_{g_N} \\cdots L_{g_1}$。我们将此性质应用于该网络。\n\n1.  **仿射映射的利普希茨常数**: 设 $g(\\mathbf{x}) = W\\mathbf{x} + \\mathbf{b}$。对于任何 $\\mathbf{u}, \\mathbf{v}$，我们有\n    $$\n    \\|g(\\mathbf{u}) - g(\\mathbf{v})\\| = \\|(W\\mathbf{u} + \\mathbf{b}) - (W\\mathbf{v} + \\mathbf{b})\\| = \\|W(\\mathbf{u} - \\mathbf{v})\\|\n    $$\n    根据诱导算子范数的定义，对于任何向量 $\\mathbf{z}$，$\\|W\\mathbf{z}\\| \\le \\|W\\|_2 \\|\\mathbf{z}\\|$。因此，\n    $$\n    \\|g(\\mathbf{u}) - g(\\mathbf{v})\\| \\le \\|W\\|_2 \\|\\mathbf{u} - \\mathbf{v}\\|\n    $$\n    仿射映射的利普希茨常数是其线性部分的谱范数。因此，$g_1$ 和 $g_3$ 的利普希茨常数分别为 $L_{g_1} = \\|\\widetilde{W}_1(a)\\|_2$ 和 $L_{g_3} = \\|\\widetilde{W}_2(a)\\|_2$。\n\n2.  **激活函数的利普希茨常数**: 题目说明修正线性单元 $\\operatorname{ReLU}(z) = \\max(0, z)$ 是 $1$-利普希茨的。对于逐元素向量函数 $g_2(\\mathbf{h}) = \\operatorname{ReLU}(\\mathbf{h})$，其关于 $\\ell_2$-范数的利普希茨常数也是 $1$。要证明这一点，\n    $$\n    \\|g_2(\\mathbf{u}) - g_2(\\mathbf{v})\\|_2^2 = \\|\\operatorname{ReLU}(\\mathbf{u}) - \\operatorname{ReLU}(\\mathbf{v})\\|_2^2 = \\sum_j (\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j))^2\n    $$\n    由于标量ReLU是 $1$-利普希茨的，所以 $|\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j)| \\le |u_j - v_j|$。两边平方得到 $(\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j))^2 \\le (u_j - v_j)^2$。对所有分量求和得到\n    $$\n    \\sum_j (\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j))^2 \\le \\sum_j (u_j - v_j)^2 \\implies \\|g_2(\\mathbf{u}) - g_2(\\mathbf{v})\\|_2^2 \\le \\|\\mathbf{u} - \\mathbf{v}\\|_2^2\n    $$\n    取平方根，我们发现 $\\|g_2(\\mathbf{u}) - g_2(\\mathbf{v})\\|_2 \\le \\|\\mathbf{u} - \\mathbf{v}\\|_2$。因此，$L_{g_2} = 1$。\n\n3.  **复合**: 整个网络 $f_a$ 的利普希茨常数 $L(f_a)$ 受各个常数乘积的约束：\n    $$\n    L(f_a) \\le L_{g_3} \\cdot L_{g_2} \\cdot L_{g_1} = \\|\\widetilde{W}_2(a)\\|_2 \\cdot 1 \\cdot \\|\\widetilde{W}_1(a)\\|_2\n    $$\n    使用惩罚规则 $\\widetilde{W}_\\ell(a) = \\frac{1}{1+a}W_\\ell$ 和性质 $\\|\\lambda W\\|_2 = |\\lambda|\\|W\\|_2$，我们得到上界，记为 $L(a)$：\n    $$\n    L(a) = \\left\\| \\frac{1}{1+a}W_2 \\right\\|_2 \\left\\| \\frac{1}{1+a}W_1 \\right\\|_2 = \\frac{1}{(1+a)^2} \\|W_2\\|_2 \\|W_1\\|_2\n    $$\n    这就是 $f_a$ 的利普希茨常数的可计算上界。\n\n### 任务2：样本级鲁棒性下界的推导\n\n我们的目标是找到一个扰动 $\\mathbf{\\delta}$ 的 $\\ell_2$-范数下界，该扰动能翻转样本 $(\\mathbf{x}_i, y_i)$ 的分类。如果 $y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) \\le 0$，则发生符号翻转，前提是原始分类是正确的，即 $y_i f_a(\\mathbf{x}_i) > 0$。有符号间隔为 $m_i(a) = y_i f_a(\\mathbf{x}_i)$。\n\n1.  **情况1：正确分类的样本** ($m_i(a) > 0$)。\n    根据 $f_a$ 的利普希茨常数 $L(a)$ 的定义：\n    $$\n    |f_a(\\mathbf{x}_i + \\mathbf{\\delta}) - f_a(\\mathbf{x}_i)| \\le L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    乘以 $|y_i|=1$ 不改变该不等式：\n    $$\n    |y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) - y_i f_a(\\mathbf{x}_i)| \\le L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    这等价于双边不等式：\n    $$\n    -L(a) \\|\\mathbf{\\delta}\\|_2 \\le y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) - y_i f_a(\\mathbf{x}_i) \\le L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    整理左侧不等式可以得到扰动后间隔的下界：\n    $$\n    y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) \\ge y_i f_a(\\mathbf{x}_i) - L(a) \\|\\mathbf{\\delta}\\|_2 = m_i(a) - L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    为了可能发生符号翻转，新的间隔 $y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta})$ 必须能够变为非正数。这要求其下界为非正数：\n    $$\n    m_i(a) - L(a) \\|\\mathbf{\\delta}\\|_2 \\le 0\n    $$\n    这意味着任何成功的扰动 $\\mathbf{\\delta}$ 都必须满足：\n    $$\n    \\|\\mathbf{\\delta}\\|_2 \\ge \\frac{m_i(a)}{L(a)}\n    $$\n    因此，$\\frac{m_i(a)}{L(a)}$ 是导致误分类所需的最小扰动幅值的下界。\n\n2.  **情况2：错误分类的样本** ($m_i(a) \\le 0$)。\n    如果样本已经被错误分类或位于决策边界上，则条件 $y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) \\le 0$ 对于零扰动 $\\mathbf{\\delta} = \\mathbf{0}$ 是满足的。该扰动的范数为 $\\|\\mathbf{0}\\|_2 = 0$。因此，所需扰动范数的下界是 $0$。\n\n综合两种情况，样本级鲁棒性下界 $r_i(a)$ 可以表示为：\n$$\nr_i(a) = \\frac{\\max(0, m_i(a))}{L(a)} = \\frac{\\max(0, y_i f_a(\\mathbf{x}_i))}{L(a)}\n$$\n平均下界为 $\\overline{r}(a) = \\frac{1}{n} \\sum_{i=1}^n r_i(a)$。\n\n### 任务3：经验0-1风险\n\n经验 $0$-$1$ 风险，或称训练分类误差，衡量训练集中被模型错误分类的样本比例。它被定义为数据集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 上 $0$-$1$ 损失的平均值。$0$-$1$ 损失函数由 $\\mathbf{1}\\{y f(\\mathbf{x}) \\le 0\\}$ 给出，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数（如果条件为真则为 $1$，否则为 $0$）。\n因此，经验风险为：\n$$\n\\widehat{R}(f_a) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{y_i f_a(\\mathbf{x}_i) \\le 0\\} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{m_i(a) \\le 0\\}\n$$\n其计算方法是：在所有 $n=8$ 个训练样本上评估模型，检查每个样本的间隔符号，计算非正间隔的数量，然后除以 $n$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Lipschitz bound, robustness bound, and risk for a two-layer NN.\n    \"\"\"\n    # 1. Define the initial network parameters and dataset.\n    W1 = np.array([\n        [1.0, 0.5],\n        [-0.3, 1.2],\n        [0.7, -0.8]\n    ])\n    b1 = np.array([0.1, -0.2, 0.05])\n    \n    W2 = np.array([[0.9, 0.7, -0.6]])\n    b2 = -0.1\n\n    # Dataset\n    X = np.array([\n        [1.0, 1.0], [1.2, 0.9], [0.8, 1.1], [1.1, 1.2],  # Positive class\n        [-1.0, -1.0], [-1.2, -0.9], [-0.8, -1.1], [-1.1, -1.2]  # Negative class\n    ])\n    y = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n    n = len(y)\n\n    # Test suite for penalty parameter 'a'\n    a_values = [0.0, 0.5, 3.0, 50.0]\n\n    # 2. Pre-compute constant parts of the Lipschitz bound.\n    # The spectral norm (ord=2) of a 1D array is its Euclidean norm.\n    norm_W1 = np.linalg.norm(W1, ord=2)\n    norm_W2 = np.linalg.norm(W2, ord=2)\n    L0 = norm_W1 * norm_W2\n\n    # 3. Define the network's forward pass.\n    def forward_pass(x_in, w1_p, b1_p, w2_p, b2_p):\n        \"\"\"Computes the forward pass of the neural network.\"\"\"\n        # Layer 1: linear transformation + bias\n        z1 = x_in @ w1_p.T + b1_p\n        # Layer 1: ReLU activation\n        h = np.maximum(0, z1)\n        # Layer 2: linear transformation + bias\n        output = h @ w2_p.T + b2_p\n        return output.flatten()\n\n    results = []\n    # 4. Iterate through each penalty parameter 'a' and perform computations.\n    for a in a_values:\n        # Enforce the spectral norm penalty on weights\n        factor = 1.0 / (1.0 + a)\n        W1_a = W1 * factor\n        W2_a = W2 * factor\n        # Biases remain unchanged as per the problem statement\n        b1_a = b1\n        b2_a = b2\n\n        # Task 1: Compute the Lipschitz upper bound L(a)\n        # L(a) = ||W2_a|| * ||W1_a|| = (1/(1+a))^2 * ||W2|| * ||W1||\n        L_a = L0 / ((1.0 + a) ** 2)\n\n        # Compute network outputs for the current penalized model f_a\n        outputs_a = forward_pass(X, W1_a, b1_a, W2_a, b2_a)\n        \n        # Compute signed margins m_i(a) = y_i * f_a(x_i)\n        margins_a = y * outputs_a\n\n        # Task 2: Compute the average sample-wise robustness lower bound r_bar(a)\n        # r_i(a) = max(0, m_i(a)) / L(a)\n        if L_a > 0:\n            robustness_bounds = np.maximum(0, margins_a) / L_a\n        else: # Handle a -> infinity case where L_a -> 0\n            # If margin is positive, bound is infinite. If zero/negative, it's zero.\n            # Not strictly needed for the given 'a' values but good practice.\n            robustness_bounds = np.zeros_like(margins_a)\n            robustness_bounds[margins_a > 0] = np.inf\n            \n        r_bar_a = np.mean(robustness_bounds)\n\n        # Task 3: Compute the empirical 0-1 risk R_hat(f_a)\n        # R_hat = (1/n) * sum(I(m_i(a) = 0))\n        num_errors = np.sum(margins_a = 0)\n        R_hat_a = num_errors / n\n        \n        # Store results for this 'a'\n        results.append([L_a, r_bar_a, R_hat_a])\n\n    # 5. Format and print the final output as specified.\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "模型的准确率只讲述了故事的一部分；其置信度分数的可信度对于决策同样重要。本练习介绍“温度缩放”（Temperature Scaling），这是一种强大的技术，用于校准模型的概率输出，使其更好地反映事件的真实可能性。你将实现逐类的温度缩放，并使用 Brier 分数来量化这种调整如何纠正模型的过高或过低置信度，尤其是在处理类别不平衡的数据集时。",
            "id": "3113339",
            "problem": "考虑一个多类别分类器，它为每个输入输出一个实值分数向量，通常称为 logits。在深度学习中，这些 logits 通过多项式逻辑斯蒂模型与概率预测相关联，该模型将 logits 映射到一个概率单纯形。温度缩放（Temperature Scaling, TS）在构造概率之前调整这些 logits，以校正错误校准（miscalibration）。在逐类温度缩放中，每个类别都有其自己的正温度值。本问题的目标是根据基本的多项式逻辑斯蒂原理推导逐类温度缩放映射，并实现一个评估，分析这种缩放如何影响被称为布里尔分数（Brier score）的平方误差校准风险。\n\n从以下基本前提开始：\n- 该分类器是概率性的，通过应用一个保持概率单纯形且与多项式模型的最大似然估计兼容的链接函数，生成 $C$ 个类别上的分类分布。\n- 类别标签 $y \\in \\{0,\\dots,C-1\\}$ 的独热编码（one-hot encoding）是一个向量，其在真实类别对应的坐标上为 $1$，在其他位置为 $0$。\n- 概率单纯形上的平方偏差是一种有意义且经过充分检验的预测误差度量。\n\n你的任务：\n1. 从多项式逻辑斯蒂原理出发，推导应用于 logits 的逐类温度缩放变换以及由此产生的概率映射。确保逐类温度是严格的正实数，并解释较大温度与较小温度对置信度的影响。\n2. 使用独热编码，将布里尔分数（Brier score）表示为数据集上预测概率与真实标签之间平方偏差的均值。解释为什么这个量度量了概率单纯形上平方误差意义下的校准质量。\n3. 实现一个程序，对于提供的测试套件，计算逐类温度缩放前后的布里尔分数，并返回其变化量。\n\n测试套件：\n- 情况 $1$（理想情况，标签不平衡，中等温度）。令 $C = 3$。令 logits 矩阵 $Z^{(1)} \\in \\mathbb{R}^{5 \\times 3}$，标签 $y^{(1)} \\in \\{0,1,2\\}^5$ 和逐类温度 $T^{(1)} \\in \\mathbb{R}_{0}^3$ 为：\n$$\nZ^{(1)} =\n\\begin{bmatrix}\n2.5  1.0  0.0 \\\\\n1.8  0.2  0.1 \\\\\n0.5  1.3  2.0 \\\\\n1.2  0.8  1.5 \\\\\n3.0  2.5  0.5\n\\end{bmatrix},\\quad\ny^{(1)} = \\begin{bmatrix}0 \\\\ 0 \\\\ 2 \\\\ 2 \\\\ 0\\end{bmatrix},\\quad\nT^{(1)} = \\begin{bmatrix}1.8 \\\\ 1.0 \\\\ 0.7\\end{bmatrix}.\n$$\n- 情况 $2$（边界条件，中性温度）。令 $C = 3$。令\n$$\nZ^{(2)} =\n\\begin{bmatrix}\n0.0  0.0  0.0 \\\\\n2.0  2.0  2.0 \\\\\n1.5  0.5  1.0 \\\\\n0.2  0.1  0.3\n\\end{bmatrix},\\quad\ny^{(2)} = \\begin{bmatrix}0 \\\\ 2 \\\\ 1 \\\\ 1\\end{bmatrix},\\quad\nT^{(2)} = \\begin{bmatrix}1.0 \\\\ 1.0 \\\\ 1.0\\end{bmatrix}.\n$$\n- 情况 $3$（边缘情况，对主要 logit 进行强烈的过自信校正，锐化少数类别）。令 $C = 3$。令\n$$\nZ^{(3)} =\n\\begin{bmatrix}\n4.0  1.0  1.0 \\\\\n5.0  0.0  0.5 \\\\\n3.0  2.5  2.5 \\\\\n4.5  1.5  1.5 \\\\\n6.0  0.5  0.5 \\\\\n5.5  0.1  0.1\n\\end{bmatrix},\\quad\ny^{(3)} = \\begin{bmatrix}1 \\\\ 2 \\\\ 1 \\\\ 2 \\\\ 1 \\\\ 2\\end{bmatrix},\\quad\nT^{(3)} = \\begin{bmatrix}5.0 \\\\ 0.5 \\\\ 0.5\\end{bmatrix}.\n$$\n\n实现细节：\n- 对于每种情况 $i \\in \\{1,2,3\\}$，通过将多项式逻辑斯蒂链接函数应用于 $Z^{(i)}$ 来计算缩放前的概率向量。然后，通过首先将每个 logit 坐标按其类别温度的倒数进行缩放，再应用相同的多项式逻辑斯蒂链接函数，来计算逐类温度缩放后的概率。请使用数值稳定的实现。\n- 将布里尔分数计算为预测概率与 $y^{(i)}$ 的独热向量之间平方偏差在所有类别上求和后，再在样本间求均值。\n- 报告每种情况下的三个实数：缩放前的布里尔分数（$b_i$），缩放后的布里尔分数（$a_i$），以及定义为 $a_i - b_i$ 的变化量（记为 $\\Delta_i$）。不涉及物理单位、角度或百分比。\n\n最终输出格式：\n你的程序应生成单行输出，包含 $9$ 个用逗号分隔的实数，并用方括号括起来，顺序如下：\n$$\n[b_1, a_1, \\Delta_1, b_2, a_2, \\Delta_2, b_3, a_3, \\Delta_3].\n$$",
            "solution": "首先将根据指定标准对问题陈述进行验证。\n\n### 步骤 1：提取已知信息\n- **主题**：统计学习中的深度学习原理，特别是用于分类器校准的逐类温度缩放。\n- **基本概念**：\n    - 分类器是概率性的，生成 $C$ 个类别上的分类分布。\n    - 链接函数保持概率单纯形，并与多项式模型的最大似然估计兼容。\n    - 类别标签 $y \\in \\{0, \\dots, C-1\\}$ 的独热编码是一个向量，其在真实类别对应的坐标上为 $1$，在其他位置为 $0$。\n    - 概率单纯形上的平方偏差是一种预测误差的度量。\n- **任务 1**：从多项式逻辑斯蒂原理推导逐类温度缩放变换及由此产生的概率映射。解释温度的作用。逐类温度是严格的正实数 ($T \\in \\mathbb{R}_{0}^C$)。\n- **任务 2**：将布里尔分数表示为预测概率与真实独热编码标签之间平方偏差的均值。解释为什么这个量度量了校准质量。\n- **任务 3**：实现一个程序，为三个测试用例计算逐类温度缩放前后的布里尔分数，以及它们之间的变化。\n- **测试用例**：\n    - **情况 1**：$C=3$。\n        $Z^{(1)} = \\begin{bmatrix} 2.5  1.0  0.0 \\\\ 1.8  0.2  0.1 \\\\ 0.5  1.3  2.0 \\\\ 1.2  0.8  1.5 \\\\ 3.0  2.5  0.5 \\end{bmatrix}$， $y^{(1)} = [0, 0, 2, 2, 0]^\\top$， $T^{(1)} = [1.8, 1.0, 0.7]^\\top$。\n    - **情况 2**：$C=3$。\n        $Z^{(2)} = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 2.0  2.0  2.0 \\\\ 1.5  0.5  1.0 \\\\ 0.2  0.1  0.3 \\end{bmatrix}$， $y^{(2)} = [0, 2, 1, 1]^\\top$， $T^{(2)} = [1.0, 1.0, 1.0]^\\top$。\n    - **情况 3**：$C=3$。\n        $Z^{(3)} = \\begin{bmatrix} 4.0  1.0  1.0 \\\\ 5.0  0.0  0.5 \\\\ 3.0  2.5  2.5 \\\\ 4.5  1.5  1.5 \\\\ 6.0  0.5  0.5 \\\\ 5.5  0.1  0.1 \\end{bmatrix}$， $y^{(3)} = [1, 2, 1, 2, 1, 2]^\\top$， $T^{(3)} = [5.0, 0.5, 0.5]^\\top$。\n- **实现细节**：\n    - 对于一个 logit 矩阵 $Z^{(i)}$，使用多项式逻辑斯蒂链接函数计算缩放前的概率。\n    - 对于缩放后的概率，首先将每个 logit 坐标 $z_j$ 缩放 $1/T_j$ 倍，然后应用链接函数。\n    - 布里尔分数是预测概率与独热标签之间平方偏差之和在样本间的均值。\n- **输出格式**：包含 $9$ 个逗号分隔数字的单行：$[b_1, a_1, \\Delta_1, b_2, a_2, \\Delta_2, b_3, a_3, \\Delta_3]$，其中 $b_i$ 是缩放前的布里尔分数，$a_i$ 是缩放后的分数，$\\Delta_i = a_i - b_i$。\n\n### 步骤 2：使用提取的已知信息进行验证\n根据提供的标准对问题进行评估。\n- **科学性**：该问题基于统计学习和深度学习的既定原则。多项式逻辑斯蒂模型（softmax 函数）、温度缩放和布里尔分数都是模型校准领域中标准且有据可查的概念。其前提在事实上和科学上都是合理的。\n- **良态性**（Well-Posed）：问题定义清晰。输入（logits、标签、温度）被明确提供。所需输出有精确的格式规定。数学运算（softmax、布里尔分数计算）定义明确，确保每个测试用例都存在唯一解。\n- **客观性**：问题以精确、客观、形式化的数学语言陈述。它没有主观性、模糊性和基于观点的论断。\n\n该问题没有表现出任何列出的无效性缺陷。它在科学上是合理的、可形式化的、完整的、现实的且良态的。\n\n### 步骤 3：结论与行动\n此问题是**有效的**。将提供解决方案。\n\n### 解决方案\n\n本解决方案分两部分进行：首先，根据问题陈述对所要求的量进行理论推导；其次，通过实现一个计算程序来得出具体的数值结果。\n\n#### 1. 逐类温度缩放及概率映射的推导\n\n对于一个在 $C$ 个类别上进行分类的概率性分类器，与多项式逻辑斯蒂模型兼容的基本链接函数是 softmax 函数。对于单个输入实例，分类器生成一个实值分数向量，即 logits，$\\mathbf{z} = [z_0, z_1, \\dots, z_{C-1}]^\\top \\in \\mathbb{R}^C$。分配给类别 $j$ 的概率 $p_j$ 由下式给出：\n$$\np_j = \\frac{\\exp(z_j)}{\\sum_{k=0}^{C-1} \\exp(z_k)}\n$$\n概率向量 $\\mathbf{p} = [p_0, p_1, \\dots, p_{C-1}]^\\top$ 位于 $(C-1)$ 维概率单纯形上，即对所有 $j$ 都有 $p_j \\ge 0$ 且 $\\sum_{j=0}^{C-1} p_j = 1$。\n\n逐类温度缩放引入一个正温度向量 $\\mathbf{T} = [T_0, T_1, \\dots, T_{C-1}]^\\top$，其中每个 $T_j \\in \\mathbb{R}_{0}$。此缩放变换在 softmax 函数之前应用于 logits。按照规定，每个 logit $z_j$ 按其对应类别温度 $T_j$ 的倒数进行缩放。类别 $j$ 的缩放后 logit，记为 $z'_j$，为：\n$$\nz'_j = \\frac{z_j}{T_j}\n$$\n由此产生的逐类温度缩放后的概率 $p'_j$，是通过对缩放后的 logits 向量 $\\mathbf{z'} = [z'_0, z'_1, \\dots, z'_{C-1}]^\\top$ 应用 softmax 函数得到的：\n$$\np'_j = \\frac{\\exp(z'_j)}{\\sum_{k=0}^{C-1} \\exp(z'_k)} = \\frac{\\exp(z_j / T_j)}{\\sum_{k=0}^{C-1} \\exp(z_k / T_k)}\n$$\n这就是逐类温度缩放的推导出的概率映射。条件 $T_j  0$ 至关重要，以避免除以零并保持 logits 的顺序，这对于将它们解释为分数是基础性的（即，在其他条件相同的情况下，更高的 logit 应对应更高的概率）。\n\n温度 $T_j$ 的作用是调节对类别 $j$ 预测的置信度。\n- 如果 $T_j  1$，logit $z_j$ 的量值会减小。这会“软化”输出的概率分布，使其更接近于均匀分布。一个很大的温度（$T_j \\to \\infty$）会使类别 $j$ 的概率趋向于 $1/C$（假设所有其他温度也很大），从而降低模型的置信度。\n- 如果 $0  T_j  1$，logit $z_j$ 的量值会被放大。这会“锐化”输出的概率分布，使其更集中在具有最高缩放后 logit 的类别上。当 $T_j \\to 0^+$ 时，概率 $p'_j$ 会趋向于 $0$ 或 $1$，从而增加模型表达的置信度。\n- 如果 $T_j = 1$，logit $z_j$ 保持不变，该映射简化为标准的 softmax 函数。逐类缩放允许对不同类别的置信度进行差异化调整，这在类别不平衡或模型对不同类别表现出系统性不同校准误差的情况下非常有用。标准的“温度缩放”是所有 $T_j$ 都等于单个标量 $T$ 的一种特殊情况。\n\n#### 2. 用于校准评估的布里尔分数\n\n布里尔分数是一种合适的评分规则（proper scoring rule），用于度量概率预测的准确性。对于单个实例，给定一个预测概率向量 $\\mathbf{p} = [p_0, \\dots, p_{C-1}]^\\top$ 和真实类别标签 $c_{true}$，我们首先将真实标签表示为一个独热向量 $\\mathbf{y} \\in \\{0, 1\\}^C$，其中如果 $j=c_{true}$，则 $y_j = 1$，否则 $y_j=0$。\n\n该单个实例的布里尔分数是预测概率向量 $\\mathbf{p}$ 和真实独热向量 $\\mathbf{y}$ 之间的平方欧几里得距离：\n$$\nB_{\\text{instance}} = \\sum_{j=0}^{C-1} (p_j - y_j)^2\n$$\n对于一个包含 $N$ 个实例的数据集，其预测概率矩阵为 $P \\in \\mathbb{R}^{N \\times C}$，独热标签矩阵为 $Y \\in \\mathbb{R}^{N \\times C}$，总的布里尔分数是各实例分数的平均值：\n$$\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=0}^{C-1} (P_{ij} - Y_{ij})^2\n$$\n这个量之所以能度量校准质量，是因为它直接量化了预测概率相对于真实结果的均方误差。对于特定实例，一个完美校准且准确的模型会为真实类别分配概率 $1$，为所有其他类别分配概率 $0$，即 $\\mathbf{p} = \\mathbf{y}$。在这种理想情况下，该实例的分数贡献为 $0$。布里尔分数同时惩罚过自信（例如，对一个结果不正确的类别预测 $p_j=0.9$，而真实情况是 $y_j=0$）和不自信（例如，对一个结果正确的类别预测 $p_j=0.6$，而真实情况是 $y_j=1$）。最小化布里尔分数会鼓励模型生成在欧几里得意义上更接近于对应真实结果的概率单纯形顶点的概率向量。较低的布里尔分数表示更好的校准和准确性。\n\n#### 3. 计算实现\n\n将实现以下算法来解决问题的计算任务。对于每个测试用例 $i \\in \\{1, 2, 3\\}$：\n1.  设给定数据为 $Z^{(i)}$、$y^{(i)}$ 和 $T^{(i)}$。令 $N$ 为样本数，$C$ 为类别数。\n2.  从标签向量 $y^{(i)}$ 构建独热编码的标签矩阵 $Y^{(i)} \\in \\mathbb{R}^{N \\times C}$。\n3.  **缩放前**：\n    a. 对 logit 矩阵 $Z^{(i)}$ 的每一行应用数值稳定的 softmax 函数，计算概率矩阵 $P_{\\text{before}}^{(i)}$。对于 logit 向量 $\\mathbf{z}$，稳定的 softmax 函数为 $p_j = \\frac{\\exp(z_j - z_{\\max})}{\\sum_k \\exp(z_k - z_{\\max})}$，其中 $z_{\\max} = \\max_k z_k$。\n    b. 计算布里尔分数 $b_i = \\frac{1}{N} \\sum_{k=1}^{N} \\sum_{j=0}^{C-1} ( (P_{\\text{before}}^{(i)})_{kj} - Y_{kj}^{(i)} )^2$。\n4.  **缩放后**：\n    a. 计算缩放后的 logit 矩阵 $Z_{\\text{scaled}}^{(i)}$，其中 $(Z_{\\text{scaled}}^{(i)})_{kj} = Z_{kj}^{(i)} / T_j^{(i)}$。\n    b. 对 $Z_{\\text{scaled}}^{(i)}$ 的每一行应用稳定的 softmax 函数，计算概率矩阵 $P_{\\text{after}}^{(i)}$。\n    c. 计算布里尔分数 $a_i = \\frac{1}{N} \\sum_{k=1}^{N} \\sum_{j=0}^{C-1} ( (P_{\\text{after}}^{(i)})_{kj} - Y_{kj}^{(i)} )^2$。\n5.  **变化量**：计算差值 $\\Delta_i = a_i - b_i$。\n6.  收集结果 $[b_1, a_1, \\Delta_1, b_2, a_2, \\Delta_2, b_3, a_3, \\Delta_3]$ 作为最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Brier score before and after class-wise temperature scaling\n    for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Z\": np.array([\n                [2.5, 1.0, 0.0],\n                [1.8, 0.2, 0.1],\n                [0.5, 1.3, 2.0],\n                [1.2, 0.8, 1.5],\n                [3.0, 2.5, 0.5]\n            ]),\n            \"y\": np.array([0, 0, 2, 2, 0]),\n            \"T\": np.array([1.8, 1.0, 0.7])\n        },\n        {\n            \"Z\": np.array([\n                [0.0, 0.0, 0.0],\n                [2.0, 2.0, 2.0],\n                [1.5, 0.5, 1.0],\n                [0.2, 0.1, 0.3]\n            ]),\n            \"y\": np.array([0, 2, 1, 1]),\n            \"T\": np.array([1.0, 1.0, 1.0])\n        },\n        {\n            \"Z\": np.array([\n                [4.0, 1.0, 1.0],\n                [5.0, 0.0, 0.5],\n                [3.0, 2.5, 2.5],\n                [4.5, 1.5, 1.5],\n                [6.0, 0.5, 0.5],\n                [5.5, 0.1, 0.1]\n            ]),\n            \"y\": np.array([1, 2, 1, 2, 1, 2]),\n            \"T\": np.array([5.0, 0.5, 0.5])\n        }\n    ]\n\n    def stable_softmax(z, axis=-1):\n        \"\"\"\n        Numerically stable softmax function for a matrix of logits.\n        Args:\n            z (np.ndarray): A matrix of logits, shape (N, C).\n            axis (int): The axis to apply softmax over.\n        Returns:\n            np.ndarray: A matrix of probabilities, shape (N, C).\n        \"\"\"\n        z_max = np.max(z, axis=axis, keepdims=True)\n        exp_z = np.exp(z - z_max)\n        sum_exp_z = np.sum(exp_z, axis=axis, keepdims=True)\n        return exp_z / sum_exp_z\n\n    def calculate_brier_score(P, y_one_hot):\n        \"\"\"\n        Calculates the Brier score.\n        Args:\n            P (np.ndarray): Probability matrix, shape (N, C).\n            y_one_hot (np.ndarray): One-hot encoded labels, shape (N, C).\n        Returns:\n            float: The mean Brier score.\n        \"\"\"\n        # Sum of squared differences for each sample\n        sum_sq_diff = np.sum((P - y_one_hot)**2, axis=1)\n        # Mean across all samples\n        return np.mean(sum_sq_diff)\n\n    results = []\n    for case in test_cases:\n        Z = case[\"Z\"]\n        y = case[\"y\"]\n        T = case[\"T\"]\n\n        N, C = Z.shape\n\n        # Create one-hot encoded labels\n        y_one_hot = np.eye(C)[y]\n\n        # --- Before scaling ---\n        # Compute probabilities\n        P_before = stable_softmax(Z)\n        # Compute Brier score\n        brier_before = calculate_brier_score(P_before, y_one_hot)\n\n        # --- After scaling ---\n        # Scale the logits\n        Z_scaled = Z / T\n        # Compute probabilities\n        P_after = stable_softmax(Z_scaled)\n        # Compute Brier score\n        brier_after = calculate_brier_score(P_after, y_one_hot)\n\n        # --- Change ---\n        delta = brier_after - brier_before\n        \n        results.extend([brier_before, brier_after, delta])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "深度学习模型通常存在許多不同的参数配置（称为“模式”）可以达到相似的低误差。本实践将深入探究损失景观的迷人几何特性，通过研究连接两个良好解的简单线性路径是否会穿过高损失“障碍”。通过检验模式连通性和沿路径的神经表征一致性，你将深入了解模型的对称性，以及为何不同的训练过程能找到功能相似但参数迥异的解。",
            "id": "3113426",
            "problem": "要求您通过评估沿线性参数路径的经验风险和量化表示一致性来检验一个简单的两层神经网络中的模式连通性。目标是实现一个程序，该程序针对一组给定的测试用例，判断其端点在参数空间中是否通过一条低损失的线性路径相连，并报告一个表示一致性统计量。\n\n定义、模型和数据集：\n- 考虑一个带有整流线性单元（ReLU）激活函数的两层前馈神经网络。其假设类别为\n$$\nf_{\\theta}(x) \\;=\\; W_2 \\,\\sigma\\!\\left(W_1 x + b_1\\right) + b_2,\n$$\n参数为 $\\theta=(W_1,b_1,W_2,b_2)$，其中 $\\sigma(z)=\\max\\{0,z\\}$ 逐元素应用，输入 $x\\in\\mathbb{R}^d$，隐藏层宽度为 $h$，输出维度为 $1$。在所有测试用例中，均使用 $d=h=2$，$b_1=\\mathbf{0}\\in\\mathbb{R}^2$ 和 $b_2=0$。\n- 使用数据集 $\\{(x_i,y_i)\\}_{i=1}^n$，其中包含 $n=6$ 个输入向量和标量标签：\n  - 输入 $x_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$，$x_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$，$x_3=\\begin{bmatrix}1\\\\1\\end{bmatrix}$，$x_4=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$，$x_5=\\begin{bmatrix}-1\\\\1\\end{bmatrix}$，$x_6=\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$。\n  - 标签由一个固定目标函数定义：$y_i=\\max\\{0, x_{i,1}\\}+\\max\\{0, x_{i,2}\\}$，其中 $i\\in\\{1,\\dots,6\\}$。\n- 经验风险是均方误差（MSE），它是经验风险最小化（ERM）的一个特例：\n$$\nL(\\theta)\\;=\\;\\frac{1}{n}\\sum_{i=1}^n \\left( f_{\\theta}(x_i) - y_i \\right)^2.\n$$\n\n线性参数路径与目标：\n- 对于两个参数集 $\\theta_a$ 和 $\\theta_b$，定义参数空间中的线性插值\n$$\n\\theta(t) \\;=\\; (1-t)\\,\\theta_a + t\\,\\theta_b,\\quad t\\in[0,1],\n$$\n并逐分量地应用于 $W_1$、$b_1$、$W_2$ 和 $b_2$。\n- 对于一个包含 $m=21$ 个在 $[0,1]$ 区间内等距值的有限网格 $T=\\{t_k\\}_{k=0}^{m-1}$（即 $t_k=\\frac{k}{20}$），评估沿路径的经验风险：$\\{L(\\theta(t_k))\\}_{k=0}^{m-1}$。\n- 定义损失屏障高度\n$$\nB(\\theta_a,\\theta_b)\\;=\\;\\max_{t\\in T} L(\\theta(t)) \\;-\\; \\max\\{L(\\theta_a),\\,L(\\theta_b)\\}.\n$$\n如果 $B(\\theta_a,\\theta_b)\\le \\delta$，则称参数对 $(\\theta_a,\\theta_b)$ 在容差 $\\delta$ 下是线性模式连接的。在本问题中，使用 $\\delta=10^{-9}$。\n- 将参数 $\\theta$ 下数据集上的隐藏表示定义为堆叠的隐藏层激活值\n$$\nH(\\theta) \\;\\in\\; \\mathbb{R}^{n\\times h},\\qquad H(\\theta)_{i,:} \\;=\\; \\sigma\\!\\left(W_1 x_i + b_1\\right)^{\\top}.\n$$\n令 $v(\\theta)\\in\\mathbb{R}^{nh}$ 为 $H(\\theta)$ 经过行主序扁平化后的向量化形式。定义余弦相似度\n$$\n\\mathrm{sim}(u,v)\\;=\\;\\frac{\\langle u,v\\rangle}{\\|u\\|_2\\,\\|v\\|_2},\n$$\n约定如下：如果 $\\|u\\|_2=\\|v\\|_2=0$，则 $\\mathrm{sim}(u,v)=1$；如果 $\\|u\\|_2$ 或 $\\|v\\|_2$ 中恰好有一个为 $0$，则 $\\mathrm{sim}(u,v)=0$。\n- 对于路径 $\\theta(t)$，将 $t$ 时刻的表示一致性定义为\n$$\nR(t)\\;=\\;\\max\\!\\left\\{\\,\\mathrm{sim}\\big(v(\\theta(t)),v(\\theta_a)\\big),\\;\\mathrm{sim}\\big(v(\\theta(t)),v(\\theta_b)\\big)\\right\\}.\n$$\n用其最差情况下的表示一致性来总结一条路径\n$$\nR_{\\min}\\;=\\;\\min_{t\\in T} R(t).\n$$\n\n测试套件：\n评估以下三个测试用例。在所有用例中，均取 $b_1=\\mathbf{0}$ 和 $b_2=0$。\n- 用例 #1 (相同端点)：\n  - $\\theta_a$: $W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$, $W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n  - $\\theta_b$: $W_{1,b}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$, $W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n- 用例 #2 (置换隐藏单元)：\n  - $\\theta_a$: $W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$, $W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n  - $\\theta_b$: $W_{1,b}=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$, $W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n- 用例 #3 (微小扰动)：\n  - $\\theta_a$: $W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$, $W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n  - $\\theta_b$: $W_{1,b}=\\begin{bmatrix}1  0.05\\\\ 0.05  1\\end{bmatrix}$, $W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n\n您必须实现的内容：\n- 对于每个用例，计算：\n  - 在网格 $T$ 上的屏障 $B(\\theta_a,\\theta_b)$ 和布尔连通性决策 $\\mathbf{conn}=\\big(B(\\theta_a,\\theta_b)\\le \\delta\\big)$。\n  - 在同一网格 $T$ 上的最差情况表示一致性 $R_{\\min}$。\n- 输出格式：\n  - 您的程序应生成单行输出，其中包含一个列表，每个测试用例对应一个元素。每个元素本身必须是一个包含两个元素、形式为 $[\\mathbf{conn}, r]$ 的列表，其中 $\\mathbf{conn}$ 是一个布尔值，$r$ 是一个浮点数，等于 $R_{\\min}$ 四舍五入到六位小数的值。\n  - 具体来说，输出必须看起来像一行上的单个 Python 风格的列表字面量，例如 $[[\\text{boolean}, \\text{float}], [\\text{boolean}, \\text{float}], [\\text{boolean}, \\text{float}]]$.\n\n仅限科学起点：\n- 仅使用经验风险最小化（ERM）、均方误差（MSE）、整流线性单元（ReLU）的定义、参数空间中的线性插值和余弦相似度的基本原理来推导您的计算。\n- 不涉及角度或物理单位，因此除了上述定义之外，不需要其他单位约定。\n\n您的任务：\n- 实现一个完整的程序，按照上述三个用例的规定执行计算，并以所描述的精确输出格式打印结果。",
            "solution": "用户要求对一个简单的两层神经网络进行模式连通性分析。这需要验证问题陈述的有效性，如果有效，则提供详细的解决方案和相应的 Python 实现。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **模型 ($f_{\\theta}(x)$)**: 一个带有 ReLU 激活函数的两层前馈神经网络，$f_{\\theta}(x) = W_2 \\sigma(W_1 x + b_1) + b_2$。\n*   **参数 ($\\theta$)**: $\\theta=(W_1,b_1,W_2,b_2)$。\n*   **激活函数 ($\\sigma$)**: 逐元素 ReLU, $\\sigma(z)=\\max\\{0,z\\}$。\n*   **维度**: 输入维度 $d=2$，隐藏层宽度 $h=2$，输出维度 $1$。\n*   **固定参数**: 对于所有测试用例，偏置均为零：$b_1=\\mathbf{0}\\in\\mathbb{R}^2$ 和 $b_2=0$。\n*   **数据集**: $n=6$ 个输入-输出对 $\\{(x_i,y_i)\\}_{i=1}^6$。\n    *   输入 $X$: $x_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}, x_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}, x_3=\\begin{bmatrix}1\\\\1\\end{bmatrix}, x_4=\\begin{bmatrix}1\\\\-1\\end{bmatrix}, x_5=\\begin{bmatrix}-1\\\\1\\end{bmatrix}, x_6=\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$。\n    *   标签 $Y$: $y_i=\\max\\{0, x_{i,1}\\}+\\max\\{0, x_{i,2}\\}$。\n*   **经验风险 ($L(\\theta)$)**: 均方误差 (MSE), $L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n ( f_{\\theta}(x_i) - y_i )^2$。\n*   **参数路径**: 线性插值 $\\theta(t) = (1-t)\\theta_a + t\\theta_b$ for $t\\in[0,1]$。\n*   **评估网格 ($T$)**: 在 $[0,1]$ 区间上的 $m=21$ 个等距点，$t_k=k/(m-1) = k/20$。\n*   **损失屏障 ($B(\\theta_a,\\theta_b)$)**: $B(\\theta_a,\\theta_b) = \\max_{t\\in T} L(\\theta(t)) - \\max\\{L(\\theta_a), L(\\theta_b)\\}$。\n*   **连通性条件**: 如果 $B(\\theta_a,\\theta_b) \\le \\delta$，则为线性模式连接，其中容差 $\\delta=10^{-9}$。\n*   **隐藏表示 ($H(\\theta)$)**: $n \\times h$ 矩阵，其中 $H(\\theta)_{i,:} = \\sigma(W_1 x_i + b_1)^{\\top}$。\n*   **向量化表示 ($v(\\theta)$)**: $H(\\theta)$ 按行主序扁平化后的向量，$v(\\theta) \\in \\mathbb{R}^{nh}$。\n*   **余弦相似度 ($\\mathrm{sim}(u,v)$)**: $\\frac{\\langle u,v\\rangle}{\\|u\\|_2\\,\\|v\\|_2}$。约定：若 $\\|u\\|_2=\\|v\\|_2=0$ 则 $\\mathrm{sim}=1$；若只有一个范数为零则 $\\mathrm{sim}=0$。\n*   **表示一致性 ($R(t)$)**: $R(t) = \\max\\{\\mathrm{sim}(v(\\theta(t)),v(\\theta_a)), \\mathrm{sim}(v(\\theta(t)),v(\\theta_b))\\}$。\n*   **最差情况一致性 ($R_{\\min}$)**: $R_{\\min} = \\min_{t\\in T} R(t)$。\n*   **测试用例**:\n    1.  **相同**: $\\theta_a=\\theta_b$，$W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}, W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n    2.  **置换**: $\\theta_a$ 同用例1。$\\theta_b$ 的参数为 $W_{1,b}=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}, W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n    3.  **扰动**: $\\theta_a$ 同用例1。$\\theta_b$ 的参数为 $W_{1,b}=\\begin{bmatrix}1  0.05\\\\ 0.05  1\\end{bmatrix}, W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n*   **输出规范**: 单行打印一个 Python 列表的列表：`[[conn, r], [conn, r], [conn, r]]`，其中 `conn` 是布尔值，$r$ 是 $R_{\\min}$ 四舍五入到6位小数后的值。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n该问题在深度学习理论领域具有科学依据，特别是关于损失景观和模型对称性的研究。问题是良构的，为唯一的确定性计算提供了所有必要的数据、参数和定义。语言客观且正式。没有矛盾、信息缺失或违反科学原则之处。计算任务是可行的，并且所用定义在该领域是标准的。\n\n**步骤 3：结论与行动**\n\n该问题是**有效的**。将开发并实现一个解决方案。\n\n### 推导与方案设计\n\n该问题要求实现一个计算过程，以评估简单神经网络参数空间中线性路径的性质。我们将通过首先定义核心组件，然后将它们应用于指定的测试用例来构建解决方案。\n\n**1. 模型与损失函数**\n由于零偏置约束（$b_1=\\mathbf{0}, b_2=0$），模型简化为：\n$$\nf_{\\theta}(x) = W_2 \\sigma(W_1 x)\n$$\n参数为 $\\theta = (W_1, W_2)$。经验风险 $L(\\theta)$ 是在数据集上，$f_{\\theta}(x_i)$ 与目标标签 $y_i=\\max\\{0, x_{i,1}\\}+\\max\\{0, x_{i,2}\\}$ 之间差值平方的均值。\n\n分析用例#1中参数所代表的函数是有益的。这里，$W_{1,a} = I$（单位矩阵），$W_{2,a} = \\begin{bmatrix}1  1\\end{bmatrix}$。网络计算如下：\n$$\nf_{\\theta_a}(x) = \\begin{bmatrix}1  1\\end{bmatrix} \\sigma\\left(\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\\right) = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix}\\max\\{0, x_1\\} \\\\ \\max\\{0, x_2\\}\\end{bmatrix} = \\max\\{0, x_1\\} + \\max\\{0, x_2\\}\n$$\n这恰好是目标函数 $y$。因此，对于 $\\theta_a$（以及任何其他定义相同函数的 $\\theta$），损失 $L(\\theta)$ 为 $0$。\n\n**2. 路径插值与分析**\n对于每个测试用例，我们考虑两个端点 $\\theta_a$ 和 $\\theta_b$。线性路径定义为 $\\theta(t) = ((1-t)W_{1,a} + tW_{1,b}, (1-t)W_{2,a} + tW_{2,b})$，其中 $t \\in [0, 1]$。我们使用网格 $T=\\{0, 1/20, 2/20, \\dots, 1\\}$ 对该路径进行离散化。\n\n对于每个 $t_k \\in T$，我们必须计算：\n-   **损失 $L(\\theta(t_k))$**: 这包括使用插值参数 $\\theta(t_k)$ 对 $n=6$ 个数据点中的每一个执行前向传播，并计算 MSE。\n-   **表示一致性 $R(t_k)$**: 这需要：\n    a.  计算隐藏表示 $H(\\theta_a)$、$H(\\theta_b)$ 和 $H(\\theta(t_k))$。$H(\\theta)$ 的第 $(i,:)$ 行为 $\\sigma(W_1 x_i)^\\top$。\n    b.  将这些矩阵向量化为 $v(\\theta_a)$、$v(\\theta_b)$ 和 $v(\\theta(t_k))$。\n    c.  使用提供的定义和对零向量的约定，计算余弦相似度 $\\mathrm{sim}(v(\\theta(t_k)), v(\\theta_a))$ 和 $\\mathrm{sim}(v(\\theta(t_k)), v(\\theta_b))$。\n    d.  $R(t_k)$ 是这两个相似度中的最大值。\n\n**3. 最终指标**\n在网格 $T$ 上评估路径属性后，我们为每个测试用例计算汇总统计量：\n-   **屏障高度 $B(\\theta_a, \\theta_b)$**: 根据收集的损失 $\\{L(\\theta(t_k))\\}$ 计算得出，即 $\\max_{k} L(\\theta(t_k)) - \\max\\{L(\\theta(t_0)), L(\\theta(t_{m-1}))\\}$。\n-   **连通性 $\\mathbf{conn}$**: 一个布尔值，如果 $B(\\theta_a, \\theta_b) \\le 10^{-9}$ 则为 `True`，否则为 `False`。\n-   **最差情况一致性 $R_{\\min}$**: 收集到的一致性值中的最小值，即 $\\min_{k} R(t_k)$。\n\n该过程将系统地应用于三个测试用例中的每一个。\n\n-   **用例#1 (相同)**: 由于 $\\theta_a=\\theta_b$，路径是恒定的：对所有 $t$，$\\theta(t)=\\theta_a$。所有 $k$ 的损失 $L(\\theta(t_k))$ 都将为 $0$。因此 $B=0$，$\\mathbf{conn}$ 为 `True`。表示 $v(\\theta(t_k))$ 也是恒定的，并且与 $v(\\theta_a)$ 和 $v(\\theta_b)$ 相同，因此对于所有 $k$，$\\mathrm{sim}=1$。因此 $R_{\\min}=1$。\n\n-   **用例#2 (置换)**: 在这里，$\\theta_b$ 表示与 $\\theta_a$ 相同的函数，但隐藏神经元的位置互换。$W_{1,b}$ 是一个置换矩阵，$W_{2,b}$ 是对称的，因此置换的效果被抵消了。所以 $L(\\theta_a)=L(\\theta_b)=0$。然而，已知在这些置换解之间的线性插值会穿过一个高损失区域，在该区域网络的有效容量会降低。我们预计 $B  \\delta$ 且 $\\mathbf{conn}$ 为 `False`。表示也会沿路径发散，尤其是在 $t=0.5$ 附近，导致 $R_{\\min}  1$。\n\n-   **用例#3 (扰动)**: $\\theta_b$ 是 $\\theta_a$ 的一个小扰动。两个点都位于同一个低损失盆地内。它们之间的线性路径预计将停留在这个盆地中。我们预期损失屏障非常小，$B \\approx 0$，所以 $\\mathbf{conn}$ 应为 `True`。隐藏表示在整个路径上应保持非常相似，从而得到一个非常接近 $1$ 的 $R_{\\min}$。\n\n实现将遵循此逻辑，为前向传播、损失计算、表示提取和相似度计算创建辅助函数，然后遍历测试用例以计算并格式化最终结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the mode connectivity problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Definitions ---\n\n    # Dataset\n    X = np.array([\n        [1, 0], [0, 1], [1, 1], [1, -1], [-1, 1], [-1, -1]\n    ])\n    # Target function y = max(0, x_1) + max(0, x_2)\n    Y = np.maximum(0, X[:, 0]) + np.maximum(0, X[:, 1])\n    n = X.shape[0]\n\n    # Parameters for analysis\n    m = 21  # Number of grid points\n    delta = 1e-9  # Connectivity tolerance\n    t_grid = np.linspace(0.0, 1.0, m)\n\n    # --- Helper Functions ---\n\n    def relu(z):\n        \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n        return np.maximum(0, z)\n\n    def forward_pass(W1, W2, x):\n        \"\"\"Computes the network output for a single input vector x.\"\"\"\n        # Biases b1 and b2 are zero\n        a1 = relu(W1 @ x)\n        output = W2 @ a1\n        return output[0] # Return scalar\n\n    def mse_loss(W1, W2, X_data, Y_data):\n        \"\"\"Computes the Mean Squared Error loss over the dataset.\"\"\"\n        squared_errors = []\n        for i in range(X_data.shape[0]):\n            y_pred = forward_pass(W1, W2, X_data[i])\n            squared_errors.append((y_pred - Y_data[i])**2)\n        return np.mean(squared_errors)\n\n    def get_vectorized_representation(W1, X_data):\n        \"\"\"Computes the vectorized hidden representation v(theta).\"\"\"\n        # H(theta) is n x h\n        h = W1.shape[0]\n        n_data = X_data.shape[0]\n        H = np.zeros((n_data, h))\n        for i in range(n_data):\n            H[i, :] = relu(W1 @ X_data[i])\n        return H.flatten() # Row-major flattening by default\n\n    def cosine_similarity(u, v):\n        \"\"\"Computes cosine similarity with special conventions for zero vectors.\"\"\"\n        norm_u = np.linalg.norm(u)\n        norm_v = np.linalg.norm(v)\n\n        if norm_u == 0 and norm_v == 0:\n            return 1.0\n        if norm_u == 0 or norm_v == 0:\n            return 0.0\n        \n        return np.dot(u, v) / (norm_u * norm_v)\n\n    # --- Test Case Definitions ---\n\n    # Case #1: Identical endpoints\n    W1_a1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_a1 = np.array([[1.0, 1.0]])\n    W1_b1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_b1 = np.array([[1.0, 1.0]])\n\n    # Case #2: Permuted hidden units\n    W1_a2 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_a2 = np.array([[1.0, 1.0]])\n    W1_b2 = np.array([[0.0, 1.0], [1.0, 0.0]])\n    W2_b2 = np.array([[1.0, 1.0]])\n\n    # Case #3: Small perturbation\n    W1_a3 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_a3 = np.array([[1.0, 1.0]])\n    W1_b3 = np.array([[1.0, 0.05], [0.05, 1.0]])\n    W2_b3 = np.array([[1.0, 1.0]])\n\n    test_cases = [\n        (W1_a1, W2_a1, W1_b1, W2_b1),\n        (W1_a2, W2_a2, W1_b2, W2_b2),\n        (W1_a3, W2_a3, W1_b3, W2_b3)\n    ]\n\n    final_results = []\n    \n    # --- Main Calculation Loop ---\n    \n    for W1_a, W2_a, W1_b, W2_b in test_cases:\n        \n        path_losses = []\n        path_consistencies = []\n\n        # Pre-compute representations at endpoints\n        v_a = get_vectorized_representation(W1_a, X)\n        v_b = get_vectorized_representation(W1_b, X)\n\n        for t in t_grid:\n            # Interpolate parameters\n            W1_t = (1 - t) * W1_a + t * W1_b\n            W2_t = (1 - t) * W2_a + t * W2_b\n\n            # 1. Calculate loss along the path\n            loss_t = mse_loss(W1_t, W2_t, X, Y)\n            path_losses.append(loss_t)\n\n            # 2. Calculate representation consistency\n            v_t = get_vectorized_representation(W1_t, X)\n            \n            sim_a = cosine_similarity(v_t, v_a)\n            sim_b = cosine_similarity(v_t, v_b)\n            \n            R_t = max(sim_a, sim_b)\n            path_consistencies.append(R_t)\n\n        # Calculate final metrics for the case\n        loss_endpoints = max(path_losses[0], path_losses[-1])\n        max_path_loss = max(path_losses)\n        \n        barrier = max_path_loss - loss_endpoints\n        conn = barrier = delta\n        \n        R_min = min(path_consistencies)\n        \n        # Store result in the required format\n        final_results.append([conn, round(R_min, 6)])\n\n    # --- Format and Print Output ---\n    \n    # Format each inner list to string, join with commas, and wrap in brackets\n    str_results = [f\"[{'True' if res[0] else 'False'}, {res[1]}]\" for res in final_results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        }
    ]
}