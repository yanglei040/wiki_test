{
    "hands_on_practices": [
        {
            "introduction": "本次实践将让你超越将多层感知机（MLP）视为黑箱的传统视角。你将从第一性原理出发，亲手构建一个双层ReLU网络，以完美地插值给定的训练数据。这项练习  旨在揭示ReLU网络作为分段线性函数逼近器的内在能力，并让你对模型的表达能力和函数间隔等概念建立起深刻的直观理解。",
            "id": "3151121",
            "problem": "您需要实现并分析一个最小但数学上一致的分类设置，该设置使用带有修正线性单元（ReLU）非线性的两层前馈网络（多层感知机 (MLP)），以探索过参数化状态下的零训练误差，并将所得间隔与测试性能相关联。将输入视为一维实数，并考虑 $\\{-1, +1\\}$ 中的二元标签。总体目标是，从第一性原理出发，构建一个能精确插值训练数据的显式过参数化两层 ReLU MLP，计算该模型的有原则的间隔，并根据指定的基准真相分类器评估其测试准确率。\n\n基本基础和定义：\n- 两层前馈网络实现的是平移和缩放后的修正线性单元 (ReLU) 激活值的有限和加上一个输出偏置的函数；即，输出形式为 $f(x) = \\sum_{j=1}^{m} a_j \\max(0, w_j x + b_j) + c$，其中 $m$ 是宽度（隐藏单元的数量）。\n- 修正线性单元 (ReLU) 定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。\n- 由 $f$ 导出的分类器预测 $\\widehat{y}(x) = \\mathrm{sign}(f(x)) \\in \\{-1,+1\\}$，其中如果 $t  0$，则 $\\mathrm{sign}(t) = -1$，否则为 $+1$。\n- 在数据集 $\\{(x_i,y_i)\\}_{i=1}^{n}$ 上的经验 0–1 分类损失是 $n$ 个点上指示函数 $\\mathbb{1}\\{\\widehat{y}(x_i) \\neq y_i\\}$ 的平均值，但在本任务中，您必须确保对所有 $i$ 都有 $\\widehat{y}(x_i) = y_i$，即零训练误差。\n- 对于一维分段线性函数 $f$，在闭区间 $[x_{\\min}, x_{\\max}]$ 上的（全局）利普希茨常数定义为 $L = \\sup_{x \\in [x_{\\min}, x_{\\max}]} |f'(x)|$，其中 $f'$ 几乎处处存在且是分段常数。此 $L$ 等于由隐藏单元导出的各线性片段上 $f$ 的最大绝对斜率。\n- 将训练集上的函数间隔定义为 $\\gamma = \\min_{i} \\frac{y_i f(x_i)}{L}$，并约定如果 $L = 0$，则 $\\gamma = +\\infty$。\n\n您的构建任务：\n- 对于每个提供的训练集，构建一个宽度至少为 $n$ 的显式两层 ReLU MLP，该网络能精确插值给定的训练样本 $(x_i, y_i)$，从而实现零训练误差。您不能使用迭代训练；相反，应直接根据 $f(x_i) = y_i$ 的要求和 ReLU 网络的分段线性结构来推导权重。您的构建应是有原则的，仅依赖于 ReLU 和形成分段线性函数的基本属性，并且必须对严格递增的输入 $x_1  x_2  \\cdots  x_n$ 有效。\n- 计算利普希茨常数 $L$，即连续 $x_i$ 之间线性段的最大绝对斜率。\n- 使用上述公式计算间隔 $\\gamma$。\n\n测试性能的基准真相：\n- 使用固定的基准真相分类器 $h^\\star(x) = \\mathrm{sign}(x)$ 来定义任何实数 $x$ 处的真实标签，并约定 $\\mathrm{sign}(0) = +1$。\n\n测试准确率规范：\n- 对于每个测试用例，在由该用例的训练输入所跨越的闭区间 $[x_{\\min}, x_{\\max}]$（其中 $x_{\\min} = \\min_i x_i$ 且 $x_{\\max} = \\max_i x_i$）上的一个包含 $M$ 个测试点的均匀网格上，评估由您构建的 $f$ 导出的分类器。将测试准确率计算为 $\\mathrm{sign}(f(x))$ 与 $h^\\star(x)$ 相匹配的测试点所占的比例（以小数形式表示）。\n\n测试套件：\n- 共有四个测试用例。每个用例包含一个三元组 $(\\text{输入}, \\text{标签}, M)$，其中：\n    1. 用例 A（类别分离良好，理想路径）：输入 $[-2.0, -1.0, 1.0, 2.0]$，标签 $[-1, -1, +1, +1]$，$M = 401$。\n    2. 用例 B（高度局部交替，边界压力）：输入 $[-0.1, 0.0, 0.1]$，标签 $[-1, +1, -1]$，$M = 401$。\n    3. 用例 C（中等距离上的交替标签，振荡性）：输入 $[-1.0, -0.5, 0.0, 0.5, 1.0]$，标签 $[-1, +1, -1, +1, -1]$，$M = 801$。\n    4. 用例 D（统一负标签，极端平滑边界情况）：输入 $[-2.0, -0.5, 0.5, 2.0]$，标签 $[-1, -1, -1, -1]$，$M = 801$。\n\n要求和约束：\n- 构建的网络必须是过参数化的，即宽度 $m$ 至少等于训练样本数 $n$，并且严格大于输入维度（$1$）。\n- 在所有用例中，您必须通过精确插值而非优化来达到零训练误差。\n- 对于每个用例，输出一个数对 $[\\gamma, \\text{accuracy}]$，其中 $\\gamma$ 是所定义的间隔，$\\text{accuracy}$ 是在指定网格上根据 $h^\\star(x)$ 计算出的测试准确率；两者都应表示为浮点数。如果 $L = 0$，则返回 $\\gamma = +\\infty$。\n- 最终输出格式：您的程序应生成单行文本，其中包含四个用例的结果，格式为逗号分隔的列表的列表，形式为 \n  $[\\,[\\gamma_1, a_1], [\\gamma_2, a_2], [\\gamma_3, a_3], [\\gamma_4, a_4]\\,]$。\n\n本问题不涉及物理单位或角度单位。所有数值答案必须是纯小数。",
            "solution": "该问题是有效的，因为它在数学上是明确定义的、自洽的，并且基于神经网络和统计学习理论的既定原则。该任务要求直接构建一个函数并计算其属性，这是一个可行且客观的过程。\n\n对于每个测试用例，解决方案分四个主要步骤进行：\n1.  **构建插值 MLP**：我们构建一个两层 ReLU 网络，该网络能精确插值训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$。\n2.  **计算利普希茨常数 ($L$)**：我们确定所构建函数的最大绝对斜率。\n3.  **计算间隔 ($\\gamma$)**：我们应用提供的函数间隔公式。\n4.  **评估测试准确率**：我们在指定的测试网格上，将我们的分类器的预测与基准真相分类器进行比较。\n\n### 1. 构建插值 MLP\n问题要求一个形式为 $f(x) = \\sum_{j=1}^{m} a_j \\max(0, w_j x + b_j) + c$ 的两层前馈网络来插值训练数据，即对所有 $i=1, \\dots, n$ 满足 $f(x_i) = y_i$。我们已知输入是排序的：$x_1  x_2  \\dots  x_n$。\n\n一个由 ReLU 单元之和组成的函数是连续且分段线性的。这种函数的“拐点”或不可微点出现在 ReLU 函数的参数为零的地方。我们可以通过在每个 $x_i$ 处设置拐点来构建一个穿过所有点 $(x_i, y_i)$ 的分段线性函数。这导向了对网络参数的一种特定选择。\n\n让我们定义一个宽度为 $m=n$ 的网络，其中 $n$ 是数据点的数量。这满足过参数化约束 $m \\ge n$。对于每个隐藏单元 $j \\in \\{1, \\dots, n\\}$，我们设置其权重和偏置以在 $x_j$ 处创建一个拐点。一个简单的选择是 $w_j = 1$ 和 $b_j = -x_j$，因此激活函数为 $\\max(0, x - x_j)$。\n\n完整的函数可以表示为：\n$$f(x) = c + \\sum_{j=1}^{n} a_j \\max(0, x - x_j)$$\n这个函数是一个有效的两层 ReLU 网络。第一层计算 $z_j = x - x_j$，第二层结合这些 $z_j$ 的 ReLU 激活值。\n\n为了确保 $f(x_i)=y_i$，我们必须确定系数 $c$ 和 $a_j$。函数的一阶导数 $f'(x)$ 是一个阶跃函数。在任何区间 $(x_k, x_{k+1})$ 内的斜率由 $\\sum_{j=1}^k a_j$ 给出。为了使 $f(x)$ 成为这些点的线性插值函数，其在区间 $(x_k, x_{k+1})$ 内的斜率必须是：\n$$s_k = \\frac{y_{k+1} - y_k}{x_{k+1} - x_k} \\quad \\text{对于 } k=1, \\dots, n-1$$\n我们还强制函数在数据范围之外是平的，即对于 $x  x_1$ 和 $x  x_n$，将斜率设为 $0$。令 $s_0 = 0$ 和 $s_n = 0$。\n\n系数 $a_j$ 代表在 $x_j$ 处斜率的变化。因此：\n- 在 $x_1$ 处，斜率从 $s_0=0$ 变为 $s_1$。所以，$a_1 = s_1 - s_0 = s_1$。\n- 对于 $k=2, \\dots, n-1$，在 $x_k$ 处，斜率从 $s_{k-1}$ 变为 $s_k$。所以，$a_k = s_k - s_{k-1}$。\n- 在 $x_n$ 处，斜率从 $s_{n-1}$ 变为 $s_n=0$。所以，$a_n = s_n - s_{n-1} = -s_{n-1}$。\n\n最后，输出偏置 $c$ 由条件 $f(x_1) = y_1$ 确定。当 $x=x_1$ 时，对于 $j \\geq 1$，所有 $\\max(0, x_1-x_j)$ 项都为零。因此，$f(x_1) = c$，这意味着 $c = y_1$。\n\n最终构建的函数是：\n$$f(x) = y_1 + \\sum_{j=1}^{n} a_j \\max(0, x - x_j)$$\n这种构建保证了对所有 $i$ 都有 $f(x_i) = y_i$，从而实现零训练误差。\n\n### 2. 计算利普希茨常数 ($L$)\n在区间 $[x_{\\min}, x_{\\max}] = [x_1, x_n]$ 上的利普希茨常数 $L$ 是函数导数的最大绝对值，即 $L = \\sup_{x \\in [x_1, x_n]} |f'(x)|$。\n由于我们的函数 $f(x)$ 在段 $(x_k, x_{k+1})$ 上是斜率为 $s_k$ 的分段线性函数，利普希茨常数就是最大绝对斜率：\n$$L = \\max_{k=1, \\dots, n-1} |s_k| = \\max_{k=1, \\dots, n-1} \\left| \\frac{y_{k+1} - y_k}{x_{k+1} - x_k} \\right|$$\n如果 $n \\le 1$ 或者所有斜率都为零，则 $L=0$。\n\n### 3. 计算间隔 ($\\gamma$)\n函数间隔定义为 $\\gamma = \\min_{i} \\frac{y_i f(x_i)}{L}$。根据我们的构建，$f(x_i) = y_i$。标签为 $y_i \\in \\{-1, +1\\}$。因此，对于每个训练点 $i$：\n$$y_i f(x_i) = y_i^2 = 1$$\n间隔的表达式简化为：\n$$\\gamma = \\min_{i} \\frac{1}{L} = \\frac{1}{L}$$\n根据问题定义，如果 $L=0$，则间隔为 $\\gamma = +\\infty$。\n\n### 4. 评估测试准确率\n测试准确率是在跨越训练数据区间 $[x_{\\min}, x_{\\max}]$ 的一个包含 $M$ 个点的均匀网格上计算的。对于每个测试点 $x_{test}$：\n- 基准真相标签是 $y_{true} = h^\\star(x_{test}) = \\mathrm{sign}(x_{test})$，其中 $\\mathrm{sign}(0)=+1$。\n- 模型的预测是 $\\hat{y}_{pred} = \\mathrm{sign}(f(x_{test}))$，其中对于 $t \\ge 0$，$\\mathrm{sign}(t)=+1$，对于 $t0$，$\\mathrm{sign}(t)=-1$。\n准确率是 $y_{true} = \\hat{y}_{pred}$ 的测试点所占的比例。\n\n对提供的四个测试用例中的每一个都实施此过程。例如，在用例 A 中，斜率分别为 $s_1=0$、$s_2=1$ 和 $s_3=0$，得出 $L=1$ 和 $\\gamma=1/1=1$。构建的函数 $f(x) = -1 + \\max(0, x+1) - \\max(0, x-1)$ 在其定义域上与基准真相 $h^\\star(x)$ 完全一致，从而得到 $1.0$ 的测试准确率。其他用例的求解方法类似。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It defines the test cases, processes each one, and prints the formatted results.\n    \"\"\"\n    test_cases = [\n        # Case A (well-separated classes, happy path)\n        ({'inputs': [-2.0, -1.0, 1.0, 2.0], 'labels': [-1, -1, 1, 1], 'M': 401}),\n        # Case B (highly localized alternation, boundary stress)\n        ({'inputs': [-0.1, 0.0, 0.1], 'labels': [-1, 1, -1], 'M': 401}),\n        # Case C (alternating labels over moderate distances, oscillatory)\n        ({'inputs': [-1.0, -0.5, 0.0, 0.5, 1.0], 'labels': [-1, 1, -1, 1, -1], 'M': 801}),\n        # Case D (uniform negative labels, extreme smoothness edge case)\n        ({'inputs': [-2.0, -0.5, 0.5, 2.0], 'labels': [-1, -1, -1, -1], 'M': 801})\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = analyze_case(case['inputs'], case['labels'], case['M'])\n        all_results.append(result)\n\n    # Format and print the final output as a comma-separated list of lists.\n    # e.g., [[gamma1,acc1],[gamma2,acc2],...]\n    print(f\"[{','.join(f'[{g},{a}]' for g, a in all_results)}]\")\n\ndef analyze_case(inputs, labels, M):\n    \"\"\"\n    Constructs an MLP, computes its margin and test accuracy for a single case.\n\n    Args:\n        inputs (list): Sorted list of 1D input features.\n        labels (list): List of binary labels {-1, 1}.\n        M (int): Number of points in the test grid.\n\n    Returns:\n        list: A pair [gamma, accuracy].\n    \"\"\"\n    x = np.array(inputs, dtype=np.float64)\n    y = np.array(labels, dtype=np.float64)\n    n = len(x)\n\n    # 1. Compute slopes of the piecewise linear interpolant.\n    # m_k is the slope in the interval [x_k, x_{k+1}].\n    if n > 1:\n        m = (y[1:] - y[:-1]) / (x[1:] - x[:-1])\n    else:\n        m = np.array([])\n\n    # 2. Compute the Lipschitz constant L.\n    # L is the maximum absolute slope over the domain [x_min, x_max].\n    L = np.max(np.abs(m)) if m.size > 0 else 0.0\n\n    # 3. Compute the functional margin gamma.\n    # Since f(x_i) = y_i and y_i^2 = 1, gamma simplifies to 1/L.\n    gamma = math.inf if L == 0.0 else 1.0 / L\n\n    # 4. Construct the interpolating function f(x).\n    # f(x) = y_1 + sum_{j=1..n} a_j * ReLU(x - x_j), where a_j are slope changes.\n    a = np.zeros(n, dtype=np.float64)\n    if n > 1:\n        # a_1 = m_1 (in 0-indexing: a[0] = m[0])\n        a[0] = m[0]\n        # a_k = m_k - m_{k-1} for k=2..n-1\n        for i in range(1, n - 1):\n            a[i] = m[i] - m[i - 1]\n        # a_n = -m_{n-1} (in 0-indexing: a[n-1] = -m[n-2])\n        a[n - 1] = -m[n - 2]\n    \n    def f(xt_val):\n        # The function value is computed from its piecewise linear definition.\n        val = y[0]\n        # This loop implements sum_{j=1..n} a_j * max(0, xt - x_j)\n        for j in range(n):\n            val += a[j] * max(0, xt_val - x[j])\n        return val\n\n    # 5. Compute test accuracy.\n    x_min, x_max = x[0], x[-1]\n    test_grid = np.linspace(x_min, x_max, M, dtype=np.float64)\n    \n    matches = 0\n    for xt in test_grid:\n        # Ground truth: h*(x) = sign(x), with sign(0)=+1.\n        y_true = 1 if xt >= 0 else -1\n        \n        # Model's prediction: sign(f(x)), with sign(t>=0)=+1.\n        fx_val = f(xt)\n        y_pred = 1 if fx_val >= 0 else -1\n        \n        if y_true == y_pred:\n            matches += 1\n            \n    accuracy = matches / M\n    \n    return [gamma, accuracy]\n\nsolve()\n```"
        },
        {
            "introduction": "激活函数的选择是一项关键的架构决策，它决定了MLP所学习函数的特征。本次练习  要求你将经典的修正线性单元（ReLU）与现代的平滑激活函数，如Sigmoid线性单元（SiLU）和高斯误差线性单元（GELU），进行比较。通过测量代表函数分段线性和平滑度的经验指标，你将直观地理解这一选择如何在清晰的线性区域和更平滑的决策边界之间进行权衡。",
            "id": "3151225",
            "problem": "给定一个双层前馈多层感知机（MLP）族，它们的权重和偏置相同，但激活函数不同。该网络将一个二维输入映射到一个标量输出。目的是比较平滑激活函数（特别是Sigmoid线性单元（SiLU）和高斯误差线性单元（GELU））与修正线性单元（ReLU）的选择，如何影响两个可测量的属性：一个是一维切片上的线性区域的经验计数，另一个是二维网格上决策函数的有限差分平滑度度量。\n\n基本原理。使用具有一个宽度为 $h$ 的隐藏层的MLP的标准前馈组合，输入为 $x \\in \\mathbb{R}^{2}$，权重矩阵为 $W_{1} \\in \\mathbb{R}^{h \\times 2}$ 和 $W_{2} \\in \\mathbb{R}^{1 \\times h}$，偏置为 $b_{1} \\in \\mathbb{R}^{h}$ 和 $b_{2} \\in \\mathbb{R}$。标量输出为\n$$\nf(x) \\;=\\; W_{2}\\,\\phi(W_{1}x + b_{1}) \\;+\\; b_{2},\n$$\n其中 $\\phi$ 是一个逐元素的激活函数。考虑以下三种激活函数：\n- 修正线性单元（ReLU）：$\\phi(z) = \\max\\{0,z\\}$，\n- Sigmoid线性单元（SiLU）：$\\phi(z) = z \\cdot \\sigma(z)$，其中逻辑S型函数为 $\\sigma(z) = \\frac{1}{1+e^{-z}}$，\n- 高斯误差线性单元（GELU）：使用标准的平滑近似\n$$\n\\phi(z) \\;=\\; \\tfrac{1}{2}\\,z\\left(1 + \\tanh\\!\\big(\\sqrt{\\tfrac{2}{\\pi}}\\,(z + 0.044715\\,z^{3})\\big)\\right).\n$$\n\n架构与参数。固定隐藏层宽度为 $h=10$。使用三角函数以弧度为单位确定性地定义权重和偏置（要求角度以弧度为单位）：\n- 对于索引 $i \\in \\{0,1,\\dots,9\\}$ 和 $j \\in \\{0,1\\}$，\n$$\n(W_{1})_{i,j} \\;=\\; 0.7\\,\\sin(i + 2j) \\;+\\; 0.05\\,\\cos(3i - j),\n$$\n- 对于 $i \\in \\{0,1,\\dots,9\\}$，\n$$\n(b_{1})_{i} \\;=\\; 0.1\\,\\sin(i),\n$$\n- 对于 $k \\in \\{0,1,\\dots,9\\}$，\n$$\n(W_{2})_{0,k} \\;=\\; 0.6\\,\\cos(k) \\;+\\; 0.05\\,\\sin(2k),\n$$\n- 输出偏置为 $b_{2}=0$。\n\n两个经验性度量。\n1) 一维线性区域的经验计数。考虑对角线切片 $t \\mapsto x(t) = (t,t)$，其中 $t \\in [-2,2]$。在 $[-2,2]$ 上采样 $N=1025$ 个等距点 $t_{0},t_{1},\\dots,t_{N-1}$，并评估 $y_{\\ell}=f(x(t_{\\ell}))$。令一阶差分为 $s_{\\ell} = y_{\\ell+1}-y_{\\ell}$（对于 $\\ell \\in \\{0,1,\\dots,N-2\\}$），二阶差分为 $d_{\\ell} = s_{\\ell+1}-s_{\\ell}$（对于 $\\ell \\in \\{0,1,\\dots,N-3\\}$）。定义一个拐点阈值 $\\tau = 10^{-2}$。经验性线性区域计数为\n$$\nR \\;=\\; \\sum_{\\ell=0}^{N-3} \\mathbf{1}\\big(|d_{\\ell}|  \\tau\\big),\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数，当条件为真时等于 $1$，否则为 $0$。\n\n2) 通过有限差分梯度变化的二维经验平滑度。在方形域 $[-2,2]\\times[-2,2]$ 上，构建一个每轴有 $G=81$ 个点的均匀网格，间距为 $\\Delta = \\frac{4}{G-1}$。在所有网格点上评估 $f$ 以获得一个标量场 $F[u,v]$，其中 $u,v \\in \\{0,1,\\dots,G-1\\}$。通过中心差分来近似内部点的梯度：\n$$\n\\partial_{x}F[u,v] \\approx \\frac{F[u,v+1] - F[u,v-1]}{2\\Delta}, \\quad\n\\partial_{y}F[u,v] \\approx \\frac{F[u+1,v] - F[u-1,v]}{2\\Delta},\n$$\n对于 $u,v \\in \\{1,2,\\dots,G-2\\}$。令 $g[u,v] \\in \\mathbb{R}^{2}$ 表示内部的这个近似梯度。定义平均邻域梯度变化\n$$\nS \\;=\\; \\frac{1}{E}\\left(\\sum_{u=1}^{G-2}\\sum_{v=1}^{G-3} \\left\\| g[u,v+1]-g[u,v]\\right\\|_{2}\n\\;+\\; \\sum_{u=1}^{G-3}\\sum_{v=1}^{G-2} \\left\\| g[u+1,v]-g[u,v]\\right\\|_{2}\\right),\n$$\n其中 $E = 2\\,(G-2)\\,(G-3)$ 是内部水平和垂直相邻点对的总数，$\\|\\cdot\\|_{2}$ 表示欧几里得范数。标量 $S$ 作为离散平滑度的代理：较小的值表示决策函数的梯度在整个平面上变化更平滑。\n\n任务。实现一个程序，按上述方式构建确定性的权重和偏置，为三种激活函数（$\\mathrm{ReLU}$、$\\mathrm{SiLU}$ 和 $\\mathrm{GELU}$）中的每一种实例化MLP，并为每种激活函数计算配对 $(R,S)$，其中 $R$ 是经验性一维线性区域计数，$S$ 是上面定义的经验性平滑度度量。\n\n测试套件。使用以下激活函数集：\n- 情况 1：$\\mathrm{ReLU}$，\n- 情况 2：$\\mathrm{SiLU}$，\n- 情况 3：$\\mathrm{GELU}$。\n\n答案规格与输出格式。\n- 对于每种情况，计算定义的 $R$ 和 $S$。将 $R$ 报告为整数，将 $S$ 四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的扁平列表形式的结果，顺序为 $[R_{\\mathrm{ReLU}},S_{\\mathrm{ReLU}},R_{\\mathrm{SiLU}},S_{\\mathrm{SiLU}},R_{\\mathrm{GELU}},S_{\\mathrm{GELU}}]$，并用方括号括起来。例如，一个有效的输出格式是 `[12,0.037591,0,0.004112,0,0.003987]`。\n- 不提供外部输入；所有参数均按上文所述固定。",
            "solution": "用户希望在使用三种不同的激活函数时，分析和比较一个双层多层感知机（MLP）的属性：修正线性单元（ReLU）、Sigmoid线性单元（SiLU）和高斯误差线性单元（GELU）。比较基于两个经验性指标：一个用于测量函数输出在一维切片上的“拐点”（非线性点）数量，另一个用于测量函数梯度在二维域上的平滑度。\n\n### **问题验证**\n\n首先，对问题陈述进行验证。\n\n**步骤1：提取已知条件**\n\n- **MLP 模型**：$f(x) \\;=\\; W_{2}\\,\\phi(W_{1}x + b_{1}) \\;+\\; b_{2}$，其中 $x \\in \\mathbb{R}^{2}$。\n- **架构**：隐藏层宽度 $h=10$。\n- **激活函数 ($\\phi$)**：ReLU ($\\max\\{0,z\\}$)、SiLU ($z \\cdot \\sigma(z)$) 以及GELU的一个近似 ($\\tfrac{1}{2}\\,z\\left(1 + \\tanh(\\sqrt{\\tfrac{2}{\\pi}}\\,(z + 0.044715\\,z^{3}))\\right)$)。\n- **参数**：$W_{1} \\in \\mathbb{R}^{10 \\times 2}$，$b_{1} \\in \\mathbb{R}^{10}$，$W_{2} \\in \\mathbb{R}^{1 \\times 10}$，以及 $b_{2}=0$。这些值由特定的三角函数公式确定性地定义。\n- **度量1（线性区域计数, $R$）**：\n    - 函数切片：$x(t) = (t,t)$，其中 $t \\in [-2,2]$。\n    - 采样：$N=1025$ 个点。\n    - 计算：$R = \\sum \\mathbf{1}(|d_{\\ell}|  \\tau)$，其中 $d_{\\ell}$ 是函数在切片上取值的二阶差分，$\\tau = 10^{-2}$ 是一个阈值。\n- **度量2（平滑度, $S$）**：\n    - 域：$[-2,2]\\times[-2,2]$。\n    - 网格：每轴 $G=81$ 个点。\n    - 计算：$S$ 是网格上相邻内部点之间梯度差异范数的平均值，通过中心差分计算。公式已明确给出。\n- **任务**：为三种激活函数中的每一种计算配对 $(R,S)$。\n- **输出**：一个扁平的、逗号分隔的计算值列表 $[R_{\\mathrm{ReLU}},S_{\\mathrm{ReLU}},R_{\\mathrm{SiLU}},S_{\\mathrm{SiLU}},R_{\\mathrm{GELU}},S_{\\mathrm{GELU}}]$。\n\n**步骤2：使用提取的已知条件进行验证**\n\n该问题在神经网络和数值分析领域具有科学依据。所有函数、参数和程序都经过了精确的数学定义。问题的确定性确保了其适定性，从而导向一个唯一的、可验证的解。术语客观且明确。该问题未违反任何无效性标准。例如，它在科学上并非不健全、不完整或不适定。\n\n**步骤3：结论与行动**\n\n该问题被判定为 **有效**。将提供一个解决方案。\n\n### **基于原理的解决方案设计**\n\n问题的核心是实现MLP函数 $f(x)$，然后用它为每个指定的激活函数计算两个度量指标 $R$ 和 $S$。\n\n**1. 模型实现**\n\nMLP由 $f(x) = W_{2} \\phi(W_{1}x + b_{1}) + b_{2}$ 定义。这可以实现为一系列矩阵-向量运算。首先，根据给定的涉及正弦和余弦函数的确定性公式生成权重和偏置。关键是要确保角度被视为弧度，这是像NumPy这样的标准数值库的默认设置。\n\n根据它们的数学定义实现三种激活函数：ReLU、SiLU和GELU。ReLU是一个简单的取最大值操作。SiLU涉及逻辑S型函数 $\\sigma(z) = (1+e^{-z})^{-1}$。GELU则以一个涉及双曲正切函数的特定平滑近似形式给出。\n\n构建一个通用的MLP模型函数。它接受一组输入向量 $x$ 和一个激活函数 $\\phi$，并计算相应的输出。这样设计可以高效地处理批量输入，例如，同时处理一维切片或二维网格上的所有点。对于一个形状为 $(\\text{num\\_points}, 2)$ 的输入矩阵 $X$，计算过程如下：\n1.  计算预激活值：$Z = X W_{1}^T + b_{1}$。广播机制处理偏置向量 $b_1$ 的加法。\n2.  应用激活函数：$A = \\phi(Z)$。这是一个逐元素操作。\n3.  计算最终输出：$Y = A W_{2}^T + b_{2}$。\n\n**2. 度量R：经验性线性区域计数**\n问题将此度量定义在输入空间的一维切片 $x(t)=(t,t)$ 上。\n1.  在区间 $[-2,2]$ 中生成 $N=1025$ 个等距点 $t_{\\ell}$。这些点构成了输入向量 $x(t_{\\ell})$。\n2.  在这些点上评估MLP模型 $f$ 以获得输出 $y_{\\ell}$。\n3.  计算一阶差分 $s_{\\ell} = y_{\\ell+1} - y_{\\ell}$ 和二阶差分 $d_{\\ell} = s_{\\ell+1} - s_{\\ell}$。二阶差分是二阶导数的离散近似。对于像ReLU网络产生的这种分段线性函数，二阶导数除了在“拐点”处（此处未定义，为狄拉克δ函数）外，处处为零。数值二阶差分将在这些点上呈现一个尖峰。\n4.  通过对二阶差分绝对值 $|d_{\\ell}|$ 超过阈值 $\\tau = 10^{-2}$ 的实例进行求和，来计算拐点数 $R$。对于平滑函数（SiLU、GELU），二阶导数是良定义且连续的，因此预期二阶差分会很小，不会超过阈值，从而导致 $R=0$。对于ReLU，预期会得到一个正的计数值。\n\n**3. 度量S：经验性平滑度**\n此度量量化了函数在二维网格上的平滑度。\n1.  在方形域 $[-2,2] \\times [-2,2]$ 上创建一个 $G \\times G$（其中 $G=81$）的均匀网格。在此网格的每个点上评估MLP $f$，得到一个标量场 $F[u,v]$。\n2.  使用中心差分公式在每个内部网格点上近似 $f$ 的梯度。这提供了一个向量场 $g[u,v] \\approx \\nabla f$。\n3.  平滑度度量 $S$ 被计算为该梯度场在相邻点之间的平均变化。这涉及计算水平和垂直方向上相邻网格点梯度向量之差的欧几里得范数。\n4.  这些范数差异的总和通过考虑的总对数 $E = 2(G-2)(G-3)$进行归一化。较小的 $S$ 值表示梯度变化更慢，意味着函数更平滑。我们预期ReLU，作为仅为 $C^0$ 连续的函数，其 $S$ 值会比无限可微的SiLU和GELU函数更大。\n该实现将为三种激活函数中的每一种系统地执行这些计算，并按规定格式化结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes empirical linearity and smoothness metrics for an MLP \n    with different activation functions.\n    \"\"\"\n    # ---- 1. Define Constants and Parameters ----\n    h = 10  # Hidden layer width\n\n    # Parameters for measure R (Linear Region Count)\n    N = 1025\n    tau = 1e-2\n\n    # Parameters for measure S (Smoothness)\n    G = 81\n    delta = 4 / (G - 1)\n\n    # ---- 2. Define Activation Functions ----\n    def relu(z):\n        \"\"\"Rectified Linear Unit activation function.\"\"\"\n        return np.maximum(0, z)\n\n    def silu(z):\n        \"\"\"Sigmoid Linear Unit activation function.\"\"\"\n        # This implementation is numerically stable for large negative z.\n        return np.where(z >= 0, z / (1 + np.exp(-z)), z * np.exp(z) / (1 + np.exp(z)))\n\n    def gelu(z):\n        \"\"\"Gaussian Error Linear Unit activation function (tanh approximation).\"\"\"\n        c = np.sqrt(2 / np.pi)\n        return 0.5 * z * (1 + np.tanh(c * (z + 0.044715 * z**3)))\n\n    activation_functions = {\n        \"ReLU\": relu,\n        \"SiLU\": silu,\n        \"GELU\": gelu,\n    }\n\n    # ---- 3. Generate Deterministic Weights and Biases ----\n    # W1: shape (10, 2)\n    i_w1 = np.arange(h).reshape(h, 1)\n    j_w1 = np.arange(2).reshape(1, 2)\n    W1 = 0.7 * np.sin(i_w1 + 2 * j_w1) + 0.05 * np.cos(3 * i_w1 - j_w1)\n\n    # b1: shape (10,)\n    i_b1 = np.arange(h)\n    b1 = 0.1 * np.sin(i_b1)\n\n    # W2: shape (1, 10)\n    k_w2 = np.arange(h)\n    W2 = (0.6 * np.cos(k_w2) + 0.05 * np.sin(2 * k_w2)).reshape(1, h)\n\n    # b2: scalar\n    b2 = 0.0\n\n    # ---- 4. Define Helper Functions for MLP and Metrics ----\n    def create_mlp_model(activation_fn):\n        \"\"\"Factory function to create an MLP model with a specific activation.\"\"\"\n        def model(x):\n            # x has shape (num_points, 2)\n            pre_activation = x @ W1.T + b1\n            hidden_output = activation_fn(pre_activation)\n            output = hidden_output @ W2.T + b2\n            return output.flatten()\n        return model\n\n    def calculate_R(f, n_samples, threshold):\n        \"\"\"Calculates the empirical linear region count R.\"\"\"\n        t = np.linspace(-2, 2, n_samples)\n        x_slice = np.stack([t, t], axis=-1)\n        y = f(x_slice)\n        \n        s = np.diff(y)  # First differences\n        d = np.diff(s)  # Second differences\n        \n        R = np.sum(np.abs(d) > threshold)\n        return int(R)\n\n    def calculate_S(f, grid_size, grid_spacing):\n        \"\"\"Calculates the empirical smoothness measure S.\"\"\"\n        ax_pts = np.linspace(-2, 2, grid_size)\n        xx, yy = np.meshgrid(ax_pts, ax_pts, indexing='xy')\n        grid_points = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n\n        F = f(grid_points).reshape(grid_size, grid_size)\n        \n        # Approximate gradient via central differences. F[u,v] is f(x_v, y_u).\n        dF_dx = (F[1:-1, 2:] - F[1:-1, :-2]) / (2 * grid_spacing)\n        dF_dy = (F[2:, 1:-1] - F[:-2, 1:-1]) / (2 * grid_spacing)\n        \n        g = np.stack([dF_dx, dF_dy], axis=-1)\n        \n        # Horizontal gradient variation\n        g_diff_h = g[:, 1:, :] - g[:, :-1, :]\n        sum1 = np.sum(np.linalg.norm(g_diff_h, axis=-1))\n\n        # Vertical gradient variation\n        g_diff_v = g[1:, :, :] - g[:-1, :, :]\n        sum2 = np.sum(np.linalg.norm(g_diff_v, axis=-1))\n\n        E = 2.0 * (grid_size - 2) * (grid_size - 3)\n        S = (sum1 + sum2) / E\n        return S\n\n    # ---- 5. Main Execution Loop ----\n    test_cases = [\"ReLU\", \"SiLU\", \"GELU\"]\n    final_results = []\n    \n    for case_name in test_cases:\n        activation_fn = activation_functions[case_name]\n        mlp_model = create_mlp_model(activation_fn)\n        \n        R = calculate_R(mlp_model, N, tau)\n        S = calculate_S(mlp_model, G, delta)\n        \n        final_results.append(str(R))\n        final_results.append(f\"{S:.6f}\")\n\n    # ---- 6. Format and Print Final Output ----\n    print(f\"[{','.join(final_results)}]\")\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "经典的统计学习理论预测测试误差会呈现U型曲线，但现代的过参数化模型通常表现出不同的行为。在本次实践中 ，你将设置一个数值实验，亲眼见证“双峰下降”（double descent）现象。通过系统地增加模型容量，使其超过插值点，你将观察到测试误差如何在高度过参数化的区域出人意料地再次下降，这对传统的偏见-方差权衡提出了挑战。",
            "id": "3151120",
            "problem": "您的任务是通过扫描参数-样本比，并将插值阈值与测试误差的尖峰联系起来，以经验方式验证前馈多层感知机（MLP）中的双重下降现象。核心设置如下。\n\n从平方损失下的经验风险最小化这一基础出发。设输入空间为 $\\mathbb{R}^d$。考虑一个带有修正线性单元（ReLU）非线性的两层前馈多层感知机（MLP），其中隐藏层权重是固定的，只有输出层权重被训练。对于一个输入向量 $x \\in \\mathbb{R}^d$，MLP 计算\n$$\n\\phi(x) = \\big(\\sigma(w_1^\\top x + b_1), \\ldots, \\sigma(w_m^\\top x + b_m)\\big) \\in \\mathbb{R}^m,\n$$\n其中 $\\sigma(z) = \\max\\{0, z\\}$ 是 ReLU 激活函数，$w_j \\in \\mathbb{R}^d$ 和 $b_j \\in \\mathbb{R}$ 是固定的隐藏层参数，$m$ 是隐藏单元的数量。预测输出为\n$$\n\\hat{y}(x) = a^\\top \\phi(x),\n$$\n其中 $a \\in \\mathbb{R}^m$ 是被训练的输出层权重。给定训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$，定义设计矩阵\n$$\n\\Phi \\in \\mathbb{R}^{n \\times m}, \\quad \\Phi_{ij} = \\sigma(w_j^\\top x_i + b_j).\n$$\n在平方损失下的经验风险最小化问题旨在寻找最小化 $\\sum_{i=1}^n (\\hat{y}(x_i) - y_i)^2$ 的 $a$，这是一个线性最小二乘问题。最小范数解由 Moore–Penrose 伪逆给出：\n$$\na^\\star = \\Phi^+ y,\n$$\n其中 $y = (y_1, \\ldots, y_n)^\\top$ 且 $\\Phi^+$ 表示 $\\Phi$ 的伪逆。当训练均方误差变为零时，达到插值阈值，这通常在 $m$ 增长使得 $\\Phi$ 达到满行秩时发生，从而系统 $\\Phi a = y$ 有解。参数-样本比定义为 $p/n$，其中 $p$ 是被训练的参数数量。在此设置中，$p = m$。\n\n双重下降现象指的是一种典型行为，即测试误差作为模型容量的函数，最初减少，然后在插值阈值（训练误差降至零的位置）附近增加，之后随着容量的进一步增加再次减少。您的程序将从一个教师网络构建合成数据，并在 $m$ 值的扫描范围内测量测试均方误差，这些 $m$ 值对应于不同的参数-样本比。\n\n数据生成与评估协议：\n- 从标准正态分布中独立抽取输入 $x \\in \\mathbb{R}^d$。\n- 使用具有少量 ReLU 单元的固定教师网络生成标签：\n$$\ny = \\sum_{k=1}^{m_{\\text{teacher}}} \\beta_k \\, \\sigma(u_k^\\top x + c_k) + \\varepsilon,\n$$\n其中 $u_k \\in \\mathbb{R}^d$、$c_k \\in \\mathbb{R}$ 和 $\\beta_k \\in \\mathbb{R}$ 是固定的教师参数，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 是标准差为 $\\sigma$ 的独立高斯噪声。使用 $m_{\\text{teacher}} = 5$。\n- 对于每个选定的 $m$，通过计算 $a^\\star = \\Phi^+ y$ 来训练学生 MLP。\n- 计算训练均方误差\n$$\n\\mathrm{MSE}_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}(x_i) - y_i \\right)^2,\n$$\n以及在一个大小为 $n_{\\text{test}}$ 的独立测试集上的测试均方误差，\n$$\n\\mathrm{MSE}_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\left(\\hat{y}(x_i^{\\text{test}}) - y_i^{\\text{test}} \\right)^2.\n$$\n设置 $n_{\\text{test}} = \\max\\{3n, 200\\}$。\n\n插值阈值与尖峰检测：\n- 定义插值容差 $\\epsilon = 10^{-10}$ 和尖峰边际 $\\delta = 0.1$（表示 $10\\%$）。\n- 对于给定的 $n$ 和一系列扫描的 $m$ 值，将插值阈值 $m_{\\text{interp}}$ 定义为扫描中使得 $\\mathrm{MSE}_{\\text{train}}(m) \\le \\epsilon$ 的最小 $m$。\n- 如果在插值阈值处的 $\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}})$ 超过所有其他 $m$ 值的 $\\mathrm{MSE}_{\\text{test}}(m)$ 的中位数至少 $(1+\\delta)$ 倍，则定义为出现尖峰。形式上，如果\n$$\n\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}})  (1 + \\delta) \\cdot \\operatorname{median}\\left(\\{\\mathrm{MSE}_{\\text{test}}(m) : m \\in \\mathcal{M}, m \\ne m_{\\text{interp}}\\}\\right),\n$$\n则输出布尔值 $\\mathrm{True}$；否则输出 $\\mathrm{False}$。如果扫描中没有 $m$ 达到插值，则输出 $\\mathrm{False}$。\n\n扫描设计：\n- 对于每个测试用例，通过用比率 $\\{0.5, 0.8, 1.0, 1.2, 1.5\\}$ 缩放 $n$ 并四舍五入到最近的整数来构建隐藏单元数列表 $\\mathcal{M}$，确保 $m \\ge 1$。即，\n$$\n\\mathcal{M} = \\left\\{ \\max\\left(1, \\left\\lfloor r \\cdot n + 0.5 \\right\\rfloor \\right) : r \\in \\{0.5, 0.8, 1.0, 1.2, 1.5\\} \\right\\}.\n$$\n在单个测试用例中，确保所有学生隐藏层参数 $\\{(w_j, b_j)\\}_{j=1}^{m_{\\max}}$ 在最大的 $m_{\\max} = \\max \\mathcal{M}$ 处一次性固定，而较小的 $m$ 的模型重用前 $m$ 个特征，以使扫描具有可比性。\n\n测试套件：\n在以下四个测试用例上运行您的程序。对于每个用例，报告一个布尔值，指示根据上述规则是否在插值阈值处检测到尖峰。每个用例使用独立的随机种子来确定性地固定所有随机性。\n\n- 用例 1：$n = 60$, $d = 20$, $\\sigma = 0.5$, $\\text{seed} = 0$。\n- 用例 2：$n = 60$, $d = 20$, $\\sigma = 0.0$, $\\text{seed} = 1$。\n- 用例 3：$n = 24$, $d = 8$, $\\sigma = 0.5$, $\\text{seed} = 2$。\n- 用例 4：$n = 80$, $d = 30$, $\\sigma = 0.8$, $\\text{seed} = 3$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $\\left[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4\\right]$，其中每个 $\\mathrm{result}_i$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$，对应于用例 $i$ 的尖峰检测结果。",
            "solution": "该问题陈述是有效的。它具有科学依据，问题设定良好、客观，并为在随机特征模型中进行双重下降现象的经验研究提供了一套完整且一致的指令。其中没有矛盾、模糊或事实不准确之处。因此，我们可以着手解决。\n\n目标是经验性地研究一个简化的多层感知机（MLP）中的双重下降现象。该现象描述了测试误差作为模型容量的函数呈现出的一个先U形后下降的曲线。最初的下降对应于传统欠参数化区域中的偏差-方差权衡。随后测试误差在插值阈值附近达到峰值，此时模型刚好有足够的容量来完美拟合训练数据。超过此点，在过参数化区域中，测试误差出人意料地再次下降。\n\n该方法论是基于教师-学生框架构建的受控数值实验。\n\n**1. 模型设定与训练**\n\n该模型是一个带有修正线性单元（ReLU）激活函数 $\\sigma(z) = \\max\\{0, z\\}$ 的两层前馈网络。对于一个输入 $x \\in \\mathbb{R}^d$，输出为 $\\hat{y}(x) = a^\\top \\phi(x)$，其中 $\\phi(x) = (\\sigma(w_1^\\top x + b_1), \\ldots, \\sigma(w_m^\\top x + b_m))$ 是一个特征激活向量。此问题的一个关键方面是，隐藏层参数 $\\{w_j, b_j\\}_{j=1}^m$ 在随机初始化后是固定的。只有输出层权重 $a \\in \\mathbb{R}^m$ 被训练。这使得非线性神经网络问题转化为一个高维特征空间中的线性回归问题，即所谓的随机特征模型。隐藏单元的数量 $m$ 直接对应于可训练参数的数量，并作为我们衡量模型容量的指标。\n\n给定 $n$ 个训练样本 $\\{(x_i, y_i)\\}_{i=1}^n$，我们构建设计矩阵 $\\Phi \\in \\mathbb{R}^{n \\times m}$，其中每个元素为 $\\Phi_{ij} = \\sigma(w_j^\\top x_i + b_j)$。目标是找到最小化平方损失 $\\mathcal{L}(a) = \\sum_{i=1}^n (y_i - a^\\top \\phi(x_i))^2 = \\|y - \\Phi a\\|_2^2$ 的权重向量 $a$。\n\n这个线性最小二乘问题的解，同时具有最小欧几里得范数 $\\|a\\|_2$，由 $a^\\star = \\Phi^+ y$ 给出。这里，$\\Phi^+$ 是设计矩阵 $\\Phi$ 的 Moore-Penrose 伪逆，而 $y = (y_1, \\ldots, y_n)^\\top$ 是训练标签的向量。伪逆为任何形状的 $\\Phi$ 提供了唯一、稳定的解，正确地处理了欠参数化（$m  n$，通常为满列秩）和过参数化（$m  n$，通常为满行秩）两种情况。\n\n**2. 数据生成与评估**\n\n我们采用教师-学生设置来创建一个具有已知真实值的合成数据集。\n- 一个具有 $m_{\\text{teacher}} = 5$ 个隐藏单元的同架构固定“教师”网络生成标签：$y = \\sum_{k=1}^{m_{\\text{teacher}}} \\beta_k \\, \\sigma(u_k^\\top x + c_k) + \\varepsilon$。输入向量 $x \\in \\mathbb{R}^d$ 从标准正态分布中抽取。高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 被加到输出上，以模拟固有的测量误差或未建模的效应。\n- 然后，一个“学生”模型（我们的可训练模型）在一组包含 $n$ 个这样的数据点上进行训练。\n- 训练后的学生模型的性能通过在训练集和一个独立的、更大的测试集（大小为 $n_{\\text{test}} = \\max\\{3n, 200\\}$）上的均方误差（MSE）进行评估。\n  - $\\mathrm{MSE}_{\\text{train}}(m) = \\frac{1}{n} \\|\\Phi a^\\star - y\\|^2$\n  - $\\mathrm{MSE}_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\| \\Phi_{\\text{test}} a^\\star - y_{\\text{test}}\\|^2$\n\n**3. 实验流程与尖峰检测**\n\n实验的核心是当我们扫描模型容量 $m$ 时，追踪 $\\mathrm{MSE}_{\\text{test}}(m)$ 的变化。问题定义了一个特定的扫描协议：容量集合 $\\mathcal{M}$ 是通过将样本数 $n$ 与比率 $\\{0.5, 0.8, 1.0, 1.2, 1.5\\}$ 相乘生成的。这组比率旨在探测模型在欠参数化区域（$m/n  1$）、插值阈值附近（$m/n \\approx 1$）以及过参数化区域（$m/n  1$）的行为。为确保扫描结果具有可比性，最大模型（$m_{\\max} = \\max \\mathcal{M}$）的随机特征只生成一次，较小的模型仅使用这些特征的一个子集。\n\n双重下降假说预测，在模型首次完美拟合训练数据的点附近，$\\mathrm{MSE}_{\\text{test}}$ 会出现一个尖峰。这个点被形式化为插值阈值 $m_{\\text{interp}}$，定义为 $\\mathcal{M}$ 中使 $\\mathrm{MSE}_{\\text{train}}(m) \\le \\epsilon$ 成立的最小 $m$，其中容差 $\\epsilon = 10^{-10}$。\n\n如果在此阈值处的测试误差显著高于其他容量下的典型测试误差，则称检测到“尖峰”。该条件由不等式给出：\n$$\n\\mathrm{MSE}_{\\text{test}}(m_{\\text{interp}})  (1 + \\delta) \\cdot \\operatorname{median}\\left(\\{\\mathrm{MSE}_{\\text{test}}(m) : m \\in \\mathcal{M}, m \\ne m_{\\text{interp}}\\}\\right)\n$$\n其中尖峰边际为 $\\delta = 0.1$。如果满足此条件，则该测试用例的结果为 $\\mathrm{True}$；否则为 $\\mathrm{False}$。如果扫描中没有 $m$ 达到插值，结果也为 $\\mathrm{False}$。\n\n实现将对四个指定的测试用例中的每一个执行这整个过程，使用给定的随机种子以保证可复现性，并报告尖峰检测测试的布尔结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_case(n, d, sigma, seed):\n    \"\"\"\n    Runs a single simulation case for the double descent experiment.\n    \n    Args:\n        n (int): Number of training samples.\n        d (int): Input dimension.\n        sigma (float): Standard deviation of label noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        bool: True if a spike is detected, False otherwise.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Generate Teacher Network\n    m_teacher = 5\n    u_teacher = np.random.randn(m_teacher, d)\n    c_teacher = np.random.randn(m_teacher)\n    beta_teacher = np.random.randn(m_teacher)\n    \n    def teacher_model(X):\n        activations = np.maximum(0, X @ u_teacher.T + c_teacher)\n        return activations @ beta_teacher\n\n    # 2. Generate Training and Test Data\n    X_train = np.random.randn(n, d)\n    noise_train = sigma * np.random.randn(n)\n    y_train = teacher_model(X_train) + noise_train\n\n    n_test = max(3 * n, 200)\n    X_test = np.random.randn(n_test, d)\n    noise_test = sigma * np.random.randn(n_test)\n    y_test = teacher_model(X_test) + noise_test\n\n    # 3. Define Model Sweep\n    ratios = [0.5, 0.8, 1.0, 1.2, 1.5]\n    m_values = sorted(list(set([max(1, int(np.round(r * n))) for r in ratios])))\n    m_max = m_values[-1]\n\n    # 4. Generate Student Network's Fixed Features\n    W_student = np.random.randn(m_max, d)\n    b_student = np.random.randn(m_max)\n\n    train_mses = []\n    test_mses = []\n\n    # 5. Sweep through model capacities (m)\n    for m in m_values:\n        # Select the first m features\n        W_m = W_student[:m, :]\n        b_m = b_student[:m]\n        \n        # Construct design matrix for training\n        Phi_train = np.maximum(0, X_train @ W_m.T + b_m)\n        \n        # Train model using Moore-Penrose pseudoinverse\n        # a_star = pinv(Phi_train) @ y_train\n        a_star = np.linalg.pinv(Phi_train) @ y_train\n        \n        # Evaluate Training MSE\n        y_train_pred = Phi_train @ a_star\n        train_mse = np.mean((y_train_pred - y_train) ** 2)\n        train_mses.append(train_mse)\n        \n        # Evaluate Test MSE\n        Phi_test = np.maximum(0, X_test @ W_m.T + b_m)\n        y_test_pred = Phi_test @ a_star\n        test_mse = np.mean((y_test_pred - y_test) ** 2)\n        test_mses.append(test_mse)\n\n    # 6. Analyze Results for Spike Detection\n    epsilon = 1e-10\n    delta = 0.1\n    \n    np_train_mses = np.array(train_mses)\n    np_test_mses = np.array(test_mses)\n    \n    # Find interpolation threshold m_interp\n    interp_indices = np.where(np_train_mses = epsilon)[0]\n    \n    if len(interp_indices) == 0:\n        # No model achieved interpolation\n        return False\n        \n    idx_interp = interp_indices[0]\n    # m_interp = m_values[idx_interp] # not needed for calculation\n    mse_test_at_interp = np_test_mses[idx_interp]\n\n    # Get test MSEs for all other m values\n    other_indices = np.arange(len(m_values)) != idx_interp\n    other_test_mses = np_test_mses[other_indices]\n\n    if len(other_test_mses) == 0:\n        return False\n\n    median_other_mses = np.median(other_test_mses)\n    \n    # Check for spike condition\n    is_spike = mse_test_at_interp > (1 + delta) * median_other_mses\n    \n    return is_spike\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, d, sigma, seed)\n        (60, 20, 0.5, 0),\n        (60, 20, 0.0, 1),\n        (24, 8, 0.5, 2),\n        (80, 30, 0.8, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, d, sigma, seed = case\n        result = run_single_case(n, d, sigma, seed)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}