{
    "hands_on_practices": [
        {
            "introduction": "The performance of Uniform Manifold Approximation and Projection (UMAP) is fundamentally tied to the initial high-dimensional graph it constructs, which in turn depends critically on the chosen distance metric. This exercise explores a scenario where the default Euclidean metric is inadequate for capturing the data's true structure. By working with periodic data, which naturally lives on a circle, you will see how a naive distance calculation can artificially sever the manifold's topology, and you will implement a principled fix using a circular distance metric to ensure the neighborhood graph is correctly represented . This practice provides a concrete demonstration of why tailoring the metric to the known geometry of the data is a crucial first step for effective dimensionality reduction.",
            "id": "3190448",
            "problem": "You are given the task of examining how Uniform Manifold Approximation and Projection (UMAP) handles periodic data where the intrinsic geometry is cyclic, such as angles. UMAP begins by constructing a weighted $k$-nearest neighbor graph in the original space and then learns a low-dimensional representation by optimizing an objective over this graph. A periodic variable such as angle lives naturally on a circle, where the true neighborhood structure must account for cyclic wrap-around between $0$ and $2\\pi$. If a standard Euclidean distance on the angle value is used, the wrap-around neighborhood between angles near $0$ and angles near $2\\pi$ is artificially severed, leading to unwarranted cuts in the learned manifold representation.\n\nStarting from core principles in metric spaces and neighborhood graphs, do the following:\n\n- Construct synthetic datasets of a single periodic variable representing angles uniformly distributed on the circle. Each dataset is of size $n$, with angles in radians in the interval $[0,2\\pi)$. Some test cases will add small centered Gaussian noise of standard deviation $\\sigma$ to the base angles before wrapping mod $2\\pi$; any angles must remain in $[0,2\\pi)$.\n\n- For each dataset, build a $k$-nearest neighbor list using two choices of distance on the angle variable:\n  1. The naive Euclidean distance on the angle values.\n  2. A circular distance that respects periodicity on the circle.\n\n- Define and compute a cyclic continuity score $S$ as follows: For each angle $\\theta_i$, determine its two closest neighbors on the circle using the circular distance. Then check whether both of these two closest circular neighbors are contained in the $k$-nearest neighbor list of $\\theta_i$ constructed under the chosen metric. The score $S$ is the fraction of points for which this condition holds, expressed as a decimal number between $0$ and $1$.\n\n- Interpret $S$ as a proxy for whether UMAP’s initial neighborhood graph preserves cyclicity. Explain why using the circular distance metric is a principled trick to prevent unwarranted cuts at the wrap boundary and how this relates to UMAP’s reliance on the high-dimensional neighborhood graph.\n\nYour program must implement the above and compute $S$ for each dataset in the test suite below. Angles must be in radians. The final results must be rounded to three decimal places.\n\nTest suite parameter sets to cover:\n- Case A (happy path, naive metric shows boundary cut): $n=100$, $k=2$, $\\sigma=0.0$, metric = Euclidean on angles.\n- Case B (happy path, circular metric fixes wrap-around): $n=100$, $k=2$, $\\sigma=0.0$, metric = circular distance.\n- Case C (edge case, insufficient neighborhood size): $n=50$, $k=1$, $\\sigma=0.0$, metric = circular distance.\n- Case D (noisy angles, naive metric): $n=80$, $k=3$, $\\sigma=0.01$, metric = Euclidean on angles, use a fixed random seed for reproducibility.\n- Case E (noisy angles, circular metric): $n=80$, $k=3$, $\\sigma=0.01$, metric = circular distance, use the same fixed random seed as Case D.\n\nYour program should produce a single line of output containing the results [$S_A,S_B,S_C,S_D,S_E$] as a comma-separated list enclosed in square brackets, with each $S$ rounded to three decimal places (e.g., $[0.980,1.000,0.000,0.975,1.000]$).",
            "solution": "The problem requires an analysis of how Uniform Manifold Approximation and Projection (UMAP) handles periodic data, specifically by examining the construction of the initial high-dimensional neighborhood graph. The fidelity of UMAP's low-dimensional representation is critically dependent on this initial graph, which in turn is determined by the choice of distance metric used to define neighborhoods.\n\nA variable representing an angle $\\theta$ is intrinsically periodic, living on a circle, which can be denoted as the manifold $\\mathbb{S}^1$. The set of angles is typically represented by values in an interval, such as $[0, 2\\pi)$. A key challenge arises at the boundary of this interval: an angle of $2\\pi - \\epsilon$ is, on the circle, very close to an angle of $\\epsilon$ for a small $\\epsilon > 0$. A naive distance metric that treats these values as numbers on a line will fail to capture this proximity, leading to an incorrect topological representation.\n\nLet us consider a dataset of $n$ angles $\\{\\theta_i\\}_{i=1}^n$, where each $\\theta_i \\in [0, 2\\pi)$.\n\nFirst, we define the two distance metrics under consideration.\n$1$. The **naive Euclidean distance** $d_E$ on the interval $[0, 2\\pi)$ is defined as:\n$$ d_E(\\theta_i, \\theta_j) = |\\theta_i - \\theta_j| $$\nThis metric incorrectly assigns a large distance to points that are cyclically adjacent but fall on opposite sides of the $0/2\\pi$ boundary. For example, $d_E(0.1, 2\\pi - 0.1) = 2\\pi - 0.2$, which is close to the maximum possible distance, whereas on the circle these points are separated by an arc length of only $0.2$.\n\n$2$. The **circular distance** $d_C$ correctly measures the shortest arc length between two points on a circle of circumference $2\\pi$. It is defined as:\n$$ d_C(\\theta_i, \\theta_j) = \\min(|\\theta_i - \\theta_j|, 2\\pi - |\\theta_i - \\theta_j|) $$\nUsing this metric, $d_C(0.1, 2\\pi - 0.1) = \\min(2\\pi - 0.2, 2\\pi - (2\\pi - 0.2)) = \\min(2\\pi - 0.2, 0.2) = 0.2$. This correctly reflects the proximity of the two points on the circle.\n\nUMAP's first step is to construct a weighted $k$-nearest neighbor ($k$-NN) graph. For each point $\\theta_i$, it finds the set $N_k(\\theta_i)$ containing the $k$ points $\\{\\theta_j | j \\neq i\\}$ with the smallest distance $d(\\theta_i, \\theta_j)$. The choice of metric, $d_E$ or $d_C$, is therefore fundamental. If $d_E$ is used, a point $\\theta_i$ near $0$ will have its neighbors selected from other points near $0$, while its true neighbor near $2\\pi$ will be considered distant. This artificially \"cuts\" the circle at the boundary, creating a graph that is topologically equivalent to a line segment, not a circle. Since UMAP's objective is to preserve the topological structure of this high-dimensional graph in the low-dimensional embedding, a severed graph will inevitably result in a severed embedding.\n\nUsing the circular distance $d_C$ is a principled method to prevent this. It is not an arbitrary \"trick,\" but rather the incorporation of prior knowledge about the data's topology into the analysis. By defining distance in a way that respects the cyclic nature of the data, we ensure that the $k$-NN graph correctly represents the local structure everywhere, including at the wrap-around boundary. Consequently, UMAP will be guided to learn a low-dimensional embedding that preserves the circular topology.\n\nTo quantify this preservation of cyclic structure, we define the cyclic continuity score $S$. For each point $\\theta_i$, its two \"true\" neighbors on the circle are the points immediately preceding and succeeding it when all points are sorted by angle. Let these two true neighbors be $\\theta_{p(i)}$ and $\\theta_{s(i)}$. The score $S$ is the fraction of points for which both of these true neighbors are captured within the $k$-NN list computed using the chosen metric:\n$$ S = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(\\{\\theta_{p(i)}, \\theta_{s(i)}\\} \\subseteq N_k(\\theta_i)) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if the condition is true and $0$ otherwise.\n\nA score of $S=1.0$ indicates perfect preservation of the local cyclic structure, while $S < 1.0$ indicates that the $k$-NN graph has broken connections.\n\nAnalysis of the test cases:\n- **Case A ($n=100, k=2, \\sigma=0.0$, Euclidean):** For uniformly spaced points, the point at $\\theta=0$ has true neighbors at $2\\pi/100$ and $2\\pi(99/100)$. The Euclidean metric will find its $2$ nearest neighbors to be at $2\\pi/100$ and $4\\pi/100$, missing the neighbor at $2\\pi(99/100)$. A similar failure occurs for the point at $2\\pi(99/100)$. All other $98$ points will have their local neighbors correctly identified. Thus, we expect $S = 98/100 = 0.98$.\n- **Case B ($n=100, k=2, \\sigma=0.0$, circular):** The circular metric correctly identifies the true neighbors for all points, including those at the boundary. Since $k=2$, the $2$-NN list for every point will contain exactly its two true neighbors. Thus, we expect $S = 100/100 = 1.0$.\n- **Case C ($n=50, k=1$, circular):** The condition for the score requires that *both* true neighbors be in the $k$-NN list. When $k=1$, the list can only contain one neighbor. Therefore, the condition can never be met for any point, regardless of the metric. We expect $S = 0/50 = 0.0$.\n- **Case D ($n=80, k=3, \\sigma=0.01$, Euclidean):** With added noise, the situation is analogous to Case A. The small noise $\\sigma=0.01$ is unlikely to change the fundamental flaw of the Euclidean metric at the boundary. The points with the smallest and largest angles will very likely fail the test. For $n=80$, the failure of $2$ points would yield a score of $S = 78/80 = 0.975$.\n- **Case E ($n=80, k=3, \\sigma=0.01$, circular):** The circular metric combined with $k=3$ should be robust to small noise. The two true neighbors will almost certainly be the two closest points. Since $k=3$, which is greater than the required $2$, the $3$-NN list will contain both true neighbors for every point. We expect $S=1.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the cyclic continuity score for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A: n=100, k=2, sigma=0.0, metric=Euclidean\n        {'n': 100, 'k': 2, 'sigma': 0.0, 'metric': 'euclidean', 'seed': None},\n        # Case B: n=100, k=2, sigma=0.0, metric=circular\n        {'n': 100, 'k': 2, 'sigma': 0.0, 'metric': 'circular', 'seed': None},\n        # Case C: n=50, k=1, sigma=0.0, metric=circular\n        {'n': 50, 'k': 1, 'sigma': 0.0, 'metric': 'circular', 'seed': None},\n        # Case D: n=80, k=3, sigma=0.01, metric=Euclidean, seed=42\n        {'n': 80, 'k': 3, 'sigma': 0.01, 'metric': 'euclidean', 'seed': 42},\n        # Case E: n=80, k=3, sigma=0.01, metric=circular, seed=42\n        {'n': 80, 'k': 3, 'sigma': 0.01, 'metric': 'circular', 'seed': 42},\n    ]\n\n    results = []\n    for params in test_cases:\n        score = calculate_cyclic_continuity_score(\n            n=params['n'],\n            k=params['k'],\n            sigma=params['sigma'],\n            metric=params['metric'],\n            seed=params['seed']\n        )\n        results.append(f\"{score:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_cyclic_continuity_score(n, k, sigma, metric, seed):\n    \"\"\"\n    Calculates the cyclic continuity score S for a given set of parameters.\n\n    Args:\n        n (int): Number of points.\n        k (int): Number of nearest neighbors.\n        sigma (float): Standard deviation of Gaussian noise.\n        metric (str): The distance metric to use ('euclidean' or 'circular').\n        seed (int or None): Random seed for reproducibility.\n\n    Returns:\n        float: The calculated cyclic continuity score S.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    # 1. Generate synthetic data\n    base_angles = np.linspace(0, 2 * np.pi, n, endpoint=False)\n    if sigma > 0:\n        noise = np.random.normal(loc=0.0, scale=sigma, size=n)\n        angles = (base_angles + noise) % (2 * np.pi)\n    else:\n        angles = base_angles\n\n    # 2. Determine the two true circular neighbors for each point\n    # These are the adjacent points on the sorted circle.\n    sorted_indices = np.argsort(angles)\n    ranks = np.empty_like(sorted_indices)\n    ranks[sorted_indices] = np.arange(n)\n    \n    true_neighbors = {}\n    for i in range(n):\n        rank_i = ranks[i]\n        # Find ranks of predecessor and successor with wrap-around\n        pred_rank = (rank_i - 1 + n) % n\n        succ_rank = (rank_i + 1) % n\n        # Get original indices of these neighbors\n        pred_idx = sorted_indices[pred_rank]\n        succ_idx = sorted_indices[succ_rank]\n        true_neighbors[i] = {pred_idx, succ_idx}\n\n    # 3. Build k-nearest neighbor list using the chosen metric\n    # Reshape for broadcasting\n    points_col = angles[:, np.newaxis]\n    \n    if metric == 'euclidean':\n        # Naive Euclidean distance on angle values\n        dist_matrix = np.abs(points_col - points_col.T)\n    elif metric == 'circular':\n        # Distance on the circle\n        diff = np.abs(points_col - points_col.T)\n        dist_matrix = np.minimum(diff, 2 * np.pi - diff)\n    else:\n        raise ValueError(\"Invalid metric specified\")\n\n    # Exclude self from being a neighbor\n    np.fill_diagonal(dist_matrix, np.inf)\n\n    # Get indices of the k nearest neighbors for each point\n    knn_indices = np.argsort(dist_matrix, axis=1)[:, :k]\n\n    # 4. Compute the cyclic continuity score S\n    successful_points = 0\n    for i in range(n):\n        # The k-NN list for point i\n        point_knn_set = set(knn_indices[i])\n        # The set of two true neighbors for point i\n        point_true_neighbors = true_neighbors[i]\n        \n        # Check if both true neighbors are in the k-NN list\n        if point_true_neighbors.issubset(point_knn_set):\n            successful_points += 1\n            \n    score = successful_points / n\n    return score\n\nsolve()\n```"
        },
        {
            "introduction": "Building upon the principle of using topology-aware metrics, we now extend the challenge from a simple circle to a more complex manifold: the torus. A torus, defined by two independent angular coordinates, presents two fundamental cycles that a faithful embedding should preserve. This practice requires you to construct a custom toroidal distance metric and design a UMAP-inspired embedding that explicitly uses local coordinate information to maintain the manifold's structure . This advanced exercise demonstrates the power and generality of encoding prior geometric knowledge into the manifold learning process, moving beyond simple data structures to more intricate topological spaces.",
            "id": "3190430",
            "problem": "You are given a two-dimensional torus manifold parameterized by two angular coordinates. The Uniform Manifold Approximation and Projection (UMAP) approach in statistical learning can be understood from first principles as a graph-based embedding method whose purpose is to preserve local neighborhood structure when mapping data from a high-dimensional space into a lower-dimensional space. In this problem you will generate synthetic data sampled from a torus, design and evaluate embeddings with and without a topology-preserving constraint informed by the toroidal metric, and report whether the embedding preserves both fundamental cycles of the torus.\n\nStart from the following fundamental base:\n- A metric space is a set with a distance function that satisfies nonnegativity, identity of indiscernibles, symmetry, and the triangle inequality. The torus parameterization uses two angles, $\\theta \\in [0,2\\pi)$ and $\\phi \\in [0,2\\pi)$, interpreted on the circle so that differences are taken modulo $2\\pi$.\n- A graph built from $k$-nearest neighbors captures local connectivity on a manifold when the metric accurately reflects the manifold’s geometry.\n- A low-dimensional embedding is judged successful if local neighborhood relations induced by the chosen metric in the original space are reflected in the embedded distances.\n\nData generation:\n- Sample $n$ points with angles $\\theta_i \\sim \\text{Uniform}(0,2\\pi)$ and $\\phi_i \\sim \\text{Uniform}(0,2\\pi)$. Embed them in $\\mathbb{R}^3$ via a standard torus with major radius $R$ and minor radius $r$ via the parametric map to coordinates $(x_i,y_i,z_i)$, where:\n$$\n\\begin{align*}\nx_i &= (R + r\\cos \\theta_i)\\cos \\phi_i \\\\\ny_i &= (R + r\\cos \\theta_i)\\sin \\phi_i \\\\\nz_i &= r\\sin \\theta_i\n\\end{align*}\n$$\n- Use $R=2$ and $r=1$.\n\nToroidal metric:\n- Define the angular wrap difference for any angle pair as the signed minimal difference in $(-\\pi,\\pi]$. Define the toroidal distance between two angle pairs $(\\theta_i,\\phi_i)$ and $(\\theta_j,\\phi_j)$ as $\\sqrt{\\Delta\\theta_{ij}^2 + \\Delta\\phi_{ij}^2}$, where $\\Delta\\theta_{ij}$ and $\\Delta\\phi_{ij}$ are the wrapped differences.\n\nEmbedding procedures to implement:\n- Procedure A (collapsed baseline): Construct a two-dimensional embedding that uses only one angular coordinate, for example mapping each point to $[\\cos \\phi_i,\\sin \\phi_i]$. This intentionally collapses one fundamental cycle and serves as a negative control.\n- Procedure B (UMAP-inspired with toroidal constraint): Build a $k$-nearest neighbor graph using the toroidal metric on $(\\theta,\\phi)$, and compute a two-dimensional embedding $\\mathbf{y}_i \\in \\mathbb{R}^2$ that best respects local signed angular differences along graph edges by solving a least-squares matching of local differences. Conceptually, for neighbors $(i,j)$ in the graph enforce that $\\mathbf{y}_i - \\mathbf{y}_j$ approximates the signed wrapped differences $[\\Delta\\theta_{ij},\\Delta\\phi_{ij}]$, with an appropriate gauge fixing to remove the translation ambiguity. This constitutes a topology-preserving constraint informed by the toroidal metric and reflects the spirit of preserving manifold local structure in UMAP-like methods.\n\nEvaluation of cycle preservation:\n- For a given embedding $\\{\\mathbf{y}_i\\}$ and the original angles, estimate whether both cycles are preserved by computing the Pearson correlation between embedded pairwise distances $D_{ij}=\\|\\mathbf{y}_i-\\mathbf{y}_j\\|_2$ and each wrapped angular difference magnitude $|\\Delta\\theta_{ij}|$ and $|\\Delta\\phi_{ij}|$ over a large random subset of index pairs. Declare that both cycles are preserved if both correlations exceed a threshold $\\tau$.\n- Use $\\tau=0.7$.\n\nTest suite:\n- Use the following three test cases. Each test case is a tuple of four values (`method`, `n`, `k`, `s`) where `method` $\\in \\{0,1\\}$ selects the embedding procedure ($0$ for Procedure A, $1$ for Procedure B), $n$ is the number of points, $k$ is the number of neighbors for the graph in Procedure B (ignored for Procedure A), and $s$ is the random seed for data generation.\n    1. $(0,256,10,1)$\n    2. $(1,256,10,2)$\n    3. $(1,64,6,3)$\n\nRequired output:\n- For each test case, produce an integer $b \\in \\{0,1\\}$ where $b=1$ if both cycles are preserved and $b=0$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[1,0,1]`).\n\nAngle units:\n- All angles are measured in radians.\n\nNotes:\n- Ensure all randomization uses the provided seed $s$ for reproducibility.\n- No external input is required. The code must run to completion using only the specified libraries and output exactly one line as specified.",
            "solution": "We proceed from first principles by modeling the torus as a product of circles with an appropriate metric, constructing a local neighborhood graph, and designing an embedding that preserves local relations derived from the manifold structure.\n\nTorus model and metric:\n- The torus is the Cartesian product $S^1 \\times S^1$. Points are parameterized by angles $\\theta \\in [0,2\\pi)$ and $\\phi \\in [0,2\\pi)$.\n- The wrap operator on angles defines the signed minimal difference $\\operatorname{wrap}(\\alpha) \\in (-\\pi,\\pi]$ for any real $\\alpha$ by shifting $\\alpha$ by integer multiples of $2\\pi$ until it lies in that interval. For two angles $a$ and $b$, the signed difference is $\\Delta(a,b)=\\operatorname{wrap}(a-b)$.\n- The toroidal metric on $(\\theta,\\phi)$ is the Euclidean metric on the product of circles measured with wrapped differences:\n$$\nd_T\\big((\\theta_i,\\phi_i),(\\theta_j,\\phi_j)\\big)=\\sqrt{\\Delta(\\theta_i,\\theta_j)^2 + \\Delta(\\phi_i,\\phi_j)^2}.\n$$\nThis metric is intrinsic to the $S^1\\times S^1$ manifold and is the appropriate basis for local neighborhood graphs when the goal is to preserve both cycles.\n\nData generation:\n- For $i \\in \\{1,\\dots,n\\}$ we sample $\\theta_i \\sim \\text{Uniform}(0,2\\pi)$ and $\\phi_i \\sim \\text{Uniform}(0,2\\pi)$ independently.\n- We embed in $\\mathbb{R}^3$ by the smooth map\n$$\n\\begin{align*}\nx_i &= (R + r\\cos \\theta_i)\\cos \\phi_i, \\\\\ny_i &= (R + r\\cos \\theta_i)\\sin \\phi_i, \\\\\nz_i &= r\\sin \\theta_i,\n\\end{align*}\n$$\nwith $R=2$ and $r=1$. This is the standard torus parametrization.\n\nNeighborhood graph:\n- Given points with angles $\\{(\\theta_i,\\phi_i)\\}_{i=1}^n$ and an integer $k$, we compute for each node $i$ its $k$ nearest neighbors under $d_T$. The directed $k$-nearest neighbor graph captures local adjacency consistent with the toroidal geometry.\n\nEmbedding design:\n- Procedure A constructs a baseline collapsed embedding by ignoring one angular coordinate, for example\n$$\n\\mathbf{y}_i^{(A)} = [\\cos \\phi_i,\\sin \\phi_i],\n$$\nwhich maps all points on the torus to the unit circle and necessarily collapses the cycle parameterized by $\\theta$.\n- Procedure B imposes a topology-preserving toroidal constraint inspired by the locality-preserving philosophy of Uniform Manifold Approximation and Projection (UMAP). UMAP seeks to match local structures between spaces using a similarity graph; here we encode the manifold’s local coordinate differences as target constraints on neighbor edges. Specifically, for each directed neighbor edge $(i,j)$ we consider the signed, wrapped angular differences\n$$\n\\mathbf{s}_{ij} = [\\Delta(\\theta_i,\\theta_j), \\Delta(\\phi_i,\\phi_j)].\n$$\nWe then seek embedded coordinates $\\{\\mathbf{y}_i\\}_{i=1}^n \\subset \\mathbb{R}^2$ that minimize the sum of squared deviations of local differences:\n$$\n\\min_{\\{\\mathbf{y}_i\\}} \\sum_{(i,j)\\in E} \\left\\| (\\mathbf{y}_i - \\mathbf{y}_j) - \\mathbf{s}_{ij} \\right\\|_2^2,\n$$\nwhere $E$ is the set of directed edges in the $k$-nearest neighbor graph. This objective is convex and separable across the two embedding coordinates. Introducing a gauge constraint to fix translation, e.g., $\\mathbf{y}_1=\\mathbf{0}$, yields a unique least-squares solution. By construction, local differences in the embedding match the toroidal angular differences, which preserves both fundamental cycles in the sense of local linearization consistent with the manifold coordinates.\n\nSolving the least squares:\n- For each edge $(i,j)$, the contribution to the objective for the first coordinate is $((y^{(1)}_i - y^{(1)}_j) - s^{(1)}_{ij})^2$. Collecting all such equations and adding a single anchor equation $y^{(1)}_1=0$ forms an overdetermined linear system $A\\,y^{(1)}=b^{(1)}$; similarly for the second coordinate. The normal equations $A^\\top A\\,y^{(d)}=A^\\top b^{(d)}$ (for $d \\in \\{1,2\\}$) correspond to a graph Laplacian system with a fixed reference. We solve each system in the least-squares sense using a standard sparse solver. This is equivalent to minimizing the convex quadratic objective above and yields the desired $\\{\\mathbf{y}_i\\}$.\n\nEvaluation of cycle preservation:\n- Let $D_{ij}=\\|\\mathbf{y}_i-\\mathbf{y}_j\\|_2$ denote embedded Euclidean distances. Let $T_{ij}=|\\Delta(\\theta_i,\\theta_j)|$ and $P_{ij}=|\\Delta(\\phi_i,\\phi_j)|$ denote the toroidal angular separations along the two cycles. We estimate the Pearson correlation coefficients\n$$\nr_\\theta = \\operatorname{corr}(D_{ij}, T_{ij}),\\qquad r_\\phi = \\operatorname{corr}(D_{ij}, P_{ij})\n$$\nover a large random sample of unordered index pairs. We declare that both cycles are preserved if $r_\\theta \\ge \\tau$ and $r_\\phi \\ge \\tau$ with $\\tau=0.7$. This threshold demands that embedded distances increase monotonically and strongly with separations along both angular coordinates.\n\nTest suite rationale:\n- Case $(0,256,10,1)$ uses Procedure A and is expected to fail the two-cycle test (output $0$) because $\\mathbf{y}_i^{(A)}$ is a function only of $\\phi_i$.\n- Case $(1,256,10,2)$ uses Procedure B with a reasonably dense graph, expected to pass (output $1$).\n- Case $(1,64,6,3)$ uses Procedure B with fewer points and neighbors, exercising a boundary condition; the local constraints remain sufficient to reconstruct both cycles (output $1$) provided the graph is connected and the least-squares is well-posed.\n\nThe final program implements:\n- Sampling of $(\\theta,\\phi)$ and $(x,y,z)$ with fixed seed.\n- The wrap and toroidal distance functions.\n- The construction of the $k$-nearest neighbor graph under the toroidal metric.\n- Procedure A and Procedure B as described.\n- The correlation-based topology preservation test with $\\tau=0.7$.\n- Execution of the three specified test cases and printing of results as one list on a single line.\n\nNo physical units are involved, angles are in radians, and all outputs are integers in $\\{0,1\\}$ as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import coo_matrix\nfrom scipy.sparse.linalg import lsqr\n\n# -----------------------------\n# Utilities for torus sampling.\n# -----------------------------\ndef sample_torus(n, R=2.0, r=1.0, seed=0):\n    rng = np.random.default_rng(seed)\n    theta = rng.uniform(0.0, 2.0*np.pi, size=n)\n    phi = rng.uniform(0.0, 2.0*np.pi, size=n)\n    x = (R + r*np.cos(theta)) * np.cos(phi)\n    y = (R + r*np.cos(theta)) * np.sin(phi)\n    z = r * np.sin(theta)\n    xyz = np.column_stack([x, y, z])\n    return theta, phi, xyz\n\n# -----------------------------\n# Toroidal angle operations.\n# -----------------------------\ndef wrap_signed(angle):\n    # Wrap to (-pi, pi]\n    wrapped = (angle + np.pi) % (2.0*np.pi) - np.pi\n    # Avoid -pi by mapping to pi for consistency\n    wrapped[wrapped = -np.pi] = np.pi\n    return wrapped\n\ndef angle_diff_signed(a, b):\n    # Difference a - b wrapped to (-pi, pi]\n    return wrap_signed(a - b)\n\ndef toroidal_distance(theta1, phi1, theta2, phi2):\n    dtheta = angle_diff_signed(theta1, theta2)\n    dphi = angle_diff_signed(phi1, phi2)\n    return np.sqrt(dtheta**2 + dphi**2)\n\n# ---------------------------------\n# Build kNN graph under toroidal metric.\n# ---------------------------------\ndef knn_graph_toroidal(theta, phi, k):\n    n = theta.shape[0]\n    # Compute full pairwise toroidal distances (O(n^2)), acceptable for n up to a few hundred.\n    t1 = theta[:, None]\n    p1 = phi[:, None]\n    t2 = theta[None, :]\n    p2 = phi[None, :]\n\n    dtheta = angle_diff_signed(t1, t2)\n    dphi = angle_diff_signed(p1, p2)\n    D = np.sqrt(dtheta**2 + dphi**2)\n\n    # Exclude self by setting diagonal to inf\n    np.fill_diagonal(D, np.inf)\n\n    # For each i, find indices of k nearest neighbors\n    nn_idx = np.argpartition(D, kth=k, axis=1)[:, :k]  # unsorted k nearest\n    # Sort neighbors by distance for consistency\n    row_indices = []\n    col_indices = []\n    for i in range(n):\n        nbrs = nn_idx[i]\n        # sort these neighbors\n        nbrs_sorted = nbrs[np.argsort(D[i, nbrs])]\n        row_indices.extend([i]*k)\n        col_indices.extend(list(nbrs_sorted))\n    rows = np.array(row_indices, dtype=int)\n    cols = np.array(col_indices, dtype=int)\n    return rows, cols  # directed edges rows -> cols\n\n# ---------------------------------------------------\n# Procedure A: collapsed embedding using only phi.\n# ---------------------------------------------------\ndef embedding_collapsed(theta, phi):\n    return np.column_stack([np.cos(phi), np.sin(phi)])\n\n# ---------------------------------------------------\n# Procedure B: constrained least-squares embedding.\n# Enforce y_i - y_j ≈ [Δθ_ij, Δφ_ij] for kNN edges.\n# ---------------------------------------------------\ndef embedding_constrained(theta, phi, k):\n    n = theta.shape[0]\n    rows, cols = knn_graph_toroidal(theta, phi, k)\n    m = rows.shape[0]\n\n    # Build sparse A matrices and b vectors for each dimension separately.\n    # For dim 0 target is Δθ, for dim 1 target is Δφ.\n    # Each edge contributes one equation: (y_i - y_j) = s_ij\n    # Add one anchor equation y_0 = 0 for each dimension to fix translation.\n    # Assemble in COO format.\n    def assemble_and_solve(target_signed_diffs):\n        # target_signed_diffs shape (m,), corresponds to edges rows -> cols: s_ij\n        # A has m+1 rows, n columns\n        data = []\n        i_idx = []\n        j_idx = []\n\n        # Edge equations\n        # Row r: +1 at i, -1 at j\n        for r in range(m):\n            i = rows[r]\n            j = cols[r]\n            i_idx.append(r)\n            j_idx.append(i)\n            data.append(1.0)\n            i_idx.append(r)\n            j_idx.append(j)\n            data.append(-1.0)\n\n        # Anchor equation at last row: y_0 = 0\n        i_idx.append(m)\n        j_idx.append(0)\n        data.append(1.0)\n\n        A = coo_matrix((np.array(data), (np.array(i_idx), np.array(j_idx))), shape=(m+1, n))\n        b = np.zeros(m+1, dtype=float)\n        b[:m] = target_signed_diffs\n        b[m] = 0.0  # anchor\n\n        # Solve least squares A y = b\n        y = lsqr(A, b, atol=1e-10, btol=1e-10, iter_lim=1000)[0]\n        return y\n\n    # Compute signed diffs for edges\n    dtheta_edges = angle_diff_signed(theta[rows], theta[cols])\n    dphi_edges = angle_diff_signed(phi[rows], phi[cols])\n\n    y0 = assemble_and_solve(dtheta_edges)\n    y1 = assemble_and_solve(dphi_edges)\n    Y = np.column_stack([y0, y1])\n    return Y\n\n# -------------------------------------------\n# Evaluation: correlation-based cycle test.\n# -------------------------------------------\ndef pearson_corr(x, y):\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    x_mean = x.mean()\n    y_mean = y.mean()\n    x_c = x - x_mean\n    y_c = y - y_mean\n    denom = np.sqrt(np.sum(x_c**2)) * np.sqrt(np.sum(y_c**2))\n    if denom == 0.0:\n        return 0.0\n    return float(np.dot(x_c, y_c) / denom)\n\ndef test_cycles_preserved(Y, theta, phi, rng, num_pairs=4000, threshold=0.7):\n    n = theta.shape[0]\n    # Sample pairs without replacement per batch; if too many, just random with replacement\n    i_idx = rng.integers(0, n, size=num_pairs)\n    j_idx = rng.integers(0, n, size=num_pairs)\n    # Avoid i==j by resampling those indices\n    mask_same = (i_idx == j_idx)\n    if np.any(mask_same):\n        j_idx[mask_same] = (j_idx[mask_same] + 1) % n\n\n    # Distances in embedding\n    diff = Y[i_idx] - Y[j_idx]\n    D = np.sqrt(np.sum(diff**2, axis=1))\n\n    # Toroidal angular separations magnitudes\n    dtheta_abs = np.abs(angle_diff_signed(theta[i_idx], theta[j_idx]))\n    dphi_abs = np.abs(angle_diff_signed(phi[i_idx], phi[j_idx]))\n\n    r_theta = pearson_corr(D, dtheta_abs)\n    r_phi = pearson_corr(D, dphi_abs)\n\n    return int((r_theta >= threshold) and (r_phi >= threshold))\n\n# -----------------------------\n# Main solve function.\n# -----------------------------\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (method, n, k, seed)\n    test_cases = [\n        (0, 256, 10, 1),\n        (1, 256, 10, 2),\n        (1, 64, 6, 3),\n    ]\n\n    results = []\n    for method, n, k, seed in test_cases:\n        theta, phi, xyz = sample_torus(n, R=2.0, r=1.0, seed=seed)\n        rng = np.random.default_rng(seed + 1000)\n        if method == 0:\n            Y = embedding_collapsed(theta, phi)\n        else:\n            Y = embedding_constrained(theta, phi, k=k)\n        res = test_cycles_preserved(Y, theta, phi, rng=rng, num_pairs=4000, threshold=0.7)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Beyond the intrinsic geometry of the data manifold, the characteristics of noise in the ambient space can also significantly impact UMAP's performance. This problem investigates the interplay between a common preprocessing technique, prewhitening, and the assumptions underlying UMAP's use of Euclidean distance. You will derive the conditions under which whitening aligns the neighborhood geometry with the underlying manifold, effectively correcting for anisotropic noise structures where simple Euclidean distance is misleading . This exercise provides deep statistical insight, connecting the geometric intuition of UMAP to the concept of Mahalanobis distance and revealing how data preparation is essential for uncovering the true manifold in the presence of correlated noise.",
            "id": "3190525",
            "problem": "Consider high-dimensional observations generated by a one-dimensional latent coordinate $s \\in \\mathbb{R}$ embedded in a two-dimensional ambient space via a smooth mapping $m(s) \\in \\mathbb{R}^{2}$, with additive zero-mean noise $\\epsilon$ that is Gaussian with covariance matrix $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$. Observed points are $x(s) = m(s) + \\epsilon$. Uniform Manifold Approximation and Projection (UMAP) constructs a neighborhood graph using pairwise distances in the chosen feature metric. Prewhitening the features means applying a linear transform $W \\in \\mathbb{R}^{2 \\times 2}$ such that the transformed features have identity covariance. \n\nAssume local neighborhoods are sufficiently small that manifold differences can be linearized. Your tasks are:\n\n$1.$ Starting from fundamental definitions of covariance, linearization, and the effect of a linear transform on distances, derive the expression for the squared infinitesimal distance between points with latent coordinates $s$ and $s + \\Delta s$ after prewhitening, in terms of the manifold tangent $t(s) = \\frac{dm}{ds}$ and the covariance matrix $\\Sigma$.\n\n$2.$ From first principles, derive a necessary and sufficient condition under which prewhitening yields neighbor distances that are uniformly proportional to the latent coordinate distance $|\\Delta s|$ along the manifold, thereby aligning the neighborhood geometry used by UMAP with the underlying manifold geometry.\n\n$3.$ Validate with synthetic correlated data: let the manifold be $m(s) = \\begin{pmatrix} s \\\\ 2s \\end{pmatrix}$ and let the noise covariance be $\\Sigma = \\begin{pmatrix} 3  2 \\\\ 2  5 \\end{pmatrix}$. Under the conditions you derived, compute the proportionality constant $c$ multiplying $\\Delta s^{2}$ in the whitened squared neighbor distance. Express the final numeric value of $c$ and round your answer to four significant figures.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of statistical learning and differential geometry, is well-posed with a clear objective, and provides a complete and consistent set of givens. We may therefore proceed with a full solution.\n\nThe problem asks for three related derivations concerning the effect of prewhitening on neighborhood distances for data sampled from a noisy manifold, a core concept in understanding algorithms like Uniform Manifold Approximation and Projection (UMAP).\n\n**1. Derivation of the Squared Infinitesimal Distance after Prewhitening**\n\nWe are given observed data points $x(s) = m(s) + \\epsilon$, where $s \\in \\mathbb{R}$ is a latent coordinate, $m(s) \\in \\mathbb{R}^2$ is a smooth manifold embedding, and $\\epsilon$ is zero-mean Gaussian noise with covariance $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$.\n\nPrewhitening is a linear transformation $x \\mapsto Wx$ where $W \\in \\mathbb{R}^{2 \\times 2}$ is chosen such that the covariance of the transformed noise, $W\\epsilon$, is the identity matrix $I$. The covariance of $W\\epsilon$ is given by $E[(W\\epsilon)(W\\epsilon)^T] = W E[\\epsilon \\epsilon^T] W^T = W \\Sigma W^T$. Thus, the whitening condition is:\n$$W \\Sigma W^T = I$$\nA valid choice for $W$, assuming $\\Sigma$ is positive definite, is $W = \\Sigma^{-1/2}$, where $\\Sigma^{-1/2}$ is the inverse of a matrix square root of $\\Sigma$. Since $\\Sigma$ is a a symmetric covariance matrix, its square root $\\Sigma^{1/2}$ and its inverse $\\Sigma^{-1/2}$ are also symmetric. The condition is satisfied:\n$$\\Sigma^{-1/2} \\Sigma (\\Sigma^{-1/2})^T = \\Sigma^{-1/2} \\Sigma \\Sigma^{-1/2} = \\Sigma^{-1/2} (\\Sigma^{1/2}\\Sigma^{1/2}) \\Sigma^{-1/2} = (\\Sigma^{-1/2}\\Sigma^{1/2})(\\Sigma^{1/2}\\Sigma^{-1/2}) = I \\cdot I = I$$\nFrom this, we can also deduce that:\n$$W^T W = (\\Sigma^{-1/2})^T \\Sigma^{-1/2} = \\Sigma^{-1/2} \\Sigma^{-1/2} = \\Sigma^{-1}$$\n\nThe neighborhood geometry is defined by distances between points. The relevant distance for understanding the manifold structure is the distance between the mean locations of the data points, which are the noise-free points on the manifold itself. Let's consider two such points corresponding to latent coordinates $s$ and $s + \\Delta s$. Their locations on the manifold are $m(s)$ and $m(s + \\Delta s)$.\n\nAfter applying the whitening transform $W$, these points are mapped to $W m(s)$ and $W m(s+\\Delta s)$. The squared Euclidean distance between these transformed points is what UMAP would effectively use in its neighborhood construction. Let this squared distance be $d^2$.\n$$d^2(s, s+\\Delta s) = \\|W m(s+\\Delta s) - W m(s)\\|^2 = \\|W (m(s+\\Delta s) - m(s))\\|^2$$\nWe are asked for the infinitesimal distance, which implies $\\Delta s$ is small. We use the specified linearization assumption: the manifold difference can be approximated by the tangent vector.\n$$m(s+\\Delta s) - m(s) \\approx \\frac{dm}{ds} \\Delta s = t(s) \\Delta s$$\nHere, $t(s) = \\frac{dm}{ds}$ is the tangent vector to the manifold at the point corresponding to $s$.\n\nSubstituting this approximation into the expression for $d^2$:\n$$d^2 \\approx \\|W (t(s) \\Delta s)\\|^2$$\nSince $\\Delta s$ is a scalar, we can factor it out:\n$$d^2 \\approx (\\Delta s)^2 \\|W t(s)\\|^2$$\nThe squared norm is given by the dot product of the vector with itself:\n$$\\|W t(s)\\|^2 = (W t(s))^T (W t(s)) = t(s)^T W^T W t(s)$$\nAs established earlier, $W^T W = \\Sigma^{-1}$. Substituting this into the expression gives the final result for the squared infinitesimal distance:\n$$d^2(s, s+\\Delta s) = (\\Delta s)^2 \\left( t(s)^T \\Sigma^{-1} t(s) \\right)$$\n\n**2. Condition for Uniform Proportionality**\n\nThe second task is to find a necessary and sufficient condition for the whitened neighbor distance, $d$, to be uniformly proportional to the latent coordinate distance, $|\\Delta s|$. This means that $d = k |\\Delta s|$ for some constant $k  0$ that does not depend on the position $s$ along the manifold.\n\nSquaring this relationship, we get $d^2 = k^2 (\\Delta s)^2$.\nLetting $c = k^2$, the condition is that $d^2(s, s+\\Delta s) = c (\\Delta s)^2$ for a constant $c  0$ independent of $s$.\n\nFrom Part $1$, we derived the expression for the squared distance:\n$$d^2(s, s+\\Delta s) = (\\Delta s)^2 \\left( t(s)^T \\Sigma^{-1} t(s) \\right)$$\nComparing this with the desired form $d^2 = c (\\Delta s)^2$, we can immediately see that the two are equivalent if and only if the term in the parenthesis is equal to the constant $c$.\n\nTherefore, the necessary and sufficient condition is:\n$$t(s)^T \\Sigma^{-1} t(s) = c$$\nfor some constant $c  0$ that is independent of $s$.\n\nThis condition has a clear geometric interpretation. The expression $v^T M v$ for a vector $v$ and a positive definite matrix $M$ defines a squared norm induced by the metric $M$. Here, $\\Sigma^{-1}$ defines a metric on the ambient space $\\mathbb{R}^2$. The condition requires that the length of the manifold's tangent vector $t(s)$, when measured using the metric defined by the inverse covariance matrix $\\Sigma^{-1}$, must be constant everywhere along the manifold. This ensures that movement along the latent coordinate $s$ corresponds to a uniform amount of \"statistical distance\" (Mahalanobis distance) in the ambient space, which is precisely what prewhitening corrects for.\n\n**3. Validation and Calculation**\n\nWe are given the specific manifold $m(s) = \\begin{pmatrix} s \\\\ 2s \\end{pmatrix}$ and noise covariance $\\Sigma = \\begin{pmatrix} 3  2 \\\\ 2  5 \\end{pmatrix}$.\n\nFirst, we compute the tangent vector $t(s)$:\n$$t(s) = \\frac{dm}{ds} = \\frac{d}{ds} \\begin{pmatrix} s \\\\ 2s \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nWe observe that the tangent vector $t(s)$ is a constant vector, $t = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, independent of $s$.\n\nNext, we compute the inverse of the covariance matrix $\\Sigma^{-1}$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nThe determinant of $\\Sigma$ is $\\det(\\Sigma) = (3)(5) - (2)(2) = 15 - 4 = 11$.\n$$\\Sigma^{-1} = \\frac{1}{11} \\begin{pmatrix} 5  -2 \\\\ -2  3 \\end{pmatrix}$$\nNow, we check the condition from Part $2$ and compute the proportionality constant $c$.\n$$c = t(s)^T \\Sigma^{-1} t(s)$$\nSince $t(s)$ is constant, $c$ will also be a constant, so the condition for uniform proportionality is satisfied for this manifold. We proceed to compute $c$:\n$$c = \\begin{pmatrix} 1  2 \\end{pmatrix} \\left( \\frac{1}{11} \\begin{pmatrix} 5  -2 \\\\ -2  3 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nWe can factor out the scalar $\\frac{1}{11}$:\n$$c = \\frac{1}{11} \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 5  -2 \\\\ -2  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nFirst, we compute the product of the first two matrices:\n$$\\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 5  -2 \\\\ -2  3 \\end{pmatrix} = \\begin{pmatrix} (1)(5) + (2)(-2)  (1)(-2) + (2)(3) \\end{pmatrix} = \\begin{pmatrix} 5 - 4  -2 + 6 \\end{pmatrix} = \\begin{pmatrix} 1  4 \\end{pmatrix}$$\nNow, we complete the calculation:\n$$c = \\frac{1}{11} \\begin{pmatrix} 1  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{11} ((1)(1) + (4)(2)) = \\frac{1}{11} (1 + 8) = \\frac{9}{11}$$\nThe problem asks for the final numeric value rounded to four significant figures.\n$$c = \\frac{9}{11} \\approx 0.81818181...$$\nRounding to four significant figures, we get $c = 0.8182$.",
            "answer": "$$\\boxed{0.8182}$$"
        }
    ]
}