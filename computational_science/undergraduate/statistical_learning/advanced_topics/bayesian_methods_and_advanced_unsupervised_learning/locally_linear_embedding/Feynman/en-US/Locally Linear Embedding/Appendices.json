{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Locally Linear Embedding (LLE), there is no substitute for building it from the ground up. This first exercise guides you through a complete implementation, focusing on one of the most critical practical aspects of any distance-based algorithm: sensitivity to feature scaling. By observing how LLE's output changes with different transformations of the input data (), you will develop a deep, practical understanding of why preprocessing steps like standardization are not just a formality, but a necessity for meaningful results.",
            "id": "3141684",
            "problem": "Implement a complete program that from first principles constructs Locally Linear Embedding (LLE) for given point clouds and uses it to quantitatively compare embeddings under different input scalings. Your program must not rely on any external machine learning library. It must use only linear algebra, Euclidean geometry, and eigen-decomposition. Work in purely mathematical terms as follows.\n\nDefinitions to use as the fundamental base:\n- Given a point cloud with $N$ samples in $D$ dimensions arranged as a matrix $X \\in \\mathbb{R}^{N \\times D}$, the $k$ nearest neighbors of a sample are those with smallest Euclidean distances (ties may be broken by index order). For any vectors $x,y \\in \\mathbb{R}^{D}$, the Euclidean distance is $\\lVert x - y \\rVert_{2}$.\n- For each sample $x_i \\in \\mathbb{R}^{D}$ with neighbor indices $\\mathcal{N}(i)$, define the local least-squares reconstruction problem to find reconstruction weights $w_{ij}$ that minimize a squared error subject to a unit-sum constraint. Formulate this using the local data matrix built from neighbor differences.\n- Assemble a global cost from the local reconstructions and recover a low-dimensional embedding $Y \\in \\mathbb{R}^{N \\times d}$ by solving an eigen-decomposition problem derived from the weights. Use the nontrivial eigenvectors associated with the smallest eigenvalues (excluding the one associated with the trivial constant mode).\n- A standard tool for comparing two embeddings that are only defined up to rigid transformations and uniform rescaling is orthogonal Procrustes analysis. To compare $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$, center each to zero mean, normalize each to unit Frobenius norm, compute the optimal orthogonal alignment by singular value decomposition (SVD), and report the sum of squared residuals after alignment as the disparity. This yields a nonnegative real number; smaller is closer.\n\nInvariance concepts to explore:\n- Uniform global scaling: replacing $X$ by $s X$ for a scalar $s \\in \\mathbb{R}_{+}$ multiplies all pairwise distances by the same factor. Investigate how this affects the neighbor sets, local reconstructions, and the final embedding.\n- Anisotropic feature scaling: replacing $X$ by $X A$ for a diagonal matrix $A \\in \\mathbb{R}^{D \\times D}$ with unequal positive entries rescales each feature differently. Investigate how this affects neighbor sets, local reconstructions, and the final embedding.\n- Per-feature standardization: transforming $X$ to $\\tilde{X}$ by standardizing each feature to zero mean and unit variance using the empirical mean and variance across the $N$ samples, computed featurewise.\n\nAlgorithmic requirements for your implementation:\n- Implement $k$-nearest neighbors using exact Euclidean distances on the full dataset.\n- For each point $x_i$, pose and solve the constrained least-squares reconstruction problem of $x_i$ from its $k$ neighbors with a unit-sum constraint on the weights. To ensure numerical stability when the local Gram matrix is ill-conditioned, add a Tikhonov regularizer proportional to its trace times a small scalar $\\epsilon$.\n- Assemble the global matrix from the weights and compute the embedding of dimension $d$ as the eigenvectors associated with the $d$ smallest nontrivial eigenvalues of the symmetric positive semidefinite matrix arising from the reconstruction operator.\n- Implement Procrustes alignment as described above using linear algebra and singular value decomposition.\n\nUse the following fixed data and parameters as a test suite. All angles are dimensionless; there are no physical units. All numerical values must be treated as exact scalars.\n\nShared parameters across tests:\n- Number of neighbors $k = 8$.\n- Target embedding dimension $d = 2$.\n- Regularization parameter $\\epsilon = 10^{-3}$ used by scaling the identity matrix by $\\epsilon$ times the trace of the local Gram matrix.\n- Procrustes disparity thresholds are given below for each boolean test.\n\nDatasets:\n1) Curved two-dimensional manifold in three dimensions.\n- Construct a regular grid with $m = 11$ so that $u$ and $v$ each take the $m$ equally spaced values in $[0,1]$, namely $\\{0, \\frac{1}{m-1}, \\ldots, 1\\}$. Stack all $N = m^2$ pairs $(u,v)$ in lexicographic order into a matrix $U \\in \\mathbb{R}^{N \\times 2}$.\n- Map each $(u,v)$ to a point in $\\mathbb{R}^{3}$ via $x = \\begin{bmatrix} u & v & u^2 + v^2 \\end{bmatrix}$, forming $X_{\\text{base}} \\in \\mathbb{R}^{N \\times 3}$.\n- Define three transformed versions:\n  a) Global scaling: $X_{\\text{glob}} = s X_{\\text{base}}$ with $s = 7$.\n  b) Strong anisotropic feature scaling: $X_{\\text{aniso}} = X_{\\text{base}} A$ with $A = \\mathrm{diag}(100, 0.01, 50)$.\n  c) Per-feature standardizations: $\\mathrm{Std}(X)$ denotes featurewise standardization to zero mean and unit variance. Compute $X_{\\text{std-base}} = \\mathrm{Std}(X_{\\text{base}})$ and $X_{\\text{std-aniso}} = \\mathrm{Std}(X_{\\text{aniso}})$.\n\n2) Approximately balanced linear manifold in three dimensions.\n- Using the same $U$ as above, define $X_{\\text{bal}} \\in \\mathbb{R}^{N \\times 3}$ by $x = \\begin{bmatrix} u & v & \\frac{u+v}{2} \\end{bmatrix}$ and its standardized version $X_{\\text{std-bal}} = \\mathrm{Std}(X_{\\text{bal}})$.\n\nFor each dataset $X$, compute the LLE embedding $Y \\in \\mathbb{R}^{N \\times d}$.\n\nQuantitative comparisons to compute:\n- Let $\\Delta(\\cdot,\\cdot)$ denote the Procrustes disparity between two embeddings of the same dataset size.\n- Compute the following five boolean test outcomes:\n  1) $b_1$: Invariance to uniform global scaling on the curved manifold. Require $b_1 = [\\Delta(Y_{\\text{base}}, Y_{\\text{glob}}) < 10^{-6}]$.\n  2) $b_2$: Sensitivity to anisotropic feature scaling on the curved manifold. Require $b_2 = [\\Delta(Y_{\\text{base}}, Y_{\\text{aniso}}) > 2 \\times 10^{-2}]$.\n  3) $b_3$: Standardization cancels anisotropy up to small residuals on the curved manifold. Require $b_3 = [\\Delta(Y_{\\text{std-base}}, Y_{\\text{std-aniso}}) < 2 \\times 10^{-2}]$.\n  4) $b_4$: For the curved manifold, standardizing the raw input improves closeness to the raw embedding compared to the heavily anisotropically scaled input. Require $b_4 = [\\Delta(Y_{\\text{base}}, Y_{\\text{std-base}}) < \\Delta(Y_{\\text{base}}, Y_{\\text{aniso}})]$.\n  5) $b_5$: On the approximately balanced linear manifold, standardization does not substantially change the embedding. Require $b_5 = [\\Delta(Y_{\\text{bal}}, Y_{\\text{std-bal}}) < 5 \\times 10^{-3}]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the five boolean outcomes, in order $[b_1, b_2, b_3, b_4, b_5]$, enclosed in square brackets. For example, a valid output line would look like $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{True}]$ but with the actual boolean values computed by your program.\n\nNotes:\n- All computations must be deterministic. Do not use randomness.\n- Angles do not appear. There are no physical units in this problem.",
            "solution": "The user requests an implementation of the Locally Linear Embedding (LLE) algorithm from first principles to investigate its sensitivity to different feature scaling transformations. The implementation must adhere to a specific set of algorithmic and parametric definitions.\n\nThe process is divided into three main components: the LLE algorithm itself, a method for comparing embedding geometries (Procrustes analysis), and a series of quantitative tests on generated datasets.\n\n**1. Locally Linear Embedding (LLE) Algorithm**\n\nThe LLE algorithm constructs a low-dimensional embedding of a high-dimensional dataset by preserving local linear reconstruction properties. The algorithm proceeds in three steps for a given dataset $X \\in \\mathbb{R}^{N \\times D}$.\n\n**Step 1: Neighbor Identification**\nFor each data point $x_i$, we identify its $k$ nearest neighbors. The neighborhood $\\mathcal{N}(i)$ consists of the $k$ points $x_j$ ($j \\neq i$) that have the smallest Euclidean distance $\\lVert x_i - x_j \\rVert_2$. To ensure deterministic results, any ties in distance are resolved by favoring the point with the smaller index $j$.\n\n**Step 2: Local Reconstruction Weight Computation**\nThe core assumption of LLE is that each point $x_i$ can be well-approximated by a linear combination of its neighbors. We seek a set of reconstruction weights $W_{ij}$ that minimize the total reconstruction error, subject to two constraints: $W_{ij} = 0$ if $x_j$ is not a neighbor of $x_i$, and the weights for each point must sum to unity, $\\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1$. This leads to the constrained least-squares problem for each point $x_i$:\n$$ \\min_{W} \\left\\| x_i - \\sum_{j \\in \\mathcal{N}(i)} W_{ij} x_j \\right\\|^2 \\quad \\text{subject to} \\quad \\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1 $$\nThis problem is invariant to rotation, translation, and uniform scaling of the input data. By substituting the constraint, the objective can be rewritten as finding weights $w \\in \\mathbb{R}^k$ for point $x_i$ that minimize:\n$$ \\left\\| \\sum_{j=1}^k w_j (x_i - x_{\\eta_j}) \\right\\|^2 = w^T G_i w $$\nwhere $\\{x_{\\eta_j}\\}_{j=1}^k$ are the neighbors of $x_i$, and $G_i$ is the local Gram matrix. The elements of $G_i$ are $(G_i)_{jk} = (x_i-x_{\\eta_j})^T(x_i-x_{\\eta_k})$. The solution for the optimal weights $w$ is found by solving the linear system $G_i z = \\mathbf{1}$ (where $\\mathbf{1}$ is a vector of ones) and then normalizing $w = z / \\sum z_l$.\n\nTo handle ill-conditioned Gram matrices, a Tikhonov regularization term is added. The problem specifies adding a term proportional to the trace of the Gram matrix, which makes the regularization itself scale-invariant. The modified Gram matrix $G'_i$ is:\n$$ G'_i = G_i + \\epsilon \\cdot \\mathrm{tr}(G_i) \\cdot I_k $$\nwhere $I_k$ is the $k \\times k$ identity matrix and $\\epsilon = 10^{-3}$ is a small scalar. We then solve $G'_i z = \\mathbf{1}$ for $z$ to find the weights.\n\n**Step 3: Global Embedding Computation**\nAfter computing the $N \\times N$ weight matrix $W$ (where $W_{ij}=0$ for non-neighbors), we seek a low-dimensional embedding $Y \\in \\mathbb{R}^{N \\times d}$ that preserves these local reconstruction weights. This is achieved by minimizing the embedding cost function:\n$$ \\Phi(Y) = \\sum_{i=1}^N \\left\\| y_i - \\sum_{j=1}^N W_{ij} y_j \\right\\|^2 $$\nThis quadratic form can be expressed as $\\Phi(Y) = \\mathrm{Tr}(Y^T M Y)$, where $M = (I-W)^T(I-W)$. The matrix $M$ is symmetric and positive semidefinite. To minimize $\\Phi(Y)$ while avoiding a trivial collapsed solution ($Y=0$), we impose centering and unit covariance constraints on $Y$. The solution is given by the eigenvectors of $M$ corresponding to its smallest eigenvalues. The very smallest eigenvalue of $M$ is always zero (or near-zero numerically), with a corresponding eigenvector of all ones. This represents a trivial constant embedding and is discarded. The desired $d$-dimensional embedding $Y$ is constructed from the eigenvectors associated with the next $d$ smallest eigenvalues (i.e., eigenvalues $2$ through $d+1$).\n\n**2. Orthogonal Procrustes Analysis for Embedding Comparison**\n\nTo quantitatively compare two embeddings, $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$, which are defined only up to rigid transformations (rotation, reflection) and uniform scaling, we use orthogonal Procrustes analysis. The disparity $\\Delta(Y_1, Y_2)$ is the minimum sum of squared differences after optimal alignment. The procedure is:\n1.  **Center**: Both $Y_1$ and $Y_2$ are translated so their centroids are at the origin.\n2.  **Normalize**: Both centered matrices are scaled to have a unit Frobenius norm, $\\|Y\\|_F = \\sqrt{\\sum_{i,j} Y_{ij}^2} = 1$.\n3.  **Align**: The optimal orthogonal matrix $R$ that minimizes $\\|Y_1 R - Y_2\\|_F^2$ is found using the singular value decomposition (SVD) of the matrix $C = Y_1^T Y_2$. If $C = U \\Sigma V^T$, the optimal rotation is $R=UV^T$.\n4.  **Compute Disparity**: The minimum squared residual, or Procrustes disparity, can be calculated efficiently as $\\Delta = \\|Y_1\\|_F^2 + \\|Y_2\\|_F^2 - 2 \\sum_i \\sigma_i$, where $\\sigma_i$ are the singular values of $C$. After normalization, this simplifies to $\\Delta = 2(1 - \\sum_i \\sigma_i)$. A smaller disparity indicates greater similarity between the embeddings.\n\n**3. Datasets and Quantitative Tests**\n\nThe analysis is performed on several transformations of two base datasets, using parameters $k=8$, $d=2$, and $\\epsilon=10^{-3}$. The tests are designed to probe the known properties of LLE:\n- $b_1$: Tests invariance to uniform global scaling. LLE is theoretically invariant, so this should be true.\n- $b_2$: Tests sensitivity to anisotropic feature scaling. This is a known weakness of LLE, so the disparity should be large.\n- $b_3$: Tests whether per-feature standardization can counteract the effect of anisotropy. Standardizing the features of $X_{\\text{base}}$ and $X_{\\text{aniso}}$ results in identical datasets, thus their embeddings should be identical and their disparity near-zero.\n- $b_4$: Compares the effect of standardization versus strong anisotropic scaling. Standardization is expected to be a much less disruptive transformation.\n- $b_5$: Examines the effect of standardization on a nearly balanced linear manifold. The change is expected to be minor, resulting in a small disparity.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, svd\n\ndef solve():\n    \"\"\"\n    Implements Locally Linear Embedding (LLE) from first principles and uses it to\n    compare embeddings under different input scalings.\n    \"\"\"\n\n    # -----------------------------------------------------\n    # Algorithm Implementation based on Problem Description\n    # -----------------------------------------------------\n\n    def standardize(X):\n        \"\"\"Standardizes each feature of X to have zero mean and unit variance.\"\"\"\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0)\n        # Avoid division by zero for constant features\n        std[np.isclose(std, 0)] = 1.0\n        return (X - mean) / std\n\n    def find_neighbors(X, k):\n        \"\"\"Finds the k-nearest neighbors for each point in X.\"\"\"\n        N = X.shape[0]\n        # Calculate squared Euclidean distances to avoid sqrt\n        sum_X_sq = np.sum(X**2, axis=1)\n        dist_sq_matrix = np.add.outer(sum_X_sq, sum_X_sq) - 2 * (X @ X.T)\n        dist_sq_matrix[dist_sq_matrix  0] = 0.0 # for numerical stability\n        \n        # Argsort on distances to find nearest neighbors\n        # kind='stable' ensures ties are broken by index order as required\n        neighbor_indices = np.argsort(dist_sq_matrix, axis=1, kind='stable')\n        \n        # Return indices of k-nearest neighbors.\n        # Index 0 is the point itself, so we take from 1 to k+1.\n        return neighbor_indices[:, 1:k+1]\n\n    def compute_weights(X, neighbor_indices, k, epsilon):\n        \"\"\"Computes the LLE reconstruction weights for each point.\"\"\"\n        N = X.shape[0]\n        W = np.zeros((N, N))\n        \n        for i in range(N):\n            neighbors = neighbor_indices[i]\n            \n            # Local data matrix of neighbor differences\n            C_i = X[i] - X[neighbors]  # Uses broadcasting\n            \n            # Local Gram matrix\n            G_i = C_i @ C_i.T\n            \n            # Tikhonov regularization as specified\n            trace_G = np.trace(G_i)\n            reg_val = epsilon * trace_G\n            G_i += reg_val * np.identity(k)\n            \n            # Solve for weights by solving G_i * z = 1\n            ones = np.ones(k)\n            try:\n                z = np.linalg.solve(G_i, ones)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if singular despite regularization\n                z = np.linalg.pinv(G_i) @ ones\n\n            # Normalize weights to sum to 1\n            w_i = z / np.sum(z)\n            \n            # Populate sparse weight matrix W\n            W[i, neighbors] = w_i\n            \n        return W\n\n    def compute_embedding(W, N, d):\n        \"\"\"Computes the low-dimensional embedding from the weight matrix.\"\"\"\n        # Construct the cost matrix M = (I-W)^T(I-W)\n        I = np.identity(N)\n        M = (I - W).T @ (I - W)\n        \n        # Find eigenvectors for the d smallest non-trivial eigenvalues.\n        # eigh returns eigenvalues in ascending order.\n        # We need eigenvectors for indices 1 up to and including d.\n        # This corresponds to the 2nd to (d+1)-th smallest eigenvalues.\n        _, eigvecs = eigh(M, subset_by_index=[1, d])\n        \n        return eigvecs\n\n    def procrustes_disparity(Y1, Y2):\n        \"\"\"Computes the orthogonal Procrustes disparity between two embeddings.\"\"\"\n        # 1. Center the embeddings to have zero mean.\n        Y1_c = Y1 - np.mean(Y1, axis=0)\n        Y2_c = Y2 - np.mean(Y2, axis=0)\n\n        # 2. Normalize to unit Frobenius norm.\n        norm1 = np.linalg.norm(Y1_c, 'fro')\n        norm2 = np.linalg.norm(Y2_c, 'fro')\n        Y1_norm = Y1_c / norm1 if norm1 > 1e-10 else Y1_c\n        Y2_norm = Y2_c / norm2 if norm2 > 1e-10 else Y2_c\n\n        # 3. Compute optimal alignment via SVD of the cross-covariance matrix.\n        M = Y1_norm.T @ Y2_norm\n        _, s, _ = svd(M)\n        \n        # 4. Compute disparity using the singular values.\n        # Disparity = ||Y1_norm||^2 + ||Y2_norm||^2 - 2 * tr(Sigma)\n        #           = 1 + 1 - 2 * sum(s) = 2 * (1 - sum(s))\n        disparity = 2.0 - 2.0 * np.sum(s)\n        return disparity\n\n    def run_lle(X, k, d, epsilon):\n        \"\"\"A wrapper function to run the full LLE pipeline.\"\"\"\n        neighbor_indices = find_neighbors(X, k)\n        W = compute_weights(X, neighbor_indices, k, epsilon)\n        N = X.shape[0]\n        Y = compute_embedding(W, N, d)\n        return Y\n\n    # ------------------\n    # Main Logic\n    # ------------------\n    \n    # Shared parameters from the problem description\n    k = 8\n    d = 2\n    epsilon = 1e-3\n    m = 11\n\n    # 1. Generate Datasets\n    u_vals = np.linspace(0.0, 1.0, m)\n    v_vals = np.linspace(0.0, 1.0, m)\n    uu, vv = np.meshgrid(u_vals, v_vals, indexing='ij')\n    U = np.stack([uu.ravel(), vv.ravel()], axis=1)\n\n    # Curved manifold datasets\n    X_base = np.column_stack((U[:, 0], U[:, 1], U[:, 0]**2 + U[:, 1]**2))\n    X_glob = 7.0 * X_base\n    A = np.diag([100.0, 0.01, 50.0])\n    X_aniso = X_base @ A\n    X_std_base = standardize(X_base)\n    X_std_aniso = standardize(X_aniso)\n\n    # Balanced linear manifold datasets\n    X_bal = np.column_stack((U[:, 0], U[:, 1], (U[:, 0] + U[:, 1]) / 2.0))\n    X_std_bal = standardize(X_bal)\n\n    datasets = {\n        \"base\": X_base, \"glob\": X_glob, \"aniso\": X_aniso,\n        \"std-base\": X_std_base, \"std-aniso\": X_std_aniso,\n        \"bal\": X_bal, \"std-bal\": X_std_bal,\n    }\n\n    # 2. Compute LLE Embeddings for all datasets\n    embeddings = {name: run_lle(X, k, d, epsilon) for name, X in datasets.items()}\n\n    # 3. Perform Quantitative Comparisons\n    Y_base, Y_glob, Y_aniso = embeddings[\"base\"], embeddings[\"glob\"], embeddings[\"aniso\"]\n    Y_std_base, Y_std_aniso = embeddings[\"std-base\"], embeddings[\"std-aniso\"]\n    Y_bal, Y_std_bal = embeddings[\"bal\"], embeddings[\"std-bal\"]\n\n    # Test 1: Invariance to uniform global scaling\n    d1 = procrustes_disparity(Y_base, Y_glob)\n    b1 = bool(d1  1e-6)\n\n    # Test 2: Sensitivity to anisotropic feature scaling\n    d2 = procrustes_disparity(Y_base, Y_aniso)\n    b2 = bool(d2 > 2e-2)\n\n    # Test 3: Standardization cancels anisotropy\n    d3 = procrustes_disparity(Y_std_base, Y_std_aniso)\n    b3 = bool(d3  2e-2)\n\n    # Test 4: Standardization vs. Anisotropic Scaling\n    d4_std = procrustes_disparity(Y_base, Y_std_base)\n    b4 = bool(d4_std  d2)\n    \n    # Test 5: Standardization on a balanced manifold\n    d5 = procrustes_disparity(Y_bal, Y_std_bal)\n    b5 = bool(d5  5e-3)\n\n    results = [b1, b2, b3, b4, b5]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data is rarely as clean as textbook examples; it often contains degeneracies like duplicated or near-duplicated points, which can cause algorithms to fail. This practice explores precisely such a failure mode in LLE, where degenerate local neighborhoods lead to singular matrices that make the weight calculation step ill-posed. Through this exercise (), you will learn to diagnose this instability and apply fundamental regularization techniques like data jittering and diagonal loading to create a more robust and reliable embedding.",
            "id": "3141726",
            "problem": "You are given a data matrix $X \\in \\mathbb{R}^{N \\times D}$ with rows $x_{i} \\in \\mathbb{R}^{D}$. For each index $i$, consider the $K$ nearest neighbors of $x_{i}$ under the Euclidean norm, with ties broken by ascending row index. Define the neighbor-difference matrix $Z_{i} \\in \\mathbb{R}^{K \\times D}$ by stacking the row vectors $x_{j} - x_{i}$ for the selected neighbors $j$. The locally linear embedding construction forms the local covariance $C_{i} \\in \\mathbb{R}^{K \\times K}$ as $C_{i} = Z_{i} Z_{i}^{\\top}$. You will analyze how exact duplicate rows in $X$ degenerate $C_{i}$ and evaluate two remedies based on jittering.\n\nFundamental base: Use the following core definitions and facts.\n- The Euclidean norm of $v \\in \\mathbb{R}^{D}$ is $\\lVert v \\rVert_{2} = \\sqrt{\\sum_{d=1}^{D} v_{d}^{2}}$.\n- For any matrix $A$, the rank satisfies $\\mathrm{rank}(A A^{\\top}) = \\mathrm{rank}(A)$, and $A A^{\\top}$ is symmetric positive semidefinite.\n- For a symmetric positive semidefinite matrix $M$, the eigenvalues are real and nonnegative. The two-norm condition number is $\\kappa_{2}(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$ when $\\lambda_{\\min}(M)  0$.\n- If two data points are exact duplicates, then their Euclidean distance is $0$, and subtracting them yields the zero vector.\n\nYou must implement the following, starting from these principles and definitions, without using any shortcut formulas presented as hints in this problem statement.\n\nComputational specification.\n- Nearest neighbors: For a given $i$, compute all distances $\\lVert x_{j} - x_{i} \\rVert_{2}$ for $j \\neq i$, sort increasingly by distance and then by $j$, and take the first $K$ indices.\n- Local covariance: Form $Z_{i}$ by stacking $x_{j} - x_{i}$ for the chosen neighbors, then set $C_{i} = Z_{i} Z_{i}^{\\top}$.\n- Singular detection: Let $\\tau = 10^{-12}$. Decide that $C_{i}$ is singular if its smallest eigenvalue is less than or equal to $\\tau$, and nonsingular otherwise.\n\nJittering strategies to evaluate.\n- Isotropic row jitter in $Z_{i}$: Add independent Gaussian noise with mean $0$ and standard deviation $\\varepsilon$ to each entry of $Z_{i}$ before forming $C_{i}$. Use $\\varepsilon = 10^{-3}$ and a fixed random seed of $0$ for reproducibility.\n- Diagonal loading (ridge) of $C_{i}$: Given a target condition number $\\kappa_{\\mathrm{target}}  1$, find the smallest nonnegative scalar $r_{\\min}$ such that the two-norm condition number of $C_{i} + r I$ is at most $\\kappa_{\\mathrm{target}}$. Express $r_{\\min}$ rounded to $6$ decimal places.\n\nTest suite.\nUse the following fixed datasets and parameters. All points are in $\\mathbb{R}^{4}$.\n- Case $1$ (happy path, no duplicates): $X_{A}$ has $N = 6$ rows,\n  $x_{0} = [0, 0, 0, 0]$,\n  $x_{1} = [1, 0, 0, 0]$,\n  $x_{2} = [0, 1, 0, 0]$,\n  $x_{3} = [0, 0, 1, 0]$,\n  $x_{4} = [0, 0, 0, 1]$,\n  $x_{5} = [1, 1, 1, 1]$.\n  Evaluate at $i = 5$ with $K = 4$. Output a boolean indicating whether $C_{5}$ is nonsingular under the singularity rule with threshold $\\tau$.\n- Case $2$ (single exact duplicate neighbor): $X_{B}$ extends $X_{A}$ by appending a duplicate of $x_{5}$, so $N = 7$ and\n  $x_{6} = [1, 1, 1, 1]$.\n  Evaluate at $i = 5$ with $K = 4$. Output a boolean indicating whether $C_{5}$ is singular under the same rule.\n- Case $3$ (all neighbors identical to the center): $X_{C}$ has $N = 5$ rows, all equal to $[0, 0, 0, 0]$. Evaluate at $i = 0$ with $K = 4$. Apply isotropic row jitter in $Z_{0}$ using $\\varepsilon = 10^{-3}$ and seed $0$, form $C_{0}$ from the jittered $Z_{0}$, and output a boolean indicating whether the resulting $C_{0}$ is nonsingular under the same rule.\n- Case $4$ (ridge needed for a target condition number): Reuse $X_{B}$ at $i = 5$ with $K = 4$. Let $\\kappa_{\\mathrm{target}} = 10^{6}$. Compute the smallest nonnegative $r_{\\min}$ such that the two-norm condition number of $C_{5} + r I$ is at most $\\kappa_{\\mathrm{target}}$. Output this $r_{\\min}$ as a float rounded to $6$ decimal places.\n\nFinal output format.\nYour program should produce a single line of output containing the results for Cases $1$ through $4$ in order, as a comma-separated list enclosed in square brackets (for example, $[b_{1},b_{2},b_{3},r]$ where $b_{1}$, $b_{2}$, and $b_{3}$ are booleans and $r$ is a float rounded to $6$ decimal places). No other text should be printed.",
            "solution": "The problem requires an analysis of the local covariance matrix $C_i$ used in the Locally Linear Embedding (LLE) algorithm. We will investigate its properties, particularly its potential for singularity due to duplicate data points, and evaluate two methods for numerical stabilization: jittering and diagonal loading (ridge regularization).\n\nThe core procedure is as follows. Given a data matrix $X \\in \\mathbb{R}^{N \\times D}$ and a point of interest $x_i$, we first identify its $K$ nearest neighbors. The neighbors are determined by sorting all other points $x_j$ ($j \\neq i$) based on the Euclidean distance $\\lVert x_j - x_i \\rVert_2$ in increasing order. Any ties in distance are resolved by sorting by the row index $j$ in ascending order.\n\nFrom these $K$ neighbors, whose indices we denote $j_1, \\dots, j_K$, we construct the neighbor-difference matrix $Z_i \\in \\mathbb{R}^{K \\times D}$. The $k$-th row of $Z_i$ is the vector $x_{j_k} - x_i$.\n\nThe local covariance matrix $C_i \\in \\mathbb{R}^{K \\times K}$ is then defined as the Gram matrix $C_i = Z_i Z_i^\\top$. Each element $(a, b)$ of $C_i$ is the dot product of the $a$-th and $b$-th difference vectors: $(C_i)_{ab} = (x_{j_a} - x_i) \\cdot (x_{j_b} - x_i)$.\n\nBy its construction, $C_i$ is symmetric and positive semidefinite. Its rank is determined by the rank of $Z_i$, which is the dimension of the vector space spanned by the neighbor-difference vectors $\\{x_{j_k} - x_i\\}_{k=1}^K$. The rank of $Z_i$ is at most $\\min(K, D)$. If the rank is less than $K$, the $K \\times K$ matrix $C_i$ is singular, implying it has at least one zero eigenvalue. For numerical purposes, we will classify $C_i$ as singular if its smallest eigenvalue, $\\lambda_{\\min}(C_i)$, satisfies $\\lambda_{\\min}(C_i) \\le \\tau$, where the threshold is given as $\\tau = 10^{-12}$.\n\n### Case 1: No Duplicates (Nonsingular Case)\nWe are given the dataset $X_A \\in \\mathbb{R}^{6 \\times 4}$ and asked to evaluate $C_5$ for the point $x_5 = [1, 1, 1, 1]$ with $K=4$.\n\nFirst, we compute the squared Euclidean distances from $x_5$ to all other points $x_j$ for $j \\in \\{0, 1, 2, 3, 4\\}$:\n- $\\lVert x_0 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, -1] \\rVert_2^2 = 4$\n- $\\lVert x_1 - x_5 \\rVert_2^2 = \\lVert [0, -1, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_2 - x_5 \\rVert_2^2 = \\lVert [-1, 0, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_3 - x_5 \\rVert_2^2 = \\lVert [-1, -1, 0, -1] \\rVert_2^2 = 3$\n- $\\lVert x_4 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, 0] \\rVert_2^2 = 3$\n\nThe four smallest distances are identical ($\\sqrt{3}$), corresponding to points $x_1, x_2, x_3, x_4$. Since indices are already in ascending order, these are the $K=4$ nearest neighbors. The neighbor-difference matrix $Z_5$ is formed by rows $x_j - x_5$:\n$$Z_5 = \\begin{pmatrix} 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\\\ -1  -1  -1  0 \\end{pmatrix}$$\nThe local covariance matrix $C_5 = Z_5 Z_5^\\top$ is:\n$$C_5 = \\begin{pmatrix} 3  2  2  2 \\\\ 2  3  2  2 \\\\ 2  2  3  2 \\\\ 2  2  2  3 \\end{pmatrix}$$\nThis matrix has the form $2J + I$, where $J$ is the all-ones matrix and $I$ is the identity matrix. The eigenvalues of a $k \\times k$ matrix of the form $aJ+bI$ are $a \\cdot k + b$ (with multiplicity $1$) and $b$ (with multiplicity $k-1$). For $C_5$, we have $k=4$, $a=2$, and $b=1$. The eigenvalues are $2 \\cdot 4 + 1 = 9$ and $1$ (with multiplicity $3$).\nThe smallest eigenvalue is $\\lambda_{\\min}(C_5) = 1$. Since $1  \\tau=10^{-12}$, the matrix is nonsingular. The result for this case is `True`.\n\n### Case 2: Single Exact Duplicate Neighbor (Singular Case)\nThe dataset $X_B$ is $X_A$ augmented with $x_6 = x_5 = [1, 1, 1, 1]$. We re-evaluate for $x_5$ with $K=4$.\nThe distance from $x_5$ to its duplicate $x_6$ is $\\lVert x_6 - x_5 \\rVert_2 = 0$. This is the minimum possible distance, so $x_6$ is the closest neighbor. The remaining three neighbors are $x_1, x_2, x_3$, each at distance $\\sqrt{3}$.\nThe neighbor-difference matrix $Z_5$ is:\n$$Z_5 = \\begin{pmatrix} x_6-x_5 \\\\ x_1-x_5 \\\\ x_2-x_5 \\\\ x_3-x_5 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0 \\\\ 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\end{pmatrix}$$\nSince the first row of $Z_5$ is the zero vector, the rows are linearly dependent. Thus, $\\mathrm{rank}(Z_5)  4$. The Gram matrix $C_5 = Z_5 Z_5^\\top$ must therefore be singular, with $\\mathrm{rank}(C_5) = \\mathrm{rank}(Z_5)  4$. This implies that its smallest eigenvalue is exactly $\\lambda_{\\min}(C_5)=0$. Since $0 \\le \\tau=10^{-12}$, the matrix is singular. The result for this case is `True`.\n\n### Case 3: Jittering for Regularization\nThe dataset $X_C$ consists of $N=5$ identical points $[0, 0, 0, 0]$. We evaluate for $x_0$ with $K=4$.\nThe neighbors are $x_1, x_2, x_3, x_4$. The distance to each is $0$. The difference vectors $x_j - x_0$ are all zero vectors. The initial neighbor-difference matrix $Z_0$ is a $4 \\times 4$ zero matrix:\n$$Z_0 = \\mathbf{0}_{4 \\times 4}$$\nThis matrix is maximally singular with rank $0$. To remedy this, we apply isotropic jittering. A noise matrix, with entries drawn independently from a Gaussian distribution $\\mathcal{N}(0, \\varepsilon^2)$ with $\\varepsilon = 10^{-3}$, is added to $Z_0$ to form a new matrix $Z'_0$. Since the entries of $Z'_0$ are drawn from a continuous probability distribution, the matrix will be invertible with probability $1$. Its rank will be $4$.\nThe new covariance matrix $C_0 = Z'_0 (Z'_0)^\\top$ will also have rank $4$ and thus be nonsingular. Its smallest eigenvalue $\\lambda_{\\min}(C_0)$ will be strictly positive. Given the non-zero variance of the noise, it is practically certain that $\\lambda_{\\min}(C_0)$ will be greater than the small tolerance $\\tau = 10^{-12}$. Thus, the jittered matrix is nonsingular. The result for this case is `True`.\n\n### Case 4: Diagonal Loading for Regularization\nWe reuse the singular matrix $C_5$ from Case 2. Its eigenvalues are $\\{0, 1, 1, 7\\}$. We need to find the smallest nonnegative scalar $r_{\\min}$ such that the regularized matrix $C'_5 = C_5 + rI$ has a condition number $\\kappa_2(C'_5) \\le \\kappa_{\\mathrm{target}}$, where $\\kappa_{\\mathrm{target}} = 10^6$.\nThe eigenvalues of $C'_5$ are obtained by shifting the eigenvalues of $C_5$ by $r$. For $r \\ge 0$, they are $\\{r, 1+r, 1+r, 7+r\\}$. The smallest and largest eigenvalues are $\\lambda'_{\\min} = r$ and $\\lambda'_{\\max} = 7+r$.\nThe condition number (for $r  0$) is $\\kappa_2(C'_5) = \\frac{\\lambda'_{\\max}}{\\lambda'_{\\min}} = \\frac{7+r}{r}$.\nWe solve the inequality:\n$$\\frac{7+r}{r} \\le \\kappa_{\\mathrm{target}}$$\nAs $r$ must be positive (otherwise $\\kappa_2$ is undefined or infinite), we can multiply by $r$:\n$$7 + r \\le r \\cdot \\kappa_{\\mathrm{target}} \\implies 7 \\le r (\\kappa_{\\mathrm{target}} - 1)$$\nThis gives the condition for $r$:\n$$r \\ge \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$$\nThe smallest nonnegative value $r$ can take is $r_{\\min} = \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$. Substituting $\\kappa_{\\mathrm{target}} = 10^6$:\n$$r_{\\min} = \\frac{7}{10^6 - 1} = \\frac{7}{999999} \\approx 7.000007000007 \\times 10^{-6}$$\nRounding this value to $6$ decimal places yields $0.000007$.",
            "answer": "```python\nimport numpy as np\n\ndef find_neighbors_and_form_CZ(X, i, K):\n    \"\"\"\n    Finds K nearest neighbors for X[i] and computes C_i and Z_i.\n    Ties are broken by ascending row index.\n    \"\"\"\n    x_i = X[i, :]\n    num_points = X.shape[0]\n    \n    distances = []\n    for j in range(num_points):\n        if i == j:\n            continue\n        dist = np.linalg.norm(X[j, :] - x_i)\n        distances.append((dist, j))\n    \n    # Sort by distance, then by index for tie-breaking\n    distances.sort()\n    \n    # Get the K nearest neighbor indices\n    neighbor_indices = [j for dist, j in distances[:K]]\n    \n    # Form the Z_i matrix. The shape should be K x D\n    Z_i = np.zeros((K, X.shape[1]))\n    for k, j in enumerate(neighbor_indices):\n        Z_i[k, :] = X[j, :] - x_i\n    \n    # Form the C_i matrix\n    C_i = Z_i @ Z_i.T\n    \n    return C_i, Z_i\n\ndef solve():\n    \"\"\"\n    Solves the four test cases specified in the problem.\n    \"\"\"\n    # Define datasets\n    X_A = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_B = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_C = np.zeros((5, 4), dtype=float)\n    \n    tau = 1e-12\n    results = []\n\n    # Case 1: Happy path, no duplicates. Check for nonsingular.\n    C5_A, _ = find_neighbors_and_form_CZ(X_A, i=5, K=4)\n    eigenvalues_A = np.linalg.eigh(C5_A)[0]\n    lambda_min_A = eigenvalues_A[0]\n    results.append(lambda_min_A > tau)\n\n    # Case 2: Single exact duplicate neighbor. Check for singular.\n    C5_B, _ = find_neighbors_and_form_CZ(X_B, i=5, K=4)\n    eigenvalues_B = np.linalg.eigh(C5_B)[0]\n    lambda_min_B = eigenvalues_B[0]\n    results.append(lambda_min_B = tau)\n\n    # Case 3: All neighbors identical. Jitter and check for nonsingular.\n    epsilon = 1e-3\n    seed = 0\n    _, Z0_C = find_neighbors_and_form_CZ(X_C, i=0, K=4)\n    \n    np.random.seed(seed)\n    noise = np.random.normal(scale=epsilon, size=Z0_C.shape)\n    Z0_jittered = Z0_C + noise\n    \n    C0_jittered = Z0_jittered @ Z0_jittered.T\n    eigenvalues_C = np.linalg.eigh(C0_jittered)[0]\n    lambda_min_C = eigenvalues_C[0]\n    results.append(lambda_min_C > tau)\n    \n    # Case 4: Ridge needed for a target condition number.\n    kappa_target = 10**6\n    # Re-use eigenvalues from Case 2.\n    lambda_min_B_case4 = eigenvalues_B[0]\n    lambda_max_B_case4 = eigenvalues_B[-1]\n    \n    # We need to find the smallest r >= 0 such that:\n    # (lambda_max + r) / (lambda_min + r) = kappa_target\n    # This leads to: r >= (lambda_max - kappa_target * lambda_min) / (kappa_target - 1)\n    if kappa_target > 1:\n        r_min_candidate = (lambda_max_B_case4 - kappa_target * lambda_min_B_case4) / (kappa_target - 1)\n        r_min = max(0.0, r_min_candidate)\n    else:\n        # This case is not relevant for the problem, but for completeness:\n        # If target condition number = 1, it's generally impossible unless a matrix is scalar multiple of identity.\n        r_min = np.inf \n\n    results.append(r_min)\n    \n    # Format final output string as per requirements\n    formatted_results = []\n    for item in results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item))\n        elif isinstance(item, float):\n            # Format the float to 6 decimal places for the output string\n            formatted_results.append(f\"{item:.6f}\")\n        else:\n            formatted_results.append(str(item))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful machine learning implementation hinges on more than just correct mathematical theory; it requires careful attention to the nuances of numerical computation. This final practice delves into the crucial topic of numerical stability, exploring how two algebraically identical formulas can produce different results in finite-precision arithmetic. By comparing the stable product form $M = (I - W)^\\top (I - W)$ with the expanded form susceptible to catastrophic cancellation (), you will gain an appreciation for how the very structure of your code can impact the accuracy and reliability of your scientific computations.",
            "id": "3141701",
            "problem": "You are given the task of examining, via computation, the rounding error impact of two algebraically equivalent ways to form the Locally Linear Embedding (LLE) matrix used in Locally Linear Embedding (LLE). In LLE, one constructs a weight matrix $W \\in \\mathbb{R}^{n \\times n}$ whose rows $w_i^\\top$ contain weights that reconstruct each data point $x_i \\in \\mathbb{R}^d$ from its $k$ nearest neighbors under the affine constraint that the weights sum to $1$. The standard graph Laplacian-like matrix used to find the low-dimensional embedding is constructed as $M = (I - W)^\\top (I - W)$, where $I$ is the $n \\times n$ identity matrix. Using matrix distributivity and transpose rules, this product can be expanded to an algebraically equivalent expression $M = I + W^\\top W - W - W^\\top$. Although these two expressions are mathematically identical in exact arithmetic, in floating-point arithmetic they can exhibit different rounding error behavior.\n\nBase principles permitted for use include: the distributivity and associativity of matrix multiplication, the transpose rule $(AB)^\\top = B^\\top A^\\top$, the Euclidean norm and its Frobenius variant, and the standard floating-point rounding model that a single arithmetic operation introduces a relative error of order machine precision. You must not assume any unproven properties beyond these fundamentals.\n\nImplement a program that:\n- Constructs $W$ from data by solving the local linear reconstruction problem with $k$ nearest neighbors, using a Tikhonov-style regularization to stabilize singular local covariances.\n- Forms two versions of $M$: the product form $M_{\\text{prod}} = (I - W)^\\top (I - W)$ and the expanded form $M_{\\text{exp}} = I + W^\\top W - W - W^\\top$.\n- Compares their numerical behavior using quantifiable metrics.\n\nWeight construction details:\n- For each point $x_i \\in \\mathbb{R}^d$, find its $k$ nearest neighbors by Euclidean distance, excluding itself.\n- Let $Z \\in \\mathbb{R}^{k \\times d}$ collect neighbor differences $x_{j} - x_i$ for the chosen neighbors. Form the local covariance $C = Z Z^\\top \\in \\mathbb{R}^{k \\times k}$.\n- Regularize $C$ by $C \\leftarrow C + \\lambda \\operatorname{trace}(C) I_k$, where $\\lambda gt; 0$ is a given scalar and $I_k$ is the $k \\times k$ identity matrix.\n- Solve $C w = \\mathbf{1}$ for $w \\in \\mathbb{R}^k$, and then normalize $w \\leftarrow w / (\\mathbf{1}^\\top w)$ to enforce the affine constraint $\\mathbf{1}^\\top w = 1$. Place these weights into row $i$ of $W$ at the neighbor indices.\n\nMetrics to compute for each case:\n- The Frobenius norm of the disagreement between the two computed matrices: $e_F = \\lVert M_{\\text{prod}} - M_{\\text{exp}} \\rVert_F$.\n- A symmetry residual for each form: $r_{\\text{sym,prod}} = \\lVert M_{\\text{prod}} - M_{\\text{prod}}^\\top \\rVert_F / \\lVert M_{\\text{prod}} \\rVert_F$ and $r_{\\text{sym,exp}} = \\lVert M_{\\text{exp}} - M_{\\text{exp}}^\\top \\rVert_F / \\lVert M_{\\text{exp}} \\rVert_F$, with the convention that $0$ is returned if the denominator is $0$.\n- The minimum eigenvalue of the symmetrized matrices $\\tfrac{1}{2}(M + M^\\top)$ for each form, denoted $\\lambda_{\\min,\\text{prod}}$ and $\\lambda_{\\min,\\text{exp}}$, respectively.\n\nTest suite:\nImplement and evaluate the above on the following $3$ cases. All random elements must use the specified seeds to ensure deterministic outputs.\n\n- Case $1$ (well-conditioned, nearly uniform manifold):\n  - $n = 64$, $d = 2$, $k = 8$, $\\lambda = 10^{-3}$.\n  - Data: points on a noisy unit circle. Let $\\theta_j = 2\\pi j / n + \\eta_j$ for $j = 0, \\ldots, n-1$, where $\\eta_j$ are independent Gaussian draws with mean $0$ and standard deviation $10^{-3}$ using random seed $0$. Set $x_j = (\\cos \\theta_j, \\sin \\theta_j)$.\n\n- Case $2$ (nearly collinear neighborhoods):\n  - $n = 80$, $d = 2$, $k = 10$, $\\lambda = 10^{-12}$.\n  - Data: points along a nearly straight line with tiny noise. Let $t_j = j/(n-1)$ for $j = 0, \\ldots, n-1$. Define $x_j = (t_j, 10^{-6} t_j + \\nu_j)$ where $\\nu_j$ are independent Gaussian draws with mean $0$ and standard deviation $10^{-8}$ using random seed $1$.\n\n- Case $3$ (high-dimensional anisotropic scaling):\n  - $n = 120$, $d = 10$, $k = 12$, $\\lambda = 10^{-3}$.\n  - Data: draw $n \\times d$ independent standard normal samples using random seed $2$, then scale column $i$ by $10^{i-1}$ for $i = 1, \\ldots, d$.\n\nFor each case, compute the metrics in the following order:\n- $e_F$\n- $r_{\\text{sym,prod}}$\n- $r_{\\text{sym,exp}}$\n- $\\lambda_{\\min,\\text{prod}}$\n- $\\lambda_{\\min,\\text{exp}}$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases and metrics as described, namely:\n$[e_F^{(1)}, r_{\\text{sym,prod}}^{(1)}, r_{\\text{sym,exp}}^{(1)}, \\lambda_{\\min,\\text{prod}}^{(1)}, \\lambda_{\\min,\\text{exp}}^{(1)}, e_F^{(2)}, r_{\\text{sym,prod}}^{(2)}, r_{\\text{sym,exp}}^{(2)}, \\lambda_{\\min,\\text{prod}}^{(2)}, \\lambda_{\\min,\\text{exp}}^{(2)}, e_F^{(3)}, r_{\\text{sym,prod}}^{(3)}, r_{\\text{sym,exp}}^{(3)}, \\lambda_{\\min,\\text{prod}}^{(3)}, \\lambda_{\\min,\\text{exp}}^{(3)}]$.\n\nAll outputs must be plain real numbers (floating-point) without units. Angles and physical units do not apply in this problem. The code must be fully self-contained and not accept any external input.",
            "solution": "The problem requires a computational investigation into the numerical stability of two algebraically equivalent expressions for the Locally Linear Embedding (LLE) matrix, denoted as $M$. The core of the task is to construct this matrix using prescribed data and methods, and then to quantify the numerical discrepancies between the two forms using a set of defined metrics.\n\nThe two expressions for the LLE matrix $M \\in \\mathbb{R}^{n \\times n}$ are:\n1.  The product form: $M_{\\text{prod}} = (I - W)^\\top (I - W)$\n2.  The expanded form: $M_{\\text{exp}} = I + W^\\top W - W - W^\\top$\n\nHere, $I$ is the $n \\times n$ identity matrix, and $W \\in \\mathbb{R}^{n \\times n}$ is the weight matrix that encodes the local linear relationships within the data. In exact arithmetic, these two forms are identical. However, in floating-point arithmetic, their behavior can differ significantly. The product form is generally more stable. The expanded form involves the subtraction of potentially large, nearly-equal matrices ($I + W^\\top W$ and $W + W^\\top$), a classic scenario for catastrophic cancellation, which can lead to a substantial loss of precision.\n\nThe solution is implemented by following a sequence of well-defined steps for each test case provided.\n\n### Step 1: Weight Matrix Construction\nThe weight matrix $W$ is constructed row by row. For each data point $x_i \\in \\mathbb{R}^d$ out of a total of $n$ points:\n\n1.  **Neighbor Identification**: The $k$ nearest neighbors of $x_i$ are identified by computing the squared Euclidean distance $d(x_i, x_j)^2 = \\lVert x_i - x_j \\rVert_2^2$ to all other points $x_j$ (where $j \\neq i$) and selecting the $k$ points with the smallest distances. Let the set of indices of these neighbors be $\\mathcal{N}_i$.\n\n2.  **Local Covariance**: A matrix $Z_i \\in \\mathbb{R}^{k \\times d}$ is formed, where each row consists of the vector difference between a neighbor and $x_i$. That is, the rows of $Z_i$ are $(x_j - x_i)$ for all $j \\in \\mathcal{N}_i$. The local covariance (or Gram) matrix is then computed as $C_i = Z_i Z_i^\\top$. This $k \\times k$ matrix encapsulates the geometry of the neighborhood of $x_i$.\n\n3.  **Regularization**: If the neighbors of $x_i$ are collinear or coplanar in a way that makes $C_i$ singular or ill-conditioned, solving for the reconstruction weights becomes unstable. To mitigate this, Tikhonov regularization is applied. The regularized covariance matrix is $C_{i, \\text{reg}} = C_i + \\lambda \\operatorname{trace}(C_i) I_k$, where $\\lambda  0$ is a small regularization parameter and $I_k$ is the $k \\times k$ identity matrix. This addition ensures that the matrix is positive definite and thus invertible.\n\n4.  **Weight Calculation**: The reconstruction weights for point $x_i$, denoted by the vector $w_i \\in \\mathbb{R}^k$, are found by solving the linear system $C_{i, \\text{reg}} w_i = \\mathbf{1}$, where $\\mathbf{1}$ is a vector of $k$ ones. This system arises from minimizing the reconstruction error $\\lVert x_i - \\sum_{j \\in \\mathcal{N}_i} w_{ij} x_j \\rVert_2^2$ under the constraint that the weights sum to one.\n\n5.  **Normalization**: The problem specifies an affine transformation (weights sum to $1$). The solution $w_i$ from the previous step does not strictly enforce this, so it is normalized: $w_i \\leftarrow w_i / (\\mathbf{1}^\\top w_i)$. This ensures $\\sum_j w_{ij} = 1$.\n\n6.  **Assembly of $W$**: A sparse $n \\times n$ matrix $W$ is implicitly formed. For each point $i$, the computed normalized weights are placed in the $i$-th row of $W$ at the columns corresponding to the neighbor indices in $\\mathcal{N}_i$. That is, $W_{ij} = (w_i)_m$ if the $m$-th neighbor is point $j$, and $W_{ij} = 0$ otherwise.\n\n### Step 2: Construction of $M_{\\text{prod}}$ and $M_{\\text{exp}}$\nOnce the weight matrix $W$ is computed, the two forms of the LLE matrix $M$ are constructed using standard matrix operations:\n- $M_{\\text{prod}}$ is computed as $(I - W)^\\top (I - W)$. This involves one matrix subtraction and one matrix multiplication. From a numerical standpoint, this construction is robust and guarantees (up to floating-point error) a symmetric positive semi-definite result.\n- $M_{\\text{exp}}$ is computed as $I + W^\\top W - W - W^\\top$. This involves multiple additions and subtractions. It is this formulation that is susceptible to numerical instability.\n\n### Step 3: Evaluation of Numerical Metrics\nThe numerical properties of $M_{\\text{prod}}$ and $M_{\\text{exp}}$ are quantified using five metrics:\n\n1.  **Frobenius Norm of Disagreement ($e_F$)**: Calculated as $e_F = \\lVert M_{\\text{prod}} - M_{\\text{exp}} \\rVert_F$. This metric directly measures the magnitude of the difference between the two computed matrices. A non-zero value is a direct consequence of floating-point inaccuracies.\n\n2.  **Symmetry Residual ($r_{\\text{sym}}$)**: Calculated for each form as $r_{\\text{sym, M}} = \\lVert M - M^\\top \\rVert_F / \\lVert M \\rVert_F$. Theoretically, both $M_{\\text{prod}}$ and $M_{\\text{exp}}$ are symmetric. This metric quantifies the loss of symmetry due to numerical errors. A smaller value indicates better preservation of this structural property.\n\n3.  **Minimum Eigenvalue of Symmetrized Matrix ($\\lambda_{\\min}$)**: Calculated for each form as $\\lambda_{\\min, M} = \\lambda_{\\min}\\left(\\frac{1}{2}(M + M^\\top)\\right)$. The LLE matrix $M$ should be positive semi-definite, meaning its eigenvalues must be non-negative. Catastrophic cancellation in $M_{\\text{exp}}$ can lead to a computed matrix that is not positive semi-definite. Symmetrizing the matrix ensures that its eigenvalues are real, allowing for a meaningful check. A negative minimum eigenvalue is a strong indicator of numerical failure, as it implies the quadratic form associated with the matrix is not convex, contrary to the theory of LLE.\n\nThese steps are systematically applied to each of the three test cases, which are designed to probe the numerical methods under different conditions: a well-behaved manifold, a nearly-degenerate configuration of points, and a high-dimensional anisotropic dataset. The results are then aggregated and formatted as required. The implementation utilizes `NumPy` for efficient array operations and `SciPy`'s robust linear algebra solvers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve, eigvalsh\n\ndef solve_problem():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Well-conditioned, nearly uniform manifold\n        {'n': 64, 'd': 2, 'k': 8, 'lambda_reg': 1e-3, 'seed': 0, 'case_id': 1},\n        # Case 2: Nearly collinear neighborhoods\n        {'n': 80, 'd': 2, 'k': 10, 'lambda_reg': 1e-12, 'seed': 1, 'case_id': 2},\n        # Case 3: High-dimensional anisotropic scaling\n        {'n': 120, 'd': 10, 'k': 12, 'lambda_reg': 1e-3, 'seed': 2, 'case_id': 3},\n    ]\n\n    all_results = []\n    for params in test_cases:\n        case_results = run_case(params)\n        all_results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef generate_data(n, d, seed, case_id):\n    \"\"\"\n    Generates synthetic data for a given test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    if case_id == 1:\n        # Noisy circle\n        eta = rng.normal(0, 1e-3, size=n)\n        theta = 2 * np.pi * np.arange(n) / n + eta\n        X = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n        return X\n    \n    elif case_id == 2:\n        # Nearly straight line with tiny noise\n        t = np.linspace(0, 1, n)\n        nu = rng.normal(0, 1e-8, size=n)\n        X = np.stack([t, 1e-6 * t + nu], axis=1)\n        return X\n\n    elif case_id == 3:\n        # High-dimensional anisotropic scaled data\n        A = rng.standard_normal(size=(n, d))\n        scaling_factors = 10.0**np.arange(d)\n        X = A * scaling_factors\n        return X\n    \n    raise ValueError(\"Invalid case_id\")\n\ndef construct_weight_matrix(X, n, k, lambda_reg):\n    \"\"\"\n    Constructs the LLE weight matrix W.\n    \"\"\"\n    W = np.zeros((n, n))\n    I_k = np.identity(k)\n    ones_k = np.ones(k)\n    \n    for i in range(n):\n        # Calculate squared Euclidean distances from point i\n        dist_sq = np.sum((X - X[i, :])**2, axis=1)\n        \n        # Get indices of k nearest neighbors (excluding self)\n        neighbor_indices = np.argsort(dist_sq)[1 : k + 1]\n        \n        # Form the neighborhood matrix Z (k x d)\n        neighbors = X[neighbor_indices, :]\n        Z = neighbors - X[i, :]\n        \n        # Form the local covariance matrix C (k x k)\n        C = Z @ Z.T\n        \n        # Regularize C\n        trace_C = np.trace(C)\n        if trace_C > 0:\n            reg_term = lambda_reg * trace_C\n            C_reg = C + reg_term * I_k\n        else:\n            C_reg = C\n        \n        # Solve C_reg * w = 1 for weights w\n        w = solve(C_reg, ones_k, assume_a='pos')\n        \n        # Normalize weights to sum to 1\n        sum_w = np.sum(w)\n        if sum_w != 0:\n            w_norm = w / sum_w\n        else:\n            # Fallback for the unlikely case of sum(w) = 0\n            w_norm = np.ones(k) / k\n        \n        # Place weights into the matrix W\n        W[i, neighbor_indices] = w_norm\n        \n    return W\n\ndef run_case(params):\n    \"\"\"\n    Executes one full test case and computes all metrics.\n    \"\"\"\n    n, d, k, lambda_reg, seed, case_id = params['n'], params['d'], params['k'], params['lambda_reg'], params['seed'], params['case_id']\n    \n    X = generate_data(n, d, seed, case_id)\n    W = construct_weight_matrix(X, n, k, lambda_reg)\n    \n    I = np.identity(n)\n    \n    # Form M using product form\n    A = I - W\n    M_prod = A.T @ A\n    \n    # Form M using expanded form\n    M_exp = I + W.T @ W - W - W.T\n    \n    # --- Compute Metrics ---\n    \n    # 1. Frobenius norm of the disagreement\n    e_F = np.linalg.norm(M_prod - M_exp, 'fro')\n    \n    # 2. Symmetry residual for product form\n    norm_M_prod = np.linalg.norm(M_prod, 'fro')\n    r_sym_prod = np.linalg.norm(M_prod - M_prod.T, 'fro') / norm_M_prod if norm_M_prod > 0 else 0.0\n        \n    # 3. Symmetry residual for expanded form\n    norm_M_exp = np.linalg.norm(M_exp, 'fro')\n    r_sym_exp = np.linalg.norm(M_exp - M_exp.T, 'fro') / norm_M_exp if norm_M_exp > 0 else 0.0\n        \n    # 4. Minimum eigenvalue of symmetrized product form\n    M_prod_sym = 0.5 * (M_prod + M_prod.T)\n    eigvals_prod = eigvalsh(M_prod_sym)\n    lambda_min_prod = eigvals_prod[0] # eigvalsh returns sorted eigenvalues\n    \n    # 5. Minimum eigenvalue of symmetrized expanded form\n    M_exp_sym = 0.5 * (M_exp + M_exp.T)\n    eigvals_exp = eigvalsh(M_exp_sym)\n    lambda_min_exp = eigvals_exp[0]\n    \n    return [e_F, r_sym_prod, r_sym_exp, lambda_min_prod, lambda_min_exp]\n\n# Execute the main function\nsolve_problem()\n```"
        }
    ]
}