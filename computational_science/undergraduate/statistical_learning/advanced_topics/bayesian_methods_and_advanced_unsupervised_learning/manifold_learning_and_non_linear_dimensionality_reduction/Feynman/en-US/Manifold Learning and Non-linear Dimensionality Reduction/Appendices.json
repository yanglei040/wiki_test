{
    "hands_on_practices": [
        {
            "introduction": "Linear methods like Principal Component Analysis (PCA) are powerful, but they fail when data lies on an intrinsically non-linear manifold. This exercise provides a hands-on demonstration using a classic \"cusp\" manifold, where nearby points in the ambient space are far apart along the manifold's true structure. By comparing the performance of PCA with a non-linear method like Isomap, you will see firsthand why manifold learning is necessary and learn to use quantitative metrics to evaluate how well an embedding preserves local geometry and avoids \"tears\" in the data .",
            "id": "3144257",
            "problem": "Consider the planar curve defined parametrically by $p(t) = (t^2, t^3)$ for $t \\in [-1,1]$. This set is a one-dimensional subset of $\\mathbb{R}^2$ with a cusp at $t = 0$. You will analyze non-linear dimensionality reduction procedures that attempt to map this subset into a one-dimensional target space while preserving local neighborhood structure and continuity. The goal is to construct the data, define an approximation to the intrinsic (geodesic) metric, produce embeddings using two algorithms, and quantify distortion using local Lipschitz estimates and a neighborhood preservation score.\n\nFundamental base:\n- Let $M \\subset \\mathbb{R}^2$ be the set $\\{p(t) : t \\in [-1,1]\\}$ sampled at $N$ points. The ambient Euclidean distance between two points $x, x' \\in \\mathbb{R}^2$ is $\\|x - x'\\|_2$.\n- A $k$-Nearest Neighbor (kNN) graph is constructed on the sample using ambient Euclidean distances, with undirected edges of weight equal to the Euclidean distance between neighbors. Let $k_{\\text{graph}}$ denote the graph connectivity parameter (number of neighbors).\n- The intrinsic geodesic distance $d_M(i,j)$ between samples $i$ and $j$ on $M$ is approximated by the shortest-path distance in this kNN graph, with edge weights equal to ambient Euclidean distances.\n- Classical Multidimensional Scaling (MDS) embeds points into a target dimension $d$ from a given symmetric distance matrix $D$ by double-centering the squared distances and extracting the top $d$ eigenpairs of the Gram matrix. Specifically, let $H = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ and $B = -\\frac{1}{2}H(D^{\\circ 2})H$, where $D^{\\circ 2}$ is the elementwise square. If $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots$ are the eigenvalues of $B$ with corresponding orthonormal eigenvectors $v_1, v_2, \\dots$, then an embedding into dimension $d$ is given by $Y = [\\sqrt{\\lambda_1} v_1, \\dots, \\sqrt{\\lambda_d} v_d]$ using only strictly positive eigenvalues.\n- Principal Component Analysis (PCA) embeds into $d=1$ by projecting centered data $X \\in \\mathbb{R}^{N \\times 2}$ onto the leading eigenvector of the sample covariance matrix.\n\nDistortion quantification:\n- Define a mapping $f: M \\to \\mathbb{R}$ that yields the one-dimensional embedding coordinate for each sampled point. For a fixed local neighborhood size $k_{\\text{local}}$, define for each index $i$ the set of intrinsic neighbors $N_{k_{\\text{local}}}^M(i)$ as the $k_{\\text{local}}$ indices with smallest $d_M(i,j)$ for $j \\ne i$. Define the local Lipschitz estimate at $i$ by\n$$\nL_i = \\max_{j \\in N_{k_{\\text{local}}}^M(i)} \\frac{|f(i) - f(j)|}{d_M(i,j)}.\n$$\nAggregate with the mean $L_{\\text{avg}} = \\frac{1}{N}\\sum_{i=1}^N L_i$ and the upper-tail descriptor $L_{95}$ defined as the $95$th percentile of $\\{L_i\\}_{i=1}^N$. Large values indicate local stretching and potential tears where nearby points in $M$ are mapped far apart.\n- Let $N_{k_{\\text{local}}}^Y(i)$ be the $k_{\\text{local}}$ indices with smallest embedding-space distances $|f(i) - f(j)|$ for $j \\ne i$. Define the neighborhood preservation score\n$$\nQ = \\frac{1}{N} \\sum_{i=1}^N \\frac{|N_{k_{\\text{local}}}^M(i) \\cap N_{k_{\\text{local}}}^Y(i)|}{k_{\\text{local}}}.\n$$\nValues of $Q$ in $[0,1]$ closer to $1$ indicate better local neighborhood preservation (continuity).\n\nAlgorithms to test:\n- Isometric Mapping (Isomap-like): compute $d_M$ via shortest paths on the kNN graph, then apply Classical Multidimensional Scaling (MDS) to $d_M$ to obtain an embedding into $d=1$. Denote the resulting coordinate function by $f_{\\text{iso}}$.\n- Principal Component Analysis (PCA): compute the leading principal component of the centered ambient coordinates $X$ and project onto it to obtain a one-dimensional embedding. Denote the resulting coordinate function by $f_{\\text{pca}}$.\n\nTasks to implement in a complete, runnable program:\n1. Sample $N$ points from the cusp curve by taking $t$ evenly spaced in $[-1,1]$ and mapping $p(t) = (t^2, t^3)$.\n2. Construct the undirected weighted kNN graph with parameter $k_{\\text{graph}}$ using ambient Euclidean distances among the sampled points.\n3. Compute the approximate intrinsic geodesic distance matrix $d_M$ using shortest paths on the kNN graph.\n4. Produce a one-dimensional embedding by:\n   - For Isomap-like: apply Classical Multidimensional Scaling (MDS) to $d_M$ to obtain $f_{\\text{iso}}$.\n   - For PCA: project onto the leading principal component to obtain $f_{\\text{pca}}$.\n5. For each embedding, compute $Q$, $L_{\\text{avg}}$, and $L_{95}$ using $k_{\\text{local}}$ intrinsic neighbors from $d_M$.\n\nTest suite:\n- Use $N=200$ and $k_{\\text{local}}=6$ for all cases. Evaluate the following four parameter cases that probe a happy path, a boundary, and an edge case:\n  1. Method Isomap-like with $k_{\\text{graph}}=8$ (happy path).\n  2. Method Isomap-like with $k_{\\text{graph}}=4$ (boundary: sparser graph, still connected).\n  3. Method Isomap-like with $k_{\\text{graph}}=30$ (edge case: potential short-circuiting near the cusp).\n  4. Method PCA with $k_{\\text{graph}}=8$ (linear baseline; $k_{\\text{graph}}$ used only for defining $d_M$ in the metrics).\nFor each case, the program must compute and return the triple $[Q, L_{\\text{avg}}, L_{95}]$ as floats.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the four triples, each triple itself being a list of three floats, enclosed in square brackets. The printed floats must have six digits after the decimal point. For example, the output should look like\n$[[q_1, \\ell_1^{\\text{avg}}, \\ell_1^{95}], [q_2, \\ell_2^{\\text{avg}}, \\ell_2^{95}], [q_3, \\ell_3^{\\text{avg}}, \\ell_3^{95}], [q_4, \\ell_4^{\\text{avg}}, \\ell_4^{95}]]$\nwith all numbers printed as decimal floats (no units).",
            "solution": "The problem requires an analysis of two dimensionality reduction algorithms, Principal Component Analysis (PCA) and an Isometric Mapping (Isomap) variant, on a synthetic dataset. The dataset is a sampling of the planar curve $p(t) = (t^2, t^3)$ for $t \\in [-1,1]$, which forms a one-dimensional manifold with a cusp. The quality of the one-dimensional embeddings produced by these algorithms is to be quantified using a neighborhood preservation score and local Lipschitz estimates.\n\nThe solution proceeds systematically through the tasks outlined in the problem statement.\n\n**1. Data Generation and k-Nearest Neighbor (kNN) Graph Construction**\n\nFirst, we generate the data. $N=200$ points are sampled from the manifold $M = \\{p(t) = (t^2, t^3) \\mid t \\in [-1,1]\\}$ by creating a vector of $N$ equally spaced values of $t$ in the interval $[-1, 1]$ and then computing the corresponding $(x,y)$ coordinates. Let this data matrix be $X \\in \\mathbb{R}^{N \\times 2}$.\n\nNext, for a given connectivity parameter $k_{\\text{graph}}$, we construct a weighted, undirected kNN graph. The vertices of the graph are the $N$ sampled points. An edge is established between two points based on their proximity in the ambient space $\\mathbb{R}^2$. The weight of an edge $(i, j)$ is the Euclidean distance $\\|x_i - x_j\\|_2$. An undirected graph is constructed by connecting points $i$ and $j$ if either is within the other's $k_{\\text{graph}}$ nearest neighbors. The all-pairs Euclidean distance matrix is first computed. Then, for each point $i$, its $k_{\\text{graph}}$ nearest neighbors are identified. Edges are added to an adjacency matrix for each such neighborhood relationship, ensuring the matrix is symmetric to represent an undirected graph. Non-existent edges are assigned an infinite weight.\n\n**2. Intrinsic Geodesic Distance Approximation**\n\nThe intrinsic geodesic distance $d_M(i,j)$ between two points $x_i, x_j \\in M$ is the length of the shortest path between them along the manifold. This is approximated by computing the all-pairs shortest-path distances on the constructed kNN graph. An algorithm such as Floyd-Warshall or running Dijkstra's from each node is used for this purpose. This computation yields the geodesic distance matrix $d_M \\in \\mathbb{R}^{N \\times N}$, which serves as the input for the Isomap-like embedding and as the ground truth for calculating distortion metrics.\n\n**3. Embedding Algorithms**\n\nTwo methods are used to embed the $N$ points into a one-dimensional space, $\\mathbb{R}^1$. The resulting embedding is a vector $f \\in \\mathbb{R}^N$, where $f(i)$ is the coordinate of the $i$-th point.\n\n- **Isomap-like (Classical MDS on $d_M$)**: This method, central to the Isomap algorithm, aims to find an embedding that preserves the geodesic distances. It applies Classical Multidimensional Scaling (MDS) to the geodesic distance matrix $d_M$. The procedure is as follows:\n    1.  From the distance matrix $D = d_M$, compute the element-wise squared distance matrix $D^{\\circ 2}$.\n    2.  Apply a double-centering transformation to $D^{\\circ 2}$ to obtain the Gram matrix $B = -\\frac{1}{2} H (D^{\\circ 2}) H$, where $H = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ is the centering matrix.\n    3.  Perform an eigendecomposition of the symmetric matrix $B$, yielding eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots$ and corresponding orthonormal eigenvectors $v_1, v_2, \\dots$.\n    4.  The one-dimensional embedding is constructed using the largest strictly positive eigenvalue $\\lambda_1$ and its eigenvector $v_1$: $f_{\\text{iso}} = \\sqrt{\\lambda_1} v_1$.\n\n- **Principal Component Analysis (PCA)**: PCA is a linear dimensionality reduction technique that projects the data onto the direction of maximum variance.\n    1.  The original data $X \\in \\mathbb{R}^{N \\times 2}$ is centered by subtracting the mean of each feature: $X_{\\text{centered}} = X - \\bar{X}$.\n    2.  The sample covariance matrix $C = \\frac{1}{N-1}X_{\\text{centered}}^\\top X_{\\text{centered}}$ is computed.\n    3.  An eigendecomposition of the $2 \\times 2$ matrix $C$ is performed. The eigenvector $u_1$ corresponding to the largest eigenvalue is the first principal component.\n    4.  The one-dimensional embedding is obtained by projecting the centered data onto this principal component: $f_{\\text{pca}} = X_{\\text{centered}} u_1$.\n\n**4. Distortion Quantification**\n\nFor each embedding $f$, we quantify its quality using the pre-computed intrinsic distance matrix $d_M$ and a local neighborhood size $k_{\\text{local}}=6$.\n\n- **Neighborhood Preservation Score ($Q$)**: This metric evaluates how well local neighborhoods are preserved. For each point $i$, we identify its $k_{\\text{local}}$ nearest neighbors in the intrinsic space, $N_{k_{\\text{local}}}^M(i)$, and in the embedding space, $N_{k_{\\text{local}}}^Y(i)$. The score is the average fractional overlap between these two neighborhood sets over all points:\n$$Q = \\frac{1}{N} \\sum_{i=1}^N \\frac{|N_{k_{\\text{local}}}^M(i) \\cap N_{k_{\\text{local}}}^Y(i)|}{k_{\\text{local}}}$$\nA value of $Q$ near $1$ implies excellent preservation of local structure.\n\n- **Local Lipschitz Estimates ($L_i$, $L_{\\text{avg}}$, $L_{95}$)**: These metrics measure local stretching of the manifold. For each point $i$, the local Lipschitz estimate is the maximum ratio of the change in embedding coordinates to the change in intrinsic distance, over its intrinsic neighborhood:\n$$L_i = \\max_{j \\in N_{k_{\\text{local}}}^M(i)} \\frac{|f(i) - f(j)|}{d_M(i,j)}$$\nFor a perfect isometric embedding, all $L_i=1$. Large values of $L_i$ indicate that points close on the manifold are mapped far apart in the embedding, suggesting a \"tear\". We compute the mean ($L_{\\text{avg}}$) and the $95$th percentile ($L_{95}$) of the set $\\{L_i\\}_{i=1}^N$ to summarize the distribution of these estimates.\n\nThe program implements these steps for the four specified test cases and computes the triple $[Q, L_{\\text{avg}}, L_{95}]$ for each.\n- **PCA vs. Isomap**: PCA, being linear, is expected to fail at \"unrolling\" the cusp curve, instead folding its two branches on top of each other. This will lead to poor neighborhood preservation ($Q \\ll 1$) near the origin. Isomap is designed for this task and should perform better.\n- **Isomap and $k_{\\text{graph}}$**: The choice of $k_{\\text{graph}}$ is critical for Isomap. A small $k_{\\text{graph}}$ (e.g., $4$) risks creating a disconnected graph. A large $k_{\\text{graph}}$ (e.g., $30$) can cause \"short-circuit\" errors, where the kNN graph incorrectly connects points across the cusp due to their small Euclidean distance, violating the manifold's intrinsic topology. This would degrade the geodesic distance approximation and the final embedding. An intermediate value (e.g., $8$) is expected to yield the best result.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import shortest_path\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Solves the manifold learning problem specified in the prompt.\n    \"\"\"\n\n    def run_isomap(d_M, N):\n        \"\"\"\n        Performs Isomap-like embedding using Classical MDS.\n        \"\"\"\n        # Element-wise square of the distance matrix\n        D2 = d_M**2\n        \n        # Double-centering\n        H = np.eye(N) - (1/N) * np.ones((N, N))\n        B = -0.5 * H @ D2 @ H\n        \n        # Eigendecomposition of the Gram matrix\n        # np.linalg.eigh is used for symmetric matrices and returns sorted eigenvalues\n        eigvals, eigvecs = np.linalg.eigh(B)\n        \n        # Select the top eigenpair (largest eigenvalue)\n        # eigh sorts in ascending order, so we take the last one.\n        lambda_1 = eigvals[-1]\n        v_1 = eigvecs[:, -1]\n        \n        # Handle potential small negative eigenvalues due to numerical precision\n        if lambda_1 < 0:\n            lambda_1 = 0\n            \n        # Return 1D embedding\n        return np.sqrt(lambda_1) * v_1\n\n    def run_pca(X, N):\n        \"\"\"\n        Performs PCA for 1D embedding.\n        \"\"\"\n        # Center the data\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute covariance matrix\n        cov_mat = (X_centered.T @ X_centered) / (N - 1)\n        \n        # Eigendecomposition of the covariance matrix\n        eigvals, eigvecs = np.linalg.eigh(cov_mat)\n        \n        # Select the principal eigenvector (corresponding to the largest eigenvalue)\n        # eigh sorts in ascending order, so we take the last one.\n        u_1 = eigvecs[:, -1]\n        \n        # Project centered data onto the principal component\n        return X_centered @ u_1\n    \n    def compute_metrics(d_M, f_embedding, N, k_local):\n        \"\"\"\n        Computes the Q, L_avg, and L_95 metrics.\n        \"\"\"\n        # Find intrinsic and embedding space neighborhoods\n        intrinsic_neighbors = np.argsort(d_M, axis=1)[:, 1:k_local + 1]\n        \n        embedding_dists = np.abs(f_embedding[:, np.newaxis] - f_embedding)\n        embedding_neighbors = np.argsort(embedding_dists, axis=1)[:, 1:k_local + 1]\n\n        # Compute Neighborhood Preservation Score (Q)\n        q_sum = 0\n        for i in range(N):\n            intersection_size = len(np.intersect1d(intrinsic_neighbors[i], embedding_neighbors[i]))\n            q_sum += intersection_size / k_local\n        Q = q_sum / N\n\n        # Compute Local Lipschitz Estimates (L)\n        L_vals = np.zeros(N)\n        for i in range(N):\n            neighbors_i = intrinsic_neighbors[i]\n            d_M_local = d_M[i, neighbors_i]\n            d_f_local = np.abs(f_embedding[i] - f_embedding[neighbors_i])\n            \n            # np.divide handles division by zero (or inf) correctly for this case.\n            # If d_M_local is 0, it would be an error, but it won't be for j != i.\n            # If d_M_local is inf (disconnected graph), ratio becomes 0, which is fine.\n            ratios = np.divide(d_f_local, d_M_local, out=np.zeros_like(d_f_local), where=d_M_local!=0)\n            L_vals[i] = np.max(ratios)\n\n        L_avg = np.mean(L_vals)\n        L_95 = np.percentile(L_vals, 95)\n        \n        return [Q, L_avg, L_95]\n\n    # Global parameters\n    N = 200\n    k_local = 6\n    \n    # Define the four test cases\n    test_cases = [\n        {\"method\": \"isomap\", \"k_graph\": 8},\n        {\"method\": \"isomap\", \"k_graph\": 4},\n        {\"method\": \"isomap\", \"k_graph\": 30},\n        {\"method\": \"pca\", \"k_graph\": 8}\n    ]\n\n    # Generate the dataset from the cusp curve\n    t_vals = np.linspace(-1, 1, N)\n    X = np.vstack((t_vals**2, t_vals**3)).T\n\n    # Pre-compute Euclidean distances once\n    euclidean_dists = squareform(pdist(X, 'euclidean'))\n    \n    final_results = []\n    for case in test_cases:\n        method = case[\"method\"]\n        k_graph = case[\"k_graph\"]\n        \n        # 1. Construct kNN graph and compute geodesic distance matrix d_M\n        # Adjacency matrix weighted by distance, infinite for non-edges\n        adj_mat = np.full((N, N), np.inf)\n        np.fill_diagonal(adj_mat, 0)\n        \n        # Build a symmetric kNN graph (mutual kNN)\n        for i in range(N):\n            # Find k_graph nearest neighbors (excluding self)\n            neighbors = np.argsort(euclidean_dists[i, :])[1:k_graph + 1]\n            for j in neighbors:\n                adj_mat[i, j] = euclidean_dists[i, j]\n                adj_mat[j, i] = euclidean_dists[j, i] # ensure symmetry\n        \n        d_M = shortest_path(csgraph=adj_mat, directed=False)\n        \n        # Check for graph connectivity, problem statement implies it stays connected\n        if np.any(np.isinf(d_M)):\n            raise RuntimeError(f\"Graph is not connected for k_graph={k_graph}\")\n\n        # 2. Produce the 1D embedding\n        if method == \"isomap\":\n            f_embedding = run_isomap(d_M, N)\n        elif method == \"pca\":\n            f_embedding = run_pca(X, N)\n        \n        # 3. Compute and store metrics for the current case\n        metrics = compute_metrics(d_M, f_embedding, N, k_local)\n        final_results.append(metrics)\n    \n    # Format the final output string\n    result_str_parts = []\n    for res in final_results:\n        q, l_avg, l_95 = res\n        part = f\"[{q:.6f},{l_avg:.6f},{l_95:.6f}]\"\n        result_str_parts.append(part)\n        \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "After understanding the need for graph-based methods, the next step is to explore the nuances of their construction. The behavior of algorithms like Diffusion Maps depends critically on how the graph's affinity matrix is normalized to create a transition matrix or a graph Laplacian. This practice guides you through implementing and comparing two fundamental approaches: the row-stochastic normalization common in random walk perspectives and the symmetric normalization used in spectral clustering. By examining the resulting embeddings on datasets with varying point densities, you will gain a crucial understanding of how these choices impact the geometry of the embedding space .",
            "id": "3144175",
            "problem": "You are given a set of finite point clouds embedded in Euclidean space, and you will construct a weighted graph on each point cloud. From this graph, you will form two normalizations to define two related Markov chains: a row-stochastic random walk normalization and a symmetric normalization. Your task is to implement both normalizations, compute their leading embeddings, and quantify how the choice of normalization affects the Euclidean distances in embedding space relative to the diffusion distance. All computations should be performed in pure mathematical terms without physical units, and angles must be specified in radians.\n\nFoundational base: Start from the following fundamental definitions and facts.\n\n- Let $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ be a dataset. Define a symmetric, nonnegative weight matrix $W \\in \\mathbb{R}^{n \\times n}$ by\n$$\nW_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right) \\quad \\text{for } i \\neq j, \\quad W_{ii} = 0,\n$$\nwhere $\\sigma > 0$ is a specified bandwidth and $\\|\\cdot\\|$ is the Euclidean norm.\n\n- Define the diagonal degree matrix $D \\in \\mathbb{R}^{n \\times n}$ with entries $D_{ii} = \\sum_{j=1}^n W_{ij}$.\n\n- Define the row-stochastic random walk matrix $P \\in \\mathbb{R}^{n \\times n}$ by\n$$\nP = D^{-1} W,\n$$\nwhich satisfies $\\sum_{j=1}^n P_{ij} = 1$ for each $i$. The largest eigenvalue of $P$ is $1$, and for connected, undirected graphs derived from symmetric $W$, $P$ is similar to a symmetric matrix and has real eigenvalues.\n\n- Define the symmetric normalization $S \\in \\mathbb{R}^{n \\times n}$ by\n$$\nS = D^{-1/2} W D^{-1/2}.\n$$\nThe matrices $P$ and $S$ are related by similarity:\n$$\nS = D^{1/2} P D^{-1/2}, \\quad \\text{and} \\quad P = D^{-1/2} S D^{1/2}.\n$$\nTherefore, $P$ and $S$ have the same set of eigenvalues.\n\n- Let $\\{\\lambda_k\\}_{k=0}^{n-1}$ denote the eigenvalues of $S$ in descending order, with $\\lambda_0 = 1$, and let $\\{u_k\\}_{k=0}^{n-1}$ be corresponding orthonormal eigenvectors of $S$ (orthonormal in the standard Euclidean inner product). The right eigenvectors $\\{\\psi_k\\}_{k=0}^{n-1}$ of $P$ are related to the eigenvectors of $S$ via\n$$\n\\psi_k = D^{-1/2} u_k,\n$$\nand satisfy $P \\psi_k = \\lambda_k \\psi_k$.\n\n- For integer diffusion time $t \\geq 1$, consider the Diffusion Maps (DM) embedding derived from the row-stochastic normalization (using the right eigenvectors of $P$):\n$$\n\\Phi_t(i) = \\left(\\lambda_1^t \\psi_1(i), \\lambda_2^t \\psi_2(i), \\ldots, \\lambda_m^t \\psi_m(i)\\right) \\in \\mathbb{R}^m,\n$$\nfor a chosen embedding dimension $m \\in \\mathbb{N}$ excluding the trivial eigenpair $\\lambda_0 = 1$. The Euclidean distance in this embedding equals the diffusion distance:\n$$\nD_t(i,j)^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(\\psi_k(i) - \\psi_k(j)\\right)^2,\n$$\nwhen $m$ is large enough to capture the spectrum, and is a monotone approximation when truncated.\n\n- Consider also the embedding from the symmetric normalization:\n$$\n\\Psi_t(i) = \\left(\\lambda_1^t u_1(i), \\lambda_2^t u_2(i), \\ldots, \\lambda_m^t u_m(i)\\right) \\in \\mathbb{R}^m.\n$$\nIts Euclidean distances are\n$$\n\\tilde{D}_t(i,j)^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(u_k(i) - u_k(j)\\right)^2.\n$$\nIn general, $\\tilde{D}_t(i,j)$ is not equal to $D_t(i,j)$ unless the graph has constant degree, in which case $u_k(i) = \\sqrt{D_{ii}} \\, \\psi_k(i)$ implies $\\tilde{D}_t(i,j) = \\sqrt{D_{ii}} \\, D_t(i,j)$ for all $i$ and $j$, yielding a constant scaling factor.\n\nYour program must:\n\n1. For each test case, construct $W$, compute $D$, and form both $P$ and $S$.\n2. Compute the leading $m$ nontrivial eigenpairs $(\\lambda_k, u_k)$ of $S$ (excluding the trivial eigenvalue $\\lambda_0 = 1$), and obtain $\\psi_k = D^{-1/2} u_k$.\n3. Build the embeddings $\\Phi_t$ (row-stochastic) and $\\Psi_t$ (symmetric) of dimension $m$ for diffusion time $t$.\n4. Compute all pairwise Euclidean distances between embedded points for both embeddings:\n   - For the row-stochastic embedding, compute $D_t(i,j)$.\n   - For the symmetric embedding, compute $\\tilde{D}_t(i,j)$.\n5. Quantify the discrepancy between the two sets of distances using:\n   - The relative root-mean-square difference:\n     $$\n     E_{\\mathrm{rel}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{(i,j), i<j} \\left(\\tilde{D}_t(i,j) - D_t(i,j)\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{(i,j), i<j} D_t(i,j)^2}},\n     $$\n     where the sum is over all unordered pairs $(i,j)$ with $i<j$, and $M = \\frac{n(n-1)}{2}$ is the number of pairs.\n   - The standard deviation of the pairwise ratios:\n     $$\n     \\mathrm{StdRatio} = \\mathrm{Std}\\left(\\left\\{\\frac{\\tilde{D}_t(i,j)}{D_t(i,j)} \\,:\\, i<j\\right\\}\\right),\n     $$\n     interpreting $\\frac{\\tilde{D}_t(i,j)}{D_t(i,j)}$ only for $D_t(i,j) > 0$.\n\n6. Output, for each test case, the two floats $E_{\\mathrm{rel}}$ and $\\mathrm{StdRatio}$, rounded to six decimal places.\n\nTest Suite:\n\n- Case $1$ (regular degrees on a circle):\n  - $n = 30$ points on the unit circle in $\\mathbb{R}^2$ at angles $\\theta_i = \\frac{2\\pi i}{n}$ for $i = 0, 1, \\ldots, n-1$, where angles are in radians; points are $x_i = (\\cos(\\theta_i), \\sin(\\theta_i))$.\n  - Bandwidth $\\sigma = 0.3$.\n  - Embedding dimension $m = 3$.\n  - Diffusion time $t = 1$.\n  Expected behavior: $\\mathrm{StdRatio}$ is near zero because degrees are equal, and $E_{\\mathrm{rel}}$ is near zero up to a constant scaling absorbed by the ratio behavior.\n\n- Case $2$ (moderately heterogeneous degrees on a line):\n  - $n = 30$ points in $\\mathbb{R}^1$, consisting of $18$ points uniformly spaced on $[0, 0.4]$ and $12$ points uniformly spaced on $[0.6, 1.0]$.\n  - Bandwidth $\\sigma = 0.15$.\n  - Embedding dimension $m = 3$.\n  - Diffusion time $t = 1$.\n  Expected behavior: Both $E_{\\mathrm{rel}}$ and $\\mathrm{StdRatio}$ are non-negligible due to degree heterogeneity.\n\n- Case $3$ (strongly heterogeneous degrees; near-disconnected clusters):\n  - $n = 30$ points in $\\mathbb{R}^1$, consisting of $25$ points uniformly spaced on $[0, 0.3]$ and $5$ points uniformly spaced on $[0.7, 1.0]$.\n  - Bandwidth $\\sigma = 0.1$.\n  - Embedding dimension $m = 3$.\n  - Diffusion time $t = 1$.\n  Expected behavior: Larger $E_{\\mathrm{rel}}$ and $\\mathrm{StdRatio}$ due to very uneven degrees and weak inter-cluster connectivity.\n\nFinal Output Format:\n\nYour program should produce a single line of output containing the six rounded results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\text{Case 1 } E_{\\mathrm{rel}}, \\text{Case 1 } \\mathrm{StdRatio}, \\text{Case 2 } E_{\\mathrm{rel}}, \\text{Case 2 } \\mathrm{StdRatio}, \\text{Case 3 } E_{\\mathrm{rel}}, \\text{Case 3 } \\mathrm{StdRatio}],\n$$\nfor example, $\\left[0.000123,0.000456,0.012345,0.067890,0.123456,0.234567\\right]$.",
            "solution": "The problem requires a comparison between two different normalization schemes for a graph affinity matrix in the context of manifold learning. Specifically, we will analyze the difference in Euclidean distances between points embedded using a row-stochastic normalization (standard for Diffusion Maps) and a symmetric normalization.\n\nThe solution proceeds in four main stages for each test case:\n1.  Construction of the graph and relevant matrices.\n2.  Spectral decomposition to find eigenvalues and eigenvectors.\n3.  Construction of the low-dimensional embeddings.\n4.  Computation of pairwise distances and the specified discrepancy metrics.\n\n**1. Graph Construction**\n\nGiven a set of points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$, we first construct a weighted graph where the points are vertices. The affinity between two distinct points $x_i$ and $x_j$ is quantified by a weight $W_{ij}$, computed using a Gaussian kernel with a specified bandwidth $\\sigma > 0$. The weight matrix $W \\in \\mathbb{R}^{n \\times n}$ has entries:\n$$ W_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right) \\quad \\text{for } i \\neq j, \\quad W_{ii} = 0 $$\nwhere $\\|\\cdot\\|$ denotes the Euclidean norm. This choice of kernel ensures that closer points have a stronger connection (larger weight). The matrix $W$ is symmetric and non-negative.\n\nFrom $W$, we define the diagonal degree matrix $D \\in \\mathbb{R}^{n \\times n}$, where each diagonal entry $D_{ii}$ represents the total affinity of point $x_i$:\n$$ D_{ii} = \\sum_{j=1}^n W_{ij} $$\nSince $W_{ij} > 0$ for all $i \\neq j$ (due to the nature of the exponential function), every point is connected to every other point, ensuring $D_{ii} > 0$ for all $i$. Thus, $D$ is invertible.\n\nTwo normalizations are then defined:\n- The **row-stochastic random walk matrix** $P \\in \\mathbb{R}^{n \\times n}$:\n$$ P = D^{-1} W $$\nEach row of $P$ sums to $1$, so $P_{ij}$ can be interpreted as the probability of transitioning from node $i$ to node $j$ in one step of a random walk on the graph.\n- The **symmetric normalization** $S \\in \\mathbb{R}^{n \\times n}$:\n$$ S = D^{-1/2} W D^{-1/2} $$\nwhere $D^{-1/2}$ is the diagonal matrix with entries $(D^{-1/2})_{ii} = 1/\\sqrt{D_{ii}}$. The matrix $S$ is symmetric, which is advantageous for numerical eigendecomposition.\n\n**2. Spectral Analysis**\n\nThe core of the method lies in the spectral properties of these matrices. $P$ and $S$ are related by a similarity transformation, $S = D^{1/2} P D^{-1/2}$, and thus share the same set of eigenvalues, which we denote as $\\{\\lambda_k\\}_{k=0}^{n-1}$ in descending order ($1 = \\lambda_0 \\geq \\lambda_1 \\geq \\dots \\geq \\lambda_{n-1}$).\n\nWe compute the eigenpairs of the symmetric matrix $S$. Let $\\{u_k\\}_{k=0}^{n-1}$ be the set of orthonormal eigenvectors of $S$ corresponding to the eigenvalues $\\{\\lambda_k\\}_{k=0}^{n-1}$.\n$$ S u_k = \\lambda_k u_k $$\nThe right eigenvectors of $P$, denoted $\\{\\psi_k\\}_{k=0}^{n-1}$, are then obtained from the eigenvectors of $S$ via the transformation:\n$$ \\psi_k = D^{-1/2} u_k $$\nThese vectors satisfy the eigenvalue equation $P \\psi_k = \\lambda_k \\psi_k$. The eigenvector $\\psi_0$ corresponding to $\\lambda_0=1$ is a constant vector and is considered trivial, so it is excluded from the embedding.\n\n**3. Embedding Construction**\n\nFor a chosen embedding dimension $m \\in \\mathbb{N}$ and diffusion time $t \\geq 1$, we construct two different embeddings into $\\mathbb{R}^m$.\n\n- The first embedding, $\\Phi_t: \\{x_i\\}_{i=1}^n \\to \\mathbb{R}^m$, is the standard Diffusion Map embedding derived from the row-stochastic matrix $P$. The $i$-th point $x_i$ is mapped to:\n$$ \\Phi_t(i) = \\left(\\lambda_1^t \\psi_1(i), \\lambda_2^t \\psi_2(i), \\ldots, \\lambda_m^t \\psi_m(i)\\right) $$\n- The second embedding, $\\Psi_t: \\{x_i\\}_{i=1}^n \\to \\mathbb{R}^m$, is constructed similarly but uses the eigenvectors of the symmetric matrix $S$:\n$$ \\Psi_t(i) = \\left(\\lambda_1^t u_1(i), \\lambda_2^t u_2(i), \\ldots, \\lambda_m^t u_m(i)\\right) $$\n\n**4. Distance Comparison and Metric Calculation**\n\nThe central task is to quantify the difference between the geometric structures of these two embeddings. We do this by comparing the pairwise Euclidean distances.\nFor any two points $x_i, x_j$, the squared Euclidean distance in the first embedding is the squared diffusion distance $D_t(i,j)^2$:\n$$ D_t(i,j)^2 = \\|\\Phi_t(i) - \\Phi_t(j)\\|^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(\\psi_k(i) - \\psi_k(j)\\right)^2 $$\nThe squared Euclidean distance in the second embedding is:\n$$ \\tilde{D}_t(i,j)^2 = \\|\\Psi_t(i) - \\Psi_t(j)\\|^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(u_k(i) - u_k(j)\\right)^2 $$\n\nWe compute two metrics to quantify the discrepancy between the sets of distances $\\{D_t(i,j)\\}$ and $\\{\\tilde{D}_t(i,j)\\}$ over all pairs $(i,j)$ with $i<j$.\n- The **relative root-mean-square difference** $E_{\\mathrm{rel}}$ measures the overall magnitude of the difference relative to the magnitude of the diffusion distances:\n$$ E_{\\mathrm{rel}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{i<j} \\left(\\tilde{D}_t(i,j) - D_t(i,j)\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{i<j} D_t(i,j)^2}} = \\frac{\\left(\\sum_{i<j} (\\tilde{D}_t(i,j) - D_t(i,j))^2\\right)^{1/2}}{\\left(\\sum_{i<j} D_t(i,j)^2\\right)^{1/2}} $$\nwhere $M = n(n-1)/2$.\n- The **standard deviation of the pairwise ratios** $\\mathrm{StdRatio}$ measures the non-uniformity of the relationship between the two distance measures:\n$$ \\mathrm{StdRatio} = \\mathrm{Std}\\left(\\left\\{\\frac{\\tilde{D}_t(i,j)}{D_t(i,j)} \\mid i<j, D_t(i,j) > 0\\right\\}\\right) $$\nIf the degrees are uniform, $D$ is a multiple of the identity matrix, which implies $\\psi_k$ is proportional to $u_k$. This results in a constant ratio $\\tilde{D}_t(i,j)/D_t(i,j)$ for all pairs, and thus $\\mathrm{StdRatio}$ will be zero. Heterogeneity in data density leads to non-uniform degrees and a non-zero $\\mathrm{StdRatio}$.\n\nThe implementation calculates these quantities for each specified test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import pdist, cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'id': 1, 'n': 30, 'sigma': 0.3, 'm': 3, 't': 1},\n        {'id': 2, 'n': 30, 'sigma': 0.15, 'm': 3, 't': 1},\n        {'id': 3, 'n': 30, 'sigma': 0.1, 'm': 3, 't': 1},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Generate data for the test case\n        n, sigma, m, t = case['n'], case['sigma'], case['m'], case['t']\n        \n        if case['id'] == 1:\n            # Case 1: Points on a unit circle\n            angles = 2 * np.pi * np.arange(n) / n\n            X = np.column_stack([np.cos(angles), np.sin(angles)])\n        elif case['id'] == 2:\n            # Case 2: Moderately heterogeneous degrees on a line\n            n1, n2 = 18, 12\n            X1 = np.linspace(0.0, 0.4, n1)\n            X2 = np.linspace(0.6, 1.0, n2)\n            X = np.concatenate((X1, X2)).reshape(-1, 1)\n        elif case['id'] == 3:\n            # Case 3: Strongly heterogeneous degrees on a line\n            n1, n2 = 25, 5\n            X1 = np.linspace(0.0, 0.3, n1)\n            X2 = np.linspace(0.7, 1.0, n2)\n            X = np.concatenate((X1, X2)).reshape(-1, 1)\n        \n        # 2. Construct W, D, and S\n        # Compute pairwise squared Euclidean distances to build the affinity matrix W\n        dist_sq = cdist(X, X, 'sqeuclidean')\n        W = np.exp(-dist_sq / (2 * sigma**2))\n        np.fill_diagonal(W, 0)\n        \n        # Compute degree matrix D and its inverse square root\n        D_diag = W.sum(axis=1)\n        D_diag[D_diag == 0] = 1e-12 # Avoid division by zero for isolated points\n        D_inv_sqrt_diag = 1.0 / np.sqrt(D_diag)\n        D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n        \n        # Compute the symmetrically normalized matrix S\n        S = D_inv_sqrt @ W @ D_inv_sqrt\n        \n        # 3. Compute eigenpairs of S\n        evals, evecs = eigh(S)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(evals)[::-1]\n        evals = evals[idx]\n        evecs = evecs[:, idx]\n        \n        # 4. Extract non-trivial eigenpairs and compute eigenvectors of P\n        # We exclude the first eigenpair (lambda_0=1, u_0)\n        lambdas_m = evals[1:m+1]\n        u_m = evecs[:, 1:m+1]\n        \n        # Compute right eigenvectors of P, psi_k = D^(-1/2) u_k\n        psi_m = D_inv_sqrt @ u_m\n        \n        # 5. Build the two embeddings\n        lambdas_m_t = lambdas_m**t\n        \n        # Embedding from row-stochastic normalization (P)\n        phi_embedding = psi_m * lambdas_m_t\n        # Embedding from symmetric normalization (S)\n        psi_embedding_sym = u_m * lambdas_m_t\n        \n        # 6. Compute pairwise Euclidean distances in both embedding spaces\n        dist_phi = pdist(phi_embedding, 'euclidean')\n        dist_psi_sym = pdist(psi_embedding_sym, 'euclidean')\n        \n        # 7. Quantify the discrepancy between distance sets\n        \n        # Relative root-mean-square difference (E_rel)\n        norm_dist_phi = np.linalg.norm(dist_phi)\n        if norm_dist_phi < 1e-12:\n            norm_diff = np.linalg.norm(dist_psi_sym - dist_phi)\n            E_rel = 0.0 if norm_diff < 1e-12 else np.inf\n        else:\n            E_rel = np.linalg.norm(dist_psi_sym - dist_phi) / norm_dist_phi\n            \n        # Standard deviation of pairwise ratios (StdRatio)\n        valid_indices = np.where(dist_phi > 1e-12)[0]\n        if len(valid_indices) > 1:\n            ratios = dist_psi_sym[valid_indices] / dist_phi[valid_indices]\n            StdRatio = np.std(ratios)\n        else:\n            StdRatio = 0.0 # Not enough pairs with non-zero distance for std dev\n            \n        results.extend([E_rel, StdRatio])\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The graph Laplacian is more than just a tool for dimensionality reduction; it is a fundamental object that encodes the intrinsic geometry of the data. This exercise demonstrates how this geometric information can be integrated into predictive models, specifically in a semi-supervised learning context. You will derive the connection between the discrete graph Laplacian and continuous Tikhonov regularization on a manifold, and then apply this principle to infer labels for unlabeled data points. This practice reveals how manifold assumptions can regularize a model, encouraging the learned function to be smooth with respect to the data's underlying structure .",
            "id": "3144216",
            "problem": "Consider a compact smooth one-dimensional manifold $\\mathcal{M}$ embedded in $\\mathbb{R}^{2}$, from which $n=3$ samples $\\{x_{1},x_{2},x_{3}\\}$ are drawn along a smooth curve. Construct an undirected weighted graph with symmetric positive weights $w_{ij}=w_{ji}$, and define the weighted adjacency matrix $W=[w_{ij}]$, the degree matrix $D=\\operatorname{diag}(d_{1},d_{2},d_{3})$ with $d_{i}=\\sum_{j=1}^{3}w_{ij}$, and the combinatorial graph Laplacian $L=D-W$. Assume the nonlocal graph energy $\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}$ approximates the Dirichlet energy $\\int_{\\mathcal{M}}\\|\\nabla f\\|^{2}\\,\\mathrm{d}\\mu$ used in Tikhonov smoothing on $\\mathcal{M}$, where $f:\\{x_{1},x_{2},x_{3}\\}\\to\\mathbb{R}$ is a function on the sampled points.\n\nYou are given the specific graph with weights $w_{12}=1$, $w_{23}=2$, and $w_{13}=0$. Let $y_{1}=1$, $y_{3}=0$ be two labels and $y_{2}$ be unknown. Consider semi-supervised regression with a manifold prior by minimizing the objective\n$$\nJ(f)=\\frac{1}{2}\\sum_{i\\in\\mathcal{L}}(f_{i}-y_{i})^{2}+\\frac{\\gamma}{2}\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{3}f_{i}^{2},\n$$\nwhere $\\mathcal{L}=\\{1,3\\}$ is the labeled index set, $\\gamma>0$ controls the manifold smoothness, and $\\lambda>0$ is a Tikhonov ridge parameter. Take $\\gamma=1$ and $\\lambda=1$. Let $M=\\operatorname{diag}(m_{1},m_{2},m_{3})$ be the diagonal selector with $m_{i}=1$ if $i\\in\\mathcal{L}$ and $m_{i}=0$ otherwise.\n\nTasks:\n- Starting from the definitions of $W$, $D$, and $L$, derive the exact algebraic identity that connects the graph quadratic form and the pairwise difference energy, thereby establishing the discrete counterpart of Tikhonov smoothing.\n- Using the objective $J(f)$ and only the above foundational definitions, derive the first-order optimality condition that determines the unique minimizer $f^{\\star}$.\n- Compute the explicit values of $f^{\\star}=(f_{1}^{\\star},f_{2}^{\\star},f_{3}^{\\star})$ for the given graph, labels, and parameters.\n\nGive the final $f^{\\star}$ in exact rational form for each component. Do not round.",
            "solution": "The problem is well-posed and scientifically sound, set within the standard framework of graph-based semi-supervised learning. We will address the three tasks sequentially as requested.\n\nLet $f = (f_1, f_2, f_3)^T \\in \\mathbb{R}^3$ be the vector of function values on the graph nodes. The problem provides the adjacency matrix weights $w_{12}=1$, $w_{23}=2$, and $w_{13}=0$. Since the graph is undirected, the weights are symmetric, i.e., $w_{ij}=w_{ji}$.\n\n### Task 1: Identity between Pairwise Energy and Graph Laplacian Quadratic Form\n\nThe first task is to establish the algebraic identity connecting the pairwise difference energy, $\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}$, and the graph Laplacian quadratic form, $f^T L f$.\n\nThe graph Laplacian is defined as $L=D-W$, where $W=[w_{ij}]$ is the weighted adjacency matrix and $D=\\operatorname{diag}(d_{1},d_{2},d_{3})$ is the degree matrix with entries $d_i = \\sum_{j=1}^{3} w_{ij}$.\n\nLet's expand the pairwise difference energy term:\n$$\n\\sum_{i,j=1}^{3} w_{ij}(f_i - f_j)^2 = \\sum_{i,j=1}^{3} w_{ij}(f_i^2 - 2f_i f_j + f_j^2)\n$$\nWe can separate this into three parts:\n$$\n\\sum_{i,j=1}^{3} w_{ij}f_i^2 - 2\\sum_{i,j=1}^{3} w_{ij}f_i f_j + \\sum_{i,j=1}^{3} w_{ij}f_j^2\n$$\nLet's analyze each part in matrix notation.\nThe first part can be rewritten by factoring out $f_i^2$:\n$$\n\\sum_{i=1}^{3} f_i^2 \\left(\\sum_{j=1}^{3} w_{ij}\\right) = \\sum_{i=1}^{3} d_i f_i^2 = f^T D f\n$$\nThe second part is directly related to the quadratic form associated with the adjacency matrix $W$:\n$$\n-2\\sum_{i,j=1}^{3} w_{ij}f_i f_j = -2 f^T W f\n$$\nFor the third part, we can swap the summation indices $i$ and $j$:\n$$\n\\sum_{i,j=1}^{3} w_{ij}f_j^2 = \\sum_{j,i=1}^{3} w_{ji}f_i^2\n$$\nSince the weights are symmetric ($w_{ij}=w_{ji}$), this becomes:\n$$\n\\sum_{i,j=1}^{3} w_{ij}f_i^2 = \\sum_{i=1}^{3} f_i^2 \\left(\\sum_{j=1}^{3} w_{ij}\\right) = \\sum_{i=1}^{3} d_i f_i^2 = f^T D f\n$$\nCombining all three parts, we get:\n$$\n\\sum_{i,j=1}^{3} w_{ij}(f_i - f_j)^2 = f^T D f - 2 f^T W f + f^T D f = 2 f^T D f - 2 f^T W f\n$$\nFactoring out $2f^T$ on the left and $f$ on the right gives:\n$$\n2 f^T (D - W) f = 2 f^T L f\n$$\nThus, the exact algebraic identity is:\n$$\n\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2} = 2 f^T L f\n$$\nThis demonstrates that the graph Laplacian quadratic form is half the pairwise energy sum, which is the discrete analogue of the squared $H^1$ semi-norm (Dirichlet energy).\n\n### Task 2: First-Order Optimality Condition\n\nThe second task is to derive the first-order optimality condition for the unique minimizer $f^{\\star}$ of the objective function $J(f)$.\nThe objective function is:\n$$\nJ(f)=\\frac{1}{2}\\sum_{i\\in\\mathcal{L}}(f_{i}-y_{i})^{2}+\\frac{\\gamma}{2}\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{3}f_{i}^{2}\n$$\nWe can rewrite this function in matrix form. Let $y$ be a vector in $\\mathbb{R}^3$ where $y_i$ is the given label if $i \\in \\mathcal{L}$ and $0$ otherwise. Given $y_1=1$ and $y_3=0$, we define $y = (1, 0, 0)^T$. The selector matrix $M=\\operatorname{diag}(m_1, m_2, m_3)$ has $m_i=1$ if $i \\in \\mathcal{L}=\\{1,3\\}$ and $m_i=0$ otherwise, so $M = \\operatorname{diag}(1,0,1)$.\nThe first term is:\n$$\n\\frac{1}{2}\\sum_{i\\in\\mathcal{L}}(f_{i}-y_{i})^{2} = \\frac{1}{2}\\sum_{i=1}^{3}m_{i}(f_{i}-y_{i})^{2} = \\frac{1}{2}(f-y)^T M (f-y)\n$$\nUsing the identity from Task 1, the second term is:\n$$\n\\frac{\\gamma}{2}\\sum_{i,j=1}^{3}w_{ij}(f_{i}-f_{j})^{2} = \\frac{\\gamma}{2}(2 f^T L f) = \\gamma f^T L f\n$$\nThe third term is a standard ridge penalty:\n$$\n\\frac{\\lambda}{2}\\sum_{i=1}^{3}f_{i}^{2} = \\frac{\\lambda}{2} f^T f = \\frac{\\lambda}{2} f^T I f\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix.\nThe full objective function in matrix form is:\n$$\nJ(f) = \\frac{1}{2}(f-y)^T M (f-y) + \\gamma f^T L f + \\frac{\\lambda}{2} f^T I f\n$$\n$J(f)$ is a strictly convex function because it is a sum of quadratic forms with positive semi-definite matrices $M$ and $L$, and a positive definite matrix $\\lambda I$ (since $\\lambda>0$). Therefore, a unique minimizer $f^\\star$ exists. To find it, we compute the gradient of $J(f)$ with respect to $f$ and set it to the zero vector.\nUsing standard matrix calculus identities, and noting that $M$, $L$, and $I$ are symmetric, the gradient is:\n$$\n\\nabla_f J(f) = M(f-y) + 2\\gamma L f + \\lambda I f\n$$\nSetting $\\nabla_f J(f^\\star) = 0$:\n$$\nM(f^\\star - y) + 2\\gamma L f^\\star + \\lambda I f^\\star = 0\n$$\n$$\n(M + 2\\gamma L + \\lambda I) f^\\star = M y\n$$\nThis is the first-order optimality condition that determines the unique minimizer $f^\\star$.\n\n### Task 3: Computation of the Explicit Values\n\nThe final task is to compute $f^\\star = (f_1^\\star, f_2^\\star, f_3^\\star)^T$ for the given parameters $\\gamma=1$ and $\\lambda=1$.\nFirst, construct the matrices $W$, $D$, and $L$.\nThe weights are $w_{12}=1$, $w_{23}=2$, $w_{13}=0$.\nThe adjacency matrix $W$ is:\n$$\nW = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 2 \\\\ 0 & 2 & 0 \\end{pmatrix}\n$$\nThe degrees are $d_1 = w_{12}+w_{13} = 1$, $d_2 = w_{21}+w_{23} = 1+2=3$, $d_3 = w_{31}+w_{32} = 0+2=2$.\nThe degree matrix $D$ is:\n$$\nD = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}\n$$\nThe graph Laplacian $L=D-W$ is:\n$$\nL = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 3 & -2 \\\\ 0 & -2 & 2 \\end{pmatrix}\n$$\nThe selector matrix for $\\mathcal{L}=\\{1,3\\}$ is $M=\\operatorname{diag}(1,0,1)$:\n$$\nM = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe label vector is $y=(y_1, 0, y_3)^T=(1, 0, 0)^T$.\nThe optimality condition with $\\gamma=1, \\lambda=1$ is $(M + 2L + I)f^\\star = My$.\nLet's compute the matrix $A = M + 2L + I$:\n$$\nA = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + 2\\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 3 & -2 \\\\ 0 & -4 & 4 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 2 & -2 & 0 \\\\ -2 & 6 & -4 \\\\ 0 & -4 & 4 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & -2 & 0 \\\\ -2 & 7 & -4 \\\\ 0 & -4 & 6 \\end{pmatrix}\n$$\nNow, compute the right-hand side vector $b = My$:\n$$\nb = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nWe must solve the linear system $Af^\\star=b$:\n$$\n\\begin{pmatrix} 4 & -2 & 0 \\\\ -2 & 7 & -4 \\\\ 0 & -4 & 6 \\end{pmatrix} \\begin{pmatrix} f_1^\\star \\\\ f_2^\\star \\\\ f_3^\\star \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives a system of three linear equations:\n1. $4f_1^\\star - 2f_2^\\star = 1$\n2. $-2f_1^\\star + 7f_2^\\star - 4f_3^\\star = 0$\n3. $-4f_2^\\star + 6f_3^\\star = 0$\n\nFrom equation (3), $6f_3^\\star = 4f_2^\\star$, so $f_3^\\star = \\frac{4}{6}f_2^\\star = \\frac{2}{3}f_2^\\star$.\nFrom equation (1), $4f_1^\\star = 1+2f_2^\\star$, so $f_1^\\star = \\frac{1}{4} + \\frac{1}{2}f_2^\\star$.\nSubstitute these into equation (2):\n$$\n-2\\left(\\frac{1}{4} + \\frac{1}{2}f_2^\\star\\right) + 7f_2^\\star - 4\\left(\\frac{2}{3}f_2^\\star\\right) = 0\n$$\n$$\n-\\frac{1}{2} - f_2^\\star + 7f_2^\\star - \\frac{8}{3}f_2^\\star = 0\n$$\n$$\n6f_2^\\star - \\frac{8}{3}f_2^\\star = \\frac{1}{2}\n$$\n$$\n\\left(\\frac{18}{3} - \\frac{8}{3}\\right)f_2^\\star = \\frac{1}{2}\n$$\n$$\n\\frac{10}{3}f_2^\\star = \\frac{1}{2}\n$$\n$$\nf_2^\\star = \\frac{1}{2} \\cdot \\frac{3}{10} = \\frac{3}{20}\n$$\nNow, we find $f_1^\\star$ and $f_3^\\star$:\n$$\nf_1^\\star = \\frac{1}{4} + \\frac{1}{2}f_2^\\star = \\frac{1}{4} + \\frac{1}{2}\\left(\\frac{3}{20}\\right) = \\frac{1}{4} + \\frac{3}{40} = \\frac{10}{40} + \\frac{3}{40} = \\frac{13}{40}\n$$\n$$\nf_3^\\star = \\frac{2}{3}f_2^\\star = \\frac{2}{3}\\left(\\frac{3}{20}\\right) = \\frac{2}{20} = \\frac{1}{10}\n$$\nThe solution is $f^\\star = (\\frac{13}{40}, \\frac{3}{20}, \\frac{1}{10})^T$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{13}{40} \\\\\n\\frac{3}{20} \\\\\n\\frac{1}{10}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}