{
    "hands_on_practices": [
        {
            "introduction": "理论的最佳检验是实践。让我们从一个拉普拉斯近似表现出色的理想情境开始：贝叶斯线性回归。在这个模型中，后验分布恰好是高斯分布，这意味着拉普拉斯近似是精确的，而非近似。这个练习将让你直观地看到拉普拉斯近似的一个核心优势——它能够准确捕捉参数之间的相关性结构，这与一些更简单的方法（如平均场变分推断）形成了鲜明对比。",
            "id": "3137211",
            "problem": "你必须编写一个完整、可运行的程序，构建一个具有强相关性的双参数贝叶斯后验，计算拉普拉斯近似的协方差和相关性，并通过评估库尔贝克-莱布勒散度（KL）将其与独立高斯变分近似进行比较。请在纯粹的数学设置下进行全部工作，并使用以下生成模型作为基础。\n\n考虑一个带有两个参数的贝叶斯线性模型。对于给定的数据矩阵 $X \\in \\mathbb{R}^{n \\times 2}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，假设似然是方差已知的高斯分布，先验是各向同性高斯分布：\n- 似然：$y \\mid \\theta \\sim \\mathcal{N}(X \\theta, \\sigma^{2} I)$，其中 $\\theta \\in \\mathbb{R}^{2}$，$I$ 是单位矩阵。\n- 先验：$\\theta \\sim \\mathcal{N}(0, \\tau^{2} I)$。\n\n你必须根据贝叶斯法则和多元正态分布的标准恒等式，推导出后验 $p(\\theta \\mid y)$ 的形式、关于其众数的拉普拉斯近似的形式，以及最小化从 $q(\\theta)$ 到拉普拉斯近似的库尔贝克-莱布勒散度（KL）的最佳独立高斯变分近似 $q(\\theta)$。除了这些基础事实和恒等式，你不能假设任何快捷公式。\n\n每个测试用例的数据生成应按以下方式执行：\n- 从标准正态分布中抽取独立同分布的条目，得到 $x_{1} \\in \\mathbb{R}^{n}$。\n- 从标准正态分布中抽取独立同分布的条目，得到 $z \\in \\mathbb{R}^{n}$，且与 $x_{1}$ 独立。\n- 构建 $x_{2} = \\rho x_{1} + \\sqrt{1 - \\rho^{2}} \\, z$ 以通过 $\\rho \\in (-1, 1)$ 控制共线性。\n- 构成 $X = [x_{1}, x_{2}] \\in \\mathbb{R}^{n \\times 2}$。\n- 固定一个真实参数向量 $\\theta_{\\text{true}} \\in \\mathbb{R}^{2}$，生成噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$，并设 $y = X \\theta_{\\text{true}} + \\varepsilon$。\n\n你的程序必须：\n- 计算在最大后验（MAP）点处的拉普拉斯近似。在此模型中，众数处的拉普拉斯协方差等于众数处后验精度（负海森矩阵）的逆。\n- 计算拉普拉斯协方差下的后验相关性，即拉普拉斯协方差所隐含的 $\\theta$ 两个坐标之间的相关性。\n- 计算最佳独立高斯变分近似 $q(\\theta) = \\mathcal{N}(\\mu, \\operatorname{diag}(s^{2}))$，该近似最小化了从 $q(\\theta)$ 到拉普拉斯近似的库尔贝克-莱布勒散度（KL），然后评估此 KL 值。\n- 为每个测试用例报告两个浮点数：拉普拉斯后验相关性和最佳独立高斯变分近似相对于拉普拉斯近似的 KL 值。\n\n此问题不涉及角度。不涉及物理单位；报告纯数字。所有最终数值输出应四舍五入到 $6$ 位小数。\n\n测试套件：\n使用以下测试用例，其中 $n$ 是样本数，$\\rho$ 控制特征共线性，$\\sigma$ 是噪声标准差，$\\tau$ 是先验标准差，最后一个元素是随机种子以确保可复现性。在所有情况下，使用固定的真实参数 $\\theta_{\\text{true}} = [1.5, -1.0]^{\\top}$。\n\n- 案例 $1$：$(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.95, 0.5, 3.0, 0)$\n- 案例 $2$：$(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.50, 0.5, 3.0, 1)$\n- 案例 $3$：$(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.995, 0.5, 3.0, 2)$\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，包含一个含 $2 \\times 3 = 6$ 个浮点数的列表：对于按上述顺序给出的每个测试用例，先附加拉普拉斯后验相关性，然后是 KL 值。最终输出必须是遵循以下确切格式的单行：\n- 示例形状（非实际值）：$[c_{1},k_{1},c_{2},k_{2},c_{3},k_{3}]$\n- 每个值必须四舍五入到 $6$ 位小数。",
            "solution": "该问题是有效的，因为它具有科学依据、问题设定良好、客观且自成体系。我们将进行完整的推导和解答。\n\n### 1. 后验分布推导\n\n分析始于使用贝叶斯法则来寻找参数 $\\theta$ 的后验分布。后验分布与似然和先验的乘积成正比：\n$$\np(\\theta | y, X) \\propto p(y | \\theta, X) p(\\theta)\n$$\n似然和先验被给出为高斯分布：\n-   似然：$p(y | \\theta, X) = \\mathcal{N}(y | X\\theta, \\sigma^2 I) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta)\\right)$\n-   先验：$p(\\theta) = \\mathcal{N}(\\theta | 0, \\tau^2 I) \\propto \\exp\\left(-\\frac{1}{2\\tau^2}\\theta^T\\theta\\right)$\n\n因此，后验分布的对数为：\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\n其中 $C$ 是一个与 $\\theta$ 无关的归一化常数。为了确定后验的形式，我们展开这些项并按 $\\theta$ 的幂次进行分组：\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y^Ty - 2y^TX\\theta + \\theta^T X^T X \\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2}\\left( \\theta^T \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right) \\theta - 2\\theta^T \\left(\\frac{1}{\\sigma^2}X^Ty\\right) \\right) + C'\n$$\n这个表达式是 $\\theta$ 的二次型，这表明后验分布也是一个多元高斯分布，即 $p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Sigma_p)$。一个通用的多元高斯分布 $\\mathcal{N}(\\mu, \\Sigma)$ 的对数密度为 $-\\frac{1}{2}(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu) + \\text{const} = -\\frac{1}{2}(\\theta^T\\Sigma^{-1}\\theta - 2\\theta^T\\Sigma^{-1}\\mu + \\text{const})$。\n\n通过将对数后验中的项与通用形式进行比较，我们可以确定后验精度矩阵 $\\Lambda_p = \\Sigma_p^{-1}$ 和后验均值 $\\mu_p$：\n$$\n\\Lambda_p = \\Sigma_p^{-1} = \\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\n$$\n$$\n\\Lambda_p \\mu_p = \\frac{1}{\\sigma^2}X^Ty \\implies \\mu_p = \\Lambda_p^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right) = \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right)^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right)\n$$\n因此，后验分布为 $p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Lambda_p^{-1})$。\n\n### 2. 拉普拉斯近似与后验相关性\n\n拉普拉斯近似为后验分布提供了一个高斯近似，其中心位于后验的众数（最大后验估计或 MAP 估计，$\\theta_{\\text{MAP}}$）。这个高斯分布的协方差由在众数处评估的对数后验的海森矩阵的负逆给出。\n\n首先，我们通过将对数后验关于 $\\theta$ 的梯度设为零来找到众数：\n$$\n\\nabla_\\theta \\log p(\\theta | y, X) = \\frac{1}{\\sigma^2}(X^Ty - X^TX\\theta) - \\frac{1}{\\tau^2}\\theta = 0\n$$\n解出 $\\theta$ 得 $\\theta_{\\text{MAP}} = \\mu_p$。由于后验是精确的高斯分布，其众数等于其均值。\n\n接下来，我们计算海森矩阵（二阶导数矩阵）：\n$$\n\\nabla_\\theta^2 \\log p(\\theta | y, X) = -\\frac{1}{\\sigma^2}X^TX - \\frac{1}{\\tau^2}I = -\\Lambda_p\n$$\n海森矩阵是常数，不依赖于 $\\theta$。拉普拉斯近似的协方差 $\\Sigma_L$ 为：\n$$\n\\Sigma_L = \\left(-\\nabla_\\theta^2 \\log p(\\theta | y, X)\\Big|_{\\theta_{\\text{MAP}}}\\right)^{-1} = (\\Lambda_p)^{-1} = \\Sigma_p\n$$\n对于这个特定模型，拉普拉斯近似不是一个近似；它是精确的后验分布，即 $q_L(\\theta) = p(\\theta | y, X)$。所需后验相关性从这个精确的协方差矩阵 $\\Sigma_L = \\Sigma_p$ 计算得出。对于一个 $2 \\times 2$ 的协方差矩阵 $\\Sigma_L = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$，相关性为：\n$$\n\\rho_{12} = \\frac{\\Sigma_{12}}{\\sqrt{\\Sigma_{11}\\Sigma_{22}}}\n$$\n\n### 3. 变分近似与 KL 散度\n\n我们寻求最佳的独立高斯变分近似 $q(\\theta) = q_1(\\theta_1)q_2(\\theta_2) = \\mathcal{N}(\\theta | \\mu_q, \\Sigma_q)$，其中 $\\Sigma_q$ 是一个对角矩阵。目标是最小化从 $q(\\theta)$ 到真实后验 $p(\\theta|y,X)$ 的库尔贝克-莱布勒（KL）散度，在本例中这等同于拉普拉斯近似 $q_L(\\theta)$。目标是最小化 $\\text{KL}(q || q_L)$。\n\n两个多元高斯分布 $q=\\mathcal{N}(\\mu_q, \\Sigma_q)$ 和 $p=\\mathcal{N}(\\mu_p, \\Sigma_p)$ 之间的 KL 散度通用公式为：\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - d + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) + (\\mu_p - \\mu_q)^T\\Sigma_p^{-1}(\\mu_p - \\mu_q) \\right]\n$$\n其中 $d$ 是维度（此处 $d=2$）。当 $\\mu_q = \\mu_p$ 时，涉及均值的项在零处最小化。\n\n对于协方差，平均场变分推断对高斯目标 $p(\\theta) = \\mathcal{N}(\\mu_p, \\Sigma_p)$ 的标准结果是，最优的分解分布 $q(\\theta) = \\mathcal{N}(\\mu_q, \\Sigma_q)$（其中 $\\Sigma_q$ 为对角阵）具有 $\\mu_q = \\mu_p$ 和一个精度矩阵 $\\Lambda_q = \\Sigma_q^{-1}$，该精度矩阵等于目标精度矩阵的对角部分，即 $\\Lambda_q = \\text{diag}(\\Lambda_p)$。也就是说，$\\Sigma_q = (\\text{diag}(\\Lambda_p))^{-1}$。\n\n当 $\\mu_q = \\mu_p$ 时，KL 散度简化为：\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - 2 + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) \\right]\n$$\n让我们评估这些项。设 $\\Lambda_p = \\Sigma_p^{-1}$。变分精度为 $\\Lambda_q = \\text{diag}(\\Lambda_{p,11}, \\Lambda_{p,22})$。\n迹项变为：\n$$\n\\text{tr}(\\Sigma_p^{-1}\\Sigma_q) = \\text{tr}(\\Lambda_p \\Lambda_q^{-1}) = \\text{tr}\\left( \\begin{pmatrix} \\Lambda_{p,11} & \\Lambda_{p,12} \\\\ \\Lambda_{p,21} & \\Lambda_{p,22} \\end{pmatrix} \\begin{pmatrix} 1/\\Lambda_{p,11} & 0 \\\\ 0 & 1/\\Lambda_{p,22} \\end{pmatrix} \\right) = \\text{tr}\\begin{pmatrix} 1 & \\dots \\\\ \\dots & 1 \\end{pmatrix} = 2\n$$\n对数行列式比为：\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{|\\Lambda_q|}{|\\Lambda_p|} = \\frac{\\Lambda_{p,11}\\Lambda_{p,22}}{\\Lambda_{p,11}\\Lambda_{p,22} - \\Lambda_{p,12}^2} = \\frac{1}{1 - \\frac{\\Lambda_{p,12}^2}{\\Lambda_{p,11}\\Lambda_{p,22}}}\n$$\n从协方差矩阵 $\\Sigma_p$ 计算的相关性是 $\\rho_{12}$。从精度矩阵 $\\Lambda_p$ 计算的相关性是 $\\rho_{\\Lambda,12} = \\frac{\\Lambda_{p,12}}{\\sqrt{\\Lambda_{p,11}\\Lambda_{p,22}}}$。对于一个 $2 \\times 2$ 矩阵，成立 $\\rho_{12} = -\\rho_{\\Lambda,12}$。因此，$\\rho_{12}^2 = \\rho_{\\Lambda,12}^2$。\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{1}{1 - \\rho_{12}^2}\n$$\n将这些代入 KL 公式，得到一个非常简单的结果：\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\left(\\frac{1}{1 - \\rho_{12}^2}\\right) - 2 + 2 \\right] = -\\frac{1}{2}\\log(1 - \\rho_{12}^2)\n$$\nKL 散度，用于衡量分解近似的不充分性，仅取决于参数的后验相关性的平方。\n\n### 4. 算法\n\n对于每个测试用例：\n1.  设置随机种子。根据指定的程序，使用参数 $n$ 和 $\\rho$ 生成具有相关列的数据矩阵 $X \\in \\mathbb{R}^{n \\times 2}$。\n2.  计算后验精度矩阵 $\\Lambda_p = \\frac{1}{\\sigma^2}X^T X + \\frac{1}{\\tau^2}I$。\n3.  对 $\\Lambda_p$ 求逆，以找到后验协方差矩阵 $\\Sigma_p = \\Sigma_L = \\Lambda_p^{-1}$。\n4.  从 $\\Sigma_L$ 中提取元素 $\\Sigma_{11}$、$\\Sigma_{22}$ 和 $\\Sigma_{12}$。\n5.  计算后验相关性 $\\rho_{12} = \\Sigma_{12} / \\sqrt{\\Sigma_{11}\\Sigma_{22}}$。\n6.  计算最佳独立高斯变分近似的 KL 散度为 $\\text{KL} = -0.5 \\log(1 - \\rho_{12}^2)$。\n7.  将两个结果都四舍五入到 $6$ 位小数并存储它们。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For a Bayesian linear model with Gaussian likelihood and prior, the posterior\n    is also Gaussian. This function computes its properties.\n    \"\"\"\n    \n    # Test cases: (n, rho, sigma, tau, seed)\n    test_cases = [\n        (200, 0.95, 0.5, 3.0, 0),\n        (200, 0.50, 0.5, 3.0, 1),\n        (200, 0.995, 0.5, 3.0, 2),\n    ]\n\n    # Fixed true parameter vector for data generation\n    theta_true = np.array([1.5, -1.0])\n\n    results = []\n    for case in test_cases:\n        n, rho, sigma, tau, seed = case\n\n        # 1. Data Generation\n        # Set a random number generator with a seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate x1 and z from standard normal distributions.\n        x1 = rng.standard_normal(n)\n        z = rng.standard_normal(n)\n        \n        # Construct x2 to have a specified correlation rho with x1.\n        x2 = rho * x1 + np.sqrt(1 - rho**2) * z\n        \n        # Form the data matrix X.\n        X = np.stack([x1, x2], axis=1) # Shape: (n, 2)\n        \n        # Generate response vector y. Note: y is not needed for the posterior\n        # covariance, correlation, or the KL divergence, as these depend only\n        # on X, sigma, and tau in this model.\n        # eps = rng.normal(0, sigma, n)\n        # y = X @ theta_true + eps\n\n        # 2. Compute Posterior Precision and Covariance (Laplace Approximation)\n        # The posterior precision matrix is Lambda_p = (1/sigma^2) * X'X + (1/tau^2) * I\n        XtX = X.T @ X\n        lambda_p = (1 / sigma**2) * XtX + (1 / tau**2) * np.eye(2)\n        \n        # The posterior covariance matrix is the inverse of the precision matrix.\n        # This is also the covariance of the Laplace approximation.\n        sigma_l = np.linalg.inv(lambda_p)\n\n        # 3. Compute Posterior Correlation\n        # Extract elements of the covariance matrix.\n        sigma_11 = sigma_l[0, 0]\n        sigma_22 = sigma_l[1, 1]\n        sigma_12 = sigma_l[0, 1]\n        \n        # Calculate the correlation coefficient.\n        correlation = sigma_12 / np.sqrt(sigma_11 * sigma_22)\n\n        # 4. Compute KL Divergence\n        # The KL divergence for the best independent Gaussian variational approximation\n        # has a simple closed form related to the posterior correlation.\n        kl_divergence = -0.5 * np.log(1 - correlation**2)\n\n        # Append rounded results to the list.\n        results.append(round(correlation, 6))\n        results.append(round(kl_divergence, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在理想情况之后，我们转向一个更常见且更具挑战性的场景：当后验分布不对称时。我们将使用一个经典的泊松-伽马共轭模型，其后验是一个倾斜的伽马分布。由于分布的偏度，其众数（峰值所在）与均值（质量中心）并不重合，而标准的拉普拉斯近似正是围绕众数构建的。这个练习旨在让你量化这种错位导致的误差，通过计算在真实后验和拉普拉斯近似下预期损失的差异，从而深刻理解该方法的一个关键局限性。",
            "id": "3137250",
            "problem": "考虑以下贝叶斯单参数模型，其中独立观测值从泊松分布中抽取，且先验为 Gamma 分布。设 $\\{y_i\\}_{i=1}^n$ 是从参数为 $\\lambda > 0$ 的泊松分布 $\\text{Poisson}(\\lambda)$ 中抽取的独立样本，并使用形状参数为 $a > 0$、率参数为 $b > 0$ 的 Gamma 先验 $\\lambda \\sim \\text{Gamma}(a,b)$。使用贝叶斯定理以及泊松似然和 Gamma 先验的定义作为基本依据。最大后验 (MAP) 估计量是后验众数。拉普拉斯近似通过一个以众数为中心的高斯分布来近似 MAP 附近的后验密度，其方差等于对数后验在众数处的负二阶导数（对于标量即负海森矩阵）的倒数。\n\n你的任务：\n- 推导后验密度（不含归一化常数），并将其表示为一种公认的参数形式。\n- 通过对对数后验求导，推导后验均值和后验众数。\n- 使用以验后众数为中心的拉普拉斯近似，根据对数后验在众数处的二阶导数推导标量高斯方差。\n- 对于由平方损失 $L(\\lambda; t) = (\\lambda - t)^2$ 给出的凸风险，计算在精确后验和拉普拉斯近似下的期望损失。仅使用这些分布的期望定义和你推导出的量进行计算。\n- 实现一个程序，对下面测试套件中的每个测试用例，计算精确期望损失与拉普拉斯近似期望损失之间的绝对差。\n\n测试套件：\n- 案例 1：先验 $(a,b) = (2.5, 1.0)$，数据 $y = (0,1,2)$，损失目标 $t$ 等于此案例的后验众数。\n- 案例 2：先验 $(a,b) = (0.3, 1.0)$，数据 $y = (1)$，损失目标 $t$ 等于此案例数据的样本均值。\n- 案例 3：先验 $(a,b) = (2.0, 1.0)$，数据 $y = (10,12,9,11,8)$，损失目标 $t = 10.0$。\n- 案例 4：先验 $(a,b) = (3.0, 0.5)$，数据 $y = (0,0,0,1)$，损失目标 $t = 0.8$。\n\n程序要求：\n- 对每个案例，计算后验形状参数 $\\alpha$ 和率参数 $\\beta$、后验均值和众数、众数处的拉普拉斯方差，以及在精确后验和拉普拉斯近似下的期望平方损失。\n- 对每个案例，以浮点数形式输出精确期望损失与拉普拉斯近似期望损失之间的绝对差。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个数字四舍五入到六位小数（例如，$[0.123456,0.000001,2.718282,1.000000]$）。\n\n本问题陈述中所有的数学符号、变量、函数、运算符和数字均使用 LaTeX 书写。本问题不涉及物理单位、角度或百分比；所有数值输出必须是无单位的实数。",
            "solution": "该问题要求对一个涉及泊松似然和 Gamma 先验的贝叶斯单参数模型进行深入分析。此框架是贝叶斯统计中共轭族的一个经典例子。我们将首先验证问题陈述的合理性（已确认其合理），然后系统地推导所需的量。\n\n该模型规定如下：\n- 数据 $\\{y_i\\}_{i=1}^n$ 是从参数为 $\\lambda$ 的泊松分布中抽取的独立同分布样本：$y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$。\n- 未知参数 $\\lambda$ 的先验分布是形状参数为 $a$、率参数为 $b$ 的 Gamma 分布：$\\lambda \\sim \\text{Gamma}(a, b)$。\n\n泊松分布的概率质量函数 (PMF) 为 $P(y \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}$，其中 $y \\in \\{0, 1, 2, \\dots\\}$。Gamma 分布的概率密度函数 (PDF) 为 $p(\\lambda \\mid a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}e^{-b\\lambda}$，其中 $\\lambda > 0$。\n\n首先，我们推导 $\\lambda$ 的后验密度。根据贝叶斯定理，后验密度正比于似然与先验密度的乘积：\n$$ p(\\lambda \\mid y_1, \\dots, y_n) \\propto P(y_1, \\dots, y_n \\mid \\lambda) \\, p(\\lambda) $$\n鉴于观测值是独立的，似然函数是个体概率质量函数的乘积：\n$$ P(y_1, \\dots, y_n \\mid \\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} $$\n作为 $\\lambda$ 的函数，似然函数正比于 $e^{-n\\lambda} \\lambda^{\\sum y_i}$。先验密度正比于 $\\lambda^{a-1}e^{-b\\lambda}$。\n将两者结合，后验密度为：\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\left(e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}\\right) \\left(\\lambda^{a-1}e^{-b\\lambda}\\right) $$\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{(\\sum y_i + a) - 1} e^{-(n+b)\\lambda} $$\n该表达式是 Gamma 分布的核。因此，后验分布也是一个 Gamma 分布，这证明了 Gamma 先验对于泊松似然的共轭性。后验分布为 $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$，其更新后的参数为：\n- 后验形状参数：$\\alpha = \\sum_{i=1}^n y_i + a$\n- 后验率参数：$\\beta = n + b$\n\n其次，我们推导后验均值和后验众数。对于 Gamma 分布 $\\text{Gamma}(\\alpha, \\beta)$，其均值和众数是众所周知的量。\n- 后验均值为 $E[\\lambda \\mid \\mathbf{y}] = \\frac{\\alpha}{\\beta} = \\frac{\\sum y_i + a}{n + b}$。\n- 后验众数（最大后验估计，$\\lambda_{\\text{MAP}}$）通过最大化后验密度得到。对于形状参数 $\\alpha > 1$ 的 Gamma 分布，其众数为 $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta} = \\frac{\\sum y_i + a - 1}{n + b}$。问题中提供的所有测试用例都满足条件 $\\alpha > 1$。\n\n第三，我们推导拉普拉斯近似的方差。拉普拉斯近似使用一个以验后众数 $\\lambda_{\\text{MAP}}$ 为中心的高斯（正态）分布来近似后验密度。其方差（我们记为 $\\sigma_L^2$）是在众数处求值的对数后验密度的负二阶导数的倒数。\n对数后验（不含加法常数）为：\n$$ \\log p(\\lambda \\mid \\mathbf{y}) = (\\alpha-1)\\log\\lambda - \\beta\\lambda + C $$\n关于 $\\lambda$ 的一阶导数是：\n$$ \\frac{d}{d\\lambda} \\log p(\\lambda \\mid \\mathbf{y}) = \\frac{\\alpha-1}{\\lambda} - \\beta $$\n二阶导数是：\n$$ \\frac{d^2}{d\\lambda^2} \\log p(\\lambda \\mid \\mathbf{y}) = -\\frac{\\alpha-1}{\\lambda^2} $$\n在众数 $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta}$ 处计算负二阶导数（即观测费雪信息）：\n$$ J(\\lambda_{\\text{MAP}}) = -\\left(-\\frac{\\alpha-1}{(\\frac{\\alpha-1}{\\beta})^2}\\right) = \\frac{\\alpha-1}{(\\alpha-1)^2 / \\beta^2} = \\frac{\\beta^2}{\\alpha-1} $$\n拉普拉斯近似的方差是这个量的倒数：\n$$ \\sigma_L^2 = [J(\\lambda_{\\text{MAP}})]^{-1} = \\frac{\\alpha-1}{\\beta^2} $$\n因此，对后验的拉普拉斯近似为 $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$，其中均值为 $\\mu_L = \\lambda_{\\text{MAP}} = \\frac{\\alpha-1}{\\beta}$，方差为 $\\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$。\n\n第四，我们计算期望平方损失 $L(\\lambda; t) = (\\lambda - t)^2$。在 $\\lambda$ 的某个概率分布下，期望损失由 $E[(\\lambda-t)^2]$ 给出。这可以使用方差的定义 $\\text{Var}(\\lambda) = E[\\lambda^2] - (E[\\lambda])^2$ 来展开：\n$$ E[(\\lambda-t)^2] = E[\\lambda^2 - 2t\\lambda + t^2] = E[\\lambda^2] - 2tE[\\lambda] + t^2 $$\n$$ E[(\\lambda-t)^2] = (\\text{Var}(\\lambda) + (E[\\lambda])^2) - 2tE[\\lambda] + t^2 = \\text{Var}(\\lambda) + (E[\\lambda] - t)^2 $$\n这个公式将期望损失与方差以及分布均值相对于目标 $t$ 的偏差平方联系起来。\n\n我们将此公式应用于精确后验和拉普拉斯近似。\n- 对于精确后验，$\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$：\n  - 均值：$E_{\\text{post}}[\\lambda] = \\frac{\\alpha}{\\beta}$\n  - 方差：$\\text{Var}_{\\text{post}}(\\lambda) = \\frac{\\alpha}{\\beta^2}$\n  - 期望损失：$E_{\\text{exact}} = \\text{Var}_{\\text{post}}(\\lambda) + (E_{\\text{post}}[\\lambda] - t)^2 = \\frac{\\alpha}{\\beta^2} + \\left(\\frac{\\alpha}{\\beta} - t\\right)^2$。\n- 对于拉普拉斯近似，$\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$：\n  - 均值：$E_{\\text{Laplace}}[\\lambda] = \\mu_L = \\frac{\\alpha-1}{\\beta}$\n  - 方差：$\\text{Var}_{\\text{Laplace}}(\\lambda) = \\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$\n  - 期望损失：$E_{\\text{Laplace}} = \\text{Var}_{\\text{Laplace}}(\\lambda) + (E_{\\text{Laplace}}[\\lambda] - t)^2 = \\frac{\\alpha-1}{\\beta^2} + \\left(\\frac{\\alpha-1}{\\beta} - t\\right)^2$。\n\n程序将实现这些最终公式，为每个测试用例计算绝对差 $|E_{\\text{exact}} - E_{\\text{Laplace}}|$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by calculating the absolute difference\n    between the expected squared loss under the exact posterior and under the Laplace approximation.\n    \"\"\"\n    \n    # Test suite definition: (prior_a, prior_b, data_y, t_config)\n    # t_config is a tuple (type, value) where type is 'mode', 'mean', or 'value'.\n    test_cases = [\n        (2.5, 1.0, [0, 1, 2], ('mode', None)),\n        (0.3, 1.0, [1], ('mean', None)),\n        (2.0, 1.0, [10, 12, 9, 11, 8], ('value', 10.0)),\n        (3.0, 0.5, [0, 0, 0, 1], ('value', 0.8)),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a_prior, b_prior, y, t_config = case\n        \n        # Convert y to a numpy array for easier calculations\n        y = np.array(y)\n        \n        # Calculate sufficient statistics from data\n        n = len(y)\n        sum_y = np.sum(y)\n        \n        # Calculate posterior parameters\n        # Posterior is Gamma(alpha, beta)\n        alpha_post = sum_y + a_prior\n        beta_post = float(n + b_prior)\n\n        # Ensure posterior mode is well-defined (alpha > 1)\n        if alpha_post = 1:\n            # This case is not expected based on problem validation\n            # but good practice to handle.\n            results.append(np.nan)\n            continue\n\n        # Determine the loss target t based on the configuration\n        t_type, t_val = t_config\n        t = 0.0\n        if t_type == 'value':\n            t = t_val\n        elif t_type == 'mode':\n            # Posterior mode (MAP)\n            t = (alpha_post - 1) / beta_post\n        elif t_type == 'mean':\n            # Sample mean of the data\n            t = np.mean(y)\n\n        # === Calculations for the exact posterior: Gamma(alpha_post, beta_post) ===\n        \n        # Mean of the exact posterior\n        mean_exact = alpha_post / beta_post\n        # Variance of the exact posterior\n        var_exact = alpha_post / (beta_post**2)\n        # Expected squared loss for the exact posterior\n        expected_loss_exact = var_exact + (mean_exact - t)**2\n\n        # === Calculations for the Laplace approximation: Normal(mu_L, sigma_L^2) ===\n        \n        # Mean of the Laplace approximation is the posterior mode\n        mean_laplace = (alpha_post - 1) / beta_post\n        # Variance of the Laplace approximation\n        var_laplace = (alpha_post - 1) / (beta_post**2)\n        # Expected squared loss for the Laplace approximation\n        expected_loss_laplace = var_laplace + (mean_laplace - t)**2\n\n        # Calculate the absolute difference between the two expected losses\n        abs_diff = abs(expected_loss_exact - expected_loss_laplace)\n        results.append(abs_diff)\n    \n    # Format the output as a comma-separated list of strings with 6 decimal places\n    output_str = \",\".join([f\"{res:.6f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "认识到局限性是改进的第一步。在上一个练习的基础上，我们将探索一种强大的技术来克服后验分布偏斜带来的问题。这里的核心思想是“重参数化”：通过对参数进行非线性变换（如此处的对数变换），我们常常可以使后验分布在新尺度上变得更对称、更接近高斯形态。这个动手实践将指导你实施并验证这种修正策略，展示一个简单的变量替换如何能显著提高拉普拉斯近似对后验方差等关键量的估计精度。",
            "id": "3137173",
            "problem": "要求您研究拉普拉斯近似在有偏的贝叶斯后验分布下的行为，并实现一个基于对数凹重参数化的简单校正。考虑以下单参数贝叶斯模型：设 $y_1, \\dots, y_n$ 为独立观测值，其中 $y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$，并使用共轭先验 $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$，其形状参数为 $\\alpha$，率参数为 $\\beta$（因此先验密度对于 $\\lambda  0$ 与 $\\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$成正比）。记 $S = \\sum_{i=1}^n y_i$。后验分布为 $\\lambda \\mid y_{1:n} \\sim \\mathrm{Gamma}(\\alpha + S, \\beta + n)$。\n\n您的任务是：\n- 从泊松似然和伽马先验的定义出发，确定后验分布及其方差（用 $\\alpha$、$\\beta$ 和 $S$ 表示）。\n- 在原始参数 $\\lambda$ 上执行标准的拉普拉斯近似，方法是将对数后验在其内部众数处展开至二阶（假设测试用例满足 $\\alpha + S  1$，因此众数是内部的）。利用曲率生成一个高斯近似，并从中读出 $\\lambda$ 尺度上的近似方差。\n- 提出并实现一个对数凹倾斜校正，方法如下：使用 $\\phi = \\log \\lambda$ 进行重参数化。在 $\\phi$ 尺度上，在 $\\phi$ 的对数后验众数处执行二阶拉普拉斯近似。然后，为了校正反变换的不对称性，在 $\\phi$ 上应用一个常数平移，使得原始 $\\lambda$ 尺度上的近似均值与精确的后验均值相匹配。最后，映射回 $\\lambda$ 尺度并提取隐含的近似方差。\n- 对于下方的每个测试用例，计算精确的后验方差、标准的拉普拉斯方差（在 $\\lambda$ 上）和对数凹倾斜方差（通过上述基于 $\\phi$ 的过程获得）。对于每个用例，输出一个布尔值，表示对数凹倾斜方差相对于精确方差的绝对误差是否严格小于标准拉普拉斯方差的绝对误差。\n\n使用以下测试套件。每个用例由 $(\\alpha, \\beta, n, S)$ 指定：\n- 用例 1（严重倾斜，内部众数几乎不存在）：$(1.1, 2.0, 1, 0)$。\n- 用例 2（中度倾斜）：$(0.6, 0.4, 3, 1)$。\n- 用例 3（近似对称）：$(3.0, 1.0, 2, 5)$。\n- 用例 4（边缘靠近边界但为内部众数）：$(1.01, 1.0, 5, 0)$。\n\n您的程序应生成单行输出，其中包含一个对应于上述用例顺序的四个布尔值的列表。每个布尔值为真当且仅当对数凹倾斜方差比标准拉普拉斯方差更接近精确的后验方差，否则为假。输出必须是形如“[b1,b2,b3,b4]”的单行精确格式。此问题不涉及物理单位或角度，所有数值答案均为不带百分比符号的实数。",
            "solution": "用户的问题要求比较两种不同的拉普拉斯近似方法计算贝叶斯后验分布方差的精确度，并与真实值进行对比。该模型使用泊松似然和伽马先验，从而得到伽马后验。\n\n让我们首先对问题陈述进行形式化验证。\n\n### 步骤 1：提取给定信息\n- **模型似然**：$y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$，其中 $i=1, \\dots, n$。给定 $\\lambda$ 时，观测值 $y_i$ 是独立的。\n- **先验分布**：$\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$，其密度为 $p(\\lambda) \\propto \\lambda^{\\alpha-1} \\exp(-\\beta \\lambda)$，其中 $\\lambda  0$。\n- **数据摘要**：$S = \\sum_{i=1}^n y_i$。\n- **后验分布**：问题陈述断言 $\\lambda \\mid y_{1:n} \\sim \\mathrm{Gamma}(\\alpha + S, \\beta + n)$。\n- **约束**：所有测试用例都将满足 $\\alpha + S  1$，确保后验众数位于参数空间 $(\\lambda  0)$ 的内部。\n- **任务 1**：推导精确的后验方差。\n- **任务 2**：在参数 $\\lambda$ 上实现标准的拉普拉斯近似，以找到一个近似方差。\n- **任务 3**：通过使用 $\\phi = \\log \\lambda$ 进行重参数化，实现一种“对数凹倾斜校正”。在 $\\phi$ 尺度上执行拉普拉斯近似，调整该近似以匹配精确的后验均值，然后映射回去以找到 $\\lambda$ 的隐含方差。\n- **任务 4**：对于每个测试用例，比较两种近似方法相对于精确方差的绝对误差。如果倾斜校正方法更准确，则返回布尔值`True`。\n- **测试用例**：为四个用例给出了参数 $(\\alpha, \\beta, n, S)$：\n    1. $(1.1, 2.0, 1, 0)$\n    2. $(0.6, 0.4, 3, 1)$\n    3. $(3.0, 1.0, 2, 5)$\n    4. $(1.01, 1.0, 5, 0)$\n\n### 步骤 2：使用提取的给定信息进行验证\n- **科学上合理**：该问题植根于标准的贝叶斯统计学。泊松-伽马模型是共轭先验的一个典型例子。拉普拉斯近似是渐近统计学和贝叶斯计算中的一种基本技术。所描述的“对数凹倾斜校正”是一种数学上合理的、用于改进近似的启发式过程。该问题在科学和数学上是有效的。\n- **适定性**：问题已完全指定。模型和先验所需的所有参数都在测试用例中提供。目标（比较近似误差）已用精确的布尔标准明确定义。条件 $\\alpha+S1$ 保证了后验众数位于严格为正的值上，这是在众数处进行泰勒展开所必需的。\n- **客观性**：问题以客观的数学语言表述，没有歧义或主观性陈述。\n\n### 步骤 3：结论和行动\n该问题是有效的。这是一个关于贝叶斯计算方法的明确定义的练习。我现在将着手解决。\n\n### 推导与求解\n\n设后验形状和率参数分别为 $\\alpha' = \\alpha + S$ 和 $\\beta' = \\beta + n$。问题陈述正确地指出后验分布为 $\\lambda \\mid y_{1:n} \\sim \\mathrm{Gamma}(\\alpha', \\beta')$。我们验证如下：\n似然为 $p(y_{1:n}|\\lambda) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!} \\propto \\lambda^{\\sum y_i} e^{-n\\lambda} = \\lambda^S e^{-n\\lambda}$。\n先验为 $p(\\lambda) \\propto \\lambda^{\\alpha-1} e^{-\\beta\\lambda}$。\n根据贝葉斯定理，后验与似然和先验的乘积成正比：\n$$p(\\lambda|y_{1:n}) \\propto p(y_{1:n}|\\lambda) p(\\lambda) \\propto (\\lambda^S e^{-n\\lambda}) (\\lambda^{\\alpha-1} e^{-\\beta\\lambda}) = \\lambda^{\\alpha+S-1} e^{-(\\beta+n)\\lambda}$$\n$$p(\\lambda|y_{1:n}) \\propto \\lambda^{\\alpha'-1} e^{-\\beta'\\lambda}$$\n这是一个伽马分布的核，所以后验确实是 $\\mathrm{Gamma}(\\alpha', \\beta')$。\n\n**1. 精确的后验方差**\n对于随机变量 $X \\sim \\mathrm{Gamma}(k, \\theta)$，其形状参数为 $k$，尺度参数为 $\\theta$，方差为 $V[X] = k\\theta^2$。用形状参数 $\\alpha'$ 和率参数 $\\beta'$ 表示，其中 $\\theta = 1/\\beta'$，方差为 $V[X] = \\alpha'/(\\beta')^2$。\n因此，$\\lambda$ 的后验分布的精确方差为：\n$$V_{\\text{exact}}[\\lambda] = \\frac{\\alpha'}{(\\beta')^2} = \\frac{\\alpha+S}{(\\beta+n)^2}$$\n\n**2. 在 $\\lambda$ 上的标准拉普拉斯近似**\n拉普拉斯近似用一个以以后验众数为中心的高斯分布来近似后验密度。这个高斯分布的方差由在众数处评估的对数后验二阶导数的负倒数给出。\n对数后验为 $L(\\lambda) = \\log p(\\lambda|y_{1:n}) = (\\alpha'-1)\\log\\lambda - \\beta'\\lambda + C$，其中 $C$ 是一个常数。\n\n首先，我们通过将一阶导数设为零来找到众数 $\\hat{\\lambda}$：\n$$L'(\\lambda) = \\frac{d}{d\\lambda}L(\\lambda) = \\frac{\\alpha'-1}{\\lambda} - \\beta' = 0 \\implies \\hat{\\lambda} = \\frac{\\alpha'-1}{\\beta'}$$\n条件 $\\alpha' = \\alpha+S  1$ 确保了 $\\hat{\\lambda}  0$。\n\n接下来，我们求二阶导数：\n$$L''(\\lambda) = \\frac{d^2}{d\\lambda^2} L(\\lambda) = -\\frac{\\alpha'-1}{\\lambda^2}$$\n拉普拉斯近似的方差 $V_{\\text{std-laplace}}$ 是 $-1/L''(\\hat{\\lambda})$：\n$$V_{\\text{std-laplace}} = \\frac{-1}{-\\frac{\\alpha'-1}{\\hat{\\lambda}^2}} = \\frac{\\hat{\\lambda}^2}{\\alpha'-1} = \\frac{\\left(\\frac{\\alpha'-1}{\\beta'}\\right)^2}{\\alpha'-1} = \\frac{\\alpha'-1}{(\\beta')^2}$$\n代回 $\\alpha' = \\alpha+S$ 和 $\\beta' = \\beta+n$：\n$$V_{\\text{std-laplace}} = \\frac{\\alpha+S-1}{(\\beta+n)^2}$$\n\n**3. 在 $\\phi = \\log\\lambda$ 上的对数凹倾斜校正**\n此方法涉及重参数化至 $\\phi = \\log\\lambda$，即 $\\lambda = e^\\phi$。目标是通过在一个后验分布更对称的尺度上进行操作来找到更好的近似。\n\n首先，我们求 $\\phi$ 的后验密度。使用变量替换公式，$p_\\phi(\\phi) = p_\\lambda(e^\\phi) |\\frac{d\\lambda}{d\\phi}|$。由于 $\\lambda=e^\\phi$，我们有 $|\\frac{d\\lambda}{d\\phi}| = e^\\phi$。\n$$p_\\phi(\\phi) \\propto (e^\\phi)^{\\alpha'-1} e^{-\\beta'e^\\phi} \\cdot e^\\phi = (e^\\phi)^{\\alpha'} e^{-\\beta'e^\\phi}$$\n$\\phi$ 的对数后验是 $H(\\phi) = \\alpha'\\phi - \\beta'e^\\phi + C'$。此函数是全局凹的，因为其二阶导数 $H''(\\phi) = -\\beta'e^\\phi  0$对所有$\\phi$成立。\n\n我们求 $H(\\phi)$ 的众数 $\\hat{\\phi}$：\n$$H'(\\phi) = \\alpha' - \\beta'e^\\phi = 0 \\implies e^{\\hat{\\phi}} = \\frac{\\alpha'}{\\beta'} \\implies \\hat{\\phi} = \\log\\left(\\frac{\\alpha'}{\\beta'}\\right)$$\n在众数处的二阶导数是 $H''(\\hat{\\phi}) = -\\beta'e^{\\hat{\\phi}} = -\\beta'\\left(\\frac{\\alpha'}{\\beta'}\\right) = -\\alpha'$。\n$\\phi$ 的拉普拉斯近似是高斯分布 $N(\\hat{\\phi}, \\sigma^2_\\phi)$，其中方差 $\\sigma^2_\\phi = [-H''(\\hat{\\phi})]^{-1} = 1/\\alpha'$。\n\n问题现在指定了一个“倾斜校正”。我们调整 $\\phi$ 的高斯近似的均值，使得得到的 $\\lambda$ 的对数正态近似的均值与 $\\lambda$ 的精确后验均值相匹配。设调整后的近似为 $\\phi' \\sim N(\\mu', \\sigma^2_\\phi)$，其中 $\\mu'$ 是调整后的均值。\n精确的后验均值为 $E[\\lambda] = \\alpha'/\\beta'$。\n$\\lambda = e^{\\phi'}$ 的均值是对数正态分布的均值，即 $E[\\lambda] = \\exp(\\mu' + \\sigma^2_\\phi/2)$。\n我们令二者相等：\n$$E[\\lambda] = \\frac{\\alpha'}{\\beta'} = \\exp\\left(\\mu' + \\frac{1}{2\\alpha'}\\right)$$\n两边取对数：\n$$\\log\\left(\\frac{\\alpha'}{\\beta'}\\right) = \\mu' + \\frac{1}{2\\alpha'}$$\n由于 $\\hat{\\phi} = \\log(\\alpha'/\\beta')$，我们解出调整后的均值 $\\mu'$：\n$$\\mu' = \\hat{\\phi} - \\frac{1}{2\\alpha'}$$\n因此，我们对 $\\phi$ 的校正近似是 $\\phi' \\sim N\\left(\\hat{\\phi} - \\frac{1}{2\\alpha'}, \\frac{1}{\\alpha'}\\right)$。\n\n最后，我们求在此校正近似下 $\\lambda$ 的隐含方差。对于 $\\lambda=e^{\\phi'}$，其中 $\\phi' \\sim N(\\mu', \\sigma'^2)$，对数正态分布的方差为 $V[\\lambda] = (e^{\\sigma'^2} - 1)e^{2\\mu'+\\sigma'^2}$。\n这里，$\\mu' = \\hat{\\phi} - 1/(2\\alpha')$ 且 $\\sigma'^2 = \\sigma^2_\\phi = 1/\\alpha'$。\n项 $e^{2\\mu'+\\sigma'^2}$ 可简化为 $(E[\\lambda])^2$：\n$$e^{2\\mu'+\\sigma'^2} = e^{2(\\hat{\\phi} - 1/(2\\alpha')) + 1/\\alpha'} = e^{2\\hat{\\phi}} = (e^{\\hat{\\phi}})^2 = \\left(\\frac{\\alpha'}{\\beta'}\\right)^2$$\n因此，倾斜校正后的方差是：\n$$V_{\\text{tilt}} = \\left(e^{1/\\alpha'} - 1\\right) \\left(\\frac{\\alpha'}{\\beta'}\\right)^2 = \\left(\\exp\\left(\\frac{1}{\\alpha+S}\\right) - 1\\right) \\left(\\frac{\\alpha+S}{\\beta+n}\\right)^2$$\n\n**用于比较的公式摘要**\n对于每个测试用例 $(\\alpha, \\beta, n, S)$：\n1.  定义 $\\alpha' = \\alpha+S$ 和 $\\beta' = \\beta+n$。\n2.  精确方差：$V_{\\text{exact}} = \\frac{\\alpha'}{(\\beta')^2}$。\n3.  标准拉普拉斯方差：$V_{\\text{std-laplace}} = \\frac{\\alpha'-1}{(\\beta')^2}$。\n4.  倾斜校正的对数拉普拉斯方差：$V_{\\text{tilt}} = \\left(e^{1/\\alpha'} - 1\\right) \\left(\\frac{\\alpha'}{\\beta'}\\right)^2$。\n5.  比较基于 $|V_{\\text{tilt}} - V_{\\text{exact}}|  |V_{\\text{std-laplace}} - V_{\\text{exact}}|$ 是否成立。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing two Laplace approximation methods for the\n    variance of a Gamma posterior distribution.\n    \"\"\"\n\n    # Test cases are specified by (alpha, beta, n, S).\n    test_cases = [\n        (1.1, 2.0, 1, 0),    # Case 1: Dramatically skewed\n        (0.6, 0.4, 3, 1),    # Case 2: Moderately skewed\n        (3.0, 1.0, 2, 5),    # Case 3: Nearly symmetric\n        (1.01, 1.0, 5, 0),   # Case 4: Edge near boundary\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta, n, S = case\n\n        # Posterior parameters for Gamma(alpha', beta')\n        alpha_prime = alpha + S\n        beta_prime = beta + n\n\n        # Ensure the mode is interior, as per the problem statement\n        if alpha_prime = 1:\n            # This path should not be taken with the given test cases.\n            # It would invalidate the standard Laplace approximation formula.\n            raise ValueError(\"alpha + S must be > 1 for an interior mode.\")\n\n        # 1. Exact posterior variance\n        # For a Gamma(k, theta) distribution (shape k, rate theta),\n        # the variance is k / theta^2.\n        v_exact = alpha_prime / (beta_prime ** 2)\n\n        # 2. Standard Laplace approximation variance\n        # This is derived from the curvature of the log-posterior at the mode\n        # on the original lambda scale.\n        # Formula: (alpha' - 1) / beta'^2\n        v_std_laplace = (alpha_prime - 1) / (beta_prime ** 2)\n\n        # 3. Log-concave tilt corrected variance\n        # This is derived from a Laplace approximation on the log-lambda scale,\n        # with a correction to match the exact posterior mean. The result is\n        # the variance of a log-normal distribution.\n        # Formula: (exp(1/alpha') - 1) * (alpha' / beta')^2\n        term1 = np.exp(1.0 / alpha_prime) - 1.0\n        term2 = (alpha_prime / beta_prime) ** 2\n        v_tilt = term1 * term2\n\n        # 4. Compare absolute errors\n        err_std = abs(v_std_laplace - v_exact)\n        err_tilt = abs(v_tilt - v_exact)\n\n        # The result is True if the tilt-corrected method has a strictly\n        # smaller absolute error.\n        is_tilt_better = err_tilt  err_std\n        results.append(is_tilt_better)\n\n    # Format the output as a list of booleans string.\n    # The boolean values must be lowercase for the final print format.\n    formatted_results = [str(r).lower() for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}