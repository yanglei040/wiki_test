{
    "hands_on_practices": [
        {
            "introduction": "The definition of a universal hash family provides a powerful guarantee, but how can we be sure a given family of functions meets this standard? This exercise takes a hands-on analytical approach, asking you to test a plausible-looking hash family against the strict definition of universality. By exploring the connection between hashing, modular arithmetic, and number theory, you will discover for yourself why this particular family fails and, in doing so, gain a much deeper intuition for the properties that are essential for good hash function design .",
            "id": "3281273",
            "problem": "You are given a hash table of size $m$ and a universe of keys $U = \\{0, 1, 2, \\dots, m-1\\}$. Consider the family $\\mathcal{H}$ of hash functions parameterized by a multiplier $a$ chosen uniformly at random from $\\{0, 1, 2, \\dots, m-1\\}$, where for each $a \\in \\{0, 1, 2, \\dots, m-1\\}$ the hash function $h_a : U \\to \\{0, 1, \\dots, m-1\\}$ is defined by\n$$\nh_a(x) \\triangleq (a \\cdot x) \\bmod m.\n$$\nA hashing family is called universal if for every distinct $x, y \\in U$, the collision probability $\\Pr_{h \\sim \\mathcal{H}}[h(x) = h(y)]$ is at most $1/m$. Your task is to show that the above family $\\mathcal{H}$ is not universal by explicitly finding distinct keys $x \\neq y$ for which the collision probability exceeds $1/m$, and by giving a systematic approach for finding such pairs for a general $m$.\n\nUsing only the core definitions of hashing families, collisions, and basic properties of modular arithmetic and linear congruences as your starting point, do the following for the specific table size $m = 36$:\n\n- Derive, from first principles, an expression for the collision probability $\\Pr_{a}[h_a(x) = h_a(y)]$ for arbitrary distinct $x, y \\in U$ in terms of $m$ and the difference $d \\triangleq (x - y) \\bmod m$.\n- From that expression, devise a systematic search method that, given $m$, produces a pair of distinct keys $x \\neq y$ with collision probability strictly greater than $1/m$. Describe your method in terms of structural properties of $m$.\n- Apply your method to $m = 36$ to select concrete $x, y \\in U$ that maximize the collision probability in this family.\n- Compute the exact collision probability for your chosen $x, y$. Express your final answer as a single reduced fraction. Do not round and do not include any units.\n\nYour final answer must be only that single reduced fraction.",
            "solution": "The problem statement is a valid mathematical problem in the field of algorithm analysis and is well-posed. We shall proceed with a complete solution.\n\nThe problem asks us to show that the hash family $\\mathcal{H} = \\{h_a | a \\in \\{0, 1, \\dots, m-1\\}\\}$, with $h_a(x) \\triangleq (a \\cdot x) \\bmod m$, is not a universal hashing family. A family of hash functions is defined as universal if for any pair of distinct keys $x, y$ from the universe $U$, the probability of a collision is no more than $1/m$, i.e., $\\Pr_{h \\in \\mathcal{H}}[h(x) = h(y)] \\le 1/m$. In our case, the random choice of hash function $h$ corresponds to a uniform random choice of the parameter $a$ from the set $\\{0, 1, \\dots, m-1\\}$.\n\nFirst, we derive a general expression for the collision probability for two distinct keys $x, y \\in U = \\{0, 1, \\dots, m-1\\}$. A collision occurs if and only if $h_a(x) = h_a(y)$.\n$$\n(a \\cdot x) \\bmod m = (a \\cdot y) \\bmod m\n$$\nThis is equivalent to the statement that $m$ divides the difference $(a \\cdot x) - (a \\cdot y)$. In the language of modular arithmetic, this is expressed as a linear congruence:\n$$\na \\cdot x - a \\cdot y \\equiv 0 \\pmod{m}\n$$\n$$\na \\cdot (x - y) \\equiv 0 \\pmod{m}\n$$\nLet $d \\triangleq (x - y) \\bmod m$. Since $x$ and $y$ are distinct and belong to $\\{0, 1, \\dots, m-1\\}$, the difference $x-y$ is a non-zero integer in the range $[-(m-1), m-1]$. Consequently, $d$ is a non-zero integer in the range $\\{1, 2, \\dots, m-1\\}$. The congruence becomes:\n$$\na \\cdot d \\equiv 0 \\pmod{m}\n$$\nWe need to find the number of solutions for $a$ in the set $\\{0, 1, \\dots, m-1\\}$. This is a standard problem in elementary number theory. A linear congruence of the form $ax \\equiv b \\pmod{n}$ has solutions for $x$ if and only if $\\gcd(a, n)$ divides $b$. If it does, there are exactly $\\gcd(a, n)$ solutions modulo $n$. In our case, the congruence is $a \\cdot d \\equiv 0 \\pmod{m}$, and we are solving for $a$. The number of solutions for $a$ is given by $\\gcd(d, m)$.\n\nTo be rigorous, let $g = \\gcd(d, m)$. The congruence is $ad \\equiv 0 \\pmod m$. This is equivalent to saying that $m \\mid ad$. Dividing by $g$, we get $(m/g) \\mid a(d/g)$. Since $\\gcd(d/g, m/g) = 1$, by Euclid's lemma, we must have $(m/g) \\mid a$. Thus, $a$ must be a multiple of $m/g$. The possible values for $a$ in the set $\\{0, 1, \\dots, m-1\\}$ are:\n$$\n0 \\cdot \\frac{m}{g}, 1 \\cdot \\frac{m}{g}, 2 \\cdot \\frac{m}{g}, \\dots, (g-1) \\cdot \\frac{m}{g}\n$$\nThere are exactly $g = \\gcd(d, m)$ such values of $a$.\n\nThe parameter $a$ is chosen uniformly at random from a set of $m$ possibilities. The probability of a collision is therefore the ratio of the number of choices for $a$ that cause a collision to the total number of choices for $a$:\n$$\n\\Pr_{a}[h_a(x) = h_a(y)] = \\frac{\\text{Number of solutions for } a}{\\text{Total choices for } a} = \\frac{\\gcd(d, m)}{m} = \\frac{\\gcd((x-y) \\bmod m, m)}{m}\n$$\nThis is the derived expression for the collision probability.\n\nNext, we devise a systematic method to find a pair of keys $x, y$ for which the collision probability exceeds $1/m$. The universality condition requires $\\Pr \\le 1/m$, which translates to:\n$$\n\\frac{\\gcd(d, m)}{m} \\le \\frac{1}{m}\n$$\nThis simplifies to $\\gcd(d, m) \\le 1$. Since the greatest common divisor is always a positive integer, this means we must have $\\gcd(d, m) = 1$. For the family to be universal, this condition must hold for all possible values of $d = (x-y) \\bmod m$, which spans the set $\\{1, 2, \\dots, m-1\\}$. The condition that $\\gcd(d, m) = 1$ for all $d \\in \\{1, 2, \\dots, m-1\\}$ is, by definition, true if and only if $m$ is a prime number.\n\nIf $m$ is a composite number, there must exist at least one integer $k \\in \\{2, \\dots, m-1\\}$ that is a divisor of $m$. For such a $k$, we have $\\gcd(k, m) = k  1$.\nThis provides our systematic method:\n1. If $m$ is composite, find a proper divisor $k$ of $m$ (i.e., a divisor other than $1$ and $m$).\n2. Choose a pair of distinct keys $x, y \\in U$ such that $(x-y) \\bmod m = k$. A simple choice is $x=k$ and $y=0$. Since $k \\in \\{2, \\dots, m-1\\}$, both $x$ and $y$ are valid distinct keys in $U$.\n3. For this pair, the collision probability is $\\frac{\\gcd(k, m)}{m} = \\frac{k}{m}$, which is strictly greater than $\\frac{1}{m}$ since $k  1$. This proves the family is not universal for any composite $m$.\n\nTo maximize the collision probability, we need to maximize $\\gcd(d, m)$ for $d \\in \\{1, 2, \\dots, m-1\\}$. The maximum possible value for $\\gcd(d, m)$ where $d  m$ is the largest proper divisor of $m$. This is equal to $m/p$, where $p$ is the smallest prime factor of $m$. We should therefore choose $d$ to be this largest proper divisor.\n\nWe now apply this method to the specific case where $m=36$.\nThe number $m=36$ is composite. Its prime factorization is $2^2 \\cdot 3^2$. The set of proper divisors of $36$ is $\\{2, 3, 4, 6, 9, 12, 18\\}$.\nTo maximize the collision probability, we need to choose $d = (x-y) \\bmod 36$ such that $\\gcd(d, 36)$ is maximized. The maximum value of $\\gcd(d, 36)$ for $d \\in \\{1, \\dots, 35\\}$ is the largest proper divisor of $36$. The smallest prime factor of $36$ is $2$, so the largest proper divisor is $36/2 = 18$.\nThus, we should choose $d=18$.\n\nWe select a pair of keys $(x, y)$ such that $(x-y) \\bmod 36 = 18$. The simplest choice is $x=18$ and $y=0$. Both keys are distinct and belong to the universe $U = \\{0, 1, \\dots, 35\\}$.\n\nFinally, we compute the exact collision probability for this chosen pair $(18, 0)$ with $m=36$.\n$$\n\\Pr_{a}[h_a(18) = h_a(0)] = \\frac{\\gcd(18-0, 36)}{36} = \\frac{\\gcd(18, 36)}{36}\n$$\nThe greatest common divisor of $18$ and $36$ is $18$.\n$$\n\\Pr = \\frac{18}{36} = \\frac{1}{2}\n$$\nThis probability of $1/2$ is substantially greater than the universality threshold of $1/36$, which explicitly demonstrates that this family of hash functions is not universal. The final answer is this probability expressed as a reduced fraction.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "Now that we understand the criteria for a hash family to be universal, we can explore the powerful performance guarantees it provides. This practice challenges you to quantify the \"evenness\" of the key distribution by deriving the variance of the load in a hash bin when using a strongly 2-universal family. By leveraging fundamental tools like indicator random variables and the property of pairwise independence, you will analytically confirm the well-behaved nature of universal hashing, a key step in proving that no single bin is likely to become overly full .",
            "id": "3281240",
            "problem": "A hash table has $m$ bins, indexed by the set $\\{0,1,\\dots,m-1\\}$. Let $U$ be a universe of keys, and let $H$ be a strongly $2$-universal family of hash functions from $U$ to $\\{0,1,\\dots,m-1\\}$. By strongly $2$-universal, we mean that for any distinct $x,y \\in U$ and for any $a,b \\in \\{0,1,\\dots,m-1\\}$, if $h$ is sampled uniformly at random from $H$, then $\\Pr[h(x)=a]=\\frac{1}{m}$ and $\\Pr[h(x)=a \\wedge h(y)=b]=\\frac{1}{m^{2}}$.\n\nSuppose $n$ distinct keys $x_{1},x_{2},\\dots,x_{n} \\in U$ are inserted into the hash table using a function $h$ drawn uniformly at random from $H$. Fix a particular bin, say bin $0$, and let $L$ denote the load on bin $0$, that is, the number of inserted keys that hash to $0$.\n\nUsing only core definitions from probability (such as expectation, variance, indicator random variables, and pairwise independence), derive an exact closed-form expression for the variance $\\mathrm{Var}(L)$ in terms of $n$ and $m$. Your final answer must be a single analytic expression. Do not provide an inequality or an equation. No rounding is required.",
            "solution": "We start from the definitions of expectation and variance. Let $L$ be the number of inserted keys that hash to bin $0$. Define indicator random variables $X_{i}$ for $i \\in \\{1,2,\\dots,n\\}$ by\n$$\nX_{i} \\triangleq \\begin{cases}\n1  \\text{if } h(x_{i})=0,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\nThen\n$$\nL=\\sum_{i=1}^{n} X_{i}.\n$$\n\nWe first compute $\\mathbb{E}[X_{i}]$. By the uniformity property of the strongly $2$-universal family, for each fixed $x_{i}$ and bin $0$,\n$$\n\\Pr[X_{i}=1]=\\Pr[h(x_{i})=0]=\\frac{1}{m}.\n$$\nTherefore,\n$$\n\\mathbb{E}[X_{i}]=1 \\cdot \\frac{1}{m}+0 \\cdot \\left(1-\\frac{1}{m}\\right)=\\frac{1}{m}.\n$$\n\nNext, we compute $\\mathrm{Var}(X_{i})$. Since $X_{i}$ is a Bernoulli random variable with parameter $p=\\frac{1}{m}$,\n$$\n\\mathrm{Var}(X_{i})=\\mathbb{E}[X_{i}^{2}]-\\left(\\mathbb{E}[X_{i}]\\right)^{2}.\n$$\nBecause $X_{i} \\in \\{0,1\\}$, we have $X_{i}^{2}=X_{i}$, so $\\mathbb{E}[X_{i}^{2}]=\\mathbb{E}[X_{i}]=\\frac{1}{m}$. Hence\n$$\n\\mathrm{Var}(X_{i})=\\frac{1}{m}-\\left(\\frac{1}{m}\\right)^{2}=\\frac{1}{m}\\left(1-\\frac{1}{m}\\right).\n$$\n\nTo compute $\\mathrm{Var}(L)$, we use the definition\n$$\n\\mathrm{Var}(L)=\\mathrm{Var}\\!\\left(\\sum_{i=1}^{n} X_{i}\\right)=\\sum_{i=1}^{n} \\mathrm{Var}(X_{i})+2\\sum_{1 \\le ij \\le n} \\mathrm{Cov}(X_{i},X_{j}).\n$$\nWe show that the covariance terms are $0$. For $i \\ne j$, by strong $2$-universality, the pair $(h(x_{i}),h(x_{j}))$ is uniformly distributed over $\\{0,1,\\dots,m-1\\}^{2}$, which implies $h(x_{i})$ and $h(x_{j})$ are independent. Therefore the indicators $X_{i}$ and $X_{j}$ are also independent. Independence implies\n$$\n\\mathrm{Cov}(X_{i},X_{j})=\\mathbb{E}[X_{i}X_{j}]-\\mathbb{E}[X_{i}]\\,\\mathbb{E}[X_{j}]=0.\n$$\nConsequently,\n$$\n\\mathrm{Var}(L)=\\sum_{i=1}^{n} \\mathrm{Var}(X_{i})=n \\cdot \\frac{1}{m}\\left(1-\\frac{1}{m}\\right).\n$$\n\nThis is the exact variance of the load on the fixed bin under a strongly $2$-universal hash family.",
            "answer": "$$\\boxed{\\frac{n}{m}\\left(1-\\frac{1}{m}\\right)}$$"
        },
        {
            "introduction": "Theoretical guarantees are essential, but seeing their practical impact provides the most compelling lesson. This hands-on coding exercise moves from theory to implementation, tasking you with empirically comparing a simple deterministic hash function against a truly universal one. By simulating performance with both random and deliberately adversarial datasets, you will observe firsthand how randomization provides robust protection against worst-case scenarios that can cripple a deterministic hashing scheme, solidifying your understanding of why universal hashing is a cornerstone of modern algorithm design .",
            "id": "3281122",
            "problem": "You are to design and implement a complete, runnable program that empirically compares the distribution of worst-case search costs in chained hash tables when using a fixed deterministic hash function versus when using a hash function selected uniformly at random from a universal family. The program must compute empirical complementary cumulative distribution function (CCDF) values for the worst-case search cost and output these values for a specified test suite. Every mathematical entity in this specification is written using LaTeX notation to provide precise meaning.\n\nThe fundamental definitions and facts to be used as the base for this assignment are as follows:\n\n- A hash table with separate chaining stores a set of keys from a universe in an array of $m$ buckets, where each bucket holds a linked list of keys mapped to that bucket. For a key $x$ and hash function $h$, the bucket index is $h(x) \\in \\{0,1,\\dots,m-1\\}$.\n- The worst-case search cost under separate chaining is equal to the length of the longest chain. Let $L_{\\max}$ denote the maximum number of keys in any bucket. When searching for an arbitrary key under separate chaining, the worst-case time is proportional to $L_{\\max}$.\n- A family $\\mathcal{H}$ of hash functions from a universe $\\mathcal{U}$ to $\\{0,1,\\dots,m-1\\}$ is called universal if, for any distinct keys $x,y \\in \\mathcal{U}$, $$\\Pr_{h \\sim \\mathcal{H}}[h(x) = h(y)] \\le \\frac{1}{m}.$$ Universal hashing chooses $h$ uniformly at random from $\\mathcal{H}$ before accessing the keys.\n- Under the simple uniform hashing assumption (SUHA), for any fixed set of $n$ distinct keys and a randomly chosen hash function that maps each key independently and uniformly to $\\{0,1,\\dots,m-1\\}$, the occupancy $X_i$ of any fixed bucket $i$ is distributed as a binomial random variable with parameters $(n,p)$ where $p = \\frac{1}{m}$. In this model, standard tail bounds (for example, Chernoff bounds for sums of independent indicator random variables) imply that individual bucket occupancy tails decrease exponentially as the threshold deviates above the mean. Consequently, the maximum occupancy $L_{\\max}$ over $m$ buckets concentrates around its typical scale determined by $n$, $m$, and independence effects.\n\nYour program will empirically estimate the CCDF of $L_{\\max}$ at specified thresholds. For a given threshold $t$, the CCDF value is $$\\Pr[L_{\\max} \\ge t],$$ estimated as the fraction of trials for which the maximum chain length is at least $t$. To isolate the effects of determinism versus universality:\n\n- Deterministic hashing uses the fixed function $h(x) = x \\bmod m$ for all trials. Randomness arises only from randomly generated input sets of keys when applicable.\n- Universal hashing uses the family $$h_{a,b}(x) = \\big((a \\cdot x + b) \\bmod p\\big) \\bmod m,$$ where $p$ is a prime greater than the largest possible key, $a$ is chosen uniformly from $\\{1,2,\\dots,p-1\\}$, and $b$ is chosen uniformly from $\\{0,1,\\dots,p-1\\}$. A fresh pair $(a,b)$ is drawn independently for each trial.\n\nKey generation regimes:\n\n- Random keys: Generate $n$ distinct keys uniformly at random from $\\{0,1,\\dots,U-1\\}$, where $U$ is a specified upper bound on the key universe and $U \\ge n$.\n- Adversarial keys for the deterministic function $h(x) = x \\bmod m$: Use the specific set $$S = \\{r + k \\cdot m \\mid k \\in \\{0,1,\\dots,n-1\\}\\},$$ with $r = 0$, which forces all keys to collide into the same bucket under the deterministic hash function. Ensure $U \\ge r + (n-1) \\cdot m + 1$ so that all keys lie in $\\{0,1,\\dots,U-1\\}$.\n\nImplementation constraints:\n\n- The hash table uses separate chaining, and the worst-case search cost is measured as $L_{\\max}$, the maximum bucket occupancy after all $n$ keys are inserted.\n- The empirical CCDF at thresholds $\\{t_1,t_2,\\dots\\}$ is computed by running a specified number of independent trials, each trial hashing the full set of $n$ keys, forming bucket occupancies, and recording whether $L_{\\max} \\ge t_j$.\n- Express empirical probabilities as decimal numbers rounded to four decimal places.\n\nTest suite specification:\n\nRun the following cases to probe diverse behaviors including happy-path random inputs, adversarial behavior under determinism, and heavy load factors:\n\n1. Case A (deterministic, random keys): $m = 64$, $n = 64$, $U = 2^{20}$, number of trials $T = 2000$, thresholds $\\{3,5,7,9\\}$, deterministic hash $h(x) = x \\bmod m$, keys are $n$ distinct uniformly random elements of $\\{0,1,\\dots,U-1\\}$.\n\n2. Case B (universal, random keys): $m = 64$, $n = 64$, $U = 2^{20}$, prime $p = 10^9 + 7$, number of trials $T = 2000$, thresholds $\\{3,5,7,9\\}$, universal hash $h_{a,b}(x)$, keys are $n$ distinct uniformly random elements of $\\{0,1,\\dots,U-1\\}$, with fresh $(a,b)$ per trial.\n\n3. Case C (deterministic, adversarial keys): $m = 64$, $n = 64$, $U = m \\cdot n + 1$, number of trials $T = 100$, thresholds $\\{16,32,48,64\\}$, deterministic hash $h(x) = x \\bmod m$, keys $S = \\{0 + k \\cdot m \\mid k \\in \\{0,1,\\dots,n-1\\}\\}$ used identically in each trial.\n\n4. Case D (universal, random keys under heavy load): $m = 16$, $n = 256$, $U = 2^{20}$, prime $p = 10^9 + 7$, number of trials $T = 1000$, thresholds $\\{16,24,32,40,48\\}$, universal hash $h_{a,b}(x)$ with fresh $(a,b)$ per trial, keys are $n$ distinct uniformly random elements of $\\{0,1,\\dots,U-1\\}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list entries must be the concatenation of the empirically estimated CCDF values (rounded to four decimals) in the exact order:\n  - Case A thresholds in ascending order,\n  - followed by Case B thresholds in ascending order,\n  - followed by Case C thresholds in ascending order,\n  - followed by Case D thresholds in ascending order.\n- For example, the output should resemble $$[\\text{A}_1,\\text{A}_2,\\text{A}_3,\\text{A}_4,\\text{B}_1,\\dots,\\text{D}_5],$$ with no spaces and all entries expressed as decimal numbers rounded to four decimals.",
            "solution": "We explain the scientific and algorithmic basis for the comparison and the computation of empirical complementary cumulative distribution function (CCDF) values of the worst-case search cost in a hash table with chaining.\n\nFundamental base and definitions:\n\n- A hash table with chaining consists of $m$ buckets. Hashing maps each key $x$ to an index $h(x) \\in \\{0,1,\\dots,m-1\\}$. After inserting $n$ keys, the occupancy of bucket $i$ is $$X_i = \\left|\\{x \\text{ among the } n \\text{ keys} : h(x) = i\\}\\right|,$$ and the worst-case search cost (in units of comparisons) scales with $$L_{\\max} = \\max_{i \\in \\{0,1,\\dots,m-1\\}} X_i.$$\n\n- The load factor is defined as $$\\alpha = \\frac{n}{m}.$$ Under balanced hashing, a typical bucket has around $\\alpha$ keys.\n\n- Universal hashing chooses $h$ uniformly at random from a universal family $\\mathcal{H}$ that guarantees, for distinct keys $x \\ne y$, $$\\Pr_{h \\sim \\mathcal{H}}[h(x) = h(y)] \\le \\frac{1}{m}.$$ A classic construction is $$h_{a,b}(x) = \\big((a \\cdot x + b) \\bmod p\\big) \\bmod m,$$ with prime $p  \\max(\\mathcal{U})$, $a$ uniform over $\\{1,2,\\dots,p-1\\}$, and $b$ uniform over $\\{0,1,\\dots,p-1\\}$.\n\n- The simple uniform hashing assumption (SUHA) models hashing as mapping each key independently and uniformly to buckets, implying for a fixed set of $n$ distinct keys that $X_i \\sim \\text{Binomial}(n, p)$ with $p = \\frac{1}{m}$ for each bucket $i$.\n\nWhy universality helps:\n\n- Under SUHA, for bucket $i$ the mean occupancy is $$\\mu = \\mathbb{E}[X_i] = n \\cdot \\frac{1}{m} = \\alpha.$$ Tail bounds such as the Chernoff inequality for binomial random variables state that for any $\\delta  0$,\n  $$\\Pr[X_i \\ge (1+\\delta)\\mu] \\le \\left(\\frac{e^{\\delta}}{(1+\\delta)^{(1+\\delta)}}\\right)^{\\mu},$$\n  demonstrating exponentially decaying tails for exceeding the mean. Applying the union bound over $m$ buckets yields\n  $$\\Pr\\left[L_{\\max} \\ge t\\right] = \\Pr\\left[\\bigcup_{i=0}^{m-1}\\{X_i \\ge t\\}\\right] \\le m \\cdot \\Pr[X_0 \\ge t],$$\n  so the probability that any bucket exceeds a threshold $t$ can be bounded using the single-bucket tail. For regimes such as $n = m$, this suggests that $L_{\\max}$ concentrates around a slowly growing function of $n$ (for fully independent hashing, the typical scale is known to be $\\Theta\\!\\left(\\frac{\\log n}{\\log \\log n}\\right)$).\n\n- Deterministic hashing with a fixed function $h$ is vulnerable: if an adversary can choose keys after seeing $h$, it can make all keys collide into one bucket, making $L_{\\max} = n$. In contrast, universal hashing draws $h$ randomly before hashing, preventing such adversarial concentration when keys do not depend on $h$.\n\nEmpirical methodology:\n\n- We empirically estimate the CCDF of $L_{\\max}$ by running $T$ independent trials per case. In each trial, we:\n  1. Generate the set $S$ of $n$ distinct keys according to the caseâ€™s regime:\n     - Random regime: sample $S \\subseteq \\{0,1,\\dots,U-1\\}$ of size $n$ uniformly without replacement.\n     - Adversarial regime against $h(x) = x \\bmod m$: define $$S = \\{r + k \\cdot m \\mid k \\in \\{0,1,\\dots,n-1\\}\\},$$ with $r = 0$, ensuring that $h(x)$ maps every key in $S$ to the same bucket.\n  2. Choose the hash function:\n     - Deterministic: use $h(x) = x \\bmod m$.\n     - Universal: draw fresh $(a,b)$ for each trial and use $$h_{a,b}(x) = \\big((a \\cdot x + b) \\bmod p\\big) \\bmod m.$$\n  3. Compute occupancies $X_0,\\dots,X_{m-1}$ by hashing all $n$ keys in $S$ and counting bucket indices.\n  4. Compute $L_{\\max} = \\max_i X_i$ and record indicators $\\mathbf{1}\\{L_{\\max} \\ge t\\}$ for each threshold $t$ in the case.\n- The empirical CCDF at threshold $t$ is the mean of $\\mathbf{1}\\{L_{\\max} \\ge t\\}$ across trials, which estimates $\\Pr[L_{\\max} \\ge t]$ via the law of large numbers.\n\nAlgorithmic design and complexity:\n\n- For each trial, hashing $n$ keys and counting occupancies runs in $O(n + m)$ time using an array or vectorized counting. With $T$ trials, the per-case time is $O(T(n + m))$, practical for the specified sizes.\n- Universal hashing uses arithmetic modulo $p$ and introduces randomness through $a$ and $b$. Choosing $p$ as the prime $10^9 + 7$ ensures $p  U$ in all cases, making the mapping well-defined and avoiding modular biases.\n\nInterpretation of the test suite:\n\n- Case A ($m = 64$, $n = 64$) under deterministic hashing with random keys captures a benign scenario: because keys are uniformly random and $h(x) = x \\bmod m$, residues are close to uniform and the occupancy distribution of a fixed bucket is binomial with $p = \\frac{1}{m}$, resembling SUHA. We expect $L_{\\max}$ near typical values for $n=m$ with relatively small upper tails at thresholds like $t \\in \\{3,5,7,9\\}$.\n\n- Case B mirrors Case A but with universal hashing per trial. Because we draw $h$ uniformly at random, we expect very similar occupancy behavior to Case A under SUHA, and hence similar tails at the same thresholds. Minor differences arise from the precise independence structure and randomization source.\n\n- Case C is adversarial against determinism: all keys collide under $h(x) = x \\bmod m$, giving $L_{\\max} = n = 64$ deterministically for every trial, so the CCDF at threshold $t$ equals $1$ for all $t \\le n$ and $0$ above $n$. The thresholds $\\{16,32,48,64\\}$ demonstrate this staircase.\n\n- Case D tests heavy load ($\\alpha = \\frac{n}{m} = \\frac{256}{16} = 16$) under universal hashing. The mean bucket occupancy is $16$, so thresholds $\\{16,24,32,40,48\\}$ probe increasingly rare events in the tail. Under SUHA, CCDF values should decrease quickly as $t$ moves far above $\\alpha$.\n\nOutput specification:\n\n- The program prints a single line with a bracketed, comma-separated list of empirical CCDF values, in the exact concatenation order:\n  - Case A thresholds ascending,\n  - Case B thresholds ascending,\n  - Case C thresholds ascending,\n  - Case D thresholds ascending.\n- All probabilities are decimal values rounded to four decimal places, with no additional text or spaces.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hash_deterministic_x_mod_m(keys: np.ndarray, m: int) - np.ndarray:\n    \"\"\"Deterministic hash: h(x) = x mod m.\"\"\"\n    return keys % m\n\ndef hash_universal_axpb_mod_p_mod_m(keys: np.ndarray, m: int, p: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Universal hash: h_{a,b}(x) = ((a*x + b) mod p) mod m, with fresh (a,b) per call.\"\"\"\n    # Draw a in {1,...,p-1}, b in {0,...,p-1}\n    a = rng.integers(1, p, dtype=np.int64)\n    b = rng.integers(0, p, dtype=np.int64)\n    # Compute ((a*keys + b) % p) % m safely with int64\n    # Use modular arithmetic to avoid overflow: Python ints are arbitrary precision, but numpy int64 is fine here.\n    hashed = (np.mod(a * keys + b, p)) % m\n    return hashed\n\ndef empirical_ccdf_Lmax(m: int,\n                        n: int,\n                        U: int,\n                        thresholds: np.ndarray,\n                        trials: int,\n                        mode: str,\n                        p: int | None,\n                        rng: np.random.Generator) - list[float]:\n    \"\"\"\n    Estimate CCDF P[L_max = t] at each threshold t via trials.\n    mode:\n      - 'deterministic_random': fixed h(x)=x mod m, random distinct keys\n      - 'deterministic_adversarial': fixed h(x)=x mod m, adversarial keys colliding to one bucket\n      - 'universal_random': universal hashing per trial, random distinct keys\n    p: prime modulus for universal hashing when needed\n    \"\"\"\n    tail_counts = np.zeros(len(thresholds), dtype=np.int64)\n\n    # Precompute adversarial key set if needed\n    if mode == 'deterministic_adversarial':\n        # Keys S = {0 + k*m | k=0..n-1}\n        # Ensure within [0, U-1]; Problem guarantees U = m*n + 1\n        keys_fixed = (np.arange(n, dtype=np.int64) * m) % U\n\n    for _ in range(trials):\n        if mode == 'deterministic_random':\n            # Sample n distinct keys uniformly from [0, U-1]\n            keys = rng.choice(U, size=n, replace=False).astype(np.int64)\n            buckets = hash_deterministic_x_mod_m(keys, m)\n        elif mode == 'deterministic_adversarial':\n            # Use fixed adversarial keys every trial\n            buckets = hash_deterministic_x_mod_m(keys_fixed, m)\n        elif mode == 'universal_random':\n            keys = rng.choice(U, size=n, replace=False).astype(np.int64)\n            buckets = hash_universal_axpb_mod_p_mod_m(keys, m, p, rng)\n        else:\n            raise ValueError(\"Unknown mode\")\n\n        # Count occupancies in m buckets\n        counts = np.bincount(buckets, minlength=m)\n        Lmax = int(counts.max())\n\n        # Update tail counts for all thresholds\n        tail_counts += (Lmax = thresholds).astype(np.int64)\n\n    # Convert to probabilities\n    probs = tail_counts / float(trials)\n    # Round to 4 decimals\n    return [float(f\"{prob:.4f}\") for prob in probs]\n\ndef solve():\n    rng = np.random.default_rng(seed=42)  # Fixed seed for reproducibility\n\n    # Define prime p for universal hashing\n    p = 1_000_000_007\n\n    # Define test cases from the problem statement.\n    # Each case specifies (m, n, U, thresholds, trials, mode)\n    test_cases = [\n        # Case A: deterministic, random keys\n        {\n            \"m\": 64,\n            \"n\": 64,\n            \"U\": 2**20,\n            \"thresholds\": np.array([3, 5, 7, 9], dtype=np.int64),\n            \"trials\": 2000,\n            \"mode\": \"deterministic_random\",\n            \"p\": None,\n        },\n        # Case B: universal, random keys\n        {\n            \"m\": 64,\n            \"n\": 64,\n            \"U\": 2**20,\n            \"thresholds\": np.array([3, 5, 7, 9], dtype=np.int64),\n            \"trials\": 2000,\n            \"mode\": \"universal_random\",\n            \"p\": p,\n        },\n        # Case C: deterministic, adversarial keys\n        {\n            \"m\": 64,\n            \"n\": 64,\n            \"U\": 64*64 + 1,\n            \"thresholds\": np.array([16, 32, 48, 64], dtype=np.int64),\n            \"trials\": 100,\n            \"mode\": \"deterministic_adversarial\",\n            \"p\": None,\n        },\n        # Case D: universal, random keys under heavy load\n        {\n            \"m\": 16,\n            \"n\": 256,\n            \"U\": 2**20,\n            \"thresholds\": np.array([16, 24, 32, 40, 48], dtype=np.int64),\n            \"trials\": 1000,\n            \"mode\": \"universal_random\",\n            \"p\": p,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case[\"m\"]\n        n = case[\"n\"]\n        U = case[\"U\"]\n        thresholds = case[\"thresholds\"]\n        trials = case[\"trials\"]\n        mode = case[\"mode\"]\n        p_mod = case[\"p\"]\n        probs = empirical_ccdf_Lmax(m=m, n=n, U=U, thresholds=thresholds, trials=trials, mode=mode, p=p_mod, rng=rng)\n        results.extend(probs)\n\n    # Final print statement in the exact required format: single line, comma-separated list in brackets, no spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}