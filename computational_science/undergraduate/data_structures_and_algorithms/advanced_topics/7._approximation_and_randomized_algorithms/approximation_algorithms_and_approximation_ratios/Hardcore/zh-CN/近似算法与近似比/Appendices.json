{
    "hands_on_practices": [
        {
            "introduction": "并非所有直观的“贪心”策略都能有效地解决问题。在算法设计中，一项至关重要的技能是通过寻找一个“反例”——即算法会给出极差结果的特定输入——来检验算法的逻辑。这个练习将挑战你对一个看似合理但实则有缺陷的背包问题算法进行分析，通过构建一个具体的实例来揭示其最坏情况下的性能，从而让你深刻理解严格的最坏情况分析为何如此重要。",
            "id": "3207612",
            "problem": "考虑标准的 $0$-$1$ 背包问题：给定 $n$ 个物品，其中物品 $i$ 具有正价值 $v_i$ 和正重量 $w_i$，以及一个背包容量 $C0$，目标是选择一个子集 $S \\subseteq \\{1,\\dots,n\\}$，使得在满足约束 $\\sum_{i \\in S} w_i \\le C$ 的前提下，最大化 $\\sum_{i \\in S} v_i$。令物品 $i$ 的价值重量比为 $r_i = \\frac{v_i}{w_i}$。\n\n分析以下用于 $0$-$1$ 背包问题的“逆向贪心”算法：从包含所有物品的集合 $S$ 开始，并且每次迭代移除一个物品，总是移除 $S$ 中剩余物品里当前比率 $r_i$ 最小的物品，直到总重量 $\\sum_{i \\in S} w_i \\le C$。算法随后返回剩余的集合 $S$。\n\n使用 $0$-$1$ 背包问题、价值重量比和近似比的核心定义，推导该算法的最坏情况近似比。算法的近似比定义为 $\\inf_{I} \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$，其中 $\\mathrm{ALG}(I)$ 是算法在实例 $I$ 上产生的值，$\\mathrm{OPT}(I)$ 是实例 $I$ 的最优值。你的推导必须从这些定义开始，并构造一个科学合理的实例族来证明这个界。将你的最终答案表示为一个实数。不需要四舍五入。",
            "solution": "对问题陈述进行验证。\n\n### 步驟 1：提取已知条件\n- **问题类型**：$0$-$1$ 背包问题。\n- **输入**：一个包含 $n$ 个物品的集合，其中物品 $i \\in \\{1, \\dots, n\\}$ 具有正价值 $v_i  0$ 和正重量 $w_i  0$。背包容量 $C  0$。\n- **目标**：对于物品子集 $S$，在满足约束条件 $\\sum_{i \\in S} w_i \\le C$ 的前提下，最大化总价值 $\\sum_{i \\in S} v_i$。\n- **定义**：物品 $i$ 的价值重量比为 $r_i = \\frac{v_i}{w_i}$。\n- **待分析算法（“逆向贪心”）**：\n    1. 初始化解集 $S$，使其包含所有物品，$S = \\{1, \\dots, n\\}$。\n    2. 当 $S$ 中物品的总重量超过容量时，即 $\\sum_{i \\in S} w_i  C$：\n        a. 找出 $S$ 中当前所有物品里价值重量比最小的物品 $j$，即 $r_j = \\min_{k \\in S} \\{r_k\\}$。\n        b. 从 $S$ 中移除物品 $j$。\n    3. 算法返回最终的集合 $S$。\n- **评估指标**：最坏情况近似比，定义为 $\\inf_{I} \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$，其中 $I$ 代表问题的一个实例，$\\mathrm{ALG}(I)$ 是逆向贪心算法得到的值，$\\mathrm{OPT}(I)$ 是最优值。\n\n### 步驟 2：使用提取的已知条件进行验证\n- **科学依据**：该问题属于理论计算机科学这一成熟领域，特别是近似算法的分析。$0$-$1$ 背包问题是一个经典的 NP-hard 优化问题。所使用的所有概念——价值、重量、容量、近似比——都是标准的且有严格定义。提出的“逆向贪心”算法是一种合理的启发式算法，其性能可以进行数学分析。该问题是科学合理的。\n- **良定性**：问题陈述清晰。它明确了算法的逻辑、目标函数、约束条件，以及待推导量（最坏情况近似比）的精确定义。预期会有一个唯一的、有意义的数值答案。\n- **客观性**：问题使用精确、形式化的语言描述，没有主观或模糊的术语。\n- **结论**：该问题是自洽的、一致的且结构形式化的。它没有违反任何无效性标准。\n\n### 步驟 3：结论与行动\n该问题是**有效的**。将提供完整解答。\n\n### 解答推导\n令 $\\mathrm{ALG}(I)$为逆向贪心算法在给定问题实例 $I$ 上选择的物品的总价值，令 $\\mathrm{OPT}(I)$ 为该实例的最大可能价值（最优解）。该算法的近似比是比率 $\\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)}$ 在所有可能实例 $I$ 上的下确界。\n\n为了确定最坏情况近似比，我们试图构造一个使该比率最小化的实例族。逆向贪心算法的策略是通过首先移除低比率的物品来保留高价值重量比（$r_i = v_i/w_i$）的物品。该算法的一个失败案例可能涉及这样一种情况：一个比率较低的物品本身价值极高，并且是最优解的关键部分。而该算法根据其设计，可能会丢弃这个物品。\n\n考虑以下问题实例族，由一个小的正实数 $\\epsilon$ 参数化。设背包容量 $C$ 为任何大于 $1$ 的值。实例 $I_\\epsilon$ 由两个物品组成：\n\n- **物品 1**：$v_1 = 1$, $w_1 = C$。\n- **物品 2**：$v_2 = \\epsilon$, $w_2 = \\epsilon$。\n\n我们要求 $v_i  0$, $w_i  0$ 且 $C  0$。通过选择 $C  1$ 和 $0  \\epsilon  1$，这些条件得到满足。\n\n让我们分析逆向贪心算法的行为，并确定此实例 $I_\\epsilon$ 的最优解。\n\n**1. 算法的性能 ($\\mathrm{ALG}(I_\\epsilon)$):**\n首先，我们计算这两个物品的价值重量比：\n- $r_1 = \\frac{v_1}{w_1} = \\frac{1}{C}$\n- $r_2 = \\frac{v_2}{w_2} = \\frac{\\epsilon}{\\epsilon} = 1$\n\n由于我们选择了 $C  1$，因此有 $\\frac{1}{C}  1$，所以 $r_1  r_2$。\n\n算法从集合 $S = \\{1, 2\\}$ 开始。总重量为 $W_{total} = w_1 + w_2 = C + \\epsilon$。\n由于 $W_{total}  C$，算法必须移除一个物品。根据其定义，它移除比率最小的物品。因为 $r_1  r_2$，所以物品 1 被移除。\n\n得到的集合是 $S_{ALG} = \\{2\\}$。该集合的总重量为 $w_2 = \\epsilon$。由于我们选择了 $\\epsilon  1$ 且 $C  1$，我们有 $w_2  C$，因此重量约束得到满足，算法终止。\n算法得到的值是物品 2 的价值：\n$$ \\mathrm{ALG}(I_\\epsilon) = v_2 = \\epsilon $$\n\n**2. 最优解 ($\\mathrm{OPT}(I_\\epsilon)$):**\n我们现在通过考虑所有可行的物品子集来找到最优解。\n- **子集 {1}**：重量为 $w_1 = C$，满足容量约束 $w_1 \\le C$。价值为 $v_1 = 1$。这是一个可行解。\n- **子集 {2}**：重量为 $w_2 = \\epsilon$。由于 $\\epsilon  C$，这也是一个可行解。价值为 $v_2 = \\epsilon$。\n- **子集 {1, 2}**：重量为 $w_1 + w_2 = C + \\epsilon$，大于 $C$。该子集不可行。\n- **子集 $\\emptyset$**：价值为 $0$。\n\n最优值是所有可行子集中的最大价值：\n$$ \\mathrm{OPT}(I_\\epsilon) = \\max(\\{1, \\epsilon\\}) $$\n由于我们选择了 $0  \\epsilon  1$，最大值为 $1$。\n$$ \\mathrm{OPT}(I_\\epsilon) = 1 $$\n\n**3. 近似比计算：**\n对于实例 $I_\\epsilon$，比率为：\n$$ \\frac{\\mathrm{ALG}(I_\\epsilon)}{\\mathrm{OPT}(I_\\epsilon)} = \\frac{\\epsilon}{1} = \\epsilon $$\n\n我们正在寻找最坏情况近似比，即该比率在所有可能实例上的下确界。我们构造的实例族 $I_\\epsilon$ 表明，对于任何任意小的正数 $\\delta$，我们可以选择 $\\epsilon = \\delta$（前提是 $\\delta  1$）来创建一个近似比为 $\\delta$ 的实例。\n\n形式上，我们实例族的可能比率集合是 $\\{ \\epsilon \\mid 0  \\epsilon  1 \\}$。该集合的下确界是：\n$$ \\inf_{\\epsilon \\in (0,1)} \\{\\epsilon\\} = 0 $$\n由于任何实例的近似比都必须是非负的（因为所有价值 $v_i$ 都是正的），并且我们已经证明该比率可以任意接近于 $0$，所以该算法的最坏情况近似比是 $0$。\n$$ \\inf_I \\frac{\\mathrm{ALG}(I)}{\\mathrm{OPT}(I)} = 0 $$\n这表明不存在常数 $c  0$ 使得 $\\mathrm{ALG}(I) \\ge c \\cdot \\mathrm{OPT}(I)$ 对所有实例 $I$ 都成立。与最优解相比，该算法的表现可以任意差。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "在揭示了一个算法的缺陷之后，我们转向另一个核心任务：为一个优秀的算法的性能提供严格的证明。集合覆盖问题的贪心算法是一个经典的近似算法，其近似比是广为人知的。本练习将引导你分析一个精心设计的实例，在该实例上，贪心算法的性能恰好达到了其理论上的最坏情况边界，从而让你亲手验证该理论界限是“紧致的”，而非一个松散的估计。",
            "id": "3207647",
            "problem": "您将分析加权集合覆盖问题的贪心算法，以构建一个实例族，在该实例族上，其近似比在极限情况下等于第 $m$ 个调和数。请从基本定义开始。\n\n定义：\n- 在加权集合覆盖问题中，给定一个包含 $m$ 个元素的有限全集 $U$，一个 $U$ 的子集族 $\\mathcal{S}$，以及一个成本函数 $c : \\mathcal{S} \\to \\mathbb{R}_{0}$。目标是找到一个子集族 $\\mathcal{C} \\subseteq \\mathcal{S}$，它能覆盖 $U$（即 $\\bigcup_{S \\in \\mathcal{C}} S = U$），并且总成本 $\\sum_{S \\in \\mathcal{C}} c(S)$ 最小。\n- 贪心算法重复选择集合 $S \\in \\mathcal{S}$，该集合使得比率 $c(S) / |S \\setminus C|$ 最小，其中 $C$ 表示已覆盖元素的集合，并任意打破平局，直到 $U$ 的所有元素都被覆盖。\n- 一个实例上的近似比是该实例上贪心算法的总成本与最优总成本之比。\n- 第 $m$ 个调和数定义为 $H_m \\triangleq \\sum_{i=1}^{m} \\frac{1}{i}$。\n\n对于给定的整数 $m \\geq 2$ 和参数 $\\delta$（满足 $0  \\delta  1$），考虑以下实例 $(U,\\mathcal{S},c)$：\n- 全集 $U = \\{u_1, u_2, \\ldots, u_m\\}$。\n- 集合族 $\\mathcal{S}$ 由两类集合组成：\n  1. 对于每个 $i \\in \\{1,2,\\ldots,m\\}$，一个单元素集合 $T_i \\triangleq \\{u_i\\}$，其成本为 $c(T_i) \\triangleq \\frac{1}{i}$。\n  2. 对于每个 $j \\in \\{1,2,\\ldots,m\\}$，一个前缀集合 $P_j \\triangleq \\{u_1, u_2, \\ldots, u_j\\}$，其成本为 $c(P_j) \\triangleq 1 + \\delta$。\n\n任务：\n1. 根据上述定义，确定并证明贪心算法在该实例上选择的集合的精确序列，并计算其总成本，表示为 $m$ 和 $\\delta$ 的函数。\n2. 计算最优总成本，表示为 $m$ 和 $\\delta$ 的函数，并从第一性原理证明其最优性。\n3. 令 $R(m,\\delta)$ 表示该实例上贪心算法成本与最优成本之比。计算极限 $\\lim_{\\delta \\to 0^{+}} R(m,\\delta)$，并将您的最终答案表示为关于 $m$ 和 $H_m$ 的单个闭式解析表达式。\n\n您的最终答案必须是一个单一的閉式表达式。无需进行舍入。",
            "solution": "该问题提供了一个加权集合覆盖问题的具体实例，并要求分析贪心算法在该实例上的性能。验证过程确认了该问题是良定义的、数学上合理的，并且所有定义都是完整和一致的。我们按要求分三部分进行解答。\n\n第 1 部分：贪心算法分析\n\n加权集合覆盖的贪心算法迭代地选择集合 $S$，该集合使其成本与新覆盖元素数量之比最小。设 $C_{k-1}$ 为第 $k$ 次迭代前已覆盖的元素集合。算法选择 $S \\in \\mathcal{S}$ 以最小化值 $\\frac{c(S)}{|S \\setminus C_{k-1}|}$。\n\n让我们追踪算法在给定实例上的执行过程。\n设 $\\mathcal{C}_{greedy}$ 为算法选择的集合。\n\n**第 1 次迭代 ($k=1$)**：\n最初，已覆盖元素的集合为空，即 $C_0 = \\emptyset$。任何集合 $S$ 覆盖的新元素数量就是其基数 $|S|$。我们计算 $\\mathcal{S}$ 中每个集合的成本效益比。\n对于单元素集合 $T_i = \\{u_i\\}$（$i \\in \\{1, 2, \\ldots, m\\}$）：\n比率为 $\\frac{c(T_i)}{|T_i \\setminus C_0|} = \\frac{c(T_i)}{|T_i|} = \\frac{1/i}{1} = \\frac{1}{i}$。\n对于前缀集合 $P_j = \\{u_1, u_2, \\ldots, u_j\\}$（$j \\in \\{1, 2, \\ldots, m\\}$）：\n比率为 $\\frac{c(P_j)}{|P_j \\setminus C_0|} = \\frac{c(P_j)}{|P_j|} = \\frac{1+\\delta}{j}$。\n\n算法选择比率最小的集合。我们需要在所有可能比率的集合中找到最小值：$\\{\\frac{1}{1}, \\frac{1}{2}, \\ldots, \\frac{1}{m}\\} \\cup \\{\\frac{1+\\delta}{1}, \\frac{1+\\delta}{2}, \\ldots, \\frac{1+\\delta}{m}\\}$。\n比较有效大小相同的集合的比率，对于任何 $k \\in \\{1, \\ldots, m\\}$，$T_k$ 的比率是 $\\frac{1}{k}$，$P_k$ 的比率是 $\\frac{1+\\delta}{k}$。因为 $\\delta  0$，所以 $1  1+\\delta$，这意味着 $\\frac{1}{k}  \\frac{1+\\delta}{k}$。这对所有 $k$ 都成立。\n所有单元素集合中的最小比率是 $\\frac{1}{m}$（对于 $T_m$）。所有前缀集合中的最小比率是 $\\frac{1+\\delta}{m}$（对于 $P_m$）。因此，总的最小比率是 $\\frac{1}{m}$，它由集合 $S_1 = T_m$ 唯一实现。\n因此，第一个被选择的集合是 $T_m$。已覆盖元素的集合变为 $C_1 = \\{u_m\\}$。\n\n**选择序列的归纳證明**：\n我们断言贪心算法按序列 $T_m, T_{m-1}, \\ldots, T_1$ 选择单元素集合。我们用归纳法证明这一点。\n归纳基础：对于 $k=1$，我们已经证明算法会选择 $T_m$。这建立了我们对应于元素 $u_m$ 的归纳基础。\n归纳假设：假设在前 $k$ 步（$1 \\le k  m$）中，算法选择了集合 $T_m, T_{m-1}, \\ldots, T_{m-k+1}$。$k$ 步后已覆盖元素的集合是 $C_k = \\{u_m, u_{m-1}, \\ldots, u_{m-k+1}\\}$。未覆盖元素的集合是 $U \\setminus C_k = \\{u_1, u_2, \\ldots, u_{m-k}\\}$。\n\n归纳步骤（第 $k+1$ 次迭代）：\n我们现在确定要选择的集合。任何集合 $S$ 的成本效益比是 $\\frac{c(S)}{|S \\setminus C_k|}$。\n对于任何剩余的单元素集合 $T_i$（$i \\in \\{1, 2, \\ldots, m-k\\}$），元素 $u_i$ 是未覆盖的。因此， $|T_i \\setminus C_k| = 1$。比率为 $\\frac{c(T_i)}{1} = \\frac{1}{i}$。这些比率中的最小值是 $\\frac{1}{m-k}$，由 $T_{m-k}$ 实现。\n对于任何前缀集合 $P_j$（$j \\in \\{1, 2, \\ldots, m\\}$），它覆盖的新元素数量是 $|P_j \\setminus C_k| = |P_j \\cap (U \\setminus C_k)| = |\\{u_1, \\ldots, u_j\\} \\cap \\{u_1, \\ldots, u_{m-k}\\}|$。这个基数是 $\\min(j, m-k)$。\n$P_j$ 的比率为 $\\frac{c(P_j)}{|P_j \\setminus C_k|} = \\frac{1+\\delta}{\\min(j, m-k)}$。\n为了找到所有前缀集合中的最小比率，我们必须最大化分母 $\\min(j, m-k)$。$\\min(j, m-k)$ 的最大值是 $m-k$，这在 $j \\ge m-k$ 时可以达到。所以，任何前缀集合的最小比率是 $\\frac{1+\\delta}{m-k}$。\n现在我们比较来自单元素集合的最小比率 $\\frac{1}{m-k}$（来自 $T_{m-k}$）和来自前缀集合的最小比率 $\\frac{1+\\delta}{m-k}$。\n因为 $\\delta  0$，我们有 $1  1+\\delta$，因此 $\\frac{1}{m-k}  \\frac{1+\\delta}{m-k}$。\n唯一最小的比率是 $\\frac{1}{m-k}$，由集合 $T_{m-k}$ 实现。因此，在第 $k+1$ 步，算法选择集合 $S_{k+1}=T_{m-k}$。\n\n根据数学归纳法原理，贪心算法将按此顺序选择集合 $T_m, T_{m-1}, \\ldots, T_1$。这覆盖了 $U$ 的所有元素。\n贪心算法找到的覆盖的总成本 $\\text{Cost}_{greedy}$ 是这些集合的成本之和：\n$$ \\text{Cost}_{greedy} = c(T_m) + c(T_{m-1}) + \\cdots + c(T_1) = \\sum_{i=1}^{m} c(T_i) = \\sum_{i=1}^{m} \\frac{1}{i} $$\n根据定义，这个和是第 $m$ 个调和数 $H_m$。\n$$ \\text{Cost}_{greedy} = H_m $$\n\n第 2 部分：最优成本的计算\n\n我们必须找到一个有效的覆盖 $\\mathcal{C} \\subseteq \\mathcal{S}$，使其总成本 $\\text{Cost}_{opt}$ 尽可能小。\n最优覆盖有两个主要候选方案：\n1. 一个仅由单元素集合组成的覆盖。要覆盖整个全集 $U=\\{u_1, \\ldots, u_m\\}$，这个覆盖必须是 $\\mathcal{C}_1 = \\{T_1, T_2, \\ldots, T_m\\}$。其成本为 $\\text{Cost}(\\mathcal{C}_1) = \\sum_{i=1}^m c(T_i) = H_m$。\n2. 一个使用一个或多个前缀集合的覆盖。如果一个覆盖包含任何前缀集合 $P_j$，其成本至少为 $c(P_j)=1+\\delta$。这种类型中最有效的覆盖是单个集合 $P_m = \\{u_1, \\ldots, u_m\\}$，它本身就覆盖了整个 $U$。这个覆盖 $\\mathcal{C}_2 = \\{P_m\\}$ 的成本是 $\\text{Cost}(\\mathcal{C}_2) = c(P_m) = 1+\\delta$。任何其他包含前缀集合的覆盖的成本都至少为 $1+\\delta$，所以 $\\{P_m\\}$ 是使用前缀集合的覆盖中最好的。\n\n最优成本必须是所有可能覆盖的成本中的最小值。根据上述分析，任何覆盖要么是类型 1（只有单元素集合），要么包含至少一个前缀集合（导致成本至少为 $1+\\delta$）。因此，最优成本是 $\\mathcal{C}_1$ 和 $\\mathcal{C}_2$ 成本中的最小值：\n$$ \\text{Cost}_{opt}(m, \\delta) = \\min(H_m, 1+\\delta) $$\n问题要求在 $\\delta \\to 0^+$ 的极限情况下进行分析。对于 $m \\ge 2$，调和数 $H_m = 1 + \\frac{1}{2} + \\cdots + \\frac{1}{m} > 1$。因此，$H_m-1 > 0$。我们可以选择 $\\delta$ 使得 $0  \\delta  H_m-1$，这意味着 $1+\\delta  H_m$。由于极限考虑的是任意小的正 $\\delta$，这个条件将会成立。\n因此，为了本次分析的目的，最优解是覆盖 $\\{P_m\\}$，其成本为：\n$$ \\text{Cost}_{opt}(m, \\delta) = 1+\\delta $$\n\n第 3 部分：近似比极限的计算\n\n近似比 $R(m,\\delta)$ 是该实例上贪心算法成本与最优成本之比。\n$$ R(m,\\delta) = \\frac{\\text{Cost}_{greedy}}{\\text{Cost}_{opt}} = \\frac{H_m}{1+\\delta} $$\n我们被要求计算当 $\\delta$ 从正方向趋近于 $0$ 时该比率的极限。\n$$ \\lim_{\\delta \\to 0^+} R(m,\\delta) = \\lim_{\\delta \\to 0^+} \\frac{H_m}{1+\\delta} $$\n由于 $H_m$ 是关于 $\\delta$ 的常数，并且函数 $f(\\delta) = \\frac{H_m}{1+\\delta}$ 在 $\\delta=0$ 处是连续的，我们可以通过直接代入来计算极限：\n$$ \\lim_{\\delta \\to 0^+} \\frac{H_m}{1+\\delta} = \\frac{H_m}{1+0} = H_m $$\n所求的极限是第 $m$ 个调和数 $H_m$。",
            "answer": "$$ \\boxed{H_m} $$"
        },
        {
            "introduction": "理论上的最坏情况分析虽然至关重要，但有时可能会显得过于悲观。这个练习将我们的视角从纯理论转向实验。通过亲手实现著名的顶点覆盖2-近似算法，并在随机图上进行测试，你将能够测量它在“平均”情况下的实际性能。这将让你直观地比较算法的实际表现与其理论保证的最坏性能，从而更深入地理解近似算法在现实世界中的应用价值。",
            "id": "3207616",
            "problem": "你需要实现一个实验，以估计一个用于顶点覆盖问题的经典2-近似算法在Erdős–Rényi随机图上的平均情况近似比。目标是设计一个完整、可运行的程序，该程序能生成随机图，应用近似算法，计算精确的最优顶点覆盖以衡量近似比，并在多次试验中汇总结果。\n\n你必须使用以下基本概念：\n- 无向图的顶点覆盖是一个接触到每条边的顶点子集。形式上，对于无向图 $G = (V, E)$，一个集合 $C \\subseteq V$ 是顶点覆盖，如果对于每条边 $(u, v) \\in E$，都有 $u \\in C$ 或 $v \\in C$ 成立。\n- Erdős–Rényi 随机图模型 $G(n, p)$ 在一个大小为 $n$ 的顶点集上，以概率 $p$ 独立地抽取每条可能的边。\n- 对于一个优化问题，近似算法通过其近似比进行评估。给定一个实例 $I$，对于一个最小化问题，其最优值为 $\\operatorname{OPT}(I)$，算法输出值为 $\\operatorname{ALG}(I)$，则其在 $I$ 上的近似比定义为 $\\rho(I) = \\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)}$。约定如果 $\\operatorname{OPT}(I) = 0$，则近似比定义为 $1$（这适用于没有边的图，此时最优和算法所得的顶点覆盖大小均为 $0$）。\n- 基于极大匹配的经典顶点覆盖2-近似算法构造任意一个极大匹配 $M$，并返回 $M$ 中所有边的端点集合。该集合的大小为 $2 \\lvert M \\rvert$，并且是一个有效的顶点覆盖。\n\n你的程序必须：\n- 实现上述基于极大匹配的2-近似算法，其中匹配通过按固定的确定性顺序扫描边来贪心构造。\n- 为每个生成的图实例计算精确的最优顶点覆盖大小。你必须精确地（而不是近似地）完成此计算，可以使用任何正确的精确方法。禁止使用提示；但可接受正确的精确回溯或分支定界搜索。\n- 对于每个随机图实例 $G \\sim G(n, p)$，按上述規定计算比率 $\\rho(G)$。\n- 对于每个参数元组，使用蒙特卡洛估计量来估计平均情况近似比，即对从 $G(n, p)$ 中抽取的 $s$ 个独立样本 $G$ 的 $\\rho(G)$ 值取算术平均值，并使用固定的随机种子以确保可复现性。\n- 如果生成的图没有边（因此最优顶点覆盖大小为 $0$），则定义 $\\rho(G) = 1$。\n- 所有计算都是无单位且纯组合的；不涉及物理单位。\n\n从基本定义出发需要实现的设计细节：\n- 基于极大匹配的2-近似算法的正确性源于任何匹配 $M$ 的大小都是任何顶点覆盖大小的一个下界。由于匹配中的每条边都需要一个不同的顶点来覆盖它，因此 $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$。该算法返回一个大小为 $2 \\lvert M \\rvert$ 的顶点覆盖，因此其大小最多是最优解的两倍。\n- 蒙特卡洛估计量使用算术平均值作为在 $G(n, p)$ 模型下期望近似比的估计，这是基于大数定律，该定律指出随着样本数量的增加，样本均值会收敛于期望值。\n\n输入是隐式的；你必须硬编码并运行以下参数值的测试套件。每个测试用例是一个元组 $(n, p, s, \\text{seed})$，其中 $n$ 是顶点数， $p$ 是边概率， $s$ 是独立图样本的数量， $\\text{seed}$ 是随机数生成器的种子：\n- 测试 1: $(n, p, s, \\text{seed}) = (10, 0.2, 80, 101)$。\n- 测试 2: $(n, p, s, \\text{seed}) = (12, 0.5, 30, 202)$。\n- 测试 3: $(n, p, s, \\text{seed}) = (8, 0.0, 60, 303)$。\n- 测试 4: $(n, p, s, \\text{seed}) = (9, 0.9, 40, 404)$。\n- 测试 5: $(n, p, s, \\text{seed}) = (1, 0.7, 10, 505)$。\n\n算法要求与约束：\n- $G(n, p)$ 的图生成必须以概率 $p$ 独立地包含每个无序对 $\\{i, j\\}$（其中 $0 \\le i  j  n$），且不能包含自环或多重边。\n- 必须通过正确的精确算法计算精确的最优顶点覆盖。一个可接受的方法是分支定界搜索，它重复地选择一条未覆盖的边 $(u, v)$，并分支为将 $u$ 或 $v$ 加入覆盖集中，同时使用记忆化或剪枝来使计算在所用的小图上是可行的。\n- 极大匹配必须通过按字典序扫描边来确定性地构造，如果边的两个端点当前都未匹配，则添加该边，直到无法再添加更多边为止。\n\n输出：\n- 对于每个测试用例，输出一个浮点数，等于 $s$ 个图的 $\\rho(G)$ 的样本均值，四舍五入到 $6$ 位小数。\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内容为按上述测试套件顺序排列的结果，例如 $[r_1, r_2, r_3, r_4, r_5]$，其中每个 $r_i$ 是测试 $i$ 的四舍五入后的浮点结果。\n\n没有用户输入。最终输出必须是指定格式的单行文本。所有随机化必须使用提供的种子。通过为每个测试用例独立使用给定的种子来为随机数生成器设定种子，以确保可复现性。答案是无单位的实数。不涉及角度或物理单位。",
            "solution": "我们从图论和近似分析的核心定义出发，推导出完整的解决方案，然后设计实验所需的算法组件。\n\n1. 定义与目标量\n- 设 $G = (V, E)$ 是一个无向简单图，其中 $\\lvert V \\rvert = n$，边集为 $E$。顶点覆盖 $C \\subseteq V$ 满足对于每条边 $(u, v) \\in E$，都有 $u \\in C$ 或 $v \\in C$。\n- 在 Erdős–Rényi 模型 $G(n, p)$ 中，对于 $0 \\le i  j  n$，每条可能的边 $\\{i, j\\}$都以概率 $p$ 独立存在。\n- 对于一个最优值为 $\\operatorname{OPT}(I)$、算法输出为 $\\operatorname{ALG}(I)$ 的最小化问题实例 $I$，其近似比为 $\\rho(I) = \\frac{\\operatorname{ALG}(I)}{\\operatorname{OPT}(I)}$。对于我们的实验，当 $\\operatorname{OPT}(G) = 0$ 时（即没有边），我们定义 $\\rho(G) = 1$，因为最优解和算法输出均为 $0$，我们赋予一个中性的比率 $1$。\n- 在 $G(n, p)$ 模型下的平均情况近似比为 $\\mathbb{E}_{G \\sim G(n, p)}[\\rho(G)]$。我们通过蒙特卡洛估计量 $\\hat{\\rho} = \\frac{1}{s} \\sum_{i=1}^{s} \\rho(G_i)$ 来估计它，其中 $G_i$ 是从 $G(n, p)$ 中抽取的独立样本。\n\n2. 2-近似算法及其保证\n- 匹配 $M \\subseteq E$ 是一组不相交的边。任何顶点覆盖都必须包含 $M$ 中每条边的至少一个端点，因此 $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$。\n- 经典的2-近似算法构造任意一个极大匹配 $M$，并返回 $M$ 中所有边的端点集合 $C$。那么 $\\lvert C \\rvert = 2 \\lvert M \\rvert$。由于 $\\lvert M \\rvert \\le \\operatorname{OPT}(G)$，我们有 $\\lvert C \\rvert \\le 2 \\operatorname{OPT}(G)$，因此该算法实现了最坏情况下的2倍近似因子。\n- 我们实现一个确定性的贪心极大匹配算法：按字典序遍历所有边，如果某条边的两个端点都未被匹配，则将其加入匹配。返回的覆盖大小为 $2 \\lvert M \\rvert$。\n\n3. 精确最优顶点覆盖计算\n- 需要进行精确计算才能精确地衡量 $\\rho(G)$。对于测试套件中的小图，分支定界搜索就足够了。\n- 预处理步骤：将边索引为 $e_0, e_1, \\dots, e_{m-1}$，其中 $m = \\lvert E \\rvert$。对于每个顶点 $v \\in \\{0, 1, \\dots, n-1\\}$，预先计算一个位掩码 $B_v \\in \\{0, 1\\}^m$（存储为整数），指示哪些边与 $v$ 相关联。用一个位掩码 $X \\in \\{0, 1\\}^m$ 表示当前未覆盖的边集，其中如果 $e_i$ 未被覆盖，则第 $i$ 位为 $1$。\n- 递归关系：如果 $X = 0$，则所有边都已被覆盖，额外需要的顶点数为 $0$。否则，选择任意一条未覆盖的边 $e_i = (u, v)$，其在 $X$ 中的对应位为 $1$。为了覆盖 $e_i$，我们必须包含 $u$ 或 $v$。进行分支：\n  - 包含 $u$：新的未覆盖边掩码为 $X' = X \\text{ \\ } (\\sim B_u)$，成本为 $1$ 加上覆盖 $X'$ 所需的最小成本。\n  - 包含 $v$：新的未覆盖边掩码为 $X'' = X \\text{ \\ } (\\sim B_v)$，成本为 $1$ 加上覆盖 $X''$ 所需的最小成本。\n  取两个结果中的最小值。\n- 剪枝和记忆化：\n  - 上界初始化：使用2-近似算法计算一个可行覆盖的大小，以初始化一个上界 $U$。\n  - 维护迄今为止找到的最佳解值，并放弃任何已经选择了至少 $U$ 个顶点的递归分支。\n  - 对于给定的未覆盖边掩码 $X$，记忆化遇到的最小选择数；如果一个递归调用以更多或相等的选择数到达同一个 $X$，则剪枝该调用，因为它不可能导出更好的解。\n  - 为改进剪枝效果，当对 $(u, v)$进行分支时，首先探索 $\\{u, v\\}$ 中能覆盖更多当前未覆盖边的顶点，即比较 $X \\text{ \\ } B_u$ 和 $X \\text{ \\ } B_v$ 的置位位数（popcount），并首先对较大的那个进行递归。\n- 正确性可以通过对未覆盖边数量的归纳来证明：在每一步，必须选择一条未覆盖边的至少一个端点，而算法探索了两种选择，确保找到最小解。剪枝和记忆化不会移除任何潜在的最优解，因为它们只避免探索那些不可能改善当前最佳解或被先前已见状态支配的分支。\n\n4. 蒙特卡洛平均情况估计\n- 对于每个测试用例 $(n, p, s, \\text{seed})$，使用给定的种子初始化一个伪随机数生成器。通过以概率 $p$ 包含每条可能的边，从 $G(n, p)$ 中独立抽样 $s$ 个图。\n- 对于每个抽样图 $G$，通过2-近似算法计算 $\\operatorname{ALG}(G)$，通过精确方法计算 $\\operatorname{OPT}(G)$。如果 $\\operatorname{OPT}(G) = 0$，则定义 $\\rho(G)$ 为 $1$，否则 $\\rho(G) = \\frac{\\operatorname{ALG}(G)}{\\operatorname{OPT}(G)}$。\n- 估计的平均情况近似比为 $\\hat{\\rho} = \\frac{1}{s} \\sum_{i=1}^{s} \\rho(G_i)$。\n\n5. 测试套件与输出\n- 实现五个测试用例：\n  - $(n, p, s, \\text{seed}) = (10, 0.2, 80, 101)$。\n  - $(n, p, s, \\text{seed}) = (12, 0.5, 30, 202)$。\n  - $(n, p, s, \\text{seed}) = (8, 0.0, 60, 303)$。\n  - $(n, p, s, \\text{seed}) = (9, 0.9, 40, 404)$。\n  - $(n, p, s, \\text{seed}) = (1, 0.7, 10, 505)$。\n- 对于每个测试用例，输出一个浮点数，等于 $\\rho(G)$ 的样本均值，四舍五入到 $6$ 位小数。\n- 最终程序必须打印一行，包含五个结果的列表，按顺序排列，形式为方括号括起来的逗号分隔列表，例如 $[1.234567,1.111111,1.000000,1.050000,1.000000]$。\n\n该设计植根于图论和近似分析的核心定义，对小实例采用严格正确的精确计算来获得最优解，利用可证明的2-近似方法作为实验对象，并使用蒙特卡洛方法估计在Erdős–Rényi模型下的期望近似比。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom typing import List, Tuple, Dict\n\ndef generate_er_graph(n: int, p: float, rng: np.random.RandomState) - List[Tuple[int, int]]:\n    \"\"\"\n    Generate an undirected simple graph G(n, p) as a list of edges (u, v) with u  v.\n    \"\"\"\n    edges = []\n    if n = 1 or p == 0.0:\n        return edges\n    # Iterate lexicographically over pairs (i, j), i  j\n    for i in range(n - 1):\n        # Generate a vector of randoms for edges (i, j) for j > i\n        # However, generating one by one is simpler and avoids large arrays.\n        for j in range(i + 1, n):\n            if rng.rand()  p:\n                edges.append((i, j))\n    return edges\n\ndef approx_vertex_cover_size(edges: List[Tuple[int, int]], n: int) - int:\n    \"\"\"\n    Two-approximation via maximal matching: greedily add edges in lex order if both endpoints free,\n    then return the number of endpoints in the matching (i.e., 2*|M|).\n    \"\"\"\n    matched = [False] * n\n    cover_size = 0\n    # Edges already lexicographically ordered by construction in generate_er_graph\n    for u, v in edges:\n        if not matched[u] and not matched[v]:\n            matched[u] = True\n            matched[v] = True\n            cover_size += 2\n    return cover_size\n\ndef exact_min_vertex_cover_size(edges: List[Tuple[int, int]], n: int, approx_upper_bound: int) - int:\n    \"\"\"\n    Exact minimum vertex cover using branch-and-bound over uncovered-edge bitmasks.\n    - edges: list of (u, v) with u  v\n    - n: number of vertices\n    - approx_upper_bound: an initial upper bound for pruning (e.g., 2-approx size)\n    Returns the exact size of a minimum vertex cover.\n    \"\"\"\n    m = len(edges)\n    if m == 0:\n        return 0\n\n    # Map each vertex to the bitmask of incident edges\n    incident_masks = [0] * n\n    for idx, (u, v) in enumerate(edges):\n        incident_masks[u] |= (1  idx)\n        incident_masks[v] |= (1  idx)\n\n    # Initial uncovered edges mask: all edges uncovered\n    full_mask = (1  m) - 1\n\n    # Best found so far (upper bound). Initialize with approx_upper_bound, but it might be 0 if no edges.\n    best = [approx_upper_bound if approx_upper_bound > 0 else m]  # wrap in list to allow mutation in closure\n\n    # Memoization: for a given uncovered-edge mask X, store the minimum number of selections (taken) encountered.\n    seen: Dict[int, int] = {}\n\n    # Precompute an order of edges to select the first uncovered edge quickly by index.\n    # We simply keep the list and will find the first set bit when needed.\n\n    def dfs(uncovered_mask: int, taken: int) -> None:\n        # Branch-and-bound pruning by current best\n        if taken >= best[0]:\n            return\n\n        if uncovered_mask == 0:\n            # All edges covered\n            if taken  best[0]:\n                best[0] = taken\n            return\n\n        # Memoization pruning: if we've seen this mask with = taken, no need to proceed\n        prev = seen.get(uncovered_mask)\n        if prev is not None and taken >= prev:\n            return\n        seen[uncovered_mask] = taken\n\n        # Pick one uncovered edge: get index of a set bit in uncovered_mask\n        # Use least significant set bit\n        lsb = uncovered_mask  -uncovered_mask\n        edge_index = (lsb.bit_length() - 1)\n        u, v = edges[edge_index]\n\n        # Compute degrees with respect to uncovered edges to decide branch order\n        deg_u = (uncovered_mask  incident_masks[u]).bit_count()\n        deg_v = (uncovered_mask  incident_masks[v]).bit_count()\n\n        # Explore the vertex that covers more uncovered edges first to prune faster\n        if deg_u >= deg_v:\n            # Include u\n            dfs(uncovered_mask  ~incident_masks[u], taken + 1)\n            # Include v\n            dfs(uncovered_mask  ~incident_masks[v], taken + 1)\n        else:\n            # Include v\n            dfs(uncovered_mask  ~incident_masks[v], taken + 1)\n            # Include u\n            dfs(uncovered_mask  ~incident_masks[u], taken + 1)\n\n    dfs(full_mask, 0)\n    return best[0]\n\ndef estimate_average_ratio(n: int, p: float, samples: int, seed: int) -> float:\n    \"\"\"\n    For given (n, p), estimate the average approximation ratio over 'samples' graphs,\n    using the provided seed for reproducibility.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    ratios_sum = 0.0\n    for _ in range(samples):\n        edges = generate_er_graph(n, p, rng)\n        # Approximate cover size\n        approx_size = approx_vertex_cover_size(edges, n)\n        # Exact optimal cover size\n        opt_size = exact_min_vertex_cover_size(edges, n, approx_size)\n        if opt_size == 0:\n            ratio = 1.0  # by convention for edgeless graphs\n        else:\n            ratio = approx_size / opt_size\n        ratios_sum += ratio\n    return ratios_sum / samples if samples > 0 else float('nan')\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (n, p, samples, seed)\n    test_cases = [\n        (10, 0.2, 80, 101),\n        (12, 0.5, 30, 202),\n        (8, 0.0, 60, 303),\n        (9, 0.9, 40, 404),\n        (1, 0.7, 10, 505),\n    ]\n\n    results = []\n    for n, p, s, seed in test_cases:\n        avg_ratio = estimate_average_ratio(n, p, s, seed)\n        results.append(f\"{avg_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}