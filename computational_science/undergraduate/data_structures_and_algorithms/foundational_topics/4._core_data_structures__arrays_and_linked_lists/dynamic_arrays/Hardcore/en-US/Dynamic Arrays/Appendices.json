{
    "hands_on_practices": [
        {
            "introduction": "To truly understand why the doubling strategy for dynamic arrays is so efficient, it is helpful to look beyond the total cost and consider the journey of a single element. This exercise challenges you to quantify the maximum number of times any one element will be copied during a series of appends. By analyzing this \"worst-case\" for an individual element, you will build a strong intuition for the logarithmic nature of resizes that underpins the amortized constant time performance of the structure .",
            "id": "3206830",
            "problem": "Consider a dynamic array that initially has capacity $1$ and is empty. The array supports an append operation that places a new element at the end in constant time when unused capacity is available. When an append finds the array full, the array performs a resize: it allocates a new contiguous block whose capacity is exactly double the current capacity, and then copies all current elements from the old block into the new block in order, after which the new element is appended. Assume no deletions occur. Define a “copy” of an element to mean a movement of that element during a resize (the initial placement of an element when it is appended does not count as a copy).\n\nAfter performing exactly $N \\ge 1$ appends, determine, as a closed-form expression in terms of $N$, the maximum possible number of times that any single element has been copied due solely to resizes during this sequence of $N$ appends. Provide your answer as an exact expression; no numerical approximation or rounding is required.",
            "solution": "The user wants me to solve a problem about dynamic arrays.\n\n### Step 1: Extract Givens\n- Initial array capacity: $1$.\n- Initial array size: $0$.\n- Append operation: Places a new element at the end.\n- Resize condition: Occurs when an append is attempted on a full array.\n- Resize mechanism:\n    1. New capacity is allocated as $2 \\times$ current capacity.\n    2. All existing elements are copied to the new block.\n    3. The new element is then appended.\n- Definition of \"copy\": Movement of an element during a resize. The initial insertion of an element is not a copy.\n- Total operations: Exactly $N$ appends are performed, with $N \\ge 1$.\n- Objective: Determine the maximum possible number of times any single element has been copied.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is a standard exercise in the analysis of algorithms, specifically concerning the amortized analysis of a dynamic array (also known as a vector or ArrayList). The model described (doubling capacity on resize) is a classic implementation strategy. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. The initial conditions, the rules for operation, and the quantity to be determined are all specified precisely. A unique, deterministic solution exists.\n- **Objectivity**: The problem statement is objective and uses formal, unambiguous language.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined question in the field of computer science about data structures and algorithms. I will now proceed with a formal solution.\n\n### Solution Derivation\nThe problem asks for the maximum number of times any single element has been copied after a sequence of $N$ append operations. To maximize the number of copies for an element, we should consider an element that has been present in the array for the longest possible duration, as it would be subject to every resize operation that occurs after its insertion. The first element to be appended, let us call it $E_1$, satisfies this condition. Therefore, the maximum number of copies for any element is the number of times $E_1$ is copied.\n\nLet's analyze the state of the dynamic array, specifically its capacity. The capacity, let's denote it by $C$, changes only during resize operations. According to the problem statement, the initial capacity is $C_0 = 1$. A resize operation doubles the capacity. Thus, the sequence of capacities the array can have are powers of $2$: $1, 2, 4, 8, \\dots, 2^k$ for non-negative integers $k$. A capacity of $C=2^k$ can hold up to $2^k$ elements.\n\nA resize is triggered when an append operation is attempted on an array that is full. That is, if the array has a capacity of $C$ and already contains $C$ elements, the next append causes a resize.\n\nLet us trace the process:\n1.  **Initial State**: Capacity $C=1$, size $S=0$.\n2.  **Append 1**: We append element $E_1$. The array has space, so no resize occurs. The state becomes $C=1$, $S=1$. The number of copies for $E_1$ is $0$.\n3.  **Append 2**: We try to append $E_2$. The array is full ($S=C=1$). A resize is triggered.\n    - A new array of capacity $2 \\times 1 = 2$ is allocated.\n    - Element $E_1$ is copied from the old array to the new one. This is the **first copy** of $E_1$.\n    - Element $E_2$ is appended.\n    - The state becomes $C=2$, $S=2$.\n4.  **Append 3**: We try to append $E_3$. The array is full ($S=C=2$). A resize is triggered.\n    - A new array of capacity $2 \\times 2 = 4$ is allocated.\n    - Elements $E_1$ and $E_2$ are copied. This is the **second copy** of $E_1$.\n    - Element $E_3$ is appended.\n    - The state becomes $C=4$, $S=3$.\n5.  **Append 4**: We append $E_4$. The array is not full ($S=3, C=4$). No resize. State becomes $C=4, S=4$. $E_1$ has still been copied twice.\n6.  **Append 5**: We try to append $E_5$. The array is full ($S=C=4$). A resize is triggered to capacity $2 \\times 4 = 8$. This involves the **third copy** of $E_1$.\n\nFrom this analysis, it is clear that $E_1$ is copied on every resize event that happens after its initial placement. The number of times $E_1$ is copied is therefore equal to the total number of resizes that occur during the $N$ appends.\n\nA resize from capacity $2^k$ to $2^{k+1}$ is triggered by the $(2^k+1)$-th append operation. Element $E_1$ is present for all resizes that increase the capacity from its initial value of $2^0$.\n\nAfter $N$ appends, the array must hold $N$ elements. The capacity of the array, $C_N$, must be at least $N$. Since capacities are powers of two, $C_N$ must be the smallest power of two that is greater than or equal to $N$. Let this capacity be $C_N = 2^k$.\nThe condition $C_N = 2^k \\ge N$ with $k$ being the smallest such integer implies that the previous capacity, $2^{k-1}$, was insufficient, i.e., $2^{k-1} < N$.\nSo, we have the inequality:\n$$2^{k-1} < N \\le 2^k$$\nThis inequality holds for $N > 1$. If $N=1$, the capacity is $1=2^0$, so $k=0$.\nFor $N > 1$, we can take the logarithm base $2$ of the inequality:\n$$\\log_2(2^{k-1}) < \\log_2(N) \\le \\log_2(2^k)$$\n$$k-1 < \\log_2(N) \\le k$$\nThis relationship is the definition of the ceiling function. Thus, for $N > 1$, the exponent $k$ of the final capacity is given by:\n$$k = \\lceil \\log_2(N) \\rceil$$\nLet's check this for $N=1$. The final capacity is $1 = 2^0$, so $k=0$. The formula gives $\\lceil \\log_2(1) \\rceil = \\lceil 0 \\rceil = 0$. So the formula holds for all $N \\ge 1$.\n\nThe sequence of capacities that the array must adopt to reach a capacity of $2^k$ is $2^0, 2^1, 2^2, \\ldots, 2^k$.\nEach step in this sequence from $2^j$ to $2^{j+1}$ for $j=0, 1, \\ldots, k-1$ corresponds to one resize operation.\nThe first element, $E_1$, is present in the array from the point where the capacity is $C=2^0$. It will be copied in every subsequent resize. The resizes are:\n- from $2^0 \\to 2^1$\n- from $2^1 \\to 2^2$\n- ...\n- from $2^{k-1} \\to 2^k$\n\nThe total number of these resize events is $(k-1) - 0 + 1 = k$.\nTherefore, the maximum number of times any single element is copied is $k$. Substituting the expression for $k$:\n$$\\text{Maximum copies} = \\lceil \\log_2(N) \\rceil$$\nThis expression is a closed form in terms of $N$ and is valid for all $N \\ge 1$.",
            "answer": "$$\\boxed{\\lceil \\log_2(N) \\rceil}$$"
        },
        {
            "introduction": "A data structure's true value is revealed by the range of operations it can perform efficiently. While a standard dynamic array excels at adding elements to its end, what happens when we need to efficiently remove elements from the front? This practice guides you through implementing both a naive `pop_front` operation to see its linear-time limitations, and then designing a highly efficient circular buffer to achieve amortized $\\mathcal{O}(1)$ performance for both additions and removals . This is a fundamental exercise in adapting a basic structure to meet new performance requirements.",
            "id": "3230221",
            "problem": "You are given the task of reasoning from first principles about dynamic arrays and amortized analysis, and to validate your reasoning by implementing two concrete array-backed structures. Use only the following foundational base: array indexing in Random Access Memory (RAM) is constant time; copying or assigning a single element has unit cost; copying $k$ elements has cost proportional to $k$; creating a new array and copying existing elements are the only sources of data movement aside from explicit assignments; the dynamic array capacity is always a power of $2$ when grown by doubling.\n\nYour tasks are:\n\n- Implement a standard dynamic array that supports $push\\_back$ in amortized constant time using doubling when full. Implement $pop\\_front$ by shifting all remaining elements one position to the left. Analyze the time cost of $pop\\_front$ purely by counting the number of element assignments it performs within the $pop\\_front$ operation. Ignore any cost incurred by reallocation due to $push\\_back$ when answering about $pop\\_front$.\n\n- Design and implement an array-backed queue with both $push\\_back$ and $pop\\_front$ that achieves amortized $\\mathcal{O}(1)$ for both operations. Use a circular buffer (ring) over a dynamic array with doubling on overflow and halving when, after an operation, the size is at most one quarter of the capacity. When resizing (either growing or shrinking), copy all elements into a new array in logical order starting at index $0$. Perform at most one resize per operation. Count the number of element copies done during resizing, and only during resizing, to quantify cost.\n\nFrom the above base and your implementations, derive why the naive $pop\\_front$ has linear cost in the current size, and why the ring buffer achieves amortized constant time for both $push\\_back$ and $pop\\_front$ under the specified doubling/halving policy.\n\nTest suite and required outputs:\n\nImplement your program to execute the following four test cases, using exact sequences of operations. For the naive dynamic array, count only the number of element assignments performed inside $pop\\_front$ due to shifting. For the ring buffer, count only the number of element copies performed during resizing. For any $pop\\_front$ on an empty structure, do nothing and return a distinguished value (such as a null) internally; such a no-op should not affect counters.\n\n- Test case $1$ (naive dynamic array shift cost):\n  - Start empty.\n  - Perform $push\\_back$ of the integers $1$ through $10$ in increasing order.\n  - Perform $pop\\_front$ exactly $3$ times.\n  - Perform $push\\_back$ of the integers $11$ through $15$ in increasing order.\n  - Perform $pop\\_front$ exactly $4$ times.\n  - Perform $push\\_back$ of the integers $16$ through $20$ in increasing order.\n  - Output for this case: a single integer equal to the total number of element assignments performed by all $pop\\_front$ operations (ignore any reallocations caused by $push\\_back$).\n\n- Test case $2$ (ring buffer resize copy count under mixed operations):\n  - Start empty.\n  - Perform $push\\_back$ of $1$ through $10$.\n  - Perform $pop\\_front$ $3$ times.\n  - Perform $push\\_back$ of $11$ through $15$.\n  - Perform $pop\\_front$ $4$ times.\n  - Perform $push\\_back$ of $16$ through $20$.\n  - Output for this case: a single integer equal to the total number of element copies performed during all resizes (both grow and shrink).\n\n- Test case $3$ (ring buffer correctness and wrap-around behavior):\n  - Start empty.\n  - Perform $push\\_back$ of $1$ through $6$.\n  - Perform $pop\\_front$ $4$ times.\n  - Perform $push\\_back$ of $7$, $8$, $9$, $10$.\n  - Perform $pop\\_front$ $2$ times.\n  - Output for this case: the final content of the ring buffer as a list of integers in logical order from front to back.\n\n- Test case $4$ (ring buffer growth and shrink copy count):\n  - Start empty.\n  - Perform $push\\_back$ of $1$ through $16$.\n  - Perform $pop\\_front$ $12$ times.\n  - Perform $push\\_back$ of $17$ through $24$.\n  - Perform $pop\\_front$ $8$ times.\n  - Output for this case: a single integer equal to the total number of element copies performed during all resizes (both grow and shrink).\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the four test case results in order: $[$result\\_1,result\\_2,result\\_3,result\\_4$]$. The first, second, and fourth results are integers. The third result is a list of integers. For example, a syntactically valid output looks like $[42,7,[3,5,8],19]$ where the specific numbers must match the correct results for the test suite above.",
            "solution": "The problem requires an analysis and implementation of two array-backed data structures to compare their performance for queue-like operations, specifically `push_back` and `pop_front`. The analysis must be derived from first principles, grounded in a specified cost model where element assignments and copies are the primary measure of work.\n\n**Foundational Principles and Cost Model**\n\nThe analysis is based on the following axioms:\n1.  Random Access Memory (RAM) allows for constant time, $\\mathcal{O}(1)$, access to any array element by its index.\n2.  The cost of a single element assignment or copy is a unit cost, which we can denote as $\\mathcal{O}(1)$.\n3.  The cost of copying a block of $k$ contiguous elements is $\\mathcal{O}(k)$.\n4.  Data movement is restricted to explicit assignments or block copies during array resizing.\n5.  The capacity of a dynamic array, when grown, is always a power of $2$, achieved by doubling the current capacity.\n\n**Part 1: Naive Dynamic Array with Shifting `pop_front`**\n\nA standard dynamic array supports `push_back` by appending an element to the end. If the underlying array is full, a new, larger array is allocated (in this case, of double the capacity), all existing elements are copied to it, and the old array is discarded. This doubling strategy ensures that the amortized cost of `push_back` is $\\mathcal{O}(1)$.\n\nThe problem specifies a naive implementation for `pop_front`. This operation must remove the element at index $0$. To maintain the contiguity of the array with its first logical element at index $0$, all subsequent elements must be shifted one position to the left.\n\n**Cost Analysis of Naive `pop_front`**\nLet the dynamic array contain $N$ elements, indexed from $0$ to $N-1$.\n1.  The element at index $0$ is removed.\n2.  The element at index $1$ is copied to index $0$.\n3.  The element at index $2$ is copied to index $1$.\n4.  This process continues until the last element at index $N-1$ is copied to index $N-2$.\n\nThis sequence involves $N-1$ individual element assignment operations. According to the stated cost model, where each assignment has a unit cost, the total cost of a single `pop_front` operation is directly proportional to $N-1$. Therefore, the time complexity of the naive `pop_front` is $\\mathcal{O}(N)$, where $N$ is the current number of elements in the array. This cost is incurred for every `pop_front` call, so there is no opportunity for amortization; the worst-case and average-case costs are both linear in the size of the array.\n\n**Part 2: Array-Backed Queue with a Circular Buffer (Ring Buffer)**\n\nTo achieve efficient `push_back` and `pop_front` operations simultaneously, a circular buffer (or ring buffer) is employed. This structure is implemented over a dynamic array but treats the array's memory as a circle. Two pointers, `head` and `tail` (or an equivalent representation like `head` and `size`), are used to mark the logical beginning and end of the queue.\n\n-   `push_back`: Adds an element at the `tail` position and advances the `tail`.\n-   `pop_front`: Removes an element from the `head` position and advances the `head`.\n\nBoth pointers wrap around to the beginning of the array when they pass the end, typically using the modulo operator: `new_index = (old_index + 1) % capacity`. In the absence of resizing, both `push_back` and `pop_front` only require updating a pointer and a single element read/write. These are $\\mathcal{O}(1)$ operations.\n\n**Amortized Analysis of Ring Buffer Resizing**\nThe cost of this structure is dominated by its resizing operations. The specified policy is:\n-   **Growth:** If `size == capacity` upon a `push_back`, double the capacity ($C \\to 2C$).\n-   **Shrinking:** If `size <= capacity / 4` after an operation, halve the capacity ($C \\to C/2$).\n\nWhen a resize occurs, all $N$ elements are copied from the (potentially wrapped) old array into a new, contiguous array starting at index $0$. This \"unwraps\" the queue and costs $\\mathcal{O}(N)$. We can use a potential function analysis to show that the amortized cost of both operations is $\\mathcal{O}(1)$.\n\nLet $N$ be the number of elements and $C$ be the capacity. We define a potential function $\\Phi$ that stores \"credit\" to pay for future expensive resizes. A suitable potential function for this scheme is:\n$$ \\Phi = \\begin{cases} 2N - C & \\text{if } N/C \\ge 1/2 \\\\ C/2 - N & \\text{if } N/C < 1/2 \\end{cases} $$\nThe amortized cost $\\hat{c}$ of an operation is its actual cost $c$ plus the change in potential, $\\Delta\\Phi$.\n\n1.  **`push_back` causes growth**: Suppose we have $N=C$ elements and perform a `push_back`. The actual cost $c$ is $1$ (for the element write) $+ C$ (for copying $C$ elements to the new array). The total is $c = C+1$. The capacity changes from $C$ to $C' = 2C$, and the size becomes $N' = C+1$.\n    -   Old potential (with $N=C$, so $N/C=1 \\ge 1/2$): $\\Phi_{old} = 2N - C = 2C - C = C$.\n    -   New potential (with $N' = C+1, C'=2C$, so $N'/C' \\approx 1/2$): $\\Phi_{new} = 2N' - C' = 2(C+1) - 2C = 2$.\n    -   $\\Delta\\Phi = \\Phi_{new} - \\Phi_{old} = 2 - C$.\n    -   Amortized cost: $\\hat{c} = c + \\Delta\\Phi = (C+1) + (2-C) = 3$. This is $\\mathcal{O}(1)$.\n\n2.  **`pop_front` causes shrinking**: Suppose a `pop_front` reduces the size to $N = C/4$. The actual cost $c$ is $1$ (for the pop logic) $+ N$ (for copying $N$ elements). The total is $c = C/4+1$. The capacity changes from $C$ to $C' = C/2$, and the size remains $N' = N = C/4$.\n    -   The state just before shrinking was $N_{pre} = C/4+1$ elements in a capacity $C$ array. Since $N_{pre}/C < 1/2$, the potential was $\\Phi_{old} = C/2 - N_{pre} = C/2 - (C/4+1) = C/4 - 1$.\n    -   New potential (with $N' = C/4, C' = C/2$, so $N'/C' = 1/2$): $\\Phi_{new} = 2N' - C' = 2(C/4) - C/2 = 0$.\n    -   $\\Delta\\Phi = \\Phi_{new} - \\Phi_{old} = 0 - (C/4-1) = 1-C/4$.\n    -   Amortized cost: $\\hat{c} = c + \\Delta\\Phi = (C/4+1) + (1-C/4) = 2$. This is $\\mathcal{O}(1)$.\n\nIn all cases (including non-resizing ones, which can be similarly analyzed), the amortized cost is bounded by a small constant. The resizing strategy ensures that after a resize, there must be a number of operations proportional to the new capacity before another resize is required. This gap allows sufficient \"credit\" (potential) to accumulate to pay for the next linear-cost resize, rendering the amortized cost for every single operation $\\mathcal{O}(1)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes the four test cases defined in the problem statement\n    and prints the results in the specified format.\n    \"\"\"\n\n    # --- Test Case 1: Naive Dynamic Array Shift Cost ---\n    \n    class NaiveDynamicArray:\n        def __init__(self):\n            self.size = 0\n            self.capacity = 0\n            self.data = np.empty(self.capacity, dtype=object)\n            self.pop_front_cost = 0\n\n        def _resize(self, new_capacity):\n            new_data = np.empty(new_capacity, dtype=object)\n            if self.size > 0:\n                new_data[0:self.size] = self.data[0:self.size]\n            self.data = new_data\n            self.capacity = new_capacity\n\n        def push_back(self, value):\n            if self.size == self.capacity:\n                new_cap = 1 if self.capacity == 0 else self.capacity * 2\n                self._resize(new_cap)\n            self.data[self.size] = value\n            self.size += 1\n\n        def pop_front(self):\n            if self.size == 0:\n                return None\n            \n            value = self.data[0]\n            # Cost is the number of element assignments for the shift.\n            shift_cost = self.size - 1\n            self.pop_front_cost += shift_cost\n            \n            # Shift all elements to the left.\n            if shift_cost > 0:\n                self.data[0:shift_cost] = self.data[1:self.size]\n            \n            self.size -= 1\n            return value\n\n    nda = NaiveDynamicArray()\n    for i in range(1, 11): nda.push_back(i)\n    for _ in range(3): nda.pop_front()\n    for i in range(11, 16): nda.push_back(i)\n    for _ in range(4): nda.pop_front()\n    for i in range(16, 21): nda.push_back(i)\n    result_1 = nda.pop_front_cost\n\n    # --- Test Cases 2, 3, 4: Ring Buffer ---\n    \n    class RingBuffer:\n        def __init__(self):\n            self.size = 0\n            self.capacity = 0\n            self.head = 0\n            self.data = np.empty(self.capacity, dtype=object)\n            self.resize_cost = 0\n\n        def _resize(self, new_capacity):\n            # Cost is the number of elements copied.\n            self.resize_cost += self.size\n            \n            new_data = np.empty(new_capacity, dtype=object)\n            # Unwraps the circular buffer into a linear array.\n            for i in range(self.size):\n                old_idx = (self.head + i) % self.capacity\n                new_data[i] = self.data[old_idx]\n            \n            self.data = new_data\n            self.capacity = new_capacity\n            self.head = 0\n\n        def push_back(self, value):\n            if self.size == self.capacity:\n                new_cap = 1 if self.capacity == 0 else self.capacity * 2\n                self._resize(new_cap)\n            \n            tail_idx = (self.head + self.size) % self.capacity\n            self.data[tail_idx] = value\n            self.size += 1\n            # Per problem spec, shrink check is done after an operation.\n            # A push_back will never cause size = capacity / 4 if the\n            # invariant held before, so no shrink check needed here.\n            \n        def pop_front(self):\n            if self.size == 0:\n                return None\n            \n            value = self.data[self.head]\n            self.head = (self.head + 1) % self.capacity\n            self.size -= 1\n            \n            # Shrink condition: size is at most one quarter of capacity.\n            # Must also ensure capacity > 0 to avoid division by zero.\n            if self.capacity > 0 and self.size = self.capacity // 4:\n                # Halve capacity, minimum capacity is 1 if not empty.\n                new_cap = max(1, self.capacity // 2)\n                if new_cap  self.capacity:\n                    self._resize(new_cap)\n\n            return value\n\n        def get_contents(self):\n            contents = []\n            for i in range(self.size):\n                idx = (self.head + i) % self.capacity\n                contents.append(self.data[idx])\n            return contents\n\n    # Test Case 2\n    rb2 = RingBuffer()\n    for i in range(1, 11): rb2.push_back(i)\n    for _ in range(3): rb2.pop_front()\n    for i in range(11, 16): rb2.push_back(i)\n    for _ in range(4): rb2.pop_front()\n    for i in range(16, 21): rb2.push_back(i)\n    result_2 = rb2.resize_cost\n\n    # Test Case 3\n    rb3 = RingBuffer()\n    for i in range(1, 7): rb3.push_back(i)\n    for _ in range(4): rb3.pop_front()\n    for i in range(7, 11): rb3.push_back(i)\n    for _ in range(2): rb3.pop_front()\n    result_3 = rb3.get_contents()\n\n    # Test Case 4\n    rb4 = RingBuffer()\n    for i in range(1, 17): rb4.push_back(i)\n    for _ in range(12): rb4.pop_front()\n    for i in range(17, 25): rb4.push_back(i)\n    for _ in range(8): rb4.pop_front()\n    result_4 = rb4.resize_cost\n    \n    # Final output formatting\n    # The f-string conversion of a list automatically includes spaces, e.g., '[7, 8]'.\n    # We must remove them to match a strict list representation like '[7,8,9,10]'.\n    result_3_str = str(result_3).replace(\" \", \"\")\n\n    print(f\"[{result_1},{result_2},{result_3_str},{result_4}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simply storing and retrieving data, dynamic arrays serve as a powerful foundation for implementing complex algorithms. This exercise moves from basic operations to a more advanced application: finding the $k$-th smallest element in an unsorted array in expected linear time. You will implement the classic randomized Quickselect algorithm, which operates directly on the contiguous memory of a dynamic array you build from scratch, demonstrating how the properties of the data structure enable efficient, in-place computations .",
            "id": "3230152",
            "problem": "You are to design, analyze, and implement an algorithm that, given an unsorted dynamic array and an index $k$, returns the $k$-th smallest element using a method whose expected running time is linear in the length of the array. The dynamic array must be implemented explicitly using contiguous storage with a doubling-resize strategy, and your selection algorithm must operate directly on this structure without using any built-in sorting or selection routines.\n\nBase definitions to use:\n- A dynamic array is a sequence of elements stored in contiguous memory blocks such that the logical length $n$ (the number of elements) can be less than or equal to a separate capacity $C$, with $C$ indicating the number of elements for which space is currently reserved. When $n = C$ and a new element is appended, the dynamic array allocates a new block of size $2C$ and copies the $n$ elements, after which $C$ is updated to $2C$ and the append proceeds.\n- Under the doubling-resize discipline, the append operation has amortized constant time $O(1)$ measured on the Random Access Machine (RAM) model.\n- A partition-based selection algorithm repeatedly partitions the array around a pivot and recurses (or iterates) into the side that contains the target rank. If the pivot is chosen uniformly at random from the current subarray, the expected running time to find the $k$-th smallest element is linear in $n$.\n\nYour tasks:\n1. Starting from the definitions above, derive a correct algorithmic strategy that selects a pivot uniformly at random and partitions the dynamic array in place so that only one side needs further processing. Explain why the expected running time is linear in the number of elements $n$, and explain why using a doubling-resize dynamic array preserves amortized constant-time appends while building the array.\n2. Implement, from first principles, a DynamicArray data structure supporting at least the operations append, read by index, write by index, and length retrieval, under the doubling-resize strategy. Indexing should be zero-based, but the selection query uses $k$ as a one-based order statistic (that is, $k = 1$ means the minimum).\n3. Implement a randomized in-place selection algorithm that returns the $k$-th smallest element for valid $k$ in $\\{1,\\dots,n\\}$. Your implementation must not use any built-in sorting or selection utilities.\n4. Use your DynamicArray implementation to construct the arrays for the following test suite by appending elements one by one, and then run the selection algorithm on each pair $(A_i,k_i)$.\n\nTest suite:\n- Case $1$: $A_1 = [\\,7,2,9,4,1,5\\,]$, $k_1 = 3$.\n- Case $2$: $A_2 = [\\,-5,-1,-3,-2\\,]$, $k_2 = 1$.\n- Case $3$: $A_3 = [\\,3,3,3,3\\,]$, $k_3 = 4$.\n- Case $4$: $A_4 = [\\,5,1,5,2,5,3,5\\,]$, $k_4 = 5$.\n- Case $5$: $A_5 = [\\,9,8,7,6,5,4,3,2,1\\,]$, $k_5 = 9$.\n- Case $6$: $A_6$ is the descending sequence $[\\,1000,999,\\dots,2,1\\,]$, $k_6 = 500$.\n- Case $7$: $A_7 = [\\,0,-10,10,0,-10,10\\,]$, $k_7 = 4$.\n\nAll $k_i$ values are guaranteed valid, that is $1 \\le k_i \\le |A_i|$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for Cases $1$ through $7$ as a comma-separated list enclosed in square brackets, in order, for example $[\\,r_1,r_2,\\dots,r_7\\,]$, where each $r_i$ is the returned $k_i$-th smallest element for Case $i$.\n- There are no physical units, angles, or percentages involved in this task.\n\nConstraints and notes:\n- The algorithm must run in expected linear time $O(n)$ per selection query when the pivot is chosen uniformly at random in each partition step.\n- The program must be fully self-contained and must not read input or write anything other than the specified single-line output.\n- You must use the dynamic array you implement to store and process the data, constructing each $A_i$ by a sequence of $|A_i|$ appends.",
            "solution": "The problem requires the design, analysis, and implementation of a randomized selection algorithm to find the $k$-th smallest element in a dynamic array. The solution is presented in two parts: first, a theoretical derivation and analysis of the data structure and algorithm, and second, the concrete implementation.\n\n**1. Algorithmic and Data Structure Analysis**\n\n**1.1. Dynamic Array with Doubling-Resize Strategy**\n\nA dynamic array must accommodate a variable number of elements, $n$, while being stored in a fixed-size contiguous memory block of capacity $C$, where $n \\le C$. The doubling-resize strategy is employed when an element is appended and the array is full ($n=C$). A new memory block of capacity $2C$ is allocated, the existing $n$ elements are copied to the new block, and the old block is deallocated. The capacity is then updated, $C \\leftarrow 2C$, and the append operation proceeds.\n\nWe analyze the amortized cost of the `append` operation using the aggregate method. Let us consider a sequence of $N$ append operations on an initially empty dynamic array with an initial capacity of $C_0=1$. The cost of an append operation, $c_i$ for $i \\in \\{1, \\dots, N\\}$, is typically $1$ unit (for the write itself) plus a copy cost if a resize is triggered. A resize occurs when appending the $(2^j+1)$-th element, for $j \\ge 0$. At this point, the number of elements is $n = 2^j$ and the capacity is $C = 2^j$. The cost of this append operation is $c_{2^j+1} = 1 + 2^j$ (1 for the write, $2^j$ for copying the existing elements). All other appends have a cost of $1$.\n\nThe total cost, $T(N)$, for $N$ appends is the sum of the costs of all non-resizing appends and all resizing appends.\nLet $N$ be the total number of elements. The number of non-resizing appends is $N$ itself (each append involves at least one write). The resizes occur when the number of elements is $1, 2, 4, \\dots, 2^m$ where $2^m  N$. The cost of these resizes is the sum of the number of elements copied at each step.\n$$ T(N) = \\sum_{i=1}^N c_i = N + \\sum_{j=0}^{\\lfloor\\log_2(N-1)\\rfloor} 2^j $$\nThe summation is a geometric series: $\\sum_{j=0}^{k} 2^j = 2^{k+1} - 1$. Here, $k = \\lfloor\\log_2(N-1)\\rfloor$.\n$$ \\sum_{j=0}^{\\lfloor\\log_2(N-1)\\rfloor} 2^j  \\sum_{j=0}^{\\log_2(N)} 2^j = 2^{\\log_2(N)+1} - 1 = 2N - 1 $$\nSo, the total cost for copying is bounded by $2N$. The total cost for $N$ appends is $T(N)  N + 2N = 3N$.\nTherefore, $T(N) = O(N)$. The amortized cost per operation is the total cost divided by the number of operations, which is $T(N)/N = O(N)/N = O(1)$. This confirms that the append operation has an amortized constant time complexity.\n\n**1.2. Randomized Partition-Based Selection (Quickselect)**\n\nThe algorithm to find the $k$-th smallest element in an unsorted array of length $n$ operates by recursively partitioning the array. This algorithm is commonly known as Quickselect.\n\nThe strategy is as follows:\n1.  Given a subarray (initially, the entire array), choose a pivot element uniformly at random from this subarray.\n2.  Partition the subarray into three parts: elements smaller than the pivot, the pivot itself, and elements larger than the pivot. This can be done in-place in linear time, $O(m)$, where $m$ is the size of the subarray. After partitioning, the pivot element is in its final sorted position, say index $p$.\n3.  Let the number of elements in the partition less than or equal to the pivot be $L$. This corresponds to the pivot's rank. If the target rank $k$ is equal to $L$, then the pivot is the $k$-th smallest element, and the algorithm terminates.\n4.  If $k  L$, the $k$-th smallest element must be in the subarray of smaller elements. The algorithm recurses on this smaller subarray, still searching for the $k$-th smallest element.\n5.  If $k  L$, the $k$-th smallest element must be in the subarray of larger elements. The algorithm recurses on this larger subarray, but now searches for the $(k-L)$-th smallest element, as $L$ elements have been discarded from the left.\n\nThe key to efficiency is that we only recurse into one of the two partitions, not both (as in Quicksort).\n\n**Expected Time Complexity Analysis:**\nLet $T(n)$ be the expected running time of the algorithm on an array of size $n$. The partitioning step takes $cn$ time for some constant $c$. The pivot is chosen uniformly at random, so it has an equal probability ($1/n$) of being the $i$-th smallest element for any $i \\in \\{1, \\dots, n\\}$.\nIf the pivot is the $i$-th smallest, we are left with a subproblem of size $i-1$ (if $k  i$) or $n-i$ (if $k  i$). The size of the recursive call is thus $\\max(i-1, n-i)$. The recurrence for the expected time is:\n$$ T(n) \\le cn + \\frac{1}{n}\\sum_{i=1}^{n} T(\\max(i-1, n-i)) $$\nThe term $\\max(i-1, n-i)$ appears twice for most values.\n$$ T(n) \\le cn + \\frac{2}{n}\\sum_{i=\\lceil n/2 \\rceil}^{n-1} T(i) $$\nWe can prove $T(n) = O(n)$ by induction. Assume that $T(m) \\le am$ for all $m  n$ for some constant $a$.\n$$ T(n) \\le cn + \\frac{2}{n}\\sum_{i=\\lceil n/2 \\rceil}^{n-1} (ai) $$\n$$ T(n) \\le cn + \\frac{2a}{n} \\left( \\sum_{i=1}^{n-1} i - \\sum_{i=1}^{\\lceil n/2 \\rceil - 1} i \\right) $$\nUsing the formula for the sum of the first $k$ integers, $\\sum_{i=1}^{k}i = \\frac{k(k+1)}{2}$:\n$$ \\sum_{i=\\lceil n/2 \\rceil}^{n-1} i \\approx \\frac{(n-1)n}{2} - \\frac{(n/2)(n/2-1)}{2} = \\frac{n^2-n}{2} - \\frac{n^2/4-n/2}{2} \\approx \\frac{n^2}{2} - \\frac{n^2}{8} = \\frac{3n^2}{8} $$\nSubstituting this back into the recurrence:\n$$ T(n) \\le cn + \\frac{2a}{n} \\left( \\frac{3n^2}{8} \\right) = cn + \\frac{3a}{4}n = n \\left(c + \\frac{3a}{4}\\right) $$\nFor the induction to hold, we need $n(c + \\frac{3a}{4}) \\le an$. This is true if $c + \\frac{3a}{4} \\le a$, which simplifies to $c \\le \\frac{a}{4}$, or $a \\ge 4c$. We can always choose a constant $a$ large enough to satisfy this condition.\nThus, by the principle of mathematical induction, $T(n) = O(n)$. The expected running time is linear in the number of elements. The worst-case time is $O(n^2)$, which occurs if the pivot is consistently chosen to be the minimum or maximum element, but the probability of this happening with random pivot selection is exceedingly small.",
            "answer": "```python\nimport numpy as np\nimport random\nimport sys\n\n# Set a higher recursion limit for safety, although the iterative implementation avoids this.\n# sys.setrecursionlimit(2000)\n\nclass DynamicArray:\n    \"\"\"\n    A dynamic array implementation using a contiguous numpy array with a doubling-resize strategy.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes an empty dynamic array with an initial capacity of 1.\n        \"\"\"\n        self._n = 0  # Number of elements\n        self._capacity = 1  # Allocated memory size\n        self._A = np.empty(self._capacity, dtype=object)  # Underlying storage\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of elements in the array.\n        \"\"\"\n        return self._n\n\n    def __getitem__(self, k):\n        \"\"\"\n        Returns the element at index k.\n        \"\"\"\n        if not 0 = k  self._n:\n            raise IndexError('Index out of range')\n        return self._A[k]\n\n    def __setitem__(self, k, v):\n        \"\"\"\n        Sets the element at index k to value v.\n        \"\"\"\n        if not 0 = k  self._n:\n            raise IndexError('Index out of range')\n        self._A[k] = v\n\n    def append(self, item):\n        \"\"\"\n        Appends an item to the end of the array.\n        Resizes the array by doubling its capacity if it's full.\n        \"\"\"\n        if self._n == self._capacity:\n            self._resize(2 * self._capacity)\n        self._A[self._n] = item\n        self._n += 1\n\n    def _resize(self, new_capacity):\n        \"\"\"\n        Resizes the internal array to a new capacity.\n        \"\"\"\n        B = np.empty(new_capacity, dtype=object)\n        for i in range(self._n):\n            B[i] = self._A[i]\n        self._A = B\n        self._capacity = new_capacity\n\n    def __repr__(self):\n        \"\"\"\n        String representation of the array content.\n        \"\"\"\n        return f\"[{', '.join(map(str, [self._A[i] for i in range(self._n)]))}]\"\n\n\ndef _partition(arr, low, high):\n    \"\"\"\n    Helper function to partition the subarray arr[low..high] using the Lomuto partition scheme.\n    A random element is chosen as the pivot.\n    Returns the final index of the pivot.\n    \"\"\"\n    # Select a random pivot and move it to the end\n    pivot_index = random.randint(low, high)\n    arr[pivot_index], arr[high] = arr[high], arr[pivot_index]\n    pivot_value = arr[high]\n\n    # i is the index of the last element smaller than or equal to the pivot\n    i = low - 1\n    for j in range(low, high):\n        if arr[j] = pivot_value:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n\n    # Place the pivot in its final sorted position\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n\ndef randomized_select(arr, k):\n    \"\"\"\n    Finds the k-th smallest element in a DynamicArray in expected linear time.\n    arr: A DynamicArray instance.\n    k: The one-based rank of the element to find (e.g., k=1 for minimum).\n    \"\"\"\n    n = len(arr)\n    if not 1 = k = n:\n        raise ValueError(\"k must be between 1 and the length of the array\")\n\n    # The algorithm works with 0-based indices. The k-th smallest element\n    # is at index k-1 in a sorted array.\n    target_idx = k - 1\n    \n    low, high = 0, n - 1\n\n    while True:\n        # If the subarray has only one element, it must be the one we're looking for.\n        if low == high:\n            return arr[low]\n\n        pivot_idx = _partition(arr, low, high)\n\n        if pivot_idx == target_idx:\n            return arr[pivot_idx]\n        elif target_idx  pivot_idx:\n            high = pivot_idx - 1\n        else:  # target_idx > pivot_idx\n            low = pivot_idx + 1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final output.\n    \"\"\"\n    # Set a seed for the random number generator for reproducible results.\n    random.seed(42)\n\n    test_cases = [\n        {'A': [7, 2, 9, 4, 1, 5], 'k': 3},\n        {'A': [-5, -1, -3, -2], 'k': 1},\n        {'A': [3, 3, 3, 3], 'k': 4},\n        {'A': [5, 1, 5, 2, 5, 3, 5], 'k': 5},\n        {'A': [9, 8, 7, 6, 5, 4, 3, 2, 1], 'k': 9},\n        {'A': list(range(1000, 0, -1)), 'k': 500},\n        {'A': [0, -10, 10, 0, -10, 10], 'k': 4},\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Construct the dynamic array by appending elements one by one.\n        dynamic_arr = DynamicArray()\n        for x in case['A']:\n            dynamic_arr.append(x)\n        \n        # 2. Find the k-th smallest element.\n        k = case['k']\n        result = randomized_select(dynamic_arr, k)\n        results.append(result)\n\n    # 3. Print the results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}