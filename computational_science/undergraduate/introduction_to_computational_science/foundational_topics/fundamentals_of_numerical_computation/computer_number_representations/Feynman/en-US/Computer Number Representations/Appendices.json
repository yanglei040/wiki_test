{
    "hands_on_practices": [
        {
            "introduction": "Floating-point numbers, unlike the real numbers they approximate, are not continuous; there are finite gaps between any two adjacent representable values. This exercise () challenges you to quantify this gap for a very large number by exploring the fundamental concept of the Unit in the Last Place (ULP). By determining the smallest increment that can actually change a large stored value in a hypothetical scheduler, you will gain a concrete understanding of how floating-point precision is relative and why certain operations can appear to fail silently.",
            "id": "3109820",
            "problem": "A real-time scheduler records wall-clock time in seconds using the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 floating-point format and performs additions with the default round-to-nearest, ties-to-even rule. Let the current recorded time be $t = 10^{12}$ seconds. By starting from the core definitions of normalized IEEE 754 binary64 numbers (value of the form $m \\times 2^{e}$ with $m \\in [1,2)$ and a $53$-bit precision significand) and the definition of rounding to the nearest representable value, derive the smallest positive real increment $\\delta_{\\min}$ such that the computed floating-point sum $(t + \\delta_{\\min})$ is strictly greater than $t$. Then compute $\\delta_{\\min}$ numerically. Express your final answer in seconds and round your answer to four significant figures.\n\nBriefly relate your result to whether a scheduler that uses microsecond ticks, i.e., an intended increment of $\\Delta t = 10^{-6}$ seconds per tick, will successfully advance time at $t = 10^{12}$ seconds.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- Number format: IEEE 754 binary64.\n- Rounding rule: Round-to-nearest, ties-to-even.\n- Current time: $t = 10^{12}$ seconds.\n- Normalized number representation: $m \\times 2^{e}$, where $m \\in [1,2)$ and the significand has $53$-bit precision.\n- Task 1: Find the smallest positive real increment $\\delta_{\\min}$ such that the computed sum $(t + \\delta_{\\min})$ is strictly greater than $t$.\n- Task 2: Compute $\\delta_{\\min}$ numerically, rounded to four significant figures.\n- Task 3: Relate the result to a scheduler tick of $\\Delta t = 10^{-6}$ seconds.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the IEEE 754 standard, a cornerstone of computational science. All principles involved are well-established.\n- **Well-Posed:** The problem seeks a specific value, $\\delta_{\\min}$, based on a defined arithmetic system. The quantities and operations are well-defined. There is a subtle ambiguity in the term \"smallest positive real increment\" when the set of solutions is an open interval, but this is a common pedagogical phrasing where the intended answer is the infimum or threshold of that set. This does not render the problem unsolvable.\n- **Objective:** The problem is stated in precise, technical language with no subjective elements.\n- **Conclusion:** The problem is deemed valid as it is self-contained, scientifically sound, and objective. The ambiguity in phrasing is minor and has a standard interpretation in the context of numerical analysis.\n\n### Solution Derivation\nThe solution proceeds by first determining the floating-point representation of $t = 10^{12}$, then analyzing the effect of adding a small increment $\\delta$ under the specified rounding rules.\n\nA normalized IEEE 754 binary64 number is of the form $v = (-1)^s \\times m \\times 2^e$, where $s$ is the sign bit, $m$ is the significand with $53$ bits of precision satisfying $1 \\le m  2$, and $e$ is the integer exponent. The significand can be written as $m = 1 + f$, where $f$ is the fractional part represented by $52$ bits, so $m$ takes the form $1 + k \\cdot 2^{-52}$ for some integer $k \\in [0, 2^{52}-1]$.\n\n1.  **Representing $t = 10^{12}$ in binary64 format.**\n    First, we determine the exponent $e$. The exponent is the unique integer such that $2^e \\le t  2^{e+1}$.\n    $$e = \\lfloor \\log_2(t) \\rfloor = \\lfloor \\log_2(10^{12}) \\rfloor = \\lfloor 12 \\log_2(10) \\rfloor$$\n    Using the approximation $\\log_2(10) \\approx 3.321928$, we find:\n    $$e = \\lfloor 12 \\times 3.321928 \\rfloor = \\lfloor 39.863136 \\rfloor = 39$$\n    The biased exponent $E$ would be $e + 1023 = 39 + 1023 = 1062$, which is well within the valid range for the $11$-bit exponent field.\n\n    The ideal significand is $m_{\\text{ideal}} = \\frac{t}{2^e} = \\frac{10^{12}}{2^{39}}$.\n    To determine if $t$ is exactly representable, we check if its significand $m_{\\text{ideal}}$ can be expressed in the form $1 + k \\cdot 2^{-52}$. This is equivalent to checking if $t$ is a multiple of the smallest representable step size in its range. The step size between representable numbers in the interval $[2^e, 2^{e+1})$ is the Unit in the Last Place (ULP), given by:\n    $$\\text{ulp}(t) = 2^{e} \\times 2^{-52} = 2^{e-52}$$\n    For $e = 39$, the ULP is $\\text{ulp}(t) = 2^{39-52} = 2^{-13}$.\n    An arbitrary number $x$ in this range is representable if and only if it is an integer multiple of this ULP. We check if $t$ is a multiple of $2^{-13}$:\n    $$\\frac{t}{\\text{ulp}(t)} = \\frac{10^{12}}{2^{-13}} = 10^{12} \\times 2^{13} = (2 \\times 5)^{12} \\times 2^{13} = 2^{12} \\times 5^{12} \\times 2^{13} = 5^{12} \\times 2^{25}$$\n    Since $5^{12} \\times 2^{25}$ is an integer, $t = 10^{12}$ is an exactly representable binary64 number. Let $fl(t)$ denote the floating-point representation of $t$. Then $fl(t) = t$.\n\n2.  **Finding the smallest increment $\\delta_{\\min}$.**\n    Let $t$ be an exactly representable floating-point number. The next larger representable number is $t_{\\text{next}} = t + \\text{ulp}(t)$.\n    $$t_{\\text{next}} = t + 2^{-13}$$\n    We are looking for the smallest positive real increment $\\delta$ such that the computed sum, $fl(t + \\delta)$, is strictly greater than $t$. This means $fl(t + \\delta)$ must be at least $t_{\\text{next}}$.\n    The rounding rule is \"round-to-nearest, ties-to-even\". A real value $x$ is rounded to the nearest representable value. If $x$ is exactly halfway between two representable values, it is rounded to the one whose significand has a least significant bit of $0$ (the \"even\" one).\n\n    The midpoint between $t$ and $t_{\\text{next}}$ is:\n    $$t_{\\text{mid}} = t + \\frac{t_{\\text{next}} - t}{2} = t + \\frac{\\text{ulp}(t)}{2} = t + \\frac{2^{-13}}{2} = t + 2^{-14}$$\n    For the sum $t+\\delta$ to round up to $t_{\\text{next}}$, it must be either strictly greater than the midpoint $t_{\\text{mid}}$, or exactly equal to the midpoint if $t_{\\text{next}}$ is the \"even\" neighbor.\n\n    We must determine if $t$ or $t_{\\text{next}}$ is the \"even\" one. A floating-point number is \"even\" if the integer representation of its significand is even. The significand of $t$ is $m_t = \\frac{10^{12}}{2^{39}}$. It can be written as $m_t = \\frac{K_t}{2^{52}}$, where $K_t$ is an integer.\n    $$K_t = m_t \\times 2^{52} = \\frac{10^{12}}{2^{39}} \\times 2^{52} = 10^{12} \\times 2^{13} = 5^{12} \\times 2^{25}$$\n    Since $25  0$, $K_t$ is an even integer. Thus, $t$ is an \"even\" floating-point number. Consequently, $t_{\\text{next}}$ (whose significand corresponds to the integer $K_t+1$) must be \"odd\".\n\n    Now consider the sum $s = t + \\delta$.\n    - If $s  t_{\\text{mid}}$, $fl(s) = t$.\n    - If $s  t_{\\text{mid}}$, $fl(s) = t_{\\text{next}}$.\n    - If $s = t_{\\text{mid}}$, this is a tie. The value is rounded to the \"even\" neighbor, which is $t$. So, $fl(t_{\\text{mid}}) = t$.\n\n    For $fl(t + \\delta)$ to be strictly greater than $t$, we need $fl(t + \\delta) = t_{\\text{next}}$. This requires the exact sum $t+\\delta$ to be strictly greater than the midpoint:\n    $$t + \\delta  t_{\\text{mid}}$$\n    $$\\delta  t_{\\text{mid}} - t = 2^{-14}$$\n    The set of positive real values for $\\delta$ that satisfy this condition is the open interval $(\\frac{\\text{ulp}(t)}{2}, \\infty)$. This set does not contain a minimum element. However, the problem asks for \"the smallest positive real increment $\\delta_{\\min}$\". In this context, this is understood to mean the critical threshold or infimum of the set of successful values. This threshold is the boundary value that separates rounding down from rounding up.\n    $$\\delta_{\\min} = \\frac{\\text{ulp}(t)}{2} = 2^{-14}$$\n\n3.  **Numerical Calculation of $\\delta_{\\min}$.**\n    We compute the numerical value of $\\delta_{\\min}$ and round to four significant figures.\n    $$\\delta_{\\min} = 2^{-14} = \\frac{1}{16384} = 0.00006103515625 \\text{ seconds}$$\n    In scientific notation, this is $6.103515625 \\times 10^{-5}$ s. Rounding to four significant figures gives:\n    $$\\delta_{\\min} \\approx 6.104 \\times 10^{-5} \\text{ s}$$\n\n4.  **Relation to Scheduler Ticks.**\n    The scheduler uses an intended time increment of $\\Delta t = 10^{-6}$ s. We compare this to the threshold $\\delta_{\\min}$ required to produce a change in the stored time $t$.\n    The required increment is $\\delta  \\delta_{\\min} \\approx 6.104 \\times 10^{-5}$ s.\n    The scheduler's increment is $\\Delta t = 10^{-6}$ s $= 0.1 \\times 10^{-5}$ s.\n    We have $\\Delta t  \\delta_{\\min}$.\n    Since the increment $\\Delta t$ is less than the threshold $\\delta_{\\min} = \\text{ulp}(t)/2$, the computed sum $fl(t + \\Delta t)$ will be rounded down to $t$.\n    $$fl(10^{12} + 10^{-6}) = 10^{12}$$\n    Therefore, the scheduler will fail to advance the clock. Each tick is \"lost\" due to insufficient floating-point precision at this magnitude of time. The clock will remain stuck at $t=10^{12}$ seconds.",
            "answer": "$$\n\\boxed{6.104 \\times 10^{-5}}\n$$"
        },
        {
            "introduction": "When a calculation's exact result falls between two representable floating-point numbers, a rounding rule must decide the outcome. This hands-on coding exercise () asks you to simulate the four standard IEEE 754 rounding modes on a simple iterative sum, without relying on hardware settings. You will discover how a seemingly negligible increment leads to drastically different results over many iterations depending on the chosen mode, vividly illustrating the long-term impact of rounding policies.",
            "id": "3109818",
            "problem": "A developer is asked to compare how different rounding strategies affect a simple iterative computation under binary floating-point arithmetic. The computation is the recurrence defined by $x_{k+1} = x_k + \\delta$ with $\\delta = 2^{-54}$ and initial value $x_0 = 1$. The comparison must be performed for the following rounding strategies defined by the Institute of Electrical and Electronics Engineers (IEEE) 754 standard: rounding to nearest (ties to even), rounding toward zero, rounding toward plus infinity, and rounding toward minus infinity. The fundamental base for this task is the representation of real numbers in IEEE 754 binary64 format, specifically the spacing between adjacent representable numbers in the interval $[1, 2)$ and the operational definition of the rounding modes. In IEEE 754 binary64, normalized numbers in $[1, 2)$ have a constant spacing of $2^{-52}$ between adjacent representable values, which is the unit in the last place (ULP) at this magnitude. The objective is to design and implement an experiment that simulates the recurrence strictly under each rounding mode, without relying on hardware or runtime rounding mode settings, by applying rounding decisions derived from first principles.\n\nThe program must proceed as follows:\n- Model $x_k$ as a representable binary64 value in $[1, 2)$, conceptualized as $x_k = 1 + n_k \\cdot 2^{-52}$ for some integer $n_k \\in \\{0, 1, 2, \\dots\\}$ while $x_k \\in [1, 2)$, and apply the rounding decision per step using the exact comparison of the increment in units of ULP.\n- At each step, determine the rounded result of the exact sum $x_k + \\delta$ under each rounding mode by comparing the fractional increment $\\delta$ relative to the local ULP $2^{-52}$ and update $x_{k+1}$ accordingly.\n- Do not use any external hardware rounding controls; the rounding behavior must be emulated by logic that is consistent with the IEEE 754 rounding mode definitions.\n- Report the final $x_N$ as a binary64 value for each rounding mode and each test case.\n\nTest suite specification:\n- Use the set of iteration counts $N \\in \\{0, 1, 3, 4, 10^6\\}$ (the number $10^6$ is one million).\n- For each $N$ in the specified order, compute the final values $x_N$ under the rounding modes in the fixed order: rounding to nearest (ties to even), rounding toward zero, rounding toward plus infinity, rounding toward minus infinity.\n- All reported values must be scalar binary64 floating-point numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list must be flattened in the order described above. Specifically, the output must be $[x_N^{\\text{nearest}}, x_N^{\\text{toward-zero}}, x_N^{\\text{plus-inf}}, x_N^{\\text{minus-inf}}, x_N^{\\text{nearest}}, \\dots]$ where the block of four numbers is repeated for $N = 0$, then $N = 1$, then $N = 3$, then $N = 4$, then $N = 10^6$.\n- No units or angles are involved in the output; all numbers are pure dimensionless floating-point values.",
            "solution": "The problem statement is critically evaluated for validity prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Recurrence Relation**: $x_{k+1} = x_k + \\delta$\n- **Initial Value**: $x_0 = 1$\n- **Increment**: $\\delta = 2^{-54}$\n- **Arithmetic System**: IEEE 754 binary64 floating-point.\n- **Rounding Strategies**: \n  1. Rounding to nearest (ties to even)\n  2. Rounding toward zero\n  3. Rounding toward plus infinity\n  4. Rounding toward minus infinity\n- **Range of Interest**: The variable $x_k$ resides in the interval $[1, 2)$.\n- **ULP Definition**: In the interval $[1, 2)$, the spacing between adjacent representable binary64 numbers, known as the unit in the last place (ULP), is a constant $2^{-52}$.\n- **Modeling Constraint**: The simulation must emulate the specified rounding modes through explicit logic derived from first principles, without relying on hardware or runtime environment settings for rounding.\n- **Test Suite**: The number of iterations $N$ is drawn from the set $\\{0, 1, 3, 4, 10^6\\}$.\n- **Output Specification**: For each $N$, report the final value $x_N$ for each of the four rounding modes in a fixed order (nearest, toward-zero, plus-inf, minus-inf). The final output is a single flattened list of these results.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is fundamentally sound. It is based on the well-defined and standardized principles of IEEE 754 floating-point arithmetic. The values provided, such as the ULP for numbers in $[1, 2)$ being $2^{-52}$ and the increment $\\delta = 2^{-54}$, are correct and appropriately chosen to test the nuances of rounding.\n- **Well-Posed**: The problem is well-posed. It specifies a deterministic recurrence relation with a clear initial condition. For each rounding mode, the rules of arithmetic are unambiguous, leading to a unique and stable solution for any given number of iterations $N$.\n- **Objective**: The problem is stated in precise, objective language. It describes a computational experiment with no subjective or ambiguous terminology.\n- **Flaw Analysis**:\n  1. **Scientific Unsoundness**: None. The premises conform to established computer arithmetic standards.\n  2. **Non-Formalizable**: None. The problem is a classic numerical analysis exercise that is perfectly formalizable. It is directly relevant to the topic of computer number representations.\n  3. **Incomplete/Contradictory Setup**: None. All necessary information is provided. The constraint to emulate rounding logic is a key part of the problem design, not a contradiction.\n  4. **Unrealistic/Infeasible**: None. The computation is feasible. $10^6$ iterations are computationally trivial. The values remain well within the standard binary64 range.\n  5. **Ill-Posed**: None. The solution is unique and stable for each case.\n  6. **Pseudo-Profound/Trivial**: The problem is not trivial. While the implementation may appear simple after the correct analysis, arriving at that analysis requires a rigorous understanding of floating-point arithmetic. The choice of $\\delta$ relative to the ULP is specifically designed to probe the rounding rules for increments smaller than half a ULP, which is a key conceptual challenge.\n  7. **Outside Scientific Verifiability**: None. The results are verifiable by applying the rules of the IEEE 754 standard.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A reasoned solution will be developed.\n\n### Principle-Based Design and Solution\nThe objective is to simulate the recurrence relation $x_{k+1} = x_k + \\delta$ for $k = 0, 1, \\dots, N-1$, starting with $x_0 = 1$. The core of the task is to correctly model the effect of a single floating-point addition under the specified IEEE 754 rounding modes, assuming the result of each addition is immediately rounded to a binary64 representable number.\n\nThe initial value is $x_0 = 1$. The analysis will focus on the interval $[1, 2)$, where the exponent of a binary64 number is fixed, and the significand determines its value. In this interval, the unit in the last place (ULP) is constant and equal to $u = 2^{-52}$. All representable numbers in this interval are of the form $1 + n \\cdot u$ for some integer $n$.\n\nThe increment is $\\delta = 2^{-54}$. The crucial step is to relate $\\delta$ to the ULP, $u$:\n$$\n\\delta = 2^{-54} = 2^{-2} \\cdot 2^{-52} = \\frac{1}{4}u\n$$\nAt any step $k$, we consider the exact mathematical sum $S_k = x_k + \\delta$. Since $x_k$ is a representable number, $S_k$ is of the form:\n$$\nS_k = x_k + \\frac{1}{4}u\n$$\nThis exact sum $S_k$ lies between two adjacent representable numbers: $x_k$ and its successor $x_k^{+} = x_k + u$. To determine the rounded value $x_{k+1}$, we must compare $S_k$ to these representable numbers according to the rules of each rounding mode. The midpoint between $x_k$ and $x_k^{+}$ is $M = x_k + \\frac{1}{2}u$.\n\nThe analysis for each rounding mode is as follows:\n\n1.  **Rounding toward Plus Infinity (`roundTowardPositive`)**: This mode rounds to the smallest representable number that is greater than or equal to the exact result. Since $S_k = x_k + \\frac{1}{4}u$ is strictly greater than $x_k$ and is not itself representable, it must be rounded up to the next representable number, $x_k^{+}$.\n    $$\n    x_{k+1} = x_k^{+} = x_k + u\n    $$\n    This means that at every iteration, the value is incremented by exactly one ULP. After $N$ iterations, the final value will be:\n    $$\n    x_N = x_0 + N \\cdot u = 1 + N \\cdot 2^{-52}\n    $$\n\n2.  **Rounding toward Zero (`roundTowardZero`)**: This mode rounds toward $0$. For a positive number like $S_k$, this is equivalent to truncation. The exact sum $S_k$ lies in the open interval $(x_k, x_k^{+})$. The largest representable number less than or equal to $S_k$ is $x_k$.\n    $$\n    x_{k+1} = x_k\n    $$\n    The value never changes. After $N$ iterations, the final value is:\n    $$\n    x_N = x_0 = 1\n    $$\n\n3.  **Rounding toward Minus Infinity (`roundTowardNegative`)**: This mode rounds to the largest representable number that is less than or equal to the exact result (the floor function). For $S_k \\in (x_k, x_k^{+})$, this is $x_k$.\n    $$\n    x_{k+1} = x_k\n    $$\n    Similar to rounding toward zero, the value never changes. The final value is:\n    $$\n    x_N = x_0 = 1\n    $$\n\n4.  **Rounding to Nearest (ties to even) (`roundTiesToEven`)**: This mode rounds to the nearest representable number. In case of a tie (the exact result is precisely at the midpoint), it rounds to the number with an even least significant bit in its significand.\n    We compare the exact sum $S_k = x_k + \\frac{1}{4}u$ to the midpoint $M = x_k + \\frac{1}{2}u$.\n    $$\n    S_k  M\n    $$\n    Since the exact sum is always strictly closer to $x_k$ than to $x_k^{+}$, the rounding rule dictates rounding to $x_k$. A tie condition is never met.\n    $$\n    x_{k+1} = x_k\n    $$\n    The value never changes from its initial state. The final value is:\n    $$\n    x_N = x_0 = 1\n    $$\n\nThis analysis reveals that for the given recurrence and strict adherence to binary64 rounding after each step, only the `roundTowardPositive` mode results in an accumulating value. The other three modes result in stagnation, as the increment $\\delta$ is too small to overcome the rounding threshold.\n\nThe algorithm is a direct implementation of these derived formulas for each $N$ in the test suite.\nFor any $N \\in \\{0, 1, 3, 4, 10^6\\}$:\n- $x_N^{\\text{nearest}} = 1.0$\n- $x_N^{\\text{toward-zero}} = 1.0$\n- $x_N^{\\text{plus-inf}} = 1.0 + N \\cdot 2^{-52}$\n- $x_N^{\\text{minus-inf}} = 1.0$\n\nThese formulas are valid even for $N=0$, where they correctly yield $1.0$ for all modes. The implementation will compute these values for each $N$ and format them as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates an iterative computation under different IEEE 754 rounding strategies\n    by applying rounding decisions derived from first principles.\n    \"\"\"\n\n    # Define the test cases from the problem statement for the number of iterations N.\n    test_cases = [0, 1, 3, 4, 10**6]\n\n    # Initialize a list to store the final results.\n    results = []\n\n    # Define constants based on the problem statement.\n    # The initial value x_0.\n    x0 = np.float64(1.0)\n    # The unit in the last place (ULP) for numbers in the interval [1, 2) in binary64.\n    ulp = np.float64(2**-52)\n\n    # The problem asks to simulate the recurrence x_{k+1} = x_k + delta, where\n    # delta = 2**-54. The core of the problem is to understand how a single\n    # floating-point addition `x_k + delta` is rounded.\n    # The increment delta is 1/4 of the ULP.\n    # delta = ulp / 4.\n\n    # Analysis of rounding modes:\n    # The exact sum is S_k = x_k + ulp/4.\n    # This sum is between the representable numbers x_k and x_k_plus = x_k + ulp.\n    # The midpoint is M = x_k + ulp/2.\n\n    # 1. Round to nearest: S_k is closer to x_k than x_k_plus. Result is x_k. Value never changes.\n    # 2. Round toward zero: For positive numbers, this truncates. Result is x_k. Value never changes.\n    # 3. Round toward minus infinity: This is a floor operation. Result is x_k. Value never changes.\n    # 4. Round toward plus infinity: This is a ceil operation. Result is x_k_plus. Value increases by ulp each step.\n\n    for N in test_cases:\n        # Calculate the final value x_N for each rounding mode based on the analysis.\n        \n        # Mode 1: Rounding to nearest (ties to even)\n        # The recurrence x_{k+1} = round_nearest(x_k + delta) results in x_{k+1} = x_k.\n        # Thus, x_N = x_0.\n        xN_nearest = x0\n\n        # Mode 2: Rounding toward zero\n        # The recurrence x_{k+1} = round_zero(x_k + delta) results in x_{k+1} = x_k.\n        # Thus, x_N = x_0.\n        xN_toward_zero = x0\n\n        # Mode 3: Rounding toward plus infinity\n        # The recurrence x_{k+1} = round_plus_inf(x_k + delta) results in x_{k+1} = x_k + ulp.\n        # Thus, x_N = x_0 + N * ulp.\n        # The multiplication N * ulp is exact because N is an integer.\n        xN_plus_inf = x0 + np.float64(N) * ulp\n\n        # Mode 4: Rounding toward minus infinity\n        # The recurrence x_{k+1} = round_minus_inf(x_k + delta) results in x_{k+1} = x_k.\n        # Thus, x_N = x_0.\n        xN_minus_inf = x0\n\n        # Append the results for the current N to the list.\n        results.extend([xN_nearest, xN_toward_zero, xN_plus_inf, xN_minus_inf])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While the previous exercises explored the behavior of numbers with large or moderate magnitudes, this practice focuses on the other extreme: values approaching zero. Standard floating-point arithmetic includes 'gradual underflow', enabled by special representations known as subnormal numbers. In this problem (), you will see how these numbers provide a safety net that prevents catastrophic division-by-zero errors in a common scenario like an adaptive ODE solver, highlighting the stark difference between standard behavior and the 'flush-to-zero' mode.",
            "id": "3109800",
            "problem": "You will implement and analyze an adaptive step-size computation for a single-step explicit method applied to a simple linear ordinary differential equation, focusing on how binary floating-point subnormal (denormal) numbers in the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE-754) prevent a divide-by-zero event, and how enabling Flush-To-Zero (FTZ) alters that behavior. The analysis must be grounded in first principles of binary floating-point representation and the local error scaling of the numerical method.\n\nConsider the initial value problem for an ordinary differential equation\n$$\n\\frac{dy}{dt} = -\\lambda y, \\quad y(0) = y_0,\n$$\nwith parameters $y_0 \\in \\mathbb{R}$ and $\\lambda  0$. Use the forward Euler method with step size $h  0$ to compute one-step and step-doubling approximations. The one-step approximation is\n$$\ny_1 = y_n + h f(y_n) = y_n - h \\lambda y_n,\n$$\nand the two half-steps approximation is\n$$\ny_{1/2} = y_n + \\frac{h}{2} f(y_n), \\quad y_{2} = y_{1/2} + \\frac{h}{2} f(y_{1/2}).\n$$\nDefine the local error estimate by the absolute difference\n$$\n\\mathrm{err} = \\lvert y_2 - y_1 \\rvert.\n$$\n\nUse the following fundamental bases for your reasoning:\n- The IEEE-754 binary64 format has a smallest positive normal number $2^{-1022} \\approx 2.2250738585072014 \\times 10^{-308}$ and supports subnormal numbers in the interval $(0, 2^{-1022})$ to provide gradual underflow.\n- In Flush-To-Zero (FTZ) mode, any subnormal result is replaced by $0$ before further use.\n- The local truncation error of forward Euler scales quadratically with the step size, which is consistent with the observation that the one-step versus two half-steps difference is governed by a term of order $h^2$.\n\nYour program must:\n1. Compute $\\mathrm{err}$ as defined above from $y_n$, $h$, and $\\lambda$ using floating-point arithmetic.\n2. Emulate FTZ behavior by replacing any computed $\\mathrm{err}$ with $0$ if $0  \\mathrm{err}  2^{-1022}$, and leaving it unchanged otherwise. Also compute results in a mode without FTZ (no replacement).\n3. Compute an adaptive step-size scale factor $s$ based on the quadratic local error scaling. Clamp $s$ to a closed interval $[s_{\\min}, s_{\\max}]$ with $s_{\\min} = 0.1$ and $s_{\\max} = 10.0$. If a division by zero occurs when computing $s$, detect it and record that event explicitly.\n4. Produce results for each test case that quantify the behavior with and without FTZ. For each test case, report the pair $[s_{\\text{no\\_ftz}}, \\text{ftz\\_div\\_zero}]$, where $s_{\\text{no\\_ftz}}$ is the computed scale factor without FTZ, and $\\text{ftz\\_div\\_zero}$ is a boolean indicating whether a divide-by-zero occurred when FTZ was emulated.\n\nThere are no physical units in this problem. All angles, if any, must be in radians, but angles do not appear here. All outputs must be numeric floats or booleans or lists thereof.\n\nTest Suite:\nUse the following four test cases to ensure coverage of typical behavior, boundary conditions, and extreme edge cases. In all cases, use double-precision (binary64) arithmetic.\n- Case $1$ (happy path): $y_n = 1.0$, $h = 0.1$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-6}$.\n- Case $2$ (boundary into subnormal): $y_n = 2^{-1022}$, $h = 1.0$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-12}$.\n- Case $3$ (extreme underflow): $y_n = 10^{-320}$, $h = 1.0$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-12}$.\n- Case $4$ (threshold normal error): $y_n = 4 \\cdot 2^{-1022}$, $h = 1.0$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-12}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must be the pair $[s_{\\text{no\\_ftz}}, \\text{ftz\\_div\\_zero}]$ for the corresponding test case. For example, the overall format must be\n$$\n[\\,[s_{1,\\text{no\\_ftz}}, \\text{ftz\\_div\\_zero}_1], [s_{2,\\text{no\\_ftz}}, \\text{ftz\\_div\\_zero}_2], [s_{3,\\text{no\\_ftz}}, \\text{ftz\\_div\\_zero}_3], [s_{4,\\text{no\\_ftz}}, \\text{ftz\\_div\\_zero}_4]\\,],\n$$\nprinted as a single line with no additional text.",
            "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Differential Equation:** $\\frac{dy}{dt} = -\\lambda y$, with initial condition $y(0) = y_0$.\n-   **Parameters:** $y_n \\in \\mathbb{R}$, $\\lambda  0$, $h  0$.\n-   **Numerical Method:** Forward Euler.\n-   **One-Step Approximation ($y_1$):** $y_1 = y_n + h f(y_n) = y_n - h \\lambda y_n$.\n-   **Two Half-Steps Approximation ($y_2$):** $y_{1/2} = y_n + \\frac{h}{2} f(y_n)$, and $y_{2} = y_{1/2} + \\frac{h}{2} f(y_{1/2})$.\n-   **Error Estimate:** $\\mathrm{err} = \\lvert y_2 - y_1 \\rvert$.\n-   **Floating-Point Standard:** IEEE-754 binary64.\n-   **Smallest Positive Normal Number ($N_{\\min}$):** $N_{\\min} = 2^{-1022} \\approx 2.225 \\times 10^{-308}$.\n-   **Subnormal Numbers:** Numbers in the interval $(0, 2^{-1022})$.\n-   **Flush-To-Zero (FTZ) Emulation:** If a computed $\\mathrm{err}$ satisfies $0  \\mathrm{err}  2^{-1022}$, it is replaced by $0$. Otherwise, it is unchanged.\n-   **Step-Size Scaling:** The adaptive step-size scale factor $s$ is based on quadratic local error scaling, i.e., $\\mathrm{err} \\propto h^2$.\n-   **Scale Factor Clamping:** $s$ is clamped to the interval $[s_{\\min}, s_{\\max}] = [0.1, 10.0]$.\n-   **Division-by-Zero Detection:** The program must detect and report if a division by zero occurs when computing $s$ under the FTZ emulation.\n-   **Output per Test Case:** A pair $[s_{\\text{no\\_ftz}}, \\text{ftz\\_div\\_zero}]$, where $s_{\\text{no\\_ftz}}$ is the scale factor without FTZ, and $\\text{ftz\\_div\\_zero}$ is a boolean indicating a division-by-zero event with FTZ.\n-   **Test Cases (using binary64 arithmetic):**\n    1.  $y_n = 1.0$, $h = 0.1$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-6}$.\n    2.  $y_n = 2^{-1022}$, $h = 1.0$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-12}$.\n    3.  $y_n = 10^{-320}$, $h = 1.0$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-12}$.\n    4.  $y_n = 4 \\cdot 2^{-1022}$, $h = 1.0$, $\\lambda = 1.0$, $\\mathrm{tol} = 10^{-12}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding:** The problem is firmly grounded in established principles of numerical analysis (explicit single-step methods, local error estimation, adaptive step-size control) and computer arithmetic (IEEE-754 floating-point representation, subnormal numbers, flush-to-zero behavior). The physical model is a standard first-order linear ODE. The problem is scientifically sound.\n-   **Well-Posedness:** All necessary parameters ($y_n, h, \\lambda, \\mathrm{tol}$) and definitions are provided for a unique solution to be computed. The formula for the step-size scale factor $s$ is not explicitly stated but is unambiguously implied by the standard practice for adaptive step-sizing and the provided information that the error estimator scales quadratically ($O(h^2)$). For a desired tolerance $\\mathrm{tol}$, the new step size $h_{new} = s \\cdot h$ is chosen such that the new error is approximately $\\mathrm{tol}$. Given $\\mathrm{err} \\approx C h^2$, we want $\\mathrm{tol} \\approx C (s h)^2 = s^2 (C h^2) \\approx s^2 \\cdot \\mathrm{err}$. This leads to $s^2 \\approx \\mathrm{tol}/\\mathrm{err}$, so $s \\approx \\sqrt{\\mathrm{tol}/\\mathrm{err}}$. This formulation is standard and renders the problem well-posed.\n-   **Objectivity:** The problem is stated in precise, objective mathematical and computational terms, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid as it is scientifically grounded, well-posed, objective, and contains no inconsistencies or flaws. A complete solution will be provided.\n\n### Solution\n\nThe solution requires computing an adaptive step-size scaling factor for the forward Euler method, analyzing its behavior under standard IEEE-754 arithmetic and an emulated Flush-To-Zero (FTZ) mode.\n\n**1. Derivation of the Error Estimate, $\\mathrm{err}$**\n\nFirst, we derive an analytical expression for the error estimate $\\mathrm{err} = \\lvert y_2 - y_1 \\rvert$.\n\nThe one-step approximation is:\n$$y_1 = y_n - h \\lambda y_n = y_n(1 - h\\lambda)$$\n\nThe two half-steps approximation is computed as:\n$$y_{1/2} = y_n + \\frac{h}{2}f(y_n) = y_n - \\frac{h}{2}\\lambda y_n = y_n\\left(1 - \\frac{h\\lambda}{2}\\right)$$\n$$y_2 = y_{1/2} + \\frac{h}{2}f(y_{1/2}) = y_{1/2} - \\frac{h}{2}\\lambda y_{1/2} = y_{1/2}\\left(1 - \\frac{h\\lambda}{2}\\right)$$\nSubstituting the expression for $y_{1/2}$ into the equation for $y_2$:\n$$y_2 = y_n\\left(1 - \\frac{h\\lambda}{2}\\right)\\left(1 - \\frac{h\\lambda}{2}\\right) = y_n\\left(1 - h\\lambda + \\frac{h^2\\lambda^2}{4}\\right)$$\n\nNow, we compute the difference $y_2 - y_1$:\n$$y_2 - y_1 = y_n\\left(1 - h\\lambda + \\frac{h^2\\lambda^2}{4}\\right) - y_n(1 - h\\lambda) = y_n\\left(\\frac{h^2\\lambda^2}{4}\\right)$$\n\nThe error estimate is the absolute value of this difference:\n$$\\mathrm{err} = \\lvert y_2 - y_1 \\rvert = \\left\\lvert y_n \\frac{h^2\\lambda^2}{4} \\right\\rvert = \\frac{\\lvert y_n \\rvert h^2 \\lambda^2}{4}$$\nThis expression confirms that the local error estimate scales quadratically with the step size $h$.\n\n**2. Adaptive Step-Size Scale Factor, $s$**\n\nThe goal of adaptive step-sizing is to adjust the step size $h$ so that the local error estimate meets a specified tolerance $\\mathrm{tol}$. Let the new step be $h_{\\text{new}} = s \\cdot h$. The error for the new step size, $\\mathrm{err}_{\\text{new}}$, is expected to be $\\mathrm{tol}$. Using the quadratic scaling relationship $\\mathrm{err} \\propto h^2$:\n$$\\frac{\\mathrm{err}_{\\text{new}}}{\\mathrm{err}} \\approx \\frac{(h_{\\text{new}})^2}{h^2} = \\frac{(s \\cdot h)^2}{h^2} = s^2$$\nSetting $\\mathrm{err}_{\\text{new}} = \\mathrm{tol}$, we get:\n$$\\frac{\\mathrm{tol}}{\\mathrm{err}} \\approx s^2 \\implies s \\approx \\sqrt{\\frac{\\mathrm{tol}}{\\mathrm{err}}}$$\nThe problem requires clamping this scale factor to the interval $[s_{\\min}, s_{\\max}] = [0.1, 10.0]$. Thus, the computed scale factor is:\n$$s = \\max\\left(0.1, \\min\\left(10.0, \\sqrt{\\frac{\\mathrm{tol}}{\\mathrm{err}}}\\right)\\right)$$\nThis computation is only possible if $\\mathrm{err}  0$. If $\\mathrm{err} = 0$, a division by zero occurs.\n\n**3. Analysis of Floating-Point Behavior (Standard vs. FTZ)**\n\nThe core of the problem lies in the behavior of $\\mathrm{err}$ when its value is very small. In the IEEE-754 binary64 standard, numbers are represented in floating-point format.\n-   **Normal Numbers:** Have magnitudes in the approximate range $[2^{-1022}, 2^{1024})$. The smallest positive normal number is $N_{\\min} = 2^{-1022}$.\n-   **Subnormal (Denormal) Numbers:** Fill the gap between $0$ and $N_{\\min}$, allowing for \"gradual underflow\". They have magnitudes in $(0, 2^{-1022})$.\n-   **Flush-To-Zero (FTZ):** An alternative arithmetic mode where any operation resulting in a subnormal number is \"flushed\" to $0$. In our emulation, if $0  \\mathrm{err}  N_{\\min}$, we set $\\mathrm{err}_{\\text{ftz}}=0$.\n\nIf FTZ causes $\\mathrm{err}$ to become $0$, the computation of $s$ will involve a division by zero. Standard arithmetic, which supports subnormal numbers, will compute a very small non-zero $\\mathrm{err}$, avoiding the division by zero.\n\n**4. Analysis of Test Cases**\n\nLet $N_{\\min} = 2^{-1022}$.\n\n**Case 1:** $y_n = 1.0, h = 0.1, \\lambda = 1.0, \\mathrm{tol} = 10^{-6}$\n$\\mathrm{err} = \\frac{|1.0| \\cdot (0.1)^2 \\cdot (1.0)^2}{4} = \\frac{0.01}{4} = 0.0025$.\nThis is a normal number, well above $N_{\\min}$.\n-   **No-FTZ:** $s_{\\text{raw}} = \\sqrt{10^{-6} / 0.0025} = \\sqrt{4 \\times 10^{-4}} = 0.02$. Clamping gives $s_{\\text{no\\_ftz}} = \\max(0.1, 0.02) = 0.1$.\n-   **FTZ:** $\\mathrm{err}$ is normal, so it is not flushed to zero. No division by zero occurs. $\\text{ftz\\_div\\_zero} = \\mathrm{False}$.\n-   Result: $[0.1, \\mathrm{False}]$\n\n**Case 2:** $y_n = 2^{-1022}, h = 1.0, \\lambda = 1.0, \\mathrm{tol} = 10^{-12}$\n$\\mathrm{err} = \\frac{|2^{-1022}| \\cdot (1.0)^2 \\cdot (1.0)^2}{4} = \\frac{2^{-1022}}{4} = 2^{-1022} \\cdot 2^{-2} = 2^{-1024}$.\nThe value $2^{-1024}$ is less than $N_{\\min} = 2^{-1022}$ and greater than $0$, so it is a subnormal number.\n-   **No-FTZ:** $\\mathrm{err}$ is a very small positive number. $s_{\\text{raw}} = \\sqrt{10^{-12} / 2^{-1024}}$. Since the denominator is extremely small, $s_{\\text{raw}}$ will be a very large number. Clamping gives $s_{\\text{no\\_ftz}} = \\min(10.0, s_{\\text{raw}}) = 10.0$.\n-   **FTZ:** Since $\\mathrm{err}$ is subnormal, it is flushed to $0$. The computation of $s$ becomes $\\sqrt{10^{-12}/0}$, which is a division by zero. Thus, $\\text{ftz\\_div\\_zero} = \\mathrm{True}$.\n-   Result: $[10.0, \\mathrm{True}]$\n\n**Case 3:** $y_n = 10^{-320}, h = 1.0, \\lambda = 1.0, \\mathrm{tol} = 10^{-12}$\nNote that $10^{-320} \\approx (10^3)^{-106.67} \\approx (2^{10})^{-106.67} = 2^{-1066.7}$. This is less than $N_{\\min} \\approx 2.225 \\times 10^{-308}$, so $y_n$ is itself a subnormal number.\n$\\mathrm{err} = \\frac{|10^{-320}| \\cdot (1.0)^2 \\cdot (1.0)^2}{4} = 0.25 \\times 10^{-320}$.\nThis result is also deep in the subnormal range.\n-   **No-FTZ:** $\\mathrm{err}$ is a very small positive number. $s_{\\text{raw}} = \\sqrt{10^{-12} / (0.25 \\times 10^{-320})}$ will be a very large number. Clamping gives $s_{\\text{no\\_ftz}} = \\min(10.0, s_{\\text{raw}}) = 10.0$.\n-   **FTZ:** Since $\\mathrm{err}$ is subnormal, it is flushed to $0$. Division by zero occurs. $\\text{ftz\\_div\\_zero} = \\mathrm{True}$.\n-   Result: $[10.0, \\mathrm{True}]$\n\n**Case 4:** $y_n = 4 \\cdot 2^{-1022}, h = 1.0, \\lambda = 1.0, \\mathrm{tol} = 10^{-12}$\n$y_n = 2^2 \\cdot 2^{-1022} = 2^{-1020}$, which is a small normal number.\n$\\mathrm{err} = \\frac{|4 \\cdot 2^{-1022}| \\cdot (1.0)^2 \\cdot (1.0)^2}{4} = 2^{-1022}$.\nThe result for $\\mathrm{err}$ is exactly $N_{\\min}$, the smallest positive normal number.\n-   **No-FTZ:** $\\mathrm{err}$ is a small positive number. $s_{\\text{raw}} = \\sqrt{10^{-12} / 2^{-1022}}$ will be a very large number. Clamping gives $s_{\\text{no\\_ftz}} = \\min(10.0, s_{\\text{raw}}) = 10.0$.\n-   **FTZ:** The condition for flushing is $0  \\mathrm{err}  N_{\\min}$. Here, $\\mathrm{err} = N_{\\min}$, so the condition is not met. $\\mathrm{err}$ is not flushed to zero. No division by zero occurs. $\\text{ftz\\_div\\_zero} = \\mathrm{False}$.\n-   Result: $[10.0, \\mathrm{False}]$\n\nThe implementation will follow this logic to compute the results for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and analyzes an adaptive step-size scale factor for the forward Euler method,\n    focusing on the impact of IEEE-754 subnormal numbers and Flush-To-Zero (FTZ) behavior.\n    \"\"\"\n    \n    # Define constants based on the problem statement.\n    # Smallest positive normal number in binary64.\n    N_min = 2**-1022\n    s_min = 0.1\n    s_max = 10.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        (np.float64(1.0), np.float64(0.1), np.float64(1.0), np.float64(1e-6)),\n        # Case 2 (boundary into subnormal)\n        (np.float64(N_min), np.float64(1.0), np.float64(1.0), np.float64(1e-12)),\n        # Case 3 (extreme underflow)\n        (np.float64(1e-320), np.float64(1.0), np.float64(1.0), np.float64(1e-12)),\n        # Case 4 (threshold normal error)\n        (np.float64(4 * N_min), np.float64(1.0), np.float64(1.0), np.float64(1e-12)),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        y_n, h, lam, tol = case\n\n        # Compute the one-step approximation y1\n        y1 = y_n - h * lam * y_n\n\n        # Compute the two half-steps approximation y2\n        y_half = y_n - (h / 2.0) * lam * y_n\n        y2 = y_half - (h / 2.0) * lam * y_half\n\n        # Compute the local error estimate\n        err = np.abs(y2 - y1)\n\n        # --- Part 1: No-FTZ (standard behavior with subnormals) ---\n        \n        # Check if err is exactly zero to avoid division by zero.\n        # This would happen if y_n=0. For non-zero y_n, err is non-zero.\n        # If error is zero, the step is perfect; can increase to maximum.\n        if err == 0.0:\n            s_no_ftz = s_max\n        else:\n            # Calculate the raw scale factor\n            s_raw = np.sqrt(tol / err)\n            # Clamp the scale factor to the specified interval\n            s_no_ftz = max(s_min, min(s_max, s_raw))\n\n        # --- Part 2: FTZ (emulated behavior) ---\n\n        # Emulate FTZ: if err is subnormal, flush it to zero.\n        # A number x is subnormal if 0  |x|  N_min.\n        err_ftz = 0.0 if (0  err  N_min) else err\n\n        # Check if a division by zero would occur when computing the scale factor.\n        # This happens if the error used in the denominator is zero.\n        # tol is guaranteed to be non-zero in all test cases.\n        ftz_div_zero = (err_ftz == 0.0)\n        \n        results.append([s_no_ftz, ftz_div_zero])\n\n    # Final print statement in the exact required format.\n    # e.g., [[s1,b1],[s2,b2],...] with no spaces inside inner brackets and boolean lowercase.\n    output_parts = [f\"[{s},{str(b).lower()}]\" for s, b in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        }
    ]
}