{
    "hands_on_practices": [
        {
            "introduction": "This first practice delves into the heart of catastrophic cancellation by examining the simple subtraction $f(x_1, x_2) = x_1 - x_2$. By deriving the condition number from first principles, we will see precisely how subtracting two nearly equal numbers becomes an ill-conditioned problem. This exercise  provides a concrete, computational link between the abstract concepts of forward error, backward error, and the condition number that governs their relationship.",
            "id": "3132110",
            "problem": "You are given the scalar function $f:\\mathbb{R}^2\\to\\mathbb{R}$ defined by $f(\\mathbf{x})=x_1-x_2$, where $\\mathbf{x}=(x_1,x_2)$. Consider a simple floating-point model that first rounds each input to a fixed number of significant decimal digits and then performs subtraction exactly. Specifically, for a given positive integer $t$, the computed value is\n$$\\widehat{f}=\\operatorname{fl}_t(x_1)-\\operatorname{fl}_t(x_2),$$\nwhere $\\operatorname{fl}_t(\\cdot)$ denotes rounding to $t$ significant decimal digits using round-to-nearest with ties-to-even.\n\nYour tasks are:\n1. Starting only from core definitions of forward error, backward error, and conditioning, reason about how the subtraction $f(\\mathbf{x})=x_1-x_2$ behaves when $x_1\\approx x_2$. You must:\n   - Use the definition of relative forward error for an output $y=f(\\mathbf{x})$ and a computed output $\\widehat{y}$:\n     $$\\text{relative forward error}=\\frac{|\\widehat{y}-y|}{|y|},$$\n     provided $y\\neq 0$.\n   - Use a backward-error point of view under the rounding-first model: interpret $\\widehat{f}$ as the exact result $f(\\tilde{\\mathbf{x}})$ at some perturbed input $\\tilde{\\mathbf{x}}=(\\tilde{x}_1,\\tilde{x}_2)$, and take as the relative backward error in $\\mathbf{x}$ the quantity\n     $$\\eta=\\max\\!\\left(\\frac{|\\tilde{x}_1-x_1|}{|x_1|},\\frac{|\\tilde{x}_2-x_2|}{|x_2|}\\right),$$\n     where, in this model, a natural choice is $\\tilde{x}_i=\\operatorname{fl}_t(x_i)$.\n   - Starting from the foundational definition of a relative condition number at $\\mathbf{x}$,\n     $$\\kappa(\\mathbf{x})=\\lim_{\\rho\\to 0^+}\\ \\sup_{\\substack{\\Delta \\mathbf{x}\\neq \\mathbf{0}\\\\ \\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)\\le \\rho}}\\ \\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{\\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)},$$\n     derive an explicit formula for $\\kappa(\\mathbf{x})$ specialized to $f(\\mathbf{x})=x_1-x_2$, and explain why it becomes large when $x_1\\approx x_2$.\n\n2. Implement a program that, for each specified test case, computes:\n   - The exact value $f(\\mathbf{x})=x_1-x_2$.\n   - The computed value $\\widehat{f}=\\operatorname{fl}_t(x_1)-\\operatorname{fl}_t(x_2)$ with $\\operatorname{fl}_t(\\cdot)$ as defined above.\n   - The relative forward error $\\varepsilon=\\frac{|\\widehat{f}-f(\\mathbf{x})|}{|f(\\mathbf{x})|}$ (all answers must be reported as decimals, not percentages).\n   - The relative backward error in $\\mathbf{x}$ under the rounding-first model:\n     $$\\eta=\\max\\!\\left(\\frac{|\\operatorname{fl}_t(x_1)-x_1|}{|x_1|},\\frac{|\\operatorname{fl}_t(x_2)-x_2|}{|x_2|}\\right).$$\n   - The relative condition number $\\kappa(\\mathbf{x})$ obtained in Task 1 from first principles (implement your derived expression).\n   - The amplification factor $A=\\varepsilon/\\eta$, which quantifies how much the tiny relative input error (backward error) is amplified in the output (forward error).\n\n3. Use the following test suite of parameter values. Each test case is a triple $(x_1,x_2,t)$, where $t$ is the number of significant decimal digits used in rounding. None of these cases yields $f(\\mathbf{x})=0$, so the relative forward error is well-defined.\n   - Case 1 (well-conditioned): $(x_1,x_2,t)=\\left(12345.6789,\\ 2345.678901,\\ 7\\right)$.\n   - Case 2 (strong cancellation at small scale): $(x_1,x_2,t)=\\left(1.0000005,\\ 1.0000004,\\ 7\\right)$.\n   - Case 3 (strong cancellation at large scale): $(x_1,x_2,t)=\\left(100000003.0,\\ 100000000.0,\\ 7\\right)$.\n   - Case 4 (moderate cancellation): $(x_1,x_2,t)=\\left(0.123456789,\\ 0.123446789,\\ 7\\right)$.\n\n4. Final output specification:\n   - For each test case, output the list $[A,\\ \\varepsilon,\\ \\eta,\\ \\kappa(\\mathbf{x})]$ in that exact order.\n   - Aggregate all cases into a single line as a comma-separated list of these lists, enclosed in square brackets, for example:\n     $$\\left[\\ [A_1,\\varepsilon_1,\\eta_1,\\kappa_1],\\ [A_2,\\varepsilon_2,\\eta_2,\\kappa_2],\\ \\ldots\\ \\right].$$\n   - Print all floating-point numbers in scientific notation with exactly $10$ significant digits (for example, $1.234567890\\times 10^{-3}$ must be printed as $1.2345678900\\mathrm{e}{-03}$).\n\nScientific realism and constraints:\n- Treat all quantities as dimensionless real numbers.\n- Angles are not involved.\n- All error quantities must be decimals or fractions, not percentages.\n- Use the rounding-to-nearest with ties-to-even rule for $\\operatorname{fl}_t(\\cdot)$ in base ten.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, as specified above.",
            "solution": "The problem posed is a valid and classic exercise in introductory computational science, specifically concerning the forward and backward error analysis of a fundamental arithmetic operation. It is scientifically grounded, well-posed, objective, and complete. The task is to analyze the numerical stability of the subtraction $f(\\mathbf{x})=x_1-x_2$, particularly in the case of subtractive cancellation, where $x_1 \\approx x_2$.\n\nWe will first derive the necessary theoretical results from the provided definitions and then implement a program to compute the specified quantities for the given test cases.\n\n### Part 1: Theoretical Analysis from First Principles\n\n**1. Derivation of the Relative Condition Number $\\kappa(\\mathbf{x})$**\n\nThe problem provides the formal definition of the relative condition number of a function $f:\\mathbb{R}^2\\to\\mathbb{R}$ at a point $\\mathbf{x}=(x_1, x_2)$ with respect to relative perturbations in the input:\n$$\n\\kappa(\\mathbf{x})=\\lim_{\\rho\\to 0^+}\\ \\sup_{\\substack{\\Delta \\mathbf{x}\\neq \\mathbf{0}\\\\ \\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)\\le \\rho}}\\ \\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{\\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)}\n$$\nLet's specialize this definition for the function $f(\\mathbf{x}) = x_1 - x_2$. We assume $f(\\mathbf{x}) = x_1 - x_2 \\neq 0$, and $x_1, x_2 \\neq 0$.\n\nThe change in the function's output due to a perturbation $\\Delta\\mathbf{x}=(\\Delta x_1, \\Delta x_2)$ is:\n$$\nf(\\mathbf{x}+\\Delta \\mathbf{x}) - f(\\mathbf{x}) = ((x_1+\\Delta x_1) - (x_2+\\Delta x_2)) - (x_1 - x_2) = \\Delta x_1 - \\Delta x_2\n$$\nThe relative change in the output is therefore:\n$$\n\\frac{|f(\\mathbf{x}+\\Delta \\mathbf{x}) - f(\\mathbf{x})|}{|f(\\mathbf{x})|} = \\frac{|\\Delta x_1 - \\Delta x_2|}{|x_1 - x_2|}\n$$\nLet the maximum relative input perturbation be denoted by $E_{\\text{rel}} = \\max\\left(\\frac{|\\Delta x_1|}{|x_1|}, \\frac{|\\Delta x_2|}{|x_2|}\\right)$. This implies $|\\Delta x_1| \\le E_{\\text{rel}} |x_1|$ and $|\\Delta x_2| \\le E_{\\text{rel}} |x_2|$.\n\nWe can now express the ratio inside the supremum. Using the triangle inequality on the numerator:\n$$ |\\Delta x_1 - \\Delta x_2| \\le |\\Delta x_1| + |\\Delta x_2| $$\nSubstituting the bounds from the constraints gives:\n$$ |\\Delta x_1 - \\Delta x_2| \\le E_{\\text{rel}}|x_1| + E_{\\text{rel}}|x_2| = E_{\\text{rel}}(|x_1| + |x_2|) $$\nThis upper bound represents the supremum, as it can be attained by choosing appropriate perturbations (e.g., $\\Delta x_1 = E_{\\text{rel}} |x_1|$ and $\\Delta x_2 = -E_{\\text{rel}} |x_2|$, which satisfy the constraints $|\\Delta x_i|/|x_i| \\le E_{\\text{rel}}$).\n\nTherefore, the supremum of the ratio of relative output change to relative input change is:\n$$ \\sup \\frac{|\\Delta x_1 - \\Delta x_2|/|x_1 - x_2|}{E_{\\text{rel}}} = \\frac{E_{\\text{rel}}(|x_1| + |x_2|)/|x_1 - x_2|}{E_{\\text{rel}}} = \\frac{|x_1| + |x_2|}{|x_1 - x_2|} $$\nSince this expression does not depend on the size of the perturbation $\\rho$ (or $E_{\\text{rel}}$), the limit as $\\rho \\to 0^+$ is trivial. Thus, the derived formula is:\n$$\n\\kappa(\\mathbf{x}) = \\frac{|x_1| + |x_2|}{|x_1 - x_2|}\n$$\n\n**2. Behavior of $\\kappa(\\mathbf{x})$ for $x_1 \\approx x_2$**\n\nWhen $x_1$ and $x_2$ are close in value ($x_1 \\approx x_2$), the denominator $|x_1 - x_2|$ approaches $0$. Assuming $x_1$ and $x_2$ are not close to $0$ and have the same sign (the typical scenario for catastrophic cancellation), the numerator $|x_1| + |x_2|$ is approximately $2|x_1|$. The ratio $\\kappa(\\mathbf{x})$ thus becomes very large. A large condition number signifies that the problem is \"ill-conditioned,\" meaning that small relative errors in the input can be magnified into large relative errors in the output.\n\n**3. Relationship between Forward Error, Backward Error, and Conditioning**\n\nThe problem defines a \"rounding-first\" computational model where $\\widehat{f} = \\operatorname{fl}_t(x_1) - \\operatorname{fl}_t(x_2)$. This can be viewed from a backward error perspective by defining a perturbed input $\\tilde{\\mathbf{x}} = (\\tilde{x}_1, \\tilde{x}_2) = (\\operatorname{fl}_t(x_1), \\operatorname{fl}_t(x_2))$. The computed result is then the exact result for this perturbed input: $\\widehat{f} = f(\\tilde{\\mathbf{x}})$.\n\nThe relative backward error $\\eta$ is the size of the relative perturbation to the input:\n$$\n\\eta = \\max\\left(\\frac{|\\tilde{x}_1-x_1|}{|x_1|},\\frac{|\\tilde{x}_2-x_2|}{|x_2|}\\right)\n$$\nThe relative forward error $\\varepsilon$ is the resulting relative perturbation in the output:\n$$\n\\varepsilon = \\frac{|\\widehat{f}-f(\\mathbf{x})|}{|f(\\mathbf{x})|} = \\frac{|f(\\tilde{\\mathbf{x}})-f(\\mathbf{x})|}{|f(\\mathbf{x})|}\n$$\nThe condition number $\\kappa(\\mathbf{x})$ links these two quantities. For small perturbations, the relationship is approximately:\n$$\n\\varepsilon \\approx \\kappa(\\mathbf{x}) \\cdot \\eta \\quad \\text{or more formally} \\quad \\varepsilon \\le \\kappa(\\mathbf{x}) \\cdot \\eta + O(\\eta^2)\n$$\nThe amplification factor $A = \\varepsilon / \\eta$ is an empirical quantification of this relationship for the specific perturbation introduced by rounding. Since the condition number is defined as the supremum over all possible small perturbations, we have the inequality $A \\le \\kappa(\\mathbf{x})$.\n\nWhen $x_1 \\approx x_2$, $\\kappa(\\mathbf{x})$ is large. The rounding process introduces a small backward error $\\eta$, typically on the order of machine precision. However, this small input error is amplified by the large condition number, leading to a potentially large forward error $\\varepsilon$. This phenomenon is known as **catastrophic cancellation**: the subtraction of nearly equal quantities causes a loss of significant digits in the result, not because subtraction itself is inaccurate, but because it magnifies the pre-existing rounding errors in the inputs.\n\n**A Note on Output Formatting**\nThe problem statement requests \"exactly 10 significant digits\" but provides an example formatting of the number $1.234567890 \\times 10^{-3}$ as `1.2345678900e-03`. The example output has 11 significant digits (1 before the decimal, 10 after). This presents a minor ambiguity. We will adhere to the explicit example, as it is more specific than the general description. This corresponds to the Python format specifier `\"{:.10e}\"`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fl_t(x: float, t: int) -> float:\n    \"\"\"\n    Rounds a floating-point number x to t significant decimal digits.\n    Uses round-to-nearest, ties-to-even rule.\n    \"\"\"\n    if x == 0.0:\n        return 0.0\n\n    # Using np.float64 for calculations to maintain precision.\n    x = np.float64(x)\n\n    # Calculate the base-10 exponent of the number to find its magnitude.\n    exponent = np.floor(np.log10(np.abs(x)))\n    \n    # Calculate the power of 10 to scale the number.\n    # To round to t sig-figs, we round x / 10^(e-t+1) to an integer.\n    scale_power = exponent - t + 1\n    \n    # Scale the number. For example, to round 12345.67 to 3 sig figs,\n    # exponent=4, t=3. scale_power = 4-3+1 = 2.\n    # scaled_val = 12345.67 / 100 = 123.4567\n    scaled_val = x / (10.0**scale_power)\n    \n    # Python 3's round() and np.round() both implement round-half-to-even.\n    rounded_scaled_val = np.round(scaled_val)\n    \n    # Scale back to the original magnitude.\n    # result = 123.0 * 100 = 12300.0\n    result = rounded_scaled_val * (10.0**scale_power)\n    \n    return float(result)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-conditioned)\n        (12345.6789, 2345.678901, 7),\n        # Case 2 (strong cancellation at small scale)\n        (1.0000005, 1.0000004, 7),\n        # Case 3 (strong cancellation at large scale)\n        (100000003.0, 100000000.0, 7),\n        # Case 4 (moderate cancellation)\n        (0.123456789, 0.123446789, 7)\n    ]\n\n    all_results = []\n\n    for x1_in, x2_in, t in test_cases:\n        # Use high-precision numpy floats for internal calculations\n        x1 = np.float64(x1_in)\n        x2 = np.float64(x2_in)\n\n        # 1. Exact value\n        f_exact = x1 - x2\n\n        # 2. Rounded inputs and computed value\n        x1_rounded = fl_t(x1, t)\n        x2_rounded = fl_t(x2, t)\n        f_computed = x1_rounded - x2_rounded\n\n        # 3. Relative forward error\n        # Problem statement guarantees f_exact is non-zero\n        rel_forward_error = np.abs(f_computed - f_exact) / np.abs(f_exact)\n\n        # 4. Relative backward error\n        # Problem statement inputs are non-zero.\n        rel_err_x1 = np.abs(x1_rounded - x1) / np.abs(x1)\n        rel_err_x2 = np.abs(x2_rounded - x2) / np.abs(x2)\n        rel_backward_error = np.max([rel_err_x1, rel_err_x2])\n\n        # 5. Relative condition number\n        condition_number = (np.abs(x1) + np.abs(x2)) / np.abs(f_exact)\n\n        # 6. Amplification factor\n        # If backward error is zero, forward error must also be zero.\n        # This implies exact computation, so amplification is not straightforward.\n        # For the given test cases, rel_backward_error will not be zero.\n        if rel_backward_error == 0.0:\n            # This case means the inputs didn't need rounding, so perturbation is 0.\n            # Forward error is also 0. The ratio 0/0 is indeterminate.\n            # A reasonable interpretation is that the amplification is bounded by kappa.\n            # We can also consider it 1.0, as there's no error to amplify.\n            # Let's assign it kappa, as it's the theoretical limit.\n            # However, for the given tests this branch is not taken.\n            amplification_factor = condition_number\n        else:\n            amplification_factor = rel_forward_error / rel_backward_error\n\n        # Store results for this case\n        all_results.append([amplification_factor, rel_forward_error, rel_backward_error, condition_number])\n\n    # Format the output as specified in the problem statement.\n    # The example \"1.2345678900e-03\" has 11 significant digits (1 before decimal, 10 after).\n    # This corresponds to the format specifier \"{:.10e}\".\n    def format_list(data_list):\n        return [f\"{val:.10e}\" for val in data_list]\n\n    formatted_cases = []\n    for result_case in all_results:\n        formatted_list_str =\",\".join(format_list(result_case))\n        formatted_cases.append(f\"[{formatted_list_str}]\")\n        \n    final_output_str = f\"[{','.join(formatted_cases)}]\"\n\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the foundational understanding of cancellation, this practice moves from analysis to action. We will investigate the function $f(x) = \\sqrt{1+x} - 1$, which is prone to numerical instability for small $x$, and then engineer a mathematically equivalent, stable algorithm to compute it accurately. Through this hands-on coding exercise , you will learn to diagnose numerical issues and apply algebraic reformulation, a critical skill for any computational scientist.",
            "id": "3232048",
            "problem": "Design and implement a program that performs a principled forward error and backward error study for computing the function $f(x)=\\sqrt{1+x}-1$ when $x$ is small in magnitude. Begin from the core definitions of forward error and backward error in floating-point computation, and from the standard, well-tested floating-point model: for any basic arithmetic operation or elementary function evaluation, the computed result satisfies $\\mathrm{fl}(a\\ \\mathrm{op}\\ b) = (a\\ \\mathrm{op}\\ b)(1+\\varepsilon)$ with $|\\varepsilon|\\le u$, where $u$ is the unit roundoff of the format, and $\\mathrm{fl}(g(a)) = g(a)(1+\\varepsilon)$ with $|\\varepsilon|\\le u$ for a well-conditioned elementary function $g$. Your derivation and algorithm design must rely on these principles.\n\nTasks:\n- Construct two algorithms for evaluating $f(x)$ in binary $64$-bit floating point arithmetic (that is, $u \\approx 2^{-53}$): \n  - Algorithm A: the direct formula that exhibits subtractive cancellation for small $x$.\n  - Algorithm B: a mathematically equivalent, numerically stable form derived by algebraic manipulation that avoids catastrophic cancellation.\n- For each algorithm, define the forward error for an input $x$ as $| \\widehat{f}(x) - f(x) |$, where $\\widehat{f}(x)$ is the floating-point result of the algorithm and $f(x)$ is the mathematically exact value. Use high-precision arithmetic to approximate $f(x)$ to serve as a ground truth surrogate.\n- For each algorithm, define the backward error $\\delta$ by solving $\\widehat{f}(x) = f(x+\\delta)$ exactly for $\\delta$ in terms of $\\widehat{f}(x)$ and $x$. Report the magnitude $|\\delta|$ as the backward error. Your program must compute this $\\delta$ numerically with high-precision arithmetic to avoid spurious rounding in the diagnostic itself.\n- Use the following test suite of inputs:\n  - $x = 10^{-8}$,\n  - $x = 10^{-16}$,\n  - $x = -10^{-8}$,\n  - $x = 0$,\n  - $x = -1 + 10^{-15}$.\n- For each $x$ in the test suite, output a list of four floating-point numbers in the following order: \n  - the absolute forward error for Algorithm A, \n  - the absolute forward error for Algorithm B, \n  - the absolute backward error $|\\delta|$ for Algorithm A, \n  - the absolute backward error $|\\delta|$ for Algorithm B.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the four-number list for a corresponding test input, in the same order as listed above.\n\nNotes and constraints:\n- Frame all derivations using only the core definitions and the floating-point model given above. Do not use or quote any specialized, pre-derived error bounds for this specific expression in the problem statement.\n- Use high-precision arithmetic internally to approximate exact values for computing the forward and backward errors; ensure that the surrogate exactness dominates the floating-point noise of binary $64$-bit arithmetic used by the two algorithms.\n- There are no physical units involved; all quantities are dimensionless real numbers.\n- Angles are not involved in this task.\n- All reported numbers must be standard decimal floating-point values.",
            "solution": "The problem as stated is valid: it is scientifically grounded in the principles of numerical analysis, well-posed with a clear objective and sufficient data, and free from subjective or ambiguous language. We may therefore proceed with the derivation and implementation.\n\nThe task is to perform a forward and backward error analysis for computing the function $f(x) = \\sqrt{1+x} - 1$ for values of $x$ small in magnitude, and also for $x$ near $-1$. We will design two algorithms, analyze their behavior based on the standard model of floating-point arithmetic, and implement a program to compute and report the errors for a given set of test inputs.\n\nLet the floating-point model be defined as follows: for any binary operation $\\mathrm{op}$ and for any elementary function $g$, the computed results satisfy $\\mathrm{fl}(a\\ \\mathrm{op}\\ b) = (a\\ \\mathrm{op}\\ b)(1+\\varepsilon_1)$ and $\\mathrm{fl}(g(a)) = g(a)(1+\\varepsilon_2)$, where $|\\varepsilon_i| \\le u$ for the unit roundoff $u$. For the specified binary $64$-bit arithmetic, $u = 2^{-53}$.\n\n**Algorithm Design**\n\n**Algorithm A (Direct Method):**\nThis algorithm computes $f(x)$ using the formula as given:\n$f_A(x) = \\sqrt{1+x} - 1$.\nFor an input $x$ where $|x|$ is small, $1+x$ is a value very close to $1$. The square root $\\sqrt{1+x}$ is therefore also very close to $1$. The final step involves the subtraction of two nearly identical numbers, a phenomenon known as catastrophic cancellation. This operation results in a loss of significant digits. The computed result $\\widehat{f_A}(x)$ will have a large relative error, and the absolute error will be on the order of the unit roundoff $u$, regardless of the magnitude of the true value $f(x) \\approx x/2$. This indicates numerical instability.\n\n**Algorithm B (Stable Method):**\nTo avert catastrophic cancellation, we can reformulate the expression for $f(x)$ by multiplying and dividing by the conjugate of the numerator, $\\sqrt{1+x}+1$:\n$$ f(x) = (\\sqrt{1+x} - 1) \\times \\frac{\\sqrt{1+x} + 1}{\\sqrt{1+x} + 1} = \\frac{(1+x) - 1^2}{\\sqrt{1+x} + 1} = \\frac{x}{\\sqrt{1+x} + 1} $$\nThis leads to Algorithm B:\n$f_B(x) = \\frac{x}{\\sqrt{1+x} + 1}$.\nWhen $|x|$ is small, the denominator $\\sqrt{1+x}+1$ is close to $2$. The operations involved are addition to $1$, a square root, addition of $1$, and a division by a number far from zero. None of these operations are numerically sensitive in this context. The computation avoids the subtraction of nearly equal quantities, and is therefore expected to be numerically stable. The relative error of the computed result $\\widehat{f_B}(x)$ should be a small multiple of the unit roundoff $u$.\n\n**Error Analysis Framework**\n\n**Forward Error:**\nThe absolute forward error is defined as the magnitude of the difference between the computed value $\\widehat{f}(x)$ and the true mathematical value $f(x)$:\n$$ E_f = |\\widehat{f}(x) - f(x)| $$\nTo compute this quantity for our analysis, $f(x)$ must be calculated with a much higher precision than the binary $64$-bit arithmetic being studied. This serves as a \"ground truth\" surrogate.\n\n**Backward Error:**\nThe backward error is a measure of the smallest perturbation $\\delta$ to the input $x$ that would make the computed result $\\widehat{f}(x)$ the exact result for the perturbed input. We must find $\\delta$ such that:\n$$ \\widehat{f}(x) = f(x+\\delta) = \\sqrt{1 + (x+\\delta)} - 1 $$\nWe can solve this equation for $\\delta$. Let $y = \\widehat{f}(x)$:\n$$ y + 1 = \\sqrt{1 + x + \\delta} $$\n$$ (y + 1)^2 = 1 + x + \\delta $$\n$$ \\delta = (y + 1)^2 - 1 - x $$\nThe absolute backward error is $|\\delta|$. This formula will be used to compute $\\delta$ numerically, again using high-precision arithmetic to ensure the diagnostic calculation itself is not a source of error.\n\nA numerically stable algorithm for a well-conditioned problem will have both a small forward error and a small backward error. For an ill-conditioned problem, a stable algorithm is characterized by a small backward error, but may still exhibit a large forward error because the problem intrinsically amplifies input perturbations.\n\n**Analysis of Test Cases**\n\n1.  For $x = 10^{-8}$ and $x = -10^{-8}$:\n    Here, $|x|$ is small. Algorithm A will suffer from catastrophic cancellation, leading to a large forward error (absolute error $\\approx u \\approx 10^{-16}$) and a large backward error relative to $x$. Algorithm B is stable and will produce small forward and backward errors.\n\n2.  For $x = 10^{-16}$:\n    This value of $x$ is smaller than the unit roundoff for binary $64$-bit arithmetic ($u = 2^{-53} \\approx 1.11 \\times 10^{-16}$). Thus, in floating-point arithmetic, the operation $1.0 + x$ rounds to $1.0$.\n    For Algorithm A, $\\widehat{f_A}(x) = \\mathrm{fl}(\\sqrt{1.0} - 1.0) = 0$. The true value is $f(10^{-16}) \\approx 0.5 \\times 10^{-16}$. The forward error is $|0 - f(10^{-16})| \\approx 0.5 \\times 10^{-16}$. The backward error $\\delta$ comes from solving $f(10^{-16}+\\delta)=0$, which implies $\\delta = -10^{-16}$.\n    For Algorithm B, $\\widehat{f_B}(x) = \\mathrm{fl}(x / (\\sqrt{1.0} + 1.0)) = \\mathrm{fl}(x/2)$. The result will be very close to the true value $f(x) \\approx x/2$, so both forward and backward errors will be extremely small.\n\n3.  For $x = 0$:\n    Both algorithms will compute exactly $0$, and the true value is $f(0)=0$. All error metrics will be $0$.\n\n4.  For $x = -1 + 10^{-15}$:\n    Here $x$ is close to $-1$, not $0$. The condition number of the function $f(x)$ is $C_f(x) = \\left| \\frac{x f'(x)}{f(x)} \\right| = \\left| \\frac{\\sqrt{1+x}+1}{2\\sqrt{1+x}} \\right|$. As $x \\to -1^+$, $1+x \\to 0^+$ and $C_f(x) \\to \\infty$. The problem is highly ill-conditioned.\n    The subtractive cancellation issue of Algorithm A does not occur here, as $\\sqrt{1+x} = \\sqrt{10^{-15}} \\approx 3.16 \\times 10^{-8}$, which is not close to $1$. Both Algorithm A and Algorithm B are stable for this input.\n    Therefore, both algorithms should exhibit small backward errors, on the order of $u \\approx 10^{-16}$. However, due to the extreme ill-conditioning, this small backward error (and the initial input representation error) will be greatly amplified, resulting in a large forward error for both algorithms. The forward error magnitude will be approximately $C_f(x) \\times |x| \\times u$.\n\nThe implementation will follow these principles, using high-precision arithmetic for ground truth and error calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Performs a forward and backward error study for two algorithms computing\n    f(x) = sqrt(1+x) - 1.\n    \"\"\"\n    # Set a high precision for the decimal module to serve as \"exact\" arithmetic.\n    decimal.setcontext(decimal.Context(prec=100))\n\n    # Define the test cases. Using strings preserves precision for Decimal.\n    # The case '-1+1e-15' requires special construction.\n    test_case_defs = ['1e-8', '1e-16', '-1e-8', '0', ('-1', '1e-15')]\n\n    def get_decimal_from_def(case_def):\n        if isinstance(case_def, str):\n            return decimal.Decimal(case_def)\n        elif isinstance(case_def, tuple):\n            return decimal.Decimal(case_def[0]) + decimal.Decimal(case_def[1])\n\n    test_cases = [get_decimal_from_def(d) for d in test_case_defs]\n\n    all_results = []\n\n    one_dec = decimal.Decimal('1')\n\n    for x_dec in test_cases:\n        # Step 1: Compute the \"ground truth\" value using high-precision arithmetic.\n        # The function is defined for x >= -1.\n        if x_dec < -one_dec:\n            # This path is not taken by the problem's test suite.\n            continue\n        \n        # Exact mathematical value f(x)\n        f_exact = (one_dec + x_dec).sqrt() - one_dec\n\n        # Step 2: Convert input to standard float64 for algorithmic computation.\n        x_float = np.float64(x_dec)\n\n        # Step 3: Execute Algorithm A (direct, unstable for x approx 0).\n        # f_hat_A = sqrt(1+x) - 1\n        f_hat_A = np.sqrt(np.float64(1.0) + x_float) - np.float64(1.0)\n\n        # Step 4: Execute Algorithm B (re-rationalized, stable for x approx 0).\n        # f_hat_B = x / (sqrt(1+x) + 1)\n        if x_float == 0.0:\n            f_hat_B = np.float64(0.0)\n        else:\n            denominator = np.sqrt(np.float64(1.0) + x_float) + np.float64(1.0)\n            f_hat_B = x_float / denominator\n        \n        # Step 5: Compute forward errors. Convert float results to Decimal for\n        # high-precision subtraction from the exact value.\n        f_hat_A_dec = decimal.Decimal(f_hat_A)\n        f_hat_B_dec = decimal.Decimal(f_hat_B)\n        \n        forward_error_A = abs(f_hat_A_dec - f_exact)\n        forward_error_B = abs(f_hat_B_dec - f_exact)\n\n        # Step 6: Compute backward errors using the derived formula:\n        # delta = (f_hat(x) + 1)^2 - 1 - x\n        # All calculations are done in high precision.\n        delta_A = (f_hat_A_dec + one_dec)**2 - one_dec - x_dec\n        backward_error_A = abs(delta_A)\n\n        delta_B = (f_hat_B_dec + one_dec)**2 - one_dec - x_dec\n        backward_error_B = abs(delta_B)\n\n        # Append the four required error metrics for this test case.\n        # Convert final Decimal error values to float for output.\n        all_results.append([\n            float(forward_error_A),\n            float(forward_error_B),\n            float(backward_error_A),\n            float(backward_error_B)\n        ])\n\n    # Step 7: Format the final output string exactly as required.\n    # The output should be a string representing a list of lists,\n    # with no spaces inside the inner lists.\n    inner_lists_str = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice deepens the theoretical understanding gained from the previous numerical explorations. Here, we perform a rigorous, first-principles error analysis for the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$ when $x$ is large, a scenario that also leads to catastrophic cancellation. By deriving asymptotic expressions for both forward and backward error , you will develop the analytical skills needed to predict and quantify the numerical behavior of algorithms without running a computer.",
            "id": "3231896",
            "problem": "Let $f(x) = \\sqrt{x+1} - \\sqrt{x}$ and suppose it is evaluated on a machine conforming to the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) in round-to-nearest mode with unit roundoff $u$. Assume the standard first-order rounding model for each elementary operation and square root: for any operation $\\circ \\in \\{+, -, \\times, \\div\\}$ and any $a,b$ in the floating-point range, $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\le u$, and $\\operatorname{fl}(\\sqrt{y}) = \\sqrt{y}(1 + \\delta)$ with $|\\delta| \\le u$. Consider the naive computation of $f(x)$ by evaluating $s = \\sqrt{x}$, $t = \\sqrt{x+1}$, and then forming the floating-point subtraction $\\widehat{f} = \\operatorname{fl}(t - s)$.\n\nUsing only the elementary identity $(a - b)(a + b) = a^{2} - b^{2}$ and the above rounding model as the foundational base, analyze the catastrophic cancellation in this naive evaluation for large $x$. Derive, to first order in $u$, a leading-order worst-case asymptotic expression as $x \\to \\infty$ for:\n1) the relative forward error $\\varepsilon_{f} = (\\widehat{f} - f)/f$, and\n2) a backward error interpretation in which there exists a perturbation $\\Delta$ such that $\\widehat{f} = f(x + \\Delta)$ to first order; determine the leading-order worst-case asymptotic magnitude of the smallest such $|\\Delta|$ as $x \\to \\infty$.\n\nReport your final result as a two-entry row matrix $\\begin{pmatrix}E & B\\end{pmatrix}$, where $E$ is the leading-order worst-case asymptotic expression for the relative forward error and $B$ is the leading-order worst-case asymptotic expression for the absolute backward error $|\\Delta|$, both expressed in terms of $x$ and $u$. The final answer must be a single analytic expression; do not include units. No rounding is required.",
            "solution": "Let the computed values be denoted with a hat. The computational steps are modeled as follows, assuming $x$ is an exact floating-point number and the addition $x+1$ is also exact.\nThe first computed value is $\\widehat{s}$:\n$$ \\widehat{s} = \\operatorname{fl}(\\sqrt{x}) = \\sqrt{x}(1 + \\delta_1), \\quad |\\delta_1| \\le u $$\nThe second computed value is $\\widehat{t}$:\n$$ \\widehat{t} = \\operatorname{fl}(\\sqrt{x+1}) = \\sqrt{x+1}(1 + \\delta_2), \\quad |\\delta_2| \\le u $$\nThe final result $\\widehat{f}$ is the floating-point subtraction of $\\widehat{s}$ from $\\widehat{t}$:\n$$ \\widehat{f} = \\operatorname{fl}(\\widehat{t} - \\widehat{s}) = (\\widehat{t} - \\widehat{s})(1 + \\delta_3), \\quad |\\delta_3| \\le u $$\n\nSubstituting the expressions for $\\widehat{s}$ and $\\widehat{t}$ into the equation for $\\widehat{f}$:\n$$ \\widehat{f} = (\\sqrt{x+1}(1 + \\delta_2) - \\sqrt{x}(1 + \\delta_1))(1 + \\delta_3) $$\nExpanding this expression and retaining only terms up to the first order in the error terms $\\delta_i$ (i.e., neglecting terms like $\\delta_i \\delta_j$ which are of order $u^2$):\n$$ \\widehat{f} \\approx (\\sqrt{x+1} - \\sqrt{x}) + \\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x} + \\delta_3(\\sqrt{x+1} - \\sqrt{x}) $$\nRecognizing that $f(x) = \\sqrt{x+1} - \\sqrt{x}$, the absolute error is:\n$$ \\widehat{f} - f(x) \\approx \\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x} + \\delta_3 f(x) $$\n\n**1. Relative Forward Error Analysis**\n\nThe relative forward error is $\\varepsilon_f = (\\widehat{f} - f(x))/f(x)$. Using the absolute error expression:\n$$ \\varepsilon_f \\approx \\frac{\\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x}}{f(x)} + \\delta_3 $$\nThis expression reveals catastrophic cancellation. The term $f(x)$ is the difference of two nearly equal large numbers for large $x$, so its value is small, amplifying the error from the numerator. To analyze this, we use the identity suggested by the problem, $(a-b)(a+b)=a^2-b^2$, which allows us to rewrite $f(x)$:\n$$ f(x) = \\sqrt{x+1} - \\sqrt{x} = \\frac{(\\sqrt{x+1} - \\sqrt{x})(\\sqrt{x+1} + \\sqrt{x})}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{(x+1) - x}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}} $$\nSubstituting this into the expression for $\\varepsilon_f$:\n$$ \\varepsilon_f \\approx (\\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x})(\\sqrt{x+1} + \\sqrt{x}) + \\delta_3 $$\nExpanding the product:\n$$ \\varepsilon_f \\approx \\delta_2(x+1) + \\delta_2\\sqrt{x(x+1)} - \\delta_1\\sqrt{x(x+1)} - \\delta_1 x + \\delta_3 $$\n$$ \\varepsilon_f \\approx x(\\delta_2 - \\delta_1) + \\delta_2 + (\\delta_2 - \\delta_1)\\sqrt{x^2+x} + \\delta_3 $$\nFor large $x$, we use the asymptotic expansion $\\sqrt{x^2+x} = x\\sqrt{1+1/x} \\approx x(1 + \\frac{1}{2x}) = x + \\frac{1}{2}$.\n$$ \\varepsilon_f \\approx x(\\delta_2 - \\delta_1) + \\delta_2 + (\\delta_2 - \\delta_1)(x + \\frac{1}{2}) + \\delta_3 $$\n$$ \\varepsilon_f \\approx x\\delta_2 - x\\delta_1 + \\delta_2 + x\\delta_2 - x\\delta_1 + \\frac{1}{2}\\delta_2 - \\frac{1}{2}\\delta_1 + \\delta_3 $$\n$$ \\varepsilon_f \\approx 2x(\\delta_2 - \\delta_1) + \\frac{3}{2}\\delta_2 - \\frac{1}{2}\\delta_1 + \\delta_3 $$\nAs $x \\to \\infty$, the dominant term is $2x(\\delta_2 - \\delta_1)$. To find the worst-case magnitude, we must maximize $|\\delta_2 - \\delta_1|$. Since $|\\delta_1| \\le u$ and $|\\delta_2| \\le u$, the maximum value of $|\\delta_2 - \\delta_1|$ is $2u$, which occurs when $\\delta_1$ and $\\delta_2$ have opposite signs and maximum magnitude (e.g., $\\delta_2=u$ and $\\delta_1=-u$).\nThe leading-order worst-case asymptotic expression for the relative forward error is thus:\n$$ |\\varepsilon_f| \\approx |2x(2u)| = 4xu $$\nThe first requested expression is $E = 4xu$.\n\n**2. Absolute Backward Error Analysis**\n\nBackward error analysis seeks to find a perturbation $\\Delta$ on the input $x$ such that the computed result $\\widehat{f}$ is the exact result for the perturbed input, i.e., $\\widehat{f} = f(x+\\Delta)$.\nUsing a first-order Taylor expansion of $f(x+\\Delta)$ around $x$:\n$$ f(x+\\Delta) \\approx f(x) + \\Delta f'(x) $$\nEquating this to the computed value gives $\\widehat{f} \\approx f(x) + \\Delta f'(x)$.\nThis implies that the absolute error is $\\widehat{f} - f(x) \\approx \\Delta f'(x)$.\nWe can solve for $\\Delta$:\n$$ \\Delta \\approx \\frac{\\widehat{f} - f(x)}{f'(x)} $$\nWe already have the expression for the absolute error: $\\widehat{f} - f(x) \\approx \\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x} + \\delta_3 f(x)$.\nNext, we find the derivative of $f(x)$:\n$$ f'(x) = \\frac{d}{dx}(\\sqrt{x+1} - \\sqrt{x}) = \\frac{1}{2\\sqrt{x+1}} - \\frac{1}{2\\sqrt{x}} = \\frac{\\sqrt{x} - \\sqrt{x+1}}{2\\sqrt{x(x+1)}} $$\nNow we find the asymptotic behavior for $x \\to \\infty$ for both the numerator and the denominator of the expression for $\\Delta$.\n\nFor the numerator, the absolute error $\\widehat{f} - f(x)$: As $x \\to \\infty$, $\\sqrt{x+1}\\approx\\sqrt{x}$ and $f(x) \\to 0$. The dominant term in the absolute error is $(\\delta_2-\\delta_1)\\sqrt{x}$.\n$$ \\widehat{f} - f(x) \\approx (\\delta_2\\sqrt{x}(1+\\frac{1}{2x}...) - \\delta_1\\sqrt{x}) + O(u/{\\sqrt{x}}) \\approx (\\delta_2-\\delta_1)\\sqrt{x} $$\n\nFor the denominator, $f'(x)$: As $x \\to \\infty$, the numerator $\\sqrt{x} - \\sqrt{x+1} = -f(x) \\approx -\\frac{1}{2\\sqrt{x}}$. The denominator $2\\sqrt{x(x+1)} \\approx 2x$.\n$$ f'(x) \\approx \\frac{-1/(2\\sqrt{x})}{2x} = -\\frac{1}{4x^{3/2}} $$\nCombining these to find the asymptotic expression for $\\Delta$:\n$$ \\Delta \\approx \\frac{(\\delta_2-\\delta_1)\\sqrt{x}}{-1/(4x^{3/2})} = -4x^2(\\delta_2-\\delta_1) $$\nTo find the worst-case magnitude of $|\\Delta|$, we again maximize $|\\delta_2 - \\delta_1|$, which is $2u$.\nThe leading-order worst-case asymptotic magnitude of the backward error is:\n$$ |\\Delta| \\approx |-4x^2(2u)| = 8x^2 u $$\nThe second requested expression is $B = 8x^2u$.\n\nThe final result is the two-entry row matrix containing the expression for the worst-case relative forward error and the worst-case absolute backward error magnitude.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4xu & 8x^2u\n\\end{pmatrix}\n}\n$$"
        }
    ]
}