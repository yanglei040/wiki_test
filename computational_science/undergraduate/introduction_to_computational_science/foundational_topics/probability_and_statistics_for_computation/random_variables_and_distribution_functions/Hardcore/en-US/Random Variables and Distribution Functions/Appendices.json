{
    "hands_on_practices": [
        {
            "introduction": "A common task in science is to understand the distribution of a quantity that is a function of another, more fundamental random variable. This exercise provides practice in the foundational \"method of distributions\" for deriving the cumulative distribution function (CDF) of a transformed variable . Mastering this technique is crucial for building and analyzing probabilistic models, especially when dealing with non-monotonic transformations that require careful application of first principles.",
            "id": "1416753",
            "problem": "Let $X$ be a continuous random variable whose cumulative distribution function (CDF), $F_X(x)$, is given by the following piecewise expression:\n$$\nF_X(x) =\n\\begin{cases}\n0  \\text{for } x \\le -1 \\\\\n\\frac{1}{2}(x+1)^2  \\text{for } -1  x \\le 0 \\\\\n1 - \\frac{1}{2}(1-x)^2  \\text{for } 0  x \\le 1 \\\\\n1  \\text{for } x  1\n\\end{cases}\n$$\nA new random variable $Y$ is defined by the transformation $Y = X^2$. Find the cumulative distribution function, $F_Y(y)$, for the random variable $Y$. The answer should be presented as a single, closed-form analytic expression for $F_Y(y)$, which may be defined piecewise.",
            "solution": "We are given a continuous random variable $X$ with support in $[-1,1]$ and cumulative distribution function\n$$\nF_{X}(x)=\n\\begin{cases}\n0  x \\le -1 \\\\\n\\frac{1}{2}(x+1)^{2}  -1  x \\le 0 \\\\\n1 - \\frac{1}{2}(1-x)^{2}  0  x \\le 1 \\\\\n1  x  1\n\\end{cases}\n$$\nDefine $Y=X^{2}$. The support of $Y$ is $[0,1]$ because $|X| \\le 1$. For any $y \\in \\mathbb{R}$, the cumulative distribution function of $Y$ is\n$$\nF_{Y}(y)=\\mathbb{P}(Y \\le y)=\\mathbb{P}(X^{2} \\le y).\n$$\nIf $y \\le 0$, then $X^{2} \\le y$ cannot occur for a continuous $X$ except at a single point of probability zero, so\n$$\nF_{Y}(y)=0 \\quad \\text{for } y \\le 0.\n$$\nIf $y \\ge 1$, then $X^{2} \\le y$ holds surely because $X \\in [-1,1]$, so\n$$\nF_{Y}(y)=1 \\quad \\text{for } y \\ge 1.\n$$\nFor $0y1$, write $a=\\sqrt{y} \\in (0,1)$. Then\n$$\nF_{Y}(y)=\\mathbb{P}(-a \\le X \\le a)=F_{X}(a)-F_{X}(-a).\n$$\nUsing the given $F_{X}$ on the appropriate intervals,\n$$\nF_{X}(a)=1-\\frac{1}{2}(1-a)^{2}, \\qquad F_{X}(-a)=\\frac{1}{2}(1-a)^{2},\n$$\nso\n$$\nF_{Y}(y)=\\left[1-\\frac{1}{2}(1-a)^{2}\\right]-\\left[\\frac{1}{2}(1-a)^{2}\\right]=1-(1-a)^{2}=2a-a^{2}.\n$$\nSubstituting $a=\\sqrt{y}$ gives\n$$\nF_{Y}(y)=2\\sqrt{y}-y \\quad \\text{for } 0y1.\n$$\nThis expression extends continuously to $y=0$ and $y=1$, yielding the final piecewise CDF:\n$$\nF_{Y}(y)=\n\\begin{cases}\n0  y \\le 0 \\\\\n2\\sqrt{y}-y  0 y \\le 1 \\\\\n1  y  1\n\\end{cases}\n$$\nwhich is right-continuous and nondecreasing, as required for a CDF.",
            "answer": "$$\\boxed{\nF_{Y}(y)=\n\\begin{cases}\n0  y \\le 0 \\\\\n2\\sqrt{y}-y  0 y \\le 1 \\\\\n1  y  1\n\\end{cases}\n}$$"
        },
        {
            "introduction": "After learning to derive CDFs, the next step is to use them to generate data for simulations. This practice introduces the powerful inverse transform sampling method, a cornerstone of computational modeling that directly connects a variable's CDF to a procedure for generating random samples . You will work with a mixed random variable, which combines continuous and discrete components, and learn to derive the generalized inverse CDF to handle the complexities often found in real-world systems.",
            "id": "3183232",
            "problem": "Let $X$ be a mixed random variable whose law consists of an absolutely continuous component with a piecewise-defined probability density function (PDF) and two point-mass spikes (atoms). Use the following fundamental definitions as the base:\n- The cumulative distribution function (CDF) of $X$ is $F_X(x) = \\mathbb{P}(X \\le x)$.\n- Where $X$ is absolutely continuous, $F_X$ is differentiable and its derivative equals the PDF: $f_X(x) = \\frac{d}{dx}F_X(x)$.\n- At any atom located at $x_0$, the jump size of the CDF is $F_X(x_0) - F_X(x_0^-) = \\mathbb{P}(X = x_0)$.\n- The generalized inverse CDF (also called the quantile function) is $F_X^{-1}(u) = \\inf\\{x \\in \\mathbb{R} : F_X(x) \\ge u\\}$ for $u \\in [0,1]$.\n- The inverse CDF sampling method uses a Uniform Random Variable (URV) $U \\sim \\mathrm{Uniform}(0,1)$ and sets $X = F_X^{-1}(U)$.\n\nThe distribution of $X$ is specified as follows:\n- Continuous part with PDF $f_X(x)$:\n  1. For $x \\in [0,1)$, $f_X(x) = a$ with $a = 0.4$.\n  2. For $x \\in [1,2)$, $f_X(x) = b\\,(2-x)$ with $b = 0.8$.\n- Two atoms (spikes):\n  1. At $x = 0.5$ with mass $p_1 = 0.1$.\n  2. At $x = 2$ with mass $p_2 = 0.1$.\n\nTasks:\n1. Starting strictly from the above fundamental definitions, derive the complete piecewise expression of the CDF $F_X(x)$ for all real $x$. Your derivation must correctly account for the continuous accumulation on the intervals $[0,1)$ and $[1,2)$ and the jumps at $x=0.5$ and $x=2$ of sizes $p_1$ and $p_2$ respectively.\n2. Using only the definition $F_X^{-1}(u) = \\inf\\{x : F_X(x) \\ge u\\}$, derive a complete, piecewise-analytic expression for the generalized inverse CDF $F_X^{-1}(u)$ on $u \\in [0,1]$. Your expression must explicitly handle the discontinuities of $F_X$ at $u$ corresponding to the jumps at $x=0.5$ and $x=2$.\n3. Implement a program that:\n   - Computes and uses your derived $F_X^{-1}(u)$ to sample $X$ via inverse CDF given $U \\sim \\mathrm{Uniform}(0,1)$.\n   - Verifies correctness across discontinuities by checking that specific $u$ values map to the atom locations, and by empirically validating the probabilities of five disjoint sets that partition the support in a way that isolates both continuous regions and atoms:\n     - $S_1 = \\{X  0.5\\}$ with expected probability $0.2$.\n     - $S_2 = \\{X = 0.5\\}$ with expected probability $0.1$.\n     - $S_3 = \\{0.5  X  1\\}$ with expected probability $0.2$.\n     - $S_4 = \\{1 \\le X  2\\}$ with expected probability $0.4$.\n     - $S_5 = \\{X = 2\\}$ with expected probability $0.1$.\n   - Uses a tolerance parameter $\\varepsilon$ to declare empirical validation success if the maximum absolute deviation between empirical and theoretical probabilities across $S_1,\\dots,S_5$ is less than or equal to $\\varepsilon$.\n\nTest suite:\n- Case $1$ (deterministic spike mapping): Verify $F_X^{-1}(u)$ at $u$ values $[0.2, 0.25, 0.3, 0.9, 0.95, 1.0]$ equals $[0.5, 0.5, 0.5, 2, 2, 2]$ respectively.\n- Case $2$ (sampling, moderate size): Sample $N=5000$ values with seed $123$ and tolerance $\\varepsilon = 0.02$; return a boolean indicating whether all five sets $S_1$–$S_5$ pass simultaneously.\n- Case $3$ (sampling, larger size): Sample $N=50000$ values with seed $456$ and tolerance $\\varepsilon = 0.005$; return a boolean indicating whether all five sets $S_1$–$S_5$ pass simultaneously.\n- Case $4$ (monotonicity of the quantile): Check that $F_X^{-1}(u)$ is nondecreasing on the grid $[0.0, 0.1, 0.19, 0.2, 0.25, 0.3, 0.31, 0.49, 0.5, 0.7, 0.89, 0.9, 0.95, 1.0]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be a boolean indicating pass or fail for the corresponding test case in the order given.\n- No additional text must be printed.",
            "solution": "The problem requires the derivation of the cumulative distribution function (CDF) $F_X(x)$ and its generalized inverse $F_X^{-1}(u)$ for a mixed random variable $X$, followed by an implementation to verify the derivations.\n\nThe law of the random variable $X$ is composed of an absolutely continuous part with density $f_X(x)$ and a discrete part with two atoms. The total probability must equal $1$. First, we verify this condition.\nThe total probability from the continuous part is:\n$$\nP_c = \\int_0^1 a \\,dx + \\int_1^2 b(2-x) \\,dx\n$$\nGiven $a=0.4$ and $b=0.8$:\n$$\nP_c = \\int_0^1 0.4 \\,dx + \\int_1^2 0.8(2-x) \\,dx = 0.4[x]_0^1 + 0.8\\left[2x - \\frac{x^2}{2}\\right]_1^2\n$$\n$$\nP_c = 0.4(1-0) + 0.8\\left( (4-2) - (2 - \\frac{1}{2}) \\right) = 0.4 + 0.8\\left(2 - \\frac{3}{2}\\right) = 0.4 + 0.8(0.5) = 0.4 + 0.4 = 0.8\n$$\nThe total probability from the discrete atoms is the sum of their masses:\n$$\nP_d = p_1 + p_2 = 0.1 + 0.1 = 0.2\n$$\nThe total probability is $P_{total} = P_c + P_d = 0.8 + 0.2 = 1.0$. The distribution is valid.\n\n### Part 1: Derivation of the Cumulative Distribution Function (CDF) $F_X(x)$\n\nWe derive the CDF, $F_X(x) = \\mathbb{P}(X \\le x)$, by accumulating probability mass across the domain of $X$. For a mixed random variable, this involves integrating the probability density function $f_X(x)$ and adding the masses of any atoms at locations less than or equal to $x$.\n\n1.  **For $x  0$**: The support of $X$ is on $[0, 2]$. Thus, $\\mathbb{P}(X \\le x) = 0$.\n    $F_X(x) = 0$.\n\n2.  **For $0 \\le x  0.5$**: The probability accumulates solely from the continuous density $f_X(t) = 0.4$.\n    $F_X(x) = \\int_{-\\infty}^x f_X(t) \\,dt = \\int_0^x 0.4 \\,dt = 0.4x$.\n\n3.  **At $x = 0.5$**: There is an atom of mass $p_1 = 0.1$. The CDF jumps at this point.\n    The value just before the jump is $F_X(0.5^-) = \\lim_{t \\uparrow 0.5} F_X(t) = 0.4(0.5) = 0.2$.\n    The CDF at $x=0.5$ is $F_X(0.5) = F_X(0.5^-) + \\mathbb{P}(X=0.5) = 0.2 + p_1 = 0.2 + 0.1 = 0.3$.\n\n4.  **For $0.5 \\le x  1$**: The probability is the sum of the mass up to and including $x=0.5$, plus the accumulated probability from the density $f_X(t)=0.4$ on $(0.5, x]$.\n    $F_X(x) = F_X(0.5) + \\int_{0.5}^x 0.4 \\,dt = 0.3 + 0.4(x - 0.5) = 0.3 + 0.4x - 0.2 = 0.4x + 0.1$.\n    Note that this expression correctly gives $F_X(0.5) = 0.4(0.5)+0.1 = 0.3$.\n\n5.  **For $1 \\le x  2$**: The CDF is continuous at $x=1$ as there is no atom.\n    $F_X(1) = F_X(1^-) = 0.4(1) + 0.1 = 0.5$.\n    For $x \\in [1, 2)$, we add the accumulated probability from the density $f_X(t) = 0.8(2-t)$.\n    $F_X(x) = F_X(1) + \\int_1^x 0.8(2-t) \\,dt = 0.5 + 0.8\\left[2t - \\frac{t^2}{2}\\right]_1^x$\n    $F_X(x) = 0.5 + 0.8\\left( (2x - \\frac{x^2}{2}) - (2 - \\frac{1}{2}) \\right) = 0.5 + 0.8\\left(2x - \\frac{x^2}{2} - \\frac{3}{2}\\right)$\n    $F_X(x) = 0.5 + 1.6x - 0.4x^2 - 1.2 = -0.4x^2 + 1.6x - 0.7$.\n\n6.  **At $x = 2$**: There is an atom of mass $p_2 = 0.1$. The CDF jumps again.\n    The value before the jump is $F_X(2^-) = \\lim_{t \\uparrow 2} (-0.4t^2 + 1.6t - 0.7) = -0.4(4) + 1.6(2) - 0.7 = -1.6 + 3.2 - 0.7 = 0.9$.\n    The CDF at $x=2$ is $F_X(2) = F_X(2^-) + \\mathbb{P}(X=2) = 0.9 + p_2 = 0.9 + 0.1 = 1.0$.\n\n7.  **For $x  2$**: All probability has been accounted for.\n    $F_X(x) = 1.0$.\n\nCombining these pieces, the complete CDF is:\n$$\nF_X(x) = \\begin{cases}\n0  \\text{if } x  0 \\\\\n0.4x  \\text{if } 0 \\le x  0.5 \\\\\n0.4x + 0.1  \\text{if } 0.5 \\le x  1 \\\\\n-0.4x^2 + 1.6x - 0.7  \\text{if } 1 \\le x  2 \\\\\n1  \\text{if } x \\ge 2\n\\end{cases}\n$$\n\n### Part 2: Derivation of the Generalized Inverse CDF $F_X^{-1}(u)$\n\nWe derive the generalized inverse CDF, $F_X^{-1}(u) = \\inf\\{x \\in \\mathbb{R} : F_X(x) \\ge u\\}$, by inverting each segment of the piecewise CDF $F_X(x)$ for $u \\in [0, 1]$.\n\n1.  **For $u \\in [0, 0.2)$**: This corresponds to $x \\in [0, 0.5)$. We invert $u = 0.4x$.\n    $x = u / 0.4 = 2.5u$. So, $F_X^{-1}(u) = 2.5u$.\n\n2.  **For $u \\in [0.2, 0.3]$**: This range of $u$ corresponds to the jump in $F_X$ at $x=0.5$.\n    $F_X(0.5^-)=0.2$ and $F_X(0.5)=0.3$. For any $u \\in [0.2, 0.3]$, the set $\\{x : F_X(x) \\ge u\\}$ is $[0.5, \\infty)$. The infimum of this set is $0.5$.\n    Thus, $F_X^{-1}(u) = 0.5$.\n\n3.  **For $u \\in (0.3, 0.5]$**: This corresponds to $x \\in (0.5, 1]$. We invert $u = 0.4x + 0.1$.\n    $0.4x = u - 0.1 \\implies x = (u - 0.1)/0.4 = 2.5u - 0.25$.\n    So, $F_X^{-1}(u) = 2.5u - 0.25$.\n\n4.  **For $u \\in (0.5, 0.9]$**: This corresponds to $x \\in (1, 2]$. We invert $u = -0.4x^2 + 1.6x - 0.7$.\n    Rearranging gives a quadratic equation for $x$: $0.4x^2 - 1.6x + (u+0.7) = 0$.\n    Using the quadratic formula, $x = \\frac{1.6 \\pm \\sqrt{1.6^2 - 4(0.4)(u+0.7)}}{2(0.4)} = \\frac{1.6 \\pm \\sqrt{2.56 - 1.6u - 1.12}}{0.8} = \\frac{1.6 \\pm \\sqrt{1.44 - 1.6u}}{0.8}$.\n    This simplifies to $x = 2 \\pm \\frac{\\sqrt{1.44 - 1.6u}}{0.8} = 2 \\pm \\sqrt{2.25 - 2.5u}$.\n    Since $x$ must be in the interval $(1, 2]$, we must choose the negative root.\n    $F_X^{-1}(u) = 2 - \\sqrt{2.25 - 2.5u}$.\n\n5.  **For $u \\in (0.9, 1.0]$**: This range of $u$ corresponds to the jump at $x=2$.\n    $F_X(2^-)=0.9$ and $F_X(2)=1.0$. For any $u \\in (0.9, 1.0]$, the set $\\{x : F_X(x) \\ge u\\}$ is $[2, \\infty)$. The infimum of this set is $2$.\n    Thus, $F_X^{-1}(u) = 2$.\n\nCombining these pieces, the complete inverse CDF is:\n$$\nF_X^{-1}(u) = \\begin{cases}\n2.5u  \\text{if } 0 \\le u  0.2 \\\\\n0.5  \\text{if } 0.2 \\le u \\le 0.3 \\\\\n2.5u - 0.25  \\text{if } 0.3  u \\le 0.5 \\\\\n2 - \\sqrt{2.25 - 2.5u}  \\text{if } 0.5  u \\le 0.9 \\\\\n2  \\text{if } 0.9  u \\le 1.0\n\\end{cases}\n$$\n\n### Part 3: Implementation\n\nThe derived expression for $F_X^{-1}(u)$ is implemented in Python. The function takes a uniform random variable sample $u$ and returns a sample $x$ from the distribution of $X$. This function is then used to perform the validation tests described in the problem statement. The code defines the piecewise inverse CDF and then executes four test cases: deterministic mapping of spikes, two sampling validations with different sample sizes and tolerances, and a monotonicity check. The results of these boolean tests are collected and printed in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the solution by deriving the inverse CDF, and then running the specified test cases.\n    \"\"\"\n\n    def inverse_cdf_scalar(u):\n        \"\"\"\n        Computes the generalized inverse CDF F_X^{-1}(u) for a scalar u.\n        This function implements the piecewise formula derived in the solution.\n        \"\"\"\n        if u  0.0:\n            # Although u should be in [0,1], handle out-of-bounds for robustness.\n            return 0.0\n        elif u  0.2:\n            # Corresponds to x in [0, 0.5)\n            return 2.5 * u\n        elif u = 0.3:\n            # Corresponds to the atom at x = 0.5\n            return 0.5\n        elif u = 0.5:\n            # Corresponds to x in [0.5, 1)\n            return 2.5 * u - 0.25\n        elif u = 0.9:\n            # Corresponds to x in [1, 2)\n            # This is the solution to 0.4x^2 - 1.6x + (u+0.7) = 0\n            return 2.0 - np.sqrt(2.25 - 2.5 * u)\n        else: # u  0.9\n            # Corresponds to the atom at x = 2 and u  1\n            return 2.0\n\n    # Vectorize the scalar function for efficient application to numpy arrays.\n    inverse_cdf = np.vectorize(inverse_cdf_scalar)\n\n    results = []\n\n    # Case 1: Deterministic spike mapping\n    u_vals_1 = np.array([0.2, 0.25, 0.3, 0.9, 0.95, 1.0])\n    expected_x = np.array([0.5, 0.5, 0.5, 2.0, 2.0, 2.0])\n    actual_x = inverse_cdf(u_vals_1)\n    # Use np.allclose for safe floating-point comparison\n    results.append(np.allclose(actual_x, expected_x))\n\n    def run_sampling_test(N, seed, epsilon):\n        \"\"\"\n        Helper function for Cases 2 and 3.\n        Generates N samples and validates their distribution against expected probabilities.\n        \"\"\"\n        np.random.seed(seed)\n        u_samples = np.random.rand(N)\n        x_samples = inverse_cdf(u_samples)\n\n        # Calculate empirical probabilities for the five disjoint sets.\n        # The comparisons are exact since the inverse CDF returns exact values for atoms.\n        p1_emp = np.sum(x_samples  0.5) / N   # S1: X  0.5\n        p2_emp = np.sum(x_samples == 0.5) / N  # S2: X = 0.5\n        p3_emp = np.sum((x_samples  0.5)  (x_samples  1.0)) / N # S3: 0.5  X  1\n        p4_emp = np.sum((x_samples = 1.0)  (x_samples  2.0)) / N # S4: 1 = X  2\n        p5_emp = np.sum(x_samples == 2.0) / N  # S5: X = 2\n\n        p_emp = np.array([p1_emp, p2_emp, p3_emp, p4_emp, p5_emp])\n        p_exp = np.array([0.2, 0.1, 0.2, 0.4, 0.1])\n        \n        # Check if the maximum absolute deviation is within the tolerance.\n        max_deviation = np.max(np.abs(p_emp - p_exp))\n        return max_deviation = epsilon\n\n    # Case 2: Sampling, moderate size\n    results.append(run_sampling_test(N=5000, seed=123, epsilon=0.02))\n\n    # Case 3: Sampling, larger size\n    results.append(run_sampling_test(N=50000, seed=456, epsilon=0.005))\n    \n    # Case 4: Monotonicity of the quantile function\n    u_grid = np.array([0.0, 0.1, 0.19, 0.2, 0.25, 0.3, 0.31, 0.49, 0.5, 0.7, 0.89, 0.9, 0.95, 1.0])\n    x_vals = inverse_cdf(u_grid)\n    # Check if the sequence of x values is non-decreasing using np.diff.\n    # A small tolerance is used for floating point arithmetic safety.\n    is_monotonic = np.all(np.diff(x_vals) = -1e-9)\n    results.append(is_monotonic)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [bool(r) for r in results]))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While inverse transform sampling is powerful, it requires an invertible CDF, which is not always available. This exercise introduces the Metropolis-Hastings algorithm, a fundamental Markov Chain Monte Carlo (MCMC) method for sampling from distributions even with complex or unknown CDFs . You will learn to implement this sampler and diagnose its performance by tuning its parameters and measuring convergence, a critical skill for any computational scientist working with advanced statistical models.",
            "id": "3183207",
            "problem": "Implement a program that uses the Metropolis–Hastings algorithm to sample from a heavy-tailed target distribution and to compare the empirical cumulative distribution function (CDF) to the true cumulative distribution function to diagnose mixing and bias across chains. Work entirely in purely mathematical and logical terms. All variables, symbols, and numbers must be treated as mathematical entities.\n\nYou must use the standard Cauchy distribution as the heavy-tailed target distribution, defined for a real-valued random variable $X$ with location $\\theta = 0$ and scale $\\gamma = 1$. The target probability density function $f_X$ and cumulative distribution function $F_X$ are:\n- $f_X(x) = \\dfrac{1}{\\pi \\gamma} \\dfrac{1}{1 + \\left(\\dfrac{x - \\theta}{\\gamma}\\right)^2}$, with $\\theta = 0$ and $\\gamma = 1$.\n- $F_X(x) = \\dfrac{1}{2} + \\dfrac{1}{\\pi} \\arctan\\!\\left(\\dfrac{x - \\theta}{\\gamma}\\right)$, with $\\theta = 0$ and $\\gamma = 1$.\n\nUse the Metropolis–Hastings algorithm with a symmetric Gaussian random-walk proposal. That is, given a current state $x$, propose $x' \\sim \\mathcal{N}(x, s^2)$, where $s$ is the proposal standard deviation. Because the proposal is symmetric, the acceptance probability $\\alpha(x \\to x')$ must be chosen to satisfy detailed balance so that the stationary distribution is the target. From first principles, the acceptance probability for a symmetric proposal is:\n$$\n\\alpha(x \\to x') \\;=\\; \\min\\left(1,\\; \\frac{f_X(x')}{f_X(x)}\\right).\n$$\n\nFundamental base and principles to use:\n- Definition of a random variable $X$ and its cumulative distribution function $F_X(x) = \\mathbb{P}(X \\le x)$, and probability density function $f_X(x) = \\dfrac{d}{dx} F_X(x)$ for continuous $X$.\n- For a Markov chain with transition kernel that satisfies detailed balance with respect to a target density $f_X$, the target density is stationary, and under standard regularity conditions the chain converges in distribution to the target.\n- The Metropolis–Hastings construction ensures detailed balance by accepting proposals with probability $\\alpha(x \\to x')$ as specified above for symmetric proposals.\n\nTask requirements:\n- Implement a Metropolis–Hastings sampler that generates samples from the standard Cauchy target using the symmetric Gaussian proposal with standard deviation $s$. Use a fixed pseudorandom seed for reproducibility: set the seed to $12345$.\n- For each chain, discard a specified burn-in and analyze the retained samples (post burn-in).\n- To compare the empirical cumulative distribution function $\\widehat{F}_X$ of the retained samples to the true cumulative distribution function $F_X$, compute the Kolmogorov distance:\n$$\nD = \\sup_{x \\in \\mathcal{G}} \\left| \\widehat{F}_X(x) - F_X(x) \\right|,\n$$\nwhere $\\mathcal{G}$ is a uniform grid of $x$ values. Use a grid $\\mathcal{G}$ consisting of $1001$ evenly spaced points on the interval $[-25, 25]$.\n- For each chain, also compute the post burn-in acceptance rate, defined as the number of accepted proposals that occur after burn-in divided by the number of proposals made after burn-in. Report the acceptance rate as a decimal number in $[0,1]$ (not using a percentage sign).\n- Use a Gaussian proposal with standard deviation parameter $s$ as specified in each test case below.\n\nTest suite:\nRun exactly the following $4$ independent chains. Each chain is specified by a tuple $(\\text{length}, \\text{burn\\_in}, s, x_0)$ where $\\text{length}$ is the total number of states to retain including burn-in, $\\text{burn\\_in}$ is the number of initial states to discard, $s$ is the proposal standard deviation for the Gaussian random walk, and $x_0$ is the starting point.\n- Case $1$: $(20000, 2000, 2.5, 0.0)$ — a general happy-path configuration.\n- Case $2$: $(20000, 2000, 0.1, 0.0)$ — very small proposals to test poor mixing due to high autocorrelation.\n- Case $3$: $(20000, 2000, 10.0, 0.0)$ — very large proposals to test poor mixing due to low acceptance.\n- Case $4$: $(20000, 2000, 2.5, 50.0)$ — starting far in the tail to test robustness of convergence and bias.\n\nFor each case, compute:\n- The post burn-in acceptance rate $a$.\n- The Kolmogorov distance $D$ between $\\widehat{F}_X$ and $F_X$ over the grid $\\mathcal{G}$.\n\nFinal output format:\n- Your program must produce a single line of output containing all results for the $4$ cases, as a comma-separated list of floats enclosed in square brackets, with the two floats per case ordered as $[a_1, D_1, a_2, D_2, a_3, D_3, a_4, D_4]$.\n- Round each float to exactly $6$ decimal places.\n- No other text should be printed.\n\nImplementation notes and constraints:\n- You must implement the Metropolis–Hastings algorithm yourself using the definitions above.\n- The random number generator must be initialized with the fixed seed $12345$.\n- The empirical cumulative distribution function $\\widehat{F}_X$ at a point $x$ is defined as the fraction of retained samples that are less than or equal to $x$.\n- Angles within $\\arctan(\\cdot)$ are in radians.\n- There are no physical units involved.",
            "solution": "The problem requires the implementation and analysis of the Metropolis-Hastings algorithm for sampling from a specified heavy-tailed distribution. The analysis involves validating the algorithm's performance across different parameterizations by comparing the empirical distribution of the generated samples to the true theoretical distribution.\n\n## Problem Validation\nThe problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the established theory of Markov Chain Monte Carlo (MCMC) methods, is mathematically well-posed, and provides a complete and consistent set of definitions, parameters, and constraints. All necessary information is present, and there are no contradictions, ambiguities, or pseudoscientific claims. The task is a standard exercise in computational statistics and is perfectly suited for the specified topic and field.\n\n## Methodological Framework\n\n### Target Distribution\nThe target distribution is the standard Cauchy distribution, which is a member of the Student's t-distribution family with one degree of freedom. It is characterized by its heavy tails, meaning that extreme values occur with much higher probability than for a Gaussian distribution. For a random variable $X$, the probability density function (PDF) $f_X(x)$ and cumulative distribution function (CDF) $F_X(x)$ are given with location parameter $\\theta=0$ and scale parameter $\\gamma=1$:\n$$\nf_X(x) = \\frac{1}{\\pi(1+x^2)}\n$$\n$$\nF_X(x) = \\frac{1}{2} + \\frac{1}{\\pi} \\arctan(x)\n$$\n\n### Metropolis-Hastings Algorithm\nThe Metropolis-Hastings algorithm is an MCMC method for generating a sequence of random samples from a probability distribution for which direct sampling is difficult. We construct a Markov chain whose stationary distribution is the desired target distribution, $f_X(x)$.\n\nThe algorithm proceeds as follows. Given the state of the chain at step $t$, $x_t$:\n$1$. A candidate for the next state, $x'$, is drawn from a proposal distribution $q(x'|x_t)$. The problem specifies a symmetric Gaussian random-walk proposal:\n$$\nx' \\sim \\mathcal{N}(x_t, s^2)\n$$\nwhere $s$ is the proposal standard deviation. The density of this proposal distribution is symmetric, i.e., $q(x'|x_t) = q(x_t|x')$, because it only depends on the distance $|x' - x_t|$.\n\n$2$. The acceptance probability, $\\alpha(x_t \\to x')$, is calculated. For a symmetric proposal, this simplifies to the Metropolis criterion:\n$$\n\\alpha(x_t \\to x') = \\min\\left(1, \\frac{f_X(x')}{f_X(x_t)}\\right)\n$$\nSubstituting the Cauchy PDF, the ratio of densities is:\n$$\n\\frac{f_X(x')}{f_X(x_t)} = \\frac{\\frac{1}{\\pi(1+(x')^2)}}{\\frac{1}{\\pi(1+x_t^2)}} = \\frac{1+x_t^2}{1+(x')^2}\n$$\n\n$3$. A random number $u$ is drawn from a uniform distribution on $[0, 1)$. The next state, $x_{t+1}$, is determined by:\n$$\nx_{t+1} = \\begin{cases} x'  \\text{if } u  \\alpha(x_t \\to x') \\\\ x_t  \\text{otherwise} \\end{cases}\n$$\n\nThis process is repeated for a specified number of steps, `length`. The initial portion of the chain, the `burn_in` period, is discarded to allow the chain to converge to its stationary distribution.\n\n### Performance Diagnostics\nFor each generated chain, two metrics are computed on the post-burn-in samples. Let the total length of the chain be $L$ and the burn-in period be $B$. The retained samples are $\\{x_B, x_{B+1}, \\dots, x_{L-1}\\}$, with a total of $N = L-B$ samples.\n\n$1$. **Post Burn-in Acceptance Rate ($a$)**: This measures the fraction of proposed moves that were accepted after the burn-in period. It is an indicator of the sampler's efficiency.\n$$\na = \\frac{\\text{Number of accepted proposals for } t \\in \\{B, \\dots, L-1\\}}{L-B}\n$$\n\n$2$. **Kolmogorov Distance ($D$)**: This quantitative measure assesses the \"distance\" between the empirical cumulative distribution function ($\\widehat{F}_X$) of the retained samples and the true CDF ($F_X$). It is defined as the maximum absolute difference between the two functions over a specified grid of points $\\mathcal{G}$.\n$$\nD = \\sup_{x \\in \\mathcal{G}} \\left| \\widehat{F}_X(x) - F_X(x) \\right|\n$$\nThe evaluation is performed on a discrete grid $\\mathcal{G}$ of $1001$ points on the interval $[-25, 25]$, so the supremum becomes a maximum. The empirical CDF, for any given point $x$, is the fraction of retained samples less than or equal to $x$:\n$$\n\\widehat{F}_X(x) = \\frac{1}{N} \\sum_{i=B}^{L-1} \\mathbb{I}(x_i \\le x)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. A small value of $D$ indicates that the empirical distribution of the samples is a good approximation of the target distribution.\n\n## Implementation Strategy\nA single function will implement the Metropolis-Hastings sampler for a given test case $(\\text{length}, \\text{burn\\_in}, s, x_0)$. A `numpy` random number generator will be initialized with the fixed seed $12345$ to ensure that the sequence of generated random numbers is identical for each run, making the entire simulation deterministic and reproducible.\n\nThe core of the implementation is a loop that runs for `length` iterations. Inside the loop, a proposal is generated from the Gaussian distribution, the acceptance probability is computed using the Cauchy density ratio, and the move is accepted or rejected based on a draw from a uniform distribution.\n\nAfter the loop completes, the post-burn-in samples are isolated. The acceptance rate is calculated from a counter that tallies accepted moves during the post-burn-in phase. To compute the Kolmogorov distance, the true CDF and the empirical CDF are evaluated on the specified grid $\\mathcal{G}$. The maximum absolute difference between them is then found.\n\nThe main execution block will iterate through the four test cases provided, calling the sampler for each and aggregating the results. Finally, the collected results will be formatted into a single string as per the problem's output specification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Metropolis-Hastings simulations for all test cases\n    and print the formatted results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (length, burn_in, proposal_std, start_x)\n    test_cases = [\n        (20000, 2000, 2.5, 0.0),\n        (20000, 2000, 0.1, 0.0),\n        (20000, 2000, 10.0, 0.0),\n        (20000, 2000, 2.5, 50.0),\n    ]\n\n    # Initialize a single random number generator for reproducibility across all runs.\n    rng = np.random.default_rng(12345)\n    \n    results = []\n    for case in test_cases:\n        length, burn_in, s, x_0 = case\n        acceptance_rate, kolmogorov_dist = run_mcmc_chain(length, burn_in, s, x_0, rng)\n        results.extend([acceptance_rate, kolmogorov_dist])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\ndef target_pdf_ratio(x_proposed, x_current):\n    \"\"\"\n    Computes the ratio of the target PDF f(x_proposed) / f(x_current).\n    For the standard Cauchy distribution, f(x) = 1 / (pi * (1 + x^2)).\n    The pi factors cancel, leaving (1 + x_current^2) / (1 + x_proposed^2).\n    \"\"\"\n    return (1.0 + x_current**2) / (1.0 + x_proposed**2)\n\ndef true_cdf(x):\n    \"\"\"\n    Computes the true CDF of the standard Cauchy distribution.\n    F(x) = 0.5 + arctan(x) / pi.\n    \"\"\"\n    return 0.5 + np.arctan(x) / np.pi\n\ndef run_mcmc_chain(length, burn_in, s, x_0, rng):\n    \"\"\"\n    Runs a single Metropolis-Hastings chain.\n\n    Args:\n        length (int): Total number of states to generate.\n        burn_in (int): Number of initial states to discard.\n        s (float): Standard deviation of the Gaussian proposal distribution.\n        x_0 (float): Starting point of the chain.\n        rng (np.random.Generator): The random number generator to use.\n\n    Returns:\n        tuple[float, float]: A tuple containing:\n            - The post-burn-in acceptance rate.\n            - The Kolmogorov distance D.\n    \"\"\"\n    samples = np.zeros(length)\n    samples[0] = x_0\n    \n    post_burnin_accepted_count = 0\n    current_x = x_0\n\n    for i in range(1, length):\n        # Propose a new state from a symmetric Gaussian random walk\n        proposed_x = rng.normal(loc=current_x, scale=s)\n        \n        # Calculate acceptance probability\n        acceptance_prob = min(1.0, target_pdf_ratio(proposed_x, current_x))\n        \n        # Accept or reject the proposal\n        if rng.uniform(0, 1)  acceptance_prob:\n            current_x = proposed_x\n            if i = burn_in:\n                post_burnin_accepted_count += 1\n        \n        samples[i] = current_x\n\n    # --- Analysis ---\n    \n    # 1. Post burn-in acceptance rate\n    num_post_burnin_steps = length - burn_in\n    acceptance_rate = post_burnin_accepted_count / num_post_burnin_steps\n    \n    # 2. Kolmogorov distance\n    retained_samples = samples[burn_in:]\n    \n    # Define the grid for CDF comparison\n    grid = np.linspace(-25.0, 25.0, 1001)\n    \n    # Calculate true CDF on the grid\n    true_cdf_values = true_cdf(grid)\n    \n    # Calculate empirical CDF on the grid\n    # For each grid point `g`, count how many samples are = g, then divide by total samples.\n    # Broadcasting provides an efficient way to do this without a Python loop.\n    ecdf_values = np.mean(retained_samples[:, np.newaxis] = grid, axis=0)\n    \n    # Calculate Kolmogorov distance D\n    kolmogorov_dist = np.max(np.abs(ecdf_values - true_cdf_values))\n    \n    return acceptance_rate, kolmogorov_dist\n\nsolve()\n```"
        }
    ]
}