{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of computational science is the ability to connect analytical theory with numerical validation. This practice solidifies your understanding of fundamental Bayesian inference by tasking you with deriving the properties of a Beta-Binomial model from first principles. By then implementing a Monte Carlo simulation to verify your analytical results, you will gain hands-on experience with the vital interplay between mathematical theory and computational practice, a skill essential for debugging and building confidence in complex models .",
            "id": "3126330",
            "problem": "You are asked to implement and validate the law of total expectation and moment calculations in a Bayesian updating scenario using a Beta prior and Bernoulli data. The unknown parameter is a probability $X \\in [0,1]$ with prior $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$. You observe $n$ Independent and Identically Distributed (IID) Bernoulli trials with success probability $X$, summarized as a count of successes $k$. Your tasks are:\n1) Analytically derive, from first principles starting with the definitions of expectation, variance, and Bayes’ theorem, the posterior distribution of $X$ given the data. From that posterior, compute the posterior mean and posterior variance. Also compute the prior mean and the unconditional expected number of successes $E[K]$ using the law of total expectation. Do not use any prepackaged “shortcut” formulas; your derivation must start from the definition of the Beta density, the Bernoulli likelihood, Bayes’ theorem, and the definitions of expectation and variance.\n2) Numerically approximate the posterior mean and posterior variance using Monte Carlo sampling from the posterior distribution. In addition, numerically validate the law of total expectation for the total number of successes by simulating the hierarchical process: first draw $X$ from the prior and then draw $K$ from a Binomial distribution with parameters $n$ and $X$, and estimate $E[K]$.\n3) For numerical reproducibility, use a fixed random seed equal to $123456$. Use $M_{\\text{post}}=200000$ samples to estimate posterior moments and $M_{\\text{tot}}=200000$ samples to estimate $E[K]$ under the law of total expectation.\n4) For each test case, compute and report the following seven quantities in this exact order:\n- Analytic prior mean $E[X]$.\n- Analytic unconditional expected successes $E[K]$ computed via the law of total expectation.\n- Analytic posterior mean $E[X \\mid \\text{data}]$.\n- Analytic posterior variance $\\mathrm{Var}[X \\mid \\text{data}]$.\n- Monte Carlo posterior mean estimate $\\widehat{E}[X \\mid \\text{data}]$.\n- Monte Carlo posterior variance estimate $\\widehat{\\mathrm{Var}}[X \\mid \\text{data}]$.\n- Monte Carlo unconditional expected successes estimate $\\widehat{E}[K]$ via the hierarchical simulation.\nRound each reported value to exactly $6$ decimal places.\nTest suite:\nProvide results for the following $5$ test cases, each specified as a quadruple $(\\alpha,\\beta,n,k)$:\n- Case $1$: $(\\alpha,\\beta,n,k)=(2.0,3.0,10,4)$.\n- Case $2$: $(\\alpha,\\beta,n,k)=(2.0,5.0,0,0)$.\n- Case $3$: $(\\alpha,\\beta,n,k)=(0.5,0.5,5,5)$.\n- Case $4$: $(\\alpha,\\beta,n,k)=(50.0,50.0,10,8)$.\n- Case $5$: $(\\alpha,\\beta,n,k)=(1.5,0.5,3,0)$.\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of the seven values for one test case in the specified order, rounded to $6$ decimal places. For example, the output structure must look like $[[v_{1,1},\\dots,v_{1,7}],[v_{2,1},\\dots,v_{2,7}],\\dots,[v_{5,1},\\dots,v_{5,7}]]$ with no additional text.",
            "solution": "We start from the fundamental definitions. Let $X \\in [0,1]$ denote an unknown probability. The prior density of $X$ is the Beta density with parameters $\\alpha>0$ and $\\beta>0$,\n$$\np(x \\mid \\alpha,\\beta)=\\frac{1}{B(\\alpha,\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0<x<1,\n$$\nwhere $B(\\alpha,\\beta)$ is the Beta function. The data consist of $n$ Independent and Identically Distributed (IID) Bernoulli trials with success probability $X$, summarized by the number of successes $K=\\sum_{i=1}^{n} Y_i$, where $Y_i \\in \\{0,1\\}$. Conditional on $X=x$, the likelihood for $K=k$ is the Binomial mass function:\n$$\np(k \\mid x,n) = \\binom{n}{k} x^{k} (1-x)^{n-k}, \\quad k \\in \\{0,1,\\dots,n\\}.\n$$\nBy Bayes’ theorem, the posterior density for $X$ given $K=k$ is proportional to the product $p(k \\mid x,n) p(x \\mid \\alpha,\\beta)$:\n$$\np(x \\mid k,n,\\alpha,\\beta) \\propto x^{k}(1-x)^{n-k} \\cdot x^{\\alpha-1} (1-x)^{\\beta-1} = x^{(\\alpha+k)-1} (1-x)^{(\\beta + n - k)-1}.\n$$\nRecognizing the kernel of a Beta density, we obtain the conjugate posterior\n$$\nX \\mid (k,n,\\alpha,\\beta) \\sim \\mathrm{Beta}(\\alpha',\\beta'), \\quad \\alpha'=\\alpha+k,\\ \\beta'=\\beta+n-k.\n$$\nNext, we compute moments. For a Beta random variable $Z \\sim \\mathrm{Beta}(a,b)$ with $a>0$ and $b>0$, the mean and variance follow from the definitions of expectation and variance using the Beta density and Beta function identities:\n$$\n\\mathbb{E}[Z] = \\int_{0}^{1} z \\, \\frac{1}{B(a,b)} z^{a-1} (1-z)^{b-1} \\, dz = \\frac{a}{a+b},\n$$\n$$\n\\mathrm{Var}(Z) = \\mathbb{E}[Z^{2}] - (\\mathbb{E}[Z])^{2} = \\frac{ab}{(a+b)^{2}(a+b+1)}.\n$$\nApplying these to the prior yields\n$$\n\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha+\\beta}.\n$$\nApplying them to the posterior with $(a,b)=(\\alpha',\\beta')$ yields\n$$\n\\mathbb{E}[X \\mid k] = \\frac{\\alpha+k}{\\alpha+\\beta+n}, \\quad \\mathrm{Var}(X \\mid k) = \\frac{(\\alpha+k)(\\beta+n-k)}{(\\alpha+\\beta+n)^{2}(\\alpha+\\beta+n+1)}.\n$$\nWe now connect to the law of total expectation. Let $K=\\sum_{i=1}^{n} Y_i$ denote the total number of successes from $n$ Bernoulli trials with success probability $X$. The law of total expectation states\n$$\n\\mathbb{E}[K] = \\mathbb{E}\\big[\\mathbb{E}[K \\mid X]\\big].\n$$\nConditional on $X$, $K \\mid X \\sim \\mathrm{Binomial}(n,X)$, so $\\mathbb{E}[K \\mid X]=n X$. Therefore,\n$$\n\\mathbb{E}[K] = \\mathbb{E}[n X] = n \\, \\mathbb{E}[X] = n \\, \\frac{\\alpha}{\\alpha+\\beta}.\n$$\nThis is a direct consequence of linearity of expectation and the definition of conditional expectation.\n\nAlgorithmic plan for each test case $(\\alpha,\\beta,n,k)$:\n1) Compute analytic prior mean $\\mathbb{E}[X]=\\alpha/(\\alpha+\\beta)$ from the Beta mean.\n2) Compute analytic unconditional expected successes $\\mathbb{E}[K]=n \\, \\alpha/(\\alpha+\\beta)$ using the law of total expectation.\n3) Compute analytic posterior parameters $\\alpha'=\\alpha+k$ and $\\beta'=\\beta+n-k$.\n4) Compute analytic posterior mean and variance using the Beta moments with $(a,b)=(\\alpha',\\beta')$:\n$$\n\\mathbb{E}[X \\mid k] = \\frac{\\alpha'}{\\alpha'+\\beta'}, \\quad \\mathrm{Var}(X \\mid k) = \\frac{\\alpha' \\beta'}{(\\alpha'+\\beta')^{2}(\\alpha'+\\beta'+1)}.\n$$\n5) Monte Carlo posterior approximation: draw $M_{\\text{post}}$ samples from $\\mathrm{Beta}(\\alpha',\\beta')$, compute the empirical mean and variance (using population variance, not sample-variance with degrees-of-freedom correction).\n6) Monte Carlo validation of the law of total expectation: draw $M_{\\text{tot}}$ prior samples $x_j \\sim \\mathrm{Beta}(\\alpha,\\beta)$, then for each draw $k_j \\sim \\mathrm{Binomial}(n,x_j)$, and estimate $\\widehat{\\mathbb{E}}[K]$ as the empirical mean of $\\{k_j\\}$.\n7) Use a fixed random seed $123456$ for reproducibility and set $M_{\\text{post}}=200000$ and $M_{\\text{tot}}=200000$.\n8) Round each of the seven outputs to $6$ decimal places and emit the results for all test cases as a single bracketed list of lists on one line in the specified order:\n- Analytic prior mean $\\mathbb{E}[X]$,\n- Analytic $\\mathbb{E}[K]$,\n- Analytic posterior mean $\\mathbb{E}[X \\mid \\text{data}]$,\n- Analytic posterior variance $\\mathrm{Var}[X \\mid \\text{data}]$,\n- Monte Carlo posterior mean estimate,\n- Monte Carlo posterior variance estimate,\n- Monte Carlo estimate of $\\mathbb{E}[K]$ via hierarchical simulation.\n\nDesign considerations:\n- Vectorized simulation with NumPy ensures both accuracy and performance for $M_{\\text{post}}$ and $M_{\\text{tot}}$.\n- Population variance is used to match the analytic variance $\\mathrm{Var}(X \\mid k)$.\n- Boundary cases such as $n=0$ are handled naturally: the posterior reverts to the prior and $\\mathbb{E}[K]=0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef beta_posterior_params(alpha, beta, n, k):\n    a_post = alpha + k\n    b_post = beta + n - k\n    return a_post, b_post\n\ndef beta_moments(a, b):\n    mean = a / (a + b)\n    var = (a * b) / ((a + b) ** 2 * (a + b + 1.0))\n    return mean, var\n\ndef monte_carlo_posterior_moments(rng, a_post, b_post, m_samples):\n    samples = rng.beta(a_post, b_post, size=m_samples)\n    mean = float(np.mean(samples))\n    # population variance (ddof=0) to match analytic Var\n    var = float(np.var(samples))\n    return mean, var\n\ndef monte_carlo_total_expectation_EK(rng, alpha, beta, n, m_samples):\n    # Draw hierarchical samples: X ~ Beta(alpha,beta), then K ~ Binomial(n, X)\n    x = rng.beta(alpha, beta, size=m_samples)\n    if n == 0:\n        # degenerate at 0 successes\n        k = np.zeros(m_samples, dtype=np.int64)\n    else:\n        k = rng.binomial(n, x, size=m_samples)\n    return float(np.mean(k))\n\ndef solve():\n    # Define the test cases from the problem statement: (alpha, beta, n, k)\n    test_cases = [\n        (2.0, 3.0, 10, 4),      # Case 1: general\n        (2.0, 5.0, 0, 0),       # Case 2: boundary n=0\n        (0.5, 0.5, 5, 5),       # Case 3: U-shaped prior, all successes\n        (50.0, 50.0, 10, 8),    # Case 4: strong prior\n        (1.5, 0.5, 3, 0),       # Case 5: skewed prior, no successes\n    ]\n\n    # Fixed random seed and sample sizes as specified\n    seed = 123456\n    m_post = 200000\n    m_tot = 200000\n    rng = np.random.default_rng(seed)\n\n    results_str_blocks = []\n\n    for alpha, beta, n, k in test_cases:\n        # Analytic prior mean\n        prior_mean, _ = beta_moments(alpha, beta)\n\n        # Law of total expectation analytic E[K] = n * E[X]\n        analytic_EK = n * prior_mean\n\n        # Posterior params and analytic posterior moments\n        a_post, b_post = beta_posterior_params(alpha, beta, n, k)\n        post_mean, post_var = beta_moments(a_post, b_post)\n\n        # Monte Carlo posterior moments\n        mc_post_mean, mc_post_var = monte_carlo_posterior_moments(rng, a_post, b_post, m_post)\n\n        # Monte Carlo law of total expectation for E[K]\n        mc_EK = monte_carlo_total_expectation_EK(rng, alpha, beta, n, m_tot)\n\n        # Round to 6 decimals and format\n        vals = [\n            prior_mean,\n            analytic_EK,\n            post_mean,\n            post_var,\n            mc_post_mean,\n            mc_post_var,\n            mc_EK\n        ]\n        formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in vals) + \"]\"\n        results_str_blocks.append(formatted)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_blocks)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "As we move into higher-dimensional spaces, our low-dimensional intuition about computational cost can be misleading. This exercise provides a stark and quantitative illustration of the \"curse of dimensionality,\" a critical concept in computational science, by comparing the efficiency of Monte Carlo methods against traditional grid-based quadrature for numerical integration. By deriving the scaling of sample complexity for both techniques, you will see precisely why Monte Carlo's dimension-independent error rate makes it the go-to method for high-dimensional problems in fields from finance to physics .",
            "id": "3145824",
            "problem": "Consider estimating the integral of a separable function over a unit hypercube using two methods: Monte Carlo (MC) sampling and a tensor-product grid composite trapezoidal quadrature. Let the function be defined by $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$ on $[0,1]^d$, where $g(t) = \\sin(\\pi t)$ and angles are in radians. The integral of interest is $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$. Your task is to implement a program that, for a set of dimensions $d$ and tolerances $\\varepsilon$, computes and compares the minimum sample counts needed by Monte Carlo and tensor-grid quadrature to guarantee a root mean square error at most $\\varepsilon$.\n\nStart from the following foundational bases:\n- Definitions of expectation and variance for random variables under the uniform distribution on $[0,1]^d$.\n- The classical error bound for the composite trapezoidal rule in one dimension: for a twice continuously differentiable function on $[a,b]$ partitioned into $m$ equal panels (so the step size is $h = (b-a)/m$), the absolute error satisfies $|\\int_{a}^{b} f(x)\\,dx - T_m| \\le \\frac{(b-a)}{12} h^2 \\sup_{x \\in [a,b]} |f''(x)|$. For $[0,1]$, this reduces to $|\\int_{0}^{1} f(x)\\,dx - T_m| \\le \\frac{1}{12 m^2} \\sup_{x \\in [0,1]} |f''(x)|$.\n\nUsing these bases and the separable structure of $f$:\n- Derive the variance of $f(\\mathbf{X})$ under the uniform distribution on $[0,1]^d$, with $\\mathbf{X}$ uniform on $[0,1]^d$. Use this to determine the minimal integer number of independent samples $n_{\\mathrm{MC}}$ such that the MC estimator has standard error at most $\\varepsilon$; that is, find $n_{\\mathrm{MC}}$ so that the square root of the variance of the sample mean is at most $\\varepsilon$.\n- For the tensor-grid quadrature, use the one-dimensional composite trapezoidal rule in each coordinate with the same number $m$ of panels per dimension, and take the tensor product to obtain the $d$-dimensional quadrature. Exploit the separability of $f$ to construct a computable upper bound on the $d$-dimensional quadrature error in terms of $d$, $m$, the one-dimensional integral $I_1 = \\int_{0}^{1} g(t)\\,dt$, and the one-dimensional trapezoidal error bound for $g$. From this bound, determine the minimal integer $m$ such that the absolute error is at most $\\varepsilon$, and then compute the total number of grid points $n_{\\mathrm{TG}} = (m+1)^d$ used by the tensor-grid rule.\n\nYour program should, for each test case $(d,\\varepsilon)$, compute:\n- $n_{\\mathrm{MC}}$, the minimal integer number of samples for Monte Carlo to achieve standard error at most $\\varepsilon$.\n- $n_{\\mathrm{TG}}$, the minimal integer number of tensor-grid points to achieve an upper bound on the absolute error at most $\\varepsilon$.\n- A boolean indicating whether MC dominates in sample efficiency, defined as $n_{\\mathrm{MC}} < n_{\\mathrm{TG}}$.\n\nTest Suite:\nUse the following set of $(d,\\varepsilon)$ pairs to exercise different regimes:\n- Case $1$: $d = 1$, $\\varepsilon = 10^{-3}$ (one-dimensional baseline).\n- Case $2$: $d = 5$, $\\varepsilon = 10^{-3}$ (moderate dimension, tight tolerance).\n- Case $3$: $d = 10$, $\\varepsilon = 10^{-3}$ (higher dimension, tight tolerance).\n- Case $4$: $d = 20$, $\\varepsilon = 10^{-3}$ (high dimension, tight tolerance).\n- Case $5$: $d = 1$, $\\varepsilon = 10^{-2}$ (one-dimensional, looser tolerance).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes an inner list of the form $[n_{\\mathrm{MC}},n_{\\mathrm{TG}},\\mathrm{MC\\_dominates}]$. The booleans should be printed as either $\\mathrm{True}$ or $\\mathrm{False}$. No spaces are permitted in the output. For example, the output should look like $[[\\text{case1}],[\\text{case2}],\\dots]$ with no spaces anywhere.",
            "solution": "The problem requires the determination of the minimum number of function evaluations for two numerical integration methods—Monte Carlo (MC) sampling and tensor-product composite trapezoidal quadrature—to estimate the integral $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$ to a specified tolerance $\\varepsilon$. The integrand is a separable function $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$ with $g(t) = \\sin(\\pi t)$ on the unit hypercube $[0,1]^d$.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- **Function**: $f(\\mathbf{x}) = \\prod_{i=1}^{d} g(x_i)$ on the domain $[0,1]^d$.\n- **Constituent function**: $g(t) = \\sin(\\pi t)$, with angles in radians.\n- **Integral**: $I_d = \\int_{[0,1]^d} f(\\mathbf{x}) \\, d\\mathbf{x}$.\n- **Methods**: Monte Carlo (MC) sampling and tensor-product grid composite trapezoidal quadrature.\n- **Tolerance**: $\\varepsilon$.\n- **MC Condition**: The standard error of the MC estimator must be at most $\\varepsilon$.\n- **Tensor-Grid (TG) Condition**: The absolute error of the TG quadrature must be at most $\\varepsilon$.\n- **1D Trapezoidal Error Bound**: For $m$ panels on $[0,1]$, $|\\int_{0}^{1} \\phi(x)\\,dx - T_m[\\phi]| \\le \\frac{1}{12 m^2} \\sup_{x \\in [0,1]} |\\phi''(x)|$.\n- **Outputs per test case $(d, \\varepsilon)$**:\n    1.  $n_{\\mathrm{MC}}$: minimal integer number of MC samples.\n    2.  $n_{\\mathrm{TG}}$: minimal integer number of TG grid points.\n    3.  A boolean for $n_{\\mathrm{MC}} < n_{\\mathrm{TG}}$.\n- **Test Suite**: $(d,\\varepsilon)$ pairs are $(1, 10^{-3}), (5, 10^{-3}), (10, 10^{-3}), (20, 10^{-3}), (1, 10^{-2})$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in standard principles of numerical analysis and probability theory. The function $g(t) = \\sin(\\pi t)$ is well-behaved and infinitely differentiable. The problem is well-posed, as it asks for minimal integer sample counts based on derived error bounds. The definitions and constraints are self-contained and mathematically precise, allowing for a unique solution. The setup is not contradictory, unrealistic, or ill-posed.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. We proceed with deriving the necessary formulae.\n\n### Monte Carlo Method Analysis\n\nThe Monte Carlo estimator for $I_d$ based on $n$ independent samples $\\mathbf{X}_j$ drawn uniformly from $[0,1]^d$ is the sample mean $\\hat{I}_{d,n} = \\frac{1}{n} \\sum_{j=1}^{n} f(\\mathbf{X}_j)$. This estimator is unbiased, i.e., $E[\\hat{I}_{d,n}] = I_d$.\n\nThe standard error of the estimator is the square root of its variance:\n$$\n\\mathrm{SE}(\\hat{I}_{d,n}) = \\sqrt{\\mathrm{Var}(\\hat{I}_{d,n})} = \\sqrt{\\mathrm{Var}\\left(\\frac{1}{n} \\sum_{j=1}^{n} f(\\mathbf{X}_j)\\right)} = \\frac{1}{\\sqrt{n}} \\sqrt{\\mathrm{Var}(f(\\mathbf{X}))}\n$$\nThe condition is $\\mathrm{SE}(\\hat{I}_{d,n}) \\le \\varepsilon$, which implies $\\frac{\\sqrt{\\mathrm{Var}(f(\\mathbf{X}))}}{\\sqrt{n}} \\le \\varepsilon$, or $n \\ge \\frac{\\mathrm{Var}(f(\\mathbf{X}))}{\\varepsilon^2}$.\nThe minimal integer number of samples is $n_{\\mathrm{MC}} = \\left\\lceil \\frac{\\mathrm{Var}(f(\\mathbf{X}))}{\\varepsilon^2} \\right\\rceil$.\n\nWe must compute $\\mathrm{Var}(f(\\mathbf{X})) = E[f(\\mathbf{X})^2] - (E[f(\\mathbf{X})])^2$.\nDue to the separability of $f$ and the independence of the components of $\\mathbf{X}$, the expectations are products of one-dimensional expectations. Let $X \\sim U[0,1]$.\n\n1.  The expectation of $g(X)$:\n    $$\n    E[g(X)] = \\int_0^1 g(t)\\,dt = \\int_0^1 \\sin(\\pi t)\\,dt = \\left[-\\frac{1}{\\pi}\\cos(\\pi t)\\right]_0^1 = -\\frac{1}{\\pi}(\\cos(\\pi) - \\cos(0)) = \\frac{2}{\\pi}\n    $$\n    Let this be $I_1 = \\frac{2}{\\pi}$. Then $I_d = E[f(\\mathbf{X})] = (I_1)^d = \\left(\\frac{2}{\\pi}\\right)^d$.\n\n2.  The expectation of $g(X)^2$:\n    $$\n    E[g(X)^2] = \\int_0^1 g(t)^2\\,dt = \\int_0^1 \\sin^2(\\pi t)\\,dt = \\int_0^1 \\frac{1 - \\cos(2\\pi t)}{2}\\,dt = \\frac{1}{2}\\left[t - \\frac{\\sin(2\\pi t)}{2\\pi}\\right]_0^1 = \\frac{1}{2}\n    $$\n    Then $E[f(\\mathbf{X})^2] = \\prod_{i=1}^d E[g(X_i)^2] = \\left(\\frac{1}{2}\\right)^d$.\n\nThe variance of $f(\\mathbf{X})$ is:\n$$\n\\mathrm{Var}(f(\\mathbf{X})) = \\left(\\frac{1}{2}\\right)^d - \\left(\\left(\\frac{2}{\\pi}\\right)^d\\right)^2 = \\left(\\frac{1}{2}\\right)^d - \\left(\\frac{2}{\\pi}\\right)^{2d}\n$$\nSo, the minimal number of MC samples is:\n$$\nn_{\\mathrm{MC}} = \\left\\lceil \\frac{(1/2)^d - (2/\\pi)^{2d}}{\\varepsilon^2} \\right\\rceil\n$$\n\n### Tensor-Product Quadrature Analysis\n\nThe $d$-dimensional tensor-product trapezoidal rule $T_{m,d}$ with $m$ panels in each dimension is the product of $d$ one-dimensional trapezoidal rules $T_m$. For a separable function $f(\\mathbf{x}) = \\prod_{i=1}^d g(x_i)$, the integral is $T_{m,d}[f] = \\prod_{i=1}^d T_m[g]$.\n\nThe error is $I_d - T_{m,d}[f] = (I_1)^d - (T_m[g])^d$. We can express this difference using the identity $A^d - B^d = (A-B)\\sum_{k=0}^{d-1} A^{d-1-k} B^k$. Let $E_m = I_1 - T_m[g]$.\nThe total error is $(I_1 - T_m[g]) \\sum_{k=0}^{d-1} (I_1)^{d-1-k} (T_m[g])^k = E_m \\sum_{k=0}^{d-1} (I_1)^{d-1-k} (T_m[g])^k$.\nTo bound this, we need to bound $|T_m[g]|$. The function $g(t)=\\sin(\\pi t)$ is concave on $[0,1]$ because $g''(t) = -\\pi^2 \\sin(\\pi t) \\le 0$ for $t \\in [0,1]$. For a concave function, the trapezoidal rule underestimates the integral, so $T_m[g] \\le I_1$. Since $g(t) \\ge 0$, we have $0 \\le T_m[g] \\le I_1$.\nThus, $|T_m[g]| \\le I_1 = |I_1|$.\n\nThe absolute error is bounded by:\n$$\n|I_d - T_{m,d}[f]| \\le |E_m| \\sum_{k=0}^{d-1} |I_1|^{d-1-k} |I_1|^k = |E_m| \\sum_{k=0}^{d-1} |I_1|^{d-1} = d |I_1|^{d-1} |E_m|\n$$\nNow we use the given 1D error bound $|E_m| = |I_1 - T_m[g]| \\le \\frac{1}{12 m^2} \\sup_{t \\in [0,1]} |g''(t)|$.\nFor $g(t) = \\sin(\\pi t)$, $g''(t) = -\\pi^2 \\sin(\\pi t)$. The supremum is $\\sup_{t \\in [0,1]} |-\\pi^2 \\sin(\\pi t)| = \\pi^2$.\nSo, $|E_m| \\le \\frac{\\pi^2}{12m^2}$.\n\nThe total error bound becomes:\n$$\n|I_d - T_{m,d}[f]| \\le d \\left(\\frac{2}{\\pi}\\right)^{d-1} \\frac{\\pi^2}{12m^2}\n$$\nWe require this bound to be at most $\\varepsilon$:\n$$\nd \\left(\\frac{2}{\\pi}\\right)^{d-1} \\frac{\\pi^2}{12m^2} \\le \\varepsilon \\implies m^2 \\ge \\frac{d \\pi^2}{12 \\varepsilon} \\left(\\frac{2}{\\pi}\\right)^{d-1}\n$$\nThe minimal integer number of panels $m$ per dimension is:\n$$\nm = \\left\\lceil \\sqrt{\\frac{d \\pi^2}{12 \\varepsilon} \\left(\\frac{2}{\\pi}\\right)^{d-1}} \\right\\rceil\n$$\nThe composite trapezoidal rule with $m$ panels uses $m+1$ points in each dimension. For the $d$-dimensional tensor grid, the total number of grid points is $n_{\\mathrm{TG}} = (m+1)^d$.\n\n### Summary and Computation\nFor each test case $(d, \\varepsilon)$, we compute:\n1.  $n_{\\mathrm{MC}} = \\left\\lceil \\frac{(1/2)^d - (2/\\pi)^{2d}}{\\varepsilon^2} \\right\\rceil$\n2.  $m = \\left\\lceil \\sqrt{\\frac{d \\pi^2}{12 \\varepsilon} (2/\\pi)^{d-1}} \\right\\rceil$ and $n_{\\mathrm{TG}} = (m+1)^d$.\n3.  The boolean value of $n_{\\mathrm{MC}} < n_{\\mathrm{TG}}$.\n\nThese calculations will be implemented in the final program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the sample counts for Monte Carlo and tensor-grid\n    quadrature for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 10**-3),  # Case 1\n        (5, 10**-3),  # Case 2\n        (10, 10**-3), # Case 3\n        (20, 10**-3), # Case 4\n        (1, 10**-2),  # Case 5\n    ]\n\n    results = []\n    for d, epsilon in test_cases:\n        # --- Monte Carlo Calculation ---\n        # Variance of f(X), where X is uniform on [0,1]^d\n        # Var(f) = E[f^2] - (E[f])^2\n        # E[f] = (integral_0^1 sin(pi*t) dt)^d = (2/pi)^d\n        # E[f^2] = (integral_0^1 sin^2(pi*t) dt)^d = (1/2)^d\n        var_f = (0.5)**d - (2 / np.pi)**(2 * d)\n        \n        # n_MC >= Var(f) / epsilon^2\n        # The number of samples must be an integer, so we take the ceiling.\n        n_mc = np.ceil(var_f / epsilon**2)\n        \n        # --- Tensor-Grid Quadrature Calculation ---\n        # Error_d <= d * |I_1|^(d-1) * Error_1\n        # Error_1 <= sup|g''| / (12 * m^2) = pi^2 / (12 * m^2)\n        # We need Error_d <= epsilon\n        # d * (2/pi)^(d-1) * pi^2 / (12 * m^2) <= epsilon\n        # m^2 >= (d * pi^2 / (12 * epsilon)) * (2/pi)^(d-1)\n        \n        m_squared = (d * np.pi**2 / (12 * epsilon)) * (2 / np.pi)**(d - 1)\n        \n        # Minimal integer number of panels m\n        m = np.ceil(np.sqrt(m_squared))\n        \n        # Total number of grid points is (m+1)^d.\n        # Python's int handles arbitrary-precision integers, so overflow is not an issue.\n        n_tg = (int(m) + 1)**d\n        \n        # --- Comparison ---\n        mc_dominates = bool(n_mc < n_tg)\n        \n        # Append results as integers and a boolean.\n        # Use int() to convert from numpy float types.\n        results.append([int(n_mc), n_tg, mc_dominates])\n\n    # Final print statement in the exact required format.\n    # The format is [[case1_val1,case1_val2,...],[case2_val1,...],...] with no spaces.\n    # Using str() and then replace() is a robust way to achieve this.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "While basic Monte Carlo is powerful, its efficiency can degrade dramatically when estimating the probability of rare events. This advanced practice introduces the Cross-Entropy method, a sophisticated adaptive algorithm that intelligently refines a sampling distribution to make rare events more frequent, thus enabling efficient estimation. By implementing this method, you will engage with modern computational statistics, connecting concepts from importance sampling, optimization, and information theory (via Kullback-Leibler divergence) to solve a genuinely challenging class of problems .",
            "id": "3145853",
            "problem": "Implement the Cross-Entropy method to adapt a univariate Gaussian proposal for estimating a rare-event probability and to track the Kullback–Leibler divergence progression toward the optimal importance sampling distribution, all from first principles of probability. The base distribution is the standard normal with density $f(x) = (2\\pi)^{-1/2} \\exp(-x^2/2)$, and the parametric proposal is the normal family $q_\\mu(x) = (2\\pi)^{-1/2} \\exp(-(x-\\mu)^2/2)$ with unknown mean parameter $\\mu \\in \\mathbb{R}$ and unit variance. The rare event is $A = \\{x \\in \\mathbb{R} : x \\ge a\\}$ for a given threshold $a > 0$. The rare-event probability is $p = \\mathbb{P}_f(X \\ge a)$ for $X \\sim \\mathcal{N}(0,1)$. The optimal importance distribution for estimating $p$ is the zero-variance density $p^\\star(x) = f(x)\\mathbf{1}\\{x \\in A\\}/p$. The Cross-Entropy method chooses parameters by minimizing the Kullback–Leibler divergence $\\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_\\mu)$, equivalently maximizing the cross-entropy $\\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]$. Starting from the core definitions of importance sampling, Kullback–Leibler divergence, and maximum likelihood estimation in the normal family, derive the iterative update for $\\mu$ that results from maximizing $\\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]$ when expectations under $p^\\star$ are approximated by self-normalized importance sampling using the current proposal $q_{\\mu_k}$. Show that for $q_{\\mu_k}$ the importance weight for $x$ equals $w_k(x) = f(x) / q_{\\mu_k}(x) = \\exp(-\\mu_k x + \\mu_k^2/2)$, and that the weighted maximum likelihood update for $\\mu$ reduces to the ratio of weighted first and zeroth moments restricted to the rare event:\n$$\n\\tilde{\\mu}_{k+1} \\;=\\; \\frac{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\,X\\right]}{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\right]}\n$$.\nTo enhance stability, use an exponential smoothing step\n$$\n\\mu_{k+1} \\;=\\; (1-\\alpha)\\,\\mu_k \\;+\\; \\alpha\\,\\tilde{\\mu}_{k+1}\n$$,\nwith a smoothing parameter $\\alpha \\in (0,1]$. For the final adapted proposal $q_{\\mu_K}$ after $K$ iterations, estimate:\n- the rare-event probability via importance sampling:\n$$\n\\hat{p} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}, \\quad X_i \\overset{\\text{i.i.d.}}{\\sim} q_{\\mu_K}\n$$,\n- the relative Kullback–Leibler divergence with respect to the initial proposal $q_{\\mu_0}$:\n$$\n\\Delta_K \\;=\\; \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_K}) \\;-\\; \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_0}) \\;=\\; -\\,\\mathbb{E}_{p^\\star}\\!\\left[\\log q_{\\mu_K}(X) - \\log q_{\\mu_0}(X)\\right]\n$$,\nwhich must be estimated using self-normalized importance sampling under $q_{\\mu_K}$. For the unit-variance normal family, show that $\\log q_{\\mu}(x) = -\\tfrac{1}{2}\\log(2\\pi) - \\tfrac{1}{2}(x-\\mu)^2$ and therefore\n$$\n\\log q_{\\mu_K}(x) - \\log q_{\\mu_0}(x) \\;=\\; \\mu_K x - \\tfrac{1}{2}\\mu_K^2\n$$,\nso that\n$$\n\\widehat{\\Delta}_K \\;=\\; -\\,\\frac{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}\\,\\big(\\mu_K X_i - \\tfrac{1}{2}\\mu_K^2\\big)}{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}}\n$$.\nAlgorithmic requirements:\n- Initialize $\\mu_0 = 0$ and fix a random number generator seed to ensure reproducibility.\n- At each iteration $k \\in \\{0,\\dots,K-1\\}$, draw $N$ independent samples from $q_{\\mu_k}$, compute the weights $w_k(x)$ and restricted moments on $A$, update $\\mu_{k+1}$ via smoothing with parameter $\\alpha$. If the denominator in the weighted update is numerically zero, keep $\\mu_{k+1} = \\mu_k$ unchanged for that iteration.\n- After $K$ iterations, draw $N$ fresh samples from $q_{\\mu_K}$ to compute $\\hat{p}$ and $\\widehat{\\Delta}_K$ as defined above. If the self-normalized denominator is numerically zero, set $\\hat{p} = 0$ and $\\widehat{\\Delta}_K = 0$.\nTest suite:\nRun your program for the following parameter sets $(a,N,K,\\alpha)$:\n- Case $1$: $(a,N,K,\\alpha) = (\\,2.5,\\,20000,\\,5,\\,0.7\\,)$.\n- Case $2$: $(a,N,K,\\alpha) = (\\,3.0,\\,30000,\\,7,\\,0.7\\,)$.\n- Case $3$: $(a,N,K,\\alpha) = (\\,3.5,\\,60000,\\,9,\\,0.7\\,)$.\nUse a single fixed seed $s = 123456$ to initialize your random number generator, and do not change the seed between cases.\nFinal output format:\nYour program should produce a single line of output containing a comma-separated list of the three case results in order, where each case result is the inner list $[\\hat{p},\\widehat{\\Delta}_K,\\mu_K]$ with each value rounded to exactly six digits after the decimal point, and no spaces anywhere. For example: [[0.012345,-0.678901,1.234567],[...],[...]]. No additional text should be printed.",
            "solution": "The user-provided problem is a valid and well-posed exercise in computational statistics. It is scientifically grounded in the theory of importance sampling and information theory, with all necessary parameters and algorithmic steps clearly defined. The derivations provided within the problem statement are mathematically sound and based on first principles. The problem is formal, objective, and self-contained, requiring the implementation of the Cross-Entropy method for a specific application.\n\nThe problem requires the implementation of the Cross-Entropy (CE) method to estimate a rare-event probability $p = \\mathbb{P}_f(X \\ge a)$, where $X$ is a standard normal random variable with probability density function (PDF) $f(x) = \\mathcal{N}(x; 0, 1)$. The CE method is an adaptive importance sampling technique that iteratively finds a better proposal distribution to make the rare event more likely to occur, thereby reducing the variance of the Monte Carlo estimator.\n\nThe chosen family of proposal distributions is the univariate normal family with unit variance, $q_\\mu(x) = \\mathcal{N}(x; \\mu, 1)$, parameterized by its mean $\\mu \\in \\mathbb{R}$. The goal of the CE method is to find the parameter $\\mu$ that makes the proposal density $q_\\mu(x)$ \"closest\" to the optimal, zero-variance importance sampling density, $p^\\star(x)$. The optimal density is defined as $p^\\star(x) = f(x)\\mathbf{1}\\{x \\ge a\\} / p$, where $\\mathbf{1}\\{x \\ge a\\}$ is the indicator function for the rare event $A=\\{x \\in \\mathbb{R} : x \\ge a\\}$. This optimal density is, however, unknown since it depends on the very probability $p$ we aim to estimate.\n\nThe \"closeness\" between two distributions is measured by the Kullback–Leibler (KL) divergence. The CE method seeks to find the parameter $\\mu$ that minimizes $\\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_\\mu)$:\n$$\n\\min_{\\mu} \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_\\mu) = \\min_{\\mu} \\int_{-\\infty}^{\\infty} p^\\star(x) \\log\\left(\\frac{p^\\star(x)}{q_\\mu(x)}\\right) dx\n$$\nThis minimization is equivalent to maximizing the cross-entropy term $\\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]$:\n$$\n\\max_{\\mu} \\int_{-\\infty}^{\\infty} p^\\star(x) \\log(q_\\mu(x)) dx = \\max_{\\mu} \\mathbb{E}_{p^\\star}[\\log q_\\mu(X)]\n$$\nThe logarithm of the proposal density is $\\log q_\\mu(x) = \\log\\left((2\\pi)^{-1/2} e^{-(x-\\mu)^2/2}\\right) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}(x-\\mu)^2$. Maximizing the expectation of this quantity with respect to $\\mu$ is equivalent to maximizing $\\mathbb{E}_{p^\\star}[-(X-\\mu)^2]$. Taking the derivative with respect to $\\mu$ and setting it to zero yields:\n$$\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{p^\\star}[-X^2 + 2\\mu X - \\mu^2] = \\mathbb{E}_{p^\\star}[2X - 2\\mu] = 2\\mathbb{E}_{p^\\star}[X] - 2\\mu = 0\n$$\nThis implies that the optimal parameter is $\\mu^\\star = \\mathbb{E}_{p^\\star}[X]$. Since $p^\\star(x)$ is non-zero only on the rare-event set $A$, this is the conditional expectation of $X$ given $X \\in A$.\n\nAs $\\mu^\\star$ is unknown, the CE method employs an iterative procedure. At iteration $k$, the expectation $\\mathbb{E}_{p^\\star}[X]$ is approximated using importance sampling with the current proposal distribution $q_{\\mu_k}(x)$. The importance sampling weight for a sample $x$ is the ratio of the target density to the proposal density, $w_k(x) = f(x)/q_{\\mu_k}(x)$. For the given normal distributions, this simplifies to:\n$$\nw_k(x) = \\frac{(2\\pi)^{-1/2} e^{-x^2/2}}{(2\\pi)^{-1/2} e^{-(x-\\mu_k)^2/2}} = e^{-x^2/2 + (x^2 - 2\\mu_k x + \\mu_k^2)/2} = e^{-\\mu_k x + \\mu_k^2/2}\n$$\nThe self-normalized importance sampling estimator for $\\mu^\\star = \\mathbb{E}_{p^\\star}[X]$ is used to define the next parameter estimate, $\\tilde{\\mu}_{k+1}$:\n$$\n\\tilde{\\mu}_{k+1} = \\frac{\\sum_{i=1}^N w_k(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}\\,X_i}{\\sum_{i=1}^N w_k(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}} \\approx \\frac{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\,X\\right]}{\\mathbb{E}_{q_{\\mu_k}}\\!\\left[w_k(X)\\,\\mathbf{1}\\{X \\in A\\}\\right]}\n$$\nwhere samples $X_i$ are drawn i.i.d. from $q_{\\mu_k}$. To prevent large, potentially unstable jumps in the parameter estimates, an exponential smoothing step is applied:\n$$\n\\mu_{k+1} = (1-\\alpha)\\mu_k + \\alpha\\tilde{\\mu}_{k+1}\n$$\nfor a smoothing parameter $\\alpha \\in (0,1]$.\n\nAfter $K$ iterations, the final adapted distribution $q_{\\mu_K}$ is used to estimate the rare-event probability $p$ and the improvement in the proposal distribution. The importance sampling estimator for $p = \\mathbb{E}_f[\\mathbf{1}\\{X \\in A\\}] = \\mathbb{E}_{q_{\\mu_K}}[w_K(X)\\mathbf{1}\\{X \\in A\\}]$ is:\n$$\n\\hat{p} = \\frac{1}{N}\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}, \\quad X_i \\sim q_{\\mu_K}\n$$\nThe improvement is quantified by the relative KL divergence $\\Delta_K = \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_K}) - \\mathrm{D}_{\\mathrm{KL}}(p^\\star \\Vert q_{\\mu_0})$. This simplifies to $\\Delta_K = -\\mathbb{E}_{p^\\star}[\\log q_{\\mu_K}(X) - \\log q_{\\mu_0}(X)]$. With the initial parameter $\\mu_0 = 0$, we have:\n$$\n\\log q_{\\mu_K}(x) - \\log q_{\\mu_0}(x) = \\left(-\\frac{1}{2}\\log(2\\pi) - \\frac{(x-\\mu_K)^2}{2}\\right) - \\left(-\\frac{1}{2}\\log(2\\pi) - \\frac{x^2}{2}\\right) = \\frac{1}{2}\\left(-x^2+2\\mu_K x - \\mu_K^2 + x^2\\right) = \\mu_K x - \\frac{1}{2}\\mu_K^2\n$$\nThe estimate $\\widehat{\\Delta}_K$ is obtained by applying the self-normalized importance sampler to the expectation $\\mathbb{E}_{p^\\star}[\\mu_K X - \\frac{1}{2}\\mu_K^2]$ using samples from $q_{\\mu_K}$:\n$$\n\\widehat{\\Delta}_K = -\\frac{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}\\,\\left(\\mu_K X_i - \\frac{1}{2}\\mu_K^2\\right)}{\\sum_{i=1}^N w_K(X_i)\\,\\mathbf{1}\\{X_i \\in A\\}}\n$$\nThe algorithm proceeds by initializing $\\mu_0=0$, iterating $K$ times to find $\\mu_K$, and then using a fresh set of samples from $q_{\\mu_K}$ to compute the final estimates $\\hat{p}$ and $\\widehat{\\Delta}_K$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Cross-Entropy method to estimate a rare-event probability\n    and track the Kullback-Leibler divergence progression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2.5, 20000, 5, 0.7),\n        (3.0, 30000, 7, 0.7),\n        (3.5, 60000, 9, 0.7),\n    ]\n\n    # Initialize a single random number generator for reproducibility across all cases.\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for a, N, K, alpha in test_cases:\n        # --- Iterative adaptation phase ---\n        mu = 0.0  # Initialize mu_0 = 0\n\n        for _ in range(K):\n            # 1. Draw N samples from the current proposal distribution q_mu_k\n            samples = rng.normal(loc=mu, scale=1.0, size=N)\n\n            # 2. Identify samples in the rare event set A = {x >= a}\n            indicator = samples >= a\n            \n            # If no samples fall in the rare event set, the update cannot be computed.\n            if not np.any(indicator):\n                # Per problem, keep mu_k+1 = mu_k\n                continue\n\n            successful_samples = samples[indicator]\n\n            # 3. Compute importance weights for successful samples\n            # w_k(x) = exp(-mu_k * x + mu_k^2 / 2)\n            weights = np.exp(-mu * successful_samples + mu**2 / 2.0)\n\n            # 4. Compute the un-smoothed parameter update mu_tilde\n            numerator = np.sum(weights * successful_samples)\n            denominator = np.sum(weights)\n\n            # Handle case where denominator is numerically zero\n            if denominator < 1e-100:\n                 # Per problem, keep mu_k+1 = mu_k\n                continue\n\n            mu_tilde = numerator / denominator\n\n            # 5. Apply exponential smoothing\n            mu = (1.0 - alpha) * mu + alpha * mu_tilde\n        \n        mu_K = mu\n\n        # --- Final estimation phase ---\n        # 1. Draw N fresh samples from the final proposal distribution q_mu_K\n        final_samples = rng.normal(loc=mu_K, scale=1.0, size=N)\n\n        # 2. Identify samples in the rare event set A\n        final_indicator = final_samples >= a\n\n        # If no samples fall in the rare event set\n        if not np.any(final_indicator):\n            # Per problem, set p_hat and delta_K_hat to 0\n            p_hat = 0.0\n            delta_K_hat = 0.0\n        else:\n            final_successful_samples = final_samples[final_indicator]\n\n            # 3. Compute final weights\n            # w_K(x) = exp(-mu_K * x + mu_K^2 / 2)\n            final_weights = np.exp(-mu_K * final_successful_samples + mu_K**2 / 2.0)\n\n            # 4. Estimate the rare-event probability p_hat\n            # p_hat = (1/N) * sum(w_K(X_i) * I{X_i >= a})\n            sum_of_weights = np.sum(final_weights)\n            p_hat = sum_of_weights / N\n\n            # 5. Estimate the relative KL divergence Delta_K\n            # This is the denominator for the self-normalized estimator of Delta_K\n            norm_denominator = sum_of_weights\n            \n            # Handle case where denominator is numerically zero\n            if norm_denominator < 1e-100:\n                # Per problem, p_hat is also zero in this case\n                p_hat = 0.0\n                delta_K_hat = 0.0\n            else:\n                log_q_diff_term = mu_K * final_successful_samples - 0.5 * mu_K**2\n                norm_numerator = np.sum(final_weights * log_q_diff_term)\n                delta_K_hat = -norm_numerator / norm_denominator\n\n        # Store results for this case as a pre-formatted string\n        case_result_str = f\"[{p_hat:.6f},{delta_K_hat:.6f},{mu_K:.6f}]\"\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}