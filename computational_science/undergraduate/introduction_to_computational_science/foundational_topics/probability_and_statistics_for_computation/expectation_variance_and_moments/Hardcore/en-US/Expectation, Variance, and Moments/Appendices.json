{
    "hands_on_practices": [
        {
            "introduction": "This exercise demonstrates the power of the law of total expectation within a practical Bayesian framework. By modeling uncertainty in a parameter and then using that parameter to model data, you will see how expectations can be layered to make predictions. This hands-on practice will solidify your understanding of how analytical derivations and Monte Carlo simulations work together to analyze hierarchical models. ",
            "id": "3126330",
            "problem": "You are asked to implement and validate the law of total expectation and moment calculations in a Bayesian updating scenario using a Beta prior and Bernoulli data. The unknown parameter is a probability $X \\in [0,1]$ with prior $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$. You observe $n$ Independent and Identically Distributed (IID) Bernoulli trials with success probability $X$, summarized as a count of successes $k$. Your tasks are:\n1) Analytically derive, from first principles starting with the definitions of expectation, variance, and Bayes’ theorem, the posterior distribution of $X$ given the data. From that posterior, compute the posterior mean and posterior variance. Also compute the prior mean and the unconditional expected number of successes $E[K]$ using the law of total expectation. Do not use any prepackaged “shortcut” formulas; your derivation must start from the definition of the Beta density, the Bernoulli likelihood, Bayes’ theorem, and the definitions of expectation and variance.\n2) Numerically approximate the posterior mean and posterior variance using Monte Carlo sampling from the posterior distribution. In addition, numerically validate the law of total expectation for the total number of successes by simulating the hierarchical process: first draw $X$ from the prior and then draw $K$ from a Binomial distribution with parameters $n$ and $X$, and estimate $E[K]$.\n3) For numerical reproducibility, use a fixed random seed equal to $123456$. Use $M_{\\text{post}}=200000$ samples to estimate posterior moments and $M_{\\text{tot}}=200000$ samples to estimate $E[K]$ under the law of total expectation.\n4) For each test case, compute and report the following seven quantities in this exact order:\n- Analytic prior mean $E[X]$.\n- Analytic unconditional expected successes $E[K]$ computed via the law of total expectation.\n- Analytic posterior mean $E[X \\mid \\text{data}]$.\n- Analytic posterior variance $\\mathrm{Var}[X \\mid \\text{data}]$.\n- Monte Carlo posterior mean estimate $\\widehat{E}[X \\mid \\text{data}]$.\n- Monte Carlo posterior variance estimate $\\widehat{\\mathrm{Var}}[X \\mid \\text{data}]$.\n- Monte Carlo unconditional expected successes estimate $\\widehat{E}[K]$ via the hierarchical simulation.\nRound each reported value to exactly $6$ decimal places.\nTest suite:\nProvide results for the following $5$ test cases, each specified as a quadruple $(\\alpha,\\beta,n,k)$:\n- Case $1$: $(\\alpha,\\beta,n,k)=(2.0,3.0,10,4)$.\n- Case $2$: $(\\alpha,\\beta,n,k)=(2.0,5.0,0,0)$.\n- Case $3$: $(\\alpha,\\beta,n,k)=(0.5,0.5,5,5)$.\n- Case $4$: $(\\alpha,\\beta,n,k)=(50.0,50.0,10,8)$.\n- Case $5$: $(\\alpha,\\beta,n,k)=(1.5,0.5,3,0)$.\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of the seven values for one test case in the specified order, rounded to $6$ decimal places. For example, the output structure must look like $[[v_{1,1},\\dots,v_{1,7}],[v_{2,1},\\dots,v_{2,7}],\\dots,[v_{5,1},\\dots,v_{5,7}]]$ with no additional text.",
            "solution": "We start from the fundamental definitions. Let $X \\in [0,1]$ denote an unknown probability. The prior density of $X$ is the Beta density with parameters $\\alpha0$ and $\\beta0$,\n$$\np(x \\mid \\alpha,\\beta)=\\frac{1}{B(\\alpha,\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0x1,\n$$\nwhere $B(\\alpha,\\beta)$ is the Beta function. The data consist of $n$ Independent and Identically Distributed (IID) Bernoulli trials with success probability $X$, summarized by the number of successes $K=\\sum_{i=1}^{n} Y_i$, where $Y_i \\in \\{0,1\\}$. Conditional on $X=x$, the likelihood for $K=k$ is the Binomial mass function:\n$$\np(k \\mid x,n) = \\binom{n}{k} x^{k} (1-x)^{n-k}, \\quad k \\in \\{0,1,\\dots,n\\}.\n$$\nBy Bayes’ theorem, the posterior density for $X$ given $K=k$ is proportional to the product $p(k \\mid x,n) p(x \\mid \\alpha,\\beta)$:\n$$\np(x \\mid k,n,\\alpha,\\beta) \\propto x^{k}(1-x)^{n-k} \\cdot x^{\\alpha-1} (1-x)^{\\beta-1} = x^{(\\alpha+k)-1} (1-x)^{(\\beta + n - k)-1}.\n$$\nRecognizing the kernel of a Beta density, we obtain the conjugate posterior\n$$\nX \\mid (k,n,\\alpha,\\beta) \\sim \\mathrm{Beta}(\\alpha',\\beta'), \\quad \\alpha'=\\alpha+k,\\ \\beta'=\\beta+n-k.\n$$\nNext, we compute moments. For a Beta random variable $Z \\sim \\mathrm{Beta}(a,b)$ with $a0$ and $b0$, the mean and variance follow from the definitions of expectation and variance using the Beta density and Beta function identities:\n$$\n\\mathbb{E}[Z] = \\int_{0}^{1} z \\, \\frac{1}{B(a,b)} z^{a-1} (1-z)^{b-1} \\, dz = \\frac{a}{a+b},\n$$\n$$\n\\mathrm{Var}(Z) = \\mathbb{E}[Z^{2}] - (\\mathbb{E}[Z])^{2} = \\frac{ab}{(a+b)^{2}(a+b+1)}.\n$$\nApplying these to the prior yields\n$$\n\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha+\\beta}.\n$$\nApplying them to the posterior with $(a,b)=(\\alpha',\\beta')$ yields\n$$\n\\mathbb{E}[X \\mid k] = \\frac{\\alpha+k}{\\alpha+\\beta+n}, \\quad \\mathrm{Var}(X \\mid k) = \\frac{(\\alpha+k)(\\beta+n-k)}{(\\alpha+\\beta+n)^{2}(\\alpha+\\beta+n+1)}.\n$$\nWe now connect to the law of total expectation. Let $K=\\sum_{i=1}^{n} Y_i$ denote the total number of successes from $n$ Bernoulli trials with success probability $X$. The law of total expectation states\n$$\n\\mathbb{E}[K] = \\mathbb{E}\\big[\\mathbb{E}[K \\mid X]\\big].\n$$\nConditional on $X$, $K \\mid X \\sim \\mathrm{Binomial}(n,X)$, so $\\mathbb{E}[K \\mid X]=n X$. Therefore,\n$$\n\\mathbb{E}[K] = \\mathbb{E}[n X] = n \\, \\mathbb{E}[X] = n \\, \\frac{\\alpha}{\\alpha+\\beta}.\n$$\nThis is a direct consequence of linearity of expectation and the definition of conditional expectation.\n\nAlgorithmic plan for each test case $(\\alpha,\\beta,n,k)$:\n1) Compute analytic prior mean $\\mathbb{E}[X]=\\alpha/(\\alpha+\\beta)$ from the Beta mean.\n2) Compute analytic unconditional expected successes $\\mathbb{E}[K]=n \\, \\alpha/(\\alpha+\\beta)$ using the law of total expectation.\n3) Compute analytic posterior parameters $\\alpha'=\\alpha+k$ and $\\beta'=\\beta+n-k$.\n4) Compute analytic posterior mean and variance using the Beta moments with $(a,b)=(\\alpha',\\beta')$:\n$$\n\\mathbb{E}[X \\mid k] = \\frac{\\alpha'}{\\alpha'+\\beta'}, \\quad \\mathrm{Var}(X \\mid k) = \\frac{\\alpha' \\beta'}{(\\alpha'+\\beta')^{2}(\\alpha'+\\beta'+1)}.\n$$\n5) Monte Carlo posterior approximation: draw $M_{\\text{post}}$ samples from $\\mathrm{Beta}(\\alpha',\\beta')$, compute the empirical mean and variance (using population variance, not sample-variance with degrees-of-freedom correction).\n6) Monte Carlo validation of the law of total expectation: draw $M_{\\text{tot}}$ prior samples $x_j \\sim \\mathrm{Beta}(\\alpha,\\beta)$, then for each draw $k_j \\sim \\mathrm{Binomial}(n,x_j)$, and estimate $\\widehat{\\mathbb{E}}[K]$ as the empirical mean of $\\{k_j\\}$.\n7) Use a fixed random seed $123456$ for reproducibility and set $M_{\\text{post}}=200000$ and $M_{\\text{tot}}=200000$.\n8) Round each of the seven outputs to $6$ decimal places and emit the results for all test cases as a single bracketed list of lists on one line in the specified order:\n- Analytic prior mean $\\mathbb{E}[X]$,\n- Analytic $\\mathbb{E}[K]$,\n- Analytic posterior mean $\\mathbb{E}[X \\mid \\text{data}]$,\n- Analytic posterior variance $\\mathrm{Var}[X \\mid \\text{data}]$,\n- Monte Carlo posterior mean estimate,\n- Monte Carlo posterior variance estimate,\n- Monte Carlo estimate of $\\mathbb{E}[K]$ via hierarchical simulation.\n\nDesign considerations:\n- Vectorized simulation with NumPy ensures both accuracy and performance for $M_{\\text{post}}$ and $M_{\\text{tot}}$.\n- Population variance is used to match the analytic variance $\\mathrm{Var}(X \\mid k)$.\n- Boundary cases such as $n=0$ are handled naturally: the posterior reverts to the prior and $\\mathbb{E}[K]=0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef beta_posterior_params(alpha, beta, n, k):\n    a_post = alpha + k\n    b_post = beta + n - k\n    return a_post, b_post\n\ndef beta_moments(a, b):\n    mean = a / (a + b)\n    var = (a * b) / ((a + b) ** 2 * (a + b + 1.0))\n    return mean, var\n\ndef monte_carlo_posterior_moments(rng, a_post, b_post, m_samples):\n    samples = rng.beta(a_post, b_post, size=m_samples)\n    mean = float(np.mean(samples))\n    # population variance (ddof=0) to match analytic Var\n    var = float(np.var(samples))\n    return mean, var\n\ndef monte_carlo_total_expectation_EK(rng, alpha, beta, n, m_samples):\n    # Draw hierarchical samples: X ~ Beta(alpha,beta), then K ~ Binomial(n, X)\n    x = rng.beta(alpha, beta, size=m_samples)\n    if n == 0:\n        # degenerate at 0 successes\n        k = np.zeros(m_samples, dtype=np.int64)\n    else:\n        k = rng.binomial(n, x, size=m_samples)\n    return float(np.mean(k))\n\ndef solve():\n    # Define the test cases from the problem statement: (alpha, beta, n, k)\n    test_cases = [\n        (2.0, 3.0, 10, 4),      # Case 1: general\n        (2.0, 5.0, 0, 0),       # Case 2: boundary n=0\n        (0.5, 0.5, 5, 5),       # Case 3: U-shaped prior, all successes\n        (50.0, 50.0, 10, 8),    # Case 4: strong prior\n        (1.5, 0.5, 3, 0),       # Case 5: skewed prior, no successes\n    ]\n\n    # Fixed random seed and sample sizes as specified\n    seed = 123456\n    m_post = 200000\n    m_tot = 200000\n    rng = np.random.default_rng(seed)\n\n    results_str_blocks = []\n\n    for alpha, beta, n, k in test_cases:\n        # Analytic prior mean\n        prior_mean, _ = beta_moments(alpha, beta)\n\n        # Law of total expectation analytic E[K] = n * E[X]\n        analytic_EK = n * prior_mean\n\n        # Posterior params and analytic posterior moments\n        a_post, b_post = beta_posterior_params(alpha, beta, n, k)\n        post_mean, post_var = beta_moments(a_post, b_post)\n\n        # Monte Carlo posterior moments\n        mc_post_mean, mc_post_var = monte_carlo_posterior_moments(rng, a_post, b_post, m_post)\n\n        # Monte Carlo law of total expectation for E[K]\n        mc_EK = monte_carlo_total_expectation_EK(rng, alpha, beta, n, m_tot)\n\n        # Round to 6 decimals and format\n        vals = [\n            prior_mean,\n            analytic_EK,\n            post_mean,\n            post_var,\n            mc_post_mean,\n            mc_post_var,\n            mc_EK\n        ]\n        formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in vals) + \"]\"\n        results_str_blocks.append(formatted)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_blocks)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Building on the concept of hierarchical models, this practice explores how total variance can be decomposed into meaningful components using the law of total variance. You will implement a simulation to see how the variability of an outcome can be partitioned into the average conditional variance, $E[\\mathrm{Var}(X \\mid Y)]$, and the variance of the conditional expectation, $\\mathrm{Var}(E[X \\mid Y])$. This skill is essential for diagnosing sources of uncertainty in complex computational models. ",
            "id": "3126385",
            "problem": "You are to implement a complete, runnable program that uses a hierarchical Monte Carlo (MC, Monte Carlo) simulation to explore variance decomposition for a random variable defined by a two-level generative process. The model is specified by the conditional distribution and the prior: for a latent variable $Y$ and an observed variable $X$, the conditional distribution is $X \\mid Y \\sim \\mathcal{N}(Y, 1)$ and the prior is $Y \\sim \\mathrm{Uniform}(0,1)$. Using first principles of expectation and variance, your program must compute numerical approximations of $E[X]$, $\\mathrm{Var}(X)$, and the decomposition quantity $E[\\mathrm{Var}(X \\mid Y)] + \\mathrm{Var}(E[X \\mid Y])$, relying solely on definitions of empirical expectation and variance over simulated samples.\n\nImplement the following procedure for a hierarchical Monte Carlo approximation:\n- Draw $N_{\\text{outer}}$ independent samples $Y_i$ from $\\mathrm{Uniform}(0,1)$.\n- For each $i$, draw $N_{\\text{inner}}$ independent samples $X_{i,j}$ from $\\mathcal{N}(Y_i, 1)$.\n- Define the empirical mean as\n$$\n\\widehat{m} \\;=\\; \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j}.\n$$\n- Define the empirical (population) variance of all $X_{i,j}$ as\n$$\n\\widehat{v} \\;=\\; \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} \\left(X_{i,j} - \\widehat{m}\\right)^2.\n$$\n- For each $i$, define the group mean\n$$\n\\widehat{m}_i \\;=\\; \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j},\n$$\nand the group (population) variance\n$$\n\\widehat{v}_i \\;=\\; \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} \\left(X_{i,j} - \\widehat{m}_i\\right)^2.\n$$\n- Define the hierarchical components as\n$$\n\\widehat{E[\\mathrm{Var}(X \\mid Y)]} \\;=\\; \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\widehat{v}_i,\n\\qquad\n\\widehat{\\mathrm{Var}(E[X \\mid Y])} \\;=\\; \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\left(\\widehat{m}_i - \\widehat{m}\\right)^2.\n$$\nUsing these definitions with denominators equal to the number of terms (that is, population second central moments), the exact empirical identity\n$$\n\\widehat{v} \\;=\\; \\widehat{E[\\mathrm{Var}(X \\mid Y)]} \\;+\\; \\widehat{\\mathrm{Var}(E[X \\mid Y])}\n$$\nholds for any finite $N_{\\text{outer}}$ and $N_{\\text{inner}}$ up to numerical roundoff.\n\nYour task:\n1. Implement the hierarchical Monte Carlo simulation exactly as described above, using a fixed random seed $s = 123456789$ for reproducibility.\n2. For each specified test case $(N_{\\text{outer}}, N_{\\text{inner}})$, compute four quantities in order:\n   - The empirical mean $\\widehat{m}$,\n   - The empirical variance $\\widehat{v}$,\n   - The sum $\\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}$,\n   - The absolute difference $\\left|\\widehat{v} - \\left(\\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}\\right)\\right|$.\n3. Use the following test suite of parameter pairs $(N_{\\text{outer}}, N_{\\text{inner}})$:\n   - Case $1$: $(50000, 5)$,\n   - Case $2$: $(5, 50000)$,\n   - Case $3$: $(10000, 100)$,\n   - Case $4$: $(2, 2)$.\n\nFinal output format:\n- Your program should produce a single line of output containing a single comma-separated list enclosed in square brackets.\n- The list must contain, in order, the four results for Case $1$, followed by the four results for Case $2$, followed by the four results for Case $3$, followed by the four results for Case $4$. That is, the flat list\n$$\n[\\widehat{m}_1, \\widehat{v}_1, \\widehat{t}_1, \\widehat{d}_1, \\widehat{m}_2, \\widehat{v}_2, \\widehat{t}_2, \\widehat{d}_2, \\widehat{m}_3, \\widehat{v}_3, \\widehat{t}_3, \\widehat{d}_3, \\widehat{m}_4, \\widehat{v}_4, \\widehat{t}_4, \\widehat{d}_4],\n$$\nwhere $\\widehat{t}_k = \\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}$ and $\\widehat{d}_k = \\left|\\widehat{v}_k - \\widehat{t}_k\\right|$ for case $k \\in \\{1,2,3,4\\}$. Each entry must be a real number.",
            "solution": "The user-provided problem is a valid exercise in computational science and statistics. It is scientifically grounded, well-posed, objective, and complete. It asks for the implementation of a hierarchical Monte Carlo simulation to numerically verify the law of total variance for a specific generative model. The problem is sound, and I will proceed with a full solution.\n\nThe problem centers on a hierarchical model where a random variable $X$ depends on a latent variable $Y$. The distributions are given as:\n1.  Prior distribution for the latent variable: $Y \\sim \\mathrm{Uniform}(0, 1)$\n2.  Conditional distribution for the observed variable: $X \\mid Y \\sim \\mathcal{N}(Y, 1)$, a normal distribution with mean $Y$ and variance $1$.\n\nThe primary goal is to compute numerical estimates for the expectation $E[X]$, the variance $\\mathrm{Var}(X)$, and to verify the law of total variance, which states:\n$$\n\\mathrm{Var}(X) = E[\\mathrm{Var}(X \\mid Y)] + \\mathrm{Var}(E[X \\mid Y])\n$$\nWe can first determine the theoretical values for these quantities.\n\n**Theoretical Analysis**\n\n1.  **Expectation of $X$**: Using the law of total expectation, $E[X] = E[E[X \\mid Y]]$.\n    The conditional expectation $E[X \\mid Y]$ is the mean of the normal distribution $\\mathcal{N}(Y, 1)$, which is simply $Y$.\n    Therefore, $E[X] = E[Y]$. Since $Y$ is drawn from a $\\mathrm{Uniform}(0, 1)$ distribution, its expectation is $E[Y] = \\frac{0+1}{2} = \\frac{1}{2}$.\n    Thus, the theoretical expectation of $X$ is $E[X] = 0.5$.\n\n2.  **Variance of $X$**: Using the law of total variance, we evaluate its two components.\n    *   **Inner variance term**: $E[\\mathrm{Var}(X \\mid Y)]$.\n        The conditional variance $\\mathrm{Var}(X \\mid Y)$ is the variance of the normal distribution $\\mathcal{N}(Y, 1)$, which is the constant $1$, regardless of the value of $Y$.\n        Thus, the expectation of this constant is $E[\\mathrm{Var}(X \\mid Y)] = E[1] = 1$.\n    *   **Outer variance term**: $\\mathrm{Var}(E[X \\mid Y])$.\n        We already established that $E[X \\mid Y] = Y$. So, this term becomes $\\mathrm{Var}(Y)$.\n        For a $\\mathrm{Uniform}(a, b)$ distribution, the variance is $\\frac{(b-a)^2}{12}$. For $Y \\sim \\mathrm{Uniform}(0, 1)$, the variance is $\\mathrm{Var}(Y) = \\frac{(1-0)^2}{12} = \\frac{1}{12}$.\n    *   **Total variance**: Combining the two components gives the total variance of $X$:\n        $$\n        \\mathrm{Var}(X) = 1 + \\frac{1}{12} = \\frac{13}{12} \\approx 1.08333...\n        $$\n\n**Hierarchical Monte Carlo Simulation**\n\nThe simulation procedure approximates these theoretical quantities by generating a large number of samples from the specified distributions. The two-level structure of the model is mirrored in the simulation design:\n\n1.  **Outer Loop**: $N_{\\text{outer}}$ independent samples of the latent variable, $Y_1, Y_2, \\ldots, Y_{N_{\\text{outer}}}$, are drawn from $Y \\sim \\mathrm{Uniform}(0, 1)$.\n2.  **Inner Loop**: For each sampled value $Y_i$, a set of $N_{\\text{inner}}$ independent samples of the observed variable, $X_{i,1}, X_{i,2}, \\ldots, X_{i,N_{\\text{inner}}}$, are drawn from the conditional distribution $X \\mid Y=Y_i \\sim \\mathcal{N}(Y_i, 1)$.\n\nThis process yields a total of $N_{\\text{outer}} \\times N_{\\text{inner}}$ samples of $X$. From this dataset, we compute empirical estimates for the quantities of interest.\n\n**Empirical Estimators**\n\nThe problem provides explicit formulas for the estimators, which are defined as population moments (i.e., using a denominator of $N$, not $N-1$).\n\n*   The overall empirical mean, $\\widehat{m}$, estimates $E[X]$:\n    $$\n    \\widehat{m} = \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j}\n    $$\n*   The overall empirical variance, $\\widehat{v}$, estimates $\\mathrm{Var}(X)$:\n    $$\n    \\widehat{v} = \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} (X_{i,j} - \\widehat{m})^2\n    $$\n*   To estimate the components of the total variance, we first compute statistics for each group $i$:\n    *   Group mean: $\\widehat{m}_i = \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j}$, which estimates $E[X \\mid Y=Y_i]$.\n    *   Group variance: $\\widehat{v}_i = \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} (X_{i,j} - \\widehat{m}_i)^2$, which estimates $\\mathrm{Var}(X \\mid Y=Y_i)$.\n*   The terms in the law of total variance are then estimated by averaging these group statistics over the $N_{\\text{outer}}$ samples:\n    *   Estimate for $E[\\mathrm{Var}(X \\mid Y)]$: $\\widehat{E[\\mathrm{Var}(X \\mid Y)]} = \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\widehat{v}_i$\n    *   Estimate for $\\mathrm{Var}(E[X \\mid Y)]$: $\\widehat{\\mathrm{Var}(E[X \\mid Y])} = \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} (\\widehat{m}_i - \\widehat{m})^2$\n    Note that the mean of the group means, $\\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\widehat{m}_i$, is algebraically equivalent to the overall mean $\\widehat{m}$.\n\n**The Empirical Identity**\n\nA key feature of this problem is that, due to the specific definitions of the empirical estimators as population variances, the law of total variance holds as an *exact algebraic identity* for the finite sample, not just as an asymptotic limit. We can show this by decomposing the total sum of squares:\n$$\n\\sum_{i,j} (X_{i,j} - \\widehat{m})^2 = \\sum_{i,j} (X_{i,j} - \\widehat{m}_i + \\widehat{m}_i - \\widehat{m})^2\n$$\nExpanding the square and noting that the cross-term $\\sum_{i,j} 2(X_{i,j} - \\widehat{m}_i)(\\widehat{m}_i - \\widehat{m})$ vanishes because $\\sum_{j} (X_{i,j} - \\widehat{m}_i) = 0$ for each $i$, we get:\n$$\n\\sum_{i,j} (X_{i,j} - \\widehat{m})^2 = \\sum_{i,j} (X_{i,j} - \\widehat{m}_i)^2 + \\sum_{i,j} (\\widehat{m}_i - \\widehat{m})^2\n$$\n$$\n\\sum_{i,j} (X_{i,j} - \\widehat{m})^2 = \\sum_i \\left( N_{\\text{inner}} \\widehat{v}_i \\right) + \\sum_i \\left( N_{\\text{inner}} (\\widehat{m}_i - \\widehat{m})^2 \\right)\n$$\nDividing by the total number of samples, $N_{\\text{outer}}N_{\\text{inner}}$, yields the identity:\n$$\n\\widehat{v} = \\frac{1}{N_{\\text{outer}}} \\sum_i \\widehat{v}_i + \\frac{1}{N_{\\text{outer}}} \\sum_i (\\widehat{m}_i - \\widehat{m})^2 = \\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}\n$$\nTherefore, the calculated absolute difference $\\left|\\widehat{v} - \\left(\\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}\\right)\\right|$ should be zero, subject only to floating-point precision limitations.\n\n**Implementation Details**\n\nThe implementation uses the `numpy` library for efficient, vectorized operations.\n1.  A random number generator is initialized with the specified seed $s=123456789$ for reproducibility.\n2.  For each test case $(N_{\\text{outer}}, N_{\\text{inner}})$, an array of $Y_i$ samples is generated using `rng.uniform`.\n3.  The corresponding $X_{i,j}$ samples are generated using `rng.normal`. `numpy`'s broadcasting feature allows us to provide the array of $Y_i$ values as the means, creating the `(N_outer, N_inner)` array of $X$ samples in a single, efficient call.\n4.  The estimators $\\widehat{m}$, $\\widehat{v}$, $\\widehat{m}_i$, and $\\widehat{v}_i$ are computed using `numpy.mean` and `numpy.var`. The `axis=1` argument is crucial for calculating the group statistics across the rows of the $X$ sample matrix. `numpy.var` computes the population variance by default (denominator $N$), which aligns perfectly with the problem's definitions.\n5.  The final quantities for each test case—the empirical mean $\\widehat{m}$, empirical variance $\\widehat{v}$, the sum of its decomposed components $\\widehat{t}$, and their absolute difference $\\widehat{d}$—are calculated and stored.\n\nThis procedure is repeated for all specified test cases, and the results are aggregated into a single list for output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy  # Imported to adhere to the specified environment, but not used.\n\ndef solve():\n    \"\"\"\n    Implements a hierarchical Monte Carlo simulation to verify the law of total variance.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    s = 123456789\n    rng = np.random.default_rng(s)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50000, 5),     # Case 1\n        (5, 50000),     # Case 2\n        (10000, 100),   # Case 3\n        (2, 2)          # Case 4\n    ]\n\n    results = []\n    for N_outer, N_inner in test_cases:\n        # Step 1: Draw N_outer independent samples from Uniform(0,1) for the latent variable Y.\n        # These will serve as the means for the normal distributions.\n        y_samples = rng.uniform(low=0.0, high=1.0, size=N_outer)\n\n        # Step 2: For each Y_i, draw N_inner samples from Normal(Y_i, 1).\n        # We use numpy broadcasting: y_samples is (N_outer,), we make it (N_outer, 1)\n        # so it broadcasts across the N_inner dimension.\n        # The scale (standard deviation) is sqrt(1) = 1.\n        x_samples = rng.normal(loc=y_samples[:, np.newaxis], scale=1.0, size=(N_outer, N_inner))\n\n        # Step 3: Compute the overall empirical mean and variance of all X samples.\n        # numpy.mean and numpy.var (with default ddof=0) compute the population statistics\n        # as required by the problem statement.\n        m_hat = np.mean(x_samples)\n        v_hat = np.var(x_samples)\n\n        # Step 4: Compute the group-wise statistics.\n        # m_i_hat: mean of each group i (mean of each row).\n        # v_i_hat: variance of each group i (variance of each row).\n        # The `axis=1` argument performs the calculation along each row.\n        m_i_hat = np.mean(x_samples, axis=1)  # Resulting shape: (N_outer,)\n        v_i_hat = np.var(x_samples, axis=1)   # Resulting shape: (N_outer,)\n\n        # Step 5: Compute the hierarchical components based on the problem definitions.\n        # E[Var(X|Y)] is estimated by the mean of the group variances.\n        E_var_hat = np.mean(v_i_hat)\n        \n        # Var(E[X|Y]) is estimated by the variance of the group means.\n        # The mean of m_i_hat is algebraically identical to the overall mean m_hat.\n        # Thus, np.var(m_i_hat) correctly computes the required quantity.\n        var_E_hat = np.var(m_i_hat)\n\n        # Step 6: Compute the sum of the components (t_hat) and the absolute difference (d_hat).\n        # This verifies the empirical identity.\n        t_hat = E_var_hat + var_E_hat\n        d_hat = np.abs(v_hat - t_hat)\n\n        # Append the four required quantities for this test case to the results list.\n        results.extend([m_hat, v_hat, t_hat, d_hat])\n\n    # Final print statement in the exact required format.\n    # map(str, results) ensures default float-to-string conversion.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "This practice delves into the moments of a discrete distribution arising from a combinatorial problem: the number of fixed points in a random permutation. You will first derive the exact mean, variance, and higher central moments from fundamental principles, and then write a simulation to generate empirical estimates. This exercise provides a powerful illustration of how theoretical analysis and computational experiment can be used in concert to fully characterize a random variable. ",
            "id": "3126339",
            "problem": "You are given a family of random variables defined by the number of fixed points in uniformly random permutations. Let $n$ be a positive integer and consider a uniformly random permutation of the set $\\{1,2,\\dots,n\\}$. Define the random variable $F$ to be the number of fixed points in the permutation, that is, the number of indices $i \\in \\{1,2,\\dots,n\\}$ such that the permutation maps $i$ to $i$. This problem requires you to reason from core definitions of expectation, variance, and moments, and connect them to a computational simulation that estimates these quantities.\n\nYour tasks are:\n- Starting from the fundamental definitions of expectation and variance, derive the exact expressions for the first and second moments of $F$ as functions of $n$. Do not use shortcut formulas. Use indicator variables for fixed points and the uniformity of the permutation to justify each step.\n- Define the falling factorial $(F)_k = F(F-1)\\cdots(F-k+1)$ for integer $k \\geq 1$, and express the raw moments $E[F^k]$ for $k \\in \\{1,2,3,4\\}$ in terms of the factorial moments $E[(F)_j]$ and combinatorial coefficients. Derive the third and fourth central moments $\\mu_3 = E[(F - E[F])^3]$ and $\\mu_4 = E[(F - E[F])^4]$ for finite $n$ without invoking high-level limit theorems.\n- Write a complete, runnable program that, for each specified test case, simulates $M$ independent trials, each trial being a uniformly random permutation of $\\{1,\\dots,n\\}$, computes the number of fixed points $F$ in each trial, and produces empirical estimates of the mean, variance, third central moment, and fourth central moment. Compare these empirical estimates to the exact, finite-$n$ theoretical values you derived.\n\nUse the following definitions as the fundamental base for your derivations:\n- Expectation: $E[X] = \\sum_{x} x \\, \\mathbb{P}(X=x)$.\n- Variance: $\\mathrm{Var}(X) = E[X^2] - (E[X])^2$.\n- Linearity of expectation: $E[X+Y] = E[X] + E[Y]$.\n- Covariance: $\\mathrm{Cov}(X,Y) = E[XY] - E[X]E[Y]$.\n- Falling factorial moments: $(X)_k = X(X-1)\\cdots(X-k+1)$ for integer $k \\geq 1$. \n\nProgram requirements:\n- For each test case, output three boolean checks:\n  1. The empirical mean $\\hat{\\mu}$ is within a fixed tolerance $\\tau_\\mu$ of the exact mean.\n  2. The empirical variance $\\hat{v}$ is within a fixed tolerance $\\tau_v$ of the exact variance.\n  3. Both empirical higher central moments are close to their exact values: $|\\hat{\\mu}_3 - \\mu_3| \\le \\tau_3$ and $|\\hat{\\mu}_4 - \\mu_4| \\le \\tau_4$.\n\nSimulation details:\n- Each trial must generate a uniformly random permutation of $\\{1,\\dots,n\\}$ and count the fixed points $F$ directly by comparing positions.\n- Use a fixed random seed to ensure reproducibility.\n\nTest suite:\n- Use the following five parameter sets $(n,M)$:\n  1. $(n,M) = (1,5000)$\n  2. $(n,M) = (2,5000)$\n  3. $(n,M) = (20,20000)$\n  4. $(n,M) = (100,15000)$\n  5. $(n,M) = (200,10000)$\n- Use fixed tolerances:\n  - Mean tolerance: $\\tau_\\mu = 0.02$\n  - Variance tolerance: $\\tau_v = 0.05$\n  - Third central moment tolerance: $\\tau_3 = 0.15$\n  - Fourth central moment tolerance: $\\tau_4 = 0.30$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $15$ booleans corresponding to the five test cases, each contributing three booleans in the order: mean-check, variance-check, higher-moments-check. For example, the format must be $[b_1,b_2,b_3,b_4,\\dots,b_{15}]$, where each $b_i$ is either $True$ or $False$.",
            "solution": "The problem is mathematically well-defined and internally consistent, permitting a rigorous derivation and computational verification. We proceed with the solution.\n\nLet $S_n$ be the set of all permutations of $\\{1, 2, \\dots, n\\}$. A uniformly random permutation $\\pi$ is chosen from $S_n$, with $\\mathbb{P}(\\pi) = 1/n!$ for any $\\pi \\in S_n$. The random variable $F$ denotes the number of fixed points, i.e., the number of indices $i$ such that $\\pi(i) = i$. We can express $F$ as a sum of indicator random variables:\n$$F = \\sum_{i=1}^{n} X_i$$\nwhere $X_i = \\mathbb{I}(\\pi(i)=i)$ is an indicator variable that is $1$ if $i$ is a fixed point and $0$ otherwise.\n\n**First Moment (Expectation) of F**\n\nThe expectation of $F$ is derived using the linearity of expectation.\n$$E[F] = E\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} E[X_i]$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus, we need to calculate $\\mathbb{P}(\\pi(i)=i)$.\n$$E[X_i] = \\mathbb{P}(X_i = 1) = \\mathbb{P}(\\pi(i)=i)$$\nTo calculate this probability, we count the number of permutations in which $i$ is a fixed point. If $\\pi(i)=i$, the remaining $n-1$ elements can be permuted in $(n-1)!$ ways among the remaining $n-1$ positions. The total number of permutations is $n!$. Therefore,\n$$\\mathbb{P}(\\pi(i)=i) = \\frac{(n-1)!}{n!} = \\frac{1}{n}$$\nThis probability is the same for all $i \\in \\{1, 2, \\dots, n\\}$. Substituting this back into the expression for $E[F]$:\n$$E[F] = \\sum_{i=1}^{n} \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1$$\nThis result holds for any integer $n \\ge 1$.\n\n**Second Moment and Variance of F**\n\nThe variance of $F$ is defined as $\\mathrm{Var}(F) = E[F^2] - (E[F])^2$. We have already found $E[F]=1$, so we need to compute the second raw moment, $E[F^2]$.\n$$F^2 = \\left(\\sum_{i=1}^{n} X_i\\right)^2 = \\sum_{i=1}^{n} X_i^2 + \\sum_{i=1}^{n}\\sum_{j=1, j\\neq i}^{n} X_i X_j$$\nBy linearity of expectation:\n$$E[F^2] = E\\left[\\sum_{i=1}^{n} X_i^2\\right] + E\\left[\\sum_{i \\neq j} X_i X_j\\right] = \\sum_{i=1}^{n} E[X_i^2] + \\sum_{i \\neq j} E[X_i X_j]$$\nFor an indicator variable, $X_i^2 = X_i$ since $X_i$ only takes values $0$ or $1$. Thus, $E[X_i^2] = E[X_i] = 1/n$. The first sum is:\n$$\\sum_{i=1}^{n} E[X_i^2] = \\sum_{i=1}^{n} \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1$$\nNext, we consider the term $E[X_i X_j]$ for any distinct pair of indices $i \\neq j$. The product $X_i X_j$ is an indicator variable for the event that both $i$ and $j$ are fixed points.\n$$E[X_i X_j] = \\mathbb{P}(X_i=1 \\text{ and } X_j=1) = \\mathbb{P}(\\pi(i)=i \\text{ and } \\pi(j)=j)$$\nThe number of permutations where both $i$ and $j$ are fixed is the number of ways to permute the remaining $n-2$ elements, which is $(n-2)!$. This derivation assumes $n \\ge 2$.\n$$\\mathbb{P}(\\pi(i)=i \\text{ and } \\pi(j)=j) = \\frac{(n-2)!}{n!} = \\frac{1}{n(n-1)}$$\nThe number of ordered pairs $(i,j)$ with $i \\neq j$ is $n(n-1)$. The second sum is:\n$$\\sum_{i \\neq j} E[X_i X_j] = n(n-1) \\cdot \\frac{1}{n(n-1)} = 1$$\nThis is valid for $n \\ge 2$. For $n=1$, the sum over $i \\neq j$ is empty and equals $0$.\nCombining the terms:\n- For $n=1$: $E[F^2] = \\sum_{i=1}^1 E[X_1^2] = E[X_1] = 1/1 = 1$.\n- For $n \\ge 2$: $E[F^2] = 1 + 1 = 2$.\n\nNow we can compute the variance:\n- For $n=1$: $\\mathrm{Var}(F) = E[F^2] - (E[F])^2 = 1 - 1^2 = 0$. This is expected, as for $n=1$, $F$ is a constant ($F=1$).\n- For $n \\ge 2$: $\\mathrm{Var}(F) = E[F^2] - (E[F])^2 = 2 - 1^2 = 1$.\n\n**Higher Moments of F**\n\nTo derive higher moments, it is convenient to first compute the factorial moments, $E[(F)_k]$, where $(F)_k = F(F-1)\\cdots(F-k+1)$. The $k$-th factorial moment counts the expected number of ordered $k$-tuples of distinct fixed points.\n$$(F)_k = \\sum_{i_1 \\neq i_2 \\neq \\dots \\neq i_k} X_{i_1} X_{i_2} \\cdots X_{i_k}$$\nwhere the sum is over all ordered $k$-tuples of distinct indices. By linearity of expectation:\n$$E[(F)_k] = \\sum_{i_1 \\neq i_2 \\neq \\dots \\neq i_k} E[X_{i_1} X_{i_2} \\cdots X_{i_k}]$$\nThe term $E[X_{i_1} \\cdots X_{i_k}]$ is the probability that $i_1, \\dots, i_k$ are all fixed points. This requires $k \\le n$. If $k  n$, this probability is $0$.\nFor $k \\le n$, the number of permutations fixing $k$ specified elements is $(n-k)!$.\n$$\\mathbb{P}(\\pi(i_1)=i_1, \\dots, \\pi(i_k)=i_k) = \\frac{(n-k)!}{n!} = \\frac{1}{(n)_k}$$\nThe number of ordered $k$-tuples of distinct indices is $(n)_k = n(n-1)\\cdots(n-k+1)$.\nThus, for $1 \\le k \\le n$:\n$$E[(F)_k] = (n)_k \\cdot \\frac{1}{(n)_k} = 1$$\nIf $k  n$, it is impossible to have $k$ fixed points, so $(F)_k$ is always $0$, and $E[(F)_k] = 0$.\n\nRaw moments $E[F^k]$ can be expressed in terms of factorial moments using Stirling numbers of the second kind, denoted $S(k,j)$ or $\\{{k \\atop j}\\}$.\n$$F^k = \\sum_{j=0}^{k} S(k,j) (F)_j$$\nTaking the expectation of both sides:\n$$E[F^k] = \\sum_{j=0}^{k} S(k,j) E[(F)_j]$$\nSince $E[(F)_0]=1$ by definition and $E[(F)_j]=1$ for $1 \\le j \\le n$ and $0$ for $jn$:\n$$E[F^k] = S(k,0) + \\sum_{j=1}^{\\min(k,n)} S(k,j)$$\nAs $S(k,0)=0$ for $k \\ge 1$, we have $E[F^k] = \\sum_{j=1}^{\\min(k,n)} S(k,j)$.\nThe required Stirling numbers are:\n- $S(1,1)=1$\n- $S(2,1)=1, S(2,2)=1$\n- $S(3,1)=1, S(3,2)=3, S(3,3)=1$\n- $S(4,1)=1, S(4,2)=7, S(4,3)=6, S(4,4)=1$\n\nUsing these, we can tabulate the raw moments for different $n$:\n- For $n=1$:\n  $E[F] = S(1,1) = 1$\n  $E[F^2] = S(2,1) = 1$\n  $E[F^3] = S(3,1) = 1$\n  $E[F^4] = S(4,1) = 1$\n- For $n=2$:\n  $E[F] = S(1,1) = 1$\n  $E[F^2] = S(2,1)+S(2,2) = 1+1=2$\n  $E[F^3] = S(3,1)+S(3,2) = 1+3=4$\n  $E[F^4] = S(4,1)+S(4,2) = 1+7=8$\n- For $n=3$:\n  $E[F^3] = S(3,1)+S(3,2)+S(3,3) = 1+3+1=5$\n  $E[F^4] = S(4,1)+S(4,2)+S(4,3) = 1+7+6=14$\n- For $n \\ge 4$:\n  $E[F^3] = 1+3+1=5$\n  $E[F^4] = 1+7+6+1=15$\n\nThe central moments $\\mu_k=E[(F-E[F])^k]=E[(F-1)^k]$ are derived from the raw moments:\n$\\mu_3 = E[(F-1)^3] = E[F^3] - 3E[F^2] + 3E[F] - 1$\n$\\mu_4 = E[(F-1)^4] = E[F^4] - 4E[F^3] + 6E[F^2] - 4E[F] + 1$\n\n**Summary of Theoretical Moments for Simulation**\n\n| $n$ | $E[F]=\\mu$ | $\\mathrm{Var}(F)=\\mu_2$ | $\\mu_3$ | $\\mu_4$ |\n|-----|------------|-------------------------|---------|---------|\n| 1   | $1$        | $0$                     | $0$     | $0$     |\n| 2   | $1$        | $1$                     | $0$     | $1$     |\n| 3   | $1$        | $1$                     | $1$     | $3$     |\n| $\\ge 4$ | $1$    | $1$                     | $1$     | $4$     |\n\nFor the specified test cases $(n=1, 2, 20, 100, 200)$, the theoretical moments are:\n- $n=1$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 0, 0, 0)$\n- $n=2$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 0, 1)$\n- $n=20$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 1, 4)$\n- $n=100$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 1, 4)$\n- $n=200$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 1, 4)$\n\nThese theoretical values will be compared against empirical estimates from a computational simulation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef get_theoretical_moments(n):\n    \"\"\"\n    Computes the exact theoretical moments for the number of fixed points F\n    in a random permutation of n elements.\n\n    Returns:\n        A tuple (mu, mu2, mu3, mu4) containing the mean, variance,\n        3rd central moment, and 4th central moment.\n    \"\"\"\n    if not isinstance(n, int) or n  1:\n        raise ValueError(\"n must be a positive integer.\")\n\n    mu = 1.0\n\n    if n == 1:\n        # F is deterministically 1. All central moments are 0.\n        mu2 = 0.0\n        mu3 = 0.0\n        mu4 = 0.0\n    elif n == 2:\n        # F is 0 or 2 with probability 1/2 each. E[F]=1.\n        # Var(F) = E[(F-1)^2] = (0-1)^2 * 0.5 + (2-1)^2 * 0.5 = 1.\n        # mu3 = E[(F-1)^3] = (-1)^3 * 0.5 + (1)^3 * 0.5 = 0.\n        # mu4 = E[(F-1)^4] = (-1)^4 * 0.5 + (1)^4 * 0.5 = 1.\n        mu2 = 1.0\n        mu3 = 0.0\n        mu4 = 1.0\n    elif n == 3:\n        # E[F]=1, E[F^2]=2, E[F^3]=5, E[F^4]=14.\n        mu2 = 2.0 - 1.0**2  # Var = E[F^2] - E[F]^2\n        mu3 = 5.0 - 3 * 2.0 + 3 * 1.0 - 1.0\n        mu4 = 14.0 - 4 * 5.0 + 6 * 2.0 - 4 * 1.0 + 1.0\n    else: # n = 4\n        # E[F]=1, E[F^2]=2, E[F^3]=5, E[F^4]=15.\n        mu2 = 2.0 - 1.0**2\n        mu3 = 5.0 - 3 * 2.0 + 3 * 1.0 - 1.0\n        mu4 = 15.0 - 4 * 5.0 + 6 * 2.0 - 4 * 1.0 + 1.0\n    \n    return mu, mu2, mu3, mu4\n\ndef solve():\n    \"\"\"\n    Runs simulations to estimate moments of the number of fixed points\n    in random permutations and compares them to theoretical values.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 5000),\n        (2, 5000),\n        (20, 20000),\n        (100, 15000),\n        (200, 10000)\n    ]\n\n    # Define the tolerances from the problem statement.\n    tau_mu = 0.02\n    tau_v = 0.05\n    tau_3 = 0.15\n    tau_4 = 0.30\n    \n    # Use a fixed random seed for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for n, M in test_cases:\n        # Get exact theoretical values for the current n.\n        th_mu, th_mu2, th_mu3, th_mu4 = get_theoretical_moments(n)\n        \n        # Run M trials to collect samples of F.\n        fixed_points_samples = np.zeros(M, dtype=int)\n        indices = np.arange(n)\n        for i in range(M):\n            # Generate a uniformly random permutation.\n            perm = rng.permutation(n)\n            # Count the number of fixed points.\n            fixed_points = np.sum(perm == indices)\n            fixed_points_samples[i] = fixed_points\n            \n        # Calculate empirical estimates of the moments.\n        emp_mu = np.mean(fixed_points_samples)\n        \n        # Note: np.var computes E[(X-E[X])^2] using the sample mean, which is correct\n        # for estimating the variance as a central moment.\n        # ddof=0 (default) uses 1/M, not 1/(M-1).\n        emp_mu2 = np.var(fixed_points_samples)\n        \n        # scipy.stats.moment calculates sample central moments.\n        emp_mu3 = stats.moment(fixed_points_samples, moment=3)\n        emp_mu4 = stats.moment(fixed_points_samples, moment=4)\n        \n        # Perform the three boolean checks.\n        mean_check = abs(emp_mu - th_mu) = tau_mu\n        var_check = abs(emp_mu2 - th_mu2) = tau_v\n        \n        higher_moments_check = (abs(emp_mu3 - th_mu3) = tau_3 and\n                                abs(emp_mu4 - th_mu4) = tau_4)\n        \n        results.extend([mean_check, var_check, higher_moments_check])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}