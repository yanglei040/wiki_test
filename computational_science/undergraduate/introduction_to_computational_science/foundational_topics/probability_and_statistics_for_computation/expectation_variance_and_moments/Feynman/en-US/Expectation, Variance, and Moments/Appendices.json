{
    "hands_on_practices": [
        {
            "introduction": "A robust understanding of expectation and variance begins with mastering fundamental derivation techniques. This first practice challenges you to apply the powerful method of indicator variables to a classic combinatorial problem: finding the moments of the number of fixed points in a random permutation . By deriving the exact mean and variance from first principles and then verifying your results through simulation, you will solidify the essential link between theoretical analysis and computational experiment.",
            "id": "3126339",
            "problem": "You are given a family of random variables defined by the number of fixed points in uniformly random permutations. Let $n$ be a positive integer and consider a uniformly random permutation of the set $\\{1,2,\\dots,n\\}$. Define the random variable $F$ to be the number of fixed points in the permutation, that is, the number of indices $i \\in \\{1,2,\\dots,n\\}$ such that the permutation maps $i$ to $i$. This problem requires you to reason from core definitions of expectation, variance, and moments, and connect them to a computational simulation that estimates these quantities.\n\nYour tasks are:\n- Starting from the fundamental definitions of expectation and variance, derive the exact expressions for the first and second moments of $F$ as functions of $n$. Do not use shortcut formulas. Use indicator variables for fixed points and the uniformity of the permutation to justify each step.\n- Define the falling factorial $(F)_k = F(F-1)\\cdots(F-k+1)$ for integer $k \\geq 1$, and express the raw moments $E[F^k]$ for $k \\in \\{1,2,3,4\\}$ in terms of the factorial moments $E[(F)_j]$ and combinatorial coefficients. Derive the third and fourth central moments $\\mu_3 = E[(F - E[F])^3]$ and $\\mu_4 = E[(F - E[F])^4]$ for finite $n$ without invoking high-level limit theorems.\n- Write a complete, runnable program that, for each specified test case, simulates $M$ independent trials, each trial being a uniformly random permutation of $\\{1,\\dots,n\\}$, computes the number of fixed points $F$ in each trial, and produces empirical estimates of the mean, variance, third central moment, and fourth central moment. Compare these empirical estimates to the exact, finite-$n$ theoretical values you derived.\n\nUse the following definitions as the fundamental base for your derivations:\n- Expectation: $E[X] = \\sum_{x} x \\, \\mathbb{P}(X=x)$.\n- Variance: $\\mathrm{Var}(X) = E[X^2] - (E[X])^2$.\n- Linearity of expectation: $E[X+Y] = E[X] + E[Y]$.\n- Covariance: $\\mathrm{Cov}(X,Y) = E[XY] - E[X]E[Y]$.\n- Falling factorial moments: $(X)_k = X(X-1)\\cdots(X-k+1)$ for integer $k \\geq 1$. \n\nProgram requirements:\n- For each test case, output three boolean checks:\n  1. The empirical mean $\\hat{\\mu}$ is within a fixed tolerance $\\tau_\\mu$ of the exact mean.\n  2. The empirical variance $\\hat{v}$ is within a fixed tolerance $\\tau_v$ of the exact variance.\n  3. Both empirical higher central moments are close to their exact values: $|\\hat{\\mu}_3 - \\mu_3| \\le \\tau_3$ and $|\\hat{\\mu}_4 - \\mu_4| \\le \\tau_4$.\n\nSimulation details:\n- Each trial must generate a uniformly random permutation of $\\{1,\\dots,n\\}$ and count the fixed points $F$ directly by comparing positions.\n- Use a fixed random seed to ensure reproducibility.\n\nTest suite:\n- Use the following five parameter sets $(n,M)$:\n  1. $(n,M) = (1,5000)$\n  2. $(n,M) = (2,5000)$\n  3. $(n,M) = (20,20000)$\n  4. $(n,M) = (100,15000)$\n  5. $(n,M) = (200,10000)$\n- Use fixed tolerances:\n  - Mean tolerance: $\\tau_\\mu = 0.02$\n  - Variance tolerance: $\\tau_v = 0.05$\n  - Third central moment tolerance: $\\tau_3 = 0.15$\n  - Fourth central moment tolerance: $\\tau_4 = 0.30$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $15$ booleans corresponding to the five test cases, each contributing three booleans in the order: mean-check, variance-check, higher-moments-check. For example, the format must be $[b_1,b_2,b_3,b_4,\\dots,b_{15}]$, where each $b_i$ is either $True$ or $False$.",
            "solution": "The problem is mathematically well-defined and internally consistent, permitting a rigorous derivation and computational verification. We proceed with the solution.\n\nLet $S_n$ be the set of all permutations of $\\{1, 2, \\dots, n\\}$. A uniformly random permutation $\\pi$ is chosen from $S_n$, with $\\mathbb{P}(\\pi) = 1/n!$ for any $\\pi \\in S_n$. The random variable $F$ denotes the number of fixed points, i.e., the number of indices $i$ such that $\\pi(i) = i$. We can express $F$ as a sum of indicator random variables:\n$$F = \\sum_{i=1}^{n} X_i$$\nwhere $X_i = \\mathbb{I}(\\pi(i)=i)$ is an indicator variable that is $1$ if $i$ is a fixed point and $0$ otherwise.\n\n**First Moment (Expectation) of F**\n\nThe expectation of $F$ is derived using the linearity of expectation.\n$$E[F] = E\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} E[X_i]$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus, we need to calculate $\\mathbb{P}(\\pi(i)=i)$.\n$$E[X_i] = \\mathbb{P}(X_i = 1) = \\mathbb{P}(\\pi(i)=i)$$\nTo calculate this probability, we count the number of permutations in which $i$ is a fixed point. If $\\pi(i)=i$, the remaining $n-1$ elements can be permuted in $(n-1)!$ ways among the remaining $n-1$ positions. The total number of permutations is $n!$. Therefore,\n$$\\mathbb{P}(\\pi(i)=i) = \\frac{(n-1)!}{n!} = \\frac{1}{n}$$\nThis probability is the same for all $i \\in \\{1, 2, \\dots, n\\}$. Substituting this back into the expression for $E[F]$:\n$$E[F] = \\sum_{i=1}^{n} \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1$$\nThis result holds for any integer $n \\ge 1$.\n\n**Second Moment and Variance of F**\n\nThe variance of $F$ is defined as $\\mathrm{Var}(F) = E[F^2] - (E[F])^2$. We have already found $E[F]=1$, so we need to compute the second raw moment, $E[F^2]$.\n$$F^2 = \\left(\\sum_{i=1}^{n} X_i\\right)^2 = \\sum_{i=1}^{n} X_i^2 + \\sum_{i=1}^{n}\\sum_{j=1, j\\neq i}^{n} X_i X_j$$\nBy linearity of expectation:\n$$E[F^2] = E\\left[\\sum_{i=1}^{n} X_i^2\\right] + E\\left[\\sum_{i \\neq j} X_i X_j\\right] = \\sum_{i=1}^{n} E[X_i^2] + \\sum_{i \\neq j} E[X_i X_j]$$\nFor an indicator variable, $X_i^2 = X_i$ since $X_i$ only takes values $0$ or $1$. Thus, $E[X_i^2] = E[X_i] = 1/n$. The first sum is:\n$$\\sum_{i=1}^{n} E[X_i^2] = \\sum_{i=1}^{n} \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1$$\nNext, we consider the term $E[X_i X_j]$ for any distinct pair of indices $i \\neq j$. The product $X_i X_j$ is an indicator variable for the event that both $i$ and $j$ are fixed points.\n$$E[X_i X_j] = \\mathbb{P}(X_i=1 \\text{ and } X_j=1) = \\mathbb{P}(\\pi(i)=i \\text{ and } \\pi(j)=j)$$\nThe number of permutations where both $i$ and $j$ are fixed is the number of ways to permute the remaining $n-2$ elements, which is $(n-2)!$. This derivation assumes $n \\ge 2$.\n$$\\mathbb{P}(\\pi(i)=i \\text{ and } \\pi(j)=j) = \\frac{(n-2)!}{n!} = \\frac{1}{n(n-1)}$$\nThe number of ordered pairs $(i,j)$ with $i \\neq j$ is $n(n-1)$. The second sum is:\n$$\\sum_{i \\neq j} E[X_i X_j] = n(n-1) \\cdot \\frac{1}{n(n-1)} = 1$$\nThis is valid for $n \\ge 2$. For $n=1$, the sum over $i \\neq j$ is empty and equals $0$.\nCombining the terms:\n- For $n=1$: $E[F^2] = \\sum_{i=1}^1 E[X_1^2] = E[X_1] = 1/1 = 1$.\n- For $n \\ge 2$: $E[F^2] = 1 + 1 = 2$.\n\nNow we can compute the variance:\n- For $n=1$: $\\mathrm{Var}(F) = E[F^2] - (E[F])^2 = 1 - 1^2 = 0$. This is expected, as for $n=1$, $F$ is a constant ($F=1$).\n- For $n \\ge 2$: $\\mathrm{Var}(F) = E[F^2] - (E[F])^2 = 2 - 1^2 = 1$.\n\n**Higher Moments of F**\n\nTo derive higher moments, it is convenient to first compute the factorial moments, $E[(F)_k]$, where $(F)_k = F(F-1)\\cdots(F-k+1)$. The $k$-th factorial moment counts the expected number of ordered $k$-tuples of distinct fixed points.\n$$(F)_k = \\sum_{i_1 \\neq i_2 \\neq \\dots \\neq i_k} X_{i_1} X_{i_2} \\cdots X_{i_k}$$\nwhere the sum is over all ordered $k$-tuples of distinct indices. By linearity of expectation:\n$$E[(F)_k] = \\sum_{i_1 \\neq i_2 \\neq \\dots \\neq i_k} E[X_{i_1} X_{i_2} \\cdots X_{i_k}]$$\nThe term $E[X_{i_1} \\cdots X_{i_k}]$ is the probability that $i_1, \\dots, i_k$ are all fixed points. This requires $k \\le n$. If $k > n$, this probability is $0$.\nFor $k \\le n$, the number of permutations fixing $k$ specified elements is $(n-k)!$.\n$$\\mathbb{P}(\\pi(i_1)=i_1, \\dots, \\pi(i_k)=i_k) = \\frac{(n-k)!}{n!} = \\frac{1}{(n)_k}$$\nThe number of ordered $k$-tuples of distinct indices is $(n)_k = n(n-1)\\cdots(n-k+1)$.\nThus, for $1 \\le k \\le n$:\n$$E[(F)_k] = (n)_k \\cdot \\frac{1}{(n)_k} = 1$$\nIf $k > n$, it is impossible to have $k$ fixed points, so $(F)_k$ is always $0$, and $E[(F)_k] = 0$.\n\nRaw moments $E[F^k]$ can be expressed in terms of factorial moments using Stirling numbers of the second kind, denoted $S(k,j)$ or $\\{{k \\atop j}\\}$.\n$$F^k = \\sum_{j=0}^{k} S(k,j) (F)_j$$\nTaking the expectation of both sides:\n$$E[F^k] = \\sum_{j=0}^{k} S(k,j) E[(F)_j]$$\nSince $E[(F)_0]=1$ by definition and $E[(F)_j]=1$ for $1 \\le j \\le n$ and $0$ for $j>n$:\n$$E[F^k] = S(k,0) + \\sum_{j=1}^{\\min(k,n)} S(k,j)$$\nAs $S(k,0)=0$ for $k \\ge 1$, we have $E[F^k] = \\sum_{j=1}^{\\min(k,n)} S(k,j)$.\nThe required Stirling numbers are:\n- $S(1,1)=1$\n- $S(2,1)=1, S(2,2)=1$\n- $S(3,1)=1, S(3,2)=3, S(3,3)=1$\n- $S(4,1)=1, S(4,2)=7, S(4,3)=6, S(4,4)=1$\n\nUsing these, we can tabulate the raw moments for different $n$:\n- For $n=1$:\n  $E[F] = S(1,1) = 1$\n  $E[F^2] = S(2,1) = 1$\n  $E[F^3] = S(3,1) = 1$\n  $E[F^4] = S(4,1) = 1$\n- For $n=2$:\n  $E[F] = S(1,1) = 1$\n  $E[F^2] = S(2,1)+S(2,2) = 1+1=2$\n  $E[F^3] = S(3,1)+S(3,2) = 1+3=4$\n  $E[F^4] = S(4,1)+S(4,2) = 1+7=8$\n- For $n=3$:\n  $E[F^3] = S(3,1)+S(3,2)+S(3,3) = 1+3+1=5$\n  $E[F^4] = S(4,1)+S(4,2)+S(4,3) = 1+7+6=14$\n- For $n \\ge 4$:\n  $E[F^3] = 1+3+1=5$\n  $E[F^4] = 1+7+6+1=15$\n\nThe central moments $\\mu_k=E[(F-E[F])^k]=E[(F-1)^k]$ are derived from the raw moments:\n$\\mu_3 = E[(F-1)^3] = E[F^3] - 3E[F^2] + 3E[F] - 1$\n$\\mu_4 = E[(F-1)^4] = E[F^4] - 4E[F^3] + 6E[F^2] - 4E[F] + 1$\n\n**Summary of Theoretical Moments for Simulation**\n\n| $n$ | $E[F]=\\mu$ | $\\mathrm{Var}(F)=\\mu_2$ | $\\mu_3$ | $\\mu_4$ |\n|-----|------------|-------------------------|---------|---------|\n| 1   | 1          | 0                       | 0       | 0       |\n| 2   | 1          | 1                       | 0       | 1       |\n| 3   | 1          | 1                       | 1       | 3       |\n| $\\ge 4$ | 1      | 1                       | 1       | 4       |\n\nFor the specified test cases $(n=1, 2, 20, 100, 200)$, the theoretical moments are:\n- $n=1$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 0, 0, 0)$\n- $n=2$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 0, 1)$\n- $n=20$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 1, 4)$\n- $n=100$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 1, 4)$\n- $n=200$: $(\\mu, \\mu_2, \\mu_3, \\mu_4) = (1, 1, 1, 4)$\n\nThese theoretical values will be compared against empirical estimates from a computational simulation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef get_theoretical_moments(n):\n    \"\"\"\n    Computes the exact theoretical moments for the number of fixed points F\n    in a random permutation of n elements.\n\n    Returns:\n        A tuple (mu, mu2, mu3, mu4) containing the mean, variance,\n        3rd central moment, and 4th central moment.\n    \"\"\"\n    if not isinstance(n, int) or n < 1:\n        raise ValueError(\"n must be a positive integer.\")\n\n    mu = 1.0\n\n    if n == 1:\n        # F is deterministically 1. All central moments are 0.\n        mu2 = 0.0\n        mu3 = 0.0\n        mu4 = 0.0\n    elif n == 2:\n        # F is 0 or 2 with probability 1/2 each. E[F]=1.\n        # Var(F) = E[(F-1)^2] = (0-1)^2 * 0.5 + (2-1)^2 * 0.5 = 1.\n        # mu3 = E[(F-1)^3] = (-1)^3 * 0.5 + (1)^3 * 0.5 = 0.\n        # mu4 = E[(F-1)^4] = (-1)^4 * 0.5 + (1)^4 * 0.5 = 1.\n        mu2 = 1.0\n        mu3 = 0.0\n        mu4 = 1.0\n    elif n == 3:\n        # E[F]=1, E[F^2]=2, E[F^3]=5, E[F^4]=14.\n        mu2 = 2.0 - 1.0**2  # Var = E[F^2] - E[F]^2\n        mu3 = 5.0 - 3 * 2.0 + 3 * 1.0 - 1.0\n        mu4 = 14.0 - 4 * 5.0 + 6 * 2.0 - 4 * 1.0 + 1.0\n    else: # n >= 4\n        # E[F]=1, E[F^2]=2, E[F^3]=5, E[F^4]=15.\n        mu2 = 2.0 - 1.0**2\n        mu3 = 5.0 - 3 * 2.0 + 3 * 1.0 - 1.0\n        mu4 = 15.0 - 4 * 5.0 + 6 * 2.0 - 4 * 1.0 + 1.0\n    \n    return mu, mu2, mu3, mu4\n\ndef solve():\n    \"\"\"\n    Runs simulations to estimate moments of the number of fixed points\n    in random permutations and compares them to theoretical values.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 5000),\n        (2, 5000),\n        (20, 20000),\n        (100, 15000),\n        (200, 10000)\n    ]\n\n    # Define the tolerances from the problem statement.\n    tau_mu = 0.02\n    tau_v = 0.05\n    tau_3 = 0.15\n    tau_4 = 0.30\n    \n    # Use a fixed random seed for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for n, M in test_cases:\n        # Get exact theoretical values for the current n.\n        th_mu, th_mu2, th_mu3, th_mu4 = get_theoretical_moments(n)\n        \n        # Run M trials to collect samples of F.\n        fixed_points_samples = np.zeros(M, dtype=int)\n        indices = np.arange(n)\n        for i in range(M):\n            # Generate a uniformly random permutation.\n            perm = rng.permutation(n)\n            # Count the number of fixed points.\n            fixed_points = np.sum(perm == indices)\n            fixed_points_samples[i] = fixed_points\n            \n        # Calculate empirical estimates of the moments.\n        emp_mu = np.mean(fixed_points_samples)\n        \n        # Note: np.var computes E[(X-E[X])^2] using the sample mean, which is correct\n        # for estimating the variance as a central moment.\n        # ddof=0 (default) uses 1/M, not 1/(M-1).\n        emp_mu2 = np.var(fixed_points_samples)\n        \n        # scipy.stats.moment calculates sample central moments.\n        emp_mu3 = stats.moment(fixed_points_samples, moment=3)\n        emp_mu4 = stats.moment(fixed_points_samples, moment=4)\n        \n        # Perform the three boolean checks.\n        mean_check = abs(emp_mu - th_mu) <= tau_mu\n        var_check = abs(emp_mu2 - th_mu2) <= tau_v\n        \n        higher_moments_check = (abs(emp_mu3 - th_mu3) <= tau_3 and\n                                abs(emp_mu4 - th_mu4) <= tau_4)\n        \n        results.extend([mean_check, var_check, higher_moments_check])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from pure probability to the analysis of computational models, we now explore how moments help us understand model error. Numerical methods for solving differential equations are approximations, and their accuracy can depend on uncertain inputs. This exercise demonstrates how expectation and variance can quantify the error of common Ordinary Differential Equation (ODE) solvers when initial conditions are random, introducing you to the core principles of uncertainty propagation in scientific computing .",
            "id": "3126306",
            "problem": "A developer is tasked with quantitatively comparing the behavior of two numerical solvers applied to a linear Ordinary Differential Equation (ODE) under random initial conditions, using the foundational definitions of expectation, variance, and moments. Consider the scalar initial value problem defined by the ODE $ \\dfrac{dy}{dt} = -\\lambda y $ with random initial condition $ y(0) = Y_0 $, where $ \\lambda > 0 $ is a constant and $ Y_0 $ is a real-valued random variable with a specified distribution. The exact solution at final time $ T $ is $ y_{\\text{exact}}(T) = Y_0 \\exp(-\\lambda T) $. Two single-step numerical solvers are used to approximate the solution over $ N $ equal steps of size $ h = T/N $:\n\n- Solver A (Forward Euler): $ y_{n+1}^{(A)} = y_n^{(A)} + h f(t_n, y_n^{(A)}) $ with $ f(t, y) = -\\lambda y $.\n- Solver B (Explicit Midpoint, a second-order Runge–Kutta method): $ y_{n+1}^{(B)} = y_n^{(B)} + h f\\left(t_n + \\dfrac{h}{2}, y_n^{(B)} + \\dfrac{h}{2} f(t_n, y_n^{(B)})\\right) $ with $ f(t, y) = -\\lambda y $.\n\nDefine the final-time error random variables for each solver as $ E^{(A)} = y_N^{(A)} - y_{\\text{exact}}(T) $ and $ E^{(B)} = y_N^{(B)} - y_{\\text{exact}}(T) $. Using only the base definitions of expectation $ \\mathbb{E}[X] $, variance $ \\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] $, and central moments $ \\mu_k(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^k] $, derive and implement a computation of $ \\mathbb{E}[E^{(A)}] $, $ \\mathrm{Var}(E^{(A)}) $, $ \\mu_3(E^{(A)}) $, $ \\mu_4(E^{(A)}) $, and similarly for $ E^{(B)} $.\n\nYour program must use the following test suite of parameter sets, each specifying the distribution of $ Y_0 $, the ODE parameter $ \\lambda $, the final time $ T $, and the number of steps $ N $:\n\n- Test case $ 1 $ (general case): $ Y_0 \\sim \\mathcal{N}(\\mu, \\sigma^2) $ with $ \\mu = 2.0 $, $ \\sigma = 0.5 $, $ \\lambda = 1.2 $, $ T = 1.0 $, $ N = 20 $.\n- Test case $ 2 $ (different distribution and longer horizon): $ Y_0 \\sim \\mathrm{Uniform}[a,b] $ with $ a = 0.0 $, $ b = 4.0 $, $ \\lambda = 0.4 $, $ T = 3.0 $, $ N = 60 $.\n- Test case $ 3 $ (degenerate distribution): $ Y_0 $ is deterministic with value $ y_0 = 3.0 $, $ \\lambda = 0.9 $, $ T = 2.0 $, $ N = 40 $.\n- Test case $ 4 $ (boundary condition): $ Y_0 \\sim \\mathcal{N}(\\mu, \\sigma^2) $ with $ \\mu = 1.0 $, $ \\sigma = 0.2 $, $ \\lambda = 1.5 $, $ T = 0.0 $, $ N = 1 $.\n\nFor $ \\mathcal{N}(\\mu, \\sigma^2) $, $ \\mu $ and $ \\sigma $ denote the mean and the standard deviation, respectively. For $ \\mathrm{Uniform}[a,b] $, $ a $ and $ b $ denote the interval endpoints with $ a < b $. For the deterministic $ Y_0 $, treat $ Y_0 $ as a random variable that equals $ y_0 $ with probability $ 1 $. There are no physical units in this problem. Angles do not appear. Percentages must not be used; all numerical answers must be real numbers.\n\nYour program should compute, for each test case, the list $ [\\mathbb{E}[E^{(A)}], \\mathrm{Var}(E^{(A)}), \\mu_3(E^{(A)}), \\mu_4(E^{(A)}), \\mathbb{E}[E^{(B)}], \\mathrm{Var}(E^{(B)}), \\mu_3(E^{(B)}), \\mu_4(E^{(B)})] $ as floating-point values. The final output must be a single line containing the four results (one per test case) aggregated as a comma-separated list of these lists, enclosed in square brackets, with no spaces; for example, the format must be $[[x_1,\\dots,x_8],[x_1,\\dots,x_8],[x_1,\\dots,x_8],[x_1,\\dots,x_8]]$ where each $ x_i $ is a float.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and internally consistent. It poses a standard question in numerical analysis and uncertainty quantification, based on fundamental principles of ordinary differential equations and probability theory.\n\n### 1. Analysis of Numerical Solvers\nThe problem concerns the scalar linear ordinary differential equation (ODE):\n$$\n\\frac{dy}{dt} = -\\lambda y, \\quad y(0) = Y_0\n$$\nwhere $\\lambda > 0$ is a constant and the initial condition $Y_0$ is a random variable. The exact solution at time $T$ is given by $y_{\\text{exact}}(T) = Y_0 \\exp(-\\lambda T)$.\n\nWe analyze the two numerical solvers applied over $N$ steps of size $h = T/N$. Let $y_n$ be the approximation at time $t_n = n h$.\n\n**Solver A (Forward Euler):**\nThe update rule is $y_{n+1}^{(A)} = y_n^{(A)} + h f(t_n, y_n^{(A)})$. For $f(t, y) = -\\lambda y$, this becomes:\n$$\ny_{n+1}^{(A)} = y_n^{(A)} + h(-\\lambda y_n^{(A)}) = (1 - \\lambda h) y_n^{(A)}\n$$\nThis is a linear recurrence relation. Starting from the initial condition $y_0^{(A)} = Y_0$, the solution at the final step $N$ is obtained by repeated application:\n$$\ny_N^{(A)} = (1 - \\lambda h)^N Y_0\n$$\n\n**Solver B (Explicit Midpoint):**\nThe update rule is $y_{n+1}^{(B)} = y_n^{(B)} + h f\\left(t_n + \\frac{h}{2}, y_n^{(B)} + \\frac{h}{2} f(t_n, y_n^{(B)})\\right)$. Substituting $f(t, y) = -\\lambda y$:\n$$\ny_{n+1}^{(B)} = y_n^{(B)} + h \\left(-\\lambda \\left(y_n^{(B)} + \\frac{h}{2} (-\\lambda y_n^{(B)})\\right)\\right)\n$$\n$$\ny_{n+1}^{(B)} = y_n^{(B)} - \\lambda h \\left(y_n^{(B)} - \\frac{\\lambda h}{2} y_n^{(B)}\\right)\n$$\n$$\ny_{n+1}^{(B)} = y_n^{(B)} \\left(1 - \\lambda h + \\frac{(\\lambda h)^2}{2}\\right)\n$$\nThis is also a linear recurrence. Starting with $y_0^{(B)} = Y_0$, the solution at step $N$ is:\n$$\ny_N^{(B)} = \\left(1 - \\lambda h + \\frac{(\\lambda h)^2}{2}\\right)^N Y_0\n$$\n\n### 2. Derivation of Error Moments\nThe error random variables are defined as $E^{(A)} = y_N^{(A)} - y_{\\text{exact}}(T)$ and $E^{(B)} = y_N^{(B)} - y_{\\text{exact}}(T)$. From the derivations above, we can express these errors as a product of a deterministic constant and the initial condition random variable $Y_0$.\n\nLet's define the following deterministic constants for a given set of parameters $(\\lambda, T, N)$:\n- Amplification factor for Solver A: $C_A = (1 - \\lambda h)^N$\n- Amplification factor for Solver B: $C_B = \\left(1 - \\lambda h + \\frac{(\\lambda h)^2}{2}\\right)^N$\n- Amplification factor for the exact solution: $C_{\\text{exact}} = \\exp(-\\lambda T)$\n\nThe numerical and exact solutions can be written as:\n- $y_N^{(A)} = C_A Y_0$\n- $y_N^{(B)} = C_B Y_0$\n- $y_{\\text{exact}}(T) = C_{\\text{exact}} Y_0$\n\nThe error variables are therefore:\n- $E^{(A)} = (C_A - C_{\\text{exact}}) Y_0 = \\Delta_A Y_0$, where $\\Delta_A = C_A - C_{\\text{exact}}$\n- $E^{(B)} = (C_B - C_{\\text{exact}}) Y_0 = \\Delta_B Y_0$, where $\\Delta_B = C_B - C_{\\text{exact}}$\n\nLet $E$ represent either $E^{(A)}$ or $E^{(B)}$, and let $\\Delta$ be the corresponding constant $\\Delta_A$ or $\\Delta_B$. We have $E = \\Delta Y_0$. We can now compute the moments of $E$ using the moments of $Y_0$.\n\n**Expectation (1st Raw Moment):**\nUsing the linearity of the expectation operator:\n$$\n\\mathbb{E}[E] = \\mathbb{E}[\\Delta Y_0] = \\Delta \\mathbb{E}[Y_0]\n$$\n\n**Central Moments $\\mu_k(E)$ for $k \\ge 2$:**\nThe $k$-th central moment of $E$ is defined as $\\mu_k(E) = \\mathbb{E}[(E - \\mathbb{E}[E])^k]$.\n$$\nE - \\mathbb{E}[E] = \\Delta Y_0 - \\Delta \\mathbb{E}[Y_0] = \\Delta (Y_0 - \\mathbb{E}[Y_0])\n$$\nSubstituting this into the definition:\n$$\n\\mu_k(E) = \\mathbb{E}[(\\Delta (Y_0 - \\mathbb{E}[Y_0]))^k] = \\mathbb{E}[\\Delta^k (Y_0 - \\mathbb{E}[Y_0])^k]\n$$\nSince $\\Delta^k$ is a constant, it can be factored out of the expectation:\n$$\n\\mu_k(E) = \\Delta^k \\mathbb{E}[(Y_0 - \\mathbb{E}[Y_0])^k] = \\Delta^k \\mu_k(Y_0)\n$$\nwhere $\\mu_k(Y_0)$ is the $k$-th central moment of the initial condition $Y_0$.\n\nThis single general formula allows us to compute the required statistics:\n- **Variance ($k=2$):** $\\mathrm{Var}(E) = \\mu_2(E) = \\Delta^2 \\mu_2(Y_0) = \\Delta^2 \\mathrm{Var}(Y_0)$\n- **3rd Central Moment ($k=3$):** $\\mu_3(E) = \\Delta^3 \\mu_3(Y_0)$\n- **4th Central Moment ($k=4$):** $\\mu_4(E) = \\Delta^4 \\mu_4(Y_0)$\n\n### 3. Moments of Initial Condition Distributions\nWe need the first four central moments of $Y_0$ for the specified distributions. Let $\\mathbb{E}[Y_0] = \\mu_{Y_0}$.\n\n**Normal Distribution: $Y_0 \\sim \\mathcal{N}(\\mu, \\sigma^2)$**\n- $\\mathbb{E}[Y_0] = \\mu$\n- $\\mathrm{Var}(Y_0) = \\mu_2(Y_0) = \\sigma^2$\n- $\\mu_3(Y_0) = 0$ (due to symmetry)\n- $\\mu_4(Y_0) = 3\\sigma^4$\n\n**Uniform Distribution: $Y_0 \\sim \\mathrm{Uniform}[a,b]$**\n- $\\mathbb{E}[Y_0] = \\frac{a+b}{2}$\n- $\\mathrm{Var}(Y_0) = \\mu_2(Y_0) = \\frac{(b-a)^2}{12}$\n- $\\mu_3(Y_0) = 0$ (due to symmetry)\n- $\\mu_4(Y_0) = \\frac{(b-a)^4}{80}$\n\n**Deterministic Value: $Y_0 = y_0$**\nThis is a degenerate random variable.\n- $\\mathbb{E}[Y_0] = y_0$\n- $\\mathrm{Var}(Y_0) = \\mathbb{E}[(Y_0 - y_0)^2] = \\mathbb{E}[(y_0 - y_0)^2] = 0$\n- $\\mu_k(Y_0) = 0$ for all $k \\ge 2$.\n\n### 4. Algorithmic Procedure\nFor each test case, the following steps are executed:\n1.  Extract the parameters: ODE parameter $\\lambda$, final time $T$, number of steps $N$, and the parameters of the distribution of $Y_0$.\n2.  Calculate the step size $h = T/N$. If $T=0$, then $h=0$.\n3.  Compute the moments of the initial condition $Y_0$ ($\\mathbb{E}[Y_0]$, $\\mathrm{Var}(Y_0)$, $\\mu_3(Y_0)$, $\\mu_4(Y_0)$) based on its distribution type.\n4.  Compute the deterministic constants $\\Delta_A = (1 - \\lambda h)^N - \\exp(-\\lambda T)$ and $\\Delta_B = \\left(1 - \\lambda h + \\frac{(\\lambda h)^2}{2}\\right)^N - \\exp(-\\lambda T)$.\n5.  Apply the derived moment formulas to calculate the eight required values for the errors $E^{(A)}$ and $E^{(B)}$:\n    - $\\mathbb{E}[E^{(A)}] = \\Delta_A \\mathbb{E}[Y_0]$\n    - $\\mathrm{Var}(E^{(A)}) = \\Delta_A^2 \\mathrm{Var}(Y_0)$\n    - $\\mu_3(E^{(A)}) = \\Delta_A^3 \\mu_3(Y_0)$\n    - $\\mu_4(E^{(A)}) = \\Delta_A^4 \\mu_4(Y_0)$\n    - $\\mathbb{E}[E^{(B)}] = \\Delta_B \\mathbb{E}[Y_0]$\n    - $\\mathrm{Var}(E^{(B)}) = \\Delta_B^2 \\mathrm{Var}(Y_0)$\n    - $\\mu_3(E^{(B)}) = \\Delta_B^3 \\mu_3(Y_0)$\n    - $\\mu_4(E^{(B)}) = \\Delta_B^4 \\mu_4(Y_0)$\n6.  Collect and format these eight values into a list for the final output.\n\nThis procedure provides a complete analytical solution to the problem, directly implemented in the accompanying program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes moments of the numerical error for two ODE solvers under\n    random initial conditions for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {'dist': 'normal', 'params': {'mu': 2.0, 'sigma': 0.5}, 'lambda': 1.2, 'T': 1.0, 'N': 20},\n        {'dist': 'uniform', 'params': {'a': 0.0, 'b': 4.0}, 'lambda': 0.4, 'T': 3.0, 'N': 60},\n        {'dist': 'deterministic', 'params': {'y0': 3.0}, 'lambda': 0.9, 'T': 2.0, 'N': 40},\n        {'dist': 'normal', 'params': {'mu': 1.0, 'sigma': 0.2}, 'lambda': 1.5, 'T': 0.0, 'N': 1},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        lam = case['lambda']\n        T = case['T']\n        N = case['N']\n        dist = case['dist']\n        params = case['params']\n        \n        # Step 1: Calculate the moments of the initial condition Y_0\n        mu_Y0 = 0.0\n        var_Y0 = 0.0\n        mu3_Y0 = 0.0\n        mu4_Y0 = 0.0\n        \n        if dist == 'normal':\n            mu = params['mu']\n            sigma = params['sigma']\n            mu_Y0 = mu\n            var_Y0 = sigma**2\n            mu3_Y0 = 0.0  # Normal distribution is symmetric\n            mu4_Y0 = 3.0 * (sigma**4)\n        elif dist == 'uniform':\n            a = params['a']\n            b = params['b']\n            mu_Y0 = (a + b) / 2.0\n            var_Y0 = (b - a)**2 / 12.0\n            mu3_Y0 = 0.0  # Uniform distribution is symmetric\n            mu4_Y0 = (b - a)**4 / 80.0\n        elif dist == 'deterministic':\n            y0 = params['y0']\n            mu_Y0 = y0\n            # All central moments are zero for a deterministic variable\n            var_Y0 = 0.0\n            mu3_Y0 = 0.0\n            mu4_Y0 = 0.0\n            \n        # Step 2: Calculate the deterministic constants\n        # Step size h = T/N\n        # Note: If T=0, h will be 0, which is correct.\n        h = T / N if N != 0 else 0.0\n\n        # Amplification factors\n        # For Solver A (Forward Euler)\n        C_A = (1.0 - lam * h)**N\n        # For Solver B (Explicit Midpoint)\n        C_B = (1.0 - lam * h + 0.5 * (lam * h)**2)**N\n        # For the exact solution\n        C_exact = np.exp(-lam * T)\n        \n        # Error coefficients\n        Delta_A = C_A - C_exact\n        Delta_B = C_B - C_exact\n        \n        # Step 3: Calculate the moments of the error variables\n        # E = Delta * Y0\n        # E[E] = Delta * E[Y0]\n        # mu_k(E) = Delta^k * mu_k(Y0)\n        \n        # Solver A results\n        E_A = Delta_A * mu_Y0\n        Var_A = (Delta_A**2) * var_Y0\n        mu3_A = (Delta_A**3) * mu3_Y0\n        mu4_A = (Delta_A**4) * mu4_Y0\n        \n        # Solver B results\n        E_B = Delta_B * mu_Y0\n        Var_B = (Delta_B**2) * var_Y0\n        mu3_B = (Delta_B**3) * mu3_Y0\n        mu4_B = (Delta_B**4) * mu4_Y0\n        \n        case_results = [E_A, Var_A, mu3_A, mu4_A, E_B, Var_B, mu3_B, mu4_B]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The format requires no spaces after commas inside the lists.\n    # e.g., [[x1,...,x8],[x1,...,x8],...]\n    formatted_sublists = []\n    for res_list in all_results:\n        inner_str = \"[\" + \",\".join(map(str, res_list)) + \"]\"\n        formatted_sublists.append(inner_str)\n    \n    final_output = \"[\" + \",\".join(formatted_sublists) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "To gain deeper insight into complex systems, we must understand how different sources of randomness contribute to overall uncertainty. This final practice introduces the Law of Total Variance, a powerful theorem for decomposing the variance of a variable in a hierarchical model. By simulating a two-level generative process, you will empirically verify this law, learning how to attribute variability to different stages of a model, a critical skill in diagnosing and understanding complex simulations .",
            "id": "3126385",
            "problem": "You are to implement a complete, runnable program that uses a hierarchical Monte Carlo (MC) simulation to explore variance decomposition for a random variable defined by a two-level generative process. The model is specified by the conditional distribution and the prior: for a latent variable $Y$ and an observed variable $X$, the conditional distribution is $X \\mid Y \\sim \\mathcal{N}(Y, 1)$ and the prior is $Y \\sim \\mathrm{Uniform}(0,1)$. Using first principles of expectation and variance, your program must compute numerical approximations of $E[X]$, $\\mathrm{Var}(X)$, and the decomposition quantity $E[\\mathrm{Var}(X \\mid Y)] + \\mathrm{Var}(E[X \\mid Y])$, relying solely on definitions of empirical expectation and variance over simulated samples.\n\nImplement the following procedure for a hierarchical Monte Carlo approximation:\n- Draw $N_{\\text{outer}}$ independent samples $Y_i$ from $\\mathrm{Uniform}(0,1)$.\n- For each $i$, draw $N_{\\text{inner}}$ independent samples $X_{i,j}$ from $\\mathcal{N}(Y_i, 1)$.\n- Define the empirical mean as\n$$\n\\widehat{m} \\;=\\; \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j}.\n$$\n- Define the empirical (population) variance of all $X_{i,j}$ as\n$$\n\\widehat{v} \\;=\\; \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} \\left(X_{i,j} - \\widehat{m}\\right)^2.\n$$\n- For each $i$, define the group mean\n$$\n\\widehat{m}_i \\;=\\; \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j},\n$$\nand the group (population) variance\n$$\n\\widehat{v}_i \\;=\\; \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} \\left(X_{i,j} - \\widehat{m}_i\\right)^2.\n$$\n- Define the hierarchical components as\n$$\n\\widehat{E[\\mathrm{Var}(X \\mid Y)]} \\;=\\; \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\widehat{v}_i,\n\\qquad\n\\widehat{\\mathrm{Var}(E[X \\mid Y])} \\;=\\; \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\left(\\widehat{m}_i - \\widehat{m}\\right)^2.\n$$\nUsing these definitions with denominators equal to the number of terms (that is, population second central moments), the exact empirical identity\n$$\n\\widehat{v} \\;=\\; \\widehat{E[\\mathrm{Var}(X \\mid Y)]} \\;+\\; \\widehat{\\mathrm{Var}(E[X \\mid Y])}\n$$\nholds for any finite $N_{\\text{outer}}$ and $N_{\\text{inner}}$ up to numerical roundoff.\n\nYour task:\n1. Implement the hierarchical Monte Carlo simulation exactly as described above, using a fixed random seed $s = 123456789$ for reproducibility.\n2. For each specified test case $(N_{\\text{outer}}, N_{\\text{inner}})$, compute four quantities in order:\n   - The empirical mean $\\widehat{m}$,\n   - The empirical variance $\\widehat{v}$,\n   - The sum $\\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}$,\n   - The absolute difference $\\left|\\widehat{v} - \\left(\\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}\\right)\\right|$.\n3. Use the following test suite of parameter pairs $(N_{\\text{outer}}, N_{\\text{inner}})$:\n   - Case $1$: $(50000, 5)$,\n   - Case $2$: $(5, 50000)$,\n   - Case $3$: $(10000, 100)$,\n   - Case $4$: $(2, 2)$.\n\nFinal output format:\n- Your program should produce a single line of output containing a single comma-separated list enclosed in square brackets.\n- The list must contain, in order, the four results for Case $1$, followed by the four results for Case $2$, followed by the four results for Case $3$, followed by the four results for Case $4$. That is, the flat list\n[\\widehat{m}_1, \\widehat{v}_1, \\widehat{t}_1, \\widehat{d}_1, ..., \\widehat{d}_4],\nwhere $\\widehat{t}_k = \\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}$ and $\\widehat{d}_k = \\left|\\widehat{v}_k - \\widehat{t}_k\\right|$ for case $k \\in \\{1,2,3,4\\}$. Each entry must be a real number.",
            "solution": "The user-provided problem is a valid exercise in computational science and statistics. It is scientifically grounded, well-posed, objective, and complete. It asks for the implementation of a hierarchical Monte Carlo simulation to numerically verify the law of total variance for a specific generative model. The problem is sound, and I will proceed with a full solution.\n\nThe problem centers on a hierarchical model where a random variable $X$ depends on a latent variable $Y$. The distributions are given as:\n1.  Prior distribution for the latent variable: $Y \\sim \\mathrm{Uniform}(0, 1)$\n2.  Conditional distribution for the observed variable: $X \\mid Y \\sim \\mathcal{N}(Y, 1)$, a normal distribution with mean $Y$ and variance $1$.\n\nThe primary goal is to compute numerical estimates for the expectation $E[X]$, the variance $\\mathrm{Var}(X)$, and to verify the law of total variance, which states:\n$$\n\\mathrm{Var}(X) = E[\\mathrm{Var}(X \\mid Y)] + \\mathrm{Var}(E[X \\mid Y])\n$$\nWe can first determine the theoretical values for these quantities.\n\n**Theoretical Analysis**\n\n1.  **Expectation of $X$**: Using the law of total expectation, $E[X] = E[E[X \\mid Y]]$.\n    The conditional expectation $E[X \\mid Y]$ is the mean of the normal distribution $\\mathcal{N}(Y, 1)$, which is simply $Y$.\n    Therefore, $E[X] = E[Y]$. Since $Y$ is drawn from a $\\mathrm{Uniform}(0, 1)$ distribution, its expectation is $E[Y] = \\frac{0+1}{2} = \\frac{1}{2}$.\n    Thus, the theoretical expectation of $X$ is $E[X] = 0.5$.\n\n2.  **Variance of $X$**: Using the law of total variance, we evaluate its two components.\n    *   **Inner variance term**: $E[\\mathrm{Var}(X \\mid Y)]$.\n        The conditional variance $\\mathrm{Var}(X \\mid Y)$ is the variance of the normal distribution $\\mathcal{N}(Y, 1)$, which is the constant $1$, regardless of the value of $Y$.\n        Thus, the expectation of this constant is $E[\\mathrm{Var}(X \\mid Y)] = E[1] = 1$.\n    *   **Outer variance term**: $\\mathrm{Var}(E[X \\mid Y])$.\n        We already established that $E[X \\mid Y] = Y$. So, this term becomes $\\mathrm{Var}(Y)$.\n        For a $\\mathrm{Uniform}(a, b)$ distribution, the variance is $\\frac{(b-a)^2}{12}$. For $Y \\sim \\mathrm{Uniform}(0, 1)$, the variance is $\\mathrm{Var}(Y) = \\frac{(1-0)^2}{12} = \\frac{1}{12}$.\n    *   **Total variance**: Combining the two components gives the total variance of $X$:\n        $$\n        \\mathrm{Var}(X) = 1 + \\frac{1}{12} = \\frac{13}{12} \\approx 1.08333...\n        $$\n\n**Hierarchical Monte Carlo Simulation**\n\nThe simulation procedure approximates these theoretical quantities by generating a large number of samples from the specified distributions. The two-level structure of the model is mirrored in the simulation design:\n\n1.  **Outer Loop**: $N_{\\text{outer}}$ independent samples of the latent variable, $Y_1, Y_2, \\ldots, Y_{N_{\\text{outer}}}$, are drawn from $Y \\sim \\mathrm{Uniform}(0, 1)$.\n2.  **Inner Loop**: For each sampled value $Y_i$, a set of $N_{\\text{inner}}$ independent samples of the observed variable, $X_{i,1}, X_{i,2}, \\ldots, X_{i,N_{\\text{inner}}}$, are drawn from the conditional distribution $X \\mid Y=Y_i \\sim \\mathcal{N}(Y_i, 1)$.\n\nThis process yields a total of $N_{\\text{outer}} \\times N_{\\text{inner}}$ samples of $X$. From this dataset, we compute empirical estimates for the quantities of interest.\n\n**Empirical Estimators**\n\nThe problem provides explicit formulas for the estimators, which are defined as population moments (i.e., using a denominator of $N$, not $N-1$).\n\n*   The overall empirical mean, $\\widehat{m}$, estimates $E[X]$:\n    $$\n    \\widehat{m} = \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j}\n    $$\n*   The overall empirical variance, $\\widehat{v}$, estimates $\\mathrm{Var}(X)$:\n    $$\n    \\widehat{v} = \\frac{1}{N_{\\text{outer}} N_{\\text{inner}}} \\sum_{i=1}^{N_{\\text{outer}}} \\sum_{j=1}^{N_{\\text{inner}}} (X_{i,j} - \\widehat{m})^2\n    $$\n*   To estimate the components of the total variance, we first compute statistics for each group $i$:\n    *   Group mean: $\\widehat{m}_i = \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} X_{i,j}$, which estimates $E[X \\mid Y=Y_i]$.\n    *   Group variance: $\\widehat{v}_i = \\frac{1}{N_{\\text{inner}}} \\sum_{j=1}^{N_{\\text{inner}}} (X_{i,j} - \\widehat{m}_i)^2$, which estimates $\\mathrm{Var}(X \\mid Y=Y_i)$.\n*   The terms in the law of total variance are then estimated by averaging these group statistics over the $N_{\\text{outer}}$ samples:\n    *   Estimate for $E[\\mathrm{Var}(X \\mid Y)]$: $\\widehat{E[\\mathrm{Var}(X \\mid Y)]} = \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\widehat{v}_i$\n    *   Estimate for $\\mathrm{Var}(E[X \\mid Y)]$: $\\widehat{\\mathrm{Var}(E[X \\mid Y])} = \\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} (\\widehat{m}_i - \\widehat{m})^2$\n    Note that the mean of the group means, $\\frac{1}{N_{\\text{outer}}} \\sum_{i=1}^{N_{\\text{outer}}} \\widehat{m}_i$, is algebraically equivalent to the overall mean $\\widehat{m}$.\n\n**The Empirical Identity**\n\nA key feature of this problem is that, due to the specific definitions of the empirical estimators as population variances, the law of total variance holds as an *exact algebraic identity* for the finite sample, not just as an asymptotic limit. We can show this by decomposing the total sum of squares:\n$$\n\\sum_{i,j} (X_{i,j} - \\widehat{m})^2 = \\sum_{i,j} (X_{i,j} - \\widehat{m}_i + \\widehat{m}_i - \\widehat{m})^2\n$$\nExpanding the square and noting that the cross-term $\\sum_{i,j} 2(X_{i,j} - \\widehat{m}_i)(\\widehat{m}_i - \\widehat{m})$ vanishes because $\\sum_{j} (X_{i,j} - \\widehat{m}_i) = 0$ for each $i$, we get:\n$$\n\\sum_{i,j} (X_{i,j} - \\widehat{m})^2 = \\sum_{i,j} (X_{i,j} - \\widehat{m}_i)^2 + \\sum_{i,j} (\\widehat{m}_i - \\widehat{m})^2\n$$\n$$\n\\sum_{i,j} (X_{i,j} - \\widehat{m})^2 = \\sum_i \\left( N_{\\text{inner}} \\widehat{v}_i \\right) + \\sum_i \\left( N_{\\text{inner}} (\\widehat{m}_i - \\widehat{m})^2 \\right)\n$$\nDividing by the total number of samples, $N_{\\text{outer}}N_{\\text{inner}}$, yields the identity:\n$$\n\\widehat{v} = \\frac{1}{N_{\\text{outer}}} \\sum_i \\widehat{v}_i + \\frac{1}{N_{\\text{outer}}} \\sum_i (\\widehat{m}_i - \\widehat{m})^2 = \\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}\n$$\nTherefore, the calculated absolute difference $\\left|\\widehat{v} - \\left(\\widehat{E[\\mathrm{Var}(X \\mid Y)]} + \\widehat{\\mathrm{Var}(E[X \\mid Y])}\\right)\\right|$ should be zero, subject only to floating-point precision limitations.\n\n**Implementation Details**\n\nThe implementation uses the `numpy` library for efficient, vectorized operations.\n1.  A random number generator is initialized with the specified seed $s=123456789$ for reproducibility.\n2.  For each test case $(N_{\\text{outer}}, N_{\\text{inner}})$, an array of $Y_i$ samples is generated using `rng.uniform`.\n3.  The corresponding $X_{i,j}$ samples are generated using `rng.normal`. `numpy`'s broadcasting feature allows us to provide the array of $Y_i$ values as the means, creating the `(N_outer, N_inner)` array of $X$ samples in a single, efficient call.\n4.  The estimators $\\widehat{m}$, $\\widehat{v}$, $\\widehat{m}_i$, and $\\widehat{v}_i$ are computed using `numpy.mean` and `numpy.var`. The `axis=1` argument is crucial for calculating the group statistics across the rows of the $X$ sample matrix. `numpy.var` computes the population variance by default (denominator $N$), which aligns perfectly with the problem's definitions.\n5.  The final quantities for each test case—the empirical mean $\\widehat{m}$, empirical variance $\\widehat{v}$, the sum of its decomposed components $\\widehat{t}$, and their absolute difference $\\widehat{d}$—are calculated and stored.\n\nThis procedure is repeated for all specified test cases, and the results are aggregated into a single list for output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy  # Imported to adhere to the specified environment, but not used.\n\ndef solve():\n    \"\"\"\n    Implements a hierarchical Monte Carlo simulation to verify the law of total variance.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    s = 123456789\n    rng = np.random.default_rng(s)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50000, 5),     # Case 1\n        (5, 50000),     # Case 2\n        (10000, 100),   # Case 3\n        (2, 2)          # Case 4\n    ]\n\n    results = []\n    for N_outer, N_inner in test_cases:\n        # Step 1: Draw N_outer independent samples from Uniform(0,1) for the latent variable Y.\n        # These will serve as the means for the normal distributions.\n        y_samples = rng.uniform(low=0.0, high=1.0, size=N_outer)\n\n        # Step 2: For each Y_i, draw N_inner samples from Normal(Y_i, 1).\n        # We use numpy broadcasting: y_samples is (N_outer,), we make it (N_outer, 1)\n        # so it broadcasts across the N_inner dimension.\n        # The scale (standard deviation) is sqrt(1) = 1.\n        x_samples = rng.normal(loc=y_samples[:, np.newaxis], scale=1.0, size=(N_outer, N_inner))\n\n        # Step 3: Compute the overall empirical mean and variance of all X samples.\n        # numpy.mean and numpy.var (with default ddof=0) compute the population statistics\n        # as required by the problem statement.\n        m_hat = np.mean(x_samples)\n        v_hat = np.var(x_samples)\n\n        # Step 4: Compute the group-wise statistics.\n        # m_i_hat: mean of each group i (mean of each row).\n        # v_i_hat: variance of each group i (variance of each row).\n        # The `axis=1` argument performs the calculation along each row.\n        m_i_hat = np.mean(x_samples, axis=1)  # Resulting shape: (N_outer,)\n        v_i_hat = np.var(x_samples, axis=1)   # Resulting shape: (N_outer,)\n\n        # Step 5: Compute the hierarchical components based on the problem definitions.\n        # E[Var(X|Y)] is estimated by the mean of the group variances.\n        E_var_hat = np.mean(v_i_hat)\n        \n        # Var(E[X|Y]) is estimated by the variance of the group means.\n        # The mean of m_i_hat is algebraically identical to the overall mean m_hat.\n        # Thus, np.var(m_i_hat) correctly computes the required quantity.\n        var_E_hat = np.var(m_i_hat)\n\n        # Step 6: Compute the sum of the components (t_hat) and the absolute difference (d_hat).\n        # This verifies the empirical identity.\n        t_hat = E_var_hat + var_E_hat\n        d_hat = np.abs(v_hat - t_hat)\n\n        # Append the four required quantities for this test case to the results list.\n        results.extend([m_hat, v_hat, t_hat, d_hat])\n\n    # Final print statement in the exact required format.\n    # map(str, results) ensures default float-to-string conversion.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}