{
    "hands_on_practices": [
        {
            "introduction": "Before trusting any simulation, we must first verify that our code correctly implements the intended mathematical model. Property-based testing is a powerful paradigm for this, where we check if fundamental invariants like conservation laws or symmetries hold true across a wide range of random inputs. This exercise  provides hands-on practice with this technique by challenging you to test a heat equation solver against its core physical and mathematical properties, and to see how a seemingly small bug can cause those properties to fail.",
            "id": "3109343",
            "problem": "Design and implement a self-contained program that demonstrates the computational science paradigm of validating scientific code via property-based tests. The program must instantiate a simple, physically motivated numerical model and use randomly generated inputs to test whether core invariants are respected. The model, invariants, and testing strategy are specified below.\n\nModel. Consider the one-dimensional heat equation (also known as the diffusion equation) on a periodic discrete grid with $N$ points, whose continuum form is $u_t = D u_{xx}$, where $u$ denotes a scalar field and $D$ is a diffusion coefficient. Use a standard explicit forward-Euler finite-difference time-advance with a symmetric three-point stencil, periodic boundary conditions, and non-dimensionalized parameters. Define the discrete state as a vector $u \\in \\mathbb{R}^N$ and perform one time step\n$$\nu_i^{\\text{new}} \\;=\\; u_i^{\\text{old}} \\;+\\; \\alpha \\,\\big(u_{i-1}^{\\text{old}} \\;-\\; 2\\,u_i^{\\text{old}} \\;+\\; u_{i+1}^{\\text{old}}\\big)\n$$\nfor all indices $i \\in \\{0,1,\\dots,N-1\\}$ with periodic indexing, that is $u_{-1} \\equiv u_{N-1}$ and $u_{N} \\equiv u_0$. The scalar $\\alpha \\ge 0$ encodes the non-dimensionalized time step and diffusion constant. This model is a canonical instance used across computational science for studying conservation, symmetry, and monotonicity properties.\n\nFundamental base and invariants. The following invariants arise from core physical and mathematical principles and must be used to formulate property-based tests:\n- Conservation of total quantity (mass). For periodic boundaries and the symmetric stencil above, the discrete total $S = \\sum_{i=0}^{N-1} u_i$ should be conserved under the update step, reflecting the physical conservation of mass in the absence of sources and sinks.\n- Symmetry equivariance under spatial reflection. Define the discrete reflection operator $\\mathcal{R}$ by $(\\mathcal{R}u)_i = u_{N-1-i}$. The stencil is symmetric, so performing the step on the reflected state and then reflecting the result should commute:\n$$\n\\text{step}(\\mathcal{R}u) \\;=\\; \\mathcal{R}(\\text{step}(u)).\n$$\n- Monotonicity of the range. For $0 \\le \\alpha \\le \\tfrac{1}{2}$, the update is a nonnegative convex combination of $u_{i-1}$, $u_i$, and $u_{i+1}$; consequently, the discrete range $R(u) = \\max_i u_i - \\min_i u_i$ should be non-increasing under the step (i.e., $R(u^{\\text{new}}) \\le R(u^{\\text{old}})$). This captures the smoothing character of diffusion.\n\nTesting strategy. Implement property-based tests by generating random states $u$ and automatically checking the invariants above. For numerical verification, adopt a tolerance $\\varepsilon = 10^{-10}$ and define violations as follows:\n- Conservation violation if $\\big|\\sum_i u_i^{\\text{new}} - \\sum_i u_i^{\\text{old}}\\big| > \\varepsilon$.\n- Symmetry equivariance violation if $\\max_i \\big|(\\text{step}(\\mathcal{R}u))_i - (\\mathcal{R}(\\text{step}(u)))_i\\big| > \\varepsilon$.\n- Monotonicity violation if $R(u^{\\text{new}}) - R(u^{\\text{old}}) > \\varepsilon$.\n\nDefect injection. To emulate code defects, you must implement two variants of the step:\n- A correct periodic implementation using circular shifts (i.e., periodic neighbors).\n- A buggy implementation that mistakenly uses zero boundary conditions at the ends, that is, neighbors outside $\\{0,\\dots,N-1\\}$ are taken as $0$ instead of wrapping around. This defect should break conservation and symmetry and may also affect monotonicity.\n\nRandom input generation. For each test case, draw $T$ independent random states $u$ with components sampled from a uniform distribution on $[0,1]$. Additionally, if a test case specifies “include zero,” insert one extra trial consisting of the all-zero state $u = 0$ to probe trivial invariant satisfaction. Use the provided seeds for reproducibility.\n\nTest suite. Your program must evaluate the following test cases, each given as a tuple $(N, \\alpha, T, \\text{seed}, \\text{mode}, \\text{include\\_zero})$:\n- Case $1$: $(64, 0.2, 200, 101, \\text{`correct`}, \\text{True})$.\n- Case $2$: $(64, 0.49, 200, 102, \\text{`correct`}, \\text{True})$.\n- Case $3$: $(64, 0.6, 200, 103, \\text{`correct`}, \\text{False})$.\n- Case $4$: $(64, 0.2, 200, 104, \\text{`buggy`}, \\text{True})$.\n- Case $5$: $(1, 0.2, 50, 105, \\text{`correct`}, \\text{True})$.\n- Case $6$: $(32, 0.2, 200, 106, \\text{`buggy`}, \\text{False})$.\n\nRequired outputs. For each test case, aggregate the number of violations observed over all its trials for the three invariants, in the fixed order: conservation, symmetry equivariance, monotonicity. Express each test case’s result as a list of three integers `[c,s,m]`, where `c` is the conservation violation count, `s` is the symmetry equivariance violation count, and `m` is the monotonicity violation count. Your program must produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, i.e., a Python-style list of lists. The line should therefore represent a list of $6$ elements, each of which is a list of three integers in the order described.\n\nAngle units are not applicable. No physical unit conversion is required.\n\nScientific realism. The model, invariants, and tests arise from widely used numerical analysis and physical principles. The chosen parameters are within plausible ranges. The randomized trials emulate property-based testing common in computational science.\n\nYour solution must derive, implement, and test these properties without relying on shortcut formulas not justified by the fundamental base above. The final answer must be a complete, runnable program that produces the specified single-line output.",
            "solution": "The problem requires the design and implementation of a property-based testing framework to validate a numerical model of the one-dimensional heat equation. This involves implementing the numerical scheme, formally defining its fundamental invariants, and then systematically testing for violations of these invariants using randomly generated data.\n\n### 1. Numerical Model and Implementations\n\nThe physical system is governed by the heat equation, $u_t = D u_{xx}$, on a periodic domain. We discretize this system on a grid of $N$ points, with the state represented by a vector $u \\in \\mathbb{R}^N$. The time evolution is modeled using an explicit forward-Euler method with a centered three-point stencil for the spatial second derivative. The update rule for each grid point $u_i$ is given by:\n$$\nu_i^{\\text{new}} = u_i^{\\text{old}} + \\alpha \\left(u_{i-1}^{\\text{old}} - 2u_i^{\\text{old}} + u_{i+1}^{\\text{old}}\\right)\n$$\nwhere $\\alpha \\ge 0$ is a non-dimensional parameter combining the time step and diffusion coefficient. Indices are handled periodically, meaning $u_{-1} \\equiv u_{N-1}$ and $u_N \\equiv u_0$.\n\nThis update can be expressed in a vectorized form suitable for numerical computation. The term in the parenthesis is the discrete Laplacian, $\\Delta_d u$.\n\n**Correct Implementation (Periodic Boundaries):**\nFor periodic boundaries, the neighbors of $u_i$ are $u_{(i-1) \\pmod N}$ and $u_{(i+1) \\pmod N}$. This operation can be efficiently implemented using circular shifts on the state vector $u$. Let $u^{\\text{old}}$ be the vector of state values at the previous time step. The new state vector $u^{\\text{new}}$ is:\n$$\nu^{\\text{new}} = u^{\\text{old}} + \\alpha \\left( \\text{shift}(u^{\\text{old}}, 1) - 2u^{\\text{old}} + \\text{shift}(u^{\\text{old}}, -1) \\right)\n$$\nwhere $\\text{shift}(u, k)_i = u_{(i-k) \\pmod N}$. This corresponds to `np.roll` in NumPy.\n\n**Buggy Implementation (Zero Boundaries):**\nA common defect is the incorrect handling of boundary conditions. The buggy implementation assumes zero-flux boundaries by setting ghost-cell values to zero. For $i=0$, the neighbor $u_{-1}$ is taken as $0$. For $i=N-1$, the neighbor $u_N$ is taken as $0$. The update rules at the boundaries become:\n$$\nu_0^{\\text{new}} = u_0^{\\text{old}} + \\alpha \\left(0 - 2u_0^{\\text{old}} + u_1^{\\text{old}}\\right)\n$$\n$$\nu_{N-1}^{\\text{new}} = u_{N-1}^{\\text{old}} + \\alpha \\left(u_{N-2}^{\\text{old}} - 2u_{N-1}^{\\text{old}} + 0\\right)\n$$\nInterior points $i \\in \\{1, \\dots, N-2\\}$ are updated using the standard stencil.\n\n### 2. Invariants and Property-Based Tests\n\nWe validate the implementations against three fundamental properties derived from the underlying physics and mathematics.\n\n**A. Conservation of Total Quantity**\nThis property reflects the conservation of mass or energy in a closed system. The total quantity is the sum $S = \\sum_{i=0}^{N-1} u_i$.\nFor the **correct** periodic implementation, let's sum the update rule over all $i$:\n$$\n\\sum_{i=0}^{N-1} u_i^{\\text{new}} = \\sum_{i=0}^{N-1} u_i^{\\text{old}} + \\alpha \\sum_{i=0}^{N-1} \\left(u_{i-1}^{\\text{old}} - 2u_i^{\\text{old}} + u_{i+1}^{\\text{old}}\\right)\n$$\nThe last term is a discrete telescoping sum under periodic boundaries:\n$$\n\\sum_{i=0}^{N-1} (u_{i-1} - 2u_i + u_{i+1}) = \\sum u_{i-1} - 2\\sum u_i + \\sum u_{i+1} = S^{\\text{old}} - 2S^{\\text{old}} + S^{\\text{old}} = 0\n$$\nThus, $\\sum u_i^{\\text{new}} = \\sum u_i^{\\text{old}}$. The total sum is an invariant.\nFor the **buggy** implementation, the sum of the discrete Laplacian is $\\sum_{i=0}^{N-1} L_i = -u_0 - u_{N-1}$ (as derived via term-by-term summation), which is non-zero in general. Thus, conservation is violated.\nThe test checks if $|\\sum u_i^{\\text{new}} - \\sum u_i^{\\text{old}}| > \\varepsilon$, where $\\varepsilon=10^{-10}$.\n\n**B. Symmetry Equivariance under Spatial Reflection**\nThe reflection operator is defined by $(\\mathcal{R}u)_i = u_{N-1-i}$. The symmetry of the stencil implies that the time-step operation, $\\text{step}(\\cdot)$, should commute with $\\mathcal{R}$: $\\text{step}(\\mathcal{R}u) = \\mathcal{R}(\\text{step}(u))$.\nFor the **correct** implementation, the update at index $i$ on the reflected state $(\\mathcal{R}u)$ depends on $u_{N-1-(i-1)}, u_{N-1-i}, u_{N-1-(i+1)}$. The update on the original state $u$ at index $N-1-i$ depends on $u_{N-1-i-1}, u_{N-1-i}, u_{N-1-i+1}$. Because the coefficients for the $i\\pm1$ neighbors are identical, these two operations yield the same result.\nThe **buggy** implementation breaks this symmetry. The zero boundary condition is applied at index $0$ and $N-1$. When we compute $\\text{step}(\\mathcal{R}u)$, the zero BC is applied to $(\\mathcal{R}u)_0 = u_{N-1}$ and $(\\mathcal{R}u)_{N-1} = u_0$. When we compute $\\mathcal{R}(\\text{step}(u))$, we reflect the result of applying the zero BC to $u_0$ and $u_{N-1}$. These operations are not equivalent.\nThe test checks if $\\max_i |(\\text{step}(\\mathcal{R}u))_i - (\\mathcal{R}(\\text{step}(u)))_i| > \\varepsilon$.\n\n**C. Monotonicity of the Range**\nFor $0 \\le \\alpha \\le \\frac{1}{2}$, the update rule is a convex combination of non-negative coefficients:\n$$\nu_i^{\\text{new}} = \\alpha u_{i-1}^{\\text{old}} + (1-2\\alpha)u_i^{\\text{old}} + \\alpha u_{i+1}^{\\text{old}}\n$$\nwhere $\\alpha \\ge 0$ and $(1-2\\alpha) \\ge 0$. The sum of coefficients is $1$. This property is known as the discrete maximum principle. It implies that the new value $u_i^{\\text{new}}$ cannot be outside the range of its contributing old values. Globally, this means $\\min(u^{\\text{old}}) \\le \\min(u^{\\text{new}})$ and $\\max(u^{\\text{new}}) \\le \\max(u^{\\text{old}})$. Consequently, the range $R(u) = \\max_i u_i - \\min_i u_i$ must be non-increasing: $R(u^{\\text{new}}) \\le R(u^{\\text{old}})$.\nThis property holds for both the **correct** and **buggy** implementations, as long as $0 \\le \\alpha \\le \\frac{1}{2}$, because the update rules at the boundaries in the buggy case also form convex combinations.\nThe property is not expected to hold for $\\alpha > \\frac{1}{2}$, which corresponds to a physically and numerically unstable regime.\nThe test checks if $R(u^{\\text{new}}) - R(u^{\\text{old}}) > \\varepsilon$.\n\n### 3. Testing Procedure\n\nThe program iterates through a suite of predefined test cases. For each case, it initializes a random number generator with a given seed for reproducibility. It generates $T$ random state vectors $u$, with components drawn from a uniform distribution on $[0,1]$. If specified, an additional trial with the zero vector $u=0$ is included. For each trial state, the program applies the designated (`correct` or `buggy`) time-step function and checks for violations of the three invariants. The total number of violations for each invariant is counted and reported. This automated procedure exemplifies property-based testing, where abstract properties of a system are verified over a large set of random inputs, providing strong evidence of correctness or revealing subtle bugs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef step_correct(u: np.ndarray, alpha: float) -> np.ndarray:\n    \"\"\"\n    Performs one time step of the 1D heat equation with periodic boundary conditions.\n    \"\"\"\n    if u.size == 0:\n        return np.array([])\n    # The discrete Laplacian with periodic boundaries is implemented via circular shifts.\n    laplacian = np.roll(u, 1) - 2 * u + np.roll(u, -1)\n    return u + alpha * laplacian\n\ndef step_buggy(u: np.ndarray, alpha: float) -> np.ndarray:\n    \"\"\"\n    Performs one time step of the 1D heat equation with buggy zero boundary conditions.\n    \"\"\"\n    n = u.size\n    if n == 0:\n        return np.array([])\n    \n    u_new = np.copy(u)\n    \n    # Interior points update\n    if n > 2:\n        laplacian_interior = u[:-2] - 2 * u[1:-1] + u[2:]\n        u_new[1:-1] += alpha * laplacian_interior\n\n    # Boundary points update with zero-value neighbors\n    if n >= 2:\n        # Left boundary i=0, neighbor u_{-1} is 0\n        laplacian_0 = 0 - 2 * u[0] + u[1]\n        u_new[0] += alpha * laplacian_0\n        # Right boundary i=n-1, neighbor u_{n} is 0\n        laplacian_n_minus_1 = u[n-2] - 2 * u[n-1] + 0\n        u_new[n-1] += alpha * laplacian_n_minus_1\n    elif n == 1:\n        # Special case for N=1, both neighbors are 0\n        laplacian_0 = 0 - 2 * u[0] + 0\n        u_new[0] += alpha * laplacian_0\n\n    return u_new\n\ndef solve():\n    \"\"\"\n    Main function to run the property-based tests for the given cases.\n    \"\"\"\n    test_cases = [\n        (64, 0.2, 200, 101, \"correct\", True),\n        (64, 0.49, 200, 102, \"correct\", True),\n        (64, 0.6, 200, 103, \"correct\", False),\n        (64, 0.2, 200, 104, \"buggy\", True),\n        (1, 0.2, 50, 105, \"correct\", True),\n        (32, 0.2, 200, 106, \"buggy\", False),\n    ]\n\n    all_results = []\n    epsilon = 1e-10\n\n    for n, alpha, t, seed, mode, include_zero in test_cases:\n        \n        # 1. Select the stepping function based on the test case mode\n        step_func = step_correct if mode == \"correct\" else step_buggy\n\n        # 2. Generate random states for testing\n        rng = np.random.default_rng(seed)\n        states = [rng.uniform(0, 1, size=n) for _ in range(t)]\n        if include_zero:\n            states.append(np.zeros(n))\n        \n        # 3. Initialize violation counters\n        conservation_violations = 0\n        symmetry_violations = 0\n        monotonicity_violations = 0\n\n        # 4. Iterate through test states and check invariants\n        for u_old in states:\n            if u_old.size == 0:\n                continue\n            \n            # --- Perform one time step ---\n            u_new = step_func(u_old, alpha)\n\n            # --- Test 1: Conservation of total quantity ---\n            sum_old = np.sum(u_old)\n            sum_new = np.sum(u_new)\n            if np.abs(sum_new - sum_old) > epsilon:\n                conservation_violations += 1\n\n            # --- Test 2: Symmetry equivariance ---\n            # LHS: step(reflect(u))\n            u_reflected = u_old[::-1]\n            lhs = step_func(u_reflected, alpha)\n            # RHS: reflect(step(u))\n            rhs = u_new[::-1]\n            \n            if np.max(np.abs(lhs - rhs)) > epsilon:\n                symmetry_violations += 1\n\n            # --- Test 3: Monotonicity of the range ---\n            # This property is only guaranteed for 0 <= alpha <= 0.5\n            # The test is run for all alpha to see when it fails.\n            if u_old.size > 0:\n                range_old = np.max(u_old) - np.min(u_old)\n                range_new = np.max(u_new) - np.min(u_new)\n                if range_new - range_old > epsilon:\n                    monotonicity_violations += 1\n            \n        all_results.append([conservation_violations, symmetry_violations, monotonicity_violations])\n\n    # Final print statement in the exact required format.\n    print(f\"{all_results}\")\n\nsolve()\n```"
        },
        {
            "introduction": "A verified code does not guarantee a perfect simulation, as the chosen numerical method introduces its own behaviors and artifacts. This practice  explores the crucial concept of numerical diffusion, where a scheme designed for a non-dissipative equation artificially smooths sharp features. You will learn to quantify this effect by deriving an estimator that connects the numerical smearing to an equivalent physical diffusion process, a critical skill for correctly interpreting results from computational fluid dynamics and other fields involving transport phenomena.",
            "id": "3109427",
            "problem": "You are asked to design and implement a controlled numerical experiment to estimate the amount of numerical diffusion introduced by discrete solvers of a one-dimensional linear hyperbolic partial differential equation. Consider the periodic linear advection equation on the unit interval,\n$$\n\\frac{\\partial u}{\\partial t} + a \\frac{\\partial u}{\\partial x} = 0, \\quad x \\in [0,1), \\quad t \\ge 0,\n$$\nwith constant advection speed $a > 0$. All quantities are nondimensional.\n\nFundamental base and modeling assumptions:\n- The linear advection equation expresses conservation of a passively advected scalar under constant transport speed $a$. The transport theorem implies that, in the continuous case, a square-wave initial condition remains a square wave, simply translated without deformation.\n- Standard first-order monotone discretizations of hyperbolic equations such as upwind or Lax–Friedrichs often introduce smoothing near discontinuities. This smoothing can be interpreted as an artifact analogous to an effective diffusion that is absent in the true continuous dynamics.\n\nYour objective is to quantify this smoothing by estimating an equivalent diffusion coefficient that would produce a comparable edge broadening if the solution near each interface were governed locally by a pure diffusion equation. To do this, you must:\n- Start from the known conservation law and use well-tested facts about diffusion of a step function to derive a practical estimator connecting the measured steepness of a numerically smeared front to an equivalent constant diffusion coefficient.\n- Implement two explicit finite difference schemes on a uniform grid with periodic boundary conditions for the advection equation:\n  1. Upwind (also called donor-cell) scheme for $a > 0$.\n  2. Lax–Friedrichs scheme.\n- Use a square-wave initial condition of height $1$ occupying the central half of the domain:\n  $$\n  u(x,0) = \n  \\begin{cases}\n  1, & x \\in [0.25, 0.75), \\\\\n  0, & \\text{otherwise on } [0,1).\n  \\end{cases}\n  $$\n- Set the advection speed to $a = 1$.\n- Discretize space with $N$ uniformly spaced grid points with grid spacing $\\Delta x = 1/N$. Use a constant time step $\\Delta t$ determined by the Courant–Friedrichs–Lewy (CFL) number $\\lambda = a \\Delta t / \\Delta x$, that is, $\\Delta t = \\lambda \\Delta x / a$. Evolve in time with explicit updates until the largest integer number of steps $n$ such that $n \\Delta t \\le T$, where $T$ is the target final time, and denote the achieved final time as $t_{\\text{end}} = n \\Delta t$.\n- At $t_{\\text{end}}$, use the numerical solution to compute a robust measure of the maximum slope magnitude,\n  $$\n  g_{\\max} \\approx \\max_i \\left| \\frac{u_{i+1} - u_{i-1}}{2 \\Delta x} \\right|,\n  $$\n  which approximates the steepest spatial derivative of the solution.\n\nEstimator design requirements:\n- Using a well-tested fact from diffusion theory, the local evolution of a discontinuous step under a pure diffusion equation with constant diffusion coefficient $\\nu$ leads to a self-similar error-function profile whose mid-interface slope decays in time with a known rate. Starting from this fact, derive an explicit estimator that expresses an equivalent constant diffusion coefficient $\\nu_{\\text{eff}}$ in terms of $g_{\\max}$ and $t_{\\text{end}}$. Justify the estimator logically and dimensionally based on the underlying principles without appealing to any ad hoc fitting.\n- Implement this estimator to compute $\\nu_{\\text{eff}}$ from each numerical experiment.\n\nImplementation details:\n- Use periodic boundary conditions on $[0,1)$.\n- Use central finite differences to compute the slope proxy $g_{\\max}$ as specified.\n- The solver implementations must be:\n  - Upwind for $a>0$: update $u_i^{n+1}$ using only $u_i^n$ and $u_{i-1}^n$ with the correct upwinded flux.\n  - Lax–Friedrichs: update $u_i^{n+1}$ as a symmetric average plus a central flux difference scaled by the Courant–Friedrichs–Lewy number.\n\nTest suite:\nRun the following five test cases, each specified as a tuple of method, number of grid points $N$, Courant–Friedrichs–Lewy number $\\lambda$, and target final time $T$:\n- Case $1$: method = upwind, $N = 200$, $\\lambda = 0.5$, $T = 0.2$.\n- Case $2$: method = upwind, $N = 400$, $\\lambda = 0.5$, $T = 0.2$.\n- Case $3$: method = upwind, $N = 200$, $\\lambda = 0.9$, $T = 0.2$.\n- Case $4$: method = lax\\_friedrichs, $N = 200$, $\\lambda = 0.5$, $T = 0.2$.\n- Case $5$: method = lax\\_friedrichs, $N = 400$, $\\lambda = 0.9$, $T = 0.2$.\n\nFor each case, compute and report a single floating-point number: the estimated equivalent diffusion coefficient $\\nu_{\\text{eff}}$ corresponding to the achieved final time $t_{\\text{end}}$. All quantities are nondimensional.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers rounded to six digits after the decimal point, enclosed in square brackets, in the same order as the five cases above; for example, an output line of the form\n  `[x_1,x_2,x_3,x_4,x_5]`,\n  where each $x_i$ is a float formatted to six digits after the decimal point.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded, well-posed, objective, and complete. It constitutes a standard exercise in computational science, requiring the derivation of an estimator, implementation of numerical schemes, and execution of a controlled numerical experiment.\n\n### 1. Derivation of the Effective Diffusion Coefficient Estimator\n\nThe objective is to derive an estimator for an effective diffusion coefficient, $\\nu_{\\text{eff}}$, that quantifies the numerical smearing of a sharp interface. The problem suggests modeling this smearing as a physical diffusion process governed by the pure diffusion equation:\n$$\n\\frac{\\partial u}{\\partial t} = \\nu_{\\text{eff}} \\frac{\\partial^2 u}{\\partial x^2}\n$$\nWe consider a single step-function initial condition of height $\\Delta U$ located at $x=x_0$:\n$$\nu(x,0) = \\begin{cases} U_L, & x < x_0 \\\\ U_R, & x > x_0 \\end{cases}\n$$\nwhere the jump height is $\\Delta U = |U_R - U_L|$. The analytical solution to the diffusion equation for this initial condition is expressed in terms of the error function, $\\text{erf}(z)$:\n$$\nu(x,t) = U_L + (U_R - U_L) \\frac{1}{2} \\left[ 1 + \\text{erf}\\left(\\frac{x-x_0}{2\\sqrt{\\nu_{\\text{eff}} t}}\\right) \\right]\n$$\nThe problem specifies a square-wave initial condition with a jump height of $\\Delta U = 1$ (from $0$ to $1$) at $x=0.25$ and a jump of $\\Delta U = -1$ (from $1$ to $0$) at $x=0.75$. Near each interface, for a short time, the evolution can be independently approximated by the diffusion of a single step. Let's analyze the step at $x_0 = 0.25$, where $U_L=0$ and $U_R=1$. The local solution profile is:\n$$\nu(x,t) \\approx \\frac{1}{2}\\left[ 1 + \\text{erf}\\left(\\frac{x-x_0}{2\\sqrt{\\nu_{\\text{eff}} t}}\\right) \\right]\n$$\nTo find the steepest part of the profile, we compute the spatial derivative, $\\frac{\\partial u}{\\partial x}$:\n$$\n\\frac{\\partial u}{\\partial x} = \\frac{1}{2} \\frac{d}{dx} \\left[ \\text{erf}\\left(\\frac{x-x_0}{2\\sqrt{\\nu_{\\text{eff}} t}}\\right) \\right]\n$$\nUsing the identity $\\frac{d}{dz} \\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} e^{-z^2}$ and the chain rule, we obtain:\n$$\n\\frac{\\partial u}{\\partial x} = \\frac{1}{2} \\left[ \\frac{2}{\\sqrt{\\pi}} \\exp\\left(-\\left(\\frac{x-x_0}{2\\sqrt{\\nu_{\\text{eff}} t}}\\right)^2\\right) \\right] \\cdot \\frac{1}{2\\sqrt{\\nu_{\\text{eff}} t}} = \\frac{1}{2\\sqrt{\\pi \\nu_{\\text{eff}} t}} \\exp\\left(-\\frac{(x-x_0)^2}{4 \\nu_{\\text{eff}} t}\\right)\n$$\nThe magnitude of the slope is maximum at the center of the interface, $x=x_0$. Let this maximum slope be denoted by $g_{\\max}$:\n$$\ng_{\\max} = \\left| \\frac{\\partial u}{\\partial x} \\right|_{x=x_0} = \\frac{1}{2\\sqrt{\\pi \\nu_{\\text{eff}} t}}\n$$\nThis relationship connects the maximum slope of the diffused profile to the diffusion coefficient $\\nu_{\\text{eff}}$ and time $t$. The numerical experiment measures $g_{\\max}$ at a final time $t_{\\text{end}}$. We can rearrange the equation to solve for $\\nu_{\\text{eff}}$:\n$$\ng_{\\max}^2 = \\frac{1}{4 \\pi \\nu_{\\text{eff}} t_{\\text{end}}}\n$$\n$$\n\\nu_{\\text{eff}} = \\frac{1}{4 \\pi t_{\\text{end}} g_{\\max}^2}\n$$\nThis is the required estimator for the effective diffusion coefficient. It is derived from first principles of diffusion theory as requested and is dimensionally consistent.\n\n### 2. Numerical Schemes and Implementation\n\nThe linear advection equation is solved on a uniform grid with $N$ points, $x_i = i \\Delta x$ for $i=0, \\dots, N-1$, where $\\Delta x = 1/N$. Time is discretized with a constant step $\\Delta t = \\lambda \\Delta x / a$, where $\\lambda$ is the CFL number and $a$ is the advection speed. We use periodic boundary conditions.\n\n#### 2.1 Upwind Scheme\nFor a positive advection speed ($a > 0$), the first-order upwind scheme uses a backward difference for the spatial derivative:\n$$\n\\frac{u_i^{n+1} - u_i^n}{\\Delta t} + a \\frac{u_i^n - u_{i-1}^n}{\\Delta x} = 0\n$$\nSolving for the updated state $u_i^{n+1}$:\n$$\nu_i^{n+1} = u_i^n - \\frac{a \\Delta t}{\\Delta x} (u_i^n - u_{i-1}^n) = u_i^n - \\lambda (u_i^n - u_{i-1}^n)\n$$\nThis can be rewritten as:\n$$\nu_i^{n+1} = (1 - \\lambda) u_i^n + \\lambda u_{i-1}^n\n$$\nThis scheme is stable for $0 \\le \\lambda \\le 1$.\n\n#### 2.2 Lax–Friedrichs Scheme\nThe Lax–Friedrichs scheme uses a central difference for the spatial derivative and replaces the $u_i^n$ term in the time derivative with a spatial average:\n$$\n\\frac{u_i^{n+1} - \\frac{1}{2}(u_{i+1}^n + u_{i-1}^n)}{\\Delta t} + a \\frac{u_{i+1}^n - u_{i-1}^n}{2 \\Delta x} = 0\n$$\nSolving for $u_i^{n+1}$:\n$$\nu_i^{n+1} = \\frac{1}{2}(u_{i+1}^n + u_{i-1}^n) - \\frac{a \\Delta t}{2 \\Delta x} (u_{i+1}^n - u_{i-1}^n)\n$$\nSubstituting $\\lambda = a \\Delta t / \\Delta x$:\n$$\nu_i^{n+1} = \\frac{1}{2}(u_{i+1}^n + u_{i-1}^n) - \\frac{\\lambda}{2} (u_{i+1}^n - u_{i-1}^n)\n$$\nThis scheme is stable for $|\\lambda| \\le 1$.\n\n### 3. Computational Procedure\n\nFor each test case specified by (method, $N$, $\\lambda$, $T$):\n1.  **Initialization**: Set the advection speed $a=1$. Define the spatial grid with $\\Delta x = 1/N$ and $N$ points on $[0,1)$. Initialize the solution vector $u$ with the square-wave profile: $u(x,0) = 1$ for $x \\in [0.25, 0.75)$ and $0$ otherwise.\n2.  **Time Stepping**: Calculate the time step $\\Delta t = \\lambda \\Delta x / a$. Determine the total number of time steps, $n_{\\text{steps}} = \\lfloor T / \\Delta t \\rfloor$. The final time of the simulation is $t_{\\text{end}} = n_{\\text{steps}} \\Delta t$.\n3.  **Evolution**: Evolve the solution $u$ for $n_{\\text{steps}}$ time steps using the specified numerical scheme (Upwind or Lax–Friedrichs). Periodic boundary conditions are handled by using wrapped indices (e.g., $u_{-1} \\equiv u_{N-1}$ and $u_N \\equiv u_0$).\n4.  **Slope Measurement**: After the final time step, compute the maximum slope proxy $g_{\\max}$ from the final solution $u_{\\text{final}}$ using a periodic, second-order central difference:\n    $$\n    g_{\\max} = \\max_{i=0, \\dots, N-1} \\left| \\frac{u_{i+1} - u_{i-1}}{2 \\Delta x} \\right|\n    $$\n    where indices are taken modulo $N$.\n5.  **Estimation**: Calculate the effective diffusion coefficient $\\nu_{\\text{eff}}$ using the derived estimator:\n    $$\n    \\nu_{\\text{eff}} = \\frac{1}{4 \\pi t_{\\text{end}} g_{\\max}^2}\n    $$\nThis procedure is repeated for all five test cases, and the resulting values of $\\nu_{\\text{eff}}$ are reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and implements a controlled numerical experiment to estimate \n    numerical diffusion for solvers of the 1D linear advection equation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (method, N, lambda, T)\n        ('upwind', 200, 0.5, 0.2),\n        ('upwind', 400, 0.5, 0.2),\n        ('upwind', 200, 0.9, 0.2),\n        ('lax_friedrichs', 200, 0.5, 0.2),\n        ('lax_friedrichs', 400, 0.9, 0.2),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        method, N, lam, T = case\n        \n        # --- 1. Initialization ---\n        a = 1.0  # Advection speed\n        \n        # Spatial grid setup\n        dx = 1.0 / N\n        x = np.linspace(0.0, 1.0, N, endpoint=False)\n        \n        # Initial condition: square wave\n        u = np.zeros(N)\n        u[(x >= 0.25) & (x < 0.75)] = 1.0\n        \n        # --- 2. Time Stepping ---\n        dt = lam * dx / a\n        # Ensure num_steps is non-negative, though T>0 and dt>0 makes this safe\n        num_steps = int(np.floor(T / dt)) if dt > 0 else 0\n        t_end = num_steps * dt\n\n        # If no steps are taken, diffusion is zero.\n        # This is an edge case not triggered by the problem's test suite\n        # but is good practice for robustness.\n        if t_end == 0:\n            results.append(0.0)\n            continue\n            \n        # --- 3. Evolution Loop ---\n        for _ in range(num_steps):\n            if method == 'upwind':\n                # u_i^{n+1} = (1-lambda)*u_i^n + lambda*u_{i-1}^n\n                # np.roll(u, 1) shifts elements to the right, u_i -> u_{i+1}\n                # so u_{i-1} corresponds to np.roll(u, 1)\n                u_im1 = np.roll(u, 1)\n                u = (1.0 - lam) * u + lam * u_im1\n            elif method == 'lax_friedrichs':\n                # u_i^{n+1} = 0.5*(u_{i+1}+u_{i-1}) - 0.5*lambda*(u_{i+1}-u_{i-1})\n                u_ip1 = np.roll(u, -1)\n                u_im1 = np.roll(u, 1)\n                u = 0.5 * (u_ip1 + u_im1) - 0.5 * lam * (u_ip1 - u_im1)\n\n        # --- 4. Slope Measurement ---\n        # g_max approx max| (u_{i+1} - u_{i-1}) / (2*dx) |\n        u_ip1 = np.roll(u, -1)\n        u_im1 = np.roll(u, 1)\n        gradient = (u_ip1 - u_im1) / (2.0 * dx)\n        g_max = np.max(np.abs(gradient))\n        \n        # --- 5. Estimation ---\n        # nu_eff = 1 / (4 * pi * t_end * g_max^2)\n        if g_max > 0:\n            nu_eff = 1.0 / (4.0 * np.pi * t_end * g_max**2)\n        else:\n            # If g_max is zero, it implies no diffusion (perfect translation)\n            # which equates to zero effective diffusivity.\n            nu_eff = 0.0\n\n        results.append(nu_eff)\n\n    # Format results to six decimal places for the final output.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world computational science operates under constraints, often forcing a trade-off between model accuracy and statistical power. This exercise  formalizes this classic dilemma, requiring you to select an intentionally simplified model to minimize the total error under a fixed computational budget. By navigating the balance between a model's inherent bias and the statistical variance from finite sampling, you will practice making optimal strategic decisions to maximize the quality of your results with limited resources.",
            "id": "3109334",
            "problem": "A computational scientist must estimate a scalar quantity using a family of intentionally simplified models indexed by an integer simplification level $s \\in \\{0,1,2,\\dots,S_{\\max}\\}$. Increased simplification lowers cost but introduces a deterministic bias. The scientist aggregates $n(s)$ independent samples via the sample mean under a fixed compute budget. Formalize and implement the tradeoff between bias and sampling noise subject to resource constraints, and quantify the acceptable bias that results at the optimal simplification level.\n\nFundamental base. Use the following foundational facts and core definitions.\n- Definition of bias: For a given simplification level $s$, define the bias magnitude $b(s)$ as the absolute difference between the expected value of the simplified model output and the true target. In this problem, assume the functional form $b(s) = k_b \\cdot 2^{-\\alpha s}$ for given constants $k_b > 0$ and $\\alpha > 0$.\n- Definition of per-sample variance: The variance of a single simplified model output at any level $s$ is a constant $\\sigma^2 > 0$ (identical and independent across samples).\n- Definition of computational cost: The cost per sample at level $s$ is $c(s) = k_c \\cdot 2^{\\beta s}$ for given constants $k_c > 0$ and $\\beta > 0$.\n- Budget constraint: Given a compute budget $B > 0$, the number of independent samples feasible at level $s$ is $n(s) = \\left\\lfloor \\dfrac{B}{c(s)} \\right\\rfloor$. A level $s$ is feasible only if $n(s) \\ge 1$.\n- Estimation strategy: Use the sample mean of $n(s)$ independent identically distributed outputs at level $s$ to estimate the target.\n\nTask. Starting from the above definitions and the laws of expectation and variance for independent samples, derive the mean squared error of the sample mean estimator at simplification level $s$ in terms of $b(s)$, $\\sigma^2$, and $n(s)$. Then, for each feasible $s$, define the mean squared error $E(s)$ of the estimator at that level and identify the simplification level $s^\\star$ that minimizes $E(s)$ subject to the budget constraint and $s \\in \\{0,1,2,\\dots,S_{\\max}\\}$. In case of ties in $E(s)$, choose the smallest $s$.\n\nQuantify acceptable bias under resource constraints as the bias–error share at the optimizer,\n$$\nr^\\star = \\frac{b(s^\\star)^2}{E(s^\\star)}\n$$\nwhich lies in the interval $(0,1)$. This ratio formalizes the tradeoff by reporting the fraction of the total squared error attributable to intentional misspecification under the optimal allocation.\n\nYour program must compute $r^\\star$ for each of the following test cases. For each case, the tuple $(B, k_b, \\alpha, \\sigma^2, k_c, \\beta, S_{\\max})$ is given:\n- Case 1: $(10^6, 1, 1, 1, 100, 1, 20)$\n- Case 2 (boundary feasibility): $(100, 1, 1, 1, 100, 1, 20)$\n- Case 3 (slow bias decay, steep cost growth): $(10^6, 1, 0.5, 1, 50, 2, 30)$\n- Case 4 (high intrinsic variance): $(10^6, 1, 1.2, 100, 100, 1, 20)$\n\nFinal output format. Your program should produce a single line of output containing the four results $[r^\\star_{\\text{case 1}}, r^\\star_{\\text{case 2}}, r^\\star_{\\text{case 3}}, r^\\star_{\\text{case 4}}]$ as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (for example, `[0.123456,0.500000,0.333333,0.777777]`). No other text should be printed.",
            "solution": "The user has requested a formal analysis and computational implementation for a problem in computational science concerning the tradeoff between model simplification and estimation accuracy. The first step is to derive the Mean Squared Error (MSE) of the proposed estimator, after which we will devise a strategy to minimize this error and compute the resulting bias-to-error ratio.\n\n**Derivation of the Mean Squared Error (MSE)**\n\nThe problem asks for an estimate of a true scalar quantity, let us denote it $\\mu_{\\text{true}}$. The estimation is performed using a simplified model at an integer simplification level $s$. Let $X^{(s)}$ be a random variable representing the output of a single simulation using the model at level $s$. The problem provides the following definitions:\n- The bias magnitude is $b(s) = |\\mathrm{E}[X^{(s)}] - \\mu_{\\text{true}}|$. The squared bias is therefore $b(s)^2 = (\\mathrm{E}[X^{(s)}] - \\mu_{\\text{true}})^2$.\n- The per-sample variance is $\\mathrm{Var}(X^{(s)}) = \\sigma^2$, which is constant for all $s$.\n\nThe estimation strategy is to use the sample mean of $n(s)$ independent and identically distributed (i.i.d.) samples, $\\{X_1^{(s)}, X_2^{(s)}, \\dots, X_{n(s)}^{(s)}\\}$. The estimator is:\n$$\n\\hat{\\mu}_s = \\frac{1}{n(s)} \\sum_{i=1}^{n(s)} X_i^{(s)}\n$$\nThe Mean Squared Error (MSE) of this estimator, which we denote $E(s)$, is defined as the expected squared difference between the estimator and the true value:\n$$\nE(s) = \\mathrm{E}[(\\hat{\\mu}_s - \\mu_{\\text{true}})^2]\n$$\nA fundamental result in statistical estimation theory is that the MSE can be decomposed into the sum of the variance of the estimator and its squared bias:\n$$\nE(s) = \\mathrm{Var}(\\hat{\\mu}_s) + (\\mathrm{E}[\\hat{\\mu}_s] - \\mu_{\\text{true}})^2\n$$\nWe now derive each term. First, the expectation of the estimator $\\hat{\\mu}_s$:\nBy linearity of expectation,\n$$\n\\mathrm{E}[\\hat{\\mu}_s] = \\mathrm{E}\\left[\\frac{1}{n(s)} \\sum_{i=1}^{n(s)} X_i^{(s)}\\right] = \\frac{1}{n(s)} \\sum_{i=1}^{n(s)} \\mathrm{E}[X_i^{(s)}]\n$$\nSince the samples are identically distributed, $\\mathrm{E}[X_i^{(s)}]$ is the same for all $i=1, \\dots, n(s)$. Let $\\mathrm{E}[X^{(s)}]$ denote this common expectation.\n$$\n\\mathrm{E}[\\hat{\\mu}_s] = \\frac{1}{n(s)} \\cdot n(s) \\cdot \\mathrm{E}[X^{(s)}] = \\mathrm{E}[X^{(s)}]\n$$\nThe squared bias of the estimator is therefore:\n$$\n(\\mathrm{E}[\\hat{\\mu}_s] - \\mu_{\\text{true}})^2 = (\\mathrm{E}[X^{(s)}] - \\mu_{\\text{true}})^2 = b(s)^2\n$$\nNext, we derive the variance of the estimator. Since the samples $X_i^{(s)}$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(\\hat{\\mu}_s) = \\mathrm{Var}\\left(\\frac{1}{n(s)} \\sum_{i=1}^{n(s)} X_i^{(s)}\\right) = \\frac{1}{n(s)^2} \\sum_{i=1}^{n(s)} \\mathrm{Var}(X_i^{(s)})\n$$\nGiven that the per-sample variance is $\\mathrm{Var}(X_i^{(s)})=\\sigma^2$ for all $i$,\n$$\n\\mathrm{Var}(\\hat{\\mu}_s) = \\frac{1}{n(s)^2} \\cdot n(s) \\cdot \\sigma^2 = \\frac{\\sigma^2}{n(s)}\n$$\nCombining the squared bias and variance terms, we obtain the Mean Squared Error for the estimator at level $s$:\n$$\nE(s) = b(s)^2 + \\frac{\\sigma^2}{n(s)}\n$$\nThis completes the required derivation. The MSE comprises two components: a deterministic error from model simplification (squared bias) and a statistical error from finite sampling (variance).\n\n**Minimization of the Mean Squared Error**\n\nThe next task is to find the simplification level $s^\\star$ that minimizes $E(s)$. We are given the functional forms for bias, cost, and the number of samples:\n- Bias: $b(s) = k_b \\cdot 2^{-\\alpha s}$\n- Cost per sample: $c(s) = k_c \\cdot 2^{\\beta s}$\n- Number of samples: $n(s) = \\left\\lfloor \\frac{B}{c(s)} \\right\\rfloor = \\left\\lfloor \\frac{B}{k_c \\cdot 2^{\\beta s}} \\right\\rfloor$\n\nSubstituting these into the MSE expression gives:\n$$\nE(s) = (k_b \\cdot 2^{-\\alpha s})^2 + \\frac{\\sigma^2}{\\left\\lfloor \\frac{B}{k_c \\cdot 2^{\\beta s}} \\right\\rfloor} = k_b^2 \\cdot 4^{-\\alpha s} + \\frac{\\sigma^2}{\\left\\lfloor B k_c^{-1} 4^{-\\beta s/2} \\right\\rfloor}\n$$\nThe simplification level $s$ is an integer in the range $\\{0, 1, 2, \\dots, S_{\\max}\\}$. A level $s$ is only feasible if at least one sample can be generated, i.e., $n(s) \\ge 1$. This implies $B/c(s) \\ge 1$, or $B \\ge k_c \\cdot 2^{\\beta s}$. This constraint defines the set of feasible simplification levels to search:\n$$\ns \\in \\{ i \\in \\mathbb{Z} \\mid 0 \\le i \\le S_{\\max} \\text{ and } i \\le \\frac{1}{\\beta}\\log_2\\left(\\frac{B}{k_c}\\right) \\}\n$$\nThe bias term $b(s)^2$ is a monotonically decreasing function of $s$, while the variance term $\\sigma^2/n(s)$ is a monotonically increasing function of $s$ (since $n(s)$ decreases with $s$). The total error $E(s)$ is a sum of these two opposing trends, and we expect it to possess a minimum. Due to the discrete nature of $s$ and the floor function in $n(s)$, the most direct and reliable method to find the minimum is to perform an exhaustive search over the finite set of feasible integer values of $s$.\n\nThe algorithm is as follows:\n1. Determine the set of feasible simplification levels $s$ based on the budget constraint and $S_{\\max}$.\n2. Iterate through each feasible $s$.\n3. For each $s$, calculate $n(s)$, $b(s)^2$, and the total MSE, $E(s)$.\n4. Keep track of the level $s^\\star$ that yields the minimum $E(s)$ found so far. The problem specifies that in case of a tie in $E(s)$, the smallest $s$ should be chosen. This is naturally handled by iterating $s$ in increasing order and only updating the minimum if a strictly smaller error is found.\n\n**Quantifying Acceptable Bias**\n\nOnce the optimal simplification level $s^\\star$ is identified, we compute the bias-error share $r^\\star$:\n$$\nr^\\star = \\frac{b(s^\\star)^2}{E(s^\\star)} = \\frac{b(s^\\star)^2}{b(s^\\star)^2 + \\sigma^2/n(s^\\star)}\n$$\nThis dimensionless ratio $r^\\star \\in (0,1)$ represents the fraction of the minimal achievable MSE that is attributable to the squared bias. It quantifies the optimal balance: if $r^\\star \\approx 0.5$, the errors from bias and variance are roughly equal, a common target in such tradeoff problems. If $r^\\star \\ll 0.5$, the optimal strategy is variance-dominated, whereas if $r^\\star \\gg 0.5$, it is bias-dominated.\n\nThe following Python program implements this procedure for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the computational science tradeoff problem for multiple test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (B, k_b, alpha, sigma_sq, k_c, beta, S_max)\n    test_cases = [\n        (1e6, 1.0, 1.0, 1.0, 100.0, 1.0, 20),\n        (100.0, 1.0, 1.0, 1.0, 100.0, 1.0, 20),\n        (1e6, 1.0, 0.5, 1.0, 50.0, 2.0, 30),\n        (1e6, 1.0, 1.2, 100.0, 100.0, 1.0, 20),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_optimal_bias_share(*case)\n        results.append(result)\n\n    # Format the final output according to problem specification.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_optimal_bias_share(B, k_b, alpha, sigma_sq, k_c, beta, S_max):\n    \"\"\"\n    Calculates the optimal bias-error share r_star for a given set of parameters.\n    \n    Args:\n        B (float): Total compute budget.\n        k_b (float): Bias coefficient.\n        alpha (float): Bias decay rate.\n        sigma_sq (float): Per-sample variance.\n        k_c (float): Cost coefficient.\n        beta (float): Cost growth rate.\n        S_max (int): Maximum simplification level.\n\n    Returns:\n        float: The optimal bias-error share r_star.\n    \"\"\"\n    min_E = float('inf')\n    optimal_s = -1\n\n    # A level s is feasible only if B >= c(s), which means B >= k_c * 2**(beta*s).\n    # This implies s <= log2(B/k_c) / beta.\n    # If B < k_c, then log2(B/k_c) is negative, and no non-negative s is feasible.\n    # However, s=0 is feasible if B >= k_c.\n    if B < k_c:\n        # No feasible levels, this case is not expected per problem description.\n        # But if it were to happen, we cannot proceed.\n        raise ValueError(\"Budget B is less than the base cost k_c, no feasible levels.\")\n\n    s_upper_bound = int(np.floor(np.log2(B / k_c) / beta))\n    \n    # The search space for s is from 0 up to min(S_max, s_upper_bound).\n    search_range = range(min(S_max, s_upper_bound) + 1)\n    \n    if not search_range:\n        raise ValueError(\"Feasible range for s is empty.\")\n\n    for s in search_range:\n        # Cost per sample at level s\n        cost_s = k_c * (2**(beta * s))\n        \n        # Number of samples, must be >= 1 by construction of search_range\n        n_s = np.floor(B / cost_s)\n        \n        # Squared bias at level s\n        bias_sq_s = (k_b * (2**(-alpha * s)))**2\n        \n        # Variance of the sample mean\n        var_mean_s = sigma_sq / n_s\n        \n        # Total Mean Squared Error (MSE)\n        E_s = bias_sq_s + var_mean_s\n        \n        # Update minimum if a strictly smaller error is found.\n        # This respects the tie-breaking rule of choosing the smallest s.\n        if E_s < min_E:\n            min_E = E_s\n            optimal_s = s\n\n    # After finding the optimal s (s_star), calculate the bias-error share r_star.\n    if optimal_s == -1:\n        # This should not be reachable given the checks above.\n        raise RuntimeError(\"Optimal simplification level not found.\")\n        \n    optimal_bias_sq = (k_b * (2**(-alpha * optimal_s)))**2\n    r_star = optimal_bias_sq / min_E\n    \n    return r_star\n\n# Execute the main function to produce the final answer.\nsolve()\n```"
        }
    ]
}