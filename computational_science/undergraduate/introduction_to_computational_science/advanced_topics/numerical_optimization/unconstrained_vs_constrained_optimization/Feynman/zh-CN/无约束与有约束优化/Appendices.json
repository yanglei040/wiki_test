{
    "hands_on_practices": [
        {
            "introduction": "约束不仅仅是缩小了可行域；它们从根本上改变了优化问题的几何景观。这个练习通过一个清晰的解析示例，向你展示一个无约束问题中的鞍点如何在一个简单的等式约束下转变为一个局部最小值。通过这个练习，你将深入理解可行流形和切空间在定义约束最优解中的关键作用，为你处理更复杂的约束问题打下坚实的直觉基础。",
            "id": "3201338",
            "problem": "考虑无约束目标函数 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$，定义为 $f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$。施加等式约束 $g(x_{1},x_{2})=x_{2}=0$，使得可行流形为 $\\mathcal{M}=\\{(x_{1},x_{2})\\in\\mathbb{R}^{2}\\mid x_{2}=0\\}$。使用多元微积分的基本定义和等式约束优化的 Karush–Kuhn–Tucker (KKT) 条件，完成以下任务：\n\n1. 找出 $f$ 的无约束驻点，并使用 Hessian 矩阵的局部曲率定义对点 $(0,0)$ 进行分类。解释为什么它是 $f$ 在 $\\mathbb{R}^{2}$ 上的一个鞍点。\n\n2. 构建等式约束问题 $\\min f(x_{1},x_{2})$ subject to $g(x_{1},x_{2})=0$，定义拉格朗日函数 $\\mathcal{L}(x_{1},x_{2},\\lambda)=f(x_{1},x_{2})+\\lambda g(x_{1},x_{2})$，并通过一阶 KKT 条件确定候选约束最优点 $(x^{\\star},\\lambda^{\\star})$。\n\n3. 计算在 $x^{\\star}$ 处的切空间，定义为 $T_{x^{\\star}}\\mathcal{M}=\\{v\\in\\mathbb{R}^{2}\\mid \\nabla g(x^{\\star})^{\\top}v=0\\}$，并通过在 $T_{x^{\\star}}\\mathcal{M}$ 上计算二次型 $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 来验证局部最小值的二阶条件。\n\n4. 最后，计算受限二次型 $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 在所有单位向量 $v\\in T_{x^{\\star}}\\mathcal{M}$ 上的最小值，即\n$$\n\\min\\left\\{v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v \\,\\middle|\\, v\\in T_{x^{\\star}}\\mathcal{M},\\, \\|v\\|_{2}=1\\right\\}.\n$$\n将你的最终答案表示为一个精确的实数。无需四舍五入。",
            "solution": "用户提供了一个关于无约束和约束优化的多部分问题。我将首先验证问题陈述，如果认为有效，则继续进行逐步解答。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 目标函数：$f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$，其中 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$。\n- 等式约束：$g(x_{1},x_{2})=x_{2}=0$。\n- 可行流形：$\\mathcal{M}=\\{(x_{1},x_{2})\\in\\mathbb{R}^{2}\\mid x_{2}=0\\}$。\n- 拉格朗日函数定义：$\\mathcal{L}(x_{1},x_{2},\\lambda)=f(x_{1},x_{2})+\\lambda g(x_{1},x_{2})$。\n- $x^{\\star}$ 处的切空间定义：$T_{x^{\\star}}\\mathcal{M}=\\{v\\in\\mathbb{R}^{2}\\mid \\nabla g(x^{\\star})^{\\top}v=0\\}$。\n- 任务1：找出 $f$ 的无约束驻点，并使用其 Hessian 矩阵对点 $(0,0)$ 进行分类。\n- 任务2：构建约束问题，并使用一阶 KKT 条件找到候选最优点 $(x^{\\star},\\lambda^{\\star})$。\n- 任务3：计算切空间 $T_{x^{\\star}}\\mathcal{M}$，并使用 $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 验证局部最小值的二阶条件。\n- 任务4：计算 $\\min\\left\\{v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v \\,\\middle|\\, v\\in T_{x^{\\star}}\\mathcal{M},\\, \\|v\\|_{2}=1\\right\\}$。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学基础：** 该问题是多元微积分和优化理论中的一个标准练习。所有定义（拉格朗日函数、KKT 条件、Hessian 矩阵、切空间）都是标准的且在数学上是正确的。\n- **良构性：** 问题陈述清晰，包含一系列逻辑任务。函数是良态的，确保每个部分都有唯一解。\n- **客观性：** 语言正式、精确，没有任何主观或模糊的术语。\n- **完整性与一致性：** 提供了所有必要的信息，包括函数和定义。问题是自洽的，没有矛盾。\n\n**步骤3：结论与行动**\n该问题有效。它是一个良构的、有科学依据的约束优化练习。我将继续进行解答。\n\n### 解答\n\n**第1部分：无约束分析**\n\n首先，我们通过找到其梯度为零的点来求出无约束目标函数 $f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$ 的驻点。$f$ 的梯度是：\n$$\n\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_{1}} \\\\ \\frac{\\partial f}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 2x_{1} \\\\ -2x_{2} \\end{pmatrix}\n$$\n将梯度设为零向量，$\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，得到方程组：\n$$\n2x_{1} = 0 \\implies x_{1} = 0\n$$\n$$\n-2x_{2} = 0 \\implies x_{2} = 0\n$$\n因此，唯一的无约束驻点是 $(0,0)$。\n\n接下来，我们通过检查描述局部曲率的 $f$ 的 Hessian 矩阵来对此点进行分类。Hessian 矩阵 $\\nabla^{2}f$ 是二阶偏导数矩阵：\n$$\n\\nabla^{2}f(x_{1},x_{2}) = \\begin{pmatrix} \\frac{\\partial^{2} f}{\\partial x_{1}^{2}} & \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{2}} \\\\ \\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{1}} & \\frac{\\partial^{2} f}{\\partial x_{2}^{2}} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\nHessian 矩阵是一个常数矩阵。在驻点 $(0,0)$ 处，我们有 $\\nabla^{2}f(0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$。为了对该点进行分类，我们通过求其特征值来检查该矩阵的定性。特征值是对角线元素，$\\lambda_{1}=2$ 和 $\\lambda_{2}=-2$。由于一个特征值为正，一个为负，Hessian 矩阵是不定的。驻点处的不定 Hessian 矩阵表明该点是一个鞍点。该函数沿 $x_{1}$ 轴具有正曲率（向上凹），沿 $x_{2}$ 轴具有负曲率（向下凹）。\n\n**第2部分：约束优化与 KKT 条件**\n\n我们考虑在等式约束 $g(x_{1},x_{2})=x_{2}=0$ 下最小化 $f(x_{1},x_{2})$ 的问题。该问题的拉格朗日函数为：\n$$\n\\mathcal{L}(x_{1},x_{2},\\lambda) = f(x_{1},x_{2}) + \\lambda g(x_{1},x_{2}) = x_{1}^{2}-x_{2}^{2} + \\lambda x_{2}\n$$\n一阶 Karush–Kuhn–Tucker (KKT) 条件要求拉格朗日函数对所有变量的梯度为零。\n$$\n\\nabla\\mathcal{L}(x_{1},x_{2},\\lambda) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x_{1}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial x_{2}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\end{pmatrix} = \\begin{pmatrix} 2x_{1} \\\\ -2x_{2}+\\lambda \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这给了我们一个包含三个方程的方程组：\n1. $2x_{1} = 0 \\implies x_{1} = 0$\n2. $-2x_{2}+\\lambda = 0$\n3. $x_{2} = 0$\n\n从方程 (1) 可得 $x_{1}=0$。从方程 (3) 可得 $x_{2}=0$。将 $x_{2}=0$ 代入方程 (2) 得到 $-2(0)+\\lambda=0$，这意味着 $\\lambda=0$。\n候选约束最优点是 $x^{\\star}=(x_{1}^{\\star},x_{2}^{\\star})=(0,0)$，对应的拉格朗日乘子为 $\\lambda^{\\star}=0$。\n\n**第3部分：二阶条件与切空间**\n\n现在我们验证局部最小值的二阶条件。首先，我们确定在点 $x^{\\star}=(0,0)$ 处的切空间 $T_{x^{\\star}}\\mathcal{M}$。约束函数为 $g(x_{1},x_{2})=x_{2}$。其梯度为：\n$$\n\\nabla g(x_{1},x_{2}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n该梯度是恒定的。在 $x^{\\star}=(0,0)$ 处，我们有 $\\nabla g(x^{\\star}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。切空间是所有与 $\\nabla g(x^{\\star})$ 正交的向量 $v=(v_{1}, v_{2})^{\\top} \\in \\mathbb{R}^{2}$ 的集合：\n$$\nT_{x^{\\star}}\\mathcal{M} = \\{ v \\in \\mathbb{R}^{2} \\mid \\nabla g(x^{\\star})^{\\top}v = 0 \\}\n$$\n$$\n\\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix} = 0 \\cdot v_{1} + 1 \\cdot v_{2} = v_{2} = 0\n$$\n因此，切空间是所有形式为 $v=(v_{1}, 0)^{\\top}$ 的向量的集合，即 $x_{1}$ 轴。$T_{x^{\\star}}\\mathcal{M} = \\text{span}\\left\\{\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\right\\}$。\n\n接下来，我们计算拉格朗日函数关于变量 $x=(x_{1},x_{2})$ 的 Hessian 矩阵：\n$$\n\\nabla_{x}^{2}\\mathcal{L}(x_{1},x_{2},\\lambda) = \\begin{pmatrix} \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{1}^{2}} & \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{1}\\partial x_{2}} \\\\ \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{2}\\partial x_{1}} & \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{2}^{2}} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\n我们在候选点 $(x^{\\star}, \\lambda^{\\star})=((0,0),0)$ 处计算此 Hessian 矩阵。由于 Hessian 矩阵是常数，$\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star}) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$。\n\n严格局部最小值的二阶充分条件要求二次型 $v^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 对于所有非零向量 $v \\in T_{x^{\\star}}\\mathcal{M}$ 是正定的。设 $v \\in T_{x^{\\star}}\\mathcal{M}$ 是一个非零向量，所以 $v=(v_{1},0)^{\\top}$ 且 $v_{1} \\neq 0$。\n$$\nv^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v = \\begin{pmatrix} v_{1} & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2v_{1} & 0 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ 0 \\end{pmatrix} = 2v_{1}^{2}\n$$\n由于 $v_{1} \\neq 0$，所以 $v_{1}^{2}>0$，因此 $2v_{1}^{2}>0$。条件得到满足，证实了 $x^{\\star}=(0,0)$ 是约束问题的一个严格局部最小值。\n\n**第4部分：最终计算**\n\n我们需要计算二次型 $v^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 在切空间 $T_{x^{\\star}}\\mathcal{M}$ 中所有单位向量 $v$ 上的最小值。这类向量的集合是：\n$$\n\\{v \\in T_{x^{\\star}}\\mathcal{M} \\mid \\|v\\|_{2}=1 \\}\n$$\n切空间中的向量 $v$ 的形式为 $v=(v_{1},0)^{\\top}$。单位范数条件意味着 $\\|v\\|_{2} = \\sqrt{v_{1}^{2}+0^{2}} = |v_{1}| = 1$。这意味着 $v_{1}=1$ 或 $v_{1}=-1$。所以存在两个这样的向量：$v^{(1)}=(1,0)^{\\top}$ 和 $v^{(2)}=(-1,0)^{\\top}$。\n\n从第3部分可知，二次型的值为 $2v_{1}^{2}$。我们需要在 $|v_{1}|=1$ 的条件下找到其最小值。\n如果 $v_{1}=1$，值为 $2(1)^{2}=2$。\n如果 $v_{1}=-1$，值为 $2(-1)^{2}=2$。\n在两种情况下，值都是 $2$。因此，受限二次型的最小值（也是最大值）是 $2$。\n这个值代表了目标函数在约束最小值处沿可行方向的曲率。",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "从理论几何转向实用算法，我们来探讨处理等式约束的两种核心方法。这个练习将指导你实现二次惩罚法和更高级的增广拉格朗日法。通过亲手实现和对比，你将直接观察到引入并迭代更新拉格朗日乘子所带来的优势——它不仅能更精确地满足约束，还能有效避免单纯惩罚法中因惩罚因子过大而导致的数值病态问题。",
            "id": "3201293",
            "problem": "您需要实现并比较两种无约束优化方法，用于解决一个二次等式约束最小二乘问题，且仅使用线性代数和迭代更新。其目标是研究增广拉格朗日乘子更新与纯二次惩罚方法在固定等式约束下的对比情况，从而从第一性原理探讨无约束优化与约束优化之间的关系。\n\n使用的基本原理和定义：\n- 可微函数的无约束最小值出现在其梯度为零的点。\n- 受等式约束的可微函数的约束最小值由 Karush-Kuhn-Tucker (KKT) 条件描述，这些条件通过构造拉格朗日函数并将其对决策变量和乘子的梯度都设为零来导出。\n- 欧几里得范数表示为 $\\|\\cdot\\|_2$。\n- 对于矩阵 $A$ 和向量 $b$，最小二乘目标函数可以写成 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$。\n- 对于矩阵 $C$ 和向量 $d$，等式约束可以写成 $h(x) = C x - d = 0$。\n- 等式约束的增广拉格朗日函数为 $\\mathcal{L}_\\rho(x,\\lambda) = f(x) + \\lambda^\\top h(x) + \\tfrac{\\rho}{2}\\|h(x)\\|_2^2$，其中 $\\lambda$ 是拉格朗日乘子向量，$\\rho$ 是一个正的惩罚参数。\n\n你的程序必须：\n- 实现增广拉格朗日方法，具体做法是：对于固定的 $\\lambda$，交替地最小化关于 $x$ 的 $\\mathcal{L}_\\rho(x,\\lambda)$，然后基于当前的约束残差，使用标准乘子更新规则来更新 $\\lambda$。执行固定次数的迭代，并报告最终的约束违反度 $\\|h(x)\\|_2$。\n- 实现纯二次惩罚方法，通过最小化 $f(x) + \\tfrac{\\rho}{2}\\|h(x)\\|_2^2$ 来实现，不使用乘子。在一个测试中，使用单个 $\\rho$ 值；在另一个测试中，使用一个递增的 $\\rho$ 值序列来模拟约束的收紧。报告最终的约束违反度 $\\|h(x)\\|_2$。\n\n科学真实性条件：\n- 对二次子问题使用精确的线性代数求解。\n- 乘子的迭代更新必须基于当前的约束残差。\n- 除上述基本定义外，不使用任何步长启发式方法或捷径。\n\n角度单位和物理单位：不涉及物理量或角度；因此不需要单位或角度规范。\n\n测试套件：\n- 所有问题都在 $\\mathbb{R}^n$ 中，其中 $n=2$，等式约束的维度为 $m \\in \\{1,2\\}$。\n- 对于每种情况，报告两个量：最终的增广拉格朗日约束违反度和最终的纯惩罚约束违反度。这两个值必须是浮点数。\n\n情况 $\\mathbf{1}$ (理想路径，良态数据)：\n- $A = \\begin{bmatrix} 2 & -1 \\\\ 0 & 3 \\\\ 1 & 1 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 4 \\\\ 2 \\end{bmatrix}$。\n- $C = \\begin{bmatrix} 1 & 2 \\end{bmatrix}$，$d = \\begin{bmatrix} 3 \\end{bmatrix}$。\n- 增广拉格朗日参数：$\\rho = 1.0$，迭代次数 $K = 10$，初始乘子 $\\lambda^{(0)} = \\begin{bmatrix} 0 \\end{bmatrix}$。\n- 纯惩罚参数：单个 $\\rho = 1.0$。\n\n情况 $\\mathbf{2}$ (边界条件：弱惩罚，轻度病态的 $A^\\top A$)：\n- $A = \\begin{bmatrix} 10^{-3} & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0.5 \\end{bmatrix}$。\n- $C = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$，$d = \\begin{bmatrix} 0.2 \\end{bmatrix}$。\n- 增广拉格朗日参数：$\\rho = 10^{-2}$，迭代次数 $K = 50$，初始乘子 $\\lambda^{(0)} = \\begin{bmatrix} 0 \\end{bmatrix}$。\n- 纯惩罚参数：单个 $\\rho = 10^{-2}$。\n\n情况 $\\mathbf{3}$ (边缘情况：冗余等式约束)：\n- $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}$，$b = \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix}$。\n- $C = \\begin{bmatrix} 1 & 1 \\\\ 2 & 2 \\end{bmatrix}$，$d = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 增广拉格朗日参数：$\\rho = 1.0$，迭代次数 $K = 15$，初始乘子 $\\lambda^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 纯惩罚参数：递增序列 $\\rho \\in \\{10^{-1}, 1.0, 10.0\\}$。\n\n算法要求：\n- 对于增广拉格朗日方法，在每次迭代 $k \\in \\{0,1,\\dots,K-1\\}$ 中：计算 $\\mathcal{L}_\\rho(x,\\lambda^{(k)})$ 关于 $x$ 的精确最小化子，然后通过加上 $\\rho$ 乘以当前约束残差来更新乘子。在 $K$ 次迭代后，报告 $\\|h(x^{(K)})\\|_2$。\n- 对于纯惩罚方法，计算给定 $\\rho$ (或者对于序列中的每个 $\\rho$，顺序计算) 的精确最小化子，并在最后一次求解后报告最终的 $\\|h(x)\\|_2$。\n\n最终输出格式：\n- 你的程序应该生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例贡献一个包含两个浮点数的列表，顺序为 $\\left[\\text{增广拉格朗日方法的 } \\|h(x)\\|_2, \\text{纯惩罚方法的 } \\|h(x)\\|_2\\right]$。因此，最终输出必须是 $\\big[\\,[a_1,b_1],[a_2,b_2],[a_3,b_3]\\,\\big]$ 的形式，其中每个 $a_i$ 和 $b_i$ 分别是对应于情况 $\\mathbf{1}$、$\\mathbf{2}$ 和 $\\mathbf{3}$ 的浮点数。",
            "solution": "该问题是有效的。它提出了计算科学与优化领域中的一个适定任务，该任务基于既定的数学原理。问题是自包含的，提供了所有必要的数据和定义。它是客观、科学上合理，且没有矛盾或歧义的。\n\n问题的核心是为以下等式约束二次规划 (ECQP) 问题找到最小化子 $x \\in \\mathbb{R}^n$：\n$$\n\\min_{x} f(x) = \\frac{1}{2}\\|A x - b\\|_2^2 \\quad \\text{subject to} \\quad h(x) = C x - d = 0\n$$\n其中 $A \\in \\mathbb{R}^{p \\times n}$，$b \\in \\mathbb{R}^p$，$C \\in \\mathbb{R}^{m \\times n}$，$d \\in \\mathbb{R}^m$。我们将实现并比较两种迭代方法，通过处理一系列无约束二次子问题来解决此问题。\n\n首先，我们展开目标函数及其梯度：\n$f(x) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)$\n关于 $x$ 的梯度是：\n$\\nabla_x f(x) = A^\\top A x - A^\\top b$。\n\n我们还需要约束函数的范数平方的梯度：\n$\\frac{1}{2}\\|h(x)\\|_2^2 = \\frac{1}{2}\\|Cx - d\\|_2^2 = \\frac{1}{2}(x^\\top C^\\top C x - 2 d^\\top C x + d^\\top d)$\n该项的梯度是：\n$\\nabla_x \\left(\\frac{1}{2}\\|h(x)\\|_2^2\\right) = C^\\top(Cx-d) = C^\\top C x - C^\\top d$。\n\n**1. 增广拉格朗日方法**\n\n此问题的增广拉格朗日函数由下式给出：\n$$\n\\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda^\\top h(x) + \\frac{\\rho}{2}\\|h(x)\\|_2^2\n$$\n其中 $\\lambda \\in \\mathbb{R}^m$ 是拉格朗日乘子向量，$\\rho > 0$ 是惩罚参数。\n\n增广拉格朗日方法是一个迭代过程。在每次迭代 $k$ 中，对于一个固定的乘子 $\\lambda^{(k)}$，我们通过最小化关于 $x$ 的 $\\mathcal{L}_\\rho(x, \\lambda^{(k)})$ 来找到下一个迭代点 $x^{(k+1)}$。由于 $\\mathcal{L}_\\rho(x, \\lambda^{(k)})$ 是 $x$ 的一个二次函数，其最小值在它关于 $x$ 的梯度为零处找到：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x, \\lambda^{(k)}) = \\nabla_x f(x) + \\nabla_x(\\lambda^{(k)\\top}(Cx-d)) + \\nabla_x\\left(\\frac{\\rho}{2}\\|Cx - d\\|_2^2\\right) = 0\n$$\n代入梯度表达式：\n$$\n(A^\\top A x - A^\\top b) + C^\\top \\lambda^{(k)} + \\rho (C^\\top C x - C^\\top d) = 0\n$$\n为了求解 $x$，我们将包含 $x$ 的项组合在一起：\n$$\n(A^\\top A + \\rho C^\\top C) x = A^\\top b - C^\\top \\lambda^{(k)} + \\rho C^\\top d\n$$\n这是一个 $H x = g$ 形式的线性系统，其中：\n- $H = A^\\top A + \\rho C^\\top C$\n- $g = A^\\top b + C^\\top(\\rho d - \\lambda^{(k)})$\n\n算法按以下步骤进行，对于 $k = 0, 1, \\dots, K-1$：\n1.  求解 $x^{(k+1)}$: $x^{(k+1)} = (A^\\top A + \\rho C^\\top C)^{-1} (A^\\top b + C^\\top(\\rho d - \\lambda^{(k)}))$。\n2.  更新拉格朗日乘子：$\\lambda^{(k+1)} = \\lambda^{(k)} + \\rho h(x^{(k+1)}) = \\lambda^{(k)} + \\rho(C x^{(k+1)} - d)$。\n\n经过 $K$ 次迭代后，最终的约束违反度计算为 $\\|h(x^{(K)})\\|_2 = \\|C x^{(K)} - d\\|_2$。\n\n**2. 纯二次惩罚方法**\n\n纯二次惩罚方法旨在通过最小化无约束惩罚函数来解决约束问题：\n$$\nP_\\rho(x) = f(x) + \\frac{\\rho}{2}\\|h(x)\\|_2^2\n$$\n该函数也是关于 $x$ 的二次函数。其最小值通过将其梯度设为零来找到：\n$$\n\\nabla_x P_\\rho(x) = \\nabla_x f(x) + \\rho \\nabla_x\\left(\\frac{1}{2}\\|h(x)\\|_2^2\\right) = 0\n$$\n代入梯度表达式：\n$$\n(A^\\top A x - A^\\top b) + \\rho (C^\\top C x - C^\\top d) = 0\n$$\n同样，我们将包含 $x$ 的项组合在一起以形成一个线性系统：\n$$\n(A^\\top A + \\rho C^\\top C) x = A^\\top b + \\rho C^\\top d\n$$\n这是一个 $H x = g'$ 形式的线性系统，其中：\n- $H = A^\\top A + \\rho C^\\top C$\n- $g' = A^\\top b + \\rho C^\\top d$\n\n这对应于增广拉格朗日方法在 $\\lambda$ 固定为 $0$ 时的单步。对于测试用例，我们对给定的 $\\rho$ 求解一次该系统。对于具有 $\\rho$ 值序列的情况，我们根据问题规定，对序列中的最终值求解该系统。最终的约束违反度为 $\\|h(x)\\|_2 = \\|C x - d\\|_2$。\n\n实现将遵循这些推导出的方程，使用 `numpy.linalg.solve` 对子问题进行精确的线性代数求解。",
            "answer": "```python\nimport numpy as np\n\ndef run_augmented_lagrangian(A, b, C, d, rho, K, lambda0):\n    \"\"\"\n    Implements the Augmented Lagrangian method for a quadratic equality-constrained LS problem.\n\n    Args:\n        A (np.ndarray): Matrix for the LS objective.\n        b (np.ndarray): Vector for the LS objective.\n        C (np.ndarray): Matrix for the equality constraint.\n        d (np.ndarray): Vector for the equality constraint.\n        rho (float): Penalty parameter.\n        K (int): Number of iterations.\n        lambda0 (np.ndarray): Initial Lagrange multipliers.\n\n    Returns:\n        float: The final constraint violation norm ||Cx - d||_2.\n    \"\"\"\n    AtA = A.T @ A\n    Atb = A.T @ b\n    CtC = C.T @ C\n    Ct = C.T\n\n    # The Hessian of the Lagrangian is constant for all iterations\n    H = AtA + rho * CtC\n\n    lambda_k = lambda0.copy()\n    x_k = np.zeros(A.shape[1]) # Initialize x\n\n    for _ in range(K):\n        # Form the right-hand side vector g\n        g = Atb + Ct @ (rho * d - lambda_k)\n        \n        # Solve the linear system for x_{k+1}\n        x_k = np.linalg.solve(H, g)\n        \n        # Update the Lagrange multiplier\n        constraint_residual = C @ x_k - d\n        lambda_k = lambda_k + rho * constraint_residual\n    \n    final_constraint_violation = np.linalg.norm(C @ x_k - d)\n    return final_constraint_violation\n\ndef run_penalty_method(A, b, C, d, rho_values):\n    \"\"\"\n    Implements the pure quadratic penalty method.\n\n    Args:\n        A (np.ndarray): Matrix for the LS objective.\n        b (np.ndarray): Vector for the LS objective.\n        C (np.ndarray): Matrix for the equality constraint.\n        d (np.ndarray): Vector for the equality constraint.\n        rho_values (list or tuple): A single rho or a schedule of rho values.\n\n    Returns:\n        float: The final constraint violation norm ||Cx - d||_2.\n    \"\"\"\n    # Use the final rho from the schedule as per problem description\n    rho = rho_values[-1]\n\n    AtA = A.T @ A\n    Atb = A.T @ b\n    CtC = C.T @ C\n    Ctd = C.T @ d\n    \n    H = AtA + rho * CtC\n    g_prime = Atb + rho * Ctd\n    \n    x = np.linalg.solve(H, g_prime)\n    \n    constraint_violation = np.linalg.norm(C @ x - d)\n    return constraint_violation\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the optimization algorithms.\n    \"\"\"\n    test_cases = [\n        {\n            # Case 1 (happy path, well-conditioned data)\n            \"A\": np.array([[2., -1.], [0., 3.], [1., 1.]]),\n            \"b\": np.array([1., 4., 2.]),\n            \"C\": np.array([[1., 2.]]),\n            \"d\": np.array([3.]),\n            \"alm_params\": {\"rho\": 1.0, \"K\": 10, \"lambda0\": np.array([0.])},\n            \"penalty_params\": {\"rho_values\": [1.0]}\n        },\n        {\n            # Case 2 (boundary condition: weak penalty, mildly ill-conditioned A^T A)\n            \"A\": np.array([[1e-3, 0.], [0., 1.], [1., -1.]]),\n            \"b\": np.array([0., 1., 0.5]),\n            \"C\": np.array([[1., 0.]]),\n            \"d\": np.array([0.2]),\n            \"alm_params\": {\"rho\": 1e-2, \"K\": 50, \"lambda0\": np.array([0.])},\n            \"penalty_params\": {\"rho_values\": [1e-2]}\n        },\n        {\n            # Case 3 (edge case: redundant equality constraints)\n            \"A\": np.array([[1., 2.], [3., 4.], [5., 6.]]),\n            \"b\": np.array([7., 8., 9.]),\n            \"C\": np.array([[1., 1.], [2., 2.]]),\n            \"d\": np.array([0., 0.]),\n            \"alm_params\": {\"rho\": 1.0, \"K\": 15, \"lambda0\": np.array([0., 0.])},\n            \"penalty_params\": {\"rho_values\": [1e-1, 1.0, 10.0]}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, b, C, d = case[\"A\"], case[\"b\"], case[\"C\"], case[\"d\"]\n        \n        # Run Augmented Lagrangian Method\n        alm_params = case[\"alm_params\"]\n        alm_violation = run_augmented_lagrangian(A, b, C, d, **alm_params)\n        \n        # Run Pure Penalty Method\n        penalty_params = case[\"penalty_params\"]\n        penalty_violation = run_penalty_method(A, b, C, d, **penalty_params)\n        \n        results.append(f\"[{alm_violation},{penalty_violation}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的许多优化问题都具有非凸的可行集，这给求解带来了巨大挑战。本练习将带你进入非凸优化的领域，比较两种截然不同的策略：投影梯度下降法和惩罚法。你将发现，投影法通过在每一步都强制保持可行性来确保最终解的有效性，而惩罚法在非凸背景下可能会被困在不可行的局部最小值。这个实践强调了在面对复杂约束几何时，仔细选择算法的重要性。",
            "id": "3201335",
            "problem": "您需要实现并比较两种迭代方法，用于解决一个二维约束优化问题，其中可行集是非凸的，但具有光滑的边界。目标是确定无约束代理函数何时可能存在不可行的局部极小值点，并验证在每一步梯度下降后进行正交投影能够强制保证可行性。基本设置为一个光滑的目标函数、光滑的不等式约束以及标准的欧几里得几何。\n\n用作基础的基本定义：\n- 约束优化问题定义为，在一个由光滑不等式约束 $c_i(\\mathbf{x}) \\le 0$（其中 $i$ 属于一个有限索引集）定义的可行集 $\\mathcal{F}$ 上，最小化一个光滑目标函数 $f(\\mathbf{x})$。\n- 一个点 $\\mathbf{x}$ 是可行的，当且仅当所有不等式 $c_i(\\mathbf{x}) \\le 0$ 都满足。\n- 一个点 $\\mathbf{y}$ 到一个闭集 $\\mathcal{F}$ 上的正交投影，是指在 $\\mathcal{F}$ 中的一个点 $\\Pi_{\\mathcal{F}}(\\mathbf{y})$，它在所有 $\\mathbf{x}\\in\\mathcal{F}$ 中最小化了欧几里得距离 $\\|\\mathbf{x}-\\mathbf{y}\\|$。\n\n您的程序必须实现并比较：\n- 一种在每一步对 $f(\\mathbf{x})$ 进行梯度下降和对 $\\mathcal{F}$ 进行正交投影交替进行的方法。您必须根据第一性原理，为下面给出的特定可行集证明其投影形式的合理性。\n- 一种将约束问题替换为一个无约束代理问题的方法，其中代理函数在违反约束时具有光滑的二次增长，并对该代理函数应用梯度下降。您必须根据第一性原理证明该代理函数构造的合理性。\n\n使用以下两个可行集，两者均为具有光滑边界的非凸集：\n- A 类情况（环形）：$\\mathcal{F} = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid r_{\\mathrm{in}}^2 \\le \\|\\mathbf{x}\\|^2 \\le R_{\\mathrm{out}}^2\\}$，具有两个光滑不等式约束 $c_1(\\mathbf{x}) = r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2 \\le 0$ 和 $c_2(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$。此集合因包含一个孔洞而是非凸的。\n- B 类情况（两个圆盘的并集）：$\\mathcal{F} = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid (\\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2) \\le 0\\}$，即两个以 $\\mathbf{c}_1$ 和 $\\mathbf{c}_2$ 为中心、半径为 $r$ 的不相交闭圆盘的并集。其边界是光滑的（不相交的圆），且该集合是非凸的（不连通的组件）。\n\n目标函数是光滑函数 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$。\n\n实现以下两种算法：\n- 投影梯度下降法 (Projected Gradient Descent)：迭代 $\\mathbf{y}_{k} = \\mathbf{x}_{k} - \\alpha \\nabla f(\\mathbf{x}_{k})$，然后 $\\mathbf{x}_{k+1} = \\Pi_{\\mathcal{F}}(\\mathbf{y}_{k})$，其中 $\\alpha > 0$ 是一个固定的步长。对于到环形区域的投影，需从正交投影的定义出发证明径向裁剪的合理性。对于到两个圆盘并集的投影，需证明当 $\\mathbf{y}$ 不可行时，到并集上的正交投影等于到两个圆盘中较近的那个投影；当 $\\mathbf{y}$ 已经可行时，投影等于 $\\mathbf{y}$ 本身。\n- 二次罚函数梯度下降法 (Quadratic Penalty Gradient Descent)：构建一个无约束代理函数 $F_{\\mu}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sum_i \\phi(c_i(\\mathbf{x}))$，其中 $\\phi(t)$ 是一个光滑函数，当 $t > 0$ 时呈二次增长，当 $t \\le 0$ 时为零，而 $\\mu > 0$ 是一个罚参数。对于环形区域，使用两个不等式惩罚项。对于圆盘并集，对乘积约束使用一个不等式惩罚项。应用梯度下降法 $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\beta \\nabla F_{\\mu}(\\mathbf{x}_k)$，步长 $\\beta > 0$ 固定。\n\n测试套件：\n- 测试 $1$（环形，不可行的无约束极小值点）：$r_{\\mathrm{in}} = 1$, $R_{\\mathrm{out}} = 2$，初始点 $\\mathbf{x}_0 = (0.3, 0.4)$，投影步长 $\\alpha = 0.2$，罚函数步长 $\\beta = 0.05$，罚参数 $\\mu = 0.5$，迭代次数 $N = 300$。\n- 测试 $2$（圆盘并集，不可行的无约束极小值点）：圆盘半径 $r = 1$，中心点 $\\mathbf{c}_1 = (1.5, 0)$ 和 $\\mathbf{c}_2 = (-1.5, 0)$，初始点 $\\mathbf{x}_0 = (0, 0)$，$\\alpha = 0.2$，$\\beta = 0.05$，$\\mu = 0.1$，迭代次数 $N = 300$。\n- 测试 $3$（环形，从边界开始，强惩罚）：$r_{\\mathrm{in}} = 1$, $R_{\\mathrm{out}} = 2$，初始点 $\\mathbf{x}_0 = (1.0, 0.0)$，$\\alpha = 0.2$，$\\beta = 0.05$，$\\mu = 2.0$，迭代次数 $N = 300$。\n\n对于每个测试用例，运行两种方法并按以下顺序报告四个量：\n- 投影梯度下降法 (Projected Gradient Descent) 结束后 $f(\\mathbf{x})$ 的最终值（一个浮点数）。\n- 投影梯度下降法最终点的可行性指标（一个布尔值，使用指定的不等式判断）。\n- 二次罚函数梯度下降法 (Quadratic Penalty Gradient Descent) 结束后 $f(\\mathbf{x})$ 的最终值（一个浮点数）。\n- 二次罚函数最终点的可行性指标（一个布尔值）。\n\n您的程序应生成单行输出，其中包含来自测试套件的所有结果，这些结果被展平为一个列表，形式为用方括号括起来的逗号分隔列表（例如，“[$r_1$,$b_1$,$r_2$,$b_2$,$r_3$,$b_3$,$r_4$,$b_4$,$r_5$,$b_5$,$r_6$,$b_6$]”），其中 $r_i$ 是浮点数，$b_i$ 是布尔值。不涉及任何物理单位。角度（若有）必须以弧度为单位；然而，此问题不要求任何三角函数角度的输入或输出。\n\n设计中有意包含了在 $\\mu$ 较小时，无约束代理函数可能存在不可行局部最小值的情况，以及一个具有较大 $\\mu$ 以减少不可行性的情况。您的实现必须是完全确定性的，并且不需要任何用户输入。仅使用指定的运行时环境。",
            "solution": "该问题要求实现和比较两种不同的迭代方法来解决一个约束优化问题：投影梯度下降法（PGD）和二次罚函数法。目标是在两个不同的非凸可行集 $\\mathcal{F}$ 上，对 $\\mathbf{x} \\in \\mathbb{R}^2$ 最小化光滑函数 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$。\n\n首先，我们确定目标函数的梯度。给定 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 = x_1^2 + x_2^2$，其梯度为：\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix} = 2\\mathbf{x}\n$$\n\n### 方法一：投影梯度下降法 (PGD)\n\nPGD 方法是一种用于约束优化的迭代算法。每次迭代包括两个步骤：对目标函数执行一次标准的梯度下降步，然后将结果正交投影回可行集 $\\mathcal{F}$。迭代过程定义如下：\n$$\n\\mathbf{y}_{k} = \\mathbf{x}_{k} - \\alpha \\nabla f(\\mathbf{x}_{k})\n$$\n$$\n\\mathbf{x}_{k+1} = \\Pi_{\\mathcal{F}}(\\mathbf{y}_{k})\n$$\n其中 $\\alpha > 0$ 是步长，$\\Pi_{\\mathcal{F}}(\\mathbf{y})$ 是点 $\\mathbf{y}$ 到集合 $\\mathcal{F}$ 上的正交投影。该投影是 $\\mathcal{F}$ 中在欧几里得范数意义下离 $\\mathbf{y}$ 最近的点：\n$$\n\\Pi_{\\mathcal{F}}(\\mathbf{y}) = \\arg\\min_{\\mathbf{z} \\in \\mathcal{F}} \\|\\mathbf{z} - \\mathbf{y}\\|\n$$\n代入 $\\nabla f(\\mathbf{x}_k) = 2\\mathbf{x}_k$，梯度步变为 $\\mathbf{y}_k = \\mathbf{x}_k - \\alpha(2\\mathbf{x}_k) = (1 - 2\\alpha)\\mathbf{x}_k$。\n\n我们现在必须为给定的可行集证明其投影算子的合理性。\n\n**情况 A：环形**\n可行集为 $\\mathcal{F}_A = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid r_{\\mathrm{in}} \\le \\|\\mathbf{x}\\| \\le R_{\\mathrm{out}}\\}$。为了找到投影 $\\Pi_{\\mathcal{F}_A}(\\mathbf{y})$，我们在 $\\mathbf{z} \\in \\mathcal{F}_A$ 的条件下最小化 $\\|\\mathbf{z} - \\mathbf{y}\\|^2$。由于目标函数 $\\|\\mathbf{z} - \\mathbf{y}\\|^2$ 和集合 $\\mathcal{F}_A$ 都关于原点具有旋转对称性，最优解 $\\mathbf{z}$ 必定位于从原点出发穿过 $\\mathbf{y}$ 的射线上。因此，假设 $\\mathbf{y} \\ne \\mathbf{0}$，则 $\\mathbf{z}$ 的形式必为 $\\mathbf{z} = \\rho \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|}$，其中 $\\rho$ 是某个标量。对 $\\mathbf{z}$ 的约束变为 $r_{\\mathrm{in}} \\le \\rho \\le R_{\\mathrm{out}}$。\n最小化问题简化为在区间 $[r_{\\mathrm{in}}, R_{\\mathrm{out}}]$ 中找到一个 $\\rho$，使得 $\\|\\rho \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|} - \\mathbf{y}\\|^2 = (\\rho - \\|\\mathbf{y}\\|)^2$ 最小化。这是一个一维问题，即在区间 $[r_{\\mathrm{in}}, R_{\\mathrm{out}}]$ 中找到离值 $\\|\\mathbf{y}\\|$ 最近的点。解决方案是将 $\\|\\mathbf{y}\\|$ 限制在该区间内：\n- 如果 $\\|\\mathbf{y}\\| < r_{\\mathrm{in}}$，则 $\\rho = r_{\\mathrm{in}}$。\n- 如果 $r_{\\mathrm{in}} \\le \\|\\mathbf{y}\\| \\le R_{\\mathrm{out}}$，则 $\\rho = \\|\\mathbf{y}\\|$。\n- 如果 $\\|\\mathbf{y}\\| > R_{\\mathrm{out}}$，则 $\\rho = R_{\\mathrm{out}}$。\n这可以写成 $\\rho = \\text{clip}(\\|\\mathbf{y}\\|, r_{\\mathrm{in}}, R_{\\mathrm{out}})$。因此，投影为：\n$$\n\\Pi_{\\mathcal{F}_A}(\\mathbf{y}) = \\text{clip}(\\|\\mathbf{y}\\|, r_{\\mathrm{in}}, R_{\\mathrm{out}}) \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|}\n$$\n如果 $\\mathbf{y} = \\mathbf{0}$，其范数为 $0 < r_{\\mathrm{in}}$。在 $\\mathcal{F}_A$ 中最近的点是内圆上所有半径为 $r_{\\mathrm{in}}$ 的点。任何这样的点都是一个有效的投影。为了确定性，我们可以选择一个特定的点，例如 $(r_{\\mathrm{in}}, 0)$。\n\n**情况 B：两个圆盘的并集**\n可行集为 $\\mathcal{F}_B = D_1 \\cup D_2$，其中 $D_1 = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}_1\\| \\le r\\}$ 和 $D_2 = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}_2\\| \\le r\\}$ 是两个不相交的闭圆盘。投影 $\\Pi_{\\mathcal{F}_B}(\\mathbf{y})$ 是在 $D_1 \\cup D_2$ 中使 $\\|\\mathbf{z} - \\mathbf{y}\\|$ 最小化的点 $\\mathbf{z}$。这个最小值必须在 $D_1$ 或 $D_2$ 中达到。设 $\\mathbf{p}_1 = \\Pi_{D_1}(\\mathbf{y})$ 和 $\\mathbf{p}_2 = \\Pi_{D_2}(\\mathbf{y})$ 分别是 $\\mathbf{y}$ 到各个圆盘上的投影。到并集上的投影就是离 $\\mathbf{y}$ 更近的那个：\n$$\n\\Pi_{\\mathcal{F}_B}(\\mathbf{y}) = \n\\begin{cases} \n\\mathbf{p}_1 & \\text{if } \\|\\mathbf{y} - \\mathbf{p}_1\\| \\le \\|\\mathbf{y} - \\mathbf{p}_2\\| \\\\\n\\mathbf{p}_2 & \\text{otherwise}\n\\end{cases}\n$$\n到单个圆盘 $D = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}\\| \\le r\\}$ 的投影为：\n- 如果 $\\mathbf{y} \\in D$，则 $\\Pi_D(\\mathbf{y}) = \\mathbf{y}$。\n- 如果 $\\mathbf{y} \\notin D$，投影点位于边界上，在连接中心 $\\mathbf{c}$ 和 $\\mathbf{y}$ 的线段上。因此，$\\Pi_D(\\mathbf{y}) = \\mathbf{c} + r \\frac{\\mathbf{y}-\\mathbf{c}}{\\|\\mathbf{y}-\\mathbf{c}\\|}$。\n\n### 方法二：二次罚函数梯度下降法\n\n该方法通过在目标函数中添加一个惩罚约束违反的罚项，将约束问题转化为一系列无约束问题。无约束代理函数为：\n$$\nF_{\\mu}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sum_i \\phi(c_i(\\mathbf{x}))\n$$\n其中 $\\mu > 0$ 是罚参数，$\\phi(t)$ 是一个罚函数。我们使用 $\\phi(t) = (\\max(0, t))^2$，这是一个光滑 ($C^1$) 函数，其导数为 $\\phi'(t) = 2\\max(0, t)$。然后我们对 $F_{\\mu}(\\mathbf{x})$ 应用梯度下降法：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\beta \\nabla F_{\\mu}(\\mathbf{x}_k)\n$$\n步长为 $\\beta > 0$。使用链式法则，代理函数的梯度为：\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\sum_i \\phi'(c_i(\\mathbf{x})) \\nabla c_i(\\mathbf{x}) = 2\\mathbf{x} + \\mu \\sum_i 2\\max(0, c_i(\\mathbf{x})) \\nabla c_i(\\mathbf{x})\n$$\n\n**情况 A：环形**\n约束条件为 $c_1(\\mathbf{x}) = r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2 \\le 0$ 和 $c_2(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$。它们的梯度是 $\\nabla c_1(\\mathbf{x}) = -2\\mathbf{x}$ 和 $\\nabla c_2(\\mathbf{x}) = 2\\mathbf{x}$。代理函数 $F_\\mu(\\mathbf{x})$ 的梯度是：\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = 2\\mathbf{x} + 2\\mu \\left( \\max(0, r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2)(-2\\mathbf{x}) + \\max(0, \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2)(2\\mathbf{x}) \\right)\n$$\n对于较小的罚参数 $\\mu$，代理函数 $F_{\\mu}(\\mathbf{x})$ 可以在 $\\mathbf{x}=\\mathbf{0}$ 处有一个不可行的局部最小值，这可能会使梯度下降算法陷入困境。\n\n**情况 B：两个圆盘的并集**\n可行集由单个不等式描述：$c(\\mathbf{x}) = (\\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2) \\le 0$。令 $g_1(\\mathbf{x}) = \\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2$ 和 $g_2(\\mathbf{x}) = \\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2$。则 $c(\\mathbf{x}) = g_1(\\mathbf{x})g_2(\\mathbf{x})$。根据乘积法则，$c(\\mathbf{x})$ 的梯度为：\n$$\n\\nabla c(\\mathbf{x}) = g_2(\\mathbf{x})\\nabla g_1(\\mathbf{x}) + g_1(\\mathbf{x})\\nabla g_2(\\mathbf{x}) = g_2(\\mathbf{x})(2(\\mathbf{x}-\\mathbf{c}_1)) + g_1(\\mathbf{x})(2(\\mathbf{x}-\\mathbf{c}_2))\n$$\n代理函数 $F_\\mu(\\mathbf{x})$ 的梯度是：\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = 2\\mathbf{x} + 2\\mu \\max(0, c(\\mathbf{x})) \\nabla c(\\mathbf{x})\n$$\n与环形情况类似，该代理函数可以在 $\\mathbf{x}=\\mathbf{0}$（该点到 $\\mathbf{c}_1$ 和 $\\mathbf{c}_2$ 的距离相等）处有一个不可行的局部最小值，这可能使算法陷入困境。\n\n### 可行性检查\n\n在 $N$ 次迭代后，对于最终点 $\\mathbf{x}_{\\text{final}}$，我们必须检查它是否位于可行集 $\\mathcal{F}$ 中。\n- **对于环形区域：** 检查是否同时满足 $r_{\\mathrm{in}}^2 - \\|\\mathbf{x}_{\\text{final}}\\|^2 \\le 0$ 和 $\\|\\mathbf{x}_{\\text{final}}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$。\n- **对于圆盘并集：** 检查是否满足 $(\\|\\mathbf{x}_{\\text{final}}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}_{\\text{final}}-\\mathbf{c}_2\\|^2 - r^2) \\le 0$。\n\n实现将针对指定的测试用例执行这两种算法，并报告每种算法的最终目标值和可行性状态。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Projected Gradient Descent and Quadratic Penalty methods\n    for constrained optimization on nonconvex sets.\n    \"\"\"\n\n    # --- Helper Functions for Algorithms ---\n\n    def f_obj(x):\n        \"\"\"Objective function f(x) = ||x||^2.\"\"\"\n        return np.dot(x, x)\n\n    def grad_f_obj(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return 2 * x\n\n    # --- Projected Gradient Descent (PGD) Implementation ---\n\n    def project_annulus(y, r_in, R_out):\n        \"\"\"Projects a point y onto the annulus.\"\"\"\n        norm_y = np.linalg.norm(y)\n        if norm_y == 0.0:\n            # Project origin to the closest point in the annulus.\n            # Arbitrarily choose (r_in, 0) for determinism.\n            return np.array([r_in, 0.0])\n        \n        clamped_norm = np.clip(norm_y, r_in, R_out)\n        return y * (clamped_norm / norm_y)\n\n    def project_disk(y, c, r):\n        \"\"\"Projects a point y onto a single disk.\"\"\"\n        vec_to_center = y - c\n        dist_from_center = np.linalg.norm(vec_to_center)\n        if dist_from_center = r:\n            return y\n        else:\n            return c + r * vec_to_center / dist_from_center\n\n    def project_union_disks(y, c1, c2, r):\n        \"\"\"Projects a point y onto the union of two disks.\"\"\"\n        p1 = project_disk(y, c1, r)\n        p2 = project_disk(y, c2, r)\n        \n        dist_sq_1 = np.sum((y - p1)**2)\n        dist_sq_2 = np.sum((y - p2)**2)\n        \n        if dist_sq_1 = dist_sq_2:\n            return p1\n        else:\n            return p2\n\n    def run_pgd(x0, alpha, N, proj_func, proj_params):\n        \"\"\"Runs the Projected Gradient Descent algorithm.\"\"\"\n        x = np.array(x0, dtype=float)\n        for _ in range(N):\n            y = x - alpha * grad_f_obj(x)\n            x = proj_func(y, **proj_params)\n        return x\n\n    # --- Quadratic Penalty Method Implementation ---\n\n    def grad_penalty_annulus(x, mu, r_in, R_out):\n        \"\"\"Computes the gradient of the penalty surrogate for the annulus.\"\"\"\n        norm_x_sq = np.dot(x, x)\n        c1 = r_in**2 - norm_x_sq\n        c2 = norm_x_sq - R_out**2\n        \n        grad_c1 = -2 * x\n        grad_c2 = 2 * x\n        \n        # phi'(t) = 2 * max(0, t)\n        penalty_grad = mu * (2 * np.maximum(0, c1) * grad_c1 + 2 * np.maximum(0, c2) * grad_c2)\n        return grad_f_obj(x) + penalty_grad\n\n    def grad_penalty_disks(x, mu, c1, c2, r):\n        \"\"\"Computes the gradient of the penalty surrogate for the union of disks.\"\"\"\n        g1 = np.sum((x - c1)**2) - r**2\n        g2 = np.sum((x - c2)**2) - r**2\n        c_prod = g1 * g2\n\n        if c_prod > 0:\n            grad_g1 = 2 * (x - c1)\n            grad_g2 = 2 * (x - c2)\n            grad_c_prod = g2 * grad_g1 + g1 * grad_g2\n            \n            # phi'(t) = 2 * max(0, t)\n            penalty_grad = mu * (2 * c_prod * grad_c_prod)\n            return grad_f_obj(x) + penalty_grad\n        else:\n            return grad_f_obj(x)\n\n    def run_penalty_method(x0, beta, N, grad_penalty_func, grad_params):\n        \"\"\"Runs the Gradient Descent on the quadratic penalty surrogate.\"\"\"\n        x = np.array(x0, dtype=float)\n        for _ in range(N):\n            grad = grad_penalty_func(x, **grad_params)\n            x = x - beta * grad\n        return x\n\n    # --- Feasibility Checkers ---\n    \n    def is_feasible_annulus(x, r_in, R_out):\n        \"\"\"Checks if a point is in the annulus.\"\"\"\n        norm_x_sq = np.dot(x, x)\n        return r_in**2 = norm_x_sq = R_out**2\n\n    def is_feasible_disks(x, c1, c2, r):\n        \"\"\"Checks if a point is in the union of two disks.\"\"\"\n        g1 = np.sum((x - c1)**2) - r**2\n        g2 = np.sum((x - c2)**2) - r**2\n        return (g1 * g2) = 0\n\n    # --- Test Suite ---\n    \n    test_cases = [\n        # Test 1: Annulus, infeasible unconstrained minimizer\n        {\n            \"type\": \"annulus\", \"r_in\": 1.0, \"R_out\": 2.0, \"x0\": (0.3, 0.4),\n            \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 0.5, \"N\": 300\n        },\n        # Test 2: Union of disks, infeasible unconstrained minimizer\n        {\n            \"type\": \"disks\", \"r\": 1.0, \"c1\": (1.5, 0.0), \"c2\": (-1.5, 0.0),\n            \"x0\": (0.0, 0.0), \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 0.1, \"N\": 300\n        },\n        # Test 3: Annulus, boundary start, strong penalty\n        {\n            \"type\": \"annulus\", \"r_in\": 1.0, \"R_out\": 2.0, \"x0\": (1.0, 0.0),\n            \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 2.0, \"N\": 300\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x0 = case[\"x0\"]\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        mu = case[\"mu\"]\n        N = case[\"N\"]\n\n        if case[\"type\"] == \"annulus\":\n            r_in, R_out = case[\"r_in\"], case[\"R_out\"]\n            \n            # PGD\n            proj_params = {\"r_in\": r_in, \"R_out\": R_out}\n            x_pgd = run_pgd(x0, alpha, N, project_annulus, proj_params)\n            f_pgd = f_obj(x_pgd)\n            feasible_pgd = is_feasible_annulus(x_pgd, r_in, R_out)\n            \n            # Penalty\n            grad_params = {\"mu\": mu, \"r_in\": r_in, \"R_out\": R_out}\n            x_penalty = run_penalty_method(x0, beta, N, grad_penalty_annulus, grad_params)\n            f_penalty = f_obj(x_penalty)\n            feasible_penalty = is_feasible_annulus(x_penalty, r_in, R_out)\n\n        elif case[\"type\"] == \"disks\":\n            r, c1, c2 = case[\"r\"], np.array(case[\"c1\"]), np.array(case[\"c2\"])\n\n            # PGD\n            proj_params = {\"c1\": c1, \"c2\": c2, \"r\": r}\n            x_pgd = run_pgd(x0, alpha, N, project_union_disks, proj_params)\n            f_pgd = f_obj(x_pgd)\n            feasible_pgd = is_feasible_disks(x_pgd, c1, c2, r)\n\n            # Penalty\n            grad_params = {\"mu\": mu, \"c1\": c1, \"c2\": c2, \"r\": r}\n            x_penalty = run_penalty_method(x0, beta, N, grad_penalty_disks, grad_params)\n            f_penalty = f_obj(x_penalty)\n            feasible_penalty = is_feasible_disks(x_penalty, c1, c2, r)\n\n        results.extend([f_pgd, feasible_pgd, f_penalty, feasible_penalty])\n\n    # Final print statement in the exact required format.\n    # Convert booleans to lowercase 'true'/'false'.\n    bool_to_str = {True: 'true', False: 'false'}\n    formatted_results = [bool_to_str.get(item, str(item)) for item in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}