{
    "hands_on_practices": [
        {
            "introduction": "Gradient descent is the workhorse of modern optimization, but its effectiveness can be deceptive. A seemingly simple algorithm, its success hinges on navigating the function's \"landscape,\" which can be fraught with challenges like long, narrow valleys. In this exercise, you will implement gradient descent on a function specifically designed to be ill-conditioned, allowing you to directly compare the performance of a fixed step-size against a more robust adaptive strategy . This practice illuminates why intelligent step-size control is not just a minor tweak but a fundamental requirement for efficient optimization.",
            "id": "3156569",
            "problem": "Consider the function $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ defined by $f(x,y)=x^2+100\\,y^2+\\sin(10\\,x)\\,\\sin(10\\,y)$, where all angles are measured in radians. This function combines a strongly anisotropic quadratic bowl with high-frequency oscillations, which induce many local minima. In an introduction to computational science, distinguishing local minima from a global minimum and designing a step-size schedule that can traverse narrow valleys without getting trapped is essential.\n\nStarting from the core definitions:\n- A point $(x^\\star,y^\\star)$ is a local minimum if there exists a neighborhood $\\mathcal{N}$ of $(x^\\star,y^\\star)$ such that $f(x^\\star,y^\\star)\\le f(x,y)$ for all $(x,y)\\in\\mathcal{N}$.\n- A global minimum satisfies $f(x^\\star,y^\\star)\\le f(x,y)$ for all $(x,y)\\in\\mathbb{R}^2$.\n- Gradient-based methods update $(x,y)$ by moving in the direction of steepest descent, $-\\nabla f(x,y)$, with a chosen step size.\n\nYour task is to implement two optimization schemes on $f$ over the bounded domain $[-1,1]\\times[-1,1]$, projecting back to this domain after each update to avoid unbounded growth outside the plausible search region:\n1. Constant-step gradient descent: $(x_{k+1},y_{k+1})=(x_k,y_k)-\\alpha\\,\\nabla f(x_k,y_k)$ with fixed $\\alpha$, followed by projection of $(x_{k+1},y_{k+1})$ onto $[-1,1]\\times[-1,1]$.\n2. Adaptive-step gradient descent using backtracking line search with the sufficient decrease (Armijo) condition: starting from an initial step size $\\alpha_0$, repeatedly shrink by a factor $\\rho$ until $f((x,y)-\\alpha\\,\\nabla f(x,y))\\le f(x,y)-c\\,\\alpha\\,\\|\\nabla f(x,y)\\|_2^2$, then take the step and project onto $[-1,1]\\times[-1,1]$.\n\nYou must compute the gradient $\\nabla f(x,y)$ analytically and implement the algorithms strictly from these definitions. Angles in the $\\sin(\\cdot)$ and $\\cos(\\cdot)$ functions must be treated in radians.\n\nImplement both methods and compare their behavior on a set of initial conditions and step-size parameters chosen to reveal the narrow-valley effect and the distinction between local and global minima. Use the following test suite, which specifies method type, step-size parameters, initial point $(x_0,y_0)$, and maximum iterations $K$:\n- Test $1$: Constant-step descent with $\\alpha=0.02$, $(x_0,y_0)=(0.6,0.15)$, $K=1000$.\n- Test $2$: Constant-step descent with $\\alpha=0.05$, $(x_0,y_0)=(0.6,0.15)$, $K=1000$.\n- Test $3$: Constant-step descent with $\\alpha=0.0005$, $(x_0,y_0)=(0.6,0.15)$, $K=1000$.\n- Test $4$: Adaptive-step descent with $\\alpha_0=0.1$, $\\rho=0.5$, $c=10^{-4}$, $(x_0,y_0)=(0.6,0.15)$, $K=1000$.\n- Test $5$: Adaptive-step descent with $\\alpha_0=0.1$, $\\rho=0.5$, $c=10^{-4}$, $(x_0,y_0)=(0.0,0.9)$, $K=1000$.\n\nFor each test, run the algorithm until either the maximum iteration count $K$ is reached or the gradient norm $\\|\\nabla f(x,y)\\|_2$ falls below $10^{-8}$. After the run, report the final objective value $f(x_K,y_K)$ rounded to six decimal places. No physical units are involved; angles are in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the six-decimal rounded final objective value for Test $i$ in the order listed above.",
            "solution": "The user has provided a valid, well-posed problem in computational science. The task is to implement and compare two gradient descent-based optimization algorithms on a specified function, subject to domain constraints.\n\n### Problem Formulation\n\nThe objective is to find local minima of the function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ defined as:\n$$f(x,y) = x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)$$\nThe optimization is performed over the bounded domain $\\mathcal{D} = [-1, 1] \\times [-1, 1]$. The function $f(x,y)$ consists of two parts: a quadratic bowl $x^2 + 100y^2$ and an oscillatory term $\\sin(10x)\\sin(10y)$. The quadratic term creates a long, narrow valley aligned with the $x$-axis, as evidenced by the Hessian matrix of this term, $\\begin{pmatrix} 2  0 \\\\ 0  200 \\end{pmatrix}$, which has a large condition number of $100$. The oscillatory term superimposes many local minima onto this underlying structure, making it challenging for simple optimization algorithms to find a global minimum.\n\n### Gradient Calculation\n\nGradient-based methods require the analytical gradient of the objective function, $\\nabla f$. The partial derivatives with respect to $x$ and $y$ are calculated as follows:\n$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} (x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)) = 2x + 10\\cos(10\\,x)\\sin(10\\,y) $$\n$$ \\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} (x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)) = 200y + 10\\sin(10\\,x)\\cos(10\\,y) $$\nThus, the gradient vector is:\n$$ \\nabla f(x,y) = \\begin{pmatrix} 2x + 10\\cos(10\\,x)\\sin(10\\,y) \\\\ 200y + 10\\sin(10\\,x)\\cos(10\\,y) \\end{pmatrix} $$\nAll trigonometric functions operate on angles in radians.\n\n### Optimization Algorithms\n\nThe core algorithm is Projected Gradient Descent. At each iteration $k$, a candidate point is found by moving from the current point $(x_k, y_k)$ in the direction of the negative gradient (steepest descent). This new point is then projected back onto the feasible domain $\\mathcal{D}$. The iterative update is:\n$$ (\\tilde{x}_{k+1}, \\tilde{y}_{k+1}) = (x_k, y_k) - \\alpha_k \\nabla f(x_k, y_k) $$\n$$ (x_{k+1}, y_{k+1}) = \\text{Proj}_{\\mathcal{D}}(\\tilde{x}_{k+1}, \\tilde{y}_{k+1}) $$\nwhere $\\alpha_k$ is the step size at iteration $k$, and $\\text{Proj}_{\\mathcal{D}}$ is the projection operator onto the box $[-1, 1] \\times [-1, 1]$, defined as $(\\text{clip}(x, -1, 1), \\text{clip}(y, -1, 1))$.\n\nTwo strategies for choosing the step size $\\alpha_k$ are implemented:\n\n1.  **Constant-Step Gradient Descent**: A fixed step size $\\alpha_k = \\alpha$ is used for all iterations. The choice of $\\alpha$ is critical: if it is too large, the algorithm may become unstable and diverge; if it is too small, convergence will be impractically slow. This method is sensitive to the local curvature of the function, which varies significantly in this problem.\n\n2.  **Adaptive-Step Gradient Descent with Backtracking Line Search**: The step size $\\alpha_k$ is determined dynamically at each iteration. Starting with an initial guess $\\alpha_0$, the step size is repeatedly reduced by a factor $\\rho \\in (0,1)$ until it satisfies the Armijo (or sufficient decrease) condition:\n    $$ f((x_k,y_k) - \\alpha \\nabla f(x_k,y_k)) \\le f(x_k,y_k) - c \\alpha \\|\\nabla f(x_k,y_k)\\|_2^2 $$\n    for a small constant $c$ (e.g., $10^{-4}$). This ensures that each step makes sufficient progress in decreasing the objective function value, making the algorithm more robust and generally more efficient than the constant-step method. The Armijo condition is checked on the unconstrained step, before the projection is applied, as specified.\n\n### Implementation and Execution\n\nThe algorithms are implemented in Python using the `numpy` library. For each of the five specified test cases, the corresponding algorithm is initialized with the given starting point $(x_0, y_0)$ and parameters. The iterative process continues until either the maximum number of iterations, $K$, is reached, or the L2-norm of the gradient, $\\|\\nabla f(x,y)\\|_2$, falls below a tolerance of $10^{-8}$. The final objective function value $f(x,y)$ at the terminal point is then calculated and reported. This process is repeated for each test case to compare the effectiveness of the different parameterizations and methods.\n\nThe expected behavior is that the constant-step methods with poorly chosen step sizes (Tests $1$ and $2$) will perform poorly, either oscillating or getting stuck at the domain boundary with a high objective value. The very small step size (Test $3$) will make slow progress. The adaptive-step method (Tests $4$ and $5$) is expected to be robust, finding suitable step sizes automatically and converging to a low-value local minimum, demonstrating its superiority for complex optimization landscapes. The different starting points in Tests $4$ and $5$ will likely lead to convergence to different local minima, highlighting the path-dependent nature of local optimization.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares constant-step and adaptive-step projected gradient\n    descent algorithms for a given function and test suite.\n    \"\"\"\n\n    # Define the objective function f(p), its gradient grad_f(p), and the\n    # projection operator. p is a numpy array [x, y].\n    def f(p):\n        x, y = p\n        return x**2 + 100*y**2 + np.sin(10*x) * np.sin(10*y)\n\n    def grad_f(p):\n        x, y = p\n        df_dx = 2*x + 10*np.cos(10*x)*np.sin(10*y)\n        df_dy = 200*y + 10*np.sin(10*x)*np.cos(10*y)\n        return np.array([df_dx, df_dy])\n\n    def project(p):\n        return np.clip(p, -1, 1)\n\n    # Algorithm 1: Constant-step projected gradient descent\n    def constant_step_descent(alpha, p0, K, tol):\n        p = np.array(p0, dtype=float)\n        for _ in range(K):\n            grad = grad_f(p)\n            if np.linalg.norm(grad)  tol:\n                break\n            p_next_unprojected = p - alpha * grad\n            p = project(p_next_unprojected)\n        return f(p)\n\n    # Algorithm 2: Adaptive-step projected gradient descent with backtracking\n    def adaptive_step_descent(alpha0, rho, c, p0, K, tol):\n        p = np.array(p0, dtype=float)\n        for _ in range(K):\n            grad = grad_f(p)\n            grad_norm = np.linalg.norm(grad)\n            if grad_norm  tol:\n                break\n            \n            # Backtracking line search to satisfy the Armijo condition\n            alpha = alpha0\n            current_f_val = f(p)\n            grad_norm_sq = grad_norm**2\n            \n            while True:\n                p_candidate = p - alpha * grad\n                \n                # Check Armijo condition on the unprojected candidate point\n                if f(p_candidate) = current_f_val - c * alpha * grad_norm_sq:\n                    break\n                \n                alpha *= rho\n                \n                # Failsafe for pathologically small step sizes\n                if alpha  1e-16:\n                    break\n\n            p_next_unprojected = p - alpha * grad\n            p = project(p_next_unprojected)\n        return f(p)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: Constant-step, alpha=0.02\n        {'method': 'constant', 'params': {'alpha': 0.02, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 2: Constant-step, alpha=0.05\n        {'method': 'constant', 'params': {'alpha': 0.05, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 3: Constant-step, alpha=0.0005\n        {'method': 'constant', 'params': {'alpha': 0.0005, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 4: Adaptive-step\n        {'method': 'adaptive', 'params': {'alpha0': 0.1, 'rho': 0.5, 'c': 1e-4, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 5: Adaptive-step, different initial point\n        {'method': 'adaptive', 'params': {'alpha0': 0.1, 'rho': 0.5, 'c': 1e-4, 'p0': (0.0, 0.9), 'K': 1000}},\n    ]\n\n    results = []\n    TOLERANCE = 1e-8\n    \n    for case in test_cases:\n        params = case['params']\n        if case['method'] == 'constant':\n            res = constant_step_descent(\n                alpha=params['alpha'],\n                p0=params['p0'],\n                K=params['K'],\n                tol=TOLERANCE\n            )\n        else: # 'adaptive'\n            res = adaptive_step_descent(\n                alpha0=params['alpha0'],\n                rho=params['rho'],\n                c=params['c'],\n                p0=params['p0'],\n                K=params['K'],\n                tol=TOLERANCE\n            )\n        results.append(res)\n    \n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While gradient-based methods are excellent at finding the bottom of a given valley, they are inherently local and will get trapped in the first minimum they find. To locate the *global* minimum on a landscape riddled with many local minima, we need a strategy to escape these traps. This exercise introduces the basin-hopping metaheuristic, a powerful technique that combines the strengths of local optimization with stochastic jumps to explore the entire search space . By implementing this algorithm, you will gain practical insight into the trade-off between local exploitation and global exploration that lies at the heart of global optimization.",
            "id": "3156501",
            "problem": "You are to write a complete and runnable program that implements a basin hopping metaheuristic to study local versus global minima on a two-dimensional landscape. The objective function is $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$, where all angular arguments for the sine function must be treated in radians. The computational task is to examine how the stochastic jump radius influences the ability to discover the global minimum, through repeated local minimizations initiated from random points and interleaved with randomized jumps.\n\nThe foundational base for this problem is the following well-tested definitions and facts:\n- A point $(x^\\star,y^\\star)$ is a local minimum of a differentiable function $f$ if there exists a neighborhood $U$ of $(x^\\star,y^\\star)$ such that $f(x^\\star,y^\\star) \\le f(x,y)$ for all $(x,y) \\in U$. A point $(x^\\star,y^\\star)$ is a global minimum if $f(x^\\star,y^\\star) \\le f(x,y)$ for all $(x,y)$ in the domain.\n- For differentiable functions, local minimizers must satisfy the first-order stationarity condition $\\nabla f(x^\\star,y^\\star) = \\mathbf{0}$, and the second derivative (Hessian) provides local curvature information that influences local optimality.\n- Basin hopping is a metaheuristic that alternates deterministic local minimization with stochastic jumps in the search space to escape basins of attraction and improve the chance of finding global minima.\n\nYour program must implement the following in purely mathematical and logical terms:\n1. Define the function $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$ over the bounded domain $[-6,6] \\times [-6,6]$.\n2. Implement local minimization for $f$ using a gradient-based routine that respects the given bounds. The derivative of the sine function must be computed with angles in radians.\n3. Implement basin hopping with greedy acceptance: start from a random initial point $(x_0,y_0)$, perform a local minimization to obtain a local minimum $(x_\\mathrm{loc},y_\\mathrm{loc})$, propose a random jump of radius $r$ uniformly distributed over the disk of radius $r$ centered at $(x_\\mathrm{loc},y_\\mathrm{loc})$, clip the jumped point to the domain $[-6,6] \\times [-6,6]$, then locally minimize from the clipped point. Accept the new local minimum only if it strictly improves the objective value relative to the current local minimum; otherwise, retain the current one. Repeat this process for a fixed number of steps.\n4. Use a multi-start local minimization in one dimension to compute an accurate approximation of the global minimum of $g(t) = \\sin(t) + 0.2 t^2$ on $[-6,6]$, and infer the global minimum of $f$ using separability. Specifically, compute $t^\\star \\in [-6,6]$ that minimizes $g$, then the global minimum of $f$ occurs at $(t^\\star,t^\\star)$ with value $f^\\star = 2 g(t^\\star)$.\n5. Use a Pseudo-Random Number Generator (PRNG) with deterministic seeding to ensure reproducibility. Each test case must use a seed derived deterministically from its parameters. All random angles and uniform variables used in your program must be in radians and dimensionless, respectively.\n6. For each test case, count how many independent basin hopping runs end at the global minimum (within a prescribed tolerance), and report this count as an integer.\n\nAngle unit specification:\n- All trigonometric evaluations and angular samples must be in radians.\n\nThe test suite consists of five test cases that vary the jump radius $r$ to test different facets of the algorithm:\n- Case $1$ (boundary case): $r = 0.0$ (equivalent to pure local minimization without jumps).\n- Case $2$ (small jumps): $r = 0.5$.\n- Case $3$ (moderate jumps): $r = 1.5$.\n- Case $4$ (large jumps): $r = 3.0$.\n- Case $5$ (very large jumps): $r = 5.0$.\n\nCommon parameters for all test cases:\n- Domain bounds: $[-6,6] \\times [-6,6]$.\n- Number of basin hopping steps per run: $N = 60$.\n- Number of independent runs per test case: $R = 30$.\n- Global optimality tolerance: declare success if the final objective value is within $\\varepsilon = 10^{-6}$ of the computed global minimum $f^\\star$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case is the integer count of successful runs out of $R$, in the order of the test cases above. For example, the output format is $[s_1,s_2,s_3,s_4,s_5]$, where each $s_i$ is an integer.\n\nScientific realism and constraints:\n- The basin hopping jump must be sampled uniformly in area over a disk of radius $r$. To achieve uniformity, sample the radius as $r\\sqrt{u}$ where $u$ is uniform in $[0,1]$, and the angle uniformly in $[0,2\\pi]$ in radians.\n- Local minimization must use gradient information and enforce the domain bounds through a bounded method.\n- All computations must be deterministic given the fixed seeds per test case.\n\nYour program must be self-contained, require no external input, and adhere strictly to the specified final output format.",
            "solution": "The problem requires the implementation of a basin hopping metaheuristic to explore the energy landscape of a two-dimensional function and assess how the algorithm's ability to find the global minimum is affected by the size of its stochastic jumps. The solution is constructed methodically, beginning with an analysis of the objective function to determine the true global minimum, followed by the implementation of the specified basin hopping algorithm.\n\nThe objective function is given by $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$ over the domain $(x,y) \\in [-6,6] \\times [-6,6]$. A key property of this function is its separability, meaning it can be expressed as a sum of two independent one-dimensional functions: $f(x,y) = g(x) + g(y)$, where $g(t) = \\sin(t) + 0.2t^2$. This property implies that the global minimum of $f(x,y)$ occurs at a point $(t^\\star, t^\\star)$, where $t^\\star$ is the value of $t$ that globally minimizes $g(t)$ on the interval $[-6,6]$.\n\nFirst, we must accurately determine this global minimum, which serves as the benchmark for success. The minimizers of $g(t)$ are either stationary points where the derivative $g'(t) = \\cos(t) + 0.4t$ is zero, or the boundary points $t = -6$ and $t = 6$. A numerical investigation reveals that $g(t)$ has two local minima within the interval $[-6,6]$. As stipulated, a multi-start local minimization approach is used to find the global minimum of $g(t)$. We perform bounded one-dimensional minimization on sub-intervals that contain these local minima (e.g., $[-6,0]$ and $[0,6]$) and compare the resulting values with the function values at the domain boundaries $g(-6)$ and $g(6)$. This procedure robustly identifies the global minimum of $g(t)$ at $t^\\star \\approx -1.30644$, with a value of $g(t^\\star) \\approx -0.62638$. Consequently, the global minimum of the two-dimensional function $f(x,y)$ is $f^\\star = f(t^\\star, t^\\star) = 2g(t^\\star) \\approx -1.252767$. This value will be the target for our optimization runs.\n\nThe core of the problem is the basin hopping algorithm. This is a stochastic global optimization technique designed to overcome the primary weakness of local search methods: getting trapped in local minima. The algorithm operates by iteratively applying two phases: a deterministic local minimization and a stochastic \"hop\" or perturbation to a new starting point.\n\nThe implementation proceeds as follows for each test case, which is defined by a specific jump radius $r$:\n$1$. A set of $R=30$ independent runs is performed to ensure statistically meaningful results. Each run is initialized with a deterministic seed derived from the test case and run index to guarantee reproducibility.\n$2$. Each run begins by selecting a random starting point $(x_0, y_0)$ uniformly from the domain $[-6,6] \\times [-6,6]$. From this point, a local minimization is performed to find the first local minimum, which becomes the initial \"best\" solution found, $(x_{\\text{best}}, y_{\\text{best}})$.\n$3$. The local minimization is carried out using a gradient-based quasi-Newton method, specifically the Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithm with box constraints (`L-BFGS-B`). This method is efficient and respects the domain boundaries, as required. It uses the analytical gradient (Jacobian) of $f(x,y)$, which is $\\nabla f(x,y) = (\\cos(x)+0.4x, \\cos(y)+0.4y)$, for faster convergence.\n$4$. The main basin hopping loop then executes for $N=60$ steps. In each step:\n    a. A new candidate point is generated by taking a stochastic jump from the current best minimum, $x_{\\text{best}}$. The jump is sampled uniformly from a disk of radius $r$. To achieve this, a jump magnitude $\\rho$ is sampled as $\\rho = r\\sqrt{u}$ where $u \\sim U(0,1)$, and a jump angle $\\theta$ is sampled from $\\theta \\sim U(0, 2\\pi)$. The new point is $x_{\\text{prop}} = x_{\\text{best}} + (\\rho\\cos\\theta, \\rho\\sin\\theta)$.\n    b. The proposed point $x_{\\text{prop}}$ is clipped to ensure it remains within the domain $[-6,6] \\times [-6,6]$.\n    c. A new local minimization is initiated from this clipped point, yielding a new local minimum, $x_{\\text{new}}$, with function value $f_{\\text{new}}$.\n    d. A greedy acceptance criterion is applied: if $f_{\\text{new}}  f_{\\text{best}}$, the new minimum is accepted as the current best solution. Otherwise, the algorithm remains at the previous best minimum. This ensures that the search trajectory on the landscape of local minima is monotonically decreasing in energy.\n$5$. After $N=60$ steps, the final function value $f_{\\text{best}}$ is compared against the pre-computed global minimum $f^\\star$. If $|f_{\\text{best}} - f^\\star|  \\varepsilon$ where the tolerance $\\varepsilon=10^{-6}$, the run is counted as a success.\n$6$. The total count of successful runs out of $R=30$ is recorded for the given jump radius $r$.\n\nThis entire process is repeated for each of the five specified radii ($r=0.0, 0.5, 1.5, 3.0, 5.0$), and the resulting success counts are aggregated into a final list. The case $r=0.0$ serves as a control, corresponding to a multi-start local search without any jumps. The varying radii are designed to probe the trade-off between exploration (large jumps) and exploitation (small jumps) in finding the global minimum.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize, minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements a basin hopping metaheuristic to find the global minimum of a 2D function.\n    The program evaluates the success rate for different jump radii.\n    \"\"\"\n    # Common parameters for all test cases\n    N_STEPS = 60\n    N_RUNS = 30\n    DOMAIN_BOUNDS_1D = (-6, 6)\n    DOMAIN_BOUNDS_2D = [(-6, 6), (-6, 6)]\n    GLOBAL_TOLERANCE = 1e-6\n    TEST_CASES_R = [0.0, 0.5, 1.5, 3.0, 5.0]\n\n    # --- Step 1: Define objective function and its gradient ---\n    def f(p):\n        \"\"\"The 2D objective function f(x, y).\"\"\"\n        x, y = p\n        return np.sin(x) + np.sin(y) + 0.2 * (x**2 + y**2)\n\n    def jac_f(p):\n        \"\"\"The Jacobian (gradient) of the objective function.\"\"\"\n        x, y = p\n        return np.array([np.cos(x) + 0.4 * x, np.cos(y) + 0.4 * y])\n\n    # --- Step 2: Compute the true global minimum value ---\n    def g(t):\n        \"\"\"The 1D component of the separable function f(x, y).\"\"\"\n        return np.sin(t) + 0.2 * t**2\n\n    # Use multi-start local minimization for the 1D function g(t)\n    # Search for minima in sub-intervals based on analysis of g'(t).\n    res1 = minimize_scalar(g, bounds=(-3, 0), method='bounded')\n    res2 = minimize_scalar(g, bounds=(2, 5), method='bounded')\n    \n    # Consider the domain endpoints as potential minima\n    g_at_endpoints = [g(DOMAIN_BOUNDS_1D[0]), g(DOMAIN_BOUNDS_1D[1])]\n\n    # The global minimum of g(t) is the minimum of all found local minima and endpoint values.\n    global_min_g = min(res1.fun, res2.fun, *g_at_endpoints)\n    \n    # The global minimum of f(x,y) is twice the global minimum of g(t).\n    f_star = 2 * global_min_g\n\n    # --- Step 3: Run basin hopping for each test case ---\n    results = []\n    for i, r in enumerate(TEST_CASES_R):\n        success_count = 0\n        for j in range(N_RUNS):\n            # Use a deterministic seed for reproducibility, unique for each run.\n            seed = i * N_RUNS + j\n            rng = np.random.default_rng(seed)\n\n            # Start from a random initial point within the domain.\n            x0 = rng.uniform(DOMAIN_BOUNDS_1D[0], DOMAIN_BOUNDS_1D[1], size=2)\n\n            # Perform an initial local minimization.\n            res = minimize(f, x0, jac=jac_f, method='L-BFGS-B', bounds=DOMAIN_BOUNDS_2D)\n            x_best = res.x\n            f_best = res.fun\n\n            # Perform N basin hopping steps.\n            for _ in range(N_STEPS):\n                # Propose a random jump, sampled uniformly from a disk of radius r.\n                jump_magnitude = r * np.sqrt(rng.uniform(0, 1))\n                jump_angle = rng.uniform(0, 2 * np.pi)\n                \n                jump_vector = np.array([jump_magnitude * np.cos(jump_angle), \n                                        jump_magnitude * np.sin(jump_angle)])\n                \n                x_proposed = x_best + jump_vector\n                \n                # Clip the new point to stay within the domain.\n                x_proposed_clipped = np.clip(x_proposed, DOMAIN_BOUNDS_1D[0], DOMAIN_BOUNDS_1D[1])\n\n                # Perform a local minimization from the new point.\n                res_new = minimize(f, x_proposed_clipped, jac=jac_f, method='L-BFGS-B', bounds=DOMAIN_BOUNDS_2D)\n                \n                # Greedy acceptance criterion: accept only if strictly better.\n                if res_new.fun  f_best:\n                    x_best = res_new.x\n                    f_best = res_new.fun\n\n            # After all steps, check if the found minimum is the global minimum.\n            if abs(f_best - f_star)  GLOBAL_TOLERANCE:\n                success_count += 1\n                \n        results.append(success_count)\n\n    # --- Step 4: Final Output ---\n    # Print the success counts for each test case in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An alternative to stochastic search for global optimization is the elegant continuation method, which approaches a complex problem by first solving a simplified version. By starting with a model so simple it has only one minimum, and then gradually increasing its complexity, we can track the minimum as the landscape deforms and new local minima emerge. In this practice, you will use a truncated Fourier series to construct a family of functions of increasing complexity and implement a continuation strategy to follow the global minimum . This exercise provides a hands-on demonstration of a sophisticated technique used to solve difficult problems in scientific computing by building on simpler, known solutions.",
            "id": "3156481",
            "problem": "Write a complete and runnable program that demonstrates how the emergence of local minima depends on the number of Fourier modes in a truncated series, and how a continuation strategy in the model complexity can help track minima as modes are added. The program must construct a one-parameter family of functions defined over a periodic domain and analyze them using principled definitions.\n\nUse the following setup grounded in core definitions:\n- Let $x$ denote an angle in radians on the domain $[0,2\\pi)$.\n- For a positive integer $M$, define the truncated Fourier series\n$$\nf_M(x) \\equiv \\sum_{k=1}^{M} \\frac{\\cos(k x)}{k}.\n$$\n- A point $x^\\star$ is a local minimum of $f_M$ if there exists $\\delta0$ such that $f_M(x^\\star) \\le f_M(x)$ for all $x$ with $|x-x^\\star|  \\delta$. A global minimum is a point $x^\\dagger$ such that $f_M(x^\\dagger) \\le f_M(x)$ for all $x$ in the entire domain.\n\nCore tasks to implement based on fundamental principles:\n1) Discretize the domain $[0,2\\pi)$ using a uniform grid of $N$ points (use $N=8192$), and numerically detect the number of distinct local minima of $f_M$ by discrete comparison with immediate neighbors under periodic boundary conditions. Specifically, use the discrete criterion for grid points $x_i$: a discrete local minimum is detected at index $i$ if $f_M(x_i)  f_M(x_{i-1})$ and $f_M(x_i)  f_M(x_{i+1})$, with the indices taken modulo $N$ to enforce periodicity. This provides a robust approximation of the number of continuous local minima when $N$ is sufficiently large. All angles must be treated in radians.\n2) Implement gradient descent with backtracking line search to find a local minimizer of $f_M$ starting from a given initial condition. Use the analytical gradient\n$$\n\\frac{d}{dx} f_M(x) \\;=\\; -\\sum_{k=1}^{M} \\sin(kx)\n$$\nto perform a principled steepest-descent step on $x$ (the step direction is the negative gradient). Ensure steps that decrease the objective (Armijo-type sufficient decrease) and wrap $x$ modulo $2\\pi$ after each update to remain in the domain. Stop when the gradient magnitude is sufficiently small or a maximum iteration budget is reached.\n3) Perform a continuation in the complexity parameter $M$: start at the coarsest model $M_1=1$ and find its global minimizer using an exhaustive grid search on the discretization. Then, for each subsequent $M_j$ in the test suite (strictly increasing), use the minimizer obtained for $M_{j-1}$ as the initial condition for gradient descent on $f_{M_j}$. This continuation strategy is meant to cope with the emergence of new local minima as $M$ increases.\n4) For each $M_j$ in the test suite, determine:\n   - The number of detected local minima of $f_{M_j}$ on the grid.\n   - A success indicator defined as follows: compute the global minimum value on the grid for $M_j$ and compare it to the value attained at the point returned by gradient descent initialized from the previous stage. If the obtained value is within a small nonnegative tolerance $\\varepsilon$ of the grid-global minimum value (use $\\varepsilon = 10^{-9}$), record success as $1$, otherwise record $0$. For $M_1=1$, define the success indicator as $1$ by convention because the procedure initializes at the global minimizer for the coarsest model.\n\nAngle unit requirement:\n- All angles must be in radians.\n\nTest suite to cover different facets:\n- Use $N=8192$ grid points for all computations.\n- Use the sequence of mode counts $M \\in \\{\\,1,2,5,10,20\\,\\}$. This covers:\n  - A base case with a single minimum ($M=1$).\n  - Intermediate cases where additional local minima emerge ($M=2,5$).\n  - More complex cases with many local minima ($M=10,20$).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test value of $M$ in the specified order, output a two-element list $[n_M,s_M]$ where $n_M$ is the detected number of local minima (an integer) and $s_M$ is the success indicator (either $0$ or $1$).\n- The final printed line must therefore look like\n$[[n_{1},s_{1}],[n_{2},s_{2}],[n_{5},s_{5}],[n_{10},s_{10}],[n_{20},s_{20}]]$\nwith no additional text. All values are dimensionless, and angles are in radians.",
            "solution": "The problem requires an analysis of the one-parameter family of functions $f_M(x)$ defined by a truncated Fourier series. The objective is to programmatically investigate how the number of local minima changes as the complexity parameter $M$ (the number of modes) increases, and to evaluate a continuation strategy for tracking the global minimum through this changing landscape.\n\nThe function under consideration is defined on the periodic domain $x \\in [0, 2\\pi)$ as:\n$$\nf_M(x) \\equiv \\sum_{k=1}^{M} \\frac{\\cos(k x)}{k}\n$$\nwhere $M$ is a positive integer. We will perform our analysis on a discretized version of this domain, represented by a uniform grid of $N=8192$ points, denoted $\\{x_i\\}_{i=0}^{N-1}$, where $x_i = \\frac{2\\pi i}{N}$.\n\nThe core tasks are implemented based on the following principles:\n\n1. **Numerical Detection of Local Minima:** A point $x^\\star$ is a local minimum if $f_M(x^\\star) \\le f_M(x)$ in a neighborhood of $x^\\star$. On our discrete grid, we approximate this by identifying a point $x_i$ as a discrete local minimum if its function value is strictly less than that of its immediate neighbors. To respect the periodic nature of the domain, the neighbor indices are taken modulo $N$. That is, a point $x_i$ is a detected local minimum if it satisfies the conditions:\n$$\nf_M(x_i)  f_M(x_{i-1 \\pmod N}) \\quad \\text{and} \\quad f_M(x_i)  f_M(x_{i+1 \\pmod N})\n$$\nThe total count of such points, $n_M$, provides a numerical estimate of the number of local minima for a given $M$. For a sufficiently large $N$, this discrete count is a reliable approximation of the number of minima of the continuous function.\n\n2. **Gradient Descent Optimization:** To find a minimizer from a given starting point, we employ the gradient descent algorithm. This iterative method seeks to minimize $f_M(x)$ by taking steps in the direction of the negative gradient, which is the direction of steepest descent. The analytical gradient of $f_M(x)$ is:\n$$\n\\frac{d}{dx} f_M(x) = -\\sum_{k=1}^{M} \\sin(kx)\n$$\nAn iteration of gradient descent updates the current point $x_{j}$ to $x_{j+1}$ according to the rule:\n$$\nx_{j+1} = \\left( x_j - t_j \\left( \\sum_{k=1}^{M} \\sin(kx_j) \\right) \\right) \\pmod{2\\pi}\n$$\nwhere $t_j > 0$ is the step size. The new position is wrapped modulo $2\\pi$ to remain within the domain.\n\nThe step size $t_j$ is determined using a backtracking line search to ensure sufficient progress. This involves starting with a trial step size and iteratively reducing it until the Armijo condition is met:\n$$\nf_M(x_{j+1}) \\le f_M(x_j) + \\alpha t_j \\nabla f_M(x_j)^T p_j\n$$\nwhere $p_j = -\\nabla f_M(x_j)$ is the descent direction and $\\alpha \\in (0, 1)$ is a control parameter (e.g., $\\alpha=0.3$). This condition guarantees that the step provides a sufficient decrease in the function value relative to the step size and the directional derivative. The algorithm terminates when the magnitude of the gradient falls below a small tolerance or a maximum number of iterations is reached.\n\n3. **Continuation in Model Complexity:** As $M$ increases, the function $f_M(x)$ develops more local minima, making it challenging for local search methods like gradient descent to find the global minimum from an arbitrary starting point. A continuation strategy attempts to mitigate this by leveraging the solution from a simpler model. The procedure is as follows:\n- For the base case, $M_1=1$, the function $f_1(x) = \\cos(x)$ is simple, having a single global minimum at $x=\\pi$. This minimizer is found via an exhaustive search over the discrete grid.\n- For each subsequent, more complex model $M_j$ (with $j>1$), the minimizer found for the preceding model $M_{j-1}$ is used as the initial guess for the gradient descent algorithm applied to $f_{M_j}$. The principle is that for a small increment in model complexity, the location of the global minimum is expected to shift only slightly, so the previous solution provides a good starting point.\n\n4. **Evaluation of the Continuation Strategy:** To assess the effectiveness of this continuation, we define a success indicator, $s_M$. For each $M$, we compare the function value at the point found by our continuation-based gradient descent, $f_M(x_{\\text{GD}}^\\star)$, with the true global minimum value on the grid, $f_M^\\dagger = \\min_{i} f_M(x_i)$. The strategy is deemed successful if the found minimum is close enough to the global minimum:\n$$\ns_M = \\begin{cases} 1  \\text{if } f_M(x_{\\text{GD}}^\\star) \\le f_M^\\dagger + \\varepsilon \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nwhere $\\varepsilon$ is a small non-negative tolerance, set to $10^{-9}$. For the base case $M=1$, success is defined by convention, so $s_1=1$. This metric reveals whether the continuation method successfully tracked the global minimum or became trapped in a local minimum.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes a family of functions f_M(x) to demonstrate the emergence of\n    local minima and the use of a continuation strategy.\n    \"\"\"\n    # Define problem parameters from the statement.\n    N = 8192\n    M_VALUES = [1, 2, 5, 10, 20]\n    EPSILON = 1e-9\n\n    # Pre-compute the grid for x.\n    # The domain is [0, 2*pi), so endpoint is False.\n    x_grid = np.linspace(0, 2 * np.pi, N, endpoint=False)\n    \n    # Store the results for each M.\n    results = []\n    \n    # This variable will store the minimizer from the previous stage (M_{j-1})\n    # to be used as the starting point for the current stage (M_j).\n    x_continuation_min = 0.0\n\n    def f_M(x, M):\n        \"\"\"\n        Computes the function f_M(x) = sum_{k=1 to M} cos(k*x)/k.\n        This function is vectorized to handle an array of x values.\n        \"\"\"\n        k = np.arange(1, M + 1)\n        # Use broadcasting for vectorized computation.\n        # x must be a column vector for this to work with a row vector k.\n        if x.ndim == 1:\n            x_col = x[:, np.newaxis]\n        else: # Handle scalar input for gradient descent\n            x_col = np.array([[x]])\n        \n        cos_terms = np.cos(k * x_col) / k\n        # Sum over the k-axis (axis=1)\n        return np.sum(cos_terms, axis=1).squeeze()\n\n    def grad_f_M(x_scalar, M):\n        \"\"\"\n        Computes the gradient of f_M(x) at a single point x.\n        The gradient is -sum_{k=1 to M} sin(k*x).\n        \"\"\"\n        k = np.arange(1, M + 1)\n        return np.sum(-np.sin(k * x_scalar))\n\n    def gradient_descent(x_start, M):\n        \"\"\"\n        Performs gradient descent with backtracking line search to find a local\n        minimum of f_M(x).\n        \"\"\"\n        # Hyperparameters for the optimization algorithm.\n        max_iter = 1000\n        grad_tol = 1e-8\n        alpha = 0.3  # Armijo condition control parameter\n        beta = 0.8   # Backtracking step size reduction factor\n\n        x_current = x_start\n\n        for _ in range(max_iter):\n            # Calculate gradient at the current point.\n            g = grad_f_M(x_current, M)\n\n            # Stop if the gradient is sufficiently small.\n            if np.abs(g)  grad_tol:\n                break\n\n            # Set descent direction (negative gradient).\n            p_k = -g\n            \n            # --- Backtracking Line Search ---\n            t = 1.0  # Initial step size\n            f_current = f_M(np.array([x_current]), M)\n            \n            while True:\n                x_next = x_current + t * p_k\n                # Function f_M is periodic, so modulo is not strictly necessary\n                # for evaluation but good practice.\n                f_next = f_M(np.array([x_next]), M)\n                # Armijo condition: f(x+tp) = f(x) + alpha*t*grad(f)^T*p\n                # Here p = -grad(f), so grad(f)^T*p = -|grad(f)|^2\n                if f_next = f_current - alpha * t * g * g:\n                    break\n                t = beta * t\n\n            # --- Update Step ---\n            # Update position and wrap around the 2*pi domain.\n            x_current = (x_current + t * p_k) % (2 * np.pi)\n            \n        return x_current\n\n    # Main loop to iterate through the specified values of M.\n    for M in M_VALUES:\n        # 1. Evaluate the function f_M on the entire grid.\n        y_grid = f_M(x_grid, M)\n\n        # 2. Detect and count the number of local minima on the grid.\n        # A point is a local minimum if it's smaller than its left and right neighbors.\n        # np.roll handles the periodic boundary conditions.\n        is_local_min = (y_grid  np.roll(y_grid, 1))  (y_grid  np.roll(y_grid, -1))\n        n_M = int(np.sum(is_local_min))\n\n        s_M = 0  # Default to failure (0).\n\n        if M == 1:\n            # For the base case M=1, find the global minimizer by exhaustive grid search.\n            min_idx = np.argmin(y_grid)\n            x_continuation_min = x_grid[min_idx]\n            # Success is defined as 1 for the base case.\n            s_M = 1\n        else:\n            # Continuation: use the minimizer from the previous M as the starting point.\n            x_start = x_continuation_min\n            x_gd_min = gradient_descent(x_start, M)\n            \n            # The result of this optimization becomes the starting point for the next M.\n            x_continuation_min = x_gd_min\n\n            # 4. Evaluate success by comparing the found value with the grid's global minimum.\n            grid_global_min_val = np.min(y_grid)\n            val_at_gd_min = f_M(np.array([x_gd_min]), M)\n\n            if val_at_gd_min = grid_global_min_val + EPSILON:\n                s_M = 1\n        \n        results.append([n_M, s_M])\n\n    # Print the final result in the specified format: [[n1,s1],[n2,s2],...].\n    # Using str() on a list of lists and removing spaces to match the format precisely.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}