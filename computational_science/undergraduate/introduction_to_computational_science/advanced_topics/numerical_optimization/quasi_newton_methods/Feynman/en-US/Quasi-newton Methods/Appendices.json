{
    "hands_on_practices": [
        {
            "introduction": "Every iterative optimization journey begins with a single step. This first exercise focuses on that crucial starting point, where we compute the initial search direction for a quasi-Newton method. By setting the initial Hessian approximation $B_0$ to the identity matrix, a common and simple choice, you will see how the method begins by moving in the direction of steepest descent, providing a familiar anchor for the more sophisticated steps that follow. ",
            "id": "2195903",
            "problem": "An iterative optimization algorithm is employed to find a local minimum of the two-variable function $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$. The algorithm is initialized at the point $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$. At the first step (iteration $k=0$), a search direction $p_0$ is computed by solving the linear system of equations $B_0 p_0 = -\\nabla f(x_0)$, where $\\nabla f(x_0)$ is the gradient of $f$ evaluated at $x_0$, and $B_0$ is an approximation of the Hessian matrix. For this procedure, the initial approximation is chosen as the $2 \\times 2$ identity matrix, $I$. Determine the components of the initial search direction vector $p_0$.",
            "solution": "The problem asks for the initial search direction vector $p_0$, which is the solution to the linear system $B_0 p_0 = -\\nabla f(x_0)$. We can solve this problem by following three main steps: first, compute the gradient of the function $f(x_1, x_2)$; second, evaluate this gradient at the initial point $x_0$; and third, solve the given linear system for $p_0$.\n\nStep 1: Compute the gradient of the function.\nThe function is given by $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$.\nThe gradient of $f$, denoted by $\\nabla f$, is a vector of its partial derivatives:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\nThe partial derivative with respect to $x_1$ is:\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)\n$$\nThe partial derivative with respect to $x_2$ is:\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)\n$$\nTherefore, the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\cos(x_1) \\\\ \\sinh(x_2) \\end{pmatrix}\n$$\n\nStep 2: Evaluate the gradient at the initial point $x_0$.\nThe initial point is given as $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$. We substitute these values into the gradient expression:\n$$\n\\nabla f(x_0) = \\nabla f(0, \\ln(2)) = \\begin{pmatrix} \\cos(0) \\\\ \\sinh(\\ln(2)) \\end{pmatrix}\n$$\nWe evaluate each component. The cosine of 0 is:\n$$\n\\cos(0) = 1\n$$\nThe hyperbolic sine function is defined as $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$. For $y = \\ln(2)$:\n$$\n\\sinh(\\ln(2)) = \\frac{\\exp(\\ln(2)) - \\exp(-\\ln(2))}{2} = \\frac{2 - \\exp(\\ln(2^{-1}))}{2} = \\frac{2 - \\frac{1}{2}}{2} = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\n$$\nSo, the gradient vector at the initial point is:\n$$\n\\nabla f(x_0) = \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n\nStep 3: Solve the linear system for $p_0$.\nThe system to solve is $B_0 p_0 = -\\nabla f(x_0)$. We are given that $B_0$ is the $2 \\times 2$ identity matrix, $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe system becomes:\n$$\nI p_0 = -\\nabla f(x_0)\n$$\nMultiplying any vector by the identity matrix leaves the vector unchanged, so $p_0 = -\\nabla f(x_0)$.\nSubstituting the value of the gradient we found:\n$$\np_0 = - \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}\n$$\nThus, the components of the initial search direction vector $p_0$ are $-1$ and $-\\frac{3}{4}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}}$$"
        },
        {
            "introduction": "After taking the first step, how do we intelligently choose the next? The power of quasi-Newton methods lies in their ability to build a better approximation of the problem's curvature at each iteration. This practice reveals the theoretical core of this process: the secant condition. By reducing the problem to a single dimension, you will derive the connection between the general quasi-Newton update and the classic secant method for finding a root of the derivative, $f'(x)=0$, building a strong conceptual bridge from familiar ideas to advanced optimization. ",
            "id": "2195876",
            "problem": "Consider the one-dimensional unconstrained optimization problem of minimizing a twice-continuously differentiable function $f(x)$, where $x$ is a scalar variable. A quasi-Newton method is employed to find the minimum. The iterative update rule for the general multi-dimensional case is given by:\n$$x_{k+1} = x_k - (B_k)^{-1} \\nabla f(x_k)$$\nwhere $x_k$ is the current estimate of the minimum, $\\nabla f(x_k)$ is the gradient of the function at $x_k$, and $B_k$ is an approximation of the Hessian matrix at $x_k$.\n\nIn our one-dimensional case, the gradient $\\nabla f(x)$ becomes the first derivative $f'(x)$, and the Hessian matrix becomes the second derivative $f''(x)$. The approximation $B_k$ is therefore a scalar, which we will denote as $b_k$. This scalar $b_k$ is updated at each step.\n\nSuppose we are at the $k$-th iteration, having the previous iterate $x_{k-1}$ and the current iterate $x_k$. The value of $b_k$ is determined using the information from these two points via the one-dimensional secant condition:\n$$b_k (x_k - x_{k-1}) = f'(x_k) - f'(x_{k-1})$$\n\nDerive the expression for the next iterate, $x_{k+1}$. Your final answer should be expressed in terms of $x_k$, $x_{k-1}$, and the first derivative of the function, $f'$, evaluated at these points.",
            "solution": "We consider the one-dimensional quasi-Newton update, which in the scalar case reads\n$$\nx_{k+1} = x_k - b_k^{-1} f'(x_k),\n$$\nwhere $b_k$ is the scalar approximation to the second derivative at iteration $k$. The secant condition in one dimension is\n$$\nb_k (x_k - x_{k-1}) = f'(x_k) - f'(x_{k-1}).\n$$\nSolving this for $b_k$ gives\n$$\nb_k = \\frac{f'(x_k) - f'(x_{k-1})}{x_k - x_{k-1}},\n$$\nprovided $x_k \\neq x_{k-1}$ and $f'(x_k) \\neq f'(x_{k-1})$. Therefore,\n$$\nb_k^{-1} = \\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})}.\n$$\nSubstituting this into the update formula yields\n$$\nx_{k+1} = x_k - \\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})} \\, f'(x_k)\n= x_k - \\frac{f'(x_k)\\,(x_k - x_{k-1})}{f'(x_k) - f'(x_{k-1})}.\n$$\nThis is the secant-method update applied to the root-finding problem $f'(x)=0$.",
            "answer": "$$\\boxed{x_k - \\frac{f'(x_k)\\,(x_k - x_{k-1})}{f'(x_k) - f'(x_{k-1})}}$$"
        },
        {
            "introduction": "For large-scale problems, the full BFGS method is too memory-intensive, which is where its celebrated limited-memory variant, L-BFGS, becomes essential. However, this efficiency comes at a cost. This advanced exercise uses a carefully designed quadratic objective function to expose a fundamental trade-off in L-BFGS: the consequence of \"forgetting\" old curvature information. By comparing the L-BFGS search direction with the exact Newton direction, you will gain a deeper, more practical understanding of how the memory parameter $m$ influences the algorithm's performance. ",
            "id": "3264848",
            "problem": "You are to construct a strictly convex quadratic objective that exposes the effect of limited memory in the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) method forgetting curvature information from early iterations, and then compute a quantitative discrepancy between the L-BFGS and Newton search directions.\n\nConsider the function $f : \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} Q x, \n\\quad \\text{where } Q \\;=\\; \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nThus $f$ is strictly convex with anisotropic curvature in the canonical directions $e_{1}$ and $e_{2}$. Let the starting point be $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, and suppose that two exact line-search iterations of a limited-memory quasi-Newton method produce the following displacements:\n$$\ns_{0} \\;=\\; x_{1} - x_{0} \\;=\\; e_{1}, \n\\qquad\ns_{1} \\;=\\; x_{2} - x_{1} \\;=\\; e_{2}.\n$$\nFor this quadratic objective, the gradient is $\\nabla f(x) = Q x$, so the corresponding curvature pairs are\n$$\ny_{0} \\;=\\; \\nabla f(x_{1}) - \\nabla f(x_{0}) \\;=\\; Q s_{0},\n\\qquad\ny_{1} \\;=\\; \\nabla f(x_{2}) - \\nabla f(x_{1}) \\;=\\; Q s_{1}.\n$$\n\nNow apply L-BFGS with memory parameter $m = 1$ at iteration $k = 2$ using only the most recent pair $(s_{1}, y_{1})$, together with the common L-BFGS choice of a scalar initial inverse-Hessian $H_{2}^{(0)} = \\gamma_{2} I$ that is consistent with the latest pair. For the data above, this implies $H_{2}^{(0)} = I$. Compute the cosine of the angle between:\n- the L-BFGS search direction $p_{\\mathrm{LB}} = - H_{2} \\nabla f(x_{2})$, where $H_{2}$ is the inverse-Hessian approximation obtained from the single Broyden-Fletcher-Goldfarb-Shanno (BFGS) update using $(s_{1}, y_{1})$ and $H_{2}^{(0)} = I$, and\n- the exact Newton direction $p_{\\mathrm{N}} = - Q^{-1} \\nabla f(x_{2})$ at $x_{2}$.\n\nGive your answer in exact form as a single expression. No rounding is required.",
            "solution": "The objective is to compute the cosine of the angle between the L-BFGS and Newton search directions at iteration $k=2$.\n\nFirst, we determine the point $x_2$ and the gradient at that point, $\\nabla f(x_2)$.\nThe initial point is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe first iterate is $x_1 = x_0 + s_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe second iterate is $x_2 = x_1 + s_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe gradient of the function $f(x) = \\frac{1}{2} x^{\\top} Q x$ is $\\nabla f(x) = Qx$.\nAt $x_2$, the gradient is $\\nabla f(x_2) = Q x_2 = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$.\n\nNext, we compute the exact Newton direction, $p_{\\mathrm{N}}$.\nThe Newton direction is given by $p_{\\mathrm{N}} = -(\\nabla^2 f(x_2))^{-1} \\nabla f(x_2)$. The Hessian is constant, $\\nabla^2 f(x) = Q$.\nThe inverse of $Q$ is $Q^{-1} = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nSo, the Newton direction is\n$$\np_{\\mathrm{N}} = -Q^{-1} \\nabla f(x_2) = - \\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}.\n$$\n\nNow, we compute the L-BFGS direction, $p_{\\mathrm{LB}}$.\nAt iteration $k=2$ with memory $m=1$, the L-BFGS method uses only the most recent pair $(s_1, y_1)$ to construct the inverse Hessian approximation $H_2$.\nThe process starts with an initial matrix $H_2^{(0)}$. As stated in the problem, $H_2^{(0)} = I$.\nThe L-BFGS approximation $H_2$ is obtained by applying one BFGS update to $H_2^{(0)}$ using the pair $(s_1, y_1)$. The BFGS update for the inverse Hessian is:\n$$\nH_{k+1} = \\left(I - \\rho_k s_k y_k^{\\top}\\right) H_k \\left(I - \\rho_k y_k s_k^{\\top}\\right) + \\rho_k s_k s_k^{\\top}\n$$\nIn our case, we update $H_2^{(0)}$ with $(s_1, y_1)$ to get $H_2$. We need $\\rho_1 = \\frac{1}{y_1^{\\top} s_1}$.\nThe vectors are $s_{1} = e_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ and $y_{1} = Q s_{1} = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nSo, $\\rho_1 = \\frac{1}{y_1^{\\top} s_1} = \\frac{1}{1} = 1$.\nThe matrix products needed are:\n$s_1 y_1^{\\top} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$. This is also equal to $y_1 s_1^{\\top}$.\n$s_1 s_1^{\\top} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n\nSubstituting into the update formula with $H_2^{(0)} = I$:\n$$\nH_2 = \\left(I - 1 \\cdot \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) I \\left(I - 1 \\cdot \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) + 1 \\cdot \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nH_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} I \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nH_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nH_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I.\n$$\nThe L-BFGS inverse Hessian approximation is the identity matrix. This is because the initial matrix $H_2^{(0)}=I$ already satisfies the secant condition for the pair $(s_1, y_1)$, as $H_2^{(0)}y_1 = I y_1 = y_1 = s_1$.\n\nThe L-BFGS direction is then:\n$$\np_{\\mathrm{LB}} = -H_2 \\nabla f(x_2) = -I \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -1 \\end{pmatrix}.\n$$\nThis is the steepest descent direction, which is expected since the inverse Hessian approximation is the identity matrix. The information about the curvature in the $e_1$ direction, contained in the pair $(s_0, y_0)$, has been forgotten.\n\nFinally, we compute the cosine of the angle $\\theta$ between $p_{\\mathrm{LB}}$ and $p_{\\mathrm{N}}$ using the formula $\\cos(\\theta) = \\frac{p_{\\mathrm{LB}}^{\\top} p_{\\mathrm{N}}}{\\|p_{\\mathrm{LB}}\\| \\|p_{\\mathrm{N}}\\|}$.\n$$\np_{\\mathrm{LB}} = \\begin{pmatrix} -3 \\\\ -1 \\end{pmatrix}, \\quad p_{\\mathrm{N}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}\n$$\nThe dot product is:\n$$\np_{\\mathrm{LB}}^{\\top} p_{\\mathrm{N}} = (-3)(-1) + (-1)(-1) = 3 + 1 = 4.\n$$\nThe norms are:\n$$\n\\|p_{\\mathrm{LB}}\\| = \\sqrt{(-3)^2 + (-1)^2} = \\sqrt{9 + 1} = \\sqrt{10}.\n$$\n$$\n\\|p_{\\mathrm{N}}\\| = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1 + 1} = \\sqrt{2}.\n$$\nThe cosine of the angle is:\n$$\n\\cos(\\theta) = \\frac{4}{\\sqrt{10} \\sqrt{2}} = \\frac{4}{\\sqrt{20}} = \\frac{4}{\\sqrt{4 \\cdot 5}} = \\frac{4}{2\\sqrt{5}} = \\frac{2}{\\sqrt{5}}.\n$$\nTo rationalize the denominator, this can be written as $\\frac{2\\sqrt{5}}{5}$. We provide the expression in its simplest form.",
            "answer": "$$\n\\boxed{\\frac{2}{\\sqrt{5}}}\n$$"
        }
    ]
}