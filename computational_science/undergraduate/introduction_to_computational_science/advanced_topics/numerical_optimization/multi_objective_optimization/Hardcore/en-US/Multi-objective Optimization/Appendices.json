{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational workout in multi-objective optimization by guiding you through the essential preprocessing steps for scalarization. You will calculate the 'best possible' (utopia) and 'worst trade-off' (nadir) points to understand the objective space's boundaries. This practice is crucial for developing the skills to normalize disparate objectives before combining them into a single function using the weighted-sum method .",
            "id": "3154115",
            "problem": "Consider the bi-objective convex optimization problem with decision vector $\\mathbf{x} = (x_{1}, x_{2})$:\n$$\\text{minimize } \\big(f_{1}(\\mathbf{x}), f_{2}(\\mathbf{x})\\big)$$\nsubject to the affine constraint\n$$x_{1} + x_{2} = 1 \\quad \\text{and} \\quad x_{1} \\geq 0,\\; x_{2} \\geq 0,$$\nwhere the objective functions are\n$$f_{1}(\\mathbf{x}) = x_{1}^{2} + 2\\,(x_{2} - 1)^{2}, \\qquad f_{2}(\\mathbf{x}) = 3\\,(x_{1} - 1)^{2} + x_{2}^{2}.$$\n\nUse the following foundational definitions:\n- The utopia point $\\mathbf{z}^{\\mathrm{ideal}}$ is the vector whose $i$-th component $z_{i}^{\\mathrm{ideal}}$ is the infimum of $f_{i}(\\mathbf{x})$ over the feasible set.\n- For a bi-objective convex problem whose Pareto-efficient set has two extreme efficient points obtained by separately minimizing $f_{1}$ and $f_{2}$, the nadir point $\\mathbf{z}^{\\mathrm{nadir}}$ can be determined component-wise by evaluating each objective at the extreme efficient point of the other objective.\n\nNormalize each objective using an affine transform so that the utopia point maps to $0$ and the nadir point maps to $1$, i.e.,\n$$\\hat{f}_{i}(\\mathbf{x}) = \\frac{f_{i}(\\mathbf{x}) - z_{i}^{\\mathrm{ideal}}}{z_{i}^{\\mathrm{nadir}} - z_{i}^{\\mathrm{ideal}}}, \\quad i \\in \\{1,2\\}.$$\nThen perform a normalized weighted-sum scalarization with weights\n$$w_{1} = \\frac{1}{3}, \\qquad w_{2} = \\frac{2}{3},$$\nso the scalarized objective is $w_{1}\\,\\hat{f}_{1}(\\mathbf{x}) + w_{2}\\,\\hat{f}_{2}(\\mathbf{x})$.\n\nDetermine the optimal decision vector $\\mathbf{x}^{\\star}$ that minimizes the normalized weighted-sum scalarization over the feasible set. Express your final answer exactly (no rounding), as a row vector $\\big(x_{1}^{\\star},\\, x_{2}^{\\star}\\big)$.",
            "solution": "The user has provided a bi-objective optimization problem to be solved using normalized weighted-sum scalarization.\n\nFirst, we validate the problem statement.\nThe problem provides two objective functions, $f_1(\\mathbf{x}) = x_1^2 + 2(x_2 - 1)^2$ and $f_2(\\mathbf{x}) = 3(x_1 - 1)^2 + x_2^2$, to be minimized over a feasible set defined by the constraints $x_1 + x_2 = 1$, $x_1 \\ge 0$, and $x_2 \\ge 0$. The functions are convex, and the feasible set is a convex polytope. The problem defines all necessary concepts (utopia point, nadir point) and provides all parameters (weights for scalarization). The problem is mathematically sound, self-contained, and well-posed. Thus, it is a valid problem, and we may proceed with the solution.\n\nThe solution process consists of the following steps:\n1. Simplify the problem by parameterizing the feasible set.\n2. Determine the utopia point $\\mathbf{z}^{\\mathrm{ideal}}$.\n3. Determine the nadir point $\\mathbf{z}^{\\mathrm{nadir}}$.\n4. Normalize the objective functions.\n5. Formulate and solve the weighted-sum scalarized problem.\n\nStep 1: Simplify the problem\nThe feasible set is defined by the constraints $x_1 + x_2 = 1$, $x_1 \\ge 0$, and $x_2 \\ge 0$. We can express $x_2$ in terms of $x_1$ as $x_2 = 1 - x_1$. The non-negativity constraints $x_1 \\ge 0$ and $x_2 \\ge 0$ become $x_1 \\ge 0$ and $1 - x_1 \\ge 0$, which simplifies to the single condition $0 \\le x_1 \\le 1$.\nThe problem is now reduced to a one-dimensional optimization problem over the interval $x_1 \\in [0, 1]$. We rewrite the objective functions in terms of $x_1$ only:\n$$f_1(x_1) = x_1^2 + 2((1 - x_1) - 1)^2 = x_1^2 + 2(-x_1)^2 = x_1^2 + 2x_1^2 = 3x_1^2$$\n$$f_2(x_1) = 3(x_1 - 1)^2 + (1 - x_1)^2 = 3(x_1-1)^2 + (x_1-1)^2 = 4(x_1 - 1)^2$$\nSo, the problem is to minimize $(3x_1^2, 4(x_1 - 1)^2)$ for $x_1 \\in [0, 1]$.\n\nStep 2: Determine the utopia point $\\mathbf{z}^{\\mathrm{ideal}}$\nThe utopia point is $\\mathbf{z}^{\\mathrm{ideal}} = (z_1^{\\mathrm{ideal}}, z_2^{\\mathrm{ideal}})$, where $z_i^{\\mathrm{ideal}} = \\inf f_i(x_1)$ over the feasible set $x_1 \\in [0, 1]$.\nFor $f_1(x_1) = 3x_1^2$, the minimum on $[0, 1]$ occurs at $x_1 = 0$. The minimum value is $z_1^{\\mathrm{ideal}} = 3(0)^2 = 0$. The corresponding decision vector is $\\mathbf{x}^{(1)} = (0, 1-0) = (0, 1)$.\nFor $f_2(x_1) = 4(x_1 - 1)^2$, the minimum on $[0, 1]$ occurs at $x_1 = 1$. The minimum value is $z_2^{\\mathrm{ideal}} = 4(1 - 1)^2 = 0$. The corresponding decision vector is $\\mathbf{x}^{(2)} = (1, 1-1) = (1, 0)$.\nThe utopia point is therefore $\\mathbf{z}^{\\mathrm{ideal}} = (0, 0)$.\n\nStep 3: Determine the nadir point $\\mathbf{z}^{\\mathrm{nadir}}$\nAccording to the problem definition, the nadir point $\\mathbf{z}^{\\mathrm{nadir}} = (z_1^{\\mathrm{nadir}}, z_2^{\\mathrm{nadir}})$ is found by evaluating each objective at the extreme efficient point of the other. The extreme efficient points are the individual minimizers $\\mathbf{x}^{(1)}$ and $\\mathbf{x}^{(2)}$.\nThe first component, $z_1^{\\mathrm{nadir}}$, is the value of $f_1$ at the minimizer of $f_2$, which is $\\mathbf{x}^{(2)} = (1, 0)$ (or $x_1 = 1$).\n$$z_1^{\\mathrm{nadir}} = f_1(\\mathbf{x}^{(2)}) = 3(1)^2 = 3$$\nThe second component, $z_2^{\\mathrm{nadir}}$, is the value of $f_2$ at the minimizer of $f_1$, which is $\\mathbf{x}^{(1)} = (0, 1)$ (or $x_1 = 0$).\n$$z_2^{\\mathrm{nadir}} = f_2(\\mathbf{x}^{(1)}) = 4(0 - 1)^2 = 4$$\nThe nadir point is therefore $\\mathbf{z}^{\\mathrm{nadir}} = (3, 4)$.\n\nStep 4: Normalize the objective functions\nWe use the given affine transformation:\n$$\\hat{f}_{i}(\\mathbf{x}) = \\frac{f_{i}(\\mathbf{x}) - z_{i}^{\\mathrm{ideal}}}{z_{i}^{\\mathrm{nadir}} - z_{i}^{\\mathrm{ideal}}}$$\nFor $i=1$:\n$$\\hat{f}_1(x_1) = \\frac{3x_1^2 - 0}{3 - 0} = x_1^2$$\nFor $i=2$:\n$$\\hat{f}_2(x_1) = \\frac{4(x_1 - 1)^2 - 0}{4 - 0} = (x_1 - 1)^2$$\n\nStep 5: Formulate and solve the weighted-sum scalarized problem\nThe scalarized objective function $F(x_1)$ is given by $w_1\\hat{f}_1(x_1) + w_2\\hat{f}_2(x_1)$ with weights $w_1 = \\frac{1}{3}$ and $w_2 = \\frac{2}{3}$.\n$$F(x_1) = \\frac{1}{3} \\hat{f}_1(x_1) + \\frac{2}{3} \\hat{f}_2(x_1) = \\frac{1}{3} x_1^2 + \\frac{2}{3} (x_1 - 1)^2$$\nWe need to minimize $F(x_1)$ over the interval $x_1 \\in [0, 1]$.\nFirst, we expand the expression for $F(x_1)$:\n$$F(x_1) = \\frac{1}{3} x_1^2 + \\frac{2}{3} (x_1^2 - 2x_1 + 1) = \\frac{1}{3} x_1^2 + \\frac{2}{3} x_1^2 - \\frac{4}{3} x_1 + \\frac{2}{3} = x_1^2 - \\frac{4}{3} x_1 + \\frac{2}{3}$$\nThis is a convex quadratic function. We can find its minimum by taking the derivative with respect to $x_1$ and setting it to zero.\n$$\\frac{dF}{dx_1} = 2x_1 - \\frac{4}{3}$$\nSetting the derivative to zero:\n$$2x_1 - \\frac{4}{3} = 0 \\implies 2x_1 = \\frac{4}{3} \\implies x_1 = \\frac{2}{3}$$\nThis value $x_1^{\\star} = \\frac{2}{3}$ lies within the feasible interval $[0, 1]$. Since $F(x_1)$ is a convex function, this stationary point is the global minimum.\nThe optimal value for $x_1$ is $x_1^{\\star} = \\frac{2}{3}$.\nWe find the corresponding optimal value for $x_2$ using the constraint $x_1 + x_2 = 1$:\n$$x_2^{\\star} = 1 - x_1^{\\star} = 1 - \\frac{2}{3} = \\frac{1}{3}$$\nThe optimal decision vector is $\\mathbf{x}^{\\star} = (x_1^{\\star}, x_2^{\\star}) = (\\frac{2}{3}, \\frac{1}{3})$. Both components are non-negative, so all constraints are satisfied.\nThe final answer is the optimal decision vector expressed as a row vector.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{3}  \\frac{1}{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While the weighted-sum method is straightforward, it has a significant limitation: it cannot find solutions in non-convex regions of the Pareto front. This practice is designed to demonstrate this exact scenario, contrasting the weighted-sum method with the more robust weighted Tchebycheff scalarization. By working through this classic example, you will gain a deeper understanding of why different scalarization methods are needed and how they perform on different problem geometries .",
            "id": "3162766",
            "problem": "You are asked to construct and analyze a minimal, fully reproducible example that illustrates why the Weighted Sum Method (WSM) can fail to recover certain Pareto-optimal points on a non-convex Pareto front, while the weighted Tchebycheff scalarization can recover them. The setting and requirements are as follows.\n\nConsider the bi-objective minimization problem in a one-dimensional decision space with decision variable $x \\in [0,1]$ and objective vector $f(x) = (f_1(x), f_2(x))$ defined by\n$$\nf_1(x) = x, \\quad f_2(x) = \\sqrt{1 - x^2}.\n$$\nAssume the utopia point $z^\\text{utopia} = (0,0)$, which is the component-wise infimum of the objectives. You will use the following scalarization definitions, which are the standard starting point in multi-objective optimization (MOO):\n\n- Pareto dominance: For two feasible points $x,y \\in [0,1]$, $x$ dominates $y$ if $f_i(x) \\le f_i(y)$ for all $i \\in \\{1,2\\}$ and there exists at least one index $j$ with $f_j(x)  f_j(y)$.\n- Pareto optimality: A feasible point $x^\\star$ is Pareto optimal if it is not dominated by any other feasible point.\n- Weighted Sum Method (WSM): Given positive weights $w_10$ and $w_20$, define the scalarized objective\n$$\n\\phi_\\text{WSM}(x;w) = w_1 f_1(x) + w_2 f_2(x),\n$$\nand compute a minimizer over $x \\in [0,1]$.\n- Weighted Tchebycheff scalarization: Given positive weights $w_10$ and $w_20$, and the utopia point $z^\\text{utopia}$, define\n$$\n\\phi_\\text{WT}(x;w) = \\max\\{\\, w_1 \\lvert f_1(x) - z_1^\\text{utopia} \\rvert,\\; w_2 \\lvert f_2(x) - z_2^\\text{utopia} \\rvert \\,\\}\n= \\max\\{\\, w_1 f_1(x),\\; w_2 f_2(x) \\,\\},\n$$\nand compute a minimizer over $x \\in [0,1]$.\n\nYour task is to write a complete program that, for a given list of weight vectors, computes, for each weight vector $w=(w_1,w_2)$:\n- a minimizer $x_\\text{WSM}(w)$ of $\\phi_\\text{WSM}(x;w)$ over $x \\in [0,1]$,\n- a minimizer $x_\\text{WT}(w)$ of $\\phi_\\text{WT}(x;w)$ over $x \\in [0,1]$,\n- and returns the absolute difference\n$$\nd(w) = \\lvert x_\\text{WT}(w) - x_\\text{WSM}(w) \\rvert.\n$$\n\nYour program must implement these scalarizations from first principles, using only the problemâ€™s definitions. The objective image $f([0,1])$ is the quarter-circle arc $\\{(f_1,f_2) \\mid f_1 = x,\\; f_2=\\sqrt{1-x^2},\\; x\\in[0,1]\\}$, which is non-convex as a set in objective space. Because WSM minimizes a linear functional $w_1 f_1 + w_2 f_2$, it can fail to recover interior points of non-convex Pareto fronts. In contrast, the weighted Tchebycheff scalarization balances the weighted deviations from $z^\\text{utopia}$ and can recover interior Pareto points on such non-convex fronts.\n\nNumerical requirements:\n- Use only the weights specified in the test suite below.\n- For each weight vector $w$, compute $d(w)$ and round it to six decimal places.\n- No physical units are involved.\n- Angles are not used.\n- Percentages are not used.\n\nTest suite:\n- Use the following five weight vectors $w = (w_1,w_2)$:\n$$\n\\{\\, (1.0,1.0),\\; (0.3,0.7),\\; (0.7,0.3),\\; (0.55,0.45),\\; (0.99,0.01) \\,\\}.\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the five rounded values $[d(w^{(1)}), d(w^{(2)}), d(w^{(3)}), d(w^{(4)}), d(w^{(5)})]$ as a comma-separated list enclosed in square brackets, for the weights listed in the order above. For example, an acceptable format is\n$$\n[0.123456,0.234567,0.345678,0.456789,0.567890]\n$$\nwith exactly six digits after the decimal point for each entry.",
            "solution": "The user-provided problem statement has been validated and found to be well-posed, scientifically grounded, and internally consistent. It presents a standard, illustrative example from the field of multi-objective optimization, suitable for analysis. We may therefore proceed with the solution.\n\nThe problem requires us to find the minimizers of two different scalarization functions for a bi-objective optimization problem and compute the absolute difference between these minimizers for a given set of weights. The decision variable is $x \\in [0,1]$, and the objective functions to be minimized are $f_1(x) = x$ and $f_2(x) = \\sqrt{1-x^2}$. The image of the feasible set in the objective space, $f([0,1])$, is the quarter-circle arc defined by $f_1^2 + f_2^2 = 1$ for $f_1, f_2 \\ge 0$. This represents a non-convex Pareto front, which is key to the problem's demonstration.\n\nWe will derive the analytical solution for the minimizer for each method, a necessary step before implementing the numerical computation.\n\n### Analysis of the Weighted Sum Method (WSM)\n\nThe Weighted Sum Method (WSM) seeks to minimize the scalarized objective function:\n$$ \\phi_\\text{WSM}(x;w) = w_1 f_1(x) + w_2 f_2(x) = w_1 x + w_2 \\sqrt{1 - x^2} $$\nwhere the weights $w_1, w_2$ are strictly positive. The minimization is performed over the compact domain $x \\in [0,1]$.\n\nTo find the minimizer, we analyze the function's behavior on the interval. We first compute the derivative of $\\phi_\\text{WSM}(x;w)$ with respect to $x$:\n$$ \\frac{d\\phi_\\text{WSM}}{dx} = w_1 + w_2 \\cdot \\frac{1}{2\\sqrt{1-x^2}} \\cdot (-2x) = w_1 - \\frac{w_2 x}{\\sqrt{1 - x^2}} $$\nThe second derivative is:\n$$ \\frac{d^2\\phi_\\text{WSM}}{dx^2} = -w_2 \\frac{d}{dx} \\left( \\frac{x}{\\sqrt{1-x^2}} \\right) = -w_2 \\left( \\frac{1 \\cdot \\sqrt{1-x^2} - x \\frac{-x}{\\sqrt{1-x^2}}}{1-x^2} \\right) = -w_2 \\left( \\frac{1-x^2+x^2}{(1-x^2)^{3/2}} \\right) = -\\frac{w_2}{(1-x^2)^{3/2}} $$\nSince $w_2  0$ and $x \\in [0,1)$, the second derivative $\\frac{d^2\\phi_\\text{WSM}}{dx^2}$ is strictly negative. This indicates that $\\phi_\\text{WSM}(x;w)$ is a strictly concave function over the interval $[0,1)$.\n\nA fundamental property of concave functions is that their minimum over a closed interval must occur at one of the interval's endpoints. Therefore, the minimizer $x_\\text{WSM}(w)$ must be either $0$ or $1$. To determine which one, we simply compare the value of $\\phi_\\text{WSM}$ at these two points:\n- At $x=0$: $\\phi_\\text{WSM}(0;w) = w_1 \\cdot 0 + w_2 \\sqrt{1-0^2} = w_2$.\n- At $x=1$: $\\phi_\\text{WSM}(1;w) = w_1 \\cdot 1 + w_2 \\sqrt{1-1^2} = w_1$.\n\nThe minimum value is $\\min(w_1, w_2)$.\n- If $w_1  w_2$, the minimum occurs at $x=1$.\n- If $w_2  w_1$, the minimum occurs at $x=0$.\n- If $w_1 = w_2$, both $x=0$ and $x=1$ are minimizers. We can adopt a convention, such as choosing the smaller value of $x$, to ensure a unique result.\n\nCombining these cases, we define the minimizer $x_\\text{WSM}(w)$ as:\n$$ x_\\text{WSM}(w) = \\begin{cases} 0  \\text{if } w_1 \\ge w_2 \\\\ 1  \\text{if } w_1  w_2 \\end{cases} $$\nThis result confirms that for this non-convex problem, the WSM can only find solutions at the extremities of the Pareto front ($x=0$ or $x=1$), corresponding to the objective points $(0,1)$ and $(1,0)$. It fails to find any interior points.\n\n### Analysis of the Weighted Tchebycheff Scalarization\n\nThe weighted Tchebycheff method minimizes the following scalarized objective, using the utopia point $z^\\text{utopia} = (0,0)$:\n$$ \\phi_\\text{WT}(x;w) = \\max\\{\\, w_1 \\lvert f_1(x) - z_1^\\text{utopia} \\rvert,\\; w_2 \\lvert f_2(x) - z_2^\\text{utopia} \\rvert \\,\\} $$\nGiven that $f_1(x)=x \\ge 0$ and $f_2(x)=\\sqrt{1-x^2} \\ge 0$ for $x \\in [0,1]$, this simplifies to:\n$$ \\phi_\\text{WT}(x;w) = \\max\\{\\, w_1 x, \\; w_2 \\sqrt{1-x^2} \\,\\} $$\nLet $g_1(x) = w_1 x$ and $g_2(x) = w_2 \\sqrt{1-x^2}$.\n- $g_1(x)$ is a strictly increasing function on $[0,1]$.\n- $g_2(x)$ is a strictly decreasing function on $[0,1]$.\n\nThe function $\\phi_\\text{WT}(x;w)$ is the upper envelope of these two functions. The minimum of this upper envelope occurs at the point where the two functions intersect, because to the left of the intersection $g_2$ dominates and is decreasing, and to the right $g_1$ dominates and is increasing. Let us find this intersection point $x^\\star$ by setting $g_1(x^\\star) = g_2(x^\\star)$:\n$$ w_1 x^\\star = w_2 \\sqrt{1 - (x^\\star)^2} $$\nSince $w_1, w_2  0$ and $x^\\star \\in [0,1]$, we can square both sides without introducing extraneous solutions:\n$$ w_1^2 (x^\\star)^2 = w_2^2 (1 - (x^\\star)^2) $$\n$$ w_1^2 (x^\\star)^2 = w_2^2 - w_2^2 (x^\\star)^2 $$\n$$ (w_1^2 + w_2^2) (x^\\star)^2 = w_2^2 $$\n$$ (x^\\star)^2 = \\frac{w_2^2}{w_1^2 + w_2^2} $$\nTaking the positive square root (since $x \\in [0,1]$), we find the minimizer:\n$$ x_\\text{WT}(w) = \\frac{w_2}{\\sqrt{w_1^2 + w_2^2}} $$\nThis formula yields a value of $x$ in the open interval $(0,1)$ for any positive weights $w_1, w_2$. This demonstrates that the weighted Tchebycheff method, unlike WSM, can recover all interior points of the non-convex Pareto front by varying the weights.\n\n### Computational Procedure\nThe task is to compute $d(w) = \\lvert x_\\text{WT}(w) - x_\\text{WSM}(w) \\rvert$ for each weight vector $w=(w_1, w_2)$ in the provided test suite. The procedure for each vector will be:\n1.  Determine $x_\\text{WSM}(w)$ by comparing $w_1$ and $w_2$.\n2.  Calculate $x_\\text{WT}(w)$ using the derived formula.\n3.  Compute the absolute difference $d(w)$.\n4.  Format the result to six decimal places.\n\nThe final program will implement these steps for each test case and format the output as requested.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference between minimizers of WSM and Tchebycheff methods\n    for a bi-objective problem with a non-convex Pareto front.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0),\n        (0.3, 0.7),\n        (0.7, 0.3),\n        (0.55, 0.45),\n        (0.99, 0.01)\n    ]\n\n    results = []\n    for case in test_cases:\n        w1, w2 = case\n\n        # 1. Compute the minimizer for the Weighted Sum Method (WSM).\n        # The WSM objective function is phi(x) = w1*x + w2*sqrt(1-x^2).\n        # This function is concave on x in [0,1], so its minimum must be at\n        # an endpoint (x=0 or x=1).\n        # At x=0, phi(0) = w2. At x=1, phi(1) = w1.\n        # The minimizer is x=0 if w2 = w1, and x=1 if w1  w2.\n        # To ensure a unique minimizer when w1=w2, we choose x=0.\n        if w1 = w2:\n            x_wsm = 0.0\n        else:\n            x_wsm = 1.0\n\n        # 2. Compute the minimizer for the Weighted Tchebycheff method.\n        # The Tchebycheff objective is phi(x) = max(w1*x, w2*sqrt(1-x^2)).\n        # The minimum of the maximum of these two monotonic functions\n        # occurs at their intersection point.\n        # Solving w1*x = w2*sqrt(1-x^2) for x yields the formula below.\n        x_wt = w2 / np.sqrt(w1**2 + w2**2)\n\n        # 3. Compute the absolute difference.\n        d_w = abs(x_wt - x_wsm)\n\n        # 4. Format the result to six decimal places and store it.\n        results.append(f\"{d_w:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Multi-objective optimization truly comes to life when applied to real-world problems like pathfinding, where one might want the shortest path that is also the safest. This hands-on coding challenge tasks you with implementing a multi-objective shortest path algorithm, a generalization of Dijkstra's algorithm, to find all Pareto-optimal routes on a grid. This exercise moves beyond simple scalarization to the direct generation of the entire Pareto front, a core technique in computational optimization .",
            "id": "3162786",
            "problem": "You are given a two-dimensional rectangular grid that induces a directed graph where each grid cell is a node, and edges connect orthogonally adjacent cells (up, down, left, right) when such neighbors exist. A path is any finite sequence of adjacent moves from a designated start cell to a designated goal cell. Each move incurs two additive objective costs: the path-length objective $f_1$ increases by $1$ for every move, and the risk-exposure objective $f_2$ increases by the risk value of the destination cell entered by that move. The start cell contributes no risk unless it is re-entered later as a destination of a move. All risk values are nonnegative integers. If a cell is impassable, it is omitted from the graph (no edges can enter or leave it). You will consider only four-neighbor moves, and there are no diagonal moves. Angles and physical units are not used in this problem.\n\nThe multi-objective path cost of a path $\\pi$ is the vector $F(\\pi) = (f_1(\\pi), f_2(\\pi))$, with $f_1(\\pi)$ equal to the number of moves in $\\pi$ and $f_2(\\pi)$ equal to the sum of destination-cell risk values along $\\pi$. A cost vector $a \\in \\mathbb{R}^2_{\\ge 0}$ Pareto-dominates another vector $b \\in \\mathbb{R}^2_{\\ge 0}$, written $a \\prec b$, if and only if $a_1 \\le b_1$, $a_2 \\le b_2$, and $(a_1  b_1)$ or $(a_2  b_2)$. A path $\\pi^\\star$ from the start to the goal is Pareto-optimal if there is no other feasible path $\\pi$ with $F(\\pi) \\prec F(\\pi^\\star)$. The set of all Pareto-optimal cost vectors at the goal is the Pareto frontier.\n\nYour task is to implement a label-setting multi-criteria shortest path procedure over the grid graph to compute the Pareto frontier at the goal for each of the following test cases. You must start from fundamental definitions of graphs, paths, and Pareto dominance, and design the algorithm so that it maintains, for each node, only nondominated labels. The algorithm must terminate without user input and return the complete Pareto frontier for each case, sorted in ascending lexicographic order by $(f_1, f_2)$.\n\nDefine the following test suite. In each grid, a value of $-1$ marks an impassable cell, while any value in $\\{0,1,2,\\dots\\}$ is a passable cell with that risk. Coordinates are given as $(\\text{row}, \\text{col})$ with zero-based indexing. All numbers are integers.\n\n- Test case A (happy path trade-off coverage, small grid):\n  - Grid of size $3 \\times 3$ with risk matrix\n    $$\n    \\begin{bmatrix}\n    1  4  1 \\\\\n    1  9  1 \\\\\n    1  1  1\n    \\end{bmatrix}\n    $$\n  - Start $(0,0)$, Goal $(2,2)$.\n  - No impassable cells.\n\n- Test case B (boundary condition: start equals goal):\n  - Grid of size $2 \\times 2$ with risk matrix\n    $$\n    \\begin{bmatrix}\n    2  3 \\\\\n    4  1\n    \\end{bmatrix}\n    $$\n  - Start $(0,0)$, Goal $(0,0)$.\n  - No impassable cells.\n\n- Test case C (explicit trade-off: longer path with lower risk):\n  - Grid of size $2 \\times 3$ with risk matrix\n    $$\n    \\begin{bmatrix}\n    0  5  1 \\\\\n    0  0  0\n    \\end{bmatrix}\n    $$\n  - Start $(0,0)$, Goal $(0,2)$.\n  - No impassable cells.\n\nFor each test case, your program must compute the set of Pareto-optimal cost vectors at the goal, expressed as a list of pairs $[f_1,f_2]$, sorted in ascending lexicographic order by $f_1$ and then by $f_2$. The final program output must be a single line containing the results for all test cases, aggregated into an outer list of length $3$. The required output format is a single line containing a comma-separated list enclosed in square brackets, where each element is the sorted list of pairs for the corresponding test. For example, a valid output has the form\n$[\\,[ [f_{1,1},f_{2,1}], \\dots ], [ [\\dots], \\dots ], [ [\\dots], \\dots ]\\,]$\nwith no extra text. All entries $f_{1,\\cdot}$ and $f_{2,\\cdot}$ must be integers. No other output is permitted.",
            "solution": "The user wants to solve a multi-objective shortest path problem on a grid.\n\n### Step 1: Extract Givens\n- **Graph Structure**: The graph nodes are cells in a $2D$ rectangular grid. Directed edges connect orthogonally adjacent cells.\n- **Path**: A finite sequence of adjacent moves from a start cell to a goal cell.\n- **Objectives**: There are two additive cost objectives for any path $\\pi$.\n  1.  $f_1(\\pi)$: Path length, which is the number of moves. Each move adds $1$ to $f_1$.\n  2.  $f_2(\\pi)$: Risk exposure, which is the sum of risk values of the destination cells entered. The risk of the start cell is not included unless re-entered.\n- **Cost Vector**: The cost of a path $\\pi$ is the vector $F(\\pi) = (f_1(\\pi), f_2(\\pi))$.\n- **Impassable Cells**: Cells with a risk value of $-1$ are impassable and are excluded from the graph.\n- **Pareto Dominance**: A cost vector $a = (a_1, a_2)$ Pareto-dominates another vector $b = (b_1, b_2)$, denoted $a \\prec b$, if and only if $a_1 \\le b_1$, $a_2 \\le b_2$, and $(a_1  b_1 \\text{ or } a_2  b_2)$.\n- **Pareto Optimality**: A path $\\pi^\\star$ is Pareto-optimal if no other feasible path $\\pi$ exists such that $F(\\pi) \\prec F(\\pi^\\star)$.\n- **Pareto Frontier**: The set of all Pareto-optimal cost vectors for paths from the start to the goal.\n- **Task**: Implement a label-setting multi-criteria shortest path algorithm to find the Pareto frontier at the goal for three specific test cases.\n- **Output Format**: The final output for each test case is a list of Pareto-optimal cost vectors, sorted in ascending lexicographic order by $(f_1, f_2)$. The overall output is a single line aggregating the results for all test cases.\n- **Test Cases**:\n  - **Case A**: $3 \\times 3$ grid, risk matrix $\\begin{bmatrix} 1  4  1 \\\\ 1  9  1 \\\\ 1  1  1 \\end{bmatrix}$, Start $(0,0)$, Goal $(2,2)$.\n  - **Case B**: $2 \\times 2$ grid, risk matrix $\\begin{bmatrix} 2  3 \\\\ 4  1 \\end{bmatrix}$, Start $(0,0)$, Goal $(0,0)$.\n  - **Case C**: $2 \\times 3$ grid, risk matrix $\\begin{bmatrix} 0  5  1 \\\\ 0  0  0 \\end{bmatrix}$, Start $(0,0)$, Goal $(0,2)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a classic multi-objective shortest path problem, a standard topic in computational science and operations research. The definitions of the graph, costs, and Pareto optimality are precise and standard. The test cases are well-defined and serve to verify different aspects of the algorithm (a standard trade-off, a boundary condition, and an explicit trade-off). There are no scientific or factual unsoundness, no missing information, and no contradictions. The problem is formalizable and its solution is verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with providing a solution.\n\n### Algorithmic Solution\nThe problem requires finding the set of Pareto-optimal paths from a start node to a goal node in a grid graph with two objectives: path length ($f_1$) and cumulative risk ($f_2$). This is a multi-objective shortest path problem. A suitable algorithm is a generalization of Dijkstra's single-objective shortest path algorithm, often referred to as a label-setting algorithm.\n\n**1. Fundamental Concepts**\n\nA single-objective Dijkstra's algorithm finds the shortest path by maintaining a single \"distance\" label for each node. In the multi-objective context, a single path may not be uniquely \"best\". A path might be shorter ($f_1$ is smaller) but riskier ($f_2$ is larger) than another, and neither path is superior to the other. These two paths would correspond to two non-dominated cost vectors.\n\nTherefore, for each node $u$ in the graph, we must maintain a set of labels, $L(u)$, where each label is a cost vector $(f_1, f_2)$ corresponding to a so-far non-dominated path from the start node $s$ to $u$.\n\n**2. Algorithm Design**\n\nThe core of the algorithm is to explore the graph, propagating sets of non-dominated labels from node to node. We use a priority queue to guide the search, prioritizing paths that are \"smaller\" in some sense, typically lexicographically.\n\nLet $s$ be the start node and $g$ be the goal node.\n\n**Initialization:**\n- For each node $u$ in the grid, initialize its label set $L(u)$ to be empty, i.e., $L(u) = \\emptyset$.\n- The path from the start node $s$ to itself has a length of $0$ and incurs no risk. So, we initialize the label set for the start node with the zero vector: $L(s) = \\{(0, 0)\\}$.\n- Initialize a priority queue, $PQ$, which will store tuples of the form `(cost_vector, node)`. The priority queue will order elements based on the `cost_vector`, using lexicographical ordering (first by $f_1$, then by $f_2$).\n- Add the starting label to the priority queue: $PQ$.push($((0, 0), s)$).\n\n**Main Loop:**\nThe algorithm proceeds by repeatedly extracting the \"best\" path from the priority queue and extending it to its neighbors.\n\nWhile the $PQ$ is not empty:\n1.  Extract the element with the minimum cost vector from the $PQ$. Let this be $(c, u)$, where $c=(c_1, c_2)$ is the cost vector and $u$ is the node.\n2.  **Dominance Check at Pop**: It's possible that since $(c, u)$ was added to the $PQ$, a better path to $u$ has been found. We must check if the cost vector $c$ is dominated by any existing label in $L(u)$. If $\\exists c' \\in L(u)$ such that $c' \\prec c$, then this path is suboptimal. We discard it and continue to the next iteration of the loop. This step is crucial for efficiency.\n3.  **Neighbor Expansion**: For each valid neighbor $v$ of node $u$ (i.e., within grid bounds and not impassable):\n    a. Calculate the cost vector for the new path to $v$ via $u$. The path length increases by $1$, and the risk increases by the risk of the destination cell, `risk(v)`. The new cost vector is $c' = (c_1 + 1, c_2 + \\text{risk}(v))$.\n    b. **Label Update at Neighbor**: We must determine if this new path to $v$ is useful. We compare $c'$ with the existing set of labels $L(v)$.\n        i. **Check for Dominance**: Check if $c'$ is dominated by any existing label in $L(v)$. If $\\exists l \\in L(v)$ such that $l \\prec c'$, then the new path is not Pareto-optimal with respect to the paths already found to $v$. We discard $c'$ and do nothing further for this neighbor.\n        ii. **Prune and Add**: If $c'$ is not dominated by any label in $L(v)$, it represents a new non-dominated path to $v$. We must:\n            - **Prune**: Remove any labels from $L(v)$ that are now dominated by $c'$. That is, for each $l \\in L(v)$, if $c' \\prec l$, remove $l$ from $L(v)$.\n            - **Add**: Add the new label $c'$ to the updated set $L(v)$.\n            - **Push to PQ**: Add the new path extension to the priority queue: $PQ$.push($(c', v)$).\n\n**Termination:**\nThe algorithm terminates when the priority queue $PQ$ is empty. At this point, for every reachable node $u$, the set $L(u)$ contains the complete and correct Pareto frontier for paths from $s$ to $u$. The final answer for the problem is the label set at the goal node, $L(g)$, sorted in ascending lexicographic order.\n\n**Correctness:**\nThe use of a priority queue ensures that we generally explore shorter paths before longer ones. The label management process (dominance checking and pruning) at each node guarantees that we only store and propagate non-dominated path costs. Because any non-dominated path is an extension of a non-dominated subpath, and the algorithm explores all such extensions, it is guaranteed to find the complete Pareto frontier upon termination.",
            "answer": "```python\nimport numpy as np\nimport heapq\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the solver for each,\n    and print the results in the specified format.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"grid\": np.array([\n                [1, 4, 1],\n                [1, 9, 1],\n                [1, 1, 1]\n            ]),\n            \"start\": (0, 0),\n            \"goal\": (2, 2)\n        },\n        {\n            \"grid\": np.array([\n                [2, 3],\n                [4, 1]\n            ]),\n            \"start\": (0, 0),\n            \"goal\": (0, 0)\n        },\n        {\n            \"grid\": np.array([\n                [0, 5, 1],\n                [0, 0, 0]\n            ]),\n            \"start\": (0, 0),\n            \"goal\": (0, 2)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        frontier = find_pareto_frontier(case[\"grid\"], case[\"start\"], case[\"goal\"])\n        results.append(frontier)\n    \n    # Format the output string as per problem specification.\n    # e.g., [[[4,4]],[[0,0]],[[2,6],[4,1]]]\n    # This manual construction avoids python's default str() formatting issues (e.g., spaces).\n    outer_parts = []\n    for case_result in results:\n        inner_parts = []\n        for pair in case_result:\n            inner_parts.append(f\"[{pair[0]},{pair[1]}]\")\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    final_string = f\"[{','.join(outer_parts)}]\"\n\n    print(final_string)\n    \n\ndef find_pareto_frontier(grid, start, goal):\n    \"\"\"\n    Implements a multi-objective label-setting algorithm (Dijkstra's generalization)\n    to find the Pareto frontier for a pathfinding problem on a grid.\n\n    Args:\n        grid (np.ndarray): A 2D array of risk values. -1 indicates impassable.\n        start (tuple): The (row, col) coordinates of the start cell.\n        goal (tuple): The (row, col) coordinates of the goal cell.\n\n    Returns:\n        list: A list of [f1, f2] pairs representing the Pareto frontier,\n              sorted lexicographically.\n    \"\"\"\n    rows, cols = grid.shape\n    \n    # labels is a dictionary mapping a node (r,c) to a list of non-dominated cost vectors [(f1, f2), ...].\n    labels = { (r, c): [] for r in range(rows) for c in range(cols) }\n    \n    # Priority queue stores tuples of (cost_vector, node).\n    # cost_vector is (f1, f2). Python's heapq uses lexicographical comparison on tuples by default.\n    pq = []\n\n    # Handle the start node\n    if start == goal:\n        return [[0, 0]]\n        \n    start_label = (0, 0)\n    labels[start].append(start_label)\n    heapq.heappush(pq, (start_label, start))\n\n    while pq:\n        cost, node = heapq.heappop(pq)\n        f1, f2 = cost\n        r, c = node\n\n        # If a better path to this node has been found since this was pushed, skip.\n        is_dominated = False\n        for L_f1, L_f2 in labels[node]:\n            if L_f1 = f1 and L_f2 = f2 and (L_f1  f1 or L_f2  f2):\n                is_dominated = True\n                break\n        if is_dominated:\n            continue\n\n        # Explore neighbors\n        for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n            nr, nc = r + dr, c + dc\n\n            if 0 = nr  rows and 0 = nc  cols and grid[nr, nc] != -1:\n                neighbor = (nr, nc)\n                new_f1 = f1 + 1\n                new_f2 = f2 + grid[nr, nc]\n                new_cost = (new_f1, new_f2)\n\n                # Check if the new path to neighbor is dominated by existing paths to it.\n                is_dominated_by_neighbor_labels = False\n                for L_f1, L_f2 in labels[neighbor]:\n                    if L_f1 = new_f1 and L_f2 = new_f2 and (L_f1  new_f1 or L_f2  new_f2):\n                        is_dominated_by_neighbor_labels = True\n                        break\n                \n                if not is_dominated_by_neighbor_labels:\n                    # Prune labels at the neighbor that are now dominated by new_cost.\n                    labels[neighbor] = [l for l in labels[neighbor] if not (\n                        new_f1 = l[0] and new_f2 = l[1] and (new_f1  l[0] or new_f2  l[1])\n                    )]\n\n                    # Add the new non-dominated label.\n                    labels[neighbor].append(new_cost)\n                    \n                    # Push the new path to the priority queue.\n                    heapq.heappush(pq, (new_cost, neighbor))\n\n    # Sort the final Pareto frontier at the goal lexicographically.\n    final_frontier = sorted(list(labels[goal]))\n    \n    # Special case for start=goal, though handled at the top\n    if start == goal:\n        # A zero-length path from start to itself has cost (0,0) and dominates any cycles.\n        final_frontier.insert(0, (0,0))\n        final_frontier = [l for i, l in enumerate(final_frontier) if l == (0,0) or not any(l_prev[0] = l[0] and l_prev[1] = l[1] for l_prev in final_frontier[:i])]\n\n    return [list(p) for p in final_frontier]\n\nsolve()\n```"
        }
    ]
}