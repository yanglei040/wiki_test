{
    "hands_on_practices": [
        {
            "introduction": "在计算科学中，研究人员经常需要在模型的准确性和计算成本之间做出权衡。这个实践练习  让你亲手构建一个强化学习智能体，它能够学习如何自动做出这种选择。你将通过优化一个策略，将问题特征映射到最佳的计算方法，从而掌握在科学发现中平衡竞争目标的核心技能。",
            "id": "3186206",
            "problem": "你需要形式化并实现一个最小化的强化学习（RL; Reinforcement Learning）设置，用于一个有科学依据的决策任务：选择使用哪种计算近似来评估一组科学假设，同时平衡预期准确性与计算成本。该场景是一个上下文赌博机（contextual bandit），其中每个上下文对应一个具有给定潜在参数的假设，每个动作对应两种计算近似中的一种。\n\n你必须仅从基本定义出发构建和解决问题。智能体必须学习一个策略，通过最大化期望奖励将数值上下文映射到一个动作。奖励必须编码准确性与成本的权衡。你不能硬编码任何直接输出最优动作的神谕（oracle）；相反，应通过使用对期望回报的梯度上升，从第一性原理学习一个参数化策略来解决该优化问题。\n\n基本设置：\n\n- 存在一个由 $h \\in \\mathcal{H}$ 索引的有限假设集。每个假设都有一个真实的标量潜值 $ \\theta_h \\in \\mathbb{R} $。\n- 在每次决策时，智能体观察一个从 $ \\theta_h $ 构建的固定特征向量 $ x_h \\in \\mathbb{R}^d $，然后选择动作 $ a \\in \\{0,1\\} $。动作 $ a=0 $ 表示平均场近似（确定性，有偏），动作 $ a=1 $ 表示蒙特卡洛近似（随机性，无偏有方差）。\n- 智能体收到一个标量奖励 $ r $，其定义为所选近似的估计值与真实值之间的负平方误差，并由线性计算成本进行惩罚。形式上，对于瞬时估计 $ \\widehat{\\theta}_{h,a} $，奖励为\n$$\nr(h,a) \\;=\\; -\\bigl(\\widehat{\\theta}_{h,a} - \\theta_h\\bigr)^2 \\;-\\; \\lambda \\, c_a,\n$$\n其中 $ \\lambda \\ge 0 $ 是一个权衡系数，$ c_a \\ge 0 $ 是依赖于动作的成本。不涉及物理单位。\n\n- 对于平均场近似（$ a=0 $），估计是一个确定性的有偏收缩\n$$\n\\widehat{\\theta}_{h,0} \\;=\\; \\alpha \\, \\theta_h,\n$$\n其中 $ \\alpha \\in (0,1) $。\n- 对于蒙特卡洛近似（$ a=1 $），估计是一个无偏的噪声观测\n$$\n\\widehat{\\theta}_{h,1} \\;=\\; \\theta_h \\;+\\; \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\n其中方差参数为 $ \\sigma^2 \\ge 0 $。因此，当 $ a=1 $ 时，由于 $ \\varepsilon $ 的存在，奖励是一个随机变量。\n\n策略和目标：\n\n- 使用一个在两个动作上带有线性 logits 的 softmax 策略 $ \\pi_\\mathbf{W}(a \\mid x_h) $。设 $ d=2 $ 且 $ x_h = \\begin{bmatrix}1 \\\\ \\theta_h^2\\end{bmatrix} \\in \\mathbb{R}^2 $。设 $ \\mathbf{W} \\in \\mathbb{R}^{2 \\times 2} $ 在其行中收集特定于动作的参数向量。logits 为 $ z_a = \\mathbf{w}_a^{\\top} x_h $，动作概率为\n$$\n\\pi_\\mathbf{W}(a \\mid x_h) \\;=\\; \\frac{\\exp(z_a)}{\\exp(z_0) + \\exp(z_1)}.\n$$\n- 目标是在有限假设集上的平均期望奖励，\n$$\nJ(\\mathbf{W}) \\;=\\; \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\,\\mathbb{E}_{a \\sim \\pi_\\mathbf{W}(\\cdot \\mid x_h)} \\bigl[\\, \\mathbb{E}[\\, r(h,a) \\mid h, a \\,] \\,\\bigr],\n$$\n这由来于上下文赌博机的期望回报定义和全期望定律。你必须仅使用基本定义，通过梯度上升来最大化 $ J(\\mathbf{W}) $。你不能假设梯度公式；必须从 softmax 和期望的定义中推导它。\n\n评估指标：\n\n- 训练后，对于每个 $ h \\in \\mathcal{H} $，通过选择具有最大概率的动作 $ \\arg\\max_a \\pi_\\mathbf{W}(a \\mid x_h) $，将随机策略转换为确定性选择。\n- 令 $ Q(h,a) = \\mathbb{E}[\\,r(h,a) \\mid h,a\\,] $ 表示在上下文 $ h $ 中执行动作 $ a $ 的期望奖励。评估：\n    - 学习到的策略的价值 $ V_{\\text{learn}} = \\frac{1}{|\\mathcal{H}|} \\sum_h Q(h, \\arg\\max_a \\pi_\\mathbf{W}(a \\mid x_h)) $。\n    - 最优策略的价值 $ V_{\\text{opt}} = \\frac{1}{|\\mathcal{H}|} \\sum_h \\max_a Q(h,a) $。\n    - 遗憾值 (regret) $ \\Delta = V_{\\text{opt}} - V_{\\text{learn}} $。\n- 将每个测试用例的遗憾值 $ \\Delta $ 报告为一个浮点数。不涉及物理单位。将每个遗憾值四舍五入到恰好六位小数。\n\n可从第一性原理获得的 $ Q(h,a) $ 的显式表达式：\n\n- 因为 $ \\widehat{\\theta}_{h,0} = \\alpha \\theta_h $ 是确定性的，\n$$\nQ(h,0) \\;=\\; -(\\alpha\\theta_h - \\theta_h)^2 \\;-\\; \\lambda c_0 \\;=\\; -(\\alpha - 1)^2\\theta_h^2 \\;-\\; \\lambda c_0.\n$$\n- 因为 $ \\widehat{\\theta}_{h,1} = \\theta_h + \\varepsilon $，其中 $ \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2) $ 且 $ \\mathbb{E}[\\varepsilon^2] = \\sigma^2 $，\n$$\nQ(h,1) \\;=\\; -\\mathbb{E}[(\\varepsilon)^2] \\;-\\; \\lambda c_1 \\;=\\; -\\sigma^2 \\;-\\; \\lambda c_1.\n$$\n\n测试套件：\n\n使用固定的假设集\n$$\n\\mathcal{H} \\;=\\; \\{\\,-2.0,\\,-1.0,\\,-0.5,\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,2.0\\,\\},\n$$\n所以 $ |\\mathcal{H}| = 8 $，特征为 $ x_h = \\begin{bmatrix}1 \\\\ \\theta_h^2\\end{bmatrix} $。对于每个测试用例，将参数 $ (\\alpha,\\sigma,\\lambda,c_0,c_1) $ 定义为：\n\n- 情况 $1$ (典型的混合机制): $ \\alpha=0.5, \\ \\sigma=0.2, \\ \\lambda=0.5, \\ c_0=1.0, \\ c_1=1.0 $。\n- 情况 $2$ (成本主导且蒙特卡洛方法昂贵): $ \\alpha=0.7, \\ \\sigma=0.1, \\ \\lambda=2.0, \\ c_0=1.0, \\ c_1=5.0 $。\n- 情况 $3$ (仅考虑准确性, $ \\lambda=0.0 $): $ \\alpha=0.6, \\ \\sigma=0.05, \\ \\lambda=0.0, \\ c_0=0.0, \\ c_1=0.0 $。\n- 情况 $4$ (中等成本下的阈值行为): $ \\alpha=0.8, \\ \\sigma=0.1, \\ \\lambda=0.3, \\ c_0=1.0, \\ c_1=1.0 $。\n- 情况 $5$ (接近 $ |\\theta|=1 $ 的边缘平局): $ \\alpha=0.75, \\ \\sigma=0.25, \\ \\lambda=0.4, \\ c_0=1.0, \\ c_1=1.0 $。\n\n实现要求：\n\n- 使用 softmax 策略和上面定义的期望奖励 $ Q(h,a) $，对 $ J(\\mathbf{W}) $ 实现梯度上升。这些定义应从期望的基本定律和 softmax 定义中推导出来。使用基于小梯度范数或最大迭代次数的停止准则来确保收敛。将 $ \\mathbf{W} $ 初始化为零。\n- 为了可复现性，训练中不需要随机性，因为 $ Q(h,a) $ 具有闭式解。在训练或评估期间，你不能对 $ \\varepsilon $ 进行抽样；仅使用 $ Q(h,a) $。\n- 对于每个测试用例，输出四舍五入到六位小数的遗憾值 $ \\Delta $。\n\n最终输出格式：\n\n你的程序应产生单行输出，其中包含案例 $1$ 到 $5$ 的遗憾值，形式为逗号分隔的列表，并用方括号括起来（例如，\"[0.000000,0.123456,0.000100,0.000000,0.000000]\"）。不应打印任何额外的文本。",
            "solution": "提出的问题是一个上下文赌博机场景，旨在模拟计算科学中的一个常见决策：计算近似的准确性与其相关成本之间的权衡。智能体必须学习一个由权重矩阵 $ \\mathbf{W} $ 参数化的策略，以根据上下文（科学假设的特征）选择一个动作（一种近似方法），从而最大化期望奖励。奖励函数通过负平方误差编码了准确性，并同时编码了计算成本。该问题是有效的、适定的，并且要求从第一性原理推导出解决方案。\n\n我们首先形式化目标，然后推导其通过梯度上升进行优化所需的梯度。\n\n假设集为 $ \\mathcal{H} $，其中每个假设 $ h \\in \\mathcal{H} $ 都与一个真实的潜值 $ \\theta_h $ 相关联。对于每个假设，智能体观察一个特征向量 $ x_h \\in \\mathbb{R}^2 $，其中 $ x_h = [1, \\theta_h^2]^\\top $。可用动作为 $ a \\in \\{0,1\\} $。在假设 $ h $ 上采取动作 $ a $ 的期望奖励，由品质函数 $ Q(h,a) $ 表示，已给出。\n对于动作 $ a=0 $ (平均场):\n$$\nQ(h,0) = -(\\alpha - 1)^2\\theta_h^2 - \\lambda c_0\n$$\n对于动作 $ a=1 $ (蒙特卡洛):\n$$\nQ(h,1) = -\\sigma^2 - \\lambda c_1\n$$\n\n智能体的策略 $ \\pi_\\mathbf{W}(a \\mid x_h) $ 是一个基于线性 logits 的 softmax 函数。logits 为 $ z_a = \\mathbf{w}_a^\\top x_h $，其中 $ \\mathbf{w}_a^\\top $ 是权重矩阵 $ \\mathbf{W} \\in \\mathbb{R}^{2 \\times 2} $ 的第 $ a $ 行。策略为：\n$$\n\\pi_\\mathbf{W}(a \\mid x_h) = \\frac{\\exp(\\mathbf{w}_a^\\top x_h)}{\\sum_{k \\in \\{0,1\\}} \\exp(\\mathbf{w}_k^\\top x_h)}\n$$\n目标是找到参数 $ \\mathbf{W} $，以最大化所有假设上的平均期望奖励：\n$$\nJ(\\mathbf{W}) = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\mathbb{E}_{a \\sim \\pi_\\mathbf{W}(\\cdot \\mid x_h)} [Q(h,a)] = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\sum_{a \\in \\{0,1\\}} \\pi_\\mathbf{W}(a \\mid x_h) Q(h,a)\n$$\n为了最大化 $ J(\\mathbf{W}) $，我们采用梯度上升法。这需要计算 $ J(\\mathbf{W}) $ 相对于参数 $ \\mathbf{W} $（即向量 $ \\mathbf{w}_0 $ 和 $ \\mathbf{w}_1 $）的梯度。由于梯度算子和求和的线性性质，我们可以专注于单个假设 $ h $ 的期望奖励的梯度，我们将其表示为 $ J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} \\pi_\\mathbf{W}(a \\mid x_h) Q(h,a) $。\n\n$ J_h(\\mathbf{W}) $ 相对于任意权重向量 $ \\mathbf{w}_k $ ($ k \\in \\{0,1\\} $) 的梯度是：\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\nabla_{\\mathbf{w}_k} \\pi_\\mathbf{W}(a \\mid x_h)\n$$\n为了计算 softmax 策略的梯度，我们使用对数导数恒等式 $ \\nabla \\pi = \\pi \\nabla \\log \\pi $。对数策略概率的梯度是：\n$$\n\\nabla_{\\mathbf{w}_k} \\log \\pi_\\mathbf{W}(a \\mid x_h) = \\nabla_{\\mathbf{w}_k} \\left( \\mathbf{w}_a^\\top x_h - \\log\\sum_{j \\in \\{0,1\\}} e^{\\mathbf{w}_j^\\top x_h} \\right)\n$$\n$$\n= (\\nabla_{\\mathbf{w}_k} \\mathbf{w}_a^\\top x_h) - \\frac{1}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} (\\nabla_{\\mathbf{w}_k} \\sum_j e^{\\mathbf{w}_j^\\top x_h})\n$$\n使用克罗内克 delta $ \\delta_{ak} $，我们有 $ \\nabla_{\\mathbf{w}_k} \\mathbf{w}_a^\\top x_h = \\delta_{ak} x_h $。第二项变为：\n$$\n\\frac{1}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} (e^{\\mathbf{w}_k^\\top x_h} \\nabla_{\\mathbf{w}_k} \\mathbf{w}_k^\\top x_h) = \\frac{e^{\\mathbf{w}_k^\\top x_h}}{\\sum_j e^{\\mathbf{w}_j^\\top x_h}} x_h = \\pi_\\mathbf{W}(k \\mid x_h) x_h\n$$\n结合这些结果，我们得到：\n$$\n\\nabla_{\\mathbf{w}_k} \\log \\pi_\\mathbf{W}(a \\mid x_h) = (\\delta_{ak} - \\pi_\\mathbf{W}(k \\mid x_h)) x_h\n$$\n将此代回 $ J_h(\\mathbf{W}) $ 的梯度中：\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) (\\delta_{ak} - \\pi_\\mathbf{W}(k \\mid x_h)) x_h\n$$\n我们可以重新排列求和：\n$$\n= \\left( Q(h,k)\\pi_\\mathbf{W}(k \\mid x_h) - \\pi_\\mathbf{W}(k \\mid x_h) \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) \\right) x_h\n$$\n令 $ V_h(\\mathbf{W}) = \\sum_{a \\in \\{0,1\\}} Q(h,a) \\pi_\\mathbf{W}(a \\mid x_h) $ 为在策略 $ \\pi_\\mathbf{W} $ 下假设 $ h $ 的期望价值。该表达式简化为带有基线的标准策略梯度形式：\n$$\n\\nabla_{\\mathbf{w}_k} J_h(\\mathbf{W}) = \\pi_\\mathbf{W}(k \\mid x_h) (Q(h,k) - V_h(\\mathbf{W})) x_h\n$$\n这是一个通用且基本的结果。对于我们的双动作情况，我们可以进一步简化基线项 $ Q(h,k) - V_h(\\mathbf{W}) $：\n对于 $ k=0 $: $ Q(h,0) - (\\pi_0 Q_0 + \\pi_1 Q_1) = (1-\\pi_0)Q_0 - \\pi_1 Q_1 = \\pi_1 Q_0 - \\pi_1 Q_1 = \\pi_1(Q_0 - Q_1) $。\n对于 $ k=1 $: $ Q(h,1) - (\\pi_0 Q_0 + \\pi_1 Q_1) = Q_1 - \\pi_0 Q_0 - (1-\\pi_0)Q_1 = \\pi_0 Q_1 - \\pi_0 Q_0 = \\pi_0(Q_1 - Q_0) $。\n对于单个假设 $ h $，相对于 $ \\mathbf{w}_0 $ 和 $ \\mathbf{w}_1 $ 的梯度是：\n$$\n\\nabla_{\\mathbf{w}_0} J_h(\\mathbf{W}) = \\pi_0 (\\pi_1(Q_0 - Q_1)) x_h = \\pi_0\\pi_1(Q(h,0) - Q(h,1))x_h\n$$\n$$\n\\nabla_{\\mathbf{w}_1} J_h(\\mathbf{W}) = \\pi_1 (\\pi_0(Q_1 - Q_0)) x_h = \\pi_0\\pi_1(Q(h,1) - Q(h,0))x_h\n$$\n注意 $ \\nabla_{\\mathbf{w}_0} J_h(\\mathbf{W}) = -\\nabla_{\\mathbf{w}_1} J_h(\\mathbf{W}) $，这反映了策略由 logits 的差异 $ z_1 - z_0 = (\\mathbf{w}_1 - \\mathbf{w}_0)^\\top x_h $ 决定的事实。\n\n目标 $ J(\\mathbf{W}) $ 的总梯度是这些单个假设梯度在集合 $ \\mathcal{H} $ 上的平均值：\n$$\n\\nabla_\\mathbf{W} J(\\mathbf{W}) = \\frac{1}{|\\mathcal{H}|} \\sum_{h \\in \\mathcal{H}} \\nabla_\\mathbf{W} J_h(\\mathbf{W})\n$$\n学习算法则是一个简单的梯度上升过程。从 $ \\mathbf{W} $ 为零矩阵开始，我们使用学习率 $ \\eta $ 迭代更新权重：\n$$\n\\mathbf{W}_{t+1} = \\mathbf{W}_t + \\eta \\nabla_\\mathbf{W} J(\\mathbf{W}_t)\n$$\n此过程持续固定数量的迭代，或直到梯度的范数低于某个容忍度，以确保收敛到目标函数的局部最大值。\n\n训练后，计算遗憾值 $ \\Delta $。最优策略的价值 $ V_{\\text{opt}} $ 是通过为每个假设采取最优动作来确定的：$ V_{\\text{opt}} = \\frac{1}{|\\mathcal{H}|} \\sum_h \\max_{a} Q(h,a) $。学习到的策略的价值 $ V_{\\text{learn}} $ 是通过首先将随机的 softmax 策略转换为确定性策略（选择概率最高的动作 $ a^*_h = \\arg\\max_a \\pi_{\\mathbf{W}}(a \\mid x_h) $），然后计算平均奖励来找到的：$ V_{\\text{learn}} = \\frac{1}{|\\mathcal{H}|} \\sum_h Q(h, a^*_h) $。遗憾值是差值 $ \\Delta = V_{\\text{opt}} - V_{\\text{learn}} $。非零的遗憾值表明，对于至少一个假设，学习到的确定性策略没有选择能够产生最大期望奖励的动作。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the contextual bandit problem for a series of test cases using\n    gradient ascent on the expected reward.\n    \"\"\"\n    \n    # Fixed hypothesis set\n    thetas = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0])\n    H_size = len(thetas)\n    # Feature vectors x_h = [1, theta_h^2]^T\n    features = np.vstack((np.ones(H_size), thetas**2)).T\n\n    # Test cases: (alpha, sigma, lambda, c0, c1)\n    test_cases = [\n        (0.5, 0.2, 0.5, 1.0, 1.0),\n        (0.7, 0.1, 2.0, 1.0, 5.0),\n        (0.6, 0.05, 0.0, 0.0, 0.0),\n        (0.8, 0.1, 0.3, 1.0, 1.0),\n        (0.75, 0.25, 0.4, 1.0, 1.0)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        alpha, sigma, lam, c0, c1 = case\n        \n        # --- Pre-calculate Q-values for all hypotheses ---\n        # Q_values matrix of shape (H_size, 2)\n        Q_values = np.zeros((H_size, 2))\n        \n        # Q(h, 0) for all h\n        Q_values[:, 0] = -((alpha - 1)**2) * (thetas**2) - lam * c0\n        # Q(h, 1) for all h\n        Q_values[:, 1] = -(sigma**2) - lam * c1\n        \n        # --- Gradient Ascent Training ---\n        W = np.zeros((2, 2))  # Policy weights: W[0] for w0, W[1] for w1\n        learning_rate = 0.1\n        num_iterations = 20000\n\n        for _ in range(num_iterations):\n            grad_W = np.zeros((2, 2))\n            \n            # Compute logits for all hypotheses at once\n            # logits shape: (H_size, 2)\n            logits = features @ W.T\n            \n            # Stable softmax\n            logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n            exps = np.exp(logits_stable)\n            probs = exps / np.sum(exps, axis=1, keepdims=True) # shape (H_size, 2)\n            \n            # Calculate gradient contributions\n            # pi_0 * pi_1 * (Q0 - Q1) for each hypothesis\n            factor = probs[:, 0] * probs[:, 1] * (Q_values[:, 0] - Q_values[:, 1]) # shape (H_size,)\n\n            # Reshape factor to (H_size, 1) to broadcast with features (H_size, 2)\n            factor = factor[:, np.newaxis]\n            \n            # Gradient for w0 for all h\n            grad_w0 = np.sum(factor * features, axis=0)\n            # Gradient for w1 is the negative of grad_w0\n            grad_w1 = -grad_w0\n            \n            # Average gradient over all hypotheses\n            grad_W[0, :] = grad_w0 / H_size\n            grad_W[1, :] = grad_w1 / H_size\n\n            # Update weights\n            W += learning_rate * grad_W\n\n        # --- Evaluation ---\n        # Optimal policy value\n        V_opt_sum = np.sum(np.max(Q_values, axis=1))\n        V_opt = V_opt_sum / H_size\n        \n        # Learned policy value\n        # Recalculate final probabilities with trained W\n        final_logits = features @ W.T\n        final_logits_stable = final_logits - np.max(final_logits, axis=1, keepdims=True)\n        final_exps = np.exp(final_logits_stable)\n        final_probs = final_exps / np.sum(final_exps, axis=1, keepdims=True)\n        \n        # Deterministic learned actions\n        learned_actions = np.argmax(final_probs, axis=1)\n        \n        # Calculate value of learned policy\n        V_learn_sum = np.sum(Q_values[np.arange(H_size), learned_actions])\n        V_learn = V_learn_sum / H_size\n        \n        # Regret\n        regret = V_opt - V_learn\n        \n        # Format and append result\n        results.append(f\"{regret:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "强化学习不仅能优化分析，还能指导数据收集过程，尤其是在公民科学等互动式场景中。这个练习  将挑战你实现一个上下文赌博机（contextual bandit）智能体，它能学习向合适的参与者提出恰当的问题。通过有效地平衡“探索”与“利用”，你的智能体将最大化从参与者贡献中获得的科学价值。",
            "id": "3186203",
            "problem": "您的任务是设计并评估一个强化学习（RL）策略，该策略在一个公民科学平台上按顺序选择向参与者提出的下一个问题。每个问题都属于一个离散的问题类型集合，目标是最大化有助于科学发现的累积分类改进。将此交互建模为一个源于单步马尔可夫决策过程（MDP）近似的情境赌博机，其中即时奖励捕获了由参与者回答带来的分类增益。必须使用以下定义和约束来构建一个科学上合理且自洽的模拟测试平台：\n\n- 强化学习（RL）在离散时间步 $t \\in \\{1,2,\\dots,T\\}$ 上运行，动作集为 $\\mathcal{A} = \\{1,2,\\dots,K\\}$，代表不同的问题类型。环境在每一步独立采样提供一个情境向量 $c_t \\in \\mathbb{R}^d$，并维持一个标量分类准确率 $a_t \\in [0,1]$，该准确率根据参与者的贡献而演变。\n- 在每一步，智能体观察到 $c_t$，选择一个动作 $i \\in \\mathcal{A}$，并获得一个奖励 $r_t \\in \\mathbb{R}_{\\ge 0}$，该奖励代表分类准确率的增量增益。定义松弛量 $s_t = 1 - a_t$ 来捕获收益递减效应，并通过拼接一个偏置项、情境和松弛量来定义增强特征向量 $x_t \\in \\mathbb{R}^{d+2}$：\n$$\nx_t = \\begin{bmatrix} 1 \\\\ c_t \\\\ s_t \\end{bmatrix}.\n$$\n- 动作 $i$ 的真实期望奖励是特征的线性函数，其参数 $w^{(i)} \\in \\mathbb{R}^{d+2}$ 未知，该函数受到噪声干扰，并被截断以反映非负增益和收益递减。具体来说，观察到的奖励是：\n$$\nr_t = \\min\\left\\{ s_t,\\ \\max\\left\\{ 0,\\ x_t^\\top w^{(i)} + \\epsilon_t \\right\\}\\right\\},\n$$\n其中 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立高斯噪声。分类准确率随之更新为：\n$$\na_{t+1} = \\min\\{1,\\ a_t + r_t\\}.\n$$\n- 您的强化学习策略必须仅基于过去观察到的情境和奖励来选择动作，并以一种有原则的方式权衡探索与利用。本问题期望一个情境化策略，该策略利用观察到的 $x_t$ 来推断哪个动作将最大化期望累积奖励。\n\n基本依据与假设：\n- 将交互视为一个从单步MDP简化得到的情境赌博机，其原理是即时奖励封装了参与者贡献的价值。\n- 使用经过充分检验的统计建模假设：独立高斯噪声、期望奖励与特征呈线性关系，以及通过松弛量 $s_t$ 实现的收益递减。\n- 在选择动作时，不假设对真实参数 $w^{(i)}$ 有任何先验知识；智能体必须从数据中学习。\n\n您的程序必须实现上述环境和一个能学习选择动作的强化学习策略。它必须使用指定的参数运行以下测试套件，并生成所需的输出格式。\n\n测试套件规范：\n- 所有测试的通用情境分布：$c_t \\sim \\mathcal{N}(\\mu, \\Sigma)$，其中 $\\mu = [0, 0]$ 且 $\\Sigma = I_2$，$I_2$ 是 $2 \\times 2$ 单位矩阵。因此 $d = 2$ 且 $x_t \\in \\mathbb{R}^{4}$。\n- 对于所有测试，特征排序为 $x_t = [1,\\ c_{t,1},\\ c_{t,2},\\ s_t]^\\top$。\n\n测试用例 1（正常路径）：\n- 动作数量 $K = 3$。\n- 决策时域 $T = 200$。\n- 初始准确率 $a_0 = 0.5$。\n- 噪声标准差 $\\sigma = 0.05$。\n- 动作的真实参数：\n$$\nw^{(1)} = [0.05,\\ 0.8,\\ 0.0,\\ 0.5],\\quad\nw^{(2)} = [0.05,\\ -0.2,\\ 0.9,\\ 0.3],\\quad\nw^{(3)} = [0.05,\\ 0.4,\\ 0.4,\\ 0.7].\n$$\n\n测试用例 2（边界条件：接近饱和的准确率）：\n- 动作数量 $K = 2$。\n- 决策时域 $T = 100$。\n- 初始准确率 $a_0 = 0.95$。\n- 噪声标准差 $\\sigma = 0.05$。\n- 动作的真实参数：\n$$\nw^{(1)} = [0.02,\\ 0.5,\\ 0.5,\\ 0.2],\\quad\nw^{(2)} = [0.02,\\ 0.6,\\ -0.1,\\ 0.2].\n$$\n\n测试用例 3（边缘案例：高噪声环境）：\n- 动作数量 $K = 3$。\n- 决策时域 $T = 300$。\n- 初始准确率 $a_0 = 0.4$。\n- 噪声标准差 $\\sigma = 0.3$。\n- 动作的真实参数：\n$$\nw^{(1)} = [0.10,\\ 0.3,\\ 0.3,\\ 0.4],\\quad\nw^{(2)} = [0.10,\\ 0.35,\\ 0.25,\\ 0.35],\\quad\nw^{(3)} = [0.10,\\ 0.2,\\ 0.5,\\ 0.3].\n$$\n\n输出要求：\n- 对于每个测试用例，计算两个量：最终准确率 $a_T$ 和累积奖励 $R = \\sum_{t=1}^T r_t$。\n- 您的程序必须生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，每个测试用例表示为一个双元素列表。精确格式为：\n$$\n\\text{[}[a_T^{(1)}, R^{(1)}],[a_T^{(2)}, R^{(2)}],[a_T^{(3)}, R^{(3)}]\\text{]},\n$$\n- 其中所有数字必须打印为保留六位小数的小数，并且该行中不允许出现任何空格。\n- 在内部设置一个固定的随机种子以确保可复现性。不要读取任何输入，也不要写入任何文件。\n\n科学真实性与普适性：\n- 该设置期望智能体通过选择问题来提高准确率，其增益反映了对参与者情境的统计依赖性以及随着模型饱和而出现的收益递减。\n- 所有随机变量和参数都有明确定义，并且与计算科学中的标准统计建模一致。",
            "solution": "问题陈述已经过验证，被认为是合理的。它具有科学依据，定义明确且客观。为实现可复现的模拟提供了所有必要的参数和定义。该问题要求设计一个强化学习（RL）策略，来解决一个为模拟公民科学场景而构建的情境赌博机问题。以下解决方案提供了一种有原则的算法方法及其实施。\n\n该问题是线性情境赌博机的一个实例。在每个离散时间步 $t \\in \\{1, 2, \\dots, T\\}$，智能体必须从一个包含 $K$ 个可用动作的集合 $\\mathcal{A} = \\{1, 2, \\dots, K\\}$ 中选择一个动作 $i$。该决策由一个情境向量 $c_t \\in \\mathbb{R}^d$ 提供信息。问题将增强特征向量 $x_t \\in \\mathbb{R}^{d+2}$ 定义为偏置项、情境以及一个依赖于状态的松弛项 $s_t = 1 - a_t$ 的拼接，其中 $a_t \\in [0, 1]$ 是当前的分类准确率。\n$$\nx_t = \\begin{bmatrix} 1 \\\\ c_t \\\\ s_t \\end{bmatrix}\n$$\n执行动作 $i$ 的期望奖励与此特征向量呈线性关系，由一个未知的权重向量 $w^{(i)} \\in \\mathbb{R}^{d+2}$ 参数化。观测到的奖励 $r_t$ 是这个线性函数的带噪实现，并被截断以确保非负性且准确率增益不超过可用的松弛量 $s_t$。\n$$\nr_t = \\min\\left\\{ s_t,\\ \\max\\left\\{ 0,\\ x_t^\\top w^{(i)} + \\epsilon_t \\right\\}\\right\\}, \\quad \\text{其中 } \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\n$$\n系统状态，由准确率 $a_t$ 表示，根据获得的奖励演变如下：\n$$\na_{t+1} = \\min\\{1,\\ a_t + r_t\\}\n$$\n智能体不知道真实的参数 $w^{(i)}$，必须从对情境、动作和奖励的观察中学习它们，以最大化累积奖励 $R = \\sum_{t=1}^T r_t$。这需要一种能够平衡探索（尝试不同动作以学习其参数）和利用（选择被认为是最佳的动作）的策略。\n\n针对此类问题，一个高效且有理论基础的算法是线性置信上界（LinUCB）算法。LinUCB将每个动作 $i$ 的期望奖励建模为情境的线性函数，即 $\\mathbb{E}[r_t | x_t, i] \\approx x_t^\\top w^{(i)}$，并使用在线岭回归来估计未知的权重向量 $\\hat{w}^{(i)}$。\n\n对于每个动作 $i \\in \\mathcal{A}$，该算法维护一个设计矩阵 $A_i \\in \\mathbb{R}^{(d+2) \\times (d+2)}$ 和一个奖励向量 $b_i \\in \\mathbb{R}^{d+2}$。它们被初始化为 $A_i = I_{d+2}$（单位矩阵，对应于正则化先验）和 $b_i = \\mathbf{0}$。在观察到所选动作 $i_t$ 的特征-奖励对 $(x_t, r_t)$ 后，统计数据更新如下：\n$$\nA_{i_t} \\leftarrow A_{i_t} + x_t x_t^\\top\n$$\n$$\nb_{i_t} \\leftarrow b_{i_t} + r_t x_t\n$$\n在每个时间步 $t$，智能体为每个动作 $i$ 计算权重向量的估计值：\n$$\n\\hat{w}^{(i)} = A_i^{-1} b_i\n$$\nLinUCB的核心在于其动作选择策略。它不是贪婪地选择具有最高预测奖励 $x_t^\\top \\hat{w}^{(i)}$ 的动作，而是增加了一个探索奖励。这个奖励与当前情境 $x_t$ 的估计不确定性成正比。动作 $i_t$ 根据以下公式选择：\n$$\ni_t = \\arg\\max_{i \\in \\mathcal{A}} \\left( x_t^\\top \\hat{w}^{(i)} + \\alpha \\sqrt{x_t^\\top A_i^{-1} x_t} \\right)\n$$\n项 $x_t^\\top \\hat{w}^{(i)}$ 是预测奖励（利用），而项 $\\alpha \\sqrt{x_t^\\top A_i^{-1} x_t}$ 是该估计的置信上界（探索）。超参数 $\\alpha \\ge 0$ 控制着这种权衡；对于本实现，选择了一个标准值 $\\alpha = 1.0$。\n\n每个测试用例的模拟过程如下：\n$1$. 为保证可复现性，设置一个固定的随机种子。\n$2$. 初始化模拟参数：决策时域 $T$、动作数量 $K$、初始准确率 $a_0$、噪声 $\\sigma$ 以及真实权重 $w^{(i)}$。\n$3$. 初始化LinUCB智能体，其有 $K$ 个臂，特征维度为 $d+2=4$。\n$4$. 初始化当前准确率 $a_t = a_0$ 和累积奖励 $R = 0$。\n$5$. 对于从1到 $T$ 的每个时间步 $t$：\n    a. 计算当前松弛量 $s_t = 1 - a_t$。\n    b. 采样一个情境向量 $c_t \\sim \\mathcal{N}(\\mu, \\Sigma)$，其中 $\\mu=[0,0]$ 且 $\\Sigma=I_2$。\n    c. 构建增强特征向量 $x_t = [1, c_{t,1}, c_{t,2}, s_t]^\\top$。\n    d. LinUCB智能体使用UCB准则选择一个动作 $i_t$。\n    e. 环境基于真实权重 $w^{(i_t)}$、特征向量 $x_t$、一个随机噪声样本 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 生成奖励 $r_t$，并应用指定的截断。\n    f. 更新分类准确率 $a_{t+1} = \\min\\{1, a_t + r_t\\}$ 和累积奖励 $R = R + r_t$。\n    g. 用观测值 $(x_t, i_t, r_t)$ 更新智能体的内部模型。\n$6$. 在 $T$ 步之后，记录最终准确率 $a_T$ 和总累积奖励 $R$。对问题陈述中指定的所有三个测试用例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    test_cases = [\n        {\n            \"K\": 3, \"T\": 200, \"a0\": 0.5, \"sigma\": 0.05,\n            \"w\": np.array([\n                [0.05, 0.8, 0.0, 0.5],\n                [0.05, -0.2, 0.9, 0.3],\n                [0.05, 0.4, 0.4, 0.7]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        },\n        {\n            \"K\": 2, \"T\": 100, \"a0\": 0.95, \"sigma\": 0.05,\n            \"w\": np.array([\n                [0.02, 0.5, 0.5, 0.2],\n                [0.02, 0.6, -0.1, 0.2]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        },\n        {\n            \"K\": 3, \"T\": 300, \"a0\": 0.4, \"sigma\": 0.3,\n            \"w\": np.array([\n                [0.10, 0.3, 0.3, 0.4],\n                [0.10, 0.35, 0.25, 0.35],\n                [0.10, 0.2, 0.5, 0.3]\n            ]),\n            \"mu\": np.array([0.0, 0.0]), \"d\": 2\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        final_a, total_r = run_simulation(params)\n        all_results.append(f\"[{final_a:.6f},{total_r:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nclass LinUCBAgent:\n    \"\"\"\n    Implements the Linear Upper Confidence Bound (LinUCB) algorithm.\n    \"\"\"\n    def __init__(self, K, d, alpha=1.0):\n        self.K = K\n        self.dim = d + 2  # bias, context_dims, slack\n        self.alpha = alpha\n        # Initialize A as identity matrix and b as zero vector for each arm\n        self.A = [np.identity(self.dim) for _ in range(K)]\n        self.b = [np.zeros(self.dim) for _ in range(K)]\n\n    def select_action(self, x_t):\n        \"\"\"\n        Selects an action based on the UCB criterion for the given context.\n        \"\"\"\n        scores = []\n        for i in range(self.K):\n            A_inv = np.linalg.inv(self.A[i])\n            w_hat = A_inv @ self.b[i]\n            p_t_i = x_t.T @ w_hat\n            ucb = self.alpha * np.sqrt(x_t.T @ A_inv @ x_t)\n            scores.append(p_t_i + ucb)\n        \n        return np.argmax(scores)\n\n    def update(self, action_idx, x_t, r_t):\n        \"\"\"\n        Updates the model for the chosen action with the observed reward.\n        \"\"\"\n        self.A[action_idx] += np.outer(x_t, x_t)\n        self.b[action_idx] += r_t * x_t\n\ndef run_simulation(params):\n    \"\"\"\n    Runs a single simulation episode for a given set of parameters.\n    \"\"\"\n    K = params[\"K\"]\n    T = params[\"T\"]\n    a_t = params[\"a0\"]\n    sigma = params[\"sigma\"]\n    true_w = params[\"w\"]\n    mu = params[\"mu\"]\n    d = params[\"d\"]\n    \n    agent = LinUCBAgent(K, d, alpha=1.0)\n    \n    cumulative_reward = 0.0\n    context_cov = np.identity(d)\n\n    for _ in range(T):\n        s_t = 1.0 - a_t\n        \n        # Sample context and form feature vector\n        c_t = np.random.multivariate_normal(mu, context_cov)\n        x_t = np.concatenate(([1.0], c_t, [s_t]))\n\n        # Agent selects an action\n        action_idx = agent.select_action(x_t)\n\n        # Environment generates reward\n        noise = np.random.normal(0, sigma)\n        raw_reward = x_t.T @ true_w[action_idx] + noise\n        r_t = np.min([s_t, np.max([0, raw_reward])])\n\n        # Update system state\n        a_t = np.min([1.0, a_t + r_t])\n        cumulative_reward += r_t\n\n        # Agent updates its model\n        agent.update(action_idx, x_t, r_t)\n\n    return a_t, cumulative_reward\n\nsolve()\n```"
        },
        {
            "introduction": "科学发现也包含战略层面的决策，例如决定继续或终止哪些研究方向。这个练习  模拟了这种高层次的决策过程，要求你构建一个 Q-learning 智能体。你的智能体将学会一个“淘汰”假说的策略，从而将有限的研究资源最优化地分配给最有前景的研究途径，这体现了强化学习在科研组合管理中的强大潜力。",
            "id": "3186163",
            "problem": "给你一个用于强化学习（RL）的正式决策场景，其背景是选择摒弃哪些科学假说，以最大化剩余假说的信息期望值。该环境被建模为一个马尔可夫决策过程（MDP），其中每个状态编码了仍在考虑中的假说的子集。动作是准确摒弃一个当前活跃的假说。每一步的奖励基于动作执行后剩余假说的信息期望值（EVI），并通过一个简单的资源共享模型进行缩放。\n\n基本基础：\n- 马尔可夫决策过程（MDP）：一个元组 $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$，其中 $\\mathcal{S}$ 是状态集，$\\mathcal{A}$ 是动作集，$P$ 是转移概率，$r$ 是奖励函数，$\\gamma$ 是折扣因子。\n- 期望值与信息：对于一个先验概率为 $p \\in (0,1)$ 的二元假说，其信息内容通过香农熵（以自然单位奈特 (nats) 衡量）计算为 $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$。\n- 资源共享：一个固定的预算 $B$ 在所有当前活跃的假说中平均分配。如果剩下 $n$ 个假说，每个假说获得 $B/n$ 的资源份额。\n\n模型设定：\n- 设有 $N$ 个假说。每个假说 $i \\in \\{0,1,\\dots,N-1\\}$ 具有先验概率 $p_i \\in (0,1)$ 和信息权重 $w_i > 0$，代表学习关于假说 $i$ 的决策理论价值。\n- 状态 $s$ 是当前活跃假说索引的任意子集。一个动作 $a \\in s$ 会摒弃假说 $a$，得到新状态 $s' = s \\setminus \\{a\\}$。\n- 在状态 $s$ 下采取动作 $a$ 的瞬时奖励定义为\n$$\nr(s,a) = \\frac{B}{|s'|} \\sum_{j \\in s'} w_j H(p_j),\n$$\n其中 $H(p_j) = -p_j \\ln(p_j) - (1-p_j)\\ln(1-p_j)$ 且 $|s'|$ 是动作执行后剩余假说的数量。对数是自然对数；信息单位是奈特 (nats)。\n- 当执行了恰好 $R_{\\max}$ 次摒弃操作后，一个回合（episode）终止。我们要求 $R_{\\max} \\le N-1$，以确保终止时至少有一个假说保留。如果 $R_{\\max} = 0$，则不采取任何动作，基线奖励定义为\n$$\nr_{\\mathrm{baseline}}(s) = \\frac{B}{|s|} \\sum_{j \\in s} w_j H(p_j).\n$$\n\n任务：\n- 实现 Q-learning 算法来学习摒弃决策的策略。使用 $\\varepsilon$-贪心探索策略和学习率 $\\alpha$。状态可以用一个 $N$ 个假说的位掩码（bitmask）来表示。在给定状态下，只允许采取与当前活跃假说相对应的动作。Q-learning 的更新规则是\n$$\nQ(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q(s',a') \\right),\n$$\n其中 $s'$ 是在状态 $s$ 下采取动作 $a$ 后的下一个状态。\n- 训练后，评估贪心策略（在允许的动作中选择具有最大 $Q$ 值的动作），从完整的假说集合开始，直到执行了 $R_{\\max}$ 次摒弃。对于每个测试用例，报告被摒弃的假说索引列表（使用从零开始的索引）和评估期间获得的累积非折扣奖励总和。对于 $R_{\\max} = 0$ 的情况，报告一个空的摒弃列表和上面定义的基线奖励。\n- 对数底必须是自然对数。\n\n测试套件：\n为以下参数集提供结果。在每个案例中，使用给定的参数 $(N, p, w, B, R_{\\max}, \\gamma, \\text{episodes}, \\varepsilon_{\\text{init}}, \\varepsilon_{\\text{final}}, \\alpha)$，所有向量均按照以 $i \\in \\{0,1,\\dots,N-1\\}$ 索引的假说顺序给出。\n\n- 案例1（一般情况）：\n  - $N = 4$\n  - $p = [0.10, 0.70, 0.50, 0.05]$\n  - $w = [1.00, 0.90, 0.60, 0.20]$\n  - $B = 1.00$\n  - $R_{\\max} = 2$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 12000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- 案例2（边缘概率，接近 $0$ 或 $1$ 但严格在 $(0,1)$ 区间内）：\n  - $N = 5$\n  - $p = [0.01, 0.85, 0.40, 0.95, 0.20]$\n  - $w = [0.50, 1.20, 0.80, 0.10, 0.70]$\n  - $B = 1.00$\n  - $R_{\\max} = 2$\n  - $\\gamma = 0.90$\n  - $\\text{episodes} = 15000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- 案例3（平局：相同的先验概率和权重）：\n  - $N = 4$\n  - $p = [0.50, 0.50, 0.50, 0.50]$\n  - $w = [1.00, 1.00, 1.00, 1.00]$\n  - $B = 1.00$\n  - $R_{\\max} = 3$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 10000$\n  - $\\varepsilon_{\\text{init}} = 0.30$\n  - $\\varepsilon_{\\text{final}} = 0.02$\n  - $\\alpha = 0.30$\n\n- 案例4（没有摒弃预算）：\n  - $N = 3$\n  - $p = [0.40, 0.60, 0.55]$\n  - $w = [0.90, 0.80, 0.70]$\n  - $B = 1.00$\n  - $R_{\\max} = 0$\n  - $\\gamma = 0.95$\n  - $\\text{episodes} = 1$\n  - $\\varepsilon_{\\text{init}} = 0.00$\n  - $\\varepsilon_{\\text{final}} = 0.00$\n  - $\\alpha = 0.30$\n\n最终输出格式：\n你的程序应该生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，不含空格。每个案例的结果本身是一个包含两个元素的列表：摒弃序列（一个从零开始的索引列表）和累积非折扣奖励（一个浮点数）。例如，两个案例的输出将如下所示：$[[[0,2],1.234],[[],0.567]]$。",
            "solution": "我们将该决策问题建模为马尔可夫决策过程（MDP），并使用强化学习（RL）中的 Q-learning 方法来求解。目标是选择要摒弃的假说，从而在一个固定的摒弃期限内，根据一个简单的资源共享模型，最大化剩余假说的信息期望值（EVI）。\n\n原理与定义：\n- 强化学习（RL）在一个由 MDP $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$ 建模的环境中优化决策制定，其中 $\\gamma \\in [0,1)$ 是折扣因子。智能体选择动作以最大化期望的折扣累积奖励。\n- 信息期望值，基于决策论和信息论：对于一个先验概率为 $p \\in (0,1)$ 的二元假说，香农熵 $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$（使用自然对数）衡量不确定性。如果我们用 $w > 0$ 来代表学习的效用，并假设资源在活跃假说中平均共享，则瞬时奖励与剩余假说的 $w H(p)$ 之和成正比，并按资源份额进行缩放。\n\n环境构建：\n- 设有 $N$ 个假说，其先验概率为 $p_i \\in (0,1)$，权重为 $w_i > 0$。一个状态 $s \\subseteq \\{0,1,\\dots,N-1\\}$ 代表活跃的假说。一个动作 $a \\in s$ 会摒弃假说 $a$，得到 $s' = s \\setminus \\{a\\}$。\n- 如果 $n' = |s'|$，那么每个剩余假说的资源份额是 $B/n'$。瞬时奖励是\n$$\nr(s,a) = \\frac{B}{n'} \\sum_{j \\in s'} w_j H(p_j), \\quad H(p_j) = -p_j \\ln(p_j) - (1-p_j)\\ln(1-p_j).\n$$\n这是期望值定义和香农熵作为一个经过充分检验的信息度量标准的直接应用。\n- 当完成 $R_{\\max}$ 次摒弃操作后，一个回合（episode）终止。这构成了一个具有离散和确定性转移动态的有限期决策问题。\n\n算法推导：\n- Q-learning 算法逼近满足贝尔曼最优方程的最优动作-价值函数 $Q^*(s,a)$\n$$\nQ^*(s,a) = r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^*(s',a'),\n$$\n其中 $s'$ 是在状态 $s$ 下采取动作 $a$ 后的状态。\n- 学习更新规则是\n$$\nQ(s,a) \\leftarrow (1-\\alpha) Q(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q(s',a') \\right),\n$$\n其中学习率为 $\\alpha \\in (0,1]$。通过 $\\varepsilon$-贪心策略实现探索：以概率 $\\varepsilon$ 选择一个随机的允许动作；否则选择使 $Q(s,a)$ 最大化的动作。\n- 状态由长度为 $N$ 的位掩码（bitmasks）表示。在状态 $s$ 中，允许的动作是使得位掩码在位置 $i$ 处为 1 的索引 $i$。一个回合从完整掩码（所有假说均活跃）开始，并在 $R_{\\max}$ 次摒弃后结束。对于 $R_{\\max} = 0$ 的情况，不采取任何动作，基线奖励为\n$$\nr_{\\mathrm{baseline}}(s) = \\frac{B}{|s|} \\sum_{j \\in s} w_j H(p_j).\n$$\n\n科学依据：\n- 奖励设计遵循期望值是所有结果乘以其价值的总和这一原则；使用 $H(p)$ 作为不确定性下的期望信息，权重 $w$ 作为学习某个假说的效用缩放。这结合了基本的决策理论结构和经过充分检验的香农熵。\n- 摒弃一个假说会释放其资源份额，以便在剩余假说中重新平均分配，从而提高每个假说的期望信息捕获率，同时减少贡献于总和的假说数量。智能体必须在移除低信息量假说与保持足够广度之间进行权衡，这是科学项目组合管理中一个现实的取舍。\n\n实现细节：\n- 对于每个测试用例，Q-learning 智能体将按指定的回合数进行训练，探索参数 $\\varepsilon$ 从 $\\varepsilon_{\\text{init}}$ 线性退火到 $\\varepsilon_{\\text{final}}$。训练后，执行贪心策略以生成摒弃序列和累积非折扣奖励。\n- 假说使用从零开始的索引进行报告。输出是一个单行字符串，包含一个各案例结果的列表，其中每个结果本身是一个列表 $[\\text{序列}, \\text{奖励}]$。\n\n边缘案例覆盖：\n- 案例 2 使用非常接近 $0$ 或 $1$（但严格在内部）的 $p$ 值，以测试 $H(p)$ 的数值稳定性；实现中将 $p$ 夹紧在远离 $0$ 和 $1$ 的机器安全范围内以避免 $\\ln(0)$，这与定义域要求 $p \\in (0,1)$ 一致。\n- 案例 3 通过相同的 $p$ 和 $w$ 值制造了平局情况，表明学习过程中的任何对称性破缺仍然能产生一致的序列。\n- 案例 4 设置 $R_{\\max} = 0$，确保实现能正确返回无摒弃操作和定义的基线奖励。\n\n最终输出：\n- 程序精确地打印一行：一个用方括号括起来的、无空格的逗号分隔列表。每个元素是一个以列表形式表示的对：摒弃序列（整数列表）和累积非折扣奖励（浮点数），遵循指定格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef shannon_entropy_nats(p: float) -> float:\n    \"\"\"Compute Shannon entropy H(p) in nats for a binary variable.\"\"\"\n    # Clamp to avoid log(0); the problem states p in (0,1), but clamp for numerical safety.\n    eps = 1e-12\n    p = float(np.clip(p, eps, 1.0 - eps))\n    return -(p * np.log(p) + (1.0 - p) * np.log(1.0 - p))\n\nclass HypothesisEnv:\n    \"\"\"Finite deterministic environment for retiring hypotheses.\"\"\"\n    def __init__(self, p_vec, w_vec, B, Rmax):\n        self.p = np.array(p_vec, dtype=float)\n        self.w = np.array(w_vec, dtype=float)\n        self.B = float(B)\n        self.N = len(self.p)\n        self.Rmax = int(Rmax)\n        # Precompute entropies\n        self.H = np.array([shannon_entropy_nats(pi) for pi in self.p], dtype=float)\n        # Full state bitmask: all ones for N hypotheses\n        self.full_state = (1  self.N) - 1\n\n    def remaining_indices(self, state_mask: int):\n        return [i for i in range(self.N) if (state_mask >> i)  1]\n\n    def count_remaining(self, state_mask: int) - int:\n        return int(np.sum([(state_mask >> i)  1 for i in range(self.N)]))\n\n    def retired_count(self, state_mask: int) - int:\n        return self.N - self.count_remaining(state_mask)\n\n    def is_terminal(self, state_mask: int) - bool:\n        return self.retired_count(state_mask) >= self.Rmax\n\n    def allowed_actions(self, state_mask: int):\n        if self.is_terminal(state_mask):\n            return []\n        return self.remaining_indices(state_mask)\n\n    def step(self, state_mask: int, action: int):\n        \"\"\"Take action (retire hypothesis 'action'), return (next_state, reward).\"\"\"\n        if ((state_mask >> action)  1) == 0:\n            raise ValueError(\"Action must retire an active hypothesis.\")\n        next_state = state_mask  ~(1  action)\n        rem = self.remaining_indices(next_state)\n        n_rem = len(rem)\n        if n_rem == 0:\n            # Should not occur because Rmax = N-1 per problem spec\n            reward = 0.0\n        else:\n            share = self.B / n_rem\n            reward = share * float(np.sum(self.w[rem] * self.H[rem]))\n        return next_state, reward\n\n    def baseline_reward(self, state_mask: int) - float:\n        \"\"\"Reward with no retirement: resource equally split among current hypotheses.\"\"\"\n        rem = self.remaining_indices(state_mask)\n        n_rem = len(rem)\n        if n_rem == 0:\n            return 0.0\n        share = self.B / n_rem\n        return share * float(np.sum(self.w[rem] * self.H[rem]))\n\ndef q_learning(env: HypothesisEnv, gamma: float, episodes: int,\n               eps_init: float, eps_final: float, alpha: float, rng: np.random.Generator):\n    \"\"\"Train Q-learning on the environment.\"\"\"\n    num_states = 1  env.N\n    Q = np.zeros((num_states, env.N), dtype=float)\n\n    def epsilon_for_episode(ep):\n        # Linear annealing from eps_init to eps_final\n        if episodes == 1:\n            return eps_final\n        return eps_init + (eps_final - eps_init) * (ep / (episodes - 1))\n\n    for ep in range(episodes):\n        state = env.full_state\n        while not env.is_terminal(state):\n            actions = env.allowed_actions(state)\n            eps = epsilon_for_episode(ep)\n            if actions:\n                if rng.random()  eps:\n                    action = int(rng.choice(actions))\n                else:\n                    # Greedy: argmax Q among allowed actions\n                    q_vals = np.array([Q[state, a] for a in actions], dtype=float)\n                    # Break ties consistently by choosing smallest index among best\n                    best_idx = int(np.argmax(q_vals))\n                    action = int(actions[best_idx])\n            else:\n                break\n            next_state, reward = env.step(state, action)\n            next_actions = env.allowed_actions(next_state)\n            if next_actions:\n                max_next_q = np.max([Q[next_state, a] for a in next_actions])\n            else:\n                max_next_q = 0.0\n            # Q-learning update\n            Q[state, action] = (1.0 - alpha) * Q[state, action] + alpha * (reward + gamma * max_next_q)\n            state = next_state\n    return Q\n\ndef evaluate_greedy(env: HypothesisEnv, Q: np.ndarray):\n    \"\"\"Evaluate greedy policy derived from Q from full state.\"\"\"\n    state = env.full_state\n    seq = []\n    total_reward = 0.0\n    if env.Rmax == 0:\n        return seq, env.baseline_reward(state)\n    steps = 0\n    while not env.is_terminal(state):\n        actions = env.allowed_actions(state)\n        if not actions:\n            break\n        # Greedy among allowed\n        q_vals = np.array([Q[state, a] for a in actions], dtype=float)\n        best_idx = int(np.argmax(q_vals))\n        action = int(actions[best_idx])\n        next_state, reward = env.step(state, action)\n        seq.append(action)\n        total_reward += reward\n        state = next_state\n        steps += 1\n        if steps > env.Rmax:\n            # Safety check; should not happen\n            break\n    return seq, total_reward\n\ndef serialize(obj):\n    \"\"\"Serialize lists and primitive numbers without spaces.\"\"\"\n    if isinstance(obj, (int, np.integer)):\n        return str(int(obj))\n    if isinstance(obj, (float, np.floating)):\n        # Use repr to keep reasonable precision\n        return repr(float(obj))\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        return \"[\" + \",\".join(serialize(x) for x in obj) + \"]\"\n    raise TypeError(f\"Unsupported type for serialization: {type(obj)}\")\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"N\": 4,\n            \"p\": [0.10, 0.70, 0.50, 0.05],\n            \"w\": [1.00, 0.90, 0.60, 0.20],\n            \"B\": 1.00,\n            \"Rmax\": 2,\n            \"gamma\": 0.95,\n            \"episodes\": 12000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 2\n        {\n            \"N\": 5,\n            \"p\": [0.01, 0.85, 0.40, 0.95, 0.20],\n            \"w\": [0.50, 1.20, 0.80, 0.10, 0.70],\n            \"B\": 1.00,\n            \"Rmax\": 2,\n            \"gamma\": 0.90,\n            \"episodes\": 15000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 3\n        {\n            \"N\": 4,\n            \"p\": [0.50, 0.50, 0.50, 0.50],\n            \"w\": [1.00, 1.00, 1.00, 1.00],\n            \"B\": 1.00,\n            \"Rmax\": 3,\n            \"gamma\": 0.95,\n            \"episodes\": 10000,\n            \"eps_init\": 0.30,\n            \"eps_final\": 0.02,\n            \"alpha\": 0.30,\n        },\n        # Case 4\n        {\n            \"N\": 3,\n            \"p\": [0.40, 0.60, 0.55],\n            \"w\": [0.90, 0.80, 0.70],\n            \"B\": 1.00,\n            \"Rmax\": 0,\n            \"gamma\": 0.95,\n            \"episodes\": 1,\n            \"eps_init\": 0.00,\n            \"eps_final\": 0.00,\n            \"alpha\": 0.30,\n        },\n    ]\n\n    rng = np.random.default_rng(42)\n    results = []\n    for case in test_cases:\n        env = HypothesisEnv(case[\"p\"], case[\"w\"], case[\"B\"], case[\"Rmax\"])\n        Q = q_learning(\n            env=env,\n            gamma=case[\"gamma\"],\n            episodes=case[\"episodes\"],\n            eps_init=case[\"eps_init\"],\n            eps_final=case[\"eps_final\"],\n            alpha=case[\"alpha\"],\n            rng=rng,\n        )\n        seq, total_reward = evaluate_greedy(env, Q)\n        results.append([seq, total_reward])\n\n    # Final print statement in the exact required format (no spaces).\n    print(serialize(results))\n\nsolve()\n```"
        }
    ]
}