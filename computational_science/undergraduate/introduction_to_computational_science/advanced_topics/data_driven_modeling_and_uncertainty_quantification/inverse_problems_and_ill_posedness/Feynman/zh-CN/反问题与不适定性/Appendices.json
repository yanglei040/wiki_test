{
    "hands_on_practices": [
        {
            "introduction": "一个逆问题可能是病态的，因为不同的输入可能产生完全相同的输出，使得从输出反推唯一输入变得不可能。本练习将指导你构建一个简单的“模糊与降采样”算子，并构造两个完全不同的原始信号，它们在经过该算子测量后得到完全相同的结果。通过这个过程，你将亲手验证一个非平凡零空间（nullspace）是如何导致解的非唯一性的。",
            "id": "3147025",
            "problem": "考虑一个离散一维信号，表示为向量 $x \\in \\mathbb{R}^n$，其中 $n$ 为偶数。定义一个模糊并降采样的线性算子 $A : \\mathbb{R}^n \\to \\mathbb{R}^{n/2}$，该算子对相邻条目进行平均，并为每对保留一个值。具体来说，对于一个满足 $w_1 + w_2 = 1$ 的权重向量 $w = (w_1, w_2)$，该算子的作用方式为\n$$\n(Ax)_k = w_1 x_{2k-1} + w_2 x_{2k}, \\quad k = 1, 2, \\dots, \\frac{n}{2}.\n$$\n该算子模拟了一个模糊（每对内的加权平均）后跟抽取（每对保留一个值）的过程。逆问题是从测量值 $y = A x$ 中恢复 $x$。从线性算子和零空间的核心定义出发：$A$ 的零空间为 $\\{z \\in \\mathbb{R}^n : A z = 0\\}$。一个逆问题是适定的，当且仅当其解存在、唯一且连续依赖于数据；在本问题中，我们关注唯一性方面。仅使用这些基本概念，推理一个非平凡零空间的存在如何导致方程 $A x = y$ 解的非唯一性。\n\n你的任务是构造两个不同的信号 $x_1, x_2 \\in \\mathbb{R}^n$，使得对于给定的 $A$ 满足 $A x_1 = A x_2$，并通过计算证明它们产生相同的测量值。为此，对于每个测试用例：\n- 确定性地将 $x_1$ 构造为斜坡信号 $x_{1,i} = i$，其中 $i = 1, 2, \\dots, n$。\n- 通过对每对元素设置如下值，构造一个位于 $A$ 的零空间中的非零向量 $z \\in \\mathbb{R}^n$：\n$$\nz_{2k-1} = w_2, \\quad z_{2k} = -w_1, \\quad k = 1, 2, \\dots, \\frac{n}{2}.\n$$\n- 使用指定的标量 $\\alpha \\neq 0$ 构成 $x_2 = x_1 + \\alpha z$。\n- 计算 $y_1 = A x_1$ 和 $y_2 = A x_2$，并通过检查 $\\|y_1 - y_2\\|_2 \\le \\tau$ 来验证在数值容差 $\\tau$ 内 $y_1 = y_2$。\n- 通过奇异值分解（SVD）计算秩来估计 $A$ 的零空间维度（零度）。使用奇异值 $\\sigma_1, \\dots, \\sigma_{\\min(n/2, n)}$，将数值秩定义为大于 $\\tau$ 的奇异值的数量，并报告零度为 $n - \\text{rank}$。\n\n设计你的程序来处理以下参数值 $(n, w_1, w_2, \\alpha, \\tau)$ 的测试套件：\n- 测试用例1：$(n, w_1, w_2, \\alpha, \\tau) = (8, 1/2, 1/2, 1, 10^{-12})$。\n- 测试用例2：$(n, w_1, w_2, \\alpha, \\tau) = (10, 3/4, 1/4, 2, 10^{-12})$。\n- 测试用例3：$(n, w_1, w_2, \\alpha, \\tau) = (2, 1, 0, -1, 10^{-12})$。\n- 测试用例4：$(n, w_1, w_2, \\alpha, \\tau) = (8, 3/5, 2/5, 3, 10^{-12})$。\n\n对于每个测试用例，生成两个输出：\n1. 一个布尔值，指示 $A x_1$ 和 $A x_2$ 是否在容差内相等，即 $\\|y_1 - y_2\\|_2 \\le \\tau$ 是否成立。\n2. 一个等于估计的零空间维度 $n - \\text{rank}$ 的整数。\n\n最终输出格式要求：你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，结果按测试用例的顺序排列，每个测试用例的两个值被展平成一个列表。例如，输出应为以下形式\n$$\n[\\text{bool}_1, \\text{int}_1, \\text{bool}_2, \\text{int}_2, \\text{bool}_3, \\text{int}_3, \\text{bool}_4, \\text{int}_4].\n$$\n本问题不涉及物理单位；所有量纲均为无量纲。不使用角度。",
            "solution": "该问题要求通过计算证明线性逆问题解的非唯一性，这种非唯一性源于正演算子存在非平凡零空间。正演算子 $A: \\mathbb{R}^n \\to \\mathbb{R}^{n/2}$ 模拟了对一个一维信号 $x \\in \\mathbb{R}^n$（其中 $n$ 为偶数）的模糊并降采样过程。问题是从测量值 $y = Ax$ 中恢复 $x$。\n\n算子 $A$ 通过其对向量 $x$ 的作用来定义：\n$$\n(Ax)_k = w_1 x_{2k-1} + w_2 x_{2k}, \\quad k = 1, 2, \\dots, \\frac{n}{2}\n$$\n其中 $w_1$ 和 $w_2$ 是满足 $w_1 + w_2 = 1$ 的权重。这是一个线性算子。为了便于计算，我们将 $A$ 表示为一个矩阵 $A_{\\text{mat}} \\in \\mathbb{R}^{(n/2) \\times n}$。对矩阵的行和列使用从0开始的索引， $A_{\\text{mat}}$ 的第 $k$ 行（其中 $k$ 的范围从 $0$ 到 $\\frac{n}{2}-1$）对应于输出向量 $y$ 的第 $k+1$ 个分量。该行仅在第 $2k$ 列和第 $2k+1$ 列有非零项。具体来说，矩阵元素为：\n$$\n(A_{\\text{mat}})_{k,j} = \n\\begin{cases} \nw_1  \\text{若 } j = 2k \\\\\nw_2  \\text{若 } j = 2k+1 \\\\\n0  \\text{其他情况}\n\\end{cases}\n$$\n对于 $k = 0, \\dots, \\frac{n}{2}-1$ 和 $j = 0, \\dots, n-1$。\n\n逆问题 $Ax=y$ 解的唯一性与 $A$ 的零空间（或核）直接相关，零空间是所有满足 $Az=0$ 的向量 $z$ 的集合。如果零空间只包含零向量，即 $\\text{null}(A) = \\{0\\}$，那么对于任意两个满足 $Ax_1 = y$ 和 $Ax_2=y$ 的解 $x_1$ 和 $x_2$，我们有 $A(x_1-x_2) = Ax_1 - Ax_2 = y - y = 0$。这意味着 $x_1-x_2 \\in \\text{null}(A)$，所以 $x_1-x_2=0$，即 $x_1=x_2$。解是唯一的。\n\n反之，如果存在一个非零向量 $z \\in \\text{null}(A)$，解就不是唯一的。设 $x_1$ 是 $Ax=y$ 的任意一个解。对于任意非零标量 $\\alpha$，我们可以构造第二个不同的信号 $x_2 = x_1 + \\alpha z$。将算子 $A$ 应用于 $x_2$ 会得到：\n$$\nAx_2 = A(x_1 + \\alpha z) = Ax_1 + A(\\alpha z) = y + \\alpha (Az) = y + \\alpha \\cdot 0 = y\n$$\n因为 $\\alpha z \\neq 0$，$x_1$ 和 $x_2$ 是不同的向量，但它们都映射到同一个测量向量 $y$。\n\n问题要求我们通过构造这样两个信号 $x_1$ 和 $x_2$，并验证它们产生相同的输出 $y$ 来证明这一原理。\n首先，信号 $x_1$ 被定义为一个简单的斜坡函数：$x_{1,i} = i$，其中 $i=1, \\dots, n$。\n其次，构造一个非零向量 $z \\in \\text{null}(A)$。条件 $Az=0$ 意味着对于每个 $k \\in \\{1, \\dots, n/2\\}$，我们必须有 $w_1 z_{2k-1} + w_2 z_{2k} = 0$。问题为这样的 $z$ 提供了一个具体的构造方法，即对每对元素设置 $z_{2k-1} = w_2$ 和 $z_{2k} = -w_1$。这个选择确实满足零空间条件：$w_1(w_2) + w_2(-w_1) = w_1 w_2 - w_2 w_1 = 0$。\n基于这些，我们构成 $x_2 = x_1 + \\alpha z$。根据上述推理，我们预期 $Ax_1 = Ax_2$。我们通过计算 $y_1 = Ax_1$ 和 $y_2 = Ax_2$ 并检查它们差值的欧几里得范数 $\\|y_1 - y_2\\|_2$ 是否在一个小的数值容差 $\\tau$ 内，来对这一点进行数值验证。\n\n任务的第二部分是估计零空间的维度，这也被称为 $A$ 的零度。秩-零度定理指出，对于一个线性映射 $A: \\mathbb{R}^n \\to \\mathbb{R}^m$，$\\text{rank}(A) + \\text{nullity}(A) = n$。我们可以使用矩阵 $A_{\\text{mat}}$ 的奇异值分解（SVD）来估计其秩。大于一个小的容差 $\\tau$ 的奇异值 $\\sigma_i$ 的数量，为秩提供了一个数值上稳定的估计。零度则计算为 $n - \\text{rank}(A)$。对于给定的算子 $A$，只要 $w_1$ 和 $w_2$ 不都为零（由 $w_1+w_2=1$ 保证），矩阵 $A_{\\text{mat}}$ 的 $n/2$ 个行就是线性无关的。这是因为每一行 $k$ 都在任何其他行都不共享的唯一一对列（$2k$ 和 $2k+1$）中有非零项。因此，$A$ 的秩是 $n/2$，其零度是 $n - n/2 = n/2$。这意味着对于任意给定的 $y$，解空间是一个维度为 $n/2$ 的仿射子空间。\n\n实现将对每个测试用例执行这些构造和计算。\n对于每组给定的参数 $(n, w_1, w_2, \\alpha, \\tau)$：\n1. 构造 $(n/2) \\times n$ 矩阵 $A_{\\text{mat}}$。\n2. 构造向量 $x_1$、$z$ 和 $x_2$。\n3. 计算 $y_1 = A_{\\text{mat}} x_1$ 和 $y_2 = A_{\\text{mat}} x_2$。\n4. 检查是否 $\\|y_1 - y_2\\|_2 \\le \\tau$。\n5. 计算 $A_{\\text{mat}}$ 的 SVD，统计大于 $\\tau$ 的奇异值数量以得到秩，并计算零度为 $n - \\text{rank}$。\n所有测试用例的结果（一个布尔值和一个整数）然后被收集并按指定格式进行格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a suite of test cases, demonstrating non-uniqueness\n    in an inverse problem due to a non-trivial nullspace.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, w1, w2, alpha, tau)\n        (8, 1/2, 1/2, 1, 1e-12),\n        (10, 3/4, 1/4, 2, 1e-12),\n        (2, 1, 0, -1, 1e-12),\n        (8, 3/5, 2/5, 3, 1e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, w1, w2, alpha, tau = case\n        \n        # The dimension of the output space is m = n/2.\n        m = n // 2\n        \n        # 1. Construct the matrix representation of the linear operator A.\n        # The matrix A_mat has dimensions (m x n).\n        A_mat = np.zeros((m, n))\n        for k in range(m):\n            # Using 0-based indexing for rows (k) and columns.\n            # (Ax)_{k+1} corresponds to row k of the matrix product.\n            # x_{2(k+1)-1} = x_{2k+1} -> column 2k\n            # x_{2(k+1)}   = x_{2k+2} -> column 2k+1\n            # (A_mat @ x)[k] = w1 * x[2*k] + w2 * x[2*k+1]\n            A_mat[k, 2*k] = w1\n            A_mat[k, 2*k+1] = w2\n\n        # 2. Construct the signal x1 as a ramp.\n        # x1_i = i, for i=1,...,n. numpy's arange is 0-indexed, so we go from 1 to n+1.\n        x1 = np.arange(1, n + 1, dtype=float)\n\n        # 3. Construct a non-zero vector z in the nullspace of A.\n        # z_{2k-1} = w2, z_{2k} = -w1 for k=1,...,m\n        z = np.zeros(n)\n        for k in range(m):\n            # 0-based indexing: z_{2k} = w2, z_{2k+1} = -w1\n            z[2*k] = w2\n            z[2*k+1] = -w1\n\n        # 4. Construct the second signal x2 = x1 + alpha * z.\n        x2 = x1 + alpha * z\n\n        # 5. Compute the measurements y1 = A*x1 and y2 = A*x2.\n        y1 = A_mat @ x1\n        y2 = A_mat @ x2\n\n        # 6. Verify that y1 and y2 are equal within the given tolerance.\n        # This is the first result for the test case.\n        norm_diff = np.linalg.norm(y1 - y2)\n        are_equal = norm_diff = tau\n        results.append(are_equal)\n\n        # 7. Estimate the nullspace dimension (nullity) of A.\n        # The rank-nullity theorem states: rank(A) + nullity(A) = n (number of columns).\n        # We compute the rank from the Singular Value Decomposition (SVD).\n        singular_values = np.linalg.svd(A_mat, compute_uv=False)\n        \n        # The numerical rank is the number of singular values greater than the tolerance.\n        rank = np.sum(singular_values > tau)\n        \n        # The nullity is n - rank.\n        nullity = n - rank\n        \n        # This is the second result for the test case.\n        results.append(nullity)\n\n    # Final print statement in the exact required format.\n    # map(str,...) converts boolean True to 'True', False to 'False', and integers to strings.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "病态问题的另一个核心特征是对噪声的极端敏感性，即测量数据中微小的扰动可能导致解的巨大误差。本练习将利用奇异值分解（Singular Value Decomposition, SVD）来构建一个线性逆问题，其中微小的奇异值会像放大器一样放大误差。你将通过编程数值地验证这一理论预测，从而深刻理解为何病态问题在实际应用中如此具有挑战性。",
            "id": "3147053",
            "problem": "您将从线性代数的第一性原理出发，通过奇异值放大来研究线性逆问题中的不稳定性。请使用以下基本原理：奇异值分解（SVD）和 Moore–Penrose 伪逆（下文有精确定义）、欧几里得范数性质以及矩阵-向量乘法的线性性。\n\n任务。构建一个具有快速衰减奇异值的矩阵族，在数据空间中定义受控扰动，并在解空间中验证预测的放大效应。您的程序必须实现指定的构造和检查，并输出一行聚合了固定测试套件的布尔结果。\n\n用作基础的定义。对于任意实矩阵 $A \\in \\mathbb{R}^{m \\times n}$，存在正交矩阵 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$，以及一个对角矩阵 $\\Sigma \\in \\mathbb{R}^{m \\times n}$，其非负对角线元素按 $ \\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n \\ge 0$ 排序，使得 $A = U \\Sigma V^{\\mathsf{T}}$。$U$ 的列是左奇异向量 $\\{u_i\\}_{i=1}^m$，$V$ 的列是右奇异向量 $\\{v_i\\}_{i=1}^n$。Moore–Penrose 伪逆 $A^{+}$ 定义为 $A^{+} = V \\Sigma^{+} U^{\\mathsf{T}}$，其中 $\\Sigma^{+}$ 是将每个非零对角线元素 $\\sigma_i$ 替换为 $1/\\sigma_i$ 并转置形状得到的。\n\n问题要求。对于每个指定的测试用例：\n- 构造正交矩阵 $U$ 和 $V$，并根据给定的标量 $\\alpha$（其中 $0  \\alpha  1$），选择按 $\\sigma_k = \\alpha^{k-1}$ 几何衰减的奇异值 $\\{\\sigma_k\\}_{k=1}^n$。构造矩阵 $A = U \\Sigma V^{\\mathsf{T}}$。\n- 在数据空间中定义一个扰动方向，即指定索引 $i \\in \\{1,\\dots,n\\}$ 的左奇异向量 $u_i$。设右侧项为 $b = \\epsilon \\, u_i$，其中扰动大小为指定的 $\\epsilon  0$。\n- 计算最小范数最小二乘解 $x = A^{+} b$。\n- 计算放大因子 $r = \\|x\\|_2 / \\epsilon$，并根据您从所述基础推导出的结果，将其与从 SVD 结构得出的理论值进行比较。在比较中使用相对容差 $\\mathrm{rtol} = 10^{-6}$ 和绝对容差 $\\mathrm{atol} = 10^{-8}$，即如果 $|r - r_{\\mathrm{theory}}| \\le \\max(\\mathrm{atol}, \\mathrm{rtol} \\cdot |r_{\\mathrm{theory}}|)$，则判定为“通过”，否则为“失败”。不涉及物理单位。\n\n矩阵构造细节。使用任何可复现的方法生成正交的 $U$ 和 $V$（例如，通过对随机高斯矩阵进行正交化）。确保 $\\Sigma$ 的对角线元素为 $\\sigma_k = \\alpha^{k-1}$（$k = 1,\\dots,n$），其他位置为零，从而使 $A$ 具有快速衰减的奇异值。\n\n测试套件。精确实现以下五个测试用例，每个用例以元组 $(m,n,\\alpha,i,\\epsilon)$ 的形式给出：\n- $(\\,6,\\,6,\\,0.2,\\,1,\\,10^{-6}\\,)$\n- $(\\,6,\\,6,\\,0.2,\\,6,\\,10^{-6}\\,)$\n- $(\\,10,\\,6,\\,0.3,\\,3,\\,10^{-5}\\,)$\n- $(\\,10,\\,6,\\,0.3,\\,6,\\,10^{-8}\\,)$\n- $(\\,8,\\,5,\\,0.5,\\,5,\\,10^{-4}\\,)$\n\n输出内容。对于每个测试用例，输出一个布尔值，指示数值上观测到的放大因子 $r$ 是否在指定容差范围内与从 SVD 推导出的理论预测相符。您的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的结果列表（例如，“[True,False,True,True,False]”）。\n\n约束和注意事项。\n- 不涉及角度。\n- 所有计算均为实数算术中的纯数值线性代数。\n- 程序必须是完全确定性的，并且不应需要任何外部输入。",
            "solution": "该问题是有效的，因为它在科学上基于线性代数，定义良好，具有清晰客观的指令集，并且其组成部分已完全指定。任务是数值验证一个关于线性逆问题不稳定性的理论结果，这是计算科学中的一个标准课题。\n\n问题的核心在于理解当矩阵 $A$ 是病态时，线性系统 $Ax = b$ 的解。最小范数最小二乘解由 $x = A^{+}b$ 给出，其中 $A^{+}$ 是 Moore-Penrose 伪逆。我们将首先推导理论放大因子，然后实现数值验证。\n\n### 理论推导\n\n给定矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的奇异值分解（SVD）为 $A = U\\Sigma V^{\\mathsf{T}}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个对角矩阵，其主对角线上的非负奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$。$U$ 的列是左奇异向量 $\\{u_i\\}$，$V$ 的列是右奇异向量 $\\{v_i\\}$。\n\nMoore-Penrose 伪逆定义为 $A^{+} = V\\Sigma^{+}U^{\\mathsf{T}}$，其中 $\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ 是通过取 $\\Sigma$ 的非零元素的倒数然后转置矩阵得到的。对于此问题，奇异值为 $\\sigma_k = \\alpha^{k-1}$（$k=1, \\dots, n$），且 $0  \\alpha  1$。由于 $\\alpha \\neq 0$，所有 $\\sigma_k$ 均为非零。因此，$\\Sigma^{+}$ 的对角线元素为 $1/\\sigma_k$。\n\n右侧向量 $b$ 定义为沿第 $i$ 个左奇异向量 $u_i$ 方向、大小为 $\\epsilon$ 的扰动：\n$$b = \\epsilon u_i$$\n其中 $u_i$ 是 $U$ 的第 $i$ 列。\n\n然后，解 $x$ 计算如下：\n$$x = A^{+}b = (V\\Sigma^{+}U^{\\mathsf{T}})(\\epsilon u_i)$$\n利用线性性，我们可以提出标量 $\\epsilon$：\n$$x = \\epsilon (V\\Sigma^{+}U^{\\mathsf{T}}u_i)$$\n$U^{\\mathsf{T}}u_i$ 项是 $U$ 的转置与其自身第 $i$ 列的乘积。由于 $U$ 是一个正交矩阵，其列构成一个标准正交基。乘积 $u_j^{\\mathsf{T}}u_i = \\delta_{ji}$（克罗内克 δ）。因此，$U^{\\mathsf{T}}u_i$ 的结果是一个在第 $i$ 个位置为 1，其他位置为 0 的向量。这是 $\\mathbb{R}^m$ 中的第 $i$ 个标准基向量，我们记为 $e_i$。请注意，问题中的索引是基于 1 的。\n$$x = \\epsilon (V\\Sigma^{+}e_i)$$\n接下来，我们计算 $\\Sigma^{+}e_i$。矩阵 $\\Sigma^{+}$ 是一个 $n \\times m$ 的矩阵，其对角线上的值直到 A 的秩（此处为 $n$）为止为 $1/\\sigma_k$。用 $\\Sigma^{+}$ 乘以 $e_i$ 会选择 $\\Sigma^{+}$ 的第 $i$ 列。该列只有一个非零项，即在第 $i$ 个位置的 $1/\\sigma_i$。该乘积的结果是向量 $(1/\\sigma_i)e_i'$，其中 $e_i'$ 是 $\\mathbb{R}^n$ 中的第 $i$ 个标准基向量。\n$$x = \\epsilon V \\left(\\frac{1}{\\sigma_i} e_i'\\right) = \\frac{\\epsilon}{\\sigma_i} (V e_i')$$\n乘积 $Ve_i'$ 选择了矩阵 $V$ 的第 $i$ 列，即右奇异向量 $v_i$。\n$$x = \\frac{\\epsilon}{\\sigma_i} v_i$$\n现在我们可以计算解 $x$ 的欧几里得范数：\n$$\\|x\\|_2 = \\left\\|\\frac{\\epsilon}{\\sigma_i} v_i\\right\\|_2 = \\left|\\frac{\\epsilon}{\\sigma_i}\\right| \\|v_i\\|_2$$\n由于 $\\epsilon  0$ 且 $\\sigma_i = \\alpha^{i-1}  0$，绝对值是多余的。此外，由于 $V$ 是一个正交矩阵，其列是单位向量，因此 $\\|v_i\\|_2 = 1$。\n$$\\|x\\|_2 = \\frac{\\epsilon}{\\sigma_i}$$\n问题要求计算放大因子 $r$，定义为解的范数与扰动大小之比：\n$$r = \\frac{\\|x\\|_2}{\\epsilon} = \\frac{\\epsilon/\\sigma_i}{\\epsilon} = \\frac{1}{\\sigma_i}$$\n已知 $\\sigma_i = \\alpha^{i-1}$，理论放大因子为：\n$$r_{\\mathrm{theory}} = \\frac{1}{\\alpha^{i-1}} = \\alpha^{1-i}$$\n此推导表明，沿第 $i$ 个左奇异向量 $u_i$ 方向的扰动，在解中被放大了等于相应奇异值 $\\sigma_i$ 倒数倍的因子。对于小的 $\\sigma_i$（即大的 $i$），这种放大效应非常显著，证明了问题的病态性质。\n\n### 数值实现\n\n程序将对每个测试用例 $(m, n, \\alpha, i, \\epsilon)$ 执行以下步骤：\n1.  为保证可复现性，设置一个固定的随机种子。\n2.  使用随机矩阵的 QR 分解生成正交矩阵 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$。\n3.  构造对角矩阵 $\\Sigma \\in \\mathbb{R}^{m \\times n}$，其元素为 $\\sigma_k = \\alpha^{k-1}$（$k=1, \\dots, n$）。\n4.  定义扰动向量 $b = \\epsilon u_i$，其中 $u_i$ 是 $U$ 的第 $i$ 列（$i$ 使用基于 1 的索引）。\n5.  构造伪逆 $A^{+} = V\\Sigma^{+}U^{\\mathsf{T}}$。$\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ 的对角线元素为 $1/\\sigma_k$。\n6.  计算解 $x = A^{+}b$。\n7.  计算数值放大因子 $r = \\|x\\|_2 / \\epsilon$。\n8.  计算理论放大因子 $r_{\\mathrm{theory}} = 1/\\sigma_i = \\alpha^{1-i}$。\n9.  使用指定条件 $|r - r_{\\mathrm{theory}}| \\le \\max(\\mathrm{atol}, \\mathrm{rtol} \\cdot |r_{\\mathrm{theory}}|)$ 比较 $r$ 和 $r_{\\mathrm{theory}}$，其中 $\\mathrm{rtol} = 10^{-6}$ 且 $\\mathrm{atol} = 10^{-8}$。\n10. 存储比较的布尔结果。\n11. 处理完所有测试用例后，将布尔结果列表格式化为所需的字符串格式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs matrices with specified singular value decay, applies a\n    controlled perturbation, and verifies the theoretical amplification\n    of the solution norm for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (m, n, alpha, i, epsilon)\n    test_cases = [\n        (6, 6, 0.2, 1, 10**-6),\n        (6, 6, 0.2, 6, 10**-6),\n        (10, 6, 0.3, 3, 10**-5),\n        (10, 6, 0.3, 6, 10**-8),\n        (8, 5, 0.5, 5, 10**-4),\n    ]\n\n    results = []\n    \n    # Use a fixed seed for reproducible generation of orthogonal matrices.\n    np.random.seed(0)\n\n    # Tolerances for comparison as specified in the problem statement.\n    rtol = 1e-6\n    atol = 1e-8\n\n    for case in test_cases:\n        m, n, alpha, i_one_based, epsilon = case\n\n        # Step 1: Construct orthogonal matrices U and V.\n        # A reproducible method is to use QR decomposition of a random matrix.\n        U, _ = np.linalg.qr(np.random.randn(m, m))\n        V, _ = np.linalg.qr(np.random.randn(n, n))\n\n        # Step 2: Construct the Sigma matrix with geometrically decaying singular values.\n        # sigma_k = alpha**(k-1) for k = 1,...,n.\n        # Python uses 0-based indexing, so s_vals[k] is alpha**k, for k=0...n-1\n        s_vals = np.array([alpha**k for k in range(n)])\n        Sigma = np.zeros((m, n))\n        np.fill_diagonal(Sigma, s_vals)\n\n        # Step 3: Define the perturbation in the data space.\n        # b = epsilon * u_i, where u_i is the i-th left singular vector.\n        # Note: The problem uses a 1-based index `i`.\n        u_i = U[:, i_one_based - 1]\n        b = epsilon * u_i\n\n        # Step 4: Compute the minimum-norm least-squares solution x = A^+ b.\n        # We construct A^+ from SVD components as per definitions.\n        # A^+ = V * Sigma^+ * U^T\n        \n        # Construct Sigma_plus, the n x m pseudoinverse of Sigma.\n        # The reciprocals of s_vals are numerically stable as s_vals are > 0.\n        s_plus_vals = 1.0 / s_vals\n        Sigma_plus = np.zeros((n, m))\n        np.fill_diagonal(Sigma_plus, s_plus_vals)\n        \n        A_plus = V @ Sigma_plus @ U.T\n        x = A_plus @ b\n\n        # Step 5: Compute the numerical amplification factor.\n        r_numeric = np.linalg.norm(x) / epsilon\n\n        # Step 6: Compute the theoretical amplification factor.\n        # r_theory = 1 / sigma_i = 1 / alpha**(i-1).\n        # We access the i-th singular value using 0-based index `i_one_based - 1`.\n        sigma_i = s_vals[i_one_based - 1]\n        r_theory = 1.0 / sigma_i\n\n        # Step 7: Compare r_numeric to r_theory using specified tolerances.\n        # The comparison checks if |r - r_theory| = max(atol, rtol * |r_theory|).\n        # We use abs(r_theory) for robustness, though it's positive here.\n        is_close = abs(r_numeric - r_theory) = max(atol, rtol * abs(r_theory))\n        results.append(is_close)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "认识到问题的根源后，我们来探索解决方案。本练习将带你进入贝叶斯推断的世界，通过最大后验（Maximum A Posteriori, MAP）估计来推导两种不同的求解器。你将看到，对噪声的不同假设（高斯噪声 vs. 拉普拉斯噪声）会自然地引出不同的数据保真项（$\\ell_2$ 范数 vs. $\\ell_1$ 范数），并比较它们在处理含离群点（outliers）的噪声数据时的表现，从而理解为何 $\\ell_1$ 范数在逆问题中是一种强大的稳健性工具。",
            "id": "3147000",
            "problem": "给定一个线性逆模型，其观测值由 $b = A x + \\varepsilon$ 建模，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，$b \\in \\mathbb{R}^{m}$。数据噪声 $\\varepsilon$ 可能服从拉普拉斯分布或高斯分布，重建过程由一个线性算子 $L \\in \\mathbb{R}^{p \\times n}$ 正则化，该算子通过作用于 $x$ 来促进平滑性。任务是设计并实现一个程序，在最大后验 (MAP) 估计的框架下，通过比较拉普拉斯数据模型和高斯数据模型来展示对离群值的鲁棒性，过程中不使用任何预设的简化公式。从核心定义开始：概率密度函数、最大似然以及高斯先验导致二次惩罚项的概念。为两种噪声模型推导适当的目标函数，并制定计算策略以获得相应的 MAP 估计。通过使用具有明确定义的测试套件的可复现合成数据来确保科学真实性。\n\n按如下方式确定性地构造 $A$、$L$ 和一个基准真相 $x^\\star$：\n- 令 $m = 60$，$n = 40$。\n- 通过使用固定种子 $s_A = 7$ 采样独立的标准正态分布条目来生成 $A$，并将每列归一化，使其 $\\ell_2$ 范数为1。\n- 将 $L$ 定义为一阶前向差分算子：对于 $k \\in \\{0, 1, \\dots, n-2\\}$，$L$ 的第 $k$ 行在第 $k$ 列的条目为 $-1$，在第 $k+1$ 列的条目为 $+1$，其他位置为零。因此，$L \\in \\mathbb{R}^{(n-1) \\times n}$。\n- 定义一个分段常数的基准真相 $x^\\star \\in \\mathbb{R}^{n}$，采用从零开始的索引：当 $k \\in \\{5,6,\\dots,15\\}$ 时，设 $x^\\star_k = 1$；当 $k \\in \\{25,26,\\dots,35\\}$ 时，设 $x^\\star_k = -0.6$；其他情况下设 $x^\\star_k = 0$。\n\n对于每个测试用例，通过 $b = A x^\\star + \\varepsilon$ 构造观测值 $b$，其中 $\\varepsilon$ 根据指定的噪声模型和离群值配置生成。使用固定的种子以确保不同用例之间的可复现性。对于拉普拉斯噪声，从均值为零、尺度参数为 $s_L = 0.05$ 的拉普拉斯分布中抽取独立的噪声条目。对于高斯噪声，从均值为零、标准差为 $\\sigma = 0.05$ 的正态分布中抽取独立的噪声条目。在有离群值的用例中，在测量索引 $i = 3$ 和 $i = 17$（从零开始的索引）处，向 $b$ 的条目添加两个振幅为 $a_o = 5.0$ 的大扰动（离群值）。所有可能出现的角度都必须以弧度为单位，但此构造不涉及角度。\n\n对于拉普拉斯噪声模型，从拉普拉斯概率密度函数推导出相应的数据失配项，并将其与由 $L$ 引起的二次平滑惩罚项结合，以获得 MAP 估计器。对于高斯噪声模型，从高斯概率密度函数推导出相应的数据失配项，并将其与相同的二次平滑惩罚项结合，以获得 MAP 估计器。为两种估计器实现计算上稳健的求解器，并确保数值稳定性。对于拉普拉斯数据 MAP 估计器，使用一个鲁棒的迭代方案，该方案收敛到一个凸目标的最小化子。对于高斯数据 MAP 估计器，使用基于正规方程组的直接线性代数求解器。\n\n定义包含以下五个用例的测试套件，每个用例指定了噪声类型、离群值配置和正则化强度 $\\lambda$：\n- 用例1：拉普拉斯噪声，无离群值，$\\lambda = 0.1$，种子 $s_1 = 101$。\n- 用例2：拉普拉斯噪声，有离群值，$\\lambda = 0.1$，种子 $s_2 = 102$。\n- 用例3：高斯噪声，有离群值，$\\lambda = 0.1$，种子 $s_3 = 103$。\n- 用例4：高斯噪声，无离群值，$\\lambda = 0.1$，种子 $s_4 = 104$。\n- 用例5：拉普拉斯噪声，有离群值，边界情况 $\\lambda = 0$，种子 $s_5 = 105$。\n\n对于每个用例，计算：\n- 在使用 $L$ 进行二次正则化的拉普拉斯数据模型下的估计器（拉普拉斯噪声的 MAP）。\n- 在使用 $L$ 进行相同二次正则化的高斯数据模型下的估计器（高斯噪声的 MAP）。\n\n对于每个用例，通过计算两种估计器的欧几里得重建误差 $\\|x - x^\\star\\|_2$ 来量化性能。每个用例的结果必须是一个布尔值，指示拉普拉斯数据估计器是否产生严格更小的欧几里得重建误差，即如果 $\\|x_{\\text{Laplace}} - x^\\star\\|_2  \\|x_{\\text{Gaussian}} - x^\\star\\|_2$，则返回 $\\text{True}$，否则返回 $\\text{False}$。\n\n你的程序必须生成单行输出，其中包含五个用例的结果，格式为方括号内的逗号分隔列表，布尔值按上述用例顺序排列。例如，输出格式必须为 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_j$ 是 $\\text{True}$ 或 $\\text{False}$。\n\n此问题不涉及物理单位。在输出或计算的任何地方都不得使用百分比。",
            "solution": "任务是比较在一个最大后验 (MAP) 框架下推导出的两种用于线性逆问题的估计器的鲁棒性。第一个估计器基于高斯噪声模型，第二个基于拉普拉斯噪声模型。该比较在包含和不包含离群值的合成数据上进行。\n\n我们从贝叶斯视角开始。在给定测量数据 $b$ 的情况下，参数向量 $x$ 的 MAP 估计 $\\hat{x}_{\\text{MAP}}$ 最大化后验概率 $p(x|b)$。根据贝叶斯定理，这等价于最小化负对数后验：\n$$ \\hat{x}_{\\text{MAP}} = \\arg\\max_x p(x|b) = \\arg\\max_x p(b|x)p(x) = \\arg\\min_x \\left\\{ -\\log(p(b|x)) - \\log(p(x)) \\right\\} $$\n这里，$p(b|x)$ 是给定参数时数据的似然，$p(x)$ 是参数的先验概率。$-\\log(p(b|x))$ 项是数据保真项，$-\\log(p(x))$ 项是正则化项。\n\n观测模型为 $b = Ax + \\varepsilon$，其中 $\\varepsilon \\in \\mathbb{R}^m$ 是一个独立同分布 (i.i.d.) 噪声分量的向量。似然 $p(b|x)$ 由噪声 $\\varepsilon$ 的概率分布决定。\n\n对于**高斯数据模型**，每个噪声分量 $\\varepsilon_i$ 假定服从均值为 $0$、方差为 $\\sigma^2$ 的高斯分布，记作 $\\mathcal{N}(0, \\sigma^2)$。其概率密度函数 (PDF) 为 $p(\\varepsilon_i) = (\\sqrt{2\\pi}\\sigma)^{-1} \\exp(-\\varepsilon_i^2 / (2\\sigma^2))$。整个测量向量 $b$ 的似然是在残差 $\\varepsilon_i = (Ax)_i - b_i$ 处求值的各个密度的乘积：\n$$ p(b|x) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{((Ax)_i - b_i)^2}{2\\sigma^2}\\right) = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right)^m \\exp\\left(-\\frac{\\|Ax-b\\|_2^2}{2\\sigma^2}\\right) $$\n负对数似然，忽略不影响对 $x$ 最小化的常数项，与残差的欧几里得范数（$\\ell_2$-范数）的平方成正比：\n$$ -\\log(p(b|x)) \\propto \\|Ax-b\\|_2^2 $$\n\n对于**拉普拉斯数据模型**，每个噪声分量 $\\varepsilon_i$ 假定服从均值为 $0$、尺度参数为 $s_L$ 的拉普拉斯分布，记作 $\\text{Laplace}(0, s_L)$。其 PDF 为 $p(\\varepsilon_i) = (2s_L)^{-1} \\exp(-|\\varepsilon_i|/s_L)$。似然为：\n$$ p(b|x) = \\prod_{i=1}^{m} \\frac{1}{2s_L} \\exp\\left(-\\frac{|(Ax)_i - b_i|}{s_L}\\right) = \\left(\\frac{1}{2s_L}\\right)^m \\exp\\left(-\\frac{\\|Ax-b\\|_1}{s_L}\\right) $$\n相应的负对数似然与残差的 $\\ell_1$-范数成正比：\n$$ -\\log(p(b|x)) \\propto \\|Ax-b\\|_1 $$\n众所周知，$\\ell_1$-范数数据保真项比 $\\ell_2$-范数对大幅度离群值更鲁棒。\n\n正则化通过先验项 $p(x)$ 引入。问题指定了一个由线性算子 $L$ 引起的二次平滑惩罚项。这对应于变换后向量 $Lx$ 上的高斯先验，即 $p(Lx) \\propto \\exp(-\\alpha \\|Lx\\|_2^2)$，其中 $\\alpha  0$ 是某个常数。因此，负对数先验与对 $x$ 的二次惩罚项成正比：\n$$ -\\log(p(x)) \\propto \\|Lx\\|_2^2 $$\n\n将数据保真项和正则化项与正则化参数 $\\lambda$ 结合，得到最终要最小化的目标函数：\n1.  **高斯数据 MAP 估计器 ($\\hat{x}_{\\text{G}}$)**：这对应于吉洪诺夫 (Tikhonov) 正则化。\n    $$ \\hat{x}_{\\text{G}} = \\arg\\min_x J_G(x) = \\arg\\min_x \\left\\{ \\|Ax-b\\|_2^2 + \\lambda \\|Lx\\|_2^2 \\right\\} $$\n2.  **拉普拉斯数据 MAP 估计器 ($\\hat{x}_{\\text{L}}$)**：这是一种 $\\ell_1$-正则化回归的形式，但 $\\ell_1$-范数作用于数据项。\n    $$ \\hat{x}_{\\text{L}} = \\arg\\min_x J_L(x) = \\arg\\min_x \\left\\{ \\|Ax-b\\|_1 + \\lambda \\|Lx\\|_2^2 \\right\\} $$\n\n为了找到高斯模型的解，我们注意到 $J_G(x)$ 是一个可微的凸函数。通过将其关于 $x$ 的梯度设为零来找到其最小值：\n$$ \\nabla_x J_G(x) = \\nabla_x \\left( (Ax-b)^T(Ax-b) + \\lambda(Lx)^T(Lx) \\right) = 2A^T(Ax-b) + 2\\lambda L^T(Lx) = 0 $$\n重新整理得到正规方程组，这是一个线性方程组：\n$$ (A^T A + \\lambda L^T L)x = A^T b $$\n对于 $\\lambda  0$，矩阵 $(A^T A + \\lambda L^T L)$ 是对称正定的，确保存在唯一解，该解可以使用直接线性求解器高效计算。\n\n拉普拉斯模型的目标函数 $J_L(x)$ 是凸的，但由于 $\\ell_1$-范数而不可微。它不能通过直接将梯度设为零来求解。需要使用迭代方法。我们使用迭代重加权最小二乘 (IRLS) 算法。其核心思想是用加权的 $\\ell_2$-范数来近似 $\\ell_1$-范数项。使用恒等式 $|r_i| = r_i^2 / |r_i|$，我们可以将 $\\ell_1$-范数项表示为 $\\sum_i w_i ((Ax)_i - b_i)^2$，其中权重 $w_i = 1 / |(Ax)_i - b_i|$ 依赖于 $x$。在 IRLS 中，这些权重在每次迭代中使用前一次迭代的解 $x^{(k)}$ 进行更新。这导致求解一系列加权最小二乘问题。\n在第 $k+1$ 次迭代中，我们通过最小化以下式子来求解 $x^{(k+1)}$：\n$$ \\min_x \\left\\{ \\sum_{i=1}^m w_i^{(k)} ((Ax)_i - b_i)^2 + \\lambda \\|Lx\\|_2^2 \\right\\} = \\min_x \\left\\{ (Ax-b)^T W^{(k)} (Ax-b) + \\lambda \\|Lx\\|_2^2 \\right\\} $$\n其中 $W^{(k)}$ 是一个对角矩阵，其条目为 $W_{ii}^{(k)} = (\\max(|(Ax^{(k)}-b)_i|, \\epsilon))^{-1}$。小参数 $\\epsilon  0$ 通过防止除以零来确保数值稳定性。将此二次目标的梯度设为零，得到下一个迭代值 $x^{(k+1)}$ 的线性系统：\n$$ (A^T W^{(k)} A + \\lambda L^T L) x^{(k+1)} = A^T W^{(k)} b $$\n重复求解该系统，直到解 $x^{(k)}$ 收敛。初始猜测 $x^{(0)}$ 可以设为高斯数据模型的解 $\\hat{x}_{\\text{G}}$。\n\n实现将遵循这些推导来构造估计器。参数 ($m=60, n=40$)、矩阵 ($A, L$)、基准真相 $x^\\star$ 和测试用例均按问题陈述中的规定定义。每个估计器的性能通过计算欧几里得重建误差 $\\| \\hat{x} - x^\\star \\|_2$ 来评估。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the inverse problem by comparing Laplace and Gaussian data models.\n\n    Derives and implements MAP estimators for both models and evaluates their\n    robustness to outliers across a suite of test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    M, N = 60, 40\n    S_A_SEED = 7\n    S_L_SCALE = 0.05\n    SIGMA_STD = 0.05\n    OUTLIER_AMP = 5.0\n    OUTLIER_INDICES = [3, 17]\n\n    # --- Test Case Suite ---\n    test_cases = [\n        {'case': 1, 'noise_type': 'laplace', 'outliers': False, 'lambda_val': 0.1, 'seed': 101},\n        {'case': 2, 'noise_type': 'laplace', 'outliers': True, 'lambda_val': 0.1, 'seed': 102},\n        {'case': 3, 'noise_type': 'gaussian', 'outliers': True, 'lambda_val': 0.1, 'seed': 103},\n        {'case': 4, 'noise_type': 'gaussian', 'outliers': False, 'lambda_val': 0.1, 'seed': 104},\n        {'case': 5, 'noise_type': 'laplace', 'outliers': True, 'lambda_val': 0.0, 'seed': 105},\n    ]\n\n    # --- Deterministic Setup ---\n    # Generate matrix A\n    rng_A = np.random.default_rng(seed=S_A_SEED)\n    A = rng_A.standard_normal((M, N))\n    A /= np.linalg.norm(A, axis=0) # Normalize columns to unit l2 norm\n\n    # Define L, the first-order forward difference operator\n    L = np.zeros((N - 1, N))\n    rows = np.arange(N - 1)\n    L[rows, rows] = -1.0\n    L[rows, rows + 1] = 1.0\n\n    # Define ground truth x_star\n    x_star = np.zeros(N)\n    x_star[5:16] = 1.0\n    x_star[25:36] = -0.6\n\n    # Pre-compute matrix products for efficiency\n    AtA = A.T @ A\n    LtL = L.T @ L\n\n    # --- Solver Implementations ---\n\n    def compute_gaussian_estimator(A, b, L, lambda_val, AtA_p, LtL_p):\n        \"\"\"\n        Computes the MAP estimate for the Gaussian-data model.\n        Solves (A^T A + lambda * L^T L) x = A^T b.\n        \"\"\"\n        H = AtA_p + lambda_val * LtL_p\n        rhs = A.T @ b\n        try:\n            x_g = np.linalg.solve(H, rhs)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular cases (e.g., lambda=0 with ill-conditioned A^T A)\n            x_g = np.linalg.pinv(H) @ rhs\n        return x_g\n\n    def compute_laplace_estimator(A, b, L, lambda_val, AtA_p, LtL_p, x_initial):\n        \"\"\"\n        Computes the MAP estimate for the Laplace-data model using IRLS.\n        Solves argmin ||Ax-b||_1 + lambda * ||Lx||_2^2.\n        \"\"\"\n        num_iter = 20\n        epsilon = 1e-6\n        x_l = np.copy(x_initial)\n\n        for _ in range(num_iter):\n            residuals = np.abs(A @ x_l - b)\n            weights = 1.0 / np.maximum(residuals, epsilon)\n            W = np.diag(weights)\n\n            H = A.T @ W @ A + lambda_val * LtL_p\n            rhs = A.T @ W @ b\n\n            try:\n                x_l = np.linalg.solve(H, rhs)\n            except np.linalg.LinAlgError:\n                x_l = np.linalg.pinv(H) @ rhs\n        \n        return x_l\n\n    # --- Main Loop for Test Cases ---\n    results = []\n    for case in test_cases:\n        # Generate noise based on the case\n        rng_case = np.random.default_rng(seed=case['seed'])\n        if case['noise_type'] == 'laplace':\n            epsilon_noise = rng_case.laplace(0, S_L_SCALE, size=M)\n        else: # gaussian\n            epsilon_noise = rng_case.normal(0, SIGMA_STD, size=M)\n\n        # Construct observations b\n        b = A @ x_star + epsilon_noise\n        if case['outliers']:\n            b[OUTLIER_INDICES[0]] += OUTLIER_AMP\n            b[OUTLIER_INDICES[1]] += OUTLIER_AMP\n\n        # --- Compute Estimators ---\n        lambda_val = case['lambda_val']\n\n        # Gaussian-data model estimator\n        x_gaussian = compute_gaussian_estimator(A, b, L, lambda_val, AtA, LtL)\n        \n        # Laplace-data model estimator (initialize with Gaussian solution)\n        x_laplace = compute_laplace_estimator(A, b, L, lambda_val, AtA, LtL, x_initial=x_gaussian)\n\n        # --- Quantify Performance ---\n        err_gaussian = np.linalg.norm(x_gaussian - x_star)\n        err_laplace = np.linalg.norm(x_laplace - x_star)\n\n        # Compare errors and store the boolean result\n        results.append(err_laplace  err_gaussian)\n\n    # --- Final Output ---\n    # Format the list of booleans as a string \"[True,False,...]\"\n    print(f\"[{','.join(map(lambda x: str(x), results))}]\")\n\nsolve()\n```"
        }
    ]
}