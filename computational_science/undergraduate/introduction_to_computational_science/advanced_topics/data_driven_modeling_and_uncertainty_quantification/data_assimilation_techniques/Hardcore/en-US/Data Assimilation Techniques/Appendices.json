{
    "hands_on_practices": [
        {
            "introduction": "In variational data assimilation, finding the optimal state requires computing the gradient of a cost function, a task efficiently accomplished using the adjoint method. This exercise introduces the \"dot product test,\" a critical procedure for verifying that a hand-derived or coded adjoint of a (linearized) operator is correct. Mastering this fundamental check  is an indispensable skill for building reliable and complex assimilation systems.",
            "id": "3116123",
            "problem": "In variational data assimilation, an observation operator maps a state vector to the observation space. Consider the nonlinear observation operator $H: \\mathbb{R}^{2} \\to \\mathbb{R}^{3}$ defined for state $x = (T, q)^{\\top}$ by\n$$\nH(x) = \\begin{pmatrix}\nT^{2} \\\\\n\\exp(q) \\\\\nT\\,q\n\\end{pmatrix}.\n$$\nSuppose a background state $x_{b} = (T_{b}, q_{b})^{\\top}$ with $T_{b} = 2$ and $q_{b} = 0$ is given. Using a first-order Taylor expansion about $x_{b}$, the linearized observation operator at $x_{b}$ is represented by the Jacobian matrix evaluated at $x_{b}$, denoted $H'(x_{b})$. Work with the standard Euclidean inner product in both the state space and observation space.\n\nYou are given a state perturbation $\\delta x = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$ and an observation-space direction $\\delta y = \\begin{pmatrix} 2 \\\\ 5 \\\\ -1 \\end{pmatrix}$. Perform the following steps:\n\n1. Derive $H'(x_{b})$ explicitly as a $3 \\times 2$ matrix by computing the Jacobian of $H$ and evaluating it at $x_{b}$.\n2. Determine the adjoint of $H'(x_{b})$ under the Euclidean inner product on $\\mathbb{R}^{2}$ and $\\mathbb{R}^{3}$.\n3. Compute the Euclidean inner product of $H'(x_{b})\\,\\delta x$ with $\\delta y$.\n4. Compute the Euclidean inner product of $\\delta x$ with the adjoint of $H'(x_{b})$ applied to $\\delta y$.\n\nReport the common scalar value obtained in steps $3$ and $4$ as a single exact integer (unitless). Do not round; provide the exact value.",
            "solution": "The problem is validated as being scientifically grounded, well-posed, objective, and complete. All necessary data and definitions are provided, and the tasks are mathematically sound and unambiguous. We may therefore proceed with the solution.\n\nThe problem requires us to perform a series of calculations involving a nonlinear observation operator $H$, its linearization $H'$, and the corresponding adjoint operator. The state vector is $x = \\begin{pmatrix} T \\\\ q \\end{pmatrix} \\in \\mathbb{R}^{2}$ and the observation operator is $H: \\mathbb{R}^{2} \\to \\mathbb{R}^{3}$ defined by:\n$$\nH(x) = H(T, q) = \\begin{pmatrix}\nT^{2} \\\\\n\\exp(q) \\\\\nTq\n\\end{pmatrix}\n$$\nThe background state is given as $x_{b} = \\begin{pmatrix} T_{b} \\\\ q_{b} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$.\n\n**Step 1: Derive the linearized operator $H'(x_{b})$**\n\nThe linearized observation operator $H'(x)$ is the Jacobian matrix of $H$. The components of $H$ are $H_1(T, q) = T^2$, $H_2(T, q) = \\exp(q)$, and $H_3(T, q) = Tq$. We compute the partial derivatives with respect to $T$ and $q$:\n$$\n\\frac{\\partial H_1}{\\partial T} = 2T, \\quad \\frac{\\partial H_1}{\\partial q} = 0\n$$\n$$\n\\frac{\\partial H_2}{\\partial T} = 0, \\quad \\frac{\\partial H_2}{\\partial q} = \\exp(q)\n$$\n$$\n\\frac{\\partial H_3}{\\partial T} = q, \\quad \\frac{\\partial H_3}{\\partial q} = T\n$$\nThe Jacobian matrix $H'(x)$ is therefore:\n$$\nH'(x) = \\begin{pmatrix}\n\\frac{\\partial H_1}{\\partial T}  \\frac{\\partial H_1}{\\partial q} \\\\\n\\frac{\\partial H_2}{\\partial T}  \\frac{\\partial H_2}{\\partial q} \\\\\n\\frac{\\partial H_3}{\\partial T}  \\frac{\\partial H_3}{\\partial q}\n\\end{pmatrix} = \\begin{pmatrix}\n2T  0 \\\\\n0  \\exp(q) \\\\\nq  T\n\\end{pmatrix}\n$$\nNow, we evaluate this Jacobian at the background state $x_{b} = (T_{b}, q_{b})^{\\top} = (2, 0)^{\\top}$:\n$$\nH'(x_{b}) = H'(2, 0) = \\begin{pmatrix}\n2(2)  0 \\\\\n0  \\exp(0) \\\\\n0  2\n\\end{pmatrix} = \\begin{pmatrix}\n4  0 \\\\\n0  1 \\\\\n0  2\n\\end{pmatrix}\n$$\nThis is the required $3 \\times 2$ matrix representation of the linearized operator at $x_{b}$.\n\n**Step 2: Determine the adjoint of $H'(x_{b})$**\n\nThe problem specifies the use of the standard Euclidean inner product in both the state space $\\mathbb{R}^{2}$ and the observation space $\\mathbb{R}^{3}$. For a linear operator represented by a real matrix $M$ between finite-dimensional Euclidean spaces, its adjoint $M^*$ is represented by the transpose of the matrix, $M^{\\top}$.\nLet $M = H'(x_{b})$. The adjoint of $M$, denoted $M^*$, is such that for any $\\delta x \\in \\mathbb{R}^{2}$ and $\\delta y \\in \\mathbb{R}^{3}$, the following relation holds:\n$$\n\\langle M \\delta x, \\delta y \\rangle = \\langle \\delta x, M^* \\delta y \\rangle\n$$\nWith the standard Euclidean inner product (dot product), this is equivalent to $(M \\delta x)^{\\top} \\delta y = (\\delta x)^{\\top} (M^* \\delta y)$. Since $(M \\delta x)^{\\top} = (\\delta x)^{\\top} M^{\\top}$, it follows that $M^* = M^{\\top}$.\nTherefore, the adjoint of $H'(x_{b})$ is its transpose, $(H'(x_{b}))^{\\top}$:\n$$\n(H'(x_{b}))^{\\top} = \\begin{pmatrix}\n4  0 \\\\\n0  1 \\\\\n0  2\n\\end{pmatrix}^{\\top} = \\begin{pmatrix}\n4  0  0 \\\\\n0  1  2\n\\end{pmatrix}\n$$\n\n**Step 3: Compute the inner product of $H'(x_{b})\\,\\delta x$ with $\\delta y$**\n\nWe are given the state perturbation $\\delta x = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$ and the observation-space direction $\\delta y = \\begin{pmatrix} 2 \\\\ 5 \\\\ -1 \\end{pmatrix}$.\nFirst, we apply the linearized operator $H'(x_{b})$ to $\\delta x$:\n$$\nH'(x_{b})\\,\\delta x = \\begin{pmatrix}\n4  0 \\\\\n0  1 \\\\\n0  2\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n-3\n\\end{pmatrix} = \\begin{pmatrix}\n4(1) + 0(-3) \\\\\n0(1) + 1(-3) \\\\\n0(1) + 2(-3)\n\\end{pmatrix} = \\begin{pmatrix}\n4 \\\\\n-3 \\\\\n-6\n\\end{pmatrix}\n$$\nNext, we compute the Euclidean inner product (dot product) of this resulting vector with $\\delta y$:\n$$\n\\langle H'(x_{b})\\,\\delta x, \\delta y \\rangle = \\begin{pmatrix}\n4 \\\\\n-3 \\\\\n-6\n\\end{pmatrix} \\cdot \\begin{pmatrix}\n2 \\\\\n5 \\\\\n-1\n\\end{pmatrix} = 4(2) + (-3)(5) + (-6)(-1) = 8 - 15 + 6 = -1\n$$\n\n**Step 4: Compute the inner product of $\\delta x$ with the adjoint of $H'(x_{b})$ applied to $\\delta y$**\n\nFirst, we apply the adjoint operator $(H'(x_{b}))^{\\top}$ to $\\delta y$:\n$$\n(H'(x_{b}))^{\\top} \\delta y = \\begin{pmatrix}\n4  0  0 \\\\\n0  1  2\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\\n5 \\\\\n-1\n\\end{pmatrix} = \\begin{pmatrix}\n4(2) + 0(5) + 0(-1) \\\\\n0(2) + 1(5) + 2(-1)\n\\end{pmatrix} = \\begin{pmatrix}\n8 \\\\\n5 - 2\n\\end{pmatrix} = \\begin{pmatrix}\n8 \\\\\n3\n\\end{pmatrix}\n$$\nNext, we compute the Euclidean inner product of $\\delta x$ with this resulting vector:\n$$\n\\langle \\delta x, (H'(x_{b}))^{\\top} \\delta y \\rangle = \\begin{pmatrix}\n1 \\\\\n-3\n\\end{pmatrix} \\cdot \\begin{pmatrix}\n8 \\\\\n3\n\\end{pmatrix} = 1(8) + (-3)(3) = 8 - 9 = -1\n$$\nAs expected from the definition of an adjoint operator, the results from Steps $3$ and $4$ are identical. The common scalar value is $-1$.",
            "answer": "$$\n\\boxed{-1}\n$$"
        },
        {
            "introduction": "Real-world data assimilation problems are rarely linear, requiring iterative methods to find the optimal analysis. This practice guides you through the implementation of the Gauss-Newton algorithm, a cornerstone of variational data assimilation for solving nonlinear optimization problems . By iteratively linearizing the problem, you will see firsthand how an optimal solution is approached and how its convergence depends critically on the background information and observation quality.",
            "id": "3116124",
            "problem": "You are asked to derive and implement a Gauss–Newton iteration for a scalar nonlinear data assimilation problem and to demonstrate, by computation, how convergence may succeed or fail depending on the linearization point and the observation error covariance. The fundamental base for this derivation is the weighted least squares principle and first-order Taylor linearization.\n\nConsider a scalar state variable $x \\in \\mathbb{R}$ to be estimated by minimizing a Three-Dimensional Variational (3D-Var) cost function\n$$\nJ(x) = (x - x_b)^2 B^{-1} + \\left(y - h(x)\\right)^2 R^{-1},\n$$\nwhere $x_b \\in \\mathbb{R}$ is the background state, $B \\in \\mathbb{R}_{0}$ is the background error variance, $y \\in \\mathbb{R}$ is the observed value, $R \\in \\mathbb{R}_{0}$ is the observation error variance, and $h(x)$ is a nonlinear observation operator. The Gauss–Newton method results from applying the following fundamental steps: linearize the nonlinear model around the current iterate using the first-order Taylor approximation and minimize the resulting quadratic model using the weighted least squares normal equations. Do not assume any specific \"shortcut\" update formula; instead, derive the update from these principles.\n\nIn this exercise, define the observation operator as $h(x) = x^2$. The task is to implement the Gauss–Newton iteration that, at each step, constructs a linearized observation model around the current iterate and computes the increment that reduces $J(x)$. Use a stopping rule that declares success if the normalized observation misfit\n$$\nm(x) = \\sqrt{\\frac{\\left(y - h(x)\\right)^2}{R}},\n$$\nis less than a tolerance $m_{\\mathrm{tol}}$, or if the absolute update magnitude is less than an increment tolerance $\\varepsilon_{\\mathrm{inc}}$. Declare failure if the iteration stops while $m(x)$ remains above $m_{\\mathrm{tol}}$ or if the maximum number of iterations is reached without satisfying the misfit tolerance. Angles are not involved in this problem, hence no angle unit is required. There are no physical units in this problem.\n\nImplement your program to handle the following test suite, each test case specified by the tuple $(y, x_b, B, R, x_0, \\varepsilon_{\\mathrm{inc}}, m_{\\mathrm{tol}}, N_{\\max})$:\n\n- Happy-path convergence toward a positive root: $(y, x_b, B, R, x_0, \\varepsilon_{\\mathrm{inc}}, m_{\\mathrm{tol}}, N_{\\max}) = (\\,4,\\,1.5,\\,1,\\,0.25,\\,1,\\,10^{-8},\\,10^{-6},\\,50\\,)$.\n- Convergence toward a negative root guided by the background: $(\\,4,\\,-2.5,\\,0.5,\\,0.25,\\,-1,\\,10^{-8},\\,10^{-6},\\,50\\,)$.\n- Failure due to a zero-derivative linearization point and a matching background: $(\\,1,\\,0,\\,1,\\,0.25,\\,0,\\,10^{-8},\\,10^{-6},\\,50\\,)$.\n- Failure caused by a very large observation error variance (observations weakly trusted): $(\\,9,\\,-1,\\,0.1,\\,1000,\\,-1,\\,10^{-8},\\,10^{-6},\\,50\\,)$.\n- Successful convergence with strongly trusted observations and weakly trusted background: $(\\,9,\\,0,\\,10,\\,0.01,\\,0.5,\\,10^{-8},\\,10^{-6},\\,50\\,)$.\n\nYour program must, for each test case, perform the Gauss–Newton iteration using first-order linearization at the current iterate. For each test case, return a list containing: a boolean indicating success ($\\mathrm{True}$ or $\\mathrm{False}$), the final estimate $x$ as a floating-point number, the number of iterations performed as an integer, and the final normalized observation misfit $m(x)$ as a floating-point number. Aggregate the results of all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each test case result is itself a list in the same bracketed, comma-separated format (e.g., [[True,2.0,12,0.0],...]).",
            "solution": "The problem requires the derivation and implementation of the Gauss-Newton method to solve a scalar nonlinear data assimilation problem. The objective is to find the state variable $x \\in \\mathbb{R}$ that minimizes the 3D-Var cost function:\n$$\nJ(x) = (x - x_b)^2 B^{-1} + \\left(y - h(x)\\right)^2 R^{-1}\n$$\nwhere $x_b$ is the background state, $B  0$ is the background error variance, $y$ is the observation, $R  0$ is the observation error variance, and $h(x)=x^2$ is the nonlinear observation operator.\n\nThe solution is derived by applying the Gauss-Newton algorithm, which consists of iteratively linearizing the nonlinear model and solving the resulting linear least-squares problem.\n\n**Step 1: Linearization of the Cost Function**\n\nThe Gauss-Newton method approximates the nonlinear function $h(x)$ at each iteration $k$ with a first-order Taylor expansion around the current estimate $x_k$. We seek an increment $\\delta x$ such that the next estimate $x_{k+1} = x_k + \\delta x$ moves closer to the minimum of $J(x)$.\n\nThe linearization of $h(x)$ around $x_k$ is:\n$$\nh(x_{k+1}) = h(x_k + \\delta x) \\approx h(x_k) + H_k \\delta x\n$$\nwhere $H_k$ is the derivative (Jacobian) of $h(x)$ with respect to $x$, evaluated at $x_k$. For the given observation operator $h(x) = x^2$, the derivative is $h'(x) = 2x$. Therefore, the Jacobian at $x_k$ is:\n$$\nH_k = 2x_k\n$$\nSubstituting the linearized observation operator into the cost function $J(x)$, we obtain a quadratic approximation of the cost function with respect to the increment $\\delta x$, denoted as $J_k(\\delta x)$:\n$$\nJ_k(\\delta x) = (x_k + \\delta x - x_b)^2 B^{-1} + \\left(y - (h(x_k) + H_k \\delta x)\\right)^2 R^{-1}\n$$\nThis can be rewritten to emphasize the terms involving $\\delta x$:\n$$\nJ_k(\\delta x) = ((x_k - x_b) + \\delta x)^2 B^{-1} + ((y - h(x_k)) - H_k \\delta x)^2 R^{-1}\n$$\n\n**Step 2: Minimization and Derivation of the Increment**\n\nTo find the increment $\\delta x$ that minimizes this quadratic function $J_k(\\delta x)$, we compute its derivative with respect to $\\delta x$ and set it to zero.\n$$\n\\frac{dJ_k}{d(\\delta x)} = \\frac{d}{d(\\delta x)} \\left[ ((x_k - x_b) + \\delta x)^2 B^{-1} + ((y - h(x_k)) - H_k \\delta x)^2 R^{-1} \\right] = 0\n$$\nApplying the chain rule, we obtain:\n$$\n2((x_k - x_b) + \\delta x) \\cdot B^{-1} + 2((y - h(x_k)) - H_k \\delta x) \\cdot (-H_k) \\cdot R^{-1} = 0\n$$\nDividing by $2$ and expanding the terms:\n$$\nB^{-1}(x_k - x_b) + B^{-1}\\delta x - R^{-1}H_k(y - h(x_k)) + R^{-1}H_k^2 \\delta x = 0\n$$\nNow, we group the terms containing $\\delta x$ on one side of the equation and the remaining terms on the other:\n$$\n(B^{-1} + R^{-1}H_k^2) \\delta x = R^{-1}H_k(y - h(x_k)) - B^{-1}(x_k - x_b)\n$$\nTo solve for $\\delta x$, we can multiply the entire equation by $BR$ to clear the denominators, which is permissible since $B  0$ and $R  0$:\n$$\n(BR)(B^{-1} + R^{-1}H_k^2) \\delta x = (BR)(R^{-1}H_k(y - h(x_k)) - B^{-1}(x_k - x_b))\n$$\n$$\n(R + B H_k^2) \\delta x = B H_k(y - h(x_k)) - R(x_k - x_b)\n$$\nFinally, isolating the increment $\\delta x$ yields the update formula:\n$$\n\\delta x = \\frac{B H_k(y - h(x_k)) - R(x_k - x_b)}{R + B H_k^2}\n$$\n\n**Step 3: The Iterative Algorithm**\n\nThe complete Gauss-Newton iteration algorithm is as follows:\n1.  **Initialization**: Start with an initial guess $x_0$ at iteration $k=0$.\n2.  **Iteration**: For $k = 0, 1, 2, \\dots, N_{\\max}-1$:\n    a.  Calculate the observation operator value $h(x_k) = x_k^2$ and the Jacobian $H_k = 2x_k$.\n    b.  Compute the increment $\\delta x$ using the derived formula:\n        $$\n        \\delta x = \\frac{B (2x_k)(y - x_k^2) - R(x_k - x_b)}{R + B (2x_k)^2}\n        $$\n    c.  Update the state estimate: $x_{k+1} = x_k + \\delta x$.\n    d.  Check stopping criteria:\n        i.  **Successful Convergence (Misfit)**: The iteration is a success and terminates if the normalized observation misfit $m(x_{k+1}) = \\sqrt{(y - h(x_{k+1}))^2 / R}$ is less than a tolerance $m_{\\mathrm{tol}}$.\n        ii.  **Termination (Increment)**: The iteration terminates if the absolute magnitude of the update $|\\delta x|$ is less than a tolerance $\\varepsilon_{\\mathrm{inc}}$. The outcome is a success if $m(x_{k+1})  m_{\\mathrm{tol}}$ and a failure otherwise.\n3.  **Termination (Max Iterations)**: If the loop completes after $N_{\\max}$ iterations without meeting a success criterion, the process terminates and is considered a failure.\n\nThe final output for each test case will be a list containing a boolean for success/failure, the final estimate $x$, the total number of iterations performed, and the final normalized misfit $m(x)$.",
            "answer": "```python\nimport numpy as np\n\ndef gauss_newton_scalar(params):\n    \"\"\"\n    Performs Gauss-Newton iteration for a scalar 3D-Var problem.\n    \n    Args:\n        params (tuple): A tuple containing (y, xb, B, R, x0, eps_inc, m_tol, N_max).\n    \n    Returns:\n        list: [is_success, final_x, num_iterations, final_misfit]\n    \"\"\"\n    y, xb, B, R, x0, eps_inc, m_tol, N_max = params\n    \n    x_k = float(x0) # Ensure initial guess is a float\n    \n    # Check for convergence on the initial state (0 iterations)\n    h_x0 = x_k**2\n    # The problem statement guarantees R  0, so no division by zero here.\n    m_x0 = np.sqrt((y - h_x0)**2 / R)\n    if m_x0  m_tol:\n        return [True, x_k, 0, m_x0]\n\n    for k in range(N_max):\n        # Current number of iterations performed to reach the next state is k+1\n        num_iters = k + 1\n\n        # Calculate values at the current iterate x_k\n        h_xk = x_k**2\n        H_k = 2.0 * x_k\n        \n        # Calculate the increment delta_x from the derived formula\n        numerator = B * H_k * (y - h_xk) - R * (x_k - xb)\n        denominator = R + B * H_k**2\n        \n        # Since R  0 and B  0, the denominator is always positive.\n        delta_x = numerator / denominator\n        \n        # Update the state\n        x_k_plus_1 = x_k + delta_x\n\n        # Calculate misfit at the new state\n        m_xk_plus_1 = np.sqrt((y - x_k_plus_1**2)**2 / R)\n\n        # Check stopping condition 1: small increment\n        if np.abs(delta_x)  eps_inc:\n            is_success = m_xk_plus_1  m_tol\n            return [is_success, x_k_plus_1, num_iters, m_xk_plus_1]\n            \n        # Check stopping condition 2: misfit tolerance met\n        if m_xk_plus_1  m_tol:\n            return [True, x_k_plus_1, num_iters, m_xk_plus_1]\n        \n        # Prepare for the next iteration\n        x_k = x_k_plus_1\n\n    # If the loop completes, it's a failure due to max iterations\n    # The final state is the last computed x_k, and num_iters is N_max\n    m_final = np.sqrt((y - x_k**2)**2 / R)\n    # The success condition will be false if execution reaches here, as the misfit check inside the loop would have already caught a success.\n    is_success = m_final  m_tol\n    return [is_success, x_k, N_max, m_final]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy-path convergence toward a positive root\n        (4.0, 1.5, 1.0, 0.25, 1.0, 1e-8, 1e-6, 50),\n        # Case 2: Convergence toward a negative root guided by the background\n        (4.0, -2.5, 0.5, 0.25, -1.0, 1e-8, 1e-6, 50),\n        # Case 3: Failure due to a zero-derivative linearization point\n        (1.0, 0.0, 1.0, 0.25, 0.0, 1e-8, 1e-6, 50),\n        # Case 4: Failure caused by a very large observation error variance\n        (9.0, -1.0, 0.1, 1000.0, -1.0, 1e-8, 1e-6, 50),\n        # Case 5: Successful convergence with strongly trusted observations\n        (9.0, 0.0, 10.0, 0.01, 0.5, 1e-8, 1e-6, 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = gauss_newton_scalar(case)\n        # Format the result for the inner list: [bool,float,int,float]\n        results.append(f\"[{str(result[0])},{float(result[1])},{int(result[2])},{float(result[3])}]\")\n\n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An alternative to solving for the state \"all-at-once\" is sequential assimilation, where observations are incorporated as they arrive. This exercise explores whether the final result depends on the order in which observations are processed . By implementing a serial filter, you will discover that while the order is irrelevant for linear systems, the approximations required for nonlinear systems cause the final estimate to become order-dependent, a subtle but profound concept with major practical implications.",
            "id": "3116062",
            "problem": "You are given a finite-dimensional state vector $\\,\\mathbf{x}\\in\\mathbb{R}^n\\,$ with a Gaussian prior distribution characterized by mean $\\,\\boldsymbol{\\mu}_0\\,$ and covariance $\\,\\mathbf{P}_0\\,$. A set of observational data consists of scalar observations $\\,y_i\\,$ of the form $\\,y_i = h_i(\\mathbf{x}) + \\varepsilon_i\\,$, where $\\,h_i\\,$ is an observation operator and $\\,\\varepsilon_i\\,$ is observation noise. Assume $\\,\\varepsilon_i\\,$ are mutually independent, each distributed as a Gaussian with zero mean and variance $\\,r_i\\,$, so the observation error covariance matrix $\\,\\mathbf{R}\\,$ is diagonal in the multi-observation sense. In serial data assimilation, observations are incorporated one at a time to update the state. This problem asks you to implement serial assimilation in two scenarios: with linear observation operators and with nonlinear observation operators. You must compare two different serial orders of assimilation and test whether the final posterior (mean and covariance) is invariant to the order of assimilation.\n\nStart from the fundamental base of Bayes’ rule, namely $\\,p(\\mathbf{x}\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid \\mathbf{x})\\,p(\\mathbf{x})\\,$, together with the assumptions of Gaussian prior and Gaussian observation errors. For linear observation operators, use the foundational facts that the product of independent Gaussian likelihoods commutes and that the posterior under linear-Gaussian assumptions is Gaussian. For nonlinear observation operators, use a first-order Taylor approximation of $\\,h(\\mathbf{x})\\,$ about the current mean to obtain a locally linear update (the Extended Kalman Filter on first appearance shall be spelled out as Extended Kalman Filter (EKF)). Your program must implement serial assimilation updates derived from these principles without assuming or using any shortcut formula provided explicitly in the problem statement.\n\nYour program must:\n- Represent the state and covariance as $\\,\\boldsymbol{\\mu}\\,$ and $\\,\\mathbf{P}\\,$.\n- For each observation $\\,y\\,$ with operator $\\,h(\\cdot)\\,$ and noise variance $\\,r\\,$, perform a single-assimilation update step.\n- For linear operators $\\,h(\\mathbf{x})=\\mathbf{H}\\mathbf{x}\\,$, treat $\\,\\mathbf{H}\\,$ as a row vector and update $\\,(\\boldsymbol{\\mu},\\mathbf{P})\\,$ accordingly.\n- For nonlinear operators, use EKF-style local linearization: $\\,h(\\mathbf{x}) \\approx h(\\boldsymbol{\\mu}) + \\mathbf{H}(\\boldsymbol{\\mu}) (\\mathbf{x}-\\boldsymbol{\\mu})\\,$, where $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ is the Jacobian evaluated at the current mean, and then perform a linear-Gaussian update.\n- Execute two serial orders for each test case, compute the final posterior mean and covariance from each order, and determine whether the results are invariant to order within a strict numerical tolerance $\\,\\epsilon\\,$.\n\nUse the following test suite and parameters. In all cases, the measurement errors are independent, so the observation error covariance $\\,\\mathbf{R}\\,$ is diagonal, with individual variances $\\,r_i\\,$ specified below. For each test, compare the two serial orders $\\,(\\text{order } A)\\,$ and $\\,(\\text{order } B)\\,$. Report order invariance as a boolean that is $\\,\\text{True}\\,$ if both the maximum absolute difference in the posterior means and the Frobenius norm difference in the posterior covariances between the two orders are less than or equal to $\\,\\epsilon\\,$, and $\\,\\text{False}\\,$ otherwise. Use numerical tolerance $\\,\\epsilon = 10^{-10}\\,$.\n\nTest case $\\,1\\,$ (linear, happy path):\n- State dimension $\\,n = 2\\,$.\n- Prior $\\,\\boldsymbol{\\mu}_0 = [\\,1.0,\\,-2.0\\,]\\,$, $\\,\\mathbf{P}_0 = \\begin{bmatrix} 2.0  0.5 \\\\ 0.5  1.0 \\end{bmatrix}\\,$.\n- Observation $\\,1\\,$: $\\,y_1 = 0.9\\,$, $\\,h_1(\\mathbf{x}) = [\\,1,\\,0\\,]\\mathbf{x}\\,$, $\\,r_1 = 0.4\\,$.\n- Observation $\\,2\\,$: $\\,y_2 = -1.8\\,$, $\\,h_2(\\mathbf{x}) = [\\,2,\\,-1\\,]\\mathbf{x}\\,$, $\\,r_2 = 0.3\\,$.\n- Orders: $\\,(\\text{order } A) = (1,2)\\,$, $\\,(\\text{order } B) = (2,1)\\,$.\nExpected behavior: for linear $\\,\\mathbf{H}\\,$ and diagonal $\\,\\mathbf{R}\\,$, order invariance should hold.\n\nTest case $\\,2\\,$ (linear, boundary condition with one weak observation):\n- State dimension $\\,n = 1\\,$.\n- Prior $\\,\\mu_0 = 0.0\\,$, $\\,P_0 = 1.0\\,$.\n- Observation $\\,1\\,$: $\\,y_1 = 1.0\\,$, $\\,h_1(x) = x\\,$, $\\,r_1 = 0.5\\,$.\n- Observation $\\,2\\,$: $\\,y_2 = -2.0\\,$, $\\,h_2(x) = x\\,$, $\\,r_2 = 1000.0\\,$.\n- Orders: $\\,(\\text{order } A) = (1,2)\\,$, $\\,(\\text{order } B) = (2,1)\\,$.\nExpected behavior: with a very noisy second observation and diagonal $\\,\\mathbf{R}\\,$, order invariance should hold.\n\nTest case $\\,3\\,$ (nonlinear, mild nonlinearity):\n- State dimension $\\,n = 1\\,$.\n- Prior $\\,\\mu_0 = 0.5\\,$, $\\,P_0 = 0.5\\,$.\n- Observation $\\,1\\,$: $\\,y_1 = 0.0\\,$, $\\,h_1(x) = \\sin(x)\\,$, $\\,r_1 = 0.05\\,$.\n- Observation $\\,2\\,$: $\\,y_2 = 1.0\\,$, $\\,h_2(x) = x^2\\,$, $\\,r_2 = 0.05\\,$.\n- Orders: $\\,(\\text{order } A) = (1,2)\\,$, $\\,(\\text{order } B) = (2,1)\\,$.\nExpected behavior: with EKF-style linearization at the current mean, order invariance generally breaks because the linearization points differ.\n\nTest case $\\,4\\,$ (nonlinear, stronger nonlinearity):\n- State dimension $\\,n = 1\\,$.\n- Prior $\\,\\mu_0 = -1.2\\,$, $\\,P_0 = 0.2\\,$.\n- Observation $\\,1\\,$: $\\,y_1 = 1.0\\,$, $\\,h_1(x) = \\exp(x)\\,$, $\\,r_1 = 0.1\\,$.\n- Observation $\\,2\\,$: $\\,y_2 = -0.5\\,$, $\\,h_2(x) = \\tanh(x)\\,$, $\\,r_2 = 0.1\\,$.\n- Orders: $\\,(\\text{order } A) = (1,2)\\,$, $\\,(\\text{order } B) = (2,1)\\,$.\nExpected behavior: with EKF-style linearization, order invariance breaks due to different linearization points and pronounced nonlinearity.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the four test cases in the order listed above, output $\\,\\text{[result}_1\\text{,result}_2\\text{,result}_3\\text{,result}_4\\text{]}\\,$, where each $\\,\\text{result}_i\\,$ is a boolean $\\,\\text{True}\\,$ or $\\,\\text{False}\\,$ computed by comparing the two orders as specified, using tolerance $\\,\\epsilon = 10^{-10}\\,$.",
            "solution": "We begin from Bayes’ rule, $\\,p(\\mathbf{x}\\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid \\mathbf{x})\\,p(\\mathbf{x})\\,$, and the assumptions that the prior is Gaussian, $\\,\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu}_0,\\mathbf{P}_0)\\,$, and each scalar observation has independent Gaussian noise, $\\,y_i = h_i(\\mathbf{x}) + \\varepsilon_i\\,$ with $\\,\\varepsilon_i\\sim\\mathcal{N}(0,r_i)\\,$. For serial assimilation, we incorporate one observation at a time, updating the mean and covariance after each observation.\n\nUnder linear observation operators, $\\,h(\\mathbf{x}) = \\mathbf{H}\\mathbf{x}\\,$ with $\\,\\mathbf{H}\\,$ a row vector and $\\,y\\,$ scalar, the likelihood is $\\,p(y\\mid \\mathbf{x}) \\propto \\exp\\!\\left(-\\frac{1}{2r}(y-\\mathbf{H}\\mathbf{x})^2\\right)\\,$. The posterior after one observation remains Gaussian. The Kalman-style update emerges from completing the square in the exponent of the product $\\,p(y\\mid \\mathbf{x})\\,p(\\mathbf{x})\\,$. In particular, the posterior mean $\\,\\boldsymbol{\\mu}^+\\,$ and covariance $\\,\\mathbf{P}^+\\,$ are obtained by combining $\\,\\mathbf{P}^{-1}\\,$ and $\\,\\mathbf{H}^T r^{-1} \\mathbf{H}\\,$ in the precision domain and the linear term $\\,\\mathbf{H}^T r^{-1} y\\,$ in the information vector. Equivalently, the familiar Kalman gain expression may be derived:\n$$\n\\mathbf{K} = \\mathbf{P}\\mathbf{H}^T\\left(\\mathbf{H}\\mathbf{P}\\mathbf{H}^T + r\\right)^{-1},\n$$\nand the updates are\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\mathbf{K}\\left(y - \\mathbf{H}\\boldsymbol{\\mu}\\right),\\quad\n\\mathbf{P}^+ = \\left(\\mathbf{I} - \\mathbf{K}\\mathbf{H}\\right)\\mathbf{P}\\left(\\mathbf{I} - \\mathbf{K}\\mathbf{H}\\right)^T + \\mathbf{K} r \\mathbf{K}^T,\n$$\nwhere the latter is the Joseph form that preserves symmetry and positive semidefiniteness numerically.\n\nFor multiple independent observations $\\,\\{y_i\\}\\,$ with linear operators $\\,\\{\\mathbf{H}_i\\}\\,$ and variances $\\,\\{r_i\\}\\,$, the joint likelihood factors as a product $\\,\\prod_i p(y_i\\mid \\mathbf{x})\\,$. Because multiplication is commutative, $\\,\\prod_i p(y_i\\mid \\mathbf{x})\\,$ is invariant to observation ordering. In the Gaussian-linear case, serial assimilation replicates the batch result found by multiplying all likelihoods at once, so the final posterior $\\,(\\boldsymbol{\\mu}^+,\\mathbf{P}^+)\\,$ does not depend on the order. Therefore, with diagonal $\\,\\mathbf{R}\\,$ and linear $\\,\\mathbf{H}\\,$, order invariance holds.\n\nFor nonlinear observation operators, $\\,h(\\mathbf{x})\\,$, we adopt the Extended Kalman Filter (EKF) approach: at the current mean $\\,\\boldsymbol{\\mu}\\,$, approximate $\\,h(\\mathbf{x})\\,$ by its first-order Taylor expansion,\n$$\nh(\\mathbf{x}) \\approx h(\\boldsymbol{\\mu}) + \\mathbf{H}(\\boldsymbol{\\mu}) (\\mathbf{x}-\\boldsymbol{\\mu}),\n$$\nwhere $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ is the Jacobian of $\\,h\\,$ evaluated at $\\,\\boldsymbol{\\mu}\\,$. Using this local linearization in place of $\\,h\\,$, we perform a linear-Gaussian update with effective observation model $\\,y \\approx h(\\boldsymbol{\\mu}) + \\mathbf{H}(\\boldsymbol{\\mu})(\\mathbf{x}-\\boldsymbol{\\mu})\\,$. The EKF update formulas mirror the linear case but with $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ and $\\,h(\\boldsymbol{\\mu})\\,$ replacing $\\,\\mathbf{H}\\,$ and $\\,\\mathbf{H}\\boldsymbol{\\mu}\\,$, respectively.\n\nCritically, serial EKF assimilation linearizes $\\,h\\,$ at the current posterior mean before each update. If observations are assimilated in different orders, the sequence of posterior means changes, and thus the linearization points $\\,\\boldsymbol{\\mu}\\,$ change. Because $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ and $\\,h(\\boldsymbol{\\mu})\\,$ depend on $\\,\\boldsymbol{\\mu}\\,$ in nonlinear ways, the cumulative effect of two serial updates generally depends on the order. Hence order invariance is not guaranteed and often fails.\n\nAlgorithmic design:\n- Represent the state mean $\\,\\boldsymbol{\\mu}\\,$ and covariance $\\,\\mathbf{P}\\,$.\n- For a linear observation with row vector $\\,\\mathbf{H}\\,$, scalar $\\,y\\,$, variance $\\,r\\,$:\n  - Compute $\\,\\mathbf{K} = \\mathbf{P}\\mathbf{H}^T(\\mathbf{H}\\mathbf{P}\\mathbf{H}^T + r)^{-1}\\,$.\n  - Update $\\,\\boldsymbol{\\mu} \\leftarrow \\boldsymbol{\\mu} + \\mathbf{K}(y - \\mathbf{H}\\boldsymbol{\\mu})\\,$.\n  - Update $\\,\\mathbf{P} \\leftarrow (\\mathbf{I}-\\mathbf{K}\\mathbf{H})\\mathbf{P}(\\mathbf{I}-\\mathbf{K}\\mathbf{H})^T + \\mathbf{K} r \\mathbf{K}^T\\,$.\n- For a nonlinear observation with $\\,h\\,$ and Jacobian $\\,\\mathbf{H}(\\cdot)\\,$:\n  - Evaluate $\\,h(\\boldsymbol{\\mu})\\,$ and $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$.\n  - Apply the same linear update using $\\,h(\\boldsymbol{\\mu})\\,$ in the innovation and $\\,\\mathbf{H}(\\boldsymbol{\\mu})\\,$ as the linear operator.\n\nFor each test case, run the two serial orders and compute:\n- The maximum absolute difference in final means: $\\,\\Delta_\\mu = \\max_j |\\,\\mu_j^{(A)} - \\mu_j^{(B)}\\,|\\,$.\n- The Frobenius norm difference in final covariances: $\\,\\Delta_P = \\lVert \\mathbf{P}^{(A)} - \\mathbf{P}^{(B)} \\rVert_F\\,$.\nDeclare order invariance $\\,\\text{True}\\,$ if $\\,\\Delta_\\mu \\le \\epsilon\\,$ and $\\,\\Delta_P \\le \\epsilon\\,$, else $\\,\\text{False}\\,$, using $\\,\\epsilon = 10^{-10}\\,$.\n\nInterpretation for the provided tests:\n- Test $\\,1\\,$ and Test $\\,2\\,$ use linear $\\,\\mathbf{H}\\,$ and diagonal $\\,\\mathbf{R}\\,$, so the final posteriors from orders $\\,A\\,$ and $\\,B\\,$ must match within numerical tolerance, yielding $\\,\\text{True}\\,$.\n- Test $\\,3\\,$ and Test $\\,4\\,$ use nonlinear $\\,h\\,$ with EKF-style linearization; assimilating $\\,y_1\\,$ then $\\,y_2\\,$ produces different intermediate means than assimilating $\\,y_2\\,$ then $\\,y_1\\,$, thus different linearization points and generally different final posteriors, yielding $\\,\\text{False}\\,$.\n\nThe program implements these updates and prints a single line $\\,\\text{[result}_1\\text{,result}_2\\text{,result}_3\\text{,result}_4\\text{]}\\,$, where each $\\,\\text{result}_i\\,$ is the boolean outcome for the corresponding test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef kalman_update_linear(mean, cov, H, y, r):\n    \"\"\"\n    Perform a single linear-Gaussian update with scalar observation.\n    mean: (n,) state mean\n    cov: (n,n) state covariance\n    H: (n,) observation row vector\n    y: scalar observation\n    r: scalar observation variance\n    Returns updated (mean, cov)\n    \"\"\"\n    H = H.reshape(-1)  # ensure 1D\n    n = mean.shape[0]\n    S = float(H @ cov @ H.T + r)  # innovation variance (scalar)\n    K = (cov @ H.T) / S           # gain shape (n,)\n    innovation = y - float(H @ mean)\n    mean_upd = mean + K * innovation\n    I = np.eye(n)\n    KH = np.outer(K, H)  # (n,n)\n    # Joseph form for numerical stability\n    cov_upd = (I - KH) @ cov @ (I - KH).T + np.outer(K, K) * r\n    return mean_upd, cov_upd\n\ndef ekf_update(mean, cov, h_func, jacobian_func, y, r):\n    \"\"\"\n    Perform a single EKF-style update with scalar observation.\n    mean: (n,) state mean\n    cov: (n,n) state covariance\n    h_func: function h(x) - scalar\n    jacobian_func: function J(x) - (n,) row vector (Jacobian of h at x)\n    y: scalar observation\n    r: scalar observation variance\n    Returns updated (mean, cov)\n    \"\"\"\n    H = np.array(jacobian_func(mean))  # (n,)\n    y_pred = float(h_func(mean))       # scalar\n    # Re-using the linear Kalman update logic with the linearized operator\n    # The innovation y - h(mu) is used instead of y - H*mu\n    innovation = y - y_pred\n    S = float(H @ cov @ H.T + r)\n    K = (cov @ H.T) / S\n    mean_upd = mean + K * innovation\n    n = mean.shape[0]\n    I = np.eye(n)\n    KH = np.outer(K, H)\n    cov_upd = (I - KH) @ cov @ (I - KH).T + np.outer(K, K) * r\n    return mean_upd, cov_upd\n\ndef assimilate_serial(mu0, P0, observations, order):\n    \"\"\"\n    Assimilate observations serially in the given order.\n    observations: list of dicts describing each observation\n    order: list of indices indicating assimilation order\n    Returns (mean, cov)\n    \"\"\"\n    mean = mu0.copy()\n    cov = P0.copy()\n    for idx in order:\n        obs = observations[idx]\n        if obs['type'] == 'linear':\n            mean, cov = kalman_update_linear(mean, cov, obs['H'], obs['y'], obs['r'])\n        elif obs['type'] == 'nonlinear':\n            # Note: The EKF update differs from the linear one in the innovation term.\n            # A more direct implementation is clearer.\n            H = np.array(obs['jacobian'](mean))\n            y_pred = float(obs['h'](mean))\n            S = float(H @ cov @ H.T + obs['r'])\n            K = (cov @ H.T) / S\n            innovation = obs['y'] - y_pred\n            mean = mean + K * innovation\n            n = mean.shape[0]\n            I = np.eye(n)\n            KH = np.outer(K, H)\n            cov = (I - KH) @ cov @ (I - KH).T + np.outer(K, K) * obs['r']\n        else:\n            raise ValueError(\"Unknown observation type\")\n    return mean, cov\n\ndef solve():\n    # Numerical tolerance for invariance\n    EPS = 1e-10\n\n    # Define the test cases from the problem statement.\n\n    # Test case 1: linear, n=2\n    mu0_1 = np.array([1.0, -2.0])\n    P0_1 = np.array([[2.0, 0.5],\n                     [0.5, 1.0]])\n    obs1_case1 = {\n        'type': 'linear',\n        'H': np.array([1.0, 0.0]),\n        'y': 0.9,\n        'r': 0.4\n    }\n    obs2_case1 = {\n        'type': 'linear',\n        'H': np.array([2.0, -1.0]),\n        'y': -1.8,\n        'r': 0.3\n    }\n    observations_case1 = [obs1_case1, obs2_case1]\n    orderA_case1 = [0, 1]\n    orderB_case1 = [1, 0]\n\n    # Test case 2: linear, n=1\n    mu0_2 = np.array([0.0])\n    P0_2 = np.array([[1.0]])\n    obs1_case2 = {\n        'type': 'linear',\n        'H': np.array([1.0]),\n        'y': 1.0,\n        'r': 0.5\n    }\n    obs2_case2 = {\n        'type': 'linear',\n        'H': np.array([1.0]),\n        'y': -2.0,\n        'r': 1000.0\n    }\n    observations_case2 = [obs1_case2, obs2_case2]\n    orderA_case2 = [0, 1]\n    orderB_case2 = [1, 0]\n\n    # Test case 3: nonlinear, n=1\n    mu0_3 = np.array([0.5])\n    P0_3 = np.array([[0.5]])\n\n    def h1_case3(x):\n        return np.sin(x[0])\n\n    def J1_case3(x):\n        return np.array([np.cos(x[0])])\n\n    def h2_case3(x):\n        return x[0] ** 2\n\n    def J2_case3(x):\n        return np.array([2.0 * x[0]])\n\n    obs1_case3 = {\n        'type': 'nonlinear',\n        'h': h1_case3,\n        'jacobian': J1_case3,\n        'y': 0.0,\n        'r': 0.05\n    }\n    obs2_case3 = {\n        'type': 'nonlinear',\n        'h': h2_case3,\n        'jacobian': J2_case3,\n        'y': 1.0,\n        'r': 0.05\n    }\n    observations_case3 = [obs1_case3, obs2_case3]\n    orderA_case3 = [0, 1]\n    orderB_case3 = [1, 0]\n\n    # Test case 4: nonlinear, n=1\n    mu0_4 = np.array([-1.2])\n    P0_4 = np.array([[0.2]])\n\n    def h1_case4(x):\n        return np.exp(x[0])\n\n    def J1_case4(x):\n        return np.array([np.exp(x[0])])\n\n    def h2_case4(x):\n        return np.tanh(x[0])\n\n    def J2_case4(x):\n        t = np.tanh(x[0])\n        return np.array([1.0 - t * t])  # sech^2(x) = 1 - tanh^2(x)\n\n    obs1_case4 = {\n        'type': 'nonlinear',\n        'h': h1_case4,\n        'jacobian': J1_case4,\n        'y': 1.0,\n        'r': 0.1\n    }\n    obs2_case4 = {\n        'type': 'nonlinear',\n        'h': h2_case4,\n        'jacobian': J2_case4,\n        'y': -0.5,\n        'r': 0.1\n    }\n    observations_case4 = [obs1_case4, obs2_case4]\n    orderA_case4 = [0, 1]\n    orderB_case4 = [1, 0]\n\n    test_cases = [\n        (mu0_1, P0_1, observations_case1, orderA_case1, orderB_case1),\n        (mu0_2, P0_2, observations_case2, orderA_case2, orderB_case2),\n        (mu0_3, P0_3, observations_case3, orderA_case3, orderB_case3),\n        (mu0_4, P0_4, observations_case4, orderA_case4, orderB_case4),\n    ]\n\n    results = []\n    for mu0, P0, observations, orderA, orderB in test_cases:\n        muA, PA = assimilate_serial(mu0, P0, observations, orderA)\n        muB, PB = assimilate_serial(mu0, P0, observations, orderB)\n        # Compare means and covariances\n        mean_diff = np.max(np.abs(muA - muB))\n        cov_diff = np.linalg.norm(PA - PB, ord='fro')\n        invariant = (mean_diff = EPS) and (cov_diff = EPS)\n        results.append(str(invariant))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}