{
    "hands_on_practices": [
        {
            "introduction": "Regularization is fundamentally about managing the trade-off between accuracy and stability. This practice delves into this core concept by exploring how the choice of the truncation level $k$ in Truncated SVD (TSVD) affects the solution's error. You will use a Monte Carlo simulation to decompose the Mean Squared Error (MSE) into its two constituent parts: squared bias and variance, providing a quantitative look at the classic bias-variance trade-off . By observing how bias decreases and variance increases as you include more singular components, you will gain a foundational understanding of why an optimal, intermediate truncation level often yields the best result.",
            "id": "3201027",
            "problem": "Consider the linear inverse problem with a forward operator constructed from a finite-difference approximation of the first derivative. Let $n$ be the number of grid points on $[0,1]$ with uniform spacing $h = 1/(n-1)$ and grid locations $t_i = (i-1)h$ for $i = 1,2,\\dots,n$. Define the $(n-1) \\times n$ matrix $A$ by the discrete first-derivative stencil\n$$\nA_{i,i} = -\\frac{1}{h}, \\quad A_{i,i+1} = \\frac{1}{h}, \\quad \\text{for } i=1,2,\\dots,n-1,\n$$\nand all other entries are zero. Let the true signal be the vector $x_{\\text{true}} \\in \\mathbb{R}^n$ with components\n$$\nx_{\\text{true},i} = \\sin(2\\pi t_i) + 0.1\\, t_i,\n$$\nwhere the sine function uses radians as the angle unit. Observations are modeled by\n$$\nb = A x_{\\text{true}} + \\varepsilon,\n$$\nwhere $\\varepsilon \\in \\mathbb{R}^{n-1}$ is additive observational noise with independent and identically distributed components of zero mean and a specified standard deviation.\n\nLet $A$ admit the singular value decomposition (SVD) $A = U \\Sigma V^T$ with $U \\in \\mathbb{R}^{(n-1)\\times(n-1)}$, $V \\in \\mathbb{R}^{n\\times(n-1)}$ having orthonormal columns, and $\\Sigma \\in \\mathbb{R}^{(n-1)\\times(n-1)}$ diagonal with nonnegative singular values on the diagonal. Consider the truncated singular value decomposition (TSVD) estimator $x_k$ obtained by using only the leading $k$ singular triplets for $k \\in \\{0,1,\\dots,n-1\\}$.\n\nYour task is to implement a program that, for each test case specified below, constructs $A$ and $x_{\\text{true}}$, draws $L$ independent noise realizations $\\varepsilon$ from a zero-mean Gaussian distribution with the given standard deviation, and computes the TSVD solution $x_k$ for each specified truncation level $k$. Using Monte Carlo sampling over the $L$ realizations, estimate:\n1. The squared bias defined by\n$$\n\\| \\mathbb{E}[x_k] - x_{\\text{true}} \\|_2^2,\n$$\nwhere the expectation is over the noise distribution.\n2. The squared variance defined by\n$$\n\\mathbb{E}\\left[ \\| x_k - \\mathbb{E}[x_k] \\|_2^2 \\right].\n$$\n3. The mean squared error (MSE) defined by the sum of the squared bias and squared variance.\n\nFor each test case, identify and report the truncation level $k$ (an integer) that minimizes the empirical mean squared error computed from the Monte Carlo samples. If there is a tie, choose the smallest such $k$. To ensure reproducibility, initialize the random number generator with a fixed seed of $0$ before generating noise for each test case.\n\nTest suite:\n- Case 1 (general case): $n = 50$, noise standard deviation $= 10^{-2}$, number of samples $L = 400$, candidate truncation levels $k \\in \\{0,1,5,10,20,49\\}$.\n- Case 2 (low-noise boundary): $n = 50$, noise standard deviation $= 10^{-6}$, number of samples $L = 400$, candidate truncation levels $k \\in \\{0,1,5,10,20,49\\}$.\n- Case 3 (high-noise edge): $n = 50$, noise standard deviation $= 5\\times 10^{-2}$, number of samples $L = 400$, candidate truncation levels $k \\in \\{0,1,5,10,20,49\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the best truncation levels for the three test cases as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$, where each $k_i$ is the integer minimizer of the empirical mean squared error for the corresponding case.",
            "solution": "We start from the linear inverse problem $b = A x_{\\text{true}} + \\varepsilon$, where $A$ is a discrete first-derivative operator built from a uniform grid. The singular value decomposition (SVD) factorizes $A$ as $A = U \\Sigma V^T$, with orthonormal columns in $U$ and $V$, and $\\Sigma$ diagonal with nonnegative singular values $\\sigma_i$, $i=1,\\dots,n-1$. The Moore–Penrose pseudoinverse corresponds to using all nonzero singular values, but this can amplify noise substantially when small singular values are present. Truncated singular value decomposition (TSVD) regularization restricts the inversion to the leading $k$ singular components, trading variance reduction for bias introduction.\n\nFrom the SVD, the least-squares solution restricted to the span of the first $k$ right singular vectors is constructed by projecting the data onto the first $k$ left singular vectors and scaling by the inverse of the corresponding singular values. Specifically, writing $U_k \\in \\mathbb{R}^{(n-1)\\times k}$ for the first $k$ columns of $U$, $V_k \\in \\mathbb{R}^{n\\times k}$ for the first $k$ columns of $V$, and $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ for the diagonal matrix of the first $k$ singular values, the TSVD estimator can be written as\n$$\nx_k = V_k \\Sigma_k^{-1} U_k^T b.\n$$\nSubstituting $b = A x_{\\text{true}} + \\varepsilon$ and using $A = U \\Sigma V^T$, the expected value of $x_k$ over the noise (with $\\mathbb{E}[\\varepsilon] = 0$) is\n$$\n\\mathbb{E}[x_k] = V_k \\Sigma_k^{-1} U_k^T A x_{\\text{true}} = V_k \\Sigma_k^{-1} U_k^T U \\Sigma V^T x_{\\text{true}} = V_k V_k^T x_{\\text{true}}.\n$$\nThus, $\\mathbb{E}[x_k]$ is the orthogonal projection of $x_{\\text{true}}$ onto the span of the first $k$ right singular vectors. The squared bias is then\n$$\n\\| \\mathbb{E}[x_k] - x_{\\text{true}} \\|_2^2 = \\| (I - V_k V_k^T) x_{\\text{true}} \\|_2^2,\n$$\nwhich increases as more components are truncated (smaller $k$).\n\nFor variance, with $\\varepsilon$ modeled as independent and identically distributed zero-mean Gaussian noise with covariance $\\sigma^2 I$, we have the fluctuation\n$$\nx_k - \\mathbb{E}[x_k] = V_k \\Sigma_k^{-1} U_k^T \\varepsilon,\n$$\nand the covariance of $x_k$ is\n$$\n\\operatorname{Cov}(x_k) = V_k \\Sigma_k^{-1} U_k^T (\\sigma^2 I) U_k \\Sigma_k^{-1} V_k^T = \\sigma^2\\, V_k \\Sigma_k^{-2} V_k^T.\n$$\nThe expected squared norm of the deviation is the trace of this covariance:\n$$\n\\mathbb{E}\\left[ \\| x_k - \\mathbb{E}[x_k] \\|_2^2 \\right] = \\operatorname{trace}\\left( \\operatorname{Cov}(x_k) \\right) = \\sigma^2 \\sum_{i=1}^k \\frac{1}{\\sigma_i^2}.\n$$\nThis variance grows with $k$ because more singular components, especially those with smaller $\\sigma_i$, contribute to noise amplification.\n\nThe mean squared error (MSE) decomposes additively as\n$$\n\\operatorname{MSE}(k) = \\| \\mathbb{E}[x_k] - x_{\\text{true}} \\|_2^2 + \\mathbb{E}\\left[ \\| x_k - \\mathbb{E}[x_k] \\|_2^2 \\right].\n$$\nThe optimal truncation level $k$ balances the bias (which decreases with larger $k$) against the variance (which increases with larger $k$).\n\nAlgorithmic steps to implement the computation for each test case:\n1. Construct the grid $t_i = (i-1)/(n-1)$ for $i=1,\\dots,n$, with spacing $h = 1/(n-1)$.\n2. Build the matrix $A \\in \\mathbb{R}^{(n-1)\\times n}$ using the first-derivative stencil entries $A_{i,i}=-1/h$ and $A_{i,i+1}=1/h$.\n3. Define the ground-truth signal $x_{\\text{true}} \\in \\mathbb{R}^n$ with components $x_{\\text{true},i}=\\sin(2\\pi t_i) + 0.1 t_i$, using radians for the sine function argument.\n4. Compute the SVD $A = U \\Sigma V^T$ with $U \\in \\mathbb{R}^{(n-1)\\times(n-1)}$, diagonal singular values $\\sigma_i$ in $\\Sigma$, and $V \\in \\mathbb{R}^{n\\times(n-1)}$.\n5. For each candidate truncation level $k$, precompute the linear operator $M_k = V_k \\Sigma_k^{-1} U_k^T$ so that each TSVD solution can be obtained by $x_k = M_k b$.\n6. Initialize the random number generator to a fixed seed $0$. For $L$ independent trials, draw Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$, form $b = A x_{\\text{true}} + \\varepsilon$, and compute $x_k^{(\\ell)} = M_k b$ for each $k$.\n7. Estimate the squared bias by computing $\\| \\bar{x}_k - x_{\\text{true}} \\|_2^2$ where $\\bar{x}_k$ is the sample mean $\\bar{x}_k = \\frac{1}{L}\\sum_{\\ell=1}^L x_k^{(\\ell)}$.\n8. Estimate the squared variance by computing the sample average $\\frac{1}{L} \\sum_{\\ell=1}^L \\| x_k^{(\\ell)} - \\bar{x}_k \\|_2^2$.\n9. Form the empirical MSE as the sum of the estimated squared bias and squared variance for each $k$.\n10. Select the $k$ that minimizes the empirical MSE; if multiple $k$ values are tied, choose the smallest $k$.\n\nThe program executes these steps for the three specified test cases. Finally, it prints a single line containing the three selected truncation levels in the format $[k_1,k_2,k_3]$. This computation demonstrates the bias-variance trade-off inherent in TSVD: lower truncation levels reduce variance by filtering out noise-amplifying components at the cost of introducing bias, whereas higher truncation levels decrease bias but can increase variance due to amplification through small singular values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_derivative_matrix(n: int) -> np.ndarray:\n    \"\"\"\n    Construct (n-1) x n first-derivative finite-difference matrix with uniform spacing h.\n    A[i, i] = -1/h, A[i, i+1] = 1/h\n    \"\"\"\n    m = n - 1\n    h = 1.0 / (n - 1)\n    A = np.zeros((m, n), dtype=float)\n    idx = np.arange(m)\n    A[idx, idx] = -1.0 / h\n    A[idx, idx + 1] = 1.0 / h\n    return A\n\ndef construct_true_signal(n: int) -> np.ndarray:\n    \"\"\"\n    Construct x_true on [0,1]: x_true[i] = sin(2*pi*t_i) + 0.1*t_i, radians.\n    \"\"\"\n    t = np.linspace(0.0, 1.0, n)\n    x_true = np.sin(2.0 * np.pi * t) + 0.1 * t\n    return x_true\n\ndef precompute_tsvd_operators(A: np.ndarray, k_list: list[int]):\n    \"\"\"\n    Compute SVD of A and precompute M_k = V_k * Sigma_k^{-1} * U_k^T for all k in k_list.\n    Returns dict mapping k -> M_k.\n    \"\"\"\n    # Economy SVD\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    # Shapes: U (m x m), s (m,), Vt (m x n)\n    U_T = U.T  # (m x m)\n    n = Vt.shape[1]\n    M_ops = {}\n    for k in k_list:\n        if k == 0:\n            # M_0 is the zero operator (n x m)\n            M_ops[k] = np.zeros((n, A.shape[0]), dtype=float)\n            continue\n        # U_k^T: first k rows of U^T (k x m)\n        U_k_T = U_T[:k, :]\n        # V_k: first k rows of Vt, transposed -> (n x k)\n        V_k = Vt[:k, :].T\n        inv_s_k = 1.0 / s[:k]  # (k,)\n        # M_k = V_k @ diag(inv_s_k) @ U_k_T\n        # Implement diag(inv_s_k) @ U_k_T as (inv_s_k[:, None] * U_k_T)\n        M_k = V_k @ (inv_s_k[:, None] * U_k_T)\n        M_ops[k] = M_k\n    return M_ops\n\ndef empirical_bias_variance_mse(A: np.ndarray, x_true: np.ndarray, sigma: float, L: int, k_list: list[int], seed: int = 0):\n    \"\"\"\n    For given A, x_true, noise std sigma, number of samples L, and k_list,\n    compute empirical squared bias, squared variance, and MSE for each k using Monte Carlo.\n    Returns dict k -> (bias_sq, var_sq, mse) and the k minimizing mse (smallest k in case of tie).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    M_ops = precompute_tsvd_operators(A, k_list)\n    m = A.shape[0]\n    n_dim = x_true.shape[0]\n    # Precompute b0 = A @ x_true\n    b0 = A @ x_true\n\n    stats = {}\n    for k in k_list:\n        M_k = M_ops[k]\n        # Store all L samples for this k\n        x_k_samples = np.zeros((L, n_dim))\n\n        for i in range(L):\n            eps = rng.normal(loc=0.0, scale=sigma, size=m)\n            b = b0 + eps\n            x_k_samples[i, :] = M_k @ b\n        \n        # Estimate expectation (mean) of the estimator x_k\n        mean_x_k = np.mean(x_k_samples, axis=0)\n        \n        # Estimate squared bias: || E[x_k] - x_true ||^2\n        bias_sq = float(np.sum((mean_x_k - x_true)**2))\n        \n        # Estimate variance: E[ ||x_k - E[x_k]||^2 ]\n        # This is the mean of the squared norms of the deviations from the sample mean\n        var_sq = float(np.mean(np.sum((x_k_samples - mean_x_k)**2, axis=1)))\n        \n        mse = bias_sq + var_sq\n        stats[k] = (bias_sq, var_sq, mse)\n    \n    # Find k minimizing mse, tie-breaking by smallest k\n    best_k = None\n    best_mse = float('inf')\n    for k in sorted(k_list):\n        _, _, mse = stats[k]\n        if mse  best_mse:\n            best_mse = mse\n            best_k = k\n\n    return stats, best_k\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Each case: (n, noise_sigma, L, k_list)\n        (50, 1e-2, 400, [0, 1, 5, 10, 20, 49]),\n        (50, 1e-6, 400, [0, 1, 5, 10, 20, 49]),\n        (50, 5e-2, 400, [0, 1, 5, 10, 20, 49]),\n    ]\n\n    results = []\n    for n, sigma, L, k_list in test_cases:\n        A = construct_derivative_matrix(n)\n        x_true = construct_true_signal(n)\n        # Use fixed seed per case for reproducibility as instructed\n        _, best_k = empirical_bias_variance_mse(A, x_true, sigma, L, k_list, seed=0)\n        results.append(best_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While the previous exercise examined the statistical effects of random noise, this practice provides a more deterministic view of TSVD as a powerful filter. Here, we introduce a \"structured\" noise model, where the error is deliberately aligned with a specific left singular vector $u_k$. This setup allows you to see with surgical precision how TSVD isolates and handles different components of the data . You will verify that the error is amplified by the inverse of the corresponding singular value, $1/\\sigma_k$, and demonstrate that by truncating the solution at rank $k-1$, this specific source of error can be completely eliminated.",
            "id": "3280709",
            "problem": "You are given a family of ill-conditioned linear systems constructed from a diagonal matrix whose singular values decay geometrically, together with a structured noise model aligned with a specified left singular vector. Your task is to implement and analyze the effect of Truncated Singular Value Decomposition (TSVD) on the solution error under targeted noise, and to quantify how truncation at one index lower than the noise’s index mitigates contamination.\n\nConsider a square matrix $A \\in \\mathbb{R}^{n \\times n}$ with singular value decomposition (SVD) $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$, $\\Sigma = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_n)$ with $\\sigma_1 \\ge \\dots \\ge \\sigma_n  0$, and $V \\in \\mathbb{R}^{n \\times n}$. The Truncated Singular Value Decomposition (TSVD) solution of rank $r$ for a right-hand side $b \\in \\mathbb{R}^n$ is defined as the least-squares solution restricted to the span of the first $r$ right singular vectors. You must compute TSVD solutions by forming the SVD of $A$ and truncating to rank $r$ in the canonical way.\n\nUse the following deterministic instance:\n- Dimension $n = 8$.\n- Singular values defined by $\\sigma_i = 10^{-\\frac{i-1}{2}}$ for $i = 1, \\dots, 8$. Hence, $A = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_8)$.\n- True solution $x_0 \\in \\mathbb{R}^8$ with entries $(x_0)_i = \\frac{(-1)^{i+1}}{i}$ for $i = 1,\\dots,8$.\n- Noise-free right-hand side $b_0 = A x_0$.\n- Structured noise model $b = b_0 + \\alpha\\, u_k$, where $u_k$ is the $k$-th column of $U$ (the $k$-th left singular vector), $k \\in \\{1,\\dots,n\\}$, and $\\alpha \\in \\mathbb{R}$.\n\nFor each test case $(k,\\alpha)$ in the suite below, compute three TSVD solutions:\n- Rank $r = n$ (no truncation).\n- Rank $r = k$ (includes the $k$-th mode).\n- Rank $r = k-1$ (excludes the $k$-th mode; interpret $r=0$ as the zero solution).\n\nFor each of these three TSVD solutions $x_r$, compute the relative error\n$$\ne_r = \\frac{\\|x_r - x_0\\|_2}{\\|x_0\\|_2}.\n$$\nYour program should output, for each test case, the triple $(e_n, e_k, e_{k-1})$ as floats rounded to $6$ decimal places. Concatenate the triples for all test cases into a single list in the given order.\n\nTest suite (each pair is $(k,\\alpha)$):\n- $(2, 0.5)$: a small index $k$ with comparatively large targeted noise.\n- $(2, 0.01)$: the same small index $k$ with small targeted noise.\n- $(6, 0.01)$: a larger index $k$ where the singular value is much smaller, testing strong amplification.\n- $(1, 0.05)$: boundary case where $k=1$ and truncation at $k-1$ yields rank $r=0$.\n\nFinal output format: Your program should produce a single line of output containing a comma-separated list of all $12$ numbers enclosed in square brackets. The order must be\n$$\n[e_n^{(1)}, e_k^{(1)}, e_{k-1}^{(1)}, e_n^{(2)}, e_k^{(2)}, e_{k-1}^{(2)}, e_n^{(3)}, e_k^{(3)}, e_{k-1}^{(3)}, e_n^{(4)}, e_k^{(4)}, e_{k-1}^{(4)}],\n$$\nwhere the superscript indicates the test case index in the order listed above. All values must be rounded to $6$ decimal places. No physical units are involved. Angles are not present. Percentages are not permitted; use decimal fractions only.",
            "solution": "The problem requires an analysis of the Truncated Singular Value Decomposition (TSVD) as a regularization method for an ill-conditioned linear system subject to structured noise. We will first validate the problem statement and then provide a complete analytical and computational solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following data and definitions:\n- A square matrix $A \\in \\mathbb{R}^{n \\times n}$ with Singular Value Decomposition (SVD) $A = U \\Sigma V^\\top$, where $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$ and $\\sigma_1 \\ge \\dots \\ge \\sigma_n  0$.\n- The TSVD solution of rank $r$, denoted $x_r$, is the least-squares solution to $Ax=b$ restricted to the span of the first $r$ right singular vectors.\n- A specific deterministic instance is given:\n    - Dimension: $n = 8$.\n    - Singular values: $\\sigma_i = 10^{-\\frac{i-1}{2}}$ for $i = 1, \\dots, 8$.\n    - Matrix: $A = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_8)$.\n    - True solution: $x_0 \\in \\mathbb{R}^8$ with components $(x_0)_i = \\frac{(-1)^{i+1}}{i}$ for $i = 1, \\dots, 8$.\n    - Noise-free right-hand side: $b_0 = A x_0$.\n    - Noisy right-hand side: $b = b_0 + \\alpha u_k$, where $u_k$ is the $k$-th left singular vector, for a given integer $k \\in \\{1,\\dots,n\\}$ and scalar $\\alpha \\in \\mathbb{R}$.\n- Tasks per test case $(k, \\alpha)$:\n    1. Compute TSVD solution $x_r$ for rank $r = n$.\n    2. Compute TSVD solution $x_r$ for rank $r = k$.\n    3. Compute TSVD solution $x_r$ for rank $r = k-1$ (if $r=0$, the solution is the zero vector).\n- For each solution $x_r$, compute the relative error $e_r = \\frac{\\|x_r - x_0\\|_2}{\\|x_0\\|_2}$.\n- The output for each test case is the triple $(e_n, e_k, e_{k-1})$.\n- Test Suite: $(k, \\alpha)$ pairs are $(2, 0.5)$, $(2, 0.01)$, $(6, 0.01)$, and $(1, 0.05)$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is subjected to rigorous validation.\n- **Scientifically Grounded**: The problem is a standard, well-formulated exercise in numerical linear algebra. It deals with the fundamental concepts of SVD, ill-conditioned systems, and TSVD regularization. All premises are based on established mathematical principles.\n- **Well-Posed**: The problem is fully specified. The matrix $A$, true solution $x_0$, noise model, and error metric are defined precisely. The procedure for calculating the TSVD solution is canonical. There is no ambiguity, and a unique solution exists for each required computation.\n- **Objective**: The problem is stated in precise, mathematical language, free of any subjectivity or opinion.\n\nThe problem does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, contradiction, etc.). The use of a diagonal matrix $A$ is a deliberate simplification to isolate and clearly demonstrate the mechanism of error amplification and the effect of TSVD, which is a sound pedagogical approach.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A full solution will be developed.\n\n### Solution Derivation\n\nThe solution to the linear system $Ax=b$ is formally given by $x = A^{-1}b$. For an ill-conditioned matrix $A$, small perturbations in $b$ can lead to large errors in $x$. TSVD is a method to mitigate this effect.\n\n**1. SVD of the Matrix $A$**\n\nThe matrix $A$ is given as a diagonal matrix:\n$$\nA = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\n$$\nwhere $\\sigma_i = 10^{-\\frac{i-1}{2}}$. The SVD of a diagonal matrix with positive diagonal entries is straightforward. We can set the orthogonal matrices $U$ and $V$ to be the identity matrix, $I$, and the singular value matrix $\\Sigma$ to be $A$ itself.\n$$\nA = U \\Sigma V^\\top = I A I^\\top = A\n$$\nThus, we have $U = I$, $V = I$, and $\\Sigma = A = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$. The left singular vectors $u_i$ and right singular vectors $v_i$ are the standard basis vectors, $e_i$.\n\n**2. The TSVD Solution**\n\nThe general solution to $Ax=b$ can be expressed using the SVD of $A$:\n$$\nx = \\sum_{i=1}^n \\frac{u_i^\\top b}{\\sigma_i} v_i\n$$\nThe TSVD solution of rank $r$, denoted $x_r$, is obtained by truncating this series to its first $r$ terms:\n$$\nx_r = \\sum_{i=1}^r \\frac{u_i^\\top b}{\\sigma_i} v_i\n$$\nGiven our specific SVD where $u_i=v_i=e_i$, this simplifies to:\n$$\nx_r = \\sum_{i=1}^r \\frac{e_i^\\top b}{\\sigma_i} e_i\n$$\nThis means that the $j$-th component of the vector $x_r$ is:\n$$\n(x_r)_j = \\begin{cases} (e_j^\\top b) / \\sigma_j = b_j / \\sigma_j  \\text{if } j \\le r \\\\ 0  \\text{if } j  r \\end{cases}\n$$\n\n**3. Analyzing the Noisy Right-Hand Side $b$**\n\nThe right-hand side $b$ is constructed as $b = b_0 + \\alpha u_k$, where $b_0 = A x_0$ and $u_k = e_k$.\n$$\nb = A x_0 + \\alpha e_k\n$$\nThe $j$-th component of $b$ is $(b)_j = (A x_0)_j + \\alpha (e_k)_j = \\sigma_j (x_0)_j + \\alpha \\delta_{jk}$, where $\\delta_{jk}$ is the Kronecker delta.\n\n**4. The Error of the TSVD Solution**\n\nWe can now write the components of the TSVD solution $x_r$ explicitly:\n$$\n(x_r)_j = \\begin{cases} \\frac{\\sigma_j (x_0)_j + \\alpha \\delta_{jk}}{\\sigma_j} = (x_0)_j + \\frac{\\alpha \\delta_{jk}}{\\sigma_j}  \\text{if } j \\le r \\\\ 0  \\text{if } j  r \\end{cases}\n$$\nThe error vector is $x_r - x_0$. Its $j$-th component is:\n$$\n(x_r - x_0)_j = \\begin{cases} \\frac{\\alpha \\delta_{jk}}{\\sigma_j}  \\text{if } j \\le r \\\\ -(x_0)_j  \\text{if } j  r \\end{cases}\n$$\nThis vector has at most one non-zero component from the noise term (specifically at index $j=k$, if $k \\le r$) and non-zero components from the truncation for all indices $j  r$.\n\nThe squared Euclidean norm of the error vector is the sum of the squares of its components:\n$$\n\\|x_r - x_0\\|_2^2 = \\sum_{j=1}^n ((x_r - x_0)_j)^2\n$$\nWe analyze this sum by splitting it into two parts, based on whether the index $j$ is greater or less than/equal to the truncation rank $r$.\n\n- If $k \\le r$: The noise component at index $k$ is included. The error vector has a component $\\alpha/\\sigma_k$ at index $k$ and components $-(x_0)_j$ for all $j  r$.\n$$\n\\|x_r - x_0\\|_2^2 = \\left(\\frac{\\alpha}{\\sigma_k}\\right)^2 + \\sum_{j=r+1}^n (x_0)_j^2\n$$\n\n- If $k  r$: The noise component at index $k$ is truncated. The error is solely due to truncation.\n$$\n\\|x_r - x_0\\|_2^2 = \\sum_{j=r+1}^n (x_0)_j^2\n$$\n\n**5. Specific Error Formulas for the Problem**\n\nWe apply these general formulas to the three specific ranks required for each test case $(k, \\alpha)$.\n\n- **Rank $r=n$ (no truncation):** Here $r=n$, so we always have $k \\le r$. The summation term $\\sum_{j=n+1}^n$ is empty and equals $0$.\n$$\n\\|x_n - x_0\\|_2^2 = \\left(\\frac{\\alpha}{\\sigma_k}\\right)^2\n$$\nThe relative error is $e_n = \\frac{\\|x_n - x_0\\|_2}{\\|x_0\\|_2} = \\frac{|\\alpha|/\\sigma_k}{\\|x_0\\|_2}$. This shows the error is directly proportional to the noise magnitude $\\alpha$ and inversely proportional to the singular value $\\sigma_k$, a classic result for ill-conditioned systems.\n\n- **Rank $r=k$ (truncation including the noise mode):** Here $r=k$, so $k \\le r$.\n$$\n\\|x_k - x_0\\|_2^2 = \\left(\\frac{\\alpha}{\\sigma_k}\\right)^2 + \\sum_{j=k+1}^n (x_0)_j^2\n$$\nThe relative error is $e_k = \\frac{\\sqrt{(\\alpha/\\sigma_k)^2 + \\sum_{j=k+1}^n (x_0)_j^2}}{\\|x_0\\|_2}$. The error has two sources: the amplified noise and the truncation of the true solution components from $k+1$ to $n$.\n\n- **Rank $r=k-1$ (truncation excluding the noise mode):** Here $r=k-1$, so $k  r$. For $k=1$, this gives $r=0$.\n$$\n\\|x_{k-1} - x_0\\|_2^2 = \\sum_{j=(k-1)+1}^n (x_0)_j^2 = \\sum_{j=k}^n (x_0)_j^2\n$$\nThe relative error is $e_{k-1} = \\frac{\\sqrt{\\sum_{j=k}^n (x_0)_j^2}}{\\|x_0\\|_2}$. By truncating at $k-1$, the amplified noise term $\\alpha/\\sigma_k$ is completely eliminated. The error is only due to the truncation of the true solution components from $k$ to $n$. For $k=1$, $r=0$, the TSVD solution $x_0$ is the zero vector, and the error is $\\|0-x_0\\|_2 = \\|x_0\\|_2$, yielding a relative error of $e_0 = 1$. Our formula is consistent, as $\\sum_{j=1}^n (x_0)_j^2 = \\|x_0\\|_2^2$.\n\nThese formulas provide an analytical basis for the numerical computation. The Python code in the final answer will implement these derived expressions to calculate the required error values for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and analyzes the effect of Truncated SVD (TSVD) on the solution error\n    of an ill-conditioned linear system under targeted noise.\n    \"\"\"\n    # Define the deterministic parameters of the problem.\n    n = 8\n    \n    # Define the indices for vector/matrix components (1 to n).\n    i = np.arange(1, n + 1)\n    \n    # Construct the singular values sigma_i = 10^(-(i-1)/2).\n    # Numpy's broadcasting makes this concise.\n    sigma = 10**(-(i - 1) / 2)\n    \n    # Construct the true solution vector x_0 with components (-1)^(i+1) / i.\n    x0 = ((-1)**(i + 1)) / i\n    \n    # Pre-compute the L2 norm of the true solution, as it's a constant factor.\n    norm_x0 = np.linalg.norm(x0)\n    \n    # Define the test suite of (k, alpha) pairs.\n    test_cases = [\n        (2, 0.5),   # Small index k, large noise\n        (2, 0.01),  # Small index k, small noise\n        (6, 0.01),  # Large index k, small noise (strong amplification)\n        (1, 0.05)   # Boundary case k=1\n    ]\n    \n    # This list will store the final flattened results.\n    results = []\n    \n    # Iterate through each test case to compute the three relative errors.\n    for k, alpha in test_cases:\n        # Convert 1-based index k to 0-based index for numpy arrays.\n        k_idx = k - 1\n        \n        # 1. Compute error for rank r = n (no truncation).\n        # The error is due solely to the amplified noise component.\n        # error_norm_sq = (alpha / sigma_k)^2\n        # relative_error = sqrt(error_norm_sq) / norm_x0\n        en = (np.abs(alpha) / sigma[k_idx]) / norm_x0\n        \n        # 2. Compute error for rank r = k.\n        # The error has two sources: amplified noise and solution truncation.\n        # error_norm_sq = (alpha / sigma_k)^2 + sum_{j=k+1 to n} (x0_j)^2\n        # Python slice x0[k:] corresponds to components from index k to n-1, \n        # which matches j from k+1 to n in 1-based indexing.\n        truncation_error_k_sq = np.sum(x0[k:]**2)\n        noise_error_sq = (alpha / sigma[k_idx])**2\n        error_norm_k = np.sqrt(noise_error_sq + truncation_error_k_sq)\n        ek = error_norm_k / norm_x0\n\n        # 3. Compute error for rank r = k - 1.\n        # The amplified noise is removed by truncation. Error is only from truncation.\n        # error_norm_sq = sum_{j=k to n} (x0_j)^2\n        # Special case: if k=1, r=0, and the TSVD solution is the zero vector.\n        # The error is ||0 - x0|| = ||x0||, so the relative error is 1.\n        if k == 1:\n            ekm1 = 1.0\n        else:\n            # Python slice x0[k-1:] corresponds to components from k-1 to n-1,\n            # which matches j from k to n in 1-based indexing.\n            truncation_error_km1_sq = np.sum(x0[k_idx:]**2)\n            error_norm_km1 = np.sqrt(truncation_error_km1_sq)\n            ekm1 = error_norm_km1 / norm_x0\n            \n        # Append the rounded triple of errors to the results list.\n        results.extend([\n            round(en, 6),\n            round(ek, 6),\n            round(ekm1, 6)\n        ])\n        \n    # Format the final output as a single comma-separated list in brackets.\n    # map(str, ...) converts each rounded float to its string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In real-world applications, we rarely know the true solution, making it impossible to choose the truncation parameter $k$ by directly minimizing the solution error. This practice introduces a practical, algorithmic approach to this challenge by monitoring the solution's coefficients as $k$ increases. You will see that the coefficients $c_i = (u_i^{\\top} b)/\\sigma_i$ often exhibit a \"spike\" when a small singular value $\\sigma_i$ begins to amplify noise present in the data . By implementing a rule to detect this spike, you will learn a valuable heuristic for selecting a near-optimal regularization parameter, connecting theoretical understanding to a practical problem-solving technique.",
            "id": "3201000",
            "problem": "You are given a family of linear inverse problems modeled as $A x \\approx b$, where $A \\in \\mathbb{R}^{m \\times n}$ is ill-conditioned, and $b \\in \\mathbb{R}^{m}$ may be contaminated by additive noise. Consider using truncated singular value decomposition (TSVD) as a regularization strategy. The singular value decomposition of $A$ is defined as $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has nonnegative diagonal entries (the singular values) $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n  0$. For a truncation level $k \\in \\{1,2,\\dots,n\\}$, define the truncated subspace $\\mathcal{S}_k = \\mathrm{span}\\{v_1,\\dots,v_k\\}$, where $v_i$ are the right singular vectors (columns of $V$), and let $x_k$ be the element of $\\mathcal{S}_k$ that minimizes the residual norm $\\|A x - b\\|_2$ over $x \\in \\mathcal{S}_k$.\n\nYour task is to analyze the path $x_k$ as $k$ increases, specifically focusing on when components along $v_i$ with large coefficients enter the solution. The coefficient of $v_i$ in $x_k$ is the $i$-th entry of the expansion of $x_k$ in the basis $\\{v_i\\}$. For each $i$, define the coefficient magnitude sequence to be the absolute values of these coefficients as $k$ grows. In practice, when $b$ contains noise, coefficients corresponding to small singular values can become large because the residual component aligns with left singular vectors and is amplified by $1/\\sigma_i$. This phenomenon is linked to the onset of instability and is often visible as “coefficient spikes” in the sequence of coefficients.\n\nDefine a spike detection rule in purely algorithmic terms as follows. Let $c_i$ denote the coefficient associated with $v_i$ that would appear upon extending the truncation from $k=i-1$ to $k=i$ (so $c_i$ is the coefficient multiplying $v_i$ in $x_i$). Let $p=5$, $\\alpha=3$, and $\\beta=2$. Let the baseline be the mean of the first $p$ absolute coefficients, that is, $\\mathrm{baseline} = \\frac{1}{p} \\sum_{i=1}^{p} |c_i|$. A spike is declared at the first index $ip$ such that both $|c_i|  \\alpha \\cdot \\mathrm{baseline}$ and $|c_i|  \\beta \\cdot |c_{i-1}|$. If no such index exists, report $-1$.\n\nStarting only from the definitions of singular value decomposition and least squares minimization restricted to a subspace, your program must:\n- Derive the form of the coefficients $c_i$ that enter $x_k$ as $k$ increases by solving the constrained least squares problem over $\\mathcal{S}_k$ using orthogonality properties of $U$ and $V$.\n- Implement the above spike detection rule to determine the first spike index $k_{\\star}$.\n\nTest Suite. Implement the following $3$ test cases, each fully specified by deterministic pseudorandom construction. In every case, singular values are strictly decreasing and generated from a schedule that ensures ill-conditioning. Construct $A$ via an explicit singular value decomposition as follows: generate $U$ and $V$ as independent Haar-like orthogonal matrices by applying a QR factorization to standard normal matrices with a given random seed; define $\\Sigma$ to have the specified singular values on its diagonal; then set $A = U_{[:,1:n]} \\Sigma V^{\\top}$. Construct the ground truth in the right singular vector basis by specifying coefficients $\\hat{x}_i$ and setting $x_{\\mathrm{true}} = V \\hat{x}$, with $\\hat{x} = (\\hat{x}_1,\\dots,\\hat{x}_n)^{\\top}$. Then set $b = A x_{\\mathrm{true}} + \\varepsilon$, where the noise vector $\\varepsilon$ has independent normal entries with zero mean and standard deviation $\\sigma_{\\mathrm{noise}}$; define $\\sigma_{\\mathrm{noise}} = \\mathrm{level} \\cdot \\|A x_{\\mathrm{true}}\\|_2 / \\sqrt{m}$, so the noise scales relative to the clean data. Angles are not used in this problem. No physical units are involved.\n\n- Case $1$ (happy path): $m=50$, $n=40$, random seed $1234$, singular values $\\sigma_i = 10^{-(i-1)/8}$ for $i=1,\\dots,n$, coefficients $\\hat{x}_i = \\exp(-(i-1)/7)$ for $i=1,\\dots,n$, noise level $\\mathrm{level} = 10^{-3}$.\n- Case $2$ (boundary, no noise): $m=50$, $n=40$, random seed $2021$, singular values $\\sigma_i = 10^{-(i-1)/8}$, coefficients $\\hat{x}_i = \\exp(-(i-1)/7)$, noise level $\\mathrm{level} = 0$.\n- Case $3$ (edge, stronger ill-conditioning and more noise): $m=80$, $n=60$, random seed $999$, singular values $\\sigma_i = 10^{-(i-1)/6}$, coefficients $\\hat{x}_i = \\exp(-(i-1)/10)$, noise level $\\mathrm{level} = 2 \\cdot 10^{-2}$.\n\nFor each case, compute the sequence of coefficients $c_i$ associated with the incremental inclusion of $v_i$ and apply the spike detection rule with $p=5$, $\\alpha=3$, $\\beta=2$. The required output for each case is the integer index $k_{\\star}$ (using one-based indexing for $i$) at which the first spike occurs; if no spike is detected, output $-1$ for that case.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three cases in order, for example $[k_1,k_2,k_3]$. No additional text should be printed.",
            "solution": "The posed problem is valid. It is a well-defined task in computational science, grounded in the established theory of linear inverse problems and regularization using truncated singular value decomposition (TSVD). The problem is scientifically sound, self-contained, objective, and provides all necessary data and algorithmic definitions for a unique solution.\n\nThe core of the problem is to determine the coefficients of the TSVD solution and then apply a specific algorithmic rule to detect noise-induced instability. We begin by deriving the formula for these coefficients from first principles.\n\nThe problem is to find the solution $x_k$ that minimizes the residual norm for a given truncation level $k$. The solution $x_k$ is constrained to the subspace $\\mathcal{S}_k = \\mathrm{span}\\{v_1, \\dots, v_k\\}$, where $v_i$ are the right singular vectors of the matrix $A$.\n$$\nx_k = \\arg\\min_{x \\in \\mathcal{S}_k} \\|A x - b\\|_2^2\n$$\nAny vector $x \\in \\mathcal{S}_k$ can be uniquely represented as a linear combination of the orthonormal basis vectors $\\{v_1, \\dots, v_k\\}$:\n$$\nx = \\sum_{i=1}^k c_i v_i\n$$\nwhere $c_i$ are the coefficients we need to determine. Substituting this representation into the objective function, we seek to minimize:\n$$\n\\left\\| A \\left(\\sum_{i=1}^k c_i v_i\\right) - b \\right\\|_2^2 = \\left\\| \\left(\\sum_{i=1}^k c_i A v_i\\right) - b \\right\\|_2^2\n$$\nFrom the definition of the singular value decomposition, $A = U \\Sigma V^{\\top}$, we have the fundamental relationship $A v_i = \\sigma_i u_i$ for $i=1, \\dots, n$, where $u_i$ are the left singular vectors and $\\sigma_i$ are the singular values. Substituting this into the expression gives:\n$$\n\\left\\| \\left(\\sum_{i=1}^k c_i \\sigma_i u_i\\right) - b \\right\\|_2^2\n$$\nThe left singular vectors $\\{u_i\\}_{i=1}^m$ form an orthonormal basis for $\\mathbb{R}^m$. The squared Euclidean norm of a vector is the sum of the squares of its coordinates in any orthonormal basis. We can express the vector inside the norm, $r = (\\sum_{i=1}^k c_i \\sigma_i u_i) - b$, in the basis of left singular vectors. The coordinate of $r$ along $u_j$ is $u_j^{\\top} r$.\nThe squared norm is therefore:\n$$\n\\|r\\|_2^2 = \\sum_{j=1}^m (u_j^{\\top} r)^2 = \\sum_{j=1}^m \\left( u_j^{\\top} \\left( \\sum_{i=1}^k c_i \\sigma_i u_i \\right) - u_j^{\\top} b \\right)^2\n$$\nUsing the orthonormality property $u_j^{\\top} u_i = \\delta_{ij}$ (the Kronecker delta), the expression simplifies.\nFor $j \\in \\{1, \\dots, k\\}$, the term $u_j^{\\top} (\\sum_{i=1}^k c_i \\sigma_i u_i)$ becomes $c_j \\sigma_j$.\nFor $j \\in \\{k+1, \\dots, m\\}$, the term $u_j^{\\top} (\\sum_{i=1}^k c_i \\sigma_i u_i)$ becomes $0$.\nThis allows us to split the sum over $j$:\n$$\n\\|r\\|_2^2 = \\sum_{j=1}^k (c_j \\sigma_j - u_j^{\\top} b)^2 + \\sum_{j=k+1}^m (- u_j^{\\top} b)^2\n$$\nTo minimize this expression with respect to the coefficients $\\{c_1, \\dots, c_k\\}$, we only need to consider the first sum, as the second sum is independent of these coefficients. The first sum is a sum of non-negative terms. The minimum value of this sum, which is $0$, is achieved when each term is individually zero. Therefore, for each $j \\in \\{1, \\dots, k\\}$, we must have:\n$$\nc_j \\sigma_j - u_j^{\\top} b = 0\n$$\nSolving for $c_j$ yields the well-known formula for the coefficients of the TSVD solution:\n$$\nc_j = \\frac{u_j^{\\top} b}{\\sigma_j}\n$$\nThe problem defines $c_i$ as the coefficient that enters the solution as the truncation level is increased from $i-1$ to $i$. Our derivation shows that the coefficient for the basis vector $v_i$ is given by the formula above and does not depend on the overall truncation level $k$ (as long as $k \\ge i$). Thus, the sequence of coefficients to be analyzed is simply $\\{c_i\\}_{i=1}^n$.\n\nWith the form of the coefficients established, the rest of the problem is algorithmic.\n1.  For each test case, we construct the matrix $A$ from its specified singular value decomposition. This involves generating random orthogonal matrices $U$ and $V$ and a diagonal matrix $\\Sigma$ of specified singular values. The matrix $A$ is formed as $A = U_{econ} \\Sigma_{n \\times n} V^{\\top}$, where $U_{econ}$ consists of the first $n$ columns of $U$.\n2.  The \"true\" solution $x_{\\mathrm{true}}$ is constructed in the basis of right singular vectors, and the \"clean\" data vector is computed as $b_{\\mathrm{clean}} = A x_{\\mathrm{true}}$.\n3.  Noise is added to obtain the final data vector $b = b_{\\mathrm{clean}} + \\varepsilon$, where the noise standard deviation is scaled relative to the norm of the clean signal.\n4.  The coefficients $c_i$ are calculated for $i=1, \\dots, n$ using the derived formula $c_i = (u_i^{\\top} b)/\\sigma_i$.\n5.  The specified spike detection rule is applied to the sequence of absolute coefficients $|c_i|$. With parameters $p=5$, $\\alpha=3$, and $\\beta=2$, we compute a baseline from the first $p$ coefficients: $\\mathrm{baseline} = \\frac{1}{p} \\sum_{i=1}^{p} |c_i|$. We then search for the first index $i  p$ where both $|c_i|  \\alpha \\cdot \\mathrm{baseline}$ and $|c_i|  \\beta \\cdot |c_{i-1}|$. The $1$-based index of the first occurrence is the result. If no such index is found, the result is $-1$.\n\nThis procedure is implemented for each of the three test cases specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef run_case(m, n, seed, sigma_denominator, x_hat_denominator, level, p, alpha, beta):\n    \"\"\"\n    Runs a single test case for spike detection in TSVD.\n\n    Args:\n        m (int): Number of rows for matrix A.\n        n (int): Number of columns for matrix A.\n        seed (int): Random seed for reproducibility.\n        sigma_denominator (float): Denominator in the exponent for singular values.\n        x_hat_denominator (float): Denominator in the exponent for true coefficients.\n        level (float): Relative noise level.\n        p (int): Number of initial coefficients for baseline calculation.\n        alpha (float): Multiplier for the baseline threshold.\n        beta (float): Multiplier for the previous coefficient threshold.\n\n    Returns:\n        int: The 1-based index of the first detected spike, or -1 if no spike is found.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct matrices U, Sigma, V\n    # Generate Haar-like orthogonal matrices from QR of standard normal matrices\n    U_full, _ = np.linalg.qr(rng.standard_normal((m, m)))\n    V, _ = np.linalg.qr(rng.standard_normal((n, n)))\n    \n    # Use the economy-size U, which has orthonormal columns\n    U_econ = U_full[:, :n]\n\n    # Singular values (sigma_i = 10**(-(i-1)/C))\n    indices = np.arange(1, n + 1)\n    s_vals = 10.0**(-(indices - 1) / sigma_denominator)\n    Sigma_n = np.diag(s_vals)\n\n    # 2. Construct A, x_true, and b\n    A = U_econ @ Sigma_n @ V.T\n\n    # True solution coefficients in the V basis (x_hat_i = exp(-(i-1)/C))\n    x_hat = np.exp(-(indices - 1) / x_hat_denominator)\n    \n    # True solution vector\n    x_true = V @ x_hat\n\n    # Clean data vector b\n    b_clean = A @ x_true\n\n    # Additive noise\n    if level > 0.0:\n        norm_b_clean = np.linalg.norm(b_clean)\n        sigma_noise = level * norm_b_clean / np.sqrt(m)\n        noise = rng.normal(0.0, sigma_noise, size=m)\n        b = b_clean + noise\n    else:\n        b = b_clean\n\n    # 3. Compute TSVD solution coefficients\n    # c_i = (u_i^T b) / sigma_i\n    uT_b = U_econ.T @ b\n    c = uT_b / s_vals\n    c_abs = np.abs(c)\n\n    # 4. Apply spike detection rule\n    if n = p:\n        return -1\n\n    # Baseline using the first p coefficients (0-indexed to p-1)\n    baseline = np.mean(c_abs[0:p])\n\n    # Search for spike for indices i > p (1-based), which is i >= p (0-based)\n    for i in range(p, n):\n        cond1 = c_abs[i] > alpha * baseline\n        cond2 = c_abs[i] > beta * c_abs[i-1]\n        \n        if cond1 and cond2:\n            return i + 1  # Return 1-based index\n\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define spike detection parameters\n    p = 5\n    alpha = 3.0\n    beta = 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: m=50, n=40, seed=1234, sigma_i=10^-(i-1)/8, x_hat_i=exp(-(i-1)/7), level=1e-3\n        {\"m\": 50, \"n\": 40, \"seed\": 1234, \"sigma_denom\": 8.0, \"x_hat_denom\": 7.0, \"level\": 1e-3},\n        # Case 2: m=50, n=40, seed=2021, sigma_i=10^-(i-1)/8, x_hat_i=exp(-(i-1)/7), level=0\n        {\"m\": 50, \"n\": 40, \"seed\": 2021, \"sigma_denom\": 8.0, \"x_hat_denom\": 7.0, \"level\": 0.0},\n        # Case 3: m=80, n=60, seed=999, sigma_i=10^-(i-1)/6, x_hat_i=exp(-(i-1)/10), level=2e-2\n        {\"m\": 80, \"n\": 60, \"seed\": 999, \"sigma_denom\": 6.0, \"x_hat_denom\": 10.0, \"level\": 2e-2},\n    ]\n\n    results = []\n    for case in test_cases:\n        k_star = run_case(\n            m=case[\"m\"],\n            n=case[\"n\"],\n            seed=case[\"seed\"],\n            sigma_denominator=case[\"sigma_denom\"],\n            x_hat_denominator=case[\"x_hat_denom\"],\n            level=case[\"level\"],\n            p=p,\n            alpha=alpha,\n            beta=beta\n        )\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}