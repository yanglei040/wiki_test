{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of Truncated Singular Value Decomposition (TSVD) as a regularization method hinges on a fundamental concept in statistics and machine learning: the bias-variance trade-off. This practice will guide you through a Monte Carlo simulation to visualize this trade-off directly . By systematically analyzing how the solution's mean squared error ($MSE$) changes with the truncation level $k$, you will gain an intuitive and quantitative understanding of how TSVD balances the bias introduced by truncating the solution space against the variance caused by noise amplification.",
            "id": "3201027",
            "problem": "Consider the linear inverse problem with a forward operator constructed from a finite-difference approximation of the first derivative. Let $n$ be the number of grid points on $[0,1]$ with uniform spacing $h = 1/(n-1)$ and grid locations $t_i = (i-1)h$ for $i = 1,2,\\dots,n$. Define the $(n-1) \\times n$ matrix $A$ by the discrete first-derivative stencil\n$$\nA_{i,i} = -\\frac{1}{h}, \\quad A_{i,i+1} = \\frac{1}{h}, \\quad \\text{for } i=1,2,\\dots,n-1,\n$$\nand all other entries are zero. Let the true signal be the vector $x_{\\text{true}} \\in \\mathbb{R}^n$ with components\n$$\nx_{\\text{true},i} = \\sin(2\\pi t_i) + 0.1\\, t_i,\n$$\nwhere the sine function uses radians as the angle unit. Observations are modeled by\n$$\nb = A x_{\\text{true}} + \\varepsilon,\n$$\nwhere $\\varepsilon \\in \\mathbb{R}^{n-1}$ is additive observational noise with independent and identically distributed components of zero mean and a specified standard deviation.\n\nLet $A$ admit the singular value decomposition (SVD) $A = U \\Sigma V^T$ with $U \\in \\mathbb{R}^{(n-1)\\times(n-1)}$, $V \\in \\mathbb{R}^{n\\times(n-1)}$ having orthonormal columns, and $\\Sigma \\in \\mathbb{R}^{(n-1)\\times(n-1)}$ diagonal with nonnegative singular values on the diagonal. Consider the truncated singular value decomposition (TSVD) estimator $x_k$ obtained by using only the leading $k$ singular triplets for $k \\in \\{0,1,\\dots,n-1\\}$.\n\nYour task is to implement a program that, for each test case specified below, constructs $A$ and $x_{\\text{true}}$, draws $L$ independent noise realizations $\\varepsilon$ from a zero-mean Gaussian distribution with the given standard deviation, and computes the TSVD solution $x_k$ for each specified truncation level $k$. Using Monte Carlo sampling over the $L$ realizations, estimate:\n1. The squared bias defined by\n$$\n\\| \\mathbb{E}[x_k] - x_{\\text{true}} \\|_2^2,\n$$\nwhere the expectation is over the noise distribution.\n2. The squared variance defined by\n$$\n\\mathbb{E}\\left[ \\| x_k - \\mathbb{E}[x_k] \\|_2^2 \\right].\n$$\n3. The mean squared error (MSE) defined by the sum of the squared bias and squared variance.\n\nFor each test case, identify and report the truncation level $k$ (an integer) that minimizes the empirical mean squared error computed from the Monte Carlo samples. If there is a tie, choose the smallest such $k$. To ensure reproducibility, initialize the random number generator with a fixed seed of $0$ before generating noise for each test case.\n\nTest suite:\n- Case 1 (general case): $n = 50$, noise standard deviation $= 10^{-2}$, number of samples $L = 400$, candidate truncation levels $k \\in \\{0,1,5,10,20,49\\}$.\n- Case 2 (low-noise boundary): $n = 50$, noise standard deviation $= 10^{-6}$, number of samples $L = 400$, candidate truncation levels $k \\in \\{0,1,5,10,20,49\\}$.\n- Case 3 (high-noise edge): $n = 50$, noise standard deviation $= 5\\times 10^{-2}$, number of samples $L = 400$, candidate truncation levels $k \\in \\{0,1,5,10,20,49\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the best truncation levels for the three test cases as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$, where each $k_i$ is the integer minimizer of the empirical mean squared error for the corresponding case.",
            "solution": "We start from the linear inverse problem $b = A x_{\\text{true}} + \\varepsilon$, where $A$ is a discrete first-derivative operator built from a uniform grid. The singular value decomposition (SVD) factorizes $A$ as $A = U \\Sigma V^T$, with orthonormal columns in $U$ and $V$, and $\\Sigma$ diagonal with nonnegative singular values $\\sigma_i$, $i=1,\\dots,n-1$. The Moore–Penrose pseudoinverse corresponds to using all nonzero singular values, but this can amplify noise substantially when small singular values are present. Truncated singular value decomposition (TSVD) regularization restricts the inversion to the leading $k$ singular components, trading variance reduction for bias introduction.\n\nFrom the SVD, the least-squares solution restricted to the span of the first $k$ right singular vectors is constructed by projecting the data onto the first $k$ left singular vectors and scaling by the inverse of the corresponding singular values. Specifically, writing $U_k \\in \\mathbb{R}^{(n-1)\\times k}$ for the first $k$ columns of $U$, $V_k \\in \\mathbb{R}^{n\\times k}$ for the first $k$ columns of $V$, and $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ for the diagonal matrix of the first $k$ singular values, the TSVD estimator can be written as\n$$\nx_k = V_k \\Sigma_k^{-1} U_k^T b.\n$$\nSubstituting $b = A x_{\\text{true}} + \\varepsilon$ and using $A = U \\Sigma V^T$, the expected value of $x_k$ over the noise (with $\\mathbb{E}[\\varepsilon] = 0$) is\n$$\n\\mathbb{E}[x_k] = V_k \\Sigma_k^{-1} U_k^T A x_{\\text{true}} = V_k \\Sigma_k^{-1} U_k^T U \\Sigma V^T x_{\\text{true}} = V_k V_k^T x_{\\text{true}}.\n$$\nThus, $\\mathbb{E}[x_k]$ is the orthogonal projection of $x_{\\text{true}}$ onto the span of the first $k$ right singular vectors. The squared bias is then\n$$\n\\| \\mathbb{E}[x_k] - x_{\\text{true}} \\|_2^2 = \\| (I - V_k V_k^T) x_{\\text{true}} \\|_2^2,\n$$\nwhich increases as more components are truncated (smaller $k$).\n\nFor variance, with $\\varepsilon$ modeled as independent and identically distributed zero-mean Gaussian noise with covariance $\\sigma^2 I$, we have the fluctuation\n$$\nx_k - \\mathbb{E}[x_k] = V_k \\Sigma_k^{-1} U_k^T \\varepsilon,\n$$\nand the covariance of $x_k$ is\n$$\n\\operatorname{Cov}(x_k) = V_k \\Sigma_k^{-1} U_k^T (\\sigma^2 I) U_k \\Sigma_k^{-1} V_k^T = \\sigma^2\\, V_k \\Sigma_k^{-2} V_k^T.\n$$\nThe expected squared norm of the deviation is the trace of this covariance:\n$$\n\\mathbb{E}\\left[ \\| x_k - \\mathbb{E}[x_k] \\|_2^2 \\right] = \\operatorname{trace}\\left( \\operatorname{Cov}(x_k) \\right) = \\sigma^2 \\sum_{i=1}^k \\frac{1}{\\sigma_i^2}.\n$$\nThis variance grows with $k$ because more singular components, especially those with smaller $\\sigma_i$, contribute to noise amplification.\n\nThe mean squared error (MSE) decomposes additively as\n$$\n\\operatorname{MSE}(k) = \\| \\mathbb{E}[x_k] - x_{\\text{true}} \\|_2^2 + \\mathbb{E}\\left[ \\| x_k - \\mathbb{E}[x_k] \\|_2^2 \\right].\n$$\nThe optimal truncation level $k$ balances the bias (which decreases with larger $k$) against the variance (which increases with larger $k$).\n\nAlgorithmic steps to implement the computation for each test case:\n1. Construct the grid $t_i = (i-1)/(n-1)$ for $i=1,\\dots,n$, with spacing $h = 1/(n-1)$.\n2. Build the matrix $A \\in \\mathbb{R}^{(n-1)\\times n}$ using the first-derivative stencil entries $A_{i,i}=-1/h$ and $A_{i,i+1}=1/h$.\n3. Define the ground-truth signal $x_{\\text{true}} \\in \\mathbb{R}^n$ with components $x_{\\text{true},i}=\\sin(2\\pi t_i) + 0.1 t_i$, using radians for the sine function argument.\n4. Compute the singular value decomposition $A = U \\Sigma V^T$ with $U \\in \\mathbb{R}^{(n-1)\\times(n-1)}$, diagonal singular values $\\sigma_i$ in $\\Sigma$, and $V \\in \\mathbb{R}^{n\\times(n-1)}$.\n5. For each candidate truncation level $k$, precompute the linear operator $M_k = V_k \\Sigma_k^{-1} U_k^T$ so that each TSVD solution can be obtained by $x_k = M_k b$.\n6. Initialize the random number generator to a fixed seed $0$. For $L$ independent trials, draw Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$, form $b = A x_{\\text{true}} + \\varepsilon$, and compute $x_k^{(\\ell)} = M_k b$ for each $k$.\n7. Estimate the squared bias by computing $\\| \\bar{x}_k - x_{\\text{true}} \\|_2^2$ where $\\bar{x}_k$ is the sample mean $\\bar{x}_k = \\frac{1}{L}\\sum_{\\ell=1}^L x_k^{(\\ell)}$.\n8. Estimate the squared variance by computing the sample average $\\frac{1}{L} \\sum_{\\ell=1}^L \\| x_k^{(\\ell)} - \\bar{x}_k \\|_2^2$.\n9. Form the empirical mean squared error as the sum of the estimated squared bias and squared variance for each $k$.\n10. Select the $k$ that minimizes the empirical mean squared error; if multiple $k$ values are tied, choose the smallest $k$.\n\nThe program executes these steps for the three specified test cases. Finally, it prints a single line containing the three selected truncation levels in the format $[k_1,k_2,k_3]$. This computation demonstrates the bias-variance trade-off inherent in truncated singular value decomposition: lower truncation levels reduce variance by filtering out noise-amplifying components at the cost of introducing bias, whereas higher truncation levels decrease bias but can increase variance due to amplification through small singular values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_derivative_matrix(n: int) -> np.ndarray:\n    \"\"\"\n    Construct (n-1) x n first-derivative finite-difference matrix with uniform spacing h.\n    A[i, i] = -1/h, A[i, i+1] = 1/h\n    \"\"\"\n    m = n - 1\n    h = 1.0 / (n - 1)\n    A = np.zeros((m, n), dtype=float)\n    idx = np.arange(m)\n    A[idx, idx] = -1.0 / h\n    A[idx, idx + 1] = 1.0 / h\n    return A\n\ndef construct_true_signal(n: int) -> np.ndarray:\n    \"\"\"\n    Construct x_true on [0,1]: x_true[i] = sin(2*pi*t_i) + 0.1*t_i, radians.\n    \"\"\"\n    t = np.linspace(0.0, 1.0, n)\n    x_true = np.sin(2.0 * np.pi * t) + 0.1 * t\n    return x_true\n\ndef precompute_tsvd_operators(A: np.ndarray, k_list: list[int]):\n    \"\"\"\n    Compute SVD of A and precompute M_k = V_k * Sigma_k^{-1} * U_k^T for all k in k_list.\n    Returns dict mapping k -> M_k.\n    \"\"\"\n    # Economy SVD\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    # Shapes: U (m x m), s (m,), Vt (m x n)\n    U_T = U.T  # (m x m)\n    n = Vt.shape[1]\n    M_ops = {}\n    for k in k_list:\n        if k == 0:\n            # M_0 is the zero operator (n x m)\n            M_ops[k] = np.zeros((n, A.shape[0]), dtype=float)\n            continue\n        # U_k^T: first k rows of U^T (k x m)\n        U_k_T = U_T[:k, :]\n        # V_k: first k rows of Vt, transposed -> (n x k)\n        V_k = Vt[:k, :].T\n        inv_s_k = 1.0 / s[:k]  # (k,)\n        # M_k = V_k @ diag(inv_s_k) @ U_k_T\n        # Implement diag(inv_s_k) @ U_k_T as (inv_s_k[:, None] * U_k_T)\n        M_k = V_k @ (inv_s_k[:, None] * U_k_T)\n        M_ops[k] = M_k\n    return M_ops\n\ndef empirical_bias_variance_mse(A: np.ndarray, x_true: np.ndarray, sigma: float, L: int, k_list: list[int], seed: int = 0):\n    \"\"\"\n    For given A, x_true, noise std sigma, number of samples L, and k_list,\n    compute empirical squared bias, squared variance, and MSE for each k using Monte Carlo.\n    Returns dict k -> (bias_sq, var_sq, mse) and the k minimizing mse (smallest k in case of tie).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    M_ops = precompute_tsvd_operators(A, k_list)\n    m = A.shape[0]\n    # Precompute b0 = A @ x_true\n    b0 = A @ x_true\n\n    # For each k, collect samples of x_k to compute mean and variance efficiently\n    stats = {}\n    # We will store sample means and second moments to avoid keeping all samples\n    # Welford's algorithm for vector means and sum of squared deviations\n    for k in k_list:\n        M_k = M_ops[k]\n        mean = np.zeros_like(x_true)\n        # sum of squared deviations for variance norm (E[||x - mean||^2]) estimation\n        # We'll accumulate sum of ||x - mean||^2 online\n        ssd = 0.0\n        count = 0\n        for _ in range(L):\n            eps = rng.normal(loc=0.0, scale=sigma, size=m)\n            b = b0 + eps\n            x_k = M_k @ b\n            count += 1\n            # Online mean update\n            delta = x_k - mean\n            mean += delta / count\n            # Online sum of squared deviations (Chan's update for sum of squares of deviations)\n            ssd += np.dot(x_k - mean, x_k - mean)\n        # Bias squared\n        bias_sq = float(np.dot(mean - x_true, mean - x_true))\n        # Variance squared: average squared deviation norm\n        var_sq = float(ssd / L)\n        mse = bias_sq + var_sq\n        stats[k] = (bias_sq, var_sq, mse)\n\n    # Find k minimizing mse, tie-breaking by smallest k\n    best_k = None\n    best_mse = None\n    for k in sorted(k_list):\n        _, _, mse = stats[k]\n        if best_mse is None or mse < best_mse or (mse == best_mse and k < best_k):\n            best_mse = mse\n            best_k = k\n\n    return stats, best_k\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Each case: (n, noise_sigma, L, k_list)\n        (50, 1e-2, 400, [0, 1, 5, 10, 20, 49]),\n        (50, 1e-6, 400, [0, 1, 5, 10, 20, 49]),\n        (50, 5e-2, 400, [0, 1, 5, 10, 20, 49]),\n    ]\n\n    results = []\n    for n, sigma, L, k_list in test_cases:\n        A = construct_derivative_matrix(n)\n        x_true = construct_true_signal(n)\n        # Use fixed seed per case for reproducibility as instructed\n        _, best_k = empirical_bias_variance_mse(A, x_true, sigma, L, k_list, seed=0)\n        results.append(best_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While the bias-variance trade-off provides the theoretical foundation for choosing a regularization parameter, practical applications require heuristics to estimate an optimal truncation level $k$. This exercise introduces a powerful diagnostic technique: monitoring the solution coefficients for signs of instability . You will implement an algorithm to detect \"spikes\" in the coefficient sequence $c_i = (u_i^{\\top} b) / \\sigma_i$, which serve as a clear warning that noise is beginning to dominate the solution.",
            "id": "3201000",
            "problem": "You are given a family of linear inverse problems modeled as $A x \\approx b$, where $A \\in \\mathbb{R}^{m \\times n}$ is ill-conditioned, and $b \\in \\mathbb{R}^{m}$ may be contaminated by additive noise. Consider using truncated singular value decomposition (truncated SVD) as a regularization strategy. The singular value decomposition of $A$ is defined as $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has nonnegative diagonal entries (the singular values) $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n > 0$. For a truncation level $k \\in \\{1,2,\\dots,n\\}$, define the truncated subspace $\\mathcal{S}_k = \\mathrm{span}\\{v_1,\\dots,v_k\\}$, where $v_i$ are the right singular vectors (columns of $V$), and let $x_k$ be the element of $\\mathcal{S}_k$ that minimizes the residual norm $\\|A x - b\\|_2$ over $x \\in \\mathcal{S}_k$.\n\nYour task is to analyze the path $x_k$ as $k$ increases, specifically focusing on when components along $v_i$ with large coefficients enter the solution. The coefficient of $v_i$ in $x_k$ is the $i$-th entry of the expansion of $x_k$ in the basis $\\{v_i\\}$. For each $i$, define the coefficient magnitude sequence to be the absolute values of these coefficients as $k$ grows. In practice, when $b$ contains noise, coefficients corresponding to small singular values can become large because the residual component aligns with left singular vectors and is amplified by $1/\\sigma_i$. This phenomenon is linked to the onset of instability and is often visible as “coefficient spikes” in the sequence of coefficients.\n\nDefine a spike detection rule in purely algorithmic terms as follows. Let $c_i$ denote the coefficient associated with $v_i$ that would appear upon extending the truncation from $k=i-1$ to $k=i$ (so $c_i$ is the coefficient multiplying $v_i$ in $x_i$). Let $p=5$, $\\alpha=3$, and $\\beta=2$. Let the baseline be the mean of the first $p$ absolute coefficients, that is, $\\mathrm{baseline} = \\frac{1}{p} \\sum_{i=1}^{p} |c_i|$. A spike is declared at the first index $i>p$ such that both $|c_i| > \\alpha \\cdot \\mathrm{baseline}$ and $|c_i| > \\beta \\cdot |c_{i-1}|$. If no such index exists, report $-1$.\n\nStarting only from the definitions of singular value decomposition and least squares minimization restricted to a subspace, your program must:\n- Derive the form of the coefficients $c_i$ that enter $x_k$ as $k$ increases by solving the constrained least squares problem over $\\mathcal{S}_k$ using orthogonality properties of $U$ and $V$.\n- Implement the above spike detection rule to determine the first spike index $k_{\\star}$.\n\nTest Suite. Implement the following $3$ test cases, each fully specified by deterministic pseudorandom construction. In every case, singular values are strictly decreasing and generated from a schedule that ensures ill-conditioning. Construct $A$ via an explicit singular value decomposition as follows: generate $U$ and $V$ as independent Haar-like orthogonal matrices by applying a QR factorization to standard normal matrices with a given random seed; define $\\Sigma$ to have the specified singular values on its diagonal; then set $A = U_{[:,1:n]} \\Sigma V^{\\top}$. Construct the ground truth in the right singular vector basis by specifying coefficients $\\hat{x}_i$ and setting $x_{\\mathrm{true}} = V \\hat{x}$, with $\\hat{x} = (\\hat{x}_1,\\dots,\\hat{x}_n)^{\\top}$. Then set $b = A x_{\\mathrm{true}} + \\varepsilon$, where the noise vector $\\varepsilon$ has independent normal entries with zero mean and standard deviation $\\sigma_{\\mathrm{noise}}$; define $\\sigma_{\\mathrm{noise}} = \\mathrm{level} \\cdot \\|A x_{\\mathrm{true}}\\|_2 / \\sqrt{m}$, so the noise scales relative to the clean data. Angles are not used in this problem. No physical units are involved.\n\n- Case $1$ (happy path): $m=50$, $n=40$, random seed $1234$, singular values $\\sigma_i = 10^{-(i-1)/8}$ for $i=1,\\dots,n$, coefficients $\\hat{x}_i = \\exp(-(i-1)/7)$ for $i=1,\\dots,n$, noise level $\\mathrm{level} = 10^{-3}$.\n- Case $2$ (boundary, no noise): $m=50$, $n=40$, random seed $2021$, singular values $\\sigma_i = 10^{-(i-1)/8}$, coefficients $\\hat{x}_i = \\exp(-(i-1)/7)$, noise level $\\mathrm{level} = 0$.\n- Case $3$ (edge, stronger ill-conditioning and more noise): $m=80$, $n=60$, random seed $999$, singular values $\\sigma_i = 10^{-(i-1)/6}$, coefficients $\\hat{x}_i = \\exp(-(i-1)/10)$, noise level $\\mathrm{level} = 2 \\cdot 10^{-2}$.\n\nFor each case, compute the sequence of coefficients $c_i$ associated with the incremental inclusion of $v_i$ and apply the spike detection rule with $p=5$, $\\alpha=3$, $\\beta=2$. The required output for each case is the integer index $k_{\\star}$ (using one-based indexing for $i$) at which the first spike occurs; if no spike is detected, output $-1$ for that case.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three cases in order, for example $[k_1,k_2,k_3]$. No additional text should be printed.",
            "solution": "The posed problem is valid. It is a well-defined task in computational science, grounded in the established theory of linear inverse problems and regularization using truncated singular value decomposition (TSVD). The problem is scientifically sound, self-contained, objective, and provides all necessary data and algorithmic definitions for a unique solution.\n\nThe core of the problem is to determine the coefficients of the TSVD solution and then apply a specific algorithmic rule to detect noise-induced instability. We begin by deriving the formula for these coefficients from first principles.\n\nThe problem is to find the solution $x_k$ that minimizes the residual norm for a given truncation level $k$. The solution $x_k$ is constrained to the subspace $\\mathcal{S}_k = \\mathrm{span}\\{v_1, \\dots, v_k\\}$, where $v_i$ are the right singular vectors of the matrix $A$.\n$$\nx_k = \\arg\\min_{x \\in \\mathcal{S}_k} \\|A x - b\\|_2^2\n$$\nAny vector $x \\in \\mathcal{S}_k$ can be uniquely represented as a linear combination of the orthonormal basis vectors $\\{v_1, \\dots, v_k\\}$:\n$$\nx = \\sum_{i=1}^k c_i v_i\n$$\nwhere $c_i$ are the coefficients we need to determine. Substituting this representation into the objective function, we seek to minimize:\n$$\n\\left\\| A \\left(\\sum_{i=1}^k c_i v_i\\right) - b \\right\\|_2^2 = \\left\\| \\left(\\sum_{i=1}^k c_i A v_i\\right) - b \\right\\|_2^2\n$$\nFrom the definition of the singular value decomposition, $A = U \\Sigma V^{\\top}$, we have the fundamental relationship $A v_i = \\sigma_i u_i$ for $i=1, \\dots, n$, where $u_i$ are the left singular vectors and $\\sigma_i$ are the singular values. Substituting this into the expression gives:\n$$\n\\left\\| \\left(\\sum_{i=1}^k c_i \\sigma_i u_i\\right) - b \\right\\|_2^2\n$$\nThe left singular vectors $\\{u_i\\}_{i=1}^m$ form an orthonormal basis for $\\mathbb{R}^m$. The squared Euclidean norm of a vector is the sum of the squares of its coordinates in any orthonormal basis. We can express the vector inside the norm, $r = (\\sum_{i=1}^k c_i \\sigma_i u_i) - b$, in the basis of left singular vectors. The coordinate of $r$ along $u_j$ is $u_j^{\\top} r$.\nThe squared norm is therefore:\n$$\n\\|r\\|_2^2 = \\sum_{j=1}^m (u_j^{\\top} r)^2 = \\sum_{j=1}^m \\left( u_j^{\\top} \\left( \\sum_{i=1}^k c_i \\sigma_i u_i \\right) - u_j^{\\top} b \\right)^2\n$$\nUsing the orthonormality property $u_j^{\\top} u_i = \\delta_{ij}$ (the Kronecker delta), the expression simplifies.\nFor $j \\in \\{1, \\dots, k\\}$, the term $u_j^{\\top} (\\sum_{i=1}^k c_i \\sigma_i u_i)$ becomes $c_j \\sigma_j$.\nFor $j \\in \\{k+1, \\dots, m\\}$, the term $u_j^{\\top} (\\sum_{i=1}^k c_i \\sigma_i u_i)$ becomes $0$.\nThis allows us to split the sum over $j$:\n$$\n\\|r\\|_2^2 = \\sum_{j=1}^k (c_j \\sigma_j - u_j^{\\top} b)^2 + \\sum_{j=k+1}^m (- u_j^{\\top} b)^2\n$$\nTo minimize this expression with respect to the coefficients $\\{c_1, \\dots, c_k\\}$, we only need to consider the first sum, as the second sum is independent of these coefficients. The first sum is a sum of non-negative terms. The minimum value of this sum, which is $0$, is achieved when each term is individually zero. Therefore, for each $j \\in \\{1, \\dots, k\\}$, we must have:\n$$\nc_j \\sigma_j - u_j^{\\top} b = 0\n$$\nSolving for $c_j$ yields the well-known formula for the coefficients of the TSVD solution:\n$$\nc_j = \\frac{u_j^{\\top} b}{\\sigma_j}\n$$\nThe problem defines $c_i$ as the coefficient that enters the solution as the truncation level is increased from $i-1$ to $i$. Our derivation shows that the coefficient for the basis vector $v_i$ is given by the formula above and does not depend on the overall truncation level $k$ (as long as $k \\ge i$). Thus, the sequence of coefficients to be analyzed is simply $\\{c_i\\}_{i=1}^n$.\n\nWith the form of the coefficients established, the rest of the problem is algorithmic.\n1.  For each test case, we construct the matrix $A$ from its specified singular value decomposition. This involves generating random orthogonal matrices $U$ and $V$ and a diagonal matrix $\\Sigma$ of specified singular values. The matrix $A$ is formed as $A = U_{econ} \\Sigma_{n \\times n} V^{\\top}$, where $U_{econ}$ consists of the first $n$ columns of $U$.\n2.  The \"true\" solution $x_{\\mathrm{true}}$ is constructed in the basis of right singular vectors, and the \"clean\" data vector is computed as $b_{\\mathrm{clean}} = A x_{\\mathrm{true}}$.\n3.  Noise is added to obtain the final data vector $b = b_{\\mathrm{clean}} + \\varepsilon$, where the noise standard deviation is scaled relative to the norm of the clean signal.\n4.  The coefficients $c_i$ are calculated for $i=1, \\dots, n$ using the derived formula $c_i = (u_i^{\\top} b)/\\sigma_i$.\n5.  The specified spike detection rule is applied to the sequence of absolute coefficients $|c_i|$. With parameters $p=5$, $\\alpha=3$, and $\\beta=2$, we compute a baseline from the first $p$ coefficients: $\\mathrm{baseline} = \\frac{1}{p} \\sum_{i=1}^{p} |c_i|$. We then search for the first index $i > p$ where both $|c_i| > \\alpha \\cdot \\mathrm{baseline}$ and $|c_i| > \\beta \\cdot |c_{i-1}|$. The $1$-based index of the first occurrence is the result. If no such index is found, the result is $-1$.\n\nThis procedure is implemented for each of the three test cases specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef run_case(m, n, seed, sigma_denominator, x_hat_denominator, level, p, alpha, beta):\n    \"\"\"\n    Runs a single test case for spike detection in TSVD.\n\n    Args:\n        m (int): Number of rows for matrix A.\n        n (int): Number of columns for matrix A.\n        seed (int): Random seed for reproducibility.\n        sigma_denominator (float): Denominator in the exponent for singular values.\n        x_hat_denominator (float): Denominator in the exponent for true coefficients.\n        level (float): Relative noise level.\n        p (int): Number of initial coefficients for baseline calculation.\n        alpha (float): Multiplier for the baseline threshold.\n        beta (float): Multiplier for the previous coefficient threshold.\n\n    Returns:\n        int: The 1-based index of the first detected spike, or -1 if no spike is found.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct matrices U, Sigma, V\n    # Generate Haar-like orthogonal matrices from QR of standard normal matrices\n    U_full, _ = np.linalg.qr(rng.standard_normal((m, m)))\n    V, _ = np.linalg.qr(rng.standard_normal((n, n)))\n    \n    # Use the economy-size U, which has orthonormal columns\n    U_econ = U_full[:, :n]\n\n    # Singular values (sigma_i = 10**(-(i-1)/C))\n    indices = np.arange(1, n + 1)\n    s_vals = 10.0**(-(indices - 1) / sigma_denominator)\n    Sigma_n = np.diag(s_vals)\n\n    # 2. Construct A, x_true, and b\n    A = U_econ @ Sigma_n @ V.T\n\n    # True solution coefficients in the V basis (x_hat_i = exp(-(i-1)/C))\n    x_hat = np.exp(-(indices - 1) / x_hat_denominator)\n    \n    # True solution vector\n    x_true = V @ x_hat\n\n    # Clean data vector b\n    b_clean = A @ x_true\n\n    # Additive noise\n    if level > 0.0:\n        norm_b_clean = np.linalg.norm(b_clean)\n        sigma_noise = level * norm_b_clean / np.sqrt(m)\n        noise = rng.normal(0.0, sigma_noise, size=m)\n        b = b_clean + noise\n    else:\n        b = b_clean\n\n    # 3. Compute TSVD solution coefficients\n    # c_i = (u_i^T b) / sigma_i\n    uT_b = U_econ.T @ b\n    c = uT_b / s_vals\n    c_abs = np.abs(c)\n\n    # 4. Apply spike detection rule\n    if n <= p:\n        return -1\n\n    # Baseline using the first p coefficients (0-indexed to p-1)\n    baseline = np.mean(c_abs[0:p])\n\n    # Search for spike for indices i > p (1-based), which is i >= p (0-based)\n    for i in range(p, n):\n        cond1 = c_abs[i] > alpha * baseline\n        cond2 = c_abs[i] > beta * c_abs[i-1]\n        \n        if cond1 and cond2:\n            return i + 1  # Return 1-based index\n\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define spike detection parameters\n    p = 5\n    alpha = 3.0\n    beta = 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: m=50, n=40, seed=1234, sigma_i=10^-(i-1)/8, x_hat_i=exp(-(i-1)/7), level=1e-3\n        {\"m\": 50, \"n\": 40, \"seed\": 1234, \"sigma_denom\": 8.0, \"x_hat_denom\": 7.0, \"level\": 1e-3},\n        # Case 2: m=50, n=40, seed=2021, sigma_i=10^-(i-1)/8, x_hat_i=exp(-(i-1)/7), level=0\n        {\"m\": 50, \"n\": 40, \"seed\": 2021, \"sigma_denom\": 8.0, \"x_hat_denom\": 7.0, \"level\": 0.0},\n        # Case 3: m=80, n=60, seed=999, sigma_i=10^-(i-1)/6, x_hat_i=exp(-(i-1)/10), level=2e-2\n        {\"m\": 80, \"n\": 60, \"seed\": 999, \"sigma_denom\": 6.0, \"x_hat_denom\": 10.0, \"level\": 2e-2},\n    ]\n\n    results = []\n    for case in test_cases:\n        k_star = run_case(\n            m=case[\"m\"],\n            n=case[\"n\"],\n            seed=case[\"seed\"],\n            sigma_denominator=case[\"sigma_denom\"],\n            x_hat_denominator=case[\"x_hat_denom\"],\n            level=case[\"level\"],\n            p=p,\n            alpha=alpha,\n            beta=beta\n        )\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The performance of any numerical algorithm, including TSVD, can be significantly affected by the conditioning of the input matrix. This hands-on practice explores preconditioning, a technique to transform a problem into a form that is better suited for numerical solution . You will implement column scaling to normalize a poorly scaled matrix $A$ and discover how this simple preprocessing step can improve the matrix's condition number, leading to a more accurate and stable TSVD solution.",
            "id": "3201033",
            "problem": "You are given a sequence of linear inverse problems of the form $A x \\approx b$ where $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, and $b \\in \\mathbb{R}^{m}$. The task is to study how scaling the columns of $A$ changes the singular spectrum and how this impacts the solution obtained by truncated singular value decomposition (TSVD), and to propose a preprocessing step that improves numerical stability. Your program must implement the following, starting only from core definitions.\n\nFundamental base permitted:\n- The definition of the singular value decomposition (SVD): any real matrix $A \\in \\mathbb{R}^{m \\times n}$ admits a factorization into $A = U \\Sigma V^{\\top}$ where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative diagonal entries (the singular values) in nonincreasing order.\n- The least squares objective $\\min_{x} \\|A x - b\\|_{2}$ and its solution through the pseudoinverse of $A$ when $A$ has full column rank.\n- The concept that noisy small singular values amplify noise in $b$ when inverted, motivating truncation.\n\nRequired approach:\n1. Implement a TSVD solver that, given $A$, $b$, and a truncation index $k$, computes an approximate solution by restricting the solution to the span of the top $k$ right singular vectors of $A$. Do not assume any pre-derived formula beyond the above definitions.\n2. Implement a column scaling preprocessing of $A$ using a diagonal matrix $D = \\mathrm{diag}(d_{1},\\dots,d_{n})$ with $d_{j} = \\max\\{\\|A_{:,j}\\|_{2}, \\delta\\}$ for a small floor parameter $\\delta > 0$ to avoid division by zero. Define the scaled system as $\\bar{A} = A D^{-1}$ and the variable change $y = D x$ so that $\\bar{A} y \\approx b$. Solve for $y$ via TSVD and recover $x$ by $x = D^{-1} y$.\n3. Quantify the effect of scaling on the singular spectrum via the spectral condition number $\\kappa(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$ and report the improvement factor $\\kappa(A)/\\kappa(\\bar{A})$.\n4. Quantify the effect of scaling on the TSVD solution quality via the relative error $\\|x_{k} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$, both without scaling and with the proposed scaling, and report whether scaling improved the error.\n\nTest suite:\nFor each test case below, construct $b$ as $b = A x_{\\mathrm{true}} + \\eta$, where $\\eta$ is a given fixed noise vector. Use the truncation index $k$ as specified. Use the floor parameter $\\delta = 10^{-12}$ for all cases.\n\n- Case $1$ (well-scaled baseline):\n  - $m = 4$, $n = 3$, $k = 2$.\n  - $A_{1}$ has rows $(1.0, 0.9, -0.3)$, $(0.4, -1.2, 0.5)$, $(-0.7, 0.3, 1.1)$, $(0.6, -0.8, 0.2)$.\n  - $x_{\\mathrm{true},1} = (0.5, -1.0, 0.3)$.\n  - $\\eta_{1} = (10^{-4}, -2 \\cdot 10^{-4}, 1.5 \\cdot 10^{-4}, -10^{-4})$.\n\n- Case $2$ (severely unbalanced column scales):\n  - $m = 4$, $n = 3$, $k = 2$.\n  - Base matrix $A_{2}^{\\mathrm{base}}$ has rows $(1.0, 0.0, 0.001)$, $(0.0, 1.0, 0.002)$, $(0.001, 0.002, 1.0)$, $(1.0, -1.0, 0.5)$.\n  - Column scaling vector $s = (1.0, 10^{-3}, 10^{3})$; construct $A_{2} = A_{2}^{\\mathrm{base}} \\cdot \\mathrm{diag}(s)$ (i.e., scale each column $j$ by $s_{j}$).\n  - $x_{\\mathrm{true},2} = (0.5, -0.3, 0.2)$.\n  - $\\eta_{2} = (0.1, -0.2, 0.15, -0.1)$.\n\n- Case $3$ (nearly collinear columns):\n  - $m = 4$, $n = 3$, $k = 2$.\n  - $A_{3}$ has rows $(1.0, 1.0001, 0.0)$, $(2.0, 2.0002, 0.001)$, $(-1.0, -1.0001, -0.002)$, $(0.5, 0.50005, 0.0005)$.\n  - $x_{\\mathrm{true},3} = (1.0, -1.0, 2.0)$.\n  - $\\eta_{3} = (10^{-4}, -5 \\cdot 10^{-5}, 2 \\cdot 10^{-4}, -1.5 \\cdot 10^{-4})$.\n\n- Case $4$ (nearly zero column):\n  - $m = 4$, $n = 3$, $k = 2$.\n  - $A_{4}$ has rows $(10^{-8}, 1.0, 0.0)$, $(2 \\cdot 10^{-8}, 0.0, 1.0)$, $(-10^{-8}, -1.0, -1.0)$, $(0.0, 0.5, 0.5)$.\n  - $x_{\\mathrm{true},4} = (1.0, 1.0, -1.0)$.\n  - $\\eta_{4} = (10^{-5}, -10^{-5}, 2 \\cdot 10^{-5}, -2 \\cdot 10^{-5})$.\n\nFor each case, compute and aggregate the following four quantities in order:\n- The relative error without scaling, $\\|x_{k}^{\\mathrm{unscaled}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$, as a float.\n- The relative error with scaling, $\\|x_{k}^{\\mathrm{scaled}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$, as a float.\n- A boolean flag equal to true if the scaled relative error is strictly smaller than the unscaled relative error, and false otherwise.\n- The condition number improvement factor $\\kappa(A)/\\kappa(\\bar{A})$, as a float.\n\nFinal output format:\nYour program should produce a single line of output containing a single, flat list with the results for all four cases concatenated in order, enclosed in square brackets and separated by commas, i.e.,\n$[e_{1}^{\\mathrm{un}}, e_{1}^{\\mathrm{sc}}, \\mathrm{improve}_{1}, r_{1}, e_{2}^{\\mathrm{un}}, e_{2}^{\\mathrm{sc}}, \\mathrm{improve}_{2}, r_{2}, e_{3}^{\\mathrm{un}}, e_{3}^{\\mathrm{sc}}, \\mathrm{improve}_{3}, r_{3}, e_{4}^{\\mathrm{un}}, e_{4}^{\\mathrm{sc}}, \\mathrm{improve}_{4}, r_{4}]$,\nwhere $e_{i}^{\\mathrm{un}}$ and $e_{i}^{\\mathrm{sc}}$ are floats, $\\mathrm{improve}_{i}$ are booleans, and $r_{i}$ are floats. Round all floats to $6$ decimal places before printing.",
            "solution": "The problem requires the implementation and analysis of a truncated singular value decomposition (TSVD) solver for linear inverse problems of the form $A x \\approx b$, and an evaluation of a column-scaling preprocessing technique. The analysis will be performed on several test cases designed to highlight different sources of numerical ill-conditioning.\n\n### 1. Truncated Singular Value Decomposition (TSVD)\n\nThe problem is to find a solution $x \\in \\mathbb{R}^{n}$ that minimizes the least-squares objective function:\n$$ \\min_{x} \\|A x - b\\|_{2}^{2} $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, and $b \\in \\mathbb{R}^{m}$. The Singular Value Decomposition (SVD) of $A$ provides a powerful tool for analyzing and solving this problem. The SVD of $A$ is given by:\n$$ A = U \\Sigma V^{\\top} $$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative real numbers $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} \\ge 0$ on its diagonal, which are the singular values of $A$. The columns of $U$, denoted $\\{u_i\\}_{i=1}^m$, and the columns of $V$, denoted $\\{v_i\\}_{i=1}^n$, are the left and right singular vectors, respectively.\n\nSubstituting the SVD into the objective function:\n$$ \\|U \\Sigma V^{\\top} x - b\\|_{2}^{2} $$\nSince $U$ is an orthogonal matrix, multiplication by $U^{\\top}$ preserves the Euclidean norm, $\\|z\\|_2 = \\|U^{\\top}z\\|_2$. We can thus transform the objective:\n$$ \\|U^{\\top}(U \\Sigma V^{\\top} x - b)\\|_{2}^{2} = \\|\\Sigma V^{\\top} x - U^{\\top} b\\|_{2}^{2} $$\nLet's introduce a change of variables $z = V^{\\top} x$. Since $V$ is orthogonal, $x = Vz$. The vector $z$ represents the coordinates of $x$ in the basis formed by the right singular vectors $\\{v_i\\}$. The objective function becomes:\n$$ \\|\\Sigma z - c\\|_{2}^{2}, \\quad \\text{where } c = U^{\\top} b $$\nExpanding this expression in terms of its components yields:\n$$ \\sum_{i=1}^{n} (\\sigma_{i} z_{i} - c_{i})^{2} + \\sum_{i=n+1}^{m} c_{i}^{2} $$\nThe second term is the unchangeable part of the residual. To minimize the sum, we must choose $z_i$ to minimize each term in the first sum. For non-zero singular values $\\sigma_i > 0$, the minimum is achieved at $z_{i} = c_{i} / \\sigma_{i}$. For $\\sigma_i = 0$, any $z_i$ is a minimizer, but typically one chooses $z_i=0$ for the minimum norm solution. This gives the well-known pseudoinverse solution:\n$$ x = Vz = \\sum_{i=1}^{n} \\frac{c_i}{\\sigma_i} v_i = \\sum_{i=1}^{n} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\nwhere the sum is restricted to indices with $\\sigma_i > 0$. In ill-posed problems, some singular values $\\sigma_i$ are very small (but non-zero), causing the term $(u_i^\\top b)/\\sigma_i$ to be very large. If the data vector $b$ contains noise, this small $\\sigma_i$ will amplify the noise component in the direction of $u_i$, corrupting the solution.\n\nTSVD is a regularization method that addresses this by truncating the sum, thereby filtering out the contributions from small singular values. The solution is restricted to the subspace spanned by the first $k$ right singular vectors, where $k$ is the truncation index. The TSVD solution, $x_k$, is defined as:\n$$ x_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\nThis is equivalent to computing $x_k = A_k^\\dagger b$, where $A_k = U_k \\Sigma_k V_k^\\top$ is the best rank-$k$ approximation of $A$.\n\n### 2. Column Scaling Preprocessing\n\nIf the columns of matrix $A$ have widely varying norms, the matrix can be ill-conditioned. Column scaling is a form of preconditioning designed to mitigate this issue. We define a diagonal scaling matrix $D \\in \\mathbb{R}^{n \\times n}$ where the diagonal entries are based on the norms of the columns of $A$:\n$$ D = \\mathrm{diag}(d_{1}, d_{2}, \\dots, d_{n}) \\quad \\text{with} \\quad d_{j} = \\max\\{\\|A_{:,j}\\|_{2}, \\delta\\} $$\nHere, $A_{:,j}$ is the $j$-th column of $A$, and $\\delta > 0$ is a small floor parameter to prevent division by zero for any zero columns.\n\nWe then define a scaled matrix $\\bar{A} = A D^{-1}$. The original problem $Ax \\approx b$ is transformed through a change of variables, $y = D x$, which implies $x = D^{-1} y$. Substituting this into the problem yields:\n$$ A(D^{-1} y) \\approx b \\implies (A D^{-1}) y \\approx b \\implies \\bar{A} y \\approx b $$\nThe columns of $\\bar{A}$ are now normalized (their L2-norm is approximately $1$), which typically improves the conditioning of the system. The procedure to solve the scaled problem is:\n1.  Compute the scaling matrix $D$ and the scaled matrix $\\bar{A} = A D^{-1}$.\n2.  Solve the scaled system $\\bar{A} y \\approx b$ for $y$ using TSVD with truncation parameter $k$, obtaining $y_k$.\n$$ y_k = \\sum_{i=1}^{k} \\frac{\\bar{u}_{i}^{\\top} b}{\\bar{\\sigma}_{i}} \\bar{v}_{i} $$\nwhere $\\bar{A} = \\bar{U} \\bar{\\Sigma} \\bar{V}^\\top$ is the SVD of the scaled matrix.\n3.  Recover the solution for the original variable $x$ by reversing the change of variables: $x_{k}^{\\mathrm{scaled}} = D^{-1} y_k$.\n\n### 3. Analysis Metrics\n\nTo evaluate the effectiveness of this scaling strategy, we use two primary metrics:\n\n1.  **Spectral Condition Number**: For an $m \\times n$ matrix $A$ of rank $n$, the condition number is $\\kappa(A) = \\sigma_{1}(A) / \\sigma_{n}(A)$. A large condition number indicates that the problem is ill-conditioned. We measure the improvement due to scaling by the ratio $\\kappa(A)/\\kappa(\\bar{A})$. A ratio greater than $1$ signifies an improvement in conditioning.\n\n2.  **Relative Solution Error**: Given the true solution $x_{\\mathrm{true}}$, we can measure the accuracy of a computed solution $x_k$ using the relative error:\n$$ E_{\\mathrm{rel}} = \\frac{\\|x_{k} - x_{\\mathrm{true}}\\|_{2}}{\\|x_{\\mathrm{true}}\\|_{2}} $$\nWe will compute this error for both the unscaled TSVD solution ($x_k^{\\mathrm{unscaled}}$) and the solution from the scaled system ($x_k^{\\mathrm{scaled}}$) and compare them.\n\nBy applying this methodology to the specified test cases, we can systematically investigate how column scaling affects the singular value spectrum and the quality of the regularized solution under different types of ill-conditioning.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    delta = 1e-12\n\n    # Case 1: Well-scaled baseline\n    A1 = np.array([\n        [1.0, 0.9, -0.3],\n        [0.4, -1.2, 0.5],\n        [-0.7, 0.3, 1.1],\n        [0.6, -0.8, 0.2]\n    ])\n    xtrue1 = np.array([0.5, -1.0, 0.3])\n    eta1 = np.array([1e-4, -2e-4, 1.5e-4, -1e-4])\n    k1 = 2\n\n    # Case 2: Severely unbalanced column scales\n    A2_base = np.array([\n        [1.0, 0.0, 0.001],\n        [0.0, 1.0, 0.002],\n        [0.001, 0.002, 1.0],\n        [1.0, -1.0, 0.5]\n    ])\n    s2 = np.array([1.0, 1e-3, 1e3])\n    A2 = A2_base @ np.diag(s2)\n    xtrue2 = np.array([0.5, -0.3, 0.2])\n    eta2 = np.array([0.1, -0.2, 0.15, -0.1])\n    k2 = 2\n\n    # Case 3: Nearly collinear columns\n    A3 = np.array([\n        [1.0, 1.0001, 0.0],\n        [2.0, 2.0002, 0.001],\n        [-1.0, -1.0001, -0.002],\n        [0.5, 0.50005, 0.0005]\n    ])\n    xtrue3 = np.array([1.0, -1.0, 2.0])\n    eta3 = np.array([1e-4, -5e-5, 2e-4, -1.5e-4])\n    k3 = 2\n\n    # Case 4: Nearly zero column\n    A4 = np.array([\n        [1e-8, 1.0, 0.0],\n        [2e-8, 0.0, 1.0],\n        [-1e-8, -1.0, -1.0],\n        [0.0, 0.5, 0.5]\n    ])\n    xtrue4 = np.array([1.0, 1.0, -1.0])\n    eta4 = np.array([1e-5, -1e-5, 2e-5, -2e-5])\n    k4 = 2\n\n    test_cases = [\n        (A1, xtrue1, eta1, k1),\n        (A2, xtrue2, eta2, k2),\n        (A3, xtrue3, eta3, k3),\n        (A4, xtrue4, eta4, k4),\n    ]\n\n    results = []\n\n    def tsvd_solver(A, b, k):\n        \"\"\"\n        Computes the TSVD solution x_k for A*x = b.\n        x_k = sum_{i=1 to k} (u_i^T * b / s_i) * v_i\n        \"\"\"\n        # We need the \"economy\" SVD where U is m x n\n        U, s, Vt = np.linalg.svd(A, full_matrices=False)\n        \n        # Truncate to k\n        Uk = U[:, :k]\n        sk = s[:k]\n        Vtk = Vt[:k, :]\n\n        # Calculate solution for x_k\n        # compute alpha_i = u_i^T * b / s_i\n        # then x_k = V_k * alpha\n        # V_k = Vtk.T\n        alpha = (Uk.T @ b) / sk\n        x_k = Vtk.T @ alpha\n        return x_k\n\n    for A, xtrue, eta, k in test_cases:\n        m, n = A.shape\n        b = A @ xtrue + eta\n        \n        # --- Unscaled Analysis ---\n        x_unscaled = tsvd_solver(A, b, k)\n        err_unscaled = np.linalg.norm(x_unscaled - xtrue) / np.linalg.norm(xtrue)\n        \n        s_unscaled = np.linalg.svd(A, compute_uv=False)\n        # Assuming rank n, use sigma_n\n        cond_A = s_unscaled[0] / s_unscaled[n-1] if s_unscaled[n-1] > 0 else np.inf\n\n        # --- Scaled Analysis ---\n        col_norms = np.linalg.norm(A, axis=0)\n        d = np.maximum(col_norms, delta)\n        D_inv = np.diag(1.0 / d)\n        \n        A_bar = A @ D_inv\n        \n        y_scaled = tsvd_solver(A_bar, b, k)\n        x_scaled = D_inv @ y_scaled\n        \n        err_scaled = np.linalg.norm(x_scaled - xtrue) / np.linalg.norm(xtrue)\n        \n        s_scaled = np.linalg.svd(A_bar, compute_uv=False)\n        cond_A_bar = s_scaled[0] / s_scaled[n-1] if s_scaled[n-1] > 0 else np.inf\n        \n        # --- Collect Results ---\n        improvement_flag = err_scaled < err_unscaled\n        cond_improvement_factor = cond_A / cond_A_bar if cond_A_bar > 0 else np.inf\n\n        results.extend([\n            round(err_unscaled, 6),\n            round(err_scaled, 6),\n            improvement_flag,\n            round(cond_improvement_factor, 6)\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}