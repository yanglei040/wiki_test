{
    "hands_on_practices": [
        {
            "introduction": "This first practice tackles the problem of signal dequantization, a common task in image processing to remove artifacts like color banding. It serves as a perfect arena to compare two foundational regularization strategies: Tikhonov ($\\ell_2$) smoothing and Total Variation (TV) minimization. By implementing both, you will gain direct insight into how penalizing the $\\ell_2$ norm of the gradient encourages general smoothness, while penalizing the $\\ell_1$ norm of the gradient (TV) promotes sharp edges and piecewise-constant solutions .",
            "id": "3185702",
            "problem": "Consider a one-dimensional discrete signal of length $N$ represented as a vector $x \\in \\mathbb{R}^{N}$. An observed signal $y \\in \\mathbb{R}^{N}$ is produced by uniform mid-tread quantization with step size $q > 0$, modeled by the operator $Q_{q} : \\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ defined component-wise as $y_{i} = Q_{q}(x_{i}) = q \\cdot \\mathrm{round}\\!\\left(\\frac{x_{i}}{q}\\right)$ for $i = 1, \\dots, N$. The inverse problem is to estimate $x$ from $y$ by imposing physically meaningful constraints that arise from quantization and by applying regularization to favor structurally simple solutions.\n\nThe quantization process implies consistency constraints: for each index $i$, the unknown $x_{i}$ must lie in the bin interval $[y_{i} - \\frac{q}{2}, \\, y_{i} + \\frac{q}{2}]$. Let the feasible set be $C = \\{x \\in \\mathbb{R}^{N} \\mid y_{i} - \\frac{q}{2} \\le x_{i} \\le y_{i} + \\frac{q}{2} \\ \\text{for all} \\ i\\}$. Define the discrete forward difference operator $D : \\mathbb{R}^{N} \\to \\mathbb{R}^{N-1}$ by $(Dx)_{i} = x_{i+1} - x_{i}$ for $i = 1, \\dots, N-1$. The Total Variation (TV) of $x$ is the $\\ell_1$ norm of its discrete gradient, namely $\\mathrm{TV}(x) = \\|Dx\\|_{1} = \\sum_{i=1}^{N-1} |x_{i+1} - x_{i}|$.\n\nYou are required to implement two reconstruction approaches:\n\n1. A TV-regularized inverse rounding method: find an estimate $x^{\\mathrm{TV}}$ that solves the constrained TV minimization problem\n$$\n\\min_{x \\in C} \\ \\mathrm{TV}(x).\n$$\nThis problem enforces fidelity to the quantization bins and penalizes rapid variations to remove banding artifacts.\n\n2. A quadratic smoothing method using the Euclidean two-norm ($\\ell_2$) of the gradient: find an estimate $x^{\\ell_2}$ that solves\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\ \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\alpha \\|Dx\\|_{2}^{2},\n$$\nwhere $\\alpha > 0$ is a regularization parameter that controls the strength of smoothing. This method does not enforce bin consistency, and therefore may blur edges and introduce smoothing artifacts.\n\nFor evaluation, use the Mean Squared Error (MSE) defined by\n$$\n\\mathrm{MSE}(x^{\\star}, x) = \\frac{1}{N} \\sum_{i=1}^{N} (x^{\\star}_{i} - x_{i})^{2},\n$$\nwhere $x^{\\star}$ is the unquantized ground truth signal.\n\nImplement the following test suite. In all cases, use $N = 64$. For each test, synthesize the ground truth $x^{\\star}$, compute $y = Q_{q}(x^{\\star})$ by rounding to the nearest multiple of $q$, perform both reconstructions, compute their MSE with respect to $x^{\\star}$, and produce a boolean indicating whether the TV-regularized reconstruction is at least as accurate as the $\\ell_2$-smoothing reconstruction, that is, whether $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_2})$.\n\n- Test case $1$ (piecewise-constant signal, coarse quantization):\n  - Ground truth $x^{\\star}$: segments of length $16$ with levels $[0.0, \\, 0.7, \\, -0.1, \\, 0.5]$.\n  - Quantization step $q = 0.25$.\n  - $\\ell_2$ regularization parameter $\\alpha = 0.5$.\n\n- Test case $2$ (constant signal exactly on a quantization level):\n  - Ground truth $x^{\\star}_{i} = 0.5$ for all $i$.\n  - Quantization step $q = 0.5$.\n  - $\\ell_2$ regularization parameter $\\alpha = 0.5$.\n\n- Test case $3$ (high-frequency sinusoid):\n  - Ground truth $x^{\\star}_{i} = 0.7 \\sin\\!\\left(2\\pi \\cdot 8 \\cdot \\frac{i-1}{N}\\right)$ for $i = 1, \\dots, N$.\n  - Quantization step $q = 0.3$.\n  - $\\ell_2$ regularization parameter $\\alpha = 0.3$.\n\n- Test case $4$ (piecewise-constant signal, fine quantization):\n  - Ground truth $x^{\\star}$: same as Test case $1$.\n  - Quantization step $q = 0.05$.\n  - $\\ell_2$ regularization parameter $\\alpha = 0.2$.\n\nYour implementation details must adhere to the following principles:\n- The constrained TV minimization should be solved as a convex optimization problem using a principled first-order method that respects the bin constraints $C$ and the TV regularizer. Use a forward difference for $D$ and the appropriate adjoint for $D^{\\top}$. Impose reflecting (Neumann-type) boundary behavior for the discrete operators.\n- The $\\ell_2$ smoothing solution should be computed by solving the normal equations that arise from the first-order optimality condition, resulting in a symmetric positive definite tridiagonal linear system.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4]$), where each entry is a boolean for the corresponding test case indicating whether $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_2})$. No physical units, angles, or percentages are involved in this problem; all quantities are dimensionless real scalars and vectors.",
            "solution": "The problem requires the implementation and comparison of two distinct regularization methods for reconstructing a one-dimensional signal $x \\in \\mathbb{R}^{N}$ from its quantized measurement $y \\in \\mathbb{R}^{N}$. The quantization is a uniform mid-tread process with step size $q > 0$, defined by $y_{i} = q \\cdot \\mathrm{round}(x_i / q)$. This relationship implies that the true signal values $x_i$ must lie within the intervals $[y_i - q/2, y_i + q/2]$, defining a convex feasibility set $C$. We will compare a method based on Total Variation (TV) minimization that enforces this constraint against a quadratic smoothing method that does not.\n\nThe first method, quadratic or $\\ell_2$ smoothing, seeks an estimate $x^{\\ell_2}$ by solving the unconstrained optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\ J(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\alpha \\|Dx\\|_{2}^{2}\n$$\nHere, $y$ is the observed quantized signal, $\\alpha > 0$ is a regularization parameter, and $D$ is the forward difference operator, $(Dx)_i = x_{i+1} - x_i$. The objective function $J(x)$ is quadratic and strictly convex. The unique minimizer is found by setting the gradient $\\nabla J(x)$ to zero.\nThe gradient is given by:\n$$\n\\nabla J(x) = (x - y) + 2\\alpha D^{\\top}Dx = 0\n$$\nThis yields the normal equations, a system of linear equations for $x$:\n$$\n(I + 2\\alpha D^{\\top}D)x = y\n$$\nwhere $I$ is the identity matrix and $D^{\\top}$ is the adjoint (transpose) of the operator $D$. The matrix $L = D^{\\top}D$ represents the one-dimensional discrete Laplacian with Neumann boundary conditions, as specified by the problem. For the defined forward difference operator $D \\in \\mathbb{R}^{(N-1) \\times N}$, the matrix $L \\in \\mathbb{R}^{N \\times N}$ is symmetric and tridiagonal:\n$$\nL = D^{\\top}D =\n\\begin{pmatrix}\n1 & -1 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 1\n\\end{pmatrix}\n$$\nThe matrix $A = I + 2\\alpha L$ is therefore also symmetric, tridiagonal, and strictly positive definite for $\\alpha > 0$. Such a linear system can be solved efficiently and robustly using a banded matrix solver.\n\nThe second method is Total Variation (TV) regularization, which finds an estimate $x^{\\mathrm{TV}}$ by solving the constrained convex optimization problem:\n$$\n\\min_{x \\in C} \\ \\mathrm{TV}(x) \\quad \\text{where} \\quad \\mathrm{TV}(x) = \\|Dx\\|_1\n$$\nThe feasible set is the hyperrectangle $C = \\{x \\in \\mathbb{R}^{N} \\mid y_i - q/2 \\le x_i \\le y_i + q/2, \\forall i \\}$. This problem minimizes the $\\ell_1$ norm of the signal's gradient subject to consistency with the quantization bins. Since the objective function $\\mathrm{TV}(x)$ is convex but non-smooth and the constraints define a convex set, this problem is well-suited for first-order primal-dual algorithms. We employ the Chambolle-Pock algorithm. The problem is written in the form $\\min_x F(Dx) + G(x)$, where $F(u) = \\|u\\|_1$ and $G(x) = I_C(x)$ is the indicator function of the set $C$, which is $0$ if $x \\in C$ and $+\\infty$ otherwise.\n\nThe Chambolle-Pock iterations (with extrapolation parameter $\\theta=1$) are:\n1. $u^{k+1} = \\mathrm{prox}_{\\sigma F^*} (u^k + \\sigma D \\bar{x}^k)$\n2. $x^{k+1} = \\mathrm{prox}_{\\tau G} (x^k - \\tau D^{\\top} u^{k+1})$\n3. $\\bar{x}^{k+1} = 2x^{k+1} - x^k$\n\nThe proximal operators are standard. The proximal operator of $G$ is the Euclidean projection onto the set $C$:\n$$\n(\\mathrm{prox}_{\\tau G}(z))_i = \\Pi_C(z)_i = \\mathrm{clip}(z_i, y_i - q/2, y_i + q/2)\n$$\nThe convex conjugate of the $\\ell_1$ norm, $F^*(u)$, is the indicator function of the $\\ell_{\\infty}$ unit ball. Its proximal operator is the projection onto this ball:\n$$\n(\\mathrm{prox}_{\\sigma F^*}(v))_i = \\Pi_{\\|\\cdot\\|_{\\infty}\\le 1}(v)_i = \\frac{v_i}{\\max(1, |v_i|)}\n$$\nThe step sizes $\\tau$ and $\\sigma$ must satisfy $\\tau \\sigma \\|D\\|^2 < 1$ for convergence. The squared operator norm $\\|D\\|^2 = \\lambda_{\\max}(D^{\\top}D)$ is bounded by $4$. We select conservative step sizes $\\sigma$ and $\\tau$ to ensure stability.\n\nFor each test case, the ground truth signal $x^{\\star}$ is generated and then quantized to obtain $y$. Both reconstruction methods are applied to $y$ to obtain $x^{\\ell_2}$ and $x^{\\mathrm{TV}}$. The accuracy of each reconstruction is measured using the Mean Squared Error (MSE) with respect to the original signal $x^{\\star}$: $\\mathrm{MSE}(x^{\\star}, x) = \\frac{1}{N} \\|x^{\\star} - x\\|_2^2$. The final output for each case is a boolean value indicating if the TV-based reconstruction is at least as accurate as the $\\ell_2$-based one, i.e., $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_2})$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the signal dequantization problem using two regularization methods\n    and compares their performance on a suite of test cases.\n    \"\"\"\n\n    def generate_x_star(case_num, N):\n        \"\"\"Generates the ground truth signal x_star for a given test case.\"\"\"\n        if case_num == 1 or case_num == 4:\n            x_star = np.zeros(N)\n            levels = [0.0, 0.7, -0.1, 0.5]\n            segment_len = N // 4\n            for i in range(4):\n                start, end = i * segment_len, (i + 1) * segment_len\n                x_star[start:end] = levels[i]\n            return x_star\n        elif case_num == 2:\n            return np.full(N, 0.5)\n        elif case_num == 3:\n            indices = np.arange(N)\n            return 0.7 * np.sin(2 * np.pi * 8 * indices / N)\n        return None\n\n    def quantize(x, q):\n        \"\"\"Performs uniform mid-tread quantization.\"\"\"\n        return q * np.round(x / q)\n\n    def mse(x_star, x):\n        \"\"\"Computes the Mean Squared Error.\"\"\"\n        return np.mean((x_star - x)**2)\n\n    def solve_l2(y, alpha, N):\n        \"\"\"Solves the l2-smoothing problem via normal equations.\"\"\"\n        # A = I + 2*alpha*D^T*D\n        # This matrix is symmetric and tridiagonal.\n        # Main diagonal: 1+2a, 1+4a, ..., 1+4a, 1+2a\n        # Off-diagonals: -2a\n        ab = np.zeros((3, N))\n        \n        # Upper diagonal (u=1)\n        ab[0, 1:] = -2 * alpha\n        \n        # Main diagonal (l=1, u=1)\n        ab[1, :] = 1 + 4 * alpha\n        ab[1, 0] = 1 + 2 * alpha\n        ab[1, -1] = 1 + 2 * alpha\n        \n        # Lower diagonal (l=1)\n        ab[2, :-1] = -2 * alpha\n        \n        x_l2 = solve_banded((1, 1), ab, y)\n        return x_l2\n\n    def solve_tv(y, q, N, iters=2000):\n        \"\"\"Solves the constrained TV minimization problem using Chambolle-Pock.\"\"\"\n        # Step sizes for primal-dual algorithm\n        # We need tau * sigma * ||D||^2 < 1. ||D||^2 <= 4 for 1D Neumann.\n        # We choose tau*sigma = 0.24, which is < 1/4.\n        sigma = 0.5\n        tau = 0.48\n\n        # Initialize variables\n        x = y.copy()\n        x_bar = x.copy()\n        u = np.zeros(N - 1)\n        \n        # Pre-calculate bin limits\n        x_min = y - q / 2.0\n        x_max = y + q / 2.0\n\n        # Operator D and D_T\n        def D(vec):\n            return vec[1:] - vec[:-1]\n\n        def D_T(vec):\n            res = np.zeros(N)\n            res[0] = -vec[0]\n            res[1:-1] = vec[:-1] - vec[1:]\n            res[-1] = vec[-1]\n            return res\n\n        for _ in range(iters):\n            # Dual update (prox of F*)\n            u_next_arg = u + sigma * D(x_bar)\n            u = u_next_arg / np.maximum(1.0, np.abs(u_next_arg))\n            \n            # Primal update (prox of G)\n            x_old = x\n            x_next_arg = x - tau * D_T(u)\n            x = np.clip(x_next_arg, x_min, x_max)\n            \n            # Extrapolation\n            x_bar = 2 * x - x_old\n            \n        return x\n\n    test_cases = [\n        # (case_num, q, alpha)\n        (1, 0.25, 0.5),\n        (2, 0.5, 0.5),\n        (3, 0.3, 0.3),\n        (4, 0.05, 0.2),\n    ]\n\n    results = []\n    N = 64\n\n    for case_num, q, alpha in test_cases:\n        x_star = generate_x_star(case_num, N)\n        y = quantize(x_star, q)\n\n        # L2-smoothing reconstruction\n        x_l2 = solve_l2(y, alpha, N)\n        mse_l2 = mse(x_star, x_l2)\n\n        # TV reconstruction\n        x_tv = solve_tv(y, q, N)\n        mse_tv = mse(x_star, x_tv)\n        \n        results.append(mse_tv <= mse_l2)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While Total Variation is excellent at preserving sharp edges, it can introduce an undesirable artifact known as \"staircasing\" in regions that should be smoothly varying. This exercise delves into more sophisticated, edge-preserving regularizers like the Huber and Perona-Malik penalties, which are designed to mitigate this very issue. By comparing these methods against standard TV on a specially designed signal, you will learn how to choose a regularizer that can preserve sharp discontinuities without flattening subtle but important gradients .",
            "id": "3185682",
            "problem": "You are given the task of designing and implementing a numerical experiment to compare regularization penalties for a one-dimensional inverse problem of denoising. The fundamental base is the variational formulation of inverse problems: given noisy observations $y \\in \\mathbb{R}^{N}$ of an unknown clean signal $x^{\\star} \\in \\mathbb{R}^{N}$, a common approach is to compute an estimate $x \\in \\mathbb{R}^{N}$ by minimizing an energy of the form\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\; \\frac{1}{2}\\lVert x - y \\rVert_{2}^{2} + \\lambda \\, R(x),\n$$\nwhere $\\lambda > 0$ controls the strength of regularization and $R(x)$ is a penalty encoding prior knowledge on $x$. In this problem, you will compare three penalties that are widely used in the context of regularization methods for inverse problems:\n\n- Total Variation (TV): $R_{\\mathrm{TV}}(x) = \\sum_{i=1}^{N-1} \\lvert x_{i+1} - x_{i} \\rvert$.\n- Huberized Total Variation: $R_{\\mathrm{Huber},\\delta}(x) = \\sum_{i=1}^{N-1} \\phi_{\\delta}(x_{i+1} - x_{i})$ with the Huber function\n$$\n\\phi_{\\delta}(d) =\n\\begin{cases}\n\\frac{1}{2\\delta} d^{2}, & \\text{if } \\lvert d \\rvert \\le \\delta, \\\\\n\\lvert d \\rvert - \\frac{\\delta}{2}, & \\text{if } \\lvert d \\rvert > \\delta,\n\\end{cases}\n$$\nfor parameter $\\delta > 0$.\n- Perona–Malik-type penalty: $R_{\\mathrm{PM},k}(x) = \\sum_{i=1}^{N-1} \\psi_{k}(x_{i+1} - x_{i})$ with\n$$\n\\psi_{k}(d) = \\frac{k^{2}}{2} \\log\\!\\big(1 + (d/k)^{2}\\big),\n$$\nfor parameter $k > 0$.\n\nThe goal is to empirically demonstrate that edge-preserving penalties, such as the Huberized Total Variation and the Perona–Malik-type penalty, can reduce the “staircasing” effect commonly observed with standard Total Variation while avoiding oversmoothing of small gradients.\n\nConstruct a synthetic ground-truth signal $x^{\\star} \\in \\mathbb{R}^{N}$ that contains both a small, nonzero gradient region and a sharp edge:\n- Let $N = 256$.\n- Define a ramp with constant slope $m > 0$ starting at index $i_{0}$ and ending at index $i_{1}$, and a sharp step of amplitude $A > 0$ at an index $j$.\n- Precisely, for indices $i \\in \\{0,1,\\dots,N-1\\}$,\n  - $x^{\\star}_{i} = 0$ for $i < i_{0}$,\n  - $x^{\\star}_{i} = m\\,(i - i_{0})$ for $i_{0} \\le i < i_{1}$,\n  - $x^{\\star}_{i} = m\\,(i_{1} - i_{0})$ for $i_{1} \\le i < j$,\n  - $x^{\\star}_{i} = m\\,(i_{1} - i_{0}) + A$ for $i \\ge j$.\n\nGenerate noisy data $y = x^{\\star} + \\eta$, where $\\eta$ is independent and identically distributed Gaussian noise with zero mean and variance $\\sigma^{2}$.\n\nFor each penalty, compute the numerical minimizer of the corresponding energy with the same data $y$ and regularization parameter $\\lambda$. Use a first-order iterative method appropriate to each penalty:\n- For the non-differentiable Total Variation, use a provably convergent first-order method based on convex duality and projections onto the dual feasible set.\n- For the differentiable penalties $R_{\\mathrm{Huber},\\delta}$ and $R_{\\mathrm{PM},k}$, use a gradient-based descent method with a monotone line search that guarantees energy decrease.\n\nDesign quantitative measures to compare the reconstructions in the ramp region $\\{i_{0}, i_{0}+1, \\dots, i_{1}-1\\}$:\n- Define the discrete gradient $g_{i} = x_{i+1} - x_{i}$ for $i \\in \\{0,1,\\dots,N-2\\}$.\n- In the ramp region, compute the “plateau ratio” $P(x)$ as the fraction of indices $i \\in \\{i_{0},\\dots,i_{1}-2\\}$ for which $\\lvert g_{i} \\rvert < \\tau$, where $\\tau$ is a fixed threshold chosen as a fraction of the true slope $m$.\n- Estimate the ramp slope as the mean of $g_{i}$ over the ramp region and report the absolute slope error $\\lvert \\widehat{m}(x) - m \\rvert$.\n\nYour program must construct the signal, add noise, solve the three minimization problems, and evaluate whether each edge-preserving penalty reduces staircasing without oversmoothing the small gradient, when compared to standard Total Variation, according to the following logical criteria:\n- Let $x^{\\mathrm{TV}}$ denote the Total Variation solution and $x^{\\mathrm{E}}$ denote an edge-preserving solution (either Huberized Total Variation or Perona–Malik-type).\n- “Reduced staircasing” is declared if $P(x^{\\mathrm{E}}) < P(x^{\\mathrm{TV}}) - \\varepsilon_{P}$ for a fixed margin $\\varepsilon_{P} > 0$.\n- “No oversmoothing of small gradients” is declared if $\\lvert \\widehat{m}(x^{\\mathrm{E}}) - m \\rvert < \\lvert \\widehat{m}(x^{\\mathrm{TV}}) - m \\rvert - \\varepsilon_{m}$ for a fixed margin $\\varepsilon_{m} > 0$.\n\nYour program should run the following test suite, where each test defines all parameters and a fixed random seed for reproducibility:\n- Test $1$: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.08$, $\\lambda=0.35$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2026$.\n- Test $2$: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.08$, $\\lambda=0.60$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2027$.\n- Test $3$: $N=256$, $i_{0}=30$, $i_{1}=130$, $j=170$, $m=0.015$, $A=1.0$, $\\sigma=0.12$, $\\lambda=0.35$, $\\delta=0.02$, $k=0.06$, $\\tau=0.5\\,m$, $\\varepsilon_{P}=0.08$, $\\varepsilon_{m}=0.0015$, seed $=2028$.\n\nFor each test, produce two boolean results:\n- One boolean for Huberized Total Variation indicating whether both conditions hold relative to Total Variation.\n- One boolean for the Perona–Malik-type penalty indicating whether both conditions hold relative to Total Variation.\n\nFinal output format: Your program should produce a single line of output containing a list of the six boolean results in order of the tests and methods, as a comma-separated list enclosed in square brackets, namely $[\\text{Huber}_{1}, \\text{PM}_{1}, \\text{Huber}_{2}, \\text{PM}_{2}, \\text{Huber}_{3}, \\text{PM}_{3}]$, where each entry is either $\\text{True}$ or $\\text{False}$.",
            "solution": "The problem presents a well-posed numerical experiment designed to compare the performance of different regularization penalties for one-dimensional signal denoising. The problem is scientifically grounded in the principles of variational inverse problems, self-contained with all necessary parameters and definitions, and its objective is formally verifiable through the specified computational procedure. The problem is therefore deemed valid.\n\nThe core task is to find an estimate $x \\in \\mathbb{R}^{N}$ of a true signal $x^{\\star} \\in \\mathbb{R}^{N}$ from noisy observations $y \\in \\mathbb{R}^{N}$ by minimizing a functional of the form:\n$$\n\\min_{x \\in \\mathbb{R}^{N}} E(x) = \\frac{1}{2}\\lVert x - y \\rVert_{2}^{2} + \\lambda \\, R(x)\n$$\nHere, $\\frac{1}{2}\\lVert x - y \\rVert_{2}^{2}$ is the data fidelity term, which promotes closeness of the solution $x$ to the measurements $y$. The term $R(x)$ is a regularization penalty, weighted by a parameter $\\lambda > 0$, that encodes prior knowledge about the structure of the signal $x^{\\star}$. We will compare three specific regularizers.\n\nLet $D: \\mathbb{R}^{N} \\to \\mathbb{R}^{N-1}$ be the discrete forward difference operator, such that $(Dx)_i = x_{i+1} - x_i$ for $i \\in \\{0, \\dots, N-2\\}$. The three regularizers are expressed in terms of the discrete gradient $Dx$:\n\n1.  **Total Variation (TV)**: $R_{\\mathrm{TV}}(x) = \\sum_{i=0}^{N-2} \\lvert (Dx)_i \\rvert = \\lVert Dx \\rVert_{1}$. This penalty is convex and promotes piecewise-constant solutions, which can lead to an undesirable artifact known as \"staircasing\" in regions of smooth gradients.\n\n2.  **Huberized Total Variation (Huber-TV)**: $R_{\\mathrm{Huber},\\delta}(x) = \\sum_{i=0}^{N-2} \\phi_{\\delta}((Dx)_i)$, where the Huber function $\\phi_{\\delta}$ is defined as:\n    $$\n    \\phi_{\\delta}(d) =\n    \\begin{cases}\n    \\frac{1}{2\\delta} d^{2}, & \\text{if } \\lvert d \\rvert \\le \\delta \\\\\n    \\lvert d \\rvert - \\frac{\\delta}{2}, & \\text{if } \\lvert d \\rvert > \\delta\n    \\end{cases}\n    $$\n    This penalty is also convex. It behaves quadratically for small gradients ($\\lvert d \\rvert \\le \\delta$) and linearly for large gradients, providing a smooth approximation to the $\\ell_1$-norm and aiming to reduce staircasing.\n\n3.  **Perona–Malik-type (PM)**: $R_{\\mathrm{PM},k}(x) = \\sum_{i=0}^{N-2} \\psi_{k}((Dx)_i)$, with the potential function:\n    $$\n    \\psi_{k}(d) = \\frac{k^{2}}{2} \\log\\!\\left(1 + \\left(\\frac{d}{k}\\right)^{2}\\right)\n    $$\n    This penalty is non-convex and is known as an edge-preserving potential. It penalizes large gradients less severely than small ones, which can preserve sharp edges while smoothing noise.\n\n**Numerical Minimization Algorithms**\n\nTo find the minimizer $x$ for each regularizer, we employ appropriate first-order iterative methods.\n\n**Total Variation Minimization**: The energy functional $E(x)$ with the TV regularizer is convex but non-differentiable due to the $\\ell_1$-norm. As suggested, we use a method based on convex duality. The primal problem is $\\min_{x} \\frac{1}{2}\\lVert x - y \\rVert_{2}^{2} + \\lambda \\lVert Dx \\rVert_{1}$. Its Fenchel-Rockafellar dual problem is to minimize the following with respect to the dual variable $p \\in \\mathbb{R}^{N-1}$:\n$$\n\\min_{p: \\lVert p \\rVert_{\\infty} \\le 1} \\frac{1}{2} \\left\\lVert y - \\lambda D^{*}p \\right\\rVert_{2}^{2}\n$$\nwhere $D^{*}: \\mathbb{R}^{N-1} \\to \\mathbb{R}^{N}$ is the adjoint of the forward difference operator (negative divergence). This is a differentiable quadratic optimization problem over a simple convex set (an infinity-norm ball). We solve it using projected gradient descent. The iterative update for $p$ is:\n$$\np^{k+1} = \\text{proj}_{\\lVert \\cdot \\rVert_{\\infty} \\le 1} \\left( p^{k} - \\alpha_{k} \\nabla_{p} f(p^k) \\right)\n$$\nwhere $f(p) = \\frac{1}{2} \\lVert y - \\lambda D^{*}p \\rVert_{2}^{2}$ and its gradient is $\\nabla_{p}f(p) = -\\lambda D(y - \\lambda D^{*}p)$. The projection $\\text{proj}_{\\lVert \\cdot \\rVert_{\\infty} \\le 1}(q)$ is a simple element-wise clipping of the vector $q$ to the interval $[-1, 1]$. The step size $\\alpha_k$ must be chosen to ensure convergence; a constant step size $\\alpha < 2/L$ is sufficient, where $L$ is the Lipschitz constant of $\\nabla_p f(p)$, which is $L = \\lambda^2 \\lVert DD^{*} \\rVert_2$. For the 1D case, $\\lVert DD^{*} \\rVert_2 \\le 4$. Once the optimal dual variable $p^{\\star}$ is found, the primal solution is recovered via the relation $x^{\\star} = y - \\lambda D^{*}p^{\\star}$.\n\n**Huber-TV and PM Minimization**: The energy functionals for Huber-TV and PM penalties are differentiable. Therefore, we can use a gradient descent method. The gradient of the energy functional $E(x)$ is:\n$$\n\\nabla E(x) = (x - y) + \\lambda \\nabla R(x)\n$$\nThe gradient of the regularizer, $\\nabla R(x)$, can be expressed compactly as $\\nabla R(x) = D^{*}\\left( \\rho'(Dx) \\right)$, where $\\rho'$ is the derivative of the potential function ($\\phi'_{\\delta}$ or $\\psi'_k$) applied element-wise to the vector of differences $Dx$. The derivatives are:\n- For Huber-TV: $\\phi'_{\\delta}(d) = \\text{clip}(d/\\delta, -1, 1)$.\n- For PM: $\\psi'_{k}(d) = \\frac{d}{1 + (d/k)^2}$.\nThe gradient descent update is $x^{k+1} = x^{k} - \\alpha_{k} \\nabla E(x^{k})$. To satisfy the requirement of a monotone line search, the step size $\\alpha_k$ is determined at each iteration using a backtracking line search. We start with a trial step size $\\alpha$ and reduce it by a factor $\\beta \\in (0,1)$ until the Armijo condition is met:\n$$\nE(x^k - \\alpha \\nabla E(x^k)) \\le E(x^k) - c \\alpha \\lVert \\nabla E(x^k) \\rVert_2^2\n$$\nfor a constant $c \\in (0,1)$, typically $c=10^{-4}$. This guarantees that the energy decreases at each step, leading to convergence to a (local) minimum.\n\n**Evaluation and Comparison**\n\nA synthetic signal $x^{\\star}$ is constructed with a ramp (small, constant gradient) and a sharp step. Gaussian noise is added to form the observation $y$. After computing the reconstructions $x^{\\mathrm{TV}}$, $x^{\\mathrm{Huber}}$, and $x^{\\mathrm{PM}}$, we evaluate them based on two quantitative metrics computed over the ramp region:\n1.  **Plateau Ratio $P(x)$**: The fraction of gradients in the ramp segment whose magnitude is close to zero, specifically smaller than a threshold $\\tau$. A high value indicates significant staircasing.\n    $P(x) = \\frac{|\\{ i \\in \\{i_{0}, \\dots, i_{1}-2\\} \\,:\\, |x_{i+1}-x_i| < \\tau \\}|}{i_1 - i_0 - 1}$\n2.  **Absolute Slope Error**: The absolute difference between the true ramp slope $m$ and the mean estimated slope $\\widehat{m}(x)$ from the reconstruction in the ramp region.\n    $\\widehat{m}(x) = \\frac{1}{i_1 - i_0 - 1}\\sum_{i=i_0}^{i_1-2} (x_{i+1}-x_i)$\n\nAn edge-preserving penalty (Huber or PM) is deemed to exhibit superior performance relative to TV if it simultaneously achieves:\n- **Reduced Staircasing**: $P(x^{\\mathrm{E}}) < P(x^{\\mathrm{TV}}) - \\varepsilon_{P}$\n- **No Oversmoothing of Small Gradients**: $\\lvert \\widehat{m}(x^{\\mathrm{E}}) - m \\rvert < \\lvert \\widehat{m}(x^{\\mathrm{TV}}) - m \\rvert - \\varepsilon_{m}$\nwhere $x^{\\mathrm{E}}$ is the reconstruction from the edge-preserving penalty, and $\\varepsilon_P, \\varepsilon_m$ are small positive margins. The following implementation carries out this comparison for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... # No scipy used\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment comparing regularization penalties.\n    \"\"\"\n\n    test_cases = [\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.08, 'lambda_reg': 0.35, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2026},\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.08, 'lambda_reg': 0.60, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2027},\n        {'N': 256, 'i0': 30, 'i1': 130, 'j': 170, 'm': 0.015, 'A': 1.0, 'sigma': 0.12, 'lambda_reg': 0.35, 'delta': 0.02, 'k': 0.06, 'tau_frac': 0.5, 'eps_p': 0.08, 'eps_m': 0.0015, 'seed': 2028},\n    ]\n\n    # --- Finite Difference Operators ---\n    def D_op(x):\n        \"\"\"Forward difference operator D.\"\"\"\n        return x[1:] - x[:-1]\n\n    def DT_op(p):\n        \"\"\"Adjoint of D (negative divergence).\"\"\"\n        N = p.shape[0] + 1\n        res = np.zeros(N, dtype=p.dtype)\n        res[1:] += p\n        res[:-1] -= p\n        return res\n\n    # --- Signal Generation ---\n    def generate_signal(N, i0, i1, j, m, A):\n        x_star = np.zeros(N)\n        indices = np.arange(N)\n        ramp_val = m * (i1 - i0)\n        \n        # Ramp region\n        mask_ramp = (indices >= i0) & (indices < i1)\n        x_star[mask_ramp] = m * (indices[mask_ramp] - i0)\n        \n        # Plateau after ramp\n        mask_plateau = (indices >= i1) & (indices < j)\n        x_star[mask_plateau] = ramp_val\n        \n        # Step region\n        mask_step = (indices >= j)\n        x_star[mask_step] = ramp_val + A\n        \n        return x_star\n\n    # --- Solvers ---\n    MAX_ITER = 1000\n\n    def solve_tv(y, lambda_reg):\n        \"\"\"Solves the TV denoising problem using dual projected gradient ascent.\"\"\"\n        N = len(y)\n        p = np.zeros(N - 1)\n        # Spectral norm of D is ~2, so ||DD*|| is ~4.\n        # Step size < 2 / (lambda^2 * ||DD*||) = 1/(2*lambda^2)\n        step_size = 0.4 / (lambda_reg**2) \n\n        for _ in range(MAX_ITER):\n            grad = -lambda_reg * D_op(y - lambda_reg * DT_op(p))\n            p_new = p - step_size * grad\n            p = np.clip(p_new, -1.0, 1.0)\n            \n        x_rec = y - lambda_reg * DT_op(p)\n        return x_rec\n\n    def solve_differentiable(y, lambda_reg, reg_type, param):\n        \"\"\"Solves denoising for differentiable regularizers (Huber, PM) using gradient descent.\"\"\"\n        x = y.copy()\n        N = len(y)\n\n        # Backtracking line search parameters\n        alpha_init = 1.0\n        beta = 0.5\n        c_armijo = 1e-4\n\n        for _ in range(MAX_ITER):\n            d = D_op(x)\n            \n            # Compute gradient of regularizer\n            if reg_type == 'huber':\n                grad_R_term = np.clip(d / param, -1.0, 1.0)\n            elif reg_type == 'pm':\n                grad_R_term = d / (1 + (d / param)**2)\n            else:\n                raise ValueError(\"Unknown regularization type\")\n\n            grad_E = (x - y) + lambda_reg * DT_op(grad_R_term)\n\n            # Compute current energy\n            if reg_type == 'huber':\n                mask = np.abs(d) <= param\n                R_val = np.sum(0.5 / param * d[mask]**2) + np.sum(np.abs(d[~mask]) - 0.5 * param)\n            elif reg_type == 'pm':\n                R_val = np.sum(0.5 * param**2 * np.log(1 + (d / param)**2))\n            \n            E_current = 0.5 * np.sum((x - y)**2) + lambda_reg * R_val\n            \n            # Backtracking line search\n            alpha = alpha_init\n            grad_E_norm_sq = np.sum(grad_E**2)\n            \n            while True:\n                x_new = x - alpha * grad_E\n                d_new = D_op(x_new)\n\n                if reg_type == 'huber':\n                    mask_new = np.abs(d_new) <= param\n                    R_val_new = np.sum(0.5 / param * d_new[mask_new]**2) + np.sum(np.abs(d_new[~mask_new]) - 0.5 * param)\n                elif reg_type == 'pm':\n                    R_val_new = np.sum(0.5 * param**2 * np.log(1 + (d_new / param)**2))\n                \n                E_new = 0.5 * np.sum((x_new - y)**2) + lambda_reg * R_val_new\n                \n                if E_new <= E_current - c_armijo * alpha * grad_E_norm_sq:\n                    break\n                alpha *= beta\n                if alpha < 1e-9: # Failsafe\n                    break\n\n            x = x - alpha * grad_E\n\n        return x\n\n    # --- Evaluation Metrics ---\n    def evaluate_reconstruction(x, i0, i1, m, tau):\n        \"\"\"Calculates plateau ratio and slope error.\"\"\"\n        ramp_grads = D_op(x[i0:i1])\n        num_grads = len(ramp_grads)\n        \n        # Plateau ratio\n        plateau_count = np.sum(np.abs(ramp_grads) < tau)\n        plateau_ratio = plateau_count / num_grads if num_grads > 0 else 0.0\n        \n        # Slope error\n        m_hat = np.mean(ramp_grads) if num_grads > 0 else 0.0\n        slope_error = np.abs(m_hat - m)\n        \n        return plateau_ratio, slope_error\n\n    # --- Main Loop ---\n    final_results = []\n    for params in test_cases:\n        # Set parameters for the current test\n        N, i0, i1, j = params['N'], params['i0'], params['i1'], params['j']\n        m, A, sigma = params['m'], params['A'], params['sigma']\n        lambda_reg, delta, k = params['lambda_reg'], params['delta'], params['k']\n        tau = params['tau_frac'] * m\n        eps_p, eps_m = params['eps_p'], params['eps_m']\n        seed = params['seed']\n        \n        # Generate signal and noise\n        np.random.seed(seed)\n        x_star = generate_signal(N, i0, i1, j, m, A)\n        noise = np.random.normal(0, sigma, N)\n        y = x_star + noise\n        \n        # Solve for all three regularizers\n        x_tv = solve_tv(y, lambda_reg)\n        x_hub = solve_differentiable(y, lambda_reg, 'huber', delta)\n        x_pm = solve_differentiable(y, lambda_reg, 'pm', k)\n        \n        # Evaluate metrics\n        p_tv, m_err_tv = evaluate_reconstruction(x_tv, i0, i1, m, tau)\n        p_hub, m_err_hub = evaluate_reconstruction(x_hub, i0, i1, m, tau)\n        p_pm, m_err_pm = evaluate_reconstruction(x_pm, i0, i1, m, tau)\n        \n        # Apply comparison criteria\n        hub_better = (p_hub < p_tv - eps_p) and (m_err_hub < m_err_tv - eps_m)\n        pm_better = (p_pm < p_tv - eps_p) and (m_err_pm < m_err_tv - eps_m)\n        \n        final_results.extend([hub_better, pm_better])\n        \n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Regularization is most powerful when it accurately reflects our prior knowledge about the solution. This practice explores the concept of *structured sparsity*, where we assume the unknown coefficients are active in predefined groups rather than individually. You will implement and compare the standard $\\ell_1$ regularizer, which promotes element-wise sparsity, with a group sparsity regularizer that encourages entire groups of coefficients to be either zero or non-zero together. By working through a simplified problem, you will clearly see how the group penalty can succeed in recovering the correct structure where the standard $\\ell_1$ norm may fail .",
            "id": "3185666",
            "problem": "Consider a linear inverse problem with a known forward operator and unknown coefficients partitioned into predefined, disjoint groups. The goal is to compare two regularization choices for estimating the unknown coefficients: the plain $\\ell_1$ norm, which promotes entrywise sparsity, and the group sparsity penalty, which promotes groupwise selection. You must write a complete program that, for the given test suite, solves both regularized inverse problems and reports which method correctly selects entire active groups.\n\nBase problem setup:\n- Forward model: given a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and data $\\mathbf{y} \\in \\mathbb{R}^{m}$, recover $\\mathbf{x} \\in \\mathbb{R}^{n}$ by minimizing an objective of the form\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\, \\mathcal{R}(\\mathbf{x}).\n$$\n- Two choices of regularizer $\\mathcal{R}$:\n  1. Plain $\\ell_1$: $\\mathcal{R}(\\mathbf{x}) = \\|\\mathbf{x}\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$.\n  2. Group sparsity (group lasso): suppose indices are partitioned into $G$ disjoint groups $\\{g_{1},\\dots,g_{G}\\}$; define $\\mathcal{R}(\\mathbf{x}) = \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$, where $\\mathbf{x}_{g}$ is the subvector of $\\mathbf{x}$ restricted to group $g$.\n\nFundamental base and assumptions you must use:\n- Use the least-squares data fidelity $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}$ and the two convex regularizers defined above.\n- In all test cases below, take $\\mathbf{A} = \\mathbf{I}_{n}$, the identity matrix of size $n \\times n$, so that the data are $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$ with no noise. This is a standard and well-tested design that isolates the effect of the regularizer on selection without conflating forward model conditioning.\n- Groups are disjoint and specified explicitly. The success criterion concerns recovery of entire active groups.\n\nDefinitions for evaluation:\n- Let the ground-truth vector be $\\mathbf{x}_{\\text{true}} \\in \\mathbb{R}^{n}$, partitioned into groups $\\{g_{1},\\dots,g_{G}\\}$. A group $g$ is called active if $\\|\\mathbf{x}_{\\text{true},g}\\|_{2} > 0$, and inactive otherwise.\n- Given an estimate $\\widehat{\\mathbf{x}}$, declare that it selects entire groups correctly if and only if:\n  - For every active group $g$, all entries of $\\widehat{\\mathbf{x}}_{g}$ are nonzero.\n  - For every inactive group $g$, all entries of $\\widehat{\\mathbf{x}}_{g}$ are exactly zero.\n- In numerical computations, treat any entry with absolute value less than a tolerance $\\varepsilon = 10^{-8}$ as zero, and any entry with absolute value greater than or equal to $\\varepsilon$ as nonzero.\n\nYour program must:\n- For each test case, solve both problems\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}$,\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$,\n  using any correct numerical method of your choice, to within a numerical tolerance that ensures stable identification according to the above rule.\n- Decide, for each method and each test case, whether entire active groups are correctly selected.\n- Map the outcome of each test case to an integer code $c \\in \\{0,1,2,3\\}$ as follows:\n  - $c = 0$: neither method selects entire active groups correctly,\n  - $c = 1$: only the plain $\\ell_1$ method does,\n  - $c = 2$: only the group sparsity method does,\n  - $c = 3$: both methods do.\n\nTest suite:\n- All cases use $n = 6$, groups $g_{1} = [0,1,2]$, $g_{2} = [3,4,5]$, $\\mathbf{A} = \\mathbf{I}_{6}$, and $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$.\n- Case $1$ (happy path where $\\ell_{1}$ fails but group sparsity succeeds): $\\mathbf{x}_{\\text{true}} = [0.6, 0.6, 0.6, 0, 0, 0]$, $\\lambda = 0.8$.\n- Case $2$ (boundary with very strong regularization): $\\mathbf{x}_{\\text{true}} = [1, 1, 1, 0, 0, 0]$, $\\lambda = 5$.\n- Case $3$ (correlated magnitudes within a group where $\\ell_{1}$ keeps only a subset): $\\mathbf{x}_{\\text{true}} = [1.2, 0.3, 0.2, 0, 0, 0]$, $\\lambda = 0.5$.\n- Case $4$ (weak regularization where both succeed): $\\mathbf{x}_{\\text{true}} = [2, 2, 2, 0, 0, 0]$, $\\lambda = 0.1$.\n\nNumerical and implementation requirements:\n- Angles are not involved; no physical units are involved.\n- Use the numerical tolerance $\\varepsilon = 10^{-8}$ for zero/nonzero decisions.\n- Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, ordered as $[c_{1}, c_{2}, c_{3}, c_{4}]$ for the four test cases.\n\nYour task:\n- Implement the solver, run it on the four specified cases, apply the selection rule, and print the single-line result in the exact format specified. No user input is required and no external data files are allowed.",
            "solution": "The problem requires a comparison of two regularization methods, the plain $\\ell_1$ norm and the group sparsity norm, for a linear inverse problem. The goal is to determine which method correctly identifies predefined groups of active coefficients. The problem is simplified by setting the forward operator $\\mathbf{A}$ to the identity matrix $\\mathbf{I}$ and providing noise-free data $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$. This setup isolates the behavior of the regularizers' associated proximal operators.\n\nThe two optimization problems to be solved for each test case are:\n1.  **Plain $\\ell_1$ Regularization (LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}\n    $$\n2.  **Group Sparsity Regularization (Group LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g \\in G} \\|\\mathbf{x}_{g}\\|_{2}\n    $$\nwhere $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$, $\\lambda$ is the regularization parameter, and $G$ is the set of predefined, disjoint groups of indices. For all test cases, the vector dimension is $n=6$, and the groups are $g_1$ (indices $\\{0, 1, 2\\}$) and $g_2$ (indices $\\{3, 4, 5\\}$).\n\nThe solutions to these optimization problems are found by applying the proximal operator of the respective regularization term to the data $\\mathbf{y}$.\n\n**Solution for Plain $\\ell_1$ Regularization**\n\nThe objective function is separable with respect to the individual components of $\\mathbf{x}$. The problem can be written as:\n$$\n\\sum_{i=1}^{n} \\min_{x_i \\in \\mathbb{R}} \\left( \\frac{1}{2}(x_i - y_i)^2 + \\lambda |x_i| \\right)\n$$\nThe solution for each component $\\widehat{x}_i$ is given by the soft-thresholding operator:\n$$\n\\widehat{x}_i = \\text{prox}_{\\lambda|\\cdot|}(y_i) = \\text{sign}(y_i) \\max(|y_i| - \\lambda, 0)\n$$\nThis operator thresholds each component individually. A component $y_i$ with magnitude less than $\\lambda$ is set to zero in the solution $\\widehat{x}_i$. This promotes entry-wise sparsity.\n\n**Solution for Group Sparsity Regularization**\n\nThe objective function is separable with respect to the coefficient groups. The problem can be written as:\n$$\n\\sum_{g \\in G} \\min_{\\mathbf{x}_g \\in \\mathbb{R}^{|g|}} \\left( \\frac{1}{2}\\|\\mathbf{x}_g - \\mathbf{y}_g\\|_2^2 + \\lambda \\|\\mathbf{x}_g\\|_2 \\right)\n$$\nThe solution for each subvector $\\widehat{\\mathbf{x}}_g$ is given by the group (or block) soft-thresholding operator:\n$$\n\\widehat{\\mathbf{x}}_g = \\text{prox}_{\\lambda\\|\\cdot\\|_2}(\\mathbf{y}_g) = \\left( 1 - \\frac{\\lambda}{\\|\\mathbf{y}_g\\|_2} \\right)_+ \\mathbf{y}_g = \\frac{\\mathbf{y}_g}{\\|\\mathbf{y}_g\\|_2} \\max(\\|\\mathbf{y}_g\\|_2 - \\lambda, 0)\n$$\nwhere $(z)_+ = \\max(z, 0)$. This operator acts on entire groups. If the Euclidean norm of a data subvector, $\\|\\mathbf{y}_g\\|_2$, is less than $\\lambda$, the entire corresponding solution subvector $\\widehat{\\mathbf{x}}_g$ is set to the zero vector. Otherwise, the subvector is scaled but its direction is preserved. This promotes group-wise sparsity, meaning either all coefficients in a group are zero, or all can be non-zero.\n\n**Evaluation and Implementation**\n\nFor each test case, we compute the estimates $\\widehat{\\mathbf{x}}_{\\ell_1}$ and $\\widehat{\\mathbf{x}}_{\\text{group}}$. We then evaluate their success based on the provided criterion. A method is successful if, for an estimate $\\widehat{\\mathbf{x}}$:\n1.  For every active group $g$ (where $\\|\\mathbf{x}_{\\text{true},g}\\|_2 > 0$), all entries of $\\widehat{\\mathbf{x}}_g$ are non-zero.\n2.  For every inactive group $g$ (where $\\|\\mathbf{x}_{\\text{true},g}\\|_2 = 0$), all entries of $\\widehat{\\mathbf{x}}_g$ are zero.\n\nNumerically, \"zero\" is defined as having an absolute value less than $\\varepsilon = 10^{-8}$.\n\nFor all test cases, $\\mathbf{x}_{\\text{true}, g_1}$ has at least one non-zero component, making $g_1$ the active group. $\\mathbf{x}_{\\text{true}, g_2}$ is the zero vector, making $g_2$ the inactive group.\n- The $\\ell_1$ method will fail the success criterion if $\\lambda$ is large enough to zero-out some, but not all, components of an active group (e.g., if components of $\\mathbf{y}_{g_1}$ have different magnitudes).\n- The group sparsity method is designed to avoid this: it treats each group as a single unit, either zeroing it out completely or keeping it (scaled). Thus, it will either set all of $\\widehat{\\mathbf{x}}_{g_1}$ to zero or keep all its components non-zero (assuming $\\mathbf{y}_{g_1}$ has no zero entries, which is true in the relevant test cases).\n\nThe program will implement these two proximal operators and the evaluation logic, compute the results for the four test cases, and map them to the specified integer codes. The final output will be a list of these codes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regularization comparison problem for a given test suite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x_true_list, lambda_val)\n    test_cases = [\n        # Case 1\n        (np.array([0.6, 0.6, 0.6, 0.0, 0.0, 0.0]), 0.8),\n        # Case 2\n        (np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0]), 5.0),\n        # Case 3\n        (np.array([1.2, 0.3, 0.2, 0.0, 0.0, 0.0]), 0.5),\n        # Case 4\n        (np.array([2.0, 2.0, 2.0, 0.0, 0.0, 0.0]), 0.1),\n    ]\n\n    # Global problem parameters\n    groups = [[0, 1, 2], [3, 4, 5]]\n    epsilon = 1e-8\n\n    def solve_l1(y, lam):\n        \"\"\"\n        Solves the l1-regularized problem using soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * ||x||_1\n        \"\"\"\n        return np.sign(y) * np.maximum(np.abs(y) - lam, 0)\n\n    def solve_group_sparsity(y, lam, groups_list):\n        \"\"\"\n        Solves the group sparsity regularized problem using block soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * sum_g ||x_g||_2\n        \"\"\"\n        x_hat = np.zeros_like(y)\n        for g_indices in groups_list:\n            y_g = y[g_indices]\n            norm_y_g = np.linalg.norm(y_g)\n            \n            if norm_y_g > lam:\n                scaler = (1 - lam / norm_y_g)\n                x_hat[g_indices] = scaler * y_g\n            else:\n                x_hat[g_indices] = 0.0\n        return x_hat\n\n    def check_success(x_hat, x_true, groups_list, tol):\n        \"\"\"\n        Checks if an estimate x_hat correctly selects entire active groups.\n        Success criterion:\n        1. For active groups, all entries of x_hat_g must be nonzero.\n        2. For inactive groups, all entries of x_hat_g must be zero.\n        \"\"\"\n        for g_indices in groups_list:\n            x_true_g = x_true[g_indices]\n            x_hat_g = x_hat[g_indices]\n            \n            # Determine if the ground-truth group is active\n            is_active = np.linalg.norm(x_true_g) > 0\n            \n            if is_active:\n                # All entries must be nonzero\n                if not np.all(np.abs(x_hat_g) >= tol):\n                    return False\n            else: # inactive\n                # All entries must be zero\n                if not np.all(np.abs(x_hat_g) < tol):\n                    return False\n        return True\n\n    results = []\n    for x_true, lam in test_cases:\n        y = x_true  # Data is noise-free\n\n        # Solve and evaluate for plain l1\n        x_hat_l1 = solve_l1(y, lam)\n        l1_succeeds = check_success(x_hat_l1, x_true, groups, epsilon)\n\n        # Solve and evaluate for group sparsity\n        x_hat_group = solve_group_sparsity(y, lam, groups)\n        group_succeeds = check_success(x_hat_group, x_true, groups, epsilon)\n        \n        # Determine the integer code based on success\n        code = 0\n        if l1_succeeds and group_succeeds:\n            code = 3\n        elif group_succeeds:\n            code = 2\n        elif l1_succeeds:\n            code = 1\n        \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}