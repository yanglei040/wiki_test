{
    "hands_on_practices": [
        {
            "introduction": "Understanding how uncertainty in model inputs affects the outputs is a fundamental task in uncertainty quantification, known as forward propagation. This first exercise provides a concrete application of this principle using a well-known physical law. You will explore how small measurement uncertainties in temperature propagate through the nonlinear Stefan-Boltzmann law to affect the calculated radiative heat rate, using analytical techniques based on series expansions to approximate the resulting uncertainty in the output . This practice builds foundational skills in error propagation analysis.",
            "id": "2536849",
            "problem": "A diffuse-gray isothermal patch of surface with emissivity $\\epsilon$, area $A$, and absolute temperature $T$ radiates to deep space. The net radiative heat rate is governed by the Stefan–Boltzmann law $q=\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\, T^{4}$, where $\\sigma_{\\mathrm{SB}}$ is the Stefan–Boltzmann constant. Suppose that $\\epsilon$, $A$, and $\\sigma_{\\mathrm{SB}}$ are known deterministic parameters, but the temperature $T$ is uncertain due to small sensor noise and process variability. Model $T$ as a Gaussian random variable with mean $\\mu_{T}>0$ and variance $\\sigma_{T}^{2}$, and assume a small coefficient of variation $c=\\sigma_{T}/\\mu_{T}$ with $c \\ll 1$.\n\nStarting from the Stefan–Boltzmann law and the definitions of expectation and variance, and using only fundamental series expansions justified by the smallness of $c$, derive the first two moments of the nondimensional radiative heat rate\n$$\nr \\equiv \\frac{q}{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\,\\mu_{T}^{4}}.\n$$\nWork consistently to leading nontrivial order in $c$ and include all terms up to and including order $c^{2}$ in the mean and the variance. Express your final answers as closed-form analytic expressions in terms of $c$ only. Report the two results as a single row matrix $\\big(\\,\\mathbb{E}[r]\\;\\;\\mathrm{Var}(r)\\,\\big)$. Since $r$ is dimensionless, do not include units in your final answer.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique solution. It is therefore deemed valid. We proceed with the derivation.\n\nThe objective is to determine the first two moments, the expectation $\\mathbb{E}[r]$ and the variance $\\mathrm{Var}(r)$, of the nondimensional radiative heat rate $r$. The problem states that the net radiative heat rate is given by the Stefan–Boltzmann law, $q=\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\, T^{4}$. The nondimensional heat rate $r$ is defined as\n$$\nr \\equiv \\frac{q}{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\,\\mu_{T}^{4}}\n$$\nSubstituting the expression for $q$ into the definition of $r$, the deterministic parameters $\\epsilon$, $\\sigma_{\\mathrm{SB}}$, and $A$ cancel:\n$$\nr = \\frac{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\, T^{4}}{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\,\\mu_{T}^{4}} = \\left(\\frac{T}{\\mu_{T}}\\right)^{4}\n$$\nThe absolute temperature $T$ is modeled as a Gaussian random variable with mean $\\mathbb{E}[T] = \\mu_{T}$ and variance $\\mathrm{Var}(T) = \\sigma_{T}^{2}$. It is convenient to define a standard normal random variable $X$ as\n$$\nX = \\frac{T - \\mu_{T}}{\\sigma_{T}}\n$$\nsuch that $X \\sim \\mathcal{N}(0,1)$. From this definition, we can express $T$ in terms of $X$:\n$$\nT = \\mu_{T} + \\sigma_{T} X\n$$\nSubstituting this into the expression for $r$ yields:\n$$\nr = \\left(\\frac{\\mu_{T} + \\sigma_{T} X}{\\mu_{T}}\\right)^{4} = \\left(1 + \\frac{\\sigma_{T}}{\\mu_{T}} X\\right)^{4}\n$$\nUsing the definition of the coefficient of variation, $c = \\sigma_{T}/\\mu_{T}$, the expression for $r$ becomes a polynomial in $X$ with coefficients that are powers of $c$:\n$$\nr = (1 + cX)^{4}\n$$\nSince the exponent is a positive integer, we can use the binomial theorem to obtain an exact, finite expansion:\n$$\nr = \\binom{4}{0}(cX)^{0} + \\binom{4}{1}(cX)^{1} + \\binom{4}{2}(cX)^{2} + \\binom{4}{3}(cX)^{3} + \\binom{4}{4}(cX)^{4}\n$$\n$$\nr = 1 + 4cX + 6c^{2}X^{2} + 4c^{3}X^{3} + c^{4}X^{4}\n$$\nTo find the expectation $\\mathbb{E}[r]$, we apply the expectation operator term by term, leveraging its linearity:\n$$\n\\mathbb{E}[r] = \\mathbb{E}[1 + 4cX + 6c^{2}X^{2} + 4c^{3}X^{3} + c^{4}X^{4}] = 1 + 4c\\mathbb{E}[X] + 6c^{2}\\mathbb{E}[X^{2}] + 4c^{3}\\mathbb{E}[X^{3}] + c^{4}\\mathbb{E}[X^{4}]\n$$\nThe moments of a standard normal random variable $X \\sim \\mathcal{N}(0,1)$ are well known: $\\mathbb{E}[X] = 0$, $\\mathbb{E}[X^{2}] = 1$, $\\mathbb{E}[X^{3}] = 0$, and $\\mathbb{E}[X^{4}] = 3$. Substituting these values gives the exact expression for the mean of $r$:\n$$\n\\mathbb{E}[r] = 1 + 4c(0) + 6c^{2}(1) + 4c^{3}(0) + c^{4}(3) = 1 + 6c^{2} + 3c^{4}\n$$\nThe problem requires the result up to and including order $c^{2}$. Truncating the exact expression, we obtain:\n$$\n\\mathbb{E}[r] \\approx 1 + 6c^{2}\n$$\nNext, we calculate the variance $\\mathrm{Var}(r)$. We use the fundamental relation $\\mathrm{Var}(r) = \\mathbb{E}[r^{2}] - (\\mathbb{E}[r])^{2}$. We must compute each term on the right-hand side consistently to the required order of accuracy. The leading term in the variance will be of order $c^{2}$, so we must compute $\\mathbb{E}[r^{2}]$ and $(\\mathbb{E}[r])^{2}$ including all terms up to $O(c^{2})$.\n\nFirst, we find an expression for $r^{2}$:\n$$\nr^{2} = ((1 + cX)^{4})^{2} = (1 + cX)^{8}\n$$\nExpanding this using the binomial theorem:\n$$\nr^{2} = 1 + 8cX + \\binom{8}{2}(cX)^{2} + O(c^{3}) = 1 + 8cX + 28c^{2}X^{2} + O(c^{3})\n$$\nTaking the expectation and keeping terms up to order $c^{2}$:\n$$\n\\mathbb{E}[r^{2}] \\approx \\mathbb{E}[1 + 8cX + 28c^{2}X^{2}] = 1 + 8c\\mathbb{E}[X] + 28c^{2}\\mathbb{E}[X^{2}] = 1 + 8c(0) + 28c^{2}(1) = 1 + 28c^{2}\n$$\nNext, we compute $(\\mathbb{E}[r])^{2}$ up to order $c^{2}$. Using our derived expression for $\\mathbb{E}[r]$:\n$$\n(\\mathbb{E}[r])^{2} = (1 + 6c^{2} + 3c^{4})^{2}\n$$\nExpanding and retaining terms up to order $c^{2}$:\n$$\n(\\mathbb{E}[r])^{2} \\approx (1 + 6c^{2})^{2} = 1^{2} + 2(1)(6c^{2}) + (6c^{2})^{2} = 1 + 12c^{2} + 36c^{4}\n$$\nTruncating to order $c^{2}$ gives:\n$$\n(\\mathbb{E}[r])^{2} \\approx 1 + 12c^{2}\n$$\nFinally, we compute the variance:\n$$\n\\mathrm{Var}(r) = \\mathbb{E}[r^{2}] - (\\mathbb{E}[r])^{2} \\approx (1 + 28c^{2}) - (1 + 12c^{2}) = 16c^{2}\n$$\nThis result is the leading nontrivial term in the expansion for the variance and is accurate to order $c^{2}$.\n\nThe first two moments of the nondimensional heat rate $r$, correct to order $c^{2}$, are:\n$$\n\\mathbb{E}[r] = 1 + 6c^{2}\n$$\n$$\n\\mathrm{Var}(r) = 16c^{2}\n$$\nThese are the required closed-form analytic expressions.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1+6c^{2} & 16c^{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While forward propagation deals with known input uncertainties, many real-world scenarios involve the opposite: using experimental data to quantify uncertainty in the model parameters themselves. This is known as an inverse problem or model calibration. This exercise guides you through a Bayesian calibration of a material's Young's modulus using noisy stress-strain data . You will learn how to combine prior knowledge with experimental evidence to derive a posterior distribution for the model parameter, providing a rigorous way to characterize what we have learned from data.",
            "id": "2707423",
            "problem": "A slender, homogeneous, linearly elastic bar is subjected to small uniaxial strains. The constitutive relation at small strain is the linear Hooke law, which may be written as the measurement model\n$$\n\\sigma_i \\;=\\; E\\,\\varepsilon_i \\;+\\; \\eta_i,\\quad i=1,\\dots,n,\n$$\nwhere $E$ is the (unknown) Young’s modulus, $\\varepsilon_i$ are prescribed strain levels, $\\sigma_i$ are measured stresses, and $\\eta_i$ are independent and identically distributed (i.i.d.) Gaussian measurement errors with zero mean and known variance $\\sigma^2$. Assume a Bayesian calibration with a conjugate prior $E \\sim \\mathcal{N}(m_0, s_0^2)$, independent of the errors.\n\nYou are given $n=4$ measurements with strains and stresses (in gigapascals, $\\mathrm{GPa}$) as follows:\n$$\n\\big(\\varepsilon_1,\\sigma_1\\big)=(0.0010,\\,0.212),\\quad\n\\big(\\varepsilon_2,\\sigma_2\\big)=(0.0015,\\,0.310),\\\\\n\\big(\\varepsilon_3,\\sigma_3\\big)=(0.0020,\\,0.425),\\quad\n\\big(\\varepsilon_4,\\sigma_4\\big)=(0.0025,\\,0.520).\n$$\nThe measurement noise variance is known to be $\\sigma^2 = (0.01)^2$ in $\\mathrm{GPa}^2$, and the prior is $m_0 = 200$ in $\\mathrm{GPa}$ and $s_0^2 = 30^2$ in $\\mathrm{GPa}^2$.\n\nTasks:\n- Starting from Bayes’ theorem and the definitions of the Gaussian likelihood and prior, derive the posterior distribution of $E$ given the data $\\{(\\varepsilon_i,\\sigma_i)\\}_{i=1}^4$. Your derivation must not invoke any unproven shortcut formulas.\n- Using the law of total probability and conditioning on $E$, derive the posterior predictive distribution for a future stress measurement $\\sigma_\\star$ at a new strain level $\\varepsilon_\\star = 0.0030$.\n- Finally, evaluate the posterior predictive mean stress at $\\varepsilon_\\star=0.0030$ numerically, using the given data and hyperparameters. Round your answer to four significant figures and express it in $\\mathrm{GPa}$. Your final response must be a single real number.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in Bayesian calibration for a linear model in solid mechanics. All necessary information is provided and is internally consistent. We proceed with the solution.\n\nLet the set of $n$ measurements be denoted by $\\mathcal{D} = \\{(\\varepsilon_i, \\sigma_i)\\}_{i=1}^n$. We are tasked with finding the posterior distribution for Young's modulus $E$ and the posterior predictive distribution for a new stress measurement $\\sigma_\\star$.\n\nThe first task is to derive the posterior distribution of $E$ given the data, $p(E|\\mathcal{D})$. According to Bayes' theorem, the posterior probability density function is proportional to the product of the likelihood and the prior:\n$$\np(E|\\mathcal{D}) \\propto p(\\mathcal{D}|E) p(E)\n$$\nThe measurement model is given by $\\sigma_i = E\\varepsilon_i + \\eta_i$, where the errors $\\eta_i$ are independent and identically distributed (i.i.d.) as $\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that the conditional distribution of a single stress measurement $\\sigma_i$ given $E$ and $\\varepsilon_i$ is Gaussian:\n$$\n\\sigma_i | E \\sim \\mathcal{N}(E\\varepsilon_i, \\sigma^2)\n$$\nThe probability density function for $\\sigma_i$ is therefore:\n$$\np(\\sigma_i|E) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(\\sigma_i - E\\varepsilon_i)^2}{2\\sigma^2} \\right)\n$$\nSince the measurements are independent, the likelihood of the entire dataset $\\mathcal{D}$ (or simply the vector of stresses $\\pmb{\\sigma} = (\\sigma_1, \\dots, \\sigma_n)^T$) is the product of the individual densities:\n$$\np(\\mathcal{D}|E) = \\prod_{i=1}^n p(\\sigma_i|E) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\sigma_i - E\\varepsilon_i)^2 \\right)\n$$\nThe prior distribution for $E$ is given as a Gaussian: $E \\sim \\mathcal{N}(m_0, s_0^2)$. Its probability density function is:\n$$\np(E) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left( -\\frac{(E - m_0)^2}{2s_0^2} \\right)\n$$\nNow, we construct the posterior. We can ignore any constant factors that do not depend on $E$.\n$$\np(E|\\mathcal{D}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\sigma_i - E\\varepsilon_i)^2 \\right) \\exp\\left( -\\frac{(E - m_0)^2}{2s_0^2} \\right)\n$$\nThe posterior is proportional to the exponential of the sum of the arguments:\n$$\np(E|\\mathcal{D}) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^n (\\sigma_i - E\\varepsilon_i)^2 + \\frac{1}{s_0^2} (E - m_0)^2 \\right] \\right)\n$$\nTo identify the form of this distribution, we expand the terms in the exponent as a quadratic function of $E$:\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^n (\\sigma_i^2 - 2E\\varepsilon_i\\sigma_i + E^2\\varepsilon_i^2) + \\frac{1}{s_0^2} (E^2 - 2Em_0 + m_0^2)\n$$\nWe collect terms involving $E^2$ and $E$:\n$$\nE^2 \\left( \\frac{\\sum_{i=1}^n \\varepsilon_i^2}{\\sigma^2} + \\frac{1}{s_0^2} \\right) - 2E \\left( \\frac{\\sum_{i=1}^n \\varepsilon_i\\sigma_i}{\\sigma^2} + \\frac{m_0}{s_0^2} \\right) + \\text{const.}\n$$\nThis quadratic form in $E$ indicates that the posterior distribution is also Gaussian, which we can write as $E|\\mathcal{D} \\sim \\mathcal{N}(m_n, s_n^2)$. The density of such a distribution is proportional to $\\exp\\left(-\\frac{(E-m_n)^2}{2s_n^2}\\right)$, and the argument of its exponent is:\n$$\n\\frac{(E-m_n)^2}{2s_n^2} = \\frac{1}{2s_n^2} (E^2 - 2Em_n + m_n^2) = E^2\\left(\\frac{1}{2s_n^2}\\right) - E\\left(\\frac{m_n}{s_n^2}\\right) + \\text{const.}\n$$\nBy comparing the coefficients of $E^2$ and $E$ with the expression derived from the posterior, we can identify the posterior precision $1/s_n^2$ and the term $m_n/s_n^2$:\n$$\n\\frac{1}{s_n^2} = \\frac{1}{s_0^2} + \\frac{\\sum_{i=1}^n \\varepsilon_i^2}{\\sigma^2}\n$$\n$$\n\\frac{m_n}{s_n^2} = \\frac{m_0}{s_0^2} + \\frac{\\sum_{i=1}^n \\varepsilon_i\\sigma_i}{\\sigma^2}\n$$\nFrom these, the posterior variance $s_n^2$ and mean $m_n$ are:\n$$\ns_n^2 = \\left( \\frac{1}{s_0^2} + \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\varepsilon_i^2 \\right)^{-1}\n$$\n$$\nm_n = s_n^2 \\left( \\frac{m_0}{s_0^2} + \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\varepsilon_i\\sigma_i \\right)\n$$\nThe posterior distribution is thus $p(E|\\mathcal{D}) = \\mathcal{N}(m_n, s_n^2)$. This completes the first task.\n\nThe second task is to derive the posterior predictive distribution for a new measurement $\\sigma_\\star$ at a new strain $\\varepsilon_\\star$. We use the law of total probability, marginalizing over the unknown parameter $E$:\n$$\np(\\sigma_\\star|\\mathcal{D}) = \\int p(\\sigma_\\star, E | \\mathcal{D}) dE = \\int p(\\sigma_\\star | E, \\mathcal{D}) p(E|\\mathcal{D}) dE\n$$\nGiven $E$, the new measurement $\\sigma_\\star$ is independent of the past data $\\mathcal{D}$, so $p(\\sigma_\\star | E, \\mathcal{D}) = p(\\sigma_\\star | E)$. The model for the new measurement is $\\sigma_\\star = E \\varepsilon_\\star + \\eta_\\star$, where $\\eta_\\star \\sim \\mathcal{N}(0, \\sigma^2)$. Therefore, the corresponding distribution is:\n$$\np(\\sigma_\\star | E) = \\mathcal{N}(\\sigma_\\star; E\\varepsilon_\\star, \\sigma^2)\n$$\nThe posterior predictive distribution is the convolution of two Gaussian distributions:\n$$\np(\\sigma_\\star|\\mathcal{D}) = \\int \\mathcal{N}(\\sigma_\\star; E\\varepsilon_\\star, \\sigma^2) \\mathcal{N}(E; m_n, s_n^2) dE\n$$\nThis integral describes the distribution of the random variable $\\sigma_\\star$, which can be expressed as a sum of two independent Gaussian random variables (conditioned on the data $\\mathcal{D}$): the uncertainty in the measurement, $\\eta_\\star \\sim \\mathcal{N}(0, \\sigma^2)$, and the uncertainty in the mean stress, $E\\varepsilon_\\star$. The variable $E\\varepsilon_\\star$ is a scaled Gaussian variable, since $E \\sim \\mathcal{N}(m_n, s_n^2)$, thus $E\\varepsilon_\\star \\sim \\mathcal{N}(m_n\\varepsilon_\\star, s_n^2\\varepsilon_\\star^2)$.\nThe sum of two independent Gaussian random variables is also a Gaussian random variable. Its mean is the sum of the means, and its variance is the sum of the variances.\nThe mean of the posterior predictive distribution is:\n$$\n\\mathbb{E}[\\sigma_\\star|\\mathcal{D}] = \\mathbb{E}[E\\varepsilon_\\star + \\eta_\\star|\\mathcal{D}] = \\mathbb{E}[E\\varepsilon_\\star|\\mathcal{D}] + \\mathbb{E}[\\eta_\\star|\\mathcal{D}] = m_n \\varepsilon_\\star + 0 = m_n \\varepsilon_\\star\n$$\nThe variance of the posterior predictive distribution is:\n$$\n\\text{Var}(\\sigma_\\star|\\mathcal{D}) = \\text{Var}(E\\varepsilon_\\star + \\eta_\\star|\\mathcal{D}) = \\text{Var}(E\\varepsilon_\\star|\\mathcal{D}) + \\text{Var}(\\eta_\\star|\\mathcal{D}) = s_n^2 \\varepsilon_\\star^2 + \\sigma^2\n$$\nSo, the posterior predictive distribution is $\\sigma_\\star|\\mathcal{D} \\sim \\mathcal{N}(m_n \\varepsilon_\\star, s_n^2 \\varepsilon_\\star^2 + \\sigma^2)$. This completes the second task.\n\nThe third task is to evaluate the posterior predictive mean stress $\\mathbb{E}[\\sigma_\\star|\\mathcal{D}] = m_n \\varepsilon_\\star$ using the provided data.\nGiven data: $n=4$, $(\\varepsilon_1,\\sigma_1)=(0.0010, 0.212)$, $(\\varepsilon_2,\\sigma_2)=(0.0015, 0.310)$, $(\\varepsilon_3,\\sigma_3)=(0.0020, 0.425)$, $(\\varepsilon_4,\\sigma_4)=(0.0025, 0.520)$.\nHyperparameters: $m_0=200$, $s_0^2=30^2=900$, $\\sigma^2=(0.01)^2=0.0001$.\nNew strain: $\\varepsilon_\\star = 0.0030$.\n\nFirst, we compute the necessary sums from the data:\n$$\n\\sum_{i=1}^4 \\varepsilon_i^2 = (0.0010)^2 + (0.0015)^2 + (0.0020)^2 + (0.0025)^2 = 1.35 \\times 10^{-5}\n$$\n$$\n\\sum_{i=1}^4 \\varepsilon_i\\sigma_i = (0.0010)(0.212) + (0.0015)(0.310) + (0.0020)(0.425) + (0.0025)(0.520) = 0.002827\n$$\nNow, we calculate the posterior parameters. First, the posterior variance $s_n^2$:\n$$\n\\frac{1}{s_n^2} = \\frac{1}{s_0^2} + \\frac{\\sum \\varepsilon_i^2}{\\sigma^2} = \\frac{1}{900} + \\frac{1.35 \\times 10^{-5}}{0.0001} = \\frac{1}{900} + 0.135 = \\frac{1}{900} + \\frac{135}{1000} = \\frac{2 + 243}{1800} = \\frac{245}{1800} = \\frac{49}{360}\n$$\n$$\ns_n^2 = \\frac{360}{49}\n$$\nNext, we calculate the posterior mean $m_n$:\n$$\n\\frac{m_n}{s_n^2} = \\frac{m_0}{s_0^2} + \\frac{\\sum \\varepsilon_i\\sigma_i}{\\sigma^2} = \\frac{200}{900} + \\frac{0.002827}{0.0001} = \\frac{2}{9} + 28.27 = \\frac{2}{9} + \\frac{2827}{100} = \\frac{200 + 25443}{900} = \\frac{25643}{900}\n$$\n$$\nm_n = s_n^2 \\left( \\frac{25643}{900} \\right) = \\frac{360}{49} \\cdot \\frac{25643}{900} = \\frac{360}{900} \\cdot \\frac{25643}{49} = \\frac{2}{5} \\cdot \\frac{25643}{49} = \\frac{51286}{245}\n$$\nNumerically, $m_n \\approx 209.3306 \\text{ GPa}$. Finally, we compute the posterior predictive mean stress at $\\varepsilon_\\star = 0.0030$:\n$$\n\\mathbb{E}[\\sigma_\\star|\\mathcal{D}] = m_n \\varepsilon_\\star = \\frac{51286}{245} \\times 0.0030 \\approx 209.330612 \\times 0.0030 \\approx 0.6279918\n$$\nRounding to four significant figures, the result is $0.6280$. The units are GPa.",
            "answer": "$$ \\boxed{0.6280} $$"
        },
        {
            "introduction": "Uncertainty can be represented in various ways, from strict deterministic bounds to full probability distributions, and the choice of method can significantly impact the results. This practice offers a comparative look at different UQ paradigms by analyzing a simple yet illustrative function . You will see how naive interval arithmetic can lead to overly conservative bounds due to the \"dependency problem\" and how more sophisticated techniques like Affine Arithmetic can yield much tighter, more accurate results, highlighting the importance of correctly handling correlations in your calculations.",
            "id": "3201124",
            "problem": "You are analyzing uncertainty in a scalar computational model with a single uncertain input $x$. Suppose all you know deterministically is that $x \\in [0,1]$. Consider the function $f(x) = x(1 - x)$. You are asked to compare deterministic interval bounds with probabilistic summaries and to diagnose overestimation in interval arithmetic, then apply Affine Arithmetic (AA) to tighten bounds.\n\nUse only the following fundamental bases:\n- Interval arithmetic: For intervals $X = [a,b]$ and $Y = [c,d]$, the set-based definitions are $X + Y = \\{ x+y \\mid x \\in X, y \\in Y\\}$, $X - Y = \\{ x-y \\mid x \\in X, y \\in Y\\}$, $X \\cdot Y = \\{ xy \\mid x \\in X, y \\in Y\\}$. In particular, $1 - X = \\{ 1 - x \\mid x \\in X\\}$ and $X \\cdot Y$ is the interval whose endpoints are the minimum and maximum among the products $ac, ad, bc, bd$.\n- Affine Arithmetic (AA): Represent an uncertain quantity by an affine form $x = x_{0} + r \\,\\varepsilon_{1}$ with $\\varepsilon_{1} \\in [-1,1]$. Linear operations propagate exactly on coefficients. Nonlinear terms are bounded by introducing a new noise symbol $\\varepsilon_{k}$ to enclose the range of the remainder term.\n- Basic calculus and probability: For a random variable $X$ with probability density, the expected value $\\mathbb{E}[g(X)]$ and variance $\\operatorname{Var}(g(X))$ are defined by integrals over its support, and for $X \\sim \\operatorname{Uniform}(0,1)$, $\\mathbb{E}[X^{n}] = \\frac{1}{n+1}$ for integer $n \\ge 0$.\n\nAnswer the following multiple-choice question. Select all options that are correct.\n\nA. Using classical interval arithmetic on $f([0,1]) = [0,1] \\cdot (1 - [0,1])$ returns $[0,1]$ and overestimates the true range because it ignores the dependency between repeated occurrences of $x$.\n\nB. Using Affine Arithmetic with $x = \\frac{1}{2} + \\frac{1}{2}\\varepsilon_{1}$ and $1 - x = \\frac{1}{2} - \\frac{1}{2}\\varepsilon_{1}$, the product $x(1 - x)$ yields a tighter bound of $[0, \\frac{1}{4}]$.\n\nC. If $X \\sim \\operatorname{Uniform}(0,1)$, then $\\mathbb{E}[f(X)] = \\frac{1}{6}$ and $\\operatorname{Var}(f(X)) = \\frac{1}{180}$, which summarize probabilistic uncertainty and are not guaranteed enclosures of all possible values.\n\nD. For $X \\sim \\operatorname{Uniform}(0,1)$, the interval $\\left[\\mathbb{E}[f(X)] - 2\\sqrt{\\operatorname{Var}(f(X))},\\ \\mathbb{E}[f(X)] + 2\\sqrt{\\operatorname{Var}(f(X))}\\right]$ is a guaranteed enclosure of $f(X)$ with probability $1$.\n\nE. Classical interval arithmetic returns the exact image $f([0,1]) = [0, \\frac{1}{4}]$ because the minimum and maximum of $x(1-x)$ over $[0,1]$ are attained at the endpoints of $[0,1]$.",
            "solution": "The problem statement is a valid exercise in introductory uncertainty quantification. It is scientifically grounded, well-posed, and contains all necessary definitions to proceed with the analysis. We will evaluate each option based on the provided fundamental bases.\n\nThe function under consideration is $f(x) = x(1-x) = x - x^2$. The uncertain input is known to be in the interval $x \\in [0,1]$.\nFirst, let us determine the true range of $f(x)$ for $x \\in [0,1]$. The function $f(x)$ describes a parabola opening downwards. Its maximum value can be found by setting its derivative to zero:\n$f'(x) = \\frac{d}{dx}(x - x^2) = 1 - 2x$.\nSetting $f'(x) = 0$ gives $1 - 2x = 0$, which yields $x = 1/2$.\nThe maximum value is $f(1/2) = \\frac{1}{2}(1 - \\frac{1}{2}) = \\frac{1}{4}$.\nThe values at the endpoints of the interval $[0,1]$ are $f(0)=0$ and $f(1) = 1(1-1) = 0$.\nTherefore, the true range of the function, which is the image $f([0,1])$, is the interval $[0, 1/4]$.\n\nNow, we will analyze each option.\n\n**A. Using classical interval arithmetic on $f([0,1]) = [0,1] \\cdot (1 - [0,1])$ returns $[0,1]$ and overestimates the true range because it ignores the dependency between repeated occurrences of $x$.**\n\nLet $X = [0,1]$ be the input interval. We want to compute $f(X) = X \\cdot (1 - X)$.\n1.  First, we compute the interval for $(1 - X)$:\n    $1 - X = 1 - [0,1] = \\{1 - x \\mid x \\in [0,1]\\}$. The minimum value is $1-1=0$ and the maximum is $1-0=1$. So, $1 - X = [0,1]$.\n2.  Next, we compute the product $X \\cdot (1 - X)$:\n    $X \\cdot (1 - X) = [0,1] \\cdot [0,1]$.\n    Using the rule for interval multiplication, for $[a,b] \\cdot [c,d]$, the result is $[\\min(ac, ad, bc, bd), \\max(ac, ad, bc, bd)]$.\n    Here, $a=0, b=1, c=0, d=1$. The four products are:\n    $0 \\cdot 0 = 0$\n    $0 \\cdot 1 = 0$\n    $1 \\cdot 0 = 0$\n    $1 \\cdot 1 = 1$\n    The minimum of these is $0$ and the maximum is $1$.\n    Thus, classical interval arithmetic returns the interval $[0,1]$.\n3.  The true range of the function is $[0, 1/4]$. The interval arithmetic result $[0,1]$ strictly contains the true range, $[0, 1/4] \\subset [0,1]$, so it is a valid but pessimistic bound. This overestimation is known as the \"dependency problem\" in interval arithmetic. When calculating $X \\cdot (1-X)$, the method treats the first $X$ and the second $X$ as independent variables, each ranging over $[0,1]$. It seeks the range of $y \\cdot z$ where $y \\in [0,1]$ and $z \\in [0,1]$. The maximum $1$ is achieved for $y=1, z=1$. However, in the original function $f(x)=x(1-x)$, if the first term $x$ is $1$, the second term $(1-x)$ must be $0$, not $1$. The dependency between the two occurrences of $x$ is lost.\nThe statement in option A is entirely correct.\n**Verdict for A: Correct.**\n\n**B. Using Affine Arithmetic with $x = \\frac{1}{2} + \\frac{1}{2}\\varepsilon_{1}$ and $1 - x = \\frac{1}{2} - \\frac{1}{2}\\varepsilon_{1}$, the product $x(1 - x)$ yields a tighter bound of $[0, \\frac{1}{4}]$.**\n\n1.  The affine form for an uncertain variable $x \\in [a,b]$ is $x = x_0 + r \\varepsilon_1$ where $x_0 = (a+b)/2$ is the midpoint and $r = (b-a)/2$ is the radius, with $\\varepsilon_1 \\in [-1,1]$. For $x \\in [0,1]$, we have $x_0 = (0+1)/2 = 1/2$ and $r = (1-0)/2 = 1/2$. So, the affine form is $x = \\frac{1}{2} + \\frac{1}{2}\\varepsilon_1$, as stated.\n2.  The affine form for $1-x$ is calculated as:\n    $1 - x = 1 - (\\frac{1}{2} + \\frac{1}{2}\\varepsilon_1) = \\frac{1}{2} - \\frac{1}{2}\\varepsilon_1$. This is also correct.\n3.  We compute the product $x(1-x)$:\n    $f(x) = x(1-x) = (\\frac{1}{2} + \\frac{1}{2}\\varepsilon_1)(\\frac{1}{2} - \\frac{1}{2}\\varepsilon_1)$\n    This is in the form $(A+B)(A-B) = A^2 - B^2$.\n    $f(x) = (\\frac{1}{2})^2 - (\\frac{1}{2}\\varepsilon_1)^2 = \\frac{1}{4} - \\frac{1}{4}\\varepsilon_1^2$.\n4.  To find the interval bounds for $f(x)$, we consider the range of $\\varepsilon_1^2$. Since $\\varepsilon_1 \\in [-1,1]$, we have $\\varepsilon_1^2 \\in [0,1]$.\n    The term $-\\frac{1}{4}\\varepsilon_1^2$ therefore has the range $[-\\frac{1}{4}(1), -\\frac{1}{4}(0)] = [-\\frac{1}{4}, 0]$.\n    Adding the constant $\\frac{1}{4}$, the range of $f(x)$ is $[\\frac{1}{4} - \\frac{1}{4}, \\frac{1}{4} + 0] = [0, \\frac{1}{4}]$.\n    This result, $[0, 1/4]$, is the exact range of the function. It is significantly tighter than the interval arithmetic result of $[0,1]$. The statement is correct.\n**Verdict for B: Correct.**\n\n**C. If $X \\sim \\operatorname{Uniform}(0,1)$, then $\\mathbb{E}[f(X)] = \\frac{1}{6}$ and $\\operatorname{Var}(f(X)) = \\frac{1}{180}$, which summarize probabilistic uncertainty and are not guaranteed enclosures of all possible values.**\n\n1.  We compute the expectation $\\mathbb{E}[f(X)]$. The problem gives $\\mathbb{E}[X^n] = \\frac{1}{n+1}$ for $X \\sim \\operatorname{Uniform}(0,1)$.\n    $\\mathbb{E}[f(X)] = \\mathbb{E}[X - X^2] = \\mathbb{E}[X] - \\mathbb{E}[X^2]$.\n    Using the formula, $\\mathbb{E}[X] = \\frac{1}{1+1} = \\frac{1}{2}$ and $\\mathbb{E}[X^2] = \\frac{1}{2+1} = \\frac{1}{3}$.\n    $\\mathbb{E}[f(X)] = \\frac{1}{2} - \\frac{1}{3} = \\frac{3-2}{6} = \\frac{1}{6}$. The expectation is correct.\n2.  We compute the variance $\\operatorname{Var}(f(X)) = \\mathbb{E}[f(X)^2] - (\\mathbb{E}[f(X)])^2$.\n    First, we need $\\mathbb{E}[f(X)^2]$:\n    $f(X)^2 = (X-X^2)^2 = X^2 - 2X^3 + X^4$.\n    $\\mathbb{E}[f(X)^2] = \\mathbb{E}[X^2 - 2X^3 + X^4] = \\mathbb{E}[X^2] - 2\\mathbb{E}[X^3] + \\mathbb{E}[X^4]$.\n    Using the formula again: $\\mathbb{E}[X^3] = \\frac{1}{3+1} = \\frac{1}{4}$ and $\\mathbb{E}[X^4] = \\frac{1}{4+1} = \\frac{1}{5}$.\n    $\\mathbb{E}[f(X)^2] = \\frac{1}{3} - 2(\\frac{1}{4}) + \\frac{1}{5} = \\frac{1}{3} - \\frac{1}{2} + \\frac{1}{5} = \\frac{10-15+6}{30} = \\frac{1}{30}$.\n    Now, we compute the variance:\n    $\\operatorname{Var}(f(X)) = \\frac{1}{30} - (\\frac{1}{6})^2 = \\frac{1}{30} - \\frac{1}{36} = \\frac{6-5}{180} = \\frac{1}{180}$. The variance is also correct.\n3.  The final part of the statement claims these statistics summarize probabilistic uncertainty and are not guaranteed enclosures. This is a correct and fundamental distinction. The mean ($\\mathbb{E}[f(X)]$) indicates the average value of the output, and the variance ($\\operatorname{Var}(f(X))$) measures the spread of the output's probability distribution around the mean. They do not define the set of all possible outcomes. The range of possible outcomes is $[0, 1/4]$, a deterministic enclosure, whereas $\\mathbb{E}[f(X)] \\pm \\sqrt{\\operatorname{Var}(f(X))}$ is a probabilistic statement about where values are likely to be concentrated.\nThe statement is fully correct.\n**Verdict for C: Correct.**\n\n**D. For $X \\sim \\operatorname{Uniform}(0,1)$, the interval $\\left[\\mathbb{E}[f(X)] - 2\\sqrt{\\operatorname{Var}(f(X))},\\ \\mathbb{E}[f(X)] + 2\\sqrt{\\operatorname{Var}(f(X))}\\right]$ is a guaranteed enclosure of $f(X)$ with probability $1$.**\n\n1.  This statement asserts that the support of the random variable $f(X)$ is contained within the interval defined by the mean plus/minus two standard deviations.\n2.  Let's compute the interval. From option C, $\\mu = \\mathbb{E}[f(X)] = 1/6$ and $\\sigma^2 = \\operatorname{Var}(f(X)) = 1/180$.\n    The standard deviation is $\\sigma = \\sqrt{1/180} = 1/\\sqrt{36 \\cdot 5} = 1/(6\\sqrt{5})$.\n    The interval is $[\\mu - 2\\sigma, \\mu + 2\\sigma] = [\\frac{1}{6} - \\frac{2}{6\\sqrt{5}}, \\frac{1}{6} + \\frac{2}{6\\sqrt{5}}] = [\\frac{1}{6} - \\frac{1}{3\\sqrt{5}}, \\frac{1}{6} + \\frac{1}{3\\sqrt{5}}]$.\n3.  To check if this is a guaranteed enclosure, we can compare its bounds to the true range of $f(X)$, which is $[0, 1/4]$.\n    Let's approximate the bounds: $\\sqrt{5} \\approx 2.236$.\n    $1/(3\\sqrt{5}) \\approx 1/(3 \\cdot 2.236) \\approx 1/6.708 \\approx 0.1491$.\n    $1/6 \\approx 0.1667$.\n    Lower bound: $0.1667 - 0.1491 \\approx 0.0176$.\n    Upper bound: $0.1667 + 0.1491 \\approx 0.3158$.\n    The interval is approximately $[0.0176, 0.3158]$.\n4.  The true range of $f(X)$ is $[0, 0.25]$. The proposed interval does not contain $0$, which is a possible value of $f(X)$ (when $X=0$ or $X=1$). For example, $f(0.01) = 0.0099$, which is less than the lower bound of $0.0176$. Thus, the interval is not a guaranteed enclosure. Chebyshev's inequality guarantees that the probability of a random variable being within $k$ standard deviations of the mean is at least $1 - 1/k^2$. For $k=2$, this is a probability of at least $75\\%$, not $100\\%$ (probability $1$).\nThe statement is incorrect.\n**Verdict for D: Incorrect.**\n\n**E. Classical interval arithmetic returns the exact image $f([0,1]) = [0, \\frac{1}{4}]$ because the minimum and maximum of $x(1-x)$ over $[0,1]$ are attained at the endpoints of $[0,1]$.**\n\nThis statement makes two claims.\n1.  **Claim 1:** \"Classical interval arithmetic returns the exact image $f([0,1]) = [0, \\frac{1}{4}]$\". As demonstrated in the analysis of option A, classical interval arithmetic returns $[0,1]$, which is an overestimation of the exact image $[0, 1/4]$. This claim is false.\n2.  **Claim 2:** \"...because the minimum and maximum of $x(1-x)$ over $[0,1]$ are attained at the endpoints of $[0,1]$\". As shown in the initial analysis, the minimum value of $f(x)$ is $0$, which is attained at the endpoints $x=0$ and $x=1$. However, the maximum value is $1/4$, which is attained at the interior point $x=1/2$. Therefore, the claim that both the minimum and maximum are attained at the endpoints is false.\nSince both the primary assertion and the justification are false, the entire statement is incorrect.\n**Verdict for E: Incorrect.**\n\nFinal summary: Options A, B, and C are correct. Options D and E are incorrect.",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}