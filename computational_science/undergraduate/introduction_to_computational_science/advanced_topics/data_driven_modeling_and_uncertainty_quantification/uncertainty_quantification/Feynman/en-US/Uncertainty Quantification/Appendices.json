{
    "hands_on_practices": [
        {
            "introduction": "Many computational models are based on physical laws that relate inputs to outputs. However, in practice, inputs derived from measurements are never known with perfect certainty. This exercise  demonstrates how to propagate small uncertainties through a non-linear equation, the Stefan–Boltzmann law, using a series approximation to estimate the resulting uncertainty in the output. This is a foundational skill in forward uncertainty quantification.",
            "id": "2536849",
            "problem": "A diffuse-gray isothermal patch of surface with emissivity $\\epsilon$, area $A$, and absolute temperature $T$ radiates to deep space. The net radiative heat rate is governed by the Stefan–Boltzmann law $q=\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\, T^{4}$, where $\\sigma_{\\mathrm{SB}}$ is the Stefan–Boltzmann constant. Suppose that $\\epsilon$, $A$, and $\\sigma_{\\mathrm{SB}}$ are known deterministic parameters, but the temperature $T$ is uncertain due to small sensor noise and process variability. Model $T$ as a Gaussian random variable with mean $\\mu_{T}0$ and variance $\\sigma_{T}^{2}$, and assume a small coefficient of variation $c=\\sigma_{T}/\\mu_{T}$ with $c \\ll 1$.\n\nStarting from the Stefan–Boltzmann law and the definitions of expectation and variance, and using only fundamental series expansions justified by the smallness of $c$, derive the first two moments of the nondimensional radiative heat rate\n$$\nr \\equiv \\frac{q}{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\,\\mu_{T}^{4}}.\n$$\nWork consistently to leading nontrivial order in $c$ and include all terms up to and including order $c^{2}$ in the mean and the variance. Express your final answers as closed-form analytic expressions in terms of $c$ only. Report the two results as a single row matrix $\\big(\\,\\mathbb{E}[r]\\;\\;\\mathrm{Var}(r)\\,\\big)$. Since $r$ is dimensionless, do not include units in your final answer.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique solution. It is therefore deemed valid. We proceed with the derivation.\n\nThe objective is to determine the first two moments, the expectation $\\mathbb{E}[r]$ and the variance $\\mathrm{Var}(r)$, of the nondimensional radiative heat rate $r$. The problem states that the net radiative heat rate is given by the Stefan–Boltzmann law, $q=\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\, T^{4}$. The nondimensional heat rate $r$ is defined as\n$$\nr \\equiv \\frac{q}{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\,\\mu_{T}^{4}}\n$$\nSubstituting the expression for $q$ into the definition of $r$, the deterministic parameters $\\epsilon$, $\\sigma_{\\mathrm{SB}}$, and $A$ cancel:\n$$\nr = \\frac{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\, T^{4}}{\\epsilon \\,\\sigma_{\\mathrm{SB}}\\, A \\,\\mu_{T}^{4}} = \\left(\\frac{T}{\\mu_{T}}\\right)^{4}\n$$\nThe absolute temperature $T$ is modeled as a Gaussian random variable with mean $\\mathbb{E}[T] = \\mu_{T}$ and variance $\\mathrm{Var}(T) = \\sigma_{T}^{2}$. It is convenient to define a standard normal random variable $X$ as\n$$\nX = \\frac{T - \\mu_{T}}{\\sigma_{T}}\n$$\nsuch that $X \\sim \\mathcal{N}(0,1)$. From this definition, we can express $T$ in terms of $X$:\n$$\nT = \\mu_{T} + \\sigma_{T} X\n$$\nSubstituting this into the expression for $r$ yields:\n$$\nr = \\left(\\frac{\\mu_{T} + \\sigma_{T} X}{\\mu_{T}}\\right)^{4} = \\left(1 + \\frac{\\sigma_{T}}{\\mu_{T}} X\\right)^{4}\n$$\nUsing the definition of the coefficient of variation, $c = \\sigma_{T}/\\mu_{T}$, the expression for $r$ becomes a polynomial in $X$ with coefficients that are powers of $c$:\n$$\nr = (1 + cX)^{4}\n$$\nSince the exponent is a positive integer, we can use the binomial theorem to obtain an exact, finite expansion:\n$$\nr = \\binom{4}{0}(cX)^{0} + \\binom{4}{1}(cX)^{1} + \\binom{4}{2}(cX)^{2} + \\binom{4}{3}(cX)^{3} + \\binom{4}{4}(cX)^{4}\n$$\n$$\nr = 1 + 4cX + 6c^{2}X^{2} + 4c^{3}X^{3} + c^{4}X^{4}\n$$\nTo find the expectation $\\mathbb{E}[r]$, we apply the expectation operator term by term, leveraging its linearity:\n$$\n\\mathbb{E}[r] = \\mathbb{E}[1 + 4cX + 6c^{2}X^{2} + 4c^{3}X^{3} + c^{4}X^{4}] = 1 + 4c\\mathbb{E}[X] + 6c^{2}\\mathbb{E}[X^{2}] + 4c^{3}\\mathbb{E}[X^{3}] + c^{4}\\mathbb{E}[X^{4}]\n$$\nThe moments of a standard normal random variable $X \\sim \\mathcal{N}(0,1)$ are well known: $\\mathbb{E}[X] = 0$, $\\mathbb{E}[X^{2}] = 1$, $\\mathbb{E}[X^{3}] = 0$, and $\\mathbb{E}[X^{4}] = 3$. Substituting these values gives the exact expression for the mean of $r$:\n$$\n\\mathbb{E}[r] = 1 + 4c(0) + 6c^{2}(1) + 4c^{3}(0) + c^{4}(3) = 1 + 6c^{2} + 3c^{4}\n$$\nThe problem requires the result up to and including order $c^{2}$. Truncating the exact expression, we obtain:\n$$\n\\mathbb{E}[r] \\approx 1 + 6c^{2}\n$$\nNext, we calculate the variance $\\mathrm{Var}(r)$. We use the fundamental relation $\\mathrm{Var}(r) = \\mathbb{E}[r^{2}] - (\\mathbb{E}[r])^{2}$. We must compute each term on the right-hand side consistently to the required order of accuracy. The leading term in the variance will be of order $c^{2}$, so we must compute $\\mathbb{E}[r^{2}]$ and $(\\mathbb{E}[r])^{2}$ including all terms up to $O(c^{2})$.\n\nFirst, we find an expression for $r^{2}$:\n$$\nr^{2} = ((1 + cX)^{4})^{2} = (1 + cX)^{8}\n$$\nExpanding this using the binomial theorem:\n$$\nr^{2} = 1 + 8cX + \\binom{8}{2}(cX)^{2} + O(c^{3}) = 1 + 8cX + 28c^{2}X^{2} + O(c^{3})\n$$\nTaking the expectation and keeping terms up to order $c^{2}$:\n$$\n\\mathbb{E}[r^{2}] \\approx \\mathbb{E}[1 + 8cX + 28c^{2}X^{2}] = 1 + 8c\\mathbb{E}[X] + 28c^{2}\\mathbb{E}[X^{2}] = 1 + 8c(0) + 28c^{2}(1) = 1 + 28c^{2}\n$$\nNext, we compute $(\\mathbb{E}[r])^{2}$ up to order $c^{2}$. Using our derived expression for $\\mathbb{E}[r]$:\n$$\n(\\mathbb{E}[r])^{2} = (1 + 6c^{2} + 3c^{4})^{2}\n$$\nExpanding and retaining terms up to order $c^{2}$:\n$$\n(\\mathbb{E}[r])^{2} \\approx (1 + 6c^{2})^{2} = 1^{2} + 2(1)(6c^{2}) + (6c^{2})^{2} = 1 + 12c^{2} + 36c^{4}\n$$\nTruncating to order $c^{2}$ gives:\n$$\n(\\mathbb{E}[r])^{2} \\approx 1 + 12c^{2}\n$$\nFinally, we compute the variance:\n$$\n\\mathrm{Var}(r) = \\mathbb{E}[r^{2}] - (\\mathbb{E}[r])^{2} \\approx (1 + 28c^{2}) - (1 + 12c^{2}) = 16c^{2}\n$$\nThis result is the leading nontrivial term in the expansion for the variance and is accurate to order $c^{2}$.\n\nThe first two moments of the nondimensional heat rate $r$, correct to order $c^{2}$, are:\n$$\n\\mathbb{E}[r] = 1 + 6c^{2}\n$$\n$$\n\\mathrm{Var}(r) = 16c^{2}\n$$\nThese are the required closed-form analytic expressions.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1+6c^{2}  16c^{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond propagating known input uncertainties, we often face the \"inverse problem\" of determining model parameters from noisy experimental data. This practice  introduces the powerful Bayesian framework to solve such problems. You will calibrate the Young's modulus of a material from stress-strain measurements, learning how to formally update your knowledge about a parameter and make predictions that incorporate this uncertainty.",
            "id": "2707423",
            "problem": "A slender, homogeneous, linearly elastic bar is subjected to small uniaxial strains. The constitutive relation at small strain is the linear Hooke law, which may be written as the measurement model\n$$\n\\sigma_i \\;=\\; E\\,\\varepsilon_i \\;+\\; \\eta_i,\\quad i=1,\\dots,n,\n$$\nwhere $E$ is the (unknown) Young’s modulus, $\\varepsilon_i$ are prescribed strain levels, $\\sigma_i$ are measured stresses, and $\\eta_i$ are independent and identically distributed (i.i.d.) Gaussian measurement errors with zero mean and known variance $\\sigma^2$. Assume a Bayesian calibration with a conjugate prior $E \\sim \\mathcal{N}(m_0, s_0^2)$, independent of the errors.\n\nYou are given $n=4$ measurements with strains and stresses (in gigapascals, $\\mathrm{GPa}$) as follows:\n$$\n\\big(\\varepsilon_1,\\sigma_1\\big)=(0.0010,\\,0.212),\\quad\n\\big(\\varepsilon_2,\\sigma_2\\big)=(0.0015,\\,0.310),\\\\\n\\big(\\varepsilon_3,\\sigma_3\\big)=(0.0020,\\,0.425),\\quad\n\\big(\\varepsilon_4,\\sigma_4\\big)=(0.0025,\\,0.520).\n$$\nThe measurement noise variance is known to be $\\sigma^2 = (0.01)^2$ in $\\mathrm{GPa}^2$, and the prior is $m_0 = 200$ in $\\mathrm{GPa}$ and $s_0^2 = 30^2$ in $\\mathrm{GPa}^2$.\n\nTasks:\n- Starting from Bayes’ theorem and the definitions of the Gaussian likelihood and prior, derive the posterior distribution of $E$ given the data $\\{(\\varepsilon_i,\\sigma_i)\\}_{i=1}^4$. Your derivation must not invoke any unproven shortcut formulas.\n- Using the law of total probability and conditioning on $E$, derive the posterior predictive distribution for a future stress measurement $\\sigma_\\star$ at a new strain level $\\varepsilon_\\star = 0.0030$.\n- Finally, evaluate the posterior predictive mean stress at $\\varepsilon_\\star=0.0030$ numerically, using the given data and hyperparameters. Round your answer to four significant figures and express it in $\\mathrm{GPa}$. Your final response must be a single real number.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in Bayesian calibration for a linear model in solid mechanics. All necessary information is provided and is internally consistent. We proceed with the solution.\n\nLet the set of $n$ measurements be denoted by $\\mathcal{D} = \\{(\\varepsilon_i, \\sigma_i)\\}_{i=1}^n$. We are tasked with finding the posterior distribution for Young's modulus $E$ and the posterior predictive distribution for a new stress measurement $\\sigma_\\star$.\n\nThe first task is to derive the posterior distribution of $E$ given the data, $p(E|\\mathcal{D})$. According to Bayes' theorem, the posterior probability density function is proportional to the product of the likelihood and the prior:\n$$\np(E|\\mathcal{D}) \\propto p(\\mathcal{D}|E) p(E)\n$$\nThe measurement model is given by $\\sigma_i = E\\varepsilon_i + \\eta_i$, where the errors $\\eta_i$ are independent and identically distributed (i.i.d.) as $\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that the conditional distribution of a single stress measurement $\\sigma_i$ given $E$ and $\\varepsilon_i$ is Gaussian:\n$$\n\\sigma_i | E \\sim \\mathcal{N}(E\\varepsilon_i, \\sigma^2)\n$$\nThe probability density function for $\\sigma_i$ is therefore:\n$$\np(\\sigma_i|E) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(\\sigma_i - E\\varepsilon_i)^2}{2\\sigma^2} \\right)\n$$\nSince the measurements are independent, the likelihood of the entire dataset $\\mathcal{D}$ (or simply the vector of stresses $\\pmb{\\sigma} = (\\sigma_1, \\dots, \\sigma_n)^T$) is the product of the individual densities:\n$$\np(\\mathcal{D}|E) = \\prod_{i=1}^n p(\\sigma_i|E) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\sigma_i - E\\varepsilon_i)^2 \\right)\n$$\nThe prior distribution for $E$ is given as a Gaussian: $E \\sim \\mathcal{N}(m_0, s_0^2)$. Its probability density function is:\n$$\np(E) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left( -\\frac{(E - m_0)^2}{2s_0^2} \\right)\n$$\nNow, we construct the posterior. We can ignore any constant factors that do not depend on $E$.\n$$\np(E|\\mathcal{D}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\sigma_i - E\\varepsilon_i)^2 \\right) \\exp\\left( -\\frac{(E - m_0)^2}{2s_0^2} \\right)\n$$\nThe posterior is proportional to the exponential of the sum of the arguments:\n$$\np(E|\\mathcal{D}) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^n (\\sigma_i - E\\varepsilon_i)^2 + \\frac{1}{s_0^2} (E - m_0)^2 \\right] \\right)\n$$\nTo identify the form of this distribution, we expand the terms in the exponent as a quadratic function of $E$:\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^n (\\sigma_i^2 - 2E\\varepsilon_i\\sigma_i + E^2\\varepsilon_i^2) + \\frac{1}{s_0^2} (E^2 - 2Em_0 + m_0^2)\n$$\nWe collect terms involving $E^2$ and $E$:\n$$\nE^2 \\left( \\frac{\\sum_{i=1}^n \\varepsilon_i^2}{\\sigma^2} + \\frac{1}{s_0^2} \\right) - 2E \\left( \\frac{\\sum_{i=1}^n \\varepsilon_i\\sigma_i}{\\sigma^2} + \\frac{m_0}{s_0^2} \\right) + \\text{const.}\n$$\nThis quadratic form in $E$ indicates that the posterior distribution is also Gaussian, which we can write as $E|\\mathcal{D} \\sim \\mathcal{N}(m_n, s_n^2)$. The density of such a distribution is proportional to $\\exp\\left(-\\frac{(E-m_n)^2}{2s_n^2}\\right)$, and the argument of its exponent is:\n$$\n\\frac{(E-m_n)^2}{2s_n^2} = \\frac{1}{2s_n^2} (E^2 - 2Em_n + m_n^2) = E^2\\left(\\frac{1}{2s_n^2}\\right) - E\\left(\\frac{m_n}{s_n^2}\\right) + \\text{const.}\n$$\nBy comparing the coefficients of $E^2$ and $E$ with the expression derived from the posterior, we can identify the posterior precision $1/s_n^2$ and the term $m_n/s_n^2$:\n$$\n\\frac{1}{s_n^2} = \\frac{1}{s_0^2} + \\frac{\\sum_{i=1}^n \\varepsilon_i^2}{\\sigma^2}\n$$\n$$\n\\frac{m_n}{s_n^2} = \\frac{m_0}{s_0^2} + \\frac{\\sum_{i=1}^n \\varepsilon_i\\sigma_i}{\\sigma^2}\n$$\nFrom these, the posterior variance $s_n^2$ and mean $m_n$ are:\n$$\ns_n^2 = \\left( \\frac{1}{s_0^2} + \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\varepsilon_i^2 \\right)^{-1}\n$$\n$$\nm_n = s_n^2 \\left( \\frac{m_0}{s_0^2} + \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\varepsilon_i\\sigma_i \\right)\n$$\nThe posterior distribution is thus $E|\\mathcal{D} \\sim \\mathcal{N}(m_n, s_n^2)$. This completes the first task.\n\nThe second task is to derive the posterior predictive distribution for a new measurement $\\sigma_\\star$ at a new strain $\\varepsilon_\\star$. We use the law of total probability, marginalizing over the unknown parameter $E$:\n$$\np(\\sigma_\\star|\\mathcal{D}) = \\int p(\\sigma_\\star, E | \\mathcal{D}) dE = \\int p(\\sigma_\\star | E, \\mathcal{D}) p(E|\\mathcal{D}) dE\n$$\nGiven $E$, the new measurement $\\sigma_\\star$ is independent of the past data $\\mathcal{D}$, so $p(\\sigma_\\star | E, \\mathcal{D}) = p(\\sigma_\\star | E)$. The model for the new measurement is $\\sigma_\\star = E \\varepsilon_\\star + \\eta_\\star$, where $\\eta_\\star \\sim \\mathcal{N}(0, \\sigma^2)$. Therefore, the corresponding distribution is:\n$$\np(\\sigma_\\star | E) = \\mathcal{N}(\\sigma_\\star; E\\varepsilon_\\star, \\sigma^2)\n$$\nThe posterior predictive distribution is the convolution of two Gaussian distributions:\n$$\np(\\sigma_\\star|\\mathcal{D}) = \\int \\mathcal{N}(\\sigma_\\star; E\\varepsilon_\\star, \\sigma^2) \\mathcal{N}(E; m_n, s_n^2) dE\n$$\nThis integral describes the distribution of the random variable $\\sigma_\\star$, which can be expressed as a sum of two independent Gaussian random variables (conditioned on the data $\\mathcal{D}$): the uncertainty in the measurement, $\\eta_\\star \\sim \\mathcal{N}(0, \\sigma^2)$, and the uncertainty in the mean stress, $E\\varepsilon_\\star$. The variable $E\\varepsilon_\\star$ is a scaled Gaussian variable, since $E \\sim \\mathcal{N}(m_n, s_n^2)$, thus $E\\varepsilon_\\star \\sim \\mathcal{N}(m_n\\varepsilon_\\star, s_n^2\\varepsilon_\\star^2)$.\nThe sum of two independent Gaussian random variables is also a Gaussian random variable. Its mean is the sum of the means, and its variance is the sum of the variances.\nThe mean of the posterior predictive distribution is:\n$$\n\\mathbb{E}[\\sigma_\\star|\\mathcal{D}] = \\mathbb{E}[E\\varepsilon_\\star + \\eta_\\star|\\mathcal{D}] = \\mathbb{E}[E\\varepsilon_\\star|\\mathcal{D}] + \\mathbb{E}[\\eta_\\star|\\mathcal{D}] = m_n \\varepsilon_\\star + 0 = m_n \\varepsilon_\\star\n$$\nThe variance of the posterior predictive distribution is:\n$$\n\\text{Var}(\\sigma_\\star|\\mathcal{D}) = \\text{Var}(E\\varepsilon_\\star + \\eta_\\star|\\mathcal{D}) = \\text{Var}(E\\varepsilon_\\star|\\mathcal{D}) + \\text{Var}(\\eta_\\star|\\mathcal{D}) = s_n^2 \\varepsilon_\\star^2 + \\sigma^2\n$$\nSo, the posterior predictive distribution is $\\sigma_\\star|\\mathcal{D} \\sim \\mathcal{N}(m_n \\varepsilon_\\star, s_n^2 \\varepsilon_\\star^2 + \\sigma^2)$. This completes the second task.\n\nThe third task is to evaluate the posterior predictive mean stress $\\mathbb{E}[\\sigma_\\star|\\mathcal{D}] = m_n \\varepsilon_\\star$ using the provided data.\nGiven data: $n=4$, $(\\varepsilon_1,\\sigma_1)=(0.0010, 0.212)$, $(\\varepsilon_2,\\sigma_2)=(0.0015, 0.310)$, $(\\varepsilon_3,\\sigma_3)=(0.0020, 0.425)$, $(\\varepsilon_4,\\sigma_4)=(0.0025, 0.520)$.\nHyperparameters: $m_0=200$, $s_0^2=30^2=900$, $\\sigma^2=(0.01)^2=0.0001$.\nNew strain: $\\varepsilon_\\star = 0.0030$.\n\nFirst, we compute the necessary sums from the data:\n$$\n\\sum_{i=1}^4 \\varepsilon_i^2 = (0.0010)^2 + (0.0015)^2 + (0.0020)^2 + (0.0025)^2 = 1.35 \\times 10^{-5}\n$$\n$$\n\\sum_{i=1}^4 \\varepsilon_i\\sigma_i = (0.0010)(0.212) + (0.0015)(0.310) + (0.0020)(0.425) + (0.0025)(0.520) = 0.002827\n$$\nNow, we calculate the posterior parameters. First, the posterior variance $s_n^2$:\n$$\n\\frac{1}{s_n^2} = \\frac{1}{s_0^2} + \\frac{\\sum \\varepsilon_i^2}{\\sigma^2} = \\frac{1}{900} + \\frac{1.35 \\times 10^{-5}}{0.0001} = \\frac{1}{900} + 0.135 = \\frac{1}{900} + \\frac{135}{1000} = \\frac{2 + 243}{1800} = \\frac{245}{1800} = \\frac{49}{360}\n$$\n$$\ns_n^2 = \\frac{360}{49}\n$$\nNext, we calculate the posterior mean $m_n$:\n$$\n\\frac{m_n}{s_n^2} = \\frac{m_0}{s_0^2} + \\frac{\\sum \\varepsilon_i\\sigma_i}{\\sigma^2} = \\frac{200}{900} + \\frac{0.002827}{0.0001} = \\frac{2}{9} + 28.27 = \\frac{2}{9} + \\frac{2827}{100} = \\frac{200 + 25443}{900} = \\frac{25643}{900}\n$$\n$$\nm_n = s_n^2 \\left( \\frac{25643}{900} \\right) = \\frac{360}{49} \\cdot \\frac{25643}{900} = \\frac{360}{900} \\cdot \\frac{25643}{49} = \\frac{2}{5} \\cdot \\frac{25643}{49} = \\frac{51286}{245}\n$$\nNumerically, $m_n \\approx 209.3306 \\text{ GPa}$. Finally, we compute the posterior predictive mean stress at $\\varepsilon_\\star = 0.0030$:\n$$\n\\mathbb{E}[\\sigma_\\star|\\mathcal{D}] = m_n \\varepsilon_\\star = \\frac{51286}{245} \\times 0.0030 \\approx 209.330612 \\times 0.0030 \\approx 0.6279918\n$$\nRounding to four significant figures, the result is $0.6280$. The units are GPa.",
            "answer": "$$ \\boxed{0.6280} $$"
        },
        {
            "introduction": "Many real-world systems, from weather patterns to vehicle navigation, are dynamic and evolve over time. This exercise  delves into uncertainty quantification for such systems using the Kalman filter, a fundamental tool for state estimation. You will investigate the critical issue of model misspecification—what happens when the filter's assumptions about system noise do not match reality—and learn how to diagnose such inconsistencies.",
            "id": "3201163",
            "problem": "Consider a one-dimensional linear Gaussian state-space model and a discrete-time Kalman filter with a potentially mis-specified process noise covariance. The true system evolves according to the equations $x_k = x_{k-1} + w_{k-1}$ and $z_k = x_k + v_k$, where $w_{k-1} \\sim \\mathcal{N}(0, Q_{\\text{true}})$ and $v_k \\sim \\mathcal{N}(0, R_{\\text{true}})$ are mutually independent and independent across time. The filter assumes the same dynamics and measurement model but uses a process noise covariance $Q_{\\text{filt}} = \\alpha \\, Q_{\\text{true}}$, where $\\alpha > 0$ is a scalar factor, and it uses the correct measurement noise covariance $R_{\\text{true}}$.\n\nStarting only from the standard Kalman filter definitions for prediction and update in linear Gaussian models, and using the fact that in steady-state the posterior covariance becomes time-invariant under constant $Q_{\\text{filt}}$ and $R_{\\text{true}}$, perform the following:\n\n1. Derive an analytical expression for the steady-state posterior covariance reported by the filter, denoted $P$, in terms of $Q_{\\text{true}}$, $R_{\\text{true}}$, and $\\alpha$. The derivation must start from the Kalman filter prediction-update covariance relations and reach a closed-form $P$ by solving the implied algebraic condition, without using or quoting any ready-made shortcut formulas.\n\n2. Using the Kalman gain computed from the mis-specified covariance, derive the steady-state variance of the actual estimation error $C = \\mathrm{Var}(x_k - \\hat{x}_{k|k})$ under the true noise covariances. Your derivation must start from the basic error dynamics implied by the Kalman filter update, express $C$ via a one-step variance recursion, and solve the steady-state condition to obtain $C$ as a function of the gain, $Q_{\\text{true}}$, and $R_{\\text{true}}$.\n\n3. Derive the expected value of the Normalized Innovation Squared (NIS), defined here as $\\mathbb{E}\\left[ \\frac{y_k^2}{S_{\\text{filt}}} \\right]$ where $y_k = z_k - \\hat{x}_{k|k-1}$ is the innovation, $S_{\\text{filt}}$ is the innovation variance predicted by the filter, and the expectation is taken with respect to the true model. Express this expected NIS in terms of the steady-state quantities and the given parameters.\n\n4. Define “filter consistency” in this context as the condition that the filter-reported posterior covariance is conservative relative to the true estimation error variance, that is, $P \\geq C$. Based on your derived expressions, determine whether the filter is conservative for each test case.\n\nFor each test case in the test suite below, compute the following outputs:\n- The steady-state posterior covariance $P$ reported by the filter.\n- The steady-state actual estimation error variance $C$ under the true noise.\n- The ratio $C / P$.\n- The expected NIS $\\mathbb{E}\\left[ \\frac{y_k^2}{S_{\\text{filt}}} \\right]$.\n- A boolean indicating conservativeness, which is true if $P \\geq C$ and false otherwise.\n\nAll floating-point outputs must be rounded to six decimal places. There are no physical units in this problem. Angles do not appear. Percentages must not be used.\n\nTest suite (each test case is a tuple $(Q_{\\text{true}}, R_{\\text{true}}, \\alpha)$):\n- $(1.0, 1.0, 1.0)$\n- $(1.0, 1.0, 0.1)$\n- $(1.0, 1.0, 5.0)$\n- $(1.0, 0.01, 0.1)$\n- $(0.001, 1.0, 10.0)$\n- $(1.0, 10.0, 0.01)$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a comma-separated sublist in square brackets. For example:\n\"[[P_1,C_1,ratio_1,NIS_1,is_consistent_1],[P_2,C_2,ratio_2,NIS_2,is_consistent_2],...]\"\nThere must be no spaces in the output string.",
            "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem in the field of estimation theory, specifically concerning the analysis of a mis-specified Kalman filter. All necessary information is provided. We proceed with the derivations and solution.\n\nThe one-dimensional linear Gaussian state-space model is given by:\nTrue system dynamics: $x_k = x_{k-1} + w_{k-1}$, where $w_{k-1} \\sim \\mathcal{N}(0, Q_{\\text{true}})$.\nTrue measurement model: $z_k = x_k + v_k$, where $v_k \\sim \\mathcal{N}(0, R_{\\text{true}})$.\nThe filter uses the correct system and measurement models, with a measurement noise covariance $R_{\\text{filt}} = R_{\\text{true}}$, but a mis-specified process noise covariance $Q_{\\text{filt}} = \\alpha Q_{\\text{true}}$ for some scalar $\\alpha > 0$. The model matrices are the identity, i.e., state transition $F=1$ and measurement matrix $H=1$.\n\n1. Derivation of the steady-state posterior covariance reported by the filter, $P$.\n\nThe Kalman filter equations for the covariance are the prediction step and the update step.\nPrediction: $P_{k|k-1} = F P_{k-1|k-1} F^T + Q_{\\text{filt}}$. For our scalar system with $F=1$, this is $P_{k|k-1} = P_{k-1|k-1} + Q_{\\text{filt}}$.\nUpdate: The Kalman gain is $K_k = P_{k|k-1} H^T (H P_{k|k-1} H^T + R_{\\text{filt}})^{-1}$. With $H=1$, this becomes $K_k = P_{k|k-1} (P_{k|k-1} + R_{\\text{filt}})^{-1}$. The posterior covariance is updated as $P_{k|k} = (I - K_k H) P_{k|k-1}$. With $H=1$, this simplifies to $P_{k|k} = (1 - K_k) P_{k|k-1}$.\n\nAt steady-state, the covariances become time-invariant. Let $P = \\lim_{k \\to \\infty} P_{k|k}$ and $P_{-} = \\lim_{k \\to \\infty} P_{k|k-1}$. The system of equations becomes:\n1. $P_{-} = P + Q_{\\text{filt}}$\n2. $K = \\frac{P_{-}}{P_{-} + R_{\\text{filt}}}$\n3. $P = (1 - K) P_{-}$\n\nSubstituting the expression for $K$ into the update equation for $P$:\n$P = \\left(1 - \\frac{P_{-}}{P_{-} + R_{\\text{filt}}}\\right) P_{-} = \\left(\\frac{P_{-} + R_{\\text{filt}} - P_{-}}{P_{-} + R_{\\text{filt}}}\\right) P_{-} = \\frac{R_{\\text{filt}} P_{-}}{P_{-} + R_{\\text{filt}}}$.\n\nNow, substitute $P_{-}$ from the prediction equation:\n$P = \\frac{R_{\\text{filt}} (P + Q_{\\text{filt}})}{(P + Q_{\\text{filt}}) + R_{\\text{filt}}}$.\nMultiplying both sides by the denominator gives:\n$P(P + Q_{\\text{filt}} + R_{\\text{filt}}) = R_{\\text{filt}} (P + Q_{\\text{filt}})$.\n$P^2 + P Q_{\\text{filt}} + P R_{\\text{filt}} = P R_{\\text{filt}} + Q_{\\text{filt}} R_{\\text{filt}}$.\n$P^2 + P Q_{\\text{filt}} - Q_{\\text{filt}} R_{\\text{filt}} = 0$.\n\nThis is a quadratic equation for $P$. Since covariance must be non-negative, we take the positive root:\n$P = \\frac{-Q_{\\text{filt}} \\pm \\sqrt{Q_{\\text{filt}}^2 - 4(1)(-Q_{\\text{filt}} R_{\\text{filt}})}}{2} = \\frac{-Q_{\\text{filt}} + \\sqrt{Q_{\\text{filt}}^2 + 4 Q_{\\text{filt}} R_{\\text{filt}}}}{2}$.\nSubstituting $Q_{\\text{filt}} = \\alpha Q_{\\text{true}}$ and $R_{\\text{filt}} = R_{\\text{true}}$ yields the final expression for $P$:\n$$P = \\frac{-\\alpha Q_{\\text{true}} + \\sqrt{(\\alpha Q_{\\text{true}})^2 + 4 \\alpha Q_{\\text{true}} R_{\\text{true}}}}{2}$$\n\n2. Derivation of the steady-state variance of the actual estimation error, $C$.\n\nLet the true estimation error be $e_{k|k} = x_k - \\hat{x}_{k|k}$. The filter's state update equation is $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (z_k - \\hat{x}_{k|k-1})$. Here, $K_k$ is the gain computed by the filter using its own (mis-specified) model. Substituting $z_k = x_k + v_k$:\n$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k(x_k + v_k - \\hat{x}_{k|k-1}) = (1-K_k)\\hat{x}_{k|k-1} + K_k x_k + K_k v_k$.\nThe posterior error is:\n$e_{k|k} = x_k - \\hat{x}_{k|k} = x_k - ((1-K_k)\\hat{x}_{k|k-1} + K_k x_k + K_k v_k) = (1-K_k)(x_k - \\hat{x}_{k|k-1}) - K_k v_k$.\nThis is $e_{k|k} = (1-K_k) e_{k|k-1} - K_k v_k$.\n\nThe prior error $e_{k|k-1} = x_k - \\hat{x}_{k|k-1}$ evolves according to the true dynamics $x_k = x_{k-1} + w_{k-1}$ and the filter's prediction $\\hat{x}_{k|k-1} = \\hat{x}_{k-1|k-1}$.\n$e_{k|k-1} = (x_{k-1} + w_{k-1}) - \\hat{x}_{k-1|k-1} = (x_{k-1} - \\hat{x}_{k-1|k-1}) + w_{k-1} = e_{k-1|k-1} + w_{k-1}$.\n\nThe variance of the posterior error, $C_k = \\mathrm{Var}(e_{k|k})$, is found by taking the variance of its recursive expression. The error $e_{k-1|k-1}$ is uncorrelated with the current noise terms $w_{k-1}$ and $v_k$.\n$C_k = \\mathrm{Var}((1-K_k)(e_{k-1|k-1}+w_{k-1}) - K_k v_k) = \\mathrm{Var}((1-K_k)e_{k|k-1}) + \\mathrm{Var}(-K_k v_k)$.\n$C_k = (1-K_k)^2 \\mathrm{Var}(e_{k|k-1}) + K_k^2 \\mathrm{Var}(v_k)$.\nThe prior error variance is $\\mathrm{Var}(e_{k|k-1}) = \\mathrm{Var}(e_{k-1|k-1} + w_{k-1}) = \\mathrm{Var}(e_{k-1|k-1}) + \\mathrm{Var}(w_{k-1}) = C_{k-1} + Q_{\\text{true}}$.\nSo, $C_k = (1-K_k)^2(C_{k-1} + Q_{\\text{true}}) + K_k^2 R_{\\text{true}}$.\n\nAt steady-state, $C_k = C_{k-1} = C$ and $K_k = K$:\n$C = (1-K)^2(C + Q_{\\text{true}}) + K^2 R_{\\text{true}}$.\nWe solve for $C$:\n$C = (1 - 2K + K^2)(C + Q_{\\text{true}}) + K^2 R_{\\text{true}}$.\n$C = C - 2KC + K^2C + (1-K)^2 Q_{\\text{true}} + K^2 R_{\\text{true}}$.\n$0 = -2KC + K^2C + (1-K)^2 Q_{\\text{true}} + K^2 R_{\\text{true}}$.\n$C(2K - K^2) = (1-K)^2 Q_{\\text{true}} + K^2 R_{\\text{true}}$.\n$C = \\frac{(1-K)^2 Q_{\\text{true}} + K^2 R_{\\text{true}}}{K(2-K)}$.\n\nThe steady-state gain $K$ is computed from the filter's steady-state quantities:\n$$K = \\frac{P + \\alpha Q_{\\text{true}}}{P + \\alpha Q_{\\text{true}} + R_{\\text{true}}}$$\nUsing this $K$, the true error variance $C$ is:\n$$C = \\frac{(1-K)^2 Q_{\\text{true}} + K^2 R_{\\text{true}}}{K(2-K)}$$\n\n3. Derivation of the expected Normalized Innovation Squared (NIS).\n\nThe NIS is defined as $\\frac{y_k^2}{S_{\\text{filt}}}$, where $y_k = z_k - \\hat{x}_{k|k-1}$ is the innovation and $S_{\\text{filt}}$ is the innovation variance predicted by the filter. We seek its expectation under the true model, $\\mathbb{E}\\left[ \\frac{y_k^2}{S_{\\text{filt}}} \\right]$.\nAt steady-state, $S_{\\text{filt}}$ is a constant, so the expectation becomes $\\frac{\\mathbb{E}[y_k^2]}{S_{\\text{filt}}}$. Let $S_{\\text{true}} = \\mathbb{E}[y_k^2]$ be the true variance of the innovation. The expected NIS is the ratio $\\frac{S_{\\text{true}}}{S_{\\text{filt}}}$.\n\nThe filter-predicted innovation variance is $S_{\\text{filt}} = H P_{-} H^T + R_{\\text{filt}}$. For our scalar model, $S_{\\text{filt}} = P_{-} + R_{\\text{filt}} = (P + Q_{\\text{filt}}) + R_{\\text{true}} = P + \\alpha Q_{\\text{true}} + R_{\\text{true}}$.\n\nThe true innovation is $y_k = z_k - \\hat{x}_{k|k-1} = (x_k+v_k) - \\hat{x}_{k|k-1} = (x_k - \\hat{x}_{k|k-1}) + v_k = e_{k|k-1} + v_k$.\nIts true variance is $S_{\\text{true}} = \\mathrm{Var}(e_{k|k-1} + v_k) = \\mathrm{Var}(e_{k|k-1}) + \\mathrm{Var}(v_k)$ since the prior error $e_{k|k-1}$ is uncorrelated with the measurement noise $v_k$.\nThe variance of the actual prior error is $\\mathrm{Var}(e_{k|k-1}) = C_{-} = C + Q_{\\text{true}}$.\nThus, $S_{\\text{true}} = (C + Q_{\\text{true}}) + R_{\\text{true}}$.\n\nThe expected NIS is therefore:\n$$\\mathbb{E}\\left[ \\frac{y_k^2}{S_{\\text{filt}}} \\right] = \\frac{S_{\\text{true}}}{S_{\\text{filt}}} = \\frac{C + Q_{\\text{true}} + R_{\\text{true}}}{P + \\alpha Q_{\\text{true}} + R_{\\text{true}}}$$\n\n4. Filter Consistency.\n\nFilter consistency is defined in this context as the condition that the filter-reported posterior covariance is conservative relative to the true estimation error variance. Mathematically, this is the inequality $P \\geq C$. This condition indicates that the filter is not overly optimistic about its own performance; the uncertainty it reports ($P$) is at least as large as the actual error variance ($C$). We will check this condition for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the mis-specified Kalman filter problem for a suite of test cases.\n    \"\"\"\n    # Test suite: each case is a tuple (Q_true, R_true, alpha)\n    test_cases = [\n        (1.0, 1.0, 1.0),\n        (1.0, 1.0, 0.1),\n        (1.0, 1.0, 5.0),\n        (1.0, 0.01, 0.1),\n        (0.001, 1.0, 10.0),\n        (1.0, 10.0, 0.01),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        q_true, r_true, alpha = case\n        \n        # Filter's assumed process noise covariance\n        q_filt = alpha * q_true\n\n        # 1. Calculate the steady-state posterior covariance P reported by the filter\n        # Solves the DARE: P^2 + P*q_filt - q_filt*r_true = 0\n        discriminant = q_filt**2 + 4 * q_filt * r_true\n        p = (-q_filt + np.sqrt(discriminant)) / 2\n\n        # 2. Calculate the actual steady-state estimation error variance C\n        \n        # First, calculate the steady-state Kalman gain K used by the filter\n        # K = (P + q_filt) / (P + q_filt + r_true)\n        k_numerator = p + q_filt\n        k_denominator = p + q_filt + r_true\n        k = k_numerator / k_denominator\n\n        # Now, calculate C using the derived formula\n        # C = ((1-K)^2 * q_true + K^2 * r_true) / (K * (2-K))\n        c_numerator = (1 - k)**2 * q_true + k**2 * r_true\n        c_denominator = k * (2 - k)\n        \n        # Handle the case where the denominator might be zero, although unlikely\n        # for these problem parameters.\n        if c_denominator == 0:\n            # This case indicates a problem, but for robustness\n            c = float('inf') if c_numerator  0 else 0.0\n        else:\n            c = c_numerator / c_denominator\n\n        # 3. Calculate the ratio C / P\n        # Handle p=0 case to avoid division by zero\n        ratio_c_p = c / p if p  0 else float('inf') if c  0 else 1.0\n\n        # 4. Calculate the expected Normalized Innovation Squared (NIS)\n        # Expected_NIS = (C + q_true + r_true) / (P + q_filt + r_true)\n        nis_numerator = c + q_true + r_true\n        nis_denominator = p + q_filt + r_true\n        expected_nis = nis_numerator / nis_denominator\n\n        # 5. Check for filter consistency (P = C)\n        is_consistent = (p = c)\n        \n        # Format the results for this case\n        case_results = [\n            round(p, 6),\n            round(c, 6),\n            round(ratio_c_p, 6),\n            round(expected_nis, 6),\n            str(is_consistent).lower()\n        ]\n        \n        # Create the string representation for the sublist\n        sublist_str = f\"[{','.join(map(str, case_results))}]\"\n        all_results.append(sublist_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}