{
    "hands_on_practices": [
        {
            "introduction": "Theory is best understood through application. This first practice problem takes us into the field of pharmacokinetics, where numerical integration is essential for analyzing how a drug behaves in the body. You will calculate the total drug exposure, known as the Area Under the Curve (AUC), for a realistic concentration model using the composite trapezoidal rule. The challenge lies in understanding how the choice of measurement times—the sampling schedule—can dramatically impact the accuracy of your result, a crucial consideration in experimental design. ",
            "id": "3108753",
            "problem": "You are to study the accuracy of the composite trapezoidal integration rule when applied to a piecewise-exponential pharmacokinetic concentration-time profile. The goal is to quantify how different sampling schedules affect the estimation error of area under the concentration-time curve (Area Under the Curve (AUC)). Work entirely in purely mathematical terms and implement a complete, runnable program that produces the requested outputs. Use the following scientifically realistic model and specifications.\n\nThe concentration-time model is a continuous, piecewise-exponential function on the interval $[0,T]$ with two change-points. Let $T = 24$ hours, with breakpoints at $t_1 = 1.5$ hours and $t_2 = 6.0$ hours. Let the initial concentration be $C_0 = 10$ milligrams per liter, and the exponential decay rates be $k_0 = 1.2$ per hour on $[0,t_1)$, $k_1 = 0.25$ per hour on $[t_1,t_2)$, and $k_2 = 0.05$ per hour on $[t_2,T]$. The concentration $c(t)$ is defined as the unique function on $[0,T]$ that is continuous and piecewise of the form $A \\exp(-k (t-t_{\\mathrm{start}}))$ on each subinterval, with the following construction:\n- On $[0,t_1)$: $c(t) = C_0 \\exp(-k_0 t)$.\n- On $[t_1,t_2)$: $c(t) = c(t_1)\\,\\exp(-k_1 (t - t_1))$, where $c(t_1) = C_0 \\exp(-k_0 t_1)$ ensures continuity at $t_1$.\n- On $[t_2,T]$: $c(t) = c(t_2)\\,\\exp(-k_2 (t - t_2))$, where $c(t_2) = c(t_1)\\,\\exp(-k_1 (t_2 - t_1))$ ensures continuity at $t_2$.\n\nYour tasks are:\n- Starting from the definition of the Riemann integral and the concept of piecewise linear approximation, implement the composite trapezoidal rule to approximate the integral of $c(t)$ over $[0,T]$ using an arbitrary nondecreasing sampling schedule $0 = t_0  t_1  \\dots  t_n = T$. The rule must correctly handle nonuniform step sizes.\n- Derive, from first principles using the antiderivative of the exponential function and additivity of integrals over adjacent intervals, an analytic expression for the exact AUC, denoted $I_{\\mathrm{exact}} = \\int_0^T c(t)\\,dt$, by summing the contributions over the three subintervals $[0,t_1]$, $[t_1,t_2]$, and $[t_2,T]$.\n- Using your composite trapezoidal rule implementation, estimate the AUC on the same interval using the sampling times for each schedule listed in the test suite below, and compute the absolute error $E = |I_{\\mathrm{trap}} - I_{\\mathrm{exact}}|$ in milligram-hour per liter.\n\nAll physical quantities must be handled consistently in hours for time and milligrams per liter for concentration. Express AUC in milligram-hour per liter. The absolute error must be reported in milligram-hour per liter. No angle units are involved. No percentages are involved.\n\nTest suite of sampling schedules (each is a list of times in hours):\n- Schedule A (uniform fine): $t_j = 0, 0.1, 0.2, \\dots, 24$.\n- Schedule B (uniform coarse): $t_j = 0, 4, 8, 12, 16, 20, 24$.\n- Schedule C (early-heavy sparse): $[0, 0.05, 0.1, 0.2, 0.5, 1.0, 1.5, 2.0, 4.0, 8.0, 12.0, 16.0, 24.0]$.\n- Schedule D (log-spaced, plus endpoints): construct $m = 18$ points by $t_i = 10^{a + i\\,(b-a)/(m-1)}$ for $i = 0, 1, \\dots, m-1$, where $a = -3$ and $b = \\log_{10}(24)$, then form the union with $\\{0, 24\\}$, remove duplicates, and sort increasingly.\n- Schedule E (endpoints only): $[0, 24]$.\n- Schedule F (aligned with change-points): $[0.0, 0.75, 1.5, 3.75, 6.0, 12.0, 24.0]$.\n\nYour program must:\n- Implement $c(t)$ exactly as defined, ensuring continuity at $t_1$ and $t_2$.\n- Compute $I_{\\mathrm{exact}}$ analytically from the definition of the model by integrating over each subinterval and summing the results.\n- Compute $I_{\\mathrm{trap}}$ for each schedule using the composite trapezoidal rule with the given times and the exact $c(t)$ values at those times.\n- For each schedule, compute the absolute error $E$ as a float in milligram-hour per liter and round each error to $10$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Schedule A error, Schedule B error, Schedule C error, Schedule D error, Schedule E error, Schedule F error], for example, \"[$e_A$,$e_B$,$e_C$,$e_D$,$e_E$,$e_F$]\". Each $e_{\\cdot}$ must be a decimal string rounded to $10$ decimal places.",
            "solution": "The problem is evaluated to be scientifically sound, well-posed, and contains all necessary information for a unique solution. We proceed with the derivation and implementation.\n\nThe problem requires the calculation of the absolute error between the exact analytical integral and a numerical approximation using the composite trapezoidal rule for a piecewise-exponential function representing pharmacokinetic concentration over time.\n\nThe concentration-time model $c(t)$ is defined on the interval $[0, T]$ where $T=24$ hours. The function is continuous and piecewise-defined with change-points at $t_1 = 1.5$ hours and $t_2 = 6.0$ hours. The defining parameters are the initial concentration $C_0 = 10$ mg/L and the exponential decay rates $k_0 = 1.2$ hr$^{-1}$, $k_1 = 0.25$ hr$^{-1}$, and $k_2 = 0.05$ hr$^{-1}$.\n\nThe function $c(t)$ is constructed as follows:\n- For $t \\in [0, t_1)$, the concentration is $c(t) = C_0 \\exp(-k_0 t)$.\n- For $t \\in [t_1, t_2)$, the concentration is $c(t) = c(t_1) \\exp(-k_1 (t - t_1))$. Continuity at $t_1$ is ensured by defining $c(t_1) = C_0 \\exp(-k_0 t_1)$.\n- For $t \\in [t_2, T]$, the concentration is $c(t) = c(t_2) \\exp(-k_2 (t - t_2))$. Continuity at $t_2$ is ensured by defining $c(t_2) = c(t_1) \\exp(-k_1 (t_2 - t_1))$.\n\nFirst, we derive the analytical expression for the exact Area Under the Curve (AUC), denoted as $I_{\\mathrm{exact}}$. The AUC is the definite integral of $c(t)$ from $0$ to $T$.\n$$I_{\\mathrm{exact}} = \\int_0^T c(t) \\, dt$$\nDue to the additivity property of integrals, we can split the integral over the subintervals defined by the change-points:\n$$I_{\\mathrm{exact}} = \\int_0^{t_1} c(t) \\, dt + \\int_{t_1}^{t_2} c(t) \\, dt + \\int_{t_2}^{T} c(t) \\, dt$$\nWe integrate each part separately.\n\n1.  Integral over $[0, t_1]$:\n    $$I_1 = \\int_0^{t_1} C_0 \\exp(-k_0 t) \\, dt = C_0 \\left[ -\\frac{1}{k_0} \\exp(-k_0 t) \\right]_0^{t_1} = C_0 \\left( -\\frac{1}{k_0} \\exp(-k_0 t_1) - \\left(-\\frac{1}{k_0} \\exp(0)\\right) \\right) = \\frac{C_0}{k_0} (1 - \\exp(-k_0 t_1))$$\n\n2.  Integral over $[t_1, t_2]$:\n    Let $c(t_1) = C_0 \\exp(-k_0 t_1)$ be the concentration at time $t_1$.\n    $$I_2 = \\int_{t_1}^{t_2} c(t_1) \\exp(-k_1 (t - t_1)) \\, dt = c(t_1) \\left[ -\\frac{1}{k_1} \\exp(-k_1(t - t_1)) \\right]_{t_1}^{t_2}$$\n    $$I_2 = c(t_1) \\left( -\\frac{1}{k_1} \\exp(-k_1(t_2 - t_1)) - \\left(-\\frac{1}{k_1} \\exp(0)\\right) \\right) = \\frac{c(t_1)}{k_1} (1 - \\exp(-k_1(t_2 - t_1)))$$\n\n3.  Integral over $[t_2, T]$:\n    Let $c(t_2) = c(t_1) \\exp(-k_1 (t_2 - t_1))$ be the concentration at time $t_2$.\n    $$I_3 = \\int_{t_2}^{T} c(t_2) \\exp(-k_2 (t - t_2)) \\, dt = c(t_2) \\left[ -\\frac{1}{k_2} \\exp(-k_2(t - t_2)) \\right]_{t_2}^{T}$$\n    $$I_3 = c(t_2) \\left( -\\frac{1}{k_2} \\exp(-k_2(T - t_2)) - \\left(-\\frac{1}{k_2} \\exp(0)\\right) \\right) = \\frac{c(t_2)}{k_2} (1 - \\exp(-k_2(T - t_2)))$$\n\nThe total exact AUC is the sum $I_{\\mathrm{exact}} = I_1 + I_2 + I_3$.\n\nNext, we define the composite trapezoidal rule for a non-uniform sampling schedule $t_0, t_1, \\dots, t_n$, where $0 = t_0  t_1  \\dots  t_n = T$. The integral is approximated by summing the areas of trapezoids formed over each subinterval $[t_{i-1}, t_i]$.\n$$I_{\\mathrm{trap}} = \\sum_{i=1}^{n} \\frac{c(t_{i-1}) + c(t_i)}{2} (t_i - t_{i-1})$$\nThis formula is implemented to compute the numerical approximation $I_{\\mathrm{trap}}$ for each of the given sampling schedules.\n\nFinally, for each schedule, the absolute error $E$ is calculated as:\n$$E = |I_{\\mathrm{trap}} - I_{\\mathrm{exact}}|$$\n\nThe overall procedure is as follows:\n1.  Define the constants $C_0, k_0, k_1, k_2, t_1, t_2, T$.\n2.  Implement the continuous, piecewise function $c(t)$ based on its definition.\n3.  Calculate the exact integral $I_{\\mathrm{exact}}$ using the derived analytical formulas.\n4.  For each test schedule (A through F):\n    a.  Generate the array of time points $t_0, t_1, \\dots, t_n$.\n    b.  Evaluate the concentration $c(t_i)$ at each time point.\n    c.  Compute the approximate integral $I_{\\mathrm{trap}}$ using the composite trapezoidal rule.\n    d.  Calculate the absolute error $E = |I_{\\mathrm{trap}} - I_{\\mathrm{exact}}|$.\n    e.  Round the error $E$ to $10$ decimal places.\n5.  Collect the rounded errors and format them into the specified output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the pharmacokinetic AUC error analysis problem.\n    \"\"\"\n    # Define model parameters\n    C0 = 10.0  # mg/L\n    k0 = 1.2   # 1/hr\n    k1 = 0.25  # 1/hr\n    k2 = 0.05  # 1/hr\n    t1 = 1.5   # hr\n    t2 = 6.0   # hr\n    T = 24.0   # hr\n\n    # Pre-calculate concentrations at breakpoints for the c(t) function\n    c_at_t1 = C0 * np.exp(-k0 * t1)\n    c_at_t2 = c_at_t1 * np.exp(-k1 * (t2 - t1))\n\n    def concentration(t):\n        \"\"\"\n        Calculates the concentration c(t) at time t or for a numpy array of times.\n        The function is vectorized using numpy.piecewise.\n        \"\"\"\n        t = np.asarray(t)\n        \n        # Conditions for the piecewise function\n        condlist = [t  t1, (t = t1)  (t  t2), t = t2]\n        \n        # Functions for each piece\n        funclist = [\n            lambda t: C0 * np.exp(-k0 * t),\n            lambda t: c_at_t1 * np.exp(-k1 * (t - t1)),\n            lambda t: c_at_t2 * np.exp(-k2 * (t - t2))\n        ]\n        \n        return np.piecewise(t, condlist, funclist)\n\n    def analytic_auc():\n        \"\"\"\n        Calculates the exact area under the curve (AUC) from t=0 to T.\n        \"\"\"\n        # Integral over [0, t1]\n        I1 = (C0 / k0) * (1.0 - np.exp(-k0 * t1))\n        \n        # Integral over [t1, t2]\n        I2 = (c_at_t1 / k1) * (1.0 - np.exp(-k1 * (t2 - t1)))\n        \n        # Integral over [t2, T]\n        I3 = (c_at_t2 / k2) * (1.0 - np.exp(-k2 * (T - t2)))\n        \n        return I1 + I2 + I3\n\n    def composite_trapezoidal_rule(time_points, concentrations):\n        \"\"\"\n        Implements the composite trapezoidal rule for non-uniform grid spacing.\n        \"\"\"\n        if len(time_points)  2:\n            return 0.0\n        \n        time_diffs = np.diff(time_points) # h_i = t_i - t_{i-1}\n        conc_sums = concentrations[:-1] + concentrations[1:] # c(t_{i-1}) + c(t_i)\n        \n        return np.sum(time_diffs * conc_sums / 2.0)\n\n    # --- Define Test Suite of Sampling Schedules ---\n\n    # Schedule A: uniform fine\n    schedule_A = np.linspace(0.0, 24.0, 241)\n\n    # Schedule B: uniform coarse\n    schedule_B = np.linspace(0.0, 24.0, 7)\n    \n    # Schedule C: early-heavy sparse\n    schedule_C = np.array([0.0, 0.05, 0.1, 0.2, 0.5, 1.0, 1.5, 2.0, 4.0, 8.0, 12.0, 16.0, 24.0])\n\n    # Schedule D: log-spaced, plus endpoints\n    m = 18\n    a = -3\n    b = np.log10(T)\n    # Generate log-spaced points, then form the union with endpoints to ensure they are included\n    log_points = np.logspace(a, b, num=m, base=10.0)\n    schedule_D = np.union1d(log_points, [0.0, T])\n\n    # Schedule E: endpoints only\n    schedule_E = np.array([0.0, 24.0])\n\n    # Schedule F: aligned with change-points\n    schedule_F = np.array([0.0, 0.75, 1.5, 3.75, 6.0, 12.0, 24.0])\n    \n    test_cases = [\n        schedule_A,\n        schedule_B,\n        schedule_C,\n        schedule_D,\n        schedule_E,\n        schedule_F\n    ]\n    \n    # --- Perform Calculations ---\n    \n    # Calculate the exact integral once\n    I_exact = analytic_auc()\n    \n    results = []\n    for schedule in test_cases:\n        # Evaluate concentrations at the sampling times\n        conc_values = concentration(schedule)\n        \n        # Compute the integral using the trapezoidal rule\n        I_trap = composite_trapezoidal_rule(schedule, conc_values)\n        \n        # Compute the absolute error\n        error = np.abs(I_trap - I_exact)\n        \n        # Round the error to 10 decimal places as required\n        rounded_error = round(error, 10)\n        \n        results.append(rounded_error)\n\n    # Format the final output string\n    # Using f-strings with a format specifier to ensure 10 decimal places are shown\n    formatted_results = [f\"{res:.10f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Numerical methods rarely operate in isolation; their errors can propagate and influence subsequent calculations. This exercise explores the critical interface between numerical integration and optimization, a common pairing in scientific modeling and machine learning. You will use composite integration rules to approximate an objective function and observe how the inherent discretization error creates a \"noisy\" gradient, which in turn affects the behavior and convergence of a gradient descent optimizer. This practice provides a tangible demonstration of how the accuracy of an inner loop (integration) dictates the performance of an outer loop (optimization). ",
            "id": "3108801",
            "problem": "You will investigate how discretization error from composite numerical integration propagates into a gradient-based optimization loop and affects convergence. Consider the scalar optimization problem of minimizing the functional\n$$\nJ(a) \\;=\\; \\int_{0}^{1} \\left(a\\,x^{2} - \\sin(2\\pi x)\\right)^{2}\\,dx,\n$$\nwhere the sine function’s argument is in radians. The outer optimization loop uses gradient descent on the scalar parameter $a$, while the inner objective evaluation uses a composite numerical integration rule. Your task is to implement a program that demonstrates how the choice of composite integration rule and mesh resolution influences descent via noisy gradients.\n\nRequired components and constraints:\n\n- Fundamental base:\n  - Use the composite trapezoidal rule and the composite Simpson’s rule, both of which are standard Newton–Cotes formulas.\n  - Use the centered finite difference formula to approximate the derivative of the quadrature-approximated objective with respect to $a$.\n  - Use fixed-step gradient descent as the outer optimizer.\n\n- Mathematical definitions to implement:\n  1. Composite trapezoidal rule on $[0,1]$ with $N$ subintervals:\n     - Let $h = \\frac{1-0}{N}$ and $x_{i} = 0 + i\\,h$ for $i=0,1,\\dots,N$. For a given function $f(x)$,\n       $$\n       \\int_{0}^{1} f(x)\\,dx \\;\\approx\\; h\\left(\\tfrac{1}{2}f(x_{0}) + \\sum_{i=1}^{N-1} f(x_{i}) + \\tfrac{1}{2}f(x_{N})\\right).\n       $$\n  2. Composite Simpson’s rule on $[0,1]$ with $N$ subintervals:\n     - Require that $N$ is even. With the same $h$ and nodes $x_{i}$,\n       $$\n       \\int_{0}^{1} f(x)\\,dx \\;\\approx\\; \\frac{h}{3}\\left(f(x_{0}) + f(x_{N}) + 4\\sum_{i=1,\\,\\text{odd}}^{N-1} f(x_{i}) + 2\\sum_{i=2,\\,\\text{even}}^{N-2} f(x_{i})\\right).\n       $$\n  3. Gradient approximation for the outer loop using a centered finite difference with step $\\delta$:\n     $$\n     \\frac{dJ}{da}(a) \\;\\approx\\; \\frac{J_{\\text{quad}}(a+\\delta) - J_{\\text{quad}}(a-\\delta)}{2\\,\\delta},\n     $$\n     where $J_{\\text{quad}}$ is the quadrature approximation to $J$ using one of the composite rules.\n  4. Gradient descent update with fixed learning rate $\\eta$:\n     $$\n     a_{k+1} \\;=\\; a_{k} \\;-\\; \\eta\\,g_{k}, \\quad \\text{where } g_{k} \\text{ is the finite-difference gradient at } a_{k}.\n     $$\n\n- Termination and stability:\n  - Initialize with $a_{0} = 0$.\n  - Use a gradient tolerance $\\varepsilon_{g} = 10^{-8}$ and a parameter tolerance $\\varepsilon_{a} = 10^{-9}$.\n  - Use a maximum number of iterations $K_{\\max} = 10000$.\n  - Declare non-convergence if the loop exceeds $K_{\\max}$ without meeting tolerances or if $|a_{k}|$ grows beyond $10^{6}$.\n\n- Reference optimum for error assessment:\n  - Because $J(a)$ is a quadratic functional in $a$, its exact minimizer $a_{\\star}$ can be obtained by minimizing the exact integral with respect to $a$ using standard calculus. Compute this $a_{\\star}$ analytically and use it only to report an absolute error $|a_{\\text{final}} - a_{\\star}|$; do not use it to guide the optimizer.\n\n- Test suite:\n  - For each test case, run the optimizer using the specified composite rule, number of subintervals $N$, learning rate $\\eta$, and finite difference step $\\delta$:\n    1. Composite trapezoidal rule, $N=16$, $\\eta=0.5$, $\\delta=10^{-4}$.\n    2. Composite trapezoidal rule, $N=64$, $\\eta=0.5$, $\\delta=10^{-4}$.\n    3. Composite Simpson’s rule, $N=16$ (even), $\\eta=0.5$, $\\delta=10^{-4}$.\n    4. Composite trapezoidal rule, $N=4$, $\\eta=0.5$, $\\delta=10^{-4}$.\n    5. Composite trapezoidal rule, $N=16$, $\\eta=1.6$, $\\delta=10^{-4}$.\n\n- Required final output format:\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n  - For each test case, output a list in the form $[\\text{converged}, \\text{iterations}, a_{\\text{final}}, \\text{abs\\_error}]$ where:\n    - $\\text{converged}$ is $1$ if convergence occurred by the defined tolerances and $0$ otherwise.\n    - $\\text{iterations}$ is the number of iterations taken (an integer).\n    - $a_{\\text{final}}$ is the final estimate of $a$, rounded to six decimal places.\n    - $\\text{abs\\_error}$ is $|a_{\\text{final}} - a_{\\star}|$, rounded to six decimal places.\n  - Example schema (not actual values): $[[1,123,-0.750000,0.010000],[0,10000,-0.300000,0.400000],\\dots]$.\n\nNotes:\n- All trigonometric evaluations must use radians.\n- No physical units are involved in this problem.\n- Your implementation must be self-contained and must not read any input.",
            "solution": "We derive the required components from fundamental numerical analysis principles and basic calculus, then assemble an algorithm to demonstrate how composite integration errors perturb gradient descent.\n\n1. Objective structure and exact minimizer. The functional is\n$$\nJ(a) \\;=\\; \\int_{0}^{1} \\left(a\\,x^{2} - \\sin(2\\pi x)\\right)^{2}\\,dx.\n$$\nExpanding the square shows that $J(a)$ is a quadratic polynomial in $a$. Differentiating the exact integral with respect to $a$ and setting the derivative to zero yields the first-order optimality condition\n$$\n\\frac{dJ}{da}(a) \\;=\\; 2\\int_{0}^{1} \\left(a\\,x^{2} - \\sin(2\\pi x)\\right) x^{2}\\,dx \\;=\\; 0,\n$$\nwhich implies\n$$\na_{\\star}\\int_{0}^{1} x^{4}\\,dx \\;=\\; \\int_{0}^{1} x^{2}\\sin(2\\pi x)\\,dx.\n$$\nThe left-hand integral is elementary:\n$$\n\\int_{0}^{1} x^{4}\\,dx \\;=\\; \\left.\\frac{x^{5}}{5}\\right|_{0}^{1} \\;=\\; \\frac{1}{5}.\n$$\nFor the right-hand side, define $k = 2\\pi$. Using integration by parts twice or a known antiderivative, one obtains\n$$\n\\int x^{2}\\sin(kx)\\,dx \\;=\\; -\\frac{x^{2}\\cos(kx)}{k} + \\frac{2x\\sin(kx)}{k^{2}} + \\frac{2\\cos(kx)}{k^{3}} + C.\n$$\nEvaluating from $x=0$ to $x=1$ with $k=2\\pi$, where $\\cos(2\\pi)=1$ and $\\sin(2\\pi)=0$, gives\n$$\n\\int_{0}^{1} x^{2}\\sin(2\\pi x)\\,dx \\;=\\; -\\frac{1}{2\\pi}.\n$$\nTherefore,\n$$\na_{\\star} \\;=\\; \\frac{\\int_{0}^{1} x^{2}\\sin(2\\pi x)\\,dx}{\\int_{0}^{1} x^{4}\\,dx} \\;=\\; \\frac{-\\frac{1}{2\\pi}}{\\frac{1}{5}} \\;=\\; -\\frac{5}{2\\pi}.\n$$\nThis exact $a_{\\star}$ provides a reference to quantify the error of the numerically obtained $a_{\\text{final}}$.\n\n2. Composite integration rules. For a function $f(x)$ on $[0,1]$ with a uniform grid of $N$ subintervals and spacing $h = \\frac{1}{N}$, define nodes $x_{i} = i\\,h$ for $i=0,1,\\dots,N$.\n- Composite trapezoidal rule:\n$$\n\\int_{0}^{1} f(x)\\,dx \\;\\approx\\; h\\left(\\tfrac{1}{2}f(x_{0}) + \\sum_{i=1}^{N-1} f(x_{i}) + \\tfrac{1}{2}f(x_{N})\\right).\n$$\nThis rule has a global truncation error of order $O(h^{2})$ for sufficiently smooth $f$.\n- Composite Simpson’s rule (with $N$ even):\n$$\n\\int_{0}^{1} f(x)\\,dx \\;\\approx\\; \\frac{h}{3}\\left(f(x_{0}) + f(x_{N}) + 4\\sum_{\\substack{i=1\\\\ i \\text{ odd}}}^{N-1} f(x_{i}) + 2\\sum_{\\substack{i=2\\\\ i \\text{ even}}}^{N-2} f(x_{i})\\right).\n$$\nThis rule has a global truncation error of order $O(h^{4})$ for sufficiently smooth $f$.\n\nIn our setting, the integrand is\n$$\nf(x;a) \\;=\\; \\left(a\\,x^{2} - \\sin(2\\pi x)\\right)^{2},\n$$\nwhich is smooth in $x$ and $a$, so both composite rules apply.\n\n3. Noisy gradients via finite differences. In an outer optimization loop, one often uses gradients. If the objective is computed via quadrature, the quadrature error makes the objective $J_{\\text{quad}}(a)$ only an approximation to $J(a)$. A straightforward way to obtain a gradient estimate entirely from these approximate objective values is the centered finite difference\n$$\ng(a) \\;\\approx\\; \\frac{J_{\\text{quad}}(a+\\delta) - J_{\\text{quad}}(a-\\delta)}{2\\,\\delta}.\n$$\nThis $g(a)$ inherits discretization-induced roughness from $J_{\\text{quad}}$. As the number of subintervals $N$ increases (smaller $h$), the approximation improves and the gradient becomes less noisy. Simpson’s rule typically has smaller error than the trapezoidal rule for the same $N$.\n\n4. Gradient descent update and stopping. Given a current estimate $a_{k}$, a fixed learning rate $\\eta$, and the gradient estimate $g_{k}$, perform\n$$\na_{k+1} \\;=\\; a_{k} \\;-\\; \\eta\\,g_{k}.\n$$\nStop if the gradient magnitude is small, $|g_{k}|  \\varepsilon_{g}$, or if the parameter update is tiny, $|a_{k+1} - a_{k}|  \\varepsilon_{a}$. Also guard against divergence by aborting if $|a_{k}|  10^{6}$ or if $k$ reaches $K_{\\max}$.\n\n5. Test suite design and expected effects.\n- With the composite trapezoidal rule and a coarse grid (e.g., $N=4$), the quadrature error is larger, the finite-difference gradient is noisier, and convergence can be slow or may fail within the iteration budget depending on $\\eta$.\n- Increasing $N$ to $N=16$ or $N=64$ reduces the quadrature error, so the gradient becomes smoother, improving convergence behavior.\n- For the same $N$, the composite Simpson’s rule (with $N$ even, e.g., $N=16$) typically yields a more accurate objective and a less noisy gradient, which should accelerate convergence compared to the trapezoidal rule.\n- A large learning rate (e.g., $\\eta=1.6$) can destabilize the descent, and noise exacerbates this, potentially triggering oscillations or divergence.\n\n6. Program outputs. For each specified test case, the program runs the outer loop and reports a list $[\\text{converged}, \\text{iterations}, a_{\\text{final}}, \\text{abs\\_error}]$, where $\\text{abs\\_error} = |a_{\\text{final}} - a_{\\star}|$ and $a_{\\star} = -\\frac{5}{2\\pi}$. The single-line program output is a list of such lists in the given order. This arrangement allows direct comparison across rules and $N$, revealing how composite integration choices and mesh sizes influence the optimizer through gradient noise.",
            "answer": "```python\nimport numpy as np\n\ndef composite_trapezoid(f, a, b, N):\n    h = (b - a) / N\n    x = np.linspace(a, b, N + 1)\n    fx = f(x)\n    return h * (0.5 * fx[0] + fx[1:-1].sum() + 0.5 * fx[-1])\n\ndef composite_simpson(f, a, b, N):\n    if N % 2 != 0:\n        raise ValueError(\"Simpson's rule requires an even N.\")\n    h = (b - a) / N\n    x = np.linspace(a, b, N + 1)\n    fx = f(x)\n    # odd indices 1..N-1 step 2, even indices 2..N-2 step 2\n    odd_sum = fx[1:-1:2].sum()\n    even_sum = fx[2:-1:2].sum()\n    return (h / 3.0) * (fx[0] + fx[-1] + 4.0 * odd_sum + 2.0 * even_sum)\n\ndef objective_quadrature(a_param, rule, N):\n    # integrand: (a x^2 - sin(2π x))^2\n    def integrand(x):\n        return (a_param * x**2 - np.sin(2.0 * np.pi * x))**2\n    if rule == \"trap\":\n        return composite_trapezoid(integrand, 0.0, 1.0, N)\n    elif rule == \"simp\":\n        return composite_simpson(integrand, 0.0, 1.0, N)\n    else:\n        raise ValueError(\"Unknown rule\")\n\ndef finite_diff_grad(a_param, rule, N, delta):\n    # centered finite difference\n    jp = objective_quadrature(a_param + delta, rule, N)\n    jm = objective_quadrature(a_param - delta, rule, N)\n    return (jp - jm) / (2.0 * delta)\n\ndef gradient_descent(rule, N, eta, delta,\n                     a0=0.0, tol_g=1e-8, tol_a=1e-9, max_iter=10000, blowup=1e6):\n    a = a0\n    converged = False\n    iters = 0\n    for k in range(max_iter):\n        g = finite_diff_grad(a, rule, N, delta)\n        # update\n        a_new = a - eta * g\n        iters = k + 1\n        if not np.isfinite(a_new) or abs(a_new)  blowup:\n            converged = False\n            a = a_new\n            break\n        # termination checks\n        if abs(g)  tol_g or abs(a_new - a)  tol_a:\n            converged = True\n            a = a_new\n            break\n        a = a_new\n    # If loop ends without break by convergence, check last state\n    return converged, iters, a\n\ndef main():\n    # Analytical minimizer a* = -5 / (2π)\n    a_star = -5.0 / (2.0 * np.pi)\n\n    # Test cases: (rule, N, eta, delta)\n    test_cases = [\n        (\"trap\", 16, 0.5, 1e-4),\n        (\"trap\", 64, 0.5, 1e-4),\n        (\"simp\", 16, 0.5, 1e-4),\n        (\"trap\", 4, 0.5, 1e-4),\n        (\"trap\", 16, 1.6, 1e-4),\n    ]\n\n    results = []\n    for rule, N, eta, delta in test_cases:\n        # Ensure Simpson has even N (already ensured in the suite)\n        try:\n            converged, iters, a_final = gradient_descent(rule, N, eta, delta)\n        except Exception:\n            converged, iters, a_final = (False, 0, float(\"nan\"))\n        abs_err = abs(a_final - a_star) if np.isfinite(a_final) else float(\"inf\")\n        # Prepare formatted result: [converged(int), iterations(int), a_final(6dp), abs_error(6dp)]\n        results.append((1 if converged else 0, iters, a_final, abs_err))\n\n    # Format output exactly as specified: list of lists on one line, comma-separated, no extra text\n    out_items = []\n    for conv, iters, a_fin, err in results:\n        # Round floats to 6 decimals; if not finite, print \"nan\" or \"inf\" consistently\n        if np.isfinite(a_fin):\n            a_str = f\"{a_fin:.6f}\"\n        else:\n            a_str = \"nan\" if np.isnan(a_fin) else \"inf\"\n        if np.isfinite(err):\n            e_str = f\"{err:.6f}\"\n        else:\n            e_str = \"nan\" if np.isnan(err) else \"inf\"\n        out_items.append(f\"[{conv},{iters},{a_str},{e_str}]\")\n    print(\"[\" + \",\".join(out_items) + \"]\")\n\nif __name__ == \"__main__\":\n    main()\n```"
        },
        {
            "introduction": "In real-time systems, answers are needed not just correctly, but quickly. This advanced exercise challenges you to design an \"anytime\" integration algorithm, one that can provide an answer within a strict computational budget and incrementally improve it if more time becomes available. By implementing a dyadic refinement scheme, you will discover an elegant and efficient relationship between the composite trapezoidal and Simpson's rules. This practice moves beyond merely applying rules to designing intelligent, adaptive numerical methods for resource-constrained environments. ",
            "id": "3108814",
            "problem": "You are tasked with designing and implementing an anytime numerical integration scheme grounded in composite integration rules for a real-time control context. In such contexts, integrals of a scalar function $f(x)$ over a finite closed interval $[a,b]$ must be approximated within a strict deadline. To model this constraint in a purely mathematical way, define a computation budget $N_{\\max}$ as the maximum number of allowed evaluations of the function $f(x)$, counted as the total number of distinct points in $[a,b]$ at which $f(x)$ is computed. The goal is to construct an algorithm that incrementally refines estimates and can output a valid approximation at any point up to the budget limit.\n\nStart from the definition of the definite integral as the limit of Riemann sums and the concept of piecewise polynomial interpolation. Consider the following requirements:\n\n- Use a dyadic refinement of the interval $[a,b]$, beginning with a single subinterval ($n=1$), and then repeatedly doubling the number of subintervals ($n \\to 2n$), to construct a uniform partition. At each refinement step from $n$ to $2n$, only evaluate $f(x)$ at the $n$ new midpoints introduced by the bisection of each subinterval. Always reuse previously computed values of $f(x)$ at all grid points.\n- Maintain two composite approximations simultaneously:\n  1. A composite trapezoidal-rule approximation on the current uniform partition with $n$ subintervals.\n  2. A composite Simpson-rule approximation that is defined when the partition has an even number of subintervals (i.e., can be expressed as pairs of adjacent subintervals).\n- The anytime property requires that the algorithm can be interrupted at any time before exceeding $N_{\\max}$ and still return the best available trapezoidal approximation. When sufficient refinement exists to form at least one Simpson-rule block (an even number of subintervals), the Simpson approximation should also be produced, updated incrementally, and returned as the best available Simpson estimate. If $N_{\\max}$ is too small to form a Simpson approximation, the algorithm should still return the trapezoidal approximation.\n\nFundamental base to be used in designing the algorithm:\n- The definition of the definite integral as the limit of Riemann sums.\n- The principle of composite rules arising from integrating local polynomial interpolants over each subinterval.\n- The computational cost model in which function evaluations dominate time cost; every evaluation of $f(x)$ at a distinct point consumes one unit of the budget.\n\nAngle unit specification:\n- For any use of the trigonometric function $\\sin(x)$, interpret $x$ in radians.\n\nTest suite:\nImplement and evaluate your anytime algorithm on the following test cases. For each case, compute the composite trapezoidal anytime approximation and the composite Simpson anytime approximation (if available within budget), compare both to the exact integral, and report whether the Simpson approximation has strictly smaller absolute error than the trapezoidal approximation under the given budget (if the Simpson approximation is not available within budget, report that it does not outperform the trapezoidal approximation). The exact integral for each case is known analytically.\n\n- Case $1$: $f_1(x) = \\sin(x)$ on $[a,b] = [0,\\pi]$ with budget $N_{\\max} = 3$. Use radians for $\\sin(x)$.\n- Case $2$: $f_2(x) = \\sin(x)$ on $[a,b] = [0,\\pi]$ with budget $N_{\\max} = 2$. Use radians for $\\sin(x)$.\n- Case $3$: $f_3(x) = e^{x}$ on $[a,b] = [0,1]$ with budget $N_{\\max} = 5$.\n- Case $4$: $f_4(x) = |x - 0.3|$ on $[a,b] = [0,1]$ with budget $N_{\\max} = 9$.\n\nOutput specification:\n- For each case, compute the exact integral $I_{\\text{true}}$ symbolically and the anytime approximations. Let $E_{\\mathrm{trap}} = |I_{\\mathrm{trap}} - I_{\\text{true}}|$ be the absolute error of the trapezoidal approximation and $E_{\\mathrm{simp}} = |I_{\\mathrm{simp}} - I_{\\text{true}}|$ be the absolute error of the Simpson approximation (if available). Output a boolean indicating whether the Simpson approximation strictly outperforms the trapezoidal approximation under the given budget, i.e., output $\\text{True}$ if $E_{\\mathrm{simp}}  E_{\\mathrm{trap}}$ and the Simpson approximation exists within budget; otherwise, output $\\text{False}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$). The results must be in the order of the cases above.\n\nDesign for coverage:\n- The test suite includes a general smooth case where higher-order rules are expected to help (Case $3$), two cases with trigonometric behavior and differing budgets including a minimal boundary budget (Cases $1$ and $2$), and a non-smooth case with a kink inside the interval (Case $4$). This covers the happy path, boundary conditions, and a significant edge case.\n\nYour implementation must be a complete, runnable program that performs the above computations and produces the output in the specified format.",
            "solution": "The user requires the design and implementation of an anytime numerical integration algorithm. The algorithm must incrementally refine an approximation of the definite integral $I = \\int_a^b f(x) dx$ and be capable of providing an estimate at any point up to a specified computational budget $N_{\\max}$, defined as the maximum number of function evaluations. The scheme must simultaneously maintain approximations from the composite trapezoidal rule and the composite Simpson's rule.\n\nThe problem is valid as it is scientifically grounded in the principles of numerical analysis, well-posed with clear objectives and constraints, and formulated objectively.\n\nThe solution is constructed based on a dyadic refinement strategy, which is both computationally efficient and naturally lends itself to recursive updates of the integral approximations.\n\n**1. Algorithmic Framework: Dyadic Refinement**\n\nThe algorithm starts with the coarsest possible partition of the interval $[a,b]$, which consists of a single subinterval ($n=1$), and iteratively refines it by bisecting every existing subinterval. This process of doubling the number of subintervals at each step ($n \\to 2n$) is known as dyadic refinement. At refinement level $k$ (for $k=0, 1, 2, \\dots$), the partition consists of $n = 2^k$ uniform subintervals, each of width $h_k = (b-a)/2^k$.\n\nA key requirement is the efficient use of the computational budget. At each refinement step from $n$ to $2n$ subintervals, we only evaluate the function $f(x)$ at the $n$ new midpoints of the previous subintervals. All previously computed function values at the existing grid points are reused.\n\nThe total number of distinct points evaluated up to refinement level $k$ is $N_k = 2^k + 1$. The algorithm proceeds through levels $k=0, 1, 2, \\dots$ as long as the required number of function evaluations does not exceed the budget $N_{\\max}$.\n\n**2. Composite Trapezoidal Rule and Recursive Update**\n\nThe composite trapezoidal rule approximates the integral by summing the areas of trapezoids over each subinterval. For a partition with $n$ subintervals of width $h = (b-a)/n$, the approximation is:\n$$ T_n = h \\left[ \\frac{1}{2}f(x_0) + \\sum_{i=1}^{n-1} f(x_i) + \\frac{1}{2}f(x_n) \\right] $$\nwhere $x_i = a + i \\cdot h$.\n\nA crucial aspect of our dyadic refinement scheme is the ability to compute $T_{2n}$ efficiently from $T_n$. When we refine from $n$ to $2n$ subintervals, the new step size becomes $h' = h/2$. The new set of grid points includes all the old points plus the midpoints of each of the old subintervals. This leads to the well-known recursive formula:\n$$ T_{2n} = \\frac{1}{2} T_n + h' \\sum_{i=1}^{n} f(x'_{2i-1}) $$\nwhere $h' = (b-a)/(2n)$ is the new step size and the sum is taken over the $n$ newly evaluated midpoints. This formula allows us to update the trapezoidal approximation at each refinement level without re-summing over all points, respecting the principle of reusing computations.\n\nThe anytime trapezoidal approximation, $I_{\\mathrm{trap}}$, is the most recently computed trapezoidal value, $T_{2^k}$, that fits within the budget $N_{\\max}$.\n\n**3. Composite Simpson's Rule via Richardson Extrapolation**\n\nThe composite Simpson's rule is applicable when the number of subintervals, $n$, is even. It is derived by integrating a local quadratic interpolant over pairs of adjacent subintervals. For $n$ subintervals of width $h=(b-a)/n$, the formula is:\n$$ S_n = \\frac{h}{3} \\left[ f(x_0) + 4\\sum_{i=1}^{n/2} f(x_{2i-1}) + 2\\sum_{i=1}^{n/2-1} f(x_{2i}) + f(x_n) \\right] $$\n\nInstead of a direct summation, which would be inefficient in our refinement scheme, we can derive the Simpson's rule approximation from the trapezoidal rule approximations at two successive refinement levels. This relationship is a cornerstone of Romberg integration and can be seen as an application of Richardson extrapolation. The formula is:\n$$ S_{2n} = \\frac{4T_{2n} - T_n}{3} $$\nThis remarkable identity allows us to compute the Simpson's approximation $S_{2n}$ with no additional function evaluations, using only the two most recent trapezoidal approximations, $T_{2n}$ and $T_n$.\n\nThe Simpson's approximation $I_{\\mathrm{simp}}$ is first available at level $k=1$ (when $n=2$ subintervals are formed) and is subsequently updated at each refinement. The anytime Simpson's approximation is the most recently computed value $S_{2^k}$ that fits within the budget. If the budget $N_{\\max}$ is insufficient to complete the $k=1$ refinement (i.e., $N_{\\max}  3$), then $I_{\\mathrm{simp}}$ is not available.\n\n**4. The Anytime Algorithm Implementation**\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization ($k=0$):**\n    - If the budget $N_{\\max}  2$, no approximation is possible. Otherwise, evaluate $f(a)$ and $f(b)$. The number of evaluations is $N=2$.\n    - Compute the initial trapezoidal approximation $T_1 = \\frac{b-a}{2}(f(a)+f(b))$. This is the current best trapezoidal estimate, $I_{\\mathrm{trap}}$.\n    - No Simpson's approximation is available yet.\n\n2.  **Refinement Loop (for $k=1, 2, \\dots$):**\n    - Determine the number of new function evaluations needed for this level: $2^{k-1}$.\n    - If the current number of evaluations plus the new evaluations exceeds $N_{\\max}$, terminate the loop. The current best estimates, $I_{\\mathrm{trap}}$ and $I_{\\mathrm{simp}}$ (if available), are the final results for the given budget.\n    - Otherwise, proceed with the refinement:\n        - Store the current trapezoidal approximation, $T_{2^{k-1}}$, as $T_{\\text{previous}}$.\n        - Evaluate $f(x)$ at the $2^{k-1}$ new midpoints and sum the results into a term $M_k$.\n        - Update the total number of evaluations $N$.\n        - Compute the new trapezoidal approximation $T_{2^k} = \\frac{1}{2}T_{\\text{previous}} + h_k M_k$, where $h_k = (b-a)/2^k$. This becomes the new $I_{\\mathrm{trap}}$.\n        - Compute the new Simpson's approximation $S_{2^k} = \\frac{4T_{2^k} - T_{\\text{previous}}}{3}$. This becomes the new $I_{\\mathrm{simp}}$.\n\n3.  **Final Comparison:**\n    - After the loop terminates due to the budget constraint, calculate the true integral $I_{\\text{true}}$ for the given test case.\n    - Compute the absolute errors $E_{\\mathrm{trap}} = |I_{\\mathrm{trap}} - I_{\\text{true}}|$ and, if a Simpson's approximation was computed, $E_{\\mathrm{simp}} = |I_{\\mathrm{simp}} - I_{\\text{true}}|$.\n    - The final result for the test case is the boolean value of the expression ($I_{\\mathrm{simp}}$ is available AND $E_{\\mathrm{simp}}  E_{\\mathrm{trap}}$).\n\nThis algorithm fulfills all problem requirements: it is anytime, uses dyadic refinement, reuses function evaluations, maintains both trapezoidal and Simpson's approximations, and adheres to the specified computational budget.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs, implements, and evaluates an anytime numerical integration scheme\n    based on composite trapezoidal and Simpson's rules with dyadic refinement.\n    \"\"\"\n\n    def f1(x):\n        return np.sin(x)\n\n    def f2(x):\n        return np.sin(x)\n\n    def f3(x):\n        return np.exp(x)\n\n    def f4(x):\n        return np.abs(x - 0.3)\n\n    test_cases = [\n        # Case 1: sin(x) on [0, pi] with budget N_max = 3\n        {'func': f1, 'a': 0.0, 'b': np.pi, 'N_max': 3, 'I_true': 2.0},\n        # Case 2: sin(x) on [0, pi] with budget N_max = 2\n        {'func': f2, 'a': 0.0, 'b': np.pi, 'N_max': 2, 'I_true': 2.0},\n        # Case 3: exp(x) on [0, 1] with budget N_max = 5\n        {'func': f3, 'a': 0.0, 'b': 1.0, 'N_max': 5, 'I_true': np.e - 1.0},\n        # Case 4: |x - 0.3| on [0, 1] with budget N_max = 9\n        {'func': f4, 'a': 0.0, 'b': 1.0, 'N_max': 9, 'I_true': 0.29},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        f = case['func']\n        a = case['a']\n        b = case['b']\n        N_max = case['N_max']\n        I_true = case['I_true']\n\n        # Anytime algorithm state variables\n        num_evals = 0\n        trap_approx = 0.0\n        simp_approx = None \n        simpson_available = False\n\n        # --- Algorithm Implementation ---\n\n        # Initialization (k=0, n=1)\n        if N_max  2:\n            # Not enough budget for the most basic approximation\n            trap_approx = np.nan\n        else:\n            f_a = f(a)\n            f_b = f(b)\n            num_evals = 2\n            \n            h = b - a\n            T_current = h * (f_a + f_b) / 2.0\n            trap_approx = T_current\n\n        # Refinement loop\n        k = 0\n        while True:\n            k += 1\n            n_prev = 2**(k - 1)\n            num_new_evals = n_prev\n            \n            # Check if budget for next refinement is available\n            if num_evals + num_new_evals  N_max:\n                break\n            \n            # Commit to the refinement\n            T_previous = T_current\n            \n            n_current = 2**k\n            h_current = (b - a) / n_current\n            \n            midpoint_sum = 0.0\n            for i in range(1, n_prev + 1):\n                midpoint_x = a + (2 * i - 1) * h_current\n                midpoint_sum += f(midpoint_x)\n            \n            num_evals += num_new_evals\n            \n            # Update trapezoidal approximation\n            T_current = 0.5 * T_previous + h_current * midpoint_sum\n            trap_approx = T_current\n            \n            # Update Simpson's approximation (now available)\n            simp_approx = (4.0 * T_current - T_previous) / 3.0\n            simpson_available = True\n        \n        # --- End of Algorithm ---\n\n        # Final comparison based on the problem statement\n        if simpson_available:\n            E_trap = np.abs(trap_approx - I_true)\n            E_simp = np.abs(simp_approx - I_true)\n            \n            # Output True if Simpson's rule is strictly better\n            result = E_simp  E_trap\n        else:\n            # Simpson's was not available within budget\n            result = False\n        \n        results.append(result)\n\n    # Format the final output exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}