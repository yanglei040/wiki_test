{
    "hands_on_practices": [
        {
            "introduction": "Solving the equation $x = \\cos(x)$ is a classic introduction to fixed-point iteration. This hands-on exercise guides you through implementing the simple iteration $x_{k+1} = \\cos(x_k)$ and connecting its observed behavior to the formal guarantees of the Contraction Mapping Theorem. You will not only verify convergence but also analyze the interesting oscillatory pattern of the sequence, deepening your understanding of how iterates approach a fixed point .",
            "id": "3231288",
            "problem": "Consider the fixed-point equation $x=\\cos x$ with the fixed-point iteration $x_{k+1}=\\cos x_k$ defined on the closed interval $[0,1]$. Use the following fundamental bases: the definition of a fixed point $x^\\star$ satisfying $x^\\star=g(x^\\star)$ for a function $g$, the Mean Value Theorem (MVT), and the Contraction Mapping Theorem (CMT), also known as the Banach Fixed-Point Theorem. The iteration uses the angle unit in radians. Your program must implement the iteration and verify convergence and monotonicity properties for a specified test suite.\n\nTasks:\n- Starting from the fundamental definitions, justify why the function $g(x)=\\cos x$ maps $[0,1]$ into itself. Use the derivative $g'(x)=-\\sin x$ and the bound $\\sup_{x\\in[0,1]}|\\sin x|=\\sin(1)$ with $\\sin(1)1$ to argue that $g$ is a contraction on $[0,1]$ and therefore admits a unique fixed point $x^\\star\\in[0,1]$ to which the sequence $\\{x_k\\}$ converges for any $x_0\\in[0,1]$.\n- Analyze the monotone behavior of the iterates. Because $g$ is strictly decreasing on $[0,1]$, the full sequence $\\{x_k\\}$ is generally not monotone. However, demonstrate that the even subsequence $\\{x_{2k}\\}$ and the odd subsequence $\\{x_{2k+1}\\}$ are each monotone and converge to the same limit $x^\\star$.\n- Implement a program that, for each initial value $x_0$ in the provided test suite, performs fixed-point iteration $x_{k+1}=\\cos x_k$ until either the successive-iterate difference satisfies $|x_{k+1}-x_k|10^{-12}$ or a maximum of $1000$ iterations is reached. Store the full sequence of iterates for each test case. The angle unit must be radians.\n- For each test case, compute and report:\n  1. The initial value $x_0$.\n  2. The final approximation of the fixed point after stopping, denoted by $x_{\\text{approx}}$.\n  3. The number of iterations $N$ performed.\n  4. A boolean indicating whether the full sequence $\\{x_k\\}_{k=0}^N$ is monotone (nondecreasing or nonincreasing).\n  5. A boolean indicating whether the even-indexed subsequence $\\{x_{2k}\\}$ is monotone (nondecreasing or nonincreasing).\n  6. A boolean indicating whether the odd-indexed subsequence $\\{x_{2k+1}\\}$ is monotone (nondecreasing or nonincreasing).\n- Use the following test suite of initial values (all in radians): $x_0\\in\\{0,\\,\\tfrac{1}{2},\\,1,\\,0.7390851332151607\\}$. The last value is a high-precision approximation to the unique fixed point $x^\\star$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the exact form $[x_0,x_{\\text{approx}},N,\\text{is\\_monotone\\_full},\\text{is\\_monotone\\_even},\\text{is\\_monotone\\_odd}]$. For example, the overall output will look like $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ with four inner lists corresponding to the four test cases.\n\nAll numerical values mentioned above must be treated as scalars in your program. The program must not read any inputs and must print the single required line as its only output.",
            "solution": "We first recall the fundamental definitions and facts. A fixed point of a function $g$ is a value $x^\\star$ satisfying $x^\\star=g(x^\\star)$. A numerical fixed-point iteration constructs a sequence $\\{x_k\\}$ by $x_{k+1}=g(x_k)$ and seeks a limit $x^\\star$ such that $x_k\\to x^\\star$ and $x^\\star=g(x^\\star)$. The Contraction Mapping Theorem (CMT) states that if $g$ maps a complete metric space into itself and satisfies a Lipschitz bound $|g(x)-g(y)|\\le L|x-y|$ for all $x,y$ in the domain with a constant $L1$, then $g$ has a unique fixed point $x^\\star$ and for any initialization $x_0$ in the domain the iteration $x_{k+1}=g(x_k)$ converges to $x^\\star$.\n\nWe analyze $g(x)=\\cos x$ on the closed interval $[0,1]$. First, we show that $g$ maps $[0,1]$ into itself. For $x\\in[0,1]$, we have $\\cos x\\in[\\cos 1,\\,\\cos 0]=[\\cos 1,\\,1]$. Since $\\cos 10$, it follows that $g([0,1])\\subset[0,1]$. Next, we compute the derivative $g'(x)=-\\sin x$. On $[0,1]$, the function $\\sin x$ is nonnegative and bounded above by $\\sin 1$, so\n$$\n\\sup_{x\\in[0,1]}|g'(x)|=\\sup_{x\\in[0,1]}|\\sin x|=\\sin(1).\n$$\nBecause $\\sin(1)1$, we can use the Mean Value Theorem (MVT) to derive a Lipschitz bound: for any $x,y\\in[0,1]$,\n$$\n|g(x)-g(y)|=|\\cos x-\\cos y|=|g'(\\xi)|\\,|x-y|\\le \\sin(1)\\,|x-y|,\n$$\nfor some $\\xi$ between $x$ and $y$. Hence $g$ is a contraction on $[0,1]$ with contraction constant $L=\\sin(1)1$. By the Contraction Mapping Theorem, there exists a unique fixed point $x^\\star\\in[0,1]$, and for any $x_0\\in[0,1]$, the sequence defined by $x_{k+1}=\\cos x_k$ converges to $x^\\star$.\n\nWe next analyze monotone behavior. Note that $g$ is strictly decreasing on $[0,1]$ because $g'(x)=-\\sin x\\le 0$ and for $x\\in(0,1]$ we have $\\sin x0$. Let $x^\\star$ be the unique fixed point. Consider the error $e_k=x_k-x^\\star$. Using the MVT, there exists $\\xi_k$ between $x_k$ and $x^\\star$ such that\n$$\ne_{k+1}=x_{k+1}-x^\\star=g(x_k)-g(x^\\star)=g'(\\xi_k)\\,(x_k-x^\\star)=-\\sin(\\xi_k)\\,e_k.\n$$\nOn $[0,1]$, $\\sin(\\xi_k)\\in[0,\\sin(1)]$ and the sign of $e_{k+1}$ is the negative of the sign of $e_k$ whenever $\\sin(\\xi_k)0$. Therefore, unless $e_k=0$, the error alternates in sign, so the full sequence $\\{x_k\\}$ generally oscillates and is not monotone. However, we can infer monotone behavior of subsequences. Because $g$ is decreasing, we have for any $x\\in[0,1]$, $xx^\\star$ implies $g(x)g(x^\\star)=x^\\star$, and $xx^\\star$ implies $g(x)x^\\star$. Starting with any $x_0\\in[0,1]$, if $x_0x^\\star$ then $x_1 x^\\star$, and then $x_2=g(x_1)g(x^\\star)=x^\\star$, thus $x_2x^\\star$. The even subsequence $\\{x_{2k}\\}$ remains below $x^\\star$ and, using the contraction property,\n$$\n|x_{2k}-x^\\star|\\le L^2 |x_{2k-2}-x^\\star|,\n$$\nso $\\{x_{2k}\\}$ is monotone increasing towards $x^\\star$ (its terms get closer to $x^\\star$ from below). Symmetrically, the odd subsequence $\\{x_{2k+1}\\}$ remains above $x^\\star$ and is monotone decreasing towards $x^\\star$. If $x_0x^\\star$, the roles of the subsequences reverse, but the same alternating monotone convergence holds. In the special case $x_0=x^\\star$, the sequence is constant and trivially monotone. The contraction bound also yields a linear convergence rate:\n$$\n|x_{k}-x^\\star|\\le L^k |x_0-x^\\star|,\n$$\nwith $L=\\sin(1)$.\n\nAlgorithm design for the program:\n- For each test case initial value $x_0$, compute iterates $x_{k+1}=\\cos x_k$ (with angles in radians) until $|x_{k+1}-x_k|10^{-12}$ or a maximum of $1000$ iterations is reached. Store all iterates to check monotonicity.\n- Determine if the full sequence is monotone by checking whether all consecutive differences are nonnegative (nondecreasing) or all are nonpositive (nonincreasing), allowing equality. Determine monotonicity for the even subsequence $\\{x_{2k}\\}$ and the odd subsequence $\\{x_{2k+1}\\}$ similarly.\n- Report for each test case the list $[x_0,x_{\\text{approx}},N,\\text{is\\_monotone\\_full},\\text{is\\_monotone\\_even},\\text{is\\_monotone\\_odd}]$, where $x_{\\text{approx}}$ is the last iterate and $N$ the number of iteration steps performed.\n- Use the test suite $x_0\\in\\{0,\\,\\tfrac{1}{2},\\,1,\\,0.7390851332151607\\}$.\n\nThe contraction constant $L=\\sin(1)$ validates both the uniqueness of the fixed point and the convergence claim. The numerical results will reflect the theoretical monotone behavior: the full sequence generally oscillates but the even and odd subsequences are monotone and both converge to the same limit $x^\\star$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fixed_point_cos(x0, tol=1e-12, max_iter=1000):\n    \"\"\"\n    Perform fixed-point iteration x_{k+1} = cos(x_k) starting from x0.\n    Angles are in radians.\n    Returns:\n        x_approx: final approximation\n        iterations: number of iterations performed\n        seq: list of iterates including the initial value\n    \"\"\"\n    seq = [float(x0)]\n    x_prev = float(x0)\n    iterations = 0\n    for k in range(max_iter):\n        x_next = float(np.cos(x_prev))\n        seq.append(x_next)\n        iterations += 1\n        if abs(x_next - x_prev)  tol:\n            break\n        x_prev = x_next\n    return seq[-1], iterations, seq\n\ndef is_monotone(sequence, tol=0.0):\n    \"\"\"\n    Check if a sequence is monotone nondecreasing or monotone nonincreasing.\n    Equality is allowed.\n    tol can be used to soften comparisons, but defaults to strict.\n    \"\"\"\n    if len(sequence) = 1:\n        return True\n    diffs = [sequence[i+1] - sequence[i] for i in range(len(sequence)-1)]\n    nondecreasing = all(d = -tol for d in diffs)\n    nonincreasing = all(d = tol for d in diffs)\n    return nondecreasing or nonincreasing\n\ndef solve():\n    # Define the test cases from the problem statement (radians).\n    test_cases = [\n        0.0,\n        0.5,\n        1.0,\n        0.7390851332151607,  # high-precision approximation to the fixed point\n    ]\n\n    results = []\n    for x0 in test_cases:\n        x_approx, iters, seq = fixed_point_cos(x0, tol=1e-12, max_iter=1000)\n        # Full sequence monotonicity\n        mono_full = is_monotone(seq, tol=0.0)\n        # Even-indexed subsequence: indices 0,2,4,...\n        even_seq = seq[0::2]\n        mono_even = is_monotone(even_seq, tol=0.0)\n        # Odd-indexed subsequence: indices 1,3,5,...\n        odd_seq = seq[1::2]\n        mono_odd = is_monotone(odd_seq, tol=0.0)\n        # Assemble result for this test case\n        results.append([x0, x_approx, iters, mono_full, mono_even, mono_odd])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A convergent fixed-point iteration is not always a practical one if it converges too slowly. This practice explores this issue by constructing a family of iterations whose convergence rate, determined by a contraction constant $L$, can be made arbitrarily slow as $L$ approaches $1$. You will then implement Aitken’s delta-squared process, a powerful acceleration technique, and quantify the dramatic speedup it provides, especially in these challenging cases .",
            "id": "3130626",
            "problem": "Consider one-dimensional fixed-point iteration on the real line defined by a mapping $g:\\mathbb{R}\\to\\mathbb{R}$ that is a contraction with Lipschitz constant $L\\in[0,1)$. Use the fundamental definition of a contraction mapping: for all $x,y\\in\\mathbb{R}$, $|g(x)-g(y)|\\le L|x-y|$, and the definition of fixed-point iteration $x_{k+1}=g(x_k)$. Your goal is to construct and analyze a family of mappings that exhibit slow linear convergence when the contraction constant $L$ is close to $1$, and then to apply a classical acceleration technique to quantify the improvement.\n\nWork with the affine family\n$g(x)=L\\,x+(1-L)\\,c,$\nwhere $c\\in\\mathbb{R}$ is a known constant. This family is a contraction whenever $L\\in[0,1)$, and its fixed point is the value $x^\\star$ satisfying $x^\\star=g(x^\\star)$.\n\nStarting from the core definitions only, perform the following tasks.\n\n- Task A (plain fixed-point iteration): For a given $L\\in[0,1)$, fixed point target $c\\in\\mathbb{R}$, initial guess $x_0\\in\\mathbb{R}$, and tolerance $\\varepsilon0$, determine the minimal number of evaluations of $g$ required by the unaccelerated fixed-point iteration $x_{k+1}=g(x_k)$ to produce an iterate $x_k$ such that $|x_k-c|\\le\\varepsilon$. Count one evaluation of $g$ per iteration. Use only quantities that are directly defined by the problem data.\n\n- Task B (acceleration): Apply a standard sequence acceleration to the fixed-point iteration, specifically Aitken’s delta-squared process (also known as Steffensen’s acceleration when applied to fixed-point iteration). Use the classical three-point transform to turn a triple $(x_k,x_{k+1},x_{k+2})$ produced by successive applications of $g$ into an accelerated estimate $\\hat{x}_k$. Count the number of evaluations of $g$ needed to obtain an accelerated estimate that satisfies $|\\hat{x}_k-c|\\le\\varepsilon$, where each accelerated estimate requires two evaluations of $g$ beyond the current $x_k$ to form $(x_{k+1},x_{k+2})$. If numerical degeneracy prevents applying the transform at some step, continue generating the next triple as needed.\n\n- Task C (quantify improvement): For each case, compute the speedup as the ratio of the number of evaluations required by plain fixed-point iteration to the number required by the accelerated method. Express this speedup as a decimal.\n\nDesign your program to solve the following test suite. In all cases, use the same tolerance $\\varepsilon=10^{-8}$, initial guess $x_0=c+1$, and fixed-point target $c=3$.\n\n- Case $1$: $L=0.9$.\n- Case $2$: $L=0.99$.\n- Case $3$: $L=0.0$ (boundary case).\n- Case $4$: $L=0.9999$ (near-boundary, very slow plain convergence).\n\nYour program must output a single line consisting of a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of the form $[N_{\\text{plain}},N_{\\text{acc}},S]$:\n- $N_{\\text{plain}}$ is the minimal number of evaluations of $g$ needed by plain fixed-point iteration to achieve $|x_k-c|\\le\\varepsilon$.\n- $N_{\\text{acc}}$ is the number of evaluations of $g$ needed by the Aitken-accelerated scheme to achieve $|\\hat{x}_k-c|\\le\\varepsilon$.\n- $S$ is the speedup $N_{\\text{plain}}/N_{\\text{acc}}$ expressed as a decimal.\n\nAs an example of the exact output format, the program should print a single line like $[[1,2,0.5],[\\dots],[\\dots],[\\dots]]$ with no spaces.\n\nNo physical units or angles are involved in this problem. All requested outputs are integers or decimals as specified.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the mathematical theory of fixed-point iteration and sequence acceleration, is well-posed with all necessary data provided, and is formulated using objective, unambiguous language. The problem is a standard exercise in introductory computational science and meets all criteria for a valid problem.\n\nWe proceed with the solution, which is divided into an analysis of the plain fixed-point iteration (Task A), an analysis of the accelerated iteration (Task B), and the calculation of the speedup (Task C).\n\nThe fixed-point iteration is defined by the mapping $g(x) = L x + (1-L)c$, where $L \\in [0,1)$, and the iterative process is $x_{k+1} = g(x_k)$. The fixed point $x^\\star$ of this iteration is the solution to $x^\\star = g(x^\\star)$.\n$$x^\\star = L x^\\star + (1-L)c$$\n$$x^\\star (1-L) = (1-L)c$$\nSince $L \\in [0,1)$, the term $(1-L)$ is non-zero, allowing us to divide by it.\n$$x^\\star = c$$\nThis confirms that the constant $c$ is the unique fixed point of the mapping $g(x)$.\n\n### Task A: Plain Fixed-Point Iteration\n\nWe analyze the convergence of the sequence $x_k$ to the fixed point $x^\\star = c$. Let the error at iteration $k$ be $e_k = x_k - c$.\n$$x_{k+1} - c = g(x_k) - c$$\n$$x_{k+1} - c = (L x_k + (1-L)c) - c$$\n$$x_{k+1} - c = L x_k - Lc$$\n$$x_{k+1} - c = L(x_k - c)$$\nThis gives the error recurrence relation $e_{k+1} = L e_k$. By induction, the error at iteration $k$ is related to the initial error $e_0 = x_0 - c$:\n$$e_k = L^k e_0$$\nThe condition for termination is $|x_k - c| \\le \\varepsilon$, which in terms of the error is $|e_k| \\le \\varepsilon$.\n$$|L^k e_0| \\le \\varepsilon$$\n$$|L|^k |x_0 - c| \\le \\varepsilon$$\nGiven that $L \\in [0,1)$, we have $|L| = L$. The problem specifies $x_0 = c+1$, so $|x_0 - c| = |(c+1) - c| = 1$. The condition simplifies to:\n$$L^k \\le \\varepsilon$$\nWe must find the minimal integer $k \\ge 1$ that satisfies this inequality. The number of evaluations of $g$ to obtain $x_k$ is exactly $k$. We denote this by $N_{\\text{plain}}$. The initial state $x_0$ is given, so no evaluations are needed for $k=0$. Since $|x_0-c|=1$ and $\\varepsilon=10^{-8}$, the condition is not met at $k=0$.\n\nWe must consider two sub-cases for $L$.\n\nCase $L=0$:\nThe mapping is $g(x) = (1-0)c = c$. The first iterate is $x_1 = g(x_0) = c$.\nThe error is $|x_1 - c| = |c-c|=0$. Since $0 \\le \\varepsilon$, the condition is met after one iteration.\nThus, for $L=0$, $N_{\\text{plain}} = 1$.\n\nCase $L \\in (0,1)$:\nWe solve the inequality $L^k \\le \\varepsilon$ by taking the natural logarithm of both sides.\n$$k \\ln(L) \\le \\ln(\\varepsilon)$$\nSince $L \\in (0,1)$, its logarithm $\\ln(L)$ is negative. Dividing by a negative number reverses the inequality sign:\n$$k \\ge \\frac{\\ln(\\varepsilon)}{\\ln(L)}$$\nThe minimal integer $k$ is the ceiling of this expression.\n$$N_{\\text{plain}} = \\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln(L)} \\right\\rceil$$\n\n### Task B: Accelerated Iteration\n\nAitken's delta-squared process generates an accelerated sequence $\\hat{x}_k$ from a given sequence $x_k$. The formula for the accelerated estimate is:\n$$\\hat{x}_k = x_k - \\frac{(x_{k+1} - x_k)^2}{x_{k+2} - 2x_{k+1} + x_k}$$\nTo compute the first accelerated term $\\hat{x}_0$, we need the triple $(x_0, x_1, x_2)$. This requires two evaluations of $g$: $x_1 = g(x_0)$ and $x_2 = g(x_1)$. To compute $\\hat{x}_k$ requires producing the sequence up to $x_{k+2}$, which takes $k+2$ evaluations of $g$. We seek the minimal number of total evaluations, $N_{\\text{acc}}$, to find an estimate $\\hat{x}_k$ such that $|\\hat{x}_k-c| \\le \\varepsilon$.\n\nLet us analyze the performance of Aitken's method on the affine sequence $x_k = c + L^k(x_0-c)$.\nThe differences are:\n$$x_{k+1} - x_k = (c + L^{k+1}(x_0-c)) - (c + L^k(x_0-c)) = (L^{k+1} - L^k)(x_0-c) = L^k(L-1)(x_0-c)$$\nThe denominator is the second-order difference:\n$$x_{k+2} - 2x_{k+1} + x_k = (x_{k+2}-x_{k+1}) - (x_{k+1}-x_k)$$\n$$= L^{k+1}(L-1)(x_0-c) - L^k(L-1)(x_0-c) = (L^{k+1}-L^k)(L-1)(x_0-c)$$\n$$= L^k(L-1)(L-1)(x_0-c) = L^k(L-1)^2(x_0-c)$$\nThe denominator is non-zero as long as $L \\neq 0$, $L \\neq 1$, and $x_0 \\neq c$. The problem constraints $L \\in [0,1)$ and $x_0 = c+1$ ensure this for $L \\in (0,1)$.\n\nThe correction term in Aitken's formula becomes:\n$$\\frac{(x_{k+1} - x_k)^2}{x_{k+2} - 2x_{k+1} + x_k} = \\frac{\\left[L^k(L-1)(x_0-c)\\right]^2}{L^k(L-1)^2(x_0-c)} = \\frac{L^{2k}(L-1)^2(x_0-c)^2}{L^k(L-1)^2(x_0-c)} = L^k(x_0-c)$$\nSubstituting this back into the formula for $\\hat{x}_k$:\n$$\\hat{x}_k = x_k - L^k(x_0-c) = (c + L^k(x_0-c)) - L^k(x_0-c) = c$$\nThis remarkable result shows that for an affine iteration, Aitken's method produces the exact fixed point $c$ in a single step, provided the denominator is non-zero.\n\nFor all $L \\in (0,1)$, the denominator for $\\hat{x}_0$ is $(L-1)^2(x_0-c) \\neq 0$. Thus, the first accelerated estimate $\\hat{x}_0$ is equal to $c$. The error $|\\hat{x}_0 - c| = 0$, which is less than or equal to any positive $\\varepsilon$. To compute $\\hat{x}_0$, we need $x_1$ and $x_2$, which requires 2 evaluations of $g$. Therefore, for all $L \\in (0,1)$, $N_{\\text{acc}} = 2$.\n\nFor the boundary case $L=0$, the sequence is $x_0=c+1, x_1=c, x_2=c, \\dots$.\nLet's calculate $\\hat{x}_0$:\n$$x_1 - x_0 = c - (c+1) = -1$$\n$$x_2 - 2x_1 + x_0 = c - 2c + (c+1) = 1$$\nThe denominator is non-zero.\n$$\\hat{x}_0 = x_0 - \\frac{(x_1 - x_0)^2}{x_2 - 2x_1 + x_0} = (c+1) - \\frac{(-1)^2}{1} = c+1 - 1 = c$$\nAgain, the exact fixed point is found. This requires computing $x_1=g(x_0)$ and $x_2=g(x_1)$, for a total of $2$ evaluations. So, for $L=0$, $N_{\\text{acc}} = 2$.\n\nIn conclusion, for all test cases, $N_{\\text{acc}} = 2$.\n\n### Task C: Quantify Improvement\n\nThe speedup $S$ is the ratio of the number of evaluations required by the plain method to that required by the accelerated method:\n$$S = \\frac{N_{\\text{plain}}}{N_{\\text{acc}}}$$\nUsing our derived results, this becomes:\n$$S = \\frac{N_{\\text{plain}}}{2}$$\n\n### Calculations for Test Cases\n- Shared parameters: $c=3$, $x_0 = c+1 = 4$, $\\varepsilon=10^{-8}$.\n- $\\ln(\\varepsilon) = \\ln(10^{-8}) = -8 \\ln(10) \\approx -18.42068$.\n\nCase 1: $L=0.9$\n$N_{\\text{plain}} = \\lceil \\frac{\\ln(10^{-8})}{\\ln(0.9)} \\rceil = \\lceil \\frac{-18.42068}{-0.10536} \\rceil = \\lceil 174.83 \\rceil = 175$.\n$N_{\\text{acc}} = 2$.\n$S = 175 / 2 = 87.5$.\n\nCase 2: $L=0.99$\n$N_{\\text{plain}} = \\lceil \\frac{\\ln(10^{-8})}{\\ln(0.99)} \\rceil = \\lceil \\frac{-18.42068}{-0.01005} \\rceil = \\lceil 1832.85 \\rceil = 1833$.\n$N_{\\text{acc}} = 2$.\n$S = 1833 / 2 = 916.5$.\n\nCase 3: $L=0.0$\n$N_{\\text{plain}} = 1$.\n$N_{\\text{acc}} = 2$.\n$S = 1 / 2 = 0.5$.\n\nCase 4: $L=0.9999$\n$N_{\\text{plain}} = \\lceil \\frac{\\ln(10^{-8})}{\\ln(0.9999)} \\rceil = \\lceil \\frac{-18.42068}{-0.000100005} \\rceil = \\lceil 184201.3 \\rceil = 184202$.\n$N_{\\text{acc}} = 2$.\n$S = 184202 / 2 = 92101.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the fixed-point iteration problem for a given test suite.\n\n    The problem analyzes an affine fixed-point iteration g(x) = L*x + (1-L)*c\n    with fixed point c. It compares the number of function evaluations\n    required for convergence by a plain iteration versus an Aitken-accelerated\n    iteration.\n    \"\"\"\n\n    # Define the shared parameters from the problem statement.\n    c = 3.0\n    x0 = c + 1.0\n    eps = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        0.9,      # Case 1\n        0.99,     # Case 2\n        0.0,      # Case 3\n        0.9999,   # Case 4\n    ]\n\n    def get_n_plain(L: float, x0: float, c: float, eps: float) - int:\n        \"\"\"\n        Calculates the minimal number of evaluations for plain fixed-point iteration.\n\n        The error e_k = x_k - c follows e_k = L^k * e_0.\n        The condition is |e_k| = eps, which means L^k * |x0 - c| = eps.\n        Given |x0 - c| = 1, this simplifies to L^k = eps.\n        \"\"\"\n        # Check if already converged (not the case for the given problem data)\n        if abs(x0 - c) = eps:\n            return 0\n        \n        # Handle the boundary case L=0 separately.\n        # x_1 = g(x_0) = (1-0)*c = c. Convergence in 1 step.\n        if L == 0.0:\n            return 1\n        \n        # For L in (0, 1), solve L^k = eps for k.\n        # k * log(L) = log(eps) - k = log(eps) / log(L)\n        # We need the smallest integer k.\n        num_iterations = np.ceil(np.log(eps) / np.log(L))\n        return int(num_iterations)\n\n    def get_n_acc(L: float, x0: float, c: float, eps: float) - int:\n        \"\"\"\n        Calculates the minimal number of evaluations for Aitken-accelerated iteration.\n        \n        For an affine iteration g(x) = L*x + (1-L)*c, Aitken's method is known\n        to converge to the exact fixed point with the first accelerated estimate,\n        x_hat_0. Calculation of x_hat_0 requires the triple (x0, x1, x2).\n        \n        x1 = g(x0)  (1st evaluation)\n        x2 = g(x1)  (2nd evaluation)\n        \n        This holds for all L in [0, 1) as long as the denominator of Aitken's\n        formula is non-zero, which is true for the given test cases.\n        \"\"\"\n        # Check trivial convergence\n        if abs(x0 - c) = eps:\n            return 0\n            \n        # As derived in the solution, Aitken's method requires 2 evaluations\n        # to compute x_hat_0, which gives the exact solution c.\n        return 2\n\n    results = []\n    for L_val in test_cases:\n        N_plain = get_n_plain(L_val, x0, c, eps)\n        N_acc = get_n_acc(L_val, x0, c, eps)\n        \n        # Speedup is the ratio of evaluations.\n        S = N_plain / N_acc\n        \n        results.append([N_plain, N_acc, S])\n\n    # Format the output string exactly as specified in the problem,\n    # constructing it part by part to avoid extra spaces from str(list).\n    inner_parts = []\n    for res in results:\n        # For case L=0.9999, S is an integer value, but problem asks for decimal.\n        # Standard float formatting handles this.\n        inner_parts.append(f'[{res[0]},{res[1]},{res[2]}]')\n    \n    final_output = f\"[{','.join(inner_parts)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "What happens when a fixed-point iteration does not converge to a single point but instead becomes trapped in a cycle? This exercise tackles one of the most common failure modes by having you work with a function specifically designed to create a stable two-cycle. Through this practice, you will learn how to programmatically detect such cycles and then apply a relaxation method to break the oscillatory behavior, successfully guiding the iteration to the true fixed point .",
            "id": "3130666",
            "problem": "You will design and analyze a deterministic fixed-point iteration and implement a runnable program that detects and avoids convergence to nontrivial cycles. The foundational base consists of the definition of a fixed point, the definition of a two-cycle, and the principle that a contraction mapping brings iterates closer by a factor bounded by a Lipschitz constant strictly less than $1$. You will work with a specific piecewise linear mapping and verify its behavior under iteration both theoretically and computationally.\n\nDefine a piecewise linear function $g:\\mathbb{R}\\to\\mathbb{R}$ by three affine branches with parameters $a=0.3$, $b=0.7$, and slope parameter $k=0.4$ outside the middle interval:\n$$\ng(x)=\n\\begin{cases}\na - k\\,(a - x),  x  a,\\\\\na + b - x,  a \\le x \\le b,\\\\\nb + k\\,(x - b),  x  b.\n\\end{cases}\n$$\nThe middle branch is a reflection across the midpoint $c=\\frac{a+b}{2}$, so for all $x$ with $a \\le x \\le b$ one has $g(g(x))=x$. The outer branches are strict contractions toward $a$ from below and toward $b$ from above.\n\nTask A (construction): Implement $g(x)$ exactly as defined above.\n\nTask B (iteration and detection): For a given initial condition $x_0$, iterate the fixed-point scheme $x_{n+1} = g(x_n)$ with a maximum of $N=200$ iterations and a tolerance $\\varepsilon=10^{-10}$. Devise and implement principled tests to determine, during iteration, whether the sequence\n$$\nx_0,\\; x_1=g(x_0),\\; x_2=g(x_1),\\;\\dots\n$$\nhas:\n- converged to a fixed point (that is, an $x_\\star$ with $x_\\star=g(x_\\star)$ within the tolerance), or\n- fallen into a cycle of period $2$ (that is, $x_{n+1}$ and $x_{n-1}$ agree within the tolerance while not being within the tolerance of a fixed point), or\n- neither within the iteration budget.\n\nYour cycle test must be designed to reliably distinguish an actual two-cycle from ordinary fixed-point convergence in the outer branches. You may additionally include a generic cycle detector that is applicable to any period.\n\nTask C (avoidance strategy): If a cycle of period $2$ is detected, replace the plain iteration with a relaxed iteration that provably breaks the two-cycle and converges to a fixed point. Use the affine relaxation\n$$\nT_r(x) = (1-r)\\,x + r\\,g(x),\n$$\nwith relaxation parameter $r=0.5$, and iterate $x_{n+1} = T_r(x_n)$ until convergence or until the iteration budget is exhausted. Explain why this strategy avoids the two-cycle in the middle branch.\n\nTest suite and output specification:\n- Use the following five initial conditions:\n  - $x_0=0.4$,\n  - $x_0=0.5$,\n  - $x_0=0.05$,\n  - $x_0=0.95$,\n  - $x_0=0.3$.\n- Use $N=200$ and $\\varepsilon=10^{-10}$, and $r=0.5$ as specified above.\n- For each initial condition, your program must compute three values:\n  1. an integer $0$-$1$ indicator for whether a two-cycle was detected during the plain iteration (before any relaxation is applied),\n  2. an integer $0$-$1$ indicator for whether the overall procedure (plain iteration possibly followed by relaxation) returned a converged fixed point within the iteration budget,\n  3. the final approximation to the fixed point, rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing all results in a single, flat list of numbers in the order of the test suite, concatenating the triples for each test case. For example, the format must be\n$[d_1,c_1,v_1,d_2,c_2,v_2,\\dots,d_5,c_5,v_5]$\nwhere each $d_i$ and $c_i$ is an integer and each $v_i$ is a float rounded to six decimals. No other text should be printed.",
            "solution": "The problem is well-defined, scientifically sound, and internally consistent. All necessary parameters and definitions for a deterministic fixed-point iteration analysis are provided. We may therefore proceed with the solution.\n\n### Part 1: Analysis of the Function and Iteration\n\nThe core of the problem is the piecewise linear function $g:\\mathbb{R}\\to\\mathbb{R}$ defined with parameters $a=0.3$, $b=0.7$, and $k=0.4$:\n$$\ng(x)=\n\\begin{cases}\na - k\\,(a - x) = (1-k)a + kx,  x  a \\\\\na + b - x,  a \\le x \\le b \\\\\nb + k\\,(x - b) = (1-k)b + kx,  x  b\n\\end{cases}\n$$\nSubstituting the given parameters:\n$$\ng(x)=\n\\begin{cases}\n0.18 + 0.4x,  x  0.3 \\\\\n1.0 - x,  0.3 \\le x \\le 0.7 \\\\\n0.42 + 0.4x,  x  0.7\n\\end{cases}\n$$\nThis function is continuous at the boundaries $x=a$ and $x=b$.\n\n**Fixed Points:**\nA fixed point $x_\\star$ satisfies the equation $x_\\star = g(x_\\star)$.\n- For $x  a$: $x = (1-k)a + kx \\implies x(1-k) = a(1-k)$. Since $k=0.4 \\neq 1$, this gives $x=a$. This is outside the domain $x  a$, so there is no fixed point in this branch.\n- For $x  b$: $x = (1-k)b + kx \\implies x(1-k) = b(1-k)$. Since $k \\neq 1$, this gives $x=b$. This is outside the domain $x  b$, so there is no fixed point in this branch.\n- For $a \\le x \\le b$: $x = a+b-x \\implies 2x = a+b \\implies x = \\frac{a+b}{2}$. Let's denote this point as $c$. With $a=0.3$ and $b=0.7$, we have $c = \\frac{0.3+0.7}{2} = 0.5$. This point lies within the interval $[0.3, 0.7]$.\nTherefore, the function $g(x)$ has a single true fixed point at $x_\\star = c = 0.5$.\n\n**Iteration Dynamics and Stability:**\nThe behavior of the fixed-point iteration $x_{n+1} = g(x_n)$ is determined by the local derivative $g'(x)$.\n- For $x  a$ and $x  b$, the derivative is $g'(x) = k = 0.4$. Since $|g'(x)|  1$, the iteration is a strict contraction in these outer regions. An initial guess $x_0  a$ will generate a sequence that converges monotonically towards $a$. Similarly, an initial guess $x_0  b$ will generate a sequence that converges monotonically towards $b$. Although $a$ and $b$ are not true fixed points (as $g(a)=b$ and $g(b)=a$), the convergence criterion specified in the problem, $|x_{n+1} - x_n|  \\varepsilon$, will be satisfied as $x_n$ approaches $a$ or $b$. For instance, if $x_n \\to a^-$, then $|x_{n+1} - x_n| = |(g(x_n)-a) - (x_n-a)| = |k(x_n-a) - (x_n-a)| = |x_n-a|(1-k) \\to 0$.\n- For $a \\le x \\le b$, the derivative is $g'(x) = -1$.\n  - If an iterate $x_n$ is in this interval (and $x_n \\ne c$), then $x_{n+1} = a+b-x_n$. The next iterate is $x_{n+2} = g(x_{n+1}) = a+b-x_{n+1} = a+b-(a+b-x_n) = x_n$.\n  - This creates a period-$2$ cycle $\\{x_n, x_{n+1}\\}$. This includes the specific case $\\{a,b\\}$, which is entered if $x_n=a$ or $x_n=b$.\n\n### Part 2: Algorithmic Design\n\n**Task B: Iteration and Detection**\nWe design an algorithm that executes the plain iteration $x_{n+1} = g(x_n)$ while monitoring for two termination conditions within a budget of $N=200$ iterations.\n1.  **Fixed-Point Convergence:** At each step $n$, we compute $x_n = g(x_{n-1})$ and check if $|x_n - x_{n-1}|  \\varepsilon$, where $\\varepsilon=10^{-10}$. If this condition is met, the iteration has converged, and the final value is $x_n$.\n2.  **Two-Cycle Detection:** If the fixed-point test fails, we check for a period-$2$ cycle. This requires storing at least the two previous iterates, $x_{n-1}$ and $x_{n-2}$. A two-cycle is detected if $|x_n - x_{n-2}|  \\varepsilon$. To distinguish this from fixed-point convergence, we must also ensure that the fixed-point condition is not met, i.e., $|x_n - x_{n-1}| \\ge \\varepsilon$. This logic is implemented by checking for the fixed point first; if that test fails, the two-cycle test is then performed.\n\n**Task C: Cycle Avoidance Strategy**\nIf a period-$2$ cycle is detected, the algorithm switches from the plain iteration to a relaxed iteration defined by the operator $T_r(x) = (1-r)x + r g(x)$, with relaxation parameter $r=0.5$. The new iteration is $x_{n+1} = T_{0.5}(x_n)$.\n\nThe justification for this strategy is as follows:\nA period-$2$ cycle can only occur for iterates within the interval $[a, b]$. For any $x \\in [a, b]$, we have $g(x) = a+b-x$. Substituting this into the relaxation formula:\n$$\nT_{0.5}(x) = (1-0.5)x + 0.5(a+b-x) = 0.5x + 0.5(a+b) - 0.5x = 0.5(a+b)\n$$\nThis simplifies to the constant value $c = \\frac{a+b}{2} = 0.5$. As established earlier, $c$ is the unique fixed point of $g(x)$. It is also a fixed point of $T_r(x)$ for any $r$, since $T_r(c) = (1-r)c + r g(c) = (1-r)c + rc = c$.\nTherefore, upon detecting a two-cycle (which implies the current iterate is in $[a,b]$), a single application of the relaxation operator $T_{0.5}$ maps the iterate directly to the fixed point $c=0.5$. The iteration converges instantly, successfully breaking the cycle.\n\nThe overall procedure for a single initial condition $x_0$ is:\n1.  Begin plain iteration.\n2.  In each step, test for fixed-point convergence. If so, terminate and report success.\n3.  If not, test for a two-cycle. If a cycle is found, switch to the relaxed iteration.\n4.  Continue with relaxed iteration until convergence is found or the total iteration budget $N$ is exhausted.\n5.  If the budget is exhausted at any stage, report failure to converge.\n\nThis combined strategy will be applied to each of the $5$ initial conditions provided. The results (cycle detection indicator, convergence indicator, final value) for each case are collected and formatted into the specified output list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g(x: float, a: float, b: float, k: float) - float:\n    \"\"\"\n    Implements the piecewise linear function g(x).\n    \n    Args:\n        x: The input value.\n        a: The lower boundary of the middle interval.\n        b: The upper boundary of the middle interval.\n        k: The slope parameter for the outer branches.\n    \n    Returns:\n        The value of g(x).\n    \"\"\"\n    if x  a:\n        return a - k * (a - x)\n    elif a = x = b:\n        return a + b - x\n    else:  # x  b\n        return b + k * (x - b)\n\ndef run_single_case(x0: float, a: float, b: float, k: float, N: int, tol: float, r: float) - tuple[int, int, float]:\n    \"\"\"\n    Runs the fixed-point iteration for a single initial condition,\n    with cycle detection and avoidance.\n    \n    Args:\n        x0: The initial condition.\n        a, b, k: Parameters for the function g(x).\n        N: The maximum number of iterations.\n        tol: The convergence tolerance.\n        r: The relaxation parameter.\n\n    Returns:\n        A tuple containing:\n        - (int): 1 if a 2-cycle was detected, 0 otherwise.\n        - (int): 1 if the iteration converged, 0 otherwise.\n        - (float): The final value, rounded to 6 decimal places.\n    \"\"\"\n    cycle_detected = 0\n    converged = 0\n    \n    # --- Phase 1: Plain Iteration ---\n    x_history = [x0]\n    for i in range(N):\n        x_current = x_history[-1]\n        x_next = g(x_current, a, b, k)\n\n        # 1. Test for fixed-point convergence\n        if np.abs(x_next - x_current)  tol:\n            converged = 1\n            x_final = x_next\n            return cycle_detected, converged, round(x_final, 6)\n\n        # 2. Test for 2-cycle (requires at least x_i-1, x_i - x_i+1)\n        # This corresponds to i = 1\n        if i = 1 and np.abs(x_next - x_history[-2])  tol:\n            cycle_detected = 1\n            \n            # --- Phase 2: Relaxed Iteration ---\n            x_relaxed_current = x_next\n            remaining_iters = N - (i + 1)\n            \n            for _ in range(remaining_iters):\n                x_relaxed_next = (1.0 - r) * x_relaxed_current + r * g(x_relaxed_current, a, b, k)\n                \n                if np.abs(x_relaxed_next - x_relaxed_current)  tol:\n                    converged = 1\n                    x_final = x_relaxed_next\n                    return cycle_detected, converged, round(x_final, 6)\n                \n                x_relaxed_current = x_relaxed_next\n            \n            # If relaxation loop finishes, it failed to converge in the remaining budget\n            converged = 0\n            x_final = x_relaxed_current\n            return cycle_detected, converged, round(x_final, 6)\n\n        x_history.append(x_next)\n    \n    # If the plain iteration loop completes without convergence or cycle detection\n    converged = 0\n    x_final = x_history[-1]\n    return cycle_detected, converged, round(x_final, 6)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n    # Define problem parameters\n    a = 0.3\n    b = 0.7\n    k = 0.4\n    N = 200\n    tol = 1.0e-10\n    r = 0.5\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        0.4,\n        0.5,\n        0.05,\n        0.95,\n        0.3,\n    ]\n\n    results = []\n    for x0 in test_cases:\n        # Run the iteration and detection logic for one case\n        d, c, v = run_single_case(x0, a, b, k, N, tol, r)\n        results.extend([d, c, v])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}