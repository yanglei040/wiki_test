{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的巩固方式是动手实践。我们从迭代法中最基本的一类——不动点迭代开始。这个练习将帮助你巩固 Q-线性收敛的关键概念，并掌握一种通过分析误差序列来从数值上验证收敛速度的实用技巧。通过这个练习，你将看到理论推导与数值实验结果如何完美地吻合。",
            "id": "3113939",
            "problem": "考虑实数轴上的一个不动点迭代，定义为 $x_{k+1} = g(x_k)$，其中 $g$ 是一个从 $\\mathbb{R}$ 到 $\\mathbb{R}$ 的函数。假设 $g$ 是关于标准绝对值度量的压缩映射，意味着存在一个常数 $L$ 满足 $0  L  1$，使得对于所有 $x,y \\in \\mathbb{R}$，不等式 $|g(x) - g(y)| \\le L |x - y|$ 成立。令 $x_\\star \\in \\mathbb{R}$ 表示 $g$ 的不动点，它满足 $g(x_\\star) = x_\\star$。定义第 $k$ 次迭代的误差为 $e_k = x_k - x_\\star$，其大小为 $|e_k|$。\n\n任务：\n- 仅从压缩性质和不动点迭代的定义出发，推导一个关联 $|e_{k+1}|$ 和 $|e_k|$ 的不等式。用此证明该迭代表现出商线性（Q-线性）收敛，其中商线性（Q-线性）意味着当 $k \\to \\infty$ 时，比率序列 $|e_{k+1}| / |e_k|$ 收敛到一个常数。然后，对于特定的仿射映射 $g(x) = a x + b$（其中 $|a|  1$），用 $a$ 和 $b$ 确定商线性因子，并说明在什么条件下单步误差不等式中的等号成立。\n- 将收缩因子与 $\\log(|e_k|)$ 关于 $k$ 的图像（自然对数）的斜率联系起来。利用对数和序列的基本性质，解释该斜率如何与商线性因子相关。\n- 实现一个程序，对于一组仿射压缩映射 $g(x) = a x + b$，生成误差序列，并用两种方法数值估计收缩因子：(i) 使用序列尾部的比率 $|e_{k+1}| / |e_k|$，以及 (ii) 对 $\\log(|e_k|)$ 关于 $k$ 进行最小二乘拟合，并通过对拟合的斜率取指数来恢复一个因子。将这两种数值估计与理论上的 Lipschitz 常数 $L$进行比较。\n\n使用以下参数集 $(a,b,x_0,N)$ 的测试套件，其中 $x_0$ 是初始迭代值， $N$ 是迭代次数：\n- 测试 1：$(a,b,x_0,N) = (0.5, 1.0, 10.0, 30)$\n- 测试 2：$(a,b,x_0,N) = (0.9, -2.0, 0.0, 60)$\n- 测试 3：$(a,b,x_0,N) = (-0.75, 3.0, -5.0, 50)$\n- 测试 4：$(a,b,x_0,N) = (0.2, 0.0, 100.0, 40)$\n\n对于每个测试，解析地计算不动点 $x_\\star$，通过 $x_{k+1} = a x_k + b$ 生成序列 $(x_k)_{k=0}^{N}$，计算误差大小 $|e_k| = |x_k - x_\\star|$，从最后一对有效的连续非零误差中估计基于比率的因子，通过对所有严格为正误差的迭代（以避免计算 $\\log(0)$）的 $\\log(|e_k|)$ 关于迭代指数 $k$ 进行最小二乘线性拟合来估计基于斜率的因子，然后对拟合的斜率取指数。仿射映射的理论 Lipschitz 常数为 $L = |a|$。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是按 $[L,\\widehat{L}_{\\mathrm{ratio}},\\widehat{L}_{\\mathrm{slope}}]$ 顺序排列的三个浮点数列表，其中 $L$ 是理论 Lipschitz 常数，$\\widehat{L}_{\\mathrm{ratio}}$ 是经验尾部比率估计，$\\widehat{L}_{\\mathrm{slope}}$ 是拟合斜率的指数。例如，输出应如下所示：$[[L_1,\\widehat{L}_{\\mathrm{ratio},1},\\widehat{L}_{\\mathrm{slope},1}],[L_2,\\widehat{L}_{\\mathrm{ratio},2},\\widehat{L}_{\\mathrm{slope},2}],\\ldots]$。不涉及任何物理单位或角度；所有报告的值都是无量纲实数。",
            "solution": "该问题要求分析一维不动点迭代 $x_{k+1} = g(x_k)$ 的收敛性质，重点关注仿射映射 $g(x) = ax+b$ 的特殊情况。我们将首先推导基本的误差不等式，然后将收敛率与误差的对数图联系起来，最后概述一个数值验证过程。\n\n### 第1部分：单步误差不等式与Q-线性收敛\n\n不动点迭代由序列 $x_{k+1} = g(x_k)$ 定义，其中 $k = 0, 1, 2, \\ldots$。不动点记为 $x_\\star$，满足方程 $g(x_\\star) = x_\\star$。第 $k$ 次迭代的误差定义为 $e_k = x_k - x_\\star$。我们关心的是误差大小 $|e_k|$ 的行为。\n\n第 $k+1$ 步的误差大小由 $|e_{k+1}| = |x_{k+1} - x_\\star|$ 给出。通过代入迭代的定义式 $x_{k+1}$ 和不动点的等价定义 $x_\\star = g(x_\\star)$，我们得到：\n$$|e_{k+1}| = |g(x_k) - g(x_\\star)|$$\n问题指出 $g$ 是关于 $\\mathbb{R}$ 上标准绝对值度量的压缩映射。这意味着存在一个 Lipschitz 常数 $L$，使得 $0  L  1$ 并且对于所有 $x, y \\in \\mathbb{R}$：\n$$|g(x) - g(y)| \\le L |x - y|$$\n应用此不等式，令 $x = x_k$ 且 $y = x_\\star$，我们得到：\n$$|g(x_k) - g(x_\\star)| \\le L |x_k - x_\\star|$$\n认识到 $|x_k - x_\\star| = |e_k|$，我们得到单步误差不等式：\n$$|e_{k+1}| \\le L |e_k|$$\n这个不等式表明，每次迭代中误差大小保证至少减小一个因子 $L$。由于 $0  L  1$，这确保了 $\\lim_{k \\to \\infty} |e_k| = 0$，因此迭代序列 $(x_k)$ 收敛到不动点 $x_\\star$。\n\n如果误差比率序列收敛到一个常数 $C \\in [0, 1)$，则确立了商线性（Q-线性）收敛：\n$$\\lim_{k \\to \\infty} \\frac{|e_{k+1}|}{|e_k|} = C$$\n常数 $C$ 被称为商线性因子或收敛率。\n\n对于仿射映射 $g(x) = ax + b$（其中 $|a|  1$）的特定情况，我们可以为误差传播建立一个精确的关系。不动点 $x_\\star$ 解方程 $x_\\star = ax_\\star + b$，得到 $x_\\star = b/(1-a)$。\n让我们重新审视 $|e_{k+1}|$ 的表达式：\n$$|e_{k+1}| = |g(x_k) - g(x_\\star)| = |(ax_k + b) - (ax_\\star + b)| = |ax_k - ax_\\star| = |a(x_k - x_\\star)|$$\n这可以简化为：\n$$|e_{k+1}| = |a| |x_k - x_\\star| = |a| |e_k|$$\n对于这个仿射函数，一般的不等式变成了一个精确的等式。如果初始误差 $e_0$ 非零，那么所有后续的误差 $e_k$ 也将非零（因为在相关的测试案例中 $|a| > 0$）。因此，我们可以除以 $|e_k|$ 来求比率：\n$$\\frac{|e_{k+1}|}{|e_k|} = |a|$$\n由于这个比率对所有 $k$ 都是常数，其当 $k \\to \\infty$ 时的极限显然是 $|a|$。因此，该迭代表现出 Q-线性收敛，并且商线性因子恰好是 $|a|$。该因子取决于 $a$ 但不取决于 $b$。单步误差不等式中等号成立的条件是函数 $g$ 是仿射的，如上所示。对于这样的函数，最紧的 Lipschitz 常数是 $L = |a|$。\n\n### 第2部分：与对数-误差图斜率的联系\n\n对于仿射映射，精确关系 $|e_{k+1}| = |a| |e_k|$ 允许我们通过递归代入找到 $|e_k|$ 的闭式表达式：\n$$|e_k| = |a| |e_{k-1}| = |a|^2 |e_{k-2}| = \\dots = |a|^k |e_0|$$\n其中 $|e_0| = |x_0 - x_\\star|$ 是初始误差大小。这个关系对所有 $k \\ge 0$ 都有效。\n\n为了揭示一个线性结构，我们对两边取自然对数（假设 $|e_0| > 0$）：\n$$\\ln(|e_k|) = \\ln(|a|^k |e_0|)$$\n利用对数的性质，特别是 $\\ln(xy) = \\ln(x) + \\ln(y)$ 和 $\\ln(x^p) = p \\ln(x)$，我们可以展开表达式：\n$$\\ln(|e_k|) = \\ln(|a|^k) + \\ln(|e_0|) = k \\ln(|a|) + \\ln(|e_0|)$$\n这个方程是直线方程 $y = mx + c$ 的形式，其中：\n- 因变量是 $y = \\ln(|e_k|)$。\n- 自变量是迭代指数 $x = k$。\n- 直线的斜率是 $m = \\ln(|a|)$。\n- y轴截距是 $c = \\ln(|e_0|)$。\n\n这表明，误差大小的自然对数关于迭代指数的图像将产生一条直线。这条线的斜率 $m$ 是 Q-线性因子 $|a|$ 的自然对数。因此，可以通过对数值估计的斜率取指数来恢复该因子：\n$$|a| = e^m$$\n这提供了第二种数值估计收敛因子的方法，即对数据点 $(k, \\ln(|e_k|))$ 进行线性最小二乘拟合。\n\n### 第3部分：数值实现计划\n\n程序将为每个提供的测试用例 $(a, b, x_0, N)$ 执行以下步骤：\n1. 理论 Lipschitz 常数 $L$ 被确定为 $|a|$。\n2. 不动点 $x_\\star$ 通过解析计算得出 $x_\\star = b / (1-a)$。\n3. 从 $x_0$ 开始，使用关系式 $x_{k+1} = a x_k + b$ 生成迭代序列 $(x_k)_{k=0}^{N}$。同时，计算并存储误差大小序列 $|e_k| = |x_k - x_\\star|$。\n4. 使用序列中的最后两个误差大小计算基于比率的估计 $\\widehat{L}_{\\mathrm{ratio}}$：$\\widehat{L}_{\\mathrm{ratio}} = |e_N| / |e_{N-1}|$。这是对“尾部比率”的合理解释，并且对于仿射映射，这个比率在理论上是恒定的。\n5. 基于斜率的估计 $\\widehat{L}_{\\mathrm{slope}}$ 通过线性最小二乘回归获得。\n    a. 对于所有 $|e_k| > 0$ 的 $k \\in \\{0, 1, \\dots, N\\}$，构建数据点集 $(k, \\ln(|e_k|))$。\n    b. 对这些点进行线性拟合，提供斜率 $m$ 的估计值。\n    c. 然后计算估计值 $\\widehat{L}_{\\mathrm{slope}} = e^m$。\n每个测试用例的最终输出将是三元组 $[L, \\widehat{L}_{\\mathrm{ratio}}, \\widehat{L}_{\\mathrm{slope}}]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating convergence factors for affine fixed-point iterations.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, b, x_0, N)\n        (0.5, 1.0, 10.0, 30),\n        (0.9, -2.0, 0.0, 60),\n        (-0.75, 3.0, -5.0, 50),\n        (0.2, 0.0, 100.0, 40),\n    ]\n\n    results = []\n    for a, b, x_0, N in test_cases:\n        # Theoretical Lipschitz constant\n        L_theoretical = np.abs(a)\n\n        # Analytical fixed point\n        # x_star = a * x_star + b  =>  x_star * (1 - a) = b  =>  x_star = b / (1 - a)\n        # Since |a|  1 is given, 1 - a is never zero.\n        if 1 - a == 0:\n            # This case should not be reached with the given |a|  1.\n            # However, for robustness, handle it.\n            # If a=1 and b=0, any x is a fixed point. If b!=0, no fixed point.\n            # Problem constraints make this moot.\n            x_star = np.nan\n        else:\n            x_star = b / (1.0 - a)\n\n        # Generate sequence and error magnitudes\n        x_k = x_0\n        e_magnitudes = np.zeros(N + 1)\n        for k in range(N + 1):\n            e_k = x_k - x_star\n            e_magnitudes[k] = np.abs(e_k)\n            # Update for next iteration\n            x_k = a * x_k + b\n\n        # (i) Estimate factor using the ratio from the tail of the sequence\n        # The problem states \"last valid pair of successive nonzero errors\".\n        # For the given affine maps with |a|>0 and e_0 != 0, errors are never zero.\n        # So we can safely use the last two elements.\n        L_ratio_estimate = e_magnitudes[N] / e_magnitudes[N-1]\n\n        # (ii) Estimate factor using a least-squares fit of log(|e_k|) vs k\n        # Filter for strictly positive errors to avoid log(0)\n        k_indices = np.arange(N + 1)\n        positive_error_mask = e_magnitudes > 0\n        \n        k_filtered = k_indices[positive_error_mask]\n        log_e_filtered = np.log(e_magnitudes[positive_error_mask])\n        \n        # Perform linear least-squares fit: log(|e_k|) = m*k + c\n        # np.polyfit returns [slope, intercept] for a degree 1 polynomial\n        if len(k_filtered)  2:\n            # Not enough data points for a fit\n            L_slope_estimate = np.nan\n        else:\n            slope, _ = np.polyfit(k_filtered, log_e_filtered, 1)\n            # The factor is the exponential of the slope\n            L_slope_estimate = np.exp(slope)\n\n        results.append([L_theoretical, L_ratio_estimate, L_slope_estimate])\n\n    # Final print statement in the exact required format.\n    # e.g., [[L1,L_ratio1,L_slope1],[L2,L_ratio2,L_slope2],...]\n    output_str = f\"[{','.join([f'[{l1},{l2},{l3}]' for l1, l2, l3 in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "接下来，我们转向另一个关键领域：求解大型线性方程组。这个实践将探索著名的共轭梯度（CG）方法。我们将超越教科书中基于条件数的收敛上界，深入研究初始误差的谱分布如何显著影响实际的收敛速度。这是一个对算法设计和分析至关重要的洞见，它揭示了理论最坏情况与实际性能之间的微妙关系。",
            "id": "3113963",
            "problem": "考虑使用共轭梯度法（CG）迭代求解一个线性系统，其中系数矩阵是“对称正定”（SPD）的。您将研究初始误差的谱分量如何影响误差衰减的经验速率，并特别关注初始误差不包含最大特征值对应特征向量（“主导特征向量”）方向分量的情况。\n\n通过指定一个正交矩阵 $Q$ 和一个具有严格正元素的对角矩阵 $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$，并设置 $A = Q \\Lambda Q^\\top$，来构造一个 SPD 矩阵 $A \\in \\mathbb{R}^{n \\times n}$。使用以下可复现的构造方法：\n\n- 维度：$n = 20$。\n- 特征值：$\\lambda_i$ 在 1 到 1000 之间（含两端）线性分布，并按升序排列。\n- 正交矩阵 $Q$：通过对一个包含独立标准正态分布元素、使用固定伪随机种子 $s = 7$ 生成的 $n \\times n$ 矩阵进行瘦 QR 分解，取其 $Q$ 因子来构成 $Q$。\n\n使用相同的种子 $s = 7$ 采样一个具有独立标准正态分布元素的向量来定义精确解 $x^\\star \\in \\mathbb{R}^n$，并设置右端项 $b = A x^\\star$。您将运行共轭梯度法（CG）来求解 $A x = b$，从五个不同的初始猜测 $x_0$ 开始，这五个初始猜测会导出五个不同的初始误差向量 $e_0 = x_0 - x^\\star$。在每种情况下，对初始误差进行归一化，使其满足 $\\|e_0\\|_A = 1$，其中 $A$-范数定义为 $\\|e\\|_A = \\sqrt{e^\\top A e}$。这五种情况是：\n\n- 情况 G（一般混合）：$e_0$ 从一个标准正态向量中抽样得出。\n- 情况 O（与主导特征向量正交）：$e_0$ 位于与 $A$ 的最小 $n/2$ 个特征值相关联的特征向量（即 $Q$ 的列向量 $q_1,\\dots,q_{n/2}$）所张成的空间中。\n- 情况 D（与主导子空间对齐）：$e_0$ 位于与 $A$ 的最大 $n/2$ 个特征值相关联的特征向量（即 $Q$ 的列向量 $q_{n/2+1},\\dots,q_n$）所张成的空间中。\n- 情况 S（单一小特征值方向）：$e_0$ 精确等于对应最小特征值的特征向量 $q_1$。\n- 情况 E（精确初始值）：$x_0 = x^\\star$，因此 $e_0 = 0$。\n\n在情况 O 和 D 中，取指定子空间中的系数为独立的标准正态分布元素，然后进行归一化以强制 $\\|e_0\\|_A = 1$。在情况 G 中，抽取一个全长的标准正态向量并进行类似归一化。在情况 S 中，只需在必要时对 $q_1$进行归一化，以使 $\\|e_0\\|_A = 1$。在情况 E 中，由于解已经精确，无需进行迭代。\n\n实现无任何预处理的 SPD 矩阵共轭梯度法（CG）。在每次迭代 $k$ 中，更新迭代解 $x_k$ 并计算当前误差 $e_k = x_k - x^\\star$。当 $\\|e_k\\|_A \\le \\tau \\|e_0\\|_A$（其中 $\\tau = 10^{-8}$）或迭代次数达到维度 $n$ 时停止。\n\n对于这五种情况中的每一种，记录满足停止条件的最小迭代次数 $k_\\tau$（对于情况 E，$k_\\tau = 0$）。您的程序应生成单行输出，包含这五个整数，顺序为 $\\big[$情况 G, 情况 O, 情况 D, 情况 S, 情况 E$\\big]$，形式为用方括号括起来的逗号分隔列表，例如 $\\big[3,7,2,1,0\\big]$。\n\n测试套件和覆盖范围说明：\n\n- 维度 $n=20$ 和从 1 到 1000 的特征值范围确保了一个非平凡的条件数和现实的 SPD 行为。\n- 情况 G 是一个具有一般误差混合的“理想路径”（happy path）。\n- 情况 O 排除了主导特征向量，旨在减缓收敛速度。\n- 情况 D 只包含主导特征向量，旨在相对于情况 O 加快收敛速度。\n- 情况 S 隔离了单个小特征值对应的特征向量，用于探究 CG 在一维谱子空间中的边界行为。\n- 情况 E 使用精确的初始值，用于探究零迭代的边界条件。\n\n最终输出格式：您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果（例如，$\\big[ \\text{result}_1,\\text{result}_2,\\dots \\big]$）。不涉及物理单位或角度单位，所有报告的量必须是整数。",
            "solution": "该问题要求研究共轭梯度法（CG）在求解线性方程组 $A x = b$ 时的收敛行为，其中矩阵 $A$ 是对称正定（SPD）的。分析的核心在于理解初始误差向量 $e_0 = x_0 - x^\\star$ 的谱分量如何影响收敛所需的迭代次数。\n\n共轭梯度法是一种迭代算法，在每次迭代 $k$ 中，它会在一个不断扩大的搜索空间上生成一个近似解 $x_k$，该解能够最小化 $A$-范数下的误差，该范数定义为 $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$。具体来说，迭代解 $x_k$ 是从仿射子空间 $x_0 + \\mathcal{K}_k(A, r_0)$ 中选择的，其中 $r_0 = b - A x_0$ 是初始残差，而 $\\mathcal{K}_k(A, r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ 是 $k$ 维 Krylov 子空间。\n\nCG 的理论收敛速率通常由以下不等式界定：\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k\n$$\n其中 $\\kappa(A) = \\lambda_{\\max} / \\lambda_{\\min}$ 是 $A$ 的谱条件数。虽然这个界限很有用，但它代表了最坏情况。CG 的实际性能高度依赖于 $A$ 的特征值分布以及初始误差 $e_0$ 在相应特征向量上的投影。\n\n如果我们将初始误差在 $A$ 的特征向量基 $\\{q_i\\}_{i=1}^n$ 中表示为 $e_0 = \\sum_{i=1}^n c_i q_i$，则 $k$ 次迭代后的误差可以写为 $e_k = P_k(A) e_0$。这里 $P_k$ 是一个次数最多为 $k$ 的唯一多项式，它满足 $P_k(0) = 1$ 并最小化 $\\|P_k(A) e_0\\|_A$。对于那些初始误差分量 $c_i$ 显著的特征值 $\\lambda_i$，该多项式的值趋于变小。CG 在迭代的早期阶段尤其能有效减少与大的、孤立的特征值相关的误差分量。本问题通过五个不同的初始误差情况设置了一个数值实验来展示这一现象。\n\n实验设置如下：\n- 构造一个维度为 $n=20$ 的矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其形式为 $A = Q \\Lambda Q^\\top$。\n- 对角矩阵 $\\Lambda$ 中的特征值是 $n=20$ 个从 $\\lambda_1 = 1$ 到 $\\lambda_{20} = 1000$ 线性分布的点。\n- 正交矩阵 $Q$ 和精确解 $x^\\star$ 是使用伪随机种子 $s=7$ 可复现地生成的。\n- 右端项是 $b = A x^\\star$。\n- 停止准则是 $\\|e_k\\|_A \\le \\tau \\|e_0\\|_A$，其中容差 $\\tau = 10^{-8}$，最大迭代次数为 $n=20$。\n- 对于所有初始误差不为零的情况，$e_0$ 都被归一化以使得 $\\|e_0\\|_A = 1$。因此，停止条件简化为 $\\|e_k\\|_A \\le 10^{-8}$。\n\n初始误差 $e_0$ 的五种情况旨在探究不同的收敛行为：\n\n1.  **情况 G（一般混合）：** 初始误差 $e_0$ 源自一个具有独立标准正态分布元素的向量。预计它在所有特征向量方向上都具有非零分量。由此产生的迭代次数可作为“典型”场景的基线。\n\n2.  **情况 O（与主导特征向量正交）：** 初始误差 $e_0$ 被人工构造，使其仅存在于由对应 10 个最小特征值的特征向量 $\\{q_1, \\dots, q_{10}\\}$ 所张成的子空间中。通过排除与最大特征值相关的分量——这些分量正是 CG 最能高效消除的——我们从一开始就迫使该方法处理谱中更具挑战性的部分。预计这将相对于一般情况减慢收敛速度。\n\n3.  **情况 D（与主导子空间对齐）：** 初始误差 $e_0$ 被构造为仅存在于由对应 10 个最大特征值的特征向量 $\\{q_{11}, \\dots, q_{20}\\}$ 所张成的子空间中。由于误差完全由 CG 最“容易”处理的分量构成，收敛速度应显著快于一般情况和正交情况。\n\n4.  **情况 S（单一小特征值方向）：** 初始误差 $e_0$ 被设置为与对应最小特征值 $\\lambda_1$ 的特征向量 $q_1$ 成比例。归一化条件 $\\|e_0\\|_A = 1$ 意味着 $e_0 = q_1 / \\sqrt{\\lambda_1}$。初始残差为 $r_0 = -A e_0 = -A (q_1 / \\sqrt{\\lambda_1}) = -\\sqrt{\\lambda_1} q_1$。因为初始残差是 $A$ 的一个特征向量，所以整个 CG 过程被限制在由 $q_1$ 张成的一维子空间内。该方法保证在单次迭代中找到精确解。因此，预期的迭代次数是 $k_\\tau = 1$。\n\n5.  **情况 E（精确初始值）：** 初始猜测是精确解 $x_0 = x^\\star$，导致初始误差为零，$e_0 = 0$。算法立即终止，迭代次数为 $k_\\tau = 0$。\n\n以下 Python 代码实现了这个数值实验，计算了五种情况下每一种所需的迭代次数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Sets up and solves the Conjugate Gradient problem for five different initial error configurations.\n    \"\"\"\n    # Problem parameters\n    n = 20\n    seed = 7\n    tol = 1e-8\n    max_iter = n\n\n    # --- Step 1: Reproducible setup of the linear system ---\n    rng = np.random.default_rng(seed)\n\n    # Eigenvalues and matrix A\n    eigs = np.linspace(1.0, 1000.0, n)\n    Lambda = np.diag(eigs)\n    Z = rng.standard_normal((n, n))\n    Q, _ = np.linalg.qr(Z)\n    A = Q @ Lambda @ Q.T\n    \n    # Exact solution and right-hand side b\n    x_star = rng.standard_normal(n)\n    b = A @ x_star\n\n    def conjugate_gradient(A_mat, b_vec, x0_vec, x_star_vec, tolerance, max_iterations):\n        \"\"\"\n        Implementation of the Conjugate Gradient method for SPD matrices.\n        The stopping condition is based on the A-norm of the true error.\n        \"\"\"\n        # Initial error and its A-norm\n        e0 = x0_vec - x_star_vec\n        # Using np.dot for clarity on vector-matrix-vector product\n        e0_A_norm_sq = np.dot(e0, A_mat @ e0)\n\n        # Handle the exact start case\n        if e0_A_norm_sq  1e-30:  # Effectively zero\n            return 0\n        \n        e0_A_norm = np.sqrt(e0_A_norm_sq)\n        threshold_A_norm = tolerance * e0_A_norm\n\n        x = x0_vec\n        r = b_vec - A_mat @ x\n        p = r\n        rs_old = np.dot(r, r)\n\n        for k in range(max_iterations):\n            Ap = A_mat @ p\n            alpha = rs_old / np.dot(p, Ap)\n            \n            x = x + alpha * p\n            r = r - alpha * Ap\n\n            # Check stopping condition based on the A-norm of the true error\n            e_k = x - x_star_vec\n            e_k_A_norm = np.sqrt(np.dot(e_k, A_mat @ e_k))\n            \n            if e_k_A_norm = threshold_A_norm:\n                return k + 1\n\n            rs_new = np.dot(r, r)\n            if np.sqrt(rs_new)  1e-15: # Residual is numerically zero\n                return k + 1 \n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n\n        return max_iterations\n\n    def normalize_e0(e0_raw, A_mat):\n        \"\"\"Normalizes an error vector to have an A-norm of 1.\"\"\"\n        norm_A_sq = np.dot(e0_raw, A_mat @ e0_raw)\n        if norm_A_sq == 0:\n            return e0_raw\n        return e0_raw / np.sqrt(norm_A_sq)\n\n    results = []\n\n    # --- Step 2: Run CG for each of the five cases ---\n\n    # Case G (General mixture)\n    e0_raw_g = rng.standard_normal(n)\n    e0_g = normalize_e0(e0_raw_g, A)\n    x0_g = x_star + e0_g\n    k_g = conjugate_gradient(A, b, x0_g, x_star, tol, max_iter)\n    results.append(k_g)\n    \n    # Case O (Orthogonal to dominant)\n    n_half = n // 2\n    Q_small = Q[:, :n_half]\n    coeffs_o = rng.standard_normal(n_half)\n    e0_raw_o = Q_small @ coeffs_o\n    e0_o = normalize_e0(e0_raw_o, A)\n    x0_o = x_star + e0_o\n    k_o = conjugate_gradient(A, b, x0_o, x_star, tol, max_iter)\n    results.append(k_o)\n\n    # Case D (Aligned with dominant)\n    Q_large = Q[:, n_half:]\n    coeffs_d = rng.standard_normal(n_half)\n    e0_raw_d = Q_large @ coeffs_d\n    e0_d = normalize_e0(e0_raw_d, A)\n    x0_d = x_star + e0_d\n    k_d = conjugate_gradient(A, b, x0_d, x_star, tol, max_iter)\n    results.append(k_d)\n    \n    # Case S (Single small-eigenvalue direction)\n    e0_raw_s = Q[:, 0]  # Eigenvector for the smallest eigenvalue\n    e0_s = normalize_e0(e0_raw_s, A)\n    x0_s = x_star + e0_s\n    k_s = conjugate_gradient(A, b, x0_s, x_star, tol, max_iter)\n    results.append(k_s)\n\n    # Case E (Exact start)\n    k_e = 0\n    results.append(k_e)\n\n    # --- Step 3: Print final results in the specified format ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，我们将进入作为现代数据科学基石的大规模优化领域。本练习将检验坐标下降法，并对比两种基本策略：确定性的循环更新和随机更新。通过分析相应迭代矩阵的谱半径，我们将从理论上理解为什么随机化方法在实践中常常更受青睐，并体会到算法设计中的权衡。",
            "id": "3113892",
            "problem": "考虑最小化严格凸二次函数 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个具有正对角元的实对称严格对角占优矩阵，且 $\\mathbf{b} \\in \\mathbb{R}^{n}$。这样的矩阵 $A$ 是对称正定（SPD）的，这保证了存在一个满足 $A \\mathbf{x}^{\\star} = \\mathbf{b}$ 的唯一极小值点 $\\mathbf{x}^{\\star}$。坐标下降法通过沿某一坐标轴对 $f$ 进行精确一维最小化来迭代地更新 $\\mathbf{x}$ 的一个坐标，同时保持其他坐标不变。我们考虑两种坐标采样方案：循环（确定性顺序）和随机（根据坐标上的分布进行随机选择）。\n\n仅从二次函数 $f$ 梯度的基本定义和精确一维最小化的概念出发，推导单坐标更新的线性误差更新表示，该表示形式为一个作用于当前误差 $\\mathbf{e} = \\mathbf{x} - \\mathbf{x}^{\\star}$ 的迭代算子。使用此表示来定义迭代矩阵，该矩阵代表：\n- 一个完整的循环周期，即按固定顺序访问所有坐标一次。\n- 一个随机步，用给定坐标采样分布下的期望算子表示。\n然后，通过作用于期望误差的相应迭代矩阵的谱半径 $\\rho(M) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } M \\}$ 来解释它们的收敛速率。解释为什么在 $k$ 个独立的随机步之后，期望误差在期望意义下遵循线性迭代，并构建对应于一个包含 $n$ 个独立随机步的随机周期的算子。\n\n您的程序必须为提供的测试套件实现以下任务，且无需任何外部输入：\n1. 对每个测试用例，根据推导出的公式构建单坐标迭代算子，并构造：\n   - 循环周期迭代矩阵（按固定顺序遍历一次的乘积）。\n   - 给定采样分布的期望单步随机迭代矩阵。\n2. 计算循环周期矩阵和期望单步随机矩阵的谱半径。同时，计算对应于 $n$ 个独立随机步的随机周期的谱半径。\n3. 对每个测试用例，输出一个包含四个值的序列：\n   - 循环周期的谱半径（浮点数）。\n   - 随机单步的谱半径（浮点数）。\n   - 随机周期的谱半径（浮点数）。\n   - 一个布尔值，指示随机周期的谱半径是否严格小于循环周期的谱半径（即，根据这种谱半径的解释，是否预测随机法更快）。\n\n测试套件：\n- 案例1（边界情况，纯对角矩阵）：$n = 3$，\n  $$A = \\begin{bmatrix}\n  4  0  0 \\\\\n  0  5  0 \\\\\n  0  0  6\n  \\end{bmatrix}, \\quad \\text{均匀采样 } \\mathbf{p} = \\left[\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\right].$$\n- 案例2（弱耦合，严格对角占优）：$n = 3$，\n  $$A = \\begin{bmatrix}\n  4  -0.9  0.2 \\\\\n  -0.9  3.5  -0.4 \\\\\n  0.2  -0.4  2.8\n  \\end{bmatrix}, \\quad \\text{均匀采样 } \\mathbf{p} = \\left[\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\right].$$\n- 案例3（规模较大，与对角元成比例的非均匀采样）：$n = 5$，\n  $$A = \\begin{bmatrix}\n  8  -0.5  0.2  0  0.1 \\\\\n  -0.5  7  0.3  -0.4  0 \\\\\n  0.2  0.3  9  -0.6  0.5 \\\\\n  0  -0.4  -0.6  6  -0.3 \\\\\n  0.1  0  0.5  -0.3  5\n  \\end{bmatrix}, \\quad \\mathbf{p} \\text{ 由 } p_j = \\frac{A_{jj}}{\\sum_{i=1}^{n} A_{ii}} \\text{ 定义，其中 } j = 1,\\dots,n.$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。将案例1的四个值、案例2的四个值、案例3的四个值依次连接起来。例如，格式为 $[\\text{c1\\_cyc},\\text{c1\\_rand\\_step},\\text{c1\\_rand\\_epoch},\\text{c1\\_better},\\text{c2\\_cyc},\\dots,\\text{c3\\_better}]$，其中每个 $\\text{c\\*}$ 值是浮点数，每个 $\\text{better}$ 值是布尔值。不涉及物理单位或角度单位；所有数值答案必须在指定的单行输出中以普通小数或布尔值的形式提供。",
            "solution": "本解答旨在推导并分析应用于二次函数的循环和随机坐标下降法的迭代矩阵，并通过谱半径分析其收敛速率。\n\n**1. 单坐标更新推导**\n\n需要最小化的目标函数是 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$。由于矩阵 $A$ 是对称的， $f(\\mathbf{x})$ 的梯度由下式给出：\n$$\n\\nabla f(\\mathbf{x}) = A \\mathbf{x} - \\mathbf{b}\n$$\n唯一极小值点 $\\mathbf{x}^{\\star}$ 是线性系统 $\\nabla f(\\mathbf{x}^{\\star}) = \\mathbf{0}$ 的解，这意味着 $A \\mathbf{x}^{\\star} = \\mathbf{b}$。\n\n坐标下降法每次更新一个坐标。设 $\\mathbf{x}^{(k)}$ 是当前迭代点。为了通过更新第 $i$ 个坐标来找到下一个迭代点 $\\mathbf{x}^{(k+1)}$，我们保持所有其他坐标固定（对于 $j \\neq i$，有 $x_j^{(k+1)} = x_j^{(k)}$），并沿着第 $i$ 个坐标轴对 $f$ 执行精确的一维最小化。这等价于求解 $x_i^{(k+1)}$，使得新点上梯度的第 $i$ 个分量为零：\n$$\n(\\nabla f(\\mathbf{x}^{(k+1)}))_i = (A \\mathbf{x}^{(k+1)} - \\mathbf{b})_i = 0\n$$\n展开该矩阵方程的第 $i$ 行得到：\n$$\n\\sum_{j=1}^{n} A_{ij} x_j^{(k+1)} - b_i = 0\n$$\n分离含 $x_i^{(k+1)}$ 的项，并代入 $j \\neq i$ 时的 $x_j^{(k+1)} = x_j^{(k)}$：\n$$\nA_{ii} x_i^{(k+1)} + \\sum_{j \\neq i} A_{ij} x_j^{(k)} - b_i = 0\n$$\n求解 $x_i^{(k+1)}$ 得到更新规则：\n$$\nx_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} x_j^{(k)} \\right)\n$$\n注意，由于 $A$ 是具有正对角元的严格对角占优矩阵，因此 $A_{ii} \\neq 0$。\n\n**2. 误差传播与单坐标算子 ($M_i$)**\n\n现在我们用误差向量 $\\mathbf{e}^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{x}^{\\star}$ 来表示此更新。将 $\\mathbf{x}^{(k)} = \\mathbf{e}^{(k)} + \\mathbf{x}^{\\star}$ 代入更新规则：\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} (e_j^{(k)} + x_j^{\\star}) \\right)\n$$\n根据最优性条件 $A \\mathbf{x}^{\\star} = \\mathbf{b}$，我们有 $b_i = \\sum_{j=1}^{n} A_{ij} x_j^{\\star} = A_{ii} x_i^{\\star} + \\sum_{j \\neq i} A_{ij} x_j^{\\star}$。用此表达式替换 $b_i$：\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( \\left( A_{ii} x_i^{\\star} + \\sum_{j \\neq i} A_{ij} x_j^{\\star} \\right) - \\sum_{j \\neq i} A_{ij} e_j^{(k)} - \\sum_{j \\neq i} A_{ij} x_j^{\\star} \\right)\n$$\n简化表达式：\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( A_{ii} x_i^{\\star} - \\sum_{j \\neq i} A_{ij} e_j^{(k)} \\right) = x_i^{\\star} - \\frac{1}{A_{ii}} \\sum_{j \\neq i} A_{ij} e_j^{(k)}\n$$\n这给出了误差向量第 $i$ 个分量的更新：\n$$\ne_i^{(k+1)} = - \\sum_{j \\neq i} \\frac{A_{ij}}{A_{ii}} e_j^{(k)}\n$$\n在此步骤中，误差的其他分量保持不变：对于 $j \\neq i$，有 $e_j^{(k+1)} = e_j^{(k)}$。对误差向量 $\\mathbf{e}^{(k)}$ 的这种线性变换可以用矩阵-向量乘积 $\\mathbf{e}^{(k+1)} = M_i \\mathbf{e}^{(k)}$ 来表示。矩阵 $M_i$ 除了第 $i$ 行外，与单位矩阵 $I$ 相同。$M_i$ 的元素为：\n$$\n(M_i)_{jk} =\n\\begin{cases}\n\\delta_{jk}  \\text{if } j \\neq i \\\\\n-A_{ik}/A_{ii}  \\text{if } j = i, k \\neq i \\\\\n0  \\text{if } j = i, k = i\n\\end{cases}\n$$\n其中 $\\delta_{jk}$ 是克罗内克δ函数。\n\n**3. 循环周期迭代算子 ($M_{\\text{cyc}}$)**\n\n一个完整的循环周期涉及按固定顺序（例如 $i=1, 2, \\dots, n$）更新坐标。如果 $\\mathbf{e}^{(k)}$ 是一个周期开始时的误差，那么更新坐标1后的误差是 $M_1 \\mathbf{e}^{(k)}$。随后更新坐标2后的误差是 $M_2 (M_1 \\mathbf{e}^{(k)})$，依此类推。经过一个完整的周期后，最终的误差 $\\mathbf{e}^{(k+1)}$ 为：\n$$\n\\mathbf{e}^{(k+1)} = M_n M_{n-1} \\cdots M_2 M_1 \\mathbf{e}^{(k)}\n$$\n一个完整循环周期的迭代矩阵是各个算子的乘积：\n$$\nM_{\\text{cyc}} = M_n M_{n-1} \\cdots M_1\n$$\n渐进收敛速率由谱半径 $\\rho(M_{\\text{cyc}})$ 决定。\n\n**4. 随机步和随机周期算子 ($M_{\\text{rand\\_step}}$, $M_{\\text{rand\\_epoch}}$)**\n\n在随机坐标下降法中，在每一步 $k$，从 $\\{1, \\dots, n\\}$ 中以概率 $p_{i_k}$ 选择一个坐标 $i_k$，其中 $\\mathbf{p} = [p_1, \\dots, p_n]^{\\top}$ 是一个概率分布。误差更新则为 $\\mathbf{e}^{(k+1)} = M_{i_k} \\mathbf{e}^{(k)}$。\n\n在给定当前误差 $\\mathbf{e}^{(k)}$ 的条件下，一步之后的期望误差为：\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+1)} | \\mathbf{e}^{(k)}] = \\sum_{j=1}^{n} p_j (M_j \\mathbf{e}^{(k)}) = \\left( \\sum_{j=1}^{n} p_j M_j \\right) \\mathbf{e}^{(k)}\n$$\n这表明期望误差遵循线性迭代。相应的算子是期望单步随机迭代矩阵：\n$$\nM_{\\text{rand\\_step}} = \\mathbb{E}[M_{i_k}] = \\sum_{j=1}^{n} p_j M_j\n$$\n期望误差的收敛性由 $\\rho(M_{\\text{rand\\_step}})$ 控制。\n\n一个随机周期定义为 $n$ 个独立的随机步。设所选索引的序列为 $i_1, i_2, \\dots, i_n$。$n$ 步后的误差为 $\\mathbf{e}^{(k+n)} = M_{i_n} \\cdots M_{i_1} \\mathbf{e}^{(k)}$。此周期后的期望误差为：\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+n)} | \\mathbf{e}^{(k)}] = \\mathbb{E}[M_{i_n} \\cdots M_{i_1}] \\mathbf{e}^{(k)}\n$$\n由于坐标选择的独立性，乘积的期望等于期望的乘积：\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+n)} | \\mathbf{e}^{(k)}] = \\mathbb{E}[M_{i_n}] \\cdots \\mathbb{E}[M_{i_1}] \\mathbf{e}^{(k)} = (M_{\\text{rand\\_step}})^n \\mathbf{e}^{(k)}\n$$\n因此，一个随机周期的算子是 $M_{\\text{rand\\_epoch}} = (M_{\\text{rand\\_step}})^n$。\n\n此周期算子的谱半径可以通过使用性质 $\\rho(M^k) = (\\rho(M))^k$（对于任何方阵 $M$ 和整数 $k \\ge 1$）与单步算子的谱半径关联起来。因此：\n$$\n\\rho(M_{\\text{rand\\_epoch}}) = \\rho((M_{\\text{rand\\_step}})^n) = (\\rho(M_{\\text{rand\\_step}}))^n\n$$\n这使我们能够通过比较 $\\rho(M_{\\text{cyc}})$ 和 $\\rho(M_{\\text{rand\\_epoch}})$ 来比较循环和随机方案的每周期收敛速率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(A, p):\n    \"\"\"\n    Computes the spectral radii for cyclic and randomized coordinate descent.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the quadratic form.\n        p (np.ndarray): The probability distribution for randomized coordinate selection.\n\n    Returns:\n        tuple: A tuple containing:\n            - rho_cyc (float): Spectral radius of the cyclic epoch operator.\n            - rho_rand_step (float): Spectral radius of the one-step randomized operator.\n            - rho_rand_epoch (float): Spectral radius of the n-step randomized epoch operator.\n            - is_faster (bool): True if randomized epoch has a strictly smaller spectral radius.\n    \"\"\"\n    n = A.shape[0]\n\n    # 1. Construct the per-coordinate iteration operators M_i\n    m_coords = []\n    for i in range(n):\n        Mi = np.identity(n)\n        # The i-th row is updated based on the derivation\n        for j in range(n):\n            if i == j:\n                Mi[i, j] = 0.0\n            else:\n                Mi[i, j] = -A[i, j] / A[i, i]\n        m_coords.append(Mi)\n\n    # 2. Construct the cyclic-epoch iteration matrix M_cyc = M_{n-1} ... M_1 M_0\n    # The order of updates is 0, 1, ..., n-1\n    M_cyc = np.identity(n)\n    for i in range(n):\n        # Product is M_{n-1} ... M_0, but since indices are 0-based, it's M_{n-1} ... M_0\n        # The derivation uses M_n ... M_1 for 1-based indexing.\n        # With 0-based indexing it's M_{n-1} ... M_0\n        # Here we follow the derivation's logic: M_n M_{n-1} ... M_1\n        # which means M[n-1] @ M[n-2] @ ... @ M[0]\n        # Let's use a fixed order 0, 1, ..., n-1\n        # e_new = M_{n-1} @ ( ... @ (M_0 @ e_old) )\n        # So M_cyc = M_{n-1} @ ... @ M_0\n        pass\n    \n    # A more robust way to compute the product M_{n-1} * ... * M_0\n    M_cyc_prod = m_coords[0]\n    for i in range(1, n):\n        M_cyc_prod = m_coords[i] @ M_cyc_prod\n\n\n    # 3. Construct the expected one-step randomized iteration matrix M_rand_step\n    M_rand_step = np.zeros((n, n))\n    for i in range(n):\n        M_rand_step += p[i] * m_coords[i]\n\n    # 4. Compute spectral radii\n    # For a complex number z = x + iy, np.abs(z) computes sqrt(x^2 + y^2)\n    rho_cyc = np.max(np.abs(np.linalg.eigvals(M_cyc_prod)))\n    rho_rand_step = np.max(np.abs(np.linalg.eigvals(M_rand_step)))\n    \n    # The spectral radius of the randomized epoch operator is (rho_rand_step)^n\n    rho_rand_epoch = rho_rand_step ** n\n\n    # 5. Compare the epoch spectral radii\n    is_faster = rho_rand_epoch  rho_cyc\n\n    return rho_cyc, rho_rand_step, rho_rand_epoch, is_faster\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 3,\n            \"A\": np.array([\n                [4.0, 0.0, 0.0],\n                [0.0, 5.0, 0.0],\n                [0.0, 0.0, 6.0]\n            ]),\n            \"p_type\": \"uniform\"\n        },\n        {\n            \"n\": 3,\n            \"A\": np.array([\n                [4.0, -0.9, 0.2],\n                [-0.9, 3.5, -0.4],\n                [0.2, -0.4, 2.8]\n            ]),\n            \"p_type\": \"uniform\"\n        },\n        {\n            \"n\": 5,\n            \"A\": np.array([\n                [8.0, -0.5, 0.2, 0.0, 0.1],\n                [-0.5, 7.0, 0.3, -0.4, 0.0],\n                [0.2, 0.3, 9.0, -0.6, 0.5],\n                [0.0, -0.4, -0.6, 6.0, -0.3],\n                [0.1, 0.0, 0.5, -0.3, 5.0]\n            ]),\n            \"p_type\": \"proportional\"\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        n = case[\"n\"]\n        \n        if case[\"p_type\"] == \"uniform\":\n            p = np.full(n, 1.0 / n)\n        elif case[\"p_type\"] == \"proportional\":\n            diagonals = np.diag(A)\n            p = diagonals / np.sum(diagonals)\n        \n        # Correctly compute metrics, assuming the order of cyclic updates is 0, 1, ..., n-1\n        n = A.shape[0]\n        m_coords = []\n        for i in range(n):\n            Mi = np.identity(n)\n            for j in range(n):\n                if i == j:\n                    Mi[i, j] = 0.0\n                else:\n                    Mi[i, j] = -A[i, j] / A[i, i]\n            m_coords.append(Mi)\n        \n        # Cyclic operator M_cyc = M_{n-1} ... M_1 M_0\n        M_cyc = m_coords[0]\n        for i in range(1, n):\n            M_cyc = m_coords[i] @ M_cyc\n\n        M_rand_step = np.zeros((n, n))\n        for i in range(n):\n            M_rand_step += p[i] * m_coords[i]\n            \n        rho_cyc = np.max(np.abs(np.linalg.eigvals(M_cyc)))\n        rho_rand_step = np.max(np.abs(np.linalg.eigvals(M_rand_step)))\n        rho_rand_epoch = rho_rand_step ** n\n        is_faster = rho_rand_epoch  rho_cyc\n        \n        metrics = (rho_cyc, rho_rand_step, rho_rand_epoch, is_faster)\n        results.extend(metrics)\n\n    # Format the final output string\n    # Booleans will be converted to \"True\" or \"False\" by str()\n    # Floats will be in standard decimal representation\n    results_str = []\n    for item in results:\n        if isinstance(item, bool):\n            results_str.append(str(item).lower())\n        else:\n            results_str.append(str(item))\n\n    output_str = f\"[{','.join(results_str)}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}