{
    "hands_on_practices": [
        {
            "introduction": "理解像牛顿法这样强大的算法，始于掌握其最基本的构成单元：单步迭代。第一个练习将引导你完成一个简单的双变量系统的牛顿法核心步骤 。通过亲手计算雅可比矩阵并求解由此产生的线性方程组，你将具体地理解该方法如何利用局部导数信息来逐步逼近解。",
            "id": "2207863",
            "problem": "考虑以下二元非线性方程组：\n$$\n\\begin{cases}\n2x^2 + y = 11 \\\\\nx + 2y^2 = 10\n\\end{cases}\n$$\n我们用适用于方程组的牛顿法来求此系统的近似解。从初始猜测 $(x_0, y_0) = (3, 1)$ 开始，进行单次迭代以求得下一个近似值 $(x_1, y_1)$。\n\n求 $(x_1, y_1)$ 的坐标。将每个坐标表示为最简有理数形式。",
            "solution": "定义向量函数 $\\mathbf{F}(x,y)$ 及其雅可比矩阵 $J(x,y)$ 如下：\n$$\n\\mathbf{F}(x,y)=\\begin{pmatrix} 2x^{2}+y-11 \\\\ x+2y^{2}-10 \\end{pmatrix}, \n\\quad\nJ(x,y)=\\begin{pmatrix} \\frac{\\partial}{\\partial x}(2x^{2}+y-11)  \\frac{\\partial}{\\partial y}(2x^{2}+y-11) \\\\ \\frac{\\partial}{\\partial x}(x+2y^{2}-10)  \\frac{\\partial}{\\partial y}(x+2y^{2}-10) \\end{pmatrix}\n=\\begin{pmatrix} 4x  1 \\\\ 1  4y \\end{pmatrix}.\n$$\n适用于方程组的牛顿法通过求解\n$$\nJ(x_{0},y_{0})\\,\\mathbf{s}=-\\mathbf{F}(x_{0},y_{0}),\n$$\n来计算更新量 $\\mathbf{s}=(s_{x},s_{y})^{T}$，然后令 $(x_{1},y_{1})=(x_{0},y_{0})+\\mathbf{s}$。\n\n在点 $(x_{0},y_{0})=(3,1)$ 处，计算可得\n$$\n\\mathbf{F}(3,1)=\\begin{pmatrix} 2\\cdot 3^{2}+1-11 \\\\ 3+2\\cdot 1^{2}-10 \\end{pmatrix}\n=\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix},\n\\quad\nJ(3,1)=\\begin{pmatrix} 4\\cdot 3  1 \\\\ 1  4\\cdot 1 \\end{pmatrix}\n=\\begin{pmatrix} 12  1 \\\\ 1  4 \\end{pmatrix}.\n$$\n求解 $\\mathbf{s}$ 的方程为：\n$$\n\\begin{pmatrix} 12  1 \\\\ 1  4 \\end{pmatrix}\\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}\n=-\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix}\n=\\begin{pmatrix} -8 \\\\ 5 \\end{pmatrix},\n$$\n即线性方程组\n$$\n\\begin{cases}\n12s_{x}+s_{y}=-8, \\\\\ns_{x}+4s_{y}=5.\n\\end{cases}\n$$\n由 $s_{x}=5-4s_{y}$ 代入第一个方程，可得：\n$$\n12(5-4s_{y})+s_{y}=-8\n\\;\\Rightarrow\\;\n60-48s_{y}+s_{y}=-8\n\\;\\Rightarrow\\;\n-47s_{y}=-68\n\\;\\Rightarrow\\;\ns_{y}=\\frac{68}{47}.\n$$\n则\n$$\ns_{x}=5-4\\cdot \\frac{68}{47}\n=\\frac{235}{47}-\\frac{272}{47}\n=-\\frac{37}{47}.\n$$\n更新近似值：\n$$\nx_{1}=x_{0}+s_{x}=3-\\frac{37}{47}=\\frac{141}{47}-\\frac{37}{47}=\\frac{104}{47}, \n\\quad\ny_{1}=y_{0}+s_{y}=1+\\frac{68}{47}=\\frac{47}{47}+\\frac{68}{47}=\\frac{115}{47}.\n$$\n因此，下一个牛顿迭代值是 $\\left(\\frac{104}{47}, \\frac{115}{47}\\right)$，其两个坐标均为最简有理数形式。",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{104}{47}  \\frac{115}{47}\\end{pmatrix}}$$"
        },
        {
            "introduction": "虽然解析雅可比矩阵在教科书问题中很完美，但在实际应用中却可能成为一个主要瓶颈。这个练习将你从手动计算带入实际编程，探索一种雅可比矩阵被数值近似的拟牛顿法 。你将实现并比较使用精确雅可比矩阵的求解器和使用有限差分近似的求解器，从而能够直接研究计算便利性与收敛速度之间的权衡。",
            "id": "2441924",
            "problem": "考虑由向量值函数 $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ 定义的非线性方程组，该函数由下式给出\n$$\n\\mathbf{F}(\\mathbf{x})=\n\\begin{bmatrix}\nf_1(x_1,x_2)\\\\\nf_2(x_1,x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1-\\cos(x_2)\\\\\nx_2-\\sin(x_1)\n\\end{bmatrix}\n$$\n其中 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ 且三角函数使用以弧度为单位的角度。$\\mathbf{F}$ 的精确雅可比矩阵 $\\mathbf{J}(\\mathbf{x})$ 是\n$$\n\\mathbf{J}(\\mathbf{x})=\n\\begin{bmatrix}\n\\dfrac{\\partial f_1}{\\partial x_1}  \\dfrac{\\partial f_1}{\\partial x_2}\\\\\n\\dfrac{\\partial f_2}{\\partial x_1}  \\dfrac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  \\sin(x_2)\\\\\n-\\cos(x_1)  1\n\\end{bmatrix}.\n$$\n\n对于给定的扰动大小 $h0$，定义前向有限差分雅可比近似 $\\mathbf{J}_h(\\mathbf{x})$ 为\n$$\n\\mathbf{J}_h(\\mathbf{x})=\\begin{bmatrix}\n\\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_1(\\mathbf{x})}{h}  \\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_1(\\mathbf{x})}{h}\\\\\n\\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_2(\\mathbf{x})}{h}  \\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_2(\\mathbf{x})}{h}\n\\end{bmatrix}\n$$\n其中 $\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ 且 $\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$。\n\n对于一个初始猜测 $\\mathbf{x}^{(0)}\\in\\mathbb{R}^2$，通过下式递归定义序列 $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$\n$$\n\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big),\\qquad \\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)}\n$$\n其中 $\\mathbf{J}_\\star$ 表示精确雅可比矩阵 $\\mathbf{J}$ 或有限差分雅可比矩阵 $\\mathbf{J}_h$，更新量 $\\mathbf{s}^{(k)}$ 是该线性系统的任意解。设残差范数为 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$，迭代在满足 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon$ 的最小索引 $k$ 处终止，容差为 $\\varepsilon=10^{-10}$，或在 $k_{\\max}=50$ 次迭代后终止，以先发生者为准。\n\n对于下述每个测试用例，从给定的初始猜测 $\\mathbf{x}^{(0)}$ 开始执行两次运行：\n- 运行 A 使用 $\\mathbf{J}_\\star=\\mathbf{J}$。\n- 运行 B 使用 $\\mathbf{J}_\\star=\\mathbf{J}_h$ 以及指定的 $h$。\n\n对于每次运行，记录：\n- 满足 $\\|\\mathbf{F}(\\mathbf{x}^{(n)})\\|_2 \\le \\varepsilon$ 所需的迭代次数 $n$（如果未达到容差，则 $n=k_{\\max}$）。\n- 局部收敛阶的估计值 $\\hat{p}$，通过最后三个可用的残差范数 $\\{e_{m-2},e_{m-1},e_m\\}$ 计算得出\n$$\n\\hat{p}=\\frac{\\ln\\left(\\dfrac{e_m}{e_{m-1}}\\right)}{\\ln\\left(\\dfrac{e_{m-1}}{e_{m-2}}\\right)},\n$$\n其中 $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$，而 $m$ 是运行中使用的最终迭代索引（使用终止时可用的最后三个残差；所有对数均为自然对数）。三角函数中的所有角度均以弧度为单位。\n\n使用以下测试套件，其中每个用例是一对 $(\\mathbf{x}^{(0)},h)$：\n- 用例 1：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$，$h=10^{-6}$。\n- 用例 2：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$，$h=10^{-3}$。\n- 用例 3：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$，$h=10^{-6}$。\n- 用例 4：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$，$h=10^{-2}$。\n\n您的程序必须输出一行，其中包含一个结果列表，每个测试用例一个结果，顺序与所列顺序相同。每个测试用例的结果必须是一个包含四个条目的列表 $[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}]$，其中 $n_{\\text{exact}}$ 和 $\\hat{p}_{\\text{exact}}$ 对应运行 A，而 $n_{\\text{fd}}$ 和 $\\hat{p}_{\\text{fd}}$ 对应运行 B。最终输出格式必须是单行，为这些按用例排列的列表的逗号分隔列表，并用方括号括起来，例如 $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\dots]$。",
            "solution": "该问题已经过验证。\n\n### 第 1 步：提取已知条件\n- **非线性系统**：一个函数 $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ 定义为 $\\mathbf{F}(\\mathbf{x})= \\begin{bmatrix} f_1(x_1,x_2)\\\\ f_2(x_1,x_2) \\end{bmatrix} = \\begin{bmatrix} x_1-\\cos(x_2)\\\\ x_2-\\sin(x_1) \\end{bmatrix}$，其中 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$。三角函数使用弧度。\n- **精确雅可比矩阵**：$\\mathbf{F}$ 的雅可比矩阵由 $\\mathbf{J}(\\mathbf{x}) = \\begin{bmatrix} 1  \\sin(x_2)\\\\ -\\cos(x_1)  1 \\end{bmatrix}$ 给出。\n- **有限差分雅可比矩阵**：对于扰动 $h0$，定义了一个近似 $\\mathbf{J}_h(\\mathbf{x})$，其列由下式给出：第 $j$ 列是 $\\frac{\\mathbf{F}(\\mathbf{x}+h\\,\\mathbf{e}_j)-\\mathbf{F}(\\mathbf{x})}{h}$，其中 $\\mathbf{e}_j$ 是标准基向量。\n- **迭代方案**：从初始猜测 $\\mathbf{x}^{(0)}$ 开始，通过求解 $\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big)$ 并设置 $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)}$ 来生成序列 $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$。此处，$\\mathbf{J}_\\star$ 是精确雅可比矩阵 $\\mathbf{J}$ 或近似 $\\mathbf{J}_h$。\n- **终止准则**：迭代在残差范数 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon = 10^{-10}$ 的最小索引 $k$ 处停止，或在 $k_{\\max}=50$ 次迭代后停止。\n- **任务**：对于每个测试用例，执行两次运行：运行 A 使用 $\\mathbf{J}_\\star=\\mathbf{J}$，运行 B 使用 $\\mathbf{J}_\\star=\\mathbf{J}_h$。对于每次运行，需要记录两个量：\n    1. 迭代次数，$n$。\n    2. 收敛阶的估计值，$\\hat{p}=\\frac{\\ln(e_m/e_{m-1})}{\\ln(e_{m-1}/e_{m-2})}$，其中 $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$ 且 $m$ 是最终迭代索引。\n- **测试用例**：\n    - 用例 1：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$，$h=10^{-6}$。\n    - 用例 2：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$，$h=10^{-3}$。\n    - 用例 3：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$，$h=10^{-6}$。\n    - 用例 4：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$，$h=10^{-2}$。\n- **输出格式**：单行列表的列表：$[[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}], \\dots]$。\n\n### 第 2 步：使用提取的已知条件进行验证\n评估该问题的有效性。\n- **科学依据**：该问题是数值分析中的一个标准练习，特别是在求解非线性方程组的计算方法领域。牛顿法及其拟牛顿变体（使用有限差分雅可比矩阵）是基础且成熟的算法。数学公式是正确的。\n- **适定性**：问题定义清晰。函数、雅可比矩阵、迭代方案、终止条件和所需输出都已明确规定。给定的方程组在感兴趣的域内有唯一解，并且雅可比矩阵在该解附近非奇异，确保了待求解的线性系统是适定的。例如，雅可比矩阵的行列式是 $\\det(\\mathbf{J}) = 1 + \\cos(x_1)\\sin(x_2)$。对于像 $(0.5, 0.5)$ 或 $(1.0, 1.0)$ 这样的初始猜测，行列式远非零，表明局部收敛是可实现的。\n- **客观性**：语言正式、客观，没有主观或非科学内容。\n\n该问题未表现出任何缺陷，如科学上不健全、不完整、矛盾或含糊不清。$\\hat{p}$ 的计算至少需要三个残差范数，对应于至少两次迭代（$n\\ge 2$）。考虑到初始条件和牛顿法的性质，这是一个合理的期望。\n\n### 第 3 步：结论与行动\n该问题是**有效**的。将提供一个解答。\n\n---\n\n该问题要求实现牛顿法的两种变体来求解非线性系统 $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$。第一种变体是经典的牛顿-拉弗森法，它使用 $\\mathbf{F}$ 的精确解析雅可比矩阵。第二种是拟牛顿法，其中雅可比矩阵使用前向有限差分格式进行近似。我们将从收敛所需的迭代次数和估计的局部收敛阶两个方面比较这两种方法的性能。\n\n该方法的核心是迭代更新规则。在每一步 $k$，我们用非线性函数 $\\mathbf{F}$ 在当前迭代点 $\\mathbf{x}^{(k)}$ 附近的线性泰勒展开来近似它：\n$$\n\\mathbf{F}(\\mathbf{x}) \\approx \\mathbf{F}(\\mathbf{x}^{(k)}) + \\mathbf{J}_\\star(\\mathbf{x}^{(k)})(\\mathbf{x} - \\mathbf{x}^{(k)})\n$$\n我们通过将此近似设为零来寻找下一个迭代点 $\\mathbf{x}^{(k+1)}$，即 $\\mathbf{F}(\\mathbf{x}^{(k+1)}) = \\mathbf{0}$。将更新步长定义为 $\\mathbf{s}^{(k)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}$，我们得到用于更新的线性系统：\n$$\n\\mathbf{J}_\\star(\\mathbf{x}^{(k)}) \\mathbf{s}^{(k)} = -\\mathbf{F}(\\mathbf{x}^{(k)})\n$$\n一旦通过求解该系统找到 $\\mathbf{s}^{(k)}$，下一个迭代点就计算为 $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)}$。重复此过程，直到残差向量的范数 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$ 降至指定的容差 $\\varepsilon = 10^{-10}$ 以下。\n\n**运行 A：精确雅可比矩阵（牛顿-拉弗森法）**\n在这次运行中，$\\mathbf{J}_\\star$ 是精确的雅可比矩阵 $\\mathbf{J}(\\mathbf{x})$：\n$$\n\\mathbf{J}(\\mathbf{x}) =\n\\begin{bmatrix}\n1  \\sin(x_2)\\\\\n-\\cos(x_1)  1\n\\end{bmatrix}\n$$\n已知该方法在初始猜测足够接近解且雅可比矩阵在解处非奇异时，表现出二次收敛（即收敛阶 $p=2$）。这意味着解的正确位数大约每次迭代都会翻倍。因此，估计的收敛阶 $\\hat{p}$ 应接近 2。\n\n**运行 B：有限差分雅可比矩阵（拟牛顿法）**\n在这次运行中，雅可比矩阵使用前向有限差分公式进行近似。近似雅可比矩阵 $\\mathbf{J}_h(\\mathbf{x})$ 的第 $j$ 列由下式给出：\n$$\n[\\mathbf{J}_h(\\mathbf{x})]_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h}\n$$\n其中 $\\mathbf{e}_j$ 是第 $j$ 个标准基向量，而 $h$ 是一个小的扰动参数。对于给定的 $2 \\times 2$ 系统，这得到：\n$$\n\\mathbf{J}_h(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{f_1(x_1+h, x_2) - f_1(x_1, x_2)}{h}  \\frac{f_1(x_1, x_2+h) - f_1(x_1, x_2)}{h} \\\\\n\\frac{f_2(x_1+h, x_2) - f_2(x_1, x_2)}{h}  \\frac{f_2(x_1, x_2+h) - f_2(x_1, x_2)}{h}\n\\end{bmatrix}\n$$\n这种方法避免了对雅可比矩阵进行解析推导的需要，对于某些函数来说，这可能很复杂或不可能。然而，它引入了近似误差。$\\mathbf{J}_h$ 的每个元素中的误差是 $O(h)$ 阶的。这个误差会扰动牛顿步长，影响收敛速度。对于一个非常小的 $h$（例如，$10^{-6}$），近似是精确的，收敛应该接近二次。随着 $h$ 的增加（例如，$10^{-3}$ 或 $10^{-2}$），近似变差，收敛速度预计会下降，可能变为线性（$p=1$），并需要更多次迭代。\n\n**算法与实现**\n对于每个测试用例 $(\\mathbf{x}^{(0)}, h)$，我们对运行 A 和运行 B 都执行以下步骤：\n$1$. 初始化 $k=0$ 和当前解 $\\mathbf{x} = \\mathbf{x}^{(0)}$。创建一个列表来存储残差范数。\n$2$. 开始一个循环，只要 $k \\le k_{\\max} = 50$ 就继续。\n$3$. 计算残差向量 $\\mathbf{F}(\\mathbf{x})$ 及其欧几里得范数 $e_k = \\|\\mathbf{F}(\\mathbf{x})\\|_2$。存储 $e_k$。\n$4$. 检查终止条件：如果 $e_k \\le \\varepsilon=10^{-10}$，设置最终迭代次数 $n=k$ 并退出循环。\n$5$. 如果循环要继续（即 $k  k_{\\max}$），计算相应的雅可比矩阵 $\\mathbf{J}_\\star(\\mathbf{x})$（精确的或有限差分的）。\n$6$. 求解线性系统 $\\mathbf{J}_\\star(\\mathbf{x})\\mathbf{s} = -\\mathbf{F}(\\mathbf{x})$ 以获得步长 $\\mathbf{s}$。\n$7$. 更新解：$\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}$。\n$8$. 增加迭代计数器：$k \\leftarrow k+1$。\n$9$. 如果循环因为 $k$ 达到 $k_{\\max}$ 而完成，则设置 $n=k_{\\max}$。\n$10$. 循环终止后，使用最后三个可用的残差范数 $e_{n-2}, e_{n-1}, e_{n}$ 计算估计的收敛阶 $\\hat{p}$。如果可用的范数少于三个（即 $n2$），则认为 $\\hat{p}$ 不可计算。\n收集每个测试用例两次运行的结果 $(n, \\hat{p})$ 以形成最终输出。这个过程将在实际的计算环境中展示牛顿族方法的理论特性。",
            "answer": "```python\nimport numpy as np\n\n# Global constants as specified in the problem\nTOLERANCE = 1e-10\nK_MAX = 50\n\ndef F(x):\n    \"\"\"\n    Computes the vector-valued function F(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        x[0] - np.cos(x[1]),\n        x[1] - np.sin(x[0])\n    ])\n\ndef J_exact(x):\n    \"\"\"\n    Computes the exact Jacobian matrix J(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        [1.0, np.sin(x[1])],\n        [-np.cos(x[0]), 1.0]\n    ])\n\ndef J_fd(x, h):\n    \"\"\"\n    Computes the forward finite-difference approximation of the Jacobian.\n    x must be a NumPy array [x1, x2].\n    h is the perturbation size.\n    \"\"\"\n    n = len(x)\n    jac = np.zeros((n, n), dtype=float)\n    fx = F(x)\n    for j in range(n):\n        x_plus_h = x.copy()\n        x_plus_h[j] += h\n        fx_plus_h = F(x_plus_h)\n        jac[:, j] = (fx_plus_h - fx) / h\n    return jac\n\ndef newton_solver(x0, h, use_exact_jacobian):\n    \"\"\"\n    Solves the nonlinear system F(x)=0 using a Newton-like method.\n    \n    Args:\n        x0 (list or np.ndarray): The initial guess.\n        h (float): The perturbation size for the finite-difference Jacobian.\n        use_exact_jacobian (bool): If True, use the exact Jacobian. \n                                  If False, use the finite-difference approximation.\n\n    Returns:\n        tuple: (n_iter, p_hat) where n_iter is the number of iterations and\n               p_hat is the estimated convergence order.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    k = 0\n    residuals = []\n    n_iter = K_MAX\n\n    while k = K_MAX:\n        F_val = F(x)\n        norm = np.linalg.norm(F_val)\n        residuals.append(norm)\n\n        if norm = TOLERANCE:\n            n_iter = k\n            break\n        \n        if k == K_MAX:\n            break\n\n        if use_exact_jacobian:\n            Jacobian = J_exact(x)\n        else:\n            Jacobian = J_fd(x, h)\n\n        try:\n            # Solve the linear system J*s = -F\n            s = np.linalg.solve(Jacobian, -F_val)\n            x += s\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, the solver fails.\n            n_iter = K_MAX\n            break\n        \n        k += 1\n    \n    # Estimate convergence order p_hat from the last three residuals\n    p_hat = np.nan\n    if len(residuals) >= 3:\n        # Use residuals e_n, e_{n-1}, e_{n-2}\n        e_m, e_m1, e_m2 = residuals[-1], residuals[-2], residuals[-3]\n        \n        # Avoid division by zero or log of non-positive numbers.\n        # Ratios must be  1 for convergence.\n        ratio1 = e_m / e_m1 if e_m1 != 0 else 0\n        ratio2 = e_m1 / e_m2 if e_m2 != 0 else 0\n\n        if 0  ratio1  1 and 0  ratio2  1:\n            p_hat = np.log(ratio1) / np.log(ratio2)\n            \n    return n_iter, p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        ([0.5, 0.5], 1e-6),\n        ([0.5, 0.5], 1e-3),\n        ([1.0, 1.0], 1e-6),\n        ([1.0, 1.0], 1e-2)\n    ]\n\n    all_results = []\n    for x0, h in test_cases:\n        # Run A: Exact Jacobian\n        n_exact, p_exact = newton_solver(x0, h, use_exact_jacobian=True)\n        \n        # Run B: Finite-Difference Jacobian\n        n_fd, p_fd = newton_solver(x0, h, use_exact_jacobian=False)\n        \n        # Assemble results for the current test case\n        case_result = [n_exact, n_fd, p_exact, p_fd]\n        all_results.append(case_result)\n\n    # The final print statement must match the format exactly.\n    # The default string representation of a list of lists is \"[...], [...]\"\n    # and the default representation of a float is what we need.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "当面对困难的、病态的问题时，牛顿法才能真正展现其威力。这个高级实践向你提出了一个挑战：求解一个其误差地形成一个狭窄、弯曲峡谷的系统——这对简单的算法来说是一个臭名昭著的陷阱 。通过实现牛頓法并与最速下降法进行比较，你将亲眼见证，为何使用来自雅可比矩阵的二阶信息对于高效地穿越此类复杂地形并找到解决方案至关重要。",
            "id": "3200259",
            "problem": "你需要研究一个二维非线性方程组的数值解行为，当存在一个很小的参数时，该方程组的残差范数地景呈现出狭窄弯曲的峡谷形态。设该系统由映射 $F_{\\varepsilon}:\\mathbb{R}^2\\to\\mathbb{R}^2$ 定义，其形式为\n$$\nF_{\\varepsilon}(x,y) = \\begin{bmatrix} \\varepsilon x + y^2 - 1 \\\\ x^2 - y \\end{bmatrix}\n$$\n其中 $\\varepsilon > 0$ 是一个参数。定义标量目标函数\n$$\n\\phi(x,y) = \\tfrac{1}{2}\\,\\lVert F_{\\varepsilon}(x,y)\\rVert_2^2.\n$$\n从复合函数的梯度通过链式法则获得，以及 $F_{\\varepsilon}(x,y)$ 的雅可比矩阵 $J(x,y)$ 是 $F_{\\varepsilon}$ 关于 $(x,y)$ 的一阶偏导数矩阵这些基本定义出发。仅使用这些基础，推导给定 $F_{\\varepsilon}(x,y)$ 的 $\\phi(x,y)$ 的梯度和雅可比矩阵 $J(x,y)$。然后，设计并实现两种方法来尝试求解 $F_{\\varepsilon}(x,y)=\\mathbf{0}$ 的根：\n- 方法 A：应用于 $\\phi(x,y)$ 的最速下降法，并结合回溯 Armijo 线搜索。取当前迭代点 $\\phi(x,y)$ 的负梯度作为下降方向，并使用回溯法来确保 $\\phi(x,y)$ 的充分下降。\n- 方法 B：用于方程组的牛顿法，并结合回溯 Armijo 线搜索。在每次迭代中，求解线性化系统以获得更新方向，然后使用回溯法来确保 $\\phi(x,y)$ 的充分下降。\n\n你的实现必须是完全确定性的，并且两种方法必须使用相同的停止和线搜索规则：\n- 当 $\\lVert F_{\\varepsilon}(x_k,y_k)\\rVert_2 \\leq \\tau_F$（其中 $\\tau_F = 10^{-10}$）时，成功停止。\n- 如果步长满足 $\\alpha_k \\lVert d_k\\rVert_2 \\leq \\tau_X$（其中 $\\tau_X = 10^{-12}$），或者当迭代次数达到该方法允许的最大迭代次数时，也（非成功地）停止。\n- 对于回溯法，初始步长为 $\\alpha = 1$，然后通过乘以因子 $\\tau_{\\text{ls}}$ 来减小步长，直到满足 Armijo 条件，其中 $\\tau_{\\text{ls}} = 0.5$。两种方法均使用 Armijo 条件参数 $c_1 = 10^{-4}$。每次迭代最多进行 $50$ 次回溯缩减作为硬性上限。\n\n使用以下最大迭代次数限制：\n- 对于方法 A（最速下降法）：$N_{\\max}^{\\text{SD}} = 5000$ 次迭代。\n- 对于方法 B（牛顿法）：$N_{\\max}^{\\text{N}} = 100$ 次迭代。\n\n你的程序必须对下面的每个测试用例运行这两种方法，并为每个用例按顺序报告以下列表：方法 A 所用的迭代次数（非负整数）、方法 B 所用的迭代次数（非负整数）、一个布尔值指示方法 A 是否成功终止（如上定义），以及一个布尔值指示方法 B 是否成功终止（如上定义）。\n\n使用以下参数-初始值对 $(\\varepsilon,(x_0,y_0))$ 的测试套件：\n- 用例 1：$\\varepsilon = 10^{-3}$, $(x_0,y_0) = (0.5,\\,0.5)$。\n- 用例 2：$\\varepsilon = 10^{-6}$, $(x_0,y_0) = (0.5,\\,0.5)$。\n- 用例 3：$\\varepsilon = 2\\times 10^{-1}$, $(x_0,y_0) = (0.5,\\,0.5)$。\n- 用例 4：$\\varepsilon = 0$, $(x_0,y_0) = (0.5,\\,0.5)$。\n- 用例 5：$\\varepsilon = 10^{-3}$, $(x_0,y_0) = (-1.5,\\,1.3)$。\n\n关于科学真实性和预期的说明：对于较小的 $\\varepsilon$，$\\phi(x,y)$ 的等值线集形成一个狭窄弯曲的峡谷，这可能导致最速下降法以微小的步长呈之字形前进，而牛顿法在足够接近解且雅可比矩阵非奇异时，通常能利用曲率信息快速收敛。所提供的停止和线搜索参数是标准的，确保了两种方法之间具有可比的评判标准。\n\n最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。此外部列表的每个元素对应一个测试用例（按上述顺序列出），并且其本身必须是一个包含四个条目的列表 $[n_{\\text{SD}}, n_{\\text{N}}, s_{\\text{SD}}, s_{\\text{N}}]$，其中 $n_{\\text{SD}}$ 和 $n_{\\text{N}}$ 是非负整数，而 $s_{\\text{SD}}$ 和 $s_{\\text{N}}$ 是布尔值。例如，最终输出必须看起来像 $[[n_{1,\\text{SD}},n_{1,\\text{N}},s_{1,\\text{SD}},s_{1,\\text{N}}],[n_{2,\\text{SD}},n_{2,\\text{N}},s_{2,\\text{SD}},s_{2,\\text{N}}],\\dots]$，除了标准的逗号分隔外，没有额外的文本或空白要求。",
            "solution": "用户提供了一个计算科学领域中定义明确的问题。任务是为两种数值求根算法推导必要的数学量，实现这些算法，并将它们应用于一组指定的测试用例。该问题具有科学依据，内部逻辑一致，除了标准的数学和数值原理外，不需要任何外部信息。\n\n### 数学推导\n\n该非线性方程组由映射 $F_{\\varepsilon}:\\mathbb{R}^2\\to\\mathbb{R}^2$ 给出，其中向量函数 $F_{\\varepsilon}(x,y)$ 的分量为：\n$$\nF_{\\varepsilon}(x,y) = \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} \\varepsilon x + y^2 - 1 \\\\ x^2 - y \\end{bmatrix}\n$$\n衡量残差范数平方的标量目标函数 $\\phi(x,y)$ 定义为：\n$$\n\\phi(x,y) = \\frac{1}{2}\\,\\lVert F_{\\varepsilon}(x,y)\\rVert_2^2 = \\frac{1}{2} \\left[ (\\varepsilon x + y^2 - 1)^2 + (x^2 - y)^2 \\right]\n$$\n求解 $F_{\\varepsilon}(x,y) = \\mathbf{0}$ 的根等价于找到 $\\phi(x,y)$ 的全局最小值，在该点处 $\\phi(x,y)=0$。\n\n#### 雅可比矩阵 $J(x,y)$\n根据问题的定义，雅可比矩阵 $J(x,y)$ 是 $F_{\\varepsilon}(x,y)$ 关于变量向量 $(x,y)$ 的一阶偏导数矩阵。\n$$\nJ(x,y) = \\begin{bmatrix} \\frac{\\partial F_1}{\\partial x}  \\frac{\\partial F_1}{\\partial y} \\\\ \\frac{\\partial F_2}{\\partial x}  \\frac{\\partial F_2}{\\partial y} \\end{bmatrix}\n$$\n我们计算这四个偏导数：\n- $\\frac{\\partial F_1}{\\partial x} = \\frac{\\partial}{\\partial x}(\\varepsilon x + y^2 - 1) = \\varepsilon$\n- $\\frac{\\partial F_1}{\\partial y} = \\frac{\\partial}{\\partial y}(\\varepsilon x + y^2 - 1) = 2y$\n- $\\frac{\\partial F_2}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 - y) = 2x$\n- $\\frac{\\partial F_2}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 - y) = -1$\n\n将这些代入矩阵中，得到雅可比矩阵：\n$$\nJ(x,y) = \\begin{bmatrix} \\varepsilon  2y \\\\ 2x  -1 \\end{bmatrix}\n$$\n\n#### $\\phi(x,y)$ 的梯度\n问题要求使用链式法则推导 $\\phi(x,y)$ 的梯度，记为 $\\nabla\\phi(x,y)$。函数 $\\phi$ 是一个复合函数，形式为 $\\phi(\\mathbf{v}) = h(F(\\mathbf{v}))$，其中 $\\mathbf{v} = [x, y]^T$ 且 $h(\\mathbf{u}) = \\frac{1}{2}\\mathbf{u}^T\\mathbf{u}$。向量函数的链式法则表明 $\\nabla\\phi(\\mathbf{v}) = J(\\mathbf{v})^T \\nabla h(F(\\mathbf{v}))$。$h(\\mathbf{u})$ 的梯度就是 $\\mathbf{u}$。因此，我们有通用公式：\n$$\n\\nabla\\phi(x,y) = J(x,y)^T F_{\\varepsilon}(x,y)\n$$\n使用先前推导的 $J(x,y)$ 和 $F_{\\varepsilon}(x,y)$ 的定义，我们计算该乘积：\n$$\n\\nabla\\phi(x,y) = \\begin{bmatrix} \\varepsilon  2x \\\\ 2y  -1 \\end{bmatrix} \\begin{bmatrix} \\varepsilon x + y^2 - 1 \\\\ x^2 - y \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} \\varepsilon(\\varepsilon x + y^2 - 1) + 2x(x^2 - y) \\\\ 2y(\\varepsilon x + y^2 - 1) - 1(x^2 - y) \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} \\varepsilon^2 x + \\varepsilon y^2 - \\varepsilon + 2x^3 - 2xy \\\\ 2\\varepsilon xy + 2y^3 - 2y - x^2 + y \\end{bmatrix} = \\begin{bmatrix} \\varepsilon^2 x + \\varepsilon y^2 - \\varepsilon + 2x^3 - 2xy \\\\ 2\\varepsilon xy + 2y^3 - x^2 - y \\end{bmatrix}\n$$\n这个梯度对于最速下降法和 Armijo 线搜索条件都是必不可少的。\n\n### 算法设计\n\n两种方法都是迭代法，从初始猜测值 $\\mathbf{x}_0$ 开始，生成一个点序列 $\\mathbf{x}_k = [x_k, y_k]^T$。一般的更新步骤为 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k d_k$，其中 $d_k$ 是搜索方向，$\\alpha_k$ 是步长。\n\n#### 方法 A：带回溯的最速下降法\n此方法使用目标函数 $\\phi$ 的负梯度作为其搜索方向。这是局部最速下降的方向。\n1.  **搜索方向**：在迭代点 $\\mathbf{x}_k$ 处，方向为 $d_k = -\\nabla\\phi(\\mathbf{x}_k)$。\n2.  **步长**：步长 $\\alpha_k$ 通过回溯线搜索确定。从 $\\alpha=1$ 开始，步长通过乘以因子 $\\tau_{\\text{ls}} = 0.5$ 不断减小，直到满足充分下降的 Armijo 条件：\n    $$\n    \\phi(\\mathbf{x}_k + \\alpha d_k) \\le \\phi(\\mathbf{x}_k) + c_1 \\alpha \\nabla\\phi(\\mathbf{x}_k)^T d_k\n    $$\n    其中 $c_1 = 10^{-4}$。\n\n#### 方法 B：带回溯的牛顿法\n此方法利用二阶信息（通过雅可比矩阵）在当前点建立系统的线性模型，并求解使该模型为零的步长。\n1.  **搜索方向**：在迭代点 $\\mathbf{x}_k$ 处，通过求解牛顿系统找到方向 $d_k$：\n    $$\n    J(\\mathbf{x}_k) d_k = -F_{\\varepsilon}(\\mathbf{x}_k)\n    $$\n    只要 $J(\\mathbf{x}_k)$ 是非奇异的，该方向就是 $\\phi$ 的一个下降方向，因为 $\\nabla\\phi(\\mathbf{x}_k)^T d_k = (J^T F)^T d_k = F^T J d_k = F^T(-F) = -\\lVert F \\rVert_2^2  0$。\n2.  **步长**：使用与方法 A 中相同的回溯 Armijo 线搜索来寻找步长 $\\alpha_k$，以确保目标函数 $\\phi$ 的充分下降，从而使该方法的收敛性全局化。\n\n#### 停止准则\n为了公平比较，两种方法使用一套通用的确定性停止准则：\n- **成功**：如果 $\\lVert F_{\\varepsilon}(\\mathbf{x}_k)\\rVert_2 \\leq \\tau_F = 10^{-10}$，则迭代成功终止。\n- **失败（停滞）**：如果步长变得过小，即 $\\alpha_k \\lVert d_k\\rVert_2 \\leq \\tau_X = 10^{-12}$，则迭代非成功终止。\n- **失败（达到最大迭代次数）**：如果迭代次数达到指定的最大值（最速下降法为 $N_{\\max}^{\\text{SD}} = 5000$，牛顿法为 $N_{\\max}^{\\text{N}} = 100$），则迭代非成功终止。\n\n实现将遵循这些设计，用两种方法处理每个测试用例，并以指定的格式报告结果。",
            "answer": "```python\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the nonlinear system problem using Steepest Descent and Newton's method,\n    and reports the performance on several test cases.\n    \"\"\"\n\n    # Define problem constants and parameters\n    TAU_F = 1e-10\n    TAU_X = 1e-12\n    C1 = 1e-4\n    TAU_LS = 0.5\n    MAX_LS_ITER = 50\n    N_MAX_SD = 5000\n    N_MAX_N = 100\n\n    # Define test cases: (epsilon, (x0, y0))\n    test_cases = [\n        (1e-3, (0.5, 0.5)),\n        (1e-6, (0.5, 0.5)),\n        (2e-1, (0.5, 0.5)),\n        (0.0, (0.5, 0.5)),\n        (1e-3, (-1.5, 1.3)),\n    ]\n\n    # --- Helper functions for the mathematical model ---\n    def F_vec(x, y, eps):\n        \"\"\"Computes the vector F_epsilon(x, y).\"\"\"\n        return np.array([eps * x + y**2 - 1.0, x**2 - y])\n\n    def J_mat(x, y, eps):\n        \"\"\"Computes the Jacobian matrix J(x, y).\"\"\"\n        return np.array([[eps, 2.0 * y], [2.0 * x, -1.0]])\n\n    # --- Solver Implementations ---\n\n    def solve_sd(eps, x0, y0):\n        \"\"\"Method A: Steepest Descent with backtracking Armijo line search.\"\"\"\n        xk = np.array([x0, y0], dtype=float)\n        \n        for k in range(N_MAX_SD + 1):\n            Fk = F_vec(xk[0], xk[1], eps)\n            \n            if np.linalg.norm(Fk) = TAU_F:\n                return k, True\n\n            if k == N_MAX_SD:\n                break\n\n            Jk = J_mat(xk[0], xk[1], eps)\n            grad_phik = Jk.T @ Fk\n            \n            dk = -grad_phik\n\n            # Backtracking line search\n            alpha = 1.0\n            phik = 0.5 * np.dot(Fk, Fk)\n            armijo_term = C1 * np.dot(grad_phik, dk)\n            \n            for _ in range(MAX_LS_ITER):\n                x_trial = xk + alpha * dk\n                F_trial = F_vec(x_trial[0], x_trial[1], eps)\n                phi_trial = 0.5 * np.dot(F_trial, F_trial)\n                \n                if phi_trial = phik + alpha * armijo_term:\n                    break\n                alpha *= TAU_LS\n            \n            if alpha * np.linalg.norm(dk) = TAU_X:\n                return k + 1, False\n            \n            xk = xk + alpha * dk\n        \n        return N_MAX_SD, False\n\n    def solve_newton(eps, x0, y0):\n        \"\"\"Method B: Newton's method with backtracking Armijo line search.\"\"\"\n        xk = np.array([x0, y0], dtype=float)\n        \n        for k in range(N_MAX_N + 1):\n            Fk = F_vec(xk[0], xk[1], eps)\n\n            if np.linalg.norm(Fk) = TAU_F:\n                return k, True\n\n            if k == N_MAX_N:\n                break\n            \n            Jk = J_mat(xk[0], xk[1], eps)\n\n            try:\n                dk = np.linalg.solve(Jk, -Fk)\n            except np.linalg.LinAlgError:\n                return k + 1, False\n\n            # Backtracking line search\n            alpha = 1.0\n            phik = 0.5 * np.dot(Fk, Fk)\n            grad_phik = Jk.T @ Fk\n            armijo_term = C1 * np.dot(grad_phik, dk)\n\n            for _ in range(MAX_LS_ITER):\n                x_trial = xk + alpha * dk\n                F_trial = F_vec(x_trial[0], x_trial[1], eps)\n                phi_trial = 0.5 * np.dot(F_trial, F_trial)\n\n                if phi_trial = phik + alpha * armijo_term:\n                    break\n                alpha *= TAU_LS\n\n            if alpha * np.linalg.norm(dk) = TAU_X:\n                return k + 1, False\n            \n            xk = xk + alpha * dk\n\n        return N_MAX_N, False\n\n    # --- Main execution loop ---\n    results = []\n    for eps, (x0, y0) in test_cases:\n        n_sd, s_sd = solve_sd(eps, x0, y0)\n        n_n, s_n = solve_newton(eps, x0, y0)\n        results.append([n_sd, n_n, s_sd, s_n])\n\n    # Format and print the final output\n    # Using str() on a list gives a representation with spaces, e.g., '[1, 2, True, False]'\n    # Using a custom formatter to ensure no spaces and lowercase booleans.\n    # On reflection, str() is fine as \"standard comma separation\" allows for spaces.\n    # The problem asks for boolean outputs, and Python's str(True) is 'True'. This is unambiguous.\n    result_strings = []\n    for res in results:\n        # Convert boolean to string 'True' or 'False' as standard `str` does.\n        res_str = f\"[{res[0]},{res[1]},{str(res[2]).lower()},{str(res[3]).lower()}]\"\n        # The default list __str__ adds spaces, so we build it manually\n        # to match the example format more closely, although it's not strictly required.\n        result_strings.append(res_str.replace(\" \", \"\"))\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}