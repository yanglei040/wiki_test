{
    "hands_on_practices": [
        {
            "introduction": "Before we can harness the full power of Newton's method, we must master its fundamental mechanics. This first exercise provides a direct, hands-on calculation of a single iteration for a simple two-dimensional system. By manually computing the function vector, the Jacobian matrix, and solving the linear system for the update step, you will build a concrete understanding of the algorithm's core components, which is essential for both theoretical analysis and successful implementation. ",
            "id": "2207863",
            "problem": "Consider the following system of two non-linear equations:\n$$\n\\begin{cases}\n2x^2 + y = 11 \\\\\nx + 2y^2 = 10\n\\end{cases}\n$$\nAn approximate solution to this system is sought using Newton's method for systems. Starting with the initial guess $(x_0, y_0) = (3, 1)$, perform a single iteration to find the next approximation $(x_1, y_1)$.\n\nFind the coordinates of $(x_1, y_1)$. Express each coordinate as a rational number in its simplest form.",
            "solution": "Define the vector function $\\mathbf{F}(x,y)$ and its Jacobian matrix $J(x,y)$ by\n$$\n\\mathbf{F}(x,y)=\\begin{pmatrix} 2x^{2}+y-11 \\\\ x+2y^{2}-10 \\end{pmatrix}, \n\\quad\nJ(x,y)=\\begin{pmatrix} \\frac{\\partial}{\\partial x}(2x^{2}+y-11) & \\frac{\\partial}{\\partial y}(2x^{2}+y-11) \\\\ \\frac{\\partial}{\\partial x}(x+2y^{2}-10) & \\frac{\\partial}{\\partial y}(x+2y^{2}-10) \\end{pmatrix}\n=\\begin{pmatrix} 4x & 1 \\\\ 1 & 4y \\end{pmatrix}.\n$$\nNewton’s method for systems computes the update $\\mathbf{s}=(s_{x},s_{y})^{T}$ by solving\n$$\nJ(x_{0},y_{0})\\,\\mathbf{s}=-\\mathbf{F}(x_{0},y_{0}),\n$$\nand then sets $(x_{1},y_{1})=(x_{0},y_{0})+\\mathbf{s}$.\n\nAt $(x_{0},y_{0})=(3,1)$, evaluate\n$$\n\\mathbf{F}(3,1)=\\begin{pmatrix} 2\\cdot 3^{2}+1-11 \\\\ 3+2\\cdot 1^{2}-10 \\end{pmatrix}\n=\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix},\n\\quad\nJ(3,1)=\\begin{pmatrix} 4\\cdot 3 & 1 \\\\ 1 & 4\\cdot 1 \\end{pmatrix}\n=\\begin{pmatrix} 12 & 1 \\\\ 1 & 4 \\end{pmatrix}.\n$$\nSolve for $\\mathbf{s}$ in\n$$\n\\begin{pmatrix} 12 & 1 \\\\ 1 & 4 \\end{pmatrix}\\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}\n=-\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix}\n=\\begin{pmatrix} -8 \\\\ 5 \\end{pmatrix},\n$$\nwhich is the linear system\n$$\n\\begin{cases}\n12s_{x}+s_{y}=-8, \\\\\ns_{x}+4s_{y}=5.\n\\end{cases}\n$$\nFrom $s_{x}=5-4s_{y}$ and substitution into the first equation,\n$$\n12(5-4s_{y})+s_{y}=-8\n\\;\\Rightarrow\\;\n60-48s_{y}+s_{y}=-8\n\\;\\Rightarrow\\;\n-47s_{y}=-68\n\\;\\Rightarrow\\;\ns_{y}=\\frac{68}{47}.\n$$\nThen\n$$\ns_{x}=5-4\\cdot \\frac{68}{47}\n=\\frac{235}{47}-\\frac{272}{47}\n=-\\frac{37}{47}.\n$$\nUpdate the approximation:\n$$\nx_{1}=x_{0}+s_{x}=3-\\frac{37}{47}=\\frac{141}{47}-\\frac{37}{47}=\\frac{104}{47}, \n\\quad\ny_{1}=y_{0}+s_{y}=1+\\frac{68}{47}=\\frac{47}{47}+\\frac{68}{47}=\\frac{115}{47}.\n$$\nThus, the next Newton iterate is $\\left(\\frac{104}{47}, \\frac{115}{47}\\right)$, with both coordinates in simplest rational form.",
            "answer": "$$\\boxed{\\left(\\frac{104}{47}, \\frac{115}{47}\\right)}$$"
        },
        {
            "introduction": "A powerful algorithm is only useful if we understand its limitations. This practice problem moves beyond pure calculation to explore the conceptual underpinnings of Newton's method's convergence. By analyzing why certain initial guesses can lead to failure for a given system, you will develop a crucial intuition for the role of the Jacobian matrix and the importance of its invertibility, a key factor in the robustness of the method. ",
            "id": "2207871",
            "problem": "An engineer is tasked with finding a numerical solution to a system of non-linear equations modeling a steady-state physical system. The equations are given by:\n$$f_1(x, y) = x^2 - y^2 - 4 = 0$$\n$$f_2(x, y) = xy - 3 = 0$$\nThe engineer decides to use Newton's method for systems. The iterative formula for this method is given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k - [J(\\mathbf{x}_k)]^{-1} \\mathbf{F}(\\mathbf{x}_k)$, where $\\mathbf{x} = (x, y)^T$, $\\mathbf{F} = (f_1, f_2)^T$, and $J(\\mathbf{x})$ is the Jacobian matrix of $\\mathbf{F}$.\n\nAfter some trials, the engineer observes that choosing an initial guess $\\mathbf{x}_0 = (x_0, y_0)$ that lies on either the x-axis (where $y_0 = 0$) or the y-axis (where $x_0 = 0$) is a poor strategy for this specific system. Which of the following statements provides the most accurate and fundamental reason for this poor performance?\n\nA. For any initial guess on an axis, the first iteration of Newton's method produces a point that is also on an axis, preventing the algorithm from ever moving towards a solution, as the solutions do not lie on an axis.\n\nB. For any initial guess on an axis, the function vector $\\mathbf{F}(\\mathbf{x}_0)$ is a zero vector, which incorrectly signals to the algorithm that a solution has been found.\n\nC. The Jacobian matrix is singular for any point on the x-axis or y-axis, causing the method to fail immediately due to an inability to compute the matrix inverse.\n\nD. The point $(0,0)$ is the only location where the Jacobian is singular. An initial guess on an axis that is close to the origin leads to a nearly-singular Jacobian, causing the subsequent iteration to produce a point extremely far from the true solution, indicating poor convergence behavior.\n\nE. The system of equations has no real solutions, so Newton's method will fail to converge regardless of the initial guess.",
            "solution": "We are given the system\n$$\nf_{1}(x,y)=x^{2}-y^{2}-4,\\qquad f_{2}(x,y)=xy-3,\n$$\nwith vector function $\\mathbf{F}(x,y)=(f_{1}(x,y),f_{2}(x,y))^{T}$ and Jacobian\n$$\nJ(x,y)=\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x} & \\frac{\\partial f_{1}}{\\partial y}\\\\[4pt]\n\\frac{\\partial f_{2}}{\\partial x} & \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\\begin{pmatrix}\n2x & -2y\\\\\ny & x\n\\end{pmatrix}.\n$$\nNewton’s method updates $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-J(\\mathbf{x}_{k})^{-1}\\mathbf{F}(\\mathbf{x}_{k})$, which requires $J$ to be invertible at the iterate.\n\nFirst, compute the determinant of the Jacobian:\n$$\n\\det J(x,y)=(2x)(x)-(-2y)(y)=2x^{2}+2y^{2}=2(x^{2}+y^{2}).\n$$\nThus $J(x,y)$ is singular if and only if $(x,y)=(0,0)$. In particular, $J$ is not singular at generic points on the axes (except at the origin). This immediately shows option C is false.\n\nNext, evaluate $\\mathbf{F}$ on the axes. On the $x$-axis with $y=0$,\n$$\n\\mathbf{F}(x,0)=\\bigl(x^{2}-4,\\,-3\\bigr),\n$$\nwhich is never the zero vector. On the $y$-axis with $x=0$,\n$$\n\\mathbf{F}(0,y)=\\bigl(-y^{2}-4,\\,-3\\bigr),\n$$\nwhich is also never the zero vector. Hence option B is false.\n\nNow check whether Newton’s method remains on an axis after one iteration. Use the Newton step $\\mathbf{s}$ defined by $J\\mathbf{s}=-\\mathbf{F}$. On the $x$-axis ($y=0$), we have\n$$\nJ(x,0)=\\begin{pmatrix}2x & 0\\\\ 0 & x\\end{pmatrix},\\qquad \\mathbf{F}(x,0)=\\begin{pmatrix}x^{2}-4\\\\ -3\\end{pmatrix}.\n$$\nThen\n$$\n\\mathbf{s}=-J^{-1}\\mathbf{F}=-\\begin{pmatrix}\\frac{1}{2x} & 0\\\\[2pt] 0 & \\frac{1}{x}\\end{pmatrix}\\begin{pmatrix}x^{2}-4\\\\ -3\\end{pmatrix}\n=\\begin{pmatrix}-\\frac{x^{2}-4}{2x}\\\\[4pt]\\frac{3}{x}\\end{pmatrix},\n$$\nso the next iterate is\n$$\nx_{1}=x-\\frac{x^{2}-4}{2x}=\\frac{x}{2}+\\frac{2}{x},\\qquad y_{1}=0+\\frac{3}{x}=\\frac{3}{x}.\n$$\nSince $y_{1}=\\frac{3}{x}\\neq 0$ for any finite $x$, the iterate immediately leaves the axis. A similar calculation on the $y$-axis ($x=0$) solves\n$$\n\\begin{pmatrix}0 & -2y\\\\ y & 0\\end{pmatrix}\\begin{pmatrix}s_{x}\\\\ s_{y}\\end{pmatrix}=-\\begin{pmatrix}-y^{2}-4\\\\ -3\\end{pmatrix}=\\begin{pmatrix}y^{2}+4\\\\ 3\\end{pmatrix},\n$$\nwhich yields $s_{x}=\\frac{3}{y}$ and $s_{y}=-\\frac{y^{2}+4}{2y}$, so\n$$\nx_{1}=0+\\frac{3}{y}=\\frac{3}{y},\\qquad y_{1}=y-\\frac{y^{2}+4}{2y}=\\frac{y}{2}-\\frac{2}{y}.\n$$\nAgain, the iterate leaves the axis immediately. Therefore option A is false.\n\nTo rule out option E, we check for real solutions. From $xy=3$ we have $y=\\frac{3}{x}$, and substituting into $x^{2}-y^{2}=4$ gives\n$$\nx^{2}-\\frac{9}{x^{2}}=4\\;\\;\\Longrightarrow\\;\\; x^{4}-4x^{2}-9=0.\n$$\nLet $t=x^{2}$. Then $t^{2}-4t-9=0$, so $t=2\\pm\\sqrt{13}$. The admissible root is $t=2+\\sqrt{13}>0$, hence\n$$\nx=\\pm\\sqrt{2+\\sqrt{13}},\\qquad y=\\frac{3}{x}=\\pm\\frac{3}{\\sqrt{2+\\sqrt{13}}},\n$$\nwhich shows there are two real solutions. Therefore option E is false.\n\nIt remains to identify the fundamental reason axes are a poor choice for initial guesses. Since\n$$\n\\det J(x,y)=2(x^{2}+y^{2}),\n$$\nthe Jacobian is singular at $(0,0)$ and becomes ill-conditioned when $(x,y)$ is close to the origin. The explicit inverse is\n$$\nJ(x,y)^{-1}=\\frac{1}{2(x^{2}+y^{2})}\\begin{pmatrix}x & 2y\\\\ -y & 2x\\end{pmatrix},\n$$\nwhose entries scale like $\\frac{1}{x^{2}+y^{2}}$ times linear functions of $x$ and $y$. Near the origin this produces large Newton steps. On the axes in particular, the Newton updates derived above contain terms like $\\frac{3}{x}$ or $\\frac{3}{y}$, which become very large in magnitude when the initial guess is on an axis and close to the origin. This ill-conditioning explains the observed poor performance and is precisely captured by option D.\n\nTherefore, the most accurate and fundamental reason is that the Jacobian is singular at the origin and nearly singular for axis-aligned initial guesses near the origin, leading to instability and poor convergence.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Bridging theory and practice is a key skill for any computational scientist. While the analytical Jacobian provides theoretical guarantees of quadratic convergence, it may be difficult or impossible to derive for complex, real-world problems. This hands-on coding exercise  challenges you to implement and compare Newton's method using both the exact Jacobian and a practical finite-difference approximation, allowing you to experimentally investigate the trade-off between computational convenience and the speed of convergence.",
            "id": "2441924",
            "problem": "Consider the nonlinear system of equations defined by the vector-valued function $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ given by\n$$\n\\mathbf{F}(\\mathbf{x})=\n\\begin{bmatrix}\nf_1(x_1,x_2)\\\\\nf_2(x_1,x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1-\\cos(x_2)\\\\\nx_2-\\sin(x_1)\n\\end{bmatrix},\n$$\nwhere $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ and the trigonometric functions use angles measured in radians. The exact Jacobian matrix $\\mathbf{J}(\\mathbf{x})$ of $\\mathbf{F}$ is\n$$\n\\mathbf{J}(\\mathbf{x})=\n\\begin{bmatrix}\n\\dfrac{\\partial f_1}{\\partial x_1} & \\dfrac{\\partial f_1}{\\partial x_2}\\\\[6pt]\n\\dfrac{\\partial f_2}{\\partial x_1} & \\dfrac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & \\sin(x_2)\\\\\n-\\cos(x_1) & 1\n\\end{bmatrix}.\n$$\n\nDefine, for a given perturbation size $h>0$, the forward finite-difference Jacobian approximation $\\mathbf{J}_h(\\mathbf{x})$ by\n$$\n\\mathbf{J}_h(\\mathbf{x})=\\begin{bmatrix}\n\\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_1(\\mathbf{x})}{h} & \\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_1(\\mathbf{x})}{h}\\\\[10pt]\n\\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_2(\\mathbf{x})}{h} & \\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_2(\\mathbf{x})}{h}\n\\end{bmatrix},\n$$\nwhere $\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ and $\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$.\n\nFor an initial guess $\\mathbf{x}^{(0)}\\in\\mathbb{R}^2$, define the sequence $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$ recursively by\n$$\n\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big),\\qquad \\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)},\n$$\nwhere $\\mathbf{J}_\\star$ denotes either the exact Jacobian $\\mathbf{J}$ or the finite-difference Jacobian $\\mathbf{J}_h$, and the update $\\mathbf{s}^{(k)}$ is any solution of the linear system. Let the residual norm be $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$, and let the iteration terminate at the smallest index $k$ such that $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon$, with tolerance $\\varepsilon=10^{-10}$, or after $k_{\\max}=50$ iterations, whichever occurs first.\n\nFor each test case below, perform two runs starting from the given initial guess $\\mathbf{x}^{(0)}$:\n- Run A uses $\\mathbf{J}_\\star=\\mathbf{J}$.\n- Run B uses $\\mathbf{J}_\\star=\\mathbf{J}_h$ with the specified $h$.\n\nFor each run, record:\n- The number of iterations $n$ required to satisfy $\\|\\mathbf{F}(\\mathbf{x}^{(n)})\\|_2 \\le \\varepsilon$ (or $n=k_{\\max}$ if the tolerance is not met).\n- An estimate $\\hat{p}$ of the local convergence order computed from the last three available residual norms $\\{e_{m-2},e_{m-1},e_m\\}$ via\n$$\n\\hat{p}=\\frac{\\ln\\left(\\dfrac{e_m}{e_{m-1}}\\right)}{\\ln\\left(\\dfrac{e_{m-1}}{e_{m-2}}\\right)},\n$$\nwhere $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$, and $m$ is the final iteration index used in the run (use the last three residuals available at termination; all logarithms are natural logarithms). All angles in trigonometric functions are in radians.\n\nUse the following test suite, where each case is a pair $(\\mathbf{x}^{(0)},h)$:\n- Case $1$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$.\n- Case $2$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$.\n- Case $3$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$.\n- Case $4$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$.\n\nYour program must output a single line containing a list of results, one per test case, in the same order as listed. Each test case result must be a list of four entries $[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}]$, where $n_{\\text{exact}}$ and $\\hat{p}_{\\text{exact}}$ correspond to Run A, and $n_{\\text{fd}}$ and $\\hat{p}_{\\text{fd}}$ correspond to Run B. The final output format must be a single line that is a comma-separated list of these per-case lists enclosed in square brackets, for example, $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\dots]$.",
            "solution": "The problem requires the implementation of two variants of Newton's method to solve the nonlinear system $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$. The first variant is the classical Newton-Raphson method, which uses the exact analytical Jacobian of $\\mathbf{F}$. The second is a quasi-Newton method where the Jacobian is approximated using a forward finite-difference scheme. We will compare the performance of these two methods in terms of the number of iterations required for convergence and the estimated local order of convergence.\n\nThe core of the method is the iterative update rule. At each step $k$, we approximate the nonlinear function $\\mathbf{F}$ with its linear Taylor expansion around the current iterate $\\mathbf{x}^{(k)}$:\n$$\n\\mathbf{F}(\\mathbf{x}) \\approx \\mathbf{F}(\\mathbf{x}^{(k)}) + \\mathbf{J}_\\star(\\mathbf{x}^{(k)})(\\mathbf{x} - \\mathbf{x}^{(k)})\n$$\nWe seek the next iterate $\\mathbf{x}^{(k+1)}$ by setting this approximation to zero, i.e., $\\mathbf{F}(\\mathbf{x}^{(k+1)}) = \\mathbf{0}$. Defining the update step as $\\mathbf{s}^{(k)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}$, we obtain the linear system for the update:\n$$\n\\mathbf{J}_\\star(\\mathbf{x}^{(k)}) \\mathbf{s}^{(k)} = -\\mathbf{F}(\\mathbf{x}^{(k)})\n$$\nOnce $\\mathbf{s}^{(k)}$ is found by solving this system, the next iterate is computed as $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)}$. This process is repeated until the norm of the residual vector, $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$, falls below a specified tolerance $\\varepsilon = 10^{-10}$.\n\n**Run A: Exact Jacobian (Newton-Raphson Method)**\nIn this run, $\\mathbf{J}_\\star$ is the exact Jacobian matrix $\\mathbf{J}(\\mathbf{x})$:\n$$\n\\mathbf{J}(\\mathbf{x}) =\n\\begin{bmatrix}\n1 & \\sin(x_2)\\\\\n-\\cos(x_1) & 1\n\\end{bmatrix}\n$$\nThis method is known to exhibit quadratic convergence (i.e., convergence order $p=2$) when the initial guess is sufficiently close to the solution and the Jacobian is non-singular at the solution. This means the number of correct digits in the solution roughly doubles with each iteration. The estimated order $\\hat{p}$ should therefore be close to $2$.\n\n**Run B: Finite-Difference Jacobian (Quasi-Newton Method)**\nIn this run, the Jacobian is approximated using the forward finite-difference formula. The $j$-th column of the approximate Jacobian $\\mathbf{J}_h(\\mathbf{x})$ is given by:\n$$\n[\\mathbf{J}_h(\\mathbf{x})]_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h}\n$$\nwhere $\\mathbf{e}_j$ is the $j$-th standard basis vector and $h$ is a small perturbation parameter. For the given $2 \\times 2$ system, this yields:\n$$\n\\mathbf{J}_h(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{f_1(x_1+h, x_2) - f_1(x_1, x_2)}{h} & \\frac{f_1(x_1, x_2+h) - f_1(x_1, x_2)}{h} \\\\\n\\frac{f_2(x_1+h, x_2) - f_2(x_1, x_2)}{h} & \\frac{f_2(x_1, x_2+h) - f_2(x_1, x_2)}{h}\n\\end{bmatrix}\n$$\nThis approach avoids the need for analytical derivation of the Jacobian, which can be complex or impossible for some functions. However, it introduces an approximation error. The error in each element of $\\mathbf{J}_h$ is of order $O(h)$. This error perturbs the Newton step, affecting the convergence rate. For a very small $h$ (e.g., $10^{-6}$), the approximation is accurate, and the convergence should be nearly quadratic. As $h$ increases (e.g., $10^{-3}$ or $10^{-2}$), the approximation worsens, and the convergence rate is expected to degrade, potentially becoming linear ($p=1$), and requiring more iterations.\n\n**Algorithm and Implementation**\nFor each test case $(\\mathbf{x}^{(0)}, h)$, we perform the following steps for both Run A and Run B:\n$1$. Initialize $k=0$ and the current solution $\\mathbf{x} = \\mathbf{x}^{(0)}$. Create a list to store residual norms.\n$2$. Begin a loop that continues as long as $k \\le k_{\\max} = 50$.\n$3$. Compute the residual vector $\\mathbf{F}(\\mathbf{x})$ and its Euclidean norm $e_k = \\|\\mathbf{F}(\\mathbf{x})\\|_2$. Store $e_k$.\n$4$. Check for termination: if $e_k \\le \\varepsilon=10^{-10}$, set the final iteration count $n=k$ and exit the loop.\n$5$. If the loop is to continue (i.e., $k < k_{\\max}$), compute the appropriate Jacobian matrix $\\mathbf{J}_\\star(\\mathbf{x})$ (either exact or finite-difference).\n$6$. Solve the linear system $\\mathbf{J}_\\star(\\mathbf{x})\\mathbf{s} = -\\mathbf{F}(\\mathbf{x})$ for the step $\\mathbf{s}$.\n$7$. Update the solution: $\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}$.\n$8$. Increment the iteration counter: $k \\leftarrow k+1$.\n$9$. If the loop completes because $k$ reached $k_{\\max}$, set $n=k_{\\max}$.\n$10$. After the loop terminates, calculate the estimated convergence order $\\hat{p}$ using the last three available residual norms, $e_{n-2}, e_{n-1}, e_{n}$. If fewer than three norms are available (i.e., $n<2$), $\\hat{p}$ is considered not computable.\nThe results $(n, \\hat{p})$ from both runs are collected for each test case to form the final output. This process will demonstrate the theoretical properties of Newton-family methods in a practical computational context.\n```python\nimport numpy as np\n\n# Global constants as specified in the problem\nTOLERANCE = 1e-10\nK_MAX = 50\n\ndef F(x):\n    \"\"\"\n    Computes the vector-valued function F(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        x[0] - np.cos(x[1]),\n        x[1] - np.sin(x[0])\n    ])\n\ndef J_exact(x):\n    \"\"\"\n    Computes the exact Jacobian matrix J(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        [1.0, np.sin(x[1])],\n        [-np.cos(x[0]), 1.0]\n    ])\n\ndef J_fd(x, h):\n    \"\"\"\n    Computes the forward finite-difference approximation of the Jacobian.\n    x must be a NumPy array [x1, x2].\n    h is the perturbation size.\n    \"\"\"\n    n = len(x)\n    jac = np.zeros((n, n), dtype=float)\n    fx = F(x)\n    for j in range(n):\n        x_plus_h = x.copy()\n        x_plus_h[j] += h\n        fx_plus_h = F(x_plus_h)\n        jac[:, j] = (fx_plus_h - fx) / h\n    return jac\n\ndef newton_solver(x0, h, use_exact_jacobian):\n    \"\"\"\n    Solves the nonlinear system F(x)=0 using a Newton-like method.\n    \n    Args:\n        x0 (list or np.ndarray): The initial guess.\n        h (float): The perturbation size for the finite-difference Jacobian.\n        use_exact_jacobian (bool): If True, use the exact Jacobian. \n                                  If False, use the finite-difference approximation.\n\n    Returns:\n        tuple: (n_iter, p_hat) where n_iter is the number of iterations and\n               p_hat is the estimated convergence order.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    k = 0\n    residuals = []\n    n_iter = K_MAX\n\n    while k <= K_MAX:\n        F_val = F(x)\n        norm = np.linalg.norm(F_val)\n        residuals.append(norm)\n\n        if norm <= TOLERANCE:\n            n_iter = k\n            break\n        \n        if k == K_MAX:\n            break\n\n        if use_exact_jacobian:\n            Jacobian = J_exact(x)\n        else:\n            Jacobian = J_fd(x, h)\n\n        try:\n            # Solve the linear system J*s = -F\n            s = np.linalg.solve(Jacobian, -F_val)\n            x += s\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, the solver fails.\n            n_iter = K_MAX\n            break\n        \n        k += 1\n    \n    # Estimate convergence order p_hat from the last three residuals\n    p_hat = np.nan\n    if len(residuals) >= 3:\n        # Use residuals e_n, e_{n-1}, e_{n-2}\n        e_m, e_m1, e_m2 = residuals[-1], residuals[-2], residuals[-3]\n        \n        # Avoid division by zero or log of non-positive numbers.\n        # Ratios must be < 1 for convergence.\n        ratio1 = e_m / e_m1 if e_m1 != 0 else 0\n        ratio2 = e_m1 / e_m2 if e_m2 != 0 else 0\n\n        if 0 < ratio1 < 1 and 0 < ratio2 < 1:\n            p_hat = np.log(ratio1) / np.log(ratio2)\n            \n    return n_iter, p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        ([0.5, 0.5], 1e-6),\n        ([0.5, 0.5], 1e-3),\n        ([1.0, 1.0], 1e-6),\n        ([1.0, 1.0], 1e-2)\n    ]\n\n    all_results = []\n    for x0, h in test_cases:\n        # Run A: Exact Jacobian\n        n_exact, p_exact = newton_solver(x0, h, use_exact_jacobian=True)\n        \n        # Run B: Finite-Difference Jacobian\n        n_fd, p_fd = newton_solver(x0, h, use_exact_jacobian=False)\n        \n        # Assemble results for the current test case\n        case_result = [n_exact, n_fd, p_exact, p_fd]\n        all_results.append(case_result)\n\n    # The final print statement must match the format exactly.\n    # The default string representation of a list of lists is \"[...], [...]\"\n    # and the default representation of a float is what we need.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the main function\n# solve()\n```",
            "answer": "[[4, 4, 1.996078028235284, 1.996079085610178], [4, 5, 1.996078028235284, 1.006930058913693], [4, 4, 1.967830386616016, 1.967830492160634], [4, 6, 1.967830386616016, 1.0007000574223253]]"
        }
    ]
}