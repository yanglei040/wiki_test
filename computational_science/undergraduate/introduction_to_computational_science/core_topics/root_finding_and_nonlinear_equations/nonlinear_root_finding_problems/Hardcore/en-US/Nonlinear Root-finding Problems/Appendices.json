{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a complete walkthrough of solving a nonlinear equation, from theory to practice. You will begin by using fundamental calculus tools like the Intermediate Value Theorem to prove that a root exists and then use derivative analysis to show it is unique. This analytical groundwork is crucial before deploying a numerical algorithm, and you will capstone the exercise by applying the powerful Newton-Raphson method to approximate the root with high precision .",
            "id": "3164859",
            "problem": "Consider the nonlinear equation $f(x) = \\cos(x) - x^{3}$ on the real line, where all angles are measured in radians. Using only foundational concepts from calculus and numerical analysis, address the following objectives to determine the real root structure and compute the unique real solution:\n1) Use the continuity of $f(x)$, together with the Intermediate Value Theorem (IVT), to establish the existence of at least one real root. Identify a closed interval $[a,b]$ within which a sign change occurs, and justify your choice of $a$ and $b$.\n2) Analyze the sign of the derivative $f^{\\prime}(x)$ and the range of $\\cos(x)$ to determine whether multiple real roots can exist. Provide a logically complete argument for the number of real roots of $f(x)$.\n3) Construct a practical initial guess $x_{0}$ for an iterative root-finding procedure by using qualitative graphical insight (based on the behavior of $\\cos(x)$ and $x^{3}$) and derivative sign checks. Justify your choice of $x_{0}$ using values of $f(x)$ at simple points and the monotonicity of $f(x)$ on a suitable interval.\n4) Derive an iterative root-finding method from the first-order Taylor polynomial of $f(x)$ centered at the current iterate, and apply it to compute the unique real root starting from your chosen $x_{0}$. Continue iterating until your approximation is consistent with reporting the root to $4$ significant figures.\n\nRound your final numerical answer to $4$ significant figures, and express it as a dimensionless real number.",
            "solution": "The problem requires an analysis of the real roots of the function $f(x) = \\cos(x) - x^{3}$. The solution is structured into four parts as requested.\n\n(1) Existence of a Real Root\n\nTo establish the existence of a real root, we will use the Intermediate Value Theorem (IVT). The function is given by $f(x) = \\cos(x) - x^{3}$. The function $\\cos(x)$ is continuous for all real numbers $x \\in \\mathbb{R}$, and the function $x^{3}$ is a polynomial, which is also continuous for all $x \\in \\mathbb{R}$. The difference of two continuous functions is continuous, so $f(x)$ is continuous on the entire real line $\\mathbb{R}$.\n\nThe IVT states that for a function $f$ continuous on a closed interval $[a, b]$, if $k$ is any number between $f(a)$ and $f(b)$, then there exists at least one number $c \\in [a, b]$ such that $f(c) = k$. To show a root exists, we need to find an interval $[a, b]$ where $f(a)$ and $f(b)$ have opposite signs, which implies that $0$ is between $f(a)$ and $f(b)$.\n\nLet us evaluate $f(x)$ at simple points.\nFor $a=0$:\n$$f(0) = \\cos(0) - 0^{3} = 1 - 0 = 1$$\nSo, $f(0)  0$.\n\nFor $b=1$:\n$$f(1) = \\cos(1) - 1^{3} = \\cos(1) - 1$$\nSince angles are in radians, and $0  1  \\frac{\\pi}{2}$, we know that $0  \\cos(1)  1$. Therefore, $f(1) = \\cos(1) - 1  0$.\n\nSince $f(x)$ is continuous on the closed interval $[0, 1]$, and $f(0) = 1  0$ while $f(1)  0$, the IVT guarantees the existence of at least one root $c$ in the open interval $(0, 1)$ such that $f(c) = 0$.\n\n(2) Number of Real Roots\n\nTo determine the total number of real roots, we first analyze the potential range for any roots and then examine the function's monotonicity. A root of $f(x)$ corresponds to a solution of the equation $\\cos(x) = x^{3}$.\n\nThe range of the cosine function is $[-1, 1]$, so any real root $x$ must satisfy $-1 \\le x^{3} \\le 1$. This implies that $-1 \\le x \\le 1$. Therefore, all real roots of $f(x)$ must lie within the closed interval $[-1, 1]$.\n\nNext, we analyze the derivative of $f(x)$ to determine its monotonicity on this interval. The derivative is:\n$$f^{\\prime}(x) = \\frac{d}{dx}(\\cos(x) - x^{3}) = -\\sin(x) - 3x^{2}$$\nWe analyze the sign of $f^{\\prime}(x)$ on the interval $[-1, 1]$.\n\nCase i: $x \\in (0, 1]$.\nIn this interval, $x  0$, so $3x^{2}  0$. Also, for $x \\in (0, 1] \\subset (0, \\pi)$, we have $\\sin(x)  0$. Therefore, both terms in the derivative are negative:\n$$f^{\\prime}(x) = \\underbrace{-\\sin(x)}_{0} + \\underbrace{(-3x^{2})}_{0}  0 \\quad \\text{for } x \\in (0, 1]$$\nSince $f^{\\prime}(x)  0$ on $(0, 1]$, the function $f(x)$ is strictly decreasing on the interval $[0, 1]$. A strictly monotonic function can have at most one root in an interval. Since we established in part (1) that a root exists in $(0, 1)$, there must be exactly one root in $[0, 1]$.\n\nCase ii: $x \\in [-1, 0)$.\nIn this interval, $x  0$. Let's examine the sign of $f(x)$ itself.\nThe term $\\cos(x)$ is positive for $x \\in [-1, 0) \\subset (-\\frac{\\pi}{2}, \\frac{\\pi}{2})$.\nThe term $-x^{3}$ is positive for $x  0$.\nThus, for $x \\in [-1, 0)$, $f(x)$ is the sum of two positive terms:\n$$f(x) = \\underbrace{\\cos(x)}_{0} + \\underbrace{(-x^{3})}_{0}  0 \\quad \\text{for } x \\in [-1, 0)$$\nSince $f(x)$ is strictly positive on $[-1, 0)$, there are no roots in this interval.\n\nCombining our findings:\n- There are no roots for $|x|  1$.\n- There are no roots for $x \\in [-1, 0)$.\n- There is exactly one root in $[0, 1]$.\nTherefore, the function $f(x) = \\cos(x) - x^{3}$ has exactly one real root.\n\n(3) Initial Guess for an Iterative Procedure\n\nFrom the analysis above, the unique root lies in the interval $(0, 1)$. A practical initial guess $x_{0}$ should be chosen within or near this interval. We seek a value $x_{0}$ where the iterative method is well-behaved. The derivative is $f'(x) = -\\sin(x) - 3x^2$. At $x=0$, $f'(0)=0$, which would cause division by zero in Newton's method. Thus, $x_0=0$ is a poor choice.\n\nLet's choose $x_{0}=1$ as our initial guess. This choice is justified because:\n- It is a simple endpoint of the interval $[0, 1]$ where the root is known to exist.\n- We have $f(1) = \\cos(1) - 1  0$.\n- The derivative at this point is $f'(1) = -\\sin(1) - 3 \\neq 0$, so the iterative step is well-defined.\n- The second derivative is $f''(x) = -\\cos(x) - 6x$. For $x \\in [0, 1]$, $\\cos(x)  0$ and $6x \\ge 0$, so $f''(x)  0$. This means $f(x)$ is concave down on $[0,1]$. Since $f(1)  0$ and $f(x)$ is concave down, an initial guess of $x_0=1$ for Newton's method will produce a sequence of iterates that converges monotonically to the root.\n\n(4) Iterative Root-Finding and Solution\n\nThe problem asks to derive an iterative method from the first-order Taylor polynomial of $f(x)$ centered at an iterate $x_{n}$. The first-order Taylor expansion is:\n$$f(x) \\approx f(x_{n}) + f^{\\prime}(x_{n})(x - x_{n})$$\nTo find the root, we set the polynomial to zero, $f(x) = 0$, and solve for $x$, which we designate as the next iterate $x_{n+1}$:\n$$0 = f(x_{n}) + f^{\\prime}(x_{n})(x_{n+1} - x_{n})$$\n$$x_{n+1} - x_{n} = -\\frac{f(x_{n})}{f^{\\prime}(x_{n})}$$\n$$x_{n+1} = x_{n} - \\frac{f(x_{n})}{f^{\\prime}(x_{n})}$$\nThis is the well-known Newton-Raphson method. For our specific function:\n$$f(x) = \\cos(x) - x^{3}$$\n$$f^{\\prime}(x) = -\\sin(x) - 3x^{2}$$\nThe iteration formula is:\n$$x_{n+1} = x_{n} - \\frac{\\cos(x_{n}) - x_{n}^{3}}{-\\sin(x_{n}) - 3x_{n}^{2}} = x_{n} + \\frac{\\cos(x_{n}) - x_{n}^{3}}{\\sin(x_{n}) + 3x_{n}^{2}}$$\nWe start with $x_{0} = 1$ and iterate until the result is stable to $4$ significant figures. All calculations must use radians.\n\nIteration $0$: $x_{0} = 1$.\n$$x_{1} = 1 + \\frac{\\cos(1) - 1^{3}}{\\sin(1) + 3(1)^{2}} \\approx 1 + \\frac{0.540302 - 1}{0.841471 + 3} \\approx 1 - 0.119668 = 0.880332$$\n\nIteration $1$: $x_{1} \\approx 0.880332$.\n$$x_{2} = 0.880332 + \\frac{\\cos(0.880332) - (0.880332)^{3}}{\\sin(0.880332) + 3(0.880332)^{2}} \\approx 0.880332 + \\frac{-0.045316}{3.095998} \\approx 0.880332 - 0.014637 = 0.865695$$\n\nIteration $2$: $x_{2} \\approx 0.865695$. The subsequent calculations are performed with higher precision to ensure convergence.\n$x_3 \\approx 0.86547407$\n\nIteration $3$: $x_{3} \\approx 0.86547407$.\n$$x_{4} = 0.86547407 + \\frac{\\cos(0.86547407) - (0.86547407)^{3}}{\\sin(0.86547407) + 3(0.86547407)^{2}} \\approx 0.86547407 + \\frac{-2.05 \\times 10^{-7}}{3.008351} \\approx 0.86547407 - 6.81 \\times 10^{-8} = 0.86547400$$\n\nComparing the iterates rounded to $4$ significant figures:\n- $x_{2} \\approx 0.8657$\n- $x_{3} \\approx 0.8655$\n- $x_{4} \\approx 0.8655$\n\nThe value $0.8655$ is consistent between the third and fourth iterations. Therefore, the unique real root, reported to $4$ significant figures, is $0.8655$.",
            "answer": "$$\\boxed{0.8655}$$"
        },
        {
            "introduction": "Iterative methods are the heart of numerical root-finding, but when are they guaranteed to work? This practice delves into the theory of convergence by reframing a root-finding problem as a fixed-point equation, $x = g(x)$ . By applying the Contraction Mapping Principle, you will derive the exact conditions on a parameter $\\alpha$ that ensure the iteration converges, providing a concrete example of how theoretical analysis underpins the reliability of numerical algorithms.",
            "id": "3164935",
            "problem": "In the study of nonlinear root-finding within introduction to computational science, a common strategy is to cast a root condition as a fixed-point equation and analyze the convergence of a simple fixed-point iteration using the Banach fixed-point theorem. Consider the parametric family of functions defined by $f(x) = x - \\alpha \\exp(-x)$ with parameter $\\alpha  0$, and the associated fixed-point map $g(x) = \\alpha \\exp(-x)$ obtained by rewriting the root condition $f(x^{\\ast}) = 0$ as $x^{\\ast} = g(x^{\\ast})$.\n\nStarting from the fundamental definition of a contraction mapping and the Banach fixed-point theorem (which states that a contraction on a complete metric space has a unique fixed point and that iterates converge to it), and using only well-tested facts such as the Mean Value Theorem (MVT), determine conditions under which the simple fixed-point iteration $x_{k+1} = g(x_{k})$ converges to $x^{\\ast}$ from any initial guess $x_{0}$ in a closed interval of nonnegative real numbers.\n\nSpecifically, carry out the following steps for $\\alpha  0$:\n\n1. Propose a closed interval $I(\\alpha)$ of the form $[0, b(\\alpha)]$ that is mapped into itself by $g$, and justify this invariance.\n2. On the interval $I(\\alpha)$, compute the least upper bound of $|g'(x)|$ and interpret this bound as a uniform contraction constant $q(\\alpha)$ by appealing to the Mean Value Theorem.\n3. Using the Banach fixed-point theorem, identify the largest parameter value $\\alpha_{\\text{max}}$ such that $g$ is a contraction on $I(\\alpha)$ and hence the iteration $x_{k+1} = g(x_{k})$ converges for any $x_{0} \\in I(\\alpha)$.\n\nExpress your final answer as the single real number $\\alpha_{\\text{max}}$. No rounding is required. Angles are not involved in this problem, and no physical units are required.",
            "solution": "The problem is to find the largest parameter value $\\alpha_{\\text{max}}$ for which a specific fixed-point iteration is guaranteed to converge based on the Banach fixed-point theorem. The function defining the root is $f(x) = x - \\alpha \\exp(-x)$ for a parameter $\\alpha  0$. The root-finding problem $f(x^{\\ast}) = 0$ is reformulated as a fixed-point problem $x^{\\ast} = g(x^{\\ast})$, where the mapping is given as $g(x) = \\alpha \\exp(-x)$. The iteration is $x_{k+1} = g(x_{k})$.\n\nTo apply the Banach fixed-point theorem, we must identify a complete metric space that is mapped into itself by $g$, and on which $g$ is a contraction mapping. The problem directs us to find a closed interval of non-negative real numbers, which is a complete metric space with the usual metric. The analysis will proceed in three steps as specified.\n\n**Step 1: Propose and justify an invariant interval $I(\\alpha) = [0, b(\\alpha)]$.**\n\nWe are given the fixed-point map $g(x) = \\alpha \\exp(-x)$, with the parameter $\\alpha  0$. We seek a closed interval of the form $I(\\alpha) = [0, b(\\alpha)]$ that is invariant under $g$, meaning $g(I(\\alpha)) \\subseteq I(\\alpha)$.\n\nFirst, let us analyze the properties of $g(x)$ on the domain of non-negative real numbers, $x \\ge 0$. The derivative of $g(x)$ is $g'(x) = -\\alpha \\exp(-x)$. Since $\\alpha  0$ and $\\exp(-x)  0$ for all real $x$, it follows that $g'(x)  0$. This implies that $g(x)$ is a strictly decreasing function for $x \\ge 0$.\n\nFor an interval $I = [0, b]$, the image under the decreasing function $g$ is the interval $[g(b), g(0)]$. Let's compute the endpoints of this image interval:\n$$g(0) = \\alpha \\exp(-0) = \\alpha$$\n$$g(b) = \\alpha \\exp(-b)$$\nSo, the image of $[0, b]$ is $g([0, b]) = [\\alpha \\exp(-b), \\alpha]$.\n\nThe condition for invariance, $g([0, b]) \\subseteq [0, b]$, requires this image interval to be a subset of $[0, b]$. This leads to two inequalities:\n$1.$ The lower bound of the image must be greater than or equal to the lower bound of the interval: $\\alpha \\exp(-b) \\ge 0$. This is always satisfied since $\\alpha  0$ and the exponential function is always positive.\n$2.$ The upper bound of the image must be less than or equal to the upper bound of the interval: $\\alpha \\le b$.\n\nSo, any interval of the form $[0, b]$ where $b \\ge \\alpha$ is an invariant set for the map $g(x)$. The problem asks us to propose a function $b(\\alpha)$. The simplest choice that satisfies the condition $b(\\alpha) \\ge \\alpha$ is to set $b(\\alpha) = \\alpha$.\n\nLet's verify this choice. The proposed interval is $I(\\alpha) = [0, \\alpha]$. As shown above, for this interval to be invariant, we need $\\alpha \\le \\alpha$, which is true. Thus, for any $\\alpha  0$, the interval $I(\\alpha) = [0, \\alpha]$ is mapped into itself by $g(x)$.\n\n**Step 2: Compute the uniform contraction constant $q(\\alpha)$ on $I(\\alpha)$.**\n\nAccording to the Mean Value Theorem, for any $x_1, x_2 \\in I(\\alpha)$, there exists a $c$ between $x_1$ and $x_2$ such that $|g(x_1) - g(x_2)| = |g'(c)||x_1 - x_2|$. For $g$ to be a contraction mapping on $I(\\alpha)$, we need to find a constant $q(\\alpha) \\in [0, 1)$ such that $|g(x_1) - g(x_2)| \\le q(\\alpha)|x_1 - x_2|$ for all $x_1, x_2 \\in I(\\alpha)$. This is satisfied if we choose $q(\\alpha)$ to be the least upper bound (supremum) of $|g'(x)|$ on the interval $I(\\alpha)$:\n$$q(\\alpha) = \\sup_{x \\in I(\\alpha)} |g'(x)|$$\nThe derivative is $g'(x) = -\\alpha \\exp(-x)$. Its absolute value is $|g'(x)| = |-\\alpha \\exp(-x)| = \\alpha \\exp(-x)$ since $\\alpha  0$.\n\nWe need to find the supremum of the function $h(x) = \\alpha \\exp(-x)$ on our chosen interval $I(\\alpha) = [0, \\alpha]$. To do this, we examine the derivative of $h(x)$:\n$$h'(x) = \\frac{d}{dx}(\\alpha \\exp(-x)) = -\\alpha \\exp(-x)$$\nSince $\\alpha  0$, $h'(x)  0$ for all $x$. This means $h(x) = |g'(x)|$ is a strictly decreasing function. Therefore, its maximum value on the closed interval $[0, \\alpha]$ occurs at the left endpoint, $x=0$.\n\nThe value of the supremum is:\n$$q(\\alpha) = \\sup_{x \\in [0, \\alpha]} |g'(x)| = |g'(0)| = \\alpha \\exp(-0) = \\alpha$$\nSo, the uniform contraction constant on the interval $I(\\alpha) = [0, \\alpha]$ is $q(\\alpha) = \\alpha$.\n\n**Step 3: Identify the largest parameter value $\\alpha_{\\text{max}}$.**\n\nThe Banach fixed-point theorem guarantees the existence of a unique fixed point and the convergence of the iteration $x_{k+1}=g(x_k)$ for any starting point $x_0 \\in I(\\alpha)$, provided that $g$ is a strict contraction on $I(\\alpha)$. This requires the contraction constant $q(\\alpha)$ to be strictly less than $1$.\n$$q(\\alpha)  1$$\nFrom Step 2, we found $q(\\alpha) = \\alpha$. Substituting this into the inequality gives:\n$$\\alpha  1$$\nThe problem is defined for $\\alpha  0$. Therefore, for any $\\alpha$ in the open interval $(0, 1)$, the map $g$ is a contraction on the invariant interval $I(\\alpha) = [0, \\alpha]$, and the fixed-point iteration is guaranteed to converge.\n\nThe question asks for the largest parameter value $\\alpha_{\\text{max}}$ for which this condition holds. The set of valid $\\alpha$ values is $(0, 1)$. This set does not contain a largest element. However, in the context of determining convergence regimes, \"the largest value\" refers to the supremum of the set of parameters for which the property holds. The supremum of the interval $(0, 1)$ is $1$.\nFor $\\alpha = 1$, the contraction constant is $q(1) = 1$, which does not satisfy the strict inequality required for the standard statement of the Banach fixed-point theorem. Thus, the boundary of the region of guaranteed convergence determined by this method is at $\\alpha=1$.\n\nTherefore, the largest parameter value is $\\alpha_{\\text{max}} = 1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Numerical computation is not just about finding the right formula; it's about making sure that formula behaves well on a computer. This exercise confronts a common and subtle pitfall known as catastrophic cancellation, where subtracting two nearly-equal numbers can lead to a massive loss of accuracy . You will learn to diagnose this issue in a seemingly simple function and apply a powerful algebraic technique—multiplying by the conjugate—to create an equivalent, numerically stable expression.",
            "id": "3164945",
            "problem": "Consider the nonlinear function $f(x)=\\sqrt{1+x}-\\sqrt{1-x}-\\alpha$ defined for $x \\in (-1,1)$, where $\\alpha$ is a real parameter. In introductory computational science, nonlinear root-finding problems often require careful management of numerical cancellation when evaluating expressions near $x=0$. Your tasks are:\n\n1. Using only algebraic identities and the standard properties of real-valued square roots, derive an exact algebraic rearrangement of $f(x)$ that avoids catastrophic cancellation when $|x|$ is small. Explain why the rearranged form improves numerical stability by discussing how it changes the magnitudes of terms involved in the evaluation.\n\n2. Treating the equation $f(x)=0$ as a root-finding problem on the interval $(-1,1)$, use the rearranged form to solve for $x$ explicitly in terms of $\\alpha$. Justify the selection of the appropriate branch, including any necessary constraints on $\\alpha$ and monotonicity arguments, and ensure your derivation does not reintroduce cancellation-sensitive steps.\n\nProvide your final answer as a single closed-form analytic expression for the root $x^{\\star}(\\alpha)$, simplified as much as possible. No rounding is required, and no units are involved.",
            "solution": "The function is given by $f(x)=\\sqrt{1+x}-\\sqrt{1-x}-\\alpha$. Catastrophic cancellation occurs when $x$ is very close to $0$. In this case, both $\\sqrt{1+x}$ and $\\sqrt{1-x}$ are very close to $1$, and their difference leads to a significant loss of precision in floating-point arithmetic. To derive a numerically stable form, we multiply the difference of square roots by its conjugate expression, $\\sqrt{1+x}+\\sqrt{1-x}$, in both the numerator and denominator. This converts the problematic subtraction into an addition.\n\n$f(x) = \\left(\\sqrt{1+x}-\\sqrt{1-x}\\right) \\frac{\\sqrt{1+x}+\\sqrt{1-x}}{\\sqrt{1+x}+\\sqrt{1-x}} - \\alpha$\n\nUsing the identity $(a-b)(a+b) = a^2-b^2$, the numerator becomes:\n$(\\sqrt{1+x})^2 - (\\sqrt{1-x})^2 = (1+x) - (1-x) = 2x$\n\nSo, the rearranged function is:\n$f(x) = \\frac{2x}{\\sqrt{1+x}+\\sqrt{1-x}} - \\alpha$\n\nThis form is numerically superior for small $|x|$. The original expression involved subtracting two quantities that approach $1$. The rearranged expression's denominator involves the sum of these same two quantities. The sum approaches $1+1=2$ as $x \\to 0$. The addition of two positive numbers is a numerically stable operation, thus avoiding catastrophic cancellation and preserving relative accuracy.\n\nTo solve the equation $f(x)=0$ for the root $x^{\\star}$, we can use both the original and rearranged forms of the equation.\n\nThe original form gives:\n$1) \\quad \\sqrt{1+x^{\\star}} - \\sqrt{1-x^{\\star}} = \\alpha$\n\nThe rearranged form (assuming $\\alpha \\neq 0$) gives:\n$\\frac{2x^{\\star}}{\\sqrt{1+x^{\\star}}+\\sqrt{1-x^{\\star}}} = \\alpha \\quad \\implies \\quad 2) \\quad \\sqrt{1+x^{\\star}}+\\sqrt{1-x^{\\star}} = \\frac{2x^{\\star}}{\\alpha}$\n\nWe now have a system of two linear equations in terms of the variables $\\sqrt{1+x^{\\star}}$ and $\\sqrt{1-x^{\\star}}$. This is a robust way to proceed as it uses the rearranged form to build the derivation.\n\nFirst, let's analyze the function $g(x) = \\sqrt{1+x}-\\sqrt{1-x}$ to determine the valid range for $\\alpha$. The derivative is $g'(x) = \\frac{1}{2\\sqrt{1+x}} + \\frac{1}{2\\sqrt{1-x}}$. For $x \\in (-1,1)$, both terms are positive, so $g'(x)  0$. This means $g(x)$ is strictly increasing on its domain. A strictly monotonic function has a unique inverse, so for any $\\alpha$ in the range of $g(x)$, there will be a unique solution $x^{\\star}$.\n\nThe range is found by evaluating the limits at the boundaries of the domain:\n$\\lim_{x\\to -1^+} g(x) = \\sqrt{0} - \\sqrt{2} = -\\sqrt{2}$\n$\\lim_{x\\to 1^-} g(x) = \\sqrt{2} - \\sqrt{0} = \\sqrt{2}$\nSince $g(x)$ is continuous and strictly increasing, its range is $(-\\sqrt{2}, \\sqrt{2})$. Therefore, a unique solution $x^{\\star} \\in (-1,1)$ exists if and only if $\\alpha \\in (-\\sqrt{2}, \\sqrt{2})$.\n\nFurthermore, note that $g(0) = 0$. Since $g(x)$ is increasing, if $\\alpha  0$, the solution $x^{\\star}$ must be greater than $0$. If $\\alpha  0$, the solution $x^{\\star}$ must be less than $0$. Thus, $\\text{sgn}(x^{\\star}) = \\text{sgn}(\\alpha)$.\n\nNow we solve the system of equations. Add equation $(1)$ and $(2)$:\n$( \\sqrt{1+x^{\\star}} - \\sqrt{1-x^{\\star}} ) + ( \\sqrt{1+x^{\\star}} + \\sqrt{1-x^{\\star}} ) = \\alpha + \\frac{2x^{\\star}}{\\alpha}$\n$2\\sqrt{1+x^{\\star}} = \\alpha + \\frac{2x^{\\star}}{\\alpha}$\n\nSquaring both sides:\n$4(1+x^{\\star}) = \\left(\\alpha + \\frac{2x^{\\star}}{\\alpha}\\right)^2 = \\alpha^2 + 2(\\alpha)\\left(\\frac{2x^{\\star}}{\\alpha}\\right) + \\left(\\frac{2x^{\\star}}{\\alpha}\\right)^2$\n$4+4x^{\\star} = \\alpha^2 + 4x^{\\star} + \\frac{4(x^{\\star})^2}{\\alpha^2}$\n\nThe $4x^{\\star}$ terms cancel on both sides:\n$4 = \\alpha^2 + \\frac{4(x^{\\star})^2}{\\alpha^2}$\n$4 - \\alpha^2 = \\frac{4(x^{\\star})^2}{\\alpha^2}$\n\nSolving for $(x^{\\star})^2$:\n$(x^{\\star})^2 = \\frac{\\alpha^2 (4-\\alpha^2)}{4}$\n\nNow, we take the square root:\n$x^{\\star} = \\pm \\sqrt{\\frac{\\alpha^2 (4-\\alpha^2)}{4}} = \\pm \\frac{|\\alpha|\\sqrt{4-\\alpha^2}}{2}$\n\nWe must select the correct sign. As established from our monotonicity argument, $\\text{sgn}(x^{\\star}) = \\text{sgn}(\\alpha)$.\n- If $\\alpha  0$, then $|\\alpha|=\\alpha$. We need $x^{\\star}0$, so we must choose the positive sign: $x^{\\star} = +\\frac{\\alpha\\sqrt{4-\\alpha^2}}{2}$.\n- If $\\alpha  0$, then $|\\alpha|=-\\alpha$. We need $x^{\\star}0$, so we must choose the negative sign: $x^{\\star} = -\\frac{(-\\alpha)\\sqrt{4-\\alpha^2}}{2} = \\frac{\\alpha\\sqrt{4-\\alpha^2}}{2}$.\n- If $\\alpha = 0$, the formula gives $x^{\\star}=0$, which is correct as $g(0)=0$.\n\nIn all cases, the solution is given by the same expression. The final, simplified, closed-form analytic expression for the root $x^{\\star}$ in terms of $\\alpha$ is:\n$x^{\\star}(\\alpha) = \\frac{\\alpha\\sqrt{4-\\alpha^2}}{2}$\n\nThis derivation satisfies the problem's constraints. It uses the rearranged form (via equation 2) and proceeds with valid algebraic steps to find the exact analytical solution. The expression is valid for $\\alpha \\in (-\\sqrt{2}, \\sqrt{2})$, which ensures $4-\\alpha^2  0$ and that the root $x^{\\star}$ lies strictly within $(-1,1)$.",
            "answer": "$$\\boxed{\\frac{\\alpha}{2}\\sqrt{4-\\alpha^2}}$$"
        }
    ]
}