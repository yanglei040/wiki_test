{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the challenge of stiffness, we begin with a classic, illustrative example. This problem explores a simple linear ODE where the stiffness is controlled by a single parameter, $\\epsilon$. By deriving the exact solution and comparing the performance of the explicit Forward Euler and implicit Backward Euler methods, you will gain a first-hand understanding of how the stability requirement of an explicit method can render it impractical, and why implicit methods are essential for efficiently solving stiff systems .",
            "id": "3198039",
            "problem": "Consider the initial value problem (IVP) for the Ordinary Differential Equation (ODE): $y^{\\prime}(t) = -\\dfrac{1}{\\epsilon} y(t) + \\sin(t)$ over the interval $t \\in [0, 10]$ with initial condition $y(0) = 0$. The parameter $\\epsilon$ is a small positive constant given by $\\epsilon = 10^{-6}$, and the angle unit for $t$ in the sine function is radians. This IVP exhibits stiffness due to the presence of the term $-\\dfrac{1}{\\epsilon} y(t)$, which introduces a fast-decaying component in the solution.\n\nStarting from fundamental definitions and well-tested facts, do the following:\n\n1. Use the integrating factor method based on the linear ODE structure to derive the exact closed-form solution $y(t)$ for the given IVP. Your derivation must begin from the definition of a first-order linear ODE and the integrating factor construction and must not rely on any shortcut formulas given to you.\n\n2. Using the definition of the derivative as a limit of difference quotients and the idea of time discretization with a fixed step size $h$, derive:\n   - The explicit forward Euler update rule by approximating $y^{\\prime}(t)$ at the left endpoint of each time step.\n   - The implicit backward Euler update rule by approximating $y^{\\prime}(t)$ at the right endpoint of each time step.\n   Both derivations must originate from the definition $y^{\\prime}(t) = \\lim_{h \\to 0} \\dfrac{y(t+h) - y(t)}{h}$ and must not assume any pre-formed discrete update formulas.\n\n3. Explain, using the linear stability analysis for the Dahlquist test equation $y^{\\prime} = \\lambda y$, why the IVP is stiff when $\\epsilon$ is small, and how this affects the permissible explicit step sizes. In particular, reason from first principles to identify constraints on $h$ for explicit methods and contrast them with the behavior of the implicit method.\n\n4. Implement both derived methods to numerically approximate $y(t)$ over $t \\in [0, 10]$ for a fixed set of time steps $h$. For each method and each time step, compute the maximum absolute error over the discrete grid points against the exact solution you derived. If during the explicit integration the numerical solution becomes non-finite (for example due to overflow), treat the maximum error for that case as $+\\infty$.\n\nAngle unit specification: all trigonometric evaluations must use radians.\n\nTest suite:\n- Parameter: $\\epsilon = 10^{-6}$.\n- Initial condition: $y(0) = 0$.\n- Interval: $[0, 10]$.\n- Step sizes: $h \\in \\{0.001, 0.01, 0.1, 1.0\\}$.\n\nFor each step size $h$ in the above set, evaluate:\n- The maximum absolute error of the explicit forward Euler method over the discrete time grid $\\{0, h, 2h, \\dots, 10\\}$.\n- The maximum absolute error of the implicit backward Euler method over the same grid.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\"). The list must aggregate the results in the following order: for each $h$ in increasing order, first the explicit method’s maximum error, then the implicit method’s maximum error, so the final output has eight entries corresponding to $h = 0.001$, $h = 0.01$, $h = 0.1$, and $h = 1.0$ in that order.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a canonical example of a stiff ordinary differential equation (ODE) and requests derivations and analyses that are fundamental to the field of computational science. The problem statement is free of contradictions or fallacies.\n\nThe solution is structured into four parts, corresponding to the tasks outlined in the problem description.\n\n### 1. Derivation of the Exact Closed-Form Solution\n\nThe given initial value problem (IVP) is:\n$$\ny^{\\prime}(t) = -\\frac{1}{\\epsilon} y(t) + \\sin(t), \\quad y(0) = 0, \\quad t \\in [0, 10]\n$$\nThis is a first-order linear ODE. The standard form for such an equation is $y' + p(t)y = q(t)$. By rearranging the given ODE, we identify the corresponding terms:\n$$\ny^{\\prime}(t) + \\frac{1}{\\epsilon} y(t) = \\sin(t)\n$$\nHere, $p(t) = \\frac{1}{\\epsilon}$ and $q(t) = \\sin(t)$.\n\nTo solve this, we use the method of integrating factors. The integrating factor, $I(t)$, is defined as:\n$$\nI(t) = e^{\\int p(t) dt}\n$$\nFor this problem, the integral is:\n$$\n\\int p(t) dt = \\int \\frac{1}{\\epsilon} dt = \\frac{t}{\\epsilon}\n$$\nThus, the integrating factor is $I(t) = e^{t/\\epsilon}$. We multiply the standard form of the ODE by $I(t)$:\n$$\ne^{t/\\epsilon} y^{\\prime}(t) + \\frac{1}{\\epsilon} e^{t/\\epsilon} y(t) = e^{t/\\epsilon} \\sin(t)\n$$\nThe left-hand side is, by the product rule of differentiation, the derivative of $y(t)I(t)$:\n$$\n\\frac{d}{dt} \\left( y(t) e^{t/\\epsilon} \\right) = e^{t/\\epsilon} \\sin(t)\n$$\nTo find $y(t)$, we integrate both sides with respect to $t$:\n$$\n\\int \\frac{d}{dt} \\left( y(t) e^{t/\\epsilon} \\right) dt = \\int e^{t/\\epsilon} \\sin(t) dt\n$$\n$$\ny(t) e^{t/\\epsilon} = \\int e^{t/\\epsilon} \\sin(t) dt\n$$\nThe integral on the right-hand side can be solved using integration by parts twice. The general solution for an integral of the form $\\int e^{at}\\sin(bt)dt$ is $\\frac{e^{at}}{a^2+b^2}(a\\sin(bt) - b\\cos(bt))$. With $a = 1/\\epsilon$ and $b = 1$, we have:\n$$\n\\int e^{t/\\epsilon} \\sin(t) dt = \\frac{e^{t/\\epsilon}}{(1/\\epsilon)^2 + 1^2} \\left( \\frac{1}{\\epsilon}\\sin(t) - 1\\cos(t) \\right) + C\n$$\nwhere $C$ is the constant of integration. Simplifying this expression:\n$$\n\\int e^{t/\\epsilon} \\sin(t) dt = \\frac{e^{t/\\epsilon}}{(1+\\epsilon^2)/\\epsilon^2} \\left( \\frac{\\sin(t) - \\epsilon\\cos(t)}{\\epsilon} \\right) + C = \\frac{\\epsilon^2 e^{t/\\epsilon}}{1+\\epsilon^2} \\frac{\\sin(t) - \\epsilon\\cos(t)}{\\epsilon} + C\n$$\n$$\n= \\frac{\\epsilon e^{t/\\epsilon}}{1+\\epsilon^2} (\\sin(t) - \\epsilon\\cos(t)) + C = \\frac{e^{t/\\epsilon}}{1+\\epsilon^2} (\\epsilon\\sin(t) - \\epsilon^2\\cos(t)) + C\n$$\nSubstituting this back into the equation for $y(t)e^{t/\\epsilon}$:\n$$\ny(t) e^{t/\\epsilon} = \\frac{e^{t/\\epsilon}}{1+\\epsilon^2} (\\epsilon\\sin(t) - \\epsilon^2\\cos(t)) + C\n$$\nSolving for $y(t)$ by multiplying by $e^{-t/\\epsilon}$:\n$$\ny(t) = \\frac{\\epsilon\\sin(t) - \\epsilon^2\\cos(t)}{1+\\epsilon^2} + C e^{-t/\\epsilon}\n$$\nNow, we apply the initial condition $y(0) = 0$ to find the constant $C$:\n$$\ny(0) = 0 = \\frac{\\epsilon\\sin(0) - \\epsilon^2\\cos(0)}{1+\\epsilon^2} + C e^{0}\n$$\n$$\n0 = \\frac{0 - \\epsilon^2(1)}{1+\\epsilon^2} + C \\implies C = \\frac{\\epsilon^2}{1+\\epsilon^2}\n$$\nSubstituting the value of $C$ back gives the exact closed-form solution to the IVP:\n$$\ny(t) = \\frac{\\epsilon\\sin(t) - \\epsilon^2\\cos(t)}{1+\\epsilon^2} + \\frac{\\epsilon^2}{1+\\epsilon^2} e^{-t/\\epsilon}\n$$\n\n### 2. Derivation of Numerical Methods\n\nWe derive the numerical update rules from the limit definition of the derivative, $y^{\\prime}(t) = \\lim_{h \\to 0} \\frac{y(t+h) - y(t)}{h}$. We consider a discrete time grid $t_n = n h$ for $n=0, 1, 2, \\dots$, where $h$ is the step size. Let $y_n$ be the numerical approximation to $y(t_n)$.\n\n**Explicit Forward Euler Method**\nThe forward Euler method approximates the derivative $y'(t)$ at the beginning of the time step, $t_n$. Using a first-order forward difference quotient based on the limit definition:\n$$\ny^{\\prime}(t_n) \\approx \\frac{y(t_{n+1}) - y(t_n)}{h}\n$$\nSubstituting this into the ODE, $y'(t) = f(t, y(t)) = -\\frac{1}{\\epsilon} y(t) + \\sin(t)$, at time $t_n$:\n$$\n\\frac{y_{n+1} - y_n}{h} = -\\frac{1}{\\epsilon} y_n + \\sin(t_n)\n$$\nSolving for $y_{n+1}$ gives the explicit forward Euler update rule:\n$$\ny_{n+1} = y_n + h \\left( -\\frac{1}{\\epsilon} y_n + \\sin(t_n) \\right)\n$$\nThis rule is \"explicit\" because $y_{n+1}$ is computed directly from known values at time $t_n$.\n\n**Implicit Backward Euler Method**\nThe backward Euler method approximates the derivative by evaluating the ODE at the end of the time step, $t_{n+1}$. The derivative $y'(t_{n+1})$ is approximated using a backward difference quotient:\n$$\ny^{\\prime}(t_{n+1}) \\approx \\frac{y(t_{n+1}) - y(t_n)}{h}\n$$\nSubstituting this into the ODE at time $t_{n+1}$:\n$$\n\\frac{y_{n+1} - y_n}{h} = -\\frac{1}{\\epsilon} y_{n+1} + \\sin(t_{n+1})\n$$\nThis equation is \"implicit\" because the unknown $y_{n+1}$ appears on both sides. We must solve for $y_{n+1}$:\n$$\ny_{n+1} - y_n = h \\left( -\\frac{1}{\\epsilon} y_{n+1} + \\sin(t_{n+1}) \\right)\n$$\n$$\ny_{n+1} - y_n = -\\frac{h}{\\epsilon} y_{n+1} + h\\sin(t_{n+1})\n$$\nGathering terms with $y_{n+1}$:\n$$\ny_{n+1} + \\frac{h}{\\epsilon} y_{n+1} = y_n + h\\sin(t_{n+1})\n$$\n$$\ny_{n+1} \\left( 1 + \\frac{h}{\\epsilon} \\right) = y_n + h\\sin(t_{n+1})\n$$\nFinally, solving for $y_{n+1}$ gives the implicit backward Euler update rule:\n$$\ny_{n+1} = \\frac{y_n + h\\sin(t_{n+1})}{1 + h/\\epsilon}\n$$\n\n### 3. Stability Analysis and Stiffness\n\nThe concept of stiffness is analyzed using the Dahlquist test equation, $y' = \\lambda y$, where $\\text{Re}(\\lambda)  0$. For the given ODE, the dynamics are dominated by the homogeneous part, $y' = -\\frac{1}{\\epsilon} y$. Thus, we identify $\\lambda = -1/\\epsilon$. Since $\\epsilon = 10^{-6}$ is a small positive constant, $\\lambda = -10^6$ is a large negative real number.\n\n**Forward Euler Stability**\nApplying the forward Euler rule to $y'=\\lambda y$:\n$$\ny_{n+1} = y_n + h(\\lambda y_n) = (1+h\\lambda) y_n\n$$\nFor the numerical solution to remain bounded (i.e., stable), the amplification factor, $R(h\\lambda) = 1+h\\lambda$, must satisfy $|R(h\\lambda)| \\le 1$.\n$$\n|1 + h\\lambda| \\le 1\n$$\nWith $\\lambda = -1/\\epsilon$, this becomes $|1 - h/\\epsilon| \\le 1$. This inequality is equivalent to $-1 \\le 1 - h/\\epsilon \\le 1$.\nThe right side, $1 - h/\\epsilon \\le 1$, implies $h/\\epsilon \\ge 0$, which is always true for positive $h$ and $\\epsilon$.\nThe left side, $-1 \\le 1 - h/\\epsilon$, implies $h/\\epsilon \\le 2$, or $h \\le 2\\epsilon$.\nFor $\\epsilon=10^{-6}$, the step size must satisfy $h \\le 2 \\times 10^{-6}$. This is a severe restriction. For any step size $h  2\\epsilon$, the numerical solution will become unbounded, exhibiting oscillations that grow exponentially.\n\n**Backward Euler Stability**\nApplying the backward Euler rule to $y'=\\lambda y$:\n$$\ny_{n+1} = y_n + h(\\lambda y_{n+1}) \\implies y_{n+1}(1 - h\\lambda) = y_n \\implies y_{n+1} = \\frac{1}{1-h\\lambda} y_n\n$$\nThe amplification factor is $R(h\\lambda) = \\frac{1}{1-h\\lambda}$. For stability, we require $|R(h\\lambda)| \\le 1$.\n$$\n\\left| \\frac{1}{1-h\\lambda} \\right| \\le 1\n$$\nWith $\\lambda = -1/\\epsilon$, this is $\\left| \\frac{1}{1+h/\\epsilon} \\right|$. Since $h  0$ and $\\epsilon  0$, the denominator $1+h/\\epsilon$ is always greater than $1$. Therefore, its reciprocal is always less than $1$ in magnitude. The method is stable for any choice of $h  0$. This property is known as A-stability.\n\n**Stiffness Explanation**\nThe IVP is stiff because the exact solution $y(t) = \\frac{\\epsilon\\sin(t) - \\epsilon^2\\cos(t)}{1+\\epsilon^2} + \\frac{\\epsilon^2}{1+\\epsilon^2} e^{-t/\\epsilon}$ contains two components with vastly different time scales.\n1. A fast-decaying transient component, proportional to $e^{-t/\\epsilon}$, which has a characteristic time scale of $\\tau_{fast} = \\epsilon = 10^{-6}$. This component becomes negligible almost instantaneously.\n2. A slowly varying component, which oscillates with a time scale of $\\tau_{slow} \\approx 2\\pi$.\n\nThe stiffness arises because the stability of an explicit method like forward Euler is governed by the fastest time scale ($\\tau_{fast}$), forcing the use of an impractically small step size ($h \\le 2\\epsilon$) even long after the fast component has vanished. An implicit method like backward Euler, being unconditionally stable for this problem, is not constrained by $\\tau_{fast}$ for stability. It can use a much larger step size $h$ determined by the accuracy requirements for capturing the slow component, making it far more efficient for stiff problems.\n\n### 4. Implementation and Error Analysis\n\nThe forward and backward Euler methods are implemented to solve the IVP over $t \\in [0, 10]$ for the step sizes $h \\in \\{0.001, 0.01, 0.1, 1.0\\}$. Since all these step sizes violate the forward Euler stability condition ($h  2 \\times 10^{-6}$), the explicit method is expected to produce numerically unstable solutions that grow without bound, resulting in an infinite maximum error. The implicit method, being stable, should produce accurate results, with the error decreasing as $h$ decreases. The maximum absolute error between the numerical solution and the exact solution is computed on the grid points for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a stiff ODE using Forward and Backward Euler methods,\n    and calculates the maximum absolute error against the exact solution.\n    \"\"\"\n    \n    # Define problem parameters from the statement\n    epsilon = 1e-6\n    t_end = 10.0\n    y0 = 0.0\n    \n    # Test suite of step sizes, in increasing order as required for output.\n    h_values = [0.001, 0.01, 0.1, 1.0]\n    \n    # List to store the results in the specified order.\n    results = []\n\n    # Exact solution derived via integrating factor\n    # y(t) = (epsilon*sin(t) - epsilon^2*cos(t))/(1 + epsilon^2) \n    #        + (epsilon^2 / (1 + epsilon^2)) * exp(-t/epsilon)\n    def y_exact(t, eps):\n        term1 = (eps * np.sin(t) - eps**2 * np.cos(t)) / (1 + eps**2)\n        term2 = (eps**2 / (1 + eps**2)) * np.exp(-t / eps)\n        return term1 + term2\n\n    # Loop over each step size\n    for h in h_values:\n        # Create a stable time grid from 0 to t_end\n        # Using np.linspace is more robust against floating point errors than np.arange\n        num_steps = int(round(t_end / h))\n        t_points = np.linspace(0, t_end, num_steps + 1)\n        \n        # --- Explicit Forward Euler Method ---\n        y_fe = np.zeros(num_steps + 1)\n        y_fe[0] = y0\n        is_finite_fe = True\n        \n        for n in range(num_steps):\n            # y_{n+1} = y_n + h * f(t_n, y_n)\n            # f(t,y) = -y/epsilon + sin(t)\n            y_fe[n+1] = y_fe[n] + h * (-y_fe[n] / epsilon + np.sin(t_points[n]))\n            # Check for overflow at each step to prevent warnings and handle correctly.\n            if not np.isfinite(y_fe[n+1]):\n                is_finite_fe = False\n                break\n        \n        # Calculate maximum absolute error for Forward Euler\n        if is_finite_fe:\n            y_true = y_exact(t_points, epsilon)\n            error_fe = np.max(np.abs(y_fe - y_true))\n        else:\n            # As per problem, if solution is non-finite, error is +inf\n            error_fe = float('inf')\n        \n        results.append(error_fe)\n        \n        # --- Implicit Backward Euler Method ---\n        y_be = np.zeros(num_steps + 1)\n        y_be[0] = y0\n        \n        for n in range(num_steps):\n            # y_{n+1} = (y_n + h*sin(t_{n+1})) / (1 + h/epsilon)\n            numerator = y_be[n] + h * np.sin(t_points[n+1])\n            denominator = 1 + h / epsilon\n            y_be[n+1] = numerator / denominator\n\n        # Calculate maximum absolute error for Backward Euler\n        y_true = y_exact(t_points, epsilon)\n        error_be = np.max(np.abs(y_be - y_true))\n        results.append(error_be)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While linear examples are excellent for building intuition, most stiff problems encountered in science and engineering are nonlinear. For these systems, the implicit step of a method like Backward Euler results in a nonlinear algebraic equation that cannot be solved by simple rearrangement. This exercise provides hands-on experience with the standard and powerful technique for overcoming this challenge: using Newton's method to find the solution at each time step, bridging the gap between implicit methods and practical, nonlinear applications .",
            "id": "2442982",
            "problem": "You are given the scalar ordinary differential equation (ODE)\n$$\n\\frac{dy}{dt} = f(t,y) = -k\\,(y - \\sin t) - b\\,y^3,\n$$\nwith initial condition\n$$\ny(0) = y_0,\n$$\non a uniform time grid over the interval\n$$\nt \\in [0,T], \\quad t_n = n\\,h, \\quad n=0,1,\\dots,N, \\quad \\text{with } Nh = T \\text{ and } h0.\n$$\nAngles in the function $\\sin t$ must be interpreted in radians. At each step, use the implicit one-step update\n$$\ny_{n+1} = y_n + h\\,f(t_{n+1}, y_{n+1}),\n$$\nand solve the resulting nonlinear algebraic equation for $y_{n+1}$ using Newton's method with an absolute stopping tolerance of $10^{-12}$ on either the residual or the Newton step, and a maximum of $50$ iterations per time step. Initialize Newton's method at each step with $y_{n}$.\n\nImplement a program that, for each parameter set in the test suite below, advances the numerical solution from $t=0$ to $t=T$ using the above method, and returns the approximation to $y(T)$.\n\nTest suite (each tuple is $(k,b,y_0,T,h)$):\n- Case A (stiff, nonlinear, forced): $(1000, 10, 0, 0.1, 0.001)$.\n- Case B (stiff, linear, forced): $(1000, 0, 1, 0.1, 0.001)$.\n- Case C (single large step, stiff, nonlinear): $(1000, 5, 1, 0.05, 0.05)$.\n- Case D (non-stiff, purely nonlinear): $(0, 50, 1, 0.02, 0.005)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the entries are the approximations to $y(T)$ for the cases A, B, C, D in this order, each rounded to exactly $8$ decimal places. For example, an output with four results must look like\n$$\n[res_A,res_B,res_C,res_D].\n$$",
            "solution": "The problem presented is a scalar ordinary differential equation (ODE) of the form $\\frac{dy}{dt} = f(t,y)$, which is to be solved numerically. The problem is well-defined, scientifically sound, and contains all necessary information for a unique solution via the specified algorithm. Therefore, I will proceed with its resolution.\n\nThe governing equation is:\n$$\n\\frac{dy}{dt} = f(t,y) = -k\\,(y - \\sin t) - b\\,y^3, \\quad y(0) = y_0\n$$\nThis equation describes a system with a linear relaxation term with rate $k$ towards a sinusoidal driving function $\\sin t$, and a nonlinear damping term proportional to $y^3$. The magnitude of $k$ determines the stiffness of the equation. For large $k$, explicit numerical methods would require prohibitively small time steps $h \\ll 1/k$ to maintain stability. The problem correctly prescribes an implicit method to handle this stiffness.\n\nThe numerical integration is performed using the implicit one-step Backward Euler method. For a uniform time grid $t_n = n\\,h$, the update rule from time $t_n$ to $t_{n+1}$ is given by:\n$$\ny_{n+1} = y_n + h\\,f(t_{n+1}, y_{n+1})\n$$\nwhere $y_n$ is the approximation of $y(t_n)$. This equation is implicit because the unknown value $y_{n+1}$ appears on both sides. To solve for $y_{n+1}$ at each time step, we must solve a nonlinear algebraic equation. This equation can be expressed by defining a residual function $R(x)$ whose root we seek:\n$$\nR(x) = x - y_n - h\\,f(t_{n+1}, x) = 0\n$$\nwhere $x$ represents the unknown $y_{n+1}$.\n\nTo find the root of $R(x)=0$, we employ Newton's method, an iterative procedure defined by:\n$$\nx^{(j+1)} = x^{(j)} - \\frac{R(x^{(j)})}{R'(x^{(j)})}\n$$\nwhere $x^{(j)}$ is the guess for the root at iteration $j$, and $R'(x) = \\frac{dR}{dx}$ is the derivative of the residual function with respect to $x$. The initial guess for each time step is specified as the value from the previous step, $x^{(0)} = y_n$.\n\nThe derivative $R'(x)$ is calculated as:\n$$\nR'(x) = \\frac{d}{dx} \\left( x - y_n - h\\,f(t_{n+1}, x) \\right) = 1 - h\\,\\frac{\\partial f}{\\partial y}(t_{n+1}, x)\n$$\nFor the given function $f(t,y)$, its partial derivative with respect to $y$ is:\n$$\n\\frac{\\partial f}{\\partial y}(t,y) = \\frac{\\partial}{\\partial y} \\left( -k\\,(y - \\sin t) - b\\,y^3 \\right) = -k - 3\\,b\\,y^2\n$$\nSubstituting this into the expression for $R'(x)$, we obtain:\n$$\nR'(x) = 1 - h\\,(-k - 3\\,b\\,x^2) = 1 + h\\,k + 3\\,h\\,b\\,x^2\n$$\nThe Newton's iteration for $y_{n+1}$ is therefore a loop that updates the guess $x^{(j)}$ until a stopping criterion is met. The criteria are an absolute tolerance of $10^{-12}$ on the magnitude of the residual, $|R(x^{(j)})|$, or on the magnitude of the Newton step, $|x^{(j+1)} - x^{(j)}|$. The process is terminated if convergence is not achieved within $50$ iterations.\n\nThe overall algorithm for each test case is as follows:\n1.  Initialize parameters $(k, b, y_0, T, h)$ and calculate the total number of steps $N = T/h$.\n2.  Set the initial solution $y = y_0$ and time $t = 0$.\n3.  Loop from $n=0$ to $N-1$:\n    a.  Determine the next time point $t_{n+1} = (n+1)h$.\n    b.  Initialize the Newton's method guess for $y_{n+1}$ as $x^{(0)} = y_n$.\n    c.  Iterate using Newton's method for a maximum of $50$ steps to find $x$ that solves $R(x)=0$. In each iteration $j$:\n        i.  Calculate the residual $R(x^{(j)})$. If its absolute value is less than $10^{-12}$, convergence is achieved. The result for the time step is $x^{(j)}$.\n        ii. Calculate the derivative $R'(x^{(j)})$.\n        iii. Calculate the Newton step $\\Delta x = -R(x^{(j)}) / R'(x^{(j)})$.\n        iv. Update the guess: $x^{(j+1)} = x^{(j)} + \\Delta x$.\n        v. If the absolute value of the step $|\\Delta x|$ is less than $10^{-12}$, convergence is achieved. The result for the time step is $x^{(j+1)}$.\n    d.  Update the solution for the next time step: $y_{n+1} = x_{converged}$.\n4.  The final value of $y$ after $N$ steps is the required approximation of $y(T)$. This procedure is repeated for all specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(k, b, y0, T, h):\n    \"\"\"\n    Solves the ODE dy/dt = -k*(y - sin(t)) - b*y^3 using Backward Euler\n    with Newton's method for the nonlinear solve.\n    \n    Args:\n        k (float): Stiffness parameter.\n        b (float): Nonlinearity parameter.\n        y0 (float): Initial condition y(0).\n        T (float): Final time.\n        h (float): Time step size.\n\n    Returns:\n        float: The numerical solution y(T).\n    \"\"\"\n\n    # Define the ODE function f(t, y)\n    def f(t, y, k, b):\n        return -k * (y - np.sin(t)) - b * y**3\n\n    # Define the partial derivative of f with respect to y, df/dy\n    def df_dy(t, y, k, b):\n        return -k - 3 * b * y**2\n\n    # Ensure n_steps is integer\n    n_steps = int(round(T / h))\n    if not np.isclose(n_steps * h, T):\n        # This case is not expected based on the problem statement's test cases\n        # but is good practice.\n        raise ValueError(\"T must be an integer multiple of h.\")\n\n    y_current = y0\n\n    # Main time-stepping loop\n    for n in range(n_steps):\n        t_next = (n + 1) * h\n        y_guess = y_current\n        \n        # Newton's method to solve y_next = y_current + h * f(t_next, y_next)\n        # This is equivalent to finding the root of R(y_next) = 0 where\n        # R(y_next) = y_next - y_current - h * f(t_next, y_next)\n        for _ in range(50): # Maximum of 50 iterations\n            \n            # Calculate residual at the current guess\n            residual = y_guess - y_current - h * f(t_next, y_guess, k, b)\n\n            # Check for convergence on residual\n            if abs(residual)  1e-12:\n                break\n                \n            # Calculate Jacobian of the residual function\n            # R'(y) = 1 - h * df/dy\n            jac_residual = 1.0 - h * df_dy(t_next, y_guess, k, b)\n            \n            # Avoid division by zero, although not an issue for this problem's parameters\n            if jac_residual == 0:\n                break\n\n            # Calculate Newton step\n            step = -residual / jac_residual\n            \n            # Update the guess\n            y_guess += step\n            \n            # Check for convergence on the step size\n            if abs(step)  1e-12:\n                break\n        \n        y_current = y_guess\n\n    return y_current\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite: (k, b, y0, T, h)\n    test_cases = [\n        (1000, 10, 0, 0.1, 0.001),     # Case A\n        (1000, 0, 1, 0.1, 0.001),      # Case B\n        (1000, 5, 1, 0.05, 0.05),       # Case C\n        (0, 50, 1, 0.02, 0.005)        # Case D\n    ]\n\n    results = []\n    for params in test_cases:\n        y_final = run_simulation(*params)\n        results.append(y_final)\n\n    # Format output as specified\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having learned to identify and solve stiff systems, we now turn to a more advanced, practical question: how can a program automatically detect stiffness and choose the right tool for the job? This capstone exercise guides you through building a \"stiffness detector,\" a core component of modern adaptive ODE solvers. You will learn to numerically estimate a system's local Jacobian, analyze its eigenvalues, and apply stability theory to decide whether a cost-effective explicit method is safe or a robust implicit method is necessary, synthesizing theory into a practical algorithm .",
            "id": "3279238",
            "problem": "You are asked to design and implement a program that, for a given autonomous Ordinary Differential Equation (ODE) system of the form $\\frac{d y}{d t} = f(y)$, constructs a local \"stiffness detector\" at a specified point $(t, y)$ and time step $h$. The detector must numerically estimate the Jacobian matrix $J = \\frac{\\partial f}{\\partial y}$ at $(t, y)$ and use a principled stability test based on the absolute stability requirement of an explicit one-step method to decide between an explicit solver and an implicit solver. The decision criterion must be derived from fundamental definitions (without using shortcut formulas provided in the problem statement): linearize the system locally, characterize the amplification of modes under the method, and determine the condition ensuring that all modes decay rather than grow.\n\nYour program must:\n- Implement a numerical Jacobian estimator using finite differences at $(t, y)$. You may assume that $f$ is sufficiently smooth in a neighborhood of $(t, y)$ and that the system dimension equals the state dimension, so $J$ is square.\n- Compute the eigenvalues of the estimated Jacobian and apply the absolute stability requirement of the explicit method by checking the modal amplification factors implied by the local linearization and the explicit update. Use a conservative decision rule: if any mode violates the strict stability requirement at the given $h$, the detector must choose the implicit solver; otherwise it must choose the explicit solver.\n- Return a boolean decision for each test case, where $\\text{True}$ means \"choose implicit solver\" and $\\text{False}$ means \"choose explicit solver.\"\n\nImplement and evaluate your detector on the following test suite, which exercises scalar, linear multi-dimensional, and nonlinear systems, including happy-path, boundary, and edge cases. In all cases below, the system is autonomous, so $f$ depends only on $y$; you must still pass $t$ to functions for interface consistency, but $t$ is unused in computations.\n\nTest cases:\n1. Scalar linear decay (happy path, explicit stability holds):\n   - System: $\\frac{d y}{d t} = -1000\\, y$\n   - Evaluation point: $t = 0$, $y = 1$\n   - Time step: $h = 10^{-3}$\n   - Expected detector decision: explicit solver is acceptable.\n2. Scalar linear decay (boundary/unstable explicit; conservative decision chooses implicit):\n   - System: $\\frac{d y}{d t} = -1000\\, y$\n   - Evaluation point: $t = 0$, $y = 1$\n   - Time step: $h = 2\\times 10^{-3}$\n   - Expected detector decision: explicit solver violates strict stability at this $h$; choose implicit.\n3. Two-dimensional damped oscillator (explicit stability holds at small step):\n   - System: $\\frac{d}{dt}\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} y_2 \\\\ -\\omega^2 y_1 - 2 \\zeta \\omega y_2 \\end{bmatrix}$ with $\\omega = 50$ and $\\zeta = 0.05$\n   - Evaluation point: $t = 0$, $y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n   - Time step: $h = 10^{-3}$\n   - Expected detector decision: explicit solver is acceptable.\n4. Two-dimensional damped oscillator (explicit instability at larger step):\n   - System: same as case 3\n   - Evaluation point: $t = 0$, $y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n   - Time step: $h = 5\\times 10^{-2}$\n   - Expected detector decision: choose implicit solver.\n5. Two-dimensional nonlinear reaction model (explicit stability holds at small step):\n   - System: $\\frac{d}{dt}\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} -k_1 y_1 + k_2 y_2^2 \\\\ k_1 y_1 - k_2 y_2^2 - k_3 y_2 \\end{bmatrix}$ with $k_1 = 1$, $k_2 = 1$, $k_3 = 1000$\n   - Evaluation point: $t = 0$, $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n   - Time step: $h = 10^{-3}$\n   - Expected detector decision: explicit solver is acceptable.\n6. Two-dimensional nonlinear reaction model (explicit instability at larger step):\n   - System: same as case 5\n   - Evaluation point: $t = 0$, $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n   - Time step: $h = 10^{-2}$\n   - Expected detector decision: choose implicit solver.\n\nNumerical Jacobian estimation requirement:\n- Use a central finite difference approximation for each column $j$ of $J$, perturbing $y_j$ by a small increment $\\delta_j$ chosen as $\\delta_j = \\sqrt{\\varepsilon}\\,\\max\\{1, |y_j|\\}$, where $\\varepsilon$ is the machine epsilon for double-precision floating point arithmetic.\n\nSolver decision rule:\n- Formally linearize the system at $(t, y)$ and apply the absolute stability requirement of the explicit one-step method to all eigenmodes. If any mode fails the strict stability requirement at the given $h$, return $\\text{True}$ (choose implicit); otherwise return $\\text{False}$ (choose explicit). Treat equality to the stability boundary as unstable for the explicit method in the detector.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5,result6]\"), where each result is the boolean decision for the corresponding test case in the order listed above.",
            "solution": "The problem requires the design and implementation of a numerical stiffness detector for an autonomous Ordinary Differential Equation (ODE) system $\\frac{d y}{d t} = f(y)$. The detector's purpose is to decide, at a given point $(t, y)$ and for a specific time step $h$, whether an explicit or an implicit solver is more appropriate. The decision must be based on the absolute stability properties of a prototype explicit one-step method applied to a local linearization of the ODE system.\n\n### Principle 1: Local Linearization\nA general nonlinear system of ODEs, $\\frac{d y}{d t} = f(y)$, can be analyzed locally around a point $y_0$ by considering the evolution of a small perturbation, $\\delta y(t) = y(t) - y_0$. A first-order Taylor expansion of $f(y)$ around $y_0$ gives:\n$$ f(y) = f(y_0) + \\frac{\\partial f}{\\partial y}\\bigg|_{y_0} (y - y_0) + \\mathcal{O}(\\|y - y_0\\|^2) $$\nLet $J(y_0) = \\frac{\\partial f}{\\partial y}\\big|_{y_0}$ be the Jacobian matrix of $f$ evaluated at $y_0$. The rate of change of the perturbation is then:\n$$ \\frac{d(\\delta y)}{dt} = \\frac{d y}{dt} = f(y) \\approx f(y_0) + J(y_0) \\delta y $$\nFor an autonomous system evaluated at a specific state $y_0$, we can study the stability of the linearized system by analyzing the homogeneous equation for the perturbation:\n$$ \\frac{d(\\delta y)}{dt} \\approx J(y_0) \\delta y $$\nThe local stability and stiffness characteristics of the original nonlinear system are governed by the properties of this linear system, specifically the eigenvalues of the Jacobian matrix $J(y_0)$.\n\n### Principle 2: Absolute Stability of Explicit Methods\nWe consider the Forward Euler method as the representative explicit one-step method for solving an ODE. The update rule is:\n$$ y_{n+1} = y_n + h f(y_n) $$\nwhere $h$ is the time step. Applying this method to the linearized perturbation equation gives an update rule for the perturbation $\\delta y_n$ from one step to the next:\n$$ \\delta y_{n+1} = \\delta y_n + h (J \\cdot \\delta y_n) = (I + hJ) \\delta y_n $$\nHere, $I$ is the identity matrix and $J$ is the Jacobian evaluated at the point of interest. The matrix $G = I + hJ$ is called the amplification matrix, as it determines how perturbations are amplified or damped from one step to the next.\n\nTo analyze the behavior of $G$, we consider its action on the eigenvectors of $J$. Let $\\{\\lambda_k\\}$ be the eigenvalues of $J$ with corresponding eigenvectors $\\{v_k\\}$. If we express a perturbation $\\delta y_n$ as a linear combination of these eigenvectors, $\\delta y_n = \\sum_k c_k v_k$, its evolution is:\n$$ \\delta y_{n+1} = (I + hJ) \\sum_k c_k v_k = \\sum_k c_k (I + hJ) v_k = \\sum_k c_k (v_k + h \\lambda_k v_k) = \\sum_k c_k (1 + h\\lambda_k) v_k $$\nFor the numerical solution to be stable, any perturbation must decay over time. This requires the magnitude of the amplification factor for each mode, $g_k = 1 + h\\lambda_k$, to be strictly less than $1$. This gives the absolute stability requirement:\n$$ |1 + h\\lambda_k|  1 \\quad \\text{for all eigenvalues } \\lambda_k \\text{ of } J $$\nThe set of complex numbers $z = h\\lambda$ that satisfy $|1+z|  1$ forms the region of absolute stability. This inequality describes the interior of a circle of radius $1$ centered at $(-1, 0)$ in the complex plane.\n\n### The Stiffness Detector Algorithm\nThe detector formalizes this stability check into a decision rule. A system is considered \"stiff\" with respect to a given step size $h$ if $h$ is too large to resolve the fastest-decaying modes of the system stably with an explicit method.\n\n1.  **Decision Criterion:** Given the system $f$, point $(t, y_0)$, and step size $h$, the detector will compute the eigenvalues $\\{\\lambda_k\\}$ of the Jacobian $J(y_0)$. It will then check if the stability condition is met. The problem specifies a conservative rule: if the condition is violated for any mode, including on the boundary, an implicit method should be chosen. Therefore, the detector will return `True` (choose implicit) if:\n    $$ \\exists k \\text{ such that } |1 + h\\lambda_k| \\ge 1 $$\n    Otherwise, if $|1 + h\\lambda_k|  1$ for all $k$, it will return `False` (choose explicit).\n\n2.  **Numerical Jacobian Estimation:** Since the analytical Jacobian may not be available, it is estimated numerically. For each column $j$ of the Jacobian $J$, we use a central finite difference approximation. This involves perturbing the $j$-th component of the state vector $y$ by a small amount $\\delta_j$ in both directions.\n    $$ J_{:,j} = \\frac{\\partial f}{\\partial y_j} \\approx \\frac{f(y + \\delta_j \\mathbf{e}_j) - f(y - \\delta_j \\mathbf{e}_j)}{2 \\delta_j} $$\n    where $\\mathbf{e}_j$ is the $j$-th standard basis vector. The perturbation size $\\delta_j$ is chosen to balance truncation error (from the Taylor approximation) and round-off error (from floating-point arithmetic). A robust choice, as specified, is:\n    $$ \\delta_j = \\sqrt{\\varepsilon} \\cdot \\max\\{1, |y_j|\\} $$\n    where $\\varepsilon$ is the machine epsilon for double-precision floating-point numbers.\n\n3.  **Overall Procedure:**\n    a. Given $f$, $t_0$, $y_0$, and $h$.\n    b. Construct the numerical Jacobian matrix $J_{num}$ at $y_0$ using the central difference formula for each column.\n    c. Compute the eigenvalues $\\{\\lambda_k\\}$ of $J_{num}$. These eigenvalues may be complex.\n    d. For each eigenvalue $\\lambda_k$, calculate the value of the complex number $z_k = h\\lambda_k$.\n    e. Check if $|1 + z_k| \\ge 1$. If this holds for any $k$, the decision is `True`.\n    f. If the condition in (e) is not met for any eigenvalue, the decision is `False`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef numerical_jacobian(f, t, y, machine_eps):\n    \"\"\"\n    Computes the Jacobian of f at (t, y) using central finite differences.\n\n    Args:\n        f (callable): The RHS function of the ODE, f(t, y).\n        t (float): The current time.\n        y (np.ndarray or float): The current state.\n        machine_eps (float): The machine epsilon for float precision.\n\n    Returns:\n        np.ndarray: The estimated Jacobian matrix.\n    \"\"\"\n    y = np.atleast_1d(y).astype(float)\n    n = len(y)\n    J = np.zeros((n, n), dtype=float)\n    \n    for j in range(n):\n        # Create a perturbation vector\n        pert_vec = np.zeros_like(y)\n        \n        # Calculate optimal perturbation size\n        delta = np.sqrt(machine_eps) * max(1.0, abs(y[j]))\n        pert_vec[j] = delta\n        \n        # Central difference formula\n        f_plus = f(t, y + pert_vec)\n        f_minus = f(t, y - pert_vec)\n        \n        J[:, j] = (f_plus - f_minus) / (2 * delta)\n        \n    return J\n\ndef stiffness_detector(f, t, y, h):\n    \"\"\"\n    Decides whether to use an implicit or explicit solver based on stability.\n    \n    Args:\n        f (callable): The RHS function of the ODE, f(t, y).\n        t (float): The evaluation time.\n        y (np.ndarray or float): The evaluation point.\n        h (float): The time step.\n        \n    Returns:\n        bool: True if an implicit solver is recommended, False otherwise.\n    \"\"\"\n    machine_eps = np.finfo(float).eps\n    J = numerical_jacobian(f, t, y, machine_eps)\n    eigenvalues = np.linalg.eigvals(J)\n    \n    # Check the absolute stability condition for Forward Euler for each eigenvalue.\n    # The region of absolute stability is |1 + h*lambda|  1.\n    # If any eigenvalue falls outside or on the boundary, we need an implicit method.\n    for lam in eigenvalues:\n        amplification_factor = 1 + h * lam\n        if np.abs(amplification_factor) = 1.0:\n            return True  # Choose implicit solver\n\n    return False  # Explicit solver is acceptable\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the ODE functions for the test cases\n    def f1(t, y):\n        return -1000.0 * y\n\n    def f3(t, y):\n        omega = 50.0\n        zeta = 0.05\n        # y is a numpy array [y1, y2]\n        return np.array([y[1], -omega**2 * y[0] - 2 * zeta * omega * y[1]])\n\n    def f5(t, y):\n        k1 = 1.0\n        k2 = 1.0\n        k3 = 1000.0\n        # y is a numpy array [y1, y2]\n        return np.array([\n            -k1 * y[0] + k2 * y[1]**2,\n            k1 * y[0] - k2 * y[1]**2 - k3 * y[1]\n        ])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Scalar linear decay (happy path)\n        (f1, 0.0, 1.0, 1e-3),\n        # 2. Scalar linear decay (boundary)\n        (f1, 0.0, 1.0, 2e-3),\n        # 3. Damped oscillator (stable)\n        (f3, 0.0, np.array([1.0, 0.0]), 1e-3),\n        # 4. Damped oscillator (unstable)\n        (f3, 0.0, np.array([1.0, 0.0]), 5e-2),\n        # 5. Nonlinear reaction (stable)\n        (f5, 0.0, np.array([1.0, 1.0]), 1e-3),\n        # 6. Nonlinear reaction (unstable)\n        (f5, 0.0, np.array([1.0, 1.0]), 1e-2),\n    ]\n\n    results = []\n    for f, t, y, h in test_cases:\n        decision = stiffness_detector(f, t, y, h)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}