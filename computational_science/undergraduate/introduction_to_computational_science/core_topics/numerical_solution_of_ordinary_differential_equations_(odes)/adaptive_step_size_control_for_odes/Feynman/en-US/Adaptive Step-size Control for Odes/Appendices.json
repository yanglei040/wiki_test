{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational experience in building an adaptive Ordinary Differential Equation (ODE) solver from the ground up. By implementing the Bogacki-Shampine 2(3) embedded pair, you will directly engage with the core mechanics of adaptive step-size control: estimating local error and using that information to dynamically adjust the step size $h$. Completing this practice  will demystify the inner workings of production-grade ODE solvers and equip you with a fundamental understanding of how numerical accuracy and efficiency are balanced in practice.",
            "id": "3259704",
            "problem": "Consider the Initial Value Problem (IVP) defined by an Ordinary Differential Equation (ODE) of the form $y'(t) = f(t,y)$ with initial condition $y(t_0) = y_0$. A one-step method advances the solution from $(t,y)$ to $(t+h, y_{\\text{new}})$ using only information from the current step. In an explicit Runge-Kutta (RK) method, the increment is constructed from a weighted combination of stage derivatives. An embedded RK pair provides two approximations of different orders computed from the same stages, enabling a local error estimate for adaptive step-size control.\n\nYour task is to implement an embedded Runge-Kutta pair that is consistent with the Bogacki-Shampine $2(3)$ construction and use it to drive an adaptive step-size selection algorithm. The algorithm must:\n\n- Compute stage derivatives and produce a higher-order approximation $y^{[p]}$ and a lower-order approximation $y^{[q]}$ at each step, with $p>q$.\n- Estimate the local truncation error from the difference $y^{[p]} - y^{[q]}$.\n- Accept a step when a scaled error norm is less than or equal to $1$ and reject otherwise.\n- Adjust the step size $h$ using the asymptotic scaling relation between the error and the step size, ensuring numerical stability by applying a safety factor and bounding the growth and decay of $h$.\n- Respect the final time $t_f$ by reducing the last step as necessary so that $t$ reaches $t_f$ exactly.\n\nYou must implement the adaptive integrator for scalar ODEs with the following scaled error norm per step:\n$$\nE = \\frac{|y^{[p]} - y^{[q]}|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[p]}|)},\n$$\nwhere $\\mathrm{atol}$ is the absolute tolerance and $\\mathrm{rtol}$ is the relative tolerance. The step is accepted if $E \\leq 1$. The step-size controller must be based on the principle that an error of order $h^{m}$ implies $h_{\\text{new}} \\propto h E^{-1/m}$ for some integer $m$ that matches the order of the error estimator. Include a multiplicative safety factor and cap the growth and decay by prescribed bounds.\n\nAngle quantities must be interpreted in radians whenever trigonometric functions appear.\n\nImplement the adaptive solver and apply it to the following test suite of IVPs. For each case, return the numerical approximation of $y(t_f)$ as a floating-point number.\n\n- Case $1$ (happy path, exponentially stable):\n  - $f(t,y) = -y$, $y(0) = 1$, $t_0 = 0$, $t_f = 5$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests general performance and stability on a smooth problem.\n\n- Case $2$ (time-dependent growth with trigonometric forcing, radians):\n  - $f(t,y) = y\\sin(t)$, $y(0) = 1$, $t_0 = 0$, $t_f = 3$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-7}$, $\\mathrm{atol} = 10^{-10}$.\n  - Rationale: tests handling of time-dependent coefficients and requires angle in radians.\n\n- Case $3$ (moderately stiff-like linear decay):\n  - $f(t,y) = -15\\,y$, $y(0) = 1$, $t_0 = 0$, $t_f = 1$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests step-size adaptation under faster decay.\n\n- Case $4$ (nonlinear growth near a singularity):\n  - $f(t,y) = y^2$, $y(0) = 1$, $t_0 = 0$, $t_f = 0.9$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests robustness near a blow-up at $t = 1$.\n\n- Case $5$ (zero dynamics, edge case):\n  - $f(t,y) = 0$, $y(0) = 5$, $t_0 = 0$, $t_f = 10$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-8}$, $\\mathrm{atol} = 10^{-12}$.\n  - Rationale: tests the controller when the estimated error is identically zero.\n\nController parameters common to all cases:\n- Safety factor $s = 0.9$,\n- Minimum growth factor $g_{\\min} = 0.2$,\n- Maximum growth factor $g_{\\max} = 5.0$,\n- Initial step size $h_0 = 10^{-3}$,\n- Minimum step size $h_{\\min} = 10^{-12}$,\n- Maximum step size $h_{\\max} = (t_f - t_0)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, for example $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the floating-point approximation of $y(t_f)$ for Case $i$.",
            "solution": "The core of the task is to construct an adaptive one-step integrator from first principles, grounded in the definitions of an Initial Value Problem (IVP), Runge-Kutta (RK) methods, and local truncation error.\n\nAn IVP specifies $y'(t) = f(t,y)$ with $y(t_0) = y_0$. A one-step method computes $y_{n+1}$ from $(t_n,y_n)$ using a single step size $h_n$ without reference to earlier history. In an explicit Runge-Kutta (RK) method, the new value is a weighted combination of stage derivatives. The general explicit RK construction is:\n$$\nk_1 = f(t_n, y_n),\\quad\nk_2 = f(t_n + c_2 h, y_n + h a_{21} k_1),\\quad \\dots,\\quad\nk_s = f(t_n + c_s h, y_n + h \\sum_{j=1}^{s-1} a_{sj} k_j),\n$$\nand an order-$p$ approximation is\n$$\ny^{[p]}_{n+1} = y_n + h \\sum_{j=1}^s b_j k_j.\n$$\nAn embedded pair provides two sets of weights $(b_j)$ and $(\\hat b_j)$ evaluated with the same stages to deliver $y^{[p]}$ and $y^{[q]}$ with $p>q$. The difference\n$$\n\\Delta = y^{[p]}_{n+1} - y^{[q]}_{n+1}\n$$\nis computable with negligible extra cost and scales with a known power of $h$ determined by the construction.\n\nFor the Bogacki-Shampine $2(3)$ pair, the method uses $s=4$ stages with nodes $c_2 = \\tfrac{1}{2}$, $c_3 = \\tfrac{3}{4}$, $c_4 = 1$. The internal coefficients are $a_{21} = \\tfrac{1}{2}$, $a_{32} = \\tfrac{3}{4}$ with $a_{31} = 0$, and $a_{41} = \\tfrac{2}{9}$, $a_{42} = \\tfrac{1}{3}$, $a_{43} = \\tfrac{4}{9}$. The higher-order weights (order $p=3$) are\n$$\nb_1 = \\tfrac{2}{9},\\quad b_2 = \\tfrac{1}{3},\\quad b_3 = \\tfrac{4}{9},\\quad b_4 = 0,\n$$\nand the lower-order weights (order $q=2$) are\n$$\n\\hat b_1 = \\tfrac{7}{24},\\quad \\hat b_2 = \\tfrac{1}{4},\\quad \\hat b_3 = \\tfrac{1}{3},\\quad \\hat b_4 = \\tfrac{1}{8}.\n$$\nThe stages and approximations are computed as\n$$\n\\begin{aligned}\nk_1 &= f(t, y),\\\\\nk_2 &= f\\Big(t + \\tfrac{1}{2}h,\\, y + h\\,\\tfrac{1}{2}\\,k_1\\Big),\\\\\nk_3 &= f\\Big(t + \\tfrac{3}{4}h,\\, y + h\\,\\tfrac{3}{4}\\,k_2\\Big),\\\\\ny^{[3]} &= y + h\\Big(\\tfrac{2}{9}k_1 + \\tfrac{1}{3}k_2 + \\tfrac{4}{9}k_3\\Big),\\\\\nk_4 &= f\\Big(t + h,\\, y^{[3]}\\Big),\\\\\ny^{[2]} &= y + h\\Big(\\tfrac{7}{24}k_1 + \\tfrac{1}{4}k_2 + \\tfrac{1}{3}k_3 + \\tfrac{1}{8}k_4\\Big).\n\\end{aligned}\n$$\nThe embedded difference $\\Delta = y^{[3]} - y^{[2]}$ provides an error estimator with leading behavior $O(h^3)$; that is, it scales like $C h^3$ for smooth $f$. This scaling justifies a controller of the form\n$$\nh_{\\text{new}} = h \\cdot s \\cdot E^{-1/3},\n$$\nwhere $s$ is a safety factor and $E$ is a scaled error norm. To ensure numerical robustness, we bound the multiplicative growth/decay factor by $g_{\\min} \\leq s E^{-1/3} \\leq g_{\\max}$, and we also clip $h_{\\text{new}}$ to $[h_{\\min}, h_{\\max}]$. When $E \\leq 1$, the step is accepted and $y$ advances to $y^{[3]}$; otherwise, the step is rejected and recomputed with a smaller $h$.\n\nThe scaled error norm for scalar problems is chosen to balance absolute and relative tolerances:\n$$\nE = \\frac{|\\Delta|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[3]}|)}.\n$$\nThis ensures that the acceptance criterion $E \\leq 1$ controls the error relative to the magnitude of the solution while preventing division by very small numbers when $y$ is near zero.\n\nAlgorithmic steps for integrating from $t_0$ to $t_f$:\n- Initialize $t = t_0$, $y = y_0$, choose $h$ in $[h_{\\min}, h_{\\max}]$ (for example $h_0$ as given), and set controller parameters $s$, $g_{\\min}$, $g_{\\max}$.\n- While $t < t_f$:\n  - If $t + h > t_f$, set $h = t_f - t$ to land exactly at $t_f$.\n  - Compute $k_1$, $k_2$, $k_3$, $y^{[3]}$, $k_4$, and $y^{[2]}$.\n  - Compute $E$ from $y$, $y^{[3]}$, $y^{[2]}$, $\\mathrm{atol}$, and $\\mathrm{rtol}$.\n  - If $E \\leq 1$, accept the step: set $t \\gets t + h$, $y \\gets y^{[3]}$.\n  - Compute the candidate growth factor $g = s \\cdot E^{-1/3}$; if $E=0$, set $g = g_{\\max}$.\n  - Bound $g$ to $[g_{\\min}, g_{\\max}]$, then update $h \\gets \\mathrm{clip}(h \\cdot g, h_{\\min}, h_{\\max})$.\n  - If a step is rejected ($E > 1$), update $h$ as above and recompute without advancing $t$ or $y$.\n- Return $y(t_f)$.\n\nWe now briefly analyze each test case and its expected behavior:\n- Case $1$: $y'(t) = -y$, exact solution $y(t) = e^{-t}$, so $y(5) = e^{-5}$. The method should take moderate steps and converge rapidly.\n- Case $2$: $y'(t) = y\\sin(t)$ in radians with exact solution $y(t) = \\exp(1 - \\cos t)$, giving $y(3) = \\exp(1 - \\cos 3)$. The algorithm must handle time-dependent forcing smoothly.\n- Case $3$: $y'(t) = -15y$ with exact $y(1) = e^{-15}$, requiring smaller steps initially due to rapid decay, but the controller will increase $h$ as $y$ shrinks.\n- Case $4$: $y'(t) = y^2$ with exact $y(t) = \\frac{1}{1 - t}$ for $t<1$, so $y(0.9) = 10$. The method must adaptively reduce $h$ as $t$ approaches the blow-up at $t=1$.\n- Case $5$: $y'(t) = 0$ yields constant solution $y(t) = 5$; the error estimator is identically zero, and the controller will enlarge $h$ up to $h_{\\max}$.\n\nBy implementing the Bogacki-Shampine $2(3)$ stages and the controller derived from the $O(h^3)$ error estimate, the adaptive solver will provide $y(t_f)$ for all cases, printed in the specified format.",
            "answer": "```python\n# Python 3.12, numpy 1.23.5 allowed; no other libraries.\nimport numpy as np\n\ndef rk23_bogacki_shampine_step(f, t, y, h):\n    \"\"\"\n    Perform one Bogacki-Shampine 2(3) step for a scalar ODE y' = f(t,y).\n    Returns (y_high, y_low) where y_high is the 3rd-order solution, y_low is the 2nd-order embedded solution.\n    \"\"\"\n    k1 = f(t, y)\n    k2 = f(t + 0.5 * h, y + h * 0.5 * k1)\n    k3 = f(t + 0.75 * h, y + h * 0.75 * k2)\n    # 3rd-order solution\n    y3 = y + h * ( (2.0/9.0) * k1 + (1.0/3.0) * k2 + (4.0/9.0) * k3 )\n    # Stage 4 evaluated at t+h, y3\n    k4 = f(t + h, y3)\n    # 2nd-order embedded solution\n    y2 = y + h * ( (7.0/24.0) * k1 + (1.0/4.0) * k2 + (1.0/3.0) * k3 + (1.0/8.0) * k4 )\n    return y3, y2\n\ndef integrate_adaptive(f, t0, tf, y0, rtol, atol,\n                       h0=1e-3, hmin=1e-12, hmax=None,\n                       safety=0.9, growth_min=0.2, growth_max=5.0):\n    \"\"\"\n    Adaptive integrator using Bogacki-Shampine 2(3) pair for scalar ODEs.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    if hmax is None:\n        hmax = abs(tf - t0)\n    h = max(hmin, min(h0, hmax))\n    # Direction of integration\n    direction = 1.0 if tf >= t0 else -1.0\n    h *= direction\n    hmin_signed = hmin * direction\n    hmax_signed = hmax * direction\n\n    # Main integration loop\n    # Guard for max iterations to prevent infinite loops in pathological cases\n    max_steps = 10_000_000\n    steps = 0\n    while (direction > 0 and t < tf) or (direction < 0 and t > tf):\n        steps += 1\n        if steps > max_steps:\n            # Fallback: give current y\n            break\n\n        # Adjust step to not overshoot tf\n        remaining = tf - t\n        if direction * h > direction * remaining:\n            h = remaining\n\n        # Take one RK23 step\n        y3, y2 = rk23_bogacki_shampine_step(f, t, y, h)\n\n        # Scaled error norm (scalar)\n        scale = atol + rtol * max(abs(y), abs(y3))\n        # Prevent zero scale\n        if scale == 0.0:\n            scale = atol\n        err = abs(y3 - y2) / scale\n\n        # Accept or reject\n        if err <= 1.0:\n            # Accept the step\n            t = t + h\n            y = y3\n            # Compute growth factor; estimator scales ~ h^3\n            if err == 0.0:\n                g = growth_max\n            else:\n                g = safety * err ** (-1.0 / 3.0)\n            # Bound growth factor\n            g = max(growth_min, min(g, growth_max))\n            # Update h and clip\n            h = h * g\n            # Clip to [hmin, hmax] with sign\n            if direction > 0:\n                h = min(max(h, hmin_signed), hmax_signed)\n            else:\n                h = max(min(h, hmin_signed), hmax_signed)\n        else:\n            # Reject step; decrease h\n            g = safety * err ** (-1.0 / 3.0)\n            g = max(growth_min, min(g, growth_max))\n            h = h * g\n            # Ensure not below minimum\n            if direction > 0:\n                h = max(h, hmin_signed)\n            else:\n                h = min(h, hmin_signed)\n            # If h becomes too small, break to avoid infinite loop\n            if abs(h) < hmin:\n                # Cannot reduce further; accept and break\n                # This is a conservative fallback\n                t = t + h\n                y = y3\n                break\n\n    return y\n\ndef solve():\n    # Define the test cases\n    # Case 1: y' = -y, y(0) = 1, tf = 5\n    def f1(t, y): return -y\n\n    # Case 2: y' = y*sin(t), radians, y(0) = 1, tf = 3\n    def f2(t, y): return y * np.sin(t)\n\n    # Case 3: y' = -15 y, y(0) = 1, tf = 1\n    def f3(t, y): return -15.0 * y\n\n    # Case 4: y' = y^2, y(0) = 1, tf = 0.9\n    def f4(t, y): return y * y\n\n    # Case 5: y' = 0, y(0) = 5, tf = 10\n    def f5(t, y): return 0.0\n\n    test_cases = [\n        # (f, t0, tf, y0, rtol, atol)\n        (f1, 0.0, 5.0, 1.0, 1e-6, 1e-9),\n        (f2, 0.0, 3.0, 1.0, 1e-7, 1e-10),\n        (f3, 0.0, 1.0, 1.0, 1e-6, 1e-9),\n        (f4, 0.0, 0.9, 1.0, 1e-6, 1e-9),\n        (f5, 0.0, 10.0, 5.0, 1e-8, 1e-12),\n    ]\n\n    results = []\n    for f, t0, tf, y0, rtol, atol in test_cases:\n        ytf = integrate_adaptive(\n            f=f, t0=t0, tf=tf, y0=y0, rtol=rtol, atol=atol,\n            h0=1e-3, hmin=1e-12, hmax=abs(tf - t0),\n            safety=0.9, growth_min=0.2, growth_max=5.0\n        )\n        results.append(ytf)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the foundational implementation, this practice challenges you to apply an adaptive solver to a more complex scenario involving a high-frequency forcing term. You will work with a higher-order method, the Dormand-Prince 5(4) pair, which is a workhorse in modern scientific computing. A key component of this exercise  is deriving the exact analytical solution to the ODE, a critical skill for verifying the accuracy of any numerical method and building confidence in your computational results.",
            "id": "3203815",
            "problem": "Consider the initial value problem for a scalar ordinary differential equation (ODE) with a high-frequency, low-amplitude forcing term,\n$$\n\\frac{dy}{dt} = -y + A \\sin(\\omega t), \\quad y(0) = 0,\n$$\nwhere $A$ is the forcing amplitude and $\\omega$ is the angular frequency (measured in radians per second). The objective is to implement an adaptive step-size control algorithm based on an explicit Runge-Kutta method with an embedded error estimator to integrate the ODE from $t=0$ to $t=T$. The adaptive algorithm must ensure that the local error in each accepted step is controlled with a combined absolute-relative tolerance criterion. The angle unit throughout must be radians.\n\nStarting from the fundamental definition of an initial value problem and the notion of local truncation error for numerical integration of ODEs, design an adaptive explicit Runge-Kutta solver that:\n- Computes stage derivatives using a fixed set of Butcher tableau coefficients for a classical embedded pair.\n- Estimates local error per step using the difference between two solutions of adjacent formal order produced by the embedded pair.\n- Accepts a step when the estimated local error is less than or equal to a user-specified tolerance and rejects otherwise, adjusting the step-size accordingly.\n- Uses a tolerance model of the form\n$$\n\\text{tol} = \\text{atol} + \\text{rtol} | y_{\\text{new}} |,\n$$\nwhere $\\text{atol}$ denotes absolute tolerance and $\\text{rtol}$ denotes relative tolerance.\n- Enforces step-size bounds with a maximum step-size $h_{\\max}$ and a minimum step-size $h_{\\min}$.\n\nDerive from first principles the analytical solution at $t=T$ for the given ODE via an integrating factor, starting from the base linear ODE identity \n$$\n\\frac{dy}{dt} + y = A \\sin(\\omega t)\n$$\nand the product rule for differentiation, and use this analytical solution to compute the absolute error at the terminal time $t=T$ for each test case.\n\nYour program must implement the adaptive solver and compute the absolute errors at $t=T$ against the analytically derived solution for the following test suite of parameter values (the angle unit is radians):\n- Test case $1$: $A = 0.01$, $\\omega = 1000$, $T = 1$, $\\text{rtol} = 10^{-6}$, $\\text{atol} = 10^{-9}$, $h_0 = 10^{-3}$, $h_{\\max} = 5 \\cdot 10^{-2}$.\n- Test case $2$: $A = 0.01$, $\\omega = 1000$, $T = 1$, $\\text{rtol} = 10^{-3}$, $\\text{atol} = 10^{-6}$, $h_0 = 10^{-2}$, $h_{\\max} = 10^{-1}$.\n- Test case $3$: $A = 0.005$, $\\omega = 2000$, $T = 0.5$, $\\text{rtol} = 10^{-7}$, $\\text{atol} = 10^{-12}$, $h_0 = 5 \\cdot 10^{-4}$, $h_{\\max} = 2 \\cdot 10^{-2}$.\n- Test case $4$: $A = 0.01$, $\\omega = 1000$, $T = 0.1$, $\\text{rtol} = 10^{-10}$, $\\text{atol} = 10^{-12}$, $h_0 = 10^{-4}$, $h_{\\max} = 10^{-3}$.\n- Test case $5$: $A = 10^{-8}$, $\\omega = 1000$, $T = 1$, $\\text{rtol} = 10^{-6}$, $\\text{atol} = 10^{-12}$, $h_0 = 10^{-2}$, $h_{\\max} = 5 \\cdot 10^{-2}$.\n\nFor each test case, output a single floating-point number equal to the absolute error at $t = T$, that is,\n$$\n\\left| y_{\\text{numerical}}(T) - y_{\\text{analytical}}(T) \\right|.\n$$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases described above (for example, $[r_1,r_2,r_3,r_4,r_5]$).",
            "solution": "The problem is evaluated as valid. It presents a well-posed initial value problem for a first-order linear ordinary differential equation (ODE) and requests a solution using a standard, well-established numerical method from scientific computing. The problem is self-contained, scientifically grounded, and objective. It specifies all necessary parameters for the ODE and the solver, with one minor omission: the value for the minimum step-size, $h_{\\min}$. A reasonable default value will be assumed for the implementation. The problem also refers to \"a classical embedded pair\" of Runge-Kutta coefficients without specifying which one. For this solution, the widely used and highly regarded Dormand-Prince 5(4) pair, denoted as DOPRI5(4), will be employed.\n\n### Analytical Solution\n\nThe problem asks for the analytical solution to the initial value problem (IVP):\n$$\n\\frac{dy}{dt} = -y + A \\sin(\\omega t), \\quad y(0) = 0\n$$\nThis is a first-order linear ODE. We can rewrite it in the standard form $\\frac{dy}{dt} + P(t)y = Q(t)$:\n$$\n\\frac{dy}{dt} + y = A \\sin(\\omega t)\n$$\nHere, $P(t) = 1$ and $Q(t) = A \\sin(\\omega t)$. The solution can be found using the integrating factor method. The integrating factor, $I(t)$, is given by:\n$$\nI(t) = e^{\\int P(t) dt} = e^{\\int 1 dt} = e^t\n$$\nMultiplying the standard form of the ODE by the integrating factor $I(t)$ yields:\n$$\ne^t \\frac{dy}{dt} + e^t y = A e^t \\sin(\\omega t)\n$$\nThe left side of this equation is, by the product rule for differentiation, the derivative of the product $y(t)e^t$:\n$$\n\\frac{d}{dt} \\left( y(t) e^t \\right) = A e^t \\sin(\\omega t)\n$$\nIntegrating both sides with respect to $t$:\n$$\ny(t) e^t = \\int A e^t \\sin(\\omega t) dt + C\n$$\nwhere $C$ is the constant of integration. The integral $\\int e^t \\sin(\\omega t) dt$ can be solved using integration by parts twice. The result of this standard integral is:\n$$\n\\int e^t \\sin(\\omega t) dt = \\frac{e^t}{1 + \\omega^2} (\\sin(\\omega t) - \\omega \\cos(\\omega t))\n$$\nSubstituting this back into the equation for $y(t)e^t$:\n$$\ny(t) e^t = A \\frac{e^t}{1 + \\omega^2} (\\sin(\\omega t) - \\omega \\cos(\\omega t)) + C\n$$\nTo find the general solution for $y(t)$, we divide by $e^t$:\n$$\ny(t) = \\frac{A}{1 + \\omega^2} (\\sin(\\omega t) - \\omega \\cos(\\omega t)) + C e^{-t}\n$$\nNow, we apply the initial condition $y(0) = 0$ to determine the constant $C$:\n$$\ny(0) = 0 = \\frac{A}{1 + \\omega^2} (\\sin(0) - \\omega \\cos(0)) + C e^{-0}\n$$\n$$\n0 = \\frac{A}{1 + \\omega^2} (0 - \\omega \\cdot 1) + C \\cdot 1\n$$\n$$\n0 = -\\frac{A\\omega}{1 + \\omega^2} + C \\implies C = \\frac{A\\omega}{1 + \\omega^2}\n$$\nSubstituting the value of $C$ back into the general solution gives the unique analytical solution to the IVP:\n$$\ny(t) = \\frac{A}{1 + \\omega^2} (\\sin(\\omega t) - \\omega \\cos(\\omega t)) + \\frac{A\\omega}{1 + \\omega^2} e^{-t}\n$$\nThis can be written more compactly as:\n$$\ny(t) = \\frac{A}{1 + \\omega^2} \\left[ \\sin(\\omega t) - \\omega \\cos(\\omega t) + \\omega e^{-t} \\right]\n$$\nThis formula will be used to compute the exact solution at the terminal time $t=T$ for each test case to evaluate the accuracy of the numerical solver.\n\n### Adaptive Step-Size Runge-Kutta Method\n\nThe numerical solution will be obtained using an adaptive explicit Runge-Kutta (RK) method. The core idea is to use an embedded RK pair, which provides two solutions of different orders, a higher-order solution $y_{n+1}$ and a lower-order solution $\\hat{y}_{n+1}$. The difference between these two solutions provides an estimate of the local truncation error, which is then used to adjust the step-size $h$.\n\nWe select the Dormand-Prince 5(4) pair, a widely used method known for its efficiency. It computes a $5^{th}$ order accurate solution using a $4^{th}$ order error estimate. It requires $7$ stage evaluations per step.\n\nThe Butcher tableau for the DOPRI5(4) method is given by coefficients $c_i$, $a_{ij}$, $b_i$ (for the $5^{th}$ order solution), and $\\hat{b}_i$ (for the $4^{th}$ order solution).\n\n- The nodes $c_i$:\n$c = [0, 1/5, 3/10, 4/5, 8/9, 1, 1]^T$\n- The main coefficients $a_{ij}$:\n$a_{21} = 1/5$\n$a_{31} = 3/40, a_{32} = 9/40$\n$a_{41} = 44/45, a_{42} = -56/15, a_{43} = 32/9$\n$a_{51} = 19372/6561, a_{52} = -25360/2187, a_{53} = 64448/6561, a_{54} = -212/729$\n$a_{61} = 9017/3168, a_{62} = -355/33, a_{63} = 46732/5247, a_{64} = 49/176, a_{65} = -5103/18656$\n$a_{71} = 35/384, a_{72} = 0, a_{73} = 500/1113, a_{74} = 125/192, a_{75} = -2187/6784, a_{76} = 11/84$\n- The weights for the $5^{th}$ order solution, $b_i$:\n$b = [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]^T$\n- The weights for the $4^{th}$ order embedded solution, $\\hat{b}_i$:\n$\\hat{b} = [5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40]^T$\n\nThe algorithm proceeds as follows for each step from time $t_n$ with solution $y_n$ and step-size $h$:\n1.  **Compute Stages**: For the ODE $\\frac{dy}{dt} = f(t, y)$, compute the stage derivatives $k_i$:\n    $$ k_1 = f(t_n, y_n) $$\n    $$ k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right), \\quad \\text{for } i = 2, \\dots, 7 $$\n\n2.  **Compute Solutions**: Compute the $5^{th}$ order solution $y_{n+1}$ and the $4^{th}$ order solution $\\hat{y}_{n+1}$:\n    $$ y_{n+1} = y_n + h \\sum_{i=1}^7 b_i k_i $$\n    $$ \\hat{y}_{n+1} = y_n + h \\sum_{i=1}^7 \\hat{b}_i k_i $$\n\n3.  **Estimate Error**: The local error estimate $E$ is the difference between the two solutions:\n    $$ E = |y_{n+1} - \\hat{y}_{n+1}| = \\left| h \\sum_{i=1}^7 (b_i - \\hat{b}_i) k_i \\right| $$\n\n4.  **Calculate Tolerance**: The problem specifies a combined absolute-relative tolerance criterion. The tolerance for the current step, $\\text{tol}$, is calculated as:\n    $$ \\text{tol} = \\text{atol} + \\text{rtol} \\cdot |y_{n+1}| $$\n    where $\\text{atol}$ is the absolute tolerance and $\\text{rtol}$ is the relative tolerance. It is common to use $\\max(|y_{n}|, |y_{n+1}|)$ for stability, but we adhere to the specific formula provided.\n\n5.  **Step-size Control**: The decision to accept or reject the step is based on comparing the error estimate $E$ with the tolerance $\\text{tol}$.\n    - If $E \\le \\text{tol}$, the step is **accepted**:\n        - The solution is advanced: $t_{n+1} = t_n + h$ and the new solution is $y_{n+1}$.\n        - A new, potentially larger, step-size $h_{\\text{new}}$ is calculated for the next step.\n    - If $E > \\text{tol}$, the step is **rejected**:\n        - The solution is not advanced: $t_{n+1}$ remains $t_n$ and the solution remains $y_n$.\n        - A new, smaller, step-size $h_{\\text{new}}$ is calculated, and the step is re-attempted.\n\n6.  **Step-size Update Formula**: The optimal step-size is estimated with the goal of making the error on the next step equal to the tolerance. A common formula, which will be implemented, is:\n    $$ h_{\\text{new}} = h \\cdot \\text{fac} \\cdot \\left(\\frac{\\text{tol}}{E}\\right)^{1/(p+1)} $$\n    where $p=4$ is the order of the error estimator, and $\\text{fac}$ is a safety factor (e.g., $0.9$) to ensure stability. The new step-size $h_{\\text{new}}$ is clipped to fall within predefined bounds: $h_{\\text{min}} \\le h_{\\text{new}} \\le h_{\\text{max}}$. For this problem, we choose $h_{\\text{min}} = 10^{-16}$. Growth and shrinkage of the step-size are also typically limited by factors (e.g., max increase factor of $5$, max decrease factor of $0.2$). The loop continues until the integration time $t$ reaches the terminal time $T$. The final step is adjusted to land exactly on $T$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Butcher tableau for the Dormand-Prince 5(4) method.\nC = np.array([0, 1/5, 3/10, 4/5, 8/9, 1, 1], dtype=np.float64)\nA = np.array([\n    [0, 0, 0, 0, 0, 0, 0],\n    [1/5, 0, 0, 0, 0, 0, 0],\n    [3/40, 9/40, 0, 0, 0, 0, 0],\n    [44/45, -56/15, 32/9, 0, 0, 0, 0],\n    [19372/6561, -25360/2187, 64448/6561, -212/729, 0, 0, 0],\n    [9017/3168, -355/33, 46732/5247, 49/176, -5103/18656, 0, 0],\n    [35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0]\n], dtype=np.float64)\n# 5th-order solution weights\nB = np.array([35/384, 0, 500/1113, 125/192, -2187/6784, 11/84, 0], dtype=np.float64)\n# 4th-order embedded solution weights\nB_HAT = np.array([5179/57600, 0, 7571/16695, 393/640, -92097/339200, 187/2100, 1/40], dtype=np.float64)\n# Error estimation weights (B - B_HAT)\nE = B - B_HAT\n# Order of the error estimator\nP = 4\n\ndef adaptive_rk_solver(f, y0, t_span, h0, h_max, rtol, atol):\n    \"\"\"\n    Adaptive step-size explicit Runge-Kutta solver using Dormand-Prince 5(4).\n    \"\"\"\n    t0, T = t_span\n    t = t0\n    y = y0\n    h = h0\n\n    # Minimum step size.\n    h_min = 1e-16\n\n    # Safety factors and step size adjustment limits\n    safety_fac = 0.9\n    fac_min = 0.2\n    fac_max = 5.0\n\n    while t < T:\n        if t + h > T:\n            h = T - t\n        \n        # Guard against step size underflow\n        if h < h_min:\n            # This case suggests the problem is too stiff or tolerances are too tight\n            # for the given h_max. We break to avoid an infinite loop.\n            break\n\n        # Compute stages k_i\n        k = np.zeros(7, dtype=np.float64)\n        k[0] = f(t, y)\n        for i in range(1, 7):\n            y_stage = y + h * np.dot(A[i, :i], k[:i])\n            k[i] = f(t + C[i] * h, y_stage)\n\n        # Compute higher-order solution (y_new) and error estimate\n        y_new = y + h * np.dot(B, k)\n        error_estimate = np.abs(h * np.dot(E, k))\n        \n        # Calculate tolerance for the current step\n        # Using max(abs(y), abs(y_new)) is more standard, but we follow the problem spec.\n        tol = atol + rtol * np.abs(y_new)\n\n        if error_estimate <= tol:\n            # Step is accepted\n            t += h\n            y = y_new\n\n            # Calculate new step size\n            if error_estimate == 0.0:\n                h_factor = fac_max\n            else:\n                h_factor = min(fac_max, safety_fac * (tol / error_estimate)**(1 / (P + 1)))\n            \n            h = min(h_max, h * h_factor)\n        else:\n            # Step is rejected\n            # Calculate new step size\n            h_factor = max(fac_min, safety_fac * (tol / error_estimate)**(1 / (P + 1)))\n            h = max(h_min, h * h_factor)\n\n    return y\n\ndef analytical_solution(t, A, omega):\n    \"\"\"\n    Computes the analytical solution of the ODE at time t.\n    y(t) = (A / (1 + omega^2)) * (sin(omega*t) - omega*cos(omega*t) + omega*exp(-t))\n    \"\"\"\n    factor = A / (1 + omega**2)\n    term1 = np.sin(omega * t)\n    term2 = -omega * np.cos(omega * t)\n    term3 = omega * np.exp(-t)\n    return factor * (term1 + term2 + term3)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute absolute errors.\n    \"\"\"\n    test_cases = [\n        # A, omega, T, rtol, atol, h0, h_max\n        (0.01, 1000, 1, 1e-6, 1e-9, 1e-3, 5e-2),\n        (0.01, 1000, 1, 1e-3, 1e-6, 1e-2, 1e-1),\n        (0.005, 2000, 0.5, 1e-7, 1e-12, 5e-4, 2e-2),\n        (0.01, 1000, 0.1, 1e-10, 1e-12, 1e-4, 1e-3),\n        (1e-8, 1000, 1, 1e-6, 1e-12, 1e-2, 5e-2),\n    ]\n\n    results = []\n    for A, omega, T, rtol, atol, h0, h_max in test_cases:\n        \n        # Define the RHS of the ODE dy/dt = f(t, y)\n        def ode_rhs(t, y):\n            return -y + A * np.sin(omega * t)\n\n        # Initial conditions\n        y0 = 0.0\n        t_span = (0.0, T)\n\n        # Compute numerical solution\n        y_numerical = adaptive_rk_solver(ode_rhs, y0, t_span, h0, h_max, rtol, atol)\n\n        # Compute analytical solution\n        y_analytical = analytical_solution(T, A, omega)\n        \n        # Calculate absolute error\n        abs_error = np.abs(y_numerical - y_analytical)\n        results.append(abs_error)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "This final practice elevates your understanding from implementing solvers to strategically choosing the right tool for the job. You will construct a quantitative cost model to compare the performance of a low-order method against a high-order one, considering the trade-offs between work-per-step and the total number of steps required. By simulating the solver's behavior on both non-stiff and stiff problems , you will gain crucial insights into why there is no single \"best\" method and how problem characteristics dictate the most efficient numerical approach.",
            "id": "3095889",
            "problem": "You must write a complete and runnable program that, from first principles of adaptive step-size control for ordinary differential equations (ODEs), constructs a simple, quantitative cost model to choose between two explicit embedded Runge-Kutta (RK) methods under a fixed absolute local error tolerance. The model must balance the local work per step against the total number of steps by explicitly simulating an adaptive controller and counting the total number of right-hand-side function evaluations required to integrate a scalar linear ODE over a fixed time interval. The two methods must be evaluated on both a stiff linear test equation and a nonstiff linear test equation.\n\nFundamental base. Use the following foundations only: the definition of an ordinary differential equation (ODE) $y'(t)=f(t,y(t))$, the concept of local truncation error per step for a method of order $p$ behaving like $O(h^{p+1})$, and the notion of absolute stability for explicit RK methods on the linear test equation $y'=-\\lambda y$. For the adaptive controller, you must derive the step-size update from the asymptotic scaling of the local error. For stiffness, you must incorporate an absolute stability constraint that enforces $h \\le r/\\lambda$, where $r$ is the approximate length of the stability interval along the negative real axis for the given method.\n\nTarget ODEs. Consider the scalar linear ODEs $y'=-\\lambda y$ with $y(0)=1$ on a finite horizon $t \\in [0,T]$:\n- Nonstiff: $\\lambda=1$.\n- Stiff: $\\lambda=1000$.\n\nEmbedded methods to compare. Implement two explicit embedded RK pairs with absolute local error control based on the difference between the higher- and lower-order solutions in the pair.\n- Method A (low order): an embedded pair of orders $\\{2,1\\}$ based on the second-order Heun method (explicit trapezoidal) with the first-order forward Euler as the embedded estimator. Count $2$ right-hand-side evaluations per attempted step. Use an approximate stability radius $r_A=2.0$ on the negative real axis for the linear test equation.\n- Method B (high order): the Cash-Karp embedded pair $\\{5,4\\}$ with $6$ stages. Count $6$ right-hand-side evaluations per attempted step. Use an approximate stability radius $r_B=2.8$ on the negative real axis for the linear test equation.\n\nAdaptive controller specification. For each attempted step of size $h$, compute the absolute local error estimate $e=\\lvert y_{\\text{high}}-y_{\\text{low}}\\rvert$. Accept the step if $e \\le \\text{tol}$ and reject otherwise. After each attempt, propose a new step size via\n$$\nh_{\\text{new}} = h \\cdot \\mathrm{clip}\\Big(s \\cdot \\big(\\tfrac{\\text{tol}}{\\max(e,\\varepsilon)}\\big)^{1/p},\\, f_{\\min},\\, f_{\\max}\\Big),\n$$\nwhere $p$ is the order of the higher-order member of the embedded pair ($p=2$ for Method A and $p=5$ for Method B), $s$ is a safety factor, $\\varepsilon$ is a very small positive number to avoid division by zero, and $\\mathrm{clip}(x,f_{\\min},f_{\\max})=\\min(\\max(x,f_{\\min}),f_{\\max})$. You must enforce the absolute stability cap $h \\le 0.95\\, r/\\lambda$ for every attempted step, and the last step must not overshoot $T$. Use the fixed controller constants $s=0.9$, $f_{\\min}=0.2$, $f_{\\max}=5.0$, and $\\varepsilon=10^{-30}$. Initialize with $t=0$, $y=1$, and a positive initial step size that respects both the stability cap and the interval length; one reasonable choice is $h_0=\\min\\big(T,\\,0.1\\,\\text{tol}^{1/p},\\,0.8\\,r/\\lambda\\big)$ for the corresponding method.\n\nCost model and decision rule. For each run, the cost is the total number of right-hand-side evaluations, that is, the number of attempted steps (accepted or rejected) multiplied by the per-step evaluations ($2$ for Method A and $6$ for Method B). For each test case, choose the method with the strictly smaller total cost. If the costs are exactly equal, choose Method A.\n\nAngle units and physical units. No physical units or angle units are involved. All quantities are dimensionless real numbers.\n\nTest suite. Your program must evaluate the following four parameter sets:\n- Case $1$: $\\lambda=1$, $T=5$, $\\text{tol}=10^{-6}$ (nonstiff, tight tolerance).\n- Case $2$: $\\lambda=1$, $T=5$, $\\text{tol}=10^{-1}$ (nonstiff, loose tolerance).\n- Case $3$: $\\lambda=1000$, $T=1$, $\\text{tol}=10^{-6}$ (stiff, tight tolerance).\n- Case $4$: $\\lambda=1000$, $T=1$, $\\text{tol}=10^{-2}$ (stiff, moderate tolerance).\n\nRequired final output format. Your program should produce a single line of output containing the decisions for the four cases, in order, as a comma-separated list enclosed in square brackets, where each entry is an integer method identifier: $0$ for Method A (low order) or $1$ for Method B (high order). For example, a valid output looks like $[1,0,0,0]$.",
            "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in the principles of numerical analysis for ordinary differential equations (ODEs), well-posed with a clear and deterministic algorithm, and self-contained with all necessary parameters and definitions provided. We may therefore proceed with a solution.\n\n### 1. Theoretical Foundation\n\n**ODE and Test Equation:**\nThe problem considers the scalar linear test equation, a fundamental model for analyzing the stability and accuracy of numerical ODE solvers:\n$$\ny'(t) = -\\lambda y(t), \\quad y(0) = 1, \\quad t \\in [0, T]\n$$\nThe exact solution is $y(t) = e^{-\\lambda t}$. The parameter $\\lambda > 0$ determines the stiffness of the equation; a large $\\lambda$ corresponds to a stiff problem where the solution decays rapidly.\n\n**Embedded Runge-Kutta Methods:**\nAn embedded RK pair provides two solutions at each step, one of a higher order $p$ ($y_{\\text{high}}$) and one of a lower order $q$ ($y_{\\text{low}}$). The difference between these solutions provides an estimate of the local truncation error (LTE) of the lower-order method:\n$$\ne = |y_{\\text{high}} - y_{\\text{low}}|\n$$\nThis error estimate is used to control the step size $h$. The two methods under consideration are:\n- **Method A:** A $\\{2,1\\}$ pair using the second-order Heun method ($p=2$) and the first-order forward Euler method ($q=1$). It requires $2$ evaluations of the right-hand-side function $f(t,y)$ per step. Its stability radius on the negative real axis is given as $r_A=2.0$.\n- **Method B:** The Cash-Karp $\\{5,4\\}$ pair ($p=5$, $q=4$). It requires $6$ function evaluations per step. Its stability radius is given as $r_B=2.8$.\n\n**Error Estimation from First Principles:**\nFor an RK method of order $q$ applied to $y'=-\\lambda y$, the LTE after one step from a point $y_n$ on the exact solution is approximately the first non-vanishing term in the Taylor series difference. The error estimate $e$ from an embedded pair is designed to approximate this LTE.\n$$\ne \\approx |\\text{LTE}_{q}| \\approx \\left| \\frac{d^{q+1}y}{dt^{q+1}} \\frac{h^{q+1}}{(q+1)!} \\right|\n$$\nFor $y(t)=e^{-\\lambda t}$, the $(q+1)$-th derivative is $y^{(q+1)}(t) = (-\\lambda)^{q+1} e^{-\\lambda t} = (-\\lambda)^{q+1} y(t)$. Thus, the error estimate can be expressed as:\n$$\ne \\approx \\frac{(h\\lambda)^{q+1}}{(q+1)!} |y(t)|\n$$\nFor Method A, with $q=1$, the error estimate is $e \\approx \\frac{1}{2}(h\\lambda)^2 |y|$. For Method B, with $q=4$, it is $e \\approx \\frac{1}{120}(h\\lambda)^5 |y|$.\n\n**State Propagation:**\nWhen a step is accepted, the numerical solution is advanced using the higher-order approximation. For any explicit RK method applied to $y'=-\\lambda y$, the update has the form $y_{n+1} = R(-h\\lambda) y_n$, where $R(z)$ is the method's stability function. For a method of order $p$, $R(z)$ is a polynomial that matches the Taylor series of $e^z$ up to the term $z^p$:\n$$\nR_p(z) = \\sum_{k=0}^{p} \\frac{z^k}{k!}\n$$\n- For Method A ($p=2$): $y_{n+1} = y_n \\left(1 - h\\lambda + \\frac{1}{2}(h\\lambda)^2\\right)$.\n- For Method B ($p=5$): $y_{n+1} = y_n \\sum_{k=0}^{5} \\frac{(-h\\lambda)^k}{k!}$.\n\n### 2. Adaptive Control Algorithm\n\nThe simulation of the adaptive integrator proceeds iteratively from $t=0$ to $T$.\n\n**Initialization:**\n- The simulation starts at $t=0$ with $y=1$.\n- The initial step size $h_0$ is chosen conservatively to respect the integration interval $T$, the tolerance $\\text{tol}$, and the stability limit of the method:\n$$\nh_0 = \\min\\left(T, 0.1 \\cdot \\text{tol}^{1/p}, 0.8 \\cdot \\frac{r}{\\lambda}\\right)\n$$\nwhere $p$ is the order of the higher-order method and $r$ is its stability radius.\n\n**Main Loop:**\nAt each iteration, with a proposed step size $h$, the following steps are performed:\n1.  **Step-size Constraints:** The actual attempted step, $h_{\\text{current}}$, is capped by the stability requirement and the remaining interval length:\n    $$\n    h_{\\text{current}} = \\min\\left(h, 0.95 \\frac{r}{\\lambda}, T-t\\right)\n    $$\n2.  **Attempt Step and Estimate Error:** The total count of function evaluations is incremented. The local error estimate $e$ is calculated using the formula derived above.\n3.  **Accept/Reject:**\n    - If $e \\le \\text{tol}$, the step is accepted. The time $t$ and solution $y$ are advanced using $h_{\\text{current}}$ and the higher-order update rule.\n    - If $e > \\text{tol}$, the step is rejected. The state $(t, y)$ remains unchanged.\n4.  **Propose Next Step:** A new step size for the next attempt is calculated using the specified PI controller formula:\n    $$\n    h_{\\text{new}} = h_{\\text{current}} \\cdot \\mathrm{clip}\\left(s \\cdot \\left(\\frac{\\text{tol}}{\\max(e, \\varepsilon)}\\right)^{1/p}, f_{\\min}, f_{\\max}\\right)\n    $$\n    The parameters are given as safety factor $s=0.9$, clipping factors $f_{\\min}=0.2$ and $f_{\\max}=5.0$, and a small regularizer $\\varepsilon=10^{-30}$. The order $p$ is that of the higher-order method in the pair.\n\n### 3. Cost Model and Decision\n\nThe total cost for a given method and test case is the total number of function evaluations accumulated throughout the simulation. The decision rule is to choose the method with the strictly lower cost. If costs are identical, Method A is chosen. This procedure is repeated for all four test cases specified in the problem statement.",
            "answer": "```python\nimport math\nimport numpy as np\n\ndef run_simulation(lambda_val, T, tol, p, q, evals_per_step, r):\n    \"\"\"\n    Simulates an adaptive step-size ODE solver to calculate the total cost.\n\n    Args:\n        lambda_val (float): The lambda parameter of the ODE y' = -lambda*y.\n        T (float): The final integration time.\n        tol (float): The absolute local error tolerance.\n        p (int): The order of the higher-order method in the embedded pair.\n        q (int): The order of the lower-order method in the embedded pair.\n        evals_per_step (int): The number of RHS evaluations per step.\n        r (float): The stability radius of the higher-order method.\n\n    Returns:\n        int: The total number of RHS evaluations (cost).\n    \"\"\"\n    # Controller constants\n    s = 0.9\n    f_min = 0.2\n    f_max = 5.0\n    epsilon = 1.0e-30\n\n    # Initial conditions\n    t = 0.0\n    y = 1.0\n    total_evals = 0\n\n    # Stability cap\n    h_stab_max = (0.95 * r / lambda_val) if lambda_val > 0 else float('inf')\n\n    # Initial step size h\n    h0_acc = 0.1 * (tol**(1.0/p))\n    h0_stab = (0.8 * r / lambda_val) if lambda_val > 0 else float('inf')\n    h = min(T, h0_acc, h0_stab)\n    \n    # Pre-calculate factorials for the stability function\n    factorials = [math.factorial(i) for i in range(p + 1)]\n    \n    q_factorial = math.factorial(q + 1)\n\n    while t < T:\n        # Guard against floating-point issues near the end of the interval\n        if T - t < 1e-14 * T:\n            break\n\n        # Apply stability and interval-end constraints to the step size\n        h_current = min(h, h_stab_max, T - t)\n        \n        # An attempt costs function evaluations regardless of acceptance\n        total_evals += evals_per_step\n\n        # Calculate error estimate based on first principles for y'=-lambda*y\n        # e ~ |y| * |(-h*lambda)^(q+1) / (q+1)!|\n        e = abs(y) * (h_current * lambda_val)**(q + 1) / q_factorial\n\n        # Accept or reject the step\n        if e <= tol:\n            # Step accepted: update time and solution\n            t += h_current\n            # Update y using the stability function R_p(z) of the higher-order method\n            z = -h_current * lambda_val\n            R_p_z = sum((z**k) / factorials[k] for k in range(p + 1))\n            y *= R_p_z\n        # If rejected, t and y do not change.\n\n        # Propose the next step size based on the current attempt\n        # Use a small number epsilon to avoid division by zero\n        ratio = tol / max(e, epsilon)\n        scale_factor = s * (ratio**(1.0/p))\n        \n        # Clip the scaling factor to prevent overly aggressive changes\n        clipped_scale_factor = min(max(scale_factor, f_min), f_max)\n        \n        h = h_current * clipped_scale_factor\n        \n    return total_evals\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and determine the optimal method for each case.\n    \"\"\"\n    test_cases = [\n        # (lambda, T, tol)\n        (1.0, 5.0, 1.0e-6),      # Case 1: Nonstiff, tight tolerance\n        (1.0, 5.0, 1.0e-1),      # Case 2: Nonstiff, loose tolerance\n        (1000.0, 1.0, 1.0e-6),   # Case 3: Stiff, tight tolerance\n        (1000.0, 1.0, 1.0e-2),   # Case 4: Stiff, moderate tolerance\n    ]\n\n    # Method A: Heun(2)/Euler(1) embedded pair\n    method_A_params = {'p': 2, 'q': 1, 'evals_per_step': 2, 'r': 2.0}\n    # Method B: Cash-Karp 5(4) embedded pair\n    method_B_params = {'p': 5, 'q': 4, 'evals_per_step': 6, 'r': 2.8}\n\n    results = []\n    for lambda_val, T, tol in test_cases:\n        cost_A = run_simulation(lambda_val, T, tol, **method_A_params)\n        cost_B = run_simulation(lambda_val, T, tol, **method_B_params)\n\n        # Decision rule: Choose method with strictly smaller cost.\n        # If costs are equal, choose Method A.\n        # 0 for Method A, 1 for Method B.\n        if cost_B < cost_A:\n            decision = 1\n        else:\n            decision = 0\n        results.append(decision)\n    \n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}