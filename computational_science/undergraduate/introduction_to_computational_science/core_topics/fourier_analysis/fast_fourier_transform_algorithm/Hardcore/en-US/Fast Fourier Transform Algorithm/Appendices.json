{
    "hands_on_practices": [
        {
            "introduction": "The Fast Fourier Transform's remarkable speed is achieved through a \"divide and conquer\" strategy, which first requires a specific reordering of the input data. This initial step, known as bit-reversal, can seem abstract but is a simple and elegant procedure. This practice provides a concrete exercise to master this fundamental mechanism, which is the gateway to understanding the data flow in the popular Decimation-In-Time FFT algorithm .",
            "id": "1717784",
            "problem": "An embedded systems engineer is tasked with implementing a real-time audio spectrum analyzer on a microcontroller with limited memory. To achieve this, the engineer decides to use the radix-2 Decimation-In-Time (DIT) Fast Fourier Transform (FFT) algorithm. The first crucial step of this algorithm involves re-ordering the input time-domain samples before the main butterfly computations begin. This re-ordering is accomplished by placing the sample from index $n$ of the original sequence, $x[n]$, into a new position, index $k$, in the scrambled sequence. The new index $k$ is found by reversing the binary representation of the original index $n$.\n\nConsider a discrete-time signal that has been sampled to produce an $N=8$ point sequence, denoted as $x[n]$ for $n=0, 1, 2, ..., 7$. According to the DIT-FFT's bit-reversal procedure, the sample $x[6]$ will be moved to a new index in the re-ordered input buffer. What is the numerical value of this new index?",
            "solution": "In a radix-2 DIT-FFT with sequence length $N=2^{m}$, the bit-reversal re-ordering maps an index $n$ to $k$ by reversing the $m$-bit binary representation of $n$. Formally, if $n=\\sum_{i=0}^{m-1} b_{i} 2^{i}$ with $b_{i}\\in\\{0,1\\}$, then the bit-reversed index is\n$$\nk=\\sum_{i=0}^{m-1} b_{i} 2^{m-1-i}.\n$$\nFor $N=8$, we have $m=\\log_{2}(8)=3$. The index $n=6$ has the 3-bit binary representation\n$$\n6=1\\cdot 2^{2}+1\\cdot 2^{1}+0\\cdot 2^{0}\\;\\Rightarrow\\;(110)_{2},\n$$\nso $b_{2}=1$, $b_{1}=1$, $b_{0}=0$. Reversing these three bits gives $(011)_{2}$, which in decimal is\n$$\nk=0\\cdot 2^{2}+1\\cdot 2^{1}+1\\cdot 2^{0}=3.\n$$\nTherefore, under the DIT-FFT bit-reversal re-ordering for $N=8$, the sample $x[6]$ is moved to index $k=3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Why do we use the FFT instead of the more straightforward Discrete Fourier Transform (DFT)? The answer lies in its astounding computational efficiency. This exercise moves beyond the high-level discussion of $O(N^2)$ versus $O(N \\log N)$ complexity to provide a tangible, quantitative comparison, allowing you to calculate the exact savings in floating-point operations for a moderately sized dataset . Understanding this difference highlights why the FFT is a cornerstone of modern digital signal processing.",
            "id": "3282537",
            "problem": "A software engineer must decide whether to implement the Discrete Fourier Transform (DFT) directly or via the radix-2 Cooley–Tukey Fast Fourier Transform (FFT) for a dataset of size $N=1024$. The engineer measures cost solely by the number of floating-point multiplications, defined as follows:\n- A complex multiplication is implemented naïvely as $4$ real multiplications.\n- Precomputation time for twiddle factors is ignored.\n- No algebraic simplifications of special twiddle values (such as $1$, $-1$, $\\mathrm{i}$, $-\\mathrm{i}$) are applied; each appearance of a twiddle factor is treated as a general complex multiplication and counted accordingly.\n\nUsing only the definition of the DFT and the radix-2 decimation principle underlying the Cooley–Tukey FFT, determine the exact difference in the total number of real floating-point multiplications between the direct DFT computation of all $N$ outputs and the radix-2 Cooley–Tukey FFT computation of all $N$ outputs, for $N=1024$. Provide your final answer as an integer; no rounding is required.",
            "solution": "The objective is to compute the difference in the number of real floating-point multiplications between the direct DFT and the radix-2 FFT for a signal of length $N=1024$.\n\n**1. Cost of the Direct DFT**\n\nThe DFT requires the computation of $N$ output samples. For each output sample, the formula $\\sum_{n=0}^{N-1} x[n] W_{N}^{kn}$ involves $N$ complex multiplications (one for each term in the sum, as per the problem's no-simplification rule). The total number of complex multiplications for the direct DFT is thus $N \\times N = N^2$.\nGiven that one complex multiplication costs 4 real multiplications, the total number of real multiplications is:\n$$ \\text{Cost}_{\\text{real, DFT}} = 4N^2 $$\n\n**2. Cost of the Radix-2 Cooley–Tukey FFT**\n\nThe standard radix-2 FFT algorithm consists of $\\log_2(N)$ stages. At each stage, $N/2$ butterfly operations are performed. Each butterfly involves one complex multiplication. Therefore, the total number of complex multiplications for the FFT is $\\frac{N}{2} \\log_2(N)$.\nThe total number of real multiplications is 4 times this amount:\n$$ \\text{Cost}_{\\text{real, FFT}} = 4 \\times \\left(\\frac{N}{2} \\log_2(N)\\right) = 2N \\log_2(N) $$\n\n**3. Calculation of the Difference**\n\nThe difference $\\Delta M$ in the number of real multiplications is:\n$$ \\Delta M = \\text{Cost}_{\\text{real, DFT}} - \\text{Cost}_{\\text{real, FFT}} = 4N^2 - 2N \\log_2(N) $$\nSubstituting $N = 1024 = 2^{10}$, so $\\log_2(1024) = 10$:\n$$ \\Delta M = 4(1024)^2 - 2(1024)(10) $$\n$$ \\Delta M = 4(1,048,576) - 20,480 $$\n$$ \\Delta M = 4,194,304 - 20,480 $$\n$$ \\Delta M = 4,173,824 $$\nThe difference in the total number of real floating-point multiplications is $4,173,824$.",
            "answer": "$$\n\\boxed{4173824}\n$$"
        },
        {
            "introduction": "One of the most powerful applications of the FFT is its ability to compute the convolution of two signals with great speed. However, the convolution theorem for DFTs involves circular convolution, not the linear convolution typically needed in filtering applications. This hands-on coding exercise will guide you through demonstrating this crucial distinction and mastering the essential technique of zero-padding to correctly compute linear convolution using the FFT .",
            "id": "3282547",
            "problem": "You are asked to investigate how the periodicity assumption inherent in the Fast Fourier Transform (FFT) algorithm impacts convolution and how zero-padding can mitigate wrap-around artifacts. Work entirely in discrete time. Your program must implement the following, starting from core definitions.\n\nLet a finite-length, nonperiodic discrete-time signal be defined by $x[n] = n + 1$ for $n \\in \\{0,1,\\dots,N-1\\}$ and $x[n] = 0$ otherwise, with $N = 10$. Let a finite-length, nonperiodic averaging kernel be defined by $h[n] = \\frac{1}{M}$ for $n \\in \\{0,1,\\dots,M-1\\}$ and $h[n] = 0$ otherwise, with $M = 4$. Define the linear convolution $y_{\\mathrm{lin}}[n]$ by\n$$\ny_{\\mathrm{lin}}[n] = \\sum_{k=-\\infty}^{\\infty} x[k]\\,h[n-k],\n$$\nwhere $x[n]$ and $h[n]$ are taken to be zero outside their specified supports. This sum is finite and $y_{\\mathrm{lin}}[n]$ has length $N+M-1 = 13$.\n\nDefine the length-$L$ Discrete Fourier Transform (DFT) of a sequence $a[n]$ supported on $\\{0,1,\\dots,L-1\\}$ by\n$$\nA_L[m] = \\sum_{n=0}^{L-1} a[n]\\,e^{-2\\pi i \\frac{mn}{L}},\\quad m=0,1,\\dots,L-1,\n$$\nand its inverse by\n$$\na[n] = \\frac{1}{L}\\sum_{m=0}^{L-1} A_L[m]\\,e^{2\\pi i \\frac{mn}{L}}.\n$$\nFor a given length $L$, form zero-padded versions $x_L[n]$ and $h_L[n]$ by\n$$\nx_L[n] = \\begin{cases}\nx[n],& 0\\le n\\le N-1\\\\\n0,& \\text{otherwise}\n\\end{cases},\\quad\nh_L[n] = \\begin{cases}\nh[n],& 0\\le n\\le M-1\\\\\n0,& \\text{otherwise}\n\\end{cases},\n$$\nfor $n\\in\\{0,1,\\dots,L-1\\}$. Compute the length-$L$ circular convolution\n$$\ny_{\\mathrm{circ},L}[n] = \\sum_{k=0}^{L-1} x_L[k]\\,h_L[(n-k)\\bmod L],\\quad n=0,1,\\dots,L-1,\n$$\nvia pointwise multiplication in the DFT domain using the Fast Fourier Transform (FFT). It is a well-tested fact that the product of DFTs corresponds to circular convolution in the time domain.\n\nTo quantify wrap-around artifacts, define for each $L$ the error\n$$\nE(L) = \\max_{0\\le n \\le L-1} \\left| y_{\\mathrm{circ},L}[n] - y_{\\mathrm{lin},L}[n] \\right|,\n$$\nwhere $y_{\\mathrm{lin},L}[n]$ is the linear convolution $y_{\\mathrm{lin}}[n]$ either truncated to its first $L$ samples if $L \\le N+M-1$, or zero-padded with $L-(N+M-1)$ trailing zeros if $L \\ge N+M-1$. This common comparison length ensures a pointwise error over $L$ samples. The presence of wrap-around artifacts is indicated by $E(L)$ being significantly larger than numerical roundoff.\n\nYour tasks are:\n- Implement the above definitions and compute $E(L)$ for a test suite of padding lengths $L \\in \\{10,12,13,16,64\\}$.\n- Interpret the results to assess three padding strategies $L$: no padding $L=N$, minimal padding $L=N+M-1$, and a power-of-two padding $L=\\min\\{2^p: 2^p\\ge N+M-1\\}$, with an additional oversized padding $L=64$ to probe numerical stability.\n\nYour program should produce a single line of output containing the results as a comma-separated list of floating-point errors in the order of the test suite, enclosed in square brackets, for example, $[e_{10},e_{12},e_{13},e_{16},e_{64}]$ where each $e_L$ is the computed $E(L)$ for that $L$. No other output is permitted.\n\nEnsure that your implementation is self-contained, uses no user input, and adheres to the definitions above. Angles in complex exponentials are in radians by definition. There are no physical units involved. The target audience is advanced undergraduate students in numerical methods and scientific computing.",
            "solution": "The problem requires an investigation into the use of the Fast Fourier Transform (FFT) to compute the linear convolution of two discrete-time signals. The central principle being examined is the convolution theorem for the Discrete Fourier Transform (DFT), which states that the DFT of a circular convolution of two sequences is the pointwise product of their individual DFTs. Linear convolution can be correctly computed via this method only if the signals are zero-padded to a sufficient length to prevent time-domain aliasing, also known as wrap-around error. This exercise aims to demonstrate and quantify this effect.\n\nThe procedure is structured as follows:\n\nFirst, we define the input signals. The primary signal is a finite-length ramp sequence $x[n] = n + 1$ for $n \\in \\{0, 1, \\dots, N-1\\}$, with $N=10$, and $x[n]=0$ otherwise. The second signal is a finite-length averaging kernel $h[n] = \\frac{1}{M}$ for $n \\in \\{0, 1, \\dots, M-1\\}$, with $M=4$, and $h[n]=0$ otherwise.\n\nSecond, we compute the true linear convolution, denoted $y_{\\mathrm{lin}}[n]$, which serves as our ground truth. It is defined by the convolution sum:\n$$\ny_{\\mathrm{lin}}[n] = \\sum_{k=-\\infty}^{\\infty} x[k]\\,h[n-k]\n$$\nFor a signal of length $N$ and a kernel of length $M$, the resulting sequence $y_{\\mathrm{lin}}[n]$ has a length of $N+M-1$. In this specific case, the length is $10+4-1=13$. This computation is performed directly in the time domain.\n\nThird, for each specified transform length $L$ from the set $\\{10, 12, 13, 16, 64\\}$, we compute the length-$L$ circular convolution, $y_{\\mathrm{circ},L}[n]$. This is accomplished by leveraging the convolution theorem. The steps are:\n1.  Create length-$L$ versions of the input signals, $x_L[n]$ and $h_L[n]$, by zero-padding the original signals $x[n]$ and $h[n]$ to length $L$.\n2.  Compute the length-$L$ DFTs of the padded signals, $X_L[m] = \\text{DFT}\\{x_L[n]\\}$ and $H_L[m] = \\text{DFT}\\{h_L[n]\\}$, using the FFT algorithm for efficiency. The DFT is defined as $A_L[m] = \\sum_{n=0}^{L-1} a[n]\\,e^{-2\\pi i \\frac{mn}{L}}$.\n3.  Perform pointwise multiplication in the frequency domain: $Y_{\\mathrm{circ},L}[m] = X_L[m] \\cdot H_L[m]$.\n4.  Compute the inverse DFT of the product, $y_{\\mathrm{circ},L}[n] = \\text{IDFT}\\{Y_{\\mathrm{circ},L}[m]\\}$, to obtain the circular convolution result in the time domain. The inverse DFT is defined as $a[n] = \\frac{1}{L}\\sum_{m=0}^{L-1} A_L[m]\\,e^{2\\pi i \\frac{mn}{L}}$.\n\nFourth, we quantify the discrepancy between the circular and linear convolutions. A comparison signal, $y_{\\mathrm{lin},L}[n]$, is created from the ground-truth $y_{\\mathrm{lin}}[n]$ by either truncating it or padding it with zeros to match the length $L$. The error for a given length $L$ is then defined as the maximum absolute difference between the two results over all sample points:\n$$\nE(L) = \\max_{0\\le n \\le L-1} \\left| y_{\\mathrm{circ},L}[n] - y_{\\mathrm{lin},L}[n] \\right|\n$$\nA non-negligible value of $E(L)$ indicates the presence of wrap-around artifacts.\n\nThe theoretical basis for this analysis is that the circular convolution computed with length $L$ is equivalent to the linear convolution if and only if $L \\ge N+M-1$.\n- If $L < N+M-1$, the tail of the linear convolution result (which extends to index $N+M-2$) \"wraps around\" and adds to the initial samples of the circular convolution result, causing aliasing. Thus, for $L=10$ and $L=12$, we expect $E(L)$ to be significantly greater than zero.\n- If $L \\ge N+M-1$, there is sufficient padding to contain the entire linear convolution result without wrap-around. In this case, $y_{\\mathrm{circ},L}[n]$ will be identical to $y_{\\mathrm{lin},L}[n]$. Therefore, for $L=13$, $L=16$, and $L=64$, we expect the error $E(L)$ to be on the order of machine floating-point precision.\n\nThe test cases are chosen to illustrate key padding strategies: $L=10$ (insufficient padding), $L=13$ (minimal sufficient padding), and $L=16$ (power-of-two padding, often chosen for FFT efficiency). The $L=64$ case further confirms stability with oversized padding. The implementation calculates $E(L)$ for each of these cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes convolution error E(L) to demonstrate the effect of zero-padding.\n    \"\"\"\n    # Define signal and kernel parameters as per the problem statement.\n    N = 10\n    M = 4\n\n    # Define the discrete-time signal x[n] = n + 1.\n    x = np.arange(1, N + 1, dtype=float)\n\n    # Define the averaging kernel h[n] = 1/M.\n    h = np.ones(M, dtype=float) / M\n\n    # Compute the ground-truth linear convolution y_lin[n].\n    # The length of the result is N + M - 1 = 13.\n    L_lin = N + M - 1\n    y_lin = np.convolve(x, h)\n\n    # Define the test suite of padding lengths L.\n    test_Ls = [10, 12, 13, 16, 64]\n    \n    results = []\n\n    # Iterate through each padding length L to compute the error E(L).\n    for L in test_Ls:\n        # Step 1: Create the comparison linear convolution y_lin_L[n] of length L.\n        # This is done by truncating or padding y_lin to length L.\n        y_lin_L = np.zeros(L)\n        len_to_copy = min(L, L_lin)\n        y_lin_L[:len_to_copy] = y_lin[:len_to_copy]\n\n        # Step 2: Create zero-padded versions of x and h to length L.\n        x_L = np.zeros(L)\n        x_L[:N] = x\n        \n        h_L = np.zeros(L)\n        h_L[:M] = h\n\n        # Step 3: Compute circular convolution via FFT.\n        # This uses the convolution theorem: IDFT{DFT{x} * DFT{h}}.\n        X_L = np.fft.fft(x_L)\n        H_L = np.fft.fft(h_L)\n        Y_circ_L = X_L * H_L\n        y_circ_L = np.fft.ifft(Y_circ_L)\n\n        # Step 4: Calculate the error E(L) as the maximum absolute difference.\n        # np.abs handles the case where y_circ_L has a tiny imaginary part\n        # due to numerical precision.\n        error = np.max(np.abs(y_circ_L - y_lin_L))\n        results.append(error)\n\n    # Print the results in the specified single-line format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}