{
    "hands_on_practices": [
        {
            "introduction": "The theory of control variates is powerful, but its practical value comes from implementation. This first exercise provides a concrete scenario where we can apply the technique by leveraging the known expectation of a simple random variable, $U$, to reduce the variance when estimating the expectation of a more complex function, $(U+1)^2$. This practice demonstrates the core mechanic of identifying a correlated \"helper\" variable and calculating the optimal adjustment to minimize variance, allowing you to quantify the efficiency gain directly .",
            "id": "1348989",
            "problem": "An analyst is trying to estimate the value of $\\theta = E[(U+1)^2]$, where $U$ is a random variable uniformly distributed on the interval $[0, 1]$. The standard approach is to use a simple Monte Carlo estimator, where one generates samples of $X = (U+1)^2$ and averages them. The precision of this estimator is determined by the variance of a single sample, $\\text{Var}(X)$.\n\nTo improve the efficiency of the estimation, a control variate technique is proposed. A new estimator is constructed using a related random variable $C = U$, for which the expected value is known. The new estimator for a single sample is given by $X(b) = X - b(C - E[C])$, where $b$ is a real-valued constant coefficient to be optimized.\n\nYour task is to determine the theoretical improvement offered by this technique. First, find the optimal coefficient $b^*$ that minimizes the variance of the new estimator, $\\text{Var}(X(b))$. Then, calculate the variance reduction factor, which is the ratio of the variance of a single optimized control variate sample to the variance of a single simple sample.\n\nExpress this variance reduction factor, $\\frac{\\text{Var}(X(b^*))}{\\text{Var}(X)}$, as an exact fraction.",
            "solution": "Let $U \\sim \\text{Unif}[0,1]$, $X=(U+1)^{2}$, and $C=U$. For $k \\geq 0$, $E[U^{k}]=\\frac{1}{k+1}$. Hence $E[U]=\\frac{1}{2}$, $E[U^{2}]=\\frac{1}{3}$, $E[U^{3}]=\\frac{1}{4}$, $E[U^{4}]=\\frac{1}{5}$, and $\\text{Var}(U)=\\frac{1}{12}$.\n\nExpand $X$ as $X=U^{2}+2U+1$. Then\n$$\nE[X]=E[U^{2}]+2E[U]+1=\\frac{1}{3}+1+1=\\frac{7}{3}.\n$$\nAlso\n$$\nX^{2}=(U^{2}+2U+1)^{2}=U^{4}+4U^{3}+6U^{2}+4U+1,\n$$\nso\n$$\nE[X^{2}]=\\frac{1}{5}+4\\cdot\\frac{1}{4}+6\\cdot\\frac{1}{3}+4\\cdot\\frac{1}{2}+1=\\frac{31}{5}.\n$$\nTherefore\n$$\n\\text{Var}(X)=E[X^{2}]-(E[X])^{2}=\\frac{31}{5}-\\left(\\frac{7}{3}\\right)^{2}=\\frac{34}{45}.\n$$\n\nCompute the covariance with $C=U$:\n$$\n\\text{Cov}(U^{2},U)=E[U^{3}]-E[U^{2}]E[U]=\\frac{1}{4}-\\frac{1}{3}\\cdot\\frac{1}{2}=\\frac{1}{12},\n$$\nso\n$$\n\\text{Cov}(X,C)=\\text{Cov}(U^{2}+2U+1,U)=\\frac{1}{12}+2\\cdot\\text{Var}(U)=\\frac{1}{12}+2\\cdot\\frac{1}{12}=\\frac{1}{4},\n$$\nand $\\text{Var}(C)=\\text{Var}(U)=\\frac{1}{12}$.\n\nFor the control variate estimator $X(b)=X-b(C-E[C])$, the variance is\n$$\n\\text{Var}(X(b))=\\text{Var}(X)-2b\\,\\text{Cov}(X,C)+b^{2}\\text{Var}(C).\n$$\nMinimizing the quadratic in $b$ yields\n$$\nb^{*}=\\frac{\\text{Cov}(X,C)}{\\text{Var}(C)}=\\frac{\\frac{1}{4}}{\\frac{1}{12}}=3,\n$$\nand the minimized variance\n$$\n\\text{Var}(X(b^{*}))=\\text{Var}(X)-\\frac{\\text{Cov}(X,C)^{2}}{\\text{Var}(C)}=\\frac{34}{45}-\\frac{\\left(\\frac{1}{4}\\right)^{2}}{\\frac{1}{12}}=\\frac{34}{45}-\\frac{3}{4}=\\frac{1}{180}.\n$$\n\nHence the variance reduction factor is\n$$\n\\frac{\\text{Var}(X(b^{*}))}{\\text{Var}(X)}=\\frac{\\frac{1}{180}}{\\frac{34}{45}}=\\frac{1}{136}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{136}}$$"
        },
        {
            "introduction": "Importance sampling offers a profound shift in perspective: instead of sampling where the problem is naturally defined, we sample where it matters most for our estimate. This practice guides you through the process of finding an optimal proposal distribution to estimate $\\mathbb{E}[\\exp(X)]$ where $X$ is a normally distributed random variable. By carefully choosing a \"shifted\" version of the original distribution as our sampling density, we can dramatically reduce, and in this idealized case, even completely eliminate the estimator's variance, revealing the true power of this advanced technique .",
            "id": "2446710",
            "problem": "Consider the task of computing the expectation of the exponential of a normally distributed random variable in a Monte Carlo setting relevant to computational economics and finance. Let $X \\sim \\mathcal{N}(0, 100)$, and define $h(x) = \\exp(x)$. You will estimate the quantity $\\mathbb{E}[\\exp(X)]$ via importance sampling using a proposal distribution $Q$ given by $Y \\sim \\mathcal{N}(\\mu, 100)$ with unknown shift parameter $\\mu \\in \\mathbb{R}$. Let $p(x)$ and $q_{\\mu}(x)$ denote the probability density functions of $\\mathcal{N}(0, 100)$ and $\\mathcal{N}(\\mu, 100)$, respectively. The importance sampling estimator based on $n$ independent and identically distributed samples $\\{Y_i\\}_{i=1}^{n}$ from $q_{\\mu}$ is\n$$\nI_n(\\mu) \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} h(Y_i)\\, w_{\\mu}(Y_i), \\quad \\text{where } w_{\\mu}(y) \\;=\\; \\frac{p(y)}{q_{\\mu}(y)}.\n$$\nWorking from first principles and without invoking any specific variance reduction formulas, do the following:\n1. Derive a closed-form expression for $\\mathbb{E}[\\exp(X)]$.\n2. Derive $\\operatorname{Var}_{q_{\\mu}}(h(Y)\\,w_{\\mu}(Y))$ as a function of $\\mu$ and identify the value $\\mu^{\\star}$ that minimizes this variance over $\\mu \\in \\mathbb{R}$.\n\nProvide your final answer as a single ordered pair consisting of the exact value of $\\mathbb{E}[\\exp(X)]$ and the optimal shift $\\mu^{\\star}$. No numerical rounding is required, and your answer should be exact with no units.",
            "solution": "We begin by recalling the definitions. The target distribution is $X \\sim \\mathcal{N}(0, 100)$ with density\n$$\np(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma^{2} = 100,\n$$\nand the proposal distribution is $Y \\sim \\mathcal{N}(\\mu, 100)$ with density\n$$\nq_{\\mu}(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma^{2} = 100.\n$$\nThe importance weight is $w_{\\mu}(x) = p(x)/q_{\\mu}(x)$.\n\nStep 1: Compute $\\mathbb{E}[\\exp(X)]$ in closed form. By definition,\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\int_{-\\infty}^{\\infty} \\exp(x)\\, p(x)\\, dx \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(x - \\frac{x^{2}}{2\\sigma^{2}}\\right) dx.\n$$\nComplete the square in the exponent. Write\n$$\nx - \\frac{x^{2}}{2\\sigma^{2}} \\;=\\; -\\frac{1}{2\\sigma^{2}}\\left(x^{2} - 2\\sigma^{2} x \\right) \\;=\\; -\\frac{1}{2\\sigma^{2}}\\left[(x - \\sigma^{2})^{2} - \\sigma^{4}\\right] \\;=\\; -\\frac{(x - \\sigma^{2})^{2}}{2\\sigma^{2}} + \\frac{\\sigma^{2}}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\exp\\!\\left(\\frac{\\sigma^{2}}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x - \\sigma^{2})^{2}}{2\\sigma^{2}}\\right) dx \\;=\\; \\exp\\!\\left(\\frac{\\sigma^{2}}{2}\\right),\n$$\nsince the integral equals $\\sqrt{2\\pi}\\, \\sigma$. With $\\sigma^{2} = 100$, this yields\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\exp(50).\n$$\n\nStep 2: Derive $\\operatorname{Var}_{q_{\\mu}}(h(Y)\\, w_{\\mu}(Y))$ and minimize it over $\\mu$. Note that\n$$\n\\operatorname{Var}_{q_{\\mu}}(h(Y)\\, w_{\\mu}(Y)) \\;=\\; \\mathbb{E}_{q_{\\mu}}\\!\\left[h(Y)^{2}\\, w_{\\mu}(Y)^{2}\\right] \\;-\\; \\left(\\mathbb{E}[\\exp(X)]\\right)^{2}.\n$$\nThe second term does not depend on $\\mu$, so minimizing the variance over $\\mu$ is equivalent to minimizing the second moment\n$$\nM(\\mu) \\;=\\; \\mathbb{E}_{q_{\\mu}}\\!\\left[h(Y)^{2}\\, w_{\\mu}(Y)^{2}\\right] \\;=\\; \\int_{-\\infty}^{\\infty} \\exp(2x)\\, \\frac{p(x)^{2}}{q_{\\mu}(x)}\\, dx.\n$$\nUsing the explicit forms of $p$ and $q_{\\mu}$ with common variance $\\sigma^{2}$,\n$$\n\\frac{p(x)^{2}}{q_{\\mu}(x)} \\;=\\; \\frac{\\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\right)^{2}}{\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)} \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nHence,\n$$\nM(\\mu) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(2x - \\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx.\n$$\nCombine exponents:\n$$\n2x - \\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}} \\;=\\; -\\frac{x^{2}}{2\\sigma^{2}} + \\left(2 - \\frac{\\mu}{\\sigma^{2}}\\right) x + \\frac{\\mu^{2}}{2\\sigma^{2}}.\n$$\nComplete the square by defining\n$$\nm \\;=\\; 2\\sigma^{2} - \\mu,\n$$\nso that\n$$\n-\\frac{x^{2}}{2\\sigma^{2}} + \\left(2 - \\frac{\\mu}{\\sigma^{2}}\\right) x \\;=\\; -\\frac{(x - m)^{2}}{2\\sigma^{2}} + \\frac{m^{2}}{2\\sigma^{2}}.\n$$\nTherefore,\n$$\nM(\\mu) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(\\frac{m^{2}}{2\\sigma^{2}} + \\frac{\\mu^{2}}{2\\sigma^{2}}\\right) \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x - m)^{2}}{2\\sigma^{2}}\\right) dx \\;=\\; \\exp\\!\\left(\\frac{m^{2} + \\mu^{2}}{2\\sigma^{2}}\\right).\n$$\nSubstituting $m = 2\\sigma^{2} - \\mu$,\n$$\nm^{2} + \\mu^{2} \\;=\\; (2\\sigma^{2} - \\mu)^{2} + \\mu^{2} \\;=\\; 4\\sigma^{4} - 4\\mu \\sigma^{2} + 2\\mu^{2},\n$$\nhence\n$$\nM(\\mu) \\;=\\; \\exp\\!\\left(\\frac{4\\sigma^{4} - 4\\mu \\sigma^{2} + 2\\mu^{2}}{2\\sigma^{2}}\\right) \\;=\\; \\exp\\!\\left(2\\sigma^{2} - 2\\mu + \\frac{\\mu^{2}}{\\sigma^{2}}\\right).\n$$\nAs a function of $\\mu$, minimizing $M(\\mu)$ is equivalent to minimizing the exponent\n$$\ng(\\mu) \\;=\\; \\frac{\\mu^{2}}{\\sigma^{2}} - 2\\mu + 2\\sigma^{2}.\n$$\nThis is a convex quadratic in $\\mu$ with derivative\n$$\ng'(\\mu) \\;=\\; \\frac{2\\mu}{\\sigma^{2}} - 2,\n$$\nwhich vanishes at\n$$\n\\mu^{\\star} \\;=\\; \\sigma^{2}.\n$$\nSince $g''(\\mu) = 2/\\sigma^{2} > 0$, this critical point is the unique minimizer. With $\\sigma^{2} = 100$, the optimal shift is\n$$\n\\mu^{\\star} \\;=\\; 100.\n$$\nAt $\\mu^{\\star} = \\sigma^{2}$, the proposal density $q_{\\mu^{\\star}}$ is proportional to $h(x)\\, p(x)$, which yields a zero-variance importance sampling estimator, consistent with\n$$\n\\operatorname{Var}_{q_{\\mu^{\\star}}}(h(Y)\\,w_{\\mu^{\\star}}(Y)) \\;=\\; M(\\mu^{\\star}) - \\left(\\mathbb{E}[\\exp(X)]\\right)^{2} \\;=\\; \\exp(\\sigma^{2}) - \\exp(\\sigma^{2}) \\;=\\; 0.\n$$\nCollecting results, the exact expectation is $\\exp(50)$ and the optimal shift is $\\mu^{\\star} = 100$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\exp(50) & 100\\end{pmatrix}}$$"
        },
        {
            "introduction": "Antithetic variates are often presented as a simple and effective trick, especially for monotone functions. However, a true expert understands a technique's limitations as well as its strengths, and this final exercise explores that boundary. You are tasked with analyzing a symmetric, non-monotone payoff function where antithetic sampling not only fails to help but actively harms the estimator by increasing its variance. This surprising result provides a crucial lesson on the importance of verifying the underlying mathematical assumptions before applying a variance reduction method .",
            "id": "2446675",
            "problem": "In a Monte Carlo (MC) simulation for pricing in computational finance, consider estimating the expected payoff $\\mu = \\mathbb{E}[g(X)]$ where $X \\sim U(0,1)$ and $g:[0,1]\\to\\mathbb{R}$ is a binary payoff function. Construct the payoff by taking a symmetric union of two boundary intervals: for a parameter $a \\in (0,\\tfrac{1}{2})$, define\n$$\ng(x) = \\mathbf{1}\\{x \\in [0,a] \\cup [1-a,1]\\}.\n$$\nLet $\\widehat{\\mu}_{c}$ be the crude MC estimator based on $2$ independent samples $X_{1},X_{2} \\sim U(0,1)$:\n$$\n\\widehat{\\mu}_{c} = \\frac{g(X_{1}) + g(X_{2})}{2},\n$$\nand let $\\widehat{\\mu}_{a}$ be the antithetic variates estimator based on one antithetic pair $(X,1-X)$ with $X \\sim U(0,1)$:\n$$\n\\widehat{\\mu}_{a} = \\frac{g(X) + g(1-X)}{2}.\n$$\nCompute the exact variance ratio\n$$\nR \\equiv \\frac{\\operatorname{Var}(\\widehat{\\mu}_{a})}{\\operatorname{Var}(\\widehat{\\mu}_{c})}.\n$$\nGive your answer as a single real number; no rounding is required.",
            "solution": "The problem requires the computation of the variance ratio $R \\equiv \\frac{\\operatorname{Var}(\\widehat{\\mu}_{a})}{\\operatorname{Var}(\\widehat{\\mu}_{c})}$, where $\\widehat{\\mu}_{c}$ is the crude Monte Carlo estimator and $\\widehat{\\mu}_{a}$ is the antithetic variates estimator for the expected value $\\mu = \\mathbb{E}[g(X)]$.\n\nFirst, we define the random variable $Y = g(X)$, where $X \\sim U(0,1)$ and the payoff function is given by $g(x) = \\mathbf{1}\\{x \\in [0,a] \\cup [1-a,1]\\}$ for $a \\in (0, \\frac{1}{2})$. The random variable $Y$ is a Bernoulli variable, as it can only take values $0$ or $1$.\n\nThe probability of success, $p$, is the expected value of $Y$:\n$$\np = \\mathbb{E}[Y] = \\mathbb{E}[g(X)] = \\int_{0}^{1} g(x) dx = \\int_{0}^{a} 1 \\,dx + \\int_{1-a}^{1} 1 \\,dx = a + (1 - (1-a)) = 2a.\n$$\nSince $Y$ is a Bernoulli random variable with parameter $p=2a$, its variance is given by:\n$$\n\\operatorname{Var}(Y) = p(1-p) = 2a(1-2a).\n$$\nThis can also be computed directly:\n$$\n\\mathbb{E}[Y^{2}] = \\mathbb{E}[g(X)^2] = \\mathbb{E}[g(X)] = 2a,\n$$\nbecause $g(x)^2 = g(x)$ for an indicator function.\nTherefore,\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = 2a - (2a)^2 = 2a(1-2a).\n$$\nSince $a \\in (0, \\frac{1}{2})$, we have $a > 0$ and $1-2a > 0$, which ensures that $\\operatorname{Var}(Y) > 0$.\n\nNext, we compute the variance of the crude Monte Carlo estimator, $\\widehat{\\mu}_{c}$.\nThis estimator is defined as $\\widehat{\\mu}_{c} = \\frac{g(X_{1}) + g(X_{2})}{2}$, where $X_{1}, X_{2}$ are independent and identically distributed (i.i.d.) samples from $U(0,1)$. Let $Y_1 = g(X_1)$ and $Y_2 = g(X_2)$. $Y_1$ and $Y_2$ are i.i.d. random variables with $\\operatorname{Var}(Y_1) = \\operatorname{Var}(Y_2) = \\operatorname{Var}(Y) = 2a(1-2a)$.\nDue to the independence of $Y_1$ and $Y_2$, the variance of their sum is the sum of their variances.\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{c}) = \\operatorname{Var}\\left(\\frac{Y_{1} + Y_{2}}{2}\\right) = \\frac{1}{4}\\operatorname{Var}(Y_1 + Y_2) = \\frac{1}{4}(\\operatorname{Var}(Y_{1}) + \\operatorname{Var}(Y_{2})).\n$$\nSubstituting the variance of $Y$:\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{c}) = \\frac{1}{4}(2a(1-2a) + 2a(1-2a)) = \\frac{1}{4}(4a(1-2a)) = a(1-2a).\n$$\nNow, we compute the variance of the antithetic variates estimator, $\\widehat{\\mu}_{a}$.\nThe estimator is defined as $\\widehat{\\mu}_{a} = \\frac{g(X) + g(1-X)}{2}$. We must analyze the relationship between $g(X)$ and $g(1-X)$.\nThe function $g(x)$ is symmetric about $x=\\frac{1}{2}$. Let's verify this property for $g(1-x)$:\n$$\ng(1-x) = \\mathbf{1}\\{1-x \\in [0,a] \\cup [1-a,1]\\}.\n$$\nThe condition $1-x \\in [0,a]$ is equivalent to $1-a \\le x \\le 1$.\nThe condition $1-x \\in [1-a,1]$ is equivalent to $0 \\le x \\le a$.\nThus, $g(1-x) = 1$ if and only if $x \\in [0,a] \\cup [1-a,1]$. This is the same condition for $g(x)=1$. For all other $x \\in (0,1)$, both $g(x)$ and $g(1-x)$ are $0$.\nTherefore, $g(x) = g(1-x)$ for all $x \\in [0,1]$.\n\nThis implies that the random variables $g(X)$ and $g(1-X)$ are identical. The antithetic estimator simplifies to:\n$$\n\\widehat{\\mu}_{a} = \\frac{g(X) + g(X)}{2} = g(X) = Y.\n$$\nThe variance of the antithetic estimator is thus the variance of $Y$:\n$$\n\\operatorname{Var}(\\widehat{\\mu}_{a}) = \\operatorname{Var}(g(X)) = 2a(1-2a).\n$$\nFinally, we compute the desired ratio $R$:\n$$\nR = \\frac{\\operatorname{Var}(\\widehat{\\mu}_{a})}{\\operatorname{Var}(\\widehat{\\mu}_{c})} = \\frac{2a(1-2a)}{a(1-2a)}.\n$$\nSince $a \\in (0, \\frac{1}{2})$, the term $a(1-2a)$ is strictly positive and can be cancelled from the numerator and the denominator.\n$$\nR = 2.\n$$\nThe ratio is a constant value, independent of the parameter $a$. This demonstrates a case where the antithetic sampling technique, applied to a symmetric function, increases the variance compared to the crude Monte Carlo method for the same number of function evaluations. Specifically, it doubles the sampling variance.",
            "answer": "$$\n\\boxed{2}\n$$"
        }
    ]
}