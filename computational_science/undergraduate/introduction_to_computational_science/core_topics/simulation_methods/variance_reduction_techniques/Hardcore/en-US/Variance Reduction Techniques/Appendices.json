{
    "hands_on_practices": [
        {
            "introduction": "The control variates method provides a powerful way to reduce variance by leveraging a correlated variable with a known expectation. This exercise  provides a concrete example of this principle, guiding you to calculate the optimal coefficient for the control variate and to quantify the resulting improvement. By working through the mechanics, you will gain a practical understanding of how to construct and optimize a more efficient estimator.",
            "id": "1348989",
            "problem": "An analyst is trying to estimate the value of $\\theta = E[(U+1)^2]$, where $U$ is a random variable uniformly distributed on the interval $[0, 1]$. The standard approach is to use a simple Monte Carlo estimator, where one generates samples of $X = (U+1)^2$ and averages them. The precision of this estimator is determined by the variance of a single sample, $\\text{Var}(X)$.\n\nTo improve the efficiency of the estimation, a control variate technique is proposed. A new estimator is constructed using a related random variable $C = U$, for which the expected value is known. The new estimator for a single sample is given by $X(b) = X - b(C - E[C])$, where $b$ is a real-valued constant coefficient to be optimized.\n\nYour task is to determine the theoretical improvement offered by this technique. First, find the optimal coefficient $b^*$ that minimizes the variance of the new estimator, $\\text{Var}(X(b))$. Then, calculate the variance reduction factor, which is the ratio of the variance of a single optimized control variate sample to the variance of a single simple sample.\n\nExpress this variance reduction factor, $\\frac{\\text{Var}(X(b^*))}{\\text{Var}(X)}$, as an exact fraction.",
            "solution": "Let $U \\sim \\text{Unif}[0,1]$, $X=(U+1)^{2}$, and $C=U$. For $k \\geq 0$, $E[U^{k}]=\\frac{1}{k+1}$. Hence $E[U]=\\frac{1}{2}$, $E[U^{2}]=\\frac{1}{3}$, $E[U^{3}]=\\frac{1}{4}$, $E[U^{4}]=\\frac{1}{5}$, and $\\text{Var}(U)=\\frac{1}{12}$.\n\nExpand $X$ as $X=U^{2}+2U+1$. Then\n$$\nE[X]=E[U^{2}]+2E[U]+1=\\frac{1}{3}+1+1=\\frac{7}{3}.\n$$\nAlso\n$$\nX^{2}=(U^{2}+2U+1)^{2}=U^{4}+4U^{3}+6U^{2}+4U+1,\n$$\nso\n$$\nE[X^{2}]=\\frac{1}{5}+4\\cdot\\frac{1}{4}+6\\cdot\\frac{1}{3}+4\\cdot\\frac{1}{2}+1=\\frac{31}{5}.\n$$\nTherefore\n$$\n\\text{Var}(X)=E[X^{2}]-(E[X])^{2}=\\frac{31}{5}-\\left(\\frac{7}{3}\\right)^{2}=\\frac{34}{45}.\n$$\n\nCompute the covariance with $C=U$:\n$$\n\\text{Cov}(U^{2},U)=E[U^{3}]-E[U^{2}]E[U]=\\frac{1}{4}-\\frac{1}{3}\\cdot\\frac{1}{2}=\\frac{1}{12},\n$$\nso\n$$\n\\text{Cov}(X,C)=\\text{Cov}(U^{2}+2U+1,U)=\\frac{1}{12}+2\\cdot\\text{Var}(U)=\\frac{1}{12}+2\\cdot\\frac{1}{12}=\\frac{1}{4},\n$$\nand $\\text{Var}(C)=\\text{Var}(U)=\\frac{1}{12}$.\n\nFor the control variate estimator $X(b)=X-b(C-E[C])$, the variance is\n$$\n\\text{Var}(X(b))=\\text{Var}(X)-2b\\,\\text{Cov}(X,C)+b^{2}\\text{Var}(C).\n$$\nMinimizing the quadratic in $b$ yields\n$$\nb^{*}=\\frac{\\text{Cov}(X,C)}{\\text{Var}(C)}=\\frac{\\frac{1}{4}}{\\frac{1}{12}}=3,\n$$\nand the minimized variance\n$$\n\\text{Var}(X(b^{*}))=\\text{Var}(X)-\\frac{\\text{Cov}(X,C)^{2}}{\\text{Var}(C)}=\\frac{34}{45}-\\frac{\\left(\\frac{1}{4}\\right)^{2}}{\\frac{1}{12}}=\\frac{34}{45}-\\frac{3}{4}=\\frac{1}{180}.\n$$\n\nHence the variance reduction factor is\n$$\n\\frac{\\text{Var}(X(b^{*}))}{\\text{Var}(X)}=\\frac{\\frac{1}{180}}{\\frac{34}{45}}=\\frac{1}{136}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{136}}$$"
        },
        {
            "introduction": "Antithetic variates aim to reduce variance by pairing each random sample with its 'opposite,' inducing negative correlation. However, this technique's success is not guaranteed and depends critically on the function being estimated. This practice  presents a thought experiment to explore this nuance, contrasting a scenario where the method fails with one where it succeeds, thereby revealing the crucial role of function monotonicity.",
            "id": "3201688",
            "problem": "A Monte Carlo (MC) estimator for $\\pi$ draws uniform points in a square and uses the indicator of being inside the corresponding circle. Consider two sampling setups and the antithetic variates (AV) strategy that maps a uniform variable $U$ to its antithetic partner $1-U$.\n\nSetup $S1$: Sample $(X,Y)$ uniformly from $[-1,1]^2$ by setting $X = 2U_1 - 1$ and $Y = 2U_2 - 1$ with $U_1, U_2 \\sim \\text{Uniform}(0,1)$ independent. The estimator uses the indicator $Z = \\mathbb{1}\\{X^2 + Y^2 \\le 1\\}$ and scales by $4$ to estimate $\\pi$.\n\nSetup $S2$: Sample $(X,Y)$ uniformly from $[0,1]^2$ with $X = U_1$ and $Y = U_2$ where $U_1, U_2 \\sim \\text{Uniform}(0,1)$ independent. The estimator again uses $Z = \\mathbb{1}\\{X^2 + Y^2 \\le 1\\}$ and scales by $4$.\n\nIn both setups, form an AV pair by reflecting each coordinate via $(U_1,U_2) \\mapsto (1-U_1,1-U_2)$, producing a second point. Average the two corresponding indicators to get a pair-average contribution. Use only foundational facts: the definition of variance $\\operatorname{Var}(W) = \\mathbb{E}[W^2] - \\mathbb{E}[W]^2$, covariance $\\operatorname{Cov}(W_1,W_2) = \\mathbb{E}[W_1 W_2] - \\mathbb{E}[W_1]\\mathbb{E}[W_2]$, and the geometric meaning of $\\mathbb{E}[Z]$ as the area proportion inside the circle.\n\nWhich statement correctly characterizes when monotonicity of the indicator with respect to the uniform inputs yields variance reduction versus no gain?\n\nA. In $S1$, the AV pair-average has the same variance as averaging $2$ independent indicators because $\\operatorname{Cov}(Z, Z^{\\text{anti}}) = 0$. In $S2$, the AV pair-average also has no variance reduction because the indicator is symmetric.\n\nB. In $S1$, the AV pair-average yields no variance reduction because $Z^{\\text{anti}} = Z$ almost surely, so $\\operatorname{Cov}(Z, Z^{\\text{anti}}) = \\operatorname{Var}(Z)  0$. In $S2$, the indicator is coordinate-wise nonincreasing on $[0,1]^2$, implying $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$, so the AV pair-average variance is strictly smaller than using $2$ independent points.\n\nC. In $S2$, the AV pair-average increases variance because $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$; points $(X,Y)$ and $(1-X,1-Y)$ both tend to lie near the origin, reinforcing each other. In $S1$, AV reduces variance because flipping signs makes the indicator monotone in $|X|$ and $|Y|$.\n\nD. AV reduces variance in both $S1$ and $S2$ whenever the indicator is symmetric, because symmetry guarantees negative correlation between $Z$ and $Z^{\\text{anti}}$.",
            "solution": "The validity of the problem statement is hereby confirmed. The problem is scientifically grounded, well-posed, objective, and complete. All terms are standard in the field of computational science and statistics.\n\nThe core principle of the antithetic variates (AV) technique is to reduce variance by introducing negative correlation between estimators. For a Monte Carlo estimator based on a random variable $Z$ which is a function of a set of uniform random variables $U = (U_1, \\dots, U_d)$, the AV method generates a pair $(Z, Z^{\\text{anti}})$. Here, $Z$ is computed from a random draw $U$, and $Z^{\\text{anti}}$ is computed from the antithetic draw $1-U = (1-U_1, \\dots, 1-U_d)$. The estimator for one pair is $\\hat{\\theta}_{AV} = \\frac{Z + Z^{\\text{anti}}}{2}$.\n\nThe variance of this pair-averaged estimator is:\n$$ \\operatorname{Var}(\\hat{\\theta}_{AV}) = \\operatorname{Var}\\left(\\frac{Z + Z^{\\text{anti}}}{2}\\right) = \\frac{1}{4}\\operatorname{Var}(Z + Z^{\\text{anti}}) $$\nUsing the formula for the variance of a sum:\n$$ \\operatorname{Var}(Z + Z^{\\text{anti}}) = \\operatorname{Var}(Z) + \\operatorname{Var}(Z^{\\text{anti}}) + 2\\operatorname{Cov}(Z, Z^{\\text{anti}}) $$\nSince $U_i$ and $1-U_i$ have the same distribution ($\\text{Uniform}(0,1)$), $Z$ and $Z^{\\text{anti}}$ are identically distributed, which implies $\\operatorname{Var}(Z) = \\operatorname{Var}(Z^{\\text{anti}})$. Thus,\n$$ \\operatorname{Var}(\\hat{\\theta}_{AV}) = \\frac{1}{4}(2\\operatorname{Var}(Z) + 2\\operatorname{Cov}(Z, Z^{\\text{anti}})) = \\frac{1}{2}(\\operatorname{Var}(Z) + \\operatorname{Cov}(Z, Z^{\\text{anti}})) $$\nFor comparison, the standard Monte Carlo estimator using the average of two independent samples, $Z_1$ and $Z_2$, is $\\hat{\\theta}_{MC} = \\frac{Z_1 + Z_2}{2}$. Its variance is:\n$$ \\operatorname{Var}(\\hat{\\theta}_{MC}) = \\operatorname{Var}\\left(\\frac{Z_1 + Z_2}{2}\\right) = \\frac{1}{4}(\\operatorname{Var}(Z_1) + \\operatorname{Var}(Z_2)) = \\frac{1}{2}\\operatorname{Var}(Z) $$\nVariance reduction is achieved if $\\operatorname{Var}(\\hat{\\theta}_{AV})  \\operatorname{Var}(\\hat{\\theta}_{MC})$, which simplifies to the condition $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$. If $\\operatorname{Cov}(Z, Z^{\\text{anti}}) = 0$, there is no gain or loss in variance compared to independent sampling. If $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$, the variance is increased.\n\nWe now analyze each setup based on this principle.\n\n**Analysis of Setup S1**\n\nIn this setup, the point $(X,Y)$ is sampled uniformly from $[-1,1]^2$ by the transformation $X = 2U_1 - 1$ and $Y = 2U_2 - 1$, where $U_1, U_2 \\sim \\text{Uniform}(0,1)$ are independent. The estimator is $Z = \\mathbb{1}\\{X^2 + Y^2 \\le 1\\}$.\n\nThe antithetic pair is formed using $(1-U_1, 1-U_2)$. The corresponding antithetic point $(X^{\\text{anti}}, Y^{\\text{anti}})$ is:\n$$ X^{\\text{anti}} = 2(1 - U_1) - 1 = 2 - 2U_1 - 1 = 1 - 2U_1 = -(2U_1 - 1) = -X $$\n$$ Y^{\\text{anti}} = 2(1 - U_2) - 1 = 2 - 2U_2 - 1 = 1 - 2U_2 = -(2U_2 - 1) = -Y $$\nThe antithetic point is $(-X, -Y)$. The indicator function evaluated at this point is:\n$$ Z^{\\text{anti}} = \\mathbb{1}\\{(X^{\\text{anti}})^2 + (Y^{\\text{anti}})^2 \\le 1\\} = \\mathbb{1}\\{(-X)^2 + (-Y)^2 \\le 1\\} = \\mathbb{1}\\{X^2 + Y^2 \\le 1\\} $$\nTherefore, for any realization $(U_1, U_2)$, we have $Z^{\\text{anti}} = Z$. This holds almost surely.\n\nThe covariance between $Z$ and $Z^{\\text{anti}}$ is:\n$$ \\operatorname{Cov}(Z, Z^{\\text{anti}}) = \\operatorname{Cov}(Z, Z) = \\operatorname{Var}(Z) $$\nThe indicator $Z$ is a Bernoulli random variable. Its expectation $\\mathbb{E}[Z]$ is the probability of a point falling inside the unit circle, which is the ratio of the area of the circle ($\\pi r^2 = \\pi$) to the area of the square ($2 \\times 2 = 4$). So, $p = \\mathbb{E}[Z] = \\pi/4$.\nThe variance of a Bernoulli variable is $\\operatorname{Var}(Z) = p(1-p) = \\frac{\\pi}{4}(1 - \\frac{\\pi}{4})$. Since $0  \\pi  4$, we have $\\operatorname{Var}(Z)  0$.\nThus, $\\operatorname{Cov}(Z, Z^{\\text{anti}}) = \\operatorname{Var}(Z)  0$. According to our criterion, this means that the antithetic variates technique not only fails to reduce variance but actually increases it compared to independent sampling. There is certainly no variance reduction.\n\n**Analysis of Setup S2**\n\nIn this setup, the point $(X,Y)$ is sampled uniformly from $[0,1]^2$ by setting $X = U_1$ and $Y = U_2$, where $U_1, U_2 \\sim \\text{Uniform}(0,1)$ are independent. The estimator is $Z = \\mathbb{1}\\{X^2 + Y^2 \\le 1\\} = \\mathbb{1}\\{U_1^2 + U_2^2 \\le 1\\}$.\n\nLet's analyze the function $f(U_1, U_2) = Z = \\mathbb{1}\\{U_1^2 + U_2^2 \\le 1\\}$ on the domain $[0,1]^2$. The expression $U_1^2 + U_2^2$ is a strictly increasing function of $U_1$ and $U_2$ on $[0,1]^2$. The indicator function $\\mathbb{1}\\{g \\le c\\}$ is non-increasing in its argument $g$. Therefore, the composite function $f(U_1, U_2)$ is a coordinate-wise non-increasing function of $U_1$ and $U_2$.\n\nThe antithetic indicator is $Z^{\\text{anti}} = f(1-U_1, 1-U_2) = \\mathbb{1}\\{(1-U_1)^2 + (1-U_2)^2 \\le 1\\}$.\nA fundamental result in variance reduction states that if a function $f(u_1, \\dots, u_d)$ is coordinate-wise monotone (either entirely non-increasing or entirely non-decreasing in all variables), then $\\operatorname{Cov}(f(U), f(1-U)) \\le 0$.\nMore specifically, if $f$ is non-increasing in its arguments, and we define a new function $g(U) = f(1-U)$, then $g$ is non-decreasing in the arguments of $U$. The covariance between a non-increasing function and a non-decreasing function is non-positive.\nIn our case, $Z$ is a non-trivial, coordinate-wise non-increasing function of $(U_1, U_2)$. Thus, the covariance is non-positive: $\\operatorname{Cov}(Z, Z^{\\text{anti}}) \\le 0$. The inequality is strict unless one variable is a deterministic function of the other in a way that nullifies the correlation, which is not the case here.\nFor instance, a small $(U_1, U_2)$ near $(0,0)$ makes $Z=1$ likely, while $(1-U_1, 1-U_2)$ is near $(1,1)$, making $Z^{\\text{anti}}=0$ likely. This inverse relationship leads to a strictly negative covariance, $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$.\nConsequently, in Setup S2, the antithetic variates technique yields a strict variance reduction.\n\n**Evaluation of Options**\n\n*   **A. In $S1$, the AV pair-average has the same variance as averaging $2$ independent indicators because $\\operatorname{Cov}(Z, Z^{\\text{anti}}) = 0$. In $S2$, the AV pair-average also has no variance reduction because the indicator is symmetric.**\n    *   The claim for S1 is incorrect. We found $\\operatorname{Cov}(Z, Z^{\\text{anti}}) = \\operatorname{Var}(Z)  0$, not $0$. This leads to an increase in variance, not the same variance.\n    *   The claim for S2 is incorrect. We found a strict variance reduction, $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$.\n    *   Therefore, option A is **Incorrect**.\n\n*   **B. In $S1$, the AV pair-average yields no variance reduction because $Z^{\\text{anti}} = Z$ almost surely, so $\\operatorname{Cov}(Z, Z^{\\text{anti}}) = \\operatorname{Var}(Z)  0$. In $S2$, the indicator is coordinate-wise nonincreasing on $[0,1]^2$, implying $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$, so the AV pair-average variance is strictly smaller than using $2$ independent points.**\n    *   The analysis for S1 is perfectly sound. It correctly identifies that $Z^{\\text{anti}} = Z$ and concludes that the covariance is positive, leading to no variance reduction.\n    *   The analysis for S2 is also perfectly sound. It correctly identifies the coordinate-wise non-increasing nature of the indicator function with respect to the input uniform variables, which implies negative covariance and thus guarantees variance reduction.\n    *   Therefore, option B is **Correct**.\n\n*   **C. In $S2$, the AV pair-average increases variance because $\\operatorname{Cov}(Z, Z^{\\text{anti}})  0$; points $(X,Y)$ and $(1-X,1-Y)$ both tend to lie near the origin, reinforcing each other. In $S1$, AV reduces variance because flipping signs makes the indicator monotone in $|X|$ and $|Y|$.**\n    *   The claim for S2 is incorrect. The covariance is negative, not positive. The reasoning is also flawed; if $(X,Y)$ is near the origin $(0,0)$, then $(1-X, 1-Y)$ is near $(1,1)$, not the origin.\n    *   The claim for S1 is incorrect. AV increases variance, it does not reduce it. The reasoning provided is obscure and not mathematically sound in this context.\n    *   Therefore, option C is **Incorrect**.\n\n*   **D. AV reduces variance in both $S1$ and $S2$ whenever the indicator is symmetric, because symmetry guarantees negative correlation between $Z$ and $Z^{\\text{anti}}$.**\n    *   The claim that AV reduces variance in S1 is false. We proved the contrary.\n    *   The reasoning that \"symmetry guarantees negative correlation\" is a false generalization. In S1, the symmetry of the problem leads to $Z = Z^{\\text{anti}}$ and thus positive correlation. The effectiveness of AV depends on the interplay between the function being estimated and the specific transformation induced by $U \\to 1-U$, not on a vague notion of symmetry.\n    *   Therefore, option D is **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Importance sampling is a highly flexible technique that improves efficiency by concentrating samples in regions that contribute most to the expectation. This hands-on problem  challenges you to find the theoretically optimal sampling distribution for a specific estimation task. By deriving the proposal distribution that minimizes—and in this hypothetical case, eliminates—variance, you will grasp the underlying power and goal of the importance sampling method.",
            "id": "2446710",
            "problem": "Consider the task of computing the expectation of the exponential of a normally distributed random variable in a Monte Carlo setting relevant to computational economics and finance. Let $X \\sim \\mathcal{N}(0, 100)$, and define $h(x) = \\exp(x)$. You will estimate the quantity $\\mathbb{E}[\\exp(X)]$ via importance sampling using a proposal distribution $Q$ given by $Y \\sim \\mathcal{N}(\\mu, 100)$ with unknown shift parameter $\\mu \\in \\mathbb{R}$. Let $p(x)$ and $q_{\\mu}(x)$ denote the probability density functions of $\\mathcal{N}(0, 100)$ and $\\mathcal{N}(\\mu, 100)$, respectively. The importance sampling estimator based on $n$ independent and identically distributed samples $\\{Y_i\\}_{i=1}^{n}$ from $q_{\\mu}$ is\n$$\nI_n(\\mu) \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} h(Y_i)\\, w_{\\mu}(Y_i), \\quad \\text{where } w_{\\mu}(y) \\;=\\; \\frac{p(y)}{q_{\\mu}(y)}.\n$$\nWorking from first principles and without invoking any specific variance reduction formulas, do the following:\n1. Derive a closed-form expression for $\\mathbb{E}[\\exp(X)]$.\n2. Derive $\\operatorname{Var}_{q_{\\mu}}(h(Y)\\,w_{\\mu}(Y))$ as a function of $\\mu$ and identify the value $\\mu^{\\star}$ that minimizes this variance over $\\mu \\in \\mathbb{R}$.\n\nProvide your final answer as a single ordered pair consisting of the exact value of $\\mathbb{E}[\\exp(X)]$ and the optimal shift $\\mu^{\\star}$. No numerical rounding is required, and your answer should be exact with no units.",
            "solution": "We begin by recalling the definitions. The target distribution is $X \\sim \\mathcal{N}(0, 100)$ with density\n$$\np(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma^{2} = 100,\n$$\nand the proposal distribution is $Y \\sim \\mathcal{N}(\\mu, 100)$ with density\n$$\nq_{\\mu}(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma^{2} = 100.\n$$\nThe importance weight is $w_{\\mu}(x) = p(x)/q_{\\mu}(x)$.\n\nStep 1: Compute $\\mathbb{E}[\\exp(X)]$ in closed form. By definition,\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\int_{-\\infty}^{\\infty} \\exp(x)\\, p(x)\\, dx \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(x - \\frac{x^{2}}{2\\sigma^{2}}\\right) dx.\n$$\nComplete the square in the exponent. Write\n$$\nx - \\frac{x^{2}}{2\\sigma^{2}} \\;=\\; -\\frac{1}{2\\sigma^{2}}\\left(x^{2} - 2\\sigma^{2} x \\right) \\;=\\; -\\frac{1}{2\\sigma^{2}}\\left[(x - \\sigma^{2})^{2} - \\sigma^{4}\\right] \\;=\\; -\\frac{(x - \\sigma^{2})^{2}}{2\\sigma^{2}} + \\frac{\\sigma^{2}}{2}.\n$$\nTherefore,\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\, \\sigma} \\exp\\!\\left(\\frac{\\sigma^{2}}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x - \\sigma^{2})^{2}}{2\\sigma^{2}}\\right) dx \\;=\\; \\exp\\!\\left(\\frac{\\sigma^{2}}{2}\\right),\n$$\nsince the integral equals $\\sqrt{2\\pi}\\, \\sigma$. With $\\sigma^{2} = 100$, this yields\n$$\n\\mathbb{E}[\\exp(X)] \\;=\\; \\exp(50).\n$$\n\nStep 2: Derive $\\operatorname{Var}_{q_{\\mu}}(h(Y)\\, w_{\\mu}(Y))$ and minimize it over $\\mu$. Note that\n$$\n\\operatorname{Var}_{q_{\\mu}}(h(Y)\\, w_{\\mu}(Y)) \\;=\\; \\mathbb{E}_{q_{\\mu}}\\!\\left[h(Y)^{2}\\, w_{\\mu}(Y)^{2}\\right] \\;-\\; \\left(\\mathbb{E}[\\exp(X)]\\right)^{2}.\n$$\nThe second term does not depend on $\\mu$, so minimizing the variance over $\\mu$ is equivalent to minimizing the second moment\n$$\nM(\\mu) \\;=\\; \\mathbb{E}_{q_{\\mu}}\\!\\left[h(Y)^{2}\\, w_{\\mu}(Y)^{2}\\right] \\;=\\; \\int_{-\\infty}^{\\infty} \\exp(2x)\\, \\frac{p(x)^{2}}{q_{\\mu}(x)}\\, dx.\n$$\nUsing the explicit forms of $p$ and $q_{\\mu}$ with common variance $\\sigma^{2}$,\n$$\n\\frac{p(x)^{2}}{q_{\\mu}(x)} \\;=\\; \\frac{\\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\right)^{2}}{\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right)} \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\nHence,\n$$\nM(\\mu) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\!\\left(2x - \\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) dx.\n$$\nCombine exponents:\n$$\n2x - \\frac{x^{2}}{\\sigma^{2}} + \\frac{(x-\\mu)^{2}}{2\\sigma^{2}} \\;=\\; -\\frac{x^{2}}{2\\sigma^{2}} + \\left(2 - \\frac{\\mu}{\\sigma^{2}}\\right) x + \\frac{\\mu^{2}}{2\\sigma^{2}}.\n$$\nComplete the square by defining\n$$\nm \\;=\\; 2\\sigma^{2} - \\mu,\n$$\nso that\n$$\n-\\frac{x^{2}}{2\\sigma^{2}} + \\left(2 - \\frac{\\mu}{\\sigma^{2}}\\right) x \\;=\\; -\\frac{(x - m)^{2}}{2\\sigma^{2}} + \\frac{m^{2}}{2\\sigma^{2}}.\n$$\nTherefore,\n$$\nM(\\mu) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(\\frac{m^{2}}{2\\sigma^{2}} + \\frac{\\mu^{2}}{2\\sigma^{2}}\\right) \\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x - m)^{2}}{2\\sigma^{2}}\\right) dx \\;=\\; \\exp\\!\\left(\\frac{m^{2} + \\mu^{2}}{2\\sigma^{2}}\\right).\n$$\nSubstituting $m = 2\\sigma^{2} - \\mu$,\n$$\nm^{2} + \\mu^{2} \\;=\\; (2\\sigma^{2} - \\mu)^{2} + \\mu^{2} \\;=\\; 4\\sigma^{4} - 4\\mu \\sigma^{2} + 2\\mu^{2},\n$$\nhence\n$$\nM(\\mu) \\;=\\; \\exp\\!\\left(\\frac{4\\sigma^{4} - 4\\mu \\sigma^{2} + 2\\mu^{2}}{2\\sigma^{2}}\\right) \\;=\\; \\exp\\!\\left(2\\sigma^{2} - 2\\mu + \\frac{\\mu^{2}}{\\sigma^{2}}\\right).\n$$\nAs a function of $\\mu$, minimizing $M(\\mu)$ is equivalent to minimizing the exponent\n$$\ng(\\mu) \\;=\\; \\frac{\\mu^{2}}{\\sigma^{2}} - 2\\mu + 2\\sigma^{2}.\n$$\nThis is a convex quadratic in $\\mu$ with derivative\n$$\ng'(\\mu) \\;=\\; \\frac{2\\mu}{\\sigma^{2}} - 2,\n$$\nwhich vanishes at\n$$\n\\mu^{\\star} \\;=\\; \\sigma^{2}.\n$$\nSince $g''(\\mu) = 2/\\sigma^{2}  0$, this critical point is the unique minimizer. With $\\sigma^{2} = 100$, the optimal shift is\n$$\n\\mu^{\\star} \\;=\\; 100.\n$$\nAt $\\mu^{\\star} = \\sigma^{2}$, the proposal density $q_{\\mu^{\\star}}$ is proportional to $h(x)\\, p(x)$, which yields a zero-variance importance sampling estimator, consistent with\n$$\n\\operatorname{Var}_{q_{\\mu^{\\star}}}(h(Y)\\,w_{\\mu^{\\star}}(Y)) \\;=\\; M(\\mu^{\\star}) - \\left(\\mathbb{E}[\\exp(X)]\\right)^{2} \\;=\\; \\exp(\\sigma^{2}) - \\exp(\\sigma^{2}) \\;=\\; 0.\n$$\nCollecting results, the exact expectation is $\\exp(50)$ and the optimal shift is $\\mu^{\\star} = 100$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\exp(50)  100\\end{pmatrix}}$$"
        }
    ]
}