{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of a rejection sampler is fundamentally measured by its acceptance probability, which tells us how many proposed samples we expect to discard. This first exercise provides a foundational walkthrough of calculating this crucial metric. By using a simple uniform proposal to sample from a Beta distribution, you will practice finding the optimal constant $M$ and see how it directly determines the theoretical chance of accepting a proposed sample .",
            "id": "1387131",
            "problem": "A data scientist is implementing a rejection sampling algorithm to generate random numbers that follow a specific target distribution. The target distribution is a Beta distribution with parameters $\\alpha = 2$ and $\\beta = 2$. The probability density function (PDF) of this target distribution, denoted by $f(x)$, is given by:\n$$f(x) = 6x(1-x) \\quad \\text{for } x \\in [0, 1]$$\nand $f(x) = 0$ otherwise.\n\nFor the proposal distribution, the data scientist uses the standard uniform distribution, U(0, 1). The PDF of this proposal distribution, denoted by $g(x)$, is:\n$$g(x) = 1 \\quad \\text{for } x \\in [0, 1]$$\nand $g(x) = 0$ otherwise.\n\nThe rejection sampling method requires finding a constant $M$ such that the inequality $f(x) \\le M g(x)$ holds for all possible values of $x$. To maximize the efficiency of the sampling process, the smallest possible value of $M$ is chosen. The algorithm then generates a sample $Y$ from the proposal distribution $g(x)$ and accepts it with probability $\\frac{f(Y)}{M g(Y)}$.\n\nCalculate the overall theoretical probability that a sample drawn from the proposal distribution is accepted. Express your answer as an exact fraction.",
            "solution": "We need the smallest constant $M$ such that $f(x) \\le M g(x)$ for all $x$. Since $g(x)=1$ for $x \\in [0,1]$ and $g(x)=0$ otherwise, the condition reduces on $[0,1]$ to $f(x) \\le M$. Therefore the minimal admissible $M$ is the supremum of $f(x)$ on $[0,1]$.\n\nCompute the maximum of $f(x)=6x(1-x)=6(x - x^{2})$ on $[0,1]$. Differentiate:\n$$\nf'(x)=6(1-2x), \\quad f''(x)=-12.\n$$\nSetting $f'(x)=0$ gives $x=\\frac{1}{2}$. Since $f''(x)<0$, this is a maximum. Evaluate\n$$\nM=\\max_{x \\in [0,1]} f(x)=f\\!\\left(\\tfrac{1}{2}\\right)=6 \\cdot \\tfrac{1}{2} \\cdot \\left(1-\\tfrac{1}{2}\\right)=\\frac{3}{2}.\n$$\n\nGiven $Y \\sim g$, the acceptance probability conditional on $Y=y$ is $\\frac{f(y)}{M g(y)}$. The overall acceptance probability is the expectation under $g$:\n$$\n\\mathbb{P}(\\text{accept})=\\mathbb{E}_{g}\\!\\left[\\frac{f(Y)}{M g(Y)}\\right]=\\int_{0}^{1} \\frac{f(x)}{M g(x)} g(x) \\, dx=\\frac{1}{M} \\int_{0}^{1} f(x) \\, dx.\n$$\nSince $f$ is a probability density on $[0,1]$, $\\int_{0}^{1} f(x) \\, dx=1$. For completeness:\n$$\n\\int_{0}^{1} 6x(1-x)\\,dx=6 \\int_{0}^{1} \\left(x - x^{2}\\right) dx=6 \\left(\\frac{1}{2}-\\frac{1}{3}\\right)=1.\n$$\nTherefore,\n$$\n\\mathbb{P}(\\text{accept})=\\frac{1}{M}=\\frac{1}{\\frac{3}{2}}=\\frac{2}{3}.\n$$",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        },
        {
            "introduction": "Building on the concept of acceptance probability, we now explore its practical consequence: the computational cost. A low acceptance rate implies more rejected samples and more wasted computation. This exercise demonstrates how the number of proposals needed to obtain a single accepted sample follows a geometric distribution, allowing you to calculate the variance of this number and thus quantify the sampler's performance variability .",
            "id": "832183",
            "problem": "Rejection sampling is a Monte Carlo method used to generate samples from a target probability distribution $p(x)$ when direct sampling is difficult. The method relies on a simpler proposal distribution $q(x)$ from which we can easily draw samples. The algorithm proceeds by drawing a sample $x$ from $q(x)$ and accepting it with a certain probability. This is repeated until a sample is accepted.\n\nConsider a target distribution whose probability density is proportional to $f(x) = x$ over the interval $[0, 1]$, and zero elsewhere. We will use a uniform proposal distribution, $q(x) = 1$ for $x \\in [0, 1]$ and zero elsewhere.\n\nThe acceptance rule for a proposed sample $x$ is based on a constant $M$ which must satisfy $p(x) \\le M q(x)$ for all $x$, where $p(x)$ is the normalized target probability density function. For optimal efficiency, the smallest possible value of $M$ is chosen. A proposed sample $x \\sim q(x)$ is accepted if a random number $u$ drawn from the uniform distribution $U(0, 1)$ satisfies the condition $u \\le \\frac{p(x)}{M q(x)}$.\n\nLet $N$ be the random variable representing the total number of proposals required to obtain the first accepted sample. Derive the variance of $N$, denoted as $\\text{Var}(N)$.",
            "solution": "1. Normalize target density:  \n$$Z=\\int_0^1 x\\,dx=\\tfrac12,\\quad p(x)=\\frac{f(x)}{Z}=\\frac{x}{\\tfrac12}=2x\\quad (0\\le x\\le1).$$\n\n2. Optimal rejection constant:  \n$$M=\\sup_{x\\in[0,1]}\\frac{p(x)}{q(x)}=\\sup_{x\\in[0,1]}2x=2.$$\n\n3. Acceptance probability per trial:  \n$$\\alpha=\\int_0^1 q(x)\\,\\frac{p(x)}{M\\,q(x)}\\,dx=\\frac1M\\int_0^1p(x)\\,dx=\\frac1M.$$\n\n4. Geometric distribution of trials $N$ until first success has  \n$$\\mathrm{Var}(N)=\\frac{1-\\alpha}{\\alpha^2}=\\frac{1-\\tfrac1M}{(\\tfrac1M)^2}=(M-1)M.$$\n\n5. Substituting $M=2$ yields  \n$$\\mathrm{Var}(N)=(2-1)\\cdot2=2.$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Real-world applications often involve target distributions that are far from simple, featuring sharp peaks or multiple modes that are difficult to sample efficiently. This advanced practice moves beyond basic setups and into the realm of sampler design. You will construct a sophisticated Gaussian mixture proposal tailored to a complex, bimodal target, learning how to ensure the proposal properly \"envelopes\" the target and analyzing how its performance is sensitive to design choices like mean and variance .",
            "id": "3186739",
            "problem": "You are asked to construct a provably valid rejection sampler for an unnormalized one-dimensional target density defined by\n$$\np(x) \\propto e^{-x^2/0.01} + 0.01\\,e^{-(x-5)^2}.\n$$\nThe target has a very narrow spike near $x=0$ and a broader bump near $x=5$. Your goal is to design a proposal density $q(x)$ and a global bound $M$ such that $p(x) \\le M\\,q(x)$ for all real $x$, and to analyze how the expected acceptance rate depends on a misalignment parameter describing the displacement of the narrow spike.\n\nStarting from the fundamental definition of rejection sampling, use only core definitions and well-tested facts. Specifically:\n- Rejection sampling draws $X \\sim q(x)$ and $U \\sim \\text{Uniform}[0,1]$, and accepts $X$ if $U \\le \\frac{p(X)}{M q(X)}$.\n- If $p(x)$ is unnormalized, only the shape matters for acceptance, and the expected acceptance probability can be expressed in terms of the integral of $p(x)$ and the bound $M$.\n\nDesign the proposal as a two-component Gaussian mixture\n$$\nq(x) = c_1\\,\\varphi(x;\\mu_1,\\tau_1^2) + c_2\\,\\varphi(x;\\mu_2,\\tau_2^2),\n$$\nwhere $c_1,c_2 \\ge 0$, $c_1+c_2=1$, and $\\varphi(x;\\mu,\\tau^2)$ denotes the normalized Gaussian probability density function with mean $\\mu$ and variance $\\tau^2$. Use the following structure:\n- For the narrow target spike, set the target component variance parameter $s_1=0.005$ so that $e^{-x^2/0.01} = \\exp\\left(-\\frac{x^2}{2 s_1}\\right)$. Choose a proposal component $\\varphi(x;\\mu_1,\\tau_1^2)$ with variance inflation factor $\\kappa_1 \\ge 1$ so that $\\tau_1^2 = \\kappa_1 s_1$, and a misalignment parameter $\\delta$ so that $\\mu_1 = \\delta$.\n- For the broad target bump, set $s_2=0.5$ so that $e^{-(x-5)^2} = \\exp\\left(-\\frac{(x-5)^2}{2 s_2}\\right)$. Choose a proposal component $\\varphi(x;\\mu_2,\\tau_2^2)$ with $\\mu_2 = 5$ and variance inflation factor $\\kappa_2 \\ge 1$ so that $\\tau_2^2 = \\kappa_2 s_2$.\n- Use mixture weights $c_1$ and $c_2=1-c_1$.\n\nTasks:\n1. For each target component, derive the smallest constant $\\alpha_i$ such that the componentwise dominance holds for all real $x$:\n$$\ne^{-\\frac{x^2}{2 s_1}} \\le \\alpha_1\\,\\varphi(x;\\delta,\\tau_1^2),\n$$\n$$\n0.01\\,e^{-\\frac{(x-5)^2}{2 s_2}} \\le \\alpha_2\\,\\varphi(x;5,\\tau_2^2).\n$$\nExpress $\\alpha_1$ and $\\alpha_2$ in closed form in terms of $(s_1,\\tau_1^2,\\delta)$ and $(s_2,\\tau_2^2)$, respectively. Determine the conditions under which $\\alpha_1$ is finite when $\\delta \\ne 0$.\n2. Combine the componentwise bounds into a single global bound $M$ such that $p(x) \\le M\\,q(x)$ for all real $x$. Justify your construction using only algebraic inequalities and properties of nonnegative functions, without numerical maximization over $x$.\n3. Using the definitions of rejection sampling and the fact that $q(x)$ is a probability density function, derive a formula for the expected acceptance probability in terms of $M$ and the integral of $p(x)$ over the real line. Evaluate this integral exactly.\n4. Sensitivity analysis: With $s_1=0.005$ and $s_2=0.5$ fixed, take $\\kappa_2=1$ and $\\mu_2=5$. For the test suite below, compute the expected acceptance rate for each parameter set. Discuss how the acceptance rate depends on the misalignment $\\delta$ and on the variance inflation $\\kappa_1$.\n\nRequired test suite. Each tuple is $(\\delta,\\kappa_1,c_1)$ with $c_2=1-c_1$ and $\\kappa_2=1$:\n- $(0.0,\\,1.0,\\,10/11)$\n- $(0.02,\\,1.5,\\,10/11)$\n- $(0.05,\\,1.5,\\,10/11)$\n- $(0.10,\\,1.5,\\,10/11)$\n- $(0.20,\\,2.0,\\,10/11)$\n- $(0.50,\\,5.0,\\,10/11)$\n- $(0.00,\\,1.0,\\,0.70)$\n- $(0.05,\\,1.01,\\,10/11)$\n\nNotes and constraints:\n- Ensure that your design and derivation start from core definitions and widely accepted facts only. Do not use any shortcut formulas not derived from these bases.\n- Angle units are not applicable. No physical units are involved.\n- Your program must compute the expected acceptance rates for all test cases using your derived formulas and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places, for example: \"[r1,r2,...]\".\n- All intermediate reasoning should be mathematically and logically sound for all real $x$, and your $M$ must be valid globally (not only on a truncated interval).",
            "solution": "The user-provided problem is a well-posed exercise in designing and analyzing a rejection sampler for a bimodal target distribution. It is scientifically grounded in probability theory and computational statistics. All parameters and constraints are clearly specified, and the tasks lead to a unique, verifiable solution. The problem is therefore deemed valid.\n\nLet the unnormalized target density be denoted by $p(x)$, where\n$$\np(x) = e^{-x^2/0.01} + 0.01\\,e^{-(x-5)^2}\n$$\nThis can be written as a sum of two components, $p(x) = p_1(x) + p_2(x)$, where $p_1(x) = e^{-x^2/(2s_1)}$ and $p_2(x) = w_2 e^{-(x-5)^2/(2s_2)}$ with $s_1=0.005$, $s_2=0.5$, and $w_2=0.01$.\n\nThe proposal density is a two-component Gaussian mixture:\n$$\nq(x) = c_1\\,\\varphi(x;\\mu_1,\\tau_1^2) + c_2\\,\\varphi(x;\\mu_2,\\tau_2^2)\n$$\nwhere $\\varphi(x;\\mu,\\tau^2) = \\frac{1}{\\sqrt{2\\pi}\\tau} e^{-\\frac{(x-\\mu)^2}{2\\tau^2}}$ is the probability density function (PDF) of a normal distribution with mean $\\mu$ and variance $\\tau^2$. The parameters are defined as $\\mu_1=\\delta$, $\\tau_1^2=\\kappa_1 s_1$ for the first component, and $\\mu_2=5$, $\\tau_2^2=\\kappa_2 s_2$ for the second. The mixture weights satisfy $c_1, c_2 \\ge 0$ and $c_1+c_2=1$.\n\nThe core principle of rejection sampling requires finding a constant $M$ such that $p(x) \\le M q(x)$ for all real $x$.\n\n### Task 1: Derivation of Component-wise Bounds\nWe seek the smallest constants $\\alpha_1$ and $\\alpha_2$ satisfying the component-wise inequalities.\n\n**Bound for the first component ($\\alpha_1$):**\nWe need to find the minimum $\\alpha_1$ such that $p_1(x) \\le \\alpha_1 \\varphi(x;\\delta, \\tau_1^2)$ for all $x$. This is equivalent to finding the maximum of the ratio:\n$$\n\\alpha_1 = \\max_{x \\in \\mathbb{R}} \\frac{p_1(x)}{\\varphi(x;\\delta, \\tau_1^2)} = \\max_{x \\in \\mathbb{R}} \\frac{e^{-x^2/(2s_1)}}{\\frac{1}{\\sqrt{2\\pi}\\tau_1} e^{-(x-\\delta)^2/(2\\tau_1^2)}} = \\sqrt{2\\pi}\\tau_1 \\max_{x \\in \\mathbb{R}} \\exp\\left( \\frac{(x-\\delta)^2}{2\\tau_1^2} - \\frac{x^2}{2s_1} \\right)\n$$\nLet's analyze the exponent, $E_1(x) = \\frac{(x-\\delta)^2}{2\\tau_1^2} - \\frac{x^2}{2s_1}$:\n$$\nE_1(x) = \\left(\\frac{1}{2\\tau_1^2} - \\frac{1}{2s_1}\\right)x^2 - \\frac{\\delta}{\\tau_1^2}x + \\frac{\\delta^2}{2\\tau_1^2}\n$$\nThis is a quadratic function of $x$. For it to have a finite maximum, the coefficient of $x^2$ must be negative.\n$$\n\\frac{1}{2\\tau_1^2} - \\frac{1}{2s_1} < 0 \\implies \\tau_1^2 > s_1\n$$\nUsing the definition $\\tau_1^2 = \\kappa_1 s_1$, this condition becomes $\\kappa_1 s_1 > s_1$, which implies $\\kappa_1 > 1$. When this holds, the maximum of the quadratic $Ax^2+Bx+C$ is $C - B^2/(4A)$. Here, $A = \\frac{1}{2}(\\frac{1}{\\tau_1^2}-\\frac{1}{s_1})$, $B = -\\frac{\\delta}{\\tau_1^2}$, and $C = \\frac{\\delta^2}{2\\tau_1^2}$. The maximum value of the exponent is:\n$$\nE_{1,max} = \\frac{\\delta^2}{2\\tau_1^2} - \\frac{(-\\delta/\\tau_1^2)^2}{4 \\cdot \\frac{1}{2}(\\frac{1}{\\tau_1^2}-\\frac{1}{s_1})} = \\frac{\\delta^2}{2\\tau_1^2} - \\frac{\\delta^2/\\tau_1^4}{2(\\frac{s_1-\\tau_1^2}{s_1\\tau_1^2})} = \\frac{\\delta^2}{2\\tau_1^2} - \\frac{\\delta^2 s_1}{2\\tau_1^2(s_1-\\tau_1^2)} = \\frac{\\delta^2 (s_1-\\tau_1^2-s_1)}{2\\tau_1^2(s_1-\\tau_1^2)} = \\frac{-\\delta^2\\tau_1^2}{2\\tau_1^2(s_1-\\tau_1^2)} = \\frac{\\delta^2}{2(\\tau_1^2-s_1)}\n$$\nSubstituting $\\tau_1^2 = \\kappa_1 s_1$, we get $E_{1,max} = \\frac{\\delta^2}{2s_1(\\kappa_1-1)}$. Thus, for $\\kappa_1 > 1$:\n$$\n\\alpha_1 = \\sqrt{2\\pi\\tau_1^2} \\exp\\left( \\frac{\\delta^2}{2s_1(\\kappa_1-1)} \\right) = \\sqrt{2\\pi\\kappa_1 s_1} \\exp\\left( \\frac{\\delta^2}{2s_1(\\kappa_1-1)} \\right)\n$$\nIf $\\delta \\ne 0$, $\\alpha_1$ is finite only if $\\kappa_1 > 1$. If $\\kappa_1 \\le 1$, the exponent is either undefined or its quadratic term is non-negative, leading to an unbounded ratio.\nA special case arises when $\\kappa_1=1$ and $\\delta=0$. The proposal component has the same shape and mean as the target component. The ratio becomes constant:\n$$\n\\alpha_1 = \\frac{e^{-x^2/(2s_1)}}{\\frac{1}{\\sqrt{2\\pi s_1}} e^{-x^2/(2s_1)}} = \\sqrt{2\\pi s_1}\n$$\n\n**Bound for the second component ($\\alpha_2$):**\nWe require $p_2(x) \\le \\alpha_2 \\varphi(x;5, \\tau_2^2)$. This is $0.01\\,e^{-(x-5)^2/(2s_2)} \\le \\alpha_2 \\varphi(x;5, \\tau_2^2)$. The means are aligned, equivalent to the case $\\delta=0$.\n$$\n\\alpha_2 = \\max_{x \\in \\mathbb{R}} \\frac{0.01\\,e^{-(x-5)^2/(2s_2)}}{\\frac{1}{\\sqrt{2\\pi}\\tau_2} e^{-(x-5)^2/(2\\tau_2^2)}} = 0.01\\sqrt{2\\pi}\\tau_2 \\max_{x \\in \\mathbb{R}} \\exp\\left( -\\frac{(x-5)^2}{2} \\left(\\frac{1}{s_2}-\\frac{1}{\\tau_2^2}\\right) \\right)\n$$\nSince $\\tau_2^2 = \\kappa_2 s_2$ and $\\kappa_2 \\ge 1$, we have $\\tau_2^2 \\ge s_2$, so $\\frac{1}{s_2}-\\frac{1}{\\tau_2^2} \\ge 0$. The exponent is non-positive, and its maximum value is $0$ at $x=5$. Therefore, the maximum of the exponential term is $1$.\n$$\n\\alpha_2 = 0.01 \\sqrt{2\\pi}\\tau_2 = 0.01 \\sqrt{2\\pi \\kappa_2 s_2}\n$$\n\n### Task 2: Construction of Global Bound M\nWe have $p(x) = p_1(x) + p_2(x)$. Using the component-wise bounds derived above:\n$$\np(x) \\le \\alpha_1 \\varphi(x;\\delta, \\tau_1^2) + \\alpha_2 \\varphi(x;5, \\tau_2^2)\n$$\nWe need to find the smallest $M$ such that $p(x) \\le M q(x)$, which is:\n$$\n\\alpha_1 \\varphi(x;\\delta, \\tau_1^2) + \\alpha_2 \\varphi(x;5, \\tau_2^2) \\le M \\left( c_1 \\varphi(x;\\delta, \\tau_1^2) + c_2 \\varphi(x;5, \\tau_2^2) \\right)\n$$\nRearranging the terms:\n$$\n(M c_1 - \\alpha_1) \\varphi(x;\\delta, \\tau_1^2) + (M c_2 - \\alpha_2) \\varphi(x;5, \\tau_2^2) \\ge 0\n$$\nSince $\\varphi(\\cdot)$ functions are PDFs, they are non-negative for all $x$. A sufficient condition for this inequality to hold is that the coefficients are non-negative. This requires:\n$$\nM c_1 - \\alpha_1 \\ge 0 \\implies M \\ge \\frac{\\alpha_1}{c_1} \\quad (\\text{for } c_1 > 0)\n$$\n$$\nM c_2 - \\alpha_2 \\ge 0 \\implies M \\ge \\frac{\\alpha_2}{c_2} \\quad (\\text{for } c_2 > 0)\n$$\nTo satisfy both conditions simultaneously, $M$ must be greater than or equal to the maximum of the two required lower bounds. The smallest such $M$ is:\n$$\nM = \\max\\left(\\frac{\\alpha_1}{c_1}, \\frac{\\alpha_2}{c_2}\\right)\n$$\n\n### Task 3: Derivation of Acceptance Probability\nThe fundamental rule of rejection sampling states that a proposed sample $X \\sim q(x)$ is accepted with probability $P(\\text{accept}|X=x) = \\frac{p_{norm}(x)}{M q(x)}$, where $p_{norm}(x)$ is the normalized target density. If we work with an unnormalized density $p(x)$, the acceptance probability is $P(\\text{accept}|X=x) = \\frac{p(x)}{M q(x)}$.\nThe overall expected acceptance probability, $P_A$, is the expectation of this conditional probability over the proposal distribution $q(x)$:\n$$\nP_A = E_{X \\sim q}\\left[\\frac{p(X)}{M q(X)}\\right] = \\int_{-\\infty}^{\\infty} \\frac{p(x)}{M q(x)} q(x) dx = \\frac{1}{M} \\int_{-\\infty}^{\\infty} p(x) dx\n$$\nWe must evaluate the integral of the unnormalized density, $I_p = \\int_{-\\infty}^{\\infty} p(x) dx$:\n$$\nI_p = \\int_{-\\infty}^{\\infty} \\left( e^{-x^2/(2s_1)} + 0.01\\,e^{-(x-5)^2/(2s_2)} \\right) dx\n$$\nUsing the standard Gaussian integral result $\\int_{-\\infty}^{\\infty} e^{-(x-\\mu)^2/(2\\sigma^2)} dx = \\sqrt{2\\pi\\sigma^2}$:\n$$\nI_p = \\sqrt{2\\pi s_1} + 0.01\\sqrt{2\\pi s_2}\n$$\nThe expected acceptance probability is therefore:\n$$\nP_A = \\frac{I_p}{M} = \\frac{\\sqrt{2\\pi s_1} + 0.01\\sqrt{2\\pi s_2}}{\\max\\left(\\frac{\\alpha_1}{c_1}, \\frac{\\alpha_2}{1-c_1}\\right)}\n$$\n\n### Task 4: Sensitivity Analysis\nFor the sensitivity analysis, we use the derived formula for $P_A$ with the fixed parameters $s_1=0.005$, $s_2=0.5$, $\\kappa_2=1$, $\\mu_2=5$ and the test values for $(\\delta, \\kappa_1, c_1)$.\nPlugging in the fixed parameters simplifies the expressions for $I_p$ and $\\alpha_2$:\n$$\nI_p = \\sqrt{2\\pi (0.005)} + 0.01\\sqrt{2\\pi (0.5)} = \\sqrt{0.01\\pi} + 0.01\\sqrt{\\pi} = (0.1+0.01)\\sqrt{\\pi} = 0.11\\sqrt{\\pi}\n$$\n$$\n\\alpha_2 = 0.01 \\sqrt{2\\pi (1) (0.5)} = 0.01\\sqrt{\\pi}\n$$\nThe acceptance rate is highly sensitive to the parameters $\\delta$ and $\\kappa_1$.\n- **Dependence on misalignment $\\delta$**: As $\\delta$ increases from $0$, the term $\\exp\\left( \\frac{\\delta^2}{2s_1(\\kappa_1-1)} \\right)$ grows exponentially, causing $\\alpha_1$ and hence $M$ to increase rapidly. This leads to a dramatic decrease in the acceptance probability. This highlights the inefficiency of rejection sampling when the proposal is poorly centered on a sharp target mode.\n- **Dependence on variance inflation $\\kappa_1$**: The role of $\\kappa_1$ is more nuanced. For a fixed non-zero $\\delta$, as $\\kappa_1 \\to 1^+$, the exponent in $\\alpha_1$ goes to $+\\infty$, making the sampler extremely inefficient (e.g., test case 8). This is because a proposal that is just as narrow as the target cannot effectively cover any misalignment. Increasing $\\kappa_1$ (i.e., using a \"fatter\" proposal) mitigates the effect of misalignment $\\delta$. However, a very large $\\kappa_1$ also increases the baseline factor $\\sqrt{\\kappa_1}$ in $\\alpha_1$, reducing efficiency even when $\\delta=0$. For any given $\\delta \\ne 0$, there exists an optimal $\\kappa_1 > 1$ that minimizes $\\alpha_1$ and maximizes efficiency. The perfect alignment case ($\\delta=0, \\kappa_1=1$) yields the highest possible acceptance rate, which becomes $1$ if the mixture weights $c_1, c_2$ are chosen optimally to balance the modes.\n\nThe Python implementation will compute the precise acceptance rates for the test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected acceptance rate for a rejection sampler\n    with a Gaussian mixture proposal for a bimodal target distribution.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (delta, kappa_1, c_1)\n    test_cases = [\n        (0.0, 1.0, 10/11),\n        (0.02, 1.5, 10/11),\n        (0.05, 1.5, 10/11),\n        (0.10, 1.5, 10/11),\n        (0.20, 2.0, 10/11),\n        (0.50, 5.0, 10/11),\n        (0.00, 1.0, 0.70),\n        (0.05, 1.01, 10/11),\n    ]\n\n    # Fixed parameters from the problem\n    s1 = 0.005\n    s2 = 0.5\n    kappa2 = 1.0\n    w2 = 0.01\n\n    # Calculate the integral of the unnormalized target density p(x)\n    # I_p = Integral(p1(x)) + Integral(p2(x))\n    # Integral(e^(-x^2/(2*s1))) = sqrt(2*pi*s1)\n    # Integral(w2 * e^(-(x-5)^2/(2*s2))) = w2 * sqrt(2*pi*s2)\n    integral_p = np.sqrt(2 * np.pi * s1) + w2 * np.sqrt(2 * np.pi * s2)\n\n    # Calculate the component-wise bound alpha_2, which is constant for all test cases\n    # alpha_2 = w2 * sqrt(2*pi*tau_2^2) = w2 * sqrt(2*pi*kappa_2*s2)\n    alpha_2 = w2 * np.sqrt(2 * np.pi * kappa2 * s2)\n\n    results = []\n    for case in test_cases:\n        delta, kappa1, c1 = case\n        c2 = 1.0 - c1\n\n        # Calculate the component-wise bound alpha_1\n        # This depends on delta and kappa1\n        if np.isclose(kappa1, 1.0):\n            if np.isclose(delta, 0.0):\n                # Special case: perfect alignment in shape and location\n                # alpha_1 = sqrt(2*pi*s1)\n                alpha_1 = np.sqrt(2 * np.pi * s1)\n            else:\n                # Misalignment with equal variance, bound is infinite\n                alpha_1 = np.inf\n        elif kappa1 > 1.0:\n            # General case for kappa1 > 1\n            # alpha_1 = sqrt(2*pi*kappa1*s1) * exp(delta^2 / (2*s1*(kappa1-1)))\n            exponent = delta**2 / (2 * s1 * (kappa1 - 1))\n            alpha_1 = np.sqrt(2 * np.pi * kappa1 * s1) * np.exp(exponent)\n        else:\n            # kappa1  1, which is not allowed by the problem statement but is invalid\n            alpha_1 = np.inf\n            \n        # Calculate the global bound M\n        # M = max(alpha_1/c_1, alpha_2/c_2)\n        if c1 == 0 or c2 == 0:\n             # This scenario would lead to an infinite M if not handled\n             M = np.inf\n        else:\n            M = max(alpha_1 / c1, alpha_2 / c2)\n\n        # Calculate the expected acceptance probability\n        # P_A = integral_p / M\n        if M == np.inf:\n            acceptance_rate = 0.0\n        else:\n            acceptance_rate = integral_p / M\n        \n        results.append(f\"{acceptance_rate:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}