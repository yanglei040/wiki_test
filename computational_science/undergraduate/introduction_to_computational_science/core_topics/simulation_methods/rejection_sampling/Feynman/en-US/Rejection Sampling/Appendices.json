{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's ground our understanding of rejection sampling with a fundamental calculation. The efficiency of any rejection sampler is determined by its acceptance probability, which hinges on how tightly the proposal distribution, $g(x)$, envelops the target distribution, $f(x)$. This first exercise guides you through calculating this theoretical probability for a common scenario: sampling from a Beta distribution using a simple uniform proposal. By finding the optimal constant $M$ and using it to find the acceptance rate, you'll practice the core mechanical steps of setting up a rejection sampler.",
            "id": "1387131",
            "problem": "A data scientist is implementing a rejection sampling algorithm to generate random numbers that follow a specific target distribution. The target distribution is a Beta distribution with parameters $\\alpha = 2$ and $\\beta = 2$. The probability density function (PDF) of this target distribution, denoted by $f(x)$, is given by:\n$$f(x) = 6x(1-x) \\quad \\text{for } x \\in [0, 1]$$\nand $f(x) = 0$ otherwise.\n\nFor the proposal distribution, the data scientist uses the standard uniform distribution, U(0, 1). The PDF of this proposal distribution, denoted by $g(x)$, is:\n$$g(x) = 1 \\quad \\text{for } x \\in [0, 1]$$\nand $g(x) = 0$ otherwise.\n\nThe rejection sampling method requires finding a constant $M$ such that the inequality $f(x) \\le M g(x)$ holds for all possible values of $x$. To maximize the efficiency of the sampling process, the smallest possible value of $M$ is chosen. The algorithm then generates a sample $Y$ from the proposal distribution $g(x)$ and accepts it with probability $\\frac{f(Y)}{M g(Y)}$.\n\nCalculate the overall theoretical probability that a sample drawn from the proposal distribution is accepted. Express your answer as an exact fraction.",
            "solution": "We need the smallest constant $M$ such that $f(x) \\le M g(x)$ for all $x$. Since $g(x)=1$ for $x \\in [0,1]$ and $g(x)=0$ otherwise, the condition reduces on $[0,1]$ to $f(x) \\le M$. Therefore the minimal admissible $M$ is the supremum of $f(x)$ on $[0,1]$.\n\nCompute the maximum of $f(x)=6x(1-x)=6(x - x^{2})$ on $[0,1]$. Differentiate:\n$$\nf'(x)=6(1-2x), \\quad f''(x)=-12.\n$$\nSetting $f'(x)=0$ gives $x=\\frac{1}{2}$. Since $f''(x)<0$, this is a maximum. Evaluate\n$$\nM=\\max_{x \\in [0,1]} f(x)=f\\!\\left(\\tfrac{1}{2}\\right)=6 \\cdot \\tfrac{1}{2} \\cdot \\left(1-\\tfrac{1}{2}\\right)=\\frac{3}{2}.\n$$\n\nGiven $Y \\sim g$, the acceptance probability conditional on $Y=y$ is $\\frac{f(y)}{M g(y)}$. The overall acceptance probability is the expectation under $g$:\n$$\n\\mathbb{P}(\\text{accept})=\\mathbb{E}_{g}\\!\\left[\\frac{f(Y)}{M g(Y)}\\right]=\\int_{0}^{1} \\frac{f(x)}{M g(x)} g(x) \\, dx=\\frac{1}{M} \\int_{0}^{1} f(x) \\, dx.\n$$\nSince $f$ is a probability density on $[0,1]$, $\\int_{0}^{1} f(x) \\, dx=1$. For completeness:\n$$\n\\int_{0}^{1} 6x(1-x)\\,dx=6 \\int_{0}^{1} \\left(x - x^{2}\\right) dx=6 \\left(\\frac{1}{2}-\\frac{1}{3}\\right)=1.\n$$\nTherefore,\n$$\n\\mathbb{P}(\\text{accept})=\\frac{1}{M}=\\frac{1}{\\frac{3}{2}}=\\frac{2}{3}.\n$$",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        },
        {
            "introduction": "Once we know how to build a sampler, we must ask: does it produce the right answer? The goal of rejection sampling is to generate samples that correctly follow the target distribution. This practice addresses this crucial point by asking you to derive the expected value of an accepted sample. By working through this problem, you will demonstrate a cornerstone principle of the methodâ€”that the distribution of the accepted samples is, in fact, the target distribution $p(x)$ we aimed for.",
            "id": "832348",
            "problem": "Rejection sampling is a Monte Carlo method for generating samples from a target probability distribution $p(x)$ using a simpler proposal distribution $q(x)$. The method requires finding a constant $M$ such that $p(x) \\leq M q(x)$ for all $x$. The algorithm proceeds as follows:\n1. Draw a sample $x_i$ from the proposal distribution $q(x)$.\n2. Draw a uniform random number $u_i$ from $U[0, 1]$.\n3. If $u_i \\leq \\frac{p(x_i)}{M q(x_i)}$, the sample $x_i$ is accepted. Otherwise, it is rejected.\n\nConsider a target probability density function $p(x)$ which is proportional to $x$ over the interval $[0, a]$ (where $a > 0$), and zero elsewhere. The proposal distribution $q(x)$ is the uniform distribution over the same interval $[0, a]$.\n\nDerive the expected value of a random variable $X$ representing a sample, given that the sample has been accepted by this rejection sampling process. Express your answer in terms of the parameter $a$.",
            "solution": "We have a target density \n$$p(x)=k\\,x\\quad(0\\le x\\le a),\\qquad q(x)=\\frac1a\\quad(0\\le x\\le a).$$ \nNormalization of $p(x)$ gives\n$$1=\\int_0^a k\\,x\\,dx=k\\frac{a^2}{2}\\quad\\Longrightarrow\\quad k=\\frac{2}{a^2}.$$\nHence\n$$p(x)=\\frac{2x}{a^2},\\quad q(x)=\\frac1a.$$\nCompute the envelope constant\n$$\\frac{p(x)}{q(x)}=\\frac{2x/a^2}{1/a}=\\frac{2x}{a},\\qquad M=\\max_{0\\le x\\le a}\\frac{2x}{a}=2.$$\nFor a draw $x$ the acceptance probability is\n$$\\frac{p(x)}{M\\,q(x)}=\\frac{2x/a^2}{2\\,(1/a)}=\\frac{x}{a}.$$\nThe overall acceptance rate is\n$$P_{\\rm acc}=\\int_0^a q(x)\\,\\frac{p(x)}{M\\,q(x)}\\,dx\n=\\frac{1}{M}\\int_0^a p(x)\\,dx=\\frac{1}{2}.$$\nThe PDF of accepted samples is\n$$f_{\\rm acc}(x)\n=\\frac{q(x)\\,\\frac{p(x)}{M\\,q(x)}}{P_{\\rm acc}}\n=\\frac{p(x)}{M\\,P_{\\rm acc}}\n=p(x).$$\nTherefore\n$$\\mathbb{E}[X\\mid\\text{accepted}]\n=\\int_0^a x\\,p(x)\\,dx\n=\\frac{2}{a^2}\\int_0^a x^2\\,dx\n=\\frac{2}{a^2}\\frac{a^3}{3}\n=\\frac{2a}{3}.$$",
            "answer": "$$\\boxed{\\frac{2a}{3}}$$"
        },
        {
            "introduction": "While uniform proposals are excellent for simple, bounded targets, real-world scientific problems often present complex distributions with features like sharp peaks and multiple modes. This advanced practice moves into the realm of practical sampler design. You will construct a sophisticated proposal distribution, a Gaussian mixture, to efficiently sample from a challenging bimodal target, analyzing how factors like misalignment and variance affect the sampler's performance. This exercise illuminates the art of crafting an effective proposal distribution, a key skill in computational science.",
            "id": "3186739",
            "problem": "You are asked to construct a provably valid rejection sampler for an unnormalized one-dimensional target density defined by\n$$\np(x) \\propto e^{-x^2/0.01} + 0.01\\,e^{-(x-5)^2}.\n$$\nThe target has a very narrow spike near $x=0$ and a broader bump near $x=5$. Your goal is to design a proposal density $q(x)$ and a global bound $M$ such that $p(x) \\le M\\,q(x)$ for all real $x$, and to analyze how the expected acceptance rate depends on a misalignment parameter describing the displacement of the narrow spike.\n\nStarting from the fundamental definition of rejection sampling, use only core definitions and well-tested facts. Specifically:\n- Rejection sampling draws $X \\sim q(x)$ and $U \\sim \\text{Uniform}[0,1]$, and accepts $X$ if $U \\le \\frac{p(X)}{M q(X)}$.\n- If $p(x)$ is unnormalized, only the shape matters for acceptance, and the expected acceptance probability can be expressed in terms of the integral of $p(x)$ and the bound $M$.\n\nDesign the proposal as a two-component Gaussian mixture\n$$\nq(x) = c_1\\,\\varphi(x;\\mu_1,\\tau_1^2) + c_2\\,\\varphi(x;\\mu_2,\\tau_2^2),\n$$\nwhere $c_1,c_2 \\ge 0$, $c_1+c_2=1$, and $\\varphi(x;\\mu,\\tau^2)$ denotes the normalized Gaussian probability density function with mean $\\mu$ and variance $\\tau^2$. Use the following structure:\n- For the narrow target spike, set the target component variance parameter $s_1=0.005$ so that $e^{-x^2/0.01} = \\exp\\!\\left(-\\frac{x^2}{2 s_1}\\right)$. Choose a proposal component $\\varphi(x;\\mu_1,\\tau_1^2)$ with variance inflation factor $\\kappa_1 \\ge 1$ so that $\\tau_1^2 = \\kappa_1 s_1$, and a misalignment parameter $\\delta$ so that $\\mu_1 = \\delta$.\n- For the broad target bump, set $s_2=0.5$ so that $e^{-(x-5)^2} = \\exp\\!\\left(-\\frac{(x-5)^2}{2 s_2}\\right)$. Choose a proposal component $\\varphi(x;\\mu_2,\\tau_2^2)$ with $\\mu_2 = 5$ and variance inflation factor $\\kappa_2 \\ge 1$ so that $\\tau_2^2 = \\kappa_2 s_2$.\n- Use mixture weights $c_1$ and $c_2=1-c_1$.\n\nTasks:\n1. For each target component, derive the smallest constant $\\alpha_i$ such that the componentwise dominance holds for all real $x$:\n   $$\n   e^{-\\frac{x^2}{2 s_1}} \\le \\alpha_1\\,\\varphi(x;\\delta,\\tau_1^2),\\qquad 0.01\\,e^{-\\frac{(x-5)^2}{2 s_2}} \\le \\alpha_2\\,\\varphi(x;5,\\tau_2^2).\n   $$\n   Express $\\alpha_1$ and $\\alpha_2$ in closed form in terms of $(s_1,\\tau_1^2,\\delta)$ and $(s_2,\\tau_2^2)$, respectively. Determine the conditions under which $\\alpha_1$ is finite when $\\delta \\ne 0$.\n2. Combine the componentwise bounds into a single global bound $M$ such that $p(x) \\le M\\,q(x)$ for all real $x$. Justify your construction using only algebraic inequalities and properties of nonnegative functions, without numerical maximization over $x$.\n3. Using the definitions of rejection sampling and the fact that $q(x)$ is a probability density function, derive a formula for the expected acceptance probability in terms of $M$ and the integral of $p(x)$ over the real line. Evaluate this integral exactly.\n4. Sensitivity analysis: With $s_1=0.005$ and $s_2=0.5$ fixed, take $\\kappa_2=1$ and $\\mu_2=5$. For the test suite below, compute the expected acceptance rate for each parameter set. Discuss how the acceptance rate depends on the misalignment $\\delta$ and on the variance inflation $\\kappa_1$.\n\nRequired test suite. Each tuple is $(\\delta,\\kappa_1,c_1)$ with $c_2=1-c_1$ and $\\kappa_2=1$:\n- $(0.0,\\,1.0,\\,10/11)$\n- $(0.02,\\,1.5,\\,10/11)$\n- $(0.05,\\,1.5,\\,10/11)$\n- $(0.10,\\,1.5,\\,10/11)$\n- $(0.20,\\,2.0,\\,10/11)$\n- $(0.50,\\,5.0,\\,10/11)$\n- $(0.00,\\,1.0,\\,0.70)$\n- $(0.05,\\,1.01,\\,10/11)$\n\nNotes and constraints:\n- Ensure that your design and derivation start from core definitions and widely accepted facts only. Do not use any shortcut formulas not derived from these bases.\n- Angle units are not applicable. No physical units are involved.\n- Your program must compute the expected acceptance rates for all test cases using your derived formulas and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places, for example: \"[r1,r2,...]\".\n- All intermediate reasoning should be mathematically and logically sound for all real $x$, and your $M$ must be valid globally (not only on a truncated interval).",
            "solution": "Let the unnormalized target density be denoted by $p(x)$, where\n$$\np(x) = e^{-x^2/0.01} + 0.01\\,e^{-(x-5)^2}\n$$\nThis can be written as a sum of two components, $p(x) = p_1(x) + p_2(x)$, where $p_1(x) = e^{-x^2/(2s_1)}$ and $p_2(x) = w_2 e^{-(x-5)^2/(2s_2)}$ with $s_1=0.005$, $s_2=0.5$, and $w_2=0.01$.\n\nThe proposal density is a two-component Gaussian mixture:\n$$\nq(x) = c_1\\,\\varphi(x;\\mu_1,\\tau_1^2) + c_2\\,\\varphi(x;\\mu_2,\\tau_2^2)\n$$\nwhere $\\varphi(x;\\mu,\\tau^2) = \\frac{1}{\\sqrt{2\\pi}\\tau} e^{-\\frac{(x-\\mu)^2}{2\\tau^2}}$ is the probability density function (PDF) of a normal distribution with mean $\\mu$ and variance $\\tau^2$. The parameters are defined as $\\mu_1=\\delta$, $\\tau_1^2=\\kappa_1 s_1$ for the first component, and $\\mu_2=5$, $\\tau_2^2=\\kappa_2 s_2$ for the second. The mixture weights satisfy $c_1, c_2 \\ge 0$ and $c_1+c_2=1$.\n\nThe core principle of rejection sampling requires finding a constant $M$ such that $p(x) \\le M q(x)$ for all real $x$.\n\n### Task 1: Derivation of Component-wise Bounds\nWe seek the smallest constants $\\alpha_1$ and $\\alpha_2$ satisfying the component-wise inequalities.\n\n**Bound for the first component ($\\alpha_1$):**\nWe need to find the minimum $\\alpha_1$ such that $p_1(x) \\le \\alpha_1 \\varphi(x;\\delta, \\tau_1^2)$ for all $x$. This is equivalent to finding the maximum of the ratio:\n$$\n\\alpha_1 = \\max_{x \\in \\mathbb{R}} \\frac{p_1(x)}{\\varphi(x;\\delta, \\tau_1^2)} = \\max_{x \\in \\mathbb{R}} \\frac{e^{-x^2/(2s_1)}}{\\frac{1}{\\sqrt{2\\pi}\\tau_1} e^{-(x-\\delta)^2/(2\\tau_1^2)}} = \\sqrt{2\\pi}\\tau_1 \\max_{x \\in \\mathbb{R}} \\exp\\left( \\frac{(x-\\delta)^2}{2\\tau_1^2} - \\frac{x^2}{2s_1} \\right)\n$$\nLet's analyze the exponent, $E_1(x) = \\frac{(x-\\delta)^2}{2\\tau_1^2} - \\frac{x^2}{2s_1}$:\n$$\nE_1(x) = \\left(\\frac{1}{2\\tau_1^2} - \\frac{1}{2s_1}\\right)x^2 - \\frac{\\delta}{\\tau_1^2}x + \\frac{\\delta^2}{2\\tau_1^2}\n$$\nThis is a quadratic function of $x$. For it to have a finite maximum, the coefficient of $x^2$ must be negative.\n$$\n\\frac{1}{2\\tau_1^2} - \\frac{1}{2s_1} < 0 \\implies \\tau_1^2 > s_1\n$$\nUsing the definition $\\tau_1^2 = \\kappa_1 s_1$, this condition becomes $\\kappa_1 s_1 > s_1$, which implies $\\kappa_1 > 1$. When this holds, the maximum of the quadratic $Ax^2+Bx+C$ is $C - B^2/(4A)$. The maximum value of the exponent is $E_{1,max} = \\frac{\\delta^2}{2(\\tau_1^2-s_1)}$.\nSubstituting $\\tau_1^2 = \\kappa_1 s_1$, we get $E_{1,max} = \\frac{\\delta^2}{2s_1(\\kappa_1-1)}$. Thus, for $\\kappa_1 > 1$:\n$$\n\\alpha_1 = \\sqrt{2\\pi\\tau_1^2} \\exp\\left( \\frac{\\delta^2}{2s_1(\\kappa_1-1)} \\right) = \\sqrt{2\\pi\\kappa_1 s_1} \\exp\\left( \\frac{\\delta^2}{2s_1(\\kappa_1-1)} \\right)\n$$\nIf $\\delta \\ne 0$, $\\alpha_1$ is finite only if $\\kappa_1 > 1$. If $\\kappa_1 \\le 1$, the exponent is either undefined or its quadratic term is non-negative, leading to an unbounded ratio.\nA special case arises when $\\kappa_1=1$ and $\\delta=0$. The proposal component has the same shape and mean as the target component. The ratio becomes constant:\n$$\n\\alpha_1 = \\frac{e^{-x^2/(2s_1)}}{\\frac{1}{\\sqrt{2\\pi s_1}} e^{-x^2/(2s_1)}} = \\sqrt{2\\pi s_1}\n$$\n\n**Bound for the second component ($\\alpha_2$):**\nWe require $p_2(x) \\le \\alpha_2 \\varphi(x;5, \\tau_2^2)$. This is $0.01\\,e^{-(x-5)^2/(2s_2)} \\le \\alpha_2 \\varphi(x;5, \\tau_2^2)$. The means are aligned, equivalent to the case $\\delta=0$.\n$$\n\\alpha_2 = \\max_{x \\in \\mathbb{R}} \\frac{0.01\\,e^{-(x-5)^2/(2s_2)}}{\\frac{1}{\\sqrt{2\\pi}\\tau_2} e^{-(x-5)^2/(2\\tau_2^2)}} = 0.01\\sqrt{2\\pi}\\tau_2 \\max_{x \\in \\mathbb{R}} \\exp\\left( -\\frac{(x-5)^2}{2} \\left(\\frac{1}{s_2}-\\frac{1}{\\tau_2^2}\\right) \\right)\n$$\nSince $\\tau_2^2 = \\kappa_2 s_2$ and $\\kappa_2 \\ge 1$, we have $\\tau_2^2 \\ge s_2$, so $\\frac{1}{s_2}-\\frac{1}{\\tau_2^2} \\ge 0$. The exponent is non-positive, and its maximum value is $0$ at $x=5$. Therefore, the maximum of the exponential term is $1$.\n$$\n\\alpha_2 = 0.01 \\sqrt{2\\pi}\\tau_2 = 0.01 \\sqrt{2\\pi \\kappa_2 s_2}\n$$\n\n### Task 2: Construction of Global Bound M\nWe have $p(x) = p_1(x) + p_2(x)$. Using the component-wise bounds derived above:\n$$\np(x) \\le \\alpha_1 \\varphi(x;\\delta, \\tau_1^2) + \\alpha_2 \\varphi(x;5, \\tau_2^2)\n$$\nWe need to find the smallest $M$ such that $p(x) \\le M q(x)$, which is:\n$$\n\\alpha_1 \\varphi(x;\\delta, \\tau_1^2) + \\alpha_2 \\varphi(x;5, \\tau_2^2) \\le M \\left( c_1 \\varphi(x;\\delta, \\tau_1^2) + c_2 \\varphi(x;5, \\tau_2^2) \\right)\n$$\nRearranging the terms:\n$$\n(M c_1 - \\alpha_1) \\varphi(x;\\delta, \\tau_1^2) + (M c_2 - \\alpha_2) \\varphi(x;5, \\tau_2^2) \\ge 0\n$$\nSince $\\varphi(\\cdot)$ functions are PDFs, they are non-negative for all $x$. A sufficient condition for this inequality to hold is that the coefficients are non-negative. This requires:\n$$\nM c_1 - \\alpha_1 \\ge 0 \\implies M \\ge \\frac{\\alpha_1}{c_1} \\quad (\\text{for } c_1 > 0)\n$$\n$$\nM c_2 - \\alpha_2 \\ge 0 \\implies M \\ge \\frac{\\alpha_2}{c_2} \\quad (\\text{for } c_2 > 0)\n$$\nTo satisfy both conditions simultaneously, $M$ must be greater than or equal to the maximum of the two required lower bounds. The smallest such $M$ is:\n$$\nM = \\max\\left(\\frac{\\alpha_1}{c_1}, \\frac{\\alpha_2}{c_2}\\right)\n$$\n\n### Task 3: Derivation of Acceptance Probability\nThe overall expected acceptance probability, $P_A$, is the expectation of the conditional probability $P(\\text{accept}|X=x) = \\frac{p(x)}{M q(x)}$ over the proposal distribution $q(x)$:\n$$\nP_A = E_{X \\sim q}\\left[\\frac{p(X)}{M q(X)}\\right] = \\int_{-\\infty}^{\\infty} \\frac{p(x)}{M q(x)} q(x) dx = \\frac{1}{M} \\int_{-\\infty}^{\\infty} p(x) dx\n$$\nWe must evaluate the integral of the unnormalized density, $I_p = \\int_{-\\infty}^{\\infty} p(x) dx$:\n$$\nI_p = \\int_{-\\infty}^{\\infty} \\left( e^{-x^2/(2s_1)} + 0.01\\,e^{-(x-5)^2/(2s_2)} \\right) dx\n$$\nUsing the standard Gaussian integral result $\\int_{-\\infty}^{\\infty} e^{-(x-\\mu)^2/(2\\sigma^2)} dx = \\sqrt{2\\pi\\sigma^2}$:\n$$\nI_p = \\sqrt{2\\pi s_1} + 0.01\\sqrt{2\\pi s_2}\n$$\nThe expected acceptance probability is therefore:\n$$\nP_A = \\frac{I_p}{M} = \\frac{\\sqrt{2\\pi s_1} + 0.01\\sqrt{2\\pi s_2}}{\\max\\left(\\frac{\\alpha_1}{c_1}, \\frac{\\alpha_2}{1-c_1}\\right)}\n$$\n\n### Task 4: Sensitivity Analysis\nFor the sensitivity analysis, we use the derived formula for $P_A$ with the fixed parameters $s_1=0.005$, $s_2=0.5$, $\\kappa_2=1$, $\\mu_2=5$ and the test values for $(\\delta, \\kappa_1, c_1)$.\nPlugging in the fixed parameters simplifies the expressions for $I_p$ and $\\alpha_2$:\n$$\nI_p = \\sqrt{2\\pi (0.005)} + 0.01\\sqrt{2\\pi (0.5)} = \\sqrt{0.01\\pi} + 0.01\\sqrt{\\pi} = (0.1+0.01)\\sqrt{\\pi} = 0.11\\sqrt{\\pi}\n$$\n$$\n\\alpha_2 = 0.01 \\sqrt{2\\pi (1) (0.5)} = 0.01\\sqrt{\\pi}\n$$\nThe acceptance rate is highly sensitive to the parameters $\\delta$ and $\\kappa_1$.\n- **Dependence on misalignment $\\delta$**: As $\\delta$ increases from $0$, the term $\\exp\\left( \\frac{\\delta^2}{2s_1(\\kappa_1-1)} \\right)$ grows exponentially, causing $\\alpha_1$ and hence $M$ to increase rapidly. This leads to a dramatic decrease in the acceptance probability. This highlights the inefficiency of rejection sampling when the proposal is poorly centered on a sharp target mode.\n- **Dependence on variance inflation $\\kappa_1$**: The role of $\\kappa_1$ is more nuanced. For a fixed non-zero $\\delta$, as $\\kappa_1 \\to 1^+$, the exponent in $\\alpha_1$ goes to $+\\infty$, making the sampler extremely inefficient (e.g., test case 8). This is because a proposal that is just as narrow as the target cannot effectively cover any misalignment. Increasing $\\kappa_1$ (i.e., using a \"fatter\" proposal) mitigates the effect of misalignment $\\delta$. However, a very large $\\kappa_1$ also increases the baseline factor $\\sqrt{\\kappa_1}$ in $\\alpha_1$, reducing efficiency even when $\\delta=0$. For any given $\\delta \\ne 0$, there exists an optimal $\\kappa_1 > 1$ that minimizes $\\alpha_1$ and maximizes efficiency. The perfect alignment case ($\\delta=0, \\kappa_1=1$) yields the highest possible acceptance rate, which becomes $1$ if the mixture weights $c_1, c_2$ are chosen optimally to balance the modes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected acceptance rate for a rejection sampler\n    with a Gaussian mixture proposal for a bimodal target distribution.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (delta, kappa_1, c_1)\n    test_cases = [\n        (0.0, 1.0, 10/11),\n        (0.02, 1.5, 10/11),\n        (0.05, 1.5, 10/11),\n        (0.10, 1.5, 10/11),\n        (0.20, 2.0, 10/11),\n        (0.50, 5.0, 10/11),\n        (0.00, 1.0, 0.70),\n        (0.05, 1.01, 10/11),\n    ]\n\n    # Fixed parameters from the problem\n    s1 = 0.005\n    s2 = 0.5\n    kappa2 = 1.0\n    w2 = 0.01\n\n    # Calculate the integral of the unnormalized target density p(x)\n    # I_p = Integral(p1(x)) + Integral(p2(x))\n    # Integral(e^(-x^2/(2*s1))) = sqrt(2*pi*s1)\n    # Integral(w2 * e^(-(x-5)^2/(2*s2))) = w2 * sqrt(2*pi*s2)\n    integral_p = np.sqrt(2 * np.pi * s1) + w2 * np.sqrt(2 * np.pi * s2)\n\n    # Calculate the component-wise bound alpha_2, which is constant for all test cases\n    # alpha_2 = w2 * sqrt(2*pi*tau_2^2) = w2 * sqrt(2*pi*kappa_2*s2)\n    alpha_2 = w2 * np.sqrt(2 * np.pi * kappa2 * s2)\n\n    results = []\n    for case in test_cases:\n        delta, kappa1, c1 = case\n        c2 = 1.0 - c1\n\n        # Calculate the component-wise bound alpha_1\n        # This depends on delta and kappa1\n        if np.isclose(kappa1, 1.0):\n            if np.isclose(delta, 0.0):\n                # Special case: perfect alignment in shape and location\n                # alpha_1 = sqrt(2*pi*s1)\n                alpha_1 = np.sqrt(2 * np.pi * s1)\n            else:\n                # Misalignment with equal variance, bound is infinite\n                alpha_1 = np.inf\n        elif kappa1 > 1.0:\n            # General case for kappa1 > 1\n            # alpha_1 = sqrt(2*pi*kappa1*s1) * exp(delta^2 / (2*s1*(kappa1-1)))\n            exponent = delta**2 / (2 * s1 * (kappa1 - 1))\n            alpha_1 = np.sqrt(2 * np.pi * kappa1 * s1) * np.exp(exponent)\n        else:\n            # kappa1 < 1, which is not allowed by the problem statement but is invalid\n            alpha_1 = np.inf\n            \n        # Calculate the global bound M\n        # M = max(alpha_1/c_1, alpha_2/c_2)\n        if c1 == 0 or c2 == 0:\n             # This scenario would lead to an infinite M if not handled\n             M = np.inf\n        else:\n            M = max(alpha_1 / c1, alpha_2 / c2)\n\n        # Calculate the expected acceptance probability\n        # P_A = integral_p / M\n        if M == np.inf:\n            acceptance_rate = 0.0\n        else:\n            acceptance_rate = integral_p / M\n        \n        results.append(f\"{acceptance_rate:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}