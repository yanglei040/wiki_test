{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how antithetic variates reduce variance, it's essential to work through the mechanics yourself. This first exercise provides a foundational opportunity to do just that by analyzing a simple monotonic function, $f(x) = (1+x)^2$. By analytically calculating the variance of both a standard Monte Carlo estimator and an antithetic estimator, you will derive the exact theoretical improvement and gain a concrete understanding of how negative correlation enhances estimation efficiency .",
            "id": "2188199",
            "problem": "In computational science, Monte Carlo methods are frequently used to approximate the value of a definite integral, $I = \\int_{0}^{1} f(x) \\,dx$. A key aspect of these methods is the reduction of the estimator's variance to improve accuracy.\n\nConsider the integral of the function $f(x) = (1+x)^2$ over the interval $[0, 1]$. We will compare two Monte Carlo estimators for this integral, both based on a total of $N$ function evaluations, where $N$ is an even integer.\n\n1.  **Standard Monte Carlo Estimator ($\\hat{I}_{\\text{std}}$):**\n    This estimator is formed by drawing $N$ independent and identically distributed random numbers $X_1, X_2, \\ldots, X_N$ from a Uniform(0,1) distribution. The estimator is given by:\n    $$ \\hat{I}_{\\text{std}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i) $$\n\n2.  **Antithetic Variates Estimator ($\\hat{I}_{\\text{anti}}$):**\n    This estimator is formed by drawing $M = N/2$ independent random numbers $U_1, U_2, \\ldots, U_M$ from a Uniform(0,1) distribution. For each $U_i$, we form a pair of 'antithetic' samples, $(U_i, 1-U_i)$. The estimator is given by:\n    $$ \\hat{I}_{\\text{anti}} = \\frac{1}{M} \\sum_{i=1}^{M} \\frac{f(U_i) + f(1-U_i)}{2} $$\n\nYour task is to analytically determine the theoretical variance reduction achieved by using antithetic variates for this specific function. Calculate the exact value of the ratio $R = \\frac{\\text{Var}(\\hat{I}_{\\text{anti}})}{\\text{Var}(\\hat{I}_{\\text{std}})}$.",
            "solution": "Let $X\\sim \\text{Uniform}(0,1)$ and $f(x)=(1+x)^{2}=1+2x+x^{2}$. The standard Monte Carlo estimator with $N$ IID samples has variance\n$$\n\\text{Var}(\\hat{I}_{\\text{std}})=\\frac{\\text{Var}(f(X))}{N}.\n$$\nCompute $\\text{Var}(f(X))$ by evaluating $E[f(X)]$ and $E[f(X)^{2}]$. Using $E[X]=\\frac{1}{2}$, $E[X^{2}]=\\frac{1}{3}$, $E[X^{3}]=\\frac{1}{4}$, $E[X^{4}]=\\frac{1}{5}$,\n$$\nE[f(X)]=E[1+2X+X^{2}]=1+2\\cdot \\frac{1}{2}+\\frac{1}{3}=\\frac{7}{3},\n$$\nand\n$$\nf(X)^{2}=(1+2X+X^{2})^{2}=1+4X+6X^{2}+4X^{3}+X^{4},\n$$\nso\n$$\nE[f(X)^{2}]=1+4\\cdot \\frac{1}{2}+6\\cdot \\frac{1}{3}+4\\cdot \\frac{1}{4}+\\frac{1}{5}=\\frac{31}{5}.\n$$\nThus\n$$\n\\text{Var}(f(X))=E[f(X)^{2}]-\\left(E[f(X)]\\right)^{2}=\\frac{31}{5}-\\frac{49}{9}=\\frac{34}{45}.\n$$\n\nFor the antithetic estimator, define $M=\\frac{N}{2}$ and $Y=\\frac{f(U)+f(1-U)}{2}$ with $U\\sim \\text{Uniform}(0,1)$. Then\n$$\n\\text{Var}(\\hat{I}_{\\text{anti}})=\\frac{\\text{Var}(Y)}{M}.\n$$\nUse\n$$\n\\text{Var}(Y)=\\frac{1}{4}\\text{Var}\\big(f(U)+f(1-U)\\big)=\\frac{1}{4}\\left(2\\,\\text{Var}(f(X))+2\\,\\text{Cov}(f(U),f(1-U))\\right)\n=\\frac{1}{2}\\left(\\text{Var}(f(X))+\\text{Cov}(f(U),f(1-U))\\right).\n$$\nTherefore the variance ratio is\n$$\nR=\\frac{\\text{Var}(\\hat{I}_{\\text{anti}})}{\\text{Var}(\\hat{I}_{\\text{std}})}\n=\\frac{\\frac{\\text{Var}(Y)}{M}}{\\frac{\\text{Var}(f(X))}{N}}\n=\\frac{\\text{Var}(Y)}{\\text{Var}(f(X))}\\cdot \\frac{N}{M}\n=\\frac{\\frac{1}{2}\\left(\\text{Var}(f(X))+\\text{Cov}(f(U),f(1-U))\\right)}{\\text{Var}(f(X))}\\cdot 2\n=1+\\frac{\\text{Cov}(f(U),f(1-U))}{\\text{Var}(f(X))}.\n$$\n\nIt remains to compute $\\text{Cov}(f(U),f(1-U))$. Since $f(1-U)=(2-U)^{2}=4-4U+U^{2}$,\n$$\nf(U)f(1-U)=(1+2U+U^{2})(4-4U+U^{2})=4+4U-3U^{2}-2U^{3}+U^{4}.\n$$\nHence\n$$\nE[f(U)f(1-U)]=4+4\\cdot \\frac{1}{2}-3\\cdot \\frac{1}{3}-2\\cdot \\frac{1}{4}+\\frac{1}{5}=\\frac{47}{10},\n$$\nand with $E[f(U)]=\\frac{7}{3}$,\n$$\n\\text{Cov}(f(U),f(1-U))=E[f(U)f(1-U)]-\\left(E[f(U)]\\right)^{2}=\\frac{47}{10}-\\frac{49}{9}=-\\frac{67}{90}.\n$$\nTherefore,\n$$\nR=1+\\frac{-\\frac{67}{90}}{\\frac{34}{45}}=1-\\frac{67}{90}\\cdot \\frac{45}{34}=1-\\frac{67}{68}=\\frac{1}{68}.\n$$\nThus the antithetic variates estimator achieves a variance that is a factor $\\frac{1}{68}$ of the standard Monte Carlo estimator for this $f$.",
            "answer": "$$\\boxed{\\frac{1}{68}}$$"
        },
        {
            "introduction": "While antithetic sampling is a powerful tool, its effectiveness is not universal and depends critically on the structure of the function being integrated. This practice explores this dependency by examining a periodic function, $f(x) = \\sin(k \\pi x)$. You will discover how the function's symmetry with respect to the center of the integration interval can lead to either perfect variance reduction or, counter-intuitively, an increase in variance, providing a crucial lesson on the method's underlying assumptions .",
            "id": "3285865",
            "problem": "Consider estimating the integral $I(k) = \\int_{0}^{1} \\sin(k \\pi x) \\, dx$ for a fixed positive integer $k$ using Monte Carlo (MC) methods with antithetic variates (AV). Let $\\{U_i\\}_{i=1}^{n}$ be independent and identically distributed samples from $\\mathrm{Uniform}(0,1)$, with $n$ even. Define the crude Monte Carlo estimator by $\\widehat{I}_{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^{n} f(U_i)$ with $f(x) = \\sin(k \\pi x)$. Define the antithetic estimator by pairing samples as $Y_i = \\frac{f(U_i) + f(1 - U_i)}{2}$ for $i = 1, \\dots, n/2$, and setting $\\widehat{I}_{\\mathrm{AV}} = \\frac{1}{n/2} \\sum_{i=1}^{n/2} Y_i$. Using only the fundamental definitions of expectation, variance, and covariance, and basic trigonometric identities, derive an exact closed-form expression (in terms of $k$ only) for the variance ratio\n$$\nR(k) = \\frac{\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}})}.\n$$\nExpress your final result as a single simplified analytical expression in $k$. No numerical rounding is required.",
            "solution": "The objective is to compute the ratio $R(k) = \\frac{\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}})}$. We begin by expressing the variances of the two estimators. Let $U$ be a random variable with distribution $\\mathrm{Uniform}(0,1)$ and $f(x) = \\sin(k \\pi x)$.\n\nThe variance of the crude Monte Carlo estimator is $\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}}) = \\frac{1}{n} \\mathrm{Var}(f(U))$. The antithetic estimator is an average of $n/2$ i.i.d. random variables $Y_i = \\frac{f(U_i) + f(1 - U_i)}{2}$. The variance of the antithetic variates estimator is $\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}}) = \\frac{2}{n} \\mathrm{Var}(Y_i)$.\n\nThe variance of a single term $Y$ is:\n$$\n\\mathrm{Var}(Y) = \\mathrm{Var}\\left(\\frac{f(U) + f(1-U)}{2}\\right) = \\frac{1}{4} \\mathrm{Var}(f(U) + f(1-U))\n$$\nUsing the formula for the variance of a sum and noting that $U$ and $1-U$ are identically distributed (so $\\mathrm{Var}(f(U)) = \\mathrm{Var}(f(1-U))$), we get:\n$$\n\\mathrm{Var}(Y) = \\frac{1}{4} \\left( 2\\mathrm{Var}(f(U)) + 2\\mathrm{Cov}(f(U), f(1-U)) \\right) = \\frac{1}{2} \\left( \\mathrm{Var}(f(U)) + \\mathrm{Cov}(f(U), f(1-U)) \\right)\n$$\nThe variance ratio $R(k)$ is therefore:\n$$\nR(k) = \\frac{\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}})} = \\frac{\\frac{2}{n} \\mathrm{Var}(Y)}{\\frac{1}{n} \\mathrm{Var}(f(U))} = \\frac{2\\mathrm{Var}(Y)}{\\mathrm{Var}(f(U))} = 1 + \\frac{\\mathrm{Cov}(f(U), f(1-U))}{\\mathrm{Var}(f(U))}\n$$\nTo evaluate this ratio, we analyze the covariance term using the symmetry of the function. We evaluate $f(1-x)$:\n$$\nf(1-x) = \\sin(k \\pi (1-x)) = \\sin(k \\pi - k \\pi x)\n$$\nUsing the trigonometric identity $\\sin(A-B) = \\sin(A)\\cos(B) - \\cos(A)\\sin(B)$ and noting that for integer $k$, $\\sin(k \\pi) = 0$ and $\\cos(k \\pi) = (-1)^k$:\n$$\nf(1-x) = \\sin(k \\pi)\\cos(k \\pi x) - \\cos(k \\pi)\\sin(k \\pi x) = -(-1)^k f(x)\n$$\nThis pivotal relationship implies that $f(1-U) = -(-1)^k f(U)$. We can now analyze the covariance:\n$$\n\\mathrm{Cov}(f(U), f(1-U)) = \\mathrm{Cov}(f(U), -(-1)^k f(U)) = -(-1)^k \\mathrm{Cov}(f(U), f(U)) = -(-1)^k \\mathrm{Var}(f(U))\n$$\nSubstituting this into our expression for the ratio $R(k)$ and assuming $\\mathrm{Var}(f(U)) \\neq 0$ (which is true for a positive integer $k$), we can cancel the variance terms:\n$$\nR(k) = 1 + \\frac{-(-1)^k \\mathrm{Var}(f(U))}{\\mathrm{Var}(f(U))} = 1 - (-1)^k\n$$\nThis single expression elegantly captures the behavior for both even and odd $k$.\n1.  If $k$ is an even integer, $R(k) = 1 - 1 = 0$. This occurs because for even $k$, $f(1-x) = -f(x)$ (odd symmetry about $x=1/2$), leading to perfect variance reduction.\n2.  If $k$ is an odd integer, $R(k) = 1 - (-1) = 2$. This occurs because for odd $k$, $f(1-x) = f(x)$ (even symmetry about $x=1/2$), causing the variance to double compared to a crude Monte Carlo estimator using the same number of function evaluations.",
            "answer": "$$\n\\boxed{1 - (-1)^k}\n$$"
        },
        {
            "introduction": "This final practice bridges the gap between theoretical concepts and practical application, a core skill in computational science. You will first prove the general principles that guarantee variance reduction for antithetic sampling, particularly for monotone functions. Then, you will implement a program to empirically test these theoretical findings, allowing you to observe the variance reduction in action and solidify your understanding by comparing the performance across different types of functions .",
            "id": "3253450",
            "problem": "You are to study and implement antithetic sampling for Monte Carlo integration on the unit interval. Let $g:[0,1]\\to\\mathbb{R}$ be a square-integrable function. The goal is to estimate the integral $I=\\int_{0}^{1} g(x)\\,dx$ and to compare the variance of a crude Monte Carlo estimator with that of an antithetic estimator that uses pairs $(U,1-U)$, where $U$ is a Uniform$([0,1])$ random variable.\n\nStarting from fundamental definitions of expectation and variance, and without relying on any precomputed \"shortcut\" formulas, address the following:\n\n- Define the crude Monte Carlo estimator $\\widehat{I}_{\\text{crude}}$ constructed from $M$ independent samples $U_1,\\dots,U_M$ with $U_i\\sim\\text{Uniform}([0,1])$ as the sample mean of $X_i=g(U_i)$. Define the antithetic estimator $\\widehat{I}_{\\text{anti}}$ using the $M$ antithetic pairs $(U_i,1-U_i)$ as the sample mean of $A_i=\\tfrac{1}{2}\\big(g(U_i)+g(1-U_i)\\big)$. Prove, from first principles and core definitions, that the variance of the antithetic per-pair average $A_i$ is less than or equal to the variance of $X_i$; consequently, the variance of $\\widehat{I}_{\\text{anti}}$ is less than or equal to the variance of $\\widehat{I}_{\\text{crude}}$ for any square-integrable $g$.\n\n- Further, for the special case where $g$ is monotone on $[0,1]$, establish that the covariance $\\mathrm{Cov}\\big(g(U),g(1-U)\\big)\\le 0$. Explain the equality case when $g(1-x)=g(x)$ for all $x\\in[0,1]$.\n\n- Implement a complete program that estimates $I$ for several test functions $g$, computes empirical per-sample variances of $X=g(U)$ and $A=\\tfrac{1}{2}\\big(g(U)+g(1-U)\\big)$, and reports the variance ratio $r=\\widehat{\\mathrm{Var}}(A)/\\widehat{\\mathrm{Var}}(X)$. Use the same number of pairs $M$ and the same base uniforms $U_i$ for both estimators so that the only difference is the antithetic pairing.\n\nUse the following test suite. For each case, the program must:\n- draw $M$ independent uniforms $U_i\\sim\\text{Uniform}([0,1])$ with the given random seed;\n- compute $X_i=g(U_i)$ and $A_i=\\tfrac{1}{2}\\big(g(U_i)+g(1-U_i)\\big)$;\n- compute the crude and antithetic estimates $\\widehat{I}_{\\text{crude}}=\\tfrac{1}{M}\\sum_{i=1}^M X_i$ and $\\widehat{I}_{\\text{anti}}=\\tfrac{1}{M}\\sum_{i=1}^M A_i$;\n- compute the unbiased sample variances $\\widehat{\\mathrm{Var}}(X)$ and $\\widehat{\\mathrm{Var}}(A)$ (with divisor $M-1$);\n- compute the ratio $r=\\widehat{\\mathrm{Var}}(A)/\\widehat{\\mathrm{Var}}(X)$;\n- compare $r$ to $1$ with a tolerance $\\tau$ and report whether $r\\le 1+\\tau$.\n\nTest suite (each line specifies $(g(x), I, M, \\text{seed})$):\n- $g_1(x)=x^2$, $I_1=\\int_0^1 x^2\\,dx=\\tfrac{1}{3}$, $M=200{,}000$, seed $=12345$.\n- $g_2(x)=\\exp(x)$, $I_2=\\int_0^1 \\exp(x)\\,dx=e-1$, $M=200{,}000$, seed $=54321$.\n- $g_3(x)=(x-\\tfrac{1}{2})^2$, $I_3=\\int_0^1 (x-\\tfrac{1}{2})^2\\,dx=\\tfrac{1}{12}$, $M=200{,}000$, seed $=20231011$.\n- $g_4(x)=x$, $I_4=\\int_0^1 x\\,dx=\\tfrac{1}{2}$, $M=200{,}000$, seed $=7$.\n\nSet the comparison tolerance to $\\tau=0.02$. The angle unit does not apply. There are no physical units in this problem. The outputs must be real numbers (floats) and booleans.\n\nFinal output format: Your program should produce a single line of output containing, for each test case in order $g_1,g_2,g_3,g_4$, the pair $(r, b)$ where $r$ is the variance ratio rounded to six decimal places and $b$ is a boolean indicating whether $r\\le 1+\\tau$. Aggregate all results into a single list in the order $[r_1,b_1,r_2,b_2,r_3,b_3,r_4,b_4]$ and print it as a comma-separated list enclosed in square brackets, for example $[0.500000,True, \\dots]$.",
            "solution": "The problem is valid as it constitutes a well-posed and standard exercise in the field of numerical methods, specifically Monte Carlo integration and variance reduction techniques. It is scientifically sound, self-contained, and objective. We will proceed with the theoretical derivations followed by the computational implementation.\n\n### Theoretical Derivations\n\nLet $U$ be a random variable following a uniform distribution on the interval $[0, 1]$, denoted $U \\sim \\text{Uniform}([0,1])$. The integral to be estimated is $I = \\int_{0}^{1} g(x)\\,dx$. By the definition of expectation for a continuous random variable, this integral is equivalent to the expected value of $g(U)$, i.e., $I = \\mathbb{E}[g(U)]$.\n\nThe random variable $V = 1-U$ also follows a $\\text{Uniform}([0,1])$ distribution. This can be seen from its cumulative distribution function: $F_V(v) = P(V \\le v) = P(1-U \\le v) = P(U \\ge 1-v) = 1 - P(U  1-v) = 1 - (1-v) = v$ for $v \\in [0,1]$. Its probability density function is $f_V(v) = dF_V/dv = 1$ on $[0,1]$. Consequently, $\\mathbb{E}[g(1-U)] = \\int_0^1 g(v) f_V(v) dv = \\int_0^1 g(v) dv = I$.\n\nWe are given two estimators based on $M$ independent draws $U_1, \\dots, U_M$ from $\\text{Uniform}([0,1])$.\nThe crude Monte Carlo estimator's per-sample term is $X_i = g(U_i)$.\nThe antithetic estimator's per-sample term is $A_i = \\frac{1}{2}(g(U_i) + g(1-U_i))$.\n\nBoth estimators are unbiased because their per-sample terms have an expectation of $I$:\n$\\mathbb{E}[X_i] = \\mathbb{E}[g(U_i)] = I$.\n$\\mathbb{E}[A_i] = \\mathbb{E}\\left[\\frac{1}{2}(g(U_i) + g(1-U_i))\\right] = \\frac{1}{2}(\\mathbb{E}[g(U_i)] + \\mathbb{E}[g(1-U_i)]) = \\frac{1}{2}(I + I) = I$.\n\nThe estimators are defined as sample means:\n$\\widehat{I}_{\\text{crude}} = \\frac{1}{M}\\sum_{i=1}^M X_i$\n$\\widehat{I}_{\\text{anti}} = \\frac{1}{M}\\sum_{i=1}^M A_i$\n\nThe variance of these estimators are:\n$\\mathrm{Var}(\\widehat{I}_{\\text{crude}}) = \\mathrm{Var}\\left(\\frac{1}{M}\\sum_{i=1}^M X_i\\right) = \\frac{1}{M^2}\\sum_{i=1}^M \\mathrm{Var}(X_i) = \\frac{M \\mathrm{Var}(X_i)}{M^2} = \\frac{\\mathrm{Var}(X_i)}{M}$.\n$\\mathrm{Var}(\\widehat{I}_{\\text{anti}}) = \\mathrm{Var}\\left(\\frac{1}{M}\\sum_{i=1}^M A_i\\right) = \\frac{1}{M^2}\\sum_{i=1}^M \\mathrm{Var}(A_i) = \\frac{M \\mathrm{Var}(A_i)}{M^2} = \\frac{\\mathrm{Var}(A_i)}{M}$.\nThe samples $(X_i)$ are i.i.d., and the samples $(A_i)$ are i.i.d.\n\n**Proof that $\\mathrm{Var}(A_i) \\le \\mathrm{Var}(X_i)$**\n\nLet's drop the index $i$ for simplicity and analyze the variance of the generic terms $X = g(U)$ and $A = \\frac{1}{2}(g(U) + g(1-U))$.\n\nThe variance of $A$ is given by:\n$$ \\mathrm{Var}(A) = \\mathrm{Var}\\left(\\frac{1}{2}(g(U) + g(1-U))\\right) $$\nUsing the property $\\mathrm{Var}(cZ) = c^2\\mathrm{Var}(Z)$ with $c = \\frac{1}{2}$:\n$$ \\mathrm{Var}(A) = \\frac{1}{4} \\mathrm{Var}(g(U) + g(1-U)) $$\nUsing the property for the variance of a sum, $\\mathrm{Var}(Z_1+Z_2) = \\mathrm{Var}(Z_1) + \\mathrm{Var}(Z_2) + 2\\mathrm{Cov}(Z_1, Z_2)$:\n$$ \\mathrm{Var}(A) = \\frac{1}{4} \\left( \\mathrm{Var}(g(U)) + \\mathrm{Var}(g(1-U)) + 2\\mathrm{Cov}(g(U), g(1-U)) \\right) $$\nSince $U$ and $1-U$ have the same distribution, the random variables $g(U)$ and $g(1-U)$ are identically distributed. Therefore, they have the same variance: $\\mathrm{Var}(g(U)) = \\mathrm{Var}(g(1-U))$. Let's denote this variance as $\\mathrm{Var}(X)$.\n$$ \\mathrm{Var}(A) = \\frac{1}{4} \\left( \\mathrm{Var}(X) + \\mathrm{Var}(X) + 2\\mathrm{Cov}(g(U), g(1-U)) \\right) $$\n$$ \\mathrm{Var}(A) = \\frac{1}{2} \\left( \\mathrm{Var}(X) + \\mathrm{Cov}(g(U), g(1-U)) \\right) $$\nTo prove that $\\mathrm{Var}(A) \\le \\mathrm{Var}(X)$, we need to show:\n$$ \\frac{1}{2} \\left( \\mathrm{Var}(X) + \\mathrm{Cov}(g(U), g(1-U)) \\right) \\le \\mathrm{Var}(X) $$\n$$ \\mathrm{Var}(X) + \\mathrm{Cov}(g(U), g(1-U)) \\le 2\\mathrm{Var}(X) $$\n$$ \\mathrm{Cov}(g(U), g(1-U)) \\le \\mathrm{Var}(X) $$\nThis inequality is a direct consequence of the Cauchy-Schwarz inequality for covariances, which states that $|\\mathrm{Cov}(Z_1, Z_2)| \\le \\sqrt{\\mathrm{Var}(Z_1)\\mathrm{Var}(Z_2)}$.\nApplying this with $Z_1 = g(U)$ and $Z_2 = g(1-U)$:\n$$ |\\mathrm{Cov}(g(U), g(1-U))| \\le \\sqrt{\\mathrm{Var}(g(U))\\mathrm{Var}(g(1-U))} $$\nSince $\\mathrm{Var}(g(U)) = \\mathrm{Var}(g(1-U)) = \\mathrm{Var}(X)$, this simplifies to:\n$$ |\\mathrm{Cov}(g(U), g(1-U))| \\le \\sqrt{\\mathrm{Var}(X) \\cdot \\mathrm{Var}(X)} = \\mathrm{Var}(X) $$\nAs any number is less than or equal to its absolute value, we have $\\mathrm{Cov}(g(U), g(1-U)) \\le |\\mathrm{Cov}(g(U), g(1-U))|$.\nCombining these, we get $\\mathrm{Cov}(g(U), g(1-U)) \\le \\mathrm{Var}(X)$, which proves the required inequality $\\mathrm{Var}(A) \\le \\mathrm{Var}(X)$.\nConsequently, $\\mathrm{Var}(\\widehat{I}_{\\text{anti}}) = \\frac{\\mathrm{Var}(A)}{M} \\le \\frac{\\mathrm{Var}(X)}{M} = \\mathrm{Var}(\\widehat{I}_{\\text{crude}})$. This shows that antithetic sampling never increases the variance of the estimator when comparing estimators based on the same number of underlying uniform draws. The magnitude of the variance reduction depends on the covariance term being negative.\n\n**Covariance for Monotone Functions**\n\nLet $g$ be a monotone function on $[0,1]$. Let's assume without loss of generality that $g$ is non-decreasing. Let $h_1(x) = g(x)$, which is non-decreasing, and $h_2(x) = g(1-x)$. Since $1-x$ is a decreasing function of $x$, and $g$ is non-decreasing, their composition $h_2(x) = g(1-x)$ is a non-increasing (i.e., decreasing) function of $x$.\n\nWe use Chebyshev's integral inequality, which states that if $f_1$ is non-decreasing and $f_2$ is non-increasing on $[a,b]$, then\n$$ \\int_a^b f_1(x)f_2(x)\\,dx \\le \\frac{1}{b-a} \\left(\\int_a^b f_1(x)\\,dx\\right) \\left(\\int_a^b f_2(x)\\,dx\\right) $$\nFor our case on $[0,1]$, this is:\n$$ \\int_0^1 g(x)g(1-x)\\,dx \\le \\left(\\int_0^1 g(x)\\,dx\\right) \\left(\\int_0^1 g(1-x)\\,dx\\right) $$\nIn terms of expectations with $U \\sim \\text{Uniform}([0,1])$:\n$$ \\mathbb{E}[g(U)g(1-U)] \\le \\mathbb{E}[g(U)] \\mathbb{E}[g(1-U)] $$\nBy definition, the covariance is $\\mathrm{Cov}(g(U), g(1-U)) = \\mathbb{E}[g(U)g(1-U)] - \\mathbb{E}[g(U)]\\mathbb{E}[g(1-U)]$.\nTherefore, for a monotone function $g$, we have $\\mathrm{Cov}(g(U), g(1-U)) \\le 0$.\n\n**Equality Case for $\\mathrm{Cov}(g(U),g(1-U))$**\n\nIf a function $g$ is both monotone (non-decreasing or non-increasing) on $[0,1]$ and symmetric about $x=1/2$ (i.e., $g(x)=g(1-x)$ for all $x \\in [0,1]$), it must be a constant function.\nTo prove this, assume $g$ is non-decreasing and symmetric. For any $x_1, x_2 \\in [0,1]$ with $x_1  x_2$:\n1. From monotonicity: $g(x_1) \\le g(x_2)$.\n2. From $x_1  x_2$, we have $1-x_2  1-x_1$. From monotonicity again: $g(1-x_2) \\le g(1-x_1)$.\n3. Using symmetry, $g(1-x_2)=g(x_2)$ and $g(1-x_1)=g(x_1)$. Substituting this into the inequality from step 2 gives $g(x_2) \\le g(x_1)$.\nCombining the results from steps 1 and 3, we have $g(x_1) \\le g(x_2)$ and $g(x_2) \\le g(x_1)$, which implies $g(x_1)=g(x_2)$. Since this holds for any pair $x_1, x_2$, the function $g$ must be constant on $[0,1]$.\nIf $g(x)=c$ for some constant $c$, then $g(U)=c$ and $g(1-U)=c$. The covariance of two constants is zero: $\\mathrm{Cov}(c,c) = 0$. Thus, for a function that is both monotone and symmetric, the covariance is zero.\n\n**Variance Ratio for Symmetric Functions**\n\nA separate consideration is when $g$ is symmetric, i.e., $g(x) = g(1-x)$, but not necessarily monotone (e.g., $g_3(x)=(x-1/2)^2$).\nIn this case, the two terms in the antithetic pair are identical: $g(U_i) = g(1-U_i)$.\nThe antithetic per-sample term becomes $A_i = \\frac{1}{2}(g(U_i) + g(U_i)) = g(U_i) = X_i$.\nThe antithetic estimator is identical to the crude estimator, and their sample variances must be equal: $\\widehat{\\mathrm{Var}}(A) = \\widehat{\\mathrm{Var}}(X)$. This leads to a variance ratio $r = 1$.\nThis corresponds to the maximum possible covariance, $\\mathrm{Cov}(g(U), g(1-U)) = \\mathrm{Var}(g(U))$, which yields $\\mathrm{Var}(A) = \\frac{1}{2}(\\mathrm{Var}(X) + \\mathrm{Var}(X)) = \\mathrm{Var}(X)$. Antithetic sampling offers no benefit in this case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs Monte Carlo integration using crude and antithetic sampling,\n    and compares their empirical variances for a suite of test functions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (function_name, g(x), I, M, seed)\n    # The exact integral I is for context and not used in the calculations.\n    test_cases = [\n        ('g1(x) = x^2', lambda x: x**2, 1/3, 200000, 12345),\n        ('g2(x) = exp(x)', lambda x: np.exp(x), np.e - 1, 200000, 54321),\n        ('g3(x) = (x-1/2)^2', lambda x: (x - 0.5)**2, 1/12, 200000, 20231011),\n        ('g4(x) = x', lambda x: x, 0.5, 200000, 7),\n    ]\n\n    # Set the comparison tolerance\n    tau = 0.02\n    \n    # List to store the final results\n    results_list = []\n\n    for name, g, I_exact, M, seed in test_cases:\n        # 1. Set up the random number generator with the specified seed.\n        rng = np.random.default_rng(seed)\n\n        # 2. Draw M independent uniform random numbers.\n        U = rng.uniform(0, 1, M)\n\n        # 3. Compute the sample values for crude and antithetic estimators.\n        # Crude samples\n        X = g(U)\n        \n        # Antithetic samples: A_i = 1/2 * (g(U_i) + g(1-U_i))\n        A = 0.5 * (g(U) + g(1 - U))\n\n        # 4. Compute the crude and antithetic estimates for the integral.\n        # This part is required by the problem description, though not for the final output.\n        I_crude_hat = np.mean(X)\n        I_anti_hat = np.mean(A)\n\n        # 5. Compute the unbiased sample variances (using ddof=1 for divisor M-1).\n        var_X = np.var(X, ddof=1)\n        var_A = np.var(A, ddof=1)\n        \n        # Handle the case where var_X is zero to avoid division by zero.\n        # This occurs when g(U) is constant, e.g., if g is a constant function.\n        # For g(x)=x, var_A is analytically zero, but might be a tiny float.\n        if var_X == 0:\n            # If var_A is also zero, ratio is undefined, but can be treated as 1 (no reduction)\n            # or 0 if reduction is perfect. Given the context, if var_A is also 0,\n            # it implies a perfect reduction from a non-constant X.\n            r = 0.0 if var_A  var_X else 1.0\n        else:\n             r = var_A / var_X\n\n        # 6. Compare the ratio to 1 with the given tolerance.\n        b = (r = 1 + tau)\n\n        # 7. Append the formatted results to the list.\n        results_list.append(round(r, 6))\n        results_list.append(b)\n\n    # 8. Print the final results in the specified format.\n    # The map(str, ...) converts all items, including booleans, to their string representation.\n    # The boolean 'True' must be capitalized as per standard Python str() conversion.\n    final_output_str = f\"[{','.join(str(item) for item in results_list)}]\"\n    print(final_output_str.replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\nsolve()\n\n```"
        }
    ]
}