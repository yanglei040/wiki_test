{
    "hands_on_practices": [
        {
            "introduction": "在共享内存并行编程中，性能不仅取决于如何划分工作，还取决于线程如何与底层内存系统交互。一个微妙但关键的性能陷阱是“伪共享”（false sharing），即由不同线程修改的逻辑上独立的数据，恰好位于同一缓存行（cache line）上。这项练习  要求你根据其症状诊断此问题，并评估不同的数据布局策略来解决它。掌握这一概念对于在现代多核处理器上编写高效代码至关重要，因为它迫使你思考数据的物理布局，而不仅仅是其逻辑结构。",
            "id": "3145329",
            "problem": "一个共享内存多核系统使用一种基于失效的缓存一致性协议，该协议与MESI（修改、独占、共享、无效）协议一致。一致性单元是一个大小为 $L = 64$ 字节的缓存行。考虑在一个处理器上，有 $T$ 个线程，每个线程固定在一个核心上，该处理器的私有缓存行大小也为 $L = 64$ 字节。在一个紧密循环中，每个线程 $i$ 在共享数组 $c[\\,]$ 的索引 $i$ 处递增一个属于该线程的计数器。$c[\\,]$ 的类型是 $64$ 位无符号整数，因此每个元素的大小为 $s = 8$ 字节。数组的基地址与 $L = 64$ 字节对齐。线程间的通信模式在循环期间不需要共享计数器的逻辑值；每个线程只更新自己的计数器，循环结束后会执行一次归约操作来合并结果。\n\n根据经验，该递增循环的吞吐量低于预期，并且当线程数减少到 $T = 1$ 时性能有所提高。你需要从缓存一致性的基本原理出发，解释观察到的性能下降是否与伪共享（false sharing）一致，如果一致，请确定哪些数据布局重新设计方案可以在不改变循环语义的情况下，保证消除因伪共享导致的缓存行乒乓（ping-pong）效应。\n\n选择所有在此假设下，对于任何 $T \\ge 1$，能够保证不同线程在更新各自计数器时不会在递增循环期间引起伪共享的选项。\n\nA. 填充每个计数器，使连续的计数器之间至少相隔 $L = 64$ 字节，并将数组基地址与 $L = 64$ 字节对齐，从而使每个线程的计数器位于一个独立的缓存行上。\n\nB. 保留原始的由 $s = 8$ 字节计数器组成的连续数组，但仅将数组基地址与 $L = 64$ 字节对齐；依赖基地址对齐来防止伪共享。\n\nC. 将数组结构（struct-of-arrays）布局替换为结构体数组（array-of-structs）布局，其中每个线程的结构体包含计数器，并进行填充，使结构体大小恰好为 $L = 64$ 字节；将数组基地址与 $L = 64$ 字节对齐。\n\nD. 在原始的由 $s = 8$ 字节计数器组成的连续数组上使用原子递增操作（例如，原子取加操作 fetch-and-add），以防止缓存行乒乓效应。\n\nE. 使用数组结构（struct-of-arrays）布局，将所有计数器连续分组以提高计数器字段的空间局部性；依赖改善的局部性来减少一致性流量。\n\n提供你的选择，并通过推导在原始布局下发生伪共享的条件以及避免伪共享所需的条件来证明你的选择，推导过程需基于缓存行和基于失效的一致性的核心定义。",
            "solution": "问题陈述描述了并行计算中一个经典的性能问题，称为伪共享（false sharing）。该问题在科学上是成立的、定义明确的，并提供了足够、一致的信息，可以从计算机体系结构的基本原理推导出解决方案。\n\n**1. 问题陈述的验证**\n\n已知条件如下：\n- 一个共享内存多核系统，有 $T$ 个线程，每个线程一个核心。\n- 一个基于失效的缓存一致性协议，与MESI一致。\n- 一致性单元（以及私有缓存行大小）为 $L = 64$ 字节。\n- 每个线程 $i$ 在一个紧密循环中递增其自己的计数器 $c[i]$。\n- 计数器是 $64$ 位无符号整数，因此它们的大小为 $s = 8$ 字节。\n- 数组 $c$ 在内存中是连续的，其基地址与一个 $64$ 字节边界对齐。\n- 在循环期间，线程之间没有逻辑上共享计数器值的需求。\n\n观察到的当 $T > 1$ 时的性能下降与伪共享现象是一致的。让我们从基本原理分析原因。\n\n**2. 从基本原理推导**\n\n缓存一致性协议在缓存行的粒度上运行。在一个像MESI这样的基于失效的协议中，当一个核心要写入一个内存位置时，它必须首先获得包含该位置的缓存行的独占所有权。这通常通过互连总线向所有其他可能拥有该缓存行副本的核心发送一个失效信号来实现。这些核心随后必须将它们的副本标记为无效（I状态）。之后，这些核心对该行进行读或写的任何尝试都将导致缓存未命中，从而强制从内存或直接从修改核心的缓存中获取更新后的行。\n\n当多个由不同线程更新的独立数据项恰好位于同一个物理缓存行上时，问题就出现了。这被称为**伪共享**。这些数据项在逻辑上是独立的，但它们在内存中的物理邻近性通过缓存一致性机制将其性能耦合在了一起。\n\n在给定场景中：\n- 单个计数器元素的大小是 $s = 8$ 字节。\n- 缓存行的大小是 $L = 64$ 字节。\n- 因此，一个缓存行可以容纳的计数器数量是 $N = L/s = 64/8 = 8$。\n\n数组 $c$ 是连续的，并且其基地址与一个 $64$ 字节边界对齐。这意味着前 $8$ 个计数器 $c[0], c[1], \\dots, c[7]$ 的内存地址将占据同一个缓存行。具体来说，如果 $c$ 的基地址是 $A_{base}$，那么 $c[i]$ 的地址是 $A_{base} + i \\times s$。对于 $i \\in \\{0, 1, \\dots, 7\\}$，这些地址都落在第一个缓存行内，该缓存行覆盖的内存范围是 $[A_{base}, A_{base} + L - 1]$。\n\n让我们考虑两个线程，线程 $0$ 和线程 $1$，它们分别在不同的核心上运行并更新 $c[0]$ 和 $c[1]$。\n1. 线程 $0$ 执行 `c[0]++`。这是一个读-修改-写操作。为了执行写操作，核心 $0$ 必须获取该缓存行的独占所有权。假设它将该行以“修改”（M）状态加载到其私有缓存中。\n2. 线程 $1$ 执行 `c[1]++`。由于 $c[1]$ 在同一个缓存行上，核心 $1$ 也必须获取独占所有权。它发送一个对该行的请求。\n3. 这个请求导致核心 $0$ 失去独占所有权。它可能会将该行写回内存和/或将其转发给核心 $1$。核心 $0$ 的该行副本被置为无效（I状态）。\n4. 核心 $1$ 现在以“修改”状态拥有该行，并递增 $c[1]$。\n5. 当线程 $0$ 循环并再次尝试递增 $c[0]$ 时，它发现自己的副本是无效的，导致缓存未命中。它必须重新获取该缓存行，这又会使核心 $1$ 缓存中的副本失效。\n\n这种缓存行在核心之间的来回传输被称为“缓存行乒乓效应”（cache line ping-pong）。这是一个代价高昂的过程，涉及内存总线或互连上的停顿和流量，这解释了观察到的低吞吐量。当 $T=1$ 时，这个问题消失了，因为没有其他核心来争夺该缓存行。\n\n为了**保证**消除伪共享，我们必须确保任意两个由不同线程修改的计数器都不会位于同一个缓存行上。由于每个线程 $i$ 只修改 $c[i]$，所以条件是对于任意两个不同的线程 $i$ 和 $j$，$c[i]$ 和 $c[j]$ 必须位于不同的缓存行上。这可以通过构建数据结构来实现，使得每个 `c[i]` 都位于一个至少为 $L=64$ 字节且不与任何其他 $c[j]$ 共享的内存块中。\n\n**3. 选项评估**\n\n**A. 填充每个计数器，使连续的计数器之间至少相隔 $L = 64$ 字节，并将数组基地址与 $L = 64$ 字节对齐，从而使每个线程的计数器位于一个独立的缓存行上。**\n这种方法明确地强制执行了必要条件。如果数组的基地址与 $64$ 字节对齐，并且每个元素都放置在一个是 $64$ 字节倍数的偏移量处，那么每个元素都将从一个新的缓存行开始。例如，$c[i]$ 的地址将是 $A_{base} + i \\times P$，其中 $P \\ge L = 64$。由于一个缓存行不能跨越两个元素，并且每个元素的起始地址与下一个元素至少相距 $L$ 字节，因此每个 $c[i]$ 都将位于一个唯一的缓存行上（相对于其他计数器而言）。这种布局直接防止了伪共享。\n结论：**正确**。\n\n**B. 保留原始的由 $s = 8$ 字节计数器组成的连续数组，但仅将数组基地址与 $L = 64$ 字节对齐；依赖基地址对齐来防止伪共享。**\n这描述了有问题的原始设置。正如推导中所示，对一个由小元素组成的连续数组的基地址进行对齐，并不能阻止这些元素共享一个缓存行。前 $8$ 个计数器 $c[0]$ 到 $c[7]$ 仍将占据第一个缓存行，导致前 $8$ 个线程之间发生伪共享。\n结论：**不正确**。\n\n**C. 将数组结构（struct-of-arrays）布局替换为结构体数组（array-of-structs）布局，其中每个线程的结构体包含计数器，并进行填充，使结构体大小恰好为 $L = 64$ 字节；将数组基地址与 $L = 64$ 字节对齐。**\n这是选项A策略的另一种常见实现方式。一个像 `struct { uint64_t counter; char padding[56]; } data[T];` 这样的结构体数组，其 `sizeof` 将等于 $64$ 字节。由于数组是连续的，并且其基地址与 $64$ 字节对齐，第 $i$ 个结构体 `data[i]` 的地址将是 $A_{base} + i \\times 64$。这意味着每个结构体，以及其中的每个计数器，都将从一个新的缓存行边界开始。线程 $i$ 的计数器与线程 $j$ ($i \\neq j$) 的计数器位于不同的缓存行上。这保证了伪共享的消除。\n结论：**正确**。\n\n**D. 在原始的由 $s = 8$ 字节计数器组成的连续数组上使用原子递增操作（例如，原子取加操作 fetch-and-add），以防止缓存行乒乓效应。**\n原子操作确保读-修改-写序列从CPU指令流的角度看是作为一个不可分割的单元执行的。这对于防止*真共享*（多个线程更新同一个变量）情况下的数据竞争至关重要。然而，它们不会改变底层的内存布局或缓存一致性协议的行为。对 $c[i]$ 的原子写操作仍然是对一个缓存行内某个内存位置的写操作。如果其他线程正在写入同一缓存行上的其他变量（如 $c[j]$），硬件仍然必须使其副本失效，以给予写核心独占访问权。原子操作不会也无法阻止由伪共享引起的缓存行乒乓效应。\n结论：**不正确**。\n\n**E. 使用数组结构（struct-of-arrays）布局，将所有计数器连续分组以提高计数器字段的空间局部性；依赖改善的局部性来减少一致性流量。**\n初始问题设置*就是*一个数组结构布局（或其对于简单数组的等效形式），其中所有计数器都是连续分组的。这种设计为*顺序*访问模式（一个线程遍历所有计数器）提供了极好的空间局部性。然而，对于给定的*并行*访问模式，即每个线程重复访问其自己的单个、不同的计数器，恰恰是这种空间局部性导致了伪共享。将计数器连续放置确保了它们会共享缓存行，这反过来*最大化*而非减少了一致性流量。\n结论：**不正确**。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "在处理了底层内存问题之后，并行编程的一个关键挑战是如何将工作均匀地分配给处理器核心，这项任务被称为负载均衡（load balancing）。当任务的计算成本不可预测时，调度策略的选择——例如 OpenMP 中的静态与动态调度选项——就变得至关重要。这项练习  要求你在一个具有“重尾”工作负载分布的真实场景中，分析这些策略之间的权衡。通过解决这个问题，你将培养一种直觉，判断何时动态调度的开销是合理的，因为它能够适应并保持更好的负载均衡，这是性能优化的核心技能。",
            "id": "3145384",
            "problem": "考虑一个包含 $N$ 次独立迭代的循环，该循环将使用 Open Multi-Processing (OpenMP) 在 $p$ 个相同的中央处理器（CPU）核心上运行。每次迭代 $i$ 的运行时间 $T_i$ 独立地从一个重尾混合分布中抽取：有 $q$ 的概率是“重”迭代，耗时 $t_H$ 个时间单位；有 $1 - q$ 的概率是“轻”迭代，耗时 $t_L$ 个时间单位。假设 $t_H \\gg t_L$，并且该混合模型是对重尾分布的一个合理替代。该循环可以被静态调度（每个核心获得一个固定的连续迭代块）或动态调度（每个核心从一个共享队列中重复拉取下一个大小为 $c$ 的可用块，直到所有迭代完成）。每次动态获取块都会在获取核心上产生 $t_s$ 的调度开销；静态调度的开销可忽略不计。\n\n在此特定情景下，取 $N = 400$，$p = 4$，$q = 0.02$，$t_L = 1\\,\\mathrm{ms}$，$t_H = 100\\,\\mathrm{ms}$，以及 $t_s = 0.2\\,\\mathrm{ms}$。对于动态调度，考虑 $c = 1$ 和 $c = 10$ 的情况。假设运行时系统在其他方面是理想的：除了每个块的开销 $t_s$ 外没有其他竞争，并且核心以所述的时间执行迭代。\n\n使用管控并行循环执行的基本定义：\n- 完工时间 $T_{\\mathrm{par}}$ 是指最后一个核心完成其分配工作的时间。\n- 在完美的负载均衡和可忽略的开销下，$T_{\\mathrm{par}}$ 约等于总工作量除以 $p$，即 $T_{\\mathrm{par}} \\approx \\left(\\sum_{i=1}^{N} T_i\\right)/p$。\n- 在静态调度下，每个核心接收恰好 $N/p$ 次迭代，因此其运行时间等于其分配块上所有 $T_i$ 的总和；完工时间由拥有最大该总和的核心决定。\n- 在动态调度下，运行时间包括迭代执行时间和获取块的累积调度开销；分块会影响负载均衡的粒度和开销。\n\n基于这些定义和给定参数，关于预期负载均衡和调度开销权衡的陈述中，哪些最为准确？\n\nA. 静态调度实现了近乎完美的负载均衡，因为每个核心都精确地获得了 $N/p$ 次迭代，所以其预期完工时间接近 $\\left(\\mathbb{E}[T]\\,N\\right)/p$，并且其性能优于 $c = 1$ 的动态调度。\n\nB. $c = 1$ 的动态调度实现了比静态调度显著更好的预期负载均衡，因此即使在计入每次迭代的调度开销 $t_s$ 后，其预期完工时间也更接近 $\\left(\\sum_{i=1}^{N} T_i\\right)/p$。\n\nC. 与 $c=1$ 相比，$c=10$ 的动态调度将调度开销减少了10倍，同时保留了动态调度的大部分负载均衡优势，因此其预期完工时间低于 $c=1$ 的情况。\n\nD. 在静态分配前随机排列迭代顺序，使得静态调度实际上等效于 $c = 1$ 的动态调度，从而在没有开销的情况下产生一个接近 $\\left(\\sum_{i=1}^{N} T_i\\right)/p$ 的预期完工时间。\n\n选择所有适用的选项。",
            "solution": "问题陈述已经过验证，被认为是科学上成立、定义明确且客观的。它展示了并行计算性能分析中的一个经典场景。\n\n我们首先根据提供的数据为我们的分析建立基准参数。\n- 迭代总数：$N = 400$\n- 处理器核心数：$p = 4$\n- 重迭代的概率：$q = 0.02$\n- 轻迭代的运行时间：$t_L = 1\\,\\mathrm{ms}$\n- 重迭代的运行时间：$t_H = 100\\,\\mathrm{ms}$\n- 动态块的调度开销：$t_s = 0.2\\,\\mathrm{ms}$\n\n首先，我们计算预期的总计算工作量 $W_{total}$。预期的重迭代次数是 $N \\times q = 400 \\times 0.02 = 8$。预期的轻迭代次数是 $N \\times (1-q) = 400 \\times (1 - 0.02) = 392$。\n总预期工作量是所有迭代时间的总和：\n$$W_{total} = (Nq) t_H + (N(1-q)) t_L = (8 \\times 100\\,\\mathrm{ms}) + (392 \\times 1\\,\\mathrm{ms}) = 800\\,\\mathrm{ms} + 392\\,\\mathrm{ms} = 1192\\,\\mathrm{ms}$$\n为了比较不同调度器，我们将这个预期工作量视为要执行的实际工作量，即 $\\sum_{i=1}^{N} T_i = 1192\\,\\mathrm{ms}$。\n\n理想完工时间 $T_{ideal}$ 假设了完美的负载均衡和零开销。它可作为预期完工时间的理论下界。\n$$T_{ideal} = \\frac{W_{total}}{p} = \\frac{1192\\,\\mathrm{ms}}{4} = 298\\,\\mathrm{ms}$$\n\n现在，我们分析每种调度策略。\n\n**静态调度**\n在静态调度下，$N=400$ 次迭代被划分为 $p=4$ 个连续块，每个核心接收 $N/p = 100$ 次迭代。开销可以忽略不计。完工时间由接收到工作量最大的块的核心决定。\n由于 $t_H \\gg t_L$，工作负载是“重尾”的。这意味着完工时间主要由 $8$ 次重迭代在 $4$ 个核心之间的分布情况决定。\n- **最佳情况（完美均衡）：** 如果 $8$ 次重迭代均匀分布，每个核心接收到 $8/4=2$ 次重迭代，那么每个核心的工作量将完全相同：$W_{core} = (2 \\times t_H) + ((100-2) \\times t_L) = (2 \\times 100) + (98 \\times 1) = 298\\,\\mathrm{ms}$。完工时间将是 $T_{static} = 298\\,\\mathrm{ms}$，即理想完工时间。\n- **可能/最差情况（不均衡）：** 迭代的运行时间是独立的，因此无法保证均匀分布。分布不均的可能性很高。例如，考虑一个可能的情景，重迭代的分布是 $(3, 2, 2, 1)$。完工时间将由拥有 $3$ 次重迭代的核心决定：$W_{max} = (3 \\times 100) + (97 \\times 1) = 397\\,\\mathrm{ms}$。在一个更倾斜的情况下，如 $(4, 2, 1, 1)$，完工时间将是 $W_{max} = (4 \\times 100) + (96 \\times 1) = 496\\,\\mathrm{ms}$。\n由于这种不均衡情况的概率很高，静态调度的*预期*完工时间将显著高于理想的 $T_{ideal} = 298\\,\\mathrm{ms}$。\n\n**$c=1$ 的动态调度**\n每个处理器每次从中央队列请求一次迭代。\n- **负载均衡：** 这种细粒度的方法提供了近乎完美的负载均衡。每当一个核心完成一次迭代（无论是轻是重），它会立即开始一个新的。工作在核心之间几乎完美地均匀分布。\n- **开销：** 每次获取块都有一个开销 $t_s$。由于 $c=1$，总共有 $N=400$ 个块。总调度开销为 $O_{c=1} = N \\times t_s = 400 \\times 0.2\\,\\mathrm{ms} = 80\\,\\mathrm{ms}$。\n- **完工时间：** 总工作量是计算工作加上开销：$W_{total} + O_{c=1} = 1192\\,\\mathrm{ms} + 80\\,\\mathrm{ms} = 1272\\,\\mathrm{ms}$。由于其出色的负载均衡，预期完工时间可以很好地用这个总工作量除以核心数来近似。\n$$E[T_{dyn, c=1}] \\approx \\frac{W_{total} + O_{c=1}}{p} = \\frac{1272\\,\\mathrm{ms}}{4} = 318\\,\\mathrm{ms}$$\n这个值由于开销而高于理想的 $298\\,\\mathrm{ms}$，但它显著优于不均衡的静态调度情况（例如，$397\\,\\mathrm{ms}$ 或 $496\\,\\mathrm{ms}$）。\n\n**$c=10$ 的动态调度**\n每个处理器每次请求一个包含 $10$ 次迭代的块。\n- **负载均衡：** 粒度比 $c=1$ 时更粗。这可能会导致一些负载不均衡，但由于有 $N/c = 400/10 = 40$ 个块要动态分配，它仍然远优于静态调度。\n- **开销：** 块的数量减少了10倍。总调度开销为 $O_{c=10} = (N/c) \\times t_s = 40 \\times 0.2\\,\\mathrm{ms} = 8\\,\\mathrm{ms}$。与 $c=1$ 时的 $80\\,\\mathrm{ms}$ 开销相比，这是一个显著的减少。\n- **完工时间：** 总工作量为 $W_{total} + O_{c=10} = 1192\\,\\mathrm{ms} + 8\\,\\mathrm{ms} = 1200\\,\\mathrm{ms}$。这种情况下的理想化完工时间将是：\n$$E[T_{dyn, c=10}]_{ideal} = \\frac{W_{total} + O_{c=10}}{p} = \\frac{1200\\,\\mathrm{ms}}{4} = 300\\,\\mathrm{ms}$$\n由于粒度较粗（一个核心可能在最后闲置，而另一个核心正在完成一个耗时长的块），实际完工时间可能会略高一些，但开销的减少是巨大的（$80\\,\\mathrm{ms}$ 对比 $8\\,\\mathrm{ms}$）。总开销减少了 $72\\,\\mathrm{ms}$，这使得每个核心的平均工作量减少了 $72/4 = 18\\,\\mathrm{ms}$。要使 $c=10$ 的情况比 $c=1$ 的情况慢，负载不均衡造成的损失需要超过这 $18\\,\\mathrm{ms}$ 的收益。在 $40$ 个块分配给 $4$ 个核心的情况下，这不太可能发生。因此，我们预期 $E[T_{dyn, c=10}]  E[T_{dyn, c=1}]$。\n\n现在让我们基于此分析评估给定的选项。\n\n**A. 静态调度实现了近乎完美的负载均衡，因为每个核心都精确地获得了 $N/p$ 次迭代，所以其预期完工时间接近 $\\left(\\mathbb{E}[T]\\,N\\right)/p$，并且其性能优于 $c = 1$ 的动态调度。**\n这个陈述在多个方面都有缺陷。首先，当迭代时间差异很大时（如本例），给每个核心相同*数量*的迭代并不能保证*工作负载*的均衡。这会导致差的负载均衡，而不是“近乎完美”。其次，由于这种差的负载均衡，预期完工时间会显著*大于*理想时间 $(\\mathbb{E}[T]\\,N)/p = 298\\,\\mathrm{ms}$。第三，我们的分析表明，静态调度的预期速度（例如，$397\\,\\mathrm{ms}$ 或更多）要慢于 $c=1$ 的动态调度（约 $318\\,\\mathrm{ms}$）。\n**结论：不正确。**\n\n**B. $c = 1$ 的动态调度实现了比静态调度显著更好的预期负载均衡，因此即使在计入每次迭代的调度开销 $t_s$ 后，其预期完工时间也更接近 $\\left(\\sum_{i=1}^{N} T_i\\right)/p$。**\n这个陈述是准确的。$c=1$ 的动态调度旨在实现出色的负载均衡，对于这种类型的工作负载，这比静态调度有显著的改进。项 $(\\sum_{i=1}^{N} T_i)/p$ 代表了每个核心的理想平均工作量，$T_{ideal}=298\\,\\mathrm{ms}$。虽然动态调度的完工时间（$E[T_{dyn, c=1}] \\approx 318\\,\\mathrm{ms}$）包含了开销，但这个开销带来的损失（$318 - 298 = 20\\,\\mathrm{ms}$）远小于静态调度下预期的严重负载不均衡所带来的损失（例如，$397 - 298 = 99\\,\\mathrm{ms}$）。因此，动态调度下的完工时间确实比静态调度下的完工时间更接近理想并行时间。\n**结论：正确。**\n\n**C. 与 $c=1$ 相比，$c=10$ 的动态调度将调度开销减少了10倍，同时保留了动态调度的大部分负载均衡优势，因此其预期完工时间低于 $c=1$ 的情况。**\n这个陈述准确地描述了这种权衡。$c=10$ 的开销是 $8\\,\\mathrm{ms}$，而 $c=1$ 的开销是 $80\\,\\mathrm{ms}$，所以确实减少了10倍。虽然负载均衡不如 $c=1$时完美，但与静态调度相比仍然非常有效，因此“保留了动态调度的大部分负载均衡优势”是一个公允的描述。开销的显著减少（总共 $72\\,\\mathrm{ms}$）导致了更低的总工作负载。如计算所示，$E[T_{dyn, c=10}]$ 预期在 $300\\,\\mathrm{ms}$ 左右，低于 $E[T_{dyn, c=1}]$ 的约 $318\\,\\mathrm{ms}$。从减少开销中获得的收益超过了在负载均衡效率上的微小损失。\n**结论：正确。**\n\n**D. 在静态分配前随机排列迭代顺序，使得静态调度实际上等效于 $c = 1$ 的动态调度，从而在没有开销的情况下产生一个接近 $(\\sum_{i=1}^{N} T_i)/p$ 的预期完工时间。**\n在静态分配前随机排列迭代有助于避免重迭代聚集在一起的最坏情况，但它不能消除不均衡的统计可能性。一个核心仍然可能偶然地被分配比其他核心更多的重迭代。这与动态调度有根本的不同，后者在运行时*适应*工作负载。因此，它并非“实际上等效于”动态调度。最终，完工时间仍会存在显著的方差，并且由于这种不均衡，预期完工时间将显著大于理想的 $(\\sum_{i=1}^{N} T_i)/p$。关于等效性的说法是错误的。\n**结论：不正确。**",
            "answer": "$$\\boxed{BC}$$"
        },
        {
            "introduction": "在一个给定的机器上，一个并行程序到底能跑多快？屋顶线模型（Roofline model）提供了一个强大的可视化和分析框架，通过将处理器的峰值性能和内存带宽与应用程序的特性联系起来，来回答这个问题。这项练习  提供了一个动手应用屋顶线模型的机会，以预测最大可实现的并行加速比。你将计算系统级约束（如内存带宽）如何创造一个限制性能的“屋顶”，从而解释为何在用尽所有可用核心之前，加速比往往就已饱和。",
            "id": "3145387",
            "problem": "一位程序员正在使用 roofline 性能视角为一个内存受限的内核评估一个共享内存多核系统。该系统拥有最多 $N_{\\max} = 32$ 个相同的核心，每个核心的峰值计算速率为 $P_{c} = 80$ GFLOP/s（每秒十亿次浮点运算）。内存子系统提供 $B = 60$ GB/s（每秒千兆字节）的持续带宽上限，但由于内存访问的并行性有限，单个核心仅能维持 $b_{1} = 12$ GB/s 的带宽。该内核的算术强度为常数 $I = 0.8$ FLOP/byte（每次浮点运算/字节）。假设以下条件成立：算术强度定义为移动每字节数据对应的浮点运算次数；持续性能受限于计算峰值和算术强度与可达带宽的乘积；可达带宽随活动核心数近似线性扩展，直至达到上限 $B$。程序员将并行加速比 $S(N)$ 定义为使用 $N$ 个核心的可达性能与使用单个核心的可达性能之比。在这些约束条件下，请推导出在所有整数 $N$（$1 \\leq N \\leq N_{\\max}$）范围内的最大可能加速比 $S_{\\max}$，并将其作为一个无量纲的十进制数给出。无需四舍五入；请提供精确值。",
            "solution": "问题要求计算在一个共享内存多核系统上，一个内存受限内核的最大可能并行加速比 $S_{\\max}$。分析将使用 roofline 性能模型进行，该模型假定可达性能是峰值计算性能和峰值内存性能中的较小者。\n\n首先，我们定义使用 $N$ 个核心时的可达性能，记为 $P(N)$。根据 roofline 模型：\n$$ P(N) = \\min(P_{\\text{compute}}(N), P_{\\text{memory}}(N)) $$\n其中 $P_{\\text{compute}}(N)$ 是 $N$ 个核心的总峰值计算速率，而 $P_{\\text{memory}}(N)$ 是在 $N$ 个核心的内存带宽下可达到的性能。\n\n已知条件如下：\n- 最大核心数：$N_{\\max} = 32$\n- 单核峰值计算速率：$P_{c} = 80$ GFLOP/s\n- 系统持续内存带宽上限：$B = 60$ GB/s\n- 单核持续内存带宽：$b_{1} = 12$ GB/s\n- 内核算术强度：$I = 0.8$ FLOP/byte\n\n$N$ 个相同核心的总峰值计算速率是它们各自峰值速率的总和：\n$$ P_{\\text{compute}}(N) = N \\times P_{c} $$\n\n峰值内存性能是内核的算术强度 $I$ 与 $N$ 个核心的可达内存带宽 $B(N)$ 的乘积：\n$$ P_{\\text{memory}}(N) = I \\times B(N) $$\n\n问题指出，可达带宽 $B(N)$ 随活动核心数 $N$ 线性扩展，直至达到系统上限 $B$。单核带宽为 $b_1$。因此，线性扩展的带宽为 $N \\times b_1$。这导出了以下 $B(N)$ 模型：\n$$ B(N) = \\min(N \\times b_1, B) $$\n\n结合这些表达式，总性能 $P(N)$ 为：\n$$ P(N) = \\min(N \\times P_{c}, I \\times \\min(N \\times b_1, B)) $$\n使用属性 $\\min(a, \\min(b, c)) = \\min(a, b, c)$，我们可以写成：\n$$ P(N) = \\min(N \\times P_{c}, N \\times I \\times b_1, I \\times B) $$\n\n让我们将给定值代入 $P(N)$ 表达式的各项中：\n- $N \\times P_{c} = N \\times 80$ GFLOP/s\n- $N \\times I \\times b_1 = N \\times 0.8 \\frac{\\text{FLOP}}{\\text{byte}} \\times 12 \\frac{\\text{GB}}{\\text{s}} = N \\times 9.6$ GFLOP/s\n- $I \\times B = 0.8 \\frac{\\text{FLOP}}{\\text{byte}} \\times 60 \\frac{\\text{GB}}{\\text{s}} = 48$ GFLOP/s\n\n因此，以 GFLOP/s 为单位的性能为：\n$$ P(N) = \\min(80N, 9.6N, 48) $$\n对于任何正数核心数 $N \\ge 1$，我们有 $9.6N  80N$。因此，$80N$ 这一项永远不会是最小值，表达式简化为：\n$$ P(N) = \\min(9.6N, 48) $$\n这证实了对于给定的算术强度，在指定范围内的任何核心数 $N$ 下，内核的性能总是受限于内存子系统，而非核心的峰值计算能力。\n\n问题将并行加速比 $S(N)$ 定义为使用 $N$ 个核心的可达性能与使用单个核心的可达性能之比。首先，我们必须计算单核性能 $P(1)$：\n$$ P(1) = \\min(9.6 \\times 1, 48) = \\min(9.6, 48) = 9.6 \\text{ GFLOP/s} $$\n\n现在，我们可以表达加速比函数 $S(N)$：\n$$ S(N) = \\frac{P(N)}{P(1)} = \\frac{\\min(9.6N, 48)}{9.6} $$\n我们可以将除法分配到 $\\min$ 函数内部：\n$$ S(N) = \\min\\left(\\frac{9.6N}{9.6}, \\frac{48}{9.6}\\right) $$\n$$ S(N) = \\min(N, 5) $$\n\n问题要求在整数核心数 $N$（范围为 $1 \\leq N \\leq N_{\\max}$，其中 $N_{\\max} = 32$）内的最大可能加速比 $S_{\\max}$。\n函数 $S(N) = \\min(N, 5)$ 的行为如下：\n- 对于 $N  5$，$S(N) = N$。加速比是线性的，并随 $N$ 增加而增加。\n- 对于 $N \\ge 5$，$S(N) = 5$。加速比在常数值 $5$ 处饱和。\n\n函数 $S(N)$ 是一个关于 $N$ 的单调非递减函数。它在定义域 $1 \\leq N \\leq 32$ 上的最大值将是其达到饱和时的值。\n- $S(1) = 1$\n- $S(2) = 2$\n- $S(3) = 3$\n- $S(4) = 4$\n- $S(5) = 5$\n- $S(6) = 5$\n- ...\n- $S(32) = 5$\n\n在给定范围内的 $N$ 值下，$S(N)$ 的最大值为 $5$。这个最大值在 $N=5$ 个核心时首次达到，并对所有更多核心数（直至 $N_{\\max}=32$）保持不变。这种加速比的限制是 Amdahl 定律的一个典型例子，其中瓶颈（在此例中为系统内存总线带宽 $B$）限制了可达到的并行性能提升。\n\n因此，最大可能加速比 $S_{\\max}$ 是 $5$。",
            "answer": "$$\\boxed{5}$$"
        }
    ]
}