{
    "hands_on_practices": [
        {
            "introduction": "The best way to truly grasp a scientific law is to build it from its foundational concepts. This first practice guides you through the process of deriving Gustafson's Law from the first principles of weak scaling, where the problem size grows with the number of processors. By translating this derivation into a computational script, you will verify the self-consistency of the model and solidify your understanding of how sequential and parallel work components combine to determine scaled speedup .",
            "id": "3139767",
            "problem": "You are asked to formalize and verify a weak-scaling prediction attributed to Gustafson’s law in the context of High-Performance Computing (HPC). The goal is to build from first principles, without assuming any end formula, and to implement a program that verifies the resulting prediction numerically on a small test suite.\n\nStart from these foundational and widely accepted definitions:\n- Let $N$ denote the number of parallel workers (for example, Central Processing Unit (CPU) cores).\n- Under weak scaling, the total problem size grows in proportion to $N$ so that the per-worker wall time remains approximately constant.\n- Decompose the total work of a parallel program for a given workload into a non-parallelizable (sequential) fraction and a perfectly parallelizable fraction. Define a parameter $\\alpha \\in [0,1]$ to be the fraction of time spent in the non-parallelizable part when running a baseline workload on a single worker, and $1-\\alpha$ to be the fraction of time spent in the parallelizable part for that same baseline workload.\n- Define the speedup $S(N)$ for weak scaling as the ratio of the time a single worker would take to execute the scaled workload of size proportional to $N$ to the time $N$ workers actually take to execute that scaled workload.\n\nFrom these bases alone, derive an expression for $S(N)$ in terms of $N$ and $\\alpha$ by reasoning about how the sequential and parallelizable parts of the work transform under weak scaling when the total workload is scaled to keep the per-worker wall time constant.\n\nImplementation requirements:\n- Take the baseline per-worker wall time as $T_{\\text{base}} = 1$ in arbitrary but consistent time units; no physical unit conversion is required.\n- For each test case ($N,\\alpha$), construct:\n  1. The modeled parallel execution time on $N$ workers for the scaled workload, $T_N$, using only the above definitions and the constraint of weak scaling (per-worker wall time constant at $T_{\\text{base}}=1$).\n  2. The modeled single-worker execution time for the same scaled workload, $T_1(N)$.\n  3. The measured speedup $S_{\\text{meas}}(N) = T_1(N)/T_N$.\n  4. The theoretically derived speedup $S_{\\text{theory}}(N)$ from your derivation.\n  5. A boolean verdict per test case that is true if $|S_{\\text{meas}}(N) - S_{\\text{theory}}(N)| \\le \\varepsilon$ with tolerance $\\varepsilon = 10^{-12}$, and false otherwise.\n\nTest suite:\n- Use the following five ($N,\\alpha$) pairs, which together cover a general use case, boundary values, and edge cases:\n  - ($N,\\alpha$) = (1, 0.3)\n  - ($N,\\alpha$) = (8, 0.1)\n  - ($N,\\alpha$) = (64, 0)\n  - ($N,\\alpha$) = (16, 1)\n  - ($N,\\alpha$) = (32, 0.25)\n\nAnswer format:\n- Your program must produce a single line of output containing a list of booleans for the five test cases in the order given, formatted as a comma-separated Python-like list (for example, $[\\text{True},\\text{False},\\ldots]$). No other output is allowed.\n- All intermediate quantities are dimensionless real numbers and do not require units.",
            "solution": "The problem statement has been subjected to a rigorous validation process. All givens were extracted verbatim, and the problem was analyzed against the criteria of scientific soundness, well-posedness, and objectivity.\n\n**Givens:**\n1.  $N$: number of parallel workers.\n2.  Weak scaling: total problem size grows in proportion to $N$ so that the per-worker wall time remains approximately constant.\n3.  $\\alpha$: fraction of time spent in the non-parallelizable part when running a baseline workload on a single worker, with $\\alpha \\in [0,1]$.\n4.  $1-\\alpha$: fraction of time spent in the parallelizable part for the same baseline workload.\n5.  $S(N)$: weak scaling speedup, defined as the ratio of the time a single worker would take to execute the scaled workload to the time $N$ workers take.\n6.  $T_{\\text{base}} = 1$: baseline per-worker wall time.\n7.  Tolerance $\\varepsilon = 10^{-12}$.\n8.  Test cases: ($N,\\alpha$) = (1, 0.3), (8, 0.1), (64, 0), (16, 1), (32, 0.25).\n\n**Verdict:**\nThe problem is **valid**. It is a scientifically grounded and well-posed exercise in deriving and verifying Gustafson's law from first principles, a fundamental concept in computational science. The definitions are clear, consistent, and sufficient for deriving a unique and meaningful solution.\n\nWe will now proceed with the derivation and subsequent numerical verification.\n\n### Derivation of Gustafson's Law from First Principles\n\nThe objective is to derive an expression for the weak-scaling speedup, $S(N)$, as a function of the number of workers, $N$, and the sequential fraction of the baseline program, $\\alpha$.\n\n**Step 1: Baseline Workload Analysis ($N=1$)**\n\nLet us consider the baseline workload executed on a single worker ($N=1$). The total execution time is given as $T_{\\text{base}}$, which we set to $1$ arbitrary time unit.\n$$\nT_1(\\text{base}) = T_{\\text{base}} = 1\n$$\nThis total time is composed of a sequential (non-parallelizable) part and a parallelizable part. According to the problem definition, $\\alpha$ is the fraction of time spent in the sequential part.\n-   Time for the sequential part: $T_{s,1} = \\alpha T_1(\\text{base}) = \\alpha \\cdot 1 = \\alpha$.\n-   Time for the parallelizable part: $T_{p,1} = (1-\\alpha) T_1(\\text{base}) = (1-\\alpha) \\cdot 1 = 1-\\alpha$.\n\nIt is useful to think in terms of the amount of computational work, which is proportional to execution time on a single worker. Assuming a single worker performs one unit of work per unit of time, the total work for the baseline case, $W_{\\text{base}}$, is:\n-   Sequential work: $W_s = T_{s,1} = \\alpha$.\n-   Parallelizable work: $W_p = T_{p,1} = 1-\\alpha$.\n\n**Step 2: Scaled Workload Analysis (for $N$ workers)**\n\nUnder weak scaling, the total problem size is increased to keep the execution time per worker constant. This is achieved by scaling the parallelizable portion of the work, while the sequential portion is assumed to remain fixed, as it often relates to overheads or problem setup/teardown that do not scale with data size.\n\n-   The sequential work for the scaled problem, $W_{s,N}$, remains constant: $W_{s,N} = W_s = \\alpha$.\n-   The parallelizable work, $W_{p,N}$, is scaled linearly with the number of workers: $W_{p,N} = N \\cdot W_p = N(1-\\alpha)$.\n\nThe total work for this new, scaled problem is the sum of its sequential and parallelizable components:\n$$\nW_{\\text{total}}(N) = W_{s,N} + W_{p,N} = \\alpha + N(1-\\alpha)\n$$\n\n**Step 3: Calculating Execution Times for the Scaled Workload**\n\nWe now calculate the execution time for this total work, $W_{\\text{total}}(N)$, on two different machine configurations: a single worker and $N$ workers.\n\n-   **Time on a single worker, $T_1(N)$**: A single worker must perform all the work, both sequential and parallelizable.\n    $$\n    T_1(N) = W_{\\text{total}}(N) = \\alpha + N(1-\\alpha)\n    $$\n-   **Time on $N$ workers, $T_N(N)$**: The $N$ workers execute the scaled workload in parallel.\n    -   The sequential work $W_{s,N}$ cannot be parallelized and must be executed by one worker, taking time $\\alpha$.\n    -   The parallelizable work $W_{p,N}$ is perfectly distributed among the $N$ workers. The time taken for this part is therefore $\\frac{W_{p,N}}{N} = \\frac{N(1-\\alpha)}{N} = 1-\\alpha$.\n    -   The total execution time on $N$ workers, $T_N(N)$, is the sum of the time for the sequential and parallel parts:\n        $$\n        T_N(N) = \\alpha + (1-\\alpha) = 1\n        $$\n    This result confirms the premise of weak scaling: the wall time for the scaled problem on $N$ workers is constant and equal to the baseline time, $T_{\\text{base}}=1$. For conciseness, we denote $T_N(N)$ as $T_N$.\n\n**Step 4: Deriving the Speedup Formula**\n\nThe problem defines the weak-scaling speedup, $S(N)$, as the ratio of the time a single worker would take to execute the scaled workload to the time $N$ workers actually take.\n$$\nS(N) = \\frac{T_1(N)}{T_N}\n$$\nSubstituting the expressions derived in Step 3:\n$$\nS(N) = \\frac{\\alpha + N(1-\\alpha)}{1}\n$$\nThis gives the theoretical formula for Gustafson's Law:\n$$\nS_{\\text{theory}}(N) = \\alpha + N(1-\\alpha)\n$$\n\n### Numerical Verification Plan\n\nThe implementation will test the consistency of this derivation. For each test case ($N, \\alpha$):\n1.  The parallel execution time for the scaled workload is $T_N = 1$.\n2.  The single-worker execution time for the scaled workload is $T_1(N) = \\alpha + N(1-\\alpha)$.\n3.  The \"measured\" speedup is calculated as $S_{\\text{meas}}(N) = T_1(N) / T_N$.\n4.  The \"theoretical\" speedup is calculated using the derived formula, $S_{\\text{theory}}(N) = \\alpha + N(1-\\alpha)$.\n5.  By construction, $S_{\\text{meas}}(N) = S_{\\text{theory}}(N)$. The verification check, $|S_{\\text{meas}}(N) - S_{\\text{theory}}(N)| \\le \\varepsilon$, will therefore confirm the self-consistency of the implementation of the derived model. For all valid inputs, the result of this check must be true.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies Gustafson's law for weak scaling speedup.\n\n    For each test case (N, alpha), the program calculates the theoretical speedup\n    and a \"measured\" speedup based on a first-principles model of execution time,\n    then compares them.\n    \"\"\"\n    # Define the test cases from the problem statement: (N, alpha) pairs.\n    # N is the number of workers, alpha is the sequential fraction.\n    test_cases = [\n        (1, 0.3),    # Base case N=1\n        (8, 0.1),    # General case\n        (64, 0.0),   # Edge case: perfectly parallelizable\n        (16, 1.0),   # Edge case: purely sequential\n        (32, 0.25)   # General case\n    ]\n\n    # Tolerance for floating-point comparison\n    epsilon = 1e-12\n\n    results = []\n    for N, alpha in test_cases:\n        # Cast N to float to ensure floating-point arithmetic throughout\n        N = float(N)\n\n        # Step 1: Modeled parallel execution time on N workers, T_N.\n        # In weak scaling, problem size is increased to keep per-worker time\n        # constant. Starting with a baseline time T_base = 1, the time on\n        # N workers for the scaled problem, T_N, is also 1.\n        # T_N_sequential = alpha\n        # T_N_parallel = (N * (1 - alpha)) / N = 1 - alpha\n        # T_N = T_N_sequential + T_N_parallel = alpha + (1 - alpha) = 1.0\n        T_N = 1.0\n\n        # Step 2: Modeled single-worker execution time for the scaled workload, T_1(N).\n        # A single worker must perform all the work of the scaled problem.\n        # The sequential work is 'alpha'.\n        # The parallelizable work, scaled by N, is N * (1 - alpha).\n        T_1_N = alpha + N * (1.0 - alpha)\n\n        # Step 3: Measured speedup, S_meas(N), based on the definition.\n        # S_meas(N) = T_1(N) / T_N\n        S_meas_N = T_1_N / T_N\n\n        # Step 4: Theoretically derived speedup, S_theory(N) (Gustafson's Law).\n        # This is the formula derived from first principles in the solution text.\n        S_theory_N = alpha + N * (1.0 - alpha)\n\n        # Step 5: Boolean verdict.\n        # Check if the measured speedup matches the theoretical formula\n        # within the given tolerance. By construction, they should be identical.\n        verdict = abs(S_meas_N - S_theory_N) <= epsilon\n        results.append(verdict)\n\n    # Final print statement in the exact required format.\n    # The output is a list of booleans, formatted as a string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world parallel applications often exhibit more complex behaviors than the simple constant-fraction model suggests. This advanced practice explores a more dynamic scenario where the serial fraction, $f(p)$, changes as the number of processors $p$ increases—a common case in applications like adaptive mesh refinement. By analyzing this model, you will connect scaled speedup to the crucial metric of parallel efficiency, $E(p)$, and investigate the long-term, asymptotic scaling behavior of the system to determine if it can approach ideal linear speedup .",
            "id": "3139830",
            "problem": "A High-Performance Computing (HPC) team plans to scale a simulation so that the wall-clock time remains fixed while increasing the number of processors. Assume the computation consists of an inherently serial portion and a perfectly parallelizable portion with no additional overhead beyond the serial part. Let the fraction of the total run time on the parallel machine spent in the serial portion be denoted by $\\alpha$, and let the number of processors be $N$. The team uses the definition of scaled speedup: the ratio of the time a single processor would require to execute the same scaled workload to the time $N$ processors actually take when the wall-clock time is held fixed.\n\nUsing only the above definitions as your starting point, determine the maximum allowable serial fraction $\\alpha$ that will meet a design target scaled speedup of $S(N)=120$ on $N=128$ processors. Do not cite any memorized formulas; derive what you need from first principles. Report your final $\\alpha$ as a pure number rounded to four significant figures.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- The computation consists of an inherently serial portion and a perfectly parallelizable portion.\n- The wall-clock time is held fixed when scaling.\n- $\\alpha$: the fraction of the total run time on the parallel machine spent in the serial portion.\n- $N$: the number of processors.\n- Scaled speedup is defined as: the ratio of the time a single processor would require to execute the same scaled workload to the time $N$ processors actually take.\n- Design target scaled speedup: $S(N) = 120$.\n- Number of processors: $N = 128$.\n- Constraint: The derivation must be from first principles, without citing memorized formulas.\n- Required output: The value of $\\alpha$ as a pure number rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it describes a standard model of parallel computation performance analysis known as scaled speedup, which is the basis for Gustafson's law. It is well-posed, providing a clear definition of scaled speedup and sufficient numerical data ($S(N)$ and $N$) to solve for the unknown variable $\\alpha$. The terminology is objective and precise. The problem does not violate any fundamental principles, is not incomplete or contradictory, and presents a solvable, non-trivial challenge within the field of computational science.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived from first principles as stipulated.\n\n### Derivation\nLet $T(N)$ be the total wall-clock time for a job running on $N$ processors. The problem states this time is held fixed. For convenience and without loss of generality, we can normalize this time to $T(N)=1$ unit.\n\nAccording to the problem definition, $\\alpha$ is the fraction of the total run time on the parallel machine ($N$ processors) that is spent in the serial portion.\nThe time spent on the serial portion on $N$ processors is:\n$$T_{\\text{serial}, N} = \\alpha T(N)$$\nThe remaining time is spent on the parallelizable portion:\n$$T_{\\text{parallel}, N} = (1 - \\alpha) T(N)$$\nThe sum is $T_{\\text{serial}, N} + T_{\\text{parallel}, N} = \\alpha T(N) + (1-\\alpha)T(N) = T(N)$, which is consistent.\n\nNext, we must determine the time it would take a single processor ($N=1$) to execute this same scaled workload. Let this time be $T(1)$. The workload consists of a serial part and a parallel part.\n\nThe serial part of the workload is, by definition, not parallelizable. Therefore, the time to execute it is independent of the number of processors. The time for the serial part on one processor is the same as the time for the serial part on $N$ processors.\n$$T_{\\text{serial}, 1} = T_{\\text{serial}, N} = \\alpha T(N)$$\n\nThe parallel part of the workload is perfectly parallelizable. When running on $N$ processors, the work is distributed among them, taking time $T_{\\text{parallel}, N}$. If this same amount of parallel work were to be executed on a single processor, it would take $N$ times as long.\n$$T_{\\text{parallel}, 1} = N \\cdot T_{\\text{parallel}, N} = N(1 - \\alpha) T(N)$$\n\nThe total time for the single processor, $T(1)$, is the sum of the times for its serial and parallel parts:\n$$T(1) = T_{\\text{serial}, 1} + T_{\\text{parallel}, 1}$$\n$$T(1) = \\alpha T(N) + N(1 - \\alpha) T(N)$$\nFactoring out $T(N)$, we get:\n$$T(1) = [\\alpha + N(1 - \\alpha)] T(N)$$\n\nThe problem defines the scaled speedup, $S(N)$, as the ratio of the time a single processor would require ($T(1)$) to the time $N$ processors actually take ($T(N)$).\n$$S(N) = \\frac{T(1)}{T(N)}$$\nSubstituting our expression for $T(1)$:\n$$S(N) = \\frac{[\\alpha + N(1 - \\alpha)] T(N)}{T(N)}$$\n$$S(N) = \\alpha + N(1 - \\alpha)$$\nThis expression, derived from first principles, relates the scaled speedup $S(N)$ to the serial fraction $\\alpha$ and the number of processors $N$.\n\nNow we must solve for $\\alpha$.\n$$S(N) = \\alpha + N - N\\alpha$$\n$$S(N) - N = \\alpha - N\\alpha$$\n$$S(N) - N = \\alpha(1 - N)$$\n$$\\alpha = \\frac{S(N) - N}{1 - N}$$\nTo avoid negative signs in the denominator, we can multiply the numerator and denominator by $-1$:\n$$\\alpha = \\frac{N - S(N)}{N - 1}$$\n\nThe problem provides the specific values $N = 128$ and a target speedup of $S(128) = 120$. We substitute these values into the derived equation for $\\alpha$.\n$$\\alpha = \\frac{128 - 120}{128 - 1}$$\n$$\\alpha = \\frac{8}{127}$$\n\nTo provide the final answer, we compute the numerical value and round to four significant figures.\n$$\\alpha \\approx 0.0629921259...$$\nThe first four significant figures are $6$, $2$, $9$, $9$. The fifth digit is $2$, which is less than $5$, so we do not round up the last significant figure.\n$$\\alpha \\approx 0.06299$$\nThis is the maximum allowable serial fraction that meets the specified design target.",
            "answer": "$$\\boxed{0.06299}$$"
        },
        {
            "introduction": "Real-world parallel applications often exhibit more complex behaviors than the simple constant-fraction model suggests. This advanced practice explores a more dynamic scenario where the serial fraction, $f(p)$, changes as the number of processors $p$ increases—a common case in applications like adaptive mesh refinement. By analyzing this model, you will connect scaled speedup to the crucial metric of parallel efficiency, $E(p)$, and investigate the long-term, asymptotic scaling behavior of the system to determine if it can approach ideal linear speedup .",
            "id": "3169108",
            "problem": "A scientific computing team is studying parallel performance for an Adaptive Mesh Refinement (AMR) simulation under the fixed-time scaling viewpoint (Gustafson’s perspective). Let $S(p)$ denote speedup on $p$ identical processors, defined as the ratio of the time to execute the scaled problem on $1$ processor to the time to execute that same scaled problem on $p$ processors. Let $E(p) = S(p)/p$ denote parallel efficiency. In the fixed-time scaling viewpoint, the total wall-clock time is held approximately constant as $p$ grows by increasing the problem size so that the parallelizable portion grows with $p$, while the inherently serial portion does not. Let $T_s$ and $T_p$ denote, respectively, the serial and parallelizable contributions to the wall-clock time on $p$ processors, so that the execution time on $p$ processors is $T_s + T_p$ and, on $1$ processor for the scaled problem, is $T_s + p\\,T_p$. Define the serial fraction of the scaled problem as $f(p) = T_s/(T_s + T_p)$. Using only these definitions, derive an expression for $S(p)$ in terms of $p$ and $f(p)$, and from it express $E(p)$.\n\nNow consider an AMR workload where the serial fraction decreases with $p$ as\n$$\nf(p) = \\frac{0.4}{1 + \\sqrt{p}}.\n$$\nUsing your derived expression, determine the asymptotic behavior of $S(p)$ as $p \\to \\infty$, and identify the smallest integer $p$ for which $E(p) \\ge 0.95$.\n\nWhich option correctly states both the asymptotic behavior and the smallest integer threshold?\n\n- A. As $p \\to \\infty$, $S(p) \\sim p$, and the smallest integer $p$ with $E(p) \\ge 0.95$ is $p = 47$.\n\n- B. As $p \\to \\infty$, $S(p)$ saturates to a constant (independent of $p$), and the smallest integer $p$ with $E(p) \\ge 0.95$ is $p \\approx 9$.\n\n- C. As $p \\to \\infty$, $S(p) \\sim \\sqrt{p}$, and the smallest integer $p$ with $E(p) \\ge 0.95$ is $p \\approx 100$.\n\n- D. As $p \\to \\infty$, $S(p) \\sim p$, but no finite $p$ can achieve $E(p) \\ge 0.95$ because of the serial portion.",
            "solution": "The problem statement will first be validated for correctness, consistency, and scientific grounding.\n\n### Step 1: Extract Givens\n- $S(p)$: Speedup on $p$ identical processors.\n- $p$: Number of processors.\n- Scaling model: Fixed-time scaling (Gustafson's perspective).\n- Definition of speedup: $S(p)$ is the ratio of the time to execute a scaled problem on $1$ processor to the time to execute the same scaled problem on $p$ processors.\n- $E(p)$: Parallel efficiency, defined as $E(p) = S(p)/p$.\n- $T_s$: Inherently serial contribution to wall-clock time, which does not grow with $p$.\n- $T_p$: Parallelizable contribution to wall-clock time on $p$ processors.\n- Execution time on $p$ processors: $T(p) = T_s + T_p$.\n- Execution time on $1$ processor for the scaled problem: $T(1)_{\\text{scaled}} = T_s + p\\,T_p$.\n- The total wall-clock time $T_s + T_p$ is held approximately constant.\n- The problem size is increased such that the parallelizable portion grows with $p$.\n- Definition of serial fraction: $f(p) = T_s/(T_s + T_p)$.\n- Specific workload model for the serial fraction: $f(p) = \\frac{0.4}{1 + \\sqrt{p}}$.\n- The problem asks for:\n    1. An expression for $S(p)$ and $E(p)$ in terms of $p$ and $f(p)$.\n    2. The asymptotic behavior of $S(p)$ as $p \\to \\infty$.\n    3. The smallest integer $p$ for which $E(p) \\ge 0.95$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically grounded within the field of computational science and parallel computing. The definitions provided for speedup, efficiency, and execution times are standard for analyzing performance under Gustafson's model of scaled speedup.\n\n- The definition of speedup is given by $S(p) = \\frac{T(1)_{\\text{scaled}}}{T(p)} = \\frac{T_s + p\\,T_p}{T_s + T_p}$. This is a correct formulation of scaled speedup.\n- The serial fraction $f(p)$ is defined with respect to the execution time on $p$ processors, which is standard in this context.\n- The functional form for $f(p)$ is a hypothetical but physically plausible model, representing a scenario where the serial overhead (e.g., communication) grows much slower than the parallelizable work.\n- The problem is internally consistent. The statement that \"the total wall-clock time is held approximately constant\" ($T_s+T_p \\approx \\text{const}$) while \"the parallelizable portion grows with $p$\" is resolved by understanding that the \"parallelizable portion\" refers to the *work*, not the *time*. If the parallel work is $W_p$, then $T_p = W_p/p$. For $T_p$ and $T_s$ to be constant, $W_p$ must grow proportionally to $p$, which is consistent with the model. In the given problem, $T_s$ and $T_p$ are not necessarily constant, but are functions of $p$ such that $T_s+T_p$ is held constant. However, the explicit expression for $f(p)$ implies a specific relationship between $T_s$ and $T_p$ as $p$ varies, which overrides the general statement. The derivations rely only on the explicit definitions.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed and scientifically sound problem in parallel performance analysis. The solution process can proceed.\n\n### Derivation and Solution\n\n**Part 1: Derive expressions for $S(p)$ and $E(p)$**\nWe are given the definitions for speedup $S(p)$ and serial fraction $f(p)$:\n$$S(p) = \\frac{T_s + p\\,T_p}{T_s + T_p}$$\n$$f(p) = \\frac{T_s}{T_s + T_p}$$\nWe can also express the parallel fraction of the runtime as:\n$$1 - f(p) = 1 - \\frac{T_s}{T_s + T_p} = \\frac{T_s + T_p - T_s}{T_s + T_p} = \\frac{T_p}{T_s + T_p}$$\nNow, we rewrite the numerator and denominator of $S(p)$ in terms of $f(p)$ and $(T_s + T_p)$:\n- Numerator: $T_s + p\\,T_p = f(p)(T_s+T_p) + p(1-f(p))(T_s+T_p) = (f(p) + p(1-f(p)))(T_s+T_p)$\n- Denominator: $T_s + T_p = (1)(T_s+T_p)$\nSubstituting these into the expression for $S(p)$:\n$$S(p) = \\frac{(f(p) + p(1-f(p)))(T_s+T_p)}{T_s+T_p} = f(p) + p(1-f(p))$$\nThis can be rewritten as $S(p) = p + f(p)(1-p)$. This is the standard formula for Gustafson's Law.\n\nNext, we derive the expression for parallel efficiency $E(p)$:\n$$E(p) = \\frac{S(p)}{p} = \\frac{f(p) + p(1-f(p))}{p} = \\frac{f(p)}{p} + 1 - f(p)$$\nThis can be rewritten as $E(p) = 1 - f(p)(1 - \\frac{1}{p}) = 1 - f(p)\\frac{p-1}{p}$.\n\n**Part 2: Asymptotic behavior of $S(p)$**\nWe are given the specific form for the serial fraction:\n$$f(p) = \\frac{0.4}{1 + \\sqrt{p}}$$\nTo find the asymptotic behavior of $S(p)$ as $p \\to \\infty$, we examine the expression $S(p) = p + f(p)(1-p)$.\n$$\\lim_{p \\to \\infty} f(p) = \\lim_{p \\to \\infty} \\frac{0.4}{1 + \\sqrt{p}} = 0$$\nTo determine the asymptotic relationship, we can analyze the ratio $S(p)/p$:\n$$\\lim_{p \\to \\infty} \\frac{S(p)}{p} = \\lim_{p \\to \\infty} E(p) = \\lim_{p \\to \\infty} \\left(1 - f(p) + \\frac{f(p)}{p}\\right)$$\nSince $\\lim_{p \\to \\infty} f(p) = 0$ and $\\lim_{p \\to \\infty} f(p)/p = 0$, we have:\n$$\\lim_{p \\to \\infty} E(p) = 1 - 0 + 0 = 1$$\nA limit of $1$ for $S(p)/p$ means that for large $p$, $S(p)$ behaves like $p$. In asymptotic notation, this is written as $S(p) \\sim p$.\n\n**Part 3: Smallest integer $p$ for which $E(p) \\ge 0.95$**\nWe need to solve the inequality $E(p) \\ge 0.95$. Using our derived expression for $E(p)$:\n$$1 - f(p)\\frac{p-1}{p} \\ge 0.95$$\n$$0.05 \\ge f(p)\\frac{p-1}{p}$$\nSubstitute the given expression for $f(p)$:\n$$0.05 \\ge \\left(\\frac{0.4}{1 + \\sqrt{p}}\\right) \\left(\\frac{p-1}{p}\\right)$$\n$$0.05 \\ge \\frac{0.4(p-1)}{p(1+\\sqrt{p})}$$\nAssuming $p>1$, we can rearrange the inequality:\n$$\\frac{p(1+\\sqrt{p})}{p-1} \\ge \\frac{0.4}{0.05}$$\n$$\\frac{p(1+\\sqrt{p})}{p-1} \\ge 8$$\nLet's solve the corresponding equality to find the boundary value of $p$:\n$$p(1+\\sqrt{p}) = 8(p-1)$$\n$$p + p\\sqrt{p} = 8p - 8$$\n$$p\\sqrt{p} - 7p + 8 = 0$$\nLet $x = \\sqrt{p}$. The equation becomes a cubic polynomial in $x$:\n$$x^3 - 7x^2 + 8 = 0$$\nWe need to find the root of the function $h(x) = x^3 - 7x^2 + 8$. We are interested in solutions for $x > 1$ (since $p>1$). Let's test some values:\n$h(6) = 6^3 - 7(6^2) + 8 = 216 - 7(36) + 8 = 224 - 252 = -28$.\n$h(7) = 7^3 - 7(7^2) + 8 = 343 - 343 + 8 = 8$.\nSince $h(6) < 0$ and $h(7) > 0$, the root $x$ lies between $6$ and $7$.\nLet's find a more accurate value.\nAt $x \\approx 6.8$, $h(6.8) = (6.8)^3 - 7(6.8)^2 + 8 = 314.432 - 7(46.24) + 8 = 322.432 - 323.68 = -1.248$.\nAt $x \\approx 6.9$, $h(6.9) = (6.9)^3 - 7(6.9)^2 + 8 = 328.509 - 7(47.61) + 8 = 336.509 - 333.27 = 3.239$.\nThe root is between $6.8$ and $6.9$.\nThe value of $p$ is $x^2$. So, the crossover point for $p$ is between $(6.8)^2 = 46.24$ and $(6.9)^2 = 47.61$.\nThe inequality is $\\frac{p(1+\\sqrt{p})}{p-1} \\ge 8$. The function on the left increases for $p>4$.\nThus, for any $p$ greater than the root we found, the inequality will hold. The root is approximately $p \\approx 46.65$ (from $x \\approx \\sqrt{46.65} \\approx 6.83$).\nWe need the smallest integer $p$ that satisfies the condition.\nFor $p=46$, $p < 46.65$, so we expect $E(46) < 0.95$.\nFor $p=47$, $p > 46.65$, so we expect $E(47) \\ge 0.95$.\nTherefore, the smallest integer $p$ for which $E(p) \\ge 0.95$ is $47$.\n\n### Option-by-Option Analysis\n\n- **A. As $p \\to \\infty$, $S(p) \\sim p$, and the smallest integer $p$ with $E(p) \\ge 0.95$ is $p = 47$.**\n  - The asymptotic behavior $S(p) \\sim p$ is correct as derived.\n  - The smallest integer $p=47$ for the efficiency condition is correct as calculated.\n  - **Verdict: Correct**\n\n- **B. As $p \\to \\infty$, $S(p)$ saturates to a constant (independent of $p$), and the smallest integer $p$ with $E(p) \\ge 0.95$ is $p \\approx 9$.**\n  - The asymptotic behavior is incorrect. $S(p) \\sim p$, it does not saturate. Saturation is characteristic of Amdahl's Law, not Gustafson's.\n  - The value $p \\approx 9$ is incorrect. Our calculation shows $p=47$.\n  - **Verdict: Incorrect**\n\n- **C. As $p \\to \\infty$, $S(p) \\sim \\sqrt{p}$, and the smallest integer $p$ with $E(p) \\ge 0.95$ is $p \\approx 100$.**\n  - The asymptotic behavior is incorrect. $S(p) \\sim p$, not $\\sqrt{p}$.\n  - While $p = 100$ does satisfy $E(p) \\ge 0.95$, it is not the *smallest* such integer.\n  - **Verdict: Incorrect**\n\n- **D. As $p \\to \\infty$, $S(p) \\sim p$, but no finite $p$ can achieve $E(p) \\ge 0.95$ because of the serial portion.**\n  - The asymptotic behavior $S(p) \\sim p$ is correct.\n  - The claim that no finite $p$ can achieve the efficiency target is incorrect. We explicitly found that for $p \\ge 47$, $E(p) \\ge 0.95$. The efficiency function $E(p)$ approaches $1$ as $p \\to \\infty$, so it must eventually exceed any threshold less than $1$.\n  - **Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}