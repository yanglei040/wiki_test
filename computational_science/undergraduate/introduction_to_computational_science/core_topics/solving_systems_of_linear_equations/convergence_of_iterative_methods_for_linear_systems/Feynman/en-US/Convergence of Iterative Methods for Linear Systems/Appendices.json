{
    "hands_on_practices": [
        {
            "introduction": "When analyzing an iterative method, a straightforward first step is to check if the system's matrix is strictly diagonally dominant. This property provides a simple, sufficient condition that guarantees convergence for methods like Jacobi and Gauss-Seidel. This exercise  sharpens your understanding by having you distinguish between the related concepts of row and column diagonal dominance, which are not equivalent.",
            "id": "2166709",
            "problem": "In numerical linear algebra, certain properties of a matrix can guarantee the convergence of iterative methods used to solve systems of linear equations. One such property is diagonal dominance.\n\nA square matrix $A$ of size $n \\times n$ with entries $a_{ij}$ is defined as **strictly row diagonally dominant** if for every row $i$, the absolute value of the diagonal entry is strictly greater than the sum of the absolute values of all other entries in that row. Mathematically, this is expressed as:\n$$|a_{ii}|  \\sum_{j \\neq i} |a_{ij}| \\quad \\text{for all } i = 1, 2, \\dots, n$$\n\nSimilarly, the matrix $A$ is defined as **strictly column diagonally dominant** if for every column $j$, the absolute value of the diagonal entry is strictly greater than the sum of the absolute values of all other entries in that column. Mathematically, this is expressed as:\n$$|a_{jj}|  \\sum_{i \\neq j} |a_{ij}| \\quad \\text{for all } j = 1, 2, \\dots, n$$\n\nConsider the following 3x3 matrices. Which one of these matrices is strictly column diagonally dominant but is **not** strictly row diagonally dominant?\n\nA.\n$A_A = \\begin{pmatrix} 10  1  2 \\\\ 3  9  4 \\\\ 1  2  8 \\end{pmatrix}$\n\nB.\n$A_B = \\begin{pmatrix} 10  3  8 \\\\ 4  8  5 \\\\ 7  2  12 \\end{pmatrix}$\n\nC.\n$A_C = \\begin{pmatrix} 10  3  6 \\\\ 4  8  5 \\\\ 5  2  12 \\end{pmatrix}$\n\nD.\n$A_D = \\begin{pmatrix} 10  4  5 \\\\ 3  8  2 \\\\ 6  5  12 \\end{pmatrix}$",
            "solution": "We use the definitions:\n- Strict row diagonal dominance (SRDD): for each row index $i$, $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$.\n- Strict column diagonal dominance (SCDD): for each column index $j$, $|a_{jj}|  \\sum_{i \\neq j} |a_{ij}|$.\nAll entries are nonnegative, so $|a_{ij}| = a_{ij}$.\n\nMatrix $A_{A} = \\begin{pmatrix} 10  1  2 \\\\ 3  9  4 \\\\ 1  2  8 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  1 + 2 = 3$.\n- Row $2$: $9  3 + 4 = 7$.\n- Row $3$: $8  1 + 2 = 3$.\nThus SRDD holds.\nColumn checks:\n- Column $1$: $10  3 + 1 = 4$.\n- Column $2$: $9  1 + 2 = 3$.\n- Column $3$: $8  2 + 4 = 6$.\nThus SCDD holds.\nConclusion: $A_{A}$ is both SRDD and SCDD, not the requested case.\n\nMatrix $A_{B} = \\begin{pmatrix} 10  3  8 \\\\ 4  8  5 \\\\ 7  2  12 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  3 + 8 = 11$ is false, so SRDD fails.\nColumn checks:\n- Column $1$: $10  4 + 7 = 11$ is false, so SCDD fails.\nConclusion: $A_{B}$ is neither SCDD nor SRDD, not the requested case.\n\nMatrix $A_{C} = \\begin{pmatrix} 10  3  6 \\\\ 4  8  5 \\\\ 5  2  12 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  3 + 6 = 9$ holds.\n- Row $2$: $8  4 + 5 = 9$ is false, so SRDD fails.\n- Row $3$: $12  5 + 2 = 7$ holds.\nThus not SRDD.\nColumn checks:\n- Column $1$: $10  4 + 5 = 9$ holds.\n- Column $2$: $8  3 + 2 = 5$ holds.\n- Column $3$: $12  6 + 5 = 11$ holds.\nThus SCDD holds.\nConclusion: $A_{C}$ is strictly column diagonally dominant but not strictly row diagonally dominant.\n\nMatrix $A_{D} = \\begin{pmatrix} 10  4  5 \\\\ 3  8  2 \\\\ 6  5  12 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  4 + 5 = 9$.\n- Row $2$: $8  3 + 2 = 5$.\n- Row $3$: $12  6 + 5 = 11$.\nThus SRDD holds.\nColumn checks:\n- Column $1$: $10  3 + 6 = 9$ holds.\n- Column $2$: $8  4 + 5 = 9$ is false, so SCDD fails.\nConclusion: $A_{D}$ is SRDD but not SCDD, not the requested case.\n\nTherefore, the only matrix that is strictly column diagonally dominant but not strictly row diagonally dominant is $A_{C}$.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "While diagonal dominance is a useful shortcut, the definitive test for convergence of a stationary iteration is whether the spectral radius of its iteration matrix, $\\rho(T)$, is less than 1. This coding challenge  brings this core theory to life by having you numerically verify the link between the spectral radius and convergence. You will analyze a fascinating case where the Jacobi method fails but the Gauss-Seidel method succeeds for the same system, revealing their distinct stability properties.",
            "id": "3205095",
            "problem": "Design and implement a complete, runnable program that constructs and analyzes linear systems to evaluate the robustness and stability of iterative solvers. Use the fundamental base that iterative methods for linear systems are derived from matrix splittings and that their convergence is governed by the spectral radius criterion. Specifically, for a linear system with matrix splitting, the Jacobi method and the Gauss-Seidel method are defined using matrices built from the coefficient matrix. The convergence of either method is ensured when the spectral radius of the associated iteration matrix is strictly less than one. The program must demonstrate a case where the Jacobi iteration diverges while the Gauss-Seidel iteration converges, explain the mechanism from first principles, and verify it numerically. All symbols, variables, functions, operators, and numbers must be written in LaTeX.\n\nUse the following foundational definitions and facts.\n\n- For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, define the diagonal part $D$, the strictly lower triangular part $L$, and the strictly upper triangular part $U$ such that $A = D + L + U$.\n- The Jacobi iteration is defined by the update $x^{k+1} = D^{-1}\\left(b - (L + U)x^{k}\\right)$, which can be written as $x^{k+1} = T_{J} x^{k} + c_{J}$ with iteration matrix $T_{J} = -D^{-1}(L+U)$.\n- The Gauss-Seidel iteration is defined by the update $x^{k+1} = (D + L)^{-1}\\left(b - U x^{k}\\right)$, which can be written as $x^{k+1} = T_{GS} x^{k} + c_{GS}$ with iteration matrix $T_{GS} = -(D+L)^{-1}U$.\n- A well-tested convergence fact is that an iterative method $x^{k+1} = T x^{k} + c$ converges for any initial vector if and only if the spectral radius $\\rho(T)  1$. Another well-tested fact is that for any symmetric positive definite matrix (SPD), the Gauss-Seidel method converges for any right-hand side $b$.\n\nYour program must:\n- For each test case, construct $D$, $L$, $U$, form $T_{J}$ and $T_{GS}$, compute their spectral radii $\\rho(T_{J})$ and $\\rho(T_{GS})$, run both iterative methods from $x^{0} = 0$ for a maximum of $N$ iterations with tolerance $\\varepsilon$ on the residual $\\|b - A x^{k}\\|_{2}$, and report whether each method converged.\n- Use a residual tolerance $\\varepsilon = 10^{-10}$ and a maximum iteration count $N = 500$. No physical units are involved.\n\nTest suite. Analyze the following four linear systems, specified by their coefficient matrices $A$ and right-hand sides $b$:\n\n- Case $1$ (target case: Jacobi diverges, Gauss-Seidel converges): \n  $$A_{1} = \\begin{bmatrix} \\tfrac{3}{2}  1  1 \\\\ 1  \\tfrac{3}{2}  1 \\\\ 1  1  \\tfrac{3}{2} \\end{bmatrix}, \\quad b_{1} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}.$$\n- Case $2$ (strictly diagonally dominant, both converge):\n  $$A_{2} = \\begin{bmatrix} 4  1  1 \\\\ 1  4  1 \\\\ 1  1  4 \\end{bmatrix}, \\quad b_{2} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n- Case $3$ (singular and ill-posed, both fail to converge under the given criteria):\n  $$A_{3} = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{bmatrix}, \\quad b_{3} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n- Case $4$ (borderline for Jacobi with $\\rho(T_{J}) = 1$, Gauss-Seidel converges):\n  $$A_{4} = \\begin{bmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{bmatrix}, \\quad b_{4} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[\\rho(T_{J}), \\rho(T_{GS}), \\text{JacobiConverged}, \\text{GaussSeidelConverged}]$, where $\\rho(T_{J})$ and $\\rho(T_{GS})$ are floating-point numbers and the convergence indicators are booleans. For example, the output must look like \n$$[\\,[\\rho_{1J}, \\rho_{1GS}, \\text{bool}, \\text{bool}], [\\rho_{2J}, \\rho_{2GS}, \\text{bool}, \\text{bool}], [\\rho_{3J}, \\rho_{3GS}, \\text{bool}, \\text{bool}], [\\rho_{4J}, \\rho_{4GS}, \\text{bool}, \\text{bool}]\\,].$$",
            "solution": "The core principle is that classical stationary iterative methods arise from matrix splittings of the linear system and converge when their iteration matrices contract the error; this is ensured if the spectral radius is strictly less than one. Starting from the fundamental definition $A = D + L + U$ where $D$ is the diagonal of $A$, $L$ is the strictly lower triangular part, and $U$ is the strictly upper triangular part, the Jacobi and Gauss-Seidel methods are defined by\n$$x^{k+1} = D^{-1}\\left(b - (L + U)x^{k}\\right), \\quad x^{k+1} = (D + L)^{-1}\\left(b - U x^{k}\\right),$$\nrespectively. These can be expressed as stationary iterations $x^{k+1} = T x^{k} + c$ with $T_{J} = -D^{-1}(L+U)$ for Jacobi and $T_{GS} = -(D+L)^{-1} U$ for Gauss-Seidel. A well-tested fact in numerical analysis is that such stationary iterations converge for any initial vector $x^{0}$ if and only if $\\rho(T)  1$, where $\\rho(T)$ denotes the spectral radius, the maximum magnitude of the eigenvalues of $T$. Another well-tested and fundamental result is that the Gauss-Seidel method converges for any symmetric positive definite (SPD) matrix $A$.\n\nWe now explain how to construct a matrix where the Jacobi iteration diverges while the Gauss-Seidel iteration converges, and why this happens. Consider the $n \\times n$ matrix family with constant diagonal and constant off-diagonal entries,\n$$A(\\alpha,\\beta) = \\alpha I + \\beta ( \\mathbf{1}\\mathbf{1}^{\\top} - I ),$$\nthat is, $A(\\alpha,\\beta)$ has diagonal entries equal to $\\alpha$ and off-diagonal entries equal to $\\beta$. For $n = 3$, this is\n$$A(\\alpha,\\beta) = \\begin{bmatrix} \\alpha  \\beta  \\beta \\\\ \\beta  \\alpha  \\beta \\\\ \\beta  \\beta  \\alpha \\end{bmatrix}.$$\nThis matrix is symmetric by construction. Its eigenstructure is well known: the eigenvalues are\n$$\\lambda_{1} = \\alpha - \\beta \\quad \\text{(with multiplicity } n-1 \\text{)}, \\qquad \\lambda_{2} = \\alpha + (n-1)\\beta \\quad \\text{(with multiplicity } 1 \\text{)}.$$\nTherefore, $A(\\alpha,\\beta)$ is symmetric positive definite precisely when $\\alpha - \\beta  0$, that is, $\\alpha  \\beta$, since both $\\lambda_{1}$ and $\\lambda_{2}$ are then strictly positive.\n\nFor the Jacobi method, we compute its iteration matrix. Since $D = \\alpha I$ and $L+U = \\beta(\\mathbf{1}\\mathbf{1}^{\\top} - I)$ for this family, the Jacobi iteration matrix is\n$$T_{J} = -D^{-1}(L+U) = -\\frac{\\beta}{\\alpha} (\\mathbf{1}\\mathbf{1}^{\\top} - I).$$\nThe eigenvalues of $\\mathbf{1}\\mathbf{1}^{\\top} - I$ are $(n-1)$ for the eigenvector $\\mathbf{1}$ and $-1$ for any vector orthogonal to $\\mathbf{1}$. Consequently, the eigenvalues of $T_{J}$ are\n$$\\mu_{1} = -\\frac{\\beta}{\\alpha} (n-1), \\qquad \\mu_{2} = \\frac{\\beta}{\\alpha} \\quad \\text{(with multiplicity } n-1 \\text{)}.$$\nThe spectral radius is thus\n$$\\rho(T_{J}) = \\max\\left\\{ \\left| -\\frac{\\beta}{\\alpha}(n-1) \\right|, \\left| \\frac{\\beta}{\\alpha} \\right| \\right\\} = \\frac{\\beta}{\\alpha}(n-1).$$\nFor $n = 3$, we obtain $\\rho(T_{J}) = \\dfrac{2\\beta}{\\alpha}$. The Jacobi method will diverge if $\\rho(T_{J}) \\geq 1$, that is, if $\\dfrac{2\\beta}{\\alpha} \\geq 1$. If we choose $\\alpha = \\tfrac{3}{2}$ and $\\beta = 1$, then $\\alpha  \\beta$ and $A(\\alpha,\\beta)$ is SPD, but\n$$\\rho(T_{J}) = \\frac{2 \\cdot 1}{\\tfrac{3}{2}} = \\frac{4}{3}  1,$$\nso the Jacobi method diverges according to the spectral radius criterion.\n\nFor the Gauss-Seidel method, the fundamental convergence result states that for any SPD matrix $A$, Gauss-Seidel converges. Since $A\\left(\\tfrac{3}{2}, 1\\right)$ is SPD, Gauss-Seidel converges even though Jacobi diverges. This shows the increased robustness of Gauss-Seidel relative to Jacobi in this setting.\n\nTo provide additional context on why such divergence cannot occur in the $2 \\times 2$ case under the standard ordering, consider $A = \\begin{bmatrix} a  b \\\\ c  d \\end{bmatrix}$ with $D = \\operatorname{diag}(a,d)$, $L = \\begin{bmatrix} 0  0 \\\\ c  0 \\end{bmatrix}$, and $U = \\begin{bmatrix} 0  b \\\\ 0  0 \\end{bmatrix}$. The Jacobi iteration matrix is\n$$T_{J} = -D^{-1}(L+U) = \\begin{bmatrix} 0  -\\frac{b}{a} \\\\ -\\frac{c}{d}  0 \\end{bmatrix},$$\nwhose eigenvalues satisfy $\\lambda^{2} = \\dfrac{bc}{ad}$, hence $\\rho(T_{J}) = \\sqrt{\\left|\\dfrac{bc}{ad}\\right|}$. The Gauss-Seidel iteration matrix is\n$$T_{GS} = -(D+L)^{-1}U = \\begin{bmatrix} 0  -\\frac{b}{a} \\\\ 0  \\frac{bc}{ad} \\end{bmatrix},$$\nwhich is upper triangular and has eigenvalues $0$ and $\\dfrac{bc}{ad}$, hence $\\rho(T_{GS}) = \\left|\\dfrac{bc}{ad}\\right|$. Therefore, in $2 \\times 2$, both methods converge or diverge together under the same inequality $\\left|\\dfrac{bc}{ad}\\right|  1$, and one cannot construct a counterexample of Jacobi diverging while Gauss-Seidel converging. This explains why the requested example requires dimension $n \\geq 3$.\n\nThe test suite is designed to cover several facets:\n- Case $1$ provides the primary scenario where Jacobi diverges ($\\rho(T_{J})  1$) while Gauss-Seidel converges due to $A$ being SPD.\n- Case $2$ is strictly diagonally dominant, a situation where both methods are known to converge; here, $\\rho(T_{J})$ and $\\rho(T_{GS})$ will be well below $1$.\n- Case $3$ is singular and ill-posed; neither method should satisfy the spectral radius convergence criterion, and numerically the residual will not reach the specified tolerance.\n- Case $4$ is the borderline where $\\rho(T_{J}) = 1$; Jacobi typically fails to converge (no contraction), while Gauss-Seidel converges since $A$ is SPD.\n\nThe program computes $T_{J}$ and $T_{GS}$ directly from $D$, $L$, and $U$, evaluates their spectral radii via eigenvalues, and performs iterations from $x^{0} = 0$ up to $N = 500$ steps with tolerance $\\varepsilon = 10^{-10}$ using the residual norm $\\|b - A x^{k}\\|_{2}$. The results are formatted exactly as required: each case contributes a list $[\\rho(T_{J}), \\rho(T_{GS}), \\text{JacobiConverged}, \\text{GaussSeidelConverged}]$, and the final output is a single line list of these case results. This ties the theoretical convergence criteria to practical numerical behavior, illustrating algorithm robustness and stability with clear quantitative checks.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef spectral_radius(mat: np.ndarray) - float:\n    \"\"\"Compute spectral radius (max absolute eigenvalue) of a square matrix.\"\"\"\n    eigvals = np.linalg.eigvals(mat)\n    return float(np.max(np.abs(eigvals)))\n\ndef jacobi_iteration(A: np.ndarray, b: np.ndarray, max_iter: int = 500, tol: float = 1e-10) - bool:\n    \"\"\"Run Jacobi iteration starting from x0=0, return True if residual norm = tol.\"\"\"\n    n = A.shape[0]\n    D = np.diag(np.diag(A))\n    L = np.tril(A, -1)\n    U = np.triu(A, 1)\n    # Precompute D^{-1}\n    try:\n        D_inv = np.linalg.inv(D)\n    except np.linalg.LinAlgError:\n        return False\n    x = np.zeros(n, dtype=float)\n    for _ in range(max_iter):\n        x = D_inv @ (b - (L + U) @ x)\n        r = b - A @ x\n        if np.linalg.norm(r, 2) = tol:\n            return True\n    return False\n\ndef gauss_seidel_iteration(A: np.ndarray, b: np.ndarray, max_iter: int = 500, tol: float = 1e-10) - bool:\n    \"\"\"Run Gauss-Seidel iteration starting from x0=0, return True if residual norm = tol.\"\"\"\n    n = A.shape[0]\n    D = np.diag(np.diag(A))\n    L = np.tril(A, -1)\n    U = np.triu(A, 1)\n    DL = D + L\n    try:\n        DL_inv = np.linalg.inv(DL)\n    except np.linalg.LinAlgError:\n        return False\n    x = np.zeros(n, dtype=float)\n    for _ in range(max_iter):\n        x = DL_inv @ (b - U @ x)\n        r = b - A @ x\n        if np.linalg.norm(r, 2) = tol:\n            return True\n    return False\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A1 = np.array([[1.5, 1.0, 1.0],\n                   [1.0, 1.5, 1.0],\n                   [1.0, 1.0, 1.5]], dtype=float)\n    b1 = np.array([1.0, -1.0, 2.0], dtype=float)\n\n    A2 = np.array([[4.0, 1.0, 1.0],\n                   [1.0, 4.0, 1.0],\n                   [1.0, 1.0, 4.0]], dtype=float)\n    b2 = np.array([1.0, 2.0, 3.0], dtype=float)\n\n    A3 = np.array([[1.0, 1.0, 1.0],\n                   [1.0, 1.0, 1.0],\n                   [1.0, 1.0, 1.0]], dtype=float)\n    b3 = np.array([1.0, 1.0, 1.0], dtype=float)\n\n    A4 = np.array([[2.0, 1.0, 1.0],\n                   [1.0, 2.0, 1.0],\n                   [1.0, 1.0, 2.0]], dtype=float)\n    b4 = np.array([0.0, 0.0, 1.0], dtype=float)\n\n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n        (A4, b4),\n    ]\n\n    results = []\n    for A, b in test_cases:\n        D = np.diag(np.diag(A))\n        L = np.tril(A, -1)\n        U = np.triu(A, 1)\n        # Iteration matrices\n        # Use safe inverses; if singular, mark spectral radii as np.nan\n        try:\n            TJ = -np.linalg.inv(D) @ (L + U)\n            rho_J = spectral_radius(TJ)\n        except np.linalg.LinAlgError:\n            rho_J = float('nan')\n        try:\n            TGS = -np.linalg.inv(D + L) @ U\n            rho_GS = spectral_radius(TGS)\n        except np.linalg.LinAlgError:\n            rho_GS = float('nan')\n\n        jacobi_conv = jacobi_iteration(A, b, max_iter=500, tol=1e-10)\n        gs_conv = gauss_seidel_iteration(A, b, max_iter=500, tol=1e-10)\n\n        results.append([rho_J, rho_GS, jacobi_conv, gs_conv])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(item) for item in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond classic deterministic solvers, many modern large-scale problems leverage stochastic methods that converge in a probabilistic sense. The Randomized Kaczmarz method is a prime example, where we analyze the expected error reduction at each step instead of a guaranteed reduction. This advanced practice  guides you through the analysis and implementation of such a method, connecting iterative algorithms to the frontiers of optimization and randomized numerical linear algebra.",
            "id": "3113564",
            "problem": "You will investigate the convergence of a Stochastic Gradient Descent (SGD)-like version of the Randomized Kaczmarz method applied to a consistent linear system $A x = b$. Let $A \\in \\mathbb{R}^{m \\times n}$ have rows $a_i^{\\top}$ for $i \\in \\{1,\\dots,m\\}$. Define the error at iteration $k$ as $e_k = x_k - x^\\star$, where $x^\\star$ is any exact solution of the consistent system. The method updates by selecting a row index $i$ at random according to a prescribed probability distribution with probabilities $\\{p_i\\}_{i=1}^m$, and performing the orthogonal projection of $x_k$ onto the hyperplane $\\{x: a_i^{\\top} x = b_i\\}$. Your task is to connect the expected single-step error reduction to the geometry of $A$ and to the sampling probabilities.\n\nStarting only from the following fundamental base:\n- The definition of an orthogonal projection in Euclidean space and its effect on the squared distance to a target point.\n- The error vector definition $e_k = x_k - x^\\star$ for a consistent system $A x^\\star = b$.\n- The Frobenius norm definition $\\|A\\|_F^2 = \\sum_{i=1}^m \\|a_i\\|_2^2$.\n- The singular values of $A$ as the square roots of the eigenvalues of $A^\\top A$.\n\nderive the exact one-step expected error ratio $r_{\\mathrm{exact}}(e_k) = \\mathbb{E}\\left[\\|e_{k+1}\\|_2^2 \\mid e_k\\right] / \\|e_k\\|_2^2$ in terms of $A$ and the sampling probabilities. Then show how to bound this ratio uniformly over all nonzero $e_k$ using the smallest eigenvalue of a symmetric positive semidefinite matrix constructed from $A$ and the sampling probabilities. In the special case where rows are sampled with probabilities proportional to their squared Euclidean norms, further connect this uniform bound to $\\|A\\|_F^2$ and the smallest singular value of $A$.\n\nProgram specification. Implement a program that, for each test case below, performs the following computations:\n- Construct the sampling probabilities as specified in the test case.\n- Construct the symmetric positive semidefinite matrix obtained by aggregating the sampling-weighted orthogonal projection operators onto the row directions of $A$ as justified by your derivation.\n- Compute its smallest eigenvalue, denoted $\\lambda_{\\min}$, and a corresponding unit eigenvector $v_{\\min}$.\n- Using the exact expression from your derivation, compute the exact one-step expected error ratio $r_{\\mathrm{exact}}(v_{\\min})$ with $e_k = v_{\\min}$.\n- Define the theoretical worst-case ratio for the given probabilities as $r_{\\mathrm{theory}} = 1 - \\lambda_{\\min}$.\n- Compute the absolute difference $\\Delta_{\\mathrm{ratio}} = |r_{\\mathrm{exact}}(v_{\\min}) - r_{\\mathrm{theory}}|$.\n- Compute the Frobenius-norm relation error $\\Delta_{\\mathrm{F}} = \\left|\\lambda_{\\min} - \\dfrac{\\sigma_{\\min}(A)^2}{\\|A\\|_F^2}\\right|$, where $\\sigma_{\\min}(A)$ is the smallest singular value of $A$. This quantity will be zero (up to numerical error) when probabilities are proportional to squared row norms and generally nonzero for arbitrary probabilities.\n\nTest suite. Use the following four test cases, covering a typical full-column-rank case, a nonuniform sampling case, an ill-conditioned case, and a simple orthonormal-columns case.\n- Case $1$ (standard sampling): $A = \\begin{bmatrix} 3  1 \\\\ 0  2 \\\\ 1  2 \\end{bmatrix}$, probabilities proportional to squared row norms. Explicitly, rows are $a_1^{\\top} = [\\,3,\\,1\\,]$, $a_2^{\\top} = [\\,0,\\,2\\,]$, $a_3^{\\top} = [\\,1,\\,2\\,]$.\n- Case $2$ (custom sampling): same $A$ as in Case $1$, with probabilities $p = [\\,0.6,\\,0.3,\\,0.1\\,]$ for rows $1$, $2$, and $3$, respectively.\n- Case $3$ (ill-conditioned, standard sampling): $A = \\begin{bmatrix} 1  0 \\\\ 1  10^{-3} \\end{bmatrix}$, probabilities proportional to squared row norms. Rows are $a_1^{\\top} = [\\,1,\\,0\\,]$, $a_2^{\\top} = [\\,1,\\,10^{-3}\\,]$.\n- Case $4$ (orthonormal columns, standard sampling): $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, probabilities proportional to squared row norms.\n\nNumerical output requirements.\n- For each case, output two floating-point numbers: $\\Delta_{\\mathrm{ratio}}$ and $\\Delta_{\\mathrm{F}}$, in this order, rounded to $10$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The ordering is Case $1$ then Case $2$ then Case $3$ then Case $4$, each contributing two numbers, so the final output format must be\n\"[delta_ratio_case1,delta_F_case1,delta_ratio_case2,delta_F_case2,delta_ratio_case3,delta_F_case3,delta_ratio_case4,delta_F_case4]\".\n\nNo physical units or angles are involved. All answers must be numeric and follow the specified format exactly.",
            "solution": "The analysis begins by formalizing the single iterative step of the Randomized Kaczmarz method and its effect on the error vector. Let the consistent linear system be $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ has rows $a_i^{\\top}$ for $i \\in \\{1,\\dots,m\\}$, which are assumed to be non-zero vectors. The error at iteration $k$ is $e_k = x_k - x^\\star$, where $x^\\star$ is some fixed solution to the system, so $a_i^{\\top} x^\\star = b_i$ for all $i$.\n\nThe iterative update rule involves selecting a row index $i_k \\in \\{1, \\dots, m\\}$ at iteration $k$ with probability $p_{i_k}$ and projecting the current iterate $x_k$ orthogonally onto the solution hyperplane $H_{i_k} = \\{x \\in \\mathbb{R}^n : a_{i_k}^{\\top} x = b_{i_k}\\}$. For simplicity, let the chosen index at step $k$ be $i$. The next iterate $x_{k+1}$ is given by:\n$$\nx_{k+1} = x_k - \\frac{a_i^{\\top} x_k - b_i}{\\|a_i\\|_2^2} a_i\n$$\nThis update rule defines the effect on the error vector $e_{k+1} = x_{k+1} - x^\\star$. Substituting the expressions for $x_{k+1}$ and $b_i$:\n$$\ne_{k+1} = \\left(x_k - \\frac{a_i^{\\top} x_k - a_i^{\\top} x^\\star}{\\|a_i\\|_2^2} a_i\\right) - x^\\star = (x_k - x^\\star) - \\frac{a_i^{\\top} (x_k - x^\\star)}{\\|a_i\\|_2^2} a_i\n$$\n$$\ne_{k+1} = e_k - \\frac{a_i^{\\top} e_k}{\\|a_i\\|_2^2} a_i\n$$\nThis equation reveals a key geometric insight: the new error vector $e_{k+1}$ is the orthogonal projection of the previous error vector $e_k$ onto the linear subspace orthogonal to the chosen row vector $a_i$. The vector $\\frac{a_i^{\\top} e_k}{\\|a_i\\|_2^2} a_i$ is the orthogonal projection of $e_k$ onto the line spanned by $a_i$.\n\nBased on the property of orthogonal projections in a Euclidean space, the vector $e_k$ can be decomposed into two orthogonal components: $e_{k+1}$ (the part of $e_k$ orthogonal to $a_i$) and its projection onto $a_i$. The Pythagorean theorem states that the squared norm of the original vector is the sum of the squared norms of its orthogonal components:\n$$\n\\|e_k\\|_2^2 = \\|e_{k+1}\\|_2^2 + \\left\\| \\frac{a_i^{\\top} e_k}{\\|a_i\\|_2^2} a_i \\right\\|_2^2\n$$\nSolving for the squared norm of the new error gives the one-step reduction:\n$$\n\\|e_{k+1}\\|_2^2 = \\|e_k\\|_2^2 - \\frac{(a_i^{\\top} e_k)^2}{\\|a_i\\|_2^4} \\|a_i\\|_2^2 = \\|e_k\\|_2^2 - \\frac{(a_i^{\\top} e_k)^2}{\\|a_i\\|_2^2}\n$$\nThis expression holds for a specific choice of row $i$. To find the expected error reduction, we take the expectation over all possible choices of $i$, conditioned on the current error $e_k$:\n$$\n\\mathbb{E}\\left[\\|e_{k+1}\\|_2^2 \\mid e_k\\right] = \\sum_{i=1}^m p_i \\left( \\|e_k\\|_2^2 - \\frac{(a_i^{\\top} e_k)^2}{\\|a_i\\|_2^2} \\right) = \\|e_k\\|_2^2 \\left( 1 - \\sum_{i=1}^m p_i \\frac{(a_i^{\\top} e_k)^2}{\\|a_i\\|_2^2 \\|e_k\\|_2^2} \\right)\n$$\nThe exact one-step expected error ratio $r_{\\mathrm{exact}}(e_k)$ is therefore:\n$$\nr_{\\mathrm{exact}}(e_k) = \\frac{\\mathbb{E}\\left[\\|e_{k+1}\\|_2^2 \\mid e_k\\right]}{\\|e_k\\|_2^2} = 1 - \\sum_{i=1}^m p_i \\frac{(a_i^{\\top} e_k)^2}{\\|a_i\\|_2^2 \\|e_k\\|_2^2}\n$$\nThe summation term can be rewritten in matrix form. Recognizing that $(a_i^{\\top} e_k)^2 = e_k^{\\top} a_i a_i^{\\top} e_k$, we have:\n$$\nr_{\\mathrm{exact}}(e_k) = 1 - \\frac{e_k^{\\top} \\left( \\sum_{i=1}^m \\frac{p_i}{\\|a_i\\|_2^2} a_i a_i^{\\top} \\right) e_k}{e_k^{\\top} e_k}\n$$\nThis expression provides the exact error ratio for a given error vector $e_k$.\n\nTo obtain a uniform bound over all non-zero error vectors $e_k$, we define the matrix $W = \\sum_{i=1}^m \\frac{p_i}{\\|a_i\\|_2^2} a_i a_i^{\\top}$. The matrix $W$ is symmetric, as it is a sum of symmetric rank-one matrices $a_i a_i^{\\top}$. It is also positive semidefinite, as for any vector $v \\in \\mathbb{R}^n$, $v^{\\top}Wv = \\sum_{i=1}^m \\frac{p_i}{\\|a_i\\|_2^2} (v^{\\top} a_i)^2 \\ge 0$. The ratio can now be written using the Rayleigh quotient of $W$:\n$$\nr_{\\mathrm{exact}}(e_k) = 1 - \\frac{e_k^{\\top} W e_k}{e_k^{\\top} e_k}\n$$\nTo find the slowest possible convergence rate, we must find the maximum value of $r_{\\mathrm{exact}}(e_k)$ over all non-zero $e_k$. This is equivalent to finding the minimum value of the Rayleigh quotient $\\frac{e_k^{\\top} W e_k}{e_k^{\\top} e_k}$. By the Rayleigh-Ritz theorem, the minimum value of the Rayleigh quotient for a symmetric matrix is its smallest eigenvalue, $\\lambda_{\\min}(W)$. The minimum is achieved when $e_k$ is an eigenvector corresponding to $\\lambda_{\\min}(W)$. Therefore, the uniform bound on the ratio is:\n$$\nr_{\\mathrm{uniform}} = \\sup_{e_k \\neq 0} r_{\\mathrm{exact}}(e_k) = 1 - \\min_{e_k \\neq 0} \\frac{e_k^{\\top} W e_k}{e_k^{\\top} e_k} = 1 - \\lambda_{\\min}(W)\n$$\nThis quantity is the theoretical worst-case one-step expected error ratio, $r_{\\mathrm{theory}}$.\n\nFinally, we analyze the special case of \"standard\" sampling, where probabilities are proportional to the squared Euclidean norms of the rows: $p_i = \\frac{\\|a_i\\|_2^2}{\\|A\\|_F^2}$, where $\\|A\\|_F^2 = \\sum_{j=1}^m \\|a_j\\|_2^2$. Substituting these probabilities into the definition of $W$:\n$$\nW = \\sum_{i=1}^m \\frac{1}{\\|a_i\\|_2^2} \\left( \\frac{\\|a_i\\|_2^2}{\\|A\\|_F^2} \\right) a_i a_i^{\\top} = \\frac{1}{\\|A\\|_F^2} \\sum_{i=1}^m a_i a_i^{\\top}\n$$\nThe sum $\\sum_{i=1}^m a_i a_i^{\\top}$ is precisely the matrix product $A^{\\top} A$. Thus, for this specific probability distribution, the matrix $W$ simplifies to:\n$$\nW = \\frac{1}{\\|A\\|_F^2} A^{\\top} A\n$$\nThe eigenvalues of $W$ are the eigenvalues of $A^{\\top} A$ scaled by $\\frac{1}{\\|A\\|_F^2}$. The eigenvalues of $A^{\\top} A$ are, by definition, the squares of the singular values of $A$, denoted $\\sigma_j(A)^2$. Therefore, the smallest eigenvalue of $W$ is related to the smallest singular value of $A$, $\\sigma_{\\min}(A)$:\n$$\n\\lambda_{\\min}(W) = \\frac{\\sigma_{\\min}(A)^2}{\\|A\\|_F^2}\n$$\nThis establishes the direct connection between the convergence rate, the geometry of $A$ via its singular values and Frobenius norm, and the specific choice of sampling probabilities. For arbitrary probabilities, $\\lambda_{\\min}(W)$ must be computed from its full definition, but for this special case, it can be computed directly from properties of $A$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes performance metrics for the Randomized Kaczmarz method on several test cases.\n    \"\"\"\n    # Test cases defined as tuples of (matrix A, probability specification).\n    # 'standard' indicates probabilities proportional to squared row norms.\n    test_cases = [\n        (np.array([[3.0, 1.0], [0.0, 2.0], [1.0, 2.0]]), 'standard'),\n        (np.array([[3.0, 1.0], [0.0, 2.0], [1.0, 2.0]]), [0.6, 0.3, 0.1]),\n        (np.array([[1.0, 0.0], [1.0, 1e-3]]), 'standard'),\n        (np.array([[1.0, 0.0], [0.0, 1.0]]), 'standard')\n    ]\n\n    # List to store final formatted results as strings\n    results_str = []\n\n    for case_params in test_cases:\n        A, p_spec = case_params\n\n        m, n = A.shape\n        row_norms_sq = np.sum(A**2, axis=1)\n\n        # Check for zero rows, although not present in test data\n        if np.any(row_norms_sq == 0):\n            raise ValueError(\"Matrix A must not contain zero rows.\")\n\n        # Determine sampling probabilities\n        if p_spec == 'standard':\n            fro_norm_sq_A = np.sum(row_norms_sq)\n            p = row_norms_sq / fro_norm_sq_A\n        else:\n            p = np.array(p_spec, dtype=float)\n\n        # Construct the matrix W = sum( (p_i / ||a_i||^2) * a_i * a_i^T )\n        W = np.zeros((n, n))\n        for i in range(m):\n            ai = A[i, :].reshape(n, 1)\n            W += (p[i] / row_norms_sq[i]) * (ai @ ai.T)\n\n        # Compute the smallest eigenvalue and corresponding eigenvector of W\n        # np.linalg.eigh is used for symmetric matrices; it returns sorted eigenvalues.\n        eigenvalues, eigenvectors = np.linalg.eigh(W)\n        lambda_min = eigenvalues[0]\n        v_min = eigenvectors[:, 0]\n\n        # Compute r_exact(v_min) using its derived summation formula\n        # r_exact(e_k) = 1 - sum( p_i * (a_i^T e_k)^2 / ||a_i||^2 ) / ||e_k||^2\n        # For e_k = v_min, ||v_min||^2 = 1.\n        sum_term = 0.0\n        for i in range(m):\n            sum_term += p[i] * (np.dot(A[i, :], v_min)**2) / row_norms_sq[i]\n        \n        r_exact_vmin = 1.0 - sum_term\n\n        # Compute the theoretical worst-case ratio\n        r_theory = 1.0 - lambda_min\n\n        # Compute the absolute difference between the two ratio calculations.\n        # This is expected to be near zero, serving as a numerical check.\n        delta_ratio = np.abs(r_exact_vmin - r_theory)\n\n        # Compute the smallest singular value of A and the squared Frobenius norm\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        # SVD for m  n returns m values. min will be 0 if rank deficient. Here m=n.\n        if singular_values.size  n: # A is wide, has structural zero singular values\n          sigma_min_A = 0.0\n        else:\n          sigma_min_A = np.min(singular_values)\n        \n        fro_norm_sq_A = np.sum(row_norms_sq)\n        \n        # Calculate the theoretical lambda_min for the standard sampling case\n        lambda_min_from_F_relation = (sigma_min_A**2) / fro_norm_sq_A\n        \n        # Compute the absolute error of the Frobenius-norm relation.\n        # This is expected to be near zero only for the standard sampling case.\n        delta_F = np.abs(lambda_min - lambda_min_from_F_relation)\n\n        # Append formatted results to the list\n        results_str.append(f\"{delta_ratio:.10f}\")\n        results_str.append(f\"{delta_F:.10f}\")\n\n    # Print the a single line of output in the required format\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}