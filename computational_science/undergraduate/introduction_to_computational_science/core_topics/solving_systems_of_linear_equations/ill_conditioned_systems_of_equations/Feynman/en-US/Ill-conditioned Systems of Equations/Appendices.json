{
    "hands_on_practices": [
        {
            "introduction": "Ill-conditioned systems are notoriously sensitive to even the smallest perturbations, including the rounding errors inherent in computer arithmetic. This exercise provides a direct and tangible demonstration of this phenomenon. By solving the same system using different levels of floating-point precision, you will observe firsthand how quickly accuracy can degrade and why understanding a system's condition number is critical for obtaining reliable numerical results .",
            "id": "3141607",
            "problem": "You are to write a complete program that empirically studies the numerical behavior of solving the linear system $A x = b$ when $A$ is ill-conditioned, by comparing solutions computed in single precision and double precision. The goal is to determine, for each test case, the minimal precision needed to achieve a specified target relative error in the solution $x$. Your study must start from the following foundational base:\n\n- The definition of a linear system $A x = b$, where $A$ is a square matrix, $x$ is a vector of unknowns, and $b$ is a known right-hand side vector.\n- The concept of floating-point arithmetic and its rounding behavior, where operations are modeled as $\\mathrm{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b) (1 + \\delta)$ with $|\\delta| \\leq u$, and $u$ is the unit roundoff of the floating-point format.\n- The definition of matrix condition number in a given norm, $\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|$, and the understanding that larger $\\kappa(A)$ implies greater sensitivity of the solution $x$ to perturbations in $A$ and $b$.\n\nYour program must implement the following tasks, expressed purely in mathematical and algorithmic terms:\n\n1. For each test case with dimension $n$ and target tolerance $\\tau$, construct the Hilbert matrix $A \\in \\mathbb{R}^{n \\times n}$ defined by $A_{ij} = \\frac{1}{i + j - 1}$ for $1 \\leq i,j \\leq n$. The Hilbert matrix is a classical example of an ill-conditioned matrix.\n2. Choose a ground-truth solution $x^\\star \\in \\mathbb{R}^n$ as the vector of ones, i.e., $x^\\star = (1,1,\\dots,1)^\\top$. Compute $b = A x^\\star$ in double precision to establish a reference right-hand side.\n3. Solve $A x = b$ twice:\n   - Once in single precision (floating-point $32$-bit), by casting $A$ and $b$ to single precision and using a standard direct solver.\n   - Once in double precision (floating-point $64$-bit), by casting $A$ and $b$ to double precision and using the same solver.\n4. For each precision, compute the relative forward error in the solution,\n   $$\n   e_{\\mathrm{rel}} = \\frac{\\|x_{\\mathrm{computed}} - x^\\star\\|_2}{\\|x^\\star\\|_2}.\n   $$\n5. Determine the minimal precision that achieves the target tolerance $\\tau$: if the single-precision relative error is less than or equal to $\\tau$, choose $32$; else if the double-precision relative error is less than or equal to $\\tau$, choose $64$; otherwise, if neither meets the tolerance, output $-1$ to indicate that neither tested precision achieves the required accuracy under the given ill-conditioning.\n\nYour program must use the following test suite of parameter values $(n, \\tau)$ designed to probe different aspects of numerical stability:\n\n- A general case where single precision suffices: $(n, \\tau) = (3, 10^{-4})$.\n- A case where single precision fails but double precision succeeds: $(n, \\tau) = (8, 10^{-5})$.\n- A strongly ill-conditioned case where even double precision fails to meet a strict tolerance: $(n, \\tau) = (12, 10^{-8})$.\n- A boundary case with moderate ill-conditioning but extremely tight tolerance: $(n, \\tau) = (5, 10^{-12})$.\n\nThere are no physical units or angle units in this problem. All numerical tolerances, dimensions, and outputs are dimensionless real numbers.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases above. Each result must be an integer among $\\{32, 64, -1\\}$, representing the minimal precision required.\n- For example, an output could look like `[32,64,-1,-1]`, but must reflect the actual outcomes computed by your program for the specified test cases.",
            "solution": "The problem requires an empirical investigation into the effects of floating-point precision on the solution of ill-conditioned linear systems. Specifically, we are tasked with determining the minimum floating-point precision—either $32$-bit (single precision) or $64$-bit (double precision)—required to solve the system $A x = b$ to within a specified relative error tolerance $\\tau$, where $A$ is a Hilbert matrix.\n\nThe process for each test case, defined by a dimension $n$ and a tolerance $\\tau$, is as follows.\n\n**Step 1: System Construction**\nFirst, we construct the components of the linear system $A x = b$. The matrix $A$ is an $n \\times n$ Hilbert matrix, whose entries are defined by the formula:\n$$\nA_{ij} = \\frac{1}{i + j - 1} \\quad \\text{for } 1 \\leq i, j \\leq n\n$$\nHilbert matrices are notoriously ill-conditioned, meaning their condition number $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$ grows very rapidly with the dimension $n$. This property makes them excellent candidates for studying numerical instability.\n\nThe ground-truth solution, denoted $x^\\star$, is defined as a vector of ones of dimension $n$:\n$$\nx^\\star = (1, 1, \\dots, 1)^\\top \\in \\mathbb{R}^n\n$$\nAll elements of this vector are exactly representable in standard floating-point formats.\n\n**Step 2: Right-Hand Side Vector Calculation**\nUsing the defined $A$ and $x^\\star$, we compute the right-hand side vector $b = A x^\\star$. To ensure that $b$ is as accurate as possible and serves as a reliable reference, this calculation is performed in double precision ($64$-bit floating-point arithmetic). This minimizes the introduction of error into the problem setup itself; any significant errors observed later can then be more confidently attributed to the subsequent solution process. Let us denote the double-precision matrix as $A_{64}$ and the resulting vector as $b_{64}$.\n\n**Step 3: Solution in Single Precision**\nTo simulate a lower-precision computation, we first cast the double-precision matrix $A_{64}$ and vector $b_{64}$ into single precision ($32$-bit) format. Let these be $A_{32}$ and $b_{32}$. The conversion process itself can introduce rounding errors, especially for the entries of the Hilbert matrix which are fractions.\n\nWe then solve the linear system $A_{32} x_{32} = b_{32}$ for the unknown vector $x_{32}$ using a standard numerical solver. The resulting vector, $x_{32}$, is the solution computed in single precision.\n\nNext, we evaluate the accuracy of this solution by computing the relative forward error, $e_{32}$. The error is measured with respect to the known ground-truth solution $x^\\star$ using the Euclidean ($L_2$) norm:\n$$\ne_{32} = \\frac{\\|x_{32} - x^\\star\\|_2}{\\|x^\\star\\|_2}\n$$\nThe norm of the true solution, $\\|x^\\star\\|_2$, is simply $\\sqrt{n}$ since all its components are $1$.\n\n**Step 4: Solution in Double Precision**\nWe repeat the solution process, but this time entirely in double precision. We solve the system $A_{64} x_{64} = b_{64}$ for the vector $x_{64}$. This computation benefits from a larger mantissa and smaller unit roundoff compared to the single-precision case, which is expected to yield a more accurate result for an ill-conditioned system.\n\nSimilarly, we compute the relative forward error for the double-precision solution:\n$$\ne_{64} = \\frac{\\|x_{64} - x^\\star\\|_2}{\\|x^\\star\\|_2}\n$$\n\n**Step 5: Minimal Precision Determination**\nThe final step is to compare the computed errors, $e_{32}$ and $e_{64}$, against the user-specified tolerance $\\tau$. The decision logic is as follows:\n- If $e_{32} \\leq \\tau$, single precision is deemed sufficient. The result for the test case is $32$.\n- If $e_{32} > \\tau$ but $e_{64} \\leq \\tau$, single precision fails while double precision succeeds. The result is $64$.\n- If neither precision achieves the target tolerance (i.e., $e_{32} > \\tau$ and $e_{64} > \\tau$), then both tested precisions are inadequate for the given level of ill-conditioning and the strictness of the tolerance. The result is $-1$.\n\nThis procedure is systematically applied to each $(n, \\tau)$ pair in the provided test suite, and the sequence of results is reported. The test cases are specifically chosen to probe scenarios where the condition number of the $n \\times n$ Hilbert matrix is low enough for single precision to suffice, high enough to require double precision, or so high that even double precision fails to meet a stringent tolerance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically studies the numerical behavior of solving ill-conditioned\n    linear systems by comparing single and double precision results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (3, 1e-4),    # General case, single precision should suffice.\n        (8, 1e-5),    # Single precision fails, double precision succeeds.\n        (12, 1e-8),   # Strongly ill-conditioned, even double precision fails.\n        (5, 1e-12),   # Moderate ill-conditioning, very tight tolerance.\n    ]\n\n    results = []\n    for n, tau in test_cases:\n        # Step 1: Construct the n x n Hilbert matrix A and ground-truth solution x_star.\n        # All initial constructions are done in double precision (float64) to maintain accuracy.\n        \n        # Create the ground-truth solution vector x_star = [1, 1, ..., 1]^T.\n        x_star = np.ones(n, dtype='float64')\n\n        # Construct the Hilbert matrix A_ij = 1 / (i + j - 1).\n        # We use 'i' and 'j' indices from 1 to n.\n        i = np.arange(1, n + 1, dtype='float64').reshape(-1, 1)\n        j = np.arange(1, n + 1, dtype='float64').reshape(1, -1)\n        A_64 = 1.0 / (i + j - 1)\n\n        # Step 2: Compute the right-hand side b = A * x_star in double precision.\n        b_64 = A_64 @ x_star\n\n        # Step 3: Solve in single precision (32-bit).\n        # Cast the matrix and vector to float32.\n        A_32 = A_64.astype('float32')\n        b_32 = b_64.astype('float32')\n        \n        # Solve the system Ax = b.\n        x_32_computed = np.linalg.solve(A_32, b_32)\n        \n        # Compute the relative forward error for the single-precision solution.\n        # The norm of x_star is sqrt(n).\n        norm_x_star = np.linalg.norm(x_star)\n        err_32 = np.linalg.norm(x_32_computed - x_star) / norm_x_star\n\n        # Step 4: Solve in double precision (64-bit).\n        # The matrix and vector are already in float64.\n        x_64_computed = np.linalg.solve(A_64, b_64)\n\n        # Compute the relative forward error for the double-precision solution.\n        err_64 = np.linalg.norm(x_64_computed - x_star) / norm_x_star\n        \n        # Step 5: Determine the minimal precision needed.\n        if err_32 <= tau:\n            results.append(32)\n        elif err_64 <= tau:\n            results.append(64)\n        else:\n            results.append(-1)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen the symptoms of ill-conditioning, we now diagnose its root cause using the powerful tool of Singular Value Decomposition (SVD). This practice reveals how small singular values in a matrix act as amplifiers for noise, leading to unstable solutions. By implementing a truncated SVD estimator, you will quantitatively explore the fundamental bias-variance trade-off, where stabilizing a solution by reducing variance comes at the cost of deviating from the true model by increasing bias .",
            "id": "3141620",
            "problem": "Consider the linear model $y = A x_{\\mathrm{true}} + \\varepsilon$, where $A \\in \\mathbb{R}^{n \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is an unknown vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ represents measurement noise with independent and identically distributed components of zero mean and variance $\\sigma_{\\varepsilon}^2$. Let $A$ be factorized by the Singular Value Decomposition (SVD), namely the Singular Value Decomposition (SVD) representation $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is diagonal with nonnegative entries $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n \\ge 0$.\n\nIn the presence of small singular values, direct inversion to solve for an estimate of $x_{\\mathrm{true}}$ can be ill-conditioned due to noise amplification. A common stabilization is the truncated SVD estimator, which retains only the first $k$ singular components. Using the definitions of bias and variance in statistical estimation, where the bias of an estimator $\\hat{x}$ is defined as $\\mathrm{bias}(\\hat{x}) = \\mathbb{E}[\\hat{x}] - x_{\\mathrm{true}}$ and the variance of an estimator $\\hat{x}$ is quantified by the expected squared deviation from its mean, derive how truncating the small singular values influences the bias and variance of the solution, and implement a program that computes, for specified values of $k$ and $\\sigma_{\\varepsilon}^2$, the squared norm of the bias and the variance term of the truncated SVD estimator.\n\nUse the following instance to ground the computation:\n- Let $n = 6$, and let $A$ be the diagonal matrix with diagonal entries $\\sigma_1 = 10^{0}$, $\\sigma_2 = 10^{-1}$, $\\sigma_3 = 10^{-2}$, $\\sigma_4 = 10^{-3}$, $\\sigma_5 = 10^{-4}$, $\\sigma_6 = 10^{-5}$, so that $A = \\mathrm{diag}(10^{0}, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5})$.\n- Let $x_{\\mathrm{true}} = [\\,1,\\,0.5,\\,0.25,\\,0.125,\\,0.0625,\\,0.03125\\,]^{\\top}$.\n- Assume the noise vector $\\varepsilon$ has components distributed as zero-mean with variance $\\sigma_{\\varepsilon}^2$ and is independent of $x_{\\mathrm{true}}$.\n\nStarting from the fundamental definitions and the SVD structure, the program must:\n- Compute the SVD of $A$ to obtain $(U,\\Sigma,V)$.\n- Form the truncated SVD estimator that retains the first $k$ singular values.\n- Using the estimator and the definitions of bias and variance, compute the squared $\\ell_2$ norm of the bias, $\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2$, and the variance term for $\\hat{x}_k$ under the given noise model, expressed in terms of the singular values and singular vectors obtained from SVD.\n\nTest suite:\nEvaluate the computations for the following parameter values $(k, \\sigma_{\\varepsilon}^2)$:\n1. $(k, \\sigma_{\\varepsilon}^2) = (3, 10^{-6})$.\n2. $(k, \\sigma_{\\varepsilon}^2) = (0, 10^{-6})$.\n3. $(k, \\sigma_{\\varepsilon}^2) = (6, 10^{-6})$.\n4. $(k, \\sigma_{\\varepsilon}^2) = (2, 10^{-10})$.\n5. $(k, \\sigma_{\\varepsilon}^2) = (4, 10^{-4})$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes an inner list with two floating-point numbers in the order $[\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2, \\mathrm{variance\\ term}]$. For example, the output should look like `[[b_1,v_1],[b_2,v_2],...]`, with the five inner lists ordered exactly as the test suite above.",
            "solution": "The objective is to derive and compute the squared norm of the bias and the variance for the truncated Singular Value Decomposition (SVD) estimator of a system of linear equations. The linear model is given by $y = A x_{\\mathrm{true}} + \\varepsilon$, where $A \\in \\mathbb{R}^{n \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is the true solution vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is a noise vector with i.i.d. components, each having a mean of $0$ and variance $\\sigma_{\\varepsilon}^2$.\n\nThe SVD of matrix $A$ is $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthonormal matrices and $\\Sigma$ is a diagonal matrix of singular values $\\sigma_1 \\ge \\dots \\ge \\sigma_n \\ge 0$. The standard least-squares solution $\\hat{x} = A^{-1}y = V \\Sigma^{-1} U^{\\top} y$ is susceptible to noise amplification when some $\\sigma_i$ are small.\n\nThe truncated SVD estimator, $\\hat{x}_k$, mitigates this by using a truncated pseudoinverse $A_k^\\dagger$ which only inverts the first $k$ largest singular values.\nThe truncated pseudoinverse is defined as $A_k^\\dagger = V \\Sigma_k^\\dagger U^{\\top}$, where $\\Sigma_k^\\dagger$ is a diagonal matrix with entries:\n$$\n(\\Sigma_k^\\dagger)_{ii} =\n\\begin{cases}\n1/\\sigma_i & \\text{if } i \\le k \\\\\n0 & \\text{if } i > k\n\\end{cases}\n$$\nThe estimator is then $\\hat{x}_k = A_k^\\dagger y$. Substituting $y = A x_{\\mathrm{true}} + \\varepsilon$:\n$$\n\\hat{x}_k = A_k^\\dagger (A x_{\\mathrm{true}} + \\varepsilon) = (V \\Sigma_k^\\dagger U^{\\top}) (U \\Sigma V^{\\top} x_{\\mathrm{true}} + \\varepsilon)\n$$\nUsing the orthonormality of $U$ ($U^{\\top}U = I$), we get:\n$$\n\\hat{x}_k = V \\Sigma_k^\\dagger \\Sigma V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\n$$\nThe product $\\Sigma_k^\\dagger \\Sigma$ is a diagonal matrix, let's call it $P_k$, with entries $(P_k)_{ii} = 1$ for $i \\le k$ and a value of $0$ for $i > k$. So, $\\hat{x}_k$ simplifies to:\n$$\n\\hat{x}_k = V P_k V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\n$$\n\n**1. Derivation of the Bias**\n\nThe bias of the estimator $\\hat{x}_k$ is defined as $\\mathrm{bias}(\\hat{x}_k) = \\mathbb{E}[\\hat{x}_k] - x_{\\mathrm{true}}$. We first compute the expected value of $\\hat{x}_k$. Since $\\mathbb{E}[\\varepsilon] = 0$, the expectation of the second term is zero:\n$$\n\\mathbb{E}[\\hat{x}_k] = \\mathbb{E}[V P_k V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\varepsilon] = V P_k V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\mathbb{E}[\\varepsilon] = V P_k V^{\\top} x_{\\mathrm{true}}\n$$\nThe bias is therefore:\n$$\n\\mathrm{bias}(\\hat{x}_k) = V P_k V^{\\top} x_{\\mathrm{true}} - x_{\\mathrm{true}}\n$$\nUsing the identity $I = V V^{\\top}$, we can write $x_{\\mathrm{true}} = V V^{\\top} x_{\\mathrm{true}}$:\n$$\n\\mathrm{bias}(\\hat{x}_k) = V P_k V^{\\top} x_{\\mathrm{true}} - V V^{\\top} x_{\\mathrm{true}} = V (P_k - I) V^{\\top} x_{\\mathrm{true}}\n$$\nThe matrix $(P_k - I)$ is diagonal with entries $(P_k-I)_{ii} = 0$ for $i \\le k$ and $(P_k-I)_{ii} = -1$ for $i > k$.\nThe squared $\\ell_2$ norm of the bias is $\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2$. Since $V$ is an orthonormal matrix, it preserves the $\\ell_2$ norm, i.e., $\\|Vz\\|_2 = \\|z\\|_2$ for any vector $z$.\n$$\n\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2 = \\|V (P_k - I) V^{\\top} x_{\\mathrm{true}}\\|_2^2 = \\|(P_k - I) V^{\\top} x_{\\mathrm{true}}\\|_2^2\n$$\nLet $z = V^{\\top} x_{\\mathrm{true}}$. The components of $z$ are $z_i = v_i^{\\top} x_{\\mathrm{true}}$, where $v_i$ is the $i$-th column of $V$. The vector $(P_k - I)z$ has components $0$ for $i \\le k$ and $-z_i$ for $i > k$. Its squared norm is:\n$$\n\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2 = \\sum_{i=k+1}^{n} (-z_i)^2 = \\sum_{i=k+1}^{n} (v_i^{\\top} x_{\\mathrm{true}})^2\n$$\nThis shows that the bias arises from discarding the components of $x_{\\mathrm{true}}$ that project onto the right singular vectors corresponding to the truncated singular values.\n\n**2. Derivation of the Variance**\n\nThe variance term is defined as the expected squared deviation from the mean: $\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}[\\|\\hat{x}_k - \\mathbb{E}[\\hat{x}_k]\\|_2^2]$.\nFrom our previous expressions:\n$$\n\\hat{x}_k - \\mathbb{E}[\\hat{x}_k] = V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\n$$\nThe variance term is then:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}[\\|V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\\|_2^2]\n$$\nAgain, since $V$ is an isometry, this simplifies to:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}[\\|\\Sigma_k^\\dagger U^{\\top} \\varepsilon\\|_2^2]\n$$\nLet's define a new random vector $\\eta = U^{\\top} \\varepsilon$. The covariance of $\\eta$ is $\\mathrm{Cov}(\\eta) = U^{\\top} \\mathrm{Cov}(\\varepsilon) U$. Given that the components of $\\varepsilon$ are i.i.d. with variance $\\sigma_{\\varepsilon}^2$, its covariance matrix is $\\mathrm{Cov}(\\varepsilon) = \\sigma_{\\varepsilon}^2 I$. Thus:\n$$\n\\mathrm{Cov}(\\eta) = U^{\\top} (\\sigma_{\\varepsilon}^2 I) U = \\sigma_{\\varepsilon}^2 U^{\\top} U = \\sigma_{\\varepsilon}^2 I\n$$\nThis means the components $\\eta_i$ are also uncorrelated, with mean $0$ and variance $\\sigma_{\\varepsilon}^2$.\nThe vector $\\Sigma_k^\\dagger \\eta$ has components $(\\Sigma_k^\\dagger \\eta)_i = (1/\\sigma_i)\\eta_i$ for $i \\le k$ and $0$ for $i > k$. Its squared norm is $\\sum_{i=1}^{k} (\\eta_i/\\sigma_i)^2$.\nTaking the expectation:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}\\left[\\sum_{i=1}^{k} \\frac{\\eta_i^2}{\\sigma_i^2}\\right] = \\sum_{i=1}^{k} \\frac{\\mathbb{E}[\\eta_i^2]}{\\sigma_i^2}\n$$\nSince $\\eta_i$ has mean $0$ and variance $\\sigma_{\\varepsilon}^2$, we have $\\mathbb{E}[\\eta_i^2] = \\mathrm{Var}(\\eta_i) + (\\mathbb{E}[\\eta_i])^2 = \\sigma_{\\varepsilon}^2 + 0 = \\sigma_{\\varepsilon}^2$.\nThe final expression for the variance term is:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\sum_{i=1}^{k} \\frac{\\sigma_{\\varepsilon}^2}{\\sigma_i^2} = \\sigma_{\\varepsilon}^2 \\sum_{i=1}^{k} \\frac{1}{\\sigma_i^2}\n$$\nThis shows that the variance is due to the amplification of noise by the inverse of the singular values that are retained.\n\n**3. Application to the Specific Problem Instance**\n\nFor the given problem, the matrix $A$ is diagonal: $A = \\mathrm{diag}(10^{0}, 10^{-1}, \\dots, 10^{-5})$. For a non-negative diagonal matrix, the SVD is trivial: $U=I$, $V=I$, and $\\Sigma=A$. The singular values $\\sigma_i$ are the diagonal entries of $A$, and the right singular vectors $v_i$ are the standard basis vectors $e_i$.\n\nWith these simplifications, our formulas become:\n- **Squared Bias Norm**: Since $v_i = e_i$, the projection $v_i^{\\top} x_{\\mathrm{true}}$ is simply the $i$-th component of $x_{\\mathrm{true}}$, denoted $x_{\\mathrm{true},i}$.\n  $$\n  \\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2 = \\sum_{i=k+1}^{n} (x_{\\mathrm{true},i})^2\n  $$\n- **Variance Term**: The formula remains unchanged but we use the specific singular values.\n  $$\n  \\mathrm{Var}(\\hat{x}_k) = \\sigma_{\\varepsilon}^2 \\sum_{i=1}^{k} \\frac{1}{\\sigma_i^2}\n  $$\n\nThe specific values are:\n- $n=6$\n- $\\sigma_i = 10^{-(i-1)}$ for $i=1, \\dots, 6$.\n- $x_{\\mathrm{true}} = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]^{\\top} = [2^{0}, 2^{-1}, 2^{-2}, 2^{-3}, 2^{-4}, 2^{-5}]^{\\top}$.\n\nWe can now compute the required quantities for each test case by applying these formulas. For a given $k$ and $\\sigma_{\\varepsilon}^2$, we sum the squared components of $x_{\\mathrm{true}}$ from index $k$ to $n-1$ (using $0$-based indexing for implementation) for the bias, and we compute the weighted sum of inverse squared singular values up to index $k-1$ for the variance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the squared bias norm and variance for the truncated SVD estimator\n    for a given linear system and a set of test cases.\n    \"\"\"\n    # Define the parameters of the problem instance.\n    n = 6\n    # Singular values: sigma_i = 10^-(i-1) for i=1,...,6\n    sigmas = np.array([10.0**(-i) for i in range(n)])\n    \n    # True solution vector: x_true_i = (1/2)^(i-1) for i=1,...,6\n    # using 0-based indexing for numpy array\n    x_true = np.array([0.5**i for i in range(n)])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, sigma_epsilon_squared)\n        (3, 10**-6),\n        (0, 10**-6),\n        (6, 10**-6),\n        (2, 10**-10),\n        (4, 10**-4),\n    ]\n\n    results = []\n    for k, sigma_eps_sq in test_cases:\n        # Calculate the squared L2 norm of the bias.\n        # Bias is due to truncating components from k to n.\n        # In 0-based indexing, this corresponds to components from index k to n-1.\n        # If k=n, the slice x_true[n:] is empty, and the sum is correctly 0.\n        # If k=0, the slice x_true[0:] is the whole array, giving ||x_true||^2.\n        bias_sq_norm = np.sum(x_true[k:]**2)\n\n        # Calculate the variance term.\n        # Variance is due to noise amplification by the kept singular values (1 to k).\n        # In 0-based indexing, this corresponds to sigmas from index 0 to k-1.\n        # If k=0, the slice sigmas[:0] is empty, and the sum is correctly 0.\n        if k > 0:\n            variance_term = sigma_eps_sq * np.sum(1.0 / sigmas[:k]**2)\n        else:\n            variance_term = 0.0\n        \n        results.append([bias_sq_norm, variance_term])\n\n    # Format the output string to exactly match the required format:\n    # [[b_1,v_1],[b_2,v_2],...] with no spaces.\n    inner_strings = [f\"[{b},{v}]\" for b, v in results]\n    output_string = f\"[{','.join(inner_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Armed with an understanding of the bias-variance dilemma, we can now explore a practical and widespread technique for managing it: Tikhonov regularization. In this exercise, you will trace the 'regularization path' by computing the solution $x(\\lambda)$ for a range of regularization parameters $\\lambda$. This will illustrate how tuning $\\lambda$ allows a practitioner to navigate the trade-off between a solution's stability and its fidelity to the original, ill-conditioned problem .",
            "id": "3141589",
            "problem": "Consider linear systems of equations in the form $A x \\approx b$ where $A \\in \\mathbb{R}^{n \\times m}$ and $b \\in \\mathbb{R}^{n}$. To analyze sensitivity in ill-conditioned settings, study the regularized least squares minimizer $x(\\lambda)$ defined as the minimizer of the objective $J(x;\\lambda) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2$ with the regularization parameter $\\lambda > 0$. The goal is to compute and compare the regularization path $x(\\lambda)$ over $\\lambda$ spanning orders of magnitude and to identify regimes where the conditioning of the system dominates the behavior of the solution.\n\nUse only foundational definitions: the $2$-norm $\\lVert \\cdot \\rVert_2$, the matrix transpose $A^\\top$, the identity matrix $I$, and the definition of the $2$-norm condition number $\\kappa_2(M) = \\sigma_{\\max}(M)/\\sigma_{\\min}(M)$ for a matrix $M$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values, respectively. Do not start from any closed-form solution formula for $x(\\lambda)$; instead, derive and implement the computation of the minimizer from first principles.\n\nYour program must:\n- Construct three test cases $(A,b)$ that probe different conditioning behaviors.\n- For each test case, evaluate the path $x(\\lambda)$ for $\\lambda$ values spanning orders of magnitude.\n- Quantify how conditioning affects the path using condition numbers and relative path changes.\n- Report, for each test case, numerical indicators that summarize the regime where conditioning dominates and the onset of a stable regime, as well as an overall monotonicity score of the solution norm along the path.\n\nDefinitions and quantities to compute:\n1. For a given test case and each $\\lambda$ in the prescribed set, compute the minimizer $x(\\lambda)$ of $J(x;\\lambda)$, the condition number $\\kappa_2\\!\\left(M(\\lambda)\\right)$ of $M(\\lambda) = A^\\top A + \\lambda I$, and the Euclidean norm $\\lVert x(\\lambda) \\rVert_2$.\n2. For consecutive $\\lambda$ values $\\lambda_k$ and $\\lambda_{k+1}$, define the relative path change\n   $$ s_k = \\frac{\\lVert x(\\lambda_{k+1}) - x(\\lambda_{k}) \\rVert_2}{\\max\\!\\big(\\lVert x(\\lambda_k) \\rVert_2, \\, 10^{-300}\\big)}. $$\n3. Define thresholds for regime identification:\n   - Dominated-by-conditioning thresholds: $\\kappa_{\\text{dom}} = 10^{12}$ and $\\tau_{\\text{dom}} = 0.5$.\n   - Stable-regime thresholds: $\\kappa_{\\text{stab}} = 10^{6}$ and $\\tau_{\\text{stab}} = 0.05$.\n4. Define regime indicators for each test case:\n   - Dominated regime extent (as a single $\\lambda$): \n     $$ \\lambda_{\\text{dom}} = \\max \\left\\{ \\lambda_k \\,\\middle|\\, \\kappa_2\\!\\left(M(\\lambda_k)\\right) \\ge \\kappa_{\\text{dom}} \\text{ and } s_k \\ge \\tau_{\\text{dom}} \\right\\}. $$\n     If the set is empty, take $\\lambda_{\\text{dom}} = 0$.\n   - Stable regime onset (as a single $\\lambda$):\n     $$ \\lambda_{\\text{stab}} = \\min \\left\\{ \\lambda_{k+1} \\,\\middle|\\, \\kappa_2\\!\\left(M(\\lambda_{k+1})\\right) \\le \\kappa_{\\text{stab}} \\text{ and } s_k \\le \\tau_{\\text{stab}} \\right\\}. $$\n     If the set is empty, take $\\lambda_{\\text{stab}}$ as the largest $\\lambda$ in the grid.\n   - Monotonicity score of the solution norm along the path:\n     $$ \\mu = \\frac{\\#\\left\\{ k \\,\\middle|\\, \\lVert x(\\lambda_{k+1}) \\rVert_2 \\le \\lVert x(\\lambda_{k}) \\rVert_2 \\right\\}}{|\\Lambda|-1}, $$\n     where $\\Lambda$ is the set of sampled $\\lambda$ values and $|\\Lambda|$ is its cardinality.\n\nTest suite and data specification:\n- Use the following $\\lambda$ grid:\n  $$ \\Lambda = \\left[10^{-12},\\,10^{-10},\\,10^{-8},\\,10^{-6},\\,10^{-4},\\,10^{-2},\\,10^{0},\\,10^{2}\\right]. $$\n- Test Case $1$ (classical ill-conditioned square system): Let $n=m=6$. Let $A$ be the $6 \\times 6$ Hilbert matrix with entries $A_{ij} = \\frac{1}{i + j + 1}$ for $i,j \\in \\{0,1,2,3,4,5\\}$ (zero-based indices), and let $b$ be the vector of ones in $\\mathbb{R}^6$.\n- Test Case $2$ (severely ill-conditioned near-collinear columns in an overdetermined system): Let $n=60$ and $m=10$. Initialize a Pseudorandom Number Generator (PRNG) with seed $0$. Draw $u \\in \\mathbb{R}^{60}$ with independent standard normal entries. For each column index $j \\in \\{0,\\dots,9\\}$, draw $\\eta_j \\in \\mathbb{R}^{60}$ with independent standard normal entries and set $A_{:,j} = u + \\epsilon \\eta_j$ with $\\epsilon = 10^{-8}$. Draw $b \\in \\mathbb{R}^{60}$ with independent standard normal entries using the same PRNG.\n- Test Case $3$ (exact rank-deficiency in an overdetermined system): Let $n=30$ and $m=6$. Using the same PRNG (continuing from its current state), draw $A \\in \\mathbb{R}^{30 \\times 6}$ with independent standard normal entries, then set the last column to be exactly the sum of the first two columns, $A_{:,5} = A_{:,0} + A_{:,1}$, creating exact linear dependence. Draw $b \\in \\mathbb{R}^{30}$ with independent standard normal entries.\n\nComputation and output requirements:\n- For each test case, compute $x(\\lambda)$ for each $\\lambda \\in \\Lambda$, then compute $\\kappa_2\\!\\left(M(\\lambda)\\right)$, $s_k$, and $\\mu$ as defined above. Use the thresholds to compute $\\lambda_{\\text{dom}}$ and $\\lambda_{\\text{stab}}$ as specified.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of three floats in the order $[\\lambda_{\\text{dom}}, \\lambda_{\\text{stab}}, \\mu]$. The final structure must be\n  `[[\\lambda_{\\text{dom}}^{(1)}, \\lambda_{\\text{stab}}^{(1)}, \\mu^{(1)}],[\\lambda_{\\text{dom}}^{(2)}, \\lambda_{\\text{stab}}^{(2)}, \\mu^{(2)}],[\\lambda_{\\text{dom}}^{(3)}, \\lambda_{\\text{stab}}^{(3)}, \\mu^{(3)}]]`\n  printed as a single line with no whitespace characters.",
            "solution": "We begin from the definition of the regularized least squares objective $J(x;\\lambda) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2$ with $\\lambda > 0$. The objective is strictly convex in $x$ because the quadratic form $\\lVert A x - b \\rVert_2^2$ is convex and the term $\\lambda \\lVert x \\rVert_2^2$ adds a strictly convex penalty. Therefore, there exists a unique minimizer $x(\\lambda)$ for each $\\lambda > 0$.\n\nTo compute $x(\\lambda)$ from first principles, we derive the first-order optimality condition by setting the gradient of $J$ with respect to $x$ equal to zero. Using the rule $\\nabla_x \\lVert A x - b \\rVert_2^2 = 2 A^\\top (A x - b)$ and $\\nabla_x \\lVert x \\rVert_2^2 = 2 x$, the gradient of $J$ is\n$$\n\\nabla_x J(x;\\lambda) = 2 A^\\top (A x - b) + 2 \\lambda x.\n$$\nSetting $\\nabla_x J(x;\\lambda)$ equal to zero yields the normal equation\n$$\nA^\\top A x + \\lambda x = A^\\top b,\n$$\nwhich can be written as\n$$\n\\left(A^\\top A + \\lambda I\\right) x = A^\\top b.\n$$\nThus, for each $\\lambda > 0$, the minimizer $x(\\lambda)$ is the unique solution to the linear system with coefficient matrix $M(\\lambda) = A^\\top A + \\lambda I$. The matrix $M(\\lambda)$ is symmetric positive definite for $\\lambda > 0$, ensuring a unique solution and ruling out singularity.\n\nWe use the $2$-norm condition number $\\kappa_2(M) = \\sigma_{\\max}(M)/\\sigma_{\\min}(M)$ to quantify conditioning of the system matrix $M(\\lambda)$. The Singular Value Decomposition (SVD) of a matrix provides its singular values and underpins the condition number. High $\\kappa_2(M)$ implies potential large relative errors in the solution when data or computations are perturbed. For the regularized normal matrix $M(\\lambda) = A^\\top A + \\lambda I$, the singular values of $M(\\lambda)$ equal the eigenvalues of $M(\\lambda)$ and can be expressed in terms of the singular values of $A$: if $\\sigma_i(A)$ are the singular values of $A$, then the eigenvalues of $A^\\top A$ are $\\sigma_i(A)^2$, and those of $M(\\lambda)$ are $\\sigma_i(A)^2 + \\lambda$. Hence,\n$$\n\\kappa_2\\!\\left(M(\\lambda)\\right) = \\frac{\\max_i \\left(\\sigma_i(A)^2 + \\lambda\\right)}{\\min_i \\left(\\sigma_i(A)^2 + \\lambda\\right)}.\n$$\nThis expression shows that as $\\lambda$ increases, the denominator increases and the ratio contracts toward $1$, reducing ill-conditioning. When $\\lambda$ is small relative to the smallest nonzero $\\sigma_i(A)^2$, $M(\\lambda)$ inherits the ill-conditioning (or singularity if some $\\sigma_i(A)=0$) of $A^\\top A$; as $\\lambda$ grows, the spectrum is lifted, improving conditioning.\n\nTo assess how conditioning affects the solution path $x(\\lambda)$, we compute for a grid $\\Lambda$ of $\\lambda$ values:\n- The minimal-norm solution vector $x(\\lambda)$ from $\\left(A^\\top A + \\lambda I\\right) x = A^\\top b$,\n- The condition number $\\kappa_2\\!\\left(M(\\lambda)\\right)$ via singular value ratios,\n- The relative path change $s_k = \\frac{\\lVert x(\\lambda_{k+1}) - x(\\lambda_{k}) \\rVert_2}{\\max(\\lVert x(\\lambda_k) \\rVert_2, 10^{-300})}$ between consecutive $\\lambda$ values,\n- The monotonicity score $\\mu$, which measures how often the solution norm decreases as $\\lambda$ increases, reflecting the regularization effect that typically shrinks $\\lVert x(\\lambda) \\rVert_2$ with larger $\\lambda$.\n\nWe then identify two regimes using threshold parameters:\n- Dominated-by-conditioning regime: We mark steps where both $\\kappa_2\\!\\left(M(\\lambda_k)\\right) \\ge \\kappa_{\\text{dom}}$ and $s_k \\ge \\tau_{\\text{dom}}$; the choice of $\\kappa_{\\text{dom}} = 10^{12}$ flags extreme ill-conditioning, and $\\tau_{\\text{dom}} = 0.5$ indicates substantial relative changes in $x(\\lambda)$. The largest $\\lambda_k$ satisfying both is reported as $\\lambda_{\\text{dom}}$; if none satisfies, we report $0$ to indicate no strongly conditioning-dominated step.\n- Stable regime onset: We find the first subsequent $\\lambda_{k+1}$ where $\\kappa_2\\!\\left(M(\\lambda_{k+1})\\right) \\le \\kappa_{\\text{stab}}$ and $s_k \\le \\tau_{\\text{stab}}$, with $\\kappa_{\\text{stab}} = 10^{6}$ signaling well-conditioned systems and $\\tau_{\\text{stab}} = 0.05$ indicating small incremental changes in the path. If such a step is not found, we take the largest sampled $\\lambda$ as $\\lambda_{\\text{stab}}$.\n\nThe test suite is designed to cover:\n- A classical ill-conditioned square system (Hilbert matrix), which exhibits large condition numbers and sensitivity at small $\\lambda$.\n- A severely ill-conditioned overdetermined system with nearly collinear columns, where $A^\\top A$ is almost rank-$1$, leading to extreme ill-conditioning that is gradually suppressed by increasing $\\lambda$.\n- An exactly rank-deficient overdetermined system, where $A^\\top A$ is singular, making regularization indispensable; here $M(\\lambda)$ becomes positive definite for any $\\lambda > 0$, and the path $x(\\lambda)$ reveals the stabilization trends as $\\lambda$ grows.\n\nAlgorithmic steps for each test case:\n1. Build $(A,b)$ according to the specification; initialize the Pseudorandom Number Generator (PRNG) with seed $0$ and use it consistently.\n2. For each $\\lambda \\in \\Lambda$, form $M(\\lambda) = A^\\top A + \\lambda I$ and $y = A^\\top b$, solve $M(\\lambda) x(\\lambda) = y$ using a numerically stable solver for symmetric positive definite matrices (here a generic solver suffices because $\\lambda > 0$).\n3. Compute $\\kappa_2\\!\\left(M(\\lambda)\\right)$ via singular values and record $\\lVert x(\\lambda) \\rVert_2$.\n4. Compute $s_k$ for consecutive $\\lambda$ pairs using the given formula with the $10^{-300}$ guard to avoid division by zero.\n5. Determine $\\lambda_{\\text{dom}}$ and $\\lambda_{\\text{stab}}$ using the threshold criteria, and compute $\\mu$.\n6. Output, for each test case, the list $[\\lambda_{\\text{dom}}, \\lambda_{\\text{stab}}, \\mu]$.\n7. Print the final aggregated list across the three test cases on a single line, formatted as specified with no whitespace.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hilbert_matrix(n: int) -> np.ndarray:\n    # Zero-based indices: H[i,j] = 1 / (i + j + 1)\n    i = np.arange(n).reshape(-1, 1)\n    j = np.arange(n).reshape(1, -1)\n    return 1.0 / (i + j + 1.0)\n\ndef compute_regularization_path_metrics(A: np.ndarray, b: np.ndarray, lambdas: list,\n                                        kappa_dom: float, tau_dom: float,\n                                        kappa_stab: float, tau_stab: float) -> list:\n    # Precompute\n    AtA = A.T @ A\n    Atb = A.T @ b\n    m = AtA.shape[0]\n    x_list = []\n    kappa_list = []\n    # Compute x(lambda) and condition numbers\n    for lam in lambdas:\n        M = AtA + lam * np.eye(m)\n        # Solve M x = Atb\n        x = np.linalg.solve(M, Atb)\n        x_list.append(x)\n        # 2-norm condition number via SVD ratio\n        svals = np.linalg.svd(M, compute_uv=False)\n        # Guard against zero singular values (shouldn't occur due to lam > 0)\n        smax = svals[0]\n        smin = svals[-1]\n        kappa = smax / smin if smin > 0 else np.inf\n        kappa_list.append(kappa)\n    # Relative path changes s_k\n    s_list = []\n    norms = [np.linalg.norm(x) for x in x_list]\n    tiny_guard = 1e-300\n    for k in range(len(lambdas) - 1):\n        diff = np.linalg.norm(x_list[k+1] - x_list[k])\n        denom = max(norms[k], tiny_guard)\n        s_list.append(diff / denom)\n    # Dominated regime: largest lambda_k with both conditions true at step k\n    dominated_indices = [\n        k for k in range(len(lambdas) - 1)\n        if (kappa_list[k] >= kappa_dom and s_list[k] >= tau_dom)\n    ]\n    lambda_dom = lambdas[max(dominated_indices)] if dominated_indices else 0.0\n    # Stable regime onset: first lambda_{k+1} with both conditions true for step k\n    stable_index = None\n    for k in range(len(lambdas) - 1):\n        if (kappa_list[k+1] <= kappa_stab) and (s_list[k] <= tau_stab):\n            stable_index = k + 1\n            break\n    lambda_stab = lambdas[stable_index] if stable_index is not None else lambdas[-1]\n    # Monotonicity score: fraction of steps where ||x_{k+1}|| <= ||x_k||\n    monotone_count = sum(1 for k in range(len(lambdas) - 1) if norms[k+1] <= norms[k])\n    monotonicity_score = monotone_count / (len(lambdas) - 1)\n    return [lambda_dom, lambda_stab, monotonicity_score]\n\ndef format_results_no_whitespace(results: list) -> str:\n    def fmt_val(v):\n        # Use default str to preserve scientific notation, ensure no spaces via manual joining\n        return str(v)\n    def fmt_list(lst):\n        return \"[\" + \",\".join(fmt_val(x) for x in lst) + \"]\"\n    return \"[\" + \",\".join(fmt_list(r) for r in results) + \"]\"\n\ndef solve():\n    # Lambda grid spanning orders of magnitude\n    lambdas = [1e-12,1e-10,1e-8,1e-6,1e-4,1e-2,1.0,1e2]\n    # Thresholds\n    kappa_dom = 1e12\n    tau_dom = 0.5\n    kappa_stab = 1e6\n    tau_stab = 0.05\n\n    results = []\n\n    # Test Case 1: Hilbert matrix (n=m=6), b=ones\n    n1 = 6\n    A1 = hilbert_matrix(n1)\n    b1 = np.ones(n1)\n    res1 = compute_regularization_path_metrics(A1, b1, lambdas, kappa_dom, tau_dom, kappa_stab, tau_stab)\n    results.append(res1)\n\n    # Initialize PRNG\n    rng = np.random.default_rng(0)\n\n    # Test Case 2: Near-collinear columns, overdetermined (n=60, m=10)\n    n2, m2 = 60, 10\n    u = rng.standard_normal(n2)\n    epsilon = 1e-8\n    A2 = np.empty((n2, m2))\n    for j in range(m2):\n        eta_j = rng.standard_normal(n2)\n        A2[:, j] = u + epsilon * eta_j\n    b2 = rng.standard_normal(n2)\n    res2 = compute_regularization_path_metrics(A2, b2, lambdas, kappa_dom, tau_dom, kappa_stab, tau_stab)\n    results.append(res2)\n\n    # Test Case 3: Rank-deficient, overdetermined (n=30, m=6), last col = sum of first two\n    n3, m3 = 30, 6\n    A3 = rng.standard_normal((n3, m3))\n    A3[:, 5] = A3[:, 0] + A3[:, 1]\n    b3 = rng.standard_normal(n3)\n    res3 = compute_regularization_path_metrics(A3, b3, lambdas, kappa_dom, tau_dom, kappa_stab, tau_stab)\n    results.append(res3)\n\n    # Final print statement in the exact required format: single line, no whitespace\n    print(format_results_no_whitespace(results))\n\nsolve()\n```"
        }
    ]
}