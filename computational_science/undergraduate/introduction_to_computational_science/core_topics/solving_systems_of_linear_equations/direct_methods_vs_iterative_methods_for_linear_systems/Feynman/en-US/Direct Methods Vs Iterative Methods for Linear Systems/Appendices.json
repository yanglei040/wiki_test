{
    "hands_on_practices": [
        {
            "introduction": "In computational science, especially when dealing with large-scale simulations, memory is often a more critical constraint than processing power. This exercise provides a practical, quantitative comparison of the memory demands of a direct solver versus an iterative one. By deriving and applying memory models for LU factorization and the GMRES method, you will gain hands-on experience in estimating resource requirements and understanding the dramatic impact of \"fill-in\" on the feasibility of direct methods for large, sparse systems .",
            "id": "3118498",
            "problem": "A sparse linear system arises from a uniform discretization of a partial differential equation on a structured grid, producing a matrix $A \\in \\mathbb{R}^{n \\times n}$ with average sparsity of $z$ nonzero entries per row. Assume the matrix is stored in Compressed Sparse Row (CSR) format, where each nonzero value is stored in double precision ($8$ bytes), each column index is stored as a $32$-bit integer ($4$ bytes), and the row pointer array has $n+1$ entries of $32$-bit integers ($4$ bytes each). Consider two solution strategies:\n- A direct method using Lower-Upper (LU) factorization, where the combined number of nonzeros in $L$ and $U$ is modeled by a fill factor $f$ times the number of nonzeros in $A$, and both $L$ and $U$ are stored in CSR with the same per-entry and per-pointer costs as $A$.\n- An iterative method using Generalized Minimal Residual (GMRES) with restart parameter $m$, denoted GMRES($m$), which stores the Arnoldi basis of $(m+1)$ vectors of length $n$ in double precision, an upper Hessenberg matrix of size $(m+1) \\times m$ in double precision, and three additional working vectors of length $n$ (the current iterate, the right-hand side, and the residual), all in double precision. Assume the matrix $A$ must be stored in CSR during GMRES.\n\nGiven the parameters $n = 10^{6}$, $z = 7$, $f = 5$, and a memory cap of $3.0 \\times 10^{8}$ bytes, use the CSR memory model to:\n1. Derive an expression for the CSR memory in bytes to store $A$ in terms of $n$ and $z$.\n2. Derive an expression for the CSR memory in bytes to store the LU factors in terms of $n$, $z$, and $f$.\n3. Derive an expression for the total memory in bytes required by GMRES($m$), including the storage for $A$, the Arnoldi basis, the Hessenberg matrix, and the three working vectors.\n\nThen determine the largest integer restart parameter $m$ such that GMRES($m$) is feasible under the memory cap while LU is not. Report only the value of $m$. No rounding is required; provide the exact integer value as your final answer.",
            "solution": "The problem will be validated and, if deemed valid, solved by following the specified derivation steps.\n\n**Problem Validation**\n\n*   **Givens Extraction:**\n    *   Matrix $A \\in \\mathbb{R}^{n \\times n}$.\n    *   Average sparsity: $z$ nonzeros per row.\n    *   Storage: Compressed Sparse Row (CSR).\n    *   Data types: double precision ($8$ bytes) for values, $32$-bit integer ($4$ bytes) for indices and pointers.\n    *   Row pointer array size: $n+1$ entries.\n    *   Direct method (LU): fill factor $f$, factors $L$ and $U$ stored in CSR.\n    *   Iterative method (GMRES($m$)): restart parameter $m$, stores Arnoldi basis of $(m+1)$ vectors (length $n$, double), Hessenberg matrix of size $(m+1) \\times m$ (double), and $3$ working vectors (length $n$, double). Matrix $A$ is also stored.\n    *   Parameters: $n = 10^{6}$, $z = 7$, $f = 5$.\n    *   Constraint: Memory cap $\\le 3.0 \\times 10^{8}$ bytes.\n\n*   **Validation Verdict:**\n    The problem is scientifically grounded, well-posed, and objective. It is based on standard, formalizable concepts in numerical linear algebra and computational science, namely the memory complexity of CSR storage, LU factorization with fill-in, and the GMRES algorithm. All parameters and models are explicitly defined, making the problem self-contained and free of ambiguity. The values provided are realistic for large-scale scientific computing. Therefore, the problem is deemed **valid**.\n\n**Solution Derivation**\n\nThe solution requires deriving memory expressions for three different scenarios and then using these expressions to find the maximum allowable restart parameter $m$ for GMRES.\n\n**1. Memory for Matrix $A$ in CSR Format**\nLet $M_A$ be the memory in bytes required to store matrix $A$. The CSR format consists of three arrays: one for nonzero values, one for their column indices, and one for row pointers.\n- The number of rows is $n$. With an average of $z$ nonzeros per row, the total number of nonzero entries is $nnz(A) = nz$.\n- Memory for nonzero values: Each of the $nz$ values is a double, requiring $8$ bytes. Total: $8nz$ bytes.\n- Memory for column indices: Each of the $nz$ nonzeros has a corresponding column index, stored as a $32$-bit integer ($4$ bytes). Total: $4nz$ bytes.\n- Memory for row pointers: This array has $n+1$ entries, each a $32$-bit integer ($4$ bytes). Total: $4(n+1)$ bytes.\nThe total memory for $A$ is the sum of these components:\n$$M_A(n, z) = 8nz + 4nz + 4(n+1)$$\n$$M_A(n, z) = 12nz + 4n + 4$$\n\n**2. Memory for LU Factors in CSR Format**\nLet $M_{LU}$ be the memory for the $L$ and $U$ factors. The problem states the combined number of nonzeros in $L$ and $U$ is $f$ times the number of nonzeros in $A$.\n- Total nonzeros in factors: $nnz(L) + nnz(U) = f \\times nnz(A) = fnz$.\n- We model the storage of the factors as a single logical entity in CSR format, with $fnz$ nonzero entries and $n$ rows (requiring $n+1$ row pointers), consistent with common practice for fill-in estimation.\n- Memory for nonzero values: $8(fnz)$ bytes.\n- Memory for column indices: $4(fnz)$ bytes.\n- Memory for row pointers: $4(n+1)$ bytes.\nThe total memory for the LU factors is:\n$$M_{LU}(n, z, f) = 8fnz + 4fnz + 4(n+1)$$\n$$M_{LU}(n, z, f) = 12fnz + 4n + 4$$\n\n**3. Total Memory for GMRES($m$)**\nLet $M_{GMRES}(m)$ be the total memory for the GMRES($m$) algorithm. This includes storing matrix $A$ and the data structures required by the algorithm.\n- Storage for matrix $A$: $M_A = 12nz + 4n + 4$.\n- Storage for the Arnoldi basis: This basis consists of $m+1$ vectors, each of length $n$ and stored in double precision. Memory: $8n(m+1)$ bytes.\n- Storage for the Hessenberg matrix: This is a dense $(m+1) \\times m$ matrix stored in double precision. Memory: $8m(m+1)$ bytes.\n- Storage for working vectors: Three vectors (current iterate, right-hand side, residual) of length $n$ in double precision. Memory: $3 \\times 8n = 24n$ bytes.\nThe total memory is the sum of these components:\n$$M_{GMRES}(n, z, m) = (12nz + 4n + 4) + 8n(m+1) + 8m(m+1) + 24n$$\nCombining terms:\n$$M_{GMRES}(n, z, m) = 12nz + (4n + 8n + 24n) + 8nm + 8m^2 + 8m + 4$$\n$$M_{GMRES}(n, z, m) = 12nz + 36n + 4 + 8nm + 8m^2 + 8m$$\n\n**Determining the Largest Integer Restart Parameter $m$**\n\nWe are given the parameters $n = 10^{6}$, $z = 7$, $f = 5$, and a memory cap of $M_{cap} = 3.0 \\times 10^{8}$ bytes. We need to find the largest integer $m$ such that $M_{LU} > M_{cap}$ and $M_{GMRES}(m) \\le M_{cap}$.\n\nFirst, verify that the LU method is not feasible:\n$$M_{LU} = 12(5)(10^{6})(7) + 4(10^{6}) + 4$$\n$$M_{LU} = 420 \\times 10^{6} + 4 \\times 10^{6} + 4$$\n$$M_{LU} = 424,000,004 \\text{ bytes}$$\nSince $424,000,004 > 3.0 \\times 10^{8}$, the LU factorization method exceeds the memory cap and is not feasible. This fulfills the first condition.\n\nNext, we establish the inequality for GMRES($m$):\n$$M_{GMRES}(m) \\le 3.0 \\times 10^{8}$$\n$$12(10^{6})(7) + 36(10^{6}) + 4 + 8(10^{6})m + 8m^2 + 8m \\le 300,000,000$$\n$$84,000,000 + 36,000,000 + 4 + 8,000,000m + 8m^2 + 8m \\le 300,000,000$$\n$$120,000,004 + 8,000,008m + 8m^2 \\le 300,000,000$$\nThis simplifies to a quadratic inequality in $m$:\n$$8m^2 + 8,000,008m - (300,000,000 - 120,000,004) \\le 0$$\n$$8m^2 + 8,000,008m - 179,999,996 \\le 0$$\nFor the expected range of $m$, the term $8m^2$ is much smaller than the term $8,000,008m$. We can approximate the solution by examining the linear part:\n$$8,000,008m \\approx 179,999,996$$\n$$m \\approx \\frac{179,999,996}{8,000,008} \\approx \\frac{1.8 \\times 10^{8}}{8.0 \\times 10^{6}} = \\frac{180}{8} = 22.5$$\nThis suggests the largest integer value for $m$ is $22$. We will verify this by testing $m = 22$ and $m = 23$ in the full quadratic inequality.\n\nFor $m = 22$:\n$$8(22)^2 + 8,000,008(22) - 179,999,996$$\n$$8(484) + 176,000,176 - 179,999,996$$\n$$3,872 + 176,000,176 - 179,999,996$$\n$$176,004,048 - 179,999,996 = -3,995,948$$\nSince $-3,995,948 \\le 0$, the condition is satisfied for $m=22$.\n\nFor $m = 23$:\n$$8(23)^2 + 8,000,008(23) - 179,999,996$$\n$$8(529) + 184,000,184 - 179,999,996$$\n$$4,232 + 184,000,184 - 179,999,996$$\n$$184,004,416 - 179,999,996 = 4,004,420$$\nSince $4,004,420 > 0$, the condition is not satisfied for $m=23$.\n\nThus, the largest integer restart parameter $m$ for which GMRES($m$) is feasible is $22$.",
            "answer": "$$\\boxed{22}$$"
        },
        {
            "introduction": "The choice between a direct and an iterative solver is not merely a matter of performance but is fundamentally linked to the underlying mathematical structure of the problem matrix. This coding practice explores how a matrix's structural and numerical properties dictate the optimal solution strategy . You will investigate how the symmetry of a sparsity pattern influences fill-in for LU factorization and, critically, how properties like symmetric positive-definiteness determine the very applicability of powerful iterative solvers like the Conjugate Gradient method.",
            "id": "3118467",
            "problem": "You are given the task of constructing and analyzing sparse matrices to examine how the symmetry of the nonzero pattern influences factorization fill-in in lower-upper (LU) factorization and the applicability of the Conjugate Gradient (CG) iterative method. The mathematical base is the linear system definition $A \\mathbf{x} = \\mathbf{b}$, the concept of Gaussian elimination leading to LU factorization, and the requirement of Symmetric Positive Definite (SPD) matrices for the Conjugate Gradient (CG) method.\n\nDefinitions and core facts to use:\n- A linear system is specified by a square matrix $A \\in \\mathbb{R}^{N \\times N}$ and a vector $\\mathbf{b} \\in \\mathbb{R}^{N}$, and seeks $\\mathbf{x} \\in \\mathbb{R}^{N}$ satisfying $A \\mathbf{x} = \\mathbf{b}$.\n- LU factorization represents $A$ (after possible row and column permutations) in the form $A = L U$, where $L$ is lower triangular and $U$ is upper triangular. Gaussian elimination introduces new nonzero entries, a phenomenon known as fill-in. The number of fill-in entries can be quantified by the difference between the total nonzeros in $L$ and $U$ and those originally in $A$.\n- The Conjugate Gradient (CG) method is defined for Symmetric Positive Definite (SPD) matrices. A matrix $A$ is SPD if $A^{\\mathsf{T}} = A$ and $\\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} > 0$ for all nonzero vectors $\\mathbf{x}$.\n- The nonzero pattern symmetry refers to whether the support (set of indices of nonzero entries) of $A$ equals the support of $A^{\\mathsf{T}}$, regardless of numerical values.\n\nYour program must construct three test matrices and, for each, compute four quantities: whether the nonzero pattern is symmetric, the fill-in count under LU with natural column ordering, the fill-in count under LU with a fill-reducing column ordering, and whether CG is applicable (i.e., whether the matrix is SPD). All mathematical quantities must be handled as specified below, and the final output must be a single line in the required format.\n\nMatrix construction and test suite:\n- Test Case $1$ (symmetric positive definite baseline): Let $n = 6$, so $N = n^2 = 36$. Define the two-dimensional discrete Laplacian on a $n \\times n$ grid with Dirichlet boundary conditions as\n$$\nT_1 = \\mathrm{diags}\\left(\\{-\\mathbf{1}_{n-1},\\ 2\\mathbf{1}_{n},\\ -\\mathbf{1}_{n-1}\\},\\ \\{-1, 0, 1\\}\\right),\n$$\n$$\nI_n = \\mathrm{eye}(n),\n$$\n$$\nA_1 = \\mathrm{kron}(I_n, T_1) + \\mathrm{kron}(T_1, I_n).\n$$\nThis $A_1 \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite and has a symmetric nonzero pattern.\n- Test Case $2$ (nonsymmetric extreme pattern): Let $N = 36$. Define\n$$\nI_N = \\mathrm{eye}(N),\n$$\n$$\nS = \\mathrm{diags}\\left(\\{\\mathbf{1}_{N-1}\\},\\ \\{1\\}\\right),\n$$\n$$\nR = \\text{the sparse matrix whose first row is all ones},\n$$\n$$\nC = \\text{the sparse matrix whose last column is all ones}.\n$$\nSet\n$$\nA_2 = I_N + S + R + C.\n$$\nThis $A_2 \\in \\mathbb{R}^{N \\times N}$ is designed to be strongly nonsymmetric in its nonzero pattern and well-conditioned enough to admit LU factorization.\n- Test Case $3$ (symmetric indefinite): Using $A_1$ above, let\n$$\nA_3 = A_1 - 10 I_N,\n$$\nwhich preserves the symmetric nonzero pattern but breaks positive definiteness.\n\nFor each matrix $A_k$ for $k \\in \\{1,2,3\\}$, perform the following computations:\n- Nonzero pattern symmetry boolean: Determine whether the support of $A_k$ equals the support of $A_k^{\\mathsf{T}}$ (this is true if, for every pair $(i,j)$ such that $A_k(i,j) \\neq 0$, we also have $A_k(j,i) \\neq 0$).\n- Fill-in count under LU with natural column ordering: Compute LU factorization of $A_k$ using natural column ordering, then compute the integer\n$$\n\\mathrm{fill\\_nat}(A_k) = \\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A_k),\n$$\nwhere $\\mathrm{nnz}(\\cdot)$ denotes the number of nonzero entries. You should use a standard sparse LU from a numerical library with natural column ordering to obtain $L$ and $U$.\n- Fill-in count under LU with a fill-reducing column ordering: Repeat the computation using a fill-reducing column ordering such as Column Approximate Minimum Degree (COLAMD), to obtain\n$$\n\\mathrm{fill\\_colamd}(A_k) = \\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A_k).\n$$\n- CG applicability boolean: Determine whether $A_k$ is suitable for Conjugate Gradient by checking symmetry and attempting to verify positive definiteness via a Cholesky factorization test. Report true if and only if $A_k$ is symmetric and positive definite.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Aggregate the results for all three test cases in the following order:\n$$\n[\\ \\mathrm{pat\\_sym}(A_1),\\ \\mathrm{fill\\_nat}(A_1),\\ \\mathrm{fill\\_colamd}(A_1),\\ \\mathrm{cg\\_app}(A_1),\\ \\mathrm{pat\\_sym}(A_2),\\ \\mathrm{fill\\_nat}(A_2),\\ \\mathrm{fill\\_colamd}(A_2),\\ \\mathrm{cg\\_app}(A_2),\\ \\mathrm{pat\\_sym}(A_3),\\ \\mathrm{fill\\_nat}(A_3),\\ \\mathrm{fill\\_colamd}(A_3),\\ \\mathrm{cg\\_app}(A_3)\\ ].\n$$\nHere $\\mathrm{pat\\_sym}(A_k)$ and $\\mathrm{cg\\_app}(A_k)$ are booleans, and $\\mathrm{fill\\_nat}(A_k)$ and $\\mathrm{fill\\_colamd}(A_k)$ are integers. No physical units apply in this problem. Your program must be self-contained, require no input, and produce exactly this single-line output.",
            "solution": "We start from the foundational definition of a linear system $A \\mathbf{x} = \\mathbf{b}$ with $A \\in \\mathbb{R}^{N \\times N}$. Direct methods, specifically Gaussian elimination, lead to a lower-upper (LU) factorization $A = L U$ (up to permutations), where $L$ is lower triangular and $U$ is upper triangular. During elimination, new nonzero entries can be created even when $A$ is sparse; this phenomenon is called fill-in. The amount of fill-in can be measured by counting the increase in nonzero entries in the factors compared to the original matrix. Formally, for a fixed ordering, the fill-in count is $\\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A)$, where $\\mathrm{nnz}(\\cdot)$ denotes the number of nonzero entries.\n\nIterative methods approach the solution differently. The Conjugate Gradient (CG) method, in particular, requires that the coefficient matrix be Symmetric Positive Definite (SPD). The SPD conditions are $A^{\\mathsf{T}} = A$ (symmetry) and $\\mathbf{x}^{\\mathsf{T}} A \\mathbf{x} > 0$ for all nonzero $\\mathbf{x} \\in \\mathbb{R}^{N}$ (positive definiteness). An equivalent practical test for positive definiteness is the existence of a Cholesky factorization $A = R^{\\mathsf{T}} R$ for an upper triangular $R$, which succeeds if and only if $A$ is SPD.\n\nInfluence of nonzero pattern symmetry on fill-in can be reasoned via the elimination graph. The sparsity pattern of $A$ defines a graph on indices $\\{1,\\dots,N\\}$ with edges representing nonzero off-diagonal entries. Gaussian elimination corresponds to eliminating vertices, which introduces edges (fill-in) among their neighbors. Symmetric patterns often allow more balanced graph structures, and fill-reducing orderings (such as Column Approximate Minimum Degree (COLAMD)) leverage this structure to minimize the degree growth during elimination, thus reducing fill-in. In contrast, strongly nonsymmetric patterns, especially those with dense rows or columns forming star or arrowhead structures, can lead to more substantial fill-in because elimination connects many nodes densely.\n\nAlgorithmic design tied to principles:\n- Construct $A_1$ as the two-dimensional discrete Laplacian on a $n \\times n$ grid with Dirichlet boundaries, using Kronecker sums. Let $n = 6$, hence $N = n^2 = 36$. The one-dimensional Laplacian $T_1 = \\mathrm{diags}(\\{-\\mathbf{1}_{n-1}, 2\\mathbf{1}_{n}, -\\mathbf{1}_{n-1}\\}, \\{-1, 0, 1\\})$ is symmetric and strictly diagonally dominant, and the Kronecker sum $A_1 = \\mathrm{kron}(I_n, T_1) + \\mathrm{kron}(T_1, I_n)$ is Symmetric Positive Definite (SPD) and has a symmetric nonzero pattern. By the SPD property, $A_1$ admits a Cholesky factorization, and thus CG is applicable. For fill-in, we use sparse LU factorization with two column orderings: natural ordering and a fill-reducing ordering (COLAMD) to quantify differences.\n- Construct $A_2$ as a matrix with pronounced nonsymmetric pattern: $A_2 = I_N + S + R + C$ where $I_N$ is the identity, $S$ is the superdiagonal, $R$ is a dense first row, and $C$ is a dense last column. The identity ensures nonsingularity. The pattern symmetry check compares the supports of $A_2$ and $A_2^{\\mathsf{T}}$; dense first row and last column break symmetry of the support. CG is inapplicable because the pattern is not symmetric, and the matrix is not SPD. LU fill-in tends to be larger in nonsymmetric, star-like structures because elimination creates many new couplings; we again measure fill-in under natural and COLAMD orderings.\n- Construct $A_3 = A_1 - 10 I_N$. This preserves the symmetric nonzero pattern but destroys positive definiteness because subtracting a sufficiently large multiple of the identity shifts all eigenvalues left, making them negative (since $A_1$ has bounded positive eigenvalues for this size). The Cholesky test fails, so CG is not applicable. LU fill-in is driven by the pattern and ordering rather than the sign of values; thus $A_3$ exhibits fill-in similar to $A_1$ for a fixed ordering, with possible minor differences due to pivoting strategies.\n\nImplementation details aligned with the above:\n- Pattern symmetry is tested by comparing the set of nonzero indices (support) of $A$ to that of $A^{\\mathsf{T}}$; equality signifies symmetry.\n- Fill-in counts are computed using a sparse LU routine that provides $L$ and $U$. We compute $\\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A)$ for both natural column ordering and COLAMD column ordering. Row pivoting for numerical stability is permitted internally and is part of realistic direct methods.\n- CG applicability is determined by two checks: (i) symmetry via $A \\stackrel{?}{=} A^{\\mathsf{T}}$ and (ii) positive definiteness via attempting a dense Cholesky factorization; success implies SPD.\n\nTest suite and outputs:\n- Test Case $1$ uses the SPD Laplacian $A_1$ with $N = 36$.\n- Test Case $2$ uses the nonsymmetric arrowhead/star $A_2$ with $N = 36$.\n- Test Case $3$ uses the symmetric indefinite $A_3 = A_1 - 10 I_N$ with $N = 36$.\nFor each $A_k$, the program outputs booleans $\\mathrm{pat\\_sym}(A_k)$ and $\\mathrm{cg\\_app}(A_k)$, and integers $\\mathrm{fill\\_nat}(A_k)$ and $\\mathrm{fill\\_colamd}(A_k)$. All twelve results are printed in a single comma-separated list within square brackets in the exact order specified in the problem statement.\n\nThis design directly reflects the interplay between nonzero pattern symmetry and direct versus iterative method behavior: symmetric patterns enable effective fill-reducing orderings and allow CG when SPD holds, whereas nonsymmetric patterns typically induce larger fill and invalidate CG.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, eye, kron, csr_matrix, coo_matrix, csc_matrix\nfrom scipy.sparse.linalg import splu\n\ndef build_laplacian_2d(n: int) -> csr_matrix:\n    \"\"\"\n    Build the 2D discrete Laplacian (Dirichlet) on an n x n grid using Kronecker sums.\n    Resulting matrix is SPD and has symmetric sparsity pattern.\n    \"\"\"\n    # 1D Laplacian: tridiagonal with [ -1, 2, -1 ]\n    main = 2.0 * np.ones(n)\n    off = -1.0 * np.ones(n - 1)\n    T1 = diags([off, main, off], offsets=[-1, 0, 1], shape=(n, n), format=\"csr\")\n    In = eye(n, format=\"csr\")\n    A = kron(In, T1, format=\"csr\") + kron(T1, In, format=\"csr\")\n    return A\n\ndef build_nonsymmetric_arrow(N: int) -> csr_matrix:\n    \"\"\"\n    Build a sparse matrix with a strongly nonsymmetric pattern:\n    Identity + superdiagonal + dense first row + dense last column.\n    \"\"\"\n    A = eye(N, format=\"csr\")\n    # Superdiagonal ones\n    A += diags([np.ones(N - 1)], offsets=[1], shape=(N, N), format=\"csr\")\n    # Dense first row of ones\n    row0 = np.zeros(N, dtype=int)\n    cols = np.arange(N, dtype=int)\n    data = np.ones(N)\n    R = csr_matrix((data, (row0, cols)), shape=(N, N))\n    A += R\n    # Dense last column of ones\n    rows = np.arange(N, dtype=int)\n    col_last = np.full(N, N - 1, dtype=int)\n    C = csr_matrix((np.ones(N), (rows, col_last)), shape=(N, N))\n    A += C\n    return A\n\ndef is_pattern_symmetric(A: csr_matrix) -> bool:\n    \"\"\"\n    Check whether the sparsity pattern of A equals that of A^T.\n    \"\"\"\n    # Create a boolean matrix of the pattern\n    A_pattern = A.astype(bool)\n    # Check if the pattern is equal to its transpose's pattern\n    # The difference will have non-zero elements where patterns disagree.\n    return (A_pattern - A_pattern.T).nnz == 0\n\ndef fill_in_count(A: csr_matrix, permc_spec: str) -> int:\n    \"\"\"\n    Compute fill-in count for sparse LU: nnz(L) + nnz(U) - nnz(A),\n    using specified column permutation strategy (permc_spec).\n    \"\"\"\n    # Convert to CSC for splu\n    Acsc = A.tocsc()\n    lu = splu(Acsc, permc_spec=permc_spec)\n    L = lu.L\n    U = lu.U\n    return L.nnz + U.nnz - A.nnz\n\ndef cg_applicable(A: csr_matrix) -> bool:\n    \"\"\"\n    Determine CG applicability: True iff A is symmetric and positive definite.\n    We test symmetry and attempt dense Cholesky.\n    \"\"\"\n    dense = A.toarray()\n    if not np.allclose(dense, dense.T, atol=1e-12):\n        return False\n    try:\n        np.linalg.cholesky(dense)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\ndef solve():\n    # Define test cases: three matrices as specified in the problem statement.\n    n = 6\n    N = n * n\n\n    # Test Case 1: SPD Laplacian\n    A1 = build_laplacian_2d(n)\n\n    # Test Case 2: Nonsymmetric arrow/star\n    A2 = build_nonsymmetric_arrow(N)\n\n    # Test Case 3: Symmetric indefinite (shifted Laplacian)\n    A3 = A1 - 10.0 * eye(N, format=\"csr\")\n\n    test_cases = [A1, A2, A3]\n\n    results = []\n    for A in test_cases:\n        # Pattern symmetry boolean\n        pat_sym = is_pattern_symmetric(A)\n        # Fill-in with natural ordering\n        fill_nat = fill_in_count(A, permc_spec=\"NATURAL\")\n        # Fill-in with COLAMD ordering\n        fill_colamd = fill_in_count(A, permc_spec=\"COLAMD\")\n        # CG applicability boolean\n        cg_app = cg_applicable(A)\n        # Append in required order per test case\n        results.extend([pat_sym, fill_nat, fill_colamd, cg_app])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond memory constraints and structural applicability, the numerical behavior of an algorithm is paramount to obtaining an accurate solution. This hands-on coding exercise contrasts the numerical stability of a direct method (LU factorization) with the convergence rate of iterative methods (Jacobi and Gauss-Seidel) . You will compute key metrics like the growth factor, which signals potential instability in Gaussian elimination, and the spectral radius, which governs the speed of convergence for iterative schemes, providing a clear window into their distinct performance characteristics.",
            "id": "3118502",
            "problem": "You must write a complete, runnable program that constructs a set of matrices with diagonal dominance and uses them to compare a direct method to iterative methods for linear systems. The direct method is Lower-Upper (LU) decomposition without pivoting, and the iterative methods are the Jacobi method and the Gauss–Seidel (GS) method. Your program must compute well-defined metrics that make this comparison quantitative and must produce a single-line output in the exact format specified below.\n\nThe fundamental base is the linear system definition and standard matrix decompositions. Consider a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a linear system $A \\mathbf{x} = \\mathbf{b}$ with $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^{n}$. For the direct method, factor $A$ as $A = L U$ with $L$ unit lower triangular and $U$ upper triangular, performed without any row pivoting. For the iterative methods, use the standard matrix splitting $A = D + L + U$, where $D$ is the diagonal part of $A$, $L$ is the strictly lower-triangular part, and $U$ is the strictly upper-triangular part. The Jacobi iteration matrix is $T_{\\mathrm{J}} = -D^{-1}(L + U)$ and the Gauss–Seidel iteration matrix is $T_{\\mathrm{GS}} = -(D + L)^{-1} U$. For any square matrix $M$, the spectral radius is defined as $\\rho(M) = \\max_{i} \\lvert \\lambda_{i}(M) \\rvert$, where $\\lambda_{i}(M)$ are the eigenvalues of $M$. A foundational fact used here is that an iterative method converges for any starting vector if and only if the spectral radius of its iteration matrix is strictly less than $1$, that is, $\\rho(M) < 1$ in a norm consistent sense.\n\nYou will compute the following quantitative metrics:\n- A LU stability proxy via the growth factor $\\gamma = \\dfrac{\\max_{i,j} \\lvert U_{i j} \\rvert}{\\max_{i,j} \\lvert A_{i j} \\rvert}$, which measures whether entries grow significantly during elimination without pivoting.\n- A LU reconstruction residual $r = \\dfrac{\\lVert A - L U \\rVert_{\\mathrm{F}}}{\\lVert A \\rVert_{\\mathrm{F}}}$, where $\\lVert \\cdot \\rVert_{\\mathrm{F}}$ is the Frobenius norm, quantifying backward error of the factorization.\n- The Jacobi spectral radius $\\rho(T_{\\mathrm{J}})$.\n- The Gauss–Seidel spectral radius $\\rho(T_{\\mathrm{GS}})$.\n\nBuild the following test suite of matrices $A$, chosen to cover a happy path, boundary conditions, and an edge case. All entries are real numbers, and dimensions are explicitly fixed:\n1. Happy path, strictly diagonally dominant, size $3 \\times 3$:\n   $$A_{1} = \\begin{bmatrix}\n   4 & -1 & 0 \\\\\n   -1 & 4 & -1 \\\\\n   0 & -1 & 3\n   \\end{bmatrix}.$$\n2. Boundary condition, weak diagonal dominance (equality in interior rows), size $5 \\times 5$, tridiagonal discrete Laplacian with Dirichlet-like interior:\n   $$A_{2} = \\begin{bmatrix}\n   2 & -1 & 0 & 0 & 0 \\\\\n   -1 & 2 & -1 & 0 & 0 \\\\\n   0 & -1 & 2 & -1 & 0 \\\\\n   0 & 0 & -1 & 2 & -1 \\\\\n   0 & 0 & 0 & -1 & 2\n   \\end{bmatrix}.$$\n3. Edge case, ill-conditioned but still strictly diagonally dominant, size $2 \\times 2$:\n   $$A_{3} = \\begin{bmatrix}\n   1 & -0.999 \\\\\n   -0.999 & 1\n   \\end{bmatrix}.$$\n4. Larger random strictly diagonally dominant matrix, size $6 \\times 6$, constructed deterministically as follows: set a pseudo-random generator with seed $123$. For each off-diagonal entry $(i, j)$ with $i \\neq j$, draw $A_{4}[i, j]$ uniformly from $[-0.2, 0.2]$. Then, for each row $i$, set the diagonal entry\n   $$A_{4}[i, i] = \\sum_{j \\neq i} \\lvert A_{4}[i, j] \\rvert + 0.5,$$\n   ensuring strict diagonal dominance by rows.\n\nFor each test matrix $A$, you must:\n- Perform LU decomposition without pivoting using the Doolittle algorithm to produce $L$ and $U$ with unit diagonal in $L$.\n- Compute the growth factor $\\gamma$ and the reconstruction residual $r$ defined above.\n- Form $T_{\\mathrm{J}} = -D^{-1}(L + U)$ and $T_{\\mathrm{GS}} = -(D + L)^{-1} U$ as defined by the splitting of $A$, and compute their spectral radii $\\rho(T_{\\mathrm{J}})$ and $\\rho(T_{\\mathrm{GS}})$.\n\nYour program must produce a single line of output containing the results for the four test matrices in the following format:\n- A comma-separated list enclosed in square brackets, where each element corresponds to one test matrix and is itself a list of the four floating-point metrics\n  $$[\\gamma,\\ r,\\ \\rho(T_{\\mathrm{J}}),\\ \\rho(T_{\\mathrm{GS}})].$$\n- All floating-point numbers must be rounded to $6$ decimal places.\n- For example, the exact structure is\n  $$[[\\gamma_{1}, r_{1}, \\rho(T_{\\mathrm{J},1}), \\rho(T_{\\mathrm{GS},1})],[\\gamma_{2}, r_{2}, \\rho(T_{\\mathrm{J},2}), \\rho(T_{\\mathrm{GS},2})],[\\gamma_{3}, r_{3}, \\rho(T_{\\mathrm{J},3}), \\rho(T_{\\mathrm{GS},3})],[\\gamma_{4}, r_{4}, \\rho(T_{\\mathrm{J},4}), \\rho(T_{\\mathrm{GS},4})]].$$\n\nThere are no physical units involved. All angles, if any were needed, would be in radians, but none are used here. All answers are floating-point numbers and must adhere to the specified rounding and format.\n\nYour program must be self-contained, require no user input, and use Python with the libraries specified. It must implement the required algorithms explicitly so that the results are reproducible and testable.",
            "solution": "The problem has been validated and is deemed valid. It is scientifically grounded in the principles of numerical linear algebra, well-posed with a unique and computable solution, and expressed objectively. The task is to compare a direct method (LU decomposition) with iterative methods (Jacobi, Gauss-Seidel) for solving linear systems $A \\mathbf{x} = \\mathbf{b}$. This comparison is performed by computing four quantitative metrics for a suite of four specially constructed matrices.\n\nThe solution is implemented in a Python program that calculates these metrics for each matrix. The process for each matrix $A \\in \\mathbb{R}^{n \\times n}$ is as follows:\n\nFirst, we construct the four test matrices as specified:\n1.  $A_{1}$: A $3 \\times 3$ strictly diagonally dominant matrix, representing a \"happy path\" case for all methods.\n    $$A_{1} = \\begin{bmatrix}\n    4 & -1 & 0 \\\\\n    -1 & 4 & -1 \\\\\n    0 & -1 & 3\n    \\end{bmatrix}$$\n2.  $A_{2}$: A $5 \\times 5$ matrix representing the 1D discrete Laplacian. It is weakly diagonally dominant and irreducible, a boundary condition for convergence theorems, but one where iterative methods are known to converge.\n    $$A_{2} = \\begin{bmatrix}\n    2 & -1 & 0 & 0 & 0 \\\\\n    -1 & 2 & -1 & 0 & 0 \\\\\n    0 & -1 & 2 & -1 & 0 \\\\\n    0 & 0 & -1 & 2 & -1 \\\\\n    0 & 0 & 0 & -1 & 2\n    \\end{bmatrix}$$\n3.  $A_{3}$: A $2 \\times 2$ matrix that is strictly diagonally dominant but ill-conditioned (close to singular). This serves as an edge case to test the numerical stability of the algorithms.\n    $$A_{3} = \\begin{bmatrix}\n    1 & -0.999 \\\\\n    -0.999 & 1\n    \\end{bmatrix}$$\n4.  $A_{4}$: A larger $6 \\times 6$ strictly diagonally dominant matrix constructed deterministically using a pseudo-random number generator with a fixed seed of $123$. Off-diagonal entries $A_{4}[i, j]$ for $i \\neq j$ are drawn from a uniform distribution on $[-0.2, 0.2]$. The diagonal entries are then set to enforce strict row diagonal dominance: $A_{4}[i, i] = \\sum_{j \\neq i} \\lvert A_{4}[i, j] \\rvert + 0.5$.\n\nFor each matrix $A$, we compute four metrics.\n\nThe first two metrics evaluate the direct method, specifically LU decomposition without pivoting ($A = LU$), where $L$ is unit lower triangular and $U$ is upper triangular. This is implemented using the Doolittle algorithm. For an $n \\times n$ matrix $A$, the entries of $L$ and $U$ are computed as:\n$$ U_{kj} = A_{kj} - \\sum_{i=0}^{k-1} L_{ki} U_{ij}, \\quad \\text{for } j=k, \\dots, n-1 $$\n$$ L_{ik} = \\frac{1}{U_{kk}} \\left( A_{ik} - \\sum_{j=0}^{k-1} L_{ij} U_{jk} \\right), \\quad \\text{for } i=k+1, \\dots, n-1 $$\nwith $L_{kk}=1$. This algorithm is guaranteed to be executable without encountering a zero pivot ($U_{kk} \\neq 0$) because all provided matrices are diagonally dominant.\n\n1.  **Growth Factor ($\\gamma$)**: This metric is a proxy for the stability of LU factorization without pivoting. It is defined as the ratio of the maximum absolute value in the computed upper triangular matrix $U$ to the maximum absolute value in the original matrix $A$.\n    $$ \\gamma = \\dfrac{\\max_{i,j} \\lvert U_{i j} \\rvert}{\\max_{i,j} \\lvert A_{i j} \\rvert} $$\n    A small growth factor (near $1$) indicates that the entries did not grow excessively during elimination, suggesting numerical stability. For diagonally dominant matrices, $\\gamma$ is expected to be small.\n\n2.  **LU Reconstruction Residual ($r$)**: This metric quantifies the backward error of the factorization by measuring how well the computed factors $L$ and $U$ reconstruct the original matrix $A$. It is calculated as the ratio of the Frobenius norm of the residual matrix $A - LU$ to the Frobenius norm of $A$.\n    $$ r = \\dfrac{\\lVert A - L U \\rVert_{\\mathrm{F}}}{\\lVert A \\rVert_{\\mathrm{F}}} $$\n    A value of $r$ close to machine precision indicates a highly accurate factorization.\n\nThe next two metrics evaluate the convergence properties of the iterative methods. The Jacobi and Gauss-Seidel methods are based on the splitting of matrix $A$ into its diagonal ($D$), strictly lower-triangular ($L_A$), and strictly upper-triangular ($U_A$) parts, i.e., $A = D + L_A + U_A$.\n\n3.  **Jacobi Spectral Radius ($\\rho(T_{\\mathrm{J}})$)**: The Jacobi iteration matrix is $T_{\\mathrm{J}} = -D^{-1}(L_A + U_A)$. The iterative method converges if and only if the spectral radius $\\rho(T_{\\mathrm{J}}) = \\max_{i} \\lvert \\lambda_{i}(T_{\\mathrm{J}}) \\rvert$ is less than $1$. The value of $\\rho(T_{\\mathrm{J}})$ determines the asymptotic rate of convergence.\n\n4.  **Gauss-Seidel Spectral Radius ($\\rho(T_{\\mathrm{GS}})$)**: The Gauss-Seidel iteration matrix is $T_{\\mathrm{GS}} = -(D + L_A)^{-1} U_A$. Similar to the Jacobi method, convergence is guaranteed if and only if its spectral radius $\\rho(T_{\\mathrm{GS}})$ is less than $1$. For the class of matrices under consideration (symmetric positive definite, which includes $A_1$, $A_2$, $A_3$, or more generally, irreducibly diagonally dominant matrices), Gauss-Seidel is expected to converge faster than Jacobi, meaning $\\rho(T_{\\mathrm{GS}}) < \\rho(T_{\\mathrm{J}})$.\n\nThe program systematically applies these computations to each of the four test matrices, collates the results, and formats them into a single-line string as specified, with each floating-point number rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef doolittle_lu(A):\n    \"\"\"\n    Performs LU decomposition of a square matrix A using the Doolittle algorithm\n    without pivoting. A = LU, where L is unit lower triangular.\n    \n    Args:\n        A (np.ndarray): The square matrix to decompose.\n    \n    Returns:\n        (np.ndarray, np.ndarray): The lower (L) and upper (U) triangular matrices.\n    \"\"\"\n    n = A.shape[0]\n    L = np.eye(n)\n    U = np.zeros((n, n))\n\n    for k in range(n):\n        # Vectorized calculation for the k-th row of U\n        U[k, k:] = A[k, k:] - L[k, :k] @ U[:k, k:]\n\n        if U[k, k] == 0:\n            # This should not occur for the given problem matrices\n            # as they are diagonally dominant.\n            raise ValueError(\"Zero pivot encountered.\")\n\n        # Vectorized calculation for the k-th column of L\n        if k + 1  n:\n            L[k+1:, k] = (A[k+1:, k] - L[k+1:, :k] @ U[:k, k]) / U[k, k]\n\n    return L, U\n\ndef compute_metrics(A):\n    \"\"\"\n    Computes the four specified metrics for a given matrix A.\n    \n    Args:\n        A (np.ndarray): The input square matrix.\n    \n    Returns:\n        list: A list containing [gamma, r, rho_J, rho_GS].\n    \"\"\"\n    # 1. Perform LU decomposition\n    L_lu, U_lu = doolittle_lu(A)\n\n    # 2. Compute growth factor (gamma)\n    max_abs_A = np.max(np.abs(A))\n    if max_abs_A == 0:\n        gamma = np.inf if np.max(np.abs(U_lu)) > 0 else 0.0\n    else:\n        gamma = np.max(np.abs(U_lu)) / max_abs_A\n\n    # 3. Compute reconstruction residual (r)\n    norm_A = np.linalg.norm(A, 'fro')\n    if norm_A == 0:\n        r = 0.0\n    else:\n        r = np.linalg.norm(A - L_lu @ U_lu, 'fro') / norm_A\n\n    # 4. Split A for iterative methods\n    D = np.diag(np.diag(A))\n    L_split = np.tril(A, k=-1)\n    U_split = np.triu(A, k=1)\n\n    # 5. Compute Jacobi spectral radius (rho_J)\n    D_inv = np.linalg.inv(D)\n    T_J = -D_inv @ (L_split + U_split)\n    rho_J = np.max(np.abs(np.linalg.eigvals(T_J)))\n\n    # 6. Compute Gauss-Seidel spectral radius (rho_GS)\n    D_plus_L_inv = np.linalg.inv(D + L_split)\n    T_GS = -D_plus_L_inv @ U_split\n    rho_GS = np.max(np.abs(np.linalg.eigvals(T_GS)))\n\n    return [gamma, r, rho_J, rho_GS]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, compute metrics, and print results.\n    \"\"\"\n    # Test case 1: Happy path, strictly diagonally dominant\n    A1 = np.array([\n        [4., -1., 0.],\n        [-1., 4., -1.],\n        [0., -1., 3.]\n    ])\n\n    # Test case 2: Boundary condition, weak diagonal dominance\n    A2 = np.array([\n        [2., -1., 0., 0., 0.],\n        [-1., 2., -1., 0., 0.],\n        [0., -1., 2., -1., 0.],\n        [0., 0., -1., 2., -1.],\n        [0., 0., 0., -1., 2.]\n    ])\n\n    # Test case 3: Edge case, ill-conditioned but strictly diagonally dominant\n    A3 = np.array([\n        [1., -0.999],\n        [-0.999, 1.]\n    ])\n\n    # Test case 4: Larger random strictly diagonally dominant matrix\n    n4 = 6\n    rng = np.random.default_rng(123)\n    A4 = rng.uniform(low=-0.2, high=0.2, size=(n4, n4))\n    for i in range(n4):\n        # Set diagonal to ensure strict row diagonal dominance\n        row_sum_abs = np.sum(np.abs(A4[i, :])) - np.abs(A4[i, i])\n        A4[i, i] = row_sum_abs + 0.5\n\n    test_cases = [A1, A2, A3, A4]\n    \n    all_results = []\n    for case in test_cases:\n        metrics = compute_metrics(case)\n        all_results.append(metrics)\n\n    # Format the final output string\n    formatted_case_results = []\n    for metrics in all_results:\n        rounded_metrics = [f\"{val:.6f}\" for val in metrics]\n        formatted_case_results.append(f\"[{','.join(rounded_metrics)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_case_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}