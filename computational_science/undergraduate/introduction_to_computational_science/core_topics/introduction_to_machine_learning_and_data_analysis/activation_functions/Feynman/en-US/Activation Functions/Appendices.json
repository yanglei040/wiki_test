{
    "hands_on_practices": [
        {
            "introduction": "The power of neural networks lies in their ability to approximate a vast range of complex, non-linear functions. This exercise demystifies this capability by focusing on the Rectified Linear Unit, or $\\mathrm{ReLU}$, one of the simplest yet most effective activation functions. You will start by discovering how to construct the absolute value function from two $\\mathrm{ReLU}$ units and then build on this foundation to create a piecewise-linear approximation of a smooth, non-linear function, providing a concrete look into the principles of universal approximation theory .",
            "id": "3094549",
            "problem": "A common activation function in computational science and machine learning is the Rectified Linear Unit (ReLU), defined for a scalar input $z$ by $\\mathrm{ReLU}(z) = \\max\\{0, z\\}$. Consider the absolute value function $f(x) = |x|$ and its generalization $|x|^{p}$ with exponent $p \\geq 1$. \n\nYou will construct a small network by composing activations and linear layers to approximate $|x|^{p}$ on a bounded interval, starting from first principles and well-tested facts.\n\n(a) Using only the definition of the Rectified Linear Unit (ReLU), derive an expression that computes $|x|$ using two ReLU units applied to the scalar input $x$.\n\n(b) Let $p = 3$ and define $f(x) = |x|^{3}$ on the interval $[-1, 1]$. Consider the uniform grid on $[0, 1]$ given by $t_{k} = k/N$ for integers $k = 0, 1, \\dots, N$, where $N \\geq 2$ is an integer. Construct a two-layer network that first maps $x$ to $t = |x|$ using your result from part (a), and then outputs a piecewise-linear function $g_{N}(x)$ that agrees with $t^{3}$ at the sample points $t_{k}$ and is linear on each subinterval $[t_{k}, t_{k+1}]$. Derive an explicit analytic expression for $g_{N}(x)$ using only algebraic combinations of $x$ and compositions of $\\mathrm{ReLU}$ with affine functions of $x$. Your derivation must start from the definition of piecewise-linear interpolation as the straight-line interpolant between successive sample points and the definition of $\\mathrm{ReLU}$; do not assume any pre-given representation of piecewise-linear functions.\n\n(c) Using Taylor’s theorem with the Lagrange form of the remainder and properties of convex, twice continuously differentiable functions, derive a uniform upper bound on the approximation error $\\sup_{x \\in [-1, 1]} |g_{N}(x) - |x|^{3}|$ as a function of $N$ alone. Express the final bound as a single closed-form analytic expression in $N$. No rounding is required.",
            "solution": "The problem will be validated against the required criteria before a solution is attempted.\n\n### Step 1: Extract Givens\n- Definition of the Rectified Linear Unit (ReLU): $\\mathrm{ReLU}(z) = \\max\\{0, z\\}$ for a scalar input $z$.\n- Function of interest: $f(x) = |x|$ and its generalization $|x|^{p}$ with $p \\geq 1$.\n- Part (a): Derive an expression for $|x|$ using two ReLU units.\n- Part (b):\n    - Set $p=3$, so the function is $f(x) = |x|^3$ on the interval $[-1, 1]$.\n    - A uniform grid on $[0, 1]$ is defined by $t_{k} = k/N$ for integers $k = 0, 1, \\dots, N$, where $N \\geq 2$.\n    - Construct a two-layer network that maps $x \\to t = |x|$ and then approximates $t^3$ with a piecewise-linear function $g_{N}(x)$.\n    - $g_{N}(x)$ must agree with $t^3$ at the sample points $t_k$ for $t=|x|$.\n    - $g_{N}(x)$ must be linear on each subinterval $[t_k, t_{k+1}]$.\n    - The derivation for the analytic expression of $g_N(x)$ must use only algebraic combinations of $x$ and compositions of $\\mathrm{ReLU}$ with affine functions of $x$, starting from first principles of piecewise-linear interpolation and the ReLU definition.\n- Part (c):\n    - Derive a uniform upper bound on the approximation error $\\sup_{x \\in [-1, 1]} |g_{N}(x) - |x|^{3}|$.\n    - The derivation must use Taylor’s theorem with the Lagrange form of the remainder and properties of convex, twice continuously differentiable functions.\n    - The final bound must be a single closed-form analytic expression in $N$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n- **Scientifically Grounded**: The problem is based on fundamental concepts in numerical analysis, approximation theory, and the theory of neural networks (specifically, function representation with ReLU units). All concepts are standard and mathematically sound.\n- **Well-Posed**: Each part of the problem asks for a specific derivation or construction with clear constraints. The objectives are unambiguous and a unique solution is expected for each part.\n- **Objective**: The problem is stated in precise, formal mathematical language, free from any subjectivity or opinion.\n- **Completeness and Consistency**: All necessary definitions ($\\mathrm{ReLU}$, grid points), constraints (interval, value of $p$, $N \\ge 2$), and required methods (Taylor's theorem) are provided. The problem parts are logically sequential and consistent.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-posed, scientifically grounded problem in computational science. A full solution will be provided.\n\n### Solution Derivation\n\n**(a) Expression for $|x|$ using ReLU units**\n\nThe absolute value function is defined as $|x| = x$ if $x \\geq 0$ and $|x| = -x$ if $x  0$. We are asked to express this using two Rectified Linear Units, defined as $\\mathrm{ReLU}(z) = \\max\\{0, z\\}$.\n\nConsider the sum of $\\mathrm{ReLU}(x)$ and $\\mathrm{ReLU}(-x)$:\n$\\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x) = \\max\\{0, x\\} + \\max\\{0, -x\\}$.\n\nWe analyze this expression in two cases:\n1.  If $x \\geq 0$, then $-x \\leq 0$. The expression becomes $x + 0 = x$. Since $x \\geq 0$, this is equal to $|x|$.\n2.  If $x  0$, then $-x  0$. The expression becomes $0 + (-x) = -x$. Since $x  0$, this is equal to $|x|$.\n\nIn both cases, the expression $\\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)$ equals $|x|$. Thus, the desired expression is:\n$$|x| = \\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)$$\nThis construction uses two ReLU units, one applied to $x$ and one to $-x$.\n\n**(b) Piecewise-linear approximation of $|x|^3$**\n\nWe want to construct a function $g_N(x)$ that is a piecewise-linear approximation of $|x|^3$ on $[-1, 1]$. The construction is done in two stages. First, the input $x$ is mapped to $t = |x| \\in [0, 1]$. Second, a function $L(t)$ approximates $h(t) = t^3$ on $[0,1]$. Thus, $g_N(x) = L(|x|)$.\n\nThe function $L(t)$ is defined as the piecewise-linear function that interpolates $h(t)=t^3$ at the grid points $t_k = k/N$ for $k=0, 1, \\dots, N$.\nThe value of $L(t)$ at a grid point is $L(t_k) = h(t_k) = (k/N)^3$.\nOn each subinterval $[t_k, t_{k+1}]$ for $k=0, \\dots, N-1$, $L(t)$ is a straight line. The slope of this line, denoted $m_k$, is:\n$$m_k = \\frac{L(t_{k+1}) - L(t_k)}{t_{k+1} - t_k} = \\frac{((k+1)/N)^3 - (k/N)^3}{(k+1)/N - k/N} = \\frac{\\frac{1}{N^3}((k+1)^3 - k^3)}{\\frac{1}{N}}$$\nUsing the expansion $(k+1)^3 = k^3 + 3k^2 + 3k + 1$, we have $(k+1)^3 - k^3 = 3k^2 + 3k + 1$.\nThe slope is therefore:\n$$m_k = \\frac{3k^2 + 3k + 1}{N^2}$$\nA continuous piecewise-linear function can be represented as a sum of scaled and shifted ReLU functions. The construction is based on summing the changes in slope at each breakpoint. Let $L(t)$ be defined starting from $t=0$. We have $L(0) = t_0^3 = 0$. The initial slope for $t \\in [t_0, t_1]$ is $m_0$. At each subsequent breakpoint $t_k$, the slope changes from $m_{k-1}$ to $m_k$.\nThe function $L(t)$ can be written as:\n$$L(t) = L(0) + m_0 t + \\sum_{k=1}^{N-1} (m_k - m_{k-1}) \\mathrm{ReLU}(t - t_k)$$\nSince $L(0)=0$, and the initial slope is $m_0 = \\frac{3(0)^2 + 3(0) + 1}{N^2} = \\frac{1}{N^2}$, we need to compute the change in slope $\\Delta m_k = m_k - m_{k-1}$ for $k=1, \\dots, N-1$:\n$$\\Delta m_k = \\frac{3k^2 + 3k + 1}{N^2} - \\frac{3(k-1)^2 + 3(k-1) + 1}{N^2}$$\n$$= \\frac{1}{N^2} \\left[ (3k^2 + 3k + 1) - (3(k^2 - 2k + 1) + 3k - 3 + 1) \\right]$$\n$$= \\frac{1}{N^2} \\left[ (3k^2 + 3k + 1) - (3k^2 - 6k + 3 + 3k - 2) \\right] = \\frac{1}{N^2} \\left[ (3k^2 + 3k + 1) - (3k^2 - 3k + 1) \\right] = \\frac{6k}{N^2}$$\nSubstituting the slopes and slope changes back into the expression for $L(t)$:\n$$L(t) = \\frac{1}{N^2} t + \\sum_{k=1}^{N-1} \\frac{6k}{N^2} \\mathrm{ReLU}(t - t_k)$$\nNow we must express $g_N(x) = L(|x|)$ in the required form. We substitute $t=|x|$:\n$$g_N(x) = \\frac{1}{N^2} |x| + \\sum_{k=1}^{N-1} \\frac{6k}{N^2} \\mathrm{ReLU}(|x| - \\frac{k}{N})$$\nTo satisfy the condition that the expression uses only compositions of ReLU with affine functions of $x$, we use the identities derived or verified from first principles. From part (a), $|x| = \\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)$.\nWe also need an expression for $\\mathrm{ReLU}(|x| - c)$ for some constant $c \\geq 0$. We propose $\\mathrm{ReLU}(|x|-c) = \\mathrm{ReLU}(x-c) + \\mathrm{ReLU}(-x-c)$.\nLet's verify this. If $x \\geq c$, the right side is $(x-c)+0 = x-c$, and the left side is $\\mathrm{ReLU}(x-c) = x-c$. If $0 \\leq x  c$, the right side is $0+0=0$, and the left side is $\\mathrm{ReLU}(x-c)=0$. By symmetry for $x  0$, the identity holds for all $x$.\n\nSubstituting these identities into the expression for $g_N(x)$:\n$$g_N(x) = \\frac{1}{N^2}(\\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)) + \\sum_{k=1}^{N-1} \\frac{6k}{N^2} \\left(\\mathrm{ReLU}\\left(x - \\frac{k}{N}\\right) + \\mathrm{ReLU}\\left(-x - \\frac{k}{N}\\right)\\right)$$\nThis is the required analytic expression for the piecewise-linear approximation.\n\n**(c) Uniform upper bound on the approximation error**\n\nWe need to find an upper bound for $\\sup_{x \\in [-1, 1]} |g_N(x) - |x|^3|$.\nLet $h(t) = t^3$ and $t = |x|$. The error is $\\sup_{t \\in [0, 1]} |L(t) - h(t)|$, where $L(t)$ is the piecewise-linear interpolant of $h(t)$.\nWe analyze the error on a single subinterval $[t_k, t_{k+1}]$. According to the error formula for linear interpolation, which can be derived from Taylor's theorem with the Lagrange remainder, for any $t \\in (t_k, t_{k+1})$, there exists some $\\xi \\in (t_k, t_{k+1})$ such that the error $E(t)=h(t)-L(t)$ is:\n$$E(t) = \\frac{h''(\\xi)}{2!} (t - t_k)(t - t_{k+1})$$\nTo find a uniform bound, we must maximize the absolute value of this expression over all $t \\in [0, 1]$.\nFirst, we find the maximum of $|(t - t_k)(t - t_{k+1})|$ for $t \\in [t_k, t_{k+1}]$. This quadratic term is maximized at the midpoint of the interval, $t = (t_k + t_{k+1})/2$. The length of the interval is $t_{k+1} - t_k = 1/N$. At the midpoint, the value is $-(\\frac{1}{2N})(\\frac{1}{2N}) = -\\frac{1}{4N^2}$. Thus, the maximum absolute value is $\\frac{1}{4N^2}$.\nNext, we bound the second derivative term, $h''(\\xi)$. The function is $h(t) = t^3$.\n$$h'(t) = 3t^2$$\n$$h''(t) = 6t$$\nThe problem states to use properties of convex functions. For $t \\in [0, 1]$, $h''(t) = 6t \\geq 0$, so $h(t)$ is a convex function. Furthermore, $h''(t)$ is a monotonically increasing function of $t$.\nOn the interval $[t_k, t_{k+1}]$, the maximum value of $h''(\\xi)$ is attained at the right endpoint:\n$$\\sup_{\\xi \\in [t_k, t_{k+1}]} |h''(\\xi)| = h''(t_{k+1}) = 6t_{k+1} = 6 \\frac{k+1}{N}$$\nCombining these results, the maximum error on the subinterval $[t_k, t_{k+1}]$ is bounded by:\n$$\\sup_{t \\in [t_k, t_{k+1}]} |E(t)| \\leq \\frac{1}{2} \\left( \\sup_{\\xi \\in [t_k, t_{k+1}]} |h''(\\xi)| \\right) \\left( \\sup_{t \\in [t_k, t_{k+1}]} |(t-t_k)(t-t_{k+1})| \\right)$$\n$$\\sup_{t \\in [t_k, t_{k+1}]} |E(t)| \\leq \\frac{1}{2} \\left( 6 \\frac{k+1}{N} \\right) \\left( \\frac{1}{4N^2} \\right) = \\frac{3(k+1)}{4N^3}$$\nTo find the uniform error bound over the entire interval $[0, 1]$, we must find the maximum of this local bound over all subintervals, i.e., for $k=0, 1, \\dots, N-1$.\n$$\\sup_{t \\in [0, 1]} |E(t)| = \\max_{k \\in \\{0, \\dots, N-1\\}} \\left( \\sup_{t \\in [t_k, t_{k+1}]} |E(t)| \\right) \\leq \\max_{k \\in \\{0, \\dots, N-1\\}} \\frac{3(k+1)}{4N^3}$$\nThe expression $\\frac{3(k+1)}{4N^3}$ is an increasing function of $k$. Its maximum value for $k \\in \\{0, \\dots, N-1\\}$ occurs at $k = N-1$.\nSubstituting $k = N-1$ gives the uniform upper bound:\n$$\\text{Error} \\leq \\frac{3((N-1)+1)}{4N^3} = \\frac{3N}{4N^3} = \\frac{3}{4N^2}$$\nThis is the final closed-form analytic expression for the uniform upper bound on the approximation error as a function of $N$.",
            "answer": "$$\n\\boxed{\\frac{3}{4N^2}}\n$$"
        },
        {
            "introduction": "Once a network is constructed, it must be trained, a process that typically relies on gradient-based optimization. However, the $\\mathrm{ReLU}$ function's non-differentiability at the origin—its \"kink\"—presents a theoretical and practical challenge. This practice provides a hands-on simulation to investigate this very issue, allowing you to explore the concept of subgradients and see firsthand how the choice of a derivative value at $z=0$ can dramatically affect the optimization trajectory and convergence of a model .",
            "id": "3171901",
            "problem": "You are given the Rectified Linear Unit (ReLU) activation function, defined for a scalar input as $f(x) = \\max(0, x)$. In deep learning backpropagation, the derivative $f'(x)$ is used in chain-rule computations. The function $f(x)$ is differentiable everywhere except at $x = 0$, where the set of valid subgradients is the interval $[0, 1]$. A subgradient choice at $x = 0$ means selecting a value $c \\in [0, 1]$ to be used wherever the chain-rule encounters $f'(0)$.\n\nConsider training a single-parameter model with one ReLU unit on a fixed dataset. The model output for an input $x$ is $\\hat{y}(x; a) = f(a x)$, where $a \\in \\mathbb{R}$ is the trainable parameter. The target outputs are $y(x) = f(x)$. The training objective is the mean squared error (MSE)\n$$\nL(a) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}(x_i; a) - y(x_i)\\right)^2,\n$$\nwith the dataset fixed as $x \\in \\{-2, -1, 0, 1, 2\\}$ and $y(x) = f(x)$ for each input. Use gradient descent with learning rate $\\eta  0$ and parameter initialization $a_0 \\in \\mathbb{R}$:\n$$\na_{t+1} = a_t - \\eta \\, \\nabla L(a_t),\n$$\nwhere the gradient $\\nabla L(a)$ must be computed via the chain rule using the derivative of $f$ defined as\n$$\nf'(z) = \\begin{cases}\n0,  z  0, \\\\\nc,  z = 0, \\\\\n1,  z  0,\n\\end{cases}\n$$\nwith the subgradient choice $c \\in [0, 1]$ applied only at $z = 0$. For any iteration where $a_t x_i = 0$ for some $i$, use the above $f'(0) = c$ consistently for those terms.\n\nYour task is to implement a program that:\n1. Simulates gradient descent on $L(a)$ as defined above.\n2. Uses the chain rule and the specified subgradient choice at $z = 0$ to compute $\\nabla L(a)$ at every step.\n3. For each test case, runs the iteration until either $L(a_t) \\le \\varepsilon$ (convergence) or a maximum number of steps $T_{\\max}$ is reached (non-convergence).\n4. Reports, for each test case, the number of steps to converge as an integer (use $-1$ if non-convergent within $T_{\\max}$), the final parameter $a_T$ as a float, and the final loss $L(a_T)$ as a float.\n\nUse the following fixed constants for all runs:\n- Tolerance $\\varepsilon = 10^{-6}$.\n- Maximum steps $T_{\\max} = 1000$.\n- Dataset $\\{x_i\\}_{i=1}^{5} = \\{-2, -1, 0, 1, 2\\}$ and $y_i = f(x_i)$.\n\nTest suite parameter sets $(c, \\eta, a_0)$:\n- Case 1: $c = 0.0$, $\\eta = 0.1$, $a_0 = 0.0$ (boundary subgradient causing potential stagnation).\n- Case 2: $c = 0.1$, $\\eta = 0.1$, $a_0 = 0.0$ (small subgradient at the kink).\n- Case 3: $c = 0.5$, $\\eta = 0.1$, $a_0 = 0.0$ (moderate subgradient).\n- Case 4: $c = 1.0$, $\\eta = 0.6$, $a_0 = 0.0$ (aggressive learning rate but theoretically stable).\n- Case 5: $c = 1.0$, $\\eta = 1.0$, $a_0 = 0.0$ (stability boundary).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a triple $[s, a_T, L(a_T)]$ for one test case in the same order as above. Here $s$ is the integer number of steps to convergence (or $-1$ if not converged), $a_T$ is the final parameter value, and $L(a_T)$ is the final loss. For example, a valid output format is\n$$\n[[s_1, a_{T,1}, L(a_{T,1})],[s_2, a_{T,2}, L(a_{T,2})],\\dots].\n$$\nNo physical units or angles are involved; all quantities are real numbers.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of machine learning and optimization, is well-posed, objective, and self-contained. It describes a clear computational task based on standard concepts such as the ReLU activation function, mean squared error loss, and gradient descent, including the use of subgradients for non-differentiable points.\n\nThe core of the problem is to simulate the training of a single-parameter model, $\\hat{y}(x; a) = f(a x)$, where $f$ is the ReLU function $f(z) = \\max(0, z)$. The goal is to find the parameter $a$ that minimizes the mean squared error (MSE) loss, $L(a)$, on a given dataset.\n\nFirst, we define the components of the problem mathematically.\nThe model output is $\\hat{y}(x_i; a) = f(a x_i)$.\nThe target output is $y_i = f(x_i)$.\nThe dataset is given by $\\{x_i\\}_{i=1}^{5} = \\{-2, -1, 0, 1, 2\\}$ and the corresponding targets $y_i$ are $\\{0, 0, 0, 1, 2\\}$.\nThe loss function is the MSE:\n$$\nL(a) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}(x_i; a) - y_i\\right)^2 = \\frac{1}{5}\\sum_{i=1}^{5}\\left(f(a x_i) - f(x_i)\\right)^2\n$$\n\nThe optimization is performed using gradient descent, with the update rule:\n$$\na_{t+1} = a_t - \\eta \\, \\nabla L(a_t)\n$$\nwhere $a_t$ is the parameter value at step $t$, $\\eta$ is the learning rate, and $\\nabla L(a_t)$ is the gradient of the loss function with respect to $a$ evaluated at $a_t$.\n\nTo compute the gradient, we apply the chain rule to the loss function:\n$$\n\\nabla L(a) = \\frac{d L}{d a} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{d}{da} \\left(f(a x_i) - y_i\\right)^2\n$$\nFor a single term in the sum, the derivative is:\n$$\n\\frac{d}{da} \\left(f(a x_i) - y_i\\right)^2 = 2 \\left(f(a x_i) - y_i\\right) \\cdot \\frac{d}{da}f(a x_i)\n$$\nApplying the chain rule again to $\\frac{d}{da}f(a x_i)$, with $z_i = a x_i$:\n$$\n\\frac{d}{da}f(a x_i) = f'(z_i) \\cdot \\frac{dz_i}{da} = f'(a x_i) \\cdot x_i\n$$\nThe problem specifies the form of the derivative (or subgradient-based derivative) of the ReLU function:\n$$\nf'(z) = \\begin{cases}\n0,  z  0 \\\\\nc,  z = 0 \\\\\n1,  z  0\n\\end{cases}\n$$\nwhere $c \\in [0, 1]$ is a given constant for the subgradient choice at the non-differentiable point $z=0$.\n\nCombining these results, the full expression for the gradient is:\n$$\n\\nabla L(a) = \\frac{2}{n}\\sum_{i=1}^{n} \\left(f(a x_i) - y_i\\right) \\cdot x_i \\cdot f'(a x_i)\n$$\n\nThe simulation proceeds iteratively. Starting with an initial parameter $a_0$, we enter a loop for a maximum of $T_{\\max} = 1000$ steps. In each step $t = 0, 1, 2, \\dots$:\n1. The current loss $L(a_t)$ is calculated.\n2. If $L(a_t) \\le \\varepsilon = 10^{-6}$, the process has converged. We record the number of steps $t$, the final parameter $a_t$, and the final loss $L(a_t)$, and terminate the simulation for the current test case.\n3. If the maximum number of steps $T_{\\max}$ is reached without convergence, the simulation is stopped. We record $-1$ for the number of steps, along with the final parameter $a_{T_{\\max}}$ and loss $L(a_{T_{\\max}})$.\n4. If not converged and not at the step limit, the gradient $\\nabla L(a_t)$ is computed using the formula above, evaluating $f'(a_t x_i)$ for each data point based on the sign of $a_t x_i$.\n5. The parameter is updated to $a_{t+1} = a_t - \\eta \\nabla L(a_t)$.\n\nThe optimal parameter value is $a=1$, at which point $\\hat{y}(x_i; 1) = f(x_i) = y_i$ for all $x_i$ in the dataset, and the loss $L(1) = 0$. The behavior of the gradient descent algorithm depends critically on the parameters $(c, \\eta, a_0)$. For example, with $a_0=0$ and $c=0$, the initial gradient $\\nabla L(0)$ is zero, causing the parameter to be permanently stuck at $a=0$, preventing convergence. For other values of $c  0$, the parameter will move away from the origin and can converge towards the optimal value $a=1$. The learning rate $\\eta$ controls the stability and speed of this convergence. For $\\eta=1.0$ with $c=1.0$, the updates cause the parameter to oscillate between $0$ and $2$, never converging.\n\nThe implementation will follow this logic for each test case provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the gradient descent simulation problem for a set of test cases.\n    \"\"\"\n    # Fixed constants for all runs\n    epsilon = 1e-6\n    T_max = 1000\n    X = np.array([-2, -1, 0, 1, 2], dtype=float)\n    Y = np.maximum(0, X)\n\n    # Test suite parameter sets (c, eta, a_0)\n    test_cases = [\n        (0.0, 0.1, 0.0),\n        (0.1, 0.1, 0.0),\n        (0.5, 0.1, 0.0),\n        (1.0, 0.6, 0.0),\n        (1.0, 1.0, 0.0),\n    ]\n\n    results = []\n\n    for c, eta, a0 in test_cases:\n        a = float(a0)\n\n        # The simulation runs for t = 0, 1, ..., T_max steps.\n        # At each step t, we check for convergence at a_t.\n        # If not converged, we compute a_{t+1}.\n        for t in range(T_max + 1):\n            \n            # 1. Calculate loss L(a_t)\n            y_hat = np.maximum(0, a * X)\n            current_loss = np.mean((y_hat - Y)**2)\n\n            # 2. Check for convergence\n            if current_loss = epsilon:\n                results.append([t, a, current_loss])\n                break\n            \n            # 3. Check for non-convergence after T_max steps\n            # This is after T_max updates, at step T_max.\n            if t == T_max:\n                results.append([-1, a, current_loss])\n                break\n\n            # 4. Compute gradient\n            grad_sum = 0.0\n            n = len(X)\n            errors = y_hat - Y\n            \n            for i in range(n):\n                z_i = a * X[i]\n                \n                # Derivative of ReLU, f'(z), based on subgradient choice c\n                if z_i  0:\n                    der_f = 1.0\n                elif z_i  0:\n                    der_f = 0.0\n                else:  # z_i == 0\n                    der_f = c\n                \n                grad_sum += errors[i] * X[i] * der_f\n            \n            grad = (2.0 / n) * grad_sum\n\n            # 5. Update parameter\n            a = a - eta * grad\n    \n    # Final print statement in the exact required format.\n    # The str() representation of a list includes spaces, e.g., '[1, 2.0, 3.0]'.\n    # Joining these with ',' results in '[1, 2.0, 3.0],[4, 5.0, 6.0]'.\n    # This matches the structure of the example.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While standard activation functions have fixed shapes, we can increase a model's flexibility by allowing the activation function itself to change during training. This exercise introduces this powerful idea through a parametric version of $\\mathrm{ReLU}$, where the slope for negative inputs is a learnable parameter $a$. By deriving and implementing the gradient descent update for $a$, you will not only train the activation function but also compare your iterative result to a closed-form solution, deepening your understanding of the optimization landscape .",
            "id": "3094506",
            "problem": "Consider the Rectified Linear Unit (ReLU) defined as $ \\mathrm{ReLU}(z) = \\max(0,z) $. Define the parametric activation function $ f_a(x) = \\mathrm{ReLU}(x) + a \\cdot \\mathrm{ReLU}(-x) $, where $ a \\in \\mathbb{R} $ is a scalar parameter to be learned by minimizing a training loss on a dataset of input-output pairs. Let a dataset be $ \\mathcal{D} = \\{(x_i,y_i)\\}_{i=1}^n $ and let the training objective be the Mean Squared Error (MSE), defined by\n$$\n\\mathcal{L}(a) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(f_a(x_i) - y_i\\right)^2.\n$$\nWe aim to follow the gradient descent update rule\n$$\na \\leftarrow a - \\eta \\, \\frac{\\partial \\mathcal{L}}{\\partial a},\n$$\nwhere $ \\eta  0 $ is the learning rate.\n\nYour tasks are:\n- Using first principles (the chain rule of calculus and the definition of $ \\mathrm{ReLU} $), derive the expression for $ \\frac{\\partial \\mathcal{L}}{\\partial a} $ in terms of $ x_i $, $ y_i $, and $ a $.\n- Derive the closed-form minimizer $ a^\\star $ of $ \\mathcal{L}(a) $ under the condition that there exists at least one index $ i $ with $ x_i  0 $. If there are no such indices (i.e., all $ x_i \\ge 0 $), then $ \\mathcal{L}(a) $ does not depend on $ a $, and for this special case define $ a^\\star := a_0 $, where $ a_0 $ is the initialization.\n- Implement gradient descent to iteratively update $ a $ according to the derived gradient for a fixed number of iterations $ T $. Use a constant learning rate $ \\eta $ and initial value $ a_0 $.\n- For each test case in the test suite below, compute and report two values: the learned parameter after $ T $ iterations, denoted $ a_T $, and the closed-form minimizer $ a^\\star $.\n\nUse the following test suite. In all cases, angles are not involved, and there are no physical units. All arrays are to be interpreted as sequences of real numbers.\n\nTest Case $ 1 $ (general happy path):\n- $ x $-values: $ [-2,-1,0,1,2] $\n- $ y $-values: $ [0.4,0.2,0,1,2] $\n- Initialization: $ a_0 = 0.5 $\n- Learning rate: $ \\eta = 0.1 $\n- Iterations: $ T = 40 $\n\nTest Case $ 2 $ (no negative inputs, boundary case):\n- $ x $-values: $ [0,1,2,3] $\n- $ y $-values: $ [0,1,2,3] $\n- Initialization: $ a_0 = 0.5 $\n- Learning rate: $ \\eta = 0.1 $\n- Iterations: $ T = 40 $\n\nTest Case $ 3 $ (only negative inputs):\n- $ x $-values: $ [-3,-1,-0.5,-2] $\n- $ y $-values: $ [2.4,0.8,0.4,1.6] $\n- Initialization: $ a_0 = 0.5 $\n- Learning rate: $ \\eta = 0.1 $\n- Iterations: $ T = 40 $\n\nTest Case $ 4 $ (mixed inputs with slight mismatch):\n- $ x $-values: $ [-2,-1,1,3,0] $\n- $ y $-values: $ [1.1,0.6,1,3,0] $\n- Initialization: $ a_0 = 0.5 $\n- Learning rate: $ \\eta = 0.1 $\n- Iterations: $ T = 40 $\n\nYour program must:\n- Implement $ \\mathrm{ReLU} $ exactly as $ \\mathrm{ReLU}(z) = \\max(0,z) $.\n- Compute $ a_T $ via gradient descent using the derived gradient expression.\n- Compute $ a^\\star $ via your closed-form derivation, with the special-case convention $ a^\\star := a_0 $ when there are no negative inputs.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$ [a_T^{(1)},a^\\star{}^{(1)},a_T^{(2)},a^\\star{}^{(2)},a_T^{(3)},a^\\star{}^{(3)},a_T^{(4)},a^\\star{}^{(4)}] $,\nwhere superscripts denote the test case index. Express each value as a decimal float rounded to $ 6 $ decimal places. No other text should be printed.",
            "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed problem in computational science. It is scientifically grounded, self-contained, and objective. We may now proceed with a complete solution.\n\nThe solution is divided into two primary parts: the analytical derivation of the gradient and the closed-form minimizer, followed by the algorithmic implementation.\n\n### Part 1: Derivation of the Gradient $ \\frac{\\partial \\mathcal{L}}{\\partial a} $\n\nThe parametric activation function is given by $ f_a(x) = \\mathrm{ReLU}(x) + a \\cdot \\mathrm{ReLU}(-x) $, where $ \\mathrm{ReLU}(z) = \\max(0, z) $. The objective is to minimize the Mean Squared Error (MSE) loss function, defined as:\n$$\n\\mathcal{L}(a) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left(f_a(x_i) - y_i\\right)^2\n$$\nTo perform gradient descent, we need to compute the derivative of $ \\mathcal{L}(a) $ with respect to the parameter $ a $. We apply the chain rule of calculus:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{2n} \\sum_{i=1}^{n} \\left(f_a(x_i) - y_i\\right)^2 \\right]\n$$\nThe derivative of a sum is the sum of the derivatives. The constant factor $ \\frac{1}{2n} $ can be pulled out.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{1}{2n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial a} \\left(f_a(x_i) - y_i\\right)^2\n$$\nApplying the chain rule, $ \\frac{d}{du} u^2 = 2u $, where $ u = f_a(x_i) - y_i $:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{1}{2n} \\sum_{i=1}^{n} 2 \\left(f_a(x_i) - y_i\\right) \\frac{\\partial}{\\partial a}\\left( f_a(x_i) - y_i \\right)\n$$\nSince $ y_i $ is a constant with respect to $ a $, its derivative is $ 0 $.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(f_a(x_i) - y_i\\right) \\frac{\\partial f_a(x_i)}{\\partial a}\n$$\nNow, we must find the partial derivative of $ f_a(x_i) $ with respect to $ a $:\n$$\n\\frac{\\partial f_a(x_i)}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ \\mathrm{ReLU}(x_i) + a \\cdot \\mathrm{ReLU}(-x_i) \\right]\n$$\nThe term $ \\mathrm{ReLU}(x_i) $ does not depend on $ a $, so its derivative is $ 0 $. The term $ a \\cdot \\mathrm{ReLU}(-x_i) $ is linear in $ a $.\n$$\n\\frac{\\partial f_a(x_i)}{\\partial a} = 0 + \\mathrm{ReLU}(-x_i) = \\mathrm{ReLU}(-x_i)\n$$\nSubstituting this result back into the expression for the gradient of the loss function, we obtain the final expression:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(f_a(x_i) - y_i\\right) \\mathrm{ReLU}(-x_i)\n$$\nSubstituting the definition of $f_a(x_i)$ gives:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\mathrm{ReLU}(x_i) + a \\cdot \\mathrm{ReLU}(-x_i) - y_i\\right) \\mathrm{ReLU}(-x_i)\n$$\nThis is the expression for the gradient that will be used in the gradient descent updates.\n\n### Part 2: Derivation of the Closed-Form Minimizer $ a^\\star $\n\nThe minimizer $ a^\\star $ of a convex function like $ \\mathcal{L}(a) $ can be found by setting its first derivative to zero and solving for $ a $.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a} = 0\n$$\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\mathrm{ReLU}(x_i) + a \\cdot \\mathrm{ReLU}(-x_i) - y_i\\right) \\mathrm{ReLU}(-x_i) = 0\n$$\nWe can ignore the constant factor $ \\frac{1}{n} $. To solve for $ a $, let's analyze the term $ \\mathrm{ReLU}(-x_i) $.\n- If $ x_i \\geq 0 $, then $ -x_i \\leq 0 $, which implies $ \\mathrm{ReLU}(-x_i) = 0 $.\n- If $ x_i  0 $, then $ -x_i  0 $, which implies $ \\mathrm{ReLU}(-x_i) = -x_i $.\n\nThis means the terms in the summation are non-zero only for those indices $ i $ where $ x_i  0 $. Let us define the set of such indices as $ I_{-} = \\{ i \\mid x_i  0 \\} $.\nThe equation simplifies to a sum over only the indices in $ I_{-} $:\n$$\n\\sum_{i \\in I_{-}} \\left(\\mathrm{ReLU}(x_i) + a \\cdot \\mathrm{ReLU}(-x_i) - y_i\\right) \\mathrm{ReLU}(-x_i) = 0\n$$\nFor any $ i \\in I_{-} $, we have $ x_i  0 $. Therefore, $ \\mathrm{ReLU}(x_i) = \\max(0, x_i) = 0 $ and $ \\mathrm{ReLU}(-x_i) = \\max(0, -x_i) = -x_i $. Substituting these into the equation:\n$$\n\\sum_{i \\in I_{-}} \\left(0 + a \\cdot (-x_i) - y_i\\right) (-x_i) = 0\n$$\n$$\n\\sum_{i \\in I_{-}} \\left(-ax_i - y_i\\right) (-x_i) = 0\n$$\n$$\n\\sum_{i \\in I_{-}} \\left(a x_i^2 + y_i x_i\\right) = 0\n$$\nWe can separate the sums:\n$$\n\\sum_{i \\in I_{-}} a x_i^2 + \\sum_{i \\in I_{-}} y_i x_i = 0\n$$\nWe can factor $ a $ out of the first sum:\n$$\na \\left( \\sum_{i \\in I_{-}} x_i^2 \\right) = - \\sum_{i \\in I_{-}} y_i x_i\n$$\nThe problem states the condition that there is at least one index $ i $ with $ x_i  0 $. This guarantees that the set $ I_{-} $ is non-empty. Since for every $ i \\in I_{-} $, $ x_i \\neq 0 $, it follows that $ x_i^2  0 $, and thus the sum $ \\sum_{i \\in I_{-}} x_i^2 $ is strictly positive. We can therefore divide by it to solve for $ a $. The solution is the optimal parameter $ a^\\star $:\n$$\na^\\star = \\frac{- \\sum_{i \\in I_{-}} y_i x_i}{\\sum_{i \\in I_{-}} x_i^2}\n$$\nIn the special case where there are no negative inputs (i.e., $ I_{-} $ is empty), the term $ \\mathrm{ReLU}(-x_i) $ is zero for all $ i $. The function $ f_a(x_i) = \\mathrm{ReLU}(x_i) $ becomes independent of $ a $, as does the loss function $ \\mathcal{L}(a) $. Consequently, the gradient $ \\frac{\\partial \\mathcal{L}}{\\partial a} $ is identically zero for all $ a $. In this scenario, any value of $ a $ is a minimizer. The problem specifies a convention for this case: $ a^\\star := a_0 $, where $ a_0 $ is the initial value.\n\n### Summary of Algorithms\n\n**1. Gradient Descent for $ a_T $:**\n- Initialize $ a \\gets a_0 $.\n- For $ k $ from $ 1 $ to $ T $:\n    - Compute the gradient $ g \\leftarrow \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\mathrm{ReLU}(x_i) + a \\cdot \\mathrm{ReLU}(-x_i) - y_i\\right) \\mathrm{ReLU}(-x_i) $.\n    - Update the parameter: $ a \\leftarrow a - \\eta g $.\n- The final value is $ a_T = a $.\n\n**2. Closed-Form Solution for $ a^\\star $:**\n- Identify the set of negative inputs $ \\{ (x_i, y_i) \\mid x_i  0 \\} $.\n- If this set is empty, then $ a^\\star \\leftarrow a_0 $.\n- Otherwise, compute $ a^\\star \\leftarrow \\frac{- \\sum_{i: x_i0} y_i x_i}{\\sum_{i: x_i0} x_i^2} $.\n\nThese algorithms are implemented in the provided Python code.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases, calculating the gradient descent result (a_T)\n    and the closed-form minimizer (a_star).\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"x_values\": [-2, -1, 0, 1, 2],\n            \"y_values\": [0.4, 0.2, 0, 1, 2],\n            \"a0\": 0.5,\n            \"eta\": 0.1,\n            \"T\": 40\n        },\n        {\n            \"x_values\": [0, 1, 2, 3],\n            \"y_values\": [0, 1, 2, 3],\n            \"a0\": 0.5,\n            \"eta\": 0.1,\n            \"T\": 40\n        },\n        {\n            \"x_values\": [-3, -1, -0.5, -2],\n            \"y_values\": [2.4, 0.8, 0.4, 1.6],\n            \"a0\": 0.5,\n            \"eta\": 0.1,\n            \"T\": 40\n        },\n        {\n            \"x_values\": [-2, -1, 1, 3, 0],\n            \"y_values\": [1.1, 0.6, 1, 3, 0],\n            \"a0\": 0.5,\n            \"eta\": 0.1,\n            \"T\": 40\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x = np.array(case[\"x_values\"], dtype=float)\n        y = np.array(case[\"y_values\"], dtype=float)\n        a0 = case[\"a0\"]\n        eta = case[\"eta\"]\n        T = case[\"T\"]\n        n = len(x)\n\n        # Compute a_star (closed-form minimizer)\n        neg_mask = x  0\n        x_neg = x[neg_mask]\n        \n        if x_neg.size == 0:\n            # Special case: no negative inputs, loss does not depend on 'a'\n            a_star = a0\n        else:\n            y_neg = y[neg_mask]\n            numerator = -np.sum(y_neg * x_neg)\n            denominator = np.sum(x_neg**2)\n            a_star = numerator / denominator\n\n        # Compute a_T (gradient descent)\n        a = a0\n        relu_x = np.maximum(0, x)\n        relu_neg_x = np.maximum(0, -x)\n        \n        for _ in range(T):\n            # Definition of the parametric activation function\n            # f_a(x) = ReLU(x) + a * ReLU(-x)\n            f_a = relu_x + a * relu_neg_x\n            \n            # Error term\n            error = f_a - y\n            \n            # Gradient of the loss function w.r.t. 'a'\n            # dL/da = (1/n) * sum((f_a(x_i) - y_i) * ReLU(-x_i))\n            gradient = (1.0 / n) * np.sum(error * relu_neg_x)\n            \n            # Gradient descent update\n            a = a - eta * gradient\n        \n        a_T = a\n        \n        results.append(a_T)\n        results.append(a_star)\n\n    # Format the output as specified\n    print(f\"[{','.join(f'{v:.6f}' for v in results)}]\")\n\nsolve()\n```"
        }
    ]
}