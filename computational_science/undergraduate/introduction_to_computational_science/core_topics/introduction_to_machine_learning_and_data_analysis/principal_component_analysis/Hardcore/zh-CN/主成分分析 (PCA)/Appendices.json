{
    "hands_on_practices": [
        {
            "introduction": "主成分分析（PCA）的核心思想是找到数据方差最大的方向。为了真正从根本上理解这一过程，我们不能仅仅停留在调用现成的算法库。这个练习将引导你从第一性原理出发，通过手工构建一个具有特定协方差矩阵的数据集，并利用约束优化来推导出其主成分，从而让你对特征值和特征向量在PCA中的几何意义有更深刻的体悟。",
            "id": "3177001",
            "problem": "考虑一个将使用主成分分析（PCA）进行分析的二维零均值数据集。设期望的样本协方差矩阵为\n$$\nC=\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}.\n$$\n您必须基于以下基本定义进行操作：对于中心化样本 $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{2}$，样本协方差定义为\n$$\nC=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top},\n$$\n对于任意单位方向 $\\boldsymbol{u}\\in\\mathbb{R}^{2}$（满足 $|\\boldsymbol{u}|=1$），数据投影到 $\\boldsymbol{u}$ 方向上的方差为\n$$\n\\operatorname{Var}(\\boldsymbol{u}^{\\top}\\boldsymbol{x})=\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}.\n$$\n任务：\n1. 构建一个具体的、至少包含 $n=4$ 个样本的零均值数据集，使其样本协方差恰好等于 $C$，并仅使用给定定义验证其样本协方差为 $C$。\n2. 使用第一性原理和约束优化，确定在约束 $|\\boldsymbol{u}|=1$ 下，使投影方差 $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ 最大化和最小化的 $\\mathbb{R}^{2}$ 中的单位方向，并计算相应的极值方差。\n3. 根据您的结果，计算第一主成分（任务2中的最大化方向）所捕获的总方差比例。该比例定义为最大投影方差与数据集总方差之比。\n\n仅报告此比例作为您的最终答案。您可以将答案表示为最简分数。无需四舍五入，最终答案不包含单位。",
            "solution": "该问题定义明确，具有科学依据，并包含推导出最终所求量的唯一解所需的所有信息。我们将按顺序完成这三项任务。\n\n## 任务1：数据集构建与验证\n\n第一项任务是构建一个至少包含 $n=4$ 个样本的零均值数据集，使其样本协方差矩阵恰好为 $C=\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}$。我们已知零均值数据的样本协方差定义为 $C=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$。\n\n我们选择指定的最小样本量 $n=4$。条件变为 $C=\\frac{1}{3}\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$，这意味着我们需要找到四个向量 $\\boldsymbol{x}_i \\in \\mathbb{R}^2$ 满足：\n1. 数据是零均值的：$\\sum_{i=1}^{4}\\boldsymbol{x}_{i} = \\boldsymbol{0}$。\n2. 外积之和为：$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix} = \\begin{pmatrix} 9  6 \\\\ 6  9 \\end{pmatrix}$。\n\n构建这样一个数据集的一个系统性方法是利用协方差矩阵 $C$ 的谱特性。$C$ 的特征向量代表数据方差的主轴。我们来求解 $C$ 的特征值和特征向量。特征方程为 $\\det(C-\\lambda I)=0$。\n$$\n\\det\\begin{pmatrix} 3-\\lambda  2 \\\\ 2  3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 4 = 0\n$$\n这得到 $3-\\lambda = \\pm 2$，因此特征值为 $\\lambda_1 = 3+2=5$ 和 $\\lambda_2 = 3-2=1$。\n\n对于第一个特征值 $\\lambda_1 = 5$，通过求解 $(C-5I)\\boldsymbol{u}_1=\\boldsymbol{0}$ 来找到特征向量 $\\boldsymbol{u}_1$：\n$$\n\\begin{pmatrix} -2  2 \\\\ 2  -2 \\end{pmatrix} \\begin{pmatrix} u_{11} \\\\ u_{12} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies -2u_{11} + 2u_{12} = 0 \\implies u_{11} = u_{12}\n$$\n对应的单位特征向量是 $\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n对于第二个特征值 $\\lambda_2 = 1$，通过求解 $(C-1I)\\boldsymbol{u}_2=\\boldsymbol{0}$ 来找到特征向量 $\\boldsymbol{u}_2$：\n$$\n\\begin{pmatrix} 2  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} u_{21} \\\\ u_{22} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies 2u_{21} + 2u_{22} = 0 \\implies u_{21} = -u_{22}\n$$\n对应的单位特征向量是 $\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n$C$ 的谱分解为 $C = \\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + \\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n我们需要 $\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n我们构造一个与这些特征向量对齐的对称、零均值点集：\n$\\boldsymbol{x}_1 = a\\boldsymbol{u}_1$, $\\boldsymbol{x}_2 = -a\\boldsymbol{u}_1$, $\\boldsymbol{x}_3 = b\\boldsymbol{u}_2$, $\\boldsymbol{x}_4 = -b\\boldsymbol{u}_2$。\n均值为 $\\boldsymbol{x}_1+\\boldsymbol{x}_2+\\boldsymbol{x}_3+\\boldsymbol{x}_4 = \\boldsymbol{0}$。外积之和为：\n$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = (a\\boldsymbol{u}_1)(a\\boldsymbol{u}_1)^\\top + (-a\\boldsymbol{u}_1)(-a\\boldsymbol{u}_1)^\\top + (b\\boldsymbol{u}_2)(b\\boldsymbol{u}_2)^\\top + (-b\\boldsymbol{u}_2)(-b\\boldsymbol{u}_2)^\\top = 2a^2 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 2b^2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n将其与 $3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$ 比较，可得：\n$2a^2 = 3\\lambda_1 = 3(5)=15 \\implies a^2 = \\frac{15}{2} \\implies a = \\sqrt{\\frac{15}{2}}$。\n$2b^2 = 3\\lambda_2 = 3(1)=3 \\implies b^2 = \\frac{3}{2} \\implies b = \\sqrt{\\frac{3}{2}}$。\n\n因此，我们的具体数据集为：\n$\\boldsymbol{x}_1 = \\sqrt{\\frac{15}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{\\sqrt{15}}{2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{15}}{2} \\\\ \\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_2 = -\\boldsymbol{x}_1 = \\begin{pmatrix} -\\frac{\\sqrt{15}}{2} \\\\ -\\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_3 = \\sqrt{\\frac{3}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{\\sqrt{3}}{2}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_4 = -\\boldsymbol{x}_3 = \\begin{pmatrix} -\\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n\n验证：根据构造，该数据集是零均值的。我们来计算 $\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top$。令 $\\boldsymbol{x}_i = (x_{i1}, x_{i2})^\\top$。\n$\\sum_{i=1}^{4} x_{i1}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = \\frac{30+6}{4} = \\frac{36}{4}=9$。\n$\\sum_{i=1}^{4} x_{i2}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = 9$。\n$\\sum_{i=1}^{4} x_{i1}x_{i2} = (\\frac{\\sqrt{15}}{2})(\\frac{\\sqrt{15}}{2}) + (-\\frac{\\sqrt{15}}{2}})(-\\frac{\\sqrt{15}}{2}}) + (\\frac{\\sqrt{3}}{2}})(-\\frac{\\sqrt{3}}{2}}) + (-\\frac{\\sqrt{3}}{2}})(\\frac{\\sqrt{3}}{2}}) = \\frac{15}{4} + \\frac{15}{4} - \\frac{3}{4} - \\frac{3}{4} = \\frac{30-6}{4} = \\frac{24}{4}=6$。\n所以，$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = \\begin{pmatrix} 9  6 \\\\ 6  9 \\end{pmatrix} = 3C$。样本协方差为 $\\frac{1}{3}\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top = C$，符合要求。\n\n## 任务2：通过约束优化求解极值方差方向\n\n我们要求解投影方差 $f(\\boldsymbol{u}) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ 在约束条件 $g(\\boldsymbol{u}) = |\\boldsymbol{u}|^2 - 1 = \\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1 = 0$ 下的极值。我们使用拉格朗日乘数法。拉格朗日函数为：\n$$\n\\mathcal{L}(\\boldsymbol{u}, \\lambda) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} - \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1)\n$$\n为了找到驻点，我们将关于 $\\boldsymbol{u}$ 的梯度设为零：\n$$\n\\nabla_{\\boldsymbol{u}} \\mathcal{L} = 2C\\boldsymbol{u} - 2\\lambda\\boldsymbol{u} = \\boldsymbol{0}\n$$\n这可以简化为特征值方程：\n$$\nC\\boldsymbol{u} = \\lambda\\boldsymbol{u}\n$$\n这表明，使投影方差取极值的单位向量 $\\boldsymbol{u}$ 是协方差矩阵 $C$ 的特征向量。拉格朗日乘数 $\\lambda$ 是对应的特征值。\n在特征向量 $\\boldsymbol{u}$ 处的投影方差值为 $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} = \\boldsymbol{u}^{\\top}(\\lambda\\boldsymbol{u}) = \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u})$。在约束条件 $\\boldsymbol{u}^{\\top}\\boldsymbol{u}=1$ 下，投影方差就是特征值 $\\lambda$。\n\n从任务1中，我们求得 $C$ 的特征值为 $\\lambda_1 = 5$ 和 $\\lambda_2 = 1$。\n最大投影方差是最大特征值 $\\lambda_{\\max} = 5$。达到该最大值的方向是对应的特征向量，也就是第一主成分：$\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$（及其负方向）。\n最小投影方差是最小特征值 $\\lambda_{\\min} = 1$。达到该最小值的方向是第二主成分：$\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$（及其负方向）。\n\n## 任务3：总方差占比\n\n最后一项任务是计算第一主成分所捕获的总方差比例。第一主成分是使投影方差最大化的方向，我们已求得该方向为 $\\boldsymbol{u}_1$。此主成分捕获的方差即为最大投影方差 $\\lambda_1 = 5$。\n\n数据集的总方差是沿各个维度的方差之和，即协方差矩阵的迹 $\\operatorname{Tr}(C)$。\n$$\n\\text{总方差} = \\operatorname{Tr}(C) = C_{11} + C_{22} = 3+3 = 6\n$$\n注意，这也等于特征值之和：$\\lambda_1 + \\lambda_2 = 5+1=6$，与预期相符。\n\n第一主成分所捕获的总方差比例是该主成分方向上的方差与总方差之比：\n$$\n\\text{比例} = \\frac{\\text{第一主成分方差}}{\\text{总方差}} = \\frac{\\lambda_1}{\\operatorname{Tr}(C)} = \\frac{5}{6}\n$$\n该分数为最简形式。",
            "answer": "$$\\boxed{\\frac{5}{6}}$$"
        },
        {
            "introduction": "在应用主成分分析于真实世界数据时，一个常见但至关重要的步骤是数据标准化。如果不同特征（变量）的数值尺度差异巨大，PCA的结果可能会被方差最大的特征所支配，从而掩盖数据中其他重要的结构性信息。通过这个编码练习，你将亲手对比在标准化前后进行PCA的区别，直观地看到特征缩放如何影响主成分的方向和解释方差的比例，这是正确使用PCA的一项核心技能。",
            "id": "2430028",
            "problem": "给定一个数据矩阵族，其各列的数值尺度差异巨大。对于每种情况，考虑一个实值数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其中包含 $d=3$ 个特征和 $n$ 个样本。对于下方的每个测试用例，通过显式公式确定性地定义 $X$。任务是比较对均值中心化数据和标准化数据执行主成分分析（PCA）的结果，并量化主导主成分方向及其解释方差分数因标准化而发生的变化。此处，主成分分析（PCA）定义为对变换后数据的样本协方差矩阵进行特征分解。特征标准化在此定义为对每一列进行中心化（减去其样本均值）并除以其样本标准差；如果某列的标准差为零，则在中心化后，该标准化列恒为零。\n\n使用的定义：\n- 给定 $X \\in \\mathbb{R}^{n \\times d}$，设 $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ 为列样本均值，并令 $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ 表示中心化数据，其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全为1的向量。样本协方差矩阵为 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n- PCA的特征值和特征向量是 $S$ 的特征对 $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$，其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$，特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分的解释方差比为 $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$。\n- 对于标准化数据，计算 $X_c$ 的列样本标准差 $\\sigma_j$。通过对所有 $\\sigma_j \\neq 0$ 的 $j$ 使用 $Z_{:,j} = X_{c,:,j}/\\sigma_j$ 来构成 $Z$，而对于任何 $\\sigma_j = 0$ 的 $j$ 则使用 $Z_{:,j} = \\mathbf{0}$。然后定义 $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ 及其特征对 $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$，排序为 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$，其解释方差比为 $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$。\n- 两个单位主方向 $\\mathbf{v}_1$ 和 $\\mathbf{u}_1$ 之间的一致性由 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$ 度量。特征向量的符号不确定性通过绝对值来处理。\n- 为了量化原始坐标轴对主成分的支配程度，定义 $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ 和 $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$，特征索引使用从零开始的编号。\n\n对于每个测试用例，您必须计算以下有序的量列表：\n- 从 $S$ 计算出的 $r_1$，\n- 从 $S^{(z)}$ 计算出的 $r_1^{(z)}$，\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$,\n- $i_{\\mathrm{before}}$,\n- $i_{\\mathrm{after}}$.\n\n测试套件（每个用例定义 $n$，然后是 $i \\in \\{0,\\dots,n-1\\}$ 的 $t_i$，以及作为 $t_i$ 函数的三个特征）：\n- 用例 1：$n=200$。对于每个 $i \\in \\{0,\\dots,199\\}$，令 $t_i = \\frac{i}{199}$，并定义\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$,\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$,\n  - $x_{i3} = 0.001\\, t_i$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 用例 2：$n=100$。对于每个 $i \\in \\{0,\\dots,99\\}$，令 $t_i = \\frac{i}{99}$，并定义\n  - $x_{i1} = 10^6\\, t_i$,\n  - $x_{i2} = 0$,\n  - $x_{i3} = 10\\,(t_i - 0.5)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 用例 3：$n=150$。对于每个 $i \\in \\{0,\\dots,149\\}$，令 $t_i = \\frac{i}{149}$，并定义\n  - $x_{i1} = 1000\\,(2 t_i - 1)$,\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$,\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个用例，输出有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。将三个用例的结果聚合成一个包含三个列表的单一列表，顺序为用例1、2、3。例如，整体打印结构必须是 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ 的形式。\n\n所有答案均为指定的无量纲实数或整数。不需要物理单位或角度单位，因为所有要求的量都是纯数。",
            "solution": "用户提供了一个有效且需要解答的问题。问题陈述在科学上基于线性代数和统计学领域，特别是主成分分析（PCA）。该问题定义明确，为构建数据、定义所有必要的数学对象和程序以及请求一组特定的可计算量提供了确定性的指令。语言客观且无歧义。因此，可以构建一个合理的、分步的解决方案。\n\n该问题要求对三种不同情况，比较对均值中心化数据与标准化数据执行PCA的结果。问题的核心在于观察缩放如何影响PCA的结果。PCA识别数据集中方差最大的方向。当特征（数据矩阵的列）具有差异巨大的尺度时，无论底层数据结构如何，方差最大的特征将主导第一主成分。标准化通过将每个特征重新缩放到均值为0、标准差为1，将所有特征置于同等地位，从而防止这种情况发生。\n\n总体步骤如下：\n1. 对每个测试用例，构建 $n \\times d$ 数据矩阵 $X$，其中 $d=3$。\n2. 对均值中心化数据 $X_c$ 执行PCA。\n    a. 计算列样本均值向量 $\\bar{\\mathbf{x}}$。\n    b. 对数据进行中心化：$X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$。\n    c. 计算样本协方差矩阵 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n    d. 通过求解特征问题 $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$ 来找到 $S$ 的特征值 $\\lambda_k$ 和特征向量 $\\mathbf{v}_k$。特征值已排序，$\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$，并且特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分方向是 $\\mathbf{v}_1$。\n    e. 计算第一主成分的解释方差比：$r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$。\n    f. 识别主导 $\\mathbf{v}_1$ 的原始特征：$i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$。\n\n3. 对标准化数据 $Z$ 执行PCA。\n    a. 使用 $n-1$ 作为除数，计算 $X$ 的列样本标准差 $\\sigma_j$。\n    b. 构建标准化数据矩阵 $Z$。每个列 $Z_{:,j}$ 是通过将相应的中心化列 $X_{c,:,j}$ 乘以 $1/\\sigma_j$ 得到的。如果 $\\sigma_j=0$，则列 $Z_{:,j}$ 设置为零向量。\n    c. 计算 $Z$ 的样本协方差矩阵：$S^{(z)} = \\frac{1}{n-1} Z^\\top Z$。该矩阵等价于 $X$ 的样本相关系数矩阵。对于任何非恒定特征，其对角线元素为 1。\n    d. 找到 $S^{(z)}$ 的特征值 $\\mu_k$ 和特征向量 $\\mathbf{u}_k$，排序为 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$。标准化数据的第一主成分方向是 $\\mathbf{u}_1$。\n    e. 计算相应的解释方差比：$r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$。分母中的总和 $\\text{Tr}(S^{(z)})$ 等于非恒定特征的数量。\n    f. 识别主导 $\\mathbf{u}_1$ 的原始特征：$i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$。\n\n4. 通过计算对齐度量 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$ 来比较两种分析的结果，该度量衡量了两个主方向之间夹角的余弦值。\n\n5. 对每个用例，最终输出是有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。\n\n具体用例分析：\n- **用例 1**：数据包含三个特征，其尺度分别为 $O(10^3)$、$O(1)$ 和 $O(10^{-3})$。第一个特征 $x_1 = 1000 \\cos(2\\pi t_i)$ 的方差将远远大于其他特征。因此，未标准化数据的第一个主成分 $\\mathbf{v}_1$ 预计将几乎完全与第一个特征轴对齐。这将得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}} = 0$。标准化后，所有特征的方差均为单位方差，并且结构关系（在 $(x_1, x_2)$ 平面上的椭圆轨迹）将变得明显。方差将更均匀地分布，导致较小的 $r_1^{(z)}$，并且 $\\mathbf{u}_1$ 将是特征1和2的组合。\n- **用例 2**：第一个特征 $x_1 = 10^6 t_i$ 具有巨大的尺度。第二个特征 $x_2=0$ 是常数，方差为零。第三个特征 $x_3 = 10(t_i-0.5)$ 的尺度远小于第一个特征。对于未标准化的数据，PCA 将由特征1主导，得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}}=0$。标准化后，常数特征 $x_2$ 仍然是零向量。特征1和3都是 $t_i$ 的线性函数，在中心化和缩放后将变得完全相关。它们的标准化版本将是相同的，$Z_{:,1} = Z_{:,3}$。数据将塌陷到 $(Z_1, Z_3)$ 平面上的一个方向。这将导致 $r_1^{(z)}=1$（因为在非恒定特征中有效秩为1）和一个形如 $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$ 的特征向量 $\\mathbf{u}_1$。\n- **用例 3**：第一个特征 $x_1 = 1000(2t_i-1)$ 具有大尺度。第二个特征 $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$ 与第一个特征高度相关，但尺度小得多。第三个特征的尺度可以忽略不计。与其他用例一样，未标准化的PCA将由第一个特征的尺度决定，因此 $r_1 \\approx 1$ 且 $i_{\\mathrm{before}}=0$。标准化后，特征1和2之间的强线性关系将是最突出的特征。第一主成分 $\\mathbf{u}_1$ 将捕获这种共享方差，代表了相关数据云主轴方向，大约在特征1和2的标准化轴之间成45度角。\n\n实现将使用 `numpy` 进行所有数值计算，特别是使用 `numpy.linalg.eigh` 对对称协方差矩阵进行特征分解。将注意处理特征值按降序排序以及标准差为零的情况。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance > 0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds > 0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z > 0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "主成分分析通过最大化方差来寻找数据的主要模式，但这也使其对异常值（outliers）异常敏感，因为单个远离中心的点可以极大地影响整体方差。这个练习将让你量化一个孤立的异常点对数据集第一主成分方向的影响。通过在一个结构清晰的椭圆点云中引入一个异常值，你将计算并观察主成分轴发生的偏转，深刻理解PCA在面对非理想数据时的稳健性问题。",
            "id": "2430058",
            "problem": "你需要量化一个孤立的远距离离群点如何影响一个二维数据集的第一主成分方向。考虑一个由$M$个点组成的确定性基础点云，这些点位于一个以原点为中心的椭圆上，由参数集定义：\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\tfrac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\tfrac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0,1,2,\\dots,M-1,\n$$\n其中 $r_x \\gt 0$ 且 $r_y \\gt 0$。通过在坐标\n$$\n\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}.\n$$\n处增加一个额外的点（离群点），形成一个增广数据集。\n\n令 $\\mathbf{S}_0$ 为基础点云的样本协方差矩阵，$\\mathbf{S}_1$ 为包含$M$个椭圆点和那个离群点的增广数据集的样本协方差矩阵。第一主成分定义为相应样本协方差矩阵最大特征值所对应的单位特征向量。记 $\\mathbf{u}_0 \\in \\mathbb{R}^2$ 为 $\\mathbf{S}_0$ 最大特征值对应的单位特征向量，$\\mathbf{u}_1 \\in \\mathbb{R}^2$ 为 $\\mathbf{S}_1$ 的类似特征向量。因为特征向量的定义可以相差一个符号，我们将两个主方向之间的锐角差 $\\Delta$ 定义为：\n$$\n\\Delta = \\arccos\\!\\left(\\left|\\mathbf{u}_0^\\top \\mathbf{u}_1\\right|\\right).\n$$\n你必须以弧度为单位计算 $\\Delta$。所有最终数值答案必须以弧度表示，并四舍五入到$6$位小数。\n\n测试组。对于下面的每个参数元组 $(r_x, r_y, M, o_x, o_y)$，按照规定构建基础点云和增广数据集，计算 $\\Delta$ 并报告结果：\n- 情况1（一般情况）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 10.0, 10.0)$。\n- 情况2（边界情况，无影响）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 0.0, 0.0)$。\n- 情况3（极端情况，离群点主导正交方向）：$(r_x, r_y, M, o_x, o_y) = (2.0, 1.0, 40, 0.0, 1000.0)$。\n- 情况4（近各向同性基础，中等离群点）：$(r_x, r_y, M, o_x, o_y) = (1.05, 1.0, 50, 5.0, 0.2)$。\n\n你的程序应该生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果顺序与上述情况相同，每个值都四舍五入到$6$位小数。例如，一个有效的输出格式是\n\"[x_1,x_2,x_3,x_4]\"\n其中每个 $x_i$ 是一个以弧度为单位的浮点数，小数点后恰好有$6$位数字。",
            "solution": "该问题已经过验证，被认为是有效的。这是一个计算物理和线性代数领域中定义良好的问题，基于既定的科学原理。所有术语都得到了足够严格的定义，所提供的数据是一致和完整的。\n\n任务是计算当引入一个离群点时，一个二维数据集的第一主成分的角度偏差 $\\Delta$。基础数据集是一个由$M$个均匀分布在椭圆上的点组成的点云，增广数据集额外包含一个离群点。\n\n数据集的第一主成分是方差最大的方向，它对应于样本协方差矩阵最大特征值所关联的单位特征向量。设$N$个数据点的集合为 $\\{\\mathbf{p}_i\\}_{i=1}^N$，其中每个 $\\mathbf{p}_i \\in \\mathbb{R}^2$。样本均值为 $\\bar{\\mathbf{p}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{p}_i$。样本协方差矩阵 $\\mathbf{S}$ 由下式给出：\n$$\n\\mathbf{S} = \\frac{1}{N-1}\\sum_{i=1}^N (\\mathbf{p}_i - \\bar{\\mathbf{p}})(\\mathbf{p}_i - \\bar{\\mathbf{p}})^\\top\n$$\n请注意，分母的具体选择，无论是$N$还是$N-1$，都无关紧要，因为它只对协方差矩阵进行缩放，而不会改变其特征向量。数值实现将使用分母为$N-1$的传统无偏估计量。\n\n首先，我们分析由以下公式定义的基础点云，它包含$M$个点 $\\{\\mathbf{x}_k\\}_{k=0}^{M-1}$：\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\frac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\frac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0, 1, \\dots, M-1\n$$\n由于余弦和正弦函数在整个周期上的对称性，对于$M > 1$，基础点云的样本均值 $\\bar{\\mathbf{x}}_0$ 是零向量：\n$$\n\\bar{\\mathbf{x}}_0 = \\frac{1}{M}\\sum_{k=0}^{M-1} \\mathbf{x}_k = \\begin{bmatrix} \\frac{r_x}{M}\\sum_{k=0}^{M-1}\\cos(\\frac{2\\pi k}{M}) \\\\ \\frac{r_y}{M}\\sum_{k=0}^{M-1}\\sin(\\frac{2\\pi k}{M}) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\n那么，基础点云的样本协方差矩阵 $\\mathbf{S}_0$ 为：\n$$\n\\mathbf{S}_0 = \\frac{1}{M-1}\\sum_{k=0}^{M-1} \\mathbf{x}_k \\mathbf{x}_k^\\top = \\frac{1}{M-1} \\sum_{k=0}^{M-1} \\begin{bmatrix} r_x^2 \\cos^2(\\theta_k)  r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) \\\\ r_x r_y \\cos(\\theta_k)\\sin(\\theta_k)  r_y^2 \\sin^2(\\theta_k) \\end{bmatrix}\n$$\n其中 $\\theta_k = \\frac{2\\pi k}{M}$。对于 $M > 2$，非对角项的和为零。对角项的和为 $\\sum \\cos^2(\\theta_k) = M/2$ 和 $\\sum \\sin^2(\\theta_k) = M/2$。因此，$\\mathbf{S}_0$ 是一个对角矩阵：\n$$\n\\mathbf{S}_0 = \\frac{M}{2(M-1)} \\begin{bmatrix} r_x^2  0 \\\\ 0  r_y^2 \\end{bmatrix}\n$$\n对角矩阵的特征向量是标准基向量。最大的特征值对应于 $r_x^2$ 和 $r_y^2$ 中较大的一个。如果 $r_x > r_y$，第一主成分是 $\\mathbf{u}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。如果 $r_y > r_x$，则为 $\\mathbf{u}_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。这与椭圆的半长轴方向一致。\n\n接下来，我们分析增广数据集，它由基础点云的$M$个点和离群点 $\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}$ 组成。总点数为 $N = M+1$。增广数据集的均值 $\\bar{\\mathbf{x}}_1$ 为：\n$$\n\\bar{\\mathbf{x}}_1 = \\frac{1}{M+1}\\left(\\sum_{k=0}^{M-1}\\mathbf{x}_k + \\mathbf{z}\\right) = \\frac{1}{M+1}\\mathbf{z}\n$$\n协方差矩阵 $\\mathbf{S}_1$ 是基于这 $M+1$ 个点计算的。\n$$\n\\mathbf{S}_1 = \\frac{1}{M} \\left( \\sum_{k=0}^{M-1}(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)^\\top + (\\mathbf{z} - \\bar{\\mathbf{x}}_1)(\\mathbf{z} - \\bar{\\mathbf{x}}_1)^\\top \\right)\n$$\n通常情况下，$\\mathbf{S}_1$ 不是一个对角矩阵。离群点，特别是当其坐标 $(o_x, o_y)$ 很大时，将显著改变均值并对协方差和贡献一个大项，从而旋转数据分布的主轴。第一主成分 $\\mathbf{u}_1$ 是与 $\\mathbf{S}_1$ 的最大特征值相对应的单位特征向量。这可以通过对数值计算出的 $\\mathbf{S}_1$ 进行特征分解来找到。\n\n角度差 $\\Delta$ 计算为两个主成分向量 $\\mathbf{u}_0$ 和 $\\mathbf{u}_1$ 之间的锐角：\n$$\n\\Delta = \\arccos(|\\mathbf{u}_0^\\top \\mathbf{u}_1|)\n$$\n点积的绝对值确保了角度是锐角，这是考虑到特征向量与其相反向量是等价的。\n\n解决每个测试用例的算法步骤如下：\n$1$. 根据 $r_x$ 和 $r_y$ 构建包含 $M$ 个点的椭圆基础数据集。\n$2$. 计算基础数据集的样本协方差矩阵 $\\mathbf{S}_0$。\n$3$. 求解 $\\mathbf{S}_0$ 的特征向量和特征值。与最大特征值相对应的特征向量 $\\mathbf{u}_0$ 是第一主成分。\n$4$. 通过将离群点 $\\mathbf{z}=(o_x, o_y)$ 添加到基础数据集中，构建增广数据集。\n$5$. 计算增广数据集的样本协方差矩阵 $\\mathbf{S}_1$。\n$6$. 求解 $\\mathbf{S}_1$ 的特征向量和特征值。与最大特征值相对应的特征向量 $\\mathbf{u}_1$ 是新的第一主成分。\n$7$. 以弧度为单位计算角度差 $\\Delta = \\arccos(|\\mathbf{u}_0 \\cdot \\mathbf{u}_1|)$。点积的参数应被裁剪到 $[-1, 1]$ 范围内以避免数值误差。\n$8$. 将结果四舍五入到$6$位小数。\n为每组提供的参数实现此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the angular difference in the first principal component of a 2D dataset\n    due to the addition of an outlier.\n    \"\"\"\n    test_cases = [\n        # (r_x, r_y, M, o_x, o_y)\n        (3.0, 1.0, 60, 10.0, 10.0),\n        (3.0, 1.0, 60, 0.0, 0.0),\n        (2.0, 1.0, 40, 0.0, 1000.0),\n        (1.05, 1.0, 50, 5.0, 0.2),\n    ]\n\n    results = []\n    \n    for r_x, r_y, M, o_x, o_y in test_cases:\n        # Step 1: Construct the base dataset\n        theta = (2 * np.pi / M) * np.arange(M)\n        x_coords = r_x * np.cos(theta)\n        y_coords = r_y * np.sin(theta)\n        base_cloud = np.stack((x_coords, y_coords), axis=1)\n\n        # Step 2  3: PCA on the base cloud\n        # Covariance matrix for a centered ellipse is diagonal, so we can determine u0 analytically.\n        # This is more robust and faster than numerical computation for this specific geometry.\n        # Although numerical computation would yield the same result.\n        # Let's use numerical computation for generality and to match the full algorithm described.\n        # The choice of divisor (N or N-1) does not affect the eigenvectors.\n        # np.cov uses N-1 by default.\n        cov_0 = np.cov(base_cloud, rowvar=False)\n        eigvals_0, eigvecs_0 = np.linalg.eigh(cov_0)\n        # eigh sorts eigenvalues in ascending order, so the last eigenvector is the principal one.\n        u_0 = eigvecs_0[:, -1]\n\n        # Step 4: Construct the augmented dataset\n        outlier = np.array([[o_x, o_y]])\n        augmented_cloud = np.vstack((base_cloud, outlier))\n\n        # Step 5  6: PCA on the augmented dataset\n        cov_1 = np.cov(augmented_cloud, rowvar=False)\n        eigvals_1, eigvecs_1 = np.linalg.eigh(cov_1)\n        u_1 = eigvecs_1[:, -1]\n\n        # Step 7: Compute the angular difference\n        # Dot product between the two unit eigenvectors\n        dot_product = np.dot(u_0, u_1)\n        \n        # Take the absolute value to find the acute angle\n        abs_dot_product = np.abs(dot_product)\n        \n        # Clip to handle potential floating point inaccuracies > 1.0\n        clipped_dot = np.clip(abs_dot_product, -1.0, 1.0)\n        \n        delta = np.arccos(clipped_dot)\n\n        # Step 8: Append rounded result\n        results.append(round(delta, 6))\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}