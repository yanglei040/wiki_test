{
    "hands_on_practices": [
        {
            "introduction": "Before we rely on computational libraries, it's essential to grasp the mechanics of Principal Component Analysis from the ground up. This exercise challenges you to derive the principal components for a small dataset by hand, directly connecting the statistical definition of the covariance matrix to the geometric goal of maximizing variance . By using first principles of constrained optimization, you will uncover why the eigenvectors of the covariance matrix are the directions we seek.",
            "id": "3177001",
            "problem": "Consider a two-dimensional, mean-zero dataset to be analyzed with Principal Component Analysis (PCA). Let the desired sample covariance matrix be \n$$\nC=\\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix}.\n$$\nYou must work from the following base definitions: for centered samples $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{2}$, the sample covariance is defined by \n$$\nC=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top},\n$$\nand for any unit direction $\\boldsymbol{u}\\in\\mathbb{R}^{2}$ with $|\\boldsymbol{u}|=1$, the variance of the projected data along $\\boldsymbol{u}$ is \n$$\n\\operatorname{Var}(\\boldsymbol{u}^{\\top}\\boldsymbol{x})=\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}.\n$$\nTasks: \n1. Construct a concrete, mean-zero dataset with at least $n=4$ samples whose sample covariance equals $C$ exactly, and verify that its sample covariance is $C$ using only the given definitions. \n2. Using first principles and constrained optimization, determine the unit directions in $\\mathbb{R}^{2}$ that maximize and minimize the projected variance $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ under the constraint $|\\boldsymbol{u}|=1$, and compute the corresponding extremal variances. \n3. From your results, compute the proportion of the total variance captured by the first principal component (the maximizing direction in Task $2$), defined as the ratio of the maximal projected variance to the total variance of the dataset. \n\nReport only this proportion as your final answer. You may express the answer as a simplified fraction. No rounding is required, and no units are to be included in the final answer.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information to derive a unique solution for the final requested quantity. We shall proceed by addressing each of the three tasks in sequence.\n\n## Task 1: Dataset Construction and Verification\n\nThe first task is to construct a mean-zero dataset with at least $n=4$ samples whose sample covariance matrix is exactly $C=\\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix}$. We are given the definition of the sample covariance for mean-zero data as $C=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$.\n\nLet us choose the minimum specified sample size, $n=4$. The condition becomes $C=\\frac{1}{3}\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$, which implies we need to find four vectors $\\boldsymbol{x}_i \\in \\mathbb{R}^2$ such that:\n$1$. The data is mean-zero: $\\sum_{i=1}^{4}\\boldsymbol{x}_{i} = \\boldsymbol{0}$.\n$2$. The sum of outer products is: $\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix} = \\begin{pmatrix} 9 & 6 \\\\ 6 & 9 \\end{pmatrix}$.\n\nA systematic way to construct such a dataset is to use the spectral properties of the covariance matrix $C$. The eigenvectors of $C$ represent the principal axes of the data's variance. Let's find the eigenvalues and eigenvectors of $C$. The characteristic equation is $\\det(C-\\lambda I)=0$.\n$$\n\\det\\begin{pmatrix} 3-\\lambda & 2 \\\\ 2 & 3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 4 = 0\n$$\nThis gives $3-\\lambda = \\pm 2$, so the eigenvalues are $\\lambda_1 = 3+2=5$ and $\\lambda_2 = 3-2=1$.\n\nFor the first eigenvalue, $\\lambda_1 = 5$, the eigenvector $\\boldsymbol{u}_1$ is found by solving $(C-5I)\\boldsymbol{u}_1=\\boldsymbol{0}$:\n$$\n\\begin{pmatrix} -2 & 2 \\\\ 2 & -2 \\end{pmatrix} \\begin{pmatrix} u_{11} \\\\ u_{12} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies -2u_{11} + 2u_{12} = 0 \\implies u_{11} = u_{12}\n$$\nThe corresponding unit eigenvector is $\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nFor the second eigenvalue, $\\lambda_2 = 1$, the eigenvector $\\boldsymbol{u}_2$ is found by solving $(C-1I)\\boldsymbol{u}_2=\\boldsymbol{0}$:\n$$\n\\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix} \\begin{pmatrix} u_{21} \\\\ u_{22} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies 2u_{21} + 2u_{22} = 0 \\implies u_{21} = -u_{22}\n$$\nThe corresponding unit eigenvector is $\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe spectral decomposition of $C$ is $C = \\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + \\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$.\nWe require $\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$.\nLet's construct a symmetric, mean-zero set of points aligned with these eigenvectors:\n$\\boldsymbol{x}_1 = a\\boldsymbol{u}_1$, $\\boldsymbol{x}_2 = -a\\boldsymbol{u}_1$, $\\boldsymbol{x}_3 = b\\boldsymbol{u}_2$, $\\boldsymbol{x}_4 = -b\\boldsymbol{u}_2$.\nThe mean is $\\boldsymbol{x}_1+\\boldsymbol{x}_2+\\boldsymbol{x}_3+\\boldsymbol{x}_4 = \\boldsymbol{0}$. The sum of outer products is:\n$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = (a\\boldsymbol{u}_1)(a\\boldsymbol{u}_1)^\\top + (-a\\boldsymbol{u}_1)(-a\\boldsymbol{u}_1)^\\top + (b\\boldsymbol{u}_2)(b\\boldsymbol{u}_2)^\\top + (-b\\boldsymbol{u}_2)(-b\\boldsymbol{u}_2)^\\top = 2a^2 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 2b^2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$.\nComparing this with $3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$ yields:\n$2a^2 = 3\\lambda_1 = 3(5)=15 \\implies a^2 = \\frac{15}{2} \\implies a = \\sqrt{\\frac{15}{2}}$.\n$2b^2 = 3\\lambda_2 = 3(1)=3 \\implies b^2 = \\frac{3}{2} \\implies b = \\sqrt{\\frac{3}{2}}$.\n\nSo, our concrete dataset is:\n$\\boldsymbol{x}_1 = \\sqrt{\\frac{15}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{\\sqrt{15}}{2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{15}}{2} \\\\ \\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_2 = -\\boldsymbol{x}_1 = \\begin{pmatrix} -\\frac{\\sqrt{15}}{2} \\\\ -\\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_3 = \\sqrt{\\frac{3}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{\\sqrt{3}}{2}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_4 = -\\boldsymbol{x}_3 = \\begin{pmatrix} -\\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n\nVerification: The dataset is mean-zero by construction. Let's compute $\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top$. Let $\\boldsymbol{x}_i = (x_{i1}, x_{i2})^\\top$.\n$\\sum_{i=1}^{4} x_{i1}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = \\frac{30+6}{4} = \\frac{36}{4}=9$.\n$\\sum_{i=1}^{4} x_{i2}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = 9$.\n$\\sum_{i=1}^{4} x_{i1}x_{i2} = (\\frac{\\sqrt{15}}{2})(\\frac{\\sqrt{15}}{2}) + (-\\frac{\\sqrt{15}}{2}})(-\\frac{\\sqrt{15}}{2}}) + (\\frac{\\sqrt{3}}{2}})(-\\frac{\\sqrt{3}}{2}}) + (-\\frac{\\sqrt{3}}{2}})(\\frac{\\sqrt{3}}{2}}) = \\frac{15}{4} + \\frac{15}{4} - \\frac{3}{4} - \\frac{3}{4} = \\frac{30-6}{4} = \\frac{24}{4}=6$.\nSo, $\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = \\begin{pmatrix} 9 & 6 \\\\ 6 & 9 \\end{pmatrix} = 3C$. The sample covariance is $\\frac{1}{3}\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top = C$, as required.\n\n## Task 2: Extremal Variance Directions via Constrained Optimization\n\nWe want to find the extrema of the projected variance $f(\\boldsymbol{u}) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ subject to the constraint $g(\\boldsymbol{u}) = |\\boldsymbol{u}|^2 - 1 = \\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1 = 0$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(\\boldsymbol{u}, \\lambda) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} - \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1)\n$$\nTo find the stationary points, we set the gradient with respect to $\\boldsymbol{u}$ to zero:\n$$\n\\nabla_{\\boldsymbol{u}} \\mathcal{L} = 2C\\boldsymbol{u} - 2\\lambda\\boldsymbol{u} = \\boldsymbol{0}\n$$\nThis simplifies to the eigenvalue equation:\n$$\nC\\boldsymbol{u} = \\lambda\\boldsymbol{u}\n$$\nThis shows that the unit vectors $\\boldsymbol{u}$ that extremize the projected variance are the eigenvectors of the covariance matrix $C$. The Lagrange multiplier $\\lambda$ is the corresponding eigenvalue.\nThe value of the projected variance at an eigenvector $\\boldsymbol{u}$ is $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} = \\boldsymbol{u}^{\\top}(\\lambda\\boldsymbol{u}) = \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u})$. Given the constraint $\\boldsymbol{u}^{\\top}\\boldsymbol{u}=1$, the projected variance is simply the eigenvalue $\\lambda$.\n\nFrom Task 1, we found the eigenvalues of $C$ to be $\\lambda_1 = 5$ and $\\lambda_2 = 1$.\nThe maximum projected variance is the largest eigenvalue, $\\lambda_{\\max} = 5$. The direction that achieves this maximum is the corresponding eigenvector, which is the first principal component: $\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ (and its negative).\nThe minimum projected variance is the smallest eigenvalue, $\\lambda_{\\min} = 1$. The direction that achieves this minimum is the second principal component: $\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ (and its negative).\n\n## Task 3: Proportion of Total Variance\n\nThe final task is to compute the proportion of the total variance captured by the first principal component. The first principal component is the direction that maximizes the projected variance, which we found to be $\\boldsymbol{u}_1$. The variance captured by this component is the maximal projected variance, $\\lambda_1 = 5$.\n\nThe total variance of the dataset is the sum of the variances along each dimension, which is the trace of the covariance matrix, $\\operatorname{Tr}(C)$.\n$$\n\\text{Total Variance} = \\operatorname{Tr}(C) = C_{11} + C_{22} = 3+3 = 6\n$$\nNote that this is also equal to the sum of the eigenvalues: $\\lambda_1 + \\lambda_2 = 5+1=6$, as expected.\n\nThe proportion of the total variance captured by the first principal component is the ratio of the variance along that component to the total variance:\n$$\n\\text{Proportion} = \\frac{\\text{Variance of PC1}}{\\text{Total Variance}} = \\frac{\\lambda_1}{\\operatorname{Tr}(C)} = \\frac{5}{6}\n$$\nThis fraction is in its simplest form.",
            "answer": "$$\\boxed{\\frac{5}{6}}$$"
        },
        {
            "introduction": "Principal Component Analysis identifies axes of maximum variance, but what happens when the original features have vastly different scales? This practical coding exercise explores a critical aspect of applying PCA: the necessity of feature standardization . You will investigate how variables with large magnitudes can dominate the analysis and how scaling the data to have unit variance reveals the true underlying structure.",
            "id": "2430028",
            "problem": "You are given a family of data matrices with columns on vastly different numerical scales. For each case, consider a real-valued data matrix $X \\in \\mathbb{R}^{n \\times d}$ with $d=3$ features and $n$ samples. Define $X$ deterministically by explicit formulas for each test case below. The task is to compare Principal Component Analysis (PCA) performed on the mean-centered data and on the standardized data, and to quantify how the dominant principal component direction and its explained variance fraction change due to standardization. Principal Component Analysis (PCA) is defined here as the eigen-decomposition of the sample covariance matrix of the transformed data. Standardization of features is defined here as centering each column (subtracting its sample mean) and dividing by its sample standard deviation; if a column has zero standard deviation, leave that standardized column identically zero after centering.\n\nDefinitions to use:\n- Given $X \\in \\mathbb{R}^{n \\times d}$, let $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ be the column-wise sample mean and let $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ denote the centered data, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones. The sample covariance matrix is $S = \\frac{1}{n-1} X_c^\\top X_c$.\n- The PCA eigenvalues and eigenvectors are the eigenpairs $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$ of $S$, with $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$ and orthonormal eigenvectors $\\{\\mathbf{v}_k\\}$. The explained variance ratio of the first principal component is $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$.\n- For standardized data, compute the column-wise sample standard deviations $\\sigma_j$ of $X_c$. Form $Z$ by $Z_{:,j} = X_{c,:,j}/\\sigma_j$ for all $j$ with $\\sigma_j \\neq 0$, and $Z_{:,j} = \\mathbf{0}$ for any $j$ with $\\sigma_j = 0$. Then define $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ and its eigenpairs $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$ sorted $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$, with explained variance ratio $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$.\n- The alignment between two unit principal directions $\\mathbf{v}_1$ and $\\mathbf{u}_1$ is measured by $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$. The sign indeterminacy of eigenvectors is handled by the absolute value.\n- To quantify component dominance by original axes, define $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ and $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$, using zero-based indexing for feature indices.\n\nFor each test case, you must compute the following ordered list of quantities:\n- $r_1$ computed from $S$,\n- $r_1^{(z)}$ computed from $S^{(z)}$,\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$,\n- $i_{\\mathrm{before}}$,\n- $i_{\\mathrm{after}}$.\n\nTest suite (each case defines $n$, then $t_i$ for $i \\in \\{0,\\dots,n-1\\}$, and the three features as functions of $t_i$):\n- Case $1$: $n=200$. For each $i \\in \\{0,\\dots,199\\}$, let $t_i = \\frac{i}{199}$, and define\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$,\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$,\n  - $x_{i3} = 0.001\\, t_i$.\n  Assemble $X$ by stacking these three features as columns.\n- Case $2$: $n=100$. For each $i \\in \\{0,\\dots,99\\}$, let $t_i = \\frac{i}{99}$, and define\n  - $x_{i1} = 10^6\\, t_i$,\n  - $x_{i2} = 0$,\n  - $x_{i3} = 10\\,(t_i - 0.5)$.\n  Assemble $X$ by stacking these three features as columns.\n- Case $3$: $n=150$. For each $i \\in \\{0,\\dots,149\\}$, let $t_i = \\frac{i}{149}$, and define\n  - $x_{i1} = 1000\\,(2 t_i - 1)$,\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$,\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$.\n  Assemble $X$ by stacking these three features as columns.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the ordered list $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$. Aggregate the three cases into a single list of three lists, in the order of cases $1$, $2$, $3$. For example, the overall printed structure must be of the form $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$.\n\nAll answers are dimensionless real numbers or integers as specified. No physical units or angle units are required because all requested quantities are pure numbers.",
            "solution": "The user has provided a problem that is valid and requires a solution. The problem statement is scientifically grounded in the fields of linear algebra and statistics, specifically Principal Component Analysis (PCA). It is well-posed, providing deterministic instructions for constructing the data, defining all necessary mathematical objects and procedures, and requesting a set of specific, computable quantities. The language is objective and free of ambiguity. Therefore, a reasoned, step-by-step solution can be constructed.\n\nThe problem requires a comparison of PCA performed on mean-centered data versus standardized data for three distinct cases. The core of the problem lies in observing how scaling affects the outcome of PCA. PCA identifies the directions of maximum variance in a dataset. When features (columns of the data matrix) have vastly different scales, the feature with the largest variance will dominate the first principal component, regardless of the underlying data structure. Standardization, which rescales each feature to have a mean of $0$ and a standard deviation of $1$, prevents this by placing all features on an equal footing.\n\nThe overall procedure is as follows:\n$1$. For each test case, construct the $n \\times d$ data matrix $X$, where $d=3$.\n$2$. Perform PCA on the mean-centered data $X_c$.\n    a. Compute the column-wise sample mean vector $\\bar{\\mathbf{x}}$.\n    b. Center the data: $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$.\n    c. Compute the sample covariance matrix $S = \\frac{1}{n-1} X_c^\\top X_c$.\n    d. Find the eigenvalues $\\lambda_k$ and eigenvectors $\\mathbf{v}_k$ of $S$ by solving the eigenproblem $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$. The eigenvalues are sorted, $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$, and the eigenvectors $\\{\\mathbf{v}_k\\}$ are orthonormal. The first principal component direction is $\\mathbf{v}_1$.\n    e. Compute the explained variance ratio for the first component: $r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$.\n    f. Identify the original feature that dominates $\\mathbf{v}_1$: $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$.\n\n$3$. Perform PCA on the standardized data $Z$.\n    a. Compute the column-wise sample standard deviations, $\\sigma_j$, of $X$ using a divisor of $n-1$.\n    b. Construct the standardized data matrix $Z$. Each column $Z_{:,j}$ is obtained by scaling the corresponding centered column $X_{c,:,j}$ by $1/\\sigma_j$. If $\\sigma_j=0$, the column $Z_{:,j}$ is set to a zero vector.\n    c. Compute the sample covariance matrix of $Z$: $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$. This matrix is equivalent to the sample correlation matrix of $X$. Its diagonal entries are $1$ for any non-constant feature.\n    d. Find the eigenvalues $\\mu_k$ and eigenvectors $\\mathbf{u}_k$ of $S^{(z)}$, sorted such that $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$. The first principal component direction of the standardized data is $\\mathbf{u}_1$.\n    e. Compute the corresponding explained variance ratio: $r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$. The sum in the denominator, $\\text{Tr}(S^{(z)})$, equals the number of non-constant features.\n    f. Identify the original feature that dominates $\\mathbf{u}_1$: $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$.\n\n$4$. Compare the results from the two analyses by computing the alignment metric $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$, which measures the cosine of the angle between the two principal directions.\n\n$5$. For each case, the final output is the ordered list $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$.\n\nCase-specific analysis:\n- **Case 1**: The data consists of three features with scales $O(10^3)$, $O(1)$, and $O(10^{-3})$. The variance of the first feature, $x_1 = 1000 \\cos(2\\pi t_i)$, will be overwhelmingly larger than the others. Thus, the first principal component $\\mathbf{v}_1$ of the unstandardized data is expected to align almost perfectly with the first feature axis. This will yield $r_1 \\approx 1$ and $i_{\\mathrm{before}} = 0$. After standardization, all features have unit variance, and the structural relationship (an elliptical trajectory in the $(x_1, x_2)$ plane) will become apparent. The variance will be distributed more equitably, leading to a smaller $r_1^{(z)}$, and $\\mathbf{u}_1$ will be a combination of features $1$ and $2$.\n- **Case 2**: The first feature, $x_1 = 10^6 t_i$, has a massive scale. The second feature, $x_2=0$, is constant and has zero variance. The third feature, $x_3 = 10(t_i-0.5)$, has a much smaller scale than the first. For the unstandardized data, PCA will be dominated by feature $1$, giving $r_1 \\approx 1$ and $i_{\\mathrm{before}}=0$. After standardization, the constant feature $x_2$ remains a zero vector. Features $1$ and $3$ are both linear functions of $t_i$ and will become perfectly correlated after centering and scaling. Their standardized versions will be identical, $Z_{:,1} = Z_{:,3}$. The data will collapse onto a single direction in the $(Z_1, Z_3)$ plane. This will result in $r_1^{(z)}=1$ (as the effective rank is $1$ among the non-constant features) and an eigenvector $\\mathbf{u}_1$ of the form $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$.\n- **Case 3**: The first feature, $x_1 = 1000(2t_i-1)$, has a large scale. The second feature, $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$, is highly correlated with the first but has a much smaller scale. The third feature is of negligible scale. As with the other cases, unstandardized PCA will be dictated by the first feature's scale, so $r_1 \\approx 1$ and $i_{\\mathrm{before}}=0$. After standardization, the strong linear relationship between features $1$ and $2$ will be the most prominent characteristic. The first principal component $\\mathbf{u}_1$ will capture this shared variance, representing a direction along the major axis of the correlated cloud, roughly at a $45$-degree angle between the standardized axes of features $1$ and $2$.\n\nThe implementation will utilize `numpy` for all numerical computations, particularly `numpy.linalg.eigh` for the eigendecomposition of the symmetric covariance matrices. Care will be taken to handle the sorting of eigenvalues in descending order and the case of zero standard deviation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance > 0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds > 0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z > 0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of PCA is often to reduce dimensionality by transforming data into a new, more compact representation. This final practice guides you through implementing the complete PCA workflow to project a dataset onto its principal components, generating the 'scores' that form the anonymized or reduced-dimension dataset . This exercise solidifies the procedure for using PCA as a powerful data transformation and feature extraction tool.",
            "id": "2421751",
            "problem": "You are given finite data matrices representing customer attributes. For each matrix, construct an anonymized representation by replacing the original features with the first $k$ principal components, defined as follows. Let $X \\in \\mathbb{R}^{n \\times d}$ denote the original data matrix with $n$ observations and $d$ features. Define the column means $\\mu \\in \\mathbb{R}^{d}$ by $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ for $j \\in \\{1,\\dots,d\\}$. Let the centered data be $X_c = X - \\mathbf{1}\\mu^{\\top}$, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones. Define the sample covariance matrix $S \\in \\mathbb{R}^{d \\times d}$ by\n$$\nS = \\frac{1}{n-1} X_c^{\\top} X_c.\n$$\nLet $(\\lambda_1, v_1), \\dots, (\\lambda_d, v_d)$ denote the eigenvalueâ€“eigenvector pairs of $S$, where the eigenvalues are ordered in nonincreasing order $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$, and $v_j \\in \\mathbb{R}^{d}$ are corresponding unit-norm eigenvectors. To ensure a unique sign convention, for each $v_j$ let $p(j)$ be the smallest index achieving $\\max_{\\ell \\in \\{1,\\dots,d\\}} |(v_j)_\\ell|$, and if $(v_j)_{p(j)} < 0$ then replace $v_j$ by $-v_j$. Define $V_k = [v_1 \\ \\cdots \\ v_k] \\in \\mathbb{R}^{d \\times k}$. The anonymized representation (principal component scores) is\n$$\nZ = X_c V_k \\in \\mathbb{R}^{n \\times k}.\n$$\nIf $k = 0$, define $Z$ to be the $n \\times 0$ empty matrix. For each test case, you must output the entries of $Z$ flattened in row-major order (i.e., by concatenating rows from the first row to the last row), with each real number rounded to exactly $6$ decimal places. If $Z$ has no entries, output an empty list.\n\nYour program must process the following test suite. For each item, $X$ is specified explicitly and $k$ is given:\n\n- Test case $1$: $X \\in \\mathbb{R}^{4 \\times 3}$ and $k = 2$,\n$$\nX =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 2 & 1 \\\\\n4 & 0 & -1\n\\end{bmatrix}.\n$$\n\n- Test case $2$: Use the same $X$ as in test case $1$ and $k = 0$.\n\n- Test case $3$: $X \\in \\mathbb{R}^{3 \\times 2}$ and $k = 1$,\n$$\nX =\n\\begin{bmatrix}\n1 & 2 \\\\\n2 & 4 \\\\\n3 & 6\n\\end{bmatrix}.\n$$\n\n- Test case $4$: $X \\in \\mathbb{R}^{5 \\times 2}$ and $k = 2$,\n$$\nX =\n\\begin{bmatrix}\n10 & 0 \\\\\n12 & 2 \\\\\n9 & -1 \\\\\n11 & 1 \\\\\n13 & 3\n\\end{bmatrix}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a comma-separated list enclosed in square brackets, containing the flattened values of $Z$ in row-major order, each rounded to exactly $6$ decimal places. For example, the overall format is\n$$\n\\big[ [z_{1,1}, z_{1,2}, \\dots], [\\ ], [\\dots], [\\dots] \\big],\n$$\nwith no spaces in the printed output and each $z_{i}$ rounded to $6$ decimal places. No other text should be printed.",
            "solution": "The problem statement has been rigorously validated and found to be self-contained, consistent, and scientifically sound. It presents a clear and formal definition of a standard procedure for Principal Component Analysis (PCA), complete with a deterministic sign convention for eigenvectors to ensure a unique solution. The problem is well-posed and its components are based on established principles of linear algebra and statistics. We shall therefore proceed with its solution.\n\nThe algorithm to be implemented follows the precise steps laid out in the problem specification. Given a data matrix $X \\in \\mathbb{R}^{n \\times d}$ with $n$ observations and $d$ features, and a non-negative integer $k$, the anonymized representation $Z \\in \\mathbb{R}^{n \\times k}$ is constructed as follows.\n\n1.  **Mean Centering of Data**:\n    First, we compute the mean of each feature (column). The mean vector $\\mu \\in \\mathbb{R}^{d}$ is defined by its components $\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$ for $j=1, \\dots, d$.\n    The data matrix $X$ is then centered by subtracting this mean vector from each observation (row). This yields the centered data matrix $X_c = X - \\mathbf{1}\\mu^{\\top}$, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is a column vector of ones. Each column of $X_c$ has a mean of $0$.\n\n2.  **Sample Covariance Matrix**:\n    Next, we compute the sample covariance matrix $S \\in \\mathbb{R}^{d \\times d}$. This is given by the formula $S = \\frac{1}{n-1} X_c^{\\top} X_c$. The use of the factor $\\frac{1}{n-1}$ corresponds to Bessel's correction for an unbiased estimate of the population covariance. This is well-defined as all test cases satisfy $n > 1$.\n\n3.  **Eigendecomposition of the Covariance Matrix**:\n    We then find the eigenvalues and eigenvectors of the symmetric, positive semi-definite matrix $S$. This is achieved by solving the eigenvalue problem $S v = \\lambda v$. This yields $d$ eigenvalue-eigenvector pairs $(\\lambda_1, v_1), (\\lambda_2, v_2), \\dots, (\\lambda_d, v_d)$. The eigenvalues are sorted in non-increasing order, $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$. The corresponding eigenvectors $v_j$ are chosen to have unit norm, i.e., $\\|v_j\\|_2 = 1$.\n\n4.  **Unique Sign Convention for Eigenvectors**:\n    Since an eigenvector $v_j$ and its negative $-v_j$ are equivalent, a deterministic sign convention is imposed to ensure a unique result. For each eigenvector $v_j$, we find the index $p(j)$ of the element with the largest absolute value. The problem specifies that $p(j)$ should be the smallest such index if there are ties: $p(j) = \\min(\\arg\\max_{\\ell \\in \\{1,\\dots,d\\}} |(v_j)_\\ell|)$. If the element at this index, $(v_j)_{p(j)}$, is negative, the eigenvector's sign is flipped: $v_j \\leftarrow -v_j$.\n\n5.  **Projection onto Principal Components**:\n    The first $k$ eigenvectors, post-normalization, form the columns of the projection matrix $V_k = [v_1, v_2, \\dots, v_k] \\in \\mathbb{R}^{d \\times k}$. The final anonymized data, also known as the principal component scores, is obtained by projecting the centered data onto these principal directions: $Z = X_c V_k$. The resulting matrix $Z$ has dimensions $n \\times k$. If $k=0$, $V_0$ is a $d \\times 0$ matrix, resulting in $Z$ being an $n \\times 0$ empty matrix.\n\nThe implementation will be a direct translation of this procedure into Python code using the `numpy` library for numerical computations. A function will encapsulate these steps, process each test case provided, and format the output according to the specified rounding and layout rules.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Processes all test cases for PCA and prints the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([\n            [1, 2, 3],\n            [2, 3, 4],\n            [3, 2, 1],\n            [4, 0, -1]\n        ], dtype=float), 2),\n        (np.array([\n            [1, 2, 3],\n            [2, 3, 4],\n            [3, 2, 1],\n            [4, 0, -1]\n        ], dtype=float), 0),\n        (np.array([\n            [1, 2],\n            [2, 4],\n            [3, 6]\n        ], dtype=float), 1),\n        (np.array([\n            [10, 0],\n            [12, 2],\n            [9, -1],\n            [11, 1],\n            [13, 3]\n        ], dtype=float), 2)\n    ]\n\n    all_case_results_str = []\n    \n    for X, k in test_cases:\n        # Main logic to calculate the PCA scores for one case.\n        n, d = X.shape\n\n        if k == 0:\n            # For k=0, the resulting matrix is empty.\n            Z = np.empty((n, 0))\n        else:\n            # Step 1: Mean Centering of Data\n            mu = np.mean(X, axis=0)\n            Xc = X - mu\n\n            # Step 2: Sample Covariance Matrix\n            # The problem assumes n > 1 for all test cases.\n            S = (Xc.T @ Xc) / (n - 1)\n\n            # Step 3: Eigendecomposition of the Covariance Matrix\n            # np.linalg.eigh is for symmetric matrices and returns eigenvalues in ascending order.\n            eigenvalues, eigenvectors = np.linalg.eigh(S)\n\n            # Sort eigenvalues and corresponding eigenvectors in descending order.\n            idx = np.argsort(eigenvalues)[::-1]\n            eigenvectors = eigenvectors[:, idx]\n\n            # Step 4: Unique Sign Convention for Eigenvectors\n            for j in range(d):\n                # Find the smallest index of the element with the maximum absolute value.\n                # np.argmax returns the first occurrence, which satisfies the condition.\n                p_j = np.argmax(np.abs(eigenvectors[:, j]))\n                # If the element at this index is negative, flip the eigenvector's sign.\n                if eigenvectors[p_j, j] < 0:\n                    eigenvectors[:, j] *= -1\n            \n            # Step 5: Projection onto Principal Components\n            # Form the matrix Vk from the first k eigenvectors.\n            Vk = eigenvectors[:, :k]\n            # Project the centered data to get the scores.\n            Z = Xc @ Vk\n\n        # Format the output string for the current test case.\n        if Z.size == 0:\n            all_case_results_str.append(\"[]\")\n        else:\n            # Flatten the Z matrix in row-major order and format each number.\n            z_flat_str = [\"{:.6f}\".format(x) for x in Z.flatten()]\n            case_str = \"[\" + \",\".join(z_flat_str) + \"]\"\n            all_case_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    # The output string is built manually to avoid spaces introduced by list-to-string conversion.\n    final_output_str = \"[\" + \",\".join(all_case_results_str) + \"]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}