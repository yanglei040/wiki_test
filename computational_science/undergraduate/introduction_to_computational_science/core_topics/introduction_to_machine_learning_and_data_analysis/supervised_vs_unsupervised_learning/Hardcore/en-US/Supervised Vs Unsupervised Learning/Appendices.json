{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is designed to offer a crystal-clear illustration of the fundamental difference between supervised and unsupervised learning objectives. We will explore datasets where a supervised classifier can achieve high accuracy by learning to separate labeled groups, while an unsupervised clustering algorithm, which only sees the geometry of the data, struggles. This practice will highlight how a feature that is irrelevant for classification can nevertheless dominate the data's structure and mislead an unsupervised method, forcing us to consider what each paradigm is truly optimizing for .",
            "id": "3199424",
            "problem": "In supervised learning, a classifier is trained to map feature vectors to given labels to minimize empirical error, while in unsupervised learning, structure is inferred from the data alone without labels. Consider points in two-dimensional Euclidean space with features denoted by $x = (x_1, x_2) \\in \\mathbb{R}^2$ and binary class labels $y \\in \\{0, 1\\}$. The supervised objective is to choose predictions $\\hat{y}$ for each $x$ that minimize the empirical classification error on the training set, a direct instance of Empirical Risk Minimization (ERM). The unsupervised objective in clustering is to partition points into $k$ groups to optimize a cohesion and separation criterion; a widely used option is the silhouette score based on pairwise Euclidean distances. This problem operationalizes the comparison: you will compute a supervised accuracy using a simple threshold classifier trained on labels, and an unsupervised silhouette score using K-Means Clustering (K-Means) run on the raw features with a deterministic initialization, then evaluate three datasets designed to highlight when the two objectives diverge.\n\nDefinitions and required computations:\n- Supervised classifier. Restrict to a one-dimensional decision on the first coordinate $x_1$. For a threshold $t \\in \\mathbb{R}$ and a left-side class $c_{\\text{left}} \\in \\{0, 1\\}$, the classifier predicts\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}},  \\text{if } x_1 \\le t,\\\\\n1 - c_{\\text{left}},  \\text{if } x_1 > t.\n\\end{cases}\n$$\nAmong all thresholds formed by midpoints between consecutive sorted $x_1$ values (including values just below the minimum and just above the maximum), choose the $(t, c_{\\text{left}})$ that maximizes the training accuracy\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right),\n$$\nwhere $N$ is the number of samples and $\\mathbf{1}(\\cdot)$ is the indicator function. In ties, choose $c_{\\text{left}} = 0$ and the smallest $t$ achieving the maximum. Report $\\text{Acc}$ as a decimal in $[0,1]$.\n- Unsupervised clustering. Run K-Means with $k = 2$ clusters on the feature vectors $x^{(i)}$ using Euclidean distance. Initialize the two centroids deterministically as the points with minimal and maximal $x_1$ values. Iteratively reassign points to the nearest centroid and update centroids as the mean of assigned points until assignments stabilize or until $100$ iterations elapse. If a cluster becomes empty, reinitialize its centroid as the point farthest (in Euclidean distance) from the other centroid. After convergence, compute the silhouette score. For each point $i$, let $a(i)$ be the average Euclidean distance from $x^{(i)}$ to all other points in its assigned cluster (if that cluster has size $1$, let $a(i) = 0$). Let $b(i)$ be the minimum, over all other clusters, of the average Euclidean distance from $x^{(i)}$ to points in that cluster. The silhouette value is\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{if } \\max\\{a(i), b(i)\\} > 0,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\nand the silhouette score is the mean of $s(i)$ over all points.\n\nTest suite datasets (each sample is given as $(x_1, x_2)$ with an associated label $y$; the order is arbitrary and all numbers are real):\n1. Dataset A (moderate separation with slight overlap along $x_1$):\n   - Class $0$: $(-0.4, 0.0)$, $(-0.2, 0.1)$, $(0.2, -0.1)$, $(0.3, 0.05)$\n   - Class $1$: $(0.2, 0.0)$, $(0.6, -0.1)$, $(0.8, 0.1)$, $(1.0, -0.05)$\n2. Dataset B (greater separation along $x_1$ but large noise along $x_2$):\n   - Class $0$: $(-0.4, 5.0)$, $(-0.2, -5.0)$, $(0.2, 7.0)$, $(0.3, -7.0)$\n   - Class $1$: $(1.2, 6.0)$, $(1.6, -6.0)$, $(1.8, 8.0)$, $(2.0, -8.0)$\n3. Dataset C (near-complete overlap: identical distributions by construction):\n   - Class $0$: $(-0.1, 0.05)$, $(0.0, -0.02)$, $(0.1, 0.03)$, $(-0.05, -0.04)$\n   - Class $1$: $(-0.08, 0.01)$, $(0.02, -0.03)$, $(0.09, 0.04)$, $(-0.02, -0.05)$\n\nYour program must implement the above procedures and, for each dataset, output the pair $[\\text{Acc}, \\text{Sil}]$, where $\\text{Acc}$ is the supervised training accuracy and $\\text{Sil}$ is the silhouette score of the final K-Means clustering. Express each value as a decimal rounded to six places. Final output format: a single line containing a top-level list of three pairs, in the dataset order A, B, C, with comma-separated entries, for example, $[[a_1,s_1],[a_2,s_2],[a_3,s_3]]$ with numerals to six decimal places.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of machine learning, is well-posed with deterministic procedures ensuring a unique solution, and is expressed in objective, formal language. All data and definitions required for a solution are present and consistent. I will now proceed with a complete solution.\n\nThe problem requires a comparative analysis of supervised and unsupervised learning on three datasets. For each dataset, we must compute two metrics: the accuracy of a simple supervised classifier and the silhouette score of an unsupervised clustering algorithm.\n\nA dataset consists of $N$ points in a two-dimensional space, where each point $x^{(i)} = (x_1^{(i)}, x_2^{(i)}) \\in \\mathbb{R}^2$ is associated with a binary label $y^{(i)} \\in \\{0, 1\\}$.\n\n### Supervised Classification: 1D Threshold Classifier\n\nThe supervised learning task is to find the optimal one-dimensional threshold classifier that minimizes classification error on the training set. This is an instance of Empirical Risk Minimization (ERM).\n\nThe classifier's prediction $\\hat{y}(x)$ for a point $x = (x_1, x_2)$ is defined based on a threshold $t$ on the first feature $x_1$ and a class assignment $c_{\\text{left}} \\in \\{0, 1\\}$ for the \"left\" side of the decision boundary:\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}},  \\text{if } x_1 \\le t, \\\\\n1 - c_{\\text{left}},  \\text{if } x_1 > t.\n\\end{cases}\n$$\nThe goal is to select the pair $(t, c_{\\text{left}})$ that maximizes the training accuracy, defined as:\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right)\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function which is $1$ if its argument is true and $0$ otherwise.\n\nTo find the optimal classifier, we must search over a set of candidate thresholds $t$ and both possible values for $c_{\\text{left}}$. The classification boundary only needs to be checked at locations that change the partitioning of the data. Let the unique sorted values of the first feature be $u_1  u_2  \\dots  u_m$. The set of meaningful thresholds consists of the midpoints between these consecutive values, $t_j = (u_j + u_{j+1})/2$ for $j=1, \\dots, m-1$. Additionally, to account for cases where all points are classified into a single group, we test thresholds below the minimum $x_1$ value (e.g., $u_1 - 1$) and above the maximum $x_1$ value (e.g., $u_m + 1$).\n\nFor each candidate threshold $t$, we evaluate the accuracy for both $c_{\\text{left}} = 0$ and $c_{\\text{left}} = 1$. We select the pair $(t, c_{\\text{left}})$ that yields the highest accuracy. The problem specifies a tie-breaking rule: in case of a tie in accuracy, we must choose the classifier with $c_{\\text{left}} = 0$. If there is still a tie, the one with the smallest threshold $t$ is chosen.\n\n### Unsupervised Clustering: K-Means with Silhouette Score\n\nThe unsupervised task is to partition the data points $x^{(i)}$ into $k=2$ clusters using the K-Means algorithm and then evaluate the quality of this partition using the silhouette score.\n\n**K-Means Algorithm**\nThe algorithm proceeds as follows:\n1.  **Initialization**: The two centroids, $\\mu_1$ and $\\mu_2$, are deterministically initialized to the data points with the minimum and maximum values of the first feature, $x_1$, respectively.\n2.  **Iteration**: The algorithm iterates between two steps until cluster assignments stabilize or a maximum of $100$ iterations is reached.\n    a.  **Assignment Step**: Each data point $x^{(i)}$ is assigned to the cluster corresponding to the nearest centroid, based on Euclidean distance: $C_j \\leftarrow \\{x^{(i)} \\mid \\|x^{(i)} - \\mu_j\\|_2 \\le \\|x^{(i)} - \\mu_l\\|_2 \\text{ for all } l=1, \\dots, k\\}$.\n    b.  **Update Step**: Each centroid $\\mu_j$ is re-computed as the mean of all points assigned to its cluster: $\\mu_j \\leftarrow \\frac{1}{|C_j|} \\sum_{x \\in C_j} x$.\n3.  **Empty Cluster Handling**: If a cluster $C_j$ becomes empty during the update step, its centroid is reinitialized. The new centroid is set to be the data point in the entire dataset that is farthest (in Euclidean distance) from the other (non-empty) cluster's centroid.\n\n**Silhouette Score**\nAfter the K-Means algorithm converges, the resulting clustering is evaluated using the silhouette score. For each point $x^{(i)}$, we calculate:\n-   $a(i)$: The average Euclidean distance from $x^{(i)}$ to all other points within its own cluster. If the cluster contains only one point, $a(i) = 0$.\n-   $b(i)$: The average Euclidean distance from $x^{(i)}$ to all points in the nearest neighboring cluster. Since $k=2$, this is simply the average distance to all points in the other cluster.\n\nThe silhouette value for point $i$ is then:\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{if } \\max\\{a(i), b(i)\\} > 0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nThe silhouette score for the entire clustering is the mean of the silhouette values $s(i)$ over all data points. A score near $1$ indicates well-separated clusters, a score near $0$ indicates overlapping clusters, and a negative score suggests points might have been assigned to the wrong cluster.\n\n### Application to Datasets\n\nThe described procedures are applied to each of the three provided datasets. For each dataset, we first construct the feature matrix $X$ and the label vector $y$. Then, we execute the supervised accuracy calculation and the unsupervised K-Means/silhouette score calculation. The resulting pair of values, $[\\text{Acc}, \\text{Sil}]$, is computed for each dataset, with each value rounded to six decimal places.\n\n- **Dataset A** presents a scenario with moderate separation along $x_1$, but some overlap, making the supervised task non-trivial. The unsupervised clustering will depend on the geometric structure of the point cloud.\n- **Dataset B** is designed such that the labels are perfectly separable along $x_1$, but a large variance along $x_2$ acts as a distractor. This tests whether the unsupervised algorithm is misled by the noisy feature $x_2$, which dominates the Euclidean distance metric.\n- **Dataset C** features heavily overlapping distributions for the two classes. Both supervised and unsupervised methods are expected to perform poorly, resulting in a low accuracy (near random chance, $0.5$) and a low silhouette score (near $0$).\n\nThe final output is a list containing the $[\\text{Acc}, \\text{Sil}]$ pairs for datasets A, B, and C, in order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_accuracy(X, y):\n    \"\"\"\n    Calculates the maximum accuracy of a 1D threshold classifier on x1.\n    \"\"\"\n    x1 = X[:, 0]\n    \n    unique_x1 = np.unique(x1)\n    \n    thresholds = []\n    if len(unique_x1)  1:\n        # Midpoints between unique consecutive sorted x1 values\n        thresholds.extend((unique_x1[:-1] + unique_x1[1:]) / 2)\n    # Add thresholds to classify all points into one group\n    thresholds.append(unique_x1[0] - 1.0)\n    thresholds.append(unique_x1[-1] + 1.0)\n    \n    # Use a tuple (acc, c_left, t) for lexicographical comparison\n    # Goal: max(acc), min(c_left), min(t)\n    # This is equivalent to finding max of (acc, 1-c_left, -t)\n    best_params = (-1.0, 2, float('inf')) # (acc, c_left, t)\n\n    for t in sorted(list(set(thresholds))):\n        for c_left in [0, 1]:\n            c_right = 1 - c_left\n            y_pred = np.where(x1 = t, c_left, c_right)\n            acc = np.mean(y_pred == y)\n            \n            # Tie-breaking logic\n            current_params = (acc, c_left, t)\n            if current_params[0]  best_params[0]:\n                best_params = current_params\n            elif current_params[0] == best_params[0]:\n                if current_params[1]  best_params[1]:\n                    best_params = current_params\n                elif current_params[1] == best_params[1] and current_params[2]  best_params[2]:\n                    best_params = current_params\n                    \n    return best_params[0]\n\ndef _calculate_silhouette(X):\n    \"\"\"\n    Performs K-Means clustering (k=2) and calculates the silhouette score.\n    \"\"\"\n    N, D = X.shape\n    k = 2\n\n    if N  k:\n        return 0.0\n\n    # 1. Deterministic Initialization\n    idx_min_x1 = np.argmin(X[:, 0])\n    idx_max_x1 = np.argmax(X[:, 0])\n    \n    if idx_min_x1 == idx_max_x1:\n        # Pathological case not present in test data but good practice to handle.\n        # If all x1 are same, initialization would give one centroid.\n        # For this problem's scope, we assume this does not happen.\n        if N  1:\n             idx_max_x1 = (idx_min_x1 + 1) % N # Pick another point\n        else:\n             return 0.0 # Single point has 0 silhouette score.\n    \n    centroids = np.array([X[idx_min_x1], X[idx_max_x1]], dtype=np.float64)\n    \n    assignments = np.zeros(N, dtype=int)\n    \n    for _ in range(100):\n        # 2. Assignment step (vectorized)\n        distances = np.linalg.norm(X[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\n        new_assignments = np.argmin(distances, axis=1)\n            \n        # 3. Check for convergence\n        if np.array_equal(assignments, new_assignments):\n            break\n        assignments = new_assignments\n        \n        # 4. Update step with empty cluster handling\n        new_centroids = np.zeros_like(centroids)\n        cluster_sizes = np.bincount(assignments, minlength=k)\n\n        # First, compute centroids for non-empty clusters\n        non_empty_updated = [False, False]\n        for j in range(k):\n            if cluster_sizes[j]  0:\n                new_centroids[j] = np.mean(X[assignments == j], axis=0)\n                non_empty_updated[j] = True\n\n        # Then, handle any empty clusters\n        for j in range(k):\n            if cluster_sizes[j] == 0:\n                other_j = 1 - j\n                # Use the newly computed centroid of the other cluster\n                other_centroid = new_centroids[other_j]\n                \n                # Reinitialize as the point farthest from the other centroid\n                dists_from_other_centroid = np.linalg.norm(X - other_centroid, axis=1)\n                farthest_point_idx = np.argmax(dists_from_other_centroid)\n                new_centroids[j] = X[farthest_point_idx]\n        \n        centroids = new_centroids\n    \n    # After convergence, use the final assignments\n    assignments = new_assignments\n    \n    # 5. Silhouette score calculation\n    if len(np.unique(assignments))  2:\n        return 0.0\n\n    silhouette_values = np.zeros(N)\n    for i in range(N):\n        point_i = X[i]\n        cluster_idx = assignments[i]\n        \n        # Intra-cluster distance a(i)\n        mask_a = (assignments == cluster_idx)  (np.arange(N) != i)\n        if not np.any(mask_a):\n            a_i = 0.0\n        else:\n            a_i = np.mean(np.linalg.norm(X[mask_a] - point_i, axis=1))\n\n        # Inter-cluster distance b(i)\n        other_cluster_idx = 1 - cluster_idx\n        mask_b = assignments == other_cluster_idx\n        \n        if not np.any(mask_b): # Should not happen if len(unique(assignments))  1\n             b_i = 0.0\n        else:\n             b_i = np.mean(np.linalg.norm(X[mask_b] - point_i, axis=1))\n\n        if max(a_i, b_i) == 0:\n            silhouette_values[i] = 0.0\n        else:\n            silhouette_values[i] = (b_i - a_i) / max(a_i, b_i)\n            \n    return np.mean(silhouette_values)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"class_0\": [(-0.4, 0.0), (-0.2, 0.1), (0.2, -0.1), (0.3, 0.05)],\n            \"class_1\": [(0.2, 0.0), (0.6, -0.1), (0.8, 0.1), (1.0, -0.05)]\n        },\n        # Dataset B\n        {\n            \"class_0\": [(-0.4, 5.0), (-0.2, -5.0), (0.2, 7.0), (0.3, -7.0)],\n            \"class_1\": [(1.2, 6.0), (1.6, -6.0), (1.8, 8.0), (2.0, -8.0)]\n        },\n        # Dataset C\n        {\n            \"class_0\": [(-0.1, 0.05), (0.0, -0.02), (0.1, 0.03), (-0.05, -0.04)],\n            \"class_1\": [(-0.08, 0.01), (0.02, -0.03), (0.09, 0.04), (-0.02, -0.05)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        points_0 = np.array(case[\"class_0\"])\n        points_1 = np.array(case[\"class_1\"])\n        \n        X = np.concatenate((points_0, points_1), axis=0)\n        y = np.concatenate((np.zeros(len(points_0), dtype=int), np.ones(len(points_1), dtype=int)), axis=0)\n\n        accuracy = _calculate_accuracy(X, y)\n        silhouette = _calculate_silhouette(X)\n        \n        results.append([accuracy, silhouette])\n\n    # Final print statement in the exact required format.\n    results_as_strings = [f\"[{acc:.6f},{sil:.6f}]\" for acc, sil in results]\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from a conceptual demonstration to a practical application, this exercise situates us in the field of bioinformatics to perform a classic discovery task. You will use unsupervised clustering to group transcription factors based on their genome-wide binding patterns, a common step in uncovering functional relationships without prior knowledge. This practice introduces the silhouette score as a principled way to evaluate and select the optimal number of clusters, and contrasts the unsupervised discovery process with a simple but effective supervised classification model .",
            "id": "2432815",
            "problem": "You are given a formalization of clustering and classification tasks on genome-wide binding profiles derived from Chromatin Immunoprecipitation sequencing (ChIP-seq). Each profile corresponds to a Transcription Factor (TF). A dataset is represented as a matrix $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$, with $n$ TFs (rows) and $m$ genomic bins (columns), where $X_{ij}$ is a nonnegative integer count. For all tasks, define the following operations and quantities from first principles.\n\n- Row normalization: for each TF profile $\\mathbf{x}_i \\in \\mathbb{R}_{\\ge 0}^m$, define its $\\ell_2$-normalized vector $\\mathbf{y}_i = \\mathbf{x}_i / \\lVert \\mathbf{x}_i \\rVert_2$, where $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. If $\\lVert \\mathbf{x}_i \\rVert_2 = 0$, treat $\\mathbf{y}_i$ as the zero vector.\n- Cosine distance: for any two normalized vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^m$, define $d(\\mathbf{u}, \\mathbf{v}) = 1 - \\mathbf{u}^\\top \\mathbf{v}$.\n- Within-cluster sum of squares (WCSS) in the normalized space: for a partition $\\mathcal{P} = \\{S_1, \\dots, S_k\\}$ of $\\{1,\\dots,n\\}$ into $k$ nonempty clusters, and corresponding centroids $\\boldsymbol{\\mu}_c = \\frac{1}{|S_c|} \\sum_{i \\in S_c} \\mathbf{y}_i$, define\n$$\n\\mathrm{WCSS}(\\mathcal{P}) = \\sum_{c=1}^k \\sum_{i \\in S_c} \\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_c \\rVert_2^2.\n$$\n- Silhouette coefficient in the normalized space with cosine distance: for each $i \\in \\{1,\\dots,n\\}$ with assigned cluster $c(i)$, define\n$$\na(i) = \\frac{1}{|S_{c(i)}|-1} \\sum_{j \\in S_{c(i)},\\, j \\ne i} d(\\mathbf{y}_i, \\mathbf{y}_j)\n$$\nif $|S_{c(i)}| \\ge 2$, and set $a(i) = 0$ if $|S_{c(i)}| = 1$. For any other cluster $c' \\ne c(i)$, define\n$$\n\\bar{d}(i, c') = \\frac{1}{|S_{c'}|} \\sum_{j \\in S_{c'}} d(\\mathbf{y}_i, \\mathbf{y}_j).\n$$\nLet $b(i) = \\min_{c' \\ne c(i)} \\bar{d}(i,c')$. The silhouette of $i$ is\n$$\ns(i) = \n\\begin{cases}\n0,  \\text{if } a(i) = 0 \\text{ and } b(i) = 0, \\\\\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{otherwise}.\n\\end{cases}\n$$\nThe mean silhouette is $\\bar{s} = \\frac{1}{n} \\sum_{i=1}^n s(i)$.\n- Nearest-centroid classification in the normalized space: for labeled classes $\\mathcal{C}$ and a training set $T \\subset \\{1,\\dots,n\\}$ with labels $\\ell(i) \\in \\mathcal{C}$ for $i \\in T$, define class means $\\boldsymbol{\\nu}_c = \\frac{1}{|\\{i \\in T : \\ell(i) = c\\}|} \\sum_{i \\in T, \\ell(i)=c} \\mathbf{y}_i$. For each test sample $j \\notin T$, predict $\\hat{\\ell}(j) = \\arg\\min_{c \\in \\mathcal{C}} \\lVert \\mathbf{y}_j - \\boldsymbol{\\nu}_c \\rVert_2$. The accuracy is the fraction of correct predictions among the test samples. If a class mean is the zero vector, it is used as-is.\n\nClustering task: For a given candidate set $\\mathcal{K} \\subset \\mathbb{N}$ with all $k \\in \\mathcal{K}$ satisfying $k \\ge 2$, and a dataset without labels, for each $k \\in \\mathcal{K}$, produce a partition $\\mathcal{P}_k$ that minimizes $\\mathrm{WCSS}(\\mathcal{P}_k)$ in the normalized space. Let $\\bar{s}_k$ be the mean silhouette of $\\mathcal{P}_k$. Select $k^\\star = \\arg\\max_{k \\in \\mathcal{K}} \\bar{s}_k$. In case of ties in $\\bar{s}_k$, select the smallest $k$. If all pairwise distances are zero (all normalized vectors identical), define all silhouettes to be $0$ for all $k$.\n\nSupervised classification task: For a given labeled dataset, compute the nearest-centroid accuracy as defined above on a specified test set.\n\nData generation model: Unless stated otherwise, an unlabeled dataset is generated from a mixture of independent Poisson distributions per bin. For cluster $c$, the mean vector $\\boldsymbol{\\lambda}^{(c)} \\in \\mathbb{R}_{0}^m$ is specified; each TF $\\mathbf{x}$ in cluster $c$ is drawn with independent components $x_j \\sim \\mathrm{Poisson}(\\lambda^{(c)}_j)$ for $j \\in \\{1,\\dots,m\\}$. Labeled datasets follow the same generation, with the label given by the generating cluster. All random draws are made with the specified random seed, and clusters are realized with the specified sample counts. In the boundary case where all profiles are deterministic and identical, use the provided fixed vector for all rows.\n\nTest suite. Implement exactly the following four test cases; these constitute the full input to your program.\n\n- Test case $1$ (unlabeled clustering; happy path):\n  - $n = 90$, $m = 60$, seed $= 123$.\n  - Three clusters $C_1, C_2, C_3$, each with $30$ TFs.\n  - Background mean $\\mu_{\\mathrm{low}} = 2$ and elevated mean $\\mu_{\\mathrm{high}} = 30$.\n  - Means:\n    - For $C_1$: bins $0$ through $19$ inclusive have mean $\\mu_{\\mathrm{high}}$, all others $\\mu_{\\mathrm{low}}$.\n    - For $C_2$: bins $20$ through $39$ inclusive have mean $\\mu_{\\mathrm{high}}$, all others $\\mu_{\\mathrm{low}}$.\n    - For $C_3$: bins $40$ through $59$ inclusive have mean $\\mu_{\\mathrm{high}}$, all others $\\mu_{\\mathrm{low}}$.\n  - Candidate set $\\mathcal{K} = \\{2,3,4,5\\}$.\n  - Output for this case: a list $[k^\\star, \\bar{s}_{k^\\star}]$.\n\n- Test case $2$ (unlabeled clustering; boundary condition):\n  - $n = 20$, $m = 10$.\n  - All TFs are identical and deterministic: every row of $X$ equals the same fixed nonzero vector with all components equal to $5$.\n  - Candidate set $\\mathcal{K} = \\{2,3\\}$.\n  - Output for this case: a list $[k^\\star, \\bar{s}_{k^\\star}]$ as defined, which must reflect the specified tie-breaking and silhouette definition for zero distances.\n\n- Test case $3$ (supervised classification; labeled evaluation):\n  - $n = 100$, $m = 40$, seed $= 321$.\n  - Two classes $A$ and $B$, each with $50$ TFs, generated by the Poisson model.\n  - Background mean $\\mu_{\\mathrm{low}} = 2$ and elevated mean $\\mu_{\\mathrm{high}} = 25$.\n  - Means:\n    - For class $A$: bins $0$ through $19$ inclusive have mean $\\mu_{\\mathrm{high}}$, all others $\\mu_{\\mathrm{low}}$.\n    - For class $B$: bins $20$ through $39$ inclusive have mean $\\mu_{\\mathrm{high}}$, all others $\\mu_{\\mathrm{low}}$.\n  - Training/test split: Use exactly $30$ TFs from each class for training and the remaining $20$ TFs from each class for testing. The data are ordered with all $A$ samples first and all $B$ samples second; choose the first $30$ of each class (in this order) for training and the remaining for testing.\n  - Output for this case: a float equal to the nearest-centroid accuracy on the test set, expressed as a decimal fraction in $[0,1]$.\n\n- Test case $4$ (unlabeled clustering; high-dimensional, partially overlapping signal):\n  - $n = 60$, $m = 200$, seed $= 999$.\n  - Two clusters $C_1, C_2$, each with $30$ TFs.\n  - Background mean $\\mu_{\\mathrm{low}} = 3$ and elevated mean $\\mu_{\\mathrm{high}} = 15$.\n  - Means:\n    - For $C_1$: bins $0$ through $99$ inclusive have mean $\\mu_{\\mathrm{high}}$, all others $\\mu_{\\mathrm{low}}$.\n    - For $C_2$: bins $20$ through $119$ inclusive have mean $\\mu_{\\mathrm{high}}$, all others $\\mu_{\\mathrm{low}}$.\n  - Candidate set $\\mathcal{K} = \\{2,3\\}$.\n  - Output for this case: a list $[k^\\star, \\bar{s}_{k^\\star}]$.\n\nFinal output format. Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, in the following order: test case $1$, test case $2$, test case $3$, test case $4$. Each test case result must be either a boolean, an integer, a float, or a list composed of these types, exactly as specified above. For example, your output should look like $[ [k_1^\\star, \\bar{s}_{k_1^\\star}], [k_2^\\star, \\bar{s}_{k_2^\\star}], \\mathrm{acc}_3, [k_4^\\star, \\bar{s}_{k_4^\\star}] ]$ with the concrete numeric values computed from the definitions. No additional text should be printed.",
            "solution": "The provided problem statement is valid. It is scientifically grounded in the domain of bioinformatics and machine learning, well-posed with clear mathematical definitions and objectives, and free of any contradictions, ambiguities, or unsound premises. The problem requires the implementation of algorithms for clustering and classification of ChIP-seq data profiles, evaluated on a specified suite of test cases.\n\nMy approach involves a direct implementation of the specified mathematical formulas and procedures. The overall process is divided into two main parts: an unsupervised clustering task and a supervised classification task.\n\nFirst, I establish helper functions for common operations. The raw data, a matrix $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$ of integer counts, is generated according to a specified Poisson model $x_j \\sim \\mathrm{Poisson}(\\lambda_j)$. Each row vector $\\mathbf{x}_i$ is then normalized to a unit vector $\\mathbf{y}_i = \\mathbf{x}_i / \\lVert \\mathbf{x}_i \\rVert_2$, with care taken for the case $\\lVert \\mathbf{x}_i \\rVert_2 = 0$ where $\\mathbf{y}_i$ becomes a zero vector.\n\nFor the clustering task, the objective is to find a partition $\\mathcal{P}_k$ that minimizes the Within-Cluster Sum of Squares (WCSS), defined as $\\mathrm{WCSS}(\\mathcal{P}) = \\sum_{c=1}^k \\sum_{i \\in S_c} \\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_c \\rVert_2^2$, where $\\boldsymbol{\\mu}_c$ are the cluster centroids. This is the standard objective for K-means clustering. As finding the global minimum is an NP-hard problem, I employ the widely-used K-means algorithm (Lloyd's algorithm) as a heuristic. The implementation from `scipy.cluster.vq.kmeans` is used, which performs multiple runs with random initializations and returns the best partition found, providing a robust approximation of the WCSS minimum.\n\nOnce a partition $\\mathcal{P}_k$ is obtained for each candidate number of clusters $k \\in \\mathcal{K}$, its quality is assessed using the mean silhouette coefficient, $\\bar{s}_k$. The silhouette coefficient $s(i)$ for a single data point $i$ is a measure of its fit within its assigned cluster compared to neighboring clusters. It is defined as $s(i) = (b(i) - a(i)) / \\max\\{a(i), b(i)\\}$, where $a(i)$ is the average distance to other points in its own cluster and $b(i)$ is the minimum average distance to points in any other cluster. The distance metric used is the cosine distance, $d(\\mathbf{u}, \\mathbf{v}) = 1 - \\mathbf{u}^\\top \\mathbf{v}$, as specified. My implementation calculates $s(i)$ for all points and averages them to get $\\bar{s}_k$. I handle the special case where all pairwise distances are zero, for which the silhouette is defined to be zero. The optimal number of clusters, $k^\\star$, is chosen as the one maximizing $\\bar{s}_k$, with ties broken by selecting the smallest $k$.\n\nFor the supervised classification task, the model is a nearest-centroid classifier. Given a training set of normalized vectors $\\{\\mathbf{y}_i\\}$ with class labels $\\{\\ell(i)\\}$, I first compute the mean vector, or centroid, $\\boldsymbol{\\nu}_c$ for each class $c$. For any new test vector $\\mathbf{y}_j$, its class is predicted by finding the centroid it is closest to in Euclidean distance: $\\hat{\\ell}(j) = \\arg\\min_c \\lVert \\mathbf{y}_j - \\boldsymbol{\\nu}_c \\rVert_2$. The performance of the classifier is measured by its accuracy, which is the fraction of correctly predicted labels in the test set.\n\nThese implemented procedures are then applied to the four specified test cases, each with its own parameters for data generation and evaluation tasks. The seeds for random number generation are strictly followed to ensure reproducibility. The final results are collected and formatted into a single list as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.cluster.vq import kmeans, vq\n\ndef solve():\n    \"\"\"\n    Main function to solve the four test cases and print the final result.\n    \"\"\"\n    \n    def generate_data(cluster_means, samples_per_cluster, seed):\n        \"\"\"Generates data from a mixture of Poisson distributions.\"\"\"\n        rng = np.random.default_rng(seed)\n        X_parts = []\n        for i, lambda_vec in enumerate(cluster_means):\n            n_samples = samples_per_cluster[i]\n            m = lambda_vec.shape[0]\n            cluster_data = rng.poisson(lambda_vec, size=(n_samples, m))\n            X_parts.append(cluster_data)\n        return np.vstack(X_parts)\n\n    def normalize_data(X):\n        \"\"\"L2-normalizes the rows of matrix X.\"\"\"\n        norms = np.linalg.norm(X, axis=1, keepdims=True)\n        # Handle zero-norm vectors as per problem statement\n        Y = np.divide(X, norms, out=np.zeros_like(X, dtype=float), where=(norms != 0))\n        return Y\n\n    def calculate_mean_silhouette(Y, labels):\n        \"\"\"Calculates the mean silhouette coefficient for a given partition.\"\"\"\n        n = Y.shape[0]\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        k_found = len(unique_labels)\n        \n        # Pre-compute pairwise cosine distances\n        dot_products = Y @ Y.T\n        np.clip(dot_products, -1.0, 1.0, out=dot_products) # For floating point precision\n        D = 1.0 - dot_products\n\n        # Per problem statement for case of identical vectors\n        if np.allclose(D, 0):\n            return 0.0\n\n        s_coeffs = np.zeros(n)\n        for i in range(n):\n            c_i_label = labels[i]\n            \n            is_in_cluster_i = (labels == c_i_label)\n            count_in_cluster_i = counts[unique_labels == c_i_label][0]\n            \n            if count_in_cluster_i  1:\n                a_i = np.sum(D[i, is_in_cluster_i]) / (count_in_cluster_i - 1)\n            else: # Singleton cluster\n                a_i = 0.0\n            \n            b_i = np.inf\n            if k_found  1:\n                for c_prime_label in unique_labels:\n                    if c_prime_label == c_i_label:\n                        continue\n                    is_in_cluster_prime = (labels == c_prime_label)\n                    count_in_cluster_prime = counts[unique_labels == c_prime_label][0]\n                    d_bar = np.sum(D[i, is_in_cluster_prime]) / count_in_cluster_prime\n                    if d_bar  b_i:\n                        b_i = d_bar\n            \n            if np.isinf(b_i): # Only happens if k_found = 1\n                b_i = 0.0\n\n            if a_i == 0.0 and b_i == 0.0:\n                s_coeffs[i] = 0.0\n            else:\n                denom = max(a_i, b_i)\n                s_coeffs[i] = (b_i - a_i) / denom\n        \n        return np.mean(s_coeffs)\n\n    def test_case_1():\n        n, m, seed = 90, 60, 123\n        n_per_cluster = 30\n        mu_low, mu_high = 2, 30\n        K_set = [2, 3, 4, 5]\n        \n        np.random.seed(seed)\n        \n        lambda1 = np.full(m, mu_low); lambda1[0:20] = mu_high\n        lambda2 = np.full(m, mu_low); lambda2[20:40] = mu_high\n        lambda3 = np.full(m, mu_low); lambda3[40:60] = mu_high\n        \n        X = generate_data([lambda1, lambda2, lambda3], [n_per_cluster] * 3, seed)\n        Y = normalize_data(X)\n        \n        s_scores = []\n        for k in K_set:\n            centroids, _ = kmeans(Y, k, iter=100)\n            labels, _ = vq(Y, centroids)\n            s_k = calculate_mean_silhouette(Y, labels)\n            s_scores.append(s_k)\n\n        best_s_idx = np.argmax(s_scores)\n        k_star = K_set[best_s_idx]\n        best_s = s_scores[best_s_idx]\n\n        return [k_star, best_s]\n\n    def test_case_2():\n        n, m = 20, 10\n        K_set = [2, 3]\n        \n        X = np.full((n, m), 5.0)\n        Y = normalize_data(X)\n        \n        # All pairwise distances are zero. By definition, silhouette is 0 for all k.\n        s_scores = {k: 0.0 for k in K_set}\n\n        # Tie-breaking rule: select smallest k\n        k_star = min(K_set)\n        s_k_star = s_scores[k_star]\n        \n        return [k_star, s_k_star]\n\n    def test_case_3():\n        n, m, seed = 100, 40, 321\n        n_per_class = 50\n        mu_low, mu_high = 2, 25\n        \n        np.random.seed(seed)\n        \n        lambdaA = np.full(m, mu_low); lambdaA[0:20] = mu_high\n        lambdaB = np.full(m, mu_low); lambdaB[20:40] = mu_high\n        \n        X = generate_data([lambdaA, lambdaB], [n_per_class] * 2, seed)\n        labels = np.concatenate([np.zeros(n_per_class), np.ones(n_per_class)])\n        \n        train_indices = np.concatenate([np.arange(0, 30), np.arange(50, 80)])\n        test_indices = np.concatenate([np.arange(30, 50), np.arange(80, 100)])\n        \n        Y = normalize_data(X)\n        Y_train, labels_train = Y[train_indices], labels[train_indices]\n        Y_test, labels_test = Y[test_indices], labels[test_indices]\n        \n        mean_A = Y_train[labels_train == 0].mean(axis=0)\n        mean_B = Y_train[labels_train == 1].mean(axis=0)\n        \n        predictions = []\n        for y_j in Y_test:\n            dist_A = np.linalg.norm(y_j - mean_A)\n            dist_B = np.linalg.norm(y_j - mean_B)\n            predictions.append(0 if dist_A  dist_B else 1)\n        \n        return np.mean(np.array(predictions) == labels_test)\n        \n    def test_case_4():\n        n, m, seed = 60, 200, 999\n        n_per_cluster = 30\n        mu_low, mu_high = 3, 15\n        K_set = [2, 3]\n\n        np.random.seed(seed)\n        \n        lambda1 = np.full(m, mu_low); lambda1[0:100] = mu_high\n        lambda2 = np.full(m, mu_low); lambda2[20:120] = mu_high\n        \n        X = generate_data([lambda1, lambda2], [n_per_cluster] * 2, seed)\n        Y = normalize_data(X)\n        \n        s_scores = []\n        for k in K_set:\n            centroids, _ = kmeans(Y, k, iter=100)\n            labels, _ = vq(Y, centroids)\n            s_k = calculate_mean_silhouette(Y, labels)\n            s_scores.append(s_k)\n\n        best_s_idx = np.argmax(s_scores)\n        k_star = K_set[best_s_idx]\n        best_s = s_scores[best_s_idx]\n\n        return [k_star, best_s]\n\n    results = [\n        test_case_1(),\n        test_case_2(),\n        test_case_3(),\n        test_case_4()\n    ]\n\n    # Format the final output string as specified\n    # The default str() for lists includes spaces, which we remove.\n    print(f\"[{','.join(map(lambda x: str(x).replace(' ', ''), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice delves into the world of sequence analysis, pitting a complex supervised model against a simple, well-informed unsupervised baseline. You will train a Recurrent Neural Network (RNN) to predict protein secondary structure, a task where context is key. This exercise powerfully demonstrates that model complexity is not a guarantee of success; a sophisticated architecture can be limited by its own assumptions, while a simple unsupervised approach with access to the right features can prove highly effective, underscoring the critical role of feature engineering .",
            "id": "2432793",
            "problem": "You are given a binary, position-wise prediction task representative of protein secondary structure assignment. Let the alphabet of amino acids be the standard set of $20$ one-letter codes. For any amino acid $a$, define a scalar propensity function $\\phi(a)$ by\n- $\\phi(a) = +1$ if $a \\in \\{\\text{A}, \\text{L}, \\text{M}, \\text{Q}, \\text{E}, \\text{K}, \\text{R}, \\text{H}\\}$ (commonly helix-preferring),\n- $\\phi(a) = -1$ if $a \\in \\{\\text{V}, \\text{I}, \\text{Y}, \\text{F}, \\text{W}, \\text{T}\\}$ (commonly sheet-preferring),\n- $\\phi(a) = 0$ otherwise (neutral set $\\{\\text{C}, \\text{D}, \\text{N}, \\text{P}, \\text{G}, \\text{S}\\}$).\n\nFor a protein sequence $s = (a_1, a_2, \\dots, a_T)$, define the ground-truth secondary structure label at each position $i \\in \\{1,\\dots,T\\}$ as $y_i \\in \\{0,1\\}$ with $1$ denoting alpha-helix and $0$ denoting beta-sheet, by the local-context rule\n$$\n\\tilde{z}_i \\;=\\; \\beta_0 + \\beta_1 \\,\\phi(a_i) + \\beta_2 \\,\\phi(a_{i-1}) + \\beta_3 \\,\\phi(a_{i+1}),\n$$\nwith boundary convention $\\phi(a_0)=\\phi(a_{T+1})=0$. Then\n$$\ny_i \\;=\\; \\begin{cases}\n1  \\text{if } \\tilde{z}_i \\ge 0,\\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\nUse fixed coefficients $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 0.5$, $\\beta_3 = 0.5$.\n\nConsider a supervised model that maps the one-hot encoding $x_i \\in \\{0,1\\}^{20}$ of $a_i$ to a hidden state $h_i \\in \\mathbb{R}^d$ and an output $\\hat{y}_i \\in (0,1)$ via a Recurrent Neural Network (RNN) defined as follows. Let $h_0 = 0 \\in \\mathbb{R}^d$, then for $i=1,\\dots,T$,\n$$\nh_i \\;=\\; \\tanh\\!\\left(W_x x_i + W_h h_{i-1} + b\\right), \\quad s_i \\;=\\; U h_i + c, \\quad \\hat{y}_i \\;=\\; \\sigma(s_i),\n$$\nwhere $\\sigma(u) = \\frac{1}{1+e^{-u}}$, $W_x \\in \\mathbb{R}^{d \\times 20}$, $W_h \\in \\mathbb{R}^{d \\times d}$, $b \\in \\mathbb{R}^d$, $U \\in \\mathbb{R}^{1 \\times d}$, and $c \\in \\mathbb{R}$. The supervised objective over a set of labeled sequences is the average binary cross-entropy with $\\ell_2$-regularization,\n$$\n\\mathcal{L} \\;=\\; \\frac{1}{N}\\sum_{n=1}^N \\frac{1}{T_n} \\sum_{i=1}^{T_n} \\left[-y_i^{(n)} \\log \\hat{y}_i^{(n)} - \\left(1-y_i^{(n)}\\right)\\log\\left(1-\\hat{y}_i^{(n)}\\right)\\right] \\;+\\; \\lambda \\left(\\lVert W_x\\rVert_F^2 + \\lVert W_h\\rVert_F^2 + \\lVert U\\rVert_F^2\\right),\n$$\nwith regularization coefficient $\\lambda = 10^{-4}$ and hidden dimension $d = 6$.\n\nAlso consider an unsupervised baseline that ignores labels and performs clustering in a one-dimensional feature space using $k$-means with $k=2$. For any position $i$, define the scalar feature\n$$\nf_i \\;=\\; \\phi(a_i) + 0.5\\,\\phi(a_{i-1}) + 0.5\\,\\phi(a_{i+1}),\n$$\nwith the same boundary conventions. The clustering yields two centroids $m_1, m_2 \\in \\mathbb{R}$; assign the label $\\hat{y}^{\\text{unsup}}_i = 1$ to the cluster whose centroid is larger and $\\hat{y}^{\\text{unsup}}_i = 0$ to the other. At evaluation time, a position $i$ is assigned to the nearest centroid in Euclidean distance.\n\nTraining and evaluation data are specified as follows. The supervised model must be trained only on the training set sequences, using the labels $y_i$ defined above. The unsupervised model must be fit only on training data features $\\{f_i\\}$ constructed from the training sequences. Then both models must be evaluated on the test suite sequences below by computing the fraction of correctly predicted labels (accuracy) relative to the ground-truth $y_i$ defined above.\n\nTraining set sequences:\n- S-train-1: \"ALMEKRALMEKRAALMQEKRHALMEKRA\"\n- S-train-2: \"VIVTWIYVIVTWYVIVTWIYVIVTWI\"\n- S-train-3: \"ALMEKRGPGSNNDCALMEKRGPGS\"\n- S-train-4: \"GPGSNDNCGPGSGPGSALVIALVIALVIA\"\n- S-train-5: \"ALMEKRALVIALMEKRVIYTWALMEKRV\"\n\nTest suite sequences:\n- S1 (general mixed case): \"ALMEKRVIVTWTALMEQKRVIV\"\n- S2 (boundary length-$1$ case): \"A\"\n- S3 (all helix-favoring bias): \"ALMEKRAALMEKR\"\n- S4 (all sheet-favoring bias): \"VIVTWIYVIVTWI\"\n\nYour program must implement the following and produce the specified output:\n- Train the supervised recurrent model described above on the training set and then, for each test sequence, compute the accuracy\n$$\nA_{\\text{sup}}(\\text{S}j) \\;=\\; \\frac{1}{T_j} \\sum_{i=1}^{T_j} \\mathbb{I}\\left[\\mathbb{I}\\left(\\hat{y}_i^{(\\text{S}j)} \\ge 0.5\\right) = y_i^{(\\text{S}j)}\\right],\n$$\nfor $j \\in \\{1,2,3,4\\}$, where $T_j$ is the length of sequence $\\text{S}j$.\n- Fit the unsupervised baseline on training features $\\{f_i\\}$ and then, for each test sequence, compute the accuracy\n$$\nA_{\\text{unsup}}(\\text{S}j) \\;=\\; \\frac{1}{T_j} \\sum_{i=1}^{T_j} \\mathbb{I}\\left[\\hat{y}^{\\text{unsup}}_i(\\text{S}j) = y_i^{(\\text{S}j)}\\right].\n$$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $8$ floating-point numbers in the following order:\n$$\n\\big[ A_{\\text{sup}}(\\text{S}1),\\, A_{\\text{unsup}}(\\text{S}1),\\, A_{\\text{sup}}(\\text{S2),\\, A_{\\text{unsup}}(\\text{S}2),\\, A_{\\text{sup}}(\\text{S}3),\\, A_{\\text{unsup}}(\\text{S}3),\\, A_{\\text{sup}}(\\text{S}4),\\, A_{\\text{unsup}}(\\text{S}4) \\big].\n$$\nAll accuracies must be expressed as decimal fractions in $[0,1]$ with no percentage signs. The program must not read any input and must use only the specified training and test sequences above.",
            "solution": "The problem statement presented is a well-defined computational task in bioinformatics, comparing a supervised learning model against an unsupervised baseline for a simplified protein secondary structure prediction problem. All components—data, models, parameters, and evaluation metrics—are specified with sufficient clarity. The problem is scientifically grounded, logically consistent, and formally structured. It is therefore deemed valid.\n\nThe solution will be developed in three principal stages:\n1.  Implementation of the ground-truth label and feature generation logic.\n2.  Implementation and evaluation of the unsupervised $k$-means clustering model.\n3.  Implementation, training, and evaluation of the supervised Recurrent Neural Network (RNN) model.\n\n**1. Ground-Truth Data Generation**\n\nThe foundation of this problem is the deterministic rule for generating ground-truth labels $y_i \\in \\{0, 1\\}$. This rule depends on a scalar propensity function $\\phi(a)$ for each amino acid $a$.\n-   $\\phi(a) = +1$ for helix-preferring residues: $\\{\\text{A}, \\text{L}, \\text{M}, \\text{Q}, \\text{E}, \\text{K}, \\text{R}, \\text{H}\\}$.\n-   $\\phi(a) = -1$ for sheet-preferring residues: $\\{\\text{V}, \\text{I}, \\text{Y}, \\text{F}, \\text{W}, \\text{T}\\}$.\n-   $\\phi(a) = 0$ for neutral residues: $\\{\\text{C}, \\text{D}, \\text{N}, \\text{P}, \\text{G}, \\text{S}\\}$.\n\nFor a protein sequence of length $T$, the real-valued score $\\tilde{z}_i$ at each position $i$ is a linear combination of propensities in a local window of size $3$:\n$$\n\\tilde{z}_i = \\beta_0 + \\beta_1 \\phi(a_i) + \\beta_2 \\phi(a_{i-1}) + \\beta_3 \\phi(a_{i+1})\n$$\nWith the given coefficients $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 0.5$, $\\beta_3 = 0.5$, and boundary conditions $\\phi(a_0) = \\phi(a_{T+1}) = 0$, this simplifies to:\n$$\n\\tilde{z}_i = \\phi(a_i) + 0.5 \\phi(a_{i-1}) + 0.5 \\phi(a_{i+1})\n$$\nThe binary ground-truth label $y_i$ is then determined by the sign of $\\tilde{z}_i$:\n$$\ny_i = \\begin{cases} 1  \\text{if } \\tilde{z}_i \\ge 0 \\\\ 0  \\text{if } \\tilde{z}_i  0 \\end{cases}\n$$\nThis logic will be encapsulated in a function to generate labels for any given amino acid sequence.\n\n**2. Unsupervised Baseline Model**\n\nThe unsupervised model is based on $k$-means clustering with $k=2$. Crucially, the feature used for clustering at position $i$ is defined as:\n$$\nf_i = \\phi(a_i) + 0.5 \\phi(a_{i-1}) + 0.5 \\phi(a_{i+1})\n$$\nThis is identical to the score $\\tilde{z}_i$ used to generate the ground truth. The model operates as follows:\n-   **Fitting:** The set of all features $\\{f_i\\}$ is computed from all training sequences. A $1$-dimensional $k$-means algorithm is applied to this dataset to find two cluster centroids, $m_1$ and $m_2$.\n-   **Labeling Convention:** The cluster with the larger centroid (e.g., $\\max(m_1, m_2)$) is assigned the label $1$ (helix), and the other is assigned label $0$ (sheet).\n-   **Prediction:** For a feature $f_j$ from a test sequence, the prediction $\\hat{y}^{\\text{unsup}}_j$ is determined by assigning $f_j$ to the cluster with the nearest centroid. If we assume $m_1  m_2$, the decision boundary is at $\\frac{m_1 + m_2}{2}$. A position $j$ is predicted as helix (label $1$) if $f_j  \\frac{m_1 + m_2}{2}$ and sheet (label $0$) otherwise.\n\nThe accuracy of this model depends on how the data-driven decision boundary $\\frac{m_1 + m_2}{2}$ compares to the ground-truth boundary, which is fixed at $0$. Since the features $f_i$ are the same as the ground-truth scores $\\tilde{z}_i$, the training data for clustering naturally separates into a group of non-negative values (true label $1$) and a group of negative values (true label $0$). The $k$-means algorithm is expected to find one positive centroid and one negative centroid, resulting in a decision boundary close to $0$ and thus high accuracy.\n\n**3. Supervised RNN Model**\n\nThe supervised model is a standard Recurrent Neural Network (RNN).\n-   **Architecture:**\n    $$\n    h_i = \\tanh(W_x x_i + W_h h_{i-1} + b) \\\\\n    \\hat{y}_i = \\sigma(U h_i + c)\n    $$\n    The input $x_i \\in \\{0, 1\\}^{20}$ is the one-hot encoding of amino acid $a_i$. The hidden dimension is $d=6$.\n-   **Limitation:** This is a unidirectional RNN, meaning the state $h_i$ and prediction $\\hat{y}_i$ depend only on the input sequence up to position $i$, $(a_1, \\dots, a_i)$. However, the ground-truth label $y_i$ depends on the future amino acid $a_{i+1}$. This architectural mismatch means the model cannot, in principle, perfectly replicate the ground-truth function on arbitrary sequences. It can only succeed by learning statistical patterns from the training data that allow it to \"anticipate\" $a_{i+1}$ based on the history $(a_1, \\dots, a_i)$.\n-   **Training:** The model parameters ($W_x, W_h, b, U, c$) are optimized by minimizing the regularized binary cross-entropy loss function over the training set.\n    $$\n    \\mathcal{L} = \\text{BCE} + \\lambda \\cdot (\\lVert W_x\\rVert_F^2 + \\lVert W_h\\rVert_F^2 + \\lVert U\\rVert_F^2)\n    $$\n    Optimization is performed using stochastic gradient descent with gradients computed via the backpropagation through time (BPTT) algorithm. The regularization coefficient is $\\lambda = 10^{-4}$. We will use a fixed learning rate and a set number of training epochs, which are standard hyperparameters for such a procedure.\n-   **Prediction:** After training, the model predicts a label of $1$ if its output $\\hat{y}_i \\ge 0.5$ and $0$ otherwise.\n\n**4. Implementation and Evaluation**\n\nThe program will implement both models. For the unsupervised model, a simple iterative $k$-means algorithm is sufficient. For the supervised model, we will implement the forward pass and BPTT to train the network. Both trained models will then be evaluated on the four test sequences to compute their respective accuracies, defined as the fraction of correctly predicted labels. The final output will be a list of these eight accuracy values in the specified order. The parameters for the RNN will be initialized randomly, and the training process involves inherent stochasticity. However, given the deterministic nature of the ground truth and the small size of the problem, the training process is expected to converge to a stable and representative solution.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Implements and evaluates supervised and unsupervised models for a protein secondary structure task.\n    \"\"\"\n    \n    # ------------------- PROBLEM DEFINITION -------------------\n    \n    AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n    AA_TO_IX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n    VOCAB_SIZE = len(AMINO_ACIDS)\n\n    HELIX_PREF = set(\"ALMQEKRH\")\n    SHEET_PREF = set(\"VIYFWT\")\n\n    BETA_0, BETA_1, BETA_2, BETA_3 = 0.0, 1.0, 0.5, 0.5\n    \n    D_HIDDEN = 6\n    LAMBDA_REG = 1e-4\n    LEARNING_RATE = 0.01\n    EPOCHS = 1000\n    \n    # Using a fixed seed for reproducibility of RNN initialization and training.\n    np.random.seed(42)\n\n    TRAIN_SEQS = [\n        \"ALMEKRALMEKRAALMQEKRHALMEKRA\",\n        \"VIVTWIYVIVTWYVIVTWIYVIVTWI\",\n        \"ALMEKRGPGSNNDCALMEKRGPGS\",\n        \"GPGSNDNCGPGSGPGSALVIALVIALVIA\",\n        \"ALMEKRALVIALMEKRVIYTWALMEKRV\",\n    ]\n    \n    TEST_SUITE = {\n        \"S1\": \"ALMEKRVIVTWTALMEQKRVIV\",\n        \"S2\": \"A\",\n        \"S3\": \"ALMEKRAALMEKR\",\n        \"S4\": \"VIVTWIYVIVTWI\",\n    }\n    \n    # ------------------- HELPER FUNCTIONS -------------------\n\n    def get_propensity(aa):\n        if aa in HELIX_PREF: return 1.0\n        if aa in SHEET_PREF: return -1.0\n        return 0.0\n\n    def generate_truth(sequence):\n        T = len(sequence)\n        propensities = [get_propensity(aa) for aa in sequence]\n        z_tilde = np.zeros(T)\n        for i in range(T):\n            phi_i = propensities[i]\n            phi_prev = propensities[i-1] if i  0 else 0.0\n            phi_next = propensities[i+1] if i  T - 1 else 0.0\n            z_tilde[i] = BETA_0 + BETA_1 * phi_i + BETA_2 * phi_prev + BETA_3 * phi_next\n        \n        labels = (z_tilde = 0).astype(int)\n        features = z_tilde\n        return features, labels\n\n    def one_hot_encode(sequence):\n        T = len(sequence)\n        x = np.zeros((T, VOCAB_SIZE))\n        for i, aa in enumerate(sequence):\n            x[i, AA_TO_IX[aa]] = 1\n        return x\n\n    # ------------------- UNSUPERVISED MODEL -------------------\n    \n    class KMeans1D:\n        def __init__(self, k=2):\n            self.k = k\n            self.centroids = None\n            self.cluster_labels = None\n\n        def fit(self, data):\n            # Initialize centroids\n            centroids = np.random.choice(np.unique(data), self.k, replace=False)\n            \n            for _ in range(100): # Max iterations\n                clusters = [[] for _ in range(self.k)]\n                for point in data:\n                    distances = [np.abs(point - c) for c in centroids]\n                    closest_idx = np.argmin(distances)\n                    clusters[closest_idx].append(point)\n                \n                new_centroids = np.array([np.mean(c) if c else centroids[i] for i, c in enumerate(clusters)])\n                \n                if np.all(new_centroids == centroids):\n                    break\n                centroids = new_centroids\n            \n            self.centroids = centroids\n            # Assign label 1 to larger centroid, 0 to smaller\n            self.cluster_labels = (self.centroids == np.max(self.centroids)).astype(int)\n\n        def predict(self, data):\n            preds = []\n            for point in data:\n                distances = [np.abs(point - c) for c in self.centroids]\n                closest_idx = np.argmin(distances)\n                preds.append(self.cluster_labels[closest_idx])\n            return np.array(preds)\n            \n    # ------------------- SUPERVISED MODEL (RNN) -------------------\n    \n    class SimpleRNN:\n        def __init__(self, input_dim, hidden_dim, output_dim, reg_lambda):\n            self.input_dim = input_dim\n            self.hidden_dim = hidden_dim\n            self.reg_lambda = reg_lambda\n\n            # Xavier/Glorot initialization\n            self.Wx = np.random.randn(hidden_dim, input_dim) * np.sqrt(1.0 / input_dim)\n            self.Wh = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(1.0 / hidden_dim)\n            self.b = np.zeros(hidden_dim)\n            self.U = np.random.randn(output_dim, hidden_dim) * np.sqrt(1.0 / hidden_dim)\n            self.c = np.zeros(output_dim)\n\n        def forward(self, x_seq):\n            T = x_seq.shape[0]\n            h = np.zeros((T + 1, self.hidden_dim))\n            s = np.zeros(T)\n            y_hat = np.zeros(T)\n            \n            for t in range(T):\n                h[t+1] = np.tanh(self.Wx @ x_seq[t] + self.Wh @ h[t] + self.b)\n                s[t] = (self.U @ h[t+1] + self.c).item()\n                y_hat[t] = sigmoid(s[t])\n            return h, s, y_hat\n        \n        def train_step(self, x_seq, y_seq, learning_rate):\n            T = x_seq.shape[0]\n            \n            # Forward pass\n            h, s, y_hat = self.forward(x_seq)\n\n            # --- Backward pass (BPTT) ---\n            # Initialize gradients\n            d_Wx, d_Wh, d_b = np.zeros_like(self.Wx), np.zeros_like(self.Wh), np.zeros_like(self.b)\n            d_U, d_c = np.zeros_like(self.U), np.zeros_like(self.c)\n            \n            # Gradient of loss w.r.t. pre-sigmoid output\n            d_s = y_hat - y_seq\n            \n            d_h_next = np.zeros(self.hidden_dim)\n            \n            for t in reversed(range(T)):\n                # Output layer gradients\n                d_U += d_s[t] * h[t+1].reshape(1, -1)\n                d_c += d_s[t]\n                \n                # Propagate gradient back to hidden state\n                d_h = d_s[t] * self.U.flatten() + d_h_next\n                \n                # Propagate through tanh non-linearity\n                d_tanh = d_h * (1 - h[t+1]**2)\n                \n                # Recurrent layer gradients\n                d_b += d_tanh\n                d_Wh += np.outer(d_tanh, h[t])\n                d_Wx += np.outer(d_tanh, x_seq[t])\n                \n                # Pass gradient to previous time step\n                d_h_next = d_tanh @ self.Wh\n            \n            # Add L2 regularization gradients\n            d_Wx += 2 * self.reg_lambda * self.Wx\n            d_Wh += 2 * self.reg_lambda * self.Wh\n            d_U += 2 * self.reg_lambda * self.U\n\n            # Update parameters\n            self.Wx -= learning_rate * d_Wx\n            self.Wh -= learning_rate * d_Wh\n            self.b -= learning_rate * d_b\n            self.U -= learning_rate * d_U\n            self.c -= learning_rate * d_c\n    \n        def predict(self, x_seq):\n            _, _, y_hat = self.forward(x_seq)\n            return (y_hat = 0.5).astype(int)\n\n    # ------------------- MAIN EXECUTION LOGIC -------------------\n\n    # 1. Prepare training data\n    train_features_list = []\n    train_labels_list = []\n    train_onehot_list = []\n    \n    for seq in TRAIN_SEQS:\n        features, labels = generate_truth(seq)\n        train_features_list.append(features)\n        train_labels_list.append(labels)\n        train_onehot_list.append(one_hot_encode(seq))\n\n    all_train_features = np.concatenate(train_features_list)\n\n    # 2. Fit and evaluate unsupervised model\n    unsup_model = KMeans1D()\n    unsup_model.fit(all_train_features)\n    \n    unsup_accuracies = {}\n    for name, seq in TEST_SUITE.items():\n        test_features, test_labels = generate_truth(seq)\n        preds = unsup_model.predict(test_features)\n        acc = np.mean(preds == test_labels) if len(test_labels)  0 else 1.0\n        unsup_accuracies[name] = acc\n        \n    # 3. Train and evaluate supervised model\n    rnn = SimpleRNN(\n        input_dim=VOCAB_SIZE, \n        hidden_dim=D_HIDDEN, \n        output_dim=1,\n        reg_lambda=LAMBDA_REG\n    )\n\n    for epoch in range(EPOCHS):\n        indices = np.random.permutation(len(TRAIN_SEQS))\n        for i in indices:\n            x_seq = train_onehot_list[i]\n            y_seq = train_labels_list[i]\n            rnn.train_step(x_seq, y_seq, LEARNING_RATE)\n\n    sup_accuracies = {}\n    for name, seq in TEST_SUITE.items():\n        x_seq = one_hot_encode(seq)\n        _, test_labels = generate_truth(seq)\n        preds = rnn.predict(x_seq)\n        acc = np.mean(preds == test_labels) if len(test_labels)  0 else 1.0\n        sup_accuracies[name] = acc\n\n    # 4. Assemble and print results\n    results = []\n    for name in sorted(TEST_SUITE.keys()):\n        results.append(sup_accuracies[name])\n        results.append(unsup_accuracies[name])\n    \n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\n\nsolve()\n```"
        }
    ]
}