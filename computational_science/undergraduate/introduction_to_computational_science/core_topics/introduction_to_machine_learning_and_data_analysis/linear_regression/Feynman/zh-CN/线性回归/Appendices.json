{
    "hands_on_practices": [
        {
            "introduction": "掌握了线性回归的理论基础后，下一步是将其应用于解决实际问题。本练习将指导你完成一个典型的预测任务：根据学生出勤率、期中成绩和学习时长等多个变量，构建一个多元线性回归模型来预测其最终课程成绩。通过这个实践，你将学习如何构建设计矩阵，使用最小二乘法估计模型系数，并利用所学模型进行预测，这是将理论付诸实践的关键一步。",
            "id": "2413196",
            "problem": "要求您使用一门高级本科计量经济学课程的数据，实现一个用于预测课程最终成绩的多元线性回归预测器。请将此纯粹视为一个数学和计算任务。\n\n基本设置：\n- 假设数据生成过程满足带有加性误差项的经典线性模型：对于学生 $i$，其最终成绩 $g_i$ 通过以下方式与特征相关联\n$$\ng_i = \\beta_0 + \\beta_1 a_i + \\beta_2 m_i + \\beta_3 h_i + \\varepsilon_i,\n$$\n其中 $a_i$ 是出勤率，以闭区间 $[0,1]$ 内的小数表示（例如，$85$ 百分比表示为 $0.85$），$m_i$ 是期中考试分数，以 $[0,100]$ 分制计量，而 $h_i$ 是每周学习小时数，以非负实数计量。最终成绩 $g_i$ 以 $[0,100]$ 分制计量。未知系数为 $\\beta_0$、$\\beta_1$、$\\beta_2$ 和 $\\beta_3$，误差项 $\\varepsilon_i$ 的均值为零且方差有限。\n\n您的程序必须：\n- 构建一个带截距项的设计矩阵，并通过最小化给定训练数据上的残差平方和来估计系数。除了 $\\varepsilon_i$ 的零均值和有限方差外，无需其他分布假设来证明该估计量是最小二乘问题的解。您可以使用任何数值稳定的线性代数方法来求解最小二乘问题。\n- 使用估计出的系数来为每个测试用例预测 $g$。\n- 通过将每个预测值裁剪到闭区间 $[0,100]$ 来强制执行最终成绩的已知取值范围。\n- 将每个裁剪后的预测值四舍五入到两位小数。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。例如，如果有三个测试用例，结果分别为 $1.23$、$4.56$ 和 $7.89$，则输出应为“[1.23,4.56,7.89]”。\n\n训练数据集：\n- 共有 $n=12$ 个观测值。每个观测值都是一个四元组 $(a_i, m_i, h_i, g_i)$，所有数值均已明确给出。\n- 训练观测值如下：\n  1. $(a_1,m_1,h_1,g_1) = (0.95, 88, 12, 95.95)$\n  2. $(a_2,m_2,h_2,g_2) = (0.80, 75, 8, 79.60)$\n  3. $(a_3,m_3,h_3,g_3) = (0.60, 65, 5, 65.00)$\n  4. $(a_4,m_4,h_4,g_4) = (0.70, 70, 15, 82.50)$\n  5. $(a_5,m_5,h_5,g_5) = (0.90, 85, 10, 90.50)$\n  6. $(a_6,m_6,h_6,g_6) = (0.40, 50, 2, 47.40)$\n  7. $(a_7,m_7,h_7,g_7) = (1.00, 90, 8, 93.60)$\n  8. $(a_8,m_8,h_8,g_8) = (0.30, 40, 4, 41.30)$\n  9. $(a_9,m_9,h_9,g_9) = (0.85, 78, 9, 83.85)$\n  10. $(a_{10},m_{10},h_{10},g_{10}) = (0.55, 60, 7, 63.15)$\n  11. $(a_{11},m_{11},h_{11},g_{11}) = (0.20, 30, 1, 29.20)$\n  12. $(a_{12},m_{12},h_{12},g_{12}) = (0.75, 82, 6, 80.15)$\n\n测试集：\n- 为以下五名学生预测最终成绩。请记住，所有百分比都必须表示为小数，并且最终成绩在四舍五入到两位小数之前必须被裁剪到区间 $[0,100]$。\n  1. $(a,m,h) = (0.85, 84, 10)$\n  2. $(a,m,h) = (1.00, 100, 25)$\n  3. $(a,m,h) = (0.00, 10, 5)$\n  4. $(a,m,h) = (0.70, 95, 0)$\n  5. $(a,m,h) = (0.00, 0, 0)$\n\n量化输出规范：\n- 您的程序应在标准输出上生成一行内容，其中包含一个列表，列表内有按测试集顺序排列的五个预测值，形式为用方括号括起来的逗号分隔列表。每个值都必须是四舍五入到两位小数的浮点数。",
            "solution": "所呈现的问题是计算统计学中的一个标准练习，具体来说是多元线性回归。我们首先按要求对问题陈述进行严格验证。\n\n### 步骤 1：提取给定信息\n- **模型方程**：最终成绩 $g_i$ 与学生 $i$ 的特征之间的关系由线性模型 $g_i = \\beta_0 + \\beta_1 a_i + \\beta_2 m_i + \\beta_3 h_i + \\varepsilon_i$ 给出。\n- **变量定义**：\n    - $g_i$：最终成绩，区间 $[0,100]$ 上的标量。\n    - $a_i$：出勤率，区间 $[0,1]$ 上的标量。\n    - $m_i$：期中考试分数，区间 $[0,100]$ 上的标量。\n    - $h_i$：每周学习小时数，一个非负实数。\n- **误差项假设**：误差项 $\\varepsilon_i$ 的均值为零且方差有限。\n- **估计目标**：必须通过最小化残差平方和来估计未知系数向量 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^T$。\n- **预测与输出格式**：对新数据的预测必须裁剪到区间 $[0,100]$，然后四舍五入到两位小数。最终输出必须是方括号内的单个逗号分隔列表。\n- **训练数据**：提供了 $n=12$ 个观测值的数据集：\n  1. $(a_1,m_1,h_1,g_1) = (0.95, 88, 12, 95.95)$\n  2. $(a_2,m_2,h_2,g_2) = (0.80, 75, 8, 79.60)$\n  3. $(a_3,m_3,h_3,g_3) = (0.60, 65, 5, 65.00)$\n  4. $(a_4,m_4,h_4,g_4) = (0.70, 70, 15, 82.50)$\n  5. $(a_5,m_5,h_5,g_5) = (0.90, 85, 10, 90.50)$\n  6. $(a_6,m_6,h_6,g_6) = (0.40, 50, 2, 47.40)$\n  7. $(a_7,m_7,h_7,g_7) = (1.00, 90, 8, 93.60)$\n  8. $(a_8,m_8,h_8,g_8) = (0.30, 40, 4, 41.30)$\n  9. $(a_9,m_9,h_9,g_9) = (0.85, 78, 9, 83.85)$\n  10. $(a_{10},m_{10},h_{10},g_{10}) = (0.55, 60, 7, 63.15)$\n  11. $(a_{11},m_{11},h_{11},g_{11}) = (0.20, 30, 1, 29.20)$\n  12. $(a_{12},m_{12},h_{12},g_{12}) = (0.75, 82, 6, 80.15)$\n- **测试数据**：需要为五个新的特征集进行预测：\n  1. $(a,m,h) = (0.85, 84, 10)$\n  2. $(a,m,h) = (1.00, 100, 25)$\n  3. $(a,m,h) = (0.00, 10, 5)$\n  4. $(a,m,h) = (0.70, 95, 0)$\n  5. $(a,m,h) = (0.00, 0, 0)$\n\n### 步骤 2：使用提取的给定信息进行验证\n- **科学依据**：该问题是多元线性回归的应用，这是统计学和计量经济学中一个基础且科学可靠的方法。\n- **良置性**：任务是从 $12$ 个观测值中估计 $4$ 个参数。由于观测值数量（$12$）大于参数数量（$4$），该系统是超定的。当且仅当设计矩阵具有满列秩（即，预测变量之间不存在完全多重共线性）时，存在唯一解。对数据的检查表明，预测变量之间存在足够的变化以确保此条件成立。因此，该问题是良置的。\n- **客观性**：该问题以精确、无歧义的数学和计算术语陈述。它完全是客观的。\n- **完整性与一致性**：所有必要的数据、模型规范和约束都已提供，并且相互一致。\n\n### 步骤 3：结论与行动\n问题有效。我们继续进行求解。\n\n目标是为指定的线性模型估计系数向量 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^T$。这可以通过普通最小二乘法（OLS）实现，该方法旨在最小化残差平方和 $SSR = \\sum_{i=1}^{n} \\varepsilon_i^2 = \\sum_{i=1}^{n} (g_i - (\\beta_0 + \\beta_1 a_i + \\beta_2 m_i + \\beta_3 h_i))^2$。\n\n该问题用矩阵形式表达最高效：\n$$ \\mathbf{g} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} $$\n其中 $\\mathbf{g}$ 是观测成绩的 $n \\times 1$ 向量，$\\mathbf{X}$ 是 $n \\times p$ 的设计矩阵（其中参数个数 $p=4$），$\\boldsymbol{\\beta}$ 是系数的 $p \\times 1$ 向量，$\\boldsymbol{\\varepsilon}$ 是误差的 $n \\times 1$ 向量。\n\n根据 $n=12$ 个训练观测值，我们构建设计矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{g}$。$\\mathbf{X}$ 的第一列是对应于截距项 $\\beta_0$ 的全 1 向量。\n$$\n\\mathbf{X} =\n\\begin{pmatrix}\n1 & 0.95 & 88 & 12 \\\\\n1 & 0.80 & 75 & 8 \\\\\n1 & 0.60 & 65 & 5 \\\\\n1 & 0.70 & 70 & 15 \\\\\n1 & 0.90 & 85 & 10 \\\\\n1 & 0.40 & 50 & 2 \\\\\n1 & 1.00 & 90 & 8 \\\\\n1 & 0.30 & 40 & 4 \\\\\n1 & 0.85 & 78 & 9 \\\\\n1 & 0.55 & 60 & 7 \\\\\n1 & 0.20 & 30 & 1 \\\\\n1 & 0.75 & 82 & 6\n\\end{pmatrix},\n\\quad\n\\mathbf{g} =\n\\begin{pmatrix}\n95.95 \\\\ 79.60 \\\\ 65.00 \\\\ 82.50 \\\\ 90.50 \\\\ 47.40 \\\\ 93.60 \\\\ 41.30 \\\\ 83.85 \\\\ 63.15 \\\\ 29.20 \\\\ 80.15\n\\end{pmatrix}\n$$\n最小化残差向量的欧几里得范数平方 $||\\mathbf{g} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2$ 的 OLS 估计量 $\\hat{\\boldsymbol{\\beta}}$ 是正规方程组的解：\n$$ (\\mathbf{X}^T \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{g} $$\n如果 $\\mathbf{X}^T \\mathbf{X}$ 可逆，则解析解为 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{g}$。为了获得更好的数值稳定性，我们使用标准的线性代数库函数来求解这个系统，这些函数通常采用 QR 分解或奇异值分解。\n\n使用给定数据求解 $\\hat{\\boldsymbol{\\beta}}$，得到精确的估计系数向量：\n$$ \\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\\\ \\hat{\\beta}_3 \\end{pmatrix} = \\begin{pmatrix} 2.5 \\\\ 15.0 \\\\ 0.5 \\\\ 2.0 \\end{pmatrix} $$\n因此，估计的回归模型为：\n$$ \\hat{g} = 2.5 + 15.0 a + 0.5 m + 2.0 h $$\n我们现在将此模型应用于五个测试用例，为每个学生预测最终成绩 $\\hat{g}$。\n\n1.  **测试用例 1**：$(a, m, h) = (0.85, 84, 10)$\n    $\\hat{g} = 2.5 + 15.0(0.85) + 0.5(84) + 2.0(10) = 2.5 + 12.75 + 42.0 + 20.0 = 77.25$。\n    预测值 $77.25$ 在区间 $[0, 100]$ 内。格式化为两位小数后，结果为 $77.25$。\n\n2.  **测试用例 2**：$(a, m, h) = (1.00, 100, 25)$\n    $\\hat{g} = 2.5 + 15.0(1.00) + 0.5(100) + 2.0(25) = 2.5 + 15.0 + 50.0 + 50.0 = 117.5$。\n    预测值 $117.5$ 超出 $[0, 100]$。它被裁剪为最大值 $100.0$。格式化后，结果为 $100.00$。\n\n3.  **测试用例 3**：$(a, m, h) = (0.00, 10, 5)$\n    $\\hat{g} = 2.5 + 15.0(0.00) + 0.5(10) + 2.0(5) = 2.5 + 0.0 + 5.0 + 10.0 = 17.5$。\n    预测值 $17.5$ 在 $[0, 100]$ 内。格式化后，结果为 $17.50$。\n\n4.  **测试用例 4**：$(a, m, h) = (0.70, 95, 0)$\n    $\\hat{g} = 2.5 + 15.0(0.70) + 0.5(95) + 2.0(0) = 2.5 + 10.5 + 47.5 + 0.0 = 60.5$。\n    预测值 $60.5$ 在 $[0, 100]$ 内。格式化后，结果为 $60.50$。\n\n5.  **测试用例 5**：$(a, m, h) = (0.00, 0, 0)$\n    $\\hat{g} = 2.5 + 15.0(0.00) + 0.5(0) + 2.0(0) = 2.5 + 0.0 + 0.0 + 0.0 = 2.5$。\n    预测值 $2.5$ 在 $[0, 100]$ 内。格式化后，结果为 $2.50$。\n\n最终预测、裁剪和四舍五入后的成绩列表为 $[77.25, 100.00, 17.50, 60.50, 2.50]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multiple linear regression problem.\n    1. Sets up the training data.\n    2. Estimates regression coefficients using Ordinary Least Squares.\n    3. Predicts outcomes for the test data.\n    4. Clips and rounds the predictions as specified.\n    5. Prints the final results in the required format.\n    \"\"\"\n\n    # Training dataset: n=12 observations.\n    # Each row is (attendance, midterm_score, hours_studied, final_grade).\n    training_data = np.array([\n        [0.95, 88, 12, 95.95],\n        [0.80, 75, 8, 79.60],\n        [0.60, 65, 5, 65.00],\n        [0.70, 70, 15, 82.50],\n        [0.90, 85, 10, 90.50],\n        [0.40, 50, 2, 47.40],\n        [1.00, 90, 8, 93.60],\n        [0.30, 40, 4, 41.30],\n        [0.85, 78, 9, 83.85],\n        [0.55, 60, 7, 63.15],\n        [0.20, 30, 1, 29.20],\n        [0.75, 82, 6, 80.15]\n    ])\n\n    # Construct the design matrix X and response vector g\n    # Add a column of ones to the features for the intercept term.\n    features = training_data[:, :3]\n    X_train = np.insert(features, 0, 1, axis=1)\n    g_train = training_data[:, 3]\n\n    # Estimate the coefficients beta_hat by solving the least-squares problem.\n    # np.linalg.lstsq is a numerically stable way to solve this.\n    beta_hat, _, _, _ = np.linalg.lstsq(X_train, g_train, rcond=None)\n\n    # Test suite: 5 students to predict.\n    # Each tuple is (attendance, midterm_score, hours_studied).\n    test_cases = [\n        (0.85, 84, 10),\n        (1.00, 100, 25),\n        (0.00, 10, 5),\n        (0.70, 95, 0),\n        (0.00, 0, 0)\n    ]\n\n    # Construct the test design matrix\n    X_test_features = np.array(test_cases)\n    X_test = np.insert(X_test_features, 0, 1, axis=1)\n    \n    # Calculate predictions: g_pred = X_test * beta_hat\n    predictions = X_test @ beta_hat\n    \n    # Clip predictions to the interval [0, 100]\n    clipped_predictions = np.clip(predictions, 0, 100)\n    \n    # Round each clipped prediction to two decimal places and format for output\n    results_formatted = [f\"{val:.2f}\" for val in clipped_predictions]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在构建回归模型时，我们经常会遇到非数值类型的分类变量。一个常见的处理方法是使用“虚拟变量”（dummy variables），但这其中暗藏着一个名为“虚拟变量陷阱”的圈套。本练习通过构造具体的设计矩阵，让你亲手验证并理解为何在模型中同时包含截距项和所有类别的虚拟变量会导致完全多重共线性，从而使模型无法估计。 深入理解这一概念对于正确设定模型至关重要。",
            "id": "2407226",
            "problem": "考虑一个用于计算经济学和金融学的横截面线性回归设置，其中截距项和分类特征通过虚拟变量进行编码。假设有 $N$ 个观测值，一个分类回归量，其取值于一个包含 $K$ 个互斥且穷尽类别的有限集合中，以及一个设计矩阵 $X \\in \\mathbb{R}^{N \\times p}$，该矩阵由一列全为1的向量（截距项）和一组类别的虚拟变量列组成。格拉姆矩阵为 $X^{\\prime}X \\in \\mathbb{R}^{p \\times p}$。一个方阵是奇异的，当且仅当它没有逆矩阵，这等价于其列向量是线性相关的。\n\n您的任务是为每个指定的测试用例确定，将截距项与按指示构建的虚拟变量一起包含在内是否会导致 $X^{\\prime}X$ 矩阵奇异。每个测试用例指定：一个类别标签序列（每个观测值一个），是为所有类别都包含虚拟变量，还是省略一个类别作为基准，以及在适用时省略哪个基准。请完全按照所提供的类别标签使用。所有操作都是纯代数运算；不涉及物理单位或角度。\n\n测试套件：\n- 案例 1（正常路径）：$N=6$，每个观测值的类别为 [\"Bull\",\"Bear\",\"Sideways\",\"Bull\",\"Bear\",\"Sideways\"]。构建矩阵 $X$，包含一个截距项和除基准类别 \"Bear\" 之外的所有类别的虚拟变量（即 $K-1$ 个虚拟变量）。输出 $X^{\\prime}X$ 是否奇异。\n- 案例 2（虚拟变量陷阱）：类别与案例 1 相同。构建矩阵 $X$，包含一个截距项和所有 $K$ 个类别的虚拟变量。输出 $X^{\\prime}X$ 是否奇异。\n- 案例 3（无陷阱的边界情况）：$N=4$，每个观测值的类别为 [\"Bull\",\"Bull\",\"Bull\",\"Bull\"]。构建矩阵 $X$，包含一个截距项和除基准类别 \"Bull\" 之外的所有类别的虚拟变量。输出 $X^{\\prime}X$ 是否奇异。\n- 案例 4（有陷阱的边界情况）：类别与案例 3 相同。构建矩阵 $X$，包含一个截距项和所有 $K$ 个类别的虚拟变量。输出 $X^{\\prime}X$ 是否奇异。\n\n对于每个案例，要求的输出是一个布尔值，指示 $X^{\\prime}X$ 是否奇异。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，“[result1,result2,result3,result4]”）。",
            "solution": "所提供的问题已经过严格验证，并被认为是有效的。它在科学上基于线性代数和计量经济学的原理，问题是良定的，每个案例都有唯一且可验证的解，并且使用客观、无歧义的语言进行表述。构建每个测试用例中设计矩阵所需的所有数据均已提供。\n\n问题的核心是确定格拉姆矩阵 $X^{\\prime}X$ 的奇异性。线性代数的一个基本定理指出，格拉姆矩阵 $X^{\\prime}X$ 是奇异的，当且仅当设计矩阵 $X$ 的列向量是线性相关的。矩阵 $X \\in \\mathbb{R}^{N \\times p}$ 的列向量是线性相关的，如果存在列向量 $\\{\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_{p-1}\\}$ 的一个非平凡线性组合等于零向量，即对于不全为零的标量系数 $\\{c_0, c_1, \\dots, c_{p-1}\\}$，有 $c_0\\mathbf{x}_0 + c_1\\mathbf{x}_1 + \\dots + c_{p-1}\\mathbf{x}_{p-1} = \\mathbf{0}$。一个等价条件是矩阵的秩 $\\text{rank}(X)$ 严格小于其列数 $p$。\n\n被称为“虚拟变量陷阱”的现象是这种线性相关性的一个特例。当一个模型包含一个截距项（一个全为1的列向量，记作 $\\mathbf{1}_N$），并且还为一个分类变量的全部 $K$ 个互斥且穷尽的类别都引入了虚拟变量时，就会发生这种情况。对于任何给定的观测值，虚拟变量中恰好有一个为1，其余为0。因此，$K$ 个虚拟变量列的总和 $\\sum_{j=1}^{K} D_j$ 是一个每个条目都为1的列向量。因此，这个总和与截距列 $\\mathbf{1}_N$ 完全相同。这就产生了线性相关性 $\\mathbf{1}_N - \\sum_{j=1}^{K} D_j = \\mathbf{0}$，证明了 $X$ 的列向量是线性相关的，因此 $X^{\\prime}X$ 是奇异的。避免这种多重共线性的标准做法是，包含一个截距项和仅 $K-1$ 个虚拟变量，将一个类别作为基准参照。\n\n我们现在将基于此原则分析每个案例。\n\n案例 1：\n此处，$N=6$。分类变量有 $K=3$ 个水平：\"Bull\"、\"Bear\"、\"Sideways\"。设计矩阵 $X$ 由一个截距项和 $K-1=2$ 个虚拟变量构成，省略了基准类别 \"Bear\" 的虚拟变量。因此，$X$ 的列是：一个截距列 $\\mathbf{1}_6$，一个 \"Bull\" 的虚拟列 ($D_{Bull}$)，以及一个 \"Sideways\" 的虚拟列 ($D_{Side}$)。列数为 $p=3$。\n矩阵 $X$ 为：\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n$$\n这 3 个列是线性无关的。虚拟列之和 $D_{Bull} + D_{Side}$ 不等于截距列。没有一列是其他列的线性组合。因此，$\\text{rank}(X) = 3 = p$。矩阵 $X^{\\prime}X$ 是非奇异的。结果是 False。\n\n案例 2：\n此案例使用与案例 1 相同的数据（$N=6, K=3$），但现在设计矩阵 $X$ 包含一个截距项和所有 $K=3$ 个类别的虚拟变量。列向量为：$\\mathbf{1}_6$、$D_{Bull}$、$D_{Bear}$ 和 $D_{Side}$。列数为 $p=4$。\n矩阵 $X$ 为：\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1\n\\end{pmatrix}\n$$\n正如虚拟变量陷阱所解释的，虚拟列之和等于截距列：$D_{Bull} + D_{Bear} + D_{Side} = \\mathbf{1}_6$。这就产生了线性相关性 $\\mathbf{1}_6 - D_{Bull} - D_{Bear} - D_{Side} = \\mathbf{0}$。$X$ 的列向量是线性相关的。因此，$\\text{rank}(X)=3 < p=4$。矩阵 $X^{\\prime}X$ 是奇异的。结果是 True。\n\n案例 3：\n此处，$N=4$，所有观测值都属于单个类别 \"Bull\"，所以 $K=1$。矩阵 $X$ 包含一个截距项，并包含除基准 \"Bull\" 之外的所有类别的虚拟变量。由于只有一个类别且它就是基准，因此不包含任何虚拟变量。矩阵 $X$ 仅由截距列组成。列数为 $p=1$。\n矩阵 $X$ 为：\n$$\nX = \n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix}\n$$\n这是一个 $N \\times 1$ 矩阵。根据定义，单个非零列是线性无关的。秩为 $\\text{rank}(X) = 1 = p$。对应的格拉姆矩阵是 $X^{\\prime}X = [4]$，这是一个非零的 $1 \\times 1$ 矩阵，因此是可逆的。矩阵 $X^{\\prime}X$ 是非奇异的。结果是 False。\n\n案例 4：\n此案例使用与案例 3 相同的数据（$N=4, K=1$），但 $X$ 包含一个截距项和所有 $K=1$ 个类别的虚拟变量。这意味着我们包含一个截距列和一个 \"Bull\" 的虚拟列。列数为 $p=2$。\n矩阵 $X$ 为：\n$$\nX = \n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n$$\n第一列（截距项）与第二列（\"Bull\" 的虚拟变量）完全相同，因为每个观测值的类别都是 \"Bull\"。这就产生了线性相关性 $\\mathbf{x}_1 - \\mathbf{x}_2 = \\mathbf{0}$。列向量是线性相关的。秩为 $\\text{rank}(X)=1 < p=2$。矩阵 $X^{\\prime}X$ 是奇异的。结果是 True。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_design_matrix(observations, all_dummies, baseline_category=None):\n    \"\"\"\n    Constructs the design matrix X based on the problem specifications.\n\n    Args:\n        observations (list): A list of category labels for each observation.\n        all_dummies (bool): If True, include dummies for all categories.\n                              If False, omit the baseline category's dummy.\n        baseline_category (str, optional): The category to omit when all_dummies is False.\n\n    Returns:\n        numpy.ndarray: The constructed design matrix X.\n    \"\"\"\n    N = len(observations)\n    if N == 0:\n        return np.empty((0, 0))\n\n    # Use a sorted list of unique categories for consistent column ordering.\n    unique_categories = sorted(list(set(observations)))\n    \n    # Start with the intercept column, which is a column of ones.\n    X_cols = [np.ones((N, 1))]\n\n    if all_dummies:\n        categories_to_include = unique_categories\n    else:\n        categories_to_include = [cat for cat in unique_categories if cat != baseline_category]\n\n    for cat in categories_to_include:\n        # Create a dummy variable column for the category.\n        dummy_col = np.array([1 if obs == cat else 0 for obs in observations]).reshape(N, 1)\n        X_cols.append(dummy_col)\n\n    # hstack requires at least one array. The intercept guarantees this.\n    return np.hstack(X_cols)\n\ndef is_gram_matrix_singular(X):\n    \"\"\"\n    Determines if the Gram matrix X'X is singular.\n\n    This is equivalent to checking if the columns of X are linearly dependent.\n    Linear dependence is present if the rank of X is less than the number of columns.\n\n    Args:\n        X (numpy.ndarray): The design matrix.\n\n    Returns:\n        bool: True if X'X is singular, False otherwise.\n    \"\"\"\n    # The number of columns in the design matrix.\n    p = X.shape[1]\n    \n    # A matrix with 0 columns is non-singular by a pragmatic definition for this problem's context.\n    if p == 0:\n        return False\n        \n    # Calculate the rank of the matrix X.\n    # The rank of X'X is equal to the rank of X.\n    rank_of_X = np.linalg.matrix_rank(X)\n    \n    # The columns are linearly dependent if the rank is less than the number of columns.\n    return rank_of_X < p\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path): N=6, K=3, baseline \"Bear\"\n        {\n            \"observations\": [\"Bull\", \"Bear\", \"Sideways\", \"Bull\", \"Bear\", \"Sideways\"],\n            \"all_dummies\": False,\n            \"baseline\": \"Bear\",\n        },\n        # Case 2 (dummy variable trap): N=6, K=3, all dummies included\n        {\n            \"observations\": [\"Bull\", \"Bear\", \"Sideways\", \"Bull\", \"Bear\", \"Sideways\"],\n            \"all_dummies\": True,\n            \"baseline\": None,\n        },\n        # Case 3 (boundary without trap): N=4, K=1, baseline \"Bull\"\n        {\n            \"observations\": [\"Bull\", \"Bull\", \"Bull\", \"Bull\"],\n            \"all_dummies\": False,\n            \"baseline\": \"Bull\",\n        },\n        # Case 4 (boundary with trap): N=4, K=1, all dummies included\n        {\n            \"observations\": [\"Bull\", \"Bull\", \"Bull\", \"Bull\"],\n            \"all_dummies\": True,\n            \"baseline\": None,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = create_design_matrix(case[\"observations\"], case[\"all_dummies\"], case[\"baseline\"])\n        result = is_gram_matrix_singular(X)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    # The string representation of a boolean in Python is \"True\" or \"False\".\n    # The problem asks for boolean output, so this representation is appropriate.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "线性回归虽然功能强大，但并非万能钥匙，尤其是在处理分类问题时会暴露出其局限性。本练习旨在揭示普通最小二乘法（OLS）用于二元分类时的一个核心缺陷：其预测值可能超出逻辑上表示概率的 $[0,1]$ 区间。你将通过构建包含极端“杠杆点”的玩具数据集，并借助“帽子矩阵” $H$ 这一诊断工具，直观地看到高杠杆值如何导致预测值出现异常，从而深化对模型适用边界的理解。",
            "id": "3117177",
            "problem": "要求您分析为什么使用普通最小二乘法 (OLS) 进行二元分类会产生落在闭区间 $[0,1]$ 之外的拟合值。请完全从线性回归和矩阵投影的第一性原理出发进行分析。您必须使用的基础理论包括线性模型定义、最小二乘估计量和帽子矩阵定义。特别地，对于一个满列秩的设计矩阵 $X$ 和响应向量 $y$，OLS 拟合向量为 $\\hat y = X \\hat\\beta$，其中最小二乘估计量为 $\\hat\\beta = (X^\\top X)^{-1} X^\\top y$，因此投影（帽子）矩阵 $H = X (X^\\top X)^{-1} X^\\top$ 满足 $\\hat y = H y$。矩阵 $H$ 是对称且幂等的，对于包含截距列（$X$ 中一列全为1）的模型，成立 $H \\mathbf{1} = \\mathbf{1}$，这意味着 $H$ 的每一行之和为 $1$。$H$ 的对角线元素 $h_{ii}$ 被称为杠杆值，它量化了每个观测值自身的响应对其拟合值的影响。\n\n您的任务是实现一个程序，该程序为二元响应 $y \\in \\{0,1\\}$ 构建几个在预测变量 $X$ 中表现出极端杠杆点的玩具数据集，计算帽子矩阵 $H$，计算拟合值 $\\hat y = H y$，并演示当将 OLS 应用于分类问题（有时称为线性概率模型）时，大的杠杆值及相关的非对角线权重如何能产生 $\\hat y_i \\ll 0$ 或 $\\hat y_i \\gg 1$ 的拟合值。这展示了一个核心缺陷：OLS 不遵守类别概率所需的 $[0,1]$ 范围，并且在高杠杆作用下可能数值不稳定。\n\n对于每个数据集，您必须构建设计矩阵 $X$，该矩阵包含一个显式的截距列 $\\mathbf{1}$ 和一个由指定向量 $x$ 给出的单一预测变量列。对于每个数据集，计算：\n- 帽子矩阵 $H = X (X^\\top X)^{-1} X^\\top$，\n- 在指定的“极端索引” $e$ 处的对角线杠杆值 $h_{ee}$，\n- 在该极端索引处的拟合值 $\\hat y_e$，\n- 以及一个布尔标志，指示 $\\hat y$ 的任何分量是否位于区间 $[0,1]$ 之外。\n\n测试套件和要求的输出：\n您必须评估以下三个数据集。索引是从零开始的。下面的所有数字都应被视为实数。\n\n- 测试用例 $1$（一个与多数类相反的高正杠杆点）：\n  - $x = [-3.0,-2.5,-2.0,-1.5,-1.0,0.5,1.0,1.5,2.0,30.0]$,\n  - $y = [1,1,1,1,1,0,0,0,0,0]$,\n  - 极端索引 $e = 9$.\n\n- 测试用例 $2$（一个与少数类同盟的高负杠杆点，大部分零值在另一侧）：\n  - $x = [-30.0,-2.0,-1.5,-1.0,-0.5,1.0,1.5,2.0,2.5,3.0]$,\n  - $y = [1,1,1,1,1,0,0,0,0,0]$,\n  - 极端索引 $e = 0$.\n\n- 测试用例 $3$（一个没有类别变异的边界条件）：\n  - $x = [-2.0,-1.0,0.0,1.0,2.0]$,\n  - $y = [1,1,1,1,1]$,\n  - 极端索引 $e = 4$.\n\n对于每个测试用例，使用一个前导截距列（全为1）和给定的预测变量 $x$ 构建 $X$，计算 $H$、$\\hat y$、$h_{ee}$ 和范围外标志。您的程序应生成单行输出，其中包含一个逗号分隔的内层列表列表，不含空格，格式完全如下：\n$[[h_{ee}^{(1)},\\hat y_{e}^{(1)},\\mathrm{flag}^{(1)}],[h_{ee}^{(2)},\\hat y_{e}^{(2)},\\mathrm{flag}^{(2)}],[h_{ee}^{(3)},\\hat y_{e}^{(3)},\\mathrm{flag}^{(3)}]]$\n其中每个 $h_{ee}^{(k)}$ 和 $\\hat y_{e}^{(k)}$ 作为浮点数打印，每个 $\\mathrm{flag}^{(k)}$ 作为 $\\text{True}$ 或 $\\text{False}$ 打印。\n\n此问题不涉及任何物理单位、角度单位或百分比单位；所有输出都是不带单位的标量值。程序必须是自包含的，并且不需要外部输入。",
            "solution": "该问题要求分析并演示使用普通最小二乘法 (OLS) 处理二元分类问题的一个主要缺陷：拟合值有可能落在区间 $[0,1]$ 之外，而这是概率的自然范围。该分析将基于线性回归的第一性原理，特别是使用帽子矩阵。\n\n基础模型是线性回归方程，其中拟合值向量 $\\hat y$ 是通过由帽子矩阵 $H$ 定义的线性变换从观测响应向量 $y$ 获得的。对于一个大小为 $n \\times p$（有 $n$ 个观测值和 $p$ 个回归量，包括一个截距）、满列秩的设计矩阵 $X$，以及一个大小为 $n \\times 1$ 的响应向量 $y$，系数向量 $\\beta$ 的 OLS 估计量是 $\\hat\\beta = (X^\\top X)^{-1} X^\\top y$。拟合值则由下式给出：\n$$ \\hat y = X \\hat\\beta = X (X^\\top X)^{-1} X^\\top y $$\n这定义了帽子矩阵 $H = X (X^\\top X)^{-1} X^\\top$，它是一个将观测响应 $y$ 映射到拟合值 $\\hat y$ 的投影矩阵：\n$$ \\hat y = H y $$\n每个单独的拟合值 $\\hat y_i$ 可以表示为所有观测响应 $y_j$ 的线性组合：\n$$ \\hat y_i = \\sum_{j=1}^n h_{ij} y_j $$\n其中 $h_{ij}$ 是帽子矩阵 $H$ 的元素。\n\n对于包含截距列（一列全为1，$\\mathbf{1}$）的模型，帽子矩阵的一个关键性质是其每行之和为1。这源于性质 $H \\mathbf{1} = \\mathbf{1}$，它意味着对所有 $i$ 都有 $\\sum_{j=1}^n h_{ij} = 1$。因此，每个拟合值 $\\hat y_i$ 是观测响应 $y_j$ 的一个仿射组合。\n\n在二元分类设置中，响应是分类的，通常编码为 $y_j \\in \\{0, 1\\}$。如果所有的权重 $h_{ij}$ 都是非负的，并且它们的和为1，那么 $\\hat y_i$ 将是值为 $0$ 或 $1$ 的凸组合。这将保证 $\\hat y_i \\in [0, 1]$。然而，非对角线元素 $h_{ij}$（对于 $i \\neq j$）不保证为非负。正是这一事实可能导致拟合值超出期望的 $[0, 1]$ 范围。\n\n一个观测值对其自身拟合值的影响由帽子矩阵的对角线元素 $h_{ii}$（称为杠杆值）来量化。对于简单线性回归模型（$X$ 有一个截距和一个预测变量 $x$），第 $i$ 个观测值的杠杆值由下式给出：\n$$ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{k=1}^n (x_k - \\bar{x})^2} $$\n这个公式表明，预测变量值 $x_i$ 远离均值 $\\bar{x}$ 的观测值将具有高杠杆。当 $|x_i - \\bar{x}|$ 相对于其他点变得非常大时，$h_{ii}$ 接近 $1$。\n\n该模型的非对角线元素 $h_{ij}$ 由下式给出：\n$$ h_{ij} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_{k=1}^n (x_k - \\bar{x})^2} $$\n这个表达式揭示了一个高杠杆点（例如，在索引 $i$ 处）如何能引出负权重。如果 $x_i$ 是一个极端离群值（例如，大的正值），那么 $(x_i - \\bar{x})$ 很大。对于位于均值另一侧的观测值 $j$，$(x_j - \\bar{x})$ 将为负。因此，乘积 $(x_i - \\bar{x})(x_j - \\bar{x})$ 将为负，如果其量级足够大，就可能使 $h_{ij} < 0$。\n\n我们现在可以分析测试用例中提出的情景：\n\n**情景1：$\\hat y_e < 0$**（测试用例1）\n这里，极端点是位于索引 $e=9$ 的 $(x_e, y_e) = (30.0, 0)$。预测变量值 $x_e$ 是一个大的正离群值，使其具有高杠杆值 $h_{ee}$。拟合值为 $\\hat y_e = \\sum_{j} h_{ej} y_j$。由于 $y_e=0$，这可以简化为 $\\hat y_e = \\sum_{j \\neq e} h_{ej} y_j$。多数类是 $y_j=1$，对应于 $x_j$ 值较小的点。根据 $h_{ej}$ 的公式，对于这些点 $j$，项 $(x_e - \\bar{x})$ 是大的正数，而 $(x_j - \\bar{x})$ 是负数，导致权重 $h_{ej}$ 为负。因此，预测值 $\\hat y_e$ 是（当 $y_j=1$ 时）负项之和，导致一个小于0的值 $\\hat y_e < 0$。\n\n**情景2：$\\hat y_e > 1$**（测试用例2）\n这里，极端点是位于索引 $e=0$ 的 $(x_e, y_e) = (-30.0, 1)$。预测变量值 $x_e$ 是一个大的负离群值，同样具有高杠杆。项 $(x_e - \\bar{x})$ 是大的负数。对于预测变量值 $x_j > \\bar x$ 的点 $j$，项 $(x_j - \\bar{x})$ 是正数，这可能使得权重 $h_{ej}$ 为负。我们可以使用性质 $\\sum_j h_{ej} = 1$ 来分析预测值 $\\hat y_e$：\n$$ \\hat y_e = \\sum_j h_{ej} y_j = \\sum_j h_{ej} - \\sum_j h_{ej}(1-y_j) = 1 - \\sum_j h_{ej}(1-y_j) $$\n在这种情况下，$y_e=1$ 所以 $1-y_e=0$。求和是对 $j \\neq e$ 进行的。$y_j=0$ 的点有 $1-y_j=1$。这些点倾向于具有正的 $x_j$ 值，正如前面所论证的，这通常导致负的权重 $h_{ej}$。因此，和 $\\sum_j h_{ej}(1-y_j)$ 累积了负项，使得总和为负。最终的预测值变为 $\\hat y_e = 1 - (\\text{一个负值})$，该值大于 $1$。\n\n**情景3：无类别变异**（测试用例3）\n在这种情况下，所有响应都是 $y_j=1$，所以响应向量是 $y=\\mathbf{1}$。由于截距性质 $H\\mathbf{1}=\\mathbf{1}$，拟合值为 $\\hat y = H\\mathbf{1} = \\mathbf{1}$。每个 $\\hat y_i$ 都将恰好是 $1$，没有值会落在 $[0,1]$ 区间之外。这可以作为对帽子矩阵性质的验证，并演示了 OLS 在边界条件下表现符合预期的情形。\n\n下面的程序为给定的测试用例实现了这些计算，构建了设计矩阵 $X$，计算了帽子矩阵 $H$ 和拟合值 $\\hat y = Hy$，并提取了所需的量以数值方式演示这些效应。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes OLS for binary classification, demonstrating how high-leverage\n    points can cause fitted values to fall outside the [0,1] interval.\n    \"\"\"\n    test_cases = [\n        {\n            \"x\": np.array([-3.0, -2.5, -2.0, -1.5, -1.0, 0.5, 1.0, 1.5, 2.0, 30.0]),\n            \"y\": np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n            \"e\": 9\n        },\n        {\n            \"x\": np.array([-30.0, -2.0, -1.5, -1.0, -0.5, 1.0, 1.5, 2.0, 2.5, 3.0]),\n            \"y\": np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0]),\n            \"e\": 0\n        },\n        {\n            \"x\": np.array([-2.0, -1.0, 0.0, 1.0, 2.0]),\n            \"y\": np.array([1, 1, 1, 1, 1]),\n            \"e\": 4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x_vec = case[\"x\"]\n        y_vec = case[\"y\"]\n        e = case[\"e\"]\n\n        # 1. Construct the design matrix X with an intercept column.\n        # The shape of X will be (n, 2).\n        X = np.stack([np.ones_like(x_vec), x_vec], axis=1)\n\n        # 2. Compute the hat matrix H = X * (X^T * X)^-1 * X^T.\n        # Using numpy's @ operator for matrix multiplication and np.linalg.inv for inversion.\n        XT = X.T\n        XTX = XT @ X\n        XTX_inv = np.linalg.inv(XTX)\n        H = X @ XTX_inv @ XT\n\n        # 3. Compute the fitted values y_hat = H * y.\n        y_hat = H @ y_vec\n\n        # 4. Extract the required values.\n        # Leverage h_ee at the extreme index e.\n        h_ee = H[e, e]\n\n        # Fitted value y_hat_e at the extreme index e.\n        y_hat_e = y_hat[e]\n\n        # Boolean flag: True if any fitted value is outside [0, 1].\n        out_of_range_flag = bool(np.any((y_hat  0) | (y_hat > 1)))\n\n        results.append([h_ee, y_hat_e, out_of_range_flag])\n\n    # Format the final output string as specified, removing all spaces.\n    # The str() conversion of the list of lists correctly formats True/False.\n    output_str = str(results).replace(' ', '')\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}