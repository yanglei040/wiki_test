{
    "hands_on_practices": [
        {
            "introduction": "The best way to understand an algorithm is to perform it by hand. This first practice grounds the abstract concept of least squares in a concrete calculation. You will fit a simple straight line, $y = \\alpha + \\beta t$, to a small set of data points by deriving and solving the normal equations from scratch, giving you a foundational understanding of the mechanics behind the method. ",
            "id": "1031896",
            "problem": "Consider the data points: $(t_1, y_1) = (1, 3)$, $(t_2, y_2) = (2, 1)$, $(t_3, y_3) = (3, 4)$, $(t_4, y_4) = (4, 6)$. Using the method of least squares, fit a linear model $y = \\alpha + \\beta t$ to this data. Compute the least squares estimate of the slope parameter $\\beta$.",
            "solution": "To find the least squares estimate of $\\beta$, solve the normal equations derived from minimizing the sum of squared residuals $S = \\sum_{i=1}^4 (y_i - \\alpha - \\beta t_i)^2$:  \n$$\\frac{\\partial S}{\\partial \\alpha} = -2 \\sum_{i=1}^4 (y_i - \\alpha - \\beta t_i) = 0$$  \n$$\\frac{\\partial S}{\\partial \\beta} = -2 \\sum_{i=1}^4 t_i (y_i - \\alpha - \\beta t_i) = 0$$  \nThis simplifies to the system:  \n$$\\sum_{i=1}^4 y_i = n\\alpha + \\beta \\sum_{i=1}^4 t_i$$  \n$$\\sum_{i=1}^4 t_i y_i = \\alpha \\sum_{i=1}^4 t_i + \\beta \\sum_{i=1}^4 t_i^2$$  \nwhere $n = 4$. Compute the necessary sums:  \n$$\\sum_{i=1}^4 t_i = 1 + 2 + 3 + 4 = 10$$  \n$$\\sum_{i=1}^4 t_i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30$$  \n$$\\sum_{i=1}^4 y_i = 3 + 1 + 4 + 6 = 14$$  \n$$\\sum_{i=1}^4 t_i y_i = (1 \\cdot 3) + (2 \\cdot 1) + (3 \\cdot 4) + (4 \\cdot 6) = 3 + 2 + 12 + 24 = 41$$  \nSubstitute into the normal equations:  \n$$14 = 4\\alpha + 10\\beta \\quad (1)$$  \n$$41 = 10\\alpha + 30\\beta \\quad (2)$$  \nSolve the system. Multiply equation (1) by 5:  \n$$70 = 20\\alpha + 50\\beta \\quad (1a)$$  \nMultiply equation (2) by 2:  \n$$82 = 20\\alpha + 60\\beta \\quad (2a)$$  \nSubtract equation (1a) from (2a):  \n$$82 - 70 = (20\\alpha + 60\\beta) - (20\\alpha + 50\\beta)$$  \n$$12 = 10\\beta$$  \n$$\\beta = \\frac{12}{10} = \\frac{6}{5}$$",
            "answer": "$$\\boxed{\\dfrac{6}{5}}$$"
        },
        {
            "introduction": "With the power of computation, it's tempting to fit data with increasingly complex models, like high-degree polynomials. This exercise serves as a critical cautionary tale against such temptations by exploring Runge's phenomenon. You will write code to see firsthand how a high-degree polynomial can perfectly match a set of sample points yet oscillate wildly between them, leading to poor predictive power—a classic example of overfitting. ",
            "id": "2408214",
            "problem": "You are given a real-valued function $f:\\,[-1,1]\\to\\mathbb{R}$ defined by $f(x)=\\dfrac{1}{1+25x^{2}}$. For a given positive integer $m\\geq 2$, define equispaced sample points $x_i=-1+\\dfrac{2i}{m-1}$ for $i=0,1,\\dots,m-1$. Let $y_i=f(x_i)$ for all $i$. For a given nonnegative integer $n$ with $n\\leq m-1$, consider the polynomial space $\\mathcal{P}_n=\\{p:\\,p(x)=\\sum_{j=0}^{n}c_j x^{j}\\}$ of degree at most $n$. Define the discrete inner product on functions $g,h:[-1,1]\\to\\mathbb{R}$ by $\\langle g,h\\rangle_m=\\sum_{i=0}^{m-1} g(x_i)\\,h(x_i)$. Let $p_n\\in\\mathcal{P}_n$ be any polynomial that minimizes the discrete sum of squares $\\sum_{i=0}^{m-1}\\big(p(x_i)-y_i\\big)^2$ over all $p\\in\\mathcal{P}_n$. This $p_n$ is the orthogonal projection of $f$ onto $\\mathcal{P}_n$ with respect to $\\langle\\cdot,\\cdot\\rangle_m$.\n\nDefine a validation grid $G$ consisting of $N_v=1001$ equispaced points on $[-1,1]$. The validation root mean square (RMS) error of $p_n$ with respect to $f$ is\n$$\nE_{\\text{val}}=\\sqrt{\\frac{1}{N_v}\\sum_{x\\in G}\\big(p_n(x)-f(x)\\big)^2}\\,.\n$$\nYou must write a complete, runnable program that, for each specified test case, computes $E_{\\text{val}}$ as defined above. No physical units are involved. All angles, if any arise, must be interpreted in radians. Round each $E_{\\text{val}}$ to $10$ decimal places using standard rounding.\n\nTest suite of parameter pairs $(m,n)$ to be used:\n- Case $1$: $(m,n)=(21,5)$.\n- Case $2$: $(m,n)=(21,20)$.\n- Case $3$: $(m,n)=(2,1)$.\n- Case $4$: $(m,n)=(41,20)$.\n- Case $5$: $(m,n)=(9,8)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above. For example, an acceptable format is $[e_1,e_2,e_3,e_4,e_5]$, where each $e_k$ is the rounded validation RMS error for Case $k$ as a decimal number.",
            "solution": "The problem requires the determination of a polynomial approximation $p_n(x)$ to a given function $f(x)$ through the method of discrete least squares, followed by an evaluation of this approximation's accuracy.\n\nThe function to be approximated is the Runge function, defined as $f(x) = \\dfrac{1}{1+25x^{2}}$ on the domain $x \\in [-1, 1]$. For each test case, we are provided with an integer $m \\geq 2$ specifying the number of sample points and an integer $n \\geq 0$ specifying the maximum polynomial degree, constrained by $n \\leq m-1$.\n\nThe $m$ sample points, denoted by $x_i$ for $i=0, 1, \\dots, m-1$, are stipulated to be equispaced within the interval $[-1, 1]$, such that $x_i = -1 + \\dfrac{2i}{m-1}$. The corresponding function values at these points are $y_i = f(x_i)$.\n\nWe are tasked with finding a polynomial $p_n(x)$ within the space $\\mathcal{P}_n$ of all polynomials of degree at most $n$. This polynomial is defined in the monomial basis as $p_n(x) = \\sum_{j=0}^{n} c_j x^j$. The coefficients, represented by the vector $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$, must be chosen to minimize the sum of the squared differences between the polynomial and the function at the sample points:\n$$\nS(\\mathbf{c}) = \\sum_{i=0}^{m-1} \\left( p_n(x_i) - y_i \\right)^2 = \\sum_{i=0}^{m-1} \\left( \\left( \\sum_{j=0}^{n} c_j x_i^j \\right) - y_i \\right)^2\n$$\nThis minimization problem is a classic linear least-squares problem. It can be formulated in matrix algebra by defining an $m \\times (n+1)$ Vandermonde matrix $\\mathbf{A}$ with entries $A_{ij} = x_i^j$ for $i=0, \\dots, m-1$ and $j=0, \\dots, n$. If we let $\\mathbf{y}$ be the $m \\times 1$ column vector of sampled values, $\\mathbf{y} = [y_0, y_1, \\dots, y_{m-1}]^T$, the objective is to minimize the squared Euclidean norm of the residual vector:\n$$\nS(\\mathbf{c}) = \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2\n$$\nThe unique vector of coefficients $\\mathbf{c}$ that minimizes this expression is the solution to the system of normal equations:\n$$\n\\mathbf{A}^T \\mathbf{A} \\mathbf{c} = \\mathbf{A}^T \\mathbf{y}\n$$\nThe constraint $n \\leq m-1$ implies that there are at least $n+1$ distinct sample points, which ensures that the columns of the matrix $\\mathbf{A}$ are linearly independent. This, in turn, guarantees that the Gram matrix $\\mathbf{A}^T\\mathbf{A}$ is symmetric and positive definite, and therefore invertible, ensuring a unique solution for $\\mathbf{c}$. While the normal equations provide a theoretical solution, direct computation can suffer from numerical instability due to the potential ill-conditioning of $\\mathbf{A}^T\\mathbf{A}$. It is computationally superior to employ methods based on matrix factorizations such as QR decomposition or Singular Value Decomposition (SVD) of $\\mathbf{A}$, which are implemented in high-quality numerical libraries.\n\nIn the specific instance where $n = m-1$, the number of coefficients to determine, $n+1$, is identical to the number of sample points, $m$. The matrix $\\mathbf{A}$ becomes a square, invertible matrix. The least-squares solution then corresponds to the exact solution of the linear system $\\mathbf{A}\\mathbf{c} = \\mathbf{y}$. This signifies that the polynomial $p_n(x)$ exactly interpolates the data points, satisfying $p_n(x_i) = y_i$ for all $i$.\n\nAfter computing the optimal coefficient vector $\\mathbf{c}$ for a given $(m,n)$ pair, the polynomial $p_n(x)$ is defined. Its accuracy is then assessed on a fine validation grid $G$, which consists of $N_v = 1001$ equispaced points on $[-1, 1]$. The validation root mean square (RMS) error is defined and calculated as:\n$$\nE_{\\text{val}} = \\sqrt{\\frac{1}{N_v} \\sum_{x \\in G} \\left( p_n(x) - f(x) \\right)^2}\n$$\nThe algorithm executed for each test case $(m, n)$ is therefore as follows:\n1.  Generate the $m$ sample points $x_i$ and compute the corresponding function values $y_i=f(x_i)$.\n2.  Solve the linear least-squares problem $\\min_{\\mathbf{c}} \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2$ to obtain the polynomial coefficients $\\mathbf{c}$, where $A_{ij} = x_i^j$. A numerically stable library function is used for this purpose.\n3.  Establish the validation grid $G$ with its $N_v=1001$ points.\n4.  Evaluate the determined polynomial $p_n(x)$ and the original function $f(x)$ at all points $x \\in G$.\n5.  Compute the RMS error $E_{\\text{val}}$ from these evaluations.\n6.  The result is rounded to $10$ decimal places.\n\nThis systematic procedure is applied to each of the specified test cases to produce the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the validation RMS error for polynomial least-squares approximations\n    of the Runge function for several test cases.\n    \"\"\"\n\n    def f(x: np.ndarray) - np.ndarray:\n        \"\"\"\n        The Runge function to be approximated.\n        f(x) = 1 / (1 + 25*x^2)\n        \"\"\"\n        return 1.0 / (1.0 + 25.0 * x**2)\n\n    # Test suite of parameter pairs (m, n)\n    # m: number of sample points\n    # n: degree of the polynomial\n    test_cases = [\n        (21, 5),\n        (21, 20),\n        (2, 1),\n        (41, 20),\n        (9, 8),\n    ]\n\n    results = []\n\n    # Define the validation grid G\n    N_v = 1001\n    x_val = np.linspace(-1.0, 1.0, N_v)\n    f_val = f(x_val)\n\n    for m, n in test_cases:\n        # Step 1: Generate m equispaced sample points and their function values.\n        x_samples = np.linspace(-1.0, 1.0, m)\n        y_samples = f(x_samples)\n\n        # Step 2: Find the polynomial p_n of degree n that best fits the\n        # (x_samples, y_samples) data in a least-squares sense.\n        # The numpy.polynomial.polynomial.polyfit function solves this by\n        # finding the coefficients c that minimize the squared error.\n        # The coefficients are returned for the basis 1, x, x^2, ..., x^n.\n        coeffs = np.polynomial.polynomial.polyfit(x_samples, y_samples, n)\n\n        # Step 3: Evaluate the obtained polynomial p_n on the validation grid.\n        p_n_val = np.polynomial.polynomial.polyval(x_val, coeffs)\n\n        # Step 4: Compute the validation root mean square (RMS) error.\n        squared_errors = (p_n_val - f_val)**2\n        mean_squared_error = np.mean(squared_errors)\n        rms_error = np.sqrt(mean_squared_error)\n\n        # Step 5: Round the result to 10 decimal places as specified.\n        rounded_error = round(rms_error, 10)\n        results.append(rounded_error)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The standard least-squares method implicitly assumes that every data point is equally reliable. However, in many real-world experiments, some measurements are more precise than others—a condition known as heteroscedasticity. This practice introduces you to Weighted Least Squares (WLS), a powerful extension that improves model accuracy by giving more weight to more reliable data. Through simulation, you will compare the performance of ordinary and weighted least squares and see why accounting for data quality is crucial. ",
            "id": "3152264",
            "problem": "Consider a linear model with a design matrix $A \\in \\mathbb{R}^{m \\times n}$, an unknown parameter vector $x \\in \\mathbb{R}^{n}$, and observed data $b \\in \\mathbb{R}^{m}$. The fundamental basis is the definition of the least-squares objective: ordinary least squares minimizes the sum of squared residuals, that is, it finds the minimizer of $f_{\\mathrm{OLS}}(x) = \\lVert A x - b \\rVert_{2}^{2}$. Generalized least squares minimizes a weighted sum of squared residuals with positive weights $\\{w_{i}\\}_{i=1}^{m}$ on each observation, that is, it finds the minimizer of $f_{\\mathrm{GLS}}(x) = \\sum_{i=1}^{m} w_{i} \\left( (A x - b)_{i} \\right)^{2}$, where the weights model heteroscedastic (non-constant) noise variance across observations. For numerical stability, solve each minimization problem using a numerically sound linear algebra routine rather than expanding any normal equations by hand.\n\nYour task is to simulate data under multiple scenarios, estimate $x$ using ordinary least squares (OLS) and generalized least squares (GLS; with a diagonal weight matrix implied by the vector of weights), and compare the solutions. Use a fixed random number generator seed to make the results reproducible.\n\nSet the true parameter vector to $x_{\\mathrm{true}} = [\\beta_{0}, \\beta_{1}, \\beta_{2}]^{\\top}$ with $\\beta_{0} = 1.0$, $\\beta_{1} = -2.0$, and $\\beta_{2} = 0.5$. For each test case below:\n- Construct the design matrix $A$ for a quadratic model using input locations $x_{i}$, that is, $A$ has columns $[1, x, x^{2}]$.\n- Generate noise $\\varepsilon_{i}$ with zero mean and specified variance to form $b$ as $b = A x_{\\mathrm{true}} + \\varepsilon$, where $\\varepsilon = (\\varepsilon_{1}, \\ldots, \\varepsilon_{m})^{\\top}$.\n- For OLS, compute $\\hat{x}_{\\mathrm{OLS}}$ by minimizing $f_{\\mathrm{OLS}}(x)$.\n- For GLS, compute $\\hat{x}_{\\mathrm{GLS}}$ by minimizing $f_{\\mathrm{GLS}}(x)$ using diagonal weights $w_{i}  0$ as specified in each case.\n- Use a fixed random number generator seed of $123456$ for all noise sampling.\n- Use tolerance $\\varepsilon_{\\mathrm{tol}} = 10^{-12}$ for equality checks.\n- When reporting float results, round to $6$ decimal places.\n\nTest Suite:\n- Test $1$ (homoscedastic baseline, equality check): Let $m = 50$ points $x_{i}$ spaced uniformly on the interval $[-2, 2]$. Let $\\sigma_{0} = 0.5$ and set $\\mathrm{Var}[\\varepsilon_{i}] = \\sigma_{0}^{2}$ for all $i$. Use weights $w_{i} = 1 / \\sigma_{0}^{2}$ for GLS. Compute a boolean indicating whether $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$.\n- Test $2$ (heteroscedastic with correctly specified weights, performance ratio): Let $m = 50$ points $x_{i}$ spaced uniformly on $[-2, 2]$. Let $\\sigma_{0} = 0.5$ and $\\gamma = 3.0$, and set $\\mathrm{Var}[\\varepsilon_{i}] = \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right)$. Use weights $w_{i} = 1 / \\left( \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right) \\right)$. Compute the float ratio $r_{2} = \\dfrac{\\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_{2}}{\\lVert \\hat{x}_{\\mathrm{GLS}} - x_{\\mathrm{true}} \\rVert_{2}}$, rounded to $6$ decimals.\n- Test $3$ (heteroscedastic with mis-specified weights, performance ratio): Use the same $m$, $x_{i}$, $\\sigma_{0}$, and $\\gamma$ as in Test $2$, with the same noise generation. Use deliberately inverted weights $w_{i}^{\\mathrm{wrong}} = 1 / \\left( \\sigma_{0}^{2} \\left( 1 + \\gamma (x_{\\max} - \\lvert x_{i} \\rvert) \\right) \\right)$, where $x_{\\max} = \\max_{i} \\lvert x_{i} \\rvert$, ensuring all weights are positive. Compute the float ratio $r_{3} = \\dfrac{\\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_{2}}{\\lVert \\hat{x}_{\\mathrm{GLS}}^{\\mathrm{wrong}} - x_{\\mathrm{true}} \\rVert_{2}}$, rounded to $6$ decimals.\n- Test $4$ (boundary case with square system, equality check): Let $m = 3$ points $x = [-1, 0, 1]$. Use $\\sigma_{0} = 0.5$ and $\\gamma = 3.0$ with $\\mathrm{Var}[\\varepsilon_{i}] = \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right)$, and weights $w_{i} = 1 / \\left( \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right) \\right)$. Compute a boolean indicating whether $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Tests $1$ through $4$: $[\\text{bool}, \\text{float}, \\text{float}, \\text{bool}]$. For example, an output might look like $[True, 1.234567, 0.987654, True]$. No additional text should be printed.",
            "solution": "The core task is to estimate a parameter vector $x \\in \\mathbb{R}^n$ from a linear model $b = Ax + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix, $b \\in \\mathbb{R}^m$ is the vector of observations, and $\\varepsilon \\in \\mathbb{R}^m$ is a vector of random errors.\n\nThe problem specifies a quadratic model, so the parameter vector is $x = [\\beta_0, \\beta_1, \\beta_2]^{\\top} \\in \\mathbb{R}^3$ ($n=3$) and the design matrix $A$ for a set of input locations $\\{x_i\\}_{i=1}^m$ is constructed such that its $i$-th row is $[1, x_i, x_i^2]$.\n\n**Ordinary Least Squares (OLS)**\n\nThe OLS method finds the parameter vector $\\hat{x}_{\\mathrm{OLS}}$ that minimizes the sum of squared residuals. The objective function is:\n$$ f_{\\mathrm{OLS}}(x) = \\lVert Ax - b \\rVert_2^2 $$\nThis is the standard approach when the errors $\\varepsilon_i$ are assumed to be independent and identically distributed with constant variance (homoscedasticity). A numerically stable way to find the minimizer is to use a QR decomposition or Singular Value Decomposition (SVD) of $A$, as implemented in `numpy.linalg.lstsq`.\n\n**Generalized Least Squares (GLS)**\n\nThe GLS method is an extension of OLS for cases where the errors are heteroscedastic (i.e., $\\mathrm{Var}[\\varepsilon_i]$ is not constant) or correlated. The problem specifies heteroscedastic, uncorrelated errors and provides positive weights $\\{w_i\\}_{i=1}^m$ for each observation. The GLS objective function is:\n$$ f_{\\mathrm{GLS}}(x) = \\sum_{i=1}^{m} w_i \\left( (Ax - b)_i \\right)^2 $$\nTo solve this, we can transform it into an equivalent OLS problem. Let $W$ be a diagonal matrix with $W_{ii} = w_i$. The objective function can be written in matrix form as:\n$$ f_{\\mathrm{GLS}}(x) = (Ax - b)^{\\top} W (Ax - b) $$\nLet $W^{1/2}$ be the diagonal matrix with entries $\\sqrt{w_i}$. Then $W = (W^{1/2})^{\\top} W^{1/2}$. The objective function becomes:\n$$ f_{\\mathrm{GLS}}(x) = (W^{1/2}(Ax - b))^{\\top} (W^{1/2}(Ax - b)) = \\lVert W^{1/2}Ax - W^{1/2}b \\rVert_2^2 $$\nBy defining a transformed design matrix $A' = W^{1/2}A$ and a transformed observation vector $b' = W^{1/2}b$, the GLS problem is equivalent to solving an OLS problem for these transformed variables:\n$$ \\hat{x}_{\\mathrm{GLS}} = \\underset{x}{\\mathrm{argmin}} \\lVert A'x - b' \\rVert_2^2 $$\nThis is solved using the same numerical methods as for OLS. The optimal weights, according to the Gauss-Markov theorem and its extensions, are the inverse of the error variances, $w_i = 1 / \\mathrm{Var}[\\varepsilon_i]$.\n\n**Test Case Analysis**\n\nA single random number generator is initialized with a fixed seed, `seed` = $123456$, to ensure reproducibility. The true parameter vector is $x_{\\mathrm{true}} = [1.0, -2.0, 0.5]^{\\top}$.\n\n**Test 1: Homoscedastic Baseline**\n- Data are generated with constant error variance, $\\mathrm{Var}[\\varepsilon_i] = \\sigma_0^2$. This is the ideal case for OLS.\n- The GLS weights are set to $w_i = 1/\\sigma_0^2$, a constant.\n- The GLS objective becomes $f_{\\mathrm{GLS}}(x) = (1/\\sigma_0^2) \\sum_i ((Ax)_i - b_i)^2 = (1/\\sigma_0^2) f_{\\mathrm{OLS}}(x)$.\n- Since $1/\\sigma_0^2$ is a positive constant, minimizing $f_{\\mathrm{GLS}}(x)$ is equivalent to minimizing $f_{\\mathrm{OLS}}(x)$.\n- Therefore, we expect $\\hat{x}_{\\mathrm{OLS}}$ and $\\hat{x}_{\\mathrm{GLS}}$ to be identical. The check $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$ should evaluate to true.\n\n**Test 2: Heteroscedasticity with Correct Weights**\n- Data are generated with non-constant variance, $\\mathrm{Var}[\\varepsilon_i] \\propto 1 + \\gamma |x_i|$.\n- GLS is applied with the theoretically optimal weights $w_i = 1/\\mathrm{Var}[\\varepsilon_i]$.\n- The Gauss-Markov theorem states that for a heteroscedastic model, the GLS estimator with correct weights is the Best Linear Unbiased Estimator (BLUE), meaning it has the minimum variance among all linear unbiased estimators.\n- OLS is still unbiased but no longer has minimum variance. We expect GLS to be more \"efficient,\" meaning its estimate should be closer to $x_{\\mathrm{true}}$ on average.\n- The ratio $r_2 = \\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_2 / \\lVert \\hat{x}_{\\mathrm{GLS}} - x_{\\mathrm{true}} \\rVert_2$ is a measure of the relative error. We expect $r_2  1.0$.\n\n**Test 3: Heteroscedasticity with Misspecified Weights**\n- The same heteroscedastic data from Test $2$ is used.\n- GLS is applied with \"wrong\" weights that are largest where the error variance is lowest, and smallest where the variance is highest. This misspecification runs counter to the principle of down-weighting uncertain observations.\n- Using incorrect weights can lead to an estimator that is less efficient than OLS.\n- We expect the error of this misspecified GLS, $\\lVert \\hat{x}_{\\mathrm{GLS}}^{\\mathrm{wrong}} - x_{\\mathrm{true}} \\rVert_2$, to be potentially larger than the error of OLS, which would result in the ratio $r_3 = \\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_2 / \\lVert \\hat{x}_{\\mathrm{GLS}}^{\\mathrm{wrong}} - x_{\\mathrm{true}} \\rVert_2$ being less than $1.0$.\n\n**Test 4: Square System Boundary Case**\n- Here, the number of observations $m$ equals the number of parameters $n$ ($m=n=3$).\n- The design matrix $A$ is a $3 \\times 3$ Vandermonde matrix for $3$ distinct points, which is invertible.\n- For an invertible square matrix $A$, the system $Ax=b$ has a unique solution $x = A^{-1}b$.\n- The OLS objective $\\lVert Ax-b \\rVert_2^2$ is minimized and equals zero at $\\hat{x}_{\\mathrm{OLS}} = A^{-1}b$.\n- The GLS objective $\\lVert W^{1/2}Ax - W^{1/2}b \\rVert_2^2$ is also minimized at zero. The transformed matrix $W^{1/2}A$ is also invertible, so the solution is unique: $\\hat{x}_{\\mathrm{GLS}} = (W^{1/2}A)^{-1}(W^{1/2}b) = A^{-1}(W^{1/2})^{-1}W^{1/2}b = A^{-1}b$.\n- In a just-determined system ($m=n$ with invertible $A$), both OLS and GLS yield the exact same solution, which perfectly fits the data, regardless of the weighting scheme.\n- The check $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$ should evaluate to true.\n\nThe implementation will follow these principles, using `numpy.linalg.lstsq` for solving the OLS and transformed GLS problems, and `numpy.random.default_rng` for reproducible noise generation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes OLS and GLS estimates for a quadratic model under different\n    noise scenarios and compares their performance.\n    \"\"\"\n    # Define common parameters for all test cases.\n    x_true = np.array([1.0, -2.0, 0.5])\n    seed = 123456\n    rng = np.random.default_rng(seed)\n    tol = 1e-12\n\n    results = []\n\n    # --- Test 1: Homoscedastic baseline, equality check ---\n    m1 = 50\n    x_coords1 = np.linspace(-2.0, 2.0, m1)\n    sigma0_1 = 0.5\n    \n    A1 = np.vander(x_coords1, 3, increasing=True)\n    \n    # Generate homoscedastic noise\n    variances1 = np.full(m1, sigma0_1**2)\n    std_devs1 = np.sqrt(variances1)\n    noise1 = rng.normal(0, std_devs1)\n    b1 = A1 @ x_true + noise1\n    \n    # OLS estimation\n    x_ols1, _, _, _ = np.linalg.lstsq(A1, b1, rcond=None)\n    \n    # GLS estimation with constant weights\n    weights1 = 1.0 / variances1\n    sqrt_w1 = np.sqrt(weights1)\n    A1_prime = A1 * sqrt_w1[:, np.newaxis]\n    b1_prime = b1 * sqrt_w1\n    x_gls1, _, _, _ = np.linalg.lstsq(A1_prime, b1_prime, rcond=None)\n    \n    result1 = np.linalg.norm(x_ols1 - x_gls1) = tol\n    results.append(result1)\n\n    # --- Test 2  3: Heteroscedastic cases ---\n    m2 = 50\n    x_coords2 = np.linspace(-2.0, 2.0, m2)\n    sigma0_2 = 0.5\n    gamma2 = 3.0\n    \n    A2 = np.vander(x_coords2, 3, increasing=True)\n    \n    # Generate heteroscedastic noise (used for both Test 2 and 3)\n    variances2 = sigma0_2**2 * (1.0 + gamma2 * np.abs(x_coords2))\n    std_devs2 = np.sqrt(variances2)\n    noise2 = rng.normal(0, std_devs2)\n    b2 = A2 @ x_true + noise2\n    \n    # OLS estimation (used for both Test 2 and 3)\n    x_ols2, _, _, _ = np.linalg.lstsq(A2, b2, rcond=None)\n    err_ols2 = np.linalg.norm(x_ols2 - x_true)\n\n    # --- Test 2: GLS with correctly specified weights ---\n    weights2_correct = 1.0 / variances2\n    sqrt_w2_correct = np.sqrt(weights2_correct)\n    A2_prime_correct = A2 * sqrt_w2_correct[:, np.newaxis]\n    b2_prime_correct = b2 * sqrt_w2_correct\n    x_gls2, _, _, _ = np.linalg.lstsq(A2_prime_correct, b2_prime_correct, rcond=None)\n    err_gls2 = np.linalg.norm(x_gls2 - x_true)\n    \n    result2 = round(err_ols2 / err_gls2, 6)\n    results.append(result2)\n\n    # --- Test 3: GLS with mis-specified weights ---\n    x_max3 = np.max(np.abs(x_coords2))\n    variances3_wrong_denom = sigma0_2**2 * (1.0 + gamma2 * (x_max3 - np.abs(x_coords2)))\n    weights3_wrong = 1.0 / variances3_wrong_denom\n    sqrt_w3_wrong = np.sqrt(weights3_wrong)\n    A3_prime_wrong = A2 * sqrt_w3_wrong[:, np.newaxis]\n    b3_prime_wrong = b2 * sqrt_w3_wrong\n    x_gls3_wrong, _, _, _ = np.linalg.lstsq(A3_prime_wrong, b3_prime_wrong, rcond=None)\n    err_gls3_wrong = np.linalg.norm(x_gls3_wrong - x_true)\n    \n    result3 = round(err_ols2 / err_gls3_wrong, 6)\n    results.append(result3)\n    \n    # --- Test 4: Boundary case with square system ---\n    m4 = 3\n    x_coords4 = np.array([-1.0, 0.0, 1.0])\n    sigma0_4 = 0.5\n    gamma4 = 3.0\n    \n    A4 = np.vander(x_coords4, 3, increasing=True)\n    \n    # Generate heteroscedastic noise\n    variances4 = sigma0_4**2 * (1.0 + gamma4 * np.abs(x_coords4))\n    std_devs4 = np.sqrt(variances4)\n    noise4 = rng.normal(0, std_devs4)\n    b4 = A4 @ x_true + noise4\n    \n    # OLS estimation\n    x_ols4, _, _, _ = np.linalg.lstsq(A4, b4, rcond=None)\n    \n    # GLS estimation\n    weights4 = 1.0 / variances4\n    sqrt_w4 = np.sqrt(weights4)\n    A4_prime = A4 * sqrt_w4[:, np.newaxis]\n    b4_prime = b4 * sqrt_w4\n    x_gls4, _, _, _ = np.linalg.lstsq(A4_prime, b4_prime, rcond=None)\n    \n    result4 = np.linalg.norm(x_ols4 - x_gls4) = tol\n    results.append(result4)\n\n    # Print results in the specified format\n    # The str() of a Python bool is 'True'/'False', which matches the example format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}