{
    "hands_on_practices": [
        {
            "introduction": "掌握任何计算方法的第一步都是从最基本的情况入手。本练习将带你手动解决一个简单的线性拟合问题。通过为一组数据点拟合一条直线 $y = \\alpha + \\beta t$，你将直接应用最小二乘法的核心思想，建立并求解“正规方程组”。这个过程不仅能让你计算出最佳拟合参数，更重要的是，它揭示了你在统计软件中一键获得的回归直线背后的数学原理。",
            "id": "1031896",
            "problem": "考虑数据点：$(t_1, y_1) = (1, 3)$、$(t_2, y_2) = (2, 1)$、$(t_3, y_3) = (3, 4)$、$(t_4, y_4) = (4, 6)$。使用最小二乘法，将线性模型 $y = \\alpha + \\beta t$ 拟合到此数据。计算斜率参数 $\\beta$ 的最小二乘估计值。",
            "solution": "为了求出 $\\beta$ 的最小二乘估计，我们求解通过最小化残差平方和 $S = \\sum_{i=1}^4 (y_i - \\alpha - \\beta t_i)^2$ 得到的正规方程组：\n$$\\frac{\\partial S}{\\partial \\alpha} = -2 \\sum_{i=1}^4 (y_i - \\alpha - \\beta t_i) = 0$$  \n$$\\frac{\\partial S}{\\partial \\beta} = -2 \\sum_{i=1}^4 t_i (y_i - \\alpha - \\beta t_i) = 0$$  \n这简化为以下方程组：  \n$$\\sum_{i=1}^4 y_i = n\\alpha + \\beta \\sum_{i=1}^4 t_i$$  \n$$\\sum_{i=1}^4 t_i y_i = \\alpha \\sum_{i=1}^4 t_i + \\beta \\sum_{i=1}^4 t_i^2$$  \n其中 $n = 4$。计算必要的总和：  \n$$\\sum_{i=1}^4 t_i = 1 + 2 + 3 + 4 = 10$$  \n$$\\sum_{i=1}^4 t_i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30$$  \n$$\\sum_{i=1}^4 y_i = 3 + 1 + 4 + 6 = 14$$  \n$$\\sum_{i=1}^4 t_i y_i = (1 \\cdot 3) + (2 \\cdot 1) + (3 \\cdot 4) + (4 \\cdot 6) = 3 + 2 + 12 + 24 = 41$$  \n代入正规方程组：  \n$$14 = 4\\alpha + 10\\beta \\quad (1)$$  \n$$41 = 10\\alpha + 30\\beta \\quad (2)$$  \n求解该方程组。将方程(1)乘以5：  \n$$70 = 20\\alpha + 50\\beta \\quad (1a)$$  \n将方程(2)乘以2：  \n$$82 = 20\\alpha + 60\\beta \\quad (2a)$$  \n将方程(2a)减去方程(1a)：  \n$$82 - 70 = (20\\alpha + 60\\beta) - (20\\alpha + 50\\beta)$$  \n$$12 = 10\\beta$$  \n$$\\beta = \\frac{12}{10} = \\frac{6}{5}$$",
            "answer": "$$\\boxed{\\dfrac{6}{5}}$$"
        },
        {
            "introduction": "线性模型虽然简单，但有时不足以捕捉数据的复杂趋势。一个自然的想法是使用更高阶的多项式进行拟合。然而，模型的灵活性是一把双刃剑。本练习将通过一个经典的计算实例——龙格现象（Runge's phenomenon），让你亲身体验高阶多项式拟合的陷阱。你将编写代码来探索，当一个模型过于“努力”地穿过每一个数据点时，它在数据点之间的表现可能会变得极不稳定，从而导致严重的过拟合问题。",
            "id": "2408214",
            "problem": "给定一个定义在 $[-1,1]$ 上的实值函数 $f:\\,[-1,1]\\to\\mathbb{R}$，其表达式为 $f(x)=\\dfrac{1}{1+25x^{2}}$。对于给定的正整数 $m\\geq 2$，定义等距样本点 $x_i=-1+\\dfrac{2i}{m-1}$，其中 $i=0,1,\\dots,m-1$。令 $y_i=f(x_i)$ 对所有 $i$ 成立。对于给定的非负整数 $n$（满足 $n\\leq m-1$），考虑一个最高次数为 $n$ 的多项式空间 $\\mathcal{P}_n=\\{p:\\,p(x)=\\sum_{j=0}^{n}c_j x^{j}\\}$。在函数 $g,h:[-1,1]\\to\\mathbb{R}$上定义离散内积为 $\\langle g,h\\rangle_m=\\sum_{i=0}^{m-1} g(x_i)\\,h(x_i)$。设 $p_n\\in\\mathcal{P}_n$ 是在所有 $p\\in\\mathcal{P}_n$ 中使离散平方和 $\\sum_{i=0}^{m-1}\\big(p(x_i)-y_i\\big)^2$ 最小化的任意多项式。这个 $p_n$ 是 $f$ 在 $\\mathcal{P}_n$ 上关于内积 $\\langle\\cdot,\\cdot\\rangle_m$ 的正交投影。\n\n定义一个验证网格 $G$，它由 $[-1,1]$ 上的 $N_v=1001$ 个等距点组成。$p_n$ 相对于 $f$ 的验证均方根（RMS）误差为\n$$\nE_{\\text{val}}=\\sqrt{\\frac{1}{N_v}\\sum_{x\\in G}\\big(p_n(x)-f(x)\\big)^2}\\,.\n$$\n您必须编写一个完整的、可运行的程序，对每个指定的测试用例计算如上定义的 $E_{\\text{val}}$。不涉及物理单位。如果出现任何角度，则必须以弧度为单位进行解释。使用标准四舍五入将每个 $E_{\\text{val}}$ 精确到小数点后 $10$ 位。\n\n要使用的参数对 $(m,n)$ 的测试套件：\n- 用例 $1$：$(m,n)=(21,5)$。\n- 用例 $2$：$(m,n)=(21,20)$。\n- 用例 $3$：$(m,n)=(2,1)$。\n- 用例 $4$：$(m,n)=(41,20)$。\n- 用例 $5$：$(m,n)=(9,8)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按上述用例的顺序排列。例如，一个可接受的格式是 $[e_1,e_2,e_3,e_4,e_5]$，其中每个 $e_k$ 是用例 $k$ 的四舍五入后的验证 RMS 误差，表示为十进制数。",
            "solution": "此问题要求通过离散最小二乘法确定给定函数 $f(x)$ 的多项式近似 $p_n(x)$，然后评估该近似的准确性。\n\n待近似的函数是龙格（Runge）函数，定义为 $f(x) = \\dfrac{1}{1+25x^{2}}$，定义域为 $x \\in [-1, 1]$。对于每个测试用例，我们都给定一个整数 $m \\geq 2$（指定样本点数）和一个整数 $n \\geq 0$（指定最大多项式次数），并受约束 $n \\leq m-1$。\n\n$m$ 个样本点（记为 $x_i$，其中 $i=0, 1, \\dots, m-1$）被规定为在区间 $[-1, 1]$ 内等距分布，使得 $x_i = -1 + \\dfrac{2i}{m-1}$。在这些点上对应的函数值为 $y_i = f(x_i)$。\n\n我们的任务是在所有最高次数为 $n$ 的多项式空间 $\\mathcal{P}_n$ 中找到一个多项式 $p_n(x)$。该多项式在单项式基下定义为 $p_n(x) = \\sum_{j=0}^{n} c_j x^j$。其系数（由向量 $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$ 表示）的选择必须使多项式与函数在样本点上的差的平方和最小化：\n$$\nS(\\mathbf{c}) = \\sum_{i=0}^{m-1} \\left( p_n(x_i) - y_i \\right)^2 = \\sum_{i=0}^{m-1} \\left( \\left( \\sum_{j=0}^{n} c_j x_i^j \\right) - y_i \\right)^2\n$$\n这个最小化问题是一个经典的线性最小二乘问题。它可以通过矩阵代数来表述，方法是定义一个 $m \\times (n+1)$ 的范德蒙（Vandermonde）矩阵 $\\mathbf{A}$，其元素为 $A_{ij} = x_i^j$（其中 $i=0, \\dots, m-1$ 且 $j=0, \\dots, n$）。如果我们令 $\\mathbf{y}$ 为采样值的 $m \\times 1$ 列向量，即 $\\mathbf{y} = [y_0, y_1, \\dots, y_{m-1}]^T$，那么目标就是最小化残差向量的欧几里得范数的平方：\n$$\nS(\\mathbf{c}) = \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2\n$$\n使该表达式最小化的唯一系数向量 $\\mathbf{c}$ 是正规方程组的解：\n$$\n\\mathbf{A}^T \\mathbf{A} \\mathbf{c} = \\mathbf{A}^T \\mathbf{y}\n$$\n约束 $n \\leq m-1$ 意味着至少有 $n+1$ 个不同的样本点，这确保了矩阵 $\\mathbf{A}$ 的列是线性无关的。这反过来又保证了格拉姆（Gram）矩阵 $\\mathbf{A}^T\\mathbf{A}$ 是对称正定的，因此是可逆的，从而确保了 $\\mathbf{c}$ 有唯一解。虽然正规方程提供了理论解，但由于 $\\mathbf{A}^T\\mathbf{A}$ 可能的病态性，直接计算可能会遭受数值不稳定性的影响。在计算上，采用基于矩阵分解的方法（如对 $\\mathbf{A}$ 进行 QR 分解或奇异值分解 (SVD)）更为优越，这些方法已在高质量的数值库中实现。\n\n在 $n = m-1$ 的特定情况下，待定系数的数量 $n+1$ 与样本点的数量 $m$ 相同。矩阵 $\\mathbf{A}$ 成为一个方形可逆矩阵。此时，最小二乘解对应于线性系统 $\\mathbf{A}\\mathbf{c} = \\mathbf{y}$ 的精确解。这表示多项式 $p_n(x)$ 精确地插值了数据点，满足对所有 $i$ 都有 $p_n(x_i) = y_i$。\n\n在计算出给定 $(m,n)$ 对的最佳系数向量 $\\mathbf{c}$ 后，多项式 $p_n(x)$ 就被定义了。然后在一个由 $[-1, 1]$ 上的 $N_v = 1001$ 个等距点组成的精细验证网格 $G$ 上评估其准确性。验证均方根（RMS）误差定义并计算如下：\n$$\nE_{\\text{val}} = \\sqrt{\\frac{1}{N_v} \\sum_{x \\in G} \\left( p_n(x) - f(x) \\right)^2}\n$$\n因此，为每个测试用例 $(m, n)$ 执行的算法如下：\n1.  生成 $m$ 个样本点 $x_i$ 并计算相应的函数值 $y_i=f(x_i)$。\n2.  求解线性最小二乘问题 $\\min_{\\mathbf{c}} \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2$ 以获得多项式系数 $\\mathbf{c}$，其中 $A_{ij} = x_i^j$。为此目的，使用一个数值稳定的库函数。\n3.  建立包含 $N_v=1001$ 个点的验证网格 $G$。\n4.  在 $G$ 中的所有点 $x$ 上，计算已确定的多项式 $p_n(x)$ 和原始函数 $f(x)$ 的值。\n5.  根据这些评估值计算 RMS 误差 $E_{\\text{val}}$。\n6.  将结果四舍五入到小数点后 $10$ 位。\n\n将此系统性步骤应用于每个指定的测试用例，以生成最终结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the validation RMS error for polynomial least-squares approximations\n    of the Runge function for several test cases.\n    \"\"\"\n\n    def f(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        The Runge function to be approximated.\n        f(x) = 1 / (1 + 25*x^2)\n        \"\"\"\n        return 1.0 / (1.0 + 25.0 * x**2)\n\n    # Test suite of parameter pairs (m, n)\n    # m: number of sample points\n    # n: degree of the polynomial\n    test_cases = [\n        (21, 5),\n        (21, 20),\n        (2, 1),\n        (41, 20),\n        (9, 8),\n    ]\n\n    results = []\n\n    # Define the validation grid G\n    N_v = 1001\n    x_val = np.linspace(-1.0, 1.0, N_v)\n    f_val = f(x_val)\n\n    for m, n in test_cases:\n        # Step 1: Generate m equispaced sample points and their function values.\n        x_samples = np.linspace(-1.0, 1.0, m)\n        y_samples = f(x_samples)\n\n        # Step 2: Find the polynomial p_n of degree n that best fits the\n        # (x_samples, y_samples) data in a least-squares sense.\n        # The numpy.polynomial.polynomial.polyfit function solves this by\n        # finding the coefficients c that minimize the squared error.\n        # The coefficients are returned for the basis 1, x, x^2, ..., x^n.\n        coeffs = np.polynomial.polynomial.polyfit(x_samples, y_samples, n)\n\n        # Step 3: Evaluate the obtained polynomial p_n on the validation grid.\n        p_n_val = np.polynomial.polynomial.polyval(x_val, coeffs)\n\n        # Step 4: Compute the validation root mean square (RMS) error.\n        squared_errors = (p_n_val - f_val)**2\n        mean_squared_error = np.mean(squared_errors)\n        rms_error = np.sqrt(mean_squared_error)\n\n        # Step 5: Round the result to 10 decimal places as specified.\n        rounded_error = round(rms_error, 10)\n        results.append(rounded_error)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "到目前为止，我们默认所有数据点的测量精度都相同。但在现实世界中，某些观测值可能比其他观测值更可靠，即数据的噪声方差不是恒定的（异方差性）。本练习将引导你超越普通最小二乘法（OLS）的范畴，进入广义最小二乘法（GLS）的世界。你将通过编程模拟生成具有异方差性的数据，并比较 OLS 和 GLS 的估计结果，从而深刻理解为什么为不同精度的观测值赋予不同“权重”是获得更准确模型参数的关键。",
            "id": "3152264",
            "problem": "考虑一个线性模型，其设计矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，未知参数向量为 $x \\in \\mathbb{R}^{n}$，观测数据为 $b \\in \\mathbb{R}^{m}$。其基本原理是最小二乘目标的定义：普通最小二乘法（ordinary least squares）最小化残差平方和，即找到 $f_{\\mathrm{OLS}}(x) = \\lVert A x - b \\rVert_{2}^{2}$ 的最小化子。广义最小二乘法（Generalized least squares）最小化加权残差平方和，其中每个观测值都具有正权重 $\\{w_{i}\\}_{i=1}^{m}$，即找到 $f_{\\mathrm{GLS}}(x) = \\sum_{i=1}^{m} w_{i} \\left( (A x - b)_{i} \\right)^{2}$ 的最小化子，其中的权重用于对观测值之间异方差（非恒定）的噪声方差进行建模。为保证数值稳定性，应使用数值稳健的线性代数程序来求解每个最小化问题，而不是手动展开任何正规方程。\n\n您的任务是模拟多种场景下的数据，使用普通最小二乘法（OLS）和广义最小二乘法（GLS；权重向量隐含一个对角权重矩阵）估计 $x$，并比较其解。使用固定的随机数生成器种子以使结果可复现。\n\n将真实参数向量设为 $x_{\\mathrm{true}} = [\\beta_{0}, \\beta_{1}, \\beta_{2}]^{\\top}$，其中 $\\beta_{0} = 1.0$，$\\beta_{1} = -2.0$，$\\beta_{2} = 0.5$。对于下方的每个测试用例：\n- 使用输入位置 $x_{i}$ 为二次模型构建设计矩阵 $A$，即 $A$ 的列为 $[1, x, x^{2}]$。\n- 生成均值为零、方差指定的噪声 $\\varepsilon_{i}$，以形成 $b$，其中 $b = A x_{\\mathrm{true}} + \\varepsilon$，且 $\\varepsilon = (\\varepsilon_{1}, \\ldots, \\varepsilon_{m})^{\\top}$。\n- 对于 OLS，通过最小化 $f_{\\mathrm{OLS}}(x)$ 来计算 $\\hat{x}_{\\mathrm{OLS}}$。\n- 对于 GLS，使用每个案例中指定的对角权重 $w_{i} > 0$ 来最小化 $f_{\\mathrm{GLS}}(x)$，从而计算 $\\hat{x}_{\\mathrm{GLS}}$。\n- 所有噪声采样均使用固定的随机数生成器种子 $123456$。\n- 相等性检查使用容差 $\\varepsilon_{\\mathrm{tol}} = 10^{-12}$。\n- 报告浮点数结果时，四舍五入到6位小数。\n\n测试套件：\n- 测试 1（同方差基线，相等性检查）：设 $m = 50$ 个点 $x_{i}$ 在区间 $[-2, 2]$ 上均匀分布。设 $\\sigma_{0} = 0.5$，并对所有 $i$ 设置 $\\mathrm{Var}[\\varepsilon_{i}] = \\sigma_{0}^{2}$。GLS 使用权重 $w_{i} = 1 / \\sigma_{0}^{2}$。计算一个布尔值，指示 $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$ 是否成立。\n- 测试 2（权重正确指定的异方差，性能比）：设 $m = 50$ 个点 $x_{i}$ 在 $[-2, 2]$ 上均匀分布。设 $\\sigma_{0} = 0.5$ 和 $\\gamma = 3.0$，并设置 $\\mathrm{Var}[\\varepsilon_{i}] = \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right)$。使用权重 $w_{i} = 1 / \\left( \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right) \\right)$。计算浮点数比率 $r_{2} = \\dfrac{\\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_{2}}{\\lVert \\hat{x}_{\\mathrm{GLS}} - x_{\\mathrm{true}} \\rVert_{2}}$，四舍五入到6位小数。\n- 测试 3（权重错误指定的异方差，性能比）：使用与测试 2 相同的 $m$、$x_{i}$、$\\sigma_{0}$ 和 $\\gamma$，以及相同的噪声生成方式。使用故意反转的权重 $w_{i}^{\\mathrm{wrong}} = 1 / \\left( \\sigma_{0}^{2} \\left( 1 + \\gamma (x_{\\max} - \\lvert x_{i} \\rvert) \\right) \\right)$，其中 $x_{\\max} = \\max_{i} \\lvert x_{i} \\rvert$，确保所有权重为正。计算浮点数比率 $r_{3} = \\dfrac{\\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_{2}}{\\lVert \\hat{x}_{\\mathrm{GLS}}^{\\mathrm{wrong}} - x_{\\mathrm{true}} \\rVert_{2}}$，四舍五入到6位小数。\n- 测试 4（方形系统的边界情况，相等性检查）：设 $m = 3$ 个点 $x = [-1, 0, 1]$。使用 $\\sigma_{0} = 0.5$ 和 $\\gamma = 3.0$，其中 $\\mathrm{Var}[\\varepsilon_{i}] = \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right)$，权重为 $w_{i} = 1 / \\left( \\sigma_{0}^{2} \\left( 1 + \\gamma \\lvert x_{i} \\rvert \\right) \\right)$。计算一个布尔值，指示 $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$ 是否成立。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试 1 到 4 的顺序排列结果：$[\\text{布尔值}, \\text{浮点数}, \\text{浮点数}, \\text{布尔值}]$。例如，输出可能看起来像 $[{\\mathrm{True}}, 1.234567, 0.987654, {\\mathrm{True}}]$。不应打印任何其他文本。",
            "solution": "用户提供的问题经评估是有效的。它基于线性回归的原理，具有科学依据，其每个部分都是适定的，有唯一且可计算的解，并且陈述客观，提供了所有必要信息。该问题是计算统计学中的一个标准练习，旨在阐明普通最小二乘法（OLS）和广义最小二乘法（GLS）估计量的性质和比较性能。\n\n核心任务是从线性模型 $b = Ax + \\varepsilon$ 中估计参数向量 $x \\in \\mathbb{R}^n$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是设计矩阵，$b \\in \\mathbb{R}^m$ 是观测向量，$\\varepsilon \\in \\mathbb{R}^m$ 是随机误差向量。\n\n问题指定了一个二次模型，因此参数向量为 $x = [\\beta_0, \\beta_1, \\beta_2]^{\\top} \\in \\mathbb{R}^3$ ($n=3$)，并且为一组输入位置 $\\{x_i\\}_{i=1}^m$ 构建的设计矩阵 $A$，其第 $i$ 行为 $[1, x_i, x_i^2]$。\n\n**普通最小二乘法 (OLS)**\n\nOLS 方法找到最小化残差平方和的参数向量 $\\hat{x}_{\\mathrm{OLS}}$。目标函数为：\n$$ f_{\\mathrm{OLS}}(x) = \\lVert Ax - b \\rVert_2^2 $$\n当误差 $\\varepsilon_i$ 被假定为独立同分布且具有恒定方差（同方差性）时，这是标准方法。找到最小化子的一种数值稳定的方法是使用 $A$ 的 QR 分解或奇异值分解 (SVD)，如 `numpy.linalg.lstsq` 中所实现的。\n\n**广义最小二乘法 (GLS)**\n\nGLS 方法是 OLS 的扩展，适用于误差是异方差的（即 $\\mathrm{Var}[\\varepsilon_i]$ 不是常数）或相关的​​情况。该问题指定了异方差、不相关的误差，并为每个观测值提供正权重 $\\{w_i\\}_{i=1}^m$。GLS 目标函数为：\n$$ f_{\\mathrm{GLS}}(x) = \\sum_{i=1}^{m} w_i \\left( (Ax - b)_i \\right)^2 $$\n为解决此问题，我们可以将其转换为等价的 OLS 问题。令 $W$ 是一个对角矩阵，其对角元素为 $W_{ii} = w_i$。目标函数可以写成矩阵形式：\n$$ f_{\\mathrm{GLS}}(x) = (Ax - b)^{\\top} W (Ax - b) $$\n令 $W^{1/2}$ 是对角元素为 $\\sqrt{w_i}$ 的对角矩阵。则 $W = (W^{1/2})^{\\top} W^{1/2}$。目标函数变为：\n$$ f_{\\mathrm{GLS}}(x) = (W^{1/2}(Ax - b))^{\\top} (W^{1/2}(Ax - b)) = \\lVert W^{1/2}Ax - W^{1/2}b \\rVert_2^2 $$\n通过定义变换后的设计矩阵 $A' = W^{1/2}A$ 和变换后的观测向量 $b' = W^{1/2}b$，GLS 问题等价于为这些变换后的变量求解一个 OLS 问题：\n$$ \\hat{x}_{\\mathrm{GLS}} = \\underset{x}{\\mathrm{argmin}} \\lVert A'x - b' \\rVert_2^2 $$\n这可以使用与 OLS 相同的数值方法来求解。根据高斯-马尔可夫定理及其扩展，最优权重是误差方差的倒数，即 $w_i = 1 / \\mathrm{Var}[\\varepsilon_i]$。\n\n**测试用例分析**\n\n使用固定的种子 `seed` = $123456$ 初始化一个随机数生成器，以确保可复现性。真实参数向量为 $x_{\\mathrm{true}} = [1.0, -2.0, 0.5]^{\\top}$。\n\n**测试 1：同方差基线**\n- 数据由恒定的误差方差 $\\mathrm{Var}[\\varepsilon_i] = \\sigma_0^2$ 生成。这是 OLS 的理想情况。\n- GLS 的权重设为常数 $w_i = 1/\\sigma_0^2$。\n- GLS 的目标函数变为 $f_{\\mathrm{GLS}}(x) = (1/\\sigma_0^2) \\sum_i ((Ax)_i - b_i)^2 = (1/\\sigma_0^2) f_{\\mathrm{OLS}}(x)$。\n- 由于 $1/\\sigma_0^2$ 是一个正常数，最小化 $f_{\\mathrm{GLS}}(x)$ 等价于最小化 $f_{\\mathrm{OLS}}(x)$。\n- 因此，我们预期 $\\hat{x}_{\\mathrm{OLS}}$ 和 $\\hat{x}_{\\mathrm{GLS}}$ 是相同的。检查 $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$ 的结果应为真。\n\n**测试 2：权重正确的异方差性**\n- 数据由非恒定方差 $\\mathrm{Var}[\\varepsilon_i] \\propto 1 + \\gamma |x_i|$ 生成。\n- GLS 应用了理论上的最优权重 $w_i = 1/\\mathrm{Var}[\\varepsilon_i]$。\n- 高斯-马尔可夫定理指出，对于异方差模型，使用正确权重的 GLS 估计量是最佳线性无偏估计量 (BLUE)，这意味着它在所有线性无偏估计量中具有最小方差。\n- OLS 仍然是无偏的，但不再具有最小方差。我们预期 GLS 会更“有效”，即其估计值平均而言应更接近 $x_{\\mathrm{true}}$。\n- 比率 $r_2 = \\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_2 / \\lVert \\hat{x}_{\\mathrm{GLS}} - x_{\\mathrm{true}} \\rVert_2$ 是相对误差的一种度量。我们预期 $r_2 > 1.0$。\n\n**测试 3：权重指定错误的异方差性**\n- 使用与测试 2 相同的异方差数据。\n- GLS 应用了“错误”的权重，这些权重在误差方差最低处最大，在方差最高处最小。这种错误的指定与对不确定观测值降权的原则背道而驰。\n- 使用不正确的权重可能导致估计量的效率低于 OLS。\n- 我们预期这种错误指定的 GLS 的误差 $\\lVert \\hat{x}_{\\mathrm{GLS}}^{\\mathrm{wrong}} - x_{\\mathrm{true}} \\rVert_2$ 可能大于 OLS 的误差，这将导致比率 $r_3 = \\lVert \\hat{x}_{\\mathrm{OLS}} - x_{\\mathrm{true}} \\rVert_2 / \\lVert \\hat{x}_{\\mathrm{GLS}}^{\\mathrm{wrong}} - x_{\\mathrm{true}} \\rVert_2$ 小于 1.0。\n\n**测试 4：方形系统边界情况**\n- 在此，观测数量 $m$ 等于参数数量 $n$ ($m=n=3$)。\n- 设计矩阵 $A$ 是一个针对 3 个不同点的 $3 \\times 3$ 范德蒙矩阵，是可逆的。\n- 对于可逆的方阵 $A$，系统 $Ax=b$ 有唯一解 $x = A^{-1}b$。\n- OLS 目标函数 $\\lVert Ax-b \\rVert_2^2$ 在 $\\hat{x}_{\\mathrm{OLS}} = A^{-1}b$ 处最小化并等于零。\n- GLS 目标函数 $\\lVert W^{1/2}Ax - W^{1/2}b \\rVert_2^2$ 也在零处最小化。变换后的矩阵 $W^{1/2}A$ 也是可逆的，因此解是唯一的：$\\hat{x}_{\\mathrm{GLS}} = (W^{1/2}A)^{-1}(W^{1/2}b) = A^{-1}(W^{1/2})^{-1}W^{1/2}b = A^{-1}b$。\n- 在一个恰定系统（$m=n$ 且 $A$ 可逆）中，无论加权方案如何，OLS 和 GLS 都会得出完全相同的解，该解能完美拟合数据。\n- 检查 $\\lVert \\hat{x}_{\\mathrm{OLS}} - \\hat{x}_{\\mathrm{GLS}} \\rVert_{2} \\le \\varepsilon_{\\mathrm{tol}}$ 的结果应为真。\n\n实现将遵循这些原则，使用 `numpy.linalg.lstsq` 求解 OLS 和变换后的 GLS 问题，并使用 `numpy.random.default_rng` 进行可复现的噪声生成。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes OLS and GLS estimates for a quadratic model under different\n    noise scenarios and compares their performance.\n    \"\"\"\n    # Define common parameters for all test cases.\n    x_true = np.array([1.0, -2.0, 0.5])\n    seed = 123456\n    rng = np.random.default_rng(seed)\n    tol = 1e-12\n\n    results = []\n\n    # --- Test 1: Homoscedastic baseline, equality check ---\n    m1 = 50\n    x_coords1 = np.linspace(-2.0, 2.0, m1)\n    sigma0_1 = 0.5\n    \n    A1 = np.vander(x_coords1, 3, increasing=True)\n    \n    # Generate homoscedastic noise\n    variances1 = np.full(m1, sigma0_1**2)\n    std_devs1 = np.sqrt(variances1)\n    noise1 = rng.normal(0, std_devs1)\n    b1 = A1 @ x_true + noise1\n    \n    # OLS estimation\n    x_ols1, _, _, _ = np.linalg.lstsq(A1, b1, rcond=None)\n    \n    # GLS estimation with constant weights\n    weights1 = 1.0 / variances1\n    sqrt_w1 = np.sqrt(weights1)\n    A1_prime = A1 * sqrt_w1[:, np.newaxis]\n    b1_prime = b1 * sqrt_w1\n    x_gls1, _, _, _ = np.linalg.lstsq(A1_prime, b1_prime, rcond=None)\n    \n    result1 = np.linalg.norm(x_ols1 - x_gls1) = tol\n    results.append(result1)\n\n    # --- Test 2  3: Heteroscedastic cases ---\n    m2 = 50\n    x_coords2 = np.linspace(-2.0, 2.0, m2)\n    sigma0_2 = 0.5\n    gamma2 = 3.0\n    \n    A2 = np.vander(x_coords2, 3, increasing=True)\n    \n    # Generate heteroscedastic noise (used for both Test 2 and 3)\n    variances2 = sigma0_2**2 * (1.0 + gamma2 * np.abs(x_coords2))\n    std_devs2 = np.sqrt(variances2)\n    noise2 = rng.normal(0, std_devs2)\n    b2 = A2 @ x_true + noise2\n    \n    # OLS estimation (used for both Test 2 and 3)\n    x_ols2, _, _, _ = np.linalg.lstsq(A2, b2, rcond=None)\n    err_ols2 = np.linalg.norm(x_ols2 - x_true)\n\n    # --- Test 2: GLS with correctly specified weights ---\n    weights2_correct = 1.0 / variances2\n    sqrt_w2_correct = np.sqrt(weights2_correct)\n    A2_prime_correct = A2 * sqrt_w2_correct[:, np.newaxis]\n    b2_prime_correct = b2 * sqrt_w2_correct\n    x_gls2, _, _, _ = np.linalg.lstsq(A2_prime_correct, b2_prime_correct, rcond=None)\n    err_gls2 = np.linalg.norm(x_gls2 - x_true)\n    \n    result2 = round(err_ols2 / err_gls2, 6)\n    results.append(result2)\n\n    # --- Test 3: GLS with mis-specified weights ---\n    x_max3 = np.max(np.abs(x_coords2))\n    variances3_wrong_denom = sigma0_2**2 * (1.0 + gamma2 * (x_max3 - np.abs(x_coords2)))\n    weights3_wrong = 1.0 / variances3_wrong_denom\n    sqrt_w3_wrong = np.sqrt(weights3_wrong)\n    A3_prime_wrong = A2 * sqrt_w3_wrong[:, np.newaxis]\n    b3_prime_wrong = b2 * sqrt_w3_wrong\n    x_gls3_wrong, _, _, _ = np.linalg.lstsq(A3_prime_wrong, b3_prime_wrong, rcond=None)\n    err_gls3_wrong = np.linalg.norm(x_gls3_wrong - x_true)\n    \n    result3 = round(err_ols2 / err_gls3_wrong, 6)\n    results.append(result3)\n    \n    # --- Test 4: Boundary case with square system ---\n    m4 = 3\n    x_coords4 = np.array([-1.0, 0.0, 1.0])\n    sigma0_4 = 0.5\n    gamma4 = 3.0\n    \n    A4 = np.vander(x_coords4, 3, increasing=True)\n    \n    # Generate heteroscedastic noise\n    variances4 = sigma0_4**2 * (1.0 + gamma4 * np.abs(x_coords4))\n    std_devs4 = np.sqrt(variances4)\n    noise4 = rng.normal(0, std_devs4)\n    b4 = A4 @ x_true + noise4\n    \n    # OLS estimation\n    x_ols4, _, _, _ = np.linalg.lstsq(A4, b4, rcond=None)\n    \n    # GLS estimation\n    weights4 = 1.0 / variances4\n    sqrt_w4 = np.sqrt(weights4)\n    A4_prime = A4 * sqrt_w4[:, np.newaxis]\n    b4_prime = b4 * sqrt_w4\n    x_gls4, _, _, _ = np.linalg.lstsq(A4_prime, b4_prime, rcond=None)\n    \n    result4 = np.linalg.norm(x_ols4 - x_gls4) = tol\n    results.append(result4)\n\n    # Print results in the specified format\n    # The str() of a Python bool is 'True'/'False', which matches the example format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}