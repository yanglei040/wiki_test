{
    "hands_on_practices": [
        {
            "introduction": "The Singular Value Decomposition (SVD) provides a profound way to factor any matrix $A$ into three simpler matrices: $A = U \\Sigma V^T$. Before exploring its applications, it's essential to become comfortable with this fundamental definition. This first exercise  is a direct check of your understanding, asking you to reconstruct a matrix from its given SVD components and reinforcing the core algebraic relationship.",
            "id": "21856",
            "problem": "The Singular Value Decomposition (SVD) is a fundamental factorization of a real matrix $A$ into the product of three other matrices. For a square matrix $A$ of size $n \\times n$, the decomposition takes the form:\n$$\nA = U \\Sigma V^T\n$$\nHere, $U$ and $V$ are $n \\times n$ orthogonal matrices (meaning $U^T U = I$ and $V^T V = I$, where $I$ is the identity matrix), and $\\Sigma$ is an $n \\times n$ diagonal matrix whose diagonal entries $\\sigma_i$ are non-negative and are called the singular values of $A$. By convention, the singular values are ordered such that $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0$.\n\nConsider a specific $2 \\times 2$ matrix $A$ whose SVD components are given by:\n$$\nU = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix}\n$$\n$$\nV = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nwhere $\\sigma_1$ and $\\sigma_2$ are positive real constants representing the singular values, with $\\sigma_1 > \\sigma_2$.\n\nYour task is to reconstruct the original matrix $A$ from its SVD components. Derive the entry of matrix $A$ located in the first row and second column, denoted as $A_{12}$.",
            "solution": "We have \n$$A \\;=\\; U\\,\\Sigma\\,V^T\\;=\\;U\\,\\Sigma\\,V$$\nsince $V^T=V$ for the given $V$.  Write out the matrices:\n$$U=\\frac{1}{\\sqrt2}\\begin{pmatrix}1 & -1\\\\1 & 1\\end{pmatrix}, \n\\quad\n\\Sigma=\\begin{pmatrix}\\sigma_1&0\\\\0&\\sigma_2\\end{pmatrix},\n\\quad\nV=\\begin{pmatrix}0&1\\\\1&0\\end{pmatrix}.$$\nFirst compute \n$$\\Sigma\\,V\n=\\begin{pmatrix}\\sigma_1&0\\\\0&\\sigma_2\\end{pmatrix}\n\\begin{pmatrix}0&1\\\\1&0\\end{pmatrix}\n=\\begin{pmatrix}\n\\sigma_1\\cdot0 + 0\\cdot1 & \\sigma_1\\cdot1 + 0\\cdot0\\\\\n0\\cdot0 + \\sigma_2\\cdot1 & 0\\cdot1 + \\sigma_2\\cdot0\n\\end{pmatrix}\n=\\begin{pmatrix}0&\\sigma_1\\\\\\sigma_2&0\\end{pmatrix}.$$\nNow multiply by $U$:\n$$A\n=U\\,(\\Sigma V)\n=\\frac{1}{\\sqrt2}\\begin{pmatrix}1 & -1\\\\1 & 1\\end{pmatrix}\n\\begin{pmatrix}0&\\sigma_1\\\\\\sigma_2&0\\end{pmatrix}.$$\nThe $(1,2)$-entry of this product is\n$$A_{12}\n=\\frac{1}{\\sqrt2}\\bigl(1\\cdot\\sigma_1 + (-1)\\cdot0\\bigr)\n=\\frac{\\sigma_1}{\\sqrt2}.$$",
            "answer": "$$\\boxed{\\frac{\\sigma_1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "Beyond mere algebra, SVD offers a powerful geometric interpretation: it reveals the directions in which a linear transformation causes the most stretching or compression. This exercise  challenges you to apply SVD to a shear transformationâ€”a mapping that is not a simple rotation or scaling. By calculating its singular values, you will quantify the principal stretches of this transformation, gaining a deeper intuition for how SVD deconstructs any linear map into its fundamental geometric actions.",
            "id": "2203388",
            "problem": "In linear algebra, a horizontal shear is a linear transformation that displaces each point $(x, y)$ in a plane to a new point $(x+ky, y)$, where $k$ is a constant known as the shear factor. This transformation can be represented by a $2 \\times 2$ matrix. Consider such a transformation represented by the matrix $A$:\n$$\nA = \\begin{pmatrix} 1 & k \\\\ 0 & 1 \\end{pmatrix}\n$$\nwhere the shear factor $k$ is a non-negative real number, i.e., $k \\geq 0$.\n\nThe singular values of a matrix describe how it stretches or shrinks space. Determine the two singular values of the matrix $A$. Present your answer as a single row matrix containing the two singular values, ordered from largest to smallest.",
            "solution": "The singular values of a matrix $A$, denoted by $\\sigma_i$, are defined as the square roots of the eigenvalues of the matrix $A^T A$. The procedure is to first compute $A^T A$, then find its eigenvalues, and finally take the square root of these eigenvalues.\n\nStep 1: Compute the matrix product $A^T A$.\nFirst, we find the transpose of $A$, which is $A^T$:\n$$\nA^T = \\begin{pmatrix} 1 & 0 \\\\ k & 1 \\end{pmatrix}\n$$\nNow, we multiply $A^T$ by $A$:\n$$\nA^T A = \\begin{pmatrix} 1 & 0 \\\\ k & 1 \\end{pmatrix} \\begin{pmatrix} 1 & k \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(0) & (1)(k) + (0)(1) \\\\ (k)(1) + (1)(0) & (k)(k) + (1)(1) \\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix} 1 & k \\\\ k & k^2 + 1 \\end{pmatrix}\n$$\nLet's call this matrix $M = A^T A$.\n\nStep 2: Find the eigenvalues of $M = A^T A$.\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(M - \\lambda I) = 0$, where $I$ is the identity matrix.\n$$\n\\det \\left( \\begin{pmatrix} 1 & k \\\\ k & k^2 + 1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = 0\n$$\n$$\n\\det \\begin{pmatrix} 1 - \\lambda & k \\\\ k & k^2 + 1 - \\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant, we get:\n$$\n(1 - \\lambda)(k^2 + 1 - \\lambda) - (k)(k) = 0\n$$\n$$\n(k^2 + 1 - \\lambda) - \\lambda(k^2 + 1 - \\lambda) - k^2 = 0\n$$\n$$\nk^2 + 1 - \\lambda - \\lambda k^2 - \\lambda + \\lambda^2 - k^2 = 0\n$$\nGrouping terms, we obtain a quadratic equation in $\\lambda$:\n$$\n\\lambda^2 - (k^2 + 2)\\lambda + 1 = 0\n$$\n\nStep 3: Solve the characteristic equation for the eigenvalues $\\lambda$.\nWe use the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=-(k^2+2)$, and $c=1$.\n$$\n\\lambda = \\frac{(k^2+2) \\pm \\sqrt{(-(k^2+2))^2 - 4(1)(1)}}{2(1)}\n$$\n$$\n\\lambda = \\frac{k^2+2 \\pm \\sqrt{k^4 + 4k^2 + 4 - 4}}{2}\n$$\n$$\n\\lambda = \\frac{k^2+2 \\pm \\sqrt{k^4 + 4k^2}}{2}\n$$\nWe can factor out $k^2$ from the term under the square root:\n$$\n\\lambda = \\frac{k^2+2 \\pm \\sqrt{k^2(k^2 + 4)}}{2}\n$$\nSince it is given that $k \\geq 0$, we have $\\sqrt{k^2} = k$. Thus:\n$$\n\\lambda = \\frac{k^2+2 \\pm k\\sqrt{k^2 + 4}}{2}\n$$\nThe two eigenvalues are:\n$$\n\\lambda_1 = \\frac{k^2+2 + k\\sqrt{k^2 + 4}}{2}\n$$\n$$\n\\lambda_2 = \\frac{k^2+2 - k\\sqrt{k^2 + 4}}{2}\n$$\n\nStep 4: Calculate the singular values.\nThe singular values $\\sigma_1$ and $\\sigma_2$ are the square roots of the eigenvalues $\\lambda_1$ and $\\lambda_2$.\n$$\n\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{\\frac{k^2+2 + k\\sqrt{k^2 + 4}}{2}}\n$$\n$$\n\\sigma_2 = \\sqrt{\\lambda_2} = \\sqrt{\\frac{k^2+2 - k\\sqrt{k^2 + 4}}{2}}\n$$\nSince $k\\sqrt{k^2+4}$ is a positive term, $\\lambda_1 > \\lambda_2$, which implies $\\sigma_1 > \\sigma_2$. The problem asks for the singular values ordered from largest to smallest.\n\nThe final answer is the row matrix containing $\\sigma_1$ and $\\sigma_2$ in that order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{\\frac{k^2+2+k\\sqrt{k^2+4}}{2}} & \\sqrt{\\frac{k^2+2-k\\sqrt{k^2+4}}{2}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In computational science, theoretical concepts are put to the test through implementation. Manually calculating the SVD is feasible only for small matrices; for real-world problems, we rely on numerical algorithms. This practice  guides you through the process of adapting the power method to find a matrix's largest singular value and its corresponding singular vectors, bridging the gap between abstract theory and practical, large-scale computation.",
            "id": "3275076",
            "problem": "You are to design and implement a program that approximates the dominant singular triplet of a real matrix using only the principle of the power method, starting from first principles and core definitions. Begin from the definition that for a real matrix $A \\in \\mathbb{R}^{m \\times n}$, the largest singular value $ \\sigma_1 $ is the operator norm induced by the Euclidean norm, that is, the supremum of $ \\lVert A x \\rVert_2 $ over unit vectors $ x \\in \\mathbb{R}^n $. Use only well-established facts about symmetric positive semidefinite matrices and the power method as your conceptual base.\n\nYour task is to derive and implement an iterative algorithm that:\n- Approximates the largest singular value $ \\sigma_1 $ and corresponding left and right singular vectors $ u_1 \\in \\mathbb{R}^m $ and $ v_1 \\in \\mathbb{R}^n $.\n- Uses only matrix-vector products with $ A $ and $ A^\\top $ and vector normalizations. You must not form $ A^\\top A $ explicitly.\n- Starts from a unit-norm initial vector $ v^{(0)} \\in \\mathbb{R}^n $ obtained by sampling independent standard normal entries with a specified pseudorandom seed, and then normalizing to unit length.\n- Employs the stopping criterion that the relative change in the singular value estimate is at most a given tolerance $ \\tau $, that is, stop when $ \\frac{\\lvert \\sigma^{(k+1)} - \\sigma^{(k)} \\rvert}{\\max(\\sigma^{(k+1)}, \\varepsilon)} \\le \\tau $, where $ \\varepsilon $ is a tiny positive constant you may set to a reasonable machine-scale value. Also impose a maximum number of iterations $ k_{\\max} $.\n- Robustly handles the case that $ A $ is the zero matrix, in which case $ \\sigma_1 = 0 $ and any singular vectors are acceptable; ensure your implementation avoids division by zero.\n\nFor each test case below, your program must:\n- Run the above algorithm to produce an approximation $ \\widehat{\\sigma}_1 $.\n- Compute a reference value $ \\sigma_1^{\\star} $ using a reliable numerical routine for singular values.\n- Report the absolute error $ \\lvert \\widehat{\\sigma}_1 - \\sigma_1^{\\star} \\rvert $.\n\nTest suite:\n- Use the following five matrices $ A^{(i)} $ with dimensions as indicated. For each, also use the specified tolerance $ \\tau $, maximum iterations $ k_{\\max} $, and pseudorandom seed $ s $ for the initial vector $ v^{(0)} $.\n1. $ A^{(1)} \\in \\mathbb{R}^{4 \\times 3} $ with entries\n   $ \\left[ \\left[ 3.0, 1.0, 1.0 \\right], \\left[ 0.0, 2.0, 0.0 \\right], \\left[ 0.0, 0.0, 1.0 \\right], \\left[ 1.0, 0.0, 0.5 \\right] \\right] $, with $ \\tau = 10^{-12} $, $ k_{\\max} = 1000 $, and $ s = 0 $.\n2. $ A^{(2)} \\in \\mathbb{R}^{3 \\times 2} $ with entries\n   $ \\left[ \\left[ 0.0, 0.0 \\right], \\left[ 0.0, 0.0 \\right], \\left[ 0.0, 0.0 \\right] \\right] $, with $ \\tau = 10^{-12} $, $ k_{\\max} = 1000 $, and $ s = 1 $.\n3. $ A^{(3)} \\in \\mathbb{R}^{4 \\times 3} $ with entries\n   $ \\left[ \\left[ 10.0, 0.0, 0.0 \\right], \\left[ 0.0, 0.0, 0.0 \\right], \\left[ 0.0, 0.0, 0.0 \\right], \\left[ 0.0, 0.0, 0.0 \\right] \\right] $, with $ \\tau = 10^{-12} $, $ k_{\\max} = 1000 $, and $ s = 2 $.\n4. $ A^{(4)} \\in \\mathbb{R}^{3 \\times 3} $ with entries\n   $ \\left[ \\left[ 5.0, 0.0, 0.0 \\right], \\left[ 0.0, 5.0, 0.0 \\right], \\left[ 0.0, 0.0, 5.0 \\right] \\right] $, with $ \\tau = 10^{-12} $, $ k_{\\max} = 1000 $, and $ s = 3 $.\n5. $ A^{(5)} \\in \\mathbb{R}^{5 \\times 4} $ with entries\n   $ \\left[ \\left[ 1.0, 0.0, 0.0, 0.0 \\right], \\left[ 0.0, 0.1, 0.0, 0.0 \\right], \\left[ 0.0, 0.0, 0.01, 0.0 \\right], \\left[ 0.0, 0.0, 0.0, 10^{-6} \\right], \\left[ 1.0, 1.0, 1.0, 1.0 \\right] \\right] $, with $ \\tau = 10^{-12} $, $ k_{\\max} = 2000 $, and $ s = 4 $.\n\nFinal output specification:\n- Your program must produce a single line containing the list of the absolute errors for $ \\sigma_1 $ in the order of the five test cases above.\n- Format: a single line string of the form $ \\left[ e_1, e_2, e_3, e_4, e_5 \\right] $ with no spaces, where each $ e_i $ is the absolute error formatted in scientific notation with $ 12 $ digits after the decimal point (for example, $ 1.234000000000e-06 $).",
            "solution": "The problem requires the design and implementation of an iterative algorithm to find the dominant singular triplet ($ \\sigma_1, u_1, v_1 $) of a real matrix $A \\in \\mathbb{R}^{m \\times n}$. The derivation must start from first principles, relying on the power method and properties of symmetric positive semidefinite matrices.\n\nThe singular value decomposition (SVD) of a matrix $A$ is given by $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative entries $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$ on its main diagonal. The columns of $U$ are the left singular vectors ($u_i$) and the columns of $V$ are the right singular vectors ($v_i$). They are related by the equations:\n$$ A v_i = \\sigma_i u_i $$\n$$ A^\\top u_i = \\sigma_i v_i $$\nfor $i = 1, \\dots, \\min(m, n)$.\n\nFrom these relationships, we can derive a connection to an eigenvalue problem. Consider the matrix $A^\\top A \\in \\mathbb{R}^{n \\times n}$. This matrix is symmetric and positive semidefinite. Its properties are central to our derivation. Applying $A^\\top$ to the first SVD equation gives:\n$$(A^\\top A) v_i = A^\\top (A v_i) = A^\\top (\\sigma_i u_i) = \\sigma_i (A^\\top u_i) = \\sigma_i (\\sigma_i v_i) = \\sigma_i^2 v_i$$\nThis shows that the right singular vectors $v_i$ of $A$ are the eigenvectors of $A^\\top A$, and the squared singular values $\\sigma_i^2$ are the corresponding eigenvalues. Since $A^\\top A$ is positive semidefinite, all its eigenvalues are non-negative, i.e., $\\sigma_i^2 \\ge 0$.\n\nThe dominant singular value, $\\sigma_1$, is the largest singular value. Consequently, its square, $\\sigma_1^2$, is the largest eigenvalue of the matrix $A^\\top A$. This suggests using the power method, an iterative algorithm designed to find the eigenvalue of largest magnitude (the dominant eigenvalue) and its corresponding eigenvector.\n\nThe power method, when applied to a matrix $B$, generates a sequence of vectors $x^{(k)}$ that converges to the eigenvector associated with the dominant eigenvalue. The standard iteration is:\n$$ x^{(k+1)} = \\frac{B x^{(k)}}{\\lVert B x^{(k)} \\rVert_2} $$\nStarting with a random initial unit-norm vector $x^{(0)}$, the sequence $x^{(k)}$ converges to the dominant eigenvector (assuming a unique dominant eigenvalue and a non-zero component of $x^{(0)}$ in its direction).\n\nTo find $\\sigma_1$ and $v_1$, we apply the power method to the matrix $B = A^\\top A$. The iteration would be:\n$$ v^{(k+1)} = \\frac{(A^\\top A) v^{(k)}}{\\lVert (A^\\top A) v^{(k)} \\rVert_2} $$\nAs the number of iterations $k \\to \\infty$, the vector $v^{(k)}$ converges to $v_1$.\n\nThe problem specifies that we must not explicitly form the matrix $A^\\top A$, which is computationally advantageous for large or sparse matrices. The matrix-vector product $(A^\\top A) v^{(k)}$ can be computed as a sequence of two products: first, $w^{(k)} = A v^{(k)}$, and then, $y^{(k)} = A^\\top w^{(k)}$.\n\nA more direct and numerically stable algorithm, which also provides an estimate for $u_1$ and $\\sigma_1$ in each step, is formulated by alternating applications of $A$ and $A^\\top$ with normalization. This method is as follows:\n\n1.  Initialize with a random unit-norm vector $v^{(0)} \\in \\mathbb{R}^n$.\n2.  For $k=0, 1, 2, \\dots$:\n    a. Compute an unnormalized left singular vector: $u_{temp}^{(k+1)} = A v^{(k)}$.\n    b. The norm of this vector is an estimate for the singular value: $\\sigma^{(k+1)} = \\lVert u_{temp}^{(k+1)} \\rVert_2$.\n    c. Normalize to obtain the estimated left singular vector: $u^{(k+1)} = u_{temp}^{(k+1)} / \\sigma^{(k+1)}$.\n    d. Compute an unnormalized right singular vector: $v_{temp}^{(k+1)} = A^\\top u^{(k+1)}$.\n    e. Normalize to obtain the next iterate for the right singular vector: $v^{(k+1)} = v_{temp}^{(k+1)} / \\lVert v_{temp}^{(k+1)} \\rVert_2$.\n\nLet us verify that the update for $v$ in this scheme is equivalent to a power iteration on $A^\\top A$. Substituting the expressions from steps (a-d) into (e):\n$$ v^{(k+1)} = \\frac{A^\\top u^{(k+1)}}{\\lVert A^\\top u^{(k+1)} \\rVert_2} = \\frac{A^\\top \\left( \\frac{A v^{(k)}}{\\lVert A v^{(k)} \\rVert_2} \\right) }{\\lVert A^\\top \\left( \\frac{A v^{(k)}}{\\lVert A v^{(k)} \\rVert_2} \\right) \\rVert_2} = \\frac{ (A^\\top A) v^{(k)} } { \\lVert (A^\\top A) v^{(k)} \\rVert_2 } $$\nThis is indeed the power method applied to $A^\\top A$. Therefore, as $k \\to \\infty$, $v^{(k)}$ converges to $v_1$. Consequently, $u_{temp}^{(k+1)} = A v^{(k)}$ converges to $A v_1 = \\sigma_1 u_1$. The estimate $\\sigma^{(k+1)} = \\lVert u_{temp}^{(k+1)} \\rVert_2$ converges to $\\lVert \\sigma_1 u_1 \\rVert_2 = \\sigma_1 \\lVert u_1 \\rVert_2 = \\sigma_1$. Finally, the estimate $u^{(k+1)}$ converges to $u_1$. This algorithm correctly approximates the dominant singular triplet ($\\sigma_1, u_1, v_1$).\n\nThe algorithm terminates based on two conditions:\n1.  The maximum number of iterations $k_{\\max}$ is reached.\n2.  The relative change in the singular value estimate is below a tolerance $\\tau$:\n    $$ \\frac{\\lvert \\sigma^{(k+1)} - \\sigma^{(k)} \\rvert}{\\max(\\sigma^{(k+1)}, \\varepsilon)} \\le \\tau $$\n    where $\\varepsilon$ is a small machine-scale constant to prevent division by zero when $\\sigma^{(k+1)}$ is close to zero.\n\nFor robustness, if at any step $\\sigma^{(k+1)} = \\lVert A v^{(k)} \\rVert_2$ is zero (or numerically indistinguishable from it), this implies that either $v^{(k)}$ is in the null space of $A$ or $A$ is the zero matrix. In this context, it signals that the largest singular value is zero. The algorithm should terminate and report an estimated value $\\widehat{\\sigma}_1 = 0$.\n\nThe implementation will follow this derived procedure. For each test case, it will:\n1.  Initialize a random unit vector $v$ of appropriate dimension using the given seed.\n2.  Iteratively update $v$, $u$, and $\\sigma$ as described above until a stopping criterion is met.\n3.  The final estimate $\\widehat{\\sigma}_1$ is the last computed value of $\\sigma^{(k)}$.\n4.  This estimate is compared against a reference value $\\sigma_1^\\star$ computed using a high-fidelity library function (`numpy.linalg.norm(A, 2)`), and the absolute error $\\lvert \\widehat{\\sigma}_1 - \\sigma_1^\\star \\rvert$ is reported.",
            "answer": "```python\nimport numpy as np\n\ndef _compute_dominant_sv(A, tau, k_max, seed):\n    \"\"\"\n    Approximates the dominant singular value of a matrix A using the power method.\n\n    Args:\n        A (np.ndarray): The input matrix.\n        tau (float): The tolerance for the stopping criterion.\n        k_max (int): The maximum number of iterations.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The estimated dominant singular value sigma_1.\n    \"\"\"\n    eps = np.finfo(A.dtype).eps\n    m, n = A.shape\n\n    if n == 0:\n        return 0.0\n\n    rng = np.random.default_rng(seed)\n    v = rng.standard_normal(size=n, dtype=A.dtype)\n    \n    norm_v = np.linalg.norm(v)\n    if norm_v < eps:\n        # Extremely unlikely event, but handle robustly.\n        v = np.ones(n, dtype=A.dtype)\n        norm_v = np.sqrt(n)\n    \n    v = v / norm_v\n    \n    sigma_prev = 0.0\n    sigma_curr = 0.0\n\n    for _ in range(k_max):\n        u_unnormalized = A @ v\n        \n        sigma_curr = np.linalg.norm(u_unnormalized)\n        \n        # Check for convergence\n        relative_change = abs(sigma_curr - sigma_prev) / max(sigma_curr, eps)\n        if relative_change <= tau:\n            break\n            \n        # Handle the case of a zero or numerically zero matrix\n        if sigma_curr < eps:\n            sigma_curr = 0.0\n            break\n            \n        u = u_unnormalized / sigma_curr\n        \n        v_unnormalized = A.T @ u\n        \n        norm_v_unnormalized = np.linalg.norm(v_unnormalized)\n        if norm_v_unnormalized < eps:\n            # Convergence may occur if u is in the nullspace of A.T\n            break\n\n        v = v_unnormalized / norm_v_unnormalized\n        sigma_prev = sigma_curr\n        \n    return sigma_curr\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[3.0, 1.0, 1.0], [0.0, 2.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.5]], dtype=float),\n            1e-12, 1000, 0\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]], dtype=float),\n            1e-12, 1000, 1\n        ),\n        (\n            np.array([[10.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], dtype=float),\n            1e-12, 1000, 2\n        ),\n        (\n            np.array([[5.0, 0.0, 0.0], [0.0, 5.0, 0.0], [0.0, 0.0, 5.0]], dtype=float),\n            1e-12, 1000, 3\n        ),\n        (\n            np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 0.1, 0.0, 0.0], [0.0, 0.0, 0.01, 0.0], [0.0, 0.0, 0.0, 1e-6], [1.0, 1.0, 1.0, 1.0]], dtype=float),\n            1e-12, 2000, 4\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        A, tau, k_max, seed = case\n        \n        # Approximate sigma_1 using the derived power method\n        sigma1_hat = _compute_dominant_sv(A, tau, k_max, seed)\n        \n        # Compute the reference sigma_1 using a reliable method\n        # np.linalg.norm(A, 2) computes the largest singular value (operator 2-norm)\n        sigma1_ref = np.linalg.norm(A, 2)\n        \n        # Calculate the absolute error\n        result = abs(sigma1_hat - sigma1_ref)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.12e}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}