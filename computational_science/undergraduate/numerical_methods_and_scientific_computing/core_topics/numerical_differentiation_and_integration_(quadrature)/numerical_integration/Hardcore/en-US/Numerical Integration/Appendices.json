{
    "hands_on_practices": [
        {
            "introduction": "Before using high-level integration routines, it is crucial to understand how they are built from fundamental principles. This exercise guides you through implementing the composite trapezoidal rule not from a pre-packaged formula, but from its core definition: approximating an integrand with a series of straight lines . By verifying the rule's exactness for linear functions and observing its predictable error convergence, you will build a concrete intuition for how quadrature rules work and how their accuracy is formally characterized.",
            "id": "3214972",
            "problem": "You are to write a complete, runnable program that constructs and studies the composite trapezoidal numerical integration rule from first principles, verifying exactness on linear functions and quantifying how error grows when a small quadratic curvature is added. The foundational base must be the definition of the Riemann integral and piecewise linear interpolation of the integrand on a uniform partition. On an interval $\\,[A,B]\\,$ partitioned into $\\,n\\,$ equal subintervals of width $\\,h = (B-A)/n\\,$, the composite trapezoidal rule is defined by constructing, on each subinterval $\\,[x_i, x_{i+1}]\\,$ with $\\,x_i = A + i h\\,$, the unique line that interpolates the integrand at the endpoints $\\,x_i\\,$ and $\\,x_{i+1}\\,$, integrating that line exactly over $\\,[x_i, x_{i+1}]\\,$, and summing these contributions over all subintervals. Your program must implement this definition directly and must not invoke any closed-form trapezoidal summation formula.\n\nThe tasks are as follows.\n\n1. Verify exactness on linear functions. For functions of the form $\\,f(x) = a x + b\\,$, exactness means that the composite trapezoidal rule returns the exact value of the integral $\\,\\int_A^B f(x)\\,dx\\,$ for any $\\,n \\ge 1\\,$. Use a tolerance of $\\,10^{-12}\\,$ on the absolute difference to decide exactness.\n\n2. Quantify error growth when a small quadratic curvature is added. Consider functions of the form $\\,f(x) = a x + b + \\varepsilon x^2\\,$ with $\\,\\varepsilon\\,$ small. Compute the absolute error $\\;E = \\left|\\text{composite\\_trapezoid}(f;[A,B],n) - \\int_A^B f(x)\\,dx\\right|\\;$ and study how $\\,E\\,$ depends on $\\,\\varepsilon\\,$, the interval length $\\,B-A\\,$, and the number of subintervals $\\,n\\,$.\n\nYour program must use a pseudorandom number generator seeded deterministically to ensure reproducibility. Initialize the generator with seed $\\,1729\\,$, and draw independent $\\,a\\,$ and $\\,b\\,$ uniformly from the interval $\\,[-3,3]\\,$ as specified below. All angles, if any appear, must be in radians; however, this problem does not involve physical units or angles.\n\nImplement the following test suite and produce outputs exactly as specified. For each integral, use the exact antiderivative for evaluation of $\\,\\int_A^B f(x)\\,dx\\,$:\n$$\n\\int_A^B \\big(a x + b + \\varepsilon x^2\\big)\\,dx \\;=\\; \\frac{a}{2}\\,(B^2 - A^2) \\;+\\; b\\,(B - A) \\;+\\; \\frac{\\varepsilon}{3}\\,(B^3 - A^3).\n$$\n\nTest suite:\n\n- Linear exactness tests (produce booleans):\n  - Case L1: $\\,f(x) = 0\\cdot x + 1\\,$ on $\\,[A,B]=[0,1]\\,$, check $\\,n \\in \\{1,5,13\\}\\,$.\n  - Case L2: draw $\\,a\\,$ and $\\,b\\,$ from the seeded generator, denote them $\\,a_1\\,$ and $\\,b_1\\,$, with $\\,a_1,b_1 \\sim \\mathcal{U}([-3,3])\\,$; set $\\,f(x) = a_1 x + b_1\\,$ on $\\,[A,B]=[0,1]\\,$, check $\\,n \\in \\{1,5,13\\}\\,$.\n  - Case L3: draw another independent pair $\\,a_2\\,$ and $\\,b_2\\,$ from the same generator; set $\\,f(x) = a_2 x + b_2\\,$ on $\\,[A,B]=[-2,3]\\,$, check $\\,n \\in \\{2,7,17\\}\\,$.\n\n- Curvature error tests (produce floats):\n  For all curvature tests, reuse the first random linear coefficients $\\,a_1\\,$ and $\\,b_1\\,$ from Case L2 and add curvature $\\,\\varepsilon x^2\\,$.\n  - Case C1: $\\,\\varepsilon = 10^{-3}\\,$, $\\,[A,B]=[0,1]\\,$, $\\,n=10\\,$. Output $\\,E\\,$ as a float.\n  - Case C2: same as Case C1 but with $\\,n=20\\,$. Output $\\,E\\,$ as a float.\n  - Case C3: output the ratio $\\,E_{\\text{C1}} / E_{\\text{C2}}\\,$ as a float.\n  - Case C4: $\\,\\varepsilon = 5\\cdot 10^{-3}\\,$, $\\,[A,B]=[0,1]\\,$, $\\,n=20\\,$. Output the ratio $\\,E(\\varepsilon=5\\cdot 10^{-3}) / E(\\varepsilon=10^{-3})\\,$ as a float.\n  - Case C5: $\\,\\varepsilon = 10^{-3}\\,$, $\\,[A,B]=[-1,1]\\,$, $\\,n=40\\,$. Output $\\,E\\,$ as a float.\n  - Case C6: $\\,\\varepsilon = 10^{-3}\\,$, $\\,[A,B]=[2,5]\\,$, $\\,n=30\\,$. Output $\\,E\\,$ as a float.\n  - Case C7 (edge case): $\\,\\varepsilon = 10^{-12}\\,$, $\\,[A,B]=[0,1]\\,$, $\\,n=10\\,$. Output $\\,E\\,$ as a float.\n\nProgram output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the three booleans from Cases L1–L3 followed by the seven floats from Cases C1–C7. For example, an output line with placeholders would look like\n$\\,\\big[\\text{bool\\_L1},\\text{bool\\_L2},\\text{bool\\_L3},E_{\\text{C1}},E_{\\text{C2}},\\text{ratio\\_C3},\\text{ratio\\_C4},E_{\\text{C5}},E_{\\text{C6}},E_{\\text{C7}}\\big]\\,$.\n\nAll computations must be performed in pure mathematical terms without any physical units. Ensure numerical results are deterministic by adhering strictly to the specified random seed and generation procedure.",
            "solution": "The problem requires the construction and analysis of the composite trapezoidal numerical integration rule from first principles. This entails implementing the rule not from its final convenient summation formula, but from its fundamental definition: approximating an integrand by a series of piecewise linear functions and integrating these approximations. The analysis will verify the rule's exactness for linear functions and quantify its error for functions with quadratic curvature.\n\nThe definite integral $\\int_A^B f(x) dx$ represents the area under the curve of the function $f(x)$ from $x=A$ to $x=B$. The core idea of numerical quadrature is to approximate this area by dividing the interval $[A,B]$ into a partition and replacing the potentially complex function $f(x)$ on each sub-domain with a simpler, easily integrable function.\n\nThe composite trapezoidal rule begins by partitioning the interval $[A,B]$ into $n$ subintervals of equal width, $h = (B-A)/n$. The partition points are given by $x_i = A + i h$ for $i = 0, 1, \\dots, n$. On each subinterval $[x_i, x_{i+1}]$, the function $f(x)$ is approximated by the unique straight line, let us call it $p_i(x)$, that passes through the endpoints of the curve on that subinterval, namely $(x_i, f(x_i))$ and $(x_{i+1}, f(x_{i+1}))$. This is the linear interpolant of $f(x)$ at the nodes $x_i$ and $x_{i+1}$.\n\nThe equation for this line $p_i(x)$ is:\n$$\np_i(x) = f(x_i) + \\frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}(x - x_i)\n$$\nThe integral of $f(x)$ over this subinterval is then approximated by the integral of $p_i(x)$:\n$$\n\\int_{x_i}^{x_{i+1}} f(x) dx \\approx \\int_{x_i}^{x_{i+1}} p_i(x) dx\n$$\nThe integral on the right is the area of the trapezoid with vertices $(x_i, 0)$, $(x_{i+1}, 0)$, $(x_{i+1}, f(x_{i+1}))$, and $(x_i, f(x_i))$. We can compute this integral directly:\n$$\n\\begin{aligned}\n\\int_{x_i}^{x_{i+1}} p_i(x) dx = \\int_{x_i}^{x_{i+1}} \\left( f(x_i) + \\frac{f(x_{i+1}) - f(x_i)}{h}(x - x_i) \\right) dx \\\\\n= \\left[ f(x_i)x + \\frac{f(x_{i+1}) - f(x_i)}{h} \\frac{(x-x_i)^2}{2} \\right]_{x_i}^{x_{i+1}} \\\\\n= \\left( f(x_i)x_{i+1} + \\frac{f(x_{i+1}) - f(x_i)}{h} \\frac{h^2}{2} \\right) - \\left( f(x_i)x_i + 0 \\right) \\\\\n= f(x_i)(x_{i+1} - x_i) + \\frac{f(x_{i+1}) - f(x_i)}{2}h \\\\\n= f(x_i)h + \\frac{h}{2}f(x_{i+1}) - \\frac{h}{2}f(x_i) \\\\\n= \\frac{h}{2}(f(x_i) + f(x_{i+1}))\n\\end{aligned}\n$$\nThis result is the well-known formula for the area of a trapezoid. The composite rule is formed by summing these areas over all $n$ subintervals:\n$$\n\\int_A^B f(x) dx \\approx \\sum_{i=0}^{n-1} \\frac{h}{2}(f(x_i) + f(x_{i+1}))\n$$\nThe implementation will follow this derivation by iterating through each subinterval, calculating the area of the corresponding trapezoid, and accumulating these areas into a total sum.\n\nA key property of the trapezoidal rule is its exactness for linear functions. If $f(x) = ax + b$, the function itself is a line. The linear interpolant $p_i(x)$ on any subinterval $[x_i, x_{i+1}]$ is defined by the two points $(x_i, f(x_i))$ and $(x_{i+1}, f(x_{i+1}))$. Since these points lie on the line $f(x)$, the interpolant $p_i(x)$ is identical to $f(x)$ over the entire subinterval. Consequently, the integral of the approximation, $\\int_{x_i}^{x_{i+1}} p_i(x) dx$, is exactly equal to the integral of the function, $\\int_{x_i}^{x_{i+1}} f(x) dx$. This holds for all subintervals, so the total sum is exact. This property is independent of the number of subintervals $n \\ge 1$.\n\nFor non-linear functions, the rule introduces an error. For a function with a small quadratic component, $f(x) = ax + b + \\varepsilon x^2$, the second derivative is constant: $f''(x) = 2\\varepsilon$. The local error on a single subinterval $[x_i, x_{i+1}]$ is given by the integral of the difference between the function and its linear interpolant, $f(x) - p_i(x)$. The error term for the trapezoidal rule on one interval of width $h$ is known to be $-\\frac{1}{12} f''(\\xi) h^3$ for some $\\xi \\in (x_i, x_{i+1})$. With $f''(x) = 2\\varepsilon$, this local error is $-\\frac{2\\varepsilon}{12}h^3 = -\\frac{\\varepsilon}{6}h^3$. The total error is the sum of these local errors over all $n$ intervals:\n$$\nE = \\sum_{i=0}^{n-1} \\left( -\\frac{\\varepsilon}{6}h^3 \\right) = n \\left( -\\frac{\\varepsilon}{6}h^3 \\right)\n$$\nSubstituting $h = (B-A)/n$, we obtain the global error:\n$$\nE = n \\left( -\\frac{\\varepsilon}{6}\\left(\\frac{B-A}{n}\\right)^3 \\right) = -\\frac{\\varepsilon(B-A)^3}{6n^2}\n$$\nThe absolute error is $|E| = \\frac{|\\varepsilon|(B-A)^3}{6n^2}$. This theoretical result predicts that the error is directly proportional to the magnitude of the curvature, $|\\varepsilon|$, and inversely proportional to the square of the number of subintervals, $n^2$. The test cases are designed to verify these relationships. For instance, doubling $n$ should reduce the error by a factor of $2^2=4$. Multiplying $\\varepsilon$ by a factor of $k$ should multiply the error by the same factor $k$.\n\nThe program will implement these principles, performing calculations for the specified test suite to produce the required boolean and floating-point results, respecting the deterministic setup through a seeded random number generator. The exact integral for verification is calculated using the provided antiderivative: $\\int_A^B (ax + b + \\varepsilon x^2)dx = \\frac{a}{2}(B^2 - A^2) + b(B-A) + \\frac{\\varepsilon}{3}(B^3 - A^3)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and studies the composite trapezoidal rule, verifying its\n    properties on linear and nearly-linear functions.\n    \"\"\"\n\n    def composite_trapezoid(f, A, B, n):\n        \"\"\"\n        Computes the definite integral of f from A to B using the composite\n        trapezoidal rule with n subintervals, implemented from first principles.\n        \"\"\"\n        h = (B - A) / n\n        total_integral = 0.0\n        for i in range(n):\n            x_i = A + i * h\n            x_i1 = A + (i + 1) * h\n            y_i = f(x_i)\n            y_i1 = f(x_i1)\n            # The integral of the unique line connecting (x_i, y_i) and (x_i1, y_i1)\n            # over [x_i, x_i1] is the area of a trapezoid.\n            sub_integral = h * (y_i + y_i1) / 2.0\n            total_integral += sub_integral\n        return total_integral\n\n    def exact_integral_eval(a, b, eps, A, B):\n        \"\"\"\n        Computes the exact integral of f(x) = ax + b + eps*x^2 from A to B.\n        \"\"\"\n        term_a = a / 2.0 * (B**2 - A**2)\n        term_b = b * (B - A)\n        term_eps = eps / 3.0 * (B**3 - A**3)\n        return term_a + term_b + term_eps\n\n    results = []\n    TOL = 1e-12\n    SEED = 1729\n    rng = np.random.default_rng(seed=SEED)\n\n    # --- Linear exactness tests ---\n    # Case L1\n    a, b, eps = 0.0, 1.0, 0.0\n    A, B = 0.0, 1.0\n    ns_L1 = [1, 5, 13]\n    f_L1 = lambda x: a * x + b\n    exact_L1 = exact_integral_eval(a, b, eps, A, B)\n    is_exact_L1 = all(abs(composite_trapezoid(f_L1, A, B, n) - exact_L1)  TOL for n in ns_L1)\n    results.append(is_exact_L1)\n\n    # Case L2\n    a1, b1 = rng.uniform(-3, 3, 2)\n    eps = 0.0\n    A, B = 0.0, 1.0\n    ns_L2 = [1, 5, 13]\n    f_L2 = lambda x: a1 * x + b1\n    exact_L2 = exact_integral_eval(a1, b1, eps, A, B)\n    is_exact_L2 = all(abs(composite_trapezoid(f_L2, A, B, n) - exact_L2)  TOL for n in ns_L2)\n    results.append(is_exact_L2)\n\n    # Case L3\n    a2, b2 = rng.uniform(-3, 3, 2)\n    eps = 0.0\n    A, B = -2.0, 3.0\n    ns_L3 = [2, 7, 17]\n    f_L3 = lambda x: a2 * x + b2\n    exact_L3 = exact_integral_eval(a2, b2, eps, A, B)\n    is_exact_L3 = all(abs(composite_trapezoid(f_L3, A, B, n) - exact_L3)  TOL for n in ns_L3)\n    results.append(is_exact_L3)\n\n    # --- Curvature error tests ---\n    # Constants for curvature tests reuse a1, b1 from L2.\n    a, b = a1, b1\n\n    # Case C1\n    eps_C1 = 1e-3\n    A_C1, B_C1 = 0.0, 1.0\n    n_C1 = 10\n    f_C1 = lambda x: a * x + b + eps_C1 * x**2\n    num_C1 = composite_trapezoid(f_C1, A_C1, B_C1, n_C1)\n    exact_C1 = exact_integral_eval(a, b, eps_C1, A_C1, B_C1)\n    E_C1 = abs(num_C1 - exact_C1)\n    results.append(E_C1)\n    \n    # Case C2\n    eps_C2 = 1e-3\n    A_C2, B_C2 = 0.0, 1.0\n    n_C2 = 20\n    f_C2 = lambda x: a * x + b + eps_C2 * x**2\n    num_C2 = composite_trapezoid(f_C2, A_C2, B_C2, n_C2)\n    exact_C2 = exact_integral_eval(a, b, eps_C2, A_C2, B_C2)\n    E_C2 = abs(num_C2 - exact_C2)\n    results.append(E_C2)\n    \n    # Case C3\n    ratio_C3 = E_C1 / E_C2\n    results.append(ratio_C3)\n    \n    # Case C4\n    eps_C4 = 5e-3\n    A_C4, B_C4 = 0.0, 1.0\n    n_C4 = 20\n    f_C4 = lambda x: a * x + b + eps_C4 * x**2\n    num_C4 = composite_trapezoid(f_C4, A_C4, B_C4, n_C4)\n    exact_C4 = exact_integral_eval(a, b, eps_C4, A_C4, B_C4)\n    E_C4 = abs(num_C4 - exact_C4)\n    ratio_C4 = E_C4 / E_C2 # Compare with C2 error (same A,B,n, different eps)\n    results.append(ratio_C4)\n\n    # Case C5\n    eps_C5 = 1e-3\n    A_C5, B_C5 = -1.0, 1.0\n    n_C5 = 40\n    f_C5 = lambda x: a * x + b + eps_C5 * x**2\n    num_C5 = composite_trapezoid(f_C5, A_C5, B_C5, n_C5)\n    exact_C5 = exact_integral_eval(a, b, eps_C5, A_C5, B_C5)\n    E_C5 = abs(num_C5 - exact_C5)\n    results.append(E_C5)\n\n    # Case C6\n    eps_C6 = 1e-3\n    A_C6, B_C6 = 2.0, 5.0\n    n_C6 = 30\n    f_C6 = lambda x: a * x + b + eps_C6 * x**2\n    num_C6 = composite_trapezoid(f_C6, A_C6, B_C6, n_C6)\n    exact_C6 = exact_integral_eval(a, b, eps_C6, A_C6, B_C6)\n    E_C6 = abs(num_C6 - exact_C6)\n    results.append(E_C6)\n    \n    # Case C7\n    eps_C7 = 1e-12\n    A_C7, B_C7 = 0.0, 1.0\n    n_C7 = 10\n    f_C7 = lambda x: a * x + b + eps_C7 * x**2\n    num_C7 = composite_trapezoid(f_C7, A_C7, B_C7, n_C7)\n    exact_C7 = exact_integral_eval(a, b, eps_C7, A_C7, B_C7)\n    E_C7 = abs(num_C7 - exact_C7)\n    results.append(E_C7)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Uniformly spaced points are simple but inefficient, especially for functions with sharp, localized features where high resolution is only needed in a small region. This practice introduces adaptive quadrature, a powerful technique that makes an integrator \"smart\" by automatically concentrating computational effort where the function changes most rapidly . You will implement an adaptive algorithm based on Simpson's rule and witness firsthand how it can achieve superior accuracy with far fewer function evaluations than a fixed-grid approach.",
            "id": "3214881",
            "problem": "Consider the numerical evaluation of the definite integral of the function $f(x) = \\exp\\left(-100(x - 0.3)^2\\right)$ on the interval $[0,1]$. The task is to implement two composite numerical integration methods based on the Simpson rule and to compare their accuracy and computational effort:\n\n1. A uniform composite Simpson method over a partition of $[0,1]$ into $N$ equal subintervals, where $N$ is an even integer. If an odd $N$ is supplied, the implementation must increase it by $1$ so that the rule is applicable.\n\n2. An adaptive composite Simpson method with local error control, which recursively subdivides an interval until a specified local absolute tolerance $\\text{tol}$ is met. Local error control must be enforced by comparing the Simpson approximation on an interval with the sum of Simpson approximations on its two halves, and by distributing the tolerance across subintervals in a manner that ensures a globally controlled absolute error.\n\nThe implementation should build from the following fundamental base:\n\n- Polynomial interpolation: approximate a sufficiently smooth function on an interval by a quadratic interpolant through three points and integrate the interpolant exactly.\n- Error behavior of composite polynomial-based quadrature: for sufficiently smooth $f$, the Simpson rule has a truncation error that scales with the fifth power of the interval length for a single panel and yields a fourth-order global convergence rate when applied uniformly.\n\nCompute the true value of the integral using the known antiderivative of a Gaussian function, expressed in terms of the error function, and use it as the ground truth for error calculation.\n\nFor each method, report:\n- The absolute error with respect to the true integral.\n- The number of function evaluations used.\n\nThe program must implement both methods and run the following test suite, where each test case is a pair $(\\text{tol}, N)$:\n\n- Test case 1 (happy path): $\\text{tol} = 10^{-6}$, $N = 100$.\n- Test case 2 (tighter tolerance, finer grid): $\\text{tol} = 10^{-8}$, $N = 1000$.\n- Test case 3 (looser tolerance, coarse grid): $\\text{tol} = 10^{-4}$, $N = 10$.\n- Test case 4 (edge case with minimal uniform panels): $\\text{tol} = 10^{-10}$, $N = 2$.\n- Test case 5 (very strict tolerance): $\\text{tol} = 10^{-12}$, $N = 200$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must itself be a list of four values in the following order: $[\\text{abs\\_error\\_adaptive}, \\text{evals\\_adaptive}, \\text{abs\\_error\\_uniform}, \\text{evals\\_uniform}]$. For example, an output for two cases would have the form $[[e_1,c_1,E_1,C_1],[e_2,c_2,E_2,C_2]]$.\n\nNo physical units are involved in this problem, and no angles or percentages are required. The final output values must be real numbers or integers, as appropriate, without any additional text.",
            "solution": "The task is to evaluate the definite integral $I = \\int_0^1 f(x) \\,dx$ for $f(x) = \\exp\\left(-100(x - 0.3)^2\\right)$ using two variants of the composite Simpson's rule and compare their efficiency.\n\n**1. True Value of the Integral**\n\nThe integral of a Gaussian function can be expressed in terms of the error function, $\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-t^2} \\,dt$.\nLet $I = \\int_0^1 \\exp\\left(-100(x - 0.3)^2\\right) \\,dx$.\nWe perform a change of variables. Let $u = \\sqrt{100}(x - 0.3) = 10(x - 0.3)$. This implies $du = 10 \\,dx$, or $dx = \\frac{1}{10} \\,du$.\nThe limits of integration for $u$ are:\n- When $x = 0$, $u = 10(0 - 0.3) = -3$.\n- When $x = 1$, $u = 10(1 - 0.3) = 7$.\n\nThe integral becomes:\n$$ I = \\int_{-3}^{7} e^{-u^2} \\frac{1}{10} \\,du = \\frac{1}{10} \\int_{-3}^{7} e^{-u^2} \\,du $$\nUsing the definition of the error function:\n$$ \\int_a^b e^{-t^2} \\,dt = \\int_0^b e^{-t^2} \\,dt - \\int_0^a e^{-t^2} \\,dt = \\frac{\\sqrt{\\pi}}{2} \\left[ \\text{erf}(b) - \\text{erf}(a) \\right] $$\nWith $a = -3$ and $b = 7$, we get:\n$$ I = \\frac{1}{10} \\left[ \\frac{\\sqrt{\\pi}}{2} (\\text{erf}(7) - \\text{erf}(-3)) \\right] $$\nUsing the property $\\text{erf}(-z) = -\\text{erf}(z)$, the true value is:\n$$ I_{true} = \\frac{\\sqrt{\\pi}}{20} \\left[ \\text{erf}(7) + \\text{erf}(3) \\right] $$\nThis value will serve as our ground truth for calculating the absolute error of the numerical methods.\n\n**2. Uniform Composite Simpson's Rule**\n\nSimpson's rule approximates the integral of a function $f(x)$ over an interval $[a, b]$ by integrating a quadratic interpolant through the points $(a, f(a))$, $((a+b)/2, f((a+b)/2))$, and $(b, f(b))$. The formula is:\n$$ S(a, b) = \\frac{b-a}{6} \\left[ f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b) \\right] $$\nThe composite Simpson's rule applies this formula over a partition of the integration interval. For a uniform partition of $[a, b]$ into $N$ subintervals of equal width $h = (b-a)/N$ (where $N$ must be an even integer), the points are $x_i = a + ih$ for $i = 0, 1, \\dots, N$. The integral is approximated by:\n$$ S_N = \\frac{h}{3} \\left[ f(x_0) + 4\\sum_{i=1, i \\text{ odd}}^{N-1} f(x_i) + 2\\sum_{i=2, i \\text{ even}}^{N-2} f(x_i) + f(x_N) \\right] $$\nThe number of function evaluations for this method is $N+1$. As per the problem, if an odd $N$ is given, it is incremented to $N+1$, making the number of evaluations $N+2$.\n\n**3. Adaptive Composite Simpson's Rule**\n\nAdaptive quadrature methods adjust the density of evaluation points based on the local behavior of the integrand. The function $f(x)$ has a sharp peak around $x=0.3$ and is nearly zero elsewhere, making it a prime candidate for an adaptive approach. The method works as follows:\n\nFor an interval $[a, b]$, we compute two approximations:\n- A \"coarse\" approximation, $S_1$, using a single Simpson panel over $[a, b]$.\n- A \"fine\" approximation, $S_2$, by summing two Simpson panels over the half-intervals $[a, m]$ and $[m, b]$, where $m = (a+b)/2$.\n\nThe error of the single-panel Simpson rule on an interval of width $h=b-a$ is approximately $E_1 \\approx -\\frac{h^5}{2880} f^{(4)}(\\xi)$. The error of the two-panel approximation is $E_2 \\approx 2 \\times \\left(-\\frac{(h/2)^5}{2880} f^{(4)}(\\eta)\\right) \\approx \\frac{1}{16} E_1$.\nThe true integral $I$ can be written as $I = S_1 + E_1$ and $I = S_2 + E_2$.\nAssuming $f^{(4)}$ is nearly constant over $[a, b]$, we can establish a relationship between the approximations and the error of the finer one, $E_2$:\n$$ S_2 - S_1 \\approx E_1 - E_2 \\approx 16 E_2 - E_2 = 15 E_2 $$\nThus, an estimate for the local error in $S_2$ is $\\text{err} \\approx \\frac{|S_2 - S_1|}{15}$.\n\nThe adaptive algorithm is recursive:\n1.  For a given interval $[a, b]$ and local tolerance $\\text{tol}_{local}$, compute $S_1$ and $S_2$.\n2.  Calculate the error estimate $\\text{err} = \\frac{|S_2 - S_1|}{15}$.\n3.  If $\\text{err}  \\text{tol}_{local}$, the interval is sufficiently resolved. We accept the result and, to improve accuracy, return the Richardson-extrapolated value $I_{[a,b]} = S_2 + \\frac{S_2 - S_1}{15}$. This is a more accurate estimate of the integral over the interval.\n4.  If $\\text{err} \\ge \\text{tol}_{local}$, the interval is not resolved. Subdivide it into $[a, m]$ and $[m, b]$. Recursively call the algorithm on each sub-interval, but with the tolerance halved for each: $\\text{tol}_{local}/2$. The total integral is the sum of the results from the two recursive calls.\n\nTo ensure global error control, the tolerance is distributed proportionally to the interval widths. The initial call is made with the global tolerance $\\text{tol}$. Each recursive step on a half-interval uses half the tolerance of its parent.\n\nTo avoid redundant computations, function values at endpoints and midpoints are passed down through the recursion. A recursive step on $[a, b]$ with known $f(a)$, $f((a+b)/2)$, and $f(b)$ only needs to compute two new values: $f((a+m)/2)$ and $f((m+b)/2)$. The total number of function evaluations is tracked throughout the recursion.\n\nThis adaptive strategy concentrates computational effort on the region around the peak of the Gaussian ($x \\approx 0.3$), where the function changes rapidly, and uses very few evaluations in the flat regions near $x=0$ and $x=1$, leading to high efficiency.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and compares uniform and adaptive Simpson's rule for integrating\n    a Gaussian function, then runs a suite of test cases.\n    \"\"\"\n    \n    # 1. Define the function to integrate\n    def f(x):\n        return np.exp(-100.0 * (x - 0.3)**2)\n\n    # 2. Calculate the \"true\" value of the integral using the error function\n    # I = integral from 0 to 1 of exp(-100*(x-0.3)^2) dx\n    # Let u = 10*(x-0.3), so du = 10*dx.\n    # Limits: x=0 - u=-3; x=1 - u=7.\n    # I = (1/10) * integral from -3 to 7 of exp(-u^2) du\n    # Using erf(z) = (2/sqrt(pi)) * integral from 0 to z of exp(-t^2) dt\n    # Integral from a to b = sqrt(pi)/2 * (erf(b) - erf(a))\n    # I = (1/10) * sqrt(pi)/2 * (erf(7) - erf(-3))\n    #   = (sqrt(pi)/20) * (erf(7) + erf(3))\n    true_value = (math.sqrt(math.pi) / 20.0) * (math.erf(7.0) + math.erf(3.0))\n\n    # 3. Uniform Composite Simpson's Rule Implementation\n    def uniform_simpson(func, a, b, N):\n        \"\"\"\n        Calculates the definite integral using the uniform composite Simpson's rule.\n        \n        Args:\n            func: The function to integrate.\n            a, b: The interval of integration.\n            N: The number of subintervals (must be even, adjusted if not).\n            \n        Returns:\n            A tuple (integral_value, num_evaluations).\n        \"\"\"\n        if N % 2 != 0:\n            N += 1\n        \n        h = (b - a) / float(N)\n        x = np.linspace(a, b, N + 1)\n        y = func(x)\n        \n        # Simpson's formula: h/3 * (y0 + 4y1 + 2y2 + ... + 4y(N-1) + yN)\n        # Slicing: y[1:-1:2] gets odd indices, y[2:-2:2] gets even indices.\n        integral = (h / 3.0) * (y[0] + 4.0 * np.sum(y[1:-1:2]) + 2.0 * np.sum(y[2:-2:2]) + y[-1])\n        \n        evals = N + 1\n        return integral, evals\n\n    # 4. Adaptive Composite Simpson's Rule Implementation\n    def adaptive_simpson(func, a, b, tol):\n        \"\"\"\n        Wrapper for the recursive adaptive Simpson's rule integrator.\n        \n        Args:\n            func: The function to integrate.\n            a, b: The interval of integration.\n            tol: The desired absolute error tolerance.\n            \n        Returns:\n            A tuple (integral_value, num_evaluations).\n        \"\"\"\n        \n        eval_count = [0] # Use a list as a mutable counter\n\n        def f_counted(x):\n            eval_count[0] += 1\n            return func(x)\n        \n        # Initial function evaluations\n        fa = f_counted(a)\n        fm = f_counted((a + b) / 2.0)\n        fb = f_counted(b)\n        \n        integral = _adaptive_simpson_recursive(f_counted, a, b, tol, fa, fm, fb)\n        \n        return integral, eval_count[0]\n\n    def _adaptive_simpson_recursive(func, a, b, tol, fa, fm, fb):\n        \"\"\"Recursive helper for adaptive Simpson's rule.\"\"\"\n        h = b - a\n        m = (a + b) / 2.0\n        \n        # Coarse approximation (1 panel)\n        s1 = (h / 6.0) * (fa + 4.0 * fm + fb)\n        \n        # Finer approximation (2 panels) requires 2 new function evaluations\n        ml = (a + m) / 2.0\n        mr = (m + b) / 2.0\n        fml = func(ml)\n        fmr = func(mr)\n        \n        s2 = (h / 12.0) * (fa + 4.0 * fml + 2.0 * fm + 4.0 * fmr + fb)\n        \n        # Error estimation: |(S2 - S1) / 15| is an estimate of the error in S2.\n        error_estimate = abs(s2 - s1) / 15.0\n        \n        if error_estimate  tol:\n            # Richardson extrapolation for a more accurate result\n            return s2 + (s2 - s1) / 15.0\n        else:\n            # Subdivide and recurse, distributing tolerance\n            left_integral = _adaptive_simpson_recursive(func, a, m, tol / 2.0, fa, fml, fm)\n            right_integral = _adaptive_simpson_recursive(func, m, b, tol / 2.0, fm, fmr, fb)\n            return left_integral + right_integral\n\n    # 5. Define test cases\n    test_cases = [\n        (1e-6, 100),\n        (1e-8, 1000),\n        (1e-4, 10),\n        (1e-10, 2),\n        (1e-12, 200),\n    ]\n\n    all_results = []\n\n    # 6. Run the test suite\n    for tol, N in test_cases:\n        # Run adaptive method\n        res_adaptive, evals_adaptive = adaptive_simpson(f, 0.0, 1.0, tol)\n        error_adaptive = abs(res_adaptive - true_value)\n\n        # Run uniform method\n        res_uniform, evals_uniform = uniform_simpson(f, 0.0, 1.0, N)\n        error_uniform = abs(res_uniform - true_value)\n        \n        # Store results for this test case\n        case_result = [error_adaptive, evals_adaptive, error_uniform, evals_uniform]\n        all_results.append(case_result)\n\n    # 7. Format and print the final output\n    # The output format must be exact: [[v1,v2,v3,v4],[v5,v6,v7,v8]] with no spaces.\n    # repr() gives the correct bracket/comma structure, and .replace removes spaces.\n    output_str = repr(all_results).replace(' ', '')\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In numerical computing, discretization error is not the only adversary; the finite precision of computer arithmetic can introduce subtle but devastating rounding errors. This exercise tackles the challenge of integrating highly oscillatory functions, where the sum involves many positive and negative terms that are large in magnitude but nearly cancel out . You will explore the phenomenon of catastrophic cancellation and implement the Kahan compensated summation algorithm, a classic technique for preserving accuracy in such ill-conditioned sums.",
            "id": "3258506",
            "problem": "You are to study numerical integration of the highly oscillatory integrand given by the function $f(x) = \\sin(e^x)$ on bounded intervals, and to analyze the catastrophic cancellation that arises when a standard composite quadrature rule is implemented with naive summation in finite precision arithmetic. The objective is to design a program that implements and contrasts summation strategies for a fixed quadrature rule, quantifies cancellation, and reports empirical error behavior across a small test suite.\n\nBegin from the following fundamental base:\n- The Riemann integral of a continuous function $f$ over a bounded interval $[a,b]$ is defined as the limit of Riemann sums as the partition mesh size tends to $0$. A standard way to construct a practical numerical approximation is to partition $[a,b]$ into $N$ uniform subintervals of width $h = (b-a)/N$, evaluate $f$ at nodes $x_i = a + i h$ for $i = 0,1,\\dots,N$, and combine these samples with weights to form a consistent approximation to the integral $\\int_a^b f(x)\\,dx$.\n- The composite trapezoidal rule follows from approximating $f$ by a piecewise-linear interpolant over each subinterval, then integrating that interpolant exactly on each subinterval and summing the contributions. This rule is consistent with the Riemann integral and converges for continuous $f$ as $N \\to \\infty$ for fixed $[a,b]$.\n- Finite precision floating-point arithmetic as standardized by the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) models each basic arithmetic operation as $fl(x \\circ y) = (x \\circ y)(1+\\delta)$ with $|\\delta| \\le u$, where $\\circ$ is any basic operation, $u$ is the unit roundoff (half of one unit in the last place), and $fl(\\cdot)$ denotes the floating-point result. When summing a long sequence of numbers with large alternating signs, the exact sum can be much smaller in magnitude than the sum of absolute values of the terms, leading to catastrophic cancellation: small relative errors in the terms can accumulate and be amplified in the final result.\n\nTasks to perform and report:\n1. Implement the composite trapezoidal rule for $f(x) = \\sin(e^x)$ over a given interval $[a,b]$ with a specified number of uniform subintervals $N$. Represent the result as a sum of elementary contributions $c_i$ that correspond to the weighted nodal values so that $\\sum_{i=0}^N c_i$ equals the trapezoidal rule approximation. Derive the contribution structure starting from the Riemann integral definition and the piecewise-linear interpolation argument, without using any pre-stated quadrature formula in your implementation design.\n2. Implement two distinct summation strategies to evaluate $\\sum_{i=0}^N c_i$:\n   - A naive left-to-right summation that accumulates into a single floating-point variable.\n   - A compensated summation using the Kahan algorithm, which augments the running sum with a compensation variable to capture low-order bits lost to rounding. The Kahan algorithm is defined by a simple recurrence that uses only basic floating-point operations and is designed to mitigate loss of significance.\n3. Quantify cancellation by computing the cancellation index $C$ defined by\n   $$ C = \\frac{\\sum_{i=0}^N |c_i|}{\\left|\\sum_{i=0}^N c_i\\right|}. $$\n   Large values of $C$ indicate severe cancellation and a high potential amplification of rounding error relative to the magnitude of the result.\n4. For empirical error assessment, construct a denser partition using a refinement factor $r \\in \\mathbb{N}$ and compute a reference value $I_{\\mathrm{ref}}$ by applying the same composite trapezoidal rule on the refined mesh with the Kahan compensated summation. Use this $I_{\\mathrm{ref}}$ as an empirical proxy for the unknown exact integral. For each test case, report the absolute errors\n   $$ e_{\\mathrm{naive}} = \\left| I_{\\mathrm{naive}} - I_{\\mathrm{ref}} \\right|, \\quad e_{\\mathrm{Kahan}} = \\left| I_{\\mathrm{Kahan}} - I_{\\mathrm{ref}} \\right|, $$\n   together with the improvement ratio\n   $$ R = \\frac{e_{\\mathrm{naive}}}{\\max\\{e_{\\mathrm{Kahan}}, \\varepsilon\\}}, $$\n   where $\\varepsilon$ is a small positive safeguard constant to avoid division by zero.\n5. Your implementation must use angles in radians. There are no physical units to report.\n\nTest suite:\nYou must apply your program to the following four test cases, each specified by an interval $[a,b]$ and an integer $N$, together with a fixed refinement factor $r = 8$ for the empirical reference:\n- Case A (moderate oscillation): $a = 0$, $b = 1$, $N = 4000$.\n- Case B (strong oscillation on the right): $a = 0$, $b = 5$, $N = 40000$.\n- Case C (mixed growth and oscillation): $a = -2$, $b = 5$, $N = 60000$.\n- Case D (high oscillation, narrow window): $a = 5$, $b = 7$, $N = 80000$.\n\nFor each case, compute:\n- $I_{\\mathrm{naive}}$: the composite trapezoidal rule result using naive summation.\n- $I_{\\mathrm{Kahan}}$: the composite trapezoidal rule result using Kahan compensated summation.\n- $C$: the cancellation index as defined above.\n- $e_{\\mathrm{naive}}$ and $e_{\\mathrm{Kahan}}$ with refinement factor $r = 8$.\n- $R = e_{\\mathrm{naive}} / \\max\\{e_{\\mathrm{Kahan}}, \\varepsilon\\}$ with a chosen $\\varepsilon = 10^{-30}$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of four lists, one per test case, in the order A, B, C, D. Each inner list must contain six floating-point numbers in the order\n$[I_{\\mathrm{naive}}, I_{\\mathrm{Kahan}}, C, e_{\\mathrm{naive}}, e_{\\mathrm{Kahan}}, R]$,\nso that the overall output looks like\n$[[\\dots],[\\dots],[\\dots],[\\dots]]$\nwith commas separating entries and brackets denoting lists. No other text should be printed.",
            "solution": "The objective is to analyze the numerical integration of the highly oscillatory function $f(x) = \\sin(e^x)$ over a bounded interval $[a,b]$. This analysis will focus on the phenomenon of catastrophic cancellation that occurs when using standard quadrature rules with finite-precision arithmetic. We will implement the composite trapezoidal rule and contrast a naive summation method with the Kahan compensated summation algorithm to demonstrate and quantify the loss of precision.\n\nFirst, we derive the composite trapezoidal rule and identify the structure of the sum. The integral $I = \\int_a^b f(x) \\, dx$ is approximated by partitioning the interval $[a,b]$ into $N$ uniform subintervals of width $h = (b-a)/N$. The nodes are $x_i = a + i h$ for $i = 0, 1, \\dots, N$. The integral is the sum of integrals over these subintervals:\n$$ I = \\sum_{i=0}^{N-1} \\int_{x_i}^{x_{i+1}} f(x) \\, dx $$\nOn each subinterval $[x_i, x_{i+1}]$, we approximate $f(x)$ by a straight line connecting the points $(x_i, f(x_i))$ and $(x_{i+1}, f(x_{i+1}))$. The integral of this linear function is the area of a trapezoid:\n$$ \\int_{x_i}^{x_{i+1}} f(x) \\, dx \\approx \\frac{h}{2} (f(x_i) + f(x_{i+1})) $$\nSumming these approximations for all subintervals gives the composite trapezoidal rule approximation, $I_N$:\n$$ I_N = \\sum_{i=0}^{N-1} \\frac{h}{2} (f(x_i) + f(x_{i+1})) $$\nBy regrouping the terms, we can express this as a weighted sum of function evaluations at the nodes:\n$$ I_N = \\frac{h}{2}f(x_0) + h\\sum_{i=1}^{N-1} f(x_i) + \\frac{h}{2}f(x_N) $$\nFollowing the problem's requirement to express this as a sum $\\sum_{i=0}^{N} c_i$ of $N+1$ elementary contributions, we define the terms $c_i$ as the weighted nodal values:\n$$ c_i = \\begin{cases} \\frac{h}{2} f(x_0)  i=0 \\\\ h f(x_i)  1 \\le i \\le N-1 \\\\ \\frac{h}{2} f(x_N)  i=N \\end{cases} $$\nThus, the task is to compute $I_N = \\sum_{i=0}^{N} c_i$ using two different summation algorithms.\n\nThe first method is naive left-to-right summation. In floating-point arithmetic, this is performed by initializing a sum $S=0$ and iteratively adding each term: $S_{k+1} = fl(S_k + c_k)$. When summing a long sequence of values with alternating signs, the running sum $S_k$ can become much larger in magnitude than the individual terms $c_k$ or the final sum. When a small number is added to a large number, the lower-order bits of the small number are lost due to the finite mantissa length. This loss of information is known as rounding error. For an ill-conditioned sum where the true result is close to zero, this accumulated error can dominate the result, a phenomenon called catastrophic cancellation.\n\nThe second method is Kahan compensated summation. This algorithm is designed to mitigate the accumulation of rounding errors. It maintains a running compensation variable, $c_{\\text{comp}}$, that accumulates the \"lost\" parts of each addition. The algorithm proceeds as follows for a sequence of inputs:\nInitialize sum $S = 0.0$ and compensation $c_{\\text{comp}} = 0.0$.\nFor each term $c_i$:\n$y = c_i - c_{\\text{comp}}$\n$t = S + y$\n$c_{\\text{comp}} = (t - S) - y$\n$S = t$\nHere, $y$ is the current term corrected by the error from the previous step. The next sum is $t = S+y$. The crucial step is the calculation of the new compensation: $(t-S)$ represents the part of $y$ that was actually added to $S$, so $(t-S)-y$ is the negative of the part of $y$ that was lost in the addition. This lost part is stored in $c_{\\text{comp}}$ and carried over to the next iteration.\n\nTo quantify the degree of cancellation, we compute the cancellation index $C$:\n$$ C = \\frac{\\sum_{i=0}^N |c_i|}{\\left|\\sum_{i=0}^N c_i\\right|} $$\nA large value of $C$ ($C \\gg 1$) indicates that the sum of the absolute values of the terms is much larger than the magnitude of the final sum. This signifies that the terms largely cancel each other, making the sum susceptible to significant relative error from rounding. The calculation of the denominator $|\\sum c_i|$ will use the more accurate Kahan sum.\n\nFinally, we assess the empirical error of both summation methods. Since the exact value of the integral is unknown, we compute a highly accurate reference value, $I_{\\text{ref}}$. This reference is calculated using the same composite trapezoidal rule but on a much finer grid with $N_{\\text{ref}} = r \\times N$ subintervals, where $r = 8$ is the refinement factor. To minimize rounding error in the reference value itself, $I_{\\text{ref}}$ is computed using Kahan summation.\nThe absolute errors for the naive and Kahan summations on the original grid (with $N$ subintervals) are then:\n$$ e_{\\mathrm{naive}} = \\left| I_{\\mathrm{naive}} - I_{\\mathrm{ref}} \\right| $$\n$$ e_{\\mathrm{Kahan}} = \\left| I_{\\mathrm{Kahan}} - I_{\\mathrm{ref}} \\right| $$\nThe error $e_{\\mathrm{Kahan}}$ is expected to be primarily due to the discretization error of the trapezoidal rule, while $e_{\\mathrm{naive}}$ contains both discretization error and a significant rounding error component.\nThe improvement ratio $R$ is defined as:\n$$ R = \\frac{e_{\\mathrm{naive}}}{\\max\\{e_{\\mathrm{Kahan}}, \\varepsilon\\}} $$\nwhere $\\varepsilon = 10^{-30}$ is a safeguard against division by zero. This ratio measures how many times more accurate the Kahan summation is compared to the naive approach.\n\nThe computational procedure will be applied to the four specified test cases, each characterized by an interval $[a,b]$ and number of subintervals $N$. For each case, we will compute and report the six required values: $I_{\\mathrm{naive}}$, $I_{\\mathrm{Kahan}}$, $C$, $e_{\\mathrm{naive}}$, $e_{\\mathrm{Kahan}}$, and $R$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and contrasts summation strategies for the composite trapezoidal rule\n    applied to the oscillatory integral of f(x) = sin(exp(x)).\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Format: (a, b, N)\n    test_cases = [\n        (0.0, 1.0, 4000),    # Case A\n        (0.0, 5.0, 40000),   # Case B\n        (-2.0, 5.0, 60000),  # Case C\n        (5.0, 7.0, 80000),   # Case D\n    ]\n\n    r = 8  # Refinement factor for reference value\n    epsilon = 1e-30  # Safeguard for division\n\n    results = []\n    for a, b, N in test_cases:\n        # Step 1: Generate the terms c_i for the trapezoidal rule sum\n        x = np.linspace(a, b, N + 1, dtype=np.float64)\n        y = np.sin(np.exp(x))\n        h = (b - a) / N\n        \n        terms = h * y\n        terms[0] *= 0.5\n        terms[-1] *= 0.5\n\n        # Step 2.1: Naive Summation\n        I_naive = np.sum(terms)\n\n        # Step 2.2: Kahan Compensated Summation\n        s = np.float64(0.0)\n        c = np.float64(0.0)\n        for term in terms:\n            y_kahan = term - c\n            t = s + y_kahan\n            c = (t - s) - y_kahan\n            s = t\n        I_Kahan = s\n        \n        # Step 3: Quantify cancellation index C\n        sum_abs_c = np.sum(np.abs(terms))\n        abs_sum_c = np.abs(I_Kahan)\n        \n        # Handle case where the integral is exactly zero, although unlikely\n        cancellation_index = sum_abs_c / abs_sum_c if abs_sum_c  0 else np.inf\n\n        # Step 4: Empirical error assessment\n        # Compute reference value on a refined grid with Kahan summation\n        N_ref = N * r\n        x_ref = np.linspace(a, b, N_ref + 1, dtype=np.float64)\n        y_ref = np.sin(np.exp(x_ref))\n        h_ref = (b - a) / N_ref\n        \n        terms_ref = h_ref * y_ref\n        terms_ref[0] *= 0.5\n        terms_ref[-1] *= 0.5\n\n        s_ref = np.float64(0.0)\n        c_ref = np.float64(0.0)\n        for term_ref in terms_ref:\n            y_kahan_ref = term_ref - c_ref\n            t_ref = s_ref + y_kahan_ref\n            c_ref = (t_ref - s_ref) - y_kahan_ref\n            s_ref = t_ref\n        I_ref = s_ref\n\n        # Compute absolute errors\n        e_naive = np.abs(I_naive - I_ref)\n        e_Kahan = np.abs(I_Kahan - I_ref)\n\n        # Compute improvement ratio\n        improvement_ratio = e_naive / max(e_Kahan, epsilon)\n\n        case_results = [\n            I_naive, \n            I_Kahan, \n            cancellation_index, \n            e_naive, \n            e_Kahan, \n            improvement_ratio\n        ]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string representation\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}