{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering implicit methods is to understand the core mechanic of a single time step. Unlike explicit methods, an implicit step requires solving an algebraic equation to find the solution's value at the next point in time. This exercise will guide you through this fundamental process by hand, using a simplified model of CPU cooling to provide a tangible context. By working through this calculation , you will build the foundational understanding needed to implement and analyze more complex implicit solvers.",
            "id": "2178316",
            "problem": "A simplified thermal model for a Central Processing Unit (CPU) under a variable load is described by the following first-order ordinary differential equation:\n$$ \\frac{dT}{dt} = -k(T - T_a) + P(t) $$\nwhere $T(t)$ is the temperature of the CPU in degrees Celsius ($^\\circ\\text{C}$) at time $t$ in minutes, $k$ is the heat dissipation constant, $T_a$ is the constant ambient temperature, and $P(t)$ represents the heat generated by the CPU's activity.\n\nSuppose the parameters are given as:\n- Heat dissipation constant, $k = 0.5 \\text{ min}^{-1}$\n- Ambient temperature, $T_a = 25^\\circ\\text{C}$\n- Heat generation function, $P(t) = P_0 \\cos(\\omega t)$, with $P_0 = 10^\\circ\\text{C}/\\text{min}$ and $\\omega = \\frac{\\pi}{6} \\text{ rad/min}$.\n- The initial temperature of the CPU is $T(0) = 85^\\circ\\text{C}$.\n\nTo predict the temperature evolution, a numerical method is employed. The temperature at the next time step, $T_{n+1}$ at time $t_{n+1}$, is estimated from the temperature at the current time step, $T_n$ at time $t_n$, using the following update rule:\n$$ T_{n+1} = T_n + h \\left( \\frac{dT}{dt} \\right)_{t=t_{n+1}, T=T_{n+1}} $$\nwhere $h = t_{n+1} - t_n$ is the time step.\n\nUsing a single step of this numerical method with a step size of $h = 0.5$ minutes, calculate the approximate temperature of the CPU at $t = 0.5$ minutes.\n\nExpress your answer in degrees Celsius, rounded to four significant figures.",
            "solution": "We are given the first-order ODE\n$$\n\\frac{dT}{dt}=-k\\left(T-T_{a}\\right)+P(t), \\quad P(t)=P_{0}\\cos(\\omega t),\n$$\nand we are to apply one step of the implicit (backward) Euler method with step size $h$ from $t_{0}=0$ to $t_{1}=h$. The backward Euler update is\n$$\nT_{1}=T_{0}+h\\left.\\frac{dT}{dt}\\right|_{t=t_{1},\\,T=T_{1}}=T_{0}+h\\big(-k(T_{1}-T_{a})+P_{0}\\cos(\\omega t_{1})\\big).\n$$\nSolve this equation for $T_{1}$. Expand the right-hand side and collect $T_{1}$ terms:\n$$\nT_{1}=T_{0}-hkT_{1}+hkT_{a}+hP_{0}\\cos(\\omega t_{1}).\n$$\nBring the $T_{1}$ terms to the left-hand side:\n$$\nT_{1}+hkT_{1}=T_{0}+hkT_{a}+hP_{0}\\cos(\\omega t_{1}).\n$$\nFactor the left-hand side and solve for $T_{1}$:\n$$\nT_{1}=\\frac{T_{0}+hkT_{a}+hP_{0}\\cos(\\omega t_{1})}{1+hk}.\n$$\nNow substitute the given data $T_{0}=85$, $k=0.5$, $T_{a}=25$, $P_{0}=10$, $\\omega=\\frac{\\pi}{6}$, $h=0.5$, and $t_{1}=0.5$:\n$$\n1+hk=1+0.5\\times 0.5=1.25,\n$$\n$$\nhkT_{a}=0.5\\times 0.5\\times 25=6.25,\n$$\n$$\nhP_{0}\\cos(\\omega t_{1})=0.5\\times 10\\times \\cos\\!\\left(\\frac{\\pi}{6}\\times 0.5\\right)=5\\cos\\!\\left(\\frac{\\pi}{12}\\right).\n$$\nThus\n$$\nT_{1}=\\frac{85+6.25+5\\cos\\!\\left(\\frac{\\pi}{12}\\right)}{1.25}.\n$$\nUsing $\\cos\\!\\left(\\frac{\\pi}{12}\\right)\\approx 0.9659258263$, we get\n$$\nT_{1}\\approx \\frac{85+6.25+4.829629131}{1.25}=\\frac{96.079629131}{1.25}\\approx 76.86370331.\n$$\nRounded to four significant figures, the approximate temperature at $t=0.5$ minutes is $76.86$ degrees Celsius.",
            "answer": "$$\\boxed{76.86}$$"
        },
        {
            "introduction": "When implicit methods are applied to large systems, such as those arising from the spatial discretization of partial differential equations, the resulting algebraic system can be enormous. A naive approach using dense matrix solvers would be computationally prohibitive. This practice  introduces a crucial concept in scientific computing: exploiting the special structure of the system matrix. You will implement a highly efficient solver for tridiagonal systems, demonstrating how to achieve linear-time complexity, which makes implicit methods practical for large-scale problems.",
            "id": "3241515",
            "problem": "You are to design and analyze an efficient implicit time-stepping method for an Initial Value Problem (IVP) in Ordinary Differential Equations (ODEs). Begin from the definition of an Ordinary Differential Equation (ODE) and the statement of an Initial Value Problem (IVP). Consider a linear system of the form $y'(t) = A y(t)$ for $t \\ge 0$, where $y(t) \\in \\mathbb{R}^n$ and $A \\in \\mathbb{R}^{n \\times n}$ is a tridiagonal matrix. The goal is to construct specific tridiagonal matrices $A$ and implement the Backward Euler method to advance the solution in time using an algorithm that avoids a full matrix inversion, while maintaining computational efficiency and numerical stability.\n\nYour program must:\n- Construct a tridiagonal matrix $A$ of specified size $n$ for each test case. A tridiagonal matrix has nonzero entries only on its main diagonal and the first sub- and super-diagonals. Denote the sub-diagonal by $a_i$ for $i=1,\\dots,n-1$, the diagonal by $d_i$ for $i=1,\\dots,n$, and the super-diagonal by $c_i$ for $i=1,\\dots,n-1$.\n- Derive and implement a single-step Backward Euler method (Backward Euler method (BE)) for the IVP $y'(t)=A y(t)$ without performing a dense inversion. At each time step from $t^n$ to $t^{n+1} = t^n + h$, set up and solve the linear system for $y^{n+1}$ using a tridiagonal solver based on Gaussian elimination specialized for tridiagonal matrices (often called the Thomas algorithm), which operates in $\\mathcal{O}(n)$ time and $\\mathcal{O}(n)$ memory.\n- Validate the tridiagonal solver by comparing its results to those obtained via a dense linear solver for the same linear system at each step, and report the maximum absolute difference.\n\nUse the following test suite. For each case, perform the specified operations and record a single floating-point number equal to the infinity-norm (maximum absolute value) of the difference between the solution computed by the tridiagonal solver and the solution computed by a dense solver. No physical units apply in this problem.\n\nTest suite:\n- Case $1$ (happy path, small $n$): Let $n=5$. Define $A$ by $d_i=-2$ for $i=1,\\dots,5$, $a_i=1$ for $i=1,\\dots,4$, and $c_i=1$ for $i=1,\\dots,4$. Use time step $h=0.05$. Take initial condition $y^0 = [1,0,0,0,0]^T$. Perform a single Backward Euler step and record the infinity-norm of the difference between the tridiagonal solution and the dense solution.\n- Case $2$ (diagonal-only, boundary condition coverage): Let $n=10$. Define $A$ diagonal by $d_i=-i$ for $i=1,\\dots,10$, and $a_i=0$, $c_i=0$. Use $h=0.2$. Take $y^0=[1,1,1,1,1,1,1,1,1,1]^T$. Perform a single Backward Euler step and record the difference.\n- Case $3$ (larger dimension, strong diagonal dominance): Let $n=50$. Define $A$ by $d_i=-5$ for $i=1,\\dots,50$, $a_i=-2$ for $i=1,\\dots,49$, $c_i=-1$ for $i=1,\\dots,49$. Use $h=0.1$. Take $y^0$ with components $y^0_j = j/50$ for $j=0,\\dots,49$. Perform a single Backward Euler step and record the difference.\n- Case $4$ (edge case $h=0$): Let $n=8$. Define $A$ by $d_i=-3$ for $i=1,\\dots,8$, $a_i=1$ for $i=1,\\dots,7$, $c_i=1$ for $i=1,\\dots,7$. Use $h=0$. Take $y^0$ with components $y^0_j = j$ for $j=0,\\dots,7$. Perform a single Backward Euler step and record the difference.\n- Case $5$ (multiple steps, cumulative stability): Let $n=20$. Define $A$ by $d_i=-1.6$ for $i=1,\\dots,20$, $a_i=0.8$ for $i=1,\\dots,19$, $c_i=0.9$ for $i=1,\\dots,19$. Use $h=0.05$. Take $y^0=[1,1,\\dots,1]^T \\in \\mathbb{R}^{20}$. Perform $100$ uniform Backward Euler steps to reach $t=5.0$, and record the difference between the tridiagonal-based result and the dense-based result.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the floating-point difference specified for Case $i$.\n\nThe implementation must avoid any full matrix inversion and must use an algorithm that exploits the tridiagonal structure to achieve linear-time complexity per solve. All computations are in pure mathematics with no physical units. Angles are not involved. Percentages are not involved; any fraction should be represented as a decimal number. The program must be self-contained, require no input, and adhere to the specified execution environment.",
            "solution": "An Initial Value Problem (IVP) for a system of Ordinary Differential Equations (ODEs) is defined by the differential equation $y'(t) = f(t, y(t))$ for $t \\ge t_0$, along with an initial condition $y(t_0) = y_0$. The problem under consideration is a linear, time-invariant system of the form:\n$$\ny'(t) = A y(t), \\quad y(0) = y^0\n$$\nwhere $y(t) \\in \\mathbb{R}^n$ is the state vector and $A \\in \\mathbb{R}^{n \\times n}$ is a constant, tridiagonal matrix.\n\nTo solve this IVP numerically, we employ a time-stepping method. The problem specifies the use of the Backward Euler (BE) method, which is an implicit method known for its strong stability properties (A-stability). We discretize the time domain with a uniform time step $h$, such that $t^k = k h$ for $k=0, 1, 2, \\dots$. The solution at each time step is denoted by $y^k \\approx y(t^k)$.\n\nThe Backward Euler method approximates the derivative at the next time step, $t^{n+1}$, using a backward difference formula:\n$$\ny'(t^{n+1}) \\approx \\frac{y^{n+1} - y^n}{h}\n$$\nSubstituting this approximation into the ODE evaluated at $t=t^{n+1}$, we get:\n$$\n\\frac{y^{n+1} - y^n}{h} = A y^{n+1}\n$$\nThis equation is implicit because the unknown, $y^{n+1}$, appears on both sides. To solve for $y^{n+1}$, we rearrange the terms:\n$$\ny^{n+1} - h A y^{n+1} = y^n\n$$\nFactoring out $y^{n+1}$ yields a linear system of equations:\n$$\n(I - hA) y^{n+1} = y^n\n$$\nwhere $I$ is the $n \\times n$ identity matrix. At each time step, we must solve this linear system for the unknown vector $y^{n+1}$, given the known vector $y^n$ from the previous step.\n\nThe matrix $A$ is tridiagonal. Let its diagonals be represented by the vectors $d$ (main diagonal, length $n$), $a$ (sub-diagonal, length $n-1$), and $c$ (super-diagonal, length $n-1$). Using $1$-based indexing, the non-zero entries of $A$ are $A_{i,i} = d_i$ for $i=1,\\dots,n$, $A_{i+1,i} = a_i$ for $i=1,\\dots,n-1$, and $A_{i,i+1} = c_i$ for $i=1,\\dots,n-1$.\n\nThe matrix of the linear system to be solved, $M = I - hA$, inherits this tridiagonal structure. Its diagonals are:\n- Main diagonal: $\\tilde{d}_i = 1 - h d_i$ for $i=1,\\dots,n$.\n- Sub-diagonal: $\\tilde{a}_i = -h a_i$ for $i=1,\\dots,n-1$.\n- Super-diagonal: $\\tilde{c}_i = -h c_i$ for $i=1,\\dots,n-1$.\n\nSolving a tridiagonal system can be done much more efficiently than solving a general dense system. A direct inversion of the matrix $M$ would be an $\\mathcal{O}(n^3)$ operation and would destroy the sparse structure, requiring $\\mathcal{O}(n^2)$ memory. Instead, we use the Thomas algorithm (also known as the Tridiagonal Matrix Algorithm or TDMA), which is a specialized form of Gaussian elimination that solves the system in $\\mathcal{O}(n)$ time using $\\mathcal{O}(n)$ memory.\n\nThe Thomas algorithm consists of two phases: forward elimination and backward substitution. Consider the system $M x = y^n$, written component-wise using the diagonals of $M$: $\\tilde{a}_i$ (sub), $\\tilde{d}_i$ (main), and $\\tilde{c}_i$ (super).\n$$\n\\tilde{a}_{i-1} x_{i-1} + \\tilde{d}_i x_i + \\tilde{c}_i x_{i+1} = y^n_i \\quad (\\text{with } \\tilde{a}_0=0, \\tilde{c}_n=0)\n$$\n1.  **Forward Elimination**: The algorithm modifies the super-diagonal coefficients and the right-hand side vector. We compute new coefficients $c'_i$ and $y'_{i}$ (using $0$-based indexing for implementation clarity):\n    - For $i=0$:\n      $$\n      c'_0 = \\frac{\\tilde{c}_0}{\\tilde{d}_0}, \\quad y'_0 = \\frac{y^n_0}{\\tilde{d}_0}\n      $$\n    - For $i=1, \\dots, n-2$:\n      $$\n      c'_i = \\frac{\\tilde{c}_i}{\\tilde{d}_i - \\tilde{a}_{i-1} c'_{i-1}}, \\quad y'_i = \\frac{y^n_i - \\tilde{a}_{i-1} y'_{i-1}}{\\tilde{d}_i - \\tilde{a}_{i-1} c'_{i-1}}\n      $$\n    - For $i=n-1$:\n      $$\n      y'_{n-1} = \\frac{y^n_{n-1} - \\tilde{a}_{n-2} y'_{n-2}}{\\tilde{d}_{n-1} - \\tilde{a}_{n-2} c'_{n-2}}\n      $$\n    This transforms the system into an upper bidiagonal form.\n\n2.  **Backward Substitution**: The solution $x = y^{n+1}$ is found by substituting backwards:\n    - For $i=n-1$:\n      $$\n      x_{n-1} = y'_{n-1}\n      $$\n    - For $i=n-2, \\dots, 0$:\n      $$\n      x_i = y'_i - c'_i x_{i+1}\n      $$\n\nFor each test case, we perform one or more steps of the Backward Euler method. We construct the matrix $M=I-hA$ and its diagonals, and the right-hand side vector $y^n$. We then solve for $y^{n+1}$ using both the implemented Thomas algorithm ($y_{\\text{tri}}^{n+1}$) and a general-purpose dense linear solver ($y_{\\text{dense}}^{n+1}$) provided by a standard library for validation. The final result for each case is the infinity-norm of the difference vector, $\\max_i |(y_{\\text{tri}}^{n+1})_i - (y_{\\text{dense}}^{n+1})_i|$. For the multi-step case, this comparison is performed after the final time step. The matrices in the test cases are chosen such that $I-hA$ is diagonally dominant, ensuring the numerical stability of the Thomas algorithm without pivoting.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Backward Euler method\n    with a tridiagonal solver.\n    \"\"\"\n\n    def construct_dense_A(n, d_vals, a_vals, c_vals):\n        \"\"\"Constructs the dense n x n tridiagonal matrix A.\"\"\"\n        A = np.zeros((n, n))\n        # Populate main diagonal\n        A += np.diag(d_vals)\n        # Populate sub-diagonal\n        if len(a_vals)  0:\n            A += np.diag(a_vals, k=-1)\n        # Populate super-diagonal\n        if len(c_vals)  0:\n            A += np.diag(c_vals, k=1)\n        return A\n\n    def thomas_algorithm(a, d, c, b):\n        \"\"\"\n        Solves a tridiagonal system of equations Ax=b using the Thomas algorithm.\n        a: sub-diagonal (length n-1)\n        d: main diagonal (length n)\n        c: super-diagonal (length n-1)\n        b: right-hand side vector (length n)\n        \n        The algorithm is not performed in-place to avoid side effects.\n        \"\"\"\n        n = len(d)\n        if n == 0:\n            return np.array([])\n        if n == 1:\n            return np.array([b[0] / d[0]])\n\n        # Create copies to avoid modifying original arrays\n        c_prime = np.zeros(n - 1)\n        d_prime = np.zeros(n)\n        x = np.zeros(n)\n\n        # Forward elimination\n        c_prime[0] = c[0] / d[0]\n        d_prime[0] = b[0] / d[0]\n\n        for i in range(1, n - 1):\n            denom = d[i] - a[i - 1] * c_prime[i - 1]\n            c_prime[i] = c[i] / denom\n            d_prime[i] = (b[i] - a[i - 1] * d_prime[i - 1]) / denom\n\n        denom_last = d[n - 1] - a[n - 2] * c_prime[n - 2]\n        d_prime[n - 1] = (b[n-1] - a[n-2] * d_prime[n-2]) / denom_last\n\n        # Backward substitution\n        x[n - 1] = d_prime[n - 1]\n        for i in range(n - 2, -1, -1):\n            x[i] = d_prime[i] - c_prime[i] * x[i + 1]\n\n        return x\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (n, d_func, a_func, c_func, h, y0_func, steps)\n    test_cases = [\n        # Case 1\n        (5, lambda n: np.full(n, -2.0), lambda n: np.full(n - 1, 1.0), lambda n: np.full(n - 1, 1.0), 0.05, \n         lambda n: np.array([1.0] + [0.0]*(n-1)), 1),\n        # Case 2\n        (10, lambda n: -np.arange(1, n + 1, dtype=float), lambda n: np.full(n - 1, 0.0), lambda n: np.full(n - 1, 0.0), 0.2, \n         lambda n: np.ones(n), 1),\n        # Case 3\n        (50, lambda n: np.full(n, -5.0), lambda n: np.full(n - 1, -2.0), lambda n: np.full(n - 1, -1.0), 0.1, \n         lambda n: np.arange(n, dtype=float) / n, 1),\n        # Case 4\n        (8, lambda n: np.full(n, -3.0), lambda n: np.full(n - 1, 1.0), lambda n: np.full(n - 1, 1.0), 0.0,\n         lambda n: np.arange(n, dtype=float), 1),\n        # Case 5\n        (20, lambda n: np.full(n, -1.6), lambda n: np.full(n - 1, 0.8), lambda n: np.full(n - 1, 0.9), 0.05,\n         lambda n: np.ones(n), 100),\n    ]\n\n    results = []\n    for n, d_func, a_func, c_func, h, y0_func, steps in test_cases:\n        # Construct diagonals and initial condition\n        d_A = d_func(n)\n        a_A = a_func(n)\n        c_A = c_func(n)\n        y0 = y0_func(n)\n\n        # Construct dense matrix A for validation\n        A_dense = construct_dense_A(n, d_A, a_A, c_A)\n        \n        # System to solve is (I - hA)y_next = y_prev\n        M_dense = np.eye(n) - h * A_dense\n\n        # Diagonals of M = I - hA\n        # Sub-diagonal mapping: a_M[i] is for row i+1, corresponds to a_A[i]\n        # Super-diagonal mapping: c_M[i] is for row i, corresponds to c_A[i]\n        d_M = 1.0 - h * d_A\n        if n  1:\n            a_M = -h * a_A\n            c_M = -h * c_A\n        else: # Handle n=1 case\n            a_M = np.array([])\n            c_M = np.array([])\n            \n        y_tri = y0.copy()\n        y_dense = y0.copy()\n\n        for _ in range(steps):\n            # Solve using Thomas algorithm\n            y_tri = thomas_algorithm(a_M, d_M, c_M, y_tri)\n            # Solve using dense solver for validation\n            y_dense = np.linalg.solve(M_dense, y_dense)\n\n        # Calculate the infinity norm of the difference\n        diff = np.max(np.abs(y_tri - y_dense))\n        results.append(diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A fixed step size is often inefficient, taking needlessly small steps in smooth regions or failing when the solution changes abruptly. This final practice  moves into the realm of modern, adaptive solvers. You will learn to create a \"smart\" algorithm that automatically adjusts its step size $h$ based on the difficulty of the problem. By using the number of Newton iterations as a proxy for step difficulty, you will build a controller that speeds up when possible and slows down when necessary, leading to a much more robust and efficient solver.",
            "id": "3241651",
            "problem": "Implement a program that solves a family of initial value problems (IVPs) for ordinary differential equations using an implicit one-step method with adaptive time-stepping. The adaptivity must be governed solely by the number of Newton iterations required to converge at each time step.\n\nYou must base your method on the following foundational elements:\n- An initial value problem is given by $y'(t) = f(t,y(t))$ with initial condition $y(t_0) = y_0$.\n- The implicit one-step method to be used is the backward Euler method, which defines the next solution value $y_{n+1}$ as the solution of an implicit algebraic equation constructed at the new time $t_{n+1}$.\n- The nonlinear algebraic equation at each step must be solved by Newton's method for root-finding.\n- The step-size controller must adjust the step size using only the observed Newton iteration count per successful step, and must reduce the step size and retry the same step when Newton fails to converge.\n\nYour program must:\n1. Start from the definition of the backward Euler method and Newton's method, and implement a Newton solver for the scalar implicit equation that arises at each time step. You must use the derivative of the residual with respect to the unknown new state.\n2. Implement an adaptive time-stepping controller that uses only the number of Newton iterations used on the last successful step to adjust the next step size. You must include a fallback mechanism that, upon Newton failure, reduces the current step size and retries the same step until convergence or until a minimum step size is hit.\n3. Use a predictor to initialize Newton's method at each step. The predictor must be any explicit first-order guess constructed from available information at the previous step.\n4. Use a stopping criterion for Newton's method that is norm-based and scale-aware. Conclude success if either the absolute residual is sufficiently small or the Newton correction is sufficiently small relative to the current iterate.\n\nNumerical parameters that your implementation must use:\n- Newton stopping tolerances: absolute residual tolerance $\\varepsilon_{\\mathrm{abs}} = 10^{-10}$ and relative correction tolerance $\\varepsilon_{\\mathrm{rel}} = 10^{-10}$.\n- Maximum Newton iterations per attempt: $m_{\\max} = 20$.\n- Step-size controller targets and factors:\n  - Target iteration window $[m_{\\mathrm{lo}}, m_{\\mathrm{hi}}] = [2,4]$.\n  - If a successful step uses $m \\le m_{\\mathrm{lo}}$, grow the next step by a factor $g = 1.5$.\n  - If a successful step uses $m_{\\mathrm{lo}}  m \\le m_{\\mathrm{hi}}$, keep the next step size unchanged.\n  - If a successful step uses $m  m_{\\mathrm{hi}}$, shrink the next step by a factor $s = 0.7$.\n  - If Newton fails to converge within $m_{\\max}$ iterations on a step attempt, shrink the current step on retry by a factor $s_{\\mathrm{fail}} = 0.3$.\n- Initial step size $h_0 = \\min\\{10^{-2}, (T-t_0)/10\\}$, minimum step size $h_{\\min} = 10^{-6}$, and maximum step size $h_{\\max} = 2\\times 10^{-1}$.\n- All trigonometric functions, where present, must use angles in radians.\n\nNewton stopping rules to be implemented:\n- Let $F(y)$ denote the scalar residual defining the implicit equation at time $t_{n+1}$, and let $\\Delta y$ be the Newton correction. Accept convergence if either $|F(y)| \\le \\varepsilon_{\\mathrm{abs}}$ or $|\\Delta y| \\le \\varepsilon_{\\mathrm{rel}} \\max\\{1, |y|\\}$.\n\nImplicit step equation to be solved at each step:\n- Define $t_{n+1} = t_n + h$ for the (possibly adapted) step size $h$, and find $y_{n+1}$ such that the backward Euler defining equation holds at $t_{n+1}$. The residual $F(y_{n+1})$ is the discrepancy between the left-hand side and right-hand side of this defining equation.\n- Your Newton iteration must use the derivative of $F(y)$ with respect to $y$ evaluated at the current Newton iterate.\n\nTest suite to be solved by your program:\n- Solve each of the following three IVPs with the specified final time $T$ and initial value $y_0$. For each problem, the function $f(t,y)$ and its derivative with respect to $y$, denoted $f_y(t,y)$, are:\n  1. Stiff linear decay:\n     - $f(t,y) = -a\\,y$, with $a = 100$.\n     - $f_y(t,y) = -a$.\n     - $t_0 = 0$, $y_0 = 1$, $T = 1$.\n  2. Logistic growth:\n     - $f(t,y) = r\\,y\\,(1 - y)$, with $r = 30$.\n     - $f_y(t,y) = r\\,(1 - 2y)$.\n     - $t_0 = 0$, $y_0 = 10^{-2}$, $T = 2$.\n  3. Forced stable linear system:\n     - $f(t,y) = -\\lambda\\,(y - \\sin t)$, with $\\lambda = 50$.\n     - $f_y(t,y) = -\\lambda$.\n     - $t_0 = 0$, $y_0 = 0$, $T = 3$.\n\nProgram output specification:\n- For each of the three IVPs, compute the numerical solution at the final time $T$ and return the final value $y(T)$ as a floating-point number.\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each number formatted to exactly six digits after the decimal point, in the order of the test cases listed above. For example: \"[0.123456,0.000001,1.000000]\".\n- No additional text or formatting must be printed.\n\nAll quantities in this problem are dimensionless, and there are no physical units involved. Angles in trigonometric functions must be interpreted in radians.",
            "solution": "We begin from the definition of an initial value problem: find a function $y(t)$ such that $y'(t) = f(t,y(t))$ with $y(t_0) = y_0$. The backward Euler method is an implicit one-step method built from the fundamental relationship of the derivative as a difference quotient. Over a step of size $h$ from $t_n$ to $t_{n+1} = t_n + h$, backward Euler replaces the derivative by $(y_{n+1} - y_n)/h$ and evaluates the right-hand side at the new time. This yields the defining equation\n$$\n\\frac{y_{n+1} - y_n}{h} = f(t_{n+1}, y_{n+1}),\n$$\nwhich is equivalent to the residual equation\n$$\nF(y_{n+1}) := y_{n+1} - y_n - h\\, f(t_{n+1}, y_{n+1}) = 0.\n$$\nAt each step we must solve the scalar nonlinear equation $F(y) = 0$ for the unknown $y = y_{n+1}$. This can be accomplished by Newton's method, which is derived from the first-order Taylor expansion of $F$ around a current iterate $y^{(k)}$:\n$$\nF(y^{(k+1)}) \\approx F(y^{(k)}) + F'(y^{(k)})(y^{(k+1)} - y^{(k)}).\n$$\nSetting the left-hand side to zero and solving for the correction $\\Delta y^{(k)} = y^{(k+1)} - y^{(k)}$ gives the Newton update\n$$\n\\Delta y^{(k)} = - \\frac{F(y^{(k)})}{F'(y^{(k)})}, \\quad y^{(k+1)} = y^{(k)} + \\Delta y^{(k)}.\n$$\nFor the backward Euler residual,\n$$\nF(y) = y - y_n - h\\, f(t_{n+1}, y),\n$$\nits derivative with respect to $y$ follows from the chain rule and is\n$$\nF'(y) = 1 - h \\, \\frac{\\partial f}{\\partial y}(t_{n+1}, y).\n$$\nA practical implementation requires a starting guess $y^{(0)}$. A first-order consistent predictor may be constructed from known information at $t_n$, for example an explicit Euler predictor $y^{(0)} = y_n + h\\, f(t_n, y_n)$, which leverages the value of the right-hand side at the previous time. Convergence of Newton's method is declared when either the absolute residual or the Newton correction is sufficiently small. With tolerances $\\varepsilon_{\\mathrm{abs}}$ and $\\varepsilon_{\\mathrm{rel}}$, we accept the iteration if\n$$\n|F(y^{(k+1)})| \\le \\varepsilon_{\\mathrm{abs}} \\quad \\text{or} \\quad |\\Delta y^{(k)}| \\le \\varepsilon_{\\mathrm{rel}} \\max\\{1, |y^{(k+1)}|\\}.\n$$\nWe bound the number of Newton iterations by $m_{\\max}$ to avoid infinite loops in the presence of nonconvergence.\n\nThe adaptive time-stepping logic uses the Newton iteration count as a proxy for step difficulty:\n- If a successful Newton solve uses $m$ iterations within a desired range $[m_{\\mathrm{lo}}, m_{\\mathrm{hi}}]$, the next step size is maintained. If it is too easy, $m \\le m_{\\mathrm{lo}}$, we increase $h$ by a growth factor $g$, and if it is too hard, $m  m_{\\mathrm{hi}}$, we reduce $h$ by a shrink factor $s$. This heuristic aims to keep the nonlinear solve moderately challenging but efficient.\n- If Newton fails to converge within $m_{\\max}$ iterations for the current step attempt, we reduce the step size by a more aggressive factor $s_{\\mathrm{fail}}$ and retry the same step. This is a safeguard to enforce convergence by reducing the local nonlinearity strength as $h \\to 0$.\n\nWe also enforce bounds $h_{\\min} \\le h \\le h_{\\max}$ and ensure that the final step does not overshoot the final time $T$ by taking $h = \\min\\{h, T - t_n\\}$ for the last increment. The initial step size is chosen as $h_0 = \\min\\{10^{-2}, (T - t_0)/10\\}$ to be cautious.\n\nFor the three scalar test problems, we provide $f(t,y)$ and its partial derivative $f_y(t,y)$, which are required to evaluate $F$ and $F'$:\n1. Stiff linear decay with $a = 100$: $f(t,y) = -a y$ and $f_y(t,y) = -a$.\n2. Logistic growth with $r = 30$: $f(t,y) = r y (1 - y)$ and $f_y(t,y) = r (1 - 2 y)$.\n3. Forced stable linear system with $\\lambda = 50$: $f(t,y) = -\\lambda (y - \\sin t)$, $f_y(t,y) = -\\lambda$, with angles in radians.\n\nAlgorithmic outline for one IVP:\n- Initialize $t \\leftarrow t_0$, $y \\leftarrow y_0$, $h \\leftarrow h_0$.\n- While $t  T$:\n  - Set $h \\leftarrow \\min\\{h, T - t\\}$.\n  - Form $t_{\\text{new}} = t + h$ and predictor $y^{(0)} = y + h\\, f(t, y)$.\n  - Run Newton iterations on $F(y) = y - y_{\\text{prev}} - h f(t_{\\text{new}}, y)$ with $F'(y) = 1 - h f_y(t_{\\text{new}}, y)$ starting from $y^{(0)}$.\n  - If Newton converges within $m_{\\max}$ iterations, accept $y \\leftarrow y^{\\star}$ and $t \\leftarrow t_{\\text{new}}$, record $m$ and adapt $h$ using the controller with factors $g$, $s$ and limits $h_{\\min}$, $h_{\\max}$.\n  - If Newton fails, reduce $h \\leftarrow \\max\\{h \\cdot s_{\\mathrm{fail}}, h_{\\min}\\}$ and retry the same step. If $h$ reaches $h_{\\min}$ and still fails, the algorithm would stop; for our test problems and parameters chosen, convergence is achieved before this bound.\n\nThe numerical parameters used are:\n- $\\varepsilon_{\\mathrm{abs}} = 10^{-10}$, $\\varepsilon_{\\mathrm{rel}} = 10^{-10}$, $m_{\\max} = 20$,\n- $m_{\\mathrm{lo}} = 2$, $m_{\\mathrm{hi}} = 4$, $g = 1.5$, $s = 0.7$, $s_{\\mathrm{fail}} = 0.3$,\n- $h_0 = \\min\\{10^{-2}, (T - t_0)/10\\}$, $h_{\\min} = 10^{-6}$, $h_{\\max} = 2 \\times 10^{-1}$.\n\nApplying this scheme to the test suite produces a final value $y(T)$ for each IVP. The output must be a single line string of the three values formatted to six digits after the decimal point in the order listed. The implicit method combined with the iteration-count-based adaptivity ensures robustness for the stiff linear problems and efficiency via step-size growth where Newton converges rapidly. The logistic case exercises the nonlinearity, while the forced linear case checks correct handling of time-dependent forcing with angles in radians. The choices of $h_{\\min}$, $h_{\\max}$, and the controller parameters balance stability, convergence, and computational efficiency without relying on an explicit local truncation error estimator, adhering to the requirement that step size is controlled by the number of Newton iterations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef newton_solve(y_prev, t_prev, t_new, h, f, df_dy,\n                 atol=1e-10, rtol=1e-10, maxit=20):\n    \"\"\"\n    Solve F(y) = y - y_prev - h * f(t_new, y) = 0 by Newton's method.\n    Returns (y_star, iterations, success).\n    \"\"\"\n    # Explicit Euler predictor as initial guess\n    y = y_prev + h * f(t_prev, y_prev)\n\n    def F(val):\n        return val - y_prev - h * f(t_new, val)\n\n    def dF(val):\n        return 1.0 - h * df_dy(t_new, val)\n\n    # Evaluate initial residual\n    for k in range(maxit):\n        Fy = F(y)\n        dFy = dF(y)\n        # Safeguard: if derivative is extremely small, abort to avoid blow-up\n        if dFy == 0.0:\n            return y, k + 1, False\n        delta = -Fy / dFy\n        y_next = y + delta\n        # Check stopping conditions\n        if abs(Fy) = atol or abs(delta) = rtol * max(1.0, abs(y_next)):\n            return y_next, k + 1, True\n        y = y_next\n\n    # If we reached max iterations without satisfying stopping criteria\n    return y, maxit, False\n\n\ndef solve_ivp_adaptive(t0, y0, T, f, df_dy,\n                       h0, h_min, h_max,\n                       m_lo=2, m_hi=4, grow=1.5, shrink=0.7, fail_shrink=0.3,\n                       newton_atol=1e-10, newton_rtol=1e-10, newton_maxit=20):\n    \"\"\"\n    Backward Euler with Newton solve and adaptive step size controlled by\n    Newton iteration count.\n    \"\"\"\n    t = t0\n    y = y0\n    h = min(h0, T - t0)\n    # Safety to avoid infinite loops\n    max_total_steps = 10_000\n\n    steps = 0\n    while t  T and steps  max_total_steps:\n        # Do not overshoot final time\n        h = min(h, T - t)\n        # Attempt Newton solve at this step\n        y_star, iters, ok = newton_solve(\n            y_prev=y, t_prev=t, t_new=t + h, h=h,\n            f=f, df_dy=df_dy, atol=newton_atol, rtol=newton_rtol, maxit=newton_maxit\n        )\n\n        if ok:\n            # Accept the step\n            t += h\n            y = y_star\n            # Adapt step size for next step based on iteration count\n            if iters = m_lo:\n                h = min(h * grow, h_max)\n            elif iters = m_hi:\n                # keep h unchanged\n                h = min(max(h, h_min), h_max)\n            else:\n                h = max(h * shrink, h_min)\n        else:\n            # Newton failed: shrink and retry same step\n            new_h = max(h * fail_shrink, h_min)\n            # If we are at minimum and still cannot converge, break\n            if new_h == h:\n                break\n            h = new_h\n\n        steps += 1\n\n    return y\n\n\ndef build_problems():\n    # Define ODEs and their y-derivatives for the test suite\n    def lin_f_factory(a):\n        def f(t, y):\n            return -a * y\n        def df_dy(t, y):\n            return -a\n        return f, df_dy\n\n    def logistic_f_factory(r):\n        def f(t, y):\n            return r * y * (1.0 - y)\n        def df_dy(t, y):\n            return r * (1.0 - 2.0 * y)\n        return f, df_dy\n\n    def forced_f_factory(lam):\n        def f(t, y):\n            return -lam * (y - np.sin(t))\n        def df_dy(t, y):\n            return -lam\n        return f, df_dy\n\n    problems = []\n    # Problem 1: Stiff linear decay\n    f1, df1 = lin_f_factory(a=100.0)\n    problems.append({\n        \"t0\": 0.0, \"y0\": 1.0, \"T\": 1.0,\n        \"f\": f1, \"df_dy\": df1\n    })\n    # Problem 2: Logistic growth\n    f2, df2 = logistic_f_factory(r=30.0)\n    problems.append({\n        \"t0\": 0.0, \"y0\": 1.0e-2, \"T\": 2.0,\n        \"f\": f2, \"df_dy\": df2\n    })\n    # Problem 3: Forced stable linear system\n    f3, df3 = forced_f_factory(lam=50.0)\n    problems.append({\n        \"t0\": 0.0, \"y0\": 0.0, \"T\": 3.0,\n        \"f\": f3, \"df_dy\": df3\n    })\n\n    return problems\n\n\ndef solve():\n    # Controller and Newton parameters as specified\n    newton_atol = 1e-10\n    newton_rtol = 1e-10\n    newton_maxit = 20\n    m_lo, m_hi = 2, 4\n    grow, shrink, fail_shrink = 1.5, 0.7, 0.3\n    h_min, h_max = 1e-6, 2e-1\n\n    problems = build_problems()\n    results = []\n\n    for prob in problems:\n        t0 = prob[\"t0\"]\n        y0 = prob[\"y0\"]\n        T = prob[\"T\"]\n        f = prob[\"f\"]\n        df_dy = prob[\"df_dy\"]\n\n        # Initial step size rule: min(1e-2, (T - t0)/10)\n        h0 = min(1e-2, (T - t0) / 10.0)\n\n        yT = solve_ivp_adaptive(\n            t0=t0, y0=y0, T=T, f=f, df_dy=df_dy,\n            h0=h0, h_min=h_min, h_max=h_max,\n            m_lo=m_lo, m_hi=m_hi, grow=grow, shrink=shrink, fail_shrink=fail_shrink,\n            newton_atol=newton_atol, newton_rtol=newton_rtol, newton_maxit=newton_maxit\n        )\n        results.append(yT)\n\n    # Format to exactly six decimals as required\n    formatted = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}