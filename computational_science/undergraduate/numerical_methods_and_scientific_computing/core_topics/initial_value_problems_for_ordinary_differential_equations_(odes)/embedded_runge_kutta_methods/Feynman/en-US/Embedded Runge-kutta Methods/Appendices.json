{
    "hands_on_practices": [
        {
            "introduction": "To truly master a tool, it helps to build one from scratch. This exercise guides you through the fundamental design process of an embedded Runge-Kutta method by having you derive the coefficients for a simple yet functional 2(1) pair from first principles. By working through the Taylor series expansions to establish the order conditions, you will gain a first-hand look at the mathematical engine that powers these sophisticated solvers. ",
            "id": "3224413",
            "problem": "Consider the initial value problem for an ordinary differential equation, given by $y^{\\prime}(t) = f(t,y(t))$ with $y(t_{0}) = y_{0}$, where $f$ is sufficiently smooth to admit Taylor expansions in both arguments. You will design a two-stage explicit embedded Runge–Kutta (RK) method whose primary update is of order $2$ and whose embedded update is of order $1$. Embedded Runge–Kutta methods, as exemplified by the Runge–Kutta–Fehlberg (RKF) family, deliver two approximations of different orders from shared stage evaluations to estimate the local truncation error.\n\nLet the two-stage explicit RK scheme be defined by stage slopes\n$$\nk_{1} = f(t_{n},y_{n}), \\quad\nk_{2} = f\\!\\big(t_{n} + c_{2} h, \\; y_{n} + a_{21} h k_{1}\\big),\n$$\nand the primary update\n$$\ny_{n+1} = y_{n} + h\\!\\left(b_{1} k_{1} + b_{2} k_{2}\\right),\n$$\ntogether with an embedded update\n$$\n\\widehat{y}_{n+1} = y_{n} + h\\!\\left(\\widehat{b}_{1} k_{1} + \\widehat{b}_{2} k_{2}\\right).\n$$\nStarting from first principles, namely the Taylor expansion of the exact solution $y(t_{n}+h)$ about $(t_{n},y_{n})$ and the chain-rule expansion of $f(t_{n}+c_{2} h,\\; y_{n}+a_{21} h k_{1})$ about $(t_{n},y_{n})$, derive the order conditions that the coefficients must satisfy for the primary update to achieve order $2$ and for the embedded update to achieve order $1$. Impose the standard internal consistency for explicit RK stages, and require all coefficients to be real and nonnegative.\n\nThen, choose $c_{2}$ to be the smallest positive integer consistent with the derived order conditions and nonnegativity, and determine uniquely the coefficients $a_{21}$, $c_{2}$, $b_{1}$, $b_{2}$, $\\widehat{b}_{1}$, and $\\widehat{b}_{2}$.\n\nProvide your final answer as a single row matrix in the order $\\big(a_{21},\\, c_{2},\\, b_{1},\\, b_{2},\\, \\widehat{b}_{1},\\, \\widehat{b}_{2}\\big)$. No rounding is required.",
            "solution": "The problem requires the derivation of coefficients for a two-stage explicit embedded Runge-Kutta method. The primary method must be of order $2$, and the embedded method of order $1$. We begin by performing a Taylor series expansion of the exact solution and the numerical approximations.\n\nLet the initial value problem be $y^{\\prime}(t) = f(t,y(t))$ with $y(t_n) = y_n$. The Taylor expansion of the exact solution $y(t)$ around $t_n$ is:\n$$\ny(t_n + h) = y(t_n) + h y^{\\prime}(t_n) + \\frac{h^2}{2} y^{\\prime\\prime}(t_n) + O(h^3)\n$$\nWe express the derivatives of $y$ in terms of $f$ and its partial derivatives, evaluated at $(t_n, y_n)$. For brevity, we denote $f(t_n, y_n)$ as $f$, $\\frac{\\partial f}{\\partial t}(t_n, y_n)$ as $f_t$, and $\\frac{\\partial f}{\\partial y}(t_n, y_n)$ as $f_y$.\nThe first derivative is simply $y^{\\prime}(t_n) = f(t_n, y_n) = f$.\nThe second derivative is found using the chain rule:\n$$\ny^{\\prime\\prime}(t) = \\frac{d}{dt}f(t, y(t)) = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} = f_t + f_y f\n$$\nSubstituting these into the Taylor expansion gives the exact solution up to order $h^2$:\n$$\ny(t_n+h) = y_n + hf + \\frac{h^2}{2}(f_t + f_y f) + O(h^3)\n$$\nNext, we expand the numerical approximations. The two stage values are given by:\n$$\nk_1 = f(t_n, y_n) = f\n$$\n$$\nk_2 = f(t_n + c_2 h, y_n + a_{21} h k_1) = f(t_n + c_2 h, y_n + a_{21} h f)\n$$\nWe expand $k_2$ using a multivariable Taylor series around $(t_n, y_n)$:\n$$\nk_2 = f(t_n, y_n) + (c_2 h) f_t + (a_{21} h f) f_y + O(h^2) = f + c_2 h f_t + a_{21} h f f_y + O(h^2)\n$$\nThe primary, second-order update is $y_{n+1} = y_n + h(b_1 k_1 + b_2 k_2)$. Substituting the expansions for $k_1$ and $k_2$:\n$$\ny_{n+1} = y_n + h(b_1 f + b_2 [f + c_2 h f_t + a_{21} h f f_y + O(h^2)])\n$$\n$$\ny_{n+1} = y_n + (b_1 + b_2)hf + (b_2 c_2)h^2 f_t + (b_2 a_{21})h^2 f f_y + O(h^3)\n$$\nFor $y_{n+1}$ to be of order $2$, its expansion must match the expansion of $y(t_n+h)$ up to the $h^2$ terms. By comparing coefficients of the terms involving $f$, $f_t$, and $f_y f$, we obtain the following order conditions:\n\\begin{itemize}\n    \\item Order $h$: The coefficients of $hf$ must match.\n    $$b_1 + b_2 = 1$$\n    \\item Order $h^2$: The coefficients of $h^2 f_t$ and $h^2 f f_y$ must match.\n    $$b_2 c_2 = \\frac{1}{2}$$\n    $$b_2 a_{21} = \\frac{1}{2}$$\n\\end{itemize}\nThe embedded, first-order update is $\\widehat{y}_{n+1} = y_n + h(\\widehat{b}_1 k_1 + \\widehat{b}_2 k_2)$. We expand this similarly:\n$$\n\\widehat{y}_{n+1} = y_n + h(\\widehat{b}_1 f + \\widehat{b}_2 [f + O(h)]) = y_n + (\\widehat{b}_1 + \\widehat{b}_2)hf + O(h^2)\n$$\nFor $\\widehat{y}_{n+1}$ to be of order $1$, its expansion must match the expansion of $y(t_n+h)$ up to the $h^1$ term. This yields one order condition:\n$$\n\\widehat{b}_1 + \\widehat{b}_2 = 1\n$$\nWe now solve for the coefficients using the additional constraints provided. The standard internal consistency condition for a two-stage explicit RK method is $c_2 = a_{21}$. From the order $2$ conditions, we see that $b_2 c_2 = b_2 a_{21}$. Assuming a true two-stage method where the second stage has an impact, we must have $b_2 \\neq 0$. Thus, $c_2 = a_{21}$ is automatically satisfied. The problem specifies that $c_2$ is the smallest positive integer, so we set $c_2 = 1$. With this choice, the coefficients for the primary method are uniquely determined:\nFrom $c_2=1$, the consistency condition gives $a_{21} = 1$.\nFrom $b_2 c_2 = 1/2$, we have $b_2(1) = 1/2$, which gives $b_2 = 1/2$.\nFrom $b_1 + b_2 = 1$, we have $b_1 + 1/2 = 1$, which gives $b_1 = 1/2$.\nThese coefficients ($a_{21}=1$, $c_2=1$, $b_1=1/2$, $b_2=1/2$) are all non-negative as required. This defines the second-order method, often known as the explicit midpoint rule or Heun's method.\n\nFor the embedded method, we have the condition $\\widehat{b}_1 + \\widehat{b}_2 = 1$ and the non-negativity constraints $\\widehat{b}_1 \\ge 0$ and $\\widehat{b}_2 \\ge 0$. This system is underdetermined. However, the problem states that the coefficients must be determined uniquely. This implies we must invoke a standard principle in method design, which is to construct the lower-order method to be as simple as possible. A method of order $1$ can be constructed with only one stage evaluation ($k_1$). This corresponds to the Forward Euler method, $\\widehat{y}_{n+1} = y_n + h k_1$. To achieve this structure, we must set the coefficient of the second stage, $\\widehat{b}_2$, to zero. This choice makes the solution unique.\nSetting $\\widehat{b}_2 = 0$.\nFrom the order condition $\\widehat{b}_1 + \\widehat{b}_2 = 1$, we get $\\widehat{b}_1 = 1$.\nThese coefficients are non-negative.\n\nThus, the complete set of uniquely determined coefficients is:\n$a_{21} = 1$\n$c_2 = 1$\n$b_1 = 1/2$\n$b_2 = 1/2$\n$\\widehat{b}_1 = 1$\n$\\widehat{b}_2 = 0$\nWe present these coefficients in the specified matrix format.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & \\frac{1}{2} & \\frac{1}{2} & 1 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Theoretical scaling laws provide powerful predictions about an algorithm's performance, but verifying them empirically builds deep intuition. In this practice, you will implement the widely-used Runge-Kutta-Fehlberg 4(5) method and use it to solve several ODEs across a range of tolerances. By analyzing the relationship between the error tolerance $\\text{tol}$ and the total number of steps $N$, you will computationally test the theoretical prediction that $N \\propto \\text{tol}^{-1/p}$, confirming the efficiency of adaptive step-size control. ",
            "id": "3224450",
            "problem": "Consider an initial value problem for an ordinary differential equation (ODE), defined as $y'(t) = f(t,y)$ with $y(t_0) = y_0$, where $f$ is sufficiently smooth. Implement an adaptive time-stepping solver based on the embedded Runge–Kutta–Fehlberg (RKF) method of order $4(5)$, where the error estimator is of order $p$ and scales as $h^p$ for step size $h$. Use the higher-order solution to advance the state and the embedded difference to estimate the local error at each step. The step size should be adapted so that the local error estimate is approximately equal to a prescribed scalar tolerance $\\,\\text{tol}\\,$, with appropriate safety and bounds to ensure stable progression.\n\nFrom the fundamental definition of local truncation error for one-step methods and the standard assumption that the local error estimate $E$ behaves as $E \\approx C h^p$ for some problem-dependent constant $C$, it follows that selecting $h$ to maintain $E \\approx \\text{tol}$ leads to $h \\propto \\text{tol}^{1/p}$ and hence to a total number of accepted steps $N$ scaling as $N \\propto \\text{tol}^{-1/p}$ over a fixed interval. Your task is to empirically investigate this scaling using the embedded Runge–Kutta–Fehlberg method.\n\nImplement the solver and perform the following test suite. In each case, compute the total number of accepted steps $N$ for each tolerance in the set $\\{10^{-2},10^{-3},10^{-4},10^{-5},10^{-6}\\}$, then estimate the slope $s$ of the linear fit of $\\log N$ versus $\\log \\text{tol}$ over these five points. Compare $s$ to the theoretical value $-1/p$ and report whether $\\lvert s - (-1/p)\\rvert \\le \\epsilon$, where $\\epsilon$ is a prescribed threshold.\n\nTake $p=5$ for the Runge–Kutta–Fehlberg $4(5)$ embedded pair and use $\\epsilon = 0.15$. The ODEs, initial conditions, and intervals are:\n\n- Case A (smooth exponential decay): $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 10$.\n- Case B (moderately oscillatory forcing): $f(t,y) = \\cos(10 t)$, $t_0 = 0$, $y_0 = 0$, $t_{\\text{end}} = 10$.\n- Case C (nonlinear growth away from singularity): $f(t,y) = y^2$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 0.9$.\n\nFor each case, produce a boolean indicating whether the measured slope $s$ is within $\\epsilon$ of the theoretical value $-1/p$. Your program should produce a single line of output containing the three boolean results as a comma-separated list enclosed in square brackets, in the order of the cases A, B, C (for example, $[{\\tt True},{\\tt False},{\\tt True}]$). No physical units are involved in this problem, and angles, if any appear, are in radians by default; no angle conversions are required here.",
            "solution": "The user requests an implementation of the adaptive Runge-Kutta-Fehlberg method of order $4(5)$ to empirically validate the theoretical scaling relationship between the number of integration steps and the prescribed error tolerance.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **ODE**: $y'(t) = f(t,y)$ with initial condition $y(t_0) = y_0$.\n*   **Method**: Embedded Runge-Kutta-Fehlberg (RKF) of order $4(5)$.\n*   **Error Estimator**: Order $p=5$, with error estimate $E$ scaling as $E \\approx C h^p$.\n*   **State Propagation**: Use the higher-order (5th order) solution to advance the state.\n*   **Step-Size Control**: Adapt step size $h$ to keep the local error estimate approximately equal to a scalar tolerance $\\text{tol}$.\n*   **Theoretical Scaling**: The total number of accepted steps $N$ is expected to scale as $N \\propto \\text{tol}^{-1/p}$.\n*   **Task**: For three specific test cases, compute $N$ for each tolerance in $\\text{tol} \\in \\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\}$.\n*   **Analysis**: Estimate the slope $s$ of the linear fit of $\\log N$ versus $\\log \\text{tol}$.\n*   **Verification**: Check if the condition $\\lvert s - (-1/p)\\rvert \\le \\epsilon$ is met.\n*   **Parameters**: $p=5$, $\\epsilon = 0.15$.\n*   **Test Cases**:\n    *   **Case A**: $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 10$.\n    *   **Case B**: $f(t,y) = \\cos(10 t)$, $t_0 = 0$, $y_0 = 0$, $t_{\\text{end}} = 10$.\n    *   **Case C**: $f(t,y) = y^2$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 0.9$.\n*   **Output**: A list of three boolean values corresponding to the verification result for each case.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is a standard exercise in numerical analysis for solving ordinary differential equations. The Runge-Kutta-Fehlberg method and the theory of adaptive step-size control are fundamental concepts in scientific computing. The problem is well-established and scientifically sound.\n2.  **Well-Posed**: The problem is clearly defined. The ODEs, initial conditions, integration intervals, and tolerances are all specified. The task of implementing the solver, gathering data, performing a linear regression, and checking the result against a criterion is unambiguous.\n3.  **Objective**: The language is precise and free of subjective or opinion-based statements. The evaluation criterion is a quantitative and objective test.\n4.  **Complete and Consistent**: The problem provides all necessary information to proceed. While the specific Butcher tableau for the RKF45 method is not given, it is a standard, well-documented set of coefficients that can be readily obtained from canonical references in numerical analysis. The order $p=5$ for the error estimator is consistent with a $4(5)$ pair where the error of the 4th-order method is being estimated. The problem is self-consistent and complete.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. It is a well-posed, scientifically grounded problem from the field of numerical methods. A complete solution will be provided.\n\n### Principle-Based Design and Solution\n\nThe core of this problem is to verify a fundamental property of adaptive step-size ordinary differential equation (ODE) solvers. We are given an initial value problem (IVP) of the form:\n$$ y'(t) = f(t, y), \\quad y(t_0) = y_0 $$\nwhere $y(t)$ can be a scalar or a vector. The problems provided are all scalar.\n\n**1. Embedded Runge-Kutta Methods**\n\nAn embedded Runge-Kutta method uses two one-step formulas of different orders to approximate the solution at each step. The Runge-Kutta-Fehlberg $4(5)$ method calculates two estimates: a fourth-order accurate solution $y_{n+1}^{(4)}$ and a fifth-order accurate solution $y_{n+1}^{(5)}$. The local truncation error (LTE) of the fourth-order method is $\\mathcal{O}(h^5)$, while the LTE of the fifth-order method is $\\mathcal{O}(h^6)$.\n\nThe difference between these two estimates provides an efficient approximation of the error in the lower-order solution:\n$$ E_{n+1} = \\| y_{n+1}^{(5)} - y_{n+1}^{(4)} \\| \\approx \\| y(t_{n+1}) - y_{n+1}^{(4)} \\| $$\nThis error estimate, $E_{n+1}$, is of order $p=5$, meaning it scales with the step size $h$ as $E_{n+1} \\approx C h^5$ for some constant $C$ that depends on the derivatives of the solution. The problem specifies that the higher-order solution, $y_{n+1}^{(5)}$, should be used to advance the state, a technique known as local extrapolation, which generally improves accuracy.\n\nThe RKF45 method requires six function evaluations (stages $k_1, \\dots, k_6$) per step to compute both solutions. The formulas are:\n$$ k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right) $$\n$$ y_{n+1}^{(4)} = y_n + h \\sum_{i=1}^{6} b_i k_i $$\n$$ y_{n+1}^{(5)} = y_n + h \\sum_{i=1}^{6} b_i^* k_i $$\nThe coefficients $c_i$, $a_{ij}$, $b_i$, and $b_i^*$ define the specific method. For this implementation, the standard coefficients derived by Fehlberg are used.\n\n**2. Adaptive Step-Size Control**\n\nThe goal of an adaptive solver is to adjust the step size $h$ to maintain the local error estimate $E$ below a specified tolerance, $\\text{tol}$. Assuming the error scales as $E \\approx C h^p$, we can derive a rule for choosing the next step size. If a step $h_{old}$ produced an error $E_{old}$, we want to find a new step size $h_{new}$ that would yield an error approximately equal to $\\text{tol}$.\n$$ \\frac{E_{old}}{\\text{tol}} \\approx \\frac{C h_{old}^p}{C h_{new}^p} = \\left(\\frac{h_{old}}{h_{new}}\\right)^p $$\nSolving for $h_{new}$ gives the update formula:\n$$ h_{new} = h_{old} \\left( \\frac{\\text{tol}}{E_{old}} \\right)^{1/p} $$\nTo ensure stability and prevent overly aggressive step size changes, a safety factor $S < 1$ (typically $S \\approx 0.9$) is introduced, and the change factor is bounded:\n$$ h_{new} = h_{old} \\cdot \\min\\left(f_{max}, \\max\\left(f_{min}, S \\left( \\frac{\\text{tol}}{E_{old}} \\right)^{1/p}\\right)\\right) $$\nIf $E_{old} \\le \\text{tol}$, the step is accepted, and $h_{new}$ is used for the next step. If $E_{old} > \\text{tol}$, the step is rejected, and the current step is retried with the smaller step size $h_{new}$.\n\n**3. Empirical Verification of Scaling Law**\n\nFor a fixed integration interval of length $L = t_{\\text{end}} - t_0$, the total number of accepted steps $N$ is related to the average step size $h_{avg}$ by $N \\approx L/h_{avg}$. If the adaptive algorithm successfully maintains $E \\approx \\text{tol}$ at each step, then on average $h_{avg} \\propto \\text{tol}^{1/p}$. Consequently, the total number of steps should scale as:\n$$ N \\propto \\frac{1}{h_{avg}} \\propto \\frac{1}{\\text{tol}^{1/p}} = \\text{tol}^{-1/p} $$\nTaking the logarithm of both sides reveals a linear relationship:\n$$ \\log N = \\log(\\text{constant}) - \\frac{1}{p} \\log(\\text{tol}) $$\nThis equation is of the form $Y = C + sX$, where $Y = \\log N$, $X = \\log(\\text{tol})$, and the theoretical slope is $s = -1/p$. For this problem, with $p=5$, the theoretical slope is $-0.2$.\n\nThe procedure is to run the solver for a set of tolerances, record the number of steps for each, and perform a linear regression on the $\\log N$ vs. $\\log \\text{tol}$ data. The slope $s$ of the best-fit line is given by the formula:\n$$ s = \\frac{n \\sum(x_i y_i) - (\\sum x_i)(\\sum y_i)}{n \\sum(x_i^2) - (\\sum x_i)^2} $$\nwhere $x_i = \\log(\\text{tol}_i)$, $y_i = \\log(N_i)$, and $n$ is the number of data points. The computed slope $s$ is then compared to the theoretical value $-1/p$ to verify if it falls within the given threshold $\\epsilon$. This confirms that the implemented solver behaves according to numerical theory.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements an adaptive RKF45 solver and verifies its step-count-to-tolerance scaling.\n    \"\"\"\n    \n    # --- Define ODEs and problem parameters ---\n    def f_A(t, y):\n        return -y\n\n    def f_B(t, y):\n        return np.cos(10 * t)\n\n    def f_C(t, y):\n        # The ODE y' = y^2 has a singularity at t=1 for y(0)=1.\n        # The solution is y(t) = 1 / (1-t).\n        return y**2\n\n    test_cases = [\n        {'name': 'A', 'f': f_A, 't0': 0.0, 'y0': 1.0, 't_end': 10.0},\n        {'name': 'B', 'f': f_B, 't0': 0.0, 'y0': 0.0, 't_end': 10.0},\n        {'name': 'C', 'f': f_C, 't0': 0.0, 'y0': 1.0, 't_end': 0.9}\n    ]\n    tols = np.array([1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n    p = 5.0\n    epsilon = 0.15\n\n    # --- RKF45 adaptive solver implementation ---\n    def rkf45_solver(f, t0, y0, t_end, tol):\n        \"\"\"\n        Solves a scalar ODE using the adaptive Runge-Kutta-Fehlberg 4(5) method.\n        \"\"\"\n        # Butcher tableau coefficients for RKF45\n        c = np.array([0, 1/4, 3/8, 12/13, 1, 1/2], dtype=float)\n        a21 = 1/4\n        a31, a32 = 3/32, 9/32\n        a41, a42, a43 = 1932/2197, -7200/2197, 7296/2197\n        a51, a52, a53, a54 = 439/216, -8, 3680/513, -845/4104\n        a61, a62, a63, a64, a65 = -8/27, 2, -3544/2565, 1859/4104, -11/40\n\n        # Coefficients for the 5th-order solution (used for advancing state)\n        b_star = np.array([16/135, 0, 6656/12825, 28561/56430, -9/50, 2/55], dtype=float)\n        # Coefficients for error estimation (b_star - b)\n        err_coeffs = b_star - np.array([25/216, 0, 1408/2565, 2197/4104, -1/5, 0], dtype=float)\n\n        # Solver parameters\n        safety = 0.9\n        fac_min, fac_max = 0.1, 5.0\n        h_max = (t_end - t0)\n        h_min = 1e-12\n\n        t = t0\n        y = y0\n        h = (t_end - t0) * 1e-3  # Initial guess for step size\n        accepted_steps = 0\n\n        while t < t_end:\n            h = min(h, t_end - t)\n            \n            step_accepted = False\n            while not step_accepted:\n                # Prevent step size from becoming too small\n                if abs(h) < h_min:\n                    raise RuntimeError(f\"Step size {h} is below minimum {h_min}\")\n                \n                 # Calculate the six stages (k_i)\n                k1 = f(t, y)\n                k2 = f(t + c[1] * h, y + h * (a21 * k1))\n                k3 = f(t + c[2] * h, y + h * (a31 * k1 + a32 * k2))\n                k4 = f(t + c[3] * h, y + h * (a41 * k1 + a42 * k2 + a43 * k3))\n                k5 = f(t + c[4] * h, y + h * (a51 * k1 + a52 * k2 + a53 * k3 + a54 * k4))\n                k6 = f(t + c[5] * h, y + h * (a61 * k1 + a62 * k2 + a63 * k3 + a64 * k4 + a65 * k5))\n                ks = np.array([k1, k2, k3, k4, k5, k6])\n\n                # Estimate the local error\n                error_est = abs(h * np.dot(err_coeffs, ks))\n\n                if error_est <= tol:\n                    step_accepted = True\n                    t += h\n                    y += h * np.dot(b_star, ks) # Advance with higher-order solution\n                    accepted_steps += 1\n                \n                # Determine scaling factor for the next step size\n                if error_est == 0.0:\n                    h_scale_factor = fac_max\n                else:\n                    h_scale_factor = safety * (tol / error_est)**(1.0 / p)\n\n                h_scale_factor = min(fac_max, max(fac_min, h_scale_factor))\n                h *= h_scale_factor\n                h = min(h, h_max)\n\n        return accepted_steps\n    \n    # --- Main test loop ---\n    results = []\n    for case in test_cases:\n        N_values = []\n        for tol_val in tols:\n            N = rkf45_solver(case['f'], case['t0'], case['y0'], case['t_end'], tol_val)\n            N_values.append(N)\n        \n        # --- Linear regression to find the slope ---\n        log_tols = np.log(tols)\n        log_N = np.log(np.array(N_values))\n        n = len(log_tols)\n\n        sum_x = np.sum(log_tols)\n        sum_y = np.sum(log_N)\n        sum_xy = np.sum(log_tols * log_N)\n        sum_x2 = np.sum(log_tols**2)\n        \n        # Calculate slope 's' of the line log(N) vs log(tol)\n        numerator = n * sum_xy - sum_x * sum_y\n        denominator = n * sum_x2 - sum_x**2\n        \n        if denominator == 0:\n            raise ValueError(\"Cannot perform linear regression: zero denominator.\")\n        \n        slope = numerator / denominator\n        \n        # --- Validation check ---\n        theoretical_slope = -1.0 / p\n        is_close = np.abs(slope - theoretical_slope) <= epsilon\n        results.append(is_close)\n        \n    final_output = f\"[{','.join(str(r) for r in results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "An adaptive solver is only as reliable as its error estimator. This exercise presents a \"numerical forensics\" challenge where a single faulty coefficient in a simple embedded pair leads to pathological behavior. By analyzing this defective method, you will see firsthand how a seemingly minor implementation error can produce a misleading error estimate of zero, causing the solver to fail catastrophically on a simple test problem. This practice underscores the importance of understanding the intricate dependencies within the Butcher tableau. ",
            "id": "3224521",
            "problem": "Consider the idea behind an embedded Runge–Kutta–Fehlberg (RKF) pair: two explicit Runge–Kutta formulas of different algebraic orders are computed with shared stage evaluations so that a cheap local error estimate is obtained from their difference. Start from the standard $2$-stage explicit Runge–Kutta method with an embedded partner (a Fehlberg-style $2(1)$ pair, often called the Heun–Euler pair). The intended Butcher tableau is\n$$\n\\begin{array}{c|cc}\n0 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n\\hline\n & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n & 1 & 0\n\\end{array}\n$$\nNow deliberately introduce a single incorrect coefficient by replacing $a_{21}=1$ with $a_{21}=0$, yielding the faulty embedded pair with tableau\n$$\n\\begin{array}{c|cc}\n0 & 0 & 0 \\\\\n1 & \\color{red}{0} & 0 \\\\\n\\hline\n & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n & 1 & 0\n\\end{array}\n$$\nAnalyze this faulty embedded pair on the scalar linear test equation $y'=-y$ with constant step size $h>0$ by reasoning from the definitions of explicit Runge–Kutta methods, the notion of linear stability via the stability function $R(z)$ with $z=h\\lambda$, and the embedded error estimate defined as the difference between the two updates. Which of the following statements are true?\n\nA. The primary method degenerates to the explicit Euler method; the embedded error estimate is identically zero on $y'=-y$, so an adaptive algorithm that trusts this estimate alone may increase $h$ past the linear stability limit $h=2$, after which $\\lvert y_n\\rvert$ grows without bound.\n\nB. The primary method keeps algebraic order $2$ on smooth problems because the weights $b_1=b_2=\\tfrac{1}{2}$ are unchanged.\n\nC. For $y'=-y$, one step satisfies $y_{n+1}=(1-h)\\,y_n$ and the embedded estimate $\\hat e_n:=y_{n+1}-\\tilde y_{n+1}$ is exactly zero for all $h$.\n\nD. The modified stability function is $R(z)=1+z+\\tfrac{1}{2}z^2$, so the negative real stability interval is $0<h<2$; in addition, the embedded error estimate equals $y_n\\,\\tfrac{1}{2}z^2$, which properly flags instability near $h=2$.\n\nSelect all that apply.",
            "solution": "The user wants an analysis of a faulty embedded Runge-Kutta method.\n\n### Step 1: Validation of the Problem Statement\n\nFirst, I will extract the givens of the problem and validate its premises.\n\n**Extracted Givens:**\n1.  **Problem Type:** Analysis of an embedded Runge-Kutta-Fehlberg (RKF) pair.\n2.  **Original (Correct) Tableau:** A $2(1)$ Heun-Euler pair.\n    $$\n    \\begin{array}{c|cc}\n    0 & 0 & 0 \\\\\n    1 & 1 & 0 \\\\\n    \\hline\n     & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n     & 1 & 0\n    \\end{array}\n    $$\n3.  **Faulty Tableau:** The coefficient $a_{21}=1$ is replaced by $a_{21}=0$.\n    $$\n    \\begin{array}{c|cc}\n    0 & 0 & 0 \\\\\n    1 & 0 & 0 \\\\\n    \\hline\n     & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n     & 1 & 0\n    \\end{array}\n    $$\n4.  **Test Equation:** The scalar linear test equation $y'=-y$.\n5.  **Step Size:** Constant step size $h>0$.\n6.  **Analysis tools:** Definitions of explicit Runge-Kutta methods, linear stability function $R(z)$ with $z=h\\lambda$, and the embedded error estimate $\\hat e_n = y_{n+1} - \\tilde y_{n+1}$.\n\n**Validation:**\n1.  **Scientific Groundness:** The problem is firmly located within the standard theory of numerical methods for ordinary differential equations. It employs well-defined concepts like Butcher tableaus, Runge-Kutta methods, stability functions, and embedded error estimators. The premise of analyzing a method with a faulty coefficient is a common and valid pedagogical tool to test fundamental understanding.\n2.  **Well-Posedness:** The problem is well-posed. The modified Butcher tableau is explicitly given, the test equation is standard, and the questions pertain to derivable properties of the method. All necessary information is provided.\n3.  **Objectivity:** The problem is stated in precise, objective mathematical language.\n4.  **Consistency and Completeness:** The problem is self-contained and free of contradictions.\n\n**Verdict:** The problem statement is valid. I will proceed with the solution.\n\n### Step 2: Analysis of the Faulty Method\n\nLet's define the general form of a $2$-stage explicit Runge-Kutta method for an initial value problem $y' = f(t, y)$, $y(t_n)=y_n$.\nThe stage evaluations are:\n$$k_1 = f(t_n, y_n)$$\n$$k_2 = f(t_n + c_2 h, y_n + h a_{21} k_1)$$\nThe primary (higher-order) update is:\n$$y_{n+1} = y_n + h(b_1 k_1 + b_2 k_2)$$\nThe embedded (lower-order) update is:\n$$\\tilde y_{n+1} = y_n + h(\\tilde b_1 k_1 + \\tilde b_2 k_2)$$\n\nFrom the faulty Butcher tableau, we have the coefficients:\n- $c_1 = 0$, $c_2 = 1$\n- $a_{21} = 0$\n- Primary weights: $b_1 = \\frac{1}{2}$, $b_2 = \\frac{1}{2}$\n- Embedded weights: $\\tilde b_1 = 1$, $\\tilde b_2 = 0$\n\nLet's compute the stages for the faulty method:\n$$k_1 = f(t_n, y_n)$$\n$$k_2 = f(t_n + 1 \\cdot h, y_n + h \\cdot 0 \\cdot k_1) = f(t_n + h, y_n)$$\nNotice that the second stage $k_2$ is evaluated at the start of the interval in the $y$-component, $y_n$, completely ignoring the information from the first stage $k_1$.\n\nThe primary update is:\n$$y_{n+1} = y_n + h\\left(\\frac{1}{2} k_1 + \\frac{1}{2} k_2\\right) = y_n + \\frac{h}{2}\\left(f(t_n, y_n) + f(t_n + h, y_n)\\right)$$\nThe embedded update is:\n$$\\tilde y_{n+1} = y_n + h(1 \\cdot k_1 + 0 \\cdot k_2) = y_n + hf(t_n, y_n)$$\nThe embedded method is clearly the explicit Euler method.\n\nThe local error estimate is:\n$$\\hat e_n = y_{n+1} - \\tilde y_{n+1} = \\left(y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_n + h, y_n))\\right) - (y_n + hf(t_n, y_n))$$\n$$\\hat e_n = \\frac{h}{2}(f(t_n+h, y_n) - f(t_n, y_n))$$\n\n### Step 3: Application to the Test Equation $y'=-y$\n\nNow, we apply these formulas to the test equation $y'=-y$, for which $f(t, y) = -y$. The function $f$ is autonomous (independent of $t$).\n$$k_1 = f(y_n) = -y_n$$\n$$k_2 = f(y_n) = -y_n$$\nNote that because $f$ is autonomous, $f(t_n+h, y_n) = f(y_n)$.\n\nThe primary update becomes:\n$$y_{n+1} = y_n + h\\left(\\frac{1}{2}(-y_n) + \\frac{1}{2}(-y_n)\\right) = y_n - h y_n = (1-h)y_n$$\nThis is the update rule for the explicit Euler method. The problem is posed with $y'=\\lambda y$ where $\\lambda=-1$, so $z=h\\lambda=-h$. The update is $y_{n+1}=(1+z)y_n$. The stability function is $R(z) = 1+z$.\n\nThe embedded update becomes:\n$$\\tilde y_{n+1} = y_n + h(-y_n) = (1-h)y_n$$\nThis is also the explicit Euler update.\n\nThe local error estimate is:\n$$\\hat e_n = y_{n+1} - \\tilde y_{n+1} = (1-h)y_n - (1-h)y_n = 0$$\nThe error estimate is identically zero for the test equation $y'=-y$.\n\n### Step 4: Evaluation of Options\n\n**A. The primary method degenerates to the explicit Euler method; the embedded error estimate is identically zero on $y'=-y$, so an adaptive algorithm that trusts this estimate alone may increase $h$ past the linear stability limit $h=2$, after which $\\lvert y_n\\rvert$ grows without bound.**\n\n-   **\"The primary method degenerates to the explicit Euler method\"**: Our analysis on the test equation $y'=-y$ shows that $y_{n+1}=(1-h)y_n$, which is precisely the explicit Euler update. More generally, for any autonomous equation $y'=f(y)$, the method becomes $y_{n+1} = y_n + h f(y_n)$, which is explicit Euler. This statement is correct.\n-   **\"the embedded error estimate is identically zero on $y'=-y$\"**: Our calculation showed $\\hat e_n=0$. This is correct.\n-   **\"so an adaptive algorithm...\"**: The stability function is $R(z)=1+z$. For $z=-h$, stability requires $|R(-h)| = |1-h| \\le 1$, which gives $0 \\le h \\le 2$. The linear stability limit is $h=2$. An adaptive step-size algorithm aims to keep the estimated error $\\hat e_n$ within a certain tolerance. Since the estimate is always $0$, the algorithm will perceive the solution as exact and will increase the step size $h$ to improve efficiency. It is highly likely to increase $h$ beyond the stability limit of $h=2$.\n-   **\"after which $\\lvert y_n\\rvert$ grows without bound\"**: For $h>2$, we have $|1-h|>1$. This implies $|y_{n+1}| = |1-h| |y_n| > |y_n|$. The numerical solution will grow in magnitude at each step, leading to instability. The reasoning is sound.\n\nVerdict on A: **Correct**.\n\n**B. The primary method keeps algebraic order $2$ on smooth problems because the weights $b_1=b_2=\\tfrac{1}{2}$ are unchanged.**\n\nTo check the order, we find the local truncation error (LTE). The true solution expands as $y(t_{n+1}) = y_n + h y'(t_n) + \\frac{h^2}{2} y''(t_n) + O(h^3)$, where $y' = f$ and $y'' = \\frac{d}{dt}f(t, y(t)) = f_t + f_y f$.\nThe numerical method gives $y_{n+1} = y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_n+h, y_n))$. Expanding $f(t_n+h, y_n)$ in $h$:\n$y_{n+1} = y_n + \\frac{h}{2}(f(t_n, y_n) + [f(t_n, y_n) + h f_t(t_n, y_n) + O(h^2)]) = y_n + hf(t_n, y_n) + \\frac{h^2}{2}f_t(t_n, y_n) + O(h^3)$.\nThe LTE is $y(t_{n+1}) - y_{n+1} = (\\frac{h^2}{2}(f_t + f_y f)) - (\\frac{h^2}{2}f_t) + O(h^3) = \\frac{h^2}{2}f_y f + O(h^3)$.\nThe LTE is of order $O(h^2)$, which means the method is of algebraic order $1$. The statement that it keeps order $2$ is false. The reason given is also insufficient; order depends on all of the $A, b, c$ coefficients satisfying a set of \"order conditions\".\n\nVerdict on B: **Incorrect**.\n\n**C. For $y'=-y$, one step satisfies $y_{n+1}=(1-h)\\,y_n$ and the embedded estimate $\\hat e_n:=y_{n+1}-\\tilde y_{n+1}$ is exactly zero for all $h$.**\n\n-   **\"one step satisfies $y_{n+1}=(1-h)\\,y_n$\"**: As derived in Step 3, this is the exact update formula for the primary method when applied to $y'=-y$. This part is correct.\n-   **\"the embedded estimate ... is exactly zero for all $h$\"**: As derived in Step 3, $\\hat e_n = (1-h)y_n - (1-h)y_n = 0$. This part is also correct.\n\nVerdict on C: **Correct**.\n\n**D. The modified stability function is $R(z)=1+z+\\tfrac{1}{2}z^2$, so the negative real stability interval is $0<h<2$; in addition, the embedded error estimate equals $y_n\\,\\tfrac{1}{2}z^2$, which properly flags instability near $h=2$.**\n\n-   **\"The modified stability function is $R(z)=1+z+\\tfrac{1}{2}z^2$\"**: We found in Step 3 that the stability function for the faulty primary method is $R(z)=1+z$. The function $1+z+\\frac{1}{2}z^2$ corresponds to the *original, correct* Heun's method. This statement is false.\n-   **\"so the negative real stability interval is $0<h<2$\"**: The stability interval for the actual method ($R(z)=1+z$) is indeed $0 \\le h \\le 2$. The stability interval for the stated (but incorrect) $R(z)=1+z+\\frac{1}{2}z^2$ is also $0 \\le h \\le 2$. So the interval is coincidentally correct, but it is derived from a false premise.\n-   **\"the embedded error estimate equals $y_n\\,\\tfrac{1}{2}z^2$\"**: We found the error estimate is $\\hat e_n=0$. The expression $y_n \\frac{1}{2}z^2 = y_n \\frac{1}{2}(-h)^2 = \\frac{h^2}{2}y_n$ is non-zero. This expression is the correct error estimate for the *original, correct* Heun-Euler pair, not the faulty one. This statement is false.\n-   **\"which properly flags instability\"**: Since the actual error estimate is $0$, it fails completely to flag anything, including instability. This statement is false.\n\nVerdict on D: **Incorrect**.\n\n### Summary\n-   Option A is correct.\n-   Option B is incorrect.\n-   Option C is correct.\n-   Option D is incorrect.\n\nThe true statements are A and C.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}