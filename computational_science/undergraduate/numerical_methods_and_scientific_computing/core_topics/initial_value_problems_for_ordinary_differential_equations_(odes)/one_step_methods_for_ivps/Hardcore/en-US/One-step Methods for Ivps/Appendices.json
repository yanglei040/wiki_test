{
    "hands_on_practices": [
        {
            "introduction": "In numerical integration, choosing the step size $h$ presents a fundamental dilemma. A large $h$ is fast but risks high truncation error, while a very small $h$ increases computational cost and accumulates floating-point round-off errors. This practice allows you to explore this trade-off firsthand by numerically finding the 'sweet spot'—the optimal fixed step size $h^{\\star}$ that minimizes the total error for the fourth-order Runge-Kutta method . You will discover that for every problem, there is a limit to the accuracy achievable with fixed-step integration in finite-precision arithmetic.",
            "id": "3259605",
            "problem": "Consider the initial value problem (IVP) defined by the ordinary differential equation $y^{\\prime}(t) = f(t,y)$ with initial condition $y(t_{0}) = y_{0}$. A one-step method with fixed step size $h$ advances an approximation $y_{n}$ at $t_{n}$ to $y_{n+1}$ at $t_{n+1} = t_{n} + h$ using only information from the current step. For such methods, the total error at a fixed final time $T$ typically arises from two sources: truncation error due to the method’s finite order and floating-point round-off error from finite-precision arithmetic. The truncation error depends on $h$ through the method’s order and the smoothness of $f$, while round-off error depends on the number of steps $N = T/h$ and the arithmetic used. Your task is to numerically find, for specific IVPs and a single one-step method, the fixed step size $h$ that minimizes the absolute final error by balancing truncation and floating-point round-off error.\n\nFundamental base to rely on:\n- The IVP definition $y^{\\prime}(t) = f(t,y)$ with $y(t_{0}) = y_{0}$.\n- The definition of a one-step method, such as the classical fourth-order Runge–Kutta method, which constructs $y_{n+1}$ from $(t_{n}, y_{n})$ and $h$ using a fixed formula.\n- The well-tested fact that, for a method of order $p$ applied on a fixed interval, the global truncation error scales like a constant times $h^{p}$ when $h$ is sufficiently small and the solution is smooth.\n- The well-tested fact that floating-point round-off errors accumulate over many arithmetic operations, and the number of steps $N$ scales like $T/h$ for a fixed interval $[0,T]$.\n\nYour program must use the classical fourth-order Runge–Kutta (RK4) method as the one-step integrator for all test cases. You must not use any built-in ordinary differential equation solvers. For each specified test case, you must:\n- Integrate from $t_{0}$ to $T$ with a fixed step size $h = T/N$ for each $N$ in a provided candidate list.\n- Compute the absolute final error $\\left|y_{N} - y_{\\text{true}}(T)\\right|$ using the exact solution $y_{\\text{true}}(t)$ provided for that test case.\n- Choose the step size $h^{\\star}$ among the candidates that minimizes the absolute final error. If two or more candidates produce final errors that are numerically indistinguishable within an absolute tolerance of $10^{-18}$, select the largest $h$ among those tied candidates.\n\nAngle unit: whenever $\\sin(\\cdot)$ or $\\cos(\\cdot)$ appear, interpret their arguments in radians.\n\nUse double-precision floating-point arithmetic. No randomness is allowed.\n\nTest suite:\n- Case $1$ (growth, interior minimum expected):\n  - $f(t,y) = y$, $y(0) = 1$, $T = 50$. Exact solution $y_{\\text{true}}(t) = e^{t}$.\n- Case $2$ (decay, boundary behavior expected):\n  - $f(t,y) = -2y$, $y(0) = 1$, $T = 50$. Exact solution $y_{\\text{true}}(t) = e^{-2t}$.\n- Case $3$ (forced linear, bounded solution):\n  - $f(t,y) = \\sin(t) - y$, $y(0) = 1$, $T = 30$. Exact solution $y_{\\text{true}}(t) = \\tfrac{3}{2} e^{-t} + \\tfrac{1}{2}(\\sin t - \\cos t)$.\n\nFor each case, evaluate the following candidate subdivisions $N \\in \\{50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200\\}$ and thus $h = T/N$. For each $N$, perform exactly $N$ fixed-size RK4 steps to reach time $T$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above. Specifically, the output must be the list $[h^{\\star}_{1}, h^{\\star}_{2}, h^{\\star}_{3}]$, where each $h^{\\star}_{i}$ is the chosen optimal $h$ for Case $i$. The entries must be printed as decimal floating-point numbers. No units are involved.\n\nImportant constraints:\n- Implement the RK4 one-step method explicitly. Do not reuse any library time-steppers.\n- Ensure that every candidate $h$ produces exactly $N$ uniform steps so that the final time is exactly $T$.\n- Use the exact solutions given above to compute $\\left|y_{N} - y_{\\text{true}}(T)\\right|$ accurately.\n\nThe final output format must be exactly one line: for example, $[h^{\\star}_{1},h^{\\star}_{2},h^{\\star}_{3}]$.",
            "solution": "The user has provided a problem that requires finding the optimal fixed step size $h$ for solving several initial value problems (IVPs) using the classical fourth-order Runge-Kutta (RK4) method. The optimization criterion is the minimization of the absolute error at a fixed final time $T$, which involves balancing the method's truncation error against the accumulated floating-point round-off error.\n\n### Step 1: Extract Givens\n\n- **IVP Definition**: $y^{\\prime}(t) = f(t,y)$ with initial condition $y(t_{0}) = y_{0}$.\n- **Numerical Method**: The classical fourth-order Runge-Kutta (RK4) method is to be used.\n- **Task**: For a given list of candidate step counts $N$, find the step size $h = T/N$ that minimizes the absolute final error $\\left|y_{N} - y_{\\text{true}}(T)\\right|$.\n- **Candidate Subdivisions $N$**: The set of candidates for the number of steps is $N \\in \\{50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200\\}$.\n- **Tie-Breaking Rule**: If multiple candidates for $h$ yield a minimum error (values are within an absolute tolerance of $10^{-18}$), the largest such $h$ must be chosen.\n- **Arithmetic**: Double-precision floating-point arithmetic.\n- **Test Case 1**:\n  - $f(t,y) = y$\n  - $y(0) = 1$\n  - $T = 50$\n  - Exact solution: $y_{\\text{true}}(t) = e^{t}$\n- **Test Case 2**:\n  - $f(t,y) = -2y$\n  - $y(0) = 1$\n  - $T = 50$\n  - Exact solution: $y_{\\text{true}}(t) = e^{-2t}$\n- **Test Case 3**:\n  - $f(t,y) = \\sin(t) - y$\n  - $y(0) = 1$\n  - $T = 30$\n  - Exact solution: $y_{\\text{true}}(t) = \\tfrac{3}{2} e^{-t} + \\tfrac{1}{2}(\\sin t - \\cos t)$\n- **Angle Units**: Arguments of trigonometric functions are in radians.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is reviewed against the validation criteria:\n\n- **Scientifically Grounded**: The problem is fundamentally sound. It explores a classic concept in numerical analysis: the trade-off between truncation error, which decreases with smaller step sizes ($h$), and round-off error, which accumulates with the number of computational steps ($N = T/h$). The RK4 method is a standard, well-understood algorithm for solving ODEs. The chosen ODEs are canonical examples with known analytical solutions, suitable for error analysis.\n- **Well-Posed**: The problem is well-posed. It provides all necessary components: the differential equations, initial conditions, a specific numerical method, a discrete set of step sizes to test, and an unambiguous objective function (minimizing absolute final error). The explicit tie-breaking rule ensures a unique solution.\n- **Objective**: The problem is stated in objective, mathematical terms. The evaluation is based on quantitative error calculation, leaving no room for subjective interpretation.\n\nAll other checks (completeness, consistency, feasibility, etc.) are passed. The problem does not exhibit any of the defined flaws.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe core of the problem is to implement a numerical experiment. For each of the three IVPs, we must simulate the solution's evolution using the RK4 method over a range of step sizes and identify which step size yields the most accurate result at the final time $T$.\n\n**The Fourth-Order Runge-Kutta (RK4) Method**\nGiven an IVP $y'(t) = f(t,y)$, $y(t_n) = y_n$, the RK4 method computes the approximation $y_{n+1}$ at time $t_{n+1} = t_n + h$ using the following sequence of calculations:\n$$\nk_1 = f(t_n, y_n)\n$$\n$$\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right)\n$$\n$$\nk_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right)\n$$\n$$\nk_4 = f\\left(t_n + h, y_n + hk_3\\right)\n$$\nThe next value $y_{n+1}$ is then given by a weighted average of these intermediate slopes:\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\nThis formula is applied iteratively $N$ times to advance the solution from the initial condition $(t_0, y_0)$ to the final time $T = t_0 + N \\cdot h$.\n\n**Error Dynamics**\nThe total absolute error at the final time $T$, denoted $E(h)$, is a combination of two primary error sources:\n1.  **Global Truncation Error ($E_T$)**: This error is inherent to the method's approximation of the true solution. For RK4, which is a fourth-order method, this error scales with the fourth power of the step size, i.e., $E_T \\propto h^4$. As $h \\to 0$, this error diminishes rapidly.\n2.  **Round-off Error ($E_R$)**: This error arises from the finite precision of floating-point arithmetic. Each step of the RK4 method introduces a small round-off error. These errors accumulate over the $N = (T-t_0)/h$ steps. A simplified model suggests that the total round-off error grows with the number of steps, so $E_R \\propto N \\propto 1/h$.\n\nThe total error can be modeled as $E(h) \\approx C_1 h^4 + C_2/h$. This function has a characteristic shape: it is large for large $h$ (where truncation error dominates), decreases as $h$ is reduced, reaches a minimum at an optimal step size $h^{\\star}$, and then increases again for very small $h$ (where round-off error begins to dominate). Our task is to find this $h^{\\star}$ empirically from a discrete set of candidates.\n\n**Computational Procedure**\nFor each of the three test cases, the following algorithm is executed:\n1.  The true solution at the final time, $y_{\\text{true}}(T)$, is computed once for reference.\n2.  A loop iterates through each candidate number of steps $N$ from the provided list $\\{50, 100, \\dots, 51200\\}$.\n3.  Inside the loop, for each $N$:\n    a. The step size $h$ is calculated as $h = (T - t_0) / N$. Note that for all cases, $t_0=0$, so $h = T/N$.\n    b. The RK4 integration is performed, starting with $y_0$ at $t_0$ and executing exactly $N$ steps to obtain the numerical approximation $y_N$ at time $T$.\n    c. The absolute error is computed: $\\epsilon = |y_N - y_{\\text{true}}(T)|$.\n    d. The pair $(h, \\epsilon)$ is stored.\n4.  After testing all candidate values of $N$, the collected list of $(h, \\epsilon)$ pairs is analyzed to find the minimum error, $\\epsilon_{min}$.\n5.  In accordance with the tie-breaking rule, all candidates $(h, \\epsilon)$ where $|\\epsilon - \\epsilon_{min}| \\le 10^{-18}$ are identified.\n6.  Among these tied candidates, the one with the largest value of $h$ is selected as the optimal step size, $h^{\\star}$.\n7.  The three resulting optimal step sizes, one for each test case, are collected and formatted for the final output.\n\nThis procedure is systematically applied to determine the optimal step size for each IVP, revealing the practical effects of the error trade-off in numerical integration.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rk4_solver(f, t0, y0, T, N):\n    \"\"\"\n    Solves an IVP y'(t) = f(t, y) using the classical RK4 method.\n\n    Args:\n        f (callable): The function f(t, y) defining the ODE.\n        t0 (float): The initial time.\n        y0 (float): The initial value y(t0).\n        T (float): The final time.\n        N (int): The number of steps to take.\n\n    Returns:\n        float: The numerical approximation of y(T).\n    \"\"\"\n    h = (T - t0) / float(N)\n    t = float(t0)\n    y = float(y0)\n\n    for _ in range(N):\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        y += (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n        t += h\n        \n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and find the optimal step size h.\n    \"\"\"\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"y_true\": lambda t: np.exp(t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 50.0,\n        },\n        {\n            \"f\": lambda t, y: -2.0 * y,\n            \"y_true\": lambda t: np.exp(-2.0 * t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 50.0,\n        },\n        {\n            \"f\": lambda t, y: np.sin(t) - y,\n            \"y_true\": lambda t: 1.5 * np.exp(-t) + 0.5 * (np.sin(t) - np.cos(t)),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 30.0,\n        },\n    ]\n\n    N_candidates = [50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200]\n    optimal_hs = []\n    TIE_TOLERANCE = 1e-18\n\n    for case in test_cases:\n        f = case[\"f\"]\n        y_true_func = case[\"y_true\"]\n        t0, y0, T = case[\"t0\"], case[\"y0\"], case[\"T\"]\n        \n        y_exact_at_T = y_true_func(T)\n        \n        results_for_case = []\n        for N in N_candidates:\n            h = (T - t0) / float(N)\n            y_N = rk4_solver(f, t0, y0, T, N)\n            error = np.abs(y_N - y_exact_at_T)\n            results_for_case.append({\"h\": h, \"error\": error})\n\n        # Find the minimum error among all candidates\n        if not results_for_case:\n            continue\n            \n        min_error = min(r['error'] for r in results_for_case)\n        \n        # Identify all candidates that are tied for the minimum error\n        # A tie is defined as being within an absolute tolerance of the minimum\n        tied_candidates = [\n            r for r in results_for_case \n            if np.isclose(r['error'], min_error, rtol=0, atol=TIE_TOLERANCE)\n        ]\n        \n        # From the tied candidates, select the one with the largest step size h\n        if not tied_candidates: # Should not happen if results_for_case is not empty\n             continue\n\n        optimal_h = max(r['h'] for r in tied_candidates)\n        optimal_hs.append(optimal_h)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, optimal_hs))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many crucial systems in physics and engineering, from planetary orbits to electrical circuits, are oscillatory. For these problems, simply getting the amplitude right isn't enough; preserving the correct phase over long periods is essential. This exercise  challenges you to investigate the long-term behavior of different one-step methods—Euler, Leapfrog, and RK4—on the simple harmonic oscillator, highlighting how seemingly small per-step errors can accumulate into significant phase drift.",
            "id": "3259724",
            "problem": "Consider the initial value problem (IVP) given by the simple harmonic oscillator, which is defined by the system of ordinary differential equations\n$$\\frac{dx}{dt} = v,\\quad \\frac{dv}{dt} = -\\omega^2 x,$$\nwith initial conditions\n$$x(0) = 1,\\quad v(0) = 0,$$\nwhere $\\omega$ is the angular frequency in radians per second. The exact solution is periodic with period $$T = \\frac{2\\pi}{\\omega}.$$\nThe exact motion traces a circle in the scaled phase space obtained by the mapping $x \\mapsto x$ and $v \\mapsto v/\\omega$, so that the exact state satisfies\n$$x(t) = \\cos(\\omega t),\\quad \\frac{v(t)}{\\omega} = -\\sin(\\omega t).$$\nDefine the exact phase angle at time $t$ by\n$$\\phi_{\\mathrm{exact}}(t) = \\omega t,$$\nand define the numerical phase angle at discrete time $t_n = n h$ by\n$$\\phi_n = \\operatorname{atan2}\\!\\left(\\frac{v_n}{\\omega}, x_n\\right),$$\nwhere $h$ is the time step and $(x_n, v_n)$ is the numerical approximation to $(x(t_n), v(t_n))$. The phase error at time $t_n$ is the wrapped difference\n$$\\Delta\\phi_n = \\mathrm{wrap}_{(-\\pi,\\pi]}\\!\\left(\\phi_n - \\phi_{\\mathrm{exact}}(t_n)\\right),$$\nwhere $\\mathrm{wrap}_{(-\\pi,\\pi]}(\\theta)$ returns the unique representative of $\\theta$ modulo $2\\pi$ in the interval $(-\\pi,\\pi]$. All angles must be expressed in radians.\n\nStarting from the fundamental definition of a one-step method for IVPs, namely a map that advances the state from $t_n$ to $t_{n+1} = t_n + h$ using only the information at $t_n$ and possibly evaluations within the same step, implement the following one-step numerical integrators for the system:\n- Explicit Euler method.\n- Leapfrog method implemented in the one-step velocity Verlet form, which uses the current state and within-step evaluations to advance $(x_n, v_n)$ to $(x_{n+1}, v_{n+1})$ in a single step.\n- Classical fourth-order Runge–Kutta method.\n\nYour program must compute the phase error after many periods for each method by integrating up to $t_{\\mathrm{end}} = N_{\\mathrm{periods}}\\,T$ with a constant time step $h = 2\\pi/m$ for specified integers $m$, and report only the final phase error $\\Delta\\phi_{n_{\\mathrm{final}}}$ at $t_{\\mathrm{end}}$. Use $\\omega = 1$ so that $T = 2\\pi$ and the exact final phase at $t_{\\mathrm{end}}$ is $\\phi_{\\mathrm{exact}}(t_{\\mathrm{end}}) = 2\\pi N_{\\mathrm{periods}}$, which is congruent to $0$ modulo $2\\pi$.\n\nTest Suite:\nFor each method in the fixed order of methods $\\{\\text{Euler}, \\text{Leapfrog}, \\text{RK4}\\}$, and for each step parameter $m \\in \\{4, 20, 200\\}$, compute the final phase error for two durations $N_{\\mathrm{periods}} \\in \\{10, 100\\}$, that is, at $t_{\\mathrm{end}} = N_{\\mathrm{periods}} \\cdot 2\\pi$. This defines a test suite of $18$ cases. The expected output for each case is a single real number representing $\\Delta\\phi_{n_{\\mathrm{final}}}$ in radians. All angle quantities must be in radians.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the $18$ test cases as a comma-separated list enclosed in square brackets, in the following fixed order:\n- For method $\\text{Euler}$: $(m=4, N_{\\mathrm{periods}}=10)$, $(m=4, N_{\\mathrm{periods}}=100)$, $(m=20, N_{\\mathrm{periods}}=10)$, $(m=20, N_{\\mathrm{periods}}=100)$, $(m=200, N_{\\mathrm{periods}}=10)$, $(m=200, N_{\\mathrm{periods}}=100)$.\n- For method $\\text{Leapfrog}$: the same six cases in the same $m$ and $N_{\\mathrm{periods}}$ order.\n- For method $\\text{RK4}$: the same six cases in the same $m$ and $N_{\\mathrm{periods}}$ order.\n\nThe final printed line must be exactly of the form\n$$[\\Delta\\phi_1,\\Delta\\phi_2,\\ldots,\\Delta\\phi_{18}]$$\nwith no additional text. All values must be in radians. No physical units other than radians are involved, and angles must not be expressed with a percentage sign.",
            "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem in the field of numerical methods for ordinary differential equations. All necessary information is provided, and the objective is clear and formalizable.\n\nThe task is to compute the phase error of three different one-step numerical integrators—Explicit Euler, Leapfrog (Velocity Verlet), and an explicit fourth-order Runge-Kutta (RK4)—when applied to the simple harmonic oscillator problem.\n\nThe system of first-order ordinary differential equations (ODEs) for the simple harmonic oscillator is given by:\n$$\n\\frac{dx}{dt} = v \\\\\n\\frac{dv}{dt} = -\\omega^2 x\n$$\nWe are given the angular frequency $\\omega=1\\,\\mathrm{rad/s}$, so the system simplifies to:\n$$\n\\frac{dx}{dt} = v \\\\\n\\frac{dv}{dt} = -x\n$$\nThis can be written in vector form $\\mathbf{y}'(t) = \\mathbf{f}(\\mathbf{y}(t))$, where $\\mathbf{y} = \\begin{pmatrix} x \\\\ v \\end{pmatrix}$ and the function $\\mathbf{f}$ is:\n$$\n\\mathbf{f}(x, v) = \\begin{pmatrix} v \\\\ -x \\end{pmatrix}\n$$\nThe initial conditions are $\\mathbf{y}(0) = \\begin{pmatrix} x(0) \\\\ v(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nA one-step numerical method approximates the solution at time $t_{n+1} = t_n + h$ using the state at time $t_n$, denoted $\\mathbf{y}_n = \\begin{pmatrix} x_n \\\\ v_n \\end{pmatrix}$. We will now detail the implementation for each required method.\n\n**1. Explicit Euler Method**\nThis is a first-order method. The update rule is $\\mathbf{y}_{n+1} = \\mathbf{y}_n + h \\mathbf{f}(\\mathbf{y}_n)$. In component form, this gives:\n$$\nx_{n+1} = x_n + h v_n \\\\\nv_{n+1} = v_n + h (-x_n) = v_n - h x_n\n$$\n\n**2. Leapfrog Method (Velocity Verlet Form)**\nThis method is a second-order, symplectic integrator. In its one-step velocity Verlet formulation, it advances the state $(x_n, v_n)$ to $(x_{n+1}, v_{n+1})$ as follows:\nFirst, an intermediate velocity at the half-step, $v_{n+1/2}$, is computed. The acceleration is $a(x) = -x$.\n$$ v_{n+1/2} = v_n + \\frac{h}{2} a(x_n) = v_n - \\frac{h}{2} x_n $$\nNext, the position is updated to the full step using this intermediate velocity:\n$$ x_{n+1} = x_n + h v_{n+1/2} $$\nFinally, the velocity is updated from the half-step to the full step using the acceleration at the new position, $a(x_{n+1})$:\n$$ v_{n+1} = v_{n+1/2} + \\frac{h}{2} a(x_{n+1}) = v_{n+1/2} - \\frac{h}{2} x_{n+1} $$\nThese steps are performed in sequence within a single time step integration.\n\n**3. Classical Fourth-Order Runge-Kutta Method (RK4)**\nThis is a fourth-order method. The update rule is $\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\frac{h}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)$, where the stages $\\mathbf{k}_i$ are vectors with components for $x$ and $v$. For our specific system $\\mathbf{f}(x,v) = (v, -x)$:\n\nStage 1:\n$$ \\mathbf{k}_1 = \\mathbf{f}(x_n, v_n) = \\begin{pmatrix} v_n \\\\ -x_n \\end{pmatrix} $$\nStage 2:\n$$ \\mathbf{k}_2 = \\mathbf{f}\\left(x_n + \\frac{h}{2} k_{1,x}, v_n + \\frac{h}{2} k_{1,v}\\right) = \\mathbf{f}\\left(x_n + \\frac{h}{2} v_n, v_n - \\frac{h}{2} x_n\\right) = \\begin{pmatrix} v_n - \\frac{h}{2} x_n \\\\ -\\left(x_n + \\frac{h}{2} v_n\\right) \\end{pmatrix} $$\nStage 3:\n$$ \\mathbf{k}_3 = \\mathbf{f}\\left(x_n + \\frac{h}{2} k_{2,x}, v_n + \\frac{h}{2} k_{2,v}\\right) = \\begin{pmatrix} v_n + \\frac{h}{2}k_{2,v} \\\\ -\\left(x_n + \\frac{h}{2}k_{2,x}\\right) \\end{pmatrix} $$\nStage 4:\n$$ \\mathbf{k}_4 = \\mathbf{f}\\left(x_n + h k_{3,x}, v_n + h k_{3,v}\\right) = \\begin{pmatrix} v_n + h k_{3,v} \\\\ -\\left(x_n + h k_{3,x}\\right) \\end{pmatrix} $$\n\nThe final state $(x_{n+1}, v_{n+1})$ is then:\n$$\nx_{n+1} = x_n + \\frac{h}{6} (k_{1,x} + 2k_{2,x} + 2k_{3,x} + k_{4,x}) \\\\\nv_{n+1} = v_n + \\frac{h}{6} (k_{1,v} + 2k_{2,v} + 2k_{3,v} + k_{4,v})\n$$\n\n**Phase Error Calculation**\nFor each test case defined by the method, the step parameter $m$, and the number of periods $N_{\\mathrm{periods}}$:\n1.  The time step is $h = 2\\pi/m$.\n2.  The final integration time is $t_{\\mathrm{end}} = N_{\\mathrm{periods}} \\cdot T = N_{\\mathrm{periods}} \\cdot 2\\pi$ (since $T=2\\pi/\\omega$ and $\\omega=1$).\n3.  The total number of steps is $N_{\\mathrm{steps}} = t_{\\mathrm{end}}/h = (N_{\\mathrm{periods}} \\cdot 2\\pi) / (2\\pi/m) = N_{\\mathrm{periods}} \\cdot m$.\n4.  The chosen integrator is applied $N_{\\mathrm{steps}}$ times, starting from $(x_0, v_0) = (1, 0)$, to obtain the final numerical state $(x_{n_{\\mathrm{final}}}, v_{n_{\\mathrm{final}}})$.\n5.  The final numerical phase angle is calculated as $\\phi_{n_{\\mathrm{final}}} = \\operatorname{atan2}(v_{n_{\\mathrm{final}}}/\\omega, x_{n_{\\mathrm{final}}}) = \\operatorname{atan2}(v_{n_{\\mathrm{final}}}, x_{n_{\\mathrm{final}}})$.\n6.  The exact phase angle at the final time is $\\phi_{\\mathrm{exact}}(t_{\\mathrm{end}}) = \\omega t_{\\mathrm{end}} = 1 \\cdot (N_{\\mathrm{periods}} \\cdot 2\\pi) = 2\\pi N_{\\mathrm{periods}}$.\n7.  The final phase error is the wrapped difference:\n    $$ \\Delta\\phi_{n_{\\mathrm{final}}} = \\mathrm{wrap}_{(-\\pi,\\pi]}\\!\\left(\\phi_{n_{\\mathrm{final}}} - \\phi_{\\mathrm{exact}}(t_{\\mathrm{end}})\\right) $$\n    The wrapping function $\\mathrm{wrap}_{(-\\pi,\\pi]}(\\theta)$ maps an angle $\\theta$ to its unique equivalent in the interval $(-\\pi, \\pi]$. This can be implemented numerically by the transformation $\\theta' = ((\\theta + \\pi) \\pmod{2\\pi}) - \\pi$, with a special case to map any result of $-\\pi$ to $\\pi$ to match the specified interval.\n\nThe program will execute this procedure for all $18$ test cases and report the resulting phase errors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the final phase error for three numerical\n    methods applied to the simple harmonic oscillator.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    methods = [\"Euler\", \"Leapfrog\", \"RK4\"]\n    m_values = [4, 20, 200]\n    n_periods_values = [10, 100]\n    \n    test_cases = []\n    for method in methods:\n        for m in m_values:\n            for n_periods in n_periods_values:\n                test_cases.append({'method': method, 'm': m, 'n_periods': n_periods})\n    \n    results = []\n\n    def wrap_to_pi(theta):\n        \"\"\"\n        Wraps an angle theta in radians to the interval (-pi, pi].\n        \"\"\"\n        val = (theta + np.pi) % (2 * np.pi) - np.pi\n        # The modulo operation gives a result in [-pi, pi).\n        # We map -pi to pi to match the interval (-pi, pi].\n        if np.isclose(val, -np.pi):\n            return np.pi\n        return val\n\n    # ODE definition: x' = v, v' = -omega^2 * x. Here omega = 1.\n    omega = 1.0\n\n    # ----- Integrator Implementations -----\n\n    def euler_step(x, v, h):\n        \"\"\"Performs a single step of the Explicit Euler method.\"\"\"\n        x_new = x + h * v\n        v_new = v - h * (omega**2) * x\n        return x_new, v_new\n\n    def leapfrog_verlet_step(x, v, h):\n        \"\"\"Performs a single step of the Leapfrog (Velocity Verlet) method.\"\"\"\n        # Acceleration at current position\n        a_n = -(omega**2) * x\n        # Half-step velocity update\n        v_half = v + 0.5 * h * a_n\n        # Full-step position update\n        x_new = x + h * v_half\n        # Acceleration at new position\n        a_n1 = -(omega**2) * x_new\n        # Final velocity update\n        v_new = v_half + 0.5 * h * a_n1\n        return x_new, v_new\n\n    def rk4_step(x, v, h):\n        \"\"\"Performs a single step of the classical RK4 method.\"\"\"\n        \n        def f(x_in, v_in):\n            return v_in, -(omega**2) * x_in\n\n        # k1\n        k1_x, k1_v = f(x, v)\n        \n        # k2\n        k2_x, k2_v = f(x + 0.5 * h * k1_x, v + 0.5 * h * k1_v)\n        \n        # k3\n        k3_x, k3_v = f(x + 0.5 * h * k2_x, v + 0.5 * h * k2_v)\n        \n        # k4\n        k4_x, k4_v = f(x + h * k3_x, v + h * k3_v)\n        \n        # Update\n        x_new = x + (h / 6.0) * (k1_x + 2*k2_x + 2*k3_x + k4_x)\n        v_new = v + (h / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n        \n        return x_new, v_new\n\n    integrators = {\n        \"Euler\": euler_step,\n        \"Leapfrog\": leapfrog_verlet_step,\n        \"RK4\": rk4_step,\n    }\n\n    for case in test_cases:\n        method = case['method']\n        m = case['m']\n        n_periods = case['n_periods']\n\n        # Setup simulation parameters\n        h = 2.0 * np.pi / m\n        t_end = n_periods * 2.0 * np.pi\n        num_steps = int(round(t_end / h))\n        \n        # Initial conditions\n        x, v = 1.0, 0.0\n\n        # Select integration function\n        step_func = integrators[method]\n\n        # Run simulation\n        for _ in range(num_steps):\n            x, v = step_func(x, v, h)\n        \n        # Calculate final phase error\n        # Final numerical phase\n        phi_numerical = np.arctan2(v / omega, x)\n        \n        # Final exact phase\n        phi_exact = omega * t_end\n        \n        # Phase error is the wrapped difference\n        phase_error = wrap_to_pi(phi_numerical - phi_exact)\n        \n        results.append(phase_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The most powerful ODE solvers don't use a fixed step size; they adapt it on the fly, taking small steps when the solution changes rapidly and large steps when it behaves smoothly. This practice guides you through the implementation of such a smart-stepping algorithm from the ground up . You will use an embedded Runge-Kutta pair to estimate the local error at each step and build a controller that automatically adjusts the step size to meet a desired accuracy tolerance, a technique at the core of modern scientific computing.",
            "id": "3259704",
            "problem": "Consider the Initial Value Problem (IVP) defined by an Ordinary Differential Equation (ODE) of the form $y'(t) = f(t,y)$ with initial condition $y(t_0) = y_0$. A one-step method advances the solution from $(t,y)$ to $(t+h, y_{\\text{new}})$ using only information from the current step. In an explicit Runge-Kutta (RK) method, the increment is constructed from a weighted combination of stage derivatives. An embedded RK pair provides two approximations of different orders computed from the same stages, enabling a local error estimate for adaptive step-size control.\n\nYour task is to implement an embedded Runge-Kutta pair that is consistent with the Bogacki–Shampine $2(3)$ construction and use it to drive an adaptive step-size selection algorithm. The algorithm must:\n\n- Compute stage derivatives and produce a higher-order approximation $y^{[p]}$ and a lower-order approximation $y^{[q]}$ at each step, with $pq$.\n- Estimate the local truncation error from the difference $y^{[p]} - y^{[q]}$.\n- Accept a step when a scaled error norm is less than or equal to $1$ and reject otherwise.\n- Adjust the step size $h$ using the asymptotic scaling relation between the error and the step size, ensuring numerical stability by applying a safety factor and bounding the growth and decay of $h$.\n- Respect the final time $t_f$ by reducing the last step as necessary so that $t$ reaches $t_f$ exactly.\n\nYou must implement the adaptive integrator for scalar ODEs with the following scaled error norm per step:\n$$\nE = \\frac{|y^{[p]} - y^{[q]}|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[p]}|)},\n$$\nwhere $\\mathrm{atol}$ is the absolute tolerance and $\\mathrm{rtol}$ is the relative tolerance. The step is accepted if $E \\leq 1$. The step-size controller must be based on the principle that an error of order $h^{m}$ implies $h_{\\text{new}} \\propto h\\,E^{-1/m}$ for some integer $m$ that matches the order of the error estimator. Include a multiplicative safety factor and cap the growth and decay by prescribed bounds.\n\nAngle quantities must be interpreted in radians whenever trigonometric functions appear.\n\nImplement the adaptive solver and apply it to the following test suite of IVPs. For each case, return the numerical approximation of $y(t_f)$ as a floating-point number.\n\n- Case $1$ (happy path, exponentially stable):\n  - $f(t,y) = -y$, $y(0) = 1$, $t_0 = 0$, $t_f = 5$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests general performance and stability on a smooth problem.\n\n- Case $2$ (time-dependent growth with trigonometric forcing, radians):\n  - $f(t,y) = y\\sin(t)$, $y(0) = 1$, $t_0 = 0$, $t_f = 3$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-7}$, $\\mathrm{atol} = 10^{-10}$.\n  - Rationale: tests handling of time-dependent coefficients and requires angle in radians.\n\n- Case $3$ (moderately stiff-like linear decay):\n  - $f(t,y) = -15y$, $y(0) = 1$, $t_0 = 0$, $t_f = 1$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests step-size adaptation under faster decay.\n\n- Case $4$ (nonlinear growth near a singularity):\n  - $f(t,y) = y^2$, $y(0) = 1$, $t_0 = 0$, $t_f = 0.9$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests robustness near a blow-up at $t = 1$.\n\n- Case $5$ (zero dynamics, edge case):\n  - $f(t,y) = 0$, $y(0) = 5$, $t_0 = 0$, $t_f = 10$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-8}$, $\\mathrm{atol} = 10^{-12}$.\n  - Rationale: tests the controller when the estimated error is identically zero.\n\nController parameters common to all cases:\n- Safety factor $s = 0.9$,\n- Minimum growth factor $g_{\\min} = 0.2$,\n- Maximum growth factor $g_{\\max} = 5.0$,\n- Initial step size $h_0 = 10^{-3}$,\n- Minimum step size $h_{\\min} = 10^{-12}$,\n- Maximum step size $h_{\\max} = (t_f - t_0)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, for example $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the floating-point approximation of $y(t_f)$ for Case $i$.",
            "solution": "The core of the task is to construct an adaptive one-step integrator from first principles, grounded in the definitions of an Initial Value Problem (IVP), Runge-Kutta (RK) methods, and local truncation error.\n\nAn IVP specifies $y'(t) = f(t,y)$ with $y(t_0) = y_0$. A one-step method computes $y_{n+1}$ from $(t_n,y_n)$ using a single step size $h_n$ without reference to earlier history. In an explicit Runge-Kutta (RK) method, the new value is a weighted combination of stage derivatives. The general explicit RK construction is:\n$$\nk_1 = f(t_n, y_n),\\quad\nk_2 = f(t_n + c_2 h, y_n + h a_{21} k_1),\\quad \\dots,\\quad\nk_s = f(t_n + c_s h, y_n + h \\sum_{j=1}^{s-1} a_{sj} k_j),\n$$\nand an order-$p$ approximation is\n$$\ny^{[p]}_{n+1} = y_n + h \\sum_{j=1}^s b_j k_j.\n$$\nAn embedded pair provides two sets of weights, $b_j$ and $\\hat b_j$, evaluated with the same stages to deliver $y^{[p]}$ and $y^{[q]}$ with $pq$. The difference\n$$\n\\Delta = y^{[p]}_{n+1} - y^{[q]}_{n+1}\n$$\nis computable with negligible extra cost and scales with a known power of $h$ determined by the construction.\n\nFor the Bogacki–Shampine $2(3)$ pair, the method uses $s=4$ stages with nodes $c_2 = \\tfrac{1}{2}$, $c_3 = \\tfrac{3}{4}$, $c_4 = 1$. The internal coefficients are $a_{21} = \\tfrac{1}{2}$, $a_{32} = \\tfrac{3}{4}$ with $a_{31} = 0$, and $a_{41} = \\tfrac{2}{9}$, $a_{42} = \\tfrac{1}{3}$, $a_{43} = \\tfrac{4}{9}$. The higher-order weights (order $p=3$) are\n$$\nb_1 = \\tfrac{2}{9},\\quad b_2 = \\tfrac{1}{3},\\quad b_3 = \\tfrac{4}{9},\\quad b_4 = 0,\n$$\nand the lower-order weights (order $q=2$) are\n$$\n\\hat b_1 = \\tfrac{7}{24},\\quad \\hat b_2 = \\tfrac{1}{4},\\quad \\hat b_3 = \\tfrac{1}{3},\\quad \\hat b_4 = \\tfrac{1}{8}.\n$$\nThe stages and approximations are computed as\n$$\n\\begin{aligned}\nk_1 = f(t, y),\\\\\nk_2 = f\\Big(t + \\tfrac{1}{2}h,\\, y + h\\,\\tfrac{1}{2}\\,k_1\\Big),\\\\\nk_3 = f\\Big(t + \\tfrac{3}{4}h,\\, y + h\\,\\tfrac{3}{4}\\,k_2\\Big),\\\\\ny^{[3]} = y + h\\Big(\\tfrac{2}{9}k_1 + \\tfrac{1}{3}k_2 + \\tfrac{4}{9}k_3\\Big),\\\\\nk_4 = f\\Big(t + h,\\, y^{[3]}\\Big),\\\\\ny^{[2]} = y + h\\Big(\\tfrac{7}{24}k_1 + \\tfrac{1}{4}k_2 + \\tfrac{1}{3}k_3 + \\tfrac{1}{8}k_4\\Big).\n\\end{aligned}\n$$\nThe embedded difference $\\Delta = y^{[3]} - y^{[2]}$ provides an error estimator with leading behavior $O(h^3)$; that is, it scales like $C h^3$ for smooth $f$. This scaling justifies a controller of the form\n$$\nh_{\\text{new}} = h \\cdot s \\cdot E^{-1/3},\n$$\nwhere $s$ is a safety factor and $E$ is a scaled error norm. To ensure numerical robustness, we bound the multiplicative growth/decay factor by $g_{\\min} \\leq s E^{-1/3} \\leq g_{\\max}$, and we also clip $h_{\\text{new}}$ to $[h_{\\min}, h_{\\max}]$. When $E \\leq 1$, the step is accepted and $y$ advances to $y^{[3]}$; otherwise, the step is rejected and recomputed with a smaller $h$.\n\nThe scaled error norm for scalar problems is chosen to balance absolute and relative tolerances:\n$$\nE = \\frac{|\\Delta|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[3]}|)}.\n$$\nThis ensures that the acceptance criterion $E \\leq 1$ controls the error relative to the magnitude of the solution while preventing division by very small numbers when $y$ is near zero.\n\nAlgorithmic steps for integrating from $t_0$ to $t_f$:\n- Initialize $t = t_0$, $y = y_0$, choose $h$ in $[h_{\\min}, h_{\\max}]$ (for example $h_0$ as given), and set controller parameters $s$, $g_{\\min}$, $g_{\\max}$.\n- While $t  t_f$:\n  - If $t + h  t_f$, set $h = t_f - t$ to land exactly at $t_f$.\n  - Compute $k_1$, $k_2$, $k_3$, $y^{[3]}$, $k_4$, and $y^{[2]}$.\n  - Compute $E$ from $y$, $y^{[3]}$, $y^{[2]}$, $\\mathrm{atol}$, and $\\mathrm{rtol}$.\n  - If $E \\leq 1$, accept the step: set $t \\gets t + h$, $y \\gets y^{[3]}$.\n  - Compute the candidate growth factor $g = s \\cdot E^{-1/3}$; if $E=0$, set $g = g_{\\max}$.\n  - Bound $g$ to $[g_{\\min}, g_{\\max}]$, then update $h \\gets \\mathrm{clip}(h \\cdot g, h_{\\min}, h_{\\max})$.\n  - If a step is rejected ($E  1$), update $h$ as above and recompute without advancing $t$ or $y$.\n- Return $y(t_f)$.\n\nWe now briefly analyze each test case and its expected behavior:\n- Case $1$: $y'(t) = -y$, exact solution $y(t) = e^{-t}$, so $y(5) = e^{-5}$. The method should take moderate steps and converge rapidly.\n- Case $2$: $y'(t) = y\\sin(t)$ in radians with exact solution $y(t) = \\exp(1 - \\cos t)$, giving $y(3) = \\exp(1 - \\cos 3)$. The algorithm must handle time-dependent forcing smoothly.\n- Case $3$: $y'(t) = -15y$ with exact $y(1) = e^{-15}$, requiring smaller steps initially due to rapid decay, but the controller will increase $h$ as $y$ shrinks.\n- Case $4$: $y'(t) = y^2$ with exact $y(t) = \\frac{1}{1 - t}$ for $t1$, so $y(0.9) = 10$. The method must adaptively reduce $h$ as $t$ approaches the blow-up at $t=1$.\n- Case $5$: $y'(t) = 0$ yields constant solution $y(t) = 5$; the error estimator is identically zero, and the controller will enlarge $h$ up to $h_{\\max}$.\n\nBy implementing the Bogacki–Shampine $2(3)$ stages and the controller derived from the $O(h^3)$ error estimate, the adaptive solver will provide $y(t_f)$ for all cases, printed in the specified format.",
            "answer": "```python\n# Python 3.12, numpy 1.23.5 allowed; no other libraries.\nimport numpy as np\n\ndef rk23_bogacki_shampine_step(f, t, y, h):\n    \"\"\"\n    Perform one Bogacki-Shampine 2(3) step for a scalar ODE y' = f(t,y).\n    Returns (y_high, y_low) where y_high is the 3rd-order solution, y_low is the 2nd-order embedded solution.\n    \"\"\"\n    k1 = f(t, y)\n    k2 = f(t + 0.5 * h, y + h * 0.5 * k1)\n    k3 = f(t + 0.75 * h, y + h * 0.75 * k2)\n    # 3rd-order solution\n    y3 = y + h * ( (2.0/9.0) * k1 + (1.0/3.0) * k2 + (4.0/9.0) * k3 )\n    # Stage 4 evaluated at t+h, y3\n    k4 = f(t + h, y3)\n    # 2nd-order embedded solution\n    y2 = y + h * ( (7.0/24.0) * k1 + (1.0/4.0) * k2 + (1.0/3.0) * k3 + (1.0/8.0) * k4 )\n    return y3, y2\n\ndef integrate_adaptive(f, t0, tf, y0, rtol, atol,\n                       h0=1e-3, hmin=1e-12, hmax=None,\n                       safety=0.9, growth_min=0.2, growth_max=5.0):\n    \"\"\"\n    Adaptive integrator using Bogacki-Shampine 2(3) pair for scalar ODEs.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    if hmax is None:\n        hmax = abs(tf - t0)\n    h = max(hmin, min(h0, hmax))\n    # Direction of integration\n    direction = 1.0 if tf >= t0 else -1.0\n    h *= direction\n    hmin_signed = hmin * direction\n    hmax_signed = hmax * direction\n\n    # Main integration loop\n    # Guard for max iterations to prevent infinite loops in pathological cases\n    max_steps = 10_000_000\n    steps = 0\n    while (direction > 0 and t  tf) or (direction  0 and t > tf):\n        steps += 1\n        if steps > max_steps:\n            # Fallback: give current y\n            break\n\n        # Adjust step to not overshoot tf\n        remaining = tf - t\n        if direction * h > direction * remaining:\n            h = remaining\n\n        # Take one RK23 step\n        y3, y2 = rk23_bogacki_shampine_step(f, t, y, h)\n\n        # Scaled error norm (scalar)\n        scale = atol + rtol * max(abs(y), abs(y3))\n        # Prevent zero scale\n        if scale == 0.0:\n            scale = atol\n        err = abs(y3 - y2) / scale\n\n        # Accept or reject\n        if err = 1.0:\n            # Accept the step\n            t = t + h\n            y = y3\n            # Compute growth factor; estimator scales ~ h^3\n            if err == 0.0:\n                g = growth_max\n            else:\n                g = safety * err ** (-1.0 / 3.0)\n            # Bound growth factor\n            g = max(growth_min, min(g, growth_max))\n            # Update h and clip\n            h = h * g\n            # Clip to [hmin, hmax] with sign\n            if direction > 0:\n                h = min(max(h, hmin_signed), hmax_signed)\n            else:\n                h = max(min(h, hmin_signed), hmax_signed)\n        else:\n            # Reject step; decrease h\n            g = safety * err ** (-1.0 / 3.0)\n            g = max(growth_min, min(g, growth_max))\n            h = h * g\n            # Ensure not below minimum\n            if direction > 0:\n                h = max(h, hmin_signed)\n            else:\n                h = min(h, hmin_signed)\n            # If h becomes too small, break to avoid infinite loop\n            if abs(h)  hmin:\n                # Cannot reduce further; accept and break\n                # This is a conservative fallback\n                t = t + h\n                y = y3\n                break\n\n    return y\n\ndef solve():\n    # Define the test cases\n    # Case 1: y' = -y, y(0) = 1, tf = 5\n    def f1(t, y): return -y\n\n    # Case 2: y' = y*sin(t), radians, y(0) = 1, tf = 3\n    def f2(t, y): return y * np.sin(t)\n\n    # Case 3: y' = -15 y, y(0) = 1, tf = 1\n    def f3(t, y): return -15.0 * y\n\n    # Case 4: y' = y^2, y(0) = 1, tf = 0.9\n    def f4(t, y): return y * y\n\n    # Case 5: y' = 0, y(0) = 5, tf = 10\n    def f5(t, y): return 0.0\n\n    test_cases = [\n        # (f, t0, tf, y0, rtol, atol)\n        (f1, 0.0, 5.0, 1.0, 1e-6, 1e-9),\n        (f2, 0.0, 3.0, 1.0, 1e-7, 1e-10),\n        (f3, 0.0, 1.0, 1.0, 1e-6, 1e-9),\n        (f4, 0.0, 0.9, 1.0, 1e-6, 1e-9),\n        (f5, 0.0, 10.0, 5.0, 1e-8, 1e-12),\n    ]\n\n    results = []\n    for f, t0, tf, y0, rtol, atol in test_cases:\n        ytf = integrate_adaptive(\n            f=f, t0=t0, tf=tf, y0=y0, rtol=rtol, atol=atol,\n            h0=1e-3, hmin=1e-12, hmax=abs(tf - t0),\n            safety=0.9, growth_min=0.2, growth_max=5.0\n        )\n        results.append(ytf)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}