{
    "hands_on_practices": [
        {
            "introduction": "在求解微分方程时，我们通常无法得知其精确解。那么，我们如何评估数值解的准确性呢？本练习将介绍一种无需精确解即可估计局部截断误差的强大技术——步长加倍法。通过比较不同步长下的计算结果，我们可以推断出误差的大小，并验证其与方法阶数的关系，这是构建自适应步长求解器的关键第一步。",
            "id": "3259715",
            "problem": "考虑一个常微分方程（ODE）的初值问题（IVP）：找到一个函数 $y(t)$，使得对于 $t \\in [t_0, t_0 + h]$，有 $y'(t) = f(t,y(t))$，且 $y(t_0) = y_0$，其中精确解没有闭式形式。你将应用一种单步数值方法从 $t_0$ 推进到 $t_0 + h$，并使用步长加倍法在不使用精确解的情况下估计局部截断误差。你的任务是从第一性原理推导步长加倍误差估计量，并数值验证该估计量随步长缩放的阶数。\n\n使用以下与上下文相符的基础：\n- $p$ 阶单步方法的定义：当从精确值 $y(t_0)$ 开始时，其从 $t_0$ 到 $t_0 + h$ 的单步局部截断误差的缩放行为类似于 $C h^{p+1}$，其中常数 $C$ 取决于 $f$、$t_0$ 和 $y_0$，而与 $h$ 无关。\n- 局部截断误差的定义：当从精确初始值 $y(t_0)$ 开始时，单步数值推进结果与精确值 $y(t_0 + h)$ 之间的差值。\n- 步长加倍：比较大小为 $h$ 的单步与大小为 $h/2$ 的两个连续步，两者都从相同的初始值 $y(t_0)$ 开始。\n\n你必须实现以下单步方法：\n- 前向欧拉法（$p = 1$ 阶）。\n- 显式中点法（$p = 2$ 阶）。\n- 四阶经典龙格–库塔法（$p = 4$ 阶）。\n\n使用 ODE\n$$\nf(t,y) = e^{-t^2}\\sin(y) + t\\,y,\n$$\n初始条件为 $y(t_0) = y_0$，其中 $t_0 = 0$，$y_0 = 0.3$。该 ODE 的精确解应被视为未知。\n\n根据单步方法的定义，推导出一个仅依赖于可计算量的、针对更精细解（两个半步）的局部截断误差的步长加倍估计量。然后，通过为两个步长 $h$ 和 $h/2$ 计算该估计量，并计算观测到的阶数\n$$\nq = \\frac{\\ln\\left(\\frac{E(h)}{E(h/2)}\\right)}{\\ln(2)},\n$$\n来验证此估计量的缩放阶数，其中 $E(\\cdot)$ 表示在总推进大小为 $h$ 的情况下，更精细解的步长加倍局部误差估计。\n\n测试套件：\n- 情况 1：前向欧拉法，使用 $h = 0.2$ 和 $h/2 = 0.1$。\n- 情况 2：显式中点法，使用 $h = 0.2$ 和 $h/2 = 0.1$。\n- 情况 3：四阶经典龙格–库塔法，使用 $h = 0.2$ 和 $h/2 = 0.1$。\n- 情况 4（使用较小步长的边缘情况）：显式中点法，使用 $h = 0.05$ 和 $h/2 = 0.025$。\n\n对于每种情况，计算观测阶 $q$ 并将其与理论值 $p+1$ 进行比较，使用容差 $\\varepsilon = 0.3$；如果 $\\lvert q - (p+1) \\rvert \\le \\varepsilon$，则声明该情况已验证。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔结果列表。对于测试套件中的每种情况，包括观测阶 $q$（一个浮点数），紧随其后的是验证布尔值。因此，输出应包含按上述情况排序的八个条目，形成一个形如 $[q_1,\\text{pass}_1,q_2,\\text{pass}_2,q_3,\\text{pass}_3,q_4,\\text{pass}_4]$ 的列表。",
            "solution": "目标是推导单步数值方法的步长加倍误差估计量，并数值验证其收敛阶。这将针对前向欧拉法、显式中点法和经典龙格-库塔法进行。\n\n### 1. 步长加倍误差估计量的推导\n\n设初值问题由 $y'(t) = f(t, y(t))$ 定义，初始条件为 $y(t_0) = y_0$。设采用一个 $p$ 阶单步数值方法来近似解。\n\n局部截断误差 (LTE) 是从精确解 $y(t_n)$ 开始，在大小为 $h$ 的单步中产生的误差。它被定义为在下一个时间点，数值近似值与精确解之间的差。对于一个 $p$ 阶方法，从 $t_n$ 到 $t_n + h$ 的一步中的 LTE 具有渐近展开：\n$$\n\\text{LTE}(h) = y_{\\text{approx}}(t_n+h) - y(t_n+h) = C h^{p+1} + O(h^{p+2})\n$$\n其中常数 $C$ 取决于函数 $f$ 及其在 $(t_n, y(t_n))$ 处的导数，但与 $h$ 无关。\n\n我们考虑两种数值近似方法，将解从 $t_0$ 推进到 $t_0 + h$：\n1. 一个大小为 $h$ 的“粗略”单步。设所得近似值为 $y_1$。\n2. 两个连续的“精细”步，每步大小为 $h/2$。设最终近似值为 $y_2$。\n\n两种计算都从相同的初始值 $y(t_0) = y_0$ 开始。根据局部截断误差的定义，我们可以写出每种近似的误差，假设我们从精确值 $y(t_0)$ 开始。\n\n对于粗略步，误差为：\n$$\nLTE_1 = y_1 - y(t_0+h) = C h^{p+1} + O(h^{p+2})\n$$\n\n对于精细步，总误差是每一步局部截断误差之和。经过两步大小为 $h/2$ 的计算后，误差近似为：\n$$\nLTE_2 = y_2 - y(t_0+h) \\approx 2 \\times \\left( C \\left(\\frac{h}{2}\\right)^{p+1} \\right) = \\frac{C h^{p+1}}{2^p} + O(h^{p+2})\n$$\n此近似假设从前半步传播的误差不会显著改变后半步的局部误差常数 $C$，对于小的 $h$ 这是一个有效的假设。\n\n我们现在有一个包含两个未知数（$y(t_0+h)$ 和误差系数项 $C h^{p+1}$）的方程组。我们希望为更精确的精细解的误差 $LTE_2$ 找到一个估计量。我们可以通过减去两个近似表达式来消去未知的精确解 $y(t_0+h)$：\n$$\ny_2 - y_1 = (y(t_0+h) + LTE_2) - (y(t_0+h) + LTE_1) = LTE_2 - LTE_1\n$$\n代入误差的渐近形式，我们得到：\n$$\ny_2 - y_1 \\approx \\left(\\frac{C h^{p+1}}{2^p}\\right) - (C h^{p+1}) = C h^{p+1} \\left(\\frac{1}{2^p} - 1\\right)\n$$\n我们可以解出项 $C h^{p+1}$：\n$$\nC h^{p+1} \\approx \\frac{y_2 - y_1}{\\frac{1}{2^p} - 1}\n$$\n问题要求为更精细解的局部截断误差提供一个估计量，我们将其表示为 $E(h)$。该估计量是 $LTE_2$ 的一个近似：\n$$\nE(h) \\approx LTE_2 \\approx \\frac{C h^{p+1}}{2^p}\n$$\n代入 $C h^{p+1}$ 的表达式，得到精细网格解的步长加倍误差估计量：\n$$\nE(h) \\approx \\frac{1}{2^p} \\left( \\frac{y_2 - y_1}{\\frac{1}{2^p} - 1} \\right) = \\frac{y_2 - y_1}{1 - 2^p}\n$$\n该公式仅使用可计算量（$y_1$ 和 $y_2$）和方法的阶数 $p$ 来估计误差 $y_2 - y(t_0+h)$。\n\n### 2. 估计量阶数的验证\n\n推导出的误差估计量是 $E(h) = \\frac{y_2 - y_1}{1 - 2^p}$。如其推导过程所示，该估计量的主导项是 $\\frac{C h^{p+1}}{2^p}$。这意味着估计量本身随步长 $h$ 的缩放方式为：\n$$\nE(h) \\propto h^{p+1}\n$$\n为了数值验证这一点，我们为两个不同的步长 $h$ 和 $h/2$ 计算估计量。设它们为 $E(h)$ 和 $E(h/2)$。然后我们可以使用以下公式计算观测到的收敛阶 $q$：\n$$\nq = \\frac{\\ln\\left(\\frac{|E(h)|}{|E(h/2)|}\\right)}{\\ln(2)}\n$$\n使用绝对值是一种标准做法，以确保对数的参数为正，特别是当 $h$ 尚未完全进入渐近区域时。如果估计量按 $E(h) \\approx K h^{p+1}$ 缩放（其中 $K$ 为某个常数），则该比值变为：\n$$\n\\frac{|E(h)|}{|E(h/2)|} \\approx \\frac{|K h^{p+1}|}{|K (h/2)^{p+1}|} = 2^{p+1}\n$$\n将此代入 $q$ 的公式中，得到：\n$$\nq \\approx \\frac{\\ln(2^{p+1})}{\\ln(2)} = \\frac{(p+1)\\ln(2)}{\\ln(2)} = p+1\n$$\n因此，我们期望观测到的阶 $q$ 约等于 $p+1$。\n\n### 3. 数值实现细节\n\n将对 ODE $y' = f(t,y) = e^{-t^2}\\sin(y) + t\\,y$ 进行数值验证，初始条件为 $y(0) = 0.3$。将要实现的单步方法是：\n- **前向欧拉法 ($p=1$)：** $y_{n+1} = y_n + h f(t_n, y_n)$\n- **显式中点法 ($p=2$)：** $k_1 = f(t_n, y_n)$; $k_2 = f(t_n + h/2, y_n + (h/2)k_1)$; $y_{n+1} = y_n + h k_2$\n- **经典 RK4 ($p=4$)：** $k_1 = f(t_n, y_n)$; $k_2 = f(t_n + h/2, y_n + (h/2)k_1)$; $k_3 = f(t_n + h/2, y_n + (h/2)k_2)$; $k_4 = f(t_n + h, y_n + h k_3)$; $y_{n+1} = y_n + (h/6)(k_1 + 2k_2 + 2k_3 + k_4)$\n\n对于每个测试用例，我们执行以下步骤：\n1.  选择一个方法（及其阶数 $p$）和主步长 $h_a$。用于比较的第二个步长将是 $h_b = h_a/2$。\n2.  计算 $E(h_a)$：\n    a. 使用大小为 $h_a$ 的一步计算 $y_1$。\n    b. 使用两步大小为 $h_a/2$ 的计算 $y_2$。\n    c. 计算 $E(h_a) = (y_2 - y_1) / (1 - 2^p)$。\n3.  计算 $E(h_b)$：\n    a. 使用大小为 $h_b$ 的一步计算 $y'_1$。\n    b. 使用两步大小为 $h_b/2$ 的计算 $y'_2$。\n    c. 计算 $E(h_b) = (y'_2 - y'_1) / (1 - 2^p)$。\n4.  计算观测阶 $q = \\ln(|E(h_a)|/|E(h_b)|) / \\ln(2)$。\n5.  通过检查是否 $|q - (p+1)| \\le \\varepsilon$ 来验证结果，其中 $\\varepsilon = 0.3$。\n指定测试套件的结果由提供的 Python 代码生成。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to derive, implement, and verify the step-doubling error estimator.\n    \"\"\"\n    # Define the ODE function f(t, y)\n    def f(t, y):\n        return np.exp(-t**2) * np.sin(y) + t * y\n\n    # Define the one-step methods\n    def forward_euler_step(y_start, t_start, h, f_func):\n        return y_start + h * f_func(t_start, y_start)\n\n    def explicit_midpoint_step(y_start, t_start, h, f_func):\n        k1 = f_func(t_start, y_start)\n        k2 = f_func(t_start + h / 2.0, y_start + (h / 2.0) * k1)\n        return y_start + h * k2\n\n    def rk4_step(y_start, t_start, h, f_func):\n        k1 = f_func(t_start, y_start)\n        k2 = f_func(t_start + h / 2.0, y_start + (h / 2.0) * k1)\n        k3 = f_func(t_start + h / 2.0, y_start + (h / 2.0) * k2)\n        k4 = f_func(t_start + h, y_start + h * k3)\n        return y_start + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    # Store methods and their orders in a dictionary\n    methods = {\n        'euler': {'func': forward_euler_step, 'p': 1},\n        'midpoint': {'func': explicit_midpoint_step, 'p': 2},\n        'rk4': {'func': rk4_step, 'p': 4}\n    }\n\n    # Test suite from the problem statement\n    # Format: (method_name, h_large)\n    test_cases = [\n        ('euler', 0.2),\n        ('midpoint', 0.2),\n        ('rk4', 0.2),\n        ('midpoint', 0.05),\n    ]\n\n    # Initial conditions\n    t0 = 0.0\n    y0 = 0.3\n    \n    # Verification tolerance\n    epsilon = 0.3\n\n    results = []\n\n    def compute_error_estimate(method_func, p, h, t_start, y_start, f_func):\n        \"\"\"\n        Computes the step-doubling error estimate for an advance of size h.\n        \"\"\"\n        # One coarse step of size h\n        y1 = method_func(y_start, t_start, h, f_func)\n        \n        # Two fine steps of size h/2\n        h_half = h / 2.0\n        y_mid = method_func(y_start, t_start, h_half, f_func)\n        y2 = method_func(y_mid, t_start + h_half, h_half, f_func)\n        \n        # Richardson extrapolation based error formula for the finer solution\n        error_est = (y2 - y1) / (1.0 - 2.0**p)\n        return error_est\n\n    for method_name, h_large in test_cases:\n        method = methods[method_name]['func']\n        p = methods[method_name]['p']\n        \n        h_small = h_large / 2.0\n        \n        # Calculate error estimate for step size h_large\n        E_h_large = compute_error_estimate(method, p, h_large, t0, y0, f)\n        \n        # Calculate error estimate for step size h_small\n        E_h_small = compute_error_estimate(method, p, h_small, t0, y0, f)\n        \n        # Calculate observed order of convergence of the estimator\n        # Using abs() is crucial for numerical stability\n        ratio = abs(E_h_large) / abs(E_h_small)\n        q = np.log(ratio) / np.log(2.0)\n        \n        # Verify if observed order is within tolerance of theoretical order p+1\n        verified = abs(q - (p + 1.0)) = epsilon\n        \n        results.extend([q, verified])\n\n    # Format the final output string\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "减小步长 $h$ 能够有效降低截断误差，但这是否意味着步长越小越好？本练习将引导你探索一个实际计算中至关重要的问题：截断误差与浮点运算累积的舍入误差之间的权衡。你将通过数值实验，为一个给定的问题找到一个能够最小化总误差的“最优”步长，从而深刻理解数值计算的内在局限性与优化策略。",
            "id": "3259605",
            "problem": "考虑由常微分方程 $y^{\\prime}(t) = f(t,y)$ 和初始条件 $y(t_{0}) = y_{0}$ 定义的初值问题 (IVP)。一个固定步长为 $h$ 的单步法，仅使用当前步的信息，将 $t_{n}$ 处的近似值 $y_{n}$推进到 $t_{n+1} = t_{n} + h$ 处的 $y_{n+1}$。对于此类方法，在固定的最终时间 $T$ 处的总误差通常来自两个来源：由方法的有限阶数引起的截断误差和由有限精度算术引起的浮点舍入误差。截断误差通过方法的阶数和 $f$ 的光滑度与 $h$ 相关，而舍入误差则取决于步数 $N = T/h$ 和所使用的算术。您的任务是针对特定的初值问题和一个单步法，通过平衡截断误差和浮点舍入误差，数值地找到能够最小化最终绝对误差的固定步长 $h$。\n\n基本依据：\n- 初值问题的定义：$y^{\\prime}(t) = f(t,y)$，初始条件为 $y(t_{0}) = y_{0}$。\n- 单步法的定义，例如经典的四阶龙格-库塔方法，它使用固定的公式从 $(t_{n}, y_{n})$ 和 $h$ 构造 $y_{n+1}$。\n- 一个经过充分检验的事实：对于一个阶数为 $p$ 的方法，当应用于固定区间且解是光滑的时，如果 $h$ 足够小，全局截断误差的尺度与一个常数乘以 $h^{p}$ 成正比。\n- 一个经过充分检验的事实：浮点舍入误差会在多次算术运算中累积，并且对于固定的区间 $[0,T]$，步数 $N$ 的尺度与 $T/h$ 成正比。\n\n您的程序必须对所有测试用例使用经典的四阶龙格-库塔 (RK4) 方法作为单步积分器。您不能使用任何内置的常微分方程求解器。对于每个指定的测试用例，您必须：\n- 对于所提供的候选列表中的每个 $N$，使用固定的步长 $h = T/N$ 从 $t_{0}$ 积分到 $T$。\n- 使用为该测试用例提供的精确解 $y_{\\text{true}}(t)$，计算最终绝对误差 $\\left|y_{N} - y_{\\text{true}}(T)\\right|$。\n- 从候选步长中选择使最终绝对误差最小化的步长 $h^{\\star}$。如果两个或多个候选步长产生的最终误差在数值上无法在 $10^{-18}$ 的绝对容差内区分，则选择这些并列候选者中最大的 $h$。\n\n角度单位：当出现 $\\sin(\\cdot)$ 或 $\\cos(\\cdot)$ 时，其参数应解释为弧度。\n\n使用双精度浮点算术。不允许使用随机性。\n\n测试套件：\n- 情况1（增长，预期为内部最小值）：\n  - $f(t,y) = y$, $y(0) = 1$, $T = 50$。精确解 $y_{\\text{true}}(t) = e^{t}$。\n- 情况2（衰减，预期为边界行为）：\n  - $f(t,y) = -2\\,y$, $y(0) = 1$, $T = 50$。精确解 $y_{\\text{true}}(t) = e^{-2t}$。\n- 情况3（受迫线性，有界解）：\n  - $f(t,y) = \\sin(t) - y$, $y(0) = 1$, $T = 30$。精确解 $y_{\\text{true}}(t) = \\tfrac{3}{2} e^{-t} + \\tfrac{1}{2}(\\sin t - \\cos t)$。\n\n对于每种情况，评估以下候选划分数 $N \\in \\{50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200\\}$，并由此计算 $h = T/N$。对于每个 $N$，执行恰好 $N$ 个固定大小的 RK4 步以达到时间 $T$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果按上述测试用例的顺序排列。具体来说，输出必须是列表 $[h^{\\star}_{1}, h^{\\star}_{2}, h^{\\star}_{3}]$，其中每个 $h^{\\star}_{i}$ 是为情况 $i$ 选择的最优 $h$。条目必须打印为十进制浮点数。不涉及单位。\n\n重要约束：\n- 明确地实现 RK4 单步法。不要重用任何库中的时间步进器。\n- 确保每个候选 $h$ 都产生恰好 $N$ 个均匀的步长，以使最终时间恰好为 $T$。\n- 使用上面给出的精确解来准确计算 $\\left|y_{N} - y_{\\text{true}}(T)\\right|$。\n\n最终输出格式必须恰好为一行：例如，$[h^{\\star}_{1},h^{\\star}_{2},h^{\\star}_{3}]$。",
            "solution": "用户提供了一个问题，要求使用经典的四阶龙格-库塔 (RK4) 方法求解几个初值问题 (IVP)，并找到最优的固定步长 $h$。优化标准是在固定的最终时间 $T$ 最小化绝对误差，这涉及到平衡方法的截断误差和累积的浮点舍入误差。\n\n### 步骤1：提取已知条件\n\n- **IVP 定义**：$y^{\\prime}(t) = f(t,y)$，初始条件为 $y(t_{0}) = y_{0}$。\n- **数值方法**：将使用经典的四阶龙格-库塔 (RK4) 方法。\n- **任务**：对于给定的候选步数列表 $N$，找到步长 $h = T/N$，使得最终绝对误差 $\\left|y_{N} - y_{\\text{true}}(T)\\right|$ 最小。\n- **候选划分数 $N$**：步数的候选集合为 $N \\in \\{50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200\\}$。\n- **平局打破规则**：如果多个 $h$ 候选值产生的最小误差（值在 $10^{-18}$ 的绝对容差范围内），则必须选择其中最大的 $h$。\n- **算术**：双精度浮点算术。\n- **测试用例1**：\n  - $f(t,y) = y$\n  - $y(0) = 1$\n  - $T = 50$\n  - 精确解：$y_{\\text{true}}(t) = e^{t}$\n- **测试用例2**：\n  - $f(t,y) = -2y$\n  - $y(0) = 1$\n  - $T = 50$\n  - 精确解：$y_{\\text{true}}(t) = e^{-2t}$\n- **测试用例3**：\n  - $f(t,y) = \\sin(t) - y$\n  - $y(0) = 1$\n  - $T = 30$\n  - 精确解：$y_{\\text{true}}(t) = \\tfrac{3}{2} e^{-t} + \\tfrac{1}{2}(\\sin t - \\cos t)$\n- **角度单位**：三角函数的参数以弧度为单位。\n\n### 步骤2：使用提取的已知条件进行验证\n\n根据验证标准对问题进行审查：\n\n- **科学依据**：该问题在根本上是合理的。它探讨了数值分析中的一个经典概念：截断误差（随步长 $h$ 减小而减小）与舍入误差（随计算步数 $N = T/h$ 增加而累积）之间的权衡。RK4 方法是求解常微分方程 (ODE) 的标准、易于理解的算法。所选的常微分方程是具有已知解析解的典型示例，适用于误差分析。\n- **适定性**：该问题是适定的。它提供了所有必要的组成部分：微分方程、初始条件、特定的数值方法、一组用于测试的离散步长，以及一个明确的目标函数（最小化最终绝对误差）。明确的平局打破规则确保了解决方案的唯一性。\n- **客观性**：问题以客观的数学术语陈述。评估基于定量的误差计算，没有主观解释的余地。\n\n所有其他检查（完整性、一致性、可行性等）均已通过。该问题没有表现出任何已定义的缺陷。\n\n### 步骤3：结论与行动\n\n问题陈述是**有效的**。将提供一个解决方案。\n\n### 基于原理的解决方案设计\n\n问题的核心是实现一个数值实验。对于三个初值问题中的每一个，我们都必须使用 RK4 方法在一系列步长上模拟解的演化，并确定哪个步长在最终时间 $T$ 产生最准确的结果。\n\n**四阶龙格-库塔 (RK4) 方法**\n给定一个初值问题 $y'(t) = f(t,y)$，$y(t_n) = y_n$，RK4 方法通过以下计算序列来计算在时间 $t_{n+1} = t_n + h$ 的近似值 $y_{n+1}$：\n$$\nk_1 = f(t_n, y_n)\n$$\n$$\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right)\n$$\n$$\nk_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right)\n$$\n$$\nk_4 = f\\left(t_n + h, y_n + hk_3\\right)\n$$\n然后，下一个值 $y_{n+1}$ 由这些中间斜率的加权平均值给出：\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\n该公式被迭代应用 $N$ 次，将解从初始条件 $(t_0, y_0)$ 推进到最终时间 $T = t_0 + N \\cdot h$。\n\n**误差动态**\n在最终时间 $T$ 的总绝对误差，记为 $E(h)$，是两个主要误差源的组合：\n1.  **全局截断误差 ($E_T$)**：此误差是方法对真实解近似所固有的。对于四阶方法 RK4，此误差与步长的四次方成比例，即 $E_T \\propto h^4$。当 $h \\to 0$ 时，此误差迅速减小。\n2.  **舍入误差 ($E_R$)**：此误差源于浮点算术的有限精度。RK4 方法的每一步都会引入一个小的舍入误差。这些误差在 $N = (T-t_0)/h$ 个步骤中累积。一个简化的模型表明，总舍入误差随步数增长，因此 $E_R \\propto N = 1/h$。\n\n总误差可以建模为 $E(h) \\approx C_1 h^4 + C_2/h$。这个函数具有一个特征形状：对于大的 $h$（截断误差占主导），它很大；随着 $h$ 的减小而减小；在最优步长 $h^{\\star}$ 处达到最小值；然后对于非常小的 $h$（舍入误差开始占主导），它又再次增加。我们的任务是从一组离散的候选中凭经验找到这个 $h^{\\star}$。\n\n**计算流程**\n对于三个测试用例中的每一个，执行以下算法：\n1.  计算一次最终时间的真实解 $y_{\\text{true}}(T)$，以供参考。\n2.  一个循环遍历所提供列表 $\\{50, 100, \\dots, 51200\\}$ 中的每个候选步数 $N$。\n3.  在循环内部，对于每个 $N$：\n    a. 步长 $h$ 计算为 $h = (T - t_0) / N$。请注意，在所有情况下，$t_0=0$，因此 $h = T/N$。\n    b. 执行 RK4 积分，从 $t_0$ 处的 $y_0$ 开始，执行恰好 $N$ 步，以获得在时间 $T$ 的数值近似值 $y_N$。\n    c. 计算绝对误差：$\\epsilon = |y_N - y_{\\text{true}}(T)|$。\n    d. 存储数对 $(h, \\epsilon)$。\n4.  在测试了所有候选值 $N$ 之后，分析收集到的 $(h, \\epsilon)$ 数对列表，以找到最小误差 $\\epsilon_{min}$。\n5.  根据平局打破规则，识别所有满足 $|\\epsilon - \\epsilon_{min}| \\le 10^{-18}$ 的候选者 $(h, \\epsilon)$。\n6.  在这些并列的候选者中，选择具有最大 $h$ 值的那个作为最优步长 $h^{\\star}$。\n7.  将每个测试用例得出的三个最优步长收集起来，并格式化为最终输出。\n\n系统地应用此过程来确定每个初值问题的最优步长，从而揭示数值积分中误差权衡的实际效果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rk4_solver(f, t0, y0, T, N):\n    \"\"\"\n    Solves an IVP y'(t) = f(t, y) using the classical RK4 method.\n\n    Args:\n        f (callable): The function f(t, y) defining the ODE.\n        t0 (float): The initial time.\n        y0 (float): The initial value y(t0).\n        T (float): The final time.\n        N (int): The number of steps to take.\n\n    Returns:\n        float: The numerical approximation of y(T).\n    \"\"\"\n    h = (T - t0) / float(N)\n    t = float(t0)\n    y = float(y0)\n\n    for _ in range(N):\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        y += (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n        t += h\n        \n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and find the optimal step size h.\n    \"\"\"\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"y_true\": lambda t: np.exp(t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 50.0,\n        },\n        {\n            \"f\": lambda t, y: -2.0 * y,\n            \"y_true\": lambda t: np.exp(-2.0 * t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 50.0,\n        },\n        {\n            \"f\": lambda t, y: np.sin(t) - y,\n            \"y_true\": lambda t: 1.5 * np.exp(-t) + 0.5 * (np.sin(t) - np.cos(t)),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 30.0,\n        },\n    ]\n\n    N_candidates = [50, 100, 200, 400, 800, 1600, 3200, 6400, 12800, 25600, 51200]\n    optimal_hs = []\n    TIE_TOLERANCE = 1e-18\n\n    for case in test_cases:\n        f = case[\"f\"]\n        y_true_func = case[\"y_true\"]\n        t0, y0, T = case[\"t0\"], case[\"y0\"], case[\"T\"]\n        \n        y_exact_at_T = y_true_func(T)\n        \n        results_for_case = []\n        for N in N_candidates:\n            h = (T - t0) / float(N)\n            y_N = rk4_solver(f, t0, y0, T, N)\n            error = np.abs(y_N - y_exact_at_T)\n            results_for_case.append({\"h\": h, \"error\": error})\n\n        # Find the minimum error among all candidates\n        if not results_for_case:\n            continue\n            \n        min_error = min(r['error'] for r in results_for_case)\n        \n        # Identify all candidates that are tied for the minimum error\n        # A tie is defined as being within an absolute tolerance of the minimum\n        tied_candidates = [\n            r for r in results_for_case \n            if np.isclose(r['error'], min_error, rtol=0, atol=TIE_TOLERANCE)\n        ]\n        \n        # From the tied candidates, select the one with the largest step size h\n        if not tied_candidates: # Should not happen if results_for_case is not empty\n             continue\n\n        optimal_h = max(r['h'] for r in tied_candidates)\n        optimal_hs.append(optimal_h)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, optimal_hs))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了误差估计的原理后，我们可以构建一个更智能、更高效的求解器。本练习将带领你实现一个自适应步长求解器，它能够根据预设的精度要求自动调整步长。我们将使用一种称为嵌入式龙格－库塔对（如 Bogacki-Shampine 方法）的技术，在每一步中同时计算误差估计并控制步长，这正是现代常微分方程求解器高效与稳健性的核心所在。",
            "id": "3259704",
            "problem": "考虑一个由形如 $y'(t) = f(t,y)$ 的常微分方程（ODE）和初始条件 $y(t_0) = y_0$ 定义的初值问题（IVP）。单步法仅使用当前步的信息将解从 $(t,y)$ 推进到 $(t+h, y_{\\text{new}})$。在显式龙格-库塔（RK）方法中，增量由阶段导数的加权组合构成。嵌入式RK对可以从相同的阶段计算出两个不同阶的近似值，从而能够进行局部误差估计，以实现自适应步长控制。\n\n您的任务是实现一个与 Bogacki–Shampine $2(3)$ 构造一致的嵌入式龙格-库塔对，并用它来驱动一个自适应步长选择算法。该算法必须：\n\n- 在每一步计算阶段导数，并生成一个高阶近似 $y^{[p]}$ 和一个低阶近似 $y^{[q]}$，其中 $pq$。\n- 根据差值 $y^{[p]} - y^{[q]}$ 估计局部截断误差。\n- 当缩放误差范数小于或等于 $1$ 时接受该步，否则拒绝。\n- 使用误差和步长之间的渐近缩放关系调整步长 $h$，通过应用安全因子并限制 $h$ 的增长和衰减来确保数值稳定性。\n- 遵守最终时间 $t_f$，必要时缩短最后一步，使 $t$ 精确到达 $t_f$。\n\n您必须为标量常微分方程实现该自适应积分器，其每步的缩放误差范数如下：\n$$\nE = \\frac{|y^{[p]} - y^{[q]}|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[p]}|)},\n$$\n其中 $\\mathrm{atol}$ 是绝对容差，$\\mathrm{rtol}$ 是相对容差。如果 $E \\leq 1$，则接受该步。步长控制器必须基于以下原理：阶数为 $h^{m}$ 的误差意味着 $h_{\\text{new}} \\propto h\\,E^{-1/m}$，其中整数 $m$ 与误差估计器的阶数相匹配。控制器应包含一个乘法安全因子，并通过预设的界限来限制增长和衰减。\n\n当出现三角函数时，角度量必须以弧度为单位进行解释。\n\n实现该自适应求解器，并将其应用于以下初值问题测试套件。对于每种情况，返回 $y(t_f)$ 的浮点数数值近似值。\n\n- 情况 1（理想路径，指数稳定）：\n  - $f(t,y) = -y$, $y(0) = 1$, $t_0 = 0$, $t_f = 5$。\n  - 容差：$\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$。\n  - 理由：测试在平滑问题上的常规性能和稳定性。\n\n- 情况 2（带三角函数驱动项的随时间增长，弧度单位）：\n  - $f(t,y) = y\\sin(t)$, $y(0) = 1$, $t_0 = 0$, $t_f = 3$。\n  - 容差：$\\mathrm{rtol} = 10^{-7}$, $\\mathrm{atol} = 10^{-10}$。\n  - 理由：测试对随时间变化的系数的处理能力，并要求角度以弧度为单位。\n\n- 情况 3（中等刚性类的线性衰减）：\n  - $f(t,y) = -15\\,y$, $y(0) = 1$, $t_0 = 0$, $t_f = 1$。\n  - 容差：$\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$。\n  - 理由：测试在更快衰减条件下的步长自适应能力。\n\n- 情况 4（接近奇点的非线性增长）：\n  - $f(t,y) = y^2$, $y(0) = 1$, $t_0 = 0$, $t_f = 0.9$。\n  - 容差：$\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$。\n  - 理由：测试在 $t=1$ 处爆破点附近的鲁棒性。\n\n- 情况 5（零动态，边界情况）：\n  - $f(t,y) = 0$, $y(0) = 5$, $t_0 = 0$, $t_f = 10$。\n  - 容差：$\\mathrm{rtol} = 10^{-8}$, $\\mathrm{atol} = 10^{-12}$。\n  - 理由：测试当估计误差恒等于零时控制器的行为。\n\n所有情况通用的控制器参数：\n- 安全因子 $s = 0.9$，\n- 最小增长因子 $g_{\\min} = 0.2$，\n- 最大增长因子 $g_{\\max} = 5.0$，\n- 初始步长 $h_0 = 10^{-3}$，\n- 最小步长 $h_{\\min} = 10^{-12}$，\n- 最大步长 $h_{\\max} = (t_f - t_0)$。\n\n您的程序必须生成单行输出，其中包含用方括号括起来的结果列表，结果之间用逗号分隔，并按情况 1 到 5 的顺序排列，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是情况 $i$ 的 $y(t_f)$ 的浮点数近似值。",
            "solution": "该任务的核心是从第一性原理出发，基于初值问题（IVP）、龙格-库塔（RK）方法和局部截断误差的定义，构建一个自适应单步积分器。\n\n一个初值问题指定了 $y'(t) = f(t,y)$ 和 $y(t_0) = y_0$。单步法使用单个步长 $h_n$ 从 $(t_n,y_n)$ 计算 $y_{n+1}$，无需参考之前的历史记录。在显式龙格-库塔（RK）方法中，新值是阶段导数的加权组合。通用的显式RK构造如下：\n$$\nk_1 = f(t_n, y_n),\\quad\nk_2 = f(t_n + c_2 h, y_n + h a_{21} k_1),\\quad \\dots,\\quad\nk_s = f(t_n + c_s h, y_n + h \\sum_{j=1}^{s-1} a_{sj} k_j),\n$$\n一个 $p$ 阶近似为\n$$\ny^{[p]}_{n+1} = y_n + h \\sum_{j=1}^s b_j k_j.\n$$\n嵌入式对提供两组权重 $(b_j)$ 和 $(\\hat b_j)$，它们使用相同的阶段进行评估，以得出 $pq$ 的 $y^{[p]}$ 和 $y^{[q]}$。其差值\n$$\n\\Delta = y^{[p]}_{n+1} - y^{[q]}_{n+1}\n$$\n的计算所产生的额外成本可忽略不计，并且它与 $h$ 的某个已知次幂成比例，该次幂由构造决定。\n\n对于 Bogacki–Shampine $2(3)$ 对，该方法使用 $s=4$ 个阶段，节点为 $c_2 = \\tfrac{1}{2}$、$c_3 = \\tfrac{3}{4}$、$c_4 = 1$。内部系数为 $a_{21} = \\tfrac{1}{2}$、$a_{32} = \\tfrac{3}{4}$（其中 $a_{31} = 0$），以及 $a_{41} = \\tfrac{2}{9}$、$a_{42} = \\tfrac{1}{3}$、$a_{43} = \\tfrac{4}{9}$。高阶（$p=3$ 阶）权重为\n$$\nb_1 = \\tfrac{2}{9},\\quad b_2 = \\tfrac{1}{3},\\quad b_3 = \\tfrac{4}{9},\\quad b_4 = 0,\n$$\n低阶（$q=2$ 阶）权重为\n$$\n\\hat b_1 = \\tfrac{7}{24},\\quad \\hat b_2 = \\tfrac{1}{4},\\quad \\hat b_3 = \\tfrac{1}{3},\\quad \\hat b_4 = \\tfrac{1}{8}.\n$$\n阶段和近似值的计算如下\n$$\n\\begin{aligned}\nk_1 = f(t, y),\\\\\nk_2 = f\\Big(t + \\tfrac{1}{2}h,\\, y + h\\,\\tfrac{1}{2}\\,k_1\\Big),\\\\\nk_3 = f\\Big(t + \\tfrac{3}{4}h,\\, y + h\\,\\tfrac{3}{4}\\,k_2\\Big),\\\\\ny^{[3]} = y + h\\Big(\\tfrac{2}{9}k_1 + \\tfrac{1}{3}k_2 + \\tfrac{4}{9}k_3\\Big),\\\\\nk_4 = f\\Big(t + h,\\, y^{[3]}\\Big),\\\\\ny^{[2]} = y + h\\Big(\\tfrac{7}{24}k_1 + \\tfrac{1}{4}k_2 + \\tfrac{1}{3}k_3 + \\tfrac{1}{8}k_4\\Big).\n\\end{aligned}\n$$\n嵌入差分 $\\Delta = y^{[3]} - y^{[2]}$ 提供了一个误差估计器，其主导行为为 $O(h^3)$；也就是说，对于平滑的 $f$，它与 $C h^3$ 成比例。这种缩放特性为以下形式的控制器提供了理论依据：\n$$\nh_{\\text{new}} = h \\cdot s \\cdot E^{-1/3},\n$$\n其中 $s$ 是一个安全因子，$E$ 是一个缩放误差范数。为确保数值鲁棒性，我们将乘法增长/衰减因子限制在 $g_{\\min} \\leq s E^{-1/3} \\leq g_{\\max}$ 范围内，并且还将 $h_{\\text{new}}$ 截断到 $[h_{\\min}, h_{\\max}]$ 区间内。当 $E \\leq 1$ 时，接受该步，并将 $y$ 推进到 $y^{[3]}$；否则，拒绝该步，并用一个更小的 $h$ 重新计算。\n\n对于标量问题，选择缩放误差范数以平衡绝对容差和相对容差：\n$$\nE = \\frac{|\\Delta|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[3]}|)}.\n$$\n这确保了接受准则 $E \\leq 1$ 能够在相对于解的量级来控制误差的同时，防止当 $y$ 接近零时出现除以极小数的情况。\n\n从 $t_0$ 积分到 $t_f$ 的算法步骤：\n- 初始化 $t = t_0, y = y_0$，在 $[h_{\\min}, h_{\\max}]$ 内选择 $h$（例如，使用给定的 $h_0$），并设置控制器参数 $s, g_{\\min}, g_{\\max}$。\n- 当 $t  t_f$ 时循环：\n  - 如果 $t + h  t_f$，则设置 $h = t_f - t$ 以精确到达 $t_f$。\n  - 计算 $k_1, k_2, k_3, y^{[3]}, k_4$ 和 $y^{[2]}$。\n  - 根据 $y, y^{[3]}, y^{[2]}, \\mathrm{atol}$ 和 $\\mathrm{rtol}$ 计算 $E$。\n  - 如果 $E \\leq 1$，接受该步：设置 $t \\gets t + h, y \\gets y^{[3]}$。\n  - 计算候选增长因子 $g = s \\cdot E^{-1/3}$；如果 $E=0$，则设置 $g = g_{\\max}$。\n  - 将 $g$ 限制在 $[g_{\\min}, g_{\\max}]$ 范围内，然后更新 $h \\gets \\mathrm{clip}(h \\cdot g, h_{\\min}, h_{\\max})$。\n  - 如果一个步长被拒绝 ($E  1$)，则如上所述更新 $h$，并在不推进 $t$ 或 $y$ 的情况下重新计算。\n- 返回 $y(t_f)$。\n\n我们现在简要分析每个测试用例及其预期行为：\n- 情况 1：$y'(t) = -y$，精确解为 $y(t) = e^{-t}$，因此 $y(5) = e^{-5}$。该方法应采取中等大小的步长并快速收敛。\n- 情况 2：$y'(t) = y\\sin(t)$（以弧度为单位），其精确解为 $y(t) = \\exp(1 - \\cos t)$，得到 $y(3) = \\exp(1 - \\cos 3)$。算法必须能够平滑地处理随时间变化的驱动项。\n- 情况 3：$y'(t) = -15y$，精确解为 $y(1) = e^{-15}$。由于快速衰减，初始时需要较小的步长，但随着 $y$ 的减小，控制器将增大 $h$。\n- 情况 4：$y'(t) = y^2$，对于 $t1$，精确解为 $y(t) = \\frac{1}{1 - t}$，因此 $y(0.9) = 10$。当 $t$ 接近 $t=1$ 的爆破点时，该方法必须自适应地减小 $h$。\n- 情况 5：$y'(t) = 0$ 产生常数解 $y(t) = 5$；误差估计器恒等于零，控制器会将 $h$ 增大到 $h_{\\max}$。\n\n通过实现 Bogacki–Shampine $2(3)$ 阶段和从 $O(h^3)$ 误差估计推导出的控制器，该自适应求解器将为所有情况提供 $y(t_f)$，并以指定格式打印。",
            "answer": "```python\n# Python 3.12, numpy 1.23.5 allowed; no other libraries.\nimport numpy as np\n\ndef rk23_bogacki_shampine_step(f, t, y, h):\n    \"\"\"\n    Perform one Bogacki-Shampine 2(3) step for a scalar ODE y' = f(t,y).\n    Returns (y_high, y_low) where y_high is the 3rd-order solution, y_low is the 2nd-order embedded solution.\n    \"\"\"\n    k1 = f(t, y)\n    k2 = f(t + 0.5 * h, y + h * 0.5 * k1)\n    k3 = f(t + 0.75 * h, y + h * 0.75 * k2)\n    # 3rd-order solution\n    y3 = y + h * ( (2.0/9.0) * k1 + (1.0/3.0) * k2 + (4.0/9.0) * k3 )\n    # Stage 4 evaluated at t+h, y3\n    k4 = f(t + h, y3)\n    # 2nd-order embedded solution\n    y2 = y + h * ( (7.0/24.0) * k1 + (1.0/4.0) * k2 + (1.0/3.0) * k3 + (1.0/8.0) * k4 )\n    return y3, y2\n\ndef integrate_adaptive(f, t0, tf, y0, rtol, atol,\n                       h0=1e-3, hmin=1e-12, hmax=None,\n                       safety=0.9, growth_min=0.2, growth_max=5.0):\n    \"\"\"\n    Adaptive integrator using Bogacki-Shampine 2(3) pair for scalar ODEs.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    if hmax is None:\n        hmax = abs(tf - t0)\n    h = max(hmin, min(h0, hmax))\n    # Direction of integration\n    direction = 1.0 if tf >= t0 else -1.0\n    h *= direction\n    hmin_signed = hmin * direction\n    hmax_signed = hmax * direction\n\n    # Main integration loop\n    # Guard for max iterations to prevent infinite loops in pathological cases\n    max_steps = 10_000_000\n    steps = 0\n    while (direction > 0 and t  tf) or (direction  0 and t > tf):\n        steps += 1\n        if steps > max_steps:\n            # Fallback: give current y\n            break\n\n        # Adjust step to not overshoot tf\n        remaining = tf - t\n        if direction * h > direction * remaining:\n            h = remaining\n\n        # Take one RK23 step\n        y3, y2 = rk23_bogacki_shampine_step(f, t, y, h)\n\n        # Scaled error norm (scalar)\n        scale = atol + rtol * max(abs(y), abs(y3))\n        # Prevent zero scale\n        if scale == 0.0:\n            scale = atol\n        err = abs(y3 - y2) / scale\n\n        # Accept or reject\n        if err = 1.0:\n            # Accept the step\n            t = t + h\n            y = y3\n            # Compute growth factor; estimator scales ~ h^3\n            if err == 0.0:\n                g = growth_max\n            else:\n                g = safety * err ** (-1.0 / 3.0)\n            # Bound growth factor\n            g = max(growth_min, min(g, growth_max))\n            # Update h and clip\n            h = h * g\n            # Clip to [hmin, hmax] with sign\n            if direction > 0:\n                h = min(max(h, hmin_signed), hmax_signed)\n            else:\n                h = max(min(h, hmin_signed), hmax_signed)\n        else:\n            # Reject step; decrease h\n            g = safety * err ** (-1.0 / 3.0)\n            g = max(growth_min, min(g, growth_max))\n            h = h * g\n            # Ensure not below minimum\n            if direction > 0:\n                h = max(h, hmin_signed)\n            else:\n                h = min(h, hmin_signed)\n            # If h becomes too small, break to avoid infinite loop\n            if abs(h)  hmin:\n                # Cannot reduce further; accept and break\n                # This is a conservative fallback\n                t = t + h\n                y = y3\n                break\n\n    return y\n\ndef solve():\n    # Define the test cases\n    # Case 1: y' = -y, y(0) = 1, tf = 5\n    def f1(t, y): return -y\n\n    # Case 2: y' = y*sin(t), radians, y(0) = 1, tf = 3\n    def f2(t, y): return y * np.sin(t)\n\n    # Case 3: y' = -15 y, y(0) = 1, tf = 1\n    def f3(t, y): return -15.0 * y\n\n    # Case 4: y' = y^2, y(0) = 1, tf = 0.9\n    def f4(t, y): return y * y\n\n    # Case 5: y' = 0, y(0) = 5, tf = 10\n    def f5(t, y): return 0.0\n\n    test_cases = [\n        # (f, t0, tf, y0, rtol, atol)\n        (f1, 0.0, 5.0, 1.0, 1e-6, 1e-9),\n        (f2, 0.0, 3.0, 1.0, 1e-7, 1e-10),\n        (f3, 0.0, 1.0, 1.0, 1e-6, 1e-9),\n        (f4, 0.0, 0.9, 1.0, 1e-6, 1e-9),\n        (f5, 0.0, 10.0, 5.0, 1e-8, 1e-12),\n    ]\n\n    results = []\n    for f, t0, tf, y0, rtol, atol in test_cases:\n        ytf = integrate_adaptive(\n            f=f, t0=t0, tf=tf, y0=y0, rtol=rtol, atol=atol,\n            h0=1e-3, hmin=1e-12, hmax=abs(tf - t0),\n            safety=0.9, growth_min=0.2, growth_max=5.0\n        )\n        results.append(ytf)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}