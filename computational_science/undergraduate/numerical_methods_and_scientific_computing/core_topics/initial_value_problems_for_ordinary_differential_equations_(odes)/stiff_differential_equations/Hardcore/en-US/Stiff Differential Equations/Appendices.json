{
    "hands_on_practices": [
        {
            "introduction": "To build intuition for stiff equations, we begin with a classic example that starkly reveals the limitations of standard explicit methods. This exercise challenges you to analyze a simple-looking linear ordinary differential equation (ODE) whose solution contains two vastly different time scales. By deriving the exact solution and implementing both the explicit Forward Euler and implicit Backward Euler methods, you will directly observe why the stability of an explicit method is constrained by the fastest, often irrelevant, component of the solution, making implicit methods indispensable for efficiently solving such problems. ",
            "id": "3198039",
            "problem": "Consider the initial value problem (IVP) for the Ordinary Differential Equation (ODE): $y^{\\prime}(t) = -\\dfrac{1}{\\epsilon} y(t) + \\sin(t)$ over the interval $t \\in [0, 10]$ with initial condition $y(0) = 0$. The parameter $\\epsilon$ is a small positive constant given by $\\epsilon = 10^{-6}$, and the angle unit for $t$ in the sine function is radians. This IVP exhibits stiffness due to the presence of the term $-\\dfrac{1}{\\epsilon} y(t)$, which introduces a fast-decaying component in the solution.\n\nStarting from fundamental definitions and well-tested facts, do the following:\n\n1. Use the integrating factor method based on the linear ODE structure to derive the exact closed-form solution $y(t)$ for the given IVP. Your derivation must begin from the definition of a first-order linear ODE and the integrating factor construction and must not rely on any shortcut formulas given to you.\n\n2. Using the definition of the derivative as a limit of difference quotients and the idea of time discretization with a fixed step size $h$, derive:\n   - The explicit forward Euler update rule by approximating $y^{\\prime}(t)$ at the left endpoint of each time step.\n   - The implicit backward Euler update rule by approximating $y^{\\prime}(t)$ at the right endpoint of each time step.\n   Both derivations must originate from the definition $y^{\\prime}(t) = \\lim_{h \\to 0} \\dfrac{y(t+h) - y(t)}{h}$ and must not assume any pre-formed discrete update formulas.\n\n3. Explain, using the linear stability analysis for the Dahlquist test equation $y^{\\prime} = \\lambda y$, why the IVP is stiff when $\\epsilon$ is small, and how this affects the permissible explicit step sizes. In particular, reason from first principles to identify constraints on $h$ for explicit methods and contrast them with the behavior of the implicit method.\n\n4. Implement both derived methods to numerically approximate $y(t)$ over $t \\in [0, 10]$ for a fixed set of time steps $h$. For each method and each time step, compute the maximum absolute error over the discrete grid points against the exact solution you derived. If during the explicit integration the numerical solution becomes non-finite (for example due to overflow), treat the maximum error for that case as $+\\infty$.\n\nAngle unit specification: all trigonometric evaluations must use radians.\n\nTest suite:\n- Parameter: $\\epsilon = 10^{-6}$.\n- Initial condition: $y(0) = 0$.\n- Interval: $[0, 10]$.\n- Step sizes: $h \\in \\{1.0, 0.1, 0.01, 0.001\\}$.\n\nFor each step size $h$ in the above set, evaluate:\n- The maximum absolute error of the explicit forward Euler method over the discrete time grid $\\{0, h, 2h, \\dots, 10\\}$.\n- The maximum absolute error of the implicit backward Euler method over the same grid.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\"). The list must aggregate the results in the following order: for each $h$ in increasing order, first the explicit method’s maximum error, then the implicit method’s maximum error, so the final output has eight entries corresponding to $h = 0.001$, $h = 0.01$, $h = 0.1$, and $h = 1.0$ in that order.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a canonical example of a stiff ordinary differential equation (ODE) and requests derivations and analyses that are fundamental to the field of computational science. The problem statement is free of contradictions or fallacies.\n\nThe solution is structured into four parts, corresponding to the tasks outlined in the problem description.\n\n### 1. Derivation of the Exact Closed-Form Solution\n\nThe given initial value problem (IVP) is:\n$$\ny^{\\prime}(t) = -\\frac{1}{\\epsilon} y(t) + \\sin(t), \\quad y(0) = 0, \\quad t \\in [0, 10]\n$$\nThis is a first-order linear ODE. The standard form for such an equation is $y' + p(t)y = q(t)$. By rearranging the given ODE, we identify the corresponding terms:\n$$\ny^{\\prime}(t) + \\frac{1}{\\epsilon} y(t) = \\sin(t)\n$$\nHere, $p(t) = \\frac{1}{\\epsilon}$ and $q(t) = \\sin(t)$.\n\nTo solve this, we use the method of integrating factors. The integrating factor, $I(t)$, is defined as:\n$$\nI(t) = e^{\\int p(t) dt}\n$$\nFor this problem, the integral is:\n$$\n\\int p(t) dt = \\int \\frac{1}{\\epsilon} dt = \\frac{t}{\\epsilon}\n$$\nThus, the integrating factor is $I(t) = e^{t/\\epsilon}$. We multiply the standard form of the ODE by $I(t)$:\n$$\ne^{t/\\epsilon} y^{\\prime}(t) + \\frac{1}{\\epsilon} e^{t/\\epsilon} y(t) = e^{t/\\epsilon} \\sin(t)\n$$\nThe left-hand side is, by the product rule of differentiation, the derivative of $y(t)I(t)$:\n$$\n\\frac{d}{dt} \\left( y(t) e^{t/\\epsilon} \\right) = e^{t/\\epsilon} \\sin(t)\n$$\nTo find $y(t)$, we integrate both sides with respect to $t$:\n$$\n\\int \\frac{d}{dt} \\left( y(t) e^{t/\\epsilon} \\right) dt = \\int e^{t/\\epsilon} \\sin(t) dt\n$$\n$$\ny(t) e^{t/\\epsilon} = \\int e^{t/\\epsilon} \\sin(t) dt\n$$\nThe integral on the right-hand side can be solved using integration by parts twice. The general solution for an integral of the form $\\int e^{at}\\sin(bt)dt$ is $\\frac{e^{at}}{a^2+b^2}(a\\sin(bt) - b\\cos(bt))$. With $a = 1/\\epsilon$ and $b = 1$, we have:\n$$\n\\int e^{t/\\epsilon} \\sin(t) dt = \\frac{e^{t/\\epsilon}}{(1/\\epsilon)^2 + 1^2} \\left( \\frac{1}{\\epsilon}\\sin(t) - 1\\cos(t) \\right) + C\n$$\nwhere $C$ is the constant of integration. Simplifying this expression:\n$$\n\\int e^{t/\\epsilon} \\sin(t) dt = \\frac{e^{t/\\epsilon}}{(1+\\epsilon^2)/\\epsilon^2} \\left( \\frac{\\sin(t) - \\epsilon\\cos(t)}{\\epsilon} \\right) + C = \\frac{\\epsilon^2 e^{t/\\epsilon}}{1+\\epsilon^2} \\frac{\\sin(t) - \\epsilon\\cos(t)}{\\epsilon} + C\n$$\n$$\n= \\frac{\\epsilon e^{t/\\epsilon}}{1+\\epsilon^2} (\\sin(t) - \\epsilon\\cos(t)) + C = \\frac{e^{t/\\epsilon}}{1+\\epsilon^2} (\\epsilon\\sin(t) - \\epsilon^2\\cos(t)) + C\n$$\nSubstituting this back into the equation for $y(t)e^{t/\\epsilon}$:\n$$\ny(t) e^{t/\\epsilon} = \\frac{e^{t/\\epsilon}}{1+\\epsilon^2} (\\epsilon\\sin(t) - \\epsilon^2\\cos(t)) + C\n$$\nSolving for $y(t)$ by multiplying by $e^{-t/\\epsilon}$:\n$$\ny(t) = \\frac{\\epsilon\\sin(t) - \\epsilon^2\\cos(t)}{1+\\epsilon^2} + C e^{-t/\\epsilon}\n$$\nNow, we apply the initial condition $y(0) = 0$ to find the constant $C$:\n$$\ny(0) = 0 = \\frac{\\epsilon\\sin(0) - \\epsilon^2\\cos(0)}{1+\\epsilon^2} + C e^{0}\n$$\n$$\n0 = \\frac{0 - \\epsilon^2(1)}{1+\\epsilon^2} + C \\implies C = \\frac{\\epsilon^2}{1+\\epsilon^2}\n$$\nSubstituting the value of $C$ back gives the exact closed-form solution to the IVP:\n$$\ny(t) = \\frac{\\epsilon\\sin(t) - \\epsilon^2\\cos(t)}{1+\\epsilon^2} + \\frac{\\epsilon^2}{1+\\epsilon^2} e^{-t/\\epsilon}\n$$\n\n### 2. Derivation of Numerical Methods\n\nWe derive the numerical update rules from the limit definition of the derivative, $y^{\\prime}(t) = \\lim_{h \\to 0} \\frac{y(t+h) - y(t)}{h}$. We consider a discrete time grid $t_n = n h$ for $n=0, 1, 2, \\dots$, where $h$ is the step size. Let $y_n$ be the numerical approximation to $y(t_n)$.\n\n**Explicit Forward Euler Method**\nThe forward Euler method approximates the derivative $y'(t)$ at the beginning of the time step, $t_n$. Using a first-order forward difference quotient based on the limit definition:\n$$\ny^{\\prime}(t_n) \\approx \\frac{y(t_{n+1}) - y(t_n)}{h}\n$$\nSubstituting this into the ODE, $y'(t) = f(t, y(t)) = -\\frac{1}{\\epsilon} y(t) + \\sin(t)$, at time $t_n$:\n$$\n\\frac{y_{n+1} - y_n}{h} = -\\frac{1}{\\epsilon} y_n + \\sin(t_n)\n$$\nSolving for $y_{n+1}$ gives the explicit forward Euler update rule:\n$$\ny_{n+1} = y_n + h \\left( -\\frac{1}{\\epsilon} y_n + \\sin(t_n) \\right)\n$$\nThis rule is \"explicit\" because $y_{n+1}$ is computed directly from known values at time $t_n$.\n\n**Implicit Backward Euler Method**\nThe backward Euler method approximates the derivative by evaluating the ODE at the end of the time step, $t_{n+1}$. The derivative $y'(t_{n+1})$ is approximated using a backward difference quotient:\n$$\ny^{\\prime}(t_{n+1}) \\approx \\frac{y(t_{n+1}) - y(t_n)}{h}\n$$\nSubstituting this into the ODE at time $t_{n+1}$:\n$$\n\\frac{y_{n+1} - y_n}{h} = -\\frac{1}{\\epsilon} y_{n+1} + \\sin(t_{n+1})\n$$\nThis equation is \"implicit\" because the unknown $y_{n+1}$ appears on both sides. We must solve for $y_{n+1}$:\n$$\ny_{n+1} - y_n = h \\left( -\\frac{1}{\\epsilon} y_{n+1} + \\sin(t_{n+1}) \\right)\n$$\n$$\ny_{n+1} - y_n = -\\frac{h}{\\epsilon} y_{n+1} + h\\sin(t_{n+1})\n$$\nGathering terms with $y_{n+1}$:\n$$\ny_{n+1} + \\frac{h}{\\epsilon} y_{n+1} = y_n + h\\sin(t_{n+1})\n$$\n$$\ny_{n+1} \\left( 1 + \\frac{h}{\\epsilon} \\right) = y_n + h\\sin(t_{n+1})\n$$\nFinally, solving for $y_{n+1}$ gives the implicit backward Euler update rule:\n$$\ny_{n+1} = \\frac{y_n + h\\sin(t_{n+1})}{1 + h/\\epsilon}\n$$\n\n### 3. Stability Analysis and Stiffness\n\nThe concept of stiffness is analyzed using the Dahlquist test equation, $y' = \\lambda y$, where $\\text{Re}(\\lambda)  0$. For the given ODE, the dynamics are dominated by the homogeneous part, $y' = -\\frac{1}{\\epsilon} y$. Thus, we identify $\\lambda = -1/\\epsilon$. Since $\\epsilon = 10^{-6}$ is a small positive constant, $\\lambda = -10^6$ is a large negative real number.\n\n**Forward Euler Stability**\nApplying the forward Euler rule to $y'=\\lambda y$:\n$$\ny_{n+1} = y_n + h(\\lambda y_n) = (1+h\\lambda) y_n\n$$\nFor the numerical solution to remain bounded (i.e., stable), the amplification factor, $R(h\\lambda) = 1+h\\lambda$, must satisfy $|R(h\\lambda)| \\le 1$.\n$$\n|1 + h\\lambda| \\le 1\n$$\nWith $\\lambda = -1/\\epsilon$, this becomes $|1 - h/\\epsilon| \\le 1$. This inequality is equivalent to $-1 \\le 1 - h/\\epsilon \\le 1$.\nThe right side, $1 - h/\\epsilon \\le 1$, implies $h/\\epsilon \\ge 0$, which is always true for positive $h$ and $\\epsilon$.\nThe left side, $-1 \\le 1 - h/\\epsilon$, implies $h/\\epsilon \\le 2$, or $h \\le 2\\epsilon$.\nFor $\\epsilon=10^{-6}$, the step size must satisfy $h \\le 2 \\times 10^{-6}$. This is a severe restriction. For any step size $h > 2\\epsilon$, the numerical solution will become unbounded, exhibiting oscillations that grow exponentially.\n\n**Backward Euler Stability**\nApplying the backward Euler rule to $y'=\\lambda y$:\n$$\ny_{n+1} = y_n + h(\\lambda y_{n+1}) \\implies y_{n+1}(1 - h\\lambda) = y_n \\implies y_{n+1} = \\frac{1}{1-h\\lambda} y_n\n$$\nThe amplification factor is $R(h\\lambda) = \\frac{1}{1-h\\lambda}$. For stability, we require $|R(h\\lambda)| \\le 1$.\n$$\n\\left| \\frac{1}{1-h\\lambda} \\right| \\le 1\n$$\nWith $\\lambda = -1/\\epsilon$, this is $\\left| \\frac{1}{1+h/\\epsilon} \\right|$. Since $h > 0$ and $\\epsilon > 0$, the denominator $1+h/\\epsilon$ is always greater than $1$. Therefore, its reciprocal is always less than $1$ in magnitude. The method is stable for any choice of $h > 0$. This property is known as A-stability.\n\n**Stiffness Explanation**\nThe IVP is stiff because the exact solution $y(t) = \\frac{\\epsilon\\sin(t) - \\epsilon^2\\cos(t)}{1+\\epsilon^2} + \\frac{\\epsilon^2}{1+\\epsilon^2} e^{-t/\\epsilon}$ contains two components with vastly different time scales.\n1. A fast-decaying transient component, proportional to $e^{-t/\\epsilon}$, which has a characteristic time scale of $\\tau_{fast} = \\epsilon = 10^{-6}$. This component becomes negligible almost instantaneously.\n2. A slowly varying component, which oscillates with a time scale of $\\tau_{slow} \\approx 2\\pi$.\n\nThe stiffness arises because the stability of an explicit method like forward Euler is governed by the fastest time scale ($\\tau_{fast}$), forcing the use of an impractically small step size ($h \\le 2\\epsilon$) even long after the fast component has vanished. An implicit method like backward Euler, being unconditionally stable for this problem, is not constrained by $\\tau_{fast}$ for stability. It can use a much larger step size $h$ determined by the accuracy requirements for capturing the slow component, making it far more efficient for stiff problems.\n\n### 4. Implementation and Error Analysis\n\nThe forward and backward Euler methods are implemented to solve the IVP over $t \\in [0, 10]$ for the step sizes $h \\in \\{0.001, 0.01, 0.1, 1.0\\}$. Since all these step sizes violate the forward Euler stability condition ($h > 2 \\times 10^{-6}$), the explicit method is expected to produce numerically unstable solutions that grow without bound, resulting in an infinite maximum error. The implicit method, being stable, should produce accurate results, with the error decreasing as $h$ decreases. The maximum absolute error between the numerical solution and the exact solution is computed on the grid points for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a stiff ODE using Forward and Backward Euler methods,\n    and calculates the maximum absolute error against the exact solution.\n    \"\"\"\n    \n    # Define problem parameters from the statement\n    epsilon = 1e-6\n    t_end = 10.0\n    y0 = 0.0\n    \n    # Test suite of step sizes, in increasing order as required for output.\n    h_values = [0.001, 0.01, 0.1, 1.0]\n    \n    # List to store the results in the specified order.\n    results = []\n\n    # Exact solution derived via integrating factor\n    # y(t) = (epsilon*sin(t) - epsilon^2*cos(t))/(1 + epsilon^2) \n    #        + (epsilon^2 / (1 + epsilon^2)) * np.exp(-t/epsilon)\n    def y_exact(t, eps):\n        term1 = (eps * np.sin(t) - eps**2 * np.cos(t)) / (1 + eps**2)\n        term2 = (eps**2 / (1 + eps**2)) * np.exp(-t / eps)\n        return term1 + term2\n\n    # Loop over each step size\n    for h in h_values:\n        # Create a stable time grid from 0 to t_end\n        # Using np.linspace is more robust against floating point errors than np.arange\n        num_steps = int(round(t_end / h))\n        t_points = np.linspace(0, t_end, num_steps + 1)\n        \n        # --- Explicit Forward Euler Method ---\n        y_fe = np.zeros(num_steps + 1)\n        y_fe[0] = y0\n        is_finite_fe = True\n        \n        for n in range(num_steps):\n            # y_{n+1} = y_n + h * f(t_n, y_n)\n            # f(t,y) = -y/epsilon + sin(t)\n            y_fe[n+1] = y_fe[n] + h * (-y_fe[n] / epsilon + np.sin(t_points[n]))\n            # Check for overflow at each step to prevent warnings and handle correctly.\n            if not np.isfinite(y_fe[n+1]):\n                is_finite_fe = False\n                break\n        \n        # Calculate maximum absolute error for Forward Euler\n        if is_finite_fe:\n            y_true = y_exact(t_points, epsilon)\n            error_fe = np.max(np.abs(y_fe - y_true))\n        else:\n            # As per problem, if solution is non-finite, error is +inf\n            error_fe = float('inf')\n        \n        results.append(error_fe)\n        \n        # --- Implicit Backward Euler Method ---\n        y_be = np.zeros(num_steps + 1)\n        y_be[0] = y0\n        \n        for n in range(num_steps):\n            # y_{n+1} = (y_n + h*sin(t_{n+1})) / (1 + h/epsilon)\n            numerator = y_be[n] + h * np.sin(t_points[n+1])\n            denominator = 1 + h / epsilon\n            y_be[n+1] = numerator / denominator\n\n        # Calculate maximum absolute error for Backward Euler\n        y_true = y_exact(t_points, epsilon)\n        error_be = np.max(np.abs(y_be - y_true))\n        results.append(error_be)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Stiffness is not limited to scalar equations; it is a critical feature of many real-world systems of ODEs, from chemical kinetics to structural mechanics. This exercise generalizes stability analysis to linear systems, where the eigenvalues of the system matrix dictate the behavior of numerical methods. You will derive the maximum stable step size for the Forward Euler method and use it to directly compare the efficiency and accuracy against the Backward Euler method, providing a quantitative grasp of why implicit solvers are superior for stiff systems. ",
            "id": "3279271",
            "problem": "Consider a linear autonomous system of stiff ordinary differential equations given by $y'(t) = A y(t)$ with an initial condition $y(0) = y_0$, where $A \\in \\mathbb{R}^{n \\times n}$ is a constant matrix and $y(t) \\in \\mathbb{R}^n$. The exact solution is given by $y(t) = \\exp(A t) y_0$, where $\\exp(A t)$ denotes the matrix exponential. A system is termed stiff when its eigenvalues have widely separated negative real parts, causing explicit methods to require very small step sizes for stability.\n\nYou must determine the largest Forward Euler step size $h_{FE}$ that ensures absolute stability for the discrete Forward Euler method applied to this system, and then compare the accuracy of $N$ Forward Euler steps of size $h_{FE}$ against a single Backward Euler step of size $N h_{FE}$. The Forward Euler method is defined by $y_{k+1} = y_k + h A y_k$, and the Backward Euler method is defined by $(I - h A) y_{k+1} = y_k$, where $I$ is the identity matrix of dimension $n$.\n\nStarting only from fundamental definitions:\n- The solution of the linear system $y'(t) = A y(t)$ is $y(t) = \\exp(A t) y_0$.\n- The Forward Euler and Backward Euler updates follow from the explicit and implicit discretizations of $y'(t) = A y(t)$.\n- Absolute stability of a discrete method applied to the linear test problem $z' = \\lambda z$ requires that the magnitude of the amplification factor for the discrete update does not exceed $1$ for each eigenvalue $\\lambda$ of $A$ with negative real part.\n\nYour task is to:\n1. Derive, from the above principles, the largest step size $h_{FE}$ that ensures absolute stability for the Forward Euler method applied to $y'(t) = A y(t)$ by enforcing the discrete stability condition separately on each eigenvalue of $A$ and then taking the most restrictive value across all eigenvalues.\n2. Implement a program that, for each test case, computes:\n   - The largest stable Forward Euler step size $h_{FE}$.\n   - The Forward Euler approximation $y_N$ after $N$ steps of size $h_{FE}$, i.e., $y_N = (I + h_{FE} A)^N y_0$.\n   - The Backward Euler approximation $y_{BE}$ after one step of size $N h_{FE}$, i.e., $y_{BE} = (I - (N h_{FE}) A)^{-1} y_0$.\n   - The exact solution at time $T = N h_{FE}$, i.e., $y_{exact} = \\exp(A T) y_0$.\n   - The Euclidean norm errors $\\| y_N - y_{exact} \\|_2$ and $\\| y_{BE} - y_{exact} \\|_2$.\n   - A boolean indicating whether Backward Euler is strictly more accurate than Forward Euler (i.e., whether $\\| y_{BE} - y_{exact} \\|_2  \\| y_N - y_{exact} \\|_2$).\n3. Express all floating-point outputs rounded to $8$ decimal places.\n\nUse only mathematical constructs; no physical units are involved. Angles are not part of this problem. The final output must be a single line of text containing a comma-separated list enclosed in square brackets, where each entry corresponds to one test case, and each test case entry is itself a list of the form $[h_{FE}, \\| y_N - y_{exact} \\|_2, \\| y_{BE} - y_{exact} \\|_2, \\text{boolean}]$.\n\nTest Suite:\n- Case $1$ (diagonal stiff with moderate ratio): \n  $$A = \\begin{bmatrix} -1  0 \\\\ 0  -100 \\end{bmatrix}, \\quad y_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad N = 50.$$\n- Case $2$ (two-dimensional rotation-damping stiff system): \n  $$A = \\begin{bmatrix} -2  -50 \\\\ 50  -2 \\end{bmatrix}, \\quad y_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad N = 400.$$\n- Case $3$ (extremely stiff separation): \n  $$A = \\begin{bmatrix} -0.1  0 \\\\ 0  -10000 \\end{bmatrix}, \\quad y_0 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}, \\quad N = 5000.$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each test case’s result is a list as specified above, for example, $[[\\text{case1}],[\\text{case2}],[\\text{case3}]]$.",
            "solution": "The analysis and solution of the stated problem proceed in two parts. First, a derivation of the stability constraint for the Forward Euler method is presented. Second, the computational methodology for solving the test cases is detailed, based on the derived principles.\n\n### 1. Derivation of the Forward Euler Stability Constraint\n\nThe problem considers a linear autonomous system of ordinary differential equations (ODEs), $y'(t) = A y(t)$, with the initial condition $y(0) = y_0$. The stability of numerical methods for such systems is analyzed by applying the method to a set of uncoupled scalar test problems. This is achieved through the diagonalization of the matrix $A$.\n\nAssuming $A$ is a diagonalizable matrix, it can be written as $A = S \\Lambda S^{-1}$, where $\\Lambda$ is a diagonal matrix whose entries $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ are the eigenvalues of $A$, and $S$ is the matrix whose columns are the corresponding eigenvectors. By defining a transformed variable $z(t) = S^{-1} y(t)$, the original system is transformed into:\n$$ z'(t) = S^{-1} y'(t) = S^{-1} A y(t) = S^{-1} (S \\Lambda S^{-1}) (S z(t)) = \\Lambda z(t) $$\nThis results in a system of $n$ independent scalar ODEs:\n$$ z_i'(t) = \\lambda_i z_i(t) \\quad \\text{for } i = 1, \\dots, n $$\nThe stability of a numerical method for the vector system $y'(t)=Ay(t)$ is equivalent to its stability for all these scalar test problems simultaneously.\n\nThe Forward Euler method is defined by the update rule $y_{k+1} = y_k + h A y_k$. Applying this to the scalar test problem $z' = \\lambda z$ gives:\n$$ z_{k+1} = z_k + h \\lambda z_k = (1 + h \\lambda) z_k $$\nThe term $g(h\\lambda) = 1 + h\\lambda$ is called the amplification factor. For the numerical solution to remain bounded (i.e., be absolutely stable), the magnitude of this factor must not exceed $1$. This condition must hold for each eigenvalue $\\lambda$ of $A$ associated with a decaying mode of the true solution, which corresponds to eigenvalues with a negative real part ($\\text{Re}(\\lambda)  0$).\n\nThe absolute stability requirement is thus:\n$$ |1 + h\\lambda| \\le 1 $$\nLet the complex eigenvalue be $\\lambda = x + iy$, where $x = \\text{Re}(\\lambda)$ and $y = \\text{Im}(\\lambda)$. The step size $h$ is a positive real number ($h > 0$). The condition becomes:\n$$ |(1+hx) + i(hy)| \\le 1 $$\nSquaring both sides gives the equivalent condition on the squared magnitude:\n$$ (1+hx)^2 + (hy)^2 \\le 1 $$\n$$ 1 + 2hx + (hx)^2 + (hy)^2 \\le 1 $$\nSubtracting $1$ from both sides:\n$$ 2hx + h^2(x^2 + y^2) \\le 0 $$\n$$ h(2x + h(x^2 + y^2)) \\le 0 $$\nSince $h > 0$, we must have:\n$$ 2x + h(x^2 + y^2) \\le 0 $$\nThe problem concerns stiff systems, where stability is dictated by eigenvalues with large negative real parts, so we focus on $\\lambda$ with $x = \\text{Re}(\\lambda)  0$. In this case, we can rearrange the inequality to solve for $h$:\n$$ h(x^2 + y^2) \\le -2x $$\n$$ h \\le \\frac{-2x}{x^2+y^2} = \\frac{-2\\text{Re}(\\lambda)}{|\\lambda|^2} $$\nThis inequality must hold for every eigenvalue $\\lambda_i$ of $A$ with $\\text{Re}(\\lambda_i)0$. To ensure the stability of the entire system, the step size $h$ must satisfy the most restrictive of these conditions. Therefore, the largest step size for the Forward Euler method, denoted $h_{FE}$, is given by:\n$$ h_{FE} = \\min_{\\lambda_i \\text{ with } \\text{Re}(\\lambda_i)0} \\left( \\frac{-2\\text{Re}(\\lambda_i)}{|\\lambda_i|^2} \\right) $$\n\n### 2. Computational Methodology\n\nFor each test case, the following quantities are computed based on the principles outlined above.\n\n1.  **Largest Stable Forward Euler Step Size ($h_{FE}$)**: The eigenvalues $\\lambda_i$ of the matrix $A$ are computed numerically. Then, $h_{FE}$ is calculated using the formula derived above, taking the minimum over all eigenvalues.\n\n2.  **Forward Euler Approximation ($y_N$)**: The approximation at time $T = N h_{FE}$ is computed by applying the Forward Euler update $N$ times. Starting with $y_0$, the solution is advanced iteratively:\n    $$ y_{k+1} = (I + h_{FE} A) y_k \\quad \\text{for } k = 0, 1, \\dots, N-1 $$\n    This is computationally implemented as a loop of matrix-vector multiplications.\n\n3.  **Backward Euler Approximation ($y_{BE}$)**: The Backward Euler method is an implicit method defined by $y_{k+1} = y_k + h A y_{k+1}$. For a single step of size $H = T = N h_{FE}$, the update is given by $(I - H A) y_{BE} = y_0$. The approximation $y_{BE}$ is found by solving this linear system for $y_{BE}$.\n\n4.  **Exact Solution ($y_{exact}$)**: The exact solution is $y(T) = \\exp(A T) y_0$. While `numpy` itself does not have a matrix exponential function, the `scipy` library does (`scipy.linalg.expm`). Within the constraints of using only `numpy`, it can be computed using the eigenvalue decomposition of $A = S \\Lambda S^{-1}$. The exact solution can be expressed as:\n    $$ y(T) = S \\exp(\\Lambda T) S^{-1} y_0 $$\n    where $\\exp(\\Lambda T)$ is a diagonal matrix with entries $e^{\\lambda_i T}$. Computationally, we first find the coordinates of $y_0$ in the eigenvector basis, $c = S^{-1} y_0$, by solving the linear system $S c = y_0$. Then, the solution is reconstructed:\n    $$ y(T) = S \\begin{pmatrix} e^{\\lambda_1 T}   0 \\\\  \\ddots  \\\\ 0   e^{\\lambda_n T} \\end{pmatrix} c $$\n\n5.  **Error Calculation and Comparison**: The accuracy of the two numerical methods is assessed by computing the Euclidean norm ($\\ell^2$-norm) of the error vectors:\n    $$ \\text{Error}_{FE} = \\| y_N - y_{exact} \\|_2 $$\n    $$ \\text{Error}_{BE} = \\| y_{BE} - y_{exact} \\|_2 $$\n    A boolean value is then determined based on whether the Backward Euler error is strictly smaller than the Forward Euler error. All floating-point results are rounded to $8$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases, calculates the required values,\n    and prints the formatted result.\n    \"\"\"\n    test_cases = [\n        (np.array([[-1.0, 0.0], [0.0, -100.0]]), np.array([1.0, 1.0]), 50),\n        (np.array([[-2.0, -50.0], [50.0, -2.0]]), np.array([1.0, 1.0]), 400),\n        (np.array([[-0.1, 0.0], [0.0, -10000.0]]), np.array([1.0, -1.0]), 5000),\n    ]\n\n    all_results = []\n\n    for A, y0, N in test_cases:\n        # Part 1: Derive h_FE\n        eigenvalues, S = np.linalg.eig(A)\n        \n        # Filter for eigenvalues with negative real part\n        stable_eigenvalues = [ev for ev in eigenvalues if ev.real  0]\n        \n        if not stable_eigenvalues:\n            # Handle cases where no eigenvalues have negative real parts, though not\n            # expected for this problem's test suite.\n            h_fe = float('inf')\n        else:\n            h_constraints = [-2.0 * ev.real / (ev.real**2 + ev.imag**2) for ev in stable_eigenvalues]\n            h_fe = min(h_constraints)\n\n        # Part 2: Calculations\n        T = N * h_fe\n        n = A.shape[0]\n\n        # Forward Euler approximation y_N\n        y_fe = y0.copy()\n        M_fe = np.identity(n) + h_fe * A\n        for _ in range(N):\n            y_fe = M_fe @ y_fe\n        \n        # Backward Euler approximation y_BE\n        M_be_inv_sys = np.identity(n) - T * A\n        y_be = np.linalg.solve(M_be_inv_sys, y0)\n\n        # Exact solution y_exact\n        # Using y(t) = S * exp(Lambda*t) * S_inv * y0\n        S_inv_y0 = np.linalg.solve(S, y0)\n        exp_lambda_T = np.exp(eigenvalues * T)\n        y_exact = S @ (exp_lambda_T * S_inv_y0)\n        # Result should be real as A and y0 are real. Take real part to discard small imaginary noise.\n        y_exact = np.real(y_exact)\n\n        # Errors\n        err_fe = np.linalg.norm(y_fe - y_exact)\n        err_be = np.linalg.norm(y_be - y_exact)\n\n        # Comparison\n        is_be_better = err_be  err_fe\n\n        # Format results for output\n        # Round floats to 8 decimal places\n        h_fe_rounded = round(h_fe, 8)\n        err_fe_rounded = round(err_fe, 8)\n        err_be_rounded = round(err_be, 8)\n\n        case_result_vals = [\n            f\"{h_fe_rounded:.8f}\",\n            f\"{err_fe_rounded:.8f}\",\n            f\"{err_be_rounded:.8f}\",\n            str(is_be_better).lower()\n        ]\n        \n        all_results.append(f\"[{','.join(case_result_vals)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The power of implicit methods lies in their stability, but this comes at a price: at each time step, we must solve an equation to find the next solution value. For nonlinear ODEs, this becomes a nonlinear algebraic equation requiring an iterative solver. This hands-on problem demystifies this process by guiding you to implement the Backward Euler method for a nonlinear stiff ODE, using Newton's method to solve for the unknown at each step—a technique central to modern scientific computing software. ",
            "id": "2442982",
            "problem": "You are given the scalar ordinary differential equation (ODE)\n$$\n\\frac{dy}{dt} = f(t,y) = -k\\,(y - \\sin t) - b\\,y^3,\n$$\nwith initial condition\n$$\ny(0) = y_0,\n$$\non a uniform time grid over the interval\n$$\nt \\in [0,T], \\quad t_n = n\\,h, \\quad n=0,1,\\dots,N, \\quad \\text{with } Nh = T \\text{ and } h0.\n$$\nAngles in the function $\\sin t$ must be interpreted in radians. At each step, use the implicit one-step update\n$$\ny_{n+1} = y_n + h\\,f(t_{n+1}, y_{n+1}),\n$$\nand solve the resulting nonlinear algebraic equation for $y_{n+1}$ using Newton's method with an absolute stopping tolerance of $10^{-12}$ on either the residual or the Newton step, and a maximum of $50$ iterations per time step. Initialize Newton's method at each step with $y_{n}$.\n\nImplement a program that, for each parameter set in the test suite below, advances the numerical solution from $t=0$ to $t=T$ using the above method, and returns the approximation to $y(T)$.\n\nTest suite (each tuple is $(k,b,y_0,T,h)$):\n- Case A (stiff, nonlinear, forced): $(1000, 10, 0, 0.1, 0.001)$.\n- Case B (stiff, linear, forced): $(1000, 0, 1, 0.1, 0.001)$.\n- Case C (single large step, stiff, nonlinear): $(1000, 5, 1, 0.05, 0.05)$.\n- Case D (non-stiff, purely nonlinear): $(0, 50, 1, 0.02, 0.005)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the entries are the approximations to $y(T)$ for the cases A, B, C, D in this order, each rounded to exactly $8$ decimal places. For example, an output with four results must look like\n$$\n[\\text{res}_A,\\text{res}_B,\\text{res}_C,\\text{res}_D].\n$$",
            "solution": "The problem presented is a scalar ordinary differential equation (ODE) of the form $\\frac{dy}{dt} = f(t,y)$, which is to be solved numerically. The problem is well-defined, scientifically sound, and contains all necessary information for a unique solution via the specified algorithm. Therefore, I will proceed with its resolution.\n\nThe governing equation is:\n$$\n\\frac{dy}{dt} = f(t,y) = -k\\,(y - \\sin t) - b\\,y^3, \\quad y(0) = y_0\n$$\nThis equation describes a system with a linear relaxation term with rate $k$ towards a sinusoidal driving function $\\sin t$, and a nonlinear damping term proportional to $y^3$. The magnitude of $k$ determines the stiffness of the equation. For large $k$, explicit numerical methods would require prohibitively small time steps $h \\ll 1/k$ to maintain stability. The problem correctly prescribes an implicit method to handle this stiffness.\n\nThe numerical integration is performed using the implicit one-step Backward Euler method. For a uniform time grid $t_n = n\\,h$, the update rule from time $t_n$ to $t_{n+1}$ is given by:\n$$\ny_{n+1} = y_n + h\\,f(t_{n+1}, y_{n+1})\n$$\nwhere $y_n$ is the approximation of $y(t_n)$. This equation is implicit because the unknown value $y_{n+1}$ appears on both sides. To solve for $y_{n+1}$ at each time step, we must solve a nonlinear algebraic equation. This equation can be expressed by defining a residual function $R(x)$ whose root we seek:\n$$\nR(x) = x - y_n - h\\,f(t_{n+1}, x) = 0\n$$\nwhere $x$ represents the unknown $y_{n+1}$.\n\nTo find the root of $R(x)=0$, we employ Newton's method, an iterative procedure defined by:\n$$\nx^{(j+1)} = x^{(j)} - \\frac{R(x^{(j)})}{R'(x^{(j)})}\n$$\nwhere $x^{(j)}$ is the guess for the root at iteration $j$, and $R'(x) = \\frac{dR}{dx}$ is the derivative of the residual function with respect to $x$. The initial guess for each time step is specified as the value from the previous step, $x^{(0)} = y_n$.\n\nThe derivative $R'(x)$ is calculated as:\n$$\nR'(x) = \\frac{d}{dx} \\left( x - y_n - h\\,f(t_{n+1}, x) \\right) = 1 - h\\,\\frac{\\partial f}{\\partial y}(t_{n+1}, x)\n$$\nFor the given function $f(t,y)$, its partial derivative with respect to $y$ is:\n$$\n\\frac{\\partial f}{\\partial y}(t,y) = \\frac{\\partial}{\\partial y} \\left( -k\\,(y - \\sin t) - b\\,y^3 \\right) = -k - 3\\,b\\,y^2\n$$\nSubstituting this into the expression for $R'(x)$, we obtain:\n$$\nR'(x) = 1 - h\\,(-k - 3\\,b\\,x^2) = 1 + h\\,k + 3\\,h\\,b\\,x^2\n$$\nThe Newton's iteration for $y_{n+1}$ is therefore a loop that updates the guess $x^{(j)}$ until a stopping criterion is met. The criteria are an absolute tolerance of $10^{-12}$ on the magnitude of the residual, $|R(x^{(j)})|$, or on the magnitude of the Newton step, $|x^{(j+1)} - x^{(j)}|$. The process is terminated if convergence is not achieved within $50$ iterations.\n\nThe overall algorithm for each test case is as follows:\n1.  Initialize parameters $(k, b, y_0, T, h)$ and calculate the total number of steps $N = T/h$.\n2.  Set the initial solution $y = y_0$ and time $t = 0$.\n3.  Loop from $n=0$ to $N-1$:\n    a.  Determine the next time point $t_{n+1} = (n+1)h$.\n    b.  Initialize the Newton's method guess for $y_{n+1}$ as $x^{(0)} = y_n$.\n    c.  Iterate using Newton's method for a maximum of $50$ steps to find $x$ that solves $R(x)=0$. In each iteration $j$:\n        i.  Calculate the residual $R(x^{(j)})$. If its absolute value is less than $10^{-12}$, convergence is achieved. The result for the time step is $x^{(j)}$.\n        ii. Calculate the derivative $R'(x^{(j)})$.\n        iii. Calculate the Newton step $\\Delta x = -R(x^{(j)}) / R'(x^{(j)})$.\n        iv. Update the guess: $x^{(j+1)} = x^{(j)} + \\Delta x$.\n        v. If the absolute value of the step $|\\Delta x|$ is less than $10^{-12}$, convergence is achieved. The result for the time step is $x^{(j+1)}$.\n    d.  Update the solution for the next time step: $y_{n+1} = x_{converged}$.\n4.  The final value of $y$ after $N$ steps is the required approximation of $y(T)$. This procedure is repeated for all specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(k, b, y0, T, h):\n    \"\"\"\n    Solves the ODE dy/dt = -k*(y - sin(t)) - b*y^3 using Backward Euler\n    with Newton's method for the nonlinear solve.\n    \n    Args:\n        k (float): Stiffness parameter.\n        b (float): Nonlinearity parameter.\n        y0 (float): Initial condition y(0).\n        T (float): Final time.\n        h (float): Time step size.\n\n    Returns:\n        float: The numerical solution y(T).\n    \"\"\"\n\n    # Define the ODE function f(t, y)\n    def f(t, y, k, b):\n        return -k * (y - np.sin(t)) - b * y**3\n\n    # Define the partial derivative of f with respect to y, df/dy\n    def df_dy(t, y, k, b):\n        return -k - 3 * b * y**2\n\n    # Ensure n_steps is integer\n    n_steps = int(round(T / h))\n    if not np.isclose(n_steps * h, T):\n        # This case is not expected based on the problem statement's test cases\n        # but is good practice.\n        raise ValueError(\"T must be an integer multiple of h.\")\n\n    y_current = y0\n\n    # Main time-stepping loop\n    for n in range(n_steps):\n        t_next = (n + 1) * h\n        y_guess = y_current\n        \n        # Newton's method to solve y_next = y_current + h * f(t_next, y_next)\n        # This is equivalent to finding the root of R(y_next) = 0 where\n        # R(y_next) = y_next - y_current - h * f(t_next, y_next)\n        for _ in range(50): # Maximum of 50 iterations\n            \n            # Calculate residual at the current guess\n            residual = y_guess - y_current - h * f(t_next, y_guess, k, b)\n\n            # Check for convergence on residual\n            if abs(residual)  1e-12:\n                break\n                \n            # Calculate Jacobian of the residual function\n            # R'(y) = 1 - h * df/dy\n            jac_residual = 1.0 - h * df_dy(t_next, y_guess, k, b)\n            \n            # Avoid division by zero, although not an issue for this problem's parameters\n            if jac_residual == 0:\n                break\n\n            # Calculate Newton step\n            step = -residual / jac_residual\n            \n            # Update the guess\n            y_guess += step\n            \n            # Check for convergence on the step size\n            if abs(step)  1e-12:\n                break\n        \n        y_current = y_guess\n\n    return y_current\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite: (k, b, y0, T, h)\n    test_cases = [\n        (1000, 10, 0, 0.1, 0.001),     # Case A\n        (1000, 0, 1, 0.1, 0.001),      # Case B\n        (1000, 5, 1, 0.05, 0.05),       # Case C\n        (0, 50, 1, 0.02, 0.005)        # Case D\n    ]\n\n    results = []\n    for params in test_cases:\n        y_final = run_simulation(*params)\n        results.append(y_final)\n\n    # Format output as specified\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}