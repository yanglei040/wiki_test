{
    "hands_on_practices": [
        {
            "introduction": "Similarity transformations preserve many important matrix properties, including the trace, determinant, and characteristic polynomial. This might suggest that any two matrices sharing these invariants must be similar. This exercise challenges that intuition by requiring you to construct two matrices that are identical in these respects yet are fundamentally not similar . This practice will deepen your understanding of what similarity truly represents, forcing a look beyond simple invariants to the more telling structural properties of a matrix, such as its minimal polynomial or Jordan Normal Form.",
            "id": "3273917",
            "problem": "Let $A$ and $B$ be real $4 \\times 4$ matrices. Using only core definitions and well-tested facts about similarity transformations and matrix invariants, construct two explicit matrices $A$ and $B$ that are not similar to each other yet share all of the following properties:\n- the same trace,\n- the same determinant,\n- the same characteristic polynomial,\n- the same Frobenius norm.\n\nYou must justify, from first principles, why $A$ and $B$ are not similar and why each of the four listed quantities is equal for the two matrices. You may use the facts that for a triangular matrix the eigenvalues are the diagonal entries, that the trace is the sum of diagonal entries and the determinant is the product of diagonal entries, and that the Frobenius norm is defined by $\\|X\\|_{F} = \\sqrt{\\sum_{i,j} x_{ij}^{2}} = \\sqrt{\\mathrm{trace}(X^{\\mathsf{T}} X)}$.\n\nFinally, report the exact value of the common Frobenius norm of your constructed matrices. Your answer must be a single closed-form expression. Do not round.",
            "solution": "The problem requires the construction of two real $4 \\times 4$ matrices, $A$ and $B$, which are not similar but share the same trace, determinant, characteristic polynomial, and Frobenius norm. We must justify each of these properties from first principles.\n\nA core principle of linear algebra is that two matrices are similar if and only if they represent the same linear transformation under different bases. This implies that similar matrices share many properties, known as similarity invariants. These include the characteristic polynomial, eigenvalues, trace, and determinant. However, the converse is not true; sharing these properties does not guarantee similarity. The crucial invariant that determines similarity is the Jordan Normal Form. Two matrices are similar if and only if they have the same Jordan Normal Form, up to a permutation of the Jordan blocks.\n\nOur strategy will be to construct two matrices, $A$ and $B$, that have the same eigenvalues but different Jordan block structures. This will ensure they have the same characteristic polynomial (and hence the same trace and determinant) but are not similar. We will specifically construct them in Jordan Normal Form, which simplifies the analysis. The final constraint is to ensure they also have the same Frobenius norm.\n\nLet's choose the eigenvalues for our $4 \\times 4$ matrices. For simplicity, we choose all eigenvalues to be $0$. The characteristic polynomial for any such matrix must be $p(\\lambda) = (\\lambda-0)^4 = \\lambda^4$. The possible Jordan Normal Forms correspond to the integer partitions of $4$. We will choose two different partitions of $4$ that coincidentally produce the same Frobenius norm. Consider the partitions $3+1$ and $2+2$.\n\nLet matrix $A$ be the Jordan Normal Form corresponding to the partition $3+1$ for the eigenvalue $\\lambda=0$. This form has one Jordan block of size $3$ and one of size $1$:\n$$ A = \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} $$\nLet matrix $B$ be the Jordan Normal Form corresponding to the partition $2+2$ for the eigenvalue $\\lambda=0$. This form has two Jordan blocks of size $2$:\n$$ B = \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} $$\nBoth $A$ and $B$ are real $4 \\times 4$ matrices. We will now validate that they satisfy all the required conditions.\n\n1.  **Characteristic Polynomial:** The characteristic polynomial is defined as $p(\\lambda) = \\det(M - \\lambda I)$. For a triangular matrix, the determinant is the product of its diagonal entries.\n    *   For matrix $A$, $A - \\lambda I$ is an upper triangular matrix with diagonal entries $(-\\lambda, -\\lambda, -\\lambda, -\\lambda)$. Thus, its characteristic polynomial is $p_A(\\lambda) = (-\\lambda)(-\\lambda)(-\\lambda)(-\\lambda) = \\lambda^4$. The eigenvalues of $A$ are all $0$.\n    *   For matrix $B$, $B - \\lambda I$ is also an upper triangular matrix with diagonal entries $(-\\lambda, -\\lambda, -\\lambda, -\\lambda)$. Thus, its characteristic polynomial is $p_B(\\lambda) = (-\\lambda)(-\\lambda)(-\\lambda)(-\\lambda) = \\lambda^4$. The eigenvalues of $B$ are all $0$.\n    Therefore, $A$ and $B$ have the same characteristic polynomial: $p_A(\\lambda) = p_B(\\lambda) = \\lambda^4$.\n\n2.  **Trace:** The trace of a matrix is the sum of its diagonal entries.\n    *   $\\mathrm{trace}(A) = 0 + 0 + 0 + 0 = 0$.\n    *   $\\mathrm{trace}(B) = 0 + 0 + 0 + 0 = 0$.\n    Therefore, $\\mathrm{trace}(A) = \\mathrm{trace}(B)$. This is also consistent with the fact that the trace is the sum of the eigenvalues.\n\n3.  **Determinant:** The determinant of a triangular matrix is the product of its diagonal entries.\n    *   $\\det(A) = 0 \\times 0 \\times 0 \\times 0 = 0$.\n    *   $\\det(B) = 0 \\times 0 \\times 0 \\times 0 = 0$.\n    Therefore, $\\det(A) = \\det(B)$. This is consistent with the fact that the determinant is the product of the eigenvalues.\n\n4.  **Frobenius Norm:** The Frobenius norm is defined as $\\|X\\|_F = \\sqrt{\\sum_{i,j=1}^4 x_{ij}^2}$.\n    *   For matrix $A$, the only non-zero entries are $A_{12}=1$ and $A_{23}=1$. The sum of the squares of the entries is $1^2 + 1^2 = 2$.\n        $$ \\|A\\|_F = \\sqrt{1^2 + 1^2} = \\sqrt{2} $$\n    *   For matrix $B$, the only non-zero entries are $B_{12}=1$ and $B_{34}=1$. The sum of the squares of the entries is $1^2 + 1^2 = 2$.\n        $$ \\|B\\|_F = \\sqrt{1^2 + 1^2} = \\sqrt{2} $$\n    Therefore, $\\|A\\|_F = \\|B\\|_F = \\sqrt{2}$.\n\n5.  **Non-Similarity:** Two matrices are not similar if they differ in any similarity invariant. A fundamental invariant is the minimal polynomial. The minimal polynomial $m_M(\\lambda)$ of a matrix $M$ is the monic polynomial of least degree such that $m_M(M) = 0$. Similar matrices must have the same minimal polynomial.\n    *   For matrix $A$, we compute its powers:\n        $$ A^2 = A \\cdot A = \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} $$\n        $$ A^3 = A^2 \\cdot A = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} = 0_{4 \\times 4} $$\n        Since $A^2 \\neq 0$ and $A^3 = 0$, the minimal polynomial of $A$ is $m_A(\\lambda) = \\lambda^3$.\n    *   For matrix $B$, we compute its powers:\n        $$ B^2 = B \\cdot B = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} = 0_{4 \\times 4} $$\n        Since $B \\neq 0$ and $B^2 = 0$, the minimal polynomial of $B$ is $m_B(\\lambda) = \\lambda^2$.\n    *   Because $m_A(\\lambda) \\neq m_B(\\lambda)$, the matrices $A$ and $B$ are not similar. This reflects their different Jordan structures: the degree of the factor $(\\lambda - \\lambda_i)$ in the minimal polynomial corresponds to the size of the largest Jordan block for the eigenvalue $\\lambda_i$. For $A$, the largest block has size $3$, while for $B$, the largest block has size $2$.\n\nWe have successfully constructed two matrices $A$ and $B$ that satisfy all the given conditions. The common Frobenius norm of these matrices is $\\sqrt{2}$.",
            "answer": "$$\n\\boxed{\\sqrt{2}}\n$$"
        },
        {
            "introduction": "Beyond being a tool for theoretical analysis, similarity transformations are a cornerstone of modern numerical algorithms. This practice demonstrates a powerful and constructive application: using a numerically stable orthogonal transformation to \"deflate\" a known eigenvalue from a matrix . By applying a carefully constructed Householder reflector, you will transform the matrix into a block form that isolates the known eigenvalue, effectively reducing the problem to a smaller, simpler one. This technique is a fundamental building block in sophisticated eigenvalue solvers like the QR algorithm.",
            "id": "3273907",
            "problem": "You are given the task of designing and implementing a numerically stable algorithm that uses a sequence of similarity transformations to deflate a known eigenvalue/eigenvector pair from a real square matrix. The foundational facts you may assume are: (i) similarity transformations preserve eigenvalues, (ii) an orthogonal similarity, implemented with an orthogonal matrix, preserves the Euclidean norm and is backward stable in floating-point arithmetic, and (iii) a Householder reflector is an orthogonal matrix that can map one nonzero vector to another of the same norm by reflecting across a hyperplane. No other results should be assumed.\n\nYour program must implement the following algorithmic goal. Given a real matrix $A \\in \\mathbb{R}^{n \\times n}$ and a known eigenpair $\\left(\\lambda, v\\right)$ with $v \\neq 0$ such that $A v = \\lambda v$, construct an orthogonal matrix $Q$ using a numerically stable Householder-based procedure so that $Q e_1 = \\widehat{v}$ with $\\widehat{v} = v / \\|v\\|_2$, where $e_1$ is the first canonical basis vector. Then form the orthogonal similarity $B = Q^\\top A Q$. Show that, under exact arithmetic, the first column of $B$ is $[\\lambda, 0, \\dots, 0]^\\top$ and therefore $B$ has the block form\n$$\nB = \\begin{bmatrix}\n\\lambda & * \\\\\n0 & T\n\\end{bmatrix},\n$$\nwhere $T \\in \\mathbb{R}^{(n-1) \\times (n-1)}$ contains the remaining eigen-information. The deflated matrix is $T$. Your implementation must compute $Q$ in a manner that avoids catastrophic cancellation when $v$ is nearly aligned with $e_1$ or nearly opposite to $e_1$. You must then verify numerically that the deflation is correct and stable by returning three diagnostics for each test case:\n- $E_{\\text{orth}} = \\|Q^\\top Q - I\\|_F$, the Frobenius norm of the orthogonality defect,\n- $E_{\\text{col}} = \\|B_{2:n,1}\\|_2$, the Euclidean norm of the entries below the first element in the first column of $B$,\n- $E_{\\text{spec}} = \\left\\| \\operatorname{sort}(\\sigma(A)) - \\operatorname{sort}\\left(\\{\\lambda\\} \\cup \\sigma(T)\\right) \\right\\|_2$, the Euclidean norm of the difference between the sorted eigenvalues of $A$ and the multiset union of $\\{\\lambda\\}$ with the sorted eigenvalues of $T$. Here $\\sigma(\\cdot)$ denotes the multiset of eigenvalues.\n\nImplementation requirements:\n- Construct $Q$ via a single Householder reflector chosen to map $e_1$ to $\\widehat{v}$ using a sign strategy that avoids cancellation. You must not use any routine that directly constructs $Q$ from $v$ without explicitly forming and applying a Householder transformation.\n- Use only orthogonal similarity transformations.\n- Compute eigenvalues with a symmetric method when applicable. All provided test matrices are symmetric; you may assume real symmetric matrices.\n\nTest suite:\nImplement and run your algorithm on the following four test cases. Each test defines $A$, $\\lambda$, and $v$ either directly or via an orthogonal construction $A = Q_{\\text{gen}} D Q_{\\text{gen}}^\\top$, where $D$ is diagonal with specified diagonal entries, and $Q_{\\text{gen}}$ is constructed from plane rotations with specified angles in radians.\n\n- Test case 1 (general $4 \\times 4$, non-normalized $v$):\n    - Dimension $n = 4$.\n    - Diagonal eigenvalues $D = \\operatorname{diag}(2.5, -1.0, 3.0, 0.5)$.\n    - $Q_{\\text{gen}} = R(0,1,\\theta_1) R(2,3,\\theta_2)$ with $\\theta_1 = 0.7$ and $\\theta_2 = -0.4$ in radians, where $R(i,j,\\theta)$ is the identity except for the $2 \\times 2$ subblock on indices $\\{i,j\\}$ equal to $\\begin{bmatrix}\\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$.\n    - $A = Q_{\\text{gen}} D Q_{\\text{gen}}^\\top$.\n    - Known pair: $\\lambda = 2.5$ and $v = 7.3 \\cdot (Q_{\\text{gen}} e_1)$.\n\n- Test case 2 (boundary $3 \\times 3$, $v$ exactly aligned with $e_1$):\n    - Dimension $n = 3$.\n    - $A = \\operatorname{diag}(10.0, 10^{-8}, -3.0)$.\n    - Known pair: $\\lambda = 10.0$ and $v = e_1$.\n\n- Test case 3 (repeated eigenvalue, $4 \\times 4$):\n    - Dimension $n = 4$.\n    - Diagonal eigenvalues $D = \\operatorname{diag}(1.0, 1.0, 2.0, -4.0)$.\n    - $Q_{\\text{gen}} = R(0,2,\\theta_3)$ with $\\theta_3 = 1.1$ radians.\n    - $A = Q_{\\text{gen}} D Q_{\\text{gen}}^\\top$.\n    - Known pair: $\\lambda = 1.0$ and $v = Q_{\\text{gen}} e_1$.\n\n- Test case 4 (sign edge, negative leading component, $3 \\times 3$):\n    - Dimension $n = 3$.\n    - Diagonal eigenvalues $D = \\operatorname{diag}(5.0, -2.0, -1.0)$.\n    - $Q_{\\text{gen}} = R(0,1,\\theta_4)$ with $\\theta_4 = 2.2$ radians.\n    - $A = Q_{\\text{gen}} D Q_{\\text{gen}}^\\top$.\n    - Known pair: $\\lambda = 5.0$ and $v = 0.3 \\cdot (Q_{\\text{gen}} e_1)$.\n\nOutput specification:\n- For each test case, compute the triple $\\left(E_{\\text{orth}}, E_{\\text{col}}, E_{\\text{spec}}\\right)$ as defined above.\n- Round each numeric value to $10$ significant digits and print in scientific notation with a lower-case $e$.\n- Your program should produce a single line of output containing a list of four sublists, one per test case, each sublist holding the three diagnostics in order, with no spaces. For example: \"[[x11,x12,x13],[x21,x22,x23],[x31,x32,x33],[x41,x42,x43]]\". No other output should be produced.",
            "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed problem in numerical linear algebra. The task is to design and implement a numerically stable algorithm for eigenvalue deflation using a Householder similarity transformation. All provided assumptions and definitions are scientifically sound and self-consistent.\n\nThe core of the problem is to construct an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ such that for a given normalized eigenvector $\\widehat{v} = v/\\|v\\|_2$, the transformation satisfies $Q e_1 = \\widehat{v}$, where $e_1 = [1, 0, \\dots, 0]^\\top$ is the first canonical basis vector. This transformation is then used to form the similarity $B = Q^\\top A Q$, which deflates the known eigenvalue $\\lambda$.\n\nFirst, we establish the theoretical basis for the deflation. Given that $A \\in \\mathbb{R}^{n \\times n}$ has an eigenpair $(\\lambda, v)$, where $Av = \\lambda v$, and $\\widehat{v} = v/\\|v\\|_2$, we have $A\\widehat{v} = \\lambda\\widehat{v}$. The matrix $B$ is formed by an orthogonal similarity transformation $B = Q^\\top A Q$. The first column of $B$ is given by the vector $B e_1$. We can compute this as follows:\n$$\nB e_1 = (Q^\\top A Q) e_1 = Q^\\top A (Q e_1)\n$$\nBy our construction requirement, $Q e_1 = \\widehat{v}$. Substituting this into the equation gives:\n$$\nB e_1 = Q^\\top A \\widehat{v}\n$$\nUsing the eigenvalue property $A\\widehat{v} = \\lambda\\widehat{v}$:\n$$\nB e_1 = Q^\\top (\\lambda \\widehat{v}) = \\lambda (Q^\\top \\widehat{v})\n$$\nSince $Q$ is an orthogonal matrix, $Q^\\top Q = I$. Applying $Q^\\top$ to our construction requirement $Q e_1 = \\widehat{v}$ yields $Q^\\top(Q e_1) = Q^\\top \\widehat{v}$, which simplifies to $I e_1 = e_1 = Q^\\top \\widehat{v}$. Substituting this result back, we find:\n$$\nB e_1 = \\lambda e_1 = \\begin{bmatrix} \\lambda \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n$$\nThis proves that the first column of $B$ is $\\lambda e_1$. Since the problem specifies that the matrix $A$ (and thus $B$) is symmetric, $B$ must have the block-diagonal form:\n$$\nB = \\begin{bmatrix}\n\\lambda & 0 \\\\\n0 & T\n\\end{bmatrix}\n$$\nwhere $T \\in \\mathbb{R}^{(n-1) \\times (n-1)}$ is the deflated matrix. The eigenvalues of $A$, denoted $\\sigma(A)$, are preserved under similarity transformation, so $\\sigma(A) = \\sigma(B)$. For a block-diagonal matrix, the set of eigenvalues is the union of the eigenvalues of the diagonal blocks. Therefore, $\\sigma(A) = \\sigma(B) = \\{\\lambda\\} \\cup \\sigma(T)$, which justifies the spectral error diagnostic $E_{\\text{spec}}$.\n\nThe main implementation challenge is the numerically stable construction of the orthogonal matrix $Q$. The problem requires $Q$ to be constructed from a single Householder reflector. A Householder reflector $H$ is of the form $H = I - 2 \\frac{u u^\\top}{u^\\top u}$ for some non-zero vector $u$. Such a matrix is symmetric ($H=H^\\top$) and orthogonal ($H^\\top H = I$). To find an orthogonal matrix $Q$ that maps $e_1$ to $\\widehat{v}$ (i.e., $Qe_1=\\widehat{v}$), we must devise a strategy that avoids catastrophic cancellation. This occurs if we form the reflection vector $u$ by subtracting two nearly identical vectors.\n\nA direct approach would be to construct a reflector $H$ that maps $e_1$ to $\\widehat{v}$ using the vector $u = e_1 - \\widehat{v}$. The resulting reflector would be $H = I - 2 \\frac{(e_1-\\widehat{v})(e_1-\\widehat{v})^\\top}{\\|e_1-\\widehat{v}\\|_2^2}$. However, if $\\widehat{v} \\approx e_1$, the vector $u$ would be close to the zero vector, and its numerical computation would suffer from catastrophic cancellation, leading to instability.\n\nA stable procedure involves choosing a reflection vector that avoids this subtraction. We note that a Householder matrix $H_u$ built from vector $u$ maps a vector $x$ to a reflection across the hyperplane orthogonal to $u$. The standard numerically stable method to construct a reflector $H$ that maps a vector $x$ to $\\alpha e_1$ uses the reflection vector $u = x - \\alpha e_1$ with $\\alpha = - \\operatorname{sign}(x_1) \\|x\\|_2$.\n\nOur problem is inverse: we want to map $e_1$ to $\\widehat{v}$. Let's construct a reflector $H$ based on the vector $u = \\widehat{v} + \\sigma e_1$, where $\\sigma$ is a sign chosen for stability. The action of this reflector on $e_1$ is:\n$$\nH e_1 = e_1 - 2 \\frac{(\\widehat{v} + \\sigma e_1)(\\widehat{v} + \\sigma e_1)^\\top e_1}{\\|\\widehat{v} + \\sigma e_1\\|_2^2} = e_1 - 2 \\frac{(\\widehat{v} + \\sigma e_1)(\\widehat{v}_1 + \\sigma)}{\\|\\widehat{v}\\|^2_2 + 2\\sigma \\widehat{v}_1 + \\sigma^2\\|e_1\\|^2_2} = e_1 - 2 \\frac{(\\widehat{v} + \\sigma e_1)(\\widehat{v}_1 + \\sigma)}{2 + 2\\sigma \\widehat{v}_1} = e_1 - \\frac{\\widehat{v} + \\sigma e_1}{\\sigma} = -\\frac{1}{\\sigma}\\widehat{v}\n$$\nTo ensure the denominator $2 + 2\\sigma \\widehat{v}_1$ is far from zero, we must choose $\\sigma$ such that $\\sigma \\widehat{v}_1 \\ge 0$. We set $\\sigma = \\operatorname{sign}(\\widehat{v}_1)$, using the convention $\\operatorname{sign}(0)=1$. This makes the denominator $2(1 + |\\widehat{v}_1|)$, which is bounded between $2$ and $4$, ensuring stability.\nWith this reflector $H$, we have $H e_1 = -(\\frac{1}{\\sigma}) \\widehat{v} = -\\sigma \\widehat{v}$ since $\\sigma = \\pm 1$. We desire $Q e_1 = \\widehat{v}$. We can achieve this by setting $Q = -\\sigma H$. Since both $\\sigma$ (a scalar of magnitude $1$) and $H$ (an orthogonal matrix) are orthogonal transformations, their product $Q$ is also orthogonal.\nThe complete algorithm is as follows:\n1.  Given $A$, $\\lambda$, and $v$, normalize $v$ to get $\\widehat{v} = v/\\|v\\|_2$.\n2.  Choose the sign $\\sigma = 1$ if $\\widehat{v}_1 \\ge 0$, and $\\sigma = -1$ if $\\widehat{v}_1 < 0$.\n3.  Construct the reflection vector $u = \\widehat{v} + \\sigma e_1$.\n4.  Construct the Householder reflector $H = I - 2 \\frac{u u^\\top}{u^\\top u}$.\n5.  Define the orthogonal matrix $Q = -\\sigma H$. This construction guarantees $Qe_1 = \\widehat{v}$ in a numerically stable way.\n6.  Compute the deflated matrix $B = Q^\\top A Q$.\n7.  Extract the submatrix $T = B_{2:n, 2:n}$.\n8.  Calculate the three diagnostic errors: $E_{\\text{orth}} = \\|Q^\\top Q - I\\|_F$, $E_{\\text{col}} = \\|B_{2:n,1}\\|_2$, and $E_{\\text{spec}} = \\| \\operatorname{sort}(\\sigma(A)) - \\operatorname{sort}(\\{\\lambda\\} \\cup \\sigma(T)) \\|_2$. For symmetric matrices, eigenvalue computation should use `numpy.linalg.eigvalsh`.\n\nThis procedure is implemented for the provided test cases. Case 2, where $v = e_1$, is a critical test of the sign choice logic. In this case, $\\widehat{v}=e_1$, so $\\widehat{v}_1=1$ and $\\sigma=1$. The vector $u$ becomes $e_1+e_1=2e_1$, which is well-defined. The unstable choice $u = \\widehat{v} - \\sigma e_1$ would have resulted in $u=0$. Case 4, where $\\widehat{v}_1 < 0$, tests the $\\sigma=-1$ path.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the eigenvalue deflation problem for a suite of test cases.\n    \"\"\"\n\n    def make_rotation_matrix(n, i, j, theta):\n        \"\"\"Constructs a Givens rotation matrix R(i,j,theta) of size n x n.\"\"\"\n        R = np.eye(n)\n        c = np.cos(theta)\n        s = np.sin(theta)\n        R[i, i] = c\n        R[i, j] = -s\n        R[j, i] = s\n        R[j, j] = c\n        return R\n\n    # Define the test cases from the problem statement.\n    test_cases_params = [\n        {\n            \"n\": 4,\n            \"D_diag\": [2.5, -1.0, 3.0, 0.5],\n            \"rotations\": [(0, 1, 0.7), (2, 3, -0.4)],\n            \"lambda\": 2.5,\n            \"v_construction\": lambda Q_gen: 7.3 * Q_gen[:, 0]\n        },\n        {\n            \"n\": 3,\n            \"A_direct\": np.diag([10.0, 1e-8, -3.0]),\n            \"lambda\": 10.0,\n            \"v_construction\": lambda Q_gen: np.array([1.0, 0.0, 0.0])\n        },\n        {\n            \"n\": 4,\n            \"D_diag\": [1.0, 1.0, 2.0, -4.0],\n            \"rotations\": [(0, 2, 1.1)],\n            \"lambda\": 1.0,\n            \"v_construction\": lambda Q_gen: Q_gen[:, 0]\n        },\n        {\n            \"n\": 3,\n            \"D_diag\": [5.0, -2.0, -1.0],\n            \"rotations\": [(0, 1, 2.2)],\n            \"lambda\": 5.0,\n            \"v_construction\": lambda Q_gen: 0.3 * Q_gen[:, 0]\n        }\n    ]\n\n    results = []\n    \n    for params in test_cases_params:\n        n = params[\"n\"]\n        lam = params[\"lambda\"]\n        \n        if \"A_direct\" in params:\n            A = params[\"A_direct\"]\n            v = params[\"v_construction\"](None)\n        else:\n            D = np.diag(params[\"D_diag\"])\n            Q_gen = np.eye(n)\n            for i, j, theta in params[\"rotations\"]:\n                Q_gen = Q_gen @ make_rotation_matrix(n, i, j, theta)\n            A = Q_gen @ D @ Q_gen.T\n            v = params[\"v_construction\"](Q_gen)\n\n        # 1. Normalize the eigenvector\n        v_hat = v / np.linalg.norm(v)\n\n        # 2. Choose sign for stability\n        sigma = 1.0 if v_hat[0] >= 0 else -1.0\n        \n        # 3. Construct the reflection vector u\n        e1 = np.zeros(n)\n        e1[0] = 1.0\n        u = v_hat + sigma * e1\n\n        # 4. Construct the Householder reflector H\n        # H = I - 2 * (u u^T) / (u^T u)\n        u_outer_u = np.outer(u, u)\n        u_dot_u = np.dot(u, u)\n        # Avoid division by zero, though theoretically u_dot_u is always >= 2.\n        if u_dot_u < np.finfo(float).eps:\n             # This case should not be reached with the stable sign choice.\n             # If it were to happen, u is zero vector, so H is identity.\n             H = np.eye(n)\n        else:\n             H = np.eye(n) - 2.0 * u_outer_u / u_dot_u\n\n        # 5. Define the orthogonal matrix Q\n        Q = -sigma * H\n        \n        # 6. Compute the deflated matrix B\n        B = Q.T @ A @ Q\n        \n        # 7. Extract the submatrix T\n        T = B[1:, 1:]\n        \n        # 8. Calculate diagnostics\n        # E_orth: Orthogonality defect\n        E_orth = np.linalg.norm(Q.T @ Q - np.eye(n), 'fro')\n        \n        # E_col: Subdiagonal column norm\n        E_col = np.linalg.norm(B[1:, 0])\n        \n        # E_spec: Spectral difference norm\n        eig_A = np.linalg.eigvalsh(A)\n        eig_T = np.linalg.eigvalsh(T)\n        \n        spec_A_sorted = np.sort(eig_A)\n        spec_B_sorted = np.sort(np.concatenate(([lam], eig_T)))\n        \n        E_spec = np.linalg.norm(spec_A_sorted - spec_B_sorted)\n        \n        results.append((E_orth, E_col, E_spec))\n\n    # Format output as specified\n    formatted_results = []\n    for res_tuple in results:\n        formatted_results.append(','.join([f'{x:.9e}' for x in res_tuple]))\n    \n    print(f\"[[{'],['.join(formatted_results)}]]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In the idealized world of exact arithmetic, any similarity transformation $B = P^{-1}AP$ perfectly preserves the eigenvalues of $A$. However, in the finite-precision world of computers, the properties of the transformation matrix $P$ are critically important. This hands-on experiment provides a dramatic, quantitative illustration of this fact by contrasting a numerically stable orthogonal transformation with an unstable one based on an ill-conditioned matrix $P$ . You will discover firsthand how an ill-conditioned change of basis can catastrophically corrupt the computed eigenvalues, providing a compelling lesson on why choosing stable algorithms is paramount in scientific computing.",
            "id": "3273813",
            "problem": "Design and implement a complete, runnable program that conducts a reproducible numerical experiment to compare the finite-precision accuracy of eigenvalues under two classes of similarity transformations applied to a real symmetric matrix: an orthogonal similarity transformation and a non-orthogonal similarity transformation with an ill-conditioned matrix. The context is numerical methods and scientific computing at an advanced undergraduate level.\n\nStart from the following foundational facts:\n- A similarity transformation is defined by $B = S^{-1} A S$ with an invertible matrix $S$ and a square matrix $A$. In exact arithmetic, the eigenvalues of $A$ and $B$ are identical.\n- A real symmetric matrix $A$ has real eigenvalues and can be diagonalized by an orthogonal matrix.\n- In floating-point arithmetic with unit roundoff $u$, orthogonal transformations are norm-preserving and typically backward stable, while forming $S^{-1} A S$ with an ill-conditioned $S$ can amplify roundoff and lead to larger backward errors.\n\nYour program must:\n1. Construct the symmetric test matrices $A$ and transformation matrices $Q$ (orthogonal) and $P$ (ill-conditioned), as specified below. Use double precision (standard Python and NumPy default) and deterministic seeds where noted.\n2. Form the transformed matrices $A_{Q} = Q^{\\mathsf{T}} A Q$ and $A_{P} = P^{-1} A P$ in floating-point arithmetic.\n3. Compute a reference spectrum $\\lambda_{\\mathrm{ref}}$ from $A$ using a symmetric eigensolver.\n4. Compute the spectra $\\lambda_{Q}$ from $A_{Q}$ using a symmetric eigensolver and $\\lambda_{P}$ from $A_{P}$ using a general eigensolver. Sort the eigenvalues by their real parts before comparison.\n5. Quantify the accuracy by the relative $2$-norm error\n   $$\\mathrm{rel\\_err}(\\lambda_{\\mathrm{ref}}, \\lambda_{\\mathrm{comp}}) = \\frac{\\lVert \\lambda_{\\mathrm{comp}} - \\lambda_{\\mathrm{ref}} \\rVert_{2}}{\\max\\left(\\lVert \\lambda_{\\mathrm{ref}} \\rVert_{2}, \\varepsilon\\right)},$$\n   where $\\varepsilon$ is the machine epsilon used to avoid division by zero. When $\\lambda_{\\mathrm{comp}}$ is complex-valued, use its real part for the comparison to focus on the deviation within the real axis that is expected for symmetric matrices.\n6. Report the ratio\n   $$r = \\frac{\\mathrm{rel\\_err}(\\lambda_{\\mathrm{ref}}, \\lambda_{P})}{\\max\\left(\\mathrm{rel\\_err}(\\lambda_{\\mathrm{ref}}, \\lambda_{Q}), \\varepsilon\\right)}.$$\n   A ratio $r$ significantly larger than $1$ indicates that the ill-conditioned non-orthogonal similarity transformation yields less accurate eigenvalues than the orthogonal similarity transformation.\n\nTest suite to be implemented:\n- Case $1$ (general “happy path”): $A$ is a random symmetric matrix of size $6 \\times 6$, generated as $\\frac{M + M^{\\mathsf{T}}}{2}$ where $M$ has independent standard normal entries with seed $0$. $Q$ is orthogonal, obtained from the $Q R$ factorization of a random $6 \\times 6$ Gaussian matrix with seed $0$. $P$ is diagonal with entries geometrically spaced between $10^{-4}$ and $10^{4}$, producing a condition number of $10^{8}$.\n- Case $2$ (boundary condition): $A$ is the $10 \\times 10$ tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals (the standard discrete one-dimensional Laplacian with Dirichlet-like structure). $Q$ is the identity $I$. $P$ is the identity $I$. This tests the baseline where both transformations are benign.\n- Case $3$ (ill-conditioned transformation, well-separated spectrum): $A$ is $8 \\times 8$ with diagonal entries $\\{1,2,3,4,5,6,7,8\\}$ plus a small symmetric perturbation $10^{-6} \\cdot S$ where $S$ is a random symmetric matrix from seed $1$. $Q$ is orthogonal from the $Q R$ factorization of a random $8 \\times 8$ Gaussian matrix with seed $2$. $P$ is diagonal with entries geometrically spaced between $10^{-6}$ and $10^{6}$, yielding a condition number of $10^{12}$.\n- Case $4$ (challenging $A$, ill-conditioned $P$): $A$ is the $5 \\times 5$ Hilbert matrix with entries $A_{i j} = \\frac{1}{i + j - 1}$, which is symmetric positive definite but numerically challenging. $Q$ is orthogonal from a random $5 \\times 5$ Gaussian matrix with seed $3$. $P$ is diagonal with entries geometrically spaced between $10^{-3}$ and $10^{3}$, yielding a condition number of $10^{6}$.\n\nComputation details:\n- For the reference eigenvalues $\\lambda_{\\mathrm{ref}}$ and for $\\lambda_{Q}$, use a symmetric eigensolver. For $\\lambda_{P}$, use a general eigensolver appropriate for possibly non-symmetric matrices.\n- Sort each eigenvalue array by ascending real parts before computing the relative error.\n\nFinal output specification:\n- Your program should produce a single line of output containing the four ratios $r$ for the four test cases as a comma-separated list enclosed in square brackets (e.g., $\"[r_1,r_2,r_3,r_4]\"$). Each $r$ is a floating-point number.\n\nNo physical units, angle units, or percentages are involved in this task. All numbers reported are unitless.",
            "solution": "The problem requires the design and implementation of a numerical experiment to demonstrate the effect of a similarity transformation's conditioning on the accuracy of computed eigenvalues. The core principle is that while a similarity transformation $B = S^{-1} A S$ preserves the eigenvalues of a matrix $A$ in exact arithmetic, this invariance can be compromised in finite-precision floating-point arithmetic. The numerical stability of the computed eigenvalues of $B$ is highly dependent on the condition number of the transformation matrix $S$, denoted $\\kappa(S) = \\lVert S \\rVert \\lVert S^{-1} \\rVert$.\n\nThe experiment contrasts two classes of similarity transformations on a real symmetric matrix $A$:\n$1$. An orthogonal transformation, $A_{Q} = Q^{\\mathsf{T}} A Q$, where $Q$ is an orthogonal matrix ($Q^{\\mathsf{T}} Q = I$). Orthogonal transformations are perfectly conditioned with respect to the $2$-norm ($\\kappa_2(Q) = 1$) and are known to be backward stable. Eigenvalue algorithms based on orthogonal transformations are among the most reliable in numerical linear algebra.\n$2$. A non-orthogonal transformation, $A_{P} = P^{-1} A P$, where $P$ is a deliberately constructed ill-conditioned matrix (i.e., $\\kappa(P) \\gg 1$). The formation of $A_P$ involves matrix inversion and two matrix multiplications, each of which introduces roundoff errors. These errors can be significantly amplified by the large condition number of $P$, leading to a matrix $A_P$ whose computed eigenvalues may substantially deviate from the true eigenvalues of $A$.\n\nThe program will systematically perform the following steps for each of the four specified test cases:\n\nFirst, it constructs the required matrices. Each test case defines a symmetric matrix $A$ of size $n \\times n$, an orthogonal matrix $Q$ of size $n \\times n$, and an ill-conditioned, non-orthogonal but invertible matrix $P$ of size $n \\times n$. The construction utilizes deterministic random seeds to ensure reproducibility. For instance, in Case $1$, $A$ is a $6 \\times 6$ random symmetric matrix, $Q$ is a random $6 \\times 6$ orthogonal matrix from a QR decomposition, and $P$ is a $6 \\times 6$ diagonal matrix with entries geometrically spaced from $10^{-4}$ to $10^{4}$, yielding a condition number $\\kappa(P) = 10^8$.\n\nSecond, it applies the similarity transformations in floating-point arithmetic to form $A_{Q} = Q^{\\mathsf{T}} A Q$ and $A_{P} = P^{-1} A P$.\n\nThird, it computes the eigenvalue spectra. A reference spectrum, $\\lambda_{\\mathrm{ref}}$, is computed from the original matrix $A$ using a specialized solver for symmetric matrices (`numpy.linalg.eigh`), which is highly accurate. The spectrum $\\lambda_{Q}$ is computed from the transformed matrix $A_Q$, also using the symmetric solver, as $A_Q$ is symmetric by construction. The spectrum $\\lambda_{P}$ is computed from the matrix $A_P$ using a general-purpose eigensolver (`numpy.linalg.eig`). This choice is critical because, even if $A$ and $P$ are symmetric, the product $P^{-1} A P$ is not generally symmetric, and numerical errors might introduce a non-symmetric part anyway. The general solver correctly handles this possibility.\n\nFourth, to ensure a valid comparison, all computed eigenvalue arrays ($\\lambda_{\\mathrm{ref}}$, $\\lambda_{Q}$, and the real parts of $\\lambda_{P}$) are sorted in ascending order. Note that $\\lambda_{P}$ may have spurious, non-zero imaginary parts due to numerical instability, so the comparison focuses on the deviation of the real parts.\n\nFifth, the program quantifies the accuracy of $\\lambda_Q$ and $\\lambda_P$ relative to the reference spectrum $\\lambda_{\\mathrm{ref}}$. This is done using the relative $2$-norm error function:\n$$\n\\mathrm{rel\\_err}(\\lambda_{\\mathrm{ref}}, \\lambda_{\\mathrm{comp}}) = \\frac{\\lVert \\mathrm{Re}(\\lambda_{\\mathrm{comp}}) - \\lambda_{\\mathrm{ref}} \\rVert_{2}}{\\max\\left(\\lVert \\lambda_{\\mathrm{ref}} \\rVert_{2}, \\varepsilon\\right)}\n$$\nHere, $\\lambda_{\\mathrm{comp}}$ is the computed spectrum ($\\lambda_Q$ or $\\lambda_P$), $\\mathrm{Re}(\\cdot)$ denotes the real part, $\\lVert \\cdot \\rVert_2$ is the Euclidean norm, and $\\varepsilon$ is the machine epsilon, which regularizes the denominator to prevent division by zero.\n\nFinally, the program computes the ratio $r$ for each test case:\n$$\nr = \\frac{\\mathrm{rel\\_err}(\\lambda_{\\mathrm{ref}}, \\lambda_{P})}{\\max\\left(\\mathrm{rel\\_err}(\\lambda_{\\mathrm{ref}}, \\lambda_{Q}), \\varepsilon\\right)}\n$$\nThis ratio directly compares the error from the ill-conditioned transformation to the error from the stable, orthogonal transformation. A value of $r \\gg 1$ provides quantitative evidence that the ill-conditioned similarity transformation leads to a significant loss of accuracy in the computed eigenvalues. The four test cases are designed to probe this phenomenon under different conditions: a general case (Case $1$), a benign baseline where both transformations are identities (Case $2$), a case with an ill-conditioned transformation applied to a matrix with well-separated eigenvalues (Case $3$), and a case involving a numerically challenging (ill-conditioned) matrix $A$ itself (Case $4$). The expected result for Case $2$ is $r \\approx 1$, serving as a control, while the other cases are expected to yield large values of $r$. The final output is a list of these four computed ratios.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a numerical experiment to compare eigenvalue accuracy under\n    orthogonal vs. ill-conditioned similarity transformations.\n    \"\"\"\n    \n    eps = np.finfo(float).eps\n\n    def calc_rel_err(lambda_ref, lambda_comp):\n        \"\"\"\n        Calculates the relative 2-norm error of computed eigenvalues.\n        \"\"\"\n        # Per problem spec, use real part of computed eigenvalues for comparison.\n        lambda_comp_real = np.real(lambda_comp)\n        \n        # Eigenvalues must be sorted for a meaningful element-wise comparison.\n        # np.linalg.eigh already returns sorted real eigenvalues.\n        # np.linalg.eig returns unsorted complex eigenvalues.\n        lambda_ref_sorted = np.sort(lambda_ref)\n        lambda_comp_sorted = np.sort(lambda_comp_real)\n        \n        norm_diff = np.linalg.norm(lambda_comp_sorted - lambda_ref_sorted)\n        norm_ref = np.linalg.norm(lambda_ref_sorted)\n        \n        return norm_diff / max(norm_ref, eps)\n\n    # Define test cases using a list of dictionaries for clarity.\n    test_case_params = [\n        {'id': 1},\n        {'id': 2},\n        {'id': 3},\n        {'id': 4},\n    ]\n\n    results = []\n\n    for params in test_case_params:\n        case_id = params['id']\n\n        # Construct matrices A, Q, P for each case\n        if case_id == 1:\n            # Case 1: General \"happy path\"\n            n = 6\n            seed_A = 0\n            seed_Q = 0\n            \n            rng_A = np.random.default_rng(seed_A)\n            M = rng_A.standard_normal((n, n))\n            A = (M + M.T) / 2\n            \n            rng_Q = np.random.default_rng(seed_Q)\n            M_Q = rng_Q.standard_normal((n, n))\n            Q, _ = np.linalg.qr(M_Q)\n            \n            P = np.diag(np.geomspace(1e-4, 1e4, n))\n\n        elif case_id == 2:\n            # Case 2: Boundary/baseline condition\n            n = 10\n            \n            A = np.diag(2 * np.ones(n)) - np.diag(np.ones(n-1), k=1) - np.diag(np.ones(n-1), k=-1)\n            Q = np.eye(n)\n            P = np.eye(n)\n\n        elif case_id == 3:\n            # Case 3: Ill-conditioned transformation, well-separated spectrum\n            n = 8\n            seed_S = 1\n            seed_Q = 2\n\n            A_diag = np.arange(1, n + 1, dtype=float)\n            rng_S = np.random.default_rng(seed_S)\n            M_S = rng_S.standard_normal((n, n))\n            S = (M_S + M_S.T) / 2\n            A = np.diag(A_diag) + 1e-6 * S\n\n            rng_Q = np.random.default_rng(seed_Q)\n            M_Q = rng_Q.standard_normal((n, n))\n            Q, _ = np.linalg.qr(M_Q)\n\n            P = np.diag(np.geomspace(1e-6, 1e6, n))\n\n        elif case_id == 4:\n            # Case 4: Challenging A (Hilbert matrix), ill-conditioned P\n            n = 5\n            seed_Q = 3\n\n            # Construct Hilbert matrix using 1-based indexing as in formula\n            i = np.arange(1, n + 1).reshape(n, 1)\n            j = np.arange(1, n + 1).reshape(1, n)\n            A = 1.0 / (i + j - 1)\n\n            rng_Q = np.random.default_rng(seed_Q)\n            M_Q = rng_Q.standard_normal((n, n))\n            Q, _ = np.linalg.qr(M_Q)\n\n            P = np.diag(np.geomspace(1e-3, 1e3, n))\n\n        # Perform the transformations\n        A_Q = Q.T @ A @ Q\n        P_inv = np.linalg.inv(P)\n        A_P = P_inv @ A @ P\n    \n        # Compute eigenvalues\n        # Use symmetric solver for A and A_Q\n        lambda_ref = np.linalg.eigh(A)[0]\n        lambda_Q = np.linalg.eigh(A_Q)[0]\n        # Use general solver for A_P, which is not guaranteed to be symmetric\n        lambda_P = np.linalg.eig(A_P)[0]\n\n        # Calculate relative errors\n        err_Q = calc_rel_err(lambda_ref, lambda_Q)\n        err_P = calc_rel_err(lambda_ref, lambda_P)\n        \n        # Calculate the final ratio\n        r = err_P / max(err_Q, eps)\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}