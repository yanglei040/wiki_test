{
    "hands_on_practices": [
        {
            "introduction": "The Inverse Power Method is an iterative algorithm, and at its heart lies a single, repeated calculation. This first exercise breaks down the method to its most fundamental step: solving the shifted linear system $(A - \\sigma I)y = x$. Understanding how to perform this core operation is the first step toward mastering the entire iterative process. ",
            "id": "1395843",
            "problem": "In a numerical algorithm, a sequence of vectors is generated starting from an initial vector $x_0$. The first unnormalized vector in this sequence, denoted as $y_1$, is found by solving the linear system $(A-\\sigma I)y_1 = x_0$, where $A$ is a square matrix, $\\sigma$ is a scalar shift, and $I$ is the identity matrix of the same dimension as $A$.\n\nGiven the matrix $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$, the shift $\\sigma = 1.5$, and the initial vector $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, determine the components of the vector $y_1$. Present your answer as a row matrix where each component is an exact fraction or decimal.",
            "solution": "We are asked to solve the linear system $(A-\\sigma I) y_{1} = x_{0}$ for $y_{1}$, where $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$, $\\sigma = \\frac{3}{2}$, and $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nFirst compute the shifted matrix:\n$$\nA - \\sigma I = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} - \\frac{3}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -1 \\\\ -1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nDenote $M = A - \\sigma I$. Then $y_{1} = M^{-1} x_{0}$. For a $2 \\times 2$ matrix $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, we use $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. Here $a = d = \\frac{3}{2}$ and $b = c = -1$, so\n$$\n\\det(M) = \\left(\\frac{3}{2}\\right)\\left(\\frac{3}{2}\\right) - (-1)(-1) = \\frac{9}{4} - 1 = \\frac{5}{4},\n$$\nand\n$$\n\\operatorname{adj}(M) = \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nTherefore,\n$$\nM^{-1} = \\frac{1}{\\frac{5}{4}} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\nMultiplying by $x_{0}$,\n$$\ny_{1} = M^{-1} x_{0} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ \\frac{4}{5} \\end{pmatrix}.\n$$\nThus the components of $y_{1}$ are $\\frac{6}{5}$ and $\\frac{4}{5}$, which we present as a row matrix.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{6}{5} & \\frac{4}{5} \\end{pmatrix}}$$"
        },
        {
            "introduction": "After mastering the core calculation, the next question is: what does the inverse power method actually find? This practice explores the convergence properties of the standard (unshifted) method, which is equivalent to applying the power method to the inverse matrix $A^{-1}$. By analyzing the eigenvalues of a matrix, you will predict which eigenpair the algorithm will converge to, reinforcing the connection between the method's mechanics and its objective. ",
            "id": "2216079",
            "problem": "Consider a real square matrix $A$. The standard inverse power method is an iterative algorithm used to find a specific eigenvalue-eigenvector pair of $A$. The method starts with a non-zero vector $x_0$ and iteratively applies the relation $x_{k+1} = \\frac{A^{-1}x_k}{\\|A^{-1}x_k\\|}$ for $k=0, 1, 2, \\dots$. For a generic choice of the initial vector $x_0$, this process converges to the eigenvector corresponding to one of the eigenvalues of $A$.\n\nA particular non-singular 4x4 real matrix $A$ is known to have the following set of eigenvalues: $\\{3, -1, 0.5+2i, 0.5-2i\\}$. If the standard inverse power method is applied to this matrix, which eigenvalue will the iteration converge to, assuming a generic initial vector is chosen?\n\nA. 3\n\nB. -1\n\nC. 0.5 + 2i\n\nD. The method will not converge because there are complex eigenvalues.\n\nE. Convergence is not unique because there are two distinct eigenvalues with the smallest magnitude.",
            "solution": "The inverse power method applies the iteration $x_{k+1}=\\dfrac{A^{-1}x_{k}}{\\|A^{-1}x_{k}\\|}$, which is equivalent to the standard power method applied to $A^{-1}$. The power method converges to an eigenvector corresponding to the eigenvalue of largest magnitude of the matrix it is applied to, provided a generic initial vector is used and there is a unique dominant magnitude.\n\nLet the eigenvalues of $A$ be $\\{\\lambda_{1},\\lambda_{2},\\lambda_{3},\\lambda_{4}\\}=\\{3,-1,0.5+2i,0.5-2i\\}$. Then the eigenvalues of $A^{-1}$ are $\\{1/\\lambda_{1},1/\\lambda_{2},1/\\lambda_{3},1/\\lambda_{4}\\}$. The magnitudes are\n$$\n\\left|\\frac{1}{\\lambda}\\right|=\\frac{1}{|\\lambda|}.\n$$\nHence the eigenvalue of $A^{-1}$ with largest magnitude corresponds to the eigenvalue of $A$ with smallest magnitude.\n\nCompute the magnitudes of the eigenvalues of $A$:\n- For $\\lambda=3$, $|\\lambda|=3$.\n- For $\\lambda=-1$, $|\\lambda|=1$.\n- For $\\lambda=0.5\\pm 2i$, $|\\lambda|=\\sqrt{(0.5)^{2}+2^{2}}=\\sqrt{\\frac{1}{4}+4}=\\sqrt{\\frac{17}{4}}=\\frac{\\sqrt{17}}{2}$.\n\nSince $1<\\frac{\\sqrt{17}}{2}<3$, the unique smallest magnitude is $|\\lambda|=1$, attained by $\\lambda=-1$. Therefore, for a generic initial vector, the inverse power method converges to the eigenvector associated with $\\lambda=-1$. The presence of complex eigenvalues does not prevent convergence in this case, and there is no ambiguity since the smallest magnitude eigenvalue is unique.\n\nThus the correct option is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "True mastery comes from building a tool from the ground up. This advanced practice challenges you to implement the Rayleigh Quotient Iteration, a sophisticated and highly efficient version of the inverse power method that converges cubically for symmetric matrices. By using a dynamic shift based on the Rayleigh quotient, you will write a complete program that demonstrates the power and practicality of this algorithm in a computational context, including handling potential numerical issues like singularity. ",
            "id": "3243386",
            "problem": "Implement a complete, runnable program that computes an eigenvalue of a real square matrix using the inverse power method with a dynamic shift given by the Rayleigh quotient of the current iterate. The goal is to approximate an eigenvalue of a real matrix by iteratively solving a shifted linear system and updating the shift from first principles.\n\nYou must design your algorithm starting from the following foundational base:\n- The definition of an eigenpair: a nonzero vector $\\boldsymbol{v} \\in \\mathbb{R}^n$ and a scalar $\\lambda \\in \\mathbb{R}$ such that $A \\boldsymbol{v} = \\lambda \\boldsymbol{v}$.\n- The Rayleigh quotient of a nonzero vector $\\boldsymbol{x} \\in \\mathbb{R}^n$ for a real matrix $A \\in \\mathbb{R}^{n \\times n}$, defined as the scalar that equals the unique scalar multiple minimizing the residual in the least-squares sense when approximating $A \\boldsymbol{x}$ by a multiple of $\\boldsymbol{x}$. This quotient equals the eigenvalue when $\\boldsymbol{x}$ is an eigenvector.\n- The inverse iteration idea: if $\\sigma$ is close to an eigenvalue $\\lambda$, then the linear system $(A - \\sigma I)\\boldsymbol{y} = \\boldsymbol{x}$ is nearly singular and its solution tends to amplify the component of $\\boldsymbol{x}$ in the direction of the corresponding eigenvector. Normalizing $\\boldsymbol{y}$ then yields an improved iterate.\n\nYou must not assume any special matrix structure beyond real entries. Your implementation must:\n- Use only real arithmetic.\n- Normalize the iterate at every step using the Euclidean norm.\n- Use the current iterateâ€™s Rayleigh quotient as the dynamic shift.\n- Solve the linear systems with Gaussian elimination with partial pivoting (standard dense direct solve). If a solve fails due to singularity or near singularity, apply a minimal diagonal regularization by solving with $(A - \\sigma I + \\epsilon I)$ where $\\epsilon = 10^{-14} \\max(1, \\lVert A \\rVert_2)$, and retry. If the initial iterate already satisfies the eigenpair equation to within tolerance, terminate immediately without attempting to solve a singular system.\n- Terminate when the residual norm $\\lVert A \\boldsymbol{x}_k - \\mu_k \\boldsymbol{x}_k \\rVert_2$ is less than or equal to a given tolerance, or when a maximum number of iterations is reached.\n\nInput is not required; your program must hard-code the following test suite and produce a single line of output as specified below.\n\nTest suite:\n- Case 1 (happy path, diagonal):\n  - $A_1 = \\mathrm{diag}(1, 2, 5)$.\n  - $\\boldsymbol{x}_0^{(1)} = [0.2, 0.2, 0.96]^T$.\n  - Tolerance $= 10^{-12}$, maximum iterations $= 100$.\n- Case 2 (boundary, immediate convergence due to exact eigenvector up to scaling):\n  - $$A_2 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$$.\n  - $\\boldsymbol{x}_0^{(2)} = [1, 1]^T$.\n  - Tolerance $= 10^{-12}$, maximum iterations $= 50$.\n- Case 3 (clustered spectrum):\n  - $$A_3 = \\begin{bmatrix} 1 & 10^{-3} & 0 \\\\ 10^{-3} & 1 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$$.\n  - $\\boldsymbol{x}_0^{(3)} = [1, -1, 0.1]^T$.\n  - Tolerance $= 10^{-12}$, maximum iterations $= 100$.\n- Case 4 (symmetric tridiagonal):\n  - $$A_4 = \\begin{bmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix}$$.\n  - $\\boldsymbol{x}_0^{(4)} = [0.5, 0.5, 0.5]^T$.\n  - Tolerance $= 10^{-12}$, maximum iterations $= 100$.\n\nFor each case, your program must compute:\n- The final approximate eigenvalue $\\mu$ given by the Rayleigh quotient of the last iterate.\n- The final residual norm $r = \\lVert A \\boldsymbol{x} - \\mu \\boldsymbol{x} \\rVert_2$.\n\nFinal output format:\n- Produce a single line containing a list of length $4$, where each element is a two-element list $[\\mu, r]$ corresponding to the cases in the order above.\n- Both $\\mu$ and $r$ must be rounded to exactly $10$ decimal places.\n- The line must contain no spaces. For example: \"[[mu1,res1],[mu2,res2],[mu3,res3],[mu4,res4]]\" where \"mu1\" and \"res1\" are decimal numbers rounded to $10$ decimal places.\n\nNo physical units or angle units are involved. Express all decimals as plain decimal numbers. The final output must be deterministic and must not require any user input or external data.",
            "solution": "The problem requires the implementation of the Rayleigh Quotient Iteration (RQI), a numerical method to find an eigenpair $(\\lambda, \\boldsymbol{v})$ of a real square matrix $A \\in \\mathbb{R}^{n \\times n}$. An eigenpair is defined by the fundamental relationship $A \\boldsymbol{v} = \\lambda \\boldsymbol{v}$, where $\\lambda$ is a scalar eigenvalue and $\\boldsymbol{v}$ is a non-zero eigenvector. The algorithm is to be developed from first principles as specified.\n\nThe core of the RQI method combines two key concepts: the Rayleigh quotient and inverse iteration.\n\n1.  **The Rayleigh Quotient**: For a given real matrix $A \\in \\mathbb{R}^{n \\times n}$ and a non-zero vector $\\boldsymbol{x} \\in \\mathbb{R}^n$, the Rayleigh quotient is defined as:\n    $$\n    \\mu(\\boldsymbol{x}) = \\frac{\\boldsymbol{x}^{T}A\\boldsymbol{x}}{\\boldsymbol{x}^{T}\\boldsymbol{x}}\n    $$\n    This scalar value provides the best approximation of an eigenvalue in a least-squares sense for the given vector $\\boldsymbol{x}$. Specifically, $\\mu(\\boldsymbol{x})$ is the scalar $\\alpha$ that minimizes the Euclidean norm of the residual, $\\lVert A\\boldsymbol{x} - \\alpha\\boldsymbol{x} \\rVert_2$. If $\\boldsymbol{x}$ is an eigenvector, its Rayleigh quotient $\\mu(\\boldsymbol{x})$ is exactly the corresponding eigenvalue $\\lambda$. In our algorithm, since the eigenvector iterate $\\boldsymbol{x}_k$ is always normalized to have a unit Euclidean norm ($\\lVert \\boldsymbol{x}_k \\rVert_2 = 1$), its denominator $\\boldsymbol{x}_k^T \\boldsymbol{x}_k = 1$, simplifying the calculation to $\\mu(\\boldsymbol{x}_k) = \\boldsymbol{x}_k^T A \\boldsymbol{x}_k$.\n\n2.  **Inverse Iteration with a Dynamic Shift**: The standard inverse iteration method computes iterates via $\\boldsymbol{x}_{k+1} \\propto (A - \\sigma I)^{-1}\\boldsymbol{x}_k$, where $\\sigma$ is a fixed shift. This method converges to the eigenvector corresponding to the eigenvalue of $A$ closest to $\\sigma$. RQI is an advanced variant where the shift $\\sigma$ is not fixed but is dynamically updated at each step, using the Rayleigh quotient of the current iterate as the shift for the next step. This choice of shift, $\\sigma_k = \\mu(\\boldsymbol{x}_k)$, typically leads to very rapid (cubic) convergence to an eigenpair.\n\nThe iterative process for RQI can be summarized as follows. Starting with an initial vector $\\boldsymbol{x}_0$, for each iteration $k = 0, 1, 2, \\dots$:\na. The current eigenvector approximation is $\\boldsymbol{x}_k$.\nb. An improved eigenvalue approximation (the shift) is computed: $\\mu_k = \\mu(\\boldsymbol{x}_k) = \\boldsymbol{x}_k^T A \\boldsymbol{x}_k$.\nc. A linear system is solved to find a vector enriched in the desired eigenvector direction: $(A - \\mu_k I) \\boldsymbol{y}_{k+1} = \\boldsymbol{x}_k$.\nd. The new eigenvector approximation is obtained by normalizing the solution: $\\boldsymbol{x}_{k+1} = \\frac{\\boldsymbol{y}_{k+1}}{\\lVert \\boldsymbol{y}_{k+1} \\rVert_2}$.\n\nThe complete algorithm, including initialization, termination, and special handling as per the problem requirements, is as follows:\n\n**Algorithm: Rayleigh Quotient Iteration**\n\n1.  **Initialization**:\n    - Given a matrix $A \\in \\mathbb{R}^{n \\times n}$, an initial vector $\\boldsymbol{x}_0$, a tolerance `tol`, and a maximum number of iterations `max_iter`.\n    - Normalize the initial vector: $\\boldsymbol{x} \\leftarrow \\boldsymbol{x}_0 / \\lVert \\boldsymbol{x}_0 \\rVert_2$.\n    - Pre-compute the matrix $2$-norm $\\lVert A \\rVert_2$ for potential regularization. The regularization parameter is $\\epsilon = 10^{-14} \\max(1, \\lVert A \\rVert_2)$. Let $I$ be the $n \\times n$ identity matrix.\n\n2.  **Iteration Loop**: For $k$ from $0$ to `max_iter - 1`:\n    a. **Compute Eigenvalue Estimate and Check for Convergence**: Calculate the Rayleigh quotient $\\mu = \\boldsymbol{x}^T A \\boldsymbol{x}$. Check the termination condition by computing the norm of the residual: $r = \\lVert A\\boldsymbol{x} - \\mu\\boldsymbol{x} \\rVert_2$. If $r \\le \\text{tol}$, the algorithm has converged. Terminate and return the final pair $(\\mu, r)$. This step also handles the case of immediate convergence if the initial vector $\\boldsymbol{x}_0$ is already a sufficiently accurate eigenvector.\n\n    b. **Solve the Shifted Linear System**: Form the shifted matrix $M = A - \\mu I$. Solve the linear system $M \\boldsymbol{y} = \\boldsymbol{x}$ for $\\boldsymbol{y}$. This is performed using a standard direct solver (Gaussian elimination with partial pivoting).\n\n    c. **Handle Singularity**: As $\\mu$ approaches an eigenvalue, the matrix $M$ becomes nearly singular. If the solver fails (e.g., raises a singularity error), the system is regularized. A new system $(M + \\epsilon I) \\boldsymbol{y} = \\boldsymbol{x}$ is solved instead. This small diagonal shift makes the matrix invertible while minimally perturbing the solution.\n\n    d. **Normalize the Next Iterate**: Update the eigenvector approximation by normalizing the solution vector: $\\boldsymbol{x} \\leftarrow \\boldsymbol{y} / \\lVert \\boldsymbol{y} \\rVert_2$.\n\n3.  **Termination**: If the loop completes without meeting the tolerance criterion (i.e., `max_iter` is reached), the algorithm terminates. The final eigenvalue estimate $\\mu$ and residual norm $r$ are calculated based on the last computed vector iterate $\\boldsymbol{x}$ and returned.\n\nThis procedure is deterministic and, for the given test cases, is expected to converge to an eigenvalue closest to the Rayleigh quotient of the initial vector.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rayleigh_quotient_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Computes an eigenvalue of a matrix using the inverse power method\n    with Rayleigh quotient dynamic shift.\n\n    Args:\n        A (np.ndarray): The real square matrix.\n        x0 (np.ndarray): The initial non-zero vector.\n        tol (float): The convergence tolerance for the residual norm.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the final approximate eigenvalue (mu) and\n               the final residual norm.\n    \"\"\"\n    if np.linalg.norm(x0) == 0:\n        raise ValueError(\"Initial vector x0 cannot be the zero vector.\")\n\n    x = x0 / np.linalg.norm(x0)\n    \n    n = A.shape[0]\n    I = np.eye(n)\n    \n    # Pre-compute matrix 2-norm for regularization\n    A_norm = np.linalg.norm(A, 2)\n    epsilon = 1e-14 * max(1.0, A_norm)\n\n    mu = 0.0\n    res_norm = float('inf')\n\n    for _ in range(max_iter):\n        # 1. Calculate Rayleigh quotient and check for convergence\n        # Since x is a unit vector, x.T @ x = 1.\n        mu = x.T @ A @ x\n        \n        residual_vec = A @ x - mu * x\n        res_norm = np.linalg.norm(residual_vec)\n        \n        if res_norm <= tol:\n            return mu, res_norm\n\n        # 2. Setup and solve the shifted linear system\n        M = A - mu * I\n        try:\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # 3. Handle singularity with regularization\n            M_reg = M + epsilon * I\n            y = np.linalg.solve(M_reg, x)\n        \n        # 4. Normalize to get the next iterate\n        norm_y = np.linalg.norm(y)\n        if norm_y == 0:\n            # In the unlikely event of a zero vector solution\n            return mu, res_norm\n        x = y / norm_y\n        \n    # After max_iter, calculate final values from the last iterate\n    mu = x.T @ A @ x\n    res_norm = np.linalg.norm(A @ x - mu * x)\n    return mu, res_norm\n\ndef solve():\n    \"\"\"\n    Runs the defined test suite for the Rayleigh Quotient Iteration algorithm\n    and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.diag([1.0, 2.0, 5.0]),\n            \"x0\": np.array([0.2, 0.2, 0.96]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        },\n        {\n            \"A\": np.array([[0.0, 1.0], [1.0, 0.0]]),\n            \"x0\": np.array([1.0, 1.0]),\n            \"tol\": 1e-12,\n            \"max_iter\": 50\n        },\n        {\n            \"A\": np.array([[1.0, 1e-3, 0.0], [1e-3, 1.0, 0.0], [0.0, 0.0, 2.0]]),\n            \"x0\": np.array([1.0, -1.0, 0.1]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        },\n        {\n            \"A\": np.array([[2.0, -1.0, 0.0], [-1.0, 2.0, -1.0], [0.0, -1.0, 2.0]]),\n            \"x0\": np.array([0.5, 0.5, 0.5]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, r = rayleigh_quotient_iteration(case[\"A\"], case[\"x0\"], case[\"tol\"], case[\"max_iter\"])\n        \n        # Round to 10 decimal places as required\n        mu_rounded = round(mu, 10)\n        r_rounded = round(r, 10)\n        \n        # Format as a string representation of a list\n        results.append(f\"[{str(mu_rounded)},{str(r_rounded)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}