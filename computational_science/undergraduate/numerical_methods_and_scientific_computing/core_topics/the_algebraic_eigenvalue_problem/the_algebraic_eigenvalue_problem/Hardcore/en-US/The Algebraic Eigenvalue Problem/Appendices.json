{
    "hands_on_practices": [
        {
            "introduction": "While many matrices encountered in introductory linear algebra are diagonalizable, real-world applications often produce matrices that are not. This exercise  challenges you to explore such a \"defective\" matrix, which lacks a full basis of eigenvectors. By calculating the eigenvalues and comparing their algebraic and geometric multiplicities, you will uncover the theoretical reason for this deficiency and construct the matrix's ultimate canonical form: the Jordan Normal Form.",
            "id": "3282394",
            "problem": "In numerical linear algebra, matrices arising from discretizations or model reductions may fail to possess a complete basis of eigenvectors. Such matrices are called defective and are characterized by an algebraic eigenvalue multiplicity that exceeds the geometric multiplicity. Consider the following deliberately constructed matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1  0 \\\\\n0  2  1 \\\\\n1  -1  3\n\\end{pmatrix}.\n$$\nStarting from the foundational definitions of the characteristic polynomial, eigenvalues, eigenvectors, algebraic multiplicity, geometric multiplicity, and the Rank-Nullity Theorem, do the following:\n\n- Derive the characteristic polynomial of $A$ and identify its eigenvalues and their algebraic multiplicities.\n- Determine the eigenspace of $A$ corresponding to each eigenvalue and compute its geometric multiplicity. Conclude, based on the definition, whether $A$ is defective.\n- Using the definition of generalized eigenvectors and Jordan chains, construct a chain for the eigenvalue you found and use it to deduce the Jordan Normal Form (JNF) of $A$. State clearly the Jordan block sizes.\n\nExpress your final answer as the Jordan Normal Form matrix. No rounding is required, and no units are involved. For clarity, define Jordan Normal Form (JNF) on first use as the canonical form to which a matrix is similar, comprising Jordan blocks associated with each eigenvalue along the diagonal, and ones on superdiagonals within each block.",
            "solution": "The problem requires a complete analysis of the matrix $A = \\begin{pmatrix} 1  1  0 \\\\ 0  2  1 \\\\ 1  -1  3 \\end{pmatrix}$ to determine its eigenvalues, multiplicities, eigenspaces, and ultimately its Jordan Normal Form (JNF). The JNF is defined as the canonical form to which a matrix is similar, comprising Jordan blocks on the diagonal.\n\nFirst, we derive the characteristic polynomial of $A$. The characteristic polynomial, $p(\\lambda)$, is defined as the determinant of the matrix $A - \\lambda I$, where $I$ is the $3 \\times 3$ identity matrix and $\\lambda$ is a scalar variable.\n$$ A - \\lambda I = \\begin{pmatrix} 1 - \\lambda  1  0 \\\\ 0  2 - \\lambda  1 \\\\ 1  -1  3 - \\lambda \\end{pmatrix} $$\nThe determinant is computed as:\n$$ p(\\lambda) = \\det(A - \\lambda I) = (1 - \\lambda) \\begin{vmatrix} 2 - \\lambda  1 \\\\ -1  3 - \\lambda \\end{vmatrix} - (1) \\begin{vmatrix} 0  1 \\\\ 1  3 - \\lambda \\end{vmatrix} + 0 $$\n$$ p(\\lambda) = (1 - \\lambda)((2 - \\lambda)(3 - \\lambda) - (1)(-1)) - (0 - 1) $$\n$$ p(\\lambda) = (1 - \\lambda)(\\lambda^2 - 5\\lambda + 6 + 1) + 1 $$\n$$ p(\\lambda) = (1 - \\lambda)(\\lambda^2 - 5\\lambda + 7) + 1 $$\n$$ p(\\lambda) = (\\lambda^2 - 5\\lambda + 7) - (\\lambda^3 - 5\\lambda^2 + 7\\lambda) + 1 $$\n$$ p(\\lambda) = -\\lambda^3 + 6\\lambda^2 - 12\\lambda + 8 $$\nThis polynomial can be factored. Recognizing the coefficients as related to a cubic expansion, we can rewrite it as:\n$$ p(\\lambda) = -(\\lambda^3 - 6\\lambda^2 + 12\\lambda - 8) = -(\\lambda - 2)^3 $$\nThe eigenvalues of $A$ are the roots of the characteristic equation $p(\\lambda) = 0$.\n$$ -(\\lambda - 2)^3 = 0 \\implies \\lambda = 2 $$\nThere is a single distinct eigenvalue, $\\lambda = 2$. The algebraic multiplicity (AM) of an eigenvalue is its multiplicity as a root of the characteristic polynomial. For $\\lambda=2$, the algebraic multiplicity is $AM(2) = 3$.\n\nNext, we determine the eigenspace corresponding to $\\lambda = 2$. The eigenspace $E_\\lambda$ is the null space (or kernel) of the matrix $A - \\lambda I$. The vectors in this space are the eigenvectors. We need to find the non-zero vectors $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$ that solve the system $(A - 2I)\\mathbf{v} = 0$.\n$$ A - 2I = \\begin{pmatrix} 1-2  1  0 \\\\ 0  2-2  1 \\\\ 1  -1  3-2 \\end{pmatrix} = \\begin{pmatrix} -1  1  0 \\\\ 0  0  1 \\\\ 1  -1  1 \\end{pmatrix} $$\nThe system of linear equations is:\n$$ \\begin{pmatrix} -1  1  0 \\\\ 0  0  1 \\\\ 1  -1  1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThis yields the equations:\n1. $-x + y = 0 \\implies x = y$\n2. $z = 0$\n3. $x - y + z = 0$\nSubstituting $x=y$ and $z=0$ into the third equation gives $y - y + 0 = 0$, which is trivially true. Thus, the solutions are of the form $\\mathbf{v} = \\begin{pmatrix} x \\\\ x \\\\ 0 \\end{pmatrix} = x \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ for any scalar $x$.\nThe eigenspace $E_2$ is spanned by the single eigenvector $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n$$ E_2 = \\text{span}\\left\\{\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\\right\\} $$\nThe geometric multiplicity (GM) of an eigenvalue is the dimension of its eigenspace. Here, $\\dim(E_2) = 1$, so the geometric multiplicity is $GM(2) = 1$.\nAlternatively, according to the Rank-Nullity Theorem, $GM(\\lambda) = n - \\text{rank}(A - \\lambda I)$, where $n=3$. The rank of $A - 2I$ is $2$, as its row-reduced echelon form has two pivots. Thus, $GM(2) = 3 - 2 = 1$.\nA matrix is defined as defective if, for any eigenvalue, its algebraic multiplicity is strictly greater than its geometric multiplicity. For $\\lambda=2$, we have $AM(2) = 3$ and $GM(2) = 1$. Since $3 > 1$, the matrix $A$ is defective.\n\nFinally, we construct the Jordan Normal Form of $A$. The number of Jordan blocks for a given eigenvalue $\\lambda$ is equal to its geometric multiplicity, $GM(\\lambda)$. The sum of the sizes of these blocks is equal to its algebraic multiplicity, $AM(\\lambda)$.\nFor $\\lambda = 2$, we have $GM(2) = 1$, so there is exactly one Jordan block. The size of this block must be equal to $AM(2) = 3$. A Jordan block of size $3$ for eigenvalue $\\lambda=2$ is given by:\n$$ J_3(2) = \\begin{pmatrix} 2  1  0 \\\\ 0  2  1 \\\\ 0  0  2 \\end{pmatrix} $$\nThis matrix is the Jordan Normal Form of $A$.\n\nTo confirm this, we can construct the Jordan chain of generalized eigenvectors. A chain of length $3$ is a set of vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3\\}$ satisfying:\n$$ (A - 2I)\\mathbf{v}_1 = 0 $$\n$$ (A - 2I)\\mathbf{v}_2 = \\mathbf{v}_1 $$\n$$ (A - 2I)\\mathbf{v}_3 = \\mathbf{v}_2 $$\nThis is equivalent to finding a vector $\\mathbf{v}_3$ such that $\\mathbf{v}_3 \\in \\ker((A-2I)^3)$ but $\\mathbf{v}_3 \\notin \\ker((A-2I)^2)$. The minimal polynomial of $A$ is $(\\lambda-2)^3$, which implies that $(A-2I)^2 \\neq 0$ and $(A-2I)^3 = 0$. Let's compute $(A-2I)^2$:\n$$ (A - 2I)^2 = \\begin{pmatrix} -1  1  0 \\\\ 0  0  1 \\\\ 1  -1  1 \\end{pmatrix} \\begin{pmatrix} -1  1  0 \\\\ 0  0  1 \\\\ 1  -1  1 \\end{pmatrix} = \\begin{pmatrix} 1  -1  1 \\\\ 1  -1  1 \\\\ 0  0  0 \\end{pmatrix} $$\nThe null space of $(A-2I)^2$ is defined by the equation $x - y + z = 0$. We need to choose $\\mathbf{v}_3$ that does not satisfy this equation. A simple choice is $\\mathbf{v}_3 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nNow we construct the chain:\n$$ \\mathbf{v}_2 = (A - 2I)\\mathbf{v}_3 = \\begin{pmatrix} -1  1  0 \\\\ 0  0  1 \\\\ 1  -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n$$ \\mathbf{v}_1 = (A - 2I)\\mathbf{v}_2 = \\begin{pmatrix} -1  1  0 \\\\ 0  0  1 \\\\ 1  -1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\nThis $\\mathbf{v}_1$ is the eigenvector we found earlier. The existence of this chain of length $3$ confirms that the JNF consists of a single $3 \\times 3$ Jordan block.\nThe Jordan Normal Form of $A$ is uniquely determined by these multiplicities.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 2  1  0 \\\\ 0  2  1 \\\\ 0  0  2 \\end{pmatrix} } $$"
        },
        {
            "introduction": "The power method is often the first iterative eigenvalue algorithm students learn, prized for its simplicity. However, its convergence relies on assumptions that are not always met in practice, especially for \"nearly-defective\" matrices where eigenvectors are almost linearly dependent. This hands-on coding problem  reveals how the method can struggle to converge to the correct dominant eigenvector, providing critical insight into the numerical stability and practical limitations of fundamental algorithms.",
            "id": "3282259",
            "problem": "You will investigate the behavior of the power method on a carefully constructed nearly-defective matrix in the algebraic eigenvalue problem. The power method attempts to approximate a dominant eigenpair by repeatedly applying a matrix to a vector and normalizing. The fundamental base for this investigation comprises the definitions of eigenvalues and eigenvectors, the notion of diagonalizability, and the definition of the power iteration. Specifically, if a matrix $A \\in \\mathbb{R}^{n \\times n}$ has an eigenvalue of largest magnitude (the spectral radius), and the initial vector has a nonzero component in the direction of the corresponding right eigenvector, then the power method tends to align with that eigenvector; otherwise, it fails to do so.\n\nConstruct a nearly-defective $2 \\times 2$ matrix as follows. Let $S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$, with $\\varepsilon = 10^{-8}$, and let $J = \\operatorname{diag}(\\lambda_1,\\lambda_2)$ with $\\lambda_1 = 1 + \\delta$ and $\\lambda_2 = 1$, where $\\delta = 10^{-4}$. Define\n$$\nA \\;=\\; S J S^{-1}.\n$$\nBy construction, $A$ is diagonalizable with right eigenvectors given by the columns of $S$, which are $\\mathbf{s}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ associated with $\\lambda_1$ and $\\mathbf{s}_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$ associated with $\\lambda_2$. Because $\\varepsilon$ is very small, the two eigenvectors are nearly parallel, and the eigenbasis is ill-conditioned, making $A$ nearly defective. The left eigenvectors (rows of $S^{-1}$) determine the decomposition coefficients of an initial vector when expressed in the right-eigenvector basis.\n\nImplement the power method iteration defined by\n$$\n\\mathbf{y}_{k+1} \\;=\\; A \\mathbf{x}_k,\\quad \\mathbf{x}_{k+1} \\;=\\; \\frac{\\mathbf{y}_{k+1}}{\\|\\mathbf{y}_{k+1}\\|_2},\n$$\nstarting from a given initial vector $\\mathbf{x}_0 \\in \\mathbb{R}^2 \\setminus \\{0\\}$. After a prescribed number of iterations $N$, report the Rayleigh quotient\n$$\n\\rho(\\mathbf{x}) \\;=\\; \\frac{\\mathbf{x}^\\top A \\mathbf{x}}{\\mathbf{x}^\\top \\mathbf{x}},\n$$\nas an estimate of the eigenvalue associated with the final direction $\\mathbf{x}$. Use this estimate to decide whether the iteration aligned numerically with the dominant eigenvalue $\\lambda_1$ or with the subdominant eigenvalue $\\lambda_2$ by comparing absolute differences $|\\rho(\\mathbf{x}) - \\lambda_1|$ and $|\\rho(\\mathbf{x}) - \\lambda_2|$.\n\nYour program must:\n- Construct $A$ from the specified $S$, $\\varepsilon = 10^{-8}$, and $J$ with $\\lambda_1 = 1 + 10^{-4}$ and $\\lambda_2 = 1$.\n- Implement the power method for exactly $N = 1000$ iterations (do not use an adaptive stopping criterion).\n- For each initial vector in the test suite below, compute the final iterate $\\mathbf{x}_N$ (normalized at each step) and its Rayleigh quotient $\\rho(\\mathbf{x}_N)$, then output a boolean that is true if the final Rayleigh quotient is closer to $\\lambda_1$ than to $\\lambda_2$, and false otherwise.\n\nTest suite of initial vectors:\n- Case $1$ (exact subdominant eigenvector): $\\mathbf{x}_0^{(1)} = \\mathbf{s}_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$.\n- Case $2$ (extremely close to subdominant eigenvector): $\\mathbf{x}_0^{(2)} = \\mathbf{s}_2 + 10^{-30} \\mathbf{s}_1$.\n- Case $3$ (exact dominant eigenvector): $\\mathbf{x}_0^{(3)} = \\mathbf{s}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n- Case $4$ (nearly orthogonal to the left dominant eigenvector): $\\mathbf{x}_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$.\n\nExpected qualitative behavior rooted in first principles:\n- In Case $1$, because $\\mathbf{x}_0$ is exactly an eigenvector for $\\lambda_2$, all iterates remain in the span of $\\mathbf{s}_2$, so the power method will produce a Rayleigh quotient equal (up to roundoff) to $\\lambda_2$, i.e., the wrong eigenvalue relative to the dominant.\n- In Case $2$, the coefficient of $\\mathbf{s}_1$ is astronomically small, and with $|\\lambda_1/\\lambda_2|$ barely greater than $1$, after $1000$ iterations the dominant component remains negligible; the method appears to converge to the wrong eigenvector.\n- In Case $3$, the method immediately yields the dominant eigenpair.\n- In Case $4$, the coefficient of $\\mathbf{s}_1$ is very small compared to that of $\\mathbf{s}_2$, so within $1000$ iterations the method will still be numerically aligned with the subdominant eigenvector.\n\nFinal output format:\n- Your program should produce a single line of output containing the four boolean results for the four cases above as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False]\".\n- No physical units or angles are involved; all numerical quantities are dimensionless real numbers.\n\nThe problem requires you to reason from the definitions of eigenvalues and eigenvectors, diagonalization, left and right eigenvectors, and the structure of the power method. You must avoid any shortcuts beyond these definitions. Ensure scientific realism by using the specified parameters and provide a robust implementation following the above instructions.",
            "solution": "The user has presented a problem from the field of numerical linear algebra, specifically concerning the behavior of the power method on a nearly-defective matrix. The problem requires implementing the power method and analyzing its convergence for a set of carefully chosen initial vectors.\n\n### Problem Validation\nThe problem statement has been validated and is deemed sound.\n- **Givens**: All parameters ($\\varepsilon = 10^{-8}$, $\\delta = 10^{-4}$), matrices ($S$, $J$, and the resulting $A$), algorithm specifications (power method with $N=1000$ iterations and $L_2$ normalization), initial conditions (four test vectors $\\mathbf{x}_0$), and output requirements (a boolean comparison of the final Rayleigh quotient against the known eigenvalues) are explicitly provided.\n- **Scientific Grounding**: The problem is based on fundamental and standard principles of the algebraic eigenvalue problem, including matrix diagonalization, left and right eigenvectors, and the power iteration method. The construction of a nearly-defective matrix to study numerical stability and convergence is a classic pedagogical tool in numerical analysis.\n- **Well-Posedness and Objectivity**: The problem is unambiguous, self-contained, and objective. It specifies a deterministic computational task that yields a unique, verifiable result for each case.\n\nThe problem is valid and a solution will be furnished.\n\n### Principle-Based Solution\nThe core task is to analyze the power method's performance for a matrix $A$ with an ill-conditioned eigenbasis. The power method is an iterative algorithm designed to find the dominant eigenvalue and a corresponding eigenvector of a matrix. Its behavior is dictated by the decomposition of the initial vector in the basis of the matrix's eigenvectors.\n\n**1. Matrix Construction and Properties**\n\nThe matrix $A$ is constructed as $A = S J S^{-1}$, where:\n$S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$ with $\\varepsilon = 10^{-8}$.\n$J = \\operatorname{diag}(\\lambda_1, \\lambda_2)$ with $\\lambda_1 = 1 + \\delta = 1 + 10^{-4}$ and $\\lambda_2 = 1$.\n\nThe columns of $S$ are the right eigenvectors of $A$:\n- $\\mathbf{s}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ corresponding to the dominant eigenvalue $\\lambda_1 = 1.0001$.\n- $\\mathbf{s}_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 10^{-8} \\end{bmatrix}$ corresponding to the subdominant eigenvalue $\\lambda_2 = 1$.\n\nSince $\\varepsilon$ is very small, the eigenvectors $\\mathbf{s}_1$ and $\\mathbf{s}_2$ are nearly parallel, making the eigenvector basis ill-conditioned. This means that $A$ is a nearly-defective matrix.\n\nThe inverse of $S$ is computed as $S^{-1} = \\frac{1}{\\det(S)} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\frac{1}{\\varepsilon} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}$. The rows of $S^{-1}$ are the left eigenvectors of $A$.\n\nThe matrix $A$ can be computed explicitly:\n$$\nA = S J S^{-1} = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}\n$$\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon + 1/\\varepsilon \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -\\delta/\\varepsilon \\\\ 0  1 \\end{bmatrix}\n$$\nSo, $A = \\begin{bmatrix} 1.0001  -10^{-4}/10^{-8} \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1.0001  -10000 \\\\ 0  1 \\end{bmatrix}$.\n\n**2. Power Method Dynamics**\n\nThe power method iteration is $\\mathbf{x}_{k+1} = \\frac{A \\mathbf{x}_k}{\\|A \\mathbf{x}_k\\|_2}$. After $N$ iterations starting from $\\mathbf{x}_0$, the resulting vector is proportional to $A^N \\mathbf{x}_0$. Let the initial vector $\\mathbf{x}_0$ be decomposed in the eigenvector basis as $\\mathbf{x}_0 = c_1 \\mathbf{s}_1 + c_2 \\mathbf{s}_2$. Then:\n$$\nA^N \\mathbf{x}_0 = A^N (c_1 \\mathbf{s}_1 + c_2 \\mathbf{s}_2) = c_1 \\lambda_1^N \\mathbf{s}_1 + c_2 \\lambda_2^N \\mathbf{s}_2\n$$\nThe vector $A^N \\mathbf{x}_0$ can be rewritten as:\n$$\nA^N \\mathbf{x}_0 = \\lambda_2^N \\left( c_1 \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N \\mathbf{s}_1 + c_2 \\mathbf{s}_2 \\right)\n$$\nThe direction of the iterated vector $\\mathbf{x}_N$ depends on the relative magnitudes of the two terms in the parenthesis. Convergence to the dominant eigenvector $\\mathbf{s}_1$ requires the first term to dominate the second. This occurs if the amplification factor $(\\lambda_1/\\lambda_2)^N$ is large enough to overcome a potentially small initial coefficient ratio $|c_1/c_2|$.\n\nIn this problem, the amplification factor is $(\\frac{1+\\delta}{1})^N = (1+10^{-4})^{1000}$. Using the approximation $(1+x)^n \\approx e^{nx}$ for small $x$, we have $(1+10^{-4})^{1000} \\approx e^{1000 \\cdot 10^{-4}} = e^{0.1} \\approx 1.10517$. This relatively small amplification factor means that the power method will converge very slowly.\n\n**3. Analysis of Test Cases**\n\nThe final output for each case is `True` if the computed Rayleigh quotient $\\rho(\\mathbf{x}_N) = \\mathbf{x}_N^\\top A \\mathbf{x}_N$ is closer to $\\lambda_1$ than to $\\lambda_2$. This is equivalent to checking if $\\rho(\\mathbf{x}_N) > (\\lambda_1 + \\lambda_2)/2 = 1 + \\delta/2$.\n\n- **Case 1**: $\\mathbf{x}_0^{(1)} = \\mathbf{s}_2$.\nThe initial vector is exactly the subdominant eigenvector. Here, $c_1=0$ and $c_2=1$. The power method iteration will remain in the eigenspace of $\\lambda_2$ for all steps: $\\mathbf{x}_k = \\mathbf{s}_2/\\|\\mathbf{s}_2\\|_2$ for all $k>0$. Thus, $\\rho(\\mathbf{x}_N)$ will be equal to $\\lambda_2 = 1$ (up to floating point error). This is closer to $\\lambda_2$ than to $\\lambda_1$. The result is **False**.\n\n- **Case 2**: $\\mathbf{x}_0^{(2)} = \\mathbf{s}_2 + 10^{-30} \\mathbf{s}_1$.\nThe initial vector has coefficients $c_1 = 10^{-30}$ and $c_2 = 1$. The ratio of the component magnitudes after $N=1000$ iterations is:\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N \\mathbf{s}_1\\|}{\\|c_2 \\mathbf{s}_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N \\approx 10^{-30} \\times 1.105 \\approx 1.105 \\times 10^{-30}\n$$\nThis ratio is extraordinarily small. The vector $\\mathbf{x}_{1000}$ will be numerically indistinguishable from $\\mathbf{s}_2$. The Rayleigh quotient will be extremely close to $\\lambda_2$. The result is **False**.\n\n- **Case 3**: $\\mathbf{x}_0^{(3)} = \\mathbf{s}_1$.\nThe initial vector is the dominant eigenvector, so $c_1=1$ and $c_2=0$. The iteration immediately yields the dominant eigenvector: $\\mathbf{x}_k = \\mathbf{s}_1/\\|\\mathbf{s}_1\\|_2$ for all $k$. The Rayleigh quotient $\\rho(\\mathbf{x}_N)$ will be equal to $\\lambda_1 = 1.0001$. This is closer to $\\lambda_1$. The result is **True**.\n\n- **Case 4**: $\\mathbf{x}_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$.\nTo find the coefficients $c_1, c_2$, we solve $\\mathbf{x}_0^{(4)} = c_1 \\mathbf{s}_1 + c_2 \\mathbf{s}_2$. This is equivalent to $\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = S^{-1} \\mathbf{x}_0^{(4)}$.\n$$\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (\\varepsilon^{-1} + 1) - 1/\\varepsilon \\\\ 1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1/\\varepsilon \\end{bmatrix}\n$$\nSo, $c_1 = 1$ and $c_2 = 1/\\varepsilon = 10^8$. The initial vector has a component along $\\mathbf{s}_2$ that is $10^8$ times larger than its component along $\\mathbf{s}_1$. The ratio of component magnitudes after $N=1000$ iterations is:\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N \\mathbf{s}_1\\|}{\\|c_2 \\mathbf{s}_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N = \\varepsilon \\times (1+\\delta)^{1000} \\approx 10^{-8} \\times 1.105 \\approx 1.105 \\times 10^{-8}\n$$\nThis ratio is very small. Despite having a non-zero component in the dominant eigenvector direction, the iterate $\\mathbf{x}_{1000}$ is still overwhelmingly dominated by the subdominant eigenvector $\\mathbf{s}_2$ due to the slow convergence and the extreme initial imbalance. $\\rho(\\mathbf{x}_N)$ will be very close to $\\lambda_2$. The result is **False**.\n\nThe expected output is therefore `[False, False, True, False]`. This demonstrates that for nearly-defective matrices with eigenvalues close to each other, the power method can exhibit extremely slow convergence, appearing to converge to a subdominant eigenvector if the initial vector is not sufficiently aligned with the dominant one.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the power method problem for a nearly-defective matrix.\n    \"\"\"\n    # 1. Define constants and parameters from the problem statement.\n    epsilon = 1e-8\n    delta = 1e-4\n    lambda1 = 1.0 + delta\n    lambda2 = 1.0\n    N = 1000\n\n    # 2. Construct the matrix A.\n    # A = S J S^-1, where S = [[1, 1], [0, epsilon]] and J = diag(lambda1, lambda2).\n    # This simplifies to A = [[1+delta, -delta/epsilon], [0, 1]].\n    A = np.array([\n        [1.0 + delta, -delta / epsilon],\n        [0.0, 1.0]\n    ], dtype=np.float64)\n\n    # 3. Define the initial vectors for the test suite.\n    # Right eigenvectors s1 and s2\n    s1 = np.array([1.0, 0.0], dtype=np.float64)\n    s2 = np.array([1.0, epsilon], dtype=np.float64)\n\n    # Case 1: Exact subdominant eigenvector\n    x0_1 = s2\n    \n    # Case 2: Extremely close to subdominant eigenvector\n    x0_2 = s2 + 1e-30 * s1\n    \n    # Case 3: Exact dominant eigenvector\n    x0_3 = s1\n    \n    # Case 4: Nearly orthogonal to the left dominant eigenvector\n    x0_4 = np.array([1.0/epsilon + 1.0, 1.0], dtype=np.float64)\n\n    test_cases = [x0_1, x0_2, x0_3, x0_4]\n    \n    results = []\n\n    # 4. Iterate through each test case.\n    for x0 in test_cases:\n        # Check for zero vector, though not expected in these cases.\n        if np.linalg.norm(x0) == 0:\n            # Handle this unlikely edge case.\n            # A zero vector will remain zero, rho is undefined.\n            # The problem assumes x0 is non-zero.\n            # We can arbitrarily assign a result or raise an error.\n            # Let's assume it doesn't happen.\n            pass\n\n        x = x0.copy()\n\n        # 5. Implement the power method for N iterations.\n        # The normalization is part of the loop.\n        for _ in range(N):\n            y = A @ x\n            norm_y = np.linalg.norm(y)\n            if norm_y == 0: # Should not happen with this non-singular matrix A\n                x = np.zeros_like(x)\n                break\n            x = y / norm_y\n        \n        x_N = x\n\n        # 6. Compute the Rayleigh quotient for the final iterate x_N.\n        # rho(x) = (x.T * A * x) / (x.T * x)\n        # Since x_N is normalized, its L2 norm is 1, so x_N.T @ x_N = 1.\n        # rho(x_N) = x_N.T @ (A @ x_N)\n        rho = x_N.T @ A @ x_N\n        \n        # 7. Compare absolute differences to determine which eigenvalue is closer.\n        is_closer_to_lambda1 = abs(rho - lambda1)  abs(rho - lambda2)\n        results.append(is_closer_to_lambda1)\n\n    # 8. Format and print the final output as specified.\n    # str(bool) gives 'True' or 'False' with capital letters, as in the example.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Solving the full eigenvalue problem for large matrices is often computationally prohibitive, so modern scientific computing relies on projection methods that approximate a portion of the spectrum. This practice  introduces one of the most important of these techniques: the Arnoldi iteration. You will implement this algorithm to build a Krylov subspace and find \"Ritz eigenvalues,\" which are approximations to the true eigenvalues of the large matrix, providing a glimpse into the powerful methods used for large-scale scientific simulations.",
            "id": "3282272",
            "problem": "Consider the algebraic eigenvalue problem: for a square matrix $A \\in \\mathbb{R}^{n \\times n}$, find scalars $\\lambda \\in \\mathbb{C}$ and nonzero vectors $x \\in \\mathbb{C}^n$ such that $A x = \\lambda x$. A projection-based approach to approximating eigenvalues proceeds by constructing a Krylov subspace of dimension $m$, defined as $K_m(A,b) = \\operatorname{span}\\{b, A b, A^2 b, \\dots, A^{m-1} b\\}$, where $b \\in \\mathbb{R}^n$ is a nonzero starting vector. Using Arnoldi iteration, one can build an orthonormal basis for $K_m(A,b)$ and then form a small upper Hessenberg matrix that represents the action of $A$ on this subspace. The eigenvalues of this small matrix, called Ritz eigenvalues, provide approximations to the true eigenvalues of $A$.\n\nTask: Starting from the definitions above, implement Arnoldi iteration to construct an orthonormal basis for $K_m(A,b)$ and the associated upper Hessenberg matrix, then compute the Ritz eigenvalues as approximations to the eigenvalues of $A$. For each test case below, measure the quality of approximation by computing the following quantity: for the set of Ritz eigenvalues $\\{r_i\\}$ and the set of true eigenvalues $\\{\\lambda_j\\}$ of $A$, return the single float\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|.\n$$\nThis quantity is the maximum, over all Ritz eigenvalues, of the nearest-eigenvalue absolute difference in the complex plane. Report this value for each test case.\n\nImplement the program with the following test suite of parameter values. Each test case is a triple $(A, b, m)$:\n\n- Test case $1$ (general non-symmetric, diagonalizable, \"happy path\"): $A$ is formed as $A = S D S^{-1}$ with\n$$\nS = \\begin{bmatrix}\n1  0.2  -0.1  0  0  0 \\\\\n0  1  0.3  0  0  0 \\\\\n0  0  1  0.4  0  0 \\\\\n0  0  0  1  0.5  0 \\\\\n0  0  0  0  1  0.6 \\\\\n0  0  0  0  0  1\n\\end{bmatrix},\\quad\nD = \\operatorname{diag}(5,4,3,2,1,-1),\n$$\n$b = [1,1,1,1,1,1]^T$, and $m = 4$.\n\n- Test case $2$ (boundary condition $m = 1$): $A$ is the tridiagonal matrix of size $5 \\times 5$ with $2$ on the diagonal and $-1$ on the sub- and super-diagonals,\n$$\nA = \\begin{bmatrix}\n2  -1  0  0  0 \\\\\n-1  2  -1  0  0 \\\\\n0  -1  2  -1  0 \\\\\n0  0  -1  2  -1 \\\\\n0  0  0  -1  2\n\\end{bmatrix},\n$$\n$b = [1,0,0,0,0]^T$, and $m = 1$.\n\n- Test case $3$ (defective matrix edge case): $A$ is the Jordan block of size $6 \\times 6$ with eigenvalue $3$, that is, $A = 3 I + J$ where $J$ has $1$ on the superdiagonal and $0$ elsewhere,\n$$\nA = \\begin{bmatrix}\n3  1  0  0  0  0 \\\\\n0  3  1  0  0  0 \\\\\n0  0  3  1  0  0 \\\\\n0  0  0  3  1  0 \\\\\n0  0  0  0  3  1 \\\\\n0  0  0  0  0  3\n\\end{bmatrix},\n$$\n$b = [1,1,1,1,1,1]^T$, and $m = 3$.\n\n- Test case $4$ (full dimension $m = n$, symmetric case): $A$ is a symmetric $4 \\times 4$ matrix,\n$$\nA = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix},\n$$\n$b = [1,1,1,1]^T$, and $m = 4$.\n\nYour program must compute the quantity $\\max_i \\min_j |r_i - \\lambda_j|$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each value must be a float rounded to $8$ decimal places, with no additional text. For example, an output line has the format $[v_1,v_2,v_3,v_4]$ where each $v_k$ is the computed float for test case $k$ rounded to $8$ decimal places. No physical units or angles are involved, and all computations are purely numerical.",
            "solution": "The user has provided a valid problem statement from the field of numerical linear algebra. The task is to implement the Arnoldi iteration for approximating eigenvalues of a matrix and to evaluate the quality of these approximations for several test cases.\n\n### Introduction to Arnoldi Iteration\n\nThe algebraic eigenvalue problem seeks to find scalars $\\lambda$ and nonzero vectors $\\mathbf{x}$ for a given square matrix $A \\in \\mathbb{R}^{n \\times n}$ such that $A \\mathbf{x} = \\lambda \\mathbf{x}$. For large matrices, direct computation of all eigenvalues can be prohibitively expensive. Projection methods offer an efficient alternative by approximating the eigenvalues of $A$ with the eigenvalues of a much smaller matrix.\n\nThe Arnoldi iteration is a projection method that operates on a Krylov subspace. For a matrix $A$ and a starting vector $\\mathbf{b}$, the $m$-th Krylov subspace is defined as:\n$$\nK_m(A,\\mathbf{b}) = \\operatorname{span}\\{\\mathbf{b}, A \\mathbf{b}, A^2 \\mathbf{b}, \\dots, A^{m-1} \\mathbf{b}\\}\n$$\nThis subspace is of dimension at most $m$. The core idea is to find the best approximation to the eigenvalues of $A$ within this subspace. This is achieved by constructing an orthonormal basis for $K_m(A,\\mathbf{b})$ and then representing the action of $A$ with respect to that basis.\n\n### The Algorithm\n\nThe Arnoldi iteration employs a modified Gram-Schmidt procedure to build an orthonormal basis $\\{\\mathbf{q}_1, \\mathbf{q}_2, \\dots, \\mathbf{q}_m\\}$ for the Krylov subspace $K_m(A,\\mathbf{b})$. The basis vectors are stored as columns of a matrix $Q_m = [\\mathbf{q}_1, \\mathbf{q}_2, \\dots, \\mathbf{q}_m] \\in \\mathbb{R}^{n \\times m}$.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: Normalize the starting vector $\\mathbf{b}$ to obtain the first basis vector. Assuming $\\mathbf{b} \\neq 0$:\n    $$\n    \\mathbf{q}_1 = \\frac{\\mathbf{b}}{\\|\\mathbf{b}\\|_2}\n    $$\n2.  **Iteration**: For $k=1, 2, \\dots, m$:\n    a. Generate a new vector by applying $A$ to the latest basis vector:\n       $$\n       \\mathbf{v} = A \\mathbf{q}_k\n       $$\n    b. Orthogonalize $\\mathbf{v}$ against all previous basis vectors $\\mathbf{q}_1, \\dots, \\mathbf{q}_k$. The coefficients of this projection form the $k$-th column of the Hessenberg matrix $H_m$:\n       $$\n       \\text{for } j=1, \\dots, k: \\quad h_{j,k} = \\mathbf{q}_j^T \\mathbf{v} \\quad \\text{and} \\quad \\mathbf{v} \\leftarrow \\mathbf{v} - h_{j,k} \\mathbf{q}_j\n       $$\n    c. The norm of the resulting vector $\\mathbf{v}$ becomes the subdiagonal element of the Hessenberg matrix:\n       $$\n       h_{k+1, k} = \\|\\mathbf{v}\\|_2\n       $$\n    d. If $h_{k+1, k}$ is zero (or numerically close to zero), the algorithm has found an $A$-invariant subspace and terminates. This is known as \"breakdown\".\n    e. Normalize $\\mathbf{v}$ to get the next basis vector:\n       $$\n       \\mathbf{q}_{k+1} = \\frac{\\mathbf{v}}{h_{k+1, k}}\n       $$\n\nAfter $m$ steps (assuming no breakdown), this process yields the Arnoldi factorization:\n$$\nA Q_m = Q_{m+1} \\tilde{H}_m\n$$\nwhere $Q_m \\in \\mathbb{R}^{n \\times m}$ and $Q_{m+1} \\in \\mathbb{R}^{n \\times (m+1)}$ are matrices with orthonormal columns, and $\\tilde{H}_m \\in \\mathbb{R}^{(m+1) \\times m}$ is an upper Hessenberg matrix. The matrix $H_m \\in \\mathbb{R}^{m \\times m}$ is formed by the first $m$ rows of $\\tilde{H}_m$. This $H_m$ is the representation of $A$ projected onto the Krylov subspace, as $H_m = Q_m^T A Q_m$.\n\n### Ritz Eigenvalues and Error Metric\n\nThe eigenvalues of the small $m \\times m$ Hessenberg matrix $H_m$ are called the **Ritz eigenvalues**. These values, denoted $\\{r_i\\}_{i=1}^m$, serve as approximations to the true eigenvalues of $A$, which we denote $\\{\\lambda_j\\}_{j=1}^n$.\n\nTo measure the quality of this approximation, we compute the Hausdorff distance between the set of Ritz eigenvalues and the set of true eigenvalues. The specific metric is:\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|\n$$\nThis formula calculates, for each Ritz eigenvalue $r_i$, the absolute distance to the nearest true eigenvalue $\\lambda_j$. The final reported value is the maximum of these minimum distances.\n\n### Implementation Plan\n\nThe solution will be implemented as a Python program following these steps for each test case $(A, \\mathbf{b}, m)$:\n\n1.  **Construct** the matrix $A \\in \\mathbb{R}^{n \\times n}$ and the starting vector $\\mathbf{b} \\in \\mathbb{R}^n$ as specified.\n2.  **Compute True Eigenvalues**: Calculate the full set of eigenvalues $\\{\\lambda_j\\}$ of $A$ using a standard numerical library function.\n3.  **Perform Arnoldi Iteration**: Implement the algorithm described above to generate the $m \\times m$ upper Hessenberg matrix $H_m$. The implementation will handle potential breakdown by returning a smaller matrix if an invariant subspace is found before $m$ iterations are completed.\n4.  **Compute Ritz Eigenvalues**: Calculate the eigenvalues $\\{r_i\\}$ of the resulting matrix $H_m$.\n5.  **Calculate Metric**: Compute the error metric $\\max_i \\min_j |r_i - \\lambda_j|$ by finding the maximum of the minimum distances between the two sets of eigenvalues in the complex plane.\n6.  **Format Output**: Collect the metric for each test case and format the final output as a comma-separated list of floating-point numbers rounded to $8$ decimal places, enclosed in brackets.\n\nAll matrix and vector operations will be performed using the `numpy` library. Since the input matrices and vectors are real, the Arnoldi iteration will be performed using real arithmetic. The resulting eigenvalues may be complex, which `numpy`'s eigenvalue solvers handle automatically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_iteration(A, b, m):\n    \"\"\"\n    Performs Arnoldi iteration to find the Hessenberg matrix H.\n\n    Args:\n        A (np.ndarray): The square matrix.\n        b (np.ndarray): The starting vector.\n        m (int): The dimension of the Krylov subspace.\n\n    Returns:\n        np.ndarray: The m x m Hessenberg matrix H_m.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Use real types since A and b are real. Eigenvalues may be complex later.\n    H = np.zeros((m + 1, m), dtype=np.float64)\n    Q = np.zeros((n, m + 1), dtype=np.float64)\n\n    Q[:, 0] = b / np.linalg.norm(b)\n\n    for k in range(m):\n        v = A @ Q[:, k]\n        for j in range(k + 1):\n            H[j, k] = np.dot(Q[:, j].T, v)\n            v = v - H[j, k] * Q[:, j]\n\n        h_next_k = np.linalg.norm(v)\n\n        # Handle breakdown: invariant subspace found\n        if h_next_k  1e-12:\n            return H[:k + 1, :k + 1]\n\n        H[k + 1, k] = h_next_k\n        Q[:, k + 1] = v / h_next_k\n        \n    return H[:m, :m]\n\ndef calculate_approximation_error(ritz_vals, true_vals):\n    \"\"\"\n    Calculates the approximation error metric max_i min_j |r_i - lambda_j|.\n\n    Args:\n        ritz_vals (np.ndarray): Array of Ritz eigenvalues.\n        true_vals (np.ndarray): Array of true eigenvalues.\n\n    Returns:\n        float: The calculated error metric.\n    \"\"\"\n    if ritz_vals.size == 0:\n        return 0.0\n\n    # Reshape for broadcasting\n    ritz_vals_col = ritz_vals[:, np.newaxis]\n    \n    # Calculate the matrix of absolute differences\n    abs_diff_matrix = np.abs(ritz_vals_col - true_vals)\n    \n    # Find the minimum distance for each Ritz value to any true eigenvalue\n    min_dists = np.min(abs_diff_matrix, axis=1)\n    \n    # The error is the maximum of these minimum distances\n    error = np.max(min_dists)\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test case 1\n    S1 = np.array([\n        [1, 0.2, -0.1, 0, 0, 0],\n        [0, 1, 0.3, 0, 0, 0],\n        [0, 0, 1, 0.4, 0, 0],\n        [0, 0, 0, 1, 0.5, 0],\n        [0, 0, 0, 0, 1, 0.6],\n        [0, 0, 0, 0, 0, 1]\n    ], dtype=np.float64)\n    D1 = np.diag([5, 4, 3, 2, 1, -1])\n    A1 = S1 @ D1 @ np.linalg.inv(S1)\n    b1 = np.ones(6)\n    m1 = 4\n\n    # Test case 2\n    n2 = 5\n    A2 = np.diag(2 * np.ones(n2)) + np.diag(-1 * np.ones(n2 - 1), k=1) + np.diag(-1 * np.ones(n2 - 1), k=-1)\n    b2 = np.zeros(n2)\n    b2[0] = 1\n    m2 = 1\n\n    # Test case 3\n    n3 = 6\n    A3 = 3 * np.eye(n3) + np.diag(np.ones(n3 - 1), k=1)\n    b3 = np.ones(n3)\n    m3 = 3\n\n    # Test case 4\n    A4 = np.array([\n        [4, 1, 0, 0],\n        [1, 3, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ], dtype=np.float64)\n    b4 = np.ones(4)\n    m4 = 4\n\n    test_cases = [\n        (A1, b1, m1),\n        (A2, b2, m2),\n        (A3, b3, m3),\n        (A4, b4, m4)\n    ]\n\n    results = []\n    for A, b, m in test_cases:\n        # Step 1: Compute true eigenvalues\n        true_eigenvalues = np.linalg.eigvals(A)\n        \n        # Step 2: Perform Arnoldi iteration\n        H_m = arnoldi_iteration(A, b, m)\n        \n        # Step 3: Compute Ritz eigenvalues\n        ritz_eigenvalues = np.linalg.eigvals(H_m)\n        \n        # Step 4: Calculate the error metric\n        error = calculate_approximation_error(ritz_eigenvalues, true_eigenvalues)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.8f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}