{
    "hands_on_practices": [
        {
            "introduction": "将理论付诸实践是掌握任何数值算法的关键。本练习将指导您为幂法开发一个实际的停止准则。通过推导并实现一个基于连续迭代向量之间夹角余弦值的判据 ，您将学习如何将收敛的理论概念转化为稳健的代码，这是实现任何迭代方法时都必须解决的基本问题。",
            "id": "3283230",
            "problem": "要求您为幂法（Power Method）设计、论证并实现一个停止准则。幂法是一种用于近似方阵主特征向量的迭代算法。考虑一个实方阵 $A \\in \\mathbb{R}^{n \\times n}$ 和一个非零初始向量 $x_0 \\in \\mathbb{R}^n$。幂法通过重复应用矩阵 $A$ 并在欧几里得范数下重新归一化来生成迭代向量。设 $x_k$ 表示第 $k$ 步的归一化迭代向量，并设 $x_k$ 与 $x_{k+1}$ 之间的夹角以弧度为单位。该夹角的余弦值是单位向量的内积。\n\n任务：\n- 从特征值和特征向量的基本定义、矩阵-向量乘法的线性性以及欧几里得范数出发，推导出一个实用的停止准则，其决策变量是连续归一化迭代向量 $x_k$ 和 $x_{k+1}$ 之间夹角的余弦值。您的准则必须仅依赖于此余弦值和用户指定的容差 $\\tau > 0$，并且必须对迭代向量的缩放保持不变。不要假定任何快捷公式；从幂法在特征基下的行为的第一性原理推导该准则。\n- 实现一个程序，使用您的停止准则来应用幂法。使用欧几里得范数（$2$-范数）对每个迭代向量进行归一化。在每一步 $k$，使用单位向量的内积计算 $x_k$ 和 $x_{k+1}$ 之间的余弦值，并仅使用此余弦值和给定的容差 $\\tau$ 来决定是否停止。\n- 如果该方法在第 $k$ 次迭代时满足停止准则（即在 $k$ 次矩阵-向量乘法之后），您的程序应记录整数 $k$。如果在规定的最大迭代次数 $K_{\\max}$ 内未满足该准则，则为该测试用例记录整数 $-1$。\n- 角度在概念上以弧度为单位，但程序应仅使用余弦值。\n\n使用以下参数值的测试套件：\n- 测试用例 1（一般顺利情况，对称矩阵，具有简单的主特征值）：$A = \\begin{bmatrix} 5.0 & 0.0 & 0.0 \\\\ 0.0 & 3.0 & 0.0 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$，$\\tau = 10^{-6}$，$K_{\\max} = 1000$。\n- 测试用例 2（非对称上三角矩阵，对齐速度较快但可能存在符号效应）：$A = \\begin{bmatrix} 2.0 & 1.0 & 0.0 \\\\ 0.0 & 4.0 & 1.0 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix}$，$\\tau = 10^{-9}$，$K_{\\max} = 5000$。\n- 测试用例 3（由于主特征值和次主特征值的模几乎相等而导致收敛缓慢）：$A = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 0.99 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 0.1 \\end{bmatrix}$，$\\tau = 10^{-6}$，$K_{\\max} = 5000$。\n- 测试用例 4（边缘情况，最大模特征值不唯一，可能不收敛）：$A = \\begin{bmatrix} 2.0 & 0.0 \\\\ 0.0 & -2.0 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$，$\\tau = 10^{-6}$，$K_{\\max} = 1000$。\n\n您的程序应生成一行输出，其中包含所有测试用例的迭代次数，格式为用方括号括起来的逗号分隔列表。例如，如果四个测试用例的计数分别为 $12$、$37$、$-1$ 和 $9$，则输出必须是且仅是这一行\n$$[12,37,-1,9]$$\n不得打印任何其他文本。所有数值输出必须是整数。此问题中没有物理单位。角度应理解为弧度，但程序仅对余弦值（无量纲）进行操作。",
            "solution": "问题要求基于连续迭代向量之间的夹角，为幂法推导并实现一个停止准则。\n\n### 停止准则的推导\n\n设 $A$ 是一个实可对角化的 $n \\times n$ 矩阵，$A \\in \\mathbb{R}^{n \\times n}$。设其特征值为 $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$，对应的特征向量为 $v_1, v_2, \\ldots, v_n$。特征向量 $\\{v_i\\}_{i=1}^n$ 构成了 $\\mathbb{R}^n$ 的一个基。幂法旨在寻找与主特征值对应的特征向量，主特征值是指具有严格最大模的特征值。我们假设存在这样的一个特征值，即 $|\\lambda_1| > |\\lambda_2| \\ge |\\lambda_3| \\ge \\ldots \\ge |\\lambda_n|$。与特征值 $\\lambda$ 相关联的特征向量 $v$ 满足基本方程 $A v = \\lambda v$。\n\n幂法是一种迭代算法，从一个非零初始向量 $x^{(0)} \\in \\mathbb{R}^n$ 开始。这个初始向量可以表示为特征向量的线性组合：\n$$x^{(0)} = c_1 v_1 + c_2 v_2 + \\cdots + c_n v_n$$\n为使该方法成功，初始向量在主特征向量 $v_1$ 方向上的分量必须非零，即 $c_1 \\neq 0$。\n\n该方法的核心是重复将矩阵 $A$ 应用于向量。经过 $k$ 次应用后，我们得到：\n$$A^k x^{(0)} = A^k \\left( \\sum_{i=1}^{n} c_i v_i \\right) = \\sum_{i=1}^{n} c_i (A^k v_i) = \\sum_{i=1}^{n} c_i (\\lambda_i^k v_i)$$\n将主特征值项 $\\lambda_1^k$ 提取出来：\n$$A^k x^{(0)} = \\lambda_1^k \\left( c_1 v_1 + c_2 \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k v_2 + \\cdots + c_n \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^k v_n \\right)$$\n由于主导条件 $|\\lambda_1| > |\\lambda_i|$ 对所有 $i > 1$ 成立，比值 $|\\lambda_i / \\lambda_1|$ 都小于 $1$。随着迭代次数 $k$ 趋于无穷大，对于 $i=2, \\ldots, n$，项 $(\\lambda_i / \\lambda_1)^k$ 趋于 $0$。因此，向量 $A^k x^{(0)}$ 变得越来越与主特征向量 $v_1$ 对齐：\n$$\\lim_{k \\to \\infty} \\frac{A^k x^{(0)}}{\\lambda_1^k} = c_1 v_1$$\n\n幂法算法生成一个归一化的迭代向量序列。设 $x^{(k)}$ 是经过 $k$ 次矩阵乘法后的归一化迭代向量。步骤如下：\n$1$. 从一个初始向量 $x_0$ 开始并将其归一化：$x^{(0)} = x_0 / \\|x_0\\|_2$。\n$2$. 对于 $k = 1, 2, \\ldots$，计算下一个迭代向量：\n   $$z^{(k)} = A x^{(k-1)}$$\n   $$x^{(k)} = \\frac{z^{(k)}}{\\|z^{(k)}\\|_2}$$\n向量 $x^{(k)}$ 与 $A^k x_0$ 平行。根据以上分析，当 $k$ 变得很大时，向量序列 $\\{x^{(k)}\\}$ 在方向上收敛于主特征向量 $v_1$。也就是说，$x^{(k)}$ 变得几乎与 $v_1$ 平行。\n\n我们可以通过度量连续迭代向量之间的变化来构建一个停止准则。如果方法正在收敛，那么 $x^{(k-1)}$ 和 $x^{(k)}$ 的方向应该变得几乎相同。当 $x^{(k-1)}$ 已经是主特征向量的一个良好近似时，即 $x^{(k-1)} \\approx \\pm v_1/\\|v_1\\|_2$，那么应用 $A$ 会得到 $A x^{(k-1)} \\approx A(\\pm v_1/\\|v_1\\|_2) = \\pm (\\lambda_1/\\|v_1\\|_2) v_1$。再次归一化该向量会得到一个与 $v_1$ 平行的向量。因此，$x^{(k)}$ 也将几乎与 $v_1$ 平行，从而也与 $x^{(k-1)}$ 平行。\n\n两个连续归一化迭代向量 $x^{(k-1)}$ 和 $x^{(k)}$ 之间的夹角 $\\theta_k$ 是收敛的一个度量。由于它们是单位向量，它们之间夹角的余弦值就是它们的内积：\n$$\\cos(\\theta_k) = (x^{(k-1)})^T x^{(k)}$$\n随着方法的收敛，向量变得共线，即 $\\theta_k \\to 0$ 或 $\\theta_k \\to \\pi$。这对应于 $\\cos(\\theta_k) \\to 1$ 或 $\\cos(\\theta_k) \\to -1$。极限的符号取决于主特征值 $\\lambda_1$ 的符号。如果 $\\lambda_1 > 0$，迭代向量将持续指向同一方向，并且 $\\cos(\\theta_k) \\to 1$。如果 $\\lambda_1 < 0$，方向将在每次迭代时翻转，并且 $\\cos(\\theta_k) \\to -1$。\n\n一个鲁棒的准则必须能处理这两种情况。我们可以通过考虑余弦的绝对值 $|\\cos(\\theta_k)| = |(x^{(k-1)})^T x^{(k)}|$ 来实现这一点。随着方法的收敛，这个值趋近于 $1$。当 $|\\cos(\\theta_k)|$ 足够接近 $1$ 时，我们可以停止迭代。\n\n我们将偏离完美对齐的量定义为 $\\delta_k = 1 - |\\cos(\\theta_k)| = 1 - |(x^{(k-1)})^T x^{(k)}|$。这个量是非负的，并在收敛时趋于 $0$。问题指定了一个用户定义的容差 $\\tau > 0$。一个实用且尺度不变的停止准则是当这个偏差小于容差时终止过程。\n\n停止准则是：在第 $k$ 次迭代时（即在第 $k$ 次矩阵-向量乘法之后），从 $x^{(k-1)}$ 计算出归一化的迭代向量 $x^{(k)}$。如果条件\n$$1 - |(x^{(k-1)})^T x^{(k)}| < \\tau$$\n得到满足，迭代停止，结果为 $k$。如果在最大迭代次数 $K_{\\max}$ 内未满足该准则，则认为方法未收敛，结果为 $-1$。该准则仅依赖于连续迭代向量之间的余弦值和容差 $\\tau$，符合要求。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_method(A, x0, tau, K_max):\n    \"\"\"\n    Approximates the dominant eigenvector of a matrix A using the Power Method.\n\n    Args:\n        A (np.ndarray): The square matrix.\n        x0 (np.ndarray): The initial non-zero vector.\n        tau (float): The tolerance for the stopping criterion.\n        K_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations until convergence, or -1 if not converged.\n    \"\"\"\n    # Normalize the initial vector x0 using the Euclidean (2-norm).\n    norm_x0 = np.linalg.norm(x0)\n    if norm_x0 < 1e-12:  # Treat as a zero vector\n        # The problem statement guarantees a non-zero initial vector.\n        # This check is for robustness.\n        return -1\n\n    x_prev = x0 / norm_x0\n\n    for k in range(1, K_max + 1):\n        # Apply the matrix A to the previous iterate\n        y = A @ x_prev\n\n        # Normalize the resulting vector to get the new iterate\n        norm_y = np.linalg.norm(y)\n        if norm_y < 1e-12:\n            # If A*x_prev is the zero vector, this means we've converged to\n            # an eigenvector with eigenvalue 0. The next iterate is undefined.\n            # In this specific context, we'll consider this a form of convergence\n            # as the direction won't change further. The cosine would be undefined.\n            # Let's assume the cosine is 1.0, satisfying the criterion for any positive tau.\n            return k\n\n        x_curr = y / norm_y\n\n        # Compute the cosine of the angle between successive normalized iterates.\n        # Since they are unit vectors, this is their dot product.\n        # We take the absolute value to handle cases where the dominant eigenvalue is negative.\n        cosine_val = np.abs(np.dot(x_prev, x_curr))\n\n        # Check the stopping criterion: 1 - |cos(theta)| < tau\n        if (1.0 - cosine_val) < tau:\n            return k\n\n        # Prepare for the next iteration\n        x_prev = x_curr\n\n    # If the loop completes without convergence, return -1.\n    return -1\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases and runs the power method for each, printing the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            np.array([[5.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 1.0]]),\n            np.array([1.0, 1.0, 1.0]),\n            1e-6,\n            1000\n        ),\n        # Test case 2\n        (\n            np.array([[2.0, 1.0, 0.0], [0.0, 4.0, 1.0], [0.0, 0.0, 1.0]]),\n            np.array([1.0, 2.0, 3.0]),\n            1e-9,\n            5000\n        ),\n        # Test case 3\n        (\n            np.array([[1.0, 0.0], [0.0, 0.99]]),\n            np.array([1.0, 0.1]),\n            1e-6,\n            5000\n        ),\n        # Test case 4\n        (\n            np.array([[2.0, 0.0], [0.0, -2.0]]),\n            np.array([1.0, 1.0]),\n            1e-6,\n            1000\n        )\n    ]\n\n    results = []\n    for A, x0, tau, K_max in test_cases:\n        iteration_count = power_method(A, x0, tau, K_max)\n        results.append(iteration_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "幂法的收敛速度通常被描述为几何级的，但这并非总是如此。这个思想实验探讨了一个特殊情况：当矩阵不可对角化时会发生什么 。通过分析一个假设的剪切矩阵，您将推导出迭代向量的精确路径，并揭示一种较慢的代数收敛形式 ($O(k^{-1})$)，从而加深对算法收敛行为背后条件的理解。",
            "id": "3283226",
            "problem": "考虑由 $A = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix}$ 定义的 $2 \\times 2$ 剪切矩阵 $A$，其中实参数 $\\alpha \\neq 0$。用于逼近矩阵主特征对的幂法（PM）通过重复乘法和归一化生成迭代序列。设初始向量为 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，定义未归一化的迭代向量为 $x^{(k)} = A^{k} x^{(0)}$，归一化的迭代向量为 $v^{(k)} = \\dfrac{x^{(k)}}{\\|x^{(k)}\\|_{2}}$，其中 $\\|\\cdot\\|_{2}$ 表示欧几里得范数。设 $e_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 表示与特征值 $1$ 对应的特征向量。从矩阵的幂和欧几里得范数的基本定义出发，推导 $x^{(k)}$ 和 $v^{(k)}$ 的闭式表达式，并用它来描绘归一化迭代向量 $v^{(k)}$ 在单位圆上的几何路径。然后，设 $\\theta_{k}$ 为 $v^{(k)}$ 和 $e_{1}$ 之间的夹角（以弧度为单位），由 $\\tan(\\theta_{k}) = \\dfrac{\\text{second component of } v^{(k)}}{\\text{first component of } v^{(k)}}$ 定义。仅使用矩阵乘法和范数的基本性质，确定 $\\theta_{k}$ 的渐近衰减，并计算极限 $\\lim_{k \\to \\infty} k \\tan(\\theta_{k})$，将其表示为含 $\\alpha$ 的闭式表达式。将最终答案表示为单个解析表达式。无需四舍五入。",
            "solution": "该问题是适定的且在科学上是合理的，基于线性代数和数值分析的基本原理。我们可以按照问题陈述中概述的步骤进行求解。\n\n首先，我们确定矩阵 $A = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix}$ 的 $k$ 次幂。我们计算前几次幂来寻找规律。\n$A^{1} = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1\\alpha \\\\ 0 & 1 \\end{pmatrix}$\n$A^{2} = A \\cdot A = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + \\alpha \\cdot 0 & 1 \\cdot \\alpha + \\alpha \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 0 & 0 \\cdot \\alpha + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2\\alpha \\\\ 0 & 1 \\end{pmatrix}$\n$A^{3} = A^{2} \\cdot A = \\begin{pmatrix} 1 & 2\\alpha \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & \\alpha + 2\\alpha \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 3\\alpha \\\\ 0 & 1 \\end{pmatrix}$\n规律很明显，$A^{k} = \\begin{pmatrix} 1 & k\\alpha \\\\ 0 & 1 \\end{pmatrix}$ 对任何整数 $k \\ge 1$ 成立。这可以通过数学归纳法严格证明。当 $k=1$ 时，基础情形成立。假设该式对某个整数 $k \\ge 1$ 成立，则我们有 $A^{k+1} = A^k A = \\begin{pmatrix} 1 & k\\alpha \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & \\alpha + k\\alpha \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & (k+1)\\alpha \\\\ 0 & 1 \\end{pmatrix}$，这就完成了归纳证明。对于 $k=0$，$A^0=I$ 也符合该公式。\n\n接下来，我们推导未归一化迭代向量 $x^{(k)} = A^{k} x^{(0)}$ 的闭式表达式。初始向量为 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n$$x^{(k)} = A^{k} x^{(0)} = \\begin{pmatrix} 1 & k\\alpha \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + k\\alpha \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix}$$\n这就是 $x^{(k)}$ 的闭式表达式。\n\n现在，我们求解归一化的迭代向量 $v^{(k)} = \\dfrac{x^{(k)}}{\\|x^{(k)}\\|_{2}}$。我们首先计算 $x^{(k)}$ 的欧几里得范数：\n$$\\|x^{(k)}\\|_{2} = \\left\\| \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix} \\right\\|_{2} = \\sqrt{(k\\alpha)^2 + 1^2} = \\sqrt{k^2\\alpha^2 + 1}$$\n因此，归一化的迭代向量 $v^{(k)}$ 是：\n$$v^{(k)} = \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} \\begin{pmatrix} k\\alpha \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}} \\\\ \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} \\end{pmatrix}$$\n这就是 $v^{(k)}$ 的闭式表达式。\n\n我们描绘 $v^{(k)}$ 的几何路径。根据定义，每个向量 $v^{(k)}$ 都是一个单位向量，因此其末端点位于 $\\mathbb{R}^2$ 中的单位圆上。$v^{(k)}$ 的第二个分量 $\\frac{1}{\\sqrt{k^2\\alpha^2 + 1}}$ 对任何 $k \\ge 0$ 恒为正。因此，所有迭代向量都位于上半圆上。\n对于 $k=0$，我们有 $v^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n当 $k \\to \\infty$ 时，我们分析 $v^{(k)}$ 的分量：\n第一个分量：$\\lim_{k \\to \\infty} \\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}} = \\lim_{k \\to \\infty} \\frac{k\\alpha}{|k\\alpha|\\sqrt{1 + \\frac{1}{k^2\\alpha^2}}} = \\lim_{k \\to \\infty} \\frac{\\alpha}{|\\alpha|\\sqrt{1 + \\frac{1}{k^2\\alpha^2}}} = \\frac{\\alpha}{|\\alpha|} = \\text{sgn}(\\alpha)$。\n第二个分量：$\\lim_{k \\to \\infty} \\frac{1}{\\sqrt{k^2\\alpha^2 + 1}} = 0$。\n所以，$\\lim_{k \\to \\infty} v^{(k)} = \\begin{pmatrix} \\text{sgn}(\\alpha) \\\\ 0 \\end{pmatrix}$。\n如果 $\\alpha > 0$，迭代向量 $v^{(k)}$ 在单位圆上描绘出一条从 $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 趋向 $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = e_1$ 的路径。\n如果 $\\alpha < 0$，迭代向量描绘出一条从 $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 趋向 $\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = -e_1$ 的路径。\n\n接下来，我们使用问题中关于 $v^{(k)}$ 和 $e_1$ 之间夹角 $\\theta_k$ 的定义：\n$$\\tan(\\theta_{k}) = \\frac{\\text{second component of } v^{(k)}}{\\text{first component of } v^{(k)}}$$\n代入 $v^{(k)}$ 的分量：\n$$\\tan(\\theta_{k}) = \\frac{\\frac{1}{\\sqrt{k^2\\alpha^2 + 1}}}{\\frac{k\\alpha}{\\sqrt{k^2\\alpha^2 + 1}}} = \\frac{1}{k\\alpha}$$\n注意这对于 $k \\ge 1$ 成立，因为当 $k=0$ 时，第一个分量为 $0$。\n\n从这个表达式中，我们确定 $\\theta_k$ 的渐近衰减。当 $k \\to \\infty$ 时，$\\tan(\\theta_k) = \\frac{1}{k\\alpha} \\to 0$。 这意味着如果 $\\alpha > 0$，则 $\\theta_k \\to 0$；如果 $\\alpha < 0$，则 $\\theta_k \\to \\pi$（考虑到 $v^{(k)}$ 各分量的符号）。对于大的 $k$，$\\tan(\\theta_k)$ 很小。使用小角度近似 $\\tan(x) \\approx x$（当 $x \\approx 0$ 时），我们发现对于 $\\alpha>0$，$\\theta_k \\approx \\frac{1}{k\\alpha}$。夹角 $\\theta_k$ 以 $O(k^{-1})$ 的渐近率衰减到 $0$。问题要求的不是 $\\theta_k$ 本身，而是它的正切值。$\\tan(\\theta_k)$ 的渐近行为精确地是 $\\frac{1}{k\\alpha}$。\n\n最后，我们计算所要求的极限：\n$$\\lim_{k \\to \\infty} k \\tan(\\theta_{k})$$\n使用我们推导出的 $\\tan(\\theta_k)$ 的表达式：\n$$\\lim_{k \\to \\infty} k \\left( \\frac{1}{k\\alpha} \\right) = \\lim_{k \\to \\infty} \\frac{k}{k\\alpha} = \\lim_{k \\to \\infty} \\frac{1}{\\alpha}$$\n因为 $\\alpha$ 是一个非零常数，所以极限就是 $\\frac{1}{\\alpha}$。这个结果对任何实数 $\\alpha \\neq 0$ 都成立。这表明对于这个亏损矩阵，向量迭代方向的收敛是代数收敛，速率为 $O(k^{-1})$，而不是幂法在处理可对角化矩阵时通常出现的几何收敛速率。",
            "answer": "$$\\boxed{\\frac{1}{\\alpha}}$$"
        },
        {
            "introduction": "在找到主特征值之后，我们如何找到下一个？这个练习介绍了一种称为“降维”（deflation）的强大技术。您将实现一种方法，在找到主特征对后，“移除”其对矩阵的影响，从而揭示次主特征值 。此外，本练习还探讨了一个关键的数值稳定性问题：对主特征向量的微小估计误差会如何影响我们对次主特征值的计算精度。",
            "id": "3283273",
            "problem": "考虑一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其特征对为 $\\{(\\lambda_i, v_i)\\}_{i=1}^n$，满足 $A v_i = \\lambda_i v_i$。其中，特征向量 $\\{v_i\\}$ 构成一组标准正交基，且特征值按 $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$ 排序。幂法通过对一个非零向量 $x_k$ 进行 $x_{k+1} = A x_k / \\|A x_k\\|_2$ 的迭代来寻找主特征对，并通过瑞利商 $x_k^\\top A x_k / (x_k^\\top x_k)$ 来估计特征值。降阶法从矩阵 $A$ 中移除主特征对的贡献，以揭示下一个特征值。如果 $(\\lambda_1, v_1)$ 已知，可以构造一个降阶矩阵 $A_1 = A - \\lambda_1 v_1 v_1^\\top$。对于该矩阵，$v_1$ 会成为一个特征值为0的特征向量，且 $A_1$ 的最大特征值在理想情况下是 $\\lambda_2$。在实践中，$v_1$ 是近似得到的，因此降阶过程是不完美的。\n\n从上述基本定义出发，实现幂法以获得一个近似的主特征对，然后执行一阶降阶来估计第二大特征值。分析降阶过程对 $v_1$ 近似值中角度误差的灵敏度。\n\n算法要求：\n\n- 实现一个函数，该函数接收一个实对称矩阵 $A$、一个初始向量 $x_0 \\ne 0$、一个容差 $\\varepsilon > 0$ 和一个最大迭代次数上限 $K_{\\max}$ 作为输入，执行迭代 $x_{k+1} = A x_k / \\|A x_k\\|_2$，直到连续的瑞利商之差小于 $\\varepsilon$ 或达到 $K_{\\max}$ 次迭代。函数应返回最终的瑞利商和归一化向量。\n- 实现一阶降阶：给定一个标量 $\\mu$ 和一个单位向量 $u$，构造矩阵 $\\tilde{A} = A - \\mu\\, u u^\\top$。\n- 为进行灵敏度分析，构造一个受扰动的主方向 $\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$，其中角度 $\\theta$ 以弧度为单位。在降阶中使用 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$，以模拟在实践中使用与近似向量相关的瑞利商。\n\n测试套件：\n\n使用以下 $4 \\times 4$ 对称矩阵和角度（以弧度为单位），初始向量 $x_0$ 为全1向量，容差 $\\varepsilon = 10^{-12}$，以及 $K_{\\max} = 10000$。\n\n- 矩阵 $A_{\\mathrm{easy}}$:\n$$\nA_{\\mathrm{easy}} = \\begin{bmatrix}\n4 & 1 & 0 & 0 \\\\\n1 & 3 & 1 & 0 \\\\\n0 & 1 & 2 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}.\n$$\n- 矩阵 $A_{\\mathrm{close}}$:\n$$\nA_{\\mathrm{close}} = \\begin{bmatrix}\n2 & 1 & 0 & 0 \\\\\n1 & 2 & 1 & 0 \\\\\n0 & 1 & 2 & 1 \\\\\n0 & 0 & 1 & 2\n\\end{bmatrix}.\n$$\n\n用于构造 $\\hat{v}_1(\\theta)$ 的角度：$\\theta \\in \\{0, 0.05, 0.2\\}$（以弧度为单位）。对于每个矩阵 $A \\in \\{A_{\\mathrm{easy}}, A_{\\mathrm{close}}\\}$ 和每个角度 $\\theta$，执行以下操作：\n\n1. 通过任何正确的数值方法计算 $A$ 的真实第二大特征值 $\\lambda_2$。\n2. 使用 $A$ 的真实 $v_1$ 和 $v_2$ 构造 $\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$。\n3. 计算 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$。\n4. 降阶：$\\tilde{A}(\\theta) = A - \\mu(\\theta)\\, \\hat{v}_1(\\theta)\\hat{v}_1(\\theta)^\\top$。\n5. 从 $x_0$ 开始，对 $\\tilde{A}(\\theta)$ 应用幂法来估计其主特征值，记为 $\\tilde{\\lambda}_1(\\theta)$，它可作为不完美降阶下 $A$ 的 $\\lambda_2$ 的估计值。\n6. 报告绝对误差 $|\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$，结果为浮点数。\n\n最终输出格式：\n\n你的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，排序如下：\n$$\n\\big[ e(A_{\\mathrm{easy}}, 0), e(A_{\\mathrm{easy}}, 0.05), e(A_{\\mathrm{easy}}, 0.2), e(A_{\\mathrm{close}}, 0), e(A_{\\mathrm{close}}, 0.05), e(A_{\\mathrm{close}}, 0.2) \\big],\n$$\n其中 $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$。将每个浮点数值四舍五入到10位小数。角度以弧度为单位。不涉及物理单位。",
            "solution": "用户提供了一个关于幂法和一阶降阶的数值线性代数问题。任务是实现这些方法，并分析降阶过程对主特征向量近似误差的灵敏度。\n\n### 问题验证\n\n**第一步：提取已知信息**\n- **矩阵：** 两个大小为 $4 \\times 4$ 的实对称矩阵，$A_{\\mathrm{easy}}$ 和 $A_{\\mathrm{close}}$。\n$$\nA_{\\mathrm{easy}} = \\begin{bmatrix}\n4 & 1 & 0 & 0 \\\\\n1 & 3 & 1 & 0 \\\\\n0 & 1 & 2 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix}, \\quad\nA_{\\mathrm{close}} = \\begin{bmatrix}\n2 & 1 & 0 & 0 \\\\\n1 & 2 & 1 & 0 \\\\\n0 & 1 & 2 & 1 \\\\\n0 & 0 & 1 & 2\n\\end{bmatrix}\n$$\n- **特征对：** 实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 具有特征对 $\\{(\\lambda_i, v_i)\\}_{i=1}^n$，其中 $A v_i = \\lambda_i v_i$。特征向量 $\\{v_i\\}$ 构成一组标准正交基，特征值按绝对值大小排序：$|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$。\n- **幂法算法：** 一种寻找主特征对的迭代方法。迭代定义为 $x_{k+1} = A x_k / \\|A x_k\\|_2$。特征值通过瑞利商 $R(x_k) = x_k^\\top A x_k / (x_k^\\top x_k)$ 进行估计。\n- **幂法实现参数：**\n    - 用于降阶矩阵的初始向量：$x_0 = [1, 1, 1, 1]^\\top$。\n    - 容差：$\\varepsilon = 10^{-12}$。\n    - 最大迭代次数：$K_{\\max} = 10000$。\n- **一阶降阶：** 一个构造新矩阵 $A_1 = A - \\lambda_1 v_1 v_1^\\top$ 的过程，该矩阵的主特征值为 $\\lambda_2$。\n- **灵敏度分析：**\n    - 构造一个受扰动的主特征向量：$\\hat{v}_1(\\theta) = \\mathrm{normalize}(\\cos \\theta\\, v_1 + \\sin \\theta\\, v_2)$，其中 $v_1$ 和 $v_2$ 是 $A$ 的真实主特征向量。\n    - 相应的近似特征值为 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$。\n    - 降阶矩阵使用这些近似值：$\\tilde{A}(\\theta) = A - \\mu(\\theta)\\, \\hat{v}_1(\\theta)\\hat{v}_1(\\theta)^\\top$。\n    - 扰动角度指定为 $\\theta \\in \\{0, 0.05, 0.2\\}$ 弧度。\n- **任务：** 对于每个矩阵和每个角度，计算绝对误差 $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$，其中 $\\tilde{\\lambda}_1(\\theta)$ 是使用幂法找到的 $\\tilde{A}(\\theta)$ 的主特征值。\n- **输出格式：** 一个包含 $(A_{\\mathrm{easy}}, \\theta=0)$, $(A_{\\mathrm{easy}}, \\theta=0.05)$, $(A_{\\mathrm{easy}}, \\theta=0.2)$, $(A_{\\mathrm{close}}, \\theta=0)$, $(A_{\\mathrm{close}}, \\theta=0.05)$, 和 $(A_{\\mathrm{close}}, \\theta=0.2)$ 的误差列表，每个值四舍五入到10位小数。\n\n**第二步：使用提取的已知信息进行验证**\n- **科学依据：** 该问题牢固地植根于数值线性代数。幂法和霍特林（Hotelling）降阶法是标准的、有充分文档记录的算法。灵敏度分析是研究数值方法的一个合理且重要的部分。\n- **适定性：** 该问题指定了所有必要的输入：矩阵、参数、初始向量以及分析步骤。对于具有唯一绝对值最大特征值的实对称矩阵（两个测试矩阵都满足此条件），幂法保证收敛。所要求的输出是一组确定的数值。\n- **客观性：** 该问题使用精确的数学语言陈述，没有歧义或主观论断。\n\n**第三步：结论与行动**\n该问题在科学上是合理的、适定的、客观的和完整的。满足了一个有效问题的所有条件。我将继续构建并呈现解决方案。\n\n### 解决方案\n\n该问题要求对主特征对仅为近似已知时一阶降阶的稳定性进行数值研究。我们将首先描述幂法和降阶的基本原理，然后概述执行指定灵敏度分析的计算步骤。\n\n**1. 幂法**\n\n幂法是一种迭代算法，用于寻找矩阵 $A$ 的具有最大绝对值的特征值 $\\lambda_1$ 及其对应的特征对 $(\\lambda_1, v_1)$。从一个不与 $v_1$ 正交的非零向量 $x_0$ 开始，该方法按以下方式进行：\n$$\ny_{k+1} = A x_k, \\qquad x_{k+1} = \\frac{y_{k+1}}{\\|y_{k+1}\\|_2}\n$$\n当 $k \\to \\infty$ 时，向量 $x_k$ 收敛到主特征向量 $v_1$（或 $-v_1$），并且由于 $x_k$ 是单位向量，瑞利商 $R(x_k) = x_k^\\top A x_k$ 收敛到主特征值 $\\lambda_1$。\n\n收敛性通过监控连续迭代中瑞利商的变化来判断。当 $|\\lambda^{(k+1)} - \\lambda^{(k)}| < \\varepsilon$ 或达到最大迭代次数 $K_{\\max}$ 时，算法终止，其中 $\\lambda^{(k)} = R(x_k)$。\n\n**2. 一阶降阶**\n\n一旦找到对称矩阵 $A$ 的主特征对 $(\\lambda_1, v_1)$，我们就可以构造一个新矩阵 $A_1$，其特征值为 $\\{0, \\lambda_2, \\lambda_3, \\dots, \\lambda_n\\}$。这通过一阶降阶（也称为霍特林（Hotelling）降阶法）实现：\n$$\nA_1 = A - \\lambda_1 v_1 v_1^\\top\n$$\n$A_1$ 的属性如下：\n- 对于特征向量 $v_1$：\n$A_1 v_1 = A v_1 - \\lambda_1 v_1 (v_1^\\top v_1) = \\lambda_1 v_1 - \\lambda_1 v_1 (1) = 0$。因此，$v_1$ 是 $A_1$ 的一个特征向量，其对应的特征值为0。\n- 对于任何其他特征向量 $v_i$（其中 $i \\ne 1$）：\n由于 $A$ 是对称的，其特征向量是正交的，所以 $v_1^\\top v_i = 0$。\n$A_1 v_i = A v_i - \\lambda_1 v_1 (v_1^\\top v_i) = \\lambda_i v_i - \\lambda_1 v_1 (0) = \\lambda_i v_i$。\n对于 $i=2, \\dots, n$ 的特征对 $(\\lambda_i, v_i)$ 得以保留。$A_1$ 的主特征值现在是 $\\lambda_2$，可以通过对 $A_1$ 应用幂法来找到。\n\n**3. 对扰动的灵敏度**\n\n在实践中，真实的特征对 $(\\lambda_1, v_1)$ 并非精确已知。我们会得到一个近似值 $(\\mu, u)$。降阶后的矩阵则为 $\\tilde{A} = A - \\mu u u^\\top$。该问题通过在主特征向量中引入一个受控的扰动来模拟这种情况。近似向量 $u$ 由前两个真实特征向量 $v_1$ 和 $v_2$ 的混合给出：\n$$\n\\hat{v}_1(\\theta) = \\cos\\theta\\, v_1 + \\sin\\theta\\, v_2\n$$\n由于 $v_1$ 和 $v_2$ 是标准正交的，$\\|\\hat{v}_1(\\theta)\\|_2 = \\sqrt{\\cos^2\\theta \\|v_1\\|_2^2 + \\sin^2\\theta \\|v_2\\|_2^2} = 1$，因此该向量已经是归一化的。角度 $\\theta$ 代表近似中的角度误差。近似特征值 $\\mu$ 是在该扰动向量上计算的瑞利商：\n$$\n\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta) = (\\cos\\theta v_1 + \\sin\\theta v_2)^\\top (\\lambda_1 \\cos\\theta v_1 + \\lambda_2 \\sin\\theta v_2) = \\lambda_1 \\cos^2\\theta + \\lambda_2 \\sin^2\\theta\n$$\n降阶后的矩阵则为 $\\tilde{A}(\\theta) = A - \\mu(\\theta) \\hat{v}_1(\\theta) \\hat{v}_1(\\theta)^\\top$。当 $\\theta=0$ 时，我们得到完美降阶，$\\tilde{A}(0)$ 的主特征值恰好是 $\\lambda_2$。对于 $\\theta > 0$，$\\tilde{A}(\\theta)$ 的特征结构受到扰动，其主特征值 $\\tilde{\\lambda}_1(\\theta)$ 将只是 $\\lambda_2$ 的一个近似值。我们预计误差 $|\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$ 会随 $\\theta$ 的增大而增大。\n\n**4. 执行计划**\n\n对于每个矩阵 $A \\in \\{A_{\\mathrm{easy}}, A_{\\mathrm{close}}\\}$ 和每个角度 $\\theta \\in \\{0, 0.05, 0.2\\}$，所需的计算将按以下步骤进行：\n1.  计算 $A$ 的完整特征系统以获得真实的特征值 $\\{\\lambda_i\\}$ 和特征向量 $\\{v_i\\}$。我们将它们排序以满足 $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots$。\n2.  提取 $\\lambda_2$、$v_1$ 和 $v_2$。\n3.  构造扰动向量 $\\hat{v}_1(\\theta) = \\cos\\theta\\, v_1 + \\sin\\theta\\, v_2$。\n4.  计算相应的瑞利商 $\\mu(\\theta) = \\hat{v}_1(\\theta)^\\top A \\hat{v}_1(\\theta)$。\n5.  构造不完美降阶的矩阵 $\\tilde{A}(\\theta) = A - \\mu(\\theta) \\hat{v}_1(\\theta) \\hat{v}_1(\\theta)^\\top$。\n6.  对 $\\tilde{A}(\\theta)$ 应用幂法，初始向量为 $x_0 = [1, 1, 1, 1]^\\top$，容差为 $\\varepsilon = 10^{-12}$，最大迭代次数为 $K_{\\max}=10000$，以找到其主特征值 $\\tilde{\\lambda}_1(\\theta)$。\n7.  计算绝对误差 $e(A,\\theta) = |\\tilde{\\lambda}_1(\\theta) - \\lambda_2|$。\n所有计算出的误差将被收集并以指定格式呈现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_method(A, x0, tol, max_iter):\n    \"\"\"\n    Implements the power method to find the dominant eigenvalue and eigenvector.\n\n    Args:\n        A (np.ndarray): The matrix.\n        x0 (np.ndarray): The initial vector.\n        tol (float): The tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the dominant eigenvalue (float) and\n               the corresponding normalized eigenvector (np.ndarray).\n    \"\"\"\n    if np.linalg.norm(x0) == 0:\n        raise ValueError(\"Initial vector x0 cannot be the zero vector.\")\n    \n    x = x0 / np.linalg.norm(x0)\n    \n    # Calculate initial Rayleigh quotient. Since x is normalized, x.T @ x = 1.\n    lambda_prev = x.T @ A @ x\n    \n    for _ in range(max_iter):\n        Ax = A @ x\n        norm_Ax = np.linalg.norm(Ax)\n        if norm_Ax == 0:\n            # This can happen if the dominant eigenvalue is 0\n            return 0.0, x\n        \n        x = Ax / norm_Ax\n        \n        lambda_curr = x.T @ A @ x\n        \n        if np.abs(lambda_curr - lambda_prev) < tol:\n            return lambda_curr, x\n            \n        lambda_prev = lambda_curr\n        \n    return lambda_curr, x\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem's test suite and print the results.\n    \"\"\"\n    # Define matrices from the problem statement\n    A_easy = np.array([\n        [4, 1, 0, 0],\n        [1, 3, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ], dtype=float)\n\n    A_close = np.array([\n        [2, 1, 0, 0],\n        [1, 2, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 2]\n    ], dtype=float)\n\n    # Define test parameters\n    test_cases = [\n        (A_easy, \"A_easy\"),\n        (A_close, \"A_close\")\n    ]\n    thetas = [0.0, 0.05, 0.2]  # Angles in radians\n    x0 = np.ones(4)\n    tolerance = 1e-12\n    max_iterations = 10000\n\n    results = []\n\n    for A, name in test_cases:\n        # 1. Compute true eigenpairs of A\n        # np.linalg.eigh returns eigenvalues in ascending order.\n        eigvals_asc, eigvecs_mat = np.linalg.eigh(A)\n        \n        # Reverse to get descending order of eigenvalues by magnitude\n        # For positive definite matrices, this is the same as descending order.\n        true_eigvals = eigvals_asc[::-1]\n        true_eigvecs = eigvecs_mat[:, ::-1]\n\n        # Extract dominant and sub-dominant pairs\n        lambda_1_true = true_eigvals[0]\n        lambda_2_true = true_eigvals[1]\n        v1_true = true_eigvecs[:, 0]\n        v2_true = true_eigvecs[:, 1]\n\n        for theta in thetas:\n            # 2. Form the perturbed vector v1_hat(theta)\n            # Since v1 and v2 are orthonormal, the result is already normalized.\n            v1_hat = np.cos(theta) * v1_true + np.sin(theta) * v2_true\n            \n            # 3. Compute the Rayleigh quotient mu(theta)\n            mu_theta = v1_hat.T @ A @ v1_hat\n            \n            # 4. Deflate the matrix A\n            A_tilde = A - mu_theta * np.outer(v1_hat, v1_hat)\n            \n            # 5. Apply power method to the deflated matrix A_tilde\n            lambda_tilde_1, _ = power_method(A_tilde, x0, tolerance, max_iterations)\n            \n            # 6. Report the absolute error\n            error = np.abs(lambda_tilde_1 - lambda_2_true)\n            results.append(error)\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}