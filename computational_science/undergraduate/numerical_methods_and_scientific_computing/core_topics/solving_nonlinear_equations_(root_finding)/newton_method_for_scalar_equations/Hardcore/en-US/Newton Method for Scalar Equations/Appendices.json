{
    "hands_on_practices": [
        {
            "introduction": "Newton's method is not just a tool for solving pre-defined equations; it can be a building block for fundamental computations. This exercise explores a classic and elegant application: computing the reciprocal $1/a$ of a number using an iterative process that, surprisingly, requires no division operations. By applying the Newton's method formula to the function $f(x) = 1/x - a$, you will derive an iteration that relies only on multiplication and subtraction, a technique historically significant for computer hardware design .",
            "id": "3255024",
            "problem": "Let $a \\neq 0$ be a fixed real constant, and consider computing the reciprocal $1/a$ by solving the scalar root-finding problem $f(x) = 1/x - a = 0$ with Newton's method. Using only the core definition of Newton's method for a scalar equation and basic algebra, derive an iteration update that does not require any explicit division in its evaluation. Your derivation should start from the general definition of Newton's method and proceed by symbolic manipulation until the division operations cancel. Assume exact arithmetic and do not introduce approximations. Provide the final update mapping $T(x)$ as a single closed-form expression containing only multiplication and addition/subtraction. The final answer must be this single expression.",
            "solution": "The fundamental base is the definition of Newton's method for a scalar equation. Given a differentiable function $f(x)$, the Newton iteration for approximating a root $x^{\\ast}$ of $f(x) = 0$ is\n$$\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}.\n$$\nWe apply this to the function $f(x) = \\frac{1}{x} - a$, where $a \\neq 0$ is a real constant. First compute the derivative:\n$$\nf'(x) = -\\frac{1}{x^{2}}.\n$$\nSubstitute $f(x_k)$ and $f'(x_k)$ into the Newton update:\n$$\nx_{k+1} = x_k - \\frac{\\frac{1}{x_k} - a}{-\\frac{1}{x_k^{2}}}.\n$$\nWe now simplify the quotient symbolically to eliminate explicit division in the final form. Multiply the numerator and the denominator:\n$$\n\\frac{\\frac{1}{x_k} - a}{-\\frac{1}{x_k^{2}}} = \\left(\\frac{1}{x_k} - a\\right)\\left(-x_k^{2}\\right) = -x_k + a x_k^{2}.\n$$\nTherefore,\n$$\nx_{k+1} = x_k - \\left(-x_k + a x_k^{2}\\right) = x_k + x_k - a x_k^{2} = x_k\\left(2 - a x_k\\right).\n$$\nThis expression involves only multiplication and addition/subtraction and contains no explicit division operations. Hence the iteration can be implemented without division once this closed form has been derived.\n\nFor completeness, we verify that this iteration indeed targets the reciprocal. Let $x^{\\ast} = \\frac{1}{a}$ be the exact root of $f(x) = 0$. Define the error $e_k = x_k - x^{\\ast}$. Using the update $x_{k+1} = x_k(2 - a x_k)$,\n$$\ne_{k+1} = x_k(2 - a x_k) - \\frac{1}{a}.\n$$\nSubstitute $x_k = \\frac{1}{a} + e_k$:\n$$\ne_{k+1} = \\left(\\frac{1}{a} + e_k\\right)\\left(2 - a\\left(\\frac{1}{a} + e_k\\right)\\right) - \\frac{1}{a}\n= \\left(\\frac{1}{a} + e_k\\right)\\left(2 - 1 - a e_k\\right) - \\frac{1}{a}\n= \\left(\\frac{1}{a} + e_k\\right)\\left(1 - a e_k\\right) - \\frac{1}{a}.\n$$\nExpand:\n$$\ne_{k+1} = \\frac{1}{a} - e_k - a e_k^{2} - \\frac{1}{a} = -a e_k^{2}.\n$$\nThus, the method exhibits quadratic convergence near $x^{\\ast}$, and the iteration form is consistent with Newton's method while requiring only multiplication and addition/subtraction in evaluation. The requested update mapping $T(x)$ is therefore\n$$\nT(x) = x\\left(2 - a x\\right).\n$$",
            "answer": "$$\\boxed{x\\left(2 - a x\\right)}$$"
        },
        {
            "introduction": "While Newton's method boasts rapid quadratic convergence, this speed is only guaranteed near a root. Far from a solution, the method can produce wildly inaccurate steps, a behavior known as \"overshooting,\" which can lead to slow convergence or outright divergence. This practice uses the function $f(x) = \\arctan(x)$ as a canonical example to demonstrate this failure mode and introduces a powerful solution: the damped Newton's method with a backtracking line search, a fundamental technique for creating robust, globally convergent algorithms .",
            "id": "3255187",
            "problem": "Consider the scalar nonlinear equation defined by the function $f(x)=\\arctan(x)$, where $\\arctan(\\cdot)$ returns angles in radians. Your task is to analyze why a direct application of the classical Newton iteration can exhibit an \"overshooting\" behavior for large $|x|$, and to implement a globally convergent damped Newton algorithm that avoids this pitfall.\n\nStart from the following fundamental base: the first-order Taylor expansion of a differentiable function $f(x)$ about a point $x$, namely $f(x+s)\\approx f(x)+f^{\\prime}(x)\\,s$, and the principle that a root-finding iteration attempts to choose an increment $s$ so that $f(x+s)=0$ is approximated by solving a local linear model. Use this base to derive the undamped Newton search direction in a way that does not rely on any pre-quoted shortcut formulas. Then analyze, for $f(x)=\\arctan(x)$, how this direction behaves asymptotically when $|x|$ is large, and explain why “overshooting” (very large steps that move far past the root) can occur.\n\nDesign and implement two iterative solvers for $f(x)=\\arctan(x)$:\n\n- An undamped Newton method that applies the derived search direction with full step length, a stopping tolerance $\\varepsilon=10^{-12}$ on the residual $|f(x)|$, and a maximum of $50$ iterations. Treat the iteration as failed if any iterate becomes non-finite or if $|x|$ exceeds $10^{16}$ at any point.\n\n- A damped Newton method that uses the same search direction but chooses a step length $\\lambda\\in(0,1]$ by a backtracking line search (Backtracking Line Search (BLS)) to enforce sufficient decrease in the merit function $\\phi(x)=\\tfrac{1}{2}f(x)^{2}$. Use the sufficient decrease (Armijo) condition\n$$\n\\phi(x+\\lambda p)\\le \\phi(x)-2c\\,\\lambda\\,\\phi(x),\n$$\nwith $c=10^{-4}$, geometric backtracking factor $\\beta=\\tfrac{1}{2}$, and initial trial step length $\\lambda=1$. If the line search fails to find a step within $100$ backtracking reductions or if any candidate iterate becomes non-finite or satisfies $|x|10^{16}$, declare failure. Use the same residual tolerance $\\varepsilon=10^{-12}$ and maximum outer iterations $50$ as for the undamped method.\n\nTest Suite and Output Specification:\n\n- Use the starting values $x_{0}\\in\\{0,1,10,10^{3},-10^{3}\\}$.\n\n- For each starting value, run the undamped and damped solvers. Report, for each solver, the integer number of iterations required to reach $|f(x)|\\le \\varepsilon$. If a solver fails under the criteria above, report $-1$ for that solver on that starting value.\n\n- The final program output must be a single line containing a comma-separated list enclosed in square brackets that aggregates the results in the following order:\n$[k_{\\text{undamped}}(0),k_{\\text{damped}}(0),k_{\\text{undamped}}(1),k_{\\text{damped}}(1),k_{\\text{undamped}}(10),k_{\\text{damped}}(10),k_{\\text{undamped}}(10^{3}),k_{\\text{damped}}(10^{3}),k_{\\text{undamped}}(-10^{3}),k_{\\text{damped}}(-10^{3})]$,\nwhere $k_{\\text{undamped}}(x_{0})$ and $k_{\\text{damped}}(x_{0})$ denote the iteration counts as defined above for the corresponding starting value $x_{0}$. Each entry must be an integer.\n\nYour implementation must be fully self-contained. No user input is permitted. The final line printed by your program must be exactly in the specified list format.",
            "solution": "We begin from the first-order Taylor model. For a differentiable scalar function $f(x)$ and a current iterate $x$, a small increment $s$ satisfies\n$$\nf(x+s)\\approx f(x)+f^{\\prime}(x)\\,s.\n$$\nTo seek a root, we impose the model equation $f(x)+f^{\\prime}(x)\\,s=0$ and solve for $s$, which yields the undamped Newton search direction\n$$\np=-\\frac{f(x)}{f^{\\prime}(x)}.\n$$\nThe corresponding undamped iteration with full step length is $x_{+}=x+p$.\n\nFor the specific function $f(x)=\\arctan(x)$, we have $f^{\\prime}(x)=\\dfrac{1}{1+x^{2}}$. The undamped Newton direction thus takes the form\n$$\np=-\\frac{\\arctan(x)}{\\tfrac{1}{1+x^{2}}}=-(1+x^{2})\\,\\arctan(x).\n$$\nTo understand overshooting, consider the asymptotics of $\\arctan(x)$ when $|x|$ is large. For $x\\to+\\infty$, $\\arctan(x)\\to \\frac{\\pi}{2}$, and for $x\\to-\\infty$, $\\arctan(x)\\to -\\frac{\\pi}{2}$. More precisely, one can use the expansion $\\arctan(x)=\\operatorname{sgn}(x)\\,\\frac{\\pi}{2}-\\frac{1}{x}+O(x^{-3})$ as $|x|\\to\\infty$. Combining this with $f^{\\prime}(x)=\\dfrac{1}{1+x^{2}}$ yields, for large $|x|$,\n$$\np\\approx -\\left(1+x^{2}\\right)\\left(\\operatorname{sgn}(x)\\,\\frac{\\pi}{2}\\right)\\sim -\\operatorname{sgn}(x)\\,\\frac{\\pi}{2}\\,x^{2}.\n$$\nHence, the full-step update $x_{+}=x+p$ behaves like\n$$\nx_{+}\\sim x-\\operatorname{sgn}(x)\\,\\frac{\\pi}{2}\\,x^{2},\n$$\nwhose magnitude grows quadratically in $|x|$. This produces very large jumps across the root at $x=0$ and typically leads to iterates that alternate signs while growing in magnitude, a hallmark of “overshooting” and divergence for the undamped method started far from the root.\n\nTo obtain a globally convergent method, we retain the Newton direction $p$ but damp the step using a line search on a merit function. A standard choice is the squared residual merit function\n$$\n\\phi(x)=\\frac{1}{2}\\,f(x)^{2}.\n$$\nIts derivative is $\\phi^{\\prime}(x)=f(x)\\,f^{\\prime}(x)$. The directional derivative along the Newton direction $p$ is\n$$\n\\phi^{\\prime}(x)\\,p=f(x)\\,f^{\\prime}(x)\\,\\left(-\\frac{f(x)}{f^{\\prime}(x)}\\right)=-f(x)^{2}=-2\\,\\phi(x)0\n$$\nwhenever $f(x)\\neq 0$, so $p$ is a strict descent direction for $\\phi$. A backtracking line search (BLS) ensures sufficient decrease by choosing a step length $\\lambda\\in(0,1]$ such that the Armijo condition holds:\n$$\n\\phi(x+\\lambda p)\\le \\phi(x)+c\\,\\lambda\\,\\phi^{\\prime}(x)\\,p=\\phi(x)-c\\,\\lambda\\,f(x)^{2}=\\phi(x)-2c\\,\\lambda\\,\\phi(x),\n$$\nwith $c\\in(0,1/2)$. Starting from $\\lambda=1$ and shrinking $\\lambda\\leftarrow\\beta\\,\\lambda$ with $\\beta\\in(0,1)$ until the inequality is satisfied yields a monotone decrease sequence $\\{\\phi(x_{k})\\}$ bounded below by $0$, guaranteeing convergence of the merit values and providing a robust globalization of Newton’s method. For $f(x)=\\arctan(x)$, this damping avoids the quadratically large steps by reducing $\\lambda$ drastically when $|x|$ is large, driving the iterates toward the root.\n\nAlgorithmic design:\n\n- Undamped method: initialize with $x_{0}$ and iterate $x_{k+1}=x_{k}+p_{k}$, where $p_{k}=-\\dfrac{f(x_{k})}{f^{\\prime}(x_{k})}$, until $|f(x_{k})|\\le \\varepsilon$ or $k$ reaches the maximum. Declare failure if any $x_{k}$ is non-finite or $|x_{k}|10^{16}$.\n\n- Damped method: at each iteration, compute the same $p_{k}$ and perform a backtracking search starting from $\\lambda=1$ with $\\beta=\\tfrac{1}{2}$ and $c=10^{-4}$, accepting the first $\\lambda$ that satisfies\n$$\n\\phi(x_{k}+\\lambda p_{k})\\le \\phi(x_{k})-2c\\,\\lambda\\,\\phi(x_{k}).\n$$\nUpdate $x_{k+1}=x_{k}+\\lambda p_{k}$. Use the same stopping and failure conditions, and limit backtracking to $100$ reductions.\n\nTest suite:\n\n- Starting points $x_{0}\\in\\{0,1,10,10^{3},-10^{3}\\}$ probe a trivial exact root, a moderate distance case, and severe large-magnitude cases where undamped Newton overshoots.\n\nExpected qualitative outcomes:\n\n- For $x_{0}=0$, both methods terminate immediately with $0$ iterations since $f(0)=0$.\n\n- For $x_{0}=1$, the undamped method converges rapidly in a few iterations; the damped method typically accepts full steps near the solution and matches the iteration count or is close.\n\n- For $x_{0}=10$, $x_{0}=10^{3}$, and $x_{0}=-10^{3}$, the undamped method exhibits overshooting and fails under the given safeguards, while the damped method backtracks to control step length and converges within the allowed iterations.\n\nThe program must print the single-line list\n$[k_{\\text{undamped}}(0),k_{\\text{damped}}(0),k_{\\text{undamped}}(1),k_{\\text{damped}}(1),k_{\\text{undamped}}(10),k_{\\text{damped}}(10),k_{\\text{undamped}}(10^{3}),k_{\\text{damped}}(10^{3}),k_{\\text{undamped}}(-10^{3}),k_{\\text{damped}}(-10^{3})]$,\nwith each $k$ an integer equal to the iteration count on convergence, or $-1$ on failure as defined.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(x: float) - float:\n    # f(x) = arctan(x)\n    return float(np.arctan(x))\n\ndef fprime(x: float) - float:\n    # f'(x) = 1 / (1 + x^2)\n    return 1.0 / (1.0 + x * x)\n\ndef phi(x: float) - float:\n    # phi(x) = 0.5 * f(x)^2\n    fx = f(x)\n    return 0.5 * fx * fx\n\ndef is_finite_scalar(x: float) - bool:\n    return np.isfinite(x)\n\ndef newton_undamped(x0: float, tol: float = 1e-12, max_iter: int = 50,\n                    blowup: float = 1e16) - int:\n    # Return iteration count to reach |f(x)| = tol, else -1 on failure\n    x = float(x0)\n    fx = f(x)\n    if not is_finite_scalar(x) or not is_finite_scalar(fx):\n        return -1\n    if abs(fx) = tol:\n        return 0\n    for k in range(1, max_iter + 1):\n        fpx = fprime(x)\n        # For arctan, f'(x)  0 always; check safety anyway.\n        if not is_finite_scalar(fpx) or fpx == 0.0:\n            return -1\n        step = -fx / fpx\n        x_new = x + step\n        if not is_finite_scalar(x_new) or abs(x_new)  blowup:\n            return -1\n        x = x_new\n        fx = f(x)\n        if not is_finite_scalar(fx):\n            return -1\n        if abs(fx) = tol:\n            return k\n    return -1\n\ndef newton_damped(x0: float, tol: float = 1e-12, max_iter: int = 50,\n                  c: float = 1e-4, beta: float = 0.5, max_backtrack: int = 100,\n                  blowup: float = 1e16) - int:\n    # Return iteration count to reach |f(x)| = tol, else -1 on failure\n    x = float(x0)\n    fx = f(x)\n    if not is_finite_scalar(x) or not is_finite_scalar(fx):\n        return -1\n    if abs(fx) = tol:\n        return 0\n    for k in range(1, max_iter + 1):\n        fpx = fprime(x)\n        if not is_finite_scalar(fpx) or fpx == 0.0:\n            return -1\n        p = -fx / fpx\n        phi_x = 0.5 * fx * fx\n        lam = 1.0\n        accepted = False\n        for _ in range(max_backtrack):\n            x_trial = x + lam * p\n            if not is_finite_scalar(x_trial) or abs(x_trial)  blowup:\n                lam *= beta\n                continue\n            phi_trial = phi(x_trial)\n            # Armijo: phi(x + lam p) = phi(x) - 2 c lam phi(x)\n            if phi_trial = phi_x - 2.0 * c * lam * phi_x:\n                x = x_trial\n                fx = f(x)\n                accepted = True\n                break\n            lam *= beta\n        if not accepted:\n            return -1\n        if not is_finite_scalar(fx):\n            return -1\n        if abs(fx) = tol:\n            return k\n    return -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [0.0, 1.0, 10.0, 1.0e3, -1.0e3]\n\n    results = []\n    for x0 in test_cases:\n        ku = newton_undamped(x0, tol=1e-12, max_iter=50, blowup=1e16)\n        kd = newton_damped(x0, tol=1e-12, max_iter=50, c=1e-4, beta=0.5,\n                           max_backtrack=100, blowup=1e16)\n        results.append(int(ku))\n        results.append(int(kd))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Robustness is a paramount concern in numerical software. While the damped method from the previous exercise improves global convergence, an even more reliable strategy is to create a hybrid algorithm that combines the speed of Newton's method with the guaranteed convergence of a bracketing method. This exercise guides you through implementing a \"safeguarded\" Newton's method, where each step is constrained to remain within an interval $[a, b]$ known to contain the root, effectively preventing any possibility of divergence .",
            "id": "3255029",
            "problem": "Consider the scalar root-finding problem defined by the function $f(x) = \\cos(x) - x$, with derivative $f'(x) = -\\sin(x) - 1$. The goal is to approximate a solution $x^\\star$ to $f(x^\\star) = 0$ by designing an algorithm that adheres to the following principle-based constraints. Begin from the fundamental base that the solution of a nonlinear equation $f(x) = 0$ can be approached by iterative methods derived from local linearization of $f$ near a current iterate, and that enclosing an unknown root within a bracket $[a,b]$ satisfying $f(a) \\cdot f(b)  0$ provides a safeguard against divergence.\n\nConstruct a combined bracketing and Newton procedure for scalar equations based on the following requirements. At each iteration, derive an update by linearly approximating $f$ around the current iterate $x_n$ and enforcing the next iterate $x_{n+1}$ to be the zero of that local approximation. Whenever the computed $x_{n+1}$ leaves the current bracketing interval $[a,b]$, project it back onto $[a,b]$ by clipping, ensuring $x_{n+1} \\in [a,b]$. After computing $x_{n+1}$, update the bracket by replacing either $a$ or $b$ so that the invariant $f(a) \\cdot f(b)  0$ is maintained, with the interval monotonically shrinking around the root. Design termination conditions based on function residual, step size, and bracket width, each compared to a given tolerance.\n\nAngles must be interpreted in radians. There are no physical units in this task. Your program must implement this safeguarded iteration specifically for $f(x) = \\cos(x) - x$ and $f'(x) = -\\sin(x) - 1$, and must handle any initial guess $x_0$ by first projecting $x_0$ onto the provided bracket $[a,b]$.\n\nYour program must run the algorithm on the following test suite, producing one floating-point output per test case:\n- Test case $1$: $a = 0$, $b = 1$, $x_0 = 0.5$, $\\text{tol} = 10^{-12}$, $\\text{max\\_iter} = 50$.\n- Test case $2$: $a = 0$, $b = 2$, $x_0 = 2.0$, $\\text{tol} = 10^{-12}$, $\\text{max\\_iter} = 50$.\n- Test case $3$: $a = 0.7$, $b = 0.8$, $x_0 = 10.0$, $\\text{tol} = 10^{-12}$, $\\text{max\\_iter} = 50$.\n- Test case $4$: $a = 0.73$, $b = 0.75$, $x_0 = 0.75$, $\\text{tol} = 10^{-12}$, $\\text{max\\_iter} = 50$.\n\nEach test case must return a single floating-point approximation to the root, rounded to $12$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[0.739085133215,0.739085133215,0.739085133215,0.739085133215]$.",
            "solution": "The problem requires the construction and implementation of a safeguarded root-finding algorithm for a scalar equation $f(x)=0$. The algorithm combines Newton's method, which is derived from local linearization, with a bracketing safeguard that prevents divergence and guarantees convergence.\n\nThe core of the iterative process is Newton's method. For a given function $f(x)$, we approximate it near a current iterate $x_n$ using a first-order Taylor expansion, which represents the tangent line to the function at that point:\n$$L(x) = f(x_n) + f'(x_n)(x - x_n)$$\nThe next iterate, $x_{n+1}$, is chosen as the root of this linear approximation, i.e., where $L(x_{n+1}) = 0$. Solving for $x_{n+1}$ yields the standard Newton's method update rule:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\nFor the specific function $f(x) = \\cos(x) - x$ and its derivative $f'(x) = -\\sin(x) - 1$, this step becomes:\n$$x_{n+1} = x_n - \\frac{\\cos(x_n) - x_n}{-\\sin(x_n) - 1}$$\n\nTo address the potential for Newton's method to produce iterates that diverge or wander far from the root, a safeguarding mechanism based on a bracketing interval is incorporated. The algorithm maintains an interval $[a_n, b_n]$ that is known to contain the root, which is guaranteed by the invariant $f(a_n) \\cdot f(b_n)  0$. This is based on the Intermediate Value Theorem.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: Given an initial bracket $[a, b]$ and an initial guess $x_0$. The first iterate is established by projecting $x_0$ onto the initial bracket, ensuring it lies within the valid range: $x_0 \\leftarrow \\max(a, \\min(b, x_0))$. The iteration counter starts at $n=0$. The initial bracketing interval for the loop is $[a_0, b_0] = [a, b]$.\n\n2.  **Iteration**: For each iteration $n=0, 1, 2, \\dots$ up to a maximum of $\\text{max\\_iter}$:\n    a.  A standard Newton step is calculated from the current iterate $x_n$ to find a candidate for the next iterate, let's call it $x_{newton} = x_n - f(x_n)/f'(x_n)$.\n    b.  **Safeguard 1 (Clipping)**: This candidate is checked against the current bracketing interval $[a_n, b_n]$. If $x_{newton}$ falls outside this interval, it is \"clipped\" or projected back to the nearest endpoint. This ensures the next iterate $x_{n+1}$ is always within the known bounds of the root: $x_{n+1} = \\max(a_n, \\min(b_n, x_{newton}))$. This step prevents the large, non-productive jumps that can lead to divergence.\n    c.  **Safeguard 2 (Bracket Update)**: The bracketing interval is shrunk to maintain the invariant and close in on the root. The new point $x_{n+1}$ is used to form a new, smaller subinterval. By evaluating the sign of $f(x_{n+1})$, we decide which endpoint to replace:\n        - If $f(a_n) \\cdot f(x_{n+1})  0$, the root lies in $[a_n, x_{n+1}]$, so the new bracket becomes $[a_{n+1}, b_{n+1}] = [a_n, x_{n+1}]$.\n        - Otherwise (since $f(a_n)$ and $f(b_n)$ have opposite signs), it must be that $f(x_{n+1}) \\cdot f(b_n)  0$, so the root lies in $[x_{n+1}, b_n]$, and the new bracket becomes $[a_{n+1}, b_{n+1}] = [x_{n+1}, b_n]$.\n        This process guarantees that the width of the bracketing interval $|b_n - a_n|$ is monotonically non-increasing.\n    d.  **Update**: The current iterate is updated for the next cycle: $x_n \\leftarrow x_{n+1}$.\n\n3.  **Termination**: The loop terminates if any of the following conditions are met, where $\\text{tol}$ is a specified tolerance:\n    - The function residual is sufficiently small: $|f(x_n)|  \\text{tol}$.\n    - The step size is sufficiently small: $|x_{n+1} - x_n|  \\text{tol}$. This indicates convergence to a point.\n    - The bracketing interval is sufficiently small: $|b_n - a_n|  \\text{tol}$. This guarantees the root is located to within the desired precision.\n    - The maximum number of iterations, $\\text{max\\_iter}$, is reached. This prevents an infinite loop in cases where convergence is not achieved.\n\nThe final returned value is the last computed iterate, which represents the algorithm's best approximation of the root $x^\\star$. The implementation applies this logic to the given test cases, using the provided parameters for the initial bracket $[a, b]$, initial guess $x_0$, tolerance $\\text{tol}$, and maximum iterations $\\text{max\\_iter}$. All calculations involving angles are performed in radians as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    \n    def safeguarded_newton(a, b, x0, tol, max_iter):\n        \"\"\"\n        Implements the safeguarded Newton's method as described in the problem.\n\n        Args:\n            a (float): The lower bound of the initial bracketing interval.\n            b (float): The upper bound of the initial bracketing interval.\n            x0 (float): The initial guess for the root.\n            tol (float): The tolerance for termination conditions.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            float: The approximated root.\n        \"\"\"\n        # Define the specific function and its derivative for the problem.\n        def f(x):\n            return np.cos(x) - x\n\n        def f_prime(x):\n            return -np.sin(x) - 1\n\n        # Initialize the bracketing interval.\n        a_n, b_n = a, b\n        \n        # Per instruction, project the initial guess onto the provided bracket.\n        x_n = np.clip(x0, a_n, b_n)\n\n        for _ in range(max_iter):\n            fx = f(x_n)\n            \n            # Termination condition 1: function residual is close to zero.\n            if abs(fx)  tol:\n                return x_n\n\n            fpx = f_prime(x_n)\n            # A general-purpose solver would check if fpx is near zero.\n            # For f'(x) = -sin(x) - 1, the derivative is never zero in the\n            # region of interest, so we can safely divide.\n            \n            # Calculate the Newton step.\n            x_newton = x_n - fx / fpx\n            \n            # Safeguard 1: Project/clip the Newton step into the current bracket.\n            x_next = np.clip(x_newton, a_n, b_n)\n\n            # Termination condition 2: step size is sufficiently small.\n            if abs(x_next - x_n)  tol:\n                return x_next\n            \n            # Safeguard 2: Update the bracketing interval.\n            f_next = f(x_next)\n            \n            # To ensure the logic works correctly, we check against one\n            # endpoint. The other case is handled by the else clause,\n            # leveraging the invariant f(a_n) * f(b_n)  0.\n            if f(a_n) * f_next  0:\n                b_n = x_next\n            else:\n                a_n = x_next\n            \n            # Termination condition 3: bracket width is sufficiently small.\n            if abs(b_n - a_n)  tol:\n                return x_next\n                \n            # Update the iterate for the next step.\n            x_n = x_next\n            \n        # If max_iter is reached, return the last computed iterate as the best guess.\n        return x_n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, b, x0, tol, max_iter)\n        (0.0, 1.0, 0.5, 1e-12, 50),\n        (0.0, 2.0, 2.0, 1e-12, 50),\n        (0.7, 0.8, 10.0, 1e-12, 50),\n        (0.73, 0.75, 0.75, 1e-12, 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, x0, tol, max_iter = case\n        root = safeguarded_newton(a, b, x0, tol, max_iter)\n        results.append(root)\n\n    # Final print statement in the exact required format.\n    # The f-string format specifier ':.12f' handles rounding to 12 decimal places.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}