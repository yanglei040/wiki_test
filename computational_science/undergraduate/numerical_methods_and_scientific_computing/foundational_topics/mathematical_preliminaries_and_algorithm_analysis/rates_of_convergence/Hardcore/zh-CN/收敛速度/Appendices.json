{
    "hands_on_practices": [
        {
            "introduction": "牛顿法因其二次收敛速度而闻名。然而，这种快速收敛仅在求解*单根*时才有保证，当根的重数大于一时，其收敛速度会降至线性。 这个练习提供了一个动手验证该理论结果的机会，通过实现牛顿法和一个经验收敛阶估计器，你将从数值上观察并量化这种性能退化，从而连接理论与计算现实。",
            "id": "3265302",
            "problem": "你需要构造并分析一个由于根的重数导致牛顿法仅呈线性收敛的案例。请从收敛速度和牛顿法的基本定义出发。你必须实现该算法，并使用精确的数值指标来量化观察到的行为。\n\n用作基础的基本定义：\n- 如果 $f(\\alpha)=0$, $f'(\\alpha)=0$, $\\dots$, $f^{(m-1)}(\\alpha)=0$ 且 $f^{(m)}(\\alpha)\\neq 0$，则称点 $\\alpha \\in \\mathbb{R}$ 是函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 的一个 $m \\in \\mathbb{N}$ 重根。\n- 对于一个可微函数 $f$，逼近其单根的牛顿法由迭代式 $x_{k+1} = x_k - \\dfrac{f(x_k)}{f'(x_k)}$ 定义，其中 $k \\ge 0$。\n- 给定一个收敛于 $\\alpha$ 的序列 $\\{x_k\\}$，定义误差 $e_k = |x_k - \\alpha|$。如果存在一个常数 $c \\in (0,1)$ 使得 $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k} = c$，则称该方法是线性收敛的；如果对于某个 $\\lambda \\in (0,\\infty)$，有 $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k^p} = \\lambda$，则称其为 $p0$ 阶收敛。收敛阶的一个经验估计量是\n$$\np_k = \\dfrac{\\ln\\!\\left(\\dfrac{e_{k+1}}{e_k}\\right)}{\\ln\\!\\left(\\dfrac{e_k}{e_{k-1}}\\right)} \\quad \\text{for } k \\ge 1,\n$$\n其中 $\\ln$ 表示自然对数。\n\n任务：\n1) 提供一个具有3重根的显式函数 $f$。根据上述定义证明其重数。\n2) 对函数 $f$ 实现牛顿法，并对几个初始猜测值计算误差序列 $e_k$。使用这些误差来估计：\n   - 随着 $k$ 增大时的渐近误差比 $r_k = \\dfrac{e_{k+1}}{e_k}$。\n   - 如上定义的经验阶 $p_k$。\n3) 通过证明 $r_k$ 稳定在严格介于 $0$ 和 $1$ 之间的一个常数附近，并且 $p_k$ 稳定在 $1$ 附近，来数值上证明牛顿法对于三重根仅是线性收敛的。\n\n为保证可复现性和评估，请使用以下参数集测试套件。在每种情况下，都给定了函数 $f$、其导数 $f'$、真根 $\\alpha$、初始猜测值 $x_0$ 和固定的迭代次数 $N$：\n- 测试 A (精确因式分解情况): $f(x) = (x-1)^3$, $f'(x) = 3(x-1)^2$, $\\alpha = 1$, $x_0 = 2$, $N = 6$。\n- 测试 B (非平凡因子情况): $f(x) = (x-1)^3 (x^2+1)$, $f'(x) = 3(x-1)^2 (x^2+1) + (x-1)^3 (2x)$, $\\alpha = 1$, $x_0 = 1.7$, $N = 10$。\n- 测试 C (解析非多项式因子情况): $f(x) = (x+0.5)^3 e^x$, $f'(x) = e^x\\left(3(x+0.5)^2 + (x+0.5)^3\\right)$, $\\alpha = -0.5$, $x_0 = 0$, $N = 12$。\n\n计算与输出要求：\n- 对于每个测试用例，从 $x_0$ 开始精确运行 $N$ 次牛顿法迭代，收集误差序列 $e_k = |x_k - \\alpha|$（其中 $k=0,1,\\dots,N$）。计算最后一个比率 $r_{N-1} = \\dfrac{e_N}{e_{N-1}}$ 和最后一个阶估计 $p_{N-1} = \\dfrac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$。\n- 你的程序应输出单行，包含一个逗号分隔的扁平列表，其值为 $[r_A, p_A, r_B, p_B, r_C, p_C]$，其中 $r_{\\cdot}$ 和 $p_{\\cdot}$ 分别是每个测试用例的最终比率和最终阶估计。\n- 将所有数字作为浮点值输出。为了清晰和可复现性，在打印前将每个值四舍五入到6位小数。\n- 不涉及物理单位，也不使用角度。\n\n你的程序必须是一个完整、可运行的程序，能够使用指定的测试套件执行所有计算，并生成所需的单行输出格式：一个单行列表，如 $[r_A,p_A,r_B,p_B,r_C,p_C]$。",
            "solution": "该问题要求对应用于具有大于一重数根的函数的牛顿法进行理论和数值分析，特别是针对 $m=3$ 的重数。我们将首先为预期的线性收敛建立理论基础，然后验证一个样本函数的根的重数，最后设计并执行一个数值实验来量化所提供测试用例的收敛行为。\n\n**1. 收敛性理论分析**\n\n牛顿法是一个由 $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$ 定义的迭代格式。这可以表示为不动点迭代 $x_{k+1} = g(x_k)$，其中迭代函数为 $g(x) = x - \\frac{f(x)}{f'(x)}$。在根 $\\alpha$ 附近的收敛行为由 $g(x)$ 在 $\\alpha$ 处的导数决定。如果 $|g'(\\alpha)|  1$，则迭代在局部收敛于 $\\alpha$。如果 $g'(\\alpha) \\neq 0$，收敛速度是线性的；如果 $g'(\\alpha) = 0$，收敛速度至少是二次的。\n\n设 $\\alpha$ 是一个重数为 $m \\in \\mathbb{N}$ 且 $m > 1$ 的根。根据定义，函数 $f(x)$ 可以写成 $f(x) = (x-\\alpha)^m h(x)$ 的形式，其中 $h(x)$ 是一个满足 $h(\\alpha) \\neq 0$ 的函数。对 $f(x)$ 求导可得：\n$$\nf'(x) = m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)\n$$\n将这些代入 $g(x)$ 的表达式中：\n$$\ng(x) = x - \\frac{(x-\\alpha)^m h(x)}{m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)} = x - \\frac{(x-\\alpha)h(x)}{m h(x) + (x-\\alpha)h'(x)}\n$$\n为了找到收敛速度，我们计算 $g'(x)$：\n$$\ng'(x) = 1 - \\frac{[h(x) + (x-\\alpha)h'(x)][m h(x) + (x-\\alpha)h'(x)] - [(x-\\alpha)h(x)][m h'(x) + h'(x) + (x-\\alpha)h''(x)]}{[m h(x) + (x-\\alpha)h'(x)]^2}\n$$\n计算当 $x \\to \\alpha$ 时的极限：\n$$\n\\lim_{x\\to\\alpha} g'(x) = g'(\\alpha) = 1 - \\frac{[h(\\alpha) + 0][m h(\\alpha) + 0] - [0]}{[m h(\\alpha) + 0]^2} = 1 - \\frac{m h(\\alpha)^2}{m^2 h(\\alpha)^2} = 1 - \\frac{1}{m}\n$$\n因此，对于一个重数 $m>1$ 的根，牛顿法是线性收敛的，其渐近误差比为 $c = |g'(\\alpha)| = |1 - 1/m|$。由于 $m > 1$，我们有 $0  c  1$。\n\n在这个问题中，我们特别关注重数为 $m=3$ 的根。因此，理论上的渐近误差比是：\n$$\nc = 1 - \\frac{1}{3} = \\frac{2}{3}\n$$\n收敛阶为 $p=1$，表明是线性收敛。\n\n**2. 根的重数证明**\n\n问题要求提供一个具有3重根的函数并加以证明。我们将使用测试A中的函数：$f(x) = (x-1)^3$。根是 $\\alpha=1$。根据所给定义，我们必须验证 $f(\\alpha)=f'(\\alpha)=f''(\\alpha)=0$ 和 $f'''(\\alpha)\\neq 0$。\n\n该函数及其逐次导数是：\n- $f(x) = (x-1)^3$\n- $f'(x) = 3(x-1)^2$\n- $f''(x) = 6(x-1)$\n- $f'''(x) = 6$\n\n在根 $\\alpha=1$ 处对这些导数求值：\n- $f(1) = (1-1)^3 = 0$\n- $f'(1) = 3(1-1)^2 = 0$\n- $f''(1) = 6(1-1) = 0$\n- $f'''(1) = 6 \\neq 0$\n\n由于在 $\\alpha=1$ 处前两个导数为零，而三阶导数非零，这证实了 $\\alpha=1$ 是 $f(x)$ 的一个重数为 $m=3$ 的根。其他给定的函数也可以类似地验证，因为它们的构造形式为 $(x-\\alpha)^3 h(x)$，其中 $h(\\alpha) \\neq 0$。\n\n**3. 数值实验与预期结果**\n\n我们将为三个测试用例（A、B、C）中的每一个实现牛顿法。对于每个用例，我们从一个初始猜测值 $x_0$ 开始，迭代 $N$ 次。过程如下：\n1. 初始化 $k=0$ 时 $x_k = x_0$。一个数组将存储 $k=0, 1, \\dots, N$ 的误差 $e_k = |x_k - \\alpha|$。\n2. 对于从 $0$ 到 $N-1$ 的 $k$，计算下一个迭代值：$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$。\n3. 计算并存储相应的误差 $e_{k+1} = |x_{k+1} - \\alpha|$。\n4. 经过 $N$ 次迭代后，可获得误差序列 $e_0, e_1, \\dots, e_N$。\n5. 计算最终的渐近误差比估计值：$r_{N-1} = \\frac{e_N}{e_{N-1}}$。\n6. 计算最终的经验收敛阶：$p_{N-1} = \\frac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$。\n\n根据我们的理论分析，数值结果应显示，对于每个测试用例，$r_{N-1}$ 的值接近理论极限 $2/3 \\approx 0.666667$，而 $p_{N-1}$ 的值接近 $1$。这将按要求从数值上证明牛顿法对三重根表现出线性收敛。最终的实现将执行这些计算并按规定格式化输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes cases where Newton’s method exhibits linear convergence\n    due to a root of multiplicity 3. The implementation runs three test cases\n    and computes the final asymptotic error ratio and empirical order of convergence.\n    \"\"\"\n    \n    # Test A (exact factorization case)\n    # f(x) = (x-1)^3\n    # f'(x) = 3(x-1)^2\n    # alpha = 1, x_0 = 2, N = 6\n    case_A = {\n        \"f\": lambda x: (x - 1.0)**3,\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2,\n        \"alpha\": 1.0,\n        \"x0\": 2.0,\n        \"N\": 6,\n    }\n\n    # Test B (nontrivial factor case)\n    # f(x) = (x-1)^3 * (x^2+1)\n    # f'(x) = 3(x-1)^2 * (x^2+1) + (x-1)^3 * (2x)\n    # alpha = 1, x_0 = 1.7, N = 10\n    case_B = {\n        \"f\": lambda x: (x - 1.0)**3 * (x**2 + 1.0),\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2 * (x**2 + 1.0) + (x - 1.0)**3 * (2.0 * x),\n        \"alpha\": 1.0,\n        \"x0\": 1.7,\n        \"N\": 10,\n    }\n\n    # Test C (analytic nonpolynomial factor case)\n    # f(x) = (x+0.5)^3 * e^x\n    # f'(x) = e^x * (3(x+0.5)^2 + (x+0.5)^3)\n    # alpha = -0.5, x_0 = 0, N = 12\n    case_C = {\n        \"f\": lambda x: (x + 0.5)**3 * np.exp(x),\n        \"fp\": lambda x: np.exp(x) * (3.0 * (x + 0.5)**2 + (x + 0.5)**3),\n        \"alpha\": -0.5,\n        \"x0\": 0.0,\n        \"N\": 12,\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    results = []\n\n    for case in test_cases:\n        f, fp = case[\"f\"], case[\"fp\"]\n        alpha, x0, N = case[\"alpha\"], case[\"x0\"], case[\"N\"]\n        \n        # Array to store the sequence of errors e_k = |x_k - alpha|\n        # Size is N+1 for e_0, e_1, ..., e_N\n        errors = np.zeros(N + 1, dtype=float)\n        \n        x_k = float(x0)\n        errors[0] = np.abs(x_k - alpha)\n        \n        # Perform N iterations of Newton's method\n        for k in range(N):\n            f_val = f(x_k)\n            fp_val = fp(x_k)\n            \n            # Newton's iteration step\n            # Note: Given problem setup ensures fp_val is not zero away from the root.\n            x_k = x_k - f_val / fp_val\n            \n            # Store the error of the new iterate\n            errors[k + 1] = np.abs(x_k - alpha)\n            \n        # The last three errors needed for calculations are e_{N-2}, e_{N-1}, e_N\n        e_N = errors[N]\n        e_N_minus_1 = errors[N - 1]\n        e_N_minus_2 = errors[N - 2]\n\n        # Calculate the final asymptotic error ratio r_{N-1}\n        # r_{N-1} = e_N / e_{N-1}\n        r_final = e_N / e_N_minus_1\n        \n        # Calculate the final empirical order of convergence p_{N-1}\n        # p_{N-1} = ln(e_N / e_{N-1}) / ln(e_{N-1} / e_{N-2})\n        # numpy.log is the natural logarithm (ln)\n        p_final = np.log(e_N / e_N_minus_1) / np.log(e_N_minus_1 / e_N_minus_2)\n        \n        results.extend([r_final, p_final])\n\n    # Final print statement in the exact required format.\n    # Output is a flat list [r_A, p_A, r_B, p_B, r_C, p_C]\n    # with each value rounded to 6 decimal places.\n    print(f\"[{','.join([f'{val:.6f}' for val in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "试位法（Regula Falsi）通常作为二分法的一种改进方法被引入。然而，该方法可能会遭遇“停滞”现象，即区间的某个端点被固定，导致收敛变得异常缓慢，这种情况通常发生在根附近具有强曲率的函数上。 本次练习将指导你实现该方法，并在专门设计的停滞场景中凭经验估算线性收敛常数。这是一个至关重要的练习，它有助于理解看似稳健的算法背后潜在的陷阱，以及分析其最坏情况行为的重要性。",
            "id": "3265240",
            "problem": "考虑伪位置法（Regula Falsi），给定区间 $[a,b]$ 上的连续函数 $f$ 且满足 $f(a)  0  f(b)$，该方法在每次迭代中通过穿过点 $(a,f(a))$ 和 $(b,f(b))$ 的割线与 $x$ 轴的交点来构造一个新点 $c$，然后替换 $a$ 或 $b$ 中的一个以维持区间包围条件。设真根为 $r$，并定义与所选近似序列 $\\{x_k\\}$ 相关联的误差序列 $e_k = |x_k - r|$。收敛速度由以下核心定义来表征：\n- 线性收敛：存在常数 $C \\in (0,1)$ 使得 $\\lim_{k \\to \\infty} \\frac{e_{k+1}}{e_k} = C$。\n- 超线性收敛：$\\lim_{k \\to \\infty} \\frac{e_{k+1}}{e_k} = 0$。\n- 二次收敛：存在常数 $M  0$ 使得对于所有足够大的 $k$，有 $e_{k+1} \\le M e_k^2$。\n\n您的任务是实现伪位置法（Regula Falsi），并在该方法因端点函数值高度不平衡而表现出停滞现象的场景中，经验性地估计渐近线性收敛常数 $C$。具体来说，使用割线交点迭代序列 $\\{c_k\\}$ 作为近似值 $x_k$，利用解析已知的根 $r$ 计算误差序列 $e_k = |c_k - r|$，并通过对迭代历史记录的最后一部分的比率 $\\frac{e_{k+1}}{e_k}$ 进行平均来估计 $C$。估计过程必须是确定性的和可复现的。\n\n使用以下科学合理的测试套件，每个案例由一个具有已知根的指数函数和一个旨在引发不同程度停滞的区间定义：\n\n- 测试案例1（极端停滞，正端点值大）：\n  - 函数：$f(x) = e^{x} - (1 + \\delta)$，其中 $\\delta = 10^{-6}$。\n  - 区间：$[a,b] = [0,20]$。\n  - 迭代次数：$N = 5000$。\n  - 真根：$r = \\ln(1+\\delta)$。\n  - 预期定性行为：线性收敛，常数 $C$ 极度接近 $1$。\n\n- 测试案例2（中度停滞，正端点值较小）：\n  - 函数：$f(x) = e^{x} - (1 + \\delta)$，其中 $\\delta = 10^{-3}$。\n  - 区间：$[a,b] = [0,5]$。\n  - 迭代次数：$N = 50$。\n  - 真根：$r = \\ln(1+\\delta)$。\n  - 预期定性行为：线性收敛，常数 $C$ 明显小于 $1$。\n\n- 测试案例3（数值精度边界情况）：\n  - 函数：$f(x) = e^{x} - (1 + \\delta)$，其中 $\\delta = 10^{-12}$。\n  - 区间：$[a,b] = [0,20]$。\n  - 迭代次数：$N = 5000$。\n  - 真根：$r = \\ln(1+\\delta)$。\n  - 预期定性行为：由于步长小于机器ε，经验常数 $C$ 在双精度浮点分辨率内实际上等于 $1$。\n\n实现要求：\n- 使用标准的伪位置法更新。在每次迭代中，计算\n  $$ c = \\frac{a \\, f(b) - b \\, f(a)}{f(b) - f(a)}, $$\n  如果 $f(a) \\cdot f(c)  0$，则用 $c$ 替换 $a$，否则用 $c$ 替换 $b$。累积序列 $\\{c_k\\}$。\n- 对每个测试案例，计算误差序列 $e_k = |c_k - r|$，为 $e_k  0$ 的 $k$ 构建比率 $\\rho_k = \\frac{e_{k+1}}{e_k}$，并估计 $C$ 为最后 $m$ 个比率的中位数，其中 $m$ 应选择为一个固定的正整数，该整数相对于 $N$ 较小但足以平滑波动。\n- 所有最终答案均表示为无量纲浮点数。不涉及物理单位。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含三个测试案例的估计常数，格式为逗号分隔的列表并用方括号括起，例如 $[C_1,C_2,C_3]$。每个 $C_i$ 必须是一个浮点数。\n\n覆盖性设计：\n- 测试套件覆盖了正常路径情况（中度停滞）、极端停滞情况（线性 $C$ 非常接近 $1$）以及停滞与浮点精度相互作用的数值边界情况。\n\n您的程序必须是自包含的，并且不得读取任何输入。它必须严格遵守指定的输出格式。",
            "solution": "问题陈述经评估有效。这是一个在数值分析领域中提法明确、有科学依据的问题，没有矛盾、歧义或事实错误。该问题要求在停滞条件下，对伪位置法的线性收敛常数进行经验性估计，这是科学计算中的一个标准课题。所提供的测试案例是用于说明此特定行为的典型示例。\n\n解决方案首先实现伪位置法，然后应用指定的统计程序，从生成的迭代序列中估计收敛常数。\n\n伪位置法是一种迭代求根算法，用于求解在区间 $[a, b]$ 上满足 $f(a)$ 和 $f(b)$ 符号相反的连续函数 $f(x)$ 的根。在每次迭代 $k$ 中，根的一个新近似值 $c_k$ 被计算为连接点 $(a_k, f(a_k))$ 和 $(b_k, f(b_k))$ 的割线的 $x$ 轴截距。$c_k$ 的公式为：\n$$\nc_k = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}\n$$\n然后，通过保留 $c_k$ 和前一个端点之一（$a_k$ 或 $b_k$）来选择下一次迭代的区间 $[a_{k+1}, b_{k+1}]$，以确保新区间继续包围根。指定的更新规则是，如果 $f(a_k) \\cdot f(c_k)  0$（即它们符号相同），则用 $c_k$ 替换 $a_k$，否则用 $c_k$ 替换 $b_k$。\n\n对于函数 $f(x) = e^{x} - (1 + \\delta)$ 且 $\\delta  0$，其二阶导数为 $f''(x) = e^x  0$，这意味着该函数是严格凸函数。对于给定的区间 $[a, b]$，其中 $a=0$ 且 $b  0$，我们有 $f(a)  0$ 和 $f(b) > 0$。由于凸性，割线始终位于函数图像之上，其 $x$ 轴截距 $c_k$ 将总是位于真根 $r$ 的左侧。这意味着 $f(c_k)  0$。由于初始的 $f(a_0)=f(0)$ 也为负，条件 $f(a_k) \\cdot f(c_k)  0$ 将在每次迭代中都满足。因此，左端点总是被更新为 $a_{k+1} = c_k$，而右端点 $b$ 在整个过程中保持不变。这种现象称为停滞，它将方法的收敛速度从超线性降至线性。\n\n收敛速度由误差序列 $e_k = |x_k - r|$ 的行为来表征，其中 $r$ 是真根，$x_k$ 是近似序列。对于本问题，我们使用割线交点，因此 $x_k = c_k$。线性收敛意味着对于大的 $k$：\n$$\ne_{k+1} \\approx C \\cdot e_k \\quad \\implies \\quad \\frac{e_{k+1}}{e_k} \\approx C\n$$\n其中 $C \\in (0,1)$ 是渐近线性收敛常数。\n\n为了对每个测试案例经验性地估计 $C$，实现了以下算法：\n1.  对于每个测试案例配置 $(\\delta, [a, b], N)$，运行伪位置法，指定迭代次数为 $N$。存储迭代序列 $\\{c_k\\}_{k=0}^{N-1}$。\n2.  解析地计算真根 $r = \\ln(1 + \\delta)$。\n3.  计算误差序列 $e_k = |c_k - r|$，其中 $k = 0, 1, \\dots, N-1$。\n4.  对于所有 $e_k  0$ 的 $k$，生成比率序列 $\\rho_k = e_{k+1}/e_k$。这避免了除以零。\n5.  将 $C$ 估计为最后 $m$ 个计算出的比率的中位数。使用中位数是为了提供一个稳健的估计，它对迭代进入完全渐近状态之前可能出现的瞬态波动不敏感。选择固定值 $m=10$，相对于测试案例中的迭代次数来说较小，但足以平滑噪声。\n\n此程序将应用于三个旨在探测该方法行为不同方面的独特测试案例：\n- **案例1：** 极端停滞（$b=20, \\delta=10^{-6}$）。$f(b)$ 相对于 $|f(a)|$ 的值很大，导致收敛非常缓慢，预期 $C$ 的值极度接近 $1$。\n- **案例2：** 中度停滞（$b=5, \\delta=10^{-3}$）。端点不平衡不太严重，导致更快（但仍为线性）的收敛，常数 $C$ 明显小于 $1$。\n- **案例3：** 数值精度极限（$b=20, \\delta=10^{-12}$）。真根 $r \\approx 10^{-12}$ 很小，理论上收敛常数 $C$ 接近 $1$。迭代值 $a_k$ 的更新步长（近似为 $a_{k+1} \\approx a_k - \\frac{b (a_k-r) f'(r)}{f(b)}$）变得如此之小，以至于当 $a_k$ 接近 $r$ 时，相对于 $a_k$ 的变化量小于机器精度。这导致 $a_k$ 在数值上停滞（$a_{k+1}$ 与 $a_k$ 在计算上无法区分），从而导致对于大的 $k$，误差 $e_k$ 恒定。因此，比率 $e_{k+1}/e_k$ 变为精确的 $1$，最终比率的中位数将是 $1$。\n\n实现将为每个案例系统地执行这些步骤，并报告估计的常数 $C$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the False Position Method to estimate the linear convergence\n    constant C for three test cases demonstrating stagnation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (extreme stagnation)\n        {'delta': 1e-6, 'interval': [0.0, 20.0], 'N': 5000},\n        # Test case 2 (moderate stagnation)\n        {'delta': 1e-3, 'interval': [0.0, 5.0], 'N': 50},\n        # Test case 3 (numerical precision edge case)\n        {'delta': 1e-12, 'interval': [0.0, 20.0], 'N': 5000},\n    ]\n\n    results = []\n    # m is the number of final ratios to use for the median calculation\n    m = 10 \n\n    for case in test_cases:\n        delta = case['delta']\n        a, b = case['interval']\n        N = case['N']\n\n        # Define the function f(x) for the current case\n        f = lambda x: np.exp(x) - (1.0 + delta)\n\n        # Calculate the true root\n        r = np.log(1.0 + delta)\n\n        c_k_sequence = []\n        \n        # Ensure initial interval is valid\n        f_a = f(a)\n        f_b = f(b)\n        \n        if f_a * f_b >= 0:\n            # This should not happen for the given test cases\n            # but is good practice for a general implementation.\n            raise ValueError(\"Function has the same sign at interval endpoints.\")\n\n        # False Position Method iteration\n        for _ in range(N):\n            # The check for (f_b - f_a == 0) is omitted as it is highly\n            # unlikely for these continuous, non-constant functions.\n            c = (a * f_b - b * f_a) / (f_b - f_a)\n            c_k_sequence.append(c)\n            \n            f_c = f(c)\n\n            # Update the interval\n            if f_a * f_c > 0:\n                a = c\n                f_a = f_c\n            else:\n                b = c\n                f_b = f_c\n\n        # Post-processing to estimate C\n        c_k = np.array(c_k_sequence, dtype=np.float64)\n        \n        # Compute the error sequence e_k = |c_k - r|\n        e_k = np.abs(c_k - r)\n        \n        # The sequence of ratios rho_k = e_{k+1} / e_k\n        # We must filter out cases where the denominator e_k is zero.\n        denominators = e_k[:-1]\n        numerators = e_k[1:]\n        \n        # Create a mask to select only elements where the denominator is > 0\n        mask = denominators > 0\n        \n        if np.any(mask):\n            valid_ratios = numerators[mask] / denominators[mask]\n\n            if len(valid_ratios) >= m:\n                # Estimate C as the median of the last m ratios\n                C_estimate = np.median(valid_ratios[-m:])\n            elif len(valid_ratios) > 0:\n                # If there are fewer than m ratios, use all available ones\n                C_estimate = np.median(valid_ratios)\n            else:\n                # No valid ratios could be computed\n                C_estimate = np.nan # Should not happen in these test cases\n        else:\n            C_estimate = np.nan\n\n        results.append(C_estimate)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "我们已经看到了一些常见的求根方法如何可能表现出缓慢的线性收敛。一个自然的问题是：我们能做得更好吗？斯特芬森方法（Steffensen's method）是一种强大的技术，它能将线性收敛的不动点迭代加速到二次收敛，并且通常不需要计算函数的导数。 在这个动手练习中，你将对收敛缓慢的$x = \\cos(x)$不动点迭代应用斯特芬森方法。通过比较原始迭代与其加速版本的收敛阶，你将亲眼见证效率的显著提升，从而体会到收敛加速技术的威力。",
            "id": "3265247",
            "problem": "编写一个完整的程序，研究和比较基本不动点迭代及其 Steffensen 加速变体对于函数 $\\cos(x)$ 的收敛速度，角度使用弧度制。你的推导必须基于收敛阶的核心定义：如果存在一个常数 $C \\in (0,\\infty)$，使得\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^p} = C.\n$$\n则序列 $\\{x_k\\}$ 以 $p \\ge 1$ 阶收敛到 $x^\\star$。\n问题是从这个定义出发，在不使用任何外部数据源的情况下，实现以下内容。\n\n任务：\n- 考虑不动点映射 $g(x) = \\cos(x)$ 和不动点迭代 $x_{k+1} = g(x_k)$。已知该迭代会线性收敛到 $g$ 的唯一不动点 $x^\\star$，即方程 $x = \\cos(x)$ 的解。\n- 对同一映射 $g$ 应用 Steffensen 方法。在当前迭代点 $x_k$ 处，该方法仅使用 $g$ 的函数值，并通过以下代数更新式给出\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k},\n$$\n只要分母不为零。如果分母等于 $0$ 或在机器精度内数值上与 $0$ 无法区分，则在该步骤中必须回退到普通的不动点更新 $x_{k+1} = g(x_k)$。\n- 使用根据高精度参考解 $x^\\star$ 计算的三个连续误差得到的观测收敛阶估计器。对于误差 $e_k = |x_k - x^\\star|$，定义\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)},\n$$\n只要对数有定义。使用由不太接近浮点数下溢的误差计算出的最新可用 $p_k$；具体来说，优先选择满足 $e_{k-2} \\ge 10^{-10}$ 的最大索引 $k$；如果不存在这样的三元组，则使用可用的最大有效 $p_k$。\n- 使用基于与 $x^\\star$ 的绝对误差低于容差或达到最大迭代次数上限的停止准则，以先发生者为准。\n\n参考解 $x^\\star$：\n- 通过应用 Newton 法，将 $f(x) = \\cos(x) - x = 0$ 的解作为 $x^\\star$ 的高精度参考值在内部计算\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}, \\quad f'(x) = -\\sin(x) - 1,\n$$\n从 $x_0 = 0.7$ 开始，迭代直到 Newton 更新步长的大小小于 $10^{-16}$ 或达到最大迭代次数 $100$ 次。在所有误差计算中使用这个内部计算出的 $x^\\star$。角度以弧度为单位。\n\n实现和数值细节：\n- 实现两个生成序列 $\\{x_k\\}$ 的求解器：\n  1. 普通不动点迭代 $x_{k+1} = \\cos(x_k)$。\n  2. 上面定义的 Steffensen 加速迭代。\n- 对于两个求解器，当 $|x_k - x^\\star|  10^{-14}$ 或达到 $200$ 次迭代后停止，以先发生者为准。\n- 对于 Steffensen 方法，如果分母 $g(g(x_k)) - 2 g(x_k) + x_k$ 在精确算术中等于 $0$ 或在浮点算术中其绝对值低于 $10^{-14}$，则该步使用回退方案 $x_{k+1} = g(x_k)$。\n- 对于每个生成的序列，使用上面定义的 $p_k$ 估计器计算观测收敛阶，并按先前指定的方式为每个序列报告一个单一的估计值。\n\n测试套件：\n- 对两种方法使用以下四个初始猜测值：$x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$。\n- 对于每个 $x_0$，运行普通不动点迭代和 Steffensen 方法。对于每个序列，返回观测到的收敛阶估计值，四舍五入到 $2$ 位小数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含 $8$ 个结果，以逗号分隔的列表形式包含在方括号内，排序如下\n$$\n[\\text{plain}(0.1),\\ \\text{steff}(0.1),\\ \\text{plain}(1.0),\\ \\text{steff}(1.0),\\ \\text{plain}(2.0),\\ \\text{steff}(2.0),\\ \\text{plain}(-1.0),\\ \\text{steff}(-1.0)].\n$$\n- 每个条目是观测到的收敛阶估计值，四舍五入到 $2$ 位小数，并且必须作为浮点数打印。\n- 无需用户输入。不涉及物理单位。角度以弧度为单位。单行输出必须严格遵循上述格式，例如：$[1.00,2.00,1.00,2.00,1.00,2.00,1.00,2.00]$。",
            "solution": "该问题要求对标准不动点迭代及其 Steffensen 加速对应方法的收敛速度进行比较研究。分析将针对函数 $g(x) = \\cos(x)$ 进行，所有计算均使用弧度制角度。任务的核心是实现这两种方法，生成迭代序列，然后对每个序列的收敛阶进行数值估计。\n\n### 理论框架\n\n函数 $g(x)$ 的不动点是一个值 $x^\\star$，满足 $x^\\star = g(x^\\star)$。对于给定的函数 $g(x) = \\cos(x)$，不动点是方程 $x = \\cos(x)$ 的唯一解，也称为 Dottie 数。\n\n**1. 普通不动点迭代**\n\n基本不动点迭代由序列 $x_{k+1} = g(x_k)$ 定义，其中 $k=0, 1, 2, \\dots$。该方法的收敛性由函数 $g(x)$ 在不动点 $x^\\star$ 邻域内的性质决定。根据不动点定理，如果 $|g'(x^\\star)|  1$，则对于任何足够接近 $x^\\star$ 的初始猜测值 $x_0$，迭代都将线性收敛。\n\n对于 $g(x) = \\cos(x)$，其导数为 $g'(x) = -\\sin(x)$。不动点 $x^\\star$ 约等于 $0.739085$。在该点，导数值为 $g'(x^\\star) = -\\sin(x^\\star) \\approx -0.674$。由于 $|g'(x^\\star)| \\approx 0.674  1$，迭代保证收敛。\n\n收敛是线性的，这意味着误差 $e_k = |x_k - x^\\star|$ 在每一步都大致减少一个常数因子。形式上，如果\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^1} = |g'(x^\\star)| = C,\n$$\n其中 $C \\in (0, 1)$ 是渐进误差常数，则序列 $\\{x_k\\}$ 以 $p=1$ 阶（线性）收敛到 $x^\\star$。对于这个问题，我们预期估计的收敛阶将接近 $p=1$。\n\n**2. Steffensen 方法**\n\nSteffensen 方法是一种加速技术，它将线性收敛的不动点迭代转化为二次收敛的迭代，而无需计算导数。对于给定的映射 $g(x)$，其更新规则为：\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k}.\n$$\n这可以看作是对函数 $f(x) = g(x) - x = 0$ 应用 Newton 法，其中导数 $f'(x) = g'(x) - 1$ 使用有限差分进行近似。在合适的条件下（具体来说，$g'(x^\\star) \\neq 1$），Steffensen 方法表现出二次收敛性。\n\n二次收敛（$p=2$ 阶）意味着每次迭代中正确有效数字的位数大约会翻倍。形式上，\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^2} = C,\n$$\n对于某个常数 $C \\in (0, \\infty)$。因此，我们预期 Steffensen 方法的估计收敛阶将接近 $p=2$。如果 Steffensen 更新式的分母在数值上接近于零，则有必要回退到普通不动点步骤 $x_{k+1}=g(x_k)$。这种情况可能发生在 $g(x_k)-x_k$ 非常小的时候，即当我们已经非常接近解时。\n\n### 实现策略\n\n**1. 高精度参考解 $x^\\star$**\n\n为了计算误差 $e_k = |x_k - x^\\star|$，需要一个高精度的 $x^\\star$ 值。这将通过对 方程 $f(x) = \\cos(x) - x = 0$ 应用 Newton 法来计算。Newton 迭代法为\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} = x_n - \\frac{\\cos(x_n) - x_n}{-\\sin(x_n) - 1}.\n$$\n从 $x_0 = 0.7$ 开始，我们进行迭代，直到更新步长的大小 $|x_{n+1} - x_n|$ 小于严格的容差 $10^{-16}$。\n\n**2. 迭代求解器**\n\n将实现两个求解器：一个用于普通不动点迭代 $x_{k+1} = \\cos(x_k)$，另一个用于 Steffensen 方法。两个求解器都将从给定的初始猜测值 $x_0$ 开始，并生成迭代序列 $\\{x_k\\}$。当绝对误差 $|x_k - x^\\star|$ 小于 $10^{-14}$ 或达到最大迭代次数 $200$ 次时，迭代将终止。Steffensen 的求解器将包含指定的回退机制，即如果 $|g(g(x_k)) - 2g(x_k) + x_k|  10^{-14}$，则使用更简单的更新式 $x_{k+1} = \\cos(x_k)$。\n\n**3. 观测收敛阶 $p_k$**\n\n收敛阶是从误差序列 $\\{e_k\\}$ 中数值估计出来的。提供的公式是：\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)}.\n$$\n该公式源自理论定义 $e_k \\approx C e_{k-1}^p$，这意味着 $\\ln(e_k) - \\ln(e_{k-1}) \\approx p (\\ln(e_{k-1}) - \\ln(e_{k-2}))$。生成序列后，我们为所有可能的索引 $k$ 计算 $p_k$。为了获得一个单一的代表性收敛阶估计，我们选择满足误差 $e_{k-2}$ 不会过小（具体为 $e_{k-2} \\ge 10^{-10}$）的最大索引 $k$ 所对应的 $p_k$ 值。这确保了估计值取自收敛的渐近区域，但在浮点精度限制误差比率的准确性之前。如果不存在这样的索引，则使用最后计算出的 $p_k$ 值。\n\n最终程序将对每个初始猜测值 $x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$ 执行此过程，总共产生八个收敛阶估计值，并按要求格式化和打印。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the numerical experiment and print the final results.\n    \"\"\"\n\n    def compute_reference_solution():\n        \"\"\"\n        Computes a high-accuracy reference solution x* for x = cos(x) using Newton's method.\n        The equation is f(x) = cos(x) - x = 0.\n        The derivative is f'(x) = -sin(x) - 1.\n        \"\"\"\n        x = 0.7  # Initial guess\n        max_iter = 100\n        tolerance = 1e-16\n        \n        for _ in range(max_iter):\n            f_x = np.cos(x) - x\n            fp_x = -np.sin(x) - 1\n            if fp_x == 0:  # Avoid division by zero\n                break\n            update = -f_x / fp_x\n            x += update\n            if np.abs(update)  tolerance:\n                break\n        return x\n\n    def run_iteration(method, x0, x_star):\n        \"\"\"\n        Generates a sequence of iterates for a given method.\n\n        Args:\n            method (str): 'plain' for fixed-point, 'steffensen' for Steffensen's method.\n            x0 (float): The initial guess.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            list: The history of iterates [x0, x1, ...].\n        \"\"\"\n        x_k = float(x0)\n        history = [x_k]\n        max_iter = 200\n        stop_tol = 1e-14\n        fallback_tol = 1e-14\n\n        for _ in range(max_iter):\n            if np.abs(x_k - x_star)  stop_tol:\n                break\n            \n            if method == 'plain':\n                x_k_plus_1 = np.cos(x_k)\n            elif method == 'steffensen':\n                g_xk = np.cos(x_k)\n                g_g_xk = np.cos(g_xk)\n                denominator = g_g_xk - 2 * g_xk + x_k\n                \n                if np.abs(denominator)  fallback_tol:\n                    # Fallback to plain fixed-point iteration\n                    x_k_plus_1 = g_xk\n                else:\n                    numerator = (g_xk - x_k)**2\n                    x_k_plus_1 = x_k - numerator / denominator\n            else:\n                raise ValueError(\"Unknown method specified.\")\n            \n            x_k = x_k_plus_1\n            history.append(x_k)\n            \n        return history\n\n    def estimate_order(x_history, x_star):\n        \"\"\"\n        Estimates the order of convergence from a sequence of iterates.\n\n        Args:\n            x_history (list): The sequence of iterates.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            float: The estimated order of convergence.\n        \"\"\"\n        if len(x_history)  3:\n            # Not enough data points to compute even one p_k value.\n            # Based on the problem, this scenario is not expected.\n            return np.nan\n\n        errors = np.abs(np.array(x_history) - x_star)\n        \n        p_k_values = []\n        for k in range(2, len(errors)):\n            e_k2, e_k1, e_k = errors[k-2], errors[k-1], errors[k]\n            \n            # Avoid log(0) and division by zero from the ratios\n            if e_k1 == 0 or e_k2 == 0 or e_k1 == e_k2:\n                continue\n            \n            # Ratios for the logarithms\n            ratio1 = e_k / e_k1\n            ratio2 = e_k1 / e_k2\n\n            # Ensure arguments to log are positive\n            if ratio1 = 0 or ratio2 = 0:\n                continue\n            \n            log_numerator = np.log(ratio1)\n            log_denominator = np.log(ratio2)\n            \n            if log_denominator == 0:\n                continue\n            \n            p_k = log_numerator / log_denominator\n            p_k_values.append((k, p_k))\n\n        if not p_k_values:\n            # Could not compute any p_k.\n            return np.nan\n        \n        # Selection rule: Find the latest p_k where e_{k-2} is not too small.\n        for k, p_k in reversed(p_k_values):\n            if errors[k-2] >= 1e-10:\n                return p_k\n        \n        # Fallback: if no such p_k exists, use the latest one available.\n        return p_k_values[-1][1]\n\n    # Compute the reference solution once\n    x_star = compute_reference_solution()\n\n    test_cases = [0.1, 1.0, 2.0, -1.0]\n    final_results = []\n\n    for x0_val in test_cases:\n        # Plain fixed-point iteration\n        history_plain = run_iteration('plain', x0_val, x_star)\n        order_plain = estimate_order(history_plain, x_star)\n        final_results.append(order_plain)\n\n        # Steffensen-accelerated iteration\n        history_steff = run_iteration('steffensen', x0_val, x_star)\n        order_steff = estimate_order(history_steff, x_star)\n        final_results.append(order_steff)\n\n    # Format the results to 2 decimal places and print\n    formatted_output = [f\"{res:.2f}\" for res in final_results]\n    print(f\"[{','.join(formatted_output)}]\")\n\nsolve()\n```"
        }
    ]
}