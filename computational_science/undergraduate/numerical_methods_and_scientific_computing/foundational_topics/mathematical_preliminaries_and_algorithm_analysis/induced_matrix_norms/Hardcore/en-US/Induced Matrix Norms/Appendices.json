{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp induced matrix norms, we must move from abstract definitions to concrete calculations. This first exercise provides an opportunity to compute the induced $1$-, $2$-, and $\\infty$-norms for a Givens rotation matrix, a fundamental tool in numerical linear algebra used for tasks like QR factorization. By working directly from the definition $\\|A\\|_{p} = \\sup_{x \\neq 0} \\|A x\\|_{p}/\\|x\\|_{p}$, you will solidify your understanding of how these different norms capture distinct properties of a matrix transformation.",
            "id": "3242259",
            "problem": "In many algorithms for numerical linear algebra within numerical methods and scientific computing, a Givens rotation is used to introduce zeros while preserving lengths. Let $n \\geq 2$ and let $G \\in \\mathbb{R}^{n \\times n}$ be a Givens rotation that acts on coordinates $i$ and $j$ (with $1 \\leq i  j \\leq n$) by the planar rotation angle $\\theta \\in \\mathbb{R}$, so that, in the $2 \\times 2$ submatrix on rows and columns $i$ and $j$, $G$ has entries $G_{ii} = \\cos \\theta$, $G_{ij} = \\sin \\theta$, $G_{ji} = -\\sin \\theta$, $G_{jj} = \\cos \\theta$, and $G$ equals the identity matrix elsewhere. Using only the definition of the induced matrix norm\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}},\n$$\nderive the induced $1$-, $2$-, and $\\infty$-norms of $G$ as explicit expressions in terms of $\\cos \\theta$ and $\\sin \\theta$. Provide exact symbolic expressions; no rounding is required. Your final answer must be a single composite expression listing the three norms in the order $1$-, $2$-, $\\infty$-norms.",
            "solution": "The problem requires the derivation of the induced $1$-, $2$-, and $\\infty$-norms of a Givens rotation matrix $G \\in \\mathbb{R}^{n \\times n}$, using only the definition of an induced matrix norm. The matrix $G$ operates on coordinates $i$ and $j$, where $1 \\leq i  j \\leq n$. Its non-trivial entries form a $2 \\times 2$ submatrix on rows and columns $i$ and $j$:\n$$\n\\begin{pmatrix} G_{ii}  G_{ij} \\\\ G_{ji}  G_{jj} \\end{pmatrix} = \\begin{pmatrix} \\cos \\theta  \\sin \\theta \\\\ -\\sin \\theta  \\cos \\theta \\end{pmatrix}\n$$\nwhere $\\theta \\in \\mathbb{R}$ is the angle of rotation. Elsewhere, $G$ is the identity matrix, meaning $G_{kk} = 1$ for $k \\notin \\{i, j\\}$ and all other off-diagonal entries are $0$. For notational convenience, let $c = \\cos \\theta$ and $s = \\sin \\theta$.\n\nThe induced $p$-norm of a matrix $A$ is defined as\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}} = \\sup_{\\|x\\|_{p}=1} \\|A x\\|_{p}\n$$\nWe will apply this definition for $p=1$, $p=2$, and $p=\\infty$.\n\nLet $x \\in \\mathbb{R}^n$ be an arbitrary vector, and let $y = Gx$. The components of $y$ are given by:\n$y_k = x_k$ for $k \\notin \\{i, j\\}$\n$y_i = c x_i + s x_j$\n$y_j = -s x_i + c x_j$\n\n**Derivation of the Induced $1$-Norm, $\\|G\\|_{1}$**\n\nThe induced $1$-norm is defined as $\\|G\\|_{1} = \\sup_{\\|x\\|_{1}=1} \\|Gx\\|_{1}$.\nLet $x \\in \\mathbb{R}^n$ such that $\\|x\\|_{1} = \\sum_{k=1}^n |x_k| = 1$. The $1$-norm of $y=Gx$ is:\n$$\n\\|Gx\\|_{1} = \\sum_{k=1}^n |y_k| = \\sum_{k \\notin \\{i,j\\}} |y_k| + |y_i| + |y_j| = \\sum_{k \\notin \\{i,j\\}} |x_k| + |c x_i + s x_j| + |-s x_i + c x_j|\n$$\nUsing the triangle inequality on the last two terms:\n$|c x_i + s x_j| \\leq |c||x_i| + |s||x_j|$\n$|-s x_i + c x_j| \\leq |-s||x_i| + |c||x_j| = |s||x_i| + |c||x_j|$\nSumming these inequalities, we get:\n$|c x_i + s x_j| + |-s x_i + c x_j| \\leq (|c|+|s|)|x_i| + (|s|+|c|)|x_j| = (|c|+|s|)(|x_i|+|x_j|)$.\nSubstituting this into the expression for $\\|Gx\\|_{1}$:\n$$\n\\|Gx\\|_{1} \\leq \\sum_{k \\notin \\{i,j\\}} |x_k| + (|c|+|s|)(|x_i|+|x_j|)\n$$\nLet $M = |c|+|s| = |\\cos\\theta| + |\\sin\\theta|$. Note that $M \\geq 1$, since $M^2 = (|c|+|s|)^2 = |c|^2+|s|^2+2|c||s| = c^2+s^2+2|cs| = 1+2|\\cos\\theta \\sin\\theta| \\geq 1$.\nLet $\\alpha = |x_i| + |x_j|$. Since $\\|x\\|_1=1$, we have $\\sum_{k \\notin \\{i,j\\}} |x_k| = 1-\\alpha$, where $0 \\leq \\alpha \\leq 1$. The inequality becomes:\n$$\n\\|Gx\\|_{1} \\leq (1-\\alpha) + M\\alpha = 1 + (M-1)\\alpha\n$$\nSince $M \\geq 1$, the expression $1 + (M-1)\\alpha$ is maximized when $\\alpha$ is maximized. The maximum value of $\\alpha$ is $1$, which occurs when $x_k = 0$ for all $k \\notin \\{i,j\\}$, i.e., when the support of $x$ is restricted to the indices $\\{i, j\\}$. For such vectors, the inequality implies $\\|Gx\\|_{1} \\leq M$.\nThis establishes an upper bound: $\\|G\\|_{1} \\leq |\\cos\\theta| + |\\sin\\theta|$.\n\nTo show that this supremum is attained, we must find a vector $x$ with $\\|x\\|_{1}=1$ for which $\\|Gx\\|_{1} = |\\cos\\theta| + |\\sin\\theta|$. Consider the standard basis vector $x = e_i$. Then $\\|e_i\\|_{1}=1$. The vector $Ge_i$ corresponds to the $i$-th column of $G$, which is $c e_i - s e_j$.\nThe $1$-norm of this vector is:\n$$\n\\|Ge_i\\|_{1} = \\|c e_i - s e_j\\|_{1} = |c| + |-s| = |\\cos\\theta| + |\\sin\\theta|\n$$\nSince we have found a vector $x$ for which $\\|Gx\\|_{1}$ equals the upper bound, the supremum must be this value.\n$$\n\\|G\\|_{1} = |\\cos\\theta| + |\\sin\\theta|\n$$\n\n**Derivation of the Induced $\\infty$-Norm, $\\|G\\|_{\\infty}$**\n\nThe induced $\\infty$-norm is defined as $\\|G\\|_{\\infty} = \\sup_{\\|x\\|_{\\infty}=1} \\|Gx\\|_{\\infty}$.\nLet $x \\in \\mathbb{R}^n$ such that $\\|x\\|_{\\infty} = \\max_k |x_k| = 1$. The $\\infty$-norm of $y=Gx$ is:\n$$\n\\|Gx\\|_{\\infty} = \\max_{k} |y_k| = \\max\\left(\\max_{k \\notin \\{i,j\\}} |x_k|, |c x_i + s x_j|, |-s x_i + c x_j|\\right)\n$$\nSince $\\|x\\|_{\\infty}=1$, we know that $|x_k| \\leq 1$ for all $k$. Thus, $\\max_{k \\notin \\{i,j\\}} |x_k| \\leq 1$.\nUsing the triangle inequality and the fact that $|x_i| \\leq 1$ and $|x_j| \\leq 1$:\n$|c x_i + s x_j| \\leq |c||x_i| + |s||x_j| \\leq |c|(1) + |s|(1) = |\\cos\\theta| + |\\sin\\theta|$.\n$|-s x_i + c x_j| \\leq |s||x_i| + |c||x_j| \\leq |s|(1) + |c|(1) = |\\cos\\theta| + |\\sin\\theta|$.\nThus, we have an upper bound for $\\|Gx\\|_{\\infty}$:\n$$\n\\|Gx\\|_{\\infty} \\leq \\max(1, |\\cos\\theta| + |\\sin\\theta|)\n$$\nAs established earlier, $|\\cos\\theta| + |\\sin\\theta| \\geq 1$. Therefore, the upper bound is $\\|G\\|_{\\infty} \\leq |\\cos\\theta| + |\\sin\\theta|$.\n\nTo show this bound is attained, we construct a vector $x$ with $\\|x\\|_{\\infty}=1$. Let $x_k = 0$ for $k \\notin \\{i,j\\}$. We choose $x_i$ and $x_j$ to maximize $|y_i| = |c x_i + s x_j|$. Let $x_i = \\mathrm{sgn}(c)$ and $x_j = \\mathrm{sgn}(s)$, where $\\mathrm{sgn}$ is the sign function. Since $c^2+s^2=1$, at least one of them is non-zero, ensuring $\\|x\\|_\\infty = \\max(|\\mathrm{sgn}(c)|, |\\mathrm{sgn}(s)|, 0) = 1$.\nFor this choice of $x$, the $i$-th component of $y=Gx$ is:\n$$\ny_i = c x_i + s x_j = c(\\mathrm{sgn}(c)) + s(\\mathrm{sgn}(s)) = |c| + |s| = |\\cos\\theta| + |\\sin\\theta|\n$$\nThe $\\infty$-norm of the resulting vector $y$ is $\\|y\\|_{\\infty} = \\max_k |y_k|$. Since $|y_i|$ is one of the components, $\\|y\\|_{\\infty} \\geq |y_i| = |\\cos\\theta| + |\\sin\\theta|$.\nCombining this with the upper bound, we must have equality.\n$$\n\\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta|\n$$\n\n**Derivation of the Induced $2$-Norm, $\\|G\\|_{2}$**\n\nThe induced $2$-norm is defined as $\\|G\\|_{2} = \\sup_{\\|x\\|_{2}=1} \\|Gx\\|_{2}$.\nLet $x \\in \\mathbb{R}^n$. We compute the squared Euclidean norm of $y=Gx$:\n$$\n\\|Gx\\|_{2}^2 = \\|y\\|_{2}^2 = \\sum_{k=1}^n y_k^2 = \\sum_{k \\notin \\{i,j\\}} y_k^2 + y_i^2 + y_j^2\n$$\nSubstituting the expressions for the components of $y$:\n$$\n\\|Gx\\|_{2}^2 = \\sum_{k \\notin \\{i,j\\}} x_k^2 + (c x_i + s x_j)^2 + (-s x_i + c x_j)^2\n$$\nExpanding the squared terms:\n$$\n(c x_i + s x_j)^2 = c^2 x_i^2 + 2cs x_i x_j + s^2 x_j^2\n$$\n$$\n(-s x_i + c x_j)^2 = s^2 x_i^2 - 2cs x_i x_j + c^2 x_j^2\n$$\nThe sum of these two terms is:\n$$\n(c^2 x_i^2 + 2cs x_i x_j + s^2 x_j^2) + (s^2 x_i^2 - 2cs x_i x_j + c^2 x_j^2) = (c^2+s^2)x_i^2 + (s^2+c^2)x_j^2\n$$\nSince $c^2+s^2 = \\cos^2\\theta + \\sin^2\\theta = 1$, this simplifies to $x_i^2+x_j^2$.\nSubstituting this back into the expression for $\\|Gx\\|_{2}^2$:\n$$\n\\|Gx\\|_{2}^2 = \\sum_{k \\notin \\{i,j\\}} x_k^2 + (x_i^2 + x_j^2) = \\sum_{k=1}^n x_k^2 = \\|x\\|_{2}^2\n$$\nThis shows that $\\|Gx\\|_{2} = \\|x\\|_{2}$ for any vector $x \\in \\mathbb{R}^n$. The Givens rotation matrix $G$ is an isometry with respect to the $2$-norm; it preserves the length of vectors.\nUsing this property in the definition of the induced $2$-norm:\n$$\n\\|G\\|_{2} = \\sup_{x \\neq 0} \\frac{\\|Gx\\|_{2}}{\\|x\\|_{2}} = \\sup_{x \\neq 0} \\frac{\\|x\\|_{2}}{\\|x\\|_{2}} = \\sup_{x \\neq 0} 1 = 1\n$$\nThus, the induced $2$-norm is $1$. This might be written as $\\cos^2\\theta+\\sin^2\\theta$, but $1$ is the canonical simplified expression.\n\nIn summary, the induced norms are:\n$\\|G\\|_{1} = |\\cos\\theta| + |\\sin\\theta|$\n$\\|G\\|_{2} = 1$\n$\\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta|$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n|\\cos\\theta| + |\\sin\\theta|  1  |\\cos\\theta| + |\\sin\\theta|\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond simple calculation, the induced $2$-norm possesses a rich geometric interpretation. It measures the maximum stretching factor a matrix applies to any vector, and the vectors that experience this maximum stretch are of special interest. This problem  delves into this concept by asking you to determine the dimension of the set of all vectors that attain the supremum in the definition of $\\|A\\|_2$. You will discover that this set forms a subspace, whose dimension is directly linked to the eigenvalues of the associated matrix $A^\\top A$.",
            "id": "3242331",
            "problem": "Let $A(\\alpha) \\in \\mathbb{R}^{3 \\times 3}$ be the real diagonal matrix\n$$\nA(\\alpha) = \\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nConsider the induced operator $2$-norm $\\|A(\\alpha)\\|_2$, defined by\n$$\n\\|A(\\alpha)\\|_2 = \\sup_{x \\in \\mathbb{R}^3,\\, x \\neq 0} \\frac{\\|A(\\alpha)x\\|_2}{\\|x\\|_2}.\n$$\nDefine the set\n$$\nS(\\alpha) = \\left\\{ x \\in \\mathbb{R}^3 : \\|A(\\alpha)x\\|_2 = \\|A(\\alpha)\\|_2\\,\\|x\\|_2 \\right\\}.\n$$\nStarting from the definition of the induced operator $2$-norm and properties of symmetric positive semidefinite matrices, determine the dimension of the subspace $S(\\alpha)$ as a function of the real parameter $\\alpha$. Express your final answer as a single closed-form piecewise expression in terms of $\\alpha$. No rounding is required.",
            "solution": "The problem asks for the dimension of the subspace $S(\\alpha)$ for a given matrix $A(\\alpha)$ as a function of the real parameter $\\alpha$. The set $S(\\alpha)$ is defined as\n$$\nS(\\alpha) = \\left\\{ x \\in \\mathbb{R}^3 : \\|A(\\alpha)x\\|_2 = \\|A(\\alpha)\\|_2\\,\\|x\\|_2 \\right\\}.\n$$\nBy definition, the induced operator $2$-norm of a matrix $M \\in \\mathbb{R}^{m \\times n}$ is given by\n$$\n\\|M\\|_2 = \\sup_{x \\in \\mathbb{R}^n,\\, x \\neq 0} \\frac{\\|Mx\\|_2}{\\|x\\|_2}.\n$$\nSquaring this expression, we get\n$$\n\\|M\\|_2^2 = \\sup_{x \\neq 0} \\frac{\\|Mx\\|_2^2}{\\|x\\|_2^2} = \\sup_{x \\neq 0} \\frac{x^T M^T M x}{x^T x}.\n$$\nThe term $\\frac{x^T M^T M x}{x^T x}$ is the Rayleigh quotient of the matrix $M^T M$. The matrix $M^T M$ is always symmetric and positive semidefinite. A fundamental property of the Rayleigh quotient for a symmetric matrix is that its supremum is equal to the largest eigenvalue of that matrix. Therefore, $\\|M\\|_2^2$ is the largest eigenvalue of $M^T M$, and $\\|M\\|_2$ is the largest singular value of $M$.\n\nThe set $S(\\alpha)$ consists of the zero vector and all non-zero vectors $x$ for which the supremum is attained. These are precisely the eigenvectors of the matrix $M^T M$ corresponding to its largest eigenvalue. The union of these eigenvectors and the zero vector forms a subspace, which is the eigenspace corresponding to the largest eigenvalue of $M^T M$. The dimension of $S(\\alpha)$ is therefore the geometric multiplicity of the largest eigenvalue of the matrix $A(\\alpha)^T A(\\alpha)$.\n\nThe given matrix is\n$$\nA(\\alpha) = \\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nSince $A(\\alpha)$ is a real diagonal matrix, it is symmetric, so $A(\\alpha)^T = A(\\alpha)$. Let us compute the matrix $A(\\alpha)^T A(\\alpha)$:\n$$\nA(\\alpha)^T A(\\alpha) = A(\\alpha)^2 = \\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha  0  0 \\\\\n0  \\alpha  0 \\\\\n0  0  1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nThe eigenvalues of this diagonal matrix are its diagonal entries: $\\alpha^2$ (with algebraic multiplicity $2$) and $1$ (with algebraic multiplicity $1$). Let $\\lambda_{\\max}$ be the largest eigenvalue of $A(\\alpha)^T A(\\alpha)$. We have $\\lambda_{\\max} = \\max(\\alpha^2, 1)$. The dimension of $S(\\alpha)$ is the dimension of the eigenspace associated with $\\lambda_{\\max}$. We analyze three cases based on the value of the parameter $\\alpha$.\n\nCase 1: $|\\alpha|  1$.\nIn this case, $\\alpha^2  1$. The largest eigenvalue is $\\lambda_{\\max} = \\alpha^2$. We need to find the dimension of the null space of the matrix $A(\\alpha)^T A(\\alpha) - \\lambda_{\\max}I$:\n$$\nA(\\alpha)^T A(\\alpha) - \\alpha^2 I = \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  1\n\\end{pmatrix} - \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  \\alpha^2\n\\end{pmatrix} = \\begin{pmatrix}\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  1 - \\alpha^2\n\\end{pmatrix}.\n$$\nThe eigenspace is the set of vectors $x = (x_1, x_2, x_3)^T$ such that $(A(\\alpha)^T A(\\alpha) - \\alpha^2 I)x = 0$. This gives the equation $(1 - \\alpha^2)x_3 = 0$. Since $\\alpha^2 \\neq 1$, we must have $x_3 = 0$. There are no constraints on $x_1$ and $x_2$. Thus, the eigenspace is spanned by the vectors $(1, 0, 0)^T$ and $(0, 1, 0)^T$. The dimension of this subspace is $2$.\nSo, for $|\\alpha|  1$, $\\dim(S(\\alpha)) = 2$.\n\nCase 2: $|\\alpha|  1$.\nIn this case, $\\alpha^2  1$. The largest eigenvalue is $\\lambda_{\\max} = 1$. We find the dimension of the null space of $A(\\alpha)^T A(\\alpha) - \\lambda_{\\max}I$:\n$$\nA(\\alpha)^T A(\\alpha) - 1 \\cdot I = \\begin{pmatrix}\n\\alpha^2  0  0 \\\\\n0  \\alpha^2  0 \\\\\n0  0  1\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n\\alpha^2 - 1  0  0 \\\\\n0  \\alpha^2 - 1  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nThe eigenspace is the set of vectors $x = (x_1, x_2, x_3)^T$ such that $(A(\\alpha)^T A(\\alpha) - I)x = 0$. This gives the equations $(\\alpha^2 - 1)x_1 = 0$ and $(\\alpha^2 - 1)x_2 = 0$. Since $\\alpha^2 \\neq 1$, we must have $x_1 = 0$ and $x_2 = 0$. There is no constraint on $x_3$. The eigenspace is spanned by the vector $(0, 0, 1)^T$. The dimension of this subspace is $1$.\nSo, for $|\\alpha|  1$, $\\dim(S(\\alpha)) = 1$.\n\nCase 3: $|\\alpha| = 1$.\nIn this case, $\\alpha^2 = 1$. The eigenvalues of $A(\\alpha)^T A(\\alpha)$ are $1$ (with multiplicity $2$) and $1$. Thus, there is only one eigenvalue $\\lambda = 1$ with algebraic multiplicity $3$. The largest eigenvalue is $\\lambda_{\\max} = 1$. We find the dimension of the null space of $A(\\alpha)^T A(\\alpha) - \\lambda_{\\max}I$:\n$$\nA(\\alpha)^T A(\\alpha) - 1 \\cdot I = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nThe equation $(A(\\alpha)^T A(\\alpha) - I)x = 0$ becomes $0 \\cdot x = 0$, which places no restriction on the vector $x \\in \\mathbb{R}^3$. The eigenspace is therefore the entire space $\\mathbb{R}^3$. The dimension of this subspace is $3$.\nSo, for $|\\alpha| = 1$, $\\dim(S(\\alpha)) = 3$.\n\nCombining all cases, we express the dimension of $S(\\alpha)$ as a piecewise function of $\\alpha$:\n$$\n\\dim(S(\\alpha)) = \\begin{cases}\n1  \\text{if } |\\alpha|  1 \\\\\n3  \\text{if } |\\alpha| = 1 \\\\\n2  \\text{if } |\\alpha|  1\n\\end{cases}\n$$\nThis function gives the dimension of the subspace $S(\\alpha)$ for any real value of $\\alpha$.",
            "answer": "$$\n\\boxed{\n\\begin{cases}\n1  \\text{if } |\\alpha|  1 \\\\\n3  \\text{if } |\\alpha| = 1 \\\\\n2  \\text{if } |\\alpha|  1\n\\end{cases}\n}\n$$"
        },
        {
            "introduction": "In practical applications, are all matrix norms created equal? This exercise explores that critical question within the context of numerical stability for ordinary differential equations. By analyzing a hypothetical scenario , you will compare the results obtained using the $1$-norm versus the $2$-norm to estimate amplification factors. The dramatic difference in the results will highlight a crucial lesson: the choice of norm is a non-trivial decision that can significantly impact the conclusions of a numerical analysis.",
            "id": "3242244",
            "problem": "Consider the forward Euler method applied to a linear Ordinary Differential Equation (ODE) of the form $\\frac{d\\mathbf{y}}{dt} = A \\mathbf{y}$, where $A \\in \\mathbb{R}^{n \\times n}$. An analyst proposes a crude norm-based stability check that bounds the per-step amplification by the induced matrix norm of $I + hA$, where $h  0$ is the time step. To study how different induced norms can lead to different conclusions about stability, let $n$ be a power of $2$, and define the Sylvester-type Hadamard matrix $H \\in \\mathbb{R}^{n \\times n}$ with entries in $\\{+1,-1\\}$ satisfying $H H^{\\top} = n I$ and $H = H^{\\top}$. Let $A = \\frac{1}{\\sqrt{n}} H$. Two analysts estimate amplification using the induced matrix $1$-norm $\\|I + hA\\|_{1}$ and the induced matrix $2$-norm $\\|I + hA\\|_{2}$, respectively.\n\nUsing only the definitions of the induced matrix $p$-norm, the vector $p$-norms, and basic spectral facts for symmetric orthogonal matrices, derive the exact closed-form expression (in terms of $n$ and $h$) for the ratio\n$$\nR(n,h) = \\frac{\\|I + hA\\|_{1}}{\\|I + hA\\|_{2}}.\n$$\nProvide the final expression for $R(n,h)$ as your answer. No rounding is required.",
            "solution": "The problem requires the derivation of the ratio $R(n,h) = \\frac{\\|I + hA\\|_{1}}{\\|I + hA\\|_{2}}$, where $A = \\frac{1}{\\sqrt{n}} H$. The matrix $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric Sylvester-type Hadamard matrix with entries in $\\{+1, -1\\}$ satisfying $H H^{\\top} = nI$, $n$ is a power of $2$, and $h0$.\n\nLet the matrix of interest be denoted by $M = I + hA = I + \\frac{h}{\\sqrt{n}} H$. Since the identity matrix $I$ and the Hadamard matrix $H$ are symmetric, the matrix $M$ is also symmetric.\n\nFirst, we compute the induced matrix $2$-norm, $\\|M\\|_{2}$. For a symmetric matrix, the induced $2$-norm is equal to its spectral radius, which is the maximum absolute value of its eigenvalues.\n$$\n\\|M\\|_{2} = \\rho(M) = \\max_i |\\lambda_i(M)|\n$$\nwhere $\\lambda_i(M)$ are the eigenvalues of $M$.\n\nThe eigenvalues of $M$ are related to the eigenvalues of $H$. Let $\\lambda$ be an eigenvalue of $H$ with a corresponding eigenvector $\\mathbf{v} \\neq \\mathbf{0}$, such that $H\\mathbf{v} = \\lambda\\mathbf{v}$.\nThe action of $M$ on $\\mathbf{v}$ is:\n$$\nM\\mathbf{v} = \\left(I + \\frac{h}{\\sqrt{n}} H\\right)\\mathbf{v} = I\\mathbf{v} + \\frac{h}{\\sqrt{n}} (H\\mathbf{v}) = \\mathbf{v} + \\frac{h}{\\sqrt{n}} (\\lambda\\mathbf{v}) = \\left(1 + \\frac{h\\lambda}{\\sqrt{n}}\\right)\\mathbf{v}\n$$\nThus, the eigenvalues of $M$ are of the form $1 + \\frac{h\\lambda}{\\sqrt{n}}$, where $\\lambda$ are the eigenvalues of $H$.\n\nTo find the eigenvalues of $H$, we use the given property $H H^{\\top} = nI$. Since $H$ is symmetric ($H=H^{\\top}$), this simplifies to $H^2 = nI$.\nLet $\\lambda$ be an eigenvalue of $H$. Then $H^2\\mathbf{v} = \\lambda^2\\mathbf{v}$. Also, $(nI)\\mathbf{v} = n\\mathbf{v}$.\nFrom $H^2 = nI$, we must have $\\lambda^2 = n$. This implies that the only possible eigenvalues of $H$ are $\\lambda = \\sqrt{n}$ and $\\lambda = -\\sqrt{n}$.\n\nThe matrix $A = \\frac{1}{\\sqrt{n}}H$ has eigenvalues $\\mu = \\frac{\\lambda}{\\sqrt{n}}$, which are therefore $\\mu=+1$ and $\\mu=-1$.\nConsequently, the eigenvalues of $M = I+hA$ are $1+h(+1) = 1+h$ and $1+h(-1) = 1-h$.\n\nThe spectral radius of $M$ is the maximum of the absolute values of these eigenvalues:\n$$\n\\rho(M) = \\max(|1+h|, |1-h|)\n$$\nSince $h0$, $1+h$ is always positive and greater than $1$. For any $h0$, the inequality $1+h  |1-h|$ holds:\n- If $0  h \\le 1$, then $1-h \\ge 0$, and we compare $1+h$ with $1-h$. $1+h  1-h$ because $2h0$.\n- If $h  1$, then $1-h  0$, and we compare $1+h$ with $-(1-h)=h-1$. $1+h  h-1$ because $20$.\nTherefore, for all $h0$, the maximum is $1+h$.\nSo, the induced matrix $2$-norm is:\n$$\n\\|I + hA\\|_{2} = 1+h\n$$\n\nNext, we compute the induced matrix $1$-norm, $\\|M\\|_{1}$. This norm is defined as the maximum absolute column sum:\n$$\n\\|M\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |M_{ij}|\n$$\nThe entries of $M = I + \\frac{h}{\\sqrt{n}} H$ are given by $M_{ij} = \\delta_{ij} + \\frac{h}{\\sqrt{n}} H_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nFor an arbitrary column $j$, the sum of the absolute values of its entries is:\n$$\n\\sum_{i=1}^{n} |M_{ij}| = |M_{jj}| + \\sum_{i=1, i \\ne j}^{n} |M_{ij}|\n$$\nSubstituting the expressions for the entries:\n- The diagonal entry is $M_{jj} = \\delta_{jj} + \\frac{h}{\\sqrt{n}} H_{jj} = 1 + \\frac{h}{\\sqrt{n}} H_{jj}$.\n- The off-diagonal entries ($i \\ne j$) are $M_{ij} = \\delta_{ij} + \\frac{h}{\\sqrt{n}} H_{ij} = \\frac{h}{\\sqrt{n}} H_{ij}$.\n\nThe sum for column $j$ becomes:\n$$\n\\sum_{i=1}^{n} |M_{ij}| = \\left|1 + \\frac{h}{\\sqrt{n}} H_{jj}\\right| + \\sum_{i=1, i \\ne j}^{n} \\left|\\frac{h}{\\sqrt{n}} H_{ij}\\right|\n$$\nThe entries of $H$ are $H_{ij} \\in \\{+1,-1\\}$, so $|H_{ij}|=1$. The sum simplifies to:\n$$\n\\sum_{i=1}^{n} |M_{ij}| = \\left|1 + \\frac{h}{\\sqrt{n}} H_{jj}\\right| + (n-1) \\frac{h}{\\sqrt{n}}\n$$\nThis column sum depends on the value of the diagonal entry $H_{jj}$, which can be either $+1$ or $-1$. Sylvester-type Hadamard matrices of order $n=2^k$ for $k \\ge 1$ have both $+1$ and $-1$ on their diagonals. For $n=1=2^0$, $H=[1]$ and $H_{11}=1$. In any case, a column with $H_{jj}=1$ always exists. To find the maximum column sum, we must consider both possibilities for $H_{jj}$.\n\nCase 1: $H_{jj} = +1$. The column sum is:\n$$\nS_{+} = \\left|1 + \\frac{h}{\\sqrt{n}}\\right| + (n-1) \\frac{h}{\\sqrt{n}} = 1 + \\frac{h}{\\sqrt{n}} + (n-1) \\frac{h}{\\sqrt{n}} = 1 + n\\frac{h}{\\sqrt{n}} = 1 + h\\sqrt{n}\n$$\n(since $h0$, $1+\\frac{h}{\\sqrt{n}}  0$).\n\nCase 2: $H_{jj} = -1$. The column sum is:\n$$\nS_{-} = \\left|1 - \\frac{h}{\\sqrt{n}}\\right| + (n-1) \\frac{h}{\\sqrt{n}}\n$$\n\nTo find $\\|M\\|_1$, we need to find $\\max(S_{+}, S_{-})$. Let us compare them.\n$S_{+} - S_{-} = (1 + h\\sqrt{n}) - \\left(\\left|1 - \\frac{h}{\\sqrt{n}}\\right| + (n-1) \\frac{h}{\\sqrt{n}}\\right)$\n$S_{+} - S_{-} = \\left(1 + \\frac{h}{\\sqrt{n}}\\right) - \\left|1 - \\frac{h}{\\sqrt{n}}\\right|$\nLet $x = \\frac{h}{\\sqrt{n}}$. Since $h0$ and $n \\ge 1$, we have $x0$. We need to compare $1+x$ with $|1-x|$. As established earlier when computing the spectral radius, for any $x0$, the inequality $1+x  |1-x|$ holds.\nThis implies that $S_{+}  S_{-}$.\n\nThe maximum column sum is therefore $S_{+}$. Thus, the induced matrix $1$-norm is:\n$$\n\\|I + hA\\|_{1} = 1 + h\\sqrt{n}\n$$\n\nFinally, we can compute the ratio $R(n,h)$:\n$$\nR(n,h) = \\frac{\\|I + hA\\|_{1}}{\\|I + hA\\|_{2}} = \\frac{1 + h\\sqrt{n}}{1+h}\n$$\nThis is the final closed-form expression for the ratio.",
            "answer": "$$\n\\boxed{\\frac{1 + h\\sqrt{n}}{1+h}}\n$$"
        }
    ]
}