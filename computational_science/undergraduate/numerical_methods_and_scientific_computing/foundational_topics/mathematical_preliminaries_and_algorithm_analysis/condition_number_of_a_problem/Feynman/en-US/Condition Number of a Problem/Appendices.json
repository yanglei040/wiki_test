{
    "hands_on_practices": [
        {
            "introduction": "A core skill in numerical analysis is the ability to assess a problem's sensitivity before attempting to solve it. This first exercise provides foundational practice by asking you to apply the formal definition of the relative condition number to a common computational task: finding a number $x$ from its exponential $y = \\exp(x)$. By deriving the condition number for the mapping $y \\mapsto \\ln(y)$, you will gain direct experience with the mechanics of the formula and learn to identify the input values for which this seemingly simple problem becomes ill-conditioned .",
            "id": "3216377",
            "problem": "A scalar measurement-to-solution problem is given by the model $y = \\exp(x)$ with $y > 0$, where the computational task is to determine $x$ from a given $y$. Using the definition of the relative condition number based on first-order perturbation analysis, derive an explicit expression for the relative condition number $\\kappa(y)$ of the mapping $y \\mapsto x$. Clearly state the domain where your expression applies, and identify for which values of $y$ this problem is ill-conditioned in the sense that small relative perturbations in $y$ can cause large relative perturbations in $x$. Give your final answer as a single symbolic expression for $\\kappa(y)$.",
            "solution": "The problem requires the derivation of the relative condition number for the computational task of determining $x$ from a given $y$, based on the model $y = \\exp(x)$.\n\nFirst, we must formalize the problem. The task is to compute $x$ as a function of $y$. We can express the relationship as $x = f(y)$. From the given model $y = \\exp(x)$, we can solve for $x$ by taking the natural logarithm of both sides:\n$$x = \\ln(y)$$\nThus, the function we are analyzing is $f(y) = \\ln(y)$. The domain of this function is $y  0$, which is consistent with the problem statement.\n\nThe relative condition number, $\\kappa(y)$, of a problem $x = f(y)$ is defined based on first-order perturbation analysis. It quantifies the sensitivity of the relative error in the output $x$ to the relative error in the input $y$. For a small perturbation $\\delta y$ in the input, the corresponding perturbation in the output $\\delta x$ is approximated by $\\delta x \\approx f'(y) \\delta y$. The relative condition number is the magnitude of the ratio of the relative change in the output to the relative change in the input, in the limit of infinitesimal perturbations:\n$$ \\kappa(y) = \\frac{|\\delta x / x|}{|\\delta y / y|} = \\left| \\frac{\\delta x}{\\delta y} \\frac{y}{x} \\right| $$\nIn the limit $\\delta y \\to 0$, the ratio $\\frac{\\delta x}{\\delta y}$ becomes the derivative $f'(y)$. Substituting $x = f(y)$, the formula becomes:\n$$ \\kappa(y) = \\left| f'(y) \\frac{y}{f(y)} \\right| = \\left| \\frac{y f'(y)}{f(y)} \\right| $$\nTo apply this formula, we need to compute the derivative of $f(y) = \\ln(y)$. The derivative is:\n$$ f'(y) = \\frac{d}{dy}(\\ln(y)) = \\frac{1}{y} $$\nNow we substitute $f(y) = \\ln(y)$ and $f'(y) = \\frac{1}{y}$ into the expression for the condition number:\n$$ \\kappa(y) = \\left| \\frac{y \\left(\\frac{1}{y}\\right)}{\\ln(y)} \\right| $$\nSimplifying the expression in the numerator gives:\n$$ \\kappa(y) = \\left| \\frac{1}{\\ln(y)} \\right| $$\nThis is the explicit expression for the relative condition number of the mapping $y \\mapsto x$.\n\nThe domain where this expression applies is determined by the domains of the functions involved. The function $f(y) = \\ln(y)$ is defined for all $y  0$. The expression for $\\kappa(y)$ has $\\ln(y)$ in the denominator, so it is undefined when $\\ln(y) = 0$. This occurs at $y = 1$. Therefore, the domain of the expression for $\\kappa(y)$ is $y \\in (0, 1) \\cup (1, \\infty)$.\n\nA problem is considered ill-conditioned if its condition number is very large, i.e., $\\kappa(y) \\gg 1$. This implies that small relative perturbations in the input $y$ can cause large relative perturbations in the output $x$. We must identify the values of $y$ for which $\\kappa(y)$ is large.\n$$ \\kappa(y) = \\left| \\frac{1}{\\ln(y)} \\right| $$\nThe value of $\\kappa(y)$ is large when the magnitude of its denominator, $|\\ln(y)|$, is small (close to $0$). The function $\\ln(y)$ approaches $0$ as $y$ approaches $1$.\n$$ \\lim_{y \\to 1} \\kappa(y) = \\lim_{y \\to 1} \\left| \\frac{1}{\\ln(y)} \\right| = \\infty $$\nTherefore, the problem is ill-conditioned for values of $y$ that are close to $1$. For such values, a small relative error in the measurement $y$ will be significantly amplified in the computed value of $x = \\ln(y)$. This makes intuitive sense: when $y$ is close to $1$, $x$ is close to $0$. The relative error in $x$, which is $\\frac{\\delta x}{x}$, becomes very sensitive to any non-zero absolute error $\\delta x$ when $x$ is near $0$.",
            "answer": "$$\\boxed{\\left| \\frac{1}{\\ln(y)} \\right|}$$"
        },
        {
            "introduction": "Building on the foundational calculation of a condition number, this next practice explores the crucial and often subtle distinction between an ill-conditioned problem and an unstable algorithm. You will analyze the task of computing $f(x) = \\sqrt{1+x}-1$ for values of $x$ near zero, a classic case study in numerical computing. This exercise  demonstrates that even when a problem is inherently well-conditioned (i.e., has a small condition number), a naive computational approach can lead to a disastrous loss of accuracy due to subtractive cancellation, reinforcing the need for careful algorithmic reformulation.",
            "id": "3216339",
            "problem": "Consider the scalar computational problem of evaluating the function $f(x)=\\sqrt{1+x}-1$ for inputs $x$ in the domain $x-1$ with $x$ near $0$. Your goal is to assess how sensitive the problem is to small perturbations in $x$ and to identify an algebraically equivalent formulation that mitigates loss of significance due to subtractive cancellation. Work from first principles: use the definition of conditioning as the ratio between relative output change and relative input change in the limit of vanishing perturbations, and use the differential relationship between small changes in input and output. Do not assume any pre-derived condition-number formula.\n\nTasks:\n- Derive the absolute conditioning of the problem at a point $x$ by relating an infinitesimal change $\\mathrm{d}x$ in the input to the corresponding change $\\mathrm{d}f$ in the output.\n- Derive the relative conditioning of the problem at a point $x$ by forming the ratio of relative output change to relative input change in the limit $\\mathrm{d}x \\to 0$.\n- Explain why the direct expression $f(x)=\\sqrt{1+x}-1$ suffers from catastrophic cancellation for small $x$, and construct an algebraically equivalent expression for $f(x)$ that avoids subtracting nearly equal numbers. Justify your construction by referencing elementary algebraic identities.\n- Using the standard first-order floating-point error model with unit roundoff $u$ (that is, each basic arithmetic operation on real numbers returns a result of the exact value multiplied by a factor $(1+\\delta)$ with $|\\delta|\\le u$), compare the leading-order behavior of the relative forward error of the direct expression and of your stable expression as $x \\to 0$.\n- As your final answer, provide the simplified closed-form expression for the relative condition number as a function of $x$.\n\nNo numerical approximation is required; express your final answer as a single exact analytic expression with no units.",
            "solution": "First, we derive the absolute conditioning of the problem. Absolute conditioning relates an absolute change in the input to the corresponding absolute change in the output. For a differentiable scalar function $f(x)$, an infinitesimal change in input, $\\mathrm{d}x$, leads to an infinitesimal change in output, $\\mathrm{d}f$, given by the first-order approximation $\\mathrm{d}f = f'(x) \\mathrm{d}x$. The absolute condition number, $\\kappa_{abs}(x)$, is the magnitude of the sensitivity of the output to this change, which is $|f'(x)|$.\nThe function is $f(x) = (1+x)^{1/2} - 1$. Its derivative with respect to $x$ is:\n$$ f'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( (1+x)^{1/2} - 1 \\right) = \\frac{1}{2} (1+x)^{-1/2} = \\frac{1}{2\\sqrt{1+x}} $$\nSince the domain is $x  -1$, the term $\\sqrt{1+x}$ is real and positive, so we do not need an absolute value for $f'(x)$.\nThe absolute condition number is:\n$$ \\kappa_{abs}(x) = |f'(x)| = \\frac{1}{2\\sqrt{1+x}} $$\nFor $x$ near $0$, $\\kappa_{abs}(x) \\approx \\frac{1}{2}$. This indicates that small absolute errors in $x$ are not amplified significantly, suggesting the problem is well-conditioned in an absolute sense.\n\nNext, we derive the relative conditioning of the problem. The relative condition number, $\\kappa_{rel}(x)$, is defined as the limiting ratio of the relative change in the output to the relative change in the input.\n$$ \\kappa_{rel}(x) = \\lim_{\\Delta x \\to 0} \\frac{|\\Delta f / f(x)|}{|\\Delta x / x|} = \\frac{|\\mathrm{d}f / f(x)|}{|\\mathrm{d}x / x|} = \\left| \\frac{\\mathrm{d}f}{\\mathrm{d}x} \\cdot \\frac{x}{f(x)} \\right| = |f'(x)| \\left| \\frac{x}{f(x)} \\right| $$\nSubstituting the expressions for $f'(x)$ and $f(x)$:\n$$ \\kappa_{rel}(x) = \\left| \\frac{1}{2\\sqrt{1+x}} \\cdot \\frac{x}{\\sqrt{1+x}-1} \\right| $$\nThis expression is indeterminate for $x=0$ and appears to grow large for small $x$. However, we can simplify it by multiplying the numerator and denominator by the conjugate of the term in the denominator, which is $\\sqrt{1+x}+1$:\n$$ \\kappa_{rel}(x) = \\left| \\frac{1}{2\\sqrt{1+x}} \\cdot \\frac{x}{\\sqrt{1+x}-1} \\cdot \\frac{\\sqrt{1+x}+1}{\\sqrt{1+x}+1} \\right| = \\left| \\frac{x(\\sqrt{1+x}+1)}{2\\sqrt{1+x} \\left( (\\sqrt{1+x})^2 - 1^2 \\right)} \\right| $$\n$$ \\kappa_{rel}(x) = \\left| \\frac{x(\\sqrt{1+x}+1)}{2\\sqrt{1+x} (1+x-1)} \\right| = \\left| \\frac{x(\\sqrt{1+x}+1)}{2x\\sqrt{1+x}} \\right| $$\nFor $x \\neq 0$, we can cancel the factor of $x$:\n$$ \\kappa_{rel}(x) = \\left| \\frac{\\sqrt{1+x}+1}{2\\sqrt{1+x}} \\right| $$\nSince $x  -1$, both the numerator and denominator are positive, so the absolute value is redundant. The final simplified form for the relative condition number is:\n$$ \\kappa_{rel}(x) = \\frac{\\sqrt{1+x}+1}{2\\sqrt{1+x}} $$\nTo evaluate the conditioning near $x=0$, we take the limit:\n$$ \\lim_{x \\to 0} \\kappa_{rel}(x) = \\lim_{x \\to 0} \\frac{\\sqrt{1+x}+1}{2\\sqrt{1+x}} = \\frac{\\sqrt{1+0}+1}{2\\sqrt{1+0}} = \\frac{1+1}{2} = 1 $$\nA relative condition number of approximately $1$ indicates that the problem of computing $f(x)$ is inherently well-conditioned for $x$ near $0$. The relative error in the output should be about the same magnitude as the relative error in the input.\n\nThe issue of \"loss of significance\" arises not from the problem's conditioning but from the specific computational algorithm used. The direct expression $f(x) = \\sqrt{1+x}-1$ suffers from catastrophic cancellation when evaluated in floating-point arithmetic for $x$ near $0$. In this case, $1+x$ is slightly greater than $1$, so $\\sqrt{1+x}$ is a number very close to $1$. The subtraction $\\sqrt{1+x}-1$ involves two nearly equal numbers. In floating-point representation, the leading significant digits of these two numbers are identical and cancel out, leaving a result dominated by the less significant digits, which are contaminated by rounding errors from the previous steps (like computing the square root). This leads to a severe loss of relative accuracy.\n\nTo create a stable algorithm, we must find an algebraically equivalent expression that avoids this subtraction. The simplification used to derive the condition number provides such an expression. By multiplying by the conjugate, we found:\n$$ f(x) = (\\sqrt{1+x}-1) \\cdot \\frac{\\sqrt{1+x}+1}{\\sqrt{1+x}+1} = \\frac{(1+x)-1}{\\sqrt{1+x}+1} = \\frac{x}{\\sqrt{1+x}+1} $$\nThis alternate form, let's call it $g(x) = \\frac{x}{\\sqrt{1+x}+1}$, is algebraically equivalent to $f(x)$ for all $x \\in (-1, \\infty)$. Computationally, it is far superior for small $x$. For $x \\approx 0$, the denominator $\\sqrt{1+x}+1$ is approximately $2$. The expression involves an addition and a division, neither of which causes loss of significance in this context.\n\nFinally, we compare the forward error propagation. Let $\\text{fl}(\\cdot)$ denote the result of a floating-point computation, and let $u$ be the unit roundoff. Each basic arithmetic operation introduces a relative error of at most $u$.\n\nFor the unstable form $f(x) = \\sqrt{1+x}-1$, the main error comes from evaluating the square root and then the subtraction. Let the computed value of the square root be $\\hat{y} = \\text{fl}(\\sqrt{1+x}) = \\sqrt{1+x}(1+\\epsilon)$, where $|\\epsilon| \\lesssim u$. The final computed result is $\\hat{f}(x) = \\text{fl}(\\hat{y}-1) = (\\hat{y}-1)(1+\\delta) = (\\sqrt{1+x}(1+\\epsilon)-1)(1+\\delta)$, where $|\\delta| \\le u$.\n$$ \\hat{f}(x) = (\\sqrt{1+x}-1 + \\epsilon\\sqrt{1+x})(1+\\delta) \\approx f(x) + \\epsilon\\sqrt{1+x} $$\nThe absolute error is $\\hat{f}(x)-f(x) \\approx \\epsilon\\sqrt{1+x}$. The relative forward error is:\n$$ \\frac{|\\hat{f}(x)-f(x)|}{|f(x)|} \\approx \\frac{|\\epsilon\\sqrt{1+x}|}{|f(x)|} $$\nFor small $x$, we know $f(x) \\approx x/2$ and $\\sqrt{1+x} \\approx 1$. Thus, the relative error behaves as:\n$$ \\frac{|\\epsilon \\cdot 1|}{|x/2|} = \\frac{2|\\epsilon|}{|x|} $$\nSince $|\\epsilon|$ is on the order of $u$, the relative error is of order $O(u/|x|)$. As $x \\to 0$, this error grows without bound, demonstrating the instability of the algorithm.\n\nFor the stable form $g(x) = \\frac{x}{\\sqrt{1+x}+1}$, the computation proceeds as $\\hat{d} = \\text{fl}(\\text{fl}(\\sqrt{1+x})+1)$ and then $\\hat{g}(x) = \\text{fl}(x/\\hat{d})$. Following a similar analysis, the total relative error accumulates from several stable operations.\nLet $\\hat{d} = (\\sqrt{1+x}(1+\\epsilon_1)+1)(1+\\epsilon_2) = (\\sqrt{1+x}+1)(1+\\frac{\\epsilon_1\\sqrt{1+x}}{\\sqrt{1+x}+1})(1+\\epsilon_2)$.\nThe final computed value is $\\hat{g}(x) = \\frac{x}{\\hat{d}}(1+\\epsilon_3) \\approx \\frac{x}{(\\sqrt{1+x}+1)} \\frac{1+\\epsilon_3}{(1+\\epsilon_2)(1+\\frac{\\epsilon_1\\sqrt{1+x}}{\\sqrt{1+x}+1})}$.\n$$ \\hat{g}(x) \\approx g(x) (1+\\epsilon_3-\\epsilon_2 - \\frac{\\epsilon_1\\sqrt{1+x}}{\\sqrt{1+x}+1}) $$\nThe relative error is $\\frac{|\\hat{g}(x)-g(x)|}{|g(x)|} \\approx |\\epsilon_3-\\epsilon_2 - \\frac{\\epsilon_1\\sqrt{1+x}}{\\sqrt{1+x}+1}|$.\nAs $x \\to 0$, this error approaches $|\\epsilon_3-\\epsilon_2 - \\epsilon_1/2|$, which is bounded by a small multiple of the unit roundoff $u$. The relative error does not grow as $x \\to 0$, demonstrating that this algorithm is stable.\n\nThe final answer requested is the closed-form expression for the relative condition number.",
            "answer": "$$ \\boxed{\\frac{\\sqrt{1+x} + 1}{2\\sqrt{1+x}}} $$"
        },
        {
            "introduction": "We now move from the analysis of scalar functions to one of the most fundamental problems in scientific computing: solving systems of linear equations $A x = b$. This hands-on programming exercise  allows you to empirically witness the dramatic consequences of ill-conditioning in a real-world scenario. By using the notoriously ill-conditioned Hilbert matrix, you will observe how the theoretical relationship between the condition number, backward error, and forward error plays out in floating-point arithmetic, leading to a catastrophic loss of significant digits in the solution as the problem size increases.",
            "id": "2428600",
            "problem": "Write a complete program that empirically demonstrates the effect of problem conditioning on the accuracy of solving linear systems in standard floating-point arithmetic. Consider the linear system $A x = b$ where $A$ is the $n \\times n$ Hilbert matrix with entries $A_{ij} = \\frac{1}{i + j - 1}$ for $1 \\le i,j \\le n$. For each test case, define the exact solution vector $x_{\\text{true}} \\in \\mathbb{R}^n$ by $x_{\\text{true}} = \\mathbf{1}$ (all ones), and construct the right-hand side $b = A x_{\\text{true}}$. Then compute the numerical solution $\\hat{x}$ using standard floating-point arithmetic conforming to Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision (binary$64$) semantics. Using $\\hat{x}$, compute the following quantities:\n- The $2$-norm condition number $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$.\n- The relative forward error in the $2$-norm, $\\frac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2}$.\n- The scaled residual (a normalized backward error proxy), $\\frac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\|\\hat{x}\\|_2 + \\|b\\|_2}$.\n- The estimated number of correct decimal digits in the solution, defined as $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, where $\\varepsilon$ denotes the smallest positive normalized IEEE $754$ double-precision number.\n\nTest Suite:\nEvaluate the above for the following problem sizes $n$:\n- $n = 3$,\n- $n = 6$,\n- $n = 10$,\n- $n = 12$.\n\nAll vector and matrix norms are the spectral norm (that is, the matrix $2$-norm and the vector $2$-norm). Angles are not involved. No physical units are involved.\n\nYour program must produce a single line of output containing all test results as a comma-separated list enclosed in square brackets, where each test result is itself a list of the form `[n, κ₂(A), relative forward error, scaled residual, estimated digits]`. The final output must therefore be a single line representing a list of lists, with no additional text. For example, the structure must be similar to `[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]` but with the actual computed numbers in place of the placeholders.",
            "solution": "The problem statement presented is valid. It is a well-posed, scientifically grounded exercise in numerical linear algebra, designed to demonstrate the fundamental concepts of problem conditioning, forward error, and backward error. The problem is self-contained, with all necessary definitions and data provided. It does not violate any scientific principles, logic, or contain any ambiguities. We shall proceed with the solution.\n\nThe problem requires an empirical investigation into the effects of ill-conditioning on the solution of a linear system of equations, $A x = b$. The matrix $A$ is chosen to be the $n \\times n$ Hilbert matrix, a classic example of a severely ill-conditioned matrix. Its entries are given by $A_{ij} = \\frac{1}{i + j - 1}$ for $i, j$ from $1$ to $n$.\n\nThe core of numerical analysis is not only to compute a solution but also to understand its accuracy. The accuracy of the computed solution, which we denote $\\hat{x}$, is affected by two main factors: the stability of the algorithm used and the intrinsic sensitivity of the problem itself. This sensitivity is quantified by the condition number.\n\nFor a linear system $A x = b$, the $2$-norm condition number of the matrix $A$ is defined as:\n$$ \\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 $$\nwhere $\\| \\cdot \\|_2$ is the spectral norm (the largest singular value). A large condition number, $\\kappa_2(A) \\gg 1$, signifies an ill-conditioned problem, where small relative perturbations in the input data ($A$ or $b$) can lead to large relative changes in the solution $x$.\n\nWhen we solve $A x = b$ using floating-point arithmetic, round-off errors are inevitably introduced. A backward stable algorithm, such as the LU decomposition employed by standard solvers, produces a computed solution $\\hat{x}$ that is the exact solution to a slightly perturbed problem:\n$$ (A + \\delta A) \\hat{x} = b + \\delta b $$\nThe \"smallness\" of these perturbations $\\delta A$ and $\\delta b$ is a measure of the algorithm's backward stability. A key result in numerical analysis establishes the following bound on the relative forward error:\n$$ \\frac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2} \\le \\kappa_2(A) \\left( \\frac{\\|\\delta A\\|_2}{\\|A\\|_2} + \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} \\right) $$\nThe right-hand side parenthetical term is the relative backward error. For a backward stable algorithm operating with machine precision $\\varepsilon_{\\text{mach}}$, the backward error is typically of order $\\mathcal{O}(\\varepsilon_{\\text{mach}})$. Machine precision for IEEE $754$ double-precision is approximately $2.22 \\times 10^{-16}$. Therefore, we expect the relative forward error to be bounded by approximately $\\kappa_2(A) \\cdot \\varepsilon_{\\text{mach}}$. This demonstrates that a large condition number amplifies the unavoidable round-off errors, potentially destroying the accuracy of the solution.\n\nThe residual vector is defined as $r = b - A \\hat{x}$. The norm of the residual, $\\|r\\|_2$, is related to the backward error. The problem asks for a specific scaled residual:\n$$ \\frac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\|\\hat{x}\\|_2 + \\|b\\|_2} $$\nThis quantity serves as a normalized proxy for the backward error. Due to the backward stability of the solver, we expect this value to remain small, on the order of $\\varepsilon_{\\text{mach}}$, even as the condition number grows and the forward error explodes. This is a crucial distinction: a small residual does not guarantee a small forward error.\n\nFinally, we estimate the number of correct decimal digits in the solution. This is directly related to the relative forward error. If the relative error is $10^{-k}$, the solution is accurate to roughly $k$ decimal digits. The given formula, $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, formalizes this. The term $\\varepsilon$ is the smallest positive normalized double-precision number, approximately $2.225 \\times 10^{-308}$, which prevents a logarithm of zero and handles cases where the error is smaller than representable precision allows.\n\nWe will now perform these calculations for the specified test suite of matrix sizes $n \\in \\{3, 6, 10, 12\\}$. For each $n$:\n1.  Construct the $n \\times n$ Hilbert matrix $A$.\n2.  Define the true solution $x_{\\text{true}}$ as a vector of $n$ ones.\n3.  Compute the right-hand side $b = A x_{\\text{true}}$.\n4.  Solve for the numerical solution $\\hat{x}$ using a standard linear solver.\n5.  Compute the four specified quantities: $\\kappa_2(A)$, relative forward error, scaled residual, and estimated digits.\n\nThe results will empirically validate the theory. The condition number of the Hilbert matrix grows extremely rapidly with $n$. For small $n$ (e.g., $n=3$), $\\kappa_2(A)$ is moderate, and we expect a reasonably accurate solution. As $n$ increases to $10$ and $12$, $\\kappa_2(A)$ will become enormous ($ 10^{13}$), leading to a relative forward error of order $1$ or greater, signifying a complete loss of accuracy. Throughout this process, the scaled residual should remain small, demonstrating the backward stability of the algorithm in contrast to the poor forward accuracy for an ill-conditioned problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hilbert\n\ndef solve():\n    \"\"\"\n    Empirically demonstrates the effect of problem conditioning on the accuracy\n    of solving linear systems Ax = b using the Hilbert matrix.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [3, 6, 10, 12]\n\n    results = []\n    \n    # Epsilon as defined in the problem: the smallest positive normalized\n    # IEEE 754 double-precision number.\n    smallest_norm_val = np.finfo(np.float64).smallest_normal\n\n    for n in test_cases:\n        # Step 1: Construct the n x n Hilbert matrix A.\n        # The problem statement uses 1-based indexing for A_ij = 1/(i+j-1).\n        # `scipy.linalg.hilbert(n)` produces this exact matrix.\n        A = hilbert(n)\n\n        # Step 2: Define the exact solution vector x_true (all ones).\n        x_true = np.ones(n)\n\n        # Step 3: Construct the right-hand side b = A * x_true.\n        # This ensures that b is consistent with A and x_true.\n        b = A @ x_true\n\n        # Step 4: Compute the numerical solution x_hat using a standard solver.\n        # numpy.linalg.solve uses LAPACK routines which are backward stable and\n        # operate in IEEE 754 double precision.\n        x_hat = np.linalg.solve(A, b)\n\n        # --- Calculate the required quantities ---\n\n        # The 2-norm condition number kappa_2(A).\n        kappa_2_A = np.linalg.cond(A, 2)\n\n        # The relative forward error in the 2-norm.\n        norm_x_true = np.linalg.norm(x_true, 2)\n        if norm_x_true == 0:\n            # Avoid division by zero, though not possible for x_true = 1.\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2)\n        else:\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2) / norm_x_true\n        \n        # The scaled residual (a normalized backward error proxy).\n        # residual = b - A @ x_hat\n        # norm_residual = ||b - A*x_hat||_2\n        # scaled_residual = norm_residual / (||A||_2 * ||x_hat||_2 + ||b||_2)\n        norm_A = np.linalg.norm(A, 2)\n        norm_x_hat = np.linalg.norm(x_hat, 2)\n        norm_b = np.linalg.norm(b, 2)\n        norm_residual = np.linalg.norm(b - A @ x_hat, 2)\n        \n        denominator = norm_A * norm_x_hat + norm_b\n        if denominator == 0:\n            # Handle potential division by zero.\n            scaled_res = norm_residual\n        else:\n            scaled_res = norm_residual / denominator\n\n        # The estimated number of correct decimal digits.\n        # est_digits = max(0, -log10(max(relative forward error, epsilon)))\n        log_val = max(rel_fwd_err, smallest_norm_val)\n        est_digits = max(0.0, -np.log10(log_val))\n        \n        # Store results for this test case.\n        results.append([n, kappa_2_A, rel_fwd_err, scaled_res, est_digits])\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a list of lists.\n    # Example: [[3, 5.2e+02, ...], [6, 1.5e+07, ...], ...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}