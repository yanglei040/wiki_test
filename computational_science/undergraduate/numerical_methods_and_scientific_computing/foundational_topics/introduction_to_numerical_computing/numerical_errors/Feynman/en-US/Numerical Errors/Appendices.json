{
    "hands_on_practices": [
        {
            "introduction": "The quadratic formula is a cornerstone of algebra, yet its direct implementation in floating-point arithmetic can lead to surprisingly inaccurate results. This exercise explores the phenomenon of catastrophic cancellation, which occurs when subtracting two nearly equal numbers, leading to a significant loss of precision. By deriving and implementing a stabilized algorithm based on Vieta's relations, you will gain practical experience in diagnosing numerical weaknesses in standard formulas and reformulating them for robustness, a critical skill in scientific computing .",
            "id": "3258060",
            "problem": "Consider the quadratic equation $a x^2 + b x + c = 0$ and analyze numerical error when computing its real roots in finite-precision arithmetic. The foundational base for this analysis must start from two facts: (i) Vieta’s relations for the roots $r_1$ and $r_2$ of a quadratic, namely $r_1 + r_2 = -\\frac{b}{a}$ and $r_1 r_2 = \\frac{c}{a}$, and (ii) the standard model of floating-point rounding in the Institute of Electrical and Electronics Engineers (IEEE) 754 binary floating-point arithmetic, in which basic operations (addition, subtraction, multiplication, and division) are rounded with a relative error bounded in magnitude by the unit roundoff $u$, where $u$ depends on the binary precision of the format. Your task is to reason from these bases to study loss of significance when $b^2 \\approx 4ac$, and to design and implement a numerically stabilized algorithm for computing the roots that avoids subtractive cancellation in the numerator when appropriate.\n\nSpecifically, perform the following:\n\n1. Analyze how computing the discriminant $\\Delta = b^2 - 4ac$ in finite precision can suffer large relative error when $b^2$ and $4ac$ are nearly equal, and explain why subtracting nearly equal quantities amplifies relative error in $\\Delta$.\n\n2. Using only the base facts stated above (Vieta’s relations and the floating-point rounding model), derive a root computation strategy that avoids subtractive cancellation in the numerator. Your derivation must proceed from the identities $r_1 + r_2 = -\\frac{b}{a}$ and $r_1 r_2 = \\frac{c}{a}$ without introducing any shortcut formulas in the problem statement. The strategy should compute one root in a way that avoids cancellation and then obtain the second root using the product-of-roots identity.\n\n3. Implement two methods in a single program:\n   - The direct method that computes both roots using the textbook quadratic formula with double-precision arithmetic: compute $\\Delta$, then compute $r_1$ and $r_2$ from $\\frac{-b \\pm \\sqrt{\\Delta}}{2a}$.\n   - The alternative method derived in step $2$, implemented with double-precision arithmetic, that avoids subtractive cancellation for one root and uses the product-of-roots identity to recover the second root.\n\n4. For validation, use a high-precision reference computed in base-$10$ arbitrary precision arithmetic to approximate the true real roots. For this, employ the Python standard library’s decimal arithmetic with at least $80$ digits of precision. Use the high-precision reference to quantify the maximum relative error of each double-precision method across the two roots. For a pair of computed roots $(\\tilde{r}_1, \\tilde{r}_2)$ and reference roots $(r_1, r_2)$, define the relative error of each root as $\\frac{|\\tilde{r}-r|}{\\max(1, |r|)}$, and define the score for the method as the maximum of the two root errors. Because root ordering may differ between methods, match computed roots to references in the way that minimizes the maximum relative error.\n\n5. Use the following test suite of parameter values, which covers nearly equal roots, boundary conditions, large-scale coefficients, and a classic cancellation scenario. Each triple is $(a,b,c)$, and all quantities are dimensionless:\n   - Test $1$: $a = 1$, $b = 2$, $c = 1 - 10^{-15}$.\n   - Test $2$: $a = 1$, $b = 2$, $c = 1 - 10^{-30}$.\n   - Test $3$: $a = 1$, $b = 10^8$, $c = \\frac{b^2}{4}\\left(1 - 10^{-16}\\right)$.\n   - Test $4$: $a = 1$, $b = 10^8$, $c = 1$.\n   - Test $5$: $a = -1$, $b = 2$, $c = -1 + 10^{-15}$.\n\n6. Your program should produce a single line of output containing, for each test case and in the same order, a boolean indicating whether the alternative method achieves a strictly smaller maximum relative error than the direct method. The required final output format is a single line with a Python-style list of booleans, for example, `[True,False,...]`. No other text may be printed.\n\nNo physical units or angle units are involved. All error quantities are pure numbers and must be computed and reported as decimals (no percentage signs). The implementation must be self-contained and must not read input or access any external resources.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the well-established domain of numerical analysis, specifically concerning floating-point arithmetic and algorithm stability. The problem is well-posed, with all necessary data, definitions, and constraints provided for a unique and verifiable solution. The language is objective and formal.\n\nThe task is to analyze numerical errors in solving the quadratic equation $a x^2 + b x + c = 0$ and to implement and compare two methods for finding its real roots: the standard direct formula and a numerically stabilized alternative derived from Vieta's relations.\n\n### Part 1: Analysis of Numerical Error in the Discriminant\n\nThe standard model for IEEE 754 floating-point arithmetic states that for any basic arithmetic operation $\\circ \\in \\{+, -, \\times, \\div\\}$, the floating-point result of operating on two real numbers $x$ and $y$ is given by $fl(x \\circ y) = (x \\circ y)(1 + \\delta)$, where the relative error $\\delta$ is bounded in magnitude by the unit roundoff $u$ (approximately $1.11 \\times 10^{-16}$ for double precision).\n\nWe are to analyze the computation of the discriminant $\\Delta = b^2 - 4ac$ when $b^2 \\approx 4ac$. Let $X = b^2$ and $Y = 4ac$. In finite precision, these quantities are computed with some error. Let $\\tilde{X}$ and $\\tilde{Y}$ be their floating-point representations. We can model these as:\n$$ \\tilde{X} = fl(b^2) = b^2(1 + \\delta_1) $$\n$$ \\tilde{Y} = fl(4ac) = 4ac(1 + \\delta_2) $$\nwhere $|\\delta_1|$ and $|\\delta_2|$ are on the order of $u$. The subtraction is then computed as:\n$$ \\tilde{\\Delta} = fl(\\tilde{X} - \\tilde{Y}) = (\\tilde{X} - \\tilde{Y})(1 + \\delta_3) \\quad \\text{with } |\\delta_3| \\le u $$\nSubstituting the expressions for $\\tilde{X}$ and $\\tilde{Y}$:\n$$ \\tilde{\\Delta} = (b^2(1 + \\delta_1) - 4ac(1 + \\delta_2))(1 + \\delta_3) $$\n$$ \\tilde{\\Delta} = (b^2 - 4ac + b^2\\delta_1 - 4ac\\delta_2)(1 + \\delta_3) $$\nIgnoring higher-order error terms, the absolute error in $\\Delta$ is:\n$$ E_{abs} = \\tilde{\\Delta} - \\Delta \\approx b^2\\delta_1 - 4ac\\delta_2 $$\nThe relative error is $E_{rel} = \\frac{E_{abs}}{\\Delta}$:\n$$ E_{rel} \\approx \\frac{b^2\\delta_1 - 4ac\\delta_2}{b^2 - 4ac} $$\nWhen $b^2 \\approx 4ac$, the denominator $b^2 - 4ac$ becomes very small. The numerator, however, is approximately $b^2(\\delta_1 - \\delta_2)$. Since $\\delta_1$ and $\\delta_2$ are independent rounding errors, their difference is generally not zero. Therefore, we are dividing a typical-sized error by a very small quantity, which can cause the relative error $E_{rel}$ to become arbitrarily large. This phenomenon is known as subtractive cancellation or loss of significance. If $b^2$ and $4ac$ are so close that their difference is smaller than the precision of the floating-point format, the computed discriminant $\\tilde{\\Delta}$ may even become zero, losing all information about the true, small, non-zero difference.\n\n### Part 2: Derivation of a Stabilized Root-Finding Algorithm\n\nThe standard quadratic formula provides the roots $r_{1}, r_{2}$ as:\n$$ r = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-b \\pm \\sqrt{\\Delta}}{2a} $$\nA different numerical instability can occur in the numerator, $-b \\pm \\sqrt{\\Delta}$, when $\\sqrt{\\Delta}$ is close to $|b|$. This happens when $\\Delta = b^2 - 4ac$ is close to $b^2$, which implies that $|4ac| \\ll b^2$.\n\nLet's assume $b>0$. The term $\\sqrt{\\Delta} = \\sqrt{b^2 - 4ac} = b\\sqrt{1-4ac/b^2} \\approx b(1 - 2ac/b^2) = b - 2ac/b$. In this case, $\\sqrt{\\Delta} \\approx b$.\n- The calculation $-b - \\sqrt{\\Delta}$ involves summing two negative numbers, which is numerically stable.\n- The calculation $-b + \\sqrt{\\Delta}$ involves subtracting two nearly equal positive numbers, leading to catastrophic subtractive cancellation.\n\nIf $b<0$, then $-b>0$, and the situation is reversed: $-b + \\sqrt{\\Delta}$ is stable (sum of positives), while $-b - \\sqrt{\\Delta}$ suffers from cancellation.\n\nTo build a stable algorithm, we must always choose the operation in the numerator that constitutes an effective addition (i.e., avoids subtracting quantities of like sign and similar magnitude). This can be achieved by choosing the sign of the radical to match the sign of $-b$. A compact way to write this is using the sign function, $\\text{sign}(b)$:\n$$ r_1 = \\frac{-b - \\text{sign}(b)\\sqrt{\\Delta}}{2a} $$\nThis formula computes one root, $r_1$, with high accuracy, regardless of the values of $a$, $b$, and $c$ (assuming $\\Delta \\ge 0$).\n\nNow, we use Vieta's relations to find the second root, $r_2$. The problem mandates this foundational approach. The relations are:\n1. $r_1 + r_2 = -b/a$\n2. $r_1 r_2 = c/a$\n\nIf we were to use the sum relation, we would find $r_2 = -b/a - r_1$. Substituting our stable expression for $r_1$:\n$$ r_2 = -\\frac{b}{a} - \\frac{-b - \\text{sign}(b)\\sqrt{\\Delta}}{2a} = \\frac{-2b - (-b - \\text{sign}(b)\\sqrt{\\Delta})}{2a} = \\frac{-b + \\text{sign}(b)\\sqrt{\\Delta}}{2a} $$\nThis derivation leads directly back to the unstable formula we sought to avoid.\n\nTherefore, we must use the product relation, $r_1 r_2 = c/a$. This yields:\n$$ r_2 = \\frac{c/a}{r_1} = \\frac{c}{a r_1} $$\nThis computation involves division by the accurately computed root $r_1$. Since division is a numerically stable operation in floating-point arithmetic, this method will yield an accurate value for $r_2$.\n\nThe stabilized algorithm is thus:\n1. Compute $\\Delta = b^2 - 4ac$.\n2. Compute the stable root $r_1 = \\frac{-b - \\text{sign}(b)\\sqrt{\\Delta}}{2a}$.\n3. Compute the second root using Vieta's product rule: $r_2 = \\frac{c}{ar_1}$.\n\n### Part 3 & 4: Implementation and Validation\n\nThe implementation will consist of three components for each test case $(a,b,c)$:\n1.  **High-Precision Reference:** The true roots are approximated using Python's `decimal` library with $80$-digit precision to provide a baseline for error measurement.\n2.  **Direct Method:** Both roots are computed using the textbook formula $r = \\frac{-b \\pm \\sqrt{\\Delta}}{2a}$ in standard double-precision floating-point arithmetic.\n3.  **Alternative Method:** The stabilized algorithm derived above is implemented in double-precision arithmetic.\n\nThe performance of each method is scored by its maximum relative error. For a computed root $\\tilde{r}$ and reference root $r$, the relative error is $\\frac{|\\tilde{r}-r|}{\\max(1, |r|)}$. This metric robustly handles roots that are close to zero. Since the ordering of roots $(\\tilde{r}_1, \\tilde{r}_2)$ may not match the reference ordering $(r_1, r_2)$, we test both possible pairings and choose the one that minimizes the maximum error of the pair. Finally, a boolean indicates whether the alternative method's score (maximum relative error) is strictly less than the direct method's score. The program will output a list of these booleans for the given test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares the numerical accuracy of two methods for solving\n    quadratic equations, producing a boolean list indicating if the alternative\n    method is more accurate than the direct method for a given test suite.\n    \"\"\"\n    # 1. Set up high precision context and test cases\n    getcontext().prec = 80\n\n    # Define test cases for double-precision calculations\n    b3_float = 1e8\n    c3_float = (b3_float**2 / 4.0) * (1.0 - 1e-16)\n    test_cases_float = [\n        (1.0, 2.0, 1.0 - 1e-15),\n        (1.0, 2.0, 1.0 - 1e-30),\n        (1.0, b3_float, c3_float),\n        (1.0, 1e8, 1.0),\n        (-1.0, 2.0, -1.0 + 1e-15),\n    ]\n\n    # Define corresponding test cases for high-precision reference calculations\n    b3_dec = Decimal('1e8')\n    c3_dec = (b3_dec**2 / Decimal(4)) * (Decimal(1) - Decimal('1e-16'))\n    test_cases_dec = [\n        (Decimal('1'), Decimal('2'), Decimal('1') - Decimal('1e-15')),\n        (Decimal('1'), Decimal('2'), Decimal('1') - Decimal('1e-30')),\n        (Decimal('1'), b3_dec, c3_dec),\n        (Decimal('1'), Decimal('1e8'), Decimal('1')),\n        (Decimal('-1'), Decimal('2'), Decimal('-1') + Decimal('1e-15')),\n    ]\n\n    results = []\n\n    for i in range(len(test_cases_float)):\n        a_f, b_f, c_f = test_cases_float[i]\n        a_d, b_d, c_d = test_cases_dec[i]\n\n        # 2. High-precision reference calculation\n        delta_d = b_d**2 - 4 * a_d * c_d\n        if delta_d < 0:\n            # This should not occur for the given test cases, as they are\n            # designed to have real roots. If it did happen due to some\n            # unforeseen issue, we treat the roots as coalescing at -b/(2a).\n            sqrt_delta_d = Decimal(0)\n        else:\n            sqrt_delta_d = delta_d.sqrt()\n        \n        ref_r1 = (-b_d + sqrt_delta_d) / (2 * a_d)\n        ref_r2 = (-b_d - sqrt_delta_d) / (2 * a_d)\n        ref_roots = [ref_r1, ref_r2]\n\n        # 3. Direct method (double-precision)\n        delta_f = b_f**2 - 4 * a_f * c_f\n        # Prevent math domain error for sqrt from small negative delta due to rounding\n        if delta_f < 0:\n            delta_f = 0.0\n        sqrt_delta_f = np.sqrt(delta_f)\n        \n        direct_r1 = (-b_f + sqrt_delta_f) / (2 * a_f)\n        direct_r2 = (-b_f - sqrt_delta_f) / (2 * a_f)\n        direct_roots = [direct_r1, direct_r2]\n\n        # 4. Alternative stabilized method (double-precision)\n        # The numerator term avoids cancellation by matching the sign of the radical\n        # to the sign of -b. np.copysign(1.0, b_f) gives a 1.0 with the sign of b_f.\n        q = -b_f - np.copysign(1.0, b_f) * sqrt_delta_f\n        \n        # Handle the case where q can be zero (e.g., if b=0 and c=0)\n        if q == 0.0:\n            # Roots are both 0.\n            alt_r1 = 0.0\n            alt_r2 = 0.0\n        else:\n            # First root is calculated from the stable numerator q\n            alt_r1 = q / (2 * a_f)\n            # Second root is from Vieta's product relation: r1*r2 = c/a\n            alt_r2 = c_f / (a_f * alt_r1)\n        alt_roots = [alt_r1, alt_r2]\n\n        # 5. Error calculation and comparison\n        def calculate_max_rel_error(computed_roots, reference_roots):\n            # Using str() when converting float to Decimal is crucial to avoid\n            # representing the float's binary approximation error in the Decimal.\n            r1_comp_d = Decimal(str(computed_roots[0]))\n            r2_comp_d = Decimal(str(computed_roots[1]))\n            r1_ref, r2_ref = reference_roots\n\n            def rel_err(comp, ref):\n                # Use max(1, |ref|) as the denominator to handle roots near zero gracefully\n                denominator = max(Decimal(1), ref.copy_abs())\n                if denominator == 0:\n                    return Decimal(0) if comp == ref else Decimal('inf')\n                return (comp - ref).copy_abs() / denominator\n            \n            # Since root order is arbitrary, check both pairings and take the minimum max error.\n            # Pairing 1: (r1_comp, r1_ref), (r2_comp, r2_ref)\n            err11 = rel_err(r1_comp_d, r1_ref)\n            err22 = rel_err(r2_comp_d, r2_ref)\n            match1_max_err = max(err11, err22)\n            \n            # Pairing 2: (r1_comp, r2_ref), (r2_comp, r1_ref)\n            err12 = rel_err(r1_comp_d, r2_ref)\n            err21 = rel_err(r2_comp_d, r1_ref)\n            match2_max_err = max(err12, err21)\n            \n            return min(match1_max_err, match2_max_err)\n\n        direct_error = calculate_max_rel_error(direct_roots, ref_roots)\n        alt_error = calculate_max_rel_error(alt_roots, ref_roots)\n        \n        results.append(alt_error < direct_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Calculating a function's derivative numerically often involves approximations like the forward difference formula, which depends on a small step size, $h$. This choice of $h$ presents a classic numerical trade-off: if $h$ is too large, the mathematical approximation is poor (high truncation error), but if $h$ is too small, the calculation becomes dominated by floating-point noise (high roundoff error). This practice guides you through the analytical process of modeling these two competing error sources to find the optimal step size that minimizes their combined effect, revealing a fundamental balancing act in numerical algorithm design .",
            "id": "3258061",
            "problem": "Consider computing the derivative of the function $f(x) = \\sin x$ at the point $x=1$ using the forward difference formula\n$$\n\\frac{f(1+h) - f(1)}{h},\n$$\nwhere $h$ is a positive step size measured in radians. Assume arithmetic follows the Institute of Electrical and Electronics Engineers (IEEE) 754 double precision standard with rounding to nearest, and unit roundoff $u = 2^{-53}$. In modeling numerical error, use the following foundations:\n- The truncation error arises from Taylor’s theorem with Lagrange remainder applied to $f(1+h)$.\n- The roundoff error arises because each evaluation of $f$ is computed in floating-point arithmetic, so the values $f(1)$ and $f(1+h)$ are perturbed by unknown relative errors of size at most $u$, independently in each evaluation.\nDerive, from these bases, a leading-order expression for the total error as a function of $h$ that balances truncation and roundoff, and determine the step size $h$ that minimizes this leading-order error when $f(x)=\\sin x$ at $x=1$. Round your final numerical value of $h$ to four significant figures, and express it in radians.",
            "solution": "The problem requires the derivation of an optimal step size, $h$, for computing the derivative of $f(x) = \\sin x$ at $x=1$ using the forward difference formula. The optimization involves balancing truncation error and roundoff error based on a specified error model.\n\nThe forward difference approximation of the derivative $f'(1)$ is given by\n$$\nD_h f(1) = \\frac{f(1+h) - f(1)}{h}\n$$\nThe total error in the computed value of the derivative, $\\hat{D}_h f(1)$, is the difference between this computed value and the true value, $f'(1)$. This error can be decomposed into two primary components: truncation error and roundoff error.\n$$\n\\text{Total Error} = \\hat{D}_h f(1) - f'(1) = (\\hat{D}_h f(1) - D_h f(1)) + (D_h f(1) - f'(1))\n$$\nHere, $E_{round} = \\hat{D}_h f(1) - D_h f(1)$ is the roundoff error, and $E_{trunc} = D_h f(1) - f'(1)$ is the truncation error. A leading-order expression for the magnitude of the total error is sought by bounding the magnitudes of these two components.\n\nFirst, we analyze the truncation error, $E_{trunc}$. According to the problem, this is derived from Taylor's theorem with the Lagrange remainder. The Taylor expansion of $f(1+h)$ around $x=1$ is:\n$$\nf(1+h) = f(1) + f'(1)h + \\frac{f''(\\xi)}{2!}h^2\n$$\nfor some $\\xi \\in (1, 1+h)$. Substituting this into the forward difference formula:\n$$\nD_h f(1) = \\frac{\\left(f(1) + f'(1)h + \\frac{f''(\\xi)}{2}h^2\\right) - f(1)}{h} = f'(1) + \\frac{f''(\\xi)}{2}h\n$$\nThe truncation error is therefore:\n$$\nE_{trunc}(h) = D_h f(1) - f'(1) = \\frac{f''(\\xi)}{2}h\n$$\nFor a small step size $h$, $\\xi \\approx 1$. The magnitude of the leading-order truncation error can be written as:\n$$\n|E_{trunc}(h)| \\approx \\frac{|f''(1)|}{2}h\n$$\n\nNext, we analyze the roundoff error, $E_{round}$. The problem specifies that the floating-point evaluation of the function, denoted $\\hat{f}(y)$, is related to the true value $f(y)$ by $\\hat{f}(y) = f(y)(1+\\delta)$, where $\\delta$ is a relative error with magnitude $|\\delta| \\le u$, and $u=2^{-53}$ is the unit roundoff for IEEE 754 double precision. The errors in evaluating $f(1+h)$ and $f(1)$ are independent.\nThe computed forward difference, ignoring roundoff errors in subtraction and division for a leading-order model, is:\n$$\n\\hat{D}_h f(1) = \\frac{\\hat{f}(1+h) - \\hat{f}(1)}{h} = \\frac{f(1+h)(1+\\delta_2) - f(1)(1+\\delta_1)}{h}\n$$\nwhere $|\\delta_1| \\le u$ and $|\\delta_2| \\le u$.\nThe roundoff error is the difference between the computed and exact approximations:\n$$\nE_{round}(h) = \\hat{D}_h f(1) - D_h f(1) = \\frac{f(1+h)\\delta_2 - f(1)\\delta_1}{h}\n$$\nUsing the triangle inequality, we can bound the magnitude of the roundoff error:\n$$\n|E_{round}(h)| \\le \\frac{|f(1+h)||\\delta_2| + |f(1)||\\delta_1|}{h} \\le \\frac{u(|f(1+h)| + |f(1)|)}{h}\n$$\nFor small $h$, we can approximate $f(1+h) \\approx f(1)$. This gives the leading-order bound for the roundoff error:\n$$\n|E_{round}(h)| \\approx \\frac{2u|f(1)|}{h}\n$$\n\nThe total error is bounded by the sum of the magnitudes of the truncation and roundoff errors. The leading-order expression for this total error bound, $e(h)$, is:\n$$\ne(h) = |E_{trunc}(h)| + |E_{round}(h)| \\approx \\frac{|f''(1)|}{2}h + \\frac{2u|f(1)|}{h}\n$$\nTo find the step size $h$ that minimizes this error expression, we differentiate $e(h)$ with respect to $h$ and set the result to zero:\n$$\n\\frac{de}{dh} = \\frac{|f''(1)|}{2} - \\frac{2u|f(1)|}{h^2} = 0\n$$\nSolving for $h^2$:\n$$\n\\frac{|f''(1)|}{2} = \\frac{2u|f(1)|}{h^2} \\implies h^2 = \\frac{4u|f(1)|}{|f''(1)|}\n$$\nThe optimal step size is thus:\n$$\nh_{opt} = \\sqrt{\\frac{4u|f(1)|}{|f''(1)|}} = 2\\sqrt{u \\frac{|f(1)|}{|f''(1)|}}\n$$\n\nNow we apply this to the specific function $f(x) = \\sin x$ at $x=1$.\nThe function and its derivatives are:\n$f(x) = \\sin x \\implies f(1) = \\sin(1)$\n$f'(x) = \\cos x \\implies f'(1) = \\cos(1)$\n$f''(x) = -\\sin x \\implies f''(1) = -\\sin(1)$\n\nSince $1$ radian is approximately $57.3^\\circ$, it lies in the first quadrant, so $\\sin(1) > 0$.\nThe magnitudes are:\n$|f(1)| = |\\sin(1)| = \\sin(1)$\n$|f''(1)| = |-\\sin(1)| = \\sin(1)$\n\nSubstituting these into the expression for $h_{opt}$:\n$$\nh_{opt} = 2\\sqrt{u \\frac{\\sin(1)}{\\sin(1)}} = 2\\sqrt{u}\n$$\nGiven the unit roundoff $u = 2^{-53}$:\n$$\nh_{opt} = 2\\sqrt{2^{-53}} = 2^1 \\cdot (2^{-53})^{1/2} = 2^1 \\cdot 2^{-53/2} = 2^{1 - 26.5} = 2^{-25.5}\n$$\nTo obtain the numerical value, we compute:\n$$\nh_{opt} = 2^{-25.5} = \\frac{1}{2^{25.5}} = \\frac{1}{2^{25}\\sqrt{2}} \\approx 2.10734243 \\times 10^{-8}\n$$\nRounding this value to four significant figures as requested:\n$$\nh_{opt} \\approx 2.107 \\times 10^{-8}\n$$\nThis is the optimal step size in radians that minimizes the leading-order total error for the given problem.",
            "answer": "$$\n\\boxed{2.107 \\times 10^{-8}}\n$$"
        },
        {
            "introduction": "Recurrence relations are powerful tools for efficiently generating sequences of values for special functions, such as the spherical Bessel functions. However, a straightforward forward application of a recurrence can be numerically unstable, as small rounding errors get amplified at each step, eventually overwhelming the true solution. This exercise demonstrates this dramatic failure and introduces Miller's algorithm, a stable backward recurrence technique, as a solution. By contrasting the unstable and stable approaches, you will learn to analyze and control error propagation in iterative computations, a vital skill for reliable scientific software .",
            "id": "3258009",
            "problem": "Consider the spherical Bessel functions of the first kind, denoted by $j_n(x)$, which arise as solutions to the spherical Bessel differential equation $x^2 y'' + 2 x y' + \\left(x^2 - n(n+1)\\right) y = 0$. Two linearly independent solutions of the associated three-term recurrence relation form a numerically delicate pair: any rounding error can excite the growing (nonphysical for $j_n$) solution under one direction of recursion. From fundamental properties, the spherical Bessel functions satisfy the three-term recurrence relation $j_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)$, valid for $n \\ge 1$, together with closed-form expressions $j_0(x) = \\frac{\\sin(x)}{x}$ and $j_1(x) = \\frac{\\sin(x)}{x^2} - \\frac{\\cos(x)}{x}$. Angles are to be interpreted in radians. All computations must use double-precision floating-point arithmetic.\n\nYour task is to implement two algorithms for computing the sequence $\\{j_n(x)\\}_{n=0}^N$:\n- A mathematically correct but numerically unstable forward recurrence that starts from $j_0(x)$ and $j_1(x)$ and computes $j_{n+1}(x)$ using $j_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)$ for $n = 1, 2, \\ldots, N-1$.\n- A numerically stable reverse-order computation (Miller’s algorithm) that proceeds downward from a large index $L \\gg N$ using the reversed recurrence $j_{n-1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n+1}(x)$, starting with arbitrary seeds $j_{L+1}(x) = 0$ and $j_L(x) = 1$ (note the overall scale is arbitrary), computing down to $n = 0$, and then rescaling the entire sequence by a single factor so that the computed $j_0(x)$ matches the known closed-form $j_0(x) = \\frac{\\sin(x)}{x}$. The rescaled values $j_n(x)$ for $n \\le N$ constitute the stable sequence.\n\nBase your reasoning and implementation on the following core definitions:\n- The spherical Bessel differential equation $x^2 y'' + 2 x y' + \\left(x^2 - n(n+1)\\right) y = 0$ defines $j_n(x)$ as a physical solution that remains bounded.\n- The three-term recurrence $j_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)$ is a well-tested identity derived from the differential equation and its solution structure.\n\nNo shortcut formulas beyond these definitions are permitted. Analyze the numerical error propagation inherent in the forward and backward directions of the recurrence and exploit this to design your stable algorithm. Ensure scientific realism by using plausible parameter sizes and by explicitly handling the angle unit in radians.\n\nImplement a program that, for the following test suite, computes the maximum absolute deviation between the unstable forward recurrence and the stable backward (Miller) sequence across orders $n \\in \\{0, 1, \\ldots, N\\}$:\n- Test case $1$: $x = 1.0$ (radians), $N = 50$.\n- Test case $2$: $x = 10.0$ (radians), $N = 30$.\n- Test case $3$: $x = 0.1$ (radians), $N = 20$.\n- Test case $4$: $x = 50.0$ (radians), $N = 40$.\n\nFor the backward (Miller) computation, use $L = N + 60$ in all test cases. The result for each test case must be a single floating-point number equal to $\\max_{0 \\le n \\le N} \\left| j_n^{\\text{forward}}(x) - j_n^{\\text{backward}}(x) \\right|$, where $j_n^{\\text{forward}}(x)$ is computed via forward recurrence from $j_0(x)$ and $j_1(x)$, and $j_n^{\\text{backward}}(x)$ is computed via Miller’s algorithm and rescaled to match $j_0(x)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the test cases listed above (e.g., $[r_1,r_2,r_3,r_4]$). Angles must be treated in radians, and no physical units beyond this angle unit are involved. The outputs are floating-point numbers.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the well-established theory of special functions and their numerical computation, specifically concerning spherical Bessel functions. The problem is well-posed, providing all necessary definitions, recurrence relations, initial conditions, and parameters for the two specified algorithms. The objective is clear and the language is precise. It represents a classic, non-trivial problem in numerical analysis concerning the stability of recurrence relations.\n\nThe core of the problem lies in the numerical properties of the three-term recurrence relation:\n$$\nj_{n+1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n-1}(x)\n$$\nThis is a linear second-order homogeneous difference equation. As such, its general solution $y_n(x)$ is a linear combination of two linearly independent solutions: $y_n(x) = A f_n(x) + B g_n(x)$. For the spherical Bessel equation, these two solutions are the spherical Bessel functions of the first kind, $j_n(x)$, and of the second kind, $y_n(x)$.\n\nThe key to understanding the numerical stability lies in their asymptotic behavior for a fixed argument $x$ as the order $n$ becomes large.\n- The desired solution, $j_n(x)$, is the \"minimal\" or \"recessive\" solution. It decays to zero rapidly as $n \\to \\infty$ for any fixed $x$.\n- The other solution, $y_n(x)$, is the \"dominant\" solution. It grows without bound as $n \\to \\infty$.\n\nAny numerical computation will introduce small floating-point rounding errors. A computed sequence, denoted $\\tilde{j}_n(x)$, will inevitably be a combination of both solutions:\n$$\n\\tilde{j}_n(x) \\approx j_n(x) + \\epsilon y_n(x)\n$$\nwhere $\\epsilon$ is a small coefficient representing the initial accumulated error.\n\n**Forward Recurrence (Unstable Method)**\nThe forward recurrence algorithm computes $j_{n+1}(x)$ from the two preceding terms, $j_n(x)$ and $j_{n-1}(x)$. We start with the known values for $j_0(x)$ and $j_1(x)$.\n1.  Initialize an array to store the sequence $\\{j_n(x)\\}_{n=0}^N$.\n2.  Compute the starting values using the provided closed-form expressions:\n    $$\n    j_0(x) = \\frac{\\sin(x)}{x}\n    $$\n    $$\n    j_1(x) = \\frac{\\sin(x)}{x^2} - \\frac{\\cos(x)}{x}\n    $$\n    Even these initial values will have a small rounding error, effectively introducing a minute component of the dominant solution $y_n(x)$.\n3.  Iteratively apply the recurrence relation $j_{k+1}(x) = \\frac{2k+1}{x} j_k(x) - j_{k-1}(x)$ for $k = 1, 2, \\ldots, N-1$.\nAs $n$ increases, especially when $2n+1 > x$, the factor $\\frac{2n+1}{x}$ becomes large, amplifying any existing error. The initially small $\\epsilon y_n(x)$ component, which also satisfies the recurrence relation, will grow rapidly due to the dominant nature of $y_n(x)$. Eventually, this error term overwhelms the true, decaying $j_n(x)$ solution, leading to completely incorrect and often divergent results. This method is therefore numerically unstable for increasing $n$.\n\n**Backward Recurrence (Miller's Algorithm - Stable Method)**\nThe instability of the forward recurrence can be overcome by applying the recurrence in the reverse direction. This is the basis of Miller's algorithm. When iterating downwards in $n$, the recurrence relation naturally suppresses the dominant solution $y_n(x)$ and enhances the desired minimal solution $j_n(x)$.\n1.  Rewrite the recurrence to solve for the term with the smallest index:\n    $$\n    j_{n-1}(x) = \\frac{2n+1}{x} j_n(x) - j_{n+1}(x)\n    $$\n2.  Choose a starting index $L$ significantly larger than the desired maximum order $N$ (here, $L=N+60$). At such a large index, the true value of $j_L(x)$ is extremely close to zero.\n3.  We initialize an unscaled sequence, let's call it $f_n$, by setting $f_{L+1}(x) = 0$ and $f_L(x) = 1$ (or any small, arbitrary non-zero constant). These initial conditions do not correspond to the true values, but this is the key to the algorithm.\n4.  Iterate the recurrence downwards for $n = L, L-1, \\ldots, 1$ to compute the sequence $\\{f_k(x)\\}_{k=0}^L$. Because we are recurring in the stable direction, the resulting sequence $f_n(x)$ will be proportional to the true minimal solution $j_n(x)$ for all $n \\ll L$. That is, $f_n(x) \\approx C \\cdot j_n(x)$ for some unknown constant of proportionality $C$.\n5.  To find $C$, we use a known value of the true sequence. The most convenient is $j_0(x)$. We have our computed $f_0(x)$ and the known analytical value $j_0^{\\text{true}}(x) = \\frac{\\sin(x)}{x}$. The scaling factor is therefore $S = \\frac{j_0^{\\text{true}}(x)}{f_0(x)}$.\n6.  The stable sequence is obtained by rescaling all computed values: $j_n^{\\text{backward}}(x) = S \\cdot f_n(x)$ for $n=0, 1, \\ldots, N$. Since $j_0^{\\text{backward}}(x)$ is rescaled to match the true value, the absolute error at $n=0$ will be zero (up to machine precision).\n\n**Final Computation**\nThe objective is to quantify the failure of the forward method by comparing it to the stable backward method. For each test case, we compute the two sequences, $\\{j_n^{\\text{forward}}(x)\\}_{n=0}^N$ and $\\{j_n^{\\text{backward}}(x)\\}_{n=0}^N$, and then find the maximum absolute deviation between them over all computed orders:\n$$\n\\text{Deviation} = \\max_{0 \\le n \\le N} \\left| j_n^{\\text{forward}}(x) - j_n^{\\text{backward}}(x) \\right|\n$$\nThis value serves as a measure of the error accumulated by the unstable forward recurrence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the maximum absolute deviation between unstable forward recurrence\n    and stable backward recurrence (Miller's algorithm) for spherical Bessel\n    functions j_n(x).\n    \"\"\"\n\n    def compute_forward(x, N):\n        \"\"\"\n        Computes the sequence {j_n(x)} for n=0 to N using forward recurrence.\n        This method is numerically unstable for n > x.\n        \"\"\"\n        # All computations use double precision (np.float64) by default.\n        j_fwd = np.zeros(N + 1, dtype=np.float64)\n\n        if x == 0.0:\n            j_fwd[0] = 1.0\n            return j_fwd\n\n        j_fwd[0] = np.sin(x) / x\n        if N > 0:\n            j_fwd[1] = (np.sin(x) / x**2) - (np.cos(x) / x)\n\n        for n in range(1, N):\n            term1 = ((2.0 * n + 1.0) / x) * j_fwd[n]\n            term2 = j_fwd[n - 1]\n            # Check for potential overflow before assignment\n            if np.isinf(term1):\n                j_fwd[n + 1:] = np.inf\n                break\n            j_fwd[n + 1] = term1 - term2\n\n        return j_fwd\n\n    def compute_backward(x, N):\n        \"\"\"\n        Computes the sequence {j_n(x)} for n=0 to N using backward recurrence\n        (Miller's algorithm). This method is numerically stable.\n        \"\"\"\n        L = N + 60\n        j_bwd_unscaled = np.zeros(L + 2, dtype=np.float64)\n\n        if x == 0.0:\n            j_bwd = np.zeros(N + 1, dtype=np.float64)\n            j_bwd[0] = 1.0\n            return j_bwd\n\n        # Set initial arbitrary values at a large index L\n        j_bwd_unscaled[L + 1] = 0.0\n        j_bwd_unscaled[L] = 1.0  # An arbitrary small number; 1.0 is fine.\n\n        # Recur downwards from n=L to n=1\n        for n in range(L, 0, -1):\n            term1 = ((2.0 * n + 1.0) / x) * j_bwd_unscaled[n]\n            term2 = j_bwd_unscaled[n + 1]\n            j_bwd_unscaled[n - 1] = term1 - term2\n\n        # Calculate the true j_0(x) for normalization\n        j0_true = np.sin(x) / x\n\n        # The computed sequence is proportional to the true sequence.\n        # Find the scaling factor by comparing the computed j_0 with the true j_0.\n        f0_computed = j_bwd_unscaled[0]\n        \n        # This case is highly unlikely for the given x values.\n        if f0_computed == 0.0:\n            scale_factor = 0.0\n        else:\n            scale_factor = j0_true / f0_computed\n\n        # Rescale the sequence to get the final result\n        j_bwd = j_bwd_unscaled[:N + 1] * scale_factor\n        return j_bwd\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 50),\n        (10.0, 30),\n        (0.1, 20),\n        (50.0, 40),\n    ]\n\n    results = []\n    for x_val, N_val in test_cases:\n        j_forward = compute_forward(x_val, N_val)\n        j_backward = compute_backward(x_val, N_val)\n\n        # Compute the maximum absolute deviation between the two sequences\n        deviation = np.max(np.abs(j_forward - j_backward))\n        results.append(deviation)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}