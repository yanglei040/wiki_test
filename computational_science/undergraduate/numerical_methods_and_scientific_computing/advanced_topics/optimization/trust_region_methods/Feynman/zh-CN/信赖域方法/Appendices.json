{
    "hands_on_practices": [
        {
            "introduction": "信赖域方法的核心在于求解一个带约束的二次子问题。这个练习将带你深入该子问题的核心，特别是在最优步长位于信赖域边界的“困难情况”。我们将从第一性原理出发，为正定海森矩阵 ($H$) 这一良好情形推导出控制边界解的关键方程（即“长期方程”），为理解信赖域算法奠定坚实的数学基础。",
            "id": "3284949",
            "problem": "考虑一个信赖域子问题，其中包含一个对称正定（SPD）矩阵 $H \\in \\mathbb{R}^{n \\times n}$、梯度 $g \\in \\mathbb{R}^{n}$ 以及信赖域半径 $\\Delta  0$：在欧几里得范数约束 $\\|p\\| \\le \\Delta$ 下，最小化二次模型 $m(p) = g^{\\mathsf{T}} p + \\tfrac{1}{2} p^{\\mathsf{T}} H p$。从基本原理（即优化问题的定义和约束优化的 Karush–Kuhn–Tucker 条件）出发，推导一个关于单个变量的标量方程，该方程刻画了在 SPD 情况下 $\\|p\\| = \\Delta$ 的边界解。然后，仅使用基本的线性代数事实和 SPD 矩阵的性质，证明为何这个标量方程对于 $\\lambda \\ge 0$ 恰好有一个解，并解释为何对应的残差函数是严格递减的。\n\n接下来，概述一种用于计算该标量方程唯一解（即拉格朗日乘子 $\\lambda \\ge 0$）的高效求根策略。你的概述应明确说明：\n- 如何利用对 $H + \\lambda I$ 的线性求解来评估残差及其导数，\n- 一种保证收敛的安全全局化策略，以及\n- 如何获得一个能在实践中实现快速收敛的初始区间或迭代点。\n\n最后，对于以下具体实例：\n- $H = \\mathrm{diag}(3, 1)$，\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$，\n- $\\Delta = 2$，\n计算满足边界条件的唯一的 $\\lambda \\ge 0$ 的精确值。请提供 $\\lambda$ 的精确值作为最终答案，不要四舍五入。",
            "solution": "该问题已经过验证，被认为是有效的。这是一个来自数值优化领域的、适定的、有科学依据的问题，所有必要的数据和定义均已提供。\n\n信赖域子问题旨在寻找一个步长 $p \\in \\mathbb{R}^{n}$，以在步长范数约束下最小化一个函数的二次模型。该问题可表述为：\n$$\n\\min_{p \\in \\mathbb{R}^{n}} m(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p \\quad \\text{满足} \\quad \\|p\\| \\le \\Delta\n$$\n其中 $H \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定（SPD）矩阵，$g \\in \\mathbb{R}^{n}$ 是一个梯度向量，$\\Delta  0$ 是信赖域半径。约束是关于欧几里得范数，即 $\\|p\\| = \\sqrt{p^{\\mathsf{T}}p}$。\n\n为了推导刻画信赖域边界上解的方程，我们采用约束优化的 Karush–Kuhn–Tucker (KKT) 条件。不等式约束为 $g_c(p) = \\|p\\|^2 - \\Delta^2 \\le 0$。拉格朗日函数是：\n$$\nL(p, \\lambda) = m(p) + \\frac{\\lambda}{2} g_c(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p + \\frac{\\lambda}{2} (p^{\\mathsf{T}}p - \\Delta^2)\n$$\n其中 $\\lambda$ 是拉格朗日乘子。引入因子 $\\frac{1}{2}$ 是为了方便。对于一个最小化子 $p^*$，KKT 条件为：\n1.  平稳性：$\\nabla_p L(p^*, \\lambda) = g + H p^* + \\lambda p^* = 0$。\n2.  原始可行性：$\\|p^*\\|^2 \\le \\Delta^2$。\n3.  对偶可行性：$\\lambda \\ge 0$。\n4.  互补松弛性：$\\lambda (\\|p^*\\|^2 - \\Delta^2) = 0$。\n\n问题要求刻画边界上的解，即 $\\|p^*\\| = \\Delta$。根据互补松弛性条件，这意味着 $\\lambda$ 可以为非负数，$\\lambda \\ge 0$。平稳性条件可以重写为：\n$$\n(H + \\lambda I) p^* = -g\n$$\n其中 $I$ 是单位矩阵。由于 $H$ 是 SPD 矩阵，其所有特征值 $\\mu_i$ 均为严格正数，即 $\\mu_i  0$。对于任意 $\\lambda \\ge 0$，矩阵 $(H + \\lambda I)$ 的特征值为 $\\mu_i + \\lambda$，这些值也都是严格正数。因此，对于所有 $\\lambda \\ge 0$，$(H + \\lambda I)$ 都是 SPD 矩阵，从而也是可逆的。于是，我们可以将解 $p^*$ 唯一地表示为 $\\lambda$ 的函数：\n$$\np(\\lambda) = -(H + \\lambda I)^{-1} g\n$$\n对于一个边界解，这个步长 $p(\\lambda)$ 必须满足约束 $\\|p(\\lambda)\\| = \\Delta$。代入 $p(\\lambda)$ 的表达式，得到关于单个变量 $\\lambda$ 的标量方程：\n$$\n\\|-(H + \\lambda I)^{-1} g\\| = \\Delta\n$$\n这是基础方程，通常被称为长期方程（secular equation），它刻画了信赖域边界上解的拉格朗日乘子 $\\lambda$。\n\n接下来，我们证明对于边界解，该方程对于 $\\lambda \\ge 0$ 恰好有一个解。我们来分析函数 $\\phi(\\lambda) = \\|p(\\lambda)\\| = \\|(H + \\lambda I)^{-1} g\\|$（对于 $\\lambda \\ge 0$）。分析其平方 $\\psi(\\lambda) = \\phi(\\lambda)^2 = \\|p(\\lambda)\\|^2$ 会更方便。设 $H = Q \\Lambda Q^{\\mathsf{T}}$ 为 $H$ 的谱分解，其中 $Q$ 是一个正交矩阵，$\\Lambda = \\mathrm{diag}(\\mu_1, \\ldots, \\mu_n)$ 是 $H$ 的正特征值构成的对角矩阵。我们可以将 $\\psi(\\lambda)$ 写为：\n$$\n\\psi(\\lambda) = g^{\\mathsf{T}} (H + \\lambda I)^{-2} g = g^{\\mathsf{T}} (Q (\\Lambda + \\lambda I) Q^{\\mathsf{T}})^{-2} g = g^{\\mathsf{T}} Q (\\Lambda + \\lambda I)^{-2} Q^{\\mathsf{T}} g\n$$\n令 $\\hat{g} = Q^{\\mathsf{T}} g$。则 $\\psi(\\lambda)$ 变为：\n$$\n\\psi(\\lambda) = \\hat{g}^{\\mathsf{T}} (\\Lambda + \\lambda I)^{-2} \\hat{g} = \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^2}\n$$\n为了分析该函数的单调性，我们计算它关于 $\\lambda$ 的导数：\n$$\n\\psi'(\\lambda) = \\sum_{i=1}^{n} \\hat{g}_i^2 \\frac{d}{d\\lambda} (\\mu_i + \\lambda)^{-2} = \\sum_{i=1}^{n} \\hat{g}_i^2 (-2)(\\mu_i + \\lambda)^{-3} (1) = -2 \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^3}\n$$\n由于 $\\mu_i  0$ 且我们考虑 $\\lambda \\ge 0$，分母 $(\\mu_i + \\lambda)^3$ 总是正的。分子 $\\hat{g}_i^2$ 是非负的。如果 $g \\neq 0$，那么 $\\hat{g} \\neq 0$，并且至少有一个 $\\hat{g}_i^2$ 是正的。因此，对于 $g \\neq 0$，在所有 $\\lambda \\ge 0$ 上 $\\psi'(\\lambda)  0$。这证明了 $\\psi(\\lambda)$ 是在 $[0, \\infty)$ 上的一个严格递减函数。由于 $\\phi(\\lambda)=\\sqrt{\\psi(\\lambda)}$ 且平方根函数对其正参数是严格递增的，所以 $\\phi(\\lambda)$ 在 $[0, \\infty)$ 上也是一个严格递减函数。\n\n方程 $\\phi(\\lambda) = \\Delta$ 是否存在唯一的 $\\lambda \\ge 0$ 解，取决于 $\\phi(0) = \\|-H^{-1}g\\|$ 的值，此为无约束最小化子（完整牛顿步）的范数。\n- 如果 $\\| -H^{-1}g \\| \\le \\Delta$，无约束解是可行的。此时 KKT 条件在 $p^* = -H^{-1}g$ 和 $\\lambda = 0$ 时得到满足。这是内部解。\n- 如果 $\\| -H^{-1}g \\|  \\Delta$，解必须位于边界上。我们分析函数 $\\phi(\\lambda)$ 在 $[0, \\infty)$ 上的行为。我们有 $\\phi(0) = \\|-H^{-1}g\\|  \\Delta$。\n在 $\\lambda \\to \\infty$ 的极限情况下，我们有 $\\phi(\\lambda) = \\|-(H+\\lambda I)^{-1}g\\| \\approx \\|\\frac{-1}{\\lambda}I g\\| = \\frac{\\|g\\|}{\\lambda} \\to 0$。\n由于 $\\phi(\\lambda)$ 是在 $[0, \\infty)$ 上的一个连续且严格递减的函数，且 $\\phi(0)  \\Delta$ 和 $\\lim_{\\lambda \\to \\infty} \\phi(\\lambda) = 0$，根据介值定理，必然存在唯一的 $\\lambda^*  0$ 使得 $\\phi(\\lambda^*) = \\Delta$。因此，在边界情况下，存在唯一的正拉格朗日乘子。\n\n求解长期方程的高效求根策略通常使用牛顿法作用于一个相关函数，例如 $r(\\lambda) = \\|p(\\lambda)\\| - \\Delta = 0$。\n牛顿迭代为 $\\lambda_{k+1} = \\lambda_k - r(\\lambda_k)/r'(\\lambda_k)$。\n为了评估 $r(\\lambda)$ 及其导数 $r'(\\lambda)$：\n1.  **评估 $r(\\lambda)$**：对于给定的 $\\lambda$，我们必须计算 $p(\\lambda) = -(H+\\lambda I)^{-1}g$。这通过求解线性系统 $(H+\\lambda I)p = -g$ 来完成。由于 $(H+\\lambda I)$ 是 SPD 矩阵，该系统可以通过 Cholesky 分解高效求解。计算出 $p$ 后，我们评估 $r(\\lambda) = \\|p\\| - \\Delta$。\n2.  **评估 $r'(\\lambda)$**：我们必须计算 $\\frac{d}{d\\lambda}\\|p(\\lambda)\\|$。\n    $r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} p'(\\lambda)}{\\|p(\\lambda)\\|}$，其中 $p'(\\lambda) = \\frac{d p(\\lambda)}{d\\lambda}$。\n    通过对等式 $(H + \\lambda I) p(\\lambda) = -g$ 关于 $\\lambda$ 求导，我们得到：\n    $I p(\\lambda) + (H + \\lambda I) p'(\\lambda) = 0 \\implies p'(\\lambda) = -(H + \\lambda I)^{-1} p(\\lambda)$。\n    令 $q = p'(\\lambda)$。为了计算 $q$，我们求解另一个线性系统 $(H + \\lambda I)q = -p(\\lambda)$。注意，计算 $p(\\lambda)$ 时得到的 $(H+\\lambda I)$ 的 Cholesky 分解可以被重用。\n    然后，$r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} q}{\\|p(\\lambda)\\|}$。\n一个牛顿步包含对同一矩阵的两次线性求解。\n- **全局化策略**：牛顿法局部二次收敛，但可能不全局收敛。因此需要一种安全策略。由于我们知道根 $\\lambda^*$ 位于 $[0, \\infty)$ 区间内，并且函数 $r(\\lambda)$ 是凸且单调的，简单的安全措施是有效的。例如，如果一个牛顿步 $\\lambda_{k+1}$ 是负数，可以用一个已知区间上的二分步来代替，例如 $[\\lambda_k/2, \\lambda_k]$。\n- **初始迭代点**：我们仅在 $\\|-H^{-1}g\\|  \\Delta$ 时需要解长期方程。在这种情况下，我们知道 $\\lambda^*  0$。一个常见且简单的初始猜测是 $\\lambda_0 = 0$。从这一点出发，牛顿迭代将产生一个正的 $\\lambda_1$。更复杂的初始猜测可以从特征值界限导出，例如 $\\lambda_0 = \\frac{\\|g\\|}{\\Delta} - \\mu_{\\min}$，尽管计算 $\\mu_{\\min}$ 可能代价高昂。在有安全保障的方案中，从 $\\lambda_0=0$ 开始是稳健的。\n\n最后，我们为给定的具体实例计算 $\\lambda$ 的精确值：\n- $H = \\mathrm{diag}(3, 1)$\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$\n- $\\Delta = 2$\n\n首先，我们检查解是否在边界上。我们计算无约束最小化子 $p_{unc} = -H^{-1}g$。\n$H^{-1} = \\mathrm{diag}(1/3, 1)$。\n$p_{unc} = -\\begin{pmatrix} 1/3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -10/3 \\\\ 0 \\end{pmatrix}$。\n该步长的范数为 $\\|p_{unc}\\| = \\sqrt{(-10/3)^2 + 0^2} = 10/3$。\n由于 $10/3 \\approx 3.33  \\Delta = 2$，解位于边界上，我们必须找到一个 $\\lambda  0$。\n\n我们求解长期方程 $\\|p(\\lambda)\\| = \\Delta$。步长 $p(\\lambda)$ 由以下公式给出：\n$p(\\lambda) = -(H + \\lambda I)^{-1} g$。\n$H + \\lambda I = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3+\\lambda  0 \\\\ 0  1+\\lambda \\end{pmatrix}$。\n$(H + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{3+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix}$。\n$p(\\lambda) = -\\begin{pmatrix} \\frac{1}{3+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix}$。\n\n现在，我们强制执行边界条件 $\\|p(\\lambda)\\| = 2$：\n$\\left\\| \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix} \\right\\| = 2$。\n$\\sqrt{\\left(-\\frac{10}{3+\\lambda}\\right)^2 + 0^2} = 2$。\n因为我们寻找 $\\lambda  0$，所以 $3+\\lambda$ 是正数，因此我们可以写成：\n$\\frac{10}{3+\\lambda} = 2$。\n$10 = 2(3+\\lambda)$。\n$10 = 6 + 2\\lambda$。\n$4 = 2\\lambda$。\n$\\lambda = 2$。\n\n这是满足边界条件的唯一的正拉格朗日乘子。",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "在确定了如何在边界上寻找解之后，一个自然的问题是：我们应该朝哪个方向前进？最简单的选择是沿着最速下降方向，这会得到柯西点。本练习通过一个精心设计的假想情景，揭示了柯西点可能是一个非常糟糕的选择，尤其是在函数存在负曲率时。这突显了信赖域方法通过利用完整的二次模型，为何能够找到远比最速下降更优的步长。",
            "id": "3284825",
            "problem": "考虑一个在无约束优化的信赖域(TR)方法中产生的二次模型。在当前迭代点，局部二阶模型为\n$$\nm(p) \\;=\\; f(x_k) \\;+\\; g^{\\top}p \\;+\\; \\frac{1}{2}p^{\\top}B\\,p,\n$$\n其中 $p \\in \\mathbb{R}^{2}$ 是步长，$g \\in \\mathbb{R}^{2}$ 是在 $x_k$ 处的梯度，$B \\in \\mathbb{R}^{2 \\times 2}$ 是一个对称的Hessian近似矩阵。TR子问题是在信赖域约束 $\\|p\\| \\leq \\Delta$ 下最小化 $m(p)$。柯西点被定义为在射线 $p = -\\alpha g$ 上，满足 $\\|p\\| \\leq \\Delta$ 约束下 $m(p)$ 的最小化子。\n\n构建一个具体的二维实例，其中\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \n\\qquad\nB \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1,\n$$\n并考虑相应的TR子问题。仅使用TR方法和二次模型的基本定义和原理，确定在柯西点的预测下降量与TR子问题全局解的预测下降量之间的比率 $\\rho$。步长 $p$ 的预测下降量定义为 $m(0) - m(p)$。\n\n请用科学记数法提供您的最终数值 $\\rho$，并四舍五入到四位有效数字。",
            "solution": "信赖域(TR)子问题求解\n$$\n\\min_{p \\in \\mathbb{R}^{2}} \\;\\; m(p) \\;=\\; g^{\\top}p + \\frac{1}{2}p^{\\top}B\\,p \n\\quad \\text{subject to} \\quad \\|p\\| \\leq \\Delta,\n$$\n其中 $f(x_k)$ 是关于 $p$ 的常数，因此在最小化中被省略。步长 $p$ 的预测下降量是\n$$\n\\text{predicted reduction}(p) \\;=\\; m(0) - m(p) \\;=\\; -\\,g^{\\top}p \\;-\\; \\frac{1}{2}p^{\\top}B\\,p.\n$$\n\n给定\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n\\qquad\nB \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1.\n$$\n我们首先计算柯西点，它在射线 $p=-\\alpha g$ 上，在约束 $\\|p\\| \\leq \\Delta$ 下最小化 $m(p)$。将 $p$ 参数化为 $p(\\alpha) = -\\alpha g = \\begin{pmatrix} -\\alpha \\\\ 0 \\end{pmatrix}$，其中 $\\alpha \\geq 0$，约束为 $\\|p(\\alpha)\\| = \\alpha\\|g\\| \\leq \\Delta$。由于 $\\|g\\| = 1$，这变为 $\\alpha \\leq \\Delta = 1$。\n\n将 $m$ 限制在这条射线上，\n$$\nm(-\\alpha g) \\;=\\; g^{\\top}(-\\alpha g) + \\frac{1}{2}(-\\alpha g)^{\\top}B(-\\alpha g).\n$$\n我们计算 $g^{\\top}(-\\alpha g) = -\\alpha \\|g\\|^{2} = -\\alpha$ 以及\n$$\n(-\\alpha g)^{\\top}B(-\\alpha g) \\;=\\; \\alpha^{2} g^{\\top}B g \\;=\\; \\alpha^{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\;=\\; \\alpha^{2} \\cdot 1 \\;=\\; \\alpha^{2}.\n$$\n因此，\n$$\nm(-\\alpha g) \\;=\\; -\\alpha + \\frac{1}{2}\\alpha^{2}.\n$$\n这是一个关于 $\\alpha$ 的一维二次函数；无约束最小化子满足\n$$\n\\frac{d}{d\\alpha}\\left(-\\alpha + \\frac{1}{2}\\alpha^{2}\\right) \\;=\\; -1 + \\alpha \\;=\\; 0\n\\quad \\Rightarrow \\quad \\alpha^{\\star} \\;=\\; 1.\n$$\n由于信赖域边界是 $\\alpha \\leq 1$，无约束最小化子位于边界上并且是可行的。因此，柯西点是\n$$\np_{\\mathrm{C}} \\;=\\; -\\alpha^{\\star} g \\;=\\; \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\n在柯西点的预测下降量是\n$$\n\\text{PR}_{\\mathrm{C}} \n\\;=\\; -\\,g^{\\top}p_{\\mathrm{C}} - \\frac{1}{2}p_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}}\n\\;=\\; -\\,\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} -1  0 \\end{pmatrix}\\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\n计算各项：\n$$\n-\\,g^{\\top}p_{\\mathrm{C}} \\;=\\; -(-1) \\;=\\; 1, \n\\qquad\np_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}} \\;=\\; (-1)^{2}\\cdot 1 + 0^{2}\\cdot (-1000) \\;=\\; 1.\n$$\n因此，\n$$\n\\text{PR}_{\\mathrm{C}} \\;=\\; 1 - \\frac{1}{2}\\cdot 1 \\;=\\; \\frac{1}{2}.\n$$\n\n接下来，我们确定全局TR解。矩阵 $B$ 是不定的，其特征值为 $1$ 和 $-1000$。沿第二个坐标方向的模型具有强负曲率。TR子问题在 $x^{2}+y^{2} \\leq 1$ 的约束下最小化\n$$\nm(p) \\;=\\; x + \\frac{1}{2}\\left(x^{2} - 1000\\,y^{2}\\right)\n$$\n对于任何可行的 $(x,y)$，项 $-\\frac{1}{2}\\cdot 1000\\,y^{2}$ 随着 $|y|$ 的增加而严格减小，这表明最小化子将在 $y$ 方向上饱和信赖域边界。在边界 $x^{2}+y^{2}=1$ 上，项 $x + \\frac{1}{2}x^{2}$ 在 $x=0$ 时最小化，因为当 $y$ 固定在其最大绝对值时，约束强制 $x=0$。因此，一个全局最小化子是\n$$\np^{\\star} \\;=\\; \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\quad \\text{或} \\quad\np^{\\star} \\;=\\; \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix},\n$$\n由于对称性，两者都产生相同的模型值。为具体起见，我们选择 $p^{\\star} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。在全局TR解处的预测下降量是\n$$\n\\text{PR}_{\\star} \n\\;=\\; -\\,g^{\\top}p^{\\star} - \\frac{1}{2}(p^{\\star})^{\\top}B\\,p^{\\star}\n\\;=\\; -\\,\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} 0  1 \\end{pmatrix}\\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\n计算各项：\n$$\n-\\,g^{\\top}p^{\\star} \\;=\\; -0 \\;=\\; 0,\n\\qquad\n(p^{\\star})^{\\top}B\\,p^{\\star} \\;=\\; 0^{2}\\cdot 1 + 1^{2}\\cdot (-1000) \\;=\\; -1000.\n$$\n因此，\n$$\n\\text{PR}_{\\star} \\;=\\; 0 - \\frac{1}{2}\\cdot(-1000) \\;=\\; 500.\n$$\n\n所要求的在柯西点的预测下降量与全局TR解的预测下降量之间的比率 $\\rho$ 是\n$$\n\\rho \\;=\\; \\frac{\\text{PR}_{\\mathrm{C}}}{\\text{PR}_{\\star}} \\;=\\; \\frac{\\frac{1}{2}}{500} \\;=\\; \\frac{1}{1000} \\;=\\; 10^{-3}.\n$$\n用科学记数法表示并四舍五入到四位有效数字，结果是\n$$\n\\rho \\;=\\; 1.000 \\times 10^{-3}.\n$$\n这表明，在这个二维实例中，TR全局解位于边界上，而相对于最优边界步长，柯西点产生的预测下降量异常差。",
            "answer": "$$\\boxed{1.000 \\times 10^{-3}}$$"
        },
        {
            "introduction": "现在，我们将综合运用前面的概念，构建并测试一个完整的信赖域算法。这个动手编程练习要求你使用“狗腿法”来实现一个信赖域方法，这是一种求解子问题的有效近似策略。你将把它应用于著名的 Rosenbrock 函数——一个具有狭窄弯曲谷底的经典测试难题，并将其性能与标准的线搜索牛顿法进行比较，从而亲身体验信赖域半径 $\\Delta_k$ 和步长接受率 $\\rho_k$ 的动态变化。",
            "id": "3284806",
            "problem": "考虑通过应用数值优化中的两种经典算法：信赖域方法和带线搜索的牛顿法，来最小化一个具有弯曲窄谷的二次连续可微函数。使用标准的 Rosenbrock 函数，这是一个具有弯曲窄谷的标准测试函数，对于 $x = (x_1,x_2) \\in \\mathbb{R}^2$ 定义为\n$$\nf(x) = (1 - x_1)^2 + 100\\,(x_2 - x_1^2)^2.\n$$\n分别使用二阶泰勒模型和 Armijo 回溯条件，从第一性原理构建和比较这两种方法。跟踪信赖域半径序列和一致性比率，并报告收敛的汇总统计信息。实现以下规范：\n\n1. 定义和使用的基本事实：\n   - 函数 $f$ 在 $x_k$ 处的二阶泰勒模型是\n     $$\n     m_k(p) = f(x_k) + g_k^\\top p + \\tfrac{1}{2} p^\\top H_k p,\n     $$\n     其中 $g_k = \\nabla f(x_k)$ 且 $H_k = \\nabla^2 f(x_k)$。\n   - 信赖域子问题是\n     $$\n     \\min_{p \\in \\mathbb{R}^2} \\ m_k(p) \\quad \\text{subject to} \\quad \\|p\\|_2 \\le \\Delta_k,\n     $$\n     其中 $\\Delta_k  0$ 是信赖域半径。当 $H_k$ 是对称正定时，使用狗腿策略；当 $H_k$ 不是对称正定时，回退到边界上的柯西点。\n   - 一致性比率是\n     $$\n     \\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}.\n     $$\n     如果 $\\rho_k \\ge \\eta$，则接受步长 $p_k$，其中 $\\eta \\in (0,1)$ 是一个固定的阈值。\n   - 使用以下带参数 $\\gamma_1 \\in (0,1)$ 和 $\\gamma_2  1$ 的信赖域半径更新规则：\n     - 如果 $\\rho_k  0.25$，设置 $\\Delta_{k+1} = \\gamma_1\\,\\Delta_k$。\n     - 如果 $\\rho_k  0.75$ 且 $\\|p_k\\|_2 \\ge 0.99\\,\\Delta_k$，设置 $\\Delta_{k+1} = \\min(\\gamma_2\\,\\Delta_k,\\Delta_{\\max})$。\n     - 否则，设置 $\\Delta_{k+1} = \\Delta_k$。\n   - 带线搜索的牛顿法通过求解以下方程计算搜索方向 $d_k$\n     $$\n     H_k d_k = -g_k,\n     $$\n     并使用回溯法找到满足 Armijo 条件的步长 $\\alpha_k$\n     $$\n     f(x_k + \\alpha_k d_k) \\le f(x_k) + c_1\\,\\alpha_k\\,g_k^\\top d_k,\n     $$\n     其中 $c_1 \\in (0,1)$ 是一个固定常数。如果 $d_k$ 不是下降方向（即 $g_k^\\top d_k \\ge 0$），则在线搜索前将 $d_k$ 替换为最速下降方向 $d_k = -g_k$。使用 $\\alpha_k \\leftarrow \\beta\\,\\alpha_k$（其中 $\\beta \\in (0,1)$）执行回溯，直到满足 Armijo 条件或达到预设的最大回溯步数。\n\n2. 推导并实现所需的组件：\n   - 对于 Rosenbrock 函数 $f$，推导并实现其梯度 $g(x)$ 和海森矩阵 $H(x)$：\n     $$\n     g(x) = \\begin{bmatrix}\n     -2(1 - x_1) - 400 x_1 (x_2 - x_1^2) \\\\\n     200 (x_2 - x_1^2)\n     \\end{bmatrix}, \\quad\n     H(x) = \\begin{bmatrix}\n     2 - 400 x_2 + 1200 x_1^2  -400 x_1 \\\\\n     -400 x_1  200\n     \\end{bmatrix}.\n     $$\n   - 在需要时，实现沿最速下降方向的柯西点 $p_{\\mathrm{C}}$：\n     $$\n     p_{\\mathrm{C}} = -\\alpha_{\\mathrm{C}}\\,g_k, \\quad \\alpha_{\\mathrm{C}} = \\begin{cases}\n     \\dfrac{\\|g_k\\|_2^2}{g_k^\\top H_k g_k},  \\text{if } g_k^\\top H_k g_k  0, \\\\\n     \\dfrac{\\Delta_k}{\\|g_k\\|_2},  \\text{otherwise},\n     \\end{cases}\n     $$\n     如果 $\\|p_{\\mathrm{C}}\\|_2  \\Delta_k$，则通过 $p = -\\Delta_k\\,g_k/\\|g_k\\|_2$ 投影到边界。\n   - 当 $H_k$ 是对称正定时，使用从柯西点到牛顿步 $p_{\\mathrm{B}} = -H_k^{-1} g_k$ 的线段实现狗腿步 $p_k$。如果 $\\|p_{\\mathrm{B}}\\|_2 \\le \\Delta_k$，选择 $p_k = p_{\\mathrm{B}}$；如果 $\\|p_{\\mathrm{C}}\\|_2 \\ge \\Delta_k$，选择 $p_k = -\\Delta_k\\,g_k/\\|g_k\\|_2$；否则，选择连接 $p_{\\mathrm{C}}$ 到 $p_{\\mathrm{B}}$ 的线段上满足 $\\|p_k\\|_2 = \\Delta_k$ 的唯一点。\n   - 跟踪信赖域半径序列 $(\\Delta_k)$ 和一致性比率 $(\\rho_k)$，并计算半径收缩（$\\Delta_{k+1}  \\Delta_k$ 的情况）和扩张（$\\Delta_{k+1} > \\Delta_k$ 的情况）的次数。同时根据阈值 $\\eta$ 统计接受和拒绝的步数。\n\n3. 两种方法的停止准则：\n   - 当梯度的欧几里得范数满足 $\\|\\nabla f(x_k)\\|_2 \\le \\text{tol}$ 或达到最大迭代次数时停止。\n\n4. 测试套件：\n   - 使用以下具有固定参数的测试用例。对于所有用例，使用终止容差 $\\text{tol} = 10^{-6}$ 和最大迭代次数 $K_{\\max} = 200$。\n   - 信赖域参数：$\\eta = 0.1$, $\\gamma_1 = 0.25$, $\\gamma_2 = 2$, 以及 $\\Delta_{\\max} = 100$。\n   - 线搜索参数：$c_1 = 10^{-4}$, $\\beta = 0.5$, 以及每次迭代最多 $M = 50$ 次回溯步数。\n   - 测试用例是：\n     - 用例 1（理想情况）：$x_0 = (-1.2,\\,1.0)$, $\\Delta_0 = 1$。\n     - 用例 2（小初始半径边界）：$x_0 = (0.0,\\,0.0)$, $\\Delta_0 = 10^{-3}$。\n     - 用例 3（接近最优点的边界情况）：$x_0 = (1.0,\\,1.0)$, $\\Delta_0 = 1$。\n     - 用例 4（偏离轴线的弯曲谷）：$x_0 = (-1.5,\\,2.0)$, $\\Delta_0 = 0.5$。\n\n5. 程序输出规范：\n   - 对于每个测试用例，计算并返回以下值列表：\n     - 信赖域迭代次数 $n_{\\mathrm{TR}}$（整数）。\n     - 牛顿线搜索迭代次数 $n_{\\mathrm{NT}}$（整数）。\n     - 信赖域的最终目标函数值 $f_{\\mathrm{TR}}$（浮点数，保留六位小数）。\n     - 牛顿线搜索的最终目标函数值 $f_{\\mathrm{NT}}$（浮点数，保留六位小数）。\n     - 信赖域半径总收缩次数（整数）。\n     - 信赖域半径总扩张次数（整数）。\n     - 在接受的信赖域步数上的平均一致性比率 $\\bar{\\rho}$（浮点数，保留六位小数；如果没有接受的步，则使用 $0$）。\n     - 牛顿线搜索在所有迭代中执行的回溯缩减总次数（整数）。\n     - 信赖域的最终梯度范数 $\\|g_{\\mathrm{TR}}\\|_2$（浮点数，保留六位小数）。\n     - 牛顿线搜索的最终梯度范数 $\\|g_{\\mathrm{NT}}\\|_2$（浮点数，保留六位小数）。\n     - 一个布尔值，指示信赖域方法是否比带线搜索的牛顿法收敛得更快 ($n_{\\mathrm{TR}}  n_{\\mathrm{NT}}$)。\n   - 你的程序应生成一行输出，其中包含所有四个测试用例的结果，形式为方括号内包含的逗号分隔列表，每个测试用例的结果本身是按上述顺序排列的列表。例如：\n     $$\n     [\\,[n_{\\mathrm{TR}},n_{\\mathrm{NT}},f_{\\mathrm{TR}},f_{\\mathrm{NT}},\\text{contractions},\\text{expansions},\\bar{\\rho},\\text{backtracks},\\|g_{\\mathrm{TR}}\\|_2,\\|g_{\\mathrm{NT}}\\|_2,\\text{TR\\_faster}],\\ldots\\,]\n     $$\n   - 不涉及物理单位；所有量均为无量纲实数。",
            "solution": "目标是比较信赖域方法和带回溯线搜索的牛顿法在一个具有弯曲窄谷的函数上的表现。Rosenbrock 函数是一个典型的选择，因为它是二次连续可微的，在 $x^\\star = (1,1)$ 处有唯一最小值，并且沿一个方向表现出强曲率和一个弯曲的窄谷，这使得不进行全局化的纯牛顿法或纯梯度法难以处理。\n\n基于原理的设计：\n\n1. 从二阶泰勒模型开始。对于在 $x_k$ 处的二次可微函数 $f$，步长 $p$ 的局部二次近似为\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\tfrac{1}{2} p^\\top H_k p,\n$$\n其中 $g_k = \\nabla f(x_k)$ 且 $H_k = \\nabla^2 f(x_k)$。该模型由泰勒定理证明其合理性，并且是牛顿更新（$m_k$ 的无约束最小化）和信赖域更新（球内的有约束最小化）的基础。\n\n2. 信赖域方法：约束问题\n$$\n\\min_{\\|p\\|_2 \\le \\Delta_k} m_k(p)\n$$\n用于确保步长的稳健性，使其不完全依赖于远离 $x_k$ 的二次模型的保真度。当 $H_k$ 是对称正定时，如果 $\\|p_{\\mathrm{B}}\\|_2 \\le \\Delta_k$，则无约束最小化点 $p_{\\mathrm{B}} = -H_k^{-1} g_k$ 位于球内，此时信赖域步等于牛顿步。否则，可以使用狗腿策略：首先沿最速下降方向移动到柯西点 $p_{\\mathrm{C}}$（它最小化了沿 $-g_k$ 的 $m_k$），然后沿从 $p_{\\mathrm{C}}$ 到 $p_{\\mathrm{B}}$ 的线段移动以达到边界 $\\|p\\|_2 = \\Delta_k$。如果 $H_k$ 不是对称正定（例如，不定），则二次模型在负曲率方向上可能不可靠。在这种情况下，选择投影到边界的柯西点是一种有原则的备用策略：\n$$\n\\alpha_{\\mathrm{C}} = \\begin{cases}\n\\dfrac{\\|g_k\\|_2^2}{g_k^\\top H_k g_k},  \\text{if } g_k^\\top H_k g_k  0, \\\\\n\\dfrac{\\Delta_k}{\\|g_k\\|_2},  \\text{otherwise},\n\\end{cases}\n\\quad p_{\\mathrm{C}} = -\\alpha_{\\mathrm{C}}\\,g_k,\n\\quad p = \\begin{cases}\np_{\\mathrm{C}},  \\|p_{\\mathrm{C}}\\|_2 \\le \\Delta_k, \\\\\n-\\Delta_k\\,\\dfrac{g_k}{\\|g_k\\|_2},  \\text{otherwise}.\n\\end{cases}\n$$\n\n3. 一致性比率和接受准则：为了量化二次模型预测实际函数下降的好坏程度，定义\n$$\n\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)} = \\frac{f(x_k) - f(x_k + p_k)}{-\\left(g_k^\\top p_k + \\tfrac{1}{2} p_k^\\top H_k p_k\\right)}.\n$$\n当 $m_k(0) - m_k(p_k)  0$ 时，该比率衡量了模型与实际下降之间的一致性。使用一个阈值 $\\eta \\in (0,1)$，如果 $\\rho_k \\ge \\eta$，则接受该步并更新 $x_{k+1} = x_k + p_k$；否则拒绝并保持 $x_{k+1} = x_k$。根据以下规则更新半径 $\\Delta_k$：\n- 如果 $\\rho_k  0.25$，收缩半径：$\\Delta_{k+1} = \\gamma_1\\,\\Delta_k$，其中 $\\gamma_1 \\in (0,1)$。\n- 如果 $\\rho_k  0.75$ 且步长几乎位于边界上 $\\|p_k\\|_2 \\ge 0.99\\,\\Delta_k$，则扩大半径：$\\Delta_{k+1} = \\min(\\gamma_2\\,\\Delta_k,\\Delta_{\\max})$，其中 $\\gamma_2  1$ 和一个上限 $\\Delta_{\\max}$。\n- 否则保持半径：$\\Delta_{k+1} = \\Delta_k$。\n\n这些规则在谨慎和积极之间取得了平衡，当模型预测准确时增大信赖域，反之则缩小信赖域。\n\n4. 带 Armijo 回溯的牛顿法：牛顿方向 $d_k$ 求解\n$$\nH_k d_k = -g_k.\n$$\n如果 $H_k$ 不合适（例如，不定或接近奇异），求解可能会产生一个非下降方向，即 $g_k^\\top d_k \\ge 0$。在这种情况下，将 $d_k$ 替换为最速下降方向 $d_k = -g_k$。使用 Armijo 回溯线搜索：从 $\\alpha_k = 1$ 开始，以 $\\beta \\in (0,1)$ 的比例缩减 $\\alpha_k \\leftarrow \\beta\\,\\alpha_k$，直到\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1\\,\\alpha_k\\,g_k^\\top d_k,\n$$\n其中 $c_1 \\in (0,1)$。Armijo 条件保证了与方向导数成比例的充分下降。限制每次迭代的回溯步数为一个固定的最大值，以确保终止。\n\n5. 停止准则：对于两种方法，均使用\n$$\n\\|\\nabla f(x_k)\\|_2 \\le \\text{tol}\n$$\n或达到最大迭代预算 $K_{\\max}$。\n\n6. Rosenbrock 函数的实现：使用推导出的公式精确计算 $f(x)$、$g(x)$ 和 $H(x)$，\n$$\ng(x) = \\begin{bmatrix}\n-2(1 - x_1) - 400 x_1 (x_2 - x_1^2) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}, \\quad\nH(x) = \\begin{bmatrix}\n2 - 400 x_2 + 1200 x_1^2  -400 x_1 \\\\\n-400 x_1  200\n\\end{bmatrix}.\n$$\n通过 Cholesky 分解检测 $H(x)$ 是否为对称正定；如果分解失败，则认为 $H(x)$ 不适合用于狗腿牛顿步，并使用柯西点策略。\n\n7. 要收集的指标：\n- 对于信赖域方法：计算迭代次数 $n_{\\mathrm{TR}}$、接受的步数、拒绝的步数、半径的收缩和扩张次数，并跟踪 $(\\Delta_k)$ 和 $(\\rho_k)$。报告在接受的步数上的平均一致性比率\n$$\n\\bar{\\rho} = \\frac{1}{N_{\\mathrm{acc}}}\\sum_{k \\in \\mathcal{A}} \\rho_k,\n$$\n约定如果没有接受的步，则 $\\bar{\\rho} = 0$。同时报告最终的 $f_{\\mathrm{TR}}$ 和 $\\|g_{\\mathrm{TR}}\\|_2$。\n- 对于牛顿线搜索：计算迭代次数 $n_{\\mathrm{NT}}$、总回溯缩减次数，并报告最终的 $f_{\\mathrm{NT}}$ 和 $\\|g_{\\mathrm{NT}}\\|_2$。\n\n8. 测试套件和输出：使用提供的参数和终止准则运行指定的四个测试用例。对于每个用例，输出列表\n$$\n\\left[n_{\\mathrm{TR}},\\,n_{\\mathrm{NT}},\\,f_{\\mathrm{TR}},\\,f_{\\mathrm{NT}},\\,\\text{contractions},\\,\\text{expansions},\\,\\bar{\\rho},\\,\\text{backtracks},\\,\\|g_{\\mathrm{TR}}\\|_2,\\,\\|g_{\\mathrm{NT}}\\|_2,\\,\\text{TR\\_faster}\\right],\n$$\n使用保留六位小数的浮点数，以及指示信赖域方法是否比牛顿线搜索使用更少迭代次数的最终布尔值。将四个列表聚合成单行，形式为方括号内包含的逗号分隔列表。\n\n此设计直接源于泰勒模型和充分下降条件，并转化为在像 Rosenbrock 函数那样的窄谷上稳健的显式算法，从而能够在信赖域全局化和线搜索全局化之间进行有原则的比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    x1, x2 = x[0], x[1]\n    f = (1 - x1)**2 + 100.0 * (x2 - x1**2)**2\n    g = np.array([\n        -2.0 * (1.0 - x1) - 400.0 * x1 * (x2 - x1**2),\n        200.0 * (x2 - x1**2)\n    ])\n    H = np.array([\n        [2.0 - 400.0 * x2 + 1200.0 * x1**2, -400.0 * x1],\n        [-400.0 * x1, 200.0]\n    ])\n    return f, g, H\n\ndef is_spd(H):\n    try:\n        np.linalg.cholesky(H)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\ndef cauchy_point(g, H, Delta):\n    g_norm = np.linalg.norm(g)\n    gHg = float(g @ (H @ g))\n    if gHg > 0:\n        alpha_c = (g_norm**2) / gHg\n        p = -alpha_c * g\n        if np.linalg.norm(p) > Delta:\n            p = -Delta * g / g_norm\n    else:\n        # Nonpositive curvature: take boundary step along -g\n        p = -Delta * g / (g_norm if g_norm > 0 else 1.0)\n    return p\n\ndef dogleg_step(g, H, Delta):\n    # If Hessian is SPD, attempt dogleg between Cauchy and Newton steps\n    if is_spd(H):\n        # Newton step\n        try:\n            pB = -np.linalg.solve(H, g)\n        except np.linalg.LinAlgError:\n            # Fallback: Cauchy to boundary\n            return cauchy_point(g, H, Delta)\n        # Cauchy step\n        pU = cauchy_point(g, H, Delta * 1.0e6)  # unconstrained cauchy magnitude\n        # If Cauchy exceeds Delta, move to boundary along -g\n        if np.linalg.norm(pU) >= Delta:\n            return -Delta * g / (np.linalg.norm(g) if np.linalg.norm(g) > 0 else 1.0)\n        # If Newton inside TR, take it\n        if np.linalg.norm(pB) = Delta:\n            return pB\n        # Else, find tau in (0,1) such that ||pU + tau*(pB - pU)|| = Delta\n        a = pB - pU\n        pu_dot_a = float(pU @ a)\n        a_norm2 = float(a @ a)\n        pu_norm2 = float(pU @ pU)\n        # Solve quadratic: (pu_norm2) + 2*tau*(pu_dot_a) + tau^2*(a_norm2) = Delta^2\n        # tau = [-pu_dot_a + sqrt(pu_dot_a^2 + a_norm2*(Delta^2 - pu_norm2))] / a_norm2\n        rad = pu_dot_a**2 + a_norm2 * (Delta**2 - pu_norm2)\n        rad = max(rad, 0.0)\n        tau = (-pu_dot_a + np.sqrt(rad)) / (a_norm2 if a_norm2 > 0 else 1.0)\n        return pU + tau * a\n    else:\n        # Fallback: Cauchy point projected to boundary\n        return cauchy_point(g, H, Delta)\n\ndef trust_region_method(x0, Delta0, tol=1e-6, max_iters=200, eta=0.1, gamma1=0.25, gamma2=2.0, Delta_max=100.0):\n    x = np.array(x0, dtype=float)\n    Delta = float(Delta0)\n    n_iters = 0\n    contractions = 0\n    expansions = 0\n    accepted = 0\n    rejected = 0\n    rho_acc_sum = 0.0\n    # histories (not printed, but tracked)\n    Delta_hist = []\n    rho_hist = []\n    for k in range(max_iters):\n        f, g, H = rosenbrock(x)\n        gnorm = np.linalg.norm(g)\n        if gnorm = tol:\n            break\n        p = dogleg_step(g, H, Delta)\n        # Predicted reduction\n        gTp = float(g @ p)\n        pTHp = float(p @ (H @ p))\n        pred_red = -(gTp + 0.5 * pTHp)\n        # Actual reduction\n        f_new, _, _ = rosenbrock(x + p)\n        act_red = f - f_new\n        if pred_red > 0:\n            rho = act_red / pred_red\n        else:\n            rho = -np.inf\n        rho_hist.append(rho)\n        # Acceptance\n        if rho >= eta and f_new  f:\n            x = x + p\n            accepted += 1\n            if np.isfinite(rho):\n                rho_acc_sum += rho\n        else:\n            rejected += 1\n            # keep x\n        # Radius update rules\n        if rho  0.25 or not np.isfinite(rho):\n            Delta = max(gamma1 * Delta, 1e-12)\n            contractions += 1\n        elif rho > 0.75 and np.linalg.norm(p) >= 0.99 * Delta:\n            new_Delta = min(gamma2 * Delta, Delta_max)\n            if new_Delta > Delta + 1e-15:\n                expansions += 1\n            Delta = new_Delta\n        # Track radius\n        Delta_hist.append(Delta)\n        n_iters += 1\n    f_final, g_final, _ = rosenbrock(x)\n    gnorm_final = float(np.linalg.norm(g_final))\n    avg_rho = (rho_acc_sum / accepted) if accepted > 0 else 0.0\n    return {\n        \"x\": x,\n        \"n_iters\": n_iters,\n        \"f\": float(f_final),\n        \"gnorm\": gnorm_final,\n        \"contractions\": contractions,\n        \"expansions\": expansions,\n        \"accepted\": accepted,\n        \"rejected\": rejected,\n        \"avg_rho\": float(avg_rho),\n        \"Delta_hist\": Delta_hist,\n        \"rho_hist\": rho_hist\n    }\n\ndef newton_line_search(x0, tol=1e-6, max_iters=200, c1=1e-4, beta=0.5, max_backtracks=50):\n    x = np.array(x0, dtype=float)\n    n_iters = 0\n    total_backtracks = 0\n    for k in range(max_iters):\n        f, g, H = rosenbrock(x)\n        gnorm = np.linalg.norm(g)\n        if gnorm = tol:\n            break\n        # Compute Newton direction\n        try:\n            d = -np.linalg.solve(H, g)\n        except np.linalg.LinAlgError:\n            d = -g\n        # Ensure descent\n        if float(g @ d) >= 0.0:\n            d = -g\n        # Backtracking Armijo\n        alpha = 1.0\n        backtracks = 0\n        gTd = float(g @ d)\n        f_curr = f\n        while backtracks  max_backtracks:\n            x_trial = x + alpha * d\n            f_trial, _, _ = rosenbrock(x_trial)\n            if f_trial = f_curr + c1 * alpha * gTd:\n                break\n            alpha *= beta\n            backtracks += 1\n        total_backtracks += backtracks\n        # If no acceptable alpha found, stop\n        if backtracks == max_backtracks and f_trial > f_curr + c1 * alpha * gTd:\n            # Unable to find sufficient decrease; terminate to avoid stagnation\n            break\n        x = x + alpha * d\n        n_iters += 1\n    f_final, g_final, _ = rosenbrock(x)\n    gnorm_final = float(np.linalg.norm(g_final))\n    return {\n        \"x\": x,\n        \"n_iters\": n_iters,\n        \"f\": float(f_final),\n        \"gnorm\": gnorm_final,\n        \"backtracks\": total_backtracks\n    }\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (x0, Delta0)\n    test_cases = [\n        (np.array([-1.2, 1.0]), 1.0),       # Case 1\n        (np.array([0.0, 0.0]), 1e-3),       # Case 2\n        (np.array([1.0, 1.0]), 1.0),        # Case 3\n        (np.array([-1.5, 2.0]), 0.5),       # Case 4\n    ]\n\n    # Common parameters\n    tol = 1e-6\n    max_iters = 200\n    # Trust region parameters\n    eta = 0.1\n    gamma1 = 0.25\n    gamma2 = 2.0\n    Delta_max = 100.0\n    # Line search parameters\n    c1 = 1e-4\n    beta = 0.5\n    max_backtracks = 50\n\n    results = []\n    for x0, Delta0 in test_cases:\n        # Trust region\n        tr = trust_region_method(\n            x0=x0, Delta0=Delta0, tol=tol, max_iters=max_iters,\n            eta=eta, gamma1=gamma1, gamma2=gamma2, Delta_max=Delta_max\n        )\n        # Newton with line search\n        nt = newton_line_search(\n            x0=x0, tol=tol, max_iters=max_iters,\n            c1=c1, beta=beta, max_backtracks=max_backtracks\n        )\n        # Build result list for this case\n        n_tr = tr[\"n_iters\"]\n        n_nt = nt[\"n_iters\"]\n        f_tr = round(tr[\"f\"], 6)\n        f_nt = round(nt[\"f\"], 6)\n        contractions = tr[\"contractions\"]\n        expansions = tr[\"expansions\"]\n        avg_rho = round(tr[\"avg_rho\"], 6)\n        backtracks_nt = nt[\"backtracks\"]\n        g_tr = round(tr[\"gnorm\"], 6)\n        g_nt = round(nt[\"gnorm\"], 6)\n        tr_faster = n_tr  n_nt\n        case_result = [n_tr, n_nt, f_tr, f_nt, contractions, expansions, avg_rho, backtracks_nt, g_tr, g_nt, tr_faster]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # Print a single line: list of per-case lists.\n    def format_item(item):\n        if isinstance(item, bool):\n            return \"True\" if item else \"False\"\n        elif isinstance(item, (int, np.integer)):\n            return str(int(item))\n        elif isinstance(item, float):\n            # Ensure standard Python float formatting (already rounded).\n            return str(item)\n        elif isinstance(item, list):\n            return \"[\" + \",\".join(format_item(elem) for elem in item) + \"]\"\n        else:\n            return str(item)\n\n    print(\"[\" + \",\".join(format_item(case) for case in results) + \"]\")\n\nsolve()\n```"
        }
    ]
}