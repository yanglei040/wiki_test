{
    "hands_on_practices": [
        {
            "introduction": "This first practice lays the mathematical groundwork for solving the trust-region subproblem. You will explore the common scenario where the local quadratic model is convex (i.e., has a positive definite Hessian) and the optimal step lies on the boundary of the trust region. By deriving the 'secular equation' from first principles and solving it for a concrete example, you will gain a deep understanding of how the Lagrange multiplier pinpoints the exact solution on the boundary .",
            "id": "3284949",
            "problem": "Consider the trust-region subproblem with a symmetric positive definite (SPD) matrix $H \\in \\mathbb{R}^{n \\times n}$, gradient $g \\in \\mathbb{R}^{n}$, and trust-region radius $\\Delta  0$: minimize the quadratic model $m(p) = g^{\\mathsf{T}} p + \\tfrac{1}{2} p^{\\mathsf{T}} H p$ subject to the Euclidean norm constraint $\\|p\\| \\le \\Delta$. Starting from first principles (namely, the definition of the optimization problem and the Karush–Kuhn–Tucker conditions for constrained optimization), derive a scalar equation in a single variable that characterizes boundary solutions where $\\|p\\| = \\Delta$ in the SPD case. Then, using only fundamental linear algebra facts and properties of SPD matrices, justify why this scalar equation admits exactly one solution for $\\lambda \\ge 0$ and explain why the corresponding residual function is strictly decreasing.\n\nNext, outline an efficient root-finding strategy for computing the unique Lagrange multiplier $\\lambda \\ge 0$ that solves the scalar equation. Your outline should specify:\n- how to evaluate the residual and its derivative using linear solves with $H + \\lambda I$,\n- a safe globalization strategy to guarantee convergence, and\n- how to obtain an initial interval or iterate that leads to rapid convergence in practice.\n\nFinally, for the specific instance with\n- $H = \\mathrm{diag}(3, 1)$,\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$,\n- $\\Delta = 2$,\ncompute the exact value of the unique $\\lambda \\ge 0$ that satisfies the boundary condition. Provide the exact value of $\\lambda$ as your final answer. Do not round.",
            "solution": "The problem is subjected to validation and is found to be valid. It is a well-posed, scientifically grounded problem from the field of numerical optimization, with all necessary data and definitions provided.\n\nThe trust-region subproblem is to find a step $p \\in \\mathbb{R}^{n}$ that minimizes a quadratic model of a function, subject to a constraint on the step length. The problem is formulated as:\n$$\n\\min_{p \\in \\mathbb{R}^{n}} m(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\nwhere $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix, $g \\in \\mathbb{R}^{n}$ is a gradient vector, and $\\Delta  0$ is the trust-region radius. The constraint is on the Euclidean norm, $\\|p\\| = \\sqrt{p^{\\mathsf{T}}p}$.\n\nTo derive the characterizing equation for a solution on the boundary of the trust region, we employ the Karush–Kuhn–Tucker (KKT) conditions for constrained optimization. The inequality constraint is $g_c(p) = \\|p\\|^2 - \\Delta^2 \\le 0$. The Lagrangian function is:\n$$\nL(p, \\lambda) = m(p) + \\frac{\\lambda}{2} g_c(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p + \\frac{\\lambda}{2} (p^{\\mathsf{T}}p - \\Delta^2)\n$$\nwhere $\\lambda$ is the Lagrange multiplier. The factor of $\\frac{1}{2}$ is introduced for convenience. The KKT conditions for a minimizer $p^*$ are:\n1.  Stationarity: $\\nabla_p L(p^*, \\lambda) = g + H p^* + \\lambda p^* = 0$.\n2.  Primal Feasibility: $\\|p^*\\|^2 \\le \\Delta^2$.\n3.  Dual Feasibility: $\\lambda \\ge 0$.\n4.  Complementary Slackness: $\\lambda (\\|p^*\\|^2 - \\Delta^2) = 0$.\n\nThe problem asks to characterize solutions on the boundary, where $\\|p^*\\| = \\Delta$. From the complementary slackness condition, this implies that $\\lambda$ can be non-negative, $\\lambda \\ge 0$. The stationarity condition can be rearranged as:\n$$\n(H + \\lambda I) p^* = -g\n$$\nwhere $I$ is the identity matrix. Since $H$ is SPD, its eigenvalues $\\mu_i$ are all strictly positive, $\\mu_i  0$. For any $\\lambda \\ge 0$, the eigenvalues of the matrix $(H + \\lambda I)$ are $\\mu_i + \\lambda$, which are also strictly positive. Therefore, $(H + \\lambda I)$ is SPD and, consequently, invertible for all $\\lambda \\ge 0$. We can thus uniquely express the solution $p^*$ as a function of $\\lambda$:\n$$\np(\\lambda) = -(H + \\lambda I)^{-1} g\n$$\nFor a boundary solution, this step $p(\\lambda)$ must satisfy the constraint $\\|p(\\lambda)\\| = \\Delta$. Substituting the expression for $p(\\lambda)$ gives the scalar equation in the single variable $\\lambda$:\n$$\n\\|-(H + \\lambda I)^{-1} g\\| = \\Delta\n$$\nThis is the fundamental equation, often called the secular equation, that characterizes the Lagrange multiplier $\\lambda$ for a solution on the boundary of the trust region.\n\nNext, we justify that for a boundary solution, this equation admits exactly one solution for $\\lambda \\ge 0$. Let us analyze the function $\\phi(\\lambda) = \\|p(\\lambda)\\| = \\|(H + \\lambda I)^{-1} g\\|$ for $\\lambda \\ge 0$. It is more convenient to analyze its square, $\\psi(\\lambda) = \\phi(\\lambda)^2 = \\|p(\\lambda)\\|^2$. Let $H = Q \\Lambda Q^{\\mathsf{T}}$ be the spectral decomposition of $H$, where $Q$ is an orthogonal matrix and $\\Lambda = \\mathrm{diag}(\\mu_1, \\ldots, \\mu_n)$ is the diagonal matrix of positive eigenvalues of $H$. We can write $\\psi(\\lambda)$ as:\n$$\n\\psi(\\lambda) = g^{\\mathsf{T}} (H + \\lambda I)^{-2} g = g^{\\mathsf{T}} (Q (\\Lambda + \\lambda I) Q^{\\mathsf{T}})^{-2} g = g^{\\mathsf{T}} Q (\\Lambda + \\lambda I)^{-2} Q^{\\mathsf{T}} g\n$$\nLet $\\hat{g} = Q^{\\mathsf{T}} g$. Then $\\psi(\\lambda)$ becomes:\n$$\n\\psi(\\lambda) = \\hat{g}^{\\mathsf{T}} (\\Lambda + \\lambda I)^{-2} \\hat{g} = \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^2}\n$$\nTo analyze the monotonicity of this function, we compute its derivative with respect to $\\lambda$:\n$$\n\\psi'(\\lambda) = \\sum_{i=1}^{n} \\hat{g}_i^2 \\frac{d}{d\\lambda} (\\mu_i + \\lambda)^{-2} = \\sum_{i=1}^{n} \\hat{g}_i^2 (-2)(\\mu_i + \\lambda)^{-3} (1) = -2 \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^3}\n$$\nSince $\\mu_i  0$ and we consider $\\lambda \\ge 0$, the denominator $(\\mu_i + \\lambda)^3$ is always positive. The numerators $\\hat{g}_i^2$ are non-negative. If $g \\neq 0$, then $\\hat{g} \\neq 0$, and at least one $\\hat{g}_i^2$ is positive. Thus, for $g \\neq 0$, $\\psi'(\\lambda)  0$ for all $\\lambda \\ge 0$. This proves that $\\psi(\\lambda)$ is a strictly decreasing function of $\\lambda$ on $[0, \\infty)$. Since $\\phi(\\lambda)=\\sqrt{\\psi(\\lambda)}$ and the square root function is strictly increasing for positive arguments, $\\phi(\\lambda)$ is also a strictly decreasing function of $\\lambda$ on $[0, \\infty)$.\n\nThe existence and uniqueness of a solution $\\lambda \\ge 0$ to $\\phi(\\lambda) = \\Delta$ depends on the value of $\\phi(0) = \\|-H^{-1}g\\|$, which is the norm of the unconstrained minimizer (the full Newton step).\n- If $\\| -H^{-1}g \\| \\le \\Delta$, the unconstrained solution is feasible. The KKT conditions are satisfied with $p^* = -H^{-1}g$ and $\\lambda = 0$. This is the interior solution.\n- If $\\| -H^{-1}g \\|  \\Delta$, the solution must lie on the boundary. We analyze the function $\\phi(\\lambda)$ on $[0, \\infty)$. We have $\\phi(0) = \\|-H^{-1}g\\|  \\Delta$.\nIn the limit as $\\lambda \\to \\infty$, we have $\\phi(\\lambda) = \\|-(H+\\lambda I)^{-1}g\\| \\approx \\|\\frac{-1}{\\lambda}I g\\| = \\frac{\\|g\\|}{\\lambda} \\to 0$.\nSince $\\phi(\\lambda)$ is a continuous and strictly decreasing function on $[0, \\infty)$ with $\\phi(0)  \\Delta$ and $\\lim_{\\lambda \\to \\infty} \\phi(\\lambda) = 0$, by the Intermediate Value Theorem, there must exist exactly one value $\\lambda^*  0$ such that $\\phi(\\lambda^*) = \\Delta$. Thus, in the boundary case, there is a unique positive Lagrange multiplier.\n\nAn efficient root-finding strategy for the secular equation typically uses Newton's method on a related function, for instance $r(\\lambda) = \\|p(\\lambda)\\| - \\Delta = 0$.\nThe Newton iteration is $\\lambda_{k+1} = \\lambda_k - r(\\lambda_k)/r'(\\lambda_k)$.\nTo evaluate $r(\\lambda)$ and its derivative $r'(\\lambda)$:\n1.  **Evaluation of $r(\\lambda)$**: For a given $\\lambda$, we must compute $p(\\lambda) = -(H+\\lambda I)^{-1}g$. This is done by solving the linear system $(H+\\lambda I)p = -g$. Since $(H+\\lambda I)$ is SPD, this system is efficiently solved via Cholesky decomposition. After computing $p$, we evaluate $r(\\lambda) = \\|p\\| - \\Delta$.\n2.  **Evaluation of $r'(\\lambda)$**: We must compute $\\frac{d}{d\\lambda}\\|p(\\lambda)\\|$.\n    $r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} p'(\\lambda)}{\\|p(\\lambda)\\|}$, where $p'(\\lambda) = \\frac{d p(\\lambda)}{d\\lambda}$.\n    By differentiating the equation $(H + \\lambda I) p(\\lambda) = -g$ with respect to $\\lambda$, we get:\n    $I p(\\lambda) + (H + \\lambda I) p'(\\lambda) = 0 \\implies p'(\\lambda) = -(H + \\lambda I)^{-1} p(\\lambda)$.\n    Let $q = p'(\\lambda)$. To compute $q$, we solve another linear system $(H + \\lambda I)q = -p(\\lambda)$. Note that the Cholesky factorization of $(H+\\lambda I)$ from the computation of $p(\\lambda)$ can be reused.\n    Then, $r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} q}{\\|p(\\lambda)\\|}$.\nA Newton step consists of two linear solves with the same matrix.\n- **Globalization Strategy**: Newton's method is locally quadratically convergent but may not converge globally. A safeguarding strategy is necessary. Since we know the root $\\lambda^*$ is bracketed by $[0, \\infty)$ and the function $r(\\lambda)$ is convex and monotonic, simple safeguards are effective. For example, if a Newton step $\\lambda_{k+1}$ is negative, it can be replaced by a bisection step on a known bracket, such as $[\\lambda_k/2, \\lambda_k]$.\n- **Initial Iterate**: We only need to solve the secular equation if $\\|-H^{-1}g\\|  \\Delta$. In this case, we know $\\lambda^*  0$. A common and simple starting guess is $\\lambda_0 = 0$. From there, the Newton iteration will produce a positive $\\lambda_1$. More sophisticated initial guesses can be derived from eigenvalue bounds, such as $\\lambda_0 = \\frac{\\|g\\|}{\\Delta} - \\mu_{\\min}$, though computing $\\mu_{\\min}$ can be costly. Starting with $\\lambda_0=0$ in a safeguarded scheme is robust.\n\nFinally, we compute the exact value of $\\lambda$ for the given specific instance:\n- $H = \\mathrm{diag}(3, 1)$\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$\n- $\\Delta = 2$\n\nFirst, we check if the solution is on the boundary. We compute the unconstrained minimizer $p_{unc} = -H^{-1}g$.\n$H^{-1} = \\mathrm{diag}(1/3, 1)$.\n$p_{unc} = -\\begin{pmatrix} 1/3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -10/3 \\\\ 0 \\end{pmatrix}$.\nThe norm of this step is $\\|p_{unc}\\| = \\sqrt{(-10/3)^2 + 0^2} = 10/3$.\nSince $10/3 \\approx 3.33  \\Delta = 2$, the solution lies on the boundary, and we must find a $\\lambda  0$.\n\nWe solve the secular equation $\\|p(\\lambda)\\| = \\Delta$. The step $p(\\lambda)$ is given by:\n$p(\\lambda) = -(H + \\lambda I)^{-1} g$.\n$H + \\lambda I = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3+\\lambda  0 \\\\ 0  1+\\lambda \\end{pmatrix}$.\n$(H + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{3+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix}$.\n$p(\\lambda) = -\\begin{pmatrix} \\frac{1}{3+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix}$.\n\nNow, we enforce the boundary condition $\\|p(\\lambda)\\| = 2$:\n$\\left\\| \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix} \\right\\| = 2$.\n$\\sqrt{\\left(-\\frac{10}{3+\\lambda}\\right)^2 + 0^2} = 2$.\nAs we seek $\\lambda  0$, the term $3+\\lambda$ is positive, so we can write:\n$\\frac{10}{3+\\lambda} = 2$.\n$10 = 2(3+\\lambda)$.\n$10 = 6 + 2\\lambda$.\n$4 = 2\\lambda$.\n$\\lambda = 2$.\n\nThis is the unique positive Lagrange multiplier that satisfies the boundary condition.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "Why not simply take a step in the direction of steepest descent? This exercise provides a vivid answer by exploring a model with an indefinite Hessian, a common occurrence in non-convex optimization. You will construct a scenario where the Cauchy point—the minimizer along the steepest descent direction—yields a dismal model improvement compared to the true subproblem solution . This stark comparison highlights the critical importance of using curvature information to navigate the optimization landscape effectively.",
            "id": "3284825",
            "problem": "Consider a quadratic model arising in a Trust Region (TR) method for unconstrained optimization. At a current iterate, the local second-order model is\n$$\nm(p) \\;=\\; f(x_k) \\;+\\; g^{\\top}p \\;+\\; \\frac{1}{2}p^{\\top}B\\,p,\n$$\nwhere $p \\in \\mathbb{R}^{2}$ is the step, $g \\in \\mathbb{R}^{2}$ is the gradient at $x_k$, and $B \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric Hessian approximation. The TR subproblem is to minimize $m(p)$ subject to the trust-region constraint $\\|p\\| \\leq \\Delta$. The Cauchy point is defined as the minimizer of $m(p)$ along the ray $p = -\\alpha g$ subject to $\\|p\\| \\leq \\Delta$.\n\nConstruct the specific two-dimensional instance with\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \n\\qquad\nB \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1,\n$$\nand consider the corresponding TR subproblem. Using only fundamental definitions and principles of TR methods and quadratic models, determine the ratio $\\rho$ between the predicted reduction at the Cauchy point and the predicted reduction at the global solution of the TR subproblem. The predicted reduction at a step $p$ is defined as $m(0) - m(p)$.\n\nProvide your final numerical value for $\\rho$ in scientific notation rounded to four significant figures.",
            "solution": "The Trust Region (TR) subproblem seeks\n$$\n\\min_{p \\in \\mathbb{R}^{2}} \\;\\; m(p) \\;=\\; g^{\\top}p + \\frac{1}{2}p^{\\top}B\\,p \n\\quad \\text{subject to} \\quad \\|p\\| \\leq \\Delta,\n$$\nwhere $f(x_k)$ is constant with respect to $p$ and thus omitted from the minimization. The predicted reduction at a step $p$ is\n$$\n\\text{predicted reduction}(p) \\;=\\; m(0) - m(p) \\;=\\; -\\,g^{\\top}p \\;-\\; \\frac{1}{2}p^{\\top}B\\,p.\n$$\n\nWe are given\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n\\qquad\nB \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1.\n$$\nWe first compute the Cauchy point, which minimizes $m(p)$ on the ray $p=-\\alpha g$ subject to $\\|p\\| \\leq \\Delta$. Parameterize $p$ as $p(\\alpha) = -\\alpha g = \\begin{pmatrix} -\\alpha \\\\ 0 \\end{pmatrix}$ with $\\alpha \\geq 0$ and the constraint $\\|p(\\alpha)\\| = \\alpha\\|g\\| \\leq \\Delta$. Since $\\|g\\| = 1$, this is $\\alpha \\leq \\Delta = 1$.\n\nRestricting $m$ to this ray,\n$$\nm(-\\alpha g) \\;=\\; g^{\\top}(-\\alpha g) + \\frac{1}{2}(-\\alpha g)^{\\top}B(-\\alpha g).\n$$\nWe compute $g^{\\top}(-\\alpha g) = -\\alpha \\|g\\|^{2} = -\\alpha$ and\n$$\n(-\\alpha g)^{\\top}B(-\\alpha g) \\;=\\; \\alpha^{2} g^{\\top}B g \\;=\\; \\alpha^{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\;=\\; \\alpha^{2} \\cdot 1 \\;=\\; \\alpha^{2}.\n$$\nTherefore,\n$$\nm(-\\alpha g) \\;=\\; -\\alpha + \\frac{1}{2}\\alpha^{2}.\n$$\nThis is a one-dimensional quadratic in $\\alpha$; the unconstrained minimizer satisfies\n$$\n\\frac{d}{d\\alpha}\\left(-\\alpha + \\frac{1}{2}\\alpha^{2}\\right) \\;=\\; -1 + \\alpha \\;=\\; 0\n\\quad \\Rightarrow \\quad \\alpha^{\\star} \\;=\\; 1.\n$$\nSince the trust-region bound is $\\alpha \\leq 1$, the unconstrained minimizer lies on the boundary and is feasible. Thus, the Cauchy point is\n$$\np_{\\mathrm{C}} \\;=\\; -\\alpha^{\\star} g \\;=\\; \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\nThe predicted reduction at the Cauchy point is\n$$\n\\text{PR}_{\\mathrm{C}} \n\\;=\\; -\\,g^{\\top}p_{\\mathrm{C}} - \\frac{1}{2}p_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}}\n\\;=\\; -\\,\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} -1  0 \\end{pmatrix}\\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\nCompute the terms:\n$$\n-\\,g^{\\top}p_{\\mathrm{C}} \\;=\\; -(-1) \\;=\\; 1, \n\\qquad\np_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}} \\;=\\; (-1)^{2}\\cdot 1 + 0^{2}\\cdot (-1000) \\;=\\; 1.\n$$\nHence,\n$$\n\\text{PR}_{\\mathrm{C}} \\;=\\; 1 - \\frac{1}{2}\\cdot 1 \\;=\\; \\frac{1}{2}.\n$$\n\nNext, we determine the global TR solution. The matrix $B$ is indefinite with eigenvalues $1$ and $-1000$. The model along the second coordinate direction has strong negative curvature. The TR subproblem minimizes\n$$\nm(p) \\;=\\; p_1 + \\frac{1}{2}\\left(p_1^{2} - 1000\\,p_2^{2}\\right)\n$$\nover $(p_1,p_2)$ with $p_1^{2}+p_2^{2} \\leq 1$. To minimize $m(p)$, we need to make the term $-1000 p_2^2$ as negative as possible, which means maximizing $|p_2|$. Subject to the constraint, the maximum possible value for $p_2^2$ is $1$, which occurs when $p_1=0$ and $p_2 = \\pm 1$. With $p_1=0$, the linear term $p_1$ is zero. Therefore, the minimum value of $m(p)$ is achieved at $p^\\star = (0, \\pm 1)$. Let's choose $p^{\\star} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ for concreteness. The predicted reduction at the global TR solution is\n$$\n\\text{PR}_{\\star} \n\\;=\\; -\\,g^{\\top}p^{\\star} - \\frac{1}{2}(p^{\\star})^{\\top}B\\,p^{\\star}\n\\;=\\; -\\,\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} 0  1 \\end{pmatrix}\\begin{pmatrix} 1  0 \\\\ 0  -1000 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nCompute the terms:\n$$\n-\\,g^{\\top}p^{\\star} \\;=\\; -0 \\;=\\; 0,\n\\qquad\n(p^{\\star})^{\\top}B\\,p^{\\star} \\;=\\; 0^{2}\\cdot 1 + 1^{2}\\cdot (-1000) \\;=\\; -1000.\n$$\nTherefore,\n$$\n\\text{PR}_{\\star} \\;=\\; 0 - \\frac{1}{2}\\cdot(-1000) \\;=\\; 500.\n$$\n\nThe requested ratio $\\rho$ between the predicted reduction at the Cauchy point and at the global TR solution is\n$$\n\\rho \\;=\\; \\frac{\\text{PR}_{\\mathrm{C}}}{\\text{PR}_{\\star}} \\;=\\; \\frac{\\frac{1}{2}}{500} \\;=\\; \\frac{1}{1000} \\;=\\; 10^{-3}.\n$$\nExpressed in scientific notation rounded to four significant figures, this is\n$$\n\\rho \\;=\\; 1.000 \\times 10^{-3}.\n$$\nThis demonstrates that, in this two-dimensional instance, the TR global solution lies on the boundary and the Cauchy point yields an exceptionally poor predicted reduction relative to the optimal boundary step.",
            "answer": "$$\\boxed{1.000 \\times 10^{-3}}$$"
        },
        {
            "introduction": "Modern optimization, particularly in fields like deep learning, demands algorithms that can efficiently navigate non-convex landscapes riddled with saddle points. This hands-on coding challenge demonstrates the superior ability of trust-region methods to handle such terrain . You will implement a method that detects and exploits negative curvature to escape a saddle point, and by comparing its performance against a naive Newton method that gets stuck, you will appreciate the practical power and robustness of the trust-region framework.",
            "id": "3284791",
            "problem": "You are to demonstrate, with a complete and runnable program, how trust region methods equipped with negative curvature detection can escape saddle regions of a synthetic nonconvex function more reliably than a line search Newton method. The educational context is numerical methods and scientific computing at the advanced undergraduate level. Your program must implement both algorithms from first principles, using only well-tested definitions and facts that underlie these methods, and must aggregate results for a small test suite into the specified output format.\n\nThe synthetic objective function is the two-variable scalar function\n$$\nf(x,y) \\;=\\; x^3 \\;-\\; 3\\,x\\,y^2 \\;+\\; 0.1\\,(x^2+y^2)^2.\n$$\nThis function is nonconvex, has saddle behavior near the origin, and is radially bounded by the quartic term. Starting from this explicit definition, derive the gradient and Hessian required to implement second-order optimization schemes. Use the second-order Taylor model definition and the concept of curvature to justify algorithmic decisions.\n\nImplement two algorithms:\n\n- A trust region method that solves at each iteration the subproblem\n$$\n\\min_{\\|s\\|\\le \\Delta}\\; m(s) \\;=\\; f(x) \\;+\\; \\nabla f(x)^\\top s \\;+\\; \\tfrac{1}{2}\\,s^\\top \\nabla^2 f(x)\\,s,\n$$\nwhere $\\Delta$ is the trust region radius. The subproblem must be approximately solved using the Truncated Conjugate Gradient method of Steihaug–Toint, which detects negative curvature through the sign of $d^\\top \\nabla^2 f(x)\\,d$ along a search direction $d$ and, when negative curvature is encountered, moves to the boundary of the trust region along that direction. Steps must be accepted or rejected based on the ratio of actual to predicted reduction. The trust region radius must be adapted based on whether the step is successful and whether the boundary was reached.\n\n- A line search Newton method that at each iteration forms a Newton direction by solving the linear system\n$$\n\\nabla^2 f(x)\\,p \\;=\\; -\\nabla f(x),\n$$\nand then performs a backtracking line search that enforces a sufficient decrease condition. If the Hessian is not positive definite (as determined by its eigenvalues) or the direction fails to be a descent direction, the method must not take a step at that iteration. This implements a standard, naive line search Newton scheme that is known to struggle near saddle points where the Hessian is indefinite or singular.\n\nUse a termination criterion based on the norm of the gradient being below a small tolerance, or a fixed maximum number of iterations, whichever occurs first. To quantify “escape” from the saddle region, define an escape criterion that flags success if, within the iteration limit, the iterate’s Euclidean norm exceeds a prescribed threshold while the objective function value decreases relative to the initial value.\n\nTest Suite and Parameters:\n- Implement both algorithms on the function $f(x,y)$ starting from the following initial points (each pair $(x_0,y_0)$ is a separate test case):\n    - $(10^{-3},10^{-3})$\n    - $(10^{-4},-2\\times 10^{-3})$\n    - $(5\\times 10^{-3},0)$\n    - $(0,5\\times 10^{-3})$\n- For the trust region method, use an initial trust region radius $\\Delta_0 = 10^{-2}$, a maximum radius $\\Delta_{\\max} = 1$, an acceptance threshold $\\eta = 0.1$, and a gradient-norm tolerance of $10^{-8}$.\n- For the line search Newton method, use a sufficient-decrease constant $c_1 = 10^{-4}$, start the line search with step length $1$, halve the step length on each backtracking attempt, and stop backtracking if the step length falls below $10^{-8}$. Do not take a step if the Hessian is not positive definite or if the direction fails to be a descent direction. Use the same gradient-norm tolerance $10^{-8}$.\n- Use a maximum of $100$ iterations for each method.\n- Define the escape criterion as follows: a method is considered to have escaped the saddle region if the final iterate $(x_{\\text{final}},y_{\\text{final}})$ satisfies $\\sqrt{x_{\\text{final}}^2+y_{\\text{final}}^2} \\ge 0.2$ and $f(x_{\\text{final}},y_{\\text{final}})  f(x_0,y_0)$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a two-element list of booleans in the order $[\\text{trust\\_region\\_escaped},\\text{newton\\_escaped}]$. For example, an output for four test cases must look exactly like\n$$\n[[\\text{True},\\text{False}],[\\text{True},\\text{False}],[\\text{True},\\text{False}],[\\text{True},\\text{False}]].\n$$\nNo additional text is allowed in the program’s output.",
            "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded exercise in numerical optimization that asks for the implementation and comparison of two standard second-order methods on a specified nonconvex function. The objective is to demonstrate the superior ability of a trust region method with negative curvature detection to escape a saddle region compared to a naive line search Newton method. All parameters, initial conditions, and evaluation criteria are explicitly and consistently defined.\n\nThe core of the problem lies in the local geometry of the objective function near a critical point and how different algorithms respond to it. The objective function is given by\n$$\nf(x,y) = x^3 - 3xy^2 + 0.1(x^2+y^2)^2.\n$$\nTo implement second-order methods, we must first derive the gradient vector $\\nabla f(x,y)$ and the Hessian matrix $\\nabla^2 f(x,y)$. The gradient is the vector of first partial derivatives:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 3x^2 - 3y^2 + 0.4x(x^2+y^2) \\\\ -6xy + 0.4y(x^2+y^2) \\end{pmatrix}.\n$$\nThe Hessian is the matrix of second partial derivatives:\n$$\n\\nabla^2 f(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 6x + 1.2x^2 + 0.4y^2  -6y + 0.8xy \\\\ -6y + 0.8xy  -6x + 0.4x^2 + 1.2y^2 \\end{pmatrix}.\n$$\nAt the origin, $(x,y) = (0,0)$, we have $\\nabla f(0,0) = \\mathbf{0}$ and $\\nabla^2 f(0,0) = \\mathbf{0}$. This is a degenerate critical point. For a point $(x,y)$ near the origin, the cubic term $x^3 - 3xy^2$ dominates the behavior of $f(x,y)$, while the Hessian is approximately $\\nabla^2 f(x,y) \\approx \\begin{pmatrix} 6x  -6y \\\\ -6y  -6x \\end{pmatrix}$. This matrix has a determinant of $-36x^2 - 36y^2 \\le 0$ and eigenvalues $\\pm 6\\sqrt{x^2+y^2}$, indicating that the Hessian is indefinite in any neighborhood of the origin (excluding the origin itself). This indefiniteness is the hallmark of a saddle region, where the function increases in some directions and decreases in others. An effective optimization algorithm must be able to exploit the directions of negative curvature (associated with negative eigenvalues) to make progress.\n\nThe trust region (TR) method is designed to handle such situations gracefully. At each iteration $k$, given a point $x_k$, it constructs a quadratic model of the function:\n$$\nm_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\frac{1}{2}s^\\top \\nabla^2 f(x_k) s,\n$$\nwhere $s$ is the step to be taken. The method trusts this model only within a certain radius $\\Delta_k  0$, leading to the subproblem:\n$$\n\\min_{\\|s\\| \\le \\Delta_k} m_k(s).\n$$\nThe Steihaug-Toint Truncated Conjugate Gradient (TCG) method is an efficient way to approximately solve this subproblem. It applies the Conjugate Gradient (CG) algorithm to the linear system $\\nabla^2 f(x_k) s = -\\nabla f(x_k)$, but with two crucial modifications. First, if a CG step would result in a point outside the trust region, the step is truncated to end exactly on the boundary $\\|s\\| = \\Delta_k$. Second, and most importantly for this problem, during the CG iterations, if a direction of non-positive curvature $d$ is encountered (i.e., $d^\\top \\nabla^2 f(x_k) d \\le 0$), the algorithm generates a step by moving from the current CG iterate along this direction $d$ to the trust region boundary. This action explicitly uses the negative curvature information to find a better step that further reduces the model $m_k(s)$, allowing the algorithm to effectively escape the saddle region. After finding an approximate solution $s_k$, the ratio of actual reduction in $f$ to the reduction predicted by the model, $\\rho_k$, is computed. Based on this ratio, the step is accepted or rejected, and the trust region radius $\\Delta_k$ is adjusted for the next iteration.\n\nIn contrast, the line search Newton method specified in the problem is naive. At each iteration $k$, it computes the Newton direction $p_k$ by solving the system:\n$$\n\\nabla^2 f(x_k) p_k = -\\nabla f(x_k).\n$$\nThis direction is optimal for the quadratic model only when the Hessian $\\nabla^2 f(x_k)$ is positive definite. The problem specifies that if the Hessian is found to be not positive definite (by checking its eigenvalues), or if the resulting direction $p_k$ is not a descent direction (i.e., $\\nabla f(x_k)^\\top p_k \\ge 0$), the method must not take a step. Near a saddle point, the Hessian is indefinite, so this condition will be frequently met. The method will therefore stagnate, failing to update its position for many iterations, and will be unable to escape the saddle region. If a valid descent direction is found, a backtracking line search is performed to find a step size $\\alpha_k$ that satisfies the Armijo sufficient decrease condition: $f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$ for a constant $c_1 \\in (0,1)$.\n\nThe program will implement both algorithms and apply them to initial points close to the origin. Termination occurs if the gradient norm falls below a tolerance of $10^{-8}$ or a maximum of $100$ iterations is reached. Escape is quantified by checking if the final iterate $x_{\\text{final}}$ satisfies $\\|x_{\\text{final}}\\|_2 \\ge 0.2$ and $f(x_{\\text{final}})  f(x_0)$. The trust region method is expected to successfully escape in all cases by exploiting negative curvature, while the naive Newton method is expected to fail.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison and print results.\n    \"\"\"\n\n    # --- Objective Function, Gradient, and Hessian ---\n\n    def objective_function(p):\n        \"\"\"The synthetic nonconvex objective function f(x,y).\"\"\"\n        x, y = p\n        return x**3 - 3*x*y**2 + 0.1*(x**2 + y**2)**2\n\n    def gradient(p):\n        \"\"\"The gradient of f(x,y).\"\"\"\n        x, y = p\n        r_sq = x**2 + y**2\n        gx = 3*x**2 - 3*y**2 + 0.4*x*r_sq\n        gy = -6*x*y + 0.4*y*r_sq\n        return np.array([gx, gy])\n\n    def hessian(p):\n        \"\"\"The Hessian of f(x,y).\"\"\"\n        x, y = p\n        h_xx = 6*x + 1.2*x**2 + 0.4*y**2\n        h_yy = -6*x + 0.4*x**2 + 1.2*y**2\n        h_xy = -6*y + 0.8*x*y\n        return np.array([[h_xx, h_xy], [h_xy, h_yy]])\n\n    # --- Trust Region Method with Steihaug-Toint TCG ---\n\n    def truncated_cg(grad_f, hess_f, delta, tol=1e-10):\n        \"\"\"\n        Solves the trust-region subproblem using Steihaug-Toint TCG method.\n        Approximately solves min m(s) = g's + 1/2 s'Hs s.t. ||s|| = delta.\n        This corresponds to solving Hs = -g.\n        \"\"\"\n        s = np.zeros_like(grad_f)\n        r = -grad_f\n        p = r.copy()\n        \n        if np.linalg.norm(r)  tol:\n            return s\n        \n        # Max CG iterations can be capped at the dimension of the problem.\n        for _ in range(len(grad_f)):\n            pBp = p.T @ hess_f @ p\n\n            if pBp = 0:\n                # Negative curvature detected. Move to boundary.\n                a = p.T @ p\n                b = 2 * (s.T @ p)\n                c = s.T @ s - delta**2\n                discriminant = b**2 - 4*a*c\n                # Since s is in the trust region, a TC intersection is guaranteed\n                tau = (-b + np.sqrt(discriminant)) / (2*a)\n                return s + tau * p\n\n            alpha = (r.T @ r) / pBp\n            s_new = s + alpha * p\n\n            if np.linalg.norm(s_new) = delta:\n                # Step goes outside boundary. Intersect with boundary.\n                a = p.T @ p\n                b = 2 * (s.T @ p)\n                c = s.T @ s - delta**2\n                discriminant = b**2 - 4*a*c\n                tau = (-b + np.sqrt(discriminant)) / (2*a)\n                return s + tau * p\n            \n            s = s_new\n            r_new = r - alpha * (hess_f @ p)\n\n            if np.linalg.norm(r_new)  tol:\n                break\n\n            beta = (r_new.T @ r_new) / (r.T @ r)\n            p = r_new + beta * p\n            r = r_new\n        return s\n\n    def trust_region_method(start_point, params):\n        \"\"\"Implements the trust region optimization algorithm.\"\"\"\n        x = np.array(start_point, dtype=float)\n        f_initial = objective_function(x)\n        delta = params['delta_0']\n\n        for _ in range(params['max_iter']):\n            grad = gradient(x)\n            if np.linalg.norm(grad)  params['grad_tol']:\n                break\n            \n            hess = hessian(x)\n            \n            s = truncated_cg(grad, hess, delta)\n            \n            ared = objective_function(x) - objective_function(x + s)\n            pred = -(grad.T @ s + 0.5 * s.T @ hess @ s)\n\n            # Avoid division by zero if pred is very small\n            if pred  1e-12:\n                rho = 0\n            else:\n                rho = ared / pred\n\n            if rho  0.25:\n                delta *= 0.25\n            else:\n                s_norm = np.linalg.norm(s)\n                if rho  0.75 and np.isclose(s_norm, delta):\n                    delta = min(2 * delta, params['delta_max'])\n            \n            if rho  params['eta']:\n                x = x + s\n        \n        # Check escape criterion\n        escaped = (np.linalg.norm(x) = 0.2) and (objective_function(x)  f_initial)\n        return escaped\n\n    # --- Line Search Newton Method ---\n\n    def line_search_newton(start_point, params):\n        \"\"\"Implements the naive line search Newton method.\"\"\"\n        x = np.array(start_point, dtype=float)\n        f_initial = objective_function(x)\n\n        for _ in range(params['max_iter']):\n            grad = gradient(x)\n            if np.linalg.norm(grad)  params['grad_tol']:\n                break\n\n            hess = hessian(x)\n            \n            # Check if Hessian is positive definite\n            try:\n                eigvals = np.linalg.eigvalsh(hess)\n                if np.min(eigvals) = 0:\n                    continue  # Do not take a step\n            except np.linalg.LinAlgError:\n                continue # Do not take a step\n\n            # Solve for Newton direction p\n            try:\n                p = np.linalg.solve(hess, -grad)\n            except np.linalg.LinAlgError:\n                continue # Do not take a step\n            \n            # Check for descent direction\n            if grad.T @ p = 0:\n                continue # Do not take a step\n\n            # Backtracking line search\n            alpha = 1.0\n            f_x = objective_function(x)\n            descent_term = params['c1'] * grad.T @ p\n            \n            while True:\n                if objective_function(x + alpha * p) = f_x + alpha * descent_term:\n                    break\n                alpha /= 2.0\n                if alpha  1e-8:\n                    alpha = 0.0 # Failed to find a step\n                    break\n            \n            if alpha  0.0:\n                x = x + alpha * p\n        \n        # Check escape criterion\n        escaped = (np.linalg.norm(x) = 0.2) and (objective_function(x)  f_initial)\n        return escaped\n\n    # --- Test Suite and Main Execution ---\n\n    test_cases = [\n        (1e-3, 1e-3),\n        (1e-4, -2e-3),\n        (5e-3, 0),\n        (0, 5e-3)\n    ]\n    \n    tr_params = {\n        'delta_0': 1e-2,\n        'delta_max': 1.0,\n        'eta': 0.1,\n        'grad_tol': 1e-8,\n        'max_iter': 100\n    }\n\n    newton_params = {\n        'c1': 1e-4,\n        'grad_tol': 1e-8,\n        'max_iter': 100\n    }\n\n    results = []\n    for start_point in test_cases:\n        tr_escaped = trust_region_method(start_point, tr_params)\n        newton_escaped = line_search_newton(start_point, newton_params)\n        results.append([tr_escaped, newton_escaped])\n    \n    # Format the output string exactly as specified\n    outer_parts = []\n    for res_pair in results:\n        part = f\"[{str(res_pair[0])},{str(res_pair[1])}]\"\n        outer_parts.append(part)\n    final_string = f\"[{','.join(outer_parts)}]\"\n    \n    print(final_string)\n\nsolve()\n```"
        }
    ]
}