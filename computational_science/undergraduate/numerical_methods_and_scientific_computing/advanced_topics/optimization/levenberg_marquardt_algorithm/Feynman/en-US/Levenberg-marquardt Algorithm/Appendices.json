{
    "hands_on_practices": [
        {
            "introduction": "The Levenberg-Marquardt algorithm navigates the complex landscape of a nonlinear problem by creating a sequence of local, linear approximations. The cornerstone of this approximation is the Jacobian matrix, which encodes the sensitivity of your model's residuals to small changes in its parameters. This first exercise  provides essential practice in constructing this matrix, a fundamental skill for implementing and debugging any gradient-based optimization method.",
            "id": "2217052",
            "problem": "In the field of non-linear optimization, algorithms like the Levenberg-Marquardt algorithm are used to fit a model function to a set of data points by minimizing the sum of the squares of the residuals. A key component in this process is the Jacobian matrix of the model function.\n\nConsider a model used to describe the concentration of a substance over time, given by the two-parameter rational function:\n$$f(t; a, b) = \\frac{a}{1 + bt}$$\nwhere $t$ is the independent variable (e.g., time), and $\\mathbf{p} = [a, b]^T$ is the vector of parameters to be determined.\n\nSuppose we have collected the following three data points $(t_i, y_i)$:\n- $(t_1, y_1) = (1.0, 2.5)$\n- $(t_2, y_2) = (2.0, 1.8)$\n- $(t_3, y_3) = (4.0, 1.1)$\n\nThe Jacobian matrix $\\mathbf{J}$ for this fitting problem is an $m \\times n$ matrix, where $m$ is the number of data points and $n$ is the number of parameters. Its elements are defined by $J_{ij} = \\frac{\\partial f(t_i; \\mathbf{p})}{\\partial p_j}$.\n\nCalculate the numerical value of the Jacobian matrix $\\mathbf{J}$ evaluated at the initial parameter guess $\\mathbf{p}_0 = [a_0, b_0]^T = [3.0, 0.5]^T$. All given numerical values are dimensionless.\n\nExpress your final answer as a $3 \\times 2$ matrix, with each element rounded to three significant figures.",
            "solution": "We are given the model function $f(t; a, b) = \\dfrac{a}{1 + bt}$ and the Jacobian matrix defined by $J_{ij} = \\dfrac{\\partial f(t_{i}; \\mathbf{p})}{\\partial p_{j}}$ for the parameter vector $\\mathbf{p} = [a, b]^{T}$. Thus, each row of $\\mathbf{J}$ corresponds to a data point $t_{i}$, and the two columns correspond to the derivatives with respect to $a$ and $b$.\n\nFirst, compute the partial derivatives symbolically. Write $f(t; a, b) = a(1 + bt)^{-1}$. Then, using the power rule and chain rule:\n$$\n\\frac{\\partial f}{\\partial a} = (1 + bt)^{-1} = \\frac{1}{1 + bt},\n$$\n$$\n\\frac{\\partial f}{\\partial b} = a \\cdot \\frac{\\partial}{\\partial b}(1 + bt)^{-1} = a \\cdot \\left[-(1 + bt)^{-2} \\cdot t\\right] = -\\frac{a t}{(1 + bt)^{2}}.\n$$\n\nTherefore, for each data point $t_{i}$, the Jacobian row is\n$$\n\\left[\\frac{1}{1 + b t_{i}},\\ -\\frac{a t_{i}}{(1 + b t_{i})^{2}}\\right].\n$$\n\nEvaluate at the initial guess $\\mathbf{p}_{0} = [a_{0}, b_{0}]^{T} = [3.0, 0.5]^{T}$ and the given $t$ values.\n\nFor $t_{1} = 1$:\n$$\n1 + b_{0} t_{1} = 1 + 0.5 \\cdot 1 = \\frac{3}{2},\\quad \\frac{\\partial f}{\\partial a} = \\frac{2}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 1}{(3/2)^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\nFor $t_{2} = 2$:\n$$\n1 + b_{0} t_{2} = 1 + 0.5 \\cdot 2 = 2,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{2},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 2}{2^{2}} = -\\frac{6}{4} = -\\frac{3}{2}.\n$$\n\nFor $t_{3} = 4$:\n$$\n1 + b_{0} t_{3} = 1 + 0.5 \\cdot 4 = 3,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 4}{3^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\nAssemble the Jacobian and round each element to three significant figures:\n$$\n\\mathbf{J}(\\mathbf{p}_{0}) \\approx\n\\begin{pmatrix}\n0.667 & -1.33 \\\\\n0.500 & -1.50 \\\\\n0.333 & -1.33\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0.667  -1.33 \\\\ 0.500  -1.50 \\\\ 0.333  -1.33\\end{pmatrix}}$$"
        },
        {
            "introduction": "The genius of the Levenberg-Marquardt algorithm lies in its ability to act as a hybrid, smoothly transitioning between the cautious gradient descent method and the ambitious Gauss-Newton (GN) method. This practice  uses a carefully designed one-dimensional problem to demonstrate the core reason for this hybrid nature: to correct the instability of the pure GN method. You will see firsthand how the damping parameter $\\lambda$ acts as a crucial stabilizer, preventing the wildly divergent steps that can occur when the problem is ill-conditioned.",
            "id": "3232802",
            "problem": "You are to write a complete, runnable program that analyzes the Gauss-Newton method and its Levenberg-Marquardt (LM) stabilization for a one-parameter nonlinear least-squares problem. The goal is to demonstrate, through a principled derivation and a concrete implementation, that undamped Gauss-Newton steps can oscillate or diverge when the Jacobian is nearly singular, and that adding a Levenberg-Marquardt damping term stabilizes the iterations.\n\nBegin from the following fundamental base:\n- The nonlinear least-squares objective is $F(x) = \\tfrac{1}{2}\\lVert r(x)\\rVert_2^2$, where $r(x)$ is a vector of residual functions.\n- The first-order Taylor linearization at $x$ is $r(x + \\Delta) \\approx r(x) + J(x)\\Delta$, where $J(x)$ is the Jacobian of $r(x)$.\n- Minimizing the linearized model yields the normal equations $J(x)^\\top J(x)\\Delta = -J(x)^\\top r(x)$.\n- The Levenberg-Marquardt (LM) method augments the normal matrix with a damping term $\\lambda I$ (where $I$ is the identity), solving $\\big(J(x)^\\top J(x) + \\lambda I\\big)\\Delta = -J(x)^\\top r(x)$ with $\\lambda  0$.\n\nYour program must instantiate a one-dimensional residual model $r(x) = \\sin(x)$, with angles in radians, and perform both Gauss-Newton (undamped, corresponding to $\\lambda = 0$) and Levenberg-Marquardt (damped, with a specified $\\lambda  0$) iterations. The residual model implies the least-squares objective $F(x) = \\tfrac{1}{2}\\sin^2(x)$. The Jacobian $J(x)$ is a scalar in this case. You must:\n- Derive from the base definitions the Gauss-Newton and LM update rules specialized to the one-dimensional residual $r(x) = \\sin(x)$.\n- Implement fixed-iteration solvers for both methods using the derived updates.\n- Demonstrate that near points where $J(x)$ is nearly singular (specifically near $x \\approx \\tfrac{\\pi}{2} + k\\pi$, where $k$ is an integer), the undamped Gauss-Newton iteration exhibits oscillation or divergence (large alternating steps or steps that exit a bounded domain), while the LM method with a positive damping parameter stabilizes the iterations by bounding the step size.\n\nAngles must be in radians. No physical units are involved. Your algorithm must not use any line search or trust-region adaptations beyond the constant LM damping.\n\nTest Suite and Output Specification:\n- Use the following test cases, each specified as a tuple $(x_0, \\lambda, N, L)$:\n  1. A near-singular Jacobian case to provoke instability: $x_0 = \\tfrac{\\pi}{2} - 10^{-6}$, $\\lambda = 1.0$, $N = 10$, $L = 50$.\n  2. An oscillatory-but-recoverable case: $x_0 = 1.4$, $\\lambda = 0.5$, $N = 10$, $L = 50$.\n  3. A happy-path case away from singularities: $x_0 = 2.0$, $\\lambda = 0.1$, $N = 10$, $L = 50$.\n- For each case, run $N$ iterations of undamped Gauss-Newton (i.e., set $\\lambda = 0$ for Gauss-Newton) and $N$ iterations of LM with the given $\\lambda$. For each method in each case, compute:\n  - The final residual norm $|r(x_N)|$ as a float.\n  - A stability flag as a boolean that is true if all iterates $x_k$ satisfy $|x_k| \\le L$, and false otherwise.\n  - The maximum step magnitude $\\max_k |\\Delta_k|$ as a float, where $\\Delta_k$ is the update at iteration $k$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be structured as a list of lists, one inner list per test case, each inner list containing six entries in the following order: $[\\text{gn\\_final\\_residual}, \\text{lm\\_final\\_residual}, \\text{gn\\_stable}, \\text{lm\\_stable}, \\text{gn\\_max\\_step}, \\text{lm\\_max\\_step}]$. For example, the output format must be like $[[r_{11}, r_{12}, b_{11}, b_{12}, s_{11}, s_{12}], [r_{21}, r_{22}, b_{21}, b_{22}, s_{21}, s_{22}], [r_{31}, r_{32}, b_{31}, b_{32}, s_{31}, s_{32}]]$ where $r_{ij}$ are floats and $b_{ij}$ are booleans.\n\nScientific realism and coverage:\n- The near-singular case enforces a boundary condition where the undamped method encounters a Jacobian close to zero, which can cause large steps. The LM damping term $\\lambda I$ must mitigate this instability.\n- The oscillatory case exhibits large alternating steps for undamped Gauss-Newton that are reduced under LM damping.\n- The happy-path case demonstrates convergence for both methods.\n- Angles are in radians.\n- All outputs must be basic types as specified (booleans and floats).",
            "solution": "The problem requires a validation and implementation of the Gauss-Newton (GN) method and its Levenberg-Marquardt (LM) stabilization for a specific one-dimensional nonlinear least-squares problem. The objective is to demonstrate how LM stabilization corrects the instabilities inherent in the undamped Gauss-Newton method when the Jacobian matrix is nearly singular.\n\nFirst, we establish the theoretical foundation. The general nonlinear least-squares problem aims to minimize an objective function $F(x)$ defined as half the squared Euclidean norm of a residual vector $r(x)$:\n$$\nF(x) = \\frac{1}{2}\\lVert r(x)\\rVert_2^2\n$$\nThe Gauss-Newton method approximates the objective function at each iteration by linearizing the residual function $r(x)$ around the current iterate $x_k$. The update step $\\Delta$ is found by solving the linearized least-squares problem:\n$$\n\\min_{\\Delta} \\frac{1}{2}\\lVert r(x_k) + J(x_k)\\Delta\\rVert_2^2\n$$\nwhere $J(x_k)$ is the Jacobian of $r(x)$ at $x_k$. The solution to this linear problem is given by the normal equations:\n$$\nJ(x_k)^\\top J(x_k)\\Delta_{GN} = -J(x_k)^\\top r(x_k)\n$$\nThe Levenberg-Marquardt method introduces a damping parameter $\\lambda  0$ to regularize the problem, which is particularly useful when the matrix $J(x_k)^\\top J(x_k)$ is singular or ill-conditioned. The LM update step $\\Delta_{LM}$ is found by solving the modified normal equations:\n$$\n\\left(J(x_k)^\\top J(x_k) + \\lambda I\\right)\\Delta_{LM} = -J(x_k)^\\top r(x_k)\n$$\nwhere $I$ is the identity matrix.\n\nNow, we specialize these general formulas to the given one-dimensional problem, where the residual is a scalar function $r(x) = \\sin(x)$.\nThe objective function becomes:\n$$\nF(x) = \\frac{1}{2}(r(x))^2 = \\frac{1}{2}\\sin^2(x)\n$$\nThe minima of $F(x)$ occur where $\\sin(x) = 0$, i.e., at $x = n\\pi$ for any integer $n$.\nIn this one-dimensional case, the Jacobian $J(x)$ is a $1 \\times 1$ matrix (a scalar) corresponding to the first derivative of $r(x)$:\n$$\nJ(x) = \\frac{dr}{dx} = \\frac{d}{dx}(\\sin(x)) = \\cos(x)\n$$\nThe Jacobian is (nearly) singular when $J(x) = \\cos(x) \\approx 0$, which occurs for $x \\approx \\frac{\\pi}{2} + k\\pi$ for any integer $k$.\n\nWe derive the specific update rules for both methods.\nFor Gauss-Newton (undamped, equivalent to $\\lambda=0$), the normal equation is:\n$$\n(\\cos(x_k))^\\top (\\cos(x_k))\\Delta_{GN} = -(\\cos(x_k))^\\top (\\sin(x_k))\n$$\n$$\n\\cos^2(x_k) \\Delta_{GN} = -\\sin(x_k)\\cos(x_k)\n$$\nAssuming $\\cos(x_k) \\neq 0$, we can solve for the step $\\Delta_{GN}$:\n$$\n\\Delta_{GN} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k)} = -\\frac{\\sin(x_k)}{\\cos(x_k)} = -\\tan(x_k)\n$$\nThe Gauss-Newton update rule is therefore:\n$$\nx_{k+1} = x_k + \\Delta_{GN} = x_k - \\tan(x_k)\n$$\nThe instability of this method becomes evident when $x_k$ approaches a point where the Jacobian is singular, i.e., $x_k \\to \\frac{\\pi}{2} + k\\pi$. At these points, $\\cos(x_k) \\to 0$, causing $|\\tan(x_k)| \\to \\infty$. The update step $\\Delta_{GN}$ becomes arbitrarily large, leading to divergence or violent oscillations.\n\nFor Levenberg-Marquardt, the modified normal equation includes the damping term $\\lambda  0$. In this 1D case, the identity matrix $I$ is the scalar $1$:\n$$\n(\\cos^2(x_k) + \\lambda)\\Delta_{LM} = -\\sin(x_k)\\cos(x_k)\n$$\nSince $\\cos^2(x_k) \\ge 0$ and $\\lambda  0$, the term $(\\cos^2(x_k) + \\lambda)$ is always positive and bounded away from zero by $\\lambda$. Thus, the system is always well-conditioned. The LM update step is:\n$$\n\\Delta_{LM} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\nThe Levenberg-Marquardt update rule is:\n$$\nx_{k+1} = x_k + \\Delta_{LM} = x_k - \\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\nNow consider the behavior near a singularity, where $\\cos(x_k) \\to 0$. In this limit, the numerator $\\sin(x_k)\\cos(x_k)$ also approaches $0$. The denominator approaches $\\lambda$. Therefore, the step $\\Delta_{LM}$ approaches $0$. The damping term effectively bounds the step size, preventing the catastrophic behavior observed in the undamped Gauss-Newton method. This ensures stability and promotes convergence even when starting near a region of Jacobian singularity.\n\nThe implementation will proceed by iterating these two update rules for the given test cases to numerically demonstrate this derived behavior. The results will quantify the final residual, the stability of the iteration sequence within a given bound, and the magnitude of the steps taken.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes Gauss-Newton and Levenberg-Marquardt methods for a 1D\n    nonlinear least-squares problem, demonstrating LM stabilization.\n    \"\"\"\n\n    # Test cases as tuples of (x0, lambda, N, L)\n    test_cases = [\n        (np.pi / 2.0 - 1e-6, 1.0, 10, 50.0), # Near-singular Jacobian case\n        (1.4, 0.5, 10, 50.0),               # Oscillatory-but-recoverable case\n        (2.0, 0.1, 10, 50.0)                # Happy-path case\n    ]\n\n    results = []\n\n    def run_iterations(x0, N, L, damp_lambda):\n        \"\"\"\n        Performs N iterations of a solver for the objective F(x) = 0.5*sin^2(x).\n\n        Args:\n            x0 (float): Initial guess.\n            N (int): Number of iterations.\n            L (float): Stability bound for |x_k|.\n            damp_lambda (float): Damping parameter. If 0, use Gauss-Newton.\n                                 If  0, use Levenberg-Marquardt.\n\n        Returns:\n            A tuple containing:\n            - final_residual (float): |r(x_N)|.\n            - is_stable (bool): True if all |x_k| = L.\n            - max_step (float): Maximum magnitude of any step delta_k.\n        \"\"\"\n        x = x0\n        stable = True\n        max_step_mag = 0.0\n\n        for _ in range(N):\n            # The Jacobian is singular when cos(x) is zero.\n            # Avoid division by zero for the pure Gauss-Newton case if x is exactly pi/2 + k*pi.\n            # np.tan will handle large values gracefully, returning inf.\n            cos_x = np.cos(x)\n            sin_x = np.sin(x)\n\n            if damp_lambda == 0:  # Gauss-Newton\n                # Delta = -tan(x)\n                # Avoid explicit division by zero if cos_x is extremely small.\n                # np.tan handles this by returning large numbers or inf.\n                delta = -np.tan(x)\n\n            else:  # Levenberg-Marquardt\n                # Delta = -sin(x)cos(x) / (cos^2(x) + lambda)\n                numerator = -sin_x * cos_x\n                denominator = cos_x**2 + damp_lambda\n                delta = numerator / denominator\n\n            if np.isinf(delta) or np.isnan(delta):\n                # If step is infinite/NaN, it's definitively unstable and huge.\n                # To assign a finite but large value for max_step.\n                # Using 2*L is arbitrary but indicates a large step.\n                current_step_mag = 2 * L \n                x = x + (2 * L * np.sign(-delta) if not np.isnan(delta) else 0)\n            else:\n                current_step_mag = abs(delta)\n\n            if current_step_mag  max_step_mag:\n                max_step_mag = current_step_mag\n\n            x = x + delta\n            \n            if abs(x)  L:\n                stable = False\n        \n        final_residual = abs(np.sin(x))\n        \n        return final_residual, stable, max_step_mag\n\n    for x0, lm_lambda, N, L in test_cases:\n        # Run Gauss-Newton (undamped, lambda = 0)\n        gn_final_residual, gn_stable, gn_max_step = run_iterations(x0, N, L, 0)\n\n        # Run Levenberg-Marquardt (damped)\n        lm_final_residual, lm_stable, lm_max_step = run_iterations(x0, N, L, lm_lambda)\n\n        case_results = [\n            gn_final_residual,\n            lm_final_residual,\n            gn_stable,\n            lm_stable,\n            gn_max_step,\n            lm_max_step,\n        ]\n        results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    # The default str() representation for lists includes spaces, which is acceptable.\n    # The boolean values will be represented as 'True' and 'False'.\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Having grasped the foundational concepts of the Jacobian and the stabilizing role of damping, it is time to assemble a complete, robust Levenberg-Marquardt solver. This advanced practice  guides you through the implementation of the full algorithm, focusing on the critical mechanism for automatically adjusting the damping parameter $\\lambda$. By comparing the actual versus predicted reduction in error at each step, you will build a solver that is not only stable but also efficient, intelligently navigating from a poor initial guess to an optimal solution.",
            "id": "3256843",
            "problem": "You are tasked with implementing a complete, self-contained program that solves a nonlinear least squares fitting problem using the Levenberg-Marquardt algorithm, where the damping parameter is automatically adjusted based on the ratio of actual to predicted decrease in the sum of squares. The derivation and implementation must begin from the fundamental definition of the nonlinear least squares objective and proceed to an algorithm that is scientifically and numerically sound.\n\nBegin from the core definition of the nonlinear least squares objective. Let the parameter vector be $p \\in \\mathbb{R}^m$, the independent variable vector be $x \\in \\mathbb{R}^n$, the observed data (dependent variable) be $y \\in \\mathbb{R}^n$, and the model be a function $f:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}^n$ that predicts the data from parameters. Define the residual vector $r(p) \\in \\mathbb{R}^n$ by $r(p) = y - f(x,p)$ and the objective function by $F(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2$. Define the Jacobian matrix $J(p) \\in \\mathbb{R}^{n \\times m}$ with entries $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$. Your program must use central finite differences to approximate the Jacobian numerically at every iteration.\n\nAt each iteration, construct a local quadratic model of $F(p)$ by a second-order approximation around a current parameter $p$ using $r(p)$ and $J(p)$, and compute a parameter increment that balances between the Gauss-Newton direction and a gradient-descent direction by solving a damped normal equation. The algorithm must decide whether to accept or reject the proposed step using a gain ratio that compares the actual decrease in $F(p)$ to the predicted decrease in the local model used to compute the step. The damping parameter must be adjusted automatically based on this gain ratio: reduce the damping when the step is productive and increase it when the step is unproductive. Use a termination criterion that stops the iterations when either the infinity norm of the gradient is small, the parameter update is small, or the change in the objective function is negligible.\n\nThe angle quantity $\\phi$ used in the sinusoidal model must be treated and computed in radians.\n\nYour program must implement the above for the following three test cases, each with a specific model, dataset, and initial parameter guess. For each test case, return the fitted parameter vector as a list of floats rounded to six decimal places.\n\nTest Case 1 (general convergence):\n- Model: $f(x,p) = p_1 \\exp(p_2 x) + p_3$, with parameters $p = [p_1,p_2,p_3]$.\n- Data: $x$ defined by $x_i = 0.0 + i\\Delta$ for $i=0,1,\\dots,30$ and $\\Delta = 0.1$, so $x \\in [0,3.0]$ with $31$ points. Observations defined by $y = 2.0\\exp(-0.7 x) + 0.5 + 0.01\\sin(5x)$.\n- Initial parameter: $p^{(0)} = [1.0,-0.2,0.0]$.\n\nTest Case 2 (steep nonlinearity):\n- Model: logistic curve $f(x,p) = \\frac{p_1}{1 + \\exp(-p_2(x - p_3))}$, with parameters $p = [p_1,p_2,p_3]$.\n- Data: $x$ defined by $x_i = -1.5 + i\\Delta$ for $i=0,1,\\dots,14$ and $\\Delta = \\frac{2.0 - (-1.5)}{14}$, so $x \\in [-1.5,2.0]$ with $15$ points. Observations defined by $y = \\frac{1.5}{1 + \\exp(-3.0(x - 0.5))} + 0.02\\cos(3x)$.\n- Initial parameter: $p^{(0)} = [1.0,1.0,0.0]$.\n\nTest Case 3 (near-degeneracy in sensitivity):\n- Model: sinusoid with fixed angular frequency $\\omega = 2.5$ radians per unit of $x$, $f(x,p) = p_1 \\cos(\\omega x + p_2) + p_3$, with parameters $p = [p_1,p_2,p_3]$ and $\\phi = p_2$ in radians.\n- Data: $x$ defined by $x_i = 0.0 + i\\Delta$ for $i=0,1,\\dots,20$ and $\\Delta = 0.1$, so $x \\in [0,2.0]$ with $21$ points. Observations defined by $y = 0.8\\cos(2.5 x + 0.4) - 0.05 + 0.01\\sin(7x)$.\n- Initial parameter: $p^{(0)} = [0.5,0.0,0.0]$.\n\nAlgorithmic requirements:\n- Use central finite differences for the Jacobian with a perturbation step proportional to $\\sqrt{\\varepsilon}(1 + |p_j|)$, where $\\varepsilon$ is machine precision for double-precision floating-point arithmetic.\n- Construct and solve a damped normal equation to obtain a parameter increment at each iteration.\n- Compute the actual decrease in the objective function and the predicted decrease in the local model used in the step computation. Use their ratio to decide acceptance of the step and to adapt the damping parameter.\n- Terminate when any standard smallness criterion is met: the infinity norm of the gradient is small, the parameter change is small, or the change in the objective function is small. Also include a maximum iteration cap to ensure the algorithm halts.\n- The angle $\\phi$ in the sinusoidal model must be computed in radians throughout.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the fitted parameter values rounded to six decimal places. The final output format must be exactly of the form:\n\"[[p11,p12,p13],[p21,p22,p23],[p31,p32,p33]]\"\nwhere $p1k$, $p2k$, and $p3k$ are the fitted parameters for test cases $1$, $2$, and $3$, respectively, expressed as decimal numbers.",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and computationally feasible task from the field of numerical methods. It requires the implementation of the Levenberg-Marquardt algorithm for nonlinear least squares, a standard and important technique in scientific computing. All necessary components, including the objective function, algorithmic procedures, test models, datasets, and initial conditions, are provided and internally consistent.\n\nHerein, a complete solution is derived from first principles.\n\n### 1. The Nonlinear Least Squares Problem\n\nThe core of the problem is to find the set of parameters that best fits a model to a set of observed data. Let the parameter vector be $p \\in \\mathbb{R}^m$, the vector of independent variables be $x \\in \\mathbb{R}^n$, and the corresponding observed data vector be $y \\in \\mathbb{R}^n$. A model function, $f(x, p)$, maps the parameters $p$ and independent variables $x$ to a set of predicted data points. The goal is to find the parameter vector $p$ that minimizes the sum of the squared differences between the observed data $y$ and the predicted data $f(x,p)$.\n\nThis is formally expressed by defining a residual vector $r(p) \\in \\mathbb{R}^n$ as:\n$$\nr(p) = y - f(x, p)\n$$\nThe objective is to minimize the scalar objective function $F(p)$, defined as one-half of the squared Euclidean norm of the residual vector:\n$$\nF(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2 = \\frac{1}{2} \\sum_{i=1}^{n} r_i(p)^2\n$$\nSince the model $f(x,p)$ is generally a nonlinear function of the parameters $p$, this is a nonlinear least squares problem, which must be solved using iterative methods.\n\n### 2. Iterative Solution via Local Approximation\n\nThe Levenberg-Marquardt algorithm is an iterative procedure that finds a local minimum of $F(p)$. Starting from an initial guess $p^{(0)}$, the algorithm generates a sequence of parameter vectors $p^{(1)}, p^{(2)}, \\dots$ that progressively reduce the value of $F(p)$. Each iteration computes a step vector $h$ that updates the current parameter vector: $p_{k+1} = p_k + h$.\n\nThe step $h$ is found by forming a local quadratic model of the objective function $F(p)$ around the current point $p_k$. This model is constructed by first linearizing the residual vector $r(p_k+h)$ using a first-order Taylor series expansion:\n$$\nr(p_k + h) \\approx r(p_k) + J(p_k)h\n$$\nwhere $J(p_k)$ is the Jacobian matrix of the residual vector $r$ evaluated at $p_k$. The entries of the Jacobian are given by $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$. Since $r(p) = y - f(x,p)$, this is equivalent to $J_{ij}(p) = -\\frac{\\partial f_i(x,p)}{\\partial p_j}$.\n\nSubstituting this linear approximation of the residual into the objective function $F(p_k+h)$ yields a local quadratic model $L(h)$:\n$$\nF(p_k + h) = \\frac{1}{2}\\lVert r(p_k+h) \\rVert_2^2 \\approx \\frac{1}{2}\\lVert r(p_k) + J(p_k)h \\rVert_2^2 = L(h)\n$$\nExpanding the squared norm gives:\n$$\nL(h) = \\frac{1}{2} (r_k + J_k h)^T (r_k + J_k h) = \\frac{1}{2} (r_k^T r_k + 2h^T J_k^T r_k + h^T J_k^T J_k h)\n$$\n$$\nL(h) = F(p_k) + h^T (J_k^T r_k) + \\frac{1}{2} h^T (J_k^T J_k) h\n$$\nwhere $r_k = r(p_k)$ and $J_k = J(p_k)$.\n\n### 3. The Gauss-Newton and Levenberg-Marquardt Steps\n\nThe step $h$ is chosen to minimize this quadratic model $L(h)$. Setting the gradient of $L(h)$ with respect to $h$ to zero gives:\n$$\n\\nabla_h L(h) = J_k^T r_k + (J_k^T J_k) h = 0\n$$\nThis leads to the **Gauss-Newton normal equations**:\n$$\n(J_k^T J_k) h_{gn} = -J_k^T r_k\n$$\nThe step $h_{gn}$ is the Gauss-Newton step. This method works well when the matrix $J_k^T J_k$ is well-conditioned. However, if $J_k^T J_k$ is singular or ill-conditioned, the step can be excessively large and unproductive.\n\nThe **Levenberg-Marquardt algorithm** addresses this issue by introducing a damping parameter $\\lambda \\ge 0$. The step $h_{lm}$ is found by solving a modified, or damped, normal equation:\n$$\n(J_k^T J_k + \\lambda I) h_{lm} = -J_k^T r_k\n$$\nwhere $I$ is the identity matrix. The term $\\lambda I$ is a regularization term.\n- If $\\lambda$ is small, the step $h_{lm}$ approximates the Gauss-Newton step $h_{gn}$.\n- If $\\lambda$ is large, the term $\\lambda I$ dominates $J_k^T J_k$, and the equation approximates $\\lambda I h_{lm} \\approx -J_k^T r_k$, which means $h_{lm} \\approx -\\frac{1}{\\lambda} (J_k^T r_k)$. Since the gradient of the objective function is $\\nabla F(p_k) = J_k^T r_k$, the step becomes a small step in the direction of steepest descent.\n\nThus, the parameter $\\lambda$ adaptively blends the fast-converging Gauss-Newton method with the robust but slower gradient descent method.\n\n### 4. Algorithm Implementation Details\n\n#### Numerical Jacobian Approximation\nThe Jacobian matrix $J_k$ is computed at each iteration using **central finite differences**. For each parameter $p_j$ (the $j$-th component of the vector $p$), a small perturbation $\\delta_j$ is calculated:\n$$\n\\delta_j = \\sqrt{\\varepsilon} (1 + |p_j|)\n$$\nwhere $\\varepsilon$ is the machine precision for double-precision floating-point numbers. The $j$-th column of the Jacobian, which represents the sensitivity of the residuals to the parameter $p_j$, is then approximated as:\n$$\nJ_{\\cdot, j}(p_k) = \\frac{\\partial r(p_k)}{\\partial p_j} \\approx \\frac{r(p_k + \\delta_j e_j) - r(p_k - \\delta_j e_j)}{2\\delta_j} = -\\frac{f(x, p_k + \\delta_j e_j) - f(x, p_k - \\delta_j e_j)}{2\\delta_j}\n$$\nwhere $e_j$ is the $j$-th standard basis vector.\n\n#### Damping Parameter Adaptation\nThe success of a proposed step $h=h_{lm}$ is evaluated using a **gain ratio**, $\\rho$. This ratio compares the actual reduction in the objective function to the reduction predicted by the local quadratic model $L(h)$.\n- **Actual reduction**: $\\Delta F_{actual} = F(p_k) - F(p_k + h)$\n- **Predicted reduction**: $\\Delta F_{pred} = L(0) - L(h) = -h^T J_k^T r_k - \\frac{1}{2}h^T (J_k^T J_k) h$\n\nThe gain ratio is $\\rho = \\frac{\\Delta F_{actual}}{\\Delta F_{pred}}$. The adaptation strategy is as follows:\n1.  If $\\rho$ is positive and significant (e.g., $\\rho  10^{-4}$), the step is \"productive\". It is accepted ($p_{k+1} = p_k + h$), and the damping $\\lambda$ is decreased to move closer to the faster Gauss-Newton method for the next iteration. A common update rule is $\\lambda \\leftarrow \\lambda \\cdot \\max(1/3, 1-(2\\rho-1)^3)$.\n2.  If $\\rho$ is small or negative, the step is \"unproductive\". It is rejected ($p_{k+1} = p_k$), and the damping $\\lambda$ is increased to move towards the more robust gradient descent direction. The algorithm then re-computes the step $h$ with the larger $\\lambda$ at the same iteration $k$. A common rule is $\\lambda \\leftarrow \\lambda \\cdot v$, where $v$ is a multiplicative factor, typically starting at $v=2$ and increasing on successive rejections.\n\nAn initial value for $\\lambda$ is chosen based on the scale of the problem, for instance, $\\lambda_0 = \\tau \\cdot \\max_{ii}((J_0^T J_0)_{ii})$ with a small constant $\\tau$ (e.g., $10^{-4}$).\n\n#### Termination Criteria\nThe iterative process is terminated when one of the following conditions is met, indicating that a satisfactory solution has been found:\n1.  **Small Gradient**: The magnitude of the gradient of the objective function is close to zero: $\\lVert J_k^T r_k \\rVert_{\\infty}  \\epsilon_g$.\n2.  **Small Parameter Update**: The relative change in the parameter vector is negligible: $\\lVert p_{k+1} - p_k \\rVert_2  \\epsilon_p (\\lVert p_k \\rVert_2 + \\epsilon_p)$.\n3.  **Small Objective Function Change**: The relative change in the objective function value is negligible: $|F(p_{k+1}) - F(p_k)|  \\epsilon_f (F(p_k) + \\epsilon_f)$.\n4.  **Maximum Iterations**: A predefined maximum number of iterations, $k_{max}$, is reached to prevent an infinite loop.\n\nHere, $\\epsilon_g, \\epsilon_p, \\epsilon_f$ are small tolerance values (e.g., $10^{-8}$).\n\n### 5. Summary of the Levenberg-Marquardt Algorithm\nThe complete algorithm is as follows:\n\n1.  **Initialization**:\n    - Choose an initial parameter guess $p_0$.\n    - Set tolerances $\\epsilon_g, \\epsilon_p, \\epsilon_f$ and maximum iterations $k_{max}$.\n    - Compute initial residual $r_0 = y - f(x, p_0)$ and objective $F_0 = \\frac{1}{2}r_0^T r_0$.\n    - Compute initial Jacobian $J_0$ and gradient $g_0 = J_0^T r_0$.\n    - Initialize damping parameter $\\lambda_0$ and increase factor $v$.\n    - Set iteration counter $k = 0$.\n\n2.  **Main Loop**: While termination criteria are not met:\n    a. Solve the damped normal equations $(J_k^T J_k + \\lambda_k I) h = -g_k$ for the step $h$.\n    b. Evaluate the proposed new parameter vector $p_{new} = p_k + h$.\n    c. Compute the gain ratio $\\rho = \\frac{F(p_k) - F(p_{new})}{\\Delta F_{pred}}$.\n    d. **If $\\rho  \\epsilon_{\\rho}$**:\n        i. Accept the step: $p_{k+1} = p_{new}$.\n        ii. Update objective $F_{k+1}$, residual $r_{k+1}$, Jacobian $J_{k+1}$, and gradient $g_{k+1}$.\n        iii. Decrease damping: $\\lambda_{k+1} = \\lambda_k \\cdot \\max(1/3, 1-(2\\rho-1)^3)$ and reset $v=2$.\n        iv. Check termination criteria on step size and objective change.\n        v. Increment $k \\leftarrow k+1$.\n    e. **Else ($\\rho \\le \\epsilon_{\\rho}$)**:\n        i. Reject the step. Parameters are not updated.\n        ii. Increase damping: $\\lambda_k \\leftarrow \\lambda_k \\cdot v$, and then $v \\leftarrow 2v$.\n        iii. Repeat from step 2a with the new $\\lambda_k$.\n\n3.  **Termination**: Return the final parameter vector $p_k$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef levenberg_marquardt(model_func, p_init, x_data, y_data, max_iter=100, tol_g=1e-8, tol_p=1e-8, tol_f=1e-8):\n    \"\"\"\n    Implements the Levenberg-Marquardt algorithm for nonlinear least squares.\n\n    Args:\n        model_func: The model function f(x, p).\n        p_init: Initial guess for the parameters.\n        x_data: Independent variable data.\n        y_data: Dependent variable data (observations).\n        max_iter: Maximum number of iterations.\n        tol_g: Tolerance for the infinity norm of the gradient.\n        tol_p: Tolerance for the parameter update step.\n        tol_f: Tolerance for the change in the objective function.\n\n    Returns:\n        The fitted parameter vector.\n    \"\"\"\n    p_k = np.array(p_init, dtype=float)\n    m = len(p_k)\n    eps = np.finfo(np.float64).eps\n\n    # Initial evaluation\n    r_k = y_data - model_func(x_data, p_k)\n    F_k = 0.5 * np.dot(r_k, r_k)\n\n    # Jacobian calculation function\n    def calculate_jacobian(p):\n        J = np.zeros((len(x_data), m))\n        for j in range(m):\n            delta = np.sqrt(eps) * (1.0 + np.abs(p[j]))\n            p_plus = p.copy()\n            p_minus = p.copy()\n            p_plus[j] += delta\n            p_minus[j] -= delta\n            \n            # Central difference for jacobian of residual r = y - f\n            # dr/dp = -df/dp\n            # Using (f(p - d) - f(p + d)) / 2d to get -df/dp\n            f_plus = model_func(x_data, p_plus)\n            f_minus = model_func(x_data, p_minus)\n\n            J[:, j] = (f_minus - f_plus) / (2.0 * delta)\n        return J\n\n    J_k = calculate_jacobian(p_k)\n    g_k = np.dot(J_k.T, r_k)\n\n    if np.linalg.norm(g_k, np.inf)  tol_g:\n        return p_k\n\n    # Initial damping parameter\n    A = np.dot(J_k.T, J_k)\n    tau = 1e-4\n    mu = tau * np.max(np.diag(A))\n    nu = 2.0\n    \n    k = 0\n    while k  max_iter:\n        \n        while True:\n            H_lm = A + mu * np.identity(m)\n            try:\n                h = np.linalg.solve(H_lm, -g_k)\n            except np.linalg.LinAlgError:\n                mu *= nu\n                nu *= 2.0\n                continue\n\n            p_new = p_k + h\n            r_new = y_data - model_func(x_data, p_new)\n            F_new = 0.5 * np.dot(r_new, r_new)\n\n            # Predicted reduction\n            # pred_red = - (h^T * g_k + 0.5 * h^T * A * h)\n            pred_red = - (np.dot(h, g_k) + 0.5 * np.dot(h, np.dot(A, h)))\n            \n            # Gain ratio\n            actual_red = F_k - F_new\n            \n            if pred_red  0: # Avoid division by zero or negative\n                rho = actual_red / pred_red\n            else:\n                rho = -1.0\n\n            if rho  0:  # Step is accepted\n                # Check termination criteria on parameter and function change\n                p_norm = np.linalg.norm(p_k)\n                if np.linalg.norm(h)  tol_p * (p_norm + tol_p):\n                    return p_new\n                \n                F_norm = F_k\n                if abs(actual_red)  tol_f * (F_norm + tol_f):\n                    return p_new\n\n                p_k = p_new\n                F_k = F_new\n                r_k = r_new\n                \n                J_k = calculate_jacobian(p_k)\n                A = np.dot(J_k.T, J_k)\n                g_k = np.dot(J_k.T, r_k)\n\n                # Check termination on gradient\n                if np.linalg.norm(g_k, np.inf)  tol_g:\n                    return p_k\n\n                mu = mu * max(1/3.0, 1 - (2*rho - 1)**3)\n                nu = 2.0\n                break # Exit inner loop and start next iteration\n            else: # Step is rejected\n                mu *= nu\n                nu *= 2.0\n        \n        k += 1\n\n    return p_k\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the three test cases specified in the problem.\n    \"\"\"\n    \n    # Test Case 1: Exponential Model\n    def model1(x, p):\n        return p[0] * np.exp(p[1] * x) + p[2]\n    \n    x1 = np.linspace(0.0, 3.0, 31)\n    y1 = 2.0 * np.exp(-0.7 * x1) + 0.5 + 0.01 * np.sin(5 * x1)\n    p_init1 = [1.0, -0.2, 0.0]\n    \n    # Test Case 2: Logistic Model\n    def model2(x, p):\n        return p[0] / (1.0 + np.exp(-p[1] * (x - p[2])))\n        \n    x2 = np.linspace(-1.5, 2.0, 15)\n    y2 = 1.5 / (1.0 + np.exp(-3.0 * (x2 - 0.5))) + 0.02 * np.cos(3 * x2)\n    p_init2 = [1.0, 1.0, 0.0]\n    \n    # Test Case 3: Sinusoidal Model\n    def model3(x, p):\n        omega = 2.5\n        return p[0] * np.cos(omega * x + p[1]) + p[2]\n        \n    x3 = np.linspace(0.0, 2.0, 21)\n    y3 = 0.8 * np.cos(2.5 * x3 + 0.4) - 0.05 + 0.01 * np.sin(7 * x3)\n    p_init3 = [0.5, 0.0, 0.0]\n\n    test_cases = [\n        (model1, p_init1, x1, y1),\n        (model2, p_init2, x2, y2),\n        (model3, p_init3, x3, y3),\n    ]\n\n    results = []\n    for model_func, p_init, x_data, y_data in test_cases:\n        p_fit = levenberg_marquardt(model_func, p_init, x_data, y_data)\n        rounded_p = [round(p, 6) for p in p_fit]\n        results.append(str(rounded_p).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}