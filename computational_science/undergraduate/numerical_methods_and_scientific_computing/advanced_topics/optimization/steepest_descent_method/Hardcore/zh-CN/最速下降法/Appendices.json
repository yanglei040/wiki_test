{
    "hands_on_practices": [
        {
            "introduction": "最速下降法的第一步，也是最基本的一步，是确定从当前点出发哪个方向能使函数值下降最快。这个方向正是该点梯度的反方向。本练习将通过一个经典的优化测试函数——Rosenbrock函数，来实践如何计算初始搜索方向，为后续的迭代过程打下基础。",
            "id": "2221567",
            "problem": "在数值优化领域，新算法的性能通常采用一组标准测试函数进行基准测试。其中一个函数是Rosenbrock函数，该函数因其狭窄的抛物线形山谷而难以最小化。\n\n考虑Rosenbrock函数的二维形式，其表达式为\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\n其中$a$和$b$是正实数常量。\n\n一个迭代最小化算法从点 $(x_0, y_0) = (0, 0)$ 开始。该算法的第一步是确定初始搜索方向。这个方向被定义为函数值从起始点下降最快的向量。\n\n确定该初始搜索方向向量 $\\mathbf{d}_0$。请用常数$a$和$b$将您的答案表示为列向量。",
            "solution": "函数$f$在某点下降最快的方向是该点的负梯度方向，因此从$(x_{0},y_{0})=(0,0)$出发的初始搜索方向为 $\\mathbf{d}_{0}=-\\nabla f(0,0)$。\n\n计算 $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$ 的梯度：\n- 关于$x$的偏导数为\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- 关于$y$的偏导数为\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\n在点$(0,0)$处计算它们的值：\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\n因此，\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\\0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\\0\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$"
        },
        {
            "introduction": "确定了下降方向后，我们还需要决定沿着这个方向走多远，即确定步长。对于二次函数，我们可以通过“精确线搜索”计算出最优步长，使得函数值下降最大化。通过对一个具有椭圆等值线的二次函数进行两次完整迭代，你将亲手体验最速下降法的完整计算过程，并直观地观察其特有的“锯齿形”收敛路径。",
            "id": "2221576",
            "problem": "考虑最小化函数 $f(x, y) = 10x^2 + y^2$ 的无约束优化问题。优化过程从点 $\\mathbf{x}_0 = (x_0, y_0) = (1, 1)$ 开始。\n\n将使用最速下降算法。对于每次迭代 $k$，步长（记为 $\\alpha_k  0$）通过精确线搜索确定。这意味着对于给定的点 $\\mathbf{x}_k$ 和下降方向 $\\mathbf{p}_k$，选择步长 $\\alpha_k$ 以找到单变量函数 $g(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ 的全局最小值。\n\n计算该方法经过两次完整迭代后的点 $\\mathbf{x}_2 = (x_2, y_2)$ 的坐标。最终答案中的坐标应以最简精确分数形式表示。",
            "solution": "我们使用带精确线搜索的最速下降法，从 $\\mathbf{x}_{0}=(1,1)$ 开始最小化 $f(x,y)=10x^{2}+y^{2}$。梯度和Hessian矩阵为\n$$\n\\nabla f(x,y)=\\begin{pmatrix}20x\\\\2y\\end{pmatrix},\\qquad H=\\begin{pmatrix}20  0\\\\0  2\\end{pmatrix}.\n$$\n在第 $k$ 次迭代中，$\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})$，方向为 $\\mathbf{p}_{k}=-\\mathbf{g}_{k}$，精确线搜索最小化 $g(\\alpha)=f(\\mathbf{x}_{k}+\\alpha\\mathbf{p}_{k})$。由于 $H$ 是常数（二次函数），其导数为\n$$\ng'(\\alpha)=\\nabla f(\\mathbf{x}_{k}-\\alpha \\mathbf{g}_{k})^{\\mathsf{T}}(-\\mathbf{g}_{k})=\\left(\\mathbf{g}_{k}-\\alpha H\\mathbf{g}_{k}\\right)^{\\mathsf{T}}(-\\mathbf{g}_{k})=-(\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k})+\\alpha\\,\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}.\n$$\n令 $g'(\\alpha)=0$ 得到精确步长\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}}.\n$$\n\n迭代 0：$\\mathbf{x}_{0}=(1,1)$ 给出\n$$\n\\mathbf{g}_{0}=\\begin{pmatrix}20\\\\2\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}\\mathbf{g}_{0}=404,\\quad H\\mathbf{g}_{0}=\\begin{pmatrix}400\\\\4\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}H\\mathbf{g}_{0}=8008,\n$$\n所以\n$$\n\\alpha_{0}=\\frac{404}{8008}=\\frac{101}{2002}.\n$$\n那么\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}=\\begin{pmatrix}1-20\\alpha_{0}\\\\1-2\\alpha_{0}\\end{pmatrix}=\\begin{pmatrix}1-\\frac{1010}{1001}\\\\1-\\frac{101}{1001}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}\\\\\\frac{900}{1001}\\end{pmatrix}.\n$$\n\n迭代 1：$\\mathbf{x}_{1}=\\left(-\\frac{9}{1001},\\,\\frac{900}{1001}\\right)$ 给出\n$$\n\\mathbf{g}_{1}=\\begin{pmatrix}20x_{1}\\\\2y_{1}\\end{pmatrix}=\\begin{pmatrix}-\\frac{180}{1001}\\\\\\frac{1800}{1001}\\end{pmatrix}.\n$$\n计算\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}=\\frac{180^{2}+1800^{2}}{1001^{2}}=\\frac{180^{2}\\cdot 101}{1001^{2}},\\qquad\nH\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{3600}{1001}\\\\\\frac{3600}{1001}\\end{pmatrix},\\qquad\n\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}=\\frac{180\\cdot 3600}{1001^{2}}(1+10)=\\frac{7{,}128{,}000}{1001^{2}}.\n$$\n因此\n$$\n\\alpha_{1}=\\frac{\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}}{\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}}=\\frac{3{,}272{,}400}{7{,}128{,}000}=\\frac{101}{220}.\n$$\n更新\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{1}\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{9}{1001}-\\frac{101}{220}\\left(-\\frac{180}{1001}\\right)\\\\\\frac{900}{1001}-\\frac{101}{220}\\left(\\frac{1800}{1001}\\right)\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}+\\frac{909}{11011}\\\\\\frac{900}{1001}-\\frac{9090}{11011}\\end{pmatrix}=\\begin{pmatrix}\\frac{810}{11011}\\\\\\frac{810}{11011}\\end{pmatrix}.\n$$\n分数已为最简形式，因为 $\\gcd(810,11011)=1$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{810}{11011} \\\\ \\frac{810}{11011} \\end{pmatrix}}$$"
        },
        {
            "introduction": "前面的练习暗示了最速下降法在某些情况下的低效。本练习将引导你通过编程实践，系统地探究问题本身的“形状”（即病态程度）如何影响算法的收敛速度。你将通过构建具有不同条件数的二次函数并观察其收敛性能，深刻理解为何病态问题是最速下降法的主要挑战，并体会到更高级优化算法的必要性。",
            "id": "3279006",
            "problem": "你的任务是研究在二维严格凸二次函数上使用精确线搜索的最速下降法（也称为梯度下降法），并展示由病态条件引起的缓慢收敛。\n\n考虑以下形式的二次目标函数\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,(x - x^\\star)^\\top A (x - x^\\star),\n$$\n其中 $x \\in \\mathbb{R}^2$，$x^\\star \\in \\mathbb{R}^2$ 是唯一的极小值点，$A \\in \\mathbb{R}^{2 \\times 2}$ 是对称正定（SPD）矩阵。在下文所有情况中，取 $x^\\star = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，因此 $f(x) = \\tfrac{1}{2}\\,x^\\top A x$。梯度为 $\\nabla f(x)$，最速下降迭代为\n$$\nx_{k+1} \\;=\\; x_k \\;-\\; \\alpha_k \\,\\nabla f(x_k),\n$$\n其中步长 $\\alpha_k$ 通过精确线搜索选择，以最小化关于 $\\alpha \\in \\mathbb{R}$ 的 $f(x_k - \\alpha\\,\\nabla f(x_k))$。\n\n你将利用旋转和特征值设计一个具有指定条件数的 SPD 矩阵族。令\n$$\nR(\\theta) \\;=\\; \\begin{bmatrix}\n\\cos\\theta  -\\sin\\theta \\\\\n\\sin\\theta  \\cos\\theta\n\\end{bmatrix}, \\qquad\n\\Lambda \\;=\\; \\operatorname{diag}(\\lambda_{\\min}, \\lambda_{\\max}), \\qquad\nA \\;=\\; R(\\theta)\\,\\Lambda\\,R(\\theta)^\\top,\n$$\n因此 $A$ 的谱为 $\\{\\lambda_{\\min}, \\lambda_{\\max}\\}$，其 2-范数条件数为 $\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}$。角度必须以弧度为单位指定。\n\n任务：\n1. 从第一性原理出发，通过最小化关于 $\\alpha$ 的 $f(x_k - \\alpha\\,\\nabla f(x_k))$，推导应用于上述形式的严格凸二次函数 $f(x)$ 的最速下降法的精确线搜索步长规则。\n2. 对由上述 $A$ 定义的二次函数，从初始点 $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$ 开始，使用此精确线搜索规则实现最速下降法。在恰好 $T$ 次迭代后停止，除非梯度范数在此之前恰好变为零，这种情况下你可以立即停止。\n3. 对于下面的每个测试套件中的情况，计算缩减因子\n$$\nr_T \\;=\\; \\frac{f(x_T)}{f(x_0)},\n$$\n其中 $x_T$ 是 $T$ 步后的迭代点（或者如果方法因为梯度为零而提前终止，则为最终迭代点）。该比率是无量纲的。\n\n测试套件（角度以弧度为单位）：\n- 情况 S（球形，边界条件）：$\\lambda_{\\min} = 10$，$\\lambda_{\\max} = 10$，$\\theta = 0.7$，$T = 10$。\n- 情况 M（中度病态）：$\\lambda_{\\min} = 1$，$\\lambda_{\\max} = 100$，$\\theta = \\pi/6$，$T = 10$。\n- 情况 H（高度病态）：$\\lambda_{\\min} = 1$，$\\lambda_{\\max} = 10^4$，$\\theta = \\pi/4$，$T = 10$。\n\n你的程序必须：\n- 使用给定的 $\\lambda_{\\min}$、$\\lambda_{\\max}$ 和 $\\theta$ 为每种情况构建 $A$。\n- 从 $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$ 开始，使用精确线搜索运行最速下降法 $T$ 次迭代。\n- 为每种情况计算并记录 $r_T$。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3]\"），其中三个条目是 $r_T$ 的三个值（按 S、M、H 的顺序），为浮点数。不应打印其他任何文本。",
            "solution": "该问题被评估为有效，因为它具有科学依据、问题设定良好、客观，并包含足够信息以获得唯一解。它代表了优化领域一个标准的数值实验，用以展示病态条件对最速下降法收敛性的影响。\n\n所需的任务是：\n1.  推导用于严格凸二次函数的精确线搜索步长公式。\n2.  使用该步长实现最速下降法。\n3.  对三种特定情况，计算函数值缩减因子 $r_T = f(x_T)/f(x_0)$。\n\n**1. 精确线搜索步长的推导**\n\n最速下降算法使用以下更新规则生成一系列迭代点：\n$$\nx_{k+1} = x_k - \\alpha_k \\nabla f(x_k)\n$$\n其中 $\\alpha_k  0$ 是步长。精确线搜索选择 $\\alpha_k$ 来最小化目标函数在搜索方向 $d_k = -\\nabla f(x_k)$ 上的值。我们定义一个一维函数 $\\phi(\\alpha) = f(x_k + \\alpha d_k) = f(x_k - \\alpha g_k)$，其中 $g_k = \\nabla f(x_k)$。\n\n目标函数是严格凸二次型 $f(x) = \\frac{1}{2}x^\\top A x$，其中 $A$ 是一个对称正定（SPD）矩阵。\n该函数的梯度为 $\\nabla f(x) = \\frac{1}{2}(A + A^\\top)x$。由于 $A$ 是对称的（$A=A^\\top$），梯度可以简化为：\n$$\ng_k = \\nabla f(x_k) = A x_k\n$$\n我们将更新规则代入目标函数以得到 $\\phi(\\alpha)$：\n$$\n\\phi(\\alpha) = f(x_k - \\alpha g_k) = \\frac{1}{2} (x_k - \\alpha g_k)^\\top A (x_k - \\alpha g_k)\n$$\n展开此表达式，我们得到：\n$$\n\\phi(\\alpha) = \\frac{1}{2} (x_k^\\top A x_k - \\alpha x_k^\\top A g_k - \\alpha g_k^\\top A x_k + \\alpha^2 g_k^\\top A g_k)\n$$\n由于 $g_k^\\top A x_k$ 是一个标量，它等于其转置，即 $(g_k^\\top A x_k)^\\top = x_k^\\top A^\\top g_k$。因为 $A$ 是对称的，这等于 $x_k^\\top A g_k$。因此，关于 $\\alpha$ 的两个线性项是相同的。\n$$\n\\phi(\\alpha) = \\frac{1}{2} x_k^\\top A x_k - \\alpha g_k^\\top A x_k + \\frac{1}{2} \\alpha^2 g_k^\\top A g_k\n$$\n这是一个关于 $\\alpha$ 的标量二次函数。为了找到最小化 $\\phi(\\alpha)$ 的 $\\alpha$ 值，我们计算它关于 $\\alpha$ 的导数并将其设为零。\n$$\n\\frac{d\\phi}{d\\alpha} = -g_k^\\top A x_k + \\alpha g_k^\\top A g_k = 0\n$$\n解出 $\\alpha$，我们得到最优步长，记为 $\\alpha_k$：\n$$\n\\alpha_k = \\frac{g_k^\\top A x_k}{g_k^\\top A g_k}\n$$\n我们可以利用关系式 $g_k = A x_k$ 来简化此公式。分子变为 $g_k^\\top (A x_k) = g_k^\\top g_k$。因此，步长为：\n$$\n\\alpha_k = \\frac{g_k^\\top g_k}{g_k^\\top A g_k}\n$$\n这是二次目标函数精确线搜索步长的著名公式。二阶导数 $\\frac{d^2\\phi}{d\\alpha^2} = g_k^\\top A g_k$ 为正（因为 $A$ 是正定的，且假设 $g_k \\neq 0$），这证实了 $\\alpha_k$ 产生一个最小值。如果 $g_k = 0$，则迭代点 $x_k$ 已经是 $f(x)$ 的极小值点，迭代终止。\n\n**2. 算法实现与执行**\n\n通过实现带有推导出的精确线搜索的最速下降法来进行数值实验。对于每种情况（S、M、H），执行以下步骤：\n1.  使用指定的参数 $\\lambda_{\\min}$、$\\lambda_{\\max}$ 和 $\\theta$ 来构建 SPD 矩阵 $A = R(\\theta)\\Lambda R(\\theta)^\\top$。\n2.  初始点设为 $x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}$。计算初始函数值 $f(x_0)$。\n3.  最速下降迭代 $x_{k+1} = x_k - \\alpha_k g_k$ 运行整整 $T$ 步，除非梯度范数恰好变为零。在每一步 $k$，计算梯度 $g_k = A x_k$ 和步长 $\\alpha_k = (g_k^\\top g_k) / (g_k^\\top A g_k)$。\n4.  $T$ 次迭代后（或提前终止后），最终迭代点为 $x_T$。\n5.  计算最终函数值 $f(x_T)$。\n6.  计算并存储缩减因子 $r_T = f(x_T) / f(x_0)$。\n\n对于情况 S，$\\lambda_{\\min}=\\lambda_{\\max}$，因此条件数 $\\kappa(A)=1$。矩阵 $A$ 是单位矩阵的一个标量倍，$A=10I$。$f(x)$ 的水平集是圆形，最速下降法在单步内收敛到极小值点 $x^\\star=0$。因此，$x_1=x_T=0$，$f(x_T)=0$，且 $r_T=0$。\n\n对于情况 M 和 H，条件数分别为 $\\kappa(A)=100$ 和 $\\kappa(A)=10^4$。水平集是拉长的椭圆。最速下降路径将表现出典型的 Z 字形行为，导致收敛缓慢。缩减因子 $r_T$ 预计将显著大于 0，并且情况 H 的 $r_T$ 会比情况 M 的更大，这表明性能随病态程度的增加而下降。所提供的 Python 代码实现了此过程以计算所需的值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_steepest_descent(lambda_min: float, lambda_max: float, theta: float, T: int, x0: np.ndarray) - float:\n    \"\"\"\n    Performs steepest descent with exact line search for a quadratic function.\n\n    Args:\n        lambda_min: The minimum eigenvalue of the matrix A.\n        lambda_max: The maximum eigenvalue of the matrix A.\n        theta: The rotation angle in radians for constructing A.\n        T: The number of iterations.\n        x0: The starting point as a numpy array.\n\n    Returns:\n        The reduction factor r_T = f(x_T) / f(x_0).\n    \"\"\"\n    # 1. Construct the matrix A\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array([[c, -s], [s, c]])\n    Lambda = np.diag([lambda_min, lambda_max])\n    A = R @ Lambda @ R.T\n\n    # Define the objective function\n    def f(x: np.ndarray) - float:\n        return 0.5 * x.T @ A @ x\n\n    # 2. Calculate initial function value\n    f_x0 = f(x0)\n    \n    # If starting at the minimum, reduction is not well-defined,\n    # but we can consider f(x_T) = f(x_0) = 0, so r_T would be 1.\n    # A more sensible interpretation for f(x_0)=0 is r_T=0, as there's 0 to reduce.\n    if f_x0 == 0.0:\n        return 0.0\n\n    # 3. Run steepest descent iterations\n    x = x0.copy()\n    for _ in range(T):\n        # Calculate gradient\n        g = A @ x\n        \n        # Calculate squared norm of the gradient\n        g_dot_g = g.T @ g\n        \n        # If gradient is zero, we have reached the minimum. Stop early.\n        if g_dot_g == 0.0:\n            x = np.zeros_like(x0)  # The minimum is at x=0\n            break\n            \n        # Calculate exact step length alpha\n        # Note: g.T @ A @ g cannot be zero if A is SPD and g is non-zero.\n        alpha = g_dot_g / (g.T @ A @ g)\n        \n        # Update iterate x\n        x = x - alpha * g\n        \n    # 4. Compute final function value and reduction factor\n    f_xT = f(x)\n    r_T = f_xT / f_x0\n    \n    return r_T\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the steepest descent for each, and prints results.\n    \"\"\"\n    # Initial point for all cases\n    x0 = np.array([3.0, -1.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case S (spherical)\n        {'name': 'S', 'params': {'lambda_min': 10.0, 'lambda_max': 10.0, 'theta': 0.7, 'T': 10}},\n        # Case M (moderately ill-conditioned)\n        {'name': 'M', 'params': {'lambda_min': 1.0, 'lambda_max': 100.0, 'theta': np.pi/6, 'T': 10}},\n        # Case H (highly ill-conditioned)\n        {'name': 'H', 'params': {'lambda_min': 1.0, 'lambda_max': 10**4, 'theta': np.pi/4, 'T': 10}},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_steepest_descent(\n            lambda_min=case['params']['lambda_min'],\n            lambda_max=case['params']['lambda_max'],\n            theta=case['params']['theta'],\n            T=case['params']['T'],\n            x0=x0\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}