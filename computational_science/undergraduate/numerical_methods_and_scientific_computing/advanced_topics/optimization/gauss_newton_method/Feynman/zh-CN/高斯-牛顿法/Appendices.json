{
    "hands_on_practices": [
        {
            "introduction": "要真正理解一个算法，没有什么比亲手计算一遍更好的方法了。第一个实践练习将引导你具体地、一步步地应用高斯-牛顿法。通过为一个简单的单参数模型手动计算一次迭代，你将巩固对该方法核心组成部分的理解：残差向量、雅可比矩阵以及更新方程本身 。",
            "id": "2214282",
            "problem": "在一项实验研究中，某个物理过程由函数 $y(x) = \\frac{x}{1+ax}$ 建模，其中 $a$ 是一个待确定的未知参数。一名研究人员收集了两个数据点 $(x_i, y_i)$：第一个点是 $(1, 0.5)$，第二个点是 $(2, 0.8)$。\n\n为了在最小二乘意义下找到最佳拟合数据的参数 $a$ 的最优值，研究人员决定使用高斯-牛顿法 (Gauss-Newton method)。从初始猜测值 $a_0 = 1$ 开始，执行恰好一次高斯-牛顿法的迭代，以找到参数的更新估计值，记为 $a_1$。\n\n请将 $a_1$ 的答案表示为最简精确分数形式。",
            "solution": "我们用 $y(x;a)=\\dfrac{x}{1+a x}$ 对数据进行建模。对于残差为 $r_{i}(a)=y(x_{i};a)-y_{i}$ 的最小二乘问题，从 $a_{0}$ 开始对单个参数 $a$ 的高斯-牛顿更新为\n$$\n\\Delta a=-(J^{\\top}J)^{-1}J^{\\top}r,\n$$\n其中 $J_{i}=\\dfrac{\\partial r_{i}}{\\partial a}=\\dfrac{\\partial y(x_{i};a)}{\\partial a}$ 是在 $a_{0}$ 处计算的值，而 $r$ 是在 $a_{0}$ 处计算的残差向量。于是有 $a_{1}=a_{0}+\\Delta a$。\n\n首先计算模型关于 $a$ 的导数。将 $y(x;a)$写作 $x(1+a x)^{-1}$，我们得到\n$$\n\\frac{\\partial y}{\\partial a}=-\\frac{x^{2}}{(1+a x)^{2}}.\n$$\n\n对于数据点 $(x_{1},y_{1})=(1,\\tfrac{1}{2})$ 和 $(x_{2},y_{2})=(2,\\tfrac{4}{5})$，以及初始猜测值 $a_{0}=1$，雅可比矩阵的项为\n$$\nJ_{1}=\\left.-\\frac{x_{1}^{2}}{(1+a x_{1})^{2}}\\right|_{a=1}=-\\frac{1}{(1+1)^{2}}=-\\frac{1}{4},\\quad\nJ_{2}=\\left.-\\frac{x_{2}^{2}}{(1+a x_{2})^{2}}\\right|_{a=1}=-\\frac{4}{(1+2)^{2}}=-\\frac{4}{9}.\n$$\n\n在 $a_{0}=1$ 处的残差为\n$$\nr_{1}=y(1;1)-\\frac{1}{2}=\\frac{1}{2}-\\frac{1}{2}=0,\\quad\nr_{2}=y(2;1)-\\frac{4}{5}=\\frac{2}{3}-\\frac{4}{5}=-\\frac{2}{15}.\n$$\n\n计算标量 $J^{\\top}r$ 和 $J^{\\top}J$：\n$$\nJ^{\\top}r=J_{1}r_{1}+J_{2}r_{2}=0+\\left(-\\frac{4}{9}\\right)\\left(-\\frac{2}{15}\\right)=\\frac{8}{135},\n$$\n$$\nJ^{\\top}J=J_{1}^{2}+J_{2}^{2}=\\frac{1}{16}+\\frac{16}{81}=\\frac{81+256}{1296}=\\frac{337}{1296}.\n$$\n\n因此，\n$$\n\\Delta a=-\\frac{J^{\\top}r}{J^{\\top}J}=-\\frac{\\frac{8}{135}}{\\frac{337}{1296}}=-\\frac{8}{135}\\cdot\\frac{1296}{337}=-\\frac{8\\cdot 48}{5\\cdot 337}=-\\frac{384}{1685}.\n$$\n\n因此，更新后的估计值为\n$$\na_{1}=a_{0}+\\Delta a=1-\\frac{384}{1685}=\\frac{1685-384}{1685}=\\frac{1301}{1685}.\n$$",
            "answer": "$$\\boxed{\\frac{1301}{1685}}$$"
        },
        {
            "introduction": "基本的高斯-牛顿法提供了一个强大的搜索方向，但并不能保证沿该方向迈出完整一步总能改进解。这个动手编程练习通过引入回溯线搜索来解决这个关键问题，这是一种确保每次迭代都能取得充分进展的“全局化”策略。通过实现 Armijo 条件，你将构建一个更稳健、更可靠的求解器，即使从远离真解的初始猜测点出发也能够收敛 。",
            "id": "3232827",
            "problem": "考虑将一个指数模型与数据进行非线性最小二乘拟合。设参数向量为 $x = [a,b]^{\\top}$，模型为 $m(t;a,b) = a \\exp(b t)$。对于一组给定的观测时间 $\\{t_i\\}_{i=1}^m$ 和观测值 $\\{y_i\\}_{i=1}^m$，定义残差向量 $r(x) \\in \\mathbb{R}^m$，其分量为 $r_i(x) = m(t_i;a,b) - y_i$，并定义目标函数 $\\phi(x) = \\tfrac{1}{2} \\sum_{i=1}^m r_i(x)^2$。目标是关于 $x$ 最小化 $\\phi(x)$。\n\n从非线性最小二乘、残差函数以及雅可比矩阵 $J(x) \\in \\mathbb{R}^{m \\times 2}$（其元素为 $J_{i1}(x) = \\partial r_i / \\partial a$ 和 $J_{i2}(x) = \\partial r_i / \\partial b$）的核心定义出发，通过对 $r(x)$ 在当前迭代点 $x_k$ 周围进行一阶泰勒线性化，推导出高斯-牛顿迭代法。利用此推导实现一个算法，在每次迭代 $k$ 中，通过求解由线性化残差产生的线性最小二乘子问题来计算高斯-牛顿搜索方向 $p_k$。由于局部二次模型在远离解时可能不准确，一个完整的步长 $x_{k+1} = x_k + p_k$ 可能会增加 $\\phi(x)$；因此，使用 Armijo 充分下降条件来强制执行回溯线搜索。具体而言，对于试验步长 $\\alpha \\in (0,1]$，要求\n$$\n\\phi(x_k + \\alpha p_k) \\le \\phi(x_k) + c_1 \\alpha \\nabla \\phi(x_k)^{\\top} p_k,\n$$\n其中 $c_1 \\in (0,1)$ 为一固定值，并将 $\\alpha$ 按一个在 $(0,1)$ 内的常数因子进行缩减，直到满足该条件。使用由链式法则和 $\\phi$ 的定义得出的梯度恒等式 $\\nabla \\phi(x) = J(x)^{\\top} r(x)$。\n\n你的程序必须仅使用上述定义和标准线性代数运算来实现带有回溯线搜索的高斯-牛顿法，当满足一个合理的平稳性条件或达到最大迭代次数时停止。报告完整的高斯-牛顿步长是否在第一次迭代时增加了目标函数值（通过需要回溯来检测），以及最终的拟合质量。\n\n测试套件。使用由以下定义的通用数据集：\n- 时间点 $\\{t_i\\}_{i=1}^6 = \\{0,1,2,3,4,5\\}$，\n- 真实参数 $a_{\\mathrm{true}} = 2.0$ 和 $b_{\\mathrm{true}} = 0.5$，\n- 无噪声观测值 $y_i = a_{\\mathrm{true}} \\exp(b_{\\mathrm{true}} t_i)$，其中 $i \\in \\{1,\\dots,6\\}$。\n\n运行三个仅在初始猜测值 $x_0$ 上有所不同的测试用例：\n- 用例 1（顺利路径）：$x_0 = [1.8, 0.45]^{\\top}$，接近真实参数。\n- 用例 2（预期完整步长会增加目标函数值）：$x_0 = [0.1, -1.0]^{\\top}$，远离真实参数且 $b$ 的符号错误。\n- 用例 3（起始时雅可比矩阵近奇异）：$x_0 = [10^{-6}, 0.5]^{\\top}$，这使得雅可比矩阵的第二列初始值非常小。\n\n实现要求：\n- 通过求解在 $x_k$ 处对 $r(x)$ 进行线性化得到的线性最小二乘子问题，来计算高斯-牛顿方向 $p_k$。\n- 使用回溯线搜索，参数为 $c_1 = 10^{-4}$，收缩因子 $\\beta = 1/2$，每次迭代开始时令 $\\alpha = 1$。如果 Armijo 条件不满足，则缩减 $\\alpha \\leftarrow \\beta \\alpha$，直到条件满足或达到最小步长 $\\alpha_{\\min} = 10^{-8}$，此时应接受当前的 $\\alpha$ 并继续。\n- 当 $\\|\\nabla \\phi(x_k)\\|_{\\infty} \\le 10^{-8}$ 或达到 $200$ 次迭代后终止，以先满足者为准。\n\n输出规范：\n- 对于每个测试用例，输出一个包含三个值的列表：仅在第一次迭代中应用的回溯缩减次数的整数值（如果接受了完整步长则为零），直到终止所执行的总迭代次数的整数值，以及最终目标函数值 $\\phi(x)$（四舍五入到六位小数的浮点数）。\n- 你的程序应生成一行输出，其中包含三个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个元素对应一个测试用例，并且本身是按上述顺序排列的列表。例如，一个有效的输出格式是 $[[n_1,i_1,f_1],[n_2,i_2,f_2],[n_3,i_3,f_3]]$。\n\n此问题不涉及角度。没有需要报告的物理单位，所有数值输出必须是无单位的实数。",
            "solution": "用户提供的问题是有效的。这是一个在数值优化领域内表述清晰、科学严谨的问题，其设置完整且一致。\n\n该问题要求实现带有回溯线搜索的高斯-牛顿法，以解决一个非线性最小二乘问题。目标是将模型 $m(t; a, b) = a \\exp(b t)$ 拟合到一组数据点 $\\{t_i, y_i\\}_{i=1}^m$。需要优化的参数向量是 $x = [a, b]^{\\top}$。\n\n需要最小化的目标函数是残差平方和：\n$$\n\\phi(x) = \\frac{1}{2} \\sum_{i=1}^m r_i(x)^2 = \\frac{1}{2} r(x)^{\\top} r(x)\n$$\n其中残差向量 $r(x)$ 的分量为 $r_i(x) = m(t_i; a, b) - y_i = a \\exp(b t_i) - y_i$。\n\n高斯-牛顿法是一种迭代过程，在每次迭代 $k$ 中，用一个线性问题来近似非线性问题。给定当前迭代点 $x_k$，我们寻求一个步长 $p_k$，使得 $x_{k+1} = x_k + p_k$ 能改进解。该方法通过构建残差向量 $r(x)$ 在 $x_k$ 周围的一阶泰勒级数近似来推导：\n$$\nr(x_k + p_k) \\approx r(x_k) + J(x_k) p_k\n$$\n这里，$J(x_k)$ 是残差向量 $r(x)$ 在 $x_k$ 处计算的雅可比矩阵。其元素为 $J_{ij}(x) = \\frac{\\partial r_i}{\\partial x_j}$。对于我们这个特定的模型和 $x=[a,b]^\\top$，雅可比矩阵的两列为：\n$$\n\\frac{\\partial r_i}{\\partial a} = \\frac{\\partial}{\\partial a} (a \\exp(b t_i) - y_i) = \\exp(b t_i)\n$$\n$$\n\\frac{\\partial r_i}{\\partial b} = \\frac{\\partial}{\\partial b} (a \\exp(b t_i) - y_i) = a t_i \\exp(b t_i)\n$$\n因此，对于 $x_k = [a_k, b_k]^\\top$，雅可比矩阵 $J(x_k)$ 的第 $i$ 行为 $[\\exp(b_k t_i), a_k t_i \\exp(b_k t_i)]$。\n\n将 $r(x_k + p_k)$ 的线性近似代入目标函数 $\\phi(x)$，得到目标函数在 $x_k$ 处的二次模型：\n$$\n\\phi(x_k + p_k) \\approx \\frac{1}{2} \\|r(x_k) + J(x_k) p_k\\|^2\n$$\n高斯-牛顿法通过找到使该二次近似最小化的 $p_k$ 来确定搜索方向 $p_k$。这是一个线性最小二乘问题：\n$$\np_k = \\arg\\min_{p \\in \\mathbb{R}^2} \\|J(x_k) p - (-r(x_k))\\|^2\n$$\n该子问题的解即为高斯-牛顿方向。在实践中，这个系统使用数值稳定的技术（如 QR 分解）来求解，这些技术已在标准线性代数库中实现。\n\n如果迭代点 $x_k$ 距离最小值点较远，$r(x)$ 的线性近似不准确，那么一个完整的步长 $x_{k+1} = x_k + p_k$ 可能不会减小目标函数 $\\phi(x)$。为了确保从更广泛的初始猜测值收敛，采用了回溯线搜索。更新步骤变为 $x_{k+1} = x_k + \\alpha_k p_k$，其中 $\\alpha_k \\in (0, 1]$ 是步长。线搜索通过从 $\\alpha = 1$ 开始，并逐步减小它，直到满足 Armijo 充分下降条件来找到一个合适的 $\\alpha_k$：\n$$\n\\phi(x_k + \\alpha p_k) \\le \\phi(x_k) + c_1 \\alpha \\nabla \\phi(x_k)^{\\top} p_k\n$$\n常数 $c_1$ 是一个小数值，给定为 $c_1 = 10^{-4}$。目标函数的梯度 $\\nabla \\phi(x)$ 由恒等式 $\\nabla \\phi(x) = J(x)^{\\top} r(x)$ 给出。线搜索通过将 $\\alpha$ 按一个因子 $\\beta \\in (0, 1)$（给定为 $\\beta = 1/2$）进行缩减，直到条件满足或达到最小步长 $\\alpha_{\\min} = 10^{-8}$ 为止。\n\n整个算法如下：\n1. 初始化 $x_0$，设 $k=0$，并定义容差 `tol`$=10^{-8}$ 和 `max_iter`$=200$。\n2. 对于 $k=0, 1, \\dots, \\text{max\\_iter}-1$：\n    a. 计算 $r_k = r(x_k)$、$J_k = J(x_k)$ 和梯度 $\\nabla \\phi_k = J_k^{\\top} r_k$。\n    b. 检查收敛性：如果 $\\|\\nabla \\phi_k\\|_{\\infty} \\le \\text{tol}$，则终止。\n    c. 求解线性最小二乘问题 $\\min_p \\|J_k p + r_k\\|^2$ 以找到高斯-牛顿方向 $p_k$。\n    d. 执行回溯线搜索以找到满足 Armijo 条件的步长 $\\alpha_k$。从 $\\alpha=1$ 开始，并按 $\\alpha \\leftarrow \\beta \\alpha$ 缩减。\n    e. 更新参数：$x_{k+1} = x_k + \\alpha_k p_k$。\n3. 如果循环完成，则表示已达到最大迭代次数。\n\n该实现将把此算法应用于给定的数据集和三个初始猜测值，并报告关于首次迭代行为和整体收敛性的统计数据。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gauss_newton(x0, t, y, c1, beta, alpha_min, tol, max_iter):\n    \"\"\"\n    Solves a nonlinear least-squares problem using Gauss-Newton with backtracking.\n    \"\"\"\n    \n    # Helper functions for model, residual, objective, and jacobian\n    def model(x, t_vec):\n        a, b = x\n        # Handle potential overflow for large b\n        try:\n            with np.errstate(over='raise'):\n                 return a * np.exp(b * t_vec)\n        except FloatingPointError:\n            # Return an array of infinities with the correct sign\n            return np.full_like(t_vec, np.inf * np.sign(a))\n\n    def residual(x, t_vec, y_vec):\n        return model(x, t_vec) - y_vec\n\n    def objective(x, t_vec, y_vec):\n        res = residual(x, t_vec, y_vec)\n        # If model overflowed, residual is inf, dot product is inf\n        if np.isinf(res).any():\n            return np.inf\n        return 0.5 * np.dot(res, res)\n\n    def jacobian(x, t_vec):\n        a, b = x\n        jac = np.zeros((len(t_vec), 2), dtype=float)\n        try:\n            with np.errstate(over='raise'):\n                exp_bt = np.exp(b * t_vec)\n        except FloatingPointError:\n            exp_bt = np.full_like(t_vec, np.inf)\n\n        jac[:, 0] = exp_bt\n        jac[:, 1] = a * t_vec * exp_bt\n        return jac\n\n    x_k = np.array(x0, dtype=float)\n    k = 0\n    backtrack_count_first_iter = 0\n\n    while k  max_iter:\n        r_k = residual(x_k, t, y)\n        J_k = jacobian(x_k, t)\n        \n        # Check if Jacobian has inf/nan values due to overflow\n        if not np.all(np.isfinite(J_k)):\n            # This indicates a very bad parameter region, stop iteration\n            break\n            \n        grad_phi_k = J_k.T @ r_k\n\n        if np.max(np.abs(grad_phi_k)) = tol:\n            break\n\n        # Solve linear least squares subproblem for p_k: min ||J_k * p_k - (-r_k)||^2\n        try:\n            p_k = np.linalg.lstsq(J_k, -r_k, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            # Failed to solve, likely singular matrix. Stop.\n            break\n\n        # Backtracking line search\n        alpha = 1.0\n        phi_k = 0.5 * np.dot(r_k, r_k)\n        dir_deriv = np.dot(grad_phi_k, p_k)\n        \n        n_backtrack = 0\n        while True:\n            phi_trial = objective(x_k + alpha * p_k, t, y)\n            \n            # Armijo condition\n            if phi_trial = phi_k + c1 * alpha * dir_deriv:\n                break\n            \n            if alpha = alpha_min:\n                break\n\n            alpha *= beta\n            n_backtrack += 1\n\n        if k == 0:\n            backtrack_count_first_iter = n_backtrack\n        \n        x_k += alpha * p_k\n        k += 1\n    \n    final_phi = objective(x_k, t, y)\n    \n    return [backtrack_count_first_iter, k, round(final_phi, 6)]\n\ndef solve():\n    # Dataset definition\n    t = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n    a_true = 2.0\n    b_true = 0.5\n    y = a_true * np.exp(b_true * t)\n\n    # Algorithm parameters\n    c1 = 1e-4\n    beta = 0.5\n    alpha_min = 1e-8\n    tol = 1e-8\n    max_iter = 200\n\n    # Test cases\n    test_cases = [\n        [1.8, 0.45],   # Case 1: Close to solution\n        [0.1, -1.0],   # Case 2: Far from solution\n        [1e-6, 0.5]    # Case 3: Nearly singular Jacobian\n    ]\n    \n    results = []\n    for x0 in test_cases:\n        result = run_gauss_newton(x0, t, y, c1, beta, alpha_min, tol, max_iter)\n        results.append(result)\n\n    # Format output according to specification\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "我们已经学习了如何确保算法的收敛性，但如果问题本身是病态的（ill-posed），导致高斯-牛顿步长不稳定甚至无法计算，该怎么办？这个实践将深入探讨 Tikhonov 正则化，这是一种用于稳定求解高斯-牛顿每次迭代中出现的病态或秩亏线性系统的基本技术。通过推导和实现正则化更新，你将学会如何处理那些参数难以从数据中唯一辨识的、具有挑战性的数据拟合场景 。",
            "id": "3232783",
            "problem": "考虑一个无约束非线性最小二乘问题，其参数向量为 $\\theta \\in \\mathbb{R}^p$，残差向量为 $r(\\theta) \\in \\mathbb{R}^m$，其中每个分量 $r_i(\\theta)$ 都是 $\\theta$ 的平滑函数。使用 Tikhonov 正则化来增强目标函数，得到惩罚目标函数 $\\phi(\\theta) = \\tfrac{1}{2}\\|r(\\theta)\\|_2^2 + \\tfrac{1}{2}\\lambda\\|\\theta\\|_2^2$，其中 $\\lambda \\ge 0$ 是一个固定的正则化参数。从非线性最小二乘和 Tikhonov 正则化的定义出发，使用残差 $r(\\theta + \\Delta)$ 在当前迭代点 $\\theta$ 周围的一阶泰勒线性化，推导 Gauss-Newton (GN) 步长 $\\Delta$，该步长是通过最小化 $\\phi(\\theta + \\Delta)$ 关于 $\\Delta$ 的二次近似得到的。解释推导的每一步，包括雅可比矩阵 $J(\\theta) = \\frac{\\partial r(\\theta)}{\\partial \\theta}$ 是如何引入的，并证明对于病态问题，当 $\\lambda  0$ 时，所得到的线性系统为何条件更好。\n\n然后，实现一个 GN 算法，该算法使用此正则化步长来更新 $\\theta \\leftarrow \\theta + \\Delta$ 直至收敛，当步长范数 $\\|\\Delta\\|_2$ 低于一个容差时，即宣布收敛。为具体起见，采用参数模型 $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$，残差 $r_i(\\theta) = F(x_i;\\theta) - y_i$，以及雅可比矩阵 $J(\\theta)$，其第 $i$ 行包含对应于给定 $x_i$ 值的关于 $\\theta_1$ 和 $\\theta_2$ 的偏导数。\n\n您的程序必须实现此算法，并在以下参数集的测试套件上进行评估。在所有情况下，不涉及角度，也不涉及物理单位。程序必须为每个测试用例计算收敛时最终残差向量的欧几里得范数 $\\|r(\\theta^\\star)\\|_2$，并以浮点数形式表示。\n\n测试套件：\n- 案例1（适定问题，边界情况 $\\lambda = 0$）：$x = [0.0, 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.0]$，$\\theta_{\\text{true}} = [2.0, -0.5]$，$y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$，初始猜测值 $\\theta^{(0)} = [1.0, 0.0]$，正则化参数 $\\lambda = 0.0$。\n- 案例2（病态问题，当 $x=0$ 时存在可辨识性失效）：$x = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$，$\\theta_{\\text{true}} = [1.5, 0.3]$，$y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$（常数值），初始猜测值 $\\theta^{(0)} = [1.0, 1.0]$，正则化参数 $\\lambda = 0.5$。\n- 案例3（因雅可比矩阵列近似共线导致的病态条件）：$x = [0.0, 0.004, 0.008, 0.012, 0.016, 0.02]$，$\\theta_{\\text{true}} = [2.0, 5.0]$，$y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$，初始猜测值 $\\theta^{(0)} = [1.0, 0.0]$，正则化参数 $\\lambda = 0.001$。\n\n算法细节：\n- 使用与模型 $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$ 对应的残差 $r(\\theta)$ 和雅可比矩阵 $J(\\theta)$。\n- 在每次迭代中，通过求解一个线性系统来获得从二次近似推导出的 GN 步长 $\\Delta$，并更新 $\\theta \\leftarrow \\theta + \\Delta$。\n- 当 $\\|\\Delta\\|_2 \\le \\text{tol}$ 或达到最大迭代次数时终止。使用容差 $\\text{tol} = 10^{-10}$ 和最大迭代次数 $50$ 次。\n\n要求的最终输出格式：\n您的程序应生成一行输出，其中包含三个测试用例的结果，以逗号分隔的列表形式并用方括号括起来，顺序为案例1、案例2、案例3。例如，格式必须完全像 $[r_1,r_2,r_3]$，其中每个 $r_k$ 是第 $k$ 个案例的 $\\|r(\\theta^\\star)\\|_2$ 的浮点值。",
            "solution": "用户提供了一个定义明确的数值优化问题。该问题是有效的，因为它在科学上基于非线性最小二乘法和正则化的原理，是适定的、客观的，并包含推导和实现解决方案所需的所有必要信息。\n\n### 正则化 Gauss-Newton 步长的推导\n\n目标是找到一个参数向量 $\\theta \\in \\mathbb{R}^p$ 来最小化惩罚目标函数 $\\phi(\\theta)$，该函数由平方和误差项和 Tikhonov 正则化项组成：\n$$ \\phi(\\theta) = \\frac{1}{2}\\|r(\\theta)\\|_2^2 + \\frac{1}{2}\\lambda\\|\\theta\\|_2^2 $$\n这里，$r(\\theta) \\in \\mathbb{R}^m$ 是残差向量，$\\lambda \\ge 0$ 是正则化参数。Gauss-Newton 方法是一种迭代算法，在每一步 $k$ 中，找到一个更新量 $\\Delta$ 从当前估计 $\\theta^{(k)}$ 移动到新的估计 $\\theta^{(k+1)} = \\theta^{(k)} + \\Delta$。为简化起见，我们将省略上标 $(k)$，并将当前估计表示为 $\\theta$。\n\n步长 $\\Delta$ 是通过最小化在 $\\theta + \\Delta$ 处的目标函数的二次近似来找到的。Gauss-Newton 方法的核心近似是使用其在 $\\theta$ 周围的一阶泰勒展开来线性化非线性残差函数 $r(\\theta + \\Delta)$：\n$$ r(\\theta + \\Delta) \\approx r(\\theta) + J(\\theta)\\Delta $$\n其中 $J(\\theta)$ 是残差向量的 $m \\times p$ 雅可比矩阵，其元素为 $J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$。\n\n我们将此近似代入惩罚目标函数 $\\phi(\\theta + \\Delta)$。这为 $\\phi(\\theta+\\Delta)$ 产生了一个二次模型，我们将其表示为 $\\Phi(\\Delta)$：\n$$ \\Phi(\\Delta) = \\frac{1}{2} \\|r(\\theta) + J(\\theta)\\Delta\\|_2^2 + \\frac{1}{2}\\lambda\\|\\theta + \\Delta\\|_2^2 $$\n为了找到最小化此函数的步长 $\\Delta$，我们展开各项。使用向量表示法，其中 $r \\equiv r(\\theta)$ 和 $J \\equiv J(\\theta)$：\n$$ \\Phi(\\Delta) = \\frac{1}{2} (r + J\\Delta)^T(r + J\\Delta) + \\frac{1}{2}\\lambda (\\theta + \\Delta)^T(\\theta + \\Delta) $$\n展开乘积得到：\n$$ \\Phi(\\Delta) = \\frac{1}{2} (r^T r + 2r^T J\\Delta + \\Delta^T J^T J\\Delta) + \\frac{1}{2}\\lambda (\\theta^T\\theta + 2\\theta^T\\Delta + \\Delta^T\\Delta) $$\n这个表达式是关于 $\\Delta$ 的二次函数。为了找到它的最小值，我们计算它关于 $\\Delta$ 的梯度并将其设为零。使用矩阵微积分恒等式 $\\nabla_x(a^Tx) = a$ 和 $\\nabla_x(x^TAx) = (A+A^T)x$：\n$$ \\nabla_{\\Delta} \\Phi(\\Delta) = \\frac{1}{2} (2J^T r + 2J^T J\\Delta) + \\frac{1}{2}\\lambda (2\\theta + 2I\\Delta) $$\n$$ \\nabla_{\\Delta} \\Phi(\\Delta) = (J^T r + J^T J\\Delta) + \\lambda(\\theta + I\\Delta) $$\n将梯度设为零，$\\nabla_{\\Delta} \\Phi(\\Delta) = 0$：\n$$ J^T J\\Delta + J^T r + \\lambda\\theta + \\lambda I\\Delta = 0 $$\n重新排列各项以求解 $\\Delta$，我们将包含 $\\Delta$ 的项组合在一起：\n$$ (J^T J + \\lambda I)\\Delta = -J^T r - \\lambda\\theta $$\n这就是用于求解正则化 Gauss-Newton 步长 $\\Delta$ 的线性系统。左侧的矩阵是 $\\phi(\\theta)$ 的海森矩阵的近似，右侧是 $\\phi(\\theta)$ 梯度的负值。\n\n### 在病态问题中进行正则化的理由\n\n标准的、非正则化的 Gauss-Newton 步长是通过求解正规方程组得到的，这可以通过在推导的系统中设置 $\\lambda=0$ 获得：\n$$ J^T J \\Delta = -J^T r $$\n对于许多问题，矩阵 $J^T J$ 是奇异或病态的。\n1.  **奇异性**：如果雅可比矩阵 $J$ 的列是线性相关的，那么 $J$ 是秩亏的。这意味着 $J^T J$ 是奇异的（不可逆），因为 $\\text{rank}(J^T J) = \\text{rank}(J)$。在这种情况下，求解 $\\Delta$ 的线性系统没有唯一解。这发生在具有参数不可辨识性的问题中，例如案例2，其中模型对 $\\theta_2$ 的依赖性在 $x=0$ 时消失。\n\n2.  **病态条件**：如果 $J$ 的列是近似线性相关的，那么矩阵 $J^T J$ 是病态的。它的条件数（最大特征值与最小特征值之比，$\\kappa = \\mu_{\\max}/\\mu_{\\min}$）非常大。这使得 $\\Delta$ 的解对右侧的微小扰动（例如，来自数值误差或数据中的噪声）极其敏感，导致不稳定和振荡的更新。\n\nTikhonov 正则化项 $\\lambda I$ 缓解了这些问题。需要求逆的矩阵是 $A_\\lambda = J^T J + \\lambda I$。\n- 矩阵 $J^T J$ 总是对称半正定的。它的特征值 $\\mu_j$ 都是非负的（$\\mu_j \\ge 0$）。\n- 如果 $\\lambda  0$，则矩阵 $A_\\lambda = J^T J + \\lambda I$ 的特征值为 $\\nu_j = \\mu_j + \\lambda$。因为 $\\lambda  0$，所以所有 $\\nu_j \\ge \\lambda  0$。\n- 这保证了 $A_\\lambda$ 是严格正定的，因此总是可逆的，确保了 $\\Delta$ 的唯一解存在。\n- 此外，$A_\\lambda$ 的条件数是 $\\kappa(A_\\lambda) = \\frac{\\mu_{\\max}+\\lambda}{\\mu_{\\min}+\\lambda}$。即使 $\\mu_{\\min}$ 为零或非常接近于零，分母也由 $\\lambda$ 作为下界。这防止了条件数变得无限大或过大，从而稳定了线性系统的解。正则化有效地增加了一个惩罚项，该惩罚项将解偏向于具有较小参数范数的方向，解决了病态问题中存在的模糊性。\n\n### 算法与模型详述\n\n该算法迭代求解 $\\Delta$ 并更新 $\\theta \\leftarrow \\theta + \\Delta$。\n- **模型**：$F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$，其中 $\\theta = [\\theta_1, \\theta_2]^T$。\n- **残差**：$r_i(\\theta) = F(x_i; \\theta) - y_i = \\theta_1 \\exp(\\theta_2 x_i) - y_i$。\n- **雅可比矩阵**：$J(\\theta)$ 的第 $i$ 行是 $[\\frac{\\partial r_i}{\\partial \\theta_1}, \\frac{\\partial r_i}{\\partial \\theta_2}]$。\n  $$ \\frac{\\partial r_i}{\\partial \\theta_1} = \\exp(\\theta_2 x_i) $$\n  $$ \\frac{\\partial r_i}{\\partial \\theta_2} = \\theta_1 x_i \\exp(\\theta_2 x_i) $$\n  所以，第 $i$ 行是 $[\\exp(\\theta_2 x_i), \\theta_1 x_i \\exp(\\theta_2 x_i)]$。\n- **终止条件**：当步长的欧几里得范数 $\\|\\Delta\\|_2$ 低于容差 $\\text{tol} = 10^{-10}$，或达到最大迭代次数 $50$ 次时，迭代停止。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gauss_newton(x, y, theta0, lam, tol, max_iter):\n    \"\"\"\n    Implements the regularized Gauss-Newton algorithm.\n    \"\"\"\n    theta = np.array(theta0, dtype=float)\n    p = len(theta0) # Number of parameters\n\n    for _ in range(max_iter):\n        # Unpack parameters for clarity\n        theta1, theta2 = theta\n\n        # 1. Calculate residual vector r\n        model_y = theta1 * np.exp(theta2 * x)\n        r = model_y - y\n\n        # 2. Calculate Jacobian matrix J\n        J1 = np.exp(theta2 * x)\n        J2 = theta1 * x * np.exp(theta2 * x)\n        J = np.stack((J1, J2), axis=1)\n\n        # 3. Form and solve the linear system for the step delta\n        # (J^T J + lambda*I) * delta = -(J^T r + lambda*theta)\n        JtJ = J.T @ J\n        I = np.identity(p)\n        A = JtJ + lam * I\n\n        Jtr = J.T @ r\n        b = -(Jtr + lam * theta)\n        \n        try:\n            delta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, terminate.\n            # This can happen in the unregularized case (lambda=0).\n            break\n            \n        # 4. Update the parameters\n        theta = theta + delta\n\n        # 5. Check for convergence\n        step_norm = np.linalg.norm(delta)\n        if step_norm = tol:\n            break\n\n    # After loop, calculate final residual norm\n    final_model_y = theta[0] * np.exp(theta[1] * x)\n    final_r = final_model_y - y\n    final_residual_norm = np.linalg.norm(final_r)\n    \n    return final_residual_norm\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases for the regularized Gauss-Newton algorithm.\n    \"\"\"\n    # Algorithmic parameters\n    tol = 1e-10\n    max_iter = 50\n\n    # Test Case 1: well-posed, lambda = 0\n    x1 = np.linspace(0.0, 1.0, 10)\n    theta_true1 = np.array([2.0, -0.5])\n    y1 = theta_true1[0] * np.exp(theta_true1[1] * x1)\n    theta0_1 = [1.0, 0.0]\n    lam1 = 0.0\n\n    # Test Case 2: ill-posed, non-identifiability\n    x2 = np.zeros(6)\n    theta_true2 = np.array([1.5, 0.3])\n    y2 = theta_true2[0] * np.exp(theta_true2[1] * x2) # y2 will be a constant vector\n    theta0_2 = [1.0, 1.0]\n    lam2 = 0.5\n\n    # Test Case 3: ill-conditioned, near-collinear Jacobian\n    x3 = np.arange(6, dtype=float) * 0.004\n    theta_true3 = np.array([2.0, 5.0])\n    y3 = theta_true3[0] * np.exp(theta_true3[1] * x3)\n    theta0_3 = [1.0, 0.0]\n    lam3 = 0.001\n\n    test_cases = [\n        (x1, y1, theta0_1, lam1),\n        (x2, y2, theta0_2, lam2),\n        (x3, y3, theta0_3, lam3),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        x, y, theta0, lam = case\n        result = run_gauss_newton(x, y, theta0, lam, tol, max_iter)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}