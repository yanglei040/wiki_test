{
    "hands_on_practices": [
        {
            "introduction": "The heart of the Metropolis algorithm is the acceptance probability, which governs whether the sampler accepts a proposed move. This calculation ensures that the chain properly explores the target distribution, favoring moves to higher-probability states while still allowing occasional jumps to lower-probability regions. In this exercise, you will compute this probability for a specific move, gaining a concrete understanding of this fundamental MCMC mechanism .",
            "id": "1371728",
            "problem": "A data scientist is implementing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a posterior probability distribution for a parameter $x$. The target distribution, $\\pi(x)$, is proportional to the exponential of the negative absolute value of the parameter, such that $\\pi(x) \\propto \\exp(-|x|)$.\n\nThe scientist uses the Metropolis algorithm with a symmetric proposal distribution $q(x'|x)$, where the probability of proposing a new state $x'$ given the current state $x$ is equal to the probability of proposing $x$ given $x'$ (i.e., $q(x'|x) = q(x|x')$).\n\nSuppose that at a certain step in the simulation, the current state of the chain is $x = 1.5$. The algorithm then proposes a move to a new candidate state $x' = 2.0$.\n\nCalculate the acceptance probability for this specific move. Your answer should be a dimensionless real number. Round your final answer to four significant figures.",
            "solution": "The Metropolis acceptance probability for a move from $x$ to $x'$ with a symmetric proposal $q(x'|x)=q(x|x')$ is\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\nGiven the target distribution $\\pi(x)\\propto \\exp(-|x|)$, the ratio simplifies to\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\nWith $x=1.5$ and $x'=2.0$, we have $|x|=1.5$ and $|x'|=2.0$, so\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\nTherefore,\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\nNumerically, $\\exp(-0.5)\\approx 0.6065$ when rounded to four significant figures.",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "Achieving an efficient MCMC simulation is an art, and one of its most critical aspects is tuning the proposal distribution. This exercise delves into a common but counter-intuitive diagnostic sign: an extremely high acceptance rate, which often signals poor mixing rather than success. By analyzing this scenario, you will learn why \"timid\" proposals lead to poor exploration and understand the theoretical \"sweet spot\" for acceptance rates that ensures your sampler explores the state space efficiently .",
            "id": "2408757",
            "problem": "Consider a Bayesian posterior sampling problem from computational finance in which the scalar parameter $\\theta$ represents the expected excess return of an asset in a normal–normal conjugate model. For concreteness, assume the posterior density is Gaussian with density proportional to\n$$\n\\pi(\\theta) \\propto \\exp\\!\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right),\n$$\nfor known $\\mu$ and $\\sigma^2$. You implement a random-walk Metropolis–Hastings algorithm with symmetric proposal\n$$\nq(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, \\tau^2),\n$$\nand acceptance probability\n$$\n\\alpha(\\theta, \\theta') = \\min\\!\\left\\{1,\\ \\frac{\\pi(\\theta')}{\\pi(\\theta)}\\right\\}.\n$$\nAfter a long run, you observe an average acceptance rate $\\hat{a} \\approx 0.97$ (that is, $\\hat{a} \\approx 97\\%$). Which of the following statements about the implications for state-space exploration and appropriate action are correct?\n\nA. A very high acceptance rate implies low autocorrelation and good mixing, so the current proposal scale $\\tau$ should be kept unchanged.\n\nB. A very high acceptance rate typically indicates that $\\tau$ is too small; although most proposals are accepted, the expected squared jump per iteration is very small, which leads to slow exploration (high autocorrelation). Increasing $\\tau$ to reduce acceptance toward a moderate level (for example, around $0.44$ in one dimension) can improve exploration.\n\nC. To further improve exploration and reduce finite-sample bias, one should reduce $\\tau$ even more to push the acceptance rate closer to $1$.\n\nD. For a smooth, unimodal target and a symmetric random-walk proposal, as $\\tau \\to 0$, the acceptance rate tends to $1$ but the integrated autocorrelation time tends to $\\infty$, implying poor exploration despite near-certain acceptance.\n\nE. The stationary distribution of the chain depends on $\\tau$; when the acceptance rate is very high, the chain targets a distribution more concentrated than $\\pi(\\theta)$, which explains the poor exploration.",
            "solution": "The problem statement must first be validated for scientific and logical integrity.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n-   **Model**: A Bayesian posterior sampling problem from a normal-normal conjugate model.\n-   **Parameter**: A scalar parameter $\\theta$.\n-   **Target Posterior Density**: $\\pi(\\theta) \\propto \\exp\\!\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right)$, where $\\mu$ and $\\sigma^2$ are known constants. This is the density of a Gaussian distribution, $\\mathcal{N}(\\mu, \\sigma^2)$.\n-   **Algorithm**: Random-walk Metropolis-Hastings (RWMH).\n-   **Proposal Distribution**: A symmetric proposal, $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, \\tau^2)$. The parameter $\\tau$ is the proposal scale or step size.\n-   **Acceptance Probability**: $\\alpha(\\theta, \\theta') = \\min\\!\\left\\{1,\\ \\frac{\\pi(\\theta')}{\\pi(\\theta)}\\right\\}$.\n-   **Empirical Observation**: The average acceptance rate after a long run is $\\hat{a} \\approx 0.97$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to rigorous validation.\n1.  **Scientific Grounding**: The problem describes a canonical application of the Metropolis-Hastings algorithm, a fundamental tool in computational statistics and econometrics. The model (Gaussian posterior) and proposal mechanism (Gaussian random walk) constitute a standard textbook example. The problem is scientifically sound.\n2.  **Well-Posedness**: The question asks for an interpretation of a specific, well-defined outcome ($\\hat{a} \\approx 0.97$) of the algorithm. This leads to a diagnostic analysis of the sampler's performance, which is a standard task in MCMC practice. The problem is well-posed.\n3.  **Objectivity**: The problem is stated in precise, mathematical language, free from ambiguity or subjective content.\n4.  **Completeness and Consistency**: The problem provides all necessary information to analyze the performance of the MCMC sampler. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed, scientifically grounded problem in the field of computational statistics. I will proceed with a full derivation and analysis.\n\n### Solution Derivation\nThe objective of the Metropolis-Hastings algorithm is to generate a sequence of samples $\\{\\theta_t\\}_{t=1}^N$ from a probability distribution $\\pi(\\theta)$ that is difficult to sample from directly. The algorithm constructs a Markov chain whose stationary distribution is the target distribution $\\pi(\\theta)$. For any valid proposal distribution, including the one specified with scale $\\tau > 0$, the algorithm is guaranteed to have $\\pi(\\theta)$ as its stationary distribution. The choice of $\\tau$, however, critically affects the algorithm's efficiency, i.e., the rate at which the chain explores the state space and converges to this stationary distribution.\n\nThe efficiency is determined by the trade-off between the proposal step size and the acceptance probability.\n-   If the proposal scale $\\tau$ is very small, a new proposal $\\theta' = \\theta_t + \\epsilon$ (where $\\epsilon \\sim \\mathcal{N}(0, \\tau^2)$) will be very close to the current state $\\theta_t$. Consequently, $\\pi(\\theta')$ will be very close to $\\pi(\\theta_t)$, the ratio $\\frac{\\pi(\\theta')}{\\pi(\\theta_t)}$ will be near $1$, and the acceptance probability $\\alpha(\\theta_t, \\theta')$ will be close to $1$. The chain accepts almost all proposals. However, because each step is minuscule, the chain moves very slowly. This results in a sequence of highly correlated samples and, therefore, poor exploration of the state space. This is known as slow mixing.\n-   If the proposal scale $\\tau$ is very large, a new proposal $\\theta'$ is likely to be far from the current state $\\theta_t$. If $\\theta_t$ is in a region of high probability, $\\theta'$ is likely to land in a region of much lower probability (the tails of the distribution). Thus, the ratio $\\frac{\\pi(\\theta')}{\\pi(\\theta_t)}$ will be very small, leading to a low acceptance probability. The chain will reject most proposals and remain at the same state $\\theta_t$ for many iterations. This also leads to high sample autocorrelation and poor exploration.\n\nThe observed average acceptance rate is $\\hat{a} \\approx 0.97$. This value is extremely high, indicating that the algorithm is operating in the first regime: the proposal scale $\\tau$ is too small. While the chain is constantly moving, its steps are too small to be efficient.\n\nOptimal tuning aims to maximize the sampler's efficiency, often by minimizing the autocorrelation of the samples. Theoretical research (e.g., Roberts, Gelman, and Gilks, 1997) has established optimal acceptance rates for RWMH algorithms. For a $1$-dimensional target distribution like the one in this problem, the optimal acceptance rate that maximizes exploration efficiency is approximately $0.44$. The observed rate of $0.97$ is far from this optimal value, confirming that the sampler is poorly tuned. The correct action is to increase $\\tau$ to decrease the acceptance rate towards the optimal range.\n\n### Option-by-Option Analysis\n\n**A. A very high acceptance rate implies low autocorrelation and good mixing, so the current proposal scale $\\tau$ should be kept unchanged.**\nThis statement is fundamentally flawed. A very high acceptance rate is a classic symptom of an excessively small proposal step size, which leads to small, timid moves. This results in a chain where successive states are nearly identical, causing *high* autocorrelation and *poor* mixing. The recommendation to keep $\\tau$ unchanged is therefore incorrect.\nVerdict: **Incorrect**.\n\n**B. A very high acceptance rate typically indicates that $\\tau$ is too small; although most proposals are accepted, the expected squared jump per iteration is very small, which leads to slow exploration (high autocorrelation). Increasing $\\tau$ to reduce acceptance toward a moderate level (for example, around $0.44$ in one dimension) can improve exploration.**\nThis statement is entirely correct. It accurately diagnoses the situation: an acceptance rate of $\\approx 0.97$ indicates that $\\tau$ is too small. It correctly identifies the consequence: the chain makes very small jumps, leading to slow exploration, which manifests as high autocorrelation. It correctly prescribes the remedy: increase $\\tau$. Finally, it correctly cites the theoretically optimal acceptance rate of $\\approx 0.44$ for a $1$-dimensional problem. This is the standard, textbook diagnosis and response.\nVerdict: **Correct**.\n\n**C. To further improve exploration and reduce finite-sample bias, one should reduce $\\tau$ even more to push the acceptance rate closer to $1$.**\nThis statement recommends the exact opposite of the correct action. As established, an acceptance rate approaching $1$ is a sign of inefficiency. Reducing $\\tau$ further would only exacerbate the problem, making the steps even smaller and the exploration even slower. This would increase, not decrease, the autocorrelation and the variance of any statistical estimators computed from the chain's output.\nVerdict: **Incorrect**.\n\n**D. For a smooth, unimodal target and a symmetric random-walk proposal, as $\\tau \\to 0$, the acceptance rate tends to $1$ but the integrated autocorrelation time tends to $\\infty$, implying poor exploration despite near-certain acceptance.**\nThis is a precise theoretical statement that formalizes the pathology of a very small $\\tau$. As $\\tau \\to 0$, any proposed step $\\theta' = \\theta + \\epsilon$ will be infinitesimally close to $\\theta$. For a smooth density $\\pi$, $\\pi(\\theta') \\approx \\pi(\\theta)$, so the acceptance ratio $\\frac{\\pi(\\theta')}{\\pi(\\theta)} \\to 1$, and thus the acceptance rate goes to $1$. However, the chain is now a discrete approximation of a random walk. To explore a region of a fixed size, the number of steps required scales as $1/\\tau^2$. The integrated autocorrelation time (IACT), which measures the number of correlated samples equivalent to one independent sample, can be shown to scale as $\\text{IACT} \\propto 1/\\tau^2$. Therefore, as $\\tau \\to 0$, the IACT $\\to \\infty$. This signifies a complete breakdown of efficient exploration. This statement is a cornerstone of the theory of optimal scaling for MCMC.\nVerdict: **Correct**.\n\n**E. The stationary distribution of the chain depends on $\\tau$; when the acceptance rate is very high, the chain targets a distribution more concentrated than $\\pi(\\theta)$, which explains the poor exploration.**\nThis statement contains a grave conceptual error. The Metropolis-Hastings algorithm is constructed to satisfy the detailed balance condition, $\\pi(\\theta) K(\\theta'|\\theta) = \\pi(\\theta') K(\\theta|\\theta')$, where $K$ is the transition kernel. This condition guarantees that the stationary distribution of the Markov chain is *exactly* the target distribution $\\pi(\\theta)$. This property holds for any valid choice of proposal scale $\\tau > 0$. The parameter $\\tau$ influences the *rate of convergence* to the stationary distribution and the *efficiency* of the sampler, but it does *not* alter the target distribution itself. The poor exploration is due to slow dynamics, not to targeting the wrong distribution.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "It's time to bridge theory and practice by building a complete MCMC sampler for a cornerstone problem in statistics: Bayesian linear regression. This hands-on coding challenge guides you from the first principles of Bayes' theorem to a working Metropolis-Hastings implementation. By completing this task, you will synthesize your knowledge of posterior distributions, proposal mechanisms, and chain analysis to estimate model parameters from data, solidifying your understanding in a practical context .",
            "id": "3250349",
            "problem": "You are given the linear observational model specified by the independent and identically distributed errors assumption: for each index $i$ in a dataset, the scalar response $y_i$ is generated by\n$$\ny_i \\;=\\; m\\,x_i \\;+\\; b \\;+\\; \\epsilon_i,\n$$\nwhere the error term $\\epsilon_i$ is distributed as a zero-mean Gaussian random variable with variance $\\sigma^2$, that is $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ and the errors are independent across $i$. The parameters $m$ and $b$ are unknown. Assume independent Gaussian priors $m \\sim \\mathcal{N}(0,\\tau_m^2)$ and $b \\sim \\mathcal{N}(0,\\tau_b^2)$ with known prior standard deviations $\\tau_m$ and $\\tau_b$. The variance $\\sigma^2$ is known.\n\nStarting from Bayes' theorem and the definition of the Gaussian likelihood and Gaussian prior densities, derive an implementable expression for the unnormalized logarithm of the posterior density of $(m,b)$ given observed data $(x_i,y_i)$ and known hyperparameters $(\\sigma,\\tau_m,\\tau_b)$. Then, implement a Metropolis–Hastings Markov chain Monte Carlo (MCMC) algorithm with a symmetric Gaussian random-walk proposal for the two parameters:\n- Propose $m^\\star = m + \\eta_m$ with $\\eta_m \\sim \\mathcal{N}(0,s_m^2)$.\n- Propose $b^\\star = b + \\eta_b$ with $\\eta_b \\sim \\mathcal{N}(0,s_b^2)$.\n- Accept or reject $(m^\\star,b^\\star)$ according to the Metropolis–Hastings rule based on the ratio of unnormalized posterior densities.\n\nUse these foundations to approximate the posterior distribution of $(m,b)$ for the following test suite. In each case, the observed responses $y_i$ must be generated synthetically inside your program using the provided data-generation seeds and the stated ground-truth parameters and noise level. Then, using a separate seed for the Markov chain, run the MCMC sampler and report the posterior means of $m$ and $b$ after discarding the burn-in samples. No thinning is required.\n\nAssume the starting state $(m_0,b_0)$ is $(0,0)$ for all cases. Use the same prior standard deviations $\\tau_m=\\tau_b$ as specified below for all cases.\n\nTest suite:\n- Case $1$ (happy path):\n  - Design points: $x$ are $n=20$ equally spaced values from $-2.0$ to $2.0$ inclusive.\n  - Ground truth: $m_{\\text{true}}=2.0$, $b_{\\text{true}}=-1.0$.\n  - Noise standard deviation: $\\sigma=0.5$.\n  - Prior standard deviations: $\\tau_m=\\tau_b=10.0$.\n  - Proposal standard deviations: $s_m=0.02$, $s_b=0.05$.\n  - Data-generation seed: $11$.\n  - MCMC seed: $101$.\n  - Total iterations: $20000$.\n  - Burn-in: $5000$.\n- Case $2$ (boundary-size dataset with two points and low noise):\n  - Design points: $x=[-1.0,\\,1.0]$ with $n=2$.\n  - Ground truth: $m_{\\text{true}}=-0.5$, $b_{\\text{true}}=2.0$.\n  - Noise standard deviation: $\\sigma=0.1$.\n  - Prior standard deviations: $\\tau_m=\\tau_b=10.0$.\n  - Proposal standard deviations: $s_m=0.005$, $s_b=0.01$.\n  - Data-generation seed: $22$.\n  - MCMC seed: $202$.\n  - Total iterations: $30000$.\n  - Burn-in: $10000$.\n- Case $3$ (noisy observations):\n  - Design points: $x$ are $n=30$ equally spaced values from $0.0$ to $5.0$ inclusive.\n  - Ground truth: $m_{\\text{true}}=0.3$, $b_{\\text{true}}=4.0$.\n  - Noise standard deviation: $\\sigma=5.0$.\n  - Prior standard deviations: $\\tau_m=\\tau_b=10.0$.\n  - Proposal standard deviations: $s_m=0.05$, $s_b=0.20$.\n  - Data-generation seed: $33$.\n  - MCMC seed: $303$.\n  - Total iterations: $25000$.\n  - Burn-in: $8000$.\n\nImplementation and output requirements:\n- Implement the Metropolis–Hastings sampler exactly as described, using the unnormalized logarithm of the posterior density you derived.\n- For each case, compute the posterior mean of $m$ and the posterior mean of $b$ using only the samples after burn-in.\n- Your program must generate the synthetic $y$ values using the stated ground-truth parameters and noise levels with the given data-generation seeds, and must use the provided MCMC seeds for sampling.\n- Round each posterior mean to exactly $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[m^{(1)}_{\\text{mean}},b^{(1)}_{\\text{mean}},m^{(2)}_{\\text{mean}},b^{(2)}_{\\text{mean}},m^{(3)}_{\\text{mean}},b^{(3)}_{\\text{mean}}]$, where the superscript indicates the case number. For example, the output format must look like a single line with square brackets and comma-separated decimal numbers.\n\nNotes:\n- All angles, if any were present, would be in radians; no angles are used here.\n- There are no physical units required for the outputs.\n- Ensure that all computations are deterministic by using the specified seeds for pseudorandom number generation.",
            "solution": "The problem requires the implementation of a Metropolis-Hastings Markov chain Monte Carlo (MCMC) algorithm to sample from the posterior distribution of the parameters $(m,b)$ of a simple linear regression model. The solution requires two main components: the derivation of the target probability density function and the implementation of the MCMC sampler.\n\n### Derivation of the Unnormalized Log-Posterior Density\n\nThe problem is set within a Bayesian framework. According to Bayes' theorem, the posterior probability of the parameters $(m,b)$ given the observed data $D = \\{(x_i, y_i)\\}_{i=1}^n$ and known hyperparameters $H = (\\sigma, \\tau_m, \\tau_b)$ is given by:\n$$\np(m, b \\mid D, H) = \\frac{p(D \\mid m, b, \\sigma) p(m \\mid \\tau_m) p(b \\mid \\tau_b)}{p(D \\mid H)}\n$$\nwhere $p(D \\mid m, b, \\sigma)$ is the likelihood of the data, $p(m \\mid \\tau_m)$ and $p(b \\mid \\tau_b)$ are the prior probabilities of the parameters, and $p(D \\mid H)$ is the marginal likelihood or evidence, which acts as a normalization constant. For MCMC methods like Metropolis-Hastings, the normalization constant is not required. We can work with the unnormalized posterior, which is proportional to the numerator:\n$$\np(m, b \\mid D, H) \\propto p(D \\mid m, b, \\sigma) p(m \\mid \\tau_m) p(b \\mid \\tau_b)\n$$\n\n**1. The Likelihood Function**\nThe linear observational model is $y_i = m x_i + b + \\epsilon_i$, with the error term $\\epsilon_i$ drawn from a zero-mean normal distribution with variance $\\sigma^2$, i.e., $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that each response variable $y_i$ is also normally distributed:\n$$\ny_i \\mid m, b, x_i, \\sigma \\sim \\mathcal{N}(m x_i + b, \\sigma^2)\n$$\nThe probability density function (PDF) for a single observation $(x_i, y_i)$ is:\n$$\np(y_i \\mid m, b, x_i, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - (m x_i + b))^2}{2\\sigma^2}\\right)\n$$\nAssuming the errors $\\epsilon_i$ are independent and identically distributed (i.i.d.), the likelihood of the entire dataset $D$ is the product of the individual probabilities:\n$$\np(D \\mid m, b, \\sigma) = \\prod_{i=1}^n p(y_i \\mid m, b, x_i, \\sigma)\n$$\n\n**2. The Prior Distributions**\nThe problem specifies independent Gaussian priors for the parameters $m$ and $b$:\n$$\nm \\sim \\mathcal{N}(0, \\tau_m^2) \\implies p(m \\mid \\tau_m) = \\frac{1}{\\sqrt{2\\pi\\tau_m^2}} \\exp\\left(-\\frac{m^2}{2\\tau_m^2}\\right)\n$$\n$$\nb \\sim \\mathcal{N}(0, \\tau_b^2) \\implies p(b \\mid \\tau_b) = \\frac{1}{\\sqrt{2\\pi\\tau_b^2}} \\exp\\left(-\\frac{b^2}{2\\tau_b^2}\\right)\n$$\nThe joint prior is the product of the individual priors due to their independence: $p(m, b \\mid \\tau_m, \\tau_b) = p(m \\mid \\tau_m) p(b \\mid \\tau_b)$.\n\n**3. The Unnormalized Log-Posterior**\nFor numerical stability and computational convenience, it is standard practice to work with the logarithm of the posterior density. The logarithm of the unnormalized posterior, which we denote as $\\mathcal{L}_{un}(m,b)$, is:\n$$\n\\mathcal{L}_{un}(m,b) = \\log\\left[ p(D \\mid m, b, \\sigma) p(m \\mid \\tau_m) p(b \\mid \\tau_b) \\right] = \\log p(D \\mid m, b, \\sigma) + \\log p(m \\mid \\tau_m) + \\log p(b \\mid \\tau_b)\n$$\nLet's expand each term, dropping any additive constants that do not depend on $m$ or $b$:\nThe log-likelihood term is:\n$$\n\\log p(D \\mid m, b, \\sigma) = \\sum_{i=1}^n \\log p(y_i \\mid m, b, x_i, \\sigma) = \\sum_{i=1}^n \\left( \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\frac{(y_i - (m x_i + b))^2}{2\\sigma^2} \\right)\n$$\nIgnoring the constant term $\\sum_{i=1}^n \\log(1/\\sqrt{2\\pi\\sigma^2})$, we get:\n$$\n\\log p(D \\mid m, b, \\sigma) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - m x_i - b)^2\n$$\nThe log-prior terms are:\n$$\n\\log p(m \\mid \\tau_m) \\propto -\\frac{m^2}{2\\tau_m^2} \\quad \\text{and} \\quad \\log p(b \\mid \\tau_b) \\propto -\\frac{b^2}{2\\tau_b^2}\n$$\nCombining these terms gives the final expression for the unnormalized log-posterior density:\n$$\n\\mathcal{L}_{un}(m,b) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - m x_i - b)^2 - \\frac{m^2}{2\\tau_m^2} - \\frac{b^2}{2\\tau_b^2}\n$$\nThis expression is the function we need to evaluate within the MCMC algorithm. The first term is proportional to the negative sum of squared residuals, and the second and third terms are regularization terms (L2 regularization) originating from the Gaussian priors.\n\n### Metropolis-Hastings Algorithm\n\nThe Metropolis-Hastings algorithm is used to generate a sequence of samples that constitutes a Markov chain, whose stationary distribution is the desired posterior distribution $p(m, b \\mid D, H)$. The algorithm proceeds as follows:\n\n1.  **Initialization**: Start the chain at an initial state $(m_0, b_0)$. The problem specifies $(m_0, b_0) = (0,0)$.\n\n2.  **Iteration**: For each step $t=1, 2, \\dots, N_{\\text{total}}$:\n    a.  **Proposal**: Given the current state $(m_t, b_t)$, propose a new state $(m^\\star, b^\\star)$ from a proposal distribution $Q((m^\\star, b^\\star) \\mid (m_t, b_t))$. The problem specifies a symmetric Gaussian random-walk proposal:\n        $$\n        m^\\star = m_t + \\eta_m, \\quad \\text{with} \\quad \\eta_m \\sim \\mathcal{N}(0, s_m^2)\n        $$\n        $$\n        b^\\star = b_t + \\eta_b, \\quad \\text{with} \\quad \\eta_b \\sim \\mathcal{N}(0, s_b^2)\n        $$\n    b.  **Acceptance Probability**: Calculate the acceptance probability $\\alpha$, which is the probability of moving to the proposed state $(m^\\star, b^\\star)$:\n        $$\n        \\alpha = \\min\\left(1, \\frac{p(m^\\star, b^\\star \\mid D, H)}{p(m_t, b_t \\mid D, H)} \\cdot \\frac{Q((m_t, b_t) \\mid (m^\\star, b^\\star))}{Q((m^\\star, b^\\star) \\mid (m_t, b_t))}\\right)\n        $$\n        Since the proposal distribution is symmetric (i.e., the probability of proposing a move from state A to B is the same as from B to A), the ratio of proposal densities (the Hastings ratio) is $1$. The acceptance probability simplifies to the Metropolis rule:\n        $$\n        \\alpha = \\min\\left(1, \\frac{p(m^\\star, b^\\star \\mid D, H)}{p(m_t, b_t \\mid D, H)}\\right)\n        $$\n        To avoid numerical underflow, this is computed using log-probabilities:\n        $$\n        \\log(\\text{ratio}) = \\mathcal{L}_{un}(m^\\star, b^\\star) - \\mathcal{L}_{un}(m_t, b_t)\n        $$\n        $$\n        \\alpha = \\min(1, \\exp(\\log(\\text{ratio})))\n        $$\n    c.  **Accept or Reject**: Generate a random number $u$ from a uniform distribution $U(0,1)$. If $u  \\alpha$, the proposal is accepted, and the next state is $(m_{t+1}, b_{t+1}) = (m^\\star, b^\\star)$. Otherwise, the proposal is rejected, and the chain remains at its current state, $(m_{t+1}, b_{t+1}) = (m_t, b_t)$.\n\n3.  **Analysis**: After running the chain for $N_{\\text{total}}$ iterations, the initial portion of the chain (the burn-in period, $N_{\\text{burn-in}}$) is discarded to allow the chain to converge to its stationary distribution. The posterior means of $m$ and $b$ are then estimated by computing the arithmetic mean of the post-burn-in samples:\n    $$\n    \\hat{m}_{\\text{mean}} = \\frac{1}{N_{\\text{total}} - N_{\\text{burn-in}}} \\sum_{t=N_{\\text{burn-in}}+1}^{N_{\\text{total}}} m_t\n    $$\n    $$\n    \\hat{b}_{\\text{mean}} = \\frac{1}{N_{\\text{total}} - N_{\\text{burn-in}}} \\sum_{t=N_{\\text{burn-in}}+1}^{N_{\\text{total}}} b_t\n    $$\nThis procedure will be applied to each of the three test cases specified in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Metropolis-Hastings MCMC sampler for Bayesian linear regression\n    and applies it to three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"n\": 20, \"x_range\": [-2.0, 2.0],\n            \"m_true\": 2.0, \"b_true\": -1.0, \"sigma\": 0.5,\n            \"tau_m\": 10.0, \"tau_b\": 10.0,\n            \"s_m\": 0.02, \"s_b\": 0.05,\n            \"data_seed\": 11, \"mcmc_seed\": 101,\n            \"total_iterations\": 20000, \"burn_in\": 5000\n        },\n        # Case 2 (boundary-size dataset with two points and low noise)\n        {\n            \"n\": 2, \"x_manual\": np.array([-1.0, 1.0]),\n            \"m_true\": -0.5, \"b_true\": 2.0, \"sigma\": 0.1,\n            \"tau_m\": 10.0, \"tau_b\": 10.0,\n            \"s_m\": 0.005, \"s_b\": 0.01,\n            \"data_seed\": 22, \"mcmc_seed\": 202,\n            \"total_iterations\": 30000, \"burn_in\": 10000\n        },\n        # Case 3 (noisy observations)\n        {\n            \"n\": 30, \"x_range\": [0.0, 5.0],\n            \"m_true\": 0.3, \"b_true\": 4.0, \"sigma\": 5.0,\n            \"tau_m\": 10.0, \"tau_b\": 10.0,\n            \"s_m\": 0.05, \"s_b\": 0.20,\n            \"data_seed\": 33, \"mcmc_seed\": 303,\n            \"total_iterations\": 25000, \"burn_in\": 8000\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Generate synthetic data\n        rng_data = np.random.default_rng(case[\"data_seed\"])\n        \n        if \"x_manual\" in case:\n            x = case[\"x_manual\"]\n        else:\n            x = np.linspace(case[\"x_range\"][0], case[\"x_range\"][1], case[\"n\"])\n            \n        noise = rng_data.normal(0, case[\"sigma\"], size=case[\"n\"])\n        y = case[\"m_true\"] * x + case[\"b_true\"] + noise\n\n        # Define the unnormalized log posterior function\n        def log_unnormalized_posterior(m, b, x, y, sigma, tau_m, tau_b):\n            residuals = y - (m * x + b)\n            sum_sq_residuals = np.sum(residuals**2)\n            log_likelihood = -0.5 * sum_sq_residuals / (sigma**2)\n            log_prior = -0.5 * (m**2 / tau_m**2 + b**2 / tau_b**2)\n            return log_likelihood + log_prior\n\n        # Initialize MCMC\n        rng_mcmc = np.random.default_rng(case[\"mcmc_seed\"])\n        m_curr, b_curr = 0.0, 0.0\n        \n        m_samples = np.zeros(case[\"total_iterations\"])\n        b_samples = np.zeros(case[\"total_iterations\"])\n        \n        log_post_curr = log_unnormalized_posterior(\n            m_curr, b_curr, x, y, case[\"sigma\"], case[\"tau_m\"], case[\"tau_b\"])\n\n        # Run MCMC sampler\n        for i in range(case[\"total_iterations\"]):\n            # Propose new parameters\n            m_prop = m_curr + rng_mcmc.normal(0, case[\"s_m\"])\n            b_prop = b_curr + rng_mcmc.normal(0, case[\"s_b\"])\n            \n            # Calculate log posterior of proposed parameters\n            log_post_prop = log_unnormalized_posterior(\n                m_prop, b_prop, x, y, case[\"sigma\"], case[\"tau_m\"], case[\"tau_b\"])\n            \n            # Calculate acceptance ratio\n            log_ratio = log_post_prop - log_post_curr\n            alpha = np.min([1.0, np.exp(log_ratio)])\n            \n            # Accept or reject\n            if rng_mcmc.uniform(0, 1)  alpha:\n                m_curr = m_prop\n                b_curr = b_prop\n                log_post_curr = log_post_prop\n            \n            m_samples[i] = m_curr\n            b_samples[i] = b_curr\n            \n        # Post-processing: discard burn-in and compute posterior means\n        post_burn_in_m = m_samples[case[\"burn_in\"]:]\n        post_burn_in_b = b_samples[case[\"burn_in\"]:]\n        \n        m_mean = np.mean(post_burn_in_m)\n        b_mean = np.mean(post_burn_in_b)\n        \n        results.extend([m_mean, b_mean])\n\n    # Format and print the final output\n    formatted_results = \",\".join([f\"{r:.4f}\" for r in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        }
    ]
}