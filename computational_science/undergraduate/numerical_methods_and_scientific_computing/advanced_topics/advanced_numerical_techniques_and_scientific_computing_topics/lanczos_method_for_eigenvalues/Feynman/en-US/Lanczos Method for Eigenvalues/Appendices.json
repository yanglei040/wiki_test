{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Lanczos method, it's essential to first grasp its core mechanics. This practice invites you to manually execute the first few steps of the Lanczos recurrence for a simple rank-one matrix, $A = vv^T$ . By calculating the tridiagonal matrix entries $\\alpha_j$ and $\\beta_j$ symbolically, you will build a foundational intuition for how the algorithm constructs the Krylov subspace and its corresponding tridiagonal representation.",
            "id": "2184040",
            "problem": "The Lanczos algorithm is a numerical method for finding eigenvalues and eigenvectors of a large symmetric matrix. It proceeds by constructing an orthonormal basis for a Krylov subspace and, in doing so, produces a symmetric tridiagonal matrix whose eigenvalues approximate those of the original matrix.\n\nConsider a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ defined as a rank-one matrix $A = vv^T$, where $v \\in \\mathbb{R}^n$ is a column vector with unit norm, i.e., $v^T v = 1$.\n\nThe Lanczos algorithm is initiated with a starting vector $q_1$, which is also a unit vector ($q_1^T q_1 = 1$). It is specified that the starting vector $q_1$ is not orthogonal to $v$. Let the cosine of the angle between these two vectors be denoted by $c = v^T q_1$. You are given that $0 < c < 1$.\n\nThe algorithm iteratively generates a sequence of orthonormal vectors $q_1, q_2, \\dots$ and the entries of a symmetric tridiagonal matrix $T_k$. The procedure for two steps is as follows:\n1.  Compute $w_1 = A q_1$.\n2.  Compute $\\alpha_1 = q_1^T w_1$.\n3.  Compute $w'_2 = w_1 - \\alpha_1 q_1$.\n4.  Compute $\\beta_2 = \\|w'_2\\|_2$. If $\\beta_2=0$, the algorithm terminates. Otherwise, compute $q_2 = w'_2 / \\beta_2$.\n5.  Compute $w_2 = A q_2$.\n6.  Compute $\\alpha_2 = q_2^T w_2$.\n\nYour task is to determine the $2 \\times 2$ symmetric tridiagonal matrix $T_2$ generated after two full iterations of the algorithm.\n$$\nT_2 = \\begin{pmatrix}\n\\alpha_1 & \\beta_2 \\\\\n\\beta_2 & \\alpha_2\n\\end{pmatrix}\n$$\nExpress your final answer as a matrix whose entries are functions of $c$.",
            "solution": "We have $A = v v^{T}$ with $v^{T} v = 1$ and a unit starting vector $q_{1}$ such that $c = v^{T} q_{1} \\in (0,1)$. Apply the Lanczos steps as given.\n\nCompute\n$$\nw_{1} = A q_{1} = v(v^{T} q_{1}) = c v.\n$$\nThen\n$$\n\\alpha_{1} = q_{1}^{T} w_{1} = q_{1}^{T} (c v) = c (q_{1}^{T} v) = c^{2}.\n$$\nForm\n$$\nw'_{2} = w_{1} - \\alpha_{1} q_{1} = c v - c^{2} q_{1}.\n$$\nDecompose $v$ into components parallel and orthogonal to $q_{1}$:\n$$\nv = c q_{1} + r, \\quad r = v - c q_{1}, \\quad q_{1}^{T} r = 0, \\quad \\|r\\|_2 = \\sqrt{1 - c^{2}}.\n$$\nLet $u = r / \\|r\\|_2$ so that $u$ is unit and orthogonal to $q_{1}$. Define $s = \\sqrt{1 - c^{2}}$, so $v = c q_{1} + s u$. Then\n$$\nw'_{2} = c(c q_{1} + s u) - c^{2} q_{1} = c s u,\n$$\nhence\n$$\n\\beta_{2} = \\|w'_{2}\\|_2 = c s = c \\sqrt{1 - c^{2}},\n$$\nand\n$$\nq_{2} = \\frac{w'_{2}}{\\beta_{2}} = u.\n$$\nNext,\n$$\nw_{2} = A q_{2} = v(v^{T} q_{2}) = v\\big((c q_{1} + s u)^{T} u\\big) = v s,\n$$\nso\n$$\n\\alpha_{2} = q_{2}^{T} w_{2} = u^{T} (s v) = s\\, u^{T} v = s(c q_{1}^{T} u + s u^{T} u) = s^{2} = 1 - c^{2}.\n$$\nTherefore, the tridiagonal matrix after two iterations is\n$$\nT_{2} = \\begin{pmatrix}\n\\alpha_{1} & \\beta_{2} \\\\\n\\beta_{2} & \\alpha_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nc^{2} & c \\sqrt{1 - c^{2}} \\\\\nc \\sqrt{1 - c^{2}} & 1 - c^{2}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}c^{2} & c\\sqrt{1-c^{2}} \\\\ c\\sqrt{1-c^{2}} & 1-c^{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "The effectiveness of the Lanczos method is critically dependent on the choice of the initial vector. This hands-on coding problem explores this relationship by investigating how the composition of the starting vector determines which eigenvalues can be found . You will implement the algorithm and observe firsthand why an eigenvalue is missed if the starting vector is orthogonal to its eigenvector, and how quickly it's found otherwise, reinforcing the concept of the spectral measure.",
            "id": "3246991",
            "problem": "Consider a real symmetric matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ with eigen-decomposition $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^\\top$, where $\\mathbf{U}$ is orthogonal with columns $\\mathbf{u}_j$ forming an orthonormal basis of eigenvectors and $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ contains the real eigenvalues. For any unit vector $\\mathbf{v} \\in \\mathbb{R}^n$, the spectral measure induced by $\\mathbf{v}$ places mass $w_j = |\\mathbf{u}_j^\\top \\mathbf{v}|^2$ at eigenvalue $\\lambda_j$. The Krylov subspace of order $k$ generated by $(\\mathbf{A},\\mathbf{v})$ is $\\mathcal{K}_k(\\mathbf{A},\\mathbf{v}) = \\mathrm{span}\\{\\mathbf{v},\\mathbf{A}\\mathbf{v},\\dots,\\mathbf{A}^{k-1}\\mathbf{v}\\}$. The Lanczos method constructs an orthonormal basis $\\{\\mathbf{q}_1,\\dots,\\mathbf{q}_m\\}$ for $\\mathcal{K}_m(\\mathbf{A},\\mathbf{v})$ via a three-term recurrence so that $\\mathbf{A}$ is represented in this basis by a symmetric tridiagonal matrix $\\mathbf{T}_m$ with diagonal entries $\\alpha_j$ and off-diagonal entries $\\beta_j$; the eigenvalues of $\\mathbf{T}_m$ are the Rayleigh-Ritz (RR) approximations of the eigenvalues of $\\mathbf{A}$ restricted to $\\mathcal{K}_m(\\mathbf{A},\\mathbf{v})$. If $w_j = 0$ for some $j$, then $\\mathbf{u}_j$ is orthogonal to $\\mathcal{K}_k(\\mathbf{A},\\mathbf{v})$ for all $k$, and $\\lambda_j$ cannot appear among the RR values from Lanczos started at $\\mathbf{v}$. Conversely, if $\\mathbf{v}$ equals an eigenvector $\\mathbf{u}_j$, then Lanczos terminates immediately with $\\mathbf{T}_1 = [\\lambda_j]$, yielding exact capture of $\\lambda_j$ in one step.\n\nYour task is to implement the Lanczos method to demonstrate these phenomena concretely on a small, fully specified example. Work with the matrix\n$$\n\\mathbf{A} = \\mathrm{diag}(1,2,3,4) \\in \\mathbb{R}^{4\\times 4},\n$$\nwhose orthonormal eigenvectors are the standard basis $\\{\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3,\\mathbf{e}_4\\}$ and whose eigenvalues are $\\lambda_1=1$, $\\lambda_2=2$, $\\lambda_3=3$, $\\lambda_4=4$. Implement the classical Lanczos three-term recurrence with a unit starting vector $\\mathbf{v}_1$, building the tridiagonal matrix $\\mathbf{T}_m$ until breakdown at the first index $m$ where $\\beta_m \\le \\varepsilon$, with $\\varepsilon = 10^{-12}$, and then compute the RR values as the eigenvalues of $\\mathbf{T}_m$. Use the membership tolerance $\\delta = 10^{-10}$ to test whether a specific eigenvalue appears among the RR values.\n\nFrom first principles, you must justify and test the following cases:\n- Case $1$ (boundary condition: zero spectral mass). Let $\\mathbf{v}_1^{(\\mathrm{no3})} = \\frac{1}{\\sqrt{3}}(1,1,0,1)^\\top$, so the spectral mass on $\\lambda_3$ is $w_3 = |\\mathbf{e}_3^\\top \\mathbf{v}_1^{(\\mathrm{no3})}|^2 = 0$. Run Lanczos until breakdown and check whether the RR values of $\\mathbf{T}_m$ contain $\\lambda_3$ within tolerance $\\delta$. The expected phenomenon is that $\\lambda_3$ does not appear because $\\mathcal{K}_k(\\mathbf{A},\\mathbf{v}_1^{(\\mathrm{no3})})$ is contained in $\\mathrm{span}\\{\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_4\\}$.\n- Case $2$ (edge case: immediate capture). Let $\\mathbf{v}_1^{(\\mathrm{exact3})} = \\mathbf{e}_3$. Run Lanczos and report the number of steps $m$ until breakdown. The expected phenomenon is $m=1$ and the single RR value equals $\\lambda_3$ exactly.\n- Case $3$ (happy path: nonzero spectral mass). Let $\\mathbf{v}_1^{(\\mathrm{all})} = \\frac{1}{2}(1,1,1,1)^\\top$, so $w_3 = \\left|\\mathbf{e}_3^\\top \\mathbf{v}_1^{(\\mathrm{all})}\\right|^2 = \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{4}$. Run Lanczos until breakdown and check whether the RR values of $\\mathbf{T}_m$ contain $\\lambda_3$ within tolerance $\\delta$. The expected phenomenon is that $\\lambda_3$ appears among the RR values once the Krylov subspace spans the full invariant subspace.\n\nYour implementation must adhere to the following specifications:\n- Construct $\\mathbf{T}_m$ by recording the scalars $\\alpha_j = \\mathbf{q}_j^\\top \\mathbf{A}\\mathbf{q}_j$ and $\\beta_j = \\lVert \\mathbf{w}_j \\rVert_2$, where $\\mathbf{w}_j = \\mathbf{A}\\mathbf{q}_j - \\alpha_j \\mathbf{q}_j - \\beta_{j-1}\\mathbf{q}_{j-1}$ with $\\beta_0 = 0$ and $\\mathbf{q}_0 = \\mathbf{0}$, and $\\mathbf{q}_{j+1} = \\mathbf{w}_j / \\beta_j$ whenever $\\beta_j > \\varepsilon$. Stop when $\\beta_j \\le \\varepsilon$.\n- Compute the RR values as the eigenvalues of $\\mathbf{T}_m$.\n- Use the specified tolerances $\\varepsilon = 10^{-12}$ and $\\delta = 10^{-10}$.\n\nTest Suite and Final Output Specification:\n- Apply the method to the three cases above. For Case $1$, your program must return a boolean indicating whether $\\lambda_3$ appears among the RR values (expected to be false). For Case $2$, your program must return an integer equal to the number of Lanczos steps $m$ until breakdown (expected to be $1$). For Case $3$, your program must return a boolean indicating whether $\\lambda_3$ appears among the RR values (expected to be true).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[\\text{Case 1 result}, \\text{Case 2 result}, \\text{Case 3 result}]$, for example, $[b_1,i_2,b_3]$, where $b_1$ and $b_3$ are booleans and $i_2$ is an integer.\n\nAll quantities in this problem are dimensionless, and there are no physical or angle units involved.",
            "solution": "The problem requires the implementation of the Lanczos three-term recurrence to find Rayleigh-Ritz approximations of the eigenvalues of a given real symmetric matrix, $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$. We will use this implementation to concretely demonstrate three fundamental properties of the Lanczos method related to the spectral measure of the starting vector. The matrix is specified as $\\mathbf{A} = \\mathrm{diag}(1,2,3,4)$, whose eigenvalues are $\\lambda_j = j$ for $j=1,2,3,4$, and whose corresponding orthonormal eigenvectors are the standard basis vectors $\\{\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3, \\mathbf{e}_4\\}$.\n\n### The Lanczos Algorithm\n\nThe Lanczos algorithm is an iterative method that generates an orthonormal basis $\\{\\mathbf{q}_1, \\dots, \\mathbf{q}_m\\}$ for the order-$m$ Krylov subspace $\\mathcal{K}_m(\\mathbf{A}, \\mathbf{v}_1) = \\mathrm{span}\\{\\mathbf{v}_1, \\mathbf{A}\\mathbf{v}_1, \\dots, \\mathbf{A}^{m-1}\\mathbf{v}_1\\}$, where $\\mathbf{v}_1$ is a given unit starting vector. In this basis, the action of $\\mathbf{A}$ on the subspace $\\mathcal{K}_m$ is represented by a symmetric tridiagonal matrix $\\mathbf{T}_m$.\n\nThe algorithm proceeds via the following three-term recurrence, starting with $\\mathbf{q}_1 = \\mathbf{v}_1$, $\\beta_0=0$, and $\\mathbf{q}_0 = \\mathbf{0}$:\nFor $j = 1, 2, \\dots, m$:\n1.  Compute the matrix-vector product: $\\mathbf{z}_j = \\mathbf{A} \\mathbf{q}_j$.\n2.  Compute the diagonal element of $\\mathbf{T}_m$: $\\alpha_j = \\mathbf{q}_j^\\top \\mathbf{z}_j$.\n3.  Compute the unnormalized next Lanczos vector: $\\mathbf{w}_j = \\mathbf{z}_j - \\alpha_j \\mathbf{q}_j - \\beta_{j-1} \\mathbf{q}_{j-1}$.\n4.  Compute the off-diagonal element of $\\mathbf{T}_m$: $\\beta_j = \\lVert \\mathbf{w}_j \\rVert_2$.\n5.  If $\\beta_j$ is smaller than a given tolerance $\\varepsilon$, the algorithm is said to have a (happy) breakdown. This indicates that the Krylov subspace is invariant under $\\mathbf{A}$. We set $m=j$ and terminate. The dimension of the generated subspace is $m$.\n6.  If $\\beta_j > \\varepsilon$, normalize to find the next Lanczos vector: $\\mathbf{q}_{j+1} = \\mathbf{w}_j / \\beta_j$.\n\nAfter $m$ steps, this process yields the symmetric tridiagonal matrix:\n$$\n\\mathbf{T}_m = \\begin{pmatrix}\n\\alpha_1 & \\beta_1 & & & \\\\\n\\beta_1 & \\alpha_2 & \\beta_2 & & \\\\\n& \\beta_2 & \\ddots & \\ddots & \\\\\n& & \\ddots & \\alpha_{m-1} & \\beta_{m-1} \\\\\n& & & \\beta_{m-1} & \\alpha_m\n\\end{pmatrix} \\in \\mathbb{R}^{m \\times m}\n$$\nThe eigenvalues of $\\mathbf{T}_m$, known as Rayleigh-Ritz (RR) values, are approximations to the eigenvalues of $\\mathbf{A}$.\n\n### Analysis of Test Cases\n\nWe will now apply this algorithm to the matrix $\\mathbf{A} = \\mathrm{diag}(1,2,3,4)$ with a breakdown tolerance of $\\varepsilon = 10^{-12}$ and an eigenvalue membership tolerance of $\\delta = 10^{-10}$.\n\n**Case 1: Zero Spectral Mass**\n\nThe starting vector is $\\mathbf{v}_1^{(\\mathrm{no3})} = \\frac{1}{\\sqrt{3}}(1,1,0,1)^\\top$. The spectral measure induced by this vector places mass $w_j = |\\mathbf{e}_j^\\top \\mathbf{v}_1^{(\\mathrm{no3})}|^2$ at eigenvalue $\\lambda_j$. For $\\lambda_3=3$, the mass is $w_3 = |\\mathbf{e}_3^\\top \\mathbf{v}_1^{(\\mathrm{no3})}|^2 = |0|^2 = 0$.\n\nFrom first principles, since the starting vector $\\mathbf{v}_1^{(\\mathrm{no3})}$ is a linear combination of only $\\{\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_4\\}$, it lies in the subspace $S = \\mathrm{span}\\{\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_4\\}$. Because $\\mathbf{A}$ is diagonal, this subspace is an invariant subspace of $\\mathbf{A}$, meaning that for any vector $\\mathbf{x} \\in S$, $\\mathbf{A}\\mathbf{x}$ is also in $S$. Consequently, all vectors in the Krylov sequence, $\\mathbf{A}^k \\mathbf{v}_1^{(\\mathrm{no3})}$, remain within $S$. The Lanczos procedure, which performs linear combinations of these vectors, will generate an orthonormal basis $\\{\\mathbf{q}_j\\}$ that also lies entirely within $S$. The resulting tridiagonal matrix $\\mathbf{T}_m$ is the representation of $\\mathbf{A}$ restricted to the Krylov subspace $\\mathcal{K}_m(\\mathbf{A}, \\mathbf{v}_1^{(\\mathrm{no3})}) \\subseteq S$. The eigenvalues of $\\mathbf{A}$ restricted to $S$ are $\\{\\lambda_1, \\lambda_2, \\lambda_4\\} = \\{1, 2, 4\\}$. Therefore, the RR values, which are the eigenvalues of $\\mathbf{T}_m$, can only approximate or exactly equal members of $\\{1, 2, 4\\}$. The eigenvalue $\\lambda_3=3$ cannot appear among the RR values. The algorithm should break down at $m=3$, when the Krylov subspace spans $S$. The implementation will run the Lanczos algorithm and check if any of the computed RR values are within a tolerance $\\delta$ of $\\lambda_3=3$. The expected result is `False`.\n\n**Case 2: Exact Eigenvector Start**\n\nThe starting vector is $\\mathbf{v}_1^{(\\mathrm{exact3})} = \\mathbf{e}_3$, which is the eigenvector corresponding to $\\lambda_3=3$.\n\nThe Krylov subspace of order $k>1$ is not well-defined, as the vectors become linearly dependent immediately. $\\mathcal{K}_1(\\mathbf{A}, \\mathbf{e}_3) = \\mathrm{span}\\{\\mathbf{e}_3\\}$. The vector $\\mathbf{A}\\mathbf{e}_3 = \\lambda_3 \\mathbf{e}_3 = 3\\mathbf{e}_3$ is not linearly independent of $\\mathbf{e}_3$. Let's trace the first step of the Lanczos algorithm:\n- Initialize: $\\mathbf{q}_1 = \\mathbf{e}_3 = (0,0,1,0)^\\top$.\n- Step $j=1$:\n  - $\\mathbf{z}_1 = \\mathbf{A}\\mathbf{q}_1 = \\mathbf{A}\\mathbf{e}_3 = 3\\mathbf{e}_3 = (0,0,3,0)^\\top$.\n  - $\\alpha_1 = \\mathbf{q}_1^\\top \\mathbf{z}_1 = \\mathbf{e}_3^\\top (3\\mathbf{e}_3) = 3$.\n  - $\\mathbf{w}_1 = \\mathbf{z}_1 - \\alpha_1\\mathbf{q}_1 = 3\\mathbf{e}_3 - 3\\mathbf{e}_3 = \\mathbf{0}$.\n  - $\\beta_1 = \\lVert \\mathbf{w}_1 \\rVert_2 = 0$.\nSince $\\beta_1=0 \\le \\varepsilon$, the algorithm breaks down at $m=1$. The resulting matrix is $\\mathbf{T}_1 = [\\alpha_1] = [3]$. The only RR value is $3$, which is the exact eigenvalue. The process terminates in one step. The task is to report the number of steps, which is $m=1$.\n\n**Case 3: Non-zero Spectral Mass**\n\nThe starting vector is $\\mathbf{v}_1^{(\\mathrm{all})} = \\frac{1}{2}(1,1,1,1)^\\top$. This vector has a non-zero component along every eigenvector $\\mathbf{e}_j$. The spectral mass on $\\lambda_3=3$ is $w_3 = |\\mathbf{e}_3^\\top \\mathbf{v}_1^{(\\mathrm{all})}|^2 = |1/2|^2 = 1/4 \\neq 0$.\n\nBecause the starting vector is not deficient in any eigenvector direction, the Krylov subspace $\\mathcal{K}_k(\\mathbf{A}, \\mathbf{v}_1^{(\\mathrm{all})})$ will grow in dimension with each step until it spans the entire space $\\mathbb{R}^4$. This is because the matrix $\\mathbf{A}$ has distinct eigenvalues. The Lanczos algorithm will therefore run for $m=4$ steps, generating a basis for $\\mathbb{R}^4$. The resulting matrix $\\mathbf{T}_4$ will be a $4 \\times 4$ tridiagonal matrix. The Lanczos relation is $\\mathbf{A} \\mathbf{Q}_4 = \\mathbf{Q}_4 \\mathbf{T}_4$, where $\\mathbf{Q}_4$ is the orthogonal matrix with columns $\\{\\mathbf{q}_1, \\mathbf{q}_2, \\mathbf{q}_3, \\mathbf{q}_4\\}$. This implies that $\\mathbf{T}_4 = \\mathbf{Q}_4^\\top \\mathbf{A} \\mathbf{Q}_4$, so $\\mathbf{T}_4$ is orthogonally similar to $\\mathbf{A}$. Thus, $\\mathbf{T}_4$ must have the same eigenvalues as $\\mathbf{A}$, which are $\\{1, 2, 3, 4\\}$. The algorithm should terminate at $m=4$ because $\\mathcal{K}_4$ is an invariant subspace (it's the whole $\\mathbb{R}^4$), leading to $\\beta_4=0$. The set of RR values will contain $\\lambda_3=3$. The implementation will confirm this by checking if any RR value is within $\\delta$ of $3$. The expected result is `True`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lanczos(A, v_start, epsilon):\n    \"\"\"\n    Implements the Lanczos three-term recurrence.\n\n    Args:\n        A (np.ndarray): The real symmetric matrix.\n        v_start (np.ndarray): The starting unit vector.\n        epsilon (float): The breakdown tolerance for beta.\n\n    Returns:\n        tuple: A tuple containing:\n            - m (int): The number of steps until breakdown.\n            - T_m (np.ndarray): The m x m symmetric tridiagonal matrix.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Ensure v_start is a unit vector\n    q = v_start / np.linalg.norm(v_start)\n    q_prev = np.zeros(n)\n    \n    alphas = []\n    betas_off_diag = []\n    \n    m = 0\n    \n    for j in range(n):  # Loop corresponds to j=1, 2, ..., n\n        z = A @ q\n        alpha = q.T @ z\n        alphas.append(alpha)\n        \n        # w = z - alpha*q - beta_{j-1}*q_{j-1}\n        w = z - alpha * q\n        if j > 0:\n            w -= betas_off_diag[-1] * q_prev\n        \n        beta = np.linalg.norm(w)\n        \n        m = j + 1\n        if beta <= epsilon:\n            break\n        \n        betas_off_diag.append(beta)\n        q_prev = q\n        q = w / beta\n    \n    # Construct the tridiagonal matrix T_m of size m x m\n    T_m = np.diag(alphas)\n    if m > 1:\n        T_m += np.diag(betas_off_diag, k=1)\n        T_m += np.diag(betas_off_diag, k=-1)\n        \n    return m, T_m\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three specified cases.\n    \"\"\"\n    # Define matrix, target eigenvalue, and tolerances\n    A = np.diag([1.0, 2.0, 3.0, 4.0])\n    lambda3 = 3.0\n    epsilon = 1e-12\n    delta = 1e-10\n\n    # Define starting vectors for the three cases\n    v_no3 = np.array([1.0, 1.0, 0.0, 1.0]) / np.sqrt(3.0)\n    v_exact3 = np.array([0.0, 0.0, 1.0, 0.0])\n    v_all = np.array([1.0, 1.0, 1.0, 1.0]) / 2.0\n    \n    test_cases = [\n        {\"name\": \"Case 1\", \"vector\": v_no3},\n        {\"name\": \"Case 2\", \"vector\": v_exact3},\n        {\"name\": \"Case 3\", \"vector\": v_all},\n    ]\n\n    results = []\n\n    # Process Case 1: Zero spectral mass on lambda_3\n    m1, T1 = lanczos(A, test_cases[0][\"vector\"], epsilon)\n    rr_values1 = np.linalg.eigvalsh(T1)\n    case1_result = np.any(np.isclose(rr_values1, lambda3, atol=delta))\n    results.append(case1_result)\n\n    # Process Case 2: Exact eigenvector start\n    m2, T2 = lanczos(A, test_cases[1][\"vector\"], epsilon)\n    case2_result = m2\n    results.append(case2_result)\n\n    # Process Case 3: Non-zero spectral mass on lambda_3\n    m3, T3 = lanczos(A, test_cases[2][\"vector\"], epsilon)\n    rr_values3 = np.linalg.eigvalsh(T3)\n    case3_result = np.any(np.isclose(rr_values3, lambda3, atol=delta))\n    results.append(case3_result)\n    \n    # Convert booleans to lowercase strings for printing, if needed,\n    # but standard str() gives 'True'/'False'. The example format\n    # '[b_1,i_2,b_3]' implies placeholders. Using standard str() representation.\n    # The `map(str, ...)` handles conversion of all types.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practical applications, we don't run the Lanczos iteration for a fixed number of steps; instead, we stop when an eigenvalue has converged to a desired accuracy. This advanced exercise challenges you to design and implement an adaptive Lanczos algorithm that does just that . You will derive the elegant and efficient stopping criterion from first principles, based on the residual norm of a Ritz pair, and apply it to create a robust solver that automatically determines the right number of iterations.",
            "id": "3247013",
            "problem": "You are to design and implement an adaptive algorithm based on the Lanczos method that automatically determines the number of steps $k$ needed to achieve a specified accuracy for approximating the largest eigenvalue of a real symmetric matrix. The algorithm must be grounded in first principles of Krylov subspace methods and real symmetric matrix theory.\n\nStart from the following foundational base:\n- A real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ has $n$ real eigenvalues and admits an orthonormal basis of eigenvectors.\n- For a given nonzero initial vector $q_1 \\in \\mathbb{R}^n$, the $k$-dimensional Krylov subspace generated by $A$ and $q_1$ is $\\mathcal{K}_k(A,q_1) = \\operatorname{span}\\{q_1, Aq_1, A^2 q_1, \\dots, A^{k-1} q_1\\}$.\n- Orthonormalizing the sequence $\\{q_1, Aq_1, A^2 q_1, \\dots\\}$ with the Gramâ€“Schmidt process tailored to real symmetric matrices yields an orthonormal basis $\\{q_1, q_2, \\dots, q_k\\}$ of $\\mathcal{K}_k(A,q_1)$ that satisfies a three-term recurrence with real scalars $\\alpha_j$ and $\\beta_j$.\n\nFrom these principles, derive and implement an adaptive Lanczos iteration that:\n- Constructs the orthonormal basis vectors $\\{q_j\\}$ and the corresponding real symmetric tridiagonal matrix $T_k \\in \\mathbb{R}^{k \\times k}$ whose diagonal consists of $\\alpha_1, \\dots, \\alpha_k$ and whose sub- and super-diagonal consists of $\\beta_1, \\dots, \\beta_{k-1}$.\n- At each iteration $k$, computes the eigenvalues and eigenvectors of $T_k$ (the Ritz pairs), selects the largest Ritz value $\\theta_{\\max}^{(k)}$, and uses a principled residual estimate to decide whether to stop or continue.\n\nYour stopping criterion must be justified from first principles and must not rely on any pre-provided shortcut formulas. In particular, base your stopping decision on the residual of the Ritz pair derived from the Lanczos relation, expressed in terms of quantities that are intrinsic to the iteration at step $k$. The algorithm should terminate when the residual estimate for the largest Ritz pair is less than or equal to a user-specified tolerance $\\tau$, or when a mathematically justified breakdown condition is encountered (for example, when the next basis vector cannot be formed because the norm required by orthonormalization is zero), or when the iteration reaches the ceiling $k_{\\max} = n$.\n\nImplementation constraints:\n- Input matrices must be real symmetric.\n- The initial vector must be $q_1 = \\frac{\\mathbf{1}}{\\|\\mathbf{1}\\|_2}$, where $\\mathbf{1}$ is the vector of all ones in $\\mathbb{R}^n$.\n- The algorithm must be completely deterministic for the provided test suite.\n\nTest suite:\nImplement your algorithm and apply it to the following four test cases. For each case, report the triple $[k, \\theta_{\\max}, r]$ where $k$ is the number of Lanczos steps used upon termination (an integer), $\\theta_{\\max}$ is the largest Ritz value at termination (a float), and $r$ is the residual estimate used for stopping (a float). In all cases, round the floats to $10$ decimal places in the final output.\n\n1. Happy path case: $A_1 \\in \\mathbb{R}^{10 \\times 10}$ defined by $A_1 = \\frac{G + G^{\\top}}{2}$ where $G$ has independent standard normal entries drawn with a pseudorandom generator seeded by $42$. Use tolerance $\\tau_1 = 10^{-8}$.\n2. Boundary condition: $A_2 = I_8$ (the $8 \\times 8$ identity matrix). Use tolerance $\\tau_2 = 10^{-12}$.\n3. Structured edge case: $A_3 \\in \\mathbb{R}^{50 \\times 50}$ is the tridiagonal Toeplitz matrix with main diagonal equal to $2$ and sub- and super-diagonals equal to $-1$. Use tolerance $\\tau_3 = 10^{-10}$.\n4. Tight tolerance near machine precision: $A_4 \\in \\mathbb{R}^{12 \\times 12}$ defined by $A_4 = \\frac{H + H^{\\top}}{2}$ where $H$ has independent standard normal entries drawn with a pseudorandom generator seeded by $7$. Use tolerance $\\tau_4 = 10^{-14}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the triple $[k,\\theta_{\\max},r]$ for the corresponding test case, in the order $A_1, A_2, A_3, A_4$. For example, the output should look like:\n$[\\,[k_1,\\theta_{\\max,1},r_1],[k_2,\\theta_{\\max,2},r_2],[k_3,\\theta_{\\max,3},r_3],[k_4,\\theta_{\\max,4},r_4]\\,]$.\nAll floats must be rounded to $10$ decimal places, and all integers must be exact.",
            "solution": "The problem requires the design and implementation of an adaptive Lanczos algorithm to approximate the largest eigenvalue of a real symmetric matrix $A$. The core of the task is to derive a principled stopping criterion from the fundamental relations of the Lanczos process and use it to automatically determine the necessary number of iterations, $k$.\n\nWe begin by establishing the theoretical foundation. The Lanczos algorithm is a specific instance of an Arnoldi iteration tailored for real symmetric matrices. It constructs an orthonormal basis $\\{q_1, q_2, \\dots, q_k\\}$ for the $k$-dimensional Krylov subspace $\\mathcal{K}_k(A, q_1) = \\operatorname{span}\\{q_1, A q_1, \\dots, A^{k-1} q_1\\}$, where $q_1$ is a given starting vector with $\\|q_1\\|_2 = 1$. Due to the symmetry of $A$, the orthonormalization process simplifies to a three-term recurrence relation.\n\nLet $Q_k = [q_1, q_2, \\dots, q_k]$ be the $n \\times k$ matrix with the Lanczos vectors as its columns. The three-term recurrence is given by:\n$$ \\beta_j q_{j+1} = A q_j - \\alpha_j q_j - \\beta_{j-1} q_{j-1} $$\nfor $j = 1, 2, \\dots, k$, with the initial conditions $\\beta_0 = 0$ and $q_0 = \\mathbf{0}$. The coefficients are determined as follows:\n- The diagonal coefficients, $\\alpha_j$, are chosen to make the next vector $A q_j - \\alpha_j q_j$ orthogonal to $q_j$. Since the previous vectors $\\{q_i\\}_{i<j}$ are already orthogonal to $q_j$, this is achieved by setting $\\alpha_j = q_j^\\top A q_j$.\n- The off-diagonal coefficients, $\\beta_j$, are normalization factors. Let $w_j = A q_j - \\alpha_j q_j - \\beta_{j-1} q_{j-1}$. Then $\\beta_j = \\|w_j\\|_2$, and the next Lanczos vector is $q_{j+1} = w_j / \\beta_j$. The symmetry of $A$ ensures that $q_{j+1}$ is automatically orthogonal to all preceding vectors $q_1, \\dots, q_{j-1}$.\n\nThis recurrence can be expressed in matrix form as:\n$$ A Q_k = Q_k T_k + \\beta_k q_{k+1} e_k^\\top $$\nwhere $e_k$ is the $k$-th standard basis vector in $\\mathbb{R}^k$, and $T_k$ is a $k \\times k$ real symmetric tridiagonal matrix:\n$$ T_k = \\begin{pmatrix}\n\\alpha_1 & \\beta_1 & & & \\\\\n\\beta_1 & \\alpha_2 & \\beta_2 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & \\beta_{k-2} & \\alpha_{k-1} & \\beta_{k-1} \\\\\n& & & \\beta_{k-1} & \\alpha_k\n\\end{pmatrix} $$\nThe eigenvalues of $T_k$, denoted $\\theta_i^{(k)}$, are called the Ritz values, and they serve as approximations to the eigenvalues of $A$. The extremality of Ritz values makes them particularly good approximations for the extremal eigenvalues of $A$.\n\nThe next crucial step is to derive the stopping criterion. The problem asks for this to be based on first principles. Let $(\\theta, s)$ be an eigenpair of $T_k$, so $T_k s = \\theta s$ with $\\|s\\|_2 = 1$. The corresponding Ritz vector is $y = Q_k s \\in \\mathbb{R}^n$. We measure the quality of this approximation by the norm of the residual vector, $\\|A y - \\theta y\\|_2$.\n\nUsing the matrix relation $A Q_k = Q_k T_k + \\beta_k q_{k+1} e_k^\\top$, we can express the residual as:\n$$ A y - \\theta y = A (Q_k s) - \\theta (Q_k s) $$\n$$ = (Q_k T_k + \\beta_k q_{k+1} e_k^\\top) s - \\theta Q_k s $$\nBy distributing $s$, we get:\n$$ = Q_k (T_k s) + \\beta_k q_{k+1} (e_k^\\top s) - \\theta Q_k s $$\nSince $T_k s = \\theta s$, the first term becomes $Q_k (\\theta s) = \\theta (Q_k s)$. The expression simplifies significantly:\n$$ = \\theta (Q_k s) + \\beta_k q_{k+1} (e_k^\\top s) - \\theta (Q_k s) $$\n$$ = \\beta_k (e_k^\\top s) q_{k+1} $$\nThe term $e_k^\\top s$ is a scalar that extracts the last component of the eigenvector $s$, which we denote as $s_k$.\n$$ A y - \\theta y = \\beta_k s_k q_{k+1} $$\nWe now compute the Euclidean norm of the residual. Since $q_{k+1}$ is a unit vector ($\\|q_{k+1}\\|_2 = 1$), the norm is:\n$$ \\|A y - \\theta y\\|_2 = \\| \\beta_k s_k q_{k+1} \\|_2 = |\\beta_k s_k| \\cdot \\|q_{k+1}\\|_2 = |\\beta_k s_k| $$\nThis provides an elegant and inexpensive way to compute the exact norm of the residual for any Ritz pair without ever forming the dense Ritz vector $y$. The stopping criterion for our adaptive algorithm is therefore to terminate at iteration $k$ if the residual estimate for the largest Ritz value, $\\theta_{\\max}^{(k)}$, falls below a specified tolerance $\\tau$. That is, we stop when $|\\beta_k s_{\\max, k}| \\le \\tau$, where $s_{\\max, k}$ is the last component of the eigenvector of $T_k$ corresponding to $\\theta_{\\max}^{(k)}$.\n\nThe complete adaptive Lanczos algorithm is as follows:\n\n1.  **Initialization**: Given a real symmetric matrix $A \\in \\mathbb{R}^{n\\times n}$ and a tolerance $\\tau$.\n    - Set the maximum number of iterations $k_{\\max} = n$.\n    - Initialize the starting vector $q_1 = \\mathbf{1} / \\sqrt{n}$.\n    - Set $q_0 = \\mathbf{0} \\in \\mathbb{R}^n$ and $\\beta_0 = 0$.\n    - Let $q_{prev} = q_0$ and $q_{curr} = q_1$.\n    - Initialize empty lists for $\\alpha$ and $\\beta$ coefficients.\n\n2.  **Iteration**: Loop for $k = 1, 2, \\dots, k_{\\max}$:\n    a. Compute the next vector in the Krylov sequence: $v = A q_{curr}$.\n    b. Compute the diagonal element: $\\alpha_k = q_{curr}^\\top v$.\n    c. Compute the unnormalized next vector, making it orthogonal to $q_{curr}$ and $q_{prev}$: $w = v - \\alpha_k q_{curr} - \\beta_{k-1} q_{prev}$.\n    d. Compute the off-diagonal element: $\\beta_k = \\|w\\|_2$.\n    e. Store $\\alpha_k$ in the list of alphas. At this point, the list contains $\\{\\alpha_1, \\dots, \\alpha_k\\}$.\n    f. Construct the tridiagonal matrix $T_k \\in \\mathbb{R}^{k\\times k}$ using the collected lists of alphas (as the diagonal) and betas (as the off-diagonals).\n    g. Compute the eigenvalues and eigenvectors of $T_k$. Since $T_k$ is real and symmetric, this is a numerically stable operation. A specialized solver like `numpy.linalg.eigh` is suitable.\n    h. Identify the largest eigenvalue $\\theta_{\\max}^{(k)}$ and its corresponding eigenvector $s_{\\max}$.\n    i. Compute the residual estimate: $r = |\\beta_k \\cdot s_{\\max, k}|$, where $s_{\\max, k}$ is the last component of $s_{\\max}$.\n    j. **Check for Termination**:\n        - If $r \\le \\tau$ (tolerance met), or\n        - If $\\beta_k$ is numerically indistinguishable from zero (breakdown, indicating an exact eigenvector has been found), or\n        - If $k = k_{\\max}$ (maximum iterations reached),\n        then terminate the loop and return $[k, \\theta_{\\max}^{(k)}, r]$.\n    k. If not terminating, store $\\beta_k$ in the list of betas.\n    l. Prepare for the next iteration:\n        - $q_{next} = w / \\beta_k$.\n        - $q_{prev} = q_{curr}$.\n        - $q_{curr} = q_{next}$.\n        - $\\beta_{k-1}$ is conceptually updated to $\\beta_k$ for the next step's recurrence.\n\nThis principled, adaptive algorithm robustly determines the largest eigenvalue to a desired precision by dynamically increasing the size of the Krylov subspace until the Ritz approximation is satisfactory.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef adaptive_lanczos(A: np.ndarray, tol: float):\n    \"\"\"\n    Performs the adaptive Lanczos iteration to find the largest eigenvalue\n    of a real symmetric matrix A.\n\n    Args:\n        A (np.ndarray): The real symmetric matrix.\n        tol (float): The tolerance for the stopping criterion.\n\n    Returns:\n        A tuple (k, theta_max, residual_est) containing:\n        - k (int): The number of iterations performed.\n        - theta_max (float): The largest approximate eigenvalue (Ritz value).\n        - residual_est (float): The final residual norm estimate.\n    \"\"\"\n    n = A.shape[0]\n    \n    q_curr = np.ones(n) / np.sqrt(n)\n    q_prev = np.zeros(n)\n    \n    alphas = []\n    betas_off_diag = [] # Betas for T_k: beta_1, ..., beta_{k-1}\n    \n    beta_prev = 0.0\n\n    # Numerical zero for breakdown condition\n    # A small positive value to avoid division by zero and handle convergence\n    # where beta becomes extremely small.\n    numerical_zero = 1e-15\n\n    for k in range(1, n + 1):\n        v = A @ q_curr\n        alpha = np.dot(q_curr.T, v)\n        alphas.append(alpha)\n\n        # Orthogonalize against previous two Lanczos vectors\n        w = v - alpha * q_curr - beta_prev * q_prev\n        beta_curr = np.linalg.norm(w)\n\n        # Construct T_k\n        T_k = np.diag(alphas)\n        if k > 1:\n            T_k += np.diag(betas_off_diag, k=1) + np.diag(betas_off_diag, k=-1)\n\n        # Find eigenpairs of T_k\n        # eigh returns sorted eigenvalues and corresponding eigenvectors\n        eigvals, eigvecs = np.linalg.eigh(T_k)\n        \n        theta_max = eigvals[-1]\n        s_max = eigvecs[:, -1]\n        s_max_k = s_max[-1]\n\n        # Residual estimate for the largest Ritz pair\n        residual_est = np.abs(beta_curr * s_max_k)\n        \n        # Check termination conditions\n        if residual_est <= tol or k == n or beta_curr < numerical_zero:\n            return k, theta_max, residual_est\n\n        # Prepare for next iteration\n        betas_off_diag.append(beta_curr)\n        q_next = w / beta_curr\n        \n        q_prev = q_curr\n        q_curr = q_next\n        beta_prev = beta_curr\n        \n    # This part should not be reached due to k == n condition in the loop.\n    # Included for completeness.\n    return n, theta_max, residual_est\n\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases for the adaptive Lanczos algorithm.\n    \"\"\"\n    \n    # Test Case 1: Happy path\n    seed1 = 42\n    n1 = 10\n    tol1 = 1e-8\n    rng1 = np.random.default_rng(seed1)\n    G = rng1.standard_normal((n1, n1))\n    A1 = (G + G.T) / 2.0\n    \n    # Test Case 2: Boundary condition\n    n2 = 8\n    tol2 = 1e-12\n    A2 = np.identity(n2)\n    \n    # Test Case 3: Structured edge case\n    n3 = 50\n    tol3 = 1e-10\n    diag_main = 2.0 * np.ones(n3)\n    diag_off = -1.0 * np.ones(n3 - 1)\n    A3 = np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)\n\n    # Test Case 4: Tight tolerance\n    seed4 = 7\n    n4 = 12\n    tol4 = 1e-14\n    rng4 = np.random.default_rng(seed4)\n    H = rng4.standard_normal((n4, n4))\n    A4 = (H + H.T) / 2.0\n    \n    test_cases = [\n        (A1, tol1),\n        (A2, tol2),\n        (A3, tol3),\n        (A4, tol4),\n    ]\n\n    results = []\n    for A, tol in test_cases:\n        k, theta_max, r = adaptive_lanczos(A, tol)\n        \n        # Format results: k is int, others are floats rounded to 10 decimal places.\n        # Python's round() can have unexpected behavior. Using string formatting is safer for decimal representation.\n        theta_max_rounded = float(f\"{theta_max:.10f}\")\n        r_rounded = float(f\"{r:.10f}\")\n        \n        results.append(f\"[{k},{theta_max_rounded},{r_rounded}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}