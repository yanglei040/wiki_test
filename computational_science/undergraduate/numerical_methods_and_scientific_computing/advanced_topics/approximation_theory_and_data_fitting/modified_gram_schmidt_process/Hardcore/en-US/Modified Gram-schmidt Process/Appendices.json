{
    "hands_on_practices": [
        {
            "introduction": "In the world of exact arithmetic, both the Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS) algorithms produce identical orthonormal bases. This exercise  demonstrates why, in the practical world of finite-precision computers, MGS is vastly superior. You will explore a scenario inspired by the \"eigenfaces\" method for face recognition, where you will compare the ability of CGS and MGS to create a basis for a set of very similar images. By quantifying the reconstruction error for each method, you will gain a concrete, hands-on appreciation for the numerical stability that makes MGS an essential tool in scientific computing.",
            "id": "3252976",
            "problem": "You are given a simplified framing of the \"eigenfaces\" setting in which each face image is represented as a column vector in the real vector space $\\mathbb{R}^d$. Let $A \\in \\mathbb{R}^{d \\times m}$ denote the data matrix with $m$ face vectors as columns. Define the mean face vector $\\mu \\in \\mathbb{R}^d$ by averaging the columns of $A$, and the mean-centered data matrix $M = A - \\mu \\mathbf{1}^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^m$ denotes the vector of ones. Consider constructing an orthonormal basis for the column space of $M$ using two different orthonormalization procedures: the classical Gram-Schmidt (CGS) process and the modified Gram-Schmidt (MGS) process. Both procedures operate under the standard Euclidean inner product on $\\mathbb{R}^d$ and aim to produce column-wise orthonormal bases.\n\nStarting from the foundational definitions of inner product spaces, orthogonality, and orthonormal bases, implement both the classical Gram-Schmidt (CGS) process and the modified Gram-Schmidt (MGS) process to produce orthonormal bases for the column space of $M$. Using the property that orthonormal bases allow constructing orthogonal projections onto subspaces, compute for each basis the linear reconstruction of the original data by projecting $M$ onto the span of the first $k$ basis vectors and then adding back the mean vector $\\mu$. Quantify reconstruction quality via the relative Frobenius norm reconstruction error, defined as the Frobenius norm of the residual divided by the Frobenius norm of the original matrix.\n\nYour program must generate synthetic datasets that model \"very similar faces\" in a scientifically sound manner by perturbing a common base vector with small variations. Use deterministic random number generation for reproducibility. For each dataset, compute the mean-centered matrix $M$, construct orthonormal bases using CGS and MGS, and for each specified $k$, compute the relative Frobenius reconstruction error for the CGS basis and for the MGS basis. Report, for each dataset and each $k$, the difference in errors $\\mathrm{err}_{\\mathrm{CGS}} - \\mathrm{err}_{\\mathrm{MGS}}$.\n\nUse the following test suite. In all cases, let $d = 64$ so that face vectors correspond to flattened images of size $8 \\times 8$. Let $k$ range over $\\{1, 2, 3\\}$.\n\n- Test case $1$ (general, moderately similar faces):\n  - Parameters: $d = 64$, $m = 10$, noise standard deviation $\\sigma = 10^{-3}$, random seed $42$, duplicates flag set to false.\n- Test case $2$ (extremely similar faces, near-duplicate columns to stress numerical stability):\n  - Parameters: $d = 64$, $m = 10$, noise standard deviation $\\sigma = 10^{-8}$, random seed $7$, duplicates flag set to true (half of the columns are constructed to be nearly duplicates of an earlier column).\n- Test case $3$ (boundary case with few images and high similarity, potential effective low rank):\n  - Parameters: $d = 64$, $m = 4$, noise standard deviation $\\sigma = 10^{-6}$, random seed $99$, duplicates flag set to true.\n\nScientific realism requirements:\n- The base face vector must be a smooth, nontrivial pattern over the $8 \\times 8$ grid, such as a mixture of a two-dimensional Gaussian centered in the image and a gentle horizontal gradient, scaled to unit norm in $\\ell_2$.\n- Each face is generated by applying small global intensity scaling and bias to the base vector and adding independent Gaussian noise with standard deviation $\\sigma$.\n- When the duplicates flag is true, construct half of the columns to be near duplicates of an earlier column by forming a linear combination with a coefficient close to $1$ and adding noise of scale $\\sigma$.\n\nError metric:\n- For a chosen $k \\in \\{1, 2, 3\\}$ and an orthonormal basis $Q \\in \\mathbb{R}^{d \\times r}$ (with $r \\leq m$), define the reconstruction of $A$ by projecting $M$ onto the span of the first $k$ columns of $Q$ and then adding back $\\mu$. Compute the relative Frobenius norm reconstruction error as the Frobenius norm of the difference between $A$ and its reconstruction divided by the Frobenius norm of $A$.\n\nImplementation constraints:\n- Implement both CGS and MGS to produce orthonormal bases from $M$. If, during orthonormalization, a candidate vector has Euclidean norm below a small threshold, discard it to avoid division by numbers that would be treated as zero in finite precision. Use a fixed threshold suitable for double-precision arithmetic.\n- Use only the Euclidean inner product structure of $\\mathbb{R}^d$ and the properties of orthonormal bases and orthogonal projections. Do not use Principal Component Analysis (PCA) or Singular Value Decomposition (SVD).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of three items, where each item corresponds to one test case and is itself a list of three floating-point numbers $[\\Delta_1, \\Delta_2, \\Delta_3]$ with $\\Delta_k = \\mathrm{err}_{\\mathrm{CGS}}(k) - \\mathrm{err}_{\\mathrm{MGS}}(k)$ for $k \\in \\{1,2,3\\}$. The output line must have the exact format:\n  - Example structure: $[[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3]]$.\n\nNo physical units are involved. Angles are not used. Percentages must not be used; express errors as decimal floating-point numbers. The program must be self-contained, require no user input, and rely only on the specified runtime environment.",
            "solution": "We begin from the foundational definitions of Euclidean inner product spaces and orthogonal projections. Let $\\mathbb{R}^d$ be equipped with its standard inner product $\\langle x, y \\rangle = \\sum_{i=1}^d x_i y_i$, and define the Euclidean norm $\\|x\\|_2 = \\sqrt{\\langle x, x \\rangle}$. A set of vectors $\\{q_1, q_2, \\dots, q_r\\}$ in $\\mathbb{R}^d$ is orthonormal if $\\langle q_i, q_j \\rangle = 0$ for $i \\neq j$ and $\\|q_i\\|_2 = 1$ for all $i$. Given an orthonormal set $\\{q_1, \\dots, q_r\\}$, the orthogonal projection $P$ onto $\\mathrm{span}\\{q_1, \\dots, q_r\\}$ maps a vector $x \\in \\mathbb{R}^d$ to the unique vector in the subspace that minimizes $\\|x - y\\|_2$ over $y$ in the subspace; this projection is characterized by the property that $x - P x$ is orthogonal to the subspace. For an orthonormal basis matrix $Q \\in \\mathbb{R}^{d \\times r}$ whose columns are $\\{q_i\\}$, the projection of a vector $x$ can be constructed using the inner product coefficients $\\langle q_i, x \\rangle$ and the orthonormality relations.\n\nFrom this base, we consider constructing orthonormal bases for the column space of the mean-centered matrix $M = A - \\mu \\mathbf{1}^\\top$, where $A \\in \\mathbb{R}^{d \\times m}$ is the data matrix and $\\mu \\in \\mathbb{R}^d$ is the mean vector of the columns of $A$. We compare two procedures:\n\n- Classical Gram-Schmidt (CGS): Conceptually, at step $j$, we form the orthogonal component of the $j$-th input vector by removing its projections onto the previously constructed orthonormal vectors using the inner products with the original input vector, and then normalize the result to unit norm, provided its norm is sufficiently large. In exact arithmetic, this produces an orthonormal set spanning the same subspace as the input vectors. In floating-point arithmetic, CGS can lose orthogonality when the input vectors are nearly linearly dependent, because the subtraction of nearly equal quantities exacerbates rounding errors and the use of inner products computed against the original vector does not mitigate the accumulating numerical correlation.\n- Modified Gram-Schmidt (MGS): At step $j$, we iteratively remove components of the current working vector along each previously constructed orthonormal vector, updating the working vector after each subtraction, and then normalize the final result if its norm is sufficiently large. In floating-point arithmetic, MGS is more numerically stable because reorthogonalization is performed in a way that reduces the growth of rounding error and better maintains orthogonality among the constructed basis vectors.\n\nBoth procedures rely on the core definition of orthogonality and normalization within the inner product space. The projection-based reconstruction uses the property that for an orthonormal basis, the best approximation in the least-squares sense is obtained by projecting onto the span of the selected basis vectors. Specifically, if $Q \\in \\mathbb{R}^{d \\times r}$ has orthonormal columns and we choose $k \\leq r$, then the projection of $M$ onto the span of the first $k$ columns of $Q$ yields an approximation $\\widehat{M}$ in the subspace. The reconstructed data matrix is then $\\widehat{A} = \\mu \\mathbf{1}^\\top + \\widehat{M}$, and the reconstruction quality is quantified via the relative Frobenius norm error\n$$\n\\mathrm{err} = \\frac{\\|A - \\widehat{A}\\|_F}{\\|A\\|_F},\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm $\\|X\\|_F = \\sqrt{\\sum_{i,j} X_{ij}^2}$.\n\nAlgorithmic design:\n- Generate synthetic datasets modeling \"very similar faces.\" Construct a base face vector over an $8 \\times 8$ grid using a smooth, nontrivial pattern (for example, a two-dimensional Gaussian centered at the image midpoint added to a gentle horizontal gradient), then scale it to unit $\\ell_2$ norm. Each face vector is obtained by applying a small global intensity scaling and bias to the base vector and adding independent Gaussian noise with standard deviation $\\sigma$. When the duplicates flag is true, create half of the columns to be near duplicates of an earlier column by scaling that column by a coefficient close to $1$ and adding small Gaussian noise of scale $\\sigma$. This produces columns that are very nearly linearly dependent, which stresses numerical stability.\n- Compute the mean vector $\\mu$ by averaging the columns of $A$ and form $M = A - \\mu \\mathbf{1}^\\top$.\n- Implement CGS and MGS to produce orthonormal bases from $M$. To ensure scientific realism in finite precision arithmetic, if the norm of a candidate vector falls below a small threshold (for example, a tolerance on the order of machine precision in double precision), discard the vector to avoid unstable normalization.\n- For each $k \\in \\{1, 2, 3\\}$, reconstruct $\\widehat{A}$ by projecting $M$ onto the span of the first $k$ basis vectors of $Q$ (for both CGS and MGS), adding back the mean $\\mu$, and compute the relative Frobenius reconstruction error. If fewer than $k$ basis vectors are available, use all available basis vectors.\n- Report the differences $\\Delta_k = \\mathrm{err}_{\\mathrm{CGS}}(k) - \\mathrm{err}_{\\mathrm{MGS}}(k)$ for each $k$ and each test case.\n\nWhy this comparison is meaningful:\n- In exact arithmetic, both CGS and MGS generate orthonormal bases spanning the same subspace, and for a fixed $k$, the orthogonal projector onto the span of the first $k$ basis vectors depends only on the subspace, not on the particular orthonormal basis. However, in floating-point arithmetic, CGS can suffer from loss of orthogonality when vectors are nearly linearly dependent, leading to a basis that is not perfectly orthonormal and hence a projection that deviates from the ideal. MGS is known to mitigate this effect. Therefore, on datasets of very similar faces, MGS typically yields more accurate reconstructions for small $k$ relative to CGS, and the differences $\\Delta_k$ provide a quantitative assessment of this stability gap.\n\nTest suite coverage:\n- Test case $1$ exercises a general \"happy path\" with moderately similar faces ($d = 64$, $m = 10$, $\\sigma = 10^{-3}$), where both CGS and MGS should perform comparably but small differences may appear due to finite precision.\n- Test case $2$ introduces extremely similar faces and near-duplicate columns ($d = 64$, $m = 10$, $\\sigma = 10^{-8}$, duplicates true), stressing CGS with a strong loss-of-orthogonality risk and highlighting the stability of MGS.\n- Test case $3$ provides a boundary case with few images and high similarity ($d = 64$, $m = 4$, $\\sigma = 10^{-6}$, duplicates true), potentially yielding an effectively low-rank $M$ so that requesting $k = 3$ exceeds the number of reliable basis vectors, ensuring the implementation correctly handles truncated bases.\n\nThe program aggregates results for the three test cases into a single output line with the exact format $[[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3]]$, where each inner list corresponds to one test case and the entries to $k \\in \\{1,2,3\\}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_base_face_vector(d: int) -> np.ndarray:\n    \"\"\"\n    Create a smooth, nontrivial base face vector over an 8x8 grid:\n    a mixture of a 2D Gaussian centered at the image midpoint and a gentle horizontal gradient.\n    The resulting vector is scaled to unit l2 norm.\n    \"\"\"\n    side = int(np.sqrt(d))\n    if side * side != d:\n        raise ValueError(\"d must be a perfect square for an 8x8 (or side x side) image grid.\")\n    x = np.linspace(0.0, 1.0, side)\n    y = np.linspace(0.0, 1.0, side)\n    xx, yy = np.meshgrid(x, y, indexing=\"xy\")\n    # 2D Gaussian centered near (0.5, 0.5) with moderate spread\n    sigma = 0.18\n    gaussian = np.exp(-((xx - 0.5)**2 + (yy - 0.5)**2) / (2.0 * sigma**2))\n    # Gentle horizontal gradient\n    gradient = 0.3 * xx\n    base = gaussian + gradient\n    base_vec = base.reshape(d).astype(np.float64)\n    # Normalize to unit l2 norm\n    norm = np.linalg.norm(base_vec)\n    if norm == 0.0:\n        return base_vec\n    return base_vec / norm\n\n\ndef generate_faces(d: int, m: int, sigma: float, seed: int, duplicates: bool) -> np.ndarray:\n    \"\"\"\n    Generate a synthetic face dataset A of shape (d, m) with \"very similar faces\":\n    - Start from a smooth base face vector.\n    - Apply small global intensity scaling and bias.\n    - Add independent Gaussian noise with standard deviation sigma.\n    - If duplicates is True, make half of the columns near duplicates of an earlier column.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    base = make_base_face_vector(d)\n    A = np.zeros((d, m), dtype=np.float64)\n\n    # Generate m faces with small global variations and noise.\n    for j in range(m):\n        gain = rng.uniform(-0.05, 0.05)  # small scaling\n        bias = rng.uniform(-0.02, 0.02)  # small bias\n        noise = rng.normal(loc=0.0, scale=sigma, size=d)\n        # Add a tiny structured variation correlated with horizontal gradient to avoid triviality\n        side = int(np.sqrt(d))\n        xx = np.linspace(0.0, 1.0, side)\n        grad_x = np.repeat(xx[np.newaxis, :], side, axis=0).reshape(d)\n        structured = 0.01 * bias * grad_x\n        A[:, j] = base * (1.0 + gain) + bias + structured + noise\n\n    if duplicates and m >= 2:\n        # Make half the columns (from index m//2 onward) near duplicates of earlier ones.\n        # Use a coefficient close to 1 and add small noise of scale sigma.\n        base_idx = 0\n        coeff = 0.999999\n        for j in range(m // 2, m):\n            noise = rng.normal(loc=0.0, scale=sigma, size=d)\n            A[:, j] = coeff * A[:, base_idx] + noise\n            # Cycle base_idx to duplicate different earlier columns if possible\n            base_idx = (base_idx + 1) % (m // 2 if m // 2 > 0 else 1)\n\n    return A\n\n\ndef mean_center_columns(A: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the mean vector mu and mean-centered matrix M = A - mu * 1^T.\n    \"\"\"\n    mu = np.mean(A, axis=1)\n    M = A - mu[:, None]\n    return mu, M\n\n\ndef cgs_orthonormal_basis(M: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Classical Gram-Schmidt (CGS) to form an orthonormal basis of the column space of M.\n    At step j, compute inner products with the original column M[:, j] for all existing basis vectors,\n    subtract the combined projection once, then normalize if norm exceeds tol.\n    \"\"\"\n    d, m = M.shape\n    Q_cols = []\n    for j in range(m):\n        a_j = M[:, j].copy()\n        # Compute projection coefficients using the original a_j\n        coeffs = [float(np.dot(q, a_j)) for q in Q_cols]\n        # Subtract combined projection\n        v = a_j.copy()\n        for coeff, q in zip(coeffs, Q_cols):\n            v -= coeff * q\n        nrm = np.linalg.norm(v)\n        if nrm > tol:\n            q_new = v / nrm\n            Q_cols.append(q_new)\n        # If nrm <= tol, discard as numerically zero\n    if len(Q_cols) == 0:\n        return np.zeros((d, 0), dtype=np.float64)\n    return np.column_stack(Q_cols)\n\n\ndef mgs_orthonormal_basis(M: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Modified Gram-Schmidt (MGS) to form an orthonormal basis of the column space of M.\n    At step j, iteratively remove components of the working vector along each existing basis vector,\n    updating the working vector after each subtraction, then normalize if norm exceeds tol.\n    \"\"\"\n    d, m = M.shape\n    Q_cols = []\n    for j in range(m):\n        v = M[:, j].copy()\n        for i in range(len(Q_cols)):\n            alpha = float(np.dot(Q_cols[i], v))\n            v -= alpha * Q_cols[i]\n        nrm = np.linalg.norm(v)\n        if nrm > tol:\n            q_new = v / nrm\n            Q_cols.append(q_new)\n        # If nrm <= tol, discard as numerically zero\n    if len(Q_cols) == 0:\n        return np.zeros((d, 0), dtype=np.float64)\n    return np.column_stack(Q_cols)\n\n\ndef reconstruction_error_relative(A: np.ndarray, Q: np.ndarray, mu: np.ndarray, k: int) -> float:\n    \"\"\"\n    Compute the relative Frobenius norm reconstruction error when projecting onto the span\n    of the first k columns of Q (assumed columns are orthonormal), then adding back mu.\n    If fewer than k basis vectors are available, use all available vectors.\n    \"\"\"\n    M = A - mu[:, None]\n    if Q.shape[1] == 0 or k <= 0:\n        M_hat = np.zeros_like(M)\n    else:\n        kk = min(k, Q.shape[1])\n        Qk = Q[:, :kk]\n        # Orthogonal projection onto span(Qk)\n        M_hat = Qk @ (Qk.T @ M)\n    A_hat = mu[:, None] + M_hat\n    num = np.linalg.norm(A - A_hat, ord='fro')\n    den = np.linalg.norm(A, ord='fro')\n    # Avoid division by zero (shouldn't happen with nontrivial data)\n    return float(num / den) if den != 0.0 else 0.0\n\n\ndef run_case(d: int, m: int, sigma: float, seed: int, duplicates: bool, k_list: list[int]) -> list[float]:\n    \"\"\"\n    Generate dataset, compute CGS and MGS bases, and return a list of differences\n    err_CGS(k) - err_MGS(k) for each k in k_list.\n    \"\"\"\n    A = generate_faces(d=d, m=m, sigma=sigma, seed=seed, duplicates=duplicates)\n    mu, M = mean_center_columns(A)\n    Q_cgs = cgs_orthonormal_basis(M, tol=1e-12)\n    Q_mgs = mgs_orthonormal_basis(M, tol=1e-12)\n\n    diffs = []\n    for k in k_list:\n        err_cgs = reconstruction_error_relative(A, Q_cgs, mu, k)\n        err_mgs = reconstruction_error_relative(A, Q_mgs, mu, k)\n        diffs.append(err_cgs - err_mgs)\n    return diffs\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: general, moderately similar faces\n        {\"d\": 64, \"m\": 10, \"sigma\": 1e-3, \"seed\": 42, \"duplicates\": False},\n        # Test case 2: extremely similar faces, near-duplicate columns\n        {\"d\": 64, \"m\": 10, \"sigma\": 1e-8, \"seed\": 7, \"duplicates\": True},\n        # Test case 3: boundary case, few images, high similarity\n        {\"d\": 64, \"m\": 4, \"sigma\": 1e-6, \"seed\": 99, \"duplicates\": True},\n    ]\n    k_list = [1, 2, 3]\n\n    results = []\n    for case in test_cases:\n        diffs = run_case(\n            d=case[\"d\"], m=case[\"m\"], sigma=case[\"sigma\"],\n            seed=case[\"seed\"], duplicates=case[\"duplicates\"],\n            k_list=k_list\n        )\n        results.append(diffs)\n\n    # Final print statement in the exact required format.\n    # Print a single line: a list of three lists, each with three floats.\n    print(f\"[{','.join('[' + ','.join(map(str, res)) + ']' for res in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After seeing that MGS is more numerically stable than CGS, this next practice invites you to investigate the underlying reasons in a controlled experiment. You will construct a special matrix whose columns are designed to be nearly linearly dependent, a situation that is challenging for any orthogonalization algorithm . By adjusting a small perturbation parameter $\\delta$, you will observe how the conditioning of the input matrix directly impacts two key diagnostic measures: the magnitude of the diagonal entries of the upper triangular matrix $R$, and the degree of orthogonality lost among the columns of the computed matrix $Q$. This exercise provides a deeper insight into the mechanics of numerical stability and how MGS behaves at the limits of machine precision.",
            "id": "3253014",
            "problem": "You will investigate the numerical behavior of the Modified Gram-Schmidt (MGS) orthonormalization process when applied to a column sequence that is nearly linearly dependent by construction. Work entirely in real Euclidean space with the standard inner product, where the projection of a vector $x$ onto a unit vector $u$ is $u(u^{\\top}x)$, and an orthonormal set $\\{q_1,\\dots,q_k\\}$ satisfies $q_i^{\\top}q_j = 0$ for $i\\neq j$ and $\\lVert q_i\\rVert_2 = 1$. The MGS algorithm is the iterative realization of the definition of orthonormalization via successive subtraction of projections onto the already computed orthonormal vectors.\n\nConstruct a matrix $A\\in\\mathbb{R}^{n\\times m}$, with $n=m$, by the following rule:\n- Fix $n=m=12$.\n- Let the first column $a_1\\in\\mathbb{R}^n$ be given by $(a_1)_j = 1/j$ for $j=1,\\dots,n$.\n- For a given scalar perturbation parameter $\\delta\\in\\mathbb{R}$, define columns recursively by $a_{i+1} = a_i + \\delta e_i$ for $i=1,\\dots,m-1$, where $e_i$ is the $i$-th standard basis vector in $\\mathbb{R}^n$.\n\nFor each fixed $\\delta$, compute the Modified Gram-Schmidt (MGS) factorization $A=QR$ where $Q\\in\\mathbb{R}^{n\\times m}$ has columns $q_i$ that are orthonormal in exact arithmetic and $R\\in\\mathbb{R}^{m\\times m}$ is upper triangular with diagonal entries $R_{ii}>0$ whenever the $i$-th MGS step produces a nonzero vector. Use only standard double-precision arithmetic.\n\nYour program must:\n1. Implement MGS starting from the core definition of orthonormalization: at step $i$, subtract from the current vector its projections onto the previously computed $q_j$ for $j<i$, then normalize the result to obtain $q_i$ and record $R_{ii}$ as its $2$-norm.\n2. For each test value of $\\delta$ in the test suite specified below, build $A(\\delta)$ as above, perform MGS to obtain $Q(\\delta)$ and $R(\\delta)$, and then compute the following two diagnostics:\n   - The minimum positive diagonal entry $\\min\\{R_{ii}(\\delta): R_{ii}(\\delta)>0,\\, i=1,\\dots,m\\}$ (if there are no positive $R_{ii}$, return $0$ for this diagnostic).\n   - The maximum adjacent-column inner product magnitude $\\max\\{|q_i(\\delta)^{\\top}q_{i+1}(\\delta)|: \\lVert q_i(\\delta)\\rVert_2>0,\\ \\lVert q_{i+1}(\\delta)\\rVert_2>0,\\ i=1,\\dots,m-1\\}$. This measures the loss of orthogonality between neighboring columns under MGS in finite precision.\n\nTest suite (use these exact values of $\\delta$):\n- $\\delta = 10^{-1}$ (happy path),\n- $\\delta = 10^{-8}$ (moderately small),\n- $\\delta = 10^{-12}$ (small),\n- $\\delta = 10^{-16}$ (near machine precision),\n- $\\delta = 10^{-20}$ (smaller than machine precision scale).\n\nScientific realism and derivation constraints:\n- Your MGS implementation must follow directly from the definition of orthonormalization via projections in the standard inner product and must not rely on prepackaged orthogonalization or factorization routines.\n- No pivoting or reorthogonalization is to be used.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results, one per test value of $\\delta$, where each result is a three-element list $[\\delta,\\ \\text{min\\_diag},\\ \\text{max\\_adj\\_dot}]$.\n- Concretely, the output must be a single line that looks like\n  $$\n  [\\,[\\delta_1,\\ \\text{min\\_diag}_1,\\ \\text{max\\_adj\\_dot}_1],\\ \\dots,\\ [\\delta_5,\\ \\text{min\\_diag}_5,\\ \\text{max\\_adj\\_dot}_5]\\,].\n  $$\nAll numerical answers are unitless real numbers. No angles or percentages are involved. Do not print any explanatory text, only the list described above.",
            "solution": "The user-provided problem has been analyzed and validated. All givens, constraints, and definitions are scientifically sound, consistent, and well-posed. The problem is a standard numerical experiment in linear algebra designed to investigate the numerical stability of the Modified Gram-Schmidt (MGS) process when applied to a nearly singular matrix. It is a valid problem.\n\nThe solution proceeds in three main stages: constructing the test matrix $A(\\delta)$, applying the Modified Gram-Schmidt orthonormalization to obtain the factorization $A=QR$, and computing the specified numerical diagnostics from the resulting matrices $Q$ and $R$.\n\n**1. Matrix Construction**\n\nFor each scalar perturbation parameter $\\delta$ from the test suite, we construct a square matrix $A \\in \\mathbb{R}^{n \\times m}$ with dimensions $n=m=12$.\nThe first column, $a_1 \\in \\mathbb{R}^{12}$, is defined by its components $(a_1)_j = 1/j$ for $j=1, \\dots, 12$.\nSubsequent columns are generated recursively. For $i=1, \\dots, 11$, the $(i+1)$-th column is defined as $a_{i+1} = a_i + \\delta e_i$, where $e_i$ is the $i$-th standard basis vector (a vector of zeros with a $1$ at the $i$-th position).\n\nThis construction ensures that for small values of $\\delta$, adjacent columns $a_i$ and $a_{i+1}$ are very close to each other, making the set of column vectors nearly linearly dependent. The matrix $A$ thus becomes increasingly ill-conditioned as $\\delta \\to 0$. This is the key mechanism for testing the numerical limits of the orthonormalization algorithm. For values of $\\delta$ smaller than the relative machine precision (approximately $10^{-16}$ for double-precision), the term $\\delta e_i$ may be lost during the floating-point addition $a_i + \\delta e_i$, resulting in numerically identical columns, i.e., $a_{i+1} = a_i$.\n\n**2. Modified Gram-Schmidt (MGS) Algorithm**\n\nThe Gram-Schmidt process transforms a set of linearly independent vectors $\\{a_1, \\dots, a_m\\}$ into an orthonormal set $\\{q_1, \\dots, q_m\\}$ that spans the same subspace. The process can be expressed as a matrix factorization $A=QR$, where $Q$ has orthonormal columns $q_i$ and $R$ is an upper triangular matrix.\n\nThe Modified Gram-Schmidt (MGS) algorithm is a specific implementation that is known for its superior numerical stability compared to the Classical Gram-Schmidt (CGS) algorithm. The key difference lies in the order of operations. MGS orthogonalizes the remaining vectors against each new orthonormal vector $q_i$ as soon as it is computed. This reduces the accumulation of rounding errors.\n\nThe MGS algorithm is implemented as follows, where we start with a working copy of the matrix $A$, denoted as $V$:\nInitialize $V=A$.\nFor $i = 1, \\dots, m$:\n1.  Compute the $2$-norm of the current vector $v_i$: $R_{ii} = \\|v_i\\|_2$.\n2.  If $R_{ii} > 0$, normalize the vector to produce the next orthonormal vector: $q_i = v_i / R_{ii}$.\n3.  If $R_{ii} = 0$, the vector $a_i$ is linearly dependent on the preceding vectors. The resulting orthonormal vector $q_i$ is a zero vector, and no projections are performed in this step.\n4.  For all subsequent vectors $v_j$ (where $j = i+1, \\dots, m$), subtract their projection onto the new orthonormal vector $q_i$. This is done by first calculating the projection coefficient $R_{ij} = q_i^{\\top} v_j$ and then updating the vector $v_j \\leftarrow v_j - R_{ij} q_i$.\n\nThis process populates the columns of $Q$ and the upper triangular entries of $R$.\n\n**3. Diagnostic Computations**\n\nFor each value of $\\delta$, after computing $Q(\\delta)$ and $R(\\delta)$, two diagnostic quantities are calculated to assess the numerical behavior:\n\n-   **Minimum Positive Diagonal of $R$ ($\\min\\{R_{ii}\\}$):** The diagonal entries $R_{ii}$ represent the norm of each vector after it has been made orthogonal to all preceding vectors in the set. A small value of $R_{ii}$ indicates that the original vector $a_i$ was nearly a linear combination of the vectors $\\{a_1, \\dots, a_{i-1}\\}$. We compute $\\min\\{R_{ii}(\\delta) \\mid R_{ii}(\\delta)>0, \\, i=1,\\dots,m\\}$. This metric gauges the proximity to linear dependence in the column set of $A(\\delta)$.\n\n-   **Maximum Adjacent-Column Inner Product ($\\max\\{|q_i^{\\top}q_{i+1}|\\}$):** In exact arithmetic, the columns of $Q$ are perfectly orthonormal, meaning $q_i^{\\top}q_j = 0$ for $i \\neq j$. In finite-precision arithmetic, rounding errors lead to a loss of orthogonality. This is measured by computing the inner products of the resulting vectors. We calculate $\\max\\{|q_i(\\delta)^{\\top}q_{i+1}(\\delta)| \\mid \\lVert q_i(\\delta)\\rVert_2>0,\\ \\lVert q_{i+1}(\\delta)\\rVert_2>0,\\ i=1,\\dots,m-1\\}$. This value quantifies the worst-case loss of orthogonality between adjacent columns, a known indicator of numerical instability.\n\nBy executing this procedure for the specified test suite of $\\delta$ values, we can observe how the numerical quality of the MGS factorization degrades as the input matrix approaches singularity. We expect $\\min\\{R_{ii}\\}$ to decrease and $\\max\\{|q_i^{\\top}q_{i+1}|\\}$ to increase as $\\delta$ becomes smaller. For $\\delta$ below machine precision, we anticipate a breakdown where columns become numerically identical, leading to zero entries on the diagonal of $R$ and a zero value for the orthogonality measure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing matrices for different delta values,\n    applying Modified Gram-Schmidt, and computing the required diagnostics.\n    \"\"\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Performs Modified Gram-Schmidt orthonormalization on the columns of matrix A.\n\n        Args:\n            A (np.ndarray): The input matrix with dimensions n x m.\n\n        Returns:\n            tuple[np.ndarray, np.ndarray]: A tuple (Q, R) where Q is an n x m matrix\n            with orthonormal columns and R is an m x m upper triangular matrix.\n        \"\"\"\n        n, m = A.shape\n        # Create a working copy of A to be modified in place.\n        V = A.copy()\n        Q = np.zeros((n, m), dtype=float)\n        R = np.zeros((m, m), dtype=float)\n\n        for i in range(m):\n            # Compute the norm of the current vector.\n            R[i, i] = np.linalg.norm(V[:, i])\n\n            # Normalize to get the i-th orthonormal vector, if possible.\n            if R[i, i] > 0:\n                Q[:, i] = V[:, i] / R[i, i]\n            else:\n                # The column is linearly dependent on previous columns.\n                # Q[:, i] remains a zero vector, and no projections can be done.\n                continue\n\n            # Orthogonalize all subsequent vectors against the new orthonormal vector.\n            for j in range(i + 1, m):\n                R[i, j] = np.dot(Q[:, i], V[:, j])\n                V[:, j] -= R[i, j] * Q[:, i]\n\n        return Q, R\n\n    # Define the test cases from the problem statement.\n    test_cases = [1e-1, 1e-8, 1e-12, 1e-16, 1e-20]\n    n = 12\n    m = 12\n    results = []\n\n    for delta in test_cases:\n        # 1. Construct the matrix A for the given delta.\n        A = np.zeros((n, m), dtype=float)\n        \n        # Define the first column a_1.\n        A[:, 0] = 1.0 / np.arange(1, n + 1)\n        \n        # Recursively define subsequent columns a_{i+1} = a_i + delta * e_i.\n        for i in range(m - 1):\n            # e_i is the i-th standard basis vector (0-indexed).\n            e_i = np.zeros(n, dtype=float)\n            e_i[i] = 1.0\n            A[:, i + 1] = A[:, i] + delta * e_i\n\n        # 2. Perform Modified Gram-Schmidt factorization.\n        Q, R = modified_gram_schmidt(A)\n\n        # 3. Compute the two required diagnostics.\n        \n        # Diagnostic 1: Minimum positive diagonal entry of R.\n        diag_R = np.diag(R)\n        positive_diagonals = diag_R[diag_R > 0]\n        if positive_diagonals.size > 0:\n            min_diag = np.min(positive_diagonals)\n        else:\n            min_diag = 0.0\n\n        # Diagnostic 2: Maximum adjacent-column inner product magnitude of Q.\n        max_adj_dot = 0.0\n        for i in range(m - 1):\n            q_i = Q[:, i]\n            q_i_plus_1 = Q[:, i + 1]\n            \n            # Per problem spec, consider only non-zero vectors.\n            if np.linalg.norm(q_i) > 0 and np.linalg.norm(q_i_plus_1) > 0:\n                dot_product = np.abs(np.dot(q_i, q_i_plus_1))\n                if dot_product > max_adj_dot:\n                    max_adj_dot = dot_product\n        \n        result_tuple = [delta, min_diag, max_adj_dot]\n        results.append(result_tuple)\n\n    # Final print statement in the exact required format.\n    # The format is [[d1, m1, a1],[d2, m2, a2],...], which is created\n    # by joining the string representation of each inner list.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established the reliability of the Modified Gram-Schmidt process, we now apply it to solve a classic and important problem: polynomial interpolation. Finding an interpolating polynomial often requires solving a linear system $Vc=y$, where $V$ is a Vandermonde matrix that can be notoriously ill-conditioned, making direct inversion numerically unstable. This practice  guides you to use the stable QR factorization, computed via MGS, to find the polynomial's coefficients. This approach elegantly bypasses the instability of the Vandermonde matrix, showcasing MGS as a critical building block for robust numerical algorithms.",
            "id": "3253110",
            "problem": "You are given a set of nodes and corresponding function values and asked to compute the coefficients of the unique interpolating polynomial expressed in the monomial basis. The intended numerical strategy is to form the Vandermonde matrix on the given nodes and use the modified Gram-Schmidt (MGS) process to orthogonalize its columns with respect to the standard discrete inner product. Then, use the resulting orthogonal-triangular factorization to compute the interpolation coefficients stably without explicitly inverting any matrix.\n\nFoundational base to use:\n- The discrete inner product of two real column vectors $u \\in \\mathbb{R}^m$ and $v \\in \\mathbb{R}^m$ is $\\langle u, v \\rangle = \\sum_{i=1}^m u_i v_i$.\n- The Vandermonde matrix on nodes $x_0, x_1, \\dots, x_{m-1}$ for a polynomial of degree at most $n-1$ has entries $V_{i,j} = x_i^j$ for $i = 0, \\dots, m-1$ and $j = 0, \\dots, n-1$.\n- If the nodes are distinct and $m = n$, the Vandermonde matrix is nonsingular, and there is a unique interpolating polynomial $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ satisfying $p(x_i) = y_i$ for all $i$.\n- The modified Gram-Schmidt (MGS) process constructs an orthonormal set of vectors from a linearly independent set by iteratively subtracting projections using the discrete inner product, which is more numerically stable than classical Gram-Schmidt.\n\nYour task:\n1. Implement the modified Gram-Schmidt (MGS) procedure to orthonormalize the columns of a given real matrix $V \\in \\mathbb{R}^{m \\times n}$ with respect to the discrete inner product, producing an orthonormal $Q \\in \\mathbb{R}^{m \\times n}$ and an upper-triangular $R \\in \\mathbb{R}^{n \\times n}$ such that $V = Q R$.\n2. For each test case, construct the square Vandermonde matrix $V \\in \\mathbb{R}^{n \\times n}$ using the given nodes $x_0, \\dots, x_{n-1}$ and the monomial basis $\\{1, x, x^2, \\dots, x^{n-1}\\}$.\n3. Use your MGS factorization to compute the monomial coefficients $c_0, \\dots, c_{n-1}$ of the unique interpolating polynomial $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ that satisfies $p(x_i) = y_i$ for all nodes in the test case. Do this by exploiting the orthonormality produced by MGS together with an appropriate triangular solve; do not invert any matrix explicitly.\n4. Report the coefficient vector in ascending order of powers, i.e., $[c_0, c_1, \\dots, c_{n-1}]$. Round each coefficient to $10$ decimal places and print the results as a single line containing a list of lists. Use standard decimal notation (no scientific notation), and do not include any extra text.\n\nAngle units and physical units are not applicable in this task. All inputs and outputs are real numbers without units.\n\nTest suite:\nProvide your programâ€™s outputs for the following four interpolation data sets. In each case, nodes are distinct and define a square Vandermonde system.\n- Case A (happy path quadratic): nodes $x = [-1.0, 0.0, 1.0]$, values $y = [-2.0, 2.0, 4.0]$.\n- Case B (cubic with mixed signs): nodes $x = [0.0, 1.0, 2.0, 3.0]$, values $y = [1.0, 0.0, -3.0, -20.0]$.\n- Case C (mildly ill-conditioned geometry): nodes $x = [0.0, 0.0001, 0.0002]$, values $y = [1.0, 1.00020003, 1.00040012]$.\n- Case D (boundary, constant polynomial): nodes $x = [7.5]$, values $y = [4.2]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, with each inner list giving the rounded coefficients for one test case, in the same order as the cases above. For example, the output must have the shape\n$[[c^{(A)}_0, \\dots], [c^{(B)}_0, \\dots], [c^{(C)}_0, \\dots], [c^{(D)}_0, \\dots]]$\nand it must be printed on one line exactly as a Python-like list literal, with each coefficient rounded to $10$ decimal places.",
            "solution": "The user-provided problem has been analyzed and is determined to be **valid**. It is a well-posed problem in numerical linear algebra, resting on established mathematical principles and containing all necessary information for a unique solution.\n\nThe task is to find the coefficients $c = [c_0, c_1, \\dots, c_{n-1}]^T$ of an interpolating polynomial $p(x) = \\sum_{j=0}^{n-1} c_j x^j$. Given a set of $n$ distinct nodes $\\{x_i\\}_{i=0}^{n-1}$ and corresponding values $\\{y_i\\}_{i=0}^{n-1}$, the interpolation conditions $p(x_i) = y_i$ for $i=0, \\dots, n-1$ form a system of linear equations. This system can be expressed in matrix form as $Vc = y$, where $V$ is the Vandermonde matrix, $c$ is the vector of unknown coefficients, and $y$ is the vector of function values.\n\nFor a basis of monomials $\\{1, x, x^2, \\dots, x^{n-1}\\}$, the Vandermonde matrix $V \\in \\mathbb{R}^{n \\times n}$ has entries $V_{ij} = x_i^j$ for $i, j \\in \\{0, 1, \\dots, n-1\\}$. The linear system is:\n$$\n\\begin{pmatrix}\nx_0^0 & x_0^1 & \\dots & x_0^{n-1} \\\\\nx_1^0 & x_1^1 & \\dots & x_1^{n-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n-1}^0 & x_{n-1}^1 & \\dots & x_{n-1}^{n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nc_0 \\\\\nc_1 \\\\\n\\vdots \\\\\nc_{n-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_{n-1}\n\\end{pmatrix}\n$$\nDirectly solving this system by inverting $V$ is numerically unstable, especially when nodes are close together, as $V$ becomes ill-conditioned. A more robust method, as specified, is to use a QR factorization of $V$. The problem requires this factorization to be computed using the Modified Gram-Schmidt (MGS) algorithm due to its superior numerical stability over the classical version.\n\nThe MGS algorithm produces a factorization $V = QR$, where $Q \\in \\mathbb{R}^{n \\times n}$ is a matrix with orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ is an upper-triangular matrix. The columns of $Q$ form an orthonormal basis for the column space of $V$. The orthonormality of $Q$'s columns means that $Q^T Q = I$, where $I$ is the $n \\times n$ identity matrix.\n\nThe procedure is as follows:\n1.  **Modified Gram-Schmidt (MGS) Factorization**: Let the columns of $V$ be denoted by $v_0, v_1, \\dots, v_{n-1}$. We compute the columns of $Q$, denoted $q_0, q_1, \\dots, q_{n-1}$, and the entries of $R$ iteratively.\n    For $j = 0, \\dots, n-1$:\n    a. The diagonal element of $R$ is the norm of the current vector: $R_{jj} = \\|v_j\\|_2$.\n    b. The $j$-th column of $Q$ is the normalized vector: $q_j = v_j / R_{jj}$.\n    c. For all subsequent vectors $v_k$ (where $k > j$), we subtract the projection onto $q_j$. The coefficient of projection is $R_{jk} = \\langle q_j, v_k \\rangle = q_j^T v_k$.\n    d. The remaining vectors are updated: $v_k \\leftarrow v_k - R_{jk} q_j$ for $k = j+1, \\dots, n-1$.\n    The crucial feature of MGS is that the projection in step (d) uses the newly orthogonalized vector $q_j$ to update the remaining vectors, reducing the loss of orthogonality that affects Classical Gram-Schmidt.\n\n2.  **Solving the System**: We substitute $V = QR$ into the linear system $Vc = y$:\n    $$QRc = y$$\n    Multiplying from the left by $Q^T$ gives:\n    $$Q^T Q R c = Q^T y$$\n    Since $Q^T Q = I$, the system simplifies to:\n    $$Rc = Q^T y$$\n    Let $z = Q^T y$. The system becomes $Rc = z$, which is an upper-triangular system:\n    $$\n    \\begin{pmatrix}\n    R_{00} & R_{01} & \\dots & R_{0,n-1} \\\\\n    0 & R_{11} & \\dots & R_{1,n-1} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & \\dots & R_{n-1,n-1}\n    \\end{pmatrix}\n    \\begin{pmatrix}\n    c_0 \\\\\n    c_1 \\\\\n    \\vdots \\\\\n    c_{n-1}\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    z_0 \\\\\n    z_1 \\\\\n    \\vdots \\\\\n    z_{n-1}\n    \\end{pmatrix}\n    $$\n\n3.  **Back Substitution**: This triangular system is efficiently solved for $c$ using back substitution, starting from the last row and moving upwards.\n    $$c_{n-1} = \\frac{z_{n-1}}{R_{n-1,n-1}}$$\n    And for $i = n-2, \\dots, 0$:\n    $$c_i = \\frac{1}{R_{ii}} \\left( z_i - \\sum_{j=i+1}^{n-1} R_{ij} c_j \\right)$$\n    This process yields the desired coefficient vector $c$ without computing any matrix inverses, providing a numerically stable solution.\n\nThe final coefficients are rounded to $10$ decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes interpolating polynomial coefficients using MGS QR factorization.\n    \"\"\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Computes the QR factorization of a matrix A using the Modified Gram-Schmidt algorithm.\n\n        Args:\n            A (np.ndarray): A matrix of shape (m, n) with linearly independent columns.\n\n        Returns:\n            Q (np.ndarray): A matrix of shape (m, n) with orthonormal columns.\n            R (np.ndarray): An upper triangular matrix of shape (n, n) such that A = QR.\n        \"\"\"\n        m, n = A.shape\n        Q = np.zeros((m, n), dtype=float)\n        R = np.zeros((n, n), dtype=float)\n        \n        # Use a copy of A for the vectors that are iteratively modified.\n        v = A.copy().astype(float)\n\n        for j in range(n):\n            # Calculate the norm of the j-th vector for the diagonal of R.\n            norm_vj = np.linalg.norm(v[:, j])\n            \n            # Since nodes are distinct, V is non-singular and norm will not be zero.\n            R[j, j] = norm_vj\n            # Normalize the vector to get the j-th column of Q.\n            Q[:, j] = v[:, j] / R[j, j]\n            \n            # Orthogonalize remaining vectors against the new orthonormal vector q_j.\n            for k in range(j + 1, n):\n                # Calculate the projection coefficient, which is an element of R.\n                R[j, k] = np.dot(Q[:, j], v[:, k])\n                # Subtract the projection from the k-th vector.\n                v[:, k] = v[:, k] - R[j, k] * Q[:, j]\n                \n        return Q, R\n\n    def back_substitution(R, z):\n        \"\"\"\n        Solves the upper triangular system Rc = z for c.\n\n        Args:\n            R (np.ndarray): An upper triangular square matrix.\n            z (np.ndarray): A vector.\n\n        Returns:\n            np.ndarray: The solution vector c.\n        \"\"\"\n        n = R.shape[0]\n        c = np.zeros(n, dtype=float)\n        for i in range(n - 1, -1, -1):\n            # Calculate the sum of R_ij * c_j for j > i\n            s = np.dot(R[i, i + 1:], c[i + 1:])\n            # R is non-singular, so R[i, i] is non-zero.\n            c[i] = (z[i] - s) / R[i, i]\n        return c\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path quadratic\n        {'x': [-1.0, 0.0, 1.0], 'y': [-2.0, 2.0, 4.0]},\n        # Case B: cubic with mixed signs\n        {'x': [0.0, 1.0, 2.0, 3.0], 'y': [1.0, 0.0, -3.0, -20.0]},\n        # Case C: mildly ill-conditioned geometry\n        {'x': [0.0, 0.0001, 0.0002], 'y': [1.0, 1.00020003, 1.00040012]},\n        # Case D: boundary, constant polynomial\n        {'x': [7.5], 'y': [4.2]},\n    ]\n    \n    all_results = []\n    \n    for case in test_cases:\n        x_nodes = np.array(case['x'], dtype=float)\n        y_values = np.array(case['y'], dtype=float)\n        \n        n = len(x_nodes)\n        \n        # 1. Construct the Vandermonde matrix.\n        #    increasing=True corresponds to the basis {1, x, x^2, ...}.\n        V = np.vander(x_nodes, N=n, increasing=True)\n        \n        # 2. Perform MGS QR factorization.\n        Q, R = modified_gram_schmidt(V)\n        \n        # 3. Compute z = Q^T * y.\n        z = Q.T @ y_values\n        \n        # 4. Solve Rc = z using back substitution.\n        c = back_substitution(R, z)\n        \n        # 5. Round coefficients to 10 decimal places and store.\n        rounded_c = [round(val, 10) for val in c]\n        all_results.append(rounded_c)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists of floats.\n    print(all_results)\n\nsolve()\n```"
        }
    ]
}