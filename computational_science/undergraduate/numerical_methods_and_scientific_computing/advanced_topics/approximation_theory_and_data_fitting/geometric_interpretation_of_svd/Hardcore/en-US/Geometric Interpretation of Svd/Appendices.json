{
    "hands_on_practices": [
        {
            "introduction": "The Singular Value Decomposition (SVD) provides a powerful lens for understanding how a matrix transforms geometric shapes. This first practice invites you to explore the action of a projection matrix, a fundamental operator in linear algebra. By finding the SVD of a matrix that projects vectors onto a line, you will see firsthand how a zero singular value corresponds to the collapse of a dimension, transforming the unit circle into a degenerate ellipse, or line segment .",
            "id": "3234717",
            "problem": "Consider the linear map $P:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ that orthogonally projects every vector onto the line $y=2x$. Starting from the definitions of the Euclidean inner product, orthogonal projection onto a one-dimensional subspace, and the Singular Value Decomposition (SVD) as a factorization $A=U\\Sigma V^\\top$ with $U$ and $V$ orthogonal and $\\Sigma$ diagonal with nonnegative entries, do the following:\n\n- Construct the matrix representation of $P$ by expressing it in terms of a unit direction vector for the line $y=2x$.\n- Determine the SVD of $P$ by identifying its singular values and the corresponding left and right singular vectors from first principles.\n- Using the geometric interpretation of SVD, describe how $P$ transforms the unit circle in $\\mathbb{R}^{2}$, including the semi-axis lengths and directions of the resulting image, and comment on whether the image is a degenerate ellipse.\n\nExpress your final answer as the ordered pair of singular values $(\\sigma_{1},\\sigma_{2})$ in a single row using the $\\mathrm{pmatrix}$ environment. No rounding is required.",
            "solution": "The problem asks for the construction of a projection matrix $P$, its Singular Value Decomposition (SVD), and a geometric interpretation of its action on the unit circle.\n\nFirst, we construct the matrix representation of the linear map $P$ that orthogonally projects vectors in $\\mathbb{R}^{2}$ onto the line $y=2x$.\nA direction vector for the line $y=2x$ is $\\mathbf{d} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. To create an orthogonal projection matrix, we first need a unit vector in this direction. The norm of $\\mathbf{d}$ is $\\|\\mathbf{d}\\| = \\sqrt{1^{2} + 2^{2}} = \\sqrt{5}$.\nThe unit direction vector is thus $\\mathbf{u} = \\frac{\\mathbf{d}}{\\|\\mathbf{d}\\|} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe matrix $P$ for orthogonal projection onto the one-dimensional subspace spanned by a unit vector $\\mathbf{u}$ is given by the outer product $P = \\mathbf{u}\\mathbf{u}^\\top$.\nSubstituting our vector $\\mathbf{u}$:\n$$P = \\left(\\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1  2 \\end{pmatrix}\\right) = \\frac{1}{5}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\begin{pmatrix} 1  2 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 1 \\cdot 1  1 \\cdot 2 \\\\ 2 \\cdot 1  2 \\cdot 2 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix}$$\nThis is the matrix representation of the projection $P$.\n\nNext, we determine the SVD of $P$. The SVD is a factorization $P = U\\Sigma V^\\top$. The singular values $\\sigma_{i}$, which form the diagonal of $\\Sigma$, are the square roots of the eigenvalues of the matrix $P^\\top P$.\nThe matrix $P$ is symmetric, i.e., $P^\\top = P$. Therefore, $P^\\top P = P^{2}$.\nFor a projection matrix, it is an idempotent operator, meaning $P^{2} = P$. We can verify this:\n$$P^{2} = \\left(\\frac{1}{5}\\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix}\\right)\\left(\\frac{1}{5}\\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix}\\right) = \\frac{1}{25}\\begin{pmatrix} 1(1)+2(2)  1(2)+2(4) \\\\ 2(1)+4(2)  2(2)+4(4) \\end{pmatrix} = \\frac{1}{25}\\begin{pmatrix} 5  10 \\\\ 10  20 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix} = P$$\nThus, we need to find the eigenvalues of $P$ itself. The eigenvalues of a projection matrix are always $1$ and $0$.\n- Any vector lying on the line of projection is an eigenvector with eigenvalue $\\lambda=1$, since $P\\mathbf{x} = \\mathbf{x}$. The eigenspace for $\\lambda_1=1$ is the line $y=2x$.\n- Any vector orthogonal to the line of projection is mapped to the zero vector, so it is an eigenvector with eigenvalue $\\lambda=0$. The eigenspace for $\\lambda_2=0$ is the line orthogonal to $y=2x$, which is $y=-x/2$.\n\nThe eigenvalues of $P$ are $\\lambda_{1}=1$ and $\\lambda_{2}=0$. The singular values are the square roots of these eigenvalues, ordered non-increasingly:\n$\\sigma_{1} = \\sqrt{\\lambda_{1}} = \\sqrt{1} = 1$\n$\\sigma_{2} = \\sqrt{\\lambda_{2}} = \\sqrt{0} = 0$\nSo, the matrix $\\Sigma$ is $\\Sigma = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\n\nThe columns of $V$ (the right singular vectors, $\\mathbf{v}_i$) are the orthonormal eigenvectors of $P^\\top P = P$.\n- For $\\lambda_{1}=1$, the eigenvector is any vector along the line $y=2x$. We choose the unit vector $\\mathbf{v}_{1} = \\mathbf{u} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n- For $\\lambda_{2}=0$, the eigenvector must be orthogonal to $\\mathbf{v}_1$. A vector orthogonal to $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ is $\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$. Normalizing this vector gives $\\mathbf{v}_{2} = \\frac{1}{\\sqrt{(-2)^{2}+1^{2}}}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$.\nThe matrix $V$ is formed by these vectors as columns: $V = \\begin{pmatrix} \\mathbf{v}_{1}  \\mathbf{v}_{2} \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1  -2 \\\\ 2  1 \\end{pmatrix}$.\n\nThe columns of $U$ (the left singular vectors, $\\mathbf{u}_i$) are determined by the relation $P\\mathbf{v}_i = \\sigma_i \\mathbf{u}_i$.\n- For $\\sigma_{1}=1$: $\\mathbf{u}_{1} = \\frac{1}{\\sigma_{1}}P\\mathbf{v}_{1} = \\frac{1}{1}P\\mathbf{v}_{1}$. Since $\\mathbf{v}_1$ is an eigenvector of $P$ with eigenvalue $1$, $P\\mathbf{v}_1 = \\mathbf{v}_1$. So, $\\mathbf{u}_{1} = \\mathbf{v}_{1} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n- For $\\sigma_{2}=0$: the relation is $P\\mathbf{v}_2 = 0\\cdot\\mathbf{u}_2 = \\mathbf{0}$, which is true since $\\mathbf{v}_2$ is in the null space of $P$. $\\mathbf{u}_2$ must be a unit vector orthogonal to $\\mathbf{u}_1$. We can choose $\\mathbf{u}_2 = \\mathbf{v}_2 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$.\nThe matrix $U$ is formed by these vectors as columns: $U = \\begin{pmatrix} \\mathbf{u}_{1}  \\mathbf{u}_{2} \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1  -2 \\\\ 2  1 \\end{pmatrix}$.\nNote that since $P$ is symmetric positive semi-definite, it is possible to choose $U=V$.\n\nThe SVD of $P$ is $P = U\\Sigma V^\\top = \\left(\\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1  -2 \\\\ 2  1 \\end{pmatrix}\\right) \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\left(\\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1  2 \\\\ -2  1 \\end{pmatrix}\\right)$.\n\nFinally, we provide the geometric interpretation of how $P$ transforms the unit circle in $\\mathbb{R}^{2}$. The SVD provides a clear picture of this transformation. The action of any matrix $A=U\\Sigma V^\\top$ on the unit circle can be seen as a sequence of three operations:\n1. A rotation/reflection by $V^\\top$, which aligns the standard basis with the right singular vectors $\\mathbf{v}_i$.\n2. A scaling along these new axes, where the scaling factor along axis $\\mathbf{v}_i$ is the singular value $\\sigma_i$.\n3. A rotation/reflection by $U$, which aligns the scaled axes with the left singular vectors $\\mathbf{u}_i$.\n\nFor the matrix $P$, the unit circle is transformed into an ellipse, whose semi-axes are given by the vectors $\\sigma_{i}\\mathbf{u}_{i}$.\n- The first semi-axis has length $\\sigma_{1}=1$ and direction $\\mathbf{u}_{1} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. This vector lies along the line $y=2x$.\n- The second semi-axis has length $\\sigma_{2}=0$ and direction $\\mathbf{u}_{2} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$. This vector is orthogonal to the line $y=2x$.\n\nSince one of the semi-axis lengths is $0$, the resulting image is a **degenerate ellipse**. The transformation collapses the entire unit circle onto a one-dimensional object: a line segment. This line segment lies on the line $y=2x$ and is defined by the set of points $\\{c\\mathbf{u}_1 : c \\in [-1, 1]\\}$. The endpoints of the segment are $-\\mathbf{u}_1$ and $\\mathbf{u}_1$, so it stretches from $(-\\frac{1}{\\sqrt{5}}, -\\frac{2}{\\sqrt{5}})$ to $(\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}})$. This outcome is expected, as an orthogonal projection onto a line must map all points in the plane onto that line.\n\nThe ordered pair of singular values is $(\\sigma_{1}, \\sigma_{2})=(1, 0)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having seen how SVD deconstructs a transformation, we now reverse the process. This exercise challenges you to synthesize a matrix that performs a specific geometric task: transforming the unit circle into an ellipse with a prescribed size and orientation. By assembling the SVD components—rotations ($U, V^\\top$) and scaling ($\\Sigma$)—you will gain a deeper, constructive understanding of how these elements work together as a powerful design tool for linear transformations .",
            "id": "3234724",
            "problem": "Consider the geometric action of a real $2 \\times 2$ matrix $A$ on the unit circle $\\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} = 1\\}$. Using only the foundational definitions of the singular value decomposition (SVD) and its geometric interpretation as a composition of orthogonal transformations and axis-aligned scalings, construct an explicit real $2 \\times 2$ matrix $A$ whose image of the unit circle is the ellipse centered at the origin with major semi-axis of length $\\sigma_{1} = 3$ and minor semi-axis of length $\\sigma_{2} = 1$, whose principal axes are tilted by $45$ degrees (equivalently $\\pi/4$ radians) counterclockwise relative to the $x$-axis. Among all possible matrices with this property, choose the one whose right singular vectors coincide with the standard basis of $\\mathbb{R}^{2}$. Provide the explicit matrix $A$. No rounding is required.",
            "solution": "The problem asks for the construction of a specific real $2 \\times 2$ matrix $A$ based on its geometric action on the unit circle and a constraint on its singular value decomposition (SVD).\n\nFirst, we validate the problem statement.\nThe givens are:\n1.  A is a real $2 \\times 2$ matrix.\n2.  The image of the unit circle $\\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} = 1\\}$ under the transformation $A$ is an ellipse.\n3.  The ellipse is centered at the origin.\n4.  The major semi-axis of the ellipse has length $\\sigma_1 = 3$.\n5.  The minor semi-axis of the ellipse has length $\\sigma_2 = 1$.\n6.  The principal axes of the ellipse are tilted by an angle of $\\pi/4$ radians ($45$ degrees) counterclockwise relative to the standard Cartesian axes.\n7.  The right singular vectors of $A$ are the standard basis vectors of $\\mathbb{R}^2$.\n\nThe problem is scientifically grounded in the theory of linear algebra, specifically the singular value decomposition. It is objective and well-posed, provided the description of the ellipse's orientation is interpreted in the standard geometric sense. This interpretation removes ambiguity and leads to a unique solution. The problem is therefore deemed valid.\n\nWe proceed with the solution by constructing the matrix $A$ from its SVD, which has the form $A = U\\Sigma V^\\top$. The geometric action of $A$ on a vector $x$ can be understood as a sequence of three operations: a rotation/reflection by $V^\\top$, an axis-aligned scaling by $\\Sigma$, and another rotation/reflection by $U$. The image of the unit circle under $A$ is an ellipse whose principal axes are aligned with the columns of $U$ (the left singular vectors) and whose semi-axis lengths are the singular values on the diagonal of $\\Sigma$.\n\n1.  **Determine the matrix of singular values, $\\Sigma$.**\n    The lengths of the semi-axes of the ellipse are given by the singular values of $A$. The problem specifies a major semi-axis of length $\\sigma_1 = 3$ and a minor semi-axis of length $\\sigma_2 = 1$. By convention, the singular values are ordered in descending magnitude. Thus, the diagonal matrix $\\Sigma$ is:\n    $$\n    \\Sigma = \\begin{pmatrix} \\sigma_1  0 \\\\ 0  \\sigma_2 \\end{pmatrix} = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}\n    $$\n\n2.  **Determine the matrix of right singular vectors, $V$.**\n    The columns of the orthogonal matrix $V$ are the right singular vectors of $A$. The problem explicitly states that these are the standard basis vectors of $\\mathbb{R}^2$. Let $\\mathbf{v}_1 = \\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\mathbf{v}_2 = \\mathbf{e}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n    Therefore, the matrix $V$ is the $2 \\times 2$ identity matrix:\n    $$\n    V = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I\n    $$\n    Consequently, its transpose $V^\\top$ is also the identity matrix, $V^\\top = I$.\n\n3.  **Determine the matrix of left singular vectors, $U$.**\n    The columns of the orthogonal matrix $U$ are the left singular vectors of $A$, denoted $\\mathbf{u}_1$ and $\\mathbf{u}_2$. These vectors define the directions of the principal axes of the resulting ellipse. The vector $\\mathbf{u}_1$ corresponds to the major axis (length $\\sigma_1 = 3$), and $\\mathbf{u}_2$ corresponds to the minor axis (length $\\sigma_2 = 1$).\n    The problem states that the principal axes are tilted by $\\pi/4$ radians counterclockwise relative to the standard axes. This implies that the orthonormal basis $(\\mathbf{u}_1, \\mathbf{u}_2)$ is obtained by rotating the standard basis $(\\mathbf{e}_1, \\mathbf{e}_2)$ by an angle of $\\pi/4$. The matrix representing this rotation is the standard $2 \\times 2$ rotation matrix $R_{\\theta}$ with $\\theta = \\pi/4$. This matrix is our matrix $U$.\n    The columns of $U$ are:\n    $$\n    \\mathbf{u}_1 = R_{\\pi/4} \\mathbf{e}_1 = \\begin{pmatrix} \\cos(\\pi/4) \\\\ \\sin(\\pi/4) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n    $$\n    $$\n    \\mathbf{u}_2 = R_{\\pi/4} \\mathbf{e}_2 = \\begin{pmatrix} -\\sin(\\pi/4) \\\\ \\cos(\\pi/4) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n    $$\n    Thus, the matrix $U$ is:\n    $$\n    U = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\\\ 1  1 \\end{pmatrix}\n    $$\n\n4.  **Construct the matrix $A$.**\n    Now we assemble the matrix $A$ using the determined components $U$, $\\Sigma$, and $V^\\top$.\n    $$\n    A = U \\Sigma V^\\top = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\\\ 1  1 \\end{pmatrix} \\right) \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}^\\top\n    $$\n    Since $V^\\top = I$, the expression simplifies to $A = U\\Sigma$.\n    $$\n    A = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}\n    $$\n    Performing the matrix multiplication:\n    $$\n    A = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} (1)(3) + (-1)(0)  (1)(0) + (-1)(1) \\\\ (1)(3) + (1)(0)  (1)(0) + (1)(1) \\end{pmatrix}\n    $$\n    $$\n    A = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3  -1 \\\\ 3  1 \\end{pmatrix}\n    $$\n    Writing the matrix with explicit components:\n    $$\n    A = \\begin{pmatrix} \\frac{3}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\\\ \\frac{3}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n    $$\n    This is the explicit real $2 \\times 2$ matrix that satisfies all the conditions given in the problem statement.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{3}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\\\ \\frac{3}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "We conclude by applying our geometric understanding of SVD to a classic problem in scientific computing: aligning two 3D point clouds. This task, known as the orthogonal Procrustes problem, is crucial in fields like structural biology for comparing molecular conformations. In this hands-on coding exercise, you will use SVD to derive and implement the Kabsch algorithm, which finds the optimal rotation to superimpose two sets of points, demonstrating the power of SVD to solve complex real-world optimization problems .",
            "id": "3234685",
            "problem": "You are given two sets of three-dimensional points representing the same molecule in different spatial conformations. Let $X \\in \\mathbb{R}^{N \\times 3}$ and $Y \\in \\mathbb{R}^{N \\times 3}$ denote the two point clouds, each row being a point. The task is to compute the optimal rotation matrix $R \\in \\mathbb{R}^{3 \\times 3}$ with $R^\\top R = I$ and $\\det(R) = 1$ that minimizes the squared Frobenius-norm discrepancy between the centered point clouds. Start from the following fundamental base: the definition of the Frobenius norm, the definition of an orthogonal matrix, and the definition of Singular Value Decomposition (SVD). Derive why the minimizer is obtained by aligning the principal directions of the cross-covariance of the centered clouds using Singular Value Decomposition (SVD), ensuring a proper rotation with determinant equal to $1$.\n\nAfter computing $R$, apply it to align $X$ to $Y$ by combining rotation with translation inferred from the centroids, and compute the following diagnostics:\n1. The root-mean-square deviation (RMSD) between the aligned $X$ and $Y$.\n2. The Frobenius-norm deviation from orthogonality, $\\lVert R^\\top R - I \\rVert_F$.\n3. The determinant $\\det(R)$.\n\nAll floating-point outputs must be rounded to $6$ decimal places. No angles need to be output, but any angles used in constructing the test suite are specified in radians.\n\nUse the following test suite of four cases. For each case, the array $X$ is specified explicitly, and $Y$ is constructed from a known rigid transformation (rotation and translation), possibly including a reflection in one case to test the determinant correction. In all cases, $Y$ is constructed as $Y = X R^\\top + t$, except where an additional reflection matrix $S$ is explicitly included.\n\nTest case $1$ (happy path):\n- Points $X_1$:\n  - ($0$, $0$, $0$)\n  - ($1$, $0$, $0$)\n  - ($0$, $1$, $0$)\n  - ($0$, $0$, $1$)\n  - ($1$, $1.2$, $-0.7$)\n  - ($-0.3$, $2.1$, $-1.5$)\n- Rotation: about the $z$-axis by angle $\\theta = \\pi/6$ radians.\n- Translation $t_1$: ($1$, $-2$, $0.5$).\n- Construction: $Y_1 = X_1 R_z(\\theta)^\\top + t_1$.\n\nTest case $2$ (identity alignment):\n- Points $X_2$:\n  - ($0.2$, $-1.3$, $2$)\n  - ($3.5$, $0.4$, $-0.2$)\n  - ($1.1$, $1$, $1$)\n  - ($2.1$, $-0.9$, $0.3$)\n- Construction: $Y_2 = X_2$.\n\nTest case $3$ (reflection trap to verify determinant correction):\n- Points $X_3$:\n  - ($0$, $0$, $0$)\n  - ($1$, $2$, $3$)\n  - ($-1$, $-0.5$, $0.7$)\n  - ($0.8$, $-1.2$, $2.5$)\n- Rotation: about the $y$-axis by angle $\\theta = \\pi/4$ radians.\n- Reflection matrix $S = \\operatorname{diag}(1, 1, -1)$, i.e., reflection across the plane $z = 0$.\n- Translation $t_3$: ($-0.5$, $2$, $1$).\n- Construction: $Y_3 = X_3 R_y(\\theta)^\\top S^\\top + t_3$.\n\nTest case $4$ (degenerate geometry: two points on a line):\n- Points $X_4$:\n  - ($0$, $0$, $0$)\n  - ($2$, $0$, $0$)\n- Rotation: about the $x$-axis by angle $\\theta = \\pi/3$ radians.\n- Translation $t_4$: ($0$, $1$, $-1$).\n- Construction: $Y_4 = X_4 R_x(\\theta)^\\top + t_4$.\n\nFor each test case, compute the optimal rotation $R$ by centering $X$ and $Y$ at their centroids, forming the $3 \\times 3$ cross-covariance matrix of the centered clouds, performing Singular Value Decomposition (SVD), and enforcing $\\det(R) = 1$. Then compute:\n- The root-mean-square deviation (RMSD) between $X$ aligned by $R$ and $Y$.\n- The orthogonality deviation $\\lVert R^\\top R - I \\rVert_F$.\n- The determinant $\\det(R)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a sub-list of three floating-point numbers in the order $[\\text{RMSD}, \\lVert R^\\top R - I \\rVert_F, \\det(R)]$, rounded to $6$ decimal places, resulting in a nested list like $[[r_1,o_1,d_1],[r_2,o_2,d_2],[r_3,o_3,d_3],[r_4,o_4,d_4]]$.",
            "solution": "The user-provided problem is a valid and well-posed scientific question concerning the geometric interpretation of Singular Value Decomposition (SVD) in the context of aligning two three-dimensional point clouds. The problem is a classic instance of Procrustes analysis, specifically finding the optimal rotation to superimpose two molecular conformations. The solution is obtained via the Kabsch algorithm, which is derived below.\n\n### 1. Problem Formulation\n\nWe are given two sets of $N$ points in three-dimensional space, represented by matrices $X \\in \\mathbb{R}^{N \\times 3}$ and $Y \\in \\mathbb{R}^{N \\times 3}$. Our goal is to find the optimal rigid transformation (rotation $R$ and translation $t$) that minimizes the mean squared error between the transformed points of $X$ and the corresponding points in $Y$. The problem statement simplifies this by focusing on minimizing the discrepancy between centered point clouds, a key step we will justify. The objective is to find a rotation matrix $R \\in \\mathbb{R}^{3 \\times 3}$ that minimizes the squared Frobenius norm of the difference between the rotated centered cloud $X_c$ and the centered cloud $Y_c$:\n$$ \\min_{R} \\lVert X_c R^\\top - Y_c \\rVert_F^2 $$\nsubject to the constraints that $R$ is a proper rotation matrix, meaning it is orthogonal ($R^\\top R = I$, where $I$ is the $3 \\times 3$ identity matrix) and has a determinant of $+1$ ($\\det(R) = 1$).\n\n### 2. Decoupling Translation and Rotation\n\nFirst, we demonstrate that the problem of finding the optimal rotation and translation can be decoupled. The full objective function is to minimize the sum of squared Euclidean distances:\n$$ E(R, t) = \\sum_{i=1}^N \\lVert (x_i R^\\top + t) - y_i \\rVert_2^2 $$\nwhere $x_i$ and $y_i$ are the $i$-th row vectors (points) of $X$ and $Y$, and $t$ is the translation vector.\n\nTo find the optimal translation $t$ for a fixed rotation $R$, we differentiate $E$ with respect to $t$ and set the gradient to zero.\n$$ \\frac{\\partial E}{\\partial t} = \\sum_{i=1}^N 2(x_i R^\\top + t - y_i) = 0 $$\n$$ N t + \\left(\\sum_{i=1}^N x_i\\right) R^\\top - \\sum_{i=1}^N y_i = 0 $$\nLet the centroids of the point clouds be $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i$ and $\\bar{y} = \\frac{1}{N}\\sum_{i=1}^N y_i$. The equation becomes:\n$$ N t + N \\bar{x} R^\\top - N \\bar{y} = 0 $$\n$$ t = \\bar{y} - \\bar{x} R^\\top $$\nThis shows that the optimal translation aligns the centroid of the rotated $X$ cloud with the centroid of the $Y$ cloud. Substituting this optimal $t$ back into the objective function yields:\n$$ E(R) = \\sum_{i=1}^N \\lVert (x_i R^\\top + \\bar{y} - \\bar{x} R^\\top) - y_i \\rVert_2^2 = \\sum_{i=1}^N \\lVert (x_i - \\bar{x})R^\\top - (y_i - \\bar{y}) \\rVert_2^2 $$\nLet $X_c$ and $Y_c$ be the matrices of centered coordinates, with rows $x_{c,i} = x_i - \\bar{x}$ and $y_{c,i} = y_i - \\bar{y}$. The objective function simplifies to minimizing the squared Frobenius norm of the difference between the centered clouds, as stated in the problem:\n$$ E(R) = \\lVert X_c R^\\top - Y_c \\rVert_F^2 $$\nOur task is now to find the optimal rotation matrix $R$ that minimizes this expression.\n\n### 3. Rotation Optimization using Singular Value Decomposition (SVD)\n\nWe expand the Frobenius norm term:\n$$ \\lVert X_c R^\\top - Y_c \\rVert_F^2 = \\mathrm{tr}\\left( (X_c R^\\top - Y_c)^\\top (X_c R^\\top - Y_c) \\right) $$\n$$ = \\mathrm{tr}\\left( (R X_c^\\top - Y_c^\\top) (X_c R^\\top - Y_c) \\right) $$\n$$ = \\mathrm{tr}(R X_c^\\top X_c R^\\top - R X_c^\\top Y_c - Y_c^\\top X_c R^\\top + Y_c^\\top Y_c) $$\nUsing the linearity and cyclic property of the trace operator ($\\mathrm{tr}(A+B) = \\mathrm{tr}(A)+\\mathrm{tr}(B)$ and $\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$):\n$$ E(R) = \\mathrm{tr}(X_c^\\top X_c R^\\top R) - \\mathrm{tr}(R X_c^\\top Y_c) - \\mathrm{tr}(Y_c^\\top X_c R^\\top) + \\mathrm{tr}(Y_c^\\top Y_c) $$\nSince $R$ is orthogonal, $R^\\top R = I$. Also, $\\mathrm{tr}(M^\\top) = \\mathrm{tr}(M)$, so $\\mathrm{tr}(Y_c^\\top X_c R^\\top) = \\mathrm{tr}((Y_c^\\top X_c R^\\top)^\\top) = \\mathrm{tr}(R X_c^\\top Y_c)$. The expression becomes:\n$$ E(R) = \\lVert X_c \\rVert_F^2 + \\lVert Y_c \\rVert_F^2 - 2 \\cdot \\mathrm{tr}(R X_c^\\top Y_c) $$\nTo minimize $E(R)$, we must maximize the term $\\mathrm{tr}(R X_c^\\top Y_c)$. Let $H = X_c^\\top Y_c$ be the $3 \\times 3$ cross-covariance matrix. Our problem reduces to:\n$$ \\max_{R} \\mathrm{tr}(R H) \\quad \\text{subject to } R^\\top R = I $$\n\nWe use the Singular Value Decomposition (SVD) of $H$, which is defined as $H = U \\Sigma V^\\top$, where $U$ and $V$ are $3 \\times 3$ orthogonal matrices and $\\Sigma$ is a $3 \\times 3$ diagonal matrix with non-negative singular values on its diagonal, $\\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$, ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$.\n\nSubstituting the SVD of $H$ into the trace expression:\n$$ \\mathrm{tr}(R H) = \\mathrm{tr}(R U \\Sigma V^\\top) $$\nUsing the cyclic property of the trace:\n$$ \\mathrm{tr}(R U \\Sigma V^\\top) = \\mathrm{tr}(\\Sigma V^\\top R U) $$\nLet $M = V^\\top R U$. Since $V$, $R$, and $U$ are all orthogonal matrices, their product $M$ is also an orthogonal matrix. The expression becomes:\n$$ \\mathrm{tr}(\\Sigma M) = \\sum_{i=1}^3 (\\Sigma M)_{ii} = \\sum_{i=1}^3 \\sigma_i M_{ii} $$\nSince $M$ is orthogonal, its column vectors are orthonormal, which implies that its elements are bounded: $|M_{ii}| \\le 1$. To maximize the sum $\\sum \\sigma_i M_{ii}$ given that $\\sigma_i \\ge 0$, we should choose $M_{ii}$ to be as large as possible. The maximum possible value is achieved when $M_{ii} = 1$ for all $i$, which means $M = I$.\nIf we set $M = I$, then $V^\\top R U = I$, which gives the solution for the optimal orthogonal matrix $R$:\n$$ R = V I U^\\top = V U^\\top $$\n\n### 4. Handling Reflections and Ensuring $\\det(R) = 1$\n\nThe matrix $R = V U^\\top$ is guaranteed to be orthogonal, but its determinant can be either $+1$ or $-1$:\n$$ \\det(R) = \\det(V U^\\top) = \\det(V) \\det(U^\\top) = \\det(V) \\det(U) $$\nThe determinants of $U$ and $V$ from an SVD can each be $\\pm 1$.\n\nCase 1: $\\det(V)\\det(U) = 1$. In this case, $\\det(R) = 1$, and $R = V U^\\top$ is a proper rotation. This is our desired solution.\n\nCase 2: $\\det(V)\\det(U) = -1$. In this case, $\\det(R) = -1$, meaning $R$ represents a reflection (an improper rotation). The problem requires a proper rotation with $\\det(R) = 1$. We must find the best proper rotation. This corresponds to solving the maximization of $\\sum \\sigma_i M_{ii}$ under the additional constraint that $\\det(M) = \\det(V^\\top R U) = \\det(V^\\top)\\det(R)\\det(U) = \\det(V)\\det(U)\\det(R) = (-1)(1) = -1$.\nThe maximum of $\\sigma_1 M_{11} + \\sigma_2 M_{22} + \\sigma_3 M_{33}$ for an orthogonal matrix $M$ with $\\det(M) = -1$ is achieved when $M = \\mathrm{diag}(1, 1, -1)$. This choice yields a value of $\\sigma_1 + \\sigma_2 - \\sigma_3$, which is the highest possible value under the constraint. The change is made to the component associated with the smallest singular value, $\\sigma_3$, to minimize the reduction from the unconstrained maximum $\\sigma_1 + \\sigma_2 + \\sigma_3$.\n\nThus, if $\\det(V U^\\top) = -1$, the optimal proper rotation is obtained by setting $M = \\mathrm{diag}(1, 1, -1)$.\n$$ V^\\top R U = \\mathrm{diag}(1, 1, -1) \\implies R = V \\cdot \\mathrm{diag}(1, 1, -1) \\cdot U^\\top $$\n\nA general formula covering both cases is:\n$$ R = V \\cdot \\mathrm{diag}(1, 1, \\det(V U^\\top)) \\cdot U^\\top $$\n\n### 5. Summary of the Algorithm and Diagnostics\n\n1.  Given point clouds $X, Y \\in \\mathbb{R}^{N \\times 3}$.\n2.  Compute centroids: $\\bar{x} = \\frac{1}{N}\\sum x_i$ and $\\bar{y} = \\frac{1}{N}\\sum y_i$.\n3.  Center the point clouds: $X_c = X - \\mathbf{1}\\bar{x}$ and $Y_c = Y - \\mathbf{1}\\bar{y}$.\n4.  Compute the cross-covariance matrix $H = X_c^\\top Y_c$.\n5.  Perform SVD on $H$ to get $H = U \\Sigma V^\\top$.\n6.  Calculate the correction factor for the determinant: $d = \\det(V U^\\top)$.\n7.  Compute the optimal rotation matrix: $R = V \\cdot \\mathrm{diag}(1, 1, d) \\cdot U^\\top$. This ensures $\\det(R) = 1$.\n8.  The required diagnostic metrics are then computed:\n    -   **Root-Mean-Square Deviation (RMSD)**: This measures the average distance between the points in $Y$ and the optimally aligned points from $X$. The aligned points are $X_{\\text{aligned}} = X_c R^\\top + \\mathbf{1}\\bar{y}$. The RMSD is $\\sqrt{\\frac{1}{N} \\lVert X_{\\text{aligned}} - Y \\rVert_F^2}$.\n    -   **Orthogonality Deviation**: This checks how close the computed $R$ is to being perfectly orthogonal. It is calculated as the Frobenius norm of the residual: $\\lVert R^\\top R - I \\rVert_F$. For a perfect rotation, this should be zero.\n    -   **Determinant**: This confirms that the resulting transformation is a proper rotation. It should be equal to $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the alignment algorithm,\n    and print the results in the specified format.\n    \"\"\"\n\n    def kabsch_algorithm(X, Y):\n        \"\"\"\n        Computes the optimal rotation matrix to align point cloud X to Y.\n\n        Args:\n            X (np.ndarray): An (N, 3) array of 3D points.\n            Y (np.ndarray): An (N, 3) array of 3D points.\n\n        Returns:\n            tuple: A tuple containing:\n                - R (np.ndarray): The optimal (3, 3) rotation matrix.\n                - centroid_X (np.ndarray): The centroid of X.\n                - centroid_Y (np.ndarray): The centroid of Y.\n        \"\"\"\n        N = X.shape[0]\n        # 1. Center the point clouds\n        centroid_X = np.mean(X, axis=0)\n        centroid_Y = np.mean(Y, axis=0)\n        X_c = X - centroid_X\n        Y_c = Y - centroid_Y\n\n        # 2. Compute the cross-covariance matrix\n        H = X_c.T @ Y_c\n\n        # 3. Perform SVD\n        U, S, Vt = np.linalg.svd(H)\n        V = Vt.T\n\n        # 4. Compute the optimal rotation matrix\n        # First, calculate the orthogonal matrix that does not guarantee det(R)=1\n        R_ortho = V @ U.T\n\n        # 5. Correct for reflections to ensure det(R)=1 (proper rotation)\n        d = np.linalg.det(R_ortho)\n        correction_matrix = np.diag([1, 1, d])\n        \n        R = V @ correction_matrix @ U.T\n        \n        return R, centroid_X, centroid_Y\n\n    def compute_diagnostics(X, Y, R, centroid_X, centroid_Y):\n        \"\"\"\n        Computes RMSD, orthogonality deviation, and determinant.\n\n        Args:\n            X (np.ndarray): Original (N, 3) points.\n            Y (np.ndarray): Target (N, 3) points.\n            R (np.ndarray): The (3, 3) rotation matrix.\n            centroid_X (np.ndarray): Centroid of X.\n            centroid_Y (np.ndarray): Centroid of Y.\n\n        Returns:\n            tuple: A tuple containing (rmsd, orth_dev, det_R).\n        \"\"\"\n        # 1. RMSD\n        X_c = X - centroid_X\n        X_aligned = X_c @ R.T + centroid_Y\n        rmsd = np.sqrt(np.mean(np.sum((X_aligned - Y)**2, axis=1)))\n\n        # 2. Orthogonality Deviation\n        orth_dev = np.linalg.norm(R.T @ R - np.identity(3), 'fro')\n\n        # 3. Determinant\n        det_R = np.linalg.det(R)\n\n        return rmsd, orth_dev, det_R\n\n    # Define test cases\n    X1 = np.array([\n        [0., 0., 0.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.],\n        [1., 1.2, -0.7], [-0.3, 2.1, -1.5]\n    ])\n    theta1 = np.pi / 6\n    c1, s1 = np.cos(theta1), np.sin(theta1)\n    Rz = np.array([[c1, -s1, 0], [s1, c1, 0], [0, 0, 1]])\n    t1 = np.array([1., -2., 0.5])\n    Y1 = X1 @ Rz.T + t1\n\n    X2 = np.array([\n        [0.2, -1.3, 2.], [3.5, 0.4, -0.2], [1.1, 1., 1.], [2.1, -0.9, 0.3]\n    ])\n    Y2 = X2.copy()\n\n    X3 = np.array([\n        [0., 0., 0.], [1., 2., 3.], [-1., -0.5, 0.7], [0.8, -1.2, 2.5]\n    ])\n    theta3 = np.pi / 4\n    c3, s3 = np.cos(theta3), np.sin(theta3)\n    Ry = np.array([[c3, 0, s3], [0, 1, 0], [-s3, 0, c3]])\n    S_reflect = np.diag([1., 1., -1.])\n    t3 = np.array([-0.5, 2., 1.])\n    Y3 = X3 @ Ry.T @ S_reflect.T + t3\n\n    X4 = np.array([[0., 0., 0.], [2., 0., 0.]])\n    theta4 = np.pi / 3\n    c4, s4 = np.cos(theta4), np.sin(theta4)\n    Rx = np.array([[1, 0, 0], [0, c4, -s4], [0, s4, c4]])\n    t4 = np.array([0., 1., -1.])\n    Y4 = X4 @ Rx.T + t4\n\n    test_cases = [(X1, Y1), (X2, Y2), (X3, Y3), (X4, Y4)]\n\n    all_results = []\n    for X, Y in test_cases:\n        R, cx, cy = kabsch_algorithm(X, Y)\n        rmsd, orth_dev, det_R = compute_diagnostics(X, Y, R, cx, cy)\n        all_results.append(f\"[{rmsd:.6f},{orth_dev:.6f},{det_R:.6f}]\")\n\n    print(f\"[[{','.join(all_results)}]]\")\n\nsolve()\n```"
        }
    ]
}