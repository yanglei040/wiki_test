{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on exploration with the most fundamental case: approximating a function with a single constant. This exercise grounds the concept of continuous least squares in first principles, requiring you to find the optimal constant by directly minimizing the weighted squared error integral using calculus. By including a non-uniform weight function $w(x)$, this problem  also reveals how the \"best\" constant is effectively a weighted average of the function, a key insight that extends to more complex approximations.",
            "id": "3218137",
            "problem": "Consider a function $f:[a,b]\\to\\mathbb{R}$ and a strictly positive weight function $w:[a,b]\\to\\mathbb{R}$ that is integrable on the interval $[a,b]$. The task is to find a constant $c\\in\\mathbb{R}$ that best approximates $f$ in the continuous least squares sense with respect to $w$, meaning $c$ minimizes the weighted mean-square error\n$$\n\\int_{a}^{b} w(x)\\,\\big(f(x)-c\\big)^{2}\\,dx.\n$$\nStarting only from the fundamental definition of the weighted mean-square error, derive the condition that characterizes the minimizing constant $c$, and express $c$ in terms of integrals involving $f$ and $w$ over $[a,b]$. Then, apply your derivation to the specific case $a=1$, $b=e$, $f(x)=\\ln(x)$, and $w(x)=x$ to compute the exact minimizing constant. Express your final answer as an exact symbolic expression; do not round.",
            "solution": "The problem requires finding the constant $c \\in \\mathbb{R}$ that best approximates a function $f:[a,b]\\to\\mathbb{R}$ in the continuous least squares sense with respect to a weight function $w:[a,b]\\to\\mathbb{R}$. This is achieved by minimizing the weighted mean-square error, given by the functional $E(c)$:\n$$\nE(c) = \\int_{a}^{b} w(x)\\,\\big(f(x)-c\\big)^{2}\\,dx.\n$$\nTo find the value of $c$ that minimizes $E(c)$, we employ standard methods from calculus. A necessary condition for a minimum is that the derivative of $E(c)$ with respect to $c$ must be zero. Since the integration is with respect to $x$, we can apply the Leibniz integral rule (differentiation under the integral sign) to compute the derivative of $E(c)$ with respect to $c$:\n$$\n\\frac{dE}{dc} = \\frac{d}{dc} \\int_{a}^{b} w(x)\\,\\big(f(x)-c\\big)^{2}\\,dx = \\int_{a}^{b} \\frac{\\partial}{\\partial c} \\left[ w(x)\\,\\big(f(x)-c\\big)^{2} \\right] \\,dx.\n$$\nUsing the chain rule for the integrand, we find the partial derivative:\n$$\n\\frac{\\partial}{\\partial c} \\left[ w(x)\\,(f(x)-c)^{2} \\right] = w(x) \\cdot 2(f(x)-c) \\cdot \\frac{\\partial}{\\partial c}(f(x)-c) = w(x) \\cdot 2(f(x)-c) \\cdot (-1) = -2w(x)(f(x)-c).\n$$\nSubstituting this back into the expression for the derivative of $E(c)$:\n$$\n\\frac{dE}{dc} = \\int_{a}^{b} -2w(x)(f(x)-c) \\,dx = -2 \\int_{a}^{b} \\big(w(x)f(x) - cw(x)\\big) \\,dx.\n$$\nSetting this derivative to zero provides the condition for the optimal $c$:\n$$\n-2 \\int_{a}^{b} \\big(w(x)f(x) - cw(x)\\big) \\,dx = 0.\n$$\nSince $-2 \\neq 0$, the integral itself must be zero:\n$$\n\\int_{a}^{b} \\big(w(x)f(x) - cw(x)\\big) \\,dx = 0.\n$$\nBy the linearity of the integral operator, we can separate the terms:\n$$\n\\int_{a}^{b} w(x)f(x) \\,dx - \\int_{a}^{b} cw(x) \\,dx = 0.\n$$\nSince $c$ is a constant with respect to the integration variable $x$, we can factor it out of the second integral:\n$$\n\\int_{a}^{b} w(x)f(x) \\,dx - c \\int_{a}^{b} w(x) \\,dx = 0.\n$$\nSolving for $c$ yields:\n$$\nc \\int_{a}^{b} w(x) \\,dx = \\int_{a}^{b} w(x)f(x) \\,dx.\n$$\nThe problem states that $w(x)$ is a strictly positive weight function, so for $a < b$, the integral $\\int_{a}^{b} w(x) \\,dx$ is positive and non-zero. Thus, we can divide by it to obtain the general expression for the minimizing constant $c$:\n$$\nc = \\frac{\\int_{a}^{b} w(x)f(x) \\,dx}{\\int_{a}^{b} w(x) \\,dx}.\n$$\nTo verify that this value of $c$ corresponds to a minimum, we check the second derivative, $\\frac{d^{2}E}{dc^{2}}$. Differentiating $\\frac{dE}{dc} = -2\\int_{a}^{b} w(x)f(x) \\,dx + 2c\\int_{a}^{b} w(x) \\,dx$ with respect to $c$:\n$$\n\\frac{d^{2}E}{dc^{2}} = \\frac{d}{dc} \\left( -2\\int_{a}^{b} w(x)f(x) \\,dx + 2c\\int_{a}^{b} w(x) \\,dx \\right) = 2 \\int_{a}^{b} w(x) \\,dx.\n$$\nAs established, since $w(x) > 0$ on $[a,b]$, the integral $\\int_{a}^{b} w(x) \\,dx$ is positive. Consequently, $\\frac{d^{2}E}{dc^{2}} > 0$, confirming that the critical point is a minimum. Since $E(c)$ is a quadratic in $c$ with a positive leading coefficient, this is a global minimum.\n\nWe now apply this result to the specific case given: $a=1$, $b=e$, $f(x)=\\ln(x)$, and $w(x)=x$. The formula for $c$ is:\n$$\nc = \\frac{\\int_{1}^{e} x \\ln(x) \\,dx}{\\int_{1}^{e} x \\,dx}.\n$$\nWe compute the numerator integral, $\\int_{1}^{e} x \\ln(x) \\,dx$, using integration by parts, $\\int u \\,dv = uv - \\int v \\,du$. Let $u=\\ln(x)$ and $dv=x\\,dx$, which gives $du=\\frac{1}{x}\\,dx$ and $v=\\frac{x^{2}}{2}$.\n$$\n\\int_{1}^{e} x \\ln(x) \\,dx = \\left[ \\frac{x^{2}}{2}\\ln(x) \\right]_{1}^{e} - \\int_{1}^{e} \\frac{x^{2}}{2} \\cdot \\frac{1}{x} \\,dx = \\left( \\frac{e^{2}}{2}\\ln(e) - \\frac{1^{2}}{2}\\ln(1) \\right) - \\int_{1}^{e} \\frac{x}{2} \\,dx.\n$$\nSince $\\ln(e)=1$ and $\\ln(1)=0$, this simplifies to:\n$$\n= \\left( \\frac{e^{2}}{2} - 0 \\right) - \\left[ \\frac{x^{2}}{4} \\right]_{1}^{e} = \\frac{e^{2}}{2} - \\left( \\frac{e^{2}}{4} - \\frac{1^{2}}{4} \\right) = \\frac{e^{2}}{2} - \\frac{e^{2}}{4} + \\frac{1}{4} = \\frac{2e^{2}-e^{2}+1}{4} = \\frac{e^{2}+1}{4}.\n$$\nNext, we compute the denominator integral, $\\int_{1}^{e} x \\,dx$:\n$$\n\\int_{1}^{e} x \\,dx = \\left[ \\frac{x^{2}}{2} \\right]_{1}^{e} = \\frac{e^{2}}{2} - \\frac{1^{2}}{2} = \\frac{e^{2}-1}{2}.\n$$\nFinally, we substitute these results back into the expression for $c$:\n$$\nc = \\frac{\\frac{e^{2}+1}{4}}{\\frac{e^{2}-1}{2}} = \\frac{e^{2}+1}{4} \\cdot \\frac{2}{e^{2}-1} = \\frac{e^{2}+1}{2(e^{2}-1)}.\n$$",
            "answer": "$$\n\\boxed{\\frac{\\exp(2)+1}{2(\\exp(2)-1)}}\n$$"
        },
        {
            "introduction": "Having mastered constant approximations, we now advance to approximating a function with a polynomial from a given subspace. This practice frames the problem geometrically, asking you to find the orthogonal projection of a function onto a vector space of polynomials. By solving the normal equations, you will discover how underlying mathematical structures, such as the parity of functions on a symmetric interval, can be exploited to simplify the problem by leveraging orthogonality .",
            "id": "3218261",
            "problem": "Let $C([-1,1])$ denote the space of continuous real-valued functions on $[-1,1]$, and equip it with the standard inner product $\\langle g,h\\rangle=\\int_{-1}^{1} g(x)\\,h(x)\\,dx$ and induced norm $\\|g\\|=\\sqrt{\\langle g,g\\rangle}$. Consider the function $f(x)=x^{3}$ and the subspace $V=\\operatorname{span}\\{1,x,x^{2}\\}$.\n\nUsing only the definitions of inner product, norm, and orthogonal projection in this inner product space, determine the unique function $p^{\\ast}\\in V$ that minimizes $\\|f-p\\|$ over $p\\in V$ by enforcing that the residual $r=f-p^{\\ast}$ is orthogonal to $V$. Then, compute the minimal squared error $\\|f-p^{\\ast}\\|^{2}$, and explain how the parity of $f$ and the basis functions in $V$ affects orthogonality on the symmetric interval $[-1,1]$.\n\nProvide your final answer as the exact value of the minimal squared error $\\|f-p^{\\ast}\\|^{2}$, expressed as a reduced fraction. Do not round.",
            "solution": "The problem is to find the function $p^{\\ast}(x)$ in the subspace $V = \\operatorname{span}\\{1, x, x^2\\}$ that best approximates the function $f(x) = x^3$ on the interval $[-1, 1]$ in the least squares sense. The space of functions $C([-1,1])$ is equipped with the inner product $\\langle g, h \\rangle = \\int_{-1}^{1} g(x)h(x)dx$. The best approximation $p^{\\ast}(x)$ is the unique function in $V$ that minimizes the error norm $\\|f - p\\|$, where $\\|g\\| = \\sqrt{\\langle g, g \\rangle}$. This minimum occurs when the residual vector, $r(x) = f(x) - p^{\\ast}(x)$, is orthogonal to the subspace $V$.\n\nAny function $p(x) \\in V$ can be written as a linear combination of the basis functions: $p(x) = c_0 \\cdot 1 + c_1 \\cdot x + c_2 \\cdot x^2$. Let the basis functions be denoted as $\\phi_0(x) = 1$, $\\phi_1(x) = x$, and $\\phi_2(x) = x^2$. The condition that the residual $r(x) = f(x) - p^{\\ast}(x)$ is orthogonal to $V$ means that $r(x)$ must be orthogonal to every basis function of $V$. This gives rise to a system of three linear equations, known as the normal equations:\n$$ \\langle f - p^{\\ast}, \\phi_i \\rangle = 0 \\quad \\text{for } i = 0, 1, 2 $$\n$$ \\langle f, \\phi_i \\rangle = \\langle p^{\\ast}, \\phi_i \\rangle \\quad \\text{for } i = 0, 1, 2 $$\nSubstituting $p^{\\ast}(x) = c_0^{\\ast}\\phi_0(x) + c_1^{\\ast}\\phi_1(x) + c_2^{\\ast}\\phi_2(x)$, we get:\n$$ \\langle f, \\phi_i \\rangle = c_0^{\\ast}\\langle \\phi_0, \\phi_i \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_i \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_i \\rangle \\quad \\text{for } i = 0, 1, 2 $$\n\nBefore computing the integrals, we can simplify the problem by considering the parity of the functions involved over the symmetric interval $[-1, 1]$.\nA function $g(x)$ is even if $g(-x) = g(x)$ and odd if $g(-x) = -g(x)$. The integral of an odd function over a symmetric interval $[-a, a]$ is always zero. The product of two even functions or two odd functions is even. The product of an even and an odd function is odd.\nOur function to approximate is $f(x) = x^3$, which is odd.\nThe basis for $V$ consists of $\\phi_0(x) = 1$ (even), $\\phi_1(x) = x$ (odd), and $\\phi_2(x) = x^2$ (even).\n\nThis parity structure affects orthogonality directly. The inner product $\\langle g, h \\rangle = \\int_{-1}^{1} g(x)h(x)dx$ will be zero if the integrand $g(x)h(x)$ is an odd function. This occurs when one of the functions is even and the other is odd. This simplifies the system of normal equations significantly.\n\nLet's compute the required inner products:\nRight-hand side of the normal equations, $\\langle f, \\phi_i \\rangle$:\n$f(x) = x^3$ is odd.\n$\\langle f, \\phi_0 \\rangle = \\langle x^3, 1 \\rangle = \\int_{-1}^{1} x^3 dx = 0$ (integrand is odd).\n$\\langle f, \\phi_1 \\rangle = \\langle x^3, x \\rangle = \\int_{-1}^{1} x^4 dx = \\left[\\frac{x^5}{5}\\right]_{-1}^{1} = \\frac{1}{5} - (-\\frac{1}{5}) = \\frac{2}{5}$.\n$\\langle f, \\phi_2 \\rangle = \\langle x^3, x^2 \\rangle = \\int_{-1}^{1} x^5 dx = 0$ (integrand is odd).\n\nLeft-hand side matrix elements (Gram matrix), $\\langle \\phi_j, \\phi_i \\rangle$:\n$\\langle \\phi_0, \\phi_0 \\rangle = \\langle 1, 1 \\rangle = \\int_{-1}^{1} 1 dx = 2$.\n$\\langle \\phi_0, \\phi_1 \\rangle = \\langle 1, x \\rangle = \\int_{-1}^{1} x dx = 0$.\n$\\langle \\phi_0, \\phi_2 \\rangle = \\langle 1, x^2 \\rangle = \\int_{-1}^{1} x^2 dx = \\frac{2}{3}$.\n$\\langle \\phi_1, \\phi_1 \\rangle = \\langle x, x \\rangle = \\int_{-1}^{1} x^2 dx = \\frac{2}{3}$.\n$\\langle \\phi_1, \\phi_2 \\rangle = \\langle x, x^2 \\rangle = \\int_{-1}^{1} x^3 dx = 0$.\n$\\langle \\phi_2, \\phi_2 \\rangle = \\langle x^2, x^2 \\rangle = \\int_{-1}^{1} x^4 dx = \\frac{2}{5}$.\n\nThe system of equations for the coefficients $(c_0^{\\ast}, c_1^{\\ast}, c_2^{\\ast})$ becomes:\n1. $c_0^{\\ast}\\langle \\phi_0, \\phi_0 \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_0 \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_0 \\rangle = \\langle f, \\phi_0 \\rangle \\implies c_0^{\\ast}(2) + c_1^{\\ast}(0) + c_2^{\\ast}(\\frac{2}{3}) = 0$.\n2. $c_0^{\\ast}\\langle \\phi_0, \\phi_1 \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_1 \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_1 \\rangle = \\langle f, \\phi_1 \\rangle \\implies c_0^{\\ast}(0) + c_1^{\\ast}(\\frac{2}{3}) + c_2^{\\ast}(0) = \\frac{2}{5}$.\n3. $c_0^{\\ast}\\langle \\phi_0, \\phi_2 \\rangle + c_1^{\\ast}\\langle \\phi_1, \\phi_2 \\rangle + c_2^{\\ast}\\langle \\phi_2, \\phi_2 \\rangle = \\langle f, \\phi_2 \\rangle \\implies c_0^{\\ast}(\\frac{2}{3}) + c_1^{\\ast}(0) + c_2^{\\ast}(\\frac{2}{5}) = 0$.\n\nThe system decouples based on parity. The second equation, involving only the odd basis function $\\phi_1$, can be solved independently:\n$\\frac{2}{3} c_1^{\\ast} = \\frac{2}{5} \\implies c_1^{\\ast} = \\frac{3}{5}$.\n\nThe first and third equations form a system for the coefficients of the even basis functions:\n$$ 2c_0^{\\ast} + \\frac{2}{3}c_2^{\\ast} = 0 $$\n$$ \\frac{2}{3}c_0^{\\ast} + \\frac{2}{5}c_2^{\\ast} = 0 $$\nThis is a homogeneous linear system. The determinant of the coefficient matrix is $(2)(\\frac{2}{5}) - (\\frac{2}{3})(\\frac{2}{3}) = \\frac{4}{5} - \\frac{4}{9} = \\frac{36-20}{45} = \\frac{16}{45} \\neq 0$. Since the matrix is non-singular, the only solution is the trivial one: $c_0^{\\ast} = 0$ and $c_2^{\\ast} = 0$.\n\nThis illustrates a general principle: the best approximation of an odd function $f$ from a basis consisting of even and odd functions will be composed solely of the odd basis functions. The projection of $f$ onto the subspace of even functions is zero.\nThe coefficients are $c_0^{\\ast}=0$, $c_1^{\\ast}=\\frac{3}{5}$, $c_2^{\\ast}=0$. Thus, the best approximating polynomial is $p^{\\ast}(x) = \\frac{3}{5}x$.\n\nNext, we compute the minimal squared error, $\\|f - p^{\\ast}\\|^2$.\n$\\|f - p^{\\ast}\\|^2 = \\langle f - p^{\\ast}, f - p^{\\ast} \\rangle$.\nBy the construction of $p^{\\ast}$, the residual $f-p^{\\ast}$ is orthogonal to any function in $V$. Since $p^{\\ast} \\in V$, we have $\\langle f - p^{\\ast}, p^{\\ast} \\rangle = 0$.\nTherefore, we can expand the squared norm as:\n$\\|f - p^{\\ast}\\|^2 = \\langle f - p^{\\ast}, f \\rangle - \\langle f - p^{\\ast}, p^{\\ast} \\rangle = \\langle f, f \\rangle - \\langle p^{\\ast}, f \\rangle$.\n\nWe compute the two required terms:\n$\\|f\\|^2 = \\langle f, f \\rangle = \\langle x^3, x^3 \\rangle = \\int_{-1}^{1} x^6 dx = \\left[\\frac{x^7}{7}\\right]_{-1}^{1} = \\frac{1}{7} - (-\\frac{1}{7}) = \\frac{2}{7}$.\n$\\langle p^{\\ast}, f \\rangle = \\langle \\frac{3}{5}x, x^3 \\rangle = \\frac{3}{5} \\langle x, x^3 \\rangle = \\frac{3}{5} \\int_{-1}^{1} x^4 dx = \\frac{3}{5} \\left( \\frac{2}{5} \\right) = \\frac{6}{25}$.\n\nThe minimal squared error is:\n$\\|f - p^{\\ast}\\|^2 = \\frac{2}{7} - \\frac{6}{25} = \\frac{2 \\cdot 25}{7 \\cdot 25} - \\frac{6 \\cdot 7}{25 \\cdot 7} = \\frac{50}{175} - \\frac{42}{175} = \\frac{8}{175}$.\nThis fraction is in reduced form since $8=2^3$ and $175=5^2 \\cdot 7$.\nAlternatively, one could directly compute the integral of the squared residual:\n$\\|f - p^{\\ast}\\|^2 = \\int_{-1}^{1} \\left(x^3 - \\frac{3}{5}x\\right)^2 dx = \\int_{-1}^{1} \\left(x^6 - \\frac{6}{5}x^4 + \\frac{9}{25}x^2\\right) dx$.\nSince the integrand is an even function:\n$= 2 \\left[\\frac{x^7}{7} - \\frac{6}{5}\\frac{x^5}{5} + \\frac{9}{25}\\frac{x^3}{3}\\right]_{0}^{1} = 2 \\left(\\frac{1}{7} - \\frac{6}{25} + \\frac{3}{25}\\right) = 2 \\left(\\frac{1}{7} - \\frac{3}{25}\\right) = 2 \\left(\\frac{25-21}{175}\\right) = 2\\left(\\frac{4}{175}\\right) = \\frac{8}{175}$.\nBoth methods yield the same result. The minimal squared error is $\\frac{8}{175}$.",
            "answer": "$$\n\\boxed{\\frac{8}{175}}\n$$"
        },
        {
            "introduction": "Our final practice bridges the gap between analytical theory and practical, high-performance scientific computing. While solving the normal equations works for low-degree polynomials, the method becomes numerically unstable for higher degrees when using a simple monomial basis like $\\{1, x, x^2, \\dots\\}$. This comprehensive exercise  guides you through building a robust algorithm from the ground up, using the properties of Legendre polynomials—an orthogonal basis—to ensure stable and efficient computation, culminating in a complete Python implementation.",
            "id": "3218252",
            "problem": "You are asked to connect the mathematics of orthogonal polynomials with an efficient and stable numerical method for continuous least squares approximation over $[-1,1]$. The foundational starting point for this task is the inner-product space of square-integrable functions on $[-1,1]$ with the inner product $\\langle f,g\\rangle=\\int_{-1}^{1} f(x)g(x)\\,dx$, and the definition of the Legendre polynomials $\\{P_n\\}_{n\\ge 0}$ as the unique sequence of real polynomials where $P_n$ has degree $n$, $P_n(1)=1$, and $\\langle P_m,P_n\\rangle=0$ for $m\\ne n$. You may also use the Rodrigues definition $P_n(x)=\\dfrac{1}{2^n n!}\\dfrac{d^n}{dx^n}\\left(x^2-1\\right)^n$, and the well-established facts that orthogonal polynomial sequences with respect to a positive weight on an interval satisfy a three-term recurrence, and that Gauss-Legendre quadrature with $Q$ nodes exactly integrates all polynomials of degree at most $2Q-1$.\n\nTask:\n1) Derive from first principles the existence of a three-term recurrence of the form\n$$\nP_{n+1}(x)=\\alpha_n\\,x\\,P_n(x)+\\beta_n\\,P_n(x)+\\gamma_n\\,P_{n-1}(x),\\quad n\\ge 1,\n$$\nwith suitable constants $\\alpha_n,\\beta_n,\\gamma_n$ independent of $x$, using only the orthogonality, degree, and normalization properties above. Determine these constants explicitly in terms of $n$. You must start from the fundamental definition of orthogonality on $[-1,1]$ and the structural properties of orthogonal polynomials; do not assume any pre-stated recurrence.\n\n2) Using only the inner-product definition and the normalization chosen for $P_n$, derive the continuous least squares coefficients $a_n$ in the expansion\n$$\nf(x)\\approx \\sum_{n=0}^{N} a_n\\,P_n(x)\n$$\nthat minimizes $\\int_{-1}^{1}\\left(f(x)-\\sum_{n=0}^{N} a_n\\,P_n(x)\\right)^2\\,dx$. Express $a_n$ in terms of $\\int_{-1}^{1} f(x)P_n(x)\\,dx$ and $\\int_{-1}^{1} P_n(x)^2\\,dx$. Then, using any permitted foundational fact you have already justified or derived, simplify this expression to a closed form that depends only on $\\int_{-1}^{1} f(x)P_n(x)\\,dx$ and $n$.\n\n3) Design an algorithm that computes $\\{a_n\\}_{n=0}^{N}$ using only:\n- stable evaluation of $\\{P_n(x_j)\\}$ via the three-term recurrence you derived at a set of quadrature nodes $\\{x_j\\}_{j=1}^{Q}$,\n- a numerical approximation of the inner products $\\int_{-1}^{1} f(x)P_n(x)\\,dx$ by Gauss-Legendre quadrature with $Q$ nodes and weights $\\{w_j\\}_{j=1}^{Q}$,\n- pointwise evaluations of $f$ at $\\{x_j\\}$.\nArgue why your approach is computationally $\\mathcal{O}(QN)$ and numerically stable on $[-1,1]$.\n\n4) Implement the algorithm as a complete, runnable program that:\n- uses only the Python programming language, version $3.12$, with the library NumPy version $1.23.5$ and the Python standard library,\n- evaluates $P_n(x)$ for $0\\le n\\le N$ at the Gauss-Legendre nodes by the three-term recurrence you derived; you must not form explicit polynomial coefficients,\n- computes the least squares coefficients $a_n$ for each test case specified below,\n- rounds every reported coefficient to exactly $12$ digits after the decimal point,\n- prints a single line containing a list of lists in the following exact format: an outer list with one inner list per test case, each inner list containing the coefficients $\\left[a_0,\\dots,a_N\\right]$ for that case. No spaces are allowed anywhere in the printed line. For example, if there were two cases with coefficients $\\left[1.0,0.0\\right]$ and $\\left[2.5\\right]$, the output would be printed as: $[[1.000000000000,0.000000000000],[2.500000000000]]$.\n\nAngle units: whenever trigonometric functions appear, interpret all angles in radians.\n\nTest Suite:\n- Case $1$ (general smooth function): $f(x)=e^x$, $N=5$, use Gauss-Legendre quadrature with $Q=200$ nodes.\n- Case $2$ (polynomial exactness): $f(x)=x^3-x$, $N=5$, use Gauss-Legendre quadrature with $Q=6$ nodes. This choice guarantees exactness because the integrands $f(x)P_n(x)$ have degree at most $8$, and Gauss-Legendre with $Q=6$ is exact for degrees $\\le 11$.\n- Case $3$ (constant function and orthogonality): $f(x)=1$, $N=7$, use Gauss-Legendre quadrature with $Q=2$ nodes.\n- Case $4$ (boundary degree): $f(x)=x$, $N=0$, use Gauss-Legendre quadrature with $Q=4$ nodes.\n- Case $5$ (higher-degree stability): $f(x)=\\cos(3x)$, $N=20$, use Gauss-Legendre quadrature with $Q=300$ nodes.\n\nYour program must produce a single line of output containing the results as a comma-separated list of inner lists enclosed in square brackets, with no spaces anywhere in the line, and every coefficient rendered with exactly $12$ digits after the decimal point.",
            "solution": "This problem requires a multi-part solution involving the derivation of fundamental properties of Legendre polynomials, the formulation of a continuous least squares approximation, the design of a stable numerical algorithm, and its implementation. We shall address each task in sequence.\n\n### Part 1: Three-Term Recurrence Derivation\n\nWe are asked to derive the three-term recurrence relation for Legendre polynomials $\\{P_n(x)\\}_{n \\geq 0}$ on the interval $[-1, 1]$. The derivation will use the properties provided:\n1.  $P_n(x)$ is a polynomial of degree $n$.\n2.  The polynomials are orthogonal with respect to the inner product $\\langle f, g \\rangle = \\int_{-1}^{1} f(x)g(x) dx$.\n3.  The normalization is $P_n(1) = 1$ for all $n \\ge 0$.\n\nLet's start by considering the polynomial $xP_n(x)$. This is a polynomial of degree $n+1$. Since the set $\\{P_k(x)\\}_{k=0}^{n+1}$ forms a basis for the space of polynomials of degree at most $n+1$, we can express $xP_n(x)$ as a linear combination:\n$$\nxP_n(x) = \\sum_{k=0}^{n+1} c_k P_k(x)\n$$\nThe coefficients $c_k$ are found by taking the inner product with $P_m(x)$ and using orthogonality:\n$$\n\\langle xP_n, P_m \\rangle = \\left\\langle \\sum_{k=0}^{n+1} c_k P_k, P_m \\right\\rangle = \\sum_{k=0}^{n+1} c_k \\langle P_k, P_m \\rangle = c_m \\langle P_m, P_m \\rangle\n$$\nSo, $c_m = \\frac{\\langle xP_n, P_m \\rangle}{\\langle P_m, P_m \\rangle}$.\nBy the symmetry of the inner product, $\\langle xP_n, P_m \\rangle = \\langle P_n, xP_m \\rangle$. The term $xP_m(x)$ is a polynomial of degree $m+1$. Due to the orthogonality of $P_n(x)$ to any polynomial of degree less than $n$, if $m+1 < n$, this inner product is zero.\n$$\nm+1 < n \\implies \\langle P_n, xP_m \\rangle = 0\n$$\nThis implies that $c_m=0$ for all $m < n-1$. Consequently, the expansion simplifies to a sum of only three terms:\n$$\nxP_n(x) = c_{n+1}P_{n+1}(x) + c_n P_n(x) + c_{n-1}P_{n-1}(x)\n$$\nThis holds for $n \\ge 1$. We can determine the coefficient $c_n$. Legendre polynomials have a defined parity: $P_k(-x) = (-1)^k P_k(x)$. Thus, $P_n(x)^2$ is an even function for any $n$. The function $xP_n(x)^2$ is therefore an odd function.\n$$\nc_n \\langle P_n, P_n \\rangle = \\langle xP_n, P_n \\rangle = \\int_{-1}^{1} x P_n(x)^2 dx = 0\n$$\nSince $\\langle P_n, P_n \\rangle \\neq 0$, we must have $c_n=0$.\nThe recurrence relation further simplifies to:\n$$\nxP_n(x) = c_{n+1}P_{n+1}(x) + c_{n-1}P_{n-1}(x)\n$$\nRearranging for $P_{n+1}(x)$:\n$$\nP_{n+1}(x) = \\frac{1}{c_{n+1}} xP_n(x) - \\frac{c_{n-1}}{c_{n+1}} P_{n-1}(x)\n$$\nThis matches the requested form $P_{n+1}(x) = \\alpha_n x P_n(x) + \\beta_n P_n(x) + \\gamma_n P_{n-1}(x)$ with $\\beta_n=0$, where $\\alpha_n=1/c_{n+1}$ and $\\gamma_n = -c_{n-1}/c_{n+1}$.\n\nTo find explicit expressions for $\\alpha_n$ and $\\gamma_n$, we use the well-established facts about the leading coefficients $k_n$ of $P_n(x)$ and their norms $\\langle P_n, P_n \\rangle$, which are consequences of the Rodrigues definition and normalization.\nLet $P_n(x) = k_n x^n + \\dots$. Then $xP_n(x) = k_n x^{n+1} + \\dots$.\nFrom $xP_n(x) = c_{n+1}P_{n+1}(x) + c_{n-1}P_{n-1}(x)$, by comparing the coefficients of $x^{n+1}$, we get $k_n = c_{n+1} k_{n+1}$, so $c_{n+1} = k_n/k_{n+1}$.\nThus, $\\alpha_n = 1/c_{n+1} = k_{n+1}/k_n$. Using the known ratio of leading coefficients for Legendre polynomials, $\\frac{k_{n+1}}{k_n} = \\frac{2n+1}{n+1}$, we find:\n$$\n\\alpha_n = \\frac{2n+1}{n+1}\n$$\nNow for $\\gamma_n$. We have $c_{n-1} = \\frac{\\langle xP_n, P_{n-1} \\rangle}{\\langle P_{n-1}, P_{n-1} \\rangle} = \\frac{\\langle P_n, xP_{n-1} \\rangle}{\\langle P_{n-1}, P_{n-1} \\rangle}$.\nFrom the recurrence for $n-1$, we have $xP_{n-1}(x) = c'_n P_n(x) + c'_{n-2}P_{n-2}(x)$, where $c'_n = k_{n-1}/k_n$.\nSo, $\\langle P_n, xP_{n-1} \\rangle = \\langle P_n, \\frac{k_{n-1}}{k_n}P_n(x) + \\dots \\rangle = \\frac{k_{n-1}}{k_n} \\langle P_n, P_n \\rangle$.\nThis gives $c_{n-1} = \\frac{k_{n-1}}{k_n} \\frac{\\langle P_n, P_n \\rangle}{\\langle P_{n-1}, P_{n-1} \\rangle}$.\nThen, $\\gamma_n = -\\frac{c_{n-1}}{c_{n+1}} = - \\left( \\frac{k_{n-1}}{k_n} \\frac{\\langle P_n, P_n \\rangle}{\\langle P_{n-1}, P_{n-1} \\rangle} \\right) \\left( \\frac{k_{n+1}}{k_n} \\right)$.\nUsing the known norm-squared $\\langle P_n, P_n \\rangle = \\frac{2}{2n+1}$, the ratio is $\\frac{\\langle P_n, P_n \\rangle}{\\langle P_{n-1}, P_{n-1} \\rangle} = \\frac{2n-1}{2n+1}$.\nAnd using the leading coefficient ratios $\\frac{k_{n-1}}{k_n} = \\frac{n}{2n-1}$ and $\\frac{k_{n+1}}{k_n} = \\frac{2n+1}{n+1}$:\n$$\n\\gamma_n = - \\left(\\frac{n}{2n-1}\\right) \\left(\\frac{2n-1}{2n+1}\\right) \\left(\\frac{2n+1}{n+1}\\right) = -\\frac{n}{n+1}\n$$\nThe constants are $\\alpha_n = \\frac{2n+1}{n+1}$, $\\beta_n = 0$, and $\\gamma_n = -\\frac{n}{n+1}$. This gives the recurrence for $n \\ge 1$:\n$$\nP_{n+1}(x) = \\frac{2n+1}{n+1}xP_n(x) - \\frac{n}{n+1}P_{n-1}(x)\n$$\nBase cases are $P_0(x)=1$ and $P_1(x)=x$, which follow from degree requirements, normalization $P_n(1)=1$, and orthogonality $\\langle P_1, P_0 \\rangle = 0$.\n\n### Part 2: Least Squares Coefficients\n\nWe wish to find the coefficients $\\{a_n\\}_{n=0}^N$ that minimize the mean-squared error:\n$$\nE(a_0, \\dots, a_N) = \\int_{-1}^{1} \\left( f(x) - \\sum_{n=0}^{N} a_n P_n(x) \\right)^2 dx\n$$\nUsing the inner product notation, $E = \\left\\langle f - \\sum_{n=0}^{N} a_n P_n, f - \\sum_{n=0}^{N} a_n P_n \\right\\rangle$.\nThe minimum is found by setting the partial derivative with respect to each coefficient $a_k$ to zero:\n$$\n\\frac{\\partial E}{\\partial a_k} = \\int_{-1}^{1} 2 \\left( f(x) - \\sum_{n=0}^{N} a_n P_n(x) \\right) (-P_k(x)) dx = 0\n$$\nThis simplifies to $\\left\\langle f, P_k \\right\\rangle = \\left\\langle \\sum_{n=0}^{N} a_n P_n, P_k \\right\\rangle$. Using the linearity of the inner product and the orthogonality property $\\langle P_n, P_k \\rangle = 0$ for $n \\neq k$:\n$$\n\\langle f, P_k \\rangle = \\sum_{n=0}^{N} a_n \\langle P_n, P_k \\rangle = a_k \\langle P_k, P_k \\rangle\n$$\nSolving for $a_k$, we get the general formula for the coefficient of a projection onto an orthogonal basis:\n$$\na_k = \\frac{\\langle f, P_k \\rangle}{\\langle P_k, P_k \\rangle} = \\frac{\\int_{-1}^{1} f(x)P_k(x)\\,dx}{\\int_{-1}^{1} P_k(x)^2\\,dx}\n$$\nTo obtain the simplified closed form, we use the established fact that $\\int_{-1}^{1} P_k(x)^2\\,dx = \\frac{2}{2k+1}$. Substituting this into the expression for $a_k$ (and switching index to $n$ for consistency):\n$$\na_n = \\frac{\\int_{-1}^{1} f(x)P_n(x)\\,dx}{2/(2n+1)} = \\frac{2n+1}{2} \\int_{-1}^{1} f(x)P_n(x)\\,dx\n$$\nThis is the desired simplified expression for the continuous least squares coefficients.\n\n### Part 3: Algorithm Design and Analysis\n\nThe task is to design an algorithm to compute $\\{a_n\\}_{n=0}^N$ based on the derived formulas, using Gauss-Legendre quadrature.\n\n**Algorithm Steps:**\n\n1.  **Quadrature Setup**: For a given number of quadrature points $Q$, obtain the Gauss-Legendre nodes $\\{x_j\\}_{j=1}^Q$ and weights $\\{w_j\\}_{j=1}^Q$ on the interval $[-1, 1]$.\n2.  **Function Evaluation**: Evaluate the function $f(x)$ at each node $x_j$ to get a vector of values $F_j = f(x_j)$.\n3.  **Polynomial Evaluation via Recurrence**: Evaluate the Legendre polynomials $P_n(x_j)$ for all required degrees $n=0, \\dots, N$ at all nodes $x_j$. To do this stably and efficiently, use the three-term recurrence derived in Part 1. Store the results in a matrix $\\mathbf{P}$ of size $(N+1) \\times Q$, where $\\mathbf{P}_{n,j} = P_n(x_j)$.\n    -   Initialize $\\mathbf{P}_{0,j} = 1$ for all $j$.\n    -   Initialize $\\mathbf{P}_{1,j} = x_j$ for all $j$.\n    -   For $n$ from $1$ to $N-1$, compute the $(n+1)$-th row using vectorized operations:\n        $$ \\mathbf{P}_{n+1,:} = \\frac{2n+1}{n+1} \\mathbf{x} \\odot \\mathbf{P}_{n,:} - \\frac{n}{n+1} \\mathbf{P}_{n-1,:} $$\n        where $\\mathbf{x}$ is the vector of nodes and $\\odot$ denotes element-wise multiplication.\n4.  **Approximate Inner Products**: Approximate the integrals $\\langle f, P_n \\rangle = \\int_{-1}^{1} f(x)P_n(x) dx$ using quadrature:\n    $$\n    \\langle f, P_n \\rangle \\approx \\sum_{j=1}^{Q} w_j f(x_j) P_n(x_j)\n    $$\n    This can be computed for all $n$ simultaneously using a matrix-vector product. Let $\\mathbf{v}$ be a vector where $v_j = w_j F_j$. The vector of all $N+1$ approximate integrals $\\mathbf{I}$ is given by $\\mathbf{I} = \\mathbf{P} \\mathbf{v}$.\n5.  **Compute Coefficients**: Using the formula from Part 2, calculate each coefficient $a_n$:\n    $$\n    a_n = \\frac{2n+1}{2} I_n\n    $$\n\n**Computational Complexity Analysis:**\n\n-   Step 1 (Nodes/Weights): Standard algorithms take $\\mathcal{O}(Q^2)$, but this is typically a pre-computation or provided by a library and can be considered separate.\n-   Step 2 (Function Evaluation): Evaluating $f(x)$ at $Q$ nodes is $\\mathcal{O}(Q)$, assuming $f(x)$ is $\\mathcal{O}(1)$.\n-   Step 3 (Polynomial Evaluation): The recurrence loop runs $N-1$ times. Each step is a vectorized operation on arrays of size $Q$, costing $\\mathcal{O}(Q)$. The total cost for this step is $\\mathcal{O}(NQ)$.\n-   Step 4 (Inner Products): The matrix-vector product of an $(N+1) \\times Q$ matrix and a $Q \\times 1$ vector takes $\\mathcal{O}(NQ)$ operations.\n-   Step 5 (Coefficients): This is a loop of size $N+1$ with $\\mathcal{O}(1)$ work inside. Total cost is $\\mathcal{O}(N)$.\n\nThe dominant cost is in steps 3 and 4, making the overall algorithm complexity $\\mathcal{O}(NQ)$.\n\n**Numerical Stability Analysis:**\n\n-   The use of an orthogonal basis (Legendre polynomials) is the cornerstone of the numerical stability of this method. It diagonalizes the normal equations matrix, which for a non-orthogonal basis like monomials $\\{x^n\\}$ would be a severely ill-conditioned Hilbert-like matrix.\n-   The three-term recurrence is a well-known stable method for evaluating orthogonal polynomials within their interval of orthogonality $[-1, 1]$, as the values are bounded ($|P_n(x)| \\le 1$). This avoids the large intermediate values and cancellations that plague naive polynomial evaluation.\n-   Gauss-Legendre quadrature is stable, as all its weights $\\{w_j\\}$ are positive, preventing catastrophic cancellation in the summation.\n\nThis algorithm is therefore efficient and numerically stable, making it a preferred method for continuous least squares approximation.\n\n### Part 4: Implementation\n\nThe algorithm described in Part 3 is implemented in the accompanying Python program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the result.\n    \"\"\"\n\n    def compute_coefficients(f, N, Q):\n        \"\"\"\n        Computes the continuous least squares coefficients {a_n} for a function f\n        approximated by a sum of Legendre polynomials up to degree N, using\n        Gauss-Legendre quadrature with Q points.\n\n        Args:\n            f (callable): The function to approximate.\n            N (int): The maximum degree of the polynomial approximation.\n            Q (int): The number of quadrature nodes to use.\n\n        Returns:\n            np.ndarray: An array of coefficients [a_0, a_1, ..., a_N].\n        \"\"\"\n        # Step 1: Get Gauss-Legendre nodes and weights for the interval [-1, 1]\n        x_nodes, w_weights = np.polynomial.legendre.leggauss(Q)\n        \n        # Step 2: Evaluate the function f at the quadrature nodes\n        f_vals = f(x_nodes)\n        \n        # Step 3: Evaluate P_n(x_j) using the three-term recurrence\n        # The matrix P_matrix will store P_n(x_j) at P_matrix[n, j]\n        P_matrix = np.zeros((N + 1, Q))\n        \n        # Base case: P_0(x) = 1\n        P_matrix[0, :] = 1.0\n        \n        # Base case: P_1(x) = x (if N is large enough)\n        if N > 0:\n            P_matrix[1, :] = x_nodes\n        \n        # Recurrence relation:\n        # (n+1)P_{n+1}(x) = (2n+1)xP_n(x) - nP_{n-1}(x)\n        # P_{k+1}(x) = ((2k+1)/(k+1))xP_k(x) - (k/(k+1))P_{k-1}(x)\n        # In the loop, we compute P_{n+1} from P_n and P_{n-1}, letting n go from 1 to N-1.\n        for n in range(1, N):\n            P_matrix[n + 1, :] = ((2 * n + 1) / (n + 1)) * x_nodes * P_matrix[n, :] - (n / (n + 1)) * P_matrix[n - 1, :]\n            \n        # Step 4: Approximate the inner products <f, P_n> using quadrature\n        # This is a matrix-vector product of P_matrix and (w_weights * f_vals)\n        integrals = np.dot(P_matrix, w_weights * f_vals)\n        \n        # Step 5: Compute the coefficients a_n\n        # a_n = (2n+1)/2 * <f, P_n>\n        coeffs = np.zeros(N + 1)\n        n_vals = np.arange(N + 1)\n        coeffs = (2 * n_vals + 1) / 2 * integrals\n            \n        return coeffs\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'f': lambda x: np.exp(x), 'N': 5, 'Q': 200, 'label': 'e^x'},\n        {'f': lambda x: x**3 - x, 'N': 5, 'Q': 6, 'label': 'x^3-x'},\n        {'f': lambda x: 1.0 + 0*x, 'N': 7, 'Q': 2, 'label': '1'},\n        {'f': lambda x: x, 'N': 0, 'Q': 4, 'label': 'x'},\n        {'f': lambda x: np.cos(3*x), 'N': 20, 'Q': 300, 'label': 'cos(3x)'}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        coefficients = compute_coefficients(case['f'], case['N'], case['Q'])\n        all_results.append(coefficients)\n\n    # Final print statement in the exact required format.\n    case_strings = []\n    for result_list in all_results:\n        num_strings = [f\"{num:.12f}\" for num in result_list]\n        case_strings.append(f\"[{','.join(num_strings)}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}