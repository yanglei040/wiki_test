{
    "hands_on_practices": [
        {
            "introduction": "While ready-made formulas for standard polynomial regression are abundant, a true grasp of the least squares method comes from applying its fundamental principles to custom scenarios. This practice challenges you to derive the normal equations for a quadratic model with a specific constraint: it must pass through the origin. By working from the first principle of minimizing the sum of squared residuals, you will reinforce the foundational calculus behind the method and gain the skill to adapt it to non-standard problems you may encounter in practice .",
            "id": "3223314",
            "problem": "Consider the discrete data set consisting of $7$ points $(x_i,y_i)$ with\n$(x_1,y_1)=(-3,18)$, $(x_2,y_2)=(-2,9)$, $(x_3,y_3)=(-1,4)$, $(x_4,y_4)=(0,3)$, $(x_5,y_5)=(1,6)$, $(x_6,y_6)=(2,13)$, $(x_7,y_7)=(3,24)$. These values arise from the underlying relation $y=2x^2+x+3$ sampled at the listed $x$-values. Let $p(x)$ be a quadratic polynomial constrained to pass through the origin, i.e., $p(0)=0$, and write it as $p(x)=a x^2 + b x$.\n\nUsing only the fundamental definition of a discrete least squares approximation—namely, that $p(x)$ minimizes the sum of squared residuals $\\sum_{i=1}^{7} \\big(p(x_i)-y_i\\big)^2$ over the given data—determine the coefficients $a$ and $b$ and provide the explicit analytic expression for the best-fit quadratic $p(x)$ that satisfies $p(0)=0$. Express your final answer in exact form; no rounding is required.",
            "solution": "The problem is to find the coefficients $a$ and $b$ of the quadratic polynomial $p(x) = ax^2 + bx$ that provides the best discrete least squares approximation to a given set of $7$ data points. The polynomial is constrained to pass through the origin, which is satisfied by its form $p(0) = a(0)^2 + b(0) = 0$.\n\nThe fundamental principle of the method of least squares is to minimize the sum of the squared residuals, $E$, which is the sum of the squared differences between the model's predictions $p(x_i)$ and the observed data values $y_i$. The error function $E(a,b)$ is given by:\n$$E(a,b) = \\sum_{i=1}^{7} \\left(p(x_i) - y_i\\right)^2 = \\sum_{i=1}^{7} \\left(ax_i^2 + bx_i - y_i\\right)^2$$\nTo find the values of $a$ and $b$ that minimize this function, we must find the critical point where the partial derivatives of $E$ with respect to $a$ and $b$ are both equal to zero.\n$$ \\frac{\\partial E}{\\partial a} = 0 \\quad \\text{and} \\quad \\frac{\\partial E}{\\partial b} = 0 $$\nCalculating the partial derivative with respect to $a$:\n$$ \\frac{\\partial E}{\\partial a} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial a}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i^2) $$\nSetting this to zero and dividing by $2$:\n$$ \\sum_{i=1}^{7} (ax_i^4 + bx_i^3 - y_i x_i^2) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^4\\right) + b\\left(\\sum_{i=1}^{7} x_i^3\\right) = \\sum_{i=1}^{7} y_i x_i^2 $$\nThis is the first of the two normal equations.\n\nCalculating the partial derivative with respect to $b$:\n$$ \\frac{\\partial E}{\\partial b} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial b}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i) $$\nSetting this to zero and dividing by $2$:\n$$ \\sum_{i=1}^{7} (ax_i^3 + bx_i^2 - y_i x_i) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^3\\right) + b\\left(\\sum_{i=1}^{7} x_i^2\\right) = \\sum_{i=1}^{7} y_i x_i $$\nThis is the second normal equation.\n\nThe next step is to compute the required sums from the given data set:\n$(x_1, y_1)=(-3, 18)$, $(x_2, y_2)=(-2, 9)$, $(x_3, y_3)=(-1, 4)$, $(x_4, y_4)=(0, 3)$, $(x_5, y_5)=(1, 6)$, $(x_6, y_6)=(2, 13)$, $(x_7, y_7)=(3, 24)$.\nThe set of $x$-values is $\\{ -3, -2, -1, 0, 1, 2, 3 \\}$. This set is symmetric about $x=0$, which simplifies the sums of odd powers of $x_i$ to zero.\n\nWe calculate the sums:\n$$ \\sum_{i=1}^{7} x_i^2 = (-3)^2 + (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 3^2 = 9 + 4 + 1 + 0 + 1 + 4 + 9 = 28 $$\n$$ \\sum_{i=1}^{7} x_i^3 = (-3)^3 + (-2)^3 + (-1)^3 + 0^3 + 1^3 + 2^3 + 3^3 = -27 - 8 - 1 + 0 + 1 + 8 + 27 = 0 $$\n$$ \\sum_{i=1}^{7} x_i^4 = (-3)^4 + (-2)^4 + (-1)^4 + 0^4 + 1^4 + 2^4 + 3^4 = 81 + 16 + 1 + 0 + 1 + 16 + 81 = 196 $$\nNext, we compute the sums involving $y_i$:\n$$ \\sum_{i=1}^{7} y_i x_i = (18)(-3) + (9)(-2) + (4)(-1) + (3)(0) + (6)(1) + (13)(2) + (24)(3) \\\\ = -54 - 18 - 4 + 0 + 6 + 26 + 72 = -76 + 104 = 28 $$\n$$ \\sum_{i=1}^{7} y_i x_i^2 = (18)(-3)^2 + (9)(-2)^2 + (4)(-1)^2 + (3)(0)^2 + (6)(1)^2 + (13)(2)^2 + (24)(3)^2 \\\\ = 18(9) + 9(4) + 4(1) + 3(0) + 6(1) + 13(4) + 24(9) \\\\ = 162 + 36 + 4 + 0 + 6 + 52 + 216 = 476 $$\nNow, we substitute these numerical values into the normal equations:\n1. $a(196) + b(0) = 476$\n2. $a(0) + b(28) = 28$\n\nThis forms a simple, uncoupled system of linear equations for $a$ and $b$.\nFrom the first equation:\n$$ 196a = 476 \\implies a = \\frac{476}{196} $$\nTo simplify the fraction, we can divide the numerator and denominator by common factors. Both are divisible by $4$:\n$$ a = \\frac{476 \\div 4}{196 \\div 4} = \\frac{119}{49} $$\nNow we recognize that $119 = 7 \\times 17$ and $49 = 7 \\times 7$:\n$$ a = \\frac{7 \\times 17}{7 \\times 7} = \\frac{17}{7} $$\nFrom the second equation:\n$$ 28b = 28 \\implies b = 1 $$\nThe coefficients are $a = \\frac{17}{7}$ and $b = 1$. The resulting best-fit quadratic polynomial is:\n$$ p(x) = \\frac{17}{7}x^2 + x $$\nThe problem states that the data arise from the relation $y = 2x^2 + x + 3$. The least squares fit retrieves the coefficient $b=1$ exactly. This is because for a symmetric set of $x_i$ values and data generated by $y_i=f(x_i)$, the cross-correlation terms between even and odd parts of the function vanish. The coefficient $a$ is $\\frac{17}{7} \\approx 2.428$, which differs from the original coefficient $2$ because the model $p(x)$ is forced to be $0$ at $x=0$, while the data point is $(0,3)$. The least squares method adjusts the curvature parameter $a$ to minimize the total squared error over all points, accommodating the large residual at the origin.",
            "answer": "$$\\boxed{p(x) = \\frac{17}{7}x^2 + x}$$"
        },
        {
            "introduction": "The discrete least squares method is a cornerstone of data fitting, but it has a well-known Achilles' heel: its sensitivity to outliers. Because the method minimizes the sum of *squared* errors, a single data point with a large error can disproportionately pull the entire fit towards it. This hands-on exercise allows you to computationally investigate this phenomenon, exploring how both an outlier's vertical distance from the true model (its residual) and its horizontal position (its leverage) impact the final solution .",
            "id": "3223340",
            "problem": "You are tasked with analyzing the sensitivity of a discrete least squares polynomial fit to the presence of a single, massive outlier. Work within the following purely mathematical and computational framework.\n\nGiven a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$, the discrete least squares polynomial of degree $m$ is the polynomial $p_m(x) = \\sum_{k=0}^m c_k x^k$ whose coefficients $\\boldsymbol{c} = [c_0,\\dots,c_m]^\\top$ minimize the sum of squared residuals $\\sum_{i=1}^n (y_i - p_m(x_i))^2$. In matrix form, if $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (m+1)}$ has entries $A_{i,k} = x_i^k$ and $\\boldsymbol{y} \\in \\mathbb{R}^n$ has entries $y_i$, the coefficients solve the minimization problem $\\min_{\\boldsymbol{c} \\in \\mathbb{R}^{m+1}} \\|\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{y}\\|_2^2$. Assume the matrix $\\boldsymbol{A}$ has full column rank so that the minimizer is unique.\n\nStart from the following base dataset corresponding to a noise-free quadratic:\n- Degree $m = 2$.\n- Baseline abscissae $\\boldsymbol{x}_{\\text{base}} = [-2, -1, 0, 1, 2]$.\n- Baseline ordinates $\\boldsymbol{y}_{\\text{base}}$ given by $y_i = x_i^2$ for each $x_i \\in \\boldsymbol{x}_{\\text{base}}$.\n\nLet the baseline least squares coefficients be $\\boldsymbol{c}^{(\\text{base})} \\in \\mathbb{R}^{3}$. Consider augmenting the dataset by a single additional point $(x_{\\text{out}}, y_{\\text{out}})$ to form a new fit with coefficients $\\boldsymbol{c}^{(\\text{aug})} \\in \\mathbb{R}^{3}$. For each specified outlier, quantify the effect of the outlier on the fit by computing the Euclidean norm of the coefficient change $\\|\\boldsymbol{c}^{(\\text{aug})} - \\boldsymbol{c}^{(\\text{base})}\\|_2$ and the new coefficients $\\boldsymbol{c}^{(\\text{aug})}$.\n\nBase your reasoning on fundamental definitions of discrete least squares approximation, normal equations, and linear algebraic properties of full column rank matrices. Do not assume any formula for the effect of rank-one updates; derive any needed relations from first principles.\n\nYour program must:\n- Construct the baseline least squares fit for the given baseline dataset with degree $m = 2$.\n- For each test case below, form the augmented dataset by appending the single given outlier $(x_{\\text{out}}, y_{\\text{out}})$ and recompute the least squares coefficients.\n- For each test case, output a list of four floating point numbers: $[\\|\\boldsymbol{c}^{(\\text{aug})} - \\boldsymbol{c}^{(\\text{base})}\\|_2, c_0^{(\\text{aug})}, c_1^{(\\text{aug})}, c_2^{(\\text{aug})}]$, each rounded to six decimal places.\n\nTest suite to evaluate different leverage and residual regimes:\n- Case $1$ (low-leverage, massive residual at center): $(x_{\\text{out}}, y_{\\text{out}}) = (0, 1000)$.\n- Case $2$ (moderate high-leverage, massive residual outside range): $(x_{\\text{out}}, y_{\\text{out}}) = (3, 1000)$.\n- Case $3$ (edge leverage, massive negative residual at boundary): $(x_{\\text{out}}, y_{\\text{out}}) = (2, -1000)$.\n- Case $4$ (extreme leverage, massive residual far outside range): $(x_{\\text{out}}, y_{\\text{out}}) = (-10, 1000)$.\n- Case $5$ (control, no residual outlier; consistent with the model): $(x_{\\text{out}}, y_{\\text{out}}) = (3, 9)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each element corresponds to one test case in the order listed above and is itself a list of four rounded floats. For example, the format must be exactly like\n- $[[0.123456,0.000001,0.000002,0.999997],[\\dots],\\dots]$\nwith no additional text and no spaces.",
            "solution": "The user's request is a valid and well-posed problem in numerical linear algebra. It is scientifically grounded in the theory of discrete least squares approximation, provides all necessary data and constraints, and is expressed in objective, formal language. There are no contradictions, ambiguities, or pseudo-scientific claims. The problem asks for a direct application of fundamental principles to analyze the sensitivity of a polynomial fit to outliers.\n\nThe problem requires finding a polynomial $p_m(x) = \\sum_{k=0}^m c_k x^k$ that best fits a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$ in the least squares sense. This is achieved by minimizing the sum of squared residuals, $S(\\boldsymbol{c})$, defined as:\n$$S(\\boldsymbol{c}) = \\sum_{i=1}^n (y_i - p_m(x_i))^2$$\nThe vector of coefficients is $\\boldsymbol{c} = [c_0, c_1, \\dots, c_m]^\\top \\in \\mathbb{R}^{m+1}$. This minimization problem can be expressed in matrix form. Let $\\boldsymbol{y} \\in \\mathbb{R}^n$ be the vector of ordinates $y_i$, and let $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times (m+1)}$ be the Vandermonde-like matrix with entries $A_{i,k} = x_i^k$ for $i=1, \\dots, n$ and $k=0, \\dots, m$. The product $\\boldsymbol{A}\\boldsymbol{c}$ is a vector whose $i$-th entry is $p_m(x_i)$. The minimization problem is then equivalent to:\n$$\\min_{\\boldsymbol{c} \\in \\mathbb{R}^{m+1}} \\|\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{y}\\|_2^2$$\nTo find the minimum, we take the gradient of the squared norm, $E(\\boldsymbol{c}) = \\|\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{y}\\|_2^2$, with respect to $\\boldsymbol{c}$ and set it to zero. The squared norm can be written as:\n$$E(\\boldsymbol{c}) = (\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{y})^\\top (\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{y}) = \\boldsymbol{c}^\\top \\boldsymbol{A}^\\top \\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{c}^\\top \\boldsymbol{A}^\\top \\boldsymbol{y} - \\boldsymbol{y}^\\top \\boldsymbol{A}\\boldsymbol{c} + \\boldsymbol{y}^\\top \\boldsymbol{y}$$\nSince $\\boldsymbol{y}^\\top \\boldsymbol{A}\\boldsymbol{c}$ is a scalar, it is equal to its transpose, $(\\boldsymbol{y}^\\top \\boldsymbol{A}\\boldsymbol{c})^\\top = \\boldsymbol{c}^\\top \\boldsymbol{A}^\\top \\boldsymbol{y}$. Thus, we have:\n$$E(\\boldsymbol{c}) = \\boldsymbol{c}^\\top (\\boldsymbol{A}^\\top \\boldsymbol{A})\\boldsymbol{c} - 2\\boldsymbol{y}^\\top \\boldsymbol{A}\\boldsymbol{c} + \\boldsymbol{y}^\\top \\boldsymbol{y}$$\nThe gradient of $E(\\boldsymbol{c})$ with respect to $\\boldsymbol{c}$ is:\n$$\\nabla_{\\boldsymbol{c}} E(\\boldsymbol{c}) = 2(\\boldsymbol{A}^\\top \\boldsymbol{A})\\boldsymbol{c} - 2\\boldsymbol{A}^\\top \\boldsymbol{y}$$\nSetting the gradient to $\\boldsymbol{0}$ yields the **Normal Equations**:\n$$(\\boldsymbol{A}^\\top \\boldsymbol{A})\\boldsymbol{c} = \\boldsymbol{A}^\\top \\boldsymbol{y}$$\nThe problem states that the matrix $\\boldsymbol{A}$ has full column rank. This implies that the Gram matrix $\\boldsymbol{A}^\\top \\boldsymbol{A}$ is symmetric, positive definite, and therefore invertible. This guarantees the existence of a unique solution for the coefficient vector $\\boldsymbol{c}$:\n$$\\boldsymbol{c} = (\\boldsymbol{A}^\\top \\boldsymbol{A})^{-1} \\boldsymbol{A}^\\top \\boldsymbol{y}$$\nThe procedure to solve the problem is as follows:\n\nFirst, we compute the baseline coefficients $\\boldsymbol{c}^{(\\text{base})}$. The problem specifies a polynomial degree $m=2$. The baseline dataset has $n=5$ points, with abscissae $\\boldsymbol{x}_{\\text{base}} = [-2, -1, 0, 1, 2]$ and ordinates $\\boldsymbol{y}_{\\text{base}}$ given by $y_i = x_i^2$, which are $[4, 1, 0, 1, 4]$. We construct the $5 \\times 3$ matrix $\\boldsymbol{A}_{\\text{base}}$ and the vector $\\boldsymbol{y}_{\\text{base}}$, and solve the linear system $(\\boldsymbol{A}_{\\text{base}}^\\top \\boldsymbol{A}_{\\text{base}})\\boldsymbol{c}^{(\\text{base})} = \\boldsymbol{A}_{\\text{base}}^\\top \\boldsymbol{y}_{\\text{base}}$ for $\\boldsymbol{c}^{(\\text{base})}$. Since the data perfectly fit the polynomial $p(x) = 0 \\cdot x^0 + 0 \\cdot x^1 + 1 \\cdot x^2$, we expect to find $\\boldsymbol{c}^{(\\text{base})} = [0, 0, 1]^\\top$.\n\nSecond, for each test case, we augment the baseline dataset with the specified outlier point $(x_{\\text{out}}, y_{\\text{out}})$. This creates a new dataset with $n=6$ points. We form the new $6 \\times 3$ matrix $\\boldsymbol{A}_{\\text{aug}}$ and the new vector $\\boldsymbol{y}_{\\text{aug}}$. We then solve the corresponding normal equations, $(\\boldsymbol{A}_{\\text{aug}}^\\top \\boldsymbol{A}_{\\text{aug}})\\boldsymbol{c}^{(\\text{aug})} = \\boldsymbol{A}_{\\text{aug}}^\\top \\boldsymbol{y}_{\\text{aug}}$, for the new coefficient vector $\\boldsymbol{c}^{(\\text{aug})}$.\n\nFinally, for each case, we quantify the effect of the outlier by computing the Euclidean norm of the difference between the augmented and baseline coefficient vectors, $\\|\\boldsymbol{c}^{(\\text{aug})} - \\boldsymbol{c}^{(\\text{base})}\\|_2$. The required output for each case is a list containing this norm and the three components of the new coefficient vector $\\boldsymbol{c}^{(\\text{aug})} = [c_0^{(\\text{aug})}, c_1^{(\\text{aug})}, c_2^{(\\text{aug})}]^\\top$. This will be done for all five specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the sensitivity of a discrete least squares polynomial fit to a single outlier.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 1000),      # Case 1: low-leverage, massive residual at center\n        (3, 1000),      # Case 2: moderate high-leverage, massive residual outside range\n        (2, -1000),     # Case 3: edge leverage, massive negative residual at boundary\n        (-10, 1000),    # Case 4: extreme leverage, massive residual far outside range\n        (3, 9)          # Case 5: control, no residual outlier\n    ]\n\n    # Degree of the polynomial\n    m = 2\n    \n    # Baseline dataset\n    x_base = np.array([-2, -1, 0, 1, 2], dtype=float)\n    y_base = x_base**2\n    \n    # Construct the baseline Vandermonde matrix A_base\n    # A_{i,k} = x_i^k, so we use increasing=True\n    A_base = np.vander(x_base, m + 1, increasing=True)\n    \n    # Solve the normal equations for the baseline coefficients c_base\n    # (A.T @ A) c = A.T @ y\n    # np.linalg.solve is numerically preferable to inv()\n    c_base = np.linalg.solve(A_base.T @ A_base, A_base.T @ y_base)\n\n    results = []\n    \n    # Iterate through each test case (outlier)\n    for x_out, y_out in test_cases:\n        # Form the augmented dataset\n        x_aug = np.append(x_base, x_out)\n        y_aug = np.append(y_base, y_out)\n        \n        # Construct the augmented Vandermonde matrix A_aug\n        A_aug = np.vander(x_aug, m + 1, increasing=True)\n        \n        # Solve the normal equations for the augmented coefficients c_aug\n        c_aug = np.linalg.solve(A_aug.T @ A_aug, A_aug.T @ y_aug)\n        \n        # Calculate the Euclidean norm of the change in coefficients\n        norm_diff = np.linalg.norm(c_aug - c_base)\n        \n        # Collect the results for this case\n        case_result = [norm_diff, c_aug[0], c_aug[1], c_aug[2]]\n        results.append(case_result)\n\n    # Format the final output string as specified\n    # Each sublist is formatted as \"[d,c0,c1,c2]\" with 6 decimal places\n    # The outer list is a comma-separated join of the sublist strings\n    inner_strings = []\n    for R in results:\n        # Format each number to 6 decimal places and join with commas\n        formatted_r = [f\"{val:.6f}\" for val in R]\n        inner_strings.append(f\"[{','.join(formatted_r)}]\")\n        \n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "In the world of numerical computing, a mathematically correct formula is not always a numerically stable one. This practice explores a classic example: solving a least squares problem using the Normal Equations versus using QR factorization. While mathematically equivalent, this exercise demonstrates how the Normal Equations can become catastrophically unstable for ill-conditioned problems, which often arise from nearly-collinear data points. By comparing the results, you will gain a crucial appreciation for why robust numerical algorithms like QR factorization are the professional standard .",
            "id": "3223242",
            "problem": "You are given a discrete data fitting task in which a polynomial model is fitted to data points using discrete least squares approximation. The goal is to examine how extremely close but distinct abscissae affect numerical stability. You must implement two solvers and quantify the impact on stability by carefully chosen metrics.\n\nStart from the following foundational base: given a set of points $\\{(x_i,y_i)\\}_{i=1}^n$ and a polynomial model of degree $m$, construct the design matrix (Vandermonde matrix) $V$ whose $(i,j)$ entry is $x_i^{j-1}$ for $i=1,\\dots,n$ and $j=1,\\dots,m+1$. Then, the discrete least squares approximation problem is to find coefficients $c \\in \\mathbb{R}^{m+1}$ that minimize the squared residual norm $\\lVert Vc - y \\rVert_2^2$, where $y \\in \\mathbb{R}^n$ is the vector of observed values.\n\nYour program must:\n- Compute the polynomial coefficients using two methods:\n  1. Solve the first-order optimality conditions associated with the squared residual minimization, using the matrix of the optimality conditions to obtain a coefficient vector $c_{\\mathrm{NE}}$ (this approach is commonly referred to as solving via the normal equations).\n  2. Use Orthogonal Triangular (QR) factorization, writing $V = QR$ with $Q$ orthogonal and $R$ upper triangular, then solving for a coefficient vector $c_{\\mathrm{QR}}$ by back-substitution.\n- Quantify numerical stability by computing:\n  - The $2$-norm condition number $\\kappa_2(V)$ of the Vandermonde matrix $V$.\n  - The $2$-norm condition number $\\kappa_2(V^\\top V)$.\n  - The relative difference between the two coefficient vectors: $\\dfrac{\\lVert c_{\\mathrm{NE}} - c_{\\mathrm{QR}} \\rVert_2}{\\lVert c_{\\mathrm{QR}} \\rVert_2}$.\n  - The absolute difference in residual norms: $\\big|\\lVert y - V c_{\\mathrm{NE}} \\rVert_2 - \\lVert y - V c_{\\mathrm{QR}} \\rVert_2\\big|$.\n\nUse the underlying cubic polynomial $p(x) = 1 - 2x + 3x^2 - x^3$ as the generative model, and form data by $y_i = p(x_i) + \\delta_i$, where $\\delta_i = 10^{-8}(i+1)$ for $i \\in \\{0,1,\\dots,n-1\\}$. There is no randomness; all values are deterministic. There are no physical units in this problem.\n\nImplement the solution for the following test suite, each case specifying the abscissae $x_i$ and the degree $m$:\n- Case $1$ (well-spaced points, happy path): $x = [0.0, 0.5, 1.0, 1.5, 2.0]$, $m = 3$.\n- Case $2$ (two points extremely close): $x = [0.0, 1.0, 1.0 + 10^{-12}, 2.0, 3.0]$, $m = 3$.\n- Case $3$ (closer still, near the limits of double precision without equality): $x = [-1.0, 0.0, 1.0, 1.0 + 10^{-15}, 2.0]$, $m = 3$.\n\nYour program must produce, for each case, a list $[\\kappa_2(V), \\kappa_2(V^\\top V), \\text{relative\\_coefficient\\_difference}, \\text{residual\\_norm\\_difference}]$. The final output should be a single line containing a comma-separated list of these lists enclosed in square brackets, like $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$, where all $a_k$, $b_k$, $c_k$, and $d_k$ are floating-point numbers computed by your program.\n\nNo user input should be read. The output must be deterministic and reproducible. Angles are not used, and no physical units are involved. Express all quantities as floating-point numbers. The final output format must be exactly one printed line as specified above.",
            "solution": "The task is to analyze the numerical stability of two different methods for solving a discrete polynomial least squares problem: the normal equations method and the QR factorization method. The stability will be evaluated under conditions where the input data points are extremely close, which is known to cause ill-conditioning in the problem's design matrix.\n\nThe discrete least squares approximation problem seeks to find the coefficients $c \\in \\mathbb{R}^{m+1}$ of a polynomial $p_m(x) = \\sum_{j=0}^{m} c_j x^j$ that best fits a given set of $n$ data points $\\{(x_i, y_i)\\}_{i=0}^{n-1}$. \"Best fit\" is defined in the sense of minimizing the sum of the squared residuals, $S(c) = \\sum_{i=0}^{n-1} (p_m(x_i) - y_i)^2$. This objective function can be expressed in matrix form as minimizing the squared $2$-norm of the residual vector:\n$$\nS(c) = \\lVert Vc - y \\rVert_2^2\n$$\nwhere $y = [y_0, y_1, \\dots, y_{n-1}]^\\top$ is the vector of observed values, $c = [c_0, c_1, \\dots, c_m]^\\top$ is the vector of polynomial coefficients, and $V$ is the $n \\times (m+1)$ Vandermonde matrix defined by its entries $V_{ij} = x_i^j$ for $i \\in \\{0, \\dots, n-1\\}$ and $j \\in \\{0, \\dots, m\\}$.\n\nThe problem specifies using a generative model $p(x) = 1 - 2x + 3x^2 - x^3$ to create the data. The observed values $y_i$ are given by $y_i = p(x_i) + \\delta_i$, where $\\delta_i = 10^{-8}(i+1)$ is a small, deterministic perturbation. We will compare two methods for finding the coefficient vector $c$.\n\n**Method 1: The Normal Equations**\n\nThe minimum of the squared residual $S(c)$ occurs where its gradient with respect to $c$ is zero. The gradient is:\n$$\n\\nabla_c S(c) = \\nabla_c (Vc - y)^\\top(Vc - y) = \\nabla_c (c^\\top V^\\top V c - 2y^\\top V c + y^\\top y) = 2V^\\top V c - 2V^\\top y\n$$\nSetting the gradient to zero, $\\nabla_c S(c) = 0$, yields the system of linear equations known as the normal equations:\n$$\n(V^\\top V) c = V^\\top y\n$$\nThe solution, which we denote as $c_{\\mathrm{NE}}$, can be found by solving this $(m+1) \\times (m+1)$ system. A critical issue with this method is that the condition number of the matrix $V^\\top V$ is the square of the condition number of $V$: $\\kappa_2(V^\\top V) = [\\kappa_2(V)]^2$. If $V$ is ill-conditioned (i.e., has a large condition number), $V^\\top V$ will be severely ill-conditioned. This can lead to a computed solution $c_{\\mathrm{NE}}$ that is highly sensitive to perturbations and floating-point errors.\n\n**Method 2: QR Factorization**\n\nThis method avoids the explicit formation of $V^\\top V$. It relies on the QR factorization of the design matrix $V$, which decomposes it into a product $V = QR$, where $Q$ is an $n \\times (m+1)$ matrix with orthonormal columns ($Q^\\top Q = I_{m+1}$), and $R$ is an $(m+1) \\times (m+1)$ upper triangular matrix. This is the \"thin\" or \"reduced\" QR factorization.\n\nSubstituting $V=QR$ into the objective function, we get:\n$$\n\\lVert Vc - y \\rVert_2^2 = \\lVert QRc - y \\rVert_2^2\n$$\nSince $Q$ has orthonormal columns, multiplying by $Q^\\top$ from the left does not change the $2$-norm of a vector in the column space of $Q$. While a full argument involves the full QR decomposition, the result is that the minimization problem is equivalent to solving the upper triangular system:\n$$\nRc = Q^\\top y\n$$\nThis system can be efficiently and stably solved for the coefficient vector, denoted $c_{\\mathrm{QR}}$, using back-substitution. The numerical advantage of this method is that it operates on matrices ($Q$ and $R$) whose conditioning is related to that of the original matrix $V$, not its square. Thus, QR factorization is generally much more numerically stable for solving least squares problems than the normal equations.\n\n**Numerical Stability Analysis**\n\nTo quantify the difference in numerical stability, we will compute four metrics for each test case:\n$1$. The $2$-norm condition number of the Vandermonde matrix, $\\kappa_2(V)$. This measures the sensitivity of the least squares problem itself.\n$2$. The $2$-norm condition number of the normal equations matrix, $\\kappa_2(V^\\top V)$. This highlights the squaring effect on the condition number.\n$3$. The relative difference in the computed coefficient vectors, $\\dfrac{\\lVert c_{\\mathrm{NE}} - c_{\\mathrm{QR}} \\rVert_2}{\\lVert c_{\\mathrm{QR}} \\rVert_2}$. This measures how much the solutions from the two methods diverge. The solution $c_{\\mathrm{QR}}$ is considered the more accurate benchmark.\n$4$. The absolute difference in the norms of the final residual vectors, $\\big|\\lVert y - V c_{\\mathrm{NE}} \\rVert_2 - \\lVert y - V c_{\\mathrm{QR}} \\rVert_2\\big|$. A stable algorithm should yield a solution that comes closer to minimizing the true residual.\n\nThe test cases are designed to show how these metrics change as the abscissae $x_i$ become closer, which increases the ill-conditioning of $V$.\n\n**Procedure for Each Test Case:**\n$1$. Given the abscissae $x$ and polynomial degree $m=3$, construct the vector of $y$ values using $y_i = (1 - 2x_i + 3x_i^2 - x_i^3) + 10^{-8}(i+1)$.\n$2$. Form the $5 \\times 4$ Vandermonde matrix $V$.\n$3$. Solve $(V^\\top V)c_{\\mathrm{NE}} = V^\\top y$ to get $c_{\\mathrm{NE}}$.\n$4$. Compute the QR factorization $V=QR$ and solve $Rc_{\\mathrm{QR}} = Q^\\top y$ to get $c_{\\mathrm{QR}}$.\n$5$. Calculate and record the four specified metrics.\nThis procedure will be implemented for all three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the discrete least squares problem using Normal Equations and QR factorization\n    for three test cases, and computes metrics to evaluate numerical stability.\n    \"\"\"\n\n    # Generative polynomial: p(x) = 1 - 2x + 3x^2 - x^3\n    # The coefficients are for powers x^0, x^1, x^2, x^3\n    poly_coeffs_gen = np.array([1.0, -2.0, 3.0, -1.0])\n    \n    def p(x_vals, coeffs):\n        \"\"\"Evaluates a polynomial with given coefficients at x_vals.\"\"\"\n        # Using Horner's method implicitly via np.polyval\n        # The coefficients need to be in descending order of power\n        return np.polyval(coeffs[::-1], x_vals)\n\n    test_cases = [\n        # Case 1: Well-spaced points\n        {'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0]), 'm': 3},\n        # Case 2: Two points extremely close\n        {'x': np.array([0.0, 1.0, 1.0 + 1e-12, 2.0, 3.0]), 'm': 3},\n        # Case 3: Closer still, near double precision limits\n        {'x': np.array([-1.0, 0.0, 1.0, 1.0 + 1e-15, 2.0]), 'm': 3},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        x = case['x']\n        m = case['m']\n        n = len(x)\n\n        # 1. Generate data\n        delta = 1e-8 * (np.arange(n) + 1)\n        y = p(x, poly_coeffs_gen) + delta\n\n        # 2. Construct Vandermonde matrix V\n        # V_ij = x_i^j, for j=0,...,m\n        V = np.vander(x, m + 1, increasing=True)\n\n        # 3. Solve with Normal Equations\n        # (V^T V) c = V^T y\n        try:\n            VTV = V.T @ V\n            VTy = V.T @ y\n            c_NE = np.linalg.solve(VTV, VTy)\n        except np.linalg.LinAlgError:\n            # Handle cases where VTV is singular to machine precision\n            c_NE = np.full(m + 1, np.nan)\n\n\n        # 4. Solve with QR factorization\n        # V = QR, solve Rc = Q^T y\n        Q, R = np.linalg.qr(V, mode='reduced')\n        QTY = Q.T @ y\n        c_QR = np.linalg.solve(R, QTY)\n\n        # 5. Calculate metrics\n        # Metric 1: Condition number of V\n        kappa_V = np.linalg.cond(V, 2)\n\n        # Metric 2: Condition number of V^T V\n        kappa_VTV = np.linalg.cond(VTV, 2)\n\n        # Metric 3: Relative difference in coefficients\n        # Use QR solution as the more accurate baseline\n        # Handle NaN case for c_NE\n        if np.any(np.isnan(c_NE)):\n            rel_coeff_diff = np.inf\n        else:\n            norm_c_QR = np.linalg.norm(c_QR, 2)\n            if norm_c_QR == 0:\n                rel_coeff_diff = np.linalg.norm(c_NE - c_QR, 2)\n            else:\n                rel_coeff_diff = np.linalg.norm(c_NE - c_QR, 2) / norm_c_QR\n\n        # Metric 4: Absolute difference in residual norms\n        if np.any(np.isnan(c_NE)):\n            res_norm_diff = np.inf\n        else:\n            resid_NE = y - V @ c_NE\n            resid_QR = y - V @ c_QR\n            norm_resid_NE = np.linalg.norm(resid_NE, 2)\n            norm_resid_QR = np.linalg.norm(resid_QR, 2)\n            res_norm_diff = abs(norm_resid_NE - norm_resid_QR)\n        \n        case_results = [kappa_V, kappa_VTV, rel_coeff_diff, res_norm_diff]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # str(results).replace(' ', '') ensures the output matches the required format\n    # e.g., \"[[a,b,c],[d,e,f]]\" with no spaces.\n    print(str(results).replace(' ', ''))\n\n```"
        }
    ]
}