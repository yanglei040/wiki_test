{
    "hands_on_practices": [
        {
            "introduction": "The best way to begin understanding a new algorithm is to perform a calculation by hand. This exercise guides you through the very first iteration of the Conjugate Gradient (CG) method for a simple $2 \\times 2$ system. By manually computing the initial residual, search direction, step size $\\alpha_0$, and the first updated solution $x_1$, you will build a concrete understanding of the mechanics that drive this powerful optimization technique . Mastering these fundamental steps is essential before moving on to automated implementations.",
            "id": "1393666",
            "problem": "Consider the linear system of equations $Ax=b$, where the matrix $A$ and the vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\nThe matrix $A$ is symmetric and positive-definite.\n\nStarting with an initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one iteration of the Conjugate Gradient method to find the first updated solution, $x_1$.\n\nExpress your answer as a row matrix containing the components of $x_1$ as exact fractions.",
            "solution": "We apply the Conjugate Gradient method for a symmetric positive-definite matrix $A$ starting from $x_{0}$. The standard first-iteration formulas are:\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\nGiven $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$, compute\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\nNext, compute $A p_{0}$:\n$$\nA p_{0} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\nCompute the scalar products:\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\nThus,\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\nUpdate the solution:\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\nExpressed as a row matrix, the components of $x_{1}$ are $\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The theoretical guarantees of the Conjugate Gradient method—such as monotonic convergence in the $A$-norm and termination in at most $n$ steps—hinge on the matrix $A$ being symmetric and positive-definite (SPD). But what happens if we apply the algorithm to a matrix that does not meet these criteria? This coding exercise  invites you to explore this question by applying the standard CG algorithm to non-symmetric matrices. By observing the potentially erratic or non-convergent behavior of the residual, you will gain critical insight into the algorithm's limitations and the importance of its underlying assumptions.",
            "id": "2382427",
            "problem": "You are given square matrices, right-hand side vectors, an initial vector, a tolerance, and a maximum number of allowed iterations. For each case, generate the sequence produced by the standard unpreconditioned conjugate gradient iteration for solving the linear system $A x = b$ starting from $x_0$, and assess its behavior by three quantitative criteria. Use the Euclidean vector norm throughout.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$, and $x_0 \\in \\mathbb{R}^{n}$. Define the initial residual $r_0 = b - A x_0$ and the initial search direction $p_0 = r_0$. For iteration index $k = 0, 1, 2, \\dots$, define\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\nIf $p_k^\\top A p_k = 0$ at any iteration, declare a breakdown and stop. At each step, track the relative residual norm\n$$\n\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}.\n$$\nStop the iteration when either $\\rho_k \\le \\tau$ or the iteration count reaches $k_{\\max}$ or a breakdown is detected. Define that the sequence $\\{\\rho_k\\}$ exhibits monotone nonincreasing behavior if $\\rho_{k+1} \\le \\rho_k + \\varepsilon$ for all consecutive pairs, where $\\varepsilon = 10^{-12}$.\n\nFor each test case below, produce a result list containing four entries in the following order:\n1. A boolean indicating whether the stopping condition $\\rho_k \\le \\tau$ was met before reaching $k_{\\max}$ iterations and without breakdown.\n2. An integer equal to the number of iterations actually performed (the count of updates from $x_k$ to $x_{k+1}$).\n3. A float equal to the final relative residual norm $\\rho_{\\text{final}}$ rounded to six decimal places.\n4. A boolean indicating whether the sequence $\\{\\rho_k\\}$ was monotone nonincreasing according to the definition above.\n\nTest suite (use exactly these data, in the order listed):\n\n- Case $1$ (symmetric positive definite reference):\n  - Dimension $n = 10$.\n  - Matrix $A_1$ with entries $(A_1)_{ii} = 2$ for $i = 1, \\dots, n$, $(A_1)_{i,i+1} = (A_1)_{i+1,i} = -1$ for $i = 1, \\dots, n-1$, and zero elsewhere.\n  - Right-hand side $b_1$ with components $(b_1)_i = 1$ for $i = 1, \\dots, n$.\n  - Initial vector $x_{0,1}$ with $(x_{0,1})_i = 0$ for $i = 1, \\dots, n$.\n  - Tolerance $\\tau_1 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,1} = 10$.\n\n- Case $2$ (mildly non-symmetric):\n  - Dimension $n = 10$.\n  - Let $S$ be the strictly skew-symmetric matrix with $S_{i,i+1} = 1$ and $S_{i+1,i} = -1$ for $i = 1, \\dots, n-1$, and zero elsewhere.\n  - Matrix $A_2 = A_1 + \\gamma S$ with $\\gamma = 0.1$.\n  - Right-hand side $b_2 = b_1$.\n  - Initial vector $x_{0,2} = x_{0,1}$.\n  - Tolerance $\\tau_2 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,2} = 10$.\n\n- Case $3$ (strongly non-symmetric, upper triangular non-normal):\n  - Dimension $n = 10$.\n  - Matrix $A_3$ with $(A_3)_{ii} = 2$ for $i = 1, \\dots, n$, $(A_3)_{ij} = 1$ for $i < j$, and $(A_3)_{ij} = 0$ for $i > j$.\n  - Right-hand side $b_3 = b_1$.\n  - Initial vector $x_{0,3} = x_{0,1}$.\n  - Tolerance $\\tau_3 = 10^{-10}$.\n  - Maximum iterations $k_{\\max,3} = 10$.\n\n- Case $4$ (boundary, minimal dimension):\n  - Dimension $n = 1$.\n  - Matrix $A_4 = [2]$.\n  - Right-hand side $b_4 = [1]$.\n  - Initial vector $x_{0,4} = [0]$.\n  - Tolerance $\\tau_4 = 10^{-14}$.\n  - Maximum iterations $k_{\\max,4} = 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sublist in the same order as above. The format must be:\n\"[[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool]]\"\nFor the float, round to six decimal places. No physical units are involved, angles are not used, and no percentages appear in the output.",
            "solution": "The problem as stated is a valid and well-posed exercise in the field of computational physics and numerical linear algebra. It is scientifically grounded, free of contradictions, and provides all necessary information to proceed with a solution. The task is to implement the standard Conjugate Gradient (CG) algorithm and evaluate its performance on a series of well-defined test cases.\n\nThe Conjugate Gradient method is an iterative algorithm designed for solving large, sparse linear systems of equations of the form $A x = b$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive-definite (SPD). The method's effectiveness is based on the iterative construction of a set of $A$-orthogonal (or conjugate) search directions $\\{p_k\\}_{k=0}^{n-1}$, which satisfy $p_i^\\top A p_j = 0$ for $i \\neq j$. This $A$-orthogonality guarantees that the error is minimized in the $A$-norm at each step over the expanding Krylov subspace, $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$. A key consequence is that the sequence of residuals $\\{r_k\\}$ are mutually orthogonal, i.e., $r_i^\\top r_j = 0$ for $i \\neq j$. In exact arithmetic, this process guarantees convergence to the exact solution in at most $n$ iterations.\n\nWhen the matrix $A$ is not symmetric, as in Cases $2$ and $3$ of the problem, the theoretical foundations of the CG method are no longer valid. The properties of $A$-orthogonality of search directions and orthogonality of residuals are lost. As a result, convergence is not guaranteed. The residual norm, $\\lVert r_k \\rVert_2$, may exhibit erratic, non-monotonic behavior, and the algorithm may fail to converge or even diverge. The provided test cases are structured to demonstrate this principle:\n- Case $1$: The matrix $A_1$ is a symmetric positive-definite matrix (the discrete one-dimensional Laplacian), representing the ideal scenario for the CG method.\n- Case $2$: The matrix $A_2$ is a mildly non-symmetric perturbation of $A_1$. The CG method may still converge, but its ideal performance characteristics, such as monotonic residual reduction, may be lost.\n- Case $3$: The matrix $A_3$ is a strongly non-symmetric, non-normal, upper triangular matrix. Applying the formal CG algorithm here is expected to yield poor results, demonstrating the method's limitations.\n- Case $4$: A trivial $n=1$ case which must converge in a single step to the exact solution.\n\nThe implementation will strictly follow the algorithm provided in the problem statement. Starting from an initial guess $x_0$, we compute the initial residual $r_0 = b - A x_0$ and the initial search direction $p_0 = r_0$. The iterative process for $k = 0, 1, 2, \\dots$ involves the following calculations:\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\nThe iteration terminates based on three conditions:\n1.  **Convergence**: The relative residual norm $\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}$ drops below a specified tolerance $\\tau$.\n2.  **Maximum Iterations**: The number of iterations reaches the maximum allowed, $k_{\\max}$.\n3.  **Breakdown**: The denominator in the expression for $\\alpha_k$, $p_k^\\top A p_k$, becomes zero. For an SPD matrix this can only occur if $p_k=0$, which implies the solution has been found. For a general matrix, this can happen for $p_k \\neq 0$, constituting a fatal breakdown of the algorithm.\n\nFrom the execution trace of the algorithm on each test case, the four specified quantitative metrics will be determined:\n1.  A boolean indicating if convergence (via the $\\tau$ condition) was achieved without breakdown and within the $k_{\\max}$ limit.\n2.  The total number of iterations performed.\n3.  The final relative residual norm $\\rho_{\\text{final}}$, rounded to six decimal places.\n4.  A boolean indicating whether the sequence of relative residual norms $\\{\\rho_k\\}$ was monotonically non-increasing, defined as $\\rho_{k+1} \\le \\rho_k + \\varepsilon$ for all $k$, where the tolerance $\\varepsilon = 10^{-12}$ accounts for minor floating-point fluctuations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the CG solver, and print results.\n    \"\"\"\n\n    def cg_solver(A, b, x0, tau, k_max):\n        \"\"\"\n        Implements the Conjugate Gradient algorithm as specified in the problem.\n\n        Args:\n            A (np.ndarray): The square matrix.\n            b (np.ndarray): The right-hand side vector.\n            x0 (np.ndarray): The initial guess vector.\n            tau (float): The tolerance for the relative residual norm.\n            k_max (int): The maximum number of iterations.\n\n        Returns:\n            list: A list containing the four required result metrics.\n        \"\"\"\n        epsilon_monotonicity = 1e-12\n        # A small number to check against for floating point zero\n        machine_zero = 1e-40\n\n        x = np.copy(x0).astype(float)\n        r = b - A @ x\n        p = np.copy(r)\n\n        norm_r0 = np.linalg.norm(r)\n\n        # If the initial guess is already the solution\n        if norm_r0  machine_zero:\n            return [True, 0, 0.0, True]\n\n        rho_history = [1.0]\n        \n        num_iterations = 0\n        converged_by_tau = False\n        breakdown = False\n\n        r_T_r = r.T @ r\n\n        for k in range(k_max):\n            # Calculate alpha_k\n            Ap = A @ p\n            p_T_Ap = p.T @ Ap\n            \n            # Breakdown condition\n            if abs(p_T_Ap)  machine_zero:\n                breakdown = True\n                break\n\n            alpha_k = r_T_r / p_T_Ap\n\n            # Update solution and residual\n            x = x + alpha_k * p\n            r_next = r - alpha_k * Ap\n            \n            num_iterations += 1\n            \n            # Check stopping condition based on relative residual norm\n            rho_k_plus_1 = np.linalg.norm(r_next) / norm_r0\n            rho_history.append(rho_k_plus_1)\n            \n            if rho_k_plus_1 = tau:\n                converged_by_tau = True\n                r = r_next # Finalize r for correct rho reporting\n                break\n\n            # Update search direction\n            r_next_T_r_next = r_next.T @ r_next\n            \n            # Robustness check to avoid division by zero if r becomes zero\n            if r_T_r  machine_zero:\n                converged_by_tau = True # Implicitly converged\n                r = r_next\n                break\n\n            beta_k_plus_1 = r_next_T_r_next / r_T_r\n            p = r_next + beta_k_plus_1 * p\n            \n            # Update residual for the next iteration\n            r = r_next\n            r_T_r = r_next_T_r_next\n\n        # Post-processing after the loop\n        final_rho = rho_history[-1]\n        \n        is_converged_output = converged_by_tau and not breakdown\n        \n        is_monotone = True\n        for i in range(len(rho_history) - 1):\n            if rho_history[i+1] > rho_history[i] + epsilon_monotonicity:\n                is_monotone = False\n                break\n                \n        return [is_converged_output, num_iterations, round(final_rho, 6), is_monotone]\n\n    # --- Define Test Cases ---\n    \n    # Common parameters for cases 1-3\n    n = 10\n    b_common = np.ones(n, dtype=float)\n    x0_common = np.zeros(n, dtype=float)\n    \n    # Case 1: Symmetric positive definite\n    A1 = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    case1 = (A1, b_common, x0_common, 1e-10, 10)\n\n    # Case 2: Mildly non-symmetric\n    gamma = 0.1\n    S = np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n    A2 = A1 + gamma * S\n    case2 = (A2, b_common, x0_common, 1e-10, 10)\n\n    # Case 3: Strongly non-symmetric, upper triangular\n    A3 = np.diag(np.full(n, 2.0)) + np.triu(np.ones((n, n)), k=1)\n    case3 = (A3, b_common, x0_common, 1e-10, 10)\n    \n    # Case 4: Minimal dimension (scalar)\n    A4 = np.array([[2.0]])\n    b4 = np.array([1.0])\n    x04 = np.array([0.0])\n    case4 = (A4, b4, x04, 1e-14, 1)\n    \n    test_cases = [case1, case2, case3, case4]\n\n    results = []\n    for A, b, x0, tau, kmax in test_cases:\n        result = cg_solver(A, b, x0, tau, kmax)\n        results.append(result)\n\n    # Format output as specified: \"[[...],[...],...]\"\n    # The str() on a list gives a string representation \"[...]\"\n    # Joining these with a comma and enclosing in brackets produces the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "One of the most remarkable features of the Conjugate Gradient method is that its convergence rate is not governed by the size of the matrix, but by the distribution of its eigenvalues. Specifically, in exact arithmetic, the algorithm is guaranteed to converge in at most $k$ iterations, where $k$ is the number of distinct eigenvalues. This practice  combines theoretical reasoning with numerical verification, guiding you to first understand why this property holds and then to observe it in action. By constructing matrices with a prescribed number of distinct eigenvalues, you will see firsthand how the spectral properties of a matrix directly influence the algorithm's practical performance.",
            "id": "3216667",
            "problem": "You are to reason from first principles about the Conjugate Gradient method and then implement a complete, runnable program that constructs explicit symmetric positive definite matrices with a prescribed number of distinct eigenvalues and measures the number of iterations needed by Conjugate Gradient to reach a specified tolerance. The mathematical goal is to show that, in exact arithmetic, Conjugate Gradient converges in at most the number of distinct eigenvalues. The computational goal is to verify this behavior numerically on carefully chosen diagonal test matrices.\n\nBegin with the following foundational base, without invoking any unproven shortcut results:\n- The definition of a symmetric positive definite (SPD) matrix: a real matrix $A \\in \\mathbb{R}^{n \\times n}$ is SPD if $x^{\\top} A x \\gt 0$ for all nonzero $x \\in \\mathbb{R}^{n}$ and $A = A^{\\top}$.\n- The definition of the Krylov subspace $\\mathcal{K}_{k}(A,r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$ built from an initial residual $r_{0} = b - A x_{0}$.\n- The Galerkin condition for Conjugate Gradient: at iteration $k$, the error $e_{k} = x_{\\star} - x_{k}$ is orthogonal (in the $A$-inner product) to $\\mathcal{K}_{k}(A,r_{0})$, and $x_{k}$ minimizes the $A$-norm of the error over the affine space $x_{0} + \\mathcal{K}_{k}(A,r_{0})$.\n\nUsing only these principles, explain why for an SPD matrix $A$ with exactly $k$ distinct eigenvalues, Conjugate Gradient must produce the exact solution in at most $k$ iterations in exact arithmetic when the initial residual has nonzero projection onto each eigenspace. Then, justify what happens if some eigenspace has zero projection in the initial residual.\n\nNext, implement a program that:\n- Constructs diagonal SPD matrices $A$ with a prescribed multiset of positive eigenvalues. For a diagonal matrix with diagonal entries $\\{\\lambda_{i}\\}_{i=1}^{n}$ where each $\\lambda_{i} \\gt 0$, the matrix is SPD.\n- Uses Conjugate Gradient to solve $A x = b$ from an initial guess $x_{0}$, counting the number of iterations required to satisfy $\\|r_{k}\\|_{2} \\le \\text{tol} \\cdot \\|r_{0}\\|_{2}$, where $r_{k} = b - A x_{k}$ and $\\|\\cdot\\|_{2}$ denotes the Euclidean norm.\n- Reports, for each test case, the integer number of iterations taken.\n\nTest suite and coverage:\n- Use the following four test cases. In each case, $A$ is diagonal with diagonal entries listed by multiplicity, $x_{0}$ is the zero vector, and $\\text{tol} = 10^{-12}$.\n  1. Happy path with exactly three distinct eigenvalues: $A = \\operatorname{diag}([\\,2,2,2,5,5,5,11,11\\,])$ so there are $k = 3$ distinct eigenvalues $\\{2,5,11\\}$ in dimension $n = 8$. Use $b = \\mathbf{1} \\in \\mathbb{R}^{8}$ (all entries equal to $1$).\n  2. Boundary case with a single distinct eigenvalue: $A = \\operatorname{diag}([\\,7,7,7,7,7,7\\,])$ so there is $k = 1$ in dimension $n = 6$. Use $b = [\\,1,2,3,4,5,6\\,]^{\\top}$.\n  3. Two distinct eigenvalues with balanced multiplicities: $A = \\operatorname{diag}([\\,1,1,1,1,1,9,9,9,9,9\\,])$ so there is $k = 2$ in dimension $n = 10$. Use $b = [\\,1,2,3,4,5,6,7,8,9,10\\,]^{\\top}$.\n  4. Nuanced edge case with three distinct eigenvalues but zero projection on one eigenspace: $A = \\operatorname{diag}([\\,2,2,2,4,4,4,8,8,8\\,])$ so there is matrix-level $k = 3$ in dimension $n = 9$, but set $b = [\\,1,1,1,0,0,0,2,2,2\\,]^{\\top}$ so that the component corresponding to the eigenvalue $4$ is exactly zero, making the effective number of eigenvalues present in $r_{0}$ equal to $2$.\n\nImplementation requirements:\n- The Conjugate Gradient routine must be implemented explicitly and must operate on dense NumPy arrays.\n- Use $x_{0} = 0$ in all tests and a maximum iteration limit of $n$ for each problem of size $n$.\n- The stopping criterion is $\\|r_{k}\\|_{2} \\le \\text{tol} \\cdot \\|r_{0}\\|_{2}$ with $\\text{tol} = 10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. Concretely, it must be of the form $[\\text{it}_{1},\\text{it}_{2},\\text{it}_{3},\\text{it}_{4}]$, where $\\text{it}_{j}$ is the integer number of iterations used by Conjugate Gradient for test case $j$.\n\nNo external input is allowed. The code must be self-contained and runnable as is.",
            "solution": "The problem is valid. It is a well-posed and scientifically grounded problem in numerical linear algebra that asks for a theoretical derivation and a computational verification of a fundamental property of the Conjugate Gradient method. All necessary data and conditions are provided, and the problem is free from ambiguity, contradiction, or scientific unsoundness.\n\nWe are tasked with deriving, from first principles, the convergence behavior of the Conjugate Gradient (CG) method for solving linear systems $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix with $k$ distinct eigenvalues. We will then verify this behavior numerically.\n\nThe foundational principles provided are:\n1.  An SPD matrix $A$ satisfies $x^{\\top} A x  0$ for all nonzero $x \\in \\mathbb{R}^{n}$ and $A = A^{\\top}$.\n2.  The Krylov subspace is defined as $\\mathcal{K}_{j}(A,r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{j-1} r_{0}\\}$, where $r_{0} = b - A x_{0}$ is the initial residual.\n3.  The CG iterate $x_{j}$ is chosen from the affine subspace $x_{0} + \\mathcal{K}_{j}(A,r_{0})$ such that it minimizes the $A$-norm of the error, $\\|e_{j}\\|_{A} = \\sqrt{e_{j}^{\\top} A e_{j}}$, where $e_{j} = x_{\\star} - x_{j}$ and $x_{\\star} = A^{-1}b$ is the exact solution. This is equivalent to the Galerkin condition that the residual $r_{j} = b - Ax_{j}$ is orthogonal to the subspace $\\mathcal{K}_{j}(A,r_{0})$.\n\nLet us begin the derivation. The $j$-th iterate $x_{j}$ is in $x_{0} + \\mathcal{K}_{j}(A,r_{0})$, which implies that the vector $x_{j} - x_{0}$ is a linear combination of the basis vectors of the Krylov subspace. Thus, we can write $x_{j} - x_{0}$ as a polynomial in $A$ of degree at most $j-1$ acting on $r_0$:\n$$\nx_{j} - x_{0} = P_{j-1}(A) r_{0}\n$$\nfor some polynomial $P_{j-1}$ of degree at most $j-1$.\n\nThe error at iteration $j$ is $e_j = x_\\star - x_j$. We can express this in terms of the initial error $e_0 = x_\\star - x_0$.\n$$\ne_{j} = x_{\\star} - (x_{0} + P_{j-1}(A)r_{0}) = (x_{\\star} - x_{0}) - P_{j-1}(A)r_{0} = e_{0} - P_{j-1}(A)r_{0}\n$$\nThe initial residual is related to the initial error by $r_0 = b - Ax_0 = A x_\\star - A x_0 = A(x_\\star - x_0) = A e_0$. Substituting this into the expression for $e_j$:\n$$\ne_{j} = e_{0} - P_{j-1}(A) A e_{0} = (I - A P_{j-1}(A)) e_{0}\n$$\nLet us define a new polynomial $Q_{j}(t) = 1 - t P_{j-1}(t)$. This polynomial $Q_{j}$ has a degree of at most $j$ and, by its construction, satisfies $Q_{j}(0) = 1$. With this definition, the error can be written compactly as:\n$$\ne_{j} = Q_{j}(A) e_{0}\n$$\nThe CG method's core property is that it selects the iterate $x_j$ (and thus the polynomial $P_{j-1}$, or equivalently $Q_j$) to minimize the $A$-norm of the error $\\|e_j\\|_A$. Therefore, the polynomial $Q_j$ generated by the CG algorithm at step $j$ is the solution to the minimization problem:\n$$\n\\|e_{j}\\|_{A} = \\min_{\\substack{Q \\in \\mathcal{P}_j \\\\ Q(0)=1}} \\|Q(A) e_{0}\\|_{A}\n$$\nwhere $\\mathcal{P}_j$ is the set of all polynomials of degree at most $j$.\n\nNow, let us analyze this minimization problem using the spectral properties of $A$. Since $A$ is symmetric, it has a complete set of orthonormal eigenvectors $v_1, \\dots, v_n$ with corresponding real eigenvalues $\\lambda_1, \\dots, \\lambda_n$. Since $A$ is positive definite, all $\\lambda_i  0$. We can expand the initial error $e_0$ in this eigenvector basis: $e_{0} = \\sum_{i=1}^{n} c_i v_i$.\n\nThe $A$-norm squared of the error $e_j = Q_j(A)e_0$ is:\n$$\n\\|e_{j}\\|_{A}^2 = e_{j}^{\\top} A e_{j} = (Q_{j}(A)e_{0})^{\\top} A (Q_{j}(A)e_{0}) = e_{0}^{\\top} Q_{j}(A)^{\\top} A Q_{j}(A) e_{0}\n$$\nSince $A$ is symmetric, any polynomial in $A$ is also symmetric, so $Q_{j}(A)^{\\top} = Q_{j}(A)$. This gives:\n$$\n\\|e_{j}\\|_{A}^2 = e_{0}^{\\top} A Q_{j}(A)^2 e_{0} = \\left(\\sum_{i=1}^{n} c_i v_i\\right)^{\\top} A Q_{j}(A)^2 \\left(\\sum_{l=1}^{n} c_l v_l\\right)\n$$\nUsing $A v_i = \\lambda_i v_i$ and $Q_j(A)v_i = Q_j(\\lambda_i)v_i$, and the orthonormality of eigenvectors ($v_i^\\top v_l = \\delta_{il}$):\n$$\n\\|e_{j}\\|_{A}^2 = \\left(\\sum_{i=1}^{n} c_i v_i\\right)^{\\top} \\left(\\sum_{l=1}^{n} c_l \\lambda_l Q_{j}(\\lambda_l)^2 v_l\\right) = \\sum_{i=1}^{n} c_i^2 \\lambda_i Q_{j}(\\lambda_i)^2\n$$\nNow, suppose the matrix $A$ has exactly $k$ distinct eigenvalues, which we denote as $\\mu_1, \\mu_2, \\dots, \\mu_k$. We want to show that the CG algorithm converges in at most $k$ iterations. This requires showing that $e_k = 0$. By the minimization property of CG, if we can find *any* polynomial $\\tilde{Q} \\in \\mathcal{P}_k$ with $\\tilde{Q}(0)=1$ that makes the error zero, the polynomial $Q_k$ found by CG will result in an error norm that is less than or equal to zero, which means the error itself must be zero.\n\nLet us construct such a polynomial. Consider the polynomial $m_A(t) = \\prod_{i=1}^{k} (t - \\mu_i)$, which is the minimal polynomial of $A$. It has degree $k$. We require a polynomial that is zero at each $\\mu_i$ but equals $1$ at $t=0$. We can define this polynomial as:\n$$\n\\tilde{Q}_{k}(t) = \\frac{\\prod_{i=1}^{k} (t - \\mu_i)}{\\prod_{i=1}^{k} (0 - \\mu_i)} = \\prod_{i=1}^{k} \\frac{t - \\mu_i}{-\\mu_i} = \\prod_{i=1}^{k} \\left(1 - \\frac{t}{\\mu_i}\\right)\n$$\nThis polynomial $\\tilde{Q}_{k}(t)$ has degree $k$ and satisfies $\\tilde{Q}_{k}(0) = 1$. Crucially, for any eigenvalue $\\lambda_j$ of $A$, $\\lambda_j$ must be one of the distinct eigenvalues $\\mu_1, \\dots, \\mu_k$. Therefore, $\\tilde{Q}_{k}(\\lambda_j) = 0$ for all $j=1, \\dots, n$.\n\nLet us evaluate the error norm corresponding to this polynomial at step $j=k$:\n$$\n\\|\\tilde{Q}_{k}(A) e_{0}\\|_{A}^2 = \\sum_{i=1}^{n} c_i^2 \\lambda_i \\tilde{Q}_{k}(\\lambda_i)^2 = \\sum_{i=1}^{n} c_i^2 \\lambda_i (0)^2 = 0\n$$\nThis implies that $\\tilde{Q}_{k}(A)e_0 = 0$. Since the CG algorithm at step $k$ finds the polynomial $Q_k$ that minimizes this error norm, we must have:\n$$\n\\|e_k\\|_A^2 = \\|Q_k(A)e_0\\|_A^2 \\le \\|\\tilde{Q}_k(A)e_0\\|_A^2 = 0\n$$\nSince the $A$-norm is a true norm for an SPD matrix, $\\|e_k\\|_A = 0$ if and only if $e_k = 0$. Therefore, at step $k$, the error is zero, which means $x_k = x_\\star$, the exact solution. This completes the proof that in exact arithmetic, CG converges in at most $k$ iterations, where $k$ is the number of distinct eigenvalues of $A$. This argument implicitly assumes that the initial error $e_0$ (and hence $r_0$) has a non-zero projection onto the eigenspaces of all $k$ distinct eigenvalues (i.e., all relevant $c_i \\neq 0$).\n\nNow, we justify what happens if the initial residual $r_0$ (and thus $e_0=A^{-1}r_0$) has zero projection onto the eigenspaces corresponding to some distinct eigenvalues. Let the set of $k$ distinct eigenvalues of $A$ be $\\Sigma = \\{\\mu_1, \\dots, \\mu_k\\}$. Suppose $e_0$ is constructed such that it is orthogonal to the eigenspaces associated with a subset of these eigenvalues. This means the coefficients $c_i$ in the eigenvector expansion $e_0 = \\sum c_i v_i$ are zero for any eigenvector $v_i$ whose eigenvalue $\\lambda_i$ belongs to this subset. Let the set of distinct eigenvalues for which the corresponding coefficients $c_i$ are not all zero be $\\Sigma' = \\{\\mu'_{1}, \\dots, \\mu'_{k'}\\} \\subset \\Sigma$, where $k'  k$. The sum for the error norm now only runs over indices $i$ corresponding to eigenvalues in $\\Sigma'$:\n$$\n\\|e_j\\|_A^2 = \\sum_{\\lambda_i \\in \\Sigma'} c_i^2 \\lambda_i Q_j(\\lambda_i)^2\n$$\nTo make this sum zero, we no longer need a polynomial that is zero on all of $\\Sigma$, but only one that is zero on $\\Sigma'$. We can construct such a polynomial of degree $k'$:\n$$\n\\tilde{Q}_{k'}(t) = \\prod_{i=1}^{k'} \\left(1 - \\frac{t}{\\mu'_i}\\right)\n$$\nThis polynomial has degree $k'$, satisfies $\\tilde{Q}_{k'}(0)=1$, and is zero for all eigenvalues $\\mu'_i \\in \\Sigma'$. The same argument as before shows that at step $j=k'$, CG will find a solution $x_{k'}$ such that $e_{k'}=0$. Therefore, CG converges in at most $k'$ iterations, where $k'$ is the number of distinct eigenvalues of $A$ \"seen\" by the initial residual vector $r_0$. Test case $4$ is designed to demonstrate this specific phenomenon. The initial residual $r_0=b$ has components only in the eigenspaces for eigenvalues $2$ and $8$, so convergence is expected in $2$ steps, not $3$.\n\nWe will now implement the CG algorithm and test these theoretical predictions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0, tol, max_iter):\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Conjugate Gradient\n    method.\n\n    Args:\n        A (np.ndarray): A symmetric positive-definite square matrix.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The relative tolerance for the residual norm.\n        max_iter (int): The maximum number of iterations allowed.\n\n    Returns:\n        int: The number of iterations performed.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    \n    norm_r0 = np.linalg.norm(r)\n    \n    # If the initial guess is already the solution, 0 iterations.\n    if norm_r0 == 0:\n        return 0\n        \n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    for i in range(max_iter):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        # Check the stopping criterion\n        if np.sqrt(rs_new) = tol * norm_r0:\n            return i + 1\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    # Return max_iter if convergence was not reached within the limit.\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the Conjugate Gradient method\n    to verify its convergence properties related to distinct eigenvalues.\n    \"\"\"\n    tol = 1e-12\n\n    # Test Case 1: 3 distinct eigenvalues\n    # n=8, k=3\n    diag1 = [2]*3 + [5]*3 + [11]*2\n    A1 = np.diag(diag1)\n    b1 = np.ones(A1.shape[0])\n    x0_1 = np.zeros(A1.shape[0])\n    \n    # Test Case 2: 1 distinct eigenvalue\n    # n=6, k=1\n    diag2 = [7]*6\n    A2 = np.diag(diag2)\n    b2 = np.arange(1, A2.shape[0] + 1)\n    x0_2 = np.zeros(A2.shape[0])\n\n    # Test Case 3: 2 distinct eigenvalues\n    # n=10, k=2\n    diag3 = [1]*5 + [9]*5\n    A3 = np.diag(diag3)\n    b3 = np.arange(1, A3.shape[0] + 1)\n    x0_3 = np.zeros(A3.shape[0])\n    \n    # Test Case 4: 3 distinct eigenvalues, but r0 is in a 2-eigenvalue subspace\n    # n=9, matrix k=3, effective k=2\n    diag4 = [2]*3 + [4]*3 + [8]*3\n    A4 = np.diag(diag4)\n    b4 = np.array([1,1,1, 0,0,0, 2,2,2], dtype=float)\n    x0_4 = np.zeros(A4.shape[0])\n\n    test_cases = [\n        (A1, b1, x0_1),\n        (A2, b2, x0_2),\n        (A3, b3, x0_3),\n        (A4, b4, x0_4)\n    ]\n\n    results = []\n    for A, b, x0 in test_cases:\n        n = A.shape[0]\n        iterations = conjugate_gradient(A, b, x0, tol, max_iter=n)\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}