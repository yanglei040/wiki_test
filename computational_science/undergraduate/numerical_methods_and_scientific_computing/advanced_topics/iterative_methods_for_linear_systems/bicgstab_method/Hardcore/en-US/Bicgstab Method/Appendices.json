{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate why the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method was developed, it is instructive to see it succeed where its predecessor, the Bi-Conjugate Gradient (BiCG) method, fails. This practice provides a concrete numerical example demonstrating a classic breakdown in BiCG, where the residual and shadow residual become orthogonal, causing the algorithm to halt. By applying BiCGSTAB to the same system , you will see firsthand how its stabilizing steps navigate this issue, highlighting the fundamental reason for its enhanced robustness.",
            "id": "2376326",
            "problem": "You are asked to demonstrate, from first principles of Krylov subspace iterative methods, how breakdowns in the Bi-Conjugate Gradient (BiCG) method can occur while the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method still converges on the same linear system. Consider the linear system $A x = b$ with\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 0 & 1\\\\\n1 & 3 & 0\\\\\n0 & 0 & 4\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix},\n\\qquad\nx_0 \\;=\\; \\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}.\n$$\nFor Bi-Conjugate Gradient (BiCG), set the initial residual $r_0 = b - A x_0$ and the initial shadow residual $\\tilde{r}_0 = r_0$. Using only the definitions of the first update step in BiCG (residuals and shadow residuals defined by single-step Krylov projections with a biorthogonality pairing), explicitly compute $r_1$ and $\\tilde{r}_1$ and verify that $r_1 \\neq 0$, $\\tilde{r}_1 \\neq 0$, and $r_1^{T}\\tilde{r}_1 = 0$, which implies BiCG breakdown at the next step.\n\nNext, apply the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method to the same system with the same initial guess $x_0$, the same initial residual $r_0$, and a fixed shadow residual $\\hat{r}$ chosen as\n$$\n\\hat{r} \\;=\\; r_0 + \\begin{pmatrix} 0\\\\ 1\\\\ 0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1\\\\ 1\\\\ 0 \\end{pmatrix}.\n$$\nUsing the standard definitions of the BiCGSTAB recurrence (scalar coefficients defined by inner products of $r_{k-1}$, $\\hat{r}$, and Krylov images under $A$), carry out the first BiCGSTAB iteration to compute the smoothing parameter $\\omega_1$.\n\nYour task is to provide the value of the scalar $\\omega_1$. Express your final answer exactly as a reduced fraction. No rounding is required. The final answer must be a single number with no units.",
            "solution": "The posed problem is scientifically grounded, well-posed, and objective. It provides a specific, verifiable numerical example to demonstrate a known phenomenon in computational physics: the breakdown of the Bi-Conjugate Gradient (BiCG) method and the robustness of the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method under certain conditions. All necessary data and definitions are provided, and the task is a direct computational one. The problem is therefore valid.\n\nThe linear system under consideration is $A x = b$, with the givens:\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 0 & 1\\\\\n1 & 3 & 0\\\\\n0 & 0 & 4\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix},\n\\qquad\nx_0 \\;=\\; \\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}.\n$$\n\nFirst, we analyze the BiCG method. The initial residual is $r_0 = b - A x_0 = b - 0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$.\nThe problem specifies setting the initial shadow residual $\\tilde{r}_0 = r_0$. Thus, $\\tilde{r}_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$.\nThe BiCG algorithm sets the initial search directions as $p_0 = r_0$ and $\\tilde{p}_0 = \\tilde{r}_0$.\nThe scalar $\\alpha_k$ is computed at each step $k$ as $\\alpha_k = \\frac{\\tilde{r}_k^T r_k}{\\tilde{p}_k^T A p_k}$.\nFor the first step ($k=0$):\nThe numerator is $\\tilde{r}_0^T r_0 = r_0^T r_0 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = 1$.\nTo compute the denominator, we first find $A p_0$:\n$A p_0 = A r_0 = \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix}$.\nThe denominator is then $\\tilde{p}_0^T A p_0 = \\tilde{r}_0^T (A r_0) = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = 2$.\nThus, $\\alpha_0 = \\frac{1}{2}$.\n\nThe updated residuals $r_1$ and $\\tilde{r}_1$ are computed as follows:\n$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1\\\\ 1/2\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix}$.\n$\\tilde{r}_1 = \\tilde{r}_0 - \\alpha_0 A^T \\tilde{p}_0$. The transpose of $A$ is $A^T = \\begin{pmatrix} 2 & 1 & 0\\\\ 0 & 3 & 0\\\\ 1 & 0 & 4 \\end{pmatrix}$.\n$A^T \\tilde{p}_0 = A^T \\tilde{r}_0 = \\begin{pmatrix} 2 & 1 & 0\\\\ 0 & 3 & 0\\\\ 1 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2\\\\ 0\\\\ 1 \\end{pmatrix}$.\n$\\tilde{r}_1 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 2\\\\ 0\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1\\\\ 0\\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 0\\\\ -1/2 \\end{pmatrix}$.\n\nWe verify the conditions stated in the problem:\n1. $r_1 = \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix} \\neq 0$.\n2. $\\tilde{r}_1 = \\begin{pmatrix} 0\\\\ 0\\\\ -1/2 \\end{pmatrix} \\neq 0$.\n3. $r_1^T \\tilde{r}_1 = \\begin{pmatrix} 0 & -1/2 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 0\\\\ -1/2 \\end{pmatrix} = (0)(0) + (-1/2)(0) + (0)(-1/2) = 0$.\n\nThe next step in the BiCG algorithm would be to compute $\\beta_0 = \\frac{\\tilde{r}_1^T r_1}{\\tilde{r}_0^T r_0} = \\frac{0}{1} = 0$.\nThis leads to new search directions $p_1 = r_1 + \\beta_0 p_0 = r_1$ and $\\tilde{p}_1 = \\tilde{r}_1 + \\beta_0 \\tilde{p}_0 = \\tilde{r}_1$.\nThen, the next scalar $\\alpha_1$ is computed as $\\alpha_1 = \\frac{\\tilde{r}_1^T r_1}{\\tilde{p}_1^T A p_1} = \\frac{0}{\\tilde{r}_1^T A r_1}$.\nThe denominator term is $\\tilde{r}_1^T A r_1 = \\begin{pmatrix} 0 & 0 & -1/2 \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & -2 \\end{pmatrix} \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix} = 0$.\nSince both the numerator and denominator for $\\alpha_1$ are zero, this constitutes a serious breakdown of the BiCG algorithm.\n\nNext, we apply the BiCGSTAB method. The initial residual is $r_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$. The fixed shadow residual is given as $\\hat{r} = \\begin{pmatrix} 1\\\\ 1\\\\ 0 \\end{pmatrix}$.\nThe standard algorithm starts with initial values $\\rho_0=1$, $\\alpha_0=1$, $\\omega_0=1$, $p_0=0$, and $v_0=0$. We perform the first iteration ($k=1$).\n\n1. Compute $\\rho_1 = \\hat{r}^T r_0$:\n$\\rho_1 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = 1$.\n\n2. Compute $\\beta_1 = \\frac{\\rho_1}{\\rho_0} \\frac{\\alpha_0}{\\omega_0}$:\n$\\beta_1 = \\frac{1}{1} \\cdot \\frac{1}{1} = 1$.\n\n3. Compute $p_1 = r_0 + \\beta_1 (p_0 - \\omega_0 v_0)$:\n$p_1 = r_0 + 1 (0 - 1 \\cdot 0) = r_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$.\n\n4. Compute $v_1 = A p_1$:\n$v_1 = A r_0 = \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix}$.\n\n5. Compute $\\alpha_1 = \\frac{\\rho_1}{\\hat{r}^T v_1}$:\n$\\hat{r}^T v_1 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = (1)(2) + (1)(1) + (0)(0) = 3$.\n$\\alpha_1 = \\frac{1}{3}$.\n\n6. Compute the temporary residual $s_1 = r_0 - \\alpha_1 v_1$:\n$s_1 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 2/3\\\\ 0 - 1/3\\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} 1/3\\\\ -1/3\\\\ 0 \\end{pmatrix}$.\n\n7. Compute $t_1 = A s_1$:\n$t_1 = \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1/3\\\\ -1/3\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(1/3) + 0(-1/3) + 1(0) \\\\ 1(1/3) + 3(-1/3) + 0(0) \\\\ 0(1/3) + 0(-1/3) + 4(0) \\end{pmatrix} = \\begin{pmatrix} 2/3\\\\ 1/3 - 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2/3\\\\ -2/3\\\\ 0 \\end{pmatrix}$.\n\n8. Compute the smoothing parameter $\\omega_1 = \\frac{t_1^T s_1}{t_1^T t_1}$:\nThe numerator is $t_1^T s_1 = \\begin{pmatrix} 2/3 & -2/3 & 0 \\end{pmatrix} \\begin{pmatrix} 1/3\\\\ -1/3\\\\ 0 \\end{pmatrix} = (\\frac{2}{3})(\\frac{1}{3}) + (\\frac{-2}{3})(\\frac{-1}{3}) = \\frac{2}{9} + \\frac{2}{9} = \\frac{4}{9}$.\nThe denominator is $t_1^T t_1 = \\begin{pmatrix} 2/3 & -2/3 & 0 \\end{pmatrix} \\begin{pmatrix} 2/3\\\\ -2/3\\\\ 0 \\end{pmatrix} = (\\frac{2}{3})^2 + (\\frac{-2}{3})^2 = \\frac{4}{9} + \\frac{4}{9} = \\frac{8}{9}$.\n$\\omega_1 = \\frac{4/9}{8/9} = \\frac{4}{8} = \\frac{1}{2}$.\n\nThe value of the smoothing parameter $\\omega_1$ is $\\frac{1}{2}$. This demonstrates that the BiCGSTAB algorithm, with a suitable choice of $\\hat{r}$, does not break down and proceeds with the computation.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "While BiCGSTAB is significantly more robust than BiCG, it is not entirely immune to numerical difficulties. This exercise explores a different kind of potential pitfall: a scenario where the stabilizing parameter $\\omega_k$ becomes zero. This situation, known as a pivot breakdown, occurs when the intermediate residual vector $s_k$ is orthogonal to its own image under the matrix $A$. Working through this specific case  reveals the mechanics of the stabilizing step and helps identify the conditions under which the algorithm might temporarily stagnate.",
            "id": "2376337",
            "problem": "A linear system arises in a non-symmetric discretization context: solve $A x = b$ with the Biconjugate Gradient Stabilized (BiCGSTAB) method. Consider\n$$\nA=\\begin{pmatrix}\n1 & 1\\\\\n-1 & 0\n\\end{pmatrix}, \\quad\nb=\\begin{pmatrix}\n1\\\\\n0\n\\end{pmatrix}, \\quad\nx_{0}=\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}.\n$$\nLet the initial residual be $r_{0}=b-A x_{0}$ and choose the shadow residual $\\hat{r}=r_{0}$. Use the standard Euclidean inner product. In the first outer iteration of BiCGSTAB (with $p_{0}=r_{0}$), define\n- $v_{0}=A p_{0}$,\n- $\\alpha_{1}=\\dfrac{\\hat{r}^{T} r_{0}}{\\hat{r}^{T} v_{0}}$,\n- $s_{1}=r_{0}-\\alpha_{1} v_{0}$,\n- $t_{1}=A s_{1}$,\n- $\\omega_{1}=\\dfrac{t_{1}^{T} s_{1}}{t_{1}^{T} t_{1}}$.\nCompute the exact value of $\\omega_{1}$. Provide your answer as a single real number. No rounding is required.",
            "solution": "The problem statement is critically examined and found to be valid. It is a well-posed problem in computational linear algebra, specifically concerning the application of the Biconjugate Gradient Stabilized (BiCGSTAB) method. All required data and definitions are provided, the context is scientifically sound, and the objective is clear. We may proceed with the solution.\n\nThe task is to compute the value of $\\omega_{1}$ in the first iteration of the BiCGSTAB method for the given linear system $A x = b$. The steps are provided in the problem statement. We will follow them systematically.\n\nThe given matrix and vectors are:\n$$\nA=\\begin{pmatrix}\n1 & 1\\\\\n-1 & 0\n\\end{pmatrix}, \\quad\nb=\\begin{pmatrix}\n1\\\\\n0\n\\end{pmatrix}, \\quad\nx_{0}=\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\n$$\n\n**Step 1: Compute the initial residual $r_{0}$**\nThe initial residual $r_{0}$ is defined as $r_{0} = b - A x_{0}$. First, we compute the product $A x_{0}$:\n$$\nA x_{0} = \\begin{pmatrix} 1 & 1\\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (1)(0) \\\\ (-1)(0) + (0)(0) \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix}\n$$\nNow, we can find $r_{0}$:\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}\n$$\n\n**Step 2: Initialize shadow residual $\\hat{r}$ and search direction $p_{0}$**\nAccording to the problem, the shadow residual is chosen as $\\hat{r} = r_{0}$, and the initial search direction is $p_{0} = r_{0}$.\n$$\n\\hat{r} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}, \\quad p_{0} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}\n$$\n\n**Step 3: Compute the vector $v_{0}$**\nThe vector $v_{0}$ is defined as $v_{0} = A p_{0}$:\n$$\nv_{0} = A p_{0} = \\begin{pmatrix} 1 & 1\\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (1)(0) \\\\ (-1)(1) + (0)(0) \\end{pmatrix} = \\begin{pmatrix} 1\\\\ -1 \\end{pmatrix}\n$$\n\n**Step 4: Compute the scalar $\\alpha_{1}$**\nThe scalar $\\alpha_{1}$ is given by the formula $\\alpha_{1}=\\dfrac{\\hat{r}^{T} r_{0}}{\\hat{r}^{T} v_{0}}$. We need to compute the two inner products in the numerator and denominator. The inner product is the standard Euclidean one, equivalent to a vector transpose multiplication.\nNumerator:\n$$\n\\hat{r}^{T} r_{0} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = (1)(1) + (0)(0) = 1\n$$\nDenominator:\n$$\n\\hat{r}^{T} v_{0} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ -1 \\end{pmatrix} = (1)(1) + (0)(-1) = 1\n$$\nThus, $\\alpha_{1}$ is:\n$$\n\\alpha_{1} = \\frac{1}{1} = 1\n$$\n\n**Step 5: Compute the vector $s_{1}$**\nThe vector $s_{1}$ is defined as $s_{1} = r_{0} - \\alpha_{1} v_{0}$:\n$$\ns_{1} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} - (1) \\begin{pmatrix} 1\\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 - 1\\\\ 0 - (-1) \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix}\n$$\n\n**Step 6: Compute the vector $t_{1}$**\nThe vector $t_{1}$ is defined as $t_{1} = A s_{1}$:\n$$\nt_{1} = A s_{1} = \\begin{pmatrix} 1 & 1\\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (1)(1) \\\\ (-1)(0) + (0)(1) \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}\n$$\n\n**Step 7: Compute the scalar $\\omega_{1}$**\nFinally, we compute $\\omega_{1}$ using the formula $\\omega_{1}=\\dfrac{t_{1}^{T} s_{1}}{t_{1}^{T} t_{1}}$. We compute the required inner products.\nNumerator:\n$$\nt_{1}^{T} s_{1} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 1 \\end{pmatrix} = (1)(0) + (0)(1) = 0\n$$\nDenominator:\n$$\nt_{1}^{T} t_{1} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix} = (1)(1) + (0)(0) = 1\n$$\nWith these values, $\\omega_{1}$ becomes:\n$$\n\\omega_{1} = \\frac{0}{1} = 0\n$$\nThe value of $\\omega_{1}$ is exactly zero. This occurs because the vector $s_{1}$ is orthogonal to $t_{1} = A s_{1}$. In this case, the stabilizing step of the BiCGSTAB algorithm contributes nothing to the solution update.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Beyond its practical performance, the power of BiCGSTAB is rooted in the deep theory of Krylov subspace methods. This exercise shifts our focus from numerical breakdowns to the theoretical guarantee of finite termination in exact arithmetic. By analyzing a small $2 \\times 2$ system , you will connect the structure of the BiCGSTAB residual polynomial to the minimal polynomial of the system matrix. This exploration provides a rigorous justification for why the method is guaranteed to find the exact solution in a number of steps related to the matrix dimension.",
            "id": "3210293",
            "problem": "Consider the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method applied to a linear system with a real, nonsingular, nonsymmetric matrix of dimension two. Let\n$$\nA \\;=\\; \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix},\n$$\nand suppose the initial guess is $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with right-hand side $b \\in \\mathbb{R}^{2}$ such that $b \\neq 0$, and choose the shadow residual $\\hat{r}_{0} = r_{0} := b - A x_{0} = b$. Assume no breakdowns occur in the BiCGSTAB recurrences (that is, all division operations defined by the method are well-defined for this input).\n\nUsing only the following foundational principles:\n- the definition of the Krylov subspace $ \\mathcal{K}_{k}(A, r_{0}) = \\operatorname{span}\\{ r_{0}, A r_{0}, \\dots, A^{k-1} r_{0} \\}$,\n- the definition of the minimal polynomial $\\mu_{A}(t)$ of $A$ as the unique monic polynomial of least degree such that $\\mu_{A}(A) = 0$,\n- and the update relations of the Bi-Conjugate Gradient Stabilized method that establish each BiCGSTAB residual $r_{k}$ as $r_{k} = \\phi_{k}(A)\\,r_{0}$ for some polynomial $\\phi_{k}$,\n\ndo the following:\n1) Derive, from the Bi-Conjugate Gradient Stabilized update equations, that after $k$ outer iterations the residual has the form $r_{k} = \\phi_{k}(A)\\,r_{0}$ with $\\deg(\\phi_{k}) \\leq 2k$, and that $\\phi_{k}(t)$ factors as the product of a degree-$k$ “stabilizer” polynomial and a degree-$k$ Bi-Conjugate Gradient residual polynomial. Use this structure, together with the definition of the minimal polynomial, to show that in dimension two the method finitely terminates in at most two outer iterations (i.e., $r_{2} = 0$ in exact arithmetic) whenever breakdowns do not occur.\n2) For the specific matrix $A$ above, compute the unique monic minimal polynomial $\\mu_{A}(t)$ explicitly.\n\nYour final answer must be the explicit analytic expression of the polynomial $\\mu_{A}(t)$. No numerical rounding is required. Do not include units.",
            "solution": "The problem is divided into two parts. The first part is a theoretical derivation concerning the termination property of the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method. The second part is a specific computation of the minimal polynomial for a given matrix.\n\nPart 1: BiCGSTAB Residual Structure and Finite Termination\n\nThe BiCGSTAB method is an iterative method for solving non-symmetric linear systems $A x = b$. The method generates a sequence of iterates $x_k$ such that the corresponding residuals $r_k = b - A x_k$ are progressively reduced. A key property of Krylov subspace methods like BiCGSTAB is that the residual after $k$ iterations can be expressed as a polynomial in the matrix $A$ applied to the initial residual $r_0$.\n\nAs stated in the problem, the residual of the BiCGSTAB method after $k$ outer iterations has the form $r_k = \\phi_k(A) r_0$, where $\\phi_k(t)$ is a polynomial. The problem further specifies the structure of this polynomial. A detailed derivation from the algorithm's recurrence relations confirms that $\\phi_k(t)$ is the product of two polynomials, each of degree $k$:\n$$ \\phi_k(t) = \\sigma_k(t) \\psi_k(t) $$\nHere, $\\psi_k(t)$ is a polynomial of degree $k$ that arises from the underlying Bi-Conjugate Gradient (BiCG) structure of the algorithm. It is referred to as the BiCG residual polynomial. The second polynomial, $\\sigma_k(t) = \\prod_{j=1}^{k} (1 - \\omega_j t)$, is a degree-$k$ polynomial that arises from the \"stabilizing\" steps, which are akin to one-step General Minimal Residual (GMRES) updates. The coefficients $\\omega_j$ are chosen at each step to locally minimize the norm of the residual. The total degree of the BiCGSTAB residual polynomial $\\phi_k(t)$ is therefore $\\deg(\\phi_k) = \\deg(\\sigma_k) + \\deg(\\psi_k) = k + k = 2k$. This establishes the first requested property.\n\nNow, we use this structure to demonstrate the finite termination of BiCGSTAB for a $2 \\times 2$ matrix. The argument relies on the properties of the minimal polynomial of a matrix.\nLet $A$ be an $N \\times N$ matrix. The minimal polynomial of $A$, denoted $\\mu_A(t)$, is the unique monic polynomial of least degree $m$ such that $\\mu_A(A) = 0$. By the Cayley-Hamilton theorem, $m \\le N$.\nFor any vector $v$, the minimal polynomial of $v$ with respect to $A$, denoted $\\mu_{A,v}(t)$, is the unique monic polynomial of least degree $m'$ such that $\\mu_{A,v}(A)v=0$. This polynomial divides $\\mu_A(t)$, so its degree satisfies $m' \\le m \\le N$.\n\nThe BiCG method is guaranteed to find the exact solution in at most $m'$ steps, assuming no breakdowns occur, where $m'$ is the degree of the minimal polynomial of the initial residual $r_0$ with respect to $A$. This means that the BiCG residual polynomial $\\psi_k(t)$ for $k=m'$ must be a scalar multiple of $\\mu_{A, r_0}(t)$. Since both $\\psi_{m'}(t)$ and $\\mu_{A, r_0}(t)/\\mu_{A, r_0}(0)$ are polynomials of degree $m'$ that equal $1$ at $t=0$, they must be identical. Consequently, the BiCG residual at step $m'$ is zero:\n$$ r_{m'}^{\\text{BCG}} = \\psi_{m'}(A) r_0 = \\frac{\\mu_{A, r_0}(A)}{\\mu_{A, r_0}(0)} r_0 = 0 $$\nThe problem states that the BiCGSTAB residual polynomial $\\phi_k(t)$ factors into a stabilizer part and a BiCG residual polynomial part $\\psi_k(t)$. Therefore, the BiCGSTAB residual at step $k=m'$ is given by:\n$$ r_{m'} = \\sigma_{m'}(A) \\psi_{m'}(A) r_0 $$\nSubstituting the result that $\\psi_{m'}(A) r_0 = 0$, we get:\n$$ r_{m'} = \\sigma_{m'}(A) \\cdot 0 = 0 $$\nThis shows that BiCGSTAB must also terminate in at most $m'$ steps.\n\nFor the specific case given in the problem, the matrix $A$ has dimension $N=2$. The degree of its minimal polynomial $\\mu_A(t)$ is $m \\le 2$. The degree of the minimal polynomial of $r_0$ with respect to $A$ is $m' \\le m \\le 2$. Therefore, the BiCGSTAB method is guaranteed to terminate with $r_k=0$ for some $k \\le m' \\le 2$. This means the method finds the exact solution in at most $2$ outer iterations.\n\nPart 2: Computation of the Minimal Polynomial\n\nWe are given the matrix:\n$$ A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix} $$\nTo find the minimal polynomial $\\mu_A(t)$, we first compute the characteristic polynomial $\\chi_A(t)$.\n$$ \\chi_A(t) = \\det(A - tI) = \\det\\begin{pmatrix} 2-t & 1 \\\\ 0 & 3-t \\end{pmatrix} $$\n$$ \\chi_A(t) = (2-t)(3-t) = t^2 - 5t + 6 $$\nThe minimal polynomial $\\mu_A(t)$ must divide the characteristic polynomial $\\chi_A(t)$. The roots of $\\chi_A(t)$ are the eigenvalues of $A$, which are $\\lambda_1 = 2$ and $\\lambda_2 = 3$. Since the eigenvalues are distinct, the matrix $A$ is diagonalizable. For a diagonalizable matrix, the minimal polynomial is the product of linear factors corresponding to each distinct eigenvalue.\nThus, the minimal polynomial must be:\n$$ \\mu_A(t) = (t-2)(t-3) = t^2 - 5t + 6 $$\nThe minimal polynomial is required to be monic, which this is. Its degree is $2$. The only other monic divisors of $\\chi_A(t)$ are $(t-2)$ and $(t-3)$, which are of degree $1$. We check if they annihilate $A$:\n$$ A - 2I = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix} - \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 1 \\end{pmatrix} \\neq 0 $$\n$$ A - 3I = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix} - \\begin{pmatrix} 3 & 0 \\\\ 0 & 3 \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\neq 0 $$\nSince no polynomial of degree $1$ annihilates $A$, the minimal polynomial must have degree $2$. As it must divide and be of the same degree as the characteristic polynomial (and both are monic), they must be equal.\n\nTherefore, the unique monic minimal polynomial of $A$ is $\\mu_A(t) = t^2 - 5t + 6$.",
            "answer": "$$\\boxed{t^2 - 5t + 6}$$"
        }
    ]
}