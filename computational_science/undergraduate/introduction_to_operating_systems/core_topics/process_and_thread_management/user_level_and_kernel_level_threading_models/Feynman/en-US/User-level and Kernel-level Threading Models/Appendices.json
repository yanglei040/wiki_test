{
    "hands_on_practices": [
        {
            "introduction": "Understanding threading models in theory is one thing, but identifying them in a real system is a crucial skill for any systems programmer. This first practice places you in the role of a diagnostics engineer. By analyzing system call traces—the footprints a program leaves on the operating system kernel—you can deduce the underlying threading architecture and its limitations, a skill essential for performance tuning and debugging. ",
            "id": "3689564",
            "problem": "A software engineer runs a diagnostic on three builds of the same multithreaded program using the system call tracer (strace) on a modern Operating System (OS). The program creates $U = 4$ user-level worker threads. Each worker thread repeats the following cycle indefinitely: perform computation in user space (no system calls) for approximately $100$ milliseconds, then perform a blocking read system call on its own pipe, and finally perform a short write system call to append a status byte to a shared log file. The pipe for each worker is fed by a slow producer so that reads frequently block. The system call tracer is invoked with options to follow threads and timestamps. Assume all reads and writes are standard blocking calls and no nonblocking Input/Output (I/O) or I/O multiplexing is used by the runtime library.\n\nYou are given three observations from strace, one per build, where a thread identifier displayed in the tracer corresponds to a kernel thread (sometimes called a Lightweight Process):\n- Trace A: Only a single kernel-thread identifier appears throughout the entire trace. When any worker issues a blocking read, there is an extended period with no other reads or writes appearing from any thread until that read returns. Writes from different workers never appear interleaved in time; they occur strictly one after another, separated by long idle intervals corresponding to blocking reads.\n- Trace B: Exactly $4$ distinct kernel-thread identifiers appear. When one worker blocks in a read, other kernel-thread identifiers continue to issue writes to the log file. Writes from different identifiers are frequently interleaved in time.\n- Trace C: Exactly $2$ distinct kernel-thread identifiers appear even though there are $4$ user-level workers. When more than $2$ workers are blocked in read, no additional progress is visible until one of the blocking reads returns. While one identifier is blocked in a read, the other identifier continues to issue writes attributable to some other worker.\n\nUsing only the following foundational facts and definitions:\n- A user-level thread is created and scheduled by a runtime library in user space; a kernel-level thread is created and scheduled by the kernel.\n- A blocking system call (for example, read on an empty pipe) causes the calling kernel thread to sleep inside the kernel until the operation can complete; the kernel does not schedule that sleeping kernel thread until the blocking condition is resolved.\n- The system call tracer shows system calls at the kernel boundary and tags each call with the identifier of the kernel thread that invoked it; the tracer does not show pure user-space computation steps.\n- In the many-to-one model, $M$ user-level threads are mapped to one kernel-level thread; in the one-to-one model, each user-level thread is mapped to a distinct kernel-level thread; in the many-to-many model, $M$ user-level threads are multiplexed over $N$ kernel-level threads, typically with $1 &lt; N &lt; M$.\n\nWhich assignment of threading models to the traces A, B, and C is most consistent with the observations?\n\nA. A: many-to-one, B: one-to-one, C: many-to-many\n\nB. A: one-to-one, B: many-to-one, C: many-to-many\n\nC. A: many-to-one, B: many-to-many, C: one-to-one\n\nD. A: many-to-many, B: one-to-one, C: many-to-one\n\nE. None of the above; strace cannot distinguish the mapping model from these observations",
            "solution": "The problem requires assigning the correct threading model (many-to-one, one-to-one, or many-to-many) to three observed behaviors of a multithreaded program, as revealed by the `strace` utility. The analysis hinges on the relationship between user-level threads, kernel-level threads, and the effect of blocking system calls.\n\nThe program creates $U = 4$ user-level worker threads. The system call tracer, `strace`, reports activity at the kernel level, identifying system calls by the specific kernel thread that executes them.\n\nLet's analyze each trace based on the provided definitions and observations.\n\n**Analysis of Trace A**\n\n- **Observations:** Only a single kernel-thread identifier is observed. When any user-level worker thread executes a blocking `read` system call, all system call activity ceases for the entire process. No other `read` or `write` calls are observed until the blocking `read` completes.\n- **Reasoning:** The observation of a single kernel-thread identifier for a process with $U = 4$ user threads indicates that all $4$ user-level threads are being mapped to a single kernel-level thread. This is the definition of the **many-to-one** model. In this model, if any user-level thread performs a blocking system call, the underlying (and only) kernel thread blocks. The operating system's scheduler sees only the single kernel thread, and since it is sleeping, it cannot be scheduled. Consequently, no other user-level thread can run, because the user-level thread scheduler itself cannot be executed. This prevents any other user threads from making progress and issuing their own system calls, which perfectly matches the observation that the entire process becomes idle.\n- **Conclusion:** Trace A is consistent with the **many-to-one** threading model.\n\n**Analysis of Trace B**\n\n- **Observations:** Exactly $4$ distinct kernel-thread identifiers are observed. When one kernel thread blocks on a `read`, other kernel threads associated with the process continue to execute and issue `write` system calls.\n- **Reasoning:** The number of observed kernel threads ($4$) is equal to the number of user-level threads ($U = 4$). This indicates that each user-level thread is mapped to its own dedicated kernel-level thread. This is the definition of the **one-to-one** model. In this model, when a user thread executes a blocking system call, only its corresponding kernel thread is blocked by the OS. The other $3$ kernel threads, which are backing the other $3$ user threads, remain runnable. The OS scheduler can continue to schedule these other kernel threads, allowing the user threads they support to make progress, including performing their `write` system calls. This allows for true concurrency and explains why `write` calls from different workers can be interleaved in time, even when one worker is blocked.\n- **Conclusion:** Trace B is consistent with the **one-to-one** threading model.\n\n**Analysis of Trace C**\n\n- **Observations:** Exactly $2$ distinct kernel-thread identifiers are observed, despite there being $U = 4$ user-level threads. When one kernel thread is blocked, the other can continue to execute system calls. However, if more than $2$ user workers are blocked in `read`, all progress ceases.\n- **Reasoning:** Here, we have $M=4$ user threads multiplexed on $N=2$ kernel threads. This fits the definition of the **many-to-many** model, which requires $1 < N < M$. In this case, $1 < 2 < 4$.\n    - The user-level thread scheduler is responsible for mapping the $4$ user threads onto the $2$ kernel threads.\n    - If one user thread makes a blocking call, it ties up one of the two kernel threads. The user-level scheduler can still map the remaining ready user threads to the one available kernel thread, allowing the application to make progress. This is consistent with the observation that while one identifier is blocked, the other can continue to work.\n    - The system's concurrency is limited by the number of kernel threads, $N=2$. If two user threads make blocking calls, both available kernel threads will be blocked. At this point, even if the other $2$ user threads are ready to run, there are no available kernel threads to execute them on. The entire process stalls. This perfectly explains the observation that \"When more than $2$ workers are blocked in read, no additional progress is visible.\"\n- **Conclusion:** Trace C is consistent with the **many-to-many** threading model.\n\n**Summary of Assignments:**\n-   Trace A: **many-to-one**\n-   Trace B: **one-to-one**\n-   Trace C: **many-to-many**\n\nNow, we evaluate the given options.\n\n**Option-by-Option Analysis**\n\n- **A. A: many-to-one, B: one-to-one, C: many-to-many**\n This assignment matches our derived conclusions for all three traces.\n **Verdict: Correct.**\n\n- **B. A: one-to-one, B: many-to-one, C: many-to-many**\n This incorrectly assigns the models for Trace A and Trace B. Trace A has only $1$ kernel thread, which cannot be one-to-one. Trace B has $4$ kernel threads, which cannot be many-to-one.\n **Verdict: Incorrect.**\n\n- **C. A: many-to-one, B: many-to-many, C: one-to-one**\n This incorrectly assigns the models for Trace B and Trace C. Trace B has $4$ kernel threads for $4$ user threads, which is one-to-one, not many-to-many. Trace C has $2$ kernel threads for $4$ user threads, which cannot be one-to-one.\n **Verdict: Incorrect.**\n\n- **D. A: many-to-many, B: one-to-one, C: many-to-one**\n This incorrectly assigns the models for Trace A and Trace C. Trace A has only $1$ kernel thread, which is many-to-one, not many-to-many. Trace C has $2$ kernel threads, which cannot be many-to-one.\n **Verdict: Incorrect.**\n\n- **E. None of the above; strace cannot distinguish the mapping model from these observations**\n Our analysis demonstrates that the observations from `strace`, specifically the number of kernel-thread identifiers and the behavior under blocking system calls, are sufficient to distinguish between these three fundamental threading models.\n **Verdict: Incorrect.**\n\nThe only option consistent with the analysis is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "One of the most significant challenges in user-level threading, especially in a many-to-one model, is the \"blocking problem,\" where a single thread's wait for I/O can freeze the entire application. This exercise moves from diagnosis to design, challenging you to engineer robust solutions for this issue. You will explore practical strategies, from using asynchronous APIs to offloading work, that are fundamental to building responsive and high-performance network services. ",
            "id": "3689607",
            "problem": "A runtime system implements a many-to-one user-level threading model, in which $U$ user threads are multiplexed onto $K = 1$ kernel thread. The scheduler is cooperative and event-driven, and its core invariant is that when the single kernel thread is blocked in a system call, all $U$ user threads are prevented from making progress. Consider a workload of $n$ user threads performing network-bound tasks that occasionally invoke the host-name resolution routine $getaddrinfo$ to translate domain names to addresses via the Domain Name System (DNS). Empirically, the runtime exhibits multi-millisecond stalls when $getaddrinfo$ resolves names under adverse network conditions.\n\nStarting from the following foundational facts and definitions, reason about wrapper designs for potentially blocking library functions:\n\n- In a many-to-one mapping, the operating system’s scheduler only schedules the single kernel thread; user-level scheduling occurs purely in user space, and the kernel is unaware of user threads. Therefore, any blocking operation executed by that kernel thread prevents all user threads from running until the call returns.\n- A blocking call is one whose execution may not complete immediately and may wait on external events (for example, network I/O). In Portable Operating System Interface (POSIX), a blocking system call that waits on I/O may not be interruptible by arbitrary signals, and libraries may restart interrupted calls transparently.\n- To maintain safety, any wrapper must preserve application-visible semantics, avoid undefined behavior, and integrate with the user-level scheduler without violating Application Programming Interface (API) contracts. In particular, wrappers must respect reentrancy and asynchronous-signal-safety requirements for functions that are not specified as safe.\n- The DNS resolution performed by $getaddrinfo$ may internally use sockets and I/O, but that detail is implementation-dependent and not standardized. The GNU C Library (glibc) offers an asynchronous variant $getaddrinfo\\_a$ on some systems.\n\nThe runtime engineers propose several strategies to detect and divert potentially blocking calls so that the single kernel thread never stalls the entire process. Which strategies, if correctly implemented, both prevent global stalls in the many-to-one model and preserve correctness? Select all that apply.\n\nA. Interpose a wrapper around `getaddrinfo` (for example via dynamic linker interposition) that executes the call in a dedicated pool of kernel threads created with `pthread_create`. The wrapper returns immediately to the user-level scheduler with a future-like handle; the user thread is parked, and completion is signaled through a thread-safe queue and an eventfd or pipe integrated into the event loop. The wrapper treats `getaddrinfo` as potentially blocking and never executes it on the single kernel thread.\n\nB. Install a periodic `SIGALRM` to fire after a scheduler quantum $q$ and wrap `getaddrinfo` such that the signal handler preempts the call, yielding to other user threads. The wrapper assumes that the signal will interrupt `getaddrinfo`, allowing the user-level scheduler to continue on the same kernel thread and resume the call later.\n\nC. Configure resolver timeouts aggressively by calling `res_init` and adjusting resolver options (for example, shorter retry intervals, fewer nameservers) to reduce expected resolution time $T_{\\mathrm{dns}}$. Keep executing `getaddrinfo` on the single kernel thread; rely on reduced blocking probability to avoid global stalls.\n\nD. Use `getaddrinfo_a` where available to obtain asynchronous name resolution, integrate its completion notifications into the event loop, and fall back to offloading when unavailable. Ensure that the wrapper never runs blocking resolution on the single kernel thread by using asynchronous delivery and parking the user-level thread until completion.\n\nE. Wrap `getaddrinfo` with a `fork`-and-pipe isolation: the wrapper `fork`s a child process that executes `getaddrinfo`; the child serializes the resulting `struct addrinfo` into a portable format and writes it to a pipe; the parent immediately returns to the user-level scheduler, parks the calling user thread, and later deserializes the result upon readiness of the pipe. The single kernel thread never performs the potentially blocking resolution itself.\n\nAnswer choices are mutually independent; select all choices that satisfy the stated conditions.",
            "solution": "The problem statement describes a classic challenge in systems with a many-to-one user-level threading model: a blocking system call made by the single underlying kernel thread will block all user-level threads. The goal is to evaluate several proposed strategies for wrapping the potentially blocking function `getaddrinfo` to prevent such a global stall while preserving correctness. The system has $U$ user threads on $K = 1$ kernel thread, with a workload of $n$ tasks.\n\nThe core principle for a valid solution is that the single kernel thread responsible for running the user-level scheduler's event loop must *never* execute an operation that could block for an indeterminate amount of time. The blocking operation must be offloaded to a separate execution context that the operating system kernel can schedule independently.\n\nLet us analyze each proposed strategy.\n\n**A. Interpose a wrapper around `getaddrinfo` (for example via dynamic linker interposition) that executes the call in a dedicated pool of kernel threads created with `pthread_create`. The wrapper returns immediately to the user-level scheduler with a future-like handle; the user thread is parked, and completion is signaled through a thread-safe queue and an eventfd or pipe integrated into the event loop. The wrapper treats `getaddrinfo` as potentially blocking and never executes it on the single kernel thread.**\n\nThis strategy correctly identifies the problem and proposes a robust, standard solution.\n1.  **Preventing Global Stalls**: By creating a separate pool of kernel threads using `pthread_create`, the blocking `getaddrinfo` call is executed by a worker kernel thread, not the main kernel thread running the user-level scheduler. The main thread simply enqueues the task and continues its event loop, running other ready user threads. The blocking of the worker thread does not affect the main thread, thus preventing a global stall. This effectively creates a temporary many-to-many model for the duration of the blocking call.\n2.  **Preserving Correctness**: The described mechanism for returning a result is sound. A \"future\" handle, parking the user thread, and using a thread-safe queue for the result coupled with an `eventfd` or pipe for notification are canonical patterns for integrating asynchronous work into an event-driven system. It ensures that the result is delivered back to the correct user thread without race conditions and that the application's sequential logic is preserved from the user thread's perspective.\n\n**Verdict**: **Correct**. This strategy effectively offloads the blocking work to a different kernel-schedulable entity, preventing the main event loop from stalling, and does so using a correct and safe communication pattern.\n\n**B. Install a periodic `SIGALRM` to fire after a scheduler quantum $q$ and wrap `getaddrinfo` such that the signal handler preempts the call, yielding to other user threads. The wrapper assumes that the signal will interrupt `getaddrinfo`, allowing the user-level scheduler to continue on the same kernel thread and resume the call later.**\n\nThis strategy is fundamentally flawed and unsafe.\n1.  **Preventing Global Stalls**: This approach will not work as described. If the kernel thread is blocked inside a system call (e.g., `recvmsg` waiting for a DNS reply), it is in a sleep state within the kernel. A signal like `SIGALRM` will be delivered only when the thread is scheduled to run again, which will not happen until the blocking call completes or is interrupted by the kernel. The signal handler cannot execute while the thread is blocked in the kernel. Furthermore, as the problem statement notes, system calls interrupted by signals (returning `EINTR`) are often transparently restarted by the C library, which would negate the interruption attempt. The kernel thread remains blocked, and the global stall persists.\n2.  **Preserving Correctness**: This strategy is extremely unsafe. The set of functions that are safe to call from a signal handler (async-signal-safe functions) is very limited. A user-level scheduler's \"yield\" function is almost certainly not async-signal-safe, as it would need to manipulate scheduler data structures that could be in an inconsistent state when the signal arrived. This would lead to race conditions and corruption. Additionally, `getaddrinfo` itself is not specified to be reentrant or safe to be partially executed, interrupted, and resumed later. Attempting to do so would likely result in undefined behavior.\n\n**Verdict**: **Incorrect**. This approach is based on incorrect assumptions about signal handling of blocked system calls and violates critical async-signal-safety requirements.\n\n**C. Configure resolver timeouts aggressively by calling `res_init` and adjusting resolver options (for example, shorter retry intervals, fewer nameservers) to reduce expected resolution time $T_{\\mathrm{dns}}$. Keep executing `getaddrinfo` on the single kernel thread; rely on reduced blocking probability to avoid global stalls.**\n\nThis strategy is a mitigation, not a solution.\n1.  **Preventing Global Stalls**: This approach does not prevent global stalls; it only attempts to limit their duration. If `getaddrinfo` is called and needs to wait for a network response, it will still block the single kernel thread. Even if the timeout is reduced to, say, $500$ ms, the entire system of $U$ user threads will be frozen for up to $500$ ms. The problem requires preventing the stall, not just shortening it. For any non-trivial application, a multi-millisecond stall is still a global stall.\n2.  **Preserving Correctness**: While tuning resolver options is a valid administrative task, relying on it to solve a fundamental concurrency problem is misguided. Moreover, overly aggressive timeouts can decrease the reliability of name resolution, causing resolution to fail under transient network congestion when it would have otherwise succeeded. This could be considered a failure to preserve application-visible semantics. However, the primary failure of this strategy is its inability to prevent the block.\n\n**Verdict**: **Incorrect**. This strategy reduces the maximum duration of a stall but does not prevent it, failing to meet the core requirement.\n\n**D. Use `getaddrinfo_a` where available to obtain asynchronous name resolution, integrate its completion notifications into the event loop, and fall back to offloading when unavailable. Ensure that the wrapper never runs blocking resolution on the single kernel thread by using asynchronous delivery and parking the user-level thread until completion.**\n\nThis strategy uses the tool specifically designed for this problem.\n1.  **Preventing Global Stalls**: The `getaddrinfo_a` function is an asynchronous API designed for event-driven applications. It initiates the name resolution operation and returns immediately, without blocking the calling thread. The actual work is performed in the background (e.g., by helper threads managed by the C library). The completion is signaled later (e.g., via a callback), which can be integrated into the main event loop. This ensures the main kernel thread never blocks on DNS resolution.\n2.  **Preserving Correctness**: This is the designated, correct way to perform asynchronous name resolution on systems that provide it (like those with glibc). The pattern of parking the user thread and using the event loop to handle the completion notification is the correct way to adapt such an API to a cooperative user-level threading environment. The inclusion of a fallback to another offloading method (such as the one in option A or E) makes the strategy robust and portable.\n\n**Verdict**: **Correct**. This is an ideal solution, as it uses a purpose-built, efficient, and correct API for the task.\n\n**E. Wrap `getaddrinfo` with a `fork`-and-pipe isolation: the wrapper `fork`s a child process that executes `getaddrinfo`; the child serializes the resulting `struct addrinfo` into a portable format and writes it to a pipe; the parent immediately returns to the user-level scheduler, parks the calling user thread, and later deserializes the result upon readiness of the pipe. The single kernel thread never performs the potentially blocking resolution itself.**\n\nThis strategy offloads the blocking work to a separate process.\n1.  **Preventing Global Stalls**: The `fork()` system call creates a new process, which the OS kernel schedules independently of the parent process. The blocking `getaddrinfo` call happens in the child process. The parent process—whose single kernel thread is running the user-level scheduler—can monitor the read end of the pipe in a non-blocking fashion as part of its event loop. Because the parent does not wait for the child or the pipe in a blocking manner, its kernel thread is free to run other user threads. This successfully prevents a global stall.\n2.  **Preserving Correctness**: The mechanism is conceptually sound. Using a pipe for inter-process communication (IPC) and monitoring its file descriptor in an event loop is a standard pattern. The main challenge, as noted, is the non-trivial task of serializing and deserializing the `struct addrinfo` linked list, which contains pointers. However, the problem allows us to assume a correct implementation. While `fork()` can be heavyweight (involving copying the parent's address space), it is a valid and correct way to achieve the required isolation.\n\n**Verdict**: **Correct**. Although potentially less efficient than a thread-based approach due to the overhead of process creation, this is a valid and correct strategy for isolating a blocking call and preventing a global stall.\n\nIn summary, strategies A, D, and E all describe valid methods for offloading a blocking operation from the main kernel thread in a many-to-one threading model, thus preventing a system-wide stall while maintaining correctness. Strategies B and C are flawed.",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "The choice between threading models is often a trade-off between performance, resource usage, and complexity. This final practice elevates your understanding from qualitative reasoning to quantitative analysis. You will develop a mathematical model to compare the throughput of different threading models, taking into account factors like context-switching costs and the nature of the workload, allowing you to determine precisely when one model outperforms another. ",
            "id": "3689613",
            "problem": "A computer with $P$ identical cores runs a large multithreaded workload. Each user thread alternates between a central processing unit (CPU) computation phase and a blocking input/output (I/O) phase. Define the blocking fraction $p$ so that, for a fixed per-thread cycle length $L$, the CPU computation time per cycle is $(1-p)L$ and the blocking I/O wait time per cycle is $pL$. Assume there are $T$ user threads with $T \\gg P$. Consider two threading models:\n\n- One-to-one (kernel-level) model: each user thread is a kernel thread. Blocking I/O causes the thread to block, and the operating system (OS) performs a kernel-level context switch of cost $c_k$ on each transition to blocking and again on wake-up; that is, two kernel context switches of total cost $2c_k$ per cycle. Because $T \\gg P$, there are always at least $P$ runnable kernel threads, so the cores are not starved for work due to blocking.\n\n- Many-to-many model: $K$ Light-Weight Processes (LWP) are used, where $K=P$. A user-level scheduler multiplexes user threads onto the $K$ LWPs. When a user thread performs blocking I/O synchronously, the LWP running it also blocks for a fraction $p$ of time, so at any instant the expected number of non-blocked LWPs is $P(1-p)$. The user-level scheduler performs a user-level context switch of cost $c_u$ on yield to I/O and again on resumption, for a total of $2c_u$ per cycle.\n\nUse the following foundational principles:\n\n- Throughput on $P$ cores is proportional to the total amount of useful CPU work completed per unit time.\n\n- Blocking I/O wait time $pL$ does not consume core time; context switch costs $c_k$ and $c_u$ consume core time.\n\n- For a thread or LWP that is not blocked, the fraction of its core time used for useful CPU work over one cycle is $\\frac{(1-p)L}{(1-p)L + \\text{overhead}}$, where $\\text{overhead}$ is the total context switching cost incurred on that cycle.\n\nUnder these assumptions, derive the critical blocking fraction $p^{*}$ (as a closed-form analytic expression in terms of $L$, $c_k$, and $c_u$) at which the many-to-many model achieves exactly the same throughput as the one-to-one model on $P$ cores. The many-to-many model outperforms the one-to-one model for $p < p^{*}$. No numerical rounding is required, and no physical units should be included in the final expression.",
            "solution": "We start from the definitions of throughput and the cycle structure. Each user thread’s cycle has total duration $L$, consisting of a CPU computation phase of duration $(1-p)L$ and a blocking I/O phase of duration $pL$. Blocking I/O consumes no CPU core time; context switch overheads consume core time.\n\nFor the one-to-one model, because $T \\gg P$, the OS always has at least $P$ runnable kernel threads despite some threads being blocked. Hence all $P$ cores can be kept busy. Over one cycle, a thread incurs two kernel-level context switches (one at the transition to blocking, one at wake-up), each of cost $c_k$, for a total overhead of $2c_k$. For an entity that is not blocked, the fraction of its core time used for useful CPU work is\n$$\n\\frac{(1-p)L}{(1-p)L + 2c_k}.\n$$\nSince there are $P$ cores kept busy, the aggregate useful CPU work rate (up to a proportional constant for core speed) is\n$$\nX_{1:1} = P \\cdot \\frac{(1-p)L}{(1-p)L + 2c_k}.\n$$\n\nFor the many-to-many model with $K=P$ Light-Weight Processes (LWP), synchronous blocking I/O causes each LWP to be blocked for a fraction $p$ of time. The expected number of non-blocked LWPs at any instant is $P(1-p)$, so at most $P(1-p)$ LWPs can run concurrently, potentially leaving some cores idle when $p>0$. Over one cycle for an active LWP, the user-level scheduler incurs two user-level context switches (yield on I/O and resumption), each of cost $c_u$, for a total overhead of $2c_u$. For an active LWP, the fraction of its core time used for useful CPU work is\n$$\n\\frac{(1-p)L}{(1-p)L + 2c_u}.\n$$\nAggregating over the expected number of active LWPs, the total useful CPU work rate is\n$$\nX_{m:m} = P(1-p) \\cdot \\frac{(1-p)L}{(1-p)L + 2c_u}.\n$$\n\nWe seek the critical blocking fraction $p^{*}$ such that $X_{m:m} = X_{1:1}$. Equating the two expressions and cancelling common positive factors yields\n$$\nP(1-p) \\cdot \\frac{(1-p)L}{(1-p)L + 2c_u} \\;=\\; P \\cdot \\frac{(1-p)L}{(1-p)L + 2c_k}.\n$$\nCancelling $P$ and $(1-p)L$ (valid for $p<1$), we obtain\n$$\n(1-p) \\cdot \\frac{1}{(1-p)L + 2c_u} \\;=\\; \\frac{1}{(1-p)L + 2c_k}.\n$$\nMultiplying through by the positive denominators gives\n$$\n(1-p)\\big((1-p)L + 2c_k\\big) = (1-p)L + 2c_u.\n$$\nLet $x = 1 - p$. Then the equality becomes\n$$\nx \\big(xL + 2c_k\\big) = xL + 2c_u,\n$$\nwhich expands to\n$$\nL x^{2} + 2c_k x = xL + 2c_u.\n$$\nRearranging terms yields a quadratic equation in $x$:\n$$\nL x^{2} + (2c_k - L) x - 2c_u = 0.\n$$\nSolving for $x$ using the quadratic formula,\n$$\nx = \\frac{L - 2c_k \\pm \\sqrt{(2c_k - L)^{2} + 8Lc_u}}{2L}.\n$$\nSince the product of the roots is $-2c_u/L < 0$, one root is negative and one is positive. The physically meaningful root in the interval $[0,1]$ is the one with the plus sign:\n$$\nx^{*} = \\frac{L - 2c_k + \\sqrt{(2c_k - L)^{2} + 8Lc_u}}{2L}.\n$$\nRecalling $x = 1 - p$, the critical blocking fraction is\n$$\np^{*} = 1 - x^{*} = \\frac{L + 2c_k - \\sqrt{(2c_k - L)^{2} + 8Lc_u}}{2L}.\n$$\n\nThus, the many-to-many model outperforms the one-to-one model precisely when $p < p^{*}$, where $p^{*}$ is given by the closed-form expression above. Note that the final expression is independent of $P$ because both models’ aggregate throughput scale linearly with $P$ and the equality condition involves ratios that cancel $P$.",
            "answer": "$$\\boxed{\\frac{L + 2c_k - \\sqrt{(2c_k - L)^{2} + 8Lc_u}}{2L}}$$"
        }
    ]
}