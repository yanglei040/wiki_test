{
    "hands_on_practices": [
        {
            "introduction": "To master Round-Robin (RR) scheduling, one must first grasp its fundamental mechanics and the non-negotiable cost of context switching. This exercise provides a concrete simulation of an RR scheduler, allowing you to trace the execution of several processes step-by-step. By working through this hypothetical scenario , you will see how scheduler overhead directly reduces effective CPU utilization and increases process turnaround time, building an intuition for the real-world performance implications of every scheduling decision.",
            "id": "3630387",
            "problem": "A single-core Central Processing Unit (CPU) runs a batch workload of $3$ compute-bound processes $\\{P_{1},P_{2},P_{3}\\}$ that all arrive at time $t=0$ and do not perform input/output or blocking. The operating system uses Round-Robin (RR) scheduling with a fixed time quantum $q=2\\,\\text{ms}$. A context switch is performed immediately before each time slice to dispatch the next process, and this dispatch overhead is fully serialized (no overlap with useful execution). The context switch cost depends on the cache state as follows: if the next process’s working set is still resident, the switch cost is $c_{\\text{hot}}$, otherwise it is $c_{\\text{cold}}$.\n\nDefine the process CPU burst demands as $b_{1}=7\\,\\text{ms}$, $b_{2}=3\\,\\text{ms}$, and $b_{3}=5\\,\\text{ms}$. At each dispatch, the executing process runs for $\\min\\{q,\\text{remaining burst}\\}$.\n\nConsider two preemption patterns that induce different cache states at dispatch:\n\n- Pattern $\\mathcal{H}$ (hot-retaining RR): the first dispatch at $t=0$ incurs $c_{\\text{cold}}$, and every subsequent dispatch incurs $c_{\\text{hot}}$.\n- Pattern $\\mathcal{C}$ (cold-dominated RR): every dispatch, including the first, incurs $c_{\\text{cold}}$.\n\nUse $c_{\\text{hot}}=0.1\\,\\text{ms}$ and $c_{\\text{cold}}=0.6\\,\\text{ms}$.\n\nFor this experiment, define the effective CPU utilization $U$ as the fraction of the total wall-clock time until all processes complete that is spent executing the processes’ CPU bursts (that is, excluding time spent in dispatch overhead). Define the turnaround time of process $i$ as $C_{i}-A_{i}$, where $A_{i}$ is the arrival time and $C_{i}$ is the completion time; since all arrivals are at $t=0$, the average turnaround time is $\\overline{T}=\\frac{1}{3}\\sum_{i=1}^{3}C_{i}$.\n\nCompute $U_{\\mathcal{H}}$ and $\\overline{T}_{\\mathcal{H}}$ under Pattern $\\mathcal{H}$, and $U_{\\mathcal{C}}$ and $\\overline{T}_{\\mathcal{C}}$ under Pattern $\\mathcal{C}$. Then form the combined improvement factor\n$$\nM \\;=\\; \\frac{U_{\\mathcal{H}}}{U_{\\mathcal{C}}}\\,\\cdot\\,\\frac{\\overline{T}_{\\mathcal{C}}}{\\overline{T}_{\\mathcal{H}}}\\,,\n$$\nwhich is dimensionless. Round your final value of $M$ to four significant figures and report it as a pure number with no units.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It describes a standard CPU scheduling scenario from operating systems theory with all necessary parameters provided. A unique solution can be derived by simulating the described scheduling model.\n\nThe solution requires a step-by-step simulation of the Round-Robin (RR) scheduling algorithm for the two specified context switch cost patterns, $\\mathcal{C}$ and $\\mathcal{H}$. For each pattern, we will determine the completion time $C_i$ for each process $P_i$, the total wall-clock time, the effective CPU utilization $U$, and the average turnaround time $\\overline{T}$.\n\nThe given parameters are:\n- Number of processes: $3$, namely $\\{P_{1}, P_{2}, P_{3}\\}$.\n- Arrival times: $A_{1}=A_{2}=A_{3}=0\\,\\text{ms}$.\n- CPU burst demands: $b_{1}=7\\,\\text{ms}$, $b_{2}=3\\,\\text{ms}$, and $b_{3}=5\\,\\text{ms}$.\n- Scheduling algorithm: Round-Robin (RR) with a time quantum $q=2\\,\\text{ms}$.\n- Context switch costs: $c_{\\text{hot}}=0.1\\,\\text{ms}$ and $c_{\\text{cold}}=0.6\\,\\text{ms}$.\n- Assumed scheduling order for the ready queue at $t=0$: $P_1 \\to P_2 \\to P_3$.\n\nThe total number of required time slices (and thus context switches) is the sum of the turns for each process:\n$N_{\\text{slices}} = \\lceil \\frac{b_1}{q} \\rceil + \\lceil \\frac{b_2}{q} \\rceil + \\lceil \\frac{b_3}{q} \\rceil = \\lceil \\frac{7}{2} \\rceil + \\lceil \\frac{3}{2} \\rceil + \\lceil \\frac{5}{2} \\rceil = 4 + 2 + 3 = 9$.\nThere will be a total of $9$ context switches.\n\nFirst, we analyze Pattern $\\mathcal{C}$ (cold-dominated RR), where every context switch incurs a cost of $c_{\\text{cold}}=0.6\\,\\text{ms}$.\n\nLet $t$ be the current time in $\\text{ms}$, initialized to $t=0$. Let $b_{i,r}$ be the remaining burst time for process $P_i$.\nInitially, $b_{1,r}=7$, $b_{2,r}=3$, $b_{3,r}=5$.\n1. Dispatch $P_1$: Overhead cost is $c_{\\text{cold}}=0.6$. The time is now $t=0.6$. $P_1$ runs for $q=2$. The time becomes $t=0.6+2=2.6$. Remaining burst for $P_1$ is $b_{1,r}=7-2=5$.\n2. Dispatch $P_2$: Overhead is $c_{\\text{cold}}=0.6$. $t=2.6+0.6=3.2$. $P_2$ runs for $q=2$. $t=3.2+2=5.2$. $b_{2,r}=3-2=1$.\n3. Dispatch $P_3$: Overhead is $c_{\\text{cold}}=0.6$. $t=5.2+0.6=5.8$. $P_3$ runs for $q=2$. $t=5.8+2=7.8$. $b_{3,r}=5-2=3$.\n4. Dispatch $P_1$: Overhead is $c_{\\text{cold}}=0.6$. $t=7.8+0.6=8.4$. $P_1$ runs for $q=2$. $t=8.4+2=10.4$. $b_{1,r}=5-2=3$.\n5. Dispatch $P_2$: Overhead is $c_{\\text{cold}}=0.6$. $t=10.4+0.6=11.0$. $P_2$ runs for its remaining $1\\,\\text{ms}$. $t=11.0+1=12.0$. $b_{2,r}=0$. Process $P_2$ completes, so its completion time is $C_{2, \\mathcal{C}}=12.0\\,\\text{ms}$.\n6. Dispatch $P_3$: Overhead is $c_{\\text{cold}}=0.6$. $t=12.0+0.6=12.6$. $P_3$ runs for $q=2$. $t=12.6+2=14.6$. $b_{3,r}=3-2=1$.\n7. Dispatch $P_1$: Overhead is $c_{\\text{cold}}=0.6$. $t=14.6+0.6=15.2$. $P_1$ runs for $q=2$. $t=15.2+2=17.2$. $b_{1,r}=3-2=1$.\n8. Dispatch $P_3$: Overhead is $c_{\\text{cold}}=0.6$. $t=17.2+0.6=17.8$. $P_3$ runs for its remaining $1\\,\\text{ms}$. $t=17.8+1=18.8$. $b_{3,r}=0$. Process $P_3$ completes, $C_{3, \\mathcal{C}}=18.8\\,\\text{ms}$.\n9. Dispatch $P_1$: Overhead is $c_{\\text{cold}}=0.6$. $t=18.8+0.6=19.4$. $P_1$ runs for its remaining $1\\,\\text{ms}$. $t=19.4+1=20.4$. $b_{1,r}=0$. Process $P_1$ completes, $C_{1, \\mathcal{C}}=20.4\\,\\text{ms}$.\n\nThe total time for Pattern $\\mathcal{C}$ is $T_{\\text{total}, \\mathcal{C}}=20.4\\,\\text{ms}$.\nThe total CPU burst execution time is $B_{\\text{total}} = 7+3+5=15\\,\\text{ms}$.\nThe effective CPU utilization for Pattern $\\mathcal{C}$ is $U_{\\mathcal{C}} = \\frac{B_{\\text{total}}}{T_{\\text{total}, \\mathcal{C}}} = \\frac{15}{20.4}$.\nThe completion times are $C_{1, \\mathcal{C}}=20.4$, $C_{2, \\mathcal{C}}=12.0$, $C_{3, \\mathcal{C}}=18.8$. Since all processes arrive at $A_i=0$, their turnaround times are equal to their completion times.\nThe average turnaround time for Pattern $\\mathcal{C}$ is $\\overline{T}_{\\mathcal{C}} = \\frac{1}{3}(20.4 + 12.0 + 18.8) = \\frac{51.2}{3}\\,\\text{ms}$.\n\nNext, we analyze Pattern $\\mathcal{H}$ (hot-retaining RR), where the first switch costs $c_{\\text{cold}}=0.6\\,\\text{ms}$ and the subsequent $8$ switches cost $c_{\\text{hot}}=0.1\\,\\text{ms}$.\n\n1. Dispatch $P_1$: Overhead is $c_{\\text{cold}}=0.6$. $t=0.6$. $P_1$ runs for $q=2$. $t=2.6$. $b_{1,r}=5$.\n2. Dispatch $P_2$: Overhead is $c_{\\text{hot}}=0.1$. $t=2.6+0.1=2.7$. $P_2$ runs for $q=2$. $t=4.7$. $b_{2,r}=1$.\n3. Dispatch $P_3$: Overhead is $c_{\\text{hot}}=0.1$. $t=4.7+0.1=4.8$. $P_3$ runs for $q=2. t=6.8$. $b_{3,r}=3$.\n4. Dispatch $P_1$: Overhead is $c_{\\text{hot}}=0.1$. $t=6.8+0.1=6.9$. $P_1$ runs for $q=2$. $t=8.9$. $b_{1,r}=3$.\n5. Dispatch $P_2$: Overhead is $c_{\\text{hot}}=0.1$. $t=8.9+0.1=9.0$. $P_2$ runs for $1\\,\\text{ms}$. $t=10.0$. $b_{2,r}=0$. $P_2$ completes, $C_{2, \\mathcal{H}}=10.0\\,\\text{ms}$.\n6. Dispatch $P_3$: Overhead is $c_{\\text{hot}}=0.1$. $t=10.0+0.1=10.1$. $P_3$ runs for $q=2$. $t=12.1$. $b_{3,r}=1$.\n7. Dispatch $P_1$: Overhead is $c_{\\text{hot}}=0.1$. $t=12.1+0.1=12.2$. $P_1$ runs for $q=2$. $t=14.2$. $b_{1,r}=1$.\n8. Dispatch $P_3$: Overhead is $c_{\\text{hot}}=0.1$. $t=14.2+0.1=14.3$. $P_3$ runs for $1\\,\\text{ms}$. $t=15.3$. $b_{3,r}=0$. $P_3$ completes, $C_{3, \\mathcal{H}}=15.3\\,\\text{ms}$.\n9. Dispatch $P_1$: Overhead is $c_{\\text{hot}}=0.1$. $t=15.3+0.1=15.4$. $P_1$ runs for $1\\,\\text{ms}$. $t=16.4$. $b_{1,r}=0$. $P_1$ completes, $C_{1, \\mathcal{H}}=16.4\\,\\text{ms}$.\n\nThe total time for Pattern $\\mathcal{H}$ is $T_{\\text{total}, \\mathcal{H}}=16.4\\,\\text{ms}$.\nTotal overhead is $c_{\\text{cold}} + (9-1)c_{\\text{hot}} = 0.6 + 8 \\times 0.1 = 1.4\\,\\text{ms}$.\nTotal time check: $B_{\\text{total}} + \\text{Overhead} = 15 + 1.4 = 16.4\\,\\text{ms}$, which matches.\nThe effective CPU utilization for Pattern $\\mathcal{H}$ is $U_{\\mathcal{H}} = \\frac{B_{\\text{total}}}{T_{\\text{total}, \\mathcal{H}}} = \\frac{15}{16.4}$.\nThe completion times are $C_{1, \\mathcal{H}}=16.4$, $C_{2, \\mathcal{H}}=10.0$, $C_{3, \\mathcal{H}}=15.3$.\nThe average turnaround time for Pattern $\\mathcal{H}$ is $\\overline{T}_{\\mathcal{H}} = \\frac{1}{3}(16.4 + 10.0 + 15.3) = \\frac{41.7}{3}\\,\\text{ms}$.\n\nFinally, we compute the combined improvement factor $M$:\n$$\nM = \\frac{U_{\\mathcal{H}}}{U_{\\mathcal{C}}}\\,\\cdot\\,\\frac{\\overline{T}_{\\mathcal{C}}}{\\overline{T}_{\\mathcal{H}}}\n$$\nSubstituting the derived values:\n$$\nM = \\frac{15/16.4}{15/20.4} \\cdot \\frac{51.2/3}{41.7/3} = \\frac{20.4}{16.4} \\cdot \\frac{51.2}{41.7}\n$$\n$$\nM = \\frac{20.4 \\times 51.2}{16.4 \\times 41.7} = \\frac{1044.48}{683.88} \\approx 1.5272633\n$$\nRounding the result to four significant figures, we get $1.527$.",
            "answer": "$$\\boxed{1.527}$$"
        },
        {
            "introduction": "Building on the impact of per-switch overhead, we now examine how the choice of the time quantum, $q$, affects the entire system's throughput. This practice models a system with a fixed CPU budget, a common scenario in cloud computing and embedded systems for enforcing resource limits. Deriving a formula for the maximum number of tasks the system can support  illuminates a core trade-off: a smaller quantum improves responsiveness but its associated overhead consumes more of the budget, ultimately limiting how many concurrent tasks can be serviced.",
            "id": "3678475",
            "problem": "A system uses a throttling mechanism to enforce a per-period execution budget for the Central Processing Unit (CPU). The mechanism operates on fixed-length periods of duration $T$, and in each period the total CPU time consumed by user tasks plus scheduler overhead must not exceed a budget $B$, where $0 < B \\leq T$. The scheduler is Round-Robin (RR), which assigns a fixed time quantum $q$ to each ready task. Each scheduled time slice of length $q$ is accompanied by a fixed overhead cost $c$ (covering context switch and bookkeeping), and this cost $c$ is accounted against the same budget $B$.\n\nAssume there are $n$ identical CPU-bound tasks that are always ready to run, and fairness is defined as giving exactly one $q$-long time slice to each task per period. The RR scheduler services the tasks cyclically within the period such that each of the $n$ tasks receives a time slice once per period, incurring the per-slice overhead $c$ for each slice scheduled.\n\nStarting from first principles—namely the definition of Round-Robin scheduling with fixed quantum, the budgeting rule that total CPU usage including overhead must not exceed $B$, and the assumption that each slice is accompanied by constant overhead—derive a closed-form expression for the maximum integer number of tasks $n$ that can be scheduled per period without violating the budget. Express your final answer as an analytic expression in terms of $B$, $q$, and $c$. No numerical rounding is required.",
            "solution": "The problem requires the derivation of a closed-form expression for the maximum integer number of tasks, $n$, that can be scheduled in a period, given a CPU budget $B$, a time quantum $q$, and a per-slice overhead $c$.\n\nThe derivation proceeds from the fundamental constraints provided in the problem statement.\n\n1.  **Total CPU Consumption per Period**: The total CPU time consumed in one period is the sum of the time spent executing user tasks and the time spent on scheduler overhead.\n\n2.  **User Task CPU Time**: According to the problem's definition of fairness, each of the $n$ tasks receives exactly one time slice of duration $q$ per period. Therefore, the total CPU time allocated to user tasks within one period is the number of tasks multiplied by the duration of the time quantum for each task.\n    $$\n    T_{\\text{tasks}} = n \\cdot q\n    $$\n\n3.  **Scheduler Overhead CPU Time**: For each time slice scheduled, a fixed overhead cost $c$ is incurred. Since there are $n$ tasks and each is scheduled once, there are $n$ scheduling events in total per period. The total overhead cost is the number of scheduling events multiplied by the cost per event.\n    $$\n    T_{\\text{overhead}} = n \\cdot c\n    $$\n\n4.  **Budget Constraint**: The problem states that the total CPU time consumed by user tasks plus the scheduler overhead must not exceed the budget $B$. This imposes the following inequality:\n    $$\n    T_{\\text{tasks}} + T_{\\text{overhead}} \\leq B\n    $$\n    Substituting the expressions for $T_{\\text{tasks}}$ and $T_{\\text{overhead}}$, we get:\n    $$\n    n \\cdot q + n \\cdot c \\leq B\n    $$\n\n5.  **Solving for n**: We can factor out $n$ from the left-hand side of the inequality:\n    $$\n    n(q + c) \\leq B\n    $$\n    The term $(q+c)$ represents the total cost to the budget for scheduling a single task for one time slice. Since $q$ is a time quantum, it must be positive ($q > 0$), and since $c$ is an overhead cost, it must be non-negative ($c \\geq 0$). Therefore, the sum $(q+c)$ is strictly positive. We can divide the inequality by $(q+c)$ without changing the direction of the inequality sign:\n    $$\n    n \\leq \\frac{B}{q+c}\n    $$\n    This inequality defines the upper bound for the number of tasks, $n$.\n\n6.  **Integer Constraint**: The problem asks for the *maximum integer number* of tasks. Since $n$ must be an integer, its maximum value is the largest integer that is less than or equal to the expression $\\frac{B}{q+c}$. This is, by definition, the floor of the expression. The floor function, denoted as $\\lfloor x \\rfloor$, yields the greatest integer less than or equal to $x$.\n\nTherefore, the maximum integer number of tasks, $n$, is given by:\n$$\nn = \\left\\lfloor \\frac{B}{q+c} \\right\\rfloor\n$$\nThis is the final closed-form expression for the maximum number of tasks schedulable per period without violating the budget, expressed in terms of $B$, $q$, and $c$. The period duration $T$ is a contextual parameter confirming that the budget $B$ is a valid fraction of the total time ($B \\leq T$), but it does not enter into the calculation for $n$ based on the budget constraint.",
            "answer": "$$\n\\boxed{\\left\\lfloor \\frac{B}{q+c} \\right\\rfloor}\n$$"
        },
        {
            "introduction": "We now arrive at the central question of this topic: how does an operating system designer select an optimal time quantum? This problem synthesizes the previous concepts into a practical optimization challenge. This thought experiment  models the inherent conflict between minimizing kernel processing overhead (which favors a larger $q$) and reducing interactive latency for users (which favors a smaller $q$). By deriving the quantum $q^{\\star}$ that balances these two competing costs, you will engage in the type of quantitative reasoning that underpins the design of modern, high-performance operating systems.",
            "id": "3678402",
            "problem": "Consider a time-shared Operating System (OS) using Round-Robin (RR) scheduling with $N$ runnable interactive processes. Each process alternates between short compute bursts and issuing Input/Output (I/O) to the kernel. To reduce kernel overhead, each process attempts to batch its I/O: in the absence of preemption, a typical compute burst of length $B$ (measured in seconds of CPU time) ends with a single batched system call, whose kernel execution time cost is $c_{sys}$ (measured in seconds of CPU time). However, if the RR time quantum $q$ is shorter than $B$, the burst is fragmented into multiple scheduled slices, and to maintain timely output, the process issues a system call at the end of each fragment. This fragmentation increases the number of system calls per burst and therefore increases total system call time spent in the kernel.\n\nAssume the following idealizations:\n- Context switching overhead is negligible compared to $c_{sys}$ and is ignored.\n- Each unfragmented compute burst of length $B$ ends with exactly one system call of cost $c_{sys}$; if the burst is fragmented by RR preemption, the number of system calls equals the number of fragments.\n- For $q \\leq B$, approximate $\\lceil B/q \\rceil$ by $B/q$ to obtain a differentiable cost model; additive constants independent of $q$ may be dropped when minimizing over $q$.\n- Interactive events for a given process arrive independently of scheduling and uniformly over the RR cycle. If an event arrives while the process is running, it can be handled immediately; otherwise it must wait until the process’s next scheduled slice.\n\nUsing only standard definitions of RR and the uniform-arrival assumption, derive an expression for the time quantum $q$ that minimizes the following composite objective:\n- The expected additional system call time per unit CPU time caused by fragmentation when $q \\leq B$, and\n- A weighted expected start latency for an interactive event, with weight $w$ (measured in cost per second) that converts expected latency (in seconds) to the same cost units as system call time.\n\nExpress your final answer as a single closed-form analytic expression $q^{\\star}$ in terms of $N$, $c_{sys}$, and $w$. No numerical evaluation is required, and no rounding is needed. State no units in your final answer.",
            "solution": "We begin from the core Round-Robin (RR) definitions and the uniform-arrival assumption. In RR with $N$ runnable processes and negligible overhead, the scheduler cycles through the processes, giving each up to $q$ seconds of CPU time. One full cycle length is $N q$ seconds.\n\nFirst, we model the fragmentation-induced system call cost. In the unfragmented case, one compute burst of length $B$ ends with a single system call of cost $c_{sys}$. If the time quantum is $q \\leq B$, then the burst is split into approximately $B/q$ fragments (replacing $\\lceil B/q \\rceil$ by $B/q$ for differentiability), and one system call is issued at the end of each fragment. Thus, the total number of system calls per burst is approximately $B/q$, and the total system call time per burst is approximately $(B/q) c_{sys}$. Compared to the unfragmented single-call cost $c_{sys}$, the additional system call time per burst caused by fragmentation is\n$$\n\\left(\\frac{B}{q} - 1\\right) c_{sys}.\n$$\nTo obtain a cost per unit of CPU time, divide by the burst length $B$:\n$$\n\\frac{1}{B} \\left(\\frac{B}{q} - 1\\right) c_{sys} \\;=\\; \\frac{c_{sys}}{q} \\;-\\; \\frac{c_{sys}}{B}.\n$$\nWhen minimizing with respect to $q$, the constant term $-c_{sys}/B$ does not affect the optimal $q$, so we drop it. Thus, the fragmentation-induced system call cost per unit CPU time has the form\n$$\nJ_{\\text{sys}}(q) \\;=\\; \\frac{c_{sys}}{q}.\n$$\n\nSecond, we model the expected interactive start latency under RR with uniform arrival. Consider a fixed process. Over one RR cycle of length $N q$, its own quantum occupies $q$ seconds, and the other $N-1$ processes occupy $(N-1) q$ seconds. Assume an interactive event for the process arrives uniformly over the cycle. If the event arrives during the process’s own quantum, it can be handled immediately, so the latency is $0$. If the event arrives during one of the other $N-1$ quanta, the process must wait until its next scheduled slice. Conditioned on arrival within another process’s quantum, the expected remaining time in that current quantum is $q/2$ (by uniformity within the interval), and the expected number of complete intervening quanta before the target process’s next slice is uniformly distributed over $\\{0,1,\\dots,N-2\\}$, which has mean $(N-2)/2$. Therefore, the conditional expected waiting time is\n$$\n\\frac{q}{2} \\;+\\; \\frac{N-2}{2}\\, q \\;=\\; \\frac{N-1}{2}\\, q.\n$$\nThe probability of arriving during another process’s quantum is $(N-1)/N$, and during the process’s own quantum is $1/N$. Hence, the unconditional expected start latency is\n$$\n\\mathbb{E}[T(q)] \\;=\\; \\frac{N-1}{N} \\cdot \\frac{N-1}{2}\\, q \\;=\\; \\frac{(N-1)^{2}}{2N}\\, q.\n$$\nWe convert this expected latency into the same cost units as system call time using a weight $w$ (measured in cost per second). The latency cost is\n$$\nJ_{\\text{lat}}(q) \\;=\\; w \\cdot \\mathbb{E}[T(q)] \\;=\\; w \\cdot \\frac{(N-1)^{2}}{2N}\\, q.\n$$\n\nThe total objective to minimize as a function of $q$ is the sum\n$$\nJ(q) \\;=\\; J_{\\text{sys}}(q) \\;+\\; J_{\\text{lat}}(q) \\;=\\; \\frac{c_{sys}}{q} \\;+\\; w \\cdot \\frac{(N-1)^{2}}{2N}\\, q.\n$$\nThis function is strictly convex for $q>0$ because it is the sum of a convex term in $q$ ($\\propto q$) and a convex term in $1/q$ ($\\propto 1/q$). The unique minimizer $q^{\\star}$ satisfies the first-order optimality condition $J'(q^{\\star})=0$.\n\nCompute the derivative:\n$$\nJ'(q) \\;=\\; -\\, \\frac{c_{sys}}{q^{2}} \\;+\\; w \\cdot \\frac{(N-1)^{2}}{2N}.\n$$\nSet $J'(q)=0$ and solve for $q$:\n$$\n-\\, \\frac{c_{sys}}{q^{2}} \\;+\\; w \\cdot \\frac{(N-1)^{2}}{2N} \\;=\\; 0\n\\;\\;\\Longrightarrow\\;\\;\n\\frac{c_{sys}}{q^{2}} \\;=\\; w \\cdot \\frac{(N-1)^{2}}{2N}\n\\;\\;\\Longrightarrow\\;\\;\nq^{2} \\;=\\; \\frac{2N\\, c_{sys}}{w\\, (N-1)^{2}}.\n$$\nTaking the positive square root (since $q>0$), the optimal time quantum is\n$$\nq^{\\star} \\;=\\; \\sqrt{\\frac{2N\\, c_{sys}}{w\\, (N-1)^{2}}}.\n$$\n\nThis $q^{\\star}$ balances the $1/q$ increase in system call time due to fragmentation when $q$ is small against the linear-in-$q$ increase in expected interactive start latency when $q$ is large, under the stated RR and uniform-arrival assumptions.",
            "answer": "$$\\boxed{\\sqrt{\\frac{2 N\\, c_{sys}}{w\\, (N-1)^{2}}}}$$"
        }
    ]
}