{
    "hands_on_practices": [
        {
            "introduction": "Effective thread scheduling is not just about choosing which thread to run next; it's also about managing the inherent delays within the operating system itself. In this first exercise, we dissect the microscopic sources of latency that impact system responsiveness. You will calculate the worst-case delay for a high-priority thread to start running (dispatch latency) and for a hardware interrupt to be serviced (interrupt latency), based on fundamental kernel operations like critical sections and interrupt masking . This practice provides a concrete foundation for understanding the trade-offs between kernel integrity and real-time performance.",
            "id": "3688825",
            "problem": "A single-core central processing unit (CPU) runs a priority-based, fully preemptive thread scheduler inside an Operating System (OS). The kernel sometimes executes in a critical section where thread preemption is disabled. Within a subinterval of that critical section, hardware interrupts are also masked. The following conditions hold.\n\n- When thread preemption is disabled in the kernel, a higher-priority ready thread cannot preempt the currently running thread until the kernel reaches a scheduling point where preemption is re-enabled.\n- Upon leaving the critical section, the kernel performs a reschedule check and, if a higher-priority thread is ready, the scheduler runs and performs a context switch before the new thread begins executing.\n- When interrupts are masked, the CPU defers the servicing of hardware interrupts; once interrupts are unmasked, the CPU vectors to the appropriate Interrupt Service Routine (ISR) with a finite entry latency.\n\nAssume the system behaves as follows:\n\n- The kernel disables preemption for a worst-case duration of $T_{\\text{crit}} = 84$ microseconds.\n- Inside each such critical section, interrupts are masked for a worst-case contiguous subinterval of $T_{\\text{mask}} = 31$ microseconds.\n- The scheduler’s decision-making after a reschedule check incurs an overhead of $T_{\\text{sched}} = 11$ microseconds, and the subsequent context switch overhead is $T_{\\text{cs}} = 19$ microseconds.\n- The hardware interrupt entry latency (from interrupts being unmasked to the first instruction of the ISR) is $T_{\\text{irq}} = 7$ microseconds.\n- Ignore any other delays such as cache effects or non-maskable interrupts, and assume no concurrent activity beyond what is described.\n\nAt time $t_0$, a high-priority user thread becomes ready while a lower-priority thread is running and has just entered the kernel critical section where preemption is disabled. Separately, consider a hardware event that raises an interrupt exactly when the kernel begins the masked subinterval.\n\nUsing only the core definitions of preemptive priority scheduling, kernel critical sections, scheduling points, and interrupt masking, derive and compute:\n\n1. The worst-case time from $t_0$ until the high-priority user thread begins executing its first user-mode instruction.\n2. The worst-case time from the arrival of the hardware event to the first instruction of its ISR.\n\nExpress both final answers in microseconds. No rounding is required. Your final answer should contain the two numbers in a single row in the order listed above.",
            "solution": "The problem asks for two distinct worst-case latency calculations on a single-core system with a preemptive, priority-based scheduler.\n\n**Part 1: Worst-case latency for a high-priority user thread**\n\nWe are asked to find the worst-case time from the moment a high-priority user thread becomes ready (at time $t_0$) until it begins executing its first user-mode instruction. This is also known as dispatch latency.\n\nThe scenario specifies that at time $t_0$, a lower-priority thread is running and has just entered a kernel critical section where preemption is disabled. The duration of this preemption-disabled state has a worst-case value of $T_{\\text{crit}}$.\n\n1.  **Waiting for Preemption Point:** Because the currently running lower-priority thread is in a non-preemptible critical section, the newly ready high-priority thread must wait. Since the low-priority thread has *just entered* this section, the worst-case waiting time corresponds to the entire duration of the critical section, which is $T_{\\text{crit}}$.\n\n2.  **Scheduler Overhead:** Upon exiting the critical section, the kernel reaches a scheduling point. It performs a reschedule check and finds that a higher-priority thread is ready. The scheduler is then invoked to make a decision. The problem states this decision-making process incurs an overhead of $T_{\\text{sched}}$.\n\n3.  **Context Switch Overhead:** After the scheduler has decided to run the high-priority thread, the OS must perform a context switch. This involves saving the state of the lower-priority thread and loading the state of the high-priority thread. This process has an overhead of $T_{\\text{cs}}$.\n\nThe total worst-case latency for the high-priority thread, let's call it $L_{\\text{thread}}$, is the sum of these sequential delays. The components $T_{\\text{mask}}$ and $T_{\\text{irq}}$ are irrelevant to this calculation as they pertain to hardware interrupt handling, not thread preemption.\n\nThe total time is given by the sum:\n$$L_{\\text{thread}} = T_{\\text{crit}} + T_{\\text{sched}} + T_{\\text{cs}}$$\n\nSubstituting the given values:\n$T_{\\text{crit}} = 84$ microseconds\n$T_{\\text{sched}} = 11$ microseconds\n$T_{\\text{cs}} = 19$ microseconds\n\n$$L_{\\text{thread}} = 84 \\mu s + 11 \\mu s + 19 \\mu s = 114 \\mu s$$\n\nSo, the worst-case time until the high-priority thread begins execution is $114$ microseconds.\n\n**Part 2: Worst-case latency for a hardware interrupt**\n\nWe are asked to find the worst-case time from the arrival of a hardware event to the execution of the first instruction of its corresponding Interrupt Service Routine (ISR). This is a measure of interrupt latency.\n\nThe scenario specifies that the hardware event raises an interrupt precisely when the kernel begins a contiguous subinterval where interrupts are masked. The worst-case duration of this masked period is $T_{\\text{mask}}$.\n\n1.  **Waiting for Interrupts to be Unmasked:** When the interrupt arrives, the CPU has interrupts disabled (masked). The CPU cannot respond to the interrupt signal and will defer it. The kernel continues to execute. Since the interrupt arrived at the very beginning of the masked interval, the system must wait for the entire worst-case duration of this interval to pass before interrupts are re-enabled. This time is $T_{\\text{mask}}$.\n\n2.  **Interrupt Entry Latency:** Once interrupts are unmasked, the CPU's interrupt controller will signal the pending interrupt. The CPU then saves minimal context and vectors to the start of the appropriate ISR. The problem defines the latency from the moment interrupts are unmasked to the execution of the first ISR instruction as $T_{\\text{irq}}$.\n\nThe total worst-case latency for the ISR, let's call it $L_{\\text{irq}}$, is the sum of these two sequential delays. The duration of the larger critical section ($T_{\\text{crit}}$) and the thread scheduling overheads ($T_{\\text{sched}}$, $T_{\\text{cs}}$) are not relevant here. Interrupt dispatch is a hardware-level mechanism that typically bypasses the thread scheduler. The CPU transfers control to the ISR as soon as interrupts are unmasked, irrespective of which thread was running.\n\nThe total time is given by the sum:\n$$L_{\\text{irq}} = T_{\\text{mask}} + T_{\\text{irq}}$$\n\nSubstituting the given values:\n$T_{\\text{mask}} = 31$ microseconds\n$T_{\\text{irq}} = 7$ microseconds\n\n$$L_{\\text{irq}} = 31 \\mu s + 7 \\mu s = 38 \\mu s$$\n\nSo, the worst-case time from the hardware event to the start of the ISR is $38$ microseconds.\n\nThe two computed values are $114$ and $38$, both in microseconds.",
            "answer": "$$\\boxed{\\begin{pmatrix} 114  38 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Building on our understanding of basic latencies, we now turn to the critical task of guaranteeing performance in real-time systems where tasks must meet strict deadlines. In applications from aviation to industrial control, it's not enough for a system to be fast on average; it must be predictably fast even in the worst-case scenario. This problem introduces you to Response Time Analysis (RTA), a cornerstone of real-time systems theory, where you will calculate the maximum possible response time for a thread in a fixed-priority preemptive scheduler . Mastering this analysis is key to designing systems where timing deadlines are rigorously met.",
            "id": "3688824",
            "problem": "A uniprocessor Operating System (OS) uses fixed-priority preemptive scheduling with short non-preemptible regions: whenever a thread executes in a non-preemptible region, preemption is disabled for at most $L_{\\mathrm{np}}$ time units. All threads are periodic and independent. The Central Processing Unit (CPU) is fully available to software, with no direct memory access or other autonomous devices.\n\nThere are three threads with static priorities ordered from highest to lowest as $M \\succ H \\succ L$. Thread $M$ is a maintenance thread with period $T_{M}$ and Worst-Case Execution Time (WCET) $C_{M}$; thread $H$ is the high-priority application thread of interest with WCET $C_{H}$; thread $L$ is any lower-priority background activity. Assume that:\n- At the instant a job of $H$ is released, some lower-priority thread may be executing inside a non-preemptible region, and hence $H$ may be blocked once by at most $L_{\\mathrm{np}}$ before it can begin executing.\n- During $H$’s response window, $M$ may arrive and preempt $H$ whenever $M$ has pending work, because $M$ has higher priority than $H$.\n- There are no shared resources beyond the non-preemptible regions; all overheads other than non-preemptibility are negligible and already included in the WCET parameters.\n- Assume worst-case phasing between $M$ and $H$ that maximizes the delay experienced by $H$.\n\nFrom first principles, use the definitions of fixed-priority preemptive scheduling and bounded non-preemptible blocking to derive an equation characterizing an upper bound on the response time of a job of $H$ (the elapsed time from its release until it completes), accounting for one-time blocking due to non-preemptibility and all preemptions by $M$. Then evaluate this bound for the following parameters:\n- $C_{H} = 2.0~\\mathrm{ms}$,\n- $C_{M} = 0.8~\\mathrm{ms}$,\n- $T_{M} = 3.0~\\mathrm{ms}$,\n- $L_{\\mathrm{np}} = 0.3~\\mathrm{ms}$.\n\nExpress the final response time of $H$ in milliseconds and round your answer to four significant figures.",
            "solution": "The problem asks for an upper bound on the worst-case response time, denoted as $R_H$, for a job of thread $H$. The response time is the total time elapsed from the release of a job until its completion. In a fixed-priority preemptive system, the response time of a thread $i$ is composed of three parts:\n1.  Its own worst-case execution time, $C_i$.\n2.  The maximum time it can be blocked by lower-priority threads, $B_i$.\n3.  The total execution time of higher-priority threads that preempt it, known as interference, $I_i$.\n\nThe general equation for the response time $R_i$ is:\n$$R_i = C_i + B_i + I_i$$\n\nFor thread $H$, this becomes:\n$$R_H = C_H + B_H + I_H$$\n\nWe will now determine each component for thread $H$.\n\n**1. Execution Time ($C_H$):**\nThe WCET of thread $H$ is given as $C_H$.\n\n**2. Blocking Time ($B_H$):**\nThe problem states that thread $H$ can be blocked once at the beginning of its execution by any lower-priority thread ($L$) that is inside a non-preemptible region. The maximum duration of such a region is given as $L_{\\mathrm{np}}$. Therefore, the maximum blocking time for thread $H$ is:\n$$B_H = L_{\\mathrm{np}}$$\n\n**3. Interference ($I_H$):**\nThe interference on thread $H$ is caused by executions of higher-priority threads. The only thread with higher priority than $H$ is $M$. To calculate the worst-case interference, we assume a \"critical instant,\" where a job of $H$ is released at the same time as a job of every higher-priority thread. This configuration maximizes the preemption and hence the response time.\n\nThe response time of $H$, $R_H$, defines the time window during which we must account for preemptions. During this interval of length $R_H$, thread $M$ will be released a certain number of times. The maximum number of jobs of $M$ that can be released and execute within the interval $R_H$ is given by $\\lceil \\frac{R_H}{T_M} \\rceil$, where $T_M$ is the period of $M$.\n\nEach job of $M$ executes for at most its WCET, $C_M$. Thus, the total interference from thread $M$ on thread $H$ is:\n$$I_H = \\left\\lceil \\frac{R_H}{T_M} \\right\\rceil C_M$$\n\n**4. Response Time Equation for H:**\nSubstituting the expressions for $B_H$ and $I_H$ into the response time equation for $H$, we obtain the equation that characterizes the upper bound on its response time:\n$$R_H = C_H + L_{\\mathrm{np}} + \\left\\lceil \\frac{R_H}{T_M} \\right\\rceil C_M$$\nThis equation is a recurrence relation, as $R_H$ appears on both sides. It can be solved using an iterative approach. We define a sequence of approximations for $R_H$, denoted by $R_H^{(k)}$, where $k$ is the iteration index ($k=0, 1, 2, \\dots$).\n\nThe iterative formula is:\n$$R_H^{(k+1)} = C_H + L_{\\mathrm{np}} + \\left\\lceil \\frac{R_H^{(k)}}{T_M} \\right\\rceil C_M$$\n\nA suitable initial value for the iteration, $R_H^{(0)}$, is the execution time of $H$ plus any blocking, which is the response time assuming no interference:\n$$R_H^{(0)} = C_H + L_{\\mathrm{np}}$$\n\nThe iteration continues until the value stabilizes, i.e., $R_H^{(k+1)} = R_H^{(k)}$. This stable value is the worst-case response time.\n\n### Numerical Evaluation\nWe now evaluate this bound using the provided parameters:\n- $C_{H} = 2.0~\\mathrm{ms}$\n- $C_{M} = 0.8~\\mathrm{ms}$\n- $T_{M} = 3.0~\\mathrm{ms}$\n- $L_{\\mathrm{np}} = 0.3~\\mathrm{ms}$\n\n**Iteration 0:**\nInitialize the response time with the sum of $H$'s own execution time and blocking time.\n$$R_H^{(0)} = C_H + L_{\\mathrm{np}} = 2.0 + 0.3 = 2.3~\\mathrm{ms}$$\n\n**Iteration 1:**\nCalculate the next estimate, $R_H^{(1)}$, using $R_H^{(0)}$.\n$$R_H^{(1)} = C_H + L_{\\mathrm{np}} + \\left\\lceil \\frac{R_H^{(0)}}{T_M} \\right\\rceil C_M$$\n$$R_H^{(1)} = 2.0 + 0.3 + \\left\\lceil \\frac{2.3}{3.0} \\right\\rceil \\times 0.8$$\n$$R_H^{(1)} = 2.3 + \\lceil 0.766... \\rceil \\times 0.8$$\n$$R_H^{(1)} = 2.3 + 1 \\times 0.8 = 3.1~\\mathrm{ms}$$\n\n**Iteration 2:**\nCalculate $R_H^{(2)}$ using $R_H^{(1)}$.\n$$R_H^{(2)} = C_H + L_{\\mathrm{np}} + \\left\\lceil \\frac{R_H^{(1)}}{T_M} \\right\\rceil C_M$$\n$$R_H^{(2)} = 2.0 + 0.3 + \\left\\lceil \\frac{3.1}{3.0} \\right\\rceil \\times 0.8$$\n$$R_H^{(2)} = 2.3 + \\lceil 1.033... \\rceil \\times 0.8$$\n$$R_H^{(2)} = 2.3 + 2 \\times 0.8 = 2.3 + 1.6 = 3.9~\\mathrm{ms}$$\n\n**Iteration 3:**\nCalculate $R_H^{(3)}$ using $R_H^{(2)}$.\n$$R_H^{(3)} = C_H + L_{\\mathrm{np}} + \\left\\lceil \\frac{R_H^{(2)}}{T_M} \\right\\rceil C_M$$\n$$R_H^{(3)} = 2.0 + 0.3 + \\left\\lceil \\frac{3.9}{3.0} \\right\\rceil \\times 0.8$$\n$$R_H^{(3)} = 2.3 + \\lceil 1.3 \\rceil \\times 0.8$$\n$$R_H^{(3)} = 2.3 + 2 \\times 0.8 = 2.3 + 1.6 = 3.9~\\mathrm{ms}$$\n\nSince $R_H^{(3)} = R_H^{(2)}$, the iteration has converged. The worst-case response time for thread $H$ is $R_H = 3.9~\\mathrm{ms}$.\n\nThe problem requires the answer to be rounded to four significant figures. The value $3.9$ is written as $3.900$ to meet this requirement.",
            "answer": "$$\\boxed{3.900}$$"
        },
        {
            "introduction": "Our focus now shifts from the strict deadlines of real-time systems to the goal of fairness in modern, general-purpose operating systems like Linux. When numerous applications with varying importance run concurrently, how does the scheduler ensure that each gets its proportional share of the CPU? This hands-on simulation challenges you to implement the logic behind the Completely Fair Scheduler (CFS) by using the concept of 'virtual runtime,' where a thread's execution time is normalized by its weight, $w_i$ . By deriving the update rule for virtual runtime, $v_i$, and modeling its behavior, you will gain a deep, practical insight into how contemporary schedulers achieve sophisticated, weighted resource allocation.",
            "id": "3688841",
            "problem": "You are to write a complete program that simulates a weighted fair thread scheduler based on the principle of virtual runtime normalization, in the spirit of the Completely Fair Scheduler. The simulation must be expressed in purely mathematical and logical terms and must be self-contained. The threads are always runnable, each thread $i$ has a weight $w_i$ that can change over time in predefined phases, the scheduler maintains a virtual runtime $v_i$ for each thread, and at each scheduling decision it selects the thread with the smallest $v_i$ (ties must be broken deterministically by the smallest index $i$). The simulation runs in discrete quanta of time, each of size $q$ milliseconds. All time units in the program must be treated as milliseconds and all outputs must be expressed in unitless decimal fractions rounded to $6$ decimal places.\n\nFundamental base definitions and facts:\n- Weighted processor sharing requires that over any interval with fixed weights, the fraction of processing time allocated to thread $i$ is proportional to $w_i$. If there are $N$ threads with weights $w_1,\\dots,w_N$, then the ideal share fraction for thread $i$ is $s_i = \\dfrac{w_i}{\\sum_{j=1}^{N} w_j}$. Equivalently, over a small interval of duration $\\Delta t$, thread $i$ should receive $\\Delta a_i = \\Delta t \\cdot s_i$ units of processing time.\n- A virtual runtime $v_i$ serves as a balancing potential: the scheduler always chooses the thread with the smallest $v_i$. To achieve fairness that matches weighted processor sharing, the update law for $v_i$ during execution must be derived from the above invariance principle and must ensure that lower-weight threads accumulate $v_i$ faster than higher-weight threads.\n- The total time $T$ is the sum of all phase durations. The ideal cumulative processing time $I_i$ for thread $i$ over a workload where weights can change by phases is the time integral of $s_i$ over the entire execution: integrating piecewise by phase with constant weights. The actual cumulative processing time $A_i$ is the sum of the discrete quanta executed for thread $i$.\n\nYour program must:\n1. Derive, from the invariance of weighted processor sharing and the definition of virtual runtime selection, a consistent update law for $v_i$ during an execution quantum of duration $q$ when thread $i$ runs.\n2. Simulate the scheduler for a set of test workloads where weights change across phases:\n   - Initialize $v_i = 0$ for all threads at time $t=0$.\n   - For each phase $p$ with duration $D_p$ milliseconds and constant weight vector $\\{w_i^{(p)}\\}_{i=1}^N$, perform $D_p / q$ discrete scheduling decisions of size $q$ milliseconds.\n   - At each decision, choose the thread with the smallest $v_i$ (breaking ties by smallest $i$), run it for $q$ milliseconds, update its $v_i$ according to your derived law, and update the ideal accumulation $I_i$ for all threads using the share $s_i^{(p)} = \\dfrac{w_i^{(p)}}{\\sum_{j=1}^{N} w_j^{(p)}}$ over $q$ milliseconds.\n3. After the final phase, compute the fairness gap\n   $$G = \\max_{i \\in \\{1,\\dots,N\\}} \\frac{\\left|A_i - I_i\\right|}{T},$$\n   which is a unitless decimal fraction.\n\nTest suite and parameters to implement:\n- Use a scheduling quantum $q = 1$ millisecond for all cases.\n- Case $1$ (happy path, equal weights): $N = 2$, number of phases $P = 1$, durations $\\{D_1\\} = \\{40\\}$ milliseconds, weights per phase $\\{w^{(1)}\\} = \\{10, 10\\}$.\n- Case $2$ (dynamic weights, integrality-friendly): $N = 3$, $P = 2$, durations $\\{D_1, D_2\\} = \\{60, 40\\}$ milliseconds, weights per phase $\\{w^{(1)}\\} = \\{1, 2, 3\\}$ and $\\{w^{(2)}\\} = \\{3, 1, 1\\}$.\n- Case $3$ (extreme imbalance, boundary coverage): $N = 4$, $P = 2$, durations $\\{D_1, D_2\\} = \\{50, 50\\}$ milliseconds, weights per phase $\\{w^{(1)}\\} = \\{1, 1, 50, 1\\}$ and $\\{w^{(2)}\\} = \\{1, 50, 1, 1\\}$.\n\nFinal output specification:\n- Your program must produce a single line containing a comma-separated list with square brackets enclosing the fairness gap $G$ for each case, in the order of Case $1$, Case $2$, Case $3$, rounded to $6$ decimal places. For example, the output format must be exactly of the form $[g_1,g_2,g_3]$ with each $g_k$ a decimal rounded to $6$ places.",
            "solution": "### 1. Derivation of the Virtual Runtime Update Law\n\nThe objective of a weighted fair scheduler is to allocate processing time to a set of competing threads $\\{1, \\dots, N\\}$ such that the portion of time received by each thread $i$ is proportional to its assigned weight, $w_i$. Over a given time interval, the ideal share of the processor for thread $i$ is given by $s_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j}$.\n\nThe scheduler employs a virtual runtime, $v_i$, for each thread. At any scheduling point, the thread with the minimum virtual runtime is chosen to execute. This mechanism ensures that threads that have run less in a \"normalized\" sense are prioritized, driving the system towards a state of fairness.\n\nTo establish the update law for $v_i$, we must connect the physical time a thread runs to the increase in its virtual runtime. Let thread $k$ be selected to run for a discrete time quantum of duration $q$. Its actual, physical runtime increases by $q$. For the system to be fair, the \"cost\" of this execution, represented by the increase in its virtual runtime $\\Delta v_k$, must be scaled according to its weight $w_k$.\n\nA thread with a higher weight $w_k$ is entitled to more processor time. Consequently, its virtual runtime should accumulate more slowly than that of a lower-weight thread for the same amount of physical execution time. This implies an inverse relationship between the rate of virtual runtime accumulation and the weight.\n\nThe most direct and fundamental formulation of this principle is to define the increase in virtual runtime as the physical time elapsed, scaled inversely by the thread's weight. Therefore, when thread $k$ executes for a quantum $q$, the update to its virtual runtime is:\n$$\n\\Delta v_k = \\frac{q}{w_k}\n$$\nThe new virtual runtime is then:\n$$\nv_k \\leftarrow v_k + \\frac{q}{w_k}\n$$\nThe virtual runtimes of all other non-executing threads $j \\neq k$ remain unchanged during this quantum.\n\nThis update law guarantees weighted fairness. For instance, consider two threads with weights $w_a$ and $w_b$. The rates at which their virtual runtimes increase are $\\frac{q}{w_a}$ and $\\frac{q}{w_b}$, respectively. For the scheduler to keep their virtual runtimes approximately equal over the long term (i.e., $v_a \\approx v_b$), the total physical time allocated, $A_a$ and $A_b$, must satisfy $\\frac{A_a}{w_a} \\approx \\frac{A_b}{w_b}$, which simplifies to $\\frac{A_a}{A_b} \\approx \\frac{w_a}{w_b}$. This is precisely the definition of weighted processor sharing.\n\n### 2. Simulation Algorithm\n\nThe simulation proceeds by modeling the discrete-time evolution of the system according to the rules specified.\n\n**Initialization:**\n1.  For a given workload with $N$ threads, initialize the virtual runtime $v_i$, actual cumulative time $A_i$, and ideal cumulative time $I_i$ for each thread $i \\in \\{1, \\dots, N\\}$ to $0$.\n    $$\n    v_i = 0, \\quad A_i = 0, \\quad I_i = 0 \\quad \\forall i \\in \\{1, \\dots, N\\}\n    $$\n2.  Initialize the total simulation time $T$ to $0$. The scheduling quantum is fixed at $q=1$ ms.\n\n**Execution Loop:**\nThe simulation progresses through a series of phases, $p \\in \\{1, \\dots, P\\}$. For each phase $p$:\n1.  Identify the phase duration $D_p$ and the constant weight vector $\\{w_i^{(p)}\\}_{i=1}^N$.\n2.  Add the phase duration to the total time: $T \\leftarrow T + D_p$.\n3.  Calculate the sum of weights for the current phase: $W_{sum}^{(p)} = \\sum_{j=1}^{N} w_j^{(p)}$.\n4.  Calculate the ideal share fractions for each thread $i$: $s_i^{(p)} = w_i^{(p)} / W_{sum}^{(p)}$.\n5.  Execute a loop for $D_p/q$ iterations, representing the discrete time quanta within the phase. In each iteration:\n    a. **Select Thread:** Find the thread index $k$ that minimizes the virtual runtime.\n       $$\n       k = \\arg\\min_{j \\in \\{1, \\dots, N\\}} \\{v_j\\}\n       $$\n       If multiple threads have the same minimum virtual runtime, the tie is broken by selecting the thread with the smallest index $j$.\n    b. **Update Actual Time:** Increment the actual time for the selected thread $k$.\n       $$\n       A_k \\leftarrow A_k + q\n       $$\n    c. **Update Virtual Runtime:** Update the virtual runtime for the selected thread $k$ using its current phase weight $w_k^{(p)}$.\n       $$\n       v_k \\leftarrow v_k + \\frac{q}{w_k^{(p)}}\n       $$\n    d. **Update Ideal Time:** For **all** threads $j \\in \\{1, \\dots, N\\}$, increment their ideal cumulative time based on their shares in the current phase.\n       $$\n       I_j \\leftarrow I_j + q \\cdot s_j^{(p)}\n       $$\n\n**Final Calculation:**\nAfter all phases have been simulated:\n1.  Compute the fairness gap, $G$, defined as the maximum normalized deviation between the actual and ideal cumulative times over all threads.\n    $$\n    G = \\max_{i \\in \\{1,\\dots,N\\}} \\frac{\\left|A_i - I_i\\right|}{T}\n    $$\nThe simulation is performed for each test case provided, and the resulting fairness gap $G$ is reported.\n\n```c\n#include stdio.h\n#include stdlib.h\n#include string.h\n#include math.h\n\n// Struct to hold parameters for a single test case.\ntypedef struct {\n    int n_threads;           // Number of threads (N)\n    int n_phases;            // Number of phases (P)\n    const int* durations;      // Array of phase durations (D_p)\n    const double* const* weights; // 2D array of weights (w_i^(p))\n} TestCase;\n\n// Simulates the scheduler for a given test case and returns the fairness gap.\ndouble simulate(const TestCase* tc) {\n    int n = tc-n_threads;\n    double q = 1.0; // Quantum size in milliseconds\n\n    // State variables for the simulation\n    double v[n]; // Virtual runtimes\n    double A[n]; // Actual cumulative processing times\n    double I[n]; // Ideal cumulative processing times\n    \n    // Initialize state variables\n    for (int i = 0; i  n; ++i) {\n        v[i] = 0.0;\n        A[i] = 0.0;\n        I[i] = 0.0;\n    }\n\n    double T = 0.0; // Total simulation time\n\n    // Loop through each phase\n    for (int p = 0; p  tc-n_phases; ++p) {\n        double D_p = (double)tc-durations[p];\n        const double* current_weights = tc-weights[p];\n        T += D_p;\n\n        // Calculate sum of weights and ideal shares for the current phase\n        double W_sum = 0.0;\n        for (int i = 0; i  n; ++i) {\n            W_sum += current_weights[i];\n        }\n\n        double s[n]; // Ideal share fractions\n        for (int i = 0; i  n; ++i) {\n            s[i] = current_weights[i] / W_sum;\n        }\n\n        // Run the simulation for the number of quanta in this phase\n        long num_quanta = (long)(D_p / q);\n        for (long k = 0; k  num_quanta; ++k) {\n            // 1. Select thread with minimum virtual runtime\n            int min_idx = 0;\n            double min_v = v[0];\n            for (int i = 1; i  n; ++i) {\n                if (v[i]  min_v) {\n                    min_v = v[i];\n                    min_idx = i;\n                }\n            }\n\n            // 2. Update actual cumulative time for the selected thread\n            A[min_idx] += q;\n\n            // 3. Update virtual runtime for the selected thread\n            v[min_idx] += q / current_weights[min_idx];\n\n            // 4. Update ideal cumulative time for ALL threads\n            for (int i = 0; i  n; ++i) {\n                I[i] += q * s[i];\n            }\n        }\n    }\n\n    // After all phases, compute the fairness gap G\n    double max_gap = 0.0;\n    for (int i = 0; i  n; ++i) {\n        double gap = fabs(A[i] - I[i]) / T;\n        if (gap  max_gap) {\n            max_gap = gap;\n        }\n    }\n\n    return max_gap;\n}\n\nint main(void) {\n    // --- Define Test Case 1 ---\n    const int case1_durations[] = {40};\n    const double case1_weights_p1[] = {10.0, 10.0};\n    const double* const case1_weights[] = {case1_weights_p1};\n\n    // --- Define Test Case 2 ---\n    const int case2_durations[] = {60, 40};\n    const double case2_weights_p1[] = {1.0, 2.0, 3.0};\n    const double case2_weights_p2[] = {3.0, 1.0, 1.0};\n    const double* const case2_weights[] = {case2_weights_p1, case2_weights_p2};\n\n    // --- Define Test Case 3 ---\n    const int case3_durations[] = {50, 50};\n    const double case3_weights_p1[] = {1.0, 1.0, 50.0, 1.0};\n    const double case3_weights_p2[] = {1.0, 50.0, 1.0, 1.0};\n    const double* const case3_weights[] = {case3_weights_p1, case3_weights_p2};\n\n    // Array of all test cases\n    TestCase test_cases[] = {\n        {2, 1, case1_durations, case1_weights},\n        {3, 2, case2_durations, case2_weights},\n        {4, 2, case3_durations, case3_weights},\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    double results[num_cases];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i  num_cases; ++i) {\n        results[i] = simulate(test_cases[i]);\n    }\n\n    // Print the results in the EXACT required format.\n    printf(\"[%.6f,%.6f,%.6f]\\n\", results[0], results[1], results[2]);\n\n    return EXIT_SUCCESS;\n}\n```",
            "answer": "$$\\boxed{\\text{[0.000000,0.000000,0.004434]}}$$"
        }
    ]
}