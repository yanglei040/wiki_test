{
    "hands_on_practices": [
        {
            "introduction": "Understanding swapping begins with a crucial question: at what point does memory overcommitment lead to a performance collapse, known as thrashing? This exercise provides a hands-on approach to quantifying this limit. By applying the foundational working set model to a high-performance computing workload, you will calculate the maximum number of concurrent jobs a system can support before its performance is crippled by excessive paging .",
            "id": "3685321",
            "problem": "A single High-Performance Computing (HPC) node runs an operating system that supports swapping and batch scheduling. The node has total physical Random Access Memory (RAM) of $M = 256$ GiB. A batch queue dispatches $n$ identical jobs. Each job has a steady-state memory working set of $W = 9$ GiB, measured over a fixed window $\\Delta t$ appropriate for the working set model. The operating system’s memory pressure controller maintains a reclaim headroom and begins sustained page reclamation when the aggregate resident demand exceeds a fraction $\\beta$ of the total physical memory, where $\\beta = 0.82$ and $0<\\beta<1$.\n\nUsing the canonical definition of the working set and thrashing, where thrashing is the regime in which sustained page reclamation forces working set pages to be repeatedly evicted and refetched. Formalize the condition for the onset of thrashing for this workload from first principles and derive the largest integer $n_{\\max}$ such that thrashing does not occur. Then evaluate $n_{\\max}$ numerically for the given $M$, $W$, and $\\beta$. Express your final answer as an integer count (unitless). No rounding beyond the integer maximum is required.",
            "solution": "The problem is first subjected to a validation process to ensure its scientific soundness, consistency, and well-posed nature.\n\n### Step 1: Extract Givens\n- Total physical Random Access Memory (RAM): $M = 256$ GiB.\n- Number of identical jobs: $n$.\n- Steady-state memory working set per job: $W = 9$ GiB.\n- Working set measurement window: $\\Delta t$.\n- Memory pressure threshold fraction: $\\beta = 0.82$.\n- Constraint on the threshold: $0 < \\beta < 1$.\n- Definition of thrashing: The regime in which sustained page reclamation forces working set pages to be repeatedly evicted and refetched.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the principles of operating system memory management, specifically the working set model proposed by Peter Denning. Thrashing is a well-defined phenomenon that occurs when the combined memory demands of active processes exceed the available physical memory, leading to excessive paging activity and a collapse in system throughput. The problem statement uses standard terminology and provides a clear, quantitative setup. The given values for memory size ($M = 256$ GiB), working set size ($W = 9$ GiB), and memory pressure threshold ($\\beta = 0.82$) are realistic for a High-Performance Computing (HPC) environment. The problem is self-contained, as the time window $\\Delta t$ is part of the definition of the given working set size $W$, but its specific value is not required for the calculation. The objective is to derive a maximum number of jobs $n_{\\max}$ based on these parameters, which is a well-posed question. The problem contains no scientific or factual unsoundness, no contradictions, and no ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A solution will be derived from first principles.\n\n### Derivation of the Thrashing Condition and Solution\n\nThe fundamental principle to prevent thrashing is to ensure that the aggregate working set of all concurrently running processes can be accommodated within the available physical memory. The working set of a process is the set of pages it has accessed over a recent time window $\\Delta t$. For a process to execute efficiently, its entire working set must remain resident in physical memory. If pages from the working set are evicted (swapped out), they will likely be needed again soon, leading to a page fault and subsequent I/O to fetch the page back from secondary storage. When this happens at a high rate for a significant number of processes, the system is said to be thrashing.\n\nThe problem states that there are $n$ identical jobs, each with a working set of size $W$. The total memory demand required to hold the working sets of all $n$ jobs in memory simultaneously is the aggregate working set size, which is given by:\n$$\n\\text{Total Working Set Demand} = n \\cdot W\n$$\n\nThe operating system monitors memory pressure and begins sustained page reclamation when the total resident memory demand exceeds a fraction $\\beta$ of the total physical memory $M$. This threshold, $\\beta \\cdot M$, represents the maximum amount of physical memory that the OS allows to be occupied by user processes before it takes aggressive measures (like swapping out pages that might be in active use) to free up memory. The remaining fraction, $(1-\\beta)M$, constitutes the \"reclaim headroom,\" reserved for the operating system kernel, file system caches, and to provide a buffer against sudden memory allocation spikes.\n\nTo avoid thrashing, the aggregate working set demand of all jobs must not exceed this operational memory capacity. If the total working set demand is greater than $\\beta \\cdot M$, the OS will be forced to reclaim pages that are part of one or more jobs' working sets. This action directly satisfies the problem's definition of thrashing: \"sustained page reclamation forces working set pages to be repeatedly evicted and refetched.\"\n\nTherefore, the condition to operate without thrashing is:\n$$\nn \\cdot W \\le \\beta \\cdot M\n$$\n\nThe problem asks for the largest integer $n_{\\max}$ for which this condition holds. We can solve for $n$ by rearranging the inequality:\n$$\nn \\le \\frac{\\beta \\cdot M}{W}\n$$\n\nSince $n$ must be an integer representing the number of jobs, the maximum value $n_{\\max}$ is the largest integer that satisfies this inequality. This is equivalent to taking the floor of the expression on the right-hand side:\n$$\nn_{\\max} = \\left\\lfloor \\frac{\\beta \\cdot M}{W} \\right\\rfloor\n$$\n\nNow, we substitute the numerical values provided in the problem statement to evaluate $n_{\\max}$:\n- $M = 256$ GiB\n- $W = 9$ GiB\n- $\\beta = 0.82$\n\nPlugging these values into the expression for $n_{\\max}$:\n$$\nn_{\\max} = \\left\\lfloor \\frac{0.82 \\cdot 256 \\text{ GiB}}{9 \\text{ GiB}} \\right\\rfloor\n$$\n\nThe units of GiB cancel, yielding a dimensionless quantity as expected for a count of jobs. We perform the calculation:\n$$\nn_{\\max} = \\left\\lfloor \\frac{209.92}{9} \\right\\rfloor\n$$\n$$\nn_{\\max} = \\left\\lfloor 23.32444... \\right\\rfloor\n$$\n\nThe floor of this value is $23$. Therefore, the largest integer number of jobs that can run concurrently without inducing thrashing is $23$. If $24$ jobs were to run, their aggregate working set demand ($24 \\cdot 9$ GiB $= 216$ GiB) would exceed the allowed memory threshold ($0.82 \\cdot 256$ GiB $\\approx 209.92$ GiB), triggering sustained page reclamation and, consequently, thrashing.",
            "answer": "$$\n\\boxed{23}\n$$"
        },
        {
            "introduction": "Once a system is under memory pressure, the operating system must decide *what* to evict. This decision is not trivial, as different types of memory pages have different costs associated with their eviction and retrieval. This practice problem delves into the design of a fair and efficient swapping policy, balancing the eviction pressure between file-backed pages and more costly anonymous pages . You will analyze a control-based approach to policy tuning, a technique reflecting real-world OS design, to achieve a balanced performance goal.",
            "id": "3685374",
            "problem": "An Operating System (OS) manages physical memory by evicting either file-backed pages (which can be dropped and re-read from the filesystem) or anonymous pages (which must be written to the swap area and later read back), a process known as swapping. To reason about fairness between these two classes under memory pressure, consider a controller that operates in discrete control windows and sets a scan-share parameter $\\theta \\in [0,1]$, interpreted as the fraction of eviction effort directed at anonymous pages, with the remaining $1-\\theta$ directed at file-backed pages. Assume that, within a given control window, the number of evictions from each class is approximately proportional to the scan share, an empirical observation widely used in paging systems. Furthermore, because swap-in/out involves higher Input/Output (I/O) costs for anonymous pages, we weight anonymous evictions higher than file-backed evictions in our fairness assessment.\n\nDefine the following for a single control window:\n- $E_{f}$ and $E_{a}$: evictions of file-backed and anonymous pages, respectively, measured in the window.\n- $R_{f}$ and $R_{a}$: resident file-backed and anonymous pages, respectively, at the start of the window.\n- $w_{f}$ and $w_{a}$: nonnegative weights capturing the per-eviction refill cost for file-backed and anonymous pages, respectively, with $w_{a} > w_{f}$ reflecting that anonymous pages are more expensive to reclaim and later refill.\n- A fairness metric $\\mathcal{F}$ defined as the ratio of the per-resident, cost-weighted eviction rates:\n$$\n\\mathcal{F} \\;=\\; \\frac{ \\dfrac{E_{f}}{w_{f} R_{f}} }{ \\dfrac{E_{a}}{w_{a} R_{a}} } \\, .\n$$\nThe fairness goal is $\\mathcal{F} \\approx 1$, meaning that, per resident page and after cost-weighting, each class bears comparable eviction pressure.\n\nYou are given the following measurements from the current window under $\\theta_{\\text{old}} = 0.3$: $E_{f} = 1200$, $E_{a} = 400$, $R_{f} = 6000$, $R_{a} = 4000$, $w_{f} = 1$, $w_{a} = 2$. For the next window, the total eviction demand $E_{\\text{tot}}$ is expected to remain approximately constant, and the controller’s scan-share setting $\\theta$ is assumed to determine expected evictions via $E_{a} \\approx \\theta \\, E_{\\text{tot}}$ and $E_{f} \\approx (1-\\theta)\\, E_{\\text{tot}}$.\n\nWhich update rule for $\\theta$ should the controller use for the next window to make $\\mathcal{F}$ approximately equal to $1$ under the stated proportional model?\n\nA. Set $\\theta_{\\text{new}} = \\min\\!\\big(1,\\max\\!\\big(0, \\theta_{\\text{old}} \\cdot \\sqrt{\\mathcal{F}}\\big)\\big)$.\n\nB. Set $\\theta_{\\text{new}} = \\min\\!\\big(1,\\max\\!\\big(0, \\theta_{\\text{old}} / \\sqrt{\\mathcal{F}}\\big)\\big)$.\n\nC. Set $\\theta_{\\text{new}} = \\min\\!\\Big(1,\\max\\!\\Big(0, \\dfrac{w_{a} R_{a}}{\\,w_{a} R_{a} + w_{f} R_{f}\\,}\\Big)\\Big)$.\n\nD. Keep $\\theta$ fixed: $\\theta_{\\text{new}} = \\theta_{\\text{old}}$.\n\nAnswer exactly one option.",
            "solution": "### Problem Validation\n\nFirst, I must validate the problem statement.\n\n**Step 1: Extract Givens**\n\n-   **Control Parameter**: $\\theta \\in [0,1]$, the fraction of eviction effort for anonymous pages.\n-   **System Model**:\n    -   Eviction effort for file-backed pages is $1-\\theta$.\n    -   The number of evictions is approximately proportional to the scan share.\n    -   For the next control window, this model is specified as:\n        -   $E_{a} \\approx \\theta \\, E_{\\text{tot}}$\n        -   $E_{f} \\approx (1-\\theta)\\, E_{\\text{tot}}$\n        where $E_{\\text{tot}}$ is the total number of evictions, assumed to be constant.\n-   **Definitions**:\n    -   $E_{f}, E_{a}$: Evictions of file-backed and anonymous pages.\n    -   $R_{f}, R_{a}$: Resident file-backed and anonymous pages at the start of a window.\n    -   $w_{f}, w_{a}$: Nonnegative weights for refill cost, with $w_{a} > w_{f}$.\n-   **Fairness Metric**: $\\mathcal{F} \\;=\\; \\frac{ \\frac{E_{f}}{w_{f} R_{f}} }{ \\frac{E_{a}}{w_{a} R_{a}} }$.\n-   **Goal**: To set $\\theta_{\\text{new}}$ such that the fairness metric for the next window, $\\mathcal{F}_{\\text{new}}$, is approximately $1$.\n-   **Data from Current (Old) Window**:\n    -   $\\theta_{\\text{old}} = 0.3$\n    -   $E_{f} = 1200$\n    -   $E_{a} = 400$\n    -   $R_{f} = 6000$\n    -   $R_{a} = 4000$\n    -   $w_{f} = 1$\n    -   $w_{a} = 2$\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding**: The problem is well-grounded in the principles of operating system design, specifically memory management and paging policies. The concept of balancing eviction pressure between different types of memory (anonymous vs. file-backed) using a cost-based or proportional controller is a real-world technique (e.g., as seen in the Linux kernel). The model is a simplified but valid representation for such a control problem.\n-   **Well-Posedness**: The problem is well-posed. It provides all necessary variables, a clear mathematical model for the controller's effect, and a specific objective ($\\mathcal{F}_{\\text{new}} \\approx 1$). It is possible to derive a unique control law from these components.\n-   **Objectivity**: The problem is stated in objective, formal language.\n-   **Consistency Check**: We can check the consistency of the provided data for the old window with the proportional model.\n    -   Total evictions: $E_{\\text{tot}} = E_f + E_a = 1200 + 400 = 1600$.\n    -   Model prediction for $E_a$: $\\theta_{\\text{old}} E_{\\text{tot}} = 0.3 \\times 1600 = 480$. This differs from the measured $E_a = 400$.\n    -   Model prediction for $E_f$: $(1-\\theta_{\\text{old}}) E_{\\text{tot}} = 0.7 \\times 1600 = 1120$. This differs from the measured $E_f = 1200$.\n    The problem statement explicitly notes that the proportionality is an *approximation* (\"approximately proportional\", \"expected evictions ... via $E_a \\approx \\theta E_{tot}$\"). The discrepancy between the model and the past data reflects the fact that this is a simplified model of a complex system, which is a realistic scenario in control engineering. The task is to use the *stated model* to determine the *next* control action, not to validate the model's past performance. Thus, the problem is not contradictory but rather presents a realistic modeling challenge.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. It is a standard, albeit simplified, control theory problem applied to a relevant topic in operating systems. I will now proceed to the solution.\n\n### Solution Derivation\n\nThe objective is to find an update rule for $\\theta$ that will make the fairness metric $\\mathcal{F}$ approximately equal to $1$ in the next control window. Let the new value of $\\theta$ be $\\theta_{\\text{new}}$.\n\nThe target is $\\mathcal{F}_{\\text{new}} \\approx 1$. For the sake of derivation, we will aim for the exact value $\\mathcal{F}_{\\text{new}} = 1$.\nThe definition of the fairness metric is:\n$$ \\mathcal{F}_{\\text{new}} = \\frac{ \\dfrac{E_{f, \\text{new}}}{w_{f} R_{f, \\text{new}}} }{ \\dfrac{E_{a, \\text{new}}}{w_{a} R_{a, \\text{new}}} } $$\nSetting $\\mathcal{F}_{\\text{new}} = 1$ gives the condition for perfect fairness:\n$$ \\frac{E_{f, \\text{new}}}{w_{f} R_{f, \\text{new}}} = \\frac{E_{a, \\text{new}}}{w_{a} R_{a, \\text{new}}} $$\n\nThe problem states we must use the proportional model to determine the expected evictions in the next window:\n-   $E_{a, \\text{new}} = \\theta_{\\text{new}} E_{\\text{tot}}$\n-   $E_{f, \\text{new}} = (1-\\theta_{\\text{new}}) E_{\\text{tot}}$\n\nHere, $E_{\\text{tot}}$ is the total eviction demand, which is assumed to be constant. $R_{f, \\text{new}}$ and $R_{a, \\text{new}}$ are the resident page counts at the start of the next window.\n\nSubstituting the model into the fairness condition:\n$$ \\frac{(1-\\theta_{\\text{new}}) E_{\\text{tot}}}{w_{f} R_{f, \\text{new}}} = \\frac{\\theta_{\\text{new}} E_{\\text{tot}}}{w_{a} R_{a, \\text{new}}} $$\nSince $E_{\\text{tot}}$ is a non-zero constant, it cancels from both sides:\n$$ \\frac{1-\\theta_{\\text{new}}}{w_{f} R_{f, \\text{new}}} = \\frac{\\theta_{\\text{new}}}{w_{a} R_{a, \\text{new}}} $$\nWe now solve for $\\theta_{\\text{new}}$:\n$$ (1-\\theta_{\\text{new}}) w_{a} R_{a, \\text{new}} = \\theta_{\\text{new}} w_{f} R_{f, \\text{new}} $$\n$$ w_{a} R_{a, \\text{new}} - \\theta_{\\text{new}} w_{a} R_{a, \\text{new}} = \\theta_{\\text{new}} w_{f} R_{f, \\text{new}} $$\n$$ w_{a} R_{a, \\text{new}} = \\theta_{\\text{new}} (w_{f} R_{f, \\text{new}} + w_{a} R_{a, \\text{new}}) $$\n$$ \\theta_{\\text{new}} = \\frac{w_{a} R_{a, \\text{new}}}{w_{f} R_{f, \\text{new}} + w_{a} R_{a, \\text{new}}} $$\n\nThis equation represents the fundamental principle for setting $\\theta$ to achieve the fairness goal. The controller should set $\\theta$ to the ratio of the cost-weighted resident anonymous pages to the total cost-weighted resident pages.\n\n### Option-by-Option Analysis\n\nNow I will evaluate each option. An update rule should ideally be based on the principle derived above.\n\n**C. Set $\\theta_{\\text{new}} = \\min\\!\\Big(1,\\max\\!\\Big(0, \\dfrac{w_{a} R_{a}}{\\,w_{a} R_{a} + w_{f} R_{f}\\,}\\Big)\\Big)$.**\n\nThis option's formula, $\\dfrac{w_{a} R_{a}}{w_{a} R_{a} + w_{f} R_{f}}$, is functionally identical to the one I derived. The variables $R_a$ and $R_f$ in the formula should be interpreted as the values at the start of the window for which $\\theta$ is being set. The use of $\\min(1, \\max(0, ...))$ is standard practice in control systems to ensure the output remains within the valid range $[0,1]$, which is a measure of robustness. This rule is a direct implementation of the derived principle for achieving $\\mathcal{F}=1$.\n\nTherefore, this option represents the correct, principled update rule. The controller should measure or estimate the upcoming $R_f$ and $R_a$ and apply this formula.\n\n**Verdict: Correct**\n\n**D. Keep $\\theta$ fixed: $\\theta_{\\text{new}} = \\theta_{\\text{old}}$.**\n\nTo check if this is correct, we must calculate the fairness metric for the old window, $\\mathcal{F}_{\\text{old}}$.\n$$ \\mathcal{F}_{\\text{old}} = \\frac{ \\dfrac{E_{f}}{w_{f} R_{f}} }{ \\dfrac{E_{a}}{w_{a} R_{a}} } = \\frac{ \\dfrac{1200}{1 \\cdot 6000} }{ \\dfrac{400}{2 \\cdot 4000} } = \\frac{ \\dfrac{1200}{6000} }{ \\dfrac{400}{8000} } = \\frac{0.2}{0.05} = 4 $$\nSince $\\mathcal{F}_{\\text{old}} = 4 \\neq 1$, the system is not in a fair state. The goal is to make $\\mathcal{F}_{\\text{new}} \\approx 1$. Keeping $\\theta$ fixed would not actively correct the imbalance. Therefore, this option is incorrect.\n\n**Verdict: Incorrect**\n\n**B. Set $\\theta_{\\text{new}} = \\min\\!\\big(1,\\max\\!\\big(0, \\theta_{\\text{old}} / \\sqrt{\\mathcal{F}}\\big)\\big)$.**\n\nLet's analyze the direction of this control law. We have $\\mathcal{F}_{\\text{old}} = 4$, which is greater than $1$. This indicates that the cost-weighted eviction pressure is too high on file-backed pages compared to anonymous pages. To correct this, the controller must increase the pressure on anonymous pages, which means increasing $E_a$. According to the model $E_a \\approx \\theta E_{\\text{tot}}$, this requires increasing $\\theta$.\nThis rule proposes $\\theta_{\\text{new}} = \\theta_{\\text{old}} / \\sqrt{\\mathcal{F}_{\\text{old}}}$. With $\\theta_{\\text{old}}=0.3$ and $\\mathcal{F}_{\\text{old}}=4$, we get:\n$$ \\theta_{\\text{new}} = 0.3 / \\sqrt{4} = 0.3 / 2 = 0.15 $$\nThis rule *decreases* $\\theta$ when it should be increased. The control action is directionally incorrect.\n\n**Verdict: Incorrect**\n\n**A. Set $\\theta_{\\text{new}} = \\min\\!\\big(1,\\max\\!\\big(0, \\theta_{\\text{old}} \\cdot \\sqrt{\\mathcal{F}}\\big)\\big)$.**\n\nThis rule proposes $\\theta_{\\text{new}} = \\theta_{\\text{old}} \\cdot \\sqrt{\\mathcal{F}_{\\text{old}}}$. With the given data, this yields:\n$$ \\theta_{\\text{new}} = 0.3 \\cdot \\sqrt{4} = 0.3 \\cdot 2 = 0.6 $$\nThis is a feedback control law that is directionally correct (it increases $\\theta$ when $\\mathcal{F}>1$). However, it is a heuristic. It is not derived from the system model and the fairness goal. While such multiplicative update rules can be effective, they are not guaranteed to achieve the objective in one step. The rule in Option C is derived directly to be the one-step solution under the given model. The question asks for the rule the controller *should* use to make $\\mathcal{F} \\approx 1$. The rule that is analytically derived to do exactly that (Option C) is the most appropriate and principled choice over an unsubstantiated heuristic (Option A), even if that heuristic produces a plausible numerical result for the specific data provided. The core of a scientific or engineering problem is to identify the underlying principle, which is what Option C represents.\n\n**Verdict: Incorrect**\n\nThe most rigorous and principled answer is the one derived directly from the problem's model and objective, which is Option C. It describes the fundamental relationship between the system state ($R_f, R_a, w_f, w_a$) and the control input ($\\theta$) required to achieve the desired fairness output ($\\mathcal{F}=1$).",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Ultimately, the goal of memory management is to provide good performance for applications and a smooth experience for users. This final exercise applies the principles of swapping to a modern and relatable scenario: managing memory for a multi-tab web browser . You will frame the task of selecting which tabs to swap out as an optimization problem, aiming to minimize the user-perceived latency for interactive tabs while staying within the system's physical memory constraints.",
            "id": "3685404",
            "problem": "A modern web browser runs $m$ tabs, each represented by a workload $W_i$ with a working set size $s_i$ measured in megabytes. The operating system uses swapping: when a tab is swapped out, its working set is removed from physical memory and later read back from secondary storage on access. For any interactive tab, the user-perceived access latency when resuming a swapped-out tab is modeled as $$t_i=\\delta+\\frac{s_i}{B}\\times 10^3$$, where $\\delta$ is a fixed setup overhead in milliseconds and $B$ is the sustained sequential read bandwidth in megabytes per second of the storage device. Assume swapping is done at tab granularity, so either all of $s_i$ is resident or none of it is resident.\n\nThe system has an unavoidable resident operating system overhead $S_{\\text{sys}}$ and a physical memory capacity $M$, both measured in megabytes. Let $\\mathcal{I}$ denote the set of indices of interactive tabs and let $\\mathcal{N}$ denote the set of indices of non-interactive tabs. A selection of tabs to swap out is feasible if the sum of the working set sizes of resident tabs plus $S_{\\text{sys}}$ does not exceed $M$, and any interactive tab that is swapped out has latency strictly less than a bound $\\lambda$.\n\nYou are given the following concrete workload and system parameters:\n- Number of tabs $m=7$.\n- Interactive tabs: $\\mathcal{I}=\\{1,2,3\\}$ with working set sizes $s_1=620$ MB, $s_2=420$ MB, $s_3=260$ MB.\n- Non-interactive tabs: $\\mathcal{N}=\\{4,5,6,7\\}$ with working set sizes $s_4=550$ MB, $s_5=480$ MB, $s_6=320$ MB, $s_7=190$ MB.\n- Operating system overhead $S_{\\text{sys}}=600$ MB.\n- Physical memory capacity $M=1850$ MB.\n- Storage parameters: fixed overhead $\\delta=10$ milliseconds and sustained bandwidth $B=200$ MB/s.\n\nStarting from first principles about memory capacity and swapping latency, frame this as a knapsack-like selection problem over the set of tabs and derive the selection that minimizes the worst-case interactive latency bound $\\lambda$ while satisfying feasibility. Determine the minimal value of $\\lambda$ (in milliseconds) for which there exists a feasible selection. Round your final numerical answer to four significant figures and express it in milliseconds.",
            "solution": "The problem asks for the minimal worst-case interactive latency bound, $\\lambda$, for which a feasible selection of tabs to swap exists. A selection is feasible if it satisfies both a memory capacity constraint and a latency constraint. We will first formalize these constraints and the optimization objective, and then systematically determine the minimal $\\lambda$.\n\nLet the set of all tabs be $\\mathcal{T} = \\{1, 2, 3, 4, 5, 6, 7\\}$. The set of interactive tabs is $\\mathcal{I}=\\{1,2,3\\}$ and the set of non-interactive tabs is $\\mathcal{N}=\\{4,5,6,7\\}$. Let $s_i$ be the working set size of tab $i$.\nWe define a binary decision variable $x_i$ for each tab $i \\in \\mathcal{T}$:\n$x_i = 1$ if tab $i$ is kept resident in physical memory.\n$x_i = 0$ if tab $i$ is swapped out to secondary storage.\n\nThe feasibility of a selection, represented by the set of values $\\{x_i\\}_{i=1}^7$, is determined by two conditions:\n\n1.  **Memory Constraint**: The total size of all resident tabs plus the operating system overhead, $S_{\\text{sys}}$, must not exceed the physical memory capacity, $M$.\n    $$ \\sum_{i=1}^{7} x_i s_i + S_{\\text{sys}} \\le M $$\n    Substituting the given values: $M=1850$ MB and $S_{\\text{sys}}=600$ MB.\n    $$ \\sum_{i=1}^{7} x_i s_i + 600 \\le 1850 $$\n    $$ \\sum_{i=1}^{7} x_i s_i \\le 1250 $$\n    This inequality states that the sum of working set sizes of all tabs kept in memory must not exceed $1250$ MB.\n    An alternative way to express this constraint is in terms of the swapped-out tabs. The total working set size of all tabs is:\n    $$ \\sum_{i=1}^{7} s_i = (620+420+260) + (550+480+320+190) = 1300 + 1540 = 2840 \\text{ MB} $$\n    Let $\\mathcal{S}$ be the set of indices of swapped-out tabs, i.e., $\\mathcal{S} = \\{i \\mid x_i=0\\}$. The sum of resident sizes is $\\sum_{i \\notin \\mathcal{S}} s_i = (\\sum_{i=1}^7 s_i) - \\sum_{i \\in \\mathcal{S}} s_i$. The memory constraint becomes:\n    $$ \\left(\\sum_{i=1}^{7} s_i\\right) - \\sum_{i \\in \\mathcal{S}} s_i \\le M - S_{\\text{sys}} $$\n    $$ 2840 - \\sum_{i \\in \\mathcal{S}} s_i \\le 1250 $$\n    $$ \\sum_{i \\in \\mathcal{S}} s_i \\ge 2840 - 1250 = 1590 $$\n    Therefore, to satisfy the memory constraint, the total size of swapped-out tabs must be at least $1590$ MB.\n\n2.  **Latency Constraint**: For any interactive tab $i \\in \\mathcal{I}$ that is swapped out ($x_i=0$), its access latency $t_i$ must be strictly less than the bound $\\lambda$.\n    $$ t_i < \\lambda \\quad \\forall i \\in \\mathcal{I} \\text{ such that } x_i=0 $$\n    This is equivalent to requiring $\\lambda > \\max \\{t_i \\mid i \\in \\mathcal{I} \\cap \\mathcal{S}\\}$. If no interactive tabs are swapped ($\\mathcal{I} \\cap \\mathcal{S} = \\emptyset$), this condition is satisfied for any $\\lambda > 0$. The latency $t_i$ is given by $t_i=\\delta+\\frac{s_i}{B}\\times 10^3$. With $\\delta=10$ ms and $B=200$ MB/s, the latencies for the interactive tabs are:\n    $t_1 = 10 + \\frac{620}{200} \\times 10^3 = 10 + 3.1 \\times 10^3 = 3110$ ms.\n    $t_2 = 10 + \\frac{420}{200} \\times 10^3 = 10 + 2.1 \\times 10^3 = 2110$ ms.\n    $t_3 = 10 + \\frac{260}{200} \\times 10^3 = 10 + 1.3 \\times 10^3 = 1310$ ms.\n    Note that $s_1 > s_2 > s_3$ implies $t_1 > t_2 > t_3$.\n\nThe objective is to find the minimal value of $\\lambda$ for which a feasible selection $\\{x_i\\}$ exists. This is a knapsack-like problem where we must select a set of tabs $\\mathcal{S}$ to swap out to satisfy a minimum total size constraint ($\\ge 1590$ MB), while minimizing the cost function $L(\\mathcal{S}) = \\max (\\{t_i \\mid i \\in \\mathcal{I} \\cap \\mathcal{S}\\} \\cup \\{0\\})$. The minimal $\\lambda$ will be the infimum of the set of allowed $\\lambda$ values, which corresponds to this minimal possible cost.\n\nWe can solve this by considering scenarios in increasing order of latency cost.\n\n**Scenario 1: No interactive tabs are swapped.**\nIn this case, $\\mathcal{S} \\subseteq \\mathcal{N}$. The maximum memory that can be freed is the sum of the sizes of all non-interactive tabs:\n$$ \\sum_{i \\in \\mathcal{N}} s_i = s_4+s_5+s_6+s_7 = 550+480+320+190 = 1540 \\text{ MB} $$\nThe required swap size is $1590$ MB. Since $1540 < 1590$, it is impossible to satisfy the memory constraint by swapping only non-interactive tabs. Therefore, at least one interactive tab must be swapped out. This implies that the minimal worst-case latency will be greater than $0$.\n\n**Scenario 2: The interactive tab with the lowest latency, tab 3, is swapped.**\nThis scenario explores if a feasible solution exists where the maximum latency is determined by $t_3$. To achieve a minimal latency bound, we must avoid swapping tabs $1$ and $2$ if possible. Let's test the feasibility of a selection where the set of swapped interactive tabs is exactly $\\{3\\}$. The maximum latency incurred would be $t_3 = 1310$ ms.\nFor this to be a valid configuration, we must be able to satisfy the memory constraint.\nThe size freed by swapping tab $3$ is $s_3 = 260$ MB.\nThe remaining size to be freed by swapping non-interactive tabs is $1590 - 260 = 1330$ MB.\nThe total size of all non-interactive tabs is $1540$ MB. Since $1540 \\ge 1330$, it is possible to select a subset of non-interactive tabs to swap whose sizes sum to at least $1330$ MB. For instance, we can swap all non-interactive tabs $\\{4, 5, 6, 7\\}$, which provides $1540$ MB of freed memory.\nA feasible selection is thus $\\mathcal{S}=\\{3, 4, 5, 6, 7\\}$. The total swapped size is $s_3+s_4+s_5+s_6+s_7 = 260+1540 = 1800$ MB, which is $\\ge 1590$ MB.\nFor this selection, the set of swapped interactive tabs is $\\{3\\}$, and the maximum latency is $t_3 = 1310$ ms. The latency constraint requires $\\lambda > 1310$ ms.\nThis confirms that a feasible configuration exists for any $\\lambda > 1310$.\n\n**Conclusion on Minimal $\\lambda$**\nWe have established two key points:\n1.  It is necessary to swap at least one interactive tab. This means the minimal latency cost must be at least the smallest latency of any interactive tab, which is $t_3 = 1310$ ms.\n2.  A feasible configuration exists where the maximum latency among swapped interactive tabs is exactly $t_3 = 1310$ ms.\n\nAny other scenario, such as swapping tab $2$ (latency $t_2=2110$ ms) or tab $1$ (latency $t_1=3110$ ms), would result in a higher maximum latency and thus require a larger $\\lambda$. Therefore, the minimum possible value for the worst-case interactive latency is $t_3$.\n\nThe minimal value of $\\lambda$ is the greatest lower bound (infimum) of the set of all possible valid bounds. Since a feasible selection exists with a maximum interactive latency of $1310$ ms, any $\\lambda > 1310$ ms is a valid bound. The infimum of the set $(\\text{1310}, \\infty)$ is $1310$.\n\nThe final numerical answer is $1310$ ms. The problem requires rounding to four significant figures. As the calculated value is an exact integer, $1310$, this can be represented as $1.310 \\times 10^3$, which has four significant figures.",
            "answer": "$$\\boxed{1310}$$"
        }
    ]
}