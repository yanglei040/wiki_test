{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental task of a page replacement algorithm is to keep an application's \"working set\"—the set of pages it actively needs—in memory. For workloads with predictable, repeating access patterns, we can precisely determine the memory size required to achieve optimal performance. This first practice challenges you to analyze a cyclic reference string and discover the minimum number of frames needed to eliminate page faults once the system has warmed up, linking memory capacity directly to the application's memory footprint .",
            "id": "3629752",
            "problem": "Consider the counting-based Least Frequently Used (LFU) page replacement policy with deterministic tie-breaking by Least Recently Used (LRU). In LFU, each page $p_i$ maintains a counter $c_i$ that equals the number of references to $p_i$ since the start of execution. On a page fault when memory is full, the algorithm evicts the resident page with the smallest $c_i$; if multiple pages tie for the smallest $c_i$, the Least Recently Used page among those tied is evicted. Assume no counter aging or decay.\n\nSuppose the reference string is cyclic with cycle length $L$, repeating indefinitely. Let the set of distinct pages referenced in the cycle be $\\mathcal{P} = \\{p_1, p_2, \\dots, p_K\\}$ with $K \\leq L$. Within each cycle, page $p_i$ appears exactly $f_i$ times, with $f_i \\in \\mathbb{N}$ and $\\sum_{i=1}^{K} f_i = L$. Memory has $N$ frames, initially empty, and counters satisfy $c_i = 0$ for all $i$ at time $t=0$.\n\nUsing only the fundamental definitions above, reason about steady-state behavior on cyclic sequences and determine the condition under which $N < L$ leads to periodic page faults (misses) under LFU. Then, for the concrete distribution\n$$\nL = 24,\\quad K = 8,\\quad f = (f_1, f_2, f_3, f_4, f_5, f_6, f_7, f_8) = (7, 5, 4, 3, 2, 1, 1, 1),\n$$\ncompute the minimal number of frames $N$ that eliminates cycle-induced page fault periodicity in the sense that, after a finite warm-up, every reference in each cycle is a hit (i.e., no misses occur within any cycle thereafter). Express your final answer as a single integer with no units. No rounding is required.",
            "solution": "The problem asks for the minimal number of memory frames, $N$, required to ensure that for a given cyclic reference string, a steady state is reached where no page faults occur. The page replacement algorithm is Least Frequently Used (LFU), with Least Recently Used (LRU) as a tie-breaker.\n\nLet $\\mathcal{P} = \\{p_1, p_2, \\dots, p_K\\}$ be the set of $K$ distinct pages that appear in the reference cycle. The problem states that the reference string is cyclic and repeats indefinitely, implying that all references are to pages within this set $\\mathcal{P}$. The frequency of reference for each page $p_i$ within a single cycle is $f_i \\in \\mathbb{N}$, meaning $f_i \\ge 1$. This ensures every page in $\\mathcal{P}$ is referenced in every cycle.\n\nThe system's behavior can be divided into an initial transient (warm-up) phase and a subsequent steady-state phase. The goal is to find the minimal $N$ such that in the steady state, every reference is a hit. This is equivalent to stating that the set of resident pages, $\\mathcal{M}$, must contain the entire set of pages in the reference cycle, $\\mathcal{P}$. That is, the condition for zero steady-state misses is:\n$$\n\\mathcal{P} \\subseteq \\mathcal{M}\n$$\n\nThe number of frames is $N$, which imposes a constraint on the size of the set of resident pages: $|\\mathcal{M}| \\le N$. For the condition $\\mathcal{P} \\subseteq \\mathcal{M}$ to be met, the number of frames must be at least as large as the number of distinct pages in the cycle, which is $|\\mathcal{P}| = K$. This establishes a necessary condition:\n$$\nN \\ge K\n$$\n\nWe now demonstrate that this condition is also sufficient. Assume $N \\ge K$. The system begins with empty memory frames at time $t=0$. As the reference string is processed, each page $p_i \\in \\mathcal{P}$ will eventually be referenced. The first time a page is referenced, it causes a compulsory miss (page fault), and the page is loaded into an empty frame, or if memory is full, by evicting another page. Since there are $K$ distinct pages, there will be an initial warm-up period involving at most $K$ faults to bring all pages from $\\mathcal{P}$ into memory. Because we assumed $N \\ge K$, there are sufficient frames to hold all $K$ pages simultaneously.\n\nOnce this warm-up phase is complete, all $K$ pages of the set $\\mathcal{P}$ reside in memory. Now, consider any subsequent reference in the cycle. The reference must be to some page $p_j \\in \\mathcal{P}$. Since all pages in $\\mathcal{P}$ are now resident in memory, this reference to $p_j$ will be a hit. When a hit occurs, no page fault is generated, and therefore no page eviction takes place. The set of resident pages remains unchanged, continuing to contain all of $\\mathcal{P}$. This will be true for all subsequent references, as they are all drawn from $\\mathcal{P}$. Consequently, the page fault rate drops to zero and stays there. This state corresponds to the desired outcome of eliminating cycle-induced page fault periodicity.\n\nConversely, if $N < K$, it is physically impossible to store all $K$ distinct pages in the $N$ available frames at the same time. Since every page $p_i \\in \\mathcal{P}$ is guaranteed to be referenced in each cycle (because $f_i \\ge 1$), it is inevitable that a reference will be made to a page that is not currently in memory. This will cause a page fault. The LFU algorithm will then evict one of the $N$ resident pages to make room for the new page. As the reference string is periodic, this process of faulting and evicting will continue in a periodic pattern in the steady state. Thus, the condition for periodic page faults to occur is $N < K$.\n\nTo eliminate these periodic faults, we must choose the smallest $N$ that violates this condition. The minimal integer $N$ satisfying $N \\ge K$ is precisely $N = K$.\n$$\nN_{\\text{min}} = K\n$$\nThis conclusion depends only on the number of distinct pages $K$ and not on the specific frequencies $f_i$, the cycle length $L$, or the tie-breaking rule, as these details only govern the dynamics of eviction when $N < K$.\n\nFor the specific instance provided in the problem:\nThe set of distinct pages is given as $\\mathcal{P} = \\{p_1, p_2, \\dots, p_8\\}$, so the number of distinct pages is $K=8$. The frequency vector is given as $f = (7, 5, 4, 3, 2, 1, 1, 1)$, which confirms there are $K=8$ pages. The cycle length is $L=24$, and we can verify $\\sum f_i = 7+5+4+3+2+1+1+1 = 24 = L$.\n\nUsing our derived general result, the minimal number of frames required to eliminate steady-state page faults is:\n$$\nN_{\\text{min}} = K = 8\n$$\nWith $N=8$ frames, after an initial transient period of at most $8$ faults, all $8$ pages will be resident in memory, and no further faults will occur.",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "A key weakness of a pure Least Frequently Used (LFU) policy is its inertia; pages that were popular long ago can retain high frequency counts, preventing newly important pages from entering the cache. This phenomenon is often called the \"cold-start bias\" and can lead to thrashing when access patterns shift. This practice models a scenario with bursty traffic for a new page and asks you to analyze how exponential aging, a common enhancement to LFU, helps the algorithm adapt and recognize the new \"hot\" page .",
            "id": "3629731",
            "problem": "Consider a memory system with $N$ page frames and a universe of $P$ distinct virtual pages, with $P \\geq N+1$. The operating system uses a counting-based page replacement policy: Least Frequently Used (LFU). To mitigate the well-known cold-start bias of LFU, the implementation applies exponential aging to the access counts. Specifically, each page $i$ maintains an aged count $c_i(t)$ that obeys $ \\frac{d c_i(t)}{dt} = -\\lambda c_i(t)$ between accesses, and on an access to page $i$ at time $t$ the counter updates as $c_i(t^{+}) = c_i(t^{-}) + 1$, where $\\lambda > 0$ is a fixed decay rate. On a page fault at time $t$, the policy evicts the resident page with the smallest current $c_i(t^{-})$ (break ties arbitrarily).\n\nYou will design and analyze a parametric reference string that exposes LFU’s cold-start bias under bursty accesses, and then determine a quantitative condition on the burst length that alleviates the bias under exponential aging.\n\nConstruct the following workload. There is a fixed set $\\mathcal{H} = \\{H_1,\\dots,H_{N-1}\\}$ of $N-1$ “incumbent hot” pages whose accesses, prior to time $t=0$, form independent stationary Poisson processes of common rate $\\mu$ accesses per unit time, so that just before $t=0$ the system has reached steady state and all pages in $\\mathcal{H}$ are resident. At time $t=0$, a new page $X$ begins to be referenced in repeated cycles. Each cycle consists of:\n- a burst of $b$ consecutive references to $X$ with constant inter-arrival spacing $\\tau_b$, immediately back-to-back so that the burst duration is $b\\,\\tau_b$, followed by\n- a single reference to a “disturber” page $Y$ that is not among $\\mathcal{H}$ and is not resident prior to its reference, thereby forcing a replacement on the $Y$ reference.\n\nAssume $P$ is large enough to ensure that $Y$ is a page not resident at its reference in every cycle. Also assume time-scale separation during the burst so that aging is negligible within a burst, i.e., $\\lambda\\,b\\,\\tau_b \\ll 1$, and that accesses to pages in $\\mathcal{H}$ maintain their steady-state behavior so their aged counts can be treated by their steady-state expectations.\n\nTask:\n- Using only the core definitions above and standard stochastic facts about Poisson processes, first justify that with non-aged LFU (i.e., the limiting case $\\lambda \\to 0^{+}$), for any fixed finite $b$, page $X$ will be the eviction victim on the subsequent reference to $Y$ at the end of each cycle once the system has warmed up, which leads to repeated page faults on $X$ across cycles (operational thrashing for $X$).\n- For exponential aging with fixed $\\lambda > 0$, derive an explicit closed-form expression for the smallest integer burst length $b_{\\min}$ (as a function of $\\mu$ and $\\lambda$) such that immediately after an $X$-burst and just before the following $Y$ reference, $X$ will not be the eviction victim. Equivalently, $b_{\\min}$ is the smallest integer $b$ that ensures the aged count of $X$ after the burst exceeds that of any incumbent hot page in $\\mathcal{H}$ in steady state, thereby alleviating the LFU cold-start bias for $X$ under this bursty pattern.\n\nYour final answer must be a single analytic expression for $b_{\\min}$ in terms of $\\mu$ and $\\lambda$. No numerical evaluation is required, and no units are to be included in the final expression.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in the domain of operating system performance analysis, using standard models from stochastic processes. The assumptions provided are explicit and sufficient to derive a unique solution.\n\nThe problem asks for two parts: first, a justification of thrashing for a new page under non-aged Least Frequently Used (LFU) policy, and second, the derivation of a minimum burst size $b_{\\min}$ to prevent this thrashing under LFU with exponential aging.\n\n### Part 1: Analysis for Non-Aged LFU ($\\lambda \\to 0^{+}$)\n\nIn the limiting case where the decay rate $\\lambda \\to 0^{+}$, the aging term in the count update equation, $\\frac{d c_i(t)}{dt} = -\\lambda c_i(t)$, becomes zero. This means the access counts $c_i(t)$ are non-decreasing; they only increment upon an access and never decay. This is the pure LFU algorithm.\n\nThe problem states that prior to time $t=0$, the system has reached a steady state where the $N-1$ incumbent hot pages in the set $\\mathcal{H}$ are resident. In a pure LFU system, \"steady state\" implies these pages have been accessed for a very long time. Their access counts, $c_{H_j}$ for $H_j \\in \\mathcal{H}$, will have accumulated over this long history and will be extremely large values, proportional to the product of their access rate $\\mu$ and the time elapsed.\n\nAt time $t=0$, a new page $X$ is introduced. Its initial count is $c_X = 0$. In the first cycle, it is accessed $b$ times. Since counts do not decay, its count after the burst becomes $c_X = b$.\n\nFollowing the burst, a reference to a non-resident page $Y$ triggers a page fault and forces the operating system to choose a victim page for eviction from the set of resident pages, which is $\\mathcal{H} \\cup \\{X\\}$. The LFU policy dictates evicting the page with the lowest access count.\n\nThe counts to be compared are:\n- The count of page $X$: $c_X = b$.\n- The counts of the incumbent hot pages: $c_{H_j}$ for $H_j \\in \\mathcal{H}$.\n\nFor any finite burst length $b$, the count of page $X$ will be minuscule compared to the very large, historically accumulated counts of the incumbent pages in $\\mathcal{H}$. That is, $c_X \\ll c_{H_j}$ for all $j \\in \\{1, \\dots, N-1\\}$. Consequently, page $X$ will be selected as the eviction victim.\n\nThis process repeats in every cycle. When the next cycle starts, the first reference to page $X$ causes a page fault. Page $X$ is brought into memory, its count increments during the burst to become $b$, and it is immediately evicted again upon the reference to page $Y$. This phenomenon, where a page is continuously faulted in and out of memory without making progress, is known as thrashing. The cold-start bias of pure LFU—its inability to recognize a newly popular page because its count starts from zero against incumbents with long histories—is perfectly exposed by this workload.\n\n### Part 2: Derivation of $b_{\\min}$ for LFU with Exponential Aging ($\\lambda > 0$)\n\nWith a non-zero decay rate $\\lambda > 0$, the counts of all pages continuously decay. This \"forgetting\" mechanism prevents the counts of old pages from growing indefinitely, allowing new pages a chance to establish residency if they are accessed frequently enough. The problem asks for the smallest integer burst length $b$ such that page $X$ is not the eviction victim. This requires that the count of $X$ after its initial burst exceeds the steady-state counts of the incumbent hot pages. The problem specifies that the steady-state counts of the hot pages can be treated by their expectations.\n\nThe condition for page $X$ to survive eviction is therefore:\n$$c_X^{\\text{after burst}} > E[c_H]$$\nwhere $c_X^{\\text{after burst}}$ is the count of page $X$ immediately after its burst and $E[c_H]$ is the steady-state expected count of an incumbent hot page $H \\in \\mathcal{H}$.\n\n**Step 1: Calculate the steady-state expected count $E[c_H]$**\n\nFor a page $H \\in \\mathcal{H}$, accesses form a stationary Poisson process with rate $\\mu$. The count $c_H(t)$ decays exponentially with rate $\\lambda$ and is incremented by $1$ at each access. We need to find the expected value of the count just before an eviction decision is made. The problem states the eviction is based on $c_i(t^{-})$. We can model the evolution of the count just before each access.\nLet $c'_k$ be the count of page $H$ just before its $k$-th access, which occurs at time $t_k$. The count just after this access is $c'_k+1$.\nThis count then decays until the next access at time $t_{k+1}$. The count just before the $(k+1)$-th access is:\n$$c'_{k+1} = (c'_k + 1) e^{-\\lambda (t_{k+1}-t_k)}$$\nLet $\\Delta t_k = t_{k+1}-t_k$ be the inter-arrival time. Since arrivals follow a Poisson process with rate $\\mu$, the inter-arrival times $\\Delta t_k$ are independent and identically distributed exponential random variables with probability density function $f(\\tau) = \\mu e^{-\\mu \\tau}$ for $\\tau \\ge 0$.\n\nIn steady state, the expected count is constant, so $E[c'_{k+1}] = E[c'_k] = E[c_H]$. Taking the expectation of the recurrence relation:\n$$E[c'_{k+1}] = E[(c'_k + 1) e^{-\\lambda \\Delta t_k}]$$\nSince the count $c'_k$ at time $t_k$ is independent of the future inter-arrival time $\\Delta t_k$:\n$$E[c_H] = E[c'_k + 1] E[e^{-\\lambda \\Delta t_k}] = (E[c_H] + 1) E[e^{-\\lambda \\Delta t_k}]$$\nWe must calculate the expectation of the exponential term:\n$$E[e^{-\\lambda \\Delta t_k}] = \\int_0^\\infty e^{-\\lambda \\tau} (\\mu e^{-\\mu \\tau}) d\\tau = \\mu \\int_0^\\infty e^{-(\\lambda+\\mu)\\tau} d\\tau = \\mu \\left[ \\frac{-1}{\\lambda+\\mu} e^{-(\\lambda+\\mu)\\tau} \\right]_0^\\infty = \\mu \\left( 0 - \\frac{-1}{\\lambda+\\mu} \\right) = \\frac{\\mu}{\\lambda+\\mu}$$\nSubstituting this back into the equation for $E[c_H]$:\n$$E[c_H] = (E[c_H] + 1) \\frac{\\mu}{\\lambda+\\mu}$$\n$$E[c_H](\\lambda+\\mu) = (E[c_H] + 1)\\mu$$\n$$E[c_H]\\lambda + E[c_H]\\mu = E[c_H]\\mu + \\mu$$\n$$E[c_H]\\lambda = \\mu$$\n$$E[c_H] = \\frac{\\mu}{\\lambda}$$\nThis is the steady-state expected count of a hot page. The PASTA (Poisson Arrivals See Time Averages) property implies this expected count just before an arrival is also the time-averaged expected count. We will use this value as the benchmark to beat.\n\n**Step 2: Calculate the count of page $X$ after its burst**\n\nPage $X$ is new, so its count starts at $c_X=0$. It experiences a burst of $b$ references. The problem states to assume that aging is negligible within the burst ($\\lambda b \\tau_b \\ll 1$). This means we can ignore the exponential decay during the short burst duration. The count update rule is $c_X(t^+) = c_X(t^-) + 1$ for each access. Starting from $0$, after $b$ such updates, the count of page $X$ will be:\n$$c_X^{\\text{after burst}} = b$$\n\n**Step 3: Determine the minimum burst length $b_{\\min}$**\n\nTo avoid being the eviction victim, the count of page $X$ must be strictly greater than the expected count of the incumbent hot pages.\n$$c_X^{\\text{after burst}} > E[c_H]$$\n$$b > \\frac{\\mu}{\\lambda}$$\nSince the burst length $b$ must be an integer, the smallest integer value of $b$ that satisfies this strict inequality is one greater than the floor of the right-hand side.\n$$b_{\\min} = \\left\\lfloor \\frac{\\mu}{\\lambda} \\right\\rfloor + 1$$\nThis is the minimum integer burst length required for the newly popular page $X$ to overcome the LFU cold-start bias and establish residency in the memory system.",
            "answer": "$$\\boxed{\\left\\lfloor \\frac{\\mu}{\\lambda} \\right\\rfloor + 1}$$"
        },
        {
            "introduction": "Theoretical algorithms often assume infinite precision, but real systems must represent values like frequency counts with a finite number of bits. This practice bridges the gap between theory and reality by exploring how quantizing frequency counts can introduce errors, leading to ties and incorrect eviction decisions that degrade LFU's performance. You will compare the miss rate of an ideal system to one with deterministic truncation and then analyze how a more sophisticated technique, stochastic rounding, can recover some of that lost performance by reducing quantization bias .",
            "id": "3629750",
            "problem": "A virtual memory system uses Least Frequently Used (LFU) page replacement with frequency estimates computed over a sliding window of the most recent $W$ references. Requests follow the Independent Reference Model (IRM): on each reference, page $P_i$ is requested independently with fixed probability $p_i$, and the sequence is independent and identically distributed over time. The cache holds $K=2$ pages and there are exactly $3$ pages $\\{P_1,P_2,P_3\\}$ with request probabilities $p_1 = 0.51$, $p_2 = 0.25$, and $p_3 = 0.24$. Ties in LFU frequency estimates are broken uniformly at random.\n\nThe LFU implementation stores a quantized normalized frequency estimate $\\hat{f}_i$ for each page $P_i$ using $b$-bit precision. Specifically, let the true normalized frequency be $f_i$ (the empirical count over the window divided by $W$). The quantizer has uniform step size $\\Delta = \\frac{1}{2^{b}-1}$ and maps $f_i$ to $\\hat{f}_i = \\Delta \\left\\lfloor \\frac{f_i}{\\Delta} \\right\\rfloor$ (deterministic truncation). You may assume $W$ is large so that $f_i \\approx p_i$ at steady state.\n\n1) Starting from the IRM definition and the LFU policy, derive the steady-state miss rate under perfect precision (no quantization), using only the given $p_i$ and $K$.\n\n2) Using the quantized estimates $\\hat{f}_i$ under deterministic truncation with $b=4$ bits, derive the expected miss rate by reasoning directly from the quantized ranking and the stated tie-breaking rule.\n\n3) To reduce quantization bias, consider stochastic rounding: if $f \\in [k\\Delta,(k+1)\\Delta)$, round to $(k+1)\\Delta$ with probability $\\theta = \\frac{f - k\\Delta}{\\Delta}$ and to $k\\Delta$ with probability $1-\\theta$, independently across pages. Using the same steady-state approximation $f_i \\approx p_i$, derive the expected miss rate under this stochastic rounding scheme.\n\n4) Over a horizon of $R = 10{,}000$ references, compute the reduction in the expected number of misses achieved by stochastic rounding compared to deterministic truncation, i.e., the expected misses under deterministic truncation minus the expected misses under stochastic rounding. Provide your final answer as a single real number. No rounding is required and no units should be included in your answer.",
            "solution": "The problem asks for an analysis of the Least Frequently Used (LFU) page replacement algorithm under different frequency estimation schemes. The problem is well-posed, scientifically grounded, and provides all necessary data for a unique solution. We will proceed by solving the four parts of the problem in sequence.\n\nThe system has a cache of size $K=2$ and there are three pages, $\\{P_1, P_2, P_3\\}$, with access probabilities $p_1 = 0.51$, $p_2 = 0.25$, and $p_3 = 0.24$, respectively. The reference stream follows the Independent Reference Model (IRM), so each access is independent. We use the steady-state approximation that the measured frequency over a large window, $f_i$, is approximately equal to the true probability, $f_i \\approx p_i$. A page fault, or miss, occurs when the requested page is not in the cache. The miss rate is the probability of such an event.\n\n**1) Steady-state miss rate under perfect precision**\n\nUnder perfect precision, the LFU algorithm ranks pages based on their true access probabilities $p_i$. The cache of size $K=2$ will hold the two pages with the highest probabilities. The given probabilities are $p_1 = 0.51$, $p_2 = 0.25$, and $p_3 = 0.24$. The ranking is therefore $p_1 > p_2 > p_3$.\n\nThe LFU cache will contain the pages $\\{P_1, P_2\\}$. A miss occurs if and only if the requested page is not in this set. The only page not in the cache is $P_3$. The probability of requesting $P_3$ is $p_3$.\nTherefore, the steady-state miss rate under perfect precision, $M_{ideal}$, is:\n$$\nM_{ideal} = p_3 = 0.24\n$$\n\n**2) Expected miss rate under deterministic truncation**\n\nFor this part, the frequency estimates are quantized using $b=4$ bits. The quantization step size $\\Delta$ is:\n$$\n\\Delta = \\frac{1}{2^b - 1} = \\frac{1}{2^4 - 1} = \\frac{1}{15}\n$$\nThe deterministic truncation rule maps a true frequency $f_i$ to its quantized estimate $\\hat{f}_i$ as follows:\n$$\n\\hat{f}_i = \\Delta \\left\\lfloor \\frac{f_i}{\\Delta} \\right\\rfloor\n$$\nUsing the approximation $f_i \\approx p_i$, we calculate the quantized estimates $\\hat{p}_i$ for each page:\nFor $P_1$: $p_1 = 0.51$. The number of quantization steps is $\\lfloor \\frac{p_1}{\\Delta} \\rfloor = \\lfloor 0.51 \\times 15 \\rfloor = \\lfloor 7.65 \\rfloor = 7$.\nSo, $\\hat{p}_1 = 7 \\Delta = \\frac{7}{15}$.\n\nFor $P_2$: $p_2 = 0.25$. The number of quantization steps is $\\lfloor \\frac{p_2}{\\Delta} \\rfloor = \\lfloor 0.25 \\times 15 \\rfloor = \\lfloor 3.75 \\rfloor = 3$.\nSo, $\\hat{p}_2 = 3 \\Delta = \\frac{3}{15}$.\n\nFor $P_3$: $p_3 = 0.24$. The number of quantization steps is $\\lfloor \\frac{p_3}{\\Delta} \\rfloor = \\lfloor 0.24 \\times 15 \\rfloor = \\lfloor 3.6 \\rfloor = 3$.\nSo, $\\hat{p}_3 = 3 \\Delta = \\frac{3}{15}$.\n\nThe ranking of the pages based on these quantized estimates is $\\hat{p}_1 > \\hat{p}_2 = \\hat{p}_3$. Page $P_1$ has the highest rank and will be in the cache. There is a tie for the second cache slot between $P_2$ and $P_3$. The problem states that ties are broken uniformly at random. This means the second slot will be occupied by $P_2$ with probability $0.5$ and by $P_3$ with probability $0.5$.\n\nThis results in two possible cache states at steady state:\n-   State $C_A = \\{P_1, P_2\\}$, which occurs with probability $0.5$. The miss rate in this state is $p_3 = 0.24$.\n-   State $C_B = \\{P_1, P_3\\}$, which occurs with probability $0.5$. The miss rate in this state is $p_2 = 0.25$.\n\nThe expected miss rate under deterministic truncation, $M_{trunc}$, is the weighted average of the miss rates in these states:\n$$\nM_{trunc} = 0.5 \\times p_3 + 0.5 \\times p_2 = \\frac{1}{2}(p_2 + p_3) = \\frac{1}{2}(0.25 + 0.24) = \\frac{0.49}{2} = 0.245\n$$\n\n**3) Expected miss rate under stochastic rounding**\n\nStochastic rounding is introduced to mitigate the bias from truncation. For a value $f \\in [k\\Delta, (k+1)\\Delta)$, it is rounded up to $(k+1)\\Delta$ with probability $\\theta = \\frac{f - k\\Delta}{\\Delta}$ and rounded down to $k\\Delta$ with probability $1-\\theta$. Let $\\tilde{p}_i$ denote the stochastically rounded estimate of $p_i$.\n\nFor $P_1$ ($p_1 = 0.51$): $p_1$ is in the interval $[7\\Delta, 8\\Delta)$.\nThe probability of rounding up to $8\\Delta = 8/15$ is $\\theta_1 = \\frac{0.51 - 7/15}{1/15} = 15 \\times 0.51 - 7 = 7.65 - 7 = 0.65$.\n$\\tilde{p}_1$ is $8/15$ with probability $0.65$ and $7/15$ with probability $1-0.65 = 0.35$. In either case, $\\tilde{p}_1$ is at least $7/15$.\n\nFor $P_2$ ($p_2 = 0.25$): $p_2$ is in the interval $[3\\Delta, 4\\Delta)$.\nThe probability of rounding up to $4\\Delta = 4/15$ is $\\theta_2 = \\frac{0.25 - 3/15}{1/15} = 15 \\times 0.25 - 3 = 3.75 - 3 = 0.75$.\n$\\tilde{p}_2$ is $4/15$ with probability $0.75$ and $3/15$ with probability $1-0.75 = 0.25$.\n\nFor $P_3$ ($p_3 = 0.24$): $p_3$ is in the interval $[3\\Delta, 4\\Delta)$.\nThe probability of rounding up to $4\\Delta = 4/15$ is $\\theta_3 = \\frac{0.24 - 3/15}{1/15} = 15 \\times 0.24 - 3 = 3.6 - 3 = 0.6$.\n$\\tilde{p}_3$ is $4/15$ with probability $0.6$ and $3/15$ with probability $1-0.6 = 0.4$.\n\nThe maximum possible value for $\\tilde{p}_2$ and $\\tilde{p}_3$ is $4/15$, while the minimum possible value for $\\tilde{p}_1$ is $7/15$. Thus, $\\tilde{p}_1$ is always the highest ranked estimate, and $P_1$ is always in the cache. The competition for the second cache slot is between $P_2$ and $P_3$. The rounding outcomes for $P_2$ and $P_3$ are independent. We analyze the possible rankings of $\\tilde{p}_2$ and $\\tilde{p}_3$:\n\n-   $\\tilde{p}_2 > \\tilde{p}_3$: This occurs only if $\\tilde{p}_2=4/15$ and $\\tilde{p}_3=3/15$.\n    The probability is $P(\\tilde{p}_2=4/15) \\times P(\\tilde{p}_3=3/15) = 0.75 \\times 0.4 = 0.3$.\n    In this case, the cache contains $\\{P_1, P_2\\}$, and the miss rate is $p_3 = 0.24$.\n\n-   $\\tilde{p}_3 > \\tilde{p}_2$: This occurs only if $\\tilde{p}_3=4/15$ and $\\tilde{p}_2=3/15$.\n    The probability is $P(\\tilde{p}_3=4/15) \\times P(\\tilde{p}_2=3/15) = 0.6 \\times 0.25 = 0.15$.\n    In this case, the cache contains $\\{P_1, P_3\\}$, and the miss rate is $p_2 = 0.25$.\n\n-   $\\tilde{p}_2 = \\tilde{p}_3$: This occurs if both are $4/15$ or both are $3/15$.\n    -   $P(\\tilde{p}_2=4/15 \\text{ and } \\tilde{p}_3=4/15) = 0.75 \\times 0.6 = 0.45$.\n    -   $P(\\tilde{p}_2=3/15 \\text{ and } \\tilde{p}_3=3/15) = 0.25 \\times 0.4 = 0.10$.\n    The total probability of a tie is $0.45 + 0.10 = 0.55$.\n    In case of a tie, the second slot is given to $P_2$ or $P_3$ with probability $0.5$ each. The expected miss rate during a tie is $\\frac{1}{2}p_2 + \\frac{1}{2}p_3 = 0.245$.\n\nThe total expected miss rate under stochastic rounding, $M_{stoch}$, is the sum of expectations over these outcomes:\n$$\nM_{stoch} = (0.3 \\times p_3) + (0.15 \\times p_2) + (0.55 \\times (\\frac{p_2+p_3}{2}))\n$$\n$$\nM_{stoch} = 0.3 \\times 0.24 + 0.15 \\times 0.25 + 0.55 \\times 0.245\n$$\n$$\nM_{stoch} = 0.072 + 0.0375 + 0.13475 = 0.24425\n$$\nAlternatively, we can find the total probability that $P_2$ is in the cache (and thus $P_3$ is not). This happens if $P_2$ wins outright or wins a tie.\n$P(P_2 \\text{ in cache}) = P(\\tilde{p}_2 > \\tilde{p}_3) + 0.5 \\times P(\\tilde{p}_2 = \\tilde{p}_3) = 0.3 + 0.5 \\times 0.55 = 0.3 + 0.275 = 0.575$.\nThe probability that $P_3$ is in the cache is $1 - 0.575 = 0.425$.\nThe expected miss rate is:\n$$\nM_{stoch} = P(P_2 \\text{ in cache}) \\times p_3 + P(P_3 \\text{ in cache}) \\times p_2\n$$\n$$\nM_{stoch} = 0.575 \\times 0.24 + 0.425 \\times 0.25 = 0.138 + 0.10625 = 0.24425\n$$\n\n**4) Reduction in expected number of misses**\n\nOver a horizon of $R = 10{,}000$ references, we can compute the expected number of misses for both quantization schemes.\nExpected misses with deterministic truncation:\n$$\nN_{trunc} = R \\times M_{trunc} = 10{,}000 \\times 0.245 = 2450\n$$\nExpected misses with stochastic rounding:\n$$\nN_{stoch} = R \\times M_{stoch} = 10{,}000 \\times 0.24425 = 2442.5\n$$\nThe reduction in the expected number of misses achieved by stochastic rounding compared to deterministic truncation is the difference:\n$$\n\\text{Reduction} = N_{trunc} - N_{stoch} = 2450 - 2442.5 = 7.5\n$$\nThis positive value confirms that stochastic rounding provides a performance improvement over deterministic truncation in this scenario by better preserving the relative ordering of the true probabilities.",
            "answer": "$$\n\\boxed{7.5}\n$$"
        }
    ]
}