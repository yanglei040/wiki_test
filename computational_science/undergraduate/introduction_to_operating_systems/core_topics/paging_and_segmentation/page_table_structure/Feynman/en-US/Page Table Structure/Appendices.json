{
    "hands_on_practices": [
        {
            "introduction": "Understanding page table structures begins with the virtual address itself. This first exercise reinforces the fundamental principle of how a virtual address is partitioned into distinct fields that guide the translation process through a multi-level page table. By reconstructing a full virtual address from its component indices and offset, you will gain a concrete, bit-level understanding of the mapping mechanism that lies at the heart of virtual memory .",
            "id": "3667087",
            "problem": "A system implements demand paging with a multi-level page table. Memory is byte-addressable. The Memory Management Unit (MMU) translates a Virtual Address (VA) by performing a page walk through a $k$-level page table, where each level consumes the same number of bits. The page size is a power of two. The VA is partitioned into $k$ index fields (from most significant to least significant) followed by a page offset field.\n\nConsider a configuration with a $k$-level page table where $k=3$, each level uses $b=10$ bits, and the page size is $2^{o}$ bytes with $o=10$ offset bits. The total VA width is therefore $W = k \\cdot b + o = 40$ bits. During a particular translation, the MMU recorded the page walk path: the level-$1$ index, level-$2$ index, level-$3$ index, and the page offset used to access the final byte within the page. These values are:\n- Level-$1$ index $I_{1} = (1010010110)_{2}$,\n- Level-$2$ index $I_{2} = (0111110001)_{2}$,\n- Level-$3$ index $I_{3} = (0001101101)_{2}$,\n- Page offset $x = (1100100110)_{2}$.\n\nStarting from the core definition that paging splits a VA into an ordered sequence of index fields and an offset field, and that byte-addressability with a page size of $2^{o}$ implies exactly $o$ offset bits, derive the bit positions of each levelâ€™s index field and the general forms of the bit masks and left-shift amounts required to isolate or place each index field within the VA for an arbitrary $k$, $b$, and $o$. Then, use these results to reconstruct the original VA by concatenating $I_{1}$, $I_{2}$, $I_{3}$, and $x$ into a single $40$-bit number.\n\nExpress the final reconstructed VA as an unsigned decimal integer. No rounding is required. Report only the final VA value as your answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of operating system memory management, is well-posed with a complete and consistent setup, and is expressed objectively.\n\nThe problem asks for two main derivations: first, the general formulas for bit positions, masks, and shifts for an arbitrary multi-level page table structure; and second, the reconstruction of a specific Virtual Address (VA) from its component parts.\n\nLet us begin with the general derivation. A virtual address in a system with a $k$-level page table is partitioned into several fields. According to the problem, the VA has a total width of $W$ bits, indexed from $0$ (least significant bit, LSB) to $W-1$ (most significant bit, MSB). The structure is defined as $k$ index fields followed by a page offset field. The page offset uses $o$ bits, and each of the $k$ index fields uses $b$ bits. Thus, the total VA width is $W = k \\cdot b + o$.\n\nThe fields are arranged from most to least significant as: Level-$1$ index ($I_1$), Level-$2$ index ($I_2$), ..., Level-$k$ index ($I_k$), and finally, the page offset ($x$).\n\n1.  **Bit Positions:**\n    - The page offset field, $x$, consists of the $o$ least significant bits of the VA. Its bit positions are from $0$ to $o-1$.\n    - The Level-$k$ index, $I_k$, is the least significant index field and is adjacent to the offset. It occupies the next $b$ bits. Its bit positions are from $o$ to $o+b-1$.\n    - The Level-$(k-1)$ index, $I_{k-1}$, is adjacent to $I_k$. It occupies the next $b$ bits. Its bit positions are from $o+b$ to $o+2b-1$.\n    - We can generalize this pattern. The start bit of the Level-$j$ index, $I_j$ (for $j \\in \\{1, 2, ..., k\\}$), is determined by the total number of bits in all fields less significant than it. These fields are $I_{j+1}, \\dots, I_k$ and the offset $x$. There are $k-j$ such index fields. Thus, the total number of bits below $I_j$ is $(k-j) \\cdot b + o$.\n    - Therefore, the bit field for the Level-$j$ index, $I_j$, starts at bit position $(k-j)b + o$ and ends at bit position $(k-j)b + o + b - 1 = (k-j+1)b + o - 1$.\n\n2.  **Bit Masks and Shift Amounts:**\n    - To isolate the value of the Level-$j$ index, $I_j$, from the full VA, we must first shift the VA to the right so that the LSB of $I_j$ is at bit position $0$. The required right-shift amount, which we denote as $S_j$, is equal to the starting bit position of the $I_j$ field.\n    $$S_j = (k-j)b + o$$\n    - After right-shifting the VA by $S_j$ bits, the $b$ bits of the index $I_j$ occupy bit positions $0$ to $b-1$. To isolate these bits and discard any more-significant bits, we apply a bitmask using a bitwise AND operation. The mask must have $b$ ones in its least significant positions. This mask, $M$, is given by:\n    $$M = 2^b - 1$$\n    - The full operation to extract $I_j$ is thus $I_j = (VA \\gg S_j) \\land M$.\n    - Conversely, to place an index value $I_j$ into its correct position within an empty VA, we must left-shift it by $S_j$ bits. The full VA can be constructed by summing (or, equivalently, bitwise OR-ing) the shifted components:\n    $$VA = (I_1 \\ll S_1) + (I_2 \\ll S_2) + \\dots + (I_k \\ll S_k) + x$$\n    This is equivalent to the concatenation of the binary representations of the fields: $VA_{binary} = [I_1]_{binary} [I_2]_{binary} \\dots [I_k]_{binary} [x]_{binary}$.\n\nNow, we apply these results to the specific configuration given in the problem:\n- Number of levels: $k=3$\n- Bits per index: $b=10$\n- Offset bits: $o=10$\n- Total VA width: $W = 3 \\cdot 10 + 10 = 40$ bits.\n\nThe VA is partitioned as $[I_1 | I_2 | I_3 | x]$. We calculate the shift amounts for each field:\n- For $I_1$ ($j=1$): $S_1 = (3-1)b + o = 2 \\cdot 10 + 10 = 30$.\n- For $I_2$ ($j=2$): $S_2 = (3-2)b + o = 1 \\cdot 10 + 10 = 20$.\n- For $I_3$ ($j=3$): $S_3 = (3-3)b + o = 0 \\cdot 10 + 10 = 10$.\n- For the offset $x$, the shift is $0$.\n\nThe VA can be reconstructed using the formula:\n$$VA = (I_1 \\ll 30) + (I_2 \\ll 20) + (I_3 \\ll 10) + x$$\nWhich is equivalent to $VA = I_1 \\cdot 2^{30} + I_2 \\cdot 2^{20} + I_3 \\cdot 2^{10} + x$.\n\nWe are given the following binary values:\n- $I_1 = (1010010110)_2$\n- $I_2 = (0111110001)_2$\n- $I_3 = (0001101101)_2$\n- $x = (1100100110)_2$\n\nFirst, we convert these $10$-bit binary numbers to their unsigned decimal equivalents:\n- $I_1 = 1 \\cdot 2^9 + 0 \\cdot 2^8 + 1 \\cdot 2^7 + 0 \\cdot 2^6 + 0 \\cdot 2^5 + 1 \\cdot 2^4 + 0 \\cdot 2^3 + 1 \\cdot 2^2 + 1 \\cdot 2^1 + 0 \\cdot 2^0 = 512 + 128 + 16 + 4 + 2 = 662$.\n- $I_2 = 0 \\cdot 2^9 + 1 \\cdot 2^8 + 1 \\cdot 2^7 + 1 \\cdot 2^6 + 1 \\cdot 2^5 + 1 \\cdot 2^4 + 0 \\cdot 2^3 + 0 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0 = 256 + 128 + 64 + 32 + 16 + 1 = 497$.\n- $I_3 = 0 \\cdot 2^9 + 0 \\cdot 2^8 + 0 \\cdot 2^7 + 1 \\cdot 2^6 + 1 \\cdot 2^5 + 0 \\cdot 2^4 + 1 \\cdot 2^3 + 1 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0 = 64 + 32 + 8 + 4 + 1 = 109$.\n- $x = 1 \\cdot 2^9 + 1 \\cdot 2^8 + 0 \\cdot 2^7 + 0 \\cdot 2^6 + 1 \\cdot 2^5 + 0 \\cdot 2^4 + 0 \\cdot 2^3 + 1 \\cdot 2^2 + 1 \\cdot 2^1 + 0 \\cdot 2^0 = 512 + 256 + 32 + 4 + 2 = 806$.\n\nNow, we substitute these decimal values into the reconstruction formula, noting that $2^{10} = 1024$, $2^{20} = 1048576$, and $2^{30} = 1073741824$:\n$$VA = 662 \\cdot 2^{30} + 497 \\cdot 2^{20} + 109 \\cdot 2^{10} + 806$$\n$$VA = 662 \\cdot 1073741824 + 497 \\cdot 1048576 + 109 \\cdot 1024 + 806$$\n$$VA = 710817087488 + 521142272 + 111616 + 806$$\n$$VA = 710817087488 + 521254694$$\n$$VA = 711338342182$$\n\nThe reconstructed original VA, expressed as an unsigned decimal integer, is $711338342182$.\nThis corresponds to the concatenation of the given binary values:\n$VA_{binary} = (1010010110\\ 0111110001\\ 0001101101\\ 1100100110)_2$.",
            "answer": "$$\\boxed{711338342182}$$"
        },
        {
            "introduction": "While the structure of a page table enables virtual memory, its depth has direct performance consequences, particularly on modern systems with deep hierarchies. This practice explores the latency of a hardware page walk, revealing the cost of a TLB miss in a multi-level system. You will analyze how specific architectural features, such as canonical addressing on 64-bit systems, create redundancies that can be exploited to optimize and significantly reduce memory access time .",
            "id": "3667062",
            "problem": "Consider a 64-bit architecture that implements a hardware page-walk for virtual-to-physical address translation using a multilevel page table. Each page table level indexes $9$ bits of the virtual address and the page size is $4 \\, \\text{KiB}$, so the page offset is $12$ bits. The hardware supports a $5$-level page table to accommodate up to 57-bit virtual addresses. However, the operating system uses canonical 48-bit virtual addresses in which bits above bit $47$ are sign-extended.\n\nAssume the following worst-case access scenario for a single load instruction:\n- A Translation Lookaside Buffer (TLB) miss occurs, forcing the hardware walker to traverse the page table from the root.\n- All page-table entries accessed during the walk and the final data access miss in all caches, so each memory reference goes to dynamic random-access memory (DRAM).\n- Each DRAM reference has a fixed latency of $100 \\, \\text{ns}$, and there is no parallelism or overlapping among references.\n\nStarting from core definitions of multilevel page tables and canonical addresses, derive:\n1. The worst-case memory access latency to serve the load when the full $5$-level page table is used for translation under the stated canonical-address regime.\n2. A principled flattening strategy for the upper levels that exploits the fact that canonical $48$-bit addresses constrain the top-level index(es), and the resulting number of levels actually needed for translation.\n3. The worst-case memory access latency to serve the same load after applying your flattening strategy.\n\nFinally, compute the fractional reduction in worst-case latency defined as\n$$\\frac{L_{\\text{original}} - L_{\\text{flattened}}}{L_{\\text{original}}},$$\nand provide this quantity as your single final answer. No units are required in the final answer. If you find that any intermediate quantity requires approximation, use exact values throughout and do not round the final answer.",
            "solution": "The problem requires an analysis of memory access latency for a virtual memory system, considering a hardware-managed page walk under different page table configurations. The process involves validating the problem statement, deriving the latencies, and then computing the fractional reduction.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n- Architecture: 64-bit\n- Page table structure: Multilevel, hardware page-walk\n- Page table index size: $9$ bits per level\n- Page size: $4 \\, \\text{KiB}$\n- Page offset size: $12$ bits\n- Maximum supported levels: $5$ (for up to $57$-bit virtual addresses)\n- OS virtual address space: Canonical $48$-bit (bits $48$ through $63$ are a sign extension of bit $47$)\n- Worst-case scenario: Translation Lookaside Buffer (TLB) miss, and all subsequent memory references (for page tables and data) miss in all caches.\n- Memory latency: Each DRAM reference has a fixed latency of $T_{\\text{DRAM}} = 100 \\, \\text{ns}$.\n- Access model: No parallelism or overlapping among memory references.\n\n**Validation using Extracted Givens:**\n- **Scientific Grounding**: The problem is grounded in established principles of computer architecture and operating systems, specifically virtual memory management, multilevel page tables, and canonical addressing as implemented in architectures like x86-64. The parameters provided are realistic.\n- **Well-Posed**: The problem is well-defined. It provides all necessary data and constraints to derive a unique, deterministic solution for the quantities requested.\n- **Objective**: The problem is stated in precise, objective, and technical language.\n\n**Verdict:** The problem is valid and self-contained. There are no scientific flaws, ambiguities, or contradictions. Proceeding to the solution.\n\n### Step 2: Analysis of the Address Translation Process\n\nFirst, we establish the structure of the virtual address based on the provided parameters. The page size is $4 \\, \\text{KiB}$, which is $2^{12}$ bytes. Therefore, the page offset requires $N_{\\text{offset}} = 12$ bits. Each level of the page table uses $N_{\\text{index}} = 9$ bits of the virtual address as an index.\n\nThe hardware supports a $5$-level page table, which can map a virtual address space of size $N_{\\text{bits}} = 5 \\times N_{\\text{index}} + N_{\\text{offset}} = 5 \\times 9 + 12 = 45 + 12 = 57$ bits. The virtual address (VA) is partitioned as follows, from least significant to most significant bits:\n- VA[$11$:$0$]: Page Offset ($12$ bits)\n- VA[$20$:$12$]: Level $1$ Page Table Index (PML1)\n- VA[$29$:$21$]: Level $2$ Page Table Index (PML2)\n- VA[$38$:$30$]: Level $3$ Page Table Index (PML3)\n- VA[$47$:$39$]: Level $4$ Page Table Index (PML4)\n- VA[$56$:$48$]: Level $5$ Page Table Index (PML5)\n\n### Step 3: Calculation of Original Worst-Case Latency ($L_{\\text{original}}$)\n\nThe problem states a worst-case scenario for a load instruction: a TLB miss occurs, and all memory accesses required for the page walk and the final data load result in DRAM accesses. For a $5$-level page table, the hardware page walker must perform a sequence of dependent memory reads:\n1.  Read the Level $5$ Page Table Entry (PTE).\n2.  Read the Level $4$ PTE.\n3.  Read the Level $3$ PTE.\n4.  Read the Level $2$ PTE.\n5.  Read the Level $1$ PTE.\n\nThis constitutes $5$ memory accesses for the translation process. After the physical address is determined, one more memory access is required to load the actual data.\nThe total number of sequential DRAM accesses is $M_{\\text{original}} = 5 (\\text{translation}) + 1 (\\text{data}) = 6$.\nGiven the DRAM latency $T_{\\text{DRAM}} = 100 \\, \\text{ns}$, the total worst-case latency is:\n$$L_{\\text{original}} = M_{\\text{original}} \\times T_{\\text{DRAM}} = 6 \\times T_{\\text{DRAM}}$$\n\n### Step 4: The Flattening Strategy\n\nThe OS uses a canonical $48$-bit virtual address. In this scheme, bits $48$ through $63$ of the virtual address must be identical to bit $47$. This constraint has a crucial impact on the top-level page table index. The index for the Level $5$ page table is formed by VA[$56$:$48$]. Due to the canonical addressing rule, all these $9$ bits must be equal to VA[$47$].\n- If VA[$47$] = $0$, the address is in the lower half of the canonical space. The Level $5$ index is $000000000_2$, which is $0$.\n- If VA[$47$] = $1$, the address is in the upper half of the canonical space. The Level $5$ index is $111111111_2$, which is $2^9 - 1 = 511$.\n\nThis means that for the entire $2^{48}$-byte virtual address space used by the OS, only two out of the $2^9 = 512$ entries in the Level $5$ page table are ever used: the entry at index $0$ and the one at index $511$. The Level $5$ table serves only to select one of two possible Level $4$ page table hierarchies, based on a single bit of the virtual address (VA[$47$]).\n\nA principled flattening strategy is to eliminate this redundant level of indirection. The number of levels actually required to map a $48$-bit address space is smaller. A structure with $N_{\\text{levels}}$ levels and $9$-bit indices covers $N_{\\text{levels}} \\times 9 + 12$ bits. To cover $48$ bits, we need:\n$$N_{\\text{levels}} \\times 9 + 12 = 48$$\n$$N_{\\text{levels}} \\times 9 = 36$$\n$$N_{\\text{levels}} = 4$$\nThus, a $4$-level page table is sufficient. By treating the Level $4$ tables as the root of the hierarchy (with the OS or hardware selecting between two root pointers for the upper and lower address spaces), the Level $5$ table can be completely removed from the translation process. This reduces the number of levels in the page walk from $5$ to $4$.\n\n### Step 5: Calculation of Flattened Worst-Case Latency ($L_{\\text{flattened}}$)\n\nAfter applying the flattening strategy, the memory access sequence for a page walk changes. A $4$-level page walk requires $4$ memory accesses for translation.\n1.  Read the Level $4$ PTE (now the root).\n2.  Read the Level $3$ PTE.\n3.  Read the Level $2$ PTE.\n4.  Read the Level $1$ PTE.\n\nThe total number of sequential DRAM accesses is now $M_{\\text{flattened}} = 4 (\\text{translation}) + 1 (\\text{data}) = 5$.\nThe new worst-case latency is:\n$$L_{\\text{flattened}} = M_{\\text{flattened}} \\times T_{\\text{DRAM}} = 5 \\times T_{\\text{DRAM}}$$\n\n### Step 6: Calculation of the Fractional Reduction\n\nThe problem asks for the fractional reduction in latency, defined as $\\frac{L_{\\text{original}} - L_{\\text{flattened}}}{L_{\\text{original}}}$. Substituting the derived expressions:\n$$\\text{Fractional Reduction} = \\frac{6 \\times T_{\\text{DRAM}} - 5 \\times T_{\\text{DRAM}}}{6 \\times T_{\\text{DRAM}}}$$\nThe constant $T_{\\text{DRAM}}$ cancels from the numerator and the denominator:\n$$\\text{Fractional Reduction} = \\frac{6 - 5}{6} = \\frac{1}{6}$$\nThe flattening strategy reduces the worst-case memory access latency by a fraction of $1/6$.",
            "answer": "$$\\boxed{\\frac{1}{6}}$$"
        },
        {
            "introduction": "Beyond single memory accesses, page table design profoundly impacts the performance of core operating system services like process creation. This exercise investigates the computational cost of the `fork()` system call when using a Copy-On-Write (COW) strategy, a common optimization. By analyzing the number of operations required, you will uncover a potential scalability bottleneck and be challenged to propose a more advanced design that applies the COW principle to the page tables themselves, a key technique used in modern operating systems .",
            "id": "3667096",
            "problem": "Consider an operating system with demand-paged virtual memory using a multi-level page table. The leaf-level mapping entries are Page Table Entries (PTEs). The system implements Copy-On-Write (COW), defined as the policy where a newly forked child process initially shares all mapped data pages with its parent and both processes mark those pages read-only; a private copy is created only upon the first write fault to a given page. On each fork, the operating system duplicates the parent's leaf-level PTEs for the child and, for each mapped physical page frame that remains shared, increments the frame's reference count.\n\nA single root process initially has exactly $M$ valid leaf-level PTEs (each mapping to a distinct physical page frame), and no other mappings. Starting from the root, a process tree is formed by repeated calls to the $fork$ system call until the tree contains exactly $N$ children in total (that is, the number of fork operations performed is exactly $N$). The shape of the tree is arbitrary; children may fork further children before subsequent forks elsewhere. Assume that:\n- No process performs any memory writes or unmaps until after all $N$ forks have completed.\n- Each fork duplicates only the leaf-level PTEs (that is, count a PTE copy for each leaf entry replicated into the child's page tables).\n- Each fork increments the reference count of every physical frame mapped by the parent's $M$ leaf-level PTEs by $1$, because the child inherits read-only shared mappings under COW.\n\nUsing only these premises, derive from first principles the total number of leaf-level PTE copies performed by the operating system across the entire tree construction, and the total number of physical-frame reference count increment operations executed across the entire tree construction. Express your results as closed-form analytic expressions in terms of $M$ and $N$ only. Do not assume any particular branching factor or tree depth beyond what is stated, and do not assume any writes occur before all forks complete.\n\nFinally, based on the derivation, propose a change to the page table structure or fork strategy that avoids the quadratic-time scaling in the number of PTE copies as $N$ and $M$ grow together, and briefly justify why it changes the asymptotic behavior. Your proposal should be qualitatively described; your computed expressions above must remain purely in terms of $M$ and $N$.\n\nNo rounding is required. No physical units are required.",
            "solution": "The problem requires the derivation of two quantities related to process creation using a Copy-On-Write (COW) mechanism and a proposal for an architectural improvement.\n\nFirst, a validation of the problem statement is performed.\n\n### Step 1: Extract Givens\n- The system uses demand-paged virtual memory and a multi-level page table.\n- Leaf-level entries are Page Table Entries (PTEs).\n- The system uses Copy-On-Write (COW).\n- A newly forked child process shares all mapped data pages with its parent; pages are marked read-only.\n- A private copy is made on the first write fault.\n- On each `fork`, the operating system duplicates the parent's leaf-level PTEs for the child.\n- On each `fork`, the reference count of each shared physical frame is incremented.\n- A single root process starts with exactly `$M$` valid leaf-level PTEs.\n- Each of the `$M$` PTEs maps to a distinct physical page frame.\n- A process tree is formed by a total of `$N$` calls to the `fork` system call, resulting in `$N$` children.\n- The shape of the process tree is arbitrary.\n- Assumption 1: No process performs memory writes or unmaps pages until all `$N$` forks are complete.\n- Assumption 2: A `fork` duplicates only the leaf-level PTEs.\n- Assumption 3: A `fork` increments the reference count of every physical frame mapped by the parent's `$M$` leaf-level PTEs by `$1$`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific/Factual Soundness**: The problem describes a simplified but fundamentally correct model of the `fork()` system call with Copy-On-Write, a standard technique in modern operating systems like Linux. The concepts of page tables, PTEs, reference counting, and COW are central to virtual memory management. The model is a valid abstraction for analyzing the computational cost of process creation.\n- **Well-Posedness and Completeness**: The problem provides all necessary variables (`$M$`, `$N$`) and a clear set of rules governing the system's behavior. The assumptions, particularly that no writes or unmaps occur, are crucial for creating a deterministic system whose state can be analyzed from first principles. The fact that the tree shape is arbitrary but the number of forks is fixed at `$N$` is a key constraint that makes the problem solvable and its solution independent of the specific sequence of forks. The solution is unique and stable.\n- **Objectivity and Clarity**: The problem uses precise, unambiguous technical language common in computer science and operating systems literature. Terms like \"leaf-level PTEs\" and \"reference count\" have clear, standard meanings.\n\nThe problem exhibits none of the invalidating flaws. It is a valid, formalizable problem in the domain of operating systems analysis.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivations\n\nLet `$C_{PTE}$` be the total number of leaf-level PTE copies performed, and let `$C_{RC}$` be the total number of physical-frame reference count increment operations.\n\n#### Total Number of Leaf-Level PTE Copies (`$C_{PTE}$`)\n\nThe problem states that a single root process begins with `$M$` valid leaf-level PTEs. It is also given that no process performs any memory writes or unmaps any pages until all `$N$` forks are completed. This implies that the set of memory mappings does not change for any process during the tree construction phase.\n\nConsequently, any process in the tree that calls `fork()` will itself have exactly `$M$` leaf-level PTEs, which are the inherited mappings from the original root process.\n\nThe problem specifies the cost of a single `fork` operation: \"On each fork, the operating system duplicates the parent's leaf-level PTEs for the child.\" Since every parent process has `$M$` leaf-level PTEs, each `fork` operation results in exactly `$M$` PTEs being copied from the parent's page table structure to the new child's page table structure.\n\nThe problem states that the total number of `fork` operations performed is exactly `$N$`. The total number of PTE copies is therefore the number of copies per fork multiplied by the total number of forks. This holds true regardless of the shape of the process tree (e.g., a \"flat\" tree where the root forks `$N$` times, a \"deep\" tree forming a linked list of processes, or any combination thereof), as the cost is associated with the `fork` operation itself, and there are `$N$` such operations in total.\n\nTherefore, the total number of leaf-level PTE copies is:\n$$C_{PTE} = (\\text{PTE copies per fork}) \\times (\\text{total number of forks})$$\n$$C_{PTE} = M \\times N$$\n\n#### Total Number of Reference Count Increments (`$C_{RC}$`)\n\nThe root process initially maps `$M$` PTEs to `$M$` distinct physical page frames. According to the COW policy and the \"no writes\" assumption, these `$M$` physical frames will be shared among the parent and child after each `fork`. Their state as shared and read-only persists throughout the creation of the entire process tree.\n\nThe problem explicitly states the rule for reference counting: \"Each fork increments the reference count of every physical frame mapped by the parent's `$M$` leaf-level PTEs by $1$.\"\nAs established, any parent process performing a `fork` has mappings to the original `$M$` physical frames. When a `fork` occurs, a new process (the child) now also shares these `$M$` frames. To track this new reference, the operating system must increment the reference count for each of these `$M$` frames. Thus, each single `fork` operation leads to `$M$` individual reference count increment operations.\n\nThe total number of `fork` operations is given as `$N$`. The total number of reference count increments is the number of increments per fork multiplied by the total number of forks.\n\nTherefore, the total number of physical-frame reference count increment operations is:\n$$C_{RC} = (\\text{RC increments per fork}) \\times (\\text{total number of forks})$$\n$$C_{RC} = M \\times N$$\n\nBoth derived quantities are `$MN$`. The cost is bilinear, growing with both the address space size (`$M$`) and the number of created processes (`$N$`). The problem's hint about \"quadratic-time scaling\" refers to the case where `$M$` and `$N$` grow proportionally, i.e., `$N \\propto M$`, which would make the total cost `$O(M^2)` or `$O(N^2)$`.\n\n### Proposed Architectural Change\n\nThe `$O(MN)$` complexity arises from the need to explicitly copy `$M$` PTEs for each of the `$N$` forks. This cost is directly proportional to the size of the parent's address space (`$M$`), which can be very large.\n\n**Proposal:** Apply the Copy-On-Write (COW) mechanism recursively to the page table structure itself, not just to the data pages mapped by the PTEs.\n\n**Justification:**\nIn the described system, the data pages are shared via COW, but the leaf-level page tables containing the PTEs are explicitly duplicated. The proposed change is to make the page table pages themselves shared via COW between the parent and child process.\n\nThe fork operation under this new strategy would be as follows:\n1.  Instead of allocating new page table pages for the child and copying the parent's `$M$` PTEs, the OS would make the child's higher-level page directory entries point to the *same* page table pages used by the parent.\n2.  These shared page table pages would be marked as read-only.\n3.  The reference count for each shared page table page would be incremented.\n\nThe cost of a `fork` operation would no longer depend on `$M$`. Instead, it would be proportional to the number of pointers in the top-level page directory that need to be copied, which is a small constant (or at most proportional to the number of levels in the page table structure, `$L$`, which is itself a small constant, e.g., `$4$` or `$5$` on modern x86-64 CPUs, and grows only logarithmically with the size of the virtual address space).\n\nA subsequent write by either process to a data page would cause a page fault. The OS would handle this by creating a private copy of the data page. This requires modifying the PTE for that page. Since the page table page containing that PTE is read-only, attempting to modify it would trigger a *second* page fault (a fault on the page table page itself). The OS would then handle this second fault by creating a private copy of that specific page table page, updating the PTE in the new private copy, and updating the child's page directory to point to this new page. All other page table pages would remain shared.\n\n**Asymptotic Behavior Change:**\n- **Original scheme:** Fork cost is `$O(M)$`. Total cost for `$N$` forks is `$O(MN)$`.\n- **Proposed scheme (COW on page tables):** Fork cost is essentially `$O(1)` (or `$O(L)` where `$L$` is the number of page table levels, a small constant). The cost is independent of `$M$`. The total cost for `$N$` forks becomes `$O(N)`.\n\nThis change transforms the scaling from bilinear `$O(MN)$` to linear `$O(N)`, which is a dramatic improvement for processes with large address spaces (large `$M$`), effectively eliminating the \"quadratic\" bottleneck. This technique is a key optimization for `fork()` in many real-world operating systems.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nMN & MN\n\\end{pmatrix}\n}\n$$"
        }
    ]
}