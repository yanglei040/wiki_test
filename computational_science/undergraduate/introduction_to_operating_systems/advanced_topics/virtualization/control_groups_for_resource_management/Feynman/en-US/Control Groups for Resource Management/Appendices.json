{
    "hands_on_practices": [
        {
            "introduction": "The foundation of cgroup resource management is the principle of proportional sharing, where resources are distributed according to configurable weights. This first practice allows you to test this core mechanism by calculating the theoretically ideal CPU time allocation for different cgroups and comparing it against hypothetical observed data, a fundamental skill in performance tuning and validation. By quantifying the deviation, you learn how to assess whether a scheduler is performing as expected. ",
            "id": "3628647",
            "problem": "A Linux system uses control groups (cgroups) version 2 to limit and proportionally share the Central Processing Unit (CPU) among groups via the parameter $cpu.weight$. Consider two control groups, A and B, configured with weights $w_{A}=100$ and $w_{B}=400$. There is a single fully utilized CPU core, and each cgroup has exactly one continuously runnable task pinned to this core for the entire measurement interval. Assume no other runnable entities compete for the CPU during the interval. The scheduler is the Linux Completely Fair Scheduler (CFS), which, in steady-state contention, attempts to equalize entities’ virtual runtimes scaled by their weights.\n\nYou measure over a fixed interval of length $T=12 \\text{ s}$ and obtain observed CPU runtimes $T_{A}^{\\text{obs}}=2.6 \\text{ s}$ and $T_{B}^{\\text{obs}}=9.4 \\text{ s}$ for cgroups A and B, respectively.\n\nStarting from the core fairness principle that CFS seeks to equalize weighted virtual runtimes across runnable entities, derive the expected proportional CPU time allocations $T_{A}^{\\text{exp}}$ and $T_{B}^{\\text{exp}}$ over the interval, and then quantify the discrepancy between observed and expected allocations using the root-mean-square relative error\n$$\nE \\;=\\; \\sqrt{\\frac{1}{2}\\left(\\frac{\\left(T_{A}^{\\text{obs}}-T_{A}^{\\text{exp}}\\right)^{2}}{\\left(T_{A}^{\\text{exp}}\\right)^{2}} \\;+\\; \\frac{\\left(T_{B}^{\\text{obs}}-T_{B}^{\\text{exp}}\\right)^{2}}{\\left(T_{B}^{\\text{exp}}\\right)^{2}}\\right)}.\n$$\nReport only the value of $E$ as a decimal fraction (unitless). Round your answer to four significant figures.",
            "solution": "The problem requires the calculation of the discrepancy between observed and expected CPU time allocations for two control groups, A and B, managed by the Linux Completely Fair Scheduler (CFS).\n\nFirst, we must determine the expected CPU time allocations, $T_{A}^{\\text{exp}}$ and $T_{B}^{\\text{exp}}$. The problem states that the CFS scheduler, under conditions of contention, proportionally shares the CPU among entities based on their `cpu.weight` values. The given weights for control group A and control group B are $w_{A}=100$ and $w_{B}=400$, respectively.\n\nThe total weight of the competing entities is the sum of the individual weights:\n$$\nW = w_{A} + w_{B} = 100 + 400 = 500\n$$\nThe expected proportion of CPU time for each cgroup is the ratio of its weight to the total weight.\nFor cgroup A, the expected proportion $p_{A}$ is:\n$$\np_{A} = \\frac{w_{A}}{W} = \\frac{100}{500} = \\frac{1}{5} = 0.2\n$$\nFor cgroup B, the expected proportion $p_{B}$ is:\n$$\np_{B} = \\frac{w_{B}}{W} = \\frac{400}{500} = \\frac{4}{5} = 0.8\n$$\nThe total measurement interval is given as $T=12 \\text{ s}$. The expected CPU runtimes are calculated by multiplying the total interval by the respective proportions.\nThe expected runtime for cgroup A is:\n$$\nT_{A}^{\\text{exp}} = p_{A} \\times T = 0.2 \\times 12 \\text{ s} = 2.4 \\text{ s}\n$$\nThe expected runtime for cgroup B is:\n$$\nT_{B}^{\\text{exp}} = p_{B} \\times T = 0.8 \\times 12 \\text{ s} = 9.6 \\text{ s}\n$$\nAs a check, the sum of the expected runtimes must equal the total interval: $T_{A}^{\\text{exp}} + T_{B}^{\\text{exp}} = 2.4 \\text{ s} + 9.6 \\text{ s} = 12 \\text{ s}$, which is consistent.\n\nNext, we quantify the discrepancy between the observed and expected allocations using the provided formula for the root-mean-square relative error, $E$:\n$$\nE = \\sqrt{\\frac{1}{2}\\left(\\frac{\\left(T_{A}^{\\text{obs}}-T_{A}^{\\text{exp}}\\right)^{2}}{\\left(T_{A}^{\\text{exp}}\\right)^{2}} + \\frac{\\left(T_{B}^{\\text{obs}}-T_{B}^{\\text{exp}}\\right)^{2}}{\\left(T_{B}^{\\text{exp}}\\right)^{2}}\\right)}\n$$\nThe problem provides the observed runtimes: $T_{A}^{\\text{obs}}=2.6 \\text{ s}$ and $T_{B}^{\\text{obs}}=9.4 \\text{ s}$. We can now substitute the observed and expected values into the formula for $E$.\n\nFirst, we calculate the individual relative error terms. For cgroup A:\n$$\n\\frac{T_{A}^{\\text{obs}}-T_{A}^{\\text{exp}}}{T_{A}^{\\text{exp}}} = \\frac{2.6 - 2.4}{2.4} = \\frac{0.2}{2.4} = \\frac{1}{12}\n$$\nFor cgroup B:\n$$\n\\frac{T_{B}^{\\text{obs}}-T_{B}^{\\text{exp}}}{T_{B}^{\\text{exp}}} = \\frac{9.4 - 9.6}{9.6} = \\frac{-0.2}{9.6} = -\\frac{1}{48}\n$$\nNow we substitute these into the formula for $E$:\n$$\nE = \\sqrt{\\frac{1}{2}\\left(\\left(\\frac{1}{12}\\right)^{2} + \\left(-\\frac{1}{48}\\right)^{2}\\right)}\n$$\n$$\nE = \\sqrt{\\frac{1}{2}\\left(\\frac{1}{144} + \\frac{1}{2304}\\right)}\n$$\nTo sum the fractions, we find a common denominator, which is $2304$. Note that $144 \\times 16 = 2304$.\n$$\nE = \\sqrt{\\frac{1}{2}\\left(\\frac{16}{2304} + \\frac{1}{2304}\\right)}\n$$\n$$\nE = \\sqrt{\\frac{1}{2}\\left(\\frac{17}{2304}\\right)}\n$$\n$$\nE = \\sqrt{\\frac{17}{4608}}\n$$\nFinally, we compute the numerical value and round to four significant figures as requested.\n$$\nE \\approx \\sqrt{0.003689236...} \\approx 0.06073908...\n$$\nRounding to four significant figures, we get:\n$$\nE \\approx 0.06074\n$$",
            "answer": "$$\n\\boxed{0.06074}\n$$"
        },
        {
            "introduction": "Real-world systems rarely have a flat resource management structure; instead, they use hierarchies to manage complex applications with many components. This exercise extends the concept of proportional sharing to a nested cgroup hierarchy, demonstrating how I/O bandwidth is distributed from a parent group to its children. Understanding how shares are calculated down the tree is crucial for correctly configuring resource controls for containerized services or multi-tenant systems. ",
            "id": "3628646",
            "problem": "A system uses Control Groups (cgroups) to manage Input/Output (I/O) bandwidth for a single block device with sustainable throughput $B$ megabytes per second (MiB/s). The block I/O controller applies linear proportional sharing: at any scheduler level, the available bandwidth is divided among sibling cgroups in proportion to their configured weights.\n\nConsider the following hierarchy rooted at the device controller. At the root level there are exactly two sibling cgroups:\n- Parent group $\\mathcal{P}$ with configured weight $blkio.weight = 500$.\n- Sibling group $\\mathcal{S}$ with configured weight $blkio.weight = 500$.\n\nWithin $\\mathcal{P}$ there are exactly two leaf cgroups that both continuously issue large-block I/O requests so that they fully utilize any granted bandwidth:\n- Child $\\mathcal{C}_1$ with configured weight $100$.\n- Child $\\mathcal{C}_2$ with configured weight $400$.\n\nSibling group $\\mathcal{S}$ contains a single leaf that also continuously issues I/O, so that the device is fully saturated by the three active leaves $\\mathcal{C}_1$, $\\mathcal{C}_2$, and the leaf inside $\\mathcal{S}$.\n\nAssuming ideal linear proportional sharing at each level of the hierarchy and that all three leaves have identical I/O request characteristics, compute the normalized throughput shares of $\\mathcal{C}_1$ and $\\mathcal{C}_2$ as exact fractions of the total device throughput $B$. Report your answer as a two-entry row matrix $(\\mathcal{C}_1, \\mathcal{C}_2)$. No rounding is required; express the final values as exact fractions without units.",
            "solution": "The fundamental base is the principle of linear proportional sharing: at any scheduler level, a set of sibling entities sharing a resource receive allocations in proportion to their configured weights. In the context of Control Groups (cgroups) and the block I/O controller, this implies that at each level of the hierarchy, the bandwidth share given to a child is proportional to its weight relative to the sum of its siblings, and a leaf’s long-run share equals the product of its proportional shares along its path from the root.\n\nLet the total device throughput be $B$ MiB/s. At the root level, the two sibling cgroups $\\mathcal{P}$ and $\\mathcal{S}$ have weights $500$ and $500$. Under proportional sharing, the fraction of $B$ given to $\\mathcal{P}$ is\n$$\n\\frac{500}{500 + 500} = \\frac{500}{1000} = \\frac{1}{2},\n$$\nso $\\mathcal{P}$ receives $\\frac{1}{2} B$. Similarly, $\\mathcal{S}$ receives $\\frac{1}{2} B$.\n\nWithin $\\mathcal{P}$, the two leaves $\\mathcal{C}_1$ and $\\mathcal{C}_2$ have weights $100$ and $400$, respectively. Their shares of $\\mathcal{P}$’s allocation are determined by proportional sharing among the siblings at this level. The fraction of $\\mathcal{P}$’s bandwidth given to $\\mathcal{C}_1$ is\n$$\n\\frac{100}{100 + 400} = \\frac{100}{500} = \\frac{1}{5},\n$$\nand the fraction given to $\\mathcal{C}_2$ is\n$$\n\\frac{400}{100 + 400} = \\frac{400}{500} = \\frac{4}{5}.\n$$\n\nMultiplying by $\\mathcal{P}$’s allocation from the root level, the absolute shares relative to the total device throughput $B$ are:\n- For $\\mathcal{C}_1$:\n$$\n\\left(\\frac{1}{2}\\right) \\times \\left(\\frac{1}{5}\\right) \\times B = \\frac{1}{10} B,\n$$\nwhich corresponds to a normalized fraction $\\frac{1}{10}$ of $B$.\n- For $\\mathcal{C}_2$:\n$$\n\\left(\\frac{1}{2}\\right) \\times \\left(\\frac{4}{5}\\right) \\times B = \\frac{4}{10} B = \\frac{2}{5} B,\n$$\nwhich corresponds to a normalized fraction $\\frac{2}{5}$ of $B$.\n\nTherefore, the normalized throughput shares of $\\mathcal{C}_1$ and $\\mathcal{C}_2$ as fractions of the total device throughput are $\\frac{1}{10}$ and $\\frac{2}{5}$, respectively. Reporting as a two-entry row matrix $(\\mathcal{C}_1, \\mathcal{C}_2)$ yields\n$$\n\\begin{pmatrix}\n\\frac{1}{10} & \\frac{2}{5}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{10} & \\frac{2}{5}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Resource schedulers do not operate in a vacuum; they interact with other fundamental operating system mechanisms like synchronization primitives. This final practice explores a classic OS problem—priority inversion—within the modern context of cgroup-based fair scheduling. You will analyze how a low-priority cgroup holding a lock can stall a high-priority one and learn how to use cgroup controls to diagnose and mitigate this performance hazard, a critical skill for any systems engineer. ",
            "id": "3628591",
            "problem": "A system runs two Linux Control Groups (cgroups) under the Completely Fair Scheduler (CFS). The cgroups share a single Central Processing Unit (CPU) core. cgroup A has weight $w_A = 900$ and cgroup B has weight $w_B = 100$. There is exactly one continuously runnable thread in each cgroup. A thread in cgroup A attempts to acquire a mutual exclusion (mutex) lock that is currently held by the thread in cgroup B. The lock holder must execute for $t_h = 4\\,\\mathrm{ms}$ of CPU time to reach the unlock point. Assume that both threads remain runnable until the lock is released and that the scheduler apportions CPU time among runnable entities proportionally to their weights, with no Input/Output (I/O) blocking, migrations, or other interference.\n\nFrom first principles of proportional-share scheduling and the definition that a runnable entity’s CPU share is proportional to its weight relative to the sum of runnable weights, estimate the wall-clock latency experienced by cgroup A due to this priority inversion and choose the best mitigation that uses $cpu.weight$ (the cgroup CPU share knob) and per-thread $nice$ (niceness) to reduce the inversion while preserving fairness.\n\nWhich option is correct?\n\nA. The latency is approximately $T = \\dfrac{t_h}{w_B/(w_A + w_B)} = \\dfrac{4\\,\\mathrm{ms}}{100/1000} = 40\\,\\mathrm{ms}$. Mitigation: temporarily boost the lock holder by increasing its cgroup $cpu.weight$ or lowering its thread $nice$ (for example, from $n=0$ to $n=-5$) until it releases the lock, then restore the original values, thereby increasing the lock holder’s CPU fraction and reducing $T$.\n\nB. The latency is approximately $T = t_h = 4\\,\\mathrm{ms}$ because locks are independent of scheduling share. Mitigation: reduce cgroup A’s $cpu.weight$ to avoid starving cgroup B.\n\nC. The latency is approximately $T = t_h \\cdot \\dfrac{w_B}{w_A + w_B} = 0.4\\,\\mathrm{ms}$. Mitigation: decrease cgroup B’s $cpu.weight$ to minimize interference with cgroup A.\n\nD. The latency is unbounded because CFS can starve low-weight entities; therefore, increase cgroup A’s $cpu.weight$ and reduce its $nice$ so it preempts cgroup B more aggressively, which will reduce latency.",
            "solution": "Begin from the proportional-share definition used by the Completely Fair Scheduler (CFS): when there are runnable entities with weights $\\{w_i\\}$ on a single Central Processing Unit (CPU) core, each receives a fraction of CPU time\n$$\nf_i = \\frac{w_i}{\\sum_j w_j}.\n$$\nWith two runnable cgroups, A and B, having weights $w_A$ and $w_B$, the lock holder in cgroup B receives\n$$\nf_B = \\frac{w_B}{w_A + w_B}.\n$$\nThe lock holder must accumulate $t_h$ of actual CPU execution to reach the unlock point. Under continuous contention, wall-clock time stretches because the lock holder only runs for a fraction $f_B$ of wall time. The wall-clock latency $T$ until the lock holder completes $t_h$ of CPU time is\n$$\nT = \\frac{t_h}{f_B} = \\frac{t_h}{\\frac{w_B}{w_A + w_B}}.\n$$\nSubstituting $w_A = 900$, $w_B = 100$, and $t_h = 4\\,\\mathrm{ms}$,\n$$\nf_B = \\frac{100}{900 + 100} = \\frac{100}{1000} = 0.1,\n$$\nso\n$$\nT = \\frac{4\\,\\mathrm{ms}}{0.1} = 40\\,\\mathrm{ms}.\n$$\nThus, cgroup A experiences approximately $40\\,\\mathrm{ms}$ of additional wall-clock latency due to the priority inversion.\n\nMitigation using $cpu.weight$ and $nice$ should increase the lock holder’s scheduling share during the critical section. Temporarily raising cgroup B’s $cpu.weight$ increases $f_B$, reducing $T$. Alternatively, lowering the lock-holding thread’s $nice$ (more favorable niceness, such as decreasing from $n=0$ to $n=-5$) increases its per-thread weight under CFS, also increasing its effective share and reducing $T$. After the lock is released, restoring original settings preserves fairness and avoids long-term imbalance.\n\nOption-by-option analysis:\n\nA. States $T = \\dfrac{t_h}{w_B/(w_A + w_B)}$ and computes $T = \\dfrac{4\\,\\mathrm{ms}}{100/1000} = 40\\,\\mathrm{ms}$, which matches the proportional-share derivation. The proposed mitigation—temporarily increasing cgroup B’s $cpu.weight$ or lowering the lock holder’s $nice$—directly increases $f_B$ during the critical section, reducing $T$, and then restores fairness. Verdict: Correct.\n\nB. Claims $T = t_h = 4\\,\\mathrm{ms}$, ignoring the fact that the lock holder only runs for fraction $f_B$ of wall time; this contradicts proportional-share scheduling. The mitigation—reducing cgroup A’s $cpu.weight$—does not help cgroup A while it is blocked and could further slow the lock holder, worsening inversion. Verdict: Incorrect.\n\nC. Multiplies $t_h$ by the fraction $f_B$, yielding $T = 0.4\\,\\mathrm{ms}$, which reverses the correct relationship. Under contention, wall-clock latency must be larger than $t_h$ when $f_B < 1$, not smaller. The mitigation—decreasing cgroup B’s $cpu.weight$—would reduce $f_B$ and increase $T$, worsening inversion. Verdict: Incorrect.\n\nD. Claims unbounded starvation of low-weight entities under CFS. With positive weight $w_B > 0$, CFS ensures a nonzero share $f_B$, so $T$ is bounded by $\\frac{t_h}{f_B}$. The proposed mitigation—further boosting cgroup A—does not help because cgroup A is blocked and would reduce $f_B$, increasing $T$. Verdict: Incorrect.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}