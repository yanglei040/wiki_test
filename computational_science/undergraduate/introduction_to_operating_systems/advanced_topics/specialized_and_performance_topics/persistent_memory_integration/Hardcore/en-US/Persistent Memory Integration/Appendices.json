{
    "hands_on_practices": [
        {
            "introduction": "How can you trust data that might have been written just as the power failed? This practice tackles that core problem of \"torn writes,\" where a single value is left corrupted. In this exercise , you will build a robust monotonic counter using a two-phase update protocol with parity checks, learning a concrete, low-level technique to guarantee data integrity against crashes.",
            "id": "3669254",
            "problem": "You are asked to formalize and implement a monotonic counter stored in Persistent Memory (PMem) that is resistant to torn writes by using a two-phase update protocol with a single-bit parity per half. In this problem, a monotonic counter is a value that only increases by $1$ on each successful update. The memory model is as follows: there are two disjoint halves, denoted by $S_0$ and $S_1$, each storing a triple $(v_i, p_i, ph_i)$, where $v_i$ is the unsigned counter value, $p_i \\in \\{0,1\\}$ is a parity bit, and $ph_i \\in \\{0,1\\}$ is a phase bit that toggles on each successful update. Assume a crash failure model in which a crash can occur at any time during the update, possibly leaving one half partially updated such that the stored parity no longer matches the value. The objective is to provide mathematically sound recovery steps that reconcile mismatched halves and select the correct counter value after a crash.\n\nFundamental base and assumptions:\n- Persistent Memory (PMem) is byte-addressable and non-volatile. A crash may interrupt a sequence of writes, causing a torn-write state where some fields of one half are updated but others are not. However, disjoint halves $S_0$ and $S_1$ are written in separate steps.\n- A monotonic counter increases by $1$ per complete update and never decreases by design.\n- A simple data-integrity check is the parity of the stored value: define the parity function $\\pi(v)$ as the sum of the bits of $v$ modulo $2$, i.e., $$\\pi(v) = \\left( \\sum_{j=0}^{b-1} b_j \\right) \\bmod 2,$$ where $b$ is the bit-width of $v$ (for implementation use a standard unsigned integer type), and $b_j$ are the bit values of $v$ at position $j$.\n- The two-phase update protocol writes the new value to one half and persists it (value and its parity and the toggled phase), then writes the same to the other half and persists it. The update toggles the shared phase bit from $ph$ to $ph' = 1 - ph$.\n\nRecovery must be derived from these foundations. Let the observed state after a crash be $(v_0, p_0, ph_0)$ and $(v_1, p_1, ph_1)$. Define validity of a half by whether the stored parity equals the recomputed parity, i.e., $$\\text{valid}_i = \\left[ \\pi(v_i) = p_i \\right].$$ A scientifically sound recovery routine must:\n- Prefer halves whose parity matches the value.\n- If both halves are valid and equal in phase and value, accept that value.\n- If both halves are valid but differ in phase or value, the monotonicity constraint implies the recovered value should be the larger of the two values.\n- If exactly one half is valid, accept its value.\n- If neither half is valid, return the smaller of the two values to avoid inventing a future value without integrity evidence, and mark the state as degraded.\n\nAdditionally, propose reconciliation steps to resynchronize mismatched halves back to a consistent state prior to the next update. For this problem, define a reconciliation plan as:\n- A boolean flag (expressed as an integer) indicating whether resynchronization is needed, $$\\text{needs\\_resync} \\in \\{0,1\\},$$ which is $1$ if there is any mismatch (parity, phase, or value) and $0$ otherwise.\n- An integer indicating which half to update first to repair the state, $$\\text{first\\_half\\_to\\_update} \\in \\{0,1\\},$$ where $0$ means update $S_0$ first, and $1$ means update $S_1$ first. If both halves are valid but differ, update first the half that holds the smaller value. If exactly one half is invalid, update the invalid half first. If neither is valid, update first the half that holds the smaller value. If no resynchronization is needed, choose $0$.\n\nYour task is to write a complete program that, given a fixed test suite embedded in the code, applies the recovery algorithm and outputs, for each test case, a triple $[v,\\ \\text{needs\\_resync},\\ \\text{first\\_half\\_to\\_update}]$ where $v$ is the recovered counter value as an integer. The program must produce a single line of output containing the list of these triples, formatted as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case $1$ (happy path, both halves consistent): $S_0: v_0 = 42, p_0 = \\pi(42), ph_0 = 0$; $S_1: v_1 = 42, p_1 = \\pi(42), ph_1 = 0$.\n- Case $2$ (crash between phases, left new, right old): $S_0: v_0 = 43, p_0 = \\pi(43), ph_0 = 1$; $S_1: v_1 = 42, p_1 = \\pi(42), ph_1 = 0$.\n- Case $3$ (torn write in left half, parity incorrect): $S_0: v_0 = 43, p_0 \\neq \\pi(43)$ set as $p_0 = 1 - \\pi(43), ph_0 = 1$; $S_1: v_1 = 42, p_1 = \\pi(42), ph_1 = 0$.\n- Case $4$ (torn write in right half, parity incorrect): $S_0: v_0 = 43, p_0 = \\pi(43), ph_0 = 1$; $S_1: v_1 = 43, p_1 \\neq \\pi(43)$ set as $p_1 = 1 - \\pi(43), ph_1 = 1$.\n- Case $5$ (both valid parity but values differ, equal phases, corruption edge-case): $S_0: v_0 = 100, p_0 = \\pi(100), ph_0 = 0$; $S_1: v_1 = 99, p_1 = \\pi(99), ph_1 = 0$.\n- Case $6$ (both invalid parity): $S_0: v_0 = 7, p_0 = 0$ even though $\\pi(7) = 1$, $ph_0 = 0$; $S_1: v_1 = 8, p_1 = 0$ even though $\\pi(8) = 1$, $ph_1 = 0$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of per-test-case triples, each triple formatted as $[v,\\text{needs\\_resync},\\text{first\\_half\\_to\\_update}]$, and the overall list enclosed in square brackets. For example, the format is $$[[v_1,\\text{needs}_1,\\text{first}_1],[v_2,\\text{needs}_2,\\text{first}_2],\\dots].$$",
            "solution": "The problem requires the formalization and implementation of a recovery and reconciliation algorithm for a fault-tolerant monotonic counter stored in Persistent Memory (PMem). The counter's state is replicated across two disjoint halves, $S_0$ and $S_1$, to protect against data corruption from torn writes during an update.\n\nThe state of each half $S_i$ for $i \\in \\{0, 1\\}$ is a triple $(v_i, p_i, ph_i)$, where $v_i$ is the counter's value, $p_i$ is its parity bit, and $ph_i$ is a phase bit. The core of the solution lies in a systematic, principle-based procedure to interpret the possibly inconsistent states of $S_0$ and $S_1$ after a crash and to determine both the correct current value of the counter and the necessary steps to restore consistency.\n\nThe foundation of the recovery process is the data integrity check provided by the parity bit. We define a parity function, $\\pi(v)$, which computes the sum of the bits of a value $v$ modulo $2$. A half $S_i$ is considered \"valid\" if its stored parity $p_i$ matches the recomputed parity of its value $v_i$. This check is expressed as a boolean condition: $\\text{valid}_i = [\\pi(v_i) = p_i]$.\n\nThe recovery and reconciliation algorithm proceeds in two main stages: state evaluation to determine the recovered value, and planning synchronization to restore a consistent state.\n\n**1. State Evaluation and Value Recovery**\n\nThe first step is to compute $\\text{valid}_0$ and $\\text{valid}_1$ for the post-crash states of $S_0$ and $S_1$. Based on the validity of the two halves, we apply a set of rules, grounded in the principle of monotonicity and data integrity, to determine the correct counter value, $v$.\n\n- **Case 1: Both halves are valid ($\\text{valid}_0 \\land \\text{valid}_1$).**\nIf both halves have correct parity, we trust their values. The two-phase update protocol implies that one half might store an old value while the other stores the new, incremented value. Due to the monotonic counter property (values only increase), the correct state is the one with the higher value. Therefore, the recovered value is $v = \\max(v_0, v_1)$. This single rule also correctly handles the \"happy path\" where both halves are identical ($v_0 = v_1$), as $\\max(v_0, v_0) = v_0$.\n\n- **Case 2: Exactly one half is valid ($\\text{valid}_0 \\oplus \\text{valid}_1$).**\nIf one half has a valid parity and the other does not, the invalid half is considered corrupted by a torn write. We must discard the corrupted data and trust the intact half. If $S_i$ is the valid half, the recovered value is $v = v_i$.\n\n- **Case 3: Neither half is valid ($\\neg \\text{valid}_0 \\land \\neg \\text{valid}_1$).**\nThis is the most severe failure case, indicating that both halves are likely corrupted. Without any integrity evidence, we cannot reliably determine the correct, most recent value. To avoid inventing a future state that may have never been reached, the algorithm must adopt a conservative fallback strategy. The problem mandates selecting the smaller of the two values, $v = \\min(v_0, v_1)$. This choice represents a rollback to the \"safest\" possible state, acknowledging that the system state is degraded.\n\n**2. Reconciliation Plan**\n\nAfter recovering the counter's value, the system must be restored to a consistent state where both halves are identical. The reconciliation plan consists of two components: a flag indicating whether action is needed ($\\text{needs\\_resync}$), and a directive on which half to update first ($\\text{first\\_half\\_to\\_update}$).\n\n- **$\\text{needs\\_resync} \\in \\{0, 1\\}$**: Resynchronization is required if the two halves $S_0$ and $S_1$ are not identical. A state is consistent and needs no resynchronization only if $v_0 = v_1$, $p_0 = p_1$, and $ph_0 = ph_1$. In all other cases, $\\text{needs\\_resync}$ is set to $1$.\n\n- **$\\text{first\\_half\\_to\\_update} \\in \\{0, 1\\}$**: If resynchronization is needed, this value determines the order of operations. The logic is derived from the same state evaluation:\n    - If no resynchronization is needed, this value is conventionally set to $0$.\n    - If both halves are valid but differ (in value or phase): The half containing the older (smaller) value must be updated to match the newer (larger) one. Thus, we first update the half $S_i$ where $v_i = \\min(v_0, v_1)$. If $v_0 = v_1$ but the phases differ, a deterministic tie-breaking rule must be used; we choose to update $S_0$ first.\n    - If exactly one half $S_i$ is invalid: The invalid half is the one that needs repair. We must update $S_i$ first.\n    - If neither half is valid: Both halves must be overwritten with the recovered state. The rule is to first update the half that holds the smaller value. If $v_0=v_1$, we use the same tie-breaker and update $S_0$ first.\n\nThis complete algorithm provides a rigorous and deterministic method to ensure the monotonic counter remains available and consistent despite crash failures. The final implementation translates this logic into a program that processes the given test cases.",
            "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#include <complex.h>\n#include <threads.h>\n#include <stdatomic.h>\n\n// A struct to hold the state of one half of the counter.\ntypedef struct {\n    unsigned int v;  // The counter value.\n    unsigned int p;  // The stored parity bit.\n    unsigned int ph; // The stored phase bit.\n} Half;\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    Half s0;\n    Half s1;\n} TestCase;\n\n// A struct to hold the result of the recovery and reconciliation process.\ntypedef struct {\n    unsigned int v;\n    int needs_resync;\n    int first_half_to_update;\n} Result;\n\n/**\n * @brief Calculates the parity of an unsigned integer.\n *\n * The parity is defined as the sum of its bits modulo 2.\n * This implementation counts the number of set bits (popcount).\n * An even popcount results in a parity of 0, an odd popcount in 1.\n *\n * @param n The unsigned integer.\n * @return The parity of n (0 or 1).\n */\nunsigned int calculate_parity(unsigned int n) {\n    unsigned int parity = 0;\n    while (n) {\n        parity = !parity;\n        n &= (n - 1); // This clears the least significant set bit.\n    }\n    return parity;\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    // Parity values are pre-calculated for clarity:\n    // pi(7) = 1, pi(8) = 1, pi(42) = 1, pi(43) = 0, pi(99) = 0, pi(100) = 1.\n    TestCase test_cases[] = {\n        // Case 1: Happy path, both halves consistent.\n        { {42, 1, 0}, {42, 1, 0} },\n        // Case 2: Crash between phases. S0 is new, S1 is old.\n        { {43, 0, 1}, {42, 1, 0} },\n        // Case 3: Torn write in S0 (parity incorrect), S1 is valid old state.\n        { {43, 1, 1}, {42, 1, 0} }, // S0 has p0 != pi(43)\n        // Case 4: Torn write in S1 (parity incorrect), S0 is valid new state.\n        { {43, 0, 1}, {43, 1, 1} }, // S1 has p1 != pi(43)\n        // Case 5: Both valid, values differ, same phase (corruption edge case).\n        { {100, 1, 0}, {99, 0, 0} },\n        // Case 6: Both halves have invalid parity.\n        { {7, 0, 0}, {8, 0, 0} }\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    Result results[num_cases];\n\n    for (int i = 0; i < num_cases; ++i) {\n        Half s0 = test_cases[i].s0;\n        Half s1 = test_cases[i].s1;\n\n        int valid0 = (calculate_parity(s0.v) == s0.p);\n        int valid1 = (calculate_parity(s1.v) == s1.p);\n\n        // 1. Recover the counter value.\n        unsigned int recovered_v;\n        if (valid0 && valid1) {\n            recovered_v = (s0.v > s1.v) ? s0.v : s1.v;\n        } else if (valid0 && !valid1) {\n            recovered_v = s0.v;\n        } else if (!valid0 && valid1) {\n            recovered_v = s1.v;\n        } else { // !valid0 && !valid1\n            recovered_v = (s0.v < s1.v) ? s0.v : s1.v;\n        }\n\n        // 2. Determine the reconciliation plan.\n        int needs_resync = !((s0.v == s1.v) && (s0.p == s1.p) && (s0.ph == s1.ph));\n        int first_half_to_update;\n\n        if (!needs_resync) {\n            first_half_to_update = 0;\n        } else {\n            if (valid0 && valid1) {\n                // Update the half with the smaller value. Tie-break on s0 if values are equal.\n                if (s1.v < s0.v) {\n                    first_half_to_update = 1;\n                } else { // s0.v = s1.v\n                    first_half_to_update = 0;\n                }\n            } else if (!valid0  valid1) {\n                first_half_to_update = 0; // Update invalid S0.\n            } else if (valid0  !valid1) {\n                first_half_to_update = 1; // Update invalid S1.\n            } else { // !valid0  !valid1\n                if (s0.v = s1.v) { // Tie-break with = to be deterministic.\n                    first_half_to_update = 0; // Update half with smaller value, or S0 in a tie.\n                } else {\n                    first_half_to_update = 1;\n                }\n            }\n        }\n        \n        results[i].v = recovered_v;\n        results[i].needs_resync = needs_resync;\n        results[i].first_half_to_update = first_half_to_update;\n    }\n\n    // Print the results in the exact required format before the final return statement.\n    printf(\"[\");\n    for (int i = 0; i  num_cases; ++i) {\n        printf(\"[%u,%d,%d]\", results[i].v, results[i].needs_resync, results[i].first_half_to_update);\n        if (i  num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "Ensuring a file update is atomic—for example, updating both the data and the metadata pointer—is a classic systems problem. This thought experiment  puts you in the role of a file system designer, analyzing how techniques like copy-on-write and journaling rely on the careful ordering of writes and memory fences ($SFENCE$) to prevent data corruption after a crash.",
            "id": "3669193",
            "problem": "A file system built over Persistent Memory (PMem) on a Non-Volatile Dual In-line Memory Module (NVDIMM) must update a file atomically: replace the content of one $4\\,\\mathrm{KiB}$ data block and update the inode metadata that stores a pointer to the block and the file length. The machine has a Central Processing Unit (CPU) with volatile caches; a store reaches persistence only when the corresponding cache lines have been explicitly written back to PMem and the writes are ordered with a persist barrier. Assume the following widely accepted primitives and facts as the starting base:\n- Cache Line Write Back (CLWB) or Cache Line Flush Optimized (CLFLUSHOPT) causes the specified cache line to be written back to PMem but does not, by itself, guarantee ordering with respect to subsequent stores.\n- A Store Fence (SFENCE) orders prior cache line write backs and stores with respect to subsequent stores, creating a happens-before in the persistence order.\n- On a crash at time $t_c$, the persistent state equals some prefix of the sequence of cache line write backs that reached PMem before $t_c$; volatile cache state is lost.\n- Do not assume multi-kilobyte atomic persistence; only assume failure atomicity at the cache line level after a write back completes.\n\nLet the old state be described by $(B_{\\text{old}},P_{\\text{old}},L_{\\text{old}})$ where $B_{\\text{old}}$ is the old data block content, $P_{\\text{old}}$ is the inode’s pointer to the old block, and $L_{\\text{old}}$ is the old length. The new intended state is $(B_{\\text{new}},P_{\\text{new}},L_{\\text{new}})$ where $B_{\\text{new}}$ is the new data block content, $P_{\\text{new}}$ points to the new block, and $L_{\\text{new}}$ is the new length. The crash-consistency invariant $I$ is: after any crash at time $t_c$, the recovered file must present either $(B_{\\text{old}},P_{\\text{old}},L_{\\text{old}})$ or $(B_{\\text{new}},P_{\\text{new}},L_{\\text{new}})$, but never a hybrid such as $(B_{\\text{old}},P_{\\text{new}},L_{\\text{new}})$ or $(B_{\\text{new}},P_{\\text{old}},L_{\\text{old}})$.\n\nConsider the following candidate update-and-recovery designs. Each option describes both a protocol and the recovery narrative for crashes occurring at any point between metadata and data updates. Select all options that both enforce the invariant $I$ and state an accurate recovery narrative derived from the stated base.\n\nA. Copy-on-write with ordered persistence. Allocate a fresh block for $B_{\\text{new}}$, write the new payload into it, issue CLWB on all cache lines of the new block, then issue SFENCE. Next, update the inode fields to $(P_{\\text{new}},L_{\\text{new}})$, issue CLWB on the inode cache lines, then issue SFENCE. Recovery narrative: if a crash occurs before the first SFENCE has completed, the inode remains at $(P_{\\text{old}},L_{\\text{old}})$ and readers see $B_{\\text{old}}$; if a crash occurs after the first SFENCE but before the second SFENCE completes, the inode still points to $P_{\\text{old}}$ so readers see $B_{\\text{old}}$; if a crash occurs after the second SFENCE completes, readers see $(P_{\\text{new}},L_{\\text{new}})$ and $B_{\\text{new}}$. In no case is $(P_{\\text{new}},L_{\\text{new}})$ visible without $B_{\\text{new}}$ being durable.\n\nB. Metadata-first in-place update. First, update the inode to $(P_{\\text{new}},L_{\\text{new}})$, issue CLWB on the inode cache lines, then issue SFENCE. Next, overwrite the old data block with $B_{\\text{new}}$, issue CLWB on its cache lines, then issue SFENCE. Recovery narrative: if a crash occurs after the first SFENCE but before the second SFENCE completes, the inode may persist $(P_{\\text{new}},L_{\\text{new}})$ while the block still contains $B_{\\text{old}}$ or partial data, so readers may observe a hybrid state.\n\nC. Copy-on-write without an intermediate fence. Allocate a fresh block and write $B_{\\text{new}}$ to it; issue CLWB on all cache lines of the new block, but do not issue a fence yet. Immediately update the inode to $(P_{\\text{new}},L_{\\text{new}})$, issue CLWB on the inode cache lines, then issue a single SFENCE at the end. Recovery narrative: because CLWB is used for the data first, persistence ordering is guaranteed, so a crash never exposes $(P_{\\text{new}},L_{\\text{new}})$ without $B_{\\text{new}}$.\n\nD. Write-ahead redo logging (journaling) with commit. Compose a redo log record containing the new payload $B_{\\text{new}}$ and the metadata changes $(P_{\\text{new}},L_{\\text{new}})$ along with a checksum. Append the record to the redo log, issue CLWB on all its cache lines, then issue SFENCE. Next, append a commit marker for that record, issue CLWB on the commit line(s), then issue SFENCE. After the commit is durable, apply the in-place changes to the data block and inode in any order and at any later time, issuing CLWB for touched lines; optionally batch these with a later SFENCE. Recovery narrative: on crash, the system replays only committed, checksum-valid log records, installing both $B_{\\text{new}}$ and $(P_{\\text{new}},L_{\\text{new}})$ idempotently; uncommitted records are ignored. Thus, after recovery, the file is either in the old state or in the new state, never a hybrid, even if the crash occurred after installing one field but not the other.",
            "solution": "The problem statement is scientifically and logically valid. It is grounded in the established principles of computer architecture and operating systems, specifically concerning crash consistency in file systems using persistent memory (PMem). The primitives provided—`CLWB`/`CLFLUSHOPT`, `SFENCE`, and a well-defined crash model—accurately reflect a standard, simplified model of real-world PMem programming on architectures like x86-64. The objective is to ensure an atomic update, defined by the invariant $I$, which mandates that a file operation transition from a consistent old state $(B_{\\text{old}}, P_{\\text{old}}, L_{\\text{old}})$ to a consistent new state $(B_{\\text{new}}, P_{\\text{new}}, L_{\\text{new}})$ without exposing any intermediate, inconsistent hybrid states upon recovery from a crash. The problem is well-posed, providing sufficient information to evaluate the correctness of the proposed protocols. We will now proceed to analyze each option.\n\n**Analysis of Option A: Copy-on-write with ordered persistence.**\nThe protocol is as follows:\n1.  Write new data $B_{\\text{new}}$ to a newly allocated block.\n2.  Issue `CLWB` for all cache lines of $B_{\\text{new}}$.\n3.  Issue `SFENCE`.\n4.  Update the inode to $(P_{\\text{new}}, L_{\\text{new}})$.\n5.  Issue `CLWB` for the inode's cache line(s).\n6.  Issue `SFENCE`.\n\nThis protocol implements the copy-on-write (CoW) strategy. The critical feature is the use of an `SFENCE` instruction (step $3$) after flushing the new data block and *before* updating the inode pointer. According to the problem's definition, `SFENCE` creates a happens-before relationship in the persistence order. This ensures that all cache line write-backs for $B_{\\text{new}}$ are made persistent before the subsequent write-backs for the inode can be persisted.\n\nLet's analyze crash possibilities:\n- If a crash occurs at any point before the first `SFENCE` (step $3$) completes, the persistence of the new data block $B_{\\text{new}}$ is not guaranteed. However, the inode update has not yet been made persistent, so the file system pointer $P_{\\text{old}}$ still points to the old block $B_{\\text{old}}$. Upon recovery, the system sees the original state $(B_{\\text{old}}, P_{\\text{old}}, L_{\\text{old}})$. The newly allocated block is effectively unreachable garbage.\n- If a crash occurs after the first `SFENCE` completes but before the second `SFENCE` completes, the new data $B_{\\text new}$ is guaranteed to be fully persistent. The inode update to $(P_{\\text{new}}, L_{\\text{new}})$ may or may not have become persistent. If it has not, the system recovers to the old state $(B_{\\text{old}}, P_{\\text{old}}, L_{\\text{old}})$. If the inode update *has* become persistent, the system recovers to the new state $(B_{\\text{new}}, P_{\\text{new}}, L_{\\text{new}})$. In this case, since the inode points to $P_{\\text{new}}$, the data at that location, $B_{\\text{new}}$, is already guaranteed to be durable.\nIn no scenario is it possible for a persistent inode to point to a non-persistent or partially-persistent data block. The invariant $I$ is therefore enforced.\n\nThe recovery narrative provided in the option is consistent with this analysis. It correctly identifies that the inode will point to the old block if crashes happen before the final state is committed, and that the crucial property is that a new pointer is never visible without the new data being durable.\nVerdict: **Correct**.\n\n**Analysis of Option B: Metadata-first in-place update.**\nThe protocol is:\n1.  Update inode to $(P_{\\text{new}}, L_{\\text{new}})$.\n2.  Issue `CLWB` on the inode.\n3.  Issue `SFENCE`.\n4.  Overwrite the data block with $B_{\\text{new}}$.\n5.  Issue `CLWB` on the data block.\n6.  Issue `SFENCE`.\n\nThis protocol prioritizes persisting the metadata update. A crash after step $3$ completes but before step $6$ completes creates a critical failure. In this window, the inode is durably updated to $(P_{\\text{new}}, L_{\\text{new}})$, but the data block, which is $4\\,\\mathrm{KiB}$ and thus spans multiple cache lines, may be only partially updated. For example, the inode could point to a block that contains a mix of $B_{\\text{old}}$ and $B_{\\text{new}}$, which is a corrupt state. This directly leads to a hybrid, inconsistent state, violating the invariant $I$.\n\nThe problem asks to select options that *enforce the invariant I*. This protocol fails to do so. Although its accompanying recovery narrative accurately describes this failure, the protocol itself is flawed and does not represent a valid solution to the atomic update problem.\nVerdict: **Incorrect**.\n\n**Analysis of Option C: Copy-on-write without an intermediate fence.**\nThe protocol is:\n1.  Write new data $B_{\\text{new}}$ to a new block.\n2.  Issue `CLWB` for all cache lines of $B_{\\text{new}}$.\n3.  Update the inode to $(P_{\\text{new}}, L_{\\text{new}})$.\n4.  Issue `CLWB` for the inode's cache line(s).\n5.  Issue `SFENCE`.\n\nThis protocol omits the crucial `SFENCE` between flushing the data and flushing the metadata. The problem statement specifies that `CLWB` \"does not, by itself, guarantee ordering\". The memory hardware is free to reorder the writes to PMem. Without an intervening `SFENCE`, the `CLWB` for the inode (step $4$) could be persisted before the `CLWB`s for the data block $B_{\\text{new}}$ (step $2$), even though it was issued later.\n\nIf a crash occurs after the inode is persisted but before $B_{\\text new}$ is fully persisted, the system will recover to a state where the inode has $(P_{\\text{new}}, L_{\\text{new}})$ but points to a partially written, corrupt data block. This violates the invariant $I$. The recovery narrative's claim that \"persistence ordering is guaranteed\" is factually wrong based on the problem's own definitions, as it misinterprets the behavior of `CLWB`.\nVerdict: **Incorrect**.\n\n**Analysis of Option D: Write-ahead redo logging (journaling) with commit.**\nThe protocol is a standard two-phase commit scheme using a redo log:\n1.  **Log**: Append a log record containing all changes ($B_{\\text{new}}$ and $(P_{\\text{new}}, L_{\\text{new}})$) to a journal and persist it (`CLWB` + `SFENCE`).\n2.  **Commit**: Append a commit marker for that log record and persist it (`CLWB` + `SFENCE`).\n3.  **Apply**: After commit, apply the changes from the log to their home locations.\n\nThis protocol achieves atomicity by making the durable persistence of the commit record the atomic point of decision.\n- **Recovery**: The recovery procedure scans the log.\n    - If a log record is found without a corresponding committed marker, it is ignored. This happens if a crash occurred before step $2$ completed. The home locations of the data and inode remain untouched, so the system is in the state $(B_{\\text{old}}, P_{\\text{old}}, L_{\\text{old}})$.\n    - If a committed log record is found, the recovery procedure replays the changes specified in the log, applying them to their home locations. This replay must be idempotent, meaning it can be repeated safely without changing the result. For instance, writing $B_{\\text{new}}$ to a block is idempotent. If a crash occurs during this \"apply\" phase, the recovery process will simply restart and re-apply the changes on the next boot, eventually bringing the system to the full new state $(B_{\\text{new}}, P_{\\text{new}}, L_{\\text{new}})$.\n\nThis mechanism ensures that the file system transitions from the old state to the new state atomically, from the perspective of a user after recovery. The recovery narrative correctly describes this process and its outcome. The protocol is sound and correctly enforces the invariant $I$.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Correctness is essential, but performance often determines a system's success. Now that you understand *how* to make operations atomic, this exercise  asks *which* method is faster. By modeling the performance of copy-on-write versus undo logging, you will derive a mathematical threshold that helps a system designer make a quantitative, workload-driven choice between two different consistency protocols.",
            "id": "3669271",
            "problem": "An operating system integrates Non-Volatile Memory (NVM) into its memory hierarchy. A simple persistent key-value map is stored as a single contiguous region of size $S$ bytes and is crash-consistent via either copy-on-write or undo logging. The hardware exposes a cache line size of $L$ bytes. Assume that both the record size $s$ and the region size $S$ are integer multiples of $L$, i.e., $s = qL$ and $S = mL$ for some positive integers $q$ and $m$. The platform provides two primitives: (i) a cache line persist operation that makes a single cache line durable with latency $t_{p}$, and (ii) a store fence with latency $t_{sf}$ that orders and waits for previously issued persist operations to become durable. All other costs (e.g., allocation and computation) are negligible. Assume operations are serialized and there is no overlap of persist latencies.\n\nConsider a transaction that performs $N$ updates, each update modifying exactly one record of size $s$ bytes aligned to cache line boundaries. Two update protocols are implemented:\n\n- Copy-on-Write (CoW): The system allocates a fresh copy of the entire map region, copies the whole region, persists the new region, then installs a new root pointer to the copied region and persists that pointer. A single store fence is issued after all persist operations to ensure durability before returning success.\n\n- Undo Logging: For each update, the system writes an undo log record containing the $s$ bytes being overwritten, persists that log record to ensure it is durable and ordered before the in-place update, then performs the in-place update of the record and persists the modified data. A single store fence is issued per update to enforce that the undo log is durable before the in-place data write is allowed to be considered complete, and one additional final store fence is issued after all updates to ensure all modified data are durable before returning success.\n\nUsing only the following foundational facts:\n- Crash consistency requires that the old state be recoverable if new data are partially persisted. This is achieved either by persisting an alternate copy (copy-on-write) before pointer installation or by persisting an undo log before in-place updates.\n- A cache line is durable only after its persist operation completes; ordering of durability across lines requires a store fence.\n- The time cost accumulates additively as the sum over all cache line persist latencies plus the sum over all store fence latencies,\n\nderive from first principles a closed-form expression for the smallest integer threshold $N_{\\text{th}}$ such that the total persist cost (persist latencies plus fence latencies) of the undo logging protocol for $N$ updates is greater than or equal to the total persist cost of the copy-on-write protocol, i.e., the smallest $N$ such that copy-on-write becomes no more expensive than undo logging.\n\nExpress $N_{\\text{th}}$ symbolically in terms of $S$, $s$, $L$, $t_{p}$, and $t_{sf}$. Your final answer must be a single closed-form analytic expression. Do not include units in your final answer.",
            "solution": "The user has provided a valid, well-posed problem statement. It describes a performance modeling scenario in operating systems, based on standard concepts of persistent memory, crash consistency protocols, and their associated costs. I will proceed with a rigorous derivation.\n\nThe objective is to find the smallest integer threshold `$N_{\\text{th}}$` such that the total persist cost of the undo logging protocol for `$N$` updates, denoted `$T_{\\text{undo}}$`, is greater than or equal to the total persist cost of the copy-on-write protocol, denoted `$T_{\\text{CoW}}$`. This can be expressed as the smallest integer `$N$` satisfying the inequality `$T_{\\text{undo}} \\ge T_{\\text{CoW}}$`.\n\nFirst, I will derive the expression for the total cost of the Copy-on-Write (CoW) protocol, `$T_{\\text{CoW}}$`. The cost is the sum of latencies for all required persist and store fence operations.\n1.  **Persist the new region copy:** The region size is `$S$` bytes and the cache line size is `$L$` bytes. Since `$S$` is an integer multiple of `$L$`, the number of cache lines in the region is `$\\frac{S}{L}$`. The latency to persist one cache line is `$t_p$`. Therefore, the cost to make the entire new region durable is `$\\frac{S}{L} t_p$`.\n2.  **Persist the root pointer:** The problem states the system \"persists that pointer\". The fundamental hardware primitive is a cache line persist operation. We assume the pointer is contained within a single cache line. Thus, persisting the pointer incurs the cost of one cache line persist operation, which is `$t_p$`.\n3.  **Store fence:** The protocol issues a single store fence after all persist operations are issued. This adds a latency of `$t_{sf}$`.\n\nThe total cost for the CoW protocol is the sum of these costs, as operations are serialized:\n$$T_{\\text{CoW}} = \\frac{S}{L} t_p + t_p + t_{sf} = \\left(\\frac{S}{L} + 1\\right) t_p + t_{sf}$$\n\nNext, I will derive the expression for the total cost of the Undo Logging protocol, `$T_{\\text{undo}}$`, for a transaction consisting of `$N$` updates.\nThe protocol specifies a set of operations performed for each of the `$N$` updates, plus a final operation.\nFor each of the `$N$` updates:\n1.  **Persist the undo log record:** An undo log record of size `$s$` bytes is written. Since `$s$` is an integer multiple of `$L$`, this requires persisting `$\\frac{s}{L}$` cache lines. The cost is `$\\frac{s}{L} t_p$`.\n2.  **Store fence:** A store fence is issued to ensure the undo log is durable before the in-place update. The cost is `$t_{sf}$`.\n3.  **Persist the modified data:** The in-place update modifies a record of size `$s$`, which spans `$\\frac{s}{L}$` cache lines. Persisting these modified data lines costs `$\\frac{s}{L} t_p$`.\n\nThe cost for a single update is the sum of these latencies: `$\\frac{s}{L} t_p + t_{sf} + \\frac{s}{L} t_p = 2\\frac{s}{L} t_p + t_{sf}$`.\nSince there are `$N$` such updates, the cumulative cost for all updates is `$N \\left(2\\frac{s}{L} t_p + t_{sf}\\right)$`.\nFinally, the protocol issues \"one additional final store fence\" after all updates are complete. This adds a cost of `$t_{sf}$`.\n\nThe total cost for the undo logging protocol is:\n$$T_{\\text{undo}} = N \\left(2\\frac{s}{L} t_p + t_{sf}\\right) + t_{sf}$$\n\nNow, I will establish the inequality `$T_{\\text{undo}} \\ge T_{\\text{CoW}}$` and solve for `$N$`.\n$$N \\left(2\\frac{s}{L} t_p + t_{sf}\\right) + t_{sf} \\ge \\left(\\frac{S}{L} + 1\\right) t_p + t_{sf}$$\nThe `$t_{sf}$` term on both sides can be cancelled:\n$$N \\left(2\\frac{s}{L} t_p + t_{sf}\\right) \\ge \\left(\\frac{S}{L} + 1\\right) t_p$$\nThe term `$\\left(2\\frac{s}{L} t_p + t_{sf}\\right)$` is a sum of latencies and sizes, which are all positive quantities. Thus, the term itself is positive, and we can divide both sides by it without changing the direction of the inequality.\n$$N \\ge \\frac{\\left(\\frac{S}{L} + 1\\right) t_p}{2\\frac{s}{L} t_p + t_{sf}}$$\nTo simplify the expression, I multiply the numerator and the denominator by `$L$`:\n$$N \\ge \\frac{L \\left(\\frac{S}{L} + 1\\right) t_p}{L \\left(2\\frac{s}{L} t_p + t_{sf}\\right)}$$\n$$N \\ge \\frac{(S + L) t_p}{2s t_p + L t_{sf}}$$\nThe problem asks for the smallest *integer* threshold `$N_{\\text{th}}$` that satisfies this condition. This is precisely the definition of the ceiling function applied to the right-hand side of the inequality.\nTherefore, the expression for `$N_{\\text{th}}$` is:\n$$N_{\\text{th}} = \\left\\lceil \\frac{(S + L) t_p}{2s t_p + L t_{sf}} \\right\\rceil$$\nThis is the closed-form analytical expression for the smallest integer `$N$` where the undo logging cost meets or exceeds the copy-on-write cost.",
            "answer": "$$\\boxed{\\left\\lceil \\frac{(S + L) t_p}{2s t_p + L t_{sf}} \\right\\rceil}$$"
        }
    ]
}