{
    "hands_on_practices": [
        {
            "introduction": "When multiple threads compete for a single lock, they form a queue, and the time spent waiting directly impacts application latency. This exercise applies queueing theory to build a formal model for predicting this wait time. By using the M/G/1 queue model, you will learn to quantitatively analyze how factors like arrival rate and the variability of lock holding time contribute to contention delays, moving your understanding from intuition to rigorous analysis .",
            "id": "3654523",
            "problem": "An operating system kernel uses a single mutual exclusion lock to protect updates to a shared run queue. Threads arrive to attempt lock acquisition according to a Poisson process with rate $\\lambda$ arrivals per second. Upon acquiring the lock, a thread executes a critical section whose time is a random variable $S$ (the lock holding time), independent across threads and independent of the arrival process. The lock uses First-Come First-Served (FCFS) admission, and the scheduler ensures non-preemptive service while a thread holds the lock. Assume a steady-state regime in which $\\lambda \\mathbb{E}[S] < 1$.\n\nYou may use the following foundational facts: (i) Poisson Arrivals See Time Averages (PASTA) for a Poisson arrival process, (ii) Little’s Law, and (iii) the mean residual life identity for a renewal, $\\mathbb{E}[R] = \\mathbb{E}[S^{2}] / \\left(2 \\mathbb{E}[S]\\right)$, where $R$ is the residual service time observed at a random time during a busy period.\n\nIn this workload, arrivals occur at rate $\\lambda = 150{,}000$ per second. The critical section time $S$ has a bimodal distribution: with probability $0.9$, $S = 1$ microsecond, and with probability $0.1$, $S = 10$ microseconds.\n\nStarting from the above core definitions and facts, model the lock as a single-server FCFS queue and derive an expression for the mean time an arriving thread waits before acquiring the lock (excluding its own critical section time). Then evaluate this mean waiting time under the given parameters. Express your final numerical answer in microseconds and round to four significant figures.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n-   **Model**: A single-server First-Come First-Served (FCFS) queue.\n-   **Arrival Process**: Poisson process with rate $\\lambda = 150{,}000$ arrivals per second.\n-   **Service Time**: A random variable $S$. Its distribution is bimodal: $P(S = 1 \\text{ µs}) = 0.9$ and $P(S = 10 \\text{ µs}) = 0.1$. The service times are independent across threads and independent of the arrival process.\n-   **Stability Condition**: The system is in a steady-state regime where $\\lambda \\mathbb{E}[S] < 1$.\n-   **Provided Foundational Facts**:\n    1.  Poisson Arrivals See Time Averages (PASTA).\n    2.  Little’s Law.\n    3.  Mean residual life identity: $\\mathbb{E}[R] = \\frac{\\mathbb{E}[S^2]}{2 \\mathbb{E}[S]}$, where $R$ is the residual service time observed at a random time during a busy period.\n-   **Objective**: Derive an expression for the mean waiting time in the queue, $\\mathbb{E}[W_q]$, and then evaluate it for the given parameters. The final answer should be in microseconds, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a single-server queue with Poisson arrivals and a general service time distribution (M/G/1 queue), a standard and well-understood model in queuing theory, frequently applied to computer systems performance analysis. The problem is scientifically grounded.\n\nThe problem provides all necessary parameters ($\\lambda$ and the distribution of $S$) and theoretical tools to derive the solution. The stability of the queue must be verified.\nThe mean service time $\\mathbb{E}[S]$ is:\n$$ \\mathbb{E}[S] = (1 \\text{ µs}) \\times 0.9 + (10 \\text{ µs}) \\times 0.1 = 0.9 \\text{ µs} + 1.0 \\text{ µs} = 1.9 \\text{ µs} = 1.9 \\times 10^{-6} \\text{ s} $$\nThe arrival rate is $\\lambda = 150{,}000 \\text{ s}^{-1}$.\nThe server utilization, $\\rho$, is given by $\\lambda \\mathbb{E}[S]$:\n$$ \\rho = (150{,}000 \\text{ s}^{-1}) \\times (1.9 \\times 10^{-6} \\text{ s}) = 1.5 \\times 10^5 \\times 1.9 \\times 10^{-6} = 0.285 $$\nSince $\\rho = 0.285 < 1$, the stability condition is met, and a steady-state solution exists. The problem is well-posed and internally consistent. It is objective and free of ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Mean Waiting Time\nThe system is modeled as an M/G/1 queue. We aim to find the mean waiting time in the queue, denoted as $\\mathbb{E}[W_q]$. This is the average time an arriving thread spends waiting before it begins its critical section.\n\nConsider a thread arriving at the queue. The time it must wait, $W_q$, is equal to the total time required to serve all threads already present in the system. This time consists of two components:\n1.  The remaining service time of the thread currently holding the lock (if any). Let's call this random variable $R_a$.\n2.  The sum of the full service times of all threads waiting in the queue ahead of the new arrival.\n\nThus, we can write the waiting time for an arriving thread as:\n$$ W_q = R_a + \\sum_{i=1}^{N_q} S_i $$\nwhere $N_q$ is the number of threads waiting in the queue upon arrival, and $S_i$ is the service time of the $i$-th thread in the queue.\n\nBy the linearity of expectation, the mean waiting time is:\n$$ \\mathbb{E}[W_q] = \\mathbb{E}[R_a] + \\mathbb{E}\\left[\\sum_{i=1}^{N_q} S_i\\right] $$\n\nDue to the PASTA property, an arriving thread observes the system in its steady-state. The time-average number of threads in the queue is $\\mathbb{E}[N_q]$. The service times $S_i$ are independent of $N_q$. We can apply Wald's identity to the second term:\n$$ \\mathbb{E}\\left[\\sum_{i=1}^{N_q} S_i\\right] = \\mathbb{E}[S] \\mathbb{E}[N_q] $$\nBy Little's Law applied to the queue, the mean number of threads waiting is the product of the arrival rate and the mean waiting time:\n$$ \\mathbb{E}[N_q] = \\lambda \\mathbb{E}[W_q] $$\nSubstituting this into the previous equation gives:\n$$ \\mathbb{E}\\left[\\sum_{i=1}^{N_q} S_i\\right] = \\mathbb{E}[S] (\\lambda \\mathbb{E}[W_q]) = (\\lambda \\mathbb{E}[S]) \\mathbb{E}[W_q] = \\rho \\mathbb{E}[W_q] $$\n\nNow we analyze the first term, $\\mathbb{E}[R_a]$, the mean residual service time seen by an arrival. An arriving thread finds the server (the lock) busy with probability $\\rho = \\lambda \\mathbb{E}[S]$ and idle with probability $1 - \\rho$.\nIf the server is idle, the residual time is $0$. If the server is busy, the expected residual service time is given by the mean residual life identity conditional on the server being busy, $\\mathbb{E}[R] = \\frac{\\mathbb{E}[S^2]}{2 \\mathbb{E}[S]}$.\nTherefore, the unconditional expected residual time seen by an arrival is:\n$$ \\mathbb{E}[R_a] = \\rho \\cdot \\mathbb{E}[R] + (1-\\rho) \\cdot 0 = (\\lambda \\mathbb{E}[S]) \\cdot \\frac{\\mathbb{E}[S^2]}{2 \\mathbb{E}[S]} = \\frac{\\lambda \\mathbb{E}[S^2]}{2} $$\n\nCombining the expressions for the two components, we get:\n$$ \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2} + \\rho \\mathbb{E}[W_q] $$\nWe can now solve for $\\mathbb{E}[W_q]$:\n$$ \\mathbb{E}[W_q] - \\rho \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2} $$\n$$ \\mathbb{E}[W_q] (1 - \\rho) = \\frac{\\lambda \\mathbb{E}[S^2]}{2} $$\n$$ \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2(1-\\rho)} $$\nThis is the Pollaczek-Khinchine formula for the mean waiting time.\n\n### Numerical Evaluation\nTo evaluate this expression, we first compute the necessary moments of the service time $S$. It is convenient to perform calculations with time measured in microseconds (µs).\nThe arrival rate is $\\lambda = 150{,}000 \\text{ s}^{-1} = 150{,}000 \\times 10^{-6} \\text{ µs}^{-1} = 0.15 \\text{ µs}^{-1}$.\nThe service time $S$ has the distribution $P(S=1 \\text{ µs}) = 0.9$ and $P(S=10 \\text{ µs}) = 0.1$.\n\nFirst moment (mean service time):\n$$ \\mathbb{E}[S] = (0.9 \\times 1) + (0.1 \\times 10) = 0.9 + 1.0 = 1.9 \\text{ µs} $$\n\nSecond moment of the service time:\n$$ \\mathbb{E}[S^2] = \\sum_{i} s_i^2 P(S=s_i) = (0.9 \\times 1^2) + (0.1 \\times 10^2) = (0.9 \\times 1) + (0.1 \\times 100) = 0.9 + 10 = 10.9 \\text{ µs}^2 $$\n\nServer utilization $\\rho$:\n$$ \\rho = \\lambda \\mathbb{E}[S] = (0.15 \\text{ µs}^{-1}) \\times (1.9 \\text{ µs}) = 0.285 $$\nAs confirmed during validation, the system is stable.\n\nNow, we substitute these values into the formula for $\\mathbb{E}[W_q]$:\n$$ \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2(1-\\rho)} = \\frac{(0.15 \\text{ µs}^{-1}) \\times (10.9 \\text{ µs}^2)}{2(1-0.285)} $$\n$$ \\mathbb{E}[W_q] = \\frac{1.635 \\text{ µs}}{2(0.715)} = \\frac{1.635 \\text{ µs}}{1.43} $$\n$$ \\mathbb{E}[W_q] \\approx 1.14335664... \\text{ µs} $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\mathbb{E}[W_q] \\approx 1.143 \\text{ µs} $$\nThe mean time an arriving thread waits before acquiring the lock is approximately $1.143$ microseconds.",
            "answer": "$$\\boxed{1.143}$$"
        },
        {
            "introduction": "A common strategy to improve scalability is to replace a single global lock with distributed, per-CPU data structures, but this introduces new challenges. This practice guides you through modeling the fundamental trade-off between the serialization of a global lock and the overheads of a per-CPU design, such as data consolidation and task migration. By quantitatively comparing these two architectures, you will develop the critical skill of evaluating system design choices and understanding that there is often no universally \"best\" solution, only a set of trade-offs .",
            "id": "3654522",
            "problem": "You are to design and implement a single, self-contained program that models and compares the average per-operation latency under two designs in a multiprocessing operating system context: a global lock protecting a single shared data structure versus per-Central Processing Unit (per-CPU) private data structures. The objective is to reason about lock contention and scalability from first principles, derive a quantitative model, and compute a latency speedup ratio. The speedup ratio for each test case is defined as the average latency under the global lock design divided by the average latency under the per-CPU design.\n\nBegin from the following foundational base, without using or assuming any shortcut formulas beyond widely accepted results and definitions:\n- Mutual exclusion serializes access to a critical section. When many threads attempt to enter a single critical section, they queue and wait, increasing average latency.\n- Little’s Law states that for a stable system, the average number in the system $L$ satisfies $L = \\lambda W$, where $\\lambda$ is the arrival rate and $W$ is the average time in the system.\n- The $M/M/1$ queue is a single-server queue with Poisson arrivals and exponential service times. It is a well-tested model for contention in a single resource. Use it to model contention for a global lock and for each per-CPU lock. Assume stability conditions hold, meaning that the arrival rate is strictly less than the service rate.\n\nModeling assumptions to be used by your implementation:\n- Global lock design: All operations across $n$ CPUs that require mutual exclusion are serialized by a single lock, modeling the critical section as an $M/M/1$ server with service rate $\\mu_g$ and total arrival rate $\\lambda = n r$, where $r$ is the per-CPU arrival rate of operations requiring the critical section. The average time in system (latency) under this model should be computed using the $M/M/1$ queue under stability.\n- Per-CPU design: Each CPU has its own private lock and data structure, modeled independently as $M/M/1$ queues, each with arrival rate $r$ and service rate $\\mu_\\ell$. In addition to queueing latency at the local lock, incorporate the expected cost of cross-CPU task migration and the amortized cost of periodic consolidation (aggregation of per-CPU data into a global view). Assume:\n  - Migration: Each operation independently incurs an additional cost $c_m$ with probability $p$. Model this as an expected additive latency of $p c_m$ per operation.\n  - Consolidation: A consolidation occurs every $\\Delta$ seconds and costs $c_s$ per CPU. Amortize this as an additive per-operation latency $\\frac{c_s}{r \\Delta}$.\n\nDefinitions of parameters to be used in your program:\n- $n$: number of CPUs (dimensionless).\n- $r$: per-CPU arrival rate of operations (in operations per second, $\\text{s}^{-1}$).\n- $c_g$: critical section service time for the global lock design (in seconds, $\\text{s}$). The service rate is $\\mu_g = \\frac{1}{c_g}$.\n- $c_\\ell$: critical section service time for the per-CPU design (in seconds, $\\text{s}$). The service rate is $\\mu_\\ell = \\frac{1}{c_\\ell}$.\n- $p$: migration probability per operation (dimensionless).\n- $c_m$: migration cost per migrating operation (in seconds, $\\text{s}$).\n- $\\Delta$: consolidation interval (in seconds, $\\text{s}$).\n- $c_s$: consolidation cost per CPU per consolidation (in seconds, $\\text{s}$).\n\nScientific realism requirements:\n- All times must be expressed in seconds ($\\text{s}$).\n- All arrival rates must be expressed in operations per second ($\\text{s}^{-1}$).\n- Use the $M/M/1$ stability conditions $n r < \\mu_g$ and $r < \\mu_\\ell$.\n\nYour task is to:\n- Derive the average per-operation latency for both designs using only the above base and modeling assumptions.\n- Implement a program that computes, for each test case, the latency speedup ratio $S$ defined as the global-lock latency divided by the per-CPU latency.\n- Round each output $S$ to six decimal places.\n\nTest suite and parameters:\nProvide results for the following five test cases, each specified by $(n, r, c_g, c_\\ell, p, c_m, \\Delta, c_s)$:\n\n- Case $1$ (happy path, moderate contention, light migration and consolidation):\n  - $n = 8$, $r = 4000$, $c_g = 1.2 \\times 10^{-5}$, $c_\\ell = 3.2 \\times 10^{-6}$, $p = 0.02$, $c_m = 5.0 \\times 10^{-6}$, $\\Delta = 0.2$, $c_s = 2.0 \\times 10^{-4}$.\n- Case $2$ (boundary condition: single CPU, no migration, no consolidation overhead):\n  - $n = 1$, $r = 4000$, $c_g = 1.0 \\times 10^{-5}$, $c_\\ell = 3.0 \\times 10^{-6}$, $p = 0.0$, $c_m = 1.0 \\times 10^{-6}$, $\\Delta = 1.0$, $c_s = 0.0$.\n- Case $3$ (near-global saturation, per-CPU remains fast):\n  - $n = 16$, $r = 1200$, $c_g = 5.0 \\times 10^{-5}$, $c_\\ell = 4.0 \\times 10^{-6}$, $p = 0.05$, $c_m = 1.0 \\times 10^{-5}$, $\\Delta = 0.05$, $c_s = 1.0 \\times 10^{-4}$.\n- Case $4$ (high migration and high consolidation overhead, many CPUs):\n  - $n = 32$, $r = 800$, $c_g = 3.0 \\times 10^{-5}$, $c_\\ell = 2.0 \\times 10^{-6}$, $p = 0.3$, $c_m = 1.0 \\times 10^{-5}$, $\\Delta = 0.02$, $c_s = 5.0 \\times 10^{-4}$.\n- Case $5$ (heavy consolidation cost but very infrequent):\n  - $n = 8$, $r = 5000$, $c_g = 1.1 \\times 10^{-5}$, $c_\\ell = 2.5 \\times 10^{-6}$, $p = 0.15$, $c_m = 2.0 \\times 10^{-6}$, $\\Delta = 5.0$, $c_s = 2.0 \\times 10^{-2}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the speedup results as a comma-separated list enclosed in square brackets, for example, $\\left[ s_1, s_2, s_3, s_4, s_5 \\right]$, where each $s_i$ is a float rounded to six decimal places. Concretely, print using the exact textual format:\n- \"[result1,result2,result3,result4,result5]\"",
            "solution": "The problem requires a quantitative comparison of two design patterns for managing shared data in a multi-CPU operating system: a single global lock versus per-CPU private data structures. The comparison metric is the speedup ratio $S$, defined as the average per-operation latency of the global lock design divided by that of the per-CPU design. The derivation will be based on first principles of queueing theory, specifically the $M/M/1$ model.\n\nFirst, we must validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Designs to compare**: Global lock vs. Per-CPU private data structures.\n- **Performance Metric**: Latency speedup ratio $S = W_g / W_\\ell$, where $W_g$ is the global lock latency and $W_\\ell$ is the per-CPU latency.\n- **Modeling Framework**:\n  - Mutual exclusion serializes access, leading to queueing.\n  - Little’s Law: $L = \\lambda W$.\n  - Contention is modeled using the $M/M/1$ queue.\n  - Stability requires arrival rate $\\lambda$ to be less than service rate $\\mu$.\n- **Global Lock Model ($W_g$)**:\n  - A single $M/M/1$ server.\n  - Total arrival rate $\\lambda_g = n \\cdot r$.\n  - Service rate $\\mu_g = 1/c_g$.\n- **Per-CPU Model ($W_\\ell$)**:\n  - Each of the $n$ CPUs has an independent $M/M/1$ queue.\n  - Per-CPU arrival rate $\\lambda_\\ell = r$.\n  - Per-CPU service rate $\\mu_\\ell = 1/c_\\ell$.\n  - Additional additive latency from migration: expected cost of $p \\cdot c_m$ per operation.\n  - Additional additive latency from consolidation: amortized cost of $\\frac{c_s}{r \\cdot \\Delta}$ per operation.\n- **Parameters**:\n  - $n$: number of CPUs.\n  - $r$: per-CPU arrival rate ($\\text{s}^{-1}$).\n  - $c_g$: global lock service time ($\\text{s}$).\n  - $c_\\ell$: per-CPU lock service time ($\\text{s}$).\n  - $p$: migration probability.\n  - $c_m$: migration cost ($\\text{s}$).\n  - $\\Delta$: consolidation interval ($\\text{s}$).\n  - $c_s$: per-CPU consolidation cost ($\\text{s}$).\n- **Stability Conditions**: $n \\cdot r < \\mu_g$ and $r < \\mu_\\ell$.\n- **Test Cases**: Five sets of parameters are provided.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing the standard $M/M/1$ queueing model to analyze system performance, a common practice in computer science and engineering. The concepts of lock contention, scalability, migration overhead, and data aggregation are fundamental to operating systems design. The problem is well-posed, providing all necessary parameters and a clear objective. The language is objective and quantitative.\n\nWe must verify the stability conditions for all provided test cases. The conditions are $n \\cdot r \\cdot c_g < 1$ and $r \\cdot c_\\ell < 1$.\n1.  **Case 1**: $n \\cdot r \\cdot c_g = 8 \\cdot 4000 \\cdot 1.2 \\times 10^{-5} = 0.384 < 1$. $r \\cdot c_\\ell = 4000 \\cdot 3.2 \\times 10^{-6} = 0.0128 < 1$. Valid.\n2.  **Case 2**: $n \\cdot r \\cdot c_g = 1 \\cdot 4000 \\cdot 1.0 \\times 10^{-5} = 0.04 < 1$. $r \\cdot c_\\ell = 4000 \\cdot 3.0 \\times 10^{-6} = 0.012 < 1$. Valid.\n3.  **Case 3**: $n \\cdot r \\cdot c_g = 16 \\cdot 1200 \\cdot 5.0 \\times 10^{-5} = 0.96 < 1$. $r \\cdot c_\\ell = 1200 \\cdot 4.0 \\times 10^{-6} = 0.0048 < 1$. Valid.\n4.  **Case 4**: $n \\cdot r \\cdot c_g = 32 \\cdot 800 \\cdot 3.0 \\times 10^{-5} = 0.768 < 1$. $r \\cdot c_\\ell = 800 \\cdot 2.0 \\times 10^{-6} = 0.0016 < 1$. Valid.\n5.  **Case 5**: $n \\cdot r \\cdot c_g = 8 \\cdot 5000 \\cdot 1.1 \\times 10^{-5} = 0.44 < 1$. $r \\cdot c_\\ell = 5000 \\cdot 2.5 \\times 10^{-6} = 0.0125 < 1$. Valid.\n\nAll test cases satisfy the stability conditions. The problem is self-contained, consistent, and scientifically sound.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the derivation and solution.\n\n**Derivation of Latency Formulas**\n\nThe average time an item spends in an $M/M/1$ queueing system, denoted $W$, is a standard result from queueing theory. Given an arrival rate $\\lambda$ and a service rate $\\mu$, the average number of items in the system is $L = \\frac{\\lambda}{\\mu - \\lambda}$. By Little's Law, $L = \\lambda W$, which implies $W = \\frac{L}{\\lambda} = \\frac{1}{\\mu - \\lambda}$. This formula will be used for both the global and per-CPU queueing models.\n\n**1. Average Latency for the Global Lock Design ($W_g$)**\nThis design is modeled as a single $M/M/1$ queue.\n- The total arrival rate of operations to the single global lock is the sum of rates from all $n$ CPUs:\n  $$ \\lambda_g = n \\cdot r $$\n- The service rate $\\mu_g$ is the reciprocal of the service time $c_g$:\n  $$ \\mu_g = \\frac{1}{c_g} $$\n- Applying the $M/M/1$ latency formula, the average time per operation in the system (queueing wait time plus service time) is:\n  $$ W_g = \\frac{1}{\\mu_g - \\lambda_g} = \\frac{1}{\\frac{1}{c_g} - n r} $$\n- To avoid potential floating-point issues with subtraction of large numbers, we can rewrite this as:\n  $$ W_g = \\frac{c_g}{1 - n r c_g} $$\nThis expression represents the total average latency for an operation under the global lock design.\n\n**2. Average Latency for the Per-CPU Design ($W_\\ell$)**\nThe latency in the per-CPU design is the sum of three distinct components: the local queueing latency, the expected migration cost, and the amortized consolidation cost.\n$$ W_\\ell = W_{\\text{queue}} + W_{\\text{migration}} + W_{\\text{consolidation}} $$\n\n- **Local Queueing Latency ($W_{\\text{queue}}$)**: Each CPU has its own data structure protected by its own lock. This is modeled as an independent $M/M/1$ queue.\n  - The arrival rate to a single CPU's queue is simply $r$:\n    $$ \\lambda_\\ell = r $$\n  - The service rate is the reciprocal of the local service time $c_\\ell$:\n    $$ \\mu_\\ell = \\frac{1}{c_\\ell} $$\n  - The average latency for this local queue is:\n    $$ W_{\\text{queue}} = \\frac{1}{\\mu_\\ell - \\lambda_\\ell} = \\frac{1}{\\frac{1}{c_\\ell} - r} = \\frac{c_\\ell}{1 - r c_\\ell} $$\n\n- **Migration Cost ($W_{\\text{migration}}$)**: The problem specifies that each operation has a probability $p$ of incurring an additional migration cost $c_m$. The expected value of this cost, averaged over all operations, is an additive component to the latency.\n  $$ W_{\\text{migration}} = p \\cdot c_m $$\n\n- **Consolidation Cost ($W_{\\text{consolidation}}$)**: A consolidation process runs every $\\Delta$ seconds and imposes a cost of $c_s$ on each CPU. This cost must be amortized over the operations that occur during one interval.\n  - The number of operations processed by a single CPU in an interval of $\\Delta$ seconds is $N_{ops} = r \\cdot \\Delta$.\n  - The total cost $c_s$ is distributed over these $N_{ops}$ operations. The amortized cost per operation is therefore:\n  $$ W_{\\text{consolidation}} = \\frac{c_s}{r \\Delta} $$\n\n- **Total Per-CPU Latency ($W_\\ell$)**: Summing the three components gives the total average latency for an operation under the per-CPU design:\n  $$ W_\\ell = \\frac{c_\\ell}{1 - r c_\\ell} + p c_m + \\frac{c_s}{r \\Delta} $$\n\n**3. Latency Speedup Ratio ($S$)**\nThe speedup ratio $S$ is defined as the ratio of the global lock latency to the per-CPU latency.\n$$ S = \\frac{W_g}{W_\\ell} = \\frac{\\frac{c_g}{1 - n r c_g}}{\\frac{c_\\ell}{1 - r c_\\ell} + p c_m + \\frac{c_s}{r \\Delta}} $$\nThis is the final expression that will be implemented to compute the results for the given test cases.",
            "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n// #include <complex.h> // Not used\n// #include <threads.h> // Not used\n// #include <stdatomic.h> // Not used\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int n;          // number of CPUs\n    double r;       // per-CPU arrival rate (ops/sec)\n    double cg;      // global lock service time (sec)\n    double cl;      // per-CPU lock service time (sec)\n    double p;       // migration probability\n    double cm;      // migration cost (sec)\n    double delta;   // consolidation interval (sec)\n    double cs;      // consolidation cost (sec)\n} TestCase;\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        // Case 1 (happy path, moderate contention)\n        {8, 4000.0, 1.2e-5, 3.2e-6, 0.02, 5.0e-6, 0.2, 2.0e-4},\n        // Case 2 (single CPU, no overheads)\n        {1, 4000.0, 1.0e-5, 3.0e-6, 0.0, 1.0e-6, 1.0, 0.0},\n        // Case 3 (near-global saturation)\n        {16, 1200.0, 5.0e-5, 4.0e-6, 0.05, 1.0e-5, 0.05, 1.0e-4},\n        // Case 4 (high migration/consolidation overhead)\n        {32, 800.0, 3.0e-5, 2.0e-6, 0.3, 1.0e-5, 0.02, 5.0e-4},\n        // Case 5 (heavy but infrequent consolidation)\n        {8, 5000.0, 1.1e-5, 2.5e-6, 0.15, 2.0e-6, 5.0, 2.0e-2}\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    double results[num_cases];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n\n        // 1. Calculate Average Latency for Global Lock Design (Wg)\n        // Wg = cg / (1 - n * r * cg)\n        double utilization_g = (double)tc.n * tc.r * tc.cg;\n        // Stability check: utilization must be < 1. This was validated manually.\n        double Wg = tc.cg / (1.0 - utilization_g);\n\n        // 2. Calculate Average Latency for Per-CPU Design (Wl)\n        // Wl is the sum of three components:\n        // W_queue + W_migration + W_consolidation\n\n        // W_queue = cl / (1 - r * cl)\n        double utilization_l = tc.r * tc.cl;\n        // Stability check: utilization must be < 1. This was validated manually.\n        double W_queue = tc.cl / (1.0 - utilization_l);\n\n        // W_migration = p * cm\n        double W_migration = tc.p * tc.cm;\n\n        // W_consolidation = cs / (r * delta)\n        // Handle case where r or delta might be zero to prevent division by zero,\n        // though problem constraints ensure they are positive.\n        double W_consolidation = 0.0;\n        if (tc.r > 0 && tc.delta > 0) {\n            W_consolidation = tc.cs / (tc.r * tc.delta);\n        }\n        \n        double Wl = W_queue + W_migration + W_consolidation;\n\n        // 3. Calculate Speedup Ratio S\n        // S = Wg / Wl\n        if (Wl > 0) {\n            results[i] = Wg / Wl;\n        } else {\n            // Handle edge case, though not expected with given test data.\n            results[i] = 0.0;\n        }\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement.\n    // Format: [result1,result2,result3,result4,result5]\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"%.6f\", results[i]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "The cost of lock contention extends beyond software-level delays; it has profound effects on underlying hardware resources. This exercise reveals the hidden cost of a \"hot\" spin lock by having you calculate the immense cache coherence traffic it generates on the system interconnect. By tracing the Read-For-Ownership and invalidation messages, you will gain a deeper appreciation for how software synchronization can saturate hardware bandwidth, degrading overall system performance .",
            "id": "3645691",
            "problem": "A multicore processor implements a directory-based cache coherence protocol following the Modified–Exclusive–Shared–Invalid (MESI) states. The system is Cache Coherent Non-Uniform Memory Access (ccNUMA), and all cores connect to a single on-chip interconnect. Consider a contended spin lock implemented with the Test-and-Test-and-Set (TTAS) idiom using Compare-And-Swap (CAS), where many threads repeatedly read a lock variable located in a single cache line and attempt a CAS only when reads indicate the lock might be free. Assume the following foundational facts about MESI coherence traffic:\n- A write to a cache line requires the writer to obtain exclusive ownership via a Read-For-Ownership (RFO), which invalidates all other shared copies of that line and transfers the line’s data to the writer.\n- A read of a cache line that is not present in a cache initiates a Read-Shared (RS) transaction that transfers the line’s data to the reader.\n- In a heavily contended TTAS lock, reads keep the line in the Shared state across many sharers when the lock appears free, and each successful acquisition or release is a write that triggers an RFO and invalidates all sharers, after which each invalidated spinner re-fetches the line with exactly one RS before the next write.\n\nSuppose $N=40$ threads (each pinned to its own core) spin on a single cache line of size $S=64$ bytes. The average critical section time for the lock holder is $t_{cs}=150\\times 10^{-9}$ seconds, and contention is sufficient that there is always at least one thread ready to acquire the lock immediately upon release (the lock is throughput-limited by the critical section). Each spinner polls frequently enough that it performs at least one read per lock phase between consecutive writes, so each invalidation eventually causes exactly one new RS per spinner before the next write.\n\nCoherence message sizes on the interconnect are:\n- RFO request header size $h_{RFO}=16$ bytes; each RFO triggers invalidation messages of size $h_{inv}=8$ bytes to each sharer and acknowledgments of size $h_{ack}=8$ bytes from each invalidated sharer; each RFO also results in a single data response of $S$ bytes to the writer.\n- RS request header size $h_{RS}=16$ bytes; each RS results in a single data response of $S$ bytes to the reader.\n\nIgnore all other traffic (e.g., memory write-backs unrelated to the lock, directory state updates beyond the listed message sizes, and transport-layer flits). Treat all listed message sizes as the total bytes carried on the interconnect per message.\n\nStarting from the stated MESI and TTAS behavior, derive the steady-state rates of invalidations and Read-For-Ownerships (RFOs) per second, and then compute the total coherence bandwidth in bytes per second consumed by this lock, aggregating the bytes due to:\n- RFOs and the invalidation broadcasts and acknowledgments they cause,\n- RS requests and their data responses that occur as a consequence of invalidations.\n\nExpress the final bandwidth in $\\mathrm{GBytes/s}$ and round your answer to four significant figures. The final answer must be a single real-valued number.",
            "solution": "The user wants me to validate and solve a problem concerning cache coherence bandwidth in a multicore system.\n\n### Step 1: Problem Validation\n\nFirst, I must meticulously validate the problem statement.\n\n#### Extracted Givens:\n-   **System:** A multicore processor with $N=40$ threads, each on its own core. The architecture is Cache Coherent Non-Uniform Memory Access (ccNUMA) with a single on-chip interconnect.\n-   **Protocol:** Directory-based MESI (Modified–Exclusive–Shared–Invalid).\n-   **Synchronization Primitive:** A contended spin lock using the Test-and-Test-and-Set (TTAS) idiom, implemented with Compare-And-Swap (CAS). The lock variable resides in a single cache line.\n-   **Performance Parameters:**\n    -   Cache line size: $S = 64$ bytes.\n    -   Average critical section time: $t_{cs} = 150 \\times 10^{-9}$ seconds.\n    -   Number of threads/cores: $N = 40$.\n-   **Coherence Message Sizes:**\n    -   Read-For-Ownership (RFO) request header: $h_{RFO} = 16$ bytes.\n    -   Invalidation message: $h_{inv} = 8$ bytes.\n    -   Invalidation acknowledgment: $h_{ack} = 8$ bytes.\n    -   RFO data response: $S = 64$ bytes.\n    -   Read-Shared (RS) request header: $h_{RS} = 16$ bytes.\n    -   RS data response: $S = 64$ bytes.\n-   **Assumptions and System Behavior:**\n    1.  A write requires an RFO, which invalidates all other shared copies and transfers data to the writer.\n    2.  A read miss initiates an RS, which transfers data to the reader.\n    3.  Contention is high, making the lock throughput-limited by the critical section.\n    4.  In a TTAS lock, each successful acquisition or release is a write triggering an RFO.\n    5.  After an RFO invalidates sharers, each invalidated spinner re-fetches the line with exactly one RS before the next write occurs.\n\n#### Validation Using Extracted Givens:\n-   **Scientific Grounding:** The problem is firmly grounded in the principles of computer architecture and parallel computing. MESI, ccNUMA, TTAS, RFO, and RS are standard, well-defined concepts. The behavior described is a simplified but standard model of spin-lock contention.\n-   **Well-Posedness:** The problem is well-posed. It provides all necessary numerical values ($N, S, t_{cs}$, and all message sizes) and a clear set of behavioral assumptions to permit the calculation of a unique, meaningful value for coherence bandwidth.\n-   **Objectivity:** The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n-   **Completeness and Consistency:** The problem is self-contained. The assumptions, while simplifying a complex process, are stated explicitly and are internally consistent. For example, the assumption that each invalidation leads to one RS per spinner provides a clear model for calculation.\n-   **Feasibility:** The scenario and parameters are realistic for a high-performance-computing context. The message sizes and timing are plausible.\n\n#### Verdict:\nThe problem is scientifically sound, well-posed, and internally consistent. It provides a valid basis for a quantitative analysis. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe goal is to compute the total coherence bandwidth consumed by the contended spin lock. The strategy is to determine the rate of coherence-inducing events and multiply it by the total data transferred per event.\n\n#### 1. Rate of Coherence Events\nThe problem states that the lock is throughput-limited by the critical section time, $t_{cs}$. This implies a lock handoff occurs on average every $t_{cs}$ seconds. A single lock handoff cycle consists of one thread releasing the lock and another thread immediately acquiring it.\n-   **Lock Release:** This is a write operation to set the lock variable to `free`.\n-   **Lock Acquisition:** This is a successful Compare-And-Swap (CAS), which is a read-modify-write operation, fundamentally a write.\n\nAccording to the problem, \"each successful acquisition or release is a write that triggers an RFO\". Therefore, in each time interval of $t_{cs}$, two RFO events occur. The rate of RFOs, $f_{RFO}$, is:\n$$f_{RFO} = \\frac{2}{t_{cs}}$$\nThis is also the rate of invalidation broadcasts, as each RFO triggers one. Numerically, this rate is:\n$$f_{RFO} = \\frac{2}{150 \\times 10^{-9} \\, s} = \\frac{2}{150} \\times 10^{9} \\, s^{-1} \\approx 1.33 \\times 10^{7} \\, s^{-1}$$\n\n#### 2. Coherence Traffic per RFO Event\nWe must calculate the total interconnect traffic generated by a single RFO and its direct consequences. The problem states that an RFO invalidates \"all other shared copies\". In this heavily contended scenario with $N$ threads, an RFO issued by one thread will need to invalidate the copies in the other $N-1$ threads.\n\nThe total traffic for one such event, $B_{event}$, is the sum of the traffic related to the RFO itself and the traffic from the subsequent Read-Shared (RS) requests.\n\n**A. Traffic for the RFO and its associated invalidations ($B_{RFO\\_traffic}$):**\n-   One RFO request header sent by the writing core: $h_{RFO}$.\n-   Invalidation messages sent to the $N-1$ other cores: $(N-1) \\times h_{inv}$.\n-   Acknowledgment messages sent back from the $N-1$ invalidated cores: $(N-1) \\times h_{ack}$.\n-   A single data response with the cache line to the writer: $S$.\n\n$$B_{RFO\\_traffic} = h_{RFO} + (N-1)(h_{inv} + h_{ack}) + S$$\nSubstituting the given values:\n$$B_{RFO\\_traffic} = 16 \\, \\text{bytes} + (40-1)(8 \\, \\text{bytes} + 8 \\, \\text{bytes}) + 64 \\, \\text{bytes}$$\n$$B_{RFO\\_traffic} = 16 + 39 \\times 16 + 64 = 16 + 624 + 64 = 704 \\, \\text{bytes}$$\n\n**B. Traffic for the consequential Read-Shared requests ($B_{RS\\_traffic}$):**\nThe RFO invalidates $N-1$ cores. The problem states \"each invalidated spinner re-fetches the line with exactly one RS\". This means $N-1$ cores will issue an RS request. Each RS request generates traffic for the request header and a data response.\n-   RS request headers from $N-1$ cores: $(N-1) \\times h_{RS}$.\n-   Data responses to these $N-1$ cores: $(N-1) \\times S$.\n\n$$B_{RS\\_traffic} = (N-1)(h_{RS} + S)$$\nSubstituting the given values:\n$$B_{RS\\_traffic} = (40-1)(16 \\, \\text{bytes} + 64 \\, \\text{bytes})$$\n$$B_{RS\\_traffic} = 39 \\times 80 = 3120 \\, \\text{bytes}$$\n\n**C. Total traffic per event:**\nThe total traffic associated with a single RFO-invalidation-refetch sequence is:\n$$B_{event} = B_{RFO\\_traffic} + B_{RS\\_traffic} = 704 \\, \\text{bytes} + 3120 \\, \\text{bytes} = 3824 \\, \\text{bytes}$$\n\n#### 3. Total Coherence Bandwidth\nThe total coherence bandwidth, $BW$, is the total traffic generated per unit time. In each cycle of duration $t_{cs}$, two such events occur (one for release, one for acquisition).\nThe total traffic per cycle is $B_{cycle} = 2 \\times B_{event}$.\nThe bandwidth is this traffic divided by the cycle time $t_{cs}$.\n$$BW = \\frac{B_{cycle}}{t_{cs}} = \\frac{2 \\times B_{event}}{t_{cs}}$$\nSubstituting the values:\n$$BW = \\frac{2 \\times 3824 \\, \\text{bytes}}{150 \\times 10^{-9} \\, s} = \\frac{7648}{150} \\times 10^{9} \\, \\text{Bytes/s}$$\n$$BW \\approx 50.9866... \\times 10^{9} \\, \\text{Bytes/s}$$\n\nThe problem requests the answer in GBytes/s, where $1$ GByte $= 10^9$ Bytes, rounded to four significant figures.\n$$BW \\approx 50.99 \\, \\text{GBytes/s}$$",
            "answer": "$$\\boxed{50.99}$$"
        }
    ]
}