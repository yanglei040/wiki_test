{
    "hands_on_practices": [
        {
            "introduction": "This first practice places you in the role of a system administrator analyzing a real-world performance issue. By interpreting Central Processing Unit (CPU) residency and migration logs, you will learn to connect the theoretical concepts of Non-Uniform Memory Access (NUMA), cache locality, and affinity policies to diagnose why a service's performance degraded after an unexpected configuration change . This exercise sharpens your ability to reason backward from observed symptoms to their root cause in system architecture.",
            "id": "3672794",
            "problem": "A Linux-based service runs on a dual-socket Non-Uniform Memory Access (NUMA) system with $2$ sockets (NUMA nodes), each with $8$ physical cores, for a total of $16$ logical Central Processing Units (CPUs). The service launches $8$ identical, CPU-bound worker threads and uses first-touch allocation, so the majority of its hot pages were faulted while the threads were initially running on NUMA node $0$. Automatic NUMA balancing is disabled. Initially, each worker is hard-pinned (hard processor affinity) to a distinct core on NUMA node $0$ using a core-allowlist. At time $t_0$, a misconfigured cron job executes and resets the hard affinity masks of all service threads to allow all $16$ logical CPUs, leaving only the scheduler’s soft affinity in effect (soft affinity is the scheduler’s preference to keep a task on the same CPU for cache locality but is not a strict constraint). Background batch jobs are pinned to NUMA node $1$, creating stable contention there.\n\nYou are given CPU residency and migration logs aggregated per worker thread over two $5$-minute windows: one immediately before $t_0$ and one starting immediately after $t_0$.\n\nBefore $t_0$:\n- For each worker, time spent on its primary core is $ 95\\%$, time on other cores is $ 1\\%$ each, and total migrations per minute are $ 1$.\n- Cross-node residency fraction (time spent running on CPUs of NUMA node $1$) is approximately $0\\%$.\n\nAfter $t_0$:\n- For each worker, time spent on the single most-used core is in the range $30\\%$–$40\\%$, with $5\\%$–$20\\%$ on several other cores; total distinct cores seen per worker is $ 10$.\n- Cross-node residency fraction per worker is approximately $60\\%$.\n- Total migrations per minute per worker are approximately $120$.\n- Aggregate CPU utilization of the $8$ workers remains close to $8$ fully busy CPUs over the window.\n\nAssume a conventional cache hierarchy where each core has private caches and shares a socket-local Last Level Cache (LLC), and that Translation Lookaside Buffer (TLB) state is per-core. Also assume remote memory access across sockets is slower than local access due to NUMA effects. No other system parameters change during the observation windows.\n\nBased only on these logs and the fundamental definitions of processor affinity and NUMA, which of the following best reconstructs the likely performance impact on the service after $t_0$?\n\nA. Throughput decreases and latency variance increases because removing hard affinity allows frequent cross-core and cross-socket migrations, degrading cache and Translation Lookaside Buffer (TLB) locality and increasing remote-memory accesses; the scheduler’s soft affinity is insufficient to keep threads resident where their memory is local under load balancing, so work per CPU-cycle drops even though aggregate utilization stays high.\n\nB. Throughput increases and latency decreases because the scheduler can distribute the workers over more cores, soft affinity perfectly preserves cache locality, and cross-socket cache sharing keeps the LLC warm, so remote-memory penalties are negligible.\n\nC. Throughput increases slightly because queueing delay is reduced when threads spread over more cores, and any remote-memory penalty is eliminated by automatic page migration, which immediately relocates hot pages to the running socket without measurable overhead.\n\nD. Throughput and latency remain essentially unchanged because soft affinity prevents migrations in practice, and high migration counts do not correlate with cache or NUMA penalties when total CPU utilization is unchanged.\n\nSelect the single best answer.",
            "solution": "The problem requires an analysis of the performance impact of changing from hard to soft processor affinity for a multi-threaded service on a Non-Uniform Memory Access (NUMA) system. The solution must be derived from the provided log data and fundamental principles of operating systems and computer architecture.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- **System Architecture:**\n  - $2$ sockets (NUMA nodes).\n  - $8$ physical cores per socket.\n  - $16$ total logical Central Processing Units (CPUs).\n  - Non-Uniform Memory Access (NUMA) with slower remote memory access.\n  - Cache hierarchy: Private caches per core, shared socket-local Last Level Cache (LLC).\n  - Translation Lookaside Buffer (TLB) state is per-core.\n- **Workload:**\n  - $8$ identical, CPU-bound worker threads.\n- **Initial State (before time $t_0$):**\n  - Memory Allocation: First-touch allocation resulted in hot pages being resident on NUMA node $0$.\n  - Affinity: Hard affinity pinning each of the $8$ threads to a distinct core on NUMA node $0$.\n  - System Setting: Automatic NUMA balancing is disabled.\n  - Log Data (before $t_0$):\n    - Primary core residency per worker: $ 95\\%$.\n    - Migrations per worker: $ 1$ per minute.\n    - Cross-node residency (on node $1$) per worker: $\\approx 0\\%$.\n- **State Change (at time $t_0$):**\n  - Hard affinity masks are reset, allowing threads to run on all $16$ logical CPUs.\n  - Only the scheduler's soft affinity remains in effect.\n  - Stable contention from background jobs pinned to NUMA node $1$.\n- **Final State (after time $t_0$):**\n  - Log Data (after $t_0$):\n    - Most-used core residency per worker: $30\\%–40\\%$.\n    - Distinct cores seen per worker: $ 10$.\n    - Cross-node residency (on node $1$) per worker: $\\approx 60\\%$.\n    - Migrations per worker: $\\approx 120$ per minute.\n    - Aggregate CPU utilization of the $8$ workers: $\\approx 8$ fully busy CPUs.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective.\n- **Scientific Soundness:** The scenario is based on standard concepts in modern operating systems and computer architecture, including NUMA, processor affinity, CPU scheduling, cache hierarchies, and TLBs. The described behavior is consistent with how a Linux scheduler would act under these conditions.\n- **Well-Posed and Complete:** The problem provides a clear \"before\" and \"after\" state, with sufficient quantitative data (logs) and qualitative descriptions (system configuration) to deduce the performance impact. All necessary assumptions are stated.\n- **Objectivity and Consistency:** The problem uses precise, technical language. The log data is consistent with the described change: removing hard affinity for CPU-bound tasks would lead a load-balancing scheduler to migrate them frequently, exactly as the logs show. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. A solution can be derived.\n\n**Derivation of Performance Impact**\n\n1.  **Analysis of the State Before $t_0$:**\n    The initial configuration represents a near-ideal setup for this workload on a NUMA system.\n    - **Hard Affinity:** By pinning each of the $8$ threads to one of the $8$ distinct cores on NUMA node $0$, there is no contention between the service's own threads for core execution resources.\n    - **Data Locality:** First-touch allocation during this pinned phase ensures that the memory pages accessed by the threads are allocated on the local RAM of NUMA node $0$.\n    - **Performance Characteristics:**\n        - **Cache:** Each thread's working set remains hot in its core's private caches (L1/L2) and the shared LLC of node $0$. The low migration rate ($1/\\text{min}$) confirms this stable residency.\n        - **TLB:** The TLB on each core remains warm with the virtual-to-physical address translations for its resident thread.\n        - **Memory Access:** All memory accesses are to local RAM on node $0$, which is the fastest possible path. The cross-node residency of $\\approx 0\\%$ confirms this.\n    This state is characterized by high efficiency: a high number of useful instructions per CPU cycle (IPC).\n\n2.  **Analysis of the State After $t_0$:**\n    Removing hard affinity allows the operating system's scheduler to manage the placement of the $8$ threads across all $16$ CPUs, guided only by its load-balancing algorithms and soft affinity.\n    - **Scheduler Behavior:** The scheduler's goal is to keep all CPUs evenly loaded. Seeing $8$ CPU-bound tasks, it attempts to distribute them. The presence of other jobs on node $1$ complicates balancing but does not prevent the scheduler from moving the service threads to node $1$ if it perceives an imbalance. The log data showing $\\approx 120$ migrations per minute per worker and each worker using $10$ distinct cores confirms that the scheduler is aggressively moving threads. Soft affinity, a mere preference, is clearly being overridden by the load balancer.\n    - **Performance Consequences:**\n        - **Cache and TLB Thrashing:** With frequent migrations ($\\approx 120/\\text{min}$, or $2$ per second), a thread is constantly moved to a new core. This invalidates the new core's L1/L2 cache and TLB state with respect to the thread's working set, forcing expensive re-fetches from higher-level memory and page table walks. This is known as cache and TLB thrashing.\n        - **NUMA Penalty:** The most critical impact comes from cross-socket migrations. The logs show that threads spend $\\approx 60\\%$ of their time running on cores in NUMA node $1$. However, their memory remains on NUMA node $0$ (since auto-balancing is off). Therefore, for $\\approx 60\\%$ of their execution time, these threads are performing remote memory accesses across the inter-socket link. These remote accesses have significantly higher latency and lower bandwidth than local accesses.\n        - **Impact on CPU Utilization:** The log states that aggregate CPU utilization remains high ($\\approx 8$ fully busy CPUs). This is a crucial observation. It means the CPUs are spending cycles, but it does *not* imply they are making useful progress. A significant fraction of these cycles is now wasted due to pipeline stalls while waiting for data from remote memory or recovering from cache/TLB misses. The work done per CPU cycle (IPC) must therefore drop dramatically.\n    - **Overall Performance Impact:** The combination of cache/TLB thrashing and, most importantly, frequent, high-latency remote NUMA memory accesses will lead to a severe degradation in the actual work accomplished. This means **throughput must decrease**. Because the scheduling and migration events introduce variability, the time to complete any given unit of work will become less predictable, meaning **latency variance will increase**.\n\n**Option-by-Option Analysis**\n\n- **Option A:** \"Throughput decreases and latency variance increases because removing hard affinity allows frequent cross-core and cross-socket migrations, degrading cache and Translation Lookaside Buffer (TLB) locality and increasing remote-memory accesses; the scheduler’s soft affinity is insufficient to keep threads resident where their memory is local under load balancing, so work per CPU-cycle drops even though aggregate utilization stays high.\"\n  - This statement accurately synthesizes all the consequences derived from the problem description and logs. It correctly identifies the decrease in throughput, the increase in latency variance, and the underlying causes: frequent migrations, degraded cache/TLB locality, and increased remote memory access. It correctly states that soft affinity is insufficient and correctly interprets \"high utilization\" as not equivalent to \"high throughput.\"\n  - **Verdict: Correct**\n\n- **Option B:** \"Throughput increases and latency decreases because the scheduler can distribute the workers over more cores, soft affinity perfectly preserves cache locality, and cross-socket cache sharing keeps the LLC warm, so remote-memory penalties are negligible.\"\n  - This statement contains multiple factual errors. Throughput will not increase. Soft affinity is shown by the logs to be ineffective, not perfect. LLCs are not typically shared across sockets. Remote memory penalties are the defining feature of NUMA and are not negligible, especially with $\\approx 60\\%$ cross-node residency.\n  - **Verdict: Incorrect**\n\n- **Option C:** \"Throughput increases slightly because queueing delay is reduced when threads spread over more cores, and any remote-memory penalty is eliminated by automatic page migration, which immediately relocates hot pages to the running socket without measurable overhead.\"\n  - This is incorrect. The problem explicitly states \"Automatic NUMA balancing is disabled,\" so pages are not migrated. Furthermore, even if it were enabled, page migration is a heavyweight, non-instantaneous process with measurable overhead. Any minor benefit from spreading threads is dwarfed by the massive NUMA penalty.\n  - **Verdict: Incorrect**\n\n- **Option D:** \"Throughput and latency remain essentially unchanged because soft affinity prevents migrations in practice, and high migration counts do not correlate with cache or NUMA penalties when total CPU utilization is unchanged.\"\n  - This statement is directly contradicted by the evidence. The logs show soft affinity does *not* prevent migrations ($\\approx 120/\\text{min}$). The assertion that migration counts do not correlate with performance penalties is fundamentally wrong in the context of cache and NUMA locality. It also misinterprets the meaning of \"CPU utilization.\"\n  - **Verdict: Incorrect**\n\nThe only option that correctly reconstructs the performance impact based on the provided a priori knowledge and log data is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving from retrospective analysis to proactive tuning, this exercise challenges you to optimize a latency-sensitive service. Given a set of performance counters, your task is to identify the primary bottleneck—is it CPU saturation or poor memory locality?—and select the most effective tuning action . This practice is crucial for developing the methodical approach required for live performance engineering, where choosing the right fix is as important as understanding the problem.",
            "id": "3672826",
            "problem": "A latency-sensitive Remote Procedure Call (RPC) microservice runs on a symmetric multiprocessor with two Non-Uniform Memory Access (NUMA) nodes. The machine has $32$ physical cores, split evenly as $16$ cores on NUMA node $0$ and $16$ cores on NUMA node $1$. The service is placed under a hard processor affinity mask (hard affinity) that restricts it to $8$ logical CPUs on NUMA node $0$. The operating system scheduler uses soft processor affinity (soft affinity) to prefer recently used cores but may migrate tasks to balance load.\n\nYou can tune either the soft affinity “stickiness” (to reduce migrations and increase cache locality) or the hard affinity mask (to change the set of CPUs on which the service may run). Over an observation window of $\\Delta t = 60$ s, you collect performance counters (performance counters (perf)) and service-level indicators:\n\n- Central Processing Unit (CPU) utilization on the $8$ allowed CPUs averages $92\\%$, with $3$ of those CPUs spending more than $97\\%$ busy and an average run-queue length of approximately $1.8$ runnable tasks per allowed CPU. The remaining $24$ CPUs on the machine average $28\\%$ utilization.\n- Cross-core migrations per request within the allowed set average $0.06$; cross-NUMA migrations are $0$ by construction due to the hard mask.\n- Last Level Cache (LLC) misses are low: median $3$ misses per kilo-instruction (MPKI) and ninety-ninth percentile ($P_{99}$) $6$ MPKI.\n- The measured ninety-ninth percentile latency $P_{99}$ of the service is $15$ ms. The service-level objective is a tail-latency target $P_{99} \\le X$ ms with $X = 10$ ms.\n\nFrom first principles, recall that:\n- Processor affinity constrains or biases where threads run. Hard affinity defines the legal CPU set. Soft affinity biases the scheduler toward recently used CPUs to preserve cache locality.\n- Cache locality reduces LLC misses; frequent migrations tend to increase LLC misses because caches are per-core.\n- In queueing terms, as utilization $\\rho$ of a server approaches $1$, waiting time grows rapidly; distributing work across more identical servers can reduce per-server $\\rho$ and lower tail latency.\n\nGiven the above, choose the single best next step to meet the $P_{99} \\le 10$ ms target with minimal risk of regression.\n\nA. Tighten soft affinity (increase stickiness) while keeping the same hard mask.\n\nB. Expand the hard affinity mask to include more idle CPUs on the same NUMA node and keep soft affinity unchanged.\n\nC. Expand the hard affinity mask across both NUMA nodes to include CPUs on the other NUMA node.\n\nD. Make no change and rely on further cache warm-up to lower $P_{99}$.",
            "solution": "The problem asks for the single best action to reduce the ninety-ninth percentile ($P_{99}$) latency of a Remote Procedure Call (RPC) microservice from a measured $15$ ms to meet a Service-Level Objective (SLO) of $P_{99} \\le 10$ ms.\n\nFirst, we must perform a rigorous validation of the problem statement.\n\n**Step 1: Extract Givens**\n- System Architecture: Symmetric multiprocessor, two Non-Uniform Memory Access (NUMA) nodes.\n- Core Configuration: $32$ total physical cores, with $16$ on NUMA node $0$ and $16$ on NUMA node $1$.\n- Service Configuration: The service is restricted by a hard processor affinity mask to $8$ logical CPUs on NUMA node $0$.\n- Scheduler: The operating system uses soft processor affinity.\n- Tunable Parameters: Soft affinity stickiness and hard affinity mask.\n- Observation Window: $\\Delta t = 60$ s.\n- Performance Metrics:\n    - CPU Utilization (Affinity Set): Average $92\\%$ on the $8$ allowed CPUs; $3$ of these CPUs are at $ 97\\%$ utilization.\n    - Run-Queue Length: Average of approximately $1.8$ runnable tasks per allowed CPU.\n    - CPU Utilization (Other): Average $28\\%$ on the remaining $24$ CPUs.\n    - Migrations: Average $0.06$ cross-core migrations per request within the affinity set; $0$ cross-NUMA migrations.\n    - Cache Performance: Median Last Level Cache (LLC) misses of $3$ misses per kilo-instruction (MPKI); $P_{99}$ LLC misses of $6$ MPKI.\n    - Service Latency: Measured $P_{99} = 15$ ms.\n    - Latency Target: $P_{99} \\le X$ ms, where $X = 10$ ms.\n- First Principles Provided:\n    1.  Processor Affinity: Hard affinity sets the allowed CPUs; soft affinity biases the scheduler to recently used CPUs.\n    2.  Cache Locality: Migrations increase LLC misses.\n    3.  Queueing Theory: Waiting time increases rapidly as utilization $\\rho \\to 1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective. It presents a classic performance tuning scenario in a modern multi-core, NUMA system.\n- The concepts of NUMA, processor affinity, LLC misses, MPKI, run-queue length, and tail latency are standard in computer science and operating systems.\n- The data provided is consistent and points towards a plausible performance bottleneck. High CPU utilization ($92\\%$) is causally linked to a long run-queue ($1.8  1$), which, according to queueing theory, directly leads to increased waiting times and thus high tail latency ($P_{99}$).\n- The low LLC miss rate ($3$-$6$ MPKI) and low migration rate ($0.06$ per request) consistently suggest that cache locality is not the primary issue.\n- The hard affinity mask to NUMA node $0$ correctly explains the $0$ cross-NUMA migrations.\n- The problem is not underspecified; there is sufficient information to diagnose the root cause and evaluate the options. It is not contradictory, infeasible, or trivial.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. We may proceed with the solution.\n\n**Derivation of the Solution**\n\nThe primary goal is to reduce the $P_{99}$ latency. We must first identify the root cause of the current high latency by analyzing the provided metrics.\n\n1.  **Diagnosis of the Bottleneck:**\n    The most striking evidence is the state of CPU contention. The $8$ CPUs allocated to the service have an average utilization of $92\\%$. Three of these are nearly completely saturated ($97\\%$). A utilization level approaching $\\rho = 1$ is a strong indicator of a performance bottleneck. This is corroborated by the average run-queue length of $1.8$ tasks per CPU. A run-queue length greater than $1$ means that, on average, there are more runnable tasks than available CPUs, forcing tasks to wait. This waiting time, or *queueing delay*, is a direct contributor to the overall response time of a request. Tail latency ($P_{99}$) is particularly sensitive to queueing delays, as it reflects the experience of the unluckiest requests, which are those that experience the longest waits in the queue.\n\n    In contrast, the metrics related to memory performance are favorable. The LLC miss rates of $3$ MPKI (median) and $6$ MPKI ($P_{99}$) are described as low, which is a reasonable assessment for many server workloads. This, combined with the low rate of cross-core migrations ($0.06$ per request), indicates that the application exhibits good cache locality and the OS's soft affinity is effective.\n\n    Therefore, the conclusion is clear: the dominant bottleneck is **CPU saturation**, not poor cache performance or excessive memory latency. The service is \"starved\" for CPU cycles, leading to high queueing delays and, consequently, high $P_{99}$ latency.\n\n2.  **Evaluation of Proposed Actions:**\n    The optimal solution must address the CPU saturation bottleneck directly and with minimal risk of introducing new problems.\n\n    **A. Tighten soft affinity (increase stickiness) while keeping the same hard mask.**\n    This action aims to improve cache locality by making migrations even less frequent. However, the data shows that cache locality is already good and is not the source of the problem. By increasing stickiness, this action could worsen the load imbalance. It would make it more difficult for the scheduler to move a task from a saturated ($97\\%$) core to a slightly less busy (but still overloaded) core within the $8$-CPU set. This could increase, not decrease, the worst-case queueing delays. This option is misguided as it targets a non-existent problem.\n    **Verdict: Incorrect.**\n\n    **B. Expand the hard affinity mask to include more idle CPUs on the same NUMA node and keep soft affinity unchanged.**\n    This action directly targets the root cause of the problem: CPU saturation. On NUMA node $0$, there are $16$ total cores, with only $8$ in use by the service. This leaves $16 - 8 = 8$ idle cores on the same NUMA node. Expanding the affinity mask to include some or all of these cores would allow the workload to be spread across a larger number of processors. This would decrease the average utilization $\\rho$ per core, moving the system out of the critical region where $\\rho \\approx 1$. According to the principles of queueing theory, this reduction in utilization will cause a super-linear decrease in waiting time, directly lowering tail latency. Because the new CPUs are on the *same* NUMA node, there is no penalty in terms of memory access latency; all threads will continue to have fast, local access to memory on node $0$. This is a direct, highly effective, and low-risk solution.\n    **Verdict: Correct.**\n\n    **C. Expand the hard affinity mask across both NUMA nodes to include CPUs on the other NUMA node.**\n    Like option B, this would alleviate CPU saturation. However, it introduces a significant new risk. The service's memory is presumably allocated on NUMA node $0$. If a thread is scheduled on a CPU in NUMA node $1$, its memory accesses become *remote*, traversing the inter-node interconnect. Remote memory access has substantially higher latency and lower bandwidth than local access. This effect, known as the \"NUMA tax\", could introduce a new memory-access bottleneck that negates the benefit of having more CPU cores, and could even increase overall latency. Since there are idle cores on the local NUMA node, this cross-NUMA expansion is an unnecessary and high-risk strategy.\n    **Verdict: Incorrect.**\n\n    **D. Make no change and rely on further cache warm-up to lower $P_{99}$.**\n    This option is based on a flawed premise. An observation window of $\\Delta t = 60$ s is more than sufficient for a high-traffic service to achieve a \"warm\" cache steady state. The critical metrics of high CPU utilization ($92\\%$) and a long run-queue ($1.8$) indicate a structural resource shortage, not a transient warm-up effect. The low LLC miss rate further confirms that the caches are performing well. Waiting will not resolve a fundamental mismatch between workload demand and provisioned CPU resources.\n    **Verdict: Incorrect.**\n\nBased on this analysis, the only option that correctly identifies and addresses the CPU saturation bottleneck with minimal risk is to expand the CPU set on the local NUMA node.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Theoretical understanding must be paired with the ability to make quantitative predictions. This problem presents a scenario where a seemingly subtle error in setting a hard affinity mask interacts with a processor's symmetric multithreading (SMT) capabilities . Your task is to calculate the precise drop in instruction throughput, reinforcing the connection between operating system policies and the underlying hardware's performance characteristics.",
            "id": "3672757",
            "problem": "A compute-bound microbenchmark is used to measure the effect of processor affinity on a workstation running a Linux-like scheduler. The machine has symmetric multithreading (SMT) with two hardware threads per physical core. The processor has $12$ physical cores (for a total of $24$ logical processors), and all cores run at a fixed frequency $f = 3.2 \\times 10^{9}$ cycles per second. The workload spawns $24$ identical threads, each performing arithmetic instructions in tight loops so that instructions per cycle (IPC) dominates throughput. Under full SMT utilization (both hardware threads active on a core), each thread achieves an average $\\text{IPC}$ of $1.15$ due to resource sharing between siblings. When a core runs only a single hardware thread (its sibling is idle or disallowed), that thread’s average $\\text{IPC}$ rises to $1.75$.\n\nTwo affinity configurations are tested:\n\n- Soft affinity: Threads express preferred cores but the scheduler may freely migrate them to balance load; in practice, all $24$ logical processors are utilized concurrently, and all threads run with $\\text{IPC} = 1.15$.\n- Hard affinity: Threads are pinned using strict masks that inadvertently exclude the second hardware thread on $7$ of the $12$ cores. Consequently, $7$ cores run only one thread (each with $\\text{IPC} = 1.75$), while the remaining $5$ cores run both hardware threads (each with $\\text{IPC} = 1.15$). Threads excluded by the mask do not run.\n\nAssume the widely used throughput approximation $T \\approx f \\times \\text{IPC} \\times \\text{active\\_threads}$, where $T$ is instructions per second aggregated across all running threads and $\\text{active\\_threads}$ is the number of concurrently scheduled threads. There are no other bottlenecks (for example, memory bandwidth and input/output are not limiting), and frequency $f$ is identical across all cores and does not change with load.\n\nUsing only the information above and first principles of scheduling and SMT contention, compute the absolute throughput drop $\\Delta T$ (baseline minus misconfigured) caused by the hard affinity masks. Express your answer in giga-instructions per second (GIPS) and round your final numerical value to four significant figures.",
            "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded in the principles of computer architecture and operating systems, specifically concerning symmetric multithreading (SMT), instructions per cycle (IPC), and processor affinity. All provided data are consistent, sufficient, and physically plausible, forming a well-posed problem that admits a unique, meaningful solution.\n\nThe fundamental principle for solving this problem is that the total instruction throughput of a multi-core processor is the sum of the instruction throughputs of all its concurrently active hardware threads. The throughput of a single thread, $T_{thread}$, is the product of the processor frequency, $f$, and the instructions per cycle, $\\text{IPC}$, achieved by that thread.\n$$\nT_{thread} = f \\times \\text{IPC}\n$$\nThe total system throughput, $T_{system}$, is therefore the sum of the individual thread throughputs:\n$$\nT_{system} = \\sum_{i=1}^{N_{active\\_threads}} T_{thread,i} = f \\times \\sum_{i=1}^{N_{active\\_threads}} \\text{IPC}_i\n$$\nThis is a more precise formulation of the approximation given in the problem, $T \\approx f \\times \\text{IPC} \\times \\text{active\\_threads}$, especially when IPC is not uniform across all threads.\n\nWe are given the following values:\n- Processor frequency, $f = 3.2 \\times 10^{9}$ cycles/second.\n- IPC for a thread when its sibling is active (SMT on): $\\text{IPC}_{SMT} = 1.15$.\n- IPC for a thread when its sibling is idle (SMT off): $\\text{IPC}_{single} = 1.75$.\n- Total number of physical cores: $N_{cores} = 12$.\n\nWe first calculate the baseline throughput for the soft affinity configuration, denoted as $T_{soft}$. In this scenario, all $12$ cores are fully utilized with two threads each, for a total of $N_{soft} = 12 \\times 2 = 24$ active threads. Each of these threads experiences SMT contention, so their IPC is uniformly $\\text{IPC}_{SMT}$.\n$$\nT_{soft} = N_{soft} \\times (f \\times \\text{IPC}_{SMT})\n$$\n$$\nT_{soft} = 24 \\times f \\times 1.15 = 27.6 \\times f\n$$\n\nNext, we calculate the throughput for the misconfigured hard affinity case, denoted as $T_{hard}$. The processor cores are partitioned into two groups.\nGroup 1 consists of $N_{cores,1} = 7$ cores, each running only one thread. The total number of threads in this group is $N_{threads,1} = 7 \\times 1 = 7$. As these threads run without a sibling, their IPC is $\\text{IPC}_{single}$. The aggregate throughput from this group, $T_1$, is:\n$$\nT_1 = N_{threads,1} \\times (f \\times \\text{IPC}_{single}) = 7 \\times f \\times 1.75 = 12.25 \\times f\n$$\nGroup 2 consists of the remaining $N_{cores,2} = 12 - 7 = 5$ cores. These cores run both hardware threads, for a total of $N_{threads,2} = 5 \\times 2 = 10$ threads. These threads experience SMT contention, so their IPC is $\\text{IPC}_{SMT}$. The aggregate throughput from this group, $T_2$, is:\n$$\nT_2 = N_{threads,2} \\times (f \\times \\text{IPC}_{SMT}) = 10 \\times f \\times 1.15 = 11.5 \\times f\n$$\nThe total throughput in the hard affinity configuration is the sum of the throughputs from both groups.\n$$\nT_{hard} = T_1 + T_2 = 12.25 \\times f + 11.5 \\times f = 23.75 \\times f\n$$\nNote that in this configuration, only $N_{hard} = N_{threads,1} + N_{threads,2} = 7 + 10 = 17$ threads are active, as the others are excluded by the affinity mask.\n\nThe problem asks for the absolute throughput drop, $\\Delta T$, which is the difference between the baseline and misconfigured throughputs.\n$$\n\\Delta T = T_{soft} - T_{hard}\n$$\n$$\n\\Delta T = 27.6 \\times f - 23.75 \\times f = (27.6 - 23.75) \\times f\n$$\n$$\n\\Delta T = 3.85 \\times f\n$$\n\nNow, we substitute the numerical value for the frequency $f$.\n$$\nf = 3.2 \\times 10^{9} \\text{ cycles/second}\n$$\n$$\n\\Delta T = 3.85 \\times (3.2 \\times 10^{9}) \\text{ instructions/second}\n$$\n$$\n\\Delta T = 12.32 \\times 10^{9} \\text{ instructions/second}\n$$\nThe question requires the answer in giga-instructions per second (GIPS), where $1 \\text{ GIPS} = 10^{9}$ instructions/second.\n$$\n\\Delta T = 12.32 \\text{ GIPS}\n$$\nThe problem asks for the result to be rounded to four significant figures. The calculated value $12.32$ already has exactly four significant figures, so no further rounding is necessary.",
            "answer": "$$\\boxed{12.32}$$"
        }
    ]
}