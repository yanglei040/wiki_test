{
    "hands_on_practices": [
        {
            "introduction": "Moving from theory to practice requires a solid understanding of computational cost. This first exercise provides a foundational cost analysis by comparing a single, sophisticated Newton-Krylov step against a much simpler fixed-point iteration. By breaking down the work into function evaluations, preconditioner applications, and vector operations, you will quantify why the superior convergence of Newton's method must be weighed against its higher per-iteration complexity .",
            "id": "2417772",
            "problem": "You are solving a large nonlinear system $F(u)=0$ with $u\\in\\mathbb{R}^n$ in computational engineering. The cost of evaluating the nonlinear residual $F(u)$ is $C_F$ floating-point operations, with $C_F=\\Theta(n)$, and the cost of applying a right preconditioner $M^{-1}$ to a vector is $C_P$, with $C_P=\\Theta(n)$. A vector inner product and a scaled vector addition each cost $\\Theta(n)$ operations. Consider the following two algorithms for advancing from an iterate $u_k$ to the next iterate.\n\nAlgorithm $1$ (fixed-point/Richardson): $u_{k+1}=u_k-\\tau F(u_k)$ for a given scalar $\\tau0$. This step uses the current residual and vector operations only; it does not solve any linear systems.\n\nAlgorithm $2$ (Newton–Krylov): Form the Newton step $s_k$ by approximately solving the linear system $J(u_k)s=-F(u_k)$ using $m$ steps of right-preconditioned Generalized Minimal Residual (GMRES) with no restarts, where matrix–vector products $J(u_k)v$ are computed in a matrix-free manner by first-order forward finite differences,\n$$\nJ(u_k)v \\approx \\frac{F(u_k+h v)-F(u_k)}{h},\n$$\nwith a single new evaluation of $F$ per product and with the $F(u_k)$ already available at the start of the Newton step. The preconditioner $M^{-1}$ is applied exactly once per GMRES iteration. GMRES uses modified Gram–Schmidt orthonormalization of Krylov basis vectors.\n\nUnder these implementation details and assuming $m\\ge 1$, which option correctly accounts for the number of evaluations of $F$, the number of preconditioner applications, and the asymptotic cost in vector operations per step for Algorithm $2$ compared to Algorithm $1$?\n\nA. Algorithm $2$ requires $(m+1)$ evaluations of $F$, $m$ preconditioner applications, and $\\Theta(m^2 n)$ additional vector-operation work from orthonormalization; Algorithm $1$ requires $1$ evaluation of $F$ and $\\Theta(n)$ vector-operation work.\n\nB. Algorithm $2$ requires $m$ evaluations of $F$ because $F(u_k)$ is reused, requires no preconditioner applications, and has only $\\Theta(m n)$ vector-operation work; Algorithm $1$ requires $1$ evaluation of $F$ and solving a linear system.\n\nC. Algorithm $2$ requires $1$ evaluation of $F$ regardless of $m$ due to reuse within differences and has only $\\Theta(m n)$ vector-operation work; Algorithm $1$ requires $2$ evaluations of $F$ per step to update $u_{k+1}$.\n\nD. Algorithm $2$ requires $2m$ evaluations of $F$ due to symmetric differences for each $J(u_k)v$ and $m$ preconditioner applications, with negligible vector-operation work; Algorithm $1$ requires $0$ evaluations of $F$ if $\\tau$ is known.",
            "solution": "The problem statement must first be validated for its scientific and logical integrity.\n\n### Step 1: Extract Givens\n-   **System to solve**: A large nonlinear system $F(u)=0$, where $u \\in \\mathbb{R}^n$.\n-   **Cost of residual evaluation**: $C_F = \\Theta(n)$ floating-point operations.\n-   **Cost of preconditioner application**: $C_P = \\Theta(n)$ floating-point operations for $M^{-1}v$.\n-   **Cost of vector operations**: A vector inner product costs $\\Theta(n)$ operations, and a scaled vector addition costs $\\Theta(n)$ operations.\n-   **Algorithm 1 (Fixed-point/Richardson)**: The iteration is given by $u_{k+1} = u_k - \\tau F(u_k)$ for a scalar $\\tau  0$.\n-   **Algorithm 2 (Newton-Krylov)**:\n    -   The update is $u_{k+1} = u_k + s_k$.\n    -   The Newton step $s_k$ is an approximate solution to the linear system $J(u_k)s = -F(u_k)$.\n    -   The linear system is solved using $m$ steps of right-preconditioned Generalized Minimal Residual (GMRES) without restarts ($m \\ge 1$).\n    -   Jacobian-vector products $J(u_k)v$ are approximated by a matrix-free method using a first-order forward finite difference: $J(u_k)v \\approx \\frac{F(u_k+h v)-F(u_k)}{h}$.\n    -   Each Jacobian-vector product uses one new evaluation of $F$, and the value $F(u_k)$ is pre-computed and reused.\n    -   The preconditioner $M^{-1}$ is applied exactly once per GMRES iteration.\n    -   GMRES uses modified Gram-Schmidt for orthonormalization.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard exercise in the analysis of numerical algorithms for nonlinear systems.\n-   **Scientifically Grounded**: The descriptions of the fixed-point iteration, Newton-Krylov methods, GMRES, matrix-free finite differences, and preconditioning are all standard and correct representations from the field of numerical analysis and computational engineering.\n-   **Well-Posed**: The problem asks for a computational cost analysis of two well-defined algorithms. All necessary details, such as the type of orthonormalization (modified Gram-Schmidt) and the method for Jacobian-vector products, are provided, which allows for a unique and unambiguous derivation of the costs.\n-   **Objective**: The problem is stated in precise, technical language and requires a quantitative, objective comparison.\n-   **Consistency and Completeness**: The problem provides a self-contained and consistent set of givens. There are no contradictions or missing critical pieces of information. For example, the cost of fundamental operations ($F$-evaluation, preconditioner, vector ops) are specified, which is necessary for the analysis. The detail about modified Gram-Schmidt is crucial for correctly determining the cost of orthonormalization.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the solution and analysis.\n\n### Derivation and Option Analysis\n\nWe will analyze the computational cost per step for each algorithm in terms of the number of $F$ evaluations, preconditioner applications, and vector operations.\n\n#### Analysis of Algorithm 1 (Fixed-point/Richardson)\nThe update rule for a single step from $u_k$ to $u_{k+1}$ is $u_{k+1} = u_k - \\tau F(u_k)$.\n1.  **Evaluation of $F$**: The term $F(u_k)$ must be computed. This requires exactly $1$ evaluation of the function $F$.\n2.  **Preconditioner Applications**: This algorithm does not involve a preconditioner. The number of applications is $0$.\n3.  **Vector-operation work**: The computation involves one scaled vector addition ($u_k - (\\tau F(u_k))$). The cost of a scaled vector addition is given as $\\Theta(n)$.\n\n**Summary for Algorithm 1:**\n-   $F$ evaluations: $1$.\n-   Preconditioner applications: $0$.\n-   Vector-operation work: $\\Theta(n)$.\n\n#### Analysis of Algorithm 2 (Newton-Krylov)\nA single step of this algorithm consists of computing the Newton update $s_k$ and then setting $u_{k+1} = u_k + s_k$. The primary cost is in computing $s_k$.\n\nFirst, at the beginning of the Newton step, the right-hand side of the linear system, $-F(u_k)$, must be formed.\n-   This requires $1$ evaluation of $F(u_k)$. This value is then reused throughout the GMRES solve.\n\nNext, the linear system $J(u_k)s = -F(u_k)$ is approximately solved using $m$ iterations of right-preconditioned GMRES. The system being solved by GMRES is effectively $(J(u_k)M^{-1})y = -F(u_k)$, with the final step being $s_k=M^{-1}y$. Let us analyze the cost of these $m$ iterations.\n\nFor each GMRES iteration (from $j=1$ to $m$):\n1.  **Matrix-vector product**: GMRES needs to compute one product of the form $(J(u_k)M^{-1})v$ for some vector $v$.\n    -   First, the preconditioner is applied: $w = M^{-1}v$. This counts as $1$ preconditioner application. Per the problem statement, this costs $C_P = \\Theta(n)$.\n    -   Next, the Jacobian-vector product is computed: $J(u_k)w$. The problem specifies this is done using the formula $\\frac{F(u_k+h w)-F(u_k)}{h}$. Since $F(u_k)$ is already available, this requires $1$ new evaluation of $F$ (at $u_k+hw$). This costs $C_F = \\Theta(n)$.\n    -   Over $m$ iterations, this part of the algorithm requires $m$ preconditioner applications and $m$ new evaluations of $F$.\n\n2.  **Orthonormalization**: GMRES uses modified Gram-Schmidt (MGS) to build an orthonormal basis for the Krylov subspace. At iteration $j$, the newly generated vector is made orthogonal to the previous $j-1$ basis vectors.\n    -   This involves $j-1$ inner products and $j-1$ scaled vector additions. The cost of an inner product is $\\Theta(n)$ and a scaled vector addition is $\\Theta(n)$. Thus, the cost of MGS at iteration $j$ is $(j-1) \\times \\Theta(n) = \\Theta((j-1)n)$.\n    -   The total cost for orthonormalization over $m$ iterations is the sum:\n    $$ \\sum_{j=1}^{m} \\Theta((j-1)n) = \\Theta(n) \\sum_{k=0}^{m-1} k = \\Theta(n) \\frac{(m-1)m}{2} = \\Theta(m^2 n). $$\n    The other vector operations within GMRES, such as normalizing vectors and constructing the final solution from the basis vectors (costing $\\Theta(mn)$), are of lower or equal order. The dominant cost from vector operations comes from MGS and is $\\Theta(m^2 n)$.\n\n**Summary for Algorithm 2:**\n-   **$F$ evaluations**: $1$ for the initial residual $F(u_k)$, plus $m$ for the $m$ Jacobian-vector products within GMRES. Total: $m+1$.\n-   **Preconditioner applications**: $1$ per GMRES iteration. Total: $m$.\n-   **Vector-operation work**: Dominated by the MGS orthonormalization. Total: $\\Theta(m^2 n)$.\n\n### Option-by-Option Analysis\n\n-   **Option A**: \"Algorithm $2$ requires $(m+1)$ evaluations of $F$, $m$ preconditioner applications, and $\\Theta(m^2 n)$ additional vector-operation work from orthonormalization; Algorithm $1$ requires $1$ evaluation of $F$ and $\\Theta(n)$ vector-operation work.\"\n    -   This statement accurately reflects our analysis for both Algorithm 2 (F-evals: $m+1$, Precon-apps: $m$, Vector-ops: $\\Theta(m^2 n)$) and Algorithm 1 (F-evals: $1$, Vector-ops: $\\Theta(n)$).\n    -   Verdict: **Correct**.\n\n-   **Option B**: \"Algorithm $2$ requires $m$ evaluations of $F$ because $F(u_k)$ is reused, requires no preconditioner applications, and has only $\\Theta(m n)$ vector-operation work; Algorithm $1$ requires $1$ evaluation of $F$ and solving a linear system.\"\n    -   This option incorrectly states the number of $F$ evaluations for Algorithm 2 is $m$; it is $m+1$. It incorrectly claims no preconditioner applications are used. It incorrectly states the vector-operation work is $\\Theta(mn)$ instead of $\\Theta(m^2 n)$. For Algorithm 1, it incorrectly claims a linear system is solved.\n    -   Verdict: **Incorrect**.\n\n-   **Option C**: \"Algorithm $2$ requires $1$ evaluation of $F$ regardless of $m$ due to reuse within differences and has only $\\Theta(m n)$ vector-operation work; Algorithm $1$ requires $2$ evaluations of $F$ per step to update $u_{k+1}$.\"\n    -   This option incorrectly states the number of $F$ evaluations for Algorithm 2 is $1$; it is $m+1$. It incorrectly states the vector-operation work is $\\Theta(mn)$. For Algorithm 1, it incorrectly claims $2$ evaluations of $F$ are needed per step.\n    -   Verdict: **Incorrect**.\n\n-   **Option D**: \"Algorithm $2$ requires $2m$ evaluations of $F$ due to symmetric differences for each $J(u_k)v$ and $m$ preconditioner applications, with negligible vector-operation work; Algorithm $1$ requires $0$ evaluations of $F$ if $\\tau$ is known.\"\n    -   This option incorrectly assumes a symmetric (centered) finite difference, which requires two $F$ evaluations per product, contradicting the problem statement which specifies a forward difference. It incorrectly claims vector-operation work is negligible when it is $\\Theta(m^2 n)$. For Algorithm 1, it incorrectly claims $0$ evaluations of $F$.\n    -   Verdict: **Incorrect**.\n\nBased on the detailed analysis, only Option A provides a correct accounting of all specified costs for both algorithms.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A central decision when implementing Newton-Krylov methods is whether to explicitly form the Jacobian matrix or to approximate its action using matrix-free techniques. This practice presents a realistic scenario where you must weigh the high cost of assembling a sparse Jacobian against the cumulative cost of repeated function evaluations in a Jacobian-free approach. This exercise illuminates the practical, economic trade-offs that determine the most efficient strategy for a given problem .",
            "id": "2417751",
            "problem": "A large-scale nonlinear residual map $F:\\mathbb{R}^n \\to \\mathbb{R}^n$ arises from a finite volume discretization of a steady conservation law on an unstructured mesh with $n$ unknowns. You must solve $F(x)=0$ using Newton’s method. Consider two variants for the inner linear solves at each Newton step, both using Generalized Minimal Residual (GMRES) and the same right-preconditioner that is built from problem coefficients independently of $J(x)$.\n\nAssumptions:\n- At each Newton step $k$, the linear system $J(x_k)\\,\\delta x = -F(x_k)$ is solved to a fixed relative residual tolerance using GMRES.\n- A single evaluation of $F(x_k)$ at the beginning of each Newton step is required and reused as needed; this costs $t_F$ per evaluation.\n- The preconditioner is built once per Newton step at cost $T_P$, and each application costs $t_{P\\mathrm{apply}}$.\n- Ignore vector updates, orthogonalization, line search, and all other costs not explicitly listed.\n\nApproach S (assembled Jacobian):\n- At each Newton step, form the analytical sparse Jacobian $J(x_k)$ at cost $T_J$.\n- Each GMRES iteration uses a sparse matrix–vector multiply with $J(x_k)$ costing $t_{\\mathrm{spmv}}$ plus one preconditioner application costing $t_{P\\mathrm{apply}}$.\n- The average number of GMRES iterations per Newton step is $m_S$.\n\nApproach JF (Jacobian-free Newton–Krylov):\n- Do not form $J(x_k)$. Each Krylov product $J(x_k)v$ is approximated by a finite-difference directional derivative, which requires one additional evaluation of $F$ per GMRES iteration, costing $t_F$; the preconditioner application still costs $t_{P\\mathrm{apply}}$ per iteration.\n- The average number of GMRES iterations per Newton step is $m_{JF}$.\n\nData for a particular case:\n- Number of Newton steps: $k = 5$.\n- Residual evaluation cost: $t_F = 0.075\\,\\mathrm{s}$.\n- Sparse Jacobian assembly cost: $T_J = 2.8\\,\\mathrm{s}$.\n- Preconditioner build cost: $T_P = 0.9\\,\\mathrm{s}$.\n- Sparse matrix–vector multiply cost: $t_{\\mathrm{spmv}} = 0.010\\,\\mathrm{s}$.\n- Preconditioner apply cost: $t_{P\\mathrm{apply}} = 0.005\\,\\mathrm{s}$.\n- GMRES iterations per Newton step with assembled Jacobian: $m_S = 20$.\n- GMRES iterations per Newton step with Jacobian-free product: $m_{JF} = 30$.\n\nBased on these assumptions and data, which statement about the total wall-clock time over all $k$ Newton steps is correct?\n\nA. The assembled-Jacobian approach (Approach S) is faster by approximately $3.5\\,\\mathrm{s}$ overall.\n\nB. The Jacobian-free approach (Approach JF) is faster by approximately $3.5\\,\\mathrm{s}$ overall.\n\nC. The two approaches have approximately equal total time within $1\\%$.\n\nD. There is insufficient information to decide; the step size for the finite-difference directional derivative is required to compare times.",
            "solution": "The problem statement must first be validated for scientific soundness, self-consistency, and clarity.\n\n### Step 1: Extract Givens\n\nThe provided information consists of definitions for two computational approaches and a set of numerical data.\n\n**Problem Setup  Definitions:**\n-   A nonlinear system $F(x) = 0$ is to be solved using Newton's method, where $F: \\mathbb{R}^n \\to \\mathbb{R}^n$.\n-   The linear system at each Newton step $k$ is $J(x_k)\\,\\delta x = -F(x_k)$.\n-   The linear solver for this system is the Generalized Minimal Residual (GMRES) method.\n-   A right-preconditioner is used, built from problem coefficients and independently of the Jacobian $J(x)$.\n-   All costs not explicitly listed are to be ignored (vector updates, orthogonalization, line search, etc.).\n\n**Approach S (Assembled Jacobian):**\n-   The analytical sparse Jacobian $J(x_k)$ is formed at each Newton step.\n-   GMRES iterations use sparse matrix-vector multiplication with $J(x_k)$.\n\n**Approach JF (Jacobian-free Newton–Krylov):**\n-   $J(x_k)$ is not formed explicitly.\n-   The action of the Jacobian on a vector, $J(x_k)v$, is approximated by a finite-difference directional derivative.\n-   This approximation requires one additional evaluation of $F(x)$ per GMRES iteration.\n\n**Numerical Data:**\n-   Number of Newton steps: $k = 5$.\n-   Cost of one residual evaluation $F(x)$: $t_F = 0.075\\,\\mathrm{s}$.\n-   Cost to assemble the sparse Jacobian $J(x_k)$: $T_J = 2.8\\,\\mathrm{s}$.\n-   Cost to build the preconditioner (once per Newton step): $T_P = 0.9\\,\\mathrm{s}$.\n-   Cost of one sparse matrix-vector multiply with $J(x_k)$: $t_{\\mathrm{spmv}} = 0.010\\,\\mathrm{s}$.\n-   Cost of one preconditioner application: $t_{P\\mathrm{apply}} = 0.005\\,\\mathrm{s}$.\n-   Average GMRES iterations per Newton step for Approach S: $m_S = 20$.\n-   Average GMRES iterations per Newton step for Approach JF: $m_{JF} = 30$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem describes a standard scenario in computational science and engineering: comparing the performance of two variants of the Newton–Krylov method.\n\n1.  **Scientifically Grounded:** The problem is firmly rooted in numerical linear algebra and the study of iterative methods for solving large-scale nonlinear systems. The Newton-Krylov method, both with assembled Jacobians and in its Jacobian-free form, is a fundamental and widely used technique. The cost model is a simplified but standard way to analyze computational performance. The setup is scientifically valid.\n2.  **Well-Posed:** The question asks for a direct comparison of total computational time based on a well-defined cost model and a complete set of data. A unique numerical answer can be determined.\n3.  **Objective:** The language is technical and precise. The assumptions are stated explicitly. There is no subjectivity or ambiguity.\n4.  **Complete and Consistent:** All necessary data for the defined cost model are provided. There are no contradictions in the given information. The fact that $m_{JF}  m_S$ is also plausible, as Jacobian-free approximations can sometimes lead to a slight increase in the number of Krylov iterations compared to using an exact analytical Jacobian.\n5.  **Topic Relevance:** The problem is directly on the topic of Newton-Krylov methods for large nonlinear systems.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the derivation of the solution.\n\n### Solution Derivation\n\nThe task is to compute the total wall-clock time for each approach over $k=5$ Newton steps and compare them. The total time is the cost per Newton step multiplied by the number of steps.\n\n**Cost Analysis for Approach S (Assembled Jacobian)**\n\nThe cost for a single Newton step, denoted $C_S$, is the sum of the costs of its constituent operations:\n1.  Evaluation of the residual vector $F(x_k)$: $t_F$.\n2.  Assembly of the analytical sparse Jacobian matrix $J(x_k)$: $T_J$.\n3.  Building the preconditioner: $T_P$.\n4.  Solving the linear system with GMRES, which performs $m_S$ iterations. Each iteration consists of one sparse matrix-vector product and one preconditioner application. The cost for these $m_S$ iterations is $m_S \\times (t_{\\mathrm{spmv}} + t_{P\\mathrm{apply}})$.\n\nThe total cost per Newton step for Approach S is:\n$$ C_S = t_F + T_J + T_P + m_S (t_{\\mathrm{spmv}} + t_{P\\mathrm{apply}}) $$\nSubstituting the given numerical values:\n$$ C_S = 0.075\\,\\mathrm{s} + 2.8\\,\\mathrm{s} + 0.9\\,\\mathrm{s} + 20 \\times (0.010\\,\\mathrm{s} + 0.005\\,\\mathrm{s}) $$\n$$ C_S = 3.775\\,\\mathrm{s} + 20 \\times (0.015\\,\\mathrm{s}) $$\n$$ C_S = 3.775\\,\\mathrm{s} + 0.300\\,\\mathrm{s} $$\n$$ C_S = 4.075\\,\\mathrm{s} $$\nThe total time over $k=5$ Newton steps is:\n$$ C_{S, \\text{total}} = k \\times C_S = 5 \\times 4.075\\,\\mathrm{s} = 20.375\\,\\mathrm{s} $$\n\n**Cost Analysis for Approach JF (Jacobian-free)**\n\nThe cost for a single Newton step, denoted $C_{JF}$, is calculated similarly. Note that the Jacobian is not assembled.\n1.  Evaluation of the residual vector $F(x_k)$ for the right-hand side of the Newton system: $t_F$.\n2.  Building the preconditioner: $T_P$.\n3.  Solving the linear system with GMRES, which performs $m_{JF}$ iterations. Each iteration consists of one Jacobian-vector product approximated via a finite difference (costing one function evaluation, $t_F$) and one preconditioner application ($t_{P\\mathrm{apply}}$). The cost for these $m_{JF}$ iterations is $m_{JF} \\times (t_F + t_{P\\mathrm{apply}})$.\n\nThe total cost per Newton step for Approach JF is:\n$$ C_{JF} = t_F + T_P + m_{JF} (t_F + t_{P\\mathrm{apply}}) $$\nSubstituting the given numerical values:\n$$ C_{JF} = 0.075\\,\\mathrm{s} + 0.9\\,\\mathrm{s} + 30 \\times (0.075\\,\\mathrm{s} + 0.005\\,\\mathrm{s}) $$\n$$ C_{JF} = 0.975\\,\\mathrm{s} + 30 \\times (0.080\\,\\mathrm{s}) $$\n$$ C_{JF} = 0.975\\,\\mathrm{s} + 2.400\\,\\mathrm{s} $$\n$$ C_{JF} = 3.375\\,\\mathrm{s} $$\nThe total time over $k=5$ Newton steps is:\n$$ C_{JF, \\text{total}} = k \\times C_{JF} = 5 \\times 3.375\\,\\mathrm{s} = 16.875\\,\\mathrm{s} $$\n\n**Comparison of Total Times**\n\nWe compare the total computational time for both approaches:\n-   Total time for Approach S: $C_{S, \\text{total}} = 20.375\\,\\mathrm{s}$.\n-   Total time for Approach JF: $C_{JF, \\text{total}} = 16.875\\,\\mathrm{s}$.\n\nThe difference in total time is:\n$$ \\Delta C = C_{S, \\text{total}} - C_{JF, \\text{total}} = 20.375\\,\\mathrm{s} - 16.875\\,\\mathrm{s} = 3.500\\,\\mathrm{s} $$\nA positive difference indicates that Approach S is slower than Approach JF. Therefore, Approach JF is faster by $3.5\\,\\mathrm{s}$.\n\n### Option-by-Option Analysis\n\n**A. The assembled-Jacobian approach (Approach S) is faster by approximately $3.5\\,\\mathrm{s}$ overall.**\nThis statement is incorrect. Our calculation shows that Approach S is slower, not faster, than Approach JF. The total time for S is $20.375\\,\\mathrm{s}$, while for JF it is $16.875\\,\\mathrm{s}$.\n\n**B. The Jacobian-free approach (Approach JF) is faster by approximately $3.5\\,\\mathrm{s}$ overall.**\nThis statement is correct. Our calculation shows that $C_{JF, \\text{total}} = 16.875\\,\\mathrm{s}$ and $C_{S, \\text{total}} = 20.375\\,\\mathrm{s}$. The difference is $20.375 - 16.875 = 3.5\\,\\mathrm{s}$, indicating that Approach JF is indeed faster by this amount. The word \"approximately\" is appropriate, though in this idealized model, the result is exact.\n\n**C. The two approaches have approximately equal total time within $1\\%$.**\nThis statement is incorrect. The relative difference in total time with respect to the faster approach is:\n$$ \\frac{|C_{S, \\text{total}} - C_{JF, \\text{total}}|}{C_{JF, \\text{total}}} = \\frac{3.5}{16.875} \\approx 0.2074 $$\nThis is a difference of approximately $20.7\\%$, which is significantly greater than $1\\%$.\n\n**D. There is insufficient information to decide; the step size for the finite-difference directional derivative is required to compare times.**\nThis statement is incorrect. The problem provides a simplified but complete cost model. The cost of the finite-difference Jacobian-vector product is explicitly given as $t_F$. The step size $\\epsilon$ in the finite difference formula $Jv \\approx (F(x+\\epsilon v)-F(x))/\\epsilon$ would influence the accuracy of the approximation, which in turn could affect the number of GMRES iterations, $m_{JF}$. However, $m_{JF}$ is given as data. The problem does not require us to derive $m_{JF}$; it requires us to use it. Therefore, all necessary information for the cost calculation is present.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The power of the Newton-Krylov framework lies in its \"inexact\" nature—we don't need to solve the linear systems perfectly at each step. This raises a crucial question: how accurately must the linear system be solved? This final practice contrasts a simple, fixed tolerance with a sophisticated adaptive strategy, demonstrating how dynamically adjusting the inner-solve accuracy can dramatically improve overall efficiency, especially for challenging problems far from a solution .",
            "id": "2417684",
            "problem": "Consider solving a large nonlinear system $F(x)=0$ arising in computational engineering, where $F:\\mathbb{R}^n\\to\\mathbb{R}^n$ is continuously differentiable, the Jacobian $J(x)$ is Lipschitz continuous in a neighborhood of a solution $x^\\star$, and $J(x^\\star)$ is nonsingular. You apply a Newton–Krylov method in the inexact Newton framework: at outer iteration $k$, compute a step $s_k$ by approximately solving the linear system $J(x_k)s=-F(x_k)$ so that the inexact Newton condition\n$$\\|F(x_k)+J(x_k)s_k\\|\\le \\eta_k \\|F(x_k)\\|$$\nholds, where $\\eta_k\\in[0,1)$ is a prescribed forcing term. The linear systems are solved by a Krylov method (for example, Generalized Minimal Residual (GMRES)) with a suitable preconditioner.\n\nCompare the following two forcing-term strategies on a difficult problem (one for which the initial iterates are far from $x^\\star$ and the nonlinear residual decreases slowly at first):\n\n- Strategy S1 (fixed): $\\eta_k\\equiv 0.1$ for all $k$.\n- Strategy S2 (Eisenstat–Walker): \n$$\\eta_k=\\min\\left\\{\\eta_{\\max},\\, \\gamma\\left(\\frac{\\|F(x_k)\\|}{\\|F(x_{k-1})\\|}\\right)^{\\alpha}\\right\\},\\quad k\\ge 1,$$\nwith constants $\\eta_{\\max}\\in(0,1)$, $\\gamma\\in(0,1)$, and $\\alpha1$, and some initial choice $\\eta_0\\in(0,1)$.\n\nAssume globalization (for example, line search or trust region) is used so that the sequence $\\{x_k\\}$ remains in a neighborhood where the above assumptions hold and steps are accepted. Which of the following statements is/are correct?\n\nA. Under the stated smoothness and nonsingularity assumptions, Strategy S1 yields $q$-linear local convergence of the outer iterations, whereas Strategy S2 yields at least $q$-superlinear local convergence. Consequently, although Strategy S2 may demand tighter inner solves (more Krylov iterations) near the solution than Strategy S1, it typically reduces the total number of Krylov iterations required to reach a given nonlinear residual tolerance on difficult problems.\n\nB. Strategy S1 yields $q$-quadratic convergence because solving each linear system to $10\\%$ relative residual is sufficient to recover the exact Newton rate, while Strategy S2 degrades the rate to $q$-linear by loosening inner tolerances.\n\nC. Strategy S2 tends to choose larger $\\eta_k$ in the early, far-from-solution regime (allowing cheaper inner solves) and smaller $\\eta_k$ near the solution (enforcing tighter inner solves), adapting to the observed progress. In contrast, Strategy S1 does not adapt. This adaptivity commonly reduces total computational work for Strategy S2 on difficult problems compared with Strategy S1.\n\nD. For any problem and any constants $\\eta_{\\max}$, $\\gamma$, and $\\alpha$ as above, Strategy S2 necessarily increases the number of Krylov iterations compared to Strategy S1 because it always imposes tighter inner tolerances.\n\nE. Both Strategy S1 and Strategy S2 guarantee global convergence from arbitrary initial guesses without any globalization mechanism such as line search or trust region.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A large nonlinear system is given by the equation $F(x)=0$, where $F:\\mathbb{R}^n\\to\\mathbb{R}^n$.\n- $F$ is continuously differentiable.\n- The Jacobian matrix, $J(x)$, is Lipschitz continuous in a neighborhood of a solution $x^\\star$.\n- The Jacobian at the solution, $J(x^\\star)$, is nonsingular.\n- The system is solved using a Newton–Krylov method within the inexact Newton framework.\n- At an outer iteration $k$, a step $s_k$ is computed by approximately solving the linear system $J(x_k)s=-F(x_k)$.\n- The approximation must satisfy the inexact Newton condition: $\\|F(x_k)+J(x_k)s_k\\|\\le \\eta_k \\|F(x_k)\\|$, where $\\eta_k\\in[0,1)$ is a forcing term.\n- The linear systems are solved using a Krylov method.\n- A \"difficult problem\" is considered, where initial iterates are far from $x^\\star$ and the nonlinear residual decreases slowly at first.\n- Strategy S1 (fixed forcing term): $\\eta_k\\equiv 0.1$ for all $k$.\n- Strategy S2 (Eisenstat–Walker forcing term): For $k\\ge 1$, $\\eta_k=\\min\\left\\{\\eta_{\\max},\\, \\gamma\\left(\\frac{\\|F(x_k)\\|}{\\|F(x_{k-1})\\|}\\right)^{\\alpha}\\right\\}$, with constants $\\eta_{\\max}\\in(0,1)$, $\\gamma\\in(0,1)$, $\\alpha1$, and an initial choice $\\eta_0\\in(0,1)$.\n- A globalization strategy (e.g., line search) is assumed to be active, ensuring the iterates $\\{x_k\\}$ remain in a region where the assumptions hold and steps are accepted.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is analyzed for validity.\n\n- **Scientifically Grounded**: The problem is set in the well-established field of numerical analysis, specifically concerning iterative methods for large nonlinear systems. The inexact Newton method, Newton-Krylov solvers, and the Eisenstat–Walker forcing term strategy are standard, rigorously studied topics in computational science and engineering. The assumptions on the function $F$ and its Jacobian $J$ are standard for local convergence analysis of Newton-type methods. The problem is scientifically sound.\n- **Well-Posed**: The question requires a qualitative and theoretical comparison between two clearly defined algorithmic strategies under a standard set of assumptions. The problem is well-defined and allows for a meaningful and unique analysis based on established theory.\n- **Objective**: The problem is stated using precise mathematical definitions and terminology. The term \"difficult problem\" is qualified with objective characteristics (\"initial iterates are far from $x^\\star$\" and \"nonlinear residual decreases slowly at first\"), which are standard in the context of analyzing algorithm performance. The problem is objective.\n- **No other flaws are detected**. The problem is not self-contradictory, incomplete, or based on unrealistic premises. The assumption of a globalization method is a standard and necessary component for making the local analysis meaningful in a practical context.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full solution will be derived.\n\n### Derivation\n\nThe convergence behavior of an inexact Newton method, $x_{k+1} = x_k + s_k$, is governed by the choice of the forcing terms $\\eta_k$. The step $s_k$ is an approximate solution to the Newton equation $J(x_k)s_k = -F(x_k)$. The inexactness is characterized by the residual of this linear system, $r_k = J(x_k)s_k + F(x_k)$, which satisfies $\\|r_k\\| \\le \\eta_k \\|F(x_k)\\|$.\n\nStandard convergence theory for inexact Newton methods under the given assumptions on $F(x)$ establishes the following relationship for the error $e_k = x_k - x^\\star$ in a neighborhood of the solution $x^\\star$:\n$$ \\|e_{k+1}\\| \\le C_1 \\|e_k\\|^2 + C_2 \\eta_k \\|e_k\\| $$\nfor some positive constants $C_1$ and $C_2$. This inequality dictates the local rate of convergence.\n\n1.  If $\\eta_k$ is bounded away from zero, i.e., $\\limsup_{k\\to\\infty} \\eta_k = \\bar{\\eta}  0$, the second term $C_2 \\eta_k \\|e_k\\|$ will dominate the $C_1 \\|e_k\\|^2$ term as $\\|e_k\\| \\to 0$. The convergence is $q$-linear with a rate no better than $\\bar{\\eta}$.\n2.  If $\\eta_k \\to 0$ as $k \\to \\infty$, the term multiplying $\\|e_k\\|$ on the right-hand side, $(C_1 \\|e_k\\| + C_2 \\eta_k)$, tends to zero. This results in $q$-superlinear convergence: $\\lim_{k\\to\\infty} \\frac{\\|e_{k+1}\\|}{\\|e_k\\|} = 0$.\n3.  If $\\eta_k = O(\\|F(x_k)\\|)$, which is equivalent to $\\eta_k = O(\\|e_k\\|)$ near the solution, the convergence is $q$-quadratic: $\\|e_{k+1}\\| = O(\\|e_k\\|^2)$.\n\nNow we analyze the two strategies.\n\n**Strategy S1 (fixed):** $\\eta_k = 0.1$ for all $k$.\nHere, $\\limsup_{k\\to\\infty} \\eta_k = 0.1  0$. According to case 1 above, this strategy yields $q$-linear local convergence. The number of inner Krylov iterations required to reduce the linear residual by a factor of $0.1$ is expected to be relatively stable throughout the outer iterations, provided the conditioning of the Jacobian $J(x_k)$ does not change drastically.\n\n**Strategy S2 (Eisenstat–Walker):** $\\eta_k=\\min\\left\\{\\eta_{\\max},\\, \\gamma\\left(\\frac{\\|F(x_k)\\|}{\\|F(x_{k-1})\\|}\\right)^{\\alpha}\\right\\}$.\nThe key to this strategy is its adaptive nature.\n- **Far from the solution (`early regime`):** For a \"difficult problem,\" the residual reduction is slow, so the ratio $\\|F(x_k)\\|/\\|F(x_{k-1})\\|$ is close to $1$. In this case, $\\eta_k$ will be dominated by the $\\eta_{\\max}$ term or a large value close to it. For example, if $\\eta_{\\max}=0.9$, $\\gamma=0.9$, $\\alpha=2$, and a slow step gives $\\|F(x_k)\\|/\\|F(x_{k-1})\\| = 0.95$, then $\\eta_k = \\min(0.9, 0.9 \\cdot 0.95^2) \\approx \\min(0.9, 0.81) = 0.81$. This is much larger than the $0.1$ used in S1, meaning the linear system is solved very loosely, saving a significant number of Krylov iterations.\n- **Near the solution (`asymptotic regime`):** As the method converges, the ratio $\\|F(x_k)\\|/\\|F(x_{k-1})\\|$ will decrease. It can be proven that this choice of $\\eta_k$ forces superlinear convergence. Specifically, one can show that if the convergence were merely linear with rate $\\rho  0$, then $\\eta_k \\to \\gamma \\rho^\\alpha$. The inexact Newton theory implies the rate $\\rho$ must satisfy $\\rho \\le \\gamma \\rho^\\alpha$. With $\\gamma \\in (0,1)$ and $\\alpha  1$, this inequality implies $\\rho^{\\alpha-1} \\ge 1/\\gamma  1$, which is impossible for a convergent method where $\\rho  1$. The only possibility is $\\rho=0$, meaning $\\|F(x_k)\\|/\\|F(x_{k-1})\\| \\to 0$. This, in turn, implies $\\eta_k \\to 0$, satisfying the condition for $q$-superlinear convergence (case 2). As $\\eta_k$ becomes very small, the linear systems must be solved to high accuracy, requiring more Krylov iterations per outer step than S1.\n\nIn summary, S2 is adaptive: it avoids over-solving the linear system when far from the solution and enforces accuracy when near the solution to accelerate convergence. This typically reduces the total number of Krylov iterations (the main component of computational work) compared to a non-adaptive strategy, especially on difficult problems.\n\n### Option-by-Option Analysis\n\n**A. Under the stated smoothness and nonsingularity assumptions, Strategy S1 yields $q$-linear local convergence of the outer iterations, whereas Strategy S2 yields at least $q$-superlinear local convergence. Consequently, although Strategy S2 may demand tighter inner solves (more Krylov iterations) near the solution than Strategy S1, it typically reduces the total number of Krylov iterations required to reach a given nonlinear residual tolerance on difficult problems.**\nThis statement is fully consistent with the analysis above. S1, with its constant forcing term $\\eta_k=0.1$, results in $q$-linear convergence. S2, by driving $\\eta_k \\to 0$, achieves $q$-superlinear convergence. The consequence is also correctly stated: S2 tightens the tolerance near the solution (small $\\eta_k$ means more inner work), but its adaptivity (loose solves initially, fast superlinear convergence finally) usually leads to a lower total number of inner iterations for the entire solve on difficult problems.\n**Verdict: Correct.**\n\n**B. Strategy S1 yields $q$-quadratic convergence because solving each linear system to $10\\%$ relative residual is sufficient to recover the exact Newton rate, while Strategy S2 degrades the rate to $q$-linear by loosening inner tolerances.**\nThis statement contains multiple fundamental errors. First, a constant forcing term $\\eta_k = 0.1$ does not yield $q$-quadratic convergence; it yields $q$-linear convergence. Quadratic convergence requires $\\eta_k \\to 0$ (and specifically $\\eta_k = O(\\|F(x_k)\\|)$). Second, Strategy S2 does not degrade the convergence rate to $q$-linear; it improves it to $q$-superlinear. It tightens, not just loosens, inner tolerances as the solution is approached.\n**Verdict: Incorrect.**\n\n**C. Strategy S2 tends to choose larger $\\eta_k$ in the early, far-from-solution regime (allowing cheaper inner solves) and smaller $\\eta_k$ near the solution (enforcing tighter inner solves), adapting to the observed progress. In contrast, Strategy S1 does not adapt. This adaptivity commonly reduces total computational work for Strategy S2 on difficult problems compared with Strategy S1.**\nThis statement perfectly describes the mechanism and practical benefit of the Eisenstat–Walker strategy. For difficult problems where initial progress is slow, the ratio $\\|F(x_k)\\|/\\|F(x_{k-1})\\|$ is close to $1$, leading S2 to choose a large $\\eta_k$ (e.g., close to $\\eta_{\\max}$), thus saving work. Near the solution, the ratio becomes small, forcing a small $\\eta_k$ that ensures rapid convergence. S1 is inherently non-adaptive. This adaptivity is precisely why S2 is generally more efficient in terms of total computational work (dominated by Krylov iterations).\n**Verdict: Correct.**\n\n**D. For any problem and any constants $\\eta_{\\max}$, $\\gamma$, and $\\alpha$ as above, Strategy S2 necessarily increases the number of Krylov iterations compared to Strategy S1 because it always imposes tighter inner tolerances.**\nThis is incorrect. The assertion that S2 \"always imposes tighter inner tolerances\" is false. As explained for option C, in the early stages of a difficult problem, S2 will likely select $\\eta_k  0.1$, which is a *looser* tolerance than S1. The word \"necessarily\" makes the statement definitively false.\n**Verdict: Incorrect.**\n\n**E. Both Strategy S1 and Strategy S2 guarantee global convergence from arbitrary initial guesses without any globalization mechanism such as line search or trust region.**\nThis is a fundamental misunderstanding of Newton-type methods. Neither method guarantees global convergence. They are local methods and can easily diverge if the initial guess is not sufficiently close to the solution. The problem statement itself correctly assumes that a globalization mechanism is in use to prevent such divergence. Without it, convergence from \"arbitrary initial guesses\" is not guaranteed.\n**Verdict: Incorrect.**\n\nBoth statements A and C are correct. They describe the same overall phenomenon from two valid perspectives: A focuses on the resulting mathematical convergence rates, while C focuses on the adaptive mechanism of the forcing term selection. Both accurately capture the theoretical properties and practical advantages of Strategy S2 over S1.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}