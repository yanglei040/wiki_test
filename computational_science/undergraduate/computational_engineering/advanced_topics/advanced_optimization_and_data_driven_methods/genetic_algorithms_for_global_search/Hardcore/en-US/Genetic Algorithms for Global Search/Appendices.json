{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides an excellent entry point into applying Genetic Algorithms to a complex combinatorial problem. You will develop a GA to optimize a binary image, mimicking the process of font hinting, where the goal is to create a clear and legible letterform on a low-resolution grid . This exercise is valuable because it involves a vast, discrete search space and a multi-faceted fitness function, forcing you to balance competing objectives like matching a target shape, ensuring pixel connectivity, and avoiding isolated pixels. It provides hands-on practice with the fundamental components of a canonical binary GA, from chromosome representation to the core operators of selection, crossover, and mutation.",
            "id": "2396561",
            "problem": "You are given a discrete global search problem that formalizes low-resolution letterform legibility as a combinatorial optimization task. For a given integer grid size $N \\in \\mathbb{N}$, define a binary image as a matrix $X \\in \\{0,1\\}^{N \\times N}$, where entry $X_{r,c}=1$ indicates a lit pixel at row $r$ and column $c$, and $X_{r,c}=0$ otherwise. Let $T \\in \\{0,1\\}^{N \\times N}$ be a fixed target binary image of the same shape. For a candidate $X$, define the following terms:\n- Match term $m(X,T)$:\n$$\nm(X,T) \\;=\\; 1 \\;-\\; \\frac{1}{N^2} \\sum_{r=0}^{N-1}\\sum_{c=0}^{N-1} \\left| X_{r,c} - T_{r,c} \\right|.\n$$\n- Connectivity term $c(X)$: Let $S(X)=\\{(r,c)\\,:\\,X_{r,c}=1\\}$ and define the $4$-neighborhood of $(r,c)$ as $\\{(r-1,c),(r+1,c),(r,c-1),(r,c+1)\\}$ when indices are within bounds. Let $h_{r,c}(X)=1$ if there exists a $4$-neighbor $(r',c')$ with $X_{r',c'}=1$, and $h_{r,c}(X)=0$ otherwise. Define\n$$\nc(X) \\;=\\; \\begin{cases}\n\\displaystyle \\frac{1}{|S(X)|}\\sum_{(r,c)\\in S(X)} h_{r,c}(X),  \\text{if } |S(X)| > 0, \\\\[6pt]\n0,  \\text{if } |S(X)| = 0.\n\\end{cases}\n$$\n- Isolation term $i(X)$: Let the $8$-neighborhood of $(r,c)$ be all $(r',c')$ with $\\max(|r'-r|,|c'-c|)=1$ when indices are within bounds. Let $g_{r,c}(X)=1$ if there exists an $8$-neighbor $(r',c')$ with $X_{r',c'}=1$, and $g_{r,c}(X)=0$ otherwise. Define the fraction of isolated lit pixels\n$$\ni(X) \\;=\\; \\begin{cases}\n\\displaystyle \\frac{1}{|S(X)|}\\sum_{(r,c)\\in S(X)} \\big(1 - g_{r,c}(X)\\big),  \\text{if } |S(X)| > 0, \\\\[6pt]\n0,  \\text{if } |S(X)| = 0.\n\\end{cases}\n$$\n- Area-deviation term $a(X,T)$:\n$$\na(X,T) \\;=\\; \\frac{\\big|\\;|S(X)| - |S(T)|\\;\\big|}{N^2}.\n$$\nGiven nonnegative weights $w_m,w_c,w_i,w_a \\in \\mathbb{R}_{\\ge 0}$, the objective to be maximized is\n$$F(X;T,w_m,w_c,w_i,w_a) \\;=\\; w_m\\, m(X,T) \\;+\\; w_c\\, c(X) \\;-\\; w_i\\, i(X) \\;-\\; w_a\\, a(X,T).$$\n\nYour program must, for each specified test case, search the domain $\\{0,1\\}^{N \\times N}$ subject to a strict evaluation budget $B \\in \\mathbb{N}$, and report the maximum objective value found within that budget. An evaluation is the computation of $F(X;T,w_m,w_c,w_i,w_a)$ for a specific $X$. You must round each reported value to $6$ decimal places.\n\nTest suite specification. In all cases, indices are zero-based with rows $r \\in \\{0,1,\\dots,N-1\\}$ and columns $c \\in \\{0,1,\\dots,N-1\\}$. Each target $T$ is defined explicitly as the set of lit pixels $S(T) \\subset \\{0,\\dots,N-1\\} \\times \\{0,\\dots,N-1\\}$.\n\n- Case $1$ (happy path): $N=8$, weights $(w_m,w_c,w_i,w_a)=(0.6,\\,0.3,\\,0.4,\\,0.2)$, budget $B=6000$. Target $T$ encodes a blocky letter resembling an $E$ with a vertical spine and three horizontal bars:\n  1. Vertical spine: all $(r,c)$ with $r \\in \\{1,2,3,4,5,6\\}$ and $c=1$.\n  2. Top bar: all $(r,c)$ with $r=1$ and $c \\in \\{1,2,3,4,5\\}$.\n  3. Middle bar: all $(r,c)$ with $r=3$ and $c \\in \\{1,2,3,4\\}$.\n  4. Bottom bar: all $(r,c)$ with $r=6$ and $c \\in \\{1,2,3,4,5\\}$.\n\n- Case $2$ (boundary size and stronger regularization): $N=5$, weights $(w_m,w_c,w_i,w_a)=(0.5,\\,0.2,\\,0.7,\\,0.6)$, budget $B=2000$. Target $T$ is a minimal $E$:\n  1. Vertical spine: all $(r,c)$ with $r \\in \\{1,2,3\\}$ and $c=1$.\n  2. Top bar: all $(r,c)$ with $r=1$ and $c \\in \\{1,2,3\\}$.\n  3. Middle bar: all $(r,c)$ with $r=2$ and $c \\in \\{1,2\\}$.\n  4. Bottom bar: all $(r,c)$ with $r=3$ and $c \\in \\{1,2,3\\}$.\n\n- Case $3$ (diagonal-like structure and connectivity emphasis): $N=8$, weights $(w_m,w_c,w_i,w_a)=(0.5,\\,0.6,\\,0.3,\\,0.2)$, budget $B=8000$. Target $T$ encodes a blocky letter resembling an $A$ with two vertical legs, a top bar, and a crossbar:\n  1. Left leg: all $(r,c)$ with $r \\in \\{2,3,4,5,6\\}$ and $c=1$.\n  2. Right leg: all $(r,c)$ with $r \\in \\{2,3,4,5,6\\}$ and $c=6$.\n  3. Top bar: all $(r,c)$ with $r=1$ and $c \\in \\{2,3,4,5\\}$.\n  4. Crossbar: all $(r,c)$ with $r=4 and $c \\in \\{2,3,4,5\\}$.\n\nFinal output format. Your program should produce a single line of output containing the three rounded objective values for Cases $1$, $2$, and $3$, in that order, as a comma-separated list enclosed in square brackets. For example, it must print exactly one line in the form\n$[$$v_1$$,$$v_2$$,$$v_3$$]$, where each $v_k$ is a real number rounded to $6$ decimal places with standard rounding.",
            "solution": "The problem presented is a discrete global search task, which requires maximizing a complex objective function over a high-dimensional binary space. The search space for a binary image of size $N \\times N$ is $\\{0,1\\}^{N \\times N}$, which contains $2^{N^2}$ possible configurations. For the given values of $N=5$ and $N=8$, the search spaces have $2^{25} \\approx 3.3 \\times 10^7$ and $2^{64} \\approx 1.8 \\times 10^{19}$ members, respectively. An exhaustive search is computationally infeasible. The problem imposes a strict evaluation budget, which frames it as a black-box optimization task where the goal is to find the best possible solution within a limited number of function evaluations. This structure necessitates the use of a metaheuristic search algorithm.\n\nA Genetic Algorithm (GA) is a well-suited methodology for this type of problem. GAs are inspired by the process of natural selection and are effective for exploring large, complex, and poorly understood search spaces. The objective function $F(X;T,w_m,w_c,w_i,w_a)$ is a weighted sum of several components measuring similarity, connectivity, and other structural properties, likely resulting in a rugged fitness landscape with many local optima. GAs are robust against being trapped in such local optima due to their population-based approach and stochastic operators like crossover and mutation.\n\nThe implemented solution is a standard Genetic Algorithm tailored to the specifics of this problem.\n\n**1. Representation and Population**\nAn individual in the population, representing a candidate solution, is an $N \\times N$ binary matrix $X$. The population consists of a fixed number of such individuals. For reproducibility, the pseudo-random number generator is seeded with a constant value. The initial population is seeded with the target image $T$ itself, several mutated versions of $T$, and a majority of randomly generated images to ensure a balance between exploiting a known good region and exploring the broader search space.\n\n**2. Fitness Evaluation**\nThe fitness of an individual $X$ is determined by the objective function $F(X;T,w_m,w_c,w_i,w_a)$. The components of this function are calculated as follows:\n- The match term $m(X,T) = 1 - \\frac{1}{N^2} \\sum_{r,c} |X_{r,c} - T_{r,c}|$ is the normalized agreement between the candidate and target images.\n- The area-deviation term $a(X,T) = \\frac{|\\sum X_{r,c} - \\sum T_{r,c}|}{N^2}$ penalizes differences in the number of lit pixels.\n- The connectivity term $c(X)$ and isolation term $i(X)$ require analyzing pixel neighborhoods. These are calculated efficiently using 2D convolution. The number of lit neighbors for each pixel across the entire grid is computed in a single operation using `scipy.signal.convolve2d` with appropriate kernels. For $c(X)$, which uses a $4$-neighborhood, the kernel is $\\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$. For $i(X)$, which is based on an $8$-neighborhood, the kernel is $\\begin{pmatrix} 1  1  1 \\\\ 1  0  1 \\\\ 1  1  1 \\end{pmatrix}$. This vectorized approach is significantly more efficient than iterating through pixels. An `Evaluator` class encapsulates fitness calculation and strictly enforces the evaluation budget $B$.\n\n**3. Selection**\nTournament selection is used to choose parents for the next generation. In a $k$-way tournament, $k$ individuals are chosen at random from the current population, and the one with the highest fitness is selected as a parent. This method provides a good balance between selection pressure and population diversity.\n\n**4. Genetic Operators**\n- **Crossover**: Uniform crossover is employed. For each pixel location, a random choice determines which of the two parents contributes its pixel value to the offspring. This operator effectively combines features from both parents.\n- **Mutation**: Bit-flip mutation is applied to offspring after crossover. Each pixel value (bit) has a small, independent probability of being flipped. The mutation rate is set to $1/N^2$, which on average results in one mutation per individual, introducing new genetic material and preventing premature convergence.\n\n**5. Generational Cycle and Elitism**\nThe GA proceeds in generations. In each cycle, a new population is created from the old one. Elitism is implemented, meaning a small number of the best-performing individuals from the current generation are guaranteed to survive, preserving high-quality solutions. The remainder of the new population is filled with offspring generated through selection, crossover, and mutation. The algorithm terminates once the evaluation budget $B$ is expended, and the highest fitness value found during the entire run is reported as the result.\n\nThis systematic, principle-based design ensures a robust search process capable of navigating the complex fitness landscape to find high-quality solutions for the given letterform legibility problem within the specified computational constraints.",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\n# Static seed for reproducibility across runs\nRNG = np.random.default_rng(42)\n\nclass FitnessEvaluator:\n    \"\"\"Calculates the fitness of a candidate image and manages the evaluation budget.\"\"\"\n    def __init__(self, target_matrix, weights, budget):\n        self.T = target_matrix\n        self.N = self.T.shape[0]\n        self.w_m, self.w_c, self.w_i, self.w_a = weights\n        \n        self.target_pixel_count = np.sum(self.T)\n        self.N_squared = self.N * self.N\n        \n        self.kernel_4_conn = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=np.uint8)\n        self.kernel_8_conn = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=np.uint8)\n        \n        self.eval_budget = budget\n        self.eval_count = 0\n        self.max_fitness_found = -np.inf\n\n    def calculate_fitness(self, X):\n        \"\"\"Computes the objective function F for a given image X.\"\"\"\n        if self.eval_count >= self.eval_budget:\n            return None\n\n        # Match term m(X, T)\n        match_term = 1.0 - np.sum(np.abs(X - self.T)) / self.N_squared\n        \n        pixel_count = np.sum(X)\n        \n        if pixel_count == 0:\n            conn_term = 0.0\n            iso_term = 0.0\n        else:\n            # Connectivity term c(X) using 4-neighborhood\n            neighbors_4 = convolve2d(X, self.kernel_4_conn, mode='same', boundary='fill', fillvalue=0)\n            connected_pixels = (neighbors_4 > 0) * X\n            conn_term = np.sum(connected_pixels) / pixel_count\n            \n            # Isolation term i(X) using 8-neighborhood\n            neighbors_8 = convolve2d(X, self.kernel_8_conn, mode='same', boundary='fill', fillvalue=0)\n            isolated_pixels = (neighbors_8 == 0) * X\n            iso_term = np.sum(isolated_pixels) / pixel_count\n            \n        # Area-deviation term a(X, T)\n        area_dev_term = np.abs(pixel_count - self.target_pixel_count) / self.N_squared\n        \n        fitness = (self.w_m * match_term + \n                   self.w_c * conn_term - \n                   self.w_i * iso_term - \n                   self.w_a * area_dev_term)\n        \n        self.eval_count += 1\n        if fitness > self.max_fitness_found:\n            self.max_fitness_found = fitness\n            \n        return fitness\n\ndef tournament_selection_idx(fitnesses, k):\n    \"\"\"Selects an individual's index using a tournament.\"\"\"\n    pop_size = len(fitnesses)\n    contender_indices = RNG.choice(pop_size, k, replace=False)\n    \n    best_contender_idx = -1\n    best_fitness = -np.inf\n    \n    for idx in contender_indices:\n        if fitnesses[idx] > best_fitness:\n            best_fitness = fitnesses[idx]\n            best_contender_idx = idx\n            \n    return best_contender_idx\n\ndef uniform_crossover(p1, p2):\n    \"\"\"Performs uniform crossover on two parents.\"\"\"\n    mask = RNG.integers(0, 2, size=p1.shape, dtype=np.uint8)\n    child1 = p1 * mask + p2 * (1 - mask)\n    child2 = p2 * mask + p1 * (1 - mask)\n    return child1, child2\n\ndef mutate(individual, mutation_rate):\n    \"\"\"Applies bit-flip mutation to an individual.\"\"\"\n    mutation_mask = RNG.random(individual.shape)  mutation_rate\n    individual[mutation_mask] = 1 - individual[mutation_mask]\n\ndef run_genetic_algorithm(N, target_pixels, weights, budget):\n    \"\"\"Main GA loop for a single test case.\"\"\"\n    \n    target_matrix = np.zeros((N, N), dtype=np.uint8)\n    for r, c in target_pixels:\n        target_matrix[r, c] = 1\n        \n    evaluator = FitnessEvaluator(target_matrix, weights, budget)\n    \n    # GA Parameters\n    pop_size = 100\n    elite_size = 2\n    tournament_k = 3\n    mutation_rate = 1.0 / (N * N)\n    crossover_prob = 0.9\n\n    # Initialization\n    population = []\n    fitnesses = []\n    \n    # Seed population with target, its mutations, and random individuals\n    initial_seeds = [(target_matrix.copy(), 0), (target_matrix.copy(), int(0.05 * N*N)), (target_matrix.copy(), int(0.1 * N*N))]\n    for ind, num_flips in initial_seeds:\n        if num_flips > 0:\n            indices_to_flip = RNG.choice(N * N, num_flips, replace=False)\n            flat_ind = ind.flatten()\n            flat_ind[indices_to_flip] = 1 - flat_ind[indices_to_flip]\n            ind = flat_ind.reshape((N, N))\n        \n        fit = evaluator.calculate_fitness(ind)\n        if fit is not None:\n            population.append(ind)\n            fitnesses.append(fit)\n\n    while len(population)  pop_size:\n        individual = RNG.integers(0, 2, size=(N, N), dtype=np.uint8)\n        fit = evaluator.calculate_fitness(individual)\n        if fit is None: break\n        population.append(individual)\n        fitnesses.append(fit)\n\n    if not population: return -np.inf\n\n    # Main generational loop\n    while evaluator.eval_count  evaluator.eval_budget:\n        # Sort population by fitness for elitism\n        sorted_indices = np.argsort(fitnesses)\n        new_population = [population[i] for i in sorted_indices[-elite_size:]]\n        new_fitnesses = [fitnesses[i] for i in sorted_indices[-elite_size:]]\n\n        # Generate offspring\n        while len(new_population)  pop_size:\n            parent1_idx = tournament_selection_idx(fitnesses, tournament_k)\n            parent2_idx = tournament_selection_idx(fitnesses, tournament_k)\n            parent1 = population[parent1_idx]\n            parent2 = population[parent2_idx]\n\n            if RNG.random()  crossover_prob:\n                child1, child2 = uniform_crossover(parent1, parent2)\n            else:\n                child1, child2 = parent1.copy(), parent2.copy()\n            \n            mutate(child1, mutation_rate)\n            mutate(child2, mutation_rate)\n\n            fit1 = evaluator.calculate_fitness(child1)\n            if fit1 is None: break\n            new_population.append(child1)\n            new_fitnesses.append(fit1)\n            \n            if len(new_population)  pop_size:\n                fit2 = evaluator.calculate_fitness(child2)\n                if fit2 is None: break\n                new_population.append(child2)\n                new_fitnesses.append(fit2)\n            \n        if evaluator.eval_count >= evaluator.eval_budget: break\n\n        population = new_population\n        fitnesses = new_fitnesses\n\n    return evaluator.max_fitness_found\n\ndef solve():\n    test_cases = [\n        {\n            \"N\": 8, \"weights\": (0.6, 0.3, 0.4, 0.2), \"budget\": 6000,\n            \"target_pixels\": set(\n                [(r, 1) for r in range(1, 7)] + [(1, c) for c in range(1, 6)] +\n                [(3, c) for c in range(1, 5)] + [(6, c) for c in range(1, 6)]\n            )\n        },\n        {\n            \"N\": 5, \"weights\": (0.5, 0.2, 0.7, 0.6), \"budget\": 2000,\n            \"target_pixels\": set(\n                [(r, 1) for r in range(1, 4)] + [(1, c) for c in range(1, 4)] +\n                [(2, c) for c in range(1, 3)] + [(3, c) for c in range(1, 4)]\n            )\n        },\n        {\n            \"N\": 8, \"weights\": (0.5, 0.6, 0.3, 0.2), \"budget\": 8000,\n            \"target_pixels\": set(\n                [(r, 1) for r in range(2, 7)] + [(r, 6) for r in range(2, 7)] +\n                [(1, c) for c in range(2, 6)] + [(4, c) for c in range(2, 6)]\n            )\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        max_val = run_genetic_algorithm(\n            case[\"N\"], case[\"target_pixels\"], case[\"weights\"], case[\"budget\"]\n        )\n        results.append(round(max_val, 6))\n    \n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from the discrete world of binary strings, this exercise challenges you to apply a GA to a continuous optimization problem, a common scenario in engineering design. You will implement a real-coded GA to find the optimal geometric parameters of a metamaterial's unit cell, aiming to minimize its effective thermal expansion coefficient based on a surrogate model . This practice is essential as it introduces the operators tailored for real-valued chromosomes, such as arithmetic crossover and Gaussian mutation, and demonstrates a standard technique for handling design constraints via penalty functions. It builds crucial skills for tackling real-world engineering parameter optimization tasks.",
            "id": "2396545",
            "problem": "You are tasked with implementing a real-coded Genetic Algorithm (GA) to perform a global search over a simplified, dimensionless surrogate model of the effective thermal expansion response of a metamaterial unit cell. The goal is to minimize a scalar objective that approximates an effective thermal expansion coefficient. All angles must be handled in radians. The final answers must be dimensionless real numbers.\n\nFundamental base for this task includes: (i) the definition of optimization as the search for an argument that minimizes a scalar function over a domain, (ii) the definition of a population-based randomized search (genetic algorithm) with reproduction via selection, crossover, and mutation, and (iii) penalty methods for constrained optimization.\n\nDecision variables and domain:\n- Let the chromosome be a vector $\\mathbf{x} = (t, s, \\phi)$ where $t$ is a thickness ratio, $s$ is a hinge compliance ratio, and $\\phi$ is a re-entrant angle in radians. These are dimensionless.\n- Nominal variable domains are:\n  - $t \\in [0.05, 0.95]$,\n  - $s \\in [0.10, 0.90]$,\n  - $\\phi \\in [0.10, 1.30]$ radians.\n- A derived relative density proxy is $r_d(t,s) = 0.5\\,t + 0.5\\,(1 - s)$, which must satisfy $r_d \\in [0.20, 0.80]$.\n\nObjective (to be minimized):\nDefine the surrogate objective $f(t,s,\\phi)$ by\n$$\nf(t,s,\\phi) = c_0 + c_1 t + c_2 s + c_3 \\sin(\\phi) + c_4 t s - c_5 s \\sin(\\phi) - c_6 (1 - t) \\sin(\\phi) + \\Pi_{\\text{dens}}(t,s) + \\Pi_{\\text{box}}(t,s,\\phi),\n$$\nwith constants\n$$\nc_0=0.2,\\quad c_1=0.1,\\quad c_2=0.15,\\quad c_3=0.3,\\quad c_4=0.05,\\quad c_5=0.6,\\quad c_6=0.5.\n$$\nPenalty terms enforce constraints,\n$$\n\\Pi_{\\text{dens}}(t,s) = M \\left(\\max\\{0, r_d(t,s) - 0.80\\}\\right)^2 + M \\left(\\max\\{0, 0.20 - r_d(t,s)\\}\\right)^2,\n$$\n$$\n\\Pi_{\\text{box}}(t,s,\\phi) = B\\left(\\max\\{0, t - \\overline{t}\\}\\right)^2 + B\\left(\\max\\{0, \\underline{t} - t\\}\\right)^2 + B\\left(\\max\\{0, s - \\overline{s}\\}\\right)^2 + B\\left(\\max\\{0, \\underline{s} - s\\}\\right)^2 + B\\left(\\max\\{0, \\phi - \\overline{\\phi}\\}\\right)^2 + B\\left(\\max\\{0, \\underline{\\phi} - \\phi\\}\\right)^2,\n$$\nwith $M=10$, $B=100$, and $(\\underline{t},\\overline{t})=(0.05,0.95)$, $(\\underline{s},\\overline{s})=(0.10,0.90)$, $(\\underline{\\phi},\\overline{\\phi})=(0.10,1.30)$ unless a test case specifies different bounds for $\\phi$.\n\nGenetic Algorithm design requirement:\n- Chromosome encoding: real-valued vector $\\mathbf{x}\\in\\mathbb{R}^3$.\n- Initialization: sample uniformly within the box bounds for each decision variable.\n- Fitness: identical to $f(t,s,\\phi)$; minimization objective.\n- Selection: tournament selection with tournament size $k=3$.\n- Crossover: arithmetic crossover, where two parents $\\mathbf{x}^{(1)}$ and $\\mathbf{x}^{(2)}$ produce two children\n  $\\mathbf{y}^{(1)} = \\omega \\mathbf{x}^{(1)} + (1-\\omega)\\mathbf{x}^{(2)}$, $\\mathbf{y}^{(2)} = \\omega \\mathbf{x}^{(2)} + (1-\\omega)\\mathbf{x}^{(1)}$,\n  with $\\omega \\sim \\text{Uniform}[0,1]$, applied with probability $p_c$.\n- Mutation: independent Gaussian mutation per gene applied with probability $p_m$; for gene $j$,\n  $y_j \\leftarrow y_j + \\mathcal{N}\\!\\left(0, \\sigma^2 (\\overline{b}_j - \\underline{b}_j)^2\\right)$, where $[\\underline{b}_j,\\overline{b}_j]$ are the bounds for gene $j$ and $\\sigma$ is a scalar scale factor. After mutation, clip to the box bounds.\n- Elitism: copy the best $E=\\max\\{1,\\lfloor 0.05\\,N \\rfloor\\}$ individuals unchanged to the next generation, where $N$ is the population size.\n- Termination: fixed number of generations $G$.\n- Randomness: all random number generation must be seeded as specified to ensure deterministic outputs.\n\nAngle unit:\n- All angles $\\phi$ are in radians.\n\nTest suite:\nImplement and run the GA on the following three test cases. Each test case is fully specified by its bounds, algorithmic hyperparameters, and random seed.\n\n- Test case A (general “happy path”):\n  - Bounds: $t \\in [0.05,0.95]$, $s \\in [0.10,0.90]$, $\\phi \\in [0.10,1.30]$ radians.\n  - Population size $N=60$, generations $G=120$, crossover probability $p_c=0.9$, mutation probability per gene $p_m=0.2$, mutation scale $\\sigma=0.05$.\n  - Random seed: $12345$.\n\n- Test case B (boundary in angle; small re-entrant angles):\n  - Bounds: $t \\in [0.05,0.95]$, $s \\in [0.10,0.90]$, $\\phi \\in [0.10,0.25]$ radians.\n  - Population size $N=80$, generations $G=180$, crossover probability $p_c=0.9$, mutation probability per gene $p_m=0.2$, mutation scale $\\sigma=0.04$.\n  - Random seed: $2023$.\n\n- Test case C (edge case with small population but large angles):\n  - Bounds: $t \\in [0.05,0.95]$, $s \\in [0.10,0.90]$, $\\phi \\in [1.10,1.30]$ radians.\n  - Population size $N=25$, generations $G=300$, crossover probability $p_c=0.8$, mutation probability per gene $p_m=0.3$, mutation scale $\\sigma=0.06$.\n  - Random seed: $314159$.\n\nRequired output:\n- For each test case, compute the best (lowest) objective value $f_{\\min}$ found by the GA at termination.\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, in the order A, B, C. For example, the exact format must be of the form [$x_1,x_2,x_3$], where $x_1$, $x_2$, and $x_3$ are real numbers rounded to $6$ decimal places.",
            "solution": "The task is to find the minimum value of a surrogate objective function, $f(t,s,\\phi)$, which is a function of three dimensionless decision variables: a thickness ratio $t$, a hinge compliance ratio $s$, and a re-entrant angle $\\phi$. The search is performed using a Genetic Algorithm (GA), a stochastic optimization heuristic inspired by the principles of natural selection. The solution is constructed by implementing this algorithm precisely as specified.\n\nFirst, we define the objective function to be minimized. It is composed of a base surrogate model and penalty terms to enforce constraints. The complete function is:\n$$\nf(t,s,\\phi) = c_0 + c_1 t + c_2 s + c_3 \\sin(\\phi) + c_4 t s - c_5 s \\sin(\\phi) - c_6 (1 - t) \\sin(\\phi) + \\Pi_{\\text{dens}}(t,s) + \\Pi_{\\text{box}}(t,s,\\phi)\n$$\nThe constants are given as $c_0=0.2$, $c_1=0.1$, $c_2=0.15$, $c_3=0.3$, $c_4=0.05$, $c_5=0.6$, and $c_6=0.5$.\n\nThe constraints are handled using an exterior penalty method. A constraint on a derived quantity, the relative density proxy $r_d(t,s) = 0.5\\,t + 0.5\\,(1 - s)$, requires that $r_d \\in [0.20, 0.80]$. This is enforced by the penalty term $\\Pi_{\\text{dens}}(t,s)$:\n$$\n\\Pi_{\\text{dens}}(t,s) = M \\left(\\max\\{0, r_d(t,s) - 0.80\\}\\right)^2 + M \\left(\\max\\{0, 0.20 - r_d(t,s)\\}\\right)^2\n$$\nwith penalty parameter $M=10$.\nThe box constraints on the decision variables, $t \\in [\\underline{t}, \\overline{t}]$, $s \\in [\\underline{s}, \\overline{s}]$, and $\\phi \\in [\\underline{\\phi}, \\overline{\\phi}]$, are formally handled by the penalty term $\\Pi_{\\text{box}}(t,s,\\phi)$:\n$$\n\\Pi_{\\text{box}}(t,s,\\phi) = B\\sum_{j \\in \\{t,s,\\phi\\}} \\left( \\left(\\max\\{0, x_j - \\overline{x}_j\\}\\right)^2 + \\left(\\max\\{0, \\underline{x}_j - x_j\\}\\right)^2 \\right)\n$$\nwith penalty parameter $B=100$. Although the specified GA mechanics of initialization and mutation clipping ensure that all evaluated individuals lie within these box bounds, making this term functionally zero, it is included in the objective function definition as a matter of formal completeness per the problem statement.\n\nThe Genetic Algorithm is implemented as a generational model with the following components:\n1.  **Chromosome and Population:** Each individual (chromosome) in the population is represented by a real-valued vector $\\mathbf{x} = (t, s, \\phi) \\in \\mathbb{R}^3$. A population consists of $N$ such individuals. The initial population is generated by sampling each variable for each individual uniformly at random from its specified domain.\n2.  **Fitness Evaluation:** The fitness of an individual is simply its objective function value, $f(\\mathbf{x})$. The algorithm seeks to minimize this value.\n3.  **Evolutionary Cycle:** The algorithm proceeds for a fixed number of generations, $G$. In each generation, a new population is created from the old one.\n4.  **Elitism:** To ensure the preservation of the best solutions found so far, a set of elite individuals is carried over directly to the next generation. The number of elites is $E=\\max\\{1,\\lfloor 0.05\\,N \\rfloor\\}$.\n5.  **Reproduction:** The remaining $N-E$ individuals for the new generation are produced through selection, crossover, and mutation.\n    -   **Selection:** Parents are selected from the current population using tournament selection. For each parent needed, a tournament of size $k=3$ is conducted: $3$ individuals are chosen at random from the population, and the one with the best (lowest) fitness is declared the winner and becomes a parent.\n    -   **Crossover:** Selected parents are paired. With a crossover probability $p_c$, a pair of parents $\\mathbf{x}^{(1)}$ and $\\mathbf{x}^{(2)}$ produces two offspring, $\\mathbf{y}^{(1)}$ and $\\mathbf{y}^{(2)}$, via arithmetic crossover:\n        $$\n        \\mathbf{y}^{(1)} = \\omega \\mathbf{x}^{(1)} + (1-\\omega)\\mathbf{x}^{(2)}\n        $$\n        $$\n        \\mathbf{y}^{(2)} = \\omega \\mathbf{x}^{(2)} + (1-\\omega)\\mathbf{x}^{(1)}\n        $$\n        where $\\omega$ is a random variable drawn from $\\text{Uniform}[0,1]$. If crossover does not occur, the offspring are direct copies of the parents.\n    -   **Mutation:** Each gene (component) of each new offspring is subjected to mutation with probability $p_m$. If a gene $y_j$ is selected for mutation, its value is altered by adding Gaussian noise:\n        $$\n        y_j \\leftarrow y_j + \\mathcal{N}\\!\\left(0, \\sigma^2 (\\overline{b}_j - \\underline{b}_j)^2\\right)\n        $$\n        where $[\\underline{b}_j, \\overline{b}_j]$ is the valid range for gene $j$ and $\\sigma$ is a given mutation scale factor. After mutation, the gene's value is clipped to ensure it remains within its bounds $[\\underline{b}_j, \\overline{b}_j]$.\n\nThis entire procedure is deterministic due to the use of a specified random seed for each test case. The algorithm is executed for the three distinct test cases provided, and the best fitness value found upon termination is reported for each.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and report results for all test cases.\n    \"\"\"\n    # Global constants for the objective function as per the problem statement.\n    CONSTANTS = {\n        'c': [0.2, 0.1, 0.15, 0.3, 0.05, 0.6, 0.5], # c0 through c6\n        'M': 10.0,\n        'B': 100.0\n    }\n\n    def objective_function(x, bounds):\n        \"\"\"\n        Calculates the objective function value for a given individual x.\n        x is a numpy array [t, s, phi].\n        bounds is a dictionary {'t':(min,max), 's':(min,max), 'phi':(min,max)}.\n        \"\"\"\n        t, s, phi = x[0], x[1], x[2]\n        c0, c1, c2, c3, c4, c5, c6 = CONSTANTS['c']\n        M = CONSTANTS['M']\n        B = CONSTANTS['B']\n        \n        t_min, t_max = bounds['t']\n        s_min, s_max = bounds['s']\n        phi_min, phi_max = bounds['phi']\n\n        # Base surrogate objective function F(t,s,phi)\n        f_base = (c0 + c1 * t + c2 * s + c3 * math.sin(phi) + \n                  c4 * t * s - c5 * s * math.sin(phi) - \n                  c6 * (1.0 - t) * math.sin(phi))\n\n        # Derived relative density proxy and its penalty\n        r_d = 0.5 * t + 0.5 * (1.0 - s)\n        pi_dens = M * (max(0.0, r_d - 0.8)**2) + M * (max(0.0, 0.2 - r_d)**2)\n\n        # Box constraint penalty. This is included for formal correctness, though\n        # clipping in the GA ensures individuals are always within bounds.\n        pi_box_t = B * (max(0.0, t - t_max)**2) + B * (max(0.0, t_min - t)**2)\n        pi_box_s = B * (max(0.0, s - s_max)**2) + B * (max(0.0, s_min - s)**2)\n        pi_box_phi = B * (max(0.0, phi - phi_max)**2) + B * (max(0.0, phi_min - phi)**2)\n        pi_box = pi_box_t + pi_box_s + pi_box_phi\n\n        return f_base + pi_dens + pi_box\n\n    class GeneticAlgorithm:\n        \"\"\"\n        Implements the specified real-coded Genetic Algorithm.\n        \"\"\"\n        def __init__(self, bounds, N, G, pc, pm, sigma, k, seed):\n            self.bounds = bounds\n            self.bounds_arr = np.array([bounds['t'], bounds['s'], bounds['phi']])\n            self.N = N\n            self.G = G\n            self.pc = pc\n            self.pm = pm\n            self.sigma = sigma\n            self.k = k\n            self.rng = np.random.default_rng(seed)\n            self.E = max(1, math.floor(0.05 * self.N))\n            self.population = None\n            self.best_fitness_overall = float('inf')\n\n        def _initialize_population(self):\n            pop = np.zeros((self.N, 3))\n            pop[:, 0] = self.rng.uniform(self.bounds['t'][0], self.bounds['t'][1], self.N)\n            pop[:, 1] = self.rng.uniform(self.bounds['s'][0], self.bounds['s'][1], self.N)\n            pop[:, 2] = self.rng.uniform(self.bounds['phi'][0], self.bounds['phi'][1], self.N)\n            return pop\n\n        def _evaluate_population(self, pop):\n            return np.array([objective_function(ind, self.bounds) for ind in pop])\n\n        def _tournament_selection(self, pop, fitnesses):\n            indices = self.rng.choice(self.N, size=self.k, replace=False)\n            tournament_fitnesses = fitnesses[indices]\n            winner_idx_in_tournament = np.argmin(tournament_fitnesses)\n            winner_idx_in_pop = indices[winner_idx_in_tournament]\n            return pop[winner_idx_in_pop]\n\n        def _mutate_individual(self, ind):\n            for j in range(3):\n                if self.rng.random()  self.pm:\n                    gene_range = self.bounds_arr[j, 1] - self.bounds_arr[j, 0]\n                    std_dev = self.sigma * gene_range\n                    noise = self.rng.normal(loc=0.0, scale=std_dev)\n                    ind[j] += noise\n            return np.clip(ind, self.bounds_arr[:, 0], self.bounds_arr[:, 1])\n\n        def run(self):\n            self.population = self._initialize_population()\n\n            for _ in range(self.G):\n                fitnesses = self._evaluate_population(self.population)\n                \n                # Track the best fitness found in any generation\n                min_fitness_current_gen = np.min(fitnesses)\n                if min_fitness_current_gen  self.best_fitness_overall:\n                    self.best_fitness_overall = min_fitness_current_gen\n\n                # Elitism: preserve the best E individuals\n                elite_indices = np.argsort(fitnesses)[:self.E]\n                elites = self.population[elite_indices]\n                \n                new_population = [None] * self.N\n                new_population[:self.E] = elites\n\n                # Reproduction: create N-E new individuals\n                num_offspring = self.N - self.E\n                \n                offspring_list = []\n                for _ in range(num_offspring // 2):\n                    p1 = self._tournament_selection(self.population, fitnesses)\n                    p2 = self._tournament_selection(self.population, fitnesses)\n\n                    if self.rng.random()  self.pc:\n                        omega = self.rng.random()\n                        c1 = omega * p1 + (1 - omega) * p2\n                        c2 = omega * p2 + (1 - omega) * p1\n                    else:\n                        c1, c2 = p1.copy(), p2.copy()\n                    \n                    offspring_list.append(self._mutate_individual(c1))\n                    offspring_list.append(self._mutate_individual(c2))\n\n                if num_offspring % 2 == 1:\n                    p1 = self._tournament_selection(self.population, fitnesses)\n                    c1 = p1.copy()\n                    offspring_list.append(self._mutate_individual(c1))\n\n                new_population[self.E:] = offspring_list\n                self.population = np.array(new_population)\n            \n            # Final evaluation at termination\n            final_fitnesses = self._evaluate_population(self.population)\n            final_min_fitness = np.min(final_fitnesses)\n            if final_min_fitness  self.best_fitness_overall:\n                 self.best_fitness_overall = final_min_fitness\n            \n            return self.best_fitness_overall\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"bounds\": {'t': (0.05, 0.95), 's': (0.10, 0.90), 'phi': (0.10, 1.30)},\n            \"N\": 60, \"G\": 120, \"pc\": 0.9, \"pm\": 0.2, \"sigma\": 0.05, \"k\": 3, \"seed\": 12345\n        },\n        {\n            \"bounds\": {'t': (0.05, 0.95), 's': (0.10, 0.90), 'phi': (0.10, 0.25)},\n            \"N\": 80, \"G\": 180, \"pc\": 0.9, \"pm\": 0.2, \"sigma\": 0.04, \"k\": 3, \"seed\": 2023\n        },\n        {\n            \"bounds\": {'t': (0.05, 0.95), 's': (0.10, 0.90), 'phi': (1.10, 1.30)},\n            \"N\": 25, \"G\": 300, \"pc\": 0.8, \"pm\": 0.3, \"sigma\": 0.06, \"k\": 3, \"seed\": 314159\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        ga = GeneticAlgorithm(\n            bounds=case[\"bounds\"], N=case[\"N\"], G=case[\"G\"],\n            pc=case[\"pc\"], pm=case[\"pm\"], sigma=case[\"sigma\"],\n            k=case[\"k\"], seed=case[\"seed\"]\n        )\n        best_f = ga.run()\n        results.append(best_f)\n\n    # Final print statement in the exact required format.\n    rounded_results = [round(r, 6) for r in results]\n    print(f\"[{','.join(map(str, rounded_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final, advanced practice moves beyond standard GA implementations to the design of domain-aware operators for a classic NP-hard problem: the minimum vertex cover. You will construct a specialized crossover operator that identifies and preserves \"critical sub-graphs\" within a graph structure, treating them as essential building blocks for a good solution . This exercise teaches the sophisticated but powerful concept of encoding problem-specific knowledge directly into your genetic operators, a technique that can dramatically improve a GA's performance on difficult combinatorial problems. It also reinforces the necessity of post-processing steps like repair and pruning algorithms to ensure the validity and quality of solutions.",
            "id": "2396605",
            "problem": "You are given a finite, undirected, simple graph $G = (V,E)$ with $|V| = n$ and $|E| = m$. A vertex cover is a subset $C \\subseteq V$ such that every edge $(u,v) \\in E$ has $u \\in C$ or $v \\in C$. The minimum vertex cover problem is to find a vertex cover of minimum cardinality. A chromosome is a binary vector $x \\in \\{0,1\\}^n$, where index $i$ corresponds to vertex $i \\in V$ and $x_i = 1$ if and only if vertex $i$ is included in the candidate cover. The size of the cover represented by $x$ is $|x| = \\sum_{i=0}^{n-1} x_i$. Define the uncovered edge count for a chromosome $x$ as $U(x) = |\\{(u,v)\\in E \\mid x_u = 0 \\wedge x_v = 0\\}|$. Define the penalized fitness as $f(x) = |x| + M \\cdot U(x)$, with $M = n + 1$.\n\nDefine critical sub-graphs as follows. A triangle is any $3$-vertex subset $\\{i,j,k\\} \\subseteq V$ such that all three edges $(i,j)$, $(j,k)$, and $(i,k)$ are in $E$. A star centered at $c \\in V$ is the induced sub-graph on $\\{c\\} \\cup N(c)$, where $N(c)$ is the neighborhood of $c$, under the constraint $\\deg(c) \\ge 3$. Construct a disjoint family $\\mathcal{H}$ of critical sub-graphs by the following rules, applied in order: first include all triangles; then, in non-increasing order of degree, include a star centered at $c$ only if its vertex set is disjoint from all vertex sets of sub-graphs already in $\\mathcal{H}$; stop when all centers with degree at least $3$ have been considered. Thus, every $H \\in \\mathcal{H}$ is either a triangle or a star, and the vertex sets of sub-graphs in $\\mathcal{H}$ are pairwise disjoint.\n\nA crossover operator $X$ takes as input the graph $G$, the disjoint critical sub-graphs family $\\mathcal{H}$, and two parent chromosomes $p^{(1)}, p^{(2)} \\in \\{0,1\\}^n$, and produces a child chromosome $c \\in \\{0,1\\}^n$ satisfying all of the following properties:\n\n- Preservation over critical sub-graphs: For each $H \\in \\mathcal{H}$ with vertex set $V(H)$, define the local uncovered count $U_H(p) = |\\{(u,v)\\in E \\mid u \\in V(H), v \\in V(H), p_u = 0, p_v = 0\\}|$, and the local size $|p|_H = \\sum_{i \\in V(H)} p_i$. Define the local cost $g_H(p) = |p|_H + M \\cdot U_H(p)$. The child must satisfy $c_i = p^{(k)}_i$ for all $i \\in V(H)$, where $k \\in \\{1,2\\}$ is chosen to minimize $g_H(p^{(k)})$; in case of a tie, either parent may be chosen with equal probability.\n- Outside critical sub-graphs: For all $j \\in V \\setminus \\bigcup_{H \\in \\mathcal{H}} V(H)$, the child must satisfy $c_j \\in \\{p^{(1)}_j, p^{(2)}_j\\}$, with $c_j = p^{(1)}_j$ and $c_j = p^{(2)}_j$ occurring with equal probability and independently across such positions.\n\nAfter crossover, a deterministic feasibility repair must transform $c$ into a vertex cover, followed by a deterministic pruning step that iteratively removes any redundant selected vertex (sets $x_i$ to $0$) if removal maintains feasibility, until no such removal is possible. The feasibility repair and pruning may only modify the child $c$ and must terminate in finite time.\n\nTask. Implement a complete program that:\n\n- For each test graph listed below, performs a population-based search over $\\{0,1\\}^n$ that applies the above crossover operator $X$ as the sole recombination mechanism and uses the penalized fitness $f(x)$ to guide search. The search must be stochastic but reproducible by fixing a random seed. It must include random initialization and pointwise bit mutation with per-bit mutation probability equal to $1/n$, applied to offspring before repair and pruning.\n- For each test graph, after a fixed computational budget that is the same for all graphs, return the best chromosome found under $f(x)$, together with whether it is a valid vertex cover.\n\nTest suite. Use the following graphs; vertices are labeled from $0$ to $n-1$ and each edge $(u,v)$ is undirected:\n\n- Test $1$ (non-bipartite, contains a triangle): $n = 3$, $E = \\{(0,1),(1,2),(0,2)\\}$.\n- Test $2$ (star, high-degree center): $n = 6$, $E = \\{(0,1),(0,2),(0,3),(0,4),(0,5)\\}$.\n- Test $3$ (cycle with a diagonal, contains a triangle): $n = 4$, $E = \\{(0,1),(1,2),(2,3),(3,0),(0,2)\\}$.\n- Test $4$ (boundary case, empty): $n = 5$, $E = \\emptyset$.\n\nComputational budget and reproducibility. Use the same fixed random seed for all tests. For each test, use the same population size and the same number of generations. Mutation must be applied with per-bit probability $1/n$. The feasibility repair and pruning must be applied to all individuals before evaluating $f(x)$.\n\nFinal output. Your program must produce a single line of output aggregating all test results as a comma-separated list enclosed in square brackets. For each test in the order listed above, output two integers: the best cover size found $|x^\\star|$ and an indicator of feasibility defined as $1$ if $U(x^\\star) = 0$ and $0$ otherwise. The final output format is therefore a single list of length $8$: $[|x^\\star_1|,b_1,|x^\\star_2|,b_2,|x^\\star_3|,b_3,|x^\\star_4|,b_4]$, where $b_i \\in \\{0,1\\}$ for each test $i$.",
            "solution": "The core of the task is to implement a genetic algorithm (GA) with a highly specialized crossover operator and a post-recombination repair-and-prune sequence. We will now delineate the design of this algorithm based on the principles provided.\n\nA chromosome is a binary vector $x \\in \\{0,1\\}^n$, where $n$ is the number of vertices. If $x_i = 1$, vertex $i$ is in the candidate vertex cover; if $x_i = 0$, it is not. The quality of a chromosome is assessed by the penalized fitness function $f(x) = |x| + M \\cdot U(x)$, where $|x|$ is the size of the cover (number of vertices with $x_i=1$), $U(x)$ is the number of edges not covered by the set of vertices, and $M=n+1$ is a penalty coefficient. The choice of $M  n$ ensures that any infeasible solution (with $U(x) \\ge 1$) has a worse fitness value than any feasible solution (where $U(x)=0$), as the maximum size of a feasible cover is $n$.\n\nThe specified genetic algorithm operates as follows:\n\n1.  **Critical Sub-graph Identification**: Before the evolutionary process begins, the graph $G$ is analyzed to identify a disjoint family of critical sub-graphs, $\\mathcal{H}$. The construction process is sequential and deterministic:\n    *   First, all triangles in the graph are identified. A greedy approach is taken to form a disjoint set of these triangles: they are processed in a deterministic order (e.g., sorted lexicographically by their vertex indices), and a triangle is added to $\\mathcal{H}$ only if its vertex set is disjoint from the vertices of sub-graphs already added to $\\mathcal{H}$.\n    *   Next, vertices are considered as potential star centers in non-increasing order of their degree. A star centered at $c$ with degree $\\deg(c) \\ge 3$ is added to $\\mathcal{H}$ only if its vertex set, $\\{c\\} \\cup N(c)$, is disjoint from the vertices of all sub-graphs currently in $\\mathcal{H}$.\n    This procedure yields a set $\\mathcal{H}$ of pairwise vertex-disjoint triangles and stars. Vertices not belonging to any sub-graph in $\\mathcal{H}$ are treated separately during crossover.\n\n2.  **Genetic Algorithm Loop**: The GA proceeds for a fixed number of generations with a constant population size.\n    *   **Initialization**: An initial population of random chromosomes is created. Each is then processed by the repair and prune operators to ensure the starting population consists entirely of valid, minimal vertex covers.\n    *   **Selection**: To produce offspring, parents are selected from the current population. Since the main population consists of valid covers, their fitness $f(x)$ simplifies to their size $|x|$. A standard tournament selection is appropriate: a small number of individuals are chosen at random from the population, and the one with the smallest size (best fitness) is selected as a parent.\n    *   **Crossover**: The specialized crossover operator $X$ generates a single child $c$ from two parents, $p^{(1)}$ and $p^{(2)}$.\n        *   For each critical sub-graph $H \\in \\mathcal{H}$, the operator evaluates the local cost $g_H(p) = |p|_H + M \\cdot U_H(p)$ for both parents. This cost assesses the parent's performance restricted to the vertices and induced edges of $H$. The block of genes corresponding to the vertices of $H$ is copied to the child from the parent with the superior (lower) local cost. Ties are broken randomly. It is critical to note that even for a valid global cover $p$, the local uncovered count $U_H(p)$ may be non-zero if an edge within $H$ is covered by a vertex outside $H$.\n        *   For all vertices not part of any sub-graph in $\\mathcal{H}$, the child's genes are determined by uniform crossover: $c_j$ is randomly chosen from $\\{p^{(1)}_j, p^{(2)}_j\\}$ with equal probability.\n    *   **Mutation**: The newly created child chromosome undergoes pointwise bit-flip mutation, where each bit $c_i$ is flipped with a probability of $1/n$.\n    *   **Repair and Pruning**: The (likely infeasible) child chromosome is deterministically transformed into a valid, minimal vertex cover.\n        1.  **Feasibility Repair**: The chromosome is made a valid vertex cover. A deterministic rule is applied: iterate through all edges of the graph in a fixed order. If an edge $(u,v)$ is found to be uncovered ($x_u=0, x_v=0$), one of the vertices is added to the cover (e.g., by setting $x_u=1$, where $uv$).\n        2.  **Pruning**: The resulting valid cover is made minimal (but not necessarily minimum) by a greedy iterative process. In a fixed order, iterate through each vertex $i$ in the cover ($x_i=1$). Temporarily remove it ($x_i=0$) and check if the cover remains valid. If it does, the removal is made permanent. If not, the vertex is restored. This process is repeated until no more vertices can be removed in a full pass.\n    *   **Replacement**: The new generation is formed from the offspring produced. An elitist strategy is employed, where the best individual from the parent generation is guaranteed to survive into the next, preserving the best-found solution.\n    *   **Termination**: The algorithm terminates after a fixed number of generations. The best chromosome found throughout the entire run, evaluated by its size, is reported as the result. As all evaluated individuals in the population are valid covers, the feasibility flag will be $1$.\n\nThis design fulfills all requirements of the problem statement. For reproducibility, a fixed random seed is used. For the specified test suite, this deterministic algorithm will produce a single, verifiable output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A complete and runnable Python 3.12 program.\n# This program implements the specified genetic algorithm for the minimum vertex cover problem.\n# It uses only the numpy library (version 1.23.5) as permitted.\n\n# Define GA parameters not specified in the problem statement.\n# These are kept constant across all test cases.\nPOPULATION_SIZE = 50\nGENERATIONS = 50\nTOURNAMENT_SIZE = 3\nRANDOM_SEED = 42\n\nclass VertexCoverGA:\n    \"\"\"\n    Implements the specialized genetic algorithm for the Minimum Vertex Cover problem\n    as described in the problem statement.\n    \"\"\"\n    def __init__(self, n, edges, pop_size, generations, seed):\n        self.n = n\n        # Ensure edges are consistently ordered, e.g., (min, max)\n        self.edges = sorted([tuple(sorted(e)) for e in edges])\n        self.pop_size = pop_size\n        self.generations = generations\n        self.rng = np.random.default_rng(seed)\n        self.M = n + 1\n        \n        self.adj = [set() for _ in range(n)]\n        for u, v in self.edges:\n            self.adj[u].add(v)\n            self.adj[v].add(u)\n        \n        self.H, self.non_h_vertices = self._find_critical_subgraphs()\n\n    def _find_critical_subgraphs(self):\n        \"\"\"\n        Constructs the disjoint family of critical sub-graphs H, following the\n        specified rules: triangles first, then stars.\n        \"\"\"\n        H = []\n        used_vertices = set()\n        \n        # 1. Greedily find all disjoint triangles\n        all_triangles = set()\n        if self.n >= 3:\n            for i in range(self.n):\n                # Sort neighbors to ensure deterministic processing and triangle representation\n                neighbors = sorted(list(self.adj[i]))\n                for j_idx in range(len(neighbors)):\n                    for k_idx in range(j_idx + 1, len(neighbors)):\n                        v, w = neighbors[j_idx], neighbors[k_idx]\n                        if v  i and w  i: # Avoid re-discovering triangles\n                            continue\n                        if w in self.adj[v]:\n                            triangle = tuple(sorted((i, v, w)))\n                            all_triangles.add(triangle)\n\n        # Sort triangles to have a deterministic order for greedy selection\n        sorted_triangles = sorted(list(all_triangles))\n        \n        for t_vertices in sorted_triangles:\n             t_set = set(t_vertices)\n             if not t_set.intersection(used_vertices):\n                 H.append({'type': 'triangle', 'vertices': t_set})\n                 used_vertices.update(t_set)\n\n        # 2. Greedily find disjoint stars\n        degrees = [(i, len(self.adj[i])) for i in range(self.n)]\n        degrees.sort(key=lambda x: (-x[1], x[0])) # Sort by degree desc, then index asc\n\n        for i, deg in degrees:\n            if deg  3:\n                break\n            star_vertices = {i} | self.adj[i]\n            if not star_vertices.intersection(used_vertices):\n                H.append({'type': 'star', 'vertices': star_vertices})\n                used_vertices.update(star_vertices)\n        \n        non_h_vertices = sorted([v for v in range(self.n) if v not in used_vertices])        \n        return H, non_h_vertices\n\n    def _local_cost(self, p, h_block_vertices):\n        size_h = sum(p[i] for i in h_block_vertices)\n        uncovered_h = 0\n        h_v_list = list(h_block_vertices)\n        for i in range(len(h_v_list)):\n            for j in range(i + 1, len(h_v_list)):\n                u, v = h_v_list[i], h_v_list[j]\n                if v in self.adj[u] and p[u] == 0 and p[v] == 0:\n                    uncovered_h += 1\n        return size_h + self.M * uncovered_h\n\n    def crossover(self, p1, p2):\n        child = np.zeros(self.n, dtype=np.int8)\n        \n        for h_block in self.H:\n            h_vertices = h_block['vertices']\n            cost1 = self._local_cost(p1, h_vertices)\n            cost2 = self._local_cost(p2, h_vertices)\n            \n            winner = p1 if cost1  cost2 else p2 if cost2  cost1 else (p1 if self.rng.random()  0.5 else p2)\n            \n            for v_idx in h_vertices:\n                child[v_idx] = winner[v_idx]\n\n        for j in self.non_h_vertices:\n            child[j] = p1[j] if self.rng.random()  0.5 else p2[j]\n\n        return child\n\n    def mutate(self, chromosome):\n        if self.n == 0: return chromosome\n        mutation_prob = 1.0 / self.n if self.n > 0 else 0.0\n        if mutation_prob > 0:\n            for i in range(self.n):\n                if self.rng.random()  mutation_prob:\n                    chromosome[i] = 1 - chromosome[i]\n        return chromosome\n\n    def is_cover(self, chromosome):\n        return all(chromosome[u] == 1 or chromosome[v] == 1 for u, v in self.edges)\n\n    def repair(self, chromosome):\n        for u, v in self.edges:\n            if chromosome[u] == 0 and chromosome[v] == 0:\n                chromosome[u] = 1 # Deterministic rule: add vertex with smaller index\n        return chromosome\n\n    def prune(self, chromosome):\n        changed = True\n        node_order = list(range(self.n))\n        while changed:\n            changed = False\n            for i in node_order:\n                if chromosome[i] == 1:\n                    chromosome[i] = 0\n                    if not self.is_cover(chromosome):\n                        chromosome[i] = 1\n                    else:\n                        changed = True\n        return chromosome\n\n    def run(self):\n        population = self.rng.integers(0, 2, size=(self.pop_size, self.n), dtype=np.int8)\n        \n        for i in range(self.pop_size):\n            population[i] = self.repair(population[i])\n            population[i] = self.prune(population[i])\n\n        # Handle case of n=0 graph\n        if self.n == 0:\n            return 0, 1\n\n        best_chromosome = population[0].copy()\n        best_fitness = np.sum(best_chromosome)\n\n        for _ in range(self.generations):\n            fitnesses = np.sum(population, axis=1)\n\n            current_best_idx = np.argmin(fitnesses)\n            if fitnesses[current_best_idx]  best_fitness:\n                best_fitness = fitnesses[current_best_idx]\n                best_chromosome = population[current_best_idx].copy()\n            \n            new_population = np.zeros((self.pop_size, self.n), dtype=np.int8)\n            new_population[0] = best_chromosome.copy() # Elitism\n            \n            for i in range(1, self.pop_size):\n                tourn_indices1 = self.rng.choice(self.pop_size, TOURNAMENT_SIZE, replace=False)\n                p1_idx = tourn_indices1[np.argmin(fitnesses[tourn_indices1])]\n                \n                tourn_indices2 = self.rng.choice(self.pop_size, TOURNAMENT_SIZE, replace=False)\n                p2_idx = tourn_indices2[np.argmin(fitnesses[tourn_indices2])]\n\n                parent1 = population[p1_idx]\n                parent2 = population[p2_idx]\n                \n                child = self.crossover(parent1, parent2)\n                child = self.mutate(child)\n                child = self.repair(child)\n                child = self.prune(child)\n                \n                new_population[i] = child\n            \n            population = new_population\n\n        best_size = np.sum(best_chromosome)\n        is_feasible_flag = 1 if self.is_cover(best_chromosome) else 0\n        \n        return int(best_size), is_feasible_flag\n\ndef solve():\n    \"\"\"\n    Main function to run the GA on the test suite and print the results\n    in the specified format.\n    \"\"\"\n    test_cases = [\n        {'n': 3, 'edges': [(0, 1), (1, 2), (0, 2)]},\n        {'n': 6, 'edges': [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)]},\n        {'n': 4, 'edges': [(0, 1), (1, 2), (2, 3), (3, 0), (0, 2)]},\n        {'n': 5, 'edges': []},\n    ]\n\n    results = []\n    for case in test_cases:\n        ga = VertexCoverGA(\n            n=case['n'],\n            edges=case['edges'],\n            pop_size=POPULATION_SIZE,\n            generations=GENERATIONS,\n            seed=RANDOM_SEED\n        )\n        best_size, is_feasible = ga.run()\n        results.extend([best_size, is_feasible])\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}