{
    "hands_on_practices": [
        {
            "introduction": "罚函数方法的核心思想是，将一个约束优化问题转化为一系列无约束优化问题来求解。第一个练习  将带你动手实践这一核心思想，要求你从基本原理出发，构建一个序列二次罚函数求解器。你还将研究“热启动”（warm-start）策略，这是在实际应用中加速算法收敛的一项关键技术。",
            "id": "2423453",
            "problem": "要求你实现一个用于约束优化的序列二次罚函数法，并通过实验量化热启动的益处，即重用前一个子问题的解作为下一个惩罚参数的初始猜测值。实现一个带有回溯 Armijo 线搜索的基于梯度的求解器，以最小化一系列无约束的惩罚子问题。对于固定的惩罚参数序列 $\\{\\rho_k\\}_{k=1}^K$ 且满足 $\\rho_1  \\rho_2  \\dots  \\rho_K$，比较每个测试用例的两种策略：(i) 从相同的初始点冷启动每个子问题，以及 (ii) 从子问题 $k$ 计算出的最小化器热启动子问题 $k+1$。报告加速比因子，该因子定义为在整个惩罚序列中，冷启动所用的梯度下降总迭代次数与热启动所用的总迭代次数之比。\n\n使用的基本原理和定义：\n- 一个约束最小化问题具有目标函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$、不等式约束 $g_i(x)\\le 0$（其中 $i\\in\\{1,\\dots,m\\}$）和等式约束 $h_j(x)=0$（其中 $j\\in\\{1,\\dots,p\\}$）。\n- 对于不等式的经典二次惩罚应用于其违反量，形式为 $\\max\\{0, g_i(x)\\}^2$；对于等式则为 $h_j(x)^2$。\n- 对于给定的 $\\rho0$，惩罚子问题是最小化\n$$\n\\Phi_\\rho(x)=f(x)+\\rho\\left(\\sum_{i=1}^m \\max\\{0,g_i(x)\\}^2+\\sum_{j=1}^p h_j(x)^2\\right).\n$$\n- 使用带有回溯 Armijo 准则的梯度下降法：给定当前点 $x$、梯度 $\\nabla\\Phi_\\rho(x)$、初始步长 $t_0$、收缩因子 $\\beta\\in(0,1)$ 和 Armijo 参数 $c\\in(0,1)$，从序列 $\\{t_0, \\beta t_0, \\beta^2 t_0,\\dots\\}$ 中选择满足以下条件的最大 $t$：\n$$\n\\Phi_\\rho(x - t \\nabla \\Phi_\\rho(x)) \\le \\Phi_\\rho(x) - c\\,t\\,\\|\\nabla \\Phi_\\rho(x)\\|_2^2.\n$$\n- 当 $\\|\\nabla \\Phi_\\rho(x)\\|_2\\le \\varepsilon$ 时，停止内部求解器。\n\n实现要求：\n- 完全按照上述定义实现二次罚函数法和带有回溯 Armijo 线搜索的梯度下降法。\n- 对于不等式约束，仅通过在惩罚值及其梯度中使用 $\\max\\{0,\\cdot\\}$ 结构来处理正向违反量。对于等式约束，惩罚残差的平方。\n- 使用惩罚序列 $\\rho\\in\\{10,10^2,10^3\\}$，即 $\\rho \\in \\{10,100,1000\\}$。\n- 对所有子问题，使用梯度容差 $\\varepsilon=10^{-6}$、Armijo 参数 $c=10^{-4}$、收缩因子 $\\beta=\\tfrac{1}{2}$ 和初始步长 $t_0=1$。将每个子问题的最大梯度迭代次数限制为 $N_{\\max}=10^4$。\n- 统计收敛一个子问题所需的外部梯度下降迭代次数（每次线搜索后接受的步数）；不要单独计算线搜索的回溯步数。\n\n测试套件：\n实现并求解以下三个二维测试用例。在每个用例中，返回加速比因子\n$$\nS=\\frac{N_{\\mathrm{cold}}}{N_{\\mathrm{warm}}},\n$$\n其中 $N_{\\mathrm{cold}}$ 是当每个子问题都从指定的初始点冷启动时，在所有惩罚参数上累加的梯度下降总迭代次数；$N_{\\mathrm{warm}}$ 是当每个子问题都从前一个子问题的解热启动时，累加的总迭代次数。\n\n- 用例 $\\mathbf{A}$（凸二次函数带一个有效的线性不等式）：\n  - 目标函数：$f(x,y)=(x-1)^2+2\\,(y+2)^2$。\n  - 不等式：$g_1(x,y)=1-x-y\\le 0$。\n  - 无等式。\n  - 初始点：$x_0=(0,0)$。\n\n- 用例 $\\mathbf{B}$（凸二次函数带一个等式）：\n  - 目标函数：$f(x,y)=(x-3)^2+(y-1)^2$。\n  - 等式：$h_1(x,y)=x-y=0$。\n  - 无不等式。\n  - 初始点：$x_0=(0,0)$。\n\n- 用例 $\\mathbf{C}$（凸二次函数带一个曲线不等式）：\n  - 目标函数：$f(x,y)=(x+2)^2+y^2$。\n  - 不等式：$g_1(x,y)=x^2+y^2-1\\le 0$。\n  - 无等式。\n  - 初始点：$x_0=(0,0)$。\n\n输出规范：\n- 对于每个用例，计算如上定义的加速比因子 $S$。\n- 你的程序应生成单行输出，其中包含三个加速比因子，形式为方括号括起来的逗号分隔列表，顺序为 $\\left[S_A,S_B,S_C\\right]$，其中 $S_A$ 对应于用例 $\\mathbf{A}$，$S_B$ 对应于用例 $\\mathbf{B}$，$S_C$ 对应于用例 $\\mathbf{C}$。例如，输出形式为 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$，其中包含数值。\n- 将每个加速比因子表示为浮点数。你可以在内部进行四舍五入，但打印的值必须是标准的十进制浮点数。\n\n不涉及物理单位。不使用角度。不使用百分比。\n\n最终程序必须是自包含的，不需要任何输入，并遵守指定的运行时环境。正确性将通过验证实现是否遵循定义以及热启动是否产生严格更少或至少不更多的迭代次数来评估，从而为指定的用例产生有意义的加速比因子。输出必须是指定格式的单行。",
            "solution": "该问题要求实现序列二次罚函数法来解决约束优化问题。任务的核心是比较两种初始化策略在求解一系列无约束子问题时的计算效率：即冷启动策略与热启动策略。效率将通过加速比来量化，该加速比定义为总梯度下降迭代次数的比率。\n\n约束优化问题的一般形式是最小化目标函数 $f(x)$，同时满足一组不等式约束 $g_i(x) \\le 0$（其中 $i \\in \\{1, \\dots, m\\}$）和等式约束 $h_j(x) = 0$（其中 $j \\in \\{1, \\dots, p\\}$），这里 $x \\in \\mathbb{R}^n$。\n\n二次罚函数法通过求解一系列无约束最小化问题来逼近此问题的解。对于给定的惩罚参数 $\\rho  0$，通过向原始目标函数添加惩罚项来构造惩罚目标函数 $\\Phi_\\rho(x)$，这些惩罚项用于惩罚对约束的违反。惩罚函数的具体形式是：\n$$\n\\Phi_\\rho(x) = f(x) + \\rho \\left( \\sum_{i=1}^m \\left(\\max\\{0, g_i(x)\\}\\right)^2 + \\sum_{j=1}^p \\left(h_j(x)\\right)^2 \\right)\n$$\n然后，这个函数 $\\Phi_\\rho(x)$ 将关于 $x$ 进行最小化。通过对一系列递增的惩罚参数 $\\rho_1  \\rho_2  \\dots  \\rho_K$ 求解这个无约束问题，最小化器序列 $x^*(\\rho_k)$ 将收敛到原始约束问题的解。\n\n为了最小化每个无约束子问题 $\\min_x \\Phi_\\rho(x)$，需要一种基于梯度的方法。惩罚目标函数的梯度 $\\nabla \\Phi_\\rho(x)$ 使用链式法则推导得出。对于不等式约束项 $P_i(x) = \\rho (\\max\\{0, g_i(x)\\})^2$，其梯度为 $\\nabla P_i(x) = 2 \\rho \\max\\{0, g_i(x)\\} \\nabla g_i(x)$。对于等式约束项 $Q_j(x) = \\rho (h_j(x))^2$，其梯度为 $\\nabla Q_j(x) = 2 \\rho h_j(x) \\nabla h_j(x)$。将这些与目标函数的梯度结合，得到完整的梯度为：\n$$\n\\nabla \\Phi_\\rho(x) = \\nabla f(x) + 2\\rho \\left( \\sum_{i=1}^m \\max\\{0, g_i(x)\\} \\nabla g_i(x) + \\sum_{j=1}^p h_j(x) \\nabla h_j(x) \\right)\n$$\n无约束最小化是使用梯度下降法进行的。从点 $x_k$ 开始，下一个点 $x_{k+1}$ 通过沿负梯度方向移动来找到：\n$$\nx_{k+1} = x_k - t \\nabla \\Phi_\\rho(x_k)\n$$\n步长 $t  0$ 由采用 Armijo 条件的回溯线搜索确定。对于给定的下降方向 $d_k = -\\nabla \\Phi_\\rho(x_k)$，我们寻找序列 $\\{t_0, \\beta t_0, \\beta^2 t_0, \\dots\\}$ 中满足以下条件的最大 $t$：\n$$\n\\Phi_\\rho(x_k + t d_k) \\le \\Phi_\\rho(x_k) + c \\, t \\, \\nabla \\Phi_\\rho(x_k)^T d_k\n$$\n使用 $d_k = -\\nabla \\Phi_\\rho(x_k)$，这可以简化为问题陈述中给出的形式：\n$$\n\\Phi_\\rho(x_k - t \\nabla \\Phi_\\rho(x_k)) \\le \\Phi_\\rho(x_k) - c \\, t \\, \\|\\nabla \\Phi_\\rho(x_k)\\|_2^2\n$$\n算法迭代进行，直到梯度的范数低于指定的容差 $\\varepsilon$，即 $\\|\\nabla \\Phi_\\rho(x)\\|_2 \\le \\varepsilon$。此求解器的参数是固定的：初始步长 $t_0=1$、Armijo 参数 $c=10^{-4}$、收缩因子 $\\beta=0.5$ 和梯度范数容差 $\\varepsilon=10^{-6}$。每个子问题的最大迭代次数上限为 $N_{\\max}=10^4$。\n\n实验比较了在惩罚参数序列 $\\rho \\in \\{10, 100, 1000\\}$ 上的两种策略：\n1.  **冷启动（Cold-Start）：** 每个针对 $\\rho_k$ 的子问题都从相同的起始点 $x_0$ 初始化。总迭代次数 $N_{\\mathrm{cold}}$ 是独立求解每个子问题所需迭代次数的总和。\n2.  **热启动（Warm-Start）：** 第一个子问题（对于 $\\rho_1=10$）从 $x_0$ 初始化。每个后续的 $\\rho_{k+1}$ 的子问题都使用从前一个 $\\rho_k$ 的子问题获得的解进行初始化。总迭代次数 $N_{\\mathrm{warm}}$ 是此序列中迭代次数的总和。\n\n热启动的理论依据是，解 $x^*(\\rho_k)$ 有望成为 $\\Phi_{\\rho_{k+1}}(x)$ 最小化问题的一个良好初始猜测值，尤其是在 $\\rho_{k+1}$ 不比 $\\rho_k$ 大很多的情况下。这应该会导致更快的收敛。性能提升由加速比 $S = N_{\\mathrm{cold}} / N_{\\mathrm{warm}}$ 来衡量。\n\n实现将通过为每个测试用例的目标函数、约束函数及其各自的梯度定义 Python 函数来着手。一个通用的求解器函数将执行带有 Armijo 线搜索的梯度下降。一个顶层函数将管理惩罚参数序列，应用冷启动和热启动策略，计算每种策略的总迭代次数，并计算加速比。这个过程将对提供的所有三个测试用例重复进行。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    \n    # --- Solver Parameters ---\n    SOLVER_PARAMS = {\n        'epsilon': 1e-6,\n        'c_armijo': 1e-4,\n        'beta': 0.5,\n        't0': 1.0,\n        'n_max': 10000\n    }\n    PENALTY_PARAMS = [10.0, 100.0, 1000.0]\n\n    # --- Test Case Definitions ---\n    \n    # Case A: (x-1)^2 + 2(y+2)^2, s.t. 1-x-y = 0\n    case_A = {\n        'f': lambda x: (x[0] - 1.0)**2 + 2.0 * (x[1] + 2.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 1.0), 4.0 * (x[1] + 2.0)]),\n        'g': [lambda x: 1.0 - x[0] - x[1]],\n        'grad_g': [lambda x: np.array([-1.0, -1.0])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    # Case B: (x-3)^2 + (y-1)^2, s.t. x-y = 0\n    case_B = {\n        'f': lambda x: (x[0] - 3.0)**2 + (x[1] - 1.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 3.0), 2.0 * (x[1] - 1.0)]),\n        'g': [],\n        'grad_g': [],\n        'h': [lambda x: x[0] - x[1]],\n        'grad_h': [lambda x: np.array([1.0, -1.0])],\n        'x0': np.array([0.0, 0.0])\n    }\n    \n    # Case C: (x+2)^2 + y^2, s.t. x^2+y^2-1 = 0\n    case_C = {\n        'f': lambda x: (x[0] + 2.0)**2 + x[1]**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] + 2.0), 2.0 * x[1]]),\n        'g': [lambda x: x[0]**2 + x[1]**2 - 1.0],\n        'grad_g': [lambda x: np.array([2.0 * x[0], 2.0 * x[1]])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    \n    def get_penalized_funcs(case, rho):\n        \"\"\"Creates the penalized function and its gradient for a given case and rho.\"\"\"\n        \n        def phi(x):\n            f_val = case['f'](x)\n            g_sum = sum(max(0, g_func(x))**2 for g_func in case['g'])\n            h_sum = sum(h_func(x)**2 for h_func in case['h'])\n            return f_val + rho * (g_sum + h_sum)\n\n        def grad_phi(x):\n            grad_f_val = case['grad_f'](x)\n            \n            grad_g_sum = np.zeros_like(x, dtype=float)\n            for g_func, grad_g_func in zip(case['g'], case['grad_g']):\n                g_val = g_func(x)\n                if g_val > 0:\n                    grad_g_sum += 2.0 * g_val * grad_g_func(x)\n\n            grad_h_sum = np.zeros_like(x, dtype=float)\n            for h_func, grad_h_func in zip(case['h'], case['grad_h']):\n                h_val = h_func(x)\n                grad_h_sum += 2.0 * h_val * grad_h_func(x)\n                \n            return grad_f_val + rho * (grad_g_sum + grad_h_sum)\n        \n        return phi, grad_phi\n\n    def gradient_descent(phi, grad_phi, x_init, params):\n        \"\"\"\n        Performs gradient descent with backtracking Armijo line search.\n        \"\"\"\n        x = np.copy(x_init)\n        n_iters = 0\n        \n        for k in range(params['n_max']):\n            grad = grad_phi(x)\n            grad_norm_sq = np.dot(grad, grad)\n\n            if np.sqrt(grad_norm_sq) = params['epsilon']:\n                break\n            \n            # Backtracking line search\n            t = params['t0']\n            phi_x = phi(x)\n            \n            while True:\n                x_new = x - t * grad\n                phi_new = phi(x_new)\n                armijo_check = phi_x - params['c_armijo'] * t * grad_norm_sq\n                \n                if phi_new = armijo_check:\n                    break\n                t *= params['beta']\n            \n            x = x_new\n            n_iters += 1\n        \n        return x, n_iters\n\n    def run_penalty_method(case, solver_params, penalty_params):\n        \"\"\"\n        Runs the full sequential penalty method for a case,\n        calculating iterations for both cold and warm starts.\n        \"\"\"\n        # Cold start\n        total_iters_cold = 0\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            _, n_iters = gradient_descent(phi, grad_phi, case['x0'], solver_params)\n            total_iters_cold += n_iters\n            \n        # Warm start\n        total_iters_warm = 0\n        x_warm = np.copy(case['x0'])\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            x_sol, n_iters = gradient_descent(phi, grad_phi, x_warm, solver_params)\n            total_iters_warm += n_iters\n            x_warm = x_sol\n            \n        if total_iters_warm == 0:\n             # This case should not happen in this problem, but is a safeguard.\n             # If cold is also 0, speedup is 1. If cold > 0, speedup is \"infinite\".\n            return 1.0 if total_iters_cold == 0 else float('inf')\n            \n        return float(total_iters_cold) / float(total_iters_warm)\n\n    results = []\n    for case in test_cases:\n        speedup = run_penalty_method(case, SOLVER_PARAMS, PENALTY_PARAMS)\n        results.append(speedup)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了直接求解问题，罚函数方法在更复杂的优化框架中也是一个强大的工具。最后一个练习  将挑战你解决一个特殊问题，其中天真的实现会因罚函数中的指数项导致数值溢出而失败。你需要设计一个能避免溢出的安全算法，这是开发可靠的科学计算软件的一项至关重要的技能。",
            "id": "2423412",
            "problem": "设计并实现一个稳健的外罚函数求解器，该求解器在计算从不可行域一侧逼近约束解的罚函数极小值点序列时，能够避免数值溢出。纯粹从数学角度考虑以下一维约束优化问题：\n- 最小化目标函数 $f(x) = (x - a)^2$，服从不等式约束 $c(x) \\le 0$，其中 $c(x) = e^{b x} - 1$ 且 $a  0, b  0$。可行集为 $\\{x \\in \\mathbb{R} : x \\le 0\\}$，约束解为 $x^* = 0$。\n- 对于外二次罚参数序列 $\\{\\mu_k\\}_{k=1}^{\\infty}$（其中 $\\mu_k  0$），定义罚目标函数\n$$\nF_k(x) = (x - a)^2 + \\mu_k \\,\\big(\\max\\{0, e^{b x} - 1\\}\\big)^2.\n$$\n对每个 $k$，令 $x_k^*$ 为 $F_k(x)$ 在 $x \\in \\mathbb{R}$ 上的全局极小值点。根据微积分和凸性的基本原理可知，当 $a  0$ 时，对于所有有限的 $\\mu_k$，都有 $x_k^*  0$；并且当 $\\mu_k \\to \\infty$ 时，$x_k^* \\to 0^+$。因此，该序列从不可行域一侧 $x  0$ 逼近 $x^*$。\n难点在于：在不可行域一侧计算 $e^{b x}$ 时，即使 $x_k^*$ 本身保持在接近 $0$ 的位置，当 $b$ 或 $x$ 中等大时也可能发生溢出。一个稳健的数值设计必须确保在计算 $x_k^*$ 的过程中绝不触发溢出，同时保持计算的精确性。\n\n您的任务：\n1) 从微积分的基本定义出发，论证保证在 $a  0$ 时罚函数极小值点 $x_k^*$ 存在、唯一且从右侧逼近（$x_k^* \\to 0^+$）的单调性和凸性性质。\n\n2) 设计一个算法，用于计算给定 $(a,b,\\mu)$ 下的 $x_k^*$，该算法应满足：\n- 在右侧 $x  0$ 上使用一阶最优性条件，\n$$\n\\frac{d}{dx}F(x) = 2(x - a) + 2 \\mu b \\, e^{b x}\\,(e^{b x} - 1) = 0 \\quad \\text{for } x  0,\n$$\n并利用当 $x \\le 0$ 时惩罚项为零这一事实。\n- 利用 $\\frac{d}{dx}F(x)$ 在 $(0,\\infty)$ 上关于 $x$ 的严格单调性，对导数方程安全地应用区间限定和二分法，而无需在可能导致危险的大数值参数下计算 $e^{b x}$。\n- 使用基本技术实现防溢出的计算，例如：\n  • 保护指数参数 $s = b x$，当 $2s$ 超过浮点溢出阈值时避免计算 $e^s$。\n  • 当 $|s|$ 很小时，通过数值稳定的形式使用函数 $e^s - 1$。\n您必须确保代码在任何时候都不会执行产生溢出的操作，同时仍能计算出精确的根。\n\n3) 实现一个完整的程序，对于每个指定的 $(a,b,\\mu)$，返回唯一的极小值点 $x_k^*$（实数）。您的程序必须硬编码以下测试套件，计算相应的极小值点，并将其作为最终输出打印。\n\n测试套件：\n- 案例 1：$(a,b,\\mu) = (1,\\;50,\\;10^{-6})$。\n- 案例 2：$(a,b,\\mu) = (1,\\;200,\\;10^{3})$。\n- 案例 3：$(a,b,\\mu) = (1,\\;500,\\;10^{12})$。\n- 案例 4：$(a,b,\\mu) = (1,\\;1000,\\;10^{20})$。\n\n不涉及角度单位。此问题中没有物理单位。\n\n输出规范：\n- 对于每个案例，将极小值点 $x_k^*$ 计算为浮点数，并四舍五入到小数点后10位。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如：“[0.0123456789,0.0012345678,0.0001234567,0.0000123456]”）。\n\n您的实现必须是一个完整、可运行的程序。它不能读取任何输入，也不能访问任何外部文件或网络。唯一允许使用的库是执行环境指定的 Python 标准库、NumPy 和 SciPy。您实现的数值方法必须由第1项中的推理来证明其合理性，并且必须在不可行域 $x  0$ 上对 $e^{b x}$ 的求值具有防溢出的稳健性。",
            "solution": "所提供的问题是计算工程领域一个有效且适定的练习，具体涉及约束优化的外罚函数法的稳健实现。该问题在微积分和数值分析方面有坚实的科学基础，没有任何不一致或模糊之处。\n\n在此，我为指定任务提供了完整的推理和推导。所有数学实体都严格遵守 LaTeX 格式。\n\n首先，我们按照要求分析罚目标函数的性质，以正式确立极小值点的存在性和唯一性。问题是最小化 $f(x) = (x - a)^2$，服从约束 $c(x) = e^{b x} - 1 \\le 0$。给定 $a  0$ 和 $b  0$，约束 $e^{bx} \\le 1$ 等价于 $x \\le 0$。显然，真正的约束极小值点是位于可行集边界的 $x^* = 0$。\n\n外罚函数法利用罚目标函数，对于罚参数 $\\mu  0$，该函数由下式给出：\n$$\nF(x) = (x - a)^2 + \\mu \\left( \\max\\{0, e^{bx} - 1\\} \\right)^2\n$$\n我们通过将其定义域划分为可行域（$x \\le 0$）和不可行域（$x  0$）来分析此函数。\n\n1.  **在可行域（$x \\le 0$）中的分析：**\n    对于 $x \\le 0$，约束得到满足，因此 $\\max\\{0, e^{bx} - 1\\} = 0$。罚目标函数简化为：\n    $$\n    F(x) = (x - a)^2\n    $$\n    一阶导数为 $F'(x) = 2(x - a)$。由于 $a  0$ 且 $x \\le 0$，因此 $(x - a)$ 严格为负，故 $F'(x)  0$。这意味着 $F(x)$ 在区间 $(-\\infty, 0]$ 上是严格递减的。该区间上的最小值在右端点 $x=0$ 处取得，此时 $F(0) = a^2$。二阶导数为 $F''(x) = 2  0$，证实了该函数在此区间上是严格凸的。\n\n2.  **在不可行域（$x  0$）中的分析：**\n    对于 $x  0$，约束被违反，惩罚项被激活：\n    $$\n    F(x) = (x - a)^2 + \\mu (e^{bx} - 1)^2\n    $$\n    取得极小值的一阶必要条件是 $F'(x) = 0$。我们定义 $g(x) = F'(x)$：\n    $$\n    g(x) = \\frac{dF}{dx} = 2(x - a) + 2\\mu(e^{bx} - 1) \\cdot \\frac{d}{dx}(e^{bx} - 1) = 2(x - a) + 2\\mu b e^{bx}(e^{bx} - 1)\n    $$\n    为保证在该区域存在唯一极小值点，我们分析 $g(x)$ 的性质。\n    -   **边界行为：**\n        当 $x \\to 0^+$ 时，$g(x) \\to 2(0 - a) + 2\\mu b e^0(e^0 - 1) = -2a$。由于 $a  0$，所以 $g(0^+)  0$。\n        当 $x \\to \\infty$ 时，$2(x - a)$ 和 $2\\mu b e^{bx}(e^{bx} - 1)$ 这两项都趋向于 $+\\infty$。因此，$\\lim_{x\\to\\infty} g(x) = +\\infty$。\n    -   **单调性：** 我们考察 $F(x)$ 的二阶导数，即 $g(x)$ 的一阶导数：\n        $$\n        F''(x) = g'(x) = \\frac{d}{dx} \\left[ 2(x - a) + 2\\mu b(e^{2bx} - e^{bx}) \\right] = 2 + 2\\mu b(2be^{2bx} - be^{bx}) = 2 + 2\\mu b^2 e^{bx}(2e^{bx} - 1)\n        $$\n        对于 $x  0$ 和 $b  0$，我们有 $e^{bx}  1$，这意味着 $(2e^{bx} - 1)  (2 \\cdot 1 - 1) = 1  0$。鉴于 $\\mu  0$ 且 $b^2  0$，整个与惩罚项相关的项都严格为正。因此，对于所有 $x  0$，$F''(x)  2$。\n\n    由于 $g(x)$ 是连续的，在 $x=0^+$ 处为负值，并趋向于无穷大，根据介值定理，保证在 $(0, \\infty)$ 中至少存在一个根 $x_k^*$。因为 $g'(x) = F''(x)  0$，所以 $g(x)$ 在 $(0, \\infty)$ 上是严格递增的，这确保了此根是唯一的。这个根对应于 $F(x)$ 在 $(0, \\infty)$ 上的唯一全局最小值。\n\n3.  **全局极小值点与收敛性：**\n    $F(x)$ 在 $\\mathbb{R}$ 上的全局极小值点要么是 $(-\\infty, 0]$ 上的最小值（位于 $x=0$），要么是 $(0, \\infty)$ 上的最小值（位于 $x_k^*$）。由于 $F(0) = a^2$ 且 $F'(0^+) = -2a  0$，当从 $x=0$ 进入正值域时，函数值是减小的。这必然要求最小值 $F(x_k^*)$ 必须严格小于 $F(0)$。因此，罚函数 $F(x)$ 的唯一全局极小值点是 $x_k^*$，它位于区间 $(0, \\infty)$ 内，从不可行域一侧逼近真实解。\n    \n    条件 $F'(x_k^*) = 0$ 意味着 $2(x_k^* - a) = -2\\mu_k b e^{bx_k^*}(e^{bx_k^*}-1)$。等式右侧为负，所以 $x_k^* - a  0$，从而得出 $0  x_k^*  a$。这为寻找根提供了一个安全的区间 $[0, a]$。\n    \n    随着罚参数 $\\mu_k \\to \\infty$，只有当非惩罚项被一个无穷大的项所平衡时，该等式才能成立，这要求因子 $e^{bx_k^*}(e^{bx_k^*}-1)$ 趋近于零。这种情况只在 $x_k^* \\to 0^+$ 时发生，从而证实了罚函数极小值点序列收敛于约束解。\n\n4.  **稳健的数值算法：**\n    为找到极小值点 $x_k^*$，我们必须求解非线性方程 $g(x) = 0$，其中 $x \\in (0, a)$。数值挑战源于指数项 $e^{bx}$，当 $b$ 或 $x$ 的值较大时，它可能导致浮点溢出。我们提出的算法采用了一种区间求根器，具体为 `scipy.optimize.brentq` 中实现的 Brent-Dekker 方法，该方法既稳健又高效。关键在于为其提供一个数值安全的函数来计算 $g(x)$。\n\n    这个防溢出函数，我们称之为 `g_safe(x, a, b, mu)`，将按如下方式设计：\n    -   它以 $x, a, b, \\mu$作为输入。\n    -   它计算指数的参数，$s = bx$。\n    -   它检查潜在的溢出。一个标准的64位浮点 `exp` 函数在其参数超过约 $709.78$ 时会溢出。$g(x)$ 中的主导项行为类似于 $e^{2s}$。因此，如果 $2s$ 接近这个阈值（例如 $s > 350$），函数值保证是一个非常大的正数。在这种情况下，`g_safe` 将返回一个预定义的大数值，如 `numpy.finfo(float).max`，以便在不执行溢出计算的情况下向求根器发出信号。\n    -   如果 $s$ 不在易溢出区域，函数将继续计算。为了在 $s$ 接近零时获得更高的精度，项 $e^s - 1$ 使用数值稳定的函数 `numpy.expm1(s)` 进行计算。完整表达式为 $g(x) = 2(x - a) + 2\\mu b \\cdot \\text{numpy.exp}(s) \\cdot \\text{numpy.expm1}(s)$。\n\n    通过调用 `scipy.optimize.brentq(g_safe, 0, a, args=(a, b, mu))`，我们利用了一个强大的求解器。在我们的安全求值函数的引导下，它将高效、可靠地找到唯一的根 $x_k^*$，即使在参数 $b$ 和 $\\mu$ 非常大时也不会遇到数值溢出。搜索算法会自然地避开那些 $x$ 大到足以导致溢出的区域，因为那里的函数值是巨大的正数，会将搜索区间推向零。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves for the minimizer of a penalized objective function for a suite of test cases.\n\n    The problem is to minimize f(x) = (x - a)^2 subject to c(x) = exp(b*x) - 1 = 0.\n    The penalized objective is F(x) = (x - a)^2 + mu * (max{0, c(x)})^2.\n    The minimizer x* is found by solving the first-order optimality condition F'(x) = 0\n    for x  0 using a robust, overflow-safe root-finding method.\n    \"\"\"\n\n    # Practical overflow threshold for np.exp can be determined from np.finfo.\n    # np.log(np.finfo(np.float64).max) is approximately 709.78.\n    # The penalty term behaves like exp(2*b*x), so we check if b*x  709.78 / 2.\n    # We use a safe margin for the check.\n    EXP_ARG_SAFE_LIMIT = 350.0\n\n    def g_safe(x, a, b, mu):\n        \"\"\"\n        Calculates the derivative of the penalized objective function F(x) in a numerically\n        robust and overflow-safe manner.\n\n        g(x) = F'(x) = 2(x - a) + 2*mu*b*exp(b*x)*(exp(b*x) - 1) for x  0.\n        For x = 0, g(x) = F'(x) = 2(x - a).\n\n        Args:\n            x (float): The point at which to evaluate the derivative.\n            a (float): The parameter 'a' from the objective function.\n            b (float): The parameter 'b' from the constraint function.\n            mu (float): The penalty parameter.\n\n        Returns:\n            float: The value of F'(x), or a large positive number if overflow is anticipated.\n        \"\"\"\n        # For the region x = 0, the penalty is 0, so F'(x) = 2(x-a).\n        # This branch correctly handles the lower bound of the root-finding interval.\n        if x = 0:\n            return 2.0 * (x - a)\n\n        s = b * x\n\n        # Pre-emptive overflow check. If s is large, the exponential term dominates\n        # and will be a large positive number. Returning np.finfo(float).max ensures\n        # the root-finder correctly interprets the sign of g(x) without overflow.\n        if s > EXP_ARG_SAFE_LIMIT:\n            return np.finfo(np.float64).max\n\n        # Use np.expm1(s) for numerically stable computation of exp(s) - 1,\n        # especially important when s is close to 0.\n        penalty_derivative = 2.0 * mu * b * np.exp(s) * np.expm1(s)\n        \n        return 2.0 * (x - a) + penalty_derivative\n\n    # Hard-coded test suite as specified in the problem statement.\n    test_cases = [\n        # (a, b, mu)\n        (1.0, 50.0, 1e-6),\n        (1.0, 200.0, 1e3),\n        (1.0, 500.0, 1e12),\n        (1.0, 1000.0, 1e20),\n    ]\n\n    results = []\n    for a, b, mu in test_cases:\n        # The minimizer x_k^* is guaranteed to be in the interval (0, a).\n        # g_safe(0)  0 and g_safe(a) > 0 (or inf, which is handled correctly).\n        # Therefore, we can use a bracketing root-finder.\n        # brentq is a robust and efficient choice for this task.\n        try:\n            minimizer_x = brentq(g_safe, 0.0, a, args=(a, b, mu))\n            results.append(round(minimizer_x, 10))\n        except (ValueError, RuntimeError) as e:\n            # This block should not be reached if the analysis is correct.\n            # It is included as a safeguard.\n            results.append(f\"Error: {e}\")\n\n    # Format the final output as specified.\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "除了直接求解问题，罚函数方法在更复杂的优化框架中也是一个强大的工具。最后一个练习  将你置于一个实际场景中：当某算法的一步迭代意外落入可行域之外时，需要恢复其可行性。你将把这个“可行性恢复”任务构建为一个罚函数优化问题并加以解决，从而展示该方法的多功能性。",
            "id": "2423404",
            "problem": "给定实向量空间中的一个约束二次规划族，该规划族为一个试探步产生不可行点后的可行性恢复目标建模。对于每个实例，考虑一个点 $x_0 \\in \\mathbb{R}^n$、线性不等式约束 $g_i(x) \\le 0$（其中 $g_i(x) = a_i^\\top x - b_i$）以及线性等式约束 $h_j(x) = 0$（其中 $h_j(x) = c_j^\\top x - d_j$）。对于给定的参数 $\\alpha  0$，定义惩罚恢复目标\n$$\nJ_\\alpha(x) \\;=\\; \\alpha \\,\\lVert x - x_0 \\rVert_2^2 \\;+\\; \\sum_{i=1}^{m} \\max\\!\\big(0,\\, g_i(x)\\big)^2 \\;+\\; \\sum_{j=1}^{p} h_j(x)^2,\n$$\n该目标函数通过平方铰链（squared hinge）度量与参考点 $x_0$ 的接近程度、总的不等式约束违反量，并通过平方残差度量总的等式约束违反量。对于下文指定的每个测试用例，确定最小值\n$$\nJ_\\alpha(x^\\star) \\;=\\; \\min_{x \\in \\mathbb{R}^n} J_\\alpha(x)\n$$\n至数值精度。\n\n所有计算都纯粹是在 $\\mathbb{R}^n$ 上的数学计算，不涉及物理单位。角度不会出现。对于每个测试用例，答案是一个实数。您的程序应为每个测试用例评估最小值 $J_\\alpha(x^\\star)$，并将结果四舍五入到九位小数后报告。\n\n测试套件。对于每个测试用例 $k$，数据由 $n$、$\\alpha$、$x_0 \\in \\mathbb{R}^n$、不等式矩阵 $A_{\\mathrm{in}} \\in \\mathbb{R}^{m \\times n}$ 和向量 $b_{\\mathrm{in}} \\in \\mathbb{R}^m$（其第 $i$ 行和条目定义了 $g_i(x) = a_i^\\top x - b_i$），以及等式矩阵 $A_{\\mathrm{eq}} \\in \\mathbb{R}^{p \\times n}$ 和向量 $b_{\\mathrm{eq}} \\in \\mathbb{R}^p$（其第 $j$ 行和条目定义了 $h_j(x) = c_j^\\top x - d_j$）给出。允许不等式或等式集为空，此时分别取 $m=0$ 或 $p=0$。\n\n- 测试用例 1：\n  - $n = 2$，$\\alpha = 0.2$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 1.5 \\end{bmatrix}$。\n  - $A_{\\mathrm{in}} = \\begin{bmatrix} 1  1 \\end{bmatrix}$，$b_{\\mathrm{in}} = \\begin{bmatrix} 1.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{eq}}$ 为空，$b_{\\mathrm{eq}}$ 为空。\n\n- 测试用例 2：\n  - $n = 2$，$\\alpha = 0.5$，$x_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{in}} = \\begin{bmatrix} -1  -1 \\end{bmatrix}$，$b_{\\mathrm{in}} = \\begin{bmatrix} -2.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{eq}} = \\begin{bmatrix} 1  -1 \\end{bmatrix}$，$b_{\\mathrm{eq}} = \\begin{bmatrix} 0.0 \\end{bmatrix}$。\n\n- 测试用例 3：\n  - $n = 3$，$\\alpha = 0.1$，$x_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{in}} = \\begin{bmatrix} 1  1  1 \\\\ -1  0  0 \\\\ 0  -1  0 \\\\ 0  0  -1 \\end{bmatrix}$，$b_{\\mathrm{in}} = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{eq}} = \\begin{bmatrix} 1  -2  1 \\end{bmatrix}$，$b_{\\mathrm{eq}} = \\begin{bmatrix} 0.0 \\end{bmatrix}$。\n\n- 测试用例 4：\n  - $n = 2$，$\\alpha = 0.4$，$x_0 = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{in}} = \\begin{bmatrix} 1  1 \\end{bmatrix}$，$b_{\\mathrm{in}} = \\begin{bmatrix} 3.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{eq}} = \\begin{bmatrix} 1  -1 \\end{bmatrix}$，$b_{\\mathrm{eq}} = \\begin{bmatrix} 1.0 \\end{bmatrix}$。\n\n- 测试用例 5：\n  - $n = 2$，$\\alpha = 0.3$，$x_0 = \\begin{bmatrix} 0.2 \\\\ -1.0 \\end{bmatrix}$。\n  - $A_{\\mathrm{in}}$ 为空，$b_{\\mathrm{in}}$ 为空。\n  - $A_{\\mathrm{eq}} = \\begin{bmatrix} 1  0 \\\\ 1  0 \\end{bmatrix}$，$b_{\\mathrm{eq}} = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$。\n\n最终输出格式。您的程序应生成单行输出，其中包含测试用例 1 到 5 的最小值 $J_\\alpha(x^\\star)$，按此顺序排列，四舍五入到九位小数，以逗号分隔的列表形式包含在方括号内，例如 $[v_1,v_2,v_3,v_4,v_5]$，其中每个 $v_k$ 是小数点后有九位数字的十进制数。",
            "solution": "所呈现的问题是一系列无约束优化任务。对于每个测试用例，我们需要找到定义在 $\\mathbb{R}^n$ 上的基于罚函数的目标函数 $J_\\alpha(x)$ 的最小值。该问题是适定的且有科学依据，因此是有效的并存在解。\n\n目标函数由下式给出\n$$ J_\\alpha(x) \\;=\\; \\alpha \\,\\lVert x - x_0 \\rVert_2^2 \\;+\\; \\sum_{i=1}^{m} \\max\\!\\big(0,\\, g_i(x)\\big)^2 \\;+\\; \\sum_{j=1}^{p} h_j(x)^2 $$\n其中 $g_i(x) = a_i^\\top x - b_i$ 和 $h_j(x) = c_j^\\top x - d_j$ 是 $x \\in \\mathbb{R}^n$ 的仿射函数。\n\n首先，我们分析 $J_\\alpha(x)$ 的数学性质。该函数包含三个项：\n1. 正则化项 $\\alpha \\lVert x - x_0 \\rVert_2^2$，由于参数 $\\alpha$ 被给定为严格正数（$\\alpha  0$），因此它是 $x$ 的一个严格凸二次函数。\n2. 不等式惩罚项 $\\sum_{i=1}^{m} \\max(0, g_i(x))^2$。函数 $y \\mapsto \\max(0, y)^2$ 是一个凸的、连续可微的函数。由于 $g_i(x)$ 是一个仿射映射，对于每个 $i \\in \\{1, \\dots, m\\}$，复合函数 $\\max(0, g_i(x))^2$ 也是凸的。凸函数的和仍然是凸函数。\n3. 等式惩罚项 $\\sum_{j=1}^{p} h_j(x)^2$。每一项 $(c_j^\\top x - d_j)^2$ 都是一个凸二次函数。它们的和也是凸的。\n\n目标函数 $J_\\alpha(x)$ 是多个凸函数之和，其中一个是严格凸的。因此，$J_\\alpha(x)$ 是 $\\mathbb{R}^n$ 上的一个严格凸函数。一个严格凸函数至多有一个全局最小值。此外，由于项 $\\alpha \\lVert x - x_0 \\rVert_2^2$ 的存在，函数 $J_\\alpha(x)$ 是强制的 (coercive)，即当 $\\lVert x \\rVert \\to \\infty$ 时，$J_\\alpha(x) \\to \\infty$。这保证了唯一全局最小值 $x^\\star$ 的存在。\n\n最小值 $x^\\star$ 是目标函数梯度为零的唯一点：$\\nabla J_\\alpha(x^\\star) = 0$。该函数是连续可微的 ($C^1$)，这允许使用基于梯度的优化方法。下面我们来推导梯度。\n第一项的梯度是 $\\nabla (\\alpha (x - x_0)^\\top(x-x_0)) = 2\\alpha(x-x_0)$。\n第二项的梯度可以通过链式法则求得。函数 $f(y) = \\max(0, y)^2$ 的导数是 $f'(y) = 2\\max(0, y)$。因此，\n$$ \\nabla \\left( \\sum_{i=1}^{m} \\max(0, a_i^\\top x - b_i)^2 \\right) = \\sum_{i=1}^{m} 2\\max(0, a_i^\\top x - b_i) \\nabla(a_i^\\top x - b_i) = \\sum_{i=1}^{m} 2\\max(0, a_i^\\top x - b_i) a_i $$\n第三项的梯度是\n$$ \\nabla \\left( \\sum_{j=1}^{p} (c_j^\\top x - d_j)^2 \\right) = \\sum_{j=1}^{p} 2(c_j^\\top x - d_j) \\nabla(c_j^\\top x - d_j) = \\sum_{j=1}^{p} 2(c_j^\\top x - d_j) c_j $$\n将这些结合起来，完整的梯度向量是：\n$$ \\nabla J_\\alpha(x) = 2\\alpha(x - x_0) + 2\\sum_{i=1}^{m} \\max(0, a_i^\\top x - b_i) a_i + 2\\sum_{j=1}^{p} (c_j^\\top x - d_j) c_j $$\n在矩阵表示法中，使用问题中给出的定义 $g(x) = A_{\\mathrm{in}}x - b_{\\mathrm{in}}$ 和 $h(x) = A_{\\mathrm{eq}}x - b_{\\mathrm{eq}}$，梯度为：\n$$ \\nabla J_\\alpha(x) = 2\\alpha(x - x_0) + 2 A_{\\mathrm{in}}^\\top \\max(g(x), 0) + 2 A_{\\mathrm{eq}}^\\top h(x) $$\n其中 $\\max(\\cdot, 0)$ 算子逐元素地应用于向量 $g(x)$。\n\n由于梯度具有分段性质（源于 `max` 函数），要找到 $\\nabla J_\\alpha(x)=0$ 的解析解需要对 $g_i(x)$ 的符号进行组合情况分析，这对于除了最简单的情况之外的所有情况都是不切实际的。数值方法更为优越。我们将采用一种拟牛顿法，特别是 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 算法，该算法非常适合用于连续可微函数的无约束最小化问题。此算法需要目标函数 $J_\\alpha(x)$ 及其梯度 $\\nabla J_\\alpha(x)$，我们已经推导出了这两者。\n\n计算过程如下：\n1. 对于每个测试用例，定义参数 $n$、$\\alpha$、$x_0$、$A_{\\mathrm{in}}$、$b_{\\mathrm{in}}$、$A_{\\mathrm{eq}}$ 和 $b_{\\mathrm{eq}}$。\n2. 我们实现函数来计算任意输入向量 $x$ 的 $J_\\alpha(x)$ 和 $\\nabla J_\\alpha(x)$。需要特别注意不等式或等式约束不存在的情况（$m=0$ 或 $p=0$）。\n3. 调用 `scipy.optimize.minimize` 函数并使用 BFGS 方法，向其提供目标函数、其梯度以及最小化点的初始猜测值。一个自然的选择是使用参考点 $x_0$ 作为初始猜测值。\n4. 优化程序返回最优点 $x^\\star$ 和相应的最小函数值 $J_\\alpha(x^\\star)$。\n5. 然后将此值四舍五入到指定的九位小数精度。对所有测试用例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves a series of penalized quadratic restoration problems.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"n\": 2, \"alpha\": 0.2, \"x0\": np.array([1.0, 1.5]),\n            \"A_in\": np.array([[1.0, 1.0]]), \"b_in\": np.array([1.0]),\n            \"A_eq\": np.empty((0, 2)), \"b_eq\": np.empty(0)\n        },\n        # Test case 2\n        {\n            \"n\": 2, \"alpha\": 0.5, \"x0\": np.array([0.0, 0.0]),\n            \"A_in\": np.array([[-1.0, -1.0]]), \"b_in\": np.array([-2.0]),\n            \"A_eq\": np.array([[1.0, -1.0]]), \"b_eq\": np.array([0.0])\n        },\n        # Test case 3\n        {\n            \"n\": 3, \"alpha\": 0.1, \"x0\": np.array([1.0, 1.0, 1.0]),\n            \"A_in\": np.array([\n                [1.0, 1.0, 1.0],\n                [-1.0, 0.0, 0.0],\n                [0.0, -1.0, 0.0],\n                [0.0, 0.0, -1.0]\n            ]),\n            \"b_in\": np.array([1.0, 0.0, 0.0, 0.0]),\n            \"A_eq\": np.array([[1.0, -2.0, 1.0]]), \"b_eq\": np.array([0.0])\n        },\n        # Test case 4\n        {\n            \"n\": 2, \"alpha\": 0.4, \"x0\": np.array([2.0, 1.0]),\n            \"A_in\": np.array([[1.0, 1.0]]), \"b_in\": np.array([3.0]),\n            \"A_eq\": np.array([[1.0, -1.0]]), \"b_eq\": np.array([1.0])\n        },\n        # Test case 5\n        {\n            \"n\": 2, \"alpha\": 0.3, \"x0\": np.array([0.2, -1.0]),\n            \"A_in\": np.empty((0, 2)), \"b_in\": np.empty(0),\n            \"A_eq\": np.array([[1.0, 0.0], [1.0, 0.0]]),\n            \"b_eq\": np.array([0.0, 1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        alpha = case[\"alpha\"]\n        x0 = case[\"x0\"]\n        A_in = case[\"A_in\"]\n        b_in = case[\"b_in\"]\n        A_eq = case[\"A_eq\"]\n        b_eq = case[\"b_eq\"]\n        \n        m_in, _ = A_in.shape\n        m_eq, _ = A_eq.shape\n\n        def objective(x):\n            term1 = alpha * np.sum((x - x0)**2)\n            \n            term2 = 0.0\n            if m_in > 0:\n                g = A_in @ x - b_in\n                term2 = np.sum(np.maximum(0, g)**2)\n            \n            term3 = 0.0\n            if m_eq > 0:\n                h = A_eq @ x - b_eq\n                term3 = np.sum(h**2)\n                \n            return term1 + term2 + term3\n\n        def gradient(x):\n            grad1 = 2 * alpha * (x - x0)\n            \n            grad2 = np.zeros(n)\n            if m_in > 0:\n                g = A_in @ x - b_in\n                active_g = np.maximum(0, g)\n                grad2 = 2 * A_in.T @ active_g\n            \n            grad3 = np.zeros(n)\n            if m_eq > 0:\n                h = A_eq @ x - b_eq\n                grad3 = 2 * A_eq.T @ h\n                \n            return grad1 + grad2 + grad3\n\n        initial_guess = x0\n        \n        # Use BFGS, a quasi-Newton method ideal for unconstrained optimization of C1 functions.\n        # The analytical gradient is provided for efficiency and accuracy.\n        res = minimize(\n            objective,\n            initial_guess,\n            method='BFGS',\n            jac=gradient\n        )\n        \n        # Format the result to the required precision.\n        results.append(f\"{res.fun:.9f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}