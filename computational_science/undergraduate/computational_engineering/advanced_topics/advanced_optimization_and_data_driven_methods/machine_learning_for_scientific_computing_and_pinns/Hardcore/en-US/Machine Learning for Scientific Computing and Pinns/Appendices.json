{
    "hands_on_practices": [
        {
            "introduction": "The heart of a Physics-Informed Neural Network (PINN) is its composite loss function, which guides the model to respect both observation data and governing physical laws. In this first exercise, you will deconstruct this core component by calculating a total loss for the one-dimensional heat equation. This involves combining separate terms for the PDE residual, the initial condition data, and a physical conservation law, giving you a foundational understanding of how different constraints are translated into a single, trainable objective .",
            "id": "2411025",
            "problem": "Consider the one-dimensional heat equation on a periodic domain. Let $x \\in [0,1]$ and $t \\in [0,0.1]$. The field $u(x,t)$ is modeled by a surrogate used in a Physics-Informed Neural Network (PINN), with the following parametric form:\n$$\nu(x,t;\\theta) \\;=\\; \\theta_1 \\cdot 1 \\;+\\; \\theta_2 \\sin(2\\pi x) \\;+\\; \\theta_3 \\cos(2\\pi x) \\;+\\; \\theta_4\\, t\\, \\sin(2\\pi x) \\;+\\; \\theta_5\\, t\\, \\cos(2\\pi x),\n$$\nwhere $\\theta = (\\theta_1,\\theta_2,\\theta_3,\\theta_4,\\theta_5) \\in \\mathbb{R}^5$ are trainable parameters. The governing partial differential equation (PDE) is the heat equation:\n$$\nu_t \\;-\\; \\nu\\, u_{xx} \\;=\\; 0,\n$$\nwith viscosity $\\nu \\ge 0$ and periodic boundary conditions in $x$. The initial condition is specified as\n$$\nu(x,0) \\;=\\; u_0(x) \\;=\\; 0.5 \\;+\\; \\sin(2\\pi x),\n$$\nso the conserved spatial integral (total mass) implied by periodicity is\n$$\nM_0 \\;=\\; \\int_0^1 u(x,0)\\,dx \\;=\\; 0.5.\n$$\n\nDefine the composite PINN loss that enforces the PDE, the initial condition data, and the conservation law as a soft constraint:\n$$\n\\mathcal{L}_{\\text{total}}(\\theta;\\nu,w_{\\text{pde}},w_{\\text{data}},w_{\\text{cons}}) \\;=\\; w_{\\text{pde}}\\,\\mathcal{L}_{\\text{pde}}(\\theta;\\nu) \\;+\\; w_{\\text{data}}\\,\\mathcal{L}_{\\text{data}}(\\theta) \\;+\\; w_{\\text{cons}}\\,\\mathcal{L}_{\\text{cons}}(\\theta),\n$$\nwhere\n- The PDE residual loss $\\mathcal{L}_{\\text{pde}}$ is the mean of squared residuals of $r(x,t;\\theta,\\nu) = u_t(x,t;\\theta) - \\nu\\,u_{xx}(x,t;\\theta)$ evaluated on a fixed collocation grid:\n$$\n\\mathcal{L}_{\\text{pde}}(\\theta;\\nu) \\;=\\; \\frac{1}{|\\mathcal{G}|} \\sum_{(x,t)\\in \\mathcal{G}} \\left[ u_t(x,t;\\theta) \\;-\\; \\nu\\,u_{xx}(x,t;\\theta) \\right]^2,\n$$\nwith collocation grid $\\mathcal{G} = \\{0,\\, 0.25,\\, 0.5,\\, 0.75,\\, 1.0\\} \\times \\{0,\\, 0.05,\\, 0.1\\}$.\n- The data loss $\\mathcal{L}_{\\text{data}}$ is the mean squared mismatch to the initial condition at the specified data points:\n$$\n\\mathcal{L}_{\\text{data}}(\\theta) \\;=\\; \\frac{1}{|\\mathcal{D}|} \\sum_{x\\in \\mathcal{D}} \\left[ u(x,0;\\theta) \\;-\\; u_0(x) \\right]^2,\n$$\nwith initial data set $\\mathcal{D} = \\{0,\\, 0.25,\\, 0.5,\\, 0.75,\\, 1.0\\}$.\n- The conservation soft-constraint loss $\\mathcal{L}_{\\text{cons}}$ enforces the integral of $u$ over $x$ to remain equal to $M_0$ at selected times:\n$$\n\\mathcal{L}_{\\text{cons}}(\\theta) \\;=\\; \\frac{1}{|\\mathcal{T}_{\\text{mass}}|} \\sum_{t\\in \\mathcal{T}_{\\text{mass}}} \\left[ \\left( \\int_0^1 u(x,t;\\theta)\\,dx \\right) - M_0 \\right]^2,\n$$\nwith $\\mathcal{T}_{\\text{mass}} = \\{0,\\, 0.05,\\, 0.1\\}$. The integral $\\int_0^1 u(x,t;\\theta)\\,dx$ must be computed exactly for the given surrogate $u$.\n\nThe required derivatives are defined from the surrogate by the usual calculus rules:\n$$\nu_t(x,t;\\theta) \\;=\\; \\theta_4\\,\\sin(2\\pi x) \\;+\\; \\theta_5\\,\\cos(2\\pi x),\n$$\n$$\nu_{xx}(x,t;\\theta) \\;=\\; - (2\\pi)^2 \\left[ \\theta_2 \\sin(2\\pi x) \\;+\\; \\theta_3 \\cos(2\\pi x) \\;+\\; \\theta_4\\,t\\,\\sin(2\\pi x) \\;+\\; \\theta_5\\,t\\,\\cos(2\\pi x) \\right].\n$$\nThe exact integral over one spatial period for the basis used satisfies\n$$\n\\int_0^1 \\sin(2\\pi x)\\,dx \\;=\\; 0,\\quad \\int_0^1 \\cos(2\\pi x)\\,dx \\;=\\; 0,\\quad \\int_0^1 1\\,dx \\;=\\; 1,\n$$\nso for the surrogate specified,\n$$\n\\int_0^1 u(x,t;\\theta)\\,dx \\;=\\; \\theta_1.\n$$\n\nYour program must compute $\\mathcal{L}_{\\text{total}}$ for each of the following test cases, each given by a tuple $(\\theta,\\nu,w_{\\text{pde}},w_{\\text{data}},w_{\\text{cons}})$:\n- Test case $1$: $\\theta = (0.3,\\, 0.8,\\, -0.2,\\, 0.1,\\, 0.05)$, $\\nu = 0.1$, $w_{\\text{pde}} = 1.0$, $w_{\\text{data}} = 1.0$, $w_{\\text{cons}} = 0.0$.\n- Test case $2$: $\\theta = (0.3,\\, 0.8,\\, -0.2,\\, 0.1,\\, 0.05)$, $\\nu = 0.1$, $w_{\\text{pde}} = 1.0$, $w_{\\text{data}} = 1.0$, $w_{\\text{cons}} = 10.0$.\n- Test case $3$: $\\theta = (0.7,\\, -0.5,\\, 0.25,\\, 0.0,\\, 0.0)$, $\\nu = 0.0$, $w_{\\text{pde}} = 1.0$, $w_{\\text{data}} = 1.0$, $w_{\\text{cons}} = 5.0$.\n- Test case $4$: $\\theta = (0.5,\\, 0.0,\\, 1.0,\\, 0.0,\\, 0.0)$, $\\nu = 0.05$, $w_{\\text{pde}} = 0.5$, $w_{\\text{data}} = 2.0$, $w_{\\text{cons}} = 2.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_i$ is the computed value of $\\mathcal{L}_{\\text{total}}$ for test case $i$. No additional text must be printed.",
            "solution": "The problem statement is parsed and determined to be valid. It is scientifically grounded, well-posed, objective, and self-contained, representing a standard task in the field of computational science and engineering, specifically concerning Physics-Informed Neural Networks (PINNs). We may proceed with the solution.\n\nThe objective is to compute the total loss function $\\mathcal{L}_{\\text{total}}$ for four distinct test cases. The total loss is a weighted sum of three components: a PDE residual loss $\\mathcal{L}_{\\text{pde}}$, a data mismatch loss $\\mathcal{L}_{\\text{data}}$, and a conservation law loss $\\mathcal{L}_{\\text{cons}}$.\n$$\n\\mathcal{L}_{\\text{total}}(\\theta;\\nu,w_{\\text{pde}},w_{\\text{data}},w_{\\text{cons}}) = w_{\\text{pde}}\\,\\mathcal{L}_{\\text{pde}}(\\theta;\\nu) + w_{\\text{data}}\\,\\mathcal{L}_{\\text{data}}(\\theta) + w_{\\text{cons}}\\,\\mathcal{L}_{\\text{cons}}(\\theta)\n$$\nWe will analyze and derive explicit computational formulas for each component loss.\n\n**1. Data Loss $\\mathcal{L}_{\\text{data}}(\\theta)$**\n\nThe data loss measures the mean squared error between the surrogate model's prediction at time $t=0$ and the given initial condition $u_0(x)$. The surrogate at $t=0$ is\n$$\nu(x,0;\\theta) = \\theta_1 + \\theta_2 \\sin(2\\pi x) + \\theta_3 \\cos(2\\pi x).\n$$\nThe initial condition is $u_0(x) = 0.5 + \\sin(2\\pi x)$. The error at a point $x$ is\n$$\ne_d(x;\\theta) = u(x,0;\\theta) - u_0(x) = (\\theta_1 - 0.5) + (\\theta_2 - 1)\\sin(2\\pi x) + \\theta_3 \\cos(2\\pi x).\n$$\nThe loss is the average of $e_d(x;\\theta)^2$ over the discrete set of points $\\mathcal{D} = \\{0, 0.25, 0.5, 0.75, 1.0\\}$. The size of this set is $|\\mathcal{D}| = 5$.\n$$\n\\mathcal{L}_{\\text{data}}(\\theta) = \\frac{1}{5} \\sum_{x \\in \\mathcal{D}} \\left[ e_d(x;\\theta) \\right]^2.\n$$\nWe evaluate the trigonometric functions at the points in $\\mathcal{D}$:\n- For $x \\in \\{0, 1.0\\}$, $\\sin(2\\pi x)=0$, $\\cos(2\\pi x)=1$.\n- For $x = 0.25$, $\\sin(2\\pi x)=1$, $\\cos(2\\pi x)=0$.\n- For $x = 0.5$, $\\sin(2\\pi x)=0$, $\\cos(2\\pi x)=-1$.\n- For $x = 0.75$, $\\sin(2\\pi x)=-1$, $\\cos(2\\pi x)=0$.\nThe sum of squared errors is:\n$$\n\\sum_{x \\in \\mathcal{D}} [e_d(x;\\theta)]^2 = [(\\theta_1-0.5)+\\theta_3]^2_{x=0} + [(\\theta_1-0.5)+(\\theta_2-1)]^2_{x=0.25} + [(\\theta_1-0.5)-\\theta_3]^2_{x=0.5} + [(\\theta_1-0.5)-(\\theta_2-1)]^2_{x=0.75} + [(\\theta_1-0.5)+\\theta_3]^2_{x=1.0}.\n$$\nThis expression can be implemented directly.\n\n**2. Conservation Loss $\\mathcal{L}_{\\text{cons}}(\\theta)$**\n\nThe conservation loss penalizes deviation from the conserved quantity $M_0 = 0.5$. The integral of the surrogate model over the spatial domain $[0,1]$ is given as:\n$$\n\\int_0^1 u(x,t;\\theta)\\,dx = \\int_0^1 \\left( \\theta_1 + \\theta_2 \\sin(2\\pi x) + \\theta_3 \\cos(2\\pi x) + \\theta_4 t \\sin(2\\pi x) + \\theta_5 t \\cos(2\\pi x) \\right) dx.\n$$\nGiven the orthogonality relations $\\int_0^1 \\sin(2\\pi x)dx = 0$ and $\\int_0^1 \\cos(2\\pi x)dx = 0$, the integral simplifies to:\n$$\n\\int_0^1 u(x,t;\\theta)\\,dx = \\theta_1 \\int_0^1 1\\,dx = \\theta_1.\n$$\nThis result is independent of time $t$. The loss is defined as:\n$$\n\\mathcal{L}_{\\text{cons}}(\\theta) = \\frac{1}{|\\mathcal{T}_{\\text{mass}}|} \\sum_{t \\in \\mathcal{T}_{\\text{mass}}} \\left[ \\left( \\int_0^1 u(x,t;\\theta)\\,dx \\right) - M_0 \\right]^2 = \\frac{1}{3} \\sum_{t \\in \\mathcal{T}_{\\text{mass}}} [\\theta_1 - 0.5]^2.\n$$\nSince the term to be summed does not depend on $t$, this simplifies to:\n$$\n\\mathcal{L}_{\\text{cons}}(\\theta) = \\frac{1}{3} \\cdot 3 \\cdot (\\theta_1 - 0.5)^2 = (\\theta_1 - 0.5)^2.\n$$\n\n**3. PDE Residual Loss $\\mathcal{L}_{\\text{pde}}(\\theta; \\nu)$**\n\nThe PDE residual is $r(x,t;\\theta,\\nu) = u_t - \\nu u_{xx}$. Using the provided derivatives:\n$$\nu_t(x,t;\\theta) = \\theta_4 \\sin(2\\pi x) + \\theta_5 \\cos(2\\pi x)\n$$\n$$\nu_{xx}(x,t;\\theta) = -(2\\pi)^2 \\left( (\\theta_2 + \\theta_4 t) \\sin(2\\pi x) + (\\theta_3 + \\theta_5 t) \\cos(2\\pi x) \\right)\n$$\nThe residual is:\n$$\nr(x,t) = [\\theta_4 + \\nu(2\\pi)^2(\\theta_2 + \\theta_4 t)]\\sin(2\\pi x) + [\\theta_5 + \\nu(2\\pi)^2(\\theta_3 + \\theta_5 t)]\\cos(2\\pi x).\n$$\nLet us define time-dependent coefficients for brevity:\n$$\nC_s(t) = \\theta_4 + \\nu(2\\pi)^2(\\theta_2 + \\theta_4 t)\n$$\n$$\nC_c(t) = \\theta_5 + \\nu(2\\pi)^2(\\theta_3 + \\theta_5 t)\n$$\nSo, $r(x,t) = C_s(t)\\sin(2\\pi x) + C_c(t)\\cos(2\\pi x)$. The loss is the mean of $r(x,t)^2$ over the collocation grid $\\mathcal{G} = \\{0, 0.25, 0.5, 0.75, 1.0\\} \\times \\{0, 0.05, 0.1\\}$. The grid size is $|\\mathcal{G}| = 5 \\times 3 = 15$.\n$$\n\\mathcal{L}_{\\text{pde}}(\\theta; \\nu) = \\frac{1}{15} \\sum_{t \\in \\mathcal{T}_{\\text{pde}}} \\sum_{x \\in \\mathcal{D}} [r(x,t)]^2,\n$$\nwhere $\\mathcal{T}_{\\text{pde}} = \\{0, 0.05, 0.1\\}$. For a fixed time $t$, the sum over $x \\in \\mathcal{D}$ can be simplified by evaluating the trigonometric terms:\n$$\n\\sum_{x \\in \\mathcal{D}} [r(x,t)]^2 = [C_c(t)]^2_{x=0} + [C_s(t)]^2_{x=0.25} + [-C_c(t)]^2_{x=0.5} + [-C_s(t)]^2_{x=0.75} + [C_c(t)]^2_{x=1.0}\n= 3[C_c(t)]^2 + 2[C_s(t)]^2.\n$$\nThe total sum of squared residuals is therefore:\n$$\n\\sum_{(x,t) \\in \\mathcal{G}} [r(x,t)]^2 = \\sum_{t \\in \\mathcal{T}_{\\text{pde}}} (3[C_c(t)]^2 + 2[C_s(t)]^2).\n$$\nThis provides a direct method for computing $\\mathcal{L}_{\\text{pde}}$.\n\nWith these explicit formulas, we can compute $\\mathcal{L}_{\\text{total}}$ for each test case by substituting the given parameter values. The following program implements this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total PINN loss for several test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: (theta, nu, w_pde, w_data, w_cons)\n        ((0.3, 0.8, -0.2, 0.1, 0.05), 0.1, 1.0, 1.0, 0.0),\n        # Case 2\n        ((0.3, 0.8, -0.2, 0.1, 0.05), 0.1, 1.0, 1.0, 10.0),\n        # Case 3\n        ((0.7, -0.5, 0.25, 0.0, 0.0), 0.0, 1.0, 1.0, 5.0),\n        # Case 4\n        ((0.5, 0.0, 1.0, 0.0, 0.0), 0.05, 0.5, 2.0, 2.0),\n    ]\n\n    # Define grids and constants\n    X_grid = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n    T_grid = np.array([0.0, 0.05, 0.1])\n    M0 = 0.5\n    \n    results = []\n\n    for case in test_cases:\n        theta_vec, nu, w_pde, w_data, w_cons = case\n        th1, th2, th3, th4, th5 = theta_vec\n\n        # -- 1. Compute Data Loss (L_data) --\n        # u(x,0) = th1 + th2*sin(2*pi*x) + th3*cos(2*pi*x)\n        # u0(x)  = 0.5 + sin(2*pi*x)\n        # error(x) = (th1-0.5) + (th2-1)*sin(2*pi*x) + th3*cos(2*pi*x)\n        \n        sin_vals = np.sin(2 * np.pi * X_grid)\n        cos_vals = np.cos(2 * np.pi * X_grid)\n        \n        error_data = (th1 - M0) + (th2 - 1.0) * sin_vals + th3 * cos_vals\n        L_data = np.mean(error_data**2)\n        \n        # -- 2. Compute Conservation Loss (L_cons) --\n        # integral u(x,t) dx = th1\n        # L_cons = (th1 - M0)^2\n        L_cons = (th1 - M0)**2\n        \n        # -- 3. Compute PDE Residual Loss (L_pde) --\n        # r(x,t) = u_t - nu * u_xx\n        # u_t = th4*sin(2*pi*x) + th5*cos(2*pi*x)\n        # u_xx = -(2*pi)^2 * [ (th2 + th4*t)*sin(2*pi*x) + (th3 + th5*t)*cos(2*pi*x) ]\n        \n        k = (2 * np.pi)**2\n        \n        sum_sq_res = 0.0\n        for t in T_grid:\n            # Coefficients of sin(2*pi*x) and cos(2*pi*x) in the residual\n            C_s_t = th4 + nu * k * (th2 + th4 * t)\n            C_c_t = th5 + nu * k * (th3 + th5 * t)\n            \n            # r(x,t) = C_s_t * sin(2*pi*x) + C_c_t * cos(2*pi*x)\n            # We calculated the sum over x analytically to simplify:\n            # sum_{x in D} r(x,t)^2 = 2 * C_s_t^2 + 3 * C_c_t^2\n            sum_sq_res_t = 2 * C_s_t**2 + 3 * C_c_t**2\n            sum_sq_res += sum_sq_res_t\n            \n        L_pde = sum_sq_res / (len(X_grid) * len(T_grid))\n\n        # -- 4. Compute Total Loss (L_total) --\n        L_total = w_pde * L_pde + w_data * L_data + w_cons * L_cons\n        results.append(L_total)\n\n    # Format the final output string\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful PINN technique involves designing model architectures that inherently satisfy physical laws, a method known as enforcing constraints \"by construction.\" This practice moves beyond simple loss-based penalties to embed physical knowledge directly into the model itself . You will design a model for laminar pipe flow that automatically satisfies axisymmetry and no-slip boundary conditions, thereby reducing the problem's complexity and creating a more efficient and robust network.",
            "id": "2411045",
            "problem": "You are tasked with designing and implementing a Physics-Informed Neural Network (PINN) that incorporates known axisymmetry to reduce the dimensionality of the input space for steady, fully developed, incompressible, laminar flow in a straight circular pipe. Treat all quantities as nondimensional unless explicitly defined otherwise. The goal is to exploit geometric symmetry and boundary conditions at the architectural level and then identify the model parameters by minimizing the governing equation residual.\n\nStarting point (fundamental base): The steady, incompressible Navier–Stokes equations in cylindrical coordinates reduce, under the assumptions of axisymmetry, no swirl, no radial or circumferential velocity, and fully developed axial velocity $u(r)$, to the scalar ordinary differential equation\n$$ \\frac{1}{r}\\frac{d}{dr}\\left(r \\frac{du}{dr}\\right) = S, $$\nwhere $r \\in [0,R]$ is the radial coordinate, $R$ is the pipe radius, and $S = \\frac{1}{\\mu}\\frac{dp}{dx}$ is a constant proportional to the axial pressure gradient $\\frac{dp}{dx}$ and inversely proportional to the dynamic viscosity $\\mu$. The physically realistic boundary and regularity conditions are\n$$ u(R) = 0, \\quad \\left|\\frac{du}{dr}(0)\\right| < \\infty. $$\n\nSymmetry-aware architecture: Reduce the effective input dimension by using the axisymmetric invariant $s = r^2$ and embed the boundary and regularity conditions into the trial solution by construction:\n$$ u_{\\theta}(r) = \\phi(r)\\, g_{\\theta}(s), \\quad \\phi(r) = 1 - \\left(\\frac{r}{R}\\right)^2, \\quad s = r^2. $$\nThis choice enforces $u_{\\theta}(R)=0$ for all parameter values and yields $\\frac{du_{\\theta}}{dr}(0)=0$, consistent with axisymmetry and smoothness at the axis.\n\nModel class: Use the affine model\n$$ g_{\\theta}(s) = \\theta_0 + \\theta_1 s, $$\nso that\n$$ u_{\\theta}(r) = \\left(1 - \\left(\\frac{r}{R}\\right)^2\\right)\\left(\\theta_0 + \\theta_1 r^2\\right). $$\n\nPhysics loss: Define the residual of the governing equation as\n$$ \\mathcal{R}(r;\\theta) = \\left(\\frac{d^2}{dr^2} + \\frac{1}{r}\\frac{d}{dr}\\right) u_{\\theta}(r) - S, \\quad r \\in (0,R]. $$\nMinimize the mean squared residual over a set of interior collocation points $\\{r_i\\}_{i=1}^N \\subset (0,R]$:\n$$ \\min_{\\theta \\in \\mathbb{R}^2} \\frac{1}{N} \\sum_{i=1}^N \\left(\\mathcal{R}(r_i;\\theta)\\right)^2. $$\nNote that the architecture eliminates the need for separate boundary loss terms.\n\nDerivation requirement: Starting only from the stated differential operator and the architecture definition, derive the linear system for the model parameters that results from recognizing that the residual is linear in $\\theta$ under the affine choice of $g_{\\theta}(s)$. Show how to compute the operator applied to the two basis functions\n$$ u_1(r) = \\phi(r), \\quad u_2(r) = \\phi(r)\\, r^2, $$\nso that\n$$ \\left(\\frac{d^2}{dr^2} + \\frac{1}{r}\\frac{d}{dr}\\right)u_{\\theta}(r) = \\theta_0\\, \\mathcal{L}[u_1](r) + \\theta_1\\, \\mathcal{L}[u_2](r), $$\nwith $\\mathcal{L}[\\cdot]$ denoting the differential operator. Use this to form an overdetermined linear system $A \\theta \\approx b$ with $A_{i0} = \\mathcal{L}[u_1](r_i)$, $A_{i1} = \\mathcal{L}[u_2](r_i)$, and $b_i = S$, and estimate $\\theta$ by least squares.\n\nEvaluation: For each test case below, after estimating $\\theta$, compute the maximum absolute pointwise error between the learned $u_{\\theta}(r)$ and the exact solution on a uniform evaluation grid $\\{r_j\\}_{j=1}^{M}$ with $M$ sufficiently large:\n$$ u_{\\text{exact}}(r) = -\\frac{S R^2}{4} \\left(1 - \\left(\\frac{r}{R}\\right)^2\\right), \\quad \\varepsilon_{\\infty} = \\max_{1 \\le j \\le M} \\left| u_{\\theta}(r_j) - u_{\\text{exact}}(r_j) \\right|. $$\n\nAngle unit: Not applicable. Physical units: Treat all variables as nondimensional; report errors as unitless decimal numbers.\n\nTest suite: Use the following parameter sets, each specified as $(R,\\mu,dpdx)$, with $S = \\frac{dpdx}{\\mu}$:\n- Case $1$: $(R,\\mu,dpdx) = (1.0, 1.0, -8.0)$.\n- Case $2$: $(R,\\mu,dpdx) = (0.5, 2.0, -5.0)$.\n- Case $3$ (edge: zero forcing): $(R,\\mu,dpdx) = (1.0, 1.0, 0.0)$.\n- Case $4$: $(R,\\mu,dpdx) = (1.25, 0.8, -3.333333333333333)$.\n\nYour program must:\n- Construct the symmetry-aware model and set up the least squares system as described.\n- Use an interior collocation set that excludes $r=0$ to avoid division by zero in intermediate expressions, but evaluate errors including $r=0$ by using the closed-form expressions where needed to avoid singularities.\n- For each test case, compute $\\varepsilon_{\\infty}$ as defined above.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in scientific notation with six significant digits, in the order of the test suite cases, e.g., $[a_1,a_2,a_3,a_4]$, where each $a_k$ is the value of $\\varepsilon_{\\infty}$ for case $k$.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Governing Equation:** Steady, incompressible, axisymmetric, fully developed laminar pipe flow is described by the Ordinary Differential Equation (ODE):\n$$ \\frac{1}{r}\\frac{d}{dr}\\left(r \\frac{du}{dr}\\right) = S, \\quad r \\in [0,R] $$\n- **Constants and Variables:**\n    - $u(r)$: Axial velocity profile.\n    - $r$: Radial coordinate.\n    - $R$: Pipe radius.\n    - $S = \\frac{1}{\\mu}\\frac{dp}{dx}$: Constant related to axial pressure gradient $\\frac{dp}{dx}$ and dynamic viscosity $\\mu$.\n- **Boundary and Regularity Conditions:**\n    - No-slip condition: $u(R) = 0$.\n    - Smoothness at the centerline: $\\left|\\frac{du}{dr}(0)\\right| < \\infty$.\n- **Symmetry-Aware Architecture:**\n    - Trial solution form: $u_{\\theta}(r) = \\phi(r)\\, g_{\\theta}(s)$.\n    - Input invariant: $s = r^2$.\n    - Boundary condition enforcing function: $\\phi(r) = 1 - \\left(\\frac{r}{R}\\right)^2$.\n    - Model class for the network: $g_{\\theta}(s) = \\theta_0 + \\theta_1 s$.\n    - Combined trial solution: $u_{\\theta}(r) = \\left(1 - \\left(\\frac{r}{R}\\right)^2\\right)\\left(\\theta_0 + \\theta_1 r^2\\right)$, where $\\theta = [\\theta_0, \\theta_1]^T$ are the learnable parameters.\n- **Physics Loss Formulation:**\n    - Differential operator: $\\mathcal{L}[\\cdot] = \\frac{d^2}{dr^2} + \\frac{1}{r}\\frac{d}{dr}$.\n    - Residual: $\\mathcal{R}(r;\\theta) = \\mathcal{L}[u_{\\theta}(r)] - S$.\n    - Objective function: Minimize the mean squared residual over $N$ interior collocation points $\\{r_i\\}_{i=1}^N \\subset (0,R]$: $\\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^N \\left(\\mathcal{R}(r_i;\\theta)\\right)^2$.\n- **Derivation Requirement:**\n    - Decompose the action of the operator: $\\mathcal{L}[u_{\\theta}(r)] = \\theta_0\\, \\mathcal{L}[u_1](r) + \\theta_1\\, \\mathcal{L}[u_2](r)$.\n    - Basis functions: $u_1(r) = \\phi(r)$, $u_2(r) = \\phi(r)\\, r^2$.\n    - Form overdetermined linear system $A \\theta \\approx b$ with $A_{i0} = \\mathcal{L}[u_1](r_i)$, $A_{i1} = \\mathcal{L}[u_2](r_i)$, $b_i = S$.\n- **Evaluation Metric:**\n    - Exact solution: $u_{\\text{exact}}(r) = -\\frac{S R^2}{4} \\left(1 - \\left(\\frac{r}{R}\\right)^2\\right)$.\n    - Maximum absolute pointwise error: $\\varepsilon_{\\infty} = \\max_{j} \\left| u_{\\theta}(r_j) - u_{\\text{exact}}(r_j) \\right|$ over a fine grid $\\{r_j\\}_{j=1}^{M}$.\n- **Test Suite:**\n    - Case 1: $(R,\\mu,dpdx) = (1.0, 1.0, -8.0)$.\n    - Case 2: $(R,\\mu,dpdx) = (0.5, 2.0, -5.0)$.\n    - Case 3: $(R,\\mu,dpdx) = (1.0, 1.0, 0.0)$.\n    - Case 4: $(R,\\mu,dpdx) = (1.25, 0.8, -3.333333333333333)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded:** The problem is based on the Navier-Stokes equations simplified for Hagen-Poiseuille flow, a cornerstone of fluid dynamics. The solution method, a form of the method of weighted residuals (specifically, least-squares collocation), is a standard and rigorous technique in computational science and engineering. The \"PINN\" terminology is correctly applied to a scenario where a model's parameters are determined by minimizing a physics-based residual.\n- **Well-Posed:** The problem provides a well-defined second-order ODE with sufficient boundary conditions. The proposed model architecture and loss function lead to a linear least-squares problem, which is known to have a unique and stable solution. All necessary data and definitions are provided.\n- **Objective:** The problem is stated in precise, formal, and objective mathematical and physical language. It is free from subjective or ambiguous terminology.\n- **Other Flaws:** The problem is self-contained, consistent, and computationally feasible. The given model architecture is sufficient to exactly represent the known analytical solution, which constitutes a robust check on the methodology.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-formulated exercise in scientific computing that correctly showcases the principles of physics-informed modeling. A solution will be provided.\n\n### Solution Derivation\nThe task is to find parameters $\\theta = [\\theta_0, \\theta_1]^T$ by minimizing the residual of the governing equation. The trial solution is a linear combination of basis functions:\n$$ u_{\\theta}(r) = \\theta_0 u_1(r) + \\theta_1 u_2(r) $$\nwhere the basis functions are\n$$ u_1(r) = 1 - \\frac{r^2}{R^2} $$\n$$ u_2(r) = \\left(1 - \\frac{r^2}{R^2}\\right)r^2 = r^2 - \\frac{r^4}{R^2} $$\nThe differential operator is $\\mathcal{L}[\\cdot] = \\frac{d^2}{dr^2} + \\frac{1}{r}\\frac{d}{dr}$. We must compute its action on each basis function.\n\nFor $u_1(r)$:\nFirst, we find the derivatives:\n$$ \\frac{du_1}{dr} = -\\frac{2r}{R^2} $$\n$$ \\frac{d^2u_1}{dr^2} = -\\frac{2}{R^2} $$\nNow, we apply the operator $\\mathcal{L}$:\n$$ \\mathcal{L}[u_1](r) = \\frac{d^2u_1}{dr^2} + \\frac{1}{r}\\frac{du_1}{dr} = \\left(-\\frac{2}{R^2}\\right) + \\frac{1}{r}\\left(-\\frac{2r}{R^2}\\right) = -\\frac{2}{R^2} - \\frac{2}{R^2} = -\\frac{4}{R^2} $$\n\nFor $u_2(r)$:\nFirst, we find the derivatives:\n$$ \\frac{du_2}{dr} = 2r - \\frac{4r^3}{R^2} $$\n$$ \\frac{d^2u_2}{dr^2} = 2 - \\frac{12r^2}{R^2} $$\nNow, we apply the operator $\\mathcal{L}$:\n$$ \\mathcal{L}[u_2](r) = \\frac{d^2u_2}{dr^2} + \\frac{1}{r}\\frac{du_2}{dr} = \\left(2 - \\frac{12r^2}{R^2}\\right) + \\frac{1}{r}\\left(2r - \\frac{4r^3}{R^2}\\right) $$\n$$ \\mathcal{L}[u_2](r) = 2 - \\frac{12r^2}{R^2} + 2 - \\frac{4r^2}{R^2} = 4 - \\frac{16r^2}{R^2} $$\n\nThe residual of the governing equation is given by $\\mathcal{R}(r;\\theta) = \\mathcal{L}[u_{\\theta}(r)] - S$. Using the linearity of the operator $\\mathcal{L}$:\n$$ \\mathcal{R}(r;\\theta) = \\theta_0 \\mathcal{L}[u_1](r) + \\theta_1 \\mathcal{L}[u_2](r) - S $$\nSubstituting the derived expressions:\n$$ \\mathcal{R}(r;\\theta) = \\theta_0 \\left(-\\frac{4}{R^2}\\right) + \\theta_1 \\left(4 - \\frac{16r^2}{R^2}\\right) - S $$\nTo minimize the sum of squared residuals over a set of $N$ collocation points $\\{r_i\\}_{i=1}^N$, we solve the linear system $A\\theta \\approx b$ in the least-squares sense. For each row $i$ corresponding to point $r_i$, the system is:\n$$ \\begin{pmatrix} \\mathcal{L}[u_1](r_i) & \\mathcal{L}[u_2](r_i) \\end{pmatrix} \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\end{pmatrix} = S $$\nThus, the matrix $A$ and vector $b$ are constructed as:\n$$ A = \\begin{pmatrix} -4/R^2 & 4 - 16r_1^2/R^2 \\\\ -4/R^2 & 4 - 16r_2^2/R^2 \\\\ \\vdots & \\vdots \\\\ -4/R^2 & 4 - 16r_N^2/R^2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} S \\\\ S \\\\ \\vdots \\\\ S \\end{pmatrix} $$\nThe optimal parameters $\\theta$ are found by solving the normal equations $(A^TA)\\theta = A^Tb$.\n\nA key observation is that the analytical solution $u_{\\text{exact}}(r)$ can be written as:\n$$ u_{\\text{exact}}(r) = \\left(-\\frac{SR^2}{4}\\right)\\left(1 - \\frac{r^2}{R^2}\\right) $$\nThis matches the form of the trial solution $u_{\\theta}(r)$ for the specific parameter choice $\\theta_0 = -SR^2/4$ and $\\theta_1 = 0$. For this choice, the residual is identically zero for all $r$:\n$$ \\mathcal{R}(r; \\theta_{exact}) = \\left(-\\frac{SR^2}{4}\\right)\\left(-\\frac{4}{R^2}\\right) + (0)\\left(4 - \\frac{16r^2}{R^2}\\right) - S = S - S = 0 $$\nBecause the model class contains the exact solution, the least-squares minimization of the residual must find these exact parameters, yielding a solution $u_{\\theta}(r)$ that is identical to $u_{\\text{exact}}(r)$. Consequently, the maximum absolute error $\\varepsilon_{\\infty}$ is expected to be zero, up to the limits of floating-point precision. The implementation will verify this.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted.\n\ndef solve():\n    \"\"\"\n    Designs and implements a physics-informed model for laminar pipe flow,\n    solves for its parameters using a linear least-squares formulation on the\n    governing equation's residual, and evaluates the error against the\n    analytical solution for several test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple (R, mu, dpdx).\n    test_cases = [\n        (1.0, 1.0, -8.0),\n        (0.5, 2.0, -5.0),\n        (1.0, 1.0, 0.0),\n        (1.25, 0.8, -3.333333333333333),\n    ]\n\n    # Parameters for the numerical method\n    N_collocation = 100  # Number of interior collocation points\n    M_evaluation = 1001 # Number of points for error evaluation grid\n\n    results = []\n\n    for case in test_cases:\n        R, mu, dpdx = case\n        S = dpdx / mu\n\n        # 1. Set up the least-squares problem to find theta = [theta_0, theta_1]\n        \n        # Create a set of N interior collocation points in (0, R].\n        # Avoiding r=0 as per problem, although derived expressions are safe.\n        r_colloc = np.linspace(R / N_collocation, R, N_collocation)\n\n        # Construct the matrix A for the linear system A*theta = b.\n        # A has shape (N_collocation, 2).\n        A = np.zeros((N_collocation, 2))\n\n        # Column 0 corresponds to the operator applied to the first basis function u_1.\n        # L[u_1](r) = -4 / R^2, a constant.\n        A[:, 0] = -4.0 / (R**2)\n\n        # Column 1 corresponds to the operator applied to the second basis function u_2.\n        # L[u_2](r) = 4 - 16 * r^2 / R^2.\n        A[:, 1] = 4.0 - 16.0 * (r_colloc**2) / (R**2)\n\n        # Construct the vector b.\n        # b_i = S for all i.\n        b = np.full(N_collocation, S)\n        \n        # 2. Solve for the model parameters theta using linear least squares.\n        # This minimizes the L2 norm of the residual ||A*theta - b||^2.\n        theta, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n        theta_0, theta_1 = theta[0], theta[1]\n\n        # 3. Evaluate the error between the learned and exact solutions.\n        \n        # Create a fine grid of M evaluation points over [0, R].\n        r_eval = np.linspace(0.0, R, M_evaluation)\n\n        # Calculate the learned solution u_theta(r) on the evaluation grid.\n        # u_theta(r) = (1 - (r/R)^2) * (theta_0 + theta_1 * r^2)\n        phi_r = 1.0 - (r_eval / R)**2\n        g_theta_s = theta_0 + theta_1 * (r_eval**2)\n        u_theta = phi_r * g_theta_s\n\n        # Calculate the exact analytical solution u_exact(r) on the evaluation grid.\n        # u_exact(r) = -S * R^2 / 4 * (1 - (r/R)^2)\n        if S == 0.0:\n            u_exact = np.zeros_like(r_eval)\n        else:\n            u_exact = (-S * R**2 / 4.0) * (1.0 - (r_eval / R)**2)\n\n        # Compute the maximum absolute pointwise error.\n        epsilon_inf = np.max(np.abs(u_theta - u_exact))\n        results.append(epsilon_inf)\n\n    # Final print statement in the exact required format.\n    # Format: [a_1,a_2,...] with each value in scientific notation with 6 significant digits.\n    # The format specifier \".5e\" gives 1 digit before decimal and 5 after, a total of 6.\n    formatted_results = [f\"{res:.5e}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Enforcing constraints like incompressibility, expressed by the divergence-free condition $\\nabla \\cdot \\mathbf{u} = 0$, is a classic challenge in computational fluid dynamics. This final practice explores how PINNs offer flexible new approaches to this problem by having you solve the 2D Stokes equations . You will implement and compare two common strategies: a \"soft\" constraint via a penalty in the loss function versus a \"hard\" constraint enforced by construction using a streamfunction formulation, providing insight into the trade-offs involved in tackling advanced scientific computing problems.",
            "id": "2411040",
            "problem": "You will implement and compare two Physics-Informed Neural Network (PINN) formulations for the steady, two-dimensional incompressible Navier–Stokes equations in the creeping-flow limit (Stokes equations), with particular attention to the enforcement of the divergence-free constraint. Your implementation must be a complete, runnable program.\n\nThe governing equations are the steady Stokes system on the unit square domain, written in dimensionless form as\n$$\n-\\nabla p + \\nu \\nabla^2 \\mathbf{u} + \\mathbf{f} = \\mathbf{0}, \\quad \\nabla \\cdot \\mathbf{u} = 0,\n$$\nwhere $\\mathbf{u} = (u,v)$ is the velocity, $p$ is pressure, $\\nu$ is the kinematic viscosity (a positive constant), and $\\mathbf{f}$ is a prescribed body force.\n\nTo provide a scientifically sound and testable setting, use the method of manufactured solutions. Define the exact streamfunction, velocity, and pressure as\n$$\n\\psi(x,y) = \\sin(\\pi x)\\sin(\\pi y), \\\\\nu^\\star(x,y) = \\frac{\\partial \\psi}{\\partial y} = \\pi \\sin(\\pi x)\\cos(\\pi y), \\\\\nv^\\star(x,y) = -\\frac{\\partial \\psi}{\\partial x} = -\\pi \\cos(\\pi x)\\sin(\\pi y), \\\\\np^\\star(x,y) = \\cos(\\pi x)\\cos(\\pi y),\n$$\nfor $(x,y) \\in [0,1]^2$. Let the kinematic viscosity be $\\nu = 0.01$ (a positive real number). Define the body force by substituting $(u^\\star,v^\\star,p^\\star)$ into the Stokes momentum balance:\n$$\n\\mathbf{f} = \\nabla p^\\star - \\nu \\nabla^2 \\mathbf{u}^\\star,\n$$\nso that $(\\mathbf{u}^\\star,p^\\star)$ exactly satisfy the steady Stokes equations with this $\\mathbf{f}$.\n\nYou will build two PINN variants that minimize a sum of squared residuals over a set of collocation points, anchored by sparse data. Both use a small, fixed feature basis (interpretable as a one-hidden-layer network with trigonometric activations) to make the problem computationally tractable and analytically differentiable with respect to inputs.\n\n- Method A (soft divergence penalty): Parameterize $u$, $v$, and $p$ independently as linear combinations of trigonometric features,\n$$\nu(x,y) = \\sum_{k=1}^{4} a_k \\,\\phi_k(x,y), \\quad v(x,y) = \\sum_{k=1}^{4} b_k \\,\\psi_k(x,y), \\quad p(x,y) = \\sum_{k=1}^{4} c_k \\,\\chi_k(x,y),\n$$\nwith the basis functions associated to the mode pairs $(m,n)\\in\\{(1,1),(2,1),(1,2),(2,2)\\}$:\n$$\n\\phi_{(m,n)}(x,y) = \\sin(m\\pi x)\\cos(n\\pi y), \\\\\n\\psi_{(m,n)}(x,y) = -\\cos(m\\pi x)\\sin(n\\pi y), \\\\\n\\chi_{(m,n)}(x,y) = \\cos(m\\pi x)\\cos(n\\pi y).\n$$\nMinimize the weighted sum of squared residuals consisting of the Stokes momentum residual at interior collocation points $(x_i,y_i)$, a sparse velocity data mismatch to $(u^\\star,v^\\star)$ at a set of anchor points $(\\tilde{x}_j,\\tilde{y}_j)$, one scalar pressure anchor at $(\\hat{x},\\hat{y})$, and a divergence penalty with weight $\\lambda \\ge 0$:\n$$\n\\mathcal{L}_\\text{soft} = \\frac{1}{N_\\Omega}\\sum_{i=1}^{N_\\Omega} \\left( r_u(x_i,y_i)^2 + r_v(x_i,y_i)^2 \\right) + \\alpha \\frac{1}{N_D}\\sum_{j=1}^{N_D} \\left[ \\left(u(\\tilde{x}_j,\\tilde{y}_j)-u^\\star(\\tilde{x}_j,\\tilde{y}_j)\\right)^2 + \\left(v(\\tilde{x}_j,\\tilde{y}_j)-v^\\star(\\tilde{x}_j,\\tilde{y}_j)\\right)^2 \\right] \\\\\n+ \\alpha_p \\left(p(\\hat{x},\\hat{y})-p^\\star(\\hat{x},\\hat{y})\\right)^2 + \\lambda \\frac{1}{N_\\Omega}\\sum_{i=1}^{N_\\Omega} \\left(\\frac{\\partial u}{\\partial x}(x_i,y_i) + \\frac{\\partial v}{\\partial y}(x_i,y_i)\\right)^2,\n$$\nwhere\n$$\nr_u = -\\frac{\\partial p}{\\partial x} + \\nu \\nabla^2 u + f_x, \\quad r_v = -\\frac{\\partial p}{\\partial y} + \\nu \\nabla^2 v + f_y.\n$$\nAll spatial derivatives of the trigonometric basis are analytic and must be used. The weights $\\alpha$ and $\\alpha_p$ are positive real numbers. Use $\\alpha = 10$ and $\\alpha_p = 10$. Choose $N_\\Omega = 64$ interior points defined on an $8\\times 8$ uniform grid over $(0,1)^2$ and $N_D = 10$ anchor points uniformly sampled in $(0,1)^2$ with a fixed pseudorandom seed.\n\n- Method B (hard divergence via streamfunction): Enforce $\\nabla \\cdot \\mathbf{u} = 0$ by construction using a streamfunction expansion,\n$$\n\\psi(x,y) = \\sum_{k=1}^{4} s_k \\,\\sigma_k(x,y), \\quad \\sigma_{(m,n)}(x,y) = \\sin(m\\pi x)\\sin(n\\pi y),\n$$\nand define\n$$\nu(x,y) = \\frac{\\partial \\psi}{\\partial y}(x,y), \\quad v(x,y) = -\\frac{\\partial \\psi}{\\partial x}(x,y), \\quad p(x,y) = \\sum_{k=1}^{4} c_k \\,\\chi_k(x,y),\n$$\nwith the same pressure basis $\\chi_k$. Minimize\n$$\n\\mathcal{L}_\\text{hard} = \\frac{1}{N_\\Omega}\\sum_{i=1}^{N_\\Omega} \\left( r_u(x_i,y_i)^2 + r_v(x_i,y_i)^2 \\right) + \\alpha \\frac{1}{N_D}\\sum_{j=1}^{N_D} \\left[ \\left(u(\\tilde{x}_j,\\tilde{y}_j)-u^\\star(\\tilde{x}_j,\\tilde{y}_j)\\right)^2 + \\left(v(\\tilde{x}_j,\\tilde{y}_j)-v^\\star(\\tilde{x}_j,\\tilde{y}_j)\\right)^2 \\right] \\\\\n+ \\alpha_p \\left(p(\\hat{x},\\hat{y})-p^\\star(\\hat{x},\\hat{y})\\right)^2,\n$$\nwhere $r_u$ and $r_v$ are defined as above, and all derivatives of the basis are analytic.\n\nBecause the Stokes equations are linear in $(u,v,p)$ and because the above parameterizations are linear in the coefficients, the residuals are linear in the coefficients. Therefore, each of the above optimization problems reduces to a standard linear least-squares problem of the form\n$$\n\\min_{\\boldsymbol{\\theta}} \\left\\| \\mathbf{W}(\\mathbf{A}\\boldsymbol{\\theta} - \\mathbf{b}) \\right\\|_2^2,\n$$\nwhere $\\boldsymbol{\\theta}$ collects the coefficients. You must form the design matrix $\\mathbf{A}$ and right-hand side $\\mathbf{b}$ explicitly from the analytic basis derivatives, and solve using a robust linear least-squares method.\n\nTest suite specification:\n\n- Use the above setting with $\\nu = 0.01$ (unitless). Use the exact $(u^\\star,v^\\star,p^\\star)$ and $\\mathbf{f}$ as defined.\n\n- For Method A (soft divergence), run three cases with the divergence penalty weights\n$$\n\\lambda \\in \\{0.0, 1.0, 100.0\\}.\n$$\n\n- For Method B (hard divergence), solve once (no $\\lambda$ parameter is needed).\n\nFor evaluation, define a validation grid of $20\\times 20$ uniformly spaced points on $(0,1)^2$. For each case, compute:\n\n- The divergence norm\n$$\n\\mathcal{N}_\\text{div} = \\sqrt{ \\frac{1}{N_V} \\sum_{q=1}^{N_V} \\left( \\frac{\\partial u}{\\partial x}(x_q,y_q) + \\frac{\\partial v}{\\partial y}(x_q,y_q) \\right)^2 },\n$$\nwhere $N_V = 400$ is the number of validation points.\n\n- The momentum residual norm\n$$\n\\mathcal{N}_\\text{mom} = \\sqrt{ \\frac{1}{N_V} \\sum_{q=1}^{N_V} \\left[ r_u(x_q,y_q)^2 + r_v(x_q,y_q)^2 \\right] }.\n$$\n\n- The relative velocity error\n$$\n\\mathcal{E}_\\mathbf{u} = \\frac{ \\sqrt{ \\frac{1}{N_V} \\sum_{q=1}^{N_V} \\left[ \\left(u(x_q,y_q)-u^\\star(x_q,y_q)\\right)^2 + \\left(v(x_q,y_q)-v^\\star(x_q,y_q)\\right)^2 \\right] } }{ \\sqrt{ \\frac{1}{N_V} \\sum_{q=1}^{N_V} \\left[ \\left(u^\\star(x_q,y_q)\\right)^2 + \\left(v^\\star(x_q,y_q)\\right)^2 \\right] } }.\n$$\n\nFinal output format:\n\n- Your program must produce a single line of output containing a list of twelve floats, rounded to six decimal places, in the following order:\n$$\n\\left[ \\mathcal{N}_\\text{div}^{(\\lambda=0)}, \\mathcal{N}_\\text{mom}^{(\\lambda=0)}, \\mathcal{E}_\\mathbf{u}^{(\\lambda=0)}, \\mathcal{N}_\\text{div}^{(\\lambda=1)}, \\mathcal{N}_\\text{mom}^{(\\lambda=1)}, \\mathcal{E}_\\mathbf{u}^{(\\lambda=1)}, \\mathcal{N}_\\text{div}^{(\\lambda=100)}, \\mathcal{N}_\\text{mom}^{(\\lambda=100)}, \\mathcal{E}_\\mathbf{u}^{(\\lambda=100)}, \\mathcal{N}_\\text{div}^\\text{(hard)}, \\mathcal{N}_\\text{mom}^\\text{(hard)}, \\mathcal{E}_\\mathbf{u}^\\text{(hard)} \\right].\n$$\n- The numbers must be produced in this exact sequence and formatting, including the enclosing square brackets and comma separators.\n\nAngle units do not apply. No physical units are required; all quantities are dimensionless real numbers. All random sampling must use a fixed seed so the output is deterministic. Your code must be self-contained and require no user interaction.",
            "solution": "The problem posed is a well-defined exercise in computational science, specifically in the domain of Physics-Informed Neural Networks (PINNs) applied to computational fluid dynamics. It requires the implementation and comparison of two methods for solving the steady Stokes equations on a unit square, using the method of manufactured solutions for verification.\n\n**Problem Validation**\n\nThe problem is validated as scientifically grounded, well-posed, and objective. It is based on the fundamental Stokes equations of fluid mechanics. The use of a manufactured solution is a standard verification technique. The parameterization via a finite Fourier-like basis, which can be interpreted as a simple neural network with fixed trigonometric activation functions, is a valid approach for demonstrating PINN concepts. The problem reduces to solving a linear least-squares system, which is a well-posed mathematical problem. All constants, equations, and evaluation metrics are specified with sufficient precision. The minor ambiguity regarding the placement of the single pressure anchor point is resolved by selecting a fixed, representative point, such as the domain center $(0.5, 0.5)$, a standard practice that does not invalidate the problem's integrity. Consequently, the problem is deemed valid and a rigorous solution can be formulated.\n\n**Principle-Based Solution Design**\n\nThe core of the problem is to find an approximate solution $(\\mathbf{u}, p)$ to the Stokes equations:\n$$\n-\\nabla p + \\nu \\nabla^2 \\mathbf{u} + \\mathbf{f} = \\mathbf{0} \\\\\n\\nabla \\cdot \\mathbf{u} = 0\n$$\nby representing the unknown fields as linear combinations of prescribed basis functions with unknown coefficients. The coefficients are determined by minimizing a loss function composed of residuals of the governing equations and data mismatch terms. This process is equivalent to solving a linear least-squares problem.\n\n**1. Manufactured Solution and Forcing Term**\n\nTo provide a ground truth for evaluating the accuracy of the methods, a manufactured solution is defined:\n$$\n\\psi(x,y) = \\sin(\\pi x)\\sin(\\pi y) \\\\\nu^\\star(x,y) = \\pi \\sin(\\pi x)\\cos(\\pi y) \\\\\nv^\\star(x,y) = -\\pi \\cos(\\pi x)\\sin(\\pi y) \\\\\np^\\star(x,y) = \\cos(\\pi x)\\cos(\\pi y)\n$$\nThese fields exactly satisfy the Stokes equations if the body force $\\mathbf{f} = (f_x, f_y)$ is defined as:\n$$\n\\mathbf{f} = \\nabla p^\\star - \\nu \\nabla^2 \\mathbf{u}^\\star\n$$\nComputing the necessary derivatives analytically, we obtain the forcing functions:\n$$\n\\begin{align*}\nf_x(x,y) &= \\frac{\\partial p^\\star}{\\partial x} - \\nu \\left( \\frac{\\partial^2 u^\\star}{\\partial x^2} + \\frac{\\partial^2 u^\\star}{\\partial y^2} \\right) = (-\\pi + 2\\nu\\pi^3) \\sin(\\pi x)\\cos(\\pi y) \\\\\nf_y(x,y) &= \\frac{\\partial p^\\star}{\\partial y} - \\nu \\left( \\frac{\\partial^2 v^\\star}{\\partial x^2} + \\frac{\\partial^2 v^\\star}{\\partial y^2} \\right) = (-\\pi - 2\\nu\\pi^3) \\cos(\\pi x)\\sin(\\pi y)\n\\end{align*}\n$$\nwith viscosity $\\nu = 0.01$.\n\n**2. Point Discretization**\n\nThe continuous problem is discretized using three sets of points:\n- **Collocation Points ($N_\\Omega = 64$)**: An $8 \\times 8$ uniform grid in the interior of $(0,1)^2$ where the physics residuals are enforced.\n- **Data Anchor Points ($N_D = 10$)**: Points sampled from a uniform random distribution on $(0,1)^2$ (using a fixed seed for reproducibility), where the velocity solution is constrained to match the manufactured solution $u^\\star$ and $v^\\star$.\n- **Pressure Anchor Point**: A single point, chosen as $(\\hat{x}, \\hat{y}) = (0.5, 0.5)$, to fix the pressure level, which is otherwise determined only up to an additive constant.\n\n**3. Linear Least-Squares Formulation**\n\nBoth Method A and Method B seek to find a vector of coefficients $\\boldsymbol{\\theta}$ that minimizes a weighted sum of squared errors. This can be expressed as a standard linear least-squares problem:\n$$\n\\min_{\\boldsymbol{\\theta}} \\| \\mathbf{A}\\boldsymbol{\\theta} - \\mathbf{b} \\|_2^2\n$$\nHere, $\\mathbf{A}$ is the design matrix, whose entries are values of the basis functions or their derivatives evaluated at the discrete points, and $\\mathbf{b}$ is the right-hand-side vector, containing values from the forcing term or the manufactured solution. The weights from the loss function, such as $\\alpha$, $\\alpha_p$, and $\\lambda$, are incorporated by multiplying the corresponding rows of both $\\mathbf{A}$ and $\\mathbf{b}$ by the square root of said weights. This system is overdetermined and is solved using a robust method like `scipy.linalg.lstsq`.\n\n**Method A: Soft Divergence Constraint**\n\nThe velocity components $(u,v)$ and pressure $p$ are parameterized independently:\n$$\nu(x,y) = \\sum_{k=1}^{4} a_k \\phi_k(x,y), \\quad v(x,y) = \\sum_{k=1}^{4} b_k \\psi_k(x,y), \\quad p(x,y) = \\sum_{k=1}^{4} c_k \\chi_k(x,y)\n$$\nThe coefficient vector is $\\boldsymbol{\\theta}_A = [a_1, ..., a_4, b_1, ..., b_4, c_1, ..., c_4]^T$, of size $12$. The linear system is constructed from the following equations:\n- **Momentum Residuals ($2N_\\Omega$ equations)**: At each collocation point $(x_i, y_i)$,\n$$\n\\nu \\sum_{k=1}^{4} a_k \\nabla^2\\phi_k(x_i,y_i) - \\sum_{k=1}^{4} c_k \\frac{\\partial\\chi_k}{\\partial x}(x_i,y_i) = -f_x(x_i,y_i) \\\\\n\\nu \\sum_{k=1}^{4} b_k \\nabla^2\\psi_k(x_i,y_i) - \\sum_{k=1}^{4} c_k \\frac{\\partial\\chi_k}{\\partial y}(x_i,y_i) = -f_y(x_i,y_i)\n$$\nThese rows are weighted by $\\sqrt{1/N_\\Omega}$.\n- **Velocity Data Constraints ($2N_D$ equations)**: At each anchor point $(\\tilde{x}_j, \\tilde{y}_j)$,\n$$\n\\sum_{k=1}^{4} a_k \\phi_k(\\tilde{x}_j,\\tilde{y}_j) = u^\\star(\\tilde{x}_j,\\tilde{y}_j) \\\\\n\\sum_{k=1}^{4} b_k \\psi_k(\\tilde{x}_j,\\tilde{y}_j) = v^\\star(\\tilde{x}_j,\\tilde{y}_j)\n$$\nThese rows are weighted by $\\sqrt{\\alpha/N_D}$.\n- **Pressure Anchor Constraint (1 equation)**: At $(\\hat{x}, \\hat{y})$,\n$$\n\\sum_{k=1}^{4} c_k \\chi_k(\\hat{x},\\hat{y}) = p^\\star(\\hat{x},\\hat{y})\n$$\nThis row is weighted by $\\sqrt{\\alpha_p}$.\n- **Divergence Penalty ($N_\\Omega$ equations)**: At each collocation point $(x_i, y_i)$,\n$$\n\\sum_{k=1}^{4} a_k \\frac{\\partial\\phi_k}{\\partial x}(x_i,y_i) + \\sum_{k=1}^{4} b_k \\frac{\\partial\\psi_k}{\\partial y}(x_i,y_i) = 0\n$$\nThese rows are weighted by $\\sqrt{\\lambda/N_\\Omega}$. This procedure is repeated for $\\lambda \\in \\{0.0, 1.0, 100.0\\}$.\n\n**Method B: Hard Divergence Constraint**\n\nThe incompressibility constraint $\\nabla \\cdot \\mathbf{u} = 0$ is satisfied exactly (by construction) using a streamfunction formulation:\n$$\n\\psi(x,y) = \\sum_{k=1}^{4} s_k \\sigma_k(x,y) \\implies u = \\frac{\\partial \\psi}{\\partial y}, v = -\\frac{\\partial \\psi}{\\partial x}\n$$\nThe pressure is parameterized as before. The coefficient vector is $\\boldsymbol{\\theta}_B = [s_1, ..., s_4, c_1, ..., c_4]^T$, of size $8$. The linear system is built from:\n- **Momentum Residuals ($2N_\\Omega$ equations)**: At each collocation point $(x_i, y_i)$,\n$$\n\\nu \\sum_{k=1}^{4} s_k \\frac{\\partial(\\nabla^2\\sigma_k)}{\\partial y}(x_i,y_i) - \\sum_{k=1}^{4} c_k \\frac{\\partial\\chi_k}{\\partial x}(x_i,y_i) = -f_x(x_i,y_i) \\\\\n\\nu \\sum_{k=1}^{4} s_k \\left(-\\frac{\\partial(\\nabla^2\\sigma_k)}{\\partial x}\\right)(x_i,y_i) - \\sum_{k=1}^{4} c_k \\frac{\\partial\\chi_k}{\\partial y}(x_i,y_i) = -f_y(x_i,y_i)\n$$\nThese rows are weighted by $\\sqrt{1/N_\\Omega}$.\n- **Velocity Data Constraints ($2N_D$ equations)**: At each anchor point $(\\tilde{x}_j, \\tilde{y}_j)$,\n$$\n\\sum_{k=1}^{4} s_k \\frac{\\partial\\sigma_k}{\\partial y}(\\tilde{x}_j,\\tilde{y}_j) = u^\\star(\\tilde{x}_j,\\tilde{y}_j) \\\\\n\\sum_{k=1}^{4} s_k \\left(-\\frac{\\partial\\sigma_k}{\\partial x}\\right)(\\tilde{x}_j,\\tilde{y}_j) = v^\\star(\\tilde{x}_j,\\tilde{y}_j)\n$$\nThese rows are weighted by $\\sqrt{\\alpha/N_D}$.\n- **Pressure Anchor Constraint (1 equation)**: Same as in Method A, weighted by $\\sqrt{\\alpha_p}$.\nThere are no divergence penalty equations, as the constraint is satisfied structurally.\n\n**4. Evaluation**\n\nAfter solving for the coefficients $\\boldsymbol{\\theta}$ in each case, the resulting approximate solutions $(u,v,p)$ are evaluated on a fine $20 \\times 20$ validation grid. Three metrics are computed: the root-mean-square norm of the velocity divergence $\\mathcal{N}_\\text{div}$, the root-mean-square norm of the momentum residual $\\mathcal{N}_\\text{mom}$, and the relative $L_2$ error of the velocity field $\\mathcal{E}_\\mathbf{u}$. For Method B, $\\mathcal{N}_\\text{div}$ is expected to be zero to within machine precision, providing a verification of the implementation. The results from all four cases (Method A with three values of $\\lambda$, and Method B) are then aggregated and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lstsq\n\ndef solve():\n    # --- Problem Parameters ---\n    NU = 0.01\n    ALPHA = 10.0\n    ALPHA_P = 10.0\n    N_OMEGA_SIDE = 8\n    N_OMEGA = N_OMEGA_SIDE * N_OMEGA_SIDE\n    N_D = 10\n    N_VALID_SIDE = 20\n    N_V = N_VALID_SIDE * N_VALID_SIDE\n    SEED = 42\n    MODES = [(1, 1), (2, 1), (1, 2), (2, 2)]\n    LAMBDA_VALS = [0.0, 1.0, 100.0]\n\n    # --- Grids ---\n    # Collocation points\n    x_omega_1d = np.linspace(0, 1, N_OMEGA_SIDE + 2)[1:-1]\n    y_omega_1d = np.linspace(0, 1, N_OMEGA_SIDE + 2)[1:-1]\n    x_omega_grid, y_omega_grid = np.meshgrid(x_omega_1d, y_omega_1d)\n    collocation_pts = np.vstack([x_omega_grid.ravel(), y_omega_grid.ravel()]).T\n\n    # Data anchor points\n    rng = np.random.default_rng(SEED)\n    data_pts = rng.uniform(0, 1, size=(N_D, 2))\n\n    # Pressure anchor point\n    pressure_pt = np.array([[0.5, 0.5]])\n\n    # Validation points\n    x_val_1d = np.linspace(0, 1, N_VALID_SIDE + 2)[1:-1]\n    y_val_1d = np.linspace(0, 1, N_VALID_SIDE + 2)[1:-1]\n    x_val_grid, y_val_grid = np.meshgrid(x_val_1d, y_val_1d)\n    validation_pts = np.vstack([x_val_grid.ravel(), y_val_grid.ravel()]).T\n\n    # --- Manufactured Solution and Forcing Term ---\n    def u_star(x, y):\n        return np.pi * np.sin(np.pi * x) * np.cos(np.pi * y)\n\n    def v_star(x, y):\n        return -np.pi * np.cos(np.pi * x) * np.sin(np.pi * y)\n\n    def p_star(x, y):\n        return np.cos(np.pi * x) * np.cos(np.pi * y)\n\n    def f_x(x, y):\n        return (-np.pi + 2 * NU * np.pi**3) * np.sin(np.pi * x) * np.cos(np.pi * y)\n\n    def f_y(x, y):\n        return (-np.pi - 2 * NU * np.pi**3) * np.cos(np.pi * x) * np.sin(np.pi * y)\n\n    # --- Basis Functions and Derivatives ---\n    pi = np.pi\n\n    # Method A basis\n    def phi(x, y, m, n): return np.sin(m*pi*x) * np.cos(n*pi*y)\n    def dphi_dx(x, y, m, n): return m*pi * np.cos(m*pi*x) * np.cos(n*pi*y)\n    def lapl_phi(x, y, m, n): return -(m*pi)**2 * phi(x,y,m,n) - (n*pi)**2 * phi(x,y,m,n)\n\n    def psi(x, y, m, n): return -np.cos(m*pi*x) * np.sin(n*pi*y)\n    def dpsi_dy(x, y, m, n): return -n*pi * np.cos(m*pi*x) * np.cos(n*pi*y)\n    def lapl_psi(x, y, m, n): return -(m*pi)**2 * psi(x,y,m,n) - (n*pi)**2 * psi(x,y,m,n)\n\n    # Method B basis (streamfunction)\n    def sigma(x, y, m, n): return np.sin(m*pi*x) * np.sin(n*pi*y)\n    def dsigma_dx(x, y, m, n): return m*pi * np.cos(m*pi*x) * np.sin(n*pi*y)\n    def dsigma_dy(x, y, m, n): return n*pi * np.sin(m*pi*x) * np.cos(n*pi*y)\n    def lapl_dsigma_dx(x, y, m, n): return -(n*pi)**2 * dsigma_dx(x,y,m,n) - (m*pi)**2 * dsigma_dx(x,y,m,n)\n    def lapl_dsigma_dy(x, y, m, n): return -(m*pi)**2 * dsigma_dy(x,y,m,n) - (n*pi)**2 * dsigma_dy(x,y,m,n)\n\n    # Common pressure basis\n    def chi(x, y, m, n): return np.cos(m*pi*x) * np.cos(n*pi*y)\n    def dchi_dx(x, y, m, n): return -m*pi * np.sin(m*pi*x) * np.cos(n*pi*y)\n    def dchi_dy(x, y, m, n): return -n*pi * np.cos(m*pi*x) * np.sin(n*pi*y)\n\n    # --- Solver for Method A ---\n    def solve_method_A(lambda_val):\n        num_coeffs = 12\n        num_rows = 2 * N_OMEGA + 2 * N_D + 1 + N_OMEGA\n        A = np.zeros((num_rows, num_coeffs))\n        b = np.zeros(num_rows)\n        \n        x_c, y_c = collocation_pts[:, 0], collocation_pts[:, 1]\n        x_d, y_d = data_pts[:, 0], data_pts[:, 1]\n        x_p, y_p = pressure_pt[0, 0], pressure_pt[0, 1]\n\n        w_mom = np.sqrt(1 / N_OMEGA)\n        w_data = np.sqrt(ALPHA / N_D)\n        w_p = np.sqrt(ALPHA_P)\n        w_div = np.sqrt(lambda_val / N_OMEGA)\n\n        row = 0\n        # Momentum residual (u)\n        for i in range(N_OMEGA):\n            for k, (m, n) in enumerate(MODES):\n                A[row, k] = w_mom * NU * lapl_phi(x_c[i], y_c[i], m, n)\n                A[row, k + 8] = w_mom * -dchi_dx(x_c[i], y_c[i], m, n)\n            b[row] = w_mom * -f_x(x_c[i], y_c[i])\n            row += 1\n        # Momentum residual (v)\n        for i in range(N_OMEGA):\n            for k, (m, n) in enumerate(MODES):\n                A[row, k + 4] = w_mom * NU * lapl_psi(x_c[i], y_c[i], m, n)\n                A[row, k + 8] = w_mom * -dchi_dy(x_c[i], y_c[i], m, n)\n            b[row] = w_mom * -f_y(x_c[i], y_c[i])\n            row += 1\n        # Velocity data\n        for i in range(N_D):\n            for k, (m, n) in enumerate(MODES):\n                A[row, k] = w_data * phi(x_d[i], y_d[i], m, n)\n            b[row] = w_data * u_star(x_d[i], y_d[i])\n            row += 1\n            for k, (m, n) in enumerate(MODES):\n                A[row, k + 4] = w_data * psi(x_d[i], y_d[i], m, n)\n            b[row] = w_data * v_star(x_d[i], y_d[i])\n            row += 1\n        # Pressure anchor\n        for k, (m, n) in enumerate(MODES):\n            A[row, k + 8] = w_p * chi(x_p, y_p, m, n)\n        b[row] = w_p * p_star(x_p, y_p)\n        row += 1\n        # Divergence penalty\n        for i in range(N_OMEGA):\n            for k, (m, n) in enumerate(MODES):\n                A[row, k] = w_div * dphi_dx(x_c[i], y_c[i], m, n)\n                A[row, k + 4] = w_div * dpsi_dy(x_c[i], y_c[i], m, n)\n            b[row] = 0\n            row += 1\n        \n        coeffs, _, _, _ = lstsq(A, b)\n        return coeffs\n\n    # --- Solver for Method B ---\n    def solve_method_B():\n        num_coeffs = 8\n        num_rows = 2 * N_OMEGA + 2 * N_D + 1\n        A = np.zeros((num_rows, num_coeffs))\n        b = np.zeros(num_rows)\n        \n        x_c, y_c = collocation_pts[:, 0], collocation_pts[:, 1]\n        x_d, y_d = data_pts[:, 0], data_pts[:, 1]\n        x_p, y_p = pressure_pt[0, 0], pressure_pt[0, 1]\n\n        w_mom = np.sqrt(1 / N_OMEGA)\n        w_data = np.sqrt(ALPHA / N_D)\n        w_p = np.sqrt(ALPHA_P)\n\n        row = 0\n        # Momentum residual (u)\n        for i in range(N_OMEGA):\n            for k, (m, n) in enumerate(MODES):\n                A[row, k] = w_mom * NU * lapl_dsigma_dy(x_c[i], y_c[i], m, n)\n                A[row, k + 4] = w_mom * -dchi_dx(x_c[i], y_c[i], m, n)\n            b[row] = w_mom * -f_x(x_c[i], y_c[i])\n            row += 1\n        # Momentum residual (v)\n        for i in range(N_OMEGA):\n            for k, (m, n) in enumerate(MODES):\n                A[row, k] = w_mom * NU * -lapl_dsigma_dx(x_c[i], y_c[i], m, n)\n                A[row, k + 4] = w_mom * -dchi_dy(x_c[i], y_c[i], m, n)\n            b[row] = w_mom * -f_y(x_c[i], y_c[i])\n            row += 1\n        # Velocity data\n        for i in range(N_D):\n            for k, (m, n) in enumerate(MODES):\n                A[row, k] = w_data * dsigma_dy(x_d[i], y_d[i], m, n)\n            b[row] = w_data * u_star(x_d[i], y_d[i])\n            row += 1\n            for k, (m, n) in enumerate(MODES):\n                A[row, k] = w_data * -dsigma_dx(x_d[i], y_d[i], m, n)\n            b[row] = w_data * v_star(x_d[i], y_d[i])\n            row += 1\n        # Pressure anchor\n        for k, (m, n) in enumerate(MODES):\n            A[row, k + 4] = w_p * chi(x_p, y_p, m, n)\n        b[row] = w_p * p_star(x_p, y_p)\n        row += 1\n        \n        coeffs, _, _, _ = lstsq(A, b)\n        return coeffs\n\n    # --- Evaluation ---\n    def evaluate(coeffs, method):\n        x_v, y_v = validation_pts[:, 0], validation_pts[:, 1]\n        \n        u_pred, v_pred, p_pred = np.zeros(N_V), np.zeros(N_V), np.zeros(N_V)\n        du_dx, dv_dy = np.zeros(N_V), np.zeros(N_V)\n        lapl_u, lapl_v = np.zeros(N_V), np.zeros(N_V)\n        dp_dx, dp_dy = np.zeros(N_V), np.zeros(N_V)\n\n        if method == 'A':\n            a_coeffs, b_coeffs, c_coeffs = coeffs[0:4], coeffs[4:8], coeffs[8:12]\n            for k, (m, n) in enumerate(MODES):\n                u_pred += a_coeffs[k] * phi(x_v, y_v, m, n)\n                v_pred += b_coeffs[k] * psi(x_v, y_v, m, n)\n                p_pred += c_coeffs[k] * chi(x_v, y_v, m, n)\n                du_dx += a_coeffs[k] * dphi_dx(x_v, y_v, m, n)\n                dv_dy += b_coeffs[k] * dpsi_dy(x_v, y_v, m, n)\n                lapl_u += a_coeffs[k] * lapl_phi(x_v, y_v, m, n)\n                lapl_v += b_coeffs[k] * lapl_psi(x_v, y_v, m, n)\n                dp_dx += c_coeffs[k] * dchi_dx(x_v, y_v, m, n)\n                dp_dy += c_coeffs[k] * dchi_dy(x_v, y_v, m, n)\n        elif method == 'B':\n            s_coeffs, c_coeffs = coeffs[0:4], coeffs[4:8]\n            for k, (m, n) in enumerate(MODES):\n                u_pred += s_coeffs[k] * dsigma_dy(x_v, y_v, m, n)\n                v_pred += s_coeffs[k] * -dsigma_dx(x_v, y_v, m, n)\n                p_pred += c_coeffs[k] * chi(x_v, y_v, m, n)\n                du_dx += s_coeffs[k] * n*pi * m*pi * np.cos(m*pi*x_v)*np.cos(n*pi*y_v) # d/dx(dsigma/dy)\n                dv_dy += s_coeffs[k] * -m*pi * n*pi * np.cos(m*pi*x_v)*np.cos(n*pi*y_v) # d/dy(-dsigma/dx)\n                lapl_u += s_coeffs[k] * lapl_dsigma_dy(x_v, y_v, m, n)\n                lapl_v += s_coeffs[k] * -lapl_dsigma_dx(x_v, y_v, m, n)\n                dp_dx += c_coeffs[k] * dchi_dx(x_v, y_v, m, n)\n                dp_dy += c_coeffs[k] * dchi_dy(x_v, y_v, m, n)\n        \n        # N_div\n        div_u = du_dx + dv_dy\n        n_div = np.sqrt(np.mean(div_u**2))\n        \n        # N_mom\n        r_u = -dp_dx + NU * lapl_u + f_x(x_v, y_v)\n        r_v = -dp_dy + NU * lapl_v + f_y(x_v, y_v)\n        n_mom = np.sqrt(np.mean(r_u**2 + r_v**2))\n        \n        # E_u\n        u_s, v_s = u_star(x_v, y_v), v_star(x_v, y_v)\n        error_num = np.sqrt(np.mean((u_pred - u_s)**2 + (v_pred - v_s)**2))\n        error_den = np.sqrt(np.mean(u_s**2 + v_s**2))\n        e_u = error_num / error_den if error_den > 0 else 0.0\n\n        return n_div, n_mom, e_u\n\n    # --- Main Execution Logic ---\n    results = []\n    \n    # Method A cases\n    for lambda_val in LAMBDA_VALS:\n        coeffs_A = solve_method_A(lambda_val)\n        metrics_A = evaluate(coeffs_A, 'A')\n        results.extend(metrics_A)\n        \n    # Method B case\n    coeffs_B = solve_method_B()\n    metrics_B = evaluate(coeffs_B, 'B')\n    results.extend(metrics_B)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\nsolve()\n```"
        }
    ]
}