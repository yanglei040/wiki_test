{
    "hands_on_practices": [
        {
            "introduction": "Often in engineering models, we face situations where different parameter combinations can explain our data equally well, a problem known as non-identifiability. In such cases, methods like Maximum Likelihood Estimation may fail to provide a unique solution. This exercise demonstrates the power of Bayesian inference, showing how incorporating prior knowledge through an informative prior distribution can break the ambiguity and enable robust estimation of individual parameters .",
            "id": "2374096",
            "problem": "In a computational engineering calibration task, a lumped model predicts a measured scalar response as the sum of two component parameters. A single experiment produces one measurement modeled as\n$$\ny = \\theta_1 + \\theta_2 + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith observed value $y_{\\text{obs}} = 10$ and known noise variance $\\sigma^2 = 1$. The parameters $\\theta_1$ and $\\theta_2$ represent distinct component contributions that are of engineering interest individually. Because only the sum is observed in this single experiment, the likelihood is non-identifying for $(\\theta_1,\\theta_2)$. To incorporate prior engineering knowledge, use an improper flat prior for $\\theta_1$ (i.e., $p(\\theta_1) \\propto 1$ over $\\mathbb{R}$) and a strongly informative Gaussian prior for $\\theta_2$,\n$$\n\\theta_2 \\sim \\mathcal{N}(\\mu_2,\\tau_2^2),\\quad \\mu_2 = 6,\\ \\tau_2^2 = 0.04.\n$$\nConsider the posterior inference that results from this model.\n\nWhich of the following statements are correct?\n\nA. The marginal posterior for $\\theta_1$ is Gaussian with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$; numerically, it is $\\mathcal{N}(4, 1.04)$.\n\nB. If the prior on $\\theta_2$ were also improper flat (so both $\\theta_1$ and $\\theta_2$ had flat priors), then the posterior for $\\theta_1$ given $y_{\\text{obs}}$ would be improper.\n\nC. Decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$, and this variance cannot be reduced below $\\sigma^2$.\n\nD. Under the stated model, the maximum likelihood estimate of $\\theta_1$ equals $4$.",
            "solution": "The problem requires the derivation and analysis of the posterior distribution for the parameters $\\theta_1$ and $\\theta_2$ in a simple linear model, where the likelihood is non-identifying. The use of prior information is essential to obtain a well-defined posterior. We will proceed by first deriving the marginal posterior distribution for the parameter of interest, $\\theta_1$, and then evaluating each statement.\n\nThe model is defined by the following components:\nThe likelihood function is derived from the measurement model $y = \\theta_1 + \\theta_2 + \\varepsilon$, where the error term $\\varepsilon$ follows a normal distribution $\\mathcal{N}(0, \\sigma^2)$. Given a single observation $y_{\\text{obs}}$, the likelihood of the parameters $(\\theta_1, \\theta_2)$ is:\n$$p(y_{\\text{obs}} | \\theta_1, \\theta_2, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nThe numerical values are given as $y_{\\text{obs}} = 10$ and $\\sigma^2 = 1$.\n\nThe prior distributions for the parameters are given as:\n- For $\\theta_1$: an improper uniform (flat) prior, $p(\\theta_1) \\propto 1$ for $\\theta_1 \\in \\mathbb{R}$.\n- For $\\theta_2$: a proper normal (Gaussian) prior, $\\theta_2 \\sim \\mathcal{N}(\\mu_2, \\tau_2^2)$, with $p(\\theta_2) = \\frac{1}{\\sqrt{2\\pi\\tau_2^2}} \\exp\\left(-\\frac{(\\theta_2 - \\mu_2)^2}{2\\tau_2^2}\\right)$.\nThe numerical values for the prior on $\\theta_2$ are $\\mu_2 = 6$ and $\\tau_2^2 = 0.04$.\n\nAssuming prior independence between $\\theta_1$ and $\\theta_2$, the joint prior is $p(\\theta_1, \\theta_2) = p(\\theta_1)p(\\theta_2)$.\n\nAccording to Bayes' theorem, the joint posterior distribution is proportional to the product of the likelihood and the joint prior:\n$$p(\\theta_1, \\theta_2 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(\\theta_2 - \\mu_2)^2}{2\\tau_2^2}\\right)$$\nTo find the marginal posterior for $\\theta_1$, we must integrate the joint posterior over all possible values of $\\theta_2$:\n$$p(\\theta_1 | y_{\\text{obs}}) = \\int_{-\\infty}^{\\infty} p(\\theta_1, \\theta_2 | y_{\\text{obs}}) d\\theta_2$$\nA more direct method is to first determine the effective likelihood for $\\theta_1$, denoted $p(y_{\\text{obs}} | \\theta_1)$. From the model $y = \\theta_1 + \\theta_2 + \\varepsilon$, we can write $y | \\theta_1$ as a random variable determined by the distributions of $\\theta_2$ and $\\varepsilon$. Given $\\theta_1$, the quantity $y$ is the sum of a constant $\\theta_1$ and two independent random variables: $\\theta_2 \\sim \\mathcal{N}(\\mu_2, \\tau_2^2)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The sum of independent normal random variables is also a normal random variable.\nThe expectation of $y$ given $\\theta_1$ is:\n$$E[y | \\theta_1] = E[\\theta_1 + \\theta_2 + \\varepsilon | \\theta_1] = \\theta_1 + E[\\theta_2] + E[\\varepsilon] = \\theta_1 + \\mu_2 + 0 = \\theta_1 + \\mu_2$$\nThe variance of $y$ given $\\theta_1$ is:\n$$\\text{Var}(y | \\theta_1) = \\text{Var}(\\theta_1 + \\theta_2 + \\varepsilon | \\theta_1) = \\text{Var}(\\theta_2) + \\text{Var}(\\varepsilon) = \\tau_2^2 + \\sigma^2$$\nTherefore, the distribution of $y$ conditioned on $\\theta_1$ is normal:\n$$y | \\theta_1 \\sim \\mathcal{N}(\\theta_1 + \\mu_2, \\sigma^2 + \\tau_2^2)$$\nThis gives the effective likelihood for $\\theta_1$: $p(y_{\\text{obs}}|\\theta_1) = \\mathcal{N}(y_{\\text{obs}} | \\theta_1 + \\mu_2, \\sigma^2 + \\tau_2^2)$.\n\nNow, we apply Bayes' theorem to find the posterior for $\\theta_1$:\n$$p(\\theta_1 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1) p(\\theta_1)$$\nWith the flat prior $p(\\theta_1) \\propto 1$, the posterior is proportional to the likelihood:\n$$p(\\theta_1 | y_{\\text{obs}}) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - (\\theta_1 + \\mu_2))^2}{2(\\sigma^2 + \\tau_2^2)}\\right) = \\exp\\left(-\\frac{((y_{\\text{obs}} - \\mu_2) - \\theta_1)^2}{2(\\sigma^2 + \\tau_2^2)}\\right)$$\nThis is the kernel of a normal distribution for $\\theta_1$. By inspection, the posterior distribution for $\\theta_1$ is:\n$$\\theta_1 | y_{\\text{obs}} \\sim \\mathcal{N}(\\; y_{\\text{obs}} - \\mu_2, \\; \\sigma^2 + \\tau_2^2 \\;)$$\nWe can now evaluate each statement.\n\nA. The marginal posterior for $\\theta_1$ is Gaussian with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$; numerically, it is $\\mathcal{N}(4, 1.04)$.\n\nBased on our derivation, the marginal posterior for $\\theta_1$ is indeed a normal distribution with mean $y_{\\text{obs}} - \\mu_2$ and variance $\\sigma^2 + \\tau_2^2$.\nLet's substitute the given numerical values:\n- Mean: $E[\\theta_1 | y_{\\text{obs}}] = y_{\\text{obs}} - \\mu_2 = 10 - 6 = 4$.\n- Variance: $\\text{Var}(\\theta_1 | y_{\\text{obs}}) = \\sigma^2 + \\tau_2^2 = 1 + 0.04 = 1.04$.\nThe resulting posterior is $\\mathcal{N}(4, 1.04)$. The statement is entirely consistent with our derivation.\nVerdict: **Correct**.\n\nB. If the prior on $\\theta_2$ were also improper flat (so both $\\theta_1$ and $\\theta_2$ had flat priors), then the posterior for $\\theta_1$ given $y_{\\text{obs}}$ would be improper.\n\nLet us assume $p(\\theta_1) \\propto 1$ and $p(\\theta_2) \\propto 1$. The joint prior is $p(\\theta_1, \\theta_2) \\propto 1$.\nThe joint posterior becomes:\n$$p(\\theta_1, \\theta_2 | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2) \\propto \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nTo find the marginal posterior for $\\theta_1$, we integrate over $\\theta_2$:\n$$p(\\theta_1 | y_{\\text{obs}}) = \\int_{-\\infty}^{\\infty} p(\\theta_1, \\theta_2 | y_{\\text{obs}}) d\\theta_2 \\propto \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right) d\\theta_2$$\nLet $z = \\theta_2$. The integrand is a Gaussian function of $z$ with mean $(y_{\\text{obs}} - \\theta_1)$ and variance related parameter $\\sigma^2$. The definite integral of an unnormalized Gaussian function over $\\mathbb{R}$ is a constant. Specifically, $\\int_{-\\infty}^{\\infty} e^{-(x-c)^2/(2s^2)} dx = \\sqrt{2\\pi s^2}$.\nIn our case, the integral evaluates to $\\sqrt{2\\pi\\sigma^2}$, which is a positive constant that does not depend on $\\theta_1$.\nThus, $p(\\theta_1 | y_{\\text{obs}}) \\propto \\text{constant}$ for all $\\theta_1 \\in \\mathbb{R}$.\nAn unnormalized density that is constant over the entire real line is an improper distribution, as its integral $\\int_{-\\infty}^{\\infty} c \\, d\\theta_1$ diverges for any constant $c > 0$. Therefore, the marginal posterior for $\\theta_1$ would be improper.\nVerdict: **Correct**.\n\nC. Decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$, and this variance cannot be reduced below $\\sigma^2$.\n\nFrom our derivation, the marginal posterior variance of $\\theta_1$ is $V_1 = \\text{Var}(\\theta_1 | y_{\\text{obs}}) = \\sigma^2 + \\tau_2^2$.\nThe parameter $\\sigma^2 = 1$ is a fixed constant from the measurement model. The parameter $\\tau_2^2$ represents the variance of our prior belief about $\\theta_2$, and must be non-negative ($\\tau_2^2 \\ge 0$).\nTo analyze the effect of $\\tau_2^2$ on $V_1$, we consider the derivative:\n$$\\frac{dV_1}{d(\\tau_2^2)} = \\frac{d}{d(\\tau_2^2)}(\\sigma^2 + \\tau_2^2) = 1$$\nSince the derivative is strictly positive, $V_1$ is a strictly increasing function of $\\tau_2^2$. Consequently, decreasing $\\tau_2^2$ strictly decreases the marginal posterior variance of $\\theta_1$. This confirms the first part of the statement.\nFor the second part, since $\\tau_2^2 \\ge 0$, the minimum possible value of the posterior variance is:\n$$V_1 = \\sigma^2 + \\tau_2^2 \\ge \\sigma^2 + 0 = \\sigma^2$$\nThe variance cannot be reduced below the measurement noise variance $\\sigma^2$. This bound is achieved in the limit as $\\tau_2^2 \\to 0$, which corresponds to a prior belief that $\\theta_2$ is known exactly. The statement is correct in its entirety.\nVerdict: **Correct**.\n\nD. Under the stated model, the maximum likelihood estimate of $\\theta_1$ equals $4$.\n\nThe Maximum Likelihood Estimate (MLE) is determined by maximizing the likelihood function $L(\\theta_1, \\theta_2; y_{\\text{obs}}) = p(y_{\\text{obs}} | \\theta_1, \\theta_2)$ with respect to the parameters. The prior distributions are not used in calculating the MLE.\nThe likelihood function is:\n$$L(\\theta_1, \\theta_2; y_{\\text{obs}}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2}{2\\sigma^2}\\right)$$\nMaximizing this function is equivalent to minimizing the squared error term in the exponent: $(y_{\\text{obs}} - \\theta_1 - \\theta_2)^2$.\nThe minimum value of this term is a global minimum of $0$, which is achieved for any pair of parameters $(\\theta_1, \\theta_2)$ that satisfies the condition:\n$$\\theta_1 + \\theta_2 = y_{\\text{obs}}$$\nGiven $y_{\\text{obs}} = 10$, any pair on the line $\\theta_1 + \\theta_2 = 10$ is an MLE. There is no unique MLE for the parameter vector $(\\theta_1, \\theta_2)$, and therefore no unique MLE for $\\theta_1$ individually. The set of values for $\\theta_1$ that maximize the likelihood is all of $\\mathbb{R}$.\nThe value $\\theta_1 = 4$ is the posterior mean, $E[\\theta_1 | y_{\\text{obs}}]$, and also the Maximum A Posteriori (MAP) estimate for $\\theta_1$, which maximizes the marginal posterior $p(\\theta_1 | y_{\\text{obs}})$. However, the question specifically asks for the MLE, which is distinct from these Bayesian estimators.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "Having seen the conceptual power of priors, we now turn to a complete hands-on implementation for a common engineering scenario: determining a material property from noisy experimental data. This practice guides you through estimating the thermal contact resistance, a key parameter in heat transfer analysis, by building a full Bayesian model from the ground up . You will construct a likelihood function based on Fourier's law, specify a prior for the resistance, and use numerical grid-based integration to compute the parameter's posterior distribution and expected value.",
            "id": "2374140",
            "problem": "Consider a one-dimensional, steady heat conduction experiment involving two solid bodies in contact. The net effect of microscopic roughness at the interface is modeled as a thermal contact resistance, denoted by $R$ in units of $\\mathrm{m^2\\,K/W}$. Under a uniform heat flux $q$ (in $\\mathrm{W/m^2}$) through the interface, the temperature drop across the interface is defined as $\\Delta T = T^{-} - T^{+}$, where $T^{-}$ and $T^{+}$ are the limiting interface temperatures on either side. Starting from Fourier’s law of heat conduction and the definition of interfacial thermal resistance, the interface jump condition is that the temperature drop across the interface satisfies $\\Delta T = q\\,R$.\n\nIn practice, measurements are noisy. Assume that the observed temperature drop $\\Delta T_i^{\\mathrm{obs}}$ for the $i$-th experimental condition obeys an additive Gaussian noise model\n$$\n\\Delta T_i^{\\mathrm{obs}} = q_i\\,R + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith known measurement noise standard deviation $\\sigma$ (in $\\mathrm{K}$). Measurements are conditionally independent given $R$. Adopt a strictly positive prior for $R$ given by a Log-Normal distribution: $R \\sim \\mathrm{LogNormal}(\\mu_0, s_0^2)$, meaning that $\\ln R \\sim \\mathcal{N}(\\mu_0, s_0^2)$. Use natural logarithms. The Log-Normal prior density is\n$$\n\\pi(R) = \\frac{1}{R\\,s_0\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln R - \\mu_0)^2}{2 s_0^2}\\right),\\quad R>0.\n$$\n\nYour task is to perform Bayesian inference to estimate $R$ from the data by computing the posterior mean\n$$\n\\mathbb{E}[R \\mid \\{\\Delta T_i^{\\mathrm{obs}}, q_i\\}_{i=1}^{n}],\n$$\nusing numerical integration over $R \\in [R_{\\min}, R_{\\max}]$ with a logarithmically spaced grid. Use Bayes’ rule to construct the unnormalized posterior density on this grid, normalize it numerically, and then compute the posterior mean by numerical quadrature. All computations must be carried out in International System of Units (SI). Express the final estimates of $R$ in $\\mathrm{m^2\\,K/W}$ as plain decimal floats.\n\nFundamental base for modeling:\n- Fourier’s law for one-dimensional steady conduction: $q = -k\\,\\mathrm{d}T/\\mathrm{d}x$.\n- Interface jump condition from the definition of contact resistance: the temperature drop is proportional to the flux with proportionality $R$, so $\\Delta T = q\\,R$.\n- Independent Gaussian measurement errors for temperature drops, leading to a Gaussian likelihood.\n\nImplementation requirements:\n- Use a logarithmically spaced grid with $R_{\\min} = 10^{-6}\\,\\mathrm{m^2\\,K/W}$ and $R_{\\max} = 10^{-2}\\,\\mathrm{m^2\\,K/W}$. The grid must be sufficiently dense to produce stable numerical integrals.\n- Compute the unnormalized log-posterior at each grid point as the sum of the log-prior and the log-likelihood, stabilize by subtracting the maximum log-posterior value before exponentiation, then normalize with a numerical quadrature rule over $R$.\n- Compute the posterior mean $\\mathbb{E}[R \\mid \\text{data}]$ as the ratio of two numerical integrals over $R$: the integral of $R$ times the posterior density divided by the integral of the posterior density.\n\nPrior parameters:\n- $\\mu_0 = \\ln(1.5\\times 10^{-4})$,\n- $s_0 = 0.5$.\n\nTest suite:\nFor each test case below, use the specified $\\{q_i\\}$ array in $\\mathrm{W/m^2}$, the observed temperature drops $\\{\\Delta T_i^{\\mathrm{obs}}\\}$ in $\\mathrm{K}$, and the noise standard deviation $\\sigma$ in $\\mathrm{K}$.\n\n- Case $1$ (well-conditioned, multiple flux levels):\n  - $q = [1.00\\times 10^{4},\\, 1.20\\times 10^{4},\\, 0.80\\times 10^{4},\\, 1.50\\times 10^{4}]$,\n  - $\\Delta T^{\\mathrm{obs}} = [2.12,\\, 2.45,\\, 1.50,\\, 3.08]$,\n  - $\\sigma = 0.10$.\n- Case $2$ (broader flux range):\n  - $q = [0.50\\times 10^{4},\\, 2.00\\times 10^{4},\\, 3.00\\times 10^{4},\\, 4.00\\times 10^{4},\\, 1.00\\times 10^{4}]$,\n  - $\\Delta T^{\\mathrm{obs}} = [0.58,\\, 2.47,\\, 3.55,\\, 4.95,\\, 1.21]$,\n  - $\\sigma = 0.10$.\n- Case $3$ (edge case: low information, single low flux, larger noise):\n  - $q = [0.20\\times 10^{4}]$,\n  - $\\Delta T^{\\mathrm{obs}} = [0.64]$,\n  - $\\sigma = 0.30$.\n\nAngle units do not apply. There are no percentages in this problem.\n\nYour program must output a single line containing the three posterior mean estimates for $R$ corresponding to the three cases, as a comma-separated Python-style list enclosed in square brackets, for example, $[r_1,r_2,r_3]$, where each $r_i$ is a float in $\\mathrm{m^2\\,K/W}$.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is a well-posed problem in Bayesian parameter estimation, grounded in the principles of heat transfer and statistical inference. The provided data and models are scientifically sound, complete, and consistent. We shall proceed with the derivation and numerical solution.\n\nThe objective is to compute the posterior mean of the thermal contact resistance, $R$, given a set of $n$ noisy measurements. The problem is defined by the following components:\n\n1.  **Physical Model:** The temperature drop $\\Delta T$ across an interface is related to the heat flux $q$ by $\\Delta T = qR$.\n2.  **Observation Model:** For the $i$-th measurement, the observed temperature drop $\\Delta T_i^{\\mathrm{obs}}$ is modeled as $\\Delta T_i^{\\mathrm{obs}} = q_i R + \\varepsilon_i$, where the noise term $\\varepsilon_i$ is drawn from a normal distribution with mean $0$ and known variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n3.  **Prior Distribution:** The parameter $R$ is assumed to be strictly positive. A Log-Normal prior is assigned: $R \\sim \\mathrm{LogNormal}(\\mu_0, s_0^2)$, which implies $\\ln R \\sim \\mathcal{N}(\\mu_0, s_0^2)$.\n\nLet the collected data be denoted by $\\mathcal{D} = \\{\\Delta T_i^{\\mathrm{obs}}, q_i\\}_{i=1}^{n}$. Our goal is to compute the posterior mean of $R$:\n$$\n\\mathbb{E}[R \\mid \\mathcal{D}] = \\int_0^\\infty R \\, p(R \\mid \\mathcal{D}) \\, dR\n$$\nwhere $p(R \\mid \\mathcal{D})$ is the posterior probability density function of $R$.\n\nAccording to Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior density:\n$$\np(R \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid R) \\, \\pi(R)\n$$\nwhere $p(\\mathcal{D} \\mid R)$ is the likelihood and $\\pi(R)$ is the prior.\n\n**Likelihood Function**\nFrom the observation model, each measurement $\\Delta T_i^{\\mathrm{obs}}$ is independently drawn from a normal distribution $\\mathcal{N}(q_i R, \\sigma^2)$. The probability density for a single observation is:\n$$\np(\\Delta T_i^{\\mathrm{obs}} \\mid R) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{(\\Delta T_i^{\\mathrm{obs}} - q_i R)^2}{2\\sigma^2} \\right)\n$$\nSince the measurements are conditionally independent given $R$, the total likelihood for the dataset $\\mathcal{D}$ is the product of the individual densities:\n$$\np(\\mathcal{D} \\mid R) = \\prod_{i=1}^n p(\\Delta T_i^{\\mathrm{obs}} \\mid R)\n$$\nFor numerical stability, we work with the log-likelihood. Up to an additive constant that is independent of $R$, the log-likelihood is:\n$$\n\\ln p(\\mathcal{D} \\mid R) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\Delta T_i^{\\mathrm{obs}} - q_i R)^2\n$$\n\n**Prior Distribution**\nThe prior for $R$ is a Log-Normal distribution with parameters $\\mu_0$ and $s_0^2$, given by the density:\n$$\n\\pi(R) = \\frac{1}{R\\,s_0\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln R - \\mu_0)^2}{2 s_0^2}\\right) \\quad \\text{for } R > 0\n$$\nThe log-prior, up to an additive constant, is:\n$$\n\\ln \\pi(R) \\propto -\\ln R - \\frac{(\\ln R - \\mu_0)^2}{2s_0^2}\n$$\n\n**Posterior Distribution**\nThe unnormalized log-posterior is the sum of the log-likelihood and the log-prior:\n$$\n\\ln p(R \\mid \\mathcal{D}) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\Delta T_i^{\\mathrm{obs}} - q_i R)^2 - \\ln R - \\frac{(\\ln R - \\mu_0)^2}{2s_0^2}\n$$\n\n**Numerical Computation**\nThe posterior mean is computed by numerical quadrature over a discretized grid for $R$. We define a grid of points $\\{R_j\\}$ spanning the interval $[R_{\\min}, R_{\\max}]$. The problem specifies a logarithmically spaced grid.\n\nThe posterior mean is the ratio of two integrals:\n$$\n\\mathbb{E}[R \\mid \\mathcal{D}] = \\frac{\\int_{R_{\\min}}^{R_{\\max}} R \\, p_{\\text{unnorm}}(R \\mid \\mathcal{D}) \\, dR}{\\int_{R_{\\min}}^{R_{\\max}} p_{\\text{unnorm}}(R \\mid \\mathcal{D}) \\, dR}\n$$\nwhere $p_{\\text{unnorm}}(R \\mid \\mathcal{D}) = \\exp(\\ln p_{\\text{unnorm}}(R \\mid \\mathcal{D}))$ is the unnormalized posterior density.\n\nThe algorithmic steps are as follows:\n1.  Define a logarithmically spaced grid of $N_{grid}$ points, $\\{R_j\\}$, from $R_{\\min} = 10^{-6}$ to $R_{\\max} = 10^{-2}$.\n2.  For computational efficiency, pre-compute the sums needed for the likelihood term: $S_{qq} = \\sum_{i=1}^n q_i^2$, $S_{qT} = \\sum_{i=1}^n q_i \\Delta T_i^{\\mathrm{obs}}$, and $S_{TT} = \\sum_{i=1}^n (\\Delta T_i^{\\mathrm{obs}})^2$. The log-likelihood term becomes $-\\frac{1}{2\\sigma^2}(S_{TT} - 2R S_{qT} + R^2 S_{qq})$.\n3.  Evaluate the unnormalized log-posterior, $\\mathcal{L}(R_j)$, at each grid point $R_j$.\n4.  To prevent numerical underflow during exponentiation, stabilize the log-posterior by subtracting its maximum value: $\\mathcal{L}_{\\text{stab}}(R_j) = \\mathcal{L}(R_j) - \\max_j\\{\\mathcal{L}(R_j)\\}$.\n5.  Compute the unnormalized posterior values on the grid: $p_{\\text{unnorm}}(R_j) = \\exp(\\mathcal{L}_{\\text{stab}}(R_j))$.\n6.  Approximate the integrals for the numerator and the denominator (normalization constant) using the trapezoidal rule over the non-uniform grid $\\{R_j\\}$. Let $P_j = p_{\\text{unnorm}}(R_j)$.\n    - Normalization constant: $Z = \\int p_{\\text{unnorm}}(R) \\, dR \\approx \\text{trapz}(\\{P_j\\}, \\{R_j\\})$.\n    - Numerator integral: $N = \\int R \\, p_{\\text{unnorm}}(R) \\, dR \\approx \\text{trapz}(\\{R_j \\cdot P_j\\}, \\{R_j\\})$.\n7.  The posterior mean is then calculated as $\\mathbb{E}[R \\mid \\mathcal{D}] \\approx N/Z$.\n\nThis procedure is applied to each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import trapezoid\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian inference problem for all test cases.\n    \"\"\"\n    # Prior parameters\n    mu_0 = np.log(1.5e-4) # mu_0 = ln(1.5 * 10^-4)\n    s_0 = 0.5            # s_0 = 0.5\n\n    # Numerical integration parameters\n    R_min = 1e-6         # R_min = 10^-6 m^2K/W\n    R_max = 1e-2         # R_max = 10^-2 m^2K/W\n    N_grid = 20001       # Number of grid points for integration\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"q\": np.array([1.00e4, 1.20e4, 0.80e4, 1.50e4]), # W/m^2\n            \"delta_T_obs\": np.array([2.12, 2.45, 1.50, 3.08]), # K\n            \"sigma\": 0.10, # K\n        },\n        {\n            \"q\": np.array([0.50e4, 2.00e4, 3.00e4, 4.00e4, 1.00e4]), # W/m^2\n            \"delta_T_obs\": np.array([0.58, 2.47, 3.55, 4.95, 1.21]), # K\n            \"sigma\": 0.10, # K\n        },\n        {\n            \"q\": np.array([0.20e4]), # W/m^2\n            \"delta_T_obs\": np.array([0.64]), # K\n            \"sigma\": 0.30, # K\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_posterior_mean(\n            case[\"q\"],\n            case[\"delta_T_obs\"],\n            case[\"sigma\"],\n            mu_0,\n            s_0,\n            R_min,\n            R_max,\n            N_grid\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\ndef compute_posterior_mean(q, delta_T_obs, sigma, mu_0, s_0, R_min, R_max, N_grid):\n    \"\"\"\n    Computes the posterior mean of R using numerical integration.\n\n    Args:\n        q (np.ndarray): Array of heat flux values [W/m^2].\n        delta_T_obs (np.ndarray): Array of observed temperature drops [K].\n        sigma (float): Standard deviation of measurement noise [K].\n        mu_0 (float): Mean of the logarithm of R for the Log-Normal prior.\n        s_0 (float): Standard deviation of the logarithm of R for the Log-Normal prior.\n        R_min (float): Minimum value for the integration grid for R [m^2K/W].\n        R_max (float): Maximum value for the integration grid for R [m^2K/W].\n        N_grid (int): Number of points in the integration grid.\n\n    Returns:\n        float: The posterior mean of R [m^2K/W].\n    \"\"\"\n    # 1. Create a logarithmically spaced grid for R.\n    R_grid = np.logspace(np.log10(R_min), np.log10(R_max), N_grid)\n\n    # 2. Compute log-likelihood.\n    # Pre-calculate summary statistics for efficiency.\n    S_qq = np.sum(q**2)\n    S_qT = np.sum(q * delta_T_obs)\n    S_TT = np.sum(delta_T_obs**2)\n    \n    # The term in the likelihood sum is a quadratic in R: S_TT - 2*R*S_qT + R^2*S_qq\n    likelihood_quadratic = S_TT - 2 * R_grid * S_qT + R_grid**2 * S_qq\n    log_likelihood = -0.5 / (sigma**2) * likelihood_quadratic\n\n    # 3. Compute log-prior.\n    log_R_grid = np.log(R_grid)\n    log_prior = -log_R_grid - (log_R_grid - mu_0)**2 / (2 * s_0**2)\n\n    # 4. Compute unnormalized log-posterior.\n    log_posterior = log_likelihood + log_prior\n    \n    # 5. Stabilize log-posterior to avoid numerical underflow.\n    log_posterior_stable = log_posterior - np.max(log_posterior)\n    \n    # 6. Exponentiate to get unnormalized posterior.\n    posterior_unnorm = np.exp(log_posterior_stable)\n\n    # 7. Compute integrals using the trapezoidal rule.\n    # Numerator integral: integral of R * posterior(R) dR\n    numerator_integral = trapezoid(R_grid * posterior_unnorm, R_grid)\n    \n    # Denominator integral (normalization constant): integral of posterior(R) dR\n    norm_constant = trapezoid(posterior_unnorm, R_grid)\n\n    # 8. Compute the posterior mean.\n    posterior_mean = numerator_integral / norm_constant\n\n    return posterior_mean\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data is rarely as clean as we would like, and a few anomalous data points, or outliers, can disproportionately influence our parameter estimates. This exercise addresses a critical aspect of statistical modeling: the choice of the likelihood function and its impact on robustness . By comparing the posterior distribution resulting from a standard Gaussian likelihood with one from a heavy-tailed Student's $t$-distribution, you will gain practical insight into building models that are resilient to imperfect data.",
            "id": "2374122",
            "problem": "A scalar parameter $\\theta$ represents a constant bias in a sensor model used in computational engineering. You observe $n$ independent and identically distributed (i.i.d.) measurements $y_1,\\dots,y_n$ of the form $y_i = \\theta + \\epsilon_i$. Consider two competing noise models for the measurement error $\\epsilon_i$ and a common prior for $\\theta$:\n- Prior: $\\theta \\sim \\mathcal{N}(0,1)$.\n- Gaussian likelihood: $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^2)$ with known $\\sigma^2 = 0.01$.\n- Student’s t-likelihood: $y_i \\mid \\theta \\sim t_\\nu(\\theta,s)$ with known degrees of freedom $\\nu = 3$ and scale $s = 0.1$.\n\nYou collect $n=6$ measurements\n$$\ny = \\{0.1,\\,0.2,\\,-0.1,\\,0.2,\\,0.0,\\,5.0\\},\n$$\nwhere the last entry is visibly an outlier relative to the others.\n\nLet $p_{\\mathcal{N}}(\\theta \\mid y)$ denote the posterior density of $\\theta$ under the Gaussian likelihood and $p_{t}(\\theta \\mid y)$ the posterior density under the Student’s t-likelihood, both using the same prior stated above. Which of the following statements are correct?\n\nA. Under the stated models and data, both $p_{\\mathcal{N}}(\\theta \\mid y)$ and $p_{t}(\\theta \\mid y)$ are exactly Gaussian with the same mean and variance.\n\nB. For small $\\nu$ in the Student’s t-likelihood (such as $\\nu = 3$), $p_{t}(\\theta \\mid y)$ is less sensitive to the outlier than $p_{\\mathcal{N}}(\\theta \\mid y)$, placing its bulk closer to the central cluster of the non-outlying observations and exhibiting heavier tails than $p_{\\mathcal{N}}(\\theta \\mid y)$.\n\nC. For fixed scale parameters $\\sigma$ and $s$ as given, the mean (or any reasonable central tendency) of $p_{\\mathcal{N}}(\\theta \\mid y)$ is pulled farther toward the outlier than that of $p_{t}(\\theta \\mid y)$ for the provided data.\n\nD. As $\\nu \\to \\infty$ with $s$ held fixed, $p_{t}(\\theta \\mid y)$ converges to $p_{\\mathcal{N}}(\\theta \\mid y)$ under the same prior and data.\n\nE. For the provided data and prior, using a Student’s t-likelihood with small $\\nu$ necessarily yields a posterior variance for $\\theta$ that is smaller than that obtained with the Gaussian likelihood.\n\nF. With the Student’s t-likelihood and small $\\nu$, the outlier is completely ignored, so $p_{t}(\\theta \\mid y)$ is identical to the posterior one would obtain by removing the outlying observation and analyzing the remaining five points under the Student’s t-likelihood.",
            "solution": "The user has provided a Bayesian inference problem and asks for an evaluation of several statements comparing posteriors from a Gaussian likelihood and a Student's t-likelihood, especially in the context of outlier data.\n\nThe first step is to validate the problem statement.\n\n**Problem Validation**\n\nGivens extracted from the problem statement:\n-   A scalar parameter to be estimated is $\\theta$.\n-   The data generation model is $y_i = \\theta + \\epsilon_i$ for $n$ independent and identically distributed (i.i.d.) measurements.\n-   The prior distribution for the parameter is Gaussian: $\\theta \\sim \\mathcal{N}(0,1)$, thus $p(\\theta) = \\mathcal{N}(\\theta \\mid \\mu_0=0, \\sigma_0^2=1)$.\n-   The first likelihood model is Gaussian: $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)$ with a known variance $\\sigma^2 = 0.01$. This means $p(y_i \\mid \\theta) = \\mathcal{N}(y_i \\mid \\theta, 0.01)$.\n-   The second likelihood model is a location-scale Student's t-distribution: $y_i \\mid \\theta \\sim t_\\nu(\\theta, s)$ with known degrees of freedom $\\nu = 3$ and scale parameter $s = 0.1$.\n-   The observed data consists of $n=6$ measurements: $y = \\{0.1, 0.2, -0.1, 0.2, 0.0, 5.0\\}$.\n-   The posterior density under the Gaussian likelihood is denoted $p_{\\mathcal{N}}(\\theta \\mid y)$.\n-   The posterior density under the Student's t-likelihood is denoted $p_{t}(\\theta \\mid y)$.\n\nValidation Assessment:\nThe problem is scientifically grounded, utilizing standard statistical distributions (Gaussian, Student's t) and the principles of Bayesian inference. It is a well-posed problem, as all necessary components (prior, likelihoods, data) are fully specified, allowing for the unique determination of the posterior distributions. The language is objective and precise. The problem is self-contained, consistent, and directly relevant to the specified topic of Bayesian inference in computational engineering. The problem is therefore deemed **valid**.\n\n**Derivation and Analysis**\n\nThe posterior distribution is given by Bayes' theorem: $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$, where $p(y \\mid \\theta) = \\prod_{i=1}^n p(y_i \\mid \\theta)$.\n\n**1. Posterior under the Gaussian Likelihood: $p_{\\mathcal{N}}(\\theta \\mid y)$**\n\nFor a Gaussian prior $p(\\theta) = \\mathcal{N}(\\theta \\mid \\mu_0, \\sigma_0^2)$ and a Gaussian likelihood $p(y_i \\mid \\theta) = \\mathcal{N}(y_i \\mid \\theta, \\sigma^2)$, the posterior distribution is also Gaussian, $p_{\\mathcal{N}}(\\theta \\mid y) = \\mathcal{N}(\\theta \\mid \\mu_n, \\sigma_n^2)$. The posterior parameters are found by a precision-weighted update:\n$$ \\frac{1}{\\sigma_n^2} = \\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma^2} $$\n$$ \\mu_n = \\sigma_n^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{n\\bar{y}}{\\sigma^2} \\right) $$\nWith the given values, $n=6$, $\\mu_0=0$, $\\sigma_0^2=1$, and $\\sigma^2=0.01$. The data sum is $\\sum_{i=1}^6 y_i = 0.1+0.2-0.1+0.2+0.0+5.0 = 5.4$, so the sample mean is $\\bar{y} = 5.4/6 = 0.9$.\n\nThe posterior precision is $\\frac{1}{\\sigma_n^2} = \\frac{1}{1} + \\frac{6}{0.01} = 1 + 600 = 601$.\nThe posterior variance is $\\sigma_n^2 = \\frac{1}{601}$.\n\nThe posterior mean is $\\mu_n = \\frac{1}{601} \\left( \\frac{0}{1} + \\frac{6 \\times 0.9}{0.01} \\right) = \\frac{1}{601} \\left( 540 \\right) = \\frac{540}{601} \\approx 0.8985$.\nThe center of this posterior is strongly influenced by the outlier $y_6=5.0$, pulling it far from the cluster of the first five points, which have a mean of $0.08$.\n\n**2. Posterior under the Student's t-Likelihood: $p_{t}(\\theta \\mid y)$**\n\nThe posterior is $p_{t}(\\theta \\mid y) \\propto p(\\theta) \\prod_{i=1}^n p_t(y_i \\mid \\theta)$, where $p_t$ denotes the Student's t-density.\n$$ p_{t}(\\theta \\mid y) \\propto \\exp\\left(-\\frac{\\theta^2}{2}\\right) \\prod_{i=1}^{n} \\left(1 + \\frac{(y_i - \\theta)^2}{\\nu s^2}\\right)^{-(\\nu+1)/2} $$\nWith $\\nu=3$ and $s=0.1$, this becomes:\n$$ p_{t}(\\theta \\mid y) \\propto \\exp\\left(-\\frac{\\theta^2}{2}\\right) \\prod_{i=1}^{6} \\left(1 + \\frac{(y_i - \\theta)^2}{0.03}\\right)^{-2} $$\nThis distribution is not analytically simple. Its key characteristic is robustness to outliers. The log-likelihood penalty for a large residual $|y_i - \\theta|$ scales as $\\log|y_i - \\theta|$, unlike the Gaussian likelihood's quadratic penalty $\\propto (y_i - \\theta)^2$. This weaker penalty means outliers have a much smaller influence on the posterior's location.\n\n**Evaluation of Options**\n\n**A. Under the stated models and data, both $p_{\\mathcal{N}}(\\theta \\mid y)$ and $p_{t}(\\theta \\mid y)$ are exactly Gaussian with the same mean and variance.**\n$p_{\\mathcal{N}}(\\theta \\mid y)$ is indeed Gaussian. However, $p_{t}(\\theta \\mid y)$ is the product of a Gaussian density and a product of Student's t-densities. The result is not a Gaussian distribution. This statement is fundamentally incorrect.\n**Verdict: Incorrect.**\n\n**B. For small $\\nu$ in the Student’s t-likelihood (such as $\\nu = 3$), $p_{t}(\\theta \\mid y)$ is less sensitive to the outlier than $p_{\\mathcal{N}}(\\theta \\mid y)$, placing its bulk closer to the central cluster of the non-outlying observations and exhibiting heavier tails than $p_{\\mathcal{N}}(\\theta \\mid y)$.**\nThis statement correctly captures the essential properties of using a t-likelihood.\n1.  **Less sensitive to the outlier**: Correct. This is due to the heavy tails of the t-distribution, which penalize large residuals less severely than a Gaussian distribution.\n2.  **Placing its bulk closer to the central cluster**: Correct. As a consequence of robustness, the posterior location parameter is not dragged towards the outlier at $y_6=5.0$. Its center will be much closer to the mean of the first five points ($\\approx 0.08$) than the Gaussian posterior mean ($\\approx 0.9$).\n3.  **Exhibiting heavier tails**: Correct. The tails of a posterior are determined by the asymptotic decay rate. For large $|\\theta|$, the log-density of $p_{\\mathcal{N}}(\\theta \\mid y)$ behaves as $-\\frac{1}{2}(\\frac{1}{\\sigma_0^2}+\\frac{n}{\\sigma^2})\\theta^2 = -\\frac{601}{2}\\theta^2$. The log-density of $p_{t}(\\theta \\mid y)$ behaves as $-\\frac{1}{2\\sigma_0^2}\\theta^2 - n(\\nu+1)\\log|\\theta|$, which is dominated by the prior's quadratic term $-\\frac{1}{2}\\theta^2$. Because $\\exp(-\\frac{1}{2}\\theta^2)$ decays much more slowly than $\\exp(-\\frac{601}{2}\\theta^2)$, the posterior $p_{t}(\\theta \\mid y)$ has much heavier tails than $p_{\\mathcal{N}}(\\theta \\mid y)$.\n**Verdict: Correct.**\n\n**C. For fixed scale parameters $\\sigma$ and $s$ as given, the mean (or any reasonable central tendency) of $p_{\\mathcal{N}}(\\theta \\mid y)$ is pulled farther toward the outlier than that of $p_{t}(\\theta \\mid y)$ for the provided data.**\nThis is a direct and accurate statement of the practical difference between the two models in this scenario. The Gaussian likelihood's sensitivity to the outlier at $y_6=5.0$ pulls its posterior mean significantly towards that value. The robust t-likelihood resists this pull. The statement is correct.\n**Verdict: Correct.**\n\n**D. As $\\nu \\to \\infty$ with $s$ held fixed, $p_{t}(\\theta \\mid y)$ converges to $p_{\\mathcal{N}}(\\theta \\mid y)$ under the same prior and data.**\nThis statement concerns a fundamental property of the Student's t-distribution. As the degrees of freedom $\\nu \\to \\infty$, the t-distribution $t_\\nu(\\mu, s)$ converges to a Gaussian distribution $\\mathcal{N}(\\mu, s^2)$. In this problem, the t-likelihood has scale $s=0.1$, so $s^2=0.01$. The Gaussian likelihood has variance $\\sigma^2=0.01$. Since $s^2 = \\sigma^2$, the t-likelihood indeed converges to the specified Gaussian likelihood. As the likelihoods converge and the prior is identical, the posteriors must also converge. The statement is correct.\n**Verdict: Correct.**\n\n**E. For the provided data and prior, using a Student’s t-likelihood with small $\\nu$ necessarily yields a posterior variance for $\\theta$ that is smaller than that obtained with the Gaussian likelihood.**\nThis is incorrect. The Gaussian model, by incorporating all $6$ data points with high confidence, produces a very precise posterior with a small variance of $\\sigma_n^2 = 1/601$. The Student's t-model, by down-weighting the information from the outlier, is effectively using less data to estimate $\\theta$. A reduction in the amount of effective information leads to an increase in posterior uncertainty, i.e., a larger variance. Robustness is gained at the cost of higher variance. Therefore, the posterior variance from the t-likelihood will be larger, not smaller.\n**Verdict: Incorrect.**\n\n**F. With the Student’s t-likelihood and small $\\nu$, the outlier is completely ignored, so $p_{t}(\\theta \\mid y)$ is identical to the posterior one would obtain by removing the outlying observation and analyzing the remaining five points under the Student’s t-likelihood.**\nThe term \"completely ignored\" is an overstatement and mathematically incorrect. An outlier is *down-weighted*, meaning its influence is reduced, but not eliminated. The likelihood term corresponding to the outlier, while small for values of $\\theta$ near the main cluster of data, is still a function of $\\theta$ and part of the overall posterior product. Removing the data point is equivalent to replacing this term with a constant, which is not what happens. The posterior is therefore not identical to one computed on the subset of data.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{BCD}$$"
        }
    ]
}