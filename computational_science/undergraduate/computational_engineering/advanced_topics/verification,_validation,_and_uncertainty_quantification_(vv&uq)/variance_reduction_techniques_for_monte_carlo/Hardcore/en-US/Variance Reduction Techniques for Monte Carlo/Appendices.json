{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational dive into the mechanics of control variates. You will analytically derive the exact variance reduction achieved by using an optimally chosen control variate, providing a clear, quantitative understanding of its effectiveness. This practice  builds the theoretical muscle needed to reason about why and how variance reduction techniques work.",
            "id": "2449189",
            "problem": "Let $X$ be a normally distributed random variable with mean $\\,\\mu\\,$ and variance $\\,\\sigma^{2}\\,$, where $\\,\\mu \\neq 0\\,$ and $\\,\\sigma^{2} > 0\\,$. The goal is to estimate $\\,\\theta = \\mathbb{E}[X^{2}]\\,$ via Monte Carlo using $\\,n\\,$ independent and identically distributed (i.i.d.) samples $\\,X_{1},\\dots,X_{n}\\,$.\n\nConsider the following unbiased estimators of $\\,\\theta\\,$:\n- The direct estimator $\\,\\hat{\\theta}_{\\mathrm{dir}} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}\\,$.\n- The control variate (CV) estimator $\\,\\hat{\\theta}_{\\mathrm{cv}} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(X_{i}^{2} - \\beta\\,(X_{i}-\\mu)\\right)\\,$, where $\\,\\mu = \\mathbb{E}[X]\\,$ is known and the coefficient $\\,\\beta\\,$ is chosen to minimize the estimator variance.\n\nDerive the exact ratio $\\,\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})}\\,$ in closed form as a simplified expression in terms of $\\,\\mu\\,$ and $\\,\\sigma^{2}\\,$ only (it must not depend on $\\,n\\,$). Provide your final answer as a single analytic expression. Do not approximate.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The random variable $X$ is normally distributed with mean $\\mu$ and variance $\\sigma^{2}$.\n- Constraints: $\\mu \\neq 0$ and $\\sigma^{2} > 0$.\n- The parameter to be estimated is $\\theta = \\mathbb{E}[X^{2}]$.\n- The estimation is performed using $n$ independent and identically distributed (i.i.d.) samples $X_{1}, \\dots, X_{n}$.\n- The direct estimator is $\\hat{\\theta}_{\\mathrm{dir}} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}$.\n- The control variate (CV) estimator is $\\hat{\\theta}_{\\mathrm{cv}} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(X_{i}^{2} - \\beta\\,(X_{i}-\\mu)\\right)$.\n- The mean $\\mu = \\mathbb{E}[X]$ is known.\n- The coefficient $\\beta$ is chosen to minimize $\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})$.\n- The objective is to derive the exact ratio $\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})}$ as a simplified expression in terms of $\\mu$ and $\\sigma^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in computational statistics, dealing with variance reduction for Monte Carlo estimators using control variates. All concepts—normal distribution, expectation, variance, and estimators—are fundamentally sound.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information to determine the optimal control variate coefficient $\\beta$ and subsequently calculate the required variance ratio. The existence and uniqueness of the solution are guaranteed by the principles of linear regression and variance minimization.\n- **Objective:** The problem is stated using precise, objective mathematical language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard, well-defined problem in computational engineering and statistics. I will proceed with the derivation.\n\nThe objective is to compute the ratio of the variances of two estimators for $\\theta = \\mathbb{E}[X^2]$.\nThe direct estimator is $\\hat{\\theta}_{\\mathrm{dir}} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}$. Since the samples $X_i$ are i.i.d., its variance is:\n$$\n\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} X_{i}^{2}\\right) = \\frac{1}{n^{2}}\\sum_{i=1}^{n} \\mathrm{Var}(X_{i}^{2}) = \\frac{1}{n}\\mathrm{Var}(X^{2})\n$$\nThe control variate estimator is $\\hat{\\theta}_{\\mathrm{cv}} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}$, where $Y_{i} = X_{i}^{2} - \\beta(X_{i}-\\mu)$. Its variance is:\n$$\n\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}}) = \\frac{1}{n}\\mathrm{Var}(Y_1) = \\frac{1}{n}\\mathrm{Var}(X^{2} - \\beta(X-\\mu))\n$$\nThe ratio to be computed is therefore:\n$$\n\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})} = \\frac{\\frac{1}{n}\\mathrm{Var}(X^{2} - \\beta(X-\\mu))}{\\frac{1}{n}\\mathrm{Var}(X^{2})} = \\frac{\\mathrm{Var}(X^{2} - \\beta(X-\\mu))}{\\mathrm{Var}(X^{2})}\n$$\nThe coefficient $\\beta$ must be chosen to minimize $\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})$. The optimal coefficient, denoted $\\beta^{*}$, is given by the well-known formula:\n$$\n\\beta^{*} = \\frac{\\mathrm{Cov}(X^{2}, X-\\mu)}{\\mathrm{Var}(X-\\mu)}\n$$\nWe must compute the components of this expression. The denominator is simply $\\mathrm{Var}(X-\\mu) = \\mathrm{Var}(X) = \\sigma^{2}$.\nFor the numerator, we compute the covariance:\n$$\n\\mathrm{Cov}(X^{2}, X-\\mu) = \\mathbb{E}[X^{2}(X-\\mu)] - \\mathbb{E}[X^{2}]\\mathbb{E}[X-\\mu]\n$$\nSince $\\mathbb{E}[X-\\mu] = \\mathbb{E}[X] - \\mu = \\mu - \\mu = 0$, the second term vanishes.\n$$\n\\mathrm{Cov}(X^{2}, X-\\mu) = \\mathbb{E}[X^{3} - \\mu X^{2}] = \\mathbb{E}[X^{3}] - \\mu \\mathbb{E}[X^{2}]\n$$\nTo compute these moments, let us define a standard normal variable $Z = \\frac{X-\\mu}{\\sigma}$, so $X = \\mu + \\sigma Z$. The moments of $Z$ are $\\mathbb{E}[Z]=0$, $\\mathbb{E}[Z^2]=1$, $\\mathbb{E}[Z^3]=0$, and $\\mathbb{E}[Z^4]=3$.\nThe second moment of $X$ is $\\mathbb{E}[X^{2}] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^{2} = \\sigma^{2} + \\mu^{2}$.\nThe third moment of $X$ is:\n$$\n\\mathbb{E}[X^{3}] = \\mathbb{E}[(\\mu + \\sigma Z)^{3}] = \\mathbb{E}[\\mu^{3} + 3\\mu^{2}\\sigma Z + 3\\mu\\sigma^{2}Z^{2} + \\sigma^{3}Z^{3}]\n$$\nUsing linearity of expectation and the moments of $Z$:\n$$\n\\mathbb{E}[X^{3}] = \\mu^{3} + 3\\mu^{2}\\sigma\\mathbb{E}[Z] + 3\\mu\\sigma^{2}\\mathbb{E}[Z^{2}] + \\sigma^{3}\\mathbb{E}[Z^{3}] = \\mu^{3} + 0 + 3\\mu\\sigma^{2}(1) + 0 = \\mu^{3} + 3\\mu\\sigma^{2}\n$$\nNow, substitute the moments back into the covariance expression:\n$$\n\\mathrm{Cov}(X^{2}, X-\\mu) = (\\mu^{3} + 3\\mu\\sigma^{2}) - \\mu(\\sigma^{2} + \\mu^{2}) = \\mu^{3} + 3\\mu\\sigma^{2} - \\mu\\sigma^{2} - \\mu^{3} = 2\\mu\\sigma^{2}\n$$\nThe optimal coefficient is therefore:\n$$\n\\beta^{*} = \\frac{2\\mu\\sigma^{2}}{\\sigma^{2}} = 2\\mu\n$$\nWith the optimal $\\beta^{*}$, the variance of the control variate term is given by:\n$$\n\\mathrm{Var}(X^{2} - \\beta^{*}(X-\\mu)) = \\mathrm{Var}(X^{2}) - \\frac{(\\mathrm{Cov}(X^{2}, X-\\mu))^{2}}{\\mathrm{Var}(X-\\mu)}\n$$\nWe need to compute $\\mathrm{Var}(X^{2})$.\n$$\n\\mathrm{Var}(X^{2}) = \\mathbb{E}[X^{4}] - (\\mathbb{E}[X^{2}])^{2}\n$$\nFirst, calculate the fourth moment of $X$:\n$$\n\\mathbb{E}[X^{4}] = \\mathbb{E}[(\\mu + \\sigma Z)^{4}] = \\mathbb{E}[\\mu^{4} + 4\\mu^{3}\\sigma Z + 6\\mu^{2}\\sigma^{2}Z^{2} + 4\\mu\\sigma^{3}Z^{3} + \\sigma^{4}Z^{4}]\n$$\n$$\n\\mathbb{E}[X^{4}] = \\mu^{4} + 4\\mu^{3}\\sigma\\mathbb{E}[Z] + 6\\mu^{2}\\sigma^{2}\\mathbb{E}[Z^{2}] + 4\\mu\\sigma^{3}\\mathbb{E}[Z^{3}] + \\sigma^{4}\\mathbb{E}[Z^{4}]\n$$\n$$\n\\mathbb{E}[X^{4}] = \\mu^{4} + 0 + 6\\mu^{2}\\sigma^{2}(1) + 0 + \\sigma^{4}(3) = \\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}\n$$\nNow we can compute $\\mathrm{Var}(X^{2})$:\n$$\n\\mathrm{Var}(X^{2}) = (\\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}) - (\\sigma^{2} + \\mu^{2})^{2}\n$$\n$$\n\\mathrm{Var}(X^{2}) = (\\mu^{4} + 6\\mu^{2}\\sigma^{2} + 3\\sigma^{4}) - (\\sigma^{4} + 2\\mu^{2}\\sigma^{2} + \\mu^{4}) = 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}\n$$\nWith this result, we find the minimized variance for the control variate term:\n$$\n\\mathrm{Var}(X^{2} - \\beta^{*}(X-\\mu)) = (4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}) - \\frac{(2\\mu\\sigma^{2})^{2}}{\\sigma^{2}}\n$$\n$$\n= (4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}) - \\frac{4\\mu^{2}\\sigma^{4}}{\\sigma^{2}} = 4\\mu^{2}\\sigma^{2} + 2\\sigma^{4} - 4\\mu^{2}\\sigma^{2} = 2\\sigma^{4}\n$$\nFinally, we assemble the ratio of variances:\n$$\n\\frac{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{cv}})}{\\mathrm{Var}(\\hat{\\theta}_{\\mathrm{dir}})} = \\frac{\\mathrm{Var}(X^{2} - \\beta^{*}(X-\\mu))}{\\mathrm{Var}(X^{2})} = \\frac{2\\sigma^{4}}{4\\mu^{2}\\sigma^{2} + 2\\sigma^{4}}\n$$\nSince $\\sigma^{2} > 0$, we can simplify the expression by dividing the numerator and denominator by $2\\sigma^{2}$:\n$$\n\\frac{2\\sigma^{4}}{2\\sigma^{2}(2\\mu^{2} + \\sigma^{2})} = \\frac{\\sigma^{2}}{2\\mu^{2} + \\sigma^{2}}\n$$\nThis is the final, simplified expression for the variance ratio. It depends only on $\\mu$ and $\\sigma^{2}$, as required.",
            "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{2\\mu^{2} + \\sigma^{2}}}\n$$"
        },
        {
            "introduction": "Moving from theory to computational practice, this exercise demonstrates the impressive power of the Common Random Numbers (CRN) technique. In a practical financial engineering context, you will code a simulation to directly compare the estimator variance with and without CRN. This challenge  makes the abstract concept of induced correlation tangible, showcasing how a simple design choice can yield dramatic improvements in efficiency.",
            "id": "3005265",
            "problem": "Let a geometric Brownian motion (GBM) under the risk-neutral probability measure be defined by the stochastic differential equation $dS_t = r S_t \\, dt + \\sigma S_t \\, dW_t$, where $S_t$ is the asset price at time $t$, $r$ is the continuously compounded interest rate, $\\sigma$ is the volatility, and $W_t$ is a standard Brownian motion. Consider a European call option with strike $K$ and maturity $T$ whose discounted payoff at time $T$ under a given interest rate $r$ is $e^{-rT} \\max(S_T - K, 0)$.\n\nYour task is to construct a comparative Monte Carlo estimator using the variance reduction technique known as common random numbers to estimate the variance of the paired pathwise difference in discounted call payoffs at two different interest rates. Specifically, for a fixed sample size $N$ and a specified integer seed $s$, generate a single set of $N$ standard normal random variables and use it to drive both terminal values $S_T^{(r_1)}$ and $S_T^{(r_2)}$ of the GBM at interest rates $r_1$ and $r_2$, respectively. For each outcome, compute the discounted call payoffs $C^{(r_1)}$ and $C^{(r_2)}$, form the paired difference $D = C^{(r_1)} - C^{(r_2)}$, and compute the empirical unbiased sample variance of $D$. Additionally, for comparison, estimate the variance of the difference-of-means estimator using independent sampling for each rate: draw $N$ independent standard normal variates with seed $s+1$ for $r_1$ and $N$ independent standard normal variates with seed $s+2$ for $r_2$, compute the corresponding discounted call payoffs, and estimate $\\operatorname{Var}(\\bar{C}^{(r_1)} - \\bar{C}^{(r_2)})$ via the sum of the two sample variances divided by $N$. In summary, for each test case you must output the following four quantities:\n- the unbiased empirical sample variance $s_D^2$ of the paired differences $D$ (using common random numbers),\n- the estimated variance of the difference-of-means estimator under common random numbers, $s_D^2/N$,\n- the estimated variance of the difference-of-means estimator under independent sampling, $s_{C^{(r_1)}}^2/N + s_{C^{(r_2)}}^2/N$,\n- the ratio of the independent-sampling estimator variance to the common-random-numbers estimator variance. If the common-random-numbers estimator variance is numerically zero, output the ratio as the floating-point infinity.\n\nBase your construction on the core definitions of GBM, the risk-neutral valuation principle, and Monte Carlo simulation. Do not use shortcut formulas in the problem statement; derive any necessary relationships from these foundations in your solution. All quantities must be expressed as dimensionless real numbers.\n\nUse the following test suite of parameter values, where each tuple represents $(S_0, K, \\sigma, T, r_1, r_2, N, s)$:\n- Case A (general comparison): $(100.0, 100.0, 0.2, 1.0, 0.01, 0.05, 200000, 271828)$,\n- Case B (boundary with identical rates): $(100.0, 100.0, 0.2, 1.0, 0.03, 0.03, 100000, 314159)$,\n- Case C (short maturity, large rate difference): $(100.0, 100.0, 0.2, 0.01, 0.0, 0.10, 300000, 161803)$,\n- Case D (deep out-of-the-money): $(50.0, 100.0, 0.4, 2.0, 0.02, 0.08, 250000, 141421)$.\n\nYour program must produce a single line of output containing all results in the following order as a comma-separated list enclosed in square brackets:\n$[s_{D,A}^2, v_{\\text{CRN},A}, v_{\\text{IND},A}, \\rho_A, s_{D,B}^2, v_{\\text{CRN},B}, v_{\\text{IND},B}, \\rho_B, s_{D,C}^2, v_{\\text{CRN},C}, v_{\\text{IND},C}, \\rho_C, s_{D,D}^2, v_{\\text{CRN},D}, v_{\\text{IND},D}, \\rho_D]$,\nwhere $s_{D,\\cdot}^2$ denotes the unbiased sample variance of paired differences for the specified case, $v_{\\text{CRN},\\cdot} = s_{D,\\cdot}^2/N$ denotes the common-random-numbers estimator variance, $v_{\\text{IND},\\cdot}$ denotes the independent-sampling estimator variance, and $\\rho_{\\cdot} = v_{\\text{IND},\\cdot} / v_{\\text{CRN},\\cdot}$ denotes the variance ratio for the specified case.",
            "solution": "The problem requires a comparison of variance for an estimator of the difference between two European call option prices, where the prices differ only by the risk-free interest rate, $r$. The comparison is between two Monte Carlo simulation schemes: one using independent random number streams for each option price estimation, and one using common random numbers (CRN), a variance reduction technique.\n\nFirst, we must establish the simulation formula for the terminal asset price, $S_T$. The asset price is stipulated to follow a geometric Brownian motion (GBM) under the risk-neutral measure, described by the stochastic differential equation (SDE):\n$$\ndS_t = r S_t \\, dt + \\sigma S_t \\, dW_t\n$$\nwhere $S_t$ is the asset price at time $t$, $r$ is the risk-free rate, $\\sigma$ is the volatility, and $W_t$ is a standard Wiener process (Brownian motion).\n\nTo find a solution for $S_T$, we apply Itô's lemma to the function $f(S_t) = \\ln(S_t)$. The derivatives are $f'(S_t) = 1/S_t$ and $f''(S_t) = -1/S_t^2$. According to Itô's lemma, the differential $d(\\ln S_t)$ is:\n$$\nd(\\ln S_t) = f'(S_t) dS_t + \\frac{1}{2} f''(S_t) (dS_t)^2\n$$\nThe quadratic variation term $(dS_t)^2$ is found by squaring the SDE, keeping only the lowest order terms in $dt$ according to Itô calculus rules ($dt^2 \\to 0$, $dt dW_t \\to 0$, $(dW_t)^2 \\to dt$):\n$$\n(dS_t)^2 = (r S_t \\, dt + \\sigma S_t \\, dW_t)^2 = \\sigma^2 S_t^2 (dW_t)^2 = \\sigma^2 S_t^2 dt\n$$\nSubstituting $dS_t$ and $(dS_t)^2$ into the lemma gives:\n$$\nd(\\ln S_t) = \\frac{1}{S_t}(r S_t \\, dt + \\sigma S_t \\, dW_t) + \\frac{1}{2} \\left(-\\frac{1}{S_t^2}\\right)(\\sigma^2 S_t^2 dt)\n$$\n$$\nd(\\ln S_t) = (r \\, dt + \\sigma \\, dW_t) - \\frac{1}{2} \\sigma^2 dt = \\left(r - \\frac{1}{2}\\sigma^2\\right)dt + \\sigma dW_t\n$$\nIntegrating both sides from $t=0$ to $t=T$:\n$$\n\\int_0^T d(\\ln S_t) = \\int_0^T \\left(r - \\frac{1}{2}\\sigma^2\\right)dt + \\int_0^T \\sigma dW_t\n$$\n$$\n\\ln(S_T) - \\ln(S_0) = \\left(r - \\frac{1}{2}\\sigma^2\\right)T + \\sigma (W_T - W_0)\n$$\nGiven that $W_0=0$ and the random variable $W_T$ is normally distributed with mean $0$ and variance $T$, we can write $W_T = \\sqrt{T}Z$, where $Z$ is a standard normal random variable, $Z \\sim N(0, 1)$. Exponentiating both sides yields the solution for the terminal asset price:\n$$\nS_T = S_0 \\exp\\left( \\left(r - \\frac{1}{2}\\sigma^2\\right)T + \\sigma\\sqrt{T}Z \\right)\n$$\nThis formula is the basis for the Monte Carlo simulation of terminal asset prices.\n\nThe price of a European call option is the expected value of its discounted payoff under the risk-neutral measure:\n$$\nC(r) = E\\left[ e^{-rT} \\max(S_T - K, 0) \\right]\n$$\nA Monte Carlo estimate for this price is the sample average over $N$ simulated paths:\n$$\n\\bar{C}(r) = \\frac{1}{N} \\sum_{i=1}^N C_i(r) = \\frac{1}{N} \\sum_{i=1}^N e^{-rT} \\max(S_{T,i} - K, 0)\n$$\nwhere each $S_{T,i}$ is generated using an independent draw $Z_i$ from $N(0,1)$.\n\nOur objective is to estimate the difference in option prices for two interest rates, $r_1$ and $r_2$, i.e., $\\Delta = C(r_1) - C(r_2)$. The natural estimator is $\\hat{\\Delta} = \\bar{C}(r_1) - \\bar{C}(r_2)$. We are interested in the variance of this estimator, $\\operatorname{Var}(\\hat{\\Delta})$.\n\nFirst, consider the case of **independent sampling**. We generate two independent sets of $N$ standard normal variates, $\\{Z_i^{(1)}\\}_{i=1}^N$ and $\\{Z_i^{(2)}\\}_{i=1}^N$, to compute $\\bar{C}(r_1)$ and $\\bar{C}(r_2)$ respectively. Because the estimators are independent, the variance of their difference is the sum of their variances:\n$$\n\\operatorname{Var}(\\hat{\\Delta})_{\\text{IND}} = \\operatorname{Var}(\\bar{C}(r_1) - \\bar{C}(r_2)) = \\operatorname{Var}(\\bar{C}(r_1)) + \\operatorname{Var}(\\bar{C}(r_2))\n$$\nThe variance of a sample mean is the population variance divided by the sample size, $\\operatorname{Var}(\\bar{C}(r)) = \\operatorname{Var}(C(r))/N$. We estimate this using the unbiased sample variance, $s_{C(r)}^2 = \\frac{1}{N-1}\\sum_{i=1}^N(C_i(r) - \\bar{C}(r))^2$. Thus, the estimated variance of the difference-of-means estimator is:\n$$\nv_{\\text{IND}} = \\frac{s_{C(r_1)}^2}{N} + \\frac{s_{C(r_2)}^2}{N}\n$$\n\nNext, consider the **common random numbers (CRN)** technique. We use a single set of $N$ standard normal variates, $\\{Z_i\\}_{i=1}^N$, to generate both sequences of payoffs, $\\{C_i(r_1)\\}_{i=1}^N$ and $\\{C_i(r_2)\\}_{i=1}^N$. We then estimate $\\Delta$ by computing the mean of the paired differences, $\\bar{D} = \\frac{1}{N} \\sum_{i=1}^N D_i$, where $D_i = C_i(r_1) - C_i(r_2)$. The variance of this estimator is:\n$$\n\\operatorname{Var}(\\bar{D}) = \\frac{1}{N} \\operatorname{Var}(D) = \\frac{1}{N} \\operatorname{Var}(C(r_1) - C(r_2))\n$$\n$$\nv_{\\text{CRN}} = \\frac{1}{N} \\operatorname{Var}(C(r_1) - C(r_2)) = \\frac{1}{N} \\left( \\operatorname{Var}(C(r_1)) + \\operatorname{Var}(C(r_2)) - 2\\operatorname{Cov}(C(r_1), C(r_2)) \\right)\n$$\nWe estimate this with the sample variance of the differences, $s_D^2 = \\frac{1}{N-1}\\sum_{i=1}^N(D_i - \\bar{D})^2$. The estimated variance of the estimator is then $v_{\\text{CRN}} = s_D^2 / N$. Note that $s_D^2$ is one of the required outputs.\n\nThe effectiveness of CRN depends on the covariance term. For a fixed shock $Z$, the terminal price $S_T(r) = S_0 \\exp\\left( (r - \\frac{1}{2}\\sigma^2)T + \\sigma\\sqrt{T}Z \\right)$ is a monotonically increasing function of $r$. The call payoff $\\max(S_T - K, 0)$ is monotonic in $S_T$. The discounting factor $e^{-rT}$ is monotonic decreasing in $r$. The combined function $C(r, Z) = e^{-rT} \\max(S_T(r,Z) - K, 0)$ also tends to be monotonic in $r$ over a significant part of its domain. This induces a strong positive correlation between the payoff sequences $C_i(r_1)$ and $C_i(r_2)$. As a result, $\\operatorname{Cov}(C(r_1), C(r_2))$ is positive and large, which significantly reduces the variance $v_{\\text{CRN}}$ compared to $v_{\\text{IND}}$. The ratio $\\rho = v_{\\text{IND}} / v_{\\text{CRN}}$ quantifies this variance reduction and is expected to be greater than $1$.\n\nFor the special case where $r_1=r_2$ (Case B), for any given shock $Z_i$, we will have $S_{T,i}^{(r_1)} = S_{T,i}^{(r_2)}$ and thus $C_i^{(r_1)} = C_i^{(r_2)}$. This means every paired difference $D_i = C_i^{(r_1)} - C_i^{(r_2)}$ will be exactly $0$. Consequently, the sample variance of the differences, $s_D^2$, will be $0$. The CRN estimator variance, $v_{\\text{CRN}} = s_D^2/N$, is also $0$. In this scenario, the ratio $\\rho = v_{\\text{IND}} / v_{\\text{CRN}}$ becomes infinite, indicating perfect variance reduction, as expected.\n\nThe computational procedure is as follows for each test case $(S_0, K, \\sigma, T, r_1, r_2, N, s)$:\n1.  **Common Random Numbers:**\n    a. Initialize a random number generator with seed $s$. Generate $N$ standard normal variates $Z_i$.\n    b. For each $Z_i$, compute $S_{T,i}^{(r_1)}$, $S_{T,i}^{(r_2)}$, and then the discounted payoffs $C_i^{(r_1)}$, $C_i^{(r_2)}$.\n    c. Form the differences $D_i = C_i^{(r_1)} - C_i^{(r_2)}$.\n    d. Compute the unbiased sample variance $s_D^2 = \\operatorname{Var}(D_1, \\dots, D_N)$ (with $N-1$ in the denominator).\n    e. Compute the estimator variance $v_{\\text{CRN}} = s_D^2 / N$.\n2.  **Independent Sampling:**\n    a. Initialize a generator with seed $s+1$. Generate $N$ variates $Z_i^{(1)}$ and compute the corresponding payoffs $C_i^{(r_1)}$. Calculate their sample variance $s_{C(r_1)}^2$.\n    b. Initialize a generator with seed $s+2$. Generate $N$ variates $Z_i^{(2)}$ and compute the corresponding payoffs $C_i^{(r_2)}$. Calculate their sample variance $s_{C(r_2)}^2$.\n    c. Compute the estimator variance $v_{\\text{IND}} = s_{C(r_1)}^2/N + s_{C(r_2)}^2/N$.\n3.  **Ratio:** Calculate $\\rho = v_{\\text{IND}} / v_{\\text{CRN}}$. If $v_{\\text{CRN}} = 0$, the ratio is positive infinity.\n\nThese steps are implemented for each test case to produce the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating and comparing variances for Monte Carlo\n    estimators using both common random numbers and independent sampling.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (S0, K, sigma, T, r1, r2, N, s)\n        (100.0, 100.0, 0.2, 1.0, 0.01, 0.05, 200000, 271828), # Case A\n        (100.0, 100.0, 0.2, 1.0, 0.03, 0.03, 100000, 314159), # Case B\n        (100.0, 100.0, 0.2, 0.01, 0.0, 0.10, 300000, 161803), # Case C\n        (50.0, 100.0, 0.4, 2.0, 0.02, 0.08, 250000, 141421),  # Case D\n    ]\n\n    results = []\n    \n    for S0, K, sigma, T, r1, r2, N, s in test_cases:\n        \n        # --- 1. Common Random Numbers (CRN) Calculation ---\n        rng_crn = np.random.default_rng(s)\n        Z = rng_crn.standard_normal(N)\n\n        # Calculate terminal prices for both rates using the same random numbers\n        drift1 = (r1 - 0.5 * sigma**2) * T\n        drift2 = (r2 - 0.5 * sigma**2) * T\n        diffusion = sigma * np.sqrt(T) * Z\n        \n        ST1_crn = S0 * np.exp(drift1 + diffusion)\n        ST2_crn = S0 * np.exp(drift2 + diffusion)\n        \n        # Calculate discounted call payoffs\n        C1_crn = np.exp(-r1 * T) * np.maximum(ST1_crn - K, 0)\n        C2_crn = np.exp(-r2 * T) * np.maximum(ST2_crn - K, 0)\n        \n        # Calculate paired differences\n        D = C1_crn - C2_crn\n        \n        # Unbiased empirical sample variance of the paired differences\n        s_D_sq = np.var(D, ddof=1)\n        \n        # Estimated variance of the difference-of-means estimator under CRN\n        v_crn = s_D_sq / N\n        \n        # --- 2. Independent Sampling Calculation ---\n        # Rate 1\n        rng_ind1 = np.random.default_rng(s + 1)\n        Z1_ind = rng_ind1.standard_normal(N)\n        ST1_ind = S0 * np.exp(drift1 + sigma * np.sqrt(T) * Z1_ind)\n        C1_ind = np.exp(-r1 * T) * np.maximum(ST1_ind - K, 0)\n        s_C1_sq = np.var(C1_ind, ddof=1)\n\n        # Rate 2\n        rng_ind2 = np.random.default_rng(s + 2)\n        Z2_ind = rng_ind2.standard_normal(N)\n        ST2_ind = S0 * np.exp(drift2 + sigma * np.sqrt(T) * Z2_ind)\n        C2_ind = np.exp(-r2 * T) * np.maximum(ST2_ind - K, 0)\n        s_C2_sq = np.var(C2_ind, ddof=1)\n        \n        # Estimated variance of the difference-of-means estimator under independent sampling\n        v_ind = (s_C1_sq / N) + (s_C2_sq / N)\n        \n        # --- 3. Ratio Calculation ---\n        if v_crn == 0:\n            rho = float('inf')\n        else:\n            rho = v_ind / v_crn\n            \n        results.extend([s_D_sq, v_crn, v_ind, rho])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice tackles a more advanced method: adaptive importance sampling. You will program an algorithm that automatically \"learns\" and improves its sampling distribution as the simulation runs, a key technique for complex, high-dimensional problems. This challenge  shows how to build \"smart\" Monte Carlo estimators that efficiently discover and sample an integrand's most important regions, often with little prior knowledge.",
            "id": "2449255",
            "problem": "You are to implement an adaptive Monte Carlo importance sampling estimator in order to numerically evaluate a set of nonnegative integrals. The estimator must use a parametric sampling distribution that may be updated between iterations based on previously generated samples, subject to the constraints stated below. The final program must compute one scalar estimate for each test case and print the results in a single line.\n\nThe task is to estimate, for each test case, a scalar integral of the form\n$$\nI = \\int_{\\mathcal{D}} f(x)\\,dx,\n$$\nwhere $f(x) \\ge 0$ on its domain $\\mathcal{D}$.\n\nYou must construct a Monte Carlo estimator based on importance sampling. For each integrand, use a proposal distribution $q(x;\\theta)$ from the parametric family specified for its domain, and allow the parameter vector $\\theta$ to be updated between iterations using only the samples and function evaluations already generated. The estimator must satisfy the following requirements:\n- The estimator must be unbiased for $I$.\n- For every iteration $t$, the support of $q(\\cdot;\\theta_t)$ must be contained in $\\mathcal{D}$, and $q(x;\\theta_t) > 0$ wherever $f(x) > 0$.\n- The final estimate for each test case must aggregate information from all generated samples across all iterations into a single scalar.\n\nFor domains and admissible proposal families:\n- If $\\mathcal{D} = \\mathbb{R}$, use the Gaussian family $q(x;\\theta) = \\mathcal{N}(\\mu,\\sigma^2)$ with parameters $\\theta = (\\mu,\\sigma)$, where $\\sigma > 0$.\n- If $\\mathcal{D} = (0,\\infty)$, use the Lognormal family with density $q(x;\\theta) = \\mathrm{Lognormal}(\\mu,\\sigma^2)$, i.e., $\\log X \\sim \\mathcal{N}(\\mu,\\sigma^2)$, with parameters $\\theta = (\\mu,\\sigma)$, where $\\sigma > 0$.\n\nYou must use the following test suite. In each case, the program must use the given number of iterations $T$, the number of samples per iteration $M$, the initial parameter vector $\\theta_0$, and the random seed $s$ for the pseudo-random number generator. The order of the output must match the order of the cases below.\n\n- Case $1$ (happy path on $\\mathbb{R}$):\n  - Domain: $\\mathcal{D} = \\mathbb{R}$.\n  - Integrand: $f_1(x) = \\exp\\!\\big(-\\tfrac{1}{2}(x - 2)^2\\big)$.\n  - True value (not for output, for internal validation): $\\sqrt{2\\pi}$.\n  - Iterations: $T = 5$.\n  - Samples per iteration: $M = 4000$.\n  - Initial parameters: $\\theta_0 = (\\mu_0,\\sigma_0) = (0,\\,1.5)$.\n  - Seed: $s = 123$.\n\n- Case $2$ (boundary-support case on $(0,\\infty)$):\n  - Domain: $\\mathcal{D} = (0,\\infty)$.\n  - Integrand: $f_2(x) = x^2 e^{-x}$.\n  - True value (not for output, for internal validation): $2$.\n  - Iterations: $T = 5$.\n  - Samples per iteration: $M = 4000$.\n  - Initial parameters: $\\theta_0 = (\\mu_0,\\sigma_0) = (0,\\,1)$.\n  - Seed: $s = 456$.\n\n- Case $3$ (multimodal integrand on $\\mathbb{R}$):\n  - Domain: $\\mathcal{D} = \\mathbb{R}$.\n  - Integrand:\n    $$\n    f_3(x) = 0.5 \\exp\\!\\left(-\\frac{(x - 3)^2}{2 \\cdot 0.2^2}\\right) + 0.5 \\exp\\!\\left(-\\frac{(x + 3)^2}{2 \\cdot 0.2^2}\\right).\n    $$\n  - True value (not for output, for internal validation): $0.2\\sqrt{2\\pi}$.\n  - Iterations: $T = 6$.\n  - Samples per iteration: $M = 3000$.\n  - Initial parameters: $\\theta_0 = (\\mu_0,\\sigma_0) = (0,\\,1)$.\n  - Seed: $s = 789$.\n\n- Case $4$ (shifted peak on $\\mathbb{R}$):\n  - Domain: $\\mathcal{D} = \\mathbb{R}$.\n  - Integrand: $f_4(x) = \\exp\\!\\big(-\\tfrac{1}{2}(x - 8)^2\\big)$.\n  - True value (not for output, for internal validation): $\\sqrt{2\\pi}$.\n  - Iterations: $T = 5$.\n  - Samples per iteration: $M = 3000$.\n  - Initial parameters: $\\theta_0 = (\\mu_0,\\sigma_0) = (0,\\,1)$.\n  - Seed: $s = 321$.\n\nYour program must:\n- For each case, generate $T$ iterations, draw $M$ samples per iteration from $q(\\cdot;\\theta_t)$, and, after each iteration, update $\\theta_{t+1}$ using only samples and evaluations from iterations up to and including $t$, subject to the constraints above.\n- Produce one unbiased Monte Carlo estimate for each case by aggregating all generated samples from all iterations.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the four estimates, enclosed in square brackets and in the same order as the cases above, for example, $[r_1,r_2,r_3,r_4]$.\n- Each $r_i$ must be a real number (a floating-point value). No units are involved, and no rounding rule is mandated; any standard floating-point formatting is acceptable.",
            "solution": "The problem requires the implementation of an adaptive importance sampling Monte Carlo estimator to calculate the value of integrals of the form $I = \\int_{\\mathcal{D}} f(x)\\,dx$ for non-negative functions $f(x)$. The estimator must be unbiased and must aggregate information from all generated samples. The proposal distribution $q(x; \\theta)$ is parametric and is updated between iterations.\n\nThe fundamental principle of importance sampling is to express the integral $I$ as an expectation with respect to a proposal distribution $q(x)$. Assuming the support of $q(x)$ contains the support of $f(x)$ where $f(x) > 0$, we have:\n$$\nI = \\int_{\\mathcal{D}} f(x) \\, dx = \\int_{\\mathcal{D}} \\frac{f(x)}{q(x; \\theta)} q(x; \\theta) \\, dx = E_{X \\sim q(\\cdot; \\theta)}\\left[\\frac{f(X)}{q(X; \\theta)}\\right]\n$$\nA standard Monte Carlo estimate based on $M$ samples $X_i \\sim q(\\cdot; \\theta)$ is $\\hat{I}_M = \\frac{1}{M} \\sum_{i=1}^M \\frac{f(X_i)}{q(X_i; \\theta)}$.\n\nThe problem specifies an adaptive procedure with $T$ iterations and $M$ samples per iteration. Let $\\theta_t$ be the parameter vector for the proposal distribution $q(\\cdot; \\theta_t)$ at iteration $t \\in \\{1, \\dots, T\\}$. Let $\\{X_{t,i}\\}_{i=1}^M$ be the samples drawn from $q(\\cdot; \\theta_t)$ at iteration $t$. The total number of samples generated is $N_{total} = T \\times M$.\n\nTo satisfy the constraints that the final estimator must be unbiased and must aggregate all samples, we define the estimator as the average of the importance sampling ratios across all samples from all iterations:\n$$\n\\hat{I} = \\frac{1}{N_{total}} \\sum_{t=1}^T \\sum_{i=1}^M \\frac{f(X_{t,i})}{q(X_{t,i}; \\theta_t)}\n$$\nThis estimator is unbiased. To prove this, we analyze the expectation of a single term in the sum, $E\\left[\\frac{f(X_{t,i})}{q(X_{t,i}; \\theta_t)}\\right]$. The parameter vector $\\theta_t$ is determined by the outcomes of iterations $1, \\dots, t-1$. Let $\\mathcal{H}_{t-1}$ denote the history of all random variables generated up to the end of iteration $t-1$. The parameter vector $\\theta_t$ is a function of $\\mathcal{H}_{t-1}$. Using the law of total expectation:\n$$\nE\\left[\\frac{f(X_{t,i})}{q(X_{t,i}; \\theta_t)}\\right] = E_{\\mathcal{H}_{t-1}}\\left[ E\\left[ \\frac{f(X_{t,i})}{q(X_{t,i}; \\theta_t)} \\bigg| \\mathcal{H}_{t-1} \\right] \\right]\n$$\nGiven the history $\\mathcal{H}_{t-1}$, $\\theta_t$ is fixed. The sample $X_{t,i}$ is drawn from $q(\\cdot; \\theta_t)$. Therefore, the inner expectation is:\n$$\nE\\left[ \\frac{f(X_{t,i})}{q(X_{t,i}; \\theta_t)} \\bigg| \\mathcal{H}_{t-1} \\right] = \\int_{\\mathcal{D}} \\frac{f(x)}{q(x; \\theta_t)} q(x; \\theta_t) \\, dx = I\n$$\nSince $I$ is a constant, the outer expectation is $E_{\\mathcal{H}_{t-1}}[I] = I$. This holds for every sample $(t,i)$. By linearity of expectation, the expectation of the final estimator is:\n$$\nE[\\hat{I}] = \\frac{1}{N_{total}} \\sum_{t=1}^T \\sum_{i=1}^M E\\left[\\frac{f(X_{t,i})}{q(X_{t,i}; \\theta_t)}\\right] = \\frac{1}{N_{total}} \\sum_{t=1}^T \\sum_{i=1}^M I = \\frac{N_{total} \\cdot I}{N_{total}} = I\n$$\nThe estimator is thus proven to be unbiased, satisfying a critical requirement.\n\nThe adaptation mechanism for the parameters $\\theta_t$ is not specified in the problem statement. A standard, principled approach is to update the parameters based on the weighted moments of the samples from the previous iteration. This allows the proposal distribution to progressively better approximate the shape of the integrand $f(x)$.\n\nFor iterations $t = 1, \\dots, T-1$, we update $\\theta_t$ to $\\theta_{t+1}$ as follows:\n$1$. At iteration $t$, we have samples $\\{X_{t,i}\\}_{i=1}^M$ drawn from $q(\\cdot; \\theta_t)$.\n$2$. We compute the importance weights for these samples: $w_{t,i} = \\frac{f(X_{t,i})}{q(X_{t,i}; \\theta_t)}$.\n$3$. We normalize these weights to form a discrete probability distribution: $\\bar{w}_{t,i} = \\frac{w_{t,i}}{\\sum_{j=1}^M w_{t,j}}$. If all weights are zero, we do not update the parameters, setting $\\theta_{t+1} = \\theta_t$.\n$4$. We compute the new parameters $\\theta_{t+1} = (\\mu_{t+1}, \\sigma_{t+1})$ using the weighted mean and weighted variance of the samples.\n\nThe specific update rules depend on the proposal family:\n- For $\\mathcal{D} = \\mathbb{R}$ and proposal family $q(x; \\theta) = \\mathcal{N}(\\mu, \\sigma^2)$:\n  $$\n  \\mu_{t+1} = \\sum_{i=1}^M \\bar{w}_{t,i} X_{t,i}\n  $$\n  $$\n  \\sigma_{t+1}^2 = \\sum_{i=1}^M \\bar{w}_{t,i} (X_{t,i} - \\mu_{t+1})^2\n  $$\n- For $\\mathcal{D} = (0,\\infty)$ and proposal family $q(x; \\theta) = \\mathrm{Lognormal}(\\mu, \\sigma^2)$:\n  Here, $\\log(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)$. We adapt the parameters $\\mu$ and $\\sigma$ of the underlying Normal distribution. Let $Y_{t,i} = \\log(X_{t,i})$.\n  $$\n  \\mu_{t+1} = \\sum_{i=1}^M \\bar{w}_{t,i} Y_{t,i}\n  $$\n  $$\n  \\sigma_{t+1}^2 = \\sum_{i=1}^M \\bar{w}_{t,i} (Y_{t,i} - \\mu_{t+1})^2\n  $$\n\nTo ensure numerical stability and that the proposal distribution does not become degenerate, the calculated variance $\\sigma_{t+1}^2$ will be clamped to a small positive minimum value, e.g., $10^{-9}$.\n\nThe overall algorithm for each test case is as follows:\n$1$. Initialize parameters $\\theta_1 = \\theta_0$ from the problem statement.\n$2$. Initialize an empty list to store the values $v_{t,i} = f(X_{t,i})/q(X_{t,i}; \\theta_t)$ for all samples.\n$3$. For $t = 1, \\dots, T$:\n    a. Draw $M$ samples $\\{X_{t,i}\\}_{i=1}^M$ from the proposal distribution $q(\\cdot; \\theta_t)$.\n    b. For each sample $X_{t,i}$, compute $v_{t,i}$ and append it to the list of values.\n    c. If $t < T$, compute the weights $\\{w_{t,i}\\}_{i=1}^M$ and update $\\theta_{t+1} = (\\mu_{t+1}, \\sigma_{t+1})$ using the weighted moment formulas described above.\n$4$. After the loop, calculate the final estimate $\\hat{I}$ by taking the arithmetic mean of all stored values $v_{t,i}$.\n\nThis design is scientifically sound, directly addresses all constraints of the problem, and relies on established principles of Monte Carlo methods. The implementation will use `numpy` for numerical operations and random number generation, and `scipy.stats` for probability density functions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, lognorm\n\ndef solve():\n    \"\"\"\n    Main function to run the adaptive importance sampling for all test cases.\n    \"\"\"\n\n    # Define the integrands for each case.\n    def f1(x):\n        return np.exp(-0.5 * (x - 2.0)**2)\n\n    def f2(x):\n        return (x**2) * np.exp(-x)\n\n    def f3(x):\n        term1 = 0.5 * np.exp(-(x - 3.0)**2 / (2.0 * 0.2**2))\n        term2 = 0.5 * np.exp(-(x + 3.0)**2 / (2.0 * 0.2**2))\n        return term1 + term2\n\n    def f4(x):\n        return np.exp(-0.5 * (x - 8.0)**2)\n\n    integrands = [f1, f2, f3, f4]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"id\": 1,\n            \"domain\": \"R\",\n            \"integrand\": integrands[0],\n            \"T\": 5,\n            \"M\": 4000,\n            \"theta0\": (0.0, 1.5),\n            \"seed\": 123\n        },\n        {\n            \"id\": 2,\n            \"domain\": \"(0,inf)\",\n            \"integrand\": integrands[1],\n            \"T\": 5,\n            \"M\": 4000,\n            \"theta0\": (0.0, 1.0),\n            \"seed\": 456\n        },\n        {\n            \"id\": 3,\n            \"domain\": \"R\",\n            \"integrand\": integrands[2],\n            \"T\": 6,\n            \"M\": 3000,\n            \"theta0\": (0.0, 1.0),\n            \"seed\": 789\n        },\n        {\n            \"id\": 4,\n            \"domain\": \"R\",\n            \"integrand\": integrands[3],\n            \"T\": 5,\n            \"M\": 3000,\n            \"theta0\": (0.0, 1.0),\n            \"seed\": 321\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_adaptive_is(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_adaptive_is(case_params):\n    \"\"\"\n    Runs the adaptive importance sampling simulation for a single test case.\n    \"\"\"\n    T = case_params[\"T\"]\n    M = case_params[\"M\"]\n    theta0 = case_params[\"theta0\"]\n    seed = case_params[\"seed\"]\n    f = case_params[\"integrand\"]\n    domain = case_params[\"domain\"]\n\n    rng = np.random.default_rng(seed)\n    \n    theta_current = theta0\n    all_values = []\n    \n    # Minimum variance to prevent proposal collapse\n    MIN_VAR = 1e-9\n\n    for t in range(T):\n        mu, sigma = theta_current\n        \n        # 1. Sample from the proposal distribution q_t\n        if domain == \"R\":\n            # Gaussian proposal\n            samples = rng.normal(loc=mu, scale=sigma, size=M)\n            q_pdf_values = norm.pdf(samples, loc=mu, scale=sigma)\n        elif domain == \"(0,inf)\":\n            # Lognormal proposal\n            # scipy.stats.lognorm(s=sigma, scale=exp(mu)) corresponds to log X ~ N(mu, sigma^2)\n            samples = rng.lognormal(mean=mu, sigma=sigma, size=M)\n            q_pdf_values = lognorm.pdf(samples, s=sigma, scale=np.exp(mu))\n        \n        # 2. Compute integrand values and raw estimator terms\n        f_values = f(samples)\n        \n        # Avoid division by zero if q_pdf_values is zero for some reason\n        # and f_values is non-zero (unlikely if support condition holds).\n        # We also handle cases where f(x)=0, making weights zero.\n        with np.errstate(divide='ignore', invalid='ignore'):\n            values = np.divide(f_values, q_pdf_values)\n            values[q_pdf_values == 0] = 0.0 # If f>0 but q=0, results in Inf. True estimator requires q>0.\n        \n        all_values.extend(values)\n\n        # 3. Adapt parameters for the next iteration (if not the last one)\n        if t < T - 1:\n            weights = values  # In this formulation, weights are the same as values\n            \n            sum_weights = np.sum(weights)\n\n            if sum_weights > 0 and np.isfinite(sum_weights):\n                normalized_weights = weights / sum_weights\n                \n                if domain == \"R\":\n                    # Update parameters for Gaussian proposal\n                    new_mu = np.sum(normalized_weights * samples)\n                    new_var = np.sum(normalized_weights * (samples - new_mu)**2)\n                    new_sigma = np.sqrt(max(new_var, MIN_VAR))\n                    theta_current = (new_mu, new_sigma)\n                elif domain == \"(0,inf)\":\n                    # Update parameters for Lognormal proposal (in log-space)\n                    log_samples = np.log(samples)\n                    new_mu_log = np.sum(normalized_weights * log_samples)\n                    new_var_log = np.sum(normalized_weights * (log_samples - new_mu_log)**2)\n                    new_sigma_log = np.sqrt(max(new_var_log, MIN_VAR))\n                    theta_current = (new_mu_log, new_sigma_log)\n            # If sum_weights is zero or non-finite, we don't update theta.\n            # This happens if all samples land where f(x) is zero.\n\n    # 4. Compute the final estimate\n    final_estimate = np.mean(all_values)\n    return final_estimate\n\nsolve()\n```"
        }
    ]
}