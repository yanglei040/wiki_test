{
    "hands_on_practices": [
        {
            "introduction": "One of the most powerful features of a Polynomial Chaos Expansion is the ability to compute statistical moments of a model output without resorting to expensive Monte Carlo sampling. Thanks to the orthonormality of the polynomial basis, the mean and variance can be calculated directly from the PCE coefficients. This exercise provides practice in this fundamental technique for a time-dependent output, demonstrating how the evolution of uncertainty can be tracked analytically.",
            "id": "2448415",
            "problem": "Consider a scalar random input $\\,\\xi\\,$ with distribution $\\xi \\sim \\mathrm{Uniform}(-1,1)$, and let $\\{\\psi_{k}(\\xi)\\}_{k=0}^{\\infty}$ denote the orthonormal polynomial basis with respect to the probability measure of $\\,\\xi\\,$, where $\\psi_{0}(\\xi)=1$ and $\\mathbb{E}[\\psi_{i}(\\xi)\\,\\psi_{j}(\\xi)]=\\delta_{ij}$ for all nonnegative integers $\\,i,j\\,$. A time-dependent model output $\\,Y(t,\\xi)\\,$ admits the following truncated Polynomial Chaos Expansion (PCE) of total order $\\,3\\,$:\n$$\nY(t,\\xi)=a_{0}(t)\\,\\psi_{0}(\\xi)+a_{1}(t)\\,\\psi_{1}(\\xi)+a_{2}(t)\\,\\psi_{2}(\\xi)+a_{3}(t)\\,\\psi_{3}(\\xi),\n$$\nwith time-dependent deterministic coefficients\n$$\na_{0}(t)=1+\\exp(-t),\\quad a_{1}(t)=t\\,\\exp\\!\\left(-\\frac{t}{2}\\right),\\quad a_{2}(t)=\\sin(t),\\quad a_{3}(t)=\\exp(-t)\\,\\cos(t).\n$$\nHere $\\,t \\ge 0\\,$ is time, and all trigonometric function arguments are in radians. Compute the time-dependent variance $\\,\\mathrm{Var}[Y(t)]\\,$ as a closed-form function of $\\,t\\,$. Express your final result as a single analytic expression in $\\,t\\,$. No rounding is required.",
            "solution": "The variance of a random variable $X$ is defined as $\\mathrm{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We apply this definition to the random process $Y(t, \\xi)$ for a fixed time $t$.\n\nFirst, we compute the expectation (mean) of $Y(t, \\xi)$. The expectation operator $\\mathbb{E}[\\cdot]$ is linear.\n$$ \\mathbb{E}[Y(t, \\xi)] = \\mathbb{E}\\left[ \\sum_{k=0}^{3} a_k(t) \\psi_k(\\xi) \\right] = \\sum_{k=0}^{3} a_k(t) \\mathbb{E}[\\psi_k(\\xi)] $$\nWe must evaluate the expectation of each basis polynomial. It is given that $\\psi_{0}(\\xi)=1$. Thus, its expectation is $\\mathbb{E}[\\psi_0(\\xi)] = \\mathbb{E}[1] = 1$. For any $k > 0$, we can use the provided orthonormality condition $\\mathbb{E}[\\psi_i(\\xi) \\psi_j(\\xi)] = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n$$ \\mathbb{E}[\\psi_k(\\xi)] = \\mathbb{E}[\\psi_k(\\xi) \\cdot 1] = \\mathbb{E}[\\psi_k(\\xi) \\psi_0(\\xi)] = \\delta_{k0} $$\nThis shows that $\\mathbb{E}[\\psi_k(\\xi)]$ is $1$ for $k=0$ and $0$ for $k > 0$.\nSubstituting these results back into the expression for the expectation of $Y(t, \\xi)$:\n$$ \\mathbb{E}[Y(t, \\xi)] = a_0(t) \\mathbb{E}[\\psi_0(\\xi)] + a_1(t) \\mathbb{E}[\\psi_1(\\xi)] + a_2(t) \\mathbb{E}[\\psi_2(\\xi)] + a_3(t) \\mathbb{E}[\\psi_3(\\xi)] $$\n$$ \\mathbb{E}[Y(t, \\xi)] = a_0(t) \\cdot 1 + a_1(t) \\cdot 0 + a_2(t) \\cdot 0 + a_3(t) \\cdot 0 = a_0(t) $$\nThe mean of the process is simply the coefficient of the zeroth-order basis polynomial.\n\nNext, we compute the second moment, $\\mathbb{E}[Y(t, \\xi)^2]$.\n$$ Y(t, \\xi)^2 = \\left( \\sum_{i=0}^{3} a_i(t) \\psi_i(\\xi) \\right) \\left( \\sum_{j=0}^{3} a_j(t) \\psi_j(\\xi) \\right) = \\sum_{i=0}^{3} \\sum_{j=0}^{3} a_i(t) a_j(t) \\psi_i(\\xi) \\psi_j(\\xi) $$\nTaking the expectation:\n$$ \\mathbb{E}[Y(t, \\xi)^2] = \\mathbb{E}\\left[ \\sum_{i=0}^{3} \\sum_{j=0}^{3} a_i(t) a_j(t) \\psi_i(\\xi) \\psi_j(\\xi) \\right] $$\nBy linearity of expectation, and since $a_i(t)$ are deterministic coefficients:\n$$ \\mathbb{E}[Y(t, \\xi)^2] = \\sum_{i=0}^{3} \\sum_{j=0}^{3} a_i(t) a_j(t) \\mathbb{E}[\\psi_i(\\xi) \\psi_j(\\xi)] $$\nUsing the orthonormality property $\\mathbb{E}[\\psi_i(\\xi) \\psi_j(\\xi)] = \\delta_{ij}$:\n$$ \\mathbb{E}[Y(t, \\xi)^2] = \\sum_{i=0}^{3} \\sum_{j=0}^{3} a_i(t) a_j(t) \\delta_{ij} $$\nThe Kronecker delta $\\delta_{ij}$ collapses the double summation into a single sum because terms are non-zero only when $i=j$.\n$$ \\mathbb{E}[Y(t, \\xi)^2] = \\sum_{k=0}^{3} a_k(t)^2 = a_0(t)^2 + a_1(t)^2 + a_2(t)^2 + a_3(t)^2 $$\nThis is a direct consequence of the orthonormality of the basis, known as Parseval's theorem for PCE.\n\nFinally, we assemble the variance, $\\mathrm{Var}[Y(t)]$.\n$$ \\mathrm{Var}[Y(t)] = \\mathbb{E}[Y(t, \\xi)^2] - (\\mathbb{E}[Y(t, \\xi)])^2 $$\nSubstituting the expressions we derived:\n$$ \\mathrm{Var}[Y(t)] = (a_0(t)^2 + a_1(t)^2 + a_2(t)^2 + a_3(t)^2) - (a_0(t))^2 $$\n$$ \\mathrm{Var}[Y(t)] = a_1(t)^2 + a_2(t)^2 + a_3(t)^2 = \\sum_{k=1}^{3} a_k(t)^2 $$\nThe variance of the PCE is the sum of the squares of the coefficients of the higher-order basis polynomials (i.e., for $k \\ge 1$).\n\nNow, substitute the given functional forms for the coefficients $a_1(t)$, $a_2(t)$, and $a_3(t)$:\n- $a_1(t) = t\\,\\exp(-\\frac{t}{2})$, so $a_1(t)^2 = \\left(t\\,\\exp(-\\frac{t}{2})\\right)^2 = t^2 \\exp(-t)$.\n- $a_2(t) = \\sin(t)$, so $a_2(t)^2 = \\sin^2(t)$.\n- $a_3(t) = \\exp(-t)\\,\\cos(t)$, so $a_3(t)^2 = \\left(\\exp(-t)\\,\\cos(t)\\right)^2 = \\exp(-2t)\\cos^2(t)$.\n\nCombining these terms gives the final expression for the variance as a function of time $t$:\n$$ \\mathrm{Var}[Y(t)] = t^2 \\exp(-t) + \\sin^2(t) + \\exp(-2t)\\cos^2(t) $$\nThis is the closed-form analytical expression required.",
            "answer": "$$\n\\boxed{t^{2}\\exp(-t) + \\sin^{2}(t) + \\exp(-2t)\\cos^{2}(t)}\n$$"
        },
        {
            "introduction": "Before applying PCE to approximate complex, non-polynomial functions, it is crucial to understand its capabilities on simpler ones. This problem serves as a conceptual check on the fundamental nature of the polynomial basis. It asks you to determine the exact PCE order needed to represent a simple polynomial, reinforcing the idea that a PCE of order $P$ forms a basis for the space of polynomials of degree at most $P$.",
            "id": "2448487",
            "problem": "Consider a single random input $\\,\\xi\\,$ distributed as a continuous uniform random variable on the interval $[-1,1]$, denoted $\\,\\xi \\sim \\mathcal{U}[-1,1]\\,$. Let a Polynomial Chaos Expansion (PCE) be constructed using the Legendre polynomial basis $\\{L_{n}(\\xi)\\}_{n=0}^{\\infty}$, which is orthogonal on $[-1,1]$ with respect to the constant weight function. Define the truncated PCE of order $P$ to be the finite series\n$$\n\\sum_{n=0}^{P} a_{n}\\,L_{n}(\\xi),\n$$\nwhere $a_{n}$ are deterministic coefficients and where the order $P$ is the highest polynomial degree included in the basis.\n\nGiven the model response $y(\\xi)=\\xi^{4}$, determine the minimal integer order $P$ such that the truncated Legendre PCE of order $P$ can represent $y(\\xi)$ exactly (with no truncation error). Provide your final answer as a single integer with no units. No rounding is required.",
            "solution": "The problem requires us to determine the minimal integer order $P$ of a truncated Polynomial Chaos Expansion (PCE) that can exactly represent the model response $y(\\xi) = \\xi^{4}$, where $\\xi$ is a random variable with a uniform distribution on the interval $[-1, 1]$. The expansion is constructed using the Legendre polynomial basis $\\{L_{n}(\\xi)\\}_{n=0}^{\\infty}$.\n\nA truncated PCE of order $P$ is given by the finite series:\n$$\n\\sum_{n=0}^{P} a_{n} L_{n}(\\xi)\n$$\nwhere $a_{n}$ are deterministic coefficients. The Legendre polynomial $L_{n}(\\xi)$ is a polynomial of degree $n$. Consequently, the truncated PCE of order $P$ is a polynomial of degree at most $P$.\n\nA cornerstone of approximation theory is that the set of Legendre polynomials $\\{L_{0}(\\xi), L_{1}(\\xi), \\dots, L_{P}(\\xi)\\}$ forms a complete orthogonal basis for the vector space of all polynomials of degree up to $P$. This space is denoted $\\mathbb{P}_{P}$.\n\nThe function to be represented is $y(\\xi) = \\xi^{4}$. This function is a monomial, and more specifically, it is a polynomial of degree $4$. As such, $y(\\xi)$ is an element of the space $\\mathbb{P}_{4}$.\n\nFor the PCE to represent $y(\\xi)$ exactly, the expansion must be identically equal to $\\xi^{4}$:\n$$\n\\sum_{n=0}^{P} a_{n} L_{n}(\\xi) = \\xi^{4}\n$$\nThe left side of this equation is a polynomial of degree at most $P$. The right side is a polynomial of degree $4$. By the fundamental theorem of algebra, two polynomials are identical if and only if their degrees are equal and all corresponding coefficients are equal.\n\nConsider an expansion of order $P  4$. The sum $\\sum_{n=0}^{P} a_{n} L_{n}(\\xi)$ results in a polynomial of degree at most $P$. It is a logical impossibility for a polynomial of degree less than $4$ to be equal to $\\xi^{4}$ for all $\\xi \\in [-1, 1]$. Therefore, an order $P  4$ is insufficient to achieve an exact representation. This implies that the minimal required order must be $P \\ge 4$.\n\nNow, consider an expansion of order $P=4$. The set of basis functions is $\\{L_{0}(\\xi), L_{1}(\\xi), L_{2}(\\xi), L_{3}(\\xi), L_{4}(\\xi)\\}$. This set forms a basis for the space $\\mathbb{P}_{4}$. Since $y(\\xi) = \\xi^{4}$ is an element of $\\mathbb{P}_{4}$, it can be uniquely expressed as a linear combination of these basis polynomials. That is, there exist unique coefficients $a_{0}, a_{1}, a_{2}, a_{3}, a_{4}$ such that:\n$$\n\\xi^{4} = a_{0}L_{0}(\\xi) + a_{1}L_{1}(\\xi) + a_{2}L_{2}(\\xi) + a_{3}L_{3}(\\xi) + a_{4}L_{4}(\\xi)\n$$\nThe coefficient $a_{4}$ must be non-zero. This is because the polynomial $L_{4}(\\xi)$ is the only polynomial in the basis set $\\{L_{0}(\\xi), \\dots, L_{4}(\\xi)\\}$ that contains a $\\xi^{4}$ term. All other basis polynomials $L_{n}(\\xi)$ for $n  4$ are of degree less than $4$. To match the $\\xi^{4}$ term on the right-hand side, the $a_{4}L_{4}(\\xi)$ term is mandatory.\n\nSince an order of $P=4$ is sufficient for an exact representation and any order $P4$ is insufficient, the minimal order required is precisely $P=4$. This is a direct consequence of the fact that the space of polynomials of degree up to $N$ is spanned by the first $N+1$ Legendre polynomials.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Beyond propagating uncertainty, PCE is an exceptionally efficient tool for performing Global Sensitivity Analysis (GSA). The structure of the expansion allows for the direct computation of Sobol' indices, which quantify the contribution of each input variable and their interactions to the output variance. This advanced practice challenges you to connect the PCE coefficients to these sensitivity indices for the well-known Ishigami function, a standard benchmark in UQ.",
            "id": "2448434",
            "problem": "Consider the Ishigami function defined for independent input random variables $x_1$, $x_2$, and $x_3$, each uniformly distributed on the interval $\\left[-\\pi,\\pi\\right]$, by\n$$\nf(x_1,x_2,x_3) \\;=\\; \\sin(x_1) \\;+\\; a\\,\\sin^2(x_2) \\;+\\; b\\,x_3^4\\,\\sin(x_1),\n$$\nwhere $a$ and $b$ are fixed real parameters and all angles are in radians. Let $S_i$ denote the first-order Sobol' index of input $x_i$, and $S_{ij}$ denote the second-order Sobol' index describing the interaction between inputs $x_i$ and $x_j$. Derive, from first principles, analytical formulas for the variance decomposition and the Sobol' indices $S_1$, $S_2$, $S_3$, $S_{12}$, $S_{13}$, and $S_{23}$ as functions of $a$, $b$, and $\\pi$. Then, for each parameter set in the test suite below, construct a third-order Polynomial Chaos Expansion (PCE) of $f$ under the correct orthonormal polynomial basis for the given independent uniform inputs, use that truncated PCE to compute the corresponding PCE-based Sobol' indices $S_1$, $S_2$, $S_3$, $S_{12}$, $S_{13}$, and $S_{23}$, and compare them to the analytical indices.\n\nYour program must implement the following test suite of parameter values $(a,b)$:\n- Test $1$: $(a,b) = (7,\\,0.1)$,\n- Test $2$: $(a,b) = (7,\\,0)$,\n- Test $3$: $(a,b) = (0,\\,0.1)$,\n- Test $4$: $(a,b) = (0,\\,0)$.\n\nFor each test, compute the absolute differences between the PCE-based indices and the analytical indices for the set $\\{S_1,S_2,S_3,S_{12},S_{13},S_{23}\\}$, and return the maximum absolute difference for that test as a single real number.\n\nYour program should produce a single line of output containing the results for Tests $1$ through $4$ as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the maximum absolute difference for Test $i$ expressed as a floating-point number. No other output is permitted.",
            "solution": "### Part 1: Analytical Sobol' Indices\n\nThe problem requires the derivation of analytical Sobol' indices from first principles. This is achieved through the ANOVA (Analysis of Variance) or Hoeffding-Sobol' decomposition of the function $f(x_1, x_2, x_3)$. The input variables $x_i$ are independent and identically distributed, $x_i \\sim U[-\\pi, \\pi]$, with probability density function $p(x) = 1/(2\\pi)$.\n\nFirst, we compute expectations of elementary functions of a single random variable $x \\sim U[-\\pi, \\pi]$:\n- $E[\\sin(x)] = 0$\n- $E[\\sin^2(x)] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\sin^2(x) dx = \\frac{1}{2}$\n- For $n \\in \\mathbb{N}$, $E[x^n] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} x^n dx$. This is $0$ for odd $n$ and $\\frac{\\pi^n}{n+1}$ for even $n$.\n- $E[x^4] = \\frac{\\pi^4}{5}$\n- $E[x^8] = \\frac{\\pi^8}{9}$\n\nThe ANOVA decomposition represents $f$ as a sum of terms with increasing dimensionality:\n$$f(x_1, x_2, x_3) = f_0 + \\sum_{i=1}^3 f_i(x_i) + \\sum_{1 \\le i  j \\le 3} f_{ij}(x_i, x_j) + f_{123}(x_1, x_2, x_3)$$\n\nThe terms are defined as follows:\n- **Mean ($f_0$):**\n$f_0 = E[f] = E[\\sin(x_1) + a\\sin^2(x_2) + bx_3^4\\sin(x_1)] = E[\\sin(x_1)] + aE[\\sin^2(x_2)] + bE[x_3^4]E[\\sin(x_1)] = 0 + a(\\frac{1}{2}) + b(\\frac{\\pi^4}{5})(0) = \\frac{a}{2}$.\n\n- **First-order terms ($f_i$):** $f_i(x_i) = E[f | x_i] - f_0$.\n$E[f|x_1] = \\sin(x_1) + aE[\\sin^2(x_2)] + bE[x_3^4]\\sin(x_1) = (1 + b\\frac{\\pi^4}{5})\\sin(x_1) + \\frac{a}{2} \\implies f_1(x_1) = (1 + b\\frac{\\pi^4}{5})\\sin(x_1)$.\n$E[f|x_2] = E[\\sin(x_1)] + a\\sin^2(x_2) + bE[x_3^4]E[\\sin(x_1)] = a\\sin^2(x_2) \\implies f_2(x_2) = a(\\sin^2(x_2) - \\frac{1}{2})$.\n$E[f|x_3] = E[\\sin(x_1)] + aE[\\sin^2(x_2)] + bx_3^4E[\\sin(x_1)] = \\frac{a}{2} \\implies f_3(x_3) = 0$.\n\n- **Second-order terms ($f_{ij}$):** $f_{ij}(x_i, x_j) = E[f | x_i, x_j] - f_i(x_i) - f_j(x_j) - f_0$.\n$E[f|x_1,x_2] = \\sin(x_1) + a\\sin^2(x_2) + bE[x_3^4]\\sin(x_1) = (1+b\\frac{\\pi^4}{5})\\sin(x_1) + a\\sin^2(x_2)$.\n$f_{12}(x_1,x_2) = E[f|x_1,x_2] - f_1(x_1) - f_2(x_2) - f_0 = 0$.\n$E[f|x_1,x_3] = \\sin(x_1) + aE[\\sin^2(x_2)] + bx_3^4\\sin(x_1) = (1+bx_3^4)\\sin(x_1) + \\frac{a}{2}$.\n$f_{13}(x_1,x_3) = E[f|x_1,x_3] - f_1(x_1) - f_3(x_3) - f_0 = b(x_3^4 - \\frac{\\pi^4}{5})\\sin(x_1)$.\n$E[f|x_2,x_3] = E[\\sin(x_1)] + a\\sin^2(x_2) + bx_3^4E[\\sin(x_1)] = a\\sin^2(x_2)$.\n$f_{23}(x_2,x_3) = E[f|x_2,x_3] - f_2(x_2) - f_3(x_3) - f_0 = 0$.\n\nThe sum of derived functions $f_0 + f_1 + f_2 + f_{13}$ correctly reconstructs the original function $f$, which implies that all higher-order terms, including $f_{123}$, are zero.\n\nThe partial variances are $D_i = \\text{Var}[f_i(x_i)]$ and $D_{ij} = \\text{Var}[f_{ij}(x_i, x_j)]$:\n$D_1 = \\text{Var}[f_1] = (1 + b\\frac{\\pi^4}{5})^2 \\text{Var}[\\sin(x_1)] = (1 + b\\frac{\\pi^4}{5})^2 E[\\sin^2(x_1)] = \\frac{1}{2}(1 + b\\frac{\\pi^4}{5})^2$.\n$D_2 = \\text{Var}[f_2] = a^2 \\text{Var}[\\sin^2(x_2) - \\frac{1}{2}] = a^2(E[\\sin^4(x_2)] - (E[\\sin^2(x_2)])^2) = a^2(\\frac{3}{8} - (\\frac{1}{2})^2) = \\frac{a^2}{8}$.\n$D_3 = \\text{Var}[f_3] = 0$.\n$D_{12} = \\text{Var}[f_{12}] = 0$.\n$D_{13} = \\text{Var}[f_{13}] = b^2 \\text{Var}[\\sin(x_1)] \\text{Var}[x_3^4] = b^2 E[\\sin^2(x_1)] (E[x_3^8] - (E[x_3^4])^2) = b^2 (\\frac{1}{2}) (\\frac{\\pi^8}{9} - (\\frac{\\pi^4}{5})^2) = \\frac{8b^2\\pi^8}{225}$.\n$D_{23} = \\text{Var}[f_{23}] = 0$.\n\nThe total variance $D = \\text{Var}[f]$ is the sum of all partial variances:\n$D = D_1 + D_2 + D_{13}$.\nExpanding and simplifying gives: $D = \\frac{1}{2} + \\frac{b\\pi^4}{5} + \\frac{b^2\\pi^8}{18} + \\frac{a^2}{8}$.\n\nThe Sobol' indices are $S_u = D_u/D$:\n$S_1 = D_1/D$, $S_2 = D_2/D$, $S_3=0$, $S_{12}=0$, $S_{13}=D_{13}/D$, $S_{23}=0$.\n\n### Part 2: Polynomial Chaos Expansion Approximation\n\nThe problem specifies a third-order ($p=3$) Polynomial Chaos Expansion. Since the inputs $x_i$ are uniform on $[-\\pi, \\pi]$, we first map them to standard uniform variables $\\zeta_i \\sim U[-1, 1]$ via $x_i = \\pi\\zeta_i$. The corresponding orthonormal polynomial basis consists of normalized Legendre polynomials, $P_k(\\zeta) = \\sqrt{2k+1}\\tilde{P}_k(\\zeta)$, where $\\tilde{P}_k$ are the standard Legendre polynomials.\n\nThe PCE of $f$ is given by:\n$$f(x(\\zeta)) \\approx \\sum_{|\\alpha| \\le 3} c_\\alpha \\Phi_\\alpha(\\zeta)$$\nwhere $\\alpha=(\\alpha_1, \\alpha_2, \\alpha_3)$ is a multi-index, $|\\alpha| = \\sum_i \\alpha_i$, and $\\Phi_\\alpha(\\zeta) = \\prod_i P_{\\alpha_i}(\\zeta_i)$. The coefficients $c_\\alpha$ are found by projection:\n$$c_\\alpha = E[f(x(\\zeta)) \\Phi_\\alpha(\\zeta)] = \\int_{[-1,1]^3} f(\\pi\\zeta) \\Phi_\\alpha(\\zeta) \\prod_{i=1}^3 \\frac{d\\zeta_i}{2}$$\n\nDue to the structure and symmetries of $f$, many coefficients are zero.\n$f(\\pi\\zeta) = \\sin(\\pi\\zeta_1) + a \\sin^2(\\pi\\zeta_2) + b\\pi^4\\zeta_3^4\\sin(\\pi\\zeta_1)$.\n- The term $\\sin(\\pi\\zeta_1)$ is odd in $\\zeta_1$; its expansion involves only $P_k(\\zeta_1)$ with odd $k$.\n- The term $a\\sin^2(\\pi\\zeta_2)$ is even in $\\zeta_2$; its expansion involves only $P_k(\\zeta_2)$ with even $k$.\n- The interaction term $b\\pi^4\\zeta_3^4\\sin(\\pi\\zeta_1)$ couples an odd function of $\\zeta_1$ and an even function of $\\zeta_3$. Its expansion involves terms $P_{k_1}(\\zeta_1)P_{k_3}(\\zeta_3)$ where $k_1$ is odd and $k_3$ is even.\n- There are no terms coupling $(x_1, x_2)$, $(x_2, x_3)$, or $(x_1, x_2, x_3)$.\n\nFor a PCE of total degree $p=3$, the only non-zero coefficients contributing to the variance are $c_{100}$, $c_{300}$, $c_{020}$, and $c_{102}$. All other coefficients are zero, including those for $S_3$, $S_{12}$, and $S_{23}$. The required coefficients will be computed by numerically integrating the projection integrals.\n\nThe PCE-based partial variances are sums of squared coefficients.\n$D_1^{PCE} = c_{100}^2 + c_{300}^2$\n$D_2^{PCE} = c_{020}^2$\n$D_{13}^{PCE} = c_{102}^2$\nThe total variance approximated by the PCE is $D^{PCE} = D_1^{PCE} + D_2^{PCE} + D_{13}^{PCE}$.\nThe PCE-based Sobol' indices are then $S_u^{PCE} = D_u^{PCE} / D^{PCE}$.\n\n### Part 3: Comparison\n\nThe final step for each test case $(a,b)$ is to compute the maximum absolute difference between the two sets of indices:\n$$\\max \\left( |S_1 - S_1^{PCE}|, |S_2 - S_2^{PCE}|, |S_3 - S_3^{PCE}|, |S_{12} - S_{12}^{PCE}|, |S_{13} - S_{13}^{PCE}|, |S_{23} - S_{23}^{PCE}| \\right)$$\nSince $S_3, S_{12}, S_{23}$ are zero in both analytical and PCE models, the comparison simplifies to the non-zero indices $S_1, S_2, S_{13}$. The discrepancy arises from the truncation of the infinite PCE series at order $3$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.special import legendre\n\ndef get_pce_coefficients(a, b):\n    \"\"\"\n    Computes the required PCE coefficients c_alpha for the Ishigami function\n    using numerical quadrature. The expansion is of total order 3.\n    \"\"\"\n    pi = np.pi\n\n    # Define the functions of single variables to be projected onto the Legendre basis\n    g1 = lambda z: np.sin(pi * z)\n    g2 = lambda z: np.sin(pi * z)**2\n    g3 = lambda z: z**4\n\n    # Standard Legendre polynomials from scipy.special\n    leg_1 = legendre(1)\n    leg_2 = legendre(2)\n    leg_3 = legendre(3)\n\n    def compute_projection(g, k, leg_poly):\n        \"\"\"\n        Computes E[g(zeta) * P_k(zeta)], where P_k are orthonormal Legendre polynomials.\n        E[h] = integral(h(z) * 1/2 dz) from -1 to 1.\n        P_k(z) = sqrt(2k+1) * leg_k(z).\n        \"\"\"\n        integrand = lambda z: g(z) * leg_poly(z)\n        integral_val, _ = quad(integrand, -1, 1)\n        # The projection E[g * P_k] is (sqrt(2k+1)/2) * integral(g * leg_k dz)\n        return np.sqrt(2 * k + 1) / 2.0 * integral_val\n\n    # Calculate the required projection components I(g, k)\n    I_g1_k1 = compute_projection(g1, 1, leg_1)\n    I_g1_k3 = compute_projection(g1, 3, leg_3)\n    I_g2_k2 = compute_projection(g2, 2, leg_2)\n    I_g3_k2 = compute_projection(g3, 2, leg_2)\n    \n    # E[zeta^4] for zeta ~ U[-1,1] is 1/5\n    E_zeta4 = 1.0 / 5.0\n    \n    # Assemble the non-zero coefficients based on analytical derivation\n    # c_100 and c_300 come from the sin(x1) term (with interaction)\n    c100 = I_g1_k1 * (1 + b * pi**4 * E_zeta4)\n    c300 = I_g1_k3 * (1 + b * pi**4 * E_zeta4)\n    # c_020 comes from the a*sin^2(x2) term\n    c020 = a * I_g2_k2\n    # c_102 comes from the interaction term b*x3^4*sin(x1)\n    c102 = b * pi**4 * I_g1_k1 * I_g3_k2\n    \n    return c100, c300, c020, c102\n\ndef analytical_sobol(a, b):\n    \"\"\"\n    Computes the analytical Sobol' indices for the Ishigami function\n    with parameters a and b.\n    \"\"\"\n    pi = np.pi\n    pi4 = pi**4\n    pi8 = pi**8\n\n    # Total Variance D = Var[f]\n    D = (a**2 / 8.0) + (b**2 * pi8 / 18.0) + (b * pi4 / 5.0) + 0.5\n\n    if D == 0:\n        return {'S1': 0.0, 'S2': 0.0, 'S3': 0.0, 'S12': 0.0, 'S13': 0.0, 'S23': 0.0}\n\n    # Partial Variances\n    D1 = 0.5 * (1 + b * pi4 / 5.0)**2\n    D2 = a**2 / 8.0\n    D13 = 8.0 * b**2 * pi8 / 225.0\n\n    return {\n        'S1': D1 / D, 'S2': D2 / D, 'S3': 0.0,\n        'S12': 0.0, 'S13': D13 / D, 'S23': 0.0\n    }\n\ndef pce_sobol(a, b):\n    \"\"\"\n    Computes the Sobol' indices based on a 3rd-order PCE.\n    \"\"\"\n    c100, c300, c020, c102 = get_pce_coefficients(a, b)\n\n    # PCE-based partial variances\n    D1_pce = c100**2 + c300**2\n    D2_pce = c020**2\n    D13_pce = c102**2\n    \n    # Total variance from the truncated PCE\n    D_pce = D1_pce + D2_pce + D13_pce\n\n    if D_pce == 0:\n       return {'S1': 0.0, 'S2': 0.0, 'S3': 0.0, 'S12': 0.0, 'S13': 0.0, 'S23': 0.0}\n\n    return {\n        'S1': D1_pce / D_pce, 'S2': D2_pce / D_pce, 'S3': 0.0,\n        'S12': 0.0, 'S13': D13_pce / D_pce, 'S23': 0.0\n    }\n\ndef solve():\n    \"\"\"\n    Main function to execute the validation for the given test suite.\n    \"\"\"\n    test_cases = [\n        (7.0, 0.1),\n        (7.0, 0.0),\n        (0.0, 0.1),\n        (0.0, 0.0)\n    ]\n\n    results = []\n    \n    index_keys = ['S1', 'S2', 'S3', 'S12', 'S13', 'S23']\n\n    for a, b in test_cases:\n        analytical_indices = analytical_sobol(a, b)\n        pce_indices = pce_sobol(a, b)\n        \n        abs_diffs = [\n            np.abs(analytical_indices[key] - pce_indices[key]) for key in index_keys\n        ]\n        \n        max_abs_diff = max(abs_diffs)\n        results.append(max_abs_diff)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}