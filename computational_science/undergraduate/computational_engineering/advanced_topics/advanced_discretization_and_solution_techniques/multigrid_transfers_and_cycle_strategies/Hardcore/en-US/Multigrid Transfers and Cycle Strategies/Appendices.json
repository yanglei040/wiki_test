{
    "hands_on_practices": [
        {
            "introduction": "Before we delve into the practicalities of designing smoothers, it's crucial to understand their ideal role within a multigrid cycle. This exercise is a thought experiment that asks you to consider a hypothetical scenario: what if your smoother was a perfect inverse of the system operator, $S = A^{-1}$? By analyzing this idealized case , you will uncover the fundamental purpose of the smoothing step and gain a clear insight into why the subsequent components of the V-cycle—residual calculation, restriction, and coarse-grid correction—are necessary when using practical, non-exact smoothers.",
            "id": "2416049",
            "problem": "Consider a linear system $A u = b$ arising from a symmetric positive definite discretization, where $A \\in \\mathbb{R}^{n \\times n}$, $u \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{n}$. A multigrid $V$-cycle is constructed with a single pre-smoothing step and a single post-smoothing step on each level, arbitrary restriction $R$ and prolongation $P$ operators between adjacent levels, and the coarse-grid operator defined by the Galerkin product $A_{H} = R A P$. The coarsest-grid problem is solved exactly. The smoothing update on any level is given by\n$$\nu \\leftarrow u + S \\left(b - A u\\right),\n$$\nwhere $S$ is a fixed linear operator on that level. Define the exact solution $u^{\\star}$ by $A u^{\\star} = b$ and the error $e = u - u^{\\star}$. The asymptotic convergence factor of the $V$-cycle is defined as the spectral radius $\\rho(E)$ of the linear error-propagation operator $E$ for one complete $V$-cycle applied to any initial error.\n\nAssume now that on every level the smoother is the exact inverse, i.e., $S = A^{-1}$ on that level. Determine the asymptotic convergence factor $\\rho(E)$. Provide your answer as a single real number. No rounding is required, and no units are involved.",
            "solution": "The analysis of the V-cycle's error propagation begins with the pre-smoothing step. The smoothing update rule is $u_{\\text{new}} = u_{\\text{old}} + S(b - A u_{\\text{old}})$. The error $e = u - u^{\\star}$ (where $u^{\\star}$ is the exact solution) transforms as $e_{\\text{new}} = (I - SA)e_{\\text{old}}$.\n\nThe problem states that the smoother is the exact inverse of the system matrix on every level, i.e., $S = A^{-1}$. Substituting this into the error propagation operator for a single smoothing step, $K = I - SA$, yields:\n$$\nK = I - A^{-1}A = I - I = 0\n$$\nThis means that the single pre-smoothing step annihilates the error completely, making the current iterate $u$ equal to the exact solution $u^{\\star}$.\n\nSince the iterate is exact after pre-smoothing, the residual becomes zero. Consequently, the coarse-grid correction is zero, and the post-smoothing step has no effect. The final error after one complete V-cycle is zero, regardless of the initial error.\n\nThe error propagation operator for the entire V-cycle, $E$, is therefore the zero operator ($E=0$). The asymptotic convergence factor is its spectral radius, $\\rho(E)$, which is:\n$$\n\\rho(E) = \\rho(0) = 0\n$$",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "A multigrid solver is only as effective as its ability to transfer information between grids. This hands-on practice focuses on the restriction operator, which maps the fine-grid residual to the coarse grid. Instead of using a standard operator, you will implement one with a deliberately 'incorrect' stencil whose weights sum to zero . By observing how this operator affects different frequency components of the residual, you will gain a powerful, practical understanding of the core design principle of restriction: to faithfully represent low-frequency error components on the coarse grid.",
            "id": "2416021",
            "problem": "You are given a one-dimensional discrete model of the Poisson equation on a periodic domain. Let the domain be the unit circle, discretized by $n$ equally spaced points with spacing $h = 1/n$, where $n$ is an even integer. Define the discrete negative Laplace operator $A \\in \\mathbb{R}^{n \\times n}$ with periodic boundary conditions by\n$$(A \\mathbf{u})_i = \\frac{2 u_i - u_{i-1} - u_{i+1}}{h^2}, \\quad i \\in \\{0,1,\\dots,n-1\\},$$\nwhere indexing is taken modulo $n$. For any discrete field $\\mathbf{u} \\in \\mathbb{R}^n$ and forcing $\\mathbf{f} \\in \\mathbb{R}^n$, define the residual $\\mathbf{r}(\\mathbf{u}) = \\mathbf{f} - A \\mathbf{u}$. Consider the restriction operator $R: \\mathbb{R}^n \\to \\mathbb{R}^{n/2}$ with a three-point stencil whose weights sum to zero, specified by\n$$(R \\mathbf{r})_I = w_{-1}\\, r_{2I-1} + w_0\\, r_{2I} + w_{+1}\\, r_{2I+1}, \\quad I \\in \\{0,1,\\dots,\\tfrac{n}{2}-1\\},$$\nwith periodic indexing and weights $(w_{-1}, w_0, w_{+1}) = (1,-2,1)$, so that $w_{-1} + w_0 + w_{+1} = 0$.\n\nYour task is to implement a program that, for the specified test suite below, constructs the residual $\\mathbf{r}(\\mathbf{u})$ for the initial guess $\\mathbf{u} = \\mathbf{0}$ and then computes the scalar quantity\n$$\\gamma = \\frac{\\lVert R \\mathbf{r}(\\mathbf{0}) \\rVert_2}{\\lVert \\mathbf{r}(\\mathbf{0}) \\rVert_2}.$$\nAngles used in trigonometric definitions must be in radians. No physical units are involved; the quantity $\\gamma$ is dimensionless.\n\nTest Suite:\n- Case $1$: $n = 64$, $\\mathbf{f}$ is the constant vector with entries $f_i = 1$ for all $i$.\n- Case $2$: $n = 64$, $\\mathbf{f}$ is defined by $f_i = \\sin(2 \\pi x_i)$ with $x_i = i h$ and $h = 1/n$.\n- Case $3$: $n = 64$, $\\mathbf{f}$ is defined by $f_i = (-1)^i$ for all $i$.\n- Case $4$: $n = 8$, $\\mathbf{f}$ is the constant vector with entries $f_i = 1$ for all $i$.\n\nFor each case, the program must output the corresponding value of $\\gamma$ as a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[a,b,c,d]\"), in the order of the test cases above. The answers must be floats. No user input is required, and the program must be fully self-contained and deterministic.",
            "solution": "The quantity to compute is $\\gamma = \\frac{\\lVert R \\mathbf{r}(\\mathbf{0}) \\rVert_2}{\\lVert \\mathbf{r}(\\mathbf{0}) \\rVert_2}$. Since the initial guess is $\\mathbf{u} = \\mathbf{0}$, the residual $\\mathbf{r}(\\mathbf{0}) = \\mathbf{f} - A\\mathbf{0} = \\mathbf{f}$. The expression for $\\gamma$ simplifies to:\n$$\n\\gamma = \\frac{\\lVert R \\mathbf{f} \\rVert_2}{\\lVert \\mathbf{f} \\rVert_2}\n$$\nThe components of the restricted vector $\\mathbf{v} = R\\mathbf{f}$ are given by the stencil $(1, -2, 1)$:\n$$\nv_I = (R\\mathbf{f})_I = f_{2I-1} - 2f_{2I} + f_{2I+1}\n$$\nwith periodic indexing. This stencil is a discrete approximation of the second derivative, so it acts as a high-pass filter.\n\n**Case 1 & 4: Constant Forcing Vector**\nFor a constant vector $f_i = c$, the forcing vector represents the lowest frequency (zero frequency). The restriction yields:\n$$\n(R\\mathbf{f})_I = c - 2c + c = 0\n$$\nThe restricted vector $R\\mathbf{f}$ is the zero vector, so its norm is $0$. Thus, $\\gamma = 0$ for both Case 1 and Case 4.\n\n**Case 3: High-Frequency Forcing Vector**\nFor $f_i = (-1)^i$, which is the highest frequency mode on the grid, the components are:\n$$\n(R\\mathbf{f})_I = f_{2I-1} - 2f_{2I} + f_{2I+1} = (-1) - 2(1) + (-1) = -4\n$$\nThe restricted vector $R\\mathbf{f}$ is a constant vector of size $n/2$ where every entry is $-4$.\n- $\\lVert \\mathbf{f} \\rVert_2 = \\sqrt{\\sum_{i=0}^{n-1} ((-1)^i)^2} = \\sqrt{n}$. For $n=64$, this is $8$.\n- $\\lVert R\\mathbf{f} \\rVert_2 = \\sqrt{\\sum_{I=0}^{n/2-1} (-4)^2} = \\sqrt{(n/2) \\cdot 16} = \\sqrt{8n}$. For $n=64$, this is $\\sqrt{512} = 16\\sqrt{2}$.\nThe ratio is $\\gamma = \\frac{\\sqrt{8n}}{\\sqrt{n}} = \\sqrt{8} = 2\\sqrt{2}$.\n\n**Case 2: Low-Frequency Forcing Vector**\nFor $f_i = \\sin(2 \\pi i / n)$, a low frequency mode, the restriction operator's components are:\n$$\n(R\\mathbf{f})_I = \\sin\\left(\\frac{2\\pi(2I-1)}{n}\\right) - 2\\sin\\left(\\frac{2\\pi(2I)}{n}\\right) + \\sin\\left(\\frac{2\\pi(2I+1)}{n}\\right)\n$$\nThis is a finite difference approximation of the second derivative. The operator strongly attenuates this low-frequency signal. The exact calculation yields:\n$$\n(R\\mathbf{f})_I = 2\\sin\\left(\\frac{4\\pi I}{n}\\right)\\left(\\cos\\left(\\frac{2\\pi}{n}\\right)-1\\right) = -4 \\sin^2\\left(\\frac{\\pi}{n}\\right) \\sin\\left(\\frac{4\\pi I}{n}\\right)\n$$\nThe norm of this vector is very small, proportional to $\\sin^2(\\pi/n) \\approx (\\pi/n)^2$. The code in the answer computes the exact numerical value of the ratio $\\gamma$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_gamma(n, f_vector):\n    \"\"\"\n    Calculates the ratio gamma for a given grid size n and forcing vector f.\n\n    The quantity is defined as gamma = ||R*r||_2 / ||r||_2, where r = f for u=0.\n    The restriction operator R is defined by (R*r)_I = r_{2I-1} - 2*r_{2I} + r_{2I+1}.\n\n    Args:\n        n (int): The number of grid points, must be even.\n        f_vector (np.ndarray): The forcing vector of size n.\n\n    Returns:\n        float: The computed value of gamma.\n    \"\"\"\n    \n    # For u=0, the residual r is simply the forcing vector f.\n    r = f_vector\n    \n    # Calculate the denominator: the L2 norm of the residual vector.\n    norm_r = np.linalg.norm(r)\n    \n    # If the norm of the residual is zero, gamma is undefined or 0.\n    # In this context (non-zero f), we can treat it as 0.\n    if norm_r == 0:\n        return 0.0\n\n    # The coarse grid has n/2 points.\n    n_coarse = n // 2\n    restricted_r = np.zeros(n_coarse)\n\n    # Apply the restriction operator R to the residual vector r.\n    # (R*r)_I = r_{2I-1} - 2*r_{2I} + r_{2I+1}, with periodic indexing.\n    for I in range(n_coarse):\n        # Python's % operator correctly handles negative numbers for periodic indexing.\n        # e.g., -1 % 64 = 63.\n        idx_minus_1 = (2 * I - 1) % n\n        idx_0 = (2 * I) % n\n        idx_plus_1 = (2 * I + 1) % n\n\n        restricted_r[I] = r[idx_minus_1] - 2 * r[idx_0] + r[idx_plus_1]\n    \n    # Calculate the numerator: the L2 norm of the restricted residual.\n    norm_restricted_r = np.linalg.norm(restricted_r)\n    \n    # Compute the final ratio.\n    gamma = norm_restricted_r / norm_r\n    \n    return gamma\n\ndef solve():\n    \"\"\"\n    Solves the problem for the four specified test cases and prints the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases_params = [\n        {'n': 64, 'id': 1},\n        {'n': 64, 'id': 2},\n        {'n': 64, 'id': 3},\n        {'n': 8,  'id': 4},\n    ]\n\n    results = []\n    for params in test_cases_params:\n        n = params['n']\n        case_id = params['id']\n        f = None\n\n        if case_id == 1: # n=64, f_i = 1\n            f = np.ones(n)\n        elif case_id == 2: # n=64, f_i = sin(2*pi*x_i)\n            h = 1.0 / n\n            x = np.arange(n) * h\n            f = np.sin(2 * np.pi * x)\n        elif case_id == 3: # n=64, f_i = (-1)^i\n            f = np.power(-1.0, np.arange(n))\n        elif case_id == 4: # n=8, f_i = 1\n            f = np.ones(n)\n        \n        gamma = calculate_gamma(n, f)\n        results.append(gamma)\n\n    # Final print statement in the exact required format.\n    # The format specifier ensures that the numbers are printed as floats (e.g., 0.0).\n    print(f\"[{','.join(map(str, [float(r) for r in results]))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a solid understanding of smoothers and transfer operators, we can now turn our attention to optimizing the overall cycle strategy. Standard V-cycles often use a fixed number of smoothing steps on every grid level, but is this always the most efficient approach? This practice challenges you to implement a more flexible multigrid solver where the amount of smoothing, $\\nu(l)$, varies with the grid level $l$ . By experimenting with different smoothing schedules and cycle types (V- and W-cycles), you will explore how to intelligently distribute computational work across the grid hierarchy to achieve faster convergence.",
            "id": "2416031",
            "problem": "You are asked to design and implement a variable-smoothing multigrid cycle, where the number of smoothing steps depends on the grid level. Work in one spatial dimension and consider the boundary value problem given by the Poisson equation with homogeneous Dirichlet boundary conditions on the unit interval, discretized by second-order centered finite differences. The resulting linear system is symmetric positive definite and reads, at each interior grid point, as the action of a tridiagonal operator. Your implementation must start from the following fundamental base:\n- The continuous model is the one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions, written as $-u''(x)=g(x)$ for $x\\in(0,1)$, with $u(0)=u(1)=0$.\n- The second-order centered finite difference approximation on a uniform grid with spacing $h$ leads to a linear system $A^{(l)} u^{(l)} = f^{(l)}$ on level $l$, where $A^{(l)}$ is the tridiagonal operator with entries $2/h_l^2$ on the diagonal and $-1/h_l^2$ on the immediate off-diagonals, and $h_l = 1/(n_l+1)$ with $n_l$ interior unknowns on level $l$.\n- Weighted Jacobi smoothing is defined by the iteration $u^{(l)} \\leftarrow u^{(l)} + \\omega \\left(D^{(l)}\\right)^{-1}\\left(f^{(l)} - A^{(l)} u^{(l)}\\right)$ for a chosen relaxation parameter $\\omega$, where $D^{(l)}$ is the diagonal of $A^{(l)}$.\n- Restriction and prolongation between adjacent levels use standard full-weighting restriction and linear interpolation prolongation, respectively. If $r^{(l)}$ is a fine-grid vector with $n_l = 2 n_{l+1} + 1$, then restriction is $(r^{(l+1)})_j = \\frac{1}{4} r^{(l)}_{2j} + \\frac{1}{2} r^{(l)}_{2j+1} + \\frac{1}{4} r^{(l)}_{2j+2}$ for $j=0,\\dots,n_{l+1}-1$, and prolongation is given by injection at odd indices and linear interpolation at even indices.\n- A V-cycle is implemented with one recursive coarse-grid correction per visit, while a W-cycle performs two recursive coarse-grid corrections per visit. Denote the cycle index by $\\gamma$, with $\\gamma=1$ for a V-cycle and $\\gamma=2$ for a W-cycle.\n\nYour task is to implement a multigrid cycle where the number of smoothing steps $ \\nu(l)$ depends on the grid level $ l$. The level numbering must be $l=0$ at the finest grid and $l=L-1$ at the coarsest grid, where $L$ is the number of levels. Use the same number of pre-smoothing and post-smoothing steps at each level, equal to $\\nu(l)$.\n\nThe hierarchy is constructed from a finest grid with $N$ interior points where $N = 2^L - 1$ for some integer $L \\ge 2$. The coarsest-level problem must be solved exactly by a direct solve of the corresponding tridiagonal linear system. Take the right-hand side to be constant, i.e., $g(x)\\equiv 1$, which corresponds to $f^{(0)}_i = 1$ for all interior $i$ on the finest level, and use homogeneous Dirichlet boundary conditions. Initialize the finest-level approximation $u^{(0)}$ to the zero vector.\n\nDefine the smoothing schedule $\\nu(l)$ through one of the following families, in each case ensuring $\\nu(l)$ is an integer and at least $1$:\n- Constant: $\\nu(l) = \\mathrm{round}(c)$.\n- Linear: $\\nu(l) = \\max\\{1,\\ \\mathrm{round}(a + b\\,l)\\}$.\n- Geometric: $\\nu(l) = \\max\\{1,\\ \\mathrm{round}(a\\,b^{\\,l})\\}$.\n\nUse weighted Jacobi smoothing with relaxation parameter $\\omega = 2/3$.\n\nFor a given choice of $N$, $\\gamma$, the family for $\\nu(l)$, and its parameters, run $K$ cycles starting from the zero initial guess $u^{(0)}=0$ on the finest level. Let $\\lVert r_k \\rVert_2$ denote the Euclidean norm of the finest-level residual after $k$ completed cycles, and let $\\lVert r_0 \\rVert_2$ be the norm at the start (with $u^{(0)}=0$). Report the observed average per-cycle reduction factor\n$$\n\\rho_\\mathrm{obs} = \\left(\\frac{\\lVert r_K \\rVert_2}{\\lVert r_0 \\rVert_2}\\right)^{1/K}.\n$$\nYour program must compute $\\rho_\\mathrm{obs}$ for each test case below and print a single line with the results as a comma-separated list enclosed in square brackets, with each number rounded to six digits after the decimal point. No physical units are required.\n\nTest suite:\n- Case $1$ (happy path): $N=63$, $\\gamma=1$ (V-cycle), constant $\\nu(l)$ with $c=2$, and $K=8$.\n- Case $2$ (level-dependent coarse-heavy): $N=63$, $\\gamma=1$ (V-cycle), linear $\\nu(l)$ with $a=1$, $b=1$, and $K=8$.\n- Case $3$ (W-cycle with stronger coarse smoothing): $N=63$, $\\gamma=2$ (W-cycle), linear $\\nu(l)$ with $a=1$, $b=2$, and $K=6$.\n- Case $4$ (geometric growth in smoothing and larger finest grid): $N=127$, $\\gamma=1$ (V-cycle), geometric $\\nu(l)$ with $a=1$, $b=1.5$, and $K=6$.\n\nAlgorithmic requirements to enforce scientific realism:\n- Use the exact tridiagonal operator $A^{(l)}$ implied by the centered finite difference stencil $(-1,2,-1)/h_l^2$ on each level to form residuals and to solve on the coarsest level.\n- Use full-weighting restriction and linear interpolation prolongation between adjacent levels as specified above.\n- Use weighted Jacobi with $\\omega=2/3$ for both pre-smoothing and post-smoothing, with identical counts $\\nu(l)$ at each level.\n- Implement both V-cycle and W-cycle using $\\gamma$ as defined.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[\\rho_1,\\rho_2,\\rho_3,\\rho_4]$, where each $\\rho_i$ is rounded to six digits after the decimal point. The program must read no input and must produce only this single output line.",
            "solution": "The problem requires the design and implementation of a multigrid method for the one-dimensional Poisson equation, $-u''(x) = g(x)$ on $x \\in (0, 1)$ with homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. The key feature is a variable smoothing strategy, where the number of smoothing iterations depends on the grid level.\n\nFirst, we discretize the problem. We establish a hierarchy of grids, indexed by level $l$, where $l=0$ corresponds to the finest grid and $l=L-1$ to the coarsest. The finest grid has $N$ interior points, where $N=2^L-1$ for some integer $L \\ge 2$. A standard coarsening strategy is used, where the number of interior points $n_l$ on level $l$ is given by $n_l = 2^{L-l}-1$. Consequently, the number of points on the next coarser level is $n_{l+1} = (n_l-1)/2$. The grid spacing on level $l$ is $h_l = 1/(n_l+1)$.\n\nUsing a second-order centered finite difference approximation for the second derivative, we obtain a system of linear equations for the unknown values $u^{(l)}$ at the interior grid points: $A^{(l)} u^{(l)} = f^{(l)}$. The matrix $A^{(l)}$ is an $n_l \\times n_l$ symmetric positive definite and tridiagonal matrix with entries:\n$$\nA^{(l)}_{i,j} = \\frac{1}{h_l^2}\n\\begin{cases}\n2, & i=j \\\\\n-1, & |i-j|=1 \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nThe right-hand side vector $f^{(l)}$ on the finest level is derived from $g(x) \\equiv 1$, so $f^{(0)}_i = 1$ for all $i$. On coarser levels, $f^{(l)}$ will be the restricted residual from the next finer level.\n\nThe core of the multigrid method is a recursive cycle. A single cycle on level $l$ to approximate the solution of $A^{(l)} u^{(l)} = f^{(l)}$ consists of three main steps:\n\n1.  **Pre-smoothing**: The current approximation $u^{(l)}$ is improved by applying a smoothing operator. We use $\\nu(l)$ iterations of the weighted Jacobi method:\n    $$\n    u^{(l)} \\leftarrow u^{(l)} + \\omega (D^{(l)})^{-1} (f^{(l)} - A^{(l)} u^{(l)})\n    $$\n    Here, $D^{(l)}$ is the diagonal of $A^{(l)}$, so $(D^{(l)})^{-1}_{ii} = h_l^2/2$. The relaxation parameter is given as $\\omega=2/3$. The number of smoothing steps, $\\nu(l)$, is a function of the level $l$ defined by one of the provided constant, linear, or geometric schedules.\n\n2.  **Coarse-Grid Correction**: This step solves for the error on a coarser grid, where the problem is smaller and computationally cheaper.\n    a. Compute the residual on the fine grid: $r^{(l)} = f^{(l)} - A^{(l)} u^{(l)}$.\n    b. Restrict the residual to the next coarser grid, $l+1$: $r^{(l+1)} = R^{(l \\to l+1)} r^{(l)}$. The restriction operator $R^{(l \\to l+1)}$ is the full-weighting operator, which in one dimension has the stencil $[1/4, 1/2, 1/4]$. Its action on a fine-grid vector $v^{(l)}$ is:\n    $$\n    (v^{(l+1)})_j = \\frac{1}{4} v^{(l)}_{2j} + \\frac{1}{2} v^{(l)}_{2j+1} + \\frac{1}{4} v^{(l)}_{2j+2}, \\quad j = 0, \\dots, n_{l+1}-1.\n    $$\n    c. Solve the coarse-grid residual equation $A^{(l+1)} e^{(l+1)} = r^{(l+1)}$. If level $l+1$ is the coarsest grid ($l+1 = L-1$), this equation is solved directly. In our case, the coarsest grid has $n_{L-1}=1$ unknown, so this is a trivial scalar equation. If level $l+1$ is not the coarsest, the equation is solved recursively by applying $\\gamma$ multigrid cycles starting with an initial guess of $e^{(l+1)}=0$. For a V-cycle, $\\gamma=1$; for a W-cycle, $\\gamma=2$.\n    d. Prolongate the coarse-grid error correction back to the fine grid: $e^{(l)} = P^{(l+1 \\to l)} e^{(l+1)}$. The prolongation operator $P^{(l+1 \\to l)}$ is linear interpolation. For a coarse-grid vector $v^{(l+1)}$, its action is defined by injection at odd fine-grid indices and averaging at even indices, respecting the zero boundary conditions:\n    $$\n    (v^{(l)})_{2j+1} = v^{(l+1)}_j \\\\\n    (v^{(l)})_{2j} = \\frac{1}{2} (v^{(l+1)}_{j-1} + v^{(l+1)}_j)\n    $$\n    where boundary values are taken as zero (e.g., $v^{(l+1)}_{-1} = 0$).\n    e. Update the fine-grid solution: $u^{(l)} \\leftarrow u^{(l)} + e^{(l)}$.\n\n3.  **Post-smoothing**: To damp any high-frequency errors introduced by the prolongation step, we again apply $\\nu(l)$ iterations of the weighted Jacobi smoother to the updated approximation $u^{(l)}$.\n\nThe overall process is initiated on the finest level ($l=0$) with an initial guess of $u^{(0)}=0$. After performing a specified number of cycles, $K$, the convergence is assessed. The performance is measured by the average per-cycle reduction factor in the Euclidean norm of the residual, $\\rho_{\\text{obs}}$. This is calculated from the initial residual norm $\\lVert r_0 \\rVert_2 = \\lVert f^{(0)} - A^{(0)} u^{(0)}_0 \\rVert_2$ and the final residual norm after $K$ cycles, $\\lVert r_K \\rVert_2 = \\lVert f^{(0)} - A^{(0)} u^{(0)}_K \\rVert_2$, using the formula:\n$$\n\\rho_{\\mathrm{obs}} = \\left(\\frac{\\lVert r_K \\rVert_2}{\\lVert r_0 \\rVert_2}\\right)^{1/K}\n$$\nThe implementation will construct the necessary operators and execute the recursive cycle for each of the specified test cases, calculating $\\rho_{\\mathrm{obs}}$ accordingly.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags\n\n# ===== Multigrid Component Implementations =====\n\ndef create_operator_A(n):\n    \"\"\"Creates the 1D Poisson matrix for a grid with n interior points.\"\"\"\n    h = 1.0 / (n + 1)\n    main_diag = np.full(n, 2.0 / h**2)\n    off_diag = np.full(n - 1, -1.0 / h**2)\n    return diags([off_diag, main_diag, off_diag], [-1, 0, 1], format='csr')\n\ndef apply_smoother(u, f, A, nu, omega):\n    \"\"\"Applies nu steps of the weighted Jacobi smoother.\"\"\"\n    u_new = np.copy(u)\n    # The diagonal of A is constant, so D_inv is a scalar multiplication.\n    # D_ii = 2 / h^2, so D_inv_ii = h^2 / 2\n    n = len(u)\n    if n == 0:\n        return u_new\n    h_sq = (1.0 / (n + 1))**2\n    D_inv_val = h_sq / 2.0\n    \n    for _ in range(nu):\n        residual = f - A @ u_new\n        u_new += omega * D_inv_val * residual\n    return u_new\n\ndef apply_restriction(r_fine):\n    \"\"\"Applies full-weighting restriction.\"\"\"\n    # Stencil is [1/4, 1/2, 1/4]\n    n_fine = len(r_fine)\n    n_coarse = (n_fine - 1) // 2\n    if n_coarse == 0:\n        return np.array([])\n    \n    r_coarse = np.zeros(n_coarse)\n    # This loop is clearer than vectorization for this staggered stencil.\n    for j in range(n_coarse):\n        r_coarse[j] = 0.25 * r_fine[2*j] + 0.5 * r_fine[2*j+1] + 0.25 * r_fine[2*j+2]\n    return r_coarse\n\ndef apply_prolongation(e_coarse, n_fine):\n    \"\"\"Applies linear interpolation prolongation.\"\"\"\n    n_coarse = len(e_coarse)\n    e_fine = np.zeros(n_fine)\n    \n    # Injection at odd indices\n    e_fine[1::2] = e_coarse\n    \n    # Interpolation at even indices (including boundaries)\n    padded_coarse = np.concatenate(([0], e_coarse, [0]))\n    e_fine[0::2] = 0.5 * (padded_coarse[:-1] + padded_coarse[1:])\n    \n    return e_fine\n\ndef calculate_nu(level, family, params):\n    \"\"\"Calculates the number of smoothing steps for a given level.\"\"\"\n    if family == 'constant':\n        val = params['c']\n    elif family == 'linear':\n        val = params['a'] + params['b'] * level\n    elif family == 'geometric':\n        val = params['a'] * (params['b'] ** level)\n    else:\n        raise ValueError(f\"Unknown smoothing schedule family: {family}\")\n    \n    return max(1, int(np.round(val)))\n\ndef mg_cycle(level, u, f, grid_params):\n    \"\"\"Performs one recursive multigrid cycle.\"\"\"\n    ns, As, nus, gamma, omega = grid_params\n    \n    # Base case: on the coarsest level, solve exactly\n    if level == len(ns) - 1:\n        # For n=1, this is a 1x1 system A u = f -> u = f / A[0,0]\n        # For n=0, this is an empty system, return empty array.\n        if ns[level] > 0:\n            return f / As[level][0, 0]\n        else:\n            return np.array([])\n\n\n    # 1. Pre-smoothing\n    u = apply_smoother(u, f, As[level], nus[level], omega)\n\n    # 2. Coarse-grid correction\n    residual_fine = f - As[level] @ u\n    residual_coarse = apply_restriction(residual_fine)\n    \n    error_coarse = np.zeros_like(residual_coarse)\n    \n    # Recursive calls to solve the coarse-grid error equation\n    for _ in range(gamma):\n        error_coarse = mg_cycle(level + 1, error_coarse, residual_coarse, grid_params)\n        \n    error_fine = apply_prolongation(error_coarse, ns[level])\n    u += error_fine\n\n    # 3. Post-smoothing\n    u = apply_smoother(u, f, As[level], nus[level], omega)\n\n    return u\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        {'N': 63, 'gamma': 1, 'nu_family': 'constant', 'nu_params': {'c': 2}, 'K': 8},\n        {'N': 63, 'gamma': 1, 'nu_family': 'linear', 'nu_params': {'a': 1, 'b': 1}, 'K': 8},\n        {'N': 63, 'gamma': 2, 'nu_family': 'linear', 'nu_params': {'a': 1, 'b': 2}, 'K': 6},\n        {'N': 127, 'gamma': 1, 'nu_family': 'geometric', 'nu_params': {'a': 1, 'b': 1.5}, 'K': 6},\n    ]\n\n    results = []\n    omega = 2.0 / 3.0\n\n    for case in test_cases:\n        N = case['N']\n        gamma = case['gamma']\n        nu_family = case['nu_family']\n        nu_params = case['nu_params']\n        K = case['K']\n\n        # 1. Setup grid hierarchy\n        L = int(np.log2(N + 1))\n        ns = [2**(L - l) - 1 for l in range(L)]\n        As = [create_operator_A(n) for n in ns if n > 0]\n        # Adjust ns to match the operators we actually created\n        ns = [n for n in ns if n > 0] \n        nus = [calculate_nu(l, nu_family, nu_params) for l in range(len(ns))]\n        \n        grid_params = (ns, As, nus, gamma, omega)\n\n        # 2. Initial state\n        u = np.zeros(N)\n        f = np.ones(N)\n\n        # 3. Calculate initial residual norm\n        r0_norm = np.linalg.norm(f) # Since u is zero, initial residual is f\n\n        # 4. Run K multigrid cycles\n        u_k = u\n        for _ in range(K):\n            u_k = mg_cycle(0, u_k, f, grid_params)\n\n        # 5. Calculate final residual norm and convergence factor\n        rK_norm = np.linalg.norm(f - As[0] @ u_k)\n        \n        if r0_norm == 0:\n             rho_obs = 0.0 if rK_norm == 0 else float('inf')\n        else:\n             rho_obs = (rK_norm / r0_norm)**(1.0 / K)\n        \n        results.append(f\"{rho_obs:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}