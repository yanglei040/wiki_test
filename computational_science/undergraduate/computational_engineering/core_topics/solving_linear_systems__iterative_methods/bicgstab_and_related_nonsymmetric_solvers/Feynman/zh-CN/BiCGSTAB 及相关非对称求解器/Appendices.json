{
    "hands_on_practices": [
        {
            "introduction": "理论知识需要通过实践来巩固。本练习旨在揭开 BiCGSTAB 算法的神秘面纱，引导您在一个简单的 $2 \\times 2$ 线性系统上进行完整的、一步一步的计算。通过在精确算术下追踪每一步的向量和标量更新，您将对算法的内部运作机制建立起坚实的直观理解 。",
            "id": "2374444",
            "problem": "您将应用双共轭梯度稳定 (BiCGSTAB) 方法，全称为 BiConjugate Gradient Stabilized (BiCGSTAB)，于一个非对称线性系统，以明确说明，在精确算术和标准参数选择下，从零初始猜测开始，该方法需要两次迭代才能收敛。考虑具有以下条件的线性系统：\n- 矩阵 $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$，\n- 右端项 $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，\n- 初始猜测 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，\n- 单位预条件子，以及\n- 影子残差 $\\hat{r} = r_0$，其中 $r_0 = b - A x_0$。\n\n所有内积均为标准欧几里得内积。请在精确算术下进行计算（即不要对中间量进行四舍五入）。从定义 BiCGSTAB 递推关系的核心定义（残差、搜索方向和标量系数）开始，逐步执行 BiCGSTAB 迭代，并证明：\n1. 第一次迭代后，残差不为零。\n2. 第二次迭代后，方法达到精确解（残差等于零向量）。\n\n为证实这些论断，请明确计算并报告 BiCGSTAB 直至收敛所使用的所有中间向量和标量。最后，提供在此系统的第一次 BiCGSTAB 迭代期间产生的第一个平滑参数 $\\omega_1$ 的精确值。将您的最终答案表示为一个精确的有理数。请勿四舍五入。",
            "solution": "对问题陈述进行验证。\n\n**第 1 步：提取已知条件**\n- **方法：** 双共轭梯度稳定 (BiCGSTAB)\n- **线性系统：** $A x = b$\n- **矩阵：** $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$\n- **右端项：** $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- **初始猜测：** $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **预条件子：** 单位矩阵 ($M=I$)\n- **影子残差：** $\\hat{r}_0 = r_0$，其中 $r_0 = b - A x_0$\n- **内积：** 标准欧几里得内积，记为 $(u, v) = u^T v$。\n- **算术：** 要求使用精确算术。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学依据：** 该问题是 BiCGSTAB 算法的直接和标准应用，该算法是数值线性代数中用于求解非对称线性系统的成熟方法。它在科学上是合理的。\n- **适定性：** 矩阵 $A$ 的行列式为 $\\det(A) = (1)(4) - (2)(3) = 4 - 6 = -2 \\neq 0$。因此，$A$ 是可逆的，线性系统 $Ax=b$ 有唯一解。BiCGSTAB 算法是一个定义明确的程序。对于一个大小为 $N \\times N$ 的系统，像 BiCGSTAB 这样的克雷洛夫子空间方法在精确算术下保证最多在 $N$ 次迭代内找到精确解。对于这个 $2 \\times 2$ 的系统，恰好在 2 次迭代内收敛是一个合理且可验证的论断。\n- **客观性：** 该问题使用精确的数学语言陈述，没有任何主观性或模糊性。\n\n**结论：** 该问题是有效的，因为它具有科学依据、适定性和客观性。它是计算工程领域内一个可形式化的问题。我们可以着手求解。\n\n将通过逐步应用 BiCGSTAB 算法来构建解。算法如下：\n\n**初始化：**\n1. 给定初始猜测 $x_0$。\n2. 计算初始残差 $r_0 = b - Ax_0$。\n3. 选择一个影子残差向量 $\\hat{r}_0$，使得 $(\\hat{r}_0, r_0) \\neq 0$。这里，给定 $\\hat{r}_0 = r_0$。\n4. 为递推设置初始参数：$\\rho_0 = 1$，$\\alpha_0 = 1$，$\\omega_0 = 1$。\n5. 设置初始搜索方向：$p_0 = \\mathbf{0}$，$v_0 = \\mathbf{0}$。\n\n**主循环 (对 $k = 1, 2, \\dots$)：**\n1. $\\rho_k = (\\hat{r}_0, r_{k-1})$\n2. $\\beta_k = \\frac{\\rho_k}{\\rho_{k-1}} \\frac{\\alpha_{k-1}}{\\omega_{k-1}}$\n3. $p_k = r_{k-1} + \\beta_k (p_{k-1} - \\omega_{k-1} v_{k-1})$\n4. $v_k = A p_k$\n5. $\\alpha_k = \\frac{\\rho_k}{(\\hat{r}_0, v_k)}$\n6. $s_k = r_{k-1} - \\alpha_k v_k$\n7. $t_k = A s_k$\n8. $\\omega_k = \\frac{(t_k, s_k)}{(t_k, t_k)}$\n9. $x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s_k$\n10. $r_k = s_k - \\omega_k t_k$\n\n我们现在将此算法应用于给定的系统。\n\n**初始化 ($k=0$)：**\n- 系统由 $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$ 和 $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 定义。\n- 初始猜测为 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n- 初始残差为 $r_0 = b - Ax_0 = b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n- 影子残差为 $\\hat{r}_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n- 初始参数设置为 $\\rho_0 = 1$，$\\alpha_0 = 1$，$\\omega_0 = 1$。\n- 初始方向向量为 $p_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $v_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n\n**迭代 1 ($k=1$)：**\n1. $\\rho_1 = (\\hat{r}_0, r_0) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$。\n2. $\\beta_1 = \\frac{\\rho_1}{\\rho_0} \\frac{\\alpha_0}{\\omega_0} = \\frac{1}{1} \\frac{1}{1} = 1$。\n3. $p_1 = r_0 + \\beta_1(p_0 - \\omega_0 v_0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 1 \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n4. $v_1 = A p_1 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$。\n5. $\\alpha_1$ 的分母为 $(\\hat{r}_0, v_1) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = 1$。因此，$\\alpha_1 = \\frac{\\rho_1}{(\\hat{r}_0, v_1)} = \\frac{1}{1} = 1$。\n6. $s_1 = r_0 - \\alpha_1 v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$。\n7. $t_1 = A s_1 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix}$。\n8. 对于 $\\omega_1$，我们计算内积：\n   $(t_1, s_1) = \\begin{pmatrix} -6 & -12 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = (0)(-6) + (-3)(-12) = 36$。\n   $(t_1, t_1) = \\begin{pmatrix} -6 & -12 \\end{pmatrix} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = (-6)^2 + (-12)^2 = 36 + 144 = 180$。\n   所以，$\\omega_1 = \\frac{(t_1, s_1)}{(t_1, t_1)} = \\frac{36}{180} = \\frac{1}{5}$。\n9. $x_1 = x_0 + \\alpha_1 p_1 + \\omega_1 s_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\frac{1}{5} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix}$。\n10. $r_1 = s_1 - \\omega_1 t_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -\\frac{6}{5} \\\\ -\\frac{12}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$。\n\n第一次迭代后，残差为 $r_1 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$，它不是零向量。这证实了第一个论断。\n\n**迭代 2 ($k=2$)：**\n1. $\\rho_2 = (\\hat{r}_0, r_1) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\frac{6}{5}$。\n2. $\\beta_2 = \\frac{\\rho_2}{\\rho_1} \\frac{\\alpha_1}{\\omega_1} = \\frac{6/5}{1} \\frac{1}{1/5} = \\frac{6}{5} \\cdot 5 = 6$。\n3. $p_2 = r_1 + \\beta_2(p_1 - \\omega_1 v_1) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\begin{pmatrix} \\frac{4}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} + \\frac{24}{5} \\\\ -\\frac{3}{5} - \\frac{18}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30}{5} \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix}$。\n4. $v_2 = A p_2 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 - \\frac{42}{5} \\\\ 18 - \\frac{84}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30-42}{5} \\\\ \\frac{90-84}{5} \\end{pmatrix} = \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix}$。\n5. $\\alpha_2$ 的分母为 $(\\hat{r}_0, v_2) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = -\\frac{12}{5}$。因此，$\\alpha_2 = \\frac{\\rho_2}{(\\hat{r}_0, v_2)} = \\frac{6/5}{-12/5} = -\\frac{1}{2}$。\n6. $s_2 = r_1 - \\alpha_2 v_2 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n7. 由于 $s_2 = \\mathbf{0}$，我们有 $t_2 = A s_2 = A \\mathbf{0} = \\mathbf{0}$。\n8. $\\omega_2$ 的公式变为 $\\frac{(t_2, s_2)}{(t_2, t_2)} = \\frac{0}{0}$。这是一种所谓的“幸运分解”，表明在这一步将找到精确解。我们可以设置 $\\omega_2 = 0$。\n9. $x_2 = x_1 + \\alpha_2 p_2 + \\omega_2 s_2 = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} + 0 \\cdot \\mathbf{0} = \\begin{pmatrix} 1 - 3 \\\\ -\\frac{3}{5} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -\\frac{6}{10} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{15}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix}$。\n10. $r_2 = s_2 - \\omega_2 t_2 = \\mathbf{0} - 0 \\cdot \\mathbf{0} = \\mathbf{0}$。\n\n残差 $r_2$ 是零向量，这意味着算法已收敛到精确解。这证实了第二个论断。为了验证，我们检查解：\n$A x_2 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1(-2) + 2(\\frac{3}{2}) \\\\ 3(-2) + 4(\\frac{3}{2}) \\end{pmatrix} = \\begin{pmatrix} -2 + 3 \\\\ -6 + 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = b$。\n解是正确的。\n\n问题要求第一个平滑参数 $\\omega_1$ 的值。根据第一次迭代的计算，该值为 $\\omega_1 = \\frac{1}{5}$。",
            "answer": "$$\n\\boxed{\\frac{1}{5}}\n$$"
        },
        {
            "introduction": "掌握了算法的基本步骤后，让我们来探索一个能揭示其深刻理论内涵的特殊情况。在本练习中，初始残差 $r_0$ 被巧妙地设置为矩阵 $A$ 的一个特征向量。通过观察算法在这一理想化场景下的行为，您将深入理解克雷洛夫子空间方法与矩阵谱性质之间的内在联系 。",
            "id": "2374426",
            "problem": "考虑线性系统 $A x = b$，其中\n$$\nA = \\begin{pmatrix}\n2 & 5 \\\\\n0 & 3\n\\end{pmatrix}.\n$$\n选择右端项 $b$ 为 $A$ 的一个特征向量，其关联的特征值为 $2$，具体为\n$$\nb = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n假设使用双共轭梯度稳定 (BiCGSTAB) 方法求解 $A x = b$，初始猜测值为 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，阴影残差 $\\hat{r}$ 选择为 $\\hat{r} = r_0$，其中 $r_0 = b - A x_0$。计算过程使用精确算术。\n\n确定在第一次 BiCGSTAB 迭代中计算出的标量 $\\alpha_0$。请以精确值的形式给出答案，不要四舍五入。",
            "solution": "首先必须验证问题陈述的科学合理性、完整性和客观性。\n\n步骤 1：提取已知条件。\n问题提供了以下信息：\n- 线性系统为 $A x = b$。\n- 矩阵 $A$ 为 $A = \\begin{pmatrix} 2 & 5 \\\\ 0 & 3 \\end{pmatrix}$。\n- 右端向量 $b$ 为 $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n- 声称 $b$ 是 $A$ 的一个特征向量，其关联的特征值为 $2$。\n- 将使用双共轭梯度稳定 (BiCGSTAB) 方法。\n- 初始猜测值为 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n- 初始阴影残差选择为 $\\hat{r} = r_0$，应理解为 $\\hat{r}_0 = r_0$。\n- 初始残差定义为 $r_0 = b - A x_0$。\n- 所有计算必须在精确算术下进行。\n- 目标是确定 BiCGSTAB 算法第一次迭代中的标量 $\\alpha_0$。\n\n步骤 2：使用提取的已知条件进行验证。\n该问题在数值线性代数领域内是明确定义的。BiCGSTAB 算法是一种标准的迭代方法。我们必须验证所提供的前提，即 $b$ 是 $A$ 的一个特征值为 $2$ 的特征向量。\n让我们计算乘积 $A b$：\n$$\nA b = \\begin{pmatrix} 2 & 5 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (5)(0) \\\\ (0)(1) + (3)(0) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\n现在，让我们计算当 $\\lambda = 2$ 时的乘积 $\\lambda b$：\n$$\n\\lambda b = 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\n由于 $A b = 2 b$，该声称在事实上是正确的。问题具有科学依据，是自洽的，且以客观、明确的术语提出。它不违反任何无效性标准。因此，该问题是有效的。\n\n步骤 3：进行求解。\n我们现在将按陈述求解问题。BiCGSTAB 算法以一个初始化步骤开始，然后是迭代。我们关心的是第一次迭代（索引为 $i=0$）。\n\n初始化步骤如下：\n1.  设置初始猜测值 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n2.  计算初始残差 $r_0$：\n    $$\n    r_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 & 5 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    $$\n3.  选择初始阴影残差 $\\hat{r}_0$。问题指定选择 $\\hat{r}_0 = r_0$。因此，\n    $$\n    \\hat{r}_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    $$\n4.  设置初始搜索方向 $p_0 = r_0$。\n    $$\n    p_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n    $$\n5.  计算标量 $\\rho_0$。通用公式为 $\\rho_i = \\hat{r}_0^T r_i$。对于 $i=0$，即为 $\\rho_0 = \\hat{r}_0^T r_0$。\n    $$\n    \\rho_0 = \\hat{r}_0^T r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (0)(0) = 1\n    $$\n    条件 $\\rho_0 \\neq 0$ 必须成立才能启动该方法。此处 $\\rho_0=1$，因此条件得到满足。\n\n现在，我们执行第一次迭代（$i=0$）来求 $\\alpha_0$。\n1.  计算向量 $v_0 = A p_0$。\n    $$\n    v_0 = A p_0 = \\begin{pmatrix} 2 & 5 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n    $$\n2.  计算标量 $\\alpha_0$。$\\alpha_i$ 的公式为 $\\alpha_i = \\frac{\\rho_i}{\\hat{r}_0^T v_i}$。对于 $i=0$：\n    $$\n    \\alpha_0 = \\frac{\\rho_0}{\\hat{r}_0^T v_0}\n    $$\n    我们已知 $\\rho_0 = 1$。我们需要计算分母 $\\hat{r}_0^T v_0$。\n    $$\n    \\hat{r}_0^T v_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = (1)(2) + (0)(0) = 2\n    $$\n    因此，$\\alpha_0$ 是：\n    $$\n    \\alpha_0 = \\frac{1}{2}\n    $$\n\n另一种更抽象的推导也证实了这一结果。由于 $r_0 = b$，$\\hat{r}_0 = r_0 = b$，以及 $p_0 = r_0 = b$，$\\alpha_0$ 的公式变为：\n$$\n\\alpha_0 = \\frac{\\rho_0}{\\hat{r}_0^T v_0} = \\frac{r_0^T r_0}{(r_0)^T (A p_0)} = \\frac{b^T b}{b^T (A b)}\n$$\n我们已经验证了 $b$ 是 $A$ 的一个特征值为 $\\lambda=2$ 的特征向量，所以 $A b = 2 b$。将此代入 $\\alpha_0$ 的表达式：\n$$\n\\alpha_0 = \\frac{b^T b}{b^T (2b)} = \\frac{b^T b}{2 (b^T b)}\n$$\n由于 $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，内积 $b^T b = (1)^2 + (0)^2 = 1$，非零。因此我们可以从分子和分母中消去项 $b^T b$，得到：\n$$\n\\alpha_0 = \\frac{1}{2}\n$$\n这证实了通过直接计算得到的结果。该问题有一个唯一的、精确的解。",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "理论上的精确算术与计算机的有限精度计算之间存在着鸿沟。这个编程实践旨在揭示数值计算中的一个核心挑战：病态性 (ill-conditioning)，即一个看似精确的解（小残差）可能与真实解相去甚远（大前向误差）。通过亲手实现并测试这个问题，您将学会批判性地评估迭代求解器的结果，这是计算工程师必备的一项关键技能 。",
            "id": "2374413",
            "problem": "设 $A \\in \\mathbb{R}^{n \\times n}$ 是一个实数、方阵、非奇异矩阵，并设 $x_{\\star} \\in \\mathbb{R}^{n}$ 是线性系统 $A x = b$ 的精确解，其中 $b = A x_{\\star}$。考虑使用双共轭梯度稳定 (BiConjugate Gradient Stabilized, BiCGSTAB) 方法，以初始猜测值 $x_0 = 0$ 开始迭代，在 IEEE 754 双精度 (binary64) 算术下计算近似解 $\\hat{x}$。当残差 $r_k = b - A x_k$ 的欧几里得范数满足\n$$\n\\lVert r_k \\rVert_2 \\le \\max\\left( \\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\, \\tau_{\\mathrm{abs}} \\right),\n$$\n或当达到预设的最大迭代次数 $k_{\\max}$ 时，迭代必须终止，以先发生者为准。当且仅当在达到 $k_{\\max}$ 之前满足基于残差的停止条件时，才声明该方法已“收敛”。终止后，计算相对前向误差\n$$\n\\varepsilon_{\\mathrm{fwd}} = \\frac{\\lVert \\hat{x} - x_{\\star} \\rVert_2}{\\lVert x_{\\star} \\rVert_2}.\n$$\n对于固定的阈值 $\\theta = 10^{-2}$，定义一个测试，当且仅当该方法已“收敛”且同时 $\\varepsilon_{\\mathrm{fwd}} > \\theta$ 时，“标记错误收敛”。对于每个测试用例，如果标记了错误收敛，则输出整数 $1$，否则输出 $0$。\n\n所有算术运算必须在 IEEE 754 双精度 (binary64) 下执行。不使用角度，也不涉及物理单位。\n\n测试套件。您的程序必须精确评估以下三种情况：\n\n- 情况 1 (良态，非对称，理想情况)：设 $n = 10$。首先使用固定的伪随机种子 $s = 123456$，从 $[-1,1]$ 上的均匀分布中独立抽样生成矩阵 $T$ 的元素，然后设置 $A \\in \\mathbb{R}^{10 \\times 10}$ 如下：\n$$\nA = T + 10 I_{10},\n$$\n其中 $I_{10}$ 是 $10 \\times 10$ 的单位矩阵。设 $x_{\\star} = \\mathbf{1} \\in \\mathbb{R}^{10}$ 是全为 1 的向量，并设置 $b = A x_{\\star}$。使用 $\\tau_{\\mathrm{rel}} = 10^{-8}$，$\\tau_{\\mathrm{abs}} = 0$ 和 $k_{\\max} = 200$。\n\n- 情况 2 (严重病态，在绝对停止条件下导致错误收敛的近奇异 $2 \\times 2$ 系统)：设\n$$\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + \\epsilon \\end{bmatrix}, \\quad \\epsilon = 10^{-14}.\n$$\n设 $x_{\\star} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ 并设置 $b = A x_{\\star}$。使用 $\\tau_{\\mathrm{rel}} = 0$，$\\tau_{\\mathrm{abs}} = 10^{-12}$ 和 $k_{\\max} = 200$。\n\n- 情况 3 (边界情况，标量系统)：设 $n = 1$，$A = [3]$，$x_{\\star} = [2]$，以及 $b = A x_{\\star} = [6]$。使用 $\\tau_{\\mathrm{rel}} = 10^{-14}$，$\\tau_{\\mathrm{abs}} = 0$ 和 $k_{\\max} = 20$。\n\n最终输出格式。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序为 $[z_1, z_2, z_3]$，其中 $z_i \\in \\{0,1\\}$ 是按上文定义的对情况 $i$ 的整数标记。例如，一个输出行可能看起来像\n$[0,1,0]$。",
            "solution": "在进行求解之前，必须严格验证问题陈述。\n\n首先，我们提取问题陈述中给出的已知条件。\n该问题涉及求解线性系统 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个实数、方阵、非奇异矩阵，$x_{\\star} \\in \\mathbb{R}^{n}$ 是精确解，且 $b = A x_{\\star}$。使用双共轭梯度稳定 (BiConjugate Gradient Stabilized, BiCGSTAB) 方法，在 IEEE $754$ 双精度算术下，从初始猜测值 $x_0 = 0$ 开始计算近似解 $\\hat{x}$。\n\n迭代终止条件是：\n1.  残差 $r_k = b - A x_k$ 的欧几里得范数满足 $\\lVert r_k \\rVert_2 \\le \\max\\left( \\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\, \\tau_{\\mathrm{abs}} \\right)$。\n2.  达到了最大迭代次数 $k_{\\max}$。\n\n当且仅当在达到 $k_{\\max}$ 之前满足基于残差的停止条件 (1) 时，该过程才算“收敛”。\n\n终止后，相对前向误差计算为 $\\varepsilon_{\\mathrm{fwd}} = \\frac{\\lVert \\hat{x} - x_{\\star} \\rVert_2}{\\lVert x_{\\star} \\rVert_2}$。\n\n定义了一个“错误收敛”的测试。当且仅当该方法已“收敛”且同时 $\\varepsilon_{\\mathrm{fwd}} > \\theta$ 时，测试被标记（输出 $1$），其中阈值固定为 $\\theta = 10^{-2}$。否则，输出为 $0$。\n\n测试套件包含三种情况：\n-   情况 1：$n = 10$。$A = T + 10 I_{10}$，其中 $T_{ij} \\sim U[-1,1]$ 由种子 $s = 123456$ 生成。$x_{\\star} = \\mathbf{1} \\in \\mathbb{R}^{10}$。$b = A x_{\\star}$。参数：$\\tau_{\\mathrm{rel}} = 10^{-8}$，$\\tau_{\\mathrm{abs}} = 0$，$k_{\\max} = 200$。\n-   情况 2：$n = 2$。$A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + \\epsilon \\end{bmatrix}$，其中 $\\epsilon = 10^{-14}$。$x_{\\star} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$。$b = A x_{\\star}$。参数：$\\tau_{\\mathrm{rel}} = 0$，$\\tau_{\\mathrm{abs}} = 10^{-12}$，$k_{\\max} = 200$。\n-   情况 3：$n = 1$。$A = [3]$，$x_{\\star} = [2]$，$b = A x_{\\star} = [6]$。参数：$\\tau_{\\mathrm{rel}} = 10^{-14}$，$\\tau_{\\mathrm{abs}} = 0$，$k_{\\max} = 20$。\n\n接下来，我们验证这些已知条件。该问题根植于数值线性代数，这是计算工程学的核心学科。BiCGSTAB 方法是求解非对称线性系统的标准、成熟的算法。残差、前向误差、停止准则和机器精度等概念是该领域的基础。因此，该问题具有科学依据。其定义精确、客观且无歧义。每个测试用例都用所有必要的数据（$A$、$x_{\\star}$ 和求解器参数）完全指定，确保了问题的自包含性和适定性。所要求的矩阵已被确认为非奇异。不存在内部矛盾或科学上不合理的条件。\n\n分析结论是，该问题在科学上是合理的、适定的和客观的。它被认为是有效的，我们可以着手构建一个解决方案。\n\n解决方案需要实现 BiCGSTAB 算法。对于一个线性系统 $A x = b$ 和一个初始猜测值 $x_0$，该算法迭代地改进解。我们从 $x_0 = 0$ 和相应的初始残差 $r_0 = b - A x_0 = b$ 开始。初始影子残差的标准选择是 $\\hat{r}_0 = r_0$。我们初始化标量参数 $\\rho_0 = 1$，$\\alpha_0 = 1$，$\\omega_0 = 1$，以及向量参数 $p_0 = 0$，$v_0 = 0$。\n\n对于每次迭代 $k = 1, 2, \\dots, k_{\\max}$：\n1.  计算 $\\rho_k = \\hat{r}_0^T r_{k-1}$。如果 $\\rho_k = 0$，则发生故障。\n2.  计算 $\\beta = (\\rho_k / \\rho_{k-1}) \\cdot (\\alpha_{k-1} / \\omega_{k-1})$。\n3.  更新搜索方向：$p_k = r_{k-1} + \\beta (p_{k-1} - \\omega_{k-1} v_{k-1})$。\n4.  计算矩阵向量乘积 $v_k = A p_k$。\n5.  计算步长 $\\alpha_k = \\rho_k / (\\hat{r}_0^T v_k)$。如果分母为零，则发生故障。\n6.  计算一个中间残差 $s = r_{k-1} - \\alpha_k v_k$。\n7.  计算矩阵向量乘积 $t = A s$。\n8.  计算稳定化参数 $\\omega_k = (t^T s) / (t^T t)$。如果 $t^T t = 0$，可能会发生故障。\n9.  更新解：$x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s$。\n10. 更新残差：$r_k = s - \\omega_k t$。\n11. 检查收敛性：测试是否 $\\lVert r_k \\rVert_2 \\le \\max(\\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\tau_{\\mathrm{abs}})$。如果为真，则终止并设置 `converged = True`。\n\n如果循环完成而未满足收敛准则，则过程终止并设置 `converged = False`。最终的近似解记为 $\\hat{x}$。\n\n对于每个测试用例，将执行此算法。终止后，我们计算相对前向误差 $\\varepsilon_{\\mathrm{fwd}} = \\lVert \\hat{x} - x_{\\star} \\rVert_2 / \\lVert x_{\\star} \\rVert_2$。“错误收敛”标记将在算法报告收敛（`converged = True`）但解的质量很差（$\\varepsilon_{\\mathrm{fwd}} > 10^{-2}$）时设置为 $1$。否则，标记为 $0$。\n\n情况 1 代表一个典型的良态问题。矩阵 $A$ 的条件数很小，因此我们预计 BiCGSTAB 会快速收敛，并且小的最终残差会对应于小的前向误差。标记应为 $0$。\n\n情况 2 旨在揭示一个典型的病理学问题。矩阵 $A$ 是严重病态的。真实解是 $x_{\\star} = [1, -1]^T$，这导致右侧向量 $b = [0, -\\epsilon]^T$ 非常小。初始残差 $r_0=b$ 的范数为 $\\epsilon = 10^{-14}$。这小于绝对容差 $\\tau_{\\mathrm{abs}} = 10^{-12}$。因此，算法将在迭代 $k=0$ 时终止，并设置 `converged = True` 且 $\\hat{x} = x_0 = 0$。前向误差将为 $\\varepsilon_{\\mathrm{fwd}} = \\lVert 0 - x_{\\star} \\rVert_2 / \\lVert x_{\\star} \\rVert_2 = 1$，这大于 $\\theta=10^{-2}$。因此，该情况的标记将为 $1$。\n\n情况 3 是一个简单的 $1 \\times 1$ 系统。对于如此简单的情况，预计 BiCGSTAB 将在单次迭代中找到精确解。前向误差将为 $0$，因此标记将为 $0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_bicgstab(A, b, x0, tau_rel, tau_abs, k_max):\n    \"\"\"\n    Implements the BiConjugate Gradient Stabilized (BiCGSTAB) method.\n\n    Args:\n        A (np.ndarray): The square matrix of the linear system.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tau_rel (float): The relative tolerance for the stopping condition.\n        tau_abs (float): The absolute tolerance for the stopping condition.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The approximate solution vector.\n            - bool: A flag indicating if the method converged.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Use IEEE 754 double precision (np.float64) for all arithmetic\n    x = x0.copy().astype(np.float64)\n    r = (b - A @ x).astype(np.float64)\n\n    norm_b = np.linalg.norm(b)\n    stop_tol = max(tau_rel * norm_b, tau_abs)\n\n    # Check for convergence at k=0\n    if np.linalg.norm(r) <= stop_tol:\n        return x, True\n\n    # BiCGSTAB algorithm setup\n    r_hat0 = r.copy()\n    rho_prev = 1.0\n    alpha = 1.0\n    omega = 1.0\n    p = np.zeros(n, dtype=np.float64)\n    v = np.zeros(n, dtype=np.float64)\n\n    for k in range(1, k_max + 1):\n        rho_curr = np.dot(r_hat0, r)\n        \n        # Breakdown condition 1\n        if rho_curr == 0.0:\n            return x, False\n\n        # On the first iteration, beta calculation uses rho_prev=1, alpha=1, omega=1\n        # which effectively makes p = r, as p_prev and v_prev are zero.\n        beta = (rho_curr / rho_prev) * (alpha / omega)\n        p = r + beta * (p - omega * v)\n\n        v = A @ p\n        \n        r_hat0_dot_v = np.dot(r_hat0, v)\n        \n        # Breakdown condition 2\n        if r_hat0_dot_v == 0.0:\n            return x, False\n        \n        alpha = rho_curr / r_hat0_dot_v\n\n        s = r - alpha * v\n        t = A @ s\n        \n        t_dot_t = np.dot(t, t)\n        \n        # Breakdown condition 3: t is numerically zero.\n        if t_dot_t < np.finfo(np.float64).eps:\n            x = x + alpha * p\n            r = s\n            # The residual is now 's'. Check convergence and return.\n            return x, np.linalg.norm(r) <= stop_tol\n\n        omega = np.dot(t, s) / t_dot_t\n\n        x = x + alpha * p + omega * s\n        r = s - omega * t\n        \n        # Check termination condition after iteration k\n        if np.linalg.norm(r) <= stop_tol:\n            return x, True\n\n        # Update for next iteration\n        rho_prev = rho_curr\n\n    return x, False # Max iterations reached\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the BiCGSTAB problem.\n    \"\"\"\n    theta = 1e-2\n\n    # Case 1: Well-conditioned, nonsymmetric\n    n1 = 10\n    seed = 123456\n    rng = np.random.default_rng(seed)\n    T1 = rng.uniform(-1, 1, (n1, n1))\n    A1 = T1 + 10 * np.eye(n1, dtype=np.float64)\n    x_star1 = np.ones(n1, dtype=np.float64)\n    b1 = A1 @ x_star1\n    tau_rel1 = 1e-8\n    tau_abs1 = 0.0\n    k_max1 = 200\n\n    # Case 2: Ill-conditioned, wrong convergence\n    epsilon = 1e-14\n    A2 = np.array([[1, 1], [1, 1 + epsilon]], dtype=np.float64)\n    x_star2 = np.array([1, -1], dtype=np.float64)\n    b2 = A2 @ x_star2\n    tau_rel2 = 0.0\n    tau_abs2 = 1e-12\n    k_max2 = 200\n\n    # Case 3: Scalar system\n    A3 = np.array([[3]], dtype=np.float64)\n    x_star3 = np.array([2], dtype=np.float64)\n    b3 = A3 @ x_star3\n    tau_rel3 = 1e-14\n    tau_abs3 = 0.0\n    k_max3 = 20\n\n    test_cases = [\n        (A1, x_star1, b1, tau_rel1, tau_abs1, k_max1),\n        (A2, x_star2, b2, tau_rel2, tau_abs2, k_max2),\n        (A3, x_star3, b3, tau_rel3, tau_abs3, k_max3),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, x_star, b, tau_rel, tau_abs, k_max = case\n        n = A.shape[0]\n        x0 = np.zeros(n, dtype=np.float64)\n        \n        hat_x, converged = run_bicgstab(A, b, x0, tau_rel, tau_abs, k_max)\n        \n        norm_x_star = np.linalg.norm(x_star)\n        \n        # The problem guarantees x_star is not zero, so norm_x_star is non-zero.\n        eps_fwd = np.linalg.norm(hat_x - x_star) / norm_x_star\n\n        flag = 0\n        if converged and eps_fwd > theta:\n            flag = 1\n        \n        results.append(flag)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}