{
    "hands_on_practices": [
        {
            "introduction": "The Jacobi method offers a simple, parallelizable approach to solving linear systems, but its convergence is guaranteed only under specific conditions on the system matrix. This exercise presents a scenario where the standard Jacobi iteration fails and challenges you to stabilize it using damping. Through numerical experimentation, you will gain practical insight into how relaxation parameters can be tuned to satisfy convergence criteria, providing a tangible link to the underlying theory of spectral radii.",
            "id": "2406651",
            "problem": "You are asked to implement and analyze a damped Jacobi iteration to solve linear systems that are not strictly diagonally dominant. The objective is to determine, by numerical experimentation, damping factors that guarantee convergence. Your work must be grounded in the following foundational base: the linear system definition $A x = b$, the diagonal-offdiagonal split $A = D + R$ with $D$ the diagonal of $A$, and the general principle that fixed-point iterations converge when the spectral radius of the iteration matrix is less than $1$. You must not use any pre-packaged solver; all iteration logic must be implemented explicitly.\n\nTask requirements:\n- Implement a damped Jacobi iteration for a general square real matrix $A \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^n$. The method should start from $x^{(0)} = 0$ and iterate until the absolute residual norm criterion $\\lVert b - A x^{(k)} \\rVert_2 \\le \\tau$ is met or until a maximum number of iterations is reached. Use $\\tau = 10^{-10}$ and a fixed iteration limit of $10{,}000$.\n- For a given damping factor $\\omega \\in (0,1]$, apply the damped correction to the Jacobi update derived from $A = D + R$ and the residual $r^{(k)} = b - A x^{(k)}$ without invoking any formula beyond this foundational base. Numerical stability and correctness must be ensured for all provided test cases.\n- For each test case defined below, search over the discrete set $W = \\{0.05, 0.10, 0.15, \\dots, 1.00\\}$ and return the largest $\\omega \\in W$ such that the damped Jacobi iteration converges to the tolerance within the iteration limit. If no $\\omega$ in $W$ leads to convergence, return $0.0$ for that case.\n- The systems include matrices that are symmetric positive definite (SPD) but not strictly diagonally dominant. In all cases, all quantities are unitless; no physical units are involved.\n\nTest suite specification:\n- Across all cases, use $x^{(0)} = 0$, $\\tau = 10^{-10}$, $\\text{max\\_iter} = 10{,}000$, and the search set $W = \\{0.05, 0.10, \\dots, 1.00\\}$.\n- Case $1$ (non-strictly diagonally dominant, SPD, undamped Jacobi diverges due to a negative eigenvalue below $-1$ but damping can stabilize):\n  - Define the orthonormal matrix\n    $$Q_3 = \\begin{bmatrix}\n    \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}} \\\\\n    \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0 \\\\\n    \\frac{1}{\\sqrt{6}}  \\frac{1}{\\sqrt{6}}  -\\frac{2}{\\sqrt{6}}\n    \\end{bmatrix}$$.\n  - Define the diagonal matrix\n    $$\\Lambda_3 = \\mathrm{diag}(-1.1, 0.8, 0.6)$$.\n  - Define\n    $$T_3 = Q_3 \\Lambda_3 Q_3^\\top, \\quad A_1 = I_3 - T_3, \\quad b_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$.\n- Case $2$ (weakly diagonally dominant tridiagonal Toeplitz SPD; undamped Jacobi converges):\n  - Define $A_2 \\in \\mathbb{R}^{3 \\times 3}$ by $[A_2]_{ii} = 2$ for $i \\in \\{1,2,3\\}$ and $[A_2]_{i,i+1} = [A_2]_{i+1,i} = -1$ for $i \\in \\{1,2\\}$, with all other entries zero. Define\n    $$b_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$$.\n- Case $3$ (non-strictly diagonally dominant, SPD, undamped Jacobi diverges; damping can stabilize):\n  - Define the normalized Hadamard matrix\n    $$Q_4 = \\frac{1}{2}\\begin{bmatrix}\n    1  1  1  1 \\\\\n    1  1  -1  -1 \\\\\n    1  -1  1  -1 \\\\\n    1  -1  -1  1\n    \\end{bmatrix}$$.\n  - Define the diagonal matrix\n    $$\\Lambda_4 = \\mathrm{diag}(-1.3, 0.9, 0.7, 0.2)$$.\n  - Define\n    $$T_4 = Q_4 \\Lambda_4 Q_4^\\top, \\quad A_3 = I_4 - T_4, \\quad b_3 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$$.\n- Case $4$ (boundary-like case with a highly negative eigenvalue; testing near the stability edge):\n  - Use $Q_3$ as in Case $1$ and define\n    $$\\Lambda_3' = \\mathrm{diag}(-1.98, 0.0, 0.0)$$.\n  - Define\n    $$T_3' = Q_3 \\Lambda_3' Q_3^\\top, \\quad A_4 = I_3 - T_3', \\quad b_4 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the largest damping factors found for the four cases, in order, as a comma-separated list enclosed in square brackets. For example, an output of the form\n  $$[0.95,1.0,0.85,0.65]$$\n  would be acceptable. The actual numeric values must be those obtained by your implementation using the parameters above.\n\nDesign for coverage:\n- Case $1$ verifies stabilization by damping for a non-strictly diagonally dominant matrix with a negative eigenvalue less than $-1$.\n- Case $2$ is a happy-path scenario where undamped Jacobi already converges; the largest admissible $\\omega$ should be $1.0$.\n- Case $3$ is another non-strictly diagonally dominant system with a more negative eigenvalue, testing a smaller largest admissible $\\omega$.\n- Case $4$ is a boundary case with a very negative eigenvalue close to $-2$, probing the edge of the stability region in the given grid.\n\nYour program must be a complete, runnable implementation that constructs $A$ and $b$ for each case exactly as defined above, executes the damped Jacobi method, searches over $W$, and prints the single required line with the resulting four numbers.",
            "solution": "We begin from the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^n$. Let $A = D + R$ where $D$ is the diagonal of $A$ and $R$ contains the off-diagonal entries. The Jacobi method is an instance of fixed-point iteration derived from rearranging $D x = b - R x$, yielding the map $x \\mapsto D^{-1}(b - R x)$. To fit the fundamental linear fixed-point framework $x^{(k+1)} = T x^{(k)} + c$, one writes the update in residual-correction form using the residual $r^{(k)} = b - A x^{(k)}$, and replaces the exact correction $A^{-1} r^{(k)}$ by the diagonal approximation $D^{-1} r^{(k)}$. This gives the undamped Jacobi step\n$$\nx^{(k+1)} = x^{(k)} + D^{-1} (b - A x^{(k)}).\n$$\nTo improve robustness for systems that are not strictly diagonally dominant, one applies a relaxation factor $\\omega \\in (0,1]$ to the correction, obtaining the damped Jacobi iteration\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} (b - A x^{(k)}).\n$$\nThis is a fixed-point iteration with iteration matrix\n$$\nT_\\omega = I - \\omega D^{-1} A = (1 - \\omega) I + \\omega \\left( I - D^{-1} A \\right),\n$$\nso that if $T_J = I - D^{-1} A$ denotes the undamped Jacobi iteration matrix, we have\n$$\nT_\\omega = (1 - \\omega) I + \\omega T_J.\n$$\nThe fundamental convergence criterion for linear fixed-point iterations is that the spectral radius of the iteration matrix be less than $1$:\n$$\n\\rho(T_\\omega)  1.\n$$\nIf $\\lambda$ is an eigenvalue of $T_J$ with eigenvector $v$, then $T_\\omega v = \\left( 1 - \\omega + \\omega \\lambda \\right) v$, so the eigenvalues of $T_\\omega$ are\n$$\n\\mu(\\omega; \\lambda) = 1 - \\omega + \\omega \\lambda.\n$$\nTherefore,\n$$\n\\rho(T_\\omega) = \\max_{\\lambda \\in \\mathrm{spec}(T_J)} \\left| 1 - \\omega + \\omega \\lambda \\right|.\n$$\nThis immediately clarifies the role of damping. If an eigenvalue of $T_J$ satisfies $\\lambda  1$, then for any $\\omega \\in (0,1]$,\n$$\n\\left| 1 - \\omega + \\omega \\lambda \\right| = 1 + \\omega(\\lambda - 1)  1,\n$$\nso damping cannot cure divergence caused by eigenvalues greater than $1$. In contrast, if divergence is due to a negative eigenvalue with $\\lambda  -1$, damping can stabilize the iteration. Write $\\lambda = -c$ with $c  1$. Then\n$$\n\\left| 1 - \\omega + \\omega \\lambda \\right| = \\left| 1 - \\omega (1 + c) \\right|  1\n\\quad \\Longleftrightarrow \\quad 0  \\omega  \\frac{2}{1 + c}.\n$$\nThus, when all eigenvalues of $T_J$ are less than or equal to $1$ and any unstable eigenvalues satisfy $\\lambda  -1$, stability is achieved for sufficiently small $\\omega$; in fact, any $\\omega \\in \\left(0, \\min \\left\\{ 1, \\frac{2}{1 + c_{\\max}} \\right\\} \\right)$ where $c_{\\max} = \\max \\{ -\\lambda : \\lambda \\in \\mathrm{spec}(T_J), \\lambda  -1 \\}$ guarantees $\\rho(T_\\omega)  1$.\n\nAlgorithmic design:\n- For each test system, construct $A$ and $b$, initialize $x^{(0)} = 0$, select $\\omega$ from the discrete set $W = \\{0.05, 0.10, \\dots, 1.00\\}$, and iterate\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} (b - A x^{(k)})\n$$\nuntil either the absolute residual norm $\\lVert b - A x^{(k)} \\rVert_2 \\le \\tau$ with $\\tau = 10^{-10}$ or the iteration count reaches $10{,}000$. Record whether the method converged. Over $W$, choose the largest $\\omega$ that converges.\n\nTest matrices and expectations:\n- Case $1$: Define $Q_3$, $\\Lambda_3 = \\mathrm{diag}(-1.1, 0.8, 0.6)$, $T_3 = Q_3 \\Lambda_3 Q_3^\\top$, and $A_1 = I_3 - T_3$, $b_1 = [1,2,3]^\\top$. By construction, the eigenvalues of $T_3$ are $-1.1$, $0.8$, and $0.6$. The instability is caused solely by $\\lambda_{\\min} = -1.1$. The admissible damping range for that mode is $0  \\omega  \\frac{2}{1 + 1.1} = \\frac{2}{2.1} \\approx 0.95238$. Therefore, $\\omega = 1.0$ diverges, whereas $\\omega = 0.95$ is theoretically stable. On the discrete grid $W$, we therefore expect the largest admissible $\\omega$ to be $0.95$, and the convergence factor for the slowest mode at $\\omega = 0.95$ is $\\left| 1 - 0.95(1 + 1.1) \\right| = \\left| 1 - 1.995 \\right| = 0.995$, which still allows convergence within $10{,}000$ iterations.\n- Case $2$: Tridiagonal Toeplitz $A_2$ with diagonal $2$ and off-diagonal $-1$ is symmetric positive definite (SPD) and weakly diagonally dominant. The undamped Jacobi iteration matrix has eigenvalues $\\lambda_k = \\cos \\left( \\frac{k \\pi}{n+1} \\right)$ for $k \\in \\{1,2,3\\}$, which lie in $(-1,1)$. For $\\omega = 1.0$, $\\rho(T_\\omega) = \\rho(T_J)  1$, so convergence occurs with the largest grid value $\\omega = 1.0$.\n- Case $3$: With $Q_4$ as the normalized Hadamard and $\\Lambda_4 = \\mathrm{diag}(-1.3, 0.9, 0.7, 0.2)$, the eigenvalues of $T_4$ are $-1.3$, $0.9$, $0.7$, $0.2$. The admissible damping range for the unstable negative mode is $0  \\omega  \\frac{2}{1 + 1.3} = \\frac{2}{2.3} \\approx 0.86956$. On the grid $W$, the largest admissible $\\omega$ is $0.85$. At $\\omega = 0.85$ the worst-factor is $\\left| 1 - 0.85(1 + 1.3) \\right| = \\left| 1 - 1.955 \\right| = 0.955$, which is acceptable for convergence within the iteration limit.\n- Case $4$: Using $Q_3$ and $\\Lambda_3' = \\mathrm{diag}(-1.98, 0.0, 0.0)$, the unstable negative mode requires $0  \\omega  \\frac{2}{1 + 1.98} = \\frac{2}{2.98} \\approx 0.67114$. On the grid $W$, the largest admissible $\\omega$ is $0.65$. At $\\omega = 0.65$ the worst factor is $\\left| 1 - 0.65(1 + 1.98) \\right| = \\left| 1 - 1.937 \\right| = 0.937$.\n\nPutting these together, the expected outputs are the largest admissible damping factors in $W$:\n- Case $1$: $0.95$.\n- Case $2$: $1.0$.\n- Case $3$: $0.85$.\n- Case $4$: $0.65$.\n\nThe program must construct the matrices exactly as specified from $Q_3$, $Q_4$, $\\Lambda_3$, $\\Lambda_4$, $\\Lambda_3'$, implement the damped Jacobi iteration with the stated stopping criteria, perform the grid search over $W$, and print the results in the required single-line list format.",
            "answer": "```python\nimport numpy as np\n\ndef damped_jacobi(A, b, omega, max_iter=10000, tol=1e-10):\n    \"\"\"\n    Perform damped Jacobi iteration:\n        x_{k+1} = x_k + omega * D^{-1} * (b - A x_k)\n    starting from x0 = 0. Returns (converged: bool, iterations: int).\n    \"\"\"\n    n = A.shape[0]\n    # Diagonal and its inverse\n    D = np.diag(A)\n    if np.any(D == 0):\n        return False, 0  # cannot apply Jacobi if zero diagonal\n    invD = 1.0 / D\n\n    x = np.zeros(n, dtype=float)\n    r = b - A @ x\n    rnorm = np.linalg.norm(r)\n    # Absolute tolerance\n    if rnorm = tol:\n        return True, 0\n\n    for k in range(1, max_iter + 1):\n        # Jacobi update with damping\n        r = b - A @ x\n        x = x + omega * (invD * r)\n        # Check convergence\n        r = b - A @ x\n        rnorm = np.linalg.norm(r)\n        if not np.isfinite(rnorm):\n            return False, k\n        if rnorm = tol:\n            return True, k\n        # Optional early bailout if the residual is exploding\n        if rnorm  1e20:\n            return False, k\n    return False, max_iter\n\n\ndef build_case1():\n    # Q3 orthonormal matrix\n    Q3 = np.array([\n        [1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0)],\n        [1.0/np.sqrt(2.0), -1.0/np.sqrt(2.0), 0.0],\n        [1.0/np.sqrt(6.0), 1.0/np.sqrt(6.0), -2.0/np.sqrt(6.0)]\n    ], dtype=float)\n    Lambda3 = np.diag([-1.1, 0.8, 0.6])\n    T3 = Q3 @ Lambda3 @ Q3.T\n    A1 = np.eye(3) - T3\n    b1 = np.array([1.0, 2.0, 3.0], dtype=float)\n    return A1, b1\n\n\ndef build_case2():\n    # Tridiagonal Toeplitz with diag=2, offdiag=-1, n=3\n    n = 3\n    A2 = np.zeros((n, n), dtype=float)\n    np.fill_diagonal(A2, 2.0)\n    for i in range(n - 1):\n        A2[i, i+1] = -1.0\n        A2[i+1, i] = -1.0\n    b2 = np.ones(n, dtype=float)\n    return A2, b2\n\n\ndef build_case3():\n    # Q4 normalized Hadamard matrix\n    Q4 = 0.5 * np.array([\n        [1.0,  1.0,  1.0,  1.0],\n        [1.0,  1.0, -1.0, -1.0],\n        [1.0, -1.0,  1.0, -1.0],\n        [1.0, -1.0, -1.0,  1.0]\n    ], dtype=float)\n    Lambda4 = np.diag([-1.3, 0.9, 0.7, 0.2])\n    T4 = Q4 @ Lambda4 @ Q4.T\n    A3 = np.eye(4) - T4\n    b3 = np.array([1.0, 2.0, 3.0, 4.0], dtype=float)\n    return A3, b3\n\n\ndef build_case4():\n    # Q3 as in case 1\n    Q3 = np.array([\n        [1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0), 1.0/np.sqrt(3.0)],\n        [1.0/np.sqrt(2.0), -1.0/np.sqrt(2.0), 0.0],\n        [1.0/np.sqrt(6.0), 1.0/np.sqrt(6.0), -2.0/np.sqrt(6.0)]\n    ], dtype=float)\n    Lambda3p = np.diag([-1.98, 0.0, 0.0])\n    T3p = Q3 @ Lambda3p @ Q3.T\n    A4 = np.eye(3) - T3p\n    b4 = np.array([0.0, 1.0, 0.0], dtype=float)\n    return A4, b4\n\n\ndef largest_convergent_omega(A, b, omegas, max_iter=10000, tol=1e-10):\n    last_ok = None\n    for w in omegas:\n        ok, _ = damped_jacobi(A, b, w, max_iter=max_iter, tol=tol)\n        if ok:\n            last_ok = w\n    return 0.0 if last_ok is None else float(np.round(last_ok, 10))\n\n\ndef solve():\n    # Define the test cases\n    test_cases = [\n        build_case1(),\n        build_case2(),\n        build_case3(),\n        build_case4()\n    ]\n    # Define omega grid W = {0.05, 0.10, ..., 1.00}\n    omegas = [0.05 * i for i in range(1, 21)]\n    results = []\n    for (A, b) in test_cases:\n        w_star = largest_convergent_omega(A, b, omegas, max_iter=10000, tol=1e-10)\n        results.append(w_star)\n    # Final output in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "When using preconditioned solvers, we often monitor the norm of the preconditioned residual, $z_k = M^{-1} r_k$, as a proxy for convergence. This thought-provoking exercise demonstrates that this proxy can sometimes be misleading by creating a situation where the true residual norm, $\\lVert r_k \\rVert_2$, decreases while the preconditioned residual norm, $\\lVert z_k \\rVert_2$, does not. This practice will sharpen your critical understanding of convergence monitoring and the subtle effects of the linear transformations introduced by preconditioners.",
            "id": "2406627",
            "problem": "You are asked to construct and explore a mathematically controlled teaching example in which the Euclidean norm of the true residual sequence $r_k$ decreases along iterations for solving a linear system, while the Euclidean norm of the preconditioned residual sequence $z_k = M^{-1} r_k$ does not necessarily decrease at every step. The purpose is to reason from first principles about how algorithmic choices and linear transformations interact with norms and monotonicity.\n\nStart from the following fundamental base:\n- The linear system is $A x = b$, where $A$ is symmetric positive definite (SPD).\n- Minimizing the strictly convex quadratic $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$ is equivalent to solving $A x = b$.\n- The steepest descent method uses the negative gradient direction to update the iterate $x_k$ by choosing a step-length that minimizes $f$ along the search direction. The gradient of $f$ is $-\\nabla f(x) = b - A x$, which is the true residual $r(x) = b - A x$.\n- A preconditioner is a linear operator $M$ with $M$ SPD; its action on a residual vector is represented by the preconditioned residual $z = M^{-1} r$. The Euclidean norm of $z$ is generally not the same as the Euclidean norm of $r$ and need not be monotone under an algorithm that does not explicitly minimize that norm.\n\nYour program must:\n- Implement the steepest descent method for SPD systems using exact line search, starting from an initial vector $x_0$, and produce the sequence $\\{r_k\\}_{k=0}^K$ of true residuals for a prescribed number of iterations $K$.\n- For a given SPD preconditioner $M$, compute $z_k = M^{-1} r_k$ at each recorded iteration index $k$.\n- For each test case, determine whether the sequence $\\{\\lVert r_k \\rVert_2\\}_{k=0}^K$ is strictly decreasing and whether the sequence $\\{\\lVert z_k \\rVert_2\\}_{k=0}^K$ is strictly decreasing, using a fixed numerical tolerance $\\varepsilon$ to avoid false negatives due to roundoff. For this problem, use $\\varepsilon = 10^{-12}$, and say that a sequence $\\{a_k\\}$ is strictly decreasing if for all $k$ one has $a_{k+1} \\le (1 - \\varepsilon) a_k$.\n- Report, for each test case, two integers: $1$ if the tested sequence is strictly decreasing under the above rule, and $0$ otherwise. The pair is ordered as $[\\text{flag\\_true\\_residual}, \\text{flag\\_preconditioned\\_residual}]$.\n\nTest suite (all matrices and vectors are given explicitly and are SPD where appropriate):\n- Case A (illustrates that $\\lVert r_k \\rVert_2$ can decrease even when $\\lVert z_k \\rVert_2$ does not):\n  - $$A = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$.\n  - Preconditioner action $$M^{-1} = \\begin{bmatrix} 1  0.9 \\\\ 0.9  1 \\end{bmatrix}$$.\n  - Iteration count $K = 3$.\n- Case B (identity preconditioner so the two sequences coincide):\n  - $$A = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$.\n  - Preconditioner action $$M^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$$.\n  - Iteration count $K = 3$.\n- Case C (boundary case with no iteration, monotonicity is vacuously satisfied):\n  - $$A = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$.\n  - Preconditioner action $$M^{-1} = \\begin{bmatrix} 1  0.9 \\\\ 0.9  1 \\end{bmatrix}$$.\n  - Iteration count $K = 0$.\n\nNumerical and implementation details:\n- Use double-precision floating-point arithmetic.\n- Use the Euclidean norm $\\lVert \\cdot \\rVert_2$ for residual norms.\n- Use the tolerance $\\varepsilon = 10^{-12}$ to judge strict decrease as specified above.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-integer list $[r\\_flag, z\\_flag]$.\n- For the three cases above, the format is thus a single line with no spaces of the form\n  - $[[r\\_A,z\\_A],[r\\_B,z\\_B],[r\\_C,z\\_C]]$\n  where each of $r\\_A, z\\_A, r\\_B, z\\_B, r\\_C, z\\_C$ is either $1$ or $0$.",
            "solution": "The problem as stated is subjected to validation and is found to be scientifically sound, well-posed, and objective. All necessary data and definitions for a unique solution are provided. The matrices specified as symmetric positive definite ($A$ and $M^{-1}$) are verified to possess this property. The problem is a standard exercise in numerical linear algebra, designed to illustrate fundamental properties of iterative methods. We may therefore proceed with a rigorous solution.\n\nThe problem requires the implementation and analysis of the steepest descent method for solving a linear system $A x = b$, where $A$ is a symmetric positive definite (SPD) matrix of size $n \\times n$. This method is equivalent to finding the minimum of the strictly convex quadratic function $f(x) = \\frac{1}{2} x^\\top A x - b^\\top x$. The gradient of this function, $\\nabla f(x) = A x - b$, is the negative of the true residual, $r(x) = b - A x$.\n\nThe steepest descent algorithm generates a sequence of iterates $\\{x_k\\}$ starting from an initial guess $x_0$. At each iteration $k$, the update is performed along the direction of the negative gradient, which is the residual $r_k = b - A x_k$. The update rule is:\n$$\nx_{k+1} = x_k + \\alpha_k r_k\n$$\nwhere the step length $\\alpha_k$ is chosen to minimize the function $f$ along the search direction. That is, $\\alpha_k$ is the solution to $\\min_{\\alpha \\in \\mathbb{R}} f(x_k + \\alpha r_k)$. To find this optimal $\\alpha_k$, we set the derivative of $f(x_k + \\alpha r_k)$ with respect to $\\alpha$ to zero:\n$$\n\\frac{d}{d\\alpha} f(x_k + \\alpha r_k) = \\nabla f(x_k + \\alpha r_k)^\\top r_k = (A(x_k + \\alpha r_k) - b)^\\top r_k = 0\n$$\n$$\n(A x_k - b + \\alpha A r_k)^\\top r_k = (-r_k + \\alpha A r_k)^\\top r_k = -r_k^\\top r_k + \\alpha r_k^\\top A r_k = 0\n$$\nSince $A$ is SPD and $r_k \\neq 0$, $r_k^\\top A r_k  0$, and we can solve for $\\alpha_k$:\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{r_k^\\top A r_k}\n$$\nThis is the formula for the exact line search step length.\n\nThe sequence of residuals $\\{r_k\\}$ can be computed iteratively. From the definition $r_{k+1} = b - A x_{k+1}$ and the update rule for $x_{k+1}$, we derive the recurrence:\n$$\nr_{k+1} = r_k - \\alpha_k A r_k\n$$\nThis recurrence allows for direct computation of the residual sequence, starting with $r_0 = b - A x_0$.\n\nThe problem requires an analysis of the monotonicity of two sequences of norms: the Euclidean norm of the true residual, $\\{\\|r_k\\|_2\\}_{k=0}^K$, and the Euclidean norm of the preconditioned residual, $\\{\\|z_k\\|_2\\}_{k=0}^K$, where $z_k = M^{-1} r_k$.\n\nFor the steepest descent method on an SPD system, the sequence of objective function values $f(x_k)$ is strictly decreasing. This is equivalent to a strict decrease in the energy norm of the error, $\\|x_k - x^*\\|_A^2$, where $x^*$ is the true solution. This also implies a strict decrease in the $A^{-1}$-norm of the residual, $\\|r_k\\|_{A^{-1}}^2 = r_k^\\top A^{-1} r_k$. While this does not guarantee a strict decrease of the Euclidean norm $\\|r_k\\|_2$ in general for $n  2$, for a $2$-dimensional quadratic problem as given, it can be shown that $\\|r_k\\|_2$ is indeed strictly decreasing. This is related to the property that successive residuals are orthogonal, $r_{k+1}^\\top r_k = 0$, which for a $2$-dimensional space implies that the search directions cycle between two orthogonal directions.\n\nThe preconditioned residual is $z_k = M^{-1} r_k$. The mapping from $r_k$ to $z_k$ is a linear transformation. Steepest descent does not perform any optimization with respect to $\\|z_k\\|_2$. The behavior of the sequence $\\{\\|z_k\\|_2\\}$ depends on how the transformation $M^{-1}$ interacts with the sequence of residual vectors $\\{r_k\\}$. If a residual vector $r_{k+1}$ is oriented in a direction that is significantly amplified by $M^{-1}$ compared to the direction of $r_k$, it is possible for $\\|z_{k+1}\\|_2  \\|z_k\\|_2$ even if $\\|r_{k+1}\\|_2  \\|r_k\\|_2$. The problem is constructed to demonstrate this phenomenon.\n\nThe algorithm to be implemented is as follows:\n1. For each test case, define matrices $A$, $M^{-1}$, vectors $b$, $x_0$, and integers $K$.\n2. Initialize two lists, `r_norms` and `z_norms`, to store the norm sequences.\n3. Compute the initial residual $r_0 = b - A x_0$ and the initial preconditioned residual $z_0 = M^{-1} r_0$.\n4. Calculate their Euclidean norms, $\\|r_0\\|_2$ and $\\|z_0\\|_2$, and append them to their respective lists.\n5. Loop for $k$ from $0$ to $K-1$:\n   a. Compute the step length $\\alpha_k = (r_k^\\top r_k) / (r_k^\\top A r_k)$.\n   b. Compute the next residual $r_{k+1} = r_k - \\alpha_k A r_k$.\n   c. Compute the next preconditioned residual $z_{k+1} = M^{-1} r_{k+1}$.\n   d. Calculate the norms $\\|r_{k+1}\\|_2$ and $\\|z_{k+1}\\|_2$ and append them to the lists.\n6. After the loop, analyze the monotonicity of `r_norms` and `z_norms`. A sequence $\\{a_j\\}_{j=0}^K$ is strictly decreasing if $a_{j+1} \\le (1 - \\varepsilon) a_j$ for all $j=0, \\dots, K-1$, with the given tolerance $\\varepsilon = 10^{-12}$.\n7. If $K=0$, the sequence has one element, and the condition is vacuously true. Both monotonicity flags are set to $1$. Otherwise, iterate through the sequence pairs and check the condition. If any pair violates the condition, set the corresponding flag to $0$.\n\nThis procedure will be applied to each of the three test cases to generate the required output. Case A is designed to show non-monotonicity of $\\{\\|z_k\\|_2\\}$. Case B, with $M^{-1}=I$, will show both sequences are monotonic since $z_k=r_k$. Case C covers the boundary condition of $K=0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the steepest descent method and\n    checking the monotonicity of true and preconditioned residual norms.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 4.0]]),\n            \"b\": np.array([1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n            \"M_inv\": np.array([[1.0, 0.9], [0.9, 1.0]]),\n            \"K\": 3,\n            \"epsilon\": 1e-12\n        },\n        # Case B\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 4.0]]),\n            \"b\": np.array([1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n            \"M_inv\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"K\": 3,\n            \"epsilon\": 1e-12\n        },\n        # Case C\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 4.0]]),\n            \"b\": np.array([1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n            \"M_inv\": np.array([[1.0, 0.9], [0.9, 1.0]]),\n            \"K\": 0,\n            \"epsilon\": 1e-12\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        x0 = case[\"x0\"]\n        M_inv = case[\"M_inv\"]\n        K = case[\"K\"]\n        epsilon = case[\"epsilon\"]\n\n        r_norms = []\n        z_norms = []\n\n        # Initial step (k=0)\n        r = b - A @ x0\n        z = M_inv @ r\n        \n        r_norms.append(np.linalg.norm(r, 2))\n        z_norms.append(np.linalg.norm(z, 2))\n\n        # Iterations from k=0 to K-1\n        for _ in range(K):\n            r_dot_r = r @ r\n            Ar = A @ r\n            r_dot_Ar = r @ Ar\n            \n            # Avoid division by zero if r is already zero.\n            if abs(r_dot_Ar)  1e-15:\n                # If r is effectively zero, the sequence has converged and stays constant.\n                # This would fail the strict decrease test, but we can stop.\n                # Just append the same norm to maintain sequence length.\n                r_norms.append(r_norms[-1])\n                z_norms.append(z_norms[-1])\n                continue\n\n            alpha = r_dot_r / r_dot_Ar\n            r = r - alpha * Ar\n            z = M_inv @ r\n            \n            r_norms.append(np.linalg.norm(r, 2))\n            z_norms.append(np.linalg.norm(z, 2))\n        \n        # Check for strict decrease\n        r_flag = 1\n        z_flag = 1\n\n        if K  0:\n            for k in range(K):\n                # Check true residual norm\n                if r_norms[k+1]  (1 - epsilon) * r_norms[k]:\n                    r_flag = 0\n                    \n                # Check preconditioned residual norm\n                if z_norms[k+1]  (1 - epsilon) * z_norms[k]:\n                    z_flag = 0\n        \n        # For K=0, the loops do not run, and the flags remain 1, which is\n        # the correct 'vacuously true' result.\n\n        results.append([r_flag, z_flag])\n\n    # Final print statement in the exact required format.\n    # The format requires no spaces, e.g., [[1,0],[1,1],[1,1]].\n    # str(results).replace(' ', '') achieves this.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```"
        },
        {
            "introduction": "While simple preconditioners like Jacobi can offer modest improvements, more advanced techniques are often needed to tackle challenging, ill-conditioned systems. This hands-on practice focuses on accelerating the powerful Conjugate Gradient method using a polynomial preconditioner based on a Neumann series approximation of the matrix inverse. By comparing the iteration counts of unpreconditioned, Jacobi-preconditioned, and polynomial-preconditioned CG, you will witness firsthand the dramatic impact a well-designed preconditioner can have on performance.",
            "id": "2406599",
            "problem": "You are given a family of symmetric positive definite linear systems of the form $A_n x = b$ where $A_n \\in \\mathbb{R}^{n \\times n}$ is the standard second-order finite-difference discretization of the one-dimensional negative Laplacian on the open interval $(0,1)$ with homogeneous Dirichlet boundary conditions. Explicitly, $A_n$ is the tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals, and zeros elsewhere. Let $b \\in \\mathbb{R}^n$ be the vector with all entries equal to $1$, and let the initial guess be $x_0 = 0$. Denote by $D_n = \\mathrm{diag}(A_n)$ the diagonal matrix formed by the diagonal entries of $A_n$, and by $R_n = D_n - A_n$ the strictly off-diagonal part. For a nonnegative integer $m$, define the polynomial approximate inverse (truncated Neumann series) of order $m$ by\n$$\nP_m \\;=\\; \\sum_{k=0}^{m} \\left(D_n^{-1} R_n\\right)^k \\, D_n^{-1}.\n$$\nFor each test case specified below, consider the following three iteration counts, each defined as the smallest integer $k \\in \\{0,1,2,\\dots,n\\}$ such that the stated method produces an iterate $x_k$ with relative residual $\\|b - A_n x_k\\|_2 / \\|b\\|_2 \\le \\tau$, where $\\tau = 10^{-8}$ and the maximum number of iterations is $n$:\n$1)$ $k_{\\mathrm{unpre}}(n)$, obtained by the method that generates $A_n$-conjugate search directions and minimizes the $\\|b - A_n x\\|_2$ residual over successive affine Krylov subspaces $x_0 + \\mathcal{K}_k(A_n, r_0)$ with $r_0 = b - A_n x_0$.\n$2)$ $k_{\\mathrm{jac}}(n)$, obtained by the same method applied to the left-preconditioned system defined via the diagonal preconditioning operator $D_n^{-1}$, that is, using the auxiliary update $z = D_n^{-1} r$ at each iteration in place of $z = r$.\n$3)$ $k_{\\mathrm{poly}}(n,m)$, obtained by the same method applied to the left-preconditioned system defined via the polynomial operator $P_m$, that is, using the auxiliary update $z = P_m r$ at each iteration.\n\nTest suite. For each pair $(n,m)$ below, compute and report the triple of integers $[\\,k_{\\mathrm{unpre}}(n),\\,k_{\\mathrm{jac}}(n),\\,k_{\\mathrm{poly}}(n,m)\\,]$:\n- $(n,m) = (100,0)$\n- $(n,m) = (100,3)$\n- $(n,m) = (200,3)$\n- $(n,m) = (5,10)$\n\nYour program should produce a single line of output containing the results as a comma-separated list of these triples, enclosed in square brackets. Concretely, the output format must be\n$[\\,[k_{\\mathrm{unpre}}(n_1),k_{\\mathrm{jac}}(n_1),k_{\\mathrm{poly}}(n_1,m_1)],\\,[k_{\\mathrm{unpre}}(n_2),k_{\\mathrm{jac}}(n_2),k_{\\mathrm{poly}}(n_2,m_2)],\\,\\dots\\,]$ with no extra whitespace or text.",
            "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n-   **Linear System**: $A_n x = b$, where $A_n \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD).\n-   **Matrix $A_n$**: A tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals. This represents the 1D finite-difference Laplacian.\n-   **Vector $b$**: $b \\in \\mathbb{R}^n$ where $b_i = 1$ for all $i=1, \\dots, n$.\n-   **Initial Guess $x_0$**: The zero vector, $x_0 = 0$.\n-   **Matrix Decomposition**: $A_n = D_n - R_n$, where $D_n = \\mathrm{diag}(A_n) = 2I_n$ and $R_n$ is the strictly off-diagonal part.\n-   **Polynomial Preconditioner $P_m$**: $P_m = \\sum_{k=0}^{m} (D_n^{-1} R_n)^k D_n^{-1}$.\n-   **Iterative Method**: The Conjugate Gradient (CG) method, correctly identified by its properties of generating $A_n$-conjugate search directions and minimizing the residual in a specific sense over a Krylov subspace.\n-   **Stopping Criterion**: The relative residual 2-norm must be less than or equal to a tolerance $\\tau = 10^{-8}$, i.e., $\\|b - A_n x_k\\|_2 / \\|b\\|_2 \\le \\tau$.\n-   **Maximum Iterations**: $n$.\n-   **Quantities to Compute**: Three iteration counts for each test case $(n,m)$:\n    1.  $k_{\\mathrm{unpre}}(n)$: Standard, unpreconditioned CG.\n    2.  $k_{\\mathrm{jac}}(n)$: CG preconditioned with the Jacobi preconditioner $M=D_n$.\n    3.  $k_{\\mathrm{poly}}(n,m)$: CG preconditioned with the polynomial preconditioner $M^{-1}=P_m$.\n-   **Test Cases**: $(n,m) \\in \\{(100,0), (100,3), (200,3), (5,10)\\}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the specified criteria.\n-   **Scientific Grounding**: The problem is fundamentally sound. It involves the analysis of the Conjugate Gradient method, a cornerstone algorithm in numerical linear algebra, applied to a classic SPD matrix arising from the discretization of the Poisson equation. The preconditioners presented—Jacobi and polynomial (Neumann series)—are standard techniques.\n-   **Well-Posedness**: All components are precisely defined. The matrix $A_n$ is known to be SPD, ensuring the CG method is applicable and will converge to the unique solution. The iteration counts are well-defined quantities determined by a clear stopping criterion. The polynomial preconditioner $P_m$ is based on the Neumann series for $A_n^{-1}$, which is convergent for the specified matrix $A_n$ because the spectral radius of the Jacobi iteration matrix $J=D_n^{-1}R_n$ is $\\rho(J) = \\cos(\\pi/(n+1))  1$.\n-   **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is a well-posed, scientifically grounded problem in computational engineering. A solution will be constructed.\n\n**Methodology**\nThe core of the solution lies in implementing the Preconditioned Conjugate Gradient (PCG) algorithm. The standard (unpreconditioned) CG method is a special case of PCG where the preconditioner is the identity matrix.\n\nThe PCG algorithm for solving $Ax=b$ with a preconditioner $M$ is as follows:\n1.  Initialize: $k=0$, $x_0 = 0$, $r_0 = b - A x_0 = b$.\n2.  If $\\|r_0\\|_2 / \\|b\\|_2 \\le \\tau$, terminate with iteration count $k=0$.\n3.  Solve for $z_0$: $M z_0 = r_0$.\n4.  Set search direction: $p_0 = z_0$.\n5.  Iterate for $k = 0, 1, 2, \\dots$ up to a maximum of $n-1$:\n    a. Compute step size: $\\alpha_k = \\frac{r_k^T z_k}{p_k^T A p_k}$.\n    b. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    c. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$.\n    d. Check for convergence: If $\\|r_{k+1}\\|_2 / \\|b\\|_2 \\le \\tau$, terminate with iteration count $k+1$.\n    e. Apply preconditioner: Solve $M z_{k+1} = r_{k+1}$.\n    f. Compute improvement factor: $\\beta_k = \\frac{r_{k+1}^T z_{k+1}}{r_k^T z_k}$.\n    g. Update search direction: $p_{k+1} = z_{k+1} + \\beta_k p_k$.\n\nThis single algorithm will be used to compute all required iteration counts by varying the 'solve' step ($M z = r$).\n\n**Preconditioner Implementation**\nThe three required scenarios correspond to three different definitions of the preconditioning step $z_k = M^{-1} r_k$.\n\n1.  **Unpreconditioned ($k_{\\mathrm{unpre}}$)**: This is equivalent to setting $M=I_n$, the identity matrix. The preconditioning step is trivial: $z_k = r_k$.\n\n2.  **Jacobi Preconditioning ($k_{\\mathrm{jac}}$)**: The preconditioner is $M=D_n = \\mathrm{diag}(A_n)$. For the given matrix $A_n$, $D_n = 2I_n$. The preconditioning step is a simple scaling: $z_k = D_n^{-1} r_k = \\frac{1}{2}r_k$.\n\n3.  **Polynomial Preconditioning ($k_{\\mathrm{poly}}$)**: The preconditioner is defined by its inverse, $M^{-1} = P_m$. The preconditioning step is a matrix-vector product: $z_k = P_m r_k$. The operator $P_m$ is given by\n    $$\n    P_m = \\sum_{j=0}^{m} (D_n^{-1} R_n)^j D_n^{-1}\n    $$\n    The application $z_k = P_m r_k$ is computed efficiently without explicitly forming the matrix $P_m$. Let $T = D_n^{-1} R_n$ and $v = D_n^{-1} r_k$. The problem reduces to computing $z_k = (\\sum_{j=0}^{m} T^j) v$. This can be implemented with an iterative loop:\n    -   Initialize sum $s = v$.\n    -   Initialize term $t = v$.\n    -   For $j = 1, \\dots, m$:\n        -   Update term: $t = Tt$.\n        -   Update sum: $s = s + t$.\n    -   The result is $z_k = s$.\n    Since $D_n = 2I_n$ and $R_n$ is a sparse matrix with $1$ on its first off-diagonals, the matrix-vector product $Tt$ is computationally inexpensive, with a cost of $O(n)$ floating-point operations. The total cost of applying the polynomial preconditioner is $O(m \\cdot n)$.\n\nFor the test case $(n,m)=(100,0)$, the polynomial preconditioner $P_0$ simplifies to $P_0 = (D_n^{-1}R_n)^0 D_n^{-1} = I_n D_n^{-1} = D_n^{-1}$. This is identical to the Jacobi preconditioner. Therefore, we expect $k_{\\mathrm{jac}}(100) = k_{\\mathrm{poly}}(100,0)$, which serves as a consistency check for the implementation.\n\nThe final implementation will construct the sparse matrices $A_n$ and $R_n$ and the vector $b$, then execute the PCG algorithm for each of the three preconditioning strategies for each test case $(n,m)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\n\ndef get_matrices(n):\n    \"\"\"Constructs the sparse matrices A_n and R_n.\"\"\"\n    # A_n is the 1D finite difference matrix: tridiagonal(-1, 2, -1)\n    diagonals_A = [-1 * np.ones(n - 1), 2 * np.ones(n), -1 * np.ones(n - 1)]\n    offsets_A = [-1, 0, 1]\n    A_n = sparse.diags(diagonals_A, offsets_A, shape=(n, n), format='csr')\n\n    # R_n = D_n - A_n. Since D_n = 2*I, R_n has 0 on diagonal, 1 on off-diagonals.\n    diagonals_R = [np.ones(n - 1), np.ones(n - 1)]\n    offsets_R = [-1, 1]\n    R_n = sparse.diags(diagonals_R, offsets_R, shape=(n, n), format='csr')\n    \n    return A_n, R_n\n\ndef poly_preconditioner_solve(r, R_matrix, m):\n    \"\"\"Computes z = P_m * r.\"\"\"\n    n = r.shape[0]\n    \n    # D_n = 2*I, so D_n_inv is a scaling by 0.5\n    d_inv_r = 0.5 * r\n    \n    # T = D_n_inv * R_n is a scaling of R_n by 0.5\n    T_mat = 0.5 * R_matrix\n\n    z = d_inv_r.copy()\n    t = d_inv_r.copy()\n    \n    for _ in range(m):\n        t = T_mat @ t\n        z += t\n        \n    return z\n\ndef pcg_solve(A, b, x_init, m_solve, tol, max_iter):\n    \"\"\"\n    Solves Ax=b using the Preconditioned Conjugate Gradient method.\n    \n    Args:\n        A: The system matrix (sparse).\n        b: The right-hand side vector.\n        x_init: The initial guess vector.\n        m_solve: A function that computes z = M_inv * r.\n        tol: The relative residual tolerance.\n        max_iter: The maximum number of iterations.\n        \n    Returns:\n        The number of iterations performed.\n    \"\"\"\n    x = x_init.copy()\n    r = b - A @ x\n    \n    norm_b = np.linalg.norm(b)\n    if norm_b == 0.0:\n        return 0\n\n    if np.linalg.norm(r) / norm_b = tol:\n        return 0\n\n    z = m_solve(r)\n    p = z.copy()\n    rz_old = r.dot(z)\n\n    for k in range(max_iter):\n        Ap = A @ p\n        alpha = rz_old / p.dot(Ap)\n        \n        x += alpha * p\n        r_new = r - alpha * Ap\n\n        if np.linalg.norm(r_new) / norm_b = tol:\n            return k + 1\n\n        z_new = m_solve(r_new)\n        rz_new = r_new.dot(z_new)\n        \n        beta = rz_new / rz_old\n        \n        p = z_new + beta * p\n        r = r_new\n        rz_old = rz_new\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (100, 0),\n        (100, 3),\n        (200, 3),\n        (5, 10),\n    ]\n\n    results = []\n    tau = 1e-8\n\n    for n, m in test_cases:\n        A_n, R_n = get_matrices(n)\n        b = np.ones(n)\n        x0 = np.zeros(n)\n        max_iter = n\n\n        # 1. Unpreconditioned CG\n        m_solve_unpre = lambda r: r\n        k_unpre = pcg_solve(A_n, b, x0, m_solve_unpre, tau, max_iter)\n        \n        # 2. Jacobi preconditioned CG\n        m_solve_jac = lambda r: 0.5 * r\n        k_jac = pcg_solve(A_n, b, x0, m_solve_jac, tau, max_iter)\n        \n        # 3. Polynomial preconditioned CG\n        m_solve_poly = lambda r: poly_preconditioner_solve(r, R_n, m)\n        k_poly = pcg_solve(A_n, b, x0, m_solve_poly, tau, max_iter)\n        \n        results.append([k_unpre, k_jac, k_poly])\n\n    # Format the output string as specified, with no extra whitespace.\n    inner_strings = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}