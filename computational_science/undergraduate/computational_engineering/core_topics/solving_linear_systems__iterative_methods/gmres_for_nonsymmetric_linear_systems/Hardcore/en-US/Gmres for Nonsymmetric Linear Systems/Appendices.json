{
    "hands_on_practices": [
        {
            "introduction": "To truly master an algorithm, you must build it. This first practice challenges you to implement the core GMRES solver from its foundational principles: the Arnoldi process for basis construction and the subsequent least-squares minimization. By coding the method and testing it on a variety of systems, you will gain a tangible understanding of how GMRES navigates the solution space one Krylov subspace at a time. ",
            "id": "2397285",
            "problem": "Given a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, a right-hand-side vector $b \\in \\mathbb{R}^{n}$, an initial guess $x_0 \\in \\mathbb{R}^{n}$, and a positive integer $m$ with $1 \\le m \\le n$, consider the Generalized Minimal Residual (GMRES) approximation $x_m \\in x_0 + \\mathcal{K}_m(A,r_0)$ that minimizes the $2$-norm of the residual, where $r_0 = b - A x_0$ and $\\mathcal{K}_m(A,r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{m-1} r_0\\}$. For each test case below, compute the vector $x_m$ that solves\n$$\n\\min_{x \\in x_0 + \\mathcal{K}_m(A,r_0)} \\| b - A x \\|_2\n$$\nand report the following two quantities: (i) the achieved residual norm $\\| b - A x_m \\|_2$ rounded to eight decimal places, and (ii) the number of Krylov steps actually performed $k$, defined as the dimension of the constructed subspace used by the approximation, with $k \\in \\{0,1,\\dots,m\\}$. The case $k=0$ corresponds to $\\|r_0\\|_2 = 0$, in which case $x_m = x_0$ and the residual norm is $0$. If invariance of the subspace is encountered before $m$ steps, use the attained $k < m$.\n\nThe test suite consists of five independent cases. Each case specifies $A$, $b$, $x_0$, and $m$:\n\n1) Case 1 (square, nonsymmetric, full subspace):\n- $A = \\begin{bmatrix}\n4 & 1 & 0 & 0 \\\\\n2 & 3 & 1 & 0 \\\\\n0 & 1 & 3 & 1 \\\\\n0 & 0 & 1 & 2\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n$m = 4$.\n\n2) Case 2 (square, nonsymmetric, partial subspace):\n- $A = \\begin{bmatrix}\n3 & -1 & 0 & 0 & 0 \\\\\n2 & 4 & 1 & 0 & 0 \\\\\n0 & -2 & 3 & 1 & 0 \\\\\n0 & 0 & -1 & 2 & 1 \\\\\n0 & 0 & 0 & -3 & 1\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n$m = 3$.\n\n3) Case 3 (upper triangular with large superdiagonal entries, full subspace):\n- $A = \\begin{bmatrix}\n1 & 10 & 0 & 0 & 0 \\\\\n0 & 1 & 10 & 0 & 0 \\\\\n0 & 0 & 1 & 10 & 0 \\\\\n0 & 0 & 0 & 1 & 10 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n$m = 5$.\n\n4) Case 4 (nonzero initial guess, single Krylov step):\n- $A = \\begin{bmatrix}\n2 & -1 & 0 & 0 \\\\\n1 & 2 & -1 & 0 \\\\\n0 & 1 & 2 & -1 \\\\\n0 & 0 & 1 & 2\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}$,\n$m = 1$.\n\n5) Case 5 (early exact convergence at the initial guess):\n- $A = \\begin{bmatrix}\n2 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n1 & 0 & 2\n\\end{bmatrix}$,\n$x_{\\text{true}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$,\n$b = A x_{\\text{true}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 5 \\end{bmatrix}$,\n$x_0 = x_{\\text{true}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$,\n$m = 3$.\n\nYour program must process all five cases in the stated order. For each case, compute $x_m$, then compute the residual norm $\\|b - A x_m\\|_2$, round it to eight decimal places, determine $k$, and aggregate the results into a single output line in the following format:\n- A single line containing a comma-separated list enclosed in square brackets, where each element is a two-element list $[\\text{residual\\_norm}, \\text{krylov\\_steps}]$.\n- For example, the overall output should look like $[[r_1,k_1],[r_2,k_2],[r_3,k_3],[r_4,k_4],[r_5,k_5]]$, where each $r_i$ is a float rounded to eight decimal places and each $k_i$ is an integer.",
            "solution": "The target is the Generalized Minimal Residual (GMRES) approximation, defined as follows. Given $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$, an initial guess $x_0 \\in \\mathbb{R}^{n}$, and an integer $m$ with $1 \\le m \\le n$, define the initial residual $r_0 = b - A x_0$ and the $m$-dimensional Krylov subspace $\\mathcal{K}_m(A,r_0) = \\text{span}\\{ r_0, A r_0, \\dots, A^{m-1} r_0 \\}$. The GMRES approximation $x_m$ is the unique vector in the affine space $x_0 + \\mathcal{K}_m(A,r_0)$ that minimizes the $2$-norm of the residual, that is,\n$$\nx_m \\in \\arg\\min_{x \\in x_0 + \\mathcal{K}_m(A,r_0)} \\| b - A x \\|_2.\n$$\n\nA principled way to compute $x_m$ rests on two fundamental components: an orthonormal basis of the Krylov subspace and a reduced least-squares problem. Construct an orthonormal basis of $\\mathcal{K}_m(A,r_0)$ using the Arnoldi process. Start from $r_0$ and its norm $\\beta = \\|r_0\\|_2$. If $\\beta = 0$, then $x_0$ is already a solution, the minimizing residual norm is $0$, and no Krylov steps are needed, so we set $k = 0$ and $x_m = x_0$.\n\nIf $\\beta > 0$, define $v_1 = r_0 / \\beta$. The Arnoldi process generates orthonormal vectors $v_1, v_2, \\dots, v_{k+1}$ (with $k \\le m$) and an upper Hessenberg matrix $\\bar{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$ such that\n$$\nA V_k = V_{k+1} \\bar{H}_k,\n$$\nwhere $V_k = [v_1,\\dots,v_k] \\in \\mathbb{R}^{n \\times k}$ and $V_{k+1} = [v_1,\\dots,v_{k+1}] \\in \\mathbb{R}^{n \\times (k+1)}$. The standard modified Gram–Schmidt orthogonalization produces the coefficients of $\\bar{H}_k$ and the orthonormal basis. If a subdiagonal entry $h_{j+1,j}$ becomes zero for some 0-indexed step $j  m$, the process stops early. This indicates that the Krylov subspace $\\mathcal{K}_{j+1}(A,r_0)$ is $A$-invariant. The number of Krylov steps performed is thus $k = j+1 \\le m$.\n\nThe approximation $x_m$ can be parameterized as $x = x_0 + V_k y$ for some $y \\in \\mathbb{R}^k$. Substituting into the residual and using the Arnoldi relation yields\n$$\n\\| b - A (x_0 + V_k y) \\|_2 = \\| r_0 - A V_k y \\|_2 = \\| \\beta v_1 - V_{k+1} \\bar{H}_k y \\|_2 = \\| \\beta e_1 - \\bar{H}_k y \\|_2,\n$$\nwhere $e_1 \\in \\mathbb{R}^{k+1}$ is the first standard basis vector. Therefore, the coefficient vector $y_m$ solves the reduced least-squares problem\n$$\ny_m \\in \\arg\\min_{y \\in \\mathbb{R}^k} \\| \\bar{H}_k y - \\beta e_1 \\|_2.\n$$\nAny numerically stable least-squares solver (for example, based on the singular value decomposition) yields $y_m$. The minimizing approximation is then obtained as\n$$\nx_m = x_0 + V_k y_m,\n$$\nand the minimal residual norm is\n$$\n\\| b - A x_m \\|_2 = \\| \\beta e_1 - \\bar{H}_k y_m \\|_2.\n$$\n\nThe algorithm thus follows first principles:\n1) Compute $r_0 = b - A x_0$ and $\\beta = \\|r_0\\|_2$. If $\\beta = 0$, set $k = 0$, $x_m = x_0$, residual norm $= 0$.\n2) Otherwise, build an orthonormal basis of $\\mathcal{K}_m(A,r_0)$ via the Arnoldi relation using modified Gram–Schmidt, accumulating the entries of $\\bar{H}_k$ until either $m$ steps are completed or a subdiagonal entry becomes numerically zero, which sets $k$.\n3) Solve the reduced least-squares problem for $y_m$ and form $x_m = x_0 + V_k y_m$.\n4) Compute the residual norm $\\|b - A x_m\\|_2$ and round it to eight decimal places. Report $k$ as the number of Krylov steps actually performed.\n\nFor the provided test cases, all matrices are square and nonsymmetric, the vectors are explicitly given, and the integer $m$ satisfies $1 \\le m \\le n$. Case $5$ illustrates early exact convergence at the initial guess with $k = 0$. The final output must be a single line with the aggregate list $[[r_1,k_1],[r_2,k_2],[r_3,k_3],[r_4,k_4],[r_5,k_5]]$, where each $r_i$ is the residual norm rounded to eight decimal places and each $k_i$ is an integer in $\\{0,1,\\dots,m\\}$ for its case.",
            "answer": "```python\nimport numpy as np\n\ndef arnoldi_basis(A, r0, m, tol=1e-14):\n    \"\"\"\n    Perform the Arnoldi process with modified Gram-Schmidt to generate\n    an orthonormal basis V and upper Hessenberg matrix H_bar.\n    Returns:\n        V (n x (k+1)) with columns v1..v_{k+1} if k>=1; if k==0, V is empty\n        H ( (k+1) x k ) upper Hessenberg\n        beta (norm of r0)\n        k (number of Krylov steps actually performed, 0=k=m)\n    \"\"\"\n    n = A.shape[0]\n    beta = np.linalg.norm(r0)\n    if beta == 0.0:\n        # No steps needed; exact at initial guess\n        return np.zeros((n, 0)), np.zeros((1, 0)), 0.0, 0\n\n    # Preallocate maximum sizes; we'll slice by the actual k\n    V = np.zeros((n, m + 1), dtype=float)\n    H = np.zeros((m + 1, m), dtype=float)\n\n    V[:, 0] = r0 / beta\n    k = 0\n    for j in range(m):\n        w = A @ V[:, j]\n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            H[i, j] = np.dot(V[:, i], w)\n            w = w - H[i, j] * V[:, i]\n        H[j + 1, j] = np.linalg.norm(w)\n        if H[j + 1, j] = tol:\n            # Invariant subspace reached; breakdown\n            k = j + 1  # Number of columns in V_k\n            # We cannot form V[:, j+1]; stop here\n            break\n        V[:, j + 1] = w / H[j + 1, j]\n        k = j + 1  # Update number of Krylov steps performed\n    # Slice to actual sizes: V has (k+1) columns if k>=1; H is (k+1) x k\n    V_used = V[:, : (k + 1) ] if k = 1 else np.zeros((n, 0))\n    H_used = H[: (k + 1), : k] if k = 1 else np.zeros((1, 0))\n    return V_used, H_used, beta, k\n\ndef gmres_minres(A, b, x0, m, tol=1e-14):\n    \"\"\"\n    Compute x_m in x0 + K_m(A, r0) minimizing ||b - A x||_2 via Arnoldi and least squares.\n    Returns:\n        x_m (approximate solution),\n        res_norm (float residual norm),\n        k (int number of Krylov steps actually performed, 0=k=m)\n    \"\"\"\n    r0 = b - A @ x0\n    V, H, beta, k = arnoldi_basis(A, r0, m, tol=tol)\n    if k == 0:\n        # Exact at initial guess\n        return x0.copy(), 0.0, 0\n    # Solve min || H y - beta e1 ||_2\n    e1 = np.zeros((k + 1,), dtype=float)\n    e1[0] = 1.0\n    rhs = beta * e1\n    # Least squares solution using SVD-based solver\n    y, *_ = np.linalg.lstsq(H, rhs, rcond=None)\n    # Form x_m\n    x_m = x0 + V[:, :k] @ y\n    res = b - A @ x_m\n    res_norm = float(np.linalg.norm(res))\n    return x_m, res_norm, k\n\ndef solve():\n    # Define test cases as per the problem statement.\n    tests = []\n\n    # Case 1\n    A1 = np.array([\n        [4.0, 1.0, 0.0, 0.0],\n        [2.0, 3.0, 1.0, 0.0],\n        [0.0, 1.0, 3.0, 1.0],\n        [0.0, 0.0, 1.0, 2.0]\n    ], dtype=float)\n    b1 = np.array([1.0, 2.0, 3.0, 4.0], dtype=float)\n    x01 = np.zeros(4, dtype=float)\n    m1 = 4\n    tests.append((A1, b1, x01, m1))\n\n    # Case 2\n    A2 = np.array([\n        [3.0, -1.0, 0.0, 0.0, 0.0],\n        [2.0,  4.0, 1.0, 0.0, 0.0],\n        [0.0, -2.0, 3.0, 1.0, 0.0],\n        [0.0,  0.0,-1.0, 2.0, 1.0],\n        [0.0,  0.0, 0.0,-3.0, 1.0]\n    ], dtype=float)\n    b2 = np.array([1.0, 0.0, 1.0, 0.0, 1.0], dtype=float)\n    x02 = np.zeros(5, dtype=float)\n    m2 = 3\n    tests.append((A2, b2, x02, m2))\n\n    # Case 3\n    A3 = np.array([\n        [1.0, 10.0,  0.0,  0.0,  0.0],\n        [0.0,  1.0, 10.0,  0.0,  0.0],\n        [0.0,  0.0,  1.0, 10.0,  0.0],\n        [0.0,  0.0,  0.0,  1.0, 10.0],\n        [0.0,  0.0,  0.0,  0.0,  1.0]\n    ], dtype=float)\n    b3 = np.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype=float)\n    x03 = np.zeros(5, dtype=float)\n    m3 = 5\n    tests.append((A3, b3, x03, m3))\n\n    # Case 4\n    A4 = np.array([\n        [2.0, -1.0,  0.0,  0.0],\n        [1.0,  2.0, -1.0,  0.0],\n        [0.0,  1.0,  2.0, -1.0],\n        [0.0,  0.0,  1.0,  2.0]\n    ], dtype=float)\n    b4 = np.array([1.0, 2.0, 2.0, 1.0], dtype=float)\n    x04 = np.array([0.5, -0.5, 0.5, -0.5], dtype=float)\n    m4 = 1\n    tests.append((A4, b4, x04, m4))\n\n    # Case 5\n    A5 = np.array([\n        [2.0, 1.0, 0.0],\n        [0.0, 3.0, 1.0],\n        [1.0, 0.0, 2.0]\n    ], dtype=float)\n    x_true5 = np.array([1.0, -1.0, 2.0], dtype=float)\n    b5 = A5 @ x_true5\n    x05 = x_true5.copy()\n    m5 = 3\n    tests.append((A5, b5, x05, m5))\n\n    results = []\n    for A, b, x0, m in tests:\n        _, res_norm, k = gmres_minres(A, b, x0, m, tol=1e-14)\n        # Round residual norm to eight decimal places as required\n        res_rounded = round(res_norm, 8)\n        results.append([res_rounded, int(k)])\n\n    # Print in the exact required single-line format\n    # Ensure standard Python list formatting\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Beyond correctness, the efficiency of a numerical algorithm is paramount. This exercise moves from implementation to analysis, asking you to derive the computational cost of a single GMRES iteration. By carefully counting the floating-point operations based on a specified model, you will identify the most expensive parts of the algorithm and understand how its workload scales with problem size and iteration count. ",
            "id": "2397343",
            "problem": "Consider a single iteration of the Generalized Minimal Residual method (GMRES) applied to a nonsymmetric linear system with coefficient matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A$ has $s$ nonzero entries. Assume an implementation that uses the modified Gram–Schmidt process to build an orthonormal Krylov basis $\\{v_{1},\\dots,v_{m}\\}$ and uses Givens plane rotations to update the corresponding $(m+1)\\times m$ upper Hessenberg matrix $\\bar H_{m}$ and the right-hand-side vector of the reduced least-squares problem. In the $m$-th GMRES iteration (that is, after $\\{v_{1},\\dots,v_{m}\\}$ have been constructed and before computing $v_{m+1}$), the following operations are performed in this order:\n1. Compute $w = A v_{m}$.\n2. For $j = 1,\\dots,m$, compute the inner product $h_{j,m} = v_{j}^{\\top} w$ and update $w \\leftarrow w - h_{j,m} v_{j}$.\n3. Compute $h_{m+1,m} = \\lVert w \\rVert_{2}$ and then $v_{m+1} = w / h_{m+1,m}$.\n4. Apply the $m-1$ previously accumulated Givens rotations to the new column of $\\bar H_{m}$, form the new Givens rotation that zeros $h_{m+1,m}$, apply it to the pair $(h_{m,m},h_{m+1,m})$, and update the corresponding two entries of the reduced right-hand-side vector.\n\nAdopt the following floating-point operation (flop) model: each scalar addition, subtraction, multiplication, division, or square root counts as exactly $1$ flop. Ignore all memory access costs and data movement. For a sparse matrix–vector product with $s$ nonzeros, assume it costs exactly $2s$ flops. For a vector inner product of length $n$, assume it costs exactly $2n-1$ flops. For a vector update of the form $w \\leftarrow w - \\alpha v$ with vectors of length $n$, assume it costs exactly $2n$ flops. For the Euclidean norm of a length-$n$ vector, assume it costs exactly $2n$ flops (including the final square root). For normalizing a vector of length $n$ by dividing by a scalar, assume it costs exactly $n$ flops. For applying a single Givens rotation to a pair of scalars, assume it costs exactly $6$ flops. For forming the parameters of a Givens rotation from two scalars, assume it costs exactly $6$ flops.\n\nUnder these assumptions, derive a closed-form expression $F(n,s,m)$ for the exact total number of flops performed in this single GMRES iteration. Your final answer must be a single simplified analytic expression in terms of $n$, $s$, and $m$. Do not round.",
            "solution": "We are tasked with deriving the total number of floating-point operations, $F(n,s,m)$, for the $m$-th iteration of the GMRES algorithm, following the prescribed operational steps and flop-count model. We will analyze each of the four computational stages in sequence.\n\n**Stage 1: Sparse Matrix–Vector Product**\nThe first operation is the computation of the vector $w$ via the matrix-vector product $w = A v_{m}$. The matrix $A$ has dimensions $n \\times n$ and contains $s$ nonzero entries. The vector $v_m$ is of length $n$. According to the problem's model, the cost of this operation, which we denote as $C_1$, is:\n$$C_1 = 2s$$\n\n**Stage 2: Modified Gram-Schmidt Orthogonalization**\nThis stage orthogonalizes the vector $w$ against the previously computed orthonormal basis vectors $\\{v_1, v_2, \\dots, v_m\\}$. This is accomplished through a loop that runs from $j=1$ to $j=m$. For each value of $j$, two operations are performed:\n- An inner product $h_{j,m} = v_{j}^{\\top} w$ between two vectors of length $n$. The specified cost is $2n-1$ flops.\n- A vector update $w \\leftarrow w - h_{j,m} v_{j}$. The specified cost for this operation on vectors of length $n$ is $2n$ flops.\nThe cost for a single iteration of this loop is the sum of the costs of these two operations: $(2n-1) + 2n = 4n-1$ flops. Since the loop executes $m$ times, the total cost for Stage $2$, denoted $C_2$, is:\n$$C_2 = m(4n-1) = 4nm - m$$\n\n**Stage 3: Normalization**\nAfter the orthogonalization process, the resulting vector $w$ is normalized to produce the next orthonormal basis vector, $v_{m+1}$. This involves two steps:\n- The computation of the Euclidean norm $h_{m+1,m} = \\lVert w \\rVert_{2}$ for the vector $w$ of length $n$. The specified cost is $2n$ flops.\n- The normalization of the vector $v_{m+1} = w / h_{m+1,m}$. This requires dividing each of the $n$ components of $w$ by the scalar $h_{m+1,m}$, at a specified cost of $n$ flops.\nThe total cost for Stage $3$, denoted $C_3$, is the sum of these two costs:\n$$C_3 = 2n + n = 3n$$\n\n**Stage 4: Givens Rotations Update**\nThis final stage updates the QR factorization of the $(m+1) \\times m$ upper Hessenberg matrix $\\bar{H}_m$ and the corresponding right-hand-side vector. The new, $m$-th column of $\\bar{H}_m$ is $[h_{1,m}, \\dots, h_{m+1,m}]^{\\top}$. The process consists of several distinct operations:\n- Application of the $m-1$ previously determined Givens rotations to the new column of $\\bar{H}_m$. The cost for applying one rotation is specified as $6$ flops. Thus, the cost for this part is $6(m-1)$ flops.\n- Formation of a new Givens rotation to annihilate the new subdiagonal element $h_{m+1,m}$. The specified cost is $6$ flops.\n- Application of this new rotation to update the corresponding two elements of the matrix column. The specified cost is $6$ flops.\n- Application of this same new rotation to update the corresponding two elements of the reduced right-hand-side vector. The specified cost is again $6$ flops.\nThe total cost for Stage $4$, denoted $C_4$, is the sum of these four components:\n$$C_4 = 6(m-1) + 6 + 6 + 6 = 6m - 6 + 18 = 6m + 12$$\n\n**Total Flop Count**\nThe total number of flops for the entire $m$-th iteration, $F(n,s,m)$, is the summation of the costs from all four stages:\n$$F(n,s,m) = C_1 + C_2 + C_3 + C_4$$\nSubstituting the derived expressions for each $C_i$:\n$$F(n,s,m) = (2s) + (4nm - m) + (3n) + (6m + 12)$$\nFinally, we combine like terms to obtain the simplified closed-form expression:\n$$F(n,s,m) = 4nm + 3n + (6m - m) + 2s + 12$$\n$$F(n,s,m) = 4nm + 3n + 5m + 2s + 12$$\nThis expression represents the exact total number of floating-point operations as a function of the problem size $n$, matrix sparsity $s$, and iteration number $m$, under the given computational model.",
            "answer": "$$\\boxed{4nm + 3n + 5m + 2s + 12}$$"
        },
        {
            "introduction": "The convergence path of GMRES is not always a simple downward slope; it can exhibit complex behaviors like stagnation. This final practice delves into this phenomenon, requiring you to both derive the mathematical condition for stagnation and construct a concrete numerical example that triggers it. This task of designing a problem to elicit a specific behavior bridges the gap between theoretical insight and practical verification, a cornerstone of computational science. ",
            "id": "2397339",
            "problem": "You are given the task of constructing and verifying a concrete example in which the Generalized Minimal Residual method (GMRES) exhibits stagnation for a known number of iterations when solving a nonsymmetric linear system. Work within the following mathematically precise framework.\n\nWe consider linear systems of the form $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is nonsymmetric and $b \\in \\mathbb{R}^{n}$. Let the initial guess be $x_0 \\in \\mathbb{R}^{n}$ with initial residual $r_0 = b - A x_0$. For $k \\ge 1$, the Krylov subspace of dimension $k$ is\n$$\n\\mathcal{K}_k(A,r_0) = \\operatorname{span}\\{ r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0 \\}.\n$$\nThe Generalized Minimal Residual method (GMRES) defines the iterate $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$ such that the residual $r_k = b - A x_k$ has minimal Euclidean norm among all vectors in the affine space $x_0 + \\mathcal{K}_k(A,r_0)$. We say that GMRES stagnates at iteration $k$ if $\\|r_k\\|_2 = \\|r_{k-1}\\|_2$; equivalently, the minimal residual norm at step $k$ fails to improve over step $k-1$.\n\nYou must start from the following fundamental facts:\n- The GMRES iterate $x_k$ minimizes $\\|b - A x\\|_2$ over $x \\in x_0 + \\mathcal{K}_k(A,r_0)$.\n- The Arnoldi process constructs an orthonormal basis $V_{k+1}$ of $\\mathcal{K}_{k+1}(A,r_0)$ and an upper Hessenberg matrix $\\bar{H}_k \\in \\mathbb{R}^{(k+1)\\times k}$ satisfying $A V_k = V_{k+1} \\bar{H}_k$, where $V_k$ contains the first $k$ columns of $V_{k+1}$.\n- The GMRES residual norms are nonincreasing with $k$.\n\nUsing only these foundational principles, address the following design-and-verify problem.\n\n1) Construct a specific nonsymmetric and nonsingular matrix $A \\in \\mathbb{R}^{4 \\times 4}$ along with a right-hand side $b \\in \\mathbb{R}^{4}$ such that GMRES with $x_0 = 0$ stagnates at the first iteration, i.e., $\\|r_1\\|_2 = \\|r_0\\|_2$. Your construction must be justified by first principles. Your program must implement unrestarted GMRES using the Arnoldi process with Givens rotations to track the residual norm at each iteration. \n\n2) Explain, from the minimization principle, why your $A$ and $b$ yield stagnation at iteration $k = 1$ and why the chosen $A$ is nonsymmetric and nonsingular.\n\n3) Implement a program that computes the residual norm history $\\{\\|r_k\\|_2\\}_{k=0}^{k_{\\max}}$ for $k_{\\max} = 4$ and, for each test case below, outputs the length of the initial stagnation streak, defined as the largest integer $s \\ge 0$ such that $\\|r_j\\|_2 = \\|r_{j-1}\\|_2$ for all $j \\in \\{1,\\dots,s\\}$ within a prescribed tolerance. Use an absolute tolerance\n$$\n\\tau = 10^{-12} \\, \\|r_0\\|_2 + 10^{-15}.\n$$\n\nTest suite:\n- Case A (designed stagnation): \n  $$\n  A = \\begin{bmatrix}\n  0  1  0  0\\\\\n  -1  0  0  0\\\\\n  0  0  0  2\\\\\n  0  0  -2  0\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ 4 \\end{bmatrix}, \\quad\n  x_0 = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}, \\quad\n  k_{\\max} = 4.\n  $$\n- Case B (no stagnation expected in the first step):\n  $$\n  A = \\begin{bmatrix}\n  4  1  0  0\\\\\n  0  4  1  0\\\\\n  0  0  4  1\\\\\n  0  0  0  4\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ 4 \\end{bmatrix}, \\quad\n  x_0 = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}, \\quad\n  k_{\\max} = 4.\n  $$\n- Case C (near-stagnation perturbation): Let $\\varepsilon = 10^{-6}$ and \n  $$\n  A = \\begin{bmatrix}\n  0  1  0  0\\\\\n  -1  0  0  0\\\\\n  0  0  0  2\\\\\n  0  0  -2  0\n  \\end{bmatrix} + \\varepsilon I_4, \\quad\n  b = \\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ 4 \\end{bmatrix}, \\quad\n  x_0 = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}, \\quad\n  k_{\\max} = 4.\n  $$\n\nFor each case, define the integer output as the initial stagnation length $s \\in \\{0,1,2,3,4\\}$ computed by your GMRES implementation with the tolerance $\\tau$ above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\").\n\nNo physical units are involved. All angles, if any appear, must be in radians; none are required here. All numerical parameters have been provided explicitly.",
            "solution": "The problem requires the construction and verification of a scenario where the Generalized Minimal Residual method (GMRES) exhibits stagnation. We will address this by first establishing the fundamental mathematical condition for stagnation, then verifying that the provided test case satisfies this condition, and finally implementing a numerically sound GMRES algorithm to confirm the results programmatically.\n\nA rigorous analysis begins with the definition of the GMRES method. For a linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$, the $k$-th iterate $x_k$ is found in the affine subspace $x_0 + \\mathcal{K}_k(A,r_0)$, where $x_0$ is the initial guess, $r_0 = b - A x_0$ is the initial residual, and $\\mathcal{K}_k(A,r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ is the $k$-th Krylov subspace. The iterate $x_k$ is the unique vector that minimizes the Euclidean norm of the residual, $\\|r_k\\|_2 = \\|b - A x_k\\|_2$.\n\nStagnation at iteration $k=1$ is defined by the condition $\\|r_1\\|_2 = \\|r_0\\|_2$. The GMRES residual norms are non-increasing, so this represents a complete lack of progress in the first step. We derive the condition under which this occurs.\n\nThe first iterate is of the form $x_1 = x_0 + \\alpha r_0$ for some scalar $\\alpha \\in \\mathbb{R}$, since $\\mathcal{K}_1(A,r_0) = \\operatorname{span}\\{r_0\\}$. The corresponding residual is $r_1 = b - A x_1 = b - A(x_0 + \\alpha r_0) = (b - A x_0) - \\alpha A r_0 = r_0 - \\alpha A r_0$.\n\nGMRES selects $\\alpha$ to minimize the function $f(\\alpha) = \\|r_1\\|_2^2 = \\|r_0 - \\alpha A r_0\\|_2^2$. Expanding this expression gives:\n$$f(\\alpha) = (r_0 - \\alpha A r_0)^T (r_0 - \\alpha A r_0) = r_0^T r_0 - 2\\alpha r_0^T A r_0 + \\alpha^2 (A r_0)^T (A r_0)$$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$\\frac{df}{d\\alpha} = -2 r_0^T A r_0 + 2\\alpha (A r_0)^T (A r_0) = 0$$\nAssuming $A r_0 \\neq 0$ (otherwise the problem would be trivial or solved), the optimal $\\alpha$ is:\n$$\\alpha_{\\text{opt}} = \\frac{r_0^T A r_0}{\\|A r_0\\|_2^2}$$\nFor stagnation to occur at $k=1$, the minimal residual norm, $\\|r_1\\|_2$, must be equal to $\\|r_0\\|_2$. This means that the optimal correction step is zero, i.e., $\\alpha_{\\text{opt}} = 0$. From the expression for $\\alpha_{\\text{opt}}$, this occurs if and only if the numerator is zero:\n$$r_0^T A r_0 = 0$$\nThis is the fundamental condition for GMRES stagnation at the first iteration. It signifies that the vector $A r_0$, which defines the search direction for the residual correction, is orthogonal to the initial residual $r_0$. Consequently, no component of $r_0$ can be removed by subtracting a multiple of $A r_0$, and the norm remains unchanged.\n\nNow, we verify the specific construction provided in Case A. We are given:\n$$\nA = \\begin{bmatrix}\n0  1  0  0\\\\\n-1  0  0  0\\\\\n0  0  0  2\\\\\n0  0  -2  0\n\\end{bmatrix}, \\quad\nb = \\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ 4 \\end{bmatrix}, \\quad\nx_0 = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}\n$$\nWith $x_0 = 0$, the initial residual is $r_0 = b - A x_0 = b$. We must check if $b^T A b = 0$.\nFirst, we compute the product $A b$:\n$$A b = \\begin{bmatrix}\n0  1  0  0\\\\\n-1  0  0  0\\\\\n0  0  0  2\\\\\n0  0  -2  0\n\\end{bmatrix}\n\\begin{bmatrix} 1\\\\ 2\\\\ 3\\\\ 4 \\end{bmatrix} =\n\\begin{bmatrix} 2\\\\ -1\\\\ 8\\\\ -6 \\end{bmatrix}$$\nNext, we compute the dot product $b^T (A b)$:\n$$b^T A b = \\begin{bmatrix} 1  2  3  4 \\end{bmatrix}\n\\begin{bmatrix} 2\\\\ -1\\\\ 8\\\\ -6 \\end{bmatrix} = (1)(2) + (2)(-1) + (3)(8) + (4)(-6) = 2 - 2 + 24 - 24 = 0$$\nThe condition for stagnation is satisfied.\n\nThe problem also requires justification that $A$ is nonsymmetric and nonsingular.\nNonsymmetry: The transpose of $A$ is\n$$A^T = \\begin{bmatrix}\n0  -1  0  0\\\\\n1  0  0  0\\\\\n0  0  0  -2\\\\\n0  0  2  0\n\\end{bmatrix}$$\nSince $A^T = -A$ and $A \\neq 0$, $A$ is skew-symmetric, and thus nonsymmetric.\nNonsingularity: The matrix $A$ is block diagonal, $A = \\operatorname{diag}(A_1, A_2)$, where\n$$A_1 = \\begin{bmatrix} 0  1\\\\ -1  0 \\end{bmatrix} \\quad \\text{and} \\quad A_2 = \\begin{bmatrix} 0  2\\\\ -2  0 \\end{bmatrix}$$\nThe determinant is $\\det(A) = \\det(A_1) \\det(A_2)$. We have $\\det(A_1) = (0)(0) - (1)(-1) = 1$ and $\\det(A_2) = (0)(0) - (2)(-2) = 4$.\nTherefore, $\\det(A) = 1 \\cdot 4 = 4$. Since $\\det(A) \\neq 0$, the matrix $A$ is nonsingular, and the linear system $A x = b$ has a unique solution.\n\nThe implementation of GMRES will be based on the Arnoldi process for constructing an orthonormal basis of the Krylov subspace. To track the residual norm at each step $k$ without explicitly forming the iterate $x_k$, we solve the $(k+1) \\times k$ least-squares problem for $y_k$ that minimizes $\\|\\beta e_1 - \\bar{H}_k y_k\\|_2$, where $\\beta = \\|r_0\\|_2$ and $\\bar{H}_k$ is the upper Hessenberg matrix generated by the Arnoldi process. This minimization is performed efficiently by applying a sequence of Givens rotations to transform $\\bar{H}_k$ to an upper triangular form and updating a corresponding right-hand side vector. The residual norm $\\|r_k\\|_2$ is then simply the magnitude of the $(k+1)$-th component of the transformed right-hand side vector. This approach is numerically stable and computationally efficient. The stagnation streak is then determined by comparing consecutive residual norms against the specified tolerance $\\tau = 10^{-12} \\|r_0\\|_2 + 10^{-15}$.",
            "answer": "```python\nimport numpy as np\n\ndef compute_stagnation_streak(A, b, x0, k_max):\n    \"\"\"\n    Computes the initial stagnation streak for GMRES.\n\n    Implements unrestarted GMRES using the Arnoldi process with Modified\n    Gram-Schmidt and Givens rotations to track the residual norm.\n\n    Args:\n        A (np.ndarray): The n x n coefficient matrix.\n        b (np.ndarray): The n-dimensional right-hand side vector.\n        x0 (np.ndarray): The n-dimensional initial guess.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The length of the initial stagnation streak.\n    \"\"\"\n    n = A.shape[0]\n    r0 = b - A @ x0\n    r0_norm = np.linalg.norm(r0)\n\n    # If the initial guess is the solution, no iterations are needed.\n    if r0_norm  1e-15:\n        return 0\n\n    tol = 1e-12 * r0_norm + 1e-15\n\n    V = np.zeros((n, k_max + 1))\n    H = np.zeros((k_max + 1, k_max))\n    \n    res_hist = [r0_norm]\n    \n    # Initialize Arnoldi process\n    V[:, 0] = r0 / r0_norm\n\n    # Initialize Givens rotation data\n    C = np.zeros(k_max)  # Cosines\n    S = np.zeros(k_max)  # Sines\n    g = np.zeros(k_max + 1) # Transformed RHS vector for LS problem\n    g[0] = r0_norm\n\n    for k in range(k_max):\n        # Arnoldi step (Modified Gram-Schmidt)\n        w = A @ V[:, k]\n        for j in range(k + 1):\n            H[j, k] = V[:, j].T @ w\n            w = w - H[j, k] * V[:, j]\n        \n        H[k + 1, k] = np.linalg.norm(w)\n        \n        # Check for lucky breakdown (exact convergence)\n        if H[k + 1, k]  1e-15:\n            # We don't need to normalize V[:, k+1] if we stop.\n            # Residual is zero, so fill remaining history.\n            for _ in range(k, k_max):\n                res_hist.append(0.0)\n            break\n\n        V[:, k + 1] = w / H[k + 1, k]\n\n        # Apply previous k Givens rotations to the new column of H\n        h_col = H[:k+2, k]\n        for j in range(k):\n            c_j, s_j = C[j], S[j]\n            h_j_k = c_j * h_col[j] + s_j * h_col[j+1]\n            h_jp1_k = -s_j * h_col[j] + c_j * h_col[j+1]\n            h_col[j], h_col[j+1] = h_j_k, h_jp1_k\n        \n        # Compute and store the new Givens rotation\n        # It's meant to zero-out the H[k+1, k] element\n        h_kk, h_kp1k = h_col[k], h_col[k+1]\n        rot_r = np.sqrt(h_kk**2 + h_kp1k**2)\n        if rot_r == 0:\n            c_new, s_new = 1.0, 0.0\n        else:\n            c_new = h_kk / rot_r\n            s_new = h_kp1k / rot_r\n        C[k], S[k] = c_new, s_new\n\n        # Apply the new rotation to the LS right-hand side vector g\n        g_k = g[k]\n        g[k] = c_new * g_k # g[k+1] is 0 before this rotation\n        g[k+1] = -s_new * g_k\n\n        # The new residual norm is the magnitude of the last element of g\n        res_k_norm = np.abs(g[k+1])\n        res_hist.append(res_k_norm)\n\n    # Calculate the initial stagnation streak s\n    s = 0\n    for j in range(1, len(res_hist)):\n        if np.abs(res_hist[j] - res_hist[j-1]) = tol:\n            s += 1\n        else:\n            break\n            \n    return s\n\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the GMRES stagnation analysis.\n    \"\"\"\n    x0 = np.array([0., 0., 0., 0.])\n    b = np.array([1., 2., 3., 4.])\n    k_max = 4\n\n    # Case A: Designed stagnation\n    A_A = np.array([\n        [0., 1., 0., 0.],\n        [-1., 0., 0., 0.],\n        [0., 0., 0., 2.],\n        [0., 0.,-2., 0.]\n    ])\n\n    # Case B: No stagnation expected\n    A_B = np.array([\n        [4., 1., 0., 0.],\n        [0., 4., 1., 0.],\n        [0., 0., 4., 1.],\n        [0., 0., 0., 4.]\n    ])\n\n    # Case C: Perturbed stagnation case\n    epsilon = 1e-6\n    A_C = A_A + epsilon * np.eye(4)\n    \n    test_cases = [\n        (A_A, b, x0, k_max),\n        (A_B, b, x0, k_max),\n        (A_C, b, x0, k_max)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_stagnation_streak(*case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}