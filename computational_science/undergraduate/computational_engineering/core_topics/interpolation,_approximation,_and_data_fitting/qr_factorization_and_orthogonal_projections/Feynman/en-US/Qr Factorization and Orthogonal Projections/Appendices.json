{
    "hands_on_practices": [
        {
            "introduction": "The QR factorization is not just an algebraic curiosity; it provides powerful geometric insight into a matrix's column space. This first practice focuses on using the orthogonal matrix $Q$ to compute the orthogonal projection of a vector onto that space, a calculation central to solving least-squares problems. By working through this example , you will solidify your understanding of how an orthonormal basis, provided by the columns of $Q$, dramatically simplifies projection calculations.",
            "id": "1385303",
            "problem": "Let the matrix $A$ and the vector $\\mathbf{b}$ be defined as:\n$$A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$$\nThe columns of $A$ span a subspace of $\\mathbb{R}^3$, which we will denote as $W = \\text{Col}(A)$. The QR factorization of $A$ is $A=QR$, where $Q$ is a matrix with orthonormal columns and $R$ is an upper triangular matrix. The matrix $Q$, whose columns form an orthonormal basis for $W$, is given by:\n$$Q = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\end{pmatrix}$$\nDetermine the vector $\\mathbf{p}$, which is the orthogonal projection of $\\mathbf{b}$ onto the subspace $W$. Express your final answer as a row matrix containing the three components of $\\mathbf{p}$, using exact fractions.",
            "solution": "We are given a matrix $Q$ with orthonormal columns spanning $W=\\text{Col}(A)$. The orthogonal projection $\\mathbf{p}$ of $\\mathbf{b}$ onto $W$ is given by the formula\n$$\n\\mathbf{p} = Q Q^{T} \\mathbf{b} = \\sum_{i=1}^{2} (q_{i}^{T}\\mathbf{b})\\, q_{i},\n$$\nwhere $q_{1}$ and $q_{2}$ are the columns of $Q$.\n\nFrom the given $Q$,\n$$\nq_{1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}, \\quad\nq_{2} = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\nCompute the coefficients:\n$$\nq_{1}^{T}\\mathbf{b} = \\frac{1}{\\sqrt{2}}\\cdot 1 + 0\\cdot 2 + \\frac{1}{\\sqrt{2}}\\cdot 1 = \\frac{2}{\\sqrt{2}} = \\sqrt{2},\n$$\n$$\nq_{2}^{T}\\mathbf{b} = \\frac{1}{\\sqrt{6}}\\cdot 1 + \\frac{2}{\\sqrt{6}}\\cdot 2 - \\frac{1}{\\sqrt{6}}\\cdot 1 = \\frac{1+4-1}{\\sqrt{6}} = \\frac{4}{\\sqrt{6}}.\n$$\nThus,\n$$\n\\mathbf{p} = (q_{1}^{T}\\mathbf{b})\\,q_{1} + (q_{2}^{T}\\mathbf{b})\\,q_{2}\n= \\sqrt{2}\\,q_{1} + \\frac{4}{\\sqrt{6}}\\,q_{2}.\n$$\nCompute each term:\n$$\n\\sqrt{2}\\,q_{1} = \\sqrt{2}\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n= \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\n\\quad\n\\frac{4}{\\sqrt{6}}\\,q_{2} = \\frac{4}{\\sqrt{6}}\\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{4}{6} \\\\ \\frac{8}{6} \\\\ -\\frac{4}{6} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{4}{3} \\\\ -\\frac{2}{3} \\end{pmatrix}.\n$$\nAdding these gives\n$$\n\\mathbf{p} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{4}{3} \\\\ -\\frac{2}{3} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{5}{3} \\\\ \\frac{4}{3} \\\\ \\frac{1}{3} \\end{pmatrix}.\n$$\nAs required, this is expressed with exact fractions. Interpreting the request for a row matrix, we write the components in a single row.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{3} & \\frac{4}{3} & \\frac{1}{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having seen how the $Q$ factor is used, we now turn to its construction. This comprehensive exercise  guides you through implementing a \"thin\" QR factorization from first principles, tackling practical issues like numerical rank deficiency and stability. You will not only build a core numerical tool but also verify its properties by constructing an orthogonal projector and analyzing its efficiency, bridging the gap between abstract theory and robust computational practice.",
            "id": "2430018",
            "problem": "You are asked to implement and validate a computational procedure for the \"thin\" QR factorization within a purely mathematical linear algebra setting. Begin with the following fundamental basis: the definition of an inner product, the Euclidean norm, and the concept of orthonormal sets of vectors. Specifically, use the facts that an inner product induces orthogonal projections and that orthonormal vectors simplify projection operations. Do not assume any pre-derived algorithmic formulas or identities beyond these core definitions. Build the algorithmic steps by repeatedly projecting vectors onto already constructed orthonormal directions and subtracting those projections.\n\nGiven a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, a \"thin\" QR factorization is a decomposition $A \\approx Q R$ where $Q \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns spanning the same subspace as the columns of $A$ up to numerical rank $r \\le n$, and $R \\in \\mathbb{R}^{r \\times n}$ is upper trapezoidal (upper triangular when $r = n$). The \"full\" QR factorization uses a square orthogonal factor $Q_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times m}$ and an $R_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times n}$ with the lower $(m-n)$ rows equal to zero.\n\nYour tasks:\n\n1) Implement a function that, given $A$, produces a \"thin\" QR factorization using only the definitions of inner product and orthogonal projection. Concretely, construct columns $q_j$ by iteratively subtracting the projections of the current column onto previously constructed $q_i$ and normalizing, with an optional second pass of orthogonalization to improve numerical stability. Use a numerical tolerance $\\tau$ to detect the numerical rank $r$ so that columns whose post-projection norm falls below $\\tau$ are treated as numerically dependent and excluded from $Q$. Your function must return $(Q, R, r)$ with $Q \\in \\mathbb{R}^{m \\times r}$ having orthonormal columns, $R \\in \\mathbb{R}^{r \\times n}$ upper trapezoidal, and $r$ the detected rank.\n\n2) Using the constructed $Q$, form the orthogonal projector $P = Q Q^{\\mathsf{T}} \\in \\mathbb{R}^{m \\times m}$ onto the column space of $A$ (as captured by the columns of $Q$). Verify numerically the projector properties: symmetry and idempotence. Your verification metrics must be quantitative and based on norms.\n\n3) Explain, using only dimension counting and projection properties (not pre-quoted algorithmic cost formulas), when and why the \"thin\" QR factorization is more efficient in storage and application than the \"full\" QR factorization. Quantify the storage in terms of the number of stored scalar entries of the factors.\n\nFor evaluation, your program must implement the above and produce, for each test case, a list of four floating-point numbers:\n- The relative reconstruction error $e_{\\mathrm{rec}} = \\lVert A - Q R \\rVert_{\\mathrm{F}} / \\lVert A \\rVert_{\\mathrm{F}}$.\n- The orthonormality error $e_{\\mathrm{orth}} = \\lVert Q^{\\mathsf{T}} Q - I_r \\rVert_{\\mathrm{F}}$ where $I_r$ is the $r \\times r$ identity matrix.\n- The projector idempotence error $e_{\\mathrm{proj}} = \\lVert P^2 - P \\rVert_{\\mathrm{F}}$ with $P = Q Q^{\\mathsf{T}}$.\n- The storage ratio $\\rho = \\dfrac{\\text{stored scalars in thin factors}}{\\text{stored scalars in full factors}}$, where the numerator is the total number of scalar entries in $Q \\in \\mathbb{R}^{m \\times r}$ and $R \\in \\mathbb{R}^{r \\times n}$, and the denominator is the total number of scalar entries in $Q_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times m}$ and $R_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times n}$.\n\nTest suite (three matrices), to be generated deterministically:\n- Case $1$ (tall, well-conditioned): $m = 8$, $n = 3$. Let $A \\in \\mathbb{R}^{8 \\times 3}$ have independent and identically distributed standard normal entries generated by a pseudo-random number generator with seed $7$.\n- Case $2$ (tall, nearly rank-deficient): $m = 10$, $n = 5$. Construct $A \\in \\mathbb{R}^{10 \\times 5}$ by first generating $B \\in \\mathbb{R}^{10 \\times 4}$ with independent and identically distributed standard normal entries using seed $13$, then setting the fifth column as $A_{:,5} = B_{:,1} + \\epsilon \\, \\eta$, where $\\eta \\in \\mathbb{R}^{10}$ has independent and identically distributed standard normal entries from the same generator and $\\epsilon = 10^{-16}$. The first four columns of $A$ are set equal to the columns of $B$ in order.\n- Case $3$ (square Vandermonde): $m = n = 5$. Let $x_k = \\dfrac{k}{m-1}$ for $k = 0, 1, 2, 3, 4$. Define $A \\in \\mathbb{R}^{5 \\times 5}$ by $A_{i,j} = x_i^{j}$ for $i = 0, 1, 2, 3, 4$ and $j = 0, 1, 2, 3, 4$.\n\nUse a numerical tolerance $\\tau = \\varepsilon \\cdot \\max(m,n) \\cdot \\lVert A \\rVert_{\\mathrm{F}}$ where $\\varepsilon$ is machine precision for double-precision floating-point arithmetic. All norms are Frobenius norms. Angles do not appear; no physical units are involved.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of lists, each inner list ordered as $[e_{\\mathrm{rec}}, e_{\\mathrm{orth}}, e_{\\mathrm{proj}}, \\rho]$, enclosed in square brackets (for example, $[[a,b,c,d],[a',b',c',d'],[a'',b'',c'',d'']]$), where all $a,b,c,d$ symbols denote floating-point numbers computed by your program.",
            "solution": "The problem requires the implementation and validation of a \"thin\" QR factorization algorithm derived from fundamental principles of linear algebra, namely the inner product and orthogonal projection. The algorithm must be robust to numerical rank deficiency. Subsequently, an orthogonal projector is to be constructed and its properties verified. Finally, an explanation of the computational and storage efficiency of the thin factorization relative to the full factorization is required. The problem is well-posed, scientifically sound, and contains all necessary information for a deterministic solution.\n\n### Part 1: Derivation of the QR Factorization Algorithm\n\nThe objective is to decompose a matrix $A \\in \\mathbb{R}^{m \\times n}$ with columns $\\{a_1, a_2, \\dots, a_n\\}$ into $A \\approx QR$, where $Q \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns $\\{q_1, q_2, \\dots, q_r\\}$ and $R \\in \\mathbb{R}^{r \\times n}$ is an upper trapezoidal matrix. The columns of $Q$ must form an orthonormal basis for the column space of $A$, denoted $\\mathrm{span}(A)$. The integer $r \\le n$ represents the numerical rank of $A$.\n\nThe foundation of this procedure is the Gram-Schmidt orthogonalization process, which constructs an orthonormal set from a set of linearly independent vectors. We begin with the definition of the standard Euclidean inner product for two vectors $u, v \\in \\mathbb{R}^{m}$, given by $\\langle u, v \\rangle = u^{\\mathsf{T}}v$. This inner product induces the Euclidean norm $\\lVert u \\rVert_2 = \\sqrt{\\langle u, u \\rangle}$.\n\nThe orthogonal projection of a vector $a_j$ onto the direction of a unit vector $q_i$ is given by $\\mathrm{proj}_{q_i} a_j = \\langle a_j, q_i \\rangle q_i$. To make a vector $a_j$ orthogonal to $q_i$, we subtract this projection: $a_j - \\mathrm{proj}_{q_i} a_j$. To make $a_j$ orthogonal to an entire orthonormal set $\\{q_1, \\dots, q_{k}\\}$, we subtract the projection onto each basis vector:\n$$ v_j = a_j - \\sum_{i=1}^{k} \\mathrm{proj}_{q_i} a_j = a_j - \\sum_{i=1}^{k} \\langle a_j, q_i \\rangle q_i $$\nThe resulting vector $v_j$ is orthogonal to every vector in $\\{q_1, \\dots, q_{k}\\}$.\n\nThe algorithm proceeds column by column for $j=1, \\dots, n$:\n1.  For the current column $a_j$, we compute a vector $v_j$ that is orthogonal to all previously constructed orthonormal vectors $\\{q_1, \\dots, q_{r}\\}$, where $r$ is the current number of basis vectors found.\n2.  If $v_j$ is not a zero vector (i.e., its norm is above a numerical tolerance), it represents a new direction independent of the previous ones. We normalize it to obtain the next orthonormal vector, $q_{r+1} = v_j / \\lVert v_j \\rVert_2$.\n3.  The coefficients of the projections and the normalization factor form the $j$-th column of the $R$ matrix.\n\nThis process builds the decomposition $A=QR$. Rearranging the projection formula for a full-rank matrix ($r=n$), we have for each column $a_j$:\n$$ a_j = \\sum_{i=1}^{j-1} \\langle a_j, q_i \\rangle q_i + \\lVert v_j \\rVert_2 q_j = \\sum_{i=1}^{j} r_{ij} q_i $$\nwhere $r_{ij} = \\langle a_j, q_i \\rangle$ for $i<j$ and $r_{jj} = \\lVert v_j \\rVert_2$. In matrix form, this is $A = QR$.\n\nFor numerical stability, the classical Gram-Schmidt process described above is susceptible to loss of orthogonality due to floating-point cancellation errors. The problem statement suggests a \"second pass of orthogonalization,\" which we implement as an iterated Gram-Schmidt procedure. After computing $v_j$ by subtracting projections, we repeat the process on $v_j$ itself to remove any remaining components in the directions of $\\{q_1, \\dots, q_r\\}$ that persist due to finite precision arithmetic. This is equivalent to performing the orthogonalization step twice.\n\nTo handle potential rank deficiency, we introduce a tolerance $\\tau$. The problem specifies $\\tau = \\varepsilon \\cdot \\max(m,n) \\cdot \\lVert A \\rVert_{\\mathrm{F}}$, where $\\varepsilon$ is the machine precision. After orthogonalizing a column $a_j$ against the existing basis, if the norm of the resulting vector $v_j$ is less than or equal to $\\tau$, i.e., $\\lVert v_j \\rVert_2 \\le \\tau$, we consider $a_j$ to be numerically linearly dependent on the preceding columns. In this case, we do not generate a new column for $Q$, and the rank $r$ is not incremented. The corresponding column of $R$ will contain the coefficients representing $a_j$ as a linear combination of the columns of $Q$, with zero on the diagonal and below.\n\nThe resulting algorithm (Iterated Modified Gram-Schmidt with rank detection) is as follows:\n1.  Initialize an empty list for the columns of $Q$, an $n \\times n$ zero matrix for $R$, and set the rank $r=0$.\n2.  For each column $a_j$ of $A$ (from $j=0$ to $n-1$):\n    a. Set $v = a_j$.\n    b. For two passes (re-orthogonalization):\n        i. For each existing orthonormal vector $q_i$ (from $i=0$ to $r-1$):\n           - Compute a projection coefficient $c = \\langle v, q_i \\rangle$.\n           - Subtract the projection: $v \\leftarrow v - c \\cdot q_i$.\n           - Add the coefficient to the corresponding entry in $R$: $R_{i,j} \\leftarrow R_{i,j} + c$.\n    c. Compute the norm of the orthogonalized vector, $\\lVert v \\rVert_2$.\n    d. If $\\lVert v \\rVert_2 > \\tau$:\n        i. Increment the rank: $r \\leftarrow r+1$.\n        ii. Set the diagonal element of $R$: $R_{r-1,j} = \\lVert v \\rVert_2$.\n        iii. Normalize to find the new basis vector: $q_r = v / \\lVert v \\rVert_2$. Add $q_r$ to the set of $Q$ columns.\n3.  After iterating through all columns of $A$, assemble the final matrices: $Q$ is formed by stacking the $r$ collected basis vectors, resulting in an $m \\times r$ matrix. $R$ is formed by taking the first $r$ rows of the temporary $n \\times n$ matrix, resulting in an $r \\times n$ upper trapezoidal matrix.\n\n### Part 2: Orthogonal Projector and Verification\n\nGiven the thin QR factorization $A \\approx QR$, where $Q \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns, the matrix $P = QQ^{\\mathsf{T}}$ is the orthogonal projector onto the column space of $Q$, $\\mathrm{span}(Q)$, which is computationally equivalent to $\\mathrm{span}(A)$. The matrix $P$ is in $\\mathbb{R}^{m \\times m}$.\n\nAn orthogonal projector must satisfy two key properties:\n1.  **Symmetry**: $P = P^{\\mathsf{T}}$. This is guaranteed by construction: $P^{\\mathsf{T}} = (QQ^{\\mathsf{T}})^{\\mathsf{T}} = (Q^{\\mathsf{T}})^{\\mathsf{T}}Q^{\\mathsf{T}} = QQ^{\\mathsf{T}} = P$.\n2.  **Idempotence**: $P^2 = P$. This property follows from the orthonormality of $Q$'s columns. Since $Q^{\\mathsf{T}}Q = I_r$ (the $r \\times r$ identity matrix), we have $P^2 = (QQ^{\\mathsf{T}})(QQ^{\\mathsf{T}}) = Q(Q^{\\mathsf{T}}Q)Q^{\\mathsf{T}} = Q(I_r)Q^{\\mathsf{T}} = QQ^{\\mathsf{T}} = P$.\n\nThe numerical verification of these properties involves computing the norm of the deviation from the ideal. The problem requires computing two such metrics:\n-   **Orthonormality error**: $e_{\\mathrm{orth}} = \\lVert Q^{\\mathsf{T}}Q - I_r \\rVert_{\\mathrm{F}}$. This measures how close the columns of the computed $Q$ are to being perfectly orthonormal. Any deviation from zero indicates a loss of orthogonality, which will affect the idempotence of $P$.\n-   **Projector idempotence error**: $e_{\\mathrm{proj}} = \\lVert P^2 - P \\rVert_{\\mathrm{F}}$. This directly measures the idempotence property. From the derivation above, this error is directly linked to the orthonormality error.\n\n### Part 3: Efficiency of Thin vs. Full QR Factorization\n\nThe efficiency of the \"thin\" QR factorization compared to the \"full\" version can be analyzed by considering storage requirements and the cost of application, based on the dimensions of the factors.\n\n-   **Full QR Factorization**: For a matrix $A \\in \\mathbb{R}^{m \\times n}$, the full factorization is $A = Q_{\\mathrm{full}} R_{\\mathrm{full}}$, where $Q_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix and $R_{\\mathrm{full}} \\in \\mathbb{R}^{m \\times n}$ is upper trapezoidal (its last $m-n$ rows are zero).\n-   **Thin QR Factorization**: The thin factorization is $A \\approx QR$, where $Q \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns and $R \\in \\mathbb{R}^{r \\times n}$ is upper trapezoidal. Here, $r \\le n \\le m$ is the numerical rank.\n\n**Storage Efficiency:**\n\nWe quantify storage by the total number of scalar entries to be stored for the factors.\n-   Storage for full QR: The matrix $Q_{\\mathrm{full}}$ requires storing $m \\times m = m^2$ scalars. The matrix $R_{\\mathrm{full}}$ requires storing $m \\times n$ scalars.\n    Total storage $S_{\\mathrm{full}} = m^2 + mn = m(m+n)$.\n-   Storage for thin QR: The matrix $Q$ requires $m \\times r$ scalars. The matrix $R$ requires $r \\times n$ scalars.\n    Total storage $S_{\\text{thin}} = mr + rn = r(m+n)$.\n\nThe storage ratio, as requested, is:\n$$ \\rho = \\frac{S_{\\text{thin}}}{S_{\\text{full}}} = \\frac{r(m+n)}{m(m+n)} = \\frac{r}{m} $$\nSince $r \\le n \\le m$, the ratio $\\rho$ is always less than or equal to $1$. Significant storage savings are achieved when $r$ is much smaller than $m$ ($r \\ll m$). This is particularly true for \"tall and skinny\" matrices where $n \\ll m$ (implying $r \\ll m$), or for any rank-deficient matrix where $r < n$. The full factorization computes and stores an entire orthonormal basis for $\\mathbb{R}^m$, whereas the thin factorization only computes and stores the basis for the subspace relevant to $A$, i.e., its column space.\n\n**Application Efficiency (Computational Cost):**\n\nThe efficiency gain also extends to applying the factors, which is common in solving linear systems or least-squares problems. For example, solving the least-squares problem $\\min_x \\lVert Ax-b \\rVert_2$ involves computing $Q^{\\mathsf{T}}b$.\n-   Using full factors: The product $Q_{\\mathrm{full}}^{\\mathsf{T}}b$ where $b \\in \\mathbb{R}^m$ requires an operation count proportional to $m^2$.\n-   Using thin factors: The product $Q^{\\mathsf{T}}b$ requires an operation count proportional to $mr$.\nAgain, if $r \\ll m$, the computational cost of applying the thin $Q$ factor is substantially lower than applying the full $Q_{\\mathrm{full}}$ factor. The thin factorization avoids computations involving the orthogonal complement of the column space of $A$, which are often unnecessary.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a thin QR factorization algorithm from first principles.\n    \"\"\"\n\n    def thin_qr_from_scratch(A: np.ndarray):\n        \"\"\"\n        Computes the thin QR factorization of matrix A using an iterated\n        Gram-Schmidt process with rank detection.\n\n        Args:\n            A: A real matrix of size (m, n).\n\n        Returns:\n            A tuple (Q, R, r) where:\n            - Q is an (m, r) matrix with orthonormal columns.\n            - R is an (r, n) upper trapezoidal matrix.\n            - r is the detected numerical rank.\n        \"\"\"\n        m, n = A.shape\n        \n        # Define the numerical tolerance for rank detection\n        frob_norm_A = np.linalg.norm(A, 'fro')\n        if frob_norm_A == 0:\n            return np.zeros((m, 0)), np.zeros((0, n)), 0\n            \n        eps = np.finfo(A.dtype).eps\n        tolerance = eps * max(m, n) * frob_norm_A\n\n        # Pre-allocate matrices; they will be trimmed at the end\n        Q = np.zeros((m, n), dtype=A.dtype)\n        R = np.zeros((n, n), dtype=A.dtype)\n        rank = 0\n\n        for j in range(n):\n            v = A[:, j].copy()\n            \n            # Use two passes of orthogonalization for numerical stability\n            # This is an implementation of Iterated Gram-Schmidt\n            for _ in range(2):\n                for i in range(rank):\n                    q_i = Q[:, i]\n                    # Inner product: <v, q_i>\n                    coeff = q_i.T @ v\n                    v -= coeff * q_i\n                    R[i, j] += coeff\n\n            norm_v = np.linalg.norm(v)\n\n            if norm_v > tolerance:\n                # We found a new linearly independent direction\n                R[rank, j] = norm_v\n                Q[:, rank] = v / norm_v\n                rank += 1\n        \n        # Trim matrices to their final dimensions\n        Q_thin = Q[:, :rank]\n        R_thin = R[:rank, :]\n        \n        return Q_thin, R_thin, rank\n\n    def generate_test_cases():\n        \"\"\"\n        Generates the three deterministic test cases specified in the problem.\n        \"\"\"\n        cases = []\n\n        # Case 1: Tall, well-conditioned\n        m1, n1 = 8, 3\n        rng1 = np.random.default_rng(seed=7)\n        A1 = rng1.standard_normal((m1, n1))\n        cases.append({'A': A1, 'm': m1, 'n': n1})\n\n        # Case 2: Tall, nearly rank-deficient\n        m2, n2 = 10, 5\n        rng2 = np.random.default_rng(seed=13)\n        B2 = rng2.standard_normal((m2, n2 - 1))\n        eta = rng2.standard_normal(m2)\n        epsilon = 1e-16\n        A2 = np.zeros((m2, n2))\n        A2[:, :n2 - 1] = B2\n        A2[:, n2 - 1] = B2[:, 0] + epsilon * eta\n        cases.append({'A': A2, 'm': m2, 'n': n2})\n        \n        # Case 3: Square Vandermonde\n        m3, n3 = 5, 5\n        x3 = np.linspace(0, 1, m3)\n        A3 = np.vander(x3, increasing=True).T # Our definition is A_ij = x_i^j, np.vander is A_ij = x_i^(N-1-j)\n        # So we have to re-build it or flip columns\n        A3 = np.fliplr(np.vander(x3))\n        cases.append({'A': A3, 'm': m3, 'n': n3})\n        \n        return cases\n\n    test_cases = generate_test_cases()\n    results = []\n\n    for case in test_cases:\n        A, m, n = case['A'], case['m'], case['n']\n\n        Q, R, r = thin_qr_from_scratch(A)\n        \n        norm_A = np.linalg.norm(A, 'fro')\n        \n        # 1. Relative reconstruction error\n        if norm_A > 0:\n            e_rec = np.linalg.norm(A - Q @ R, 'fro') / norm_A\n        else:\n            e_rec = 0.0\n\n        # 2. Orthonormality error\n        if r > 0:\n            e_orth = np.linalg.norm(Q.T @ Q - np.eye(r), 'fro')\n        else:\n            e_orth = 0.0\n\n        # 3. Projector idempotence error\n        if r > 0:\n            P = Q @ Q.T\n            e_proj = np.linalg.norm(P @ P - P, 'fro')\n        else:\n            e_proj = 0.0\n            \n        # 4. Storage ratio\n        storage_thin = m * r + r * n\n        storage_full = m * m + m * n\n        if storage_full > 0:\n            rho = storage_thin / storage_full\n        else:\n            rho = 0.0\n        \n        results.append([e_rec, e_orth, e_proj, rho])\n    \n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{e[0]},{e[1]},{e[2]},{e[3]}]\" for e in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In many engineering and data science applications, data arrives sequentially. Recomputing a full QR factorization with each new piece of data would be prohibitively expensive. This advanced practice  introduces an elegant and efficient method for updating a QR factorization when a new row is added to the original matrix. By using a sequence of Givens rotations to restore the upper triangular structure of the $R$ factor, you will learn a powerful technique that is essential for recursive algorithms and real-time systems.",
            "id": "2429968",
            "problem": "You are given a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with full column rank $n$, together with its thin QR factorization $A = Q R$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns that satisfy $Q^{\\top} Q = I_n$, and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular with strictly positive diagonal entries. Consider appending a single row $a^{\\top} \\in \\mathbb{R}^{1 \\times n}$ to obtain the augmented matrix $A_{+} \\in \\mathbb{R}^{(m+1) \\times n}$ defined by\n$$\nA_{+} = \\begin{bmatrix} A \\\\ a^{\\top} \\end{bmatrix}.\n$$\nYour task is to, for each specified test case, compute the unique upper triangular matrix $R_{+} \\in \\mathbb{R}^{n \\times n}$ with strictly positive diagonal entries such that\n$$\nR_{+}^{\\top} R_{+} = A_{+}^{\\top} A_{+}.\n$$\nBy definition of the thin QR factorization, this $R_{+}$ is the upper triangular factor associated with the thin QR factorization of $A_{+}$ under the convention of strictly positive diagonal entries. The computation must be based only on the initial factor $R$ and the appended row $a^{\\top}$ through fundamental identities and properties of orthogonal transformations.\n\nFor each test case, define the scalar quantity\n$$\n\\Delta = \\max_{1 \\le i,j \\le n} \\left| \\left(R_{+}\\right)_{ij} - \\left(\\widehat{R}_{+}\\right)_{ij} \\right|,\n$$\nwhere $\\widehat{R}_{+}$ denotes the upper triangular factor (with strictly positive diagonal entries) obtained by directly computing a thin QR factorization of $A_{+}$, and where $\\left(\\cdot\\right)_{ij}$ denotes the entry in row $i$ and column $j$. You must output $\\Delta$ for each test case.\n\nTest Suite:\n- Case $1$:\n  - $A = \\begin{bmatrix}\n  2 & -1 & 0 \\\\\n  1 & 2 & 1 \\\\\n  0 & 1 & 2 \\\\\n  1 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 3}$,\n  - $a^{\\top} = \\begin{bmatrix} 1 & -2 & 3 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 3}$.\n- Case $2$:\n  - $A = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}$,\n  - $a^{\\top} = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 3}$.\n- Case $3$:\n  - $A = \\begin{bmatrix}\n  1.0 & 2.0 \\\\\n  2.0 & 4.0001 \\\\\n  3.0 & 6.0002\n  \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}$,\n  - $a^{\\top} = \\begin{bmatrix} 4.0 & 8.0003 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 2}$.\n- Case $4$:\n  - $A = \\begin{bmatrix}\n  3 & 1 & 0 \\\\\n  1 & 3 & 1 \\\\\n  0 & 1 & 3\n  \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}$,\n  - $a^{\\top} = \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 3}$.\n- Case $5$:\n  - $A = \\begin{bmatrix}\n  1 \\\\\n  2 \\\\\n  3 \\\\\n  4 \\\\\n  5\n  \\end{bmatrix} \\in \\mathbb{R}^{5 \\times 1}$,\n  - $a^{\\top} = \\begin{bmatrix} 6 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 1}$.\n\nAnswer specification and output format:\n- For each case, compute $\\Delta$ as defined above.\n- Your program should produce a single line of output containing the $\\Delta$ values for Cases $1$ through $5$ in order, as a comma-separated list enclosed in square brackets, for example $[x_1,x_2,x_3,x_4,x_5]$, where each $x_k$ is a real number.\n- Each $x_k$ must be printed in scientific notation with exactly $10$ digits after the decimal point.",
            "solution": "The problem is scientifically sound, well-posed, and objective. It presents a standard task in computational engineering: updating the QR factorization of a matrix to which a row has been appended. All provided data and conditions are consistent and sufficient for arriving at a unique, meaningful solution. Thus, we proceed to the derivation and implementation of the solution.\n\nThe problem requires the computation of the upper triangular factor $R_{+} \\in \\mathbb{R}^{n \\times n}$ from the thin QR factorization of an augmented matrix $A_{+} \\in \\mathbb{R}^{(m+1) \\times n}$. The matrix $A_{+}$ is formed by appending a row $a^{\\top} \\in \\mathbb{R}^{1 \\times n}$ to a matrix $A \\in \\mathbb{R}^{m \\times n}$ which has a known thin QR factorization $A = QR$. The factor $R_{+} \\in \\mathbb{R}^{n \\times n}$ is uniquely defined by the relation $R_{+}^{\\top} R_{+} = A_{+}^{\\top} A_{+}$ in conjunction with the constraint that its diagonal entries must be strictly positive. The computation must leverage the existing factor $R$ and the new row $a^{\\top}$.\n\nFirst, we establish the fundamental algebraic relationship. The normal equations matrix for $A_{+}$ is given by:\n$$\nA_{+}^{\\top} A_{+} = \\begin{bmatrix} A^{\\top} & a \\end{bmatrix} \\begin{bmatrix} A \\\\ a^{\\top} \\end{bmatrix} = A^{\\top} A + a a^{\\top}\n$$\nFrom the initial thin QR factorization $A=QR$, where $Q^{\\top}Q=I_{n}$, we have:\n$$\nA^{\\top} A = (QR)^{\\top}(QR) = R^{\\top}Q^{\\top}QR = R^{\\top}I_{n}R = R^{\\top}R\n$$\nSubstituting this into the expression for $A_{+}^{\\top} A_{+}$, we obtain the target relation for $R_{+}$:\n$$\nR_{+}^{\\top} R_{+} = R^{\\top}R + aa^{\\top}\n$$\nThe term $aa^{\\top}$ represents a rank-one update to the matrix $R^{\\top}R$. We seek the Cholesky factor of the updated matrix. A more direct path to $R_{+}$ can be found by observing that the right-hand side can be expressed as a product:\n$$\nR^{\\top}R + aa^{\\top} = \\begin{bmatrix} R^{\\top} & a \\end{bmatrix} \\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix}\n$$\nThis reveals that $R_{+}^{\\top} R_{+}$ is the Gram matrix of the auxiliary matrix $\\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix}$. Therefore, $R_{+}$ must be the R-factor of the thin QR factorization of this auxiliary matrix. Let us define this matrix as $M \\in \\mathbb{R}^{(n+1) \\times n}$:\n$$\nM = \\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix}\n$$\nIf we find an orthogonal matrix $G \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ such that $GM$ has its last row entirely zero, i.e.,\n$$\nGM = \\begin{bmatrix} R_{+} \\\\ 0_{1 \\times n} \\end{bmatrix}\n$$\nwhere $R_{+}$ is upper triangular, then from the orthogonality of $G$ ($G^{\\top}G=I$), we have:\n$$\nM^{\\top}M = (GM)^{\\top}(GM) = \\begin{bmatrix} R_{+}^{\\top} & 0^{\\top} \\end{bmatrix} \\begin{bmatrix} R_{+} \\\\ 0 \\end{bmatrix} = R_{+}^{\\top}R_{+}\n$$\nThis confirms that the R-factor of $M$ is indeed the desired $R_{+}$.\n\nSince $R$ is already upper triangular, the matrix $M$ is nearly in the required form. Only its last row, $a^{\\top}$, violates the upper triangular structure. We can restore this structure by systematically eliminating the non-zero entries of the last row using a sequence of Givens rotations.\n\nThe procedure is as follows. Let the working matrix be denoted by $\\tilde{M}$, initialized as $\\tilde{M} = M$. We apply a sequence of $n$ Givens rotations, $G_1, G_2, \\ldots, G_n$. For $j=1, 2, \\ldots, n$, the rotation $G_j$ is designed to annihilate the element at position $(n+1, j)$ of the current matrix, by rotating row $j$ with row $n+1$.\n\nSpecifically, for each $j \\in \\{1, \\dots, n\\}$, let the current matrix elements be $\\tilde{m}_{jk}$ and $\\tilde{m}_{n+1,k}$. We construct a Givens rotation in the $(j, n+1)$ plane to zero out the element $\\tilde{m}_{n+1, j}$. Let $x = \\tilde{m}_{jj}$ and $y = \\tilde{m}_{n+1, j}$. The Givens rotation parameters $c$ and $s$ are computed such that:\n$$\n\\begin{bmatrix} c & s \\\\ -s & c \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} \\sqrt{x^2+y^2} \\\\ 0 \\end{bmatrix}\n$$\nThis rotation is then applied to all columns $k = j, \\ldots, n$ of rows $j$ and $n+1$. Applying this sequence for $j=1, \\ldots, n$ successively transforms the matrix $M$:\n$$\nG_n \\cdots G_2 G_1 \\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix} = \\begin{bmatrix} R_{+} \\\\ 0_{1 \\times n} \\end{bmatrix}\n$$\nThe resulting $n \\times n$ upper block is the required matrix $R_{+}$. Since the original diagonal entries $R_{ii}$ are strictly positive, and each new diagonal entry $(R_{+})_{jj}$ is computed as $\\sqrt{(\\tilde{m}_{jj})^2 + (\\tilde{m}_{n+1, j})^2}$, the diagonal entries of $R_{+}$ are also guaranteed to be strictly positive.\n\nThis algorithm computes $R_{+}$ using only $R$ and $a^{\\top}$, satisfying the problem constraints. For verification, the resulting $R_{+}$ is compared against $\\widehat{R}_{+}$, which is obtained from a direct thin QR factorization of $A_{+}$, ensuring the diagonal entries are positive. The maximum absolute difference $\\Delta$ between the entries of these two matrices quantifies the numerical consistency of the update procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg.blas import drotg\n\ndef get_positive_diagonal_R(A):\n    \"\"\"\n    Computes the thin QR factorization of A and ensures the R factor has\n    strictly positive diagonal entries.\n    \"\"\"\n    # The problem statement guarantees A has full column rank.\n    if A.shape[1] == 0:\n        return np.empty((0, 0))\n        \n    _, R = np.linalg.qr(A, mode='reduced')\n    \n    n = R.shape[1]\n    for i in range(n):\n        if R[i, i] < 0:\n            R[i, :] *= -1\n            \n    return R\n\ndef solve():\n    \"\"\"\n    Solves the problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [2.0, -1.0, 0.0],\n                [1.0, 2.0, 1.0],\n                [0.0, 1.0, 2.0],\n                [1.0, 0.0, 1.0]\n            ]),\n            np.array([1.0, -2.0, 3.0])\n        ),\n        (\n            np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.0, 0.0, 1.0]\n            ]),\n            np.array([0.0, 0.0, 0.0])\n        ),\n        (\n            np.array([\n                [1.0, 2.0],\n                [2.0, 4.0001],\n                [3.0, 6.0002]\n            ]),\n            np.array([4.0, 8.0003])\n        ),\n        (\n            np.array([\n                [3.0, 1.0, 0.0],\n                [1.0, 3.0, 1.0],\n                [0.0, 1.0, 3.0]\n            ]),\n            np.array([1.0, 0.0, 1.0])\n        ),\n        (\n            np.array([\n                [1.0],\n                [2.0],\n                [3.0],\n                [4.0],\n                [5.0]\n            ]),\n            np.array([6.0])\n        ),\n    ]\n\n    delta_results = []\n\n    for A, a_row in test_cases:\n        m, n = A.shape\n        \n        # Step 1: Obtain the initial R factor with positive diagonals.\n        R = get_positive_diagonal_R(A)\n        \n        # Step 2: Form the augmented matrix M = [R^T, a]^T\n        M_aug = np.vstack([R, a_row.reshape(1, n)])\n        \n        # Step 3: Use Givens rotations to zero out the last row of M_aug.\n        # The working matrix is a copy of M_aug.\n        R_plus_computed_aug = M_aug.copy()\n        \n        for j in range(n):\n            # Elements to be rotated are in column j of rows j and n.\n            x = R_plus_computed_aug[j, j]\n            y = R_plus_computed_aug[n, j]\n            \n            # drotg computes c, s for a Givens rotation.\n            # It's a low-level BLAS function. For floats it's drotg.\n            c, s = drotg(x, y)\n\n            # Define the 2x2 Givens rotation matrix\n            G = np.array([[c, s], [-s, c]])\n            \n            # Apply the rotation to the relevant part of rows j and n.\n            # This affects columns from j to n-1.\n            rows_to_update = R_plus_computed_aug[[j, n], j:]\n            R_plus_computed_aug[[j, n], j:] = G @ rows_to_update\n\n        # The updated R factor is the top n x n block\n        R_plus_computed = R_plus_computed_aug[:n, :]\n        \n        # Step 4: Compute the reference R_plus by direct QR factorization of A_plus.\n        A_plus = np.vstack([A, a_row.reshape(1, n)])\n        R_plus_ref = get_positive_diagonal_R(A_plus)\n        \n        # Step 5: Compute the delta value.\n        delta = np.max(np.abs(R_plus_computed - R_plus_ref))\n        delta_results.append(delta)\n\n    # Final print statement in the exact required format.\n    formatted_results = [\"{:.10e}\".format(res) for res in delta_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}