{
    "hands_on_practices": [
        {
            "introduction": "Theory comes to life when applied to real-world scenarios. This first practice challenges you to model a two-good economic market where supply and demand are described by nonlinear functions, a common situation in modern economics. By implementing Newton's method with a backtracking line search, you will find the market equilibrium prices, gaining hands-on experience in applying numerical root-finding to a tangible problem from another scientific discipline.",
            "id": "2441954",
            "problem": "Consider a two-good competitive market with nonlinear supply and demand. Let the price vector be $\\mathbf{p} = (p_1,p_2)$ with $p_1 > 0$ and $p_2 > 0$. The demand functions for goods $1$ and $2$ are given by\n$$\nQ_1^d(\\mathbf{p}) = a_1 - b_1\\, p_1^{\\alpha_1} + c_{12}\\, p_2^{\\gamma_{12}}, \\quad\nQ_2^d(\\mathbf{p}) = a_2 - b_2\\, p_2^{\\alpha_2} + c_{21}\\, p_1^{\\gamma_{21}}.\n$$\nThe supply functions for goods $1$ and $2$ are given by\n$$\nQ_1^s(p_1) = d_1 + e_1\\, p_1^{\\beta_1}, \\quad\nQ_2^s(p_2) = d_2 + e_2\\, p_2^{\\beta_2}.\n$$\nAn equilibrium is defined by the system of nonlinear equations enforcing market clearing in each good:\n$$\nF_1(\\mathbf{p}) = Q_1^d(\\mathbf{p}) - Q_1^s(p_1) = 0, \\quad\nF_2(\\mathbf{p}) = Q_2^d(\\mathbf{p}) - Q_2^s(p_2) = 0.\n$$\nYou must compute equilibrium prices and quantities using only first principles and general numerical analysis facts. Specifically:\n- Start from the definition of equilibrium $F(\\mathbf{p}) = \\mathbf{0}$, where $F(\\mathbf{p}) = (F_1(\\mathbf{p}),F_2(\\mathbf{p}))^\\top$.\n- Construct and implement an iterative root-finding method that uses a first-order local linearization of $F$ at the current iterate to generate a search direction, and apply a step length rule that guarantees all iterates remain strictly positive and that the residual norm decreases.\n- Stop the iteration when the Euclidean norm of the residual $\\lVert F(\\mathbf{p}) \\rVert_2$ is below a tolerance or when a maximum number of iterations is reached. If the maximum number of iterations is reached without meeting the tolerance, return the last iterate that satisfies positivity.\n\nUnits: Report prices in Currency Units (CU) and quantities in Quantity Units (QU). When printing results, round all reported numbers to $6$ decimal places.\n\nUse the following test suite of parameter sets. For each case, use the initial guess $\\mathbf{p}^{(0)} = (10,10)$ and the stopping tolerance $\\varepsilon = 10^{-10}$ with a maximum of $50$ outer iterations. In all cases, require iterates to satisfy $p_i \\ge 10^{-12}$, and use a backtracking factor of $1/2$ for step length reduction when needed.\n\n- Case $1$ (baseline, moderate cross-price effects):\n  - $a_1 = 120.0$, $b_1 = 1.2$, $\\alpha_1 = 1.5$, $c_{12} = 0.4$, $\\gamma_{12} = 1.2$, $d_1 = 10.0$, $e_1 = 0.9$, $\\beta_1 = 1.3$.\n  - $a_2 = 100.0$, $b_2 = 1.0$, $\\alpha_2 = 1.4$, $c_{21} = 0.3$, $\\gamma_{21} = 1.1$, $d_2 = 12.0$, $e_2 = 1.1$, $\\beta_2 = 1.2$.\n\n- Case $2$ (small-market scale, low prices):\n  - $a_1 = 35.0$, $b_1 = 0.9$, $\\alpha_1 = 1.2$, $c_{12} = 0.15$, $\\gamma_{12} = 1.3$, $d_1 = 5.0$, $e_1 = 0.8$, $\\beta_1 = 1.1$.\n  - $a_2 = 30.0$, $b_2 = 0.7$, $\\alpha_2 = 1.25$, $c_{21} = 0.2$, $\\gamma_{21} = 1.15$, $d_2 = 4.0$, $e_2 = 0.6$, $\\beta_2 = 1.05$.\n\n- Case $3$ (strong cross-price substitution, still well-conditioned):\n  - $a_1 = 80.0$, $b_1 = 1.1$, $\\alpha_1 = 1.6$, $c_{12} = 0.9$, $\\gamma_{12} = 1.05$, $d_1 = 8.0$, $e_1 = 1.0$, $\\beta_1 = 1.25$.\n  - $a_2 = 90.0$, $b_2 = 1.0$, $\\alpha_2 = 1.55$, $c_{21} = 0.8$, $\\gamma_{21} = 1.1$, $d_2 = 7.0$, $e_2 = 0.95$, $\\beta_2 = 1.2$.\n\nFor each parameter set, compute the equilibrium prices $\\hat{p}_1$ and $\\hat{p}_2$ in CU and the corresponding equilibrium quantities $\\hat{q}_1$ and $\\hat{q}_2$ in QU, where $\\hat{q}_1 = Q_1^d(\\hat{\\mathbf{p}}) = Q_1^s(\\hat{p}_1)$ and $\\hat{q}_2 = Q_2^d(\\hat{\\mathbf{p}}) = Q_2^s(\\hat{p}_2)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with numbers rounded to $6$ decimal places. Concatenate the results for the cases in the given order, appending for each case the four numbers $\\hat{p}_1$, $\\hat{p}_2$, $\\hat{q}_1$, $\\hat{q}_2$ in that order. For example, the output structure should be of the form\n$$\n[\\hat{p}_1^{(1)},\\hat{p}_2^{(1)},\\hat{q}_1^{(1)},\\hat{q}_2^{(1)},\\hat{p}_1^{(2)},\\hat{p}_2^{(2)},\\hat{q}_1^{(2)},\\hat{q}_2^{(2)},\\hat{p}_1^{(3)},\\hat{p}_2^{(3)},\\hat{q}_1^{(3)},\\hat{q}_2^{(3)}].\n$$",
            "solution": "The problem presented is scientifically sound and mathematically well-posed. It requires the computation of an economic equilibrium in a two-good market, which translates to finding the root of a system of two nonlinear equations. The prescribed method is a first-order iterative scheme, which is properly realized as Newton's method with a line search for globalization and to enforce physical constraints. We will construct the solution from first principles as required.\n\nThe system is defined by the market-clearing conditions:\n$$F_1(p_1, p_2) = Q_1^d(p_1, p_2) - Q_1^s(p_1) = 0$$\n$$F_2(p_1, p_2) = Q_2^d(p_1, p_2) - Q_2^s(p_2) = 0$$\nLet $\\mathbf{p} = (p_1, p_2)^\\top$ be the vector of prices. The system of equations can be written as $F(\\mathbf{p}) = \\mathbf{0}$, where $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$. Substituting the given supply and demand functions, we have:\n$$F_1(\\mathbf{p}) = (a_1 - d_1) - b_1 p_1^{\\alpha_1} - e_1 p_1^{\\beta_1} + c_{12} p_2^{\\gamma_{12}} = 0$$\n$$F_2(\\mathbf{p}) = (a_2 - d_2) - b_2 p_2^{\\alpha_2} - e_2 p_2^{\\beta_2} + c_{21} p_1^{\\gamma_{21}} = 0$$\nThese equations must be solved for $p_1 > 0$ and $p_2 > 0$.\n\nThe problem specifies an iterative method based on a first-order local linearization of $F(\\mathbf{p})$. This is precisely Newton's method. At an iterate $\\mathbf{p}^{(k)}$, we find a search direction $\\Delta\\mathbf{p}^{(k)}$ by solving the linear system that arises from the Taylor expansion of $F$ around $\\mathbf{p}^{(k)}$:\n$$F(\\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}) \\approx F(\\mathbf{p}^{(k)}) + J_F(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}^{(k)}$$\nSetting the left side to $\\mathbf{0}$ yields the Newton step equation:\n$$J_F(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}^{(k)} = -F(\\mathbf{p}^{(k)})$$\nwhere $J_F(\\mathbf{p})$ is the Jacobian matrix of $F$ with respect to $\\mathbf{p}$. The components of the Jacobian are the partial derivatives of $F_1$ and $F_2$:\n$$J_F(\\mathbf{p}) = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial p_1} & \\frac{\\partial F_1}{\\partial p_2} \\\\ \\frac{\\partial F_2}{\\partial p_1} & \\frac{\\partial F_2}{\\partial p_2} \\end{pmatrix}$$\nThe required partial derivatives are:\n$$\\frac{\\partial F_1}{\\partial p_1} = -b_1 \\alpha_1 p_1^{\\alpha_1 - 1} - e_1 \\beta_1 p_1^{\\beta_1 - 1}$$\n$$\\frac{\\partial F_1}{\\partial p_2} = c_{12} \\gamma_{12} p_2^{\\gamma_{12} - 1}$$\n$$\\frac{\\partial F_2}{\\partial p_1} = c_{21} \\gamma_{21} p_1^{\\gamma_{21} - 1}$$\n$$\\frac{\\partial F_2}{\\partial p_2} = -b_2 \\alpha_2 p_2^{\\alpha_2 - 1} - e_2 \\beta_2 p_2^{\\beta_2 - 1}$$\nThe next iterate is then found by $\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}$. However, to ensure convergence from a starting point far from the solution and to maintain the positivity of prices, a damped Newton step is used:\n$$\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\lambda^{(k)} \\Delta\\mathbf{p}^{(k)}$$\nThe step length $\\lambda^{(k)} \\in (0, 1]$ is determined by a backtracking line search. Starting with $\\lambda=1$, we test the candidate point $\\mathbf{p}_{trial} = \\mathbf{p}^{(k)} + \\lambda \\Delta\\mathbf{p}^{(k)}$. We accept the step if it satisfies two conditions:\n$1$. Positivity: All components of $\\mathbf{p}_{trial}$ must be greater than or equal to a small positive threshold, specified as $10^{-12}$.\n$2$. Sufficient decrease: The Euclidean norm of the residual at the trial point must be less than the norm at the current point: $\\lVert F(\\mathbf{p}_{trial}) \\rVert_2 < \\lVert F(\\mathbf{p}^{(k)}) \\rVert_2$.\n\nIf either condition fails, the step length $\\lambda$ is reduced by a backtracking factor of $1/2$, and the trial is repeated. This process continues until an acceptable step is found.\n\nThe overall algorithm is as follows:\n$1$. Initialize the price vector $\\mathbf{p}^{(0)} = (10, 10)$, the iteration counter $k=0$, the tolerance $\\varepsilon = 10^{-10}$, the maximum number of iterations $N_{max} = 50$, the minimum price bound $p_{min} = 10^{-12}$, and the backtracking factor $\\rho = 1/2$.\n$2$. For $k = 0, 1, 2, \\dots, N_{max}-1$:\n    a. Evaluate the residual vector $F_k = F(\\mathbf{p}^{(k)})$ and its Euclidean norm $\\lVert F_k \\rVert_2$.\n    b. Check for convergence: If $\\lVert F_k \\rVert_2 < \\varepsilon$, terminate and set the equilibrium price $\\hat{\\mathbf{p}} = \\mathbf{p}^{(k)}$.\n    c. Evaluate the Jacobian matrix $J_k = J_F(\\mathbf{p}^{(k)})$.\n    d. Solve the linear system $J_k \\Delta\\mathbf{p}^{(k)} = -F_k$ for the Newton direction $\\Delta\\mathbf{p}^{(k)}$.\n    e. Initialize step length $\\lambda = 1$.\n    f. Perform backtracking line search:\n        i. Compute trial point $\\mathbf{p}_{trial} = \\mathbf{p}^{(k)} + \\lambda \\Delta\\mathbf{p}^{(k)}$.\n        ii. If all elements of $\\mathbf{p}_{trial}$ are $\\ge p_{min}$ and $\\lVert F(\\mathbf{p}_{trial}) \\rVert_2 < \\lVert F_k \\rVert_2$, then accept the step: set $\\mathbf{p}^{(k+1)} = \\mathbf{p}_{trial}$ and break the line search.\n        iii. Otherwise, reduce step length $\\lambda \\leftarrow \\rho \\lambda$. If $\\lambda$ becomes smaller than a machine precision threshold, break the line search to prevent an infinite loop, and the iteration proceeds with an unchanged $\\mathbf{p}$.\n$3$. If the loop completes without convergence, the last valid iterate $\\mathbf{p}^{(N_{max})}$ is taken as the result.\n$4$. Once the equilibrium price vector $\\hat{\\mathbf{p}} = (\\hat{p}_1, \\hat{p}_2)$ is determined, the corresponding equilibrium quantities are calculated using the demand functions (or equivalently, the supply functions):\n$$\\hat{q}_1 = Q_1^d(\\hat{\\mathbf{p}}) = a_1 - b_1 \\hat{p}_1^{\\alpha_1} + c_{12} \\hat{p}_2^{\\gamma_{12}}$$\n$$\\hat{q}_2 = Q_2^d(\\hat{\\mathbf{p}}) = a_2 - b_2 \\hat{p}_2^{\\alpha_2} + c_{21} \\hat{p}_1^{\\gamma_{21}}$$\nThis procedure will be implemented for each of the three parameter sets provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve for market equilibrium for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple contains parameters in the order:\n    # (a1, b1, alpha1, c12, gamma12, d1, e1, beta1,\n    #  a2, b2, alpha2, c21, gamma21, d2, e2, beta2)\n    test_cases = [\n        (120.0, 1.2, 1.5, 0.4, 1.2, 10.0, 0.9, 1.3,\n         100.0, 1.0, 1.4, 0.3, 1.1, 12.0, 1.1, 1.2),\n        (35.0, 0.9, 1.2, 0.15, 1.3, 5.0, 0.8, 1.1,\n         30.0, 0.7, 1.25, 0.2, 1.15, 4.0, 0.6, 1.05),\n        (80.0, 1.1, 1.6, 0.9, 1.05, 8.0, 1.0, 1.25,\n         90.0, 1.0, 1.55, 0.8, 1.1, 7.0, 0.95, 1.2)\n    ]\n    \n    # Common numerical parameters for the solver\n    p0 = np.array([10.0, 10.0])\n    tol = 1e-10\n    max_iter = 50\n    min_p_val = 1e-12\n    backtrack_factor = 0.5\n    \n    results = []\n    for params in test_cases:\n        p_hat = find_equilibrium(params, p0, tol, max_iter, min_p_val, backtrack_factor)\n        \n        # Unpack parameters for quantity calculation\n        a1, b1, alpha1, c12, gamma12, _, _, _, \\\n        a2, b2, alpha2, c21, gamma21, _, _, _ = params\n        p1, p2 = p_hat\n        \n        # Calculate equilibrium quantities using demand functions\n        q1_hat = a1 - b1 * p1**alpha1 + c12 * p2**gamma12\n        q2_hat = a2 - b2 * p2**alpha2 + c21 * p1**gamma21\n        \n        results.extend([p1, p2, q1_hat, q2_hat])\n\n    # Format the final output string with numbers rounded to 6 decimal places.\n    output_str = f\"[{','.join([f'{x:.6f}' for x in results])}]\"\n    print(output_str)\n\ndef find_equilibrium(params, p_init, tol, max_iter, min_p, backtrack_factor):\n    \"\"\"\n    Implements Newton's method with backtracking to find equilibrium prices.\n    \n    Args:\n        params (tuple): A tuple of all model parameters.\n        p_init (np.ndarray): Initial guess for the price vector.\n        tol (float): Convergence tolerance for the residual norm.\n        max_iter (int): Maximum number of iterations.\n        min_p (float): Minimum allowed price value.\n        backtrack_factor (float): Factor for reducing step size in line search.\n\n    Returns:\n        np.ndarray: The equilibrium price vector.\n    \"\"\"\n    p = np.copy(p_init)\n    \n    # Unpack parameters\n    a1, b1, alpha1, c12, gamma12, d1, e1, beta1, \\\n    a2, b2, alpha2, c21, gamma21, d2, e2, beta2 = params\n    \n    def F(pr):\n        p1, p2 = pr[0], pr[1]\n        f1 = (a1 - d1) - b1 * p1**alpha1 - e1 * p1**beta1 + c12 * p2**gamma12\n        f2 = (a2 - d2) - b2 * p2**alpha2 - e2 * p2**beta2 + c21 * p1**gamma21\n        return np.array([f1, f2])\n\n    def J(pr):\n        p1, p2 = pr[0], pr[1]\n        j11 = -b1 * alpha1 * p1**(alpha1 - 1) - e1 * beta1 * p1**(beta1 - 1)\n        j12 = c12 * gamma12 * p2**(gamma12 - 1)\n        j21 = c21 * gamma21 * p1**(gamma21 - 1)\n        j22 = -b2 * alpha2 * p2**(alpha2 - 1) - e2 * beta2 * p2**(beta2 - 1)\n        return np.array([[j11, j12], [j21, j22]])\n\n    for _ in range(max_iter):\n        F_val = F(p)\n        res_norm = np.linalg.norm(F_val)\n        \n        if res_norm  tol:\n            break\n            \n        J_val = J(p)\n        try:\n            delta_p = np.linalg.solve(J_val, -F_val)\n        except np.linalg.LinAlgError:\n            # Jacobian is singular, cannot proceed with Newton step.\n            # This indicates a problem; break and return the current best guess.\n            break\n\n        # Backtracking line search\n        lambda_step = 1.0\n        while lambda_step > 1e-8: # Prevent excessively small steps\n            p_trial = p + lambda_step * delta_p\n            if np.all(p_trial >= min_p):\n                res_norm_trial = np.linalg.norm(F(p_trial))\n                if res_norm_trial  res_norm:\n                    p = p_trial\n                    break  # Step accepted\n            \n            lambda_step *= backtrack_factor\n    \n    return p\n\nsolve()\n```"
        },
        {
            "introduction": "While deriving an analytical Jacobian is ideal, it is often impractical or impossible for complex, real-world systems. This exercise introduces a powerful alternative: approximating the Jacobian using finite differences. You will implement and compare the performance of the standard Newton's method against this 'quasi-Newton' variant, allowing you to experimentally investigate how the accuracy of the Jacobian approximation impacts the celebrated quadratic convergence rate.",
            "id": "2441924",
            "problem": "Consider the nonlinear system of equations defined by the vector-valued function $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ given by\n$$\n\\mathbf{F}(\\mathbf{x})=\n\\begin{bmatrix}\nf_1(x_1,x_2)\\\\\nf_2(x_1,x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1-\\cos(x_2)\\\\\nx_2-\\sin(x_1)\n\\end{bmatrix},\n$$\nwhere $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ and the trigonometric functions use angles measured in radians. The exact Jacobian matrix $\\mathbf{J}(\\mathbf{x})$ of $\\mathbf{F}$ is\n$$\n\\mathbf{J}(\\mathbf{x})=\n\\begin{bmatrix}\n\\dfrac{\\partial f_1}{\\partial x_1}  \\dfrac{\\partial f_1}{\\partial x_2}\\\\ \\\\\n\\dfrac{\\partial f_2}{\\partial x_1}  \\dfrac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  \\sin(x_2)\\\\\n-\\cos(x_1)  1\n\\end{bmatrix}.\n$$\n\nDefine, for a given perturbation size $h0$, the forward finite-difference Jacobian approximation $\\mathbf{J}_h(\\mathbf{x})$ by\n$$\n\\mathbf{J}_h(\\mathbf{x})=\\begin{bmatrix}\n\\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_1(\\mathbf{x})}{h}  \\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_1(\\mathbf{x})}{h}\\\\ \\\\\n\\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_2(\\mathbf{x})}{h}  \\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_2(\\mathbf{x})}{h}\n\\end{bmatrix},\n$$\nwhere $\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ and $\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$.\n\nFor an initial guess $\\mathbf{x}^{(0)}\\in\\mathbb{R}^2$, define the sequence $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$ recursively by\n$$\n\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big),\\qquad \\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)},\n$$\nwhere $\\mathbf{J}_\\star$ denotes either the exact Jacobian $\\mathbf{J}$ or the finite-difference Jacobian $\\mathbf{J}_h$, and the update $\\mathbf{s}^{(k)}$ is any solution of the linear system. Let the residual norm be $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$, and let the iteration terminate at the smallest index $k$ such that $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon$, with tolerance $\\varepsilon=10^{-10}$, or after $k_{\\max}=50$ iterations, whichever occurs first.\n\nFor each test case below, perform two runs starting from the given initial guess $\\mathbf{x}^{(0)}$:\n- Run A uses $\\mathbf{J}_\\star=\\mathbf{J}$.\n- Run B uses $\\mathbf{J}_\\star=\\mathbf{J}_h$ with the specified $h$.\n\nFor each run, record:\n- The number of iterations $n$ required to satisfy $\\|\\mathbf{F}(\\mathbf{x}^{(n)})\\|_2 \\le \\varepsilon$ (or $n=k_{\\max}$ if the tolerance is not met).\n- An estimate $\\hat{p}$ of the local convergence order computed from the last three available residual norms $\\{e_{m-2},e_{m-1},e_m\\}$ via\n$$\n\\hat{p}=\\frac{\\ln\\left(\\dfrac{e_m}{e_{m-1}}\\right)}{\\ln\\left(\\dfrac{e_{m-1}}{e_{m-2}}\\right)},\n$$\nwhere $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$, and $m$ is the final iteration index used in the run (use the last three residuals available at termination; all logarithms are natural logarithms). All angles in trigonometric functions are in radians.\n\nUse the following test suite, where each case is a pair $(\\mathbf{x}^{(0)},h)$:\n- Case $1$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$.\n- Case $2$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$.\n- Case $3$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$.\n- Case $4$: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$.\n\nYour program must output a single line containing a list of results, one per test case, in the same order as listed. Each test case result must be a list of four entries $[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}]$, where $n_{\\text{exact}}$ and $\\hat{p}_{\\text{exact}}$ correspond to Run A, and $n_{\\text{fd}}$ and $\\hat{p}_{\\text{fd}}$ correspond to Run B. The final output format must be a single line that is a comma-separated list of these per-case lists enclosed in square brackets, for example, $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\dots]$.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Nonlinear System**: A function $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ is defined as $\\mathbf{F}(\\mathbf{x})= \\begin{bmatrix} f_1(x_1,x_2)\\\\ f_2(x_1,x_2) \\end{bmatrix} = \\begin{bmatrix} x_1-\\cos(x_2)\\\\ x_2-\\sin(x_1) \\end{bmatrix}$ for $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$. Trigonometric functions use radians.\n- **Exact Jacobian**: The Jacobian matrix of $\\mathbf{F}$ is given as $\\mathbf{J}(\\mathbf{x}) = \\begin{bmatrix} 1  \\sin(x_2)\\\\ -\\cos(x_1)  1 \\end{bmatrix}$.\n- **Finite-Difference Jacobian**: An approximation $\\mathbf{J}_h(\\mathbf{x})$ is defined for a perturbation $h0$ by its columns: the $j$-th column is $\\frac{\\mathbf{F}(\\mathbf{x}+h\\,\\mathbf{e}_j)-\\mathbf{F}(\\mathbf{x})}{h}$, where $\\mathbf{e}_j$ are the standard basis vectors.\n- **Iterative Scheme**: A sequence $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$ is generated from an initial guess $\\mathbf{x}^{(0)}$ by solving $\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big)$ and setting $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)}$. Here, $\\mathbf{J}_\\star$ is either the exact Jacobian $\\mathbf{J}$ or the approximation $\\mathbf{J}_h$.\n- **Termination Criteria**: The iteration stops at the smallest index $k$ where the residual norm $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon = 10^{-10}$, or after $k_{\\max}=50$ iterations.\n- **Tasks**: For each test case, two runs are performed: Run A with $\\mathbf{J}_\\star=\\mathbf{J}$ and Run B with $\\mathbf{J}_\\star=\\mathbf{J}_h$. For each run, two quantities are to be recorded:\n    1. The number of iterations, $n$.\n    2. An estimate of the convergence order, $\\hat{p}=\\frac{\\ln(e_m/e_{m-1})}{\\ln(e_{m-1}/e_{m-2})}$, where $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$ and $m$ is the final iteration index.\n- **Test Cases**:\n    - Case 1: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$.\n    - Case 2: $\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$.\n    - Case 3: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$.\n    - Case 4: $\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$.\n- **Output Format**: A single-line list of lists: $[[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}], \\dots]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientific Grounding**: The problem is a standard exercise in numerical analysis, specifically in the field of computational methods for solving systems of nonlinear equations. Newton's method and its quasi-Newton variants (using finite-difference Jacobians) are fundamental, well-established algorithms. The mathematical formulations are correct.\n- **Well-Posedness**: The problem is clearly defined. The functions, Jacobians, iterative scheme, termination conditions, and required outputs are all specified unambiguously. The given system of equations has a unique solution in the domain of interest, and the Jacobian is non-singular near this solution, ensuring that the linear systems to be solved are well-posed. For example, the determinant of the Jacobian is $\\det(\\mathbf{J}) = 1 + \\cos(x_1)\\sin(x_2)$. For initial guesses like $(0.5, 0.5)$ or $(1.0, 1.0)$, the determinant is well away from zero, suggesting local convergence is achievable.\n- **Objectivity**: The language is formal and objective, free from subjective or non-scientific content.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. The calculation of $\\hat{p}$ requires at least three residual norms, corresponding to at least two iterations ($n\\ge 2$). Given the initial conditions and the nature of Newton's method, this is a reasonable expectation.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n---\n\nThe problem requires the implementation of two variants of Newton's method to solve the nonlinear system $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$. The first variant is the classical Newton-Raphson method, which uses the exact analytical Jacobian of $\\mathbf{F}$. The second is a quasi-Newton method where the Jacobian is approximated using a forward finite-difference scheme. We will compare the performance of these two methods in terms of the number of iterations required for convergence and the estimated local order of convergence.\n\nThe core of the method is the iterative update rule. At each step $k$, we approximate the nonlinear function $\\mathbf{F}$ with its linear Taylor expansion around the current iterate $\\mathbf{x}^{(k)}$:\n$$\n\\mathbf{F}(\\mathbf{x}) \\approx \\mathbf{F}(\\mathbf{x}^{(k)}) + \\mathbf{J}_\\star(\\mathbf{x}^{(k)})(\\mathbf{x} - \\mathbf{x}^{(k)})\n$$\nWe seek the next iterate $\\mathbf{x}^{(k+1)}$ by setting this approximation to zero, i.e., $\\mathbf{F}(\\mathbf{x}^{(k+1)}) = \\mathbf{0}$. Defining the update step as $\\mathbf{s}^{(k)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}$, we obtain the linear system for the update:\n$$\n\\mathbf{J}_\\star(\\mathbf{x}^{(k)}) \\mathbf{s}^{(k)} = -\\mathbf{F}(\\mathbf{x}^{(k)})\n$$\nOnce $\\mathbf{s}^{(k)}$ is found by solving this system, the next iterate is computed as $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)}$. This process is repeated until the norm of the residual vector, $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$, falls below a specified tolerance $\\varepsilon = 10^{-10}$.\n\n**Run A: Exact Jacobian (Newton-Raphson Method)**\nIn this run, $\\mathbf{J}_\\star$ is the exact Jacobian matrix $\\mathbf{J}(\\mathbf{x})$:\n$$\n\\mathbf{J}(\\mathbf{x}) =\n\\begin{bmatrix}\n1  \\sin(x_2)\\\\\n-\\cos(x_1)  1\n\\end{bmatrix}\n$$\nThis method is known to exhibit quadratic convergence (i.e., convergence order $p=2$) when the initial guess is sufficiently close to the solution and the Jacobian is non-singular at the solution. This means the number of correct digits in the solution roughly doubles with each iteration. The estimated order $\\hat{p}$ should therefore be close to $2$.\n\n**Run B: Finite-Difference Jacobian (Quasi-Newton Method)**\nIn this run, the Jacobian is approximated using the forward finite-difference formula. The $j$-th column of the approximate Jacobian $\\mathbf{J}_h(\\mathbf{x})$ is given by:\n$$\n[\\mathbf{J}_h(\\mathbf{x})]_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h}\n$$\nwhere $\\mathbf{e}_j$ is the $j$-th standard basis vector and $h$ is a small perturbation parameter. For the given $2 \\times 2$ system, this yields:\n$$\n\\mathbf{J}_h(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{f_1(x_1+h, x_2) - f_1(x_1, x_2)}{h}  \\frac{f_1(x_1, x_2+h) - f_1(x_1, x_2)}{h} \\\\\n\\frac{f_2(x_1+h, x_2) - f_2(x_1, x_2)}{h}  \\frac{f_2(x_1, x_2+h) - f_2(x_1, x_2)}{h}\n\\end{bmatrix}\n$$\nThis approach avoids the need for analytical derivation of the Jacobian, which can be complex or impossible for some functions. However, it introduces an approximation error. The error in each element of $\\mathbf{J}_h$ is of order $O(h)$. This error perturbs the Newton step, affecting the convergence rate. For a very small $h$ (e.g., $10^{-6}$), the approximation is accurate, and the convergence should be nearly quadratic. As $h$ increases (e.g., $10^{-3}$ or $10^{-2}$), the approximation worsens, and the convergence rate is expected to degrade, potentially becoming linear ($p=1$), and requiring more iterations.\n\n**Algorithm and Implementation**\nFor each test case $(\\mathbf{x}^{(0)}, h)$, we perform the following steps for both Run A and Run B:\n$1$. Initialize $k=0$ and the current solution $\\mathbf{x} = \\mathbf{x}^{(0)}$. Create a list to store residual norms.\n$2$. Begin a loop that continues as long as $k \\le k_{\\max} = 50$.\n$3$. Compute the residual vector $\\mathbf{F}(\\mathbf{x})$ and its Euclidean norm $e_k = \\|\\mathbf{F}(\\mathbf{x})\\|_2$. Store $e_k$.\n$4$. Check for termination: if $e_k \\le \\varepsilon=10^{-10}$, set the final iteration count $n=k$ and exit the loop.\n$5$. If the loop is to continue (i.e., $k  k_{\\max}$), compute the appropriate Jacobian matrix $\\mathbf{J}_\\star(\\mathbf{x})$ (either exact or finite-difference).\n$6$. Solve the linear system $\\mathbf{J}_\\star(\\mathbf{x})\\mathbf{s} = -\\mathbf{F}(\\mathbf{x})$ for the step $\\mathbf{s}$.\n$7$. Update the solution: $\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}$.\n$8$. Increment the iteration counter: $k \\leftarrow k+1$.\n$9$. If the loop completes because $k$ reached $k_{\\max}$, set $n=k_{\\max}$.\n$10$. After the loop terminates, calculate the estimated convergence order $\\hat{p}$ using the last three available residual norms, $e_{n-2}, e_{n-1}, e_{n}$. If fewer than three norms are available (i.e., $n2$), $\\hat{p}$ is considered not computable.\nThe results $(n, \\hat{p})$ from both runs are collected for each test case to form the final output. This process will demonstrate the theoretical properties of Newton-family methods in a practical computational context.",
            "answer": "```python\nimport numpy as np\n\n# Global constants as specified in the problem\nTOLERANCE = 1e-10\nK_MAX = 50\n\ndef F(x):\n    \"\"\"\n    Computes the vector-valued function F(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        x[0] - np.cos(x[1]),\n        x[1] - np.sin(x[0])\n    ])\n\ndef J_exact(x):\n    \"\"\"\n    Computes the exact Jacobian matrix J(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        [1.0, np.sin(x[1])],\n        [-np.cos(x[0]), 1.0]\n    ])\n\ndef J_fd(x, h):\n    \"\"\"\n    Computes the forward finite-difference approximation of the Jacobian.\n    x must be a NumPy array [x1, x2].\n    h is the perturbation size.\n    \"\"\"\n    n = len(x)\n    jac = np.zeros((n, n), dtype=float)\n    fx = F(x)\n    for j in range(n):\n        x_plus_h = x.copy()\n        x_plus_h[j] += h\n        fx_plus_h = F(x_plus_h)\n        jac[:, j] = (fx_plus_h - fx) / h\n    return jac\n\ndef newton_solver(x0, h, use_exact_jacobian):\n    \"\"\"\n    Solves the nonlinear system F(x)=0 using a Newton-like method.\n    \n    Args:\n        x0 (list or np.ndarray): The initial guess.\n        h (float): The perturbation size for the finite-difference Jacobian.\n        use_exact_jacobian (bool): If True, use the exact Jacobian. \n                                  If False, use the finite-difference approximation.\n\n    Returns:\n        tuple: (n_iter, p_hat) where n_iter is the number of iterations and\n               p_hat is the estimated convergence order.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    k = 0\n    residuals = []\n    n_iter = K_MAX\n\n    while k = K_MAX:\n        F_val = F(x)\n        norm = np.linalg.norm(F_val)\n        residuals.append(norm)\n\n        if norm = TOLERANCE:\n            n_iter = k\n            break\n        \n        if k == K_MAX:\n            break\n\n        if use_exact_jacobian:\n            Jacobian = J_exact(x)\n        else:\n            Jacobian = J_fd(x, h)\n\n        try:\n            # Solve the linear system J*s = -F\n            s = np.linalg.solve(Jacobian, -F_val)\n            x += s\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, the solver fails.\n            n_iter = K_MAX\n            break\n        \n        k += 1\n    \n    # Estimate convergence order p_hat from the last three residuals\n    p_hat = np.nan\n    if len(residuals) >= 3:\n        # Use residuals e_n, e_{n-1}, e_{n-2}\n        e_m, e_m1, e_m2 = residuals[-1], residuals[-2], residuals[-3]\n        \n        # Avoid division by zero or log of non-positive numbers.\n        # Ratios must be  1 for convergence.\n        ratio1 = e_m / e_m1 if e_m1 != 0 else 0\n        ratio2 = e_m1 / e_m2 if e_m2 != 0 else 0\n\n        if 0  ratio1  1 and 0  ratio2  1:\n            p_hat = np.log(ratio1) / np.log(ratio2)\n            \n    return n_iter, p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        ([0.5, 0.5], 1e-6),\n        ([0.5, 0.5], 1e-3),\n        ([1.0, 1.0], 1e-6),\n        ([1.0, 1.0], 1e-2)\n    ]\n\n    all_results = []\n    for x0, h in test_cases:\n        # Run A: Exact Jacobian\n        n_exact, p_exact = newton_solver(x0, h, use_exact_jacobian=True)\n        \n        # Run B: Finite-Difference Jacobian\n        n_fd, p_fd = newton_solver(x0, h, use_exact_jacobian=False)\n        \n        # Assemble results for the current test case\n        case_result = [n_exact, n_fd, p_exact, p_fd]\n        all_results.append(case_result)\n\n    # The final print statement must match the format exactly.\n    # The default string representation of a list of lists is \"[...], [...]\"\n    # and the default representation of a float is what we need.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "Newton's method is remarkably powerful, but its convergence is not guaranteed. This final practice moves from implementation to a deeper theoretical exploration of the method's dynamics. By analyzing a carefully constructed system, you will determine the precise conditions under which the iterates, instead of converging to a root, become trapped in a periodic 2-cycle, revealing the fascinating and sometimes complex behavior hidden within this fundamental algorithm.",
            "id": "2441953",
            "problem": "Consider the one-parameter family of nonlinear systems in two variables defined by\n$$\nF_{a}(x,y)=\\begin{pmatrix}\nx^{3}-a\\,x\\\\\ny\n\\end{pmatrix},\n$$\nwhere $a\\in\\mathbb{R}$ and $(x,y)\\in\\mathbb{R}^{2}$. For Newtonâ€™s method applied to systems, the iteration is defined by\n$$\n\\mathbf{X}_{k+1}=\\mathbf{X}_{k}-J_{F_{a}}(\\mathbf{X}_{k})^{-1}F_{a}(\\mathbf{X}_{k}),\n$$\nwhere $J_{F_{a}}$ is the Jacobian matrix of $F_{a}$.\n\nStarting from the initial guess $\\mathbf{X}_{0}=(1,0)$, determine the unique real value of $a$ for which the Newton iterates enter a periodic orbit of prime period $2$ that alternates between the two distinct points $(1,0)$ and $(-1,0)$, rather than converging to a root.\n\nGive your answer as a single real number. No rounding is required and no units are involved.",
            "solution": "The problem statement has been subjected to rigorous validation and is found to be valid. It is scientifically grounded, well-posed, and objective, with all necessary information provided for a unique solution. The concepts of nonlinear systems and Newton's method are standard in computational engineering, and the existence of periodic orbits for Newton's method is a well-established phenomenon. We may therefore proceed with the solution.\n\nThe problem asks for the value of the parameter $a \\in \\mathbb{R}$ for which Newton's method, applied to the system $F_{a}(x,y)$, results in a periodic orbit of prime period $2$ between the points $\\mathbf{P}_1 = (1,0)$ and $\\mathbf{P}_2 = (-1,0)$. The system is defined as:\n$$\nF_{a}(x,y) = \\begin{pmatrix} x^{3} - a x \\\\ y \\end{pmatrix}\n$$\nThe Newton iteration for a vector $\\mathbf{X} = (x,y)^T$ is given by:\n$$\n\\mathbf{X}_{k+1} = \\mathbf{X}_{k} - J_{F_{a}}(\\mathbf{X}_{k})^{-1} F_{a}(\\mathbf{X}_{k})\n$$\nFirst, we must compute the Jacobian matrix $J_{F_{a}}$ of the function $F_{a}(x,y)$. The components of $F_a$ are $f_1(x,y) = x^3 - ax$ and $f_2(x,y) = y$.\n$$\nJ_{F_{a}}(x,y) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x}  \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x}  \\frac{\\partial f_2}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 3x^2 - a  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe inverse of the Jacobian matrix, $J_{F_{a}}(x,y)^{-1}$, exists if and only if its determinant is non-zero. The determinant is $\\det(J_{F_{a}}(x,y)) = (3x^2 - a)(1) - (0)(0) = 3x^2 - a$. Thus, the inverse exists provided $3x^2 - a \\neq 0$. The inverse is:\n$$\nJ_{F_{a}}(x,y)^{-1} = \\begin{pmatrix} \\frac{1}{3x^2 - a}  0 \\\\ 0  1 \\end{pmatrix}\n$$\nNow, we can write the Newton iteration step explicitly. Let $\\mathbf{X}_k = (x_k, y_k)^T$.\n$$\n\\mathbf{X}_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{3x_k^2 - a}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} x_k^3 - a x_k \\\\ y_k \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} - \\begin{pmatrix} \\frac{x_k^3 - a x_k}{3x_k^2 - a} \\\\ y_k \\end{pmatrix}\n$$\nThis simplifies to the component-wise iteration formulas:\n$$\nx_{k+1} = x_k - \\frac{x_k^3 - a x_k}{3x_k^2 - a} = \\frac{x_k(3x_k^2 - a) - (x_k^3 - a x_k)}{3x_k^2 - a} = \\frac{3x_k^3 - a x_k - x_k^3 + a x_k}{3x_k^2 - a} = \\frac{2x_k^3}{3x_k^2 - a}\n$$\n$$\ny_{k+1} = y_k - y_k = 0\n$$\nLet us denote the Newton iteration map as $N_a(\\mathbf{X})$. We have found that $N_a((x_k, y_k)) = (\\frac{2x_k^3}{3x_k^2 - a}, 0)$.\nThe problem specifies a periodic orbit of prime period $2$ between the points $\\mathbf{P}_1 = (1,0)$ and $\\mathbf{P}_2 = (-1,0)$. This requires the following two conditions to be met:\n$1$. $N_a(\\mathbf{P}_1) = \\mathbf{P}_2$\n$2$. $N_a(\\mathbf{P}_2) = \\mathbf{P}_1$\n\nLet's apply the first condition. We set $\\mathbf{X}_k = \\mathbf{P}_1 = (1,0)$, so $x_k = 1$ and $y_k=0$. The next iterate must be $\\mathbf{X}_{k+1} = \\mathbf{P}_2 = (-1,0)$.\nUsing the formula for $x_{k+1}$:\n$$\nx_{k+1} = \\frac{2(1)^3}{3(1)^2 - a} = \\frac{2}{3 - a}\n$$\nWe require $x_{k+1} = -1$. Therefore:\n$$\n\\frac{2}{3 - a} = -1\n$$\nSolving for $a$:\n$$\n2 = -1 \\cdot (3 - a) = a - 3\n$$\n$$\na = 5\n$$\nThe $y$-component of the next iterate is $y_{k+1} = 0$, which correctly matches the $y$-component of $\\mathbf{P}_2$. Thus, the first condition yields $a=5$.\n\nNow we must verify that this value of $a$ also satisfies the second condition, $N_a(\\mathbf{P}_2) = \\mathbf{P}_1$. We set $\\mathbf{X}_k = \\mathbf{P}_2 = (-1,0)$, so $x_k = -1$ and $y_k=0$. We expect $\\mathbf{X}_{k+1} = \\mathbf{P}_1 = (1,0)$. Let's use $a=5$ in our iteration formula:\n$$\nx_{k+1} = \\frac{2x_k^3}{3x_k^2 - a} = \\frac{2(-1)^3}{3(-1)^2 - 5} = \\frac{2(-1)}{3(1) - 5} = \\frac{-2}{3 - 5} = \\frac{-2}{-2} = 1\n$$\nThe resulting $x$-coordinate is $1$. The $y$-coordinate is again $y_{k+1} = 0$. The next iterate is indeed $(1,0) = \\mathbf{P}_1$. Both conditions are satisfied for $a=5$. The derivation yields a single, unique value for $a$.\n\nFinally, we must check that the Jacobian matrix is invertible at the points of the orbit for $a=5$. The condition for invertibility is $3x^2 - a \\neq 0$.\nAt $\\mathbf{P}_1=(1,0)$, we have $x=1$. The expression becomes $3(1)^2 - 5 = 3 - 5 = -2 \\neq 0$.\nAt $\\mathbf{P}_2=(-1,0)$, we have $x=-1$. The expression becomes $3(-1)^2 - 5 = 3 - 5 = -2 \\neq 0$.\nThe Jacobian is invertible at both points, so the Newton iteration is well-defined.\nThe orbit is $(1,0) \\to (-1,0) \\to (1,0) \\to \\dots$, which has a period of $2$. Since the points are distinct, the period is not $1$. Therefore, the prime period is $2$.\nThe unique real value of $a$ is $5$.",
            "answer": "$$\\boxed{5}$$"
        }
    ]
}