{
    "hands_on_practices": [
        {
            "introduction": "The theoretical elegance of Newton's method, which promises quadratic convergence, can sometimes mask practical pitfalls. This exercise reveals a critical subtlety that arises when dealing with non-convex functions, where the Hessian matrix is not positive definite. You will analyze a scenario where, counter-intuitively, a full Newton step successfully reduces the gradient's norm, moving closer to a stationary point, yet fails to decrease the function value itself. This foundational insight demonstrates precisely why simple Newton's method is insufficient and motivates the development of robust algorithms with safeguards like line searches or trust regions.",
            "id": "2417369",
            "problem": "Let $f:\\mathbb{R}^2 \\to \\mathbb{R}$ be defined by $f(x_1,x_2)=\\tfrac{1}{2}x_1^2-\\tfrac{1}{2}x_2^2$. Starting from $x_0=(0,1)$, generate the Newton iterates for finding a stationary point using the full-step update $x_{k+1}=x_k-\\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$. Demonstrate that the Euclidean norm of the gradient $\\|\\nabla f(x_k)\\|_2$ decreases monotonically along the produced iterates, while the function value $f(x_k)$ increases at the first iteration, that is, $f(x_1)f(x_0)$. Compute the exact value of $\\Delta f=f(x_1)-f(x_0)$. Provide your answer as an exact value with no rounding.",
            "solution": "The objective function is given by $f(x_1, x_2) = \\frac{1}{2}x_1^2 - \\frac{1}{2}x_2^2$. We seek a stationary point using Newton's method. A stationary point $x^*$ satisfies $\\nabla f(x^*) = 0$.\n\nFirst, we compute the gradient vector $\\nabla f(x)$ and the Hessian matrix $\\nabla^2 f(x)$ for a general point $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\nThe gradient is:\n$$\n\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ -x_2 \\end{pmatrix}\n$$\nThe Hessian matrix is:\n$$\n\\nabla^2 f(x) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}  \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}\n$$\nThe Hessian matrix is constant for all $x \\in \\mathbb{R}^2$. The eigenvalues of $\\nabla^2 f$ are $\\lambda_1 = 1$ and $\\lambda_2 = -1$. Since one eigenvalue is positive and one is negative, the Hessian is indefinite. This implies that the stationary point of the function is a saddle point.\nThe inverse of the Hessian is:\n$$\n\\left[\\nabla^2 f(x)\\right]^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}\n$$\nThe Hessian is its own inverse.\n\nThe Newton update rule is $x_{k+1} = x_k - \\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$. For our specific function, this becomes:\n$$\nx_{k+1} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} x_{1,k} \\\\ -x_{2,k} \\end{pmatrix} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis shows that for any starting point $x_k \\neq (0,0)$, Newton's method converges to the stationary point $(0,0)$ in a single iteration.\n\nWe start from $x_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe first iterate, $x_1$, is:\n$$\nx_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nAll subsequent iterates will remain at the stationary point: $x_k = (0,0)$ for all $k \\ge 1$.\n\nNow, we address the tasks specified in the problem.\n\nTask 1: Demonstrate that $\\|\\nabla f(x_k)\\|_2$ decreases monotonically.\nWe compute the gradient at each iterate.\nFor $k=0$:\n$$\n\\nabla f(x_0) = \\nabla f(0,1) = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\nThe Euclidean norm is $\\|\\nabla f(x_0)\\|_2 = \\sqrt{0^2 + (-1)^2} = 1$.\nFor $k=1$:\n$$\n\\nabla f(x_1) = \\nabla f(0,0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe Euclidean norm is $\\|\\nabla f(x_1)\\|_2 = \\sqrt{0^2 + 0^2} = 0$.\nFor all $k > 1$, since $x_k = (0,0)$, we have $\\nabla f(x_k) = (0,0)$ and $\\|\\nabla f(x_k)\\|_2 = 0$.\nThe sequence of the norms of the gradients is $1, 0, 0, 0, \\ldots$. This is a monotonically non-increasing (and therefore, monotonically decreasing) sequence, as $1 > 0$ and $0 \\ge 0$. The condition is satisfied.\n\nTask 2: Demonstrate that $f(x_1) > f(x_0)$.\nWe compute the function value at $x_0$ and $x_1$.\nAt the starting point $x_0 = (0,1)$:\n$$\nf(x_0) = f(0,1) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(1)^2 = -\\frac{1}{2}\n$$\nAt the first iterate $x_1 = (0,0)$:\n$$\nf(x_1) = f(0,0) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(0)^2 = 0\n$$\nComparing the two values, we have $f(x_1) = 0$ and $f(x_0) = -1/2$. Indeed, $0 > -1/2$, which demonstrates that $f(x_1) > f(x_0)$. The function value increases. This occurs because the Hessian is not positive definite, and thus the Newton direction is not guaranteed to be a descent direction. The search direction is $p_0 = x_1 - x_0 = (0,0) - (0,1) = (0,-1)$. The directional derivative along $p_0$ from $x_0$ is $\\nabla f(x_0)^T p_0 = \\begin{pmatrix} 0  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = 1 > 0$, confirming that $p_0$ is an ascent direction.\n\nTask 3: Compute the exact value of $\\Delta f = f(x_1) - f(x_0)$.\nUsing the values calculated in the previous step:\n$$\n\\Delta f = f(x_1) - f(x_0) = 0 - \\left(-\\frac{1}{2}\\right) = \\frac{1}{2}\n$$\nThe exact value of the change in the function value after the first iteration is $\\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Building on the knowledge that safeguards are essential, this practice moves from theory to implementation. You will develop and compare two of the most important algorithms in continuous optimization: a modified Newton's method and the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method, both operating within a line-search framework. By applying these methods to the notoriously difficult Rosenbrock function, you will gain first-hand experience with their performance, trade-offs, and the practical implications of choosing different termination criteria. This is a cornerstone exercise for any computational engineer working with optimization.",
            "id": "2417339",
            "problem": "You are to study the impact of alternative termination criteria on an ill-conditioned optimization problem by constructing and analyzing sequences generated by a second-order search direction and a quasi-second-order search direction. Consider the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by\n$$\nf(x) \\equiv f\\left(\\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix}\\right) = 100\\left(x_{2}-x_{1}^{2}\\right)^{2} + \\left(1-x_{1}\\right)^{2},\n$$\nwhich is known to have a narrow curved valley and a unique minimizer at $x^{\\star}=\\begin{bmatrix}1\\\\1\\end{bmatrix}$.\n\nDefine an iterative process from an initial point $x_{0}\\in\\mathbb{R}^{2}$ as follows. At iteration $k$, compute a search direction $p_{k}$ by solving\n$$\nB_{k}p_{k}=-\\nabla f(x_{k}),\n$$\nand then take a step\n$$\nx_{k+1}=x_{k}+\\alpha_{k}p_{k},\n$$\nwhere $\\alpha_{k}$ is chosen by an Armijo backtracking line-search with parameters $c_{1}=10^{-4}$ and contraction factor $\\beta=\\tfrac{1}{2}$, starting from $\\alpha_{k}=1$ and reducing $\\alpha_{k}\\leftarrow \\beta \\alpha_{k}$ until\n$$\nf(x_{k}+\\alpha_{k}p_{k}) \\le f(x_{k}) + c_{1}\\alpha_{k}\\nabla f(x_{k})^{\\top}p_{k},\n$$\nor until $\\alpha_{k}10^{-16}$, in which case the current $\\alpha_{k}$ is accepted. For the matrix $B_{k}$, consider the following two cases:\n- Case H (exact second order): $B_{k}=\\nabla^{2} f(x_{k})$ with a diagonal shift $\\tau I$ if needed so that $B_{k}$ is symmetric positive definite, where $\\tau \\ge \\max\\{0,\\delta-\\lambda_{\\min}(\\nabla^{2}f(x_{k}))\\}$ with $\\delta=10^{-6}$ and $\\lambda_{\\min}(\\cdot)$ denotes the minimal eigenvalue. If $p_{k}$ computed from $B_{k}$ does not satisfy $\\nabla f(x_{k})^{\\top}p_{k}0$, replace $p_{k}$ by $-\\nabla f(x_{k})$.\n- Case Q (quasi-second order): $B_{k}^{-1}$ is maintained by the Broyden-Fletcher-Goldfarb-Shanno (BFGS) inverse-Hessian update starting from $B_{0}^{-1}=I$, using the standard inverse BFGS formula with $s_{k}=x_{k+1}-x_{k}$, $y_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k})$, provided $y_{k}^{\\top}s_{k}0$, and skipping the update otherwise. The search direction is $p_{k}=-B_{k}^{-1}\\nabla f(x_{k})$.\n\nAdopt a maximum number of iterations $N_{\\max}=2000$. Investigate the following two termination criteria:\n- Criterion G: stop when $\\lVert \\nabla f(x_{k})\\rVert_{\\infty}  \\varepsilon$.\n- Criterion S: stop when $\\lVert x_{k}-x_{k-1}\\rVert_{2}  \\varepsilon$ (this criterion is applied only for $k\\ge 1$).\n\nYour program must run the iterative process for each test case, record the number of accepted iterations performed, the final objective value $f(x_{k})$, the final infinity-norm of the gradient $\\lVert \\nabla f(x_{k})\\rVert_{\\infty}$, and the final step two-norm $\\lVert x_{k}-x_{k-1}\\rVert_{2}$ (take this as $0$ if no step was taken). If the maximum iteration count $N_{\\max}$ is reached without satisfying the termination criterion, return the current metrics.\n\nTest Suite. For each test case, the tuple consists of: method choice (H or Q), termination criterion (G or S), initial point $x_{0}$, and tolerance $\\varepsilon$. Use the following $6$ cases:\n- Case $1$: method H, criterion G, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-8}$.\n- Case $2$: method H, criterion S, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-4}$.\n- Case $3$: method Q, criterion G, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-8}$.\n- Case $4$: method Q, criterion S, $x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-4}$.\n- Case $5$: method H, criterion G, $x_{0}=\\begin{bmatrix}1.0\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-12}$.\n- Case $6$: method Q, criterion S, $x_{0}=\\begin{bmatrix}1.0\\\\ 1.0\\end{bmatrix}$, $\\varepsilon=10^{-12}$.\n\nFinal output format. Your program should produce a single line of output containing a list of results, one per test case, in the same order as above. Each result must be the list\n$$\n[\\ \\text{method\\_id},\\ \\text{criterion\\_id},\\ \\text{iterations},\\ f(x_{k}),\\ \\lVert \\nabla f(x_{k})\\rVert_{\\infty},\\ \\lVert x_{k}-x_{k-1}\\rVert_{2}\\ ],\n$$\nwhere method identifiers are $0$ for H and $1$ for Q, criterion identifiers are $0$ for G and $1$ for S, the iteration count is an integer, and the remaining entries are real numbers. The full line must therefore be a single list of $6$ lists, for example\n$$\n[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots],\n$$\nwith no other characters printed.",
            "solution": "The objective is to find the minimizer of the Rosenbrock function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$, defined as:\n$$\nf(x) = f\\left(\\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix}\\right) = 100\\left(x_{2}-x_{1}^{2}\\right)^{2} + \\left(1-x_{1}\\right)^{2}\n$$\nThis function is a classic benchmark for optimization algorithms due to its non-convex nature and a narrow, curved valley leading to the global minimum, which makes convergence challenging. The unique minimizer is known to be $x^{\\star} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$, where $f(x^{\\star}) = 0$.\n\nFor the iterative methods, we require the gradient vector $\\nabla f(x)$ and, for Newton's method, the Hessian matrix $\\nabla^{2} f(x)$.\nThe gradient is given by:\n$$\n\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} -400x_{1}(x_{2}-x_{1}^{2}) - 2(1-x_{1}) \\\\ 200(x_{2}-x_{1}^{2}) \\end{bmatrix}\n$$\nThe Hessian matrix is:\n$$\n\\nabla^{2} f(x) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}  \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 1200x_{1}^{2} - 400x_{2} + 2  -400x_{1} \\\\ -400x_{1}  200 \\end{bmatrix}\n$$\nAll iterative methods under consideration follow the general structure:\n$$\nx_{k+1} = x_{k} + \\alpha_{k} p_{k}\n$$\nwhere $x_{k}$ is the current iterate, $p_{k}$ is the search direction, and $\\alpha_{k}$ is the step length.\n\n**Search Direction Calculation**\n\nThe primary difference between the methods lies in the computation of the search direction $p_{k}$. In general, this is based on solving a linear system of the form $B_{k}p_{k} = -\\nabla f(x_{k})$, where $B_k$ is a symmetric positive-definite matrix that is either the true Hessian or an approximation thereof.\n\n1.  **Case H: Modified Newton's Method**\n    This method uses second-order information by setting $B_{k}$ to the exact Hessian, $B_{k} = \\nabla^{2} f(x_{k})$. The search direction is then $p_{k} = -(\\nabla^{2} f(x_{k}))^{-1} \\nabla f(x_{k})$. This direction is optimal for the local quadratic model of the function. However, if the Hessian $\\nabla^{2} f(x_{k})$ is not positive definite, the quadratic model is not convex, and $p_{k}$ may not be a descent direction. To overcome this, a modification is employed. The matrix $B_k$ is adjusted to $B_k = \\nabla^{2} f(x_{k}) + \\tau I$, where $I$ is the identity matrix and $\\tau$ is a non-negative shift chosen to ensure $B_k$ is sufficiently positive definite. Specifically, $\\tau = \\max\\{0, \\delta - \\lambda_{\\min}(\\nabla^{2}f(x_{k}))\\}$ for a small constant $\\delta > 0$. This procedure, known as a modified Cholesky or eigenvalue-shifting approach, guarantees that the resulting search direction is a descent direction. As a further safeguard, if the computed $p_k$ fails the descent condition $\\nabla f(x_k)^\\top p_k  0$, the algorithm reverts to the most basic descent direction, the steepest descent direction, $p_k = -\\nabla f(x_k)$.\n\n2.  **Case Q: BFGS Quasi-Newton Method**\n    This method avoids the computational expense and potential complexities of calculating and inverting the Hessian matrix at each iteration. Instead, it builds an approximation to the *inverse* Hessian, denoted here as $B_{k}^{-1}$. The search direction is then easily computed as $p_{k} = -B_{k}^{-1}\\nabla f(x_{k})$. Starting with an initial guess, typically $B_{0}^{-1} = I$, the approximation is updated at each step using information from the most recent step. The BFGS update formula for the inverse Hessian is:\n    $$\n    B_{k+1}^{-1} = \\left(I - \\frac{s_{k}y_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\\right) B_{k}^{-1} \\left(I - \\frac{y_{k}s_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\\right) + \\frac{s_{k}s_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\n    $$\n    where $s_{k} = x_{k+1} - x_{k}$ and $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$. A crucial requirement for this update is the curvature condition, $y_{k}^{\\top}s_{k} > 0$. This condition ensures that the updated matrix $B_{k+1}^{-1}$ remains positive definite if $B_{k}^{-1}$ was. If the condition is not met, the update is skipped, and $B_{k+1}^{-1}$ is set to $B_{k}^{-1}$.\n\n**Step Length Calculation**\n\nFor both methods, once a descent direction $p_k$ is found, the step length $\\alpha_k$ is determined by an Armijo backtracking line search. This search ensures sufficient decrease in the function value without taking excessively small steps. Starting with a full step $\\alpha_k = 1$, it is successively reduced by a factor $\\beta \\in (0, 1)$ until the Armijo condition is satisfied:\n$$\nf(x_{k}+\\alpha_{k}p_{k}) \\le f(x_{k}) + c_{1}\\alpha_{k}\\nabla f(x_{k})^{\\top}p_{k}\n$$\nThe parameter $c_{1}$ (e.g., $10^{-4}$) controls how much decrease is deemed sufficient. This prevents the algorithm from taking infinitesimally small steps that yield negligible progress. A lower bound on $\\alpha_k$ is imposed to prevent the line search from running indefinitely.\n\n**Termination Criteria**\n\nThe choice of when to stop the iterative process is critical. The problem investigates two common criteria:\n\n1.  **Criterion G: Gradient-norm based.** The algorithm terminates when $\\lVert \\nabla f(x_{k})\\rVert_{\\infty}  \\varepsilon$. This is theoretically sound, as a necessary condition for a point $x^{\\star}$ to be a local minimizer is $\\nabla f(x^{\\star}) = 0$. A small gradient norm indicates that the iterate is close to a stationary point.\n\n2.  **Criterion S: Step-size based.** The algorithm terminates when $\\lVert x_{k}-x_{k-1}\\rVert_{2}  \\varepsilon$. This criterion is often used in practice because it seems intuitive that steps should become small as we approach the solution. However, it can be unreliable. An algorithm might take very small steps far from the solution if it is struggling (e.g., in a narrow valley), leading to premature termination. Conversely, in regions of high curvature, even small changes in position can correspond to large changes in the gradient. This problem's design allows for a direct comparison of these two criteria on a challenging function. A maximum number of iterations, $N_{\\max} = 2000$, serves as a fail-safe against non-convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problem for all test cases and print the results.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (method, criterion, x0, epsilon)\n        ('H', 'G', np.array([-1.2, 1.0]), 1e-8),\n        ('H', 'S', np.array([-1.2, 1.0]), 1e-4),\n        ('Q', 'G', np.array([-1.2, 1.0]), 1e-8),\n        ('Q', 'S', np.array([-1.2, 1.0]), 1e-4),\n        ('H', 'G', np.array([1.0, 1.0]), 1e-12),\n        ('Q', 'S', np.array([1.0, 1.0]), 1e-12),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        method, criterion, x0, epsilon = case\n        method_id = 0 if method == 'H' else 1\n        criterion_id = 0 if criterion == 'G' else 1\n        \n        iterations, final_f, final_grad_norm, final_step_norm = run_optimizer(\n            method, criterion, x0, epsilon\n        )\n        \n        result = [\n            method_id, \n            criterion_id, \n            iterations, \n            final_f, \n            final_grad_norm, \n            final_step_norm\n        ]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\n\ndef rosenbrock_f(x):\n    \"\"\"Rosenbrock function.\"\"\"\n    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n\ndef rosenbrock_grad(x):\n    \"\"\"Gradient of the Rosenbrock function.\"\"\"\n    g1 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n    g2 = 200 * (x[1] - x[0]**2)\n    return np.array([g1, g2])\n\ndef rosenbrock_hess(x):\n    \"\"\"Hessian of the Rosenbrock function.\"\"\"\n    h11 = 1200 * x[0]**2 - 400 * x[1] + 2\n    h12 = -400 * x[0]\n    h22 = 200\n    return np.array([[h11, h12], [h12, h22]])\n\n\ndef run_optimizer(method, criterion, x0, epsilon):\n    \"\"\"\n    Runs the optimization loop for a given configuration.\n    \"\"\"\n    \n    # Parameters\n    N_max = 2000\n    c1 = 1e-4\n    beta = 0.5\n    delta = 1e-6\n    alpha_min = 1e-16\n\n    x_k = np.copy(x0).astype(float)\n    x_prev = np.copy(x_k)\n    \n    # Initialize BFGS inverse Hessian approximation\n    if method == 'Q':\n        B_inv_k = np.identity(2)\n    \n    k = 0\n    while k  N_max:\n        grad_k = rosenbrock_grad(x_k)\n        \n        # --- Check termination criteria ---\n        if criterion == 'G':\n            if np.linalg.norm(grad_k, ord=np.inf)  epsilon:\n                break\n        elif criterion == 'S' and k > 0:\n            if np.linalg.norm(x_k - x_prev, ord=2)  epsilon:\n                break\n        \n        # --- Compute search direction p_k ---\n        if method == 'H':\n            H_k = rosenbrock_hess(x_k)\n            try:\n                # Eigenvalue modification to ensure positive definiteness\n                min_eig = np.min(np.linalg.eigvalsh(H_k))\n                tau = max(0, delta - min_eig)\n                B_k = H_k + tau * np.identity(2)\n                p_k = np.linalg.solve(B_k, -grad_k)\n                # Fallback to steepest descent if not a descent direction\n                if grad_k.T @ p_k >= 0:\n                    p_k = -grad_k\n            except np.linalg.LinAlgError:\n                # Fallback in case of singular matrix\n                p_k = -grad_k\n        \n        elif method == 'Q':\n            p_k = -B_inv_k @ grad_k\n\n        # --- Line search (Armijo backtracking) ---\n        alpha = 1.0\n        f_k = rosenbrock_f(x_k)\n        grad_dot_p = grad_k.T @ p_k\n        \n        while alpha >= alpha_min:\n            x_candidate = x_k + alpha * p_k\n            f_candidate = rosenbrock_f(x_candidate)\n            if f_candidate = f_k + c1 * alpha * grad_dot_p:\n                break\n            alpha *= beta\n        else: # loop exhausted without break\n            alpha = alpha_min\n\n        x_prev = np.copy(x_k)\n        x_k = x_k + alpha * p_k\n        \n        # --- BFGS update for inverse Hessian (Method Q) ---\n        if method == 'Q':\n            s_k = x_k - x_prev\n            # Compute gradient at new point\n            grad_k_plus_1 = rosenbrock_grad(x_k)\n            y_k = grad_k_plus_1 - grad_k\n            \n            y_dot_s = y_k.T @ s_k\n            if y_dot_s > 0:\n                rho_k = 1.0 / y_dot_s\n                I = np.identity(2)\n                V = I - rho_k * np.outer(s_k, y_k)\n                B_inv_k = V @ B_inv_k @ V.T + rho_k * np.outer(s_k, s_k)\n\n        k += 1\n\n    # --- Collect final metrics ---\n    final_f = rosenbrock_f(x_k)\n    final_grad_norm = np.linalg.norm(rosenbrock_grad(x_k), ord=np.inf)\n    if k == 0:\n        final_step_norm = 0.0\n    else:\n        final_step_norm = np.linalg.norm(x_k - x_prev, ord=2)\n\n    return k, final_f, final_grad_norm, final_step_norm\n\nsolve()\n```"
        },
        {
            "introduction": "As a powerful alternative to the line-search paradigm, trust-region methods define a region around the current iterate where the quadratic model of the function is considered reliable and then solve for the optimal step within that region. This practice focuses on the heart of the trust-region approach: solving the quadratic subproblem. You will implement the Steihaug-Toint truncated conjugate gradient method, an algorithm designed to efficiently find an approximate solution while gracefully handling the indefinite Hessians that we identified as a key challenge. This provides a crucial contrast to line-search methods and a deeper understanding of modern optimization toolkits.",
            "id": "2417374",
            "problem": "Consider the trust-region quadratic subproblem in $\\mathbb{R}^n$: minimize the quadratic model $m(\\mathbf{p}) = \\mathbf{g}^\\top \\mathbf{p} + \\tfrac{1}{2}\\mathbf{p}^\\top \\mathbf{B}\\mathbf{p}$ subject to the Euclidean norm constraint $\\|\\mathbf{p}\\|_2 \\le \\Delta$, where $\\mathbf{B}\\in\\mathbb{R}^{n\\times n}$ is symmetric, $\\mathbf{g}\\in\\mathbb{R}^n$ is given, and $\\Delta0$ is the trust-region radius. The matrices $\\mathbf{B}$ may be positive definite, positive semidefinite, or indefinite.\n\nWrite a complete program that, for each test case listed below, returns a step vector $\\mathbf{p}$ determined by the following mathematically specified truncation rules applied to the quadratic model $m$: start with $\\mathbf{p}_0=\\mathbf{0}$, residual $\\mathbf{r}_0=\\mathbf{g}$, and search direction $\\mathbf{d}_0=-\\mathbf{r}_0$. For iteration index $k\\in\\{0,1,2,\\dots\\}$, define the curvature $\\kappa_k=\\mathbf{d}_k^\\top \\mathbf{B}\\mathbf{d}_k$. If $\\kappa_k\\le 0$, return the boundary point $\\mathbf{p}_\\tau=\\mathbf{p}_k+\\tau\\mathbf{d}_k$ where $\\tau0$ is chosen so that $\\|\\mathbf{p}_k+\\tau\\mathbf{d}_k\\|_2=\\Delta$. Otherwise, define the step length $\\alpha_k=(\\mathbf{r}_k^\\top \\mathbf{r}_k)/\\kappa_k$ and the trial point $\\mathbf{p}_{k+1}=\\mathbf{p}_k+\\alpha_k\\mathbf{d}_k$. If $\\|\\mathbf{p}_{k+1}\\|_2\\ge \\Delta$, return the boundary point $\\mathbf{p}_\\tau=\\mathbf{p}_k+\\tau\\mathbf{d}_k$ where $\\tau\\in(0,\\alpha_k]$ satisfies $\\|\\mathbf{p}_k+\\tau\\mathbf{d}_k\\|_2=\\Delta$. Otherwise, update $\\mathbf{r}_{k+1}=\\mathbf{r}_k+\\alpha_k\\mathbf{B}\\mathbf{d}_k$. If $\\|\\mathbf{r}_{k+1}\\|_2$ is sufficiently small relative to $\\|\\mathbf{g}\\|_2$, return $\\mathbf{p}_{k+1}$. Otherwise, set $\\beta_k=(\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1})/(\\mathbf{r}_k^\\top \\mathbf{r}_k)$ and $\\mathbf{d}_{k+1}=-\\mathbf{r}_{k+1}+\\beta_k\\mathbf{d}_k$ and continue. This process is well-defined by the stated equations and the boundary intersection condition $\\|\\mathbf{p}_k+\\tau\\mathbf{d}_k\\|_2=\\Delta$, which is the positive root of the quadratic equation in the scalar $\\tau$.\n\nUse only the given data in each test case. No external input is allowed. Angles are not involved. There are no physical units in this problem, so answers should be pure real numbers.\n\nTest suite (each test case is a triple $(\\mathbf{B},\\mathbf{g},\\Delta)$):\n- Case $1$ (interior solution with positive definite curvature): $\\mathbf{B}=\\begin{bmatrix}2  0\\\\ 0  4\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}1\\\\ 2\\end{bmatrix}$, $\\Delta=5$.\n- Case $2$ (boundary solution with positive definite curvature): $\\mathbf{B}=\\begin{bmatrix}2  0\\\\ 0  4\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}1\\\\ 2\\end{bmatrix}$, $\\Delta=0.2$.\n- Case $3$ (indefinite matrix with immediate negative curvature): $\\mathbf{B}=\\begin{bmatrix}1  0\\\\ 0  -1\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}0\\\\ 1\\end{bmatrix}$, $\\Delta=0.5$.\n- Case $4$ (zero gradient edge case): $\\mathbf{B}=\\begin{bmatrix}1  2\\\\ 2  1\\end{bmatrix}$, $\\mathbf{g}=\\begin{bmatrix}0\\\\ 0\\end{bmatrix}$, $\\Delta=1$.\n\nFor each case, compute the step vector $\\mathbf{p}\\in\\mathbb{R}^2$ determined by the stated truncation rules.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the step vector for one test case, itself written as a comma-separated list enclosed in square brackets. For example, a valid format for four two-dimensional results is $[[p_{1,1},p_{1,2}],[p_{2,1},p_{2,2}],[p_{3,1},p_{3,2}],[p_{4,1},p_{4,2}]]$ with each $p_{i,j}$ printed as a decimal real number.",
            "solution": "This problem addresses a standard trust-region subproblem in numerical optimization. We must find a step vector $\\mathbf{p} \\in \\mathbb{R}^n$ that minimizes the quadratic model $m(\\mathbf{p}) = \\mathbf{g}^\\top \\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\top \\mathbf{B}\\mathbf{p}$ subject to the constraint $\\|\\mathbf{p}\\|_2 \\le \\Delta$, where $\\mathbf{g} \\in \\mathbb{R}^n$ is the gradient, $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ is a symmetric matrix approximating the Hessian, and $\\Delta > 0$ is the trust-region radius.\n\nThe problem statement specifies a particular algorithm to find an approximate solution. This algorithm is a truncated form of the conjugate gradient (CG) method, often known as the Steihaug-Toint method. It is designed to handle the case where $\\mathbf{B}$ is not positive definite, which is a key requirement for trust-region methods.\n\nInitial state (iteration $k=0$):\nThe starting point is the origin, $\\mathbf{p}_0 = \\mathbf{0}$. The initial residual is the gradient, $\\mathbf{r}_0 = \\mathbf{g}$, and the initial search direction is the steepest descent direction, $\\mathbf{d}_0 = -\\mathbf{r}_0 = -\\mathbf{g}$.\n\nIterative procedure for $k = 0, 1, 2, \\dots$:\nThe algorithm proceeds iteratively, building a sequence of points $\\mathbf{p}_k$ that approximate the minimizer of $m(\\mathbf{p})$, but with specific checks to ensure the solution remains within the trust region and to handle non-positive curvature.\n\n$1$. Curvature Check: At each iteration $k$, we compute the curvature along the search direction $\\mathbf{d}_k$: $\\kappa_k = \\mathbf{d}_k^\\top \\mathbf{B} \\mathbf{d}_k$. If $\\kappa_k \\le 0$, the quadratic model $m(\\mathbf{p})$ is non-convex along $\\mathbf{d}_k$. Following the path $\\mathbf{p}_k + \\tau\\mathbf{d}_k$ for increasing $\\tau > 0$ would lead to an unbounded decrease in $m(\\mathbf{p})$. The algorithm's rule is to stop and move as far as possible in this direction until the trust-region boundary is hit. This requires finding $\\tau > 0$ such that $\\|\\mathbf{p}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$. The solution is then $\\mathbf{p} = \\mathbf{p}_k + \\tau \\mathbf{d}_k$.\n\n$2$. Boundary Check: If the curvature $\\kappa_k$ is positive, the standard CG step length is computed: $\\alpha_k = (\\mathbf{r}_k^\\top \\mathbf{r}_k) / \\kappa_k$. This $\\alpha_k$ minimizes the quadratic model along the line $\\mathbf{p}_k + \\alpha\\mathbf{d}_k$. A trial point is calculated: $\\mathbf{p}_{k+1} = \\mathbf{p}_k + \\alpha_k \\mathbf{d}_k$. We must then check if this point has left the trust region. If $\\|\\mathbf{p}_{k+1}\\|_2 \\ge \\Delta$, the path from $\\mathbf{p}_k$ has crossed the boundary. The algorithm terminates and returns the intersection point $\\mathbf{p} = \\mathbf{p}_k + \\tau \\mathbf{d}_k$, where $\\tau \\in (0, \\alpha_k]$ is chosen such that $\\|\\mathbf{p}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$.\n\n$3$. Convergence and Update: If the trial point $\\mathbf{p}_{k+1}$ is inside the trust region, we update the residual using the identity $\\mathbf{r}_{k+1} = \\mathbf{r}_k + \\alpha_k \\mathbf{B} \\mathbf{d}_k$. If the norm of this new residual, $\\|\\mathbf{r}_{k+1}\\|_2$, is sufficiently small (e.g., relative to the initial residual norm $\\|\\mathbf{g}\\|_2$), the algorithm has converged, and the solution is $\\mathbf{p} = \\mathbf{p}_{k+1}$. If not converged, a new search direction is constructed using the Fletcher-Reeves formula: $\\beta_k = (\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}) / (\\mathbf{r}_k^\\top \\mathbf{r}_k)$, and $\\mathbf{d}_{k+1} = -\\mathbf{r}_{k+1} + \\beta_k \\mathbf{d}_k$. The process repeats for the next iteration.\n\nSolving for the boundary intersection point $\\tau$:\nThe condition $\\|\\mathbf{p}_k + \\tau \\mathbf{d}_k\\|_2 = \\Delta$ translates to a quadratic equation in $\\tau$:\n$$(\\mathbf{d}_k^\\top \\mathbf{d}_k) \\tau^2 + (2 \\mathbf{p}_k^\\top \\mathbf{d}_k) \\tau + (\\mathbf{p}_k^\\top \\mathbf{p}_k - \\Delta^2) = 0$$\nLet $a = \\mathbf{d}_k^\\top \\mathbf{d}_k$, $b = 2 \\mathbf{p}_k^\\top \\mathbf{d}_k$, and $c = \\mathbf{p}_k^\\top \\mathbf{p}_k - \\Delta^2$. Since the current point $\\mathbf{p}_k$ is always strictly inside the trust region when this calculation is performed, we have $\\|\\mathbf{p}_k\\|_2  \\Delta$, which implies $c  0$. As $a = \\|\\mathbf{d}_k\\|_2^2 > 0$ (unless $\\mathbf{d}_k = \\mathbf{0}$, a trivial case), the product $ac$ is negative. This guarantees the discriminant $b^2 - 4ac$ is positive, yielding two distinct real roots for $\\tau$. One root is positive and one is negative. The problem requires $\\tau > 0$, so we must select the positive root:\n$$ \\tau = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} $$\n\nThe provided test cases cover the main scenarios:\n-   Case $1$: An interior solution is found, as the unconstrained minimizer of $m(\\mathbf{p})$ lies within the trust region. The algorithm terminates after $n=2$ iterations with a near-zero residual.\n-   Case $2$: The first CG step attempts to move towards the unconstrained minimizer, but because the trust radius $\\Delta$ is small, the step leaves the trust region. The algorithm terminates by finding the intersection with the boundary.\n-   Case $3$: The matrix $\\mathbf{B}$ is indefinite, and the initial search direction proves to be a direction of negative curvature ($\\kappa_0  0$). The algorithm immediately terminates by finding the intersection with the boundary along this direction.\n-   Case $4$: The gradient is zero, $\\mathbf{g} = \\mathbf{0}$. Thus, the initial residual $\\mathbf{r}_0$ and search direction $\\mathbf{d}_0$ are also $\\mathbf{0}$. The curvature $\\kappa_0$ is $0$. The algorithm terminates in the first step, returning the starting point $\\mathbf{p} = \\mathbf{0}$.",
            "answer": "```python\nimport numpy as np\n\ndef find_tau(p, d, Delta):\n    \"\"\"\n    Solves for tau > 0 such that ||p + tau*d||_2 = Delta.\n    This is a quadratic equation in tau: a*tau^2 + b*tau + c = 0.\n    \"\"\"\n    a = np.dot(d, d)\n    b = 2 * np.dot(p, d)\n    c = np.dot(p, p) - Delta**2\n    \n    # Since p is inside the trust region, ||p||  Delta, so c  0.\n    # Also a = ||d||^2 > 0.\n    # Therefore, the discriminant b^2 - 4ac is always positive.\n    discriminant = b**2 - 4 * a * c\n    \n    # The quadratic equation has one positive and one negative root. We need the positive one.\n    # The formula (-b + sqrt(discriminant))/(2a) gives the positive root.\n    tau = (-b + np.sqrt(discriminant)) / (2 * a)\n    return tau\n\ndef solve_truncated_cg(B, g, Delta, tol=1e-12, max_iter=None):\n    \"\"\"\n    Implements the truncated conjugate gradient method for the trust-region subproblem\n    as specified in the problem description.\n    \"\"\"\n    n = len(g)\n    if max_iter is None:\n        max_iter = 2 * n  # A safe upper bound on iterations\n\n    p = np.zeros(n)\n    r = g.copy()\n    d = -r\n\n    g_norm = np.linalg.norm(g)\n    if g_norm == 0:\n        # If g=0, then r0=0, d0=0, kappa0=0. Algorithm terminates with p=0.\n        return p\n\n    r_sq_norm = np.dot(r, r)\n\n    for k in range(max_iter):\n        Bd = B @ d\n        kappa = np.dot(d, Bd)\n\n        # Termination Condition 1: Non-positive curvature\n        if kappa = 0:\n            tau = find_tau(p, d, Delta)\n            return p + tau * d\n\n        alpha = r_sq_norm / kappa\n        p_next = p + alpha * d\n\n        # Termination Condition 2: Exceeds trust region\n        if np.linalg.norm(p_next) >= Delta:\n            tau = find_tau(p, d, Delta)\n            return p + tau * d\n\n        # Update step\n        p = p_next\n        r_next = r + alpha * Bd\n\n        # Termination Condition 3: Convergence\n        if np.linalg.norm(r_next)  tol * g_norm:\n            return p\n\n        # Prepare for next iteration\n        r_sq_norm_next = np.dot(r_next, r_next)\n        beta = r_sq_norm_next / r_sq_norm\n        \n        r = r_next\n        r_sq_norm = r_sq_norm_next\n        \n        d = -r + beta * d\n        \n    return p # Should not be reached for small n, but good practice\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, solve them, and print the output\n    in the specified format.\n    \"\"\"\n    test_cases = [\n        (np.array([[2, 0], [0, 4]]), np.array([1, 2]), 5.0),\n        (np.array([[2, 0], [0, 4]]), np.array([1, 2]), 0.2),\n        (np.array([[1, 0], [0, -1]]), np.array([0, 1]), 0.5),\n        (np.array([[1, 2], [2, 1]]), np.array([0, 0]), 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        B, g, Delta = case\n        p_sol = solve_truncated_cg(B, g, Delta)\n        results.append(p_sol.tolist())\n\n    # Format the final output string precisely as required:\n    # [[p1_1,p1_2],[p2_1,p2_2],...] with no extra spaces.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}