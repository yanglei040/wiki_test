{
    "hands_on_practices": [
        {
            "introduction": "The performance of the steepest descent algorithm is not uniform across all problems; its efficiency is deeply tied to the geometry of the objective function. This first exercise allows you to numerically verify two fundamental properties: its extreme sensitivity to the scaling of variables and its invariance under rotations of the coordinate system. Understanding these characteristics is the first step toward diagnosing why the algorithm might be slow and how techniques like preconditioning can offer a remedy .",
            "id": "2448741",
            "problem": "Write a complete, runnable program that implements steepest descent (also called gradient descent in the Euclidean norm) with exact line search for unconstrained multivariate optimization of quadratic functions and uses it to numerically demonstrate the following two facts: (i) steepest descent is invariant under rotations of the coordinate system, and (ii) the performance of steepest descent is highly dependent on scaling of the variables. The only foundational facts you may assume are the definition of the gradient as the direction of steepest ascent in the Euclidean norm, the definition of steepest descent as moving along the negative gradient direction, and the definition of exact line search as choosing the step length that minimizes the objective function along the ray defined by the current point and descent direction. You must derive any other formulas you need inside your solution.\n\nImplement steepest descent for quadratic objectives of the form\n$$\nf(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top A \\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x} + c,\n$$\nwhere $A$ is symmetric positive definite (SPD), $\\mathbf{b}$ is a vector, and $c$ is a scalar. For exact line search, at iteration $k$ with current point $\\mathbf{x}_k$ and gradient $\\nabla f(\\mathbf{x}_k)$, define the univariate function along the ray $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\,\\nabla f(\\mathbf{x}_k))$ and choose $\\alpha_k$ to minimize $\\phi(\\alpha)$.\n\nYour program must run the following three test cases and aggregate their results into a single-line output. All angles must be in radians.\n\nTest case 1 (rotation invariance check):\n- Dimension $n=2$. Use $A_1 = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$ and $\\mathbf{b}_1=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$. Let the rotation angle be $\\theta = \\pi/6$ (that is, $30$ degrees), and let the rotation matrix be $R=\\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta\\end{bmatrix}$. Define the rotated quadratic by $f_R(\\mathbf{y}) = f(R\\,\\mathbf{y})$, equivalently $A_R = R^\\top A_1 R$, $\\mathbf{b}_R = R^\\top \\mathbf{b}_1$.\n- Start from $\\mathbf{x}_0=\\begin{bmatrix}2\\\\-1\\end{bmatrix}$ in the original coordinates and from $\\mathbf{y}_0=R^\\top \\mathbf{x}_0$ in the rotated coordinates.\n- Run exactly $K=20$ steepest descent iterations with exact line search in both coordinate systems (do not stop early by tolerance).\n- Define the boolean result $T_1$ to be true if, for every iteration index $k\\in\\{0,1,\\dots,K\\}$, the corresponding iterates satisfy $\\|\\mathbf{y}_k - R^\\top \\mathbf{x}_k\\|_\\infty \\le 10^{-10}$, and false otherwise. Here $\\|\\cdot\\|_\\infty$ denotes the infinity norm.\n\nTest case 2 (scaling sensitivity of performance):\n- Dimension $n=2$. Use $A_2=\\operatorname{diag}(1,100)$ and $\\mathbf{b}_2=\\begin{bmatrix}0\\\\0\\end{bmatrix}$.\n- Consider the diagonal scaling matrix $S=\\operatorname{diag}(1,10)$ and the scaled coordinates $\\mathbf{y} = S \\mathbf{x}$, which induces the scaled quadratic $f_S(\\mathbf{y}) = f(S^{-1}\\mathbf{y})$. In quadratic form, this corresponds to $A_S=S^{-\\top} A_2 S^{-1}$ and $\\mathbf{b}_S=S^{-\\top}\\mathbf{b}_2$.\n- Use the same physical starting point represented in both coordinates: $\\mathbf{x}_0=\\begin{bmatrix}1\\\\1\\end{bmatrix}$ and $\\mathbf{y}_0=S\\,\\mathbf{x}_0$.\n- Run steepest descent with exact line search in each coordinate system until the Euclidean norm of the gradient is at most $10^{-8}$, with a maximum of $10^5$ iterations to prevent infinite loops. Record the number of iterations $\\text{iters}_\\text{unscaled}$ for the unscaled run on $(A_2,\\mathbf{b}_2)$ and $\\text{iters}_\\text{scaled}$ for the scaled run on $(A_S,\\mathbf{b}_S)$.\n- Define the test output as the floating-point ratio $r=\\text{iters}_\\text{unscaled}/\\text{iters}_\\text{scaled}$.\n\nTest case 3 (isotropic edge case):\n- Dimension $n=2$. Use $A_3=5 I_2$ and $\\mathbf{b}_3=\\begin{bmatrix}0\\\\0\\end{bmatrix}$, starting from $\\mathbf{x}_0=\\begin{bmatrix}3\\\\-4\\end{bmatrix}$.\n- Run steepest descent with exact line search until the Euclidean norm of the gradient is at most $10^{-12}$, with a maximum of $10^5$ iterations. Record the number of iterations $N_3$ required to meet the tolerance.\n\nProgram output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the order $[T_1, r, N_3]$. Here $T_1$ is a boolean literal, $r$ is a floating-point number, and $N_3$ is an integer. For example, an output could look like $[True,123.0,1]$.",
            "solution": "The problem requires the implementation of the steepest descent algorithm with exact line search for unconstrained optimization of quadratic functions. This implementation will be used to numerically demonstrate the algorithm's invariance to coordinate system rotations and its sensitivity to variable scaling. The entire process, from first principles to final implementation, is detailed below.\n\nA general quadratic objective function is given by\n$$\nf(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top A \\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x} + c\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix, $\\mathbf{b} \\in \\mathbb{R}^n$ is a vector, and $c \\in \\mathbb{R}$ is a scalar constant. The SPD property of $A$ ensures that $f(\\mathbf{x})$ is strictly convex and possesses a unique global minimum.\n\nThe steepest descent method is an iterative algorithm that, starting from an initial guess $\\mathbf{x}_0$, generates a sequence of points $\\{\\mathbf{x}_k\\}$ that converge to the minimizer of $f(\\mathbf{x})$. At each iteration $k$, the method proceeds in the direction of the negative gradient, which is the direction of steepest descent in the Euclidean norm. The update rule is:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)\n$$\nwhere $\\alpha_k > 0$ is the step length. The problem specifies an exact line search, meaning $\\alpha_k$ is chosen to minimize the objective function along the search direction.\n\nTo implement this algorithm, we must derive two key formulas: the gradient of $f(\\mathbf{x})$ and the optimal step length $\\alpha_k$.\n\nFirst, we derive the gradient of the quadratic function, $\\nabla f(\\mathbf{x})$. Using elementary matrix calculus, or by differentiating with respect to each component $x_i$, we analyze the terms of $f(\\mathbf{x})$:\n$f(\\mathbf{x}) = \\frac{1}{2} \\sum_{i,j} A_{ij} x_i x_j - \\sum_i b_i x_i + c$.\nThe partial derivative with respect to $x_k$ is:\n$$\n\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2} \\sum_{i,j} A_{ij} x_i x_j \\right) - \\frac{\\partial}{\\partial x_k} \\left( \\sum_i b_i x_i \\right) + \\frac{\\partial c}{\\partial x_k}\n$$\nThe derivative of the constant $c$ is $0$. The derivative of the linear term is $\\frac{\\partial}{\\partial x_k} (\\sum_i b_i x_i) = b_k$. For the quadratic term, using the product rule and the symmetry of $A$ ($A_{ij}=A_{ji}$):\n$$\n\\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2} \\sum_{i,j} A_{ij} x_i x_j \\right) = \\frac{1}{2} \\sum_{i,j} A_{ij} (\\delta_{ik}x_j + x_i\\delta_{jk}) = \\frac{1}{2} \\left( \\sum_j A_{kj} x_j + \\sum_i A_{ik} x_i \\right) = \\frac{1}{2} \\left( [A\\mathbf{x}]_k + [A^\\top\\mathbf{x}]_k \\right) = [A\\mathbf{x}]_k\n$$\nwhere $[ \\cdot ]_k$ denotes the $k$-th component. Assembling the components into a vector gives the gradient:\n$$\n\\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}\n$$\n\nNext, we derive the formula for the optimal step length $\\alpha_k$ for exact line search. We define a univariate function $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)$, where $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ is the gradient at the current iterate $\\mathbf{x}_k$. We seek $\\alpha > 0$ that minimizes $\\phi(\\alpha)$. We find this by setting its derivative to zero: $\\frac{d\\phi}{d\\alpha} = 0$.\nSubstituting $\\mathbf{x} = \\mathbf{x}_k - \\alpha \\mathbf{g}_k$ into the expression for $f(\\mathbf{x})$:\n$$\n\\phi(\\alpha) = \\tfrac{1}{2}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)^\\top A (\\mathbf{x}_k - \\alpha \\mathbf{g}_k) - \\mathbf{b}^\\top(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) + c\n$$\nExpanding this expression and grouping terms by powers of $\\alpha$:\n$$\n\\phi(\\alpha) = \\left( \\tfrac{1}{2}\\mathbf{x}_k^\\top A \\mathbf{x}_k - \\mathbf{b}^\\top \\mathbf{x}_k + c \\right) - \\alpha(\\mathbf{g}_k^\\top A \\mathbf{x}_k - \\mathbf{g}_k^\\top \\mathbf{b}) + \\tfrac{1}{2}\\alpha^2(\\mathbf{g}_k^\\top A \\mathbf{g}_k)\n$$\nThe first term is simply $f(\\mathbf{x}_k)$. The coefficient of the linear term in $\\alpha$ simplifies using $\\mathbf{g}_k = A\\mathbf{x}_k - \\mathbf{b}$, so $\\mathbf{g}_k^\\top A \\mathbf{x}_k - \\mathbf{g}_k^\\top \\mathbf{b} = \\mathbf{g}_k^\\top (A\\mathbf{x}_k - \\mathbf{b}) = \\mathbf{g}_k^\\top \\mathbf{g}_k$.\nThus, $\\phi(\\alpha)$ is a simple quadratic in $\\alpha$:\n$$\n\\phi(\\alpha) = f(\\mathbf{x}_k) - \\alpha(\\mathbf{g}_k^\\top \\mathbf{g}_k) + \\tfrac{1}{2}\\alpha^2(\\mathbf{g}_k^\\top A \\mathbf{g}_k)\n$$\nDifferentiating with respect to $\\alpha$ and setting the result to zero gives:\n$$\n\\frac{d\\phi}{d\\alpha} = -(\\mathbf{g}_k^\\top \\mathbf{g}_k) + \\alpha(\\mathbf{g}_k^\\top A \\mathbf{g}_k) = 0\n$$\nSolving for $\\alpha$ yields the optimal step length:\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^\\top \\mathbf{g}_k}{\\mathbf{g}_k^\\top A \\mathbf{g}_k}\n$$\nThe second derivative, $\\frac{d^2\\phi}{d\\alpha^2} = \\mathbf{g}_k^\\top A \\mathbf{g}_k$, is positive since $A$ is SPD and $\\mathbf{g}_k \\neq \\mathbf{0}$ (if not at the minimum), confirming this is a minimum.\n\nThe complete steepest descent algorithm for quadratic optimization is:\n1. Initialize $\\mathbf{x}_0$. For $k=0, 1, 2, \\dots$:\n2. Compute the gradient: $\\mathbf{g}_k = A\\mathbf{x}_k - \\mathbf{b}$.\n3. Check for convergence, e.g., if $\\|\\mathbf{g}_k\\|_2 \\le \\epsilon$.\n4. Compute the step length: $\\alpha_k = (\\mathbf{g}_k^\\top \\mathbf{g}_k) / (\\mathbf{g}_k^\\top A \\mathbf{g}_k)$.\n5. Update the position: $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k$.\n\nThe problem's test cases are designed to demonstrate key properties of this algorithm.\n\nTest Case 1 (Rotation Invariance): When the coordinate system is rotated by a matrix $R$ (where $R^\\top R = I$), a point $\\mathbf{x}$ becomes $\\mathbf{y} = R^\\top \\mathbf{x}$. The objective function transforms from $f(\\mathbf{x})$ to $f_R(\\mathbf{y}) = f(R\\mathbf{y})$. This leads to new quadratic parameters $A_R=R^\\top A R$ and $\\mathbf{b}_R=R^\\top \\mathbf{b}$. The gradient transforms covariantly: $\\mathbf{g}_k^y = R^\\top \\mathbf{g}_k^x$. Both the numerator $(\\mathbf{g}_k^y)^\\top \\mathbf{g}_k^y = (\\mathbf{g}_k^x)^\\top R R^\\top \\mathbf{g}_k^x = (\\mathbf{g}_k^x)^\\top \\mathbf{g}_k^x$ and the denominator $(\\mathbf{g}_k^y)^\\top A_R \\mathbf{g}_k^y = (\\mathbf{g}_k^x)^\\top R(R^\\top A R)R^\\top \\mathbf{g}_k^x = (\\mathbf{g}_k^x)^\\top A \\mathbf{g}_k^x$ of the step length formula are invariant under rotation. Thus, $\\alpha_k^y = \\alpha_k^x$. By induction, if $\\mathbf{y}_k = R^\\top \\mathbf{x}_k$, then $\\mathbf{y}_{k+1} = \\mathbf{y}_k - \\alpha_k^y \\mathbf{g}_k^y = R^\\top \\mathbf{x}_k - \\alpha_k^x R^\\top \\mathbf{g}_k^x = R^\\top(\\mathbf{x}_k - \\alpha_k^x \\mathbf{g}_k^x) = R^\\top \\mathbf{x}_{k+1}$. The sequence of iterates in the rotated system is simply the rotated sequence of original iterates. The test confirms this identity numerically.\n\nTest Case 2 (Scaling Sensitivity): The convergence rate of steepest descent is governed by the condition number $\\kappa(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$. For the unscaled problem, $A_2 = \\operatorname{diag}(1, 100)$, so $\\kappa(A_2) = 100/1 = 100$. This high condition number implies that the level sets of the quadratic are highly elongated ellipses, causing the steepest descent path to zigzag slowly towards the minimum. For the scaled problem, the change of variables $\\mathbf{y}=S\\mathbf{x}$ leads to a new matrix $A_S = S^{-\\top}A_2S^{-1} = \\operatorname{diag}(1, 1/10)\\operatorname{diag}(1, 100)\\operatorname{diag}(1, 1/10) = \\operatorname{diag}(1, 1) = I_2$. The new system is isotropic with $\\kappa(A_S) = 1/1 = 1$. For an isotropic quadratic, steepest descent converges in a single step, as the gradient points directly towards the minimum. This test case demonstrates a dramatic improvement in performance due to preconditioning (scaling).\n\nTest Case 3 (Isotropic Edge Case): This case uses $A_3 = 5I_2$, which is also perfectly conditioned with $\\kappa(A_3)=1$. As in the scaled problem of Test Case 2, the gradient at any point $\\mathbf{x}_0$ points directly to the minimum at $\\mathbf{x}^* = \\mathbf{0}$. The exact line search will choose $\\alpha_0$ to travel this entire distance, resulting in convergence in a single iteration. This will be verified numerically.\n\nThe implementation will follow these principles, encapsulating the algorithm in a reusable function and applying it to the three specified scenarios.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef steepest_descent(A, b, x0, tol=None, max_iter=None, fixed_iter=None):\n    \"\"\"\n    Implements steepest descent with exact line search for f(x) = 0.5*x.T*A*x - b.T*x.\n\n    The function can run for a fixed number of iterations or until a tolerance is met.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix A.\n        b (np.ndarray): The vector b.\n        x0 (np.ndarray): The initial point x0.\n        tol (float, optional): The tolerance for the Euclidean norm of the gradient.\n                               Used as a stopping criterion.\n        max_iter (int, optional): The maximum number of iterations. Used with `tol`.\n        fixed_iter (int, optional): If provided, run for exactly this many iterations\n                                    and return all iterates.\n\n    Returns:\n        If `fixed_iter` is provided:\n            list[np.ndarray]: A list containing all iterates from x0 to x_K.\n        If `tol` and `max_iter` are provided:\n            tuple[np.ndarray, int]: The final point and the number of iterations taken.\n    \"\"\"\n    x = x0.astype(np.float64)\n\n    if fixed_iter is not None:\n        iterates = [x.copy()]\n        for _ in range(fixed_iter):\n            # Compute gradient g = Ax - b\n            g = A @ x - b\n            \n            # Numerator of alpha: g.T * g\n            gg = g.T @ g\n            \n            # If gradient is effectively zero, an optimum is reached.\n            # Avoid division by zero and stay at the current point.\n            if gg < 1e-30:\n                x = x\n            else:\n                # Denominator of alpha: g.T * A * g\n                gAg = g.T @ A @ g\n                alpha = gg / gAg\n                \n                # Update x: x_{k+1} = x_k - alpha * g_k\n                x = x - alpha * g\n            \n            iterates.append(x.copy())\n        return iterates\n    else:\n        for k in range(max_iter):\n            g = A @ x - b\n            grad_norm = np.linalg.norm(g)\n            \n            if grad_norm <= tol:\n                return x, k\n            \n            gg = g.T @ g\n            \n            # This check is theoretically redundant if tol > 0, but good practice.\n            if gg < 1e-30:\n                return x, k\n                \n            gAg = g.T @ A @ g\n            alpha = gg / gAg\n            x = x - alpha * g\n        \n        # Return last state if max_iter is reached before tolerance\n        return x, max_iter\n\ndef solve():\n    \"\"\"\n    Runs the three test cases and prints the results in the required format.\n    \"\"\"\n    results = []\n    \n    # --- Test Case 1: Rotation Invariance ---\n    A1 = np.array([[3.0, 1.0], [1.0, 2.0]])\n    b1 = np.array([0.0, 0.0])\n    x0_1 = np.array([2.0, -1.0])\n    theta = np.pi / 6.0\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array([[c, -s], [s, c]])\n    \n    A_R = R.T @ A1 @ R\n    b_R = R.T @ b1\n    y0_1 = R.T @ x0_1\n    \n    K = 20\n    x_iterates = steepest_descent(A=A1, b=b1, x0=x0_1, fixed_iter=K)\n    y_iterates = steepest_descent(A=A_R, b=b_R, x0=y0_1, fixed_iter=K)\n    \n    t1_check = True\n    for k in range(K + 1):\n        # Check if ||y_k - R.T * x_k||_inf <= 1e-10\n        diff = y_iterates[k] - R.T @ x_iterates[k]\n        inf_norm = np.linalg.norm(diff, ord=np.inf)\n        if inf_norm > 1e-10:\n            t1_check = False\n            break\n    results.append(t1_check)\n    \n    # --- Test Case 2: Scaling Sensitivity ---\n    A2 = np.array([[1.0, 0.0], [0.0, 100.0]])\n    b2 = np.array([0.0, 0.0])\n    x0_2 = np.array([1.0, 1.0])\n    \n    S = np.array([[1.0, 0.0], [0.0, 10.0]])\n    S_inv = np.linalg.inv(S)\n    \n    A_S = S_inv.T @ A2 @ S_inv\n    b_S = S_inv.T @ b2\n    y0_2 = S @ x0_2\n    \n    tol_2 = 1e-8\n    max_iter_2 = 100000\n    \n    _, iters_unscaled = steepest_descent(A=A2, b=b2, x0=x0_2, tol=tol_2, max_iter=max_iter_2)\n    _, iters_scaled = steepest_descent(A=A_S, b=b_S, x0=y0_2, tol=tol_2, max_iter=max_iter_2)\n    \n    r = float(iters_unscaled) / float(iters_scaled) if iters_scaled > 0 else float('inf')\n    results.append(r)\n    \n    # --- Test Case 3: Isotropic Edge Case ---\n    A3 = np.array([[5.0, 0.0], [0.0, 5.0]]) # 5 * I_2\n    b3 = np.array([0.0, 0.0])\n    x0_3 = np.array([3.0, -4.0])\n    \n    tol_3 = 1e-12\n    max_iter_3 = 100000\n    \n    _, n3 = steepest_descent(A=A3, b=b3, x0=x0_3, tol=tol_3, max_iter=max_iter_3)\n    results.append(n3)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{results[0]},{results[1]},{results[2]}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Beyond the issue of poor scaling, steepest descent can also be thwarted by the local topography of the function, particularly near saddle points. This exercise explores this critical failure mode by applying the algorithm to the simple saddle function $f(x,y) = x^2 - y^2$, where the gradient is small but the point is not a true minimum. By analytically deriving the algorithm's behavior, you will gain a precise understanding of why first-order methods can \"get stuck\" and struggle to escape such regions .",
            "id": "2448659",
            "problem": "Write a complete and runnable program that analyzes the behavior of the steepest descent iteration applied to the unconstrained multivariate function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{2}-y^{2}$. Consider the iterative mapping that starts from an initial point $(x_{0},y_{0})\\in\\mathbb{R}^{2}$ and generates a sequence $(x_{k},y_{k})$ by moving from $(x_{k},y_{k})$ along the ray in the direction of the negative gradient $-\\nabla f(x_{k},y_{k})$ to the point on that ray which minimizes $f$ along that ray, if such a point exists. If the infimum of $f$ along that ray is not attained at a finite step length, declare that the line minimization is unbounded below along that direction at that iterate and terminate. If $\\|\\nabla f(x_{k},y_{k})\\|_{2}$ is exactly zero at any iterate, declare that a stationary point has been reached and terminate.\n\nYour program must, for each specified initial point, generate this sequence subject to a maximum of $N$ iterations with $N=50$, using exact arithmetic formulas implied by the definition above wherever applicable, and must classify the outcome using the following integer code:\n- Code $0$: the starting point is already stationary (i.e., $\\|\\nabla f(x_{0},y_{0})\\|_{2}=0$), and the sequence does not move.\n- Code $1$: a stationary point is reached in a finite number of iterations $k\\ge 1$ (i.e., $\\|\\nabla f(x_{k},y_{k})\\|_{2}=0$) before reaching the iteration cap, and the process terminates there.\n- Code $2$: at some iterate before reaching stationarity, the line minimization along the steepest descent direction is unbounded below (i.e., $f$ decreases without bound as the step length increases along that direction), and the process terminates because no finite minimizer exists along that line.\n\nFor each test case, return a list containing the classification code, the number of iterations actually performed (an integer in $\\{0,1,\\dots,N\\}$), and the final point coordinates as real numbers $(x_{\\text{final}},y_{\\text{final}})$ at termination.\n\nUse the following test suite of initial points:\n- Test $1$: $(x_{0},y_{0})=(1.0,0.0)$.\n- Test $2$: $(x_{0},y_{0})=(1.0,0.99)$.\n- Test $3$: $(x_{0},y_{0})=(0.0,0.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a list formatted as $[{\\rm code},{\\rm iterations},x_{\\text{final}},y_{\\text{final}}]$ in that order. For example, the output format must be exactly like $[[c_{1},k_{1},x_{1},y_{1}],[c_{2},k_{2},x_{2},y_{2}],[c_{3},k_{3},x_{3},y_{3}]]$, with no extra whitespace requirements beyond standard printing. All numerical values are unitless real numbers.",
            "solution": "The supplied problem is a valid exercise in computational engineering, specifically concerning the analysis of the steepest descent algorithm. It is well-posed, scientifically grounded, and free of ambiguity. We shall proceed with a rigorous analytical solution.\n\nThe objective is to analyze the iterative sequence generated by the method of steepest descent for the unconstrained minimization of the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$, defined as:\n$$f(x, y) = x^2 - y^2$$\nThis function represents a saddle surface, with a stationary point at the origin $(0, 0)$ which is neither a minimum nor a maximum.\n\nThe steepest descent iteration is defined by the update rule:\n$$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$$\nwhere $\\mathbf{x}_k = (x_k, y_k)$ is the iterate at step $k$, $\\mathbf{d}_k$ is the direction of steepest descent, and $\\alpha_k > 0$ is the step length determined by an exact line search.\n\nFirst, we compute the gradient of $f(\\mathbf{x})$, which is required to determine the descent direction.\n$$\\nabla f(x, y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ -2y \\end{pmatrix}$$\nThe direction of steepest descent is the negative of the gradient:\n$$\\mathbf{d}_k = -\\nabla f(x_k, y_k) = \\begin{pmatrix} -2x_k \\\\ 2y_k \\end{pmatrix}$$\nA point $(x_k, y_k)$ is stationary if and only if $\\nabla f(x_k, y_k) = \\mathbf{0}$, which occurs only at $(x,y) = (0,0)$.\n\nThe next step is to perform an exact line search. We define a function $\\phi(\\alpha)$ representing the value of $f$ along the ray starting from $\\mathbf{x}_k$ in the direction $\\mathbf{d}_k$:\n$$\\phi(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\quad \\text{for } \\alpha \\ge 0$$\nSubstituting the expressions for $\\mathbf{x}_k$ and $\\mathbf{d}_k$:\n$$\\mathbf{x}_k + \\alpha \\mathbf{d}_k = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} + \\alpha \\begin{pmatrix} -2x_k \\\\ 2y_k \\end{pmatrix} = \\begin{pmatrix} x_k(1 - 2\\alpha) \\\\ y_k(1 + 2\\alpha) \\end{pmatrix}$$\nThus, $\\phi(\\alpha)$ becomes:\n$$\\phi(\\alpha) = (x_k(1 - 2\\alpha))^2 - (y_k(1 + 2\\alpha))^2$$\nExpanding this expression reveals a quadratic dependence on $\\alpha$:\n$$\\phi(\\alpha) = x_k^2(1 - 4\\alpha + 4\\alpha^2) - y_k^2(1 + 4\\alpha + 4\\alpha^2)$$\n$$\\phi(\\alpha) = (x_k^2 - y_k^2) - 4\\alpha(x_k^2 + y_k^2) + 4\\alpha^2(x_k^2 - y_k^2)$$\nThis is a quadratic of the form $\\phi(\\alpha) = A\\alpha^2 + B\\alpha + C$, with coefficients:\n- $A = 4(x_k^2 - y_k^2)$\n- $B = -4(x_k^2 + y_k^2)$\n- $C = x_k^2 - y_k^2 = f(x_k, y_k)$\n\nThe behavior of the line search depends critically on the sign of the quadratic coefficient $A$.\n1.  If $A > 0$, which is equivalent to $x_k^2 > y_k^2$ or $|x_k| > |y_k|$, the parabola $\\phi(\\alpha)$ opens upwards and has a unique minimum. The minimizer $\\alpha^*$ is found by setting the derivative $\\phi'(\\alpha) = 2A\\alpha + B$ to zero:\n    $$\\alpha^* = -\\frac{B}{2A} = -\\frac{-4(x_k^2 + y_k^2)}{2 \\cdot 4(x_k^2 - y_k^2)} = \\frac{x_k^2 + y_k^2}{2(x_k^2 - y_k^2)}$$\n    Since $x_k^2+y_k^2>0$ (for a non-stationary point) and $x_k^2-y_k^2>0$, we have $\\alpha^* > 0$. The optimal step length is $\\alpha_k = \\alpha^*$.\n\n2.  If $A \\le 0$, which is equivalent to $x_k^2 \\le y_k^2$ or $|x_k| \\le |y_k|$:\n    - If $A < 0$, the parabola $\\phi(\\alpha)$ opens downwards and is unbounded below as $\\alpha \\to \\infty$.\n    - If $A = 0$ (and the point is not the origin), then $\\phi(\\alpha) = B\\alpha + C$ is a line with slope $B = -4(x_k^2+y_k^2) < 0$, which is also unbounded below as $\\alpha \\to \\infty$.\n    In both sub-cases, no finite minimizer for $\\alpha \\ge 0$ exists. The problem statement dictates this is a termination condition (Code $2$).\n\nWhen a finite step is possible ($|x_k| > |y_k|$), we derive the update formulas for $\\mathbf{x}_{k+1}$:\n$$x_{k+1} = x_k(1 - 2\\alpha_k) = x_k\\left(1 - 2 \\frac{x_k^2 + y_k^2}{2(x_k^2 - y_k^2)}\\right) = x_k\\left(\\frac{x_k^2 - y_k^2 - x_k^2 - y_k^2}{x_k^2 - y_k^2}\\right) = x_k \\frac{-2y_k^2}{x_k^2 - y_k^2}$$\n$$y_{k+1} = y_k(1 + 2\\alpha_k) = y_k\\left(1 + 2 \\frac{x_k^2 + y_k^2}{2(x_k^2 - y_k^2)}\\right) = y_k\\left(\\frac{x_k^2 - y_k^2 + x_k^2 + y_k^2}{x_k^2 - y_k^2}\\right) = y_k \\frac{2x_k^2}{x_k^2 - y_k^2}$$\nLet us analyze the ratio $\\rho_k = y_k/x_k$ (for $x_k \\ne 0$). The ratio for the next iterate is:\n$$\\rho_{k+1} = \\frac{y_{k+1}}{x_{k+1}} = \\frac{y_k \\frac{2x_k^2}{x_k^2 - y_k^2}}{x_k \\frac{-2y_k^2}{x_k^2 - y_k^2}} = \\frac{y_k}{x_k} \\frac{2x_k^2}{-2y_k^2} = \\rho_k \\left(-\\frac{1}{\\rho_k^2}\\right) = -\\frac{1}{\\rho_k}$$\nThe condition for an iteration to proceed is $|x_k| > |y_k|$, or $|\\rho_k| < 1$. If this condition holds, then $|\\rho_{k+1}| = |-1/\\rho_k| = 1/|\\rho_k| > 1$. This implies $|y_{k+1}| > |x_{k+1}|$. Consequently, at the next iteration, the condition for a bounded line search ($|x_{k+1}| > |y_{k+1}|$) will fail, leading to termination with Code $2$.\nThis analysis proves that the algorithm will perform at most one successful step before terminating, unless it starts on or lands on the origin.\n\nWe now apply this framework to the specified test cases.\n\n**Test Case 1: Initial Point $(x_0, y_0) = (1.0, 0.0)$**\n- **Iteration $k=0$**: The point is $\\mathbf{x}_0 = (1.0, 0.0)$.\n- The gradient $\\nabla f(1, 0) = (2, 0)$ is non-zero, so the point is not stationary.\n- We check the line search condition: $|x_0| = 1.0 > |y_0| = 0.0$. A finite step $\\alpha_0$ exists.\n- We compute the next iterate $\\mathbf{x}_1$:\n  $x_1 = 1.0 \\frac{-2(0.0)^2}{1.0^2 - 0.0^2} = 0.0$\n  $y_1 = 0.0 \\frac{2(1.0)^2}{1.0^2 - 0.0^2} = 0.0$\n- The new point is $\\mathbf{x}_1 = (0.0, 0.0)$.\n- **Iteration $k=1$**: The point is $\\mathbf{x}_1 = (0.0, 0.0)$.\n- The gradient $\\nabla f(0, 0) = (0, 0)$ is zero. A stationary point has been reached.\n- Termination reason: Code $1$. Number of iterations: $1$. Final point: $(0.0, 0.0)$.\nResult: `[1, 1, 0.0, 0.0]`\n\n**Test Case 2: Initial Point $(x_0, y_0) = (1.0, 0.99)$**\n- **Iteration $k=0$**: The point is $\\mathbf{x}_0 = (1.0, 0.99)$.\n- The gradient is non-zero.\n- Line search condition: $|x_0| = 1.0 > |y_0| = 0.99$. A finite step $\\alpha_0$ exists.\n- We compute $\\mathbf{x}_1$:\n  $x_0^2 = 1.0$, $y_0^2 = 0.99^2 = 0.9801$.\n  $x_1 = 1.0 \\frac{-2(0.9801)}{1.0 - 0.9801} = \\frac{-1.9602}{0.0199} = -\\frac{19602}{199} \\approx -98.50251256$\n  $y_1 = 0.99 \\frac{2(1.0)}{1.0 - 0.9801} = \\frac{1.98}{0.0199} = \\frac{19800}{199} \\approx 99.49748744$\n- The new point is $\\mathbf{x}_1 \\approx (-98.50, 99.50)$.\n- **Iteration $k=1$**: The point is $\\mathbf{x}_1$.\n- Line search condition check at $\\mathbf{x}_1$: $|x_1| = \\frac{19602}{199}$, $|y_1| = \\frac{19800}{199}$.\n- We have $|y_1| > |x_1|$. The line search from $\\mathbf{x}_1$ is unbounded below.\n- The algorithm terminates.\n- Termination reason: Code $2$. Number of iterations: $1$ (to get from $\\mathbf{x}_0$ to $\\mathbf{x}_1$). Final point: $\\mathbf{x}_1$.\nResult: `[2, 1, -19602/199, 19800/199]`\n\n**Test Case 3: Initial Point $(x_0, y_0) = (0.0, 0.0)$**\n- **Iteration $k=0$**: The point is $\\mathbf{x}_0 = (0.0, 0.0)$.\n- The gradient $\\nabla f(0, 0) = (0, 0)$ is zero. The starting point is already stationary.\n- The algorithm terminates before taking any steps.\n- Termination reason: Code $0$. Number of iterations: $0$. Final point: $(0.0, 0.0)$.\nResult: `[0, 0, 0.0, 0.0]`",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# No libraries outside the standard library are needed for this problem.\n\ndef solve():\n    \"\"\"\n    Analyzes steepest descent for f(x,y) = x^2 - y^2 for a suite of test cases.\n    \"\"\"\n    \n    def analyze_single_case(x0: float, y0: float, N_max: int):\n        \"\"\"\n        Applies the steepest descent logic for a single initial point.\n        Based on the analytical derivation, the process terminates in at most 1 iteration.\n        \n        Args:\n            x0: Initial x-coordinate.\n            y0: Initial y-coordinate.\n            N_max: Maximum number of iterations (given as 50, but not reached).\n\n        Returns:\n            A list [code, iterations, x_final, y_final].\n        \"\"\"\n        x, y = float(x0), float(y0)\n        \n        # Iteration k=0\n        \n        # Check for Code 0: Initial point is stationary.\n        # This occurs only at the origin.\n        if x == 0.0 and y == 0.0:\n            return [0, 0, 0.0, 0.0]\n\n        # Check for Code 2 at k=0: Line search is unbounded from the start.\n        # This occurs if |x| <= |y|.\n        if abs(x) <= abs(y):\n            return [2, 0, x, y]\n\n        # If |x| > |y|, the line search is bounded, and we perform one iteration.\n        k = 1\n        x_sq = x * x\n        y_sq = y * y\n        denominator = x_sq - y_sq\n        \n        # Update formulas derived from exact line search\n        x_next = x * (-2.0 * y_sq) / denominator\n        y_next = y * (2.0 * x_sq) / denominator\n        \n        # After one step, the new point (x_next, y_next) is reached.\n        # The number of iterations performed is 1.\n\n        # Check for Code 1: Reached a stationary point.\n        # This happens if the initial point was on the x-axis (but not the origin),\n        # leading to the next iterate being (0,0).\n        if x_next == 0.0 and y_next == 0.0:\n            return [1, k, 0.0, 0.0]\n        \n        # Check for Code 2 after one iteration.\n        # Our analysis showed that if |x0/y0|1, then |x1/y1|>1.\n        # This means the line search from (x_next, y_next) must be unbounded.\n        # The condition abs(x_next) = abs(y_next) will be true.\n        return [2, k, x_next, y_next]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.0),\n        (1.0, 0.99),\n        (0.0, 0.0),\n    ]\n\n    # The maximum number of iterations as specified in the problem statement.\n    N = 50\n\n    all_results = []\n    for x_start, y_start in test_cases:\n        result = analyze_single_case(x_start, y_start, N)\n        all_results.append(result)\n\n    # Format the final output string as per the requirements.\n    # e.g., [[c1,k1,x1,y1],[c2,k2,x2,y2],[c3,k3,x3,y3]]\n    result_strings = [f\"[{c},{k},{x},{y}]\" for c, k, x, y in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having explored local convergence behavior, we now zoom out to consider the global landscape of a non-convex function. Steepest descent is a local search algorithm, meaning its final destination depends entirely on its starting point. This practice uses the well-known Himmelblau function, which has multiple local minima, to demonstrate how different initializations lead the algorithm to converge to different solutions, visually illustrating the concept of basins of attraction .",
            "id": "2448739",
            "problem": "Design and implement a program that, for the bivariate Himmelblau function defined by\n$$\nf(x,y) = \\left(x^2 + y - 11\\right)^2 + \\left(x + y^2 - 7\\right)^2,\n$$\nperforms steepest descent from multiple initial points and summarizes the terminal behavior. For a point $\\mathbf{x}_k = [x_k, y_k]^\\top \\in \\mathbb{R}^2$, define the steepest descent iteration by\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k),\n$$\nwhere $\\alpha_k$ is chosen to be a minimizer of the one-dimensional function\n$$\n\\phi_k(\\alpha) = f\\left(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)\\right)\n$$\nover the closed interval $\\alpha \\in [0,1]$. Use the Euclidean norm for vectors. Terminate the iteration when either $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\leq \\tau$ or $k \\geq k_{\\max}$, where $\\tau = 10^{-8}$ and $k_{\\max} = 10000$. The gradient $\\nabla f(x,y)$ is taken in the standard sense of multivariate calculus.\n\nFor each initial point in the following test suite (listed in the given order),\n$$\n(3, 2),\\quad (0, 0),\\quad (-3, -3),\\quad (-4, 4),\\quad (5, 5),\\quad (3.5, -2.5),\n$$\nrun the steepest descent method as specified and record:\n- the final iterate $\\left(x^\\star, y^\\star\\right)$ at termination,\n- the function value $f\\left(x^\\star, y^\\star\\right)$,\n- the total number of iterations $k$ executed,\n- and an integer label identifying which of the four known local minimizers the final point is nearest to in Euclidean distance:\n  - $m_0 = (3.000000, 2.000000)$,\n  - $m_1 = (-2.805118, 3.131312)$,\n  - $m_2 = (-3.779310, -3.283186)$,\n  - $m_3 = (3.584428, -1.848126)$.\nLet $d_j = \\lVert \\left(x^\\star, y^\\star\\right) - m_j \\rVert_2$ for $j \\in \\{0,1,2,3\\}$. If $\\min_j d_j \\leq 10^{-3}$, output the index $j$ attaining the minimum; otherwise, output $-1$.\n\nRequired numerical reporting:\n- Round $x^\\star$, $y^\\star$, and $f\\left(x^\\star, y^\\star\\right)$ to $6$ decimal places.\n- Report $k$ and the label as integers.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists, in the same order as the test suite, where each inner list is\n$$\n[x^\\star, y^\\star, f(x^\\star, y^\\star), k, \\text{label}].\n$$\nFor example, the overall format must be\n$$\n\\big[ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot], \\ldots \\big]\n$$\nprinted on a single line.",
            "solution": "The problem posed is a well-defined exercise in computational engineering, specifically in the domain of unconstrained multivariate optimization. It requires the implementation of the steepest descent algorithm with an exact line search to find local minima of the Himmelblau function. The problem is scientifically grounded, mathematically consistent, and contains all necessary information for its resolution. It is therefore deemed valid.\n\nThe core of the problem is to minimize the bivariate Himmelblau function, defined as:\n$$\nf(x,y) = \\left(x^2 + y - 11\\right)^2 + \\left(x + y^2 - 7\\right)^2\n$$\nLet the vector of variables be $\\mathbf{x} = [x, y]^\\top$. The function can be written as $f(\\mathbf{x})$.\n\nThe steepest descent method is an iterative optimization algorithm that proceeds from an initial guess $\\mathbf{x}_0$ and generates a sequence of points $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots$ intended to converge to a local minimum. The update rule for each iteration is given by:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\nwhere $\\mathbf{d}_k$ is the search direction and $\\alpha_k  0$ is the step size. For the method of steepest descent, the search direction is chosen to be the negative of the gradient of the objective function at the current point, as this is the direction in which the function's value decreases most rapidly. Thus,\n$$\n\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\n$$\nThe gradient of the Himmelblau function, $\\nabla f(\\mathbf{x}) = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right]^\\top$, must be derived. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x} = 2(x^2 + y - 11)(2x) + 2(x + y^2 - 7)(1) = 4x(x^2 + y - 11) + 2(x + y^2 - 7)\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11)(1) + 2(x + y^2 - 7)(2y) = 2(x^2 + y - 11) + 4y(x + y^2 - 7)\n$$\n\nThe iterative formula is therefore:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)\n$$\nThe problem specifies that the step size $\\alpha_k$ must be chosen to minimize the function along the search direction. This is known as an exact line search. We define a one-dimensional function $\\phi_k(\\alpha)$:\n$$\n\\phi_k(\\alpha) = f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))\n$$\nThe optimal step size $\\alpha_k$ is the value of $\\alpha$ that minimizes $\\phi_k(\\alpha)$ over the specified closed interval $[0, 1]$.\n$$\n\\alpha_k = \\arg\\min_{\\alpha \\in [0, 1]} \\phi_k(\\alpha)\n$$\nThis one-dimensional minimization is a standard subproblem that can be solved numerically using established methods, for instance, by using a library routine such as `scipy.optimize.minimize_scalar` with the `bounded` method.\n\nThe iterative process continues until one of two termination criteria is satisfied:\n1.  The magnitude of the gradient falls below a specified tolerance $\\tau = 10^{-8}$. This indicates that the iterate is near a stationary point (a minimum, maximum, or saddle point). The condition is $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\leq \\tau$.\n2.  The number of iterations $k$ reaches a maximum limit $k_{\\max} = 10000$. This prevents an infinite loop in cases of slow or non-convergence.\n\nUpon termination at iteration $k$ with final point $\\mathbf{x}^\\star = \\mathbf{x}_k$, the following data are recorded:\n- The final coordinates $(x^\\star, y^\\star)$.\n- The final function value $f(x^\\star, y^\\star)$.\n- The total number of iterations $k$.\n- A classification label.\n\nThe classification label is determined by comparing the final point $\\mathbf{x}^\\star$ to the four known local minimizers of the Himmelblau function:\n- $m_0 = (3.000000, 2.000000)$\n- $m_1 = (-2.805118, 3.131312)$\n- $m_2 = (-3.779310, -3.283186)$\n- $m_3 = (3.584428, -1.848126)$\n\nThe Euclidean distance $d_j = \\lVert \\mathbf{x}^\\star - m_j \\rVert_2$ is computed for each $j \\in \\{0, 1, 2, 3\\}$. If the minimum of these distances, $\\min_j d_j$, is less than or equal to $10^{-3}$, the label is the index $j$ corresponding to this minimum distance. This signifies successful convergence to the neighborhood of a known minimizer. If $\\min_j d_j  10^{-3}$, the label is $-1$, indicating that the algorithm terminated at a point not sufficiently close to any of the known minimizers.\n\nThe computational procedure for each given initial point is as follows:\n1.  Initialize $\\mathbf{x}_0$ and set the iteration counter $k = 0$.\n2.  Begin the main loop:\n    a. Calculate the gradient vector $\\nabla f(\\mathbf{x}_k)$.\n    b. Calculate the gradient norm $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2$.\n    c. If $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\leq 10^{-8}$ or $k \\geq 10000$, terminate the loop.\n    d. Define the line search function $\\phi_k(\\alpha) = f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))$.\n    e. Find $\\alpha_k = \\arg\\min_{\\alpha \\in [0, 1]} \\phi_k(\\alpha)$ using a numerical solver.\n    f. Update the iterate: $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$.\n    g. Increment the iteration counter: $k \\leftarrow k+1$.\n3.  After termination, perform the classification and reporting steps as described above, ensuring numerical values are rounded to $6$ decimal places as required.\nThis entire procedure is repeated for each of the specified initial points.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements the steepest descent algorithm for the Himmelblau function\n    from multiple starting points and formats the results.\n    \"\"\"\n    # Define problem constants and parameters\n    TOLERANCE = 1e-8\n    MAX_ITERATIONS = 10000\n    DISTANCE_THRESHOLD = 1e-3\n\n    # Define the Himmelblau function\n    def f(x, y):\n        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n    # Define the gradient of the Himmelblau function\n    def grad_f(x, y):\n        df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n        df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n        return np.array([df_dx, df_dy])\n\n    # Known local minimizers of the Himmelblau function\n    minimizers = np.array([\n        [3.0, 2.0],\n        [-2.805118, 3.131312],\n        [-3.779310, -3.283186],\n        [3.584428, -1.848126]\n    ])\n\n    # Test suite of initial points\n    test_cases = [\n        (3.0, 2.0),\n        (0.0, 0.0),\n        (-3.0, -3.0),\n        (-4.0, 4.0),\n        (5.0, 5.0),\n        (3.5, -2.5)\n    ]\n\n    all_results = []\n\n    for initial_point in test_cases:\n        x_k = np.array(initial_point, dtype=float)\n        \n        # Iteration loop for steepest descent\n        k = 0\n        while k  MAX_ITERATIONS:\n            grad = grad_f(x_k[0], x_k[1])\n            grad_norm = np.linalg.norm(grad)\n\n            # Termination condition: gradient norm is below tolerance\n            if grad_norm = TOLERANCE:\n                break\n\n            # Descent direction\n            d_k = -grad\n\n            # Line search for optimal step size alpha\n            # Define the 1D function to minimize\n            phi = lambda alpha: f(x_k[0] + alpha * d_k[0], x_k[1] + alpha * d_k[1])\n            \n            # Use a bounded scalar minimizer to find alpha in [0, 1]\n            res = minimize_scalar(phi, bounds=(0, 1), method='bounded')\n            alpha_k = res.x\n\n            # Update the iterate\n            x_k = x_k + alpha_k * d_k\n            \n            k += 1\n\n        # At this point, the loop has terminated. 'k' is the number of iterations.\n        x_star, y_star = x_k[0], x_k[1]\n        f_star = f(x_star, y_star)\n\n        # Classification: find the closest known minimizer\n        distances = np.linalg.norm(minimizers - x_k, axis=1)\n        min_dist_idx = np.argmin(distances)\n        min_dist = distances[min_dist_idx]\n        \n        label = -1\n        if min_dist = DISTANCE_THRESHOLD:\n            label = int(min_dist_idx)\n        \n        # Store results for this case\n        all_results.append([x_star, y_star, f_star, k, label])\n\n    # Format the final output string according to the specification\n    result_parts = []\n    for res in all_results:\n        x_s, y_s, f_s, num_iter, lab = res\n        part = f\"[{x_s:.6f},{y_s:.6f},{f_s:.6f},{num_iter},{lab}]\"\n        result_parts.append(part)\n    \n    final_output = f\"[{','.join(result_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}