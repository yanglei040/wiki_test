{
    "hands_on_practices": [
        {
            "introduction": "The foundation of the finite difference method lies in the Taylor series expansion. This exercise challenges you to derive a stencil from first principles, moving beyond one-dimensional derivatives to the more complex case of a mixed partial derivative in two dimensions. Mastering this process is key to understanding how numerical schemes are constructed and customized for various differential equations .",
            "id": "2418865",
            "problem": "Consider a function $f(x,y)$ that is three times continuously differentiable in a neighborhood of a point $(x_i,y_j)$. The function is sampled on a uniform Cartesian grid with spacings $h_x>0$ and $h_y>0$, where $x_{i\\pm 1}=x_i\\pm h_x$ and $y_{j\\pm 1}=y_j\\pm h_y$. Denote the grid samples by $f_{i,j}=f(x_i,y_j)$, $f_{i\\pm 1,j\\pm 1}=f(x_{i\\pm 1},y_{j\\pm 1})$.\n\nUsing the minimum number of grid points required to obtain a consistent approximation at $(x_i,y_j)$, construct a finite-difference stencil that approximates the mixed partial derivative $\\frac{\\partial^2 f}{\\partial x \\partial y}(x_i,y_j)$ on this uniform grid. Express your final result as a single closed-form analytic expression in terms of $f_{i\\pm 1,j\\pm 1}$, $h_x$, and $h_y$ only. Do not include any intermediate steps in your final result.",
            "solution": "We seek a consistent finite-difference approximation to the mixed partial derivative $\\frac{\\partial^2 f}{\\partial x \\partial y}(x_i,y_j)$ using as few grid points as possible on a uniform Cartesian grid with spacings $h_x$ and $h_y$. A minimal and natural requirement for consistency and locality at $(x_i,y_j)$ on a uniform grid implies that the stencil should be centered at $(x_i,y_j)$ and use symmetric points so that lower-order terms cancel when forming linear combinations. In two dimensions, the smallest symmetric set of points that does not include $(x_i,y_j)$ and respects the parity needed to isolate the mixed derivative consists of the four diagonal neighbors: $(x_{i+1},y_{j+1})$, $(x_{i+1},y_{j-1})$, $(x_{i-1},y_{j+1})$, and $(x_{i-1},y_{j-1})$.\n\nTo derive the coefficients from first principles, we expand $f$ in a bivariate Taylor series about $(x_i,y_j)$ to third order at these four diagonal locations. Let subscripts on $f$ denote partial derivatives evaluated at $(x_i,y_j)$, for example $f_x=\\frac{\\partial f}{\\partial x}(x_i,y_j)$, $f_{xy}=\\frac{\\partial^2 f}{\\partial x \\partial y}(x_i,y_j)$, and similarly for higher derivatives. Then\n\n$$\n\\begin{aligned}\nf_{i+1,j+1}&=f+ h_x f_x + h_y f_y + \\tfrac{1}{2} h_x^2 f_{xx} + \\tfrac{1}{2} h_y^2 f_{yy} + h_x h_y f_{xy} + \\tfrac{1}{6} h_x^3 f_{xxx} + \\tfrac{1}{6} h_y^3 f_{yyy} \\\\\n&\\quad + \\tfrac{1}{2} h_x^2 h_y f_{xxy} + \\tfrac{1}{2} h_x h_y^2 f_{xyy} + \\mathcal{O}(h_x^4, h_y^4, h_x^3 h_y, h_x h_y^3)\\\\\nf_{i+1,j-1}&=f+ h_x f_x - h_y f_y + \\tfrac{1}{2} h_x^2 f_{xx} + \\tfrac{1}{2} h_y^2 f_{yy} - h_x h_y f_{xy} + \\tfrac{1}{6} h_x^3 f_{xxx} - \\tfrac{1}{6} h_y^3 f_{yyy} \\\\\n&\\quad + \\tfrac{1}{2} h_x^2 h_y f_{xxy} - \\tfrac{1}{2} h_x h_y^2 f_{xyy} + \\mathcal{O}(h_x^4, h_y^4, h_x^3 h_y, h_x h_y^3)\\\\\nf_{i-1,j+1}&=f- h_x f_x + h_y f_y + \\tfrac{1}{2} h_x^2 f_{xx} + \\tfrac{1}{2} h_y^2 f_{yy} - h_x h_y f_{xy} - \\tfrac{1}{6} h_x^3 f_{xxx} + \\tfrac{1}{6} h_y^3 f_{yyy} \\\\\n&\\quad - \\tfrac{1}{2} h_x^2 h_y f_{xxy} + \\tfrac{1}{2} h_x h_y^2 f_{xyy} + \\mathcal{O}(h_x^4, h_y^4, h_x^3 h_y, h_x h_y^3)\\\\\nf_{i-1,j-1}&=f- h_x f_x - h_y f_y + \\tfrac{1}{2} h_x^2 f_{xx} + \\tfrac{1}{2} h_y^2 f_{yy} + h_x h_y f_{xy} - \\tfrac{1}{6} h_x^3 f_{xxx} - \\tfrac{1}{6} h_y^3 f_{yyy} \\\\\n&\\quad - \\tfrac{1}{2} h_x^2 h_y f_{xxy} - \\tfrac{1}{2} h_x h_y^2 f_{xyy} + \\mathcal{O}(h_x^4, h_y^4, h_x^3 h_y, h_x h_y^3)\n\\end{aligned}\n$$\n\nConsider the linear combination\n\n$$\nS \\equiv f_{i+1,j+1} - f_{i+1,j-1} - f_{i-1,j+1} + f_{i-1,j-1}.\n$$\n\nAdding and subtracting the above expansions term by term, the constant terms $f$ cancel, as do all terms involving a single derivative ($f_x$, $f_y$), as well as the pure second derivatives ($f_{xx}$, $f_{yy}$). The mixed second derivative terms $f_{xy}$ add constructively:\n\n$$\nS = 4 h_x h_y f_{xy} + \\mathcal{O}(h_x^3 h_y, h_x h_y^3).\n$$\n\nDividing by $4 h_x h_y$ yields the approximation\n\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x_i,y_j) = \\frac{f_{i+1,j+1} - f_{i+1,j-1} - f_{i-1,j+1} + f_{i-1,j-1}}{4 h_x h_y} + \\mathcal{O}\\left(h_x^2 + h_y^2\\right).\n$$\n\nThis stencil uses exactly four function values, which is the minimum number of grid points for a symmetric, centered, and consistent approximation of the mixed derivative at $(x_i,y_j)$ on a uniform grid. Therefore, the finite-difference stencil requested is the diagonal four-point formula above, and the closed-form analytic expression is\n\n$$\n\\frac{f_{i+1,j+1} - f_{i+1,j-1} - f_{i-1,j+1} + f_{i-1,j-1}}{4 h_x h_y}.\n$$",
            "answer": "$$\\boxed{\\frac{f_{i+1,j+1}-f_{i+1,j-1}-f_{i-1,j+1}+f_{i-1,j-1}}{4\\,h_{x}\\,h_{y}}}$$"
        },
        {
            "introduction": "From theory, we move to practical implementation. This problem guides you through the process of writing code to compute the divergence of a vector field, a fundamental operation in physics and engineering. You will apply different finite difference formulas for interior and boundary points, a common and essential task in computational science .",
            "id": "2418896",
            "problem": "You are given a three-dimensional uniform Cartesian grid with node indices $\\{i,j,k\\}$, where $i \\in \\{0,1,\\dots,N_x-1\\}$, $j \\in \\{0,1,\\dots,N_y-1\\}$, and $k \\in \\{0,1,\\dots,N_z-1\\}$. The physical coordinates of node $(i,j,k)$ are defined by $x_i = x_{\\min} + i\\,\\Delta x$, $y_j = y_{\\min} + j\\,\\Delta y$, $z_k = z_{\\min} + k\\,\\Delta z$, with uniform spacings $\\Delta x = \\dfrac{x_{\\max}-x_{\\min}}{N_x-1}$, $\\Delta y = \\dfrac{y_{\\max}-y_{\\min}}{N_y-1}$, and $\\Delta z = \\dfrac{z_{\\max}-z_{\\min}}{N_z-1}$. A vector field $\\mathbf{F}(x,y,z)$ with components $\\mathbf{F} = (P,Q,R)$ is prescribed at every grid node via analytic expressions. All trigonometric functions, if present, must interpret their arguments as angles in radians.\n\nDefine the discrete divergence at node $(i,j,k)$ by the sum of three discrete first-order partial derivatives applied to the components $P,Q,R$, respectively. The discrete first-order partial derivative in the $x$-direction at node $(i,j,k)$ is defined by\n- For interior nodes in $x$, i.e., $1 \\le i \\le N_x - 2$:\n$$\\left.\\frac{\\partial P}{\\partial x}\\right|_{i,j,k} \\approx \\frac{P_{i+1,j,k} - P_{i-1,j,k}}{2\\,\\Delta x}.$$\n- For the left boundary in $x$, i.e., $i = 0$:\n$$\\left.\\frac{\\partial P}{\\partial x}\\right|_{0,j,k} \\approx \\frac{-3\\,P_{0,j,k} + 4\\,P_{1,j,k} - P_{2,j,k}}{2\\,\\Delta x}.$$\n- For the right boundary in $x$, i.e., $i = N_x - 1$:\n$$\\left.\\frac{\\partial P}{\\partial x}\\right|_{N_x-1,j,k} \\approx \\frac{3\\,P_{N_x-1,j,k} - 4\\,P_{N_x-2,j,k} + P_{N_x-3,j,k}}{2\\,\\Delta x}.$$\n\nAnalogous formulas must be used in the $y$-direction on $Q$ using $\\Delta y$, and in the $z$-direction on $R$ using $\\Delta z$. The discrete divergence at node $(i,j,k)$ is the sum of these three approximations:\n$$\\left.\\nabla \\cdot \\mathbf{F}\\right|_{i,j,k} \\approx \\left.\\frac{\\partial P}{\\partial x}\\right|_{i,j,k} + \\left.\\frac{\\partial Q}{\\partial y}\\right|_{i,j,k} + \\left.\\frac{\\partial R}{\\partial z}\\right|_{i,j,k}.$$\n\nUse the above definitions to compute the discrete divergence for each of the following test cases. For each case, construct the grid, evaluate the field components at all nodes, compute the discrete divergence field, and extract the value at the specified node. Your program must output the four results as a single list, with each value rounded to exactly $6$ decimal places.\n\nTest Suite:\n- Case $1$:\n  - Domain: $x \\in [-1,1]$, $y \\in [-1,1]$, $z \\in [-1,1]$.\n  - Grid sizes: $N_x = 3$, $N_y = 4$, $N_z = 3$.\n  - Field: $P(x,y,z) = 2\\,x$, $Q(x,y,z) = -3\\,y$, $R(x,y,z) = 4\\,z$.\n  - Node index for reporting: $(i,j,k) = (0,0,0)$.\n- Case $2$:\n  - Domain: $x \\in [0,1]$, $y \\in [0,1]$, $z \\in [0,1]$.\n  - Grid sizes: $N_x = 5$, $N_y = 5$, $N_z = 5$.\n  - Field: $P(x,y,z) = x^2$, $Q(x,y,z) = y^2$, $R(x,y,z) = z^2$.\n  - Node index for reporting: $(i,j,k) = (2,2,3)$.\n- Case $3$:\n  - Domain: $x \\in [0,\\pi]$, $y \\in \\left[0,\\frac{\\pi}{2}\\right]$, $z \\in [0,1]$.\n  - Grid sizes: $N_x = 8$, $N_y = 7$, $N_z = 6$.\n  - Field: $P(x,y,z) = \\sin(x)$, $Q(x,y,z) = \\cos(y)$, $R(x,y,z) = e^{z}$.\n  - Node index for reporting: $(i,j,k) = (4,3,2)$.\n- Case $4$:\n  - Domain: $x \\in \\left[-\\frac{1}{2},\\frac{1}{2}\\right]$, $y \\in [0,1]$, $z \\in [-1,1]$.\n  - Grid sizes: $N_x = 4$, $N_y = 3$, $N_z = 5$.\n  - Field: $P(x,y,z) = e^{x}$, $Q(x,y,z) = y^3$, $R(x,y,z) = z$.\n  - Node index for reporting: $(i,j,k) = (3,0,4)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$).\n- Each $r_m$ must be a decimal number rounded to exactly $6$ digits after the decimal point.\n- No additional text should be printed.",
            "solution": "The problem presented requires the computation of the discrete divergence of a vector field $\\mathbf{F}(x,y,z) = (P, Q, R)$ on a three-dimensional uniform Cartesian grid. The problem is well-defined, scientifically sound, and complete. All necessary components, including the grid structure, analytical expressions for the vector field, and the specific finite difference formulas for approximating the partial derivatives, are provided. The problem is a standard exercise in computational engineering and applied mathematics, specifically in the domain of numerical discretization of differential operators. It is therefore valid and permits a direct, unambiguous solution.\n\nThe core of the problem is the computation of the divergence, defined as $\\nabla \\cdot \\mathbf{F} = \\frac{\\partial P}{\\partial x} + \\frac{\\partial Q}{\\partial y} + \\frac{\\partial R}{\\partial z}$. We are to approximate this continuous operator with a discrete equivalent at a specified node $(i,j,k)$ on the grid. The physical coordinates of any node $(i,j,k)$ are given by:\n$$x_i = x_{\\min} + i\\,\\Delta x, \\quad \\text{for } i \\in \\{0, 1, \\dots, N_x-1\\}$$\n$$y_j = y_{\\min} + j\\,\\Delta y, \\quad \\text{for } j \\in \\{0, 1, \\dots, N_y-1\\}$$\n$$z_k = z_{\\min} + k\\,\\Delta z, \\quad \\text{for } k \\in \\{0, 1, \\dots, N_z-1\\}$$\nwhere the grid spacings are uniform:\n$$\\Delta x = \\frac{x_{\\max}-x_{\\min}}{N_x-1}, \\quad \\Delta y = \\frac{y_{\\max}-y_{\\min}}{N_y-1}, \\quad \\Delta z = \\frac{z_{\\max}-z_{\\min}}{N_z-1}$$\n\nThe discrete partial derivative with respect to $x$ of the component $P$ is given by second-order accurate finite difference formulas, which depend on the position of the node index $i$:\nFor an interior node, $1 \\le i \\le N_x-2$, the central difference formula is used:\n$$\\left.\\frac{\\partial P}{\\partial x}\\right|_{i,j,k} \\approx \\frac{P(x_{i+1}, y_j, z_k) - P(x_{i-1}, y_j, z_k)}{2\\,\\Delta x}$$\nFor the left boundary node, $i=0$, the forward difference formula is used:\n$$\\left.\\frac{\\partial P}{\\partial x}\\right|_{0,j,k} \\approx \\frac{-3\\,P(x_0, y_j, z_k) + 4\\,P(x_1, y_j, z_k) - P(x_2, y_j, z_k)}{2\\,\\Delta x}$$\nFor the right boundary node, $i = N_x-1$, the backward difference formula is used:\n$$\\left.\\frac{\\partial P}{\\partial x}\\right|_{N_x-1,j,k} \\approx \\frac{3\\,P(x_{N_x-1}, y_j, z_k) - 4\\,P(x_{N_x-2}, y_j, z_k) + P(x_{N_x-3}, y_j, z_k)}{2\\,\\Delta x}$$\n\nIdentical stencil structures are applied for the partial derivatives of $Q$ with respect to $y$ (using index $j$, spacing $\\Delta y$, and grid size $N_y$) and of $R$ with respect to $z$ (using index $k$, spacing $\\Delta z$, and grid size $N_z$). The discrete divergence at node $(i,j,k)$ is the sum of these three numerical approximations:\n$$\\left.\\nabla \\cdot \\mathbf{F}\\right|_{i,j,k} \\approx \\left.\\frac{\\partial P}{\\partial x}\\right|_{i,j,k} + \\left.\\frac{\\partial Q}{\\partial y}\\right|_{i,j,k} + \\left.\\frac{\\partial R}{\\partial z}\\right|_{i,j,k}$$\n\nThe solution procedure is to systematically implement this calculation for each of the four test cases provided. For each case, we first establish the grid parameters ($x_{\\min}$, $x_{\\max}$, $N_x$, etc.). Then, we determine the grid spacing ($\\Delta x$, $\\Delta y$, $\\Delta z$) and the set of physical coordinates for each axis. For the specified node index $(i,j,k)$, we identify whether its position is on a boundary or in the interior for each dimension. Based on this, we select the appropriate finite difference formula. We then evaluate the field components $P$, $Q$, and $R$ at the necessary neighboring grid points, compute each partial derivative approximation, and sum the results to obtain the final discrete divergence. This process is repeated for all four test cases to produce the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the discrete divergence of a vector field on a 3D Cartesian grid\n    for a series of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            \"domain\": {'x': (-1, 1), 'y': (-1, 1), 'z': (-1, 1)},\n            \"grid_sizes\": {'Nx': 3, 'Ny': 4, 'Nz': 3},\n            \"field\": {\n                'P': lambda x, y, z: 2 * x,\n                'Q': lambda x, y, z: -3 * y,\n                'R': lambda x, y, z: 4 * z\n            },\n            \"node_index\": (0, 0, 0)\n        },\n        # Case 2\n        {\n            \"domain\": {'x': (0, 1), 'y': (0, 1), 'z': (0, 1)},\n            \"grid_sizes\": {'Nx': 5, 'Ny': 5, 'Nz': 5},\n            \"field\": {\n                'P': lambda x, y, z: x**2,\n                'Q': lambda x, y, z: y**2,\n                'R': lambda x, y, z: z**2\n            },\n            \"node_index\": (2, 2, 3)\n        },\n        # Case 3\n        {\n            \"domain\": {'x': (0, np.pi), 'y': (0, np.pi / 2), 'z': (0, 1)},\n            \"grid_sizes\": {'Nx': 8, 'Ny': 7, 'Nz': 6},\n            \"field\": {\n                'P': lambda x, y, z: np.sin(x),\n                'Q': lambda x, y, z: np.cos(y),\n                'R': lambda x, y, z: np.exp(z)\n            },\n            \"node_index\": (4, 3, 2)\n        },\n        # Case 4\n        {\n            \"domain\": {'x': (-0.5, 0.5), 'y': (0, 1), 'z': (-1, 1)},\n            \"grid_sizes\": {'Nx': 4, 'Ny': 3, 'Nz': 5},\n            \"field\": {\n                'P': lambda x, y, z: np.exp(x),\n                'Q': lambda x, y, z: y**3,\n                'R': lambda x, y, z: z\n            },\n            \"node_index\": (3, 0, 4)\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Extract parameters for the current case\n        x_min, x_max = case[\"domain\"]['x']\n        y_min, y_max = case[\"domain\"]['y']\n        z_min, z_max = case[\"domain\"]['z']\n        \n        Nx = case[\"grid_sizes\"]['Nx']\n        Ny = case[\"grid_sizes\"]['Ny']\n        Nz = case[\"grid_sizes\"]['Nz']\n\n        P = case[\"field\"]['P']\n        Q = case[\"field\"]['Q']\n        R = case[\"field\"]['R']\n        \n        i, j, k = case[\"node_index\"]\n\n        # Calculate grid spacings\n        # Handle division by zero for single-point dimensions (N=1)\n        delta_x = (x_max - x_min) / (Nx - 1) if Nx > 1 else 0\n        delta_y = (y_max - y_min) / (Ny - 1) if Ny > 1 else 0\n        delta_z = (z_max - z_min) / (Nz - 1) if Nz > 1 else 0\n\n        # Generate coordinate arrays\n        x_coords = np.linspace(x_min, x_max, Nx)\n        y_coords = np.linspace(y_min, y_max, Ny)\n        z_coords = np.linspace(z_min, z_max, Nz)\n        \n        x_target, y_target, z_target = x_coords[i], y_coords[j], z_coords[k]\n\n        # --- Calculate dP/dx ---\n        if Nx  3:\n            # Not enough points for the specified stencils. Problem implies N>=3.\n            # However, for robustness, we can assume derivative is 0 if not computable.\n            dP_dx = 0.0\n        elif i == 0: # Forward difference at left boundary\n            p0 = P(x_coords[0], y_target, z_target)\n            p1 = P(x_coords[1], y_target, z_target)\n            p2 = P(x_coords[2], y_target, z_target)\n            dP_dx = (-3 * p0 + 4 * p1 - p2) / (2 * delta_x)\n        elif i == Nx - 1: # Backward difference at right boundary\n            p0 = P(x_coords[Nx-1], y_target, z_target)\n            p1 = P(x_coords[Nx-2], y_target, z_target)\n            p2 = P(x_coords[Nx-3], y_target, z_target)\n            dP_dx = (3 * p0 - 4 * p1 + p2) / (2 * delta_x)\n        else: # Central difference for interior\n            p_minus = P(x_coords[i-1], y_target, z_target)\n            p_plus = P(x_coords[i+1], y_target, z_target)\n            dP_dx = (p_plus - p_minus) / (2 * delta_x)\n\n        # --- Calculate dQ/dy ---\n        if Ny  3:\n            dQ_dy = 0.0\n        elif j == 0: # Forward difference at 'left' boundary\n            q0 = Q(x_target, y_coords[0], z_target)\n            q1 = Q(x_target, y_coords[1], z_target)\n            q2 = Q(x_target, y_coords[2], z_target)\n            dQ_dy = (-3 * q0 + 4 * q1 - q2) / (2 * delta_y)\n        elif j == Ny - 1: # Backward difference at 'right' boundary\n            q0 = Q(x_target, y_coords[Ny-1], z_target)\n            q1 = Q(x_target, y_coords[Ny-2], z_target)\n            q2 = Q(x_target, y_coords[Ny-3], z_target)\n            dQ_dy = (3 * q0 - 4 * q1 + q2) / (2 * delta_y)\n        else: # Central difference for interior\n            q_minus = Q(x_target, y_coords[j-1], z_target)\n            q_plus = Q(x_target, y_coords[j+1], z_target)\n            dQ_dy = (q_plus - q_minus) / (2 * delta_y)\n\n        # --- Calculate dR/dz ---\n        if Nz  3:\n            dR_dz = 0.0\n        elif k == 0: # Forward difference at 'left' boundary\n            r0 = R(x_target, y_target, z_coords[0])\n            r1 = R(x_target, y_target, z_coords[1])\n            r2 = R(x_target, y_target, z_coords[2])\n            dR_dz = (-3 * r0 + 4 * r1 - r2) / (2 * delta_z)\n        elif k == Nz - 1: # Backward difference at 'right' boundary\n            r0 = R(x_target, y_target, z_coords[Nz-1])\n            r1 = R(x_target, y_target, z_coords[Nz-2])\n            r2 = R(x_target, y_target, z_coords[Nz-3])\n            dR_dz = (3 * r0 - 4 * r1 + r2) / (2 * delta_z)\n        else: # Central difference for interior\n            r_minus = R(x_target, y_target, z_coords[k-1])\n            r_plus = R(x_target, y_target, z_coords[k+1])\n            dR_dz = (r_plus - r_minus) / (2 * delta_z)\n            \n        divergence = dP_dx + dQ_dy + dR_dz\n        results.append(divergence)\n\n    # Format the final output string\n    # The requirement is to round to 6 decimal places.\n    # The f-string format specifier f'{x:.6f}' does this.\n    output_str = \"[\" + \",\".join(f\"{res:.6f}\" for res in results) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While finite differences are powerful, they have inherent limitations due to the trade-off between truncation error and floating-point round-off error. This practice illuminates this crucial concept by comparing the performance of standard finite difference formulas with the elegant complex-step method, which cleverly avoids round-off error amplification. This investigation will deepen your understanding of numerical precision and stability in scientific computing .",
            "id": "2418870",
            "problem": "Consider three numerical approximations to the derivative of a sufficiently smooth real-valued function $f$ at a real point $x$ using a real step size $h > 0$:\n1) The forward finite difference $D_{\\mathrm{fwd}}(f,x,h) = \\dfrac{f(x+h) - f(x)}{h}$.\n2) The central finite difference $D_{\\mathrm{ctr}}(f,x,h) = \\dfrac{f(x+h) - f(x-h)}{2h}$.\n3) The complex-step approximation $D_{\\mathrm{cs}}(f,x,h) = \\dfrac{\\operatorname{Im}\\!\\left[f(x + i h)\\right]}{h}$, where $\\operatorname{Im}[\\cdot]$ denotes the imaginary part and $i^2 = -1$.\n\nFor each of the following test cases, let $E_{\\mathrm{method}}(h) = \\left|D_{\\mathrm{method}}(f,x_0,h) - f'(x_0)\\right|$ denote the absolute error of the specified method at step size $h$. Angles for trigonometric functions must be interpreted in radians. For each test case, compute the minimum absolute error over the discrete set of step sizes $\\mathcal{H} = \\{10^{-k} : k \\in \\{1,2,\\dots,16\\}\\}$ for each method.\n\nTest suite:\n- Test case $1$: $f(x) = e^{x}$, $f'(x) = e^{x}$, $x_0 = 1.0$.\n- Test case $2$: $f(x) = \\sin(x)$, $f'(x) = \\cos(x)$, $x_0 = 1.0$.\n- Test case $3$: $f(x) = \\cos(x)$, $f'(x) = -\\sin(x)$, $x_0 = 0.0$.\n\nYour program must, for each test case, return the list $\\left[\\min_{h \\in \\mathcal{H}} E_{\\mathrm{fwd}}(h),\\, \\min_{h \\in \\mathcal{H}} E_{\\mathrm{ctr}}(h),\\, \\min_{h \\in \\mathcal{H}} E_{\\mathrm{cs}}(h)\\right]$. Aggregate the results for all test cases into a single list in the same order as above.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case is represented by a list of three floating-point numbers in scientific notation with $12$ significant digits, i.e., $[\\text{case}_1,\\text{case}_2,\\text{case}_3]$ with $\\text{case}_j = [E_{\\mathrm{fwd}},E_{\\mathrm{ctr}},E_{\\mathrm{cs}}]$. No extra spaces are permitted anywhere in the line.",
            "solution": "The problem as stated is well-posed, computationally feasible, and scientifically sound. It represents a standard exercise in numerical analysis, concerning the trade-off between truncation error and round-off error in numerical differentiation. We will proceed with a systematic solution.\n\nThe objective is to compare the accuracy of three numerical differentiation methods by finding the optimal step size $h$ from a given discrete set that minimizes the absolute error. This analysis reveals the fundamental conflict between two primary sources of error in numerical computation using finite-precision arithmetic: truncation error and round-off error.\n\n**1. Theoretical Foundation of Numerical Differentiation Errors**\n\nThe total absolute error $E(h)$ in a numerical derivative approximation is the sum of the magnitudes of the truncation error and the round-off error.\n\n*   **Truncation Error**: This is an intrinsic mathematical error arising from the approximation of the derivative operator using a finite step size $h$. It can be quantified using Taylor series expansions of a sufficiently smooth function $f(x)$.\n\n    *   For the **forward finite difference**, $D_{\\mathrm{fwd}}(f,x,h)$, the Taylor expansion of $f(x+h)$ around $x$ is:\n        $$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\mathcal{O}(h^3)$$\n        Rearranging for $f'(x)$ yields:\n        $$f'(x) = \\frac{f(x+h) - f(x)}{h} - \\frac{h}{2}f''(x) - \\mathcal{O}(h^2)$$\n        The formula for $D_{\\mathrm{fwd}}$ matches the first term on the right-hand side. The leading term of the truncation error is therefore $E_{T, \\mathrm{fwd}} \\approx -\\frac{h}{2}f''(x)$. The method is first-order accurate, with an error of order $\\mathcal{O}(h)$.\n\n    *   For the **central finite difference**, $D_{\\mathrm{ctr}}(f,x,h)$, we use two Taylor expansions:\n        $$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + \\mathcal{O}(h^4)$$\n        $$f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(x) + \\mathcal{O}(h^4)$$\n        Subtracting the second equation from the first gives:\n        $$f(x+h) - f(x-h) = 2hf'(x) + \\frac{h^3}{3}f'''(x) + \\mathcal{O}(h^5)$$\n        Rearranging for $f'(x)$ gives:\n        $$f'(x) = \\frac{f(x+h) - f(x-h)}{2h} - \\frac{h^2}{6}f'''(x) - \\mathcal{O}(h^4)$$\n        The leading truncation error is $E_{T, \\mathrm{ctr}} \\approx -\\frac{h^2}{6}f'''(x)$. The method is second-order accurate, with an error of order $\\mathcal{O}(h^2)$.\n\n    *   For the **complex-step approximation**, $D_{\\mathrm{cs}}(f,x,h)$, we assume $f$ is analytic and consider its Taylor series for a complex argument $x+ih$:\n        $$f(x+ih) = f(x) + (ih)f'(x) + \\frac{(ih)^2}{2!}f''(x) + \\frac{(ih)^3}{3!}f'''(x) + \\mathcal{O}(h^4)$$\n        $$f(x+ih) = \\left(f(x) - \\frac{h^2}{2}f''(x) + \\mathcal{O}(h^4)\\right) + i\\left(hf'(x) - \\frac{h^3}{6}f'''(x) + \\mathcal{O}(h^5)\\right)$$\n        Taking the imaginary part, $\\operatorname{Im}[\\cdot]$, and dividing by $h$:\n        $$\\frac{\\operatorname{Im}[f(x+ih)]}{h} = f'(x) - \\frac{h^2}{6}f'''(x) + \\mathcal{O}(h^4)$$\n        The leading truncation error is $E_{T, \\mathrm{cs}} \\approx -\\frac{h^2}{6}f'''(x)$. This method is also second-order accurate, with an error of order $\\mathcal{O}(h^2)$.\n\n*   **Round-off Error**: This is a computational error that arises from the finite precision of floating-point arithmetic, governed by machine epsilon, $\\epsilon_{\\text{mach}}$ (for IEEE $754$ double precision, $\\epsilon_{\\text{mach}} \\approx 2.22 \\times 10^{-16}$).\n\n    *   For $D_{\\mathrm{fwd}}$ and $D_{\\mathrm{ctr}}$, the numerators involve the subtraction of two nearly equal numbers as $h \\to 0$. This phenomenon, known as **subtractive cancellation**, leads to a catastrophic loss of significant figures. The round-off error $E_{R}$ for these methods is amplified as $h$ decreases, scaling as $E_{R} \\approx \\mathcal{O}(\\epsilon_{\\text{mach}}/h)$.\n\n    *   For $D_{\\mathrm{cs}}$, the calculation of $\\operatorname{Im}[f(x+ih)]$ does not involve subtraction of nearly equal numbers. For example, $\\operatorname{Im}[\\sin(x+ih)] = \\cos(x)\\sinh(h)$. For small $h$, $\\sinh(h)\\approx h$, so $\\operatorname{Im}[\\sin(x+ih)]\\approx h\\cos(x)$. There is no cancellation. Consequently, the round-off error is not amplified by small $h$ and remains on the order of $\\mathcal{O}(\\epsilon_{\\text{mach}})$.\n\n*   **Total Error and Optimal Step Size**: As $h$ decreases, truncation error decreases while round-off error increases for the real-valued methods. This trade-off results in an optimal step size $h_{\\text{opt}}$ that minimizes the total error.\n    *   For $D_{\\mathrm{fwd}}$, the total error $E_{\\mathrm{fwd}}(h) \\approx C_1 h + C_2 \\frac{\\epsilon_{\\text{mach}}}{h}$ is minimized at $h_{\\text{opt}} \\approx \\mathcal{O}(\\sqrt{\\epsilon_{\\text{mach}}})$. The minimum error is also $\\approx \\mathcal{O}(\\sqrt{\\epsilon_{\\text{mach}}})$.\n    *   For $D_{\\mathrm{ctr}}$, the total error $E_{\\mathrm{ctr}}(h) \\approx C_3 h^2 + C_4 \\frac{\\epsilon_{\\text{mach}}}{h}$ is minimized at $h_{\\text{opt}} \\approx \\mathcal{O}(\\epsilon_{\\text{mach}}^{1/3})$. The minimum error is $\\approx \\mathcal{O}(\\epsilon_{\\text{mach}}^{2/3})$.\n    *   For $D_{\\mathrm{cs}}$, the total error $E_{\\mathrm{cs}}(h) \\approx C_5 h^2 + C_6 \\epsilon_{\\text{mach}}$. Since the round-off term is not amplified, the error decreases with $h^2$ until it reaches a floor determined by machine precision. The smallest $h$ in the set $\\mathcal{H}$ will likely produce the minimum error, which will be close to $\\mathcal{O}(\\epsilon_{\\text{mach}})$.\n\n**2. Computational Strategy**\n\nThe solution algorithm is a direct implementation of the problem requirements. We must perform an exhaustive search over the given discrete set of step sizes $\\mathcal{H} = \\{10^{-k} : k \\in \\{1, 2, \\dots, 16\\}\\}$.\n\n1.  For each test case defined by the tuple $(f, f', x_0)$:\n2.  Pre-calculate the true derivative value $d_{\\text{exact}} = f'(x_0)$.\n3.  Initialize minimum error variables for each method, for instance, $\\min E_{\\mathrm{fwd}}$, $\\min E_{\\mathrm{ctr}}$, and $\\min E_{\\mathrm{cs}}$, to a value representing infinity.\n4.  Iterate through each step size $h$ in the set $\\mathcal{H}$.\n    a. For each $h$, compute the three derivative approximations: $D_{\\mathrm{fwd}}(f,x_0,h)$, $D_{\\mathrm{ctr}}(f,x_0,h)$, and $D_{\\mathrm{cs}}(f,x_0,h)$.\n    b. Calculate the absolute error for each approximation: $E(h) = |D(h) - d_{\\text{exact}}|$.\n    c. Compare the computed error with the current minimum for that method and update the minimum if the new error is smaller.\n5. After iterating through all $h \\in \\mathcal{H}$, the triplet of minimum errors $[\\min E_{\\mathrm{fwd}}, \\min E_{\\mathrm{ctr}}, \\min E_{\\mathrm{cs}}]$ constitutes the result for the given test case.\n6. Aggregate the results from all test cases into a final list structure as specified. The implementation will use the `numpy` library, which correctly handles complex-valued arguments for standard analytic functions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimum absolute error for three numerical differentiation methods\n    (forward difference, central difference, complex-step) over a range of step sizes.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (function, derivative_function, evaluation_point_x0).\n    # Numpy functions (exp, sin, cos) are used as they correctly handle complex inputs,\n    # which is required for the complex-step method.\n    test_cases = [\n        (np.exp, np.exp, 1.0),\n        (np.sin, np.cos, 1.0),\n        (np.cos, lambda x: -np.sin(x), 0.0)\n    ]\n\n    # Define the discrete set of step sizes H.\n    H = [10.0**(-k) for k in range(1, 17)]  # k from 1 to 16 inclusive.\n\n    all_case_results = []\n    for f, f_prime, x0 in test_cases:\n        # Calculate the exact derivative value for reference.\n        exact_derivative = f_prime(float(x0))\n\n        # Initialize minimum errors for the current test case.\n        min_err_fwd = float('inf')\n        min_err_ctr = float('inf')\n        min_err_cs = float('inf')\n\n        for h in H:\n            # 1. Forward finite difference\n            d_fwd = (f(x0 + h) - f(x0)) / h\n            err_fwd = abs(d_fwd - exact_derivative)\n            if err_fwd  min_err_fwd:\n                min_err_fwd = err_fwd\n\n            # 2. Central finite difference\n            d_ctr = (f(x0 + h) - f(x0 - h)) / (2 * h)\n            err_ctr = abs(d_ctr - exact_derivative)\n            if err_ctr  min_err_ctr:\n                min_err_ctr = err_ctr\n            \n            # 3. Complex-step approximation\n            # The imaginary unit in Python is 1j.\n            d_cs = np.imag(f(x0 + 1j * h)) / h\n            err_cs = abs(d_cs - exact_derivative)\n            if err_cs  min_err_cs:\n                min_err_cs = err_cs\n\n        # Collect the minimum errors for the current case.\n        all_case_results.append([min_err_fwd, min_err_ctr, min_err_cs])\n\n    # Format the final output string as per the problem specification.\n    # The output must be a single-line string representing a list of lists,\n    # with no spaces and with numbers in scientific notation to 12 significant digits.\n    # The format specifier \"{:.11e}\" ensures 1 digit before and 11 after the decimal point.\n    \n    formatted_case_strings = []\n    for case_res in all_case_results:\n        # Format each sublist of 3 floats into a string like \"[num1,num2,num3]\"\n        formatted_numbers = [f\"{num:.11e}\" for num in case_res]\n        formatted_case_strings.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the formatted case strings into the final output string \"[case1,case2,case3]\"\n    final_output = f\"[{','.join(formatted_case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}