{
    "hands_on_practices": [
        {
            "introduction": "The concept of an embedded error estimate is the heart of adaptive step-size control. This exercise provides a foundational, hands-on opportunity to see exactly how this estimate relates to the true local error by calculating both analytically for a single step of a simple Runge-Kutta pair . By working through this calculation, you will demystify the error estimation process and build a concrete understanding of what the solver is \"measuring\" at each step.",
            "id": "2388725",
            "problem": "Consider the initial value problem for an Ordinary Differential Equation (ODE) with a piecewise-polynomial right-hand side:\n$$\n\\frac{dy}{dt} = f(t), \\quad y(0)=0,\n$$\nwhere\n$$\nf(t) = \\begin{cases}\nt^{2},  0 \\le t \\le \\frac{1}{2} \\\\[6pt]\n\\frac{1}{4} + \\left(t - \\frac{1}{2}\\right),  \\frac{1}{2}  t \\le 1\n\\end{cases}\n$$\nThis right-hand side ensures that the analytical solution $y(t)$ is a simple piecewise polynomial on $[0,1]$.\n\nA numerical solver employs an embedded Runge-Kutta (RK) pair consisting of the first-order explicit Euler method and the second-order Heun method. For a single step from $t_{0}=0$ to $t_{1}=h$ with $0h\\frac{1}{2}$, the pair is defined by\n$$\nk_{1} = f(t_{0}, y_{0}), \\quad k_{2} = f\\!\\left(t_{0}+h,\\, y_{0} + h k_{1}\\right)\n$$,\n$$\ny^{[1]}_{1} = y_{0} + h k_{1}, \\quad y^{[2]}_{1} = y_{0} + \\frac{h}{2}\\left(k_{1}+k_{2}\\right)\n$$,\nand the solver’s embedded local error estimate is\n$$\n\\hat{e}_{1} = y^{[2]}_{1} - y^{[1]}_{1}.\n$$\n\nUsing the exact solution $y(t)$ (i.e., starting the step from the exact value $y_{0}=y(0)$), determine the exact ratio\n$$\nR = \\frac{y(t_{1}) - y^{[1]}_{1}}{\\hat{e}_{1}},\n$$\nthat is, the ratio of the true local truncation error of the first-order update to the solver’s embedded estimate for this step. Provide your answer as a single reduced fraction. No rounding is required.",
            "solution": "The problem statement is validated and found to be scientifically grounded, well-posed, and objective. It is a standard problem in the numerical analysis of ordinary differential equations, containing all necessary information for a unique solution. There are no contradictions or ambiguities. We shall proceed with the derivation.\n\nThe task is to determine the exact ratio $R = \\frac{y(t_{1}) - y^{[1]}_{1}}{\\hat{e}_{1}}$, where $y(t_{1})$ is the exact solution at time $t_{1}$, $y^{[1]}_{1}$ is the numerical approximation from the first-order method, and $\\hat{e}_{1}$ is the embedded error estimate. The single step is from $t_{0}=0$ to $t_{1}=h$, with the constraint $0  h  \\frac{1}{2}$. The initial condition is $y(0)=0$.\n\nFirst, we determine the exact solution $y(t)$ of the initial value problem (IVP). The IVP is given by\n$$\n\\frac{dy}{dt} = f(t), \\quad y(0)=0.\n$$\nFor the interval of integration $[0, h]$, since $h  \\frac{1}{2}$, the right-hand side function is given by the first case:\n$$\nf(t) = t^{2}, \\quad \\text{for } t \\in [0, h].\n$$\nWe solve this simplified ODE by direct integration:\n$$\ny(t) = \\int_{0}^{t} f(s) ds = \\int_{0}^{t} s^{2} ds = \\left[ \\frac{s^{3}}{3} \\right]_{0}^{t} = \\frac{t^{3}}{3}.\n$$\nThe exact value of the solution at the end of the step, $t_{1}=h$, is\n$$\ny(t_{1}) = y(h) = \\frac{h^{3}}{3}.\n$$\n\nNext, we compute the numerical approximations using the provided embedded Runge-Kutta pair. The step begins from $(t_{0}, y_{0})$, where $t_{0}=0$ and $y_{0}=y(0)=0$.\n\nThe stages $k_{1}$ and $k_{2}$ are calculated as follows:\nThe first stage is:\n$$\nk_{1} = f(t_{0}, y_{0}) = f(0, 0) = 0^{2} = 0.\n$$\nNote that for this problem, $f$ is a function of $t$ only, so $f(t, y) = f(t)$.\nThe second stage is:\n$$\nk_{2} = f(t_{0}+h, y_{0} + h k_{1}) = f(0+h, 0 + h \\cdot 0) = f(h) = h^{2}.\n$$\n\nNow we find the two numerical approximations for the solution at $t_{1}=h$.\nThe first-order approximation, $y^{[1]}_{1}$, using the explicit Euler method is:\n$$\ny^{[1]}_{1} = y_{0} + h k_{1} = 0 + h(0) = 0.\n$$\nThe second-order approximation, $y^{[2]}_{1}$, using the Heun method is:\n$$\ny^{[2]}_{1} = y_{0} + \\frac{h}{2}(k_{1}+k_{2}) = 0 + \\frac{h}{2}(0 + h^{2}) = \\frac{h^{3}}{2}.\n$$\n\nWith these results, we can calculate the terms required for the ratio $R$.\nThe numerator is the true local truncation error of the first-order (Euler) method:\n$$\ny(t_{1}) - y^{[1]}_{1} = y(h) - y^{[1]}_{1} = \\frac{h^{3}}{3} - 0 = \\frac{h^{3}}{3}.\n$$\nThe denominator is the solver's embedded local error estimate:\n$$\n\\hat{e}_{1} = y^{[2]}_{1} - y^{[1]}_{1} = \\frac{h^{3}}{2} - 0 = \\frac{h^{3}}{2}.\n$$\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{y(t_{1}) - y^{[1]}_{1}}{\\hat{e}_{1}} = \\frac{\\frac{h^{3}}{3}}{\\frac{h^{3}}{2}}.\n$$\nThe term $h^{3}$ cancels from the numerator and the denominator, as $h > 0$:\n$$\nR = \\frac{1/3}{1/2} = \\frac{1}{3} \\cdot \\frac{2}{1} = \\frac{2}{3}.\n$$\nThis result is independent of the step size $h$ (within the specified range) and is a reduced fraction, as required.",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "A correct local error estimate is the foundation of a reliable adaptive solver, guiding the step-size selection to achieve a desired global accuracy. This practice challenges you to implement an adaptive integrator and directly observe the consequences of a flawed error estimation strategy on the solver's performance . This coding exercise will illuminate the critical link between the local error control mechanism and the global accuracy of the final solution.",
            "id": "2388676",
            "problem": "Design and implement an adaptive time step integrator based on an embedded Runge-Kutta pair to study how an incorrect local error estimate affects the global error. Work purely in the mathematical setting of an initial value problem for an ordinary differential equation. The fundamental base you must use is the definition of an initial value problem, local truncation error, and the structure of explicit Runge-Kutta methods.\n\nProblem requirements:\n- Consider the initial value problem for a scalar ordinary differential equation given by $y'(t) = f(t,y(t))$ with $y(t_{0}) = y_{0}$. The goal is to advance from $t_{0}$ to a final time $T$.\n- Use an explicit embedded Runge-Kutta pair, where two formulas of different orders share the same internal stages to provide two approximations $y_{n+1}$ and $\\hat{y}_{n+1}$ at each step. The higher-order approximation $y_{n+1}$ is used as the step result, and the difference $e_{n+1} = y_{n+1} - \\hat{y}_{n+1}$ is used as a local error estimate for step-size control.\n- Implement a specific, widely used pair of orders $p$ and $q$ with $p > q$; for concreteness and reproducibility, use the Bogacki-Shampine pair of orders $p=q+1$, namely $p = 3$ and $q = 2$. Use the higher-order approximation to advance the solution when a step is accepted.\n- Implement two versions of the adaptive controller:\n  1. The correct controller computes the local error estimate $e_{n+1}$ using the correct embedded coefficients for the lower-order formula.\n  2. The flawed controller computes a defective local error estimate $\\hat{e}_{n+1}$ by intentionally altering the embedded lower-order weights as follows: swap the last two weights of the lower-order rule before forming the difference. In other words, if the lower-order weights are $\\{b_{1}^{(q)}, b_{2}^{(q)}, b_{3}^{(q)}, b_{4}^{(q)}\\}$, the flawed controller uses $\\{b_{1}^{(q)}, b_{2}^{(q)}, b_{4}^{(q)}, b_{3}^{(q)}\\}$ when constructing $\\hat{y}_{n+1}$, hence $\\hat{e}_{n+1} = y_{n+1} - \\hat{y}_{n+1}$. Use this flawed estimate only for step acceptance and step-size selection, while still advancing the solution with the higher-order approximation $y_{n+1}$ upon acceptance.\n- Use a standard error norm for a scalar state:\n  $$\\mathrm{err\\_norm} = \\frac{|e_{n+1}|}{\\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(|y_{n}|, |y_{n+1}|)}.$$\n  Accept a step if $\\mathrm{err\\_norm} \\le 1$.\n- Use a step-size controller derived from the scaling of local truncation error for an order-$p$ method. If a step is accepted or rejected, propose a new step size\n  $$h_{\\mathrm{new}} = h \\cdot s \\cdot \\mathrm{err\\_norm}^{-1/(p+1)},$$\n  where $s$ is a safety factor. Clip $h_{\\mathrm{new}}$ to lie within $[\\alpha_{\\min} h, \\alpha_{\\max} h]$ to avoid erratic changes. Use $p = 3$, $s = 0.9$, $\\alpha_{\\min} = 0.2$, and $\\alpha_{\\max} = 5$.\n- Use the high-order formula of the Bogacki-Shampine pair to advance the solution on accepted steps. Use the same initial step size and controller parameters for both controllers.\n\nTest problem and exact solution:\n- Let $f(t,y) = \\lambda y$ with $\\lambda  0$, $y(0) = 1$, and exact solution $y(t) = \\exp(\\lambda t)$.\n- Define the global error at the final time $T$ as\n  $$E = |y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)|.$$\n\nTest suite:\nProvide results for the following parameter sets, which form the test suite (all numbers below are dimensionless):\n- Case A (happy path): $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-3}$, $\\mathrm{atol} = 10^{-12}$.\n- Case B (tighter tolerance): $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n- Case C (more rapidly decaying dynamics): $\\lambda = -5$, $T = 2$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n\nWhat to compute for each case:\n- Compute two floats:\n  - $E_{\\mathrm{correct}}$: the final-time absolute error using the correct controller.\n  - $E_{\\mathrm{flawed}}$: the final-time absolute error using the flawed controller.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list $[E_{\\mathrm{correct}}, E_{\\mathrm{flawed}}]$. For example: [[EcA,EfA],[EcB,EfB],[EcC,EfC]]. Print numeric values as decimals in a consistent, compact format.",
            "solution": "The supplied problem is subjected to rigorous validation.\n\n**Step 1: Extract Givens**\n- **Initial Value Problem (IVP)**: A scalar ordinary differential equation (ODE) $y'(t) = f(t,y(t))$ with initial condition $y(t_{0}) = y_{0}$ is to be integrated from $t_{0}$ to a final time $T$.\n- **Numerical Method**: An explicit embedded Runge-Kutta pair of orders $p=3$ and $q=2$, specifically the Bogacki-Shampine pair. The higher-order ($p=3$) approximation, $y_{n+1}$, is used to advance the solution.\n- **Local Error Estimate**: The difference $e_{n+1} = y_{n+1} - \\hat{y}_{n+1}$ between the higher-order ($y_{n+1}$) and lower-order ($\\hat{y}_{n+1}$) approximations serves as the estimate of local truncation error.\n- **Controllers**:\n    1.  **Correct Controller**: Uses the standard local error estimate $e_{n+1}$.\n    2.  **Flawed Controller**: Uses a defective estimate $\\hat{e}_{n+1} = y_{n+1} - \\hat{y}_{n+1, \\text{flawed}}$, where $\\hat{y}_{n+1, \\text{flawed}}$ is computed using lower-order weights where the last two elements are swapped. This flawed estimate is used only for step-size control.\n- **Error Norm  Acceptance**: The scaled error norm is defined as $\\mathrm{err\\_norm} = |e_{n+1}| / (\\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(|y_{n}|, |y_{n+1}|))$. A step is accepted if $\\mathrm{err\\_norm} \\le 1$.\n- **Step-Size Control Law**: The new step size is proposed as $h_{\\mathrm{new}} = h \\cdot s \\cdot \\mathrm{err\\_norm}^{-1/(p+1)}$, where $s = 0.9$ and $p = 3$. The result is clipped to the range $[\\alpha_{\\min} h, \\alpha_{\\max} h]$ with $\\alpha_{\\min} = 0.2$ and $\\alpha_{\\max} = 5$.\n- **Test Problem**: $f(t,y) = \\lambda y$ with $y(0) = 1$. The exact solution is $y(t) = \\exp(\\lambda t)$.\n- **Global Error Metric**: $E = |y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)|$.\n- **Test Cases**:\n    - Case A: $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-3}$, $\\mathrm{atol} = 10^{-12}$.\n    - Case B: $\\lambda = -1$, $T = 10$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n    - Case C: $\\lambda = -5$, $T = 2$, $y_{0} = 1$, $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-12}$.\n- **Required Computations**: For each case, calculate the final-time global error for both the correct controller ($E_{\\mathrm{correct}}$) and the flawed controller ($E_{\\mathrm{flawed}}$).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically sound. It addresses a fundamental topic in computational engineering: the design and analysis of adaptive numerical methods for ODEs. All parameters and procedures are specified with sufficient precision to permit a unique implementation. The only unspecified parameter is the initial step size, $h_0$. This is a minor omission. For a reproducible comparison between the two controllers, it is sufficient to use the same reasonable initial step size for both. I will proceed by assuming a small, fixed initial step size, $h_0$, for all simulations, which is a standard approach. The problem is therefore deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n**Fundamental Principles and Method**\n\nThe problem requires the numerical integration of the IVP $y'(t) = f(t,y(t))$, $y(t_0) = y_0$. This is achieved using an adaptive step-size Runge-Kutta method.\n\nAn explicit Runge-Kutta method computes the solution at time $t_{n+1} = t_n + h_n$ from the solution at $t_n$ by means of a series of intermediate stage evaluations. For an $s$-stage method, we have:\n$$k_i = f\\left(t_n + c_i h_n, y_n + h_n \\sum_{j=1}^{i-1} a_{ij} k_j\\right), \\quad i=1, \\dots, s$$\n$$y_{n+1} = y_n + h_n \\sum_{i=1}^s b_i k_i$$\nThe coefficients $c_i$, $a_{ij}$, and $b_i$ define the specific method.\n\nAn embedded pair provides two solutions, $y_{n+1}$ of order $p$ and $\\hat{y}_{n+1}$ of order $q  p$, using the same set of stage values $k_i$.\n$$y_{n+1} = y_n + h_n \\sum_{i=1}^s b_i k_i \\quad (\\text{order } p)$$\n$$\\hat{y}_{n+1} = y_n + h_n \\sum_{i=1}^s \\hat{b}_i k_i \\quad (\\text{order } q)$$\nThe difference, $e_{n+1} = y_{n+1} - \\hat{y}_{n+1} = h_n \\sum_{i=1}^s (b_i - \\hat{b}_i)k_i$, provides an estimate of the local truncation error of the lower-order method. This error estimate is used to control the step size $h_n$.\n\n**The Bogacki-Shampine 3(2) Pair**\n\nThe specified Bogacki-Shampine method is a 3-stage method that produces a 3rd-order solution and a 2nd-order solution. It possesses the First Same As Last (FSAL) property, meaning the final stage evaluation of one step can be reused as the first stage of the subsequent step, increasing efficiency. Its structure is as follows:\n\n1.  Compute three intermediate stages:\n    $$k_1 = f(t_n, y_n)$$\n    $$k_2 = f(t_n + \\frac{1}{2}h, y_n + \\frac{1}{2}h k_1)$$\n    $$k_3 = f(t_n + \\frac{3}{4}h, y_n + \\frac{3}{4}h k_2)$$\n2.  Compute the 3rd-order approximation, which is used to advance the solution:\n    $$y_{n+1} = y_n + h\\left(\\frac{2}{9}k_1 + \\frac{1}{3}k_2 + \\frac{4}{9}k_3\\right)$$\n3.  Compute a fourth stage using the advanced solution $y_{n+1}$. This is the FSAL stage.\n    $$k_4 = f(t_n + h, y_{n+1})$$\n4.  Compute the 2nd-order approximation for error estimation:\n    $$\\hat{y}_{n+1} = y_n + h\\left(\\frac{7}{24}k_1 + \\frac{1}{4}k_2 + \\frac{1}{3}k_3 + \\frac{1}{8}k_4\\right)$$\nThe local error estimate is $e_{n+1} = y_{n+1} - \\hat{y}_{n+1}$.\n\n**Adaptive Step-Size Control**\n\nThe goal of the controller is to adjust the step size $h$ such that the local error estimate satisfies a given tolerance. The error is scaled relative to the solution magnitude:\n$$\\mathrm{err\\_norm} = \\frac{|e_{n+1}|}{\\mathrm{atol} + \\mathrm{rtol} \\cdot \\max(|y_{n}|, |y_{n+1}|)}$$\nwhere $\\mathrm{atol}$ and $\\mathrm{rtol}$ are absolute and relative error tolerances.\n\nA step is accepted if $\\mathrm{err\\_norm} \\le 1$. If accepted, the solution is advanced: $t_{n+1} = t_n + h$, $y_{n+1} = y_{n+1}$. If rejected, the step is re-attempted with a smaller $h$.\n\nIn either case, a new step size $h_{\\mathrm{new}}$ is proposed. Based on the assumption that the local error behaves as $C \\cdot h^{p+1}$, the optimal step size is derived from the current error:\n$$h_{\\mathrm{new}} = h \\cdot s \\cdot \\left(\\frac{1}{\\mathrm{err\\_norm}}\\right)^{1/(p+1)}$$\nHere, $p=3$ is the order of the method used for step-size prediction, and $s=0.9$ is a safety factor to ensure robustness. The new step size is clipped to avoid excessively large or small changes: $h_{\\mathrm{new}}$ is constrained to $[\\alpha_{\\min} h, \\alpha_{\\max} h] = [0.2h, 5.0h]$.\n\n**The Flawed Controller**\n\nThe problem mandates an investigation into a flawed controller. The flaw is introduced in the local error estimation. The correct weights for the 2nd-order method are $\\hat{\\mathbf{b}} = [7/24, 1/4, 1/3, 1/8]$. The flaw consists of swapping the last two weights:\n$$\\hat{\\mathbf{b}}_{\\text{flawed}} = [7/24, 1/4, 1/8, 1/3]$$\nThis leads to a flawed 2nd-order approximation:\n$$\\hat{y}_{n+1, \\text{flawed}} = y_n + h\\left(\\frac{7}{24}k_1 + \\frac{1}{4}k_2 + \\frac{1}{8}k_3 + \\frac{1}{3}k_4\\right)$$\nThe flawed error estimate is $\\hat{e}_{n+1} = y_{n+1} - \\hat{y}_{n+1, \\text{flawed}}$. This $\\hat{e}_{n+1}$ is then used in the error norm calculation and the subsequent step-size adjustment. It is critical to note that even with the flawed controller, the solution is advanced using the correct 3rd-order formula for $y_{n+1}$. The flaw only affects the adaptive mechanism, not the propagation formula itself.\n\n**Algorithm and Implementation**\n\nThe core of the implementation will be an `adaptive_integrator` function that executes the following loop:\n1.  Initialize $t=t_0$, $y=y_0$, and an initial step size $h=h_0$. We will use $h_0 = 10^{-2}$. Compute the first stage $k_1 = f(t_0, y_0)$.\n2.  Begin the main loop, which continues as long as $t  T$.\n3.  Inside the loop, start a sub-loop for step acceptance.\n4.  In the sub-loop, compute stages $k_2, k_3$, the 3rd-order solution $y_{n+1}$, and the final stage $k_4$.\n5.  Based on the controller type (correct or flawed), calculate the appropriate local error estimate ($e_{n+1}$ or $\\hat{e}_{n+1}$).\n6.  Compute the error norm $\\mathrm{err\\_norm}$.\n7.  If $\\mathrm{err\\_norm} \\le 1$, the step is accepted. Advance time and the solution ($t \\leftarrow t+h, y \\leftarrow y_{n+1}$). The stage $k_4$ becomes the new $k_1$ for the next step (FSAL). Exit the sub-loop.\n8.  If $\\mathrm{err\\_norm} > 1$, the step is rejected. The current $y_{n+1}$ is discarded. The sub-loop continues with a new, smaller step size $h$.\n9.  In both acceptance and rejection cases, calculate a new proposed step size $h_{\\mathrm{new}}$ using the control law and clipping, and update $h$.\n10. The main loop terminates when $t \\ge T$. A final adjustment is made to the last step size to ensure integration stops exactly at $T$.\n\nThis procedure will be executed for each test case, once with the correct controller and once with the flawed one. The final global error $E = |y_{\\mathrm{num}}(T) - \\exp(\\lambda T)|$ is then computed and reported. The flawed controller's behavior (whether it is more or less aggressive than the correct one) will determine whether the final global error is larger or smaller. This experiment demonstrates the sensitivity of the global error to the correctness of the local error estimator, which underpins the reliability of adaptive solvers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef adaptive_integrator(f, t0, y0, T, rtol, atol, h0, flawed_controller):\n    \"\"\"\n    Integrates an ODE using the adaptive Bogacki-Shampine 3(2) method.\n    \"\"\"\n    # Bogacki-Shampine coefficients and controller parameters\n    # The method structure:\n    # k1 = f(t, y)\n    # k2 = f(t + 1/2 h, y + 1/2 h k1)\n    # k3 = f(t + 3/4 h, y + 3/4 h k2)\n    # y_next = y + h * (2/9 k1 + 1/3 k2 + 4/9 k3)  (order 3 solution)\n    # k4 = f(t + h, y_next)                          (FSAL stage)\n    # error = y_next - y_hat_next, where y_hat_next is the order 2 solution.\n    # The error is computed using a difference of weights for better numerical stability.\n    \n    b3_weights = np.array([2/9, 1/3, 4/9, 0])      # Order 3 weights\n    b2_weights_correct = np.array([7/24, 1/4, 1/3, 1/8]) # Order 2 weights\n    b2_weights_flawed = np.array([7/24, 1/4, 1/8, 1/3])  # Flawed order 2 weights\n\n    if flawed_controller:\n        err_weights = b3_weights - b2_weights_flawed\n    else:\n        err_weights = b3_weights - b2_weights_correct\n\n    s = 0.9\n    p = 3\n    alpha_min = 0.2\n    alpha_max = 5.0\n\n    t = t0\n    y = y0\n    h = h0\n\n    # First stage evaluation (k1) for the first step\n    k1 = f(t, y)\n\n    while t  T:\n        if t + h > T:\n            h = T - t  # Adjust last step to hit T exactly\n\n        step_accepted = False\n        while not step_accepted:\n            # Prevent infinitely small step size\n            if abs(h)  1e-15 * T:\n                raise RuntimeError(\"Step size has become excessively small.\")\n\n            # Compute stages for BS(3,2)\n            k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n            k3 = f(t + 0.75 * h, y + 0.75 * h * k2)\n            y_next = y + h * (b3_weights[0] * k1 + b3_weights[1] * k2 + \n                               b3_weights[2] * k3)\n            k4 = f(t + h, y_next)\n\n            # Calculate local error estimate\n            local_error = h * (err_weights[0] * k1 + err_weights[1] * k2 + \n                               err_weights[2] * k3 + err_weights[3] * k4)\n\n            # Calculate scaled error norm\n            y_scale = atol + rtol * max(abs(y), abs(y_next))\n            err_norm = abs(local_error) / y_scale if y_scale > 0 else 0\n\n            # Step acceptance logic\n            if err_norm = 1.0:\n                step_accepted = True\n                t += h\n                y = y_next\n                # FSAL: k4 of this step is k1 of the next\n                k1 = k4\n                \n                # Update step size for the next step\n                if err_norm == 0:\n                    # Avoid division by zero and propose max increase\n                    h_new = h * alpha_max\n                else:\n                    h_new = h * s * (err_norm ** (-1.0 / (p + 1.0)))\n            else:\n                # Step rejected, reduce step size and retry\n                h_new = h * s * (err_norm ** (-1.0 / (p + 1.0)))\n\n            # Clip the new step size\n            h = max(h * alpha_min, min(h * alpha_max, h_new))\n\n    return y\n\ndef solve():\n    \"\"\"\n    Runs the simulation for the specified test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'lambda': -1.0, 'T': 10.0, 'y0': 1.0, 'rtol': 1e-3, 'atol': 1e-12},\n        # Case B\n        {'lambda': -1.0, 'T': 10.0, 'y0': 1.0, 'rtol': 1e-6, 'atol': 1e-12},\n        # Case C\n        {'lambda': -5.0, 'T': 2.0, 'y0': 1.0, 'rtol': 1e-6, 'atol': 1e-12},\n    ]\n\n    results = []\n    h0 = 1e-2  # Fixed initial step size for all runs for fair comparison\n\n    for case in test_cases:\n        lambda_val = case['lambda']\n        T = case['T']\n        y0 = case['y0']\n        rtol = case['rtol']\n        atol = case['atol']\n\n        # Define the ODE and its exact solution\n        f = lambda t, y: lambda_val * y\n        y_exact_func = lambda t: np.exp(lambda_val * t)\n        \n        y_exact_T = y_exact_func(T)\n\n        # Run with correct controller\n        y_num_correct = adaptive_integrator(f, 0, y0, T, rtol, atol, h0, flawed_controller=False)\n        E_correct = abs(y_num_correct - y_exact_T)\n\n        # Run with flawed controller\n        y_num_flawed = adaptive_integrator(f, 0, y0, T, rtol, atol, h0, flawed_controller=True)\n        E_flawed = abs(y_num_flawed - y_exact_T)\n\n        results.append([E_correct, E_flawed])\n\n    # Format the output string as specified\n    output_str = '[' + ','.join([f'[{r[0]:.5e},{r[1]:.5e}]' for r in results]) + ']'\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The theoretical guarantees of Runge-Kutta methods rely on certain smoothness properties of the differential equation's right-hand side. This exercise explores what happens when these assumptions break down, a common scenario when encountering singularities . By analyzing this situation, you will develop a deeper appreciation for the operational limits of standard adaptive solvers and learn to anticipate their failure modes.",
            "id": "2388728",
            "problem": "An explicit adaptive integrator based on an embedded Runge-Kutta pair of orders $p$ and $p+1$ advances the solution of the initial value problem $y^{\\prime}(t)=f(t,y)$, $y(t_{0})=y_{0}$, by computing two approximations at each step, $y_{n+1}^{[p]}$ and $y_{n+1}^{[p+1]}$, and using their difference as a local error estimate. The step is accepted if the scaled norm of the error estimate is below $1$, and the next stepsize is adjusted accordingly.\n\nConsider the autonomous problem $y^{\\prime}(t)=-\\sqrt{y(t)}$ with initial condition $y(0)=1$, where $\\sqrt{\\cdot}$ denotes the principal square root and the state domain is $y\\ge 0$. The exact solution reaches $y=0$ at a finite time $t^{\\ast}=2$, after which the right-hand side $f(t,y)$ is undefined for $y0$.\n\nWhich statement best describes the expected behavior of a standard explicit adaptive embedded Runge-Kutta solver as it approaches $t^{\\ast}$ under fixed absolute and relative tolerances?\n\nA. The controller decreases the stepsize aggressively; if a trial step evaluates stages with $y0$ so that $f$ is not defined, the step is rejected and the stepsize is reduced. Repeated rejections can drive the stepsize toward the minimum allowed value, at which point the solver terminates with a failure (for example, “step size underflow” or “singularity likely”).\n\nB. Because $f(t,y)$ is small near $y=0$, the local error estimate becomes small, so the controller increases the stepsize and steps past $t^{\\ast}$ in a single large step without rejections.\n\nC. The embedded pair uses the difference between orders to extrapolate the hitting time and lands exactly at $y=0$ regardless of the current stepsize, so no step rejections occur.\n\nD. Upon detecting the non-Lipschitz point, the solver automatically switches to an implicit method and continues through $y=0$ with the same tolerance and without reducing the stepsize.",
            "solution": "The supplied problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- Integrator type: Explicit adaptive integrator based on an embedded Runge-Kutta pair.\n- Orders of the pair: $p$ and $p+1$.\n- Approximations computed per step: $y_{n+1}^{[p]}$ and $y_{n+1}^{[p+1]}$.\n- Local error estimate: Derived from the difference $y_{n+1}^{[p+1]} - y_{n+1}^{[p]}$.\n- Step control logic: The step is accepted if the scaled norm of the error estimate is less than $1$. The subsequent stepsize is adjusted.\n- Initial Value Problem (IVP): $y^{\\prime}(t)=-\\sqrt{y(t)}$.\n- Initial Condition: $y(0)=1$.\n- State Domain: $y \\ge 0$.\n- RHS definition: $f(t,y) = -\\sqrt{y(t)}$ is undefined for $y0$.\n- Exact solution property: The solution reaches $y=0$ at a finite time $t^{\\ast}=2$.\n- Question: Describe the expected behavior of a standard explicit adaptive embedded Runge-Kutta solver as it approaches $t^{\\ast}$.\n\nStep 2: Validate Using Extracted Givens\n1.  **Scientific Groundedness**: The problem is scientifically sound. The IVP $y^{\\prime}=-\\sqrt{y}$, $y(0)=1$ is a standard example in the study of ordinary differential equations. The exact solution can be found by separation of variables:\n    $$ \\int y^{-1/2} dy = \\int -dt $$\n    $$ 2y^{1/2} = -t + C $$\n    Applying the initial condition $y(0)=1$:\n    $$ 2(1)^{1/2} = -0 + C \\implies C=2 $$\n    The solution is $y(t) = (1 - \\frac{t}{2})^2$. This solution reaches $y=0$ when $1 - \\frac{t}{2} = 0$, which gives $t=t^{\\ast}=2$. The problem statement is factually correct. The right-hand side $f(y) = -\\sqrt{y}$ is not Lipschitz continuous at $y=0$, as its derivative $f^{\\prime}(y) = -\\frac{1}{2\\sqrt{y}}$ is singular at $y=0$. The behavior of numerical methods for such problems is a well-established topic in numerical analysis.\n\n2.  **Well-Posedness**: The question asks to describe the behavior of a standard numerical algorithm under specific, well-defined conditions. This is a predictive analysis question, which has a definite answer based on the theory of numerical ODE solvers. The problem is well-posed.\n\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language. Terms like \"explicit adaptive integrator\" and \"embedded Runge-Kutta pair\" have standard, unambiguous meanings in computational science.\n\nStep 3: Verdict and Action\nThe problem statement is valid. It is scientifically grounded, well-posed, and objective. A solution will be derived.\n\nThe core of the problem lies in the behavior of the right-hand side function $f(t,y) = -\\sqrt{y}$ near $y=0$. The theory governing the error of Runge-Kutta methods relies on the smoothness of $f$. The local truncation error (LTE) of a method of order $p$ is proportional to $h^{p+1}$ and higher-order derivatives of the solution $y(t)$. These derivatives of $y(t)$ can be expressed in terms of $f$ and its partial derivatives with respect to $y$ using the chain rule. For example:\n$$ y^{\\prime\\prime}(t) = \\frac{d}{dt}f(t,y(t)) = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial y}y^{\\prime}(t) = \\frac{\\partial f}{\\partial y}f(t,y) $$\nIn our case, $f$ depends only on $y$, so $y^{\\prime\\prime}(t) = f^{\\prime}(y)f(y)$. The higher derivatives of the solution will involve higher derivatives of $f$. The derivatives of $f(y)=-\\sqrt{y}$ are:\n$$ f^{\\prime}(y) = -\\frac{1}{2}y^{-1/2} $$\n$$ f^{\\prime\\prime}(y) = \\frac{1}{4}y^{-3/2} $$\n$$ f^{(k)}(y) \\propto y^{(1/2)-k} $$\nAll derivatives of $f$ with respect to $y$ are singular at $y=0$, approaching $\\pm\\infty$ as $y \\to 0^{+}$. This violates the smoothness assumptions required for the standard derivation of the order and the local error estimate of a Runge-Kutta method.\n\nThe error estimate $E_{n+1} = y_{n+1}^{[p+1]} - y_{n+1}^{[p]}$ is an approximation of the LTE of the lower-order method. Its magnitude is assumed to be $\\|E_{n+1}\\| \\approx C h^{p+1}$ for a well-behaved (bounded) principal error function. However, as $y \\to 0$, the terms involving derivatives of $f$ in this function grow without bound. Consequently, for a given stepsize $h$, the magnitude of the error estimate $\\|E_{n+1}\\|$ will become much larger than what is predicted by the standard theory.\n\nThe stepsize controller adjusts the next stepsize, $h_{new}$, based on the current stepsize, $h_{old}$, and the error estimate, $E_{old}$, typically via a formula like:\n$$ h_{new} = \\text{safety\\_factor} \\times h_{old} \\times \\left( \\frac{\\text{Tolerance}}{\\|E_{old}\\|} \\right)^{1/(p+1)} $$\nAs the integration approaches $t^{\\ast}=2$, the solution $y$ approaches $0$. The error estimate $\\|E_{old}\\|$ will grow very large due to the singularity. The ratio $\\text{Tolerance}/\\|E_{old}\\|$ will become a very small number, forcing the controller to aggressively reduce the stepsize $h_{new}$.\n\nFurthermore, an explicit Runge-Kutta method computes a new solution $y_{n+1}$ using stage values. A generic stage calculation is of the form $k_i = f(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j)$. Since $y^{\\prime}0$, the solution is decreasing. The stage derivatives $k_j = -\\sqrt{y_{\\text{stage},j}}$ will be negative. The argument for a subsequent stage evaluation will be $y_n - h \\times (\\text{positive value})$. If the stepsize $h$ is not sufficiently small, it is possible for this argument to become negative. When this occurs, the evaluation $f(\\dots) = -\\sqrt{\\text{negative value}}$ fails, as the function is undefined in this domain for real arithmetic. This results in a step rejection. Following a rejection, the standard procedure is to reduce the stepsize and retry the step.\n\nThis combination of events—error estimates blowing up and stage evaluations failing—creates a feedback loop. The stepsize is reduced, the solver advances a tiny amount, the new $y$ is even closer to $0$, the next step faces an even more severe singularity, leading to another rejection or an unacceptably large error, which in turn causes further stepsize reduction. This process continues until the stepsize falls below a prescribed minimum value, at which point the solver aborts and reports a failure, often signaling a potential singularity.\n\nEvaluation of the Options:\n\nA. The controller decreases the stepsize aggressively; if a trial step evaluates stages with $y0$ so that $f$ is not defined, the step is rejected and the stepsize is reduced. Repeated rejections can drive the stepsize toward the minimum allowed value, at which point the solver terminates with a failure (for example, “step size underflow” or “singularity likely”).\nThis description aligns perfectly with the analysis above. The non-Lipschitz nature of $f$ at $y=0$ causes the error estimator to report large errors, forcing stepsize reduction. The explicit nature of the scheme can lead to trial steps with negative arguments for the square root, causing rejections and further stepsize reduction. This cascade leads to termination with a stepsize underflow error.\n**Verdict: Correct.**\n\nB. Because $f(t,y)$ is small near $y=0$, the local error estimate becomes small, so the controller increases the stepsize and steps past $t^{\\ast}$ in a single large step without rejections.\nThis is fundamentally incorrect. The local error estimate does not depend solely on the magnitude of $f$, but on its derivatives (and those of the solution). As shown, these derivatives are singular at $y=0$. The error estimate will not become small; it will grow, forcing a decrease, not an increase, in the stepsize.\n**Verdict: Incorrect.**\n\nC. The embedded pair uses the difference between orders to extrapolate the hitting time and lands exactly at $y=0$ regardless of the current stepsize, so no step rejections occur.\nThis describes a sophisticated feature known as event detection or root-finding, which is not a component of a \"standard\" explicit adaptive integrator. A standard integrator uses the embedded pair purely for error estimation and stepsize control. It has no intrinsic mechanism to detect and land precisely on a root of a function like $y(t)=0$.\n**Verdict: Incorrect.**\n\nD. Upon detecting the non-Lipschitz point, the solver automatically switches to an implicit method and continues through $y=0$ with the same tolerance and without reducing the stepsize.\nA standard explicit solver is just that: explicit. It does not contain code for an implicit method, nor a triggering logic to switch to one. Such hybrid solvers exist but are highly specialized, not \"standard\". Furthermore, even an implicit method would face challenges with the singularity and would likely need to reduce the stepsize. This statement is factually wrong on multiple levels regarding the capabilities of a standard explicit solver.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}