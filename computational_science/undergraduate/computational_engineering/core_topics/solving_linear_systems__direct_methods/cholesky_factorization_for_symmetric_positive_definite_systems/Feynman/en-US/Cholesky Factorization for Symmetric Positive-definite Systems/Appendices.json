{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, there is no substitute for working through its mechanics by hand. This exercise  challenges you to derive the Cholesky factor for a specific tridiagonal matrix, a structure that frequently appears in computational engineering, for example in discretized models of heat flow. By setting up and solving recurrence relations for the entries of the lower-triangular factor $L$, you will gain a first-principles understanding of how the properties of the original matrix $A$ directly shape its Cholesky decomposition.",
            "id": "2376470",
            "problem": "In computational engineering, the finite-difference discretization of steady one-dimensional heat conduction with Dirichlet boundary conditions on a uniform grid produces a symmetric positive-definite (SPD) linear system. Consider the resulting matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ with entries $A_{i,i} = 2$ for $1 \\leq i \\leq n$ and $A_{i,i-1} = A_{i-1,i} = -1$ for $2 \\leq i \\leq n$, and all other entries zero. Because $\\mathbf{A}$ is SPD, there exists a unique Cholesky factorization $\\mathbf{A} = \\mathbf{L}\\mathbf{L}^{\\top}$ with $\\mathbf{L}$ lower triangular and strictly positive diagonal entries.\n\nStarting only from the definition of Cholesky factorization for SPD matrices and the structure of $\\mathbf{A}$ as given above, do the following:\n- Derive the recurrence relations that the nonzero entries of $\\mathbf{L}$ must satisfy.\n- Solve these recurrences to obtain closed-form expressions for the diagonal entries $l_{k,k}$ and the first subdiagonal entries $l_{k,k-1}$ of $\\mathbf{L}$ as functions of $k$.\n\nFinally, using your derived expressions, evaluate the specific diagonal entry $l_{n,n}$ for $n = 37$. Express your final answer as an exact simplified expression with no units. Do not round.",
            "solution": "The problem statement is evaluated and found to be valid. It presents a well-posed question in numerical linear algebra, specifically concerning the Cholesky factorization of a structured symmetric positive-definite matrix, a common topic in computational engineering.\n\nThe problem defines a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ with entries:\n$$\nA_{i,j} =\n\\begin{cases}\n2 & \\text{if } i = j \\\\\n-1 & \\text{if } |i - j| = 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThis matrix is symmetric and positive-definite. The Cholesky factorization is given by $\\mathbf{A} = \\mathbf{L}\\mathbf{L}^{\\top}$, where $\\mathbf{L}$ is a lower triangular matrix with strictly positive diagonal entries, $l_{i,i} > 0$.\n\nSince $\\mathbf{A}$ is a symmetric band matrix with a semibandwidth of $1$, its Cholesky factor $\\mathbf{L}$ is a lower band matrix with the same semibandwidth. This means $\\mathbf{L}$ is a lower bidiagonal matrix. Its only nonzero entries are on the main diagonal, $l_{i,i}$, and the first subdiagonal, $l_{i,i-1}$. All other entries $l_{i,j}$ are zero for $j < i-1$ and $j > i$.\n\nThe equation $\\mathbf{A} = \\mathbf{L}\\mathbf{L}^{\\top}$ can be written in terms of its components as $A_{i,j} = \\sum_{k=1}^{\\min(i,j)} l_{i,k} l_{j,k}$. We analyze the nonzero entries of $\\mathbf{A}$.\n\nFor the diagonal entries ($i=j$):\n$A_{i,i} = \\sum_{k=1}^{i} l_{i,k}^2$. Due to the bidiagonal structure of $\\mathbf{L}$, this sum only has two potentially nonzero terms, $l_{i,i}$ and $l_{i,i-1}$.\nFor $i=1$:\n$$A_{1,1} = l_{1,1}^2$$\nFor $i \\geq 2$:\n$$A_{i,i} = l_{i,i-1}^2 + l_{i,i}^2$$\n\nFor the first subdiagonal entries ($j=i-1, i \\geq 2$):\n$A_{i,i-1} = \\sum_{k=1}^{i-1} l_{i,k} l_{i-1,k}$. The only nonzero term in this sum is when $k = i-1$.\n$$A_{i,i-1} = l_{i,i-1} l_{i-1,i-1}$$\n\nNow, we substitute the given values $A_{i,i} = 2$ and $A_{i,i-1} = -1$ to derive the recurrence relations for the entries of $\\mathbf{L}$.\n\nFor $i=1$:\n$A_{1,1} = 2 = l_{1,1}^2$. Since $l_{1,1} > 0$, we have $l_{1,1} = \\sqrt{2}$.\n\nFor $i \\geq 2$, from the subdiagonal relation:\n$A_{i,i-1} = -1 = l_{i,i-1} l_{i-1,i-1}$. This gives the first recurrence relation for the subdiagonal elements:\n$$l_{i,i-1} = -\\frac{1}{l_{i-1,i-1}}$$\n\nFor $i \\geq 2$, from the diagonal relation:\n$A_{i,i} = 2 = l_{i,i-1}^2 + l_{i,i}^2$. This gives the recurrence for the diagonal elements:\n$$l_{i,i}^2 = 2 - l_{i,i-1}^2$$\nSubstituting the first recurrence into the second, we get a recurrence solely for the diagonal entries:\n$$l_{i,i}^2 = 2 - \\left(-\\frac{1}{l_{i-1,i-1}}\\right)^2 = 2 - \\frac{1}{l_{i-1,i-1}^2}$$\n\nTo find the closed-form expressions, we compute the first few terms:\nFor $k=1$:\n$l_{1,1} = \\sqrt{2} = \\sqrt{\\frac{1+1}{1}}$\n\nFor $k=2$:\n$l_{2,1} = -\\frac{1}{l_{1,1}} = -\\frac{1}{\\sqrt{2}} = -\\sqrt{\\frac{1}{2}}$.\n$l_{2,2}^2 = 2 - l_{2,1}^2 = 2 - \\frac{1}{2} = \\frac{3}{2}$. So, $l_{2,2} = \\sqrt{\\frac{3}{2}} = \\sqrt{\\frac{2+1}{2}}$.\n\nFor $k=3$:\n$l_{3,2} = -\\frac{1}{l_{2,2}} = -\\frac{1}{\\sqrt{3/2}} = -\\sqrt{\\frac{2}{3}}$.\n$l_{3,3}^2 = 2 - l_{3,2}^2 = 2 - \\frac{2}{3} = \\frac{4}{3}$. So, $l_{3,3} = \\sqrt{\\frac{4}{3}} = \\sqrt{\\frac{3+1}{3}}$.\n\nFrom this pattern, we hypothesize the following closed-form expressions for $k \\geq 1$:\n$$l_{k,k} = \\sqrt{\\frac{k+1}{k}}$$\nAnd for $k \\geq 2$:\n$$l_{k,k-1} = -\\sqrt{\\frac{k-1}{k}}$$\n\nWe prove these by mathematical induction.\nBase Case: For $k=1$, $l_{1,1} = \\sqrt{\\frac{1+1}{1}} = \\sqrt{2}$, which is correct. For $k=2$, $l_{2,1} = -\\sqrt{\\frac{2-1}{2}} = -\\frac{1}{\\sqrt{2}}$ and $l_{2,2} = \\sqrt{\\frac{2+1}{2}} = \\sqrt{\\frac{3}{2}}$, both correct.\n\nInductive Step: Assume the formulae hold for some integer $k \\geq 1$. We must show they hold for $k+1$.\nUsing the recurrence for the subdiagonal element:\n$$l_{k+1,k} = -\\frac{1}{l_{k,k}} = -\\frac{1}{\\sqrt{\\frac{k+1}{k}}} = -\\sqrt{\\frac{k}{k+1}}$$\nThis matches the hypothesized formula for $l_{m,m-1}$ with $m=k+1$.\n\nUsing the recurrence for the diagonal element:\n$$l_{k+1,k+1}^2 = 2 - l_{k+1,k}^2 = 2 - \\left(-\\sqrt{\\frac{k}{k+1}}\\right)^2 = 2 - \\frac{k}{k+1} = \\frac{2(k+1) - k}{k+1} = \\frac{k+2}{k+1}$$\nSince $l_{k+1,k+1} > 0$, we take the principal root:\n$$l_{k+1,k+1} = \\sqrt{\\frac{k+2}{k+1}}$$\nThis matches the hypothesized formula for $l_{m,m}$ with $m=k+1$.\nThe induction is complete. The closed-form expressions are verified.\n\nThe problem asks for the specific value of $l_{n,n}$ for $n=37$. Using the derived formula for the diagonal entries $l_{k,k} = \\sqrt{\\frac{k+1}{k}}$ with $k=n=37$:\n$$l_{37,37} = \\sqrt{\\frac{37+1}{37}} = \\sqrt{\\frac{38}{37}}$$\nThis is the exact simplified value.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{38}{37}}}\n$$"
        },
        {
            "introduction": "A theoretical concept truly comes to life when you implement it in code and test its boundaries. This practice  guides you to write a Cholesky factorization routine and apply it to a family of matrices that are delicately poised on the edge of being positive-definite. By observing precisely when and why the algorithm fails—namely, when it requires taking the square root of a non-positive number—you will develop a powerful, practical intuition for the critical role of the positive-definite property.",
            "id": "2376407",
            "problem": "You are to implement and analyze the Cholesky factorization for symmetric positive definite (SPD) systems from first principles. An $n \\times n$ real matrix $A$ is symmetric positive definite (SPD) if $A = A^{\\mathsf{T}}$ and $x^{\\mathsf{T}} A x > 0$ for all nonzero vectors $x \\in \\mathbb{R}^n$. A well-tested mathematical fact is that an SPD matrix admits a unique Cholesky factorization $A = L L^{\\mathsf{T}}$ with $L$ lower triangular and all diagonal entries of $L$ strictly positive. Another well-tested fact (Sylvester’s criterion) is that a symmetric matrix is SPD if and only if all leading principal minors are positive.\n\nYour tasks:\n\n1. Implement a routine that attempts to compute the Cholesky factorization $A = L L^{\\mathsf{T}}$ for a given symmetric matrix $A$ without any pivoting. The routine must explicitly check the positivity of each computed squared diagonal pivot, and declare failure if a nonpositive value arises at any step. The implementation must not rely on black-box SPD checks; it should detect failure based purely on the computed intermediate quantities.\n\n2. Use the following parametric family of symmetric matrices:\n   $$ A(r) = \\begin{bmatrix} 1 & r \\\\ r & 1 \\end{bmatrix}, $$\n   where $r \\in \\mathbb{R}$. This family contains matrices that move from SPD to non-SPD with small changes in the single off-diagonal element $r$. Evaluate your routine on the test suite of parameter values\n   $$ r \\in \\{\\, 0.9,\\; 0.999999,\\; 1.0,\\; 1.000001 \\,\\}. $$\n   Record a boolean for each test indicating whether the Cholesky routine succeeds (return $\\,\\text{True}\\,$) or fails (return $\\,\\text{False}\\,$).\n\n3. As an additional \"happy path\" coverage case, test the following fixed $3 \\times 3$ symmetric matrix known to be SPD by Sylvester’s criterion:\n   $$ B = \\begin{bmatrix} 4 & 1 & 1 \\\\ 1 & 3 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}. $$\n   Record a boolean indicating whether the Cholesky routine succeeds on $B$.\n\n4. Output specification. Your program must produce a single line of output containing a Python-style list literal with the booleans for all tests in the following order: the four cases $A(r)$ with $r$ equal to $0.9$, $0.999999$, $1.0$, $1.000001$, followed by the single case $B$. For example, the required format is:\n   $$ [\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5] $$\n   where each $\\text{result}_i$ is either $\\text{True}$ or $\\text{False}$.\n\nNotes and constraints:\n\n- Base your reasoning on the definitions of SPD matrices and the existence condition for the Cholesky factorization stated above. Do not assume any unstated properties.\n- There are no physical units involved.\n- Angle units do not apply.\n- The only accepted output is the specified single-line list of booleans in the exact order described.",
            "solution": "The problem as stated is well-defined, self-contained, and scientifically sound, resting on established principles of numerical linear algebra. It is therefore valid, and we proceed to a full solution. The task requires the implementation of a Cholesky factorization routine and its application to a specified set of test matrices.\n\nThe fundamental principle is the factorization of a symmetric positive-definite (SPD) matrix $A$ into the product $A = LL^{\\mathsf{T}}$, where $L$ is a lower triangular matrix. The elements of $L$, denoted $L_{ij}$, can be computed directly from this equation. For an $n \\times n$ matrix $A$, the element $A_{ij}$ is given by the dot product of the $i$-th row of $L$ and the $j$-th column of $L^{\\mathsf{T}}$ (which is the $j$-th row of $L$):\n$$ A_{ij} = \\sum_{k=1}^{j} L_{ik} L_{jk} \\quad \\text{for } i \\ge j $$\nSince $A$ is symmetric, $A_{ij} = A_{ji}$, and we only need to compute $L_{ij}$ for $i \\ge j$.\n\nThe algorithm computes the columns of $L$ sequentially, from $j=1$ to $j=n$. For each column $j$, we first compute the diagonal element $L_{jj}$ and then the off-diagonal elements $L_{ij}$ for $i > j$.\n\nConsider the diagonal element $A_{jj}$:\n$$ A_{jj} = \\sum_{k=1}^{j} L_{jk}^2 = \\left( \\sum_{k=1}^{j-1} L_{jk}^2 \\right) + L_{jj}^2 $$\nFrom this, we solve for the squared diagonal element of $L$:\n$$ L_{jj}^2 = A_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2 $$\nFor the Cholesky factorization to exist with a real-valued matrix $L$ having positive diagonal entries, the term $L_{jj}^2$ must be strictly positive at every step $j$. If $L_{jj}^2 \\le 0$ for any $j$, the matrix is not positive-definite, and the factorization fails. This is the condition our routine must check. If $L_{jj}^2 > 0$, we have $L_{jj} = \\sqrt{A_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2}$.\n\nNext, consider the off-diagonal elements $A_{ij}$ for $i > j$:\n$$ A_{ij} = \\sum_{k=1}^{j} L_{ik} L_{jk} = \\left( \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right) + L_{ij} L_{jj} $$\nSolving for $L_{ij}$, we get:\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right) $$\nThis calculation is possible provided $L_{jj} \\ne 0$, which is guaranteed if $L_{jj}^2 > 0$.\n\nWe now analyze the specified test cases based on this algorithm.\n\n1.  **Parametric Family $A(r) = \\begin{bmatrix} 1 & r \\\\ r & 1 \\end{bmatrix}$**\n\n    We apply the algorithm for this $2 \\times 2$ matrix.\n    \n    For column $j=1$:\n    The squared diagonal is $L_{11}^2 = A_{11} = 1$. Since $1 > 0$, this step succeeds, and we find $L_{11} = 1$.\n    The off-diagonal element is $L_{21} = \\frac{1}{L_{11}} (A_{21}) = \\frac{r}{1} = r$.\n    \n    For column $j=2$:\n    The squared diagonal is $L_{22}^2 = A_{22} - L_{21}^2 = 1 - r^2$.\n    The factorization succeeds if and only if this quantity is strictly positive: $1 - r^2 > 0$, which is equivalent to $r^2 < 1$, or $|r| < 1$.\n\n    We evaluate the specified values of $r$:\n    - For $r = 0.9$: $L_{22}^2 = 1 - (0.9)^2 = 1 - 0.81 = 0.19 > 0$. The routine succeeds. Result: $\\text{True}$.\n    - For $r = 0.999999$: $L_{22}^2 = 1 - (0.999999)^2 > 0$. The value is positive, although small. The routine succeeds. Result: $\\text{True}$.\n    - For $r = 1.0$: $L_{22}^2 = 1 - (1.0)^2 = 0$. This is not strictly positive. The routine must fail. Result: $\\text{False}$.\n    - For $r = 1.000001$: $L_{22}^2 = 1 - (1.000001)^2 < 0$. The routine must fail. Result: $\\text{False}$.\n\n2.  **Fixed Matrix $B = \\begin{bmatrix} 4 & 1 & 1 \\\\ 1 & 3 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}$**\n\n    We apply the algorithm to this $3 \\times 3$ matrix.\n\n    For column $j=1$:\n    $L_{11}^2 = B_{11} = 4$. Since $4 > 0$, we proceed. $L_{11} = 2$.\n    $L_{21} = B_{21} / L_{11} = 1/2$.\n    $L_{31} = B_{31} / L_{11} = 1/2$.\n\n    For column $j=2$:\n    $L_{22}^2 = B_{22} - L_{21}^2 = 3 - (1/2)^2 = 3 - 1/4 = 11/4$. Since $11/4 > 0$, we proceed. $L_{22} = \\sqrt{11}/2$.\n    $L_{32} = \\frac{1}{L_{22}}(B_{32} - L_{31}L_{21}) = \\frac{1}{\\sqrt{11}/2}(1 - (1/2)(1/2)) = \\frac{2}{\\sqrt{11}}(3/4) = \\frac{3}{2\\sqrt{11}}$.\n\n    For column $j=3$:\n    $L_{33}^2 = B_{33} - (L_{31}^2 + L_{32}^2) = 2 - \\left( (1/2)^2 + \\left(\\frac{3}{2\\sqrt{11}}\\right)^2 \\right) = 2 - \\left( \\frac{1}{4} + \\frac{9}{44} \\right) = 2 - \\left( \\frac{11}{44} + \\frac{9}{44} \\right) = 2 - \\frac{20}{44} = 2 - \\frac{5}{11} = \\frac{17}{11}$.\n    Since $17/11 > 0$, the final step succeeds.\n\n    As all squared diagonal pivots are strictly positive, the Cholesky factorization for matrix $B$ succeeds. Result: $\\text{True}$.\n\nThe sequence of boolean results for the five test cases in the specified order is therefore: $\\text{True}$, $\\text{True}$, $\\text{False}$, $\\text{False}$, $\\text{True}$. The program in the final answer will implement this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Cholesky factorization from first principles,\n    tests it on several matrices, and prints the results.\n    \"\"\"\n\n    def cholesky_factorization_attempt(A: np.ndarray) -> bool:\n        \"\"\"\n        Attempts to compute the Cholesky factorization of a symmetric matrix A.\n\n        The routine follows the standard column-wise algorithm. It explicitly\n        checks for the strict positivity of the squared diagonal pivots (L_jj^2).\n        If a non-positive pivot is encountered, the matrix is not positive-definite,\n        and the factorization fails.\n\n        Args:\n            A (np.ndarray): The n x n symmetric matrix to factorize.\n\n        Returns:\n            bool: True if the factorization succeeds, False otherwise.\n        \"\"\"\n        n = A.shape[0]\n        L = np.zeros_like(A, dtype=float)\n\n        for j in range(n):\n            # Compute the sum of squares of elements in the j-th row of L up to column j-1.\n            # This corresponds to sum_{k=0}^{j-1} L[j, k]^2\n            s1 = np.dot(L[j, :j], L[j, :j])\n\n            # Compute the squared diagonal element L[j, j]^2.\n            squared_pivot = A[j, j] - s1\n\n            # The core condition for positive-definiteness in Cholesky factorization:\n            # The pivot must be strictly positive.\n            if squared_pivot <= 0:\n                return False\n\n            L[j, j] = np.sqrt(squared_pivot)\n\n            # Compute the elements in the j-th column below the diagonal.\n            if j < n - 1:\n                # This corresponds to the sum sum_{k=0}^{j-1} L[i, k] * L[j, k] for each i > j.\n                s2 = np.dot(L[j + 1:n, :j], L[j, :j])\n                L[j + 1:n, j] = (A[j + 1:n, j] - s2) / L[j, j]\n\n        return True\n\n    # 1. Define the parametric test cases A(r).\n    r_values = [0.9, 0.999999, 1.0, 1.000001]\n    test_matrices = [np.array([[1.0, r], [r, 1.0]]) for r in r_values]\n\n    # 2. Define the fixed 3x3 test case B.\n    B_matrix = np.array([\n        [4.0, 1.0, 1.0],\n        [1.0, 3.0, 1.0],\n        [1.0, 1.0, 2.0]\n    ])\n    test_matrices.append(B_matrix)\n\n    # 3. Evaluate the routine on all test cases.\n    results = []\n    for matrix in test_matrices:\n        success = cholesky_factorization_attempt(matrix)\n        results.append(success)\n\n    # 4. Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond solving linear systems, the Cholesky factorization is a powerful tool for other fundamental matrix operations. In this task , you will leverage the Cholesky factor $L$ to compute the inverse of the original matrix $A$ using the relationship $\\boldsymbol{A}^{-1} = (\\boldsymbol{L}^{-1})^{\\mathsf{T}} \\boldsymbol{L}^{-1}$. This hands-on exercise highlights a key advantage of factorization methods: they often provide a more computationally efficient and numerically stable path to the solution than direct, brute-force approaches like inverting $\\boldsymbol{A}$ itself.",
            "id": "2376430",
            "problem": "You are given that a real, symmetric positive-definite (SPD) matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$ admits a Cholesky factorization $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$, where $\\boldsymbol{L}$ is a lower-triangular matrix with strictly positive diagonal entries. For each provided test case, you are given only the Cholesky factor $\\boldsymbol{L}$ (not $\\boldsymbol{A}$). Your task is to implement a program that, for each test case, computes the inverse matrix $\\boldsymbol{A}^{-1}$ using only $\\boldsymbol{L}$ and fundamental operations, without directly inverting $\\boldsymbol{A}$. After computing $\\boldsymbol{A}^{-1}$, verify correctness by forming $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$ and evaluating the residual matrix $\\boldsymbol{R} = \\boldsymbol{A}\\boldsymbol{A}^{-1} - \\boldsymbol{I}$, where $\\boldsymbol{I}$ is the identity matrix of appropriate size. For each test case, report the maximum absolute entry of $\\boldsymbol{R}$, defined as $\\max_{i,j} |R_{ij}|$.\n\nUse the following test suite of lower-triangular Cholesky factors $\\boldsymbol{L}$:\n\n- Test case $1$ (boundary size $1 \\times 1$):\n  $$\\boldsymbol{L}_1 = \\begin{bmatrix} 3 \\end{bmatrix}.$$\n\n- Test case $2$ (size $2 \\times 2$):\n  $$\\boldsymbol{L}_2 = \\begin{bmatrix} 2 & 0 \\\\ 1 & \\sqrt{2} \\end{bmatrix}.$$\n\n- Test case $3$ (size $3 \\times 3$):\n  $$\\boldsymbol{L}_3 = \\begin{bmatrix}\n  1.5 & 0 & 0 \\\\\n  0.4 & 2.0 & 0 \\\\\n  -0.3 & 0.5 & 1.2\n  \\end{bmatrix}.$$\n\n- Test case $4$ (size $4 \\times 4$, ill-scaled yet SPD):\n  $$\\boldsymbol{L}_4 = \\begin{bmatrix}\n  10^{-3} & 0 & 0 & 0 \\\\\n  2\\times 10^{-4} & 10^{-1} & 0 & 0 \\\\\n  -10^{-4} & 3\\times 10^{-2} & 1.0 & 0 \\\\\n  5\\times 10^{-5} & -2\\times 10^{-2} & 4\\times 10^{-1} & 10.0\n  \\end{bmatrix}.$$\n\nProgram requirements:\n\n- For each test case, compute $\\boldsymbol{A}^{-1}$ using only the given $\\boldsymbol{L}$ and fundamental linear algebra operations, without directly inverting $\\boldsymbol{A}$.\n- For verification, construct $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$ and compute the residual $\\boldsymbol{R} = \\boldsymbol{A}\\boldsymbol{A}^{-1} - \\boldsymbol{I}$.\n- For each test case, output the scalar $\\rho = \\max_{i,j} |R_{ij}|$.\n- The final output must be a single line containing all four $\\rho$ values, ordered as tests $1$ through $4$, rounded to $12$ decimal places, as a comma-separated list enclosed in square brackets, for example, \"$[r_1,r_2,r_3,r_4]$\".\n\nThere are no physical quantities in this problem, so no physical units are required. All angles, if any, are irrelevant to this task. Your program must not read any input; it should run as provided and produce the required single-line output.",
            "solution": "The problem statement is valid. It is a well-posed problem in numerical linear algebra, grounded in the established theory of matrix factorizations.\n\nThe task requires the computation of the inverse of a symmetric positive-definite (SPD) matrix $\\boldsymbol{A}$ given its Cholesky factor $\\boldsymbol{L}$, where $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$. A critical constraint is that the inversion must be performed without directly computing the inverse of $\\boldsymbol{A}$. This directive is standard in high-performance scientific computing, as it points towards a method with superior numerical stability and computational efficiency.\n\nThe underlying principle is based on the algebraic properties of matrix inversion and transposition. The inverse of a product of two invertible matrices, $\\boldsymbol{X}$ and $\\boldsymbol{Y}$, follows the rule $(\\boldsymbol{X}\\boldsymbol{Y})^{-1} = \\boldsymbol{Y}^{-1}\\boldsymbol{X}^{-1}$. Furthermore, the inverse of a matrix transpose is equivalent to the transpose of the matrix inverse, expressed as $(\\boldsymbol{X}^{\\mathsf{T}})^{-1} = (\\boldsymbol{X}^{-1})^{\\mathsf{T}}$.\n\nApplying these principles to the Cholesky factorization $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$, we can formally derive an expression for $\\boldsymbol{A}^{-1}$:\n$$\n\\boldsymbol{A}^{-1} = (\\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}})^{-1}\n$$\nUsing the rule for the inverse of a product, we get:\n$$\n\\boldsymbol{A}^{-1} = (\\boldsymbol{L}^{\\mathsf{T}})^{-1} \\boldsymbol{L}^{-1}\n$$\nFinally, applying the transpose-inverse commutation rule to the term $(\\boldsymbol{L}^{\\mathsf{T}})^{-1}$ yields the desired expression:\n$$\n\\boldsymbol{A}^{-1} = (\\boldsymbol{L}^{-1})^{\\mathsf{T}} \\boldsymbol{L}^{-1}\n$$\nThis final equation provides the algorithm for computing $\\boldsymbol{A}^{-1}$. The procedure is numerically robust because it operates on the Cholesky factor $\\boldsymbol{L}$. Since $\\boldsymbol{L}$ is a lower-triangular matrix with strictly positive diagonal entries, it is always invertible. Its inverse, $\\boldsymbol{L}^{-1}$, is also lower-triangular and can be computed efficiently using a process equivalent to forward substitution to solve the matrix equation $\\boldsymbol{L}\\boldsymbol{X} = \\boldsymbol{I}$, where $\\boldsymbol{I}$ is the identity matrix.\n\nThis method is preferred because the condition number of the Cholesky factor, $\\kappa(\\boldsymbol{L})$, is the square root of the condition number of the original matrix, $\\kappa(\\boldsymbol{A})$, i.e., $\\kappa(\\boldsymbol{L}) = \\sqrt{\\kappa(\\boldsymbol{A})}$. Inverting $\\boldsymbol{L}$ is therefore a much better-conditioned numerical problem than directly inverting $\\boldsymbol{A}$, which is particularly important for ill-scaled matrices like the one in test case $4$.\n\nThe algorithm implemented to solve this problem is as follows:\n1.  For each given lower-triangular Cholesky factor $\\boldsymbol{L}$, compute its inverse, which we can denote as $\\boldsymbol{M} = \\boldsymbol{L}^{-1}$.\n2.  Calculate the inverse of the original matrix $\\boldsymbol{A}$ using the derived formula: $\\boldsymbol{A}^{-1} = \\boldsymbol{M}^{\\mathsf{T}}\\boldsymbol{M}$.\n3.  For the purpose of verification, explicitly form the matrix $\\boldsymbol{A}$ by computing the product $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$.\n4.  Calculate the residual matrix $\\boldsymbol{R} = \\boldsymbol{A}\\boldsymbol{A}^{-1} - \\boldsymbol{I}$. Ideally, $\\boldsymbol{R}$ should be the zero matrix, but due to floating-point arithmetic, its entries will be small non-zero values.\n5.  The quality of the computed inverse is assessed by finding the maximum absolute entry of the residual matrix, $\\rho = \\max_{i,j} |R_{ij}|$. This value is reported for each test case.\n\nThe provided program executes these steps for each test case using the `numpy` library for the fundamental linear algebra operations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the inverse of a matrix A from its Cholesky factor L\n    and reports the maximum absolute residual.\n    \"\"\"\n    #\n    # Execution Environment:\n    # language: Python\n    # version: 3.12\n    # libraries:\n    #     - name: numpy, version: 1.23.5\n    #     - name: scipy, version: 1.11.4\n    #\n\n    # Define the test cases from the problem statement.\n    # L1: 1x1 boundary case\n    L1 = np.array([[3.0]])\n\n    # L2: 2x2 case\n    L2 = np.array([\n        [2.0, 0.0],\n        [1.0, np.sqrt(2)]\n    ])\n\n    # L3: 3x3 case\n    L3 = np.array([\n        [1.5, 0.0, 0.0],\n        [0.4, 2.0, 0.0],\n        [-0.3, 0.5, 1.2]\n    ])\n\n    # L4: 4x4 ill-scaled case\n    L4 = np.array([\n        [1.0e-3, 0.0,    0.0,  0.0],\n        [2.0e-4, 1.0e-1, 0.0,  0.0],\n        [-1.0e-4, 3.0e-2, 1.0,  0.0],\n        [5.0e-5, -2.0e-2, 0.4, 10.0]\n    ])\n\n    test_cases = [L1, L2, L3, L4]\n\n    results = []\n    for L in test_cases:\n        # The problem requires computing A_inv from L without inverting a dense A.\n        # The method is A_inv = (L_inv)^T * L_inv, where L_inv is the inverse of L.\n        # Inverting a triangular matrix L is a fundamental and stable operation.\n        \n        # Step 1: Compute the inverse of the lower-triangular matrix L.\n        L_inv = np.linalg.inv(L)\n        \n        # Step 2: Compute A_inv using the formula A_inv = (L_inv.T) @ L_inv.\n        A_inv = L_inv.T @ L_inv\n        \n        # Step 3 (Verification): Form A = L @ L.T\n        A = L @ L.T\n        \n        # Step 4 (Verification): Compute the residual matrix R = A * A_inv - I\n        I = np.identity(A.shape[0])\n        R = A @ A_inv - I\n        \n        # Step 5: Find the maximum absolute value of the entries in R.\n        rho = np.max(np.abs(R))\n        \n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    # Output must be a single line: [r_1,r_2,r_3,r_4] with 12 decimal places.\n    formatted_results = [f\"{r:.12f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}