{
    "hands_on_practices": [
        {
            "introduction": "To build a strong intuition for numerical instability, it is invaluable to see it unfold in a simple, controlled setting. This first practice provides a small $3 \\times 3$ matrix specifically designed to cause large element growth during Gaussian elimination if pivoting is not used. By manually tracing the elimination process with and without partial pivoting, you will directly observe how a poor choice of pivot can lead to a catastrophic loss of precision and how a simple row swap can maintain stability .",
            "id": "2424558",
            "problem": "Let $\\delta$ be a fixed positive real parameter with $0 < \\delta \\ll 1$. Consider the $3 \\times 3$ matrix\n$$\nA_{\\delta} \\;=\\;\n\\begin{pmatrix}\n\\delta & 1 & 1 \\\\\n1 & 1 & 1+\\delta \\\\\n1 & 1+\\delta & 1+2\\delta\n\\end{pmatrix}.\n$$\nDefine $M_{\\mathrm{nopiv}}$ to be the maximum absolute value across all entries of the final upper-triangular factor $U$ produced by applying Gaussian elimination without any pivoting to $A_{\\delta}$. Define $M_{\\mathrm{pp}}$ analogously for Gaussian elimination with partial pivoting, where at each elimination step the pivot is chosen as the entry of largest absolute value in the current column from the current row downward and rows are swapped accordingly. Let\n$$\nR(\\delta) \\;=\\; \\frac{M_{\\mathrm{nopiv}}}{M_{\\mathrm{pp}}}.\n$$\nFor $\\delta = 1.0 \\times 10^{-4}$, compute $R(\\delta)$ and round your final result to four significant figures. The answer must be a single real number with no units.",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- A fixed positive real parameter $\\delta$ with $0 < \\delta \\ll 1$.\n- A $3 \\times 3$ matrix $A_{\\delta} = \\begin{pmatrix} \\delta & 1 & 1 \\\\ 1 & 1 & 1+\\delta \\\\ 1 & 1+\\delta & 1+2\\delta \\end{pmatrix}$.\n- $M_{\\mathrm{nopiv}}$: the maximum absolute value of entries in the upper-triangular factor $U$ from Gaussian elimination without pivoting on $A_{\\delta}$.\n- $M_{\\mathrm{pp}}$: the maximum absolute value of entries in the upper-triangular factor $U$ from Gaussian elimination with partial pivoting on $A_{\\delta}$.\n- $R(\\delta) = \\frac{M_{\\mathrm{nopiv}}}{M_{\\mathrm{pp}}}$.\n- A specific value $\\delta = 1.0 \\times 10^{-4}$ is to be used.\n- The final result for $R(\\delta)$ must be rounded to four significant figures.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra concerning the stability of Gaussian elimination and the role of pivoting strategies. The concepts are fundamental to computational science and engineering. All terms are defined unambiguously, all necessary data is provided, and the problem is free of contradictions or logical flaws.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe solution process consists of three main parts: performing Gaussian elimination without pivoting, performing it with partial pivoting, and then computing the required ratio.\n\nPart 1: Gaussian Elimination without Pivoting\n\nWe start with the matrix $A_{\\delta}$, which we denote as $A^{(1)}$:\n$$\nA^{(1)} =\n\\begin{pmatrix}\n\\delta & 1 & 1 \\\\\n1 & 1 & 1+\\delta \\\\\n1 & 1+\\delta & 1+2\\delta\n\\end{pmatrix}\n$$\nThe first pivot is $a_{11}^{(1)} = \\delta$. Since $\\delta$ is very small, we anticipate significant element growth. The multipliers for the first step are $m_{21} = \\frac{a_{21}^{(1)}}{a_{11}^{(1)}} = \\frac{1}{\\delta}$ and $m_{31} = \\frac{a_{31}^{(1)}}{a_{11}^{(1)}} = \\frac{1}{\\delta}$.\nThe row operations are $R_2 \\to R_2 - m_{21} R_1$ and $R_3 \\to R_3 - m_{31} R_1$.\nThe new entries are:\n$a_{22}^{(2)} = a_{22}^{(1)} - m_{21} a_{12}^{(1)} = 1 - \\frac{1}{\\delta} \\cdot 1 = 1 - \\frac{1}{\\delta}$.\n$a_{23}^{(2)} = a_{23}^{(1)} - m_{21} a_{13}^{(1)} = (1+\\delta) - \\frac{1}{\\delta} \\cdot 1 = 1 + \\delta - \\frac{1}{\\delta}$.\n$a_{32}^{(2)} = a_{32}^{(1)} - m_{31} a_{12}^{(1)} = (1+\\delta) - \\frac{1}{\\delta} \\cdot 1 = 1 + \\delta - \\frac{1}{\\delta}$.\n$a_{33}^{(2)} = a_{33}^{(1)} - m_{31} a_{13}^{(1)} = (1+2\\delta) - \\frac{1}{\\delta} \\cdot 1 = 1 + 2\\delta - \\frac{1}{\\delta}$.\n\nThe matrix after the first step of elimination is:\n$$\nA^{(2)} =\n\\begin{pmatrix}\n\\delta & 1 & 1 \\\\\n0 & 1 - \\frac{1}{\\delta} & 1 + \\delta - \\frac{1}{\\delta} \\\\\n0 & 1 + \\delta - \\frac{1}{\\delta} & 1 + 2\\delta - \\frac{1}{\\delta}\n\\end{pmatrix}\n$$\nThe second pivot is $a_{22}^{(2)} = 1 - \\frac{1}{\\delta}$. The multiplier for the second step is $m_{32} = \\frac{a_{32}^{(2)}}{a_{22}^{(2)}} = \\frac{1 + \\delta - 1/\\delta}{1 - 1/\\delta} = \\frac{\\delta^2 + \\delta - 1}{\\delta - 1}$.\nThe operation is $R_3 \\to R_3 - m_{32} R_2$, which gives the new entry $a_{33}^{(3)}$:\n$$\na_{33}^{(3)} = a_{33}^{(2)} - m_{32} a_{23}^{(2)} = \\left(1 + 2\\delta - \\frac{1}{\\delta}\\right) - \\frac{\\delta^2 + \\delta - 1}{\\delta - 1} \\left(1 + \\delta - \\frac{1}{\\delta}\\right)\n$$\n$$\na_{33}^{(3)} = \\frac{\\delta + 2\\delta^2 - 1}{\\delta} - \\frac{\\delta^2+\\delta-1}{\\delta-1} \\frac{\\delta^2+\\delta-1}{\\delta} = \\frac{(\\delta+2\\delta^2-1)(\\delta-1) - (\\delta^2+\\delta-1)^2}{\\delta(\\delta-1)}\n$$\nThe numerator simplifies to $(2\\delta^3-\\delta^2-2\\delta+1) - (\\delta^4+2\\delta^3-\\delta^2-2\\delta+1) = -\\delta^4$.\n$$\na_{33}^{(3)} = \\frac{-\\delta^4}{\\delta(\\delta - 1)} = \\frac{-\\delta^3}{\\delta-1} = \\frac{\\delta^3}{1-\\delta}\n$$\nThe final upper-triangular matrix $U_{\\mathrm{nopiv}}$ is:\n$$\nU_{\\mathrm{nopiv}} =\n\\begin{pmatrix}\n\\delta & 1 & 1 \\\\\n0 & 1 - \\frac{1}{\\delta} & 1 + \\delta - \\frac{1}{\\delta} \\\\\n0 & 0 & \\frac{\\delta^3}{1 - \\delta}\n\\end{pmatrix}\n$$\nTo find $M_{\\mathrm{nopiv}}$, we find the maximum absolute value of all entries in $U_{\\mathrm{nopiv}}$. For $0 < \\delta \\ll 1$:\n$|u_{11}| = \\delta$.\n$|u_{12}| = 1$.\n$|u_{13}| = 1$.\n$|u_{22}| = |1 - \\frac{1}{\\delta}| = \\frac{1-\\delta}{\\delta} = \\frac{1}{\\delta} - 1$.\n$|u_{23}| = |1 + \\delta - \\frac{1}{\\delta}| = |\\frac{\\delta^2+\\delta-1}{\\delta}| = \\frac{1-\\delta-\\delta^2}{\\delta} = \\frac{1}{\\delta} - 1 - \\delta$.\n$|u_{33}| = |\\frac{\\delta^3}{1 - \\delta}| = \\frac{\\delta^3}{1-\\delta}$.\nComparing these values, the largest is $|u_{22}| = \\frac{1}{\\delta} - 1$. Thus, $M_{\\mathrm{nopiv}} = \\frac{1-\\delta}{\\delta}$.\n\nPart 2: Gaussian Elimination with Partial Pivoting\n\nWe again start with $A^{(1)}$. In the first column, the entries are $\\delta, 1, 1$. The largest absolute value is $1$. We swap row $1$ with row $2$ (swapping with row $3$ would yield an equivalent result).\n$$\nA'^{(1)} =\n\\begin{pmatrix}\n1 & 1 & 1+\\delta \\\\\n\\delta & 1 & 1 \\\\\n1 & 1+\\delta & 1+2\\delta\n\\end{pmatrix}\n$$\nThe pivot is $a'_{11} = 1$. The multipliers are $m_{21} = \\frac{\\delta}{1} = \\delta$ and $m_{31} = \\frac{1}{1} = 1$.\nRow operations yield:\n$a_{22}^{(2)} = 1 - \\delta \\cdot 1 = 1 - \\delta$.\n$a_{23}^{(2)} = 1 - \\delta(1+\\delta) = 1 - \\delta - \\delta^2$.\n$a_{32}^{(2)} = (1+\\delta) - 1 \\cdot 1 = \\delta$.\n$a_{33}^{(2)} = (1+2\\delta) - 1(1+\\delta) = \\delta$.\nThe matrix after the first step is:\n$$\nA^{(2)} =\n\\begin{pmatrix}\n1 & 1 & 1+\\delta \\\\\n0 & 1 - \\delta & 1 - \\delta - \\delta^2 \\\\\n0 & \\delta & \\delta\n\\end{pmatrix}\n$$\nFor the second step, we consider the second column from the diagonal downwards. The entries are $1-\\delta$ and $\\delta$. Since $0 < \\delta \\ll 1$, we have $|1-\\delta| > |\\delta|$. No row swap is necessary.\nThe pivot is $a_{22}^{(2)} = 1 - \\delta$. The multiplier is $m_{32} = \\frac{\\delta}{1-\\delta}$.\nThe new entry $a_{33}^{(3)}$ is:\n$$\na_{33}^{(3)} = \\delta - \\frac{\\delta}{1-\\delta}(1-\\delta-\\delta^2) = \\frac{\\delta(1-\\delta) - \\delta(1-\\delta-\\delta^2)}{1-\\delta} = \\frac{(\\delta-\\delta^2) - (\\delta-\\delta^2-\\delta^3)}{1-\\delta} = \\frac{\\delta^3}{1-\\delta}\n$$\nThe final upper-triangular matrix $U_{\\mathrm{pp}}$ is:\n$$\nU_{\\mathrm{pp}} =\n\\begin{pmatrix}\n1 & 1 & 1+\\delta \\\\\n0 & 1 - \\delta & 1 - \\delta - \\delta^2 \\\\\n0 & 0 & \\frac{\\delta^3}{1 - \\delta}\n\\end{pmatrix}\n$$\nTo find $M_{\\mathrm{pp}}$, we find the maximum absolute value of entries in $U_{\\mathrm{pp}}$.\n$|u'_{11}| = 1$.\n$|u'_{12}| = 1$.\n$|u'_{13}| = 1+\\delta$.\n$|u'_{22}| = 1-\\delta$.\n$|u'_{23}| = 1-\\delta-\\delta^2$.\n$|u'_{33}| = \\frac{\\delta^3}{1-\\delta}$.\nFor $0 < \\delta \\ll 1$, the largest of these values is clearly $1+\\delta$. Thus, $M_{\\mathrm{pp}} = 1+\\delta$.\n\nPart 3: Calculation of $R(\\delta)$\n\nThe ratio $R(\\delta)$ is given by:\n$$\nR(\\delta) = \\frac{M_{\\mathrm{nopiv}}}{M_{\\mathrm{pp}}} = \\frac{\\frac{1-\\delta}{\\delta}}{1+\\delta} = \\frac{1-\\delta}{\\delta(1+\\delta)}\n$$\nWe are asked to compute this for $\\delta = 1.0 \\times 10^{-4}$.\nSubstituting this value:\n$$\nM_{\\mathrm{nopiv}} = \\frac{1 - 10^{-4}}{10^{-4}} = \\frac{0.9999}{10^{-4}} = 9999\n$$\n$$\nM_{\\mathrm{pp}} = 1 + 10^{-4} = 1.0001\n$$\n$$\nR(10^{-4}) = \\frac{9999}{1.0001}\n$$\nPerforming the division:\n$$\nR(10^{-4}) \\approx 9998.00019998\n$$\nThe problem requires rounding this result to four significant figures. The first four significant figures are $9, 9, 9, 8$. The fifth significant digit is $0$, so we round down.\nThe final result is $9998$.",
            "answer": "$$\n\\boxed{9998}\n$$"
        },
        {
            "introduction": "Having seen how pivoting prevents element growth, we now turn to its effect on the accuracy of a final solution for a truly difficult problem. This exercise challenges you to implement and compare several common pivoting strategies—including no pivoting, partial pivoting, scaled partial pivoting, and complete pivoting—on the notoriously ill-conditioned Hilbert matrix . By measuring the solution error for each strategy as the matrix size increases, you will gain a practical understanding of their relative effectiveness and the critical role they play in obtaining reliable results in scientific computing.",
            "id": "2424559",
            "problem": "You are given the Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ with entries $H_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1,\\dots,n\\}$. For a given $n$, define the vector $x_{\\mathrm{true}} \\in \\mathbb{R}^n$ by $(x_{\\mathrm{true}})_i = 1$ for all $i \\in \\{1,\\dots,n\\}$, and define $b = H x_{\\mathrm{true}}$. For each $n$ in a specified test suite, compute numerical approximations $\\hat{x}$ to the solution of the linear system $H x = b$ by constructing a factorization of $H$ under four pivoting strategies and then solving the triangular systems determined by that factorization. The four strategies are:\n(1) no pivoting, where $H = L U$ with $L$ unit lower triangular and $U$ upper triangular,\n(2) partial pivoting (row pivoting), where $P H = L U$ with $P$ a permutation matrix,\n(3) scaled partial pivoting (row pivoting with row scaling), where $P H = L U$ with $P$ a permutation matrix determined by row scaling considerations, and\n(4) complete pivoting (row and column pivoting), where $P H Q = L U$ with $P$ and $Q$ permutation matrices.\nFor each strategy, compute the relative $2$-norm solution error $e = \\dfrac{\\lVert \\hat{x} - x_{\\mathrm{true}} \\rVert_2}{\\lVert x_{\\mathrm{true}} \\rVert_2}$.\n\nUse the following test suite for $n$: $\\{1,3,8,12\\}$. For each $n$ in this set, return a list of four floating-point values corresponding, in order, to the relative errors for strategies $(1)$, $(2)$, $(3)$, and $(4)$. Aggregate the results for all $n$ into a single list of lists in the same order of $n$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each inner list corresponds to one $n$ in the test suite. Each floating-point value must be formatted in scientific notation with $6$ digits after the decimal point. For example, the output should have the form\n$[[e_{1,1},e_{1,2},e_{1,3},e_{1,4}],[e_{3,1},e_{3,2},e_{3,3},e_{3,4}],[e_{8,1},e_{8,2},e_{8,3},e_{8,4}],[e_{12,1},e_{12,2},e_{12,3},e_{12,4}]]$,\nwhere $e_{n,s}$ denotes the relative error for size $n$ and strategy $s$.\n\nThere are no physical units involved in this problem. Angles are not used. All final numerical answers must be real numbers printed as specified above. The answer types for all test cases are floating-point numbers. The problem is to be solved purely from the definitions of matrix factorization, permutation matrices, and the solution of triangular systems, and the description is fully self-contained and independent of any specific programming language or library.",
            "solution": "The problem as stated is valid. It is a well-posed, scientifically-grounded exercise in numerical linear algebra, centered on the critical topic of numerical stability in the solution of linear systems. All definitions and parameters are provided unambiguously.\n\nThe fundamental task is to solve the linear system of equations $H x = b$ for several dimensions $n$, where $H$ is the $n \\times n$ Hilbert matrix. The entries of $H$ are given by $H_{ij} = \\dfrac{1}{i+j-1}$ for $i,j \\in \\{1, \\dots, n\\}$. The vector $b$ is constructed such that the exact solution, denoted $x_{\\mathrm{true}}$, is a vector of ones: $(x_{\\mathrm{true}})_i = 1$ for all $i$. Therefore, the right-hand side is $b = H x_{\\mathrm{true}}$, which implies its components are $b_i = \\sum_{j=1}^{n} H_{ij} = \\sum_{j=1}^{n} \\dfrac{1}{i+j-1}$.\n\nThe Hilbert matrix is a classic example of an ill-conditioned matrix. Its condition number, $\\kappa(H) = \\lVert H \\rVert \\lVert H^{-1} \\rVert$, grows at a superexponential rate with $n$. This extreme sensitivity means that small floating-point rounding errors introduced during numerical computations can be amplified into large errors in the final solution $\\hat{x}$. The objective of this problem is to observe and quantify this effect and to demonstrate the efficacy of various pivoting strategies in mitigating such numerical instability.\n\nThe solution method is Gaussian elimination, which algorithmically corresponds to factoring the matrix $H$ into a product of a lower triangular matrix $L$ and an upper triangular matrix $U$. Once the factorization is obtained, the system is solved efficiently by successive forward and backward substitutions. We will implement and compare four variants of this procedure. For each numerical solution $\\hat{x}$, we will compute the relative error in the $2$-norm:\n$$e = \\frac{\\lVert \\hat{x} - x_{\\mathrm{true}} \\rVert_2}{\\lVert x_{\\mathrm{true}} \\rVert_2}$$\nGiven that $(x_{\\mathrm{true}})_i=1$ for all $i$, the norm of the true solution is $\\lVert x_{\\mathrm{true}} \\rVert_2 = \\sqrt{\\sum_{i=1}^n 1^2} = \\sqrt{n}$.\n\nThe four pivoting strategies are as follows:\n\n1.  **No Pivoting**:\n    The matrix $H$ is factorized directly as $H = LU$, where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. The system $Hx=b$ becomes $LUx=b$. This is solved in two stages:\n    -   First, solve $Ly=b$ for $y$ using forward substitution.\n    -   Then, solve $Ux=y$ for $x$ using backward substitution.\n    This naive approach is known to be numerically unstable if any pivot element $U_{kk}$ is small, as it can lead to large multipliers and catastrophic loss of precision. For the Hilbert matrix, the pivots become exceedingly small, guaranteeing poor performance.\n\n2.  **Partial Pivoting (Row Pivoting)**:\n    This is the most common strategy for improving the stability of Gaussian elimination. At each step $k$ of the elimination, the algorithm searches for the element with the largest absolute value in the current pivot column (from row $k$ downwards). Let this element be in row $p$, where $p \\ge k$. Row $k$ and row $p$ are then interchanged. This process is equivalent to finding a permutation matrix $P$ such that the factorization is performed on the permuted matrix: $PH=LU$. The permutation $P$ reorders the rows of $H$ to ensure that the pivot elements used in the elimination are as large as possible, thus keeping the multipliers in $L$ bounded by $1$ in magnitude. The system $Hx=b$ is rewritten as $PHx=Pb$, which leads to $LUx=Pb$. We solve $Ly=Pb$ and then $Ux=y$.\n\n3.  **Scaled Partial Pivoting**:\n    This strategy is a refinement of partial pivoting. It is designed to prevent a large entry from being chosen as a pivot solely because its row contains large entries overall. Before elimination, a scale factor $s_i$ is computed for each row, defined as the maximum absolute value of any element in that row: $s_i = \\max_{1 \\le j \\le n} |H_{ij}|$. At step $k$, instead of choosing the raw largest element, the algorithm chooses the pivot row $p$ ($p \\ge k$) that maximizes the ratio of the pivot candidate's magnitude to its row's scale factor: $\\dfrac{|A_{pk}^{(k-1)}|}{s_p}$. The matrix $A^{(k-1)}$ here represents the matrix state after $k-1$ steps of elimination. After the pivot row is chosen, it is swapped with row $k$, and the elimination proceeds as in standard partial pivoting. The scale factors must also be swapped. The resulting factorization is again of the form $PH=LU$.\n\n4.  **Complete Pivoting**:\n    This is the most robust, but also the most computationally expensive, pivoting strategy. At each step $k$, the algorithm searches the entire remaining submatrix $A^{(k-1)}_{i,j \\ge k}$ for the element with the largest absolute value. If this element is found at position $(p, q)$, row $k$ is swapped with row $p$, and column $k$ is swapped with column $q$. This corresponds to finding two permutation matrices, $P$ (for rows) and $Q$ (for columns), such that $PHQ=LU$. The system $Hx=b$ is transformed into $(PHQ)(Q^T x) = Pb$. Let $z = Q^T x$. We first solve the system $LUz = Pb$ for $z$ using forward and backward substitution. The final solution is then recovered by undoing the column permutations: $x = Qz$. Complete pivoting offers the best theoretical guarantees on numerical stability by minimizing the growth of elements during factorization but requires a significantly larger number of comparisons at each step.\n\nWe expect to observe a clear trend: the error will decrease as the sophistication of the pivoting strategy increases, from no pivoting (largest error) to complete pivoting (smallest error). This demonstrates the crucial role of pivoting in obtaining meaningful solutions for ill-conditioned systems in finite-precision arithmetic.",
            "answer": "[[0.000000e+00,0.000000e+00,0.000000e+00,0.000000e+00],[2.585257e-14,2.238202e-14,2.238202e-14,1.884496e-14],[3.064560e-06,2.029853e-07,2.029853e-07,1.066497e-07],[2.285888e-01,1.050682e-02,1.050682e-02,3.310111e-03]]"
        },
        {
            "introduction": "In many real-world engineering applications, matrices are not just ill-conditioned but also large and sparse. For these systems, computational efficiency is as important as numerical stability, and this often involves pre-ordering the matrix to minimize \"fill-in\" during factorization. This advanced practice explores the fundamental conflict between pivoting for stability and ordering for sparsity, demonstrating how row swaps required for numerical accuracy can disrupt a fill-reducing order and increase computational cost .",
            "id": "2424537",
            "problem": "You are tasked with constructing and evaluating the effect of row pivoting on fill-reducing orderings in sparse direct factorization. For each matrix specified below, consider a permutation vector $p$ (in one-line notation) and its associated permutation matrix $P \\in \\mathbb{R}^{n \\times n}$. Define the symmetrically permuted matrix $\\hat{A} = P A P^{\\top}$. Two factorizations of $\\hat{A}$ must be considered:\n\n1. A static-ordered factorization $\\hat{A} = L_{\\mathrm{np}} U_{\\mathrm{np}}$ with $L_{\\mathrm{np}}$ unit lower triangular and $U_{\\mathrm{np}}$ upper triangular, with no row exchanges during factorization.\n\n2. A partial-pivoted factorization $P_{\\mathrm{pp}} \\hat{A} = L_{\\mathrm{pp}} U_{\\mathrm{pp}}$ with $L_{\\mathrm{pp}}$ unit lower triangular, $U_{\\mathrm{pp}}$ upper triangular, and $P_{\\mathrm{pp}}$ a permutation matrix generated by selecting, at each step $k$, a pivot in column $k$ of maximal absolute value among rows $k,k+1,\\ldots,n$.\n\nFor any matrix $M \\in \\mathbb{R}^{n \\times n}$ and tolerance $\\tau = 10^{-12}$, define the structural nonzero count $\\mathrm{nnz}_{\\tau}(M)$ as the cardinality of the set $\\{(i,j) : |M_{ij}| > \\tau\\}$. Define the fill-in counts\n$$\nF_{\\mathrm{np}} = \\mathrm{nnz}_{\\tau}(L_{\\mathrm{np}} - I) + \\mathrm{nnz}_{\\tau}(U_{\\mathrm{np}}) - \\mathrm{nnz}_{\\tau}(\\hat{A}),\n$$\n$$\nF_{\\mathrm{pp}} = \\mathrm{nnz}_{\\tau}(L_{\\mathrm{pp}} - I) + \\mathrm{nnz}_{\\tau}(U_{\\mathrm{pp}}) - \\mathrm{nnz}_{\\tau}(\\hat{A}),\n$$\nwhere $I$ denotes the identity matrix. Define the swap count $s$ as the number of row exchanges performed by the partial-pivoted factorization. Define the growth factor\n$$\n\\gamma = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |\\hat{A}_{ij}|}.\n$$\n\nFor each test case below, compute the pair $(F_{\\mathrm{np}}, F_{\\mathrm{pp}})$, the swap count $s$, and the growth factors $\\gamma_{\\mathrm{np}}$ and $\\gamma_{\\mathrm{pp}}$, where the subscripts denote the factorization used. Report $\\gamma_{\\mathrm{np}}$ and $\\gamma_{\\mathrm{pp}}$ rounded to six decimal places.\n\nAll matrices below are real-valued and dimensioned as specified, with entries defined exactly (there are no physical units in this problem). Angles are not involved. The final output should be a single line containing the results formatted as a list of per-case summaries, where each summary is a list $[F_{\\mathrm{np}}, F_{\\mathrm{pp}}, s, \\gamma_{\\mathrm{np}}, \\gamma_{\\mathrm{pp}}]$. For example, the program should print a single line of the form $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$.\n\nTest Suite:\n\n- Case 1 (baseline narrow band, no disruptive pivoting expected):\n  - Size $n = 8$.\n  - Matrix $A^{(1)} \\in \\mathbb{R}^{8 \\times 8}$ is tri-diagonal with\n    $$\n    A^{(1)}_{ii} = 4 \\text{ for } i=1,\\ldots,8,\\quad A^{(1)}_{i,i+1} = A^{(1)}_{i+1,i} = -1 \\text{ for } i=1,\\ldots,7,\n    $$\n    and all other entries zero.\n  - Permutation $p^{(1)} = [1,2,3,4,5,6,7,8]$.\n\n- Case 2 (fill-reducing ordering present, but with small diagonals to induce disruptive pivoting):\n  - Size $n = 8$.\n  - Matrix $A^{(2)} \\in \\mathbb{R}^{8 \\times 8}$ starts from the same tri-diagonal structure as $A^{(1)}$, then is modified as follows:\n    $$\n    A^{(2)}_{1,5} = A^{(2)}_{5,1} = -1,\\quad A^{(2)}_{4,8} = A^{(2)}_{8,4} = -1.\n    $$\n    Set all diagonal entries to $4$ except\n    $$\n    A^{(2)}_{1,1} = 10^{-6},\\quad A^{(2)}_{3,3} = 10^{-6}.\n    $$\n  - Fill-reducing permutation $p^{(2)} = [1,5,2,6,3,7,4,8]$.\n\n- Case 3 (strongly disruptive pivoting with multiple small diagonals):\n  - Size $n = 8$.\n  - Matrix $A^{(3)} \\in \\mathbb{R}^{8 \\times 8}$ has the same sparsity pattern and off-diagonal values as $A^{(2)}$ (that is, the same tri-diagonal entries and extra couplings $(1,5)$ and $(4,8)$ with value $-1$), but with diagonal entries\n    $$\n    A^{(3)}_{1,1} = A^{(3)}_{2,2} = A^{(3)}_{3,3} = A^{(3)}_{4,4} = 10^{-9},\\quad A^{(3)}_{i,i} = 4 \\text{ for } i \\in \\{5,6,7,8\\}.\n    $$\n  - Use the same fill-reducing permutation $p^{(3)} = [1,5,2,6,3,7,4,8]$.\n\nNumerical parameters and output specification:\n\n- Use $\\tau = 10^{-12}$ as the structural zero threshold in all counts of nonzeros.\n- For each test case $k \\in \\{1,2,3\\}$, compute and aggregate\n  $$\n  [F_{\\mathrm{np}}^{(k)}, F_{\\mathrm{pp}}^{(k)}, s^{(k)}, \\gamma_{\\mathrm{np}}^{(k)}, \\gamma_{\\mathrm{pp}}^{(k)}],\n  $$\n  where $\\gamma_{\\mathrm{np}}^{(k)}$ and $\\gamma_{\\mathrm{pp}}^{(k)}$ are rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of these three summaries enclosed in square brackets; that is,\n  $$\n  \\big[\\,[F_{\\mathrm{np}}^{(1)}, F_{\\mathrm{pp}}^{(1)}, s^{(1)}, \\gamma_{\\mathrm{np}}^{(1)}, \\gamma_{\\mathrm{pp}}^{(1)}],\\ [F_{\\mathrm{np}}^{(2)}, F_{\\mathrm{pp}}^{(2)}, s^{(2)}, \\gamma_{\\mathrm{np}}^{(2)}, \\gamma_{\\mathrm{pp}}^{(2)}],\\ [F_{\\mathrm{np}}^{(3)}, F_{\\mathrm{pp}}^{(3)}, s^{(3)}, \\gamma_{\\mathrm{np}}^{(3)}, \\gamma_{\\mathrm{pp}}^{(3)}] \\big].\n  $$",
            "solution": "The problem requires an analysis of the interplay between fill-reducing orderings and numerical pivoting strategies in the context of sparse matrix factorization. We are given three test cases, each defined by a matrix $A \\in \\mathbb{R}^{n \\times n}$ and a permutation vector $p$. The core of the task is to compare two LU factorization approaches for the symmetrically permuted matrix $\\hat{A} = P A P^{\\top}$, where $P$ is the permutation matrix corresponding to $p$.\n\nFirst, we must formalize the construction of the permuted matrix $\\hat{A}$. A permutation vector $p = [p_1, p_2, \\ldots, p_n]$ in one-line notation specifies a reordering of the matrix indices. We adopt the standard interpretation where the $i$-th row and column of the new matrix $\\hat{A}$ correspond to the $p_i$-th row and column of the original matrix $A$. This means $\\hat{A}_{ij} = A_{p_i, p_j}$. For implementation purposes, using $0$-based indexing for an $n \\times n$ matrix, if $p_{_0}$ is the $0$-based permutation index vector, then $\\hat{A}$ can be computed by applying this indexing to the rows and columns of $A$.\n\nThe two factorization methods under consideration are:\n\n$1$. Static-ordered factorization (no pivoting): We compute the LU decomposition of $\\hat{A}$ directly, such that $\\hat{A} = L_{\\mathrm{np}} U_{\\mathrm{np}}$, where $L_{\\mathrm{np}}$ is unit lower triangular and $U_{\\mathrm{np}}$ is upper triangular. This is achieved via a standard Gaussian elimination algorithm without any row interchanges. This approach strictly adheres to the provided ordering $p$, which is often chosen to minimize fill-in (the creation of new non-zero entries in the factors). However, this method can be numerically unstable if a small or zero pivot element $U_{kk}$ is encountered, leading to large values in the factors.\n\n$2$. Partial-pivoted factorization: We compute the LU decomposition of $\\hat{A}$ using partial pivoting. This results in a factorization of the form $P_{\\mathrm{pp}} \\hat{A} = L_{\\mathrm{pp}} U_{\\mathrm{pp}}$, where $L_{\\mathrm{pp}}$ is unit lower triangular, $U_{\\mathrm{pp}}$ is upper triangular, and $P_{\\mathrm{pp}}$ is a permutation matrix representing the row interchanges performed during factorization. At each step $k$ of the elimination, the algorithm selects a pivot from column $k$ (among rows $k, \\ldots, n-1$) that has the maximal absolute value and swaps its row with row $k$. This strategy is designed to ensure numerical stability by avoiding division by small numbers, thus controlling the magnitude of the elements in the factors. The trade-off is that these row swaps, represented by $P_{\\mathrm{pp}}$, can disrupt the initial fill-reducing ordering $p$, potentially leading to significantly more fill-in than the static-ordered approach.\n\nTo quantify this trade-off, we compute several metrics for each case:\n\n- The fill-in counts, $F_{\\mathrm{np}}$ and $F_{\\mathrm{pp}}$, measure the number of new non-zero elements created in the factors $L$ and $U$ compared to the permuted matrix $\\hat{A}$. The structural non-zero count $\\mathrm{nnz}_{\\tau}(M)$ is defined as the number of entries in a matrix $M$ whose absolute value exceeds a tolerance $\\tau = 10^{-12}$. The fill-in is then $F = \\mathrm{nnz}_{\\tau}(L - I) + \\mathrm{nnz}_{\\tau}(U) - \\mathrm{nnz}_{\\tau}(\\hat{A})$.\n- The swap count, $s$, is the total number of row interchanges executed by the partial pivoting algorithm. It quantifies the degree to which the initial ordering was altered for stability.\n- The growth factors, $\\gamma_{\\mathrm{np}}$ and $\\gamma_{\\mathrm{pp}}$, measure the element growth during factorization, defined as $\\gamma = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |\\hat{A}_{ij}|}$. A small growth factor is indicative of a numerically stable factorization.\n\nThe algorithmic procedure for each test case is as follows:\n$1$. Construct the matrix $A^{(k)}$ and the $1$-based permutation vector $p^{(k)}$.\n$2$. Convert $p^{(k)}$ to a $0$-based index array to compute the symmetrically permuted matrix $\\hat{A} = P A^{(k)} P^{\\top}$.\n$3$. For the no-pivoting case, implement a direct LU factorization algorithm that produces $L_{\\mathrm{np}}$ and $U_{\\mathrm{np}}$ from $\\hat{A}$ without row swaps. Calculate the fill-in $F_{\\mathrm{np}}$ and growth factor $\\gamma_{\\mathrm{np}}$.\n$4$. For the partial-pivoting case, utilize a robust library function (such as `scipy.linalg.lu_factor`) that performs LU decomposition with partial pivoting on $\\hat{A}$. From its output, extract the factors $L_{\\mathrm{pp}}$ and $U_{\\mathrm{pp}}$, and determine the number of swaps $s$. Calculate the fill-in $F_{\\mathrm{pp}}$ and growth factor $\\gamma_{\\mathrm{pp}}$.\n$5$. Aggregate the results $[F_{\\mathrm{np}}, F_{\\mathrm{pp}}, s, \\gamma_{\\mathrm{np}}, \\gamma_{\\mathrm{pp}}]$ for each case, with growth factors rounded to six decimal places, and format them into the final output structure.\nThis structured comparison will demonstrate how pivoting for stability can compromise a sparsity-preserving ordering, a fundamental consideration in the design of direct solvers for large sparse systems.",
            "answer": "[[0,0,0,1.000000,1.000000],[8,8,2,2666667.500000,1.000000],[8,21,4,2666667501.000000,1.000000]]"
        }
    ]
}