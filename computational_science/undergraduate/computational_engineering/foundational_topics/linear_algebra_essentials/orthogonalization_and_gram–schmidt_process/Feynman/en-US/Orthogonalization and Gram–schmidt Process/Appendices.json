{
    "hands_on_practices": [
        {
            "introduction": "Orthogonal polynomials, such as Legendre polynomials, are foundational tools in computational engineering for tasks like numerical integration and solving differential equations. This exercise  provides direct, hands-on practice in constructing these polynomials from first principles. By applying the Gram-Schmidt process to the standard monomial basis with a continuous inner product defined by an integral, you will gain a deeper appreciation for the structure of these important function families.",
            "id": "2422241",
            "problem": "In computational engineering, orthogonal polynomial bases on the interval $[-1,1]$ are fundamental for constructing stable numerical approximations. Consider the real vector space of polynomials of degree at most $4$ equipped with the inner product\n$$\\langle f, g \\rangle = \\int_{-1}^{1} f(x)\\,g(x)\\,\\mathrm{d}x.$$\nThere exists a unique sequence $\\{P_{n}(x)\\}_{n=0}^{4}$ of polynomials with $\\deg P_{n}=n$ such that the set is mutually orthogonal with respect to $\\langle \\cdot,\\cdot \\rangle$ and satisfies the normalization condition $P_{n}(1)=1$ for each $n$ in $\\{0,1,2,3,4\\}$.\n\nDetermine these polynomials up to degree $4$, and provide the explicit closed-form expression for $P_{4}(x)$ only. Express your final answer as a polynomial in $x$ with exact rational coefficients. No rounding is required.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Vector Space:** The real vector space of polynomials with degree at most $4$, denoted $\\mathcal{P}_4([-1, 1])$.\n- **Inner Product:** For any two polynomials $f(x)$ and $g(x)$ in the space, the inner product is defined as $\\langle f, g \\rangle = \\int_{-1}^{1} f(x)g(x)\\,\\mathrm{d}x$.\n- **Orthogonal Sequence:** A sequence of polynomials $\\{P_{n}(x)\\}_{n=0}^{4}$.\n- **Degree Condition:** The degree of each polynomial $P_n(x)$ is $n$.\n- **Orthogonality Condition:** The set $\\{P_{0}(x), P_{1}(x), P_{2}(x), P_{3}(x), P_{4}(x)\\}$ is mutually orthogonal, i.e., $\\langle P_n, P_m \\rangle = 0$ for $n \\neq m$.\n- **Normalization Condition:** Each polynomial satisfies $P_n(1) = 1$ for $n \\in \\{0, 1, 2, 3, 4\\}$.\n- **Objective:** Determine the explicit expression for the polynomial $P_4(x)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard exercise in the theory of orthogonal polynomials, which are fundamental in computational engineering and applied mathematics. The specified polynomials are a particular scaling of the well-known Legendre polynomials. The problem is well-posed; the Gram-Schmidt orthogonalization process applied to the monomial basis $\\{1, x, x^2, x^3, x^4\\}$, combined with the given normalization condition, guarantees the existence and uniqueness of such a sequence of polynomials. The problem statement is objective, complete, and contains no contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be constructed.\n\nThe sequence of orthogonal polynomials $\\{P_n(x)\\}$ can be constructed using the Gram-Schmidt orthogonalization process on the standard monomial basis $\\{v_n(x) = x^n\\}_{n=0}^{4}$. We first generate an un-normalized orthogonal basis $\\{q_n(x)\\}_{n=0}^{4}$ and then scale each polynomial to satisfy the normalization condition $P_n(1)=1$.\n\nLet $q_n(x)$ be the un-normalized orthogonal polynomials. The Gram-Schmidt process is defined by:\n$$q_n(x) = v_n(x) - \\sum_{k=0}^{n-1} \\frac{\\langle v_n, q_k \\rangle}{\\langle q_k, q_k \\rangle} q_k(x)$$\nA useful identity for the inner product is $\\int_{-1}^{1} x^k \\, \\mathrm{d}x = \\frac{2}{k+1}$ for even $k$, and $0$ for odd $k$.\n\nFor $n=0$:\n$v_0(x) = 1$.\n$q_0(x) = v_0(x) = 1$.\nThe normalization requires $P_0(1)=1$. Let $P_0(x) = c_0 q_0(x) = c_0$. Then $P_0(1) = c_0 = 1$.\nThus, $P_0(x) = 1$.\n\nFor $n=1$:\n$v_1(x) = x$.\n$q_1(x) = v_1(x) - \\frac{\\langle v_1, q_0 \\rangle}{\\langle q_0, q_0 \\rangle} q_0(x)$.\n$\\langle v_1, q_0 \\rangle = \\int_{-1}^{1} x \\cdot 1 \\, \\mathrm{d}x = 0$.\nSo, $q_1(x) = x$.\nNormalization: $P_1(x) = c_1 x$. $P_1(1) = c_1 \\cdot 1 = 1 \\implies c_1=1$.\nThus, $P_1(x) = x$.\n\nFor $n=2$:\n$v_2(x) = x^2$.\n$q_2(x) = v_2(x) - \\frac{\\langle v_2, q_0 \\rangle}{\\langle q_0, q_0 \\rangle} q_0(x) - \\frac{\\langle v_2, q_1 \\rangle}{\\langle q_1, q_1 \\rangle} q_1(x)$.\n$\\langle v_2, q_1 \\rangle = \\int_{-1}^{1} x^2 \\cdot x \\, \\mathrm{d}x = \\int_{-1}^{1} x^3 \\, \\mathrm{d}x = 0$.\n$\\langle v_2, q_0 \\rangle = \\int_{-1}^{1} x^2 \\cdot 1 \\, \\mathrm{d}x = \\frac{2}{3}$.\n$\\langle q_0, q_0 \\rangle = \\int_{-1}^{1} 1^2 \\, \\mathrm{d}x = 2$.\n$q_2(x) = x^2 - \\frac{2/3}{2} \\cdot 1 = x^2 - \\frac{1}{3}$.\nNormalization: $P_2(x) = c_2 (x^2 - \\frac{1}{3})$. $P_2(1) = c_2(1 - \\frac{1}{3}) = c_2(\\frac{2}{3}) = 1 \\implies c_2 = \\frac{3}{2}$.\nThus, $P_2(x) = \\frac{3}{2}(x^2 - \\frac{1}{3}) = \\frac{3}{2}x^2 - \\frac{1}{2}$.\n\nFor $n=3$:\n$v_3(x) = x^3$.\n$q_3(x) = v_3(x) - \\frac{\\langle v_3, q_0 \\rangle}{\\langle q_0, q_0 \\rangle} q_0(x) - \\frac{\\langle v_3, q_1 \\rangle}{\\langle q_1, q_1 \\rangle} q_1(x) - \\frac{\\langle v_3, q_2 \\rangle}{\\langle q_2, q_2 \\rangle} q_2(x)$.\nBy parity, $\\langle v_3, q_0 \\rangle = 0$ and $\\langle v_3, q_2 \\rangle = 0$.\n$\\langle v_3, q_1 \\rangle = \\int_{-1}^{1} x^3 \\cdot x \\, \\mathrm{d}x = \\int_{-1}^{1} x^4 \\, \\mathrm{d}x = \\frac{2}{5}$.\n$\\langle q_1, q_1 \\rangle = \\int_{-1}^{1} x^2 \\, \\mathrm{d}x = \\frac{2}{3}$.\n$q_3(x) = x^3 - \\frac{2/5}{2/3} x = x^3 - \\frac{3}{5}x$.\nNormalization: $P_3(x) = c_3 (x^3 - \\frac{3}{5}x)$. $P_3(1) = c_3(1 - \\frac{3}{5}) = c_3(\\frac{2}{5}) = 1 \\implies c_3 = \\frac{5}{2}$.\nThus, $P_3(x) = \\frac{5}{2}(x^3 - \\frac{3}{5}x) = \\frac{5}{2}x^3 - \\frac{3}{2}x$.\n\nFor $n=4$:\n$v_4(x) = x^4$.\n$q_4(x) = v_4(x) - \\sum_{k=0}^{3} \\frac{\\langle v_4, q_k \\rangle}{\\langle q_k, q_k \\rangle} q_k(x)$.\nBy parity, $\\langle v_4, q_1 \\rangle = 0$ and $\\langle v_4, q_3 \\rangle = 0$.\n$q_4(x) = x^4 - \\frac{\\langle v_4, q_0 \\rangle}{\\langle q_0, q_0 \\rangle} q_0(x) - \\frac{\\langle v_4, q_2 \\rangle}{\\langle q_2, q_2 \\rangle} q_2(x)$.\n$q_0(x) = 1$ and $q_2(x) = x^2 - \\frac{1}{3}$.\nTerm 1:\n$\\langle v_4, q_0 \\rangle = \\int_{-1}^{1} x^4 \\cdot 1 \\, \\mathrm{d}x = \\frac{2}{5}$.\n$\\langle q_0, q_0 \\rangle = 2$.\nProjection on $q_0$ is $\\frac{2/5}{2} q_0(x) = \\frac{1}{5}$.\nTerm 2:\n$\\langle v_4, q_2 \\rangle = \\int_{-1}^{1} x^4(x^2 - \\frac{1}{3}) \\, \\mathrm{d}x = \\int_{-1}^{1} (x^6 - \\frac{1}{3}x^4) \\, \\mathrm{d}x = \\frac{2}{7} - \\frac{1}{3}\\frac{2}{5} = \\frac{30-14}{105} = \\frac{16}{105}$.\n$\\langle q_2, q_2 \\rangle = \\int_{-1}^{1} (x^2 - \\frac{1}{3})^2 \\, \\mathrm{d}x = \\int_{-1}^{1} (x^4 - \\frac{2}{3}x^2 + \\frac{1}{9}) \\, \\mathrm{d}x = \\frac{2}{5} - \\frac{2}{3}\\frac{2}{3} + \\frac{1}{9}(2) = \\frac{2}{5} - \\frac{4}{9} + \\frac{2}{9} = \\frac{2}{5} - \\frac{2}{9} = \\frac{18-10}{45} = \\frac{8}{45}$.\nProjection on $q_2$ is $\\frac{16/105}{8/45} q_2(x) = (\\frac{16}{105} \\cdot \\frac{45}{8}) (x^2 - \\frac{1}{3}) = (\\frac{2 \\cdot 45}{105})(x^2 - \\frac{1}{3}) = \\frac{90}{105}(x^2 - \\frac{1}{3}) = \\frac{6}{7}(x^2 - \\frac{1}{3})$.\n\nAssembling $q_4(x)$:\n$q_4(x) = x^4 - \\frac{1}{5} - \\frac{6}{7}(x^2 - \\frac{1}{3}) = x^4 - \\frac{6}{7}x^2 - \\frac{1}{5} + \\frac{2}{7} = x^4 - \\frac{6}{7}x^2 + \\frac{-7+10}{35} = x^4 - \\frac{6}{7}x^2 + \\frac{3}{35}$.\n\nNormalization:\n$P_4(x) = c_4 q_4(x)$. The condition is $P_4(1) = 1$.\n$q_4(1) = 1^4 - \\frac{6}{7}(1^2) + \\frac{3}{35} = 1 - \\frac{6}{7} + \\frac{3}{35} = \\frac{35-30+3}{35} = \\frac{8}{35}$.\n$P_4(1) = c_4 \\cdot q_4(1) = c_4 \\cdot \\frac{8}{35} = 1 \\implies c_4 = \\frac{35}{8}$.\n\nFinally, the polynomial $P_4(x)$ is:\n$P_4(x) = \\frac{35}{8} \\left( x^4 - \\frac{6}{7}x^2 + \\frac{3}{35} \\right) = \\frac{35}{8}x^4 - \\frac{35}{8}\\frac{6}{7}x^2 + \\frac{35}{8}\\frac{3}{35}$.\n$P_4(x) = \\frac{35}{8}x^4 - \\frac{5 \\cdot 6}{8}x^2 + \\frac{3}{8} = \\frac{35}{8}x^4 - \\frac{30}{8}x^2 + \\frac{3}{8}$.\nSimplifying the coefficient of $x^2$:\n$P_4(x) = \\frac{35}{8}x^4 - \\frac{15}{4}x^2 + \\frac{3}{8}$.\nThis is the explicit closed-form expression for $P_4(x)$.\nThese polynomials are the standard Legendre polynomials.",
            "answer": "$$\n\\boxed{\\frac{35}{8}x^4 - \\frac{15}{4}x^2 + \\frac{3}{8}}\n$$"
        },
        {
            "introduction": "While continuous functions are mathematically elegant, many engineering problems start with data known only at discrete points. This practice  demonstrates the versatility of the Gram-Schmidt process by extending it to a vector space with a discrete inner product, defined as a sum over a set of nodes. This transition from the continuous integral to the discrete sum is a crucial conceptual step that connects orthogonalization to practical data analysis and discrete least-squares approximations.",
            "id": "2422225",
            "problem": "Consider the real vector space of polynomials of degree at most two, denoted by $\\mathcal{P}_2$. Equip $\\mathcal{P}_2$ with the discrete inner product defined by\n$$\\langle p, q \\rangle = \\sum_{i=0}^{2} p(x_i) q(x_i),$$\nwhere the nodes are $x_0=-1$, $x_1=0$, and $x_2=1$. Let $\\{1, x, x^2\\}$ be the ordered basis for $\\mathcal{P}_2$. By orthonormalizing this ordered set with respect to the given inner product, obtain an orthonormal basis $\\{\\varphi_0, \\varphi_1, \\varphi_2\\}$, where each $\\varphi_k$ has degree $k$ and has a positive leading coefficient.\n\nWhat is the leading coefficient of $\\varphi_2(x)$ when expressed in the standard monomial basis $\\{1, x, x^2\\}$? Provide the exact value; do not round.",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\nFirst, the givens are extracted verbatim from the problem statement:\n- Vector space: The real vector space of polynomials of degree at most two, denoted by $\\mathcal{P}_2$.\n- Inner product: $\\langle p, q \\rangle = \\sum_{i=0}^{2} p(x_i) q(x_i)$.\n- Nodes for the inner product: $x_0 = -1$, $x_1 = 0$, and $x_2 = 1$.\n- Ordered basis for $\\mathcal{P}_2$ to be orthonormalized: $\\{1, x, x^2\\}$.\n- Resulting orthonormal basis: $\\{\\varphi_0, \\varphi_1, \\varphi_2\\}$.\n- Constraint $1$: The degree of each polynomial $\\varphi_k$ is $k$.\n- Constraint $2$: Each $\\varphi_k$ has a positive leading coefficient.\n- Objective: Find the leading coefficient of $\\varphi_2(x)$.\n\nNext, the problem is validated against the required criteria.\n- **Scientific Grounding**: The problem is a standard application of the Gram-Schmidt orthogonalization process in a finite-dimensional vector space. The inner product is a discrete version of the standard $L^2$ inner product, commonly used in numerical analysis and approximation theory. The problem is firmly grounded in established mathematical principles of linear algebra.\n- **Well-Posedness**: The problem is well-posed. The set $\\{1, x, x^2\\}$ is a linearly independent set in $\\mathcal{P}_2$. The Gram-Schmidt process applied to a linearly independent set guarantees the existence of an orthogonal basis. The subsequent normalization is well-defined as the vectors are non-zero. The condition for a positive leading coefficient resolves the sign ambiguity in normalization, ensuring a unique orthonormal basis.\n- **Objectivity and Completeness**: The problem is stated in precise, objective mathematical language. All necessary information—the vector space, the basis, the inner product, and the constraints—is provided. The problem is self-contained and free of ambiguity.\n\nThe verdict is that the problem is valid. The solution can now proceed.\n\nThe solution requires applying the Gram-Schmidt process to the ordered basis $\\{v_0, v_1, v_2\\}$, where $v_0(x) = 1$, $v_1(x) = x$, and $v_2(x) = x^2$. The resulting orthogonal basis will be denoted $\\{u_0, u_1, u_2\\}$, which is then normalized to obtain the orthonormal basis $\\{\\varphi_0, \\varphi_1, \\varphi_2\\}$. The inner product is defined as $\\langle p, q \\rangle = p(-1)q(-1) + p(0)q(0) + p(1)q(1)$.\n\n**Step 1: Construct $\\varphi_0(x)$**\n\nThe first vector of the orthogonal basis is $u_0(x) = v_0(x) = 1$.\nIts squared norm is calculated:\n$$ \\|u_0\\|^2 = \\langle 1, 1 \\rangle = (1)(1) + (1)(1) + (1)(1) = 3 $$\nThe norm is $\\|u_0\\| = \\sqrt{3}$.\nThe first orthonormal vector $\\varphi_0(x)$ is obtained by normalizing $u_0(x)$:\n$$ \\varphi_0(x) = \\frac{u_0(x)}{\\|u_0\\|} = \\frac{1}{\\sqrt{3}} $$\nThe degree of $\\varphi_0(x)$ is $0$, and its leading coefficient is $\\frac{1}{\\sqrt{3}} > 0$. This satisfies the given conditions.\n\n**Step 2: Construct $\\varphi_1(x)$**\n\nThe second orthogonal vector $u_1(x)$ is found by subtracting the projection of $v_1(x)$ onto $u_0(x)$ from $v_1(x)$:\n$$ u_1(x) = v_1(x) - \\frac{\\langle v_1, u_0 \\rangle}{\\langle u_0, u_0 \\rangle} u_0(x) $$\nWe compute the inner product $\\langle v_1, u_0 \\rangle = \\langle x, 1 \\rangle$:\n$$ \\langle x, 1 \\rangle = (-1)(1) + (0)(1) + (1)(1) = -1 + 0 + 1 = 0 $$\nSince the inner product is $0$, the vectors $v_1$ and $u_0$ are already orthogonal. Thus:\n$$ u_1(x) = v_1(x) - \\frac{0}{3} (1) = x $$\nNext, we calculate the norm of $u_1(x)$:\n$$ \\|u_1\\|^2 = \\langle x, x \\rangle = (-1)(-1) + (0)(0) + (1)(1) = 1 + 0 + 1 = 2 $$\nThe norm is $\\|u_1\\| = \\sqrt{2}$.\nThe second orthonormal vector $\\varphi_1(x)$ is:\n$$ \\varphi_1(x) = \\frac{u_1(x)}{\\|u_1\\|} = \\frac{x}{\\sqrt{2}} $$\nThe degree of $\\varphi_1(x)$ is $1$, and its leading coefficient is $\\frac{1}{\\sqrt{2}} > 0$. This satisfies the given conditions.\n\n**Step 3: Construct $\\varphi_2(x)$**\n\nThe third orthogonal vector $u_2(x)$ is found by subtracting the projections of $v_2(x)$ onto $u_0(x)$ and $u_1(x)$ from $v_2(x)$:\n$$ u_2(x) = v_2(x) - \\frac{\\langle v_2, u_0 \\rangle}{\\langle u_0, u_0 \\rangle} u_0(x) - \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1(x) $$\nWe compute the required inner products:\n$$ \\langle v_2, u_0 \\rangle = \\langle x^2, 1 \\rangle = (-1)^2(1) + (0)^2(1) + (1)^2(1) = 1 + 0 + 1 = 2 $$\n$$ \\langle v_2, u_1 \\rangle = \\langle x^2, x \\rangle = (-1)^2(-1) + (0)^2(0) + (1)^2(1) = -1 + 0 + 1 = 0 $$\nSubstituting these values into the expression for $u_2(x)$:\n$$ u_2(x) = x^2 - \\frac{2}{3}(1) - \\frac{0}{2}(x) = x^2 - \\frac{2}{3} $$\nThe leading coefficient of $u_2(x)$ is $1$, which is positive. If it were negative, we would multiply $u_2(x)$ by $-1$ before normalization to eventually satisfy the positive leading coefficient requirement for $\\varphi_2(x)$.\n\nFinally, we normalize $u_2(x)$. First, we find its squared norm:\n$$ \\|u_2\\|^2 = \\left\\langle x^2 - \\frac{2}{3}, x^2 - \\frac{2}{3} \\right\\rangle $$\nTo compute this, we evaluate $u_2(x)$ at the nodes $x_0 = -1$, $x_1 = 0$, $x_2 = 1$:\n$u_2(-1) = (-1)^2 - \\frac{2}{3} = 1 - \\frac{2}{3} = \\frac{1}{3}$.\n$u_2(0) = (0)^2 - \\frac{2}{3} = -\\frac{2}{3}$.\n$u_2(1) = (1)^2 - \\frac{2}{3} = 1 - \\frac{2}{3} = \\frac{1}{3}$.\nThe squared norm is:\n$$ \\|u_2\\|^2 = (u_2(-1))^2 + (u_2(0))^2 + (u_2(1))^2 = \\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{9} + \\frac{4}{9} + \\frac{1}{9} = \\frac{6}{9} = \\frac{2}{3} $$\nThe norm is $\\|u_2\\| = \\sqrt{\\frac{2}{3}}$.\nThe third orthonormal vector $\\varphi_2(x)$ is:\n$$ \\varphi_2(x) = \\frac{u_2(x)}{\\|u_2\\|} = \\frac{x^2 - \\frac{2}{3}}{\\sqrt{\\frac{2}{3}}} = \\sqrt{\\frac{3}{2}}\\left(x^2 - \\frac{2}{3}\\right) = \\sqrt{\\frac{3}{2}}x^2 - \\sqrt{\\frac{3}{2}}\\frac{2}{3} $$\nThe degree of $\\varphi_2(x)$ is $2$. Its leading coefficient is $\\sqrt{\\frac{3}{2}}$, which is positive. The conditions are met.\n\nThe problem asks for the leading coefficient of $\\varphi_2(x)$. From the expression derived, this is the coefficient of the $x^2$ term.\nThe leading coefficient is $\\sqrt{\\frac{3}{2}}$.",
            "answer": "$$\\boxed{\\sqrt{\\frac{3}{2}}}$$"
        },
        {
            "introduction": "Moving from theoretical calculation to practical implementation reveals the critical role of numerical stability, a core concern in computational engineering. The classical Gram-Schmidt algorithm, while exact in theory, can suffer from a severe loss of orthogonality in floating-point arithmetic. This coding exercise  challenges you to implement the more numerically robust Modified Gram-Schmidt (MGS) algorithm and to address the practical issue of linear dependence by incorporating a tolerance-based check, creating a function-level computational tool.",
            "id": "2422283",
            "problem": "You are given a finite sequence of real column vectors to be orthogonalized using the Gram-Schmidt process. In computational engineering, numerical robustness requires detecting when a new vector adds no new direction beyond numerical resolution. Starting only from core definitions of inner product, projection, norm, and orthogonality in Euclidean space, design and implement an algorithm that orthonormalizes a set of input vectors while detecting and handling linear dependence by thresholding near-zero norms.\n\nDefinitions and requirements:\n- Work in the Euclidean space $\\mathbb{R}^n$ with the standard inner product $x^\\top y$ and the induced Euclidean norm $\\|x\\|_2 = \\sqrt{x^\\top x}$. The Frobenius norm for matrices is $\\|M\\|_F = \\sqrt{\\sum_{i,j} M_{ij}^2}$.\n- Given an ordered set of input vectors $\\{v_1,\\dots,v_m\\}$ in $\\mathbb{R}^n$, define the orthogonal projection of a vector $v$ onto a nonempty subspace spanned by $\\{q_1,\\dots,q_r\\}$ with $q_i$ pairwise orthonormal as $\\mathrm{proj}_{\\mathrm{span}\\{q_i\\}}(v) = \\sum_{i=1}^r (q_i^\\top v) q_i$.\n- The Gram-Schmidt orthogonalization constructs an orthonormal set $\\{q_1,\\dots,q_r\\}$ by iteratively subtracting projections of each new vector $v_j$ onto the span of previously accepted $q_i$ and normalizing the residual. You must use the modified Gram-Schmidt ordering that computes inner products against the current $q_i$ and updates the residual $v$ immediately after each subtraction.\n- To detect linear dependence numerically, introduce a nonnegative threshold $\\tau$. When processing input vector $v_j$, after subtracting projections onto the current span, compute the residual norm $\\|v_j^\\perp\\|_2$. If $\\|v_j^\\perp\\|_2 \\le \\tau$, treat $v_j$ as linearly dependent and do not add a new $q$; otherwise, include $q_{r+1} = v_j^\\perp / \\|v_j^\\perp\\|_2$.\n- Assemble the upper-triangular matrix $R \\in \\mathbb{R}^{r \\times m}$ with entries $r_{ij} = q_i^\\top v_j$ for $i \\le r$ and the diagonal $r_{jj} = \\|v_j^\\perp\\|_2$ only when $v_j$ is accepted; otherwise, for rejected $v_j$, do not increase $r$ and omit a new row in $R$. The relation $A \\approx Q R$ should hold, where $A \\in \\mathbb{R}^{n \\times m}$ has $v_j$ as columns, $Q \\in \\mathbb{R}^{n \\times r}$ has $q_i$ as columns, and $\\approx$ denotes equality up to floating-point arithmetic.\n- Indices of columns are zero-based integers, that is, the first column has index $0$, the second has index $1$, and so on.\n\nYour program must:\n- Implement the modified Gram-Schmidt with dependence detection described above. Inputs are a real matrix $A \\in \\mathbb{R}^{n \\times m}$ and a scalar threshold $\\tau \\ge 0$. Outputs must include:\n  - $Q \\in \\mathbb{R}^{n \\times r}$ with orthonormal columns,\n  - $R \\in \\mathbb{R}^{r \\times m}$ upper triangular in the sense that entries below row $i$ in column $j < i$ are zero,\n  - a list of zero-based indices of accepted (independent) columns,\n  - a list of zero-based indices of rejected (dependent) columns,\n  - the orthonormality error $\\|Q^\\top Q - I_r\\|_F$,\n  - the reconstruction error $\\|A - Q R\\|_F$,\n  - the estimated rank $r$ (the number of accepted vectors).\n- Use double-precision floating point arithmetic.\n- For validation, check that the orthonormality error and the reconstruction error are each less than or equal to $10^{-10}$.\n\nTest suite:\nFor each case below, apply your implementation to the specified $A$ and $\\tau$, and verify the expected outcomes. All matrices are given column-wise.\n\n- Case $1$ (general independent set):\n  - $A_1 = \\begin{bmatrix}\n  1 & 1 & 0 \\\\\n  0 & 1 & 1 \\\\\n  1 & 0 & 1\n  \\end{bmatrix}$, $\\tau_1 = 10^{-12}$\n  - Expected: rank $3$, dependent indices $[\\ ]$ (empty list).\n\n- Case $2$ (exact linear dependence and overcomplete):\n  - $A_2 = \\begin{bmatrix}\n  1 & 0 & 1 & 0 \\\\\n  0 & 1 & 1 & 0 \\\\\n  0 & 0 & 0 & 1\n  \\end{bmatrix}$ where the columns are $v_0 = [1,0,0]^\\top$, $v_1 = [0,1,0]^\\top$, $v_2 = v_0 + v_1$, $v_3 = [0,0,1]^\\top$; $\\tau_2 = 10^{-12}$\n  - Expected: rank $3$, dependent indices $[2]$.\n\n- Case $3$ (near-zero column vector):\n  - $A_3 = \\begin{bmatrix}\n  1 & 10^{-14} & 0 \\\\\n  2 & 0 & 1 \\\\\n  3 & 0 & 0\n  \\end{bmatrix}$, $\\tau_3 = 10^{-12}$\n  - Expected: rank $2$, dependent indices $[1]$.\n\n- Case $4$ (near linear dependence under tolerance):\n  - $A_4 = \\begin{bmatrix}\n  1 & 1 + 10^{-12} & 0 & 0 & 0 \\\\\n  0 & 10^{-12} & 1 & 0 & 0 \\\\\n  0 & 0 & 0 & 1 & 0 \\\\\n  0 & 0 & 0 & 0 & 1\n  \\end{bmatrix}$ with columns $v_0 = [1,0,0,0]^\\top$, $v_1 = [1+10^{-12}, 10^{-12}, 0, 0]^\\top$, $v_2 = [0,1,0,0]^\\top$, $v_3 = [0,0,1,0]^\\top$, $v_4 = [0,0,0,1]^\\top$; $\\tau_4 = 10^{-10}$\n  - Expected: rank $4$, dependent indices $[1]$.\n\nFor each case, your program must compute the outputs and then verify simultaneously that:\n- the estimated rank $r$ equals the expected rank for that case;\n- the list of dependent indices equals the expected list for that case;\n- the orthonormality error is $\\le 10^{-10}$;\n- the reconstruction error is $\\le 10^{-10}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean corresponding to Cases $1$ through $4$ in order, and is `True` if and only if all validations for that case pass, else `False`. For example, a valid output looks like `[`True`,`False`,`True`,`True`]`.",
            "solution": "The problem presented is the construction of a numerically robust, rank-revealing QR factorization based on the Modified Gram-Schmidt (MGS) algorithm. This is a fundamental task in computational engineering and numerical linear algebra. The objective is to decompose a given matrix $A \\in \\mathbb{R}^{n \\times m}$ into the product of a matrix $Q \\in \\mathbb{R}^{n \\times r}$ with orthonormal columns and a matrix $R \\in \\mathbb{R}^{r \\times m}$, such that $A \\approx QR$. Here, $r \\le \\min(n, m)$ is the numerically determined rank of $A$. The process must identify and handle linear dependence among the columns of $A$ by using a tolerance threshold $\\tau$.\n\nLet the input matrix be $A = [v_0, v_1, \\dots, v_{m-1}]$, where $v_j \\in \\mathbb{R}^n$ are the column vectors. The Gram-Schmidt process constructs an orthonormal set of vectors $\\{q_0, q_1, \\dots, q_{r-1}\\}$ that spans the same space as the linearly independent subset of $\\{v_j\\}$.\n\nThe Classical Gram-Schmidt (CGS) process computes each new orthonormal vector $q_j$ by orthogonalizing the input vector $v_j$ against all previously computed orthonormal vectors $\\{q_0, \\dots, q_{j-1}\\}$:\n$$ v_j^\\perp = v_j - \\sum_{i=0}^{j-1} (q_i^\\top v_j) q_i, \\quad q_j = \\frac{v_j^\\perp}{\\|v_j^\\perp\\|_2} $$\nIn finite-precision arithmetic, this formulation suffers from a cumulative loss of orthogonality. The computation of each inner product $(q_i^\\top v_j)$ uses the original vector $v_j$, which can reintroduce components along directions $q_i$ that were supposed to have been removed, due to rounding errors.\n\nThe Modified Gram-Schmidt (MGS) process mitigates this numerical instability. Instead of projecting a single vector $v_j$ onto the entire existing orthonormal basis at once, MGS orthogonalizes all remaining vectors against each new orthonormal vector as soon as it is generated. The algorithm proceeds as follows:\n\nLet $V^{(0)} = A$. For each step $j=0, 1, \\dots, m-1$, we take the vector $u_j = V^{(j)}_{:,j}$, which is the $j$-th column of the matrix after $j$ stages of orthogonalization. This vector is then normalized to produce the next orthonormal vector $q_j$. Subsequently, the projection of all remaining vectors $V^{(j)}_{:,k}$ for $k > j$ onto $q_j$ is subtracted.\n\nThe algorithm as specified for this problem incorporates a tolerance $\\tau$ for detecting linear dependence. It can be formalized as follows:\n\n1.  Initialize a working copy of the input matrix, $V \\leftarrow A$. Initialize an empty list for the orthonormal vectors, $Q_{\\text{list}}$, and lists to record the indices of accepted and rejected vectors.\n\n2.  Iterate through the columns of $V$ from $j=0$ to $m-1$. In each iteration $j$:\n    a.  Extract the current vector to be orthogonalized, $u \\leftarrow V_{:,j}$.\n    b.  Compute its Euclidean norm, $\\rho = \\|u\\|_2$.\n    c.  Compare the norm with the threshold $\\tau$. If $\\rho > \\tau$, the vector $u$ is considered to contribute a new independent direction.\n        i.  The vector is accepted: its original index $j$ is recorded.\n        ii. A new orthonormal vector is computed and stored: $q = u/\\rho$.\n        iii. This new vector $q$ is used to update all subsequent columns of $V$. For each $k$ from $j+1$ to $m-1$, the projection of $V_{:,k}$ onto $q$ is subtracted: $V_{:,k} \\leftarrow V_{:,k} - (q^\\top V_{:,k})q$. This is the defining step of MGS.\n    d.  If $\\rho \\le \\tau$, the vector $u$ is numerically zero. This signifies that the original vector $v_j$ is linearly dependent on the preceding vectors $\\{v_0, \\dots, v_{j-1}\\}$. The vector is rejected, its index $j$ is recorded, and no new orthonormal vector is generated. The subsequent columns of $V$ are not updated in this step as the update would be negligible.\n\n3.  After iterating through all $m$ columns, the process terminates. The set of collected orthonormal vectors $\\{q_k\\}$ forms the columns of the matrix $Q$. The rank $r$ is the number of accepted vectors. If $r > 0$, $Q$ is constructed by stacking the vectors in $Q_{\\text{list}}$. If $r=0$, $Q$ is an $n \\times 0$ empty matrix.\n\n4.  The matrix $R \\in \\mathbb{R}^{r \\times m}$ must satisfy the relation $A \\approx QR$. From this, it follows that $Q^\\top A \\approx Q^\\top(QR) = (Q^\\top Q)R = I_r R = R$. Therefore, the most direct and numerically sound method to compute $R$ after $Q$ has been determined is via the matrix product $R = Q^\\top A_{original}$.\n\n5.  The problem statement provides definitions for the entries of $R$ which must be shown consistent with this construction. The entry $r_{kj}$ (using $k$ for the row index of $R$ corresponding to $q_k$) is defined as $r_{kj} = q_k^\\top v_j$. This is precisely the definition from $R = Q^\\top A$. It is also stated that for an accepted vector $v_j$ which becomes the source for $q_k$, the \"diagonal\" entry $r_{kj}$ should be $\\|v_j^\\perp\\|_2$, where $v_j^\\perp$ is the residual of $v_j$ after orthogonalization against $\\{q_0, \\dots, q_{k-1}\\}$. The identity $\\|v_j^\\perp\\|_2 = q_k^\\top v_j$ holds because $q_k = v_j^\\perp / \\|v_j^\\perp\\|_2$ and $q_k$ is orthogonal to the components subtracted from $v_j$ to form $v_j^\\perp$. Thus, the definitions are consistent.\n\n6.  Finally, the algorithm must compute two error metrics for verification:\n    - The orthonormality error: $\\|Q^\\top Q - I_r\\|_F$, which measures how close the columns of $Q$ are to being an orthonormal set.\n    - The reconstruction error: $\\|A - QR\\|_F$, which measures how well the factorization reconstructs the original matrix $A$. The product $QR = Q(Q^\\top A) = (QQ^\\top)A$ represents the projection of $A$ onto the column space of $Q$. The error is the norm of the component of $A$ orthogonal to this space.\n\nThis correctly designed algorithm will produce the required outputs ($Q$, $R$, rank, index lists, and errors) for any given matrix $A$ and threshold $\\tau$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef modified_gram_schmidt(A, tau):\n    \"\"\"\n    Performs rank-revealing QR factorization using the Modified Gram-Schmidt process.\n\n    Args:\n        A (np.ndarray): The input matrix of shape (n, m) with columns to be orthogonalized.\n        tau (float): The non-negative threshold for detecting linear dependence.\n\n    Returns:\n        dict: A dictionary containing the following outputs:\n            'Q': The matrix with orthonormal columns, shape (n, r).\n            'R': The upper-triangular matrix, shape (r, m).\n            'accepted_indices': A list of zero-based indices of accepted columns.\n            'rejected_indices': A list of zero-based indices of rejected columns.\n            'orthonormality_error': The Frobenius norm of ||Q.T @ Q - I||.\n            'reconstruction_error': The Frobenius norm of ||A - Q @ R||.\n            'rank': The computed rank r.\n    \"\"\"\n    n, m = A.shape\n    V = A.copy().astype(np.float64)  # Use double precision\n    \n    Q_list = []\n    accepted_indices = []\n    rejected_indices = []\n    \n    for j in range(m):\n        u = V[:, j]\n        norm_u = np.linalg.norm(u)\n        \n        if norm_u > tau:\n            accepted_indices.append(j)\n            q = u / norm_u\n            Q_list.append(q)\n            \n            # Update subsequent vectors in V using the new q vector\n            for k in range(j + 1, m):\n                V[:, k] -= np.dot(q, V[:, k]) * q\n        else:\n            rejected_indices.append(j)\n\n    rank = len(accepted_indices)\n\n    if rank > 0:\n        Q = np.stack(Q_list, axis=1)\n        R = Q.T @ A\n    else:\n        # Handle the case where the rank is 0\n        Q = np.zeros((n, 0), dtype=np.float64)\n        R = np.zeros((0, m), dtype=np.float64)\n\n    # Calculate errors\n    if rank > 0:\n        I_r = np.identity(rank, dtype=np.float64)\n        orthonormality_error = np.linalg.norm(Q.T @ Q - I_r, 'fro')\n    else:\n        orthonormality_error = 0.0\n\n    reconstruction_error = np.linalg.norm(A - Q @ R, 'fro')\n\n    return {\n        'Q': Q,\n        'R': R,\n        'accepted_indices': accepted_indices,\n        'rejected_indices': rejected_indices,\n        'orthonormality_error': orthonormality_error,\n        'reconstruction_error': reconstruction_error,\n        'rank': rank\n    }\n\ndef solve():\n    \"\"\"\n    Runs the test suite and validates the results against expectations.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[1, 1, 0], \n                           [0, 1, 1], \n                           [1, 0, 1]], dtype=np.float64),\n            \"tau\": 1e-12,\n            \"expected_rank\": 3,\n            \"expected_dependent_indices\": []\n        },\n        {\n            \"A\": np.array([[1, 0, 1, 0], \n                           [0, 1, 1, 0], \n                           [0, 0, 0, 1]], dtype=np.float64),\n            \"tau\": 1e-12,\n            \"expected_rank\": 3,\n            \"expected_dependent_indices\": [2]\n        },\n        {\n            \"A\": np.array([[1, 1e-14, 0], \n                           [2, 0,     1], \n                           [3, 0,     0]], dtype=np.float64),\n            \"tau\": 1e-12,\n            \"expected_rank\": 2,\n            \"expected_dependent_indices\": [1]\n        },\n        {\n            \"A\": np.array([[1, 1 + 1e-12, 0, 0, 0], \n                           [0, 1e-12,     1, 0, 0],\n                           [0, 0,         0, 1, 0],\n                           [0, 0,         0, 0, 1]], dtype=np.float64),\n            \"tau\": 1e-10,\n            \"expected_rank\": 4,\n            \"expected_dependent_indices\": [1]\n        },\n    ]\n\n    results = []\n    error_tolerance = 1e-10\n\n    for case in test_cases:\n        A = case[\"A\"]\n        tau = case[\"tau\"]\n        \n        output = modified_gram_schmidt(A, tau)\n        \n        rank_check = output['rank'] == case['expected_rank']\n        indices_check = sorted(output['rejected_indices']) == sorted(case['expected_dependent_indices'])\n        ortho_err_check = output['orthonormality_error'] <= error_tolerance\n        recon_err_check = output['reconstruction_error'] <= error_tolerance\n\n        is_valid = all([rank_check, indices_check, ortho_err_check, recon_err_check])\n        results.append(is_valid)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}