{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we first solidify our understanding of how different norms measure the \"size\" of a matrix. This exercise asks you to calculate several common matrix norms for a Givens rotation matrix, a transformation that is fundamental to many numerical algorithms like QR decomposition. By comparing how the $\\ell_1$, $\\ell_2$, $\\ell_\\infty$, and Frobenius norms evaluate this simple rotation, you will see firsthand that a transformation's measured size depends entirely on the norm you choose, which is invaluable for learning to interpret its meaning correctly .",
            "id": "2449542",
            "problem": "In numerical linear algebra for computational engineering, a Givens rotation is used to introduce zeros while preserving Euclidean lengths. Let $n \\ge 2$ be an integer and consider the $n \\times n$ Givens rotation matrix $G$ that acts on coordinate indices $p$ and $q$ with $1 \\le p  q \\le n$ by replacing the $2 \\times 2$ principal submatrix in rows and columns $\\{p,q\\}$ with\n$$\n\\begin{pmatrix}\n\\cos\\theta  \\sin\\theta \\\\\n-\\sin\\theta  \\cos\\theta\n\\end{pmatrix},\n$$\nand leaving all other diagonal entries equal to $1$ and all other off-diagonal entries equal to $0$. Using only the definitions of induced matrix norms and the Frobenius norm, determine exact expressions, as functions of $n$ and $\\theta$, for the matrix $1$-norm $\\lVert G \\rVert_{1}$, the matrix $2$-norm $\\lVert G \\rVert_{2}$, the matrix $\\infty$-norm $\\lVert G \\rVert_{\\infty}$, and the Frobenius norm $\\lVert G \\rVert_{F}$. Provide your final answer as exact analytic expressions. No rounding is required. Finally, briefly interpret your result in terms of length preservation and norm dependence.",
            "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- $n$ is an integer, $n \\ge 2$.\n- $G$ is an $n \\times n$ Givens rotation matrix.\n- The matrix acts on coordinate indices $p$ and $q$ where $1 \\le p  q \\le n$.\n- The $2 \\times 2$ principal submatrix at rows and columns $\\{p,q\\}$ is $\\begin{pmatrix} \\cos\\theta  \\sin\\theta \\\\ -\\sin\\theta  \\cos\\theta \\end{pmatrix}$.\n- All other diagonal entries are $1$.\n- All other off-diagonal entries are $0$.\n- The task is to determine exact expressions for $\\lVert G \\rVert_{1}$, $\\lVert G \\rVert_{2}$, $\\lVert G \\rVert_{\\infty}$, and $\\lVert G \\rVert_{F}$ using only the definitions of the norms.\n- An interpretation of the result is required.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on standard, well-defined concepts in numerical linear algebra, specifically Givens rotations and matrix norms. These are fundamental tools in computational engineering.\n- **Well-Posed:** The matrix $G$ is unambiguously defined for any valid $n$, $p$, $q$, and $\\theta$. The matrix norms are standard mathematical functions with unique outputs for any given matrix. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical language, free from subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, objective, and complete. It is therefore deemed **valid**. We may proceed with the solution.\n\nThe Givens rotation matrix $G$ is an $n \\times n$ matrix that is identical to the identity matrix $I_n$ except for four entries. The entries of $G$, denoted by $g_{ij}$, are:\n$g_{pp} = \\cos\\theta$\n$g_{qq} = \\cos\\theta$\n$g_{pq} = \\sin\\theta$\n$g_{qp} = -\\sin\\theta$\n$g_{ii} = 1$ for $i \\in \\{1, \\dots, n\\} \\setminus \\{p, q\\}$\n$g_{ij} = 0$ for all other pairs $(i, j)$ with $i \\ne j$.\n\nWe will now compute the required norms based on their fundamental definitions.\n\n**1. The Matrix $1$-Norm ($\\lVert G \\rVert_{1}$)**\nThe matrix $1$-norm, or column-sum norm, is defined as the maximum absolute column sum:\n$$ \\lVert G \\rVert_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |g_{ij}| $$\nWe examine the columns of $G$:\n- For any column $j$ such that $j \\ne p$ and $j \\ne q$, the only non-zero entry is $g_{jj} = 1$. The sum of absolute values for such a column is $|1| = 1$.\n- For column $p$, the non-zero entries are $g_{pp} = \\cos\\theta$ and $g_{qp} = -\\sin\\theta$. The sum is $|g_{pp}| + |g_{qp}| = |\\cos\\theta| + |-\\sin\\theta| = |\\cos\\theta| + |\\sin\\theta|$.\n- For column $q$, the non-zero entries are $g_{pq} = \\sin\\theta$ and $g_{qq} = \\cos\\theta$. The sum is $|g_{pq}| + |g_{qq}| = |\\sin\\theta| + |\\cos\\theta|$.\n\nThe $1$-norm is the maximum of these sums:\n$$ \\lVert G \\rVert_{1} = \\max(1, |\\cos\\theta| + |\\sin\\theta|) $$\nWe know that for any real $\\theta$, $|\\cos\\theta| + |\\sin\\theta| \\ge \\sqrt{\\cos^2\\theta + \\sin^2\\theta} = 1$. The equality holds when $\\theta$ is an integer multiple of $\\frac{\\pi}{2}$. Therefore, the maximum is always $|\\cos\\theta| + |\\sin\\theta|$.\n$$ \\lVert G \\rVert_{1} = |\\cos\\theta| + |\\sin\\theta| $$\n\n**2. The Matrix $\\infty$-Norm ($\\lVert G \\rVert_{\\infty}$)**\nThe matrix $\\infty$-norm, or row-sum norm, is defined as the maximum absolute row sum:\n$$ \\lVert G \\rVert_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |g_{ij}| $$\nWe examine the rows of $G$:\n- For any row $i$ such that $i \\ne p$ and $i \\ne q$, the only non-zero entry is $g_{ii} = 1$. The sum of absolute values for such a row is $|1| = 1$.\n- For row $p$, the non-zero entries are $g_{pp} = \\cos\\theta$ and $g_{pq} = \\sin\\theta$. The sum is $|g_{pp}| + |g_{pq}| = |\\cos\\theta| + |\\sin\\theta|$.\n- For row $q$, the non-zero entries are $g_{qp} = -\\sin\\theta$ and $g_{qq} = \\cos\\theta$. The sum is $|g_{qp}| + |g_{qq}| = |-\\sin\\theta| + |\\cos\\theta| = |\\sin\\theta| + |\\cos\\theta|$.\n\nBy the same logic as for the $1$-norm, the maximum of these sums is:\n$$ \\lVert G \\rVert_{\\infty} = |\\cos\\theta| + |\\sin\\theta| $$\n\n**3. The Matrix $2$-Norm ($\\lVert G \\rVert_{2}$)**\nThe matrix $2$-norm, or spectral norm, is defined as the square root of the largest eigenvalue of the matrix $G^T G$:\n$$ \\lVert G \\rVert_{2} = \\sqrt{\\lambda_{\\max}(G^T G)} $$\nThe matrix $G$ is an orthogonal matrix. We verify this by computing $G^T G$. The transpose $G^T$ is identical to $G$ except that the roles of $g_{pq}$ and $g_{qp}$ are swapped. Thus, $g_{pq}^T = g_{qp} = -\\sin\\theta$ and $g_{qp}^T = g_{pq} = \\sin\\theta$.\nThe product $G^T G$ will be an identity matrix. To see this, consider the submatrix action. The $2 \\times 2$ submatrix of $G$ is $R(\\theta) = \\begin{pmatrix} \\cos\\theta  \\sin\\theta \\\\ -\\sin\\theta  \\cos\\theta \\end{pmatrix}$. Its transpose is $R(\\theta)^T = \\begin{pmatrix} \\cos\\theta  -\\sin\\theta \\\\ \\sin\\theta  \\cos\\theta \\end{pmatrix}$.\nThe product is:\n$$ R(\\theta)^T R(\\theta) = \\begin{pmatrix} \\cos^2\\theta + \\sin^2\\theta  \\cos\\theta\\sin\\theta - \\sin\\theta\\cos\\theta \\\\ \\sin\\theta\\cos\\theta - \\cos\\theta\\sin\\theta  \\sin^2\\theta + \\cos^2\\theta \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} $$\nSince all other rows and columns of $G$ are standard basis vectors, the full matrix product $G^T G$ results in the $n \\times n$ identity matrix, $I_n$.\nThe eigenvalues of $I_n$ are all equal to $1$. Thus, $\\lambda_{\\max}(G^T G) = 1$.\nThe $2$-norm is therefore:\n$$ \\lVert G \\rVert_{2} = \\sqrt{1} = 1 $$\n\n**4. The Frobenius Norm ($\\lVert G \\rVert_{F}$)**\nThe Frobenius norm is defined as the square root of the sum of the squares of the magnitudes of all entries:\n$$ \\lVert G \\rVert_{F} = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{n} |g_{ij}|^2} $$\nWe sum the squares of all non-zero entries of $G$:\n- $(n-2)$ entries are equal to $1$ (the diagonal entries for indices other than $p$ and $q$). Their contribution to the sum of squares is $(n-2) \\times 1^2 = n-2$.\n- The four entries in the $\\{p,q\\}$ sub-block are $\\cos\\theta$, $\\sin\\theta$, $-\\sin\\theta$, and $\\cos\\theta$. Their contribution to the sum of squares is $(\\cos\\theta)^2 + (\\sin\\theta)^2 + (-\\sin\\theta)^2 + (\\cos\\theta)^2 = 2\\cos^2\\theta + 2\\sin^2\\theta = 2(\\cos^2\\theta + \\sin^2\\theta) = 2$.\n\nThe total sum of squares is $(n-2) + 2 = n$.\nThe Frobenius norm is therefore:\n$$ \\lVert G \\rVert_{F} = \\sqrt{n} $$\n\n**Interpretation of Results**\nThe problem asks for an interpretation in terms of length preservation and norm dependence.\n- **Length Preservation:** The result $\\lVert G \\rVert_{2} = 1$ is the mathematical statement of the geometric property that Givens rotations are isometries in Euclidean space. The $2$-norm of a matrix measures the maximum factor by which it can stretch a vector's Euclidean length ($\\lVert x \\rVert_2$). A value of $1$ signifies that no vector's length is changed, i.e., $\\lVert Gx \\rVert_{2} = \\lVert x \\rVert_{2}$ for all $x \\in \\mathbb{R}^n$. This is precisely what is meant by \"preserving Euclidean lengths\".\n- **Norm Dependence:** The results demonstrate that the \"size\" of the matrix $G$ depends on the chosen norm. While it is a pure rotation of size $1$ in the Euclidean sense ($\\lVert G \\rVert_2=1$), its $1$-norm and $\\infty$-norm, $\\lVert G \\rVert_{1} = \\lVert G \\rVert_{\\infty} = |\\cos\\theta| + |\\sin\\theta|$, depend on the rotation angle $\\theta$ and can be as large as $\\sqrt{2}$. This shows that a transformation that is a pure rotation for the Euclidean norm can act as an amplification for vectors measured in other norms (e.g., the $1$-norm or $\\infty$-norm). The Frobenius norm, $\\lVert G \\rVert_{F}=\\sqrt{n}$, depends only on the dimension $n$ of the space, not the angle $\\theta$. This is because a rotation merely redistributes the matrix elements' squared magnitudes within the active sub-block, leaving their sum invariant, while the norm counts all entries, including the $n-2$ invariant unit entries on the diagonal. The different results for different norms highlight that there is no single, universal measure for the \"size\" of a linear transformation; the measure is dependent on the metric used.",
            "answer": "$$ \\boxed{\\begin{pmatrix} |\\cos\\theta| + |\\sin\\theta|  1  |\\cos\\theta| + |\\sin\\theta|  \\sqrt{n} \\end{pmatrix}} $$"
        },
        {
            "introduction": "Moving from foundational calculations to a practical application, we now explore how vector norms are used to steer and terminate large-scale computations. In this exercise, you will implement a Jacobi iterative solver for the Laplace equation, a cornerstone problem in computational engineering, and use vector norms to track the algorithm's convergence. This coding practice bridges the gap between theory and application by showing how the $\\ell_2$ and $\\ell_\\infty$ norms of a residual vector provide a quantitative measure of error, forming the basis for the stopping criteria that determine the accuracy and efficiency of the simulation .",
            "id": "2449163",
            "problem": "You will write a complete program that compares the convergence behavior of the Jacobi iterative method for the discretized Laplace equation by tracking the residual vector in two different norms: the Euclidean norm (also called the $\\ell_2$ norm) and the maximum norm (also called the $\\ell_\\infty$ norm). The setting is the unit square domain $\\Omega = [0,1]\\times[0,1]$, governed by the Laplace equation $\\Delta u = 0$ with Dirichlet boundary data chosen from a known harmonic function so that the interior source term is identically zero. The algorithmic task is to discretize the problem, implement Jacobi iteration, form the residual vector at each iteration, and record the iteration indices at which the relative residual drops below a prescribed tolerance in each norm. The goal is to quantitatively compare how many iterations are required under the $\\ell_2$ and $\\ell_\\infty$ norms.\n\nUse the following mathematically standard and widely accepted foundations as the starting point. The Laplace equation $\\Delta u = u_{xx} + u_{yy} = 0$ on a rectangular grid is discretized by the five-point central difference stencil derived from the second-derivative approximation $u_{xx}(x_{i},y_{j}) \\approx \\left(u_{i-1,j} - 2u_{i,j} + u_{i+1,j}\\right)/h^{2}$ and $u_{yy}(x_{i},y_{j}) \\approx \\left(u_{i,j-1} - 2u_{i,j} + u_{i,j+1}\\right)/h^{2}$, where $h$ is the uniform mesh spacing. For interior points, the discrete Laplace equation is equivalent to the linear system with the discrete operator whose action at grid point $(i,j)$ can be written as $4u_{i,j} - \\left(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1}\\right) = 0$ after multiplying by $h^{2}$, which does not affect the zero right-hand side. The Jacobi iteration arises by solving this equation for the new iterate at each interior point as the arithmetic average of its four neighbors, while treating boundary values as fixed. The residual vector at iteration $k$ is the discrete defect $r^{(k)}$ whose components at interior points are $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - \\left(u_{i-1,j}^{(k)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k)} + u_{i,j+1}^{(k)}\\right)$, which would be zero for an exact discrete solution. For a vector $v$ with components $v_{\\ell}$, the $\\ell_2$ norm is $\\lVert v \\rVert_{2} = \\left(\\sum_{\\ell} v_{\\ell}^{2}\\right)^{1/2}$ and the $\\ell_\\infty$ norm is $\\lVert v \\rVert_{\\infty} = \\max_{\\ell} |v_{\\ell}|$.\n\nImplement the following specification.\n\n- Domain and grid: Use the unit square $\\Omega = [0,1]\\times[0,1]$ with a uniform grid of $N\\times N$ points, where $h = 1/(N-1)$, grid indices $i,j \\in \\{0,1,\\dots,N-1\\}$, and interior indices $i,j \\in \\{1,2,\\dots,N-2\\}$.\n- Boundary data: Impose the harmonic function $u(x,y) = \\dfrac{\\sinh(\\pi y)}{\\sinh(\\pi)} \\sin(\\pi x)$ on $\\partial\\Omega$. This yields $u(x,0) = 0$, $u(0,y) = 0$, $u(1,y) = 0$, and $u(x,1) = \\sin(\\pi x)$, and it satisfies $\\Delta u = 0$ in $\\Omega$.\n- Initialization: Set all interior values to $0$ and enforce boundary values from the function above; keep boundary values fixed for all iterations.\n- Jacobi iteration: At each iteration $k \\mapsto k+1$, update interior points by $u_{i,j}^{(k+1)} = \\dfrac{1}{4}\\left(u_{i-1,j}^{(k)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k)} + u_{i,j+1}^{(k)}\\right)$, where any neighbor on the boundary uses the fixed boundary value.\n- Residuals and norms: At each iteration $k$, form the interior residual array with components $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - \\left(u_{i-1,j}^{(k)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k)} + u_{i,j+1}^{(k)}\\right)$. Compute the relative residual sequences $\\rho_{2}^{(k)} = \\lVert r^{(k)} \\rVert_{2}/\\lVert r^{(0)} \\rVert_{2}$ and $\\rho_{\\infty}^{(k)} = \\lVert r^{(k)} \\rVert_{\\infty}/\\lVert r^{(0)} \\rVert_{\\infty}$.\n- Stopping indices for comparison: For a given tolerance $\\varepsilon$ and maximum iteration count $M$, define $k_{2}$ to be the smallest nonnegative integer $k \\le M$ such that $\\rho_{2}^{(k)} \\le \\varepsilon$ (or set $k_{2} = M$ if the condition does not occur by $M$), and define $k_{\\infty}$ analogously using $\\rho_{\\infty}^{(k)}$. For comparison, also compute $\\alpha = k_{\\infty}/k_{2}$ as a floating-point value; if $k_{2} = 0$, define $\\alpha$ in your program in any consistent way that avoids division by zero.\n- Numerical units: There are no physical units in this problem. All angles in trigonometric functions are in radians.\n\nTest suite to implement and run inside your program without any user input:\n\n- Case A (boundary-value-driven, coarse grid, stringent tolerance): $N = 8$, $\\varepsilon = 10^{-8}$, $M = 20000$.\n- Case B (moderate grid): $N = 16$, $\\varepsilon = 10^{-6}$, $M = 20000$.\n- Case C (finer grid): $N = 24$, $\\varepsilon = 10^{-6}$, $M = 20000$.\n- Case D (even finer grid): $N = 32$, $\\varepsilon = 10^{-6}$, $M = 20000$.\n\nYour program must produce a single line of output containing the aggregated results for all four cases as a comma-separated list enclosed in square brackets, in the order $[k_{2}^{A},k_{\\infty}^{A},\\alpha^{A},k_{2}^{B},k_{\\infty}^{B},\\alpha^{B},k_{2}^{C},k_{\\infty}^{C},\\alpha^{C},k_{2}^{D},k_{\\infty}^{D},\\alpha^{D}]$, where the superscripts label the test cases A, B, C, and D. Each $k_{2}$ and $k_{\\infty}$ must be an integer, and each $\\alpha$ must be a floating-point number. No other text should be printed.",
            "solution": "The problem presented is a standard exercise in computational science, requiring the implementation and comparison of convergence criteria for an iterative linear solver applied to a discretized partial differential equation. It is scientifically sound, well-posed, and contains all necessary information for its resolution. I will proceed with a complete solution.\n\nThe core of the problem lies in solving the two-dimensional Laplace equation, $\\Delta u = 0$, on a unit square domain $\\Omega = [0,1] \\times [0,1]$. This is a fundamental model for steady-state phenomena such as heat conduction, electrostatics, and potential flow.\n\nFirst, we must translate the continuous problem into a discrete algebraic system. This is achieved through finite differencing on a uniform grid. The domain is discretized into a grid of $N \\times N$ points $(x_i, y_j)$, where $x_i = i \\cdot h$ and $y_j = j \\cdot h$ for $i, j \\in \\{0, 1, \\dots, N-1\\}$. The grid spacing is uniform, given by $h = 1/(N-1)$. The value of the function at a grid point is denoted $u_{i,j} \\approx u(x_i, y_j)$.\n\nThe Laplace operator $\\Delta = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is approximated using the second-order central difference formula for the second derivatives:\n$$ \\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{(x_i,y_j)} \\approx \\frac{u_{i-1,j} - 2u_{i,j} + u_{i+1,j}}{h^2} $$\n$$ \\frac{\\partial^2 u}{\\partial y^2}\\bigg|_{(x_i,y_j)} \\approx \\frac{u_{i,j-1} - 2u_{i,j} + u_{i,j+1}}{h^2} $$\nSubstituting these into the Laplace equation $\\Delta u = 0$ and multiplying by $h^2$ yields the five-point stencil approximation for each interior grid point, where $i,j \\in \\{1, 2, \\dots, N-2\\}$:\n$$ u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - 4u_{i,j} = 0 $$\nThis set of linear equations for all interior points forms a large, sparse linear system of the form $A\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector of the unknown values $u_{i,j}$ at interior points, the matrix $A$ represents the discrete Laplacian, and the vector $\\mathbf{b}$ incorporates the values from the fixed Dirichlet boundary conditions.\n\nThe problem specifies the Jacobi method for solving this system. This iterative method updates the value at each point using only the values from the previous iteration. By rearranging the discrete Laplace equation to solve for $u_{i,j}$, we obtain the Jacobi update rule:\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i-1,j}^{(k)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k)} + u_{i,j+1}^{(k)} \\right) $$\nwhere the superscript $(k)$ denotes the iteration number. The initial state of the system at $k=0$ is specified with $u_{i,j}^{(0)} = 0$ for all interior points. The boundary values are fixed for all iterations according to the harmonic function $u(x,y) = \\frac{\\sinh(\\pi y)}{\\sinh(\\pi)} \\sin(\\pi x)$. This function provides the Dirichlet data on the boundary $\\partial\\Omega$:\n- $u(x,0) = 0$\n- $u(x,1) = \\sin(\\pi x)$\n- $u(0,y) = 0$\n- $u(1,y) = 0$\nThese values are set on the boundary nodes of the discrete grid, i.e., for $i=0, i=N-1, j=0, j=N-1$.\n\nTo monitor convergence, we compute the residual vector $r^{(k)}$ at each iteration $k$. The problem defines the components of the residual at interior points $(i,j)$ as:\n$$ r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - \\left(u_{i-1,j}^{(k)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k)} + u_{i,j+1}^{(k)}\\right) $$\nThis quantity measures how well the solution at iteration $k$, $u^{(k)}$, satisfies the discrete Laplace equation. An exact discrete solution would have a zero residual. Note that from the Jacobi update rule, we can see that $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - 4u_{i,j}^{(k+1)} = -4(u_{i,j}^{(k+1)} - u_{i,j}^{(k)})$.\n\nThe magnitude of the residual vector is measured using two different vector norms: the Euclidean ($\\ell_2$) norm and the maximum ($\\ell_\\infty$) norm. For a residual vector $r$ with components $r_\\ell$ over all $(N-2)^2$ interior points, these are:\n$$ \\lVert r \\rVert_2 = \\sqrt{\\sum_{\\ell} r_{\\ell}^2} $$\n$$ \\lVert r \\rVert_\\infty = \\max_{\\ell} |r_{\\ell}| $$\nConvergence is assessed using the relative residuals, $\\rho_2^{(k)} = \\lVert r^{(k)} \\rVert_2 / \\lVert r^{(0)} \\rVert_2$ and $\\rho_\\infty^{(k)} = \\lVert r^{(k)} \\rVert_\\infty / \\lVert r^{(0)} \\rVert_\\infty$, where $r^{(0)}$ is the residual of the initial state. The iteration is considered to have converged when these values drop below a specified tolerance $\\varepsilon$. We are tasked to find the first iteration indices, $k_2$ and $k_\\infty$, at which this occurs for each norm, up to a maximum of $M$ iterations.\n\nThe algorithm proceeds as follows:\n1.  Initialize an $N \\times N$ grid. Set interior points to $0$ and boundary points according to the given function.\n2.  Compute the initial residual $r^{(0)}$ using the initial grid configuration $u^{(0)}$. Calculate its norms, $\\lVert r^{(0)} \\rVert_2$ and $\\lVert r^{(0)} \\rVert_\\infty$. These are reference values.\n3.  Begin the iteration loop for $k=1, 2, \\dots, M$.\n4.  In each iteration $k$, create a copy of the current grid, $u^{(k-1)}$.\n5.  Calculate the new grid $u^{(k)}$ by applying the Jacobi update rule to all interior points, using the values from the copied grid $u^{(k-1)}$.\n6.  Using the newly computed $u^{(k)}$, calculate the residual vector $r^{(k)}$.\n7.  Compute the norms $\\lVert r^{(k)} \\rVert_2$ and $\\lVert r^{(k)} \\rVert_\\infty$, and then the relative residuals $\\rho_2^{(k)}$ and $\\rho_\\infty^{(k)}$.\n8.  Check for convergence:\n    - If $\\rho_2^{(k)} \\le \\varepsilon$ and $k_2$ has not yet been determined, set $k_2=k$.\n    - If $\\rho_\\infty^{(k)} \\le \\varepsilon$ and $k_\\infty$ has not yet been determined, set $k_\\infty=k$.\n9.  If both $k_2$ and $k_\\infty$ have been found, the loop can be terminated early.\n10. If the loop completes up to $M$ iterations, any `k` value not yet determined is set to $M$.\n11. Finally, compute the ratio $\\alpha = k_\\infty / k_2$. A special case for $k_2=0$ is noted, but it will not occur here since $\\rho^{(0)}=1$ and $\\varepsilon  1$.\n\nThis procedure is implemented for each test case to find the values $(k_2, k_\\infty, \\alpha)$. The comparison of $k_2$ and $k_\\infty$ reveals how the choice of norm affects the measured speed of convergence for the Jacobi method on this problem. Since all norms on a finite-dimensional vector space are equivalent, the rates of convergence will be asymptotically identical. However, for a finite number of steps, the specific iteration counts may differ slightly depending on the spatial distribution of the residual throughout the convergence process.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(N, epsilon, M):\n    \"\"\"\n    Solves the discretized Laplace equation using Jacobi iteration and tracks convergence.\n\n    Args:\n        N (int): Number of points in each dimension of the grid (NxN).\n        epsilon (float): The convergence tolerance for relative residuals.\n        M (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing (k2, k_inf, alpha) where:\n               k2 (int): Iteration count to converge in L2 norm.\n               k_inf (int): Iteration count to converge in L-infinity norm.\n               alpha (float): The ratio k_inf / k2.\n    \"\"\"\n    # 1. Grid and Boundary/Initial Conditions Setup\n    h = 1.0 / (N - 1)\n    x = np.linspace(0, 1, N)\n    \n    # Initialize grid u with zeros\n    u = np.zeros((N, N), dtype=float)\n\n    # Apply boundary conditions\n    # u(x,0) = 0, u(0,y) = 0, u(1,y) = 0 are already set by zero initialization.\n    # u(x,1) = sin(pi*x)\n    u[-1, :] = np.sin(np.pi * x)\n\n    # 2. Initial Residual Calculation (k=0)\n    # The residual is calculated only on interior points.\n    u_interior = u[1:-1, 1:-1]\n    # Neighbors involved in residual calculation can be on the boundary\n    residual_k = 4.0 * u_interior - (u[0:-2, 1:-1] + u[2:, 1:-1] + u[1:-1, 0:-2] + u[1:-1, 2:])\n    \n    norm_r0_2 = np.linalg.norm(residual_k)\n    norm_r0_inf = np.max(np.abs(residual_k))\n\n    # Handle the unlikely case of a zero initial residual\n    if norm_r0_2 == 0 or norm_r0_inf == 0:\n        # If the initial state is the exact solution, convergence is immediate.\n        return 0, 0, 1.0\n\n    k_2 = -1\n    k_inf = -1\n\n    # 3. Jacobi Iteration Loop\n    for k in range(1, M + 1):\n        # Create a copy of the grid from the previous state to perform the update\n        u_old = u.copy()\n        \n        # Update all interior points simultaneously using values from u_old\n        u[1:-1, 1:-1] = 0.25 * (u_old[0:-2, 1:-1] + u_old[2:, 1:-1] + \n                               u_old[1:-1, 0:-2] + u_old[1:-1, 2:])\n\n        # Calculate residual for the current iteration k on interior points\n        # residual_k is r^(k) based on u^(k)\n        u_interior = u[1:-1, 1:-1]\n        residual_k = 4.0 * u_interior - (u[0:-2, 1:-1] + u[2:, 1:-1] + u[1:-1, 0:-2] + u[1:-1, 2:])\n\n        # Compute norms and relative residuals\n        norm_rk_2 = np.linalg.norm(residual_k)\n        norm_rk_inf = np.max(np.abs(residual_k))\n        \n        rho_2_k = norm_rk_2 / norm_r0_2\n        rho_inf_k = norm_rk_inf / norm_r0_inf\n\n        # 4. Check for convergence\n        if rho_2_k = epsilon and k_2 == -1:\n            k_2 = k\n        \n        if rho_inf_k = epsilon and k_inf == -1:\n            k_inf = k\n\n        # Terminate early if both convergence criteria are met\n        if k_2 != -1 and k_inf != -1:\n            break\n\n    # If convergence was not met, set iteration count to M\n    if k_2 == -1:\n        k_2 = M\n    if k_inf == -1:\n        k_inf = M\n\n    # Calculate alpha, handling the k_2 = 0 case\n    if k_2 == 0:\n        alpha = 1.0 if k_inf == 0 else np.inf\n    else:\n        alpha = float(k_inf) / float(k_2)\n\n    return k_2, k_inf, alpha\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, epsilon, M)\n        (8, 1e-8, 20000),   # Case A\n        (16, 1e-6, 20000),  # Case B\n        (24, 1e-6, 20000),  # Case C\n        (32, 1e-6, 20000),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        N, epsilon, M = case\n        k2, k_inf, alpha = run_simulation(N, epsilon, M)\n        results.extend([k2, k_inf, alpha])\n\n    # Final print statement in the exact required format.\n    # [k2A,kinfA,alphaA,k2B,kinfB,alphaB,...]\n    # Format alpha as float, k values as int.\n    formatted_results = []\n    for i in range(len(results)):\n        if (i + 1) % 3 == 0: # alpha values\n            formatted_results.append(f\"{results[i]}\")\n        else: # k values\n            formatted_results.append(str(int(results[i])))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice tackles one of the most critical issues in computational science: numerical stability. A linear system might be mathematically well-defined but computationally fragile if the governing matrix is ill-conditioned. This numerical experiment has you demonstrate this phenomenon by showing how a tiny perturbation in the input vector $b$ of a system $Ax=b$ can cause a massive error in the solution $x$. By working with famously ill-conditioned Hilbert matrices, you will gain a crucial, hands-on understanding of the matrix condition number—a concept rooted in matrix norms—and its profound implications for the reliability of computational solutions .",
            "id": "2449583",
            "problem": "Design and implement a complete, runnable program that performs a numerical experiment demonstrating that for an ill-conditioned matrix $A$, a small relative error in $b$ can lead to a large relative error in the solution $x$ of the linear system $A x = b$. The experiment must be based strictly on first principles: norms, relative errors, and the definition of the linear solve. All quantities must use the vector and matrix $2$-norm. The program must compute, for each specified test case, the amplification factor defined as\n$$\nr \\;=\\; \\frac{\\lVert x_{\\epsilon} - x^\\star \\rVert_{2} / \\lVert x^\\star \\rVert_{2}}{\\lVert \\delta b \\rVert_{2} / \\lVert b \\rVert_{2}},\n$$\nwhere $x^\\star$ is the exact solution corresponding to the unperturbed right-hand side $b$, $\\delta b$ is a perturbation of $b$, and $x_{\\epsilon}$ is the solution of $A x = b + \\delta b$. The program must use the following experiment setup for every test case:\n- Let $x^\\star$ be the vector in $\\mathbb{R}^n$ with all components equal to $1$.\n- Let $b = A x^\\star$.\n- Let the perturbation direction $v \\in \\mathbb{R}^n$ be defined componentwise by $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$ for $i = 1, \\dots, n$ so that $\\lVert v \\rVert_2 = 1$.\n- Let $\\delta b = \\epsilon \\, \\lVert b \\rVert_2 \\, v$, where $\\epsilon$ is the prescribed relative perturbation magnitude for the test.\n- Let $x_{\\epsilon}$ be the solution of $A x = b + \\delta b$.\n\nYour program must compute, for each test case, the amplification factor $r$ as a floating-point number. The set of test cases is as follows, each identified by its case number and executed in the listed order:\n- Case $1$: $A$ is the Hilbert matrix $H \\in \\mathbb{R}^{5 \\times 5}$ with entries $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1,\\dots,5\\}$, and $\\epsilon = 10^{-8}$.\n- Case $2$: $A$ is the Hilbert matrix $H \\in \\mathbb{R}^{10 \\times 10}$ with entries $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1,\\dots,10\\}$, and $\\epsilon = 10^{-8}$.\n- Case $3$: $A$ is the identity matrix $I \\in \\mathbb{R}^{8 \\times 8}$, and $\\epsilon = 10^{-8}$.\n- Case $4$: $A \\in \\mathbb{R}^{2 \\times 2}$ is given by\n$$\nA = \\begin{bmatrix}\n1  1 \\\\\n1  1 + 10^{-10}\n\\end{bmatrix},\n$$\nand $\\epsilon = 10^{-12}$.\n\nAll norms must be the $2$-norm. There are no physical units involved. Angles are not used. Percentages must not be used; all ratios and magnitudes must be expressed as decimal floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[r1,r2,r3,r4]\"). Each entry must be the amplification factor $r$ for the corresponding case, rounded to $6$ significant digits, in the order of cases $1$ through $4$.",
            "solution": "The experiment is designed from first principles of linear algebra and norm-based error analysis. We consider a linear system $A x = b$ with exact data and its perturbed counterpart $A x = b + \\delta b$. For a given matrix $A \\in \\mathbb{R}^{n \\times n}$ and an exact solution vector $x^\\star \\in \\mathbb{R}^n$, we define $b = A x^\\star$. We then construct a perturbation $\\delta b$ with prescribed relative magnitude $\\epsilon$ as follows. Let $v \\in \\mathbb{R}^n$ with entries $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$, for $i = 1, \\dots, n$. This $v$ satisfies $\\lVert v \\rVert_2 = 1$ by construction:\n$$\n\\lVert v \\rVert_2^2 = \\sum_{i=1}^n \\left(\\frac{1}{\\sqrt{n}}\\right)^2 = \\frac{n}{n} = 1.\n$$\nWe set $\\delta b = \\epsilon \\, \\lVert b \\rVert_2 \\, v$. Then the relative perturbation in $b$ is exactly $\\frac{\\lVert \\delta b \\rVert_2}{\\lVert b \\rVert_2} = \\epsilon$ because\n$$\n\\lVert \\delta b \\rVert_2 = \\epsilon \\, \\lVert b \\rVert_2 \\, \\lVert v \\rVert_2 = \\epsilon \\, \\lVert b \\rVert_2.\n$$\nLet $x_\\epsilon$ denote the solution to the perturbed system $A x = b + \\delta b$. The error in the solution equals\n$$\nx_\\epsilon - x^\\star = A^{-1}\\,(b+\\delta b) - A^{-1} b = A^{-1}\\,\\delta b.\n$$\nTherefore the relative error in $x$ is\n$$\n\\frac{\\lVert x_\\epsilon - x^\\star \\rVert_2}{\\lVert x^\\star \\rVert_2} = \\frac{\\lVert A^{-1}\\,\\delta b \\rVert_2}{\\lVert x^\\star \\rVert_2}.\n$$\nThe amplification factor $r$ that the program reports for each case is\n$$\nr = \\frac{\\lVert x_{\\epsilon} - x^\\star \\rVert_{2} / \\lVert x^\\star \\rVert_{2}}{\\lVert \\delta b \\rVert_{2} / \\lVert b \\rVert_{2}}.\n$$\nFrom norm properties and the definition of the matrix $2$-norm, we can relate this amplification to the condition number in the matrix $2$-norm, $\\kappa_2(A) = \\lVert A \\rVert_2 \\, \\lVert A^{-1} \\rVert_2$. Specifically, using $\\lVert A^{-1} \\delta b \\rVert_2 \\le \\lVert A^{-1} \\rVert_2 \\, \\lVert \\delta b \\rVert_2$ and $\\lVert b \\rVert_2 = \\lVert A x^\\star \\rVert_2 \\le \\lVert A \\rVert_2 \\, \\lVert x^\\star \\rVert_2$, we obtain\n$$\n\\frac{\\lVert x_\\epsilon - x^\\star \\rVert_2}{\\lVert x^\\star \\rVert_2} \\le \\lVert A^{-1} \\rVert_2 \\, \\lVert \\delta b \\rVert_2 \\,\\frac{1}{\\lVert x^\\star \\rVert_2}\n\\le \\lVert A^{-1} \\rVert_2 \\, \\lVert A \\rVert_2 \\, \\frac{\\lVert \\delta b \\rVert_2}{\\lVert b \\rVert_2} = \\kappa_2(A) \\, \\frac{\\lVert \\delta b \\rVert_2}{\\lVert b \\rVert_2}.\n$$\nHence\n$$\nr \\le \\kappa_2(A).\n$$\nThis inequality shows that the relative error in the solution can be amplified by up to approximately the condition number. For well-conditioned matrices such as the identity matrix $I$, we have $\\kappa_2(I) = 1$, and thus $r$ should be close to $1$. For ill-conditioned matrices such as Hilbert matrices, $\\kappa_2(A)$ is very large, and even a very small $\\epsilon$ can result in a large relative error in $x$, producing a large $r$.\n\nThe test suite covers several regimes:\n- Case $1$ uses a Hilbert matrix of size $5$, which is ill-conditioned but moderate in size.\n- Case $2$ uses a Hilbert matrix of size $10$, which is more ill-conditioned, typically yielding a much larger amplification $r$.\n- Case $3$ uses the identity matrix of size $8$, which is perfectly conditioned, so $r$ should be approximately $1$.\n- Case $4$ uses a nearly singular $2 \\times 2$ matrix with entries differing by $10^{-10}$ on one element, producing a very large amplification.\n\nFor each case, the program constructs $x^\\star$, $b$, the unit-norm perturbation direction $v$, the perturbation $\\delta b$ with the specified $\\epsilon$, solves for $x_\\epsilon$, computes the relative errors, and reports $r$ rounded to $6$ significant digits. The final output is a single line containing the list $[r_1, r_2, r_3, r_4]$ in that order. This procedure directly and transparently exhibits how ill-conditioning in $A$ magnifies small relative perturbations in $b$ into large relative errors in the solution $x$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hilbert(n: int) - np.ndarray:\n    # H[i,j] = 1 / (i + j + 1) with zero-based i,j; but use one-based formula directly\n    i = np.arange(1, n + 1).reshape(-1, 1)\n    j = np.arange(1, n + 1).reshape(1, -1)\n    return 1.0 / (i + j - 1.0)\n\ndef alternating_unit_vector(n: int) - np.ndarray:\n    # v_i = (-1)^(i-1) / sqrt(n), i = 1..n\n    signs = (-1.0) ** np.arange(n)\n    v = signs / np.sqrt(n)\n    # Ensure unit norm numerically\n    return v / np.linalg.norm(v, 2)\n\ndef amplification_factor(A: np.ndarray, eps: float) - float:\n    n = A.shape[0]\n    x_star = np.ones(n, dtype=float)\n    b = A @ x_star\n    nb = np.linalg.norm(b, 2)\n    if nb == 0.0:\n        # Degenerate, but not expected with provided tests; return NaN-like large value\n        return float('nan')\n    v = alternating_unit_vector(n)\n    delta_b = eps * nb * v\n    b_tilde = b + delta_b\n    # Solve for perturbed solution\n    x_tilde = np.linalg.solve(A, b_tilde)\n    # Relative errors\n    rel_b = np.linalg.norm(delta_b, 2) / nb\n    rel_x = np.linalg.norm(x_tilde - x_star, 2) / np.linalg.norm(x_star, 2)\n    # Amplification factor\n    return rel_x / rel_b\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (A_matrix, epsilon)\n    A1 = hilbert(5)\n    eps1 = 1e-8\n\n    A2 = hilbert(10)\n    eps2 = 1e-8\n\n    A3 = np.eye(8, dtype=float)\n    eps3 = 1e-8\n\n    A4 = np.array([[1.0, 1.0],\n                   [1.0, 1.0 + 1e-10]], dtype=float)\n    eps4 = 1e-12\n\n    test_cases = [\n        (A1, eps1),\n        (A2, eps2),\n        (A3, eps3),\n        (A4, eps4),\n    ]\n\n    results = []\n    for A, eps in test_cases:\n        r = amplification_factor(A, eps)\n        # Round to 6 significant digits\n        if np.isnan(r) or np.isinf(r):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{r:.6g}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}