{
    "hands_on_practices": [
        {
            "introduction": "The Minimum Description Length (MDL) principle is rooted in the idea that the best model for a set of data is the one that leads to the greatest overall compression. This exercise provides a foundational test of this concept by comparing two distinct strategies for encoding a short text string. By weighing a \"free\" but general-purpose model against a \"costly\" but perfectly tailored custom model , you will gain a concrete understanding of how model complexity and data fit are balanced in the universal currency of bits.",
            "id": "1641426",
            "problem": "An information theorist is analyzing compression strategies for short text strings using the Minimum Description Length (MDL) principle. The core idea of MDL is to find the model that permits the shortest total description of the data, which is the sum of the model's description length and the data's description length when encoded using that model.\n\nThe specific data string to be encoded is $S = \\text{\"AAAAABBCCC\"}$.\n\nTwo competing models are being considered:\n\n1.  **A Universal Model ($M_U$)**: This is a pre-existing, fixed model for a 4-character alphabet {A, B, C, D}, which is assumed to be known by both the sender and receiver. Therefore, its description length is considered to be zero. The probabilities for the characters under this model are:\n    $P_U(A) = 0.4$\n    $P_U(B) = 0.3$\n    $P_U(C) = 0.2$\n    $P_U(D) = 0.1$\n    The total description length for this strategy is just the length of the string $S$ encoded using the ideal code based on model $M_U$.\n\n2.  **A Custom Model ($M_C$)**: This model is built specifically for the string $S$. It uses the empirical frequencies of the characters within $S$ as their probabilities. The alphabet for this custom model consists only of the characters present in $S$. To use this model, the sender must first transmit the model's description to the receiver.\n    - **Model Description Length $L(M_C)$**: The model is described by transmitting the counts of each unique character in $S$. For a string of total length $N$ with $K$ unique characters, the cost of transmitting these $K$ counts is defined to be $L(M_C) = K \\log_2(N+1)$.\n    - **Data Description Length $L(D|M_C)$**: This is the length of the string $S$ encoded using the ideal code based on the probabilities from the custom model $M_C$.\n\nThe length of a string encoded with an ideal code based on a model is given by the sum of the negative base-2 logarithms of the probabilities of its characters. All description lengths are measured in bits.\n\nYour task is to determine which model provides a shorter total description length for the string $S$. Calculate the minimum possible total description length.\n\nExpress your answer in bits, rounded to four significant figures.",
            "solution": "We are to compute total description lengths for two models and select the minimum. The MDL total length is $L_{\\text{total}} = L(M) + L(D \\mid M)$, where $L(D \\mid M) = -\\sum \\log_{2} P_{M}(\\text{symbol})$ over the symbols of the string, counting multiplicity.\n\nFor the given string $S=\\text{\"AAAAABBCCC\"}$, the counts are $n_{A}=5$, $n_{B}=2$, $n_{C}=3$, $n_{D}=0$, with total length $N=10$ and number of unique characters $K=3$.\n\nUniversal model $M_{U}$: The model cost is zero, so the total is the data codelength\n$$\nL_{\\text{U}} = 5\\big(-\\log_{2}(0.4)\\big) + 2\\big(-\\log_{2}(0.3)\\big) + 3\\big(-\\log_{2}(0.2)\\big).\n$$\nUsing $-\\log_{2}(0.4)=\\log_{2}(5/2)\\approx 1.321928095$, $-\\log_{2}(0.3)\\approx 1.736965594$, and $-\\log_{2}(0.2)=\\log_{2}(5)\\approx 2.321928095$, we obtain\n$$\nL_{\\text{U}} \\approx 5(1.321928095) + 2(1.736965594) + 3(2.321928095) \\approx 17.049355948\\ \\text{bits}.\n$$\n\nCustom model $M_{C}$: The model cost is\n$$\nL(M_{C}) = K \\log_{2}(N+1) = 3\\log_{2}(11) \\approx 10.378294856.\n$$\nThe empirical probabilities are $P_{C}(A)=\\frac{5}{10}=0.5$, $P_{C}(B)=\\frac{2}{10}=0.2$, $P_{C}(C)=\\frac{3}{10}=0.3$. The data codelength is\n$$\nL(D\\mid M_{C}) = 5\\big(-\\log_{2}(0.5)\\big) + 2\\big(-\\log_{2}(0.2)\\big) + 3\\big(-\\log_{2}(0.3)\\big).\n$$\nUsing $-\\log_{2}(0.5)=1$, $-\\log_{2}(0.2)=\\log_{2}(5)\\approx 2.321928095$, and $-\\log_{2}(0.3)\\approx 1.736965594$, we get\n$$\nL(D\\mid M_{C}) \\approx 5(1) + 2(2.321928095) + 3(1.736965594) \\approx 14.854752972.\n$$\nThus the total for the custom model is\n$$\nL_{\\text{C}} = L(M_{C}) + L(D\\mid M_{C}) \\approx 10.378294856 + 14.854752972 \\approx 25.233047828\\ \\text{bits}.\n$$\n\nComparing, $L_{\\text{U}} \\approx 17.049355948$ bits and $L_{\\text{C}} \\approx 25.233047828$ bits, so the universal model yields the shorter total description length. Rounding the minimum to four significant figures gives $17.05$ bits.",
            "answer": "$$\\boxed{17.05}$$"
        },
        {
            "introduction": "Building on the fundamental MDL trade-off, we can apply this principle to solve practical problems in data analysis, such as identifying outliers. This practice challenges you to formalize the question: is it more efficient to describe a dataset with a single, simple model that is strained by an unusual data point, or to pay an explicit cost to isolate and describe that point separately? This exercise  demonstrates how MDL provides a quantitative criterion for making discrete structural decisions about data, offering a formal alternative to ad hoc heuristics.",
            "id": "1641424",
            "problem": "An analyst is evaluating a small dataset of measurements, $D = \\{2.1, 2.9, 3.1, 3.3, 3.5, 10.0\\}$, using the Minimum Description Length (MDL) principle. The goal is to determine which of two competing hypotheses provides a more concise explanation for the data. The total description length (DL) for a hypothesis is the sum of the cost to describe the model and the cost to describe the data given the model, i.e., $DL(\\text{Hypothesis}) = DL(\\text{Model}) + DL(\\text{Data} | \\text{Model})$. All description lengths are to be measured in nats.\n\n**Hypothesis H1:** All $N=6$ data points are drawn from a single Gaussian distribution $N(\\mu, \\sigma^2)$.\n\n**Hypothesis H2:** One of the $N=6$ data points is an outlier that is described explicitly, while the remaining $N-1=5$ \"inlier\" points are drawn from a Gaussian distribution $N(\\mu', \\sigma'^2)$. For this hypothesis, it is assumed that the outlier is the data point $10.0$.\n\nThe cost functions are defined as follows:\n\n1.  **Model Cost for a Gaussian Distribution:** The cost to encode the parameters of a Gaussian model fitted to $n$ data points is given by $DL(\\text{Gaussian Params}) = (k/2) \\ln(n)$, where $k=2$ is the number of parameters (μ and σ). For all Gaussian models, use the Maximum Likelihood Estimates (MLE) for the parameters, where the mean $\\mu$ is the sample mean and the variance $\\sigma^2$ is the sample variance $\\frac{1}{n} \\sum(x_i - \\mu)^2$ calculated from the $n$ points being modeled.\n\n2.  **Model Cost for the Outlier in H2:** The cost of specifying the model for H2 has two parts:\n    a. The cost of identifying which of the $N$ points is a designated outlier, which is $\\ln(N)$.\n    b. The cost of explicitly encoding the numerical value of that outlier, which is a constant $C = 8.0$ nats.\n\n3.  **Data Cost Given a Gaussian Model:** The cost to encode $n$ data points given a Gaussian model with parameters $\\mu$ and $\\sigma$ is the negative log-likelihood:\n    $DL(\\text{Data} | \\text{Gaussian}) = n \\ln(\\sigma) + \\frac{n}{2} \\ln(2\\pi) + \\frac{1}{2\\sigma^2} \\sum(x_i - \\mu)^2$.\n\nFor your calculations, use the following approximations: $\\ln(2\\pi) \\approx 1.838$, $\\ln(5) \\approx 1.609$, and $\\ln(6) \\approx 1.792$.\n\nCalculate the difference in description length, $DL(H1) - DL(H2)$. A positive result signifies that Hypothesis H2 offers a more compact description. Round your final answer to three significant figures.",
            "solution": "We follow the MDL principle with\n$$\nDL(\\text{Hypothesis})=DL(\\text{Model})+DL(\\text{Data}\\mid \\text{Model}).\n$$\nThe model cost for a Gaussian with $n$ points is $DL(\\text{Gaussian Params})=\\frac{k}{2}\\ln(n)$ with $k=2$, hence $DL=\\ln(n)$. The data cost given a Gaussian $N(\\mu,\\sigma^{2})$ is\n$$\nDL(\\text{Data}\\mid \\text{Gaussian})=n\\ln(\\sigma)+\\frac{n}{2}\\ln(2\\pi)+\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}.\n$$\nAt the MLE, $\\mu$ is the sample mean and $\\sigma^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}$. Denote $SS=\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}$. Then $\\sigma^{2}=SS/n$ and\n$$\nDL(\\text{Data}\\mid \\text{Gaussian MLE})=n\\ln(\\sigma)+\\frac{n}{2}\\ln(2\\pi)+\\frac{n}{2}\n=\\frac{n}{2}\\big[\\ln(2\\pi e)+\\ln(SS/n)\\big].\n$$\nWe will use $\\ln(2\\pi)\\approx 1.838$, so $\\ln(2\\pi e)=\\ln(2\\pi)+1\\approx 2.838$, and $\\ln(5)\\approx 1.609$, $\\ln(6)\\approx 1.792$.\n\nHypothesis H1: all $N=6$ points from one Gaussian. The data are $D=\\{2.1,2.9,3.1,3.3,3.5,10.0\\}$, so $n=6$, $\\mu_{1}=\\frac{2.1+2.9+3.1+3.3+3.5+10.0}{6}=\\frac{24.9}{6}=4.15$. The squared deviations are\n$$\n(2.1-4.15)^{2}=4.2025,\\quad (2.9-4.15)^{2}=1.5625,\\quad (3.1-4.15)^{2}=1.1025,\n$$\n$$\n(3.3-4.15)^{2}=0.7225,\\quad (3.5-4.15)^{2}=0.4225,\\quad (10.0-4.15)^{2}=34.2225,\n$$\nso\n$$\nSS_{1}=4.2025+1.5625+1.1025+0.7225+0.4225+34.2225=42.235,\\quad \\frac{SS_{1}}{n}=\\frac{42.235}{6}=7.039166\\ldots\n$$\nThus\n$$\nDL(\\text{Data}\\mid H1)=\\frac{6}{2}\\big[\\,\\ln(2\\pi e)+\\ln(SS_{1}/6)\\,\\big]\n=3\\big[\\,2.838+\\ln(7.039166\\ldots)\\,\\big].\n$$\nWith $\\ln(7.039166\\ldots)\\approx 1.95149$, this gives\n$$\nDL(\\text{Data}\\mid H1)\\approx 3\\,(2.838+1.95149)=14.3685.\n$$\nThe model cost is $DL(\\text{Model}\\mid H1)=\\ln(6)=1.792$. Therefore\n$$\nDL(H1)=1.792+14.3685=16.1605.\n$$\n\nHypothesis H2: one outlier (given as $10.0$) is encoded explicitly; the remaining $n=5$ inliers are Gaussian. The inliers are $\\{2.1,2.9,3.1,3.3,3.5\\}$, so\n$$\n\\mu_{2}=\\frac{2.1+2.9+3.1+3.3+3.5}{5}=\\frac{14.9}{5}=2.98,\n$$\nand their squared deviations are\n$$\n(2.1-2.98)^{2}=0.7744,\\quad (2.9-2.98)^{2}=0.0064,\\quad (3.1-2.98)^{2}=0.0144,\n$$\n$$\n(3.3-2.98)^{2}=0.1024,\\quad (3.5-2.98)^{2}=0.2704,\n$$\nso\n$$\nSS_{2}=0.7744+0.0064+0.0144+0.1024+0.2704=1.168,\\quad \\frac{SS_{2}}{5}=0.2336.\n$$\nHence\n$$\nDL(\\text{Data}\\mid H2)=\\frac{5}{2}\\big[\\,\\ln(2\\pi e)+\\ln(SS_{2}/5)\\,\\big]\n=\\frac{5}{2}\\big[\\,2.838+\\ln(0.2336)\\,\\big].\n$$\nWith $\\ln(0.2336)\\approx -1.45414$, we obtain\n$$\nDL(\\text{Data}\\mid H2)\\approx \\frac{5}{2}\\,(2.838-1.45414)=3.45965.\n$$\nThe model cost for H2 includes Gaussian parameters for the $5$ inliers, identification of the outlier among $6$ points, and the explicit encoding cost $C=8.0$:\n$$\nDL(\\text{Model}\\mid H2)=\\ln(5)+\\ln(6)+C=1.609+1.792+8.0=11.401.\n$$\nTherefore\n$$\nDL(H2)=11.401+3.45965=14.86065.\n$$\n\nThe requested difference is\n$$\nDL(H1)-DL(H2)=16.1605-14.86065=1.29985\\approx 1.30\\ \\text{nats},\n$$\nwhere we have rounded to three significant figures as required. The positive value indicates that H2 provides the more compact description.",
            "answer": "$$\\boxed{1.30}$$"
        },
        {
            "introduction": "The MDL principle can be expressed in several ways, and many practical applications use an approximation closely related to the Bayesian Information Criterion (BIC). This problem introduces this common formulation, which balances a model's goodness-of-fit—measured by its maximized log-likelihood—against a penalty for complexity based on its number of free parameters. By applying this criterion to decide if a change in the rate of observed events is statistically significant , you will become familiar with a powerful and widely used tool for model selection in science.",
            "id": "1641400",
            "problem": "An astrophysicist is monitoring a distant pulsar for high-energy particle emissions. Over a one-hour observation window, a detector records the timestamps of these emissions. A key question is whether the pulsar's emission rate is constant or if it changed during the observation.\n\nThe data is summarized as follows:\n- In the first 30 minutes, $k_1 = 10$ events were recorded.\n- In the second 30 minutes, $k_2 = 20$ events were recorded.\n\nTo decide between two competing hypotheses, the astrophysicist uses the Minimum Description Length (MDL) principle. The total description length, or codelength, $L$ of a model $M$ given the data $D$ is calculated in nats (using the natural logarithm) as:\n$$L(M) = -\\ln P(D|\\hat{\\theta}) + \\frac{d}{2}\\ln N$$\nwhere:\n- $\\ln P(D|\\hat{\\theta})$ is the maximized log-probability of the data given the model. For a model describing the count of events from a Poisson process, where $k$ events are observed, this term is given by $\\ln P(D|\\hat{\\theta}) = k \\ln k - k - \\ln(k!)$.\n- $d$ is the number of free parameters in the model.\n- $N$ is the total number of events observed across the entire observation window.\n\nThe two models under consideration are:\n- **Model $M_1$**: A single Poisson process with a constant rate over the entire one-hour period. This model has one free parameter (the rate).\n- **Model $M_2$**: Two independent Poisson processes, one for the first 30-minute interval and one for the second. This model has two free parameters (one rate for each interval). For this model, the total maximized log-probability is the sum of the log-probabilities from each interval.\n\nCalculate the difference in codelengths, $\\Delta L = L(M_1) - L(M_2)$. The model with the smaller codelength is preferred. Round your final answer to three significant figures.",
            "solution": "Let $k_{1}=10$, $k_{2}=20$, and the total count be $N=k_{1}+k_{2}=30$. For a Poisson count model, the maximized log-likelihood given $k$ is $\\ln P(D|\\hat{\\theta})=k\\ln k - k - \\ln(k!)$, as provided.\n\nFor model $M_{1}$ (one Poisson rate over the whole hour, $d=1$), the codelength is\n$$\nL(M_{1})=-\\ln P(D|\\hat{\\theta})+\\frac{d}{2}\\ln N\n= -\\big(N\\ln N - N - \\ln(N!)\\big) + \\frac{1}{2}\\ln N\n= -N\\ln N + N + \\ln(N!) + \\frac{1}{2}\\ln N.\n$$\n\nFor model $M_{2}$ (two independent Poisson rates for the two halves, $d=2$), the total maximized log-likelihood is the sum over the intervals, so\n$$\nL(M_{2})=-\\left[(k_{1}\\ln k_{1} - k_{1} - \\ln(k_{1}!))+(k_{2}\\ln k_{2} - k_{2} - \\ln(k_{2}!))\\right] + \\ln N\n= -k_{1}\\ln k_{1} + k_{1} + \\ln(k_{1}!) - k_{2}\\ln k_{2} + k_{2} + \\ln(k_{2}!) + \\ln N.\n$$\n\nThe difference in codelengths is\n$$\n\\Delta L=L(M_{1})-L(M_{2})\n= \\left[-N\\ln N + N + \\ln(N!) + \\frac{1}{2}\\ln N\\right]\n-\\left[-k_{1}\\ln k_{1} + k_{1} + \\ln(k_{1}!) - k_{2}\\ln k_{2} + k_{2} + \\ln(k_{2}!) + \\ln N\\right].\n$$\nUsing $N=k_{1}+k_{2}$, the linear terms in $k$ cancel, yielding\n$$\n\\Delta L=\\left(k_{1}\\ln k_{1} + k_{2}\\ln k_{2} - N\\ln N\\right) - \\frac{1}{2}\\ln N + \\left[\\ln(N!) - \\ln(k_{1}!) - \\ln(k_{2}!)\\right]\n= \\left(k_{1}\\ln k_{1} + k_{2}\\ln k_{2} - N\\ln N\\right) - \\frac{1}{2}\\ln N + \\ln\\!\\binom{N}{k_{1}}.\n$$\n\nSubstitute $k_{1}=10$, $k_{2}=20$, $N=30$:\n- Compute the logarithmic terms:\n$$\n10\\ln 10 = 23.02585093,\\quad 20\\ln 20 = 59.91464547107982,\\quad 30\\ln 30 = 102.03592144986466,\n$$\nso\n$$\nk_{1}\\ln k_{1} + k_{2}\\ln k_{2} - N\\ln N = 23.02585093 + 59.91464547107982 - 102.03592144986466 = -19.09542504878484.\n$$\n- Compute the penalty term:\n$$\n-\\frac{1}{2}\\ln N = -\\frac{1}{2}\\ln 30 = -1.7005986908310777.\n$$\n- Compute the combinatorial term via factorial logs:\n$$\n\\ln(10!)=15.104412573075516,\\quad \\ln(20!)=42.335616460753485,\\quad \\ln(30!)=74.65823634883016,\n$$\nso\n$$\n\\ln\\!\\binom{30}{10}=\\ln(30!) - \\ln(20!) - \\ln(10!)=74.65823634883016 - 42.335616460753485 - 15.104412573075516 = 17.21820731500116.\n$$\n\nCombine all parts:\n$$\n\\Delta L = \\left(-19.09542504878484\\right) + \\left(-1.7005986908310777\\right) + 17.21820731500116 = -3.577816424614758.\n$$\nRounding to three significant figures gives $-3.58$. Since $\\Delta L<0$, model $M_{1}$ has the smaller codelength and is preferred.",
            "answer": "$$\\boxed{-3.58}$$"
        }
    ]
}