## 引言
在处理序列数据（如文本或基因序列）时，如何精确预测下一个元素是一个核心挑战。虽然利用符号前的“上下文”是关键，但简单的固定长度上下文模型难以在捕捉特定长模式和应对[数据稀疏性](@entry_id:136465)之间取得平衡。部分匹配预测（Prediction by Partial Matching, PPM）算法正是在这一背景下应运而生的一种强大自适应[统计模型](@entry_id:165873)。它通过动态地在不同长度的上下文之间切换，巧妙地解决了上述难题，成为[无损数据压缩](@entry_id:266417)和序列建模领域的基石之一。

本文将分步深入剖析[PPM算法](@entry_id:272096)。在“原理与机制”一章中，我们将揭示其分层上下文模型和核心的“逃逸”机制。接着，“应用与跨学科联系”一章将展示PPM如何从数据压缩工具扩展到[计算生物学](@entry_id:146988)、认知科学等多个领域。最后，通过“动手实践”部分，您将有机会将理论应用于具体问题，从而巩固所学知识。

## 原理与机制

在信息论与[数据压缩](@entry_id:137700)领域，预测部分匹配（Prediction by Partial Matching, PPM）算法是一类极其重要和高效的自适应[统计建模](@entry_id:272466)技术。与简单的固定阶马尔可夫模型不同，PPM不依赖于固定的上下文长度，而是动态地、自适应地选择最合适的上下文信息来预测序列中的下一个符号。这种灵活性使其能够有效[平衡模型](@entry_id:636099)的特异性（使用长上下文捕捉特定模式）和泛化能力（使用短上下文应对[数据稀疏性](@entry_id:136465)），从而在通用数据压缩任务中取得了巨大成功。本章将深入探讨[PPM算法](@entry_id:272096)的核心原理与工作机制。

### 核心思想：上下文的力量与稀疏性的挑战

在对序列（如文本、[基因序列](@entry_id:191077)等）进行建模时，一个核心的直觉是：下一个符号的出现概率极大地依赖于它之前的几个符号，即其**上下文（context）**。一个强大的预测模型能够利用这种依赖关系来降低对下一个符号的不确定性。

我们可以通过一个思想实验来理解上下文的重要性。假设我们正在对一篇大型英文语料库进行建模，并需要预测两个不同二元上下文（bigram）之后的字符。第一个上下文是 `th`，第二个是 `zx`。对于 `th`，我们根据对英语的经验可以很有信心地预测，其后最可能出现的字符是 `e`（如在 "the" 中），其次可能是 `a`、`i`、`o`、`u` 等。这个预测的[概率分布](@entry_id:146404)是高度不均匀、非常“尖锐”的。相反，对于上下文 `zx`，它在英语中极为罕见，我们几乎无法确定下一个字符会是什么。因此，对 `zx` 之后字符的预测[概率分布](@entry_id:146404)会趋向于[均匀分布](@entry_id:194597)，即充满了高度的不确定性。在信息论中，这种不确定性可以用**[条件熵](@entry_id:136761)（conditional entropy）**来量化。一个尖锐、不均匀的[概率分布](@entry_id:146404)对应着较低的[条件熵](@entry_id:136761)，意味着预测更加确定；而一个平坦、均匀的[分布](@entry_id:182848)对应着较高的[条件熵](@entry_id:136761)，意味着预测的不确定性很高 。PPM模型的目标正是利用高频、信息丰富的上下文（如 `th`）来做出低熵的、精确的预测，从而实现高效的压缩。

然而，试图直接利用长上下文会立即面临一个严峻的统计挑战，即**[数据稀疏性](@entry_id:136465)（data sparsity）**，这也被称为“[维度灾难](@entry_id:143920)”的一种表现。随着上下文长度 $k$ 的增加，可能的上下文数量呈指数级增长。对于任何有限的训练数据，绝大多数可能的长上下文要么从未出现过，要么只出现过一两次。

为了具体说明这一点，我们来分析一个简短的序列 `S = BANANABANDANA`。我们考虑长度为 $k=3$ 的上下文。通过在序列上滑动一个长度为3的窗口，我们可以得到所有出现过的三元上下文。例如，从第一个字符开始是 `BAN`，第二个字符开始是 `ANA`，以此类推。整个序列中总共包含10个三元上下文的实例。对它们进行统计，我们得到如下的唯一上下文及其出现频率：
- `ANA`: 2 次
- `BAN`: 2 次
- `NAN`: 1 次
- `NAB`: 1 次
- `ABA`: 1 次
- `AND`: 1 次
- `NDA`: 1 次
- `DAN`: 1 次

在这个简短的例子中，总共有8个不同的（唯一的）三元上下文。其中，有6个（`NAN`, `NAB`, `ABA`, `AND`, `NDA`, `DAN`）在整个序列中仅仅出现了一次。这意味着，在所有观察到的独特模式中，有 $6/8 = 0.75$ 的模式都是**偶发同形（hapax legomena）** 。如果我们的模型只依赖于这些出现过一次的上下文，那么其[统计估计](@entry_id:270031)将极不可靠。更糟糕的是，许多其他可能的三元上下文（例如 `BAM` 或 `BAD`）根本没有出现，模型将无法为它们分配任何概率。PPM的核心机制正是为了解决这一根本性难题而设计的。

### PPM机制：层级化上下文与逃逸机制

PPM的精髓在于其“部分匹配”的思想。当一个长而具体的上下文无法提供足够的统计信息时，算法会“后退”一步，尝试一个更短、更通用的上下文。这个过程形成了一个从高阶到低阶的层级结构。

#### 上下文层级

PPM的预测过程始于一个预设的**最大上下文阶数（maximum context order）**，记为 $k_{max}$。为了预测当前位置的符号，算法首先尝试使用最长的可用上下文，即前 $k_{max}$ 个符号。

1.  **尝试最高阶上下文**：首先检查阶数为 $k = k_{max}$ 的上下文。
2.  **检查与匹配**：在已经处理过的历史数据中，查找这个上下文是否出现过。
3.  **失败则降阶**：如果该上下文从未出现过，或者虽然出现过但我们想预测的符号从未跟在其后，那么模型就无法在当前阶数下做出有效预测。此时，它会“逃逸”到下一个较低的阶数，即 $k-1$。
4.  **重复过程**：模型重复检查和匹配的过程，不断降低上下文的阶数（$k-1, k-2, \dots$），直到找到一个能够成功预测当前符号的上下文，或者所有上下文都尝试失败。

我们通过一个简单的例子来阐明这个流程。假设我们有一个最大阶数 $k_{max}=2$ 的PPM模型，正在处理字符串 `roses are red`。为了预测第五个字符（第二个 `s`），模型可用的历史数据是 `rose`。预测过程如下 ：
1.  **检查阶数-2**：上下文是 `se`。在历史 `rose` 中，`se` 仅作为结尾出现，从未被观察到其后跟随任何字符。因此，模型无法基于此上下文预测 `s`。它必须**逃逸**。
2.  **检查阶数-1**：上下文是 `e`。在历史 `rose` 中，`e` 也仅在末尾出现，同样没有后续字符的记录。模型再次**逃逸**。
3.  **检查阶数-0**：阶数-0的上下文是空上下文，通常表示为 $\lambda$。检查这个上下文意味着考察目标符号 `s` 是否在整个历史 `rose` 中出现过。答案是肯定的（在第三个位置）。因此，在阶数-0上，模型找到了预测 `s` 的依据，查找过程终止。

这个从长到短的上下文查找序列（$se \rightarrow e \rightarrow \lambda$）正是[PPM算法](@entry_id:272096)的核心骨架。

#### 逃逸机制 (Escape Mechanism)

“逃逸”不仅是一个流程控制的步骤，更是一个核心的[概率建模](@entry_id:168598)机制。当模型从一个高阶上下文逃逸时，它实际上是在声明：“我没有在当前这个具体的上下文里找到足够的信息来预测这个符号，因此我将把一部分概率分配给一个‘逃逸事件’，然后把问题交给更通用的低阶模型来处理。”

逃逸的触发条件主要有两个：
1.  **上下文本身是新颖的**：如果一个上下文在历史数据中从未出现过，那么模型关于它没有任何统计知识，必须立即逃逸。例如，一个最大阶数 $k=4$ 的PPM模型在处理序列 `ABCDE` 时，当它试图预测第五个符号 `E` 时，它首先查看阶数-4的上下文 `ABCD`。由于这是 `ABCD` 这个子串在整个处理过程中首次出现，模型没有任何关于其后续符号的先验记录。因此，模型别无选择，只能立即从阶数-4逃逸到阶数-3（上下文 `BCD`）。
2.  **符号在上下文中是新颖的**：即使上下文本身是已知的（即在历史中出现过），但待预测的符号从未在该上下文之后出现过。这也将触发一次逃逸。

#### 完整的后备层级

PPM的上下文层级结构构成了一个鲁棒的后备系统，确保任何符号都能被赋予一个非零概率。这个层级通常包括：
- **高阶上下文模型（Order $k > 0$）**：利用前 $k$ 个符号进行预测。
- **阶数-0 模型（Order 0）**：当所有更高阶的上下文都失败时，模型会退回到一个不考虑任何上下文的阶数-0模型。该模型基于整个历史数据中各个符号的全局频率来进行预测。
- **阶数-(-1) 模型（Order -1）**：这是最后的防线。如果一个符号甚至在阶数-0模型中都无法找到（即该符号在整个历史数据中从未出现过），模型就会逃逸到阶数-(-1)模型。这个模型通常假设所有**从未见过**的符号在该语言的字母表中是等概率的。如果字母表 $\mathcal{A}$ 已知，在没有任何观测数据之前，模型就处于这个初始状态，为字母表中的任何一个符号赋予概率 $1/|\mathcal{A}|$ 。

### 概率估计与自适应更新

现在我们来讨论PPM如何具体地计算概率。PPM模型是**自适应**的，这意味着它的统计数据（符号计数）会在每处理一个符号后动态更新。

#### 概率计算方法

有多种为PPM模型计算符号概率和[逃逸概率](@entry_id:266710)的方法，其中一种经典且直观的是**方法C (Method C)**。对于一个给定的上下文 $c$，我们维护以下统计量：
- $N(c)$：上下文 $c$ 在历史数据中被观察到的总次数。
- $T(c)$：在上下文 $c$ 之后出现过的**不同**符号类型的数量（unique symbols）。
- $N(s|c)$：特定符号 $s$ 在上下文 $c$ 之后出现的次数。

使用方法C，概率计算规则如下：
- **已见符号的概率**：如果符号 $s$ 在上下文 $c$ 后出现过（即 $N(s|c) > 0$），其条件概率为：
  $$ P(s|c) = \frac{N(s|c)}{N(c) + T(c)} $$
- **[逃逸概率](@entry_id:266710)**：如果符号 $s$ 在上下文 $c$ 后是新颖的（即 $N(s|c) = 0$），模型必须逃逸。逃逸事件的概率为：
  $$ P_{\text{esc}}(c) = \frac{T(c)}{N(c) + T(c)} $$

这个公式的直觉在于，分母 $N(c) + T(c)$ 可以看作是对总计数的平滑处理。总的概率质量被分配给了已见的 $T(c)$ 个符号和代表所有未见符号的逃逸事件。上下文后出现过的符号种类越多（$T(c)$ 越大），模型就越倾向于认为还可能有其他未见的符号出现，因此[逃逸概率](@entry_id:266710)也越高。

#### 最终概率的合成

当一个符号的预测需要经历一次或多次逃逸时，其最终概率是所有逃逸路径上的[逃逸概率](@entry_id:266710)与最终成功匹配的那个低阶模型所给出的符号概率的**乘积**。
例如，如果预测符号 $s$ 需要从阶数 $k$ 逃逸到 $k-1$，再从 $k-1$ 逃逸到 $k-2$，最终在阶数 $k-2$ 的上下文 $c_{k-2}$ 中找到，那么它的总概率为：
$$ P(s|\text{history}) = P_{\text{esc}}(c_k) \times P_{\text{esc}}(c_{k-1}) \times P(s|c_{k-2}) $$

#### 综合示例 1：单符号概率计算

让我们通过一个完整的例子来演算这个过程。给定字母表 $\mathcal{A} = \{\text{A, B, C}\}$，最大上下文阶数 $k_{max}=2$，历史序列为 $S_{obs} = \text{CAABACAB}$。我们来计算下一个符号是 `B` 的概率 。这里的概率公式稍有不同，但原理相通：$P(s|c) = \frac{\text{count}_k(s)}{\text{total}_k + d_k}$ 和 $P_{\text{esc}}(c) = \frac{d_k}{\text{total}_k + d_k}$，其中 $\text{total}_k$ 是上下文出现次数，$d_k$ 是不同后继符号数。

1.  **阶数-2 (k=2)**：当前上下文是历史序列的最后两个符号，即 $c_2 = \text{AB}$。
    - 我们在 $S_{obs}$（不包括结尾）中寻找 `AB`。它出现在位置(3,4)，其后是 `A`。
    - 统计量：出现总次数 $\text{total}_2 = 1$，后继符号集合为 $\{\text{A}\}$，不同后继符号数 $d_2 = 1$。
    - 目标符号是 `B`，它在后继符号集合中从未出现，即 $\text{count}_2(\text{B}) = 0$。因此，必须逃逸。
    - 阶数-2的[逃逸概率](@entry_id:266710)为：$P_{\text{esc}}(c_2) = \frac{d_2}{\text{total}_2 + d_2} = \frac{1}{1+1} = \frac{1}{2}$。

2.  **阶数-1 (k=1)**：上下文缩短为 $c_1 = \text{B}$。
    - 我们在 $S_{obs}$（不包括结尾）中寻找 `B`。它出现在位置4，其后是 `A`。
    - 统计量：$\text{total}_1 = 1$，后继符号集合为 $\{\text{A}\}$，$d_1 = 1$。
    - 目标符号 `B` 仍未在后继符号中出现，$\text{count}_1(\text{B}) = 0$。再次逃逸。
    - 阶数-1的[逃逸概率](@entry_id:266710)为：$P_{\text{esc}}(c_1) = \frac{d_1}{\text{total}_1 + d_1} = \frac{1}{1+1} = \frac{1}{2}$。

3.  **阶数-0 (k=0)**：上下文为空 $\lambda$。我们考察整个历史序列 $S_{obs}$。
    - 序列总长度 $\text{total}_0 = 8$。序列中包含的符号类型为 $\{\text{A, B, C}\}$，所以 $d_0 = 3$。
    - 目标符号 `B` 在序列中出现了2次，即 $\text{count}_0(\text{B}) = 2$。
    - 由于 $\text{count}_0(\text{B}) > 0$，我们终于在这里找到了匹配。阶数-0模型给出的 `B` 的概率是：$\frac{\text{count}_0(\text{B})}{\text{total}_0 + d_0} = \frac{2}{8+3} = \frac{2}{11}$。

4.  **最终概率**：将所有路径上的概率相乘。
    $$ P(\text{B} | S_{obs}) = P_{\text{esc}}(c_2) \times P_{\text{esc}}(c_1) \times P(\text{B} | \lambda) = \frac{1}{2} \times \frac{1}{2} \times \frac{2}{11} = \frac{1}{22} $$

#### 综合示例 2：自适应更新与序列概率

PPM的强大之处在于其**自适应**性。每当一个新符号被处理（编码或解码）后，它就会被加入到历史序列中，模型内部的所有上下文统计表随之更新。下面的例子展示了如何从一个空模型开始，逐步构建并计算一个完整序列的概率 。

假设字母表为 $\mathcal{A}=\{a, b\}$，$k_{max}=2$，使用方法C。我们要计算序列 "aba" 的概率 $P(\text{"aba"}) = P(\text{'a'}) \times P(\text{'b'}|\text{'a'}) \times P(\text{'a'}|\text{"ab"})$。

1.  **计算 $P(\text{'a'})$**：
    - 初始模型为空，历史为空。所有上下文（包括阶数-0）的计数都为0。
    - 从最高阶（这里是阶数-0，因为没有历史）开始，计数为0，必须逃逸到阶数-(-1)。
    - 阶数-(-1)模型给出均匀概率 $1/|\mathcal{A}| = 1/2$。
    - $P(\text{'a'}) = 1 \times \frac{1}{2} = \frac{1}{2}$。
    - **更新模型**：处理完 'a' 后，阶数-0的统计变为：$N(\lambda)=1$, $T(\lambda)=1$, $N(\text{'a'}|\lambda)=1$。

2.  **计算 $P(\text{'b'}|\text{'a'})$**：
    - 当前历史为 "a"。最高上下文是阶数-1的 "a"。
    - 对上下文 "a"，其统计表为空（从未见过 "a" 后面跟任何符号），必须逃逸。
    - 退到阶数-0。此时阶数-0的统计为 $N(\lambda)=1, T(\lambda)=1$。符号 'b' 对于此上下文是新颖的，再次逃逸。
    - 阶数-0的[逃逸概率](@entry_id:266710)为 $P_{\text{esc}}(\lambda) = \frac{T(\lambda)}{N(\lambda)+T(\lambda)} = \frac{1}{1+1} = \frac{1}{2}$。
    - 退到阶数-(-1)，给出概率 $1/2$。
    - $P(\text{'b'}|\text{'a'}) = P_{\text{esc}}(\text{'a'}) \times P_{\text{esc}}(\lambda) \times P(\text{'b'}|\text{order -1}) = 1 \times \frac{1}{2} \times \frac{1}{2} = \frac{1}{4}$。
    - **更新模型**：处理完 'b' 后，上下文 "a" 的统计变为 $N(\text{'a'})=1, T(\text{'a'})=1, N(\text{'b'}|\text{'a'})=1$。阶数-0的统计变为 $N(\lambda)=2, T(\lambda)=2$（因为 'a' 和 'b' 都见过了）。

3.  **计算 $P(\text{'a'}|\text{"ab"})$**：
    - 当前历史为 "ab"。最高上下文是阶数-2的 "ab"。其统计表为空，逃逸。
    - 退到阶数-1，上下文 "b"。其统计表也为空，逃逸。
    - 退到阶数-0。此时 $N(\lambda)=2, T(\lambda)=2$，且 $N(\text{'a'}|\lambda)=1$。符号 'a' 不再是新颖的。
    - 直接计算概率：$P(\text{'a'}|\lambda) = \frac{N(\text{'a'}|\lambda)}{N(\lambda)+T(\lambda)} = \frac{1}{2+2} = \frac{1}{4}$。
    - $P(\text{'a'}|\text{"ab"}) = 1 \times 1 \times \frac{1}{4} = \frac{1}{4}$。

4.  **最终序列概率**：
    $$ P(\text{"aba"}) = \frac{1}{2} \times \frac{1}{4} \times \frac{1}{4} = \frac{1}{32} $$

#### 综合示例 3：完整的逃逸链

当遇到一个在整个历史数据中都从未出现过的符号时，PPM模型将展示其完整的逃逸链，一直回退到阶数-(-1)模型。

假设模型基于训练文本 `BOOKKEEPER` ($k_{max}=3$)，字母表为26个大写字母。我们要计算在上下文 `KEE` 之后出现一个全新符号 `S` 的概率 。

- **阶数-3 (KEE)**: 在`BOOKKEEPER`中，`KEE` 出现一次，后面跟着 `P`。因此，$N_3=1, d_3=1$。符号 `S` 未出现，必须逃逸。$P_{\text{esc},3} = \frac{1}{1+1} = \frac{1}{2}$。
- **阶数-2 (EE)**: `EE` 出现一次，后面跟着 `P`。$N_2=1, d_2=1$。`S` 未出现，逃逸。$P_{\text{esc},2} = \frac{1}{1+1} = \frac{1}{2}$。
- **阶数-1 (E)**: `E` 出现三次，其后分别是 `E`, `P`, `R`。$N_1=3, d_1=3$。`S` 未出现，逃逸。$P_{\text{esc},1} = \frac{3}{3+3} = \frac{1}{2}$。
- **阶数-0 ($\lambda$)**: 除去首字母，文本的其余部分是 `OOKKEEPER`。$N_0=9$，不同符号有 `O,K,E,P,R` 共 $d_0=5$ 种。`S` 仍未出现，逃逸。$P_{\text{esc},0} = \frac{5}{9+5} = \frac{5}{14}$。
- **阶数-(-1)**: `S` 在整个训练文本 `BOOKKEEPER` 中都未出现。训练文本中出现的独特符号有 `B,O,K,E,P,R` 共6个。因此，字母表中还有 $26 - 6 = 20$ 个未见符号。阶数-(-1)模型在这些未见符号上[均匀分布](@entry_id:194597)概率。所以 $P(\text{S}|\text{order -1}) = \frac{1}{20}$。

最终概率为所有[逃逸概率](@entry_id:266710)与最终阶数-(-1)概率的乘积：
$$ P(\text{S}|\text{KEE}) = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} \times \frac{5}{14} \times \frac{1}{20} = \frac{1}{448} $$

### 与数据压缩的联系

PPM本身是一个概率预测模型。它在[数据压缩](@entry_id:137700)中的应用通常是与**[算术编码](@entry_id:270078)器（arithmetic coder）**结合使用。[算术编码](@entry_id:270078)器是一种最优的[熵编码](@entry_id:276455)方法，它能够将一个符号序列编码成一个小数，该小数所需的比特数几乎精确地等于该序列的**信息内容（information content）**。一个概率为 $p$ 的事件的信息内容定义为 $-\log_2(p)$ 比特。

因此，PPM模型每预测一个符号的概率 $p_i$，[算术编码](@entry_id:270078)器就使用这个概率来更新编码。编码整个序列所需的总比特数约等于每个符号信息内容的总和：
$$ \text{Total Bits} \approx \sum_i -\log_2 P(s_i | s_1 \dots s_{i-1}) = -\log_2 \prod_i P(s_i | s_1 \dots s_{i-1}) $$
注意，当预测中发生逃逸时，逃逸事件本身也贡献了信息内容。例如，如果 $P(s) = P_{\text{esc}} \times P(s|\text{lower order})$，那么编码它所需的比特数是 $(-\log_2 P_{\text{esc}}) + (-\log_2 P(s|\text{lower order}))$。

让我们看一个计算总比特数的例子。一个在线PPM编码器（最大阶数 $K=2$）已处理历史 `H_initial = CABBACCAB`，现在要[编码序列](@entry_id:204828) `BACA` 。
1.  **编码 'B'**：历史 `CABBACCAB`，上下文 `AB`。`AB` 后跟过 `B` 一次。$P(\text{B}|\text{AB}) = \frac{1}{1+1} = \frac{1}{2}$。
2.  **编码 'A'**：新历史 `CABBACCABB`，上下文 `BB`。`BB` 后跟过 `A` 一次。$P(\text{A}|\text{BB}) = \frac{1}{1+1} = \frac{1}{2}$。
3.  **编码 'C'**：新历史 `CABBACCABBA`，上下文 `BA`。`BA` 后跟过 `C` 一次。$P(\text{C}|\text{BA}) = \frac{1}{1+1} = \frac{1}{2}$。
4.  **编码 'A'**：新历史 `CABBACCABBAC`，上下文 `AC`。历史上 `AC` 历史上只跟过 `C`。所以对 `A` 必须逃逸。$P_{\text{esc}}(\text{AC}) = \frac{1}{1+1} = \frac{1}{2}$。降阶到上下文 `C`。历史上 `C` 后跟过 `A` 两次，跟过 `C` 一次。所以 $N(\text{C})=3, T(\text{C})=2$，$N(\text{A}|\text{C})=2$。$P(\text{A}|\text{C})=\frac{2}{3+2}=\frac{2}{5}$。因此编码 `A` 的总概率是 $P_{\text{esc}}(\text{AC}) \times P(\text{A}|\text{C}) = \frac{1}{2} \times \frac{2}{5} = \frac{1}{5}$。

编码整个序列 `BACA` 的总概率是所有这些概率事件的乘积：
$$ P(\text{BACA}) = \left(\frac{1}{2}\right) \times \left(\frac{1}{2}\right) \times \left(\frac{1}{2}\right) \times \left(\frac{1}{2} \times \frac{2}{5}\right) = \frac{1}{40} $$
所需的总比特数为：
$$ -\log_2\left(\frac{1}{40}\right) = \log_2(40) $$

### 改进与变种：排除原则

基础的PPM模型虽然强大，但存在一个潜在的效率问题：同一个符号可能在多个上下文层级中都被分配了概率质量。例如，在预测 `the` 中的 `e` 时，`e` 可能在阶数-2（`th`）、阶数-1（`h`）和阶数-0（全局）模型中都有非零计数。当模型从高阶逃逸到低阶时，它可能会为已经可以在高阶编码的符号再次分配概率，这造成了概率质量的“重复计算”和浪费。

为了解决这个问题，一些PPM的变种引入了**排除原则（Exclusion Principle）**。其核心思想是：当从一个高阶上下文逃逸后，在低阶上下文中计算概率时，应当**排除**所有那些本可以在更高阶上下文中被编码的符号。

例如，考虑一个PPM-C模型（使用方法C并带有排除原则）在处理序列 `MISSISSIPPI` 时，预测上下文 `IS` 之后的符号 `P` 。
1.  **阶数-2 (IS)**: 在 `MISSISSIPPI` 中，`IS` 出现了两次，每次后面都跟着 `S`。所以，在阶数-2模型中，唯一可能被直接编码的符号是 `S`。由于我们要预测 `P`，模型必须逃逸。[逃逸概率](@entry_id:266710) $P_{\text{esc},2} = \frac{1}{2+1} = \frac{1}{3}$。
2.  **阶数-1 (S) with Exclusion**: 现在我们来到阶数-1，上下文为 `S`。根据排除原则，我们必须排除掉在阶数-2中已经可以处理的符号，即 `S`。原始情况下，`S` 后面跟过 `S`（两次）和 `I`（两次）。排除 `S` 之后，在阶数-1的上下文 `S` 中，我们只考虑 `I` 的计数。此时，更新后的统计量为 $n'_1=2, c'_1=1$（只剩下 `I` 这一种符号）。目标符号 `P` 仍然是新颖的，必须再次逃逸。新的[逃逸概率](@entry_id:266710)为 $P_{\text{esc},1} = \frac{1}{2+1} = \frac{1}{3}$。
3.  **阶数-0 ($\lambda$) with Exclusion**: 我们来到阶数-0。此时，需要被排除的符号集合是所有在更高阶（阶数-2和阶数-1）中出现过的后继符号，即 `{S, I}`。全局符号计数为 `M:1, I:4, S:4, P:2`。排除 `I` 和 `S` 后，只剩下 `M` 和 `P`，其计数分别为1和2。在剩下的符号中，`P` 的相对频率为 $\frac{2}{1+2} = \frac{2}{3}$。

最终，`P` 的概率是：
$$ P(\text{P}|\text{IS}) = P_{\text{esc},2} \times P_{\text{esc},1} \times P_0(\text{P}|\text{after exclusion}) = \frac{1}{3} \times \frac{1}{3} \times \frac{2}{3} = \frac{2}{27} $$

通过排除原则，PPM模型能够更有效地分配概率质量，通常能带来更好的压缩性能。它是PPM系列算法不断演化和完善过程中的一个重要里程碑。