{
    "hands_on_practices": [
        {
            "introduction": "The power of the Han-Kobayashi scheme lies in its sophisticated structure, which uses auxiliary random variables to represent different parts of a message. This exercise  tests your conceptual understanding of this structure by exploring a specific simplification: removing one user's private message component. By analyzing the consequences of setting an auxiliary variable to a constant, you will gain insight into the distinct roles of common and private information in managing interference.",
            "id": "1628827",
            "problem": "Consider a two-user discrete memoryless interference channel (DM-IC) defined by the transition probability $p(y_1, y_2|x_1, x_2)$, where $X_1 \\in \\mathcal{X}_1$ and $X_2 \\in \\mathcal{X}_2$ are the channel inputs, and $Y_1 \\in \\mathcal{Y}_1$ and $Y_2 \\in \\mathcal{Y}_2$ are the channel outputs. The Han-Kobayashi (HK) scheme provides an achievable rate region for this channel by splitting each user's message into a common and a private part. The problem below considers a common variant of the HK scheme where the codebooks are constructed using a single common auxiliary variable ($U_0$) and two private auxiliary variables ($U_1, U_2$).\n\nThe random coding argument for this scheme relies on three auxiliary random variables, $U_0, U_1, U_2$, with a joint probability distribution $p(u_0, u_1, u_2)$. The channel inputs $X_1$ and $X_2$ are then generated according to $p(x_1|u_0, u_1)$ and $p(x_2|u_0, u_2)$, respectively. This structure implies that $U_1$ is associated with User 1's private message, $U_2$ with User 2's private message, and $U_0$ with a message component common to both users.\n\nThe general HK achievable rate region is the convex hull of the union of all rate pairs $(R_1, R_2)$ satisfying the following inequalities for some choice of the joint distribution $p(u_0, u_1, u_2)p(x_1|u_0, u_1)p(x_2|u_0, u_2)$:\n\n1. $R_1 \\le I(X_1; Y_1 | U_0, U_2)$\n2. $R_2 \\le I(X_2; Y_2 | U_0, U_1)$\n3. $R_1 + R_2 \\le I(X_1, X_2; Y_1|U_0) + I(X_2; Y_2 | X_1, U_0, U_1)$\n4. $R_1 + R_2 \\le I(X_1, X_2; Y_2|U_0) + I(X_1; Y_1 | X_2, U_0, U_2)$\n\nNow, consider a specialized coding strategy where User 1 does not send a private message. This is formally equivalent to setting the auxiliary random variable $U_1$ to be a constant. Which of the following statements provides the most accurate and complete description of the consequences of this choice?\n\nA. The achievable rate region remains unchanged because the choice of the distribution $p(u_0, u_1, u_2)$ is arbitrary and can compensate for $U_1$ being a constant.\n\nB. The scheme reduces to treating interference as noise for both users, resulting in the rate bounds $R_1 \\le I(X_1; Y_1)$ and $R_2 \\le I(X_2; Y_2)$.\n\nC. This choice forces the rate for user 1 to be zero ($R_1 = 0$), as no private message can be sent. The only achievable rates are for user 2.\n\nD. The scheme becomes one where User 1 transmits a common-only message, while User 2 transmits a message with both common and private parts. The rate bound for User 2 simplifies to $R_2 \\le I(X_2; Y_2 | U_0)$.\n\nE. The joint distribution of the inputs simplifies to $p(x_1, x_2) = p(x_1)p(x_2)$, meaning the inputs become independent and the problem reduces to two parallel point-to-point channels.",
            "solution": "The problem asks to analyze the effect of setting the auxiliary random variable $U_1$ to a constant within the Han-Kobayashi (HK) coding scheme for the two-user interference channel. Let's analyze the implications of this constraint and then evaluate each of the given options.\n\n**Analysis of the Constraint**\n\nThe auxiliary random variable $U_1$ is used in the random coding argument to generate the codebook for the private part of User 1's message. Specifically, for each private message $w_{1p}$, a codeword $u_1^n(w_{1p})$ is drawn from an i.i.d. distribution governed by $p(u_1)$. The channel input $X_1$ is then generated based on both this private component and a common component $U_0$, according to $p(x_1|u_0, u_1)$.\n\nIf we set $U_1$ to be a constant, say $U_1 = c$, this has two main consequences:\n\n1.  **Conceptual Consequence:** Since $U_1$ is constant, it cannot carry any information. The codebook for User 1's private message, which is indexed by different values of $U_1$, effectively collapses. There is only one \"message\" that can be sent through this private channel part, meaning the rate of the private message for User 1 must be zero. However, User 1 can still send information through the common message component, which is associated with the random variable $U_0$. Therefore, User 1's total rate $R_1$ is not necessarily zero, but consists only of the common message rate. The coding scheme becomes one where User 1 sends a common-only message, while User 2, which still uses both $U_0$ and $U_2$, can send a message with both common and private components.\n\n2.  **Probabilistic Consequence:** Setting $U_1$ to a constant means its probability mass function is a Dirac delta. Consequently, $H(U_1)=0$ and $U_1$ is independent of all other random variables. The joint probability distribution that defines the coding scheme, $p(u_0, u_1, u_2)p(x_1|u_0, u_1)p(x_2|u_0, u_2)$, simplifies. The term $p(x_1|u_0, u_1)$ becomes $p(x_1|u_0)$. The overall distribution is now chosen from the family $p(u_0, u_2)p(x_1|u_0)p(x_2|u_0, u_2)$.\n\nWe use these consequences to evaluate the given options.\n\n**Evaluation of Options**\n\n*   **A. The achievable rate region remains unchanged...**\n    This is incorrect. The general HK region is an optimization over all possible joint distributions $p(u_0, u_1, u_2)p(x_1|u_0, u_1)p(x_2|u_0, u_2)$. By setting $U_1$ to a constant, we are restricting this optimization to a smaller set of distributions. This results in an achievable rate region that is a subset of, and generally smaller than, the full HK region.\n\n*   **B. The scheme reduces to treating interference as noise for both users...**\n    This is incorrect. The \"treating interference as noise\" (TIN) scheme typically corresponds to a specific choice of auxiliary variables, e.g., $U_0 = \\emptyset, U_1 = X_1, U_2 = X_2$, leading to rate bounds like $R_1 \\le I(X_1; Y_1|X_2)$ and $R_2 \\le I(X_2; Y_2|X_1)$. The simplification in the problem is different. Furthermore, the bounds $R_1 \\le I(X_1; Y_1)$ and $R_2 \\le I(X_2; Y_2)$ correspond to ignoring the interference entirely, which is generally not achievable.\n\n*   **C. This choice forces the rate for user 1 to be zero ($R_1 = 0$)...**\n    This is incorrect. As discussed, only the private part of User 1's message has a zero rate. User 1 can still transmit information via the common auxiliary variable $U_0$. The first inequality in the HK bounds, $R_1 \\le I(X_1; Y_1 | U_0, U_2)$, can still be positive, allowing for a non-zero rate $R_1$ composed solely of a common message.\n\n*   **D. The scheme becomes one where User 1 transmits a common-only message...**\n    This is the correct statement. Let's break it down:\n    -   \"The scheme becomes one where User 1 transmits a common-only message, while User 2 transmits a message with both common and private parts.\" This is the correct conceptual interpretation, as explained in our initial analysis. User 1 loses its private message capability (linked to $U_1$), while User 2 retains both common ($U_0$) and private ($U_2$) capabilities.\n    -   \"The rate bound for User 2 simplifies to $R_2 \\le I(X_2; Y_2 | U_0)$.\" We check this by simplifying the second HK inequality: $R_2 \\le I(X_2; Y_2 | U_0, U_1)$. Since $U_1$ is a constant, it is independent of all other variables. A fundamental property of mutual information is that conditioning on a constant does not change its value: $I(A; B | C, K) = I(A; B | C)$ if $K$ is a constant. Thus, $I(X_2; Y_2 | U_0, U_1) = I(X_2; Y_2 | U_0)$. This mathematical simplification is correct.\n    Both parts of the statement are accurate, providing a complete description of the situation.\n\n*   **E. The joint distribution of the inputs simplifies to $p(x_1, x_2) = p(x_1)p(x_2)$...**\n    This is incorrect. The inputs $X_1$ and $X_2$ are not generally independent. After setting $U_1$ to a constant, the joint distribution of all variables is based on $p(u_0, u_2)p(x_1|u_0)p(x_2|u_0, u_2)$. To find the joint distribution of the inputs, we must marginalize over $U_0$ and $U_2$:\n    $$p(x_1, x_2) = \\sum_{u_0, u_2} p(u_0, u_2) p(x_1|u_0) p(x_2|u_0, u_2)$$\n    Since both $X_1$ and $X_2$ depend on the common random variable $U_0$, they are coupled and not independent, unless $U_0$ itself is a constant (which would remove the common message capability entirely).\n\nBased on this step-by-step evaluation, option D is the only one that provides an accurate and complete description.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Moving from abstract theory to practical application, we now consider the ubiquitous Gaussian Interference Channel. This problem  frames the Han-Kobayashi strategy as a resource allocation task, where a user's total power must be optimally divided between a common signal (for both receivers) and a private signal (for the intended receiver). By solving this optimization problem, you will uncover the fundamental tradeoff between decoding interference and treating it as noise.",
            "id": "1628794",
            "problem": "Consider a symmetric Gaussian Interference Channel (GIC) where two transmitter-receiver pairs communicate simultaneously. The received signals at the two receivers are given by the models:\n$$Y_1 = X_1 + \\alpha X_2 + Z_1$$\n$$Y_2 = \\alpha X_1 + X_2 + Z_2$$\nHere, $X_1$ and $X_2$ are the signals transmitted by user 1 and user 2, respectively, each with an average power constraint $E[X_i^2] \\le P$. The parameter $\\alpha$ represents the symmetric interference coefficient. The noises $Z_1$ and $Z_2$ are independent Gaussian random variables with zero mean and unit variance, i.e., $\\mathcal{N}(0,1)$.\n\nTo manage interference, a Han-Kobayashi coding scheme is employed. Each user $i$ splits their message into a common part and a private part. The transmitted signal is a superposition $X_i = X_{ic} + X_{ip}$, where $X_{ic}$ is the codeword for the common message and $X_{ip}$ is the codeword for the private message. The total power $P$ is distributed between these two parts using a power allocation factor $\\beta \\in [0,1]$, such that the power of the common part is $E[X_{ic}^2] = \\beta P$ and the power of the private part is $E[X_{ip}^2] = (1-\\beta) P$.\n\nWithin this scheme, an achievable sum-rate for a symmetric rate point ($R_1 = R_2$) is given by:\n$$R_{\\text{sum}}(\\beta) = \\frac{1}{2}\\log_2\\left(1 + \\frac{\\beta P(1+\\alpha^2)}{(1-\\beta)P(1+\\alpha^2) + 1}\\right) + \\log_2\\left(1 + \\frac{(1-\\beta)P}{\\alpha^2 (1-\\beta)P + 1}\\right)$$\nThe first term corresponds to the sum-rate of the common messages which must be decoded by both receivers, and the second term corresponds to the sum of the two private rates, where each receiver decodes its own private message.\n\nYour task is to find the optimal power allocation factor $\\beta$ that maximizes this achievable sum-rate $R_{\\text{sum}}(\\beta)$.\n\nAssume the channel operates in a weak interference regime where $0 < \\alpha < 1$. Furthermore, assume the power $P$ and interference $\\alpha$ are such that the optimal value of $\\beta$ lies strictly between 0 and 1. Your final answer should be a closed-form analytic expression for $\\beta$ in terms of $P$ and $\\alpha$.",
            "solution": "We are given the achievable sum-rate as a function of the power-split parameter $\\beta \\in [0,1]$:\n$$\nR_{\\text{sum}}(\\beta) = \\frac{1}{2}\\log_{2}\\left(1 + \\frac{\\beta P(1+\\alpha^{2})}{(1-\\beta)P(1+\\alpha^{2}) + 1}\\right) + \\log_{2}\\left(1 + \\frac{(1-\\beta)P}{\\alpha^{2} (1-\\beta)P + 1}\\right).\n$$\nMaximizing $R_{\\text{sum}}(\\beta)$ over $\\beta$ is equivalent to maximizing the same expression with $\\log_{2}$ replaced by $\\ln$, because $\\log_{2}(x) = \\ln(x)/\\ln 2$ and $\\ln 2$ is a positive constant. \n\nLet's simplify the terms inside the logarithms. Let $Q = (1-\\beta)P$.\nThe argument of the first logarithm is:\n$$\n1 + \\frac{\\beta P(1+\\alpha^{2})}{(1-\\beta)P(1+\\alpha^{2}) + 1}\n= \\frac{(1-\\beta)P(1+\\alpha^{2}) + 1 + \\beta P(1+\\alpha^{2})}{(1-\\beta)P(1+\\alpha^{2}) + 1}\n= \\frac{P(1+\\alpha^{2}) + 1}{Q(1+\\alpha^{2}) + 1}.\n$$\nThe argument of the second logarithm is:\n$$\n1 + \\frac{(1-\\beta)P}{\\alpha^{2} (1-\\beta)P + 1}\n= \\frac{\\alpha^{2} Q + 1 + Q}{\\alpha^{2} Q + 1}\n= \\frac{Q(1+\\alpha^{2}) + 1}{\\alpha^{2} Q + 1}.\n$$\nSubstituting these back into the sum-rate expression and using $\\log(a/b) = \\log(a) - \\log(b)$, we get:\n$$\nR_{\\text{sum}}(\\beta) = \\frac{1}{2}\\left[\\log_{2}\\big(P(1+\\alpha^{2}) + 1\\big) - \\log_{2}\\big(Q(1+\\alpha^{2}) + 1\\big)\\right] + \\left[\\log_{2}\\big(Q(1+\\alpha^{2}) + 1\\big) - \\log_{2}\\big(\\alpha^{2} Q + 1\\big)\\right].\n$$\nThis simplifies to:\n$$\nR_{\\text{sum}}(\\beta) = \\frac{1}{2}\\log_{2}\\big(P(1+\\alpha^{2}) + 1\\big) + \\frac{1}{2}\\log_{2}\\big(Q(1+\\alpha^{2}) + 1\\big) - \\log_{2}\\big(\\alpha^{2} Q + 1\\big).\n$$\nTo find the maximum, we can discard the constant first term and find the value of $Q$ that maximizes:\n$$\ng(Q) \\triangleq \\frac{1}{2}\\ln\\big(Q(1+\\alpha^{2}) + 1\\big) - \\ln\\big(\\alpha^{2} Q + 1\\big).\n$$\nWe compute the derivative with respect to $Q$ and set it to zero:\n$$\ng'(Q) = \\frac{1}{2}\\cdot\\frac{1+\\alpha^{2}}{Q(1+\\alpha^{2}) + 1} - \\frac{\\alpha^{2}}{\\alpha^{2} Q + 1} = 0.\n$$\n$$\n\\frac{1+\\alpha^{2}}{2(Q(1+\\alpha^{2}) + 1)} = \\frac{\\alpha^{2}}{\\alpha^{2} Q + 1}.\n$$\nCross-multiplying gives:\n$$\n(1+\\alpha^{2})(\\alpha^{2}Q + 1) = 2\\alpha^{2}(Q(1+\\alpha^{2}) + 1)\n$$\n$$\n\\alpha^{2}(1+\\alpha^{2})Q + 1 + \\alpha^2 = 2\\alpha^{2}(1+\\alpha^{2})Q + 2\\alpha^2\n$$\nRearranging the terms to solve for $Q$:\n$$\n1 - \\alpha^2 = \\alpha^2(1+\\alpha^2)Q\n$$\nHence the critical point is:\n$$\nQ^{\\star} = \\frac{1 - \\alpha^{2}}{\\alpha^{2}(1+\\alpha^{2})}.\n$$\nSince $0<\\alpha<1$, we have $1-\\alpha^{2}>0$, and the second derivative test would confirm this is a maximum. The problem assumes this interior solution is valid (i.e., $0 < Q^\\star < P$).\n\nTransforming back to $\\beta$ using $Q=(1-\\beta)P$:\n$$\n1 - \\beta^{\\star} = \\frac{Q^{\\star}}{P} = \\frac{1 - \\alpha^{2}}{P\\,\\alpha^{2}(1+\\alpha^{2})},\n$$\nso the optimal power split factor is:\n$$\n\\beta^{\\star} = 1 - \\frac{1 - \\alpha^{2}}{P\\,\\alpha^{2}(1+\\alpha^{2})}.\n$$",
            "answer": "$$\\boxed{1 - \\frac{1 - \\alpha^{2}}{P\\,\\alpha^{2}(1+\\alpha^{2})}}$$"
        },
        {
            "introduction": "The Han-Kobayashi achievable region is defined by the best performance across various decoding strategies. This practice problem  makes this concept concrete by having you calculate and compare achievable rates for two distinct decoding approaches on a specific binary interference channel. This hands-on calculation will demonstrate how different points on the boundary of the rate region correspond to asymmetric strategies where one receiver works harder to decode interference than the other.",
            "id": "53414",
            "problem": "Consider a two-user binary interference channel where the inputs are $X_1, X_2 \\in \\{0, 1\\}$ and the outputs are $Y_1, Y_2 \\in \\{0, 1\\}$. The channel is defined by the following additive noise model:\n$$Y_1 = X_1 \\oplus N_1$$\n$$Y_2 = X_2 \\oplus N_2$$\nwhere $\\oplus$ denotes addition modulo 2. The binary noise variables $N_1$ and $N_2$ are not independent of the inputs. Specifically, the noise on one link depends on the signal transmitted on the other link. This dependence is characterized by the following conditional probabilities:\n$$P(N_1=1 | X_2=x_2) = p(1-x_2)$$\n$$P(N_2=1 | X_1=x_1) = q(1-x_1)$$\nwhere $p, q \\in (0, 1)$ are constant channel parameters. This channel model is an \"Asymmetric Quieting Interference Channel\" (AQIC), where transmitting a '1' on the interfering link \"quiets\" the noise on the other link, making it a perfect channel.\n\nTwo different achievable sum-rates can be obtained from the Han-Kobayashi achievable region by considering two distinct decoding strategies at the receivers. These sum-rates are functions of the input probability distribution $p(x_1, x_2)$:\n1.  $S_A = I(X_1; Y_1 | X_2) + I(X_2; Y_2)$: This corresponds to a strategy where receiver 1 first decodes the interference from sender 2 and cancels it, while receiver 2 treats the interference from sender 1 as noise.\n2.  $S_B = I(X_1; Y_1) + I(X_2; Y_2 | X_1)$: This corresponds to the symmetric strategy where receiver 2 decodes and cancels interference, while receiver 1 treats interference as noise.\n\nTo compare these two strategies for a simple and common signaling scheme, assume the inputs are independent and uniformly distributed, i.e., $X_1 \\sim \\text{Bernoulli}(1/2)$ and $X_2 \\sim \\text{Bernoulli}(1/2)$.\n\nDerive the value of the difference $\\Delta S = S_A - S_B$ for this specific input distribution. Express your answer in terms of the channel parameters $p$ and $q$. Use the binary entropy function $h(x) = -x \\log_2 x - (1-x) \\log_2(1-x)$.",
            "solution": "We are asked to compute $\\Delta S = S_A - S_B$ for independent and uniform inputs, $X_1, X_2 \\sim \\text{Bernoulli}(1/2)$. The sum-rates are defined as:\n$$\nS_A = I(X_1; Y_1 | X_2) + I(X_2; Y_2)\n$$\n$$\nS_B = I(X_1; Y_1) + I(X_2; Y_2 | X_1)\n$$\nThus, $\\Delta S = [I(X_1; Y_1 | X_2) - I(X_1; Y_1)] - [I(X_2; Y_2 | X_1) - I(X_2; Y_2)]$.\n\nLet's calculate the terms for $S_A$.\nThe term $I(X_1; Y_1 | X_2)$ is the mutual information averaged over the distribution of $X_2$:\n$I(X_1; Y_1 | X_2) = P(X_2=0) I(X_1; Y_1 | X_2=0) + P(X_2=1) I(X_1; Y_1 | X_2=1)$.\n- If $X_2 = 0$, $P(N_1=1) = p(1-0) = p$. The channel $Y_1=X_1 \\oplus N_1$ is a binary symmetric channel (BSC) with crossover probability $p$. For a uniform input, the capacity is $I(X_1; Y_1 | X_2=0) = 1 - h(p)$.\n- If $X_2 = 1$, $P(N_1=1) = p(1-1) = 0$. The channel is noiseless ($Y_1=X_1$). The capacity is $I(X_1; Y_1 | X_2=1) = H(X_1) = 1$.\nSince $X_2$ is uniform, $P(X_2=0)=P(X_2=1)=1/2$. So, $I(X_1; Y_1 | X_2) = \\frac{1}{2}(1 - h(p)) + \\frac{1}{2}(1) = 1 - \\frac{1}{2}h(p)$.\n\nThe term $I(X_2; Y_2)$ is the capacity of the marginal channel from $X_2$ to $Y_2$. The noise $N_2$ depends on $X_1$, so we average over $X_1$:\n$P(N_2=1) = E_{X_1}[P(N_2=1|X_1)] = P(X_1=0)P(N_2=1|X_1=0) + P(X_1=1)P(N_2=1|X_1=1) = \\frac{1}{2}q(1-0) + \\frac{1}{2}q(1-1) = \\frac{q}{2}$.\nThe marginal channel is a BSC with crossover probability $q/2$. Thus, $I(X_2; Y_2) = 1 - h(q/2)$.\nCombining these, $S_A = (1 - \\frac{1}{2}h(p)) + (1 - h(q/2)) = 2 - \\frac{1}{2}h(p) - h(q/2)$.\n\nNow, let's calculate the terms for $S_B$. By symmetry of the problem structure, we can swap the roles of $(X_1, Y_1, p)$ and $(X_2, Y_2, q)$.\nThe term $I(X_2; Y_2 | X_1)$ is symmetric to $I(X_1; Y_1 | X_2)$, so by swapping $p \\leftrightarrow q$, we get $I(X_2; Y_2 | X_1) = 1 - \\frac{1}{2}h(q)$.\nThe term $I(X_1; Y_1)$ is symmetric to $I(X_2; Y_2)$, so by swapping $p \\leftrightarrow q$, we get $I(X_1; Y_1) = 1 - h(p/2)$.\nCombining these, $S_B = (1 - h(p/2)) + (1 - \\frac{1}{2}h(q)) = 2 - h(p/2) - \\frac{1}{2}h(q)$.\n\nFinally, we compute the difference $\\Delta S = S_A - S_B$:\n$$\n\\Delta S = \\left(2 - \\frac{1}{2}h(p) - h(q/2)\\right) - \\left(2 - h(p/2) - \\frac{1}{2}h(q)\\right)\n$$\n$$\n\\Delta S = h(p/2) - \\frac{1}{2}h(p) - h(q/2) + \\frac{1}{2}h(q)\n$$\nThis is the final expression for the difference in achievable sum-rates.",
            "answer": "$$\\boxed{ h\\left(\\frac{p}{2}\\right) - \\frac{1}{2} h(p) + \\frac{1}{2} h(q) - h\\left(\\frac{q}{2}\\right) }$$"
        }
    ]
}