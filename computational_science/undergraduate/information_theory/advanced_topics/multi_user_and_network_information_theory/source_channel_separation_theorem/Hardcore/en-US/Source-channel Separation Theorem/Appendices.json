{
    "hands_on_practices": [
        {
            "introduction": "The source-channel separation theorem's first lesson is that a channel must be robust enough to handle the information a source produces. This practice puts that principle into action by asking you to quantify the 'information load' of a multi-source system. By calculating the total entropy rate of two independent sensors, you will determine the absolute minimum channel capacity required for reliable data transmission .",
            "id": "1659351",
            "problem": "A research team is designing a simple wireless sensor network to monitor environmental conditions in a remote habitat. The network consists of two independent types of sensors, whose data must be multiplexed and transmitted over a single digital communication channel.\n\n- **Sensor A (Temperature):** This sensor reports the ambient temperature state as either \"Nominal\" or \"Elevated\". Extensive prior measurements indicate that the \"Nominal\" state occurs with a probability of $p_A = \\frac{7}{8}$. This sensor generates one report every 2.0 seconds.\n\n- **Sensor B (Vibration):** This sensor detects ground vibrations and classifies them into one of three states: \"Quiescent\", \"Minor\", or \"Significant\". The probabilities of these states are $p_Q = \\frac{1}{2}$, $p_M = \\frac{1}{4}$, and $p_S = \\frac{1}{4}$, respectively. This sensor generates one report every 0.5 seconds.\n\nTo ensure the data from both sensors can be reconstructed at the receiver with an arbitrarily low probability of error, the capacity of the communication channel must be sufficient to handle the total information rate generated by the sensors.\n\nCalculate the minimum required channel capacity, in bits per second, needed to reliably transmit the data from both sensors. Round your final answer to three significant figures.",
            "solution": "The minimum channel capacity, $C_{min}$, required for reliable transmission is equal to the total information rate, $R_{total}$, of the source. Since the two sensors operate independently, their total information rate is the sum of their individual information rates. We will calculate the information rate for each sensor separately and then add them. The information rate $R$ of a source is the product of its symbol rate $R_s$ and its entropy per symbol $H(X)$.\n\nFirst, let's analyze Sensor A. Let $X_A$ be the random variable representing the output of Sensor A. The possible outcomes are \"Nominal\" and \"Elevated\".\nThe probability of a \"Nominal\" reading is given as $P(X_A = \\text{Nominal}) = \\frac{7}{8}$.\nThe probability of an \"Elevated\" reading is therefore $P(X_A = \\text{Elevated}) = 1 - \\frac{7}{8} = \\frac{1}{8}$.\n\nThe entropy of Sensor A, $H(X_A)$, is calculated using the formula for the entropy of a discrete random variable, $H(X) = -\\sum_{i} p_i \\log_2(p_i)$.\n$$H(X_A) = - \\left( \\frac{7}{8} \\log_2\\left(\\frac{7}{8}\\right) + \\frac{1}{8} \\log_2\\left(\\frac{1}{8}\\right) \\right)$$\nUsing the logarithm property $\\log(a/b) = \\log(a) - \\log(b)$ and $\\log_2(1) = 0$, $\\log_2(8) = 3$:\n$$H(X_A) = - \\left( \\frac{7}{8} (\\log_2(7) - \\log_2(8)) + \\frac{1}{8} (\\log_2(1) - \\log_2(8)) \\right)$$\n$$H(X_A) = - \\left( \\frac{7}{8} (\\log_2(7) - 3) + \\frac{1}{8} (0 - 3) \\right)$$\n$$H(X_A) = - \\frac{7}{8} \\log_2(7) + \\frac{21}{8} - \\frac{3}{8} = \\frac{18}{8} - \\frac{7}{8} \\log_2(7) = 2.25 - \\frac{7}{8} \\log_2(7) \\text{ bits/symbol}$$\nLet's re-calculate:\n$H(X_A) = - \\frac{7}{8} \\log_2(7) + \\frac{21}{8} + \\frac{3}{8} = \\frac{24}{8} - \\frac{7}{8} \\log_2(7) = 3 - \\frac{7}{8} \\log_2(7)$ bits/symbol.\nA simpler calculation: $H(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$. With $p=1/8$:\n$H(1/8) = - (1/8)\\log_2(1/8) - (7/8)\\log_2(7/8) = - (1/8)(-3) - (7/8)(\\log_2(7)-3) = 3/8 - (7/8)\\log_2(7) + 21/8 = 24/8 - (7/8)\\log_2(7) = 3 - (7/8)\\log_2(7)$.\nThe original solution text had a minor calculation error (`+ 3/8` became `- 3/8`). Let's use the decimal value: $H(1/8) \\approx 0.54356$ bits/symbol.\nThe symbol rate of Sensor A is $R_{s,A} = \\frac{1 \\text{ report}}{2.0 \\text{ s}} = 0.5 \\text{ symbols/s}$.\nThe information rate of Sensor A is $R_A = R_{s,A} \\times H(X_A) \\approx 0.5 \\times 0.54356 \\approx 0.27178 \\text{ bits/s}$.\n\nNext, let's analyze Sensor B. Let $X_B$ be the random variable representing the output of Sensor B. The outcomes are \"Quiescent\", \"Minor\", and \"Significant\" with probabilities $p_Q = 1/2$, $p_M = 1/4$, and $p_S = 1/4$.\nThe entropy of Sensor B, $H(X_B)$, is:\n$$H(X_B) = - \\left( \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) \\right)$$\nSince $\\log_2(1/2) = -1$ and $\\log_2(1/4) = -2$:\n$$H(X_B) = - \\left( \\frac{1}{2} (-1) + \\frac{1}{4} (-2) + \\frac{1}{4} (-2) \\right) = - \\left( -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right) = \\frac{3}{2} = 1.5 \\text{ bits/symbol}$$\nThe symbol rate of Sensor B is $R_{s,B} = \\frac{1 \\text{ report}}{0.5 \\text{ s}} = 2.0 \\text{ symbols/s}$.\nThe information rate of Sensor B is $R_B = R_{s,B} \\times H(X_B)$:\n$$R_B = 2.0 \\times 1.5 = 3.0 \\text{ bits/s}$$\n\nThe total information rate $R_{total}$ is the sum of $R_A$ and $R_B$:\n$$R_{total} = R_A + R_B \\approx 0.27178 + 3.0 = 3.27178 \\text{ bits/s}$$\nThe minimum required channel capacity is $C_{min} = R_{total}$.\nRounding the result to three significant figures, we get:\n$$C_{min} \\approx 3.27 \\text{ bits/s}$$",
            "answer": "$$\\boxed{3.27}$$"
        },
        {
            "introduction": "Theoretical limits are powerful, but practical systems often operate with constraints. This exercise explores the crucial distinction between a source's fundamental entropy and the actual data rate produced by a specific, non-optimal encoder. You will see why the channel capacity must match the rate of the encoded data, not just the source's theoretical information content, providing a key insight into real-world system design .",
            "id": "1659327",
            "problem": "A minimalist weather sensor is designed to report atmospheric conditions at a remote location. It can only report one of three states: 'Sunny' (S), 'Cloudy' (C), or 'Rainy' (R). Based on historical data for the region, the probabilities of these states occurring are $P(S) = 0.5$, $P(C) = 0.25$, and $P(R) = 0.25$.\n\nTo simplify the electronic design, the engineers have implemented a simple, fixed-length binary code to represent these states before transmission. Specifically, every state reported by the sensor is encoded into a 2-bit codeword. This encoded binary stream is then transmitted over a noisy wireless channel.\n\nAccording to Shannon's channel coding theorem, for data to be transmitted reliably (with an arbitrarily low probability of error), the rate of the data entering the channel must be less than the channel's capacity.\n\nWhat is the absolute minimum channel capacity required to ensure reliable transmission of the data generated by this specific weather sensor's encoder? Express your answer in units of bits per sensor reading.",
            "solution": "Let the weather state be a random variable $X \\in \\{S,C,R\\}$ with probabilities $P(S)=0.5$, $P(C)=0.25$, and $P(R)=0.25$. Its entropy, which is the theoretical lower bound for average lossless source coding, is\n$$\nH(X)=-\\sum_{x}P(x)\\log_{2}P(x)\n= -\\left[0.5\\log_{2}(0.5)+0.25\\log_{2}(0.25)+0.25\\log_{2}(0.25)\\right]\n= 1.5\\ \\text{bits per reading}.\n$$\nHowever, the implemented encoder uses a fixed-length code with exactly 2 bits per state, so the data rate entering the channel is\n$$\nR_{\\text{in}} = 2\\ \\text{bits per sensor reading}.\n$$\nBy Shannonâ€™s channel coding theorem, reliable transmission with arbitrarily low error requires the data rate to be less than the channel capacity, i.e., $R_{\\text{in}}  C$. Therefore, the absolute minimum capacity threshold that must be met is\n$$\nC_{\\min} = R_{\\text{in}} = 2\\ \\text{bits per sensor reading},\n$$\nnoting that in practice one requires $C  2$, but the minimum threshold value is 2.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Having explored the source and the encoder, we now unite them with the channel itself. This problem integrates both sides of the source-channel separation theorem, requiring you to calculate a source's entropy and a channel's capacity to find the maximum rate of reliable communication. This exercise provides a complete, quantitative application of the theorem's core condition for reliable transmission, $H(S) \\lt C$, to determine the ultimate performance limit of a communication system .",
            "id": "1659347",
            "problem": "An extraterrestrial probe sends data from a distant planet. The data is generated by a discrete memoryless source that produces binary symbols, '0' and '1'. Due to the power source characteristics, the symbol '1' is generated with a probability of $P(1) = 0.1$. This data is encoded and transmitted through a deep space communication channel. The channel can be accurately modeled as a Binary Erasure Channel (BEC), where each transmitted bit is either received correctly or is erased. The probability of an erasure is $\\epsilon = 0.5$.\n\nAccording to the source-channel separation theorem, which sets the fundamental limit for reliable communication, what is the theoretical maximum number of source symbols that can be transmitted per 100 uses of this channel? Provide your answer as a numerical value, rounded to three significant figures.",
            "solution": "The binary source with $P(1)=0.1$ and $P(0)=0.9$ has entropy (in bits per source symbol) given by\n$$\nH(S)=-\\sum_{x \\in \\{0,1\\}} P(x)\\log_{2}P(x)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9).\n$$\nA Binary Erasure Channel with erasure probability $\\epsilon$ has capacity\n$$\nC=1-\\epsilon \\quad \\text{bits per channel use}.\n$$\nBy the source-channel separation theorem, reliable lossless transmission is possible if and only if the number of source symbols per channel use, denoted $\\alpha$, satisfies\n$$\n\\alpha H(S) \\leq C,\n$$\nso the theoretical maximum is $\\alpha_{\\max}=C/H(S)$. Over $100$ channel uses, the maximum number of source symbols is\n$$\nN_{\\max}=100\\,\\frac{C}{H(S)}=100\\,\\frac{1-\\epsilon}{-\\;0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)}.\n$$\nSubstituting $\\epsilon=0.5$ and evaluating the logarithms,\n$$\n\\log_{2}(0.1)\\approx -3.32192809489,\\quad \\log_{2}(0.9)\\approx -0.152003093445,\n$$\ngives\n$$\nH(S)\\approx -0.1(-3.321928) - 0.9(-0.152003) \\approx 0.3321928 + 0.1368027 = 0.4689955 \\text{ bits/symbol},\n$$\nand\n$$\nN_{\\max}=\\frac{100 \\times (1-0.5)}{0.4689955} = \\frac{50}{0.4689955}\\approx 106.611.\n$$\nRounding to three significant figures yields $107$.",
            "answer": "$$\\boxed{107}$$"
        }
    ]
}