{
    "hands_on_practices": [
        {
            "introduction": "To truly understand convolutional codes, we must start with the fundamental process of encoding. This exercise walks you through the step-by-step operation of a simple convolutional encoder, showing how an input message is transformed into a longer, redundant sequence. By manually tracing the state changes and output calculations, you will gain a concrete intuition for the inner workings of these powerful codes .",
            "id": "1614381",
            "problem": "A deep space probe utilizes a simple Forward Error Correction (FEC) system to protect its telemetry data. The system employs a convolutional encoder with a code rate of $R = 1/2$ and a memory of $\\nu=2$. This means for each bit of the input message, the encoder produces two output bits, and its calculations depend on the current input bit and the two preceding input bits.\n\nThe encoder's operation is defined by two generator connections. Let the input message bit at time step $k$ be $m_k$. The two previous message bits, $m_{k-1}$ and $m_{k-2}$, are stored in the encoder's memory registers. The two output bits at time step $k$, denoted $c_k^{(1)}$ and $c_k^{(2)}$, are generated by the following rules, where all additions are performed modulo 2:\n$$\n\\begin{cases}\nc_k^{(1)} = m_k + m_{k-1} + m_{k-2} \\\\\nc_k^{(2)} = m_k + m_{k-2}\n\\end{cases}\n$$\nAssume the encoder is in the all-zero state initially, meaning the memory registers corresponding to $m_{k-1}$ and $m_{k-2}$ are both zero before the first message bit is processed. The encoder is tasked with encoding the input message sequence $m = (1, 1, 0, 1)$. The final output is a single bit stream formed by interleaving the two output sequences, i.e., $c = (c_1^{(1)}, c_1^{(2)}, c_2^{(1)}, c_2^{(2)}, c_3^{(1)}, c_3^{(2)}, \\dots)$.\n\nWhat are the first 8 bits of the encoded output sequence?\n\nA. 11101000\n\nB. 11000101\n\nC. 11010100\n\nD. 11100001\n\nE. 01100110",
            "solution": "All additions are modulo 2. The encoder has memory of two previous bits, with initial state $m_{0}=0$ and $m_{-1}=0$. The generators are\n$$\n\\begin{cases}\nc_{k}^{(1)}=m_{k}+m_{k-1}+m_{k-2},\\\\\nc_{k}^{(2)}=m_{k}+m_{k-2}.\n\\end{cases}\n$$\nGiven $m=(m_{1},m_{2},m_{3},m_{4})=(1,1,0,1)$, compute for each $k$:\n\nFor $k=1$: $m_{1}=1$, $m_{0}=0$, $m_{-1}=0$,\n$$\nc_{1}^{(1)}=1+0+0=1,\\quad c_{1}^{(2)}=1+0=1.\n$$\n\nFor $k=2$: $m_{2}=1$, $m_{1}=1$, $m_{0}=0$,\n$$\nc_{2}^{(1)}=1+1+0=0,\\quad c_{2}^{(2)}=1+0=1.\n$$\n\nFor $k=3$: $m_{3}=0$, $m_{2}=1$, $m_{1}=1$,\n$$\nc_{3}^{(1)}=0+1+1=0,\\quad c_{3}^{(2)}=0+1=1.\n$$\n\nFor $k=4$: $m_{4}=1$, $m_{3}=0$, $m_{2}=1$,\n$$\nc_{4}^{(1)}=1+0+1=0,\\quad c_{4}^{(2)}=1+1=0.\n$$\n\nInterleaving yields\n$$\nc=(c_{1}^{(1)},c_{1}^{(2)},c_{2}^{(1)},c_{2}^{(2)},c_{3}^{(1)},c_{3}^{(2)},c_{4}^{(1)},c_{4}^{(2)})=(1,1,0,1,0,1,0,0),\n$$\nwhich corresponds to $11010100$, i.e., option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Not all convolutional codes offer the same level of protection against errors. A crucial metric for evaluating an encoder's performance is its free distance, $d_{\\text{free}}$, which quantifies its error-correcting capability. This practice challenges you to compare two different encoder designs by calculating their respective free distances, providing direct insight into how small changes in the generator vectors can significantly impact a code's robustness .",
            "id": "1614368",
            "problem": "In the design of a digital communication system, two engineers have proposed different convolutional encoders for forward error correction. Both encoders have a rate of $R=1/2$ and a memory of $\\nu=2$. All arithmetic is performed over the binary field $\\mathrm{GF}(2)$.\n\nThe state of the encoder at time $t$ is determined by the previous two message bits, $s_t=(m_{t-1}, m_{t-2})$. The output at time $t$, a pair of bits $(c_t^{(1)}, c_t^{(2)})$, is generated by convolving the input message sequence $m = (m_0, m_1, m_2, \\dots)$ with the encoder's generator vectors.\n\nThe two proposed designs are:\n\n**Encoder A:**\n- Generator vector for the first output bit: $g_A^{(1)} = (1, 1, 1)$\n- Generator vector for the second output bit: $g_A^{(2)} = (1, 0, 1)$\n- The output bits are calculated as $c_t^{(1)} = m_t + m_{t-1} + m_{t-2}$ and $c_t^{(2)} = m_t + m_{t-2}$.\n\n**Encoder B:**\n- Generator vector for the first output bit: $g_B^{(1)} = (1, 1, 0)$\n- Generator vector for the second output bit: $g_B^{(2)} = (1, 1, 1)$\n- The output bits are calculated as $c_t^{(1)} = m_t + m_{t-1}$ and $c_t^{(2)} = m_t + m_{t-1} + m_{t-2}$.\n\nThe error-correction capability of a convolutional code is primarily determined by its free distance, $d_{\\text{free}}$, which is defined as the minimum Hamming weight of any non-zero codeword. A higher free distance indicates a more robust code.\n\nYour task is to evaluate the two designs. Calculate the free distance for each encoder, $d_{\\text{free, A}}$ and $d_{\\text{free, B}}$, and determine which design is superior. Select the correct statement from the options below.\n\nA. $d_{\\text{free, A}} = 4$, $d_{\\text{free, B}} = 5$; Encoder B is superior.\n\nB. $d_{\\text{free, A}} = 5$, $d_{\\text{free, B}} = 4$; Encoder A is superior.\n\nC. $d_{\\text{free, A}} = 5$, $d_{\\text{free, B}} = 5$; The encoders have equivalent performance.\n\nD. $d_{\\text{free, A}} = 6$, $d_{\\text{free, B}} = 5$; Encoder A is superior.\n\nE. $d_{\\text{free, A}} = 5$, $d_{\\text{free, B}} = 6$; Encoder B is superior.",
            "solution": "We work over $\\mathrm{GF}(2)$, so all additions are modulo $2$. The free distance $d_{\\text{free}}$ is the minimum Hamming weight of any non-zero codeword. For a rate-$\\frac{1}{2}$, memory-$\\nu=2$ encoder, the trellis state is $s_{t}=(m_{t-1},m_{t-2})$. To find $d_{\\text{free}}$, we consider paths that start in the all-zero state, diverge at the first nonzero input (w.l.o.g. at $t=0$ with $m_{0}=1$), and remerge to the all-zero state as soon as possible (which requires two trailing zeros, because $\\nu=2$). We then minimize the total output Hamming weight along the divergeâ€“remerge path.\n\nEncoder A has $g_{A}^{(1)}=(1,1,1)$ and $g_{A}^{(2)}=(1,0,1)$, so\n$$\nc_{t}^{(1)}=m_{t}+m_{t-1}+m_{t-2},\\qquad c_{t}^{(2)}=m_{t}+m_{t-2}.\n$$\nStart in state $(0,0)$ and let $m_{0}=1$. With $m_{-1}=m_{-2}=0$:\n- At $t=0$: $c_{0}^{(1)}=1$, $c_{0}^{(2)}=1$, so the branch weight is $2$.\n- At $t=1$, with a generic $m_{1}\\in\\{0,1\\}$ and $(m_{0},m_{-1})=(1,0)$:\n$$\nc_{1}^{(1)}=m_{1}+1+0=m_{1}+1,\\qquad c_{1}^{(2)}=m_{1}+0=m_{1}.\n$$\nIf both $c_{1}^{(1)}=0$ and $c_{1}^{(2)}=0$, then $m_{1}+1=0$ and $m_{1}=0$, which is impossible. Therefore the branch weight at $t=1$ is at least $1$. The minimum is attained by $m_{1}=0$, which gives $(c_{1}^{(1)},c_{1}^{(2)})=(1,0)$ of weight $1$.\n- To remerge to $(0,0)$ at the earliest time $t=3$, we must set $(m_{2},m_{1})=(0,0)$. Then at $t=2$ with $(m_{2},m_{1},m_{0})=(0,0,1)$ we get\n$$\nc_{2}^{(1)}=0+0+1=1,\\qquad c_{2}^{(2)}=0+1=1,\n$$\nso the branch weight is $2$.\n\nThus the minimum total weight along an earliest remerging path is $2+1+2=5$, achieved by the input $(m_{0},m_{1},m_{2})=(1,0,0)$. Hence $d_{\\text{free,A}}=5$.\n\nEncoder B has $g_{B}^{(1)}=(1,1,0)$ and $g_{B}^{(2)}=(1,1,1)$, so\n$$\nc_{t}^{(1)}=m_{t}+m_{t-1},\\qquad c_{t}^{(2)}=m_{t}+m_{t-1}+m_{t-2}.\n$$\nStart in state $(0,0)$ and let $m_{0}=1$:\n- At $t=0$: $c_{0}^{(1)}=1$, $c_{0}^{(2)}=1$, branch weight $2$.\n- At $t=1$ with generic $m_{1}$ and $(m_{0},m_{-1})=(1,0)$:\n$$\nc_{1}^{(1)}=m_{1}+1,\\qquad c_{1}^{(2)}=m_{1}+1+0=m_{1}+1.\n$$\nChoosing $m_{1}=1$ makes $(c_{1}^{(1)},c_{1}^{(2)})=(0,0)$, i.e., branch weight $0$ and state $(m_{1},m_{0})=(1,1)$.\n- To remerge earliest at $t=4$, set $m_{2}=0$ and $m_{3}=0$. Then\nat $t=2$ with $(m_{2},m_{1},m_{0})=(0,1,1)$:\n$$\nc_{2}^{(1)}=0+1=1,\\qquad c_{2}^{(2)}=0+1+1=0,\n$$\nbranch weight $1$;\nat $t=3$ with $(m_{3},m_{2},m_{1})=(0,0,1)$:\n$$\nc_{3}^{(1)}=0+0=0,\\qquad c_{3}^{(2)}=0+0+1=1,\n$$\nbranch weight $1$.\n\nTherefore the total weight along this earliest remerging path is $2+0+1+1=4$, achieved by input $(m_{0},m_{1},m_{2},m_{3})=(1,1,0,0)$. It is not possible to reduce the sum below $4$: the first branch has weight $2$, the second can be $0$ by choosing $m_{1}=1$, and (given the state $(1,1)$) the two subsequent branches necessary to remerge cannot both be $0$ simultaneously. Hence $d_{\\text{free,B}}=4$.\n\nComparing, $d_{\\text{free,A}}=5$ and $d_{\\text{free,B}}=4$, so Encoder A is superior. The correct option is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "A well-designed error-correcting system should be resilient; however, some convolutional encoders suffer from a critical flaw known as catastrophic error propagation. This is a scenario where a finite number of channel errors can cause the decoder to produce an unending stream of mistakes. This thought experiment explores such a case, demonstrating how a specific property of the generator polynomials can lead to this disastrous outcome and revealing the importance of careful encoder design .",
            "id": "1614420",
            "problem": "An information-theoretic system employs a rate $R=1/2$ convolutional encoder to protect data transmitted over a noisy channel. The encoder is a linear time-invariant system that processes an input information sequence $u = (u_1, u_2, u_3, \\dots)$ to produce a coded sequence $c = (c_1, c_2, c_3, \\dots)$, where each $c_k = (c_k^{(1)}, c_k^{(2)})$ is a pair of output bits. The encoder is defined by its generator polynomials in the delay domain, $g^{(1)}(D) = 1+D$ and $g^{(2)}(D) = 1+D^2$.\n\nThe relationship between the input sequence transform $u(D) = \\sum_{k=1}^{\\infty} u_k D^{k-1}$ and the output sequence transforms $c^{(j)}(D) = \\sum_{k=1}^{\\infty} c_k^{(j)} D^{k-1}$ is given by $c^{(j)}(D) = u(D)g^{(j)}(D)$. This corresponds to the time-domain relations $c_k^{(1)} = u_k + u_{k-1}$ and $c_k^{(2)} = u_k + u_{k-2}$, where all additions are performed modulo-2. The encoder is assumed to start in the all-zero state, meaning $u_k = 0$ for all $k \\le 0$.\n\nSuppose the transmitted information sequence is the all-zero sequence, i.e., $u_k = 0$ for all $k \\ge 1$. Due to channel noise, a finite number of errors are introduced, and the received sequence is $r = (11, 01, 00, 00, 00, \\dots)$. A Viterbi decoder is used at the receiver to estimate the original information sequence, which we denote as $\\hat{u} = (\\hat{u}_1, \\hat{u}_2, \\hat{u}_3, \\dots)$. The Viterbi algorithm finds the path through the encoder's state trellis that has the minimum total Hamming distance to the received sequence $r$.\n\nAssuming the decoder's traceback depth is sufficiently large, which of the following statements correctly describes the eventually decoded information sequence $\\{\\hat{u}_k\\}$ for time indices $k \\ge 3$?\n\nA. $\\hat{u}_k = 0$ for all $k \\ge 3$.\n\nB. $\\hat{u}_k = 1$ for all $k \\ge 3$.\n\nC. The sequence is alternating, with $\\hat{u}_k = 1$ for odd $k \\ge 3$ and $\\hat{u}_k = 0$ for even $k \\ge 3$.\n\nD. The decoder makes one error but recovers, with $\\hat{u}_3=1$ and $\\hat{u}_k=0$ for all $k \\ge 4$.\n\nE. The decoder's output is periodic with a period of 3, given by $(\\dots, 1, 1, 0, 1, 1, 0, \\dots)$ for $k \\ge 3$.",
            "solution": "The encoder has memory $\\nu=2$ with state at time $k$ given by $(u_{k-1},u_{k-2})$. With input $u_{k}\\in\\{0,1\\}$ at state $(a,b)=(u_{k-1},u_{k-2})$, the outputs are\n$$\nc_{k}^{(1)}=u_{k}+a,\\qquad c_{k}^{(2)}=u_{k}+b,\n$$\nand the next state is $(u_{k},a)$. The received sequence is $r_{1}=(1,1)$, $r_{2}=(0,1)$, and $r_{k}=(0,0)$ for all $k\\ge 3$. The Viterbi decoder selects the path minimizing the total Hamming distance to $\\{r_{k}\\}$.\n\nFirst, observe that the information sequence $u_{k}=1$ for all $k\\ge 1$ produces\n$$\nc_{1}=(u_{1}+u_{0},u_{1}+u_{-1})=(1,1),\n$$\n$$\nc_{2}=(u_{2}+u_{1},u_{2}+u_{0})=(0,1),\n$$\n$$\nc_{3}=(u_{3}+u_{2},u_{3}+u_{1})=(0,0),\n$$\nand for all $k\\ge 4$, $c_{k}=(u_{k}+u_{k-1},u_{k}+u_{k-2})=(0,0)$. Hence this codeword matches $r$ exactly with total Hamming distance $0$. In contrast, the all-zero input yields $c_{1}=(0,0)$ and $c_{2}=(0,0)$, which differ from $r_{1}$ and $r_{2}$ with a total Hamming distance of $3$, while matching $r_{k}$ for $k\\ge 3$.\n\nTo confirm via trellis metrics, start from the all-zero state $(0,0)$ at $k=0$:\n- At $k=1$, from $(0,0)$:\n  - Input $0$ yields output $(0,0)$ with Hamming distance $2$ to $r_{1}=(1,1)$ and next state $(0,0)$.\n  - Input $1$ yields output $(1,1)$ with distance $0$ and next state $(1,0)$.\n  Survivors: metric $0$ at $(1,0)$ and metric $2$ at $(0,0)$.\n- At $k=2$, using $r_{2}=(0,1)$:\n  - From $(1,0)$ with input $1$: output $(0,1)$, distance $0$, next $(1,1)$, metric $0$.\n  - Other transitions produce metrics $2$ or $3$ at other states.\n  Survivors: metric $0$ at $(1,1)$ is strictly best.\n- At $k=3$, using $r_{3}=(0,0)$:\n  - From $(1,1)$ with input $1$: output $(0,0)$, distance $0$, next $(1,1)$, metric remains $0$.\n  - All other survivors have strictly larger metrics.\nThus from $k=2$ onward, the best path resides in state $(1,1)$ with metric $0$ and, since $r_{k}=(0,0)$ for all $k\\ge 3$, it continues with input $u_{k}=1$ (which keeps output $(0,0)$ and state $(1,1)$) without accruing any further distance. Therefore, with sufficiently large traceback depth, the decoded sequence coincides with the all-ones path, implying\n$$\n\\hat{u}_{k}=1\\quad\\text{for all }k\\ge 3.\n$$\nThis corresponds to option B.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}