{
    "hands_on_practices": [
        {
            "introduction": "我们的第一个实践是一个基础练习，旨在巩固您对核心概念的理解。我们将通过一个具体的例子来计算香农熵（Shannon entropy），这是一个量化离散随机变量不确定性的基本度量。这个过程将帮助您理解当一个随机变量由另一个随机过程转换而来时，其信息量是如何确定的。",
            "id": "1631970",
            "problem": "一个离散随机变量 $X$ 定义在结果集合 $\\{0, 1, 2, 3, 4, 5, 6, 7\\}$ 上。$X$ 的概率质量函数在该集合上是均匀分布的。第二个随机变量 $Y$ 是通过变换 $Y = X \\pmod 4$ 从 $X$ 导出的。\n\n计算随机变量 $Y$ 的香农熵。答案需以比特为单位，并以封闭形式的解析表达式给出。",
            "solution": "设 $X$ 在 $\\{0,1,2,3,4,5,6,7\\}$ 上均匀分布，因此对于每个 $x \\in \\{0,1,2,3,4,5,6,7\\}$，\n$$\n\\Pr(X=x)=\\frac{1}{8}.\n$$\n定义 $Y = X \\bmod 4$。那么 $Y$ 的取值范围是 $\\{0,1,2,3\\}$。对于任何 $y \\in \\{0,1,2,3\\}$，\n$$\n\\Pr(Y=y)=\\Pr(X \\in \\{y, y+4\\})=\\Pr(X=y)+\\Pr(X=y+4)=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\n因此 $Y$ 在 $\\{0,1,2,3\\}$ 上是均匀分布的。\n\n$Y$ 的香农熵（以比特为单位）是\n$$\nH(Y)=-\\sum_{y=0}^{3} \\Pr(Y=y)\\,\\log_{2}\\bigl(\\Pr(Y=y)\\bigr)\n= -4 \\cdot \\frac{1}{4}\\, \\log_{2}\\!\\left(\\frac{1}{4}\\right)\n= - \\log_{2}\\!\\left(\\frac{1}{4}\\right).\n$$\n使用 $\\log_{2}\\!\\left(\\frac{1}{4}\\right)=\\log_{2}\\!\\left(2^{-2}\\right)=-2$，我们得到\n$$\nH(Y)=2.\n$$\n因此熵为 $2$ 比特。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "掌握了熵的计算之后，我们将更进一步，探讨信息论在统计估计中的应用。本练习将介绍费雪信息（Fisher Information），它量化了一个可观测的随机变量中包含了多少关于其概率分布中未知参数的信息。通过分析一个数字通信系统的简化模型，您将亲身体会如何衡量从数据中估计参数的理论极限。",
            "id": "1632005",
            "problem": "在一个数字通信系统中，一个包含 $n$ 个相同比特的序列通过一个有噪声的信道传输，以建立其可靠性的基线。每个传输的比特都是“1”。由于噪声，每个比特的接收是一个独立的随机事件。正确接收到“1”的概率用 $p$ 表示，其中 $0 < p < 1$。相应地，发生错误（接收到“0”）的概率是 $1-p$。因此，这 $n$ 个接收比特的序列可以被建模为一组 $n$ 次独立的伯努利试验。\n\n费雪信息（Fisher Information）是信息论和统计学中的一个基本概念，它量化了一个随机变量所携带的关于其概率所依赖的未知参数的信息量。对于这个通信系统，我们感兴趣的参数是成功概率 $p$。\n\n请基于这 $n$ 个接收比特的序列，确定参数 $p$ 的费雪信息 $I(p)$。请将您的答案表示为关于 $n$ 和 $p$ 的单个闭式解析表达式。",
            "solution": "问题要求解 $n$ 次独立伯努利试验的成功概率参数 $p$ 的费雪信息 $I(p)$。\n\n设接收到的比特序列由随机变量 $X_1, X_2, \\ldots, X_n$ 表示。每个 $X_i$ 是一个伯努利随机变量，其中如果比特被正确接收（成功），则 $X_i=1$，如果接收不正确（失败），则 $X_i=0$。单次试验 $X_i$ 的概率质量函数（PMF）由下式给出：\n$$P(X_i=x | p) = p^x (1-p)^{1-x} \\quad \\text{for } x \\in \\{0, 1\\}$$\n\n由于各次试验是独立的，对于一个特定的观测序列 $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$，其联合概率质量函数，即似然函数 $L(p)$，是个体PMF的乘积：\n$$L(p | \\mathbf{x}) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i}$$\n这可以通过合并指数来简化：\n$$L(p | \\mathbf{x}) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^{\\sum x_i} (1-p)^{n - \\sum x_i}$$\n令 $k = \\sum_{i=1}^{n} x_i$ 为 $n$ 次试验序列中成功的总次数（正确接收的比特数）。似然函数变为：\n$$L(p | k) = p^k (1-p)^{n-k}$$\n\n费雪信息通常使用对数似然函数（记为 $\\ell(p)$）来计算。对数似然是似然函数的自然对数：\n$$\\ell(p | k) = \\ln(L(p | k)) = \\ln(p^k (1-p)^{n-k}) = k \\ln(p) + (n-k) \\ln(1-p)$$\n\n费雪信息 $I(p)$ 的一个定义是对数似然函数关于参数 $p$ 的二阶导数的期望值的负数：\n$$I(p) = -E\\left[\\frac{\\partial^2 \\ell(p|k)}{\\partial p^2}\\right]$$\n\n首先，我们求对数似然函数关于 $p$ 的一阶导数。这被称为得分函数（score function）。\n$$\\frac{\\partial \\ell}{\\partial p} = \\frac{\\partial}{\\partial p} [k \\ln(p) + (n-k) \\ln(1-p)] = \\frac{k}{p} - \\frac{n-k}{1-p}$$\n\n接下来，我们求对数似然函数关于 $p$ 的二阶导数：\n$$\\frac{\\partial^2 \\ell}{\\partial p^2} = \\frac{\\partial}{\\partial p} \\left[\\frac{k}{p} - \\frac{n-k}{1-p}\\right] = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}(-1) = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}$$\n\n现在，我们计算这个二阶导数的期望值。这里的随机变量是 $k$，即成功的次数。由于 $k$ 是 $n$ 次独立同分布的伯努利试验（成功概率为 $p$）的总和，所以 $k$ 服从二项分布，$k \\sim \\text{Bin}(n, p)$。$k$ 的期望值是 $E[k] = np$。\n\n我们将此代入 $I(p)$ 的表达式中：\n$$I(p) = -E\\left[-\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}\\right] = E\\left[\\frac{k}{p^2} + \\frac{n-k}{(1-p)^2}\\right]$$\n根据期望的线性性质：\n$$I(p) = \\frac{E[k]}{p^2} + \\frac{E[n-k]}{(1-p)^2}$$\n我们知道 $E[k] = np$。同时，$E[n-k] = E[n] - E[k] = n - np = n(1-p)$。\n代入这些期望值：\n$$I(p) = \\frac{np}{p^2} + \\frac{n(1-p)}{(1-p)^2}$$\n简化各项可得：\n$$I(p) = \\frac{n}{p} + \\frac{n}{1-p}$$\n将各项通分合并：\n$$I(p) = n \\left(\\frac{1-p+p}{p(1-p)}\\right) = \\frac{n}{p(1-p)}$$\n\n因此，$n$ 次伯努利试验中参数 $p$ 的费雪信息为 $\\frac{n}{p(1-p)}$。这个结果代表了所有 $n$ 次试验的总信息量，它就是单次试验的费雪信息 $I_1(p) = \\frac{1}{p(1-p)}$ 的 $n$ 倍。",
            "answer": "$$\\boxed{\\frac{n}{p(1-p)}}$$"
        },
        {
            "introduction": "我们的最终练习将带您领略一个深刻的统计推断原则：最小信息原则（Principle of Minimum Information）。当新的观测数据为我们提供了对系统的新约束时，我们应如何更新我们的概率模型？该练习将引导您运用库尔贝克-莱布勒（Kullback-Leibler, KL）散度，在一个假想的三面骰子场景中，找到一个既满足新数据约束又与初始均匀信念“最接近”的概率分布。",
            "id": "1631993",
            "problem": "一位分析师正在研究一枚特殊的三面骰子，其三个面分别标有数字1、2和3。基于对称性，该分析师的初始看法是这枚骰子是公平的，即每种结果出现的概率都是1/3。这一初始看法由结果集 $X = \\{1, 2, 3\\}$ 上的均匀概率分布 $u(x)$ 来表示。\n\n在观察了大量投掷后，该分析师发现结果的样本均值恰好为2.5。这一新信息促使分析师将其看法更新为一个新的概率分布 $p(x)$。根据最小信息原则，新的分布 $p(x)$ 应该是与原始分布 $u(x)$ 最接近，同时又与新数据一致的分布。\n\n两个分布之间的“距离”由库尔贝克-莱布勒（KL）散度来衡量，其定义为 $D_{KL}(p\\|u) = \\sum_{x \\in X} p(x) \\ln\\left(\\frac{p(x)}{u(x)}\\right)$，其中 $\\ln$ 表示自然对数。\n\n在这个更新后的分布下，求投出3的概率，即 $p(3)$。请将答案表示为一个精确的解析表达式。",
            "solution": "我们必须找到在 $X=\\{1,2,3\\}$ 上的分布 $p(x)$，使其到均匀分布 $u(x)$ 的库尔贝克-莱布勒散度最小，同时满足约束条件 $\\sum_{x}p(x)=1$ 和 $\\sum_{x}x\\,p(x)=\\frac{5}{2}$。这是一个约束优化问题：\n$$\n\\min_{p}\\; D_{KL}(p\\|u)=\\sum_{x=1}^{3}p(x)\\ln\\!\\left(\\frac{p(x)}{u(x)}\\right)\n$$\n约束条件为\n$$\n\\sum_{x=1}^{3}p(x)=1,\\qquad \\sum_{x=1}^{3}x\\,p(x)=\\frac{5}{2}.\n$$\n引入拉格朗日乘子 $\\lambda$ 和 $\\mu$，并考虑\n$$\n\\mathcal{L}=\\sum_{x=1}^{3}p(x)\\ln\\!\\left(\\frac{p(x)}{u(x)}\\right)+\\lambda\\!\\left(\\sum_{x=1}^{3}p(x)-1\\right)+\\mu\\!\\left(\\sum_{x=1}^{3}x\\,p(x)-\\frac{5}{2}\\right).\n$$\n令 $\\mathcal{L}$ 关于 $p(x)$ 的偏导数为零，得到\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p(x)}=\\ln\\!\\left(\\frac{p(x)}{u(x)}\\right)+1+\\lambda+\\mu x=0,\n$$\n所以\n$$\n\\ln p(x)=\\ln u(x)-1-\\lambda-\\mu x \\quad\\Rightarrow\\quad p(x)=u(x)\\,A\\,\\exp(\\eta x),\n$$\n其中 $A=\\exp(-1-\\lambda)$ 且 $\\eta=-\\mu$。归一化条件确定了 $A$：\n$$\np(x)=\\frac{u(x)\\exp(\\eta x)}{\\sum_{k=1}^{3}u(k)\\exp(\\eta k)}.\n$$\n由于 $u(x)=\\frac{1}{3}$ 是均匀的，因子 $u$ 可以消去，于是\n$$\np(x)=\\frac{\\exp(\\eta x)}{\\sum_{k=1}^{3}\\exp(\\eta k)}.\n$$\n令 $r=\\exp(\\eta)>0$。则\n$$\np(x)=\\frac{r^{x}}{r+r^{2}+r^{3}}，\\qquad \\sum_{x=1}^{3}x\\,p(x)=\\frac{r+2r^{2}+3r^{3}}{r+r^{2}+r^{3}}=\\frac{5}{2}.\n$$\n分子分母同除以 $r$ 得到\n$$\n\\frac{1+2r+3r^{2}}{1+r+r^{2}}=\\frac{5}{2}.\n$$\n交叉相乘得到\n$$\n2(1+2r+3r^{2})=5(1+r+r^{2}) \\;\\Rightarrow\\; 2+4r+6r^{2}=5+5r+5r^{2} \\;\\Rightarrow\\; r^{2}-r-3=0.\n$$\n因此\n$$\nr=\\frac{1+\\sqrt{13}}{2}.\n$$\n现在\n$$\np(3)=\\frac{r^{3}}{r+r^{2}+r^{3}}=\\frac{r^{2}}{1+r+r^{2}}.\n$$\n使用 $r^{2}=r+3$（由 $r^{2}-r-3=0$ 得到），我们求得\n$$\np(3)=\\frac{r+3}{4+2r}=\\frac{1}{2}\\cdot\\frac{r+3}{2+r}.\n$$\n代入 $r=\\frac{1+\\sqrt{13}}{2}$，\n$$\nr+3=\\frac{7+\\sqrt{13}}{2},\\quad 2+r=\\frac{5+\\sqrt{13}}{2},\n$$\n所以\n$$\np(3)=\\frac{1}{2}\\cdot\\frac{7+\\sqrt{13}}{5+\\sqrt{13}}=\\frac{1}{2}\\cdot\\frac{(7+\\sqrt{13})(5-\\sqrt{13})}{25-13}=\\frac{22-2\\sqrt{13}}{24}=\\frac{11-\\sqrt{13}}{12}.\n$$\n因此，更新后投出3的概率是 $\\frac{11-\\sqrt{13}}{12}$。",
            "answer": "$$\\boxed{\\frac{11-\\sqrt{13}}{12}}$$"
        }
    ]
}