{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's ground our understanding of information projection with a tangible example. Imagine you have a working model for a physical system, like a loaded die, but then you receive a new, precise measurement that constrains its behavior. The principle of minimum cross-entropy states that you should update your model in the most conservative way possible. This exercise  provides a foundational hands-on practice in applying this principle, using the method of Lagrange multipliers to find the probability distribution that is minimally different from a known prior while satisfying a new linear constraint.",
            "id": "1631738",
            "problem": "An engineer is modeling a loaded six-sided die. The probability distribution for the outcomes $\\{1, 2, 3, 4, 5, 6\\}$, denoted by $Q$, is found to be non-uniform. The probabilities are given as follows: $Q(1) = 1/12$, $Q(2) = 1/6$, $Q(3) = 1/12$, $Q(4) = 1/3$, $Q(5) = 1/12$, and $Q(6) = 1/4$.\n\nThe engineer needs to find a new theoretical probability distribution, $P$, for the same die that is as \"close\" as possible to $Q$, while satisfying a new design constraint. The constraint is that the total probability of rolling an even number (an outcome in the set $\\{2, 4, 6\\}$) must be exactly $1/3$.\n\n\"Closeness\" between the two distributions $P$ and $Q$ is measured by the Kullback-Leibler (KL) divergence, defined as $D_{KL}(P || Q) = \\sum_{i=1}^6 P(i) \\ln \\left(\\frac{P(i)}{Q(i)}\\right)$, where $\\ln$ denotes the natural logarithm. Your task is to find the distribution $P$ that minimizes this divergence subject to the given constraint.\n\nDetermine the six probabilities of the new distribution, $P(1)$ through $P(6)$. Express your answer as a row matrix containing the six probabilities $[P(1), P(2), P(3), P(4), P(5), P(6)]$, with each probability given as an exact fraction.",
            "solution": "The problem is to find a probability distribution $P = (p_1, p_2, p_3, p_4, p_5, p_6)$ that minimizes the Kullback-Leibler (KL) divergence from a given distribution $Q = (q_1, q_2, q_3, q_4, q_5, q_6)$, subject to certain constraints.\n\nThe objective function to minimize is the KL divergence:\n$$ D_{KL}(P || Q) = \\sum_{i=1}^{6} p_i \\ln\\left(\\frac{p_i}{q_i}\\right) $$\nThe given distribution is $Q = (1/12, 2/12, 1/12, 4/12, 1/12, 3/12) = (1/12, 1/6, 1/12, 1/3, 1/12, 1/4)$.\n\nThere are two constraints on the distribution $P$:\n1.  The probabilities must sum to one: $\\sum_{i=1}^{6} p_i = 1$.\n2.  The sum of probabilities for even outcomes must be $1/3$: $p_2 + p_4 + p_6 = 1/3$.\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. We define the Lagrangian $\\mathcal{L}$:\n$$ \\mathcal{L}(p_1, \\dots, p_6, \\lambda_0, \\lambda_1) = \\sum_{i=1}^{6} p_i \\ln\\left(\\frac{p_i}{q_i}\\right) - \\lambda_0 \\left(\\sum_{i=1}^{6} p_i - 1\\right) - \\lambda_1 \\left(p_2 + p_4 + p_6 - \\frac{1}{3}\\right) $$\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each $p_k$ to zero.\n$$ \\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left(p_k \\ln p_k - p_k \\ln q_k\\right) - \\lambda_0 - \\lambda_1 \\delta_{k \\in E} = 0 $$\nwhere $E = \\{2, 4, 6\\}$ is the set of even outcomes and $\\delta_{k \\in E}$ is the Kronecker delta, which is 1 if $k \\in E$ and 0 otherwise.\n\nThe derivative is:\n$$ (\\ln p_k + 1) - \\ln q_k - \\lambda_0 - \\lambda_1 \\delta_{k \\in E} = 0 $$\n$$ \\ln\\left(\\frac{p_k}{q_k}\\right) = \\lambda_0 - 1 + \\lambda_1 \\delta_{k \\in E} $$\nSolving for $p_k$:\n$$ p_k = q_k \\exp(\\lambda_0 - 1 + \\lambda_1 \\delta_{k \\in E}) $$\nWe can rewrite this in a more convenient form. Let $C_0 = \\exp(\\lambda_0 - 1)$ and $C_1 = \\exp(\\lambda_1)$. The form of the solution is then:\n- For odd outcomes ($k \\in \\{1, 3, 5\\}$): $p_k = q_k C_0 \\exp(0) = C_0 q_k$.\n- For even outcomes ($k \\in \\{2, 4, 6\\}$): $p_k = q_k C_0 \\exp(\\lambda_1) = C_0 C_1 q_k$.\n\nNow we use the constraints to find the constants $C_0$ and $C_1$.\nFirst, let's calculate the sum of probabilities for odd and even outcomes in the original distribution $Q$.\nSum for odd outcomes: $Q_{odd} = q_1 + q_3 + q_5 = \\frac{1}{12} + \\frac{1}{12} + \\frac{1}{12} = \\frac{3}{12} = \\frac{1}{4}$.\nSum for even outcomes: $Q_{even} = q_2 + q_4 + q_6 = \\frac{2}{12} + \\frac{4}{12} + \\frac{3}{12} = \\frac{9}{12} = \\frac{3}{4}$.\n\nNow we apply the constraints to $P$:\nConstraint 2: $p_2 + p_4 + p_6 = 1/3$.\n$$ \\sum_{k \\in E} p_k = \\sum_{k \\in E} C_0 C_1 q_k = C_0 C_1 \\sum_{k \\in E} q_k = C_0 C_1 Q_{even} = \\frac{1}{3} $$\nSubstituting the value of $Q_{even}$:\n$$ C_0 C_1 \\left(\\frac{3}{4}\\right) = \\frac{1}{3} \\implies C_0 C_1 = \\frac{4}{9} $$\n\nConstraint 1: $\\sum_{k=1}^{6} p_k = 1$.\n$$ \\sum_{k=1}^{6} p_k = \\sum_{k \\in \\{1,3,5\\}} p_k + \\sum_{k \\in \\{2,4,6\\}} p_k = 1 $$\nThe second sum is given by constraint 2 as $1/3$. The first sum is:\n$$ \\sum_{k \\in \\{1,3,5\\}} p_k = \\sum_{k \\in \\{1,3,5\\}} C_0 q_k = C_0 \\sum_{k \\in \\{1,3,5\\}} q_k = C_0 Q_{odd} $$\nSo the first constraint becomes:\n$$ C_0 Q_{odd} + \\frac{1}{3} = 1 $$\nSubstituting the value of $Q_{odd}$:\n$$ C_0 \\left(\\frac{1}{4}\\right) + \\frac{1}{3} = 1 \\implies \\frac{C_0}{4} = 1 - \\frac{1}{3} = \\frac{2}{3} \\implies C_0 = \\frac{8}{3} $$\nNow we can find $C_1$ using $C_0 C_1 = 4/9$:\n$$ \\left(\\frac{8}{3}\\right) C_1 = \\frac{4}{9} \\implies C_1 = \\frac{4}{9} \\times \\frac{3}{8} = \\frac{12}{72} = \\frac{1}{6} $$\n\nWith $C_0 = 8/3$ and $C_0 C_1 = 4/9$, we can find each probability $p_k$:\nFor odd $k \\in \\{1, 3, 5\\}$: $p_k = C_0 q_k = \\frac{8}{3} q_k$.\n$p_1 = \\frac{8}{3} \\times \\frac{1}{12} = \\frac{8}{36} = \\frac{2}{9}$.\n$p_3 = \\frac{8}{3} \\times \\frac{1}{12} = \\frac{8}{36} = \\frac{2}{9}$.\n$p_5 = \\frac{8}{3} \\times \\frac{1}{12} = \\frac{8}{36} = \\frac{2}{9}$.\n\nFor even $k \\in \\{2, 4, 6\\}$: $p_k = (C_0 C_1) q_k = \\frac{4}{9} q_k$.\n$p_2 = \\frac{4}{9} \\times q_2 = \\frac{4}{9} \\times \\frac{1}{6} = \\frac{4}{54} = \\frac{2}{27}$.\n$p_4 = \\frac{4}{9} \\times q_4 = \\frac{4}{9} \\times \\frac{1}{3} = \\frac{4}{27}$.\n$p_6 = \\frac{4}{9} \\times q_6 = \\frac{4}{9} \\times \\frac{1}{4} = \\frac{1}{9}$.\n\nLet's check our final distribution $P$:\n$p_1+p_2+p_3+p_4+p_5+p_6 = \\frac{2}{9} + \\frac{2}{27} + \\frac{2}{9} + \\frac{4}{27} + \\frac{2}{9} + \\frac{1}{9}$\n$= \\frac{6}{27} + \\frac{2}{27} + \\frac{6}{27} + \\frac{4}{27} + \\frac{6}{27} + \\frac{3}{27} = \\frac{6+2+6+4+6+3}{27} = \\frac{27}{27} = 1$. The sum is 1.\n$p_2+p_4+p_6 = \\frac{2}{27} + \\frac{4}{27} + \\frac{1}{9} = \\frac{2}{27} + \\frac{4}{27} + \\frac{3}{27} = \\frac{9}{27} = \\frac{1}{3}$. The constraint is satisfied.\n\nThe final distribution is $P = (2/9, 2/27, 2/9, 4/27, 2/9, 1/9)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{9} & \\frac{2}{27} & \\frac{2}{9} & \\frac{4}{27} & \\frac{2}{9} & \\frac{1}{9} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Building on the basic technique, we now explore how information projection handles different types of constraints. Often, new information doesn't concern individual outcomes but rather a collective property, such as a cumulative probability. This problem  demonstrates how to find the closest distribution to a uniform prior when constrained by the probability of a range of events. Solving it reveals a key feature of the I-projection: the resulting distribution is a piecewise-scaled version of the original, partitioned by the sets defined in the constraints.",
            "id": "1631708",
            "problem": "A data acquisition system monitors a process that can occupy one of ten discrete energy states, labeled by the integers in the set $S = \\{1, 2, \\ldots, 10\\}$. The principle of maximum entropy initially leads us to model the system with a uniform probability distribution, $Q$, where $Q(x) = 1/10$ for each state $x \\in S$. An updated measurement provides a new piece of information: the cumulative probability of the system being in a state $x \\le 3$ is exactly 0.5. We need to find a new probability distribution, $P$, that incorporates this constraint while being as close as possible to our original uniform model $Q$.\n\nIn information theory, this \"closest\" distribution is the one that minimizes the Kullback-Leibler (KL) divergence (also known as relative entropy) from $Q$ to $P$. The KL divergence is given by:\n$$D_{KL}(P || Q) = \\sum_{x \\in S} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$$\nFind the probability distribution $P$ that minimizes this divergence subject to the given constraint. From this distribution, determine the probability $P(X=1)$. Round your final answer to four significant figures.",
            "solution": "We must minimize the Kullback-Leibler divergence\n$$D_{KL}(P||Q)=\\sum_{x\\in S} P(x)\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)$$\nsubject to the constraints $\\sum_{x\\in S}P(x)=1$ and $\\sum_{x\\leq 3}P(x)=0.5$, with $Q(x)=\\frac{1}{10}$ for all $x\\in S=\\{1,2,\\ldots,10\\}$.\n\nIntroduce Lagrange multipliers $\\alpha$ and $\\beta$ for the normalization and the cumulative constraint, respectively, and form the Lagrangian\n$$\\mathcal{L}=\\sum_{x\\in S} P(x)\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)+\\alpha\\left(\\sum_{x\\in S}P(x)-1\\right)+\\beta\\left(\\sum_{x\\leq 3}P(x)-0.5\\right).$$\nTaking the derivative with respect to $P(x)$ and setting it to zero yields two cases:\n- For $x\\leq 3$,\n$$\\frac{\\partial \\mathcal{L}}{\\partial P(x)}=\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)+1+\\alpha+\\beta=0 \\quad\\Rightarrow\\quad P(x)=Q(x)\\exp(-1-\\alpha-\\beta).$$\n- For $x>3$,\n$$\\frac{\\partial \\mathcal{L}}{\\partial P(x)}=\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right)+1+\\alpha=0 \\quad\\Rightarrow\\quad P(x)=Q(x)\\exp(-1-\\alpha).$$\n\nThus $P$ is constant within each group. Let $a=\\exp(-1-\\alpha-\\beta)$ and $b=\\exp(-1-\\alpha)$. Then\n$$P(x)=\\frac{1}{10}a \\text{ for } x\\leq 3,\\qquad P(x)=\\frac{1}{10}b \\text{ for } x>3.$$\nImpose the constraints. From $\\sum_{x\\leq 3}P(x)=0.5$,\n$$3\\cdot \\frac{1}{10}a=0.5 \\quad\\Rightarrow\\quad a=\\frac{5}{3}.$$\nFrom normalization $\\sum_{x\\in S}P(x)=1$,\n$$3\\cdot \\frac{1}{10}a+7\\cdot \\frac{1}{10}b=1 \\quad\\Rightarrow\\quad 3a+7b=10 \\quad\\Rightarrow\\quad 3\\cdot \\frac{5}{3}+7b=10 \\quad\\Rightarrow\\quad b=\\frac{5}{7}.$$\nTherefore,\n$$P(x)=\\frac{1}{6} \\text{ for } x\\in\\{1,2,3\\},\\qquad P(x)=\\frac{1}{14} \\text{ for } x\\in\\{4,5,6,7,8,9,10\\}.$$\nHence\n$$P(X=1)=\\frac{1}{6}.$$\nRounded to four significant figures, this is $0.1667$.",
            "answer": "$$\\boxed{0.1667}$$"
        },
        {
            "introduction": "Our final practice tackles a more complex and realistic modeling scenario. Sometimes, constraints are not just simple linear equations but define a specific structural form for the probability distribution, such as a mixture model. This advanced problem  challenges you to find an information projection onto a set of distributions that is constrained by both its structural family and a specific expected value. Mastering this requires combining the optimization tools with a deeper geometric understanding of the space of probability distributions, illustrating the power and versatility of information projection in sophisticated applications.",
            "id": "1631713",
            "problem": "A data scientist is modeling a system with a discrete random variable $X$ that can take values in the set $\\mathcal{X} = \\{0, 1, 2\\}$. Their prior belief about the system is described by a probability mass function (PMF), denoted $Q(x)$, given by:\n$$\nQ(0) = \\frac{1}{2}, \\quad Q(1) = \\frac{1}{3}, \\quad Q(2) = \\frac{1}{6}\n$$\nNew experimental data and theoretical considerations impose two significant constraints on the true PMF, denoted $P(x)$:\n\n1.  The true PMF must belong to a family of mixture models. Specifically, it must be a convex combination of three basis distributions, $P_1(x)$, $P_2(x)$, and $P_3(x)$:\n    $P(x) = \\lambda_1 P_1(x) + \\lambda_2 P_2(x) + \\lambda_3 P_3(x)$, where the mixture weights satisfy $\\lambda_1, \\lambda_2, \\lambda_3 \\ge 0$ and $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$. The basis PMFs are defined as:\n    - $P_1(x)$: $P_1(0) = 1/2, P_1(1) = 1/2, P_1(2) = 0$\n    - $P_2(x)$: $P_2(0) = 0, P_2(1) = 1/2, P_2(2) = 1/2$\n    - $P_3(x)$: $P_3(0) = 1/2, P_3(1) = 0, P_3(2) = 1/2$\n\n2.  The expected value of the random variable $X$ under the true PMF must be exactly 1. That is, $E_P[X] = 1$.\n\nAccording to the principle of minimum information (also known as the principle of minimum cross-entropy), the most rational choice for the updated PMF $P(x)$ is the one which satisfies the given constraints and is \"closest\" to the prior $Q(x)$. Closeness is measured by the Kullback-Leibler (KL) divergence, defined as $D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$, where $\\ln$ denotes the natural logarithm.\n\nDetermine the optimal probability mass function $P(x)$. Express your answer as an ordered triplet of exact analytical expressions for $(P(0), P(1), P(2))$.",
            "solution": "We represent the mixture constraint explicitly. With $P(x)=\\lambda_{1}P_{1}(x)+\\lambda_{2}P_{2}(x)+\\lambda_{3}P_{3}(x)$, $\\lambda_{i}\\ge 0$, and $\\lambda_{1}+\\lambda_{2}+\\lambda_{3}=1$, the probabilities are\n$$\nP(0)=\\tfrac{1}{2}(\\lambda_{1}+\\lambda_{3}),\\quad\nP(1)=\\tfrac{1}{2}(\\lambda_{1}+\\lambda_{2}),\\quad\nP(2)=\\tfrac{1}{2}(\\lambda_{2}+\\lambda_{3}).\n$$\nBecause each $P(i)$ is half the sum of two nonnegative weights adding to at most $1$, we have $P(i)\\le \\tfrac{1}{2}$ for $i=0,1,2$. Hence the mixture family is the subset of the simplex where $P(i)\\le \\tfrac{1}{2}$ for all $i$.\n\nThe expectation constraint is $E_{P}[X]=1$, i.e.,\n$$\nP(1)+2P(2)=1,\\quad P(0)+P(1)+P(2)=1.\n$$\nEliminating, we obtain $P(0)=P(2)$ and $P(1)=1-2P(2)$. Let $t:=P(2)$; then\n$$\nP=(P(0),P(1),P(2))=(t,\\,1-2t,\\,t).\n$$\nFeasibility requires $t\\ge 0$, $1-2t\\ge 0$, and $P(i)\\le \\tfrac{1}{2}$. The inequalities $t\\le \\tfrac{1}{2}$ and $1-2t\\le \\tfrac{1}{2}$ give $t\\in\\left[\\tfrac{1}{4},\\tfrac{1}{2}\\right]$.\n\nWe minimize the Kullback-Leibler divergence $D_{KL}(P||Q)=\\sum_{x}P(x)\\ln\\!\\big(P(x)/Q(x)\\big)$ along this feasible line, with $Q=(\\tfrac{1}{2},\\tfrac{1}{3},\\tfrac{1}{6})$. Substituting $P(t)$,\n$$\nD(t)=t\\ln\\!\\Big(\\tfrac{t}{1/2}\\Big)+(1-2t)\\ln\\!\\Big(\\tfrac{1-2t}{1/3}\\Big)+t\\ln\\!\\Big(\\tfrac{t}{1/6}\\Big)\n= t\\ln(12t^{2})+(1-2t)\\ln\\!\\big(3(1-2t)\\big).\n$$\nDifferentiate and set to zero:\n$$\nD'(t)=\\ln(12t^{2})+2 -2\\ln\\!\\big(3(1-2t)\\big)-2\n=\\ln(12t^{2})-2\\ln\\!\\big(3(1-2t)\\big)=0,\n$$\nwhich gives\n$$\n12t^{2}=\\big[3(1-2t)\\big]^{2}=9(1-2t)^{2}\n\\;\\;\\Longrightarrow\\;\\;\n4t^{2}=3(1-2t)^{2}.\n$$\nSolving $8t^{2}-12t+3=0$ yields\n$$\nt=\\frac{12\\pm\\sqrt{144-96}}{16}=\\frac{12\\pm 4\\sqrt{3}}{16}=\\frac{3\\pm \\sqrt{3}}{4}.\n$$\nThe feasible interval $t\\in\\left[\\tfrac{1}{4},\\tfrac{1}{2}\\right]$ selects $t^{\\ast}=\\frac{3-\\sqrt{3}}{4}$. Strict convexity follows from\n$$\nD''(t)=\\frac{2}{t}+\\frac{4}{1-2t}>0,\n$$\nso $t^{\\ast}$ is the unique minimizer. Therefore\n$$\nP(0)=P(2)=t^{\\ast}=\\frac{3-\\sqrt{3}}{4},\\qquad\nP(1)=1-2t^{\\ast}=\\frac{\\sqrt{3}-1}{2}.\n$$\nThis $P$ satisfies the mixture constraint with nonnegative weights (indeed $\\lambda_{1}=\\lambda_{2}=P(1)$ and $\\lambda_{3}=4t^{\\ast}-1=2-\\sqrt{3}\\ge 0$) and the expectation constraint $E_{P}[X]=1$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3-\\sqrt{3}}{4} & \\frac{\\sqrt{3}-1}{2} & \\frac{3-\\sqrt{3}}{4}\\end{pmatrix}}$$"
        }
    ]
}