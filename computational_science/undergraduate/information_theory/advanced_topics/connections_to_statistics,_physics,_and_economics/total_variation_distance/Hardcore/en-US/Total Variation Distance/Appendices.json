{
    "hands_on_practices": [
        {
            "introduction": "Understanding the total variation (TV) distance begins with direct computation. This first practice provides a foundational exercise in calculating the TV distance between two well-known discrete distributions, the binomial distributions. By working through this problem , you will see how TV distance directly measures the maximum possible advantage one could have in distinguishing between two probabilistic scenarios.",
            "id": "1664855",
            "problem": "Two competing manufacturers, \"CircuitSys\" and \"Logicore,\" produce a type of specialized logic chip. Each chip contains 3 identical processing cores. The quality of a chip is determined by the number of non-defective cores it contains.\n\nFor a chip from CircuitSys, each of the 3 cores has an independent probability of being non-defective equal to $p_C = \\frac{1}{2}$.\nFor a chip from Logicore, each of the 3 cores has an independent probability of being non-defective equal to $p_L = \\frac{1}{4}$.\n\nLet the random variable $X_C$ be the number of non-defective cores in a CircuitSys chip, and $X_L$ be the number of non-defective cores in a Logicore chip. A quality control agency wants to design a binary test to distinguish between chips from the two manufacturers. The test involves specifying a set of outcomes $A \\subseteq \\{0, 1, 2, 3\\}$, where an outcome is the number of non-defective cores. To achieve the best possible separation between the two manufacturers, the agency decides to choose the set $A$ that maximizes the absolute difference in the probabilities, $|P(X_C \\in A) - P(X_L \\in A)|$.\n\nCalculate this maximum possible difference. Express your answer as an exact fraction in its simplest form.",
            "solution": "Let $X_{C} \\sim \\mathrm{Binomial}(3,\\frac{1}{2})$ and $X_{L} \\sim \\mathrm{Binomial}(3,\\frac{1}{4})$. Their probability mass functions are\n$$\nP_{C}(k)=\\binom{3}{k}\\left(\\frac{1}{2}\\right)^{k}\\left(1-\\frac{1}{2}\\right)^{3-k}=\\binom{3}{k}\\left(\\frac{1}{2}\\right)^{3},\\quad k\\in\\{0,1,2,3\\},\n$$\n$$\nP_{L}(k)=\\binom{3}{k}\\left(\\frac{1}{4}\\right)^{k}\\left(\\frac{3}{4}\\right)^{3-k},\\quad k\\in\\{0,1,2,3\\}.\n$$\nEvaluating each,\n$$\nP_{C}(0)=\\frac{1}{8},\\quad P_{C}(1)=\\frac{3}{8},\\quad P_{C}(2)=\\frac{3}{8},\\quad P_{C}(3)=\\frac{1}{8},\n$$\n$$\nP_{L}(0)=\\left(\\frac{3}{4}\\right)^{3}=\\frac{27}{64},\\quad P_{L}(1)=3\\cdot\\frac{1}{4}\\cdot\\left(\\frac{3}{4}\\right)^{2}=\\frac{27}{64},\\quad P_{L}(2)=3\\cdot\\left(\\frac{1}{4}\\right)^{2}\\cdot\\frac{3}{4}=\\frac{9}{64},\\quad P_{L}(3)=\\left(\\frac{1}{4}\\right)^{3}=\\frac{1}{64}.\n$$\nLet $d(k)=P_{C}(k)-P_{L}(k)$. Then\n$$\nd(0)=\\frac{1}{8}-\\frac{27}{64}=-\\frac{19}{64},\\quad d(1)=\\frac{3}{8}-\\frac{27}{64}=-\\frac{3}{64},\\quad d(2)=\\frac{3}{8}-\\frac{9}{64}=\\frac{15}{64},\\quad d(3)=\\frac{1}{8}-\\frac{1}{64}=\\frac{7}{64}.\n$$\nFor any $A\\subseteq\\{0,1,2,3\\}$,\n$$\nP(X_{C}\\in A)-P(X_{L}\\in A)=\\sum_{k\\in A}d(k).\n$$\nTo maximize the absolute difference, choose $A$ to include exactly those $k$ with $d(k)0$ (this is the standard characterization of the set achieving the total variation distance). Hence take $A=\\{2,3\\}$, giving\n$$\n\\max_{A}\\left|P(X_{C}\\in A)-P(X_{L}\\in A)\\right|=\\sum_{k:d(k)0}d(k)=\\frac{15}{64}+\\frac{7}{64}=\\frac{22}{64}=\\frac{11}{32}.\n$$\nEquivalently, this equals $\\frac{1}{2}\\sum_{k=0}^{3}|P_{C}(k)-P_{L}(k)|=\\frac{1}{2}\\cdot\\frac{44}{64}=\\frac{11}{32}$.",
            "answer": "$$\\boxed{\\frac{11}{32}}$$"
        },
        {
            "introduction": "A crucial property of any good distance measure in information theory is how it behaves under data transformations. This exercise explores the data processing inequality for total variation distance, which states that processing data cannot increase the distinguishability of the underlying distributions. This practice  offers a concrete numerical example to build intuition for this fundamental principle.",
            "id": "1664823",
            "problem": "In information theory, one way to measure the \"distinguishability\" of two probability distributions is the Total Variation (TV) distance. For two probability distributions $P$ and $Q$ defined on the same discrete sample space $\\mathcal{A}$, the TV distance is given by the formula:\n$$d_{TV}(P, Q) = \\frac{1}{2} \\sum_{a \\in \\mathcal{A}} |P(a) - Q(a)|$$\n\nConsider two random variables, $X_1$ and $X_2$, which both take values from the set $\\mathcal{X} = \\{0, 1, 2, 3\\}$. Their respective probability mass functions, denoted $P_1(x)$ and $P_2(x)$, are given in the table below.\n\n| $x$ | $P_1(x)$ | $P_2(x)$ |\n|:---:|:--------:|:--------:|\n| 0   | 0.4      | 0.2      |\n| 1   | 0.3      | 0.1      |\n| 2   | 0.1      | 0.4      |\n| 3   | 0.2      | 0.3      |\n\nNow, let's define a new random variable $Y$ as a function of $X$ by the transformation $Y = X \\pmod 2$. The sample space for $Y$ is $\\mathcal{Y} = \\{0, 1\\}$. Let $Q_1(y)$ and $Q_2(y)$ be the probability mass functions for the random variables $Y_1 = X_1 \\pmod 2$ and $Y_2 = X_2 \\pmod 2$, respectively.\n\nCalculate the ratio $R = \\frac{d_{TV}(Q_1, Q_2)}{d_{TV}(P_1, P_2)}$. Round your final answer to three significant figures.",
            "solution": "We are given two pmfs $P_{1}$ and $P_{2}$ on $\\mathcal{X}=\\{0,1,2,3\\}$ and the total variation distance\n$$\nd_{TV}(P,Q)=\\frac{1}{2}\\sum_{a\\in\\mathcal{A}}|P(a)-Q(a)|.\n$$\nFirst compute $d_{TV}(P_{1},P_{2})$:\n$$\n\\sum_{x\\in\\{0,1,2,3\\}}|P_{1}(x)-P_{2}(x)|=|0.4-0.2|+|0.3-0.1|+|0.1-0.4|+|0.2-0.3|.\n$$\nEvaluating each term,\n$$\n0.2+0.2+0.3+0.1=0.8,\\quad\\text{so}\\quad d_{TV}(P_{1},P_{2})=\\frac{1}{2}\\cdot 0.8=0.4.\n$$\n\nDefine $Y=X\\bmod 2$ with $\\mathcal{Y}=\\{0,1\\}$. For $Y_{1}=X_{1}\\bmod 2$ and $Y_{2}=X_{2}\\bmod 2$, their pmfs are obtained by summing over preimages of $0$ (even) and $1$ (odd):\n$$\nQ_{1}(0)=P_{1}(0)+P_{1}(2)=0.4+0.1=0.5,\\quad Q_{1}(1)=P_{1}(1)+P_{1}(3)=0.3+0.2=0.5,\n$$\n$$\nQ_{2}(0)=P_{2}(0)+P_{2}(2)=0.2+0.4=0.6,\\quad Q_{2}(1)=P_{2}(1)+P_{2}(3)=0.1+0.3=0.4.\n$$\nNow compute $d_{TV}(Q_{1},Q_{2})$:\n$$\n\\sum_{y\\in\\{0,1\\}}|Q_{1}(y)-Q_{2}(y)|=|0.5-0.6|+|0.5-0.4|=0.1+0.1=0.2,\n$$\nhence\n$$\nd_{TV}(Q_{1},Q_{2})=\\frac{1}{2}\\cdot 0.2=0.1.\n$$\n\nThe ratio is\n$$\nR=\\frac{d_{TV}(Q_{1},Q_{2})}{d_{TV}(P_{1},P_{2})}=\\frac{0.1}{0.4}=0.25.\n$$\nRounded to three significant figures, $R=0.250$.",
            "answer": "$$\\boxed{0.250}$$"
        },
        {
            "introduction": "To fully grasp a metric, it is essential to understand its bounds and what the extreme cases represent. This problem  challenges you to construct two distributions that are maximally distant, with a $d_{TV}=1$, despite sharing the same expected value. This illustrates that TV distance captures differences in the entire probability landscape, not just summary statistics, and demonstrates the concept of perfectly distinguishable or mutually singular distributions.",
            "id": "1664827",
            "problem": "Let $P$ and $Q$ be two distinct probability distributions for a random variable $X$ defined over the sample space $\\Omega = \\{1, 2, 3, 4\\}$.\n\nThe distributions are constructed based on the following rules:\n1.  The probability mass function $P(x)$ is non-zero only for outcomes in the subset $\\{1, 4\\}$, and the probabilities for these two outcomes are equal.\n2.  The probability mass function $Q(x)$ is non-zero only for outcomes in the subset $\\{2, 3\\}$, and the probabilities for these two outcomes are also equal.\n3.  The two distributions result in the same expected value for the random variable $X$, i.e., $\\mathbb{E}_P[X] = \\mathbb{E}_Q[X]$.\n\nYour task is to calculate the total variation distance between $P$ and $Q$. The total variation distance is defined as $d_{TV}(P, Q) = \\frac{1}{2} \\sum_{x \\in \\Omega} |P(x) - Q(x)|$.\n\nExpress your answer as a single real number.",
            "solution": "The problem asks for the total variation distance between two probability distributions $P$ and $Q$ on the sample space $\\Omega = \\{1, 2, 3, 4\\}$. We must first determine the explicit forms of the probability mass functions $P(x)$ and $Q(x)$ using the given rules.\n\n**Step 1: Determine the probability distribution P.**\nAccording to rule 1, the distribution $P$ has non-zero probabilities only for $x=1$ and $x=4$. Let $P(1) = p_1$ and $P(4) = p_4$. We are told these probabilities are equal, so $p_1 = p_4$. The probabilities for other outcomes are zero: $P(2) = 0$ and $P(3) = 0$.\nFor any valid probability distribution, the sum of all probabilities over the sample space must equal 1.\n$$ \\sum_{x \\in \\Omega} P(x) = P(1) + P(2) + P(3) + P(4) = 1 $$\nSubstituting the known values and the equality condition:\n$$ p_1 + 0 + 0 + p_1 = 1 $$\n$$ 2p_1 = 1 $$\n$$ p_1 = \\frac{1}{2} $$\nTherefore, $p_1 = P(1) = 1/2$ and $p_4 = P(4) = 1/2$. The full distribution $P$ is:\n$P = (P(1), P(2), P(3), P(4)) = (\\frac{1}{2}, 0, 0, \\frac{1}{2})$.\n\n**Step 2: Determine the probability distribution Q.**\nAccording to rule 2, the distribution $Q$ has non-zero probabilities only for $x=2$ and $x=3$. Let $Q(2) = q_2$ and $Q(3) = q_3$. We are told these probabilities are equal, so $q_2 = q_3$. The probabilities for other outcomes are zero: $Q(1) = 0$ and $Q(4) = 0$.\nThe sum of probabilities must be 1:\n$$ \\sum_{x \\in \\Omega} Q(x) = Q(1) + Q(2) + Q(3) + Q(4) = 1 $$\nSubstituting the known values and the equality condition:\n$$ 0 + q_2 + q_2 + 0 = 1 $$\n$$ 2q_2 = 1 $$\n$$ q_2 = \\frac{1}{2} $$\nTherefore, $q_2 = Q(2) = 1/2$ and $q_3 = Q(3) = 1/2$. The full distribution $Q$ is:\n$Q = (Q(1), Q(2), Q(3), Q(4)) = (0, \\frac{1}{2}, \\frac{1}{2}, 0)$.\n\n**Step 3: Verify the expectation constraint.**\nRule 3 states that the expectations must be equal, $\\mathbb{E}_P[X] = \\mathbb{E}_Q[X]$. Let's verify this as a consistency check.\nThe expectation for $P$ is:\n$$ \\mathbb{E}_P[X] = \\sum_{x \\in \\Omega} x P(x) = 1 \\cdot P(1) + 2 \\cdot P(2) + 3 \\cdot P(3) + 4 \\cdot P(4) $$\n$$ \\mathbb{E}_P[X] = 1 \\cdot \\left(\\frac{1}{2}\\right) + 2 \\cdot (0) + 3 \\cdot (0) + 4 \\cdot \\left(\\frac{1}{2}\\right) = \\frac{1}{2} + 2 = \\frac{5}{2} $$\nThe expectation for $Q$ is:\n$$ \\mathbb{E}_Q[X] = \\sum_{x \\in \\Omega} x Q(x) = 1 \\cdot Q(1) + 2 \\cdot Q(2) + 3 \\cdot Q(3) + 4 \\cdot Q(4) $$\n$$ \\mathbb{E}_Q[X] = 1 \\cdot (0) + 2 \\cdot \\left(\\frac{1}{2}\\right) + 3 \\cdot \\left(\\frac{1}{2}\\right) + 4 \\cdot (0) = 1 + \\frac{3}{2} = \\frac{5}{2} $$\nSince $\\mathbb{E}_P[X] = \\mathbb{E}_Q[X] = 5/2$, the distributions we found are consistent with all the rules given in the problem statement.\n\n**Step 4: Calculate the total variation distance.**\nThe total variation distance is defined as $d_{TV}(P, Q) = \\frac{1}{2} \\sum_{x \\in \\Omega} |P(x) - Q(x)|$. We compute the sum of the absolute differences of the probabilities term by term.\n$$ \\sum_{x \\in \\Omega} |P(x) - Q(x)| = |P(1) - Q(1)| + |P(2) - Q(2)| + |P(3) - Q(3)| + |P(4) - Q(4)| $$\n$$ = \\left|\\frac{1}{2} - 0\\right| + \\left|0 - \\frac{1}{2}\\right| + \\left|0 - \\frac{1}{2}\\right| + \\left|\\frac{1}{2} - 0\\right| $$\n$$ = \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{2} = 2 $$\nNow, we multiply this sum by $1/2$ to get the total variation distance:\n$$ d_{TV}(P, Q) = \\frac{1}{2} \\times 2 = 1 $$\nThe total variation distance between the two distributions is 1.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}