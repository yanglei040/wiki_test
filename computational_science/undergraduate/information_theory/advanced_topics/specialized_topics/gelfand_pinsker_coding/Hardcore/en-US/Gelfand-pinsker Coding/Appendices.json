{
    "hands_on_practices": [
        {
            "introduction": "This first practice explores a communication channel where the state acts as a random sign flip on the transmitted signal. By calculating the capacity both with and without transmitter-side state information, you will quantify the precise advantage gained from this knowledge . This exercise highlights how Gelfand-Pinsker coding can transform a noisy channel into a near-perfect one by effectively 'inverting' the distortion before it happens.",
            "id": "1626041",
            "problem": "Consider a communication channel described by the equation $Y = S \\cdot X$, where the input $X$ is chosen by the transmitter from the set $\\{-1, 1\\}$, and the output $Y$ is observed by the receiver. The channel is subject to a multiplicative state $S$, also taking values from the set $\\{-1, 1\\}$. The state $S$ is a random variable that remains constant for each use of the channel and is drawn from a distribution given by $P(S=1)=p$ and $P(S=-1)=1-p$, with $0  p  1$.\n\nWe are interested in the capacity of this channel under two different conditions of state knowledge.\n1.  **Uninformed Case:** The realization of the state $S$ is unknown to both the transmitter and the receiver. Let the capacity in this scenario be $C_0$.\n2.  **Transmitter-Informed Case:** The realization of the state $S$ is known causally to the transmitter before it chooses the input $X$, but it remains unknown to the receiver. Let the capacity in this scenario be $C_T$.\n\nCalculate the channel capacity gain, $\\Delta C = C_T - C_0$, which represents the benefit of providing state information to the transmitter.\n\nExpress your answer as a function of $p$. Use $\\log_2$ for any logarithms, as capacity is measured in bits per channel use.",
            "solution": "We analyze the two scenarios by computing the mutual information optimized over the encoder strategy.\n\nUninformed case. The channel is $Y = S X$ with $X \\in \\{-1,1\\}$ and $S \\in \\{-1,1\\}$ independent of $X$, with $P(S=1)=p$ and $P(S=-1)=1-p$. For any input distribution $P_{X}$, the mutual information is\n$$\nI(X;Y) = H(Y) - H(Y|X).\n$$\nGiven $X=x$, $Y=Sx$ is a one-to-one function of $S$, hence $H(Y|X=x)=H(S)$ for all $x$, and thus\n$$\nH(Y|X)=H(S) = -p \\log_{2} p - (1-p)\\log_{2}(1-p).\n$$\nSince $Y \\in \\{-1,1\\}$, we have $H(Y) \\leq 1$, with equality when $Y$ is equiprobable. Choosing $P(X=1)=P(X=-1)=\\frac{1}{2}$ yields $P(Y=1)=p \\cdot \\frac{1}{2} + (1-p)\\cdot \\frac{1}{2}=\\frac{1}{2}$ and hence $H(Y)=1$. Therefore,\n$$\nC_{0}=\\max_{P_{X}} I(X;Y) = 1 - \\big[-p \\log_{2} p - (1-p)\\log_{2}(1-p)\\big].\n$$\n\nTransmitter-informed case (causal state). With causal state knowledge at the transmitter, the Shannon strategy says capacity is\n$$\nC_{T}=\\max_{P_{U},\\,x(u,s)} I(U;Y),\n$$\nwhere $U$ is an auxiliary independent of $S$, and $X=x(U,S)$. Choose $U \\in \\{-1,1\\}$ equiprobable and let $x(u,s)=u s$. Then\n$$\nY = S \\cdot X = S \\cdot (U S) = U,\n$$\ndeterministically. Hence $I(U;Y)=H(U)=1$. Since the output alphabet is binary, $I(U;Y) \\leq 1$, so this is optimal and\n$$\nC_{T}=1.\n$$\n\nCapacity gain. Combining the two,\n$$\n\\Delta C = C_{T} - C_{0} = 1 - \\left[1 - \\big(-p \\log_{2} p - (1-p)\\log_{2}(1-p)\\big)\\right]\n= -p \\log_{2} p - (1-p)\\log_{2}(1-p).\n$$\nThis equals the binary entropy $H_{2}(p)$ (with base-$2$ logarithms).",
            "answer": "$$\\boxed{-p \\log_{2} p - (1-p)\\log_{2}(1-p)}$$"
        },
        {
            "introduction": "We now turn to a canonical example of Gelfand-Pinsker coding involving additive interference on a finite alphabet. This problem demonstrates the power of pre-compensation, where the transmitter, knowing the exact interference value, can \"pre-subtract\" it from the signal . Your goal is to show how this clever encoding strategy creates a completely noiseless communication path, achieving the maximum possible rate for the given alphabet size.",
            "id": "1626068",
            "problem": "Consider a digital communication system, which we will model as a \"Cyclic Interference Channel.\" The channel takes an input symbol $X$ from the alphabet $\\mathcal{X} = \\{0, 1, \\dots, M-1\\}$, where $M$ is an integer greater than 1. The channel is affected by an interference signal $S$, which is also a symbol from the alphabet $\\mathcal{S} = \\{0, 1, \\dots, M-1\\}$. The received symbol $Y$ is given by the sum of the input and the interference, computed modulo $M$:\n$$Y = (X + S) \\pmod M$$\nThe interference signal $S$ is a random variable, uniformly distributed over its alphabet $\\mathcal{S}$. Crucially, the sequence of interference symbols is known perfectly and non-causally at the encoder before transmission begins, but it is completely unknown to the receiver.\n\nAssuming that an optimal coding strategy is used, what is the capacity of this channel? Express your answer as a symbolic expression in terms of $M$. Use the base-2 logarithm in your expression.",
            "solution": "The problem asks for the capacity of a channel with state information known non-causally at the encoder. The channel is defined by the equation $Y = (X + S) \\pmod M$, where the state $S$ is uniformly distributed over the alphabet $\\mathcal{S} = \\{0, 1, \\dots, M-1\\}$ and is known to the encoder.\n\nThe capacity of such a channel is given by the Gelfand-Pinsker formula:\n$$C = \\max_{p(u,x|s)} [I(U;Y) - I(U;S)]$$\nwhere $U$ is an auxiliary random variable. Given that the state $S$ is independent of the transmission strategy, the maximization is over the joint distribution $p(u,x)$:\n$$C = \\max_{p(u,x)} [I(U;Y) - I(U;S)]$$\n\nFirst, we establish an upper bound on the capacity. The term $I(U;Y) - I(U;S)$ is always less than or equal to $I(U;Y)$, which in turn is less than or equal to $H(Y)$. The output alphabet is $\\mathcal{Y} = \\{0, 1, \\dots, M-1\\}$, which has $M$ symbols. The maximum possible entropy for a random variable defined on an alphabet of size $M$ is $\\log_2(M)$, which occurs when the distribution is uniform. Therefore, we have the upper bound $H(Y) \\leq \\log_2(M)$, which implies that the capacity $C \\leq \\log_2(M)$.\n\nTo show that this upper bound is achievable, we must find a specific coding strategy (i.e., a choice of $U$ and a mapping from $U$ and $S$ to $X$) that yields a rate of $\\log_2(M)$.\n\nLet's choose the auxiliary random variable $U$ to be uniformly distributed on the alphabet $\\{0, 1, \\dots, M-1\\}$, and let's also choose $U$ to be independent of the state $S$. The distribution of $U$ is thus $p(u) = 1/M$ for all $u \\in \\{0, 1, \\dots, M-1\\}$.\n\nSince we have chosen $U$ to be independent of $S$, their mutual information is zero:\n$$I(U;S) = 0$$\n\nNext, we define the input symbol $X$ as a deterministic function of the auxiliary variable $U$ and the known state $S$. The encoder, knowing both $U$ (as part of the codebook) and the interference $S$, can construct the input symbol $X$ as follows:\n$$X = (U - S) \\pmod M$$\nThis is a valid mapping because for any $u, s \\in \\{0, 1, \\dots, M-1\\}$, the resulting $X$ is also guaranteed to be in the input alphabet $\\mathcal{X}$.\n\nNow, let's substitute this definition of $X$ into the channel equation to find the output $Y$:\n$$Y = (X + S) \\pmod M = ((U - S) \\pmod M + S) \\pmod M$$\nBy the properties of modular arithmetic, this simplifies to:\n$$Y = (U - S + S) \\pmod M = U \\pmod M$$\nSince $U$ is already in the set $\\{0, 1, \\dots, M-1\\}$, $U \\pmod M$ is simply $U$. Thus, we have:\n$$Y = U$$\nThe channel output is exactly the auxiliary variable we chose.\n\nWith this strategy, let's calculate the terms in the Gelfand-Pinsker formula. The first term is $I(U;Y)$. Since $Y=U$, their mutual information is equal to the entropy of $U$:\n$$I(U;Y) = I(U;U) = H(U)$$\nWe chose $U$ to be uniformly distributed over $M$ possible outcomes. The entropy of such a distribution is:\n$$H(U) = \\sum_{u=0}^{M-1} -p(u) \\log_2 p(u) = \\sum_{u=0}^{M-1} -\\frac{1}{M} \\log_2\\left(\\frac{1}{M}\\right) = M \\cdot \\left(-\\frac{1}{M} \\left(-\\log_2(M)\\right)\\right) = \\log_2(M)$$\nSo, for our chosen strategy, $I(U;Y) = \\log_2(M)$.\n\nThe achievable rate for this strategy is therefore:\n$$I(U;Y) - I(U;S) = \\log_2(M) - 0 = \\log_2(M)$$\nWe have found a coding scheme that achieves a rate of $\\log_2(M)$. Since we previously established that the capacity $C$ can be no greater than $\\log_2(M)$, we have proven that the capacity is exactly this value.\n$$C = \\log_2(M)$$\nThis result demonstrates that by knowing the interference non-causally, the encoder can \"pre-subtract\" it from the signal, effectively creating an interference-free channel with capacity $\\log_2(M)$.",
            "answer": "$$\\boxed{\\log_{2}(M)}$$"
        },
        {
            "introduction": "In many real-world scenarios, interference doesn't just corrupt the signal—it can completely block it. This exercise models such a \"fading\" channel, which is either perfectly clear or entirely unusable, depending on a random state known to the transmitter . Here, the optimal strategy isn't about canceling noise, but about adapting to the channel's quality, demonstrating another versatile application of state-aware encoding.",
            "id": "1626065",
            "problem": "Consider a digital communication channel where the transmission quality is affected by an external environmental state, $S$. The state $S$ is a binary random variable, taking values in $\\{0, 1\\}$. The state is 'good' ($S=0$) with probability $1-q$ and 'bad' ($S=1$) with probability $q$, where $0  q  1$. The state for each channel use is independent and identically distributed.\n\nThe channel input $X$ and output $Y$ are also binary, $X, Y \\in \\{0, 1\\}$. The channel's behavior is described by the conditional probabilities $p(y|x,s)$ as follows:\n- If the state is good ($S=0$), the channel is noiseless: the output is identical to the input, $Y=X$.\n- If the state is bad ($S=1$), the channel output is always 0, regardless of the input: $p(Y=0|X=0, S=1) = 1$ and $p(Y=0|X=1, S=1) = 1$.\n\nCrucially, the transmitter has non-causal knowledge of the state $S$; that is, it knows the value of $S$ before choosing the input $X$ to transmit.\n\nDetermine the capacity of this channel in bits per channel use. Express your answer as a function of $q$.",
            "solution": "Let $S \\in \\{0,1\\}$ be i.i.d. with $\\Pr[S=1]=q$ and $\\Pr[S=0]=1-q$. The channel is state-dependent with conditional law $p(y|x,s)$ defined by: if $s=0$ then $y=x$ (noiseless), and if $s=1$ then $y=0$ deterministically. The transmitter knows the state non-causally; the receiver does not.\n\nBy the Gelfand–Pinsker theorem for channels with state known non-causally at the encoder, the capacity is\n$$\nC=\\max_{p(u|s),\\,x(u,s)}\\bigl[I(U;Y)-I(U;S)\\bigr].\n$$\nWe construct an achievable scheme by choosing an auxiliary $U \\in \\{0,1\\}$ and an encoding function $x=f(u,s)$ so that $Y=U$ deterministically under the channel law. This is feasible if and only if $U$ satisfies $\\Pr[U=1|S=1]=0$ (because when $s=1$ the channel output is forced to $y=0$), and when $s=0$ we set $x=u$ so that $y=x=u$. Under this choice, $Y=U$ deterministically, hence\n$$\nI(U;Y)=H(Y), \\quad I(U;S)=I(Y;S)=H(Y)-H(Y|S),\n$$\nwhich gives the achievable value\n$$\nI(U;Y)-I(U;S)=H(Y|S).\n$$\nThe conditional distribution $p(y|s)$ is constrained by the channel and encoder choice: when $s=1$, $y=0$ almost surely, so $H(Y|S=1)=0$; when $s=0$, we may choose any $p=\\Pr[Y=1|S=0] \\in [0,1]$, giving $H(Y|S=0)=H_{2}(p)$, where $H_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$. Therefore,\n$$\nH(Y|S)=(1-q)H_{2}(p)+q\\cdot 0=(1-q)H_{2}(p),\n$$\nwhich is maximized at $p=\\frac{1}{2}$ with $H_{2}\\!\\left(\\frac{1}{2}\\right)=1$. Hence the achievable rate is\n$$\nC \\ge (1-q)\\,.\n$$\n\nFor the converse, provide the state $S$ also to the receiver, which cannot decrease capacity. The resulting channel conditioned on $S$ has capacity $1$ bit when $S=0$ (noiseless) and capacity $0$ when $S=1$ (output independent of input). Averaging over $S$ yields an upper bound\n$$\nC \\le \\max_{p(x|s)} I(X;Y|S)=(1-q)\\cdot 1 + q\\cdot 0=(1-q).\n$$\nCombining the achievability and converse gives the capacity\n$$\nC(q)=1-q \\quad \\text{bits per channel use.}\n$$",
            "answer": "$$\\boxed{1-q}$$"
        }
    ]
}